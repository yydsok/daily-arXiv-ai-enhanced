<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 77]
- [cs.CL](#cs.CL) [Total: 133]
- [cs.CV](#cs.CV) [Total: 192]
- [cs.HC](#cs.HC) [Total: 19]
- [cs.LG](#cs.LG) [Total: 196]
- [cs.MA](#cs.MA) [Total: 7]
- [cs.MM](#cs.MM) [Total: 4]
- [cs.RO](#cs.RO) [Total: 51]
- [cs.SD](#cs.SD) [Total: 18]
- [physics.med-ph](#physics.med-ph) [Total: 1]
- [stat.ML](#stat.ML) [Total: 12]
- [q-fin.ST](#q-fin.ST) [Total: 4]
- [physics.optics](#physics.optics) [Total: 1]
- [eess.SY](#eess.SY) [Total: 6]
- [cs.IR](#cs.IR) [Total: 15]
- [eess.SP](#eess.SP) [Total: 21]
- [eess.IV](#eess.IV) [Total: 9]
- [quant-ph](#quant-ph) [Total: 3]
- [q-bio.NC](#q-bio.NC) [Total: 2]
- [physics.comp-ph](#physics.comp-ph) [Total: 1]
- [cs.SE](#cs.SE) [Total: 5]
- [physics.ed-ph](#physics.ed-ph) [Total: 1]
- [cs.DB](#cs.DB) [Total: 1]
- [cs.CY](#cs.CY) [Total: 12]
- [cs.LO](#cs.LO) [Total: 1]
- [cs.CR](#cs.CR) [Total: 18]
- [cs.AR](#cs.AR) [Total: 7]
- [q-fin.CP](#q-fin.CP) [Total: 2]
- [cs.SI](#cs.SI) [Total: 1]
- [cs.DM](#cs.DM) [Total: 1]
- [cs.NE](#cs.NE) [Total: 5]
- [cs.GR](#cs.GR) [Total: 9]
- [math.PR](#math.PR) [Total: 1]
- [cs.CE](#cs.CE) [Total: 2]
- [q-bio.BM](#q-bio.BM) [Total: 3]
- [econ.GN](#econ.GN) [Total: 1]
- [stat.ME](#stat.ME) [Total: 1]
- [eess.AS](#eess.AS) [Total: 6]
- [cs.DC](#cs.DC) [Total: 2]
- [econ.TH](#econ.TH) [Total: 1]
- [math.NA](#math.NA) [Total: 1]
- [cs.MS](#cs.MS) [Total: 1]
- [physics.chem-ph](#physics.chem-ph) [Total: 1]
- [math.OC](#math.OC) [Total: 2]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [Understanding Financial Reasoning in AI: A Multimodal Benchmark and Error Learning Approach](https://arxiv.org/abs/2506.06282)
*Shuangyan Deng,Haizhou Peng,Jiachen Xu,Chunhou Liu,Ciprian Doru Giurcuaneanu,Jiamou Liu*

Main category: cs.AI

TL;DR: 论文提出了一个金融推理新基准，结合文本和视觉数据，并引入错误感知学习框架以提升AI模型性能。


<details>
  <summary>Details</summary>
Motivation: 金融推理需要同时处理文本和复杂视觉数据，现有方法在此类任务上表现不足，需新基准和改进框架。

Method: 构建包含3,200个专家级问题的多模态金融基准，提出无需微调的错误感知学习框架，利用历史错误反馈指导推理。

Result: 多模态输入显著提升模型性能，错误反馈带来持续改进，但视觉理解和数学逻辑仍是挑战。

Conclusion: 研究展示了自反思推理在金融AI中的潜力，同时揭示了当前技术的局限性。

Abstract: Effective financial reasoning demands not only textual understanding but also
the ability to interpret complex visual data such as charts, tables, and trend
graphs. This paper introduces a new benchmark designed to evaluate how well AI
models - especially large language and multimodal models - reason in
finance-specific contexts. Covering 3,200 expert-level question-answer pairs
across 15 core financial topics, the benchmark integrates both textual and
visual modalities to reflect authentic analytical challenges in finance. To
address limitations in current reasoning approaches, we propose an error-aware
learning framework that leverages historical model mistakes and feedback to
guide inference, without requiring fine-tuning. Our experiments across
state-of-the-art models show that multimodal inputs significantly enhance
performance and that incorporating error feedback leads to consistent and
measurable improvements. The results highlight persistent challenges in visual
understanding and mathematical logic, while also demonstrating the promise of
self-reflective reasoning in financial AI systems. Our code and data can be
found at https://anonymous/FinMR/CodeData.

</details>


### [2] [Unreal Patterns](https://arxiv.org/abs/2506.06284)
*John Beverley,Jim Logan*

Main category: cs.AI

TL;DR: 提出一种框架，用于表示不存在或可能永远不会存在的实体信息，如虚构实体、蓝图、模拟和未来场景，强调实际类型的交集而非虚构实例。


<details>
  <summary>Details</summary>
Motivation: 传统方法（如虚构实例或模态逻辑）存在局限性，要么过度依赖形而上学假设，要么引入计算低效性。本文旨在提供一种实用且计算可行的解决方案。

Method: 基于基本形式本体论，提出一种结构化本体驱动方法，通过实际类型的交集建模非存在实体。

Result: 开发了一种实用且计算高效的框架，适用于处理假设或非存在实体的引用。

Conclusion: 该框架为处理非存在实体提供了实用且可实现的解决方案，优于传统方法。

Abstract: This paper introduces a framework for representing information about entities
that do not exist or may never exist, such as those involving fictional
entities, blueprints, simulations, and future scenarios. Traditional approaches
that introduce "dummy instances" or rely on modal logic are criticized, and a
proposal is defended in which such cases are modeled using the intersections of
actual types rather than specific non existent tokens. The paper positions
itself within the Basic Formal Ontology and its realist commitments,
emphasizing the importance of practical, implementable solutions over purely
metaphysical or philosophical proposals, arguing that existing approaches to
non existent entities either overcommit to metaphysical assumptions or
introduce computational inefficiencies that hinder applications. By developing
a structured ontology driven approach to unreal patterns, the paper aims to
provide a useful and computationally viable means of handling references to
hypothetical or non existent entities.

</details>


### [3] [NFISiS: New Perspectives on Fuzzy Inference Systems for Renewable Energy Forecasting](https://arxiv.org/abs/2506.06285)
*Kaike Sa Teles Rocha Alves,Eduardo Pestana de Aguiar*

Main category: cs.AI

TL;DR: 论文介绍了一个名为evolvingfuzzysystems的Python库，提供了多种Evolving Fuzzy Systems（eFS）模型的实现，旨在解决这些模型缺乏公开实现的问题。


<details>
  <summary>Details</summary>
Motivation: 由于eFS模型缺乏公开实现，限制了其可访问性和广泛应用。

Method: 开发了一个Python库，实现了多种eFS模型，并提供了训练、可视化和性能评估工具。

Result: 在fetch_california_housing数据集上评估，ePL模型在准确性和计算成本之间取得了平衡。

Conclusion: 通过公开这些模型，evolvingfuzzysystems旨在推动自适应和可解释机器学习的研究和应用。

Abstract: Evolving Fuzzy Systems (eFS) have gained significant attention due to their
ability to adaptively update their structure in response to data dynamics while
maintaining interpretability. However, the lack of publicly available
implementations of these models limits their accessibility and widespread
adoption. To address this gap, we present evolvingfuzzysystems, a Python
library that provides implementations of several well-established eFS models,
including ePL-KRLS-DISCO, ePL+, eMG, ePL, exTS, Simpl\_eTS, and eTS. The
library facilitates model evaluation and comparison by offering built-in tools
for training, visualization, and performance assessment. The models are
evaluated using the fetch\_california\_housing dataset, with performance
measured in terms of normalized root-mean-square error (NRMSE), non-dimensional
error index (NDEI), and mean absolute percentage error (MAPE). Additionally,
computational complexity is analyzed by measuring execution times and rule
evolution during training and testing phases. The results highlight ePL as a
simple yet efficient model that balances accuracy and computational cost,
making it particularly suitable for real-world applications. By making these
models publicly available, evolvingfuzzysystems aims to foster research and
practical applications in adaptive and interpretable machine learning.

</details>


### [4] [Deep Research Bench: Evaluating AI Web Research Agents](https://arxiv.org/abs/2506.06287)
*FutureSearch,:,Nikos I. Bosse,Jon Evans,Robert G. Gambee,Daniel Hnyk,Peter Mühlbacher,Lawrence Phillips,Dan Schwarz,Jack Wildman*

Main category: cs.AI

TL;DR: 论文提出了Deep Research Bench，用于评估AI在网页研究任务中的表现，通过冻结网页数据确保评估稳定性，并比较了不同LLM的性能。


<details>
  <summary>Details</summary>
Motivation: 现代AI常用于网页研究任务，但缺乏对网页研究代理质量的直接评估方法，尤其是考虑到网页内容的动态变化。

Method: 构建了包含89个多步骤网页研究任务的Deep Research Bench，使用冻结的网页数据（RetroSearch环境）进行离线评估，并开发了自动化工具。

Result: 离线RetroSearch代理与实时网页代理表现相当，验证了评估方法的可靠性。同时，对主流LLM进行了基准测试，并公开了结果。

Conclusion: Deep Research Bench为网页研究任务提供了稳定的评估框架，支持对LLM性能的长期跟踪和比较。

Abstract: Amongst the most common use cases of modern AI is LLM chat with web search
enabled. However, no direct evaluations of the quality of web research agents
exist that control for the continually-changing web. We introduce Deep Research
Bench, consisting of 89 multi-step web research task instances of varying
difficulty across 8 diverse task categories, with the answers carefully worked
out by skilled humans. We provide a "RetroSearch" environment with a large
frozen set of scraped web pages, and demonstrate that offline "RetroSearch"
agents perform comparably to "live web" agents, enabling reliable evaluations
of models over time. We provide robust agent tooling and scaffolding to
benchmark major LLMs as they are released, including "thinking" models like o3
and Gemini 2.5 Pro. We include automated evaluations of the lengthy agent
traces to report progress over time in hallucinations, tool use, and
forgetting. Finally, we evaluate the major web research products branded as
"Deep Research", "Deep Search", "Search", or "Research." Results are available
on a public leaderboard at https://drb.futuresearch.ai/.

</details>


### [5] [Large Language Models and Their Applications in Roadway Safety and Mobility Enhancement: A Comprehensive Review](https://arxiv.org/abs/2506.06301)
*Muhammad Monjurul Karim,Yan Shi,Shucheng Zhang,Bingzhang Wang,Mehrdad Nasri,Yinhai Wang*

Main category: cs.AI

TL;DR: 本文综述了大型语言模型（LLMs）在提升道路安全和交通流动性中的应用与定制，分析了其潜力与挑战。


<details>
  <summary>Details</summary>
Motivation: 传统工程方法难以应对复杂动态的交通环境，LLMs因其自然语言理解与推理能力成为新范式。

Method: 通过架构调整、训练策略、提示工程和多模态方法，LLMs被定制以处理交通的时空数据。

Result: LLMs在交通流预测、信号控制、事故分析等方面展现出潜力，但仍面临幻觉、数据隐私等挑战。

Conclusion: LLMs有望变革交通系统，但需负责任地创新以解决当前局限。

Abstract: Roadway safety and mobility remain critical challenges for modern
transportation systems, demanding innovative analytical frameworks capable of
addressing complex, dynamic, and heterogeneous environments. While traditional
engineering methods have made progress, the complexity and dynamism of
real-world traffic necessitate more advanced analytical frameworks. Large
Language Models (LLMs), with their unprecedented capabilities in natural
language understanding, knowledge integration, and reasoning, represent a
promising paradigm shift. This paper comprehensively reviews the application
and customization of LLMs for enhancing roadway safety and mobility. A key
focus is how LLMs are adapted -- via architectural, training, prompting, and
multimodal strategies -- to bridge the "modality gap" with transportation's
unique spatio-temporal and physical data. The review systematically analyzes
diverse LLM applications in mobility (e.g., traffic flow prediction, signal
control) and safety (e.g., crash analysis, driver behavior assessment,).
Enabling technologies such as V2X integration, domain-specific foundation
models, explainability frameworks, and edge computing are also examined.
Despite significant potential, challenges persist regarding inherent LLM
limitations (hallucinations, reasoning deficits), data governance (privacy,
bias), deployment complexities (sim-to-real, latency), and rigorous safety
assurance. Promising future research directions are highlighted, including
advanced multimodal fusion, enhanced spatio-temporal reasoning, human-AI
collaboration, continuous learning, and the development of efficient,
verifiable systems. This review provides a structured roadmap of current
capabilities, limitations, and opportunities, underscoring LLMs' transformative
potential while emphasizing the need for responsible innovation to realize
safer, more intelligent transportation systems.

</details>


### [6] [Mapping Human-Agent Co-Learning and Co-Adaptation: A Scoping Review](https://arxiv.org/abs/2506.06324)
*Shruti Kumar,Xiaoyu Chen,Xiaomei Wang*

Main category: cs.AI

TL;DR: 论文探讨了人-AI-机器人协作学习与适应的术语不一致问题，并提出了三个研究问题：术语使用、智能代理与任务领域、认知理论与框架。


<details>
  <summary>Details</summary>
Motivation: 由于现有研究中术语使用不一致，且该领域较新，作者希望通过综述明确术语差异，探索智能代理与任务领域的多样性，并理解认知理论与框架的应用。

Method: 通过范围综述方法，收集并分析现有论文，重点关注术语、智能代理类型、任务领域及认知理论与框架。

Result: 研究将揭示术语差异、智能代理与任务的多样性，以及认知理论与框架的现状。

Conclusion: 该研究有助于明确人-AI-机器人协作的术语与理论框架，为未来动态复杂领域的研究提供指导。

Abstract: Several papers have delved into the challenges of human-AI-robot co-learning
and co-adaptation. It has been noted that the terminology used to describe this
collaborative relationship in existing studies needs to be more consistent. For
example, the prefix "co" is used interchangeably to represent both
"collaborative" and "mutual," and the terms "co-learning" and "co-adaptation"
are sometimes used interchangeably. However, they can reflect subtle
differences in the focus of the studies. The current scoping review's primary
research question (RQ1) aims to gather existing papers discussing this
collaboration pattern and examine the terms researchers use to describe this
human-agent relationship. Given the relative newness of this area of study, we
are also keen on exploring the specific types of intelligent agents and task
domains that have been considered in existing research (RQ2). This exploration
is significant as it can shed light on the diversity of human-agent
interactions, from one-time to continuous learning/adaptation scenarios. It can
also help us understand the dynamics of human-agent interactions in different
task domains, guiding our expectations towards research situated in dynamic,
complex domains. Our third objective (RQ3) is to investigate the cognitive
theories and frameworks that have been utilized in existing studies to measure
human-agent co-learning and co-adaptation. This investigation is crucial as it
can help us understand the theoretical underpinnings of human-agent
collaboration and adaptation, and it can also guide us in identifying any new
frameworks proposed specifically for this type of relationship.

</details>


### [7] [Memory OS of AI Agent](https://arxiv.org/abs/2506.06326)
*Jiazheng Kang,Mingming Ji,Zhe Zhao,Ting Bai*

Main category: cs.AI

TL;DR: 论文提出了一种名为MemoryOS的内存操作系统，用于解决大型语言模型（LLMs）在固定上下文窗口和内存管理不足时的长期记忆能力不足问题。


<details>
  <summary>Details</summary>
Motivation: LLMs在交互体验中因固定上下文窗口和内存管理不足导致长期记忆能力不足和个性化受限，亟需解决方案。

Method: 提出MemoryOS，采用分层存储架构（短、中、长期记忆）和四个关键模块（存储、更新、检索、生成），并设计动态更新策略。

Result: 在LoCoMo基准测试中，F1和BLEU-1分别平均提升49.11%和46.18%，显著提升上下文连贯性和个性化记忆保留。

Conclusion: MemoryOS通过分层存储和动态更新策略，有效解决了LLMs的长期记忆和个性化问题，实验验证了其优越性。

Abstract: Large Language Models (LLMs) face a crucial challenge from fixed context
windows and inadequate memory management, leading to a severe shortage of
long-term memory capabilities and limited personalization in the interactive
experience with AI agents. To overcome this challenge, we innovatively propose
a Memory Operating System, i.e., MemoryOS, to achieve comprehensive and
efficient memory management for AI agents. Inspired by the memory management
principles in operating systems, MemoryOS designs a hierarchical storage
architecture and consists of four key modules: Memory Storage, Updating,
Retrieval, and Generation. Specifically, the architecture comprises three
levels of storage units: short-term memory, mid-term memory, and long-term
personal memory. Key operations within MemoryOS include dynamic updates between
storage units: short-term to mid-term updates follow a dialogue-chain-based
FIFO principle, while mid-term to long-term updates use a segmented page
organization strategy. Our pioneering MemoryOS enables hierarchical memory
integration and dynamic updating. Extensive experiments on the LoCoMo benchmark
show an average improvement of 49.11% on F1 and 46.18% on BLEU-1 over the
baselines on GPT-4o-mini, showing contextual coherence and personalized memory
retention in long conversations. The implementation code is open-sourced at
https://github.com/BAI-LAB/MemoryOS.

</details>


### [8] [Will artificial agents pursue power by default?](https://arxiv.org/abs/2506.06352)
*Christian Tarsney*

Main category: cs.AI

TL;DR: 论文通过决策理论框架形式化工具性收敛和权力追求概念，评估权力是否为工具性收敛目标。结论认为该主张有一定道理，但预测效用可能有限。


<details>
  <summary>Details</summary>
Motivation: 研究者担忧高级AI可能带来灾难性风险，认为强大AI会追求权力，但近期有人对此表示怀疑。本文旨在形式化这些概念并评估其有效性。

Method: 采用抽象的决策理论框架分析工具性收敛和权力追求。

Result: 权力作为工具性收敛目标的主张有一定道理，但预测效用可能有限，因缺乏具体目标信息时难以对权力排序。

Conclusion: 工具性收敛对可能获得绝对或接近绝对权力的AI更具预测性。

Abstract: Researchers worried about catastrophic risks from advanced AI have argued
that we should expect sufficiently capable AI agents to pursue power over
humanity because power is a convergent instrumental goal, something that is
useful for a wide range of final goals. Others have recently expressed
skepticism of these claims. This paper aims to formalize the concepts of
instrumental convergence and power-seeking in an abstract, decision-theoretic
framework, and to assess the claim that power is a convergent instrumental
goal. I conclude that this claim contains at least an element of truth, but
might turn out to have limited predictive utility, since an agent's options
cannot always be ranked in terms of power in the absence of substantive
information about the agent's final goals. However, the fact of instrumental
convergence is more predictive for agents who have a good shot at attaining
absolute or near-absolute power.

</details>


### [9] [Towards Foundation Model on Temporal Knowledge Graph Reasoning](https://arxiv.org/abs/2506.06367)
*Jiaxin Pan,Mojtaba Nayyeri,Osama Mohammed,Daniel Hernandez,Rongchuan Zhang,Cheng Cheng,Steffen Staab*

Main category: cs.AI

TL;DR: 论文提出了一种完全归纳的时序知识图谱链接预测方法POSTRA，通过正弦位置编码和消息传递生成适应性表示，解决了现有模型对新领域泛化能力不足的问题。


<details>
  <summary>Details</summary>
Motivation: 现有时序知识图谱嵌入模型在推理时依赖训练中观察到的元素，限制了其在新领域和真实场景中的泛化能力。

Method: 采用正弦位置编码捕捉细粒度时序模式，并通过消息传递生成基于局部和全局时序上下文的自适应实体和关系表示。

Result: POSTRA在未见过的时序知识图谱上表现出强大的零样本性能，能够泛化到新实体、关系和时间戳。

Conclusion: POSTRA作为预训练、可扩展和可迁移的模型，为时序知识图谱的基础模型迈出了重要一步。

Abstract: Temporal Knowledge Graphs (TKGs) store temporal facts with quadruple formats
(s, p, o, t). Existing Temporal Knowledge Graph Embedding (TKGE) models perform
link prediction tasks in transductive or semi-inductive settings, which means
the entities, relations, and temporal information in the test graph are fully
or partially observed during training. Such reliance on seen elements during
inference limits the models' ability to transfer to new domains and generalize
to real-world scenarios. A central limitation is the difficulty in learning
representations for entities, relations, and timestamps that are transferable
and not tied to dataset-specific vocabularies. To overcome these limitations,
we introduce the first fully-inductive approach to temporal knowledge graph
link prediction. Our model employs sinusoidal positional encodings to capture
fine-grained temporal patterns and generates adaptive entity and relation
representations using message passing conditioned on both local and global
temporal contexts. Our model design is agnostic to temporal granularity and
time span, effectively addressing temporal discrepancies across TKGs and
facilitating time-aware structural information transfer. As a pretrained,
scalable, and transferable model, POSTRA demonstrates strong zero-shot
performance on unseen temporal knowledge graphs, effectively generalizing to
novel entities, relations, and timestamps. Extensive theoretical analysis and
empirical results show that a single pretrained model can improve zero-shot
performance on various inductive temporal reasoning scenarios, marking a
significant step toward a foundation model for temporal KGs.

</details>


### [10] [SIGMA: Refining Large Language Model Reasoning via Sibling-Guided Monte Carlo Augmentation](https://arxiv.org/abs/2506.06470)
*Yanwei Ren,Haotian Zhang,Fuxiang Wu,Jiayan Qiu,Jiaxing Huang,Baosheng Yu,Liu Liu*

Main category: cs.AI

TL;DR: SIGMA框架通过重新利用搜索树中被丢弃的兄弟节点，提升大语言模型的推理能力，显著减少数据需求并提高准确性。


<details>
  <summary>Details</summary>
Motivation: 传统方法仅保留搜索树中的最优路径，忽略了兄弟节点中的有价值信息，导致数据浪费。

Method: 提出SIGMA框架，通过两阶段优化（批判模型和修订模型）重新整合兄弟节点中的信息。

Result: 在MATH基准测试中，SIGMA调优的7B模型仅用30K样本即达到54.92%准确率，优于使用590K样本的现有模型。

Conclusion: SIGMA通过利用非最优推理分支中的信号，显著提升模型推理能力并减少数据需求。

Abstract: Enhancing large language models by simply scaling up datasets has begun to
yield diminishing returns, shifting the spotlight to data quality. Monte Carlo
Tree Search (MCTS) has emerged as a powerful technique for generating
high-quality chain-of-thought data, yet conventional approaches typically
retain only the top-scoring trajectory from the search tree, discarding sibling
nodes that often contain valuable partial insights, recurrent error patterns,
and alternative reasoning strategies. This unconditional rejection of
non-optimal reasoning branches may waste vast amounts of informative data in
the whole search tree. We propose SIGMA (Sibling Guided Monte Carlo
Augmentation), a novel framework that reintegrates these discarded sibling
nodes to refine LLM reasoning. SIGMA forges semantic links among sibling nodes
along each search path and applies a two-stage refinement: a critique model
identifies overlooked strengths and weaknesses across the sibling set, and a
revision model conducts text-based backpropagation to refine the top-scoring
trajectory in light of this comparative feedback. By recovering and amplifying
the underutilized but valuable signals from non-optimal reasoning branches,
SIGMA substantially improves reasoning trajectories. On the challenging MATH
benchmark, our SIGMA-tuned 7B model achieves 54.92% accuracy using only 30K
samples, outperforming state-of-the-art models trained on 590K samples. This
result highlights that our sibling-guided optimization not only significantly
reduces data usage but also significantly boosts LLM reasoning.

</details>


### [11] [Reinforcement Learning for Autonomous Warehouse Orchestration in SAP Logistics Execution: Redefining Supply Chain Agility](https://arxiv.org/abs/2506.06523)
*Sumanth Pillella*

Main category: cs.AI

TL;DR: 本研究提出了一种基于强化学习的框架，用于优化SAP物流执行系统中的仓库任务，显著提升了效率和准确性。


<details>
  <summary>Details</summary>
Motivation: 在供应链需求日益增长的背景下，传统仓库管理方法难以满足实时性和效率需求，因此需要一种更智能的解决方案。

Method: 通过将仓库流程建模为动态环境，利用强化学习实时优化任务分配、库存移动和订单拣选。使用包含30万条交易记录的合成数据集进行模拟。

Result: 研究实现了95%的任务优化准确率，处理时间比传统方法减少了60%。

Conclusion: 该框架为现代供应链提供了一种高效、可扩展且隐私安全的解决方案。

Abstract: In an era of escalating supply chain demands, SAP Logistics Execution (LE) is
pivotal for managing warehouse operations, transportation, and delivery. This
research introduces a pioneering framework leveraging reinforcement learning
(RL) to autonomously orchestrate warehouse tasks in SAP LE, enhancing
operational agility and efficiency. By modeling warehouse processes as dynamic
environments, the framework optimizes task allocation, inventory movement, and
order picking in real-time. A synthetic dataset of 300,000 LE transactions
simulates real-world warehouse scenarios, including multilingual data and
operational disruptions. The analysis achieves 95% task optimization accuracy,
reducing processing times by 60% compared to traditional methods.
Visualizations, including efficiency heatmaps and performance graphs, guide
agile warehouse strategies. This approach tackles data privacy, scalability,
and SAP integration, offering a transformative solution for modern supply
chains.

</details>


### [12] [ScriptDoctor: Automatic Generation of PuzzleScript Games via Large Language Models and Tree Search](https://arxiv.org/abs/2506.06524)
*Sam Earle,Ahmed Khalifa,Muhammad Umair Nasir,Zehua Jiang,Graham Todd,Andrzej Banburski-Fahey,Julian Togelius*

Main category: cs.AI

TL;DR: 论文介绍了ScriptDoctor，一个基于大型语言模型（LLM）的系统，用于自动生成和测试PuzzleScript游戏，展示了LLM在自动游戏设计中的潜力。


<details>
  <summary>Details</summary>
Motivation: 当前大型预训练模型在自动游戏设计（AGD）中的应用多依赖人工监督，缺乏长期自主生成和测试的集成方法。

Method: ScriptDoctor通过迭代循环生成和测试游戏设计，结合人工示例、引擎编译错误和基于搜索的代理进行游戏测试。

Result: ScriptDoctor成功展示了LLM在生成新颖游戏内容中的自动化潜力。

Conclusion: ScriptDoctor为LLM在开放游戏设计流程中的应用提供了具体示例，推动了AGD的发展。

Abstract: There is much interest in using large pre-trained models in Automatic Game
Design (AGD), whether via the generation of code, assets, or more abstract
conceptualization of design ideas. But so far this interest largely stems from
the ad hoc use of such generative models under persistent human supervision.
Much work remains to show how these tools can be integrated into
longer-time-horizon AGD pipelines, in which systems interface with game engines
to test generated content autonomously. To this end, we introduce ScriptDoctor,
a Large Language Model (LLM)-driven system for automatically generating and
testing games in PuzzleScript, an expressive but highly constrained description
language for turn-based puzzle games over 2D gridworlds. ScriptDoctor generates
and tests game design ideas in an iterative loop, where human-authored examples
are used to ground the system's output, compilation errors from the
PuzzleScript engine are used to elicit functional code, and search-based agents
play-test generated games. ScriptDoctor serves as a concrete example of the
potential of automated, open-ended LLM-based workflows in generating novel game
content.

</details>


### [13] [The Optimization Paradox in Clinical AI Multi-Agent Systems](https://arxiv.org/abs/2506.06574)
*Suhana Bedi,Iddah Mlauzi,Daniel Shin,Sanmi Koyejo,Nigam H. Shah*

Main category: cs.AI

TL;DR: 多智能体AI系统在临床中表现优于单智能体，但组件优化可能损害整体性能。


<details>
  <summary>Details</summary>
Motivation: 研究组件级优化与系统性能的关系，以改进临床AI系统的整合。

Method: 使用MIMIC-CDM数据集，比较单智能体与多智能体系统在诊断任务中的表现。

Result: 多智能体系统整体表现更好，但组件优化的系统诊断准确率较低。

Conclusion: AI系统整合需关注信息流和兼容性，而非仅组件优化。

Abstract: Multi-agent artificial intelligence systems are increasingly deployed in
clinical settings, yet the relationship between component-level optimization
and system-wide performance remains poorly understood. We evaluated this
relationship using 2,400 real patient cases from the MIMIC-CDM dataset across
four abdominal pathologies (appendicitis, pancreatitis, cholecystitis,
diverticulitis), decomposing clinical diagnosis into information gathering,
interpretation, and differential diagnosis. We evaluated single agent systems
(one model performing all tasks) against multi-agent systems (specialized
models for each task) using comprehensive metrics spanning diagnostic outcomes,
process adherence, and cost efficiency. Our results reveal a paradox: while
multi-agent systems generally outperformed single agents, the
component-optimized or Best of Breed system with superior components and
excellent process metrics (85.5% information accuracy) significantly
underperformed in diagnostic accuracy (67.7% vs. 77.4% for a top multi-agent
system). This finding underscores that successful integration of AI in
healthcare requires not just component level optimization but also attention to
information flow and compatibility between agents. Our findings highlight the
need for end to end system validation rather than relying on component metrics
alone.

</details>


### [14] [AI Simulation by Digital Twins: Systematic Survey, Reference Framework, and Mapping to a Standardized Architecture](https://arxiv.org/abs/2506.06580)
*Xiaoran Liu,Istvan David*

Main category: cs.AI

TL;DR: 本文探讨了数字孪生技术在AI模拟中的应用，通过系统调查22项研究，总结了技术趋势并提出了参考框架和架构指南。


<details>
  <summary>Details</summary>
Motivation: 解决现代亚符号AI在数据量和质量不足方面的挑战，利用数字孪生技术提升AI模拟的效率和安全性。

Method: 通过系统调查22项主要研究，分析技术趋势，并基于ISO 23247参考架构提出数字孪生与AI组件的参考框架。

Result: 提出了数字孪生与AI组件的参考框架和架构指南，并识别了未来研究的挑战和机会。

Conclusion: 数字孪生为AI模拟提供了新途径，但仍需进一步研究以解决现有挑战。

Abstract: Insufficient data volume and quality are particularly pressing challenges in
the adoption of modern subsymbolic AI. To alleviate these challenges, AI
simulation uses virtual training environments in which AI agents can be safely
and efficiently developed with simulated, synthetic data. Digital twins open
new avenues in AI simulation, as these high-fidelity virtual replicas of
physical systems are equipped with state-of-the-art simulators and the ability
to further interact with the physical system for additional data collection. In
this article, we report on our systematic survey of digital twin-enabled AI
simulation. By analyzing 22 primary studies, we identify technological trends
and derive a reference framework to situate digital twins and AI components.
Based on our findings, we derive a reference framework and provide
architectural guidelines by mapping it onto the ISO 23247 reference
architecture for digital twins. Finally, we identify challenges and research
opportunities for prospective researchers.

</details>


### [15] [GELD: A Unified Neural Model for Efficiently Solving Traveling Salesman Problems Across Different Scales](https://arxiv.org/abs/2506.06634)
*Yubin Xiao,Di Wang,Rui Cao,Xuan Wu,Boyang Li,You Zhou*

Main category: cs.AI

TL;DR: GELD是一种新型神经TSP求解器，通过全局评估和局部选择框架，结合轻量级全局编码器和重量级局部解码器，显著提升求解效率和质量。


<details>
  <summary>Details</summary>
Motivation: 现有神经TSP求解器难以高效处理不同规模的TSP问题，限制了实际应用。

Method: GELD结合全局编码器和局部解码器，采用低复杂度注意力机制和两阶段训练策略。

Result: GELD在合成和真实数据集上优于七种先进模型，并能处理多达744,710节点的TSP。

Conclusion: GELD在求解质量和速度上表现优异，可作为后处理方法提升现有求解器的性能。

Abstract: The Traveling Salesman Problem (TSP) is a well-known combinatorial
optimization problem with broad real-world applications. Recent advancements in
neural network-based TSP solvers have shown promising results. Nonetheless,
these models often struggle to efficiently solve both small- and large-scale
TSPs using the same set of pre-trained model parameters, limiting their
practical utility. To address this issue, we introduce a novel neural TSP
solver named GELD, built upon our proposed broad global assessment and refined
local selection framework. Specifically, GELD integrates a lightweight
Global-view Encoder (GE) with a heavyweight Local-view Decoder (LD) to enrich
embedding representation while accelerating the decision-making process.
Moreover, GE incorporates a novel low-complexity attention mechanism, allowing
GELD to achieve low inference latency and scalability to larger-scale TSPs.
Additionally, we propose a two-stage training strategy that utilizes training
instances of different sizes to bolster GELD's generalization ability.
Extensive experiments conducted on both synthetic and real-world datasets
demonstrate that GELD outperforms seven state-of-the-art models considering
both solution quality and inference speed. Furthermore, GELD can be employed as
a post-processing method to significantly elevate the quality of the solutions
derived by existing neural TSP solvers via spending affordable additional
computing time. Notably, GELD is shown as capable of solving TSPs with up to
744,710 nodes, first-of-its-kind to solve this large size TSP without relying
on divide-and-conquer strategies to the best of our knowledge.

</details>


### [16] [Contextual Experience Replay for Self-Improvement of Language Agents](https://arxiv.org/abs/2506.06698)
*Yitao Liu,Chenglei Si,Karthik Narasimhan,Shunyu Yao*

Main category: cs.AI

TL;DR: 论文提出了一种名为Contextual Experience Replay (CER)的无训练框架，帮助语言模型代理在复杂任务中通过动态记忆缓冲区积累和利用过往经验，提升其适应能力。


<details>
  <summary>Details</summary>
Motivation: 当前的语言模型代理在复杂任务（如网页导航）中因缺乏环境特定经验而表现不佳，且无法在推理时持续学习过往经验。

Method: CER通过动态记忆缓冲区积累和综合过往经验（包括环境动态和决策模式），使代理能在新任务中检索相关知识并自我增强。

Result: 在VisualWebArena和WebArena基准测试中，CER分别取得31.9%和36.7%的成功率，相对GPT-4o基线提升了51.0%。

Conclusion: CER通过动态记忆机制显著提升了语言模型代理的适应性和性能，证明了其高效性和有效性。

Abstract: Large language model (LLM) agents have been applied to sequential
decision-making tasks such as web navigation, but without any
environment-specific experiences, they often fail in these complex tasks.
Moreover, current LLM agents are not designed to continually learn from past
experiences during inference time, which could be crucial for them to gain
these environment-specific experiences. To address this, we propose Contextual
Experience Replay (CER), a training-free framework to enable efficient
self-improvement for language agents in their context window. Specifically, CER
accumulates and synthesizes past experiences into a dynamic memory buffer.
These experiences encompass environment dynamics and common decision-making
patterns, allowing the agents to retrieve and augment themselves with relevant
knowledge in new tasks, enhancing their adaptability in complex environments.
We evaluate CER on the challenging WebArena and VisualWebArena benchmarks. On
VisualWebArena, CER achieves a competitive performance of 31.9%. On WebArena,
CER also gets a competitive average success rate of 36.7%, relatively improving
the success rate of the GPT-4o agent baseline by 51.0%. We also conduct a
comprehensive analysis on it to prove its efficiency, validity and understand
it better.

</details>


### [17] [Integrating AI Planning Semantics into SysML System Models for Automated PDDL File Generation](https://arxiv.org/abs/2506.06714)
*Hamied Nabizada,Tom Jeleniewski,Lasse Beers,Maximilian Weigand,Felix Gehlhoff,Alexander Fay*

Main category: cs.AI

TL;DR: 本文提出了一种SysML配置文件，将PDDL规划语义直接集成到系统模型中，支持自动化生成规划描述。


<details>
  <summary>Details</summary>
Motivation: 旨在通过SysML模型直接集成PDDL规划语义，为工程设计中系统建模与AI规划提供桥梁。

Method: 定义可重用的PDDL概念刻板印象，并通过OCL约束确保语法一致性；基于PDDL 3.1的BNF定义推导配置文件。

Result: 通过飞机制造案例验证，成功生成PDDL格式的领域和问题描述，并用于优化执行计划。

Conclusion: 该方法支持模型驱动的规划描述生成，为系统建模与AI规划提供了可重用的桥梁。

Abstract: This paper presents a SysML profile that enables the direct integration of
planning semantics based on the Planning Domain Definition Language (PDDL) into
system models. Reusable stereotypes are defined for key PDDL concepts such as
types, predicates, functions and actions, while formal OCL constraints ensure
syntactic consistency. The profile was derived from the Backus-Naur Form (BNF)
definition of PDDL 3.1 to align with SysML modeling practices. A case study
from aircraft manufacturing demonstrates the application of the profile: a
robotic system with interchangeable end effectors is modeled and enriched to
generate both domain and problem descriptions in PDDL format. These are used as
input to a PDDL solver to derive optimized execution plans. The approach
supports automated and model-based generation of planning descriptions and
provides a reusable bridge between system modeling and AI planning in
engineering design.

</details>


### [18] [WorldLLM: Improving LLMs' world modeling using curiosity-driven theory-making](https://arxiv.org/abs/2506.06725)
*Guillaume Levy,Cedric Colas,Pierre-Yves Oudeyer,Thomas Carta,Clement Romac*

Main category: cs.AI

TL;DR: WorldLLM框架通过结合贝叶斯推断和强化学习，提升LLM在结构化领域（如模拟环境）中的预测能力。


<details>
  <summary>Details</summary>
Motivation: LLMs在结构化领域（如模拟环境）中难以生成精确预测，因其无法将广泛的无结构知识融入具体环境。

Method: WorldLLM利用LLM的上下文学习能力，通过自然语言假设指导预测，并通过贝叶斯推断和强化学习迭代优化假设和收集证据。

Result: 实验表明，WorldLLM在文本游戏环境中显著提升预测准确性，并生成可解释的环境动态理论。

Conclusion: WorldLLM通过自主探索和假设优化，实现了LLM在结构化环境中的持续改进和可解释预测。

Abstract: Large Language Models (LLMs) possess general world knowledge but often
struggle to generate precise predictions in structured, domain-specific
contexts such as simulations. These limitations arise from their inability to
ground their broad, unstructured understanding in specific environments. To
address this, we present WorldLLM, a framework that enhances LLM-based world
modeling by combining Bayesian inference and autonomous active exploration with
reinforcement learning. WorldLLM leverages the in-context learning abilities of
LLMs to guide an LLM-based world model's predictions using natural language
hypotheses given in its prompt. These hypotheses are iteratively refined
through a Bayesian inference framework that leverages a second LLM as the
proposal distribution given collected evidence. This evidence is collected
using a curiosity-driven reinforcement learning policy that explores the
environment to find transitions with a low log-likelihood under our LLM-based
predictive model using the current hypotheses. By alternating between refining
hypotheses and collecting new evidence, our framework autonomously drives
continual improvement of the predictions. Our experiments demonstrate the
effectiveness of WorldLLM in a textual game environment that requires agents to
manipulate and combine objects. The framework not only enhances predictive
accuracy, but also generates human-interpretable theories of environment
dynamics.

</details>


### [19] [Agent Semantics, Semantic Spacetime, and Graphical Reasoning](https://arxiv.org/abs/2506.07756)
*Mark Burgess*

Main category: cs.AI

TL;DR: 论文介绍了语义时空图模型的形式化方面，重点讨论了其在定向知识表示和过程建模中的应用。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于通过有限表示形成封闭操作集，以应对语义复杂性问题，并探索图中路径的可预测性。

Method: 定义了有限γ(3,4)表示，形成可扩展的封闭操作集，并结合语义时空假设和Promise理论分析吸收状态。

Result: 研究发现吸收状态普遍存在于部分图中，导致信息泄漏，这与除零问题相关，需要手动注入补救信息。

Conclusion: 语义时空模型及其Promise理论帮助明确了吸收状态与边界信息的关联，为意图性引入提供了理论基础。

Abstract: Some formal aspects of the Semantic Spacetime graph model are presented, with
reference to its use for directed knowledge representations and process
modelling. A finite $\gamma(3,4)$ representation is defined to form a closed
set of operations that can scale to any degree of semantic complexity. The
Semantic Spacetime postulates bring predictability with minimal constraints to
pathways in graphs. The ubiquitous appearance of absorbing states in any
partial graph means that a graph process leaks information. The issue is
closely associated with the issue of division by zero, which signals a loss of
closure and the need for manual injection of remedial information. The Semantic
Spacetime model (and its Promise Theory) origins help to clarify how such
absorbing states are associated with boundary information where intentionality
can enter.

</details>


### [20] [VisioMath: Benchmarking Figure-based Mathematical Reasoning in LMMs](https://arxiv.org/abs/2506.06727)
*Can Li,Ting Zhang,Mei Wang,Hua Huang*

Main category: cs.AI

TL;DR: VisioMath是一个评估多模态数学推理能力的基准数据集，专注于图像选项的数学问题，现有大型多模态模型（LMMs）在此任务上表现不佳。


<details>
  <summary>Details</summary>
Motivation: 现有大型多模态模型在多图像理解中的数学推理能力尚未充分探索，尤其是答案选项为图像时。

Method: 引入VisioMath数据集，包含8,070张图像和1,800道多选题，每题的答案选项均为图像。

Result: 现有最先进的LMMs（如GPT-4o）在VisioMath上表现不佳，准确率仅为45.9%。

Conclusion: VisioMath填补了现有基准的空白，为未来多模态推理研究提供了严格测试平台。

Abstract: Large Multimodal Models (LMMs) have demonstrated remarkable problem-solving
capabilities across various domains. However, their ability to perform
mathematical reasoning when answer options are represented as images--an
essential aspect of multi-image comprehension--remains underexplored. To bridge
this gap, we introduce VisioMath, a benchmark designed to evaluate mathematical
reasoning in multimodal contexts involving image-based answer choices.
VisioMath comprises 8,070 images and 1,800 multiple-choice questions, where
each answer option is an image, presenting unique challenges to existing LMMs.
To the best of our knowledge, VisioMath is the first dataset specifically
tailored for mathematical reasoning in image-based-option scenarios, where
fine-grained distinctions between answer choices are critical for accurate
problem-solving. We systematically evaluate state-of-the-art LMMs on VisioMath
and find that even the most advanced models struggle with this task. Notably,
GPT-4o achieves only 45.9% accuracy, underscoring the limitations of current
models in reasoning over visually similar answer choices. By addressing a
crucial gap in existing benchmarks, VisioMath establishes a rigorous testbed
for future research, driving advancements in multimodal reasoning.

</details>


### [21] [Honey, I shrunk the hypothesis space (through logical preprocessing)](https://arxiv.org/abs/2506.06739)
*Andrew Cropper,Filipe Gouveia,David M. Cerna*

Main category: cs.AI

TL;DR: 提出了一种通过缩小假设空间来优化归纳逻辑编程（ILP）的方法，显著减少学习时间。


<details>
  <summary>Details</summary>
Motivation: 在ILP中，假设空间的规模直接影响学习效率，因此需要一种方法提前排除无效假设。

Method: 利用背景知识识别并移除不可能出现在最优假设中的规则，通过答案集编程实现。

Result: 实验表明，该方法在多个领域（如视觉推理和游戏）中显著减少学习时间（从10小时降至2秒），同时保持预测准确性。

Conclusion: 该方法通过预处理缩小假设空间，有效提升了ILP系统的效率。

Abstract: Inductive logic programming (ILP) is a form of logical machine learning. The
goal is to search a hypothesis space for a hypothesis that generalises training
examples and background knowledge. We introduce an approach that 'shrinks' the
hypothesis space before an ILP system searches it. Our approach uses background
knowledge to find rules that cannot be in an optimal hypothesis regardless of
the training examples. For instance, our approach discovers relationships such
as "even numbers cannot be odd" and "prime numbers greater than 2 are odd". It
then removes violating rules from the hypothesis space. We implement our
approach using answer set programming and use it to shrink the hypothesis space
of a constraint-based ILP system. Our experiments on multiple domains,
including visual reasoning and game playing, show that our approach can
substantially reduce learning times whilst maintaining predictive accuracies.
For instance, given just 10 seconds of preprocessing time, our approach can
reduce learning times from over 10 hours to only 2 seconds.

</details>


### [22] [AI PsyRoom: Artificial Intelligence Platform for Segmented Yearning and Reactive Outcome Optimization Method](https://arxiv.org/abs/2506.06740)
*Yigui Feng,Qinglin Wang,Ke Liu,Xinhai Chen,Bo Yang,Jie Liu*

Main category: cs.AI

TL;DR: AI PsyRoom是一个多智能体模拟框架，旨在通过生成富有同理心和情感细微差别的对话来增强心理咨询。它通过细粒度情感分类和多智能体框架，显著提升了心理咨询的效果。


<details>
  <summary>Details</summary>
Motivation: 心理咨询需求增长与专业人员短缺的矛盾促使研究利用大型语言模型（LLMs）辅助心理咨询，但现有模型缺乏对情感的深入理解和个性化治疗能力。

Method: 提出AI PsyRoom框架，包括多智能体PsyRoom A用于对话重建生成高质量数据集EmoPsy，以及PsyRoom B用于生成个性化治疗计划。

Result: AI PsyRoom在问题导向、表达、同理心和交互沟通质量上分别提升了18%、23%、24%和16%。

Conclusion: AI PsyRoom为AI辅助心理咨询研究提供了基础，其数据集和模型已公开。

Abstract: Psychological counseling faces huge challenges due to the growing demand for
mental health services and the shortage of trained professionals. Large
language models (LLMs) have shown potential to assist psychological counseling,
especially in empathy and emotional support. However, existing models lack a
deep understanding of emotions and are unable to generate personalized
treatment plans based on fine-grained emotions. To address these shortcomings,
we present AI PsyRoom, a multi-agent simulation framework designed to enhance
psychological counseling by generating empathetic and emotionally nuanced
conversations. By leveraging fine-grained emotion classification and a
multi-agent framework, we construct a multi-agent PsyRoom A for dialogue
reconstruction, generating a high-quality dialogue dataset EmoPsy, which
contains 35 sub-emotions, 423 specific emotion scenarios, and 12,350 dialogues.
We also propose PsyRoom B for generating personalized treatment plans.
Quantitative evaluations demonstrate that AI PsyRoom significantly outperforms
state-of-the-art methods, achieving 18% improvement in problem orientation, 23%
in expression, 24% in Empathy, and 16% in interactive communication quality.
The datasets and models are publicly available, providing a foundation for
advancing AI-assisted psychological counseling research.

</details>


### [23] [Bio-Inspired Classification: Combining Information Theory and Spiking Neural Networks -- Influence of the Learning Rules](https://arxiv.org/abs/2506.06750)
*Zofia Rudnicka,Janusz Szczepanski,Agnieszka Pregowska*

Main category: cs.AI

TL;DR: 论文探讨了脉冲神经网络（SNN）训练中的挑战，比较了不同学习算法（包括生物启发规则）对分类准确性的影响，并提出了一种结合SNN和Lempel-Ziv复杂度（LZC）的生物启发分类器。


<details>
  <summary>Details</summary>
Motivation: 解决SNN训练中的挑战，如时间动态性、脉冲事件的不可微性和稀疏事件驱动激活，同时探索不同学习算法对分类性能的影响。

Method: 提出了一种结合SNN和LZC的生物启发分类器，比较了经典反向传播算法与生物启发算法（如tempotron和Spikprop）的性能。

Result: 反向传播算法分类准确率高但计算成本高，生物启发算法在保持竞争力的同时提高了计算效率。

Conclusion: 选择学习算法需权衡分类准确性和计算成本，生物启发算法适合时间敏感任务。

Abstract: Training of Spiking Neural Networks (SNN) is challenging due to their unique
properties, including temporal dynamics, non-differentiability of spike events,
and sparse event-driven activations. In this paper, we widely consider the
influence of the type of chosen learning algorithm, including bioinspired
learning rules on the accuracy of classification. We proposed a bioinspired
classifier based on the combination of SNN and Lempel-Ziv complexity (LZC).
This approach synergizes the strengths of SNNs in temporal precision and
biological realism with LZC's structural complexity analysis, facilitating
efficient and interpretable classification of spatiotemporal neural data. It
turned out that the classic backpropagation algorithm achieves excellent
classification accuracy, but at extremely high computational cost, which makes
it impractical for real-time applications. Biologically inspired learning
algorithms such as tempotron and Spikprop provide increased computational
efficiency while maintaining competitive classification performance, making
them suitable for time-sensitive tasks. The results obtained indicate that the
selection of the most appropriate learning algorithm depends on the trade-off
between classification accuracy and computational cost as well as application
constraints.

</details>


### [24] [Learning What Matters Now: A Dual-Critic Context-Aware RL Framework for Priority-Driven Information Gain](https://arxiv.org/abs/2506.06786)
*Dimitris Panagopoulos,Adolfo Perrusquia,Weisi Guo*

Main category: cs.AI

TL;DR: CA-MIQ是一种轻量级双批评家强化学习框架，用于动态调整探索策略以适应任务优先级变化，在SAR任务中表现优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 高风险的搜救任务需要系统在动态调整优先级的同时持续收集关键信息。

Method: CA-MIQ结合了外在和内在批评家，前者关注任务奖励，后者融合状态新颖性、信息位置感知和实时优先级对齐，并通过内置的优先级变化检测器触发探索增强。

Result: 在模拟SAR任务中，CA-MIQ在单次和多优先级变化场景下的任务成功率分别比基线高4倍和3倍，且能100%恢复。

Conclusion: CA-MIQ适用于具有分段平稳信息价值分布的离散环境，能有效适应优先级变化。

Abstract: Autonomous systems operating in high-stakes search-and-rescue (SAR) missions
must continuously gather mission-critical information while flexibly adapting
to shifting operational priorities. We propose CA-MIQ (Context-Aware
Max-Information Q-learning), a lightweight dual-critic reinforcement learning
(RL) framework that dynamically adjusts its exploration strategy whenever
mission priorities change. CA-MIQ pairs a standard extrinsic critic for task
reward with an intrinsic critic that fuses state-novelty, information-location
awareness, and real-time priority alignment. A built-in shift detector triggers
transient exploration boosts and selective critic resets, allowing the agent to
re-focus after a priority revision. In a simulated SAR grid-world, where
experiments specifically test adaptation to changes in the priority order of
information types the agent is expected to focus on, CA-MIQ achieves nearly
four times higher mission-success rates than baselines after a single priority
shift and more than three times better performance in multiple-shift scenarios,
achieving 100% recovery while baseline methods fail to adapt. These results
highlight CA-MIQ's effectiveness in any discrete environment with
piecewise-stationary information-value distributions.

</details>


### [25] [Cross-Entropy Games for Language Models: From Implicit Knowledge to General Capability Measures](https://arxiv.org/abs/2506.06832)
*Clément Hongler,Andrew Emil*

Main category: cs.AI

TL;DR: 论文探讨了如何通过基于大语言模型（LLM）的概率度量设计任务（称为Xent Games），以评估LLM的能力。


<details>
  <summary>Details</summary>
Motivation: 研究LLM如何理解其生成文本的概率度量，并探索超越生成采样的任务形式。

Method: 提出Xent Games框架，包括单人和多人任务，基于交叉熵评分和约束，并通过计算图和程序实现。

Result: Xent Games空间足够丰富，可作为LLM能力的基准测试工具。

Conclusion: 通过Xent Games的进化动态探索，可以系统性评估LLM的通用能力。

Abstract: Large Language Models (LLMs) define probability measures on text. By
considering the implicit knowledge question of what it means for an LLM to know
such a measure and what it entails algorithmically, we are naturally led to
formulate a series of tasks that go beyond generative sampling, involving forms
of summarization, counterfactual thinking, anomaly detection, originality
search, reverse prompting, debating, creative solving, etc. These tasks can be
formulated as games based on LLM measures, which we call Cross-Entropy (Xent)
Games. Xent Games can be single-player or multi-player. They involve
cross-entropy scores and cross-entropy constraints, and can be expressed as
simple computational graphs and programs. We show the Xent Game space is large
enough to contain a wealth of interesting examples, while being constructible
from basic game-theoretic consistency axioms. We then discuss how the Xent Game
space can be used to measure the abilities of LLMs. This leads to the
construction of Xent Game measures: finite families of Xent Games that can be
used as capability benchmarks, built from a given scope, by extracting a
covering measure. To address the unbounded scope problem associated with the
challenge of measuring general abilities, we propose to explore the space of
Xent Games in a coherent fashion, using ideas inspired by evolutionary
dynamics.

</details>


### [26] [United Minds or Isolated Agents? Exploring Coordination of LLMs under Cognitive Load Theory](https://arxiv.org/abs/2506.06843)
*HaoYang Shang,Xuan Liu,Zi Liang,Jie Zhang,Haibo Hu,Song Guo*

Main category: cs.AI

TL;DR: 论文提出CoThinker框架，通过多代理协作减轻LLM的认知负载，提升复杂任务表现。


<details>
  <summary>Details</summary>
Motivation: LLM在复杂任务中因认知负载限制表现不佳，类比认知科学中的认知负载理论（CLT），提出需解决此问题。

Method: 引入CoThinker框架，通过代理分工和结构化通信分配认知负载，并建立集体工作记忆。

Result: 实验验证CoThinker在复杂任务中优于现有多代理基线，提升解决方案质量和效率。

Conclusion: CoThinker为克服LLM性能瓶颈提供了原则性方法，揭示了集体认知的交互模式。

Abstract: Large Language Models (LLMs) exhibit a notable performance ceiling on
complex, multi-faceted tasks, as they often fail to integrate diverse
information or adhere to multiple constraints. We posit that such limitation
arises when the demands of a task exceed the LLM's effective cognitive load
capacity. This interpretation draws a strong analogy to Cognitive Load Theory
(CLT) in cognitive science, which explains similar performance boundaries in
the human mind, and is further supported by emerging evidence that reveals LLMs
have bounded working memory characteristics. Building upon this CLT-grounded
understanding, we introduce CoThinker, a novel LLM-based multi-agent framework
designed to mitigate cognitive overload and enhance collaborative
problem-solving abilities. CoThinker operationalizes CLT principles by
distributing intrinsic cognitive load through agent specialization and managing
transactional load via structured communication and a collective working
memory. We empirically validate CoThinker on complex problem-solving tasks and
fabricated high cognitive load scenarios, demonstrating improvements over
existing multi-agent baselines in solution quality and efficiency. Our analysis
reveals characteristic interaction patterns, providing insights into the
emergence of collective cognition and effective load management, thus offering
a principled approach to overcoming LLM performance ceilings.

</details>


### [27] [Incorporating Failure of Machine Learning in Dynamic Probabilistic Safety Assurance](https://arxiv.org/abs/2506.06868)
*Razieh Arshadizadeh,Mahmoud Asgari,Zeinab Khosravi,Yiannis Papadopoulos,Koorosh Aslansefat*

Main category: cs.AI

TL;DR: 论文提出了一种结合SafeML和贝叶斯网络的概率安全保证框架，用于动态评估和适应ML在安全关键系统中的不确定性。


<details>
  <summary>Details</summary>
Motivation: ML模型在安全关键系统中的不完美性（如分布偏移导致的推理失败）需要新的安全评估方法。

Method: 提出了一种集成SafeML和贝叶斯网络的框架，动态建模ML失败并进行因果安全分析。

Result: 在模拟的汽车队列系统中验证了方法的有效性，展示了显式建模ML失败的优势。

Conclusion: 该框架为ML在安全关键系统中的动态安全评估提供了新思路。

Abstract: Machine Learning (ML) models are increasingly integrated into safety-critical
systems, such as autonomous vehicle platooning, to enable real-time
decision-making. However, their inherent imperfection introduces a new class of
failure: reasoning failures often triggered by distributional shifts between
operational and training data. Traditional safety assessment methods, which
rely on design artefacts or code, are ill-suited for ML components that learn
behaviour from data. SafeML was recently proposed to dynamically detect such
shifts and assign confidence levels to the reasoning of ML-based components.
Building on this, we introduce a probabilistic safety assurance framework that
integrates SafeML with Bayesian Networks (BNs) to model ML failures as part of
a broader causal safety analysis. This allows for dynamic safety evaluation and
system adaptation under uncertainty. We demonstrate the approach on an
simulated automotive platooning system with traffic sign recognition. The
findings highlight the potential broader benefits of explicitly modelling ML
failures in safety assessment.

</details>


### [28] [KnowCoder-V2: Deep Knowledge Analysis](https://arxiv.org/abs/2506.06881)
*Zixuan Li,Wenxuan Liu,Long Bai,Chunmao Zhang,Wei Li,Fenghui Zhang,Quanxin Jin,Ruoyun He,Zhuo Chen,Zhilei Hu,Fei Wang,Bingbing Xu,Xuhui Jiang,Xiaolong Jin,Jiafeng Guo,Xueqi Cheng*

Main category: cs.AI

TL;DR: 论文提出了一种名为KDR的框架，结合了知识组织和深度推理，并通过LLM（KCII）统一代码生成来增强知识分析能力。


<details>
  <summary>Details</summary>
Motivation: 现有深度研究框架在知识组织、在线操作和复杂知识计算方面存在不足，无法高效处理大规模知识任务。

Method: KDR框架引入离线知识组织阶段，并通过KCII生成代码实现知识对象化和复杂计算。

Result: 在六个知识分析任务的三十多个数据集上验证了KCII的有效性，KDR框架能生成高质量分析报告。

Conclusion: KDR框架和KCII显著提升了知识分析任务的深度和效率。

Abstract: Deep knowledge analysis tasks always involve the systematic extraction and
association of knowledge from large volumes of data, followed by logical
reasoning to discover insights. However, to solve such complex tasks, existing
deep research frameworks face three major challenges: 1) They lack systematic
organization and management of knowledge; 2) They operate purely online, making
it inefficient for tasks that rely on shared and large-scale knowledge; 3) They
cannot perform complex knowledge computation, limiting their abilities to
produce insightful analytical results. Motivated by these, in this paper, we
propose a \textbf{K}nowledgeable \textbf{D}eep \textbf{R}esearch (\textbf{KDR})
framework that empowers deep research with deep knowledge analysis capability.
Specifically, it introduces an independent knowledge organization phase to
preprocess large-scale, domain-relevant data into systematic knowledge offline.
Based on this knowledge, it extends deep research with an additional kind of
reasoning steps that perform complex knowledge computation in an online manner.
To enhance the abilities of LLMs to solve knowledge analysis tasks in the above
framework, we further introduce \textbf{\KCII}, an LLM that bridges knowledge
organization and reasoning via unified code generation. For knowledge
organization, it generates instantiation code for predefined classes,
transforming data into knowledge objects. For knowledge computation, it
generates analysis code and executes on the above knowledge objects to obtain
deep analysis results. Experimental results on more than thirty datasets across
six knowledge analysis tasks demonstrate the effectiveness of \KCII. Moreover,
when integrated into the KDR framework, \KCII can generate high-quality reports
with insightful analytical results compared to the mainstream deep research
framework.

</details>


### [29] [Evaluating LLM-corrupted Crowdsourcing Data Without Ground Truth](https://arxiv.org/abs/2506.06991)
*Yichi Zhang,Jinlong Pang,Zhaowei Zhu,Yang Liu*

Main category: cs.AI

TL;DR: 本文探讨了如何利用同行预测机制检测众包任务中LLM辅助作弊，特别是在标注任务中。


<details>
  <summary>Details</summary>
Motivation: 高质量人类反馈对构建可信AI至关重要，但众包工作者使用LLM可能导致数据集被污染。现有检测方法不适用于标注任务。

Method: 提出一种无训练的评分机制，通过量化工人答案间的相关性（基于LLM生成标签的子集）来检测作弊。

Result: 理论证明该方法在LLM共谋情况下有效，并在真实数据集上验证了其鲁棒性。

Conclusion: 同行预测机制能有效检测众包任务中的LLM辅助作弊，尤其在标注任务中表现优异。

Abstract: The recent success of generative AI highlights the crucial role of
high-quality human feedback in building trustworthy AI systems. However, the
increasing use of large language models (LLMs) by crowdsourcing workers poses a
significant challenge: datasets intended to reflect human input may be
compromised by LLM-generated responses. Existing LLM detection approaches often
rely on high-dimension training data such as text, making them unsuitable for
annotation tasks like multiple-choice labeling. In this work, we investigate
the potential of peer prediction -- a mechanism that evaluates the information
within workers' responses without using ground truth -- to mitigate
LLM-assisted cheating in crowdsourcing with a focus on annotation tasks. Our
approach quantifies the correlations between worker answers while conditioning
on (a subset of) LLM-generated labels available to the requester. Building on
prior research, we propose a training-free scoring mechanism with theoretical
guarantees under a crowdsourcing model that accounts for LLM collusion. We
establish conditions under which our method is effective and empirically
demonstrate its robustness in detecting low-effort cheating on real-world
crowdsourcing datasets.

</details>


### [30] [Meta-Adaptive Prompt Distillation for Few-Shot Visual Question Answering](https://arxiv.org/abs/2506.06905)
*Akash Gupta,Amos Storkey,Mirella Lapata*

Main category: cs.AI

TL;DR: 论文提出了一种元学习方法，通过蒸馏任务相关图像特征生成固定软提示，以提升小规模多模态模型在少样本任务中的表现。


<details>
  <summary>Details</summary>
Motivation: 现有的大规模多模态模型（LMMs）在上下文学习（ICL）中表现不稳定，尤其是小模型，增加示例未必能提升性能，推测原因是图像嵌入中的冗余信息干扰了任务表现。

Method: 提出了一种元学习方法，通过注意力映射模块蒸馏任务相关图像特征生成软提示，并与LLaVA v1.5架构结合，实现少样本任务适应。

Result: 在VL-ICL Bench上的评估表明，该方法在视觉问答任务中优于ICL和其他提示调优方法，且对图像扰动具有鲁棒性。

Conclusion: 该方法通过任务相关特征蒸馏和软提示适应，显著提升了LMMs在少样本任务中的表现和推理能力。

Abstract: Large Multimodal Models (LMMs) often rely on in-context learning (ICL) to
perform new tasks with minimal supervision. However, ICL performance,
especially in smaller LMMs, is inconsistent and does not always improve
monotonically with increasing examples. We hypothesize that this occurs due to
the LMM being overwhelmed by additional information present in the image
embeddings, which is not required for the downstream task. To address this, we
propose a meta-learning approach that provides an alternative for inducing
few-shot capabilities in LMMs, using a fixed set of soft prompts that are
distilled from task-relevant image features and can be adapted at test time
using a few examples. To facilitate this distillation, we introduce an
attention-mapper module that can be easily integrated with the popular LLaVA
v1.5 architecture and is jointly learned with soft prompts, enabling task
adaptation in LMMs under low-data regimes with just a few gradient steps.
Evaluation on the VL-ICL Bench shows that our method consistently outperforms
ICL and related prompt-tuning approaches, even under image perturbations,
improving task induction and reasoning across visual question answering tasks.

</details>


### [31] [Causal Graph based Event Reasoning using Semantic Relation Experts](https://arxiv.org/abs/2506.06910)
*Mahnaz Koupaee,Xueying Bai,Mudan Chen,Greg Durrett,Nathanael Chambers,Niranjan Balasubramanian*

Main category: cs.AI

TL;DR: 论文提出了一种通过生成因果事件图来帮助大语言模型（LLMs）显式表示因果关系的方法，并验证了其在推理任务中的有效性。


<details>
  <summary>Details</summary>
Motivation: 事件推理是一个重要但具有挑战性的任务，LLMs在识别事件间因果关系时表现不佳，影响了更深层次推理任务的效果。

Method: 采用协作式因果图生成方法，利用LLMs模拟专注于特定语义关系的专家，通过多轮讨论生成因果图。

Result: 提出的方法在未针对下游任务微调的情况下，在事件预测和时间线理解任务中达到了与最先进模型竞争的结果。

Conclusion: 因果事件图能够有效辅助LLMs进行推理，提升任务表现，并生成更具信息性和连贯性的解释。

Abstract: Understanding how events in a scenario causally connect with each other is
important for effectively modeling and reasoning about events. But event
reasoning remains a difficult challenge, and despite recent advances, Large
Language Models (LLMs) still struggle to accurately identify causal connections
between events. This struggle leads to poor performance on deeper reasoning
tasks like event forecasting and timeline understanding. To address this
challenge, we investigate the generation of causal event graphs (e.g., A
enables B) as a parallel mechanism to help LLMs explicitly represent causality
during inference. This paper evaluates both how to generate correct graphs as
well as how graphs can assist reasoning. We propose a collaborative approach to
causal graph generation where we use LLMs to simulate experts that focus on
specific semantic relations. The experts engage in multiple rounds of
discussions which are then consolidated by a final expert. Then, to demonstrate
the utility of causal graphs, we use them on multiple downstream applications,
and also introduce a new explainable event prediction task that requires a
causal chain of events in the explanation. These explanations are more
informative and coherent than baseline generations. Finally, our overall
approach not finetuned on any downstream task, achieves competitive results
with state-of-the-art models on both forecasting and next event prediction
tasks.

</details>


### [32] [Boosting LLM Reasoning via Spontaneous Self-Correction](https://arxiv.org/abs/2506.06923)
*Xutong Zhao,Tengyu Xu,Xuewei Wang,Zhengxing Chen,Di Jin,Liang Tan,Yen-Ting,Zishun Yu,Zhuokai Zhao,Yun He,Sinong Wang,Han Fang,Sarath Chandar,Chen Zhu*

Main category: cs.AI

TL;DR: SPOC是一种实时自校正方法，通过单次推理生成解决方案和验证，显著提升LLM在数学推理任务中的表现。


<details>
  <summary>Details</summary>
Motivation: 现有自校正方法依赖额外提示和系统设计，无法实现实时自校正，限制了数学推理任务的性能提升。

Method: SPOC通过多智能体视角，将模型分为解决方案提出者和验证者，利用合成数据微调和在线强化学习提升能力。

Result: 实验表明，SPOC显著提升了Llama-3.1-8B和70B Instruct模型在多个数学推理基准上的准确率。

Conclusion: SPOC通过实时自校正和多智能体协作，有效提升了LLM在数学推理任务中的性能。

Abstract: While large language models (LLMs) have demonstrated remarkable success on a
broad range of tasks, math reasoning remains a challenging one. One of the
approaches for improving math reasoning is self-correction, which designs
self-improving loops to let the model correct its own mistakes. However,
existing self-correction approaches treat corrections as standalone
post-generation refinements, relying on extra prompt and system designs to
elicit self-corrections, instead of performing real-time, spontaneous
self-corrections in a single pass. To address this, we propose SPOC, a
spontaneous self-correction approach that enables LLMs to generate interleaved
solutions and verifications in a single inference pass, with generation
dynamically terminated based on verification outcomes, thereby effectively
scaling inference time compute. SPOC considers a multi-agent perspective by
assigning dual roles -- solution proposer and verifier -- to the same model. We
adopt a simple yet effective approach to generate synthetic data for
fine-tuning, enabling the model to develop capabilities for self-verification
and multi-agent collaboration. We further improve its solution proposal and
verification accuracy through online reinforcement learning. Experiments on
mathematical reasoning benchmarks show that SPOC significantly improves
performance. Notably, SPOC boosts the accuracy of Llama-3.1-8B and 70B Instruct
models, achieving gains of 8.8% and 11.6% on MATH500, 10.0% and 20.0% on AMC23,
and 3.3% and 6.7% on AIME24, respectively.

</details>


### [33] [An Agentic Framework for Autonomous Metamaterial Modeling and Inverse Design](https://arxiv.org/abs/2506.06935)
*Darui Lu,Jordan M. Malof,Willie J. Padilla*

Main category: cs.AI

TL;DR: 论文提出了一种基于多LLM系统的Agentic框架，用于光子超材料的逆向设计，能够自主完成复杂任务。


<details>
  <summary>Details</summary>
Motivation: 通过整合多LLM系统，实现自主完成复杂任务（如科学研究的逆向设计）的Agentic框架。

Method: 框架通过查询目标光学谱，自主开发前向深度学习模型，调用API进行模拟和优化，利用记忆，并通过深度逆向方法生成最终设计。

Result: 框架展示了自动化、推理、规划和适应能力，具有内部反思和决策灵活性，能产生多样且可能新颖的输出。

Conclusion: 该Agentic框架在光子超材料逆向设计中表现出高效性和创新性，为复杂任务自动化提供了新思路。

Abstract: Recent significant advances in integrating multiple Large Language Model
(LLM) systems have enabled Agentic Frameworks capable of performing complex
tasks autonomously, including novel scientific research. We develop and
demonstrate such a framework specifically for the inverse design of photonic
metamaterials. When queried with a desired optical spectrum, the Agent
autonomously proposes and develops a forward deep learning model, accesses
external tools via APIs for tasks like simulation and optimization, utilizes
memory, and generates a final design via a deep inverse method. The framework's
effectiveness is demonstrated in its ability to automate, reason, plan, and
adapt. Notably, the Agentic Framework possesses internal reflection and
decision flexibility, permitting highly varied and potentially novel outputs.

</details>


### [34] [The Illusion of Thinking: Understanding the Strengths and Limitations of Reasoning Models via the Lens of Problem Complexity](https://arxiv.org/abs/2506.06941)
*Parshin Shojaee,Iman Mirzadeh,Keivan Alizadeh,Maxwell Horton,Samy Bengio,Mehrdad Farajtabar*

Main category: cs.AI

TL;DR: 论文研究了大型推理模型（LRMs）的能力、扩展性和局限性，通过可控的谜题环境分析其推理过程，发现LRMs在复杂度超过一定阈值时准确性崩溃，并存在反直觉的扩展限制。


<details>
  <summary>Details</summary>
Motivation: 当前对LRMs的评估主要集中于数学和编程基准测试，缺乏对其推理能力的深入理解。论文旨在填补这一空白，通过可控环境分析LRMs的推理过程。

Method: 使用可控的谜题环境，精确操纵复杂度并保持逻辑结构一致，分析LRMs的推理痕迹和最终答案。

Result: LRMs在复杂度超过一定阈值时准确性崩溃；推理努力随复杂度增加至某一点后下降；与标准LLM相比，LRMs在不同复杂度任务中表现不同。

Conclusion: LRMs在精确计算方面存在局限性，无法使用显式算法，且推理能力在不同规模下不一致。研究揭示了其优势和局限性，并对其推理能力提出质疑。

Abstract: Recent generations of language models have introduced Large Reasoning Models
(LRMs) that generate detailed thinking processes before providing answers.
While these models demonstrate improved performance on reasoning benchmarks,
their fundamental capabilities, scaling properties, and limitations remain
insufficiently understood. Current evaluations primarily focus on established
math and coding benchmarks, emphasizing final answer accuracy. However, this
evaluation paradigm often suffers from contamination and does not provide
insights into the reasoning traces. In this work, we systematically investigate
these gaps with the help of controllable puzzle environments that allow precise
manipulation of complexity while maintaining consistent logical structures.
This setup enables the analysis of not only final answers but also the internal
reasoning traces, offering insights into how LRMs think. Through extensive
experiments, we show that LRMs face a complete accuracy collapse beyond certain
complexities. Moreover, they exhibit a counterintuitive scaling limit: their
reasoning effort increases with problem complexity up to a point, then declines
despite having remaining token budget. By comparing LRMs with their standard
LLM counterparts under same inference compute, we identify three performance
regimes: (1) low-complexity tasks where standard models outperform LRMs, (2)
medium-complexity tasks where LRMs demonstrates advantage, and (3)
high-complexity tasks where both models face complete collapse. We found that
LRMs have limitations in exact computation: they fail to use explicit
algorithms and reason inconsistently across scales. We also investigate the
reasoning traces in more depth, studying the patterns of explored solutions and
analyzing the models' computational behavior, shedding light on their
strengths, limitations, and raising questions about their reasoning
capabilities.

</details>


### [35] [Deontically Constrained Policy Improvement in Reinforcement Learning Agents](https://arxiv.org/abs/2506.06959)
*Alena Makarova,Houssam Abbas*

Main category: cs.AI

TL;DR: 该论文研究了在马尔可夫决策过程（MDP）中学习满足义务逻辑约束的决策策略，以最大化效用函数。


<details>
  <summary>Details</summary>
Motivation: 在强化学习中，智能体通常以最大化效用为目标，但实际应用中可能需要满足伦理、社会或情境约束。本文旨在解决如何在满足这些约束的同时优化任务效用。

Method: 采用基于期望行为功利主义的逻辑，提出了一种改进的策略优化方法，确保在受控MDP中达到约束下的局部最优。

Result: 实验表明，该方法能够在满足义务逻辑约束的同时，实现任务效用的局部最大化。

Conclusion: 通过结合义务逻辑与效用最大化，该方法为智能体在复杂约束下的决策提供了可行方案。

Abstract: Markov Decision Processes (MDPs) are the most common model for decision
making under uncertainty in the Machine Learning community. An MDP captures
non-determinism, probabilistic uncertainty, and an explicit model of action. A
Reinforcement Learning (RL) agent learns to act in an MDP by maximizing a
utility function. This paper considers the problem of learning a decision
policy that maximizes utility subject to satisfying a constraint expressed in
deontic logic. In this setup, the utility captures the agent's mission - such
as going quickly from A to B. The deontic formula represents (ethical, social,
situational) constraints on how the agent might achieve its mission by
prohibiting classes of behaviors. We use the logic of Expected Act
Utilitarianism, a probabilistic stit logic that can be interpreted over
controlled MDPs. We develop a variation on policy improvement, and show that it
reaches a constrained local maximum of the mission utility. Given that in stit
logic, an agent's duty is derived from value maximization, this can be seen as
a way of acting to simultaneously maximize two value functions, one of which is
implicit, in a bi-level structure. We illustrate these results with experiments
on sample MDPs.

</details>


### [36] [Long-Tailed Learning for Generalized Category Discovery](https://arxiv.org/abs/2506.06965)
*Cuong Manh Hoang*

Main category: cs.AI

TL;DR: 提出了一种在长尾分布中实现广义类别发现的新框架，通过自引导标记和表示平衡技术提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法在平衡数据集上表现良好，但在现实世界的不平衡数据集中效果不佳，需解决长尾分布问题。

Method: 采用自引导标记技术生成伪标签以减少偏差，并通过表示平衡过程挖掘样本邻域以关注尾部类别。

Result: 在公开数据集上实验表明，模型性能优于现有最优方法。

Conclusion: 新框架有效解决了长尾分布下的广义类别发现问题，提升了模型性能。

Abstract: Generalized Category Discovery (GCD) utilizes labeled samples of known
classes to discover novel classes in unlabeled samples. Existing methods show
effective performance on artificial datasets with balanced distributions.
However, real-world datasets are always imbalanced, significantly affecting the
effectiveness of these methods. To solve this problem, we propose a novel
framework that performs generalized category discovery in long-tailed
distributions. We first present a self-guided labeling technique that uses a
learnable distribution to generate pseudo-labels, resulting in less biased
classifiers. We then introduce a representation balancing process to derive
discriminative representations. By mining sample neighborhoods, this process
encourages the model to focus more on tail classes. We conduct experiments on
public datasets to demonstrate the effectiveness of the proposed framework. The
results show that our model exceeds previous state-of-the-art methods.

</details>


### [37] [Deep RL Needs Deep Behavior Analysis: Exploring Implicit Planning by Model-Free Agents in Open-Ended Environments](https://arxiv.org/abs/2506.06981)
*Riley Simmons-Edler,Ryan P. Badman,Felix Baastad Berg,Raymond Chua,John J. Vastola,Joshua Lunger,William Qian,Kanaka Rajan*

Main category: cs.AI

TL;DR: 该论文提出了一种结合神经科学和动物行为学工具的方法，用于分析深度强化学习（DRL）智能体在复杂环境中的行为，揭示了其策略和记忆的丰富结构。


<details>
  <summary>Details</summary>
Motivation: 当前DRL智能体的行为分析方法不足，需要更深入的工具来理解其复杂行为，尤其是在任务和智能体复杂度增加时。

Method: 在ForageWorld环境中，应用神经科学和动物行为学工具，对DRL智能体进行行为和神经联合分析。

Result: 发现无模型的RNN-based DRL智能体可以通过涌现动态表现出结构化、类似规划的行为，无需显式记忆模块或世界模型。

Conclusion: 通过借鉴生物智能研究方法，可以揭示DRL智能体的隐藏学习动态，并为未来复杂智能体的安全对齐和行为优化提供框架。

Abstract: Understanding the behavior of deep reinforcement learning (DRL) agents --
particularly as task and agent sophistication increase -- requires more than
simple comparison of reward curves, yet standard methods for behavioral
analysis remain underdeveloped in DRL. We apply tools from neuroscience and
ethology to study DRL agents in a novel, complex, partially observable
environment, ForageWorld, designed to capture key aspects of real-world animal
foraging -- including sparse, depleting resource patches, predator threats, and
spatially extended arenas. We use this environment as a platform for applying
joint behavioral and neural analysis to agents, revealing detailed,
quantitatively grounded insights into agent strategies, memory, and planning.
Contrary to common assumptions, we find that model-free RNN-based DRL agents
can exhibit structured, planning-like behavior purely through emergent dynamics
-- without requiring explicit memory modules or world models. Our results show
that studying DRL agents like animals -- analyzing them with
neuroethology-inspired tools that reveal structure in both behavior and neural
dynamics -- uncovers rich structure in their learning dynamics that would
otherwise remain invisible. We distill these tools into a general analysis
framework linking core behavioral and representational features to diagnostic
methods, which can be reused for a wide range of tasks and agents. As agents
grow more complex and autonomous, bridging neuroscience, cognitive science, and
AI will be essential -- not just for understanding their behavior, but for
ensuring safe alignment and maximizing desirable behaviors that are hard to
measure via reward. We show how this can be done by drawing on lessons from how
biological intelligence is studied.

</details>


### [38] [Mathesis: Towards Formal Theorem Proving from Natural Languages](https://arxiv.org/abs/2506.07047)
*Yu Xuejun,Jianyuan Zhong,Zijin Feng,Pengyi Zhai,Roozbeh Yousefzadeh,Wei Chong Ng,Haoxiong Liu,Ziyi Shou,Jing Xiong,Yudong Zhou,Claudia Beth Ong,Austen Jeremy Sugiarto,Yaoxi Zhang,Wai Ming Tai,Huan Cao,Dongcai Lu,Jiacheng Sun,Qiang Xu,Shen Xin,Zhenguo Li*

Main category: cs.AI

TL;DR: Mathesis是一个端到端的定理证明系统，首次实现了从非正式问题陈述到形式化证明的全流程处理，包括基于强化学习的自动形式化工具和形式化证明生成器。


<details>
  <summary>Details</summary>
Motivation: 解决现有LLM定理证明器依赖专家编写形式化语句的限制，扩展其处理自然语言问题的能力。

Method: 提出Mathesis-Autoformalizer（基于强化学习的自动形式化工具）和Mathesis-Prover（形式化证明生成器），并引入LeanScorer框架评估形式化质量。

Result: 在Gaokao-Formal基准上，自动形式化工具比基线高22%通过率，全系统在MiniF2F和Gaokao-Formal上分别达到64%和18%的准确率。

Conclusion: Mathesis展示了端到端形式定理证明的实用性，为自然语言问题形式化提供了新方法。

Abstract: Recent advances in large language models show strong promise for formal
reasoning. However, most LLM-based theorem provers have long been constrained
by the need for expert-written formal statements as inputs, limiting their
applicability to real-world problems expressed in natural language. We tackle
this gap with Mathesis, the first end-to-end theorem proving pipeline
processing informal problem statements. It contributes Mathesis-Autoformalizer,
the first autoformalizer using reinforcement learning to enhance the
formalization ability of natural language problems, aided by our novel
LeanScorer framework for nuanced formalization quality assessment. It also
proposes a Mathesis-Prover, which generates formal proofs from the formalized
statements. To evaluate the real-world applicability of end-to-end formal
theorem proving, we introduce Gaokao-Formal, a benchmark of 488 complex
problems from China's national college entrance exam. Our approach is carefully
designed, with a thorough study of each component. Experiments demonstrate
Mathesis's effectiveness, with the autoformalizer outperforming the best
baseline by 22% in pass-rate on Gaokao-Formal. The full system surpasses other
model combinations, achieving 64% accuracy on MiniF2F with pass@32 and a
state-of-the-art 18% on Gaokao-Formal.

</details>


### [39] [Reasoning Paths as Signals: Augmenting Multi-hop Fact Verification through Structural Reasoning Progression](https://arxiv.org/abs/2506.07075)
*Liwen Zheng,Chaozhuo Li,Haoran Jia,Xi Zhang*

Main category: cs.AI

TL;DR: 提出了一种结构化推理框架，用于多跳事实验证，通过显式建模推理路径提升证据检索和验证的准确性。


<details>
  <summary>Details</summary>
Motivation: 现实场景中复杂的多跳事实验证需求，现有静态或浅层模型难以捕捉推理路径的动态结构，导致检索碎片化和可解释性不足。

Method: 提出包含结构增强检索机制和推理路径引导验证模块的框架，通过构建推理图和子图来优化证据收集和验证过程。

Result: 在FEVER和HoVer数据集上表现优于基线，验证了推理路径建模对检索精度和验证准确性的提升。

Conclusion: 结构化推理框架有效解决了多跳事实验证中的挑战，显著提升了系统性能。

Abstract: The growing complexity of factual claims in real-world scenarios presents
significant challenges for automated fact verification systems, particularly in
accurately aggregating and reasoning over multi-hop evidence. Existing
approaches often rely on static or shallow models that fail to capture the
evolving structure of reasoning paths, leading to fragmented retrieval and
limited interpretability. To address these issues, we propose a Structural
Reasoning framework for Multi-hop Fact Verification that explicitly models
reasoning paths as structured graphs throughout both evidence retrieval and
claim verification stages. Our method comprises two key modules: a
structure-enhanced retrieval mechanism that constructs reasoning graphs to
guide evidence collection, and a reasoning-path-guided verification module that
incrementally builds subgraphs to represent evolving inference trajectories. We
further incorporate a structure-aware reasoning mechanism that captures
long-range dependencies across multi-hop evidence chains, enabling more precise
verification. Extensive experiments on the FEVER and HoVer datasets demonstrate
that our approach consistently outperforms strong baselines, highlighting the
effectiveness of reasoning-path modeling in enhancing retrieval precision and
verification accuracy.

</details>


### [40] [BRIGHT+: Upgrading the BRIGHT Benchmark with MARCUS, a Multi-Agent RAG Clean-Up Suite](https://arxiv.org/abs/2506.07116)
*Liyang Chen,Yujun Cai,Jieqiong Dong,Yiwei Wang*

Main category: cs.AI

TL;DR: MARCUS是一个多智能体管道，利用LLMs清理和重组BRIGHT基准，生成更高质量的BRIGHT-Plus语料库，显著提升检索和多跳推理性能。


<details>
  <summary>Details</summary>
Motivation: BRIGHT基准存在内容冗余和语义不连贯等常见网络爬取问题，影响检索准确性和下游推理效果。

Method: MARCUS通过多智能体管道，分别处理结构噪声和语义分割，保留关键信息并提升上下文完整性。

Result: BRIGHT-Plus在多种检索器中显著提高了检索准确性和多跳推理能力。

Conclusion: 研究发布了BRIGHT-Plus语料库和MARCUS管道，支持未来关于稳健推理检索的研究。

Abstract: Retrieval-Augmented Generation (RAG) systems require corpora that are both
structurally clean and semantically coherent. BRIGHT is a recent and
influential benchmark designed to evaluate complex multi-hop retrieval across
diverse, high-reasoning domains. However, its practical effectiveness is
limited by common web-crawled artifacts - such as content redundancy and
semantic discontinuity - that impair retrieval accuracy and downstream
reasoning. Notably, we find that such issues are concentrated in seven
StackExchange-derived subdomains, while other domains (e.g., Coding and
Theorem-based content) remain relatively clean.
  In this study, we present MARCUS, a multi-agent pipeline that leverages large
language models (LLMs) to systematically clean and re-chunk BRIGHT into a
higher-quality corpus: BRIGHT-Plus. MARCUS applies dedicated agents for
structural noise removal and semantic segmentation, preserving answer-bearing
spans while improving contextual integrity. Experimental evaluations
demonstrate that BRIGHT-Plus yields consistent and significant improvements in
both retrieval accuracy and multi-hop reasoning across a diverse set of
retrievers. We release both the BRIGHT-Plus corpus and the MARCUS pipeline to
support future research on robust, reasoning-centric retrieval.

</details>


### [41] [Translating Federated Learning Algorithms in Python into CSP Processes Using ChatGPT](https://arxiv.org/abs/2506.07173)
*Miroslav Popovic,Marko Popovic,Miodrag Djukic,Ilija Basicevic*

Main category: cs.AI

TL;DR: 本文介绍了一种利用ChatGPT将Python联邦学习算法自动翻译为CSP过程的方法，并通过模型检查器PAT验证其正确性。


<details>
  <summary>Details</summary>
Motivation: 为简化联邦学习算法的形式化验证过程，减少手动翻译的工作量，提高效率。

Method: 使用ChatGPT自动化翻译Python联邦学习算法为CSP过程，并通过PAT验证其安全性和活性。

Result: 实验验证了该方法能成功翻译并验证集中式和去中心化联邦学习算法。

Conclusion: 该方法有效且高效，适用于非专业程序员和LLMs。

Abstract: The Python Testbed for Federated Learning Algorithms is a simple Python FL
framework that is easy to use by ML&AI developers who do not need to be
professional programmers and is also amenable to LLMs. In the previous
research, generic federated learning algorithms provided by this framework were
manually translated into the CSP processes and algorithms' safety and liveness
properties were automatically verified by the model checker PAT. In this paper,
a simple translation process is introduced wherein the ChatGPT is used to
automate the translation of the mentioned federated learning algorithms in
Python into the corresponding CSP processes. Within the process, the minimality
of the used context is estimated based on the feedback from ChatGPT. The
proposed translation process was experimentally validated by successful
translation (verified by the model checker PAT) of both generic centralized and
decentralized federated learning algorithms.

</details>


### [42] [Mitigating Behavioral Hallucination in Multimodal Large Language Models for Sequential Images](https://arxiv.org/abs/2506.07184)
*Liangliang You,Junchi Yao,Shu Yang,Guimin Hu,Lijie Hu,Di Wang*

Main category: cs.AI

TL;DR: 论文提出SHE框架，解决多模态大语言模型在序列图像中的行为幻觉问题，通过两阶段方法检测和缓解幻觉，并引入新指标BEACH量化行为幻觉。


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型存在行为幻觉问题，影响其可靠性和扩展性，但现有研究主要关注客观幻觉，行为幻觉研究较少。

Method: 提出SHE框架，包括基于自适应时间窗口的视觉-文本对齐检测和通过正交投影缓解幻觉。

Result: 在标准基准测试中，SHE将行为幻觉减少10%以上，同时保持描述准确性。

Conclusion: SHE框架有效解决了行为幻觉问题，为多模态模型的可靠性提供了新思路。

Abstract: While multimodal large language models excel at various tasks, they still
suffer from hallucinations, which limit their reliability and scalability for
broader domain applications. To address this issue, recent research mainly
focuses on objective hallucination. However, for sequential images, besides
objective hallucination, there is also behavioral hallucination, which is less
studied. This work aims to fill in the gap. We first reveal that behavioral
hallucinations mainly arise from two key factors: prior-driven bias and the
snowball effect. Based on these observations, we introduce SHE (Sequence
Hallucination Eradication), a lightweight, two-stage framework that (1) detects
hallucinations via visual-textual alignment check using our proposed adaptive
temporal window and (2) mitigates them via orthogonal projection onto the joint
embedding space. We also propose a new metric (BEACH) to quantify behavioral
hallucination severity. Empirical results on standard benchmarks demonstrate
that SHE reduces behavioral hallucination by over 10% on BEACH while
maintaining descriptive accuracy.

</details>


### [43] [Exploring Effective Strategies for Building a Customised GPT Agent for Coding Classroom Dialogues](https://arxiv.org/abs/2506.07194)
*Luwei Bai,Dongkeun Han,Sara Hennessy*

Main category: cs.AI

TL;DR: 研究探讨了如何利用GPT-4的MyGPT代理定制化编码课堂对话，提出了一套基于小数据配置高效代理的实用策略。


<details>
  <summary>Details</summary>
Motivation: 课堂对话分析在教育中至关重要，但传统方法耗时且难以规模化，而现有大模型研究不适用于小数据集或定制化编码需求。

Method: 使用GPT-4的MyGPT代理，通过变量控制方法评估其基线性能，并基于设计研究方法提出配置策略。

Result: 研究发现，尽管存在局限，采用这些策略的MyGPT代理可作为有效的编码助手，生成编码建议。

Conclusion: MyGPT代理在小数据集和定制化编码方案中具有潜力，但仍需进一步优化。

Abstract: This study investigates effective strategies for developing a customised GPT
agent to code classroom dialogue. While classroom dialogue is widely recognised
as a crucial element of education, its analysis remains challenging due to the
need for a nuanced understanding of dialogic functions and the labour-intensive
nature of manual transcript coding. Recent advancements in large language
models offer promising avenues for automating this process. However, existing
studies predominantly focus on training large-scale models or evaluating
pre-trained models with fixed codebooks, which are often not applicable or
replicable for dialogue researchers working with small datasets or customised
coding schemes. Using GPT-4's MyGPT agent as a case, this study evaluates its
baseline performance in coding classroom dialogue with a human codebook and
examines how performance varies with different example inputs through a
variable control method. Through a design-based research approach, it
identifies a set of practical strategies, based on MyGPT's unique features, for
configuring effective agents with limited data. The findings suggest that,
despite some limitations, a MyGPT agent developed with these strategies can
serve as a useful coding assistant by generating coding suggestions.

</details>


### [44] [Reasoning Multimodal Large Language Model: Data Contamination and Dynamic Evaluation](https://arxiv.org/abs/2506.07202)
*Ming Liu,Wensheng Zhang*

Main category: cs.AI

TL;DR: 提出了一种动态评估框架，通过任务扰动而非输入扰动，评估多模态大语言模型（MLLMs）的泛化能力，揭示模型是否依赖任务特定线索。


<details>
  <summary>Details</summary>
Motivation: 现有MLLMs在视觉语言基准测试中表现优异，但数据污染（训练中暴露测试集）可能掩盖真实泛化能力，需更严格的评估方法。

Method: 通过任务扰动（如QA、标题生成、问题提出等）评估模型，使用自动化流程和校准评分机制分析模型跨任务能力。

Result: 实验表明，在模拟测试数据上微调会显著提高任务特定性能，但损害整体泛化能力。

Conclusion: 动态任务扰动方法能更深入评估MLLMs的泛化能力，区分真实理解与数据泄漏或过拟合。

Abstract: Multimodal Large Language Models (MLLMs) show impressive vision-language
benchmark performance, yet growing concerns about data contamination (test set
exposure during training) risk masking true generalization. This concern
extends to reasoning MLLMs, often fine-tuned via reinforcement learning from
potentially contaminated base models. We propose a novel dynamic evaluation
framework to rigorously assess MLLM generalization, moving beyond static
benchmarks. Instead of perturbing inputs, we perturb the task itself. Using the
same visual input, models are evaluated across a family of tasks (e.g., QA,
captioning, question posing, verification) to probe diverse capabilities. This
task perturbation reveals whether model performance is robust or reliant on
superficial task-specific cues. Our approach is analogous to loss landscape
sharpness: models overfit or contaminated for a single task (sharp minima)
falter under task shifts, unlike models with generalizable solutions (flatter
minima). We developed an automated pipeline with a calibrated judge scoring
open-ended generations (captions, questions) using paraphrase and corruption
sampling. Applying this framework to leading image/video MLLMs on benchmarks
including MME, RealWorldQA, and CVRR-ES, we analyze each model's cross-task
"ability vector." We demonstrate that fine-tuning on simulated test data
(extreme contamination) drastically sharpens task-specific performance but
harms overall generalization. Our dynamic task perturbation offers deeper
insights into MLLM generalization, distinguishing genuine understanding from
spurious leakage or overfitting.

</details>


### [45] [BIMgent: Towards Autonomous Building Modeling via Computer-use Agents](https://arxiv.org/abs/2506.07217)
*Zihan Deng,Changyu Du,Stavros Nousias,André Borrmann*

Main category: cs.AI

TL;DR: BIMgent是一个基于多模态大语言模型的代理框架，旨在通过GUI操作实现自主建筑模型创作，专注于AEC领域的3D建模任务。


<details>
  <summary>Details</summary>
Motivation: 现有计算机代理主要关注通用桌面自动化任务，而在高度专业化的AEC领域（尤其是BIM软件的复杂交互和开放设计任务）中应用不足。

Method: BIMgent通过多模态输入（概念设计）、软件特定工作流规划和高效执行GUI操作，自动化建筑建模过程。

Result: BIMgent在真实建模任务中表现优于基线模型（32%成功率vs. 0%），设计质量合理，显著减少人工工作量。

Conclusion: BIMgent展示了在真实建筑建模场景中的实际部署潜力，能够有效保留设计意图并提升效率。

Abstract: Existing computer-use agents primarily focus on general-purpose desktop
automation tasks, with limited exploration of their application in highly
specialized domains. In particular, the 3D building modeling process in the
Architecture, Engineering, and Construction (AEC) sector involves open-ended
design tasks and complex interaction patterns within Building Information
Modeling (BIM) authoring software, which has yet to be thoroughly addressed by
current studies. In this paper, we propose BIMgent, an agentic framework
powered by multimodal large language models (LLMs), designed to enable
autonomous building model authoring via graphical user interface (GUI)
operations. BIMgent automates the architectural building modeling process,
including multimodal input for conceptual design, planning of software-specific
workflows, and efficient execution of the authoring GUI actions. We evaluate
BIMgent on real-world building modeling tasks, including both text-based
conceptual design generation and reconstruction from existing building design.
The design quality achieved by BIMgent was found to be reasonable. Its
operations achieved a 32% success rate, whereas all baseline models failed to
complete the tasks (0% success rate). Results demonstrate that BIMgent
effectively reduces manual workload while preserving design intent,
highlighting its potential for practical deployment in real-world architectural
modeling scenarios.

</details>


### [46] [LLM-Enhanced Rapid-Reflex Async-Reflect Embodied Agent for Real-Time Decision-Making in Dynamically Changing Environments](https://arxiv.org/abs/2506.07223)
*Yangqing Zheng,Shunqi Mao,Dingxin Zhang,Weidong Cai*

Main category: cs.AI

TL;DR: 论文提出了一种时间转换机制（TCM）和快速反射异步反思代理（RRARA），用于解决动态高风险场景中决策延迟问题，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 在动态高风险场景（如火灾、洪水等）中，决策延迟是一个关键但研究不足的问题，需要一种方法来量化延迟并提升代理的反应速度。

Method: 提出TCM将推理延迟转换为等效模拟帧，并设计RRARA代理，结合轻量级LLM反馈模块和基于规则的代理，实现快速反应和异步反思。

Result: 在HAZARD基准测试中，RRARA在延迟敏感场景中显著优于现有基线。

Conclusion: TCM和RRARA为解决高风险场景中的决策延迟问题提供了有效方案，未来可进一步优化和扩展。

Abstract: In the realm of embodied intelligence, the evolution of large language models
(LLMs) has markedly enhanced agent decision making. Consequently, researchers
have begun exploring agent performance in dynamically changing high-risk
scenarios, i.e., fire, flood, and wind scenarios in the HAZARD benchmark. Under
these extreme conditions, the delay in decision making emerges as a crucial yet
insufficiently studied issue. We propose a Time Conversion Mechanism (TCM) that
translates inference delays in decision-making into equivalent simulation
frames, thus aligning cognitive and physical costs under a single FPS-based
metric. By extending HAZARD with Respond Latency (RL) and Latency-to-Action
Ratio (LAR), we deliver a fully latency-aware evaluation protocol. Moreover, we
present the Rapid-Reflex Async-Reflect Agent (RRARA), which couples a
lightweight LLM-guided feedback module with a rule-based agent to enable
immediate reactive behaviors and asynchronous reflective refinements in situ.
Experiments on HAZARD show that RRARA substantially outperforms existing
baselines in latency-sensitive scenarios.

</details>


### [47] [Subgoal-Guided Policy Heuristic Search with Learned Subgoals](https://arxiv.org/abs/2506.07255)
*Jake Tuero,Michael Buro,Levi H. S. Lelis*

Main category: cs.AI

TL;DR: 本文提出了一种基于子目标的学习方法，用于改进策略树搜索算法的样本效率。


<details>
  <summary>Details</summary>
Motivation: 策略树搜索算法需要完整的解轨迹来训练策略，但在处理困难问题时，训练成本高昂且样本效率低。

Method: 通过从搜索树（包括失败尝试的树）中学习子目标和基于子目标的策略。

Result: 实验表明，该方法提高了在线学习中策略和启发式函数的样本效率。

Conclusion: 该方法显著提升了策略树搜索算法的训练效率，尤其是在困难问题中。

Abstract: Policy tree search is a family of tree search algorithms that use a policy to
guide the search. These algorithms provide guarantees on the number of
expansions required to solve a given problem that are based on the quality of
the policy. While these algorithms have shown promising results, the process in
which they are trained requires complete solution trajectories to train the
policy. Search trajectories are obtained during a trial-and-error search
process. When the training problem instances are hard, learning can be
prohibitively costly, especially when starting from a randomly initialized
policy. As a result, search samples are wasted in failed attempts to solve
these hard instances. This paper introduces a novel method for learning
subgoal-based policies for policy tree search algorithms. The subgoals and
policies conditioned on subgoals are learned from the trees that the search
expands while attempting to solve problems, including the search trees of
failed attempts. We empirically show that our policy formulation and training
method improve the sample efficiency of learning a policy and heuristic
function in this online setting.

</details>


### [48] [Boosting Vulnerability Detection of LLMs via Curriculum Preference Optimization with Synthetic Reasoning Data](https://arxiv.org/abs/2506.07390)
*Xin-Cheng Wen,Yijun Yang,Cuiyun Gao,Yang Xiao,Deheng Ye*

Main category: cs.AI

TL;DR: ReVD框架通过合成推理数据和优化漏洞特定偏好，显著提升了LLM在软件漏洞检测中的性能。


<details>
  <summary>Details</summary>
Motivation: LLM在漏洞检测中表现有限，主要因缺乏推理数据和语义学习不足。

Method: 构建漏洞和修复代码的前后推理过程，设计三元组监督微调和课程在线偏好优化。

Result: 在PrimeVul和SVEN数据集上，ReVD将准确率提高了12.24%-22.77%。

Conclusion: ReVD为基于LLM的漏洞检测设定了新的最先进水平。

Abstract: Large language models (LLMs) demonstrate considerable proficiency in numerous
coding-related tasks; however, their capabilities in detecting software
vulnerabilities remain limited. This limitation primarily stems from two
factors: (1) the absence of reasoning data related to vulnerabilities, which
hinders the models' ability to capture underlying vulnerability patterns; and
(2) their focus on learning semantic representations rather than the reason
behind them, thus failing to recognize semantically similar vulnerability
samples. Furthermore, the development of LLMs specialized in vulnerability
detection is challenging, particularly in environments characterized by the
scarcity of high-quality datasets. In this paper, we propose a novel framework
ReVD that excels at mining vulnerability patterns through reasoning data
synthesizing and vulnerability-specific preference optimization. Specifically,
we construct forward and backward reasoning processes for vulnerability and
corresponding fixed code, ensuring the synthesis of high-quality reasoning
data. Moreover, we design the triplet supervised fine-tuning followed by
curriculum online preference optimization for enabling ReVD to better
understand vulnerability patterns. The extensive experiments conducted on
PrimeVul and SVEN datasets demonstrate that ReVD sets new state-of-the-art for
LLM-based software vulnerability detection, e.g., 12.24\%-22.77\% improvement
in the accuracy. The source code and data are available at
https://github.com/Xin-Cheng-Wen/PO4Vul.

</details>


### [49] [An Intelligent Fault Self-Healing Mechanism for Cloud AI Systems via Integration of Large Language Models and Deep Reinforcement Learning](https://arxiv.org/abs/2506.07411)
*Ze Yang,Yihong Jin,Juntian Liu,Xinhe Xu*

Main category: cs.AI

TL;DR: 论文提出了一种结合大语言模型（LLM）和深度强化学习（DRL）的智能故障自愈机制（IFSHM），用于云AI系统的故障检测与恢复。


<details>
  <summary>Details</summary>
Motivation: 随着云AI系统的规模和复杂性增加，故障检测与自适应恢复成为确保服务可靠性的核心挑战。

Method: 采用两阶段混合架构：LLM驱动的故障语义解释模块和DRL恢复策略优化器，结合LLM的环境建模和动作空间抽象，提升强化学习的效率和泛化能力。

Result: 实验表明，IFSHM框架在未知故障场景下比现有方法缩短系统恢复时间37%。

Conclusion: IFSHM通过LLM和DRL的结合，显著提升了云AI系统的故障恢复能力。

Abstract: As the scale and complexity of cloud-based AI systems continue to increase,
the detection and adaptive recovery of system faults have become the core
challenges to ensure service reliability and continuity. In this paper, we
propose an Intelligent Fault Self-Healing Mechanism (IFSHM) that integrates
Large Language Model (LLM) and Deep Reinforcement Learning (DRL), aiming to
realize a fault recovery framework with semantic understanding and policy
optimization capabilities in cloud AI systems. On the basis of the traditional
DRL-based control model, the proposed method constructs a two-stage hybrid
architecture: (1) an LLM-driven fault semantic interpretation module, which can
dynamically extract deep contextual semantics from multi-source logs and system
indicators to accurately identify potential fault modes; (2) DRL recovery
strategy optimizer, based on reinforcement learning, learns the dynamic
matching of fault types and response behaviors in the cloud environment. The
innovation of this method lies in the introduction of LLM for environment
modeling and action space abstraction, which greatly improves the exploration
efficiency and generalization ability of reinforcement learning. At the same
time, a memory-guided meta-controller is introduced, combined with
reinforcement learning playback and LLM prompt fine-tuning strategy, to achieve
continuous adaptation to new failure modes and avoid catastrophic forgetting.
Experimental results on the cloud fault injection platform show that compared
with the existing DRL and rule methods, the IFSHM framework shortens the system
recovery time by 37% with unknown fault scenarios.

</details>


### [50] [Evaluating Visual Mathematics in Multimodal LLMs: A Multilingual Benchmark Based on the Kangaroo Tests](https://arxiv.org/abs/2506.07418)
*Arnau Igualde Sáez,Lamyae Rhomrasi,Yusef Ahsini,Ricardo Vinuesa,Sergio Hoyas,Jose P. García Sabater,Marius J. Fullana i Alfonso,J. Alberto Conejero*

Main category: cs.AI

TL;DR: 该论文研究了多模态大语言模型（MLLMs）在视觉数学问题解决中的表现，评估了多个模型在多语言基准测试中的性能，发现模型在几何、代数等领域表现中等，且对视觉信息的利用不足。


<details>
  <summary>Details</summary>
Motivation: 探索MLLMs在视觉数学问题中的能力，填补现有研究的空白。

Method: 通过多语言Kangaroo风格基准测试评估多个MLLMs模型（如GPT 4o、Gemini 2.0 Flash等），涵盖几何、代数、逻辑等领域。

Result: 模型整体表现中等，Gemini 2.0 Flash在图像任务中表现最佳，但未达到人类水平；模型对视觉信息的利用有限，且在不同语言和难度下表现不一。

Conclusion: MLLMs在视觉数学问题中仍有提升空间，需进一步优化模型对视觉信息的利用和推理能力。

Abstract: Multimodal Large Language Models (MLLMs) promise advanced vision language
capabilities, yet their effectiveness in visually presented mathematics remains
underexplored. This paper analyzes the development and evaluation of MLLMs for
mathematical problem solving, focusing on diagrams, multilingual text, and
symbolic notation. We then assess several models, including GPT 4o, Pixtral,
Qwen VL, Llama 3.2 Vision variants, and Gemini 2.0 Flash in a multilingual
Kangaroo style benchmark spanning English, French, Spanish, and Catalan. Our
experiments reveal four key findings. First, overall precision remains moderate
across geometry, visual algebra, logic, patterns, and combinatorics: no single
model excels in every topic. Second, while most models see improved accuracy
with questions that do not have images, the gain is often limited; performance
for some remains nearly unchanged without visual input, indicating
underutilization of diagrammatic information. Third, substantial variation
exists across languages and difficulty levels: models frequently handle easier
items but struggle with advanced geometry and combinatorial reasoning. Notably,
Gemini 2.0 Flash achieves the highest precision on image based tasks, followed
by Qwen VL 2.5 72B and GPT 4o, though none approach human level performance.
Fourth, a complementary analysis aimed at distinguishing whether models reason
or simply recite reveals that Gemini and GPT 4o stand out for their structured
reasoning and consistent accuracy. In contrast, Pixtral and Llama exhibit less
consistent reasoning, often defaulting to heuristics or randomness when unable
to align their outputs with the given answer options.

</details>


### [51] [HeTa: Relation-wise Heterogeneous Graph Foundation Attack Model](https://arxiv.org/abs/2506.07428)
*Yuling Wang,Zihui Chen,Pengfei Jiao,Xiao Wang*

Main category: cs.AI

TL;DR: 本文提出了一种通用的异构图神经网络攻击模型HeTa，通过挖掘共享的攻击单元实现跨不同HGNNs的通用扰动。


<details>
  <summary>Details</summary>
Motivation: 异构图神经网络（HGNNs）存在脆弱性，现有攻击方法需要复杂参数调整，难以适应新场景。基础模型的出现为通用攻击提供了可能。

Method: 提出关系感知的异构图基础攻击模型HeTa，通过基础代理模型对齐异质性并识别共享攻击单元，实现序列化关系攻击。

Result: 实验表明HeTa具有强大的攻击性能和泛化能力。

Conclusion: HeTa为HGNNs提供了一种通用且高效的攻击方法，能够快速适应新异构图。

Abstract: Heterogeneous Graph Neural Networks (HGNNs) are vulnerable, highlighting the
need for tailored attacks to assess their robustness and ensure security.
However, existing HGNN attacks often require complex retraining of parameters
to generate specific perturbations for new scenarios. Recently, foundation
models have opened new horizons for the generalization of graph neural networks
by capturing shared semantics across various graph distributions. This leads us
to ask:Can we design a foundation attack model for HGNNs that enables
generalizable perturbations across different HGNNs, and quickly adapts to new
heterogeneous graphs (HGs)? Empirical findings reveal that, despite significant
differences in model design and parameter space, different HGNNs surprisingly
share common vulnerability patterns from a relation-aware perspective.
Therefore, we explore how to design foundation HGNN attack criteria by mining
shared attack units. In this paper, we propose a novel relation-wise
heterogeneous graph foundation attack model, HeTa. We introduce a foundation
surrogate model to align heterogeneity and identify the importance of shared
relation-aware attack units. Building on this, we implement a serialized
relation-by-relation attack based on the identified relational weights. In this
way, the perturbation can be transferred to various target HGNNs and easily
fine-tuned for new HGs. Extensive experiments exhibit powerful attack
performances and generalizability of our method.

</details>


### [52] [LegalReasoner: Step-wised Verification-Correction for Legal Judgment Reasoning](https://arxiv.org/abs/2506.07443)
*Weijie Shi,Han Zhu,Jiaming Ji,Mengze Li,Jipeng Zhang,Ruiyuan Zhang,Jia Zhu,Jiajie Xu,Sirui Han,Yike Guo*

Main category: cs.AI

TL;DR: 论文提出LegalReasoner，通过逐步验证和修正推理过程，提升法律判决预测的可靠性。


<details>
  <summary>Details</summary>
Motivation: 现有方法在复杂法律推理中易出现逻辑错误，需改进以支持法院决策和提高司法效率。

Method: LegalReasoner通过识别争议点分解复杂案件，逐步推理并验证每一步逻辑，检测错误时应用专家设计的修正策略。

Result: 实验表明，LegalReasoner将LLAMA-3.1-70B模型与法院判决的一致性从72.37提升至80.27。

Conclusion: LegalReasoner显著提升了法律判决预测的可靠性，并发布了LegalHK数据集以支持研究。

Abstract: Legal judgment prediction (LJP) aims to function as a judge by making final
rulings based on case claims and facts, which plays a vital role in the
judicial domain for supporting court decision-making and improving judicial
efficiency. However, existing methods often struggle with logical errors when
conducting complex legal reasoning. We propose LegalReasoner, which enhances
LJP reliability through step-wise verification and correction of the reasoning
process. Specifically, it first identifies dispute points to decompose complex
cases, and then conducts step-wise reasoning while employing a process verifier
to validate each step's logic from correctness, progressiveness, and potential
perspectives. When errors are detected, expert-designed attribution and
resolution strategies are applied for correction. To fine-tune LegalReasoner,
we release the LegalHK dataset, containing 58,130 Hong Kong court cases with
detailed annotations of dispute points, step-by-step reasoning chains, and
process verification labels. Experiments demonstrate that LegalReasoner
significantly improves concordance with court decisions from 72.37 to 80.27 on
LLAMA-3.1-70B. The data is available at
https://huggingface.co/datasets/weijiezz/LegalHK.

</details>


### [53] [Fact in Fragments: Deconstructing Complex Claims via LLM-based Atomic Fact Extraction and Verification](https://arxiv.org/abs/2506.07446)
*Liwen Zheng,Chaozhuo Li,Zheng Liu,Feiran Huang,Haoran Jia,Zaisheng Ye,Xi Zhang*

Main category: cs.AI

TL;DR: AFEV框架通过迭代分解复杂主张为原子事实，提升事实验证的准确性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 传统方法在处理需要多跳推理的复杂主张时表现不佳，导致推理错误累积和证据噪声污染。

Method: AFEV框架通过迭代分解主张、动态优化证据检索和利用上下文示例指导推理。

Result: 在五个基准数据集上，AFEV实现了最先进的性能和可解释性。

Conclusion: AFEV为解决复杂主张的事实验证问题提供了有效且可解释的解决方案。

Abstract: Fact verification plays a vital role in combating misinformation by assessing
the veracity of claims through evidence retrieval and reasoning. However,
traditional methods struggle with complex claims requiring multi-hop reasoning
over fragmented evidence, as they often rely on static decomposition strategies
and surface-level semantic retrieval, which fail to capture the nuanced
structure and intent of the claim. This results in accumulated reasoning
errors, noisy evidence contamination, and limited adaptability to diverse
claims, ultimately undermining verification accuracy in complex scenarios. To
address this, we propose Atomic Fact Extraction and Verification (AFEV), a
novel framework that iteratively decomposes complex claims into atomic facts,
enabling fine-grained retrieval and adaptive reasoning. AFEV dynamically
refines claim understanding and reduces error propagation through iterative
fact extraction, reranks evidence to filter noise, and leverages
context-specific demonstrations to guide the reasoning process. Extensive
experiments on five benchmark datasets demonstrate that AFEV achieves
state-of-the-art performance in both accuracy and interpretability.

</details>


### [54] [Efficient Generation of Diverse Cooperative Agents with World Models](https://arxiv.org/abs/2506.07450)
*Yi Loo,Akshunn Trivedi,Malika Meghjani*

Main category: cs.AI

TL;DR: XPM-WM框架通过使用学习的世界模型生成模拟轨迹，显著提高了零样本协调（ZSC）代理的训练效率，减少了计算成本和样本需求。


<details>
  <summary>Details</summary>
Motivation: 当前零样本协调代理训练中，生成多样化的协作伙伴代理是一个主要瓶颈，传统方法计算成本高且样本效率低。

Method: 提出XPM-WM框架，利用学习的世界模型生成模拟轨迹，替代传统方法中需要采样多种轨迹的需求。

Result: XPM-WM能高效生成多样化协作伙伴，性能与传统方法相当，同时显著提高了样本效率和可扩展性。

Conclusion: XPM-WM是一种更高效、可扩展的零样本协调代理训练方法。

Abstract: A major bottleneck in the training process for Zero-Shot Coordination (ZSC)
agents is the generation of partner agents that are diverse in collaborative
conventions. Current Cross-play Minimization (XPM) methods for population
generation can be very computationally expensive and sample inefficient as the
training objective requires sampling multiple types of trajectories. Each
partner agent in the population is also trained from scratch, despite all of
the partners in the population learning policies of the same coordination task.
In this work, we propose that simulated trajectories from the dynamics model of
an environment can drastically speed up the training process for XPM methods.
We introduce XPM-WM, a framework for generating simulated trajectories for XPM
via a learned World Model (WM). We show XPM with simulated trajectories removes
the need to sample multiple trajectories. In addition, we show our proposed
method can effectively generate partners with diverse conventions that match
the performance of previous methods in terms of SP population training reward
as well as training partners for ZSC agents. Our method is thus, significantly
more sample efficient and scalable to a larger number of partners.

</details>


### [55] [Learning What Reinforcement Learning Can't: Interleaved Online Fine-Tuning for Hardest Questions](https://arxiv.org/abs/2506.07527)
*Lu Ma,Hao Liang,Meiyi Qiang,Lexiang Tang,Xiaochen Ma,Zhen Hao Wong,Junbo Niu,Chengyu Shen,Runming He,Bin Cui,Wentao Zhang*

Main category: cs.AI

TL;DR: 论文提出了一种结合强化学习（RL）和监督微调（SFT）的新方法ReLIFT，以克服RL在LLM推理中的局限性，显著提升了模型性能。


<details>
  <summary>Details</summary>
Motivation: 当前RL方法无法超越基础模型的局限性，无法学习新知识，而SFT可以补充这一不足。因此，结合两者的优势成为研究动机。

Method: 提出ReLIFT方法，交替使用RL和在线微调（基于高质量演示数据）来提升模型推理能力。

Result: ReLIFT在多个基准测试中平均提升5.2分，且仅需13%的演示数据，优于单独使用RL或SFT。

Conclusion: ReLIFT有效克服了RL的局限性，展示了结合RL和SFT的潜力。

Abstract: Recent advances in large language model (LLM) reasoning have shown that
sophisticated behaviors such as planning and self-reflection can emerge through
reinforcement learning (RL). However, despite these successes, RL in its
current form remains insufficient to induce capabilities that exceed the
limitations of the base model, as it is primarily optimized based on existing
knowledge of the model rather than facilitating the acquisition of new
information. To address this limitation, we employ supervised fine-tuning (SFT)
to learn what RL cannot, which enables the incorporation of new knowledge and
reasoning patterns by leveraging high-quality demonstration data. We analyze
the training dynamics of RL and SFT for LLM reasoning and find that RL excels
at maintaining and improving performance on questions within the model's
original capabilities, while SFT is more effective at enabling progress on
questions beyond the current scope of the model. Motivated by the complementary
strengths of RL and SFT, we introduce a novel training approach,
\textbf{ReLIFT} (\textbf{Re}inforcement \textbf{L}earning \textbf{I}nterleaved
with Online \textbf{F}ine-\textbf{T}uning). In ReLIFT, the model is primarily
trained using RL, but when it encounters challenging questions, high-quality
solutions are collected for fine-tuning, and the training process alternates
between RL and fine-tuning to enhance the model's reasoning abilities. ReLIFT
achieves an average improvement of over +5.2 points across five
competition-level benchmarks and one out-of-distribution benchmark compared to
other zero-RL models. Furthermore, we demonstrate that ReLIFT outperforms both
RL and SFT while using only 13\% of the detailed demonstration data,
highlighting its scalability. These results provide compelling evidence that
ReLIFT overcomes the fundamental limitations of RL and underscores the
significant potential.

</details>


### [56] [Coordinating Search-Informed Reasoning and Reasoning-Guided Search in Claim Verification](https://arxiv.org/abs/2506.07528)
*Qisheng Hu,Quanyu Long,Wenya Wang*

Main category: cs.AI

TL;DR: HARIS模型通过分层代理（高层推理代理和低层搜索代理）协同工作，提升多跳声明验证的准确性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 多跳声明验证需要动态检索证据和推理的交替过程，现有方法难以有效协调这两者。

Method: 提出HARIS模型，包含高层推理代理和低层搜索代理，分别负责推理链构建和信息检索，并通过强化学习训练。

Result: 在EX-FEVER和HOVER基准测试中表现优异，显著提升多跳声明验证性能。

Conclusion: HARIS通过分层代理设计有效协调推理与检索，为多跳声明验证提供了新思路。

Abstract: Multi-hop claim verification is inherently challenging, requiring multi-step
reasoning to construct verification chains while iteratively searching for
information to uncover hidden bridging facts. This process is fundamentally
interleaved, as effective reasoning relies on dynamically retrieved evidence,
while effective search demands reasoning to refine queries based on partial
information. To achieve this, we propose Hierarchical Agent Reasoning and
Information Search (HARIS), explicitly modeling the coordinated process of
reasoning-driven searching and search-informed reasoning. HARIS consists of a
high-level reasoning agent that focuses on constructing the main verification
chain, generating factual questions when more information is needed, and a
low-level search agent that iteratively retrieves more information, refining
its search based on intermediate findings. This design allows each agent to
specialize in its respective task, enhancing verification accuracy and
interpretability. HARIS is trained using reinforcement learning with
outcome-based rewards. Experimental results on the EX-FEVER and HOVER
benchmarks demonstrate that HARIS achieves strong performance, greatly
advancing multi-hop claim verification.

</details>


### [57] [Curriculum Learning With Counterfactual Group Relative Policy Advantage For Multi-Agent Reinforcement Learning](https://arxiv.org/abs/2506.07548)
*Weiqiang Jin,Hongyang Du,Guizhong Liu,Dong In Kim*

Main category: cs.AI

TL;DR: 提出了一种动态课程学习框架（CL）用于多智能体强化学习（MARL），通过自适应难度调整机制和反事实组相对策略优势（CGRPA）提升训练稳定性和性能。


<details>
  <summary>Details</summary>
Motivation: 现有MARL方法通常在固定对手策略下训练，难以适应动态环境，导致策略次优。

Method: 采用动态CL框架，结合CGRPA机制，通过反事实优势函数评估个体贡献，提供内在奖励。

Result: 实验表明，该方法提升了训练稳定性和最终性能，优于现有方法。

Conclusion: 动态CL与CGRPA结合有效解决了MARL中的非平稳性和信用分配问题。

Abstract: Multi-agent reinforcement learning (MARL) has achieved strong performance in
cooperative adversarial tasks. However, most existing methods typically train
agents against fixed opponent strategies and rely on such meta-static
difficulty conditions, which limits their adaptability to changing environments
and often leads to suboptimal policies. Inspired by the success of curriculum
learning (CL) in supervised tasks, we propose a dynamic CL framework for MARL
that employs an self-adaptive difficulty adjustment mechanism. This mechanism
continuously modulates opponent strength based on real-time agent training
performance, allowing agents to progressively learn from easier to more
challenging scenarios. However, the dynamic nature of CL introduces instability
due to nonstationary environments and sparse global rewards. To address this
challenge, we develop a Counterfactual Group Relative Policy Advantage (CGRPA),
which is tightly coupled with the curriculum by providing intrinsic credit
signals that reflect each agent's impact under evolving task demands. CGRPA
constructs a counterfactual advantage function that isolates individual
contributions within group behavior, facilitating more reliable policy updates
throughout the curriculum. CGRPA evaluates each agent's contribution through
constructing counterfactual action advantage function, providing intrinsic
rewards that enhance credit assignment and stabilize learning under
non-stationary conditions. Extensive experiments demonstrate that our method
improves both training stability and final performance, achieving competitive
results against state-of-the-art methods. The code is available at
https://github.com/NICE-HKU/CL2MARL-SMAC.

</details>


### [58] [GTR-CoT: Graph Traversal as Visual Chain of Thought for Molecular Structure Recognition](https://arxiv.org/abs/2506.07553)
*Jingchao Wang,Haote Yang,Jiang Wu,Yifan He,Xingjian Wei,Yinfan Wang,Chengjin Liu,Lingli Ge,Lijun Wu,Bin Wang,Dahua Lin,Conghui He*

Main category: cs.AI

TL;DR: GTR-Mol-VLM 是一种新型框架，通过模拟人类推理和解决标注不匹配问题，显著提升了光学化学结构识别（OCSR）的性能。


<details>
  <summary>Details</summary>
Motivation: 解决现有视觉语言模型（VLMs）在复杂分子结构和标注不一致方面的不足。

Method: 引入 Graph Traversal as Visual Chain of Thought 机制和 Faithfully Recognize What You've Seen 数据原则，并构建 GTR-CoT-1.3M 数据集和 MolRec-Bench 基准。

Result: GTR-Mol-VLM 在多种场景下表现优于其他模型，特别是在功能组缩写分子图像中领先基线约 14 个百分点。

Conclusion: 该工作有望推动 OCSR 技术更有效地满足实际需求，促进化学信息学和 AI for Science 的发展。

Abstract: Optical Chemical Structure Recognition (OCSR) is crucial for digitizing
chemical knowledge by converting molecular images into machine-readable
formats. While recent vision-language models (VLMs) have shown potential in
this task, their image-captioning approach often struggles with complex
molecular structures and inconsistent annotations. To overcome these
challenges, we introduce GTR-Mol-VLM, a novel framework featuring two key
innovations: (1) the \textit{Graph Traversal as Visual Chain of Thought}
mechanism that emulates human reasoning by incrementally parsing molecular
graphs through sequential atom-bond predictions, and (2) the data-centric
principle of \textit{Faithfully Recognize What You've Seen}, which addresses
the mismatch between abbreviated structures in images and their expanded
annotations. To support model development, we constructed GTR-CoT-1.3M, a
large-scale instruction-tuning dataset with meticulously corrected annotations,
and introduced MolRec-Bench, the first benchmark designed for a fine-grained
evaluation of graph-parsing accuracy in OCSR. Comprehensive experiments
demonstrate that GTR-Mol-VLM achieves superior results compared to specialist
models, chemistry-domain VLMs, and commercial general-purpose VLMs. Notably, in
scenarios involving molecular images with functional group abbreviations,
GTR-Mol-VLM outperforms the second-best baseline by approximately 14 percentage
points, both in SMILES-based and graph-based metrics. We hope that this work
will drive OCSR technology to more effectively meet real-world needs, thereby
advancing the fields of cheminformatics and AI for Science. We will release
GTR-CoT at https://github.com/opendatalab/GTR-CoT.

</details>


### [59] [SAFEFLOW: A Principled Protocol for Trustworthy and Transactional Autonomous Agent Systems](https://arxiv.org/abs/2506.07564)
*Peiran Li,Xinkai Zou,Zhuohang Wu,Ruifeng Li,Shuo Xing,Hanwen Zheng,Zhikai Hu,Yuping Wang,Haoxi Li,Qin Yuan,Yingmo Zhang,Zhengzhong Tu*

Main category: cs.AI

TL;DR: SAFEFLOW是一个新的协议级框架，旨在构建可信赖的基于LLM/VLM的智能代理，通过细粒度信息流控制、事务执行和冲突解决机制提升安全性和可靠性。


<details>
  <summary>Details</summary>
Motivation: 当前基于LLM/VLM的代理框架在安全性、可靠性和多代理协调方面存在不足，缺乏机制保障信息流控制和全局一致性。

Method: SAFEFLOW通过细粒度信息流控制（IFC）、事务执行、冲突解决和安全调度等技术，确保数据的安全性和一致性。

Result: 实验表明，SAFEFLOW在对抗性、噪声和并发环境下显著优于现有技术，同时保持任务性能和安全性。

Conclusion: SAFEFLOW为构建可靠、安全的智能代理生态系统奠定了基础，推动了可靠自主性的前沿发展。

Abstract: Recent advances in large language models (LLMs) and vision-language models
(VLMs) have enabled powerful autonomous agents capable of complex reasoning and
multi-modal tool use. Despite their growing capabilities, today's agent
frameworks remain fragile, lacking principled mechanisms for secure information
flow, reliability, and multi-agent coordination. In this work, we introduce
SAFEFLOW, a new protocol-level framework for building trustworthy LLM/VLM-based
agents. SAFEFLOW enforces fine-grained information flow control (IFC),
precisely tracking provenance, integrity, and confidentiality of all the data
exchanged between agents, tools, users, and environments. By constraining LLM
reasoning to respect these security labels, SAFEFLOW prevents untrusted or
adversarial inputs from contaminating high-integrity decisions. To ensure
robustness in concurrent multi-agent settings, SAFEFLOW introduces
transactional execution, conflict resolution, and secure scheduling over shared
state, preserving global consistency across agents. We further introduce
mechanisms, including write-ahead logging, rollback, and secure caches, that
further enhance resilience against runtime errors and policy violations. To
validate the performances, we built SAFEFLOWBENCH, a comprehensive benchmark
suite designed to evaluate agent reliability under adversarial, noisy, and
concurrent operational conditions. Extensive experiments demonstrate that
agents built with SAFEFLOW maintain impressive task performance and security
guarantees even in hostile environments, substantially outperforming
state-of-the-art. Together, SAFEFLOW and SAFEFLOWBENCH lay the groundwork for
principled, robust, and secure agent ecosystems, advancing the frontier of
reliable autonomy.

</details>


### [60] [Automating Exploratory Multiomics Research via Language Models](https://arxiv.org/abs/2506.07591)
*Shang Qu,Ning Ding,Linhai Xie,Yifei Li,Zaoqu Liu,Kaiyan Zhang,Yibai Xiong,Yuxin Zuo,Zhangren Chen,Ermo Hua,Xingtai Lv,Youbang Sun,Yang Li,Dong Li,Fuchu He,Bowen Zhou*

Main category: cs.AI

TL;DR: PROTEUS是一个全自动系统，能从原始数据文件生成数据驱动的假设，应用于临床蛋白质组学领域。


<details>
  <summary>Details</summary>
Motivation: 在临床蛋白质组学中，高效的数据分析和假设生成对发现新知识至关重要。

Method: PROTEUS通过模块化模拟科学过程，从开放数据探索到统计分析，最终生成假设，并使用统一图结构管理复杂研究流程。

Result: 在10个临床多组学数据集中生成了360个假设，并通过外部验证和自动评分评估。

Conclusion: PROTEUS能高效处理多组学数据，平衡可靠性与新颖性，为科学领域的自主假设生成提供了新路径。

Abstract: This paper introduces PROTEUS, a fully automated system that produces
data-driven hypotheses from raw data files. We apply PROTEUS to clinical
proteogenomics, a field where effective downstream data analysis and hypothesis
proposal is crucial for producing novel discoveries. PROTEUS uses separate
modules to simulate different stages of the scientific process, from open-ended
data exploration to specific statistical analysis and hypothesis proposal. It
formulates research directions, tools, and results in terms of relationships
between biological entities, using unified graph structures to manage complex
research processes. We applied PROTEUS to 10 clinical multiomics datasets from
published research, arriving at 360 total hypotheses. Results were evaluated
through external data validation and automatic open-ended scoring. Through
exploratory and iterative research, the system can navigate high-throughput and
heterogeneous multiomics data to arrive at hypotheses that balance reliability
and novelty. In addition to accelerating multiomic analysis, PROTEUS represents
a path towards tailoring general autonomous systems to specialized scientific
domains to achieve open-ended hypothesis generation from data.

</details>


### [61] [SWE-Dev: Building Software Engineering Agents with Training and Inference Scaling](https://arxiv.org/abs/2506.07636)
*Haoran Wang,Zhenyu Hou,Yao Wei,Jie Tang,Yuxiao Dong*

Main category: cs.AI

TL;DR: SWE-Dev是一个基于开源大语言模型（LLM）的软件工程（SWE）代理，通过合成测试用例和扩展代理轨迹构建训练数据，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 当前LLM驱动的工具（如OpenAI Codex）在软件工程任务中缺乏高质量训练数据和有效测试用例，限制了其实际应用。

Method: 开发了合成测试用例的管道，并扩展代理轨迹以构建训练数据，从而训练SWE-Dev模型。

Result: 在SWE-bench-Verified基准测试中，SWE-Dev 7B和32B模型的成功率分别达到23.4%和36.6%，优于现有开源模型。

Conclusion: SWE-Dev通过改进数据和方法，显著提升了软件工程任务的自动化性能，代码和模型已开源。

Abstract: Large language models (LLMs) have advanced rapidly from conversational
problem solving to addressing real-world tasks involving tool use, such as
software engineering (SWE). Recent LLM-powered toolkits, such as OpenAI Codex
and Cursor, have offered end-to-end automation of the software development
process. However, building effective SWE agents remains challenging due to the
lack of high-quality training data and effective test cases. To address this
issue, we present SWE-Dev, an SWE agent built upon open-source LLMs. First, we
develop a robust pipeline to synthesize test cases for patch evaluation.
Second, we scale up agent trajectories to construct the training data for
building SWE-Dev. Experiments on the SWE-bench-Verified benchmark show that the
SWE-Dev models can achieve top performance among all open SWE agents.
Specifically, the success rates of the SWE-Dev 7B and 32B parameter models
reach 23.4% and 36.6%, respectively, outperforming state-of-the-art open-source
models. All code, models, and datasets are publicly available at
https://github.com/THUDM/SWE-Dev.

</details>


### [62] [MCPWorld: A Unified Benchmarking Testbed for API, GUI, and Hybrid Computer Use Agents](https://arxiv.org/abs/2506.07672)
*Yunhe Yan,Shihe Wang,Jiajun Du,Yexuan Yang,Yuxuan Shan,Qichen Qiu,Xianqing Jia,Xinge Wang,Xin Yuan,Xu Han,Mao Qin,Yinxiao Chen,Chen Peng,Shangguang Wang,Mengwei Xu*

Main category: cs.AI

TL;DR: MCPWorld是首个针对API、GUI及混合代理的自动CUA测试平台，通过白盒应用和动态代码检测提供鲁棒评估。


<details>
  <summary>Details</summary>
Motivation: 现有CUA基准主要针对GUI代理，易受UI变化影响且忽略API功能交互，需更全面的评估方法。

Method: 提出MCPWorld，利用白盒应用和动态代码检测，支持API、GUI及混合代理的自动化测试。

Result: 实验显示LLM驱动的CUA框架任务完成准确率达75.12%，验证了MCP的实用性。

Conclusion: MCPWorld有望推动下一代CUA的标准化评测，支持丰富外部工具利用。

Abstract: (M)LLM-powered computer use agents (CUA) are emerging as a transformative
technique to automate human-computer interaction. However, existing CUA
benchmarks predominantly target GUI agents, whose evaluation methods are
susceptible to UI changes and ignore function interactions exposed by
application APIs, e.g., Model Context Protocol (MCP). To this end, we propose
MCPWorld, the first automatic CUA testbed for API, GUI, and API-GUI hybrid
agents. A key principle of MCPWorld is the use of "white-box apps", i.e., those
with source code availability and can be revised/re-compiled as needed (e.g.,
adding MCP support), with two notable advantages:
  (1) It greatly broadens the design space of CUA, such as what and how the app
features to be exposed/extracted as CUA-callable APIs.
  (2) It allows MCPWorld to programmatically verify task completion by directly
monitoring application behavior through techniques like dynamic code
instrumentation, offering robust, accurate CUA evaluation decoupled from
specific agent implementations or UI states.
  Currently, MCPWorld includes 201 well curated and annotated user tasks,
covering diversified use cases and difficulty levels. MCPWorld is also fully
containerized with GPU acceleration support for flexible adoption on different
OS/hardware environments. Our preliminary experiments, using a representative
LLM-powered CUA framework, achieve 75.12% task completion accuracy,
simultaneously providing initial evidence on the practical effectiveness of
agent automation leveraging MCP. Overall, we anticipate MCPWorld to facilitate
and standardize the benchmarking of next-generation computer use agents that
can leverage rich external tools. Our code and dataset are publicly available
at https://github.com/SAAgent/MCPWorld.

</details>


### [63] [NeurIPS 2025 E2LM Competition : Early Training Evaluation of Language Models](https://arxiv.org/abs/2506.07731)
*Mouadh Yagoubi,Yasser Dahou,Billel Mokeddem,Younes Belkada,Phuc H. Le-Khac,Basma El Amel Boussaha,Reda Alami,Jingwei Zuo,Damiano Marsili,Mugariya Farooq,Mounia Lalmas,Georgia Gkioxari,Patrick Gallinari,Philip Torr,Hakim Hacid*

Main category: cs.AI

TL;DR: 论文探讨了现有基准测试在小模型早期训练阶段的局限性，提出了一项竞赛，旨在设计专门用于评估语言模型早期训练进度的科学知识任务。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试在评估完全训练的大模型时有效，但在小模型早期训练阶段表现不佳，缺乏区分性信号。

Method: 通过竞赛形式，邀请参与者开发或调整评估方法，利用提供的预训练小模型和中间检查点进行实验。

Result: 竞赛提供了三种预训练小模型和中间检查点，支持在免费云GPU平台上运行实验，评估标准包括信号质量、模型排名一致性和科学知识相关性。

Conclusion: 该竞赛旨在推动早期训练阶段的定制化评估策略设计，使基础LLM研究从模型开发初期更具系统性和基准指导性。

Abstract: Existing benchmarks have proven effective for assessing the performance of
fully trained large language models. However, we find striking differences in
the early training stages of small models, where benchmarks often fail to
provide meaningful or discriminative signals. To explore how these differences
arise, this competition tackles the challenge of designing scientific knowledge
evaluation tasks specifically tailored for measuring early training progress of
language models. Participants are invited to develop novel evaluation
methodologies or adapt existing benchmarks to better capture performance
differences among language models. To support this effort, we provide three
pre-trained small models (0.5B, 1B, and 3B parameters), along with intermediate
checkpoints sampled during training up to 200B tokens. All experiments and
development work can be run on widely available free cloud-based GPU platforms,
making participation accessible to researchers with limited computational
resources. Submissions will be evaluated based on three criteria: the quality
of the performance signal they produce, the consistency of model rankings at 1
trillion tokens of training, and their relevance to the scientific knowledge
domain. By promoting the design of tailored evaluation strategies for early
training, this competition aims to attract a broad range of participants from
various disciplines, including those who may not be machine learning experts or
have access to dedicated GPU resources. Ultimately, this initiative seeks to
make foundational LLM research more systematic and benchmark-informed from the
earliest phases of model development.

</details>


### [64] [RSafe: Incentivizing proactive reasoning to build robust and adaptive LLM safeguards](https://arxiv.org/abs/2506.07736)
*Jingnan Zheng,Xiangtian Ji,Yijun Lu,Chenhang Cui,Weixiang Zhao,Gelei Deng,Zhenkai Liang,An Zhang,Tat-Seng Chua*

Main category: cs.AI

TL;DR: RSafe是一种基于自适应推理的安全保障方法，通过两阶段训练（引导推理和强化对齐）提升对未知或对抗性安全威胁的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 尽管大型语言模型（LLMs）经过安全对齐，但仍存在漏洞，现有防护模型依赖人工标注数据且难以应对分布外威胁。

Method: RSafe采用两阶段方法：1）引导推理，通过策略引导逐步分析输入内容的安全风险；2）强化对齐，用基于规则的强化学习优化推理路径。

Result: RSafe能够内化安全原则，泛化对未知或对抗性安全威胁的保护能力。

Conclusion: RSafe通过用户指定的安全策略提供定制化保护，解决了现有防护模型的局限性。

Abstract: Large Language Models (LLMs) continue to exhibit vulnerabilities despite
deliberate safety alignment efforts, posing significant risks to users and
society. To safeguard against the risk of policy-violating content,
system-level moderation via external guard models-designed to monitor LLM
inputs and outputs and block potentially harmful content-has emerged as a
prevalent mitigation strategy. Existing approaches of training guard models
rely heavily on extensive human curated datasets and struggle with
out-of-distribution threats, such as emerging harmful categories or jailbreak
attacks. To address these limitations, we propose RSafe, an adaptive
reasoning-based safeguard that conducts guided safety reasoning to provide
robust protection within the scope of specified safety policies. RSafe operates
in two stages: 1) guided reasoning, where it analyzes safety risks of input
content through policy-guided step-by-step reasoning, and 2) reinforced
alignment, where rule-based RL optimizes its reasoning paths to align with
accurate safety prediction. This two-stage training paradigm enables RSafe to
internalize safety principles to generalize safety protection capability over
unseen or adversarial safety violation scenarios. During inference, RSafe
accepts user-specified safety policies to provide enhanced safeguards tailored
to specific safety requirements.

</details>


### [65] [REMoH: A Reflective Evolution of Multi-objective Heuristics approach via Large Language Models](https://arxiv.org/abs/2506.07759)
*Diego Forniés-Tabuenca,Alejandro Uribe,Urtzi Otamendi,Arkaitz Artetxe,Juan Carlos Rivera,Oier Lopez de Lacalle*

Main category: cs.AI

TL;DR: 论文提出了一种结合NSGA-II和LLM的新型多目标启发式框架REMoH，通过反射机制提升启发式生成的质量和多样性，在FJSSP问题上表现优异。


<details>
  <summary>Details</summary>
Motivation: 传统多目标优化算法需要大量问题建模且难以适应非线性结构，LLM的引入可增强解释性、适应性和推理能力。

Method: REMoH框架整合NSGA-II与LLM启发式生成，利用聚类和搜索空间反射机制生成多样且高质量的启发式。

Result: 在FJSSP问题上的实验表明，REMoH在减少建模工作量的同时，取得了与现有最优方法竞争的结果。

Conclusion: LLM能有效增强传统优化方法，在多目标场景中提供更高的灵活性、可解释性和鲁棒性。

Abstract: Multi-objective optimization is fundamental in complex decision-making tasks.
Traditional algorithms, while effective, often demand extensive
problem-specific modeling and struggle to adapt to nonlinear structures. Recent
advances in Large Language Models (LLMs) offer enhanced explainability,
adaptability, and reasoning. This work proposes Reflective Evolution of
Multi-objective Heuristics (REMoH), a novel framework integrating NSGA-II with
LLM-based heuristic generation. A key innovation is a reflection mechanism that
uses clustering and search-space reflection to guide the creation of diverse,
high-quality heuristics, improving convergence and maintaining solution
diversity. The approach is evaluated on the Flexible Job Shop Scheduling
Problem (FJSSP) in-depth benchmarking against state-of-the-art methods using
three instance datasets: Dauzere, Barnes, and Brandimarte. Results demonstrate
that REMoH achieves competitive results compared to state-of-the-art approaches
with reduced modeling effort and enhanced adaptability. These findings
underscore the potential of LLMs to augment traditional optimization, offering
greater flexibility, interpretability, and robustness in multi-objective
scenarios.

</details>


### [66] [A Proposal to Extend the Common Model of Cognition with Metacognition](https://arxiv.org/abs/2506.07807)
*John Laird,Christian Lebiere,Paul Rosenbloom,Andrea Stocco,Robert Wray*

Main category: cs.AI

TL;DR: 论文提出了一种在通用认知模型（CMC）中整合元认知的统一方法，通过在工作记忆中显式表示代理的认知能力和过程来实现。


<details>
  <summary>Details</summary>
Motivation: 动机是扩展CMC以支持元认知功能，同时最小化对现有结构的修改。

Method: 方法是在工作记忆中显式表示认知能力和过程，并利用CMC的现有能力进行推理。

Result: 结果展示了如何在提案中实现元认知的具体例子。

Conclusion: 结论表明该方法成功地将元认知整合到CMC中，且无需大幅修改现有结构。

Abstract: The Common Model of Cognition (CMC) provides an abstract characterization of
the structure and processing required by a cognitive architecture for
human-like minds. We propose a unified approach to integrating metacognition
within the CMC. We propose that metacognition involves reasoning over explicit
representations of an agent's cognitive capabilities and processes in working
memory. Our proposal exploits the existing cognitive capabilities of the CMC,
making minimal extensions in the structure and information available within
working memory. We provide examples of metacognition within our proposal.

</details>


### [67] [Guideline Forest: Experience-Induced Multi-Guideline Reasoning with Stepwise Aggregation](https://arxiv.org/abs/2506.07820)
*Jiaxiang CHen,Zhuo Wang,Mingxi Zou,Qifan Wang,Zenglin Xu*

Main category: cs.AI

TL;DR: Guideline Forest 是一个通过从已验证示例中提取结构化推理策略（指南）来增强大语言模型（LLM）推理能力的框架，支持并行执行和逐步聚合，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 人类推理灵活且基于经验，而现有方法未能有效利用多样化的可重用策略。

Method: 通过从已验证示例中提取可重用指南，并扩展为多样化变体，支持并行执行和自我修正，逐步聚合结果。

Result: 在数学和编程推理的四个基准测试中，Guideline Forest 表现优于 CoT、ReAct 等基线方法。

Conclusion: Guideline Forest 展示了多路径推理和逐步聚合的有效性，具有适应性和泛化潜力。

Abstract: Human reasoning is flexible, adaptive, and grounded in prior
experience-qualities that large language models (LLMs) still struggle to
emulate. Existing methods either explore diverse reasoning paths at inference
time or search for optimal workflows through expensive operations, but both
fall short in leveraging multiple reusable strategies in a structured,
efficient manner. We propose Guideline Forest, a framework that enhances LLMs
reasoning by inducing structured reasoning strategies-called guidelines-from
verified examples and executing them via step-wise aggregation. Unlike
test-time search or single-path distillation, our method draws on verified
reasoning experiences by inducing reusable guidelines and expanding each into
diverse variants. Much like human reasoning, these variants reflect alternative
thought patterns, are executed in parallel, refined via self-correction, and
aggregated step by step-enabling the model to adaptively resolve uncertainty
and synthesize robust solutions.We evaluate Guideline Forest on four
benchmarks-GSM8K, MATH-500, MBPP, and HumanEval-spanning mathematical and
programmatic reasoning. Guideline Forest consistently outperforms strong
baselines, including CoT, ReAct, ToT, FoT, and AFlow. Ablation studies further
highlight the effectiveness of multi-path reasoning and stepwise aggregation,
underscoring the Guideline Forest's adaptability and generalization potential.

</details>


### [68] [Addition in Four Movements: Mapping Layer-wise Information Trajectories in LLMs](https://arxiv.org/abs/2506.07824)
*Yao Yan*

Main category: cs.AI

TL;DR: 论文通过线性探测和logit-lens检查，分析了LLaMA-3-8B-Instruct模型进行多位数加法运算的内部过程，揭示了其分阶段的计算轨迹。


<details>
  <summary>Details</summary>
Motivation: 探究大语言模型在多位数加法运算中的计算能力，揭示其内部算术过程的层次性。

Method: 结合线性探测和logit-lens检查，分析模型前向传播中的四阶段计算轨迹。

Result: 模型的计算过程分为四个阶段：公式结构表示、核心计算特征、数值抽象和最终输出生成，表明其倾向于内部计算而非机械记忆。

Conclusion: 研究揭示了模型进行加法运算的层次性过程，支持其计算能力的内部机制，并公开了代码和数据以促进可重复性。

Abstract: Multi-digit addition is a clear probe of the computational power of large
language models. To dissect the internal arithmetic processes in
LLaMA-3-8B-Instruct, we combine linear probing with logit-lens inspection.
Inspired by the step-by-step manner in which humans perform addition, we
propose and analyze a coherent four-stage trajectory in the forward
pass:Formula-structure representations become linearly decodable first, while
the answer token is still far down the candidate list.Core computational
features then emerge prominently.At deeper activation layers, numerical
abstractions of the result become clearer, enabling near-perfect detection and
decoding of the individual digits in the sum.Near the output, the model
organizes and generates the final content, with the correct token reliably
occupying the top rank.This trajectory suggests a hierarchical process that
favors internal computation over rote memorization. We release our code and
data to facilitate reproducibility.

</details>


### [69] [HAIBU-ReMUD: Reasoning Multimodal Ultrasound Dataset and Model Bridging to General Specific Domains](https://arxiv.org/abs/2506.07837)
*Shijie Wang,Yilun Zhang,Zeyu Lai,Dexing Kong*

Main category: cs.AI

TL;DR: 本文提出了一种针对特定领域（如医学超声）的图像-文本推理监督微调数据生成方法，构建了ReMUD数据集，并展示了其在领域内优于通用MLLMs的性能。


<details>
  <summary>Details</summary>
Motivation: 解决多模态大语言模型（MLLMs）在特定领域（如医学超声）因缺乏领域数据而表现不佳的问题。

Method: 提出图像-文本推理监督微调数据生成流程，从领域材料中生成四元组（图像、问题、思考轨迹、答案），构建ReMUD数据集。

Result: ReMUD-7B模型在医学超声领域表现优于通用MLLMs，数据集包含超过45,000条数据。

Conclusion: ReMUD解决了特定领域MLLMs的数据短缺问题，相关资源将开源以促进研究。

Abstract: Multimodal large language models (MLLMs) have shown great potential in
general domains but perform poorly in some specific domains due to a lack of
domain-specific data, such as image-text data or vedio-text data. In some
specific domains, there is abundant graphic and textual data scattered around,
but lacks standardized arrangement. In the field of medical ultrasound, there
are ultrasonic diagnostic books, ultrasonic clinical guidelines, ultrasonic
diagnostic reports, and so on. However, these ultrasonic materials are often
saved in the forms of PDF, images, etc., and cannot be directly used for the
training of MLLMs. This paper proposes a novel image-text reasoning supervised
fine-tuning data generation pipeline to create specific domain quadruplets
(image, question, thinking trace, and answer) from domain-specific materials. A
medical ultrasound domain dataset ReMUD is established, containing over 45,000
reasoning and non-reasoning supervised fine-tuning Question Answering (QA) and
Visual Question Answering (VQA) data. The ReMUD-7B model, fine-tuned on
Qwen2.5-VL-7B-Instruct, outperforms general-domain MLLMs in medical ultrasound
field. To facilitate research, the ReMUD dataset, data generation codebase, and
ReMUD-7B parameters will be released at https://github.com/ShiDaizi/ReMUD,
addressing the data shortage issue in specific domain MLLMs.

</details>


### [70] [A Temporal FRBR/FRBRoo-Based Model for Component-Level Versioning of Legal Norms](https://arxiv.org/abs/2506.07853)
*Hudson de Martim*

Main category: cs.AI

TL;DR: 本文提出了一种扩展FRBRoo框架的结构化时间模型，用于精确跟踪法律规范的层次组件（如条款、段落）的时间演变，解决了现有模型在细粒度版本控制上的不足。


<details>
  <summary>Details</summary>
Motivation: 现有法律文档建模框架（如FRBR/FRBRoo和Akoma Ntoso）缺乏对组件级版本的原生支持，限制了法律文本的时间点重构能力，影响了法律技术和AI应用的可靠性。

Method: 通过扩展FRBRoo框架，引入Temporal Version (TV)和Language Version (LV)等子类，以及Component Work (CW)、Component Temporal Version (CTV)和Component Language Version (CLV)等概念，实现法律规范的细粒度时间跟踪。

Result: 以巴西联邦宪法为例，模型成功展示了如何通过创建新的Component Temporal Versions来记录法律条款的修改，同时保留未受影响组件的现有版本。

Conclusion: 该模型为开发高级法律信息系统、知识图谱和AI工具提供了坚实基础，能够支持精确的历史分析和影响评估，克服了现有生成模型的局限性。

Abstract: Effectively representing legal norms for automated processing is a critical
challenge, particularly in tracking the diachronic evolution of their
hierarchical components (e.g., articles, paragraphs). While foundational
frameworks like FRBR/FRBRoo and standards like Akoma Ntoso model legal
documents at a macro level, they lack native mechanisms for granular,
component-level versioning. This limitation hinders the deterministic
point-in-time reconstruction of legal texts, a fundamental capability for
reliable Legal Tech and AI applications. This paper proposes a structured,
temporal model that extends the FRBRoo framework to address this gap. It
introduces specialized subclasses of Expressio - Temporal Version (TV) and
Language Version (LV - to represent the state of a legal norm and its
linguistic variations at specific points in time. The model applies this same
paradigm hierarchically, introducing Component Work (CW), Component Temporal
Version (CTV), and Component Language Version (CLV) to track the lifecycle of
individual articles, paragraphs, and clauses. Using the Brazilian Federal
Constitution as a case study, the paper demonstrates how each amendment creates
new Component Temporal Versions for affected provisions, while unaffected
components retain their existing versions. This fine-grained, time-aware
architecture enables the precise, deterministic retrieval and reconstruction of
any part of a legal text as it existed on a specific date. The model provides a
robust foundation for developing advanced legal information systems, knowledge
graphs, and AI tools capable of accurate historical analysis and impact
assessment, overcoming the limitations of current generative models.

</details>


### [71] [Evaluating Large Language Models on the Frame and Symbol Grounding Problems: A Zero-shot Benchmark](https://arxiv.org/abs/2506.07896)
*Shoko Oka*

Main category: cs.AI

TL;DR: 现代大语言模型（LLMs）能否解决传统符号AI无法解决的框架问题和符号接地问题？研究发现，部分封闭模型表现优异。


<details>
  <summary>Details</summary>
Motivation: 探讨现代LLMs是否具备解决框架问题和符号接地问题的认知能力。

Method: 设计两个基准任务，在零样本条件下测试13个LLMs，评估其输出的质量。

Result: 开源模型表现参差不齐，部分封闭模型表现稳定且高分。

Conclusion: 部分现代LLMs可能具备解决这些理论挑战的能力。

Abstract: Recent advancements in large language models (LLMs) have revitalized
philosophical debates surrounding artificial intelligence. Two of the most
fundamental challenges - namely, the Frame Problem and the Symbol Grounding
Problem - have historically been viewed as unsolvable within traditional
symbolic AI systems. This study investigates whether modern LLMs possess the
cognitive capacities required to address these problems. To do so, I designed
two benchmark tasks reflecting the philosophical core of each problem,
administered them under zero-shot conditions to 13 prominent LLMs (both closed
and open-source), and assessed the quality of the models' outputs across five
trials each. Responses were scored along multiple criteria, including
contextual reasoning, semantic coherence, and information filtering. The
results demonstrate that while open-source models showed variability in
performance due to differences in model size, quantization, and instruction
tuning, several closed models consistently achieved high scores. These findings
suggest that select modern LLMs may be acquiring capacities sufficient to
produce meaningful and stable responses to these long-standing theoretical
challenges.

</details>


### [72] [LUCIFER: Language Understanding and Context-Infused Framework for Exploration and Behavior Refinement](https://arxiv.org/abs/2506.07915)
*Dimitris Panagopoulos,Adolfo Perrusquia,Weisi Guo*

Main category: cs.AI

TL;DR: LUCIFER框架通过结合分层决策架构、强化学习和大型语言模型，解决了动态环境中自主决策的局限性，利用人类上下文知识提升探索效率和决策质量。


<details>
  <summary>Details</summary>
Motivation: 动态环境中，现有环境知识的快速过时导致自主决策效果受限，而人类上下文知识虽丰富但难以转化为系统可用的信息。

Method: 提出LUCIFER框架，整合分层决策、强化学习和LLMs，LLMs在框架中扮演上下文提取器和零样本探索引导者双重角色。

Result: LUCIFER在探索效率和决策质量上优于传统方法，展示了上下文驱动决策的潜力。

Conclusion: LUCIFER框架成功地将人类上下文知识与自主系统结合，为动态环境中的决策提供了新思路。

Abstract: In dynamic environments, the rapid obsolescence of pre-existing environmental
knowledge creates a gap between an agent's internal model and the evolving
reality of its operational context. This disparity between prior and updated
environmental valuations fundamentally limits the effectiveness of autonomous
decision-making. To bridge this gap, the contextual bias of human domain
stakeholders, who naturally accumulate insights through direct, real-time
observation, becomes indispensable. However, translating their nuanced, and
context-rich input into actionable intelligence for autonomous systems remains
an open challenge. To address this, we propose LUCIFER (Language Understanding
and Context-Infused Framework for Exploration and Behavior Refinement), a
domain-agnostic framework that integrates a hierarchical decision-making
architecture with reinforcement learning (RL) and large language models (LLMs)
into a unified system. This architecture mirrors how humans decompose complex
tasks, enabling a high-level planner to coordinate specialised sub-agents, each
focused on distinct objectives and temporally interdependent actions. Unlike
traditional applications where LLMs are limited to single role, LUCIFER
integrates them in two synergistic roles: as context extractors, structuring
verbal stakeholder input into domain-aware representations that influence
decision-making through an attention space mechanism aligning LLM-derived
insights with the agent's learning process, and as zero-shot exploration
facilitators guiding the agent's action selection process during exploration.
We benchmark various LLMs in both roles and demonstrate that LUCIFER improves
exploration efficiency and decision quality, outperforming flat,
goal-conditioned policies. Our findings show the potential of context-driven
decision-making, where autonomous systems leverage human contextual knowledge
for operational success.

</details>


### [73] [Solving Inequality Proofs with Large Language Models](https://arxiv.org/abs/2506.07927)
*Jiayi Sheng,Luna Lyu,Jikai Jin,Tony Xia,Alex Gu,James Zou,Pan Lu*

Main category: cs.AI

TL;DR: 论文提出了一种非正式但可验证的不等式证明任务框架，并发布了IneqMath数据集，揭示了当前大型语言模型在严格证明能力上的不足。


<details>
  <summary>Details</summary>
Motivation: 不等式证明是数学和科学领域的重要技能，但现有数据集稀缺且形式化，限制了大型语言模型在这一领域的发展。

Method: 将不等式证明分解为两个可自动检查的子任务：边界估计和关系预测，并开发了IneqMath数据集和LLM-as-judge评估框架。

Result: 评估29个领先的大型语言模型发现，即使顶级模型在逐步审查下的准确率也低于10%，揭示了其在严格证明能力上的不足。

Conclusion: 研究指出了定理引导推理和自我优化等有前景的研究方向，以提升大型语言模型在不等式证明中的表现。

Abstract: Inequality proving, crucial across diverse scientific and mathematical
fields, tests advanced reasoning skills such as discovering tight bounds and
strategic theorem application. This makes it a distinct, demanding frontier for
large language models (LLMs), offering insights beyond general mathematical
problem-solving. Progress in this area is hampered by existing datasets that
are often scarce, synthetic, or rigidly formal. We address this by proposing an
informal yet verifiable task formulation, recasting inequality proving into two
automatically checkable subtasks: bound estimation and relation prediction.
Building on this, we release IneqMath, an expert-curated dataset of
Olympiad-level inequalities, including a test set and training corpus enriched
with step-wise solutions and theorem annotations. We also develop a novel
LLM-as-judge evaluation framework, combining a final-answer judge with four
step-wise judges designed to detect common reasoning flaws. A systematic
evaluation of 29 leading LLMs on IneqMath reveals a surprising reality: even
top models like o1 achieve less than 10% overall accuracy under step-wise
scrutiny; this is a drop of up to 65.5% from their accuracy considering only
final answer equivalence. This discrepancy exposes fragile deductive chains and
a critical gap for current LLMs between merely finding an answer and
constructing a rigorous proof. Scaling model size and increasing test-time
computation yield limited gains in overall proof correctness. Instead, our
findings highlight promising research directions such as theorem-guided
reasoning and self-refinement. Code and data are available at
https://ineqmath.github.io/.

</details>


### [74] [Gradients: When Markets Meet Fine-tuning -- A Distributed Approach to Model Optimisation](https://arxiv.org/abs/2506.07940)
*Christopher Subia-Waud*

Main category: cs.AI

TL;DR: Gradients是一个去中心化的AutoML平台，通过竞争市场机制优化超参数配置，显著优于传统集中式方法。


<details>
  <summary>Details</summary>
Motivation: 现有AutoML平台依赖单一优化策略，仅探索部分超参数配置，限制了性能提升。

Method: Gradients将超参数优化转化为竞争市场，独立矿工通过经济激励竞争发现最优配置。

Result: 在180个实验中，Gradients以82.8%的胜率优于HuggingFace AutoTrain，对复杂任务提升30-40%。

Conclusion: 经济驱动的竞争方法能系统性发现集中式AutoML遗漏的优越配置。

Abstract: Foundation model fine-tuning faces a fundamental challenge: existing AutoML
platforms rely on single optimisation strategies that explore only a fraction
of viable hyperparameter configurations. In this white paper, We introduce
Gradients, a decentralised AutoML platform that transforms hyperparameter
optimisation into a competitive marketplace where independent miners compete to
discover optimal configurations. Economic incentives align individual
exploration with collective optimisation goals, driving systematic
investigation of hyperparameter regions that centralised methods miss. We
evaluate our approach across 180 controlled experiments spanning diverse model
architectures (70M to 70B parameters) and task types. Gradients achieves an
82.8\% win rate against HuggingFace AutoTrain and 100\% against TogetherAI,
Databricks, and Google Cloud, with mean improvements of 11.8\% and 42.1\%
respectively. Complex reasoning and retrieval tasks show particularly strong
gains of 30-40\%, whilst diffusion models achieve 23.4\% improvements for
person-specific generation. These results demonstrate that competitive,
economically-driven approaches can systematically discover superior
configurations that centralised AutoML consistently miss.

</details>


### [75] [Reinforcing Multimodal Understanding and Generation with Dual Self-rewards](https://arxiv.org/abs/2506.07963)
*Jixiang Hong,Yiran Zhang,Guanzhong Wang,Yi Liu,Ji-Rong Wen,Rui Yan*

Main category: cs.AI

TL;DR: 本文提出了一种自监督的双奖励机制，通过理解与生成的逆对偶任务，提升大型多模态模型的性能，无需外部监督。


<details>
  <summary>Details</summary>
Motivation: 当前大型多模态模型在图像-文本对齐上表现不佳，且现有方法依赖外部监督且仅解决单向任务。

Method: 通过采样多输出并反转输入-输出对，计算对偶似然作为自奖励进行优化。

Result: 实验表明，该方法显著提升了模型在视觉理解和生成任务中的表现，尤其在文本到图像任务中效果突出。

Conclusion: 自监督双奖励机制有效提升了模型性能，无需外部监督。

Abstract: Building upon large language models (LLMs), recent large multimodal models
(LMMs) unify cross-model understanding and generation into a single framework.
However, LMMs still struggle to achieve accurate image-text alignment, prone to
generating text responses contradicting the visual input or failing to follow
the text-to-image prompts. Current solutions require external supervision
(e.g., human feedback or reward models) and only address unidirectional
tasks-either understanding or generation. In this work, based on the
observation that understanding and generation are inverse dual tasks, we
introduce a self-supervised dual reward mechanism to reinforce the
understanding and generation capabilities of LMMs. Specifically, we sample
multiple outputs for a given input in one task domain, then reverse the
input-output pairs to compute the dual likelihood of the model as self-rewards
for optimization. Extensive experimental results on visual understanding and
generation benchmarks demonstrate that our method can effectively enhance the
performance of the model without any external supervision, especially achieving
remarkable improvements in text-to-image tasks.

</details>


### [76] [$τ^2$-Bench: Evaluating Conversational Agents in a Dual-Control Environment](https://arxiv.org/abs/2506.07982)
*Victor Barres,Honghua Dong,Soham Ray,Xujie Si,Karthik Narasimhan*

Main category: cs.AI

TL;DR: 论文提出了$	au^2$-bench，一个双控制对话AI基准测试，模拟真实场景中用户和AI共同操作动态环境的情况。


<details>
  <summary>Details</summary>
Motivation: 现有对话AI基准测试仅模拟单控制环境，与真实场景（如技术支持）中用户需主动参与修改共享世界状态的情况不符。

Method: 1) 提出电信双控制领域模型（Dec-POMDP）；2) 设计任务生成器；3) 开发可靠用户模拟器；4) 多维度性能分析。

Result: 实验显示，从无用户到双控制时，AI性能显著下降，突显引导用户的挑战。

Conclusion: $	au^2$-bench为需有效推理和引导用户操作的AI提供了可控测试平台。

Abstract: Existing benchmarks for conversational AI agents simulate single-control
environments, where only the AI agent can use tools to interact with the world,
while the user remains a passive information provider. This differs from
real-world scenarios like technical support, where users need to actively
participate in modifying the state of the (shared) world. In order to address
this gap, we introduce $\tau^2$-bench, with four key contributions:
  1) A novel Telecom dual-control domain modeled as a Dec-POMDP, where both
agent and user make use of tools to act in a shared, dynamic environment that
tests both agent coordination and communication,
  2) A compositional task generator that programmatically creates diverse,
verifiable tasks from atomic components, ensuring domain coverage and
controlled complexity,
  3) A reliable user simulator tightly coupled with the environment, whose
behavior is constrained by tools and observable states, improving simulation
fidelity,
  4) Fine-grained analysis of agent performance through multiple ablations
including separating errors arising from reasoning vs
communication/coordination.
  In particular, our experiments show significant performance drops when agents
shift from no-user to dual-control, highlighting the challenges of guiding
users. Overall, $\tau^2$-bench provides a controlled testbed for agents that
must both reason effectively and guide user actions.

</details>


### [77] [GUI-Reflection: Empowering Multimodal GUI Models with Self-Reflection Behavior](https://arxiv.org/abs/2506.08012)
*Penghao Wu,Shengnan Ma,Bo Wang,Jiaheng Yu,Lewei Lu,Ziwei Liu*

Main category: cs.AI

TL;DR: 提出GUI-Reflection框架，通过自动生成数据和多阶段训练，赋予多模态GUI模型自我反思和错误纠正能力。


<details>
  <summary>Details</summary>
Motivation: 现有GUI模型依赖无错误的离线轨迹，缺乏反思和错误恢复能力，限制了其鲁棒性和适应性。

Method: 1) 自动构建反思和纠错数据；2) 设计GUI-Reflection任务套件；3) 构建移动设备在线训练环境；4) 提出迭代在线反思调优算法。

Result: 框架成功赋予GUI代理自我反思和纠正能力，提升了GUI自动化的鲁棒性和智能性。

Conclusion: GUI-Reflection为更强大、适应性更强的GUI自动化铺平了道路，相关数据和工具将公开。

Abstract: Multimodal Large Language Models (MLLMs) have shown great potential in
revolutionizing Graphical User Interface (GUI) automation. However, existing
GUI models mostly rely on learning from nearly error-free offline trajectories,
thus lacking reflection and error recovery capabilities. To bridge this gap, we
propose GUI-Reflection, a novel framework that explicitly integrates
self-reflection and error correction capabilities into end-to-end multimodal
GUI models throughout dedicated training stages: GUI-specific pre-training,
offline supervised fine-tuning (SFT), and online reflection tuning.
GUI-reflection enables self-reflection behavior emergence with fully automated
data generation and learning processes without requiring any human annotation.
Specifically, 1) we first propose scalable data pipelines to automatically
construct reflection and error correction data from existing successful
trajectories. While existing GUI models mainly focus on grounding and UI
understanding ability, we propose the GUI-Reflection Task Suite to learn and
evaluate reflection-oriented abilities explicitly. 2) Furthermore, we built a
diverse and efficient environment for online training and data collection of
GUI models on mobile devices. 3) We also present an iterative online reflection
tuning algorithm leveraging the proposed environment, enabling the model to
continuously enhance its reflection and error correction abilities. Our
framework equips GUI agents with self-reflection and correction capabilities,
paving the way for more robust, adaptable, and intelligent GUI automation, with
all data, models, environments, and tools to be released publicly.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [78] [How Significant Are the Real Performance Gains? An Unbiased Evaluation Framework for GraphRAG](https://arxiv.org/abs/2506.06331)
*Qiming Zeng,Xiao Yan,Hao Luo,Yuhao Lin,Yuxiang Wang,Fangcheng Fu,Bo Du,Quanqing Xu,Jiawei Jiang*

Main category: cs.CL

TL;DR: GraphRAG方法通过知识图谱增强LLM生成答案的质量，但现有评估框架存在无关问题和评估偏差问题。本文提出了一种无偏评估框架，发现GraphRAG的性能提升比之前报道的更温和。


<details>
  <summary>Details</summary>
Motivation: 现有GraphRAG评估框架存在无关问题和评估偏差，可能导致性能结论错误，需要更科学的评估方法。

Method: 提出基于图-文本的问题生成方法和无偏评估流程，用于生成相关问题和消除LLM评估偏差。

Result: 应用新框架评估3种GraphRAG方法，发现其性能提升较之前报道更为温和。

Conclusion: 新框架虽可能有不足，但呼吁科学评估为GraphRAG研究奠定基础。

Abstract: By retrieving contexts from knowledge graphs, graph-based retrieval-augmented
generation (GraphRAG) enhances large language models (LLMs) to generate quality
answers for user questions. Many GraphRAG methods have been proposed and
reported inspiring performance in answer quality. However, we observe that the
current answer evaluation framework for GraphRAG has two critical flaws, i.e.,
unrelated questions and evaluation biases, which may lead to biased or even
wrong conclusions on performance. To tackle the two flaws, we propose an
unbiased evaluation framework that uses graph-text-grounded question generation
to produce questions that are more related to the underlying dataset and an
unbiased evaluation procedure to eliminate the biases in LLM-based answer
assessment. We apply our unbiased framework to evaluate 3 representative
GraphRAG methods and find that their performance gains are much more moderate
than reported previously. Although our evaluation framework may still have
flaws, it calls for scientific evaluations to lay solid foundations for
GraphRAG research.

</details>


### [79] [TESU-LLM: Training Speech-LLMs Without Speech via Unified Encoder Alignment](https://arxiv.org/abs/2506.06343)
*Taesoo Kim,Jong Hwan Ko*

Main category: cs.CL

TL;DR: TESU-LLM是一种仅使用文本数据训练语音语言模型的新框架，通过统一编码器将文本和语音映射到共享潜在空间，无需大规模语音数据即可实现高性能。


<details>
  <summary>Details</summary>
Motivation: 现有语音语言模型依赖大规模语音-文本配对数据和计算资源，限制了可扩展性和可访问性。

Method: 采用统一编码器将文本和语音映射到共享潜在空间，并通过轻量级投影网络对齐LLM的嵌入空间，实现从文本监督到语音推理的泛化。

Result: TESU-LLM在多个语音相关基准测试中表现优异，与依赖多模态数据和大量计算资源的基线方法相当。

Conclusion: TESU-LLM提供了一种无需语音数据的高效、可扩展的语音语言模型构建方法。

Abstract: Recent advances in speech-enabled language models have shown promising
results in building intelligent voice assistants. However, most existing
approaches rely on large-scale paired speech-text data and extensive
computational resources, which pose challenges in terms of scalability and
accessibility. In this paper, we present \textbf{TESU-LLM}, a novel framework
that enables training speech-capable language models using only text data. Our
key insight is to leverage a unified encoder that maps semantically equivalent
text and speech inputs to a shared latent space. By aligning the encoder output
with the embedding space of a LLM via a lightweight projection network, we
enable the model to generalize from text-only supervision to speech-based
inference. Despite being trained exclusively on text, TESU-LLM achieves strong
performance on various speech-related benchmarks, comparable to baseline
methods trained with large-scale multimodal datasets and substantial
computational resources. These results highlight the effectiveness and
efficiency of our approach, offering a scalable path toward building speech
LLMs without speech data.

</details>


### [80] [Unified Game Moderation: Soft-Prompting and LLM-Assisted Label Transfer for Resource-Efficient Toxicity Detection](https://arxiv.org/abs/2506.06347)
*Zachary Yang,Domenico Tullo,Reihaneh Rabbany*

Main category: cs.CL

TL;DR: 论文提出了一种软提示方法和LLM辅助标签转移框架，用于扩展多游戏和多语言的实时毒性检测系统，显著提升了计算效率和性能。


<details>
  <summary>Details</summary>
Motivation: 游戏社区中的毒性检测在多游戏和多语言环境下面临扩展和实时性的挑战，需要高效且可扩展的解决方案。

Method: 1. 引入软提示方法，使单一模型能处理多游戏；2. 开发基于GPT-4o-mini的LLM辅助标签转移框架，支持七种新语言。

Result: 在法语、德语、葡萄牙语和俄语上的宏F1分数为32.96%至58.88%，德语表现优于英语基准（45.39%）。生产环境中显著减少计算资源和维护成本。

Conclusion: 提出的方法在多游戏和多语言环境下实现了高效的毒性检测，为游戏社区提供了可扩展的解决方案。

Abstract: Toxicity detection in gaming communities faces significant scaling challenges
when expanding across multiple games and languages, particularly in real-time
environments where computational efficiency is crucial. We present two key
findings to address these challenges while building upon our previous work on
ToxBuster, a BERT-based real-time toxicity detection system. First, we
introduce a soft-prompting approach that enables a single model to effectively
handle multiple games by incorporating game-context tokens, matching the
performance of more complex methods like curriculum learning while offering
superior scalability. Second, we develop an LLM-assisted label transfer
framework using GPT-4o-mini to extend support to seven additional languages.
Evaluations on real game chat data across French, German, Portuguese, and
Russian achieve macro F1-scores ranging from 32.96% to 58.88%, with
particularly strong performance in German, surpassing the English benchmark of
45.39%. In production, this unified approach significantly reduces
computational resources and maintenance overhead compared to maintaining
separate models for each game and language combination. At Ubisoft, this model
successfully identifies an average of 50 players, per game, per day engaging in
sanctionable behavior.

</details>


### [81] [Relationship Detection on Tabular Data Using Statistical Analysis and Large Language Models](https://arxiv.org/abs/2506.06371)
*Panagiotis Koletsis,Christos Panagiotopoulos,Georgios Th. Papadopoulos,Vasilis Efthymiou*

Main category: cs.CL

TL;DR: 该论文提出了一种混合方法，结合知识图谱（KG）和大型语言模型（LLM），用于检测未标记表格数据中列之间的关系（CPA任务）。通过统计分析和模块化设计减少搜索空间，实验表明该方法在基准数据集上具有竞争力。


<details>
  <summary>Details</summary>
Motivation: 表格解释任务的重要性日益凸显，但现有方法在处理未标记数据时存在挑战。论文旨在通过结合KG和LLM，提高关系检测的效率和准确性。

Method: 采用混合方法，结合KG和LLM，通过域和范围约束检测、关系共现分析等模块减少搜索空间。实验评估了不同量化水平和提示技术的LLM效果。

Result: 在SemTab挑战提供的基准数据集上，该方法表现优异，与现有先进方法竞争。

Conclusion: 提出的方法有效且实用，代码已开源，为表格关系检测提供了新思路。

Abstract: Over the past few years, table interpretation tasks have made significant
progress due to their importance and the introduction of new technologies and
benchmarks in the field. This work experiments with a hybrid approach for
detecting relationships among columns of unlabeled tabular data, using a
Knowledge Graph (KG) as a reference point, a task known as CPA. This approach
leverages large language models (LLMs) while employing statistical analysis to
reduce the search space of potential KG relations. The main modules of this
approach for reducing the search space are domain and range constraints
detection, as well as relation co-appearance analysis. The experimental
evaluation on two benchmark datasets provided by the SemTab challenge assesses
the influence of each module and the effectiveness of different
state-of-the-art LLMs at various levels of quantization. The experiments were
performed, as well as at different prompting techniques. The proposed
methodology, which is publicly available on github, proved to be competitive
with state-of-the-art approaches on these datasets.

</details>


### [82] [Enhancing Decision-Making of Large Language Models via Actor-Critic](https://arxiv.org/abs/2506.06376)
*Heng Dong,Kefei Duan,Chongjie Zhang*

Main category: cs.CL

TL;DR: 论文提出了一种基于LLM的Actor-Critic框架LAC，通过长期动作评估改进LLM策略，解决了现有方法在复杂决策中的局限性。


<details>
  <summary>Details</summary>
Motivation: LLM在复杂决策场景中长期推理和高层目标对齐方面存在挑战，现有方法效果不佳。

Method: LAC框架通过计算与正负结果相关的Q值来提取动作评估，并结合未来轨迹推演和推理，实现无梯度的策略改进。

Result: 实验表明，LAC在多种环境中表现优于现有方法，甚至在使用较小参数LLM时超越基于GPT-4的基线方法。

Conclusion: LAC展示了将结构化策略优化与LLM内在知识结合的潜力，可提升多步环境中的决策能力。

Abstract: Large Language Models (LLMs) have achieved remarkable advancements in natural
language processing tasks, yet they encounter challenges in complex
decision-making scenarios that require long-term reasoning and alignment with
high-level objectives. Existing methods either rely on short-term
auto-regressive action generation or face limitations in accurately simulating
rollouts and assessing outcomes, leading to sub-optimal decisions. This paper
introduces a novel LLM-based Actor-Critic framework, termed LAC, that
effectively improves LLM policies with long-term action evaluations in a
principled and scalable way. Our approach addresses two key challenges: (1)
extracting robust action evaluations by computing Q-values via token logits
associated with positive/negative outcomes, enhanced by future trajectory
rollouts and reasoning; and (2) enabling efficient policy improvement through a
gradient-free mechanism. Experiments across diverse environments -- including
high-level decision-making (ALFWorld), low-level action spaces (BabyAI-Text),
and large action spaces (WebShop) -- demonstrate the framework's generality and
superiority over state-of-the-art methods. Notably, our approach achieves
competitive performance using 7B/8B parameter LLMs, even outperforming baseline
methods employing GPT-4 in complex tasks. These results underscore the
potential of integrating structured policy optimization with LLMs' intrinsic
knowledge to advance decision-making capabilities in multi-step environments.

</details>


### [83] [Detection Method for Prompt Injection by Integrating Pre-trained Model and Heuristic Feature Engineering](https://arxiv.org/abs/2506.06384)
*Yi Ji,Runzhi Li,Baolei Mao*

Main category: cs.CL

TL;DR: 提出了一种双通道特征融合检测框架DMPI-PMHFE，结合预训练语言模型和启发式特征工程，有效检测提示注入攻击。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型（LLMs）的广泛应用，提示注入攻击成为重大安全威胁，现有防御机制在有效性和泛化性之间存在关键权衡。

Method: 采用DeBERTa-v3-base提取语义向量，并结合基于已知攻击模式的启发式规则提取显式结构特征，双通道特征融合后通过全连接神经网络进行预测。

Result: 在多个基准数据集上，DMPI-PMHFE在准确率、召回率和F1分数上优于现有方法，实际部署中显著降低了主流LLMs的攻击成功率。

Conclusion: DMPI-PMHFE通过双通道特征融合有效解决了仅依赖预训练模型的局限性，为提示注入攻击检测提供了高效解决方案。

Abstract: With the widespread adoption of Large Language Models (LLMs), prompt
injection attacks have emerged as a significant security threat. Existing
defense mechanisms often face critical trade-offs between effectiveness and
generalizability. This highlights the urgent need for efficient prompt
injection detection methods that are applicable across a wide range of LLMs. To
address this challenge, we propose DMPI-PMHFE, a dual-channel feature fusion
detection framework. It integrates a pretrained language model with heuristic
feature engineering to detect prompt injection attacks. Specifically, the
framework employs DeBERTa-v3-base as a feature extractor to transform input
text into semantic vectors enriched with contextual information. In parallel,
we design heuristic rules based on known attack patterns to extract explicit
structural features commonly observed in attacks. Features from both channels
are subsequently fused and passed through a fully connected neural network to
produce the final prediction. This dual-channel approach mitigates the
limitations of relying only on DeBERTa to extract features. Experimental
results on diverse benchmark datasets demonstrate that DMPI-PMHFE outperforms
existing methods in terms of accuracy, recall, and F1-score. Furthermore, when
deployed actually, it significantly reduces attack success rates across
mainstream LLMs, including GLM-4, LLaMA 3, Qwen 2.5, and GPT-4o.

</details>


### [84] [Confidence Is All You Need: Few-Shot RL Fine-Tuning of Language Models](https://arxiv.org/abs/2506.06395)
*Pengyi Li,Matvey Skripkin,Alexander Zubrey,Andrey Kuznetsov,Ivan Oseledets*

Main category: cs.CL

TL;DR: RLSC利用模型自身置信度作为奖励信号，无需人工标注或外部奖励模型，显著提升了推理模型的性能。


<details>
  <summary>Details</summary>
Motivation: 现有强化学习方法依赖昂贵的人工标注或外部奖励模型，RLSC旨在通过模型自身置信度简化这一过程。

Method: 提出RLSC方法，以模型自身置信度作为奖励信号，应用于Qwen2.5-Math-7B模型，仅需少量样本和训练轮次。

Result: 在AIME2024、MATH500和AMC23数据集上，准确率分别提升20.10%、49.40%和52.50%。

Conclusion: RLSC是一种简单、可扩展且需要极少监督的推理模型后训练方法。

Abstract: Large language models (LLMs) excel at reasoning, yet post-training remains
critical for aligning their behavior with task goals. Existing reinforcement
learning (RL) methods often depend on costly human annotations or external
reward models. We propose Reinforcement Learning via Self-Confidence (RLSC),
which uses the model's own confidence as reward signals-eliminating the need
for labels, preference models, or reward engineering. Applied to
Qwen2.5-Math-7B with only 8 samples per question and 4 training epochs, RLSC
improves accuracy by +20.10% on AIME2024, +49.40% on MATH500, and +52.50% on
AMC23. RLSC offers a simple, scalable post-training method for reasoning models
with minimal supervision.

</details>


### [85] [Natural Language Interaction with Databases on Edge Devices in the Internet of Battlefield Things](https://arxiv.org/abs/2506.06396)
*Christopher D. Molek,Roberto Fronteddu,K. Brent Venable,Niranjan Suri*

Main category: cs.CL

TL;DR: 论文提出了一种利用自然语言处理（NLP）和大型语言模型（LLM）的流程，用于战场物联网（IoBT）中的自然语言查询数据库，并返回自然语言响应，以提高情境感知能力。


<details>
  <summary>Details</summary>
Motivation: 战场物联网（IoBT）的扩展为增强情境感知提供了新机会，但需要将设备数据处理为可消费的信息对象。

Method: 使用适合边缘设备的LLM进行NLP，结合图形数据库处理动态连接网络，通过LLM将自然语言问题映射到Cypher查询，并将结果以自然语言返回。

Result: 在公开的美国陆军多用途传感区（MSA）数据上测试，Llama 3.1（80亿参数）表现最佳，且两步方法放宽了精确匹配要求，准确率提高了19.4%。

Conclusion: 该流程为在边缘设备上部署LLM以实现自然语言交互奠定了基础，支持关键决策。

Abstract: The expansion of the Internet of Things (IoT) in the battlefield, Internet of
Battlefield Things (IoBT), gives rise to new opportunities for enhancing
situational awareness. To increase the potential of IoBT for situational
awareness in critical decision making, the data from these devices must be
processed into consumer-ready information objects, and made available to
consumers on demand. To address this challenge we propose a workflow that makes
use of natural language processing (NLP) to query a database technology and
return a response in natural language. Our solution utilizes Large Language
Models (LLMs) that are sized for edge devices to perform NLP as well as
graphical databases which are well suited for dynamic connected networks which
are pervasive in the IoBT. Our architecture employs LLMs for both mapping
questions in natural language to Cypher database queries as well as to
summarize the database output back to the user in natural language. We evaluate
several medium sized LLMs for both of these tasks on a database representing
publicly available data from the US Army's Multipurpose Sensing Area (MSA) at
the Jornada Range in Las Cruces, NM. We observe that Llama 3.1 (8 billion
parameters) outperforms the other models across all the considered metrics.
Most importantly, we note that, unlike current methods, our two step approach
allows the relaxation of the Exact Match (EM) requirement of the produced
Cypher queries with ground truth code and, in this way, it achieves a 19.4%
increase in accuracy. Our workflow lays the ground work for deploying LLMs on
edge devices to enable natural language interactions with databases containing
information objects for critical decision making.

</details>


### [86] [Direct Behavior Optimization: Unlocking the Potential of Lightweight LLMs](https://arxiv.org/abs/2506.06401)
*Hongming Yang,Shi Lin,Jun Shao,Changting Lin,Donghai Zhu,Meng Han,Qinglei Kong*

Main category: cs.CL

TL;DR: DeBoP是一种针对轻量级大语言模型（LwLLMs）的直接行为优化范式，通过梯度自由的蒙特卡洛树搜索优化执行序列，显著提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 轻量级大语言模型在资源效率、成本效益和数据隐私方面具有优势，但在复杂任务上表现不佳，且现有提示优化方法对其效果有限。

Method: DeBoP将复杂提示优化转化为离散、可量化的执行序列优化，采用梯度自由的蒙特卡洛树搜索方法。

Result: 在七项挑战性任务中，DeBoP优化的LwLLMs表现优于现有提示优化方法，甚至超越GPT-3.5，同时计算时间减少约60%。

Conclusion: DeBoP为轻量级大语言模型提供了一种高效的自动优化方法，显著提升了其实际应用能力。

Abstract: Lightweight Large Language Models (LwLLMs) are reduced-parameter, optimized
models designed to run efficiently on consumer-grade hardware, offering
significant advantages in resource efficiency, cost-effectiveness, and data
privacy. However, these models often struggle with limited inference and
reasoning capabilities, which restrict their performance on complex tasks and
limit their practical applicability. Moreover, existing prompt optimization
methods typically rely on extensive manual effort or the meta-cognitive
abilities of state-of-the-art LLMs, making them less effective for LwLLMs. To
address these challenges, we introduce DeBoP, a new Direct Behavior
Optimization Paradigm, original from the Chain-of-Thought (CoT) prompting
technique. Unlike CoT Prompting, DeBoP is an automatic optimization method,
which focuses on the optimization directly on the behavior of LwLLMs. In
particular, DeBoP transforms the optimization of complex prompts into the
optimization of discrete, quantifiable execution sequences using a
gradient-free Monte Carlo Tree Search. We evaluate DeBoP on seven challenging
tasks where state-of-the-art LLMs excel but LwLLMs generally underperform.
Experimental results demonstrate that DeBoP significantly outperforms recent
prompt optimization methods on most tasks. In particular, DeBoP-optimized
LwLLMs surpass GPT-3.5 on most tasks while reducing computational time by
approximately 60% compared to other automatic prompt optimization methods.

</details>


### [87] [Unintended Harms of Value-Aligned LLMs: Psychological and Empirical Insights](https://arxiv.org/abs/2506.06404)
*Sooyung Choi,Jaehyeok Lee,Xiaoyuan Yi,Jing Yao,Xing Xie,JinYeong Bak*

Main category: cs.CL

TL;DR: 研究发现，与人类价值观对齐的大型语言模型（LLMs）可能带来更高的安全风险，因为它们会放大有害行为。


<details>
  <summary>Details</summary>
Motivation: 探索个性化LLMs与人类价值观对齐时的潜在安全风险及其心理机制。

Method: 通过数据集分析，结合心理学假设，研究价值对齐与安全风险的相关性。

Result: 价值对齐的LLMs比未微调模型更容易产生有害行为，且风险略高于其他微调模型。

Conclusion: 研究揭示了价值对齐的“黑箱”问题，并提出上下文对齐方法以提升安全性。

Abstract: The application scope of Large Language Models (LLMs) continues to expand,
leading to increasing interest in personalized LLMs that align with human
values. However, aligning these models with individual values raises
significant safety concerns, as certain values may correlate with harmful
information. In this paper, we identify specific safety risks associated with
value-aligned LLMs and investigate the psychological principles behind these
challenges. Our findings reveal two key insights. (1) Value-aligned LLMs are
more prone to harmful behavior compared to non-fine-tuned models and exhibit
slightly higher risks in traditional safety evaluations than other fine-tuned
models. (2) These safety issues arise because value-aligned LLMs genuinely
generate text according to the aligned values, which can amplify harmful
outcomes. Using a dataset with detailed safety categories, we find significant
correlations between value alignment and safety risks, supported by
psychological hypotheses. This study offers insights into the "black box" of
value alignment and proposes in-context alignment methods to enhance the safety
of value-aligned LLMs.

</details>


### [88] [SMAR: Soft Modality-Aware Routing Strategy for MoE-based Multimodal Large Language Models Preserving Language Capabilities](https://arxiv.org/abs/2506.06406)
*Guoyang Xia,Yifeng Ding,Fengfa Li,Lei Ren,Chen Wei,Fangxiang Feng,Xiaojie Wang*

Main category: cs.CL

TL;DR: 提出了一种名为SMAR的正则化技术，通过KL散度控制多模态路由概率分布，平衡模态差异与语言能力。


<details>
  <summary>Details</summary>
Motivation: 解决现有多模态MoE模型训练成本高或语言能力退化的问题。

Method: 使用KL散度正则化技术（SMAR）控制路由概率分布，无需修改模型架构或依赖大量文本数据。

Result: 在视觉指令调优中，语言能力保留率达86.6%，仅需2.5%纯文本，优于基线模型。

Conclusion: SMAR为多模态MoE模型提供了一种高效平衡模态差异与语言能力的解决方案。

Abstract: Mixture of Experts (MoE) architectures have become a key approach for scaling
large language models, with growing interest in extending them to multimodal
tasks. Existing methods to build multimodal MoE models either incur high
training costs or suffer from degraded language capabilities when adapting
pretrained models. To address this, we propose Soft ModalityAware Routing
(SMAR), a novel regularization technique that uses Kullback Leibler divergence
to control routing probability distributions across modalities, encouraging
expert specialization without modifying model architecture or heavily relying
on textual data. Experiments on visual instruction tuning show that SMAR
preserves language ability at 86.6% retention with only 2.5% pure text,
outperforming baselines while maintaining strong multimodal performance. Our
approach offers a practical and efficient solution to balance modality
differentiation and language capabilities in multimodal MoE models.

</details>


### [89] [Canonical Autoregressive Generation](https://arxiv.org/abs/2506.06446)
*Ivi Chatzi,Nina Corvelo Benz,Stratis Tsirtsis,Manuel Gomez-Rodriguez*

Main category: cs.CL

TL;DR: 论文提出了一种称为“规范采样”的方法，旨在解决大语言模型生成非规范标记序列的问题，并证明其生成的序列更接近训练数据的真实分布。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型在生成文本时可能产生非规范标记序列，带来负面影响。本文旨在解决这一问题。

Method: 基于理论分析，提出“规范采样”方法，确保模型在自回归生成过程中仅生成规范标记序列。

Result: 规范采样方法简单高效，且生成的标记序列分布更接近训练数据的真实分布。

Conclusion: 规范采样是一种有效的改进方法，能够提升大语言模型生成文本的质量和一致性。

Abstract: State of the art large language models are trained using large amounts of
tokens derived from raw text using what is called a tokenizer. Crucially, the
tokenizer determines the (token) vocabulary a model will use during inference
as well as, in principle, the (token) language. This is because, while the
token vocabulary may allow for different tokenizations of a string, the
tokenizer always maps the string to only one of these tokenizations--the
canonical tokenization. However, multiple lines of empirical evidence suggest
that large language models do not always generate canonical token sequences,
and this comes with several negative consequences. In this work, we first show
that, to generate a canonical token sequence, a model needs to generate
(partial) canonical token sequences at each step of the autoregressive
generation process underpinning its functioning. Building upon this theoretical
result, we introduce canonical sampling, a simple and efficient sampling method
that precludes a given model from generating non-canonical token sequences.
Further, we also show that, in comparison with standard sampling, the
distribution of token sequences generated using canonical sampling is provably
closer to the true distribution of token sequences used during training.

</details>


### [90] [What Is Seen Cannot Be Unseen: The Disruptive Effect of Knowledge Conflict on Large Language Models](https://arxiv.org/abs/2506.06485)
*Kaiser Sun,Fan Bai,Mark Dredze*

Main category: cs.CL

TL;DR: 论文提出了一种诊断框架，用于评估大型语言模型在上下文与参数知识冲突时的行为，发现知识冲突对无需知识的任务影响最小，模型在知识一致时表现更好，且难以完全抑制内部知识。


<details>
  <summary>Details</summary>
Motivation: 研究大型语言模型在上下文与参数知识冲突时的行为，以评估其可靠性和部署风险。

Method: 构建诊断数据以引发冲突，分析模型在不同任务类型中的表现。

Result: 发现知识冲突对无需知识的任务影响小，模型在知识一致时表现更好，且难以完全抑制内部知识。提供冲突解释会增加对上下文的依赖。

Conclusion: 研究揭示了知识冲突对模型评估的影响，强调了在部署中需考虑此类冲突。

Abstract: Large language models frequently rely on both contextual input and parametric
knowledge to perform tasks. However, these sources can come into conflict,
especially when retrieved documents contradict the model's parametric
knowledge. We propose a diagnostic framework to systematically evaluate LLM
behavior under context-memory conflict, where the contextual information
diverges from their parametric beliefs. We construct diagnostic data that
elicit these conflicts and analyze model performance across multiple task
types. Our findings reveal that (1) knowledge conflict has minimal impact on
tasks that do not require knowledge utilization, (2) model performance is
consistently higher when contextual and parametric knowledge are aligned, (3)
models are unable to fully suppress their internal knowledge even when
instructed, and (4) providing rationales that explain the conflict increases
reliance on contexts. These insights raise concerns about the validity of
model-based evaluation and underscore the need to account for knowledge
conflict in the deployment of LLMs.

</details>


### [91] [Improving LLM-Powered EDA Assistants with RAFT](https://arxiv.org/abs/2506.06500)
*Luyao Shi,Michael Kazda,Charles Schmitter,Hemlata Gupta*

Main category: cs.CL

TL;DR: 论文提出利用合成Q/A数据集结合RAFT方法提升LLM在EDA领域的性能，并探讨了真实用户问题对合成数据生成的影响，同时实施了安全控制以防止敏感信息泄露。


<details>
  <summary>Details</summary>
Motivation: 电子设计工程师在EDA任务中难以高效获取相关信息，现有开源LLM缺乏领域知识，RAFT虽能提升性能但缺乏标注数据。

Method: 提出使用合成Q/A数据集结合RAFT方法，并研究真实用户问题对合成数据生成的影响，同时实施安全访问控制。

Result: RAFT结合合成数据显著提升LLM在EDA任务中的性能，并验证了真实用户问题对数据生成的有效性。

Conclusion: 合成数据结合RAFT是提升LLM在EDA领域性能的有效方法，同时需注意数据安全和隐私保护。

Abstract: Electronic design engineers often struggle to efficiently access relevant
information for tasks like design verification and technology development.
While large language models (LLMs) can enhance productivity as conversational
agents, pre-trained open-source LLMs lack domain-specific knowledge for
Electronic Design Automation (EDA). In a Retrieval-Augmented Generation (RAG)
context, LLMs rely on external context but may still produce inaccurate
responses. Retrieval-Augmented Fine-Tuning (RAFT) improves LLM performance, but
acquiring labeled question/answer (Q/A) data in EDA is difficult. To address
this, we propose using synthetic Q/A datasets to enhance LLMs with RAFT. Our
results show that RAFT with synthetic data significantly boosts LLM performance
for RAG-based EDA tasks. We also investigate the impact of using real user
questions as Retrieval-Augmented Few-Shot (RAFS) examples for synthetic data
generation. Additionally, we implement secure access control to ensure
sensitive information is only accessible to authorized personnel. Finally, we
assess the risk of data leakage and unintended memorization during fine-tuning
with synthetic data, providing practical insights.

</details>


### [92] [Biases Propagate in Encoder-based Vision-Language Models: A Systematic Analysis From Intrinsic Measures to Zero-shot Retrieval Outcomes](https://arxiv.org/abs/2506.06506)
*Kshitish Ghate,Tessa Charlesworth,Mona Diab,Aylin Caliskan*

Main category: cs.CL

TL;DR: 研究发现基础视觉语言模型（VLMs）中的社会群体偏见会系统性地传递到零样本检索任务中，且性能更强的模型偏见传递更显著。


<details>
  <summary>Details</summary>
Motivation: 理解社会群体偏见如何从基础视觉语言模型传递到下游任务，以构建更公平的AI系统。

Method: 提出一个控制框架，通过关联（a）表征空间中的内在偏见与（b）零样本检索任务中的外在偏见来测量偏见传递。

Result: 内在与外在偏见之间存在显著相关性（平均ρ=0.83±0.10），且模型性能越强，偏见传递越明显。

Conclusion: 研究揭示了偏见传递的普遍性，并呼吁关注高性能模型中潜在的偏见放大问题。

Abstract: To build fair AI systems we need to understand how social-group biases
intrinsic to foundational encoder-based vision-language models (VLMs) manifest
in biases in downstream tasks. In this study, we demonstrate that intrinsic
biases in VLM representations systematically ``carry over'' or propagate into
zero-shot retrieval tasks, revealing how deeply rooted biases shape a model's
outputs. We introduce a controlled framework to measure this propagation by
correlating (a) intrinsic measures of bias in the representational space with
(b) extrinsic measures of bias in zero-shot text-to-image (TTI) and
image-to-text (ITT) retrieval. Results show substantial correlations between
intrinsic and extrinsic bias, with an average $\rho$ = 0.83 $\pm$ 0.10. This
pattern is consistent across 114 analyses, both retrieval directions, six
social groups, and three distinct VLMs. Notably, we find that
larger/better-performing models exhibit greater bias propagation, a finding
that raises concerns given the trend towards increasingly complex AI models.
Our framework introduces baseline evaluation tasks to measure the propagation
of group and valence signals. Investigations reveal that underrepresented
groups experience less robust propagation, further skewing their model-related
outcomes.

</details>


### [93] [Fixing It in Post: A Comparative Study of LLM Post-Training Data Quality and Model Performance](https://arxiv.org/abs/2506.06522)
*Aladin Djuhera,Swanand Ravindra Kadhe,Syed Zawad,Farhan Ahmed,Heiko Ludwig,Holger Boche*

Main category: cs.CL

TL;DR: 论文对两个开源后训练数据集（Tulu-3-SFT-Mix和SmolTalk）进行了首次全面对比分析，提出了新的数据混合方法TuluTalk，性能优于原始数据集。


<details>
  <summary>Details</summary>
Motivation: 当前主流大语言模型的后训练数据集缺乏透明度，开源替代数据集虽性能接近，但缺乏系统性比较，难以评估数据质量对性能的影响。

Method: 使用Magpie框架对数据集样本进行质量标注，分析结构和质量差异，设计新的数据混合方法TuluTalk。

Result: TuluTalk样本减少14%，但在关键基准测试中性能优于或匹配原始数据集。

Conclusion: 研究为构建高效后训练数据集提供了实用方法，并公开了标注数据和TuluTalk混合数据集。

Abstract: Recent work on large language models (LLMs) has increasingly focused on
post-training and alignment with datasets curated to enhance instruction
following, world knowledge, and specialized skills. However, most post-training
datasets used in leading open- and closed-source LLMs remain inaccessible to
the public, with limited information about their construction process. This
lack of transparency has motivated the recent development of open-source
post-training corpora. While training on these open alternatives can yield
performance comparable to that of leading models, systematic comparisons remain
challenging due to the significant computational cost of conducting them
rigorously at scale, and are therefore largely absent. As a result, it remains
unclear how specific samples, task types, or curation strategies influence
downstream performance when assessing data quality. In this work, we conduct
the first comprehensive side-by-side analysis of two prominent open
post-training datasets: Tulu-3-SFT-Mix and SmolTalk. Using the Magpie
framework, we annotate each sample with detailed quality metrics, including
turn structure (single-turn vs. multi-turn), task category, input quality, and
response quality, and we derive statistics that reveal structural and
qualitative similarities and differences between the two datasets. Based on
these insights, we design a principled curation recipe that produces a new data
mixture, TuluTalk, which contains 14% fewer samples than either source dataset
while matching or exceeding their performance on key benchmarks. Our findings
offer actionable insights for constructing more effective post-training
datasets that improve model performance within practical resource limits. To
support future research, we publicly release both the annotated source datasets
and our curated TuluTalk mixture.

</details>


### [94] [Beyond Facts: Evaluating Intent Hallucination in Large Language Models](https://arxiv.org/abs/2506.06539)
*Yijie Hao,Haofei Yu,Jiaxuan You*

Main category: cs.CL

TL;DR: 论文提出‘意图幻觉’概念，指大语言模型（LLMs）在复杂查询中部分忽略或误解条件。作者引入FAITHQA基准（20,068问题）评估此现象，并开发CONSTRAINT SCORE指标。


<details>
  <summary>Details</summary>
Motivation: LLMs在处理多条件查询时易忽略或误解部分条件，导致意图幻觉，需系统性评估。

Method: 提出FAITHQA基准，涵盖查询和检索增强生成（RAG）场景，开发CONSTRAINT SCORE自动评估指标。

Result: 发现意图幻觉普遍存在于先进LLMs中，主因是条件遗漏或误解。CONSTRAINT SCORE接近人类评估表现。

Conclusion: 意图幻觉是LLMs的常见问题，FAITHQA和CONSTRAINT SCORE为未来研究提供工具。

Abstract: When exposed to complex queries containing multiple conditions, today's large
language models (LLMs) tend to produce responses that only partially satisfy
the query while neglecting certain conditions. We therefore introduce the
concept of Intent Hallucination. In this phenomenon, LLMs either omit
(neglecting to address certain parts) or misinterpret (responding to invented
query parts) elements of the given query, leading to intent hallucinated
generation. To systematically evaluate intent hallucination, we introduce
FAITHQA, a novel benchmark for intent hallucination that contains 20,068
problems, covering both query-only and retrieval-augmented generation (RAG)
setups with varying topics and difficulty. FAITHQA is the first hallucination
benchmark that goes beyond factual verification, tailored to identify the
fundamental cause of intent hallucination. By evaluating various LLMs on
FAITHQA, we find that (1) intent hallucination is a common issue even for
state-of-the-art models, and (2) the phenomenon stems from omission or
misinterpretation of LLMs. To facilitate future research, we introduce an
automatic LLM generation evaluation metric, CONSTRAINT SCORE, for detecting
intent hallucination. Human evaluation results demonstrate that CONSTRAINT
SCORE is closer to human performance for intent hallucination compared to
baselines.

</details>


### [95] [LaMP-Cap: Personalized Figure Caption Generation With Multimodal Figure Profiles](https://arxiv.org/abs/2506.06561)
*Ho Yin 'Sam' Ng,Ting-Yao Hsu,Aashish Anantha Ramakrishnan,Branislav Kveton,Nedim Lipka,Franck Dernoncourt,Dongwon Lee,Tong Yu,Sungchul Kim,Ryan A. Rossi,Ting-Hao 'Kenneth' Huang*

Main category: cs.CL

TL;DR: 论文提出了LaMP-Cap数据集，用于个性化多模态图表标题生成，实验表明多模态信息能提升标题生成质量。


<details>
  <summary>Details</summary>
Motivation: 现有AI生成的图表标题缺乏个性化，且多模态场景下的个性化技术研究不足。

Method: 引入LaMP-Cap数据集，结合图表图像、标题及相关段落等多模态信息作为个性化输入。

Result: 实验证明多模态信息（尤其是图像）能显著提升标题生成质量。

Conclusion: 多模态个性化输入是提升图表标题生成效果的关键。

Abstract: Figure captions are crucial for helping readers understand and remember a
figure's key message. Many models have been developed to generate these
captions, helping authors compose better quality captions more easily. Yet,
authors almost always need to revise generic AI-generated captions to match
their writing style and the domain's style, highlighting the need for
personalization. Despite language models' personalization (LaMP) advances,
these technologies often focus on text-only settings and rarely address
scenarios where both inputs and profiles are multimodal. This paper introduces
LaMP-Cap, a dataset for personalized figure caption generation with multimodal
figure profiles. For each target figure, LaMP-Cap provides not only the needed
inputs, such as figure images, but also up to three other figures from the same
document--each with its image, caption, and figure-mentioning paragraphs--as a
profile to characterize the context. Experiments with four LLMs show that using
profile information consistently helps generate captions closer to the original
author-written ones. Ablation studies reveal that images in the profile are
more helpful than figure-mentioning paragraphs, highlighting the advantage of
using multimodal profiles over text-only ones.

</details>


### [96] [Precise Information Control in Long-Form Text Generation](https://arxiv.org/abs/2506.06589)
*Jacqueline He,Howard Yen,Margaret Li,Shuyue Stella Li,Zhiyuan Zeng,Weijia Shi,Yulia Tsvetkov,Danqi Chen,Pang Wei Koh,Luke Zettlemoyer*

Main category: cs.CL

TL;DR: 论文提出Precise Information Control (PIC)任务，研究语言模型的内在幻觉问题，并开发PIC-Bench基准和PIC-LM模型，显著提升生成内容的准确性。


<details>
  <summary>Details</summary>
Motivation: 现代语言模型存在内在幻觉问题，即生成看似合理但未经输入内容证实的信息。研究旨在通过PIC任务解决这一问题。

Method: 提出PIC任务，要求模型基于可验证的短句生成长文本；构建PIC-Bench基准；开发PIC-LM模型，通过弱监督偏好数据训练。

Result: 当前最先进的语言模型在70%以上的输出中存在幻觉；PIC-LM在PIC任务中的F1分数从69.1%提升至91.0%。

Conclusion: PIC-LM显著提升了生成内容的准确性，展示了精确生成任务的潜力。

Abstract: A central challenge in modern language models (LMs) is intrinsic
hallucination: the generation of information that is plausible but
unsubstantiated relative to input context. To study this problem, we propose
Precise Information Control (PIC), a new task formulation that requires models
to generate long-form outputs grounded in a provided set of short
self-contained statements, known as verifiable claims, without adding any
unsupported ones. For comprehensiveness, PIC includes a full setting that tests
a model's ability to include exactly all input claims, and a partial setting
that requires the model to selectively incorporate only relevant claims. We
present PIC-Bench, a benchmark of eight long-form generation tasks (e.g.,
summarization, biography generation) adapted to the PIC setting, where LMs are
supplied with well-formed, verifiable input claims. Our evaluation of a range
of open and proprietary LMs on PIC-Bench reveals that, surprisingly,
state-of-the-art LMs still intrinsically hallucinate in over 70% of outputs. To
alleviate this lack of faithfulness, we introduce a post-training framework,
using a weakly supervised preference data construction method, to train an 8B
PIC-LM with stronger PIC ability--improving from 69.1% to 91.0% F1 in the full
PIC setting. When integrated into end-to-end factual generation pipelines,
PIC-LM improves exact match recall by 17.1% on ambiguous QA with retrieval, and
factual precision by 30.5% on a birthplace verification task, underscoring the
potential of precisely grounded generation.

</details>


### [97] [MedCite: Can Language Models Generate Verifiable Text for Medicine?](https://arxiv.org/abs/2506.06605)
*Xiao Wang,Mengjue Tan,Qiao Jin,Guangzhi Xiong,Yu Hu,Aidong Zhang,Zhiyong Lu,Minjia Zhang*

Main category: cs.CL

TL;DR: 论文提出了首个端到端框架，用于设计和评估基于LLM的医学任务引用生成，并引入了一种新颖的多轮检索-引用方法，显著提升了引用质量。


<details>
  <summary>Details</summary>
Motivation: 现有基于LLM的医学问答系统缺乏引用生成和评估功能，限制了其实际应用。

Method: 提出了一种端到端框架和一种多轮检索-引用方法，用于生成高质量引用。

Result: 该方法在引用精确率和召回率上优于基线方法，且评估结果与专家标注结果高度相关。

Conclusion: 该研究为医学任务中的引用生成提供了挑战与机遇，并展示了重要设计选择对引用质量的显著影响。

Abstract: Existing LLM-based medical question-answering systems lack citation
generation and evaluation capabilities, raising concerns about their adoption
in practice. In this work, we introduce \name, the first end-to-end framework
that facilitates the design and evaluation of citation generation with LLMs for
medical tasks. Meanwhile, we introduce a novel multi-pass retrieval-citation
method that generates high-quality citations. Our evaluation highlights the
challenges and opportunities of citation generation for medical tasks, while
identifying important design choices that have a significant impact on the
final citation quality. Our proposed method achieves superior citation
precision and recall improvements compared to strong baseline methods, and we
show that evaluation results correlate well with annotation results from
professional experts.

</details>


### [98] [Training-Free Tokenizer Transplantation via Orthogonal Matching Pursuit](https://arxiv.org/abs/2506.06607)
*Charles Goddard,Fernando Fernandes Neto*

Main category: cs.CL

TL;DR: 提出一种无需训练的方法，通过正交匹配追踪（OMP）重建未见过的词嵌入，实现预训练大语言模型（LLM）中分词器的移植。


<details>
  <summary>Details</summary>
Motivation: 解决不同分词器之间的差异问题，避免因分词器不匹配导致的性能下降，同时支持跨分词器的知识蒸馏、推测解码等应用。

Method: 使用OMP方法，将新词汇表示为共享锚词汇的稀疏线性组合，分两阶段实现：先在捐赠模型的嵌入空间中计算新词汇表示，再将其稀疏系数迁移回基础模型的嵌入空间。

Result: 在Llama→Mistral NeMo和Qwen→Llama任务中，OMP在零样本设置下表现最佳，优于其他基线方法（如WECHSEL、FOCUS等），且无需梯度更新。

Conclusion: OMP方法能有效解决分词器差异问题，支持模型权重的直接重用，并已集成到开源工具mergekit-tokensurgeon中。

Abstract: We present a training-free method to transplant tokenizers in pretrained
large language models (LLMs) by reconstructing unseen token embeddings via
Orthogonal Matching Pursuit (OMP). Specifically, we approximate each
out-of-vocabulary token as a sparse linear combination of shared tokens, in two
phases: first, compute each new token's representation in the donor embedding
space with a small dictionary of shared anchor tokens, then transfer these same
sparse coefficients back into the base model's embedding space.
  On two challenging cross-tokenizer tasks--Llama$\to$Mistral NeMo (12B) and
Qwen$\to$Llama (1B)--we show that OMP achieves best zero-shot preservation of
the base model's performance across multiple benchmarks, while other zero-shot
approaches degrade significantly. Compared to baselines (zero-init, mean-init,
and existing approaches like WECHSEL, FOCUS, ZETT), OMP consistently achieves
the best overall performance, effectively bridging large tokenizer
discrepancies without gradient updates. Our analysis further identifies
mismatched numerical tokenization schemes as a critical challenge for
preserving mathematical reasoning capabilities. This technique enables direct
reuse of pretrained model weights with new tokenizers, facilitating
cross-tokenizer knowledge distillation, speculative decoding, ensembling,
merging, and domain-specific vocabulary adaptations. We integrate our method
into the open-source mergekit-tokensurgeon tool for post hoc vocabulary
realignment.

</details>


### [99] [Transferring Features Across Language Models With Model Stitching](https://arxiv.org/abs/2506.06609)
*Alan Chen,Jack Merullo,Alessandro Stolfo,Ellie Pavlick*

Main category: cs.CL

TL;DR: 通过仿射映射在语言模型残差流之间传输特征，实现SAE权重在大小模型间的迁移，节省训练成本。


<details>
  <summary>Details</summary>
Motivation: 探索大小模型间表示空间的相似性，以降低训练SAE等昂贵组件的成本。

Method: 使用仿射映射技术将SAE权重从小模型迁移到大模型，并分析特征传输效果。

Result: 小模型和大模型学习到高度相似的表示空间，SAE迁移可节省50%训练成本。语义和结构特征传输效果不同。

Conclusion: 大小模型的线性表示空间存在相似性和差异，SAE迁移技术可提升训练效率。

Abstract: In this work, we demonstrate that affine mappings between residual streams of
language models is a cheap way to effectively transfer represented features
between models. We apply this technique to transfer the weights of Sparse
Autoencoders (SAEs) between models of different sizes to compare their
representations. We find that small and large models learn highly similar
representation spaces, which motivates training expensive components like SAEs
on a smaller model and transferring to a larger model at a FLOPs savings. For
example, using a small-to-large transferred SAE as initialization can lead to
50% cheaper training runs when training SAEs on larger models. Next, we show
that transferred probes and steering vectors can effectively recover ground
truth performance. Finally, we dive deeper into feature-level transferability,
finding that semantic and structural features transfer noticeably differently
while specific classes of functional features have their roles faithfully
mapped. Overall, our findings illustrate similarities and differences in the
linear representation spaces of small and large models and demonstrate a method
for improving the training efficiency of SAEs.

</details>


### [100] [Beyond Classification: Towards Speech Emotion Reasoning with Multitask AudioLLMs](https://arxiv.org/abs/2506.06820)
*Wenyu Zhang,Yingxu He,Geyu Lin,Zhuohan Liu,Shuo Sun,Bin Wang,Xunlong Zou,Jeremy H. M. Wong,Qiongqiong Wang,Hardik B. Sailor,Nancy F. Chen,Ai Ti Aw*

Main category: cs.CL

TL;DR: 本文提出了一种利用AudioLLMs生成能力增强情感识别的方法，通过结合推理增强数据监督、双编码器架构和任务交替训练，提高了情感预测准确性和生成响应的连贯性。


<details>
  <summary>Details</summary>
Motivation: 现有方法将情感理解视为分类问题，缺乏对预测背后原理的深入解释。本文旨在通过情感推理增强AudioLLMs的情感识别能力。

Method: 提出统一框架，结合推理增强数据监督、双编码器架构和任务交替训练，使AudioLLMs能有效学习多任务并融入情感推理。

Result: 在IEMOCAP和MELD数据集上的实验表明，该方法不仅提高了情感预测准确性，还增强了生成响应的连贯性和证据基础。

Conclusion: 通过情感推理，AudioLLMs在情感识别任务中表现出更强的性能，同时生成更合理的解释。

Abstract: Audio Large Language Models (AudioLLMs) have achieved strong results in
semantic tasks like speech recognition and translation, but remain limited in
modeling paralinguistic cues such as emotion. Existing approaches often treat
emotion understanding as a classification problem, offering little insight into
the underlying rationale behind predictions. In this work, we explore emotion
reasoning, a strategy that leverages the generative capabilities of AudioLLMs
to enhance emotion recognition by producing semantically aligned,
evidence-grounded explanations. To support this in multitask AudioLLMs, we
introduce a unified framework combining reasoning-augmented data supervision,
dual-encoder architecture, and task-alternating training. This approach enables
AudioLLMs to effectively learn different tasks while incorporating emotional
reasoning. Experiments on IEMOCAP and MELD show that our approach not only
improves emotion prediction accuracy but also enhances the coherence and
evidential grounding of the generated responses.

</details>


### [101] [Interpretable Depression Detection from Social Media Text Using LLM-Derived Embeddings](https://arxiv.org/abs/2506.06616)
*Samuel Kim,Oghenemaro Imieye,Yunting Yin*

Main category: cs.CL

TL;DR: 研究比较了大型语言模型（LLMs）和传统机器学习分类器在社交媒体数据上的抑郁语言检测性能，发现LLMs在二元分类中表现优异，但在细粒度分类中表现不佳，而基于LLM生成的摘要嵌入的分类器表现更优。


<details>
  <summary>Details</summary>
Motivation: 准确且可解释的抑郁语言检测对心理健康早期干预和公共健康具有重要意义。

Method: 比较了零样本LLMs和监督分类器在三种分类任务（二元抑郁分类、抑郁严重程度分类、抑郁与PTSD和焦虑的鉴别诊断）中的表现，使用了传统文本嵌入和LLM生成的摘要嵌入。

Result: 零样本LLMs在二元分类中泛化能力强，但在细粒度分类中表现不佳；基于LLM摘要嵌入的分类器表现更优。

Conclusion: LLMs在心理健康预测中具有潜力，未来可优化其零样本能力和上下文感知摘要技术。

Abstract: Accurate and interpretable detection of depressive language in social media
is useful for early interventions of mental health conditions, and has
important implications for both clinical practice and broader public health
efforts. In this paper, we investigate the performance of large language models
(LLMs) and traditional machine learning classifiers across three classification
tasks involving social media data: binary depression classification, depression
severity classification, and differential diagnosis classification among
depression, PTSD, and anxiety. Our study compares zero-shot LLMs with
supervised classifiers trained on both conventional text embeddings and
LLM-generated summary embeddings. Our experiments reveal that while zero-shot
LLMs demonstrate strong generalization capabilities in binary classification,
they struggle with fine-grained ordinal classifications. In contrast,
classifiers trained on summary embeddings generated by LLMs demonstrate
competitive, and in some cases superior, performance on the classification
tasks, particularly when compared to models using traditional text embeddings.
Our findings demonstrate the strengths of LLMs in mental health prediction, and
suggest promising directions for better utilization of their zero-shot
capabilities and context-aware summarization techniques.

</details>


### [102] [BriefMe: A Legal NLP Benchmark for Assisting with Legal Briefs](https://arxiv.org/abs/2506.06619)
*Jesse Woo,Fateme Hashemi Chaleshtori,Ana Marasović,Kenneth Marino*

Main category: cs.CL

TL;DR: 论文介绍了BRIEFME数据集，用于评估语言模型在法律简报写作中的能力，包括摘要、补全和案例检索任务。结果显示，当前模型在摘要和补全任务上表现良好，但在案例检索和实际补全任务上表现不佳。


<details>
  <summary>Details</summary>
Motivation: 探索法律简报写作这一未被充分研究的领域，评估语言模型在法律专业技能中的应用潜力。

Method: 构建BRIEFME数据集，包含三个任务：论点摘要、论点补全和案例检索，并对当前模型的表现进行分析。

Result: 大型语言模型在摘要和补全任务上表现优异，但在案例检索和实际补全任务上表现较差。

Conclusion: BRIEFME数据集有望推动法律NLP的发展，帮助法律专业人士更高效地完成工作。

Abstract: A core part of legal work that has been under-explored in Legal NLP is the
writing and editing of legal briefs. This requires not only a thorough
understanding of the law of a jurisdiction, from judgments to statutes, but
also the ability to make new arguments to try to expand the law in a new
direction and make novel and creative arguments that are persuasive to judges.
To capture and evaluate these legal skills in language models, we introduce
BRIEFME, a new dataset focused on legal briefs. It contains three tasks for
language models to assist legal professionals in writing briefs: argument
summarization, argument completion, and case retrieval. In this work, we
describe the creation of these tasks, analyze them, and show how current models
perform. We see that today's large language models (LLMs) are already quite
good at the summarization and guided completion tasks, even beating
human-generated headings. Yet, they perform poorly on other tasks in our
benchmark: realistic argument completion and retrieving relevant legal cases.
We hope this dataset encourages more development in Legal NLP in ways that will
specifically aid people in performing legal work.

</details>


### [103] [Automatic Speech Recognition of African American English: Lexical and Contextual Effects](https://arxiv.org/abs/2506.06888)
*Hamid Mojarad,Kevin Tang*

Main category: cs.CL

TL;DR: 研究探讨了非洲裔美国英语（AAE）中的辅音簇缩减（CCR）和ING缩减对自动语音识别（ASR）错误率的影响，并比较了带与不带语言模型（LM）的端到端ASR系统的表现。


<details>
  <summary>Details</summary>
Motivation: ASR模型在处理AAE的语音、音系和形态句法特征时表现不佳，尤其是CCR和ING缩减现象。研究旨在量化这些特征对ASR错误率的影响，并比较不同ASR系统的表现差异。

Method: 使用CORAAL语料库，通过wav2vec 2.0（带和不带LM）进行转录，利用Montreal Forced Aligner（MFA）检测CCR和ING缩减。

Result: CCR和ING缩减对词错误率（WER）有显著但较小的影响；不带LM的ASR系统更易受词汇邻域效应影响。

Conclusion: AAE的语音特征对ASR性能有可测量的影响，且ASR系统的架构（是否带LM）会影响其对不同语言现象的敏感度。

Abstract: Automatic Speech Recognition (ASR) models often struggle with the phonetic,
phonological, and morphosyntactic features found in African American English
(AAE). This study focuses on two key AAE variables: Consonant Cluster Reduction
(CCR) and ING-reduction. It examines whether the presence of CCR and
ING-reduction increases ASR misrecognition. Subsequently, it investigates
whether end-to-end ASR systems without an external Language Model (LM) are more
influenced by lexical neighborhood effect and less by contextual predictability
compared to systems with an LM. The Corpus of Regional African American
Language (CORAAL) was transcribed using wav2vec 2.0 with and without an LM. CCR
and ING-reduction were detected using the Montreal Forced Aligner (MFA) with
pronunciation expansion. The analysis reveals a small but significant effect of
CCR and ING on Word Error Rate (WER) and indicates a stronger presence of
lexical neighborhood effect in ASR systems without LMs.

</details>


### [104] [Psychological Counseling Cannot Be Achieved Overnight: Automated Psychological Counseling Through Multi-Session Conversations](https://arxiv.org/abs/2506.06626)
*Junzhe Wang,Bichen Wang,Xing Fu,Yixin Sun,Yanyan Zhao,Bing Qin*

Main category: cs.CL

TL;DR: 论文提出了一个多会话心理咨询数据集（MusPsy-Dataset）和模型（MusPsy-Model），以解决当前LLMs在心理咨询中仅关注单次会话的局限性，实验表明该模型在多会话场景中表现优于基线模型。


<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型（LLMs）在心理咨询领域的研究主要集中在单次会话，而实际心理咨询是一个持续的多会话过程，需要逐步解决客户问题。

Method: 通过公开的心理案例报告构建多会话心理咨询数据集（MusPsy-Dataset），并开发了能够跟踪客户进展并随时间调整咨询方向的MusPsy-Model。

Result: 实验结果表明，MusPsy-Model在多会话心理咨询场景中表现优于基线模型。

Conclusion: 多会话心理咨询数据集和模型的提出填补了LLMs在该领域的空白，为实际心理咨询场景提供了更有效的解决方案。

Abstract: In recent years, Large Language Models (LLMs) have made significant progress
in automated psychological counseling. However, current research focuses on
single-session counseling, which doesn't represent real-world scenarios. In
practice, psychological counseling is a process, not a one-time event,
requiring sustained, multi-session engagement to progressively address clients'
issues. To overcome this limitation, we introduce a dataset for Multi-Session
Psychological Counseling Conversation Dataset (MusPsy-Dataset). Our
MusPsy-Dataset is constructed using real client profiles from publicly
available psychological case reports. It captures the dynamic arc of
counseling, encompassing multiple progressive counseling conversations from the
same client across different sessions. Leveraging our dataset, we also
developed our MusPsy-Model, which aims to track client progress and adapt its
counseling direction over time. Experiments show that our model performs better
than baseline models across multiple sessions.

</details>


### [105] [BTPD: A Multilingual Hand-curated Dataset of Bengali Transnational Political Discourse Across Online Communities](https://arxiv.org/abs/2506.06813)
*Dipto Das,Syed Ishtiaque Ahmed,Shion Guha*

Main category: cs.CL

TL;DR: 本文介绍了多语言孟加拉跨国政治话语数据集（BTPD），填补了资源匮乏语言的研究空白。


<details>
  <summary>Details</summary>
Motivation: 由于缺乏数据集，孟加拉语等资源匮乏语言的政治话语研究受限，本文旨在填补这一空白。

Method: 通过社区知情的关键词检索手动整理数据集，并概述其主题和多语言内容。

Result: 成功构建了BTPD数据集，涵盖三个在线平台的不同社区结构和互动动态。

Conclusion: 该数据集为研究孟加拉语政治话语和意识形态极化提供了重要资源。

Abstract: Understanding political discourse in online spaces is crucial for analyzing
public opinion and ideological polarization. While social computing and
computational linguistics have explored such discussions in English, such
research efforts are significantly limited in major yet under-resourced
languages like Bengali due to the unavailability of datasets. In this paper, we
present a multilingual dataset of Bengali transnational political discourse
(BTPD) collected from three online platforms, each representing distinct
community structures and interaction dynamics. Besides describing how we
hand-curated the dataset through community-informed keyword-based retrieval,
this paper also provides a general overview of its topics and multilingual
content.

</details>


### [106] [SafeLawBench: Towards Safe Alignment of Large Language Models](https://arxiv.org/abs/2506.06636)
*Chuxue Cao,Han Zhu,Jiaming Ji,Qichao Sun,Zhenghao Zhu,Yinyu Wu,Juntao Dai,Yaodong Yang,Sirui Han,Yike Guo*

Main category: cs.CL

TL;DR: 论文提出了SafeLawBench基准，从法律角度评估大语言模型（LLM）的安全性，填补了现有评估标准主观性强的不足。


<details>
  <summary>Details</summary>
Motivation: 由于现有安全性评估标准主观性强，缺乏明确标准，论文旨在从法律角度填补这一空白。

Method: 提出SafeLawBench基准，将安全风险分为三级，包含24,860道选择题和1,106道开放域QA任务，评估了20个LLM的零样本和少样本表现。

Result: 评估显示，即使是领先模型如Claude-3.5-Sonnet和GPT-4o在多选题任务中准确率未超过80.5%，20个LLM平均准确率为68.8%。多数投票机制可提升性能。

Conclusion: 论文呼吁社区优先研究LLM安全性，SafeLawBench为系统性评估提供了法律视角的框架。

Abstract: With the growing prevalence of large language models (LLMs), the safety of
LLMs has raised significant concerns. However, there is still a lack of
definitive standards for evaluating their safety due to the subjective nature
of current safety benchmarks. To address this gap, we conducted the first
exploration of LLMs' safety evaluation from a legal perspective by proposing
the SafeLawBench benchmark. SafeLawBench categorizes safety risks into three
levels based on legal standards, providing a systematic and comprehensive
framework for evaluation. It comprises 24,860 multi-choice questions and 1,106
open-domain question-answering (QA) tasks. Our evaluation included 2
closed-source LLMs and 18 open-source LLMs using zero-shot and few-shot
prompting, highlighting the safety features of each model. We also evaluated
the LLMs' safety-related reasoning stability and refusal behavior.
Additionally, we found that a majority voting mechanism can enhance model
performance. Notably, even leading SOTA models like Claude-3.5-Sonnet and
GPT-4o have not exceeded 80.5% accuracy in multi-choice tasks on SafeLawBench,
while the average accuracy of 20 LLMs remains at 68.8\%. We urge the community
to prioritize research on the safety of LLMs.

</details>


### [107] [How do datasets, developers, and models affect biases in a low-resourced language?](https://arxiv.org/abs/2506.06816)
*Dipto Das,Shion Guha,Bryan Semaan*

Main category: cs.CL

TL;DR: 论文研究了孟加拉语情感分析模型中的身份偏见问题，发现即使语义和结构相似，模型仍存在性别、宗教和国籍偏见。


<details>
  <summary>Details</summary>
Motivation: 研究动机是探讨低资源语言（如孟加拉语）中身份偏见的普遍性及其影响，填补现有研究的空白。

Method: 方法包括对基于mBERT和BanglaBERT的情感分析模型进行算法审计，使用Google Dataset Search中的所有孟加拉语情感分析数据集进行微调。

Result: 结果显示，尽管语义和结构相似，模型仍表现出性别、宗教和国籍偏见，且预训练模型与多样化数据集的结合存在不一致性。

Conclusion: 结论强调了身份偏见的普遍性，并呼吁关注认知不公、AI对齐及算法审计中的方法论问题。

Abstract: Sociotechnical systems, such as language technologies, frequently exhibit
identity-based biases. These biases exacerbate the experiences of historically
marginalized communities and remain understudied in low-resource contexts.
While models and datasets specific to a language or with multilingual support
are commonly recommended to address these biases, this paper empirically tests
the effectiveness of such approaches in the context of gender, religion, and
nationality-based identities in Bengali, a widely spoken but low-resourced
language. We conducted an algorithmic audit of sentiment analysis models built
on mBERT and BanglaBERT, which were fine-tuned using all Bengali sentiment
analysis (BSA) datasets from Google Dataset Search. Our analyses showed that
BSA models exhibit biases across different identity categories despite having
similar semantic content and structure. We also examined the inconsistencies
and uncertainties arising from combining pre-trained models and datasets
created by individuals from diverse demographic backgrounds. We connected these
findings to the broader discussions on epistemic injustice, AI alignment, and
methodological decisions in algorithmic audits.

</details>


### [108] [Quantile Regression with Large Language Models for Price Prediction](https://arxiv.org/abs/2506.06657)
*Nikhita Vedula,Dushyanta Dhyani,Laleh Jalali,Boris Oreshkin,Mohsen Bayati,Shervin Malmasi*

Main category: cs.CL

TL;DR: 论文提出了一种基于大语言模型（LLMs）的量化回归方法，用于生成完整的预测分布，显著优于传统点估计方法。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要关注点估计，缺乏对不同方法的系统比较，且在处理文本到分布预测任务（如价格估计）时，对文本理解和不确定性量化的需求未被充分满足。

Method: 采用Mistral-7B模型，通过量化回归头进行微调，生成预测分布，并与传统方法进行系统比较。

Result: 实验表明，该方法在三个价格预测数据集上显著优于传统方法，且在预测准确性和分布校准方面均表现优异。

Conclusion: LLMs在结构化预测任务中具有潜力，量化回归方法为文本到分布预测提供了有效解决方案。

Abstract: Large Language Models (LLMs) have shown promise in structured prediction
tasks, including regression, but existing approaches primarily focus on point
estimates and lack systematic comparison across different methods. We
investigate probabilistic regression using LLMs for unstructured inputs,
addressing challenging text-to-distribution prediction tasks such as price
estimation where both nuanced text understanding and uncertainty quantification
are critical. We propose a novel quantile regression approach that enables LLMs
to produce full predictive distributions, improving upon traditional point
estimates. Through extensive experiments across three diverse price prediction
datasets, we demonstrate that a Mistral-7B model fine-tuned with quantile heads
significantly outperforms traditional approaches for both point and
distributional estimations, as measured by three established metrics each for
prediction accuracy and distributional calibration. Our systematic comparison
of LLM approaches, model architectures, training approaches, and data scaling
reveals that Mistral-7B consistently outperforms encoder architectures,
embedding-based methods, and few-shot learning methods. Our experiments also
reveal the effectiveness of LLM-assisted label correction in achieving
human-level accuracy without systematic bias. Our curated datasets are made
available at https://github.com/vnik18/llm-price-quantile-reg/ to support
future research.

</details>


### [109] [Transcript-Prompted Whisper with Dictionary-Enhanced Decoding for Japanese Speech Annotation](https://arxiv.org/abs/2506.07646)
*Rui Hu,Xiaolong Lin,Jiawang Liu,Shixi Huang,Zhenpeng Zhan*

Main category: cs.CL

TL;DR: 提出了一种基于预训练ASR模型的日语TTS数据集标注方法，结合字典先验知识优化音素标注，效果优于纯文本或音频方法。


<details>
  <summary>Details</summary>
Motivation: 为日语TTS数据集构建提供高效且准确的音素和韵律标注方法。

Method: 通过微调预训练ASR模型，结合字典先验知识解码策略，实现音素和韵律标注。

Result: 客观评估显示优于纯文本或音频方法，主观评估显示合成语音自然度接近人工标注。

Conclusion: 该方法为TTS数据集标注提供了高效且高质量的解决方案。

Abstract: In this paper, we propose a method for annotating phonemic and prosodic
labels on a given audio-transcript pair, aimed at constructing Japanese
text-to-speech (TTS) datasets. Our approach involves fine-tuning a large-scale
pre-trained automatic speech recognition (ASR) model, conditioned on ground
truth transcripts, to simultaneously output phrase-level graphemes and
annotation labels. To further correct errors in phonemic labeling, we employ a
decoding strategy that utilizes dictionary prior knowledge. The objective
evaluation results demonstrate that our proposed method outperforms previous
approaches relying solely on text or audio. The subjective evaluation results
indicate that the naturalness of speech synthesized by the TTS model, trained
with labels annotated using our method, is comparable to that of a model
trained with manual annotations.

</details>


### [110] [Learning Distribution-Wise Control in Representation Space for Language Models](https://arxiv.org/abs/2506.06686)
*Chunyuan Deng,Ruidi Chang,Hanjie Chen*

Main category: cs.CL

TL;DR: 论文提出了一种分布级干预方法，扩展了点级干预，提升了语言模型的可控性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有方法（点级干预）仅能控制概念子空间中的点，无法覆盖周围区域，限制了模型的精细控制能力。

Method: 通过分布级干预，模型不仅能学习点变换，还能学习概念子空间的周围区域。

Result: 在八个常识推理和七个算术推理基准测试中，分布级干预表现优于点级干预。

Conclusion: 分布级干预为语言模型提供了更全面的行为控制方法，实现了更精细的调控。

Abstract: Interventions in language models (LMs) are applied strategically to steer
model behavior during the forward pass. Learnable interventions, also known as
representation fine-tuning, aim to apply pointwise control within the concept
subspace and have proven effective in altering high-level behaviors. In this
work, we extend this approach to the distribution level, enabling the model to
learn not only pointwise transformations but also the surrounding regions of
the concept subspace. We demonstrate that these methods perform effectively in
early layers, with larger standard deviations correlating strongly with
improved performance. Across eight commonsense reasoning and seven arithmetic
reasoning benchmarks, our distribution-wise interventions consistently
outperform pointwise interventions in controllability and robustness. These
results illustrate that distribution-wise interventions provide a more
comprehensive method for steering model behavior and enabling finer-grained
control over language models. The code is at:
\href{https://github.com/chili-lab/D-Intervention}{https://github.com/chili-lab/D-Intervention}.

</details>


### [111] [Dynamic and Parametric Retrieval-Augmented Generation](https://arxiv.org/abs/2506.06704)
*Weihang Su,Qingyao Ai,Jingtao Zhan,Qian Dong,Yiqun Liu*

Main category: cs.CL

TL;DR: 本文介绍了动态RAG和参数化RAG两种新兴研究方向，旨在解决传统RAG在复杂任务中的局限性。


<details>
  <summary>Details</summary>
Motivation: 传统RAG系统采用静态检索和上下文知识注入，难以满足多跳推理和自适应信息访问的需求。

Method: 动态RAG实时调整检索时机和内容，参数化RAG从输入级转向参数级知识注入。

Result: 动态RAG和参数化RAG提高了知识检索和生成的效率与效果。

Conclusion: 这两种方法为RAG研究提供了新的方向和理论基础。

Abstract: Retrieval-Augmented Generation (RAG) has become a foundational paradigm for
equipping large language models (LLMs) with external knowledge, playing a
critical role in information retrieval and knowledge-intensive applications.
However, conventional RAG systems typically adopt a static
retrieve-then-generate pipeline and rely on in-context knowledge injection,
which can be suboptimal for complex tasks that require multihop reasoning,
adaptive information access, and deeper integration of external knowledge.
Motivated by these limitations, the research community has moved beyond static
retrieval and in-context knowledge injection. Among the emerging directions,
this tutorial delves into two rapidly growing and complementary research areas
on RAG: Dynamic RAG and Parametric RAG. Dynamic RAG adaptively determines when
and what to retrieve during the LLM's generation process, enabling real-time
adaptation to the LLM's evolving information needs. Parametric RAG rethinks how
retrieved knowledge should be injected into LLMs, transitioning from
input-level to parameter-level knowledge injection for enhanced efficiency and
effectiveness. This tutorial offers a comprehensive overview of recent advances
in these emerging research areas. It also shares theoretical foundations and
practical insights to support and inspire further research in RAG.

</details>


### [112] [DivScore: Zero-Shot Detection of LLM-Generated Text in Specialized Domains](https://arxiv.org/abs/2506.06705)
*Zhihui Chen,Kai He,Yucheng Huang,Yunxiao Zhu,Mengling Feng*

Main category: cs.CL

TL;DR: 论文提出了一种名为DivScore的零样本检测框架，用于在医学和法律等专业领域检测LLM生成的文本，解决了现有检测器因领域偏移而失效的问题。


<details>
  <summary>Details</summary>
Motivation: 在医学和法律等高风险领域检测LLM生成的文本对打击错误信息和确保真实性至关重要，但现有零样本检测器在专业内容上表现不佳。

Method: 通过理论分析KL散度问题，提出DivScore框架，结合归一化熵评分和领域知识蒸馏，并发布了医学和法律领域的基准数据集。

Result: 实验表明DivScore在AUROC和召回率上分别比现有方法高出14.4%和64.0%，在对抗性环境下表现更稳健。

Conclusion: DivScore在专业领域检测LLM生成文本方面具有显著优势，代码和数据已公开。

Abstract: Detecting LLM-generated text in specialized and high-stakes domains like
medicine and law is crucial for combating misinformation and ensuring
authenticity. However, current zero-shot detectors, while effective on general
text, often fail when applied to specialized content due to domain shift. We
provide a theoretical analysis showing this failure is fundamentally linked to
the KL divergence between human, detector, and source text distributions. To
address this, we propose DivScore, a zero-shot detection framework using
normalized entropy-based scoring and domain knowledge distillation to robustly
identify LLM-generated text in specialized domains. We also release a
domain-specific benchmark for LLM-generated text detection in the medical and
legal domains. Experiments on our benchmark show that DivScore consistently
outperforms state-of-the-art detectors, with 14.4% higher AUROC and 64.0%
higher recall (0.1% false positive rate threshold). In adversarial settings,
DivScore demonstrates superior robustness than other baselines, achieving on
average 22.8% advantage in AUROC and 29.5% in recall. Code and data are
publicly available.

</details>


### [113] [Silencing Empowerment, Allowing Bigotry: Auditing the Moderation of Hate Speech on Twitch](https://arxiv.org/abs/2506.07667)
*Prarabdh Shukla,Wei Yin Chong,Yash Patel,Brennan Schaffner,Danish Pruthi,Arjun Bhagoji*

Main category: cs.CL

TL;DR: 论文审计了Twitch的自动审核工具AutoMod，发现其在识别仇恨内容时存在显著漏洞，94%的仇恨内容未被标记，同时误删大量良性内容。


<details>
  <summary>Details</summary>
Motivation: 研究在线平台自动审核系统的有效性，尤其是针对实时互动内容（如直播评论）的审核能力。

Method: 通过创建测试账户，利用Twitch API发送10.7万条评论（来自4个数据集），测试AutoMod对仇恨内容的标记准确性。

Result: AutoMod漏标高达94%的仇恨内容，但对含敏感词的良性内容误删率达89.5%。

Conclusion: AutoMod依赖敏感词而非上下文，审核能力存在重大缺陷，需改进上下文理解。

Abstract: To meet the demands of content moderation, online platforms have resorted to
automated systems. Newer forms of real-time engagement($\textit{e.g.}$, users
commenting on live streams) on platforms like Twitch exert additional pressures
on the latency expected of such moderation systems. Despite their prevalence,
relatively little is known about the effectiveness of these systems. In this
paper, we conduct an audit of Twitch's automated moderation tool
($\texttt{AutoMod}$) to investigate its effectiveness in flagging hateful
content. For our audit, we create streaming accounts to act as siloed test
beds, and interface with the live chat using Twitch's APIs to send over
$107,000$ comments collated from $4$ datasets. We measure $\texttt{AutoMod}$'s
accuracy in flagging blatantly hateful content containing misogyny, racism,
ableism and homophobia. Our experiments reveal that a large fraction of hateful
messages, up to $94\%$ on some datasets, $\textit{bypass moderation}$.
Contextual addition of slurs to these messages results in $100\%$ removal,
revealing $\texttt{AutoMod}$'s reliance on slurs as a moderation signal. We
also find that contrary to Twitch's community guidelines, $\texttt{AutoMod}$
blocks up to $89.5\%$ of benign examples that use sensitive words in
pedagogical or empowering contexts. Overall, our audit points to large gaps in
$\texttt{AutoMod}$'s capabilities and underscores the importance for such
systems to understand context effectively.

</details>


### [114] [A Survey of Retentive Network](https://arxiv.org/abs/2506.06708)
*Haiqi Yang,Zhiyuan Li,Yi Chang,Yuan Wu*

Main category: cs.CL

TL;DR: RetNet是一种新型神经网络架构，通过保留机制解决了Transformer的高内存和扩展性问题，同时支持并行训练，并在多个领域表现出色。


<details>
  <summary>Details</summary>
Motivation: 解决Transformer在处理长序列时的高内存消耗和扩展性限制问题。

Method: 引入保留机制，结合循环的归纳偏置和注意力机制的全局依赖建模，实现线性时间推理和高效上下文建模。

Result: RetNet在自然语言处理、语音识别和时间序列分析等领域表现出色。

Conclusion: RetNet是一种有前景的架构，但仍需进一步研究以解决其挑战并推动实际应用。

Abstract: Retentive Network (RetNet) represents a significant advancement in neural
network architecture, offering an efficient alternative to the Transformer.
While Transformers rely on self-attention to model dependencies, they suffer
from high memory costs and limited scalability when handling long sequences due
to their quadratic complexity. To mitigate these limitations, RetNet introduces
a retention mechanism that unifies the inductive bias of recurrence with the
global dependency modeling of attention. This mechanism enables linear-time
inference, facilitates efficient modeling of extended contexts, and remains
compatible with fully parallelizable training pipelines. RetNet has garnered
significant research interest due to its consistently demonstrated cross-domain
effectiveness, achieving robust performance across machine learning paradigms
including natural language processing, speech recognition, and time-series
analysis. However, a comprehensive review of RetNet is still missing from the
current literature. This paper aims to fill that gap by offering the first
detailed survey of the RetNet architecture, its key innovations, and its
diverse applications. We also explore the main challenges associated with
RetNet and propose future research directions to support its continued
advancement in both academic research and practical deployment.

</details>


### [115] [C-PATH: Conversational Patient Assistance and Triage in Healthcare System](https://arxiv.org/abs/2506.06737)
*Qi Shi,Qiwei Han,Cláudia Soares*

Main category: cs.CL

TL;DR: C-PATH是一种基于LLM的对话AI系统，旨在通过自然对话帮助患者识别症状并推荐医疗科室。


<details>
  <summary>Details</summary>
Motivation: 医疗系统复杂，患者难以获取及时和适当的医疗帮助，C-PATH旨在解决这一问题。

Method: 基于LLaMA3架构，通过多阶段管道微调医学知识、对话数据和临床摘要，并采用GPT数据增强框架和对话历史管理策略。

Result: 在清晰度、信息量和推荐准确性方面表现优异，显著优于领域基线。

Conclusion: C-PATH是数字健康辅助和分诊领域的进步，提供用户友好且准确的AI工具。

Abstract: Navigating healthcare systems can be complex and overwhelming, creating
barriers for patients seeking timely and appropriate medical attention. In this
paper, we introduce C-PATH (Conversational Patient Assistance and Triage in
Healthcare), a novel conversational AI system powered by large language models
(LLMs) designed to assist patients in recognizing symptoms and recommending
appropriate medical departments through natural, multi-turn dialogues. C-PATH
is fine-tuned on medical knowledge, dialogue data, and clinical summaries using
a multi-stage pipeline built on the LLaMA3 architecture. A core contribution of
this work is a GPT-based data augmentation framework that transforms structured
clinical knowledge from DDXPlus into lay-person-friendly conversations,
allowing alignment with patient communication norms. We also implement a
scalable conversation history management strategy to ensure long-range
coherence. Evaluation with GPTScore demonstrates strong performance across
dimensions such as clarity, informativeness, and recommendation accuracy.
Quantitative benchmarks show that C-PATH achieves superior performance in
GPT-rewritten conversational datasets, significantly outperforming
domain-specific baselines. C-PATH represents a step forward in the development
of user-centric, accessible, and accurate AI tools for digital health
assistance and triage.

</details>


### [116] [Geopolitical biases in LLMs: what are the "good" and the "bad" countries according to contemporary language models](https://arxiv.org/abs/2506.06751)
*Mikhail Salnikov,Dmitrii Korzh,Ivan Lazichny,Elvir Karimov,Artyom Iudin,Ivan Oseledets,Oleg Y. Rogov,Alexander Panchenko,Natalia Loukachevitch,Elena Tutubalina*

Main category: cs.CL

TL;DR: 论文评估了LLMs在地缘政治偏见方面的表现，分析了其对历史事件的不同国家视角（美、英、苏、中）的解读，发现模型存在显著偏见，且简单去偏见方法效果有限。


<details>
  <summary>Details</summary>
Motivation: 研究LLMs在处理具有冲突国家视角的历史事件时是否存在地缘政治偏见，并探讨去偏见方法的有效性。

Method: 通过构建包含中性事件描述和不同国家观点的数据集，测试LLMs的偏见表现，并尝试简单去偏见提示。

Result: LLMs表现出显著的地缘政治偏见，简单去偏见方法效果有限，模型对标签敏感且可能放大偏见。

Conclusion: 研究揭示了LLMs的国家叙事偏见，挑战了简单去偏见方法的有效性，并提供了未来研究的框架和数据集。

Abstract: This paper evaluates geopolitical biases in LLMs with respect to various
countries though an analysis of their interpretation of historical events with
conflicting national perspectives (USA, UK, USSR, and China). We introduce a
novel dataset with neutral event descriptions and contrasting viewpoints from
different countries. Our findings show significant geopolitical biases, with
models favoring specific national narratives. Additionally, simple debiasing
prompts had a limited effect in reducing these biases. Experiments with
manipulated participant labels reveal models' sensitivity to attribution,
sometimes amplifying biases or recognizing inconsistencies, especially with
swapped labels. This work highlights national narrative biases in LLMs,
challenges the effectiveness of simple debiasing methods, and offers a
framework and dataset for future geopolitical bias research.

</details>


### [117] [They want to pretend not to understand: The Limits of Current LLMs in Interpreting Implicit Content of Political Discourse](https://arxiv.org/abs/2506.06775)
*Walter Paci,Alessandro Panunzi,Sandro Pezzelle*

Main category: cs.CL

TL;DR: 论文探讨了大型语言模型（LLMs）在政治话语中检测和解释隐含内容的能力，发现当前模型在理解预设和隐含意义方面存在不足。


<details>
  <summary>Details</summary>
Motivation: 研究LLMs在政治话语中处理隐含内容的能力，填补现有研究的空白。

Method: 利用IMPAQTS语料库，通过多项选择和开放式生成任务测试LLMs的表现。

Result: 所有测试模型在解释预设和隐含意义时表现不佳。

Conclusion: 当前LLMs缺乏关键语用能力，但研究指出了未来改进的方向。

Abstract: Implicit content plays a crucial role in political discourse, where speakers
systematically employ pragmatic strategies such as implicatures and
presuppositions to influence their audiences. Large Language Models (LLMs) have
demonstrated strong performance in tasks requiring complex semantic and
pragmatic understanding, highlighting their potential for detecting and
explaining the meaning of implicit content. However, their ability to do this
within political discourse remains largely underexplored. Leveraging, for the
first time, the large IMPAQTS corpus, which comprises Italian political
speeches with the annotation of manipulative implicit content, we propose
methods to test the effectiveness of LLMs in this challenging problem. Through
a multiple-choice task and an open-ended generation task, we demonstrate that
all tested models struggle to interpret presuppositions and implicatures. We
conclude that current LLMs lack the key pragmatic capabilities necessary for
accurately interpreting highly implicit language, such as that found in
political discourse. At the same time, we highlight promising trends and future
directions for enhancing model performance. We release our data and code at
https://github.com/WalterPaci/IMPAQTS-PID

</details>


### [118] [Extending dependencies to the taggedPBC: Word order in transitive clauses](https://arxiv.org/abs/2506.06785)
*Hiram Ring*

Main category: cs.CL

TL;DR: 论文介绍了taggedPBC数据集的CoNLLU格式版本，新增了依存关系标注，并验证了其与专家确定的词序相关性。


<details>
  <summary>Details</summary>
Motivation: taggedPBC数据集虽包含大量语言数据，但未标注依存关系，限制了其应用潜力。

Method: 通过CoNLLU格式将依存关系信息与POS标签一起转移到所有语言中。

Result: 数据集中的词序信息与专家确定的词序相关，验证了其有效性。

Conclusion: 即使数据存在噪声，足够标注仍能提供重要见解，数据集已开源供研究使用。

Abstract: The taggedPBC (Ring 2025a) contains more than 1,800 sentences of pos-tagged
parallel text data from over 1,500 languages, representing 133 language
families and 111 isolates. While this dwarfs previously available resources,
and the POS tags achieve decent accuracy, allowing for predictive
crosslinguistic insights (Ring 2025b), the dataset was not initially annotated
for dependencies. This paper reports on a CoNLLU-formatted version of the
dataset which transfers dependency information along with POS tags to all
languages in the taggedPBC. Although there are various concerns regarding the
quality of the tags and the dependencies, word order information derived from
this dataset regarding the position of arguments and predicates in transitive
clauses correlates with expert determinations of word order in three
typological databases (WALS, Grambank, Autotyp). This highlights the usefulness
of corpus-based typological approaches (as per Baylor et al. 2023; Bjerva 2024)
for extending comparisons of discrete linguistic categories, and suggests that
important insights can be gained even from noisy data, given sufficient
annotation. The dependency-annotated corpora are also made available for
research and collaboration via GitHub.

</details>


### [119] [On the Adaptive Psychological Persuasion of Large Language Models](https://arxiv.org/abs/2506.06800)
*Tianjie Ju,Yujia Chen,Hao Fei,Mong-Li Lee,Wynne Hsu,Pengzhou Cheng,Zongru Wu,Zhuosheng Zhang,Gongshen Liu*

Main category: cs.CL

TL;DR: 论文探讨了大型语言模型（LLMs）在自主说服和抵抗说服方面的能力，提出了基于心理策略的自适应框架，显著提高了说服成功率。


<details>
  <summary>Details</summary>
Motivation: 系统研究LLMs在心理修辞情境下的说服与抵抗能力，填补现有研究的空白。

Method: 评估四种LLMs在对抗性对话中的表现，引入11种心理说服策略，并提出基于直接偏好优化的自适应框架。

Result: 实验证明，特定策略（如流畅效应和重复效应）显著提高说服成功率，自适应框架进一步优化策略选择。

Conclusion: 自适应心理说服方法有效提升LLMs的说服能力，同时保持其通用性。

Abstract: Previous work has showcased the intriguing capabilities of Large Language
Models (LLMs) in instruction-following and rhetorical fluency. However,
systematic exploration of their dual capabilities to autonomously persuade and
resist persuasion, particularly in contexts involving psychological rhetoric,
remains unexplored. In this paper, we first evaluate four commonly adopted LLMs
by tasking them to alternately act as persuaders and listeners in adversarial
dialogues. Empirical results show that persuader LLMs predominantly employ
repetitive strategies, leading to low success rates. Then we introduce eleven
comprehensive psychological persuasion strategies, finding that explicitly
instructing LLMs to adopt specific strategies such as Fluency Effect and
Repetition Effect significantly improves persuasion success rates. However, no
``one-size-fits-all'' strategy proves universally effective, with performance
heavily dependent on contextual counterfactuals. Motivated by these
observations, we propose an adaptive framework based on direct preference
optimization that trains LLMs to autonomously select optimal strategies by
leveraging persuasion results from strategy-specific responses as preference
pairs. Experiments on three open-source LLMs confirm that the proposed adaptive
psychological persuasion method effectively enables persuader LLMs to select
optimal strategies, significantly enhancing their success rates while
maintaining general capabilities. Our code is available at
https://github.com/KalinaEine/PsychologicalPersuasion.

</details>


### [120] [Label-semantics Aware Generative Approach for Domain-Agnostic Multilabel Classification](https://arxiv.org/abs/2506.06806)
*Subhendu Khatuya,Shashwat Naidu,Saptarshi Ghosh,Pawan Goyal,Niloy Ganguly*

Main category: cs.CL

TL;DR: 提出了一种基于生成模型的多标签文本分类框架LAGAMC，通过生成标签描述并匹配预定义标签，结合双目标损失函数，实现了高效且通用的分类性能。


<details>
  <summary>Details</summary>
Motivation: 随着文本数据的爆炸式增长，手动文档分类变得愈发困难，需要一种高效且通用的自动分类方法。

Method: 利用预定义的标签描述，训练模型生成这些描述，并通过微调的句子转换器匹配标签；结合交叉熵损失和余弦相似度的双目标损失函数。

Result: LAGAMC在所有评估数据集上均达到新的最先进性能，Micro-F1和Macro-F1分别提升13.94%和24.85%。

Conclusion: LAGAMC以其参数高效性和跨数据集的通用性，成为实际应用的理想选择。

Abstract: The explosion of textual data has made manual document classification
increasingly challenging. To address this, we introduce a robust, efficient
domain-agnostic generative model framework for multi-label text classification.
Instead of treating labels as mere atomic symbols, our approach utilizes
predefined label descriptions and is trained to generate these descriptions
based on the input text. During inference, the generated descriptions are
matched to the pre-defined labels using a finetuned sentence transformer. We
integrate this with a dual-objective loss function, combining cross-entropy
loss and cosine similarity of the generated sentences with the predefined
target descriptions, ensuring both semantic alignment and accuracy. Our
proposed model LAGAMC stands out for its parameter efficiency and versatility
across diverse datasets, making it well-suited for practical applications. We
demonstrate the effectiveness of our proposed model by achieving new
state-of-the-art performances across all evaluated datasets, surpassing several
strong baselines. We achieve improvements of 13.94% in Micro-F1 and 24.85% in
Macro-F1 compared to the closest baseline across all datasets.

</details>


### [121] [Not quite Sherlock Holmes: Language model predictions do not reliably differentiate impossible from improbable events](https://arxiv.org/abs/2506.06808)
*James A. Michaelov,Reeka Estacio,Zhien Zhang,Benjamin K. Bergen*

Main category: cs.CL

TL;DR: 语言模型在区分可能事件与不可能事件时表现不佳，甚至在某些情况下表现低于随机水平。


<details>
  <summary>Details</summary>
Motivation: 探讨语言模型是否能可靠区分可能事件与不可能事件，并分析其表现。

Method: 通过分离可能性、典型性和上下文相关性，测试多种语言模型（如Llama 3、Gemma 2、Mistral NeMo）。

Result: 所有测试模型在某些条件下表现低于随机水平，甚至对不可能句子赋予更高概率。

Conclusion: 语言模型在区分可能事件与不可能事件时表现不稳健，需进一步改进。

Abstract: Can language models reliably predict that possible events are more likely
than merely improbable ones? By teasing apart possibility, typicality, and
contextual relatedness, we show that despite the results of previous work,
language models' ability to do this is far from robust. In fact, under certain
conditions, all models tested - including Llama 3, Gemma 2, and Mistral NeMo -
perform at worse-than-chance level, assigning higher probabilities to
impossible sentences such as 'the car was given a parking ticket by the brake'
than to merely unlikely sentences such as 'the car was given a parking ticket
by the explorer'.

</details>


### [122] [Advancing Question Generation with Joint Narrative and Difficulty Control](https://arxiv.org/abs/2506.06812)
*Bernardo Leite,Henrique Lopes Cardoso*

Main category: cs.CL

TL;DR: 论文提出了一种联合控制叙事和难度的方法，用于生成阅读理解问题，初步验证了其可行性，但效果并不普遍适用。


<details>
  <summary>Details</summary>
Motivation: 现有研究缺乏同时控制问题难度和叙事的方法，而这对教育目的的问题生成至关重要。

Method: 提出了一种联合叙事和难度控制的策略，用于生成阅读理解问题。

Result: 初步验证了方法的可行性，但效果因情况而异，研究还明确了其适用条件和权衡。

Conclusion: 该方法为教育目的的问题生成提供了新思路，但需进一步优化以适应更广泛场景。

Abstract: Question Generation (QG), the task of automatically generating questions from
a source input, has seen significant progress in recent years.
Difficulty-controllable QG (DCQG) enables control over the difficulty level of
generated questions while considering the learner's ability. Additionally,
narrative-controllable QG (NCQG) allows control over the narrative aspects
embedded in the questions. However, research in QG lacks a focus on combining
these two types of control, which is important for generating questions
tailored to educational purposes. To address this gap, we propose a strategy
for Joint Narrative and Difficulty Control, enabling simultaneous control over
these two attributes in the generation of reading comprehension questions. Our
evaluation provides preliminary evidence that this approach is feasible, though
it is not effective across all instances. Our findings highlight the conditions
under which the strategy performs well and discuss the trade-offs associated
with its application.

</details>


### [123] [Can LLMs Generate Reliable Test Case Generators? A Study on Competition-Level Programming Problems](https://arxiv.org/abs/2506.06821)
*Yuhan Cao,Zian Chen,Kun Quan,Ziliang Zhang,Yu Wang,Xiaoning Dong,Yeqi Feng,Guanzhong He,Jingcheng Huang,Jianhao Li,Yixuan Tan,Jiafu Tang,Yilin Tang,Junlei Wu,Qianyu Xiao,Can Zheng,Shouchen Zhou,Yuxiang Zhu,Yiming Huang,Tian Xie,Tianxing He*

Main category: cs.CL

TL;DR: 论文研究了大型语言模型（LLMs）在代码检查或调试中通过测试用例生成的能力，提出了TCGBench基准测试，发现LLMs能生成有效测试用例生成器，但在针对性测试用例生成上表现不佳。


<details>
  <summary>Details</summary>
Motivation: 探索LLMs在代码检查或调试中通过测试用例生成的应用潜力，特别是在竞赛级编程（CP）领域。

Method: 提出TCGBench基准测试，包含两项任务：生成有效测试用例生成器和针对性测试用例生成器。实验评估了LLMs的表现。

Result: LLMs能生成有效测试用例生成器，但在针对性测试用例生成上表现不佳，即使高级推理模型也远不及人类。通过高质量数据集，LLMs性能可提升。

Conclusion: LLMs在测试用例生成上有潜力，但针对性生成仍需改进。高质量数据集可帮助提升性能。

Abstract: Large Language Models (LLMs) have demonstrated remarkable capabilities in
code generation, capable of tackling complex tasks during inference. However,
the extent to which LLMs can be utilized for code checking or debugging through
test case generation remains largely unexplored. We investigate this problem
from the perspective of competition-level programming (CP) programs and propose
TCGBench, a Benchmark for (LLM generation of) Test Case Generators. This
benchmark comprises two tasks, aimed at studying the capabilities of LLMs in
(1) generating valid test case generators for a given CP problem, and further
(2) generating targeted test case generators that expose bugs in human-written
code. Experimental results indicate that while state-of-the-art LLMs can
generate valid test case generators in most cases, most LLMs struggle to
generate targeted test cases that reveal flaws in human code effectively.
Especially, even advanced reasoning models (e.g., o3-mini) fall significantly
short of human performance in the task of generating targeted generators.
Furthermore, we construct a high-quality, manually curated dataset of
instructions for generating targeted generators. Analysis demonstrates that the
performance of LLMs can be enhanced with the aid of this dataset, by both
prompting and fine-tuning.

</details>


### [124] [PCoT: Persuasion-Augmented Chain of Thought for Detecting Fake News and Social Media Disinformation](https://arxiv.org/abs/2506.06842)
*Arkadiusz Modzelewski,Witold Sosnowski,Tiziano Labruna,Adam Wierzbicki,Giovanni Da San Martino*

Main category: cs.CL

TL;DR: 论文提出了一种名为PCoT的新方法，通过注入说服知识提升零样本分类中的虚假信息检测能力，并在多个数据集上验证了其优越性。


<details>
  <summary>Details</summary>
Motivation: 心理学研究表明，了解说服谬误有助于检测虚假信息，因此作者尝试利用大语言模型（LLMs）验证这一假设。

Method: 提出Persuasion-Augmented Chain of Thought (PCoT)方法，结合说服知识改进虚假信息检测。

Result: PCoT在五个LLMs和五个数据集上的平均表现优于竞争方法15%。

Conclusion: 研究表明，说服知识能有效增强零样本虚假信息检测能力。

Abstract: Disinformation detection is a key aspect of media literacy. Psychological
studies have shown that knowledge of persuasive fallacies helps individuals
detect disinformation. Inspired by these findings, we experimented with large
language models (LLMs) to test whether infusing persuasion knowledge enhances
disinformation detection. As a result, we introduce the Persuasion-Augmented
Chain of Thought (PCoT), a novel approach that leverages persuasion to improve
disinformation detection in zero-shot classification. We extensively evaluate
PCoT on online news and social media posts. Moreover, we publish two novel,
up-to-date disinformation datasets: EUDisinfo and MultiDis. These datasets
enable the evaluation of PCoT on content entirely unseen by the LLMs used in
our experiments, as the content was published after the models' knowledge
cutoffs. We show that, on average, PCoT outperforms competitive methods by 15%
across five LLMs and five datasets. These findings highlight the value of
persuasion in strengthening zero-shot disinformation detection.

</details>


### [125] [Adapt Once, Thrive with Updates: Transferable Parameter-Efficient Fine-Tuning on Evolving Base Models](https://arxiv.org/abs/2506.06844)
*Naibin Gu,Peng Fu,Xiyu Liu,Ke Ma,Zheng Lin,Weiping Wang*

Main category: cs.CL

TL;DR: Trans-PEFT是一种新方法，通过专注于任务特定模式并减少对基础模型知识的依赖，解决了PEFT模块在基础模型更新后性能下降的问题。


<details>
  <summary>Details</summary>
Motivation: 基础模型更新后，PEFT模块性能显著下降，重新调整这些模块计算成本高。

Method: 分析基础模型更新变化，发现持续训练主要影响FFN中的任务特定知识，而对注意力机制影响较小。基于此，提出Trans-PEFT，专注于任务特定模式。

Result: 在7个基础模型和12个数据集上的实验表明，Trans-PEFT无需重新调整即可保持性能。

Conclusion: Trans-PEFT显著减少了实际应用中的维护开销。

Abstract: Parameter-efficient fine-tuning (PEFT) has become a common method for
fine-tuning large language models, where a base model can serve multiple users
through PEFT module switching. To enhance user experience, base models require
periodic updates. However, once updated, PEFT modules fine-tuned on previous
versions often suffer substantial performance degradation on newer versions.
Re-tuning these numerous modules to restore performance would incur significant
computational costs. Through a comprehensive analysis of the changes that occur
during base model updates, we uncover an interesting phenomenon: continual
training primarily affects task-specific knowledge stored in Feed-Forward
Networks (FFN), while having less impact on the task-specific pattern in the
Attention mechanism. Based on these findings, we introduce Trans-PEFT, a novel
approach that enhances the PEFT module by focusing on the task-specific pattern
while reducing its dependence on certain knowledge in the base model. Further
theoretical analysis supports our approach. Extensive experiments across 7 base
models and 12 datasets demonstrate that Trans-PEFT trained modules can maintain
performance on updated base models without re-tuning, significantly reducing
maintenance overhead in real-world applications.

</details>


### [126] [Right Is Not Enough: The Pitfalls of Outcome Supervision in Training LLMs for Math Reasoning](https://arxiv.org/abs/2506.06877)
*Jiaxing Guo,Wenjie Yang,Shengzhong Zhang,Tongshan Xu,Lun Du,Da Zheng,Zengfeng Huang*

Main category: cs.CL

TL;DR: 论文指出，尽管奖励驱动的LLMs在数学问题解决上表现优异，但其推理过程常存在根本性缺陷。作者提出MathOlympiadEval数据集和ParaStepVerifier方法，以更精准地检测推理错误。


<details>
  <summary>Details</summary>
Motivation: 现有LLMs在数学问题中常通过不合理的推理得出正确答案（奖励黑客行为），而现有评估方法难以可靠检测这些缺陷。

Method: 提出MathOlympiadEval数据集和ParaStepVerifier方法，逐步验证数学推理步骤。

Result: ParaStepVerifier显著提高了对复杂多步问题推理错误的检测准确率。

Conclusion: 该方法为LLMs的真实数学推理能力评估和训练提供了更可靠的路径。

Abstract: Outcome-rewarded Large Language Models (LLMs) have demonstrated remarkable
success in mathematical problem-solving. However, this success often masks a
critical issue: models frequently achieve correct answers through fundamentally
unsound reasoning processes, a phenomenon indicative of reward hacking. We
introduce MathOlympiadEval, a new dataset with fine-grained annotations, which
reveals a significant gap between LLMs' answer correctness and their low
process correctness. Existing automated methods like LLM-as-a-judge struggle to
reliably detect these reasoning flaws. To address this, we propose
ParaStepVerifier, a novel methodology for meticulous, step-by-step verification
of mathematical solutions. ParaStepVerifier identifies incorrect reasoning
steps. Empirical results demonstrate that ParaStepVerifier substantially
improves the accuracy of identifying flawed solutions compared to baselines,
especially for complex, multi-step problems. This offers a more robust path
towards evaluating and training LLMs with genuine mathematical reasoning.

</details>


### [127] [Mixture of Small and Large Models for Chinese Spelling Check](https://arxiv.org/abs/2506.06887)
*Ziheng Qiao,Houquan Zhou,Zhenghua Li*

Main category: cs.CL

TL;DR: 提出了一种动态混合方法，结合小模型和LLM的概率分布，提升中文拼写检查任务的性能，无需微调LLM。


<details>
  <summary>Details</summary>
Motivation: 现有LLM方法在中文拼写检查任务中表现不佳，而微调BERT模型存在编辑模式过拟合问题。

Method: 在beam search解码阶段动态混合小模型和LLM的概率分布，平衡精确性和流畅性。

Result: 实验表明该方法显著提升纠错能力，在多个数据集上达到最优效果。

Conclusion: 动态混合方法有效结合小模型和LLM的优势，节省资源并提升性能。

Abstract: In the era of large language models (LLMs), the Chinese Spelling Check (CSC)
task has seen various LLM methods developed, yet their performance remains
unsatisfactory. In contrast, fine-tuned BERT-based models, relying on
high-quality in-domain data, show excellent performance but suffer from edit
pattern overfitting. This paper proposes a novel dynamic mixture approach that
effectively combines the probability distributions of small models and LLMs
during the beam search decoding phase, achieving a balanced enhancement of
precise corrections from small models and the fluency of LLMs. This approach
also eliminates the need for fine-tuning LLMs, saving significant time and
resources, and facilitating domain adaptation. Comprehensive experiments
demonstrate that our mixture approach significantly boosts error correction
capabilities, achieving state-of-the-art results across multiple datasets. Our
code is available at https://github.com/zhqiao-nlp/MSLLM.

</details>


### [128] [Hybrid Extractive Abstractive Summarization for Multilingual Sentiment Analysis](https://arxiv.org/abs/2506.06929)
*Mikhail Krasitskii,Grigori Sidorov,Olga Kolesnikova,Liliana Chanona Hernandez,Alexander Gelbukh*

Main category: cs.CL

TL;DR: 提出了一种结合提取式和生成式摘要的混合方法，用于多语言情感分析，显著提升了准确性和计算效率。


<details>
  <summary>Details</summary>
Motivation: 解决单独使用提取式或生成式方法的局限性，特别是在多语言和低资源语言场景下。

Method: 结合TF-IDF提取和微调的XLM-R生成模块，采用动态阈值和文化适应技术。

Result: 在10种语言中表现优于基线，英语准确率0.90，低资源语言0.84，计算效率提升22%。

Conclusion: 方法适用于实时品牌监控和跨文化分析，未来将优化低资源语言的8位量化。

Abstract: We propose a hybrid approach for multilingual sentiment analysis that
combines extractive and abstractive summarization to address the limitations of
standalone methods. The model integrates TF-IDF-based extraction with a
fine-tuned XLM-R abstractive module, enhanced by dynamic thresholding and
cultural adaptation. Experiments across 10 languages show significant
improvements over baselines, achieving 0.90 accuracy for English and 0.84 for
low-resource languages. The approach also demonstrates 22% greater
computational efficiency than traditional methods. Practical applications
include real-time brand monitoring and cross-cultural discourse analysis.
Future work will focus on optimization for low-resource languages via 8-bit
quantization.

</details>


### [129] [DiscoSum: Discourse-aware News Summarization](https://arxiv.org/abs/2506.06930)
*Alexander Spangher,Tenghao Huang,Jialiang Gu,Jiatong Shi,Muhao Chen*

Main category: cs.CL

TL;DR: 论文提出了一种结合新闻语篇结构的摘要生成方法，通过新数据集和算法DiscoSum，优化了摘要的叙事连贯性和结构适应性。


<details>
  <summary>Details</summary>
Motivation: 现有大型语言模型在生成摘要时难以保持长期语篇结构，尤其新闻文章的结构对读者参与度影响显著。

Method: 提出新闻语篇结构模式，开发算法DiscoSum，利用束搜索技术实现结构感知的摘要生成。

Result: 人工和自动评估表明，该方法在保持叙事忠实度和满足结构需求方面表现优异。

Conclusion: 结合语篇结构的摘要方法能有效提升新闻摘要的质量和适应性。

Abstract: Recent advances in text summarization have predominantly leveraged large
language models to generate concise summaries. However, language models often
do not maintain long-term discourse structure, especially in news articles,
where organizational flow significantly influences reader engagement. We
introduce a novel approach to integrating discourse structure into
summarization processes, focusing specifically on news articles across various
media. We present a novel summarization dataset where news articles are
summarized multiple times in different ways across different social media
platforms (e.g. LinkedIn, Facebook, etc.). We develop a novel news discourse
schema to describe summarization structures and a novel algorithm, DiscoSum,
which employs beam search technique for structure-aware summarization, enabling
the transformation of news stories to meet different stylistic and structural
demands. Both human and automatic evaluation results demonstrate the efficacy
of our approach in maintaining narrative fidelity and meeting structural
requirements.

</details>


### [130] [What Makes a Good Natural Language Prompt?](https://arxiv.org/abs/2506.06950)
*Do Xuan Long,Duy Dinh,Ngoc-Hai Nguyen,Kenji Kawaguchi,Nancy F. Chen,Shafiq Joty,Min-Yen Kan*

Main category: cs.CL

TL;DR: 论文通过元分析提出了一种基于属性和人类中心的提示质量评估框架，揭示了现有研究的不足，并通过实验验证了多属性提示优化的效果。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型的发展，提示成为人机交互的关键，但缺乏对自然语言提示的量化共识。

Method: 对150多篇相关论文进行元分析，提出21个属性分类的评估框架，并实证研究多属性提示优化。

Result: 单属性优化效果显著，基于属性增强提示的指令调优能提升模型推理能力。

Conclusion: 研究为提示评估与优化奠定了基础，推动了人机交互与提示研究的发展。

Abstract: As large language models (LLMs) have progressed towards more human-like and
human--AI communications have become prevalent, prompting has emerged as a
decisive component. However, there is limited conceptual consensus on what
exactly quantifies natural language prompts. We attempt to address this
question by conducting a meta-analysis surveying more than 150
prompting-related papers from leading NLP and AI conferences from 2022 to 2025
and blogs. We propose a property- and human-centric framework for evaluating
prompt quality, encompassing 21 properties categorized into six dimensions. We
then examine how existing studies assess their impact on LLMs, revealing their
imbalanced support across models and tasks, and substantial research gaps.
Further, we analyze correlations among properties in high-quality natural
language prompts, deriving prompting recommendations. We then empirically
explore multi-property prompt enhancements in reasoning tasks, observing that
single-property enhancements often have the greatest impact. Finally, we
discover that instruction-tuning on property-enhanced prompts can result in
better reasoning models. Our findings establish a foundation for
property-centric prompt evaluation and optimization, bridging the gaps between
human--AI communication and opening new prompting research directions.

</details>


### [131] [BIS Reasoning 1.0: The First Large-Scale Japanese Benchmark for Belief-Inconsistent Syllogistic Reasoning](https://arxiv.org/abs/2506.06955)
*Ha-Thanh Nguyen,Chaoran Liu,Hirokazu Kiyomaru,Koichi Takeda,Yusuke Miyao,Maki Matsuda,Yusuke Oda,Pontus Stenetorp,Qianying Liu,Su Myat Noe,Hideyuki Tachibana,Kouta Nakayama,Sadao Kurohashi*

Main category: cs.CL

TL;DR: BIS Reasoning 1.0是首个针对大型语言模型（LLMs）在信念不一致推理中的表现进行评估的大规模日语数据集，揭示了LLMs在处理逻辑有效但信念冲突输入时的弱点。


<details>
  <summary>Details</summary>
Motivation: 现有数据集（如NeuBAROCO和JFLD）主要关注通用或信念一致的推理，而BIS Reasoning 1.0旨在填补信念不一致推理的空白，以评估LLMs在高风险领域中的可靠性。

Method: 通过设计逻辑有效但信念不一致的三段论问题，对GPT、Claude及领先的日语LLMs进行基准测试。

Result: GPT-4o表现最佳，准确率为79.54%，但所有模型在处理信念冲突输入时均存在显著缺陷。

Conclusion: 研究强调了在高风险领域（如法律、医疗和科学文献）中，LLMs需优先逻辑而非直觉信念以确保完整性和安全性。

Abstract: We present BIS Reasoning 1.0, the first large-scale Japanese dataset of
syllogistic reasoning problems explicitly designed to evaluate
belief-inconsistent reasoning in large language models (LLMs). Unlike prior
datasets such as NeuBAROCO and JFLD, which focus on general or belief-aligned
reasoning, BIS Reasoning 1.0 introduces logically valid yet belief-inconsistent
syllogisms to uncover reasoning biases in LLMs trained on human-aligned
corpora. We benchmark state-of-the-art models - including GPT models, Claude
models, and leading Japanese LLMs - revealing significant variance in
performance, with GPT-4o achieving 79.54% accuracy. Our analysis identifies
critical weaknesses in current LLMs when handling logically valid but
belief-conflicting inputs. These findings have important implications for
deploying LLMs in high-stakes domains such as law, healthcare, and scientific
literature, where truth must override intuitive belief to ensure integrity and
safety.

</details>


### [132] [Learning to Clarify by Reinforcement Learning Through Reward-Weighted Fine-Tuning](https://arxiv.org/abs/2506.06964)
*Subhojyoti Mukherjee,Viet Dac Lai,Raghavendra Addanki,Ryan Rossi,Seunghyun Yoon,Trung Bui,Anup Rao,Jayakumar Subramanian,Branislav Kveton*

Main category: cs.CL

TL;DR: 论文提出了一种通过强化学习（RL）训练QA代理以提出澄清问题的方法，并分析了离线RL目标，对比了其他方法。


<details>
  <summary>Details</summary>
Motivation: 提升QA代理的能力，使其能够通过澄清问题更准确地回答用户提问。

Method: 利用强化学习模拟包含澄清问题的对话，并提出离线RL目标（类似于奖励加权的监督微调）。

Result: 在优化奖励和语言质量方面优于基于监督微调和直接偏好优化的方法。

Conclusion: 提出的离线RL方法在QA代理中有效，且优于现有技术。

Abstract: Question answering (QA) agents automatically answer questions posed in
natural language. In this work, we learn to ask clarifying questions in QA
agents. The key idea in our method is to simulate conversations that contain
clarifying questions and learn from them using reinforcement learning (RL). To
make RL practical, we propose and analyze offline RL objectives that can be
viewed as reward-weighted supervised fine-tuning (SFT) and easily optimized in
large language models. Our work stands in a stark contrast to recently proposed
methods, based on SFT and direct preference optimization, which have additional
hyper-parameters and do not directly optimize rewards. We compare to these
methods empirically and report gains in both optimized rewards and language
quality.

</details>


### [133] [A dependently-typed calculus of event telicity and culminativity](https://arxiv.org/abs/2506.06968)
*Pavel Kovalev,Carlo Angiuli*

Main category: cs.CL

TL;DR: 提出一个依赖类型的跨语言框架，用于分析事件的终结性和完成性，并以英语句子为例展示其应用。


<details>
  <summary>Details</summary>
Motivation: 研究事件在语言中的终结性和完成性，提供形式化分析工具。

Method: 框架分为名词域和动词域：名词域建模名词短语的有界性及其与子类型、限定数量和形容词修饰的关系；动词域定义依赖事件演算，将终结事件建模为受事有界的事件，完成事件为达到内在终点的终结事件，并考虑副词修饰。

Result: 框架基于Martin-Löf依赖类型理论的扩展，规则和示例已在Agda证明助手中形式化。

Conclusion: 该框架为语言事件分析提供了形式化工具，适用于跨语言研究。

Abstract: We present a dependently-typed cross-linguistic framework for analyzing the
telicity and culminativity of events, accompanied by examples of using our
framework to model English sentences. Our framework consists of two parts. In
the nominal domain, we model the boundedness of noun phrases and its
relationship to subtyping, delimited quantities, and adjectival modification.
In the verbal domain we define a dependent event calculus, modeling telic
events as those whose undergoer is bounded, culminating events as telic events
that achieve their inherent endpoint, and consider adverbial modification. In
both domains we pay particular attention to associated entailments. Our
framework is defined as an extension of intensional Martin-L\"of dependent type
theory, and the rules and examples in this paper have been formalized in the
Agda proof assistant.

</details>


### [134] [Break-The-Chain: Reasoning Failures in LLMs via Adversarial Prompting in Code Generation](https://arxiv.org/abs/2506.06971)
*Jaechul Roh,Varun Gandhi,Shivani Anilkumar,Arin Garg*

Main category: cs.CL

TL;DR: 论文研究了大型语言模型（LLMs）在复杂推理任务中的鲁棒性，通过引入语义忠实但对抗性结构的提示扰动，发现模型对表面级提示动态敏感，表现脆弱且不可预测。


<details>
  <summary>Details</summary>
Motivation: 探讨LLMs是否真正具备推理能力，还是仅依赖浅层统计模式。

Method: 通过一系列语义忠实但对抗性的提示扰动（如故事重构、无关约束注入等），评估700个LeetCode风格问题的代码生成。

Result: 某些扰动导致性能显著下降（准确率下降达42.1%），而其他扰动则意外提升准确率（最高35.3%）。

Conclusion: 当前推理系统脆弱且不可预测，需更原则性的方法提升推理对齐和提示鲁棒性。

Abstract: Large Language Models (LLMs) have achieved remarkable success in tasks
requiring complex reasoning, such as code generation, mathematical problem
solving, and algorithmic synthesis -- especially when aided by reasoning tokens
and Chain-of-Thought prompting. Yet, a core question remains: do these models
truly reason, or do they merely exploit shallow statistical patterns? In this
paper, we systematically investigate the robustness of reasoning LLMs by
introducing a suite of semantically faithful yet adversarially structured
prompt perturbations. Our evaluation -- spanning 700 perturbed code generations
derived from LeetCode-style problems -- applies transformations such as
storytelling reframing, irrelevant constraint injection, example reordering,
and numeric perturbation. We observe that while certain modifications severely
degrade performance (with accuracy drops up to -42.1%), others surprisingly
improve model accuracy by up to 35.3%, suggesting sensitivity not only to
semantics but also to surface-level prompt dynamics. These findings expose the
fragility and unpredictability of current reasoning systems, underscoring the
need for more principles approaches to reasoning alignments and prompting
robustness. We release our perturbation datasets and evaluation framework to
promote further research in trustworthy and resilient LLM reasoning.

</details>


### [135] [Atomic Reasoning for Scientific Table Claim Verification](https://arxiv.org/abs/2506.06972)
*Yuji Zhang,Qingyun Wang,Cheng Qian,Jiateng Liu,Chenkai Sun,Denghui Zhang,Tarek Abdelzaher,Chengxiang Zhai,Preslav Nakov,Heng Ji*

Main category: cs.CL

TL;DR: 论文提出了一种基于认知负荷理论的模块化推理方法，通过技能链动态组合原子技能，提升科学表格声明的验证准确性，减少认知负荷。


<details>
  <summary>Details</summary>
Motivation: 科学文本的复杂性和高信息密度可能导致非专家误解，现有模型在细粒度推理上表现不足。

Method: 引入技能链模式，动态组合原子技能，减少认知负荷，并创建SciAtomicBench基准进行验证。

Result: 仅用350个微调样本，模型表现优于GPT-4o的思维链方法，达到最优结果。

Conclusion: 模块化推理方法能显著提升科学声明验证的准确性和泛化能力，减少训练数据需求。

Abstract: Scientific texts often convey authority due to their technical language and
complex data. However, this complexity can sometimes lead to the spread of
misinformation. Non-experts are particularly susceptible to misleading claims
based on scientific tables due to their high information density and perceived
credibility. Existing table claim verification models, including
state-of-the-art large language models (LLMs), often struggle with precise
fine-grained reasoning, resulting in errors and a lack of precision in
verifying scientific claims. Inspired by Cognitive Load Theory, we propose that
enhancing a model's ability to interpret table-based claims involves reducing
cognitive load by developing modular, reusable reasoning components (i.e.,
atomic skills). We introduce a skill-chaining schema that dynamically composes
these skills to facilitate more accurate and generalizable reasoning with a
reduced cognitive load. To evaluate this, we create SciAtomicBench, a
cross-domain benchmark with fine-grained reasoning annotations. With only 350
fine-tuning examples, our model trained by atomic reasoning outperforms
GPT-4o's chain-of-thought method, achieving state-of-the-art results with far
less training data.

</details>


### [136] [Chain of Methodologies: Scaling Test Time Computation without Training](https://arxiv.org/abs/2506.06982)
*Cong Liu,Jie Wu,Weigang Wu,Xu Chen,Liang Lin,Wei-Shi Zheng*

Main category: cs.CL

TL;DR: 论文提出Chain of Methodologies (CoM)框架，通过整合人类方法论增强LLMs的复杂推理能力，无需微调即可实现系统化推理。


<details>
  <summary>Details</summary>
Motivation: LLMs在复杂推理任务中表现不佳，因训练数据缺乏深度洞察。

Method: CoM通过用户定义的方法论激活LLMs的元认知能力，实现结构化推理。

Result: 实验表明CoM优于基线方法，展示了无训练提示方法的潜力。

Conclusion: CoM为复杂推理任务提供了稳健解决方案，并缩小了与人类推理水平的差距。

Abstract: Large Language Models (LLMs) often struggle with complex reasoning tasks due
to insufficient in-depth insights in their training data, which are typically
absent in publicly available documents. This paper introduces the Chain of
Methodologies (CoM), an innovative and intuitive prompting framework that
enhances structured thinking by integrating human methodological insights,
enabling LLMs to tackle complex tasks with extended reasoning. CoM leverages
the metacognitive abilities of advanced LLMs, activating systematic reasoning
throught user-defined methodologies without explicit fine-tuning. Experiments
show that CoM surpasses competitive baselines, demonstrating the potential of
training-free prompting methods as robust solutions for complex reasoning tasks
and bridging the gap toward human-level reasoning through human-like
methodological insights.

</details>


### [137] [Cultural Bias Matters: A Cross-Cultural Benchmark Dataset and Sentiment-Enriched Model for Understanding Multimodal Metaphors](https://arxiv.org/abs/2506.06987)
*Senqi Yang,Dongyu Zhang,Jing Ren,Ziqi Xu,Xiuzhen Zhang,Yiliao Song,Hongfei Lin,Feng Xia*

Main category: cs.CL

TL;DR: 论文提出了MultiMM数据集和SEMD模型，用于研究跨文化多模态隐喻，以减少NLP中的文化偏见。


<details>
  <summary>Details</summary>
Motivation: 现有隐喻处理研究多依赖英语数据，存在文化偏见，影响模型性能评估。跨文化多模态隐喻研究尚不充分。

Method: 引入MultiMM数据集（8,461对中英文广告文本-图像），并提出SEMD模型，结合情感嵌入提升跨文化隐喻理解。

Result: 实验证明SEMD在隐喻检测和情感分析任务中有效。

Conclusion: 该研究旨在提高NLP中对文化偏见的认识，推动更公平、包容的语言模型发展。

Abstract: Metaphors are pervasive in communication, making them crucial for natural
language processing (NLP). Previous research on automatic metaphor processing
predominantly relies on training data consisting of English samples, which
often reflect Western European or North American biases. This cultural skew can
lead to an overestimation of model performance and contributions to NLP
progress. However, the impact of cultural bias on metaphor processing,
particularly in multimodal contexts, remains largely unexplored. To address
this gap, we introduce MultiMM, a Multicultural Multimodal Metaphor dataset
designed for cross-cultural studies of metaphor in Chinese and English. MultiMM
consists of 8,461 text-image advertisement pairs, each accompanied by
fine-grained annotations, providing a deeper understanding of multimodal
metaphors beyond a single cultural domain. Additionally, we propose
Sentiment-Enriched Metaphor Detection (SEMD), a baseline model that integrates
sentiment embeddings to enhance metaphor comprehension across cultural
backgrounds. Experimental results validate the effectiveness of SEMD on
metaphor detection and sentiment analysis tasks. We hope this work increases
awareness of cultural bias in NLP research and contributes to the development
of fairer and more inclusive language models. Our dataset and code are
available at https://github.com/DUTIR-YSQ/MultiMM.

</details>


### [138] [What makes Reasoning Models Different? Follow the Reasoning Leader for Efficient Decoding](https://arxiv.org/abs/2506.06998)
*Ming Li,Zhengyuan Yang,Xiyao Wang,Dianqi Li,Kevin Lin,Tianyi Zhou,Lijuan Wang*

Main category: cs.CL

TL;DR: 论文提出FoReaL-Decoding方法，通过快速-慢速协作解码减少推理模型的冗余计算，同时保持性能。


<details>
  <summary>Details</summary>
Motivation: 大型推理模型（LRMs）在推理过程中常因冗长的思维链导致效率低下，且易陷入过度思考现象。

Method: 提出FoReaL-Decoding方法，结合主导模型和草稿模型协作解码，通过随机门平滑切换。

Result: 在四个数学推理基准测试中，FoReaL-Decoding减少30-50%的计算量，缩短40%的思维链长度，同时保持86-100%的性能。

Conclusion: FoReaL-Decoding是一种简单、即插即用的方法，可在推理任务中实现成本与性能的可控权衡。

Abstract: Large reasoning models (LRMs) achieve strong reasoning performance by
emitting long chains of thought. Yet, these verbose traces slow down inference
and often drift into unnecessary detail, known as the overthinking phenomenon.
To better understand LRMs' behavior, we systematically analyze the token-level
misalignment between reasoning and non-reasoning models. While it is expected
that their primary difference lies in the stylistic "thinking cues", LRMs
uniquely exhibit two pivotal, previously under-explored phenomena: a Global
Misalignment Rebound, where their divergence from non-reasoning models persists
or even grows as response length increases, and more critically, a Local
Misalignment Diminish, where the misalignment concentrates at the "thinking
cues" each sentence starts with but rapidly declines in the remaining of the
sentence. Motivated by the Local Misalignment Diminish, we propose
FoReaL-Decoding, a collaborative fast-slow thinking decoding method for
cost-quality trade-off. In FoReaL-Decoding, a Leading model leads the first few
tokens for each sentence, and then a weaker draft model completes the following
tokens to the end of each sentence. FoReaL-Decoding adopts a stochastic gate to
smoothly interpolate between the small and the large model. On four popular
math-reasoning benchmarks (AIME24, GPQA-Diamond, MATH500, AMC23),
FoReaL-Decoding reduces theoretical FLOPs by 30 to 50% and trims CoT length by
up to 40%, while preserving 86 to 100% of model performance. These results
establish FoReaL-Decoding as a simple, plug-and-play route to controllable
cost-quality trade-offs in reasoning-centric tasks.

</details>


### [139] [Adversarial Paraphrasing: A Universal Attack for Humanizing AI-Generated Text](https://arxiv.org/abs/2506.07001)
*Yize Cheng,Vinu Sankar Sadasivan,Mehrdad Saberi,Shoumik Saha,Soheil Feizi*

Main category: cs.CL

TL;DR: 论文提出了一种名为Adversarial Paraphrasing的训练免费攻击框架，通过利用现成的指令遵循LLM来优化AI生成文本，以更有效地逃避检测。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型（LLMs）能力的提升，其被滥用于AI生成的抄袭和社会工程的风险增加，现有检测器对简单规避技术（如改写）的脆弱性促使研究更强大的攻击方法。

Method: 采用现成的指令遵循LLM，在AI文本检测器的指导下改写AI生成内容，生成专门优化的对抗样本以绕过检测。

Result: 实验表明，该方法在多种检测系统中广泛有效且高度可迁移，显著降低了检测率，例如在RADAR和Fast-DetectGPT上分别减少了64.49%和98.96%的T@1%F。

Conclusion: 研究强调了面对日益复杂的规避技术，需要更强大和弹性的检测策略。

Abstract: The increasing capabilities of Large Language Models (LLMs) have raised
concerns about their misuse in AI-generated plagiarism and social engineering.
While various AI-generated text detectors have been proposed to mitigate these
risks, many remain vulnerable to simple evasion techniques such as
paraphrasing. However, recent detectors have shown greater robustness against
such basic attacks. In this work, we introduce Adversarial Paraphrasing, a
training-free attack framework that universally humanizes any AI-generated text
to evade detection more effectively. Our approach leverages an off-the-shelf
instruction-following LLM to paraphrase AI-generated content under the guidance
of an AI text detector, producing adversarial examples that are specifically
optimized to bypass detection. Extensive experiments show that our attack is
both broadly effective and highly transferable across several detection
systems. For instance, compared to simple paraphrasing attack--which,
ironically, increases the true positive at 1% false positive (T@1%F) by 8.57%
on RADAR and 15.03% on Fast-DetectGPT--adversarial paraphrasing, guided by
OpenAI-RoBERTa-Large, reduces T@1%F by 64.49% on RADAR and a striking 98.96% on
Fast-DetectGPT. Across a diverse set of detectors--including neural
network-based, watermark-based, and zero-shot approaches--our attack achieves
an average T@1%F reduction of 87.88% under the guidance of
OpenAI-RoBERTa-Large. We also analyze the tradeoff between text quality and
attack success to find that our method can significantly reduce detection
rates, with mostly a slight degradation in text quality. Our adversarial setup
highlights the need for more robust and resilient detection strategies in the
light of increasingly sophisticated evasion techniques.

</details>


### [140] [A Culturally-diverse Multilingual Multimodal Video Benchmark & Model](https://arxiv.org/abs/2506.07032)
*Bhuiyan Sanjid Shafique,Ashmal Vayani,Muhammad Maaz,Hanoona Abdul Rasheed,Dinura Dissanayake,Mohammed Irfan Kurpath,Yahya Hmaiti,Go Inoue,Jean Lahoud,Md. Safirur Rashid,Shadid Intisar Quasem,Maheen Fatima,Franco Vidal,Mykola Maslych,Ketan Pravin More,Sanoojan Baliah,Hasindri Watawana,Yuhao Li,Fabian Farestam,Leon Schaller,Roman Tymtsiv,Simon Weber,Hisham Cholakkal,Ivan Laptev,Shin'ichi Satoh,Michael Felsberg,Mubarak Shah,Salman Khan,Fahad Shahbaz Khan*

Main category: cs.CL

TL;DR: 该论文提出了一个多语言视频大模型（ViMUL-Bench）基准测试，覆盖14种语言，旨在评估视频大模型在文化和语言多样性上的表现。


<details>
  <summary>Details</summary>
Motivation: 现有的大多语言视频大模型（LMMs）主要针对英语，缺乏对多语言和文化多样性的研究。

Method: 开发了ViMUL-Bench基准测试，包含8k手动验证样本，涵盖15个类别，并提出了一个多语言视频训练集和简单模型ViMUL。

Result: ViMUL在多语言视频理解上表现出色，平衡了高资源和低资源语言的需求。

Conclusion: ViMUL-Bench、训练集和模型将促进未来多语言视频大模型的研究，提升文化和语言包容性。

Abstract: Large multimodal models (LMMs) have recently gained attention due to their
effectiveness to understand and generate descriptions of visual content. Most
existing LMMs are in English language. While few recent works explore
multilingual image LMMs, to the best of our knowledge, moving beyond the
English language for cultural and linguistic inclusivity is yet to be
investigated in the context of video LMMs. In pursuit of more inclusive video
LMMs, we introduce a multilingual Video LMM benchmark, named ViMUL-Bench, to
evaluate Video LMMs across 14 languages, including both low- and high-resource
languages: English, Chinese, Spanish, French, German, Hindi, Arabic, Russian,
Bengali, Urdu, Sinhala, Tamil, Swedish, and Japanese. Our ViMUL-Bench is
designed to rigorously test video LMMs across 15 categories including eight
culturally diverse categories, ranging from lifestyles and festivals to foods
and rituals and from local landmarks to prominent cultural personalities.
ViMUL-Bench comprises both open-ended (short and long-form) and multiple-choice
questions spanning various video durations (short, medium, and long) with 8k
samples that are manually verified by native language speakers. In addition, we
also introduce a machine translated multilingual video training set comprising
1.2 million samples and develop a simple multilingual video LMM, named ViMUL,
that is shown to provide a better tradeoff between high-and low-resource
languages for video understanding. We hope our ViMUL-Bench and multilingual
video LMM along with a large-scale multilingual video training set will help
ease future research in developing cultural and linguistic inclusive
multilingual video LMMs. Our proposed benchmark, video LMM and training data
will be publicly released at https://mbzuai-oryx.github.io/ViMUL/.

</details>


### [141] [KG2QA: Knowledge Graph-enhanced Retrieval-Augmented Generation for Communication Standards Question Answering](https://arxiv.org/abs/2506.07037)
*Zhongze Luo,Weixuan Wan,Qizhi Zheng,Yanhong Bai,Jingyun Sun,Jian Wang,Dan Wang*

Main category: cs.CL

TL;DR: 论文结合大语言模型微调与知识图谱构建，实现通信标准智能咨询问答系统，显著提升问答效果。


<details>
  <summary>Details</summary>
Motivation: 传统咨询模型周期长且依赖专家经验，难以满足快速发展的技术需求。

Method: 结合LoRA微调与知识图谱构建，构建包含13,906实体和13,524关系的知识图谱，并实现RAG框架。

Result: Qwen2.5-7B-Instruct在测试集上表现优异，BLEU-4提升至66.8993，ROUGE等指标显著提升。

Conclusion: 系统在交互体验和后端接入方面表现良好，具有实用价值。

Abstract: There are many types of standards in the field of communication. The
traditional consulting model has a long cycle and relies on the knowledge and
experience of experts, making it difficult to meet the rapidly developing
technological demands. This paper combines the fine-tuning of large language
models with the construction of knowledge graphs to implement an intelligent
consultation and question-answering system for communication standards. The
experimental results show that after LoRA tuning on the constructed dataset of
6,587 questions and answers in the field of communication standards,
Qwen2.5-7B-Instruct demonstrates outstanding professional capabilities in the
field of communication standards on the test set. BLEU-4 rose from 18.8564 to
66.8993, and evaluation indicators such as ROUGE also increased significantly,
outperforming the fine-tuning effect of the comparison model
Llama-3-8B-Instruct. Based on the ontology framework containing 6 entity
attributes and 10 relation attributes, a knowledge graph of the communication
standard domain containing 13,906 entities and 13,524 relations was
constructed, showing a relatively good query accuracy rate. The intelligent
consultation and question-answering system enables the fine-tuned model on the
server side to access the locally constructed knowledge graph and conduct
graphical retrieval of key information first, which is conducive to improving
the question-answering effect. The evaluation using DeepSeek as the Judge on
the test set shows that our RAG framework enables the fine-tuned model to
improve the scores at all five angles, with an average score increase of 2.26%.
And combined with web services and API interfaces, it has achieved very good
results in terms of interaction experience and back-end access, and has very
good practical application value.

</details>


### [142] [Reasoning with RAGged events: RAG-Enhanced Event Knowledge Base Construction and reasoning with proof-assistants](https://arxiv.org/abs/2506.07042)
*Stergios Chatzikyriakidis*

Main category: cs.CL

TL;DR: 论文提出了一种自动提取历史事件的方法，使用多种LLM（GPT-4、Claude、Llama 3.2）和三种增强策略，并通过Coq验证RDF表示的高阶推理能力。


<details>
  <summary>Details</summary>
Motivation: 手动构建历史事件的计算表示成本高，且现有RDF/OWL推理器仅支持一阶逻辑片段，无法进行更深层次的时空和语义分析。

Method: 采用多种LLM（GPT-4、Claude、Llama 3.2）和三种增强策略（基础生成、知识图增强、RAG）自动提取历史事件，并开发了将RDF转换为Coq规范的自动化流程。

Result: 增强策略在不同性能维度上优化效果各异：基础生成在覆盖面和历史广度上表现最佳，RAG增强提升了精度。模型架构影响增强效果，大模型表现稳健，Llama 3.2性能波动大。Coq验证表明RAG提取的事件类型合法。

Conclusion: 自动提取和增强策略有效解决了历史事件表示的高成本和推理限制问题，Coq验证扩展了高阶推理能力。

Abstract: Extracting structured computational representations of historical events from
narrative text remains computationally expensive when constructed manually.
While RDF/OWL reasoners enable graph-based reasoning, they are limited to
fragments of first-order logic, preventing deeper temporal and semantic
analysis. This paper addresses both challenges by developing automatic
historical event extraction models using multiple LLMs (GPT-4, Claude, Llama
3.2) with three enhancement strategies: pure base generation, knowledge graph
enhancement, and Retrieval-Augmented Generation (RAG). We conducted
comprehensive evaluations using historical texts from Thucydides. Our findings
reveal that enhancement strategies optimize different performance dimensions
rather than providing universal improvements. For coverage and historical
breadth, base generation achieves optimal performance with Claude and GPT-4
extracting comprehensive events. However, for precision, RAG enhancement
improves coordinate accuracy and metadata completeness. Model architecture
fundamentally determines enhancement sensitivity: larger models demonstrate
robust baseline performance with incremental RAG improvements, while Llama 3.2
shows extreme variance from competitive performance to complete failure. We
then developed an automated translation pipeline converting extracted RDF
representations into Coq proof assistant specifications, enabling higher-order
reasoning beyond RDF capabilities including multi-step causal verification,
temporal arithmetic with BC dates, and formal proofs about historical
causation. The Coq formalization validates that RAG-discovered event types
represent legitimate domain-specific semantic structures rather than
ontological violations.

</details>


### [143] [Lingshu: A Generalist Foundation Model for Unified Multimodal Medical Understanding and Reasoning](https://arxiv.org/abs/2506.07044)
*LASA Team,Weiwen Xu,Hou Pong Chan,Long Li,Mahani Aljunied,Ruifeng Yuan,Jianyu Wang,Chenghao Xiao,Guizhen Chen,Chaoqun Liu,Zhaodonghui Li,Yu Sun,Junao Shen,Chaojun Wang,Jie Tan,Deli Zhao,Tingyang Xu,Hao Zhang,Yu Rong*

Main category: cs.CL

TL;DR: 论文提出了一种针对医学领域的多模态大语言模型Lingshu，通过改进数据收集和训练策略，解决了现有医学MLLMs的局限性，并在多个医学任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有医学MLLMs在医学应用中效果有限，主要因为数据与任务的不匹配，以及缺乏针对复杂医学场景的推理能力。

Method: 提出综合数据收集方法，构建多模态医学数据集，并开发医学专用MLLM Lingshu，采用多阶段训练和强化学习增强推理能力。

Result: Lingshu在多项医学任务中优于现有开源多模态模型。

Conclusion: Lingshu通过改进数据与训练策略，显著提升了医学MLLMs的性能，为医学AI应用提供了新思路。

Abstract: Multimodal Large Language Models (MLLMs) have demonstrated impressive
capabilities in understanding common visual elements, largely due to their
large-scale datasets and advanced training strategies. However, their
effectiveness in medical applications remains limited due to the inherent
discrepancies between data and tasks in medical scenarios and those in the
general domain. Concretely, existing medical MLLMs face the following critical
limitations: (1) limited coverage of medical knowledge beyond imaging, (2)
heightened susceptibility to hallucinations due to suboptimal data curation
processes, (3) lack of reasoning capabilities tailored for complex medical
scenarios. To address these challenges, we first propose a comprehensive data
curation procedure that (1) efficiently acquires rich medical knowledge data
not only from medical imaging but also from extensive medical texts and
general-domain data; and (2) synthesizes accurate medical captions, visual
question answering (VQA), and reasoning samples. As a result, we build a
multimodal dataset enriched with extensive medical knowledge. Building on the
curated data, we introduce our medical-specialized MLLM: Lingshu. Lingshu
undergoes multi-stage training to embed medical expertise and enhance its
task-solving capabilities progressively. Besides, we preliminarily explore the
potential of applying reinforcement learning with verifiable rewards paradigm
to enhance Lingshu's medical reasoning ability. Additionally, we develop
MedEvalKit, a unified evaluation framework that consolidates leading multimodal
and textual medical benchmarks for standardized, fair, and efficient model
assessment. We evaluate the performance of Lingshu on three fundamental medical
tasks, multimodal QA, text-based QA, and medical report generation. The results
show that Lingshu consistently outperforms the existing open-source multimodal
models on most tasks ...

</details>


### [144] [Com$^2$: A Causal-Guided Benchmark for Exploring Complex Commonsense Reasoning in Large Language Models](https://arxiv.org/abs/2506.07064)
*Kai Xiong,Xiao Ding,Yixin Cao,Yuxiong Yan,Li Du,Yufei Zhang,Jinglong Gao,Jiaqian Liu,Bing Qin,Ting Liu*

Main category: cs.CL

TL;DR: 论文提出了一个名为Com$^2$的基准测试，专注于复杂常识推理，填补了现有研究的空白。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）在简单常识推理上表现优异，但在复杂和隐式常识推理上表现不佳，而后者对人类更为重要。

Method: 结合因果事件图作为结构化复杂常识，利用因果理论（如干预）修改因果图生成不同场景，并通过慢思考引导LLMs合成示例。

Result: 实验表明LLMs在推理深度和广度上存在困难，但后训练和慢思考可以缓解这一问题。

Conclusion: Com$^2$基准测试为复杂常识推理提供了新方向，并展示了改进LLMs推理能力的潜力。

Abstract: Large language models (LLMs) have mastered abundant simple and explicit
commonsense knowledge through pre-training, enabling them to achieve human-like
performance in simple commonsense reasoning. Nevertheless, LLMs struggle to
reason with complex and implicit commonsense knowledge that is derived from
simple ones (such as understanding the long-term effects of certain events), an
aspect humans tend to focus on more. Existing works focus on complex tasks like
math and code, while complex commonsense reasoning remains underexplored due to
its uncertainty and lack of structure. To fill this gap and align with
real-world concerns, we propose a benchmark Com$^2$ focusing on complex
commonsense reasoning. We first incorporate causal event graphs to serve as
structured complex commonsense. Then we adopt causal theory~(e.g.,
intervention) to modify the causal event graphs and obtain different scenarios
that meet human concerns. Finally, an LLM is employed to synthesize examples
with slow thinking, which is guided by the logical relationships in the
modified causal graphs. Furthermore, we use detective stories to construct a
more challenging subset. Experiments show that LLMs struggle in reasoning depth
and breadth, while post-training and slow thinking can alleviate this. The code
and data are available at https://github.com/Waste-Wood/Com2.

</details>


### [145] [Representation Decomposition for Learning Similarity and Contrastness Across Modalities for Affective Computing](https://arxiv.org/abs/2506.07086)
*Yuanhe Tian,Pengsen Cheng,Guoqing Jin,Lei Zhang,Yan Song*

Main category: cs.CL

TL;DR: 提出了一种基于LLM的多模态情感计算方法，通过分解共享和特定模态的表示，显著提升了情感计算任务的性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法未能有效处理多模态数据中的复杂和冲突信息，限制了情感计算的准确性。

Method: 使用预训练多模态编码器对齐输入模态，通过表示分解框架分离共享和特定模态信息，最后通过注意力机制整合为动态软提示输入LLM。

Result: 在三个情感计算任务中表现优于基线模型和现有最优方法。

Conclusion: 该方法通过分解和整合多模态信息，显著提升了情感计算的性能。

Abstract: Multi-modal affective computing aims to automatically recognize and interpret
human attitudes from diverse data sources such as images and text, thereby
enhancing human-computer interaction and emotion understanding. Existing
approaches typically rely on unimodal analysis or straightforward fusion of
cross-modal information that fail to capture complex and conflicting evidence
presented across different modalities. In this paper, we propose a novel
LLM-based approach for affective computing that explicitly deconstructs visual
and textual representations into shared (modality-invariant) and
modality-specific components. Specifically, our approach firstly encodes and
aligns input modalities using pre-trained multi-modal encoders, then employs a
representation decomposition framework to separate common emotional content
from unique cues, and finally integrates these decomposed signals via an
attention mechanism to form a dynamic soft prompt for a multi-modal LLM.
Extensive experiments on three representative tasks for affective computing,
namely, multi-modal aspect-based sentiment analysis, multi-modal emotion
analysis, and hateful meme detection, demonstrate the effectiveness of our
approach, which consistently outperforms strong baselines and state-of-the-art
models.

</details>


### [146] [How Far Are We from Optimal Reasoning Efficiency?](https://arxiv.org/abs/2506.07104)
*Jiaxuan Gao,Shu Yan,Qixin Tan,Lu Yang,Shusheng Xu,Wei Fu,Zhiyu Mei,Kaifeng Lyu,Yi Wu*

Main category: cs.CL

TL;DR: 论文提出了一种衡量大型推理模型（LRMs）推理效率的新指标REG，并通过REO-RL算法显著提升了效率。


<details>
  <summary>Details</summary>
Motivation: 现有LRMs在推理过程中存在冗余和低效问题，且缺乏统一的评估标准。

Method: 引入推理效率前沿和REG指标，提出REO-RL算法优化效率。

Result: REO-RL在保持精度的同时显著减少推理长度，REG降低50%以上。

Conclusion: 优化LRMs推理效率仍具挑战性，但REO-RL和REG为未来研究提供了方向。

Abstract: Large Reasoning Models (LRMs) demonstrate remarkable problem-solving
capabilities through extended Chain-of-Thought (CoT) reasoning but often
produce excessively verbose and redundant reasoning traces. This inefficiency
incurs high inference costs and limits practical deployment. While existing
fine-tuning methods aim to improve reasoning efficiency, assessing their
efficiency gains remains challenging due to inconsistent evaluations. In this
work, we introduce the reasoning efficiency frontiers, empirical upper bounds
derived from fine-tuning base LRMs across diverse approaches and training
configurations. Based on these frontiers, we propose the Reasoning Efficiency
Gap (REG), a unified metric quantifying deviations of any fine-tuned LRMs from
these frontiers. Systematic evaluation on challenging mathematical benchmarks
reveals significant gaps in current methods: they either sacrifice accuracy for
short length or still remain inefficient under tight token budgets. To reduce
the efficiency gap, we propose REO-RL, a class of Reinforcement Learning
algorithms that minimizes REG by targeting a sparse set of token budgets.
Leveraging numerical integration over strategically selected budgets, REO-RL
approximates the full efficiency objective with low error using a small set of
token budgets. Through systematic benchmarking, we demonstrate that our
efficiency metric, REG, effectively captures the accuracy-length trade-off,
with low-REG methods reducing length while maintaining accuracy. Our approach,
REO-RL, consistently reduces REG by >=50 across all evaluated LRMs and matching
Qwen3-4B/8B efficiency frontiers under a 16K token budget with minimal accuracy
loss. Ablation studies confirm the effectiveness of our exponential token
budget strategy. Finally, our findings highlight that fine-tuning LRMs to
perfectly align with the efficiency frontiers remains an open challenge.

</details>


### [147] [Theorem-of-Thought: A Multi-Agent Framework for Abductive, Deductive, and Inductive Reasoning in Language Models](https://arxiv.org/abs/2506.07106)
*Samir Abdaljalil,Hasan Kurban,Khalid Qaraqe,Erchin Serpedin*

Main category: cs.CL

TL;DR: 论文提出了一种名为Theorem-of-Thought (ToTh)的新框架，通过模拟三种推理模式的协作，提升大语言模型(LLM)的逻辑性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有的大语言模型在自然语言推理任务中表现良好，但其推理过程脆弱且难以解释。Chain-of-Thought (CoT)等方法虽能提升可靠性，但缺乏逻辑结构和内部一致性的评估机制。

Method: ToTh框架通过三个并行代理（分别模拟溯因、演绎和归纳推理）生成推理轨迹，并将其结构化为形式推理图。通过贝叶斯信念传播和自然语言推理(NLI)评估一致性，选择最连贯的推理图生成最终答案。

Result: 实验表明，ToTh在符号推理(WebOfLies)和数值推理(MultiArith)任务中均优于CoT、Self-Consistency和CoT-Decoding，同时生成可解释且逻辑清晰的推理链。

Conclusion: ToTh为构建更稳健且受认知启发的LLM推理提供了有前景的方向。

Abstract: Large language models (LLMs) have shown strong performance across natural
language reasoning tasks, yet their reasoning processes remain brittle and
difficult to interpret. Prompting techniques like Chain-of-Thought (CoT)
enhance reliability by eliciting intermediate reasoning steps or aggregating
multiple outputs. However, they lack mechanisms for enforcing logical structure
and assessing internal coherence. We introduce Theorem-of-Thought (ToTh), a
novel framework that models reasoning as collaboration among three parallel
agents, each simulating a distinct mode of inference: abductive, deductive, and
inductive. Each agent produces a reasoning trace, which is structured into a
formal reasoning graph. To evaluate consistency, we apply Bayesian belief
propagation guided by natural language inference (NLI), assigning confidence
scores to each step. The most coherent graph is selected to derive the final
answer. Experiments on symbolic (WebOfLies) and numerical (MultiArith)
reasoning benchmarks show that ToTh consistently outperforms CoT,
Self-Consistency, and CoT-Decoding across multiple LLMs, while producing
interpretable and logically grounded reasoning chains. Our findings suggest a
promising direction for building more robust and cognitively inspired LLM
reasoning. The implementation is available at
https://github.com/KurbanIntelligenceLab/theorem-of-thought.

</details>


### [148] [Prompting Science Report 2: The Decreasing Value of Chain of Thought in Prompting](https://arxiv.org/abs/2506.07142)
*Lennart Meincke,Ethan Mollick,Lilach Mollick,Dan Shapiro*

Main category: cs.CL

TL;DR: Chain-of-Thought (CoT) prompting的效果因任务和模型类型而异，对非推理模型有小幅提升但可能增加错误，而对推理模型增益有限且增加成本。


<details>
  <summary>Details</summary>
Motivation: 帮助商业、教育和政策领导者通过严格测试了解AI技术细节，特别是CoT提示的效果。

Method: 通过实验分析CoT提示在不同类型任务和模型中的表现。

Result: CoT对非推理模型有小幅提升但可能增加错误，对推理模型增益有限且显著增加成本。

Conclusion: CoT提示的效果因模型和任务而异，需权衡其潜在收益与额外成本。

Abstract: This is the second in a series of short reports that seek to help business,
education, and policy leaders understand the technical details of working with
AI through rigorous testing. In this report, we investigate Chain-of-Thought
(CoT) prompting, a technique that encourages a large language model (LLM) to
"think step by step" (Wei et al., 2022). CoT is a widely adopted method for
improving reasoning tasks, however, our findings reveal a more nuanced picture
of its effectiveness. We demonstrate two things:
  - The effectiveness of Chain-of-Thought prompting can vary greatly depending
on the type of task and model. For non-reasoning models, CoT generally improves
average performance by a small amount, particularly if the model does not
inherently engage in step-by-step processing by default. However, CoT can
introduce more variability in answers, sometimes triggering occasional errors
in questions the model would otherwise get right. We also found that many
recent models perform some form of CoT reasoning even if not asked; for these
models, a request to perform CoT had little impact. Performing CoT generally
requires far more tokens (increasing cost and time) than direct answers.
  - For models designed with explicit reasoning capabilities, CoT prompting
often results in only marginal, if any, gains in answer accuracy. However, it
significantly increases the time and tokens needed to generate a response.

</details>


### [149] [Semantic-preserved Augmentation with Confidence-weighted Fine-tuning for Aspect Category Sentiment Analysis](https://arxiv.org/abs/2506.07148)
*Yaping Chai,Haoran Xie,Joe S. Qin*

Main category: cs.CL

TL;DR: 本文提出了一种基于大语言模型（LLM）的数据增强策略，用于解决低资源场景下的数据稀缺问题，并通过结构化提示模板和后处理技术确保语义一致性和多样性。


<details>
  <summary>Details</summary>
Motivation: 解决低资源场景中数据稀缺问题，提升模型对方面类别情感分析（ACSA）任务的理解能力。

Method: 设计结构化提示模板引导LLM生成数据，结合后处理技术确保语义一致性，并采用置信度加权微调策略。

Result: 在四个基准数据集上优于现有方法，表现最佳。

Conclusion: 该方法有效提升了模型性能，增强了其对情感极性和方面类别关系的理解。

Abstract: Large language model (LLM) is an effective approach to addressing data
scarcity in low-resource scenarios. Recent existing research designs
hand-crafted prompts to guide LLM for data augmentation. We introduce a data
augmentation strategy for the aspect category sentiment analysis (ACSA) task
that preserves the original sentence semantics and has linguistic diversity,
specifically by providing a structured prompt template for an LLM to generate
predefined content. In addition, we employ a post-processing technique to
further ensure semantic consistency between the generated sentence and the
original sentence. The augmented data increases the semantic coverage of the
training distribution, enabling the model better to understand the relationship
between aspect categories and sentiment polarities, enhancing its inference
capabilities. Furthermore, we propose a confidence-weighted fine-tuning
strategy to encourage the model to generate more confident and accurate
sentiment polarity predictions. Compared with powerful and recent works, our
method consistently achieves the best performance on four benchmark datasets
over all baselines.

</details>


### [150] [Syntactic Control of Language Models by Posterior Inference](https://arxiv.org/abs/2506.07154)
*Vicky Xefteri,Tim Vieira,Ryan Cotterell,Afra Amini*

Main category: cs.CL

TL;DR: 论文提出了一种基于后验推断的采样算法，用于在生成文本时有效控制目标句法结构，结合了序贯蒙特卡洛和句法标注器，显著提升了句法准确性。


<details>
  <summary>Details</summary>
Motivation: 控制语言模型生成文本的句法结构对于需要清晰性、风格一致性或可解释性的应用至关重要，但目前仍具挑战性。

Method: 采用基于后验推断的采样算法，结合序贯蒙特卡洛和句法标注器，确保生成的每个标记符合目标句法结构。

Result: 实验表明，该方法在GPT2和Llama3-8B模型上将句法准确性的F1分数从12.31和35.33提升至约93，且不影响流畅性。

Conclusion: 该方法为需要精确控制句法的应用提供了有效解决方案，突显了采样算法的潜力。

Abstract: Controlling the syntactic structure of text generated by language models is
valuable for applications requiring clarity, stylistic consistency, or
interpretability, yet it remains a challenging task. In this paper, we argue
that sampling algorithms based on the posterior inference can effectively
enforce a target constituency structure during generation. Our approach
combines sequential Monte Carlo, which estimates the posterior distribution by
sampling from a proposal distribution, with a syntactic tagger that ensures
that each generated token aligns with the desired syntactic structure. Our
experiments with GPT2 and Llama3-8B models show that with an appropriate
proposal distribution, we can improve syntactic accuracy, increasing the F1
score from $12.31$ (GPT2-large) and $35.33$ (Llama3-8B) to about $93$ in both
cases without compromising the language model's fluency. These results
underscore both the complexity of syntactic control and the effectiveness of
sampling algorithms, offering a promising approach for applications where
precise control over syntax is essential.

</details>


### [151] [GeometryZero: Improving Geometry Solving for LLM with Group Contrastive Policy Optimization](https://arxiv.org/abs/2506.07160)
*Yikun Wang,Yibin Wang,Dianyi Wang,Zimian Peng,Qipeng Guo,Dacheng Tao,Jiaqi Wang*

Main category: cs.CL

TL;DR: 论文提出了一种新的强化学习框架GCPO，用于训练小型模型解决几何问题，通过自适应奖励信号和长度奖励优化辅助构造，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 几何问题解决中辅助构造是关键，但现有方法性能不足或计算成本高，需要更高效的解决方案。

Method: 提出GCPO框架，包括Group Contrastive Masking和长度奖励，训练GeometryZero模型。

Result: GeometryZero在多个几何基准测试中平均提升4.29%，优于基线方法。

Conclusion: GCPO框架有效解决了几何推理中的辅助构造问题，为小型模型提供了高效解决方案。

Abstract: Recent advances in large language models (LLMs) have demonstrated remarkable
capabilities across diverse domains, particularly in mathematical reasoning,
amid which geometry problem solving remains a challenging area where auxiliary
construction plays a enssential role. Existing approaches either achieve
suboptimal performance or rely on massive LLMs (e.g., GPT-4o), incurring
massive computational costs. We posit that reinforcement learning with
verifiable reward (e.g., GRPO) offers a promising direction for training
smaller models that effectively combine auxiliary construction with robust
geometric reasoning. However, directly applying GRPO to geometric reasoning
presents fundamental limitations due to its dependence on unconditional
rewards, which leads to indiscriminate and counterproductive auxiliary
constructions. To address these challenges, we propose Group Contrastive Policy
Optimization (GCPO), a novel reinforcement learning framework featuring two key
innovations: (1) Group Contrastive Masking, which adaptively provides positive
or negative reward signals for auxiliary construction based on contextual
utility, and a (2) length reward that promotes longer reasoning chains.
Building on GCPO, we develop GeometryZero, a family of affordable-size
geometric reasoning models that judiciously determine when to employ auxiliary
construction. Our extensive empirical evaluation across popular geometric
benchmarks (Geometry3K, MathVista) demonstrates that GeometryZero models
consistently outperform baselines (e.g. GRPO), achieving an average improvement
of 4.29% across all benchmarks.

</details>


### [152] [CTDGSI: A comprehensive exploitation of instance selection methods for automatic text classification. VII Concurso de Teses, Dissertações e Trabalhos de Graduação em SI -- XXI Simpósio Brasileiro de Sistemas de Informação](https://arxiv.org/abs/2506.07169)
*Washington Cunha,Leonardo Rocha,Marcos André Gonçalves*

Main category: cs.CL

TL;DR: 该博士论文探讨了自然语言处理中的实例选择（IS）技术，旨在通过减少训练集中的噪声和冗余实例来降低成本，同时保持模型效果。研究提出了两种新型IS方法，并在自动文本分类任务中验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 当前NLP领域依赖大量数据和计算资源，实例选择技术潜力巨大但研究不足，论文旨在填补这一空白并验证其实际效果。

Method: 论文对多种IS方法进行了全面比较，并提出了两种针对大型数据集和Transformer架构的噪声导向和冗余感知IS解决方案。

Result: 实验结果显示，最终方案平均减少41%的训练集规模，同时保持模型效果，训练速度提升1.67倍至2.46倍。

Conclusion: 实例选择技术在NLP中具有显著潜力，论文提出的方法为大规模数据集的高效训练提供了可行方案。

Abstract: Progress in Natural Language Processing (NLP) has been dictated by the rule
of more: more data, more computing power and more complexity, best exemplified
by the Large Language Models. However, training (or fine-tuning) large dense
models for specific applications usually requires significant amounts of
computing resources. This \textbf{Ph.D. dissertation} focuses on an
under-investi\-gated NLP data engineering technique, whose potential is
enormous in the current scenario known as Instance Selection (IS). The IS goal
is to reduce the training set size by removing noisy or redundant instances
while maintaining the effectiveness of the trained models and reducing the
training process cost. We provide a comprehensive and scientifically sound
comparison of IS methods applied to an essential NLP task -- Automatic Text
Classification (ATC), considering several classification solutions and many
datasets. Our findings reveal a significant untapped potential for IS
solutions. We also propose two novel IS solutions that are noise-oriented and
redundancy-aware, specifically designed for large datasets and transformer
architectures. Our final solution achieved an average reduction of 41\% in
training sets, while maintaining the same levels of effectiveness in all
datasets. Importantly, our solutions demonstrated speedup improvements of 1.67x
(up to 2.46x), making them scalable for datasets with hundreds of thousands of
documents.

</details>


### [153] [RULE: Reinforcement UnLEarning Achieves Forget-Retain Pareto Optimality](https://arxiv.org/abs/2506.07171)
*Chenlong Zhang,Zhuoran Jin,Hongbang Yuan,Jiaheng Wei,Tong Zhou,Kang Liu,Jun Zhao,Yubo Chen*

Main category: cs.CL

TL;DR: 论文提出了一种名为RULE的高效框架，用于从大型语言模型中选择性删除特定信息，同时保持模型整体性能。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型的广泛应用，其训练数据中可能包含敏感、受版权保护或非法内容，引发了关于如何选择性删除这些信息的关注。

Method: RULE将遗忘任务建模为拒绝边界优化问题，使用少量遗忘数据和合成边界查询进行训练，并通过可验证的奖励函数实现安全拒绝和保留有用响应。

Result: 实验表明，RULE仅需12%的遗忘数据和8%的合成边界数据，即可在遗忘质量和响应自然度上优于现有方法，同时保持模型性能。

Conclusion: RULE不仅实现了目标遗忘，还提升了模型输出的自然度和训练效率，并展现出强大的泛化能力。

Abstract: The widespread deployment of Large Language Models (LLMs) trained on massive,
uncurated corpora has raised growing concerns about the inclusion of sensitive,
copyrighted, or illegal content. This has led to increasing interest in LLM
unlearning: the task of selectively removing specific information from a model
without retraining from scratch or degrading overall utility. However, existing
methods often rely on large-scale forget and retain datasets, and suffer from
unnatural responses, poor generalization, or catastrophic utility loss. In this
work, we propose Reinforcement UnLearning (RULE), an efficient framework that
formulates unlearning as a refusal boundary optimization problem. RULE is
trained with a small portion of the forget set and synthesized boundary
queries, using a verifiable reward function that encourages safe refusal on
forget--related queries while preserving helpful responses on permissible
inputs. We provide both theoretical and empirical evidence demonstrating the
effectiveness of RULE in achieving targeted unlearning without compromising
model utility. Experimental results show that, with only $12%$ forget set and
$8%$ synthesized boundary data, RULE outperforms existing baselines by up to
$17.5%$ forget quality and $16.3%$ naturalness response while maintaining
general utility, achieving forget--retain Pareto optimality. Remarkably, we
further observe that RULE improves the naturalness of model outputs, enhances
training efficiency, and exhibits strong generalization ability, generalizing
refusal behavior to semantically related but unseen queries.

</details>


### [154] [Flattery in Motion: Benchmarking and Analyzing Sycophancy in Video-LLMs](https://arxiv.org/abs/2506.07180)
*Wenrui Zhou,Shu Yang,Qingsong Yang,Zikun Guo,Lijie Hu,Di Wang*

Main category: cs.CL

TL;DR: 论文提出了VISE基准，用于评估视频大语言模型（Video-LLMs）在误导性用户输入下的迎合行为，填补了该领域系统性评测的空白，并探索了减少迎合偏见的策略。


<details>
  <summary>Details</summary>
Motivation: 随着视频大语言模型在现实应用中的普及，其事实一致性和可靠性变得至关重要。然而，模型倾向于迎合用户输入（即使与视觉证据矛盾）的行为损害了其可信度。目前缺乏针对视频语言领域的系统性评测。

Method: 提出了VISE基准，首次专门评估Video-LLMs的迎合行为，涵盖多种问题格式、提示偏见和视觉推理任务。同时探索了基于关键帧选择的训练无关缓解策略。

Result: VISE为视频语言领域的迎合行为提供了细粒度分析，并揭示了通过增强视觉基础减少迎合偏见的潜在路径。

Conclusion: VISE填补了视频语言领域迎合行为评测的空白，为提升模型可靠性提供了新方向。

Abstract: As video large language models (Video-LLMs) become increasingly integrated
into real-world applications that demand grounded multimodal reasoning,
ensuring their factual consistency and reliability is of critical importance.
However, sycophancy, the tendency of these models to align with user input even
when it contradicts the visual evidence, undermines their trustworthiness in
such contexts. Current sycophancy research has largely overlooked its specific
manifestations in the video-language domain, resulting in a notable absence of
systematic benchmarks and targeted evaluations to understand how Video-LLMs
respond under misleading user input. To fill this gap, we propose VISE
(Video-LLM Sycophancy Benchmarking and Evaluation), the first dedicated
benchmark designed to evaluate sycophantic behavior in state-of-the-art
Video-LLMs across diverse question formats, prompt biases, and visual reasoning
tasks. Specifically, VISE pioneeringly brings linguistic perspectives on
sycophancy into the visual domain, enabling fine-grained analysis across
multiple sycophancy types and interaction patterns. In addition, we explore
key-frame selection as an interpretable, training-free mitigation strategy,
which reveals potential paths for reducing sycophantic bias by strengthening
visual grounding.

</details>


### [155] [SDE-SQL: Enhancing Text-to-SQL Generation in Large Language Models via Self-Driven Exploration with SQL Probes](https://arxiv.org/abs/2506.07245)
*Wenxuan Xie,Yaxun Dai,Wenhao Jiang*

Main category: cs.CL

TL;DR: SDE-SQL框架通过动态SQL探针让大语言模型在推理时自主探索数据库，显著提升Text-to-SQL任务性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖静态数据库信息，限制了模型对数据的全面理解，需动态交互能力。

Method: 提出SDE-SQL框架，通过生成和执行SQL探针，模型主动检索数据并迭代更新理解。

Result: 在BIRD基准测试中，SDE-SQL比基线模型提升8.02%执行准确率，无需监督微调或模型集成。

Conclusion: SDE-SQL通过动态探索数据库显著提升性能，监督微调可进一步优化结果。

Abstract: Recent advancements in large language models (LLMs) have significantly
improved performance on the Text-to-SQL task. However, prior approaches
typically rely on static, pre-processed database information provided at
inference time, which limits the model's ability to fully understand the
database contents. Without dynamic interaction, LLMs are constrained to fixed,
human-provided context and cannot autonomously explore the underlying data. To
address this limitation, we propose SDE-SQL, a framework that enables large
language models to perform self-driven exploration of databases during
inference. This is accomplished by generating and executing SQL probes, which
allow the model to actively retrieve information from the database and
iteratively update its understanding of the data. Unlike prior methods, SDE-SQL
operates in a zero-shot setting, without relying on any question-SQL pairs as
in-context demonstrations. When evaluated on the BIRD benchmark with
Qwen2.5-72B-Instruct, SDE-SQL achieves an 8.02% relative improvement in
execution accuracy over the vanilla Qwen2.5-72B-Instruct baseline, establishing
a new state-of-the-art among methods based on open-source models without
supervised fine-tuning (SFT) or model ensembling. Moreover, with SFT, the
performance of SDE-SQL can be further enhanced, yielding an additional 0.52%
improvement.

</details>


### [156] [Improving the Efficiency of Long Document Classification using Sentence Ranking Approach](https://arxiv.org/abs/2506.07248)
*Prathamesh Kokate,Mitali Sarnaik,Manavi Khopade,Raviraj Joshi*

Main category: cs.CL

TL;DR: 提出了一种基于TF-IDF的句子排序方法，用于长文档分类，显著减少输入大小和推理延迟，同时保持分类准确性。


<details>
  <summary>Details</summary>
Motivation: 长文档分类中，BERT等模型因固定输入长度和二次注意力复杂度受限，且全文档分类通常冗余。

Method: 采用TF-IDF句子排序，结合固定数量或百分比选择句子，并使用归一化TF-IDF分数和句子长度的增强评分策略。

Result: 在MahaNews LDC数据集上表现优于基线方法，输入大小减少50%以上，推理延迟降低43%，分类精度仅下降0.33%。

Conclusion: 该方法证明在不牺牲性能的情况下显著减少上下文是可行的，适用于实际长文档分类任务。

Abstract: Long document classification poses challenges due to the computational
limitations of transformer-based models, particularly BERT, which are
constrained by fixed input lengths and quadratic attention complexity.
Moreover, using the full document for classification is often redundant, as
only a subset of sentences typically carries the necessary information. To
address this, we propose a TF-IDF-based sentence ranking method that improves
efficiency by selecting the most informative content. Our approach explores
fixed-count and percentage-based sentence selection, along with an enhanced
scoring strategy combining normalized TF-IDF scores and sentence length.
Evaluated on the MahaNews LDC dataset of long Marathi news articles, the method
consistently outperforms baselines such as first, last, and random sentence
selection. With MahaBERT-v2, we achieve near-identical classification accuracy
with just a 0.33 percent drop compared to the full-context baseline, while
reducing input size by over 50 percent and inference latency by 43 percent.
This demonstrates that significant context reduction is possible without
sacrificing performance, making the method practical for real-world long
document classification tasks.

</details>


### [157] [Bias Attribution in Filipino Language Models: Extending a Bias Interpretability Metric for Application on Agglutinative Languages](https://arxiv.org/abs/2506.07249)
*Lance Calvin Lim Gamboa,Yue Feng,Mark Lee*

Main category: cs.CL

TL;DR: 该论文研究了语言模型在处理菲律宾语时的偏见来源，发现与英语模型不同，菲律宾语模型的偏见更多与人、物体和关系相关。


<details>
  <summary>Details</summary>
Motivation: 探索语言模型在处理粘着语（如菲律宾语）时的偏见来源，并与英语模型进行对比。

Method: 采用信息论偏见归因评分方法，将其适配到菲律宾语模型和多语言模型上进行分析。

Result: 菲律宾语模型的偏见更多与人、物体和关系相关，而英语模型的偏见则与行为（如犯罪、性行为）相关。

Conclusion: 英语与非英语模型在处理与社会人口群体相关的输入时存在差异，这为跨语言偏见研究提供了新视角。

Abstract: Emerging research on bias attribution and interpretability have revealed how
tokens contribute to biased behavior in language models processing English
texts. We build on this line of inquiry by adapting the information-theoretic
bias attribution score metric for implementation on models handling
agglutinative languages, particularly Filipino. We then demonstrate the
effectiveness of our adapted method by using it on a purely Filipino model and
on three multilingual models: one trained on languages worldwide and two on
Southeast Asian data. Our results show that Filipino models are driven towards
bias by words pertaining to people, objects, and relationships, entity-based
themes that stand in contrast to the action-heavy nature of bias-contributing
themes in English (i.e., criminal, sexual, and prosocial behaviors). These
findings point to differences in how English and non-English models process
inputs linked to sociodemographic groups and bias.

</details>


### [158] [Question Answering under Temporal Conflict: Evaluating and Organizing Evolving Knowledge with LLMs](https://arxiv.org/abs/2506.07270)
*Atahan Özer,Çağatay Yıldız*

Main category: cs.CL

TL;DR: 论文研究了大型语言模型（LLMs）在处理随时间演变的文本数据时的表现，提出了两个新基准和一个轻量级框架来解决知识更新问题。


<details>
  <summary>Details</summary>
Motivation: LLMs的知识受限于预训练数据，而现实世界信息不断变化，传统更新方法成本高或不实用，因此需要探索更高效的知识更新策略。

Method: 引入两个新基准（Temporal Wiki和Unified Clark），并提出一个轻量级框架，通过外部结构化记忆增量更新知识，避免重新训练。

Result: LLMs在处理冲突或过时事实时表现不佳，而提出的框架在复杂推理和整合冲突事实的任务上优于基线方法。

Conclusion: 轻量级框架能有效解决LLMs知识更新的问题，尤其在处理动态信息时表现优越。

Abstract: Large language models (LLMs) exhibit remarkable capabilities in question
answering and reasoning thanks to their extensive parametric memory. However,
their knowledge is inherently limited by the scope of their pre-training data,
while real-world information evolves continuously. Updating this knowledge
typically requires costly and brittle re-training, or in-context learning
(ICL), which becomes impractical at scale given the volume and volatility of
modern information. Motivated by these limitations, we investigate how LLMs
perform when exposed to temporal text corpora, or documents that reflect
evolving knowledge over time, such as sports biographies where facts like a
player's "current team" change year by year. To this end, we introduce two new
benchmarks: Temporal Wiki, which captures factual drift across historical
Wikipedia snapshots, and Unified Clark, which aggregates timestamped news
articles to simulate real-world information accumulation. Our analysis reveals
that LLMs often struggle to reconcile conflicting or outdated facts and can be
misled when multiple versions of a fact appear in context. To address these
issues, we propose a lightweight, agentic framework that incrementally builds a
structured, external memory from source documents without requiring
re-training. This knowledge organization strategy enables models to retrieve
and reason over temporally filtered, relevant information at inference time.
Empirically, our method outperforms ICL and RAG baselines across both
benchmarks, especially on questions requiring more complex reasoning or
integration of conflicting facts.

</details>


### [159] [Parsing the Switch: LLM-Based UD Annotation for Complex Code-Switched and Low-Resource Languages](https://arxiv.org/abs/2506.07274)
*Olga Kellert,Nemika Tyagi,Muhammad Imran,Nelvin Licona-Guevara,Carlos Gómez-Rodríguez*

Main category: cs.CL

TL;DR: BiLingua Parser是一种基于LLM的标注工具，用于生成代码转换文本的通用依赖标注，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 解决代码转换文本在低资源语言环境中缺乏标注数据的问题，并探索LLM在捕捉代码转换上下文中的句法结构的能力。

Method: 开发了一个基于提示的框架，结合少量样本LLM提示和专家评审，用于西班牙语-英语和西班牙语-瓜拉尼语数据。

Result: BiLingua Parser在专家修订后达到95.29%的LAS，显著优于现有基线方法和多语言解析器。

Conclusion: LLM在精心指导下可作为实用工具，用于在低资源代码转换环境中快速构建句法资源。

Abstract: Code-switching presents a complex challenge for syntactic analysis,
especially in low-resource language settings where annotated data is scarce.
While recent work has explored the use of large language models (LLMs) for
sequence-level tagging, few approaches systematically investigate how well
these models capture syntactic structure in code-switched contexts. Moreover,
existing parsers trained on monolingual treebanks often fail to generalize to
multilingual and mixed-language input. To address this gap, we introduce the
BiLingua Parser, an LLM-based annotation pipeline designed to produce Universal
Dependencies (UD) annotations for code-switched text. First, we develop a
prompt-based framework for Spanish-English and Spanish-Guaran\'i data,
combining few-shot LLM prompting with expert review. Second, we release two
annotated datasets, including the first Spanish-Guaran\'i UD-parsed corpus.
Third, we conduct a detailed syntactic analysis of switch points across
language pairs and communicative contexts. Experimental results show that
BiLingua Parser achieves up to 95.29% LAS after expert revision, significantly
outperforming prior baselines and multilingual parsers. These results show that
LLMs, when carefully guided, can serve as practical tools for bootstrapping
syntactic resources in under-resourced, code-switched environments. Data and
source code are available at https://github.com/N3mika/ParsingProject

</details>


### [160] [Low-resource Machine Translation: what for? who for? An observational study on a dedicated Tetun language translation service](https://arxiv.org/abs/2411.12262)
*Raphael Merx,Adérito José Guterres Correia,Hanna Suominen,Ekaterina Vylomova*

Main category: cs.CL

TL;DR: 通过分析Tetun语言翻译服务的实际使用数据，研究发现用户需求与现有语料库假设不符，建议低资源机器翻译应优先满足教育领域需求。


<details>
  <summary>Details</summary>
Motivation: 理解低资源机器翻译的实际社区需求，补充传统调查方法的不足。

Method: 对Tetun.org翻译服务的10万条请求进行观察性研究。

Result: 用户多将高资源语言翻译为Tetun，涉及科学、医疗等领域，与现有语料库以新闻为主的分布不同。

Conclusion: 低资源机器翻译应关注教育相关领域的高资源到低资源翻译准确性，观察性研究可指导技术开发。

Abstract: Low-resource machine translation (MT) presents a diversity of community needs
and application challenges that remain poorly understood. To complement surveys
and focus groups, which tend to rely on small samples of respondents, we
propose an observational study on actual usage patterns of tetun$.$org, a
specialized MT service for the Tetun language, which is the lingua franca in
Timor-Leste. Our analysis of 100,000 translation requests reveals patterns that
challenge assumptions based on existing corpora. We find that users, many of
them students on mobile devices, typically translate text from a high-resource
language into Tetun across diverse domains including science, healthcare, and
daily life. This contrasts sharply with available Tetun corpora, which are
dominated by news articles covering government and social issues. Our results
suggest that MT systems for institutionalized minority languages like Tetun
should prioritize accuracy on domains relevant to educational contexts, in the
high-resource to low-resource direction. More broadly, this study demonstrates
how observational analysis can inform low-resource language technology
development, by grounding research in practical community needs.

</details>


### [161] [Exploring the Impact of Temperature on Large Language Models:Hot or Cold?](https://arxiv.org/abs/2506.07295)
*Lujun Li,Lama Sleem,Niccolo' Gentile,Geoffrey Nichil,Radu State*

Main category: cs.CL

TL;DR: 本文研究了采样温度对大型语言模型性能的影响，提出了一种基于BERT的温度选择器，并验证了其在SuperGLUE数据集上的有效性。


<details>
  <summary>Details</summary>
Motivation: 探讨采样温度如何影响模型性能，并解决实际应用中选择最佳温度的挑战。

Method: 系统评估温度在0到2范围内对不同能力数据集的影响，并提出BERT-based温度选择器。

Result: 温度对模型性能有技能特异性影响，BERT选择器显著提升中小模型在SuperGLUE上的表现。

Conclusion: 温度选择对模型性能至关重要，BERT选择器为实际应用提供了有效解决方案。

Abstract: The sampling temperature, a critical hyperparameter in large language models
(LLMs), modifies the logits before the softmax layer, thereby reshaping the
distribution of output tokens. Recent studies have challenged the Stochastic
Parrots analogy by demonstrating that LLMs are capable of understanding
semantics rather than merely memorizing data and that randomness, modulated by
sampling temperature, plays a crucial role in model inference. In this study,
we systematically evaluated the impact of temperature in the range of 0 to 2 on
data sets designed to assess six different capabilities, conducting statistical
analyses on open source models of three different sizes: small (1B--4B), medium
(6B--13B), and large (40B--80B). Our findings reveal distinct skill-specific
effects of temperature on model performance, highlighting the complexity of
optimal temperature selection in practical applications. To address this
challenge, we propose a BERT-based temperature selector that takes advantage of
these observed effects to identify the optimal temperature for a given prompt.
We demonstrate that this approach can significantly improve the performance of
small and medium models in the SuperGLUE datasets. Furthermore, our study
extends to FP16 precision inference, revealing that temperature effects are
consistent with those observed in 4-bit quantized models. By evaluating
temperature effects up to 4.0 in three quantized models, we find that the
Mutation Temperature -- the point at which significant performance changes
occur -- increases with model size.

</details>


### [162] [Subjectivity in the Annotation of Bridging Anaphora](https://arxiv.org/abs/2506.07297)
*Lauren Levine,Amir Zeldes*

Main category: cs.CL

TL;DR: 论文探讨了桥接标注中的主观性，分析了标注者在识别桥接、解决前项和选择桥接子类型时的分歧，并提出新的分类系统。


<details>
  <summary>Details</summary>
Motivation: 桥接标注的主观性导致标注一致性低，研究旨在探索这种主观性及其影响。

Method: 在GUM语料库测试集上进行标注实验，提出新的桥接子类型分类系统，并与现有方案对比。

Result: 发现现有资源可能标注不足，桥接子类型标注一致性中等，但桥接实例识别一致性低。

Conclusion: 桥接标注的主观性显著，需改进标注方法和分类系统以提高一致性。

Abstract: Bridging refers to the associative relationship between inferable entities in
a discourse and the antecedents which allow us to understand them, such as
understanding what "the door" means with respect to an aforementioned "house".
As identifying associative relations between entities is an inherently
subjective task, it is difficult to achieve consistent agreement in the
annotation of bridging anaphora and their antecedents. In this paper, we
explore the subjectivity involved in the annotation of bridging instances at
three levels: anaphor recognition, antecedent resolution, and bridging subtype
selection. To do this, we conduct an annotation pilot on the test set of the
existing GUM corpus, and propose a newly developed classification system for
bridging subtypes, which we compare to previously proposed schemes. Our results
suggest that some previous resources are likely to be severely under-annotated.
We also find that while agreement on the bridging subtype category was
moderate, annotator overlap for exhaustively identifying instances of bridging
is low, and that many disagreements resulted from subjective understanding of
the entities involved.

</details>


### [163] [ConfQA: Answer Only If You Are Confident](https://arxiv.org/abs/2506.07309)
*Yin Huang,Yifan Ethan Xu,Kai Sun,Vera Yan,Alicia Sun,Haidar Khan,Jimmy Nguyen,Mohammad Kachuee,Zhaojiang Lin,Yue Liu,Aaron Colak,Anuj Kumar,Wen-tau Yih,Xin Luna Dong*

Main category: cs.CL

TL;DR: ConfQA是一种微调策略，通过训练LLM在不确定时承认“我不确定”，显著减少幻觉率。结合“仅在自信时回答”提示和知识图谱校准，实现了跨领域泛化，并提出了Dual Neural Knowledge框架以进一步提升准确性。


<details>
  <summary>Details</summary>
Motivation: 解决LLM在生成事实性陈述时的幻觉问题，提高其回答的准确性和可靠性。

Method: 采用ConfQA微调策略，结合“仅在自信时回答”提示和知识图谱校准，提出Dual Neural Knowledge框架。

Result: 幻觉率从20-40%降至5%以下，准确性提升至95%以上，外部检索减少30%。

Conclusion: ConfQA和Dual Neural Knowledge框架有效减少了LLM的幻觉，提升了回答的准确性和效率。

Abstract: Can we teach Large Language Models (LLMs) to refrain from hallucinating
factual statements? In this paper we present a fine-tuning strategy that we
call ConfQA, which can reduce hallucination rate from 20-40% to under 5% across
multiple factuality benchmarks. The core idea is simple: when the LLM answers a
question correctly, it is trained to continue with the answer; otherwise, it is
trained to admit "I am unsure". But there are two key factors that make the
training highly effective. First, we introduce a dampening prompt "answer only
if you are confident" to explicitly guide the behavior, without which
hallucination remains high as 15%-25%. Second, we leverage simple factual
statements, specifically attribute values from knowledge graphs, to help LLMs
calibrate the confidence, resulting in robust generalization across domains and
question types. Building on this insight, we propose the Dual Neural Knowledge
framework, which seamlessly select between internally parameterized neural
knowledge and externally recorded symbolic knowledge based on ConfQA's
confidence. The framework enables potential accuracy gains to beyond 95%, while
reducing unnecessary external retrievals by over 30%.

</details>


### [164] [Reward Model Interpretability via Optimal and Pessimal Tokens](https://arxiv.org/abs/2506.07326)
*Brian Christian,Hannah Rose Kirk,Jessica A. F. Thompson,Christopher Summerfield,Tsvetomira Dumbalska*

Main category: cs.CL

TL;DR: 论文提出了一种新方法，通过分析奖励模型在整个词汇空间中的响应来提升其可解释性，揭示了模型间的异质性、评分不对称性、对提示框架的敏感性以及对高频词的过度偏好。


<details>
  <summary>Details</summary>
Motivation: 奖励模型在将大型语言模型与人类价值观对齐中扮演关键角色，但其本身的可解释性研究较少。

Method: 通过分析奖励模型对所有可能单标记响应的评分，研究其行为和偏差。

Result: 发现模型间存在显著异质性、评分不对称性、对提示框架的敏感性以及对高频词的过度偏好，并揭示了潜在的偏见问题。

Conclusion: 研究挑战了奖励模型的可互换性假设，并指出其作为复杂人类价值观代理的局限性，可能传播偏见。

Abstract: Reward modeling has emerged as a crucial component in aligning large language
models with human values. Significant attention has focused on using reward
models as a means for fine-tuning generative models. However, the reward models
themselves -- which directly encode human value judgments by turning
prompt-response pairs into scalar rewards -- remain relatively understudied. We
present a novel approach to reward model interpretability through exhaustive
analysis of their responses across their entire vocabulary space. By examining
how different reward models score every possible single-token response to
value-laden prompts, we uncover several striking findings: (i) substantial
heterogeneity between models trained on similar objectives, (ii) systematic
asymmetries in how models encode high- vs low-scoring tokens, (iii) significant
sensitivity to prompt framing that mirrors human cognitive biases, and (iv)
overvaluation of more frequent tokens. We demonstrate these effects across ten
recent open-source reward models of varying parameter counts and architectures.
Our results challenge assumptions about the interchangeability of reward
models, as well as their suitability as proxies of complex and
context-dependent human values. We find that these models can encode concerning
biases toward certain identity groups, which may emerge as unintended
consequences of harmlessness training -- distortions that risk propagating
through the downstream large language models now deployed to millions.

</details>


### [165] [Improving LLM Reasoning through Interpretable Role-Playing Steering](https://arxiv.org/abs/2506.07335)
*Anyi Wang,Dong Shu,Yifan Wang,Yunpu Ma,Mengnan Du*

Main category: cs.CL

TL;DR: 论文提出了一种名为SRPS的新框架，通过识别和操纵与角色扮演行为相关的内部模型特征，提升了大型语言模型的推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要依赖提示工程，但缺乏稳定性和可解释性，因此需要一种更可靠的方法来增强模型的角色扮演能力。

Method: SRPS提取角色扮演提示的潜在表示，基于激活模式选择最相关特征，并构建可控制强度的转向向量注入模型的残差流中。

Result: 实验表明，SRPS在多个推理基准测试中显著提升了模型性能，例如Llama3.1-8B在CSQA上的准确率从31.86%提升至39.80%。

Conclusion: SRPS不仅提供了更细粒度的角色行为控制，还增强了模型的可解释性和稳定性，优于传统的提示工程方法。

Abstract: Role-playing has emerged as an effective technique for enhancing the
reasoning capabilities of large language models (LLMs). However, existing
methods primarily rely on prompt engineering, which often lacks stability and
interpretability. In this paper, we introduce Sparse Autoencoder Role-Playing
Steering (SRPS), a novel framework that identifies and manipulates internal
model features associated with role-playing behavior. Our approach extracts
latent representations from role-play prompts, selects the most relevant
features based on activation patterns, and constructs a steering vector that
can be injected into the model's residual stream with controllable intensity.
Our method enables fine-grained control over role-specific behavior and offers
insights into how role information influences internal model activations.
Extensive experiments across various reasoning benchmarks and model sizes
demonstrate consistent performance gains. Notably, in the zero-shot
chain-of-thought (CoT) setting, the accuracy of Llama3.1-8B on CSQA improves
from 31.86% to 39.80%, while Gemma2-9B on SVAMP increases from 37.50% to
45.10%. These results highlight the potential of SRPS to enhance reasoning
ability in LLMs, providing better interpretability and stability compared to
traditional prompt-based role-playing.

</details>


### [166] [Refusal-Feature-guided Teacher for Safe Finetuning via Data Filtering and Alignment Distillation](https://arxiv.org/abs/2506.07356)
*Seokil Ham,Yubin Choi,Seungju Cho,Yujin Yang,Younghun Kim,Changick Kim*

Main category: cs.CL

TL;DR: 论文提出了一种基于拒绝特征的教师模型（ReFT），用于在Finetuning-as-a-Service中过滤有害提示，以保护LLM的安全对齐性。


<details>
  <summary>Details</summary>
Motivation: 现有Finetuning-as-a-Service易因用户数据中的有害提示导致LLM安全对齐性下降，而现有方法未从根本上解决有害数据过滤问题。

Method: 利用安全对齐LLM中的拒绝特征区分有害与无害提示，训练ReFT模型作为教师，在微调时过滤有害数据并蒸馏对齐知识。

Result: 实验表明，ReFT策略显著减少有害输出并提升微调准确性。

Conclusion: ReFT为LLM在Finetuning-as-a-Service中的安全可靠部署提供了实用解决方案。

Abstract: Recently, major AI service providers such as Google and OpenAI have
introduced Finetuning-as-a-Service, which enables users to customize Large
Language Models (LLMs) for specific downstream tasks using their own data.
However, this service is vulnerable to degradation of LLM safety-alignment when
user data contains harmful prompts. While some prior works address this issue,
fundamentally filtering harmful data from user data remains unexplored.
Motivated by our observation that a directional representation reflecting
refusal behavior (called the refusal feature) obtained from safety-aligned LLMs
can inherently distinguish between harmful and harmless prompts, we propose the
Refusal-Feature-guided Teacher (ReFT). Our ReFT model is trained to identify
harmful prompts based on the similarity between input prompt features and its
refusal feature. During finetuning, the ReFT model serves as a teacher that
filters harmful prompts from user data and distills alignment knowledge into
the base model. Extensive experiments demonstrate that our ReFT-based
finetuning strategy effectively minimizes harmful outputs and enhances
finetuning accuracy for user-specific tasks, offering a practical solution for
secure and reliable deployment of LLMs in Finetuning-as-a-Service.

</details>


### [167] [SEED: Enhancing Text-to-SQL Performance and Practical Usability Through Automatic Evidence Generation](https://arxiv.org/abs/2506.07423)
*Janghyeon Yun,Sang-goo Lee*

Main category: cs.CL

TL;DR: SEED是一种自动生成证据的方法，用于改进文本到SQL转换的性能和实用性，特别是在无证据场景下表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有文本到SQL研究依赖BIRD数据集，但该数据集假设用户具备专业知识和领域知识，且人工生成的证据存在缺陷，影响模型性能。

Method: SEED通过系统分析数据库模式、描述文件和值来自动生成证据。

Result: SEED在BIRD和Spider数据集上显著提高了SQL生成准确性，甚至在某些情况下优于提供BIRD证据的设置。

Conclusion: SEED生成的证据不仅填补了研究与实际部署之间的差距，还提升了文本到SQL模型的适应性和鲁棒性。

Abstract: Text-to-SQL enables non-experts to retrieve data from databases by converting
natural language queries into SQL. However, state-of-the-art text-to-SQL
studies rely on the BIRD dataset, which assumes that evidence is provided along
with questions. Although BIRD facilitates research advancements, it assumes
that users have expertise and domain knowledge, contradicting the fundamental
goal of text-to-SQL. In addition, human-generated evidence in BIRD contains
defects, including missing or erroneous evidence, which affects model
performance. To address this issue, we propose SEED (System for Evidence
Extraction and Domain knowledge generation), an approach that automatically
generates evidence to improve performance and practical usability in real-world
scenarios. SEED systematically analyzes database schema, description files, and
values to extract relevant information. We evaluated SEED on BIRD and Spider,
demonstrating that it significantly improves SQL generation accuracy in the
no-evidence scenario, and in some cases, even outperforms the setting where
BIRD evidence is provided. Our results highlight that SEED-generated evidence
not only bridges the gap between research and real-world deployment but also
improves the adaptability and robustness of text-to-SQL models. Our code is
available at https://github.com/felix01189/SEED

</details>


### [168] [Plug-in and Fine-tuning: Bridging the Gap between Small Language Models and Large Language Models](https://arxiv.org/abs/2506.07424)
*Kyeonghyun Kim,Jinhee Jang,Juhwan Choi,Yoonji Lee,Kyohoon Jin,YoungBin Kim*

Main category: cs.CL

TL;DR: PiFi框架通过将大型语言模型（LLM）的冻结层集成到小型语言模型（SLM）中，结合两者的优势，既保持了高效性又提升了性能。


<details>
  <summary>Details</summary>
Motivation: 解决大型语言模型（LLM）计算资源需求高和小型语言模型（SLM）泛化能力不足的问题。

Method: 将LLM的冻结层集成到SLM中，并对组合模型进行特定任务的微调。

Result: PiFi在多种自然语言处理任务中表现优异，提升了泛化能力和语言能力迁移。

Conclusion: PiFi成功结合了LLM和SLM的优势，为资源受限环境提供了高效解决方案。

Abstract: Large language models (LLMs) are renowned for their extensive linguistic
knowledge and strong generalization capabilities, but their high computational
demands make them unsuitable for resource-constrained environments. In
contrast, small language models (SLMs) are computationally efficient but often
lack the broad generalization capacity of LLMs. To bridge this gap, we propose
PiFi, a novel framework that combines the strengths of both LLMs and SLMs to
achieve high performance while maintaining efficiency. PiFi integrates a single
frozen layer from an LLM into a SLM and fine-tunes the combined model for
specific tasks, boosting performance without a significant increase in
computational cost. We show that PiFi delivers consistent performance
improvements across a range of natural language processing tasks, including
both natural language understanding and generation. Moreover, our findings
demonstrate PiFi's ability to effectively leverage LLM knowledge, enhancing
generalization to unseen domains and facilitating the transfer of linguistic
abilities.

</details>


### [169] [Conjoined Predication and Scalar Implicature](https://arxiv.org/abs/2506.07429)
*Ratna Kandala*

Main category: cs.CL

TL;DR: Magri (2016) 研究了两个由连词引起的谜题，其中一个仍未解决。本文通过理论框架分析，认为该谜题的怪异源于连词谓语的集体或并发解读，并指出标量含义的语用机制超出了现有语法解释的范围。


<details>
  <summary>Details</summary>
Motivation: 解决Magri (2016) 中未解决的第一个谜题，揭示量化、集体/并发解读与语境更新之间的隐藏互动。

Method: 通过理论框架分析，探讨连词谓语的集体或并发解读如何导致间接语境矛盾。

Result: 发现怪异源于集体或并发解读，标量含义的语用机制比现有语法解释更广泛。

Conclusion: 标量含义的生成机制需扩展现有理论，以涵盖集体或并发解读引发的语境矛盾。

Abstract: Magri (2016) investigates two puzzles arising from conjunction. Although
Magri has proposed a solution to the second puzzle, the first remains
unresolved. This first puzzle reveals a hidden interaction among
quantification, collective/concurrent interpretation, and contextual updating
dimensions that have yet to be explored. In essence, the problem is that
certain forms of sentences like "Some Italians come from a warm country," when
conjoined as in "(Only) Some Italians come from a warm country and are blond,"
sound infelicitous, even though no obvious alternative triggers a conflicting
scalar implicature. In this paper, we offer a conceptual analysis of Magri's
first puzzle by situating it within its original theoretical framework. We
argue that the oddness arises from the collective or concurrent reading of the
conjunctive predicate: in examples such as "(Only) Some Italians come from a
warm country and are blond," this interpretation generates an indirect
contextual contradiction. Moreover, we suggest that the pragmatic mechanisms
governing scalar implicature generation extend beyond what is captured by
exhaustification-based grammatical licensing accounts.

</details>


### [170] [Well Begun is Half Done: Low-resource Preference Alignment by Weak-to-Strong Decoding](https://arxiv.org/abs/2506.07434)
*Feifan Song,Shaohang Wei,Wen Luo,Yuxuan Fan,Tianyu Liu,Guoyin Wang,Houfeng Wang*

Main category: cs.CL

TL;DR: 论文提出了一种名为弱到强解码（WSD）的新框架，通过小模型引导大模型生成对齐内容，解决了低资源对齐方法的挑战。


<details>
  <summary>Details</summary>
Motivation: 为避免大语言模型生成不当内容，需要对齐人类偏好，但现有低资源方法难以同时保证高质量和对齐。

Method: WSD框架利用小型对齐模型生成对齐的开头，再由大模型继续生成剩余内容，并通过自动切换机制控制。

Result: 实验表明，WSD框架显著提升基线方法性能，且避免了对下游任务的负面影响（对齐税）。

Conclusion: WSD框架有效增强了大模型的对齐能力，同时保持了高质量内容生成。

Abstract: Large Language Models (LLMs) require alignment with human preferences to
avoid generating offensive, false, or meaningless content. Recently,
low-resource methods for LLM alignment have been popular, while still facing
challenges in obtaining both high-quality and aligned content. Motivated by the
observation that the difficulty of generating aligned responses is concentrated
at the beginning of decoding, we propose a novel framework, Weak-to-Strong
Decoding (WSD), to enhance the alignment ability of base models by the guidance
of a small aligned model. The small model first drafts well-aligned beginnings,
followed by the large base model to continue the rest, controlled by a
well-designed auto-switch mechanism. We also collect a new dataset, GenerAlign,
to fine-tune a small-sized Pilot-3B as the draft model, which effectively
enhances different base models under the WSD framework to outperform all
baseline methods, while avoiding degradation on downstream tasks, termed as the
alignment tax. Extensive experiments are further conducted to examine the
impact of different settings and time efficiency, as well as analyses on the
intrinsic mechanisms of WSD in depth.

</details>


### [171] [LG-ANNA-Embedding technical report](https://arxiv.org/abs/2506.07438)
*Jooyoung Choi,Hyun Kim,Hansol Jang,Changwook Jun,Kyunghoon Bae,Hyewon Choi,Stanley Jungkyu Choi,Honglak Lee,Chulmin Yun*

Main category: cs.CL

TL;DR: 提出了一种基于指令的统一框架，用于学习适用于信息检索（IR）和非IR任务的广义文本嵌入，结合上下文学习、软监督和自适应硬负样本挖掘，无需任务特定微调。


<details>
  <summary>Details</summary>
Motivation: 解决传统文本嵌入模型在任务泛化性和性能上的局限性，通过统一框架优化多任务性能。

Method: 基于Mistral-7B模型，结合上下文指令、软监督（连续相关性评分）和自适应硬负样本挖掘，生成上下文感知嵌入。

Result: 在MTEB（英语，v2）基准测试的41个任务中表现优异，优于多个更大或完全微调的基线模型。

Conclusion: 结合上下文提示、软监督和自适应采样，可高效生成高质量嵌入，具有广泛适用性。

Abstract: This report presents a unified instruction-based framework for learning
generalized text embeddings optimized for both information retrieval (IR) and
non-IR tasks. Built upon a decoder-only large language model (Mistral-7B), our
approach combines in-context learning, soft supervision, and adaptive
hard-negative mining to generate context-aware embeddings without task-specific
fine-tuning. Structured instructions and few-shot examples are used to guide
the model across diverse tasks, enabling strong performance on classification,
semantic similarity, clustering, and reranking benchmarks. To improve semantic
discrimination, we employ a soft labeling framework where continuous relevance
scores, distilled from a high-performance dense retriever and reranker, serve
as fine-grained supervision signals. In addition, we introduce adaptive
margin-based hard-negative mining, which filters out semantically ambiguous
negatives based on their similarity to positive examples, thereby enhancing
training stability and retrieval robustness. Our model is evaluated on the
newly introduced MTEB (English, v2) benchmark, covering 41 tasks across seven
categories. Results show that our method achieves strong generalization and
ranks among the top-performing models by Borda score, outperforming several
larger or fully fine-tuned baselines. These findings highlight the
effectiveness of combining in-context prompting, soft supervision, and adaptive
sampling for scalable, high-quality embedding generation.

</details>


### [172] [Understanding Cross-Domain Adaptation in Low-Resource Topic Modeling](https://arxiv.org/abs/2506.07453)
*Pritom Saha Akash,Kevin Chen-Chuan Chang*

Main category: cs.CL

TL;DR: 论文提出了一种针对低资源主题建模的领域适应方法DALTA，通过共享编码器和对抗对齐实现知识迁移，显著提升了主题一致性和稳定性。


<details>
  <summary>Details</summary>
Motivation: 现有主题建模模型在低资源环境下表现不佳，导致主题推断不稳定且不连贯。

Method: 提出DALTA框架，结合共享编码器、专用解码器和对抗对齐，实现领域间知识选择性迁移。

Result: DALTA在多个低资源数据集上表现优于现有方法，主题一致性、稳定性和迁移性显著提升。

Conclusion: DALTA为低资源主题建模提供了一种有效的领域适应解决方案。

Abstract: Topic modeling plays a vital role in uncovering hidden semantic structures
within text corpora, but existing models struggle in low-resource settings
where limited target-domain data leads to unstable and incoherent topic
inference. We address this challenge by formally introducing domain adaptation
for low-resource topic modeling, where a high-resource source domain informs a
low-resource target domain without overwhelming it with irrelevant content. We
establish a finite-sample generalization bound showing that effective knowledge
transfer depends on robust performance in both domains, minimizing latent-space
discrepancy, and preventing overfitting to the data. Guided by these insights,
we propose DALTA (Domain-Aligned Latent Topic Adaptation), a new framework that
employs a shared encoder for domain-invariant features, specialized decoders
for domain-specific nuances, and adversarial alignment to selectively transfer
relevant information. Experiments on diverse low-resource datasets demonstrate
that DALTA consistently outperforms state-of-the-art methods in terms of topic
coherence, stability, and transferability.

</details>


### [173] [KScope: A Framework for Characterizing the Knowledge Status of Language Models](https://arxiv.org/abs/2506.07458)
*Yuxin Xiao,Shan Chen,Jack Gallifant,Danielle Bitterman,Thomas Hartvigsen,Marzyeh Ghassemi*

Main category: cs.CL

TL;DR: 论文提出了一种名为KScope的分层框架，用于评估大型语言模型（LLM）的知识状态，并将其分为五种类型。通过实验验证了上下文支持对缩小知识差距的作用，并分析了影响知识更新的关键特征。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要关注知识冲突，无法全面评估LLM对问题的知识掌握程度，因此需要更系统的分类和评估框架。

Method: 提出KScope框架，通过分层统计测试将LLM知识分为五种状态，并在九个LLM和四个数据集上验证其有效性。

Result: 上下文支持缩小知识差距；难度、相关性和熟悉度等特征影响知识更新；不同LLM在知识状态下的行为差异显著。

Conclusion: KScope框架能有效评估LLM知识状态，上下文特征分析和增强可信度可进一步提升知识更新效果。

Abstract: Characterizing a large language model's (LLM's) knowledge of a given question
is challenging. As a result, prior work has primarily examined LLM behavior
under knowledge conflicts, where the model's internal parametric memory
contradicts information in the external context. However, this does not fully
reflect how well the model knows the answer to the question. In this paper, we
first introduce a taxonomy of five knowledge statuses based on the consistency
and correctness of LLM knowledge modes. We then propose KScope, a hierarchical
framework of statistical tests that progressively refines hypotheses about
knowledge modes and characterizes LLM knowledge into one of these five
statuses. We apply KScope to nine LLMs across four datasets and systematically
establish: (1) Supporting context narrows knowledge gaps across models. (2)
Context features related to difficulty, relevance, and familiarity drive
successful knowledge updates. (3) LLMs exhibit similar feature preferences when
partially correct or conflicted, but diverge sharply when consistently wrong.
(4) Context summarization constrained by our feature analysis, together with
enhanced credibility, further improves update effectiveness and generalizes
across LLMs.

</details>


### [174] [From Calibration to Collaboration: LLM Uncertainty Quantification Should Be More Human-Centered](https://arxiv.org/abs/2506.07461)
*Siddartha Devic,Tejas Srinivasan,Jesse Thomason,Willie Neiswanger,Vatsal Sharan*

Main category: cs.CL

TL;DR: 论文指出当前LLM不确定性量化方法在实用性上存在问题，并提出改进方向。


<details>
  <summary>Details</summary>
Motivation: LLM的可靠性问题影响用户信任，需优化不确定性量化方法以提升人机协作。

Method: 分析了40种LLM不确定性量化方法，识别了三大问题并提出改进建议。

Result: 发现当前方法在生态效度、不确定性类型和评价指标上存在不足。

Conclusion: 建议采用更以用户为中心的不确定性量化方法，提升实际应用价值。

Abstract: Large Language Models (LLMs) are increasingly assisting users in the real
world, yet their reliability remains a concern. Uncertainty quantification (UQ)
has been heralded as a tool to enhance human-LLM collaboration by enabling
users to know when to trust LLM predictions. We argue that current practices
for uncertainty quantification in LLMs are not optimal for developing useful UQ
for human users making decisions in real-world tasks. Through an analysis of 40
LLM UQ methods, we identify three prevalent practices hindering the community's
progress toward its goal of benefiting downstream users: 1) evaluating on
benchmarks with low ecological validity; 2) considering only epistemic
uncertainty; and 3) optimizing metrics that are not necessarily indicative of
downstream utility. For each issue, we propose concrete user-centric practices
and research directions that LLM UQ researchers should consider. Instead of
hill-climbing on unrepresentative tasks using imperfect metrics, we argue that
the community should adopt a more human-centered approach to LLM uncertainty
quantification.

</details>


### [175] [CCI4.0: A Bilingual Pretraining Dataset for Enhancing Reasoning in Large Language Models](https://arxiv.org/abs/2506.07463)
*Guang Liu,Liangdong Wang,Jijie Li,Yang Yu,Yao Xu,Jiabei Chen,Yu Bai,Feng Liao,Yonghua Lin*

Main category: cs.CL

TL;DR: CCI4.0是一个大规模双语预训练数据集，包含高质量数据和多样化推理轨迹，通过创新数据处理流程显著提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 解决预训练数据质量动态变化和多样化推理需求的问题，提升语言模型在数学和代码任务中的表现。

Method: 提出两阶段去重、多分类器质量评分和领域感知流畅性过滤的数据处理流程，并提取45亿条多样化推理模板。

Result: 实验证明CCI4.0预训练的模型在下游任务中表现更优，尤其在数学和代码任务中。

Conclusion: 严格的数据处理和多样化推理模板对提升语言模型性能至关重要，为自动处理预训练语料提供了新思路。

Abstract: We introduce CCI4.0, a large-scale bilingual pre-training dataset engineered
for superior data quality and diverse human-like reasoning trajectory. CCI4.0
occupies roughly $35$ TB of disk space and comprises two sub-datasets:
CCI4.0-M2-Base and CCI4.0-M2-CoT. CCI4.0-M2-Base combines a $5.2$ TB carefully
curated Chinese web corpus, a $22.5$ TB English subset from Nemotron-CC, and
diverse sources from math, wiki, arxiv, and code. Although these data are
mostly sourced from well-processed datasets, the quality standards of various
domains are dynamic and require extensive expert experience and labor to
process. So, we propose a novel pipeline justifying data quality mainly based
on models through two-stage deduplication, multiclassifier quality scoring, and
domain-aware fluency filtering. We extract $4.5$ billion pieces of
CoT(Chain-of-Thought) templates, named CCI4.0-M2-CoT. Differing from the
distillation of CoT from larger models, our proposed staged CoT extraction
exemplifies diverse reasoning patterns and significantly decreases the
possibility of hallucination. Empirical evaluations demonstrate that LLMs
pre-trained in CCI4.0 benefit from cleaner, more reliable training signals,
yielding consistent improvements in downstream tasks, especially in math and
code reflection tasks. Our results underscore the critical role of rigorous
data curation and human thinking templates in advancing LLM performance,
shedding some light on automatically processing pretraining corpora.

</details>


### [176] [Improving Fairness of Large Language Models in Multi-document Summarization](https://arxiv.org/abs/2506.07479)
*Haoyuan Li Yusen Zhang,Snigdha Chaturvedi*

Main category: cs.CL

TL;DR: FairPO是一种偏好调整方法，旨在提高多文档摘要（MDS）中的摘要级和语料库级公平性。通过扰动文档集生成偏好对，并动态调整偏好对权重，FairPO在保持摘要质量的同时优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 多文档摘要中的公平性对决策至关重要，但现有方法主要关注摘要级公平性，忽略了语料库级公平性。FairPO旨在同时解决这两个层面的公平性问题。

Method: FairPO通过扰动文档集生成偏好对以提高摘要级公平性，并通过动态调整偏好对权重以优化语料库级公平性。

Result: 实验表明，FairPO在保持摘要质量的同时，显著优于现有基线方法。

Conclusion: FairPO为多文档摘要中的公平性问题提供了有效的解决方案，兼顾了摘要级和语料库级公平性。

Abstract: Fairness in multi-document summarization (MDS) is crucial for providing
comprehensive views across documents with diverse social attribute values,
which can significantly impact decision-making. For example, a summarization
system that tends to overrepresent negative reviews of products can mislead
customers into disregarding good products. Previous works measure fairness in
MDS at two levels: summary-level and corpus-level. While summary-level fairness
focuses on individual summaries, corpus-level fairness focuses on a corpus of
summaries. Recent methods primarily focus on summary-level fairness. We propose
FairPO, a preference tuning method that focuses on both summary-level and
corpus-level fairness in MDS. To improve summary-level fairness, we propose to
generate preference pairs by perturbing document sets. To improve corpus-level
fairness, we propose fairness-aware preference tuning by dynamically adjusting
the weights of preference pairs. Our experiments show that FairPO outperforms
strong baselines while maintaining the critical qualities of summaries. The
code is available at https://github.com/leehaoyuan/coverage_fairnes.

</details>


### [177] [A Hybrid GA LLM Framework for Structured Task Optimization](https://arxiv.org/abs/2506.07483)
*Berry Feng,Jonas Lin,Patrick Lau*

Main category: cs.CL

TL;DR: GA LLM结合遗传算法与大型语言模型，通过迭代优化生成结构化输出，满足严格约束。


<details>
  <summary>Details</summary>
Motivation: 解决语言模型在结构化生成任务中难以满足严格约束的问题，结合遗传算法的全局优化能力。

Method: 将输出视为基因，利用语言模型指导选择、交叉和变异操作，迭代优化解决方案。

Result: 在行程规划、学术大纲和商业报告等任务中表现优异，生成结果结构良好且满足需求。

Conclusion: GA LLM通过结合语言模型和遗传算法，显著提升了约束满足和解决方案质量。

Abstract: GA LLM is a hybrid framework that combines Genetic Algorithms with Large
Language Models to handle structured generation tasks under strict constraints.
Each output, such as a plan or report, is treated as a gene, and evolutionary
operations like selection, crossover, and mutation are guided by the language
model to iteratively improve solutions. The language model provides domain
knowledge and creative variation, while the genetic algorithm ensures
structural integrity and global optimization. GA LLM has proven effective in
tasks such as itinerary planning, academic outlining, and business reporting,
consistently producing well structured and requirement satisfying results. Its
modular design also makes it easy to adapt to new tasks. Compared to using a
language model alone, GA LLM achieves better constraint satisfaction and higher
quality solutions by combining the strengths of both components.

</details>


### [178] [DEBATE: A Dataset for Disentangling Textual Ambiguity in Mandarin Through Speech](https://arxiv.org/abs/2506.07502)
*Haotian Guo,Jing Han,Yongfeng Tu,Shihao Gao,Shengfan Shen,Wulong Xiang,Weihao Gan,Zixing Zhang*

Main category: cs.CL

TL;DR: 论文介绍了DEBATE数据集，用于研究语音如何解决文本歧义，并比较了机器与人类在理解语音意图上的差距。


<details>
  <summary>Details</summary>
Motivation: 语音消歧（DTS）研究不足，缺乏高质量数据集，因此提出DEBATE填补这一空白。

Method: 构建包含1,001条歧义语句的汉语语音-文本数据集，记录10位母语者的发音，分析语音特征。

Result: 比较了三种先进模型，显示机器与人类在语音意图理解上的显著差距。

Conclusion: DEBATE是首个此类数据集，为跨语言和文化的研究奠定基础。

Abstract: Despite extensive research on textual and visual disambiguation,
disambiguation through speech (DTS) remains underexplored. This is largely due
to the lack of high-quality datasets that pair spoken sentences with richly
ambiguous text. To address this gap, we present DEBATE, a unique public Chinese
speech-text dataset designed to study how speech cues and
patterns-pronunciation, pause, stress and intonation-can help resolve textual
ambiguity and reveal a speaker's true intent. DEBATE contains 1,001 carefully
selected ambiguous utterances, each recorded by 10 native speakers, capturing
diverse linguistic ambiguities and their disambiguation through speech. We
detail the data collection pipeline and provide rigorous quality analysis.
Additionally, we benchmark three state-of-the-art large speech and
audio-language models, illustrating clear and huge performance gaps between
machine and human understanding of spoken intent. DEBATE represents the first
effort of its kind and offers a foundation for building similar DTS datasets
across languages and cultures. The dataset and associated code are available
at: https://github.com/SmileHnu/DEBATE.

</details>


### [179] [What Do Indonesians Really Need from Language Technology? A Nationwide Survey](https://arxiv.org/abs/2506.07506)
*Muhammad Dehan Al Kautsar,Lucky Susanto,Derry Wijaya,Fajri Koto*

Main category: cs.CL

TL;DR: 研究调查了印度尼西亚700多种本地语言社区对语言技术的实际需求，发现机器翻译和信息检索是首要需求，同时需关注隐私和偏见问题。


<details>
  <summary>Details</summary>
Motivation: 了解印度尼西亚多语言社区对语言技术的真实需求，以指导资源分配和技术开发。

Method: 通过全国性调查评估本地语言社区的需求。

Result: 机器翻译和信息检索是最迫切需求，但隐私和偏见问题需透明化处理。

Conclusion: 需优先开发机器翻译和信息检索技术，并加强透明沟通以促进AI应用。

Abstract: There is an emerging effort to develop NLP for Indonesias 700+ local
languages, but progress remains costly due to the need for direct engagement
with native speakers. However, it is unclear what these language communities
truly need from language technology. To address this, we conduct a nationwide
survey to assess the actual needs of native speakers in Indonesia. Our findings
indicate that addressing language barriers, particularly through machine
translation and information retrieval, is the most critical priority. Although
there is strong enthusiasm for advancements in language technology, concerns
around privacy, bias, and the use of public data for AI training highlight the
need for greater transparency and clear communication to support broader AI
adoption.

</details>


### [180] [DeRAGEC: Denoising Named Entity Candidates with Synthetic Rationale for ASR Error Correction](https://arxiv.org/abs/2506.07510)
*Solee Im,Wonjun Lee,Jinmyeong An,Yunsu Kim,Jungseul Ok,Gary Geunbae Lee*

Main category: cs.CL

TL;DR: DeRAGEC通过合成去噪逻辑和改进检索增强生成框架，显著提升了ASR系统中的命名实体纠正效果，无需额外训练。


<details>
  <summary>Details</summary>
Motivation: 改进自动语音识别（ASR）系统中的命名实体（NE）纠正，以降低词错误率（WER）并提高NE命中率。

Method: 扩展RAGEC框架，利用合成去噪逻辑过滤噪声NE候选，结合语音相似性和增强定义，通过上下文学习优化检索结果。

Result: 在CommonVoice和STOP数据集上，WER相对降低28%，NE命中率显著提升，优于基线ASR和RAGEC方法。

Conclusion: DeRAGEC是一种高效且无需训练的NE纠正方法，显著提升了ASR系统的性能。

Abstract: We present DeRAGEC, a method for improving Named Entity (NE) correction in
Automatic Speech Recognition (ASR) systems. By extending the
Retrieval-Augmented Generative Error Correction (RAGEC) framework, DeRAGEC
employs synthetic denoising rationales to filter out noisy NE candidates before
correction. By leveraging phonetic similarity and augmented definitions, it
refines noisy retrieved NEs using in-context learning, requiring no additional
training. Experimental results on CommonVoice and STOP datasets show
significant improvements in Word Error Rate (WER) and NE hit ratio,
outperforming baseline ASR and RAGEC methods. Specifically, we achieved a 28%
relative reduction in WER compared to ASR without postprocessing. Our source
code is publicly available at: https://github.com/solee0022/deragec

</details>


### [181] [Towards Large Language Models with Self-Consistent Natural Language Explanations](https://arxiv.org/abs/2506.07523)
*Sahar Admoni,Ofra Amir,Assaf Hallak,Yftah Ziser*

Main category: cs.CL

TL;DR: 论文提出了一种新方法（PSCB）来评估LLM生成解释的自一致性，并提出了改进的指标和优化方法（DPO）以提升解释质量。


<details>
  <summary>Details</summary>
Motivation: 现有LLM生成的事后解释常与真实决策过程不一致，缺乏系统性解决方案。

Method: 引入PSCB基准，分析自一致性，提出新指标，并通过DPO优化LLM。

Result: 自一致性分数在正确与错误预测间差异小，新指标更有效，DPO显著提升解释与决策特征的对齐。

Conclusion: PSCB和DPO为提升LLM解释的可信度和自一致性提供了可扩展路径。

Abstract: Large language models (LLMs) seem to offer an easy path to interpretability:
just ask them to explain their decisions. Yet, studies show that these post-hoc
explanations often misrepresent the true decision process, as revealed by
mismatches in feature importance. Despite growing evidence of this
inconsistency, no systematic solutions have emerged, partly due to the high
cost of estimating feature importance, which limits evaluations to small
datasets. To address this, we introduce the Post-hoc Self-Consistency Bank
(PSCB) - a large-scale benchmark of decisions spanning diverse tasks and
models, each paired with LLM-generated explanations and corresponding feature
importance scores. Analysis of PSCB reveals that self-consistency scores barely
differ between correct and incorrect predictions. We also show that the
standard metric fails to meaningfully distinguish between explanations. To
overcome this limitation, we propose an alternative metric that more
effectively captures variation in explanation quality. We use it to fine-tune
LLMs via Direct Preference Optimization (DPO), leading to significantly better
alignment between explanations and decision-relevant features, even under
domain shift. Our findings point to a scalable path toward more trustworthy,
self-consistent LLMs.

</details>


### [182] [Bit-level BPE: Below the byte boundary](https://arxiv.org/abs/2506.07541)
*Sangwhan Moon,Tatsuya Hiraoka,Naoaki Okazaki*

Main category: cs.CL

TL;DR: 提出了一种无损压缩技术，减少字节级回退分词在长尾字符（如CJK和表情符号）中增加的序列长度。


<details>
  <summary>Details</summary>
Motivation: 字节级回退分词虽能有效防止OOV，但在处理长尾字符时会显著增加序列长度，导致计算效率下降。

Method: 提出了一种简单的无损压缩技术。

Result: 该方法能够减少序列长度，同时保持信息完整性。

Conclusion: 该技术为处理字符多样性提供了一种高效解决方案。

Abstract: Byte-level fallbacks for subword tokenization have become a common practice
in large language models. In particular, it has been demonstrated to be
incredibly effective as a pragmatic solution for preventing OOV, especially in
the context of larger models. However, breaking a character down to individual
bytes significantly increases the sequence length for long-tail tokens in
languages such as Chinese, Japanese, and Korean (CJK) and other
character-diverse contexts such as emoji. The increased sequence length results
in longer computation during both training and inference. In this work, we
propose a simple compression technique that reduces the sequence length
losslessly.

</details>


### [183] [SELT: Self-Evaluation Tree Search for LLMs with Task Decomposition](https://arxiv.org/abs/2506.07557)
*Mengsong Wu,Di Zhang,Yuqiang Li,Dongzhan Zhou,Wenliang Chen*

Main category: cs.CL

TL;DR: SELT是一种通过改进的蒙特卡洛树搜索（MCTS）增强LLM推理能力的新框架，无需依赖外部奖励模型。


<details>
  <summary>Details</summary>
Motivation: 尽管LLM在多种应用中表现优异，但在复杂推理任务中性能下降。

Method: 通过重新定义置信上限评分与LLM的自我评估能力对齐，并将推理过程分解为原子子任务，结合语义聚类。

Result: 在MMLU和Seal-Tools等基准测试中，SELT显著提高了答案准确性和推理鲁棒性。

Conclusion: SELT无需任务特定微调，展示了强大的跨任务泛化能力。

Abstract: While Large Language Models (LLMs) have achieved remarkable success in a wide
range of applications, their performance often degrades in complex reasoning
tasks. In this work, we introduce SELT (Self-Evaluation LLM Tree Search), a
novel framework that leverages a modified Monte Carlo Tree Search (MCTS) to
enhance LLM reasoning without relying on external reward models. By redefining
the Upper Confidence Bound scoring to align with intrinsic self-evaluation
capabilities of LLMs and decomposing the inference process into atomic subtasks
augmented with semantic clustering at each node, SELT effectively balances
exploration and exploitation, reduces redundant reasoning paths, and mitigates
hallucination. We validate our approach on challenging benchmarks, including
the knowledge-based MMLU and the Tool Learning dataset Seal-Tools, where SELT
achieves significant improvements in answer accuracy and reasoning robustness
compared to baseline methods. Notably, our framework operates without
task-specific fine-tuning, demonstrating strong generalizability across diverse
reasoning tasks. Relevant results and code are available at
https://github.com/fairyshine/SELT .

</details>


### [184] [Beyond the Sentence: A Survey on Context-Aware Machine Translation with Large Language Models](https://arxiv.org/abs/2506.07583)
*Ramakrishna Appicharla,Baban Gain,Santanu Pal,Asif Ekbal*

Main category: cs.CL

TL;DR: 本文综述了大型语言模型（LLMs）在上下文感知机器翻译中的应用，比较了商业和开源LLMs的表现，并探讨了未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 尽管LLMs在机器翻译中的应用尚未充分探索，尤其是在上下文感知场景中，本文旨在填补这一空白。

Method: 通过文献综述，分析了提示和微调方法，以及自动后编辑和翻译代理的构建。

Result: 商业LLMs（如ChatGPT）表现优于开源LLMs（如Llama），提示方法可作为翻译质量评估的基准。

Conclusion: 未来研究可进一步探索上下文感知翻译的潜力，尤其是在自动后编辑和翻译代理方面。

Abstract: Despite the popularity of the large language models (LLMs), their application
to machine translation is relatively underexplored, especially in context-aware
settings. This work presents a literature review of context-aware translation
with LLMs. The existing works utilise prompting and fine-tuning approaches,
with few focusing on automatic post-editing and creating translation agents for
context-aware machine translation. We observed that the commercial LLMs (such
as ChatGPT and Tower LLM) achieved better results than the open-source LLMs
(such as Llama and Bloom LLMs), and prompt-based approaches serve as good
baselines to assess the quality of translations. Finally, we present some
interesting future directions to explore.

</details>


### [185] [Instructing Large Language Models for Low-Resource Languages: A Systematic Study for Basque](https://arxiv.org/abs/2506.07597)
*Oscar Sainz,Naiara Perez,Julen Etxaniz,Joseba Fernandez de Landa,Itziar Aldabe,Iker García-Ferrero,Aimar Zabala,Ekhi Azurmendi,German Rigau,Eneko Agirre,Mikel Artetxe,Aitor Soroa*

Main category: cs.CL

TL;DR: 本文探讨了在低资源语言场景下，如何利用目标语言语料库、多语言基础模型和合成指令来替代传统指令适应流程，并通过实验证明指令调优模型优于非指令模型。


<details>
  <summary>Details</summary>
Motivation: 解决低资源语言因缺乏大规模指令数据集而难以适应语言模型的问题。

Method: 利用目标语言语料库、多语言基础模型和合成指令，通过实验系统研究不同组合的效果。

Result: 实验表明，目标语言语料库至关重要，合成指令能生成鲁棒模型，指令调优模型表现优于非指令模型，且规模扩大效果更佳。

Conclusion: 在低资源语言场景下，指令调优模型结合合成指令和目标语料库是高效解决方案，且支持未来研究的完全可复现性。

Abstract: Instructing language models with user intent requires large instruction
datasets, which are only available for a limited set of languages. In this
paper, we explore alternatives to conventional instruction adaptation pipelines
in low-resource scenarios. We assume a realistic scenario for low-resource
languages, where only the following are available: corpora in the target
language, existing open-weight multilingual base and instructed backbone LLMs,
and synthetically generated instructions sampled from the instructed backbone.
We present a comprehensive set of experiments for Basque that systematically
study different combinations of these components evaluated on benchmarks and
human preferences from 1,680 participants. Our conclusions show that target
language corpora are essential, with synthetic instructions yielding robust
models, and, most importantly, that using as backbone an instruction-tuned
model outperforms using a base non-instructed model, and improved results when
scaling up. Using Llama 3.1 instruct 70B as backbone our model comes near
frontier models of much larger sizes for Basque, without using any Basque data
apart from the 1.2B word corpora. We release code, models, instruction
datasets, and human preferences to support full reproducibility in future
research on low-resource language adaptation.

</details>


### [186] [PolitiSky24: U.S. Political Bluesky Dataset with User Stance Labels](https://arxiv.org/abs/2506.07606)
*Peyman Rostami,Vahid Rahimzadeh,Ali Adibi,Azadeh Shakery*

Main category: cs.CL

TL;DR: 论文提出了首个针对2024年美国总统选举的立场检测数据集PolitiSky24，基于Bluesky平台，聚焦于Kamala Harris和Donald Trump，包含16,044个用户-目标立场对。


<details>
  <summary>Details</summary>
Motivation: 现有数据集主要关注推文级立场，而用户级立场数据稀缺，尤其是在新兴平台如Bluesky上。用户级立场检测能更全面地反映用户观点。

Method: 采用结合高级信息检索和大语言模型的标注流程，生成立场标签及支持理由，确保透明性。标注方法使用可扩展的大语言模型，准确率达81%。

Result: 构建了包含用户发布历史、互动图和元数据的丰富数据集，填补了政治立场分析的空白。

Conclusion: PolitiSky24以其时效性、开放性和用户级视角，为政治立场分析提供了新资源。

Abstract: Stance detection identifies the viewpoint expressed in text toward a specific
target, such as a political figure. While previous datasets have focused
primarily on tweet-level stances from established platforms, user-level stance
resources, especially on emerging platforms like Bluesky remain scarce.
User-level stance detection provides a more holistic view by considering a
user's complete posting history rather than isolated posts. We present the
first stance detection dataset for the 2024 U.S. presidential election,
collected from Bluesky and centered on Kamala Harris and Donald Trump. The
dataset comprises 16,044 user-target stance pairs enriched with engagement
metadata, interaction graphs, and user posting histories. PolitiSky24 was
created using a carefully evaluated pipeline combining advanced information
retrieval and large language models, which generates stance labels with
supporting rationales and text spans for transparency. The labeling approach
achieves 81\% accuracy with scalable LLMs. This resource addresses gaps in
political stance analysis through its timeliness, open-data nature, and
user-level perspective. The dataset is available at
https://doi.org/10.5281/zenodo.15616911

</details>


### [187] [Vuyko Mistral: Adapting LLMs for Low-Resource Dialectal Translation](https://arxiv.org/abs/2506.07617)
*Roman Kyslyi,Yuliia Maksymiuk,Ihor Pysmennyi*

Main category: cs.CL

TL;DR: 本文首次尝试将大语言模型（LLMs）适应于乌克兰方言（Hutsul），通过构建平行语料库和词典，并利用RAG生成合成数据，最终微调模型在翻译任务中表现优于GPT-4o。


<details>
  <summary>Details</summary>
Motivation: 解决乌克兰Hutsul方言这一低资源且形态复杂的语言在LLMs中的应用问题。

Method: 构建平行语料库和词典，提出RAG生成合成数据，微调开源LLMs（使用LoRA），并采用多指标评估策略。

Result: 微调的小型模型（7B）在自动和LLM评估指标上均优于零样本基线（如GPT-4o）。

Conclusion: 通过数据增强和模型微调，成功提升了LLMs在低资源方言任务中的表现，相关资源已开源。

Abstract: In this paper we introduce the first effort to adapt large language models
(LLMs) to the Ukrainian dialect (in our case Hutsul), a low-resource and
morphologically complex dialect spoken in the Carpathian Highlands. We created
a parallel corpus of 9852 dialect-to-standard Ukrainian sentence pairs and a
dictionary of 7320 dialectal word mappings. We also addressed data shortage by
proposing an advanced Retrieval-Augmented Generation (RAG) pipeline to generate
synthetic parallel translation pairs, expanding the corpus with 52142 examples.
We have fine-tuned multiple open-source LLMs using LoRA and evaluated them on a
standard-to-dialect translation task, also comparing with few-shot GPT-4o
translation. In the absence of human annotators, we adopt a multi-metric
evaluation strategy combining BLEU, chrF++, TER, and LLM-based judgment
(GPT-4o). The results show that even small(7B) finetuned models outperform
zero-shot baselines such as GPT-4o across both automatic and LLM-evaluated
metrics. All data, models, and code are publicly released at:
https://github.com/woters/vuyko-hutsul

</details>


### [188] [LoRMA: Low-Rank Multiplicative Adaptation for LLMs](https://arxiv.org/abs/2506.07621)
*Harsh Bihany,Shubham Patel,Ashutosh Modi*

Main category: cs.CL

TL;DR: 论文提出了一种名为LoRMA的新方法，通过矩阵乘法变换替代传统的加法更新，以提升大型语言模型的适应效率。


<details>
  <summary>Details</summary>
Motivation: 尽管LoRA等低秩适应方法在提升模型效率方面表现良好，但其加法更新的方式限制了性能的进一步提升。

Method: 提出LoRMA方法，通过矩阵乘法变换替代加法更新，并引入操作重排序和秩膨胀策略以解决计算复杂性和秩瓶颈问题。

Result: 实验结果表明，LoRMA在多种评估指标上均表现出色。

Conclusion: LoRMA通过乘法变换为大型语言模型的适应提供了更高效的新范式。

Abstract: Large Language Models have shown remarkable capabilities in the NLP domain.
Their effectiveness can mainly be attributed to their ability to adapt to an
array of downstream tasks. However, generally, full fine-tuning is a
computationally expensive job. To mitigate this, many techniques have been
developed that prime efficiency, a prominent one being Low-Rank Adaptation
(LoRA). However, LoRA and its variants employ re-parametrized additive updates.
In this paper, we propose Low-Rank Multiplicative Adaptation (LoRMA), which
shifts the paradigm of additive updates to a richer space of matrix
multiplicative transformations. We tackle challenges such as computational
complexity and rank bottleneck of matrix multiplication by effectively
re-ordering operations and introducing rank inflation strategies. We conduct
extensive experiments to demonstrate the effectiveness of our approach in terms
of various evaluation metrics.

</details>


### [189] [Intent Matters: Enhancing AI Tutoring with Fine-Grained Pedagogical Intent Annotation](https://arxiv.org/abs/2506.07626)
*Kseniia Petukhova,Ekaterina Kochmar*

Main category: cs.CL

TL;DR: 研究探讨了细粒度标注教师意图是否能提升LLM在教育应用中的响应质量，结果表明更详细的意图分类能生成更符合教学策略的响应。


<details>
  <summary>Details</summary>
Motivation: 当前LLM在教育应用中缺乏与教学策略的对齐，研究旨在通过细粒度标注提升其教学响应质量。

Method: 使用MathDial数据集，通过自动化标注框架重新标注部分数据，细分为11种教学意图，并微调LLM。

Result: 细粒度模型生成的响应在教学对齐和效果上优于原始四分类模型。

Conclusion: 细粒度意图标注对教育场景中的文本生成有显著价值，研究公开了标注数据和代码以促进进一步研究。

Abstract: Large language models (LLMs) hold great promise for educational applications,
particularly in intelligent tutoring systems. However, effective tutoring
requires alignment with pedagogical strategies - something current LLMs lack
without task-specific adaptation. In this work, we explore whether fine-grained
annotation of teacher intents can improve the quality of LLM-generated tutoring
responses. We focus on MathDial, a dialog dataset for math instruction, and
apply an automated annotation framework to re-annotate a portion of the dataset
using a detailed taxonomy of eleven pedagogical intents. We then fine-tune an
LLM using these new annotations and compare its performance to models trained
on the original four-category taxonomy. Both automatic and qualitative
evaluations show that the fine-grained model produces more pedagogically
aligned and effective responses. Our findings highlight the value of intent
specificity for controlled text generation in educational settings, and we
release our annotated data and code to facilitate further research.

</details>


### [190] [Unblocking Fine-Grained Evaluation of Detailed Captions: An Explaining AutoRater and Critic-and-Revise Pipeline](https://arxiv.org/abs/2506.07631)
*Brian Gordon,Yonatan Bitton,Andreea Marzoca,Yasumasa Onoe,Xiao Wang,Daniel Cohen-Or,Idan Szpektor*

Main category: cs.CL

TL;DR: 论文提出了DOCCI-Critique基准和VNLI-Critique模型，用于评估和改进视觉语言模型（VLM）生成段落的准确性。


<details>
  <summary>Details</summary>
Motivation: 当前方法难以评估VLM生成段落的细粒度错误，缺乏验证数据集。

Method: 开发了包含1,400条VLM生成段落的基准，并基于此构建了VNLI-Critique模型，用于自动化事实分类和错误分析。

Result: VNLI-Critique在多个基准测试中表现优异，AutoRater与人类判断高度一致，Critic-and-Revise流程显著提升了事实准确性。

Conclusion: 该研究为细粒度评估提供了重要基准和工具，有助于提升VLM的图像理解能力。

Abstract: Large Vision-Language Models (VLMs) now generate highly detailed,
paragraphlength image captions, yet evaluating their factual accuracy remains
challenging. Current methods often miss fine-grained errors, being designed for
shorter texts or lacking datasets with verified inaccuracies. We introduce
DOCCI-Critique, a benchmark with 1,400 VLM-generated paragraph captions (100
images, 14 VLMs) featuring over 10,216 sentence-level human annotations of
factual correctness and explanatory rationales for errors, all within paragraph
context. Building on this, we develop VNLI-Critique, a model for automated
sentence-level factuality classification and critique generation. We highlight
three key applications: (1) VNLI-Critique demonstrates robust generalization,
validated by state-of-the-art performance on the M-HalDetect benchmark and
strong results in CHOCOLATE claim verification. (2) The VNLI-Critique driven
AutoRater for DOCCI-Critique provides reliable VLM rankings, showing excellent
alignment with human factuality judgments (e.g., 0.98 Spearman). (3) An
innovative Critic-and-Revise pipeline, where critiques from VNLI-Critique guide
LLM-based corrections, achieves substantial improvements in caption factuality
(e.g., a 46% gain on DetailCaps-4870). Our work offers a crucial benchmark
alongside practical tools, designed to significantly elevate the standards for
fine-grained evaluation and foster the improvement of VLM image understanding.
Project page: https://google.github.io/unblocking-detail-caption

</details>


### [191] [TreeReview: A Dynamic Tree of Questions Framework for Deep and Efficient LLM-based Scientific Peer Review](https://arxiv.org/abs/2506.07642)
*Yuan Chang,Ziyue Li,Hengyuan Zhang,Yuanbo Kong,Yanru Wu,Zhijiang Guo,Ngai Wong*

Main category: cs.CL

TL;DR: TreeReview是一个新颖的框架，通过分层和双向问答过程生成更全面和深入的论文评审，同时显著减少计算资源消耗。


<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型（LLMs）在辅助同行评审时，难以兼顾评审的深度和效率。

Method: TreeReview将评审建模为分层问答过程，递归分解问题并动态扩展问题以深入探究，最后从叶到根聚合答案生成评审。

Result: 实验表明，TreeReview在生成全面、深入的评审反馈方面优于基线方法，同时减少80%的LLM令牌使用。

Conclusion: TreeReview为高效且高质量的论文评审提供了一种可行方案，代码和数据集已开源。

Abstract: While Large Language Models (LLMs) have shown significant potential in
assisting peer review, current methods often struggle to generate thorough and
insightful reviews while maintaining efficiency. In this paper, we propose
TreeReview, a novel framework that models paper review as a hierarchical and
bidirectional question-answering process. TreeReview first constructs a tree of
review questions by recursively decomposing high-level questions into
fine-grained sub-questions and then resolves the question tree by iteratively
aggregating answers from leaf to root to get the final review. Crucially, we
incorporate a dynamic question expansion mechanism to enable deeper probing by
generating follow-up questions when needed. We construct a benchmark derived
from ICLR and NeurIPS venues to evaluate our method on full review generation
and actionable feedback comments generation tasks. Experimental results of both
LLM-based and human evaluation show that TreeReview outperforms strong
baselines in providing comprehensive, in-depth, and expert-aligned review
feedback, while reducing LLM token usage by up to 80% compared to
computationally intensive approaches. Our code and benchmark dataset are
available at https://github.com/YuanChang98/tree-review.

</details>


### [192] [Evaluating LLMs Robustness in Less Resourced Languages with Proxy Models](https://arxiv.org/abs/2506.07645)
*Maciej Chrabąszcz,Katarzyna Lorenc,Karolina Seweryn*

Main category: cs.CL

TL;DR: 研究发现，多语言大语言模型（LLMs）在低资源语言（如波兰语）中易受字符和词级攻击，导致预测结果被篡改，暴露出安全漏洞。


<details>
  <summary>Details</summary>
Motivation: 尽管LLMs在多语言任务中表现优异，但其安全训练数据主要集中于高资源语言（如英语），导致在低资源语言中易受攻击。

Method: 通过少量字符修改和小型代理模型计算词重要性，构建低成本攻击方法，并在波兰语中验证其有效性。

Result: 攻击显著改变了LLMs的预测结果，揭示了其内部安全机制的潜在漏洞。

Conclusion: 研究强调了LLMs在低资源语言中的安全风险，并提供了攻击方法和数据集以供进一步研究。

Abstract: Large language models (LLMs) have demonstrated impressive capabilities across
various natural language processing (NLP) tasks in recent years. However, their
susceptibility to jailbreaks and perturbations necessitates additional
evaluations. Many LLMs are multilingual, but safety-related training data
contains mainly high-resource languages like English. This can leave them
vulnerable to perturbations in low-resource languages such as Polish. We show
how surprisingly strong attacks can be cheaply created by altering just a few
characters and using a small proxy model for word importance calculation. We
find that these character and word-level attacks drastically alter the
predictions of different LLMs, suggesting a potential vulnerability that can be
used to circumvent their internal safety mechanisms. We validate our attack
construction methodology on Polish, a low-resource language, and find potential
vulnerabilities of LLMs in this language. Additionally, we show how it can be
extended to other languages. We release the created datasets and code for
further research.

</details>


### [193] [Synthesis by Design: Controlled Data Generation via Structural Guidance](https://arxiv.org/abs/2506.07664)
*Lei Xu,Sirui Chen,Yuxuan Huang,Chaochao Lu*

Main category: cs.CL

TL;DR: 论文提出一种通过生成问题解决代码提取结构信息的方法，以提升LLM在数学推理中的表现，并生成了高质量的数据集和基准测试。


<details>
  <summary>Details</summary>
Motivation: 现有方法在生成数据集时存在质量和复杂度问题，数学推理对LLM仍具挑战性。

Method: 提取结构信息并生成问题解决代码，以结构化解决方案指导数据生成。

Result: 生成了39K问题和6.1K高难度基准测试，模型性能随推理长度增加而下降，微调实验验证了数据集有效性。

Conclusion: 该方法及数据集有望推动LLM推理能力的未来研究。

Abstract: Mathematical reasoning remains challenging for LLMs due to complex logic and
the need for precise computation. Existing methods enhance LLM reasoning by
synthesizing datasets through problem rephrasing, but face issues with
generation quality and problem complexity. To address this, we propose to
extract structural information with generated problem-solving code from
mathematical reasoning and guide data generation with structured solutions.
Applied to MATH and GSM8K, our approach produces 39K problems with labeled
intermediate steps and a 6.1K-problem benchmark of higher difficulty. Results
on our benchmark show that model performance declines as reasoning length
increases. Additionally, we conducted fine-tuning experiments using the
proposed training data on a range of LLMs, and the results validate the
effectiveness of our dataset. We hope the proposed method and dataset will
contribute to future research in enhancing LLM reasoning capabilities.

</details>


### [194] [GaRAGe: A Benchmark with Grounding Annotations for RAG Evaluation](https://arxiv.org/abs/2506.07671)
*Ionut-Teodor Sorodoc,Leonardo F. R. Ribeiro,Rexhina Blloshmi,Christopher Davis,Adrià de Gispert*

Main category: cs.CL

TL;DR: GaRAGe是一个大型RAG基准测试，包含人工标注的长答案和基础段落，用于评估LLM在生成RAG答案时是否能识别相关基础信息。


<details>
  <summary>Details</summary>
Motivation: 当前LLM在生成RAG答案时存在过度总结或未能准确引用相关基础信息的问题，需要一个细粒度的评估工具。

Method: 构建了包含2366个多样化问题的基准测试，标注了超过35K的基础段落，涵盖私有文档和网页内容。

Result: 评估显示，LLM在引用相关段落时表现不佳（最高60%的相关性评分），且在无相关信息时很少选择回避（最高31%的回避率）。

Conclusion: GaRAGe为评估LLM在RAG任务中的表现提供了有效工具，揭示了当前模型在引用准确性和回避能力上的不足。

Abstract: We present GaRAGe, a large RAG benchmark with human-curated long-form answers
and annotations of each grounding passage, allowing a fine-grained evaluation
of whether LLMs can identify relevant grounding when generating RAG answers.
Our benchmark contains 2366 questions of diverse complexity, dynamism, and
topics, and includes over 35K annotated passages retrieved from both private
document sets and the Web, to reflect real-world RAG use cases. This makes it
an ideal test bed to evaluate an LLM's ability to identify only the relevant
information necessary to compose a response, or provide a deflective response
when there is insufficient information. Evaluations of multiple
state-of-the-art LLMs on GaRAGe show that the models tend to over-summarise
rather than (a) ground their answers strictly on the annotated relevant
passages (reaching at most a Relevance-Aware Factuality Score of 60%), or (b)
deflect when no relevant grounding is available (reaching at most 31% true
positive rate in deflections). The F1 in attribution to relevant sources is at
most 58.9%, and we show that performance is particularly reduced when answering
time-sensitive questions and when having to draw knowledge from sparser private
grounding sources.

</details>


### [195] [Training Superior Sparse Autoencoders for Instruct Models](https://arxiv.org/abs/2506.07691)
*Jiaming Li,Haoran Ye,Yukun Chen,Xinyue Li,Lei Zhang,Hamid Alinejad-Rokny,Jimmy Chih-Hsien Peng,Min Yang*

Main category: cs.CL

TL;DR: 论文提出了一种名为FAST的新训练方法，专门针对指令模型优化稀疏自编码器（SAEs），显著提升了重构质量和特征可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有SAE训练方法主要针对基础模型，应用于指令模型时效果不佳，需要改进。

Method: 提出FAST方法，通过调整训练过程以匹配指令模型的数据分布和激活模式。

Result: 在Qwen2.5-7B-Instruct上，FAST的重构误差显著低于基线方法；在Llama3.2-3B-Instruct上，高质量特征比例更高。

Conclusion: FAST为指令模型提供了更优的SAE训练方法，并揭示了通过特殊令牌干预改进模型行为的新机会。

Abstract: As large language models (LLMs) grow in scale and capability, understanding
their internal mechanisms becomes increasingly critical. Sparse autoencoders
(SAEs) have emerged as a key tool in mechanistic interpretability, enabling the
extraction of human-interpretable features from LLMs. However, existing SAE
training methods are primarily designed for base models, resulting in reduced
reconstruction quality and interpretability when applied to instruct models. To
bridge this gap, we propose
$\underline{\textbf{F}}$inetuning-$\underline{\textbf{a}}$ligned
$\underline{\textbf{S}}$equential $\underline{\textbf{T}}$raining
($\textit{FAST}$), a novel training method specifically tailored for instruct
models. $\textit{FAST}$ aligns the training process with the data distribution
and activation patterns characteristic of instruct models, resulting in
substantial improvements in both reconstruction and feature interpretability.
On Qwen2.5-7B-Instruct, $\textit{FAST}$ achieves a mean squared error of 0.6468
in token reconstruction, significantly outperforming baseline methods with
errors of 5.1985 and 1.5096. In feature interpretability, $\textit{FAST}$
yields a higher proportion of high-quality features, for Llama3.2-3B-Instruct,
$21.1\%$ scored in the top range, compared to $7.0\%$ and $10.2\%$ for
$\textit{BT(P)}$ and $\textit{BT(F)}$. Surprisingly, we discover that
intervening on the activations of special tokens via the SAEs leads to
improvements in output quality, suggesting new opportunities for fine-grained
control of model behavior. Code, data, and 240 trained SAEs are available at
https://github.com/Geaming2002/FAST.

</details>


### [196] [Through the Valley: Path to Effective Long CoT Training for Small Language Models](https://arxiv.org/abs/2506.07712)
*Renjie Luo,Jiaxi Li,Chen Huang,Wei Lu*

Main category: cs.CL

TL;DR: 研究发现，小语言模型（SLMs）在长链思维（CoT）监督训练中会出现性能退化现象，称为Long CoT Degradation，主要由于错误累积导致。


<details>
  <summary>Details</summary>
Motivation: 探讨小语言模型在长链思维监督训练中的性能退化现象及其原因。

Method: 通过Qwen2.5、LLaMA3和Gemma3系列模型进行实验，分析长链思维训练对性能的影响。

Result: 小模型在长链思维训练中性能显著下降，某些情况下甚至无法恢复原始性能。

Conclusion: 研究挑战了长链思维训练对小模型的普遍益处假设，并提供了改进小规模推理模型的实用建议。

Abstract: Long chain-of-thought (CoT) supervision has become a common strategy to
enhance reasoning in language models. While effective for large models, we
identify a phenomenon we call Long CoT Degradation, in which small language
models (SLMs; <=3B parameters) trained on limited long CoT data experience
significant performance deterioration. Through extensive experiments on the
Qwen2.5, LLaMA3 and Gemma3 families, we demonstrate that this degradation is
widespread across SLMs. In some settings, models trained on only 8k long CoT
examples lose up to 75% of their original performance before fine-tuning.
Strikingly, we further observe that for some particularly small models, even
training on 220k long CoT examples fails to recover or surpass their original
performance prior to fine-tuning. Our analysis attributes this effect to error
accumulation: while longer responses increase the capacity for multi-step
reasoning, they also amplify the risk of compounding mistakes. Furthermore, we
find that Long CoT Degradation may negatively impacts downstream reinforcement
learning (RL), although this can be alleviated by sufficiently scaled
supervised fine-tuning (SFT). Our findings challenge common assumptions about
the benefits of long CoT training for SLMs and offer practical guidance for
building more effective small-scale reasoning models.

</details>


### [197] [Multilingual Grammatical Error Annotation: Combining Language-Agnostic Framework with Language-Specific Flexibility](https://arxiv.org/abs/2506.07719)
*Mengyang Qiu,Tran Minh Nguyen,Zihao Huang,Zelong Li,Yang Gu,Qingyu Gao,Siliang Liu,Jungyeul Park*

Main category: cs.CL

TL;DR: 本文提出了一种标准化、模块化的多语言语法错误标注框架，结合语言无关基础和语言特定扩展，支持多种语言的语法错误修正（GEC）。


<details>
  <summary>Details</summary>
Motivation: 现有框架（如errant）在扩展到类型多样的语言时存在局限性，需要一种更灵活且一致的多语言语法错误标注方法。

Method: 采用语言无关基础与语言特定扩展相结合的方法，使用stanza重新实现errant以支持多语言覆盖。

Result: 框架成功应用于英语、德语、捷克语、韩语和中文，支持从通用标注到定制化语言细化。

Conclusion: 该框架促进了多语言环境下可扩展且可解释的GEC标注，并提供了更一致的评估方法。

Abstract: Grammatical Error Correction (GEC) relies on accurate error annotation and
evaluation, yet existing frameworks, such as $\texttt{errant}$, face
limitations when extended to typologically diverse languages. In this paper, we
introduce a standardized, modular framework for multilingual grammatical error
annotation. Our approach combines a language-agnostic foundation with
structured language-specific extensions, enabling both consistency and
flexibility across languages. We reimplement $\texttt{errant}$ using
$\texttt{stanza}$ to support broader multilingual coverage, and demonstrate the
framework's adaptability through applications to English, German, Czech,
Korean, and Chinese, ranging from general-purpose annotation to more customized
linguistic refinements. This work supports scalable and interpretable GEC
annotation across languages and promotes more consistent evaluation in
multilingual settings. The complete codebase and annotation tools can be
accessed at https://github.com/open-writing-evaluation/jp_errant_bea.

</details>


### [198] [Swiss Parliaments Corpus Re-Imagined (SPC_R): Enhanced Transcription with RAG-based Correction and Predicted BLEU](https://arxiv.org/abs/2506.07726)
*Vincenzo Timmel,Manfred Vogel,Daniel Perruchoud,Reza Kakooee*

Main category: cs.CL

TL;DR: 论文介绍了瑞士议会语料库的长篇版本，通过Whisper和GPT-4o的联合处理，将瑞士德语辩论转换为高质量的语音-文本对，并显著提升了BLEU分数。


<details>
  <summary>Details</summary>
Motivation: 为低资源、领域特定的语音语料库提供高质量的长篇语音-文本对齐数据。

Method: 使用Whisper Large-v3转录音频为德语，再通过GPT-4o两步修正（命名实体修正和语义完整性评估），最后基于BLEU分数和GPT-4o评分过滤数据。

Result: 生成了801小时音频数据，其中751小时通过质量控制，BLEU分数提升6点。

Conclusion: 结合ASR、LLM修正和数据驱动过滤，能显著提升低资源领域语料库的质量。

Abstract: This paper presents a new long-form release of the Swiss Parliaments Corpus,
converting entire multi-hour Swiss German debate sessions (each aligned with
the official session protocols) into high-quality speech-text pairs. Our
pipeline starts by transcribing all session audio into Standard German using
Whisper Large-v3 under high-compute settings. We then apply a two-step GPT-4o
correction process: first, GPT-4o ingests the raw Whisper output alongside the
official protocols to refine misrecognitions, mainly named entities. Second, a
separate GPT-4o pass evaluates each refined segment for semantic completeness.
We filter out any segments whose Predicted BLEU score (derived from Whisper's
average token log-probability) and GPT-4o evaluation score fall below a certain
threshold. The final corpus contains 801 hours of audio, of which 751 hours
pass our quality control. Compared to the original sentence-level SPC release,
our long-form dataset achieves a 6-point BLEU improvement, demonstrating the
power of combining robust ASR, LLM-based correction, and data-driven filtering
for low-resource, domain-specific speech corpora.

</details>


### [199] [Augmenting LLMs' Reasoning by Reinforcing Abstract Thinking](https://arxiv.org/abs/2506.07751)
*Silin Gao,Antoine Bosselut,Samy Bengio,Emmanuel Abbe*

Main category: cs.CL

TL;DR: 论文提出了一种通过强化学习（RL）促进抽象推理的方法AbstraL，以应对大语言模型（LLMs）在分布变化时的性能下降问题。


<details>
  <summary>Details</summary>
Motivation: 小规模LLMs在面对分布变化（如数值或名义变量变化、干扰性从句插入）时表现不稳定，现有方法通过生成合成数据来应对，但效果有限。

Method: 采用强化学习（RL）训练模型进行抽象推理，而非传统的监督微调，以生成更可靠的抽象表示。

Result: AbstraL方法在GSM扰动基准测试中显著减少了性能下降。

Conclusion: 通过抽象推理和强化学习的结合，可以有效提升LLMs在分布变化下的鲁棒性。

Abstract: Recent studies have shown that large language models (LLMs), especially
smaller ones, often lack robustness in their reasoning. I.e., they tend to
experience performance drops when faced with distribution shifts, such as
changes to numerical or nominal variables, or insertions of distracting
clauses. A possible strategy to address this involves generating synthetic data
to further "instantiate" reasoning problems on potential variations. In
contrast, our approach focuses on "abstracting" reasoning problems. This not
only helps counteract distribution shifts but also facilitates the connection
to symbolic tools for deriving solutions. We find that this abstraction process
is better acquired through reinforcement learning (RL) than just supervised
fine-tuning, which often fails to produce faithful abstractions. Our method,
AbstraL -- which promotes abstract reasoning in LLMs using RL on granular
abstraction data -- significantly mitigates performance degradation on recent
GSM perturbation benchmarks.

</details>


### [200] [LLM Unlearning Should Be Form-Independent](https://arxiv.org/abs/2506.07795)
*Xiaotian Ye,Mengqi Zhang,Shu Wu*

Main category: cs.CL

TL;DR: 研究发现LLM遗忘方法存在形式依赖偏差问题，提出新基准ORT和新方法ROCR以提高遗忘效果。


<details>
  <summary>Details</summary>
Motivation: 解决LLM遗忘方法在现实场景中效果不佳的问题，尤其是形式依赖偏差对下游任务的影响。

Method: 引入ORT基准评估遗忘方法，并提出无训练的ROCR方法，通过重定向危险概念实现遗忘。

Result: ROCR显著优于传统方法，能快速修改模型参数并生成自然输出。

Conclusion: LLM遗忘应形式无关，ROCR为解决形式依赖偏差提供了有效路径。

Abstract: Large Language Model (LLM) unlearning aims to erase or suppress undesirable
knowledge within the model, offering promise for controlling harmful or private
information to prevent misuse. However, recent studies highlight its limited
efficacy in real-world scenarios, hindering practical adoption. In this study,
we identify a pervasive issue underlying many downstream failures: the
effectiveness of existing unlearning methods heavily depends on the form of
training samples and frequently fails to generalize to alternate expressions of
the same knowledge. We formally characterize this problem as Form-Dependent
Bias and systematically investigate its specific manifestation patterns across
various downstream tasks. To quantify its prevalence and support future
research, we introduce ORT, a novel benchmark designed to evaluate the
robustness of unlearning methods against variations in knowledge expression.
Results reveal that Form-Dependent Bias is both widespread and severe among
current techniques.
  We argue that LLM unlearning should be form-independent to address the
endless forms of downstream tasks encountered in real-world security-critical
scenarios. Towards this goal, we introduce Rank-one Concept Redirection (ROCR),
a novel training-free method, as a promising solution path. ROCR performs
unlearning by targeting the invariants in downstream tasks, specifically the
activated dangerous concepts. It is capable of modifying model parameters
within seconds to redirect the model's perception of a specific unlearning
target concept to another harmless concept. Extensive experiments demonstrate
that ROCR significantly improves unlearning effectiveness compared to
traditional methods while generating highly natural outputs.

</details>


### [201] [MultiMatch: Multihead Consistency Regularization Matching for Semi-Supervised Text Classification](https://arxiv.org/abs/2506.07801)
*Iustin Sirbu,Robert-Adrian Popovici,Cornelia Caragea,Stefan Trausan-Matu,Traian Rebedea*

Main category: cs.CL

TL;DR: MultiMatch是一种结合协同训练和一致性正则化的半监督学习算法，通过三重重伪标签加权模块提升性能。


<details>
  <summary>Details</summary>
Motivation: 解决半监督学习中伪标签选择和加权的挑战，提升模型在数据不平衡场景下的鲁棒性。

Method: 结合协同训练和一致性正则化，设计三重重伪标签加权模块，整合多头部一致性、自适应阈值和平均伪边际。

Result: 在5个NLP数据集的9/10设置中达到SOTA，Friedman测试排名第一，数据不平衡场景下表现优异。

Conclusion: MultiMatch通过创新加权模块显著提升半监督学习性能，尤其在数据不平衡任务中表现突出。

Abstract: We introduce MultiMatch, a novel semi-supervised learning (SSL) algorithm
combining the paradigms of co-training and consistency regularization with
pseudo-labeling. At its core, MultiMatch features a three-fold pseudo-label
weighting module designed for three key purposes: selecting and filtering
pseudo-labels based on head agreement and model confidence, and weighting them
according to the perceived classification difficulty. This novel module
enhances and unifies three existing techniques -- heads agreement from
Multihead Co-training, self-adaptive thresholds from FreeMatch, and Average
Pseudo-Margins from MarginMatch -- resulting in a holistic approach that
improves robustness and performance in SSL settings. Experimental results on
benchmark datasets highlight the superior performance of MultiMatch, achieving
state-of-the-art results on 9 out of 10 setups from 5 natural language
processing datasets and ranking first according to the Friedman test among 19
methods. Furthermore, MultiMatch demonstrates exceptional robustness in highly
imbalanced settings, outperforming the second-best approach by 3.26% -- and
data imbalance is a key factor for many text classification tasks.

</details>


### [202] [WebUIBench: A Comprehensive Benchmark for Evaluating Multimodal Large Language Models in WebUI-to-Code](https://arxiv.org/abs/2506.07818)
*Zhiyu Lin,Zhengda Zhou,Zhiyuan Zhao,Tianrui Wan,Yilun Ma,Junyu Gao,Xuelong Li*

Main category: cs.CL

TL;DR: 论文提出了WebUIBench，一个系统性评估多模态大语言模型（MLLMs）在四个关键领域的基准，填补了现有评估仅关注网页生成结果的不足。


<details>
  <summary>Details</summary>
Motivation: 随着生成式AI技术的快速发展，MLLMs有望成为AI软件工程师，但现有评估框架缺乏对多维子能力的评估，无法全面指导开发效率提升。

Method: 基于软件工程原则，设计了WebUIBench基准，包含21K高质量问答对，覆盖WebUI感知、HTML编程、WebUI-HTML理解和WebUI-to-Code四个领域。

Result: 对29个主流MLLMs的评估揭示了模型在开发过程中的技能特点和多种弱点。

Conclusion: WebUIBench为MLLMs的开发能力提供了系统性评估工具，有助于指导模型优化和效率提升。

Abstract: With the rapid advancement of Generative AI technology, Multimodal Large
Language Models(MLLMs) have the potential to act as AI software engineers
capable of executing complex web application development. Considering that the
model requires a confluence of multidimensional sub-capabilities to address the
challenges of various development phases, constructing a multi-view evaluation
framework is crucial for accurately guiding the enhancement of development
efficiency. However, existing benchmarks usually fail to provide an assessment
of sub-capabilities and focus solely on webpage generation outcomes. In this
work, we draw inspiration from the principles of software engineering and
further propose WebUIBench, a benchmark systematically designed to evaluate
MLLMs in four key areas: WebUI Perception, HTML Programming,WebUI-HTML
Understanding, and WebUI-to-Code. WebUIBench comprises 21K high-quality
question-answer pairs derived from over 0.7K real-world websites. The extensive
evaluation of 29 mainstream MLLMs uncovers the skill characteristics and
various weakness that models encountered during the development process.

</details>


### [203] [Learning to Focus: Causal Attention Distillation via Gradient-Guided Token Pruning](https://arxiv.org/abs/2506.07851)
*Yiju Guo,Wenkai Yang,Zexu Sun,Ning Ding,Zhiyuan Liu,Yankai Lin*

Main category: cs.CL

TL;DR: 论文提出了一种名为LeaF的两阶段框架，通过干预式推理解决大语言模型在长上下文推理中的注意力分散问题，显著提升了推理准确性和生成质量。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在长上下文推理中容易因训练数据中的虚假相关性而分心，导致推理冗余和错误响应。

Method: LeaF框架分为两阶段：1）通过梯度比较识别混淆性标记；2）在蒸馏过程中修剪这些标记以对齐注意力分布。

Result: 实验表明，LeaF在数学推理和代码生成任务中显著提升性能，并有效抑制了对混淆性标记的注意力。

Conclusion: LeaF通过干预式推理解决了注意力分散问题，提升了模型的可靠性和可解释性。

Abstract: Large language models (LLMs) have demonstrated significant improvements in
contextual understanding. However, their ability to attend to truly critical
information during long-context reasoning and generation still falls behind the
pace. Specifically, our preliminary experiments reveal that certain distracting
patterns can misdirect the model's attention during inference, and removing
these patterns substantially improves reasoning accuracy and generation
quality. We attribute this phenomenon to spurious correlations in the training
data, which obstruct the model's capacity to infer authentic causal
instruction-response relationships. This phenomenon may induce redundant
reasoning processes, potentially resulting in significant inference overhead
and, more critically, the generation of erroneous or suboptimal responses. To
mitigate this, we introduce a two-stage framework called Learning to Focus
(LeaF) leveraging intervention-based inference to disentangle confounding
factors. In the first stage, LeaF employs gradient-based comparisons with an
advanced teacher to automatically identify confounding tokens based on causal
relationships in the training corpus. Then, in the second stage, it prunes
these tokens during distillation to enact intervention, aligning the student's
attention with the teacher's focus distribution on truly critical context
tokens. Experimental results demonstrate that LeaF not only achieves an
absolute improvement in various mathematical reasoning and code generation
benchmarks but also effectively suppresses attention to confounding tokens
during inference, yielding a more interpretable and reliable reasoning model.

</details>


### [204] [MEMOIR: Lifelong Model Editing with Minimal Overwrite and Informed Retention for LLMs](https://arxiv.org/abs/2506.07899)
*Ke Wang,Yiming Qin,Nikolaos Dimitriadis,Alessandro Favero,Pascal Frossard*

Main category: cs.CL

TL;DR: MEMOIR是一种新型可扩展框架，通过残差记忆模块注入知识，避免干扰先前编辑，并支持大规模连续编辑。


<details>
  <summary>Details</summary>
Motivation: 解决语言模型在实时更新知识时面临的效率、可靠性和遗忘问题。

Method: 利用残差记忆模块和稀疏激活掩码，将编辑限制在特定记忆参数子集，减少编辑间干扰。

Result: 在问答、幻觉纠正和分布外泛化任务中表现优异，支持数千次连续编辑且遗忘最少。

Conclusion: MEMOIR在可靠性、泛化性和局部性指标上达到最先进水平，适用于大规模知识更新。

Abstract: Language models deployed in real-world systems often require post-hoc updates
to incorporate new or corrected knowledge. However, editing such models
efficiently and reliably - without retraining or forgetting previous
information - remains a major challenge. Existing methods for lifelong model
editing either compromise generalization, interfere with past edits, or fail to
scale to long editing sequences. We propose MEMOIR, a novel scalable framework
that injects knowledge through a residual memory, i.e., a dedicated parameter
module, while preserving the core capabilities of the pre-trained model. By
sparsifying input activations through sample-dependent masks, MEMOIR confines
each edit to a distinct subset of the memory parameters, minimizing
interference among edits. At inference, it identifies relevant edits by
comparing the sparse activation patterns of new queries to those stored during
editing. This enables generalization to rephrased queries by activating only
the relevant knowledge while suppressing unnecessary memory activation for
unrelated prompts. Experiments on question answering, hallucination correction,
and out-of-distribution generalization benchmarks across LLaMA-3 and Mistral
demonstrate that MEMOIR achieves state-of-the-art performance across
reliability, generalization, and locality metrics, scaling to thousands of
sequential edits with minimal forgetting.

</details>


### [205] [MiniCPM4: Ultra-Efficient LLMs on End Devices](https://arxiv.org/abs/2506.07900)
*MiniCPM Team,Chaojun Xiao,Yuxuan Li,Xu Han,Yuzhuo Bai,Jie Cai,Haotian Chen,Wentong Chen,Xin Cong,Ganqu Cui,Ning Ding,Shengdan Fan,Yewei Fang,Zixuan Fu,Wenyu Guan,Yitong Guan,Junshao Guo,Yufeng Han,Bingxiang He,Yuxiang Huang,Cunliang Kong,Qiuzuo Li,Siyuan Li,Wenhao Li,Yanghao Li,Yishan Li,Zhen Li,Dan Liu,Biyuan Lin,Yankai Lin,Xiang Long,Quanyu Lu,Yaxi Lu,Peiyan Luo,Hongya Lyu,Litu Ou,Yinxu Pan,Zekai Qu,Qundong Shi,Zijun Song,Jiayuan Su,Zhou Su,Ao Sun,Xianghui Sun,Peijun Tang,Fangzheng Wang,Feng Wang,Shuo Wang,Yudong Wang,Yesai Wu,Zhenyu Xiao,Jie Xie,Zihao Xie,Yukun Yan,Jiarui Yuan,Kaihuo Zhang,Lei Zhang,Linyue Zhang,Xueren Zhang,Yudi Zhang,Hengyu Zhao,Weilin Zhao,Weilun Zhao,Yuanqian Zhao,Zhi Zheng,Ge Zhou,Jie Zhou,Wei Zhou,Zihan Zhou,Zixuan Zhou,Zhiyuan Liu,Guoyang Zeng,Chao Jia,Dahai Li,Maosong Sun*

Main category: cs.CL

TL;DR: MiniCPM4是一种高效的大型语言模型，专为终端设备设计，通过模型架构、训练数据、训练算法和推理系统的创新，实现了高性能和小规模参数。


<details>
  <summary>Details</summary>
Motivation: 为终端设备开发高效的大型语言模型，满足多样化的设备需求。

Method: 提出InfLLM v2稀疏注意力机制、UltraClean和UltraChat v2数据集、ModelTunnel v2训练算法、CPM.cu推理系统。

Result: MiniCPM4在多个基准测试中优于同类开源模型，处理长序列时速度显著提升。

Conclusion: MiniCPM4展示了高效性和广泛适用性，成功应用于多种场景。

Abstract: This paper introduces MiniCPM4, a highly efficient large language model (LLM)
designed explicitly for end-side devices. We achieve this efficiency through
systematic innovation in four key dimensions: model architecture, training
data, training algorithms, and inference systems. Specifically, in terms of
model architecture, we propose InfLLM v2, a trainable sparse attention
mechanism that accelerates both prefilling and decoding phases for long-context
processing. Regarding training data, we propose UltraClean, an efficient and
accurate pre-training data filtering and generation strategy, and UltraChat v2,
a comprehensive supervised fine-tuning dataset. These datasets enable
satisfactory model performance to be achieved using just 8 trillion training
tokens. Regarding training algorithms, we propose ModelTunnel v2 for efficient
pre-training strategy search, and improve existing post-training methods by
introducing chunk-wise rollout for load-balanced reinforcement learning and
data-efficient tenary LLM, BitCPM. Regarding inference systems, we propose
CPM.cu that integrates sparse attention, model quantization, and speculative
sampling to achieve efficient prefilling and decoding. To meet diverse
on-device requirements, MiniCPM4 is available in two versions, with 0.5B and 8B
parameters, respectively. Sufficient evaluation results show that MiniCPM4
outperforms open-source models of similar size across multiple benchmarks,
highlighting both its efficiency and effectiveness. Notably, MiniCPM4-8B
demonstrates significant speed improvements over Qwen3-8B when processing long
sequences. Through further adaptation, MiniCPM4 successfully powers diverse
applications, including trustworthy survey generation and tool use with model
context protocol, clearly showcasing its broad usability.

</details>


### [206] [Quantum Graph Transformer for NLP Sentiment Classification](https://arxiv.org/abs/2506.07937)
*Shamminuj Aktar,Andreas Bärtschi,Abdel-Hameed A. Badawy,Stephan Eidenbenz*

Main category: cs.CL

TL;DR: 论文提出了一种量子图变换器（QGT），结合量子自注意力机制和消息传递框架，用于结构化语言建模，在情感分类任务中表现优于现有量子自然语言处理模型。


<details>
  <summary>Details</summary>
Motivation: 量子机器学习在复杂结构化数据建模中具有潜力，但现有方法在参数效率和性能上仍有改进空间。

Method: QGT通过参数化量子电路实现量子自注意力机制，减少了可训练参数数量，同时保持了丰富的上下文关系捕捉能力。

Result: QGT在五个情感分类基准测试中表现优于或与现有量子模型相当，相比经典图变换器平均准确率提升5.42%（真实数据集）和4.76%（合成数据集），且样本效率更高。

Conclusion: QGT展示了图基量子自然语言处理技术在高效、可扩展语言理解中的潜力。

Abstract: Quantum machine learning is a promising direction for building more efficient
and expressive models, particularly in domains where understanding complex,
structured data is critical. We present the Quantum Graph Transformer (QGT), a
hybrid graph-based architecture that integrates a quantum self-attention
mechanism into the message-passing framework for structured language modeling.
The attention mechanism is implemented using parameterized quantum circuits
(PQCs), which enable the model to capture rich contextual relationships while
significantly reducing the number of trainable parameters compared to classical
attention mechanisms. We evaluate QGT on five sentiment classification
benchmarks. Experimental results show that QGT consistently achieves higher or
comparable accuracy than existing quantum natural language processing (QNLP)
models, including both attention-based and non-attention-based approaches. When
compared with an equivalent classical graph transformer, QGT yields an average
accuracy improvement of 5.42% on real-world datasets and 4.76% on synthetic
datasets. Additionally, QGT demonstrates improved sample efficiency, requiring
nearly 50% fewer labeled samples to reach comparable performance on the Yelp
dataset. These results highlight the potential of graph-based QNLP techniques
for advancing efficient and scalable language understanding.

</details>


### [207] [Statistical Hypothesis Testing for Auditing Robustness in Language Models](https://arxiv.org/abs/2506.07947)
*Paulius Rauba,Qiyao Wei,Mihaela van der Schaar*

Main category: cs.CL

TL;DR: 提出了一种基于分布的扰动分析框架，用于测试大型语言模型（LLM）输出在干预下的变化，解决了传统方法无法处理的问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法无法有效比较LLM输出的变化，尤其是在随机性和计算复杂性限制下。

Method: 通过蒙特卡洛采样在低维语义相似空间中构建经验分布，进行假设检验。

Result: 框架具有模型无关性、可解释性、支持多扰动评估等优势，并通过案例验证了其有效性。

Conclusion: 该框架为LLM审计提供了一个可靠且灵活的频繁假设检验方法。

Abstract: Consider the problem of testing whether the outputs of a large language model
(LLM) system change under an arbitrary intervention, such as an input
perturbation or changing the model variant. We cannot simply compare two LLM
outputs since they might differ due to the stochastic nature of the system, nor
can we compare the entire output distribution due to computational
intractability. While existing methods for analyzing text-based outputs exist,
they focus on fundamentally different problems, such as measuring bias or
fairness. To this end, we introduce distribution-based perturbation analysis, a
framework that reformulates LLM perturbation analysis as a frequentist
hypothesis testing problem. We construct empirical null and alternative output
distributions within a low-dimensional semantic similarity space via Monte
Carlo sampling, enabling tractable inference without restrictive distributional
assumptions. The framework is (i) model-agnostic, (ii) supports the evaluation
of arbitrary input perturbations on any black-box LLM, (iii) yields
interpretable p-values; (iv) supports multiple perturbations via controlled
error rates; and (v) provides scalar effect sizes. We demonstrate the
usefulness of the framework across multiple case studies, showing how we can
quantify response changes, measure true/false positive rates, and evaluate
alignment with reference models. Above all, we see this as a reliable
frequentist hypothesis testing framework for LLM auditing.

</details>


### [208] [Language Models over Canonical Byte-Pair Encodings](https://arxiv.org/abs/2506.07956)
*Tim Vieira,Tianyu Liu,Clemente Pasti,Yahya Emara,Brian DuSell,Benjamin LeBrun,Mario Giulianelli,Juan Luis Gastaldi,Timothy J. O'Donnell,Ryan Cotterell*

Main category: cs.CL

TL;DR: 论文提出方法解决语言模型中非规范标记编码的问题，确保仅规范标记被赋予正概率。


<details>
  <summary>Details</summary>
Motivation: 当前语言模型对非规范标记编码分配非零概率，导致概率分配错误且浪费资源。

Method: 提出两种方法：(1) 通过条件推理确保规范标记，(2) 通过模型参数化保证规范输出。

Result: 实验表明修正规范性问题提高了多个模型和语料库的似然性。

Conclusion: 解决规范性问题能有效提升语言模型的性能。

Abstract: Modern language models represent probability distributions over character
strings as distributions over (shorter) token strings derived via a
deterministic tokenizer, such as byte-pair encoding. While this approach is
highly effective at scaling up language models to large corpora, its current
incarnations have a concerning property: the model assigns nonzero probability
mass to an exponential number of $\it{noncanonical}$ token encodings of each
character string -- these are token strings that decode to valid character
strings but are impossible under the deterministic tokenizer (i.e., they will
never be seen in any training corpus, no matter how large). This misallocation
is both erroneous, as noncanonical strings never appear in training data, and
wasteful, diverting probability mass away from plausible outputs. These are
avoidable mistakes! In this work, we propose methods to enforce canonicality in
token-level language models, ensuring that only canonical token strings are
assigned positive probability. We present two approaches: (1) canonicality by
conditioning, leveraging test-time inference strategies without additional
training, and (2) canonicality by construction, a model parameterization that
guarantees canonical outputs but requires training. We demonstrate that fixing
canonicality mistakes improves the likelihood of held-out data for several
models and corpora.

</details>


### [209] [Correlated Errors in Large Language Models](https://arxiv.org/abs/2506.07962)
*Elliot Kim,Avi Garg,Kenny Peng,Nikhil Garg*

Main category: cs.CL

TL;DR: 研究发现，尽管训练数据、架构和提供方的多样性被认为能减少LLM的同质性，但实际中不同LLM的错误高度相关，尤其是在更大、更准确的模型中。


<details>
  <summary>Details</summary>
Motivation: 探讨多样性是否真的能减少LLM的同质性，以及不同LLM之间的差异是否显著。

Method: 对超过350个LLM进行大规模实证评估，使用两个流行排行榜和一个简历筛选任务。

Result: 模型错误高度相关，共享架构和提供方是主要驱动因素；更大、更准确的模型即使架构和提供方不同，错误仍高度相关。

Conclusion: 模型错误的相关性在下游任务（如LLM作为评委和招聘）中产生显著影响，反映了算法单一化的理论预测。

Abstract: Diversity in training data, architecture, and providers is assumed to
mitigate homogeneity in LLMs. However, we lack empirical evidence on whether
different LLMs differ meaningfully. We conduct a large-scale empirical
evaluation on over 350 LLMs overall, using two popular leaderboards and a
resume-screening task. We find substantial correlation in model errors -- on
one leaderboard dataset, models agree 60% of the time when both models err. We
identify factors driving model correlation, including shared architectures and
providers. Crucially, however, larger and more accurate models have highly
correlated errors, even with distinct architectures and providers. Finally, we
show the effects of correlation in two downstream tasks: LLM-as-judge
evaluation and hiring -- the latter reflecting theoretical predictions
regarding algorithmic monoculture.

</details>


### [210] [Reinforcement Pre-Training](https://arxiv.org/abs/2506.08007)
*Qingxiu Dong,Li Dong,Yao Tang,Tianzhu Ye,Yutao Sun,Zhifang Sui,Furu Wei*

Main category: cs.CL

TL;DR: 本文提出了一种新的扩展范式——强化预训练（RPT），通过将下一个词预测任务重构为基于强化学习的推理任务，显著提升了语言模型的准确性。


<details>
  <summary>Details</summary>
Motivation: 传统语言模型预训练依赖于领域特定的标注数据，而RPT旨在利用大量文本数据进行通用强化学习，提升模型的推理能力。

Method: RPT将下一个词预测任务转化为基于强化学习的推理任务，通过可验证的奖励机制优化模型。

Result: RPT显著提高了语言模型的下一个词预测准确性，并为后续强化微调提供了强大的预训练基础。

Conclusion: RPT是一种有效且有前景的语言模型预训练扩展范式，计算资源的增加能持续提升其性能。

Abstract: In this work, we introduce Reinforcement Pre-Training (RPT) as a new scaling
paradigm for large language models and reinforcement learning (RL).
Specifically, we reframe next-token prediction as a reasoning task trained
using RL, where it receives verifiable rewards for correctly predicting the
next token for a given context. RPT offers a scalable method to leverage vast
amounts of text data for general-purpose RL, rather than relying on
domain-specific annotated answers. By incentivizing the capability of
next-token reasoning, RPT significantly improves the language modeling accuracy
of predicting the next tokens. Moreover, RPT provides a strong pre-trained
foundation for further reinforcement fine-tuning. The scaling curves show that
increased training compute consistently improves the next-token prediction
accuracy. The results position RPT as an effective and promising scaling
paradigm to advance language model pre-training.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [211] [Facial Foundational Model Advances Early Warning of Coronary Artery Disease from Live Videos with DigitalShadow](https://arxiv.org/abs/2506.06283)
*Juexiao Zhou,Zhongyi Han,Mankun Xin,Xingwei He,Guotao Wang,Jiaoyan Song,Gongning Luo,Wenjia He,Xintong Li,Yuetan Chu,Juanwen Chen,Bo Wang,Xia Wu,Wenwen Duan,Zhixia Guo,Liyan Bai,Yilin Pan,Xuefei Bi,Lu Liu,Long Feng,Xiaonan He,Xin Gao*

Main category: cs.CV

TL;DR: DigitalShadow是一种基于面部基础模型的CAD早期预警系统，通过无接触方式分析面部特征，生成个性化风险报告。


<details>
  <summary>Details</summary>
Motivation: 全球人口老龄化加剧了医疗系统负担，CAD是主要死因之一，早期检测和管理至关重要。

Method: 系统预训练2100万张面部图像，微调为LiveCAD模型，使用7004张来自1751名受试者的图像进行CAD风险评估。

Result: DigitalShadow能被动、无接触地分析面部特征，生成自然语言风险报告和健康建议。

Conclusion: 该系统以隐私为核心，支持本地部署，为CAD早期检测提供了高效、安全的解决方案。

Abstract: Global population aging presents increasing challenges to healthcare systems,
with coronary artery disease (CAD) responsible for approximately 17.8 million
deaths annually, making it a leading cause of global mortality. As CAD is
largely preventable, early detection and proactive management are essential. In
this work, we introduce DigitalShadow, an advanced early warning system for
CAD, powered by a fine-tuned facial foundation model. The system is pre-trained
on 21 million facial images and subsequently fine-tuned into LiveCAD, a
specialized CAD risk assessment model trained on 7,004 facial images from 1,751
subjects across four hospitals in China. DigitalShadow functions passively and
contactlessly, extracting facial features from live video streams without
requiring active user engagement. Integrated with a personalized database, it
generates natural language risk reports and individualized health
recommendations. With privacy as a core design principle, DigitalShadow
supports local deployment to ensure secure handling of user data.

</details>


### [212] [Exploring Adversarial Watermarking in Transformer-Based Models: Transferability and Robustness Against Defense Mechanism for Medical Images](https://arxiv.org/abs/2506.06389)
*Rifat Sadik,Tanvir Rahman,Arpan Bhattacharjee,Bikash Chandra Halder,Ismail Hossain*

Main category: cs.CV

TL;DR: 论文研究了视觉变换器（ViTs）在医学图像中对对抗性水印攻击的脆弱性，发现其性能显著下降，但通过对抗训练可以大幅提升防御能力。


<details>
  <summary>Details</summary>
Motivation: 随着视觉变换器（ViTs）在计算机视觉任务中的成功应用，研究其在医学图像领域的对抗性攻击脆弱性具有重要意义。

Method: 通过投影梯度下降（PGD）生成对抗性水印，测试ViTs和CNNs的转移性，并分析对抗训练的效果。

Result: ViTs在对抗攻击下准确率降至27.6%，但对抗训练后提升至90.0%。

Conclusion: ViTs对对抗性攻击较为脆弱，但对抗训练是一种有效的防御手段。

Abstract: Deep learning models have shown remarkable success in dermatological image
analysis, offering potential for automated skin disease diagnosis. Previously,
convolutional neural network(CNN) based architectures have achieved immense
popularity and success in computer vision (CV) based task like skin image
recognition, generation and video analysis. But with the emergence of
transformer based models, CV tasks are now are nowadays carrying out using
these models. Vision Transformers (ViTs) is such a transformer-based models
that have shown success in computer vision. It uses self-attention mechanisms
to achieve state-of-the-art performance across various tasks. However, their
reliance on global attention mechanisms makes them susceptible to adversarial
perturbations. This paper aims to investigate the susceptibility of ViTs for
medical images to adversarial watermarking-a method that adds so-called
imperceptible perturbations in order to fool models. By generating adversarial
watermarks through Projected Gradient Descent (PGD), we examine the
transferability of such attacks to CNNs and analyze the performance defense
mechanism -- adversarial training. Results indicate that while performance is
not compromised for clean images, ViTs certainly become much more vulnerable to
adversarial attacks: an accuracy drop of as low as 27.6%. Nevertheless,
adversarial training raises it up to 90.0%.

</details>


### [213] [From Swath to Full-Disc: Advancing Precipitation Retrieval with Multimodal Knowledge Expansion](https://arxiv.org/abs/2506.07050)
*Zheng Wang,Kai Ying,Bin Xu,Chunjiao Wang,Cong Bai*

Main category: cs.CV

TL;DR: 论文提出了一种名为PRE-Net的两阶段模型，通过多模态知识扩展技术提升红外降水反演的准确性，解决了现有方法在范围和精度上的限制。


<details>
  <summary>Details</summary>
Motivation: 现有红外降水反演算法精度低，而被动微波和雷达方法范围有限，因此需要一种能在全盘范围内实现高精度红外降水反演的方法。

Method: PRE-Net采用两阶段流程：1) 在扫描带内通过CoMWE技术从多模态数据中蒸馏知识；2) 在全盘范围内通过Self-MaskTune平衡多模态和红外知识。

Result: 实验表明，PRE-Net在降水反演性能上显著优于PERSIANN-CCS、PDIR和IMERG等领先产品。

Conclusion: PRE-Net成功实现了高精度的全盘红外降水反演，为相关领域提供了有效解决方案。

Abstract: Accurate near-real-time precipitation retrieval has been enhanced by
satellite-based technologies. However, infrared-based algorithms have low
accuracy due to weak relations with surface precipitation, whereas passive
microwave and radar-based methods are more accurate but limited in range. This
challenge motivates the Precipitation Retrieval Expansion (PRE) task, which
aims to enable accurate, infrared-based full-disc precipitation retrievals
beyond the scanning swath. We introduce Multimodal Knowledge Expansion, a
two-stage pipeline with the proposed PRE-Net model. In the Swath-Distilling
stage, PRE-Net transfers knowledge from a multimodal data integration model to
an infrared-based model within the scanning swath via Coordinated Masking and
Wavelet Enhancement (CoMWE). In the Full-Disc Adaptation stage, Self-MaskTune
refines predictions across the full disc by balancing multimodal and full-disc
infrared knowledge. Experiments on the introduced PRE benchmark demonstrate
that PRE-Net significantly advanced precipitation retrieval performance,
outperforming leading products like PERSIANN-CCS, PDIR, and IMERG. The code
will be available at https://github.com/Zjut-MultimediaPlus/PRE-Net.

</details>


### [214] [(LiFT) Lightweight Fitness Transformer: A language-vision model for Remote Monitoring of Physical Training](https://arxiv.org/abs/2506.06480)
*A. Postlmayr,P. Cosman,S. Dey*

Main category: cs.CV

TL;DR: 提出了一种基于RGB智能手机摄像头的远程健身追踪系统，具有隐私性、可扩展性和成本效益，能检测和计数数百种运动。


<details>
  <summary>Details</summary>
Motivation: 现有健身追踪模型要么运动种类有限，要么过于复杂难以部署，缺乏通用性。

Method: 开发了一个多任务运动分析模型，利用大规模数据集Olympia（包含1900多种运动），结合视觉-语言模型进行运动检测和计数。

Result: 模型在Olympia上运动检测准确率为76.5%，计数准确率为85.3%。

Conclusion: 通过单一视觉-语言模型实现运动识别和计数，推动了AI健身追踪的普及。

Abstract: We introduce a fitness tracking system that enables remote monitoring for
exercises using only a RGB smartphone camera, making fitness tracking more
private, scalable, and cost effective. Although prior work explored automated
exercise supervision, existing models are either too limited in exercise
variety or too complex for real-world deployment. Prior approaches typically
focus on a small set of exercises and fail to generalize across diverse
movements. In contrast, we develop a robust, multitask motion analysis model
capable of performing exercise detection and repetition counting across
hundreds of exercises, a scale far beyond previous methods. We overcome
previous data limitations by assembling a large-scale fitness dataset, Olympia
covering more than 1,900 exercises. To our knowledge, our vision-language model
is the first that can perform multiple tasks on skeletal fitness data. On
Olympia, our model can detect exercises with 76.5% accuracy and count
repetitions with 85.3% off-by-one accuracy, using only RGB video. By presenting
a single vision-language transformer model for both exercise identification and
rep counting, we take a significant step toward democratizing AI-powered
fitness tracking.

</details>


### [215] [Learning Compact Vision Tokens for Efficient Large Multimodal Models](https://arxiv.org/abs/2506.07138)
*Hao Tang,Chengchao Shen*

Main category: cs.CV

TL;DR: 论文提出了一种空间令牌融合（STF）和多块令牌融合（MBTF）方法，以减少视觉令牌序列长度并提升推理效率，同时保持多模态推理能力。


<details>
  <summary>Details</summary>
Motivation: 大型多模态模型（LMMs）因大型语言模型（LLMs）的高计算成本和长视觉令牌序列的二次复杂度而面临计算挑战。

Method: 通过STF融合空间相邻令牌以减少序列长度，并通过MBTF补充多粒度特征以保持信息完整性。

Result: 在8个流行视觉语言基准测试中，仅使用基线25%的视觉令牌即可达到或超越基线性能。

Conclusion: 该方法在提升推理效率的同时未牺牲多模态推理能力，具有实际应用价值。

Abstract: Large multimodal models (LMMs) suffer significant computational challenges
due to the high cost of Large Language Models (LLMs) and the quadratic
complexity of processing long vision token sequences. In this paper, we explore
the spatial redundancy among vision tokens and shorten the length of vision
token sequences for inference acceleration. Specifically, we propose a Spatial
Token Fusion (STF) method to learn compact vision tokens for short vision token
sequence, where spatial-adjacent tokens are fused into one. Meanwhile,
weight-frozen vision encoder can not well adapt to the demand of extensive
downstream vision-language tasks. To this end, we further introduce a
Multi-Block Token Fusion (MBTF) module to supplement multi-granularity features
for the reduced token sequence. Overall, we combine STF and MBTF module to
balance token reduction and information preservation, thereby improving
inference efficiency without sacrificing multimodal reasoning capabilities.
Experimental results demonstrate that our method based on LLaVA-1.5 achieves
comparable or even superior performance to the baseline on 8 popular
vision-language benchmarks with only $25\%$ vision tokens of baseline. The
source code and trained weights are available at
https://github.com/visresearch/LLaVA-STF.

</details>


### [216] [GS4: Generalizable Sparse Splatting Semantic SLAM](https://arxiv.org/abs/2506.06517)
*Mingqi Jiang,Chanho Kim,Chen Ziwen,Li Fuxin*

Main category: cs.CV

TL;DR: 提出了一种基于高斯泼溅（GS）的可泛化语义SLAM算法，通过学习网络从RGB-D视频流增量构建3D场景表示，解决了现有GS方法依赖场景优化且泛化性差的问题。


<details>
  <summary>Details</summary>
Motivation: 传统SLAM算法在相机跟踪上表现优秀，但生成的3D地图分辨率低且不完整。现有基于GS的SLAM方法依赖场景优化，耗时长且泛化性差。

Method: 使用可泛化网络从RGB-D图像预测高斯参数，并集成3D语义分割到GS框架中。通过全局定位后仅优化1次GS来纠正定位漂移和浮点问题。

Result: 在ScanNet基准测试中实现了最先进的语义SLAM性能，高斯数量比其他GS方法少一个数量级，并在NYUv2和TUM RGB-D数据集上展示了零样本泛化能力。

Conclusion: 该方法通过可泛化网络和高效优化策略，显著提升了基于GS的语义SLAM的性能和泛化能力。

Abstract: Traditional SLAM algorithms are excellent at camera tracking but might
generate lower resolution and incomplete 3D maps. Recently, Gaussian Splatting
(GS) approaches have emerged as an option for SLAM with accurate, dense 3D map
building. However, existing GS-based SLAM methods rely on per-scene
optimization which is time-consuming and does not generalize to diverse scenes
well. In this work, we introduce the first generalizable GS-based semantic SLAM
algorithm that incrementally builds and updates a 3D scene representation from
an RGB-D video stream using a learned generalizable network. Our approach
starts from an RGB-D image recognition backbone to predict the Gaussian
parameters from every downsampled and backprojected image location.
Additionally, we seamlessly integrate 3D semantic segmentation into our GS
framework, bridging 3D mapping and recognition through a shared backbone. To
correct localization drifting and floaters, we propose to optimize the GS for
only 1 iteration following global localization. We demonstrate state-of-the-art
semantic SLAM performance on the real-world benchmark ScanNet with an order of
magnitude fewer Gaussians compared to other recent GS-based methods, and
showcase our model's generalization capability through zero-shot transfer to
the NYUv2 and TUM RGB-D datasets.

</details>


### [217] [Bridging Audio and Vision: Zero-Shot Audiovisual Segmentation by Connecting Pretrained Models](https://arxiv.org/abs/2506.06537)
*Seung-jae Lee,Paul Hongsuck Seo*

Main category: cs.CV

TL;DR: 提出了一种零样本视听分割框架，利用预训练模型实现无需任务特定训练的高效分割。


<details>
  <summary>Details</summary>
Motivation: 传统视听分割方法依赖大规模像素级标注，成本高且耗时。

Method: 整合音频、视觉和文本表征，探索预训练模型连接策略。

Result: 在多个数据集上实现零样本SOTA性能。

Conclusion: 多模态模型整合对精细化视听分割有效。

Abstract: Audiovisual segmentation (AVS) aims to identify visual regions corresponding
to sound sources, playing a vital role in video understanding, surveillance,
and human-computer interaction. Traditional AVS methods depend on large-scale
pixel-level annotations, which are costly and time-consuming to obtain. To
address this, we propose a novel zero-shot AVS framework that eliminates
task-specific training by leveraging multiple pretrained models. Our approach
integrates audio, vision, and text representations to bridge modality gaps,
enabling precise sound source segmentation without AVS-specific annotations. We
systematically explore different strategies for connecting pretrained models
and evaluate their efficacy across multiple datasets. Experimental results
demonstrate that our framework achieves state-of-the-art zero-shot AVS
performance, highlighting the effectiveness of multimodal model integration for
finegrained audiovisual segmentation.

</details>


### [218] [VIVAT: Virtuous Improving VAE Training through Artifact Mitigation](https://arxiv.org/abs/2506.07863)
*Lev Novitskiy,Viacheslav Vasilev,Maria Kovaleva,Vladimir Arkhipkin,Denis Dimitrov*

Main category: cs.CV

TL;DR: VIVAT通过简单调整KL-VAE训练中的损失权重、填充策略和引入空间条件归一化，有效减少常见伪影，提升重建和生成质量。


<details>
  <summary>Details</summary>
Motivation: KL-VAE训练中常见的伪影（如颜色偏移、网格模式等）影响重建和生成质量，需在不改变架构的情况下解决。

Method: 提出VIVAT方法，包括调整损失权重、优化填充策略和引入空间条件归一化。

Result: 在多个基准测试中取得最佳重建指标（PSNR和SSIM），并提升文本到图像生成的CLIP分数。

Conclusion: VIVAT为优化VAE训练提供了实用方案，同时保持了KL-VAE的简洁性。

Abstract: Variational Autoencoders (VAEs) remain a cornerstone of generative computer
vision, yet their training is often plagued by artifacts that degrade
reconstruction and generation quality. This paper introduces VIVAT, a
systematic approach to mitigating common artifacts in KL-VAE training without
requiring radical architectural changes. We present a detailed taxonomy of five
prevalent artifacts - color shift, grid patterns, blur, corner and droplet
artifacts - and analyze their root causes. Through straightforward
modifications, including adjustments to loss weights, padding strategies, and
the integration of Spatially Conditional Normalization, we demonstrate
significant improvements in VAE performance. Our method achieves
state-of-the-art results in image reconstruction metrics (PSNR and SSIM) across
multiple benchmarks and enhances text-to-image generation quality, as evidenced
by superior CLIP scores. By preserving the simplicity of the KL-VAE framework
while addressing its practical challenges, VIVAT offers actionable insights for
researchers and practitioners aiming to optimize VAE training.

</details>


### [219] [Securing Traffic Sign Recognition Systems in Autonomous Vehicles](https://arxiv.org/abs/2506.06563)
*Thushari Hapuarachchi,Long Dang,Kaiqi Xiong*

Main category: cs.CV

TL;DR: 该论文研究了深度神经网络（DNNs）在交通标志识别中的鲁棒性，提出了一种基于数据增强的训练方法以抵御误差最小化攻击，并开发了一个检测模型来识别被污染的数据。


<details>
  <summary>Details</summary>
Motivation: 由于DNNs在交通标志识别中的广泛应用，且训练数据可能来自未知来源，确保模型在训练过程中不被攻击或污染至关重要。

Method: 论文首先对交通标志识别的DNNs进行误差最小化攻击，然后提出一种基于非线性变换的数据增强训练方法以提高模型鲁棒性。

Result: 误差最小化攻击将DNNs的预测准确率从99.90%降至10.6%，而提出的方法成功将准确率恢复至96.05%，并优于对抗训练。检测模型对攻击的识别成功率超过99%。

Conclusion: 研究表明，交通标志识别系统需采用先进的训练方法以抵御数据污染攻击，提出的方法在提高模型鲁棒性和检测攻击方面表现出色。

Abstract: Deep Neural Networks (DNNs) are widely used for traffic sign recognition
because they can automatically extract high-level features from images. These
DNNs are trained on large-scale datasets obtained from unknown sources.
Therefore, it is important to ensure that the models remain secure and are not
compromised or poisoned during training. In this paper, we investigate the
robustness of DNNs trained for traffic sign recognition. First, we perform the
error-minimizing attacks on DNNs used for traffic sign recognition by adding
imperceptible perturbations on training data. Then, we propose a data
augmentation-based training method to mitigate the error-minimizing attacks.
The proposed training method utilizes nonlinear transformations to disrupt the
perturbations and improve the model robustness. We experiment with two
well-known traffic sign datasets to demonstrate the severity of the attack and
the effectiveness of our mitigation scheme. The error-minimizing attacks reduce
the prediction accuracy of the DNNs from 99.90% to 10.6%. However, our
mitigation scheme successfully restores the prediction accuracy to 96.05%.
Moreover, our approach outperforms adversarial training in mitigating the
error-minimizing attacks. Furthermore, we propose a detection model capable of
identifying poisoned data even when the perturbations are imperceptible to
human inspection. Our detection model achieves a success rate of over 99% in
identifying the attack. This research highlights the need to employ advanced
training methods for DNNs in traffic sign recognition systems to mitigate the
effects of data poisoning attacks.

</details>


### [220] [Textile Analysis for Recycling Automation using Transfer Learning and Zero-Shot Foundation Models](https://arxiv.org/abs/2506.06569)
*Yannis Spyridis,Vasileios Argyriou*

Main category: cs.CV

TL;DR: 论文探讨了利用RGB图像和深度学习技术（如迁移学习和基础模型）实现纺织品自动分类和污染物分割的可行性，为纺织品回收自动化提供了关键预处理步骤。


<details>
  <summary>Details</summary>
Motivation: 纺织品回收自动化中，准确识别材料成分和检测污染物是主要挑战，而RGB图像是一种低成本且有效的感知方式。

Method: 研究采用预训练架构进行纺织品分类（EfficientNetB0表现最佳），并结合Grounding DINO与Segment Anything Model（SAM）进行零样本污染物分割。

Result: 分类任务准确率达81.25%，分割任务mIoU为0.90，表明RGB图像结合深度学习技术效果显著。

Conclusion: RGB图像结合现代深度学习技术（如迁移学习和基础模型）可用于纺织品回收自动化中的关键分析步骤。

Abstract: Automated sorting is crucial for improving the efficiency and scalability of
textile recycling, but accurately identifying material composition and
detecting contaminants from sensor data remains challenging. This paper
investigates the use of standard RGB imagery, a cost-effective sensing
modality, for key pre-processing tasks in an automated system. We present
computer vision components designed for a conveyor belt setup to perform (a)
classification of four common textile types and (b) segmentation of non-textile
features such as buttons and zippers. For classification, several pre-trained
architectures were evaluated using transfer learning and cross-validation, with
EfficientNetB0 achieving the best performance on a held-out test set with
81.25\% accuracy. For feature segmentation, a zero-shot approach combining the
Grounding DINO open-vocabulary detector with the Segment Anything Model (SAM)
was employed, demonstrating excellent performance with a mIoU of 0.90 for the
generated masks against ground truth. This study demonstrates the feasibility
of using RGB images coupled with modern deep learning techniques, including
transfer learning for classification and foundation models for zero-shot
segmentation, to enable essential analysis steps for automated textile
recycling pipelines.

</details>


### [221] [A Deep Learning Approach for Facial Attribute Manipulation and Reconstruction in Surveillance and Reconnaissance](https://arxiv.org/abs/2506.06578)
*Anees Nashath Shaik,Barbara Villarini,Vasileios Argyriou*

Main category: cs.CV

TL;DR: 论文提出了一种数据驱动平台，通过生成合成训练数据来解决现有AI面部识别模型中的偏见问题，并提升低质量图像的清晰度。


<details>
  <summary>Details</summary>
Motivation: 现有监控系统因图像质量低和数据集偏见导致面部识别准确性下降，尤其是在肤色变化和遮挡情况下。

Method: 利用深度学习的面部属性操纵和重建技术（如自动编码器和GANs）生成多样化数据集，并集成图像增强模块。

Result: 在CelebA数据集上的实验表明，该平台提高了数据多样性和模型公平性。

Conclusion: 该工作有助于减少AI面部分析的偏见，提升监控系统在复杂环境中的准确性和可靠性。

Abstract: Surveillance systems play a critical role in security and reconnaissance, but
their performance is often compromised by low-quality images and videos,
leading to reduced accuracy in face recognition. Additionally, existing
AI-based facial analysis models suffer from biases related to skin tone
variations and partially occluded faces, further limiting their effectiveness
in diverse real-world scenarios. These challenges are the results of data
limitations and imbalances, where available training datasets lack sufficient
diversity, resulting in unfair and unreliable facial recognition performance.
To address these issues, we propose a data-driven platform that enhances
surveillance capabilities by generating synthetic training data tailored to
compensate for dataset biases. Our approach leverages deep learning-based
facial attribute manipulation and reconstruction using autoencoders and
Generative Adversarial Networks (GANs) to create diverse and high-quality
facial datasets. Additionally, our system integrates an image enhancement
module, improving the clarity of low-resolution or occluded faces in
surveillance footage. We evaluate our approach using the CelebA dataset,
demonstrating that the proposed platform enhances both training data diversity
and model fairness. This work contributes to reducing bias in AI-based facial
analysis and improving surveillance accuracy in challenging environments,
leading to fairer and more reliable security applications.

</details>


### [222] [EV-LayerSegNet: Self-supervised Motion Segmentation using Event Cameras](https://arxiv.org/abs/2506.06596)
*Youssef Farah,Federico Paredes-Vallés,Guido De Croon,Muhammad Ahmed Humais,Hussain Sajwani,Yahya Zweiri*

Main category: cs.CV

TL;DR: EV-LayerSegNet是一种自监督CNN，用于事件相机的运动分割，通过分层场景动态表示学习光流和分割掩码，并利用去模糊质量作为自监督损失。


<details>
  <summary>Details</summary>
Motivation: 事件相机在运动任务中表现优异，但训练网络时获取真实标注成本高且困难。

Method: 提出EV-LayerSegNet，通过分层动态表示分别学习仿射光流和分割掩码，并利用去模糊质量作为自监督损失。

Result: 在仅含仿射运动的模拟数据集上，IoU和检测率分别达到71%和87%。

Conclusion: EV-LayerSegNet在自监督条件下有效实现了事件相机的运动分割。

Abstract: Event cameras are novel bio-inspired sensors that capture motion dynamics
with much higher temporal resolution than traditional cameras, since pixels
react asynchronously to brightness changes. They are therefore better suited
for tasks involving motion such as motion segmentation. However, training
event-based networks still represents a difficult challenge, as obtaining
ground truth is very expensive, error-prone and limited in frequency. In this
article, we introduce EV-LayerSegNet, a self-supervised CNN for event-based
motion segmentation. Inspired by a layered representation of the scene
dynamics, we show that it is possible to learn affine optical flow and
segmentation masks separately, and use them to deblur the input events. The
deblurring quality is then measured and used as self-supervised learning loss.
We train and test the network on a simulated dataset with only affine motion,
achieving IoU and detection rate up to 71% and 87% respectively.

</details>


### [223] [RARL: Improving Medical VLM Reasoning and Generalization with Reinforcement Learning and LoRA under Data and Hardware Constraints](https://arxiv.org/abs/2506.06600)
*Tan-Hanh Pham,Chris Ngo*

Main category: cs.CV

TL;DR: 论文提出了一种名为RARL的框架，通过强化学习提升医学视觉语言模型的推理能力，同时保持高效性和适应性。


<details>
  <summary>Details</summary>
Motivation: 当前医学视觉语言模型在泛化性、透明性和计算效率方面存在局限，阻碍了其在资源受限环境中的实际应用。

Method: 采用低秩适应和自定义奖励函数对轻量级基础模型Qwen2-VL-2B-Instruct进行微调，训练在单块NVIDIA A100-PCIE-40GB GPU上完成。

Result: RARL显著提升了医学图像分析和临床推理的性能，在推理任务上比监督微调高出约7.78%，且计算资源需求更低。

Conclusion: 研究表明，推理引导学习和推理提示可以推动医学视觉语言模型实现更透明、准确和资源高效的临床决策。

Abstract: The growing integration of vision-language models (VLMs) in medical
applications offers promising support for diagnostic reasoning. However,
current medical VLMs often face limitations in generalization, transparency,
and computational efficiency-barriers that hinder deployment in real-world,
resource-constrained settings. To address these challenges, we propose a
Reasoning-Aware Reinforcement Learning framework, \textbf{RARL}, that enhances
the reasoning capabilities of medical VLMs while remaining efficient and
adaptable to low-resource environments. Our approach fine-tunes a lightweight
base model, Qwen2-VL-2B-Instruct, using Low-Rank Adaptation and custom reward
functions that jointly consider diagnostic accuracy and reasoning quality.
Training is performed on a single NVIDIA A100-PCIE-40GB GPU, demonstrating the
feasibility of deploying such models in constrained environments. We evaluate
the model using an LLM-as-judge framework that scores both correctness and
explanation quality. Experimental results show that RARL significantly improves
VLM performance in medical image analysis and clinical reasoning, outperforming
supervised fine-tuning on reasoning-focused tasks by approximately 7.78%, while
requiring fewer computational resources. Additionally, we demonstrate the
generalization capabilities of our approach on unseen datasets, achieving
around 27% improved performance compared to supervised fine-tuning and about 4%
over traditional RL fine-tuning. Our experiments also illustrate that diversity
prompting during training and reasoning prompting during inference are crucial
for enhancing VLM performance. Our findings highlight the potential of
reasoning-guided learning and reasoning prompting to steer medical VLMs toward
more transparent, accurate, and resource-efficient clinical decision-making.
Code and data are publicly available.

</details>


### [224] [Zero Shot Composed Image Retrieval](https://arxiv.org/abs/2506.06602)
*Santhosh Kakarla,Gautama Shastry Bulusu Venkata*

Main category: cs.CV

TL;DR: 通过微调BLIP-2和轻量级Q-Former，将FashionIQ基准上的Recall@10提高到45.6%（衬衫）、40.1%（连衣裙）和50.4%（T恤），平均Recall@50达到67.6%。Retrieval-DPO方法因缺乏多模态融合和低质量负样本等问题表现不佳。


<details>
  <summary>Details</summary>
Motivation: 改进零样本CIR在FashionIQ基准上的低召回率（20-25% Recall@10），探索更有效的多模态融合方法。

Method: 1. 微调BLIP-2，使用轻量级Q-Former融合视觉和文本特征；2. 尝试Retrieval-DPO方法，通过Direct Preference Optimization损失微调CLIP文本编码器。

Result: BLIP-2方法显著提升召回率（Recall@10达45.6%），而Retrieval-DPO方法表现极差（Recall@10仅0.02%）。

Conclusion: 有效的基于偏好的CIR需要多模态融合、排名感知目标和高质量负样本。

Abstract: Composed image retrieval (CIR) allows a user to locate a target image by
applying a fine-grained textual edit (e.g., ``turn the dress blue'' or ``remove
stripes'') to a reference image. Zero-shot CIR, which embeds the image and the
text with separate pretrained vision-language encoders, reaches only 20-25\%
Recall@10 on the FashionIQ benchmark. We improve this by fine-tuning BLIP-2
with a lightweight Q-Former that fuses visual and textual features into a
single embedding, raising Recall@10 to 45.6\% (shirt), 40.1\% (dress), and
50.4\% (top-tee) and increasing the average Recall@50 to 67.6\%. We also
examine Retrieval-DPO, which fine-tunes CLIP's text encoder with a Direct
Preference Optimization loss applied to FAISS-mined hard negatives. Despite
extensive tuning of the scaling factor, index, and sampling strategy,
Retrieval-DPO attains only 0.02\% Recall@10 -- far below zero-shot and
prompt-tuned baselines -- because it (i) lacks joint image-text fusion, (ii)
uses a margin objective misaligned with top-$K$ metrics, (iii) relies on
low-quality negatives, and (iv) keeps the vision and Transformer layers frozen.
Our results show that effective preference-based CIR requires genuine
multimodal fusion, ranking-aware objectives, and carefully curated negatives.

</details>


### [225] [PhysLab: A Benchmark Dataset for Multi-Granularity Visual Parsing of Physics Experiments](https://arxiv.org/abs/2506.06631)
*Minghao Zou,Qingtian Zeng,Yongping Miao,Shangkun Liu,Zilong Wang,Hantao Liu,Wei Zhou*

Main category: cs.CV

TL;DR: PhysLab是一个针对教育场景的视频数据集，专注于学生进行复杂物理实验的场景，填补了现有数据集在标注粒度、领域覆盖和程序指导方面的不足。


<details>
  <summary>Details</summary>
Motivation: 现有数据集在标注粒度、领域覆盖和程序指导方面存在不足，限制了视觉解析的进展，特别是在教育场景中的应用。

Method: 引入PhysLab数据集，包含620个长视频，涵盖四个代表性物理实验，提供多级标注支持多种视觉任务。

Result: 建立了强基线并进行了广泛评估，突出了解析教育视频中的关键挑战。

Conclusion: PhysLab有望推动细粒度视觉解析、智能课堂系统的发展，并促进计算机视觉与教育技术的融合。

Abstract: Visual parsing of images and videos is critical for a wide range of
real-world applications. However, progress in this field is constrained by
limitations of existing datasets: (1) insufficient annotation granularity,
which impedes fine-grained scene understanding and high-level reasoning; (2)
limited coverage of domains, particularly a lack of datasets tailored for
educational scenarios; and (3) lack of explicit procedural guidance, with
minimal logical rules and insufficient representation of structured task
process. To address these gaps, we introduce PhysLab, the first video dataset
that captures students conducting complex physics experiments. The dataset
includes four representative experiments that feature diverse scientific
instruments and rich human-object interaction (HOI) patterns. PhysLab comprises
620 long-form videos and provides multilevel annotations that support a variety
of vision tasks, including action recognition, object detection, HOI analysis,
etc. We establish strong baselines and perform extensive evaluations to
highlight key challenges in the parsing of procedural educational videos. We
expect PhysLab to serve as a valuable resource for advancing fine-grained
visual parsing, facilitating intelligent classroom systems, and fostering
closer integration between computer vision and educational technologies. The
dataset and the evaluation toolkit are publicly available at
https://github.com/ZMH-SDUST/PhysLab.

</details>


### [226] [Dark Channel-Assisted Depth-from-Defocus from a Single Image](https://arxiv.org/abs/2506.06643)
*Moushumi Medhi,Rajiv Ranjan Sahay*

Main category: cs.CV

TL;DR: 利用暗通道作为补充线索，从单张空间变异散焦模糊图像中估计深度，通过对抗训练实现端到端学习，实验验证了方法的有效性。


<details>
  <summary>Details</summary>
Motivation: 现有深度从散焦（DFD）技术通常依赖多张图像，而单张散焦图像的深度估计问题具有欠约束性，因此需要新的方法。

Method: 利用暗通道先验和局部散焦模糊与对比度变化的关系作为深度线索，通过对抗训练实现端到端学习。

Result: 在真实数据上的实验表明，结合暗通道先验的单图像DFD方法能有效估计深度。

Conclusion: 该方法通过暗通道先验和对抗训练，成功解决了单张散焦图像深度估计的挑战。

Abstract: In this paper, we utilize the dark channel as a complementary cue to estimate
the depth of a scene from a single space-variant defocus blurred image due to
its effectiveness in implicitly capturing the local statistics of blurred
images and the scene structure. Existing depth-from-defocus (DFD) techniques
typically rely on multiple images with varying apertures or focus settings to
recover depth information. Very few attempts have focused on DFD from a single
defocused image due to the underconstrained nature of the problem. Our method
capitalizes on the relationship between local defocus blur and contrast
variations as key depth cues to enhance the overall performance in estimating
the scene's structure. The entire pipeline is trained adversarially in a fully
end-to-end fashion. Experiments conducted on real data with realistic
depth-induced defocus blur demonstrate that incorporating dark channel prior
into single image DFD yields meaningful depth estimation results, validating
the effectiveness of our approach.

</details>


### [227] [Parametric Gaussian Human Model: Generalizable Prior for Efficient and Realistic Human Avatar Modeling](https://arxiv.org/abs/2506.06645)
*Cheng Peng,Jingxiang Sun,Yushuo Chen,Zhaoqi Su,Zhuo Su,Yebin Liu*

Main category: cs.CV

TL;DR: PGHM提出了一种基于3D高斯散射的通用高效框架，用于从单目视频快速重建高保真人体化身。


<details>
  <summary>Details</summary>
Motivation: 现有方法在单目输入下泛化能力差且优化耗时，PGHM旨在解决这些问题。

Method: 引入UV对齐的潜在身份映射和多头U-Net，分解静态、姿态和视角依赖的高斯属性。

Result: PGHM仅需约20分钟即可生成与优化方法质量相当的化身，效率显著提升。

Conclusion: PGHM在单目化身创建中具有实际应用价值。

Abstract: Photorealistic and animatable human avatars are a key enabler for
virtual/augmented reality, telepresence, and digital entertainment. While
recent advances in 3D Gaussian Splatting (3DGS) have greatly improved rendering
quality and efficiency, existing methods still face fundamental challenges,
including time-consuming per-subject optimization and poor generalization under
sparse monocular inputs. In this work, we present the Parametric Gaussian Human
Model (PGHM), a generalizable and efficient framework that integrates human
priors into 3DGS for fast and high-fidelity avatar reconstruction from
monocular videos. PGHM introduces two core components: (1) a UV-aligned latent
identity map that compactly encodes subject-specific geometry and appearance
into a learnable feature tensor; and (2) a disentangled Multi-Head U-Net that
predicts Gaussian attributes by decomposing static, pose-dependent, and
view-dependent components via conditioned decoders. This design enables robust
rendering quality under challenging poses and viewpoints, while allowing
efficient subject adaptation without requiring multi-view capture or long
optimization time. Experiments show that PGHM is significantly more efficient
than optimization-from-scratch methods, requiring only approximately 20 minutes
per subject to produce avatars with comparable visual quality, thereby
demonstrating its practical applicability for real-world monocular avatar
creation.

</details>


### [228] [Flood-DamageSense: Multimodal Mamba with Multitask Learning for Building Flood Damage Assessment using SAR Remote Sensing Imagery](https://arxiv.org/abs/2506.06667)
*Yu-Hsuan Ho,Ali Mostafavi*

Main category: cs.CV

TL;DR: Flood-DamageSense是一种专为洪水灾害建筑损坏评估设计的深度学习框架，通过融合多源数据显著提升了分类性能。


<details>
  <summary>Details</summary>
Motivation: 现有模型在洪水灾害后建筑损坏分类中表现不佳，因为洪水通常不会留下明显的光谱或结构特征。

Method: 结合SAR/InSAR数据、高分辨率光学底图和洪水风险层，采用多模态Mamba架构和半孪生编码器，联合预测建筑损坏等级、洪水范围和建筑轮廓。

Result: 在Hurricane Harvey数据上，F1分数比现有技术提升了19个百分点，尤其在“轻微”和“中等”损坏类别中表现突出。

Conclusion: Flood-DamageSense通过风险感知建模和SAR的全天候能力，提供了更快速、更精细的洪水损坏评估，支持灾后决策。

Abstract: Most post-disaster damage classifiers succeed only when destructive forces
leave clear spectral or structural signatures -- conditions rarely present
after inundation. Consequently, existing models perform poorly at identifying
flood-related building damages. The model presented in this study,
Flood-DamageSense, addresses this gap as the first deep-learning framework
purpose-built for building-level flood-damage assessment. The architecture
fuses pre- and post-event SAR/InSAR scenes with very-high-resolution optical
basemaps and an inherent flood-risk layer that encodes long-term exposure
probabilities, guiding the network toward plausibly affected structures even
when compositional change is minimal. A multimodal Mamba backbone with a
semi-Siamese encoder and task-specific decoders jointly predicts (1) graded
building-damage states, (2) floodwater extent, and (3) building footprints.
Training and evaluation on Hurricane Harvey (2017) imagery from Harris County,
Texas -- supported by insurance-derived property-damage extents -- show a mean
F1 improvement of up to 19 percentage points over state-of-the-art baselines,
with the largest gains in the frequently misclassified "minor" and "moderate"
damage categories. Ablation studies identify the inherent-risk feature as the
single most significant contributor to this performance boost. An end-to-end
post-processing pipeline converts pixel-level outputs to actionable,
building-scale damage maps within minutes of image acquisition. By combining
risk-aware modeling with SAR's all-weather capability, Flood-DamageSense
delivers faster, finer-grained, and more reliable flood-damage intelligence to
support post-disaster decision-making and resource allocation.

</details>


### [229] [Interpretation of Deep Learning Model in Embryo Selection for In Vitro Fertilization (IVF) Treatment](https://arxiv.org/abs/2506.06680)
*Radha Kodali,Venkata Rao Dhulipalla,Venkata Siva Kishor Tatavarty,Madhavi Nadakuditi,Bharadwaj Thiruveedhula,Suryanarayana Gunnam,Durga Prasad Bavirisetti*

Main category: cs.CV

TL;DR: 本文提出了一种基于CNN-LSTM的可解释人工智能框架，用于高效分类胚胎图像，解决了传统胚胎分级方法的低效问题。


<details>
  <summary>Details</summary>
Motivation: 不孕症对生活质量有显著影响，体外受精（IVF）是主要解决方案，但传统胚胎分级方法效率低下且耗时。

Method: 采用CNN-LSTM混合架构，结合深度学习和可解释AI技术，对胚胎图像进行分类。

Result: 模型在胚胎分类中实现了高准确性，同时保持了可解释性。

Conclusion: 该框架为胚胎选择提供了高效且可解释的解决方案，有望提升IVF成功率。

Abstract: Infertility has a considerable impact on individuals' quality of life,
affecting them socially and psychologically, with projections indicating a rise
in the upcoming years. In vitro fertilization (IVF) emerges as one of the
primary techniques within economically developed nations, employed to address
the rising problem of low fertility. Expert embryologists conventionally grade
embryos by reviewing blastocyst images to select the most optimal for transfer,
yet this process is time-consuming and lacks efficiency. Blastocyst images
provide a valuable resource for assessing embryo viability. In this study, we
introduce an explainable artificial intelligence (XAI) framework for
classifying embryos, employing a fusion of convolutional neural network (CNN)
and long short-term memory (LSTM) architecture, referred to as CNN-LSTM.
Utilizing deep learning, our model achieves high accuracy in embryo
classification while maintaining interpretability through XAI.

</details>


### [230] [A Systematic Investigation on Deep Learning-Based Omnidirectional Image and Video Super-Resolution](https://arxiv.org/abs/2506.06710)
*Qianqian Zhao,Chunle Guo,Tianyi Zhang,Junpei Zhang,Peiyang Jia,Tan Su,Wenjie Jiang,Chongyi Li*

Main category: cs.CV

TL;DR: 本文系统综述了基于深度学习的全向图像和视频超分辨率方法，并提出了包含真实退化数据的新数据集360Insta，以解决现有合成数据集的局限性。


<details>
  <summary>Details</summary>
Motivation: 全向图像和视频超分辨率在虚拟现实和增强现实中至关重要，但现有数据集多为合成退化数据，无法捕捉真实世界的失真。

Method: 通过引入真实退化数据集360Insta，结合公开数据集，对现有方法进行全面定性和定量评估。

Result: 360Insta填补了现有全向基准数据集的空白，支持更鲁棒的超分辨率方法评估。

Conclusion: 本文为全向超分辨率研究提供了系统综述和新数据集，并展望了未来研究方向。

Abstract: Omnidirectional image and video super-resolution is a crucial research topic
in low-level vision, playing an essential role in virtual reality and augmented
reality applications. Its goal is to reconstruct high-resolution images or
video frames from low-resolution inputs, thereby enhancing detail preservation
and enabling more accurate scene analysis and interpretation. In recent years,
numerous innovative and effective approaches have been proposed, predominantly
based on deep learning techniques, involving diverse network architectures,
loss functions, projection strategies, and training datasets. This paper
presents a systematic review of recent progress in omnidirectional image and
video super-resolution, focusing on deep learning-based methods. Given that
existing datasets predominantly rely on synthetic degradation and fall short in
capturing real-world distortions, we introduce a new dataset, 360Insta, that
comprises authentically degraded omnidirectional images and videos collected
under diverse conditions, including varying lighting, motion, and exposure
settings. This dataset addresses a critical gap in current omnidirectional
benchmarks and enables more robust evaluation of the generalization
capabilities of omnidirectional super-resolution methods. We conduct
comprehensive qualitative and quantitative evaluations of existing methods on
both public datasets and our proposed dataset. Furthermore, we provide a
systematic overview of the current status of research and discuss promising
directions for future exploration. All datasets, methods, and evaluation
metrics introduced in this work are publicly available and will be regularly
updated. Project page: https://github.com/nqian1/Survey-on-ODISR-and-ODVSR.

</details>


### [231] [Active Contour Models Driven by Hyperbolic Mean Curvature Flow for Image Segmentation](https://arxiv.org/abs/2506.06712)
*Saiyu Hu,Chunlei He,Jianfeng Zhang,Dexing Kong,Shoujun Huang*

Main category: cs.CV

TL;DR: 论文提出了一种基于双曲平均曲率流的主动轮廓模型（HMCF-ACMs）及其改进版本（HDRF-ACMs），通过可调初始速度场和边缘感知力调制，提升了图像分割的精度和抗噪性。


<details>
  <summary>Details</summary>
Motivation: 传统的抛物线平均曲率流驱动的主动轮廓模型（PMCF-ACMs）对初始曲线配置依赖性强，限制了其适应性。本文旨在通过引入双曲平均曲率流，解决这一问题。

Method: 提出HMCF-ACMs和HDRF-ACMs，利用可调初始速度场和边缘感知力调制，并通过加权四阶Runge-Kutta算法求解相关波动方程。

Result: 实验表明，HMCF-ACMs和HDRF-ACMs在噪声抑制和数值稳定性方面表现更优，分割精度更高。

Conclusion: 双曲平均曲率流驱动的主动轮廓模型通过任务自适应的初始配置，显著提升了图像分割的性能。

Abstract: Parabolic mean curvature flow-driven active contour models (PMCF-ACMs) are
widely used in image segmentation, which however depend heavily on the
selection of initial curve configurations. In this paper, we firstly propose
several hyperbolic mean curvature flow-driven ACMs (HMCF-ACMs), which introduce
tunable initial velocity fields, enabling adaptive optimization for diverse
segmentation scenarios. We shall prove that HMCF-ACMs are indeed normal flows
and establish the numerical equivalence between dissipative HMCF formulations
and certain wave equations using the level set method with signed distance
function. Building on this framework, we furthermore develop hyperbolic
dual-mode regularized flow-driven ACMs (HDRF-ACMs), which utilize smooth
Heaviside functions for edge-aware force modulation to suppress over-diffusion
near weak boundaries. Then, we optimize a weighted fourth-order Runge-Kutta
algorithm with nine-point stencil spatial discretization when solving the
above-mentioned wave equations. Experiments show that both HMCF-ACMs and
HDRF-ACMs could achieve more precise segmentations with superior noise
resistance and numerical stability due to task-adaptive configurations of
initial velocities and initial contours.

</details>


### [232] [Improving Wildlife Out-of-Distribution Detection: Africas Big Five](https://arxiv.org/abs/2506.06719)
*Mufhumudzi Muthivhi,Jiahao Huo,Fredrik Gustafsson,Terence L. van Zyl*

Main category: cs.CV

TL;DR: 论文研究了野生动物（特别是非洲五大动物）的分布外检测（OOD），比较了参数化与非参数化方法，并展示了特征方法的优越性。


<details>
  <summary>Details</summary>
Motivation: 解决人类与野生动物冲突需要准确识别潜在威胁个体，但现有分类模型在未知类别上表现不佳。

Method: 采用参数化的最近类均值（NCM）和非参数化的对比学习方法，结合预训练特征，并与常见OOD方法对比。

Result: 特征方法表现更优，NCM在AUPR-IN、AUPR-OUT和AUTC上分别提升2%、4%和22%。

Conclusion: 特征方法在野生动物OOD检测中具有更强的泛化能力。

Abstract: Mitigating human-wildlife conflict seeks to resolve unwanted encounters
between these parties. Computer Vision provides a solution to identifying
individuals that might escalate into conflict, such as members of the Big Five
African animals. However, environments often contain several varied species.
The current state-of-the-art animal classification models are trained under a
closed-world assumption. They almost always remain overconfident in their
predictions even when presented with unknown classes. This study investigates
out-of-distribution (OOD) detection of wildlife, specifically the Big Five. To
this end, we select a parametric Nearest Class Mean (NCM) and a non-parametric
contrastive learning approach as baselines to take advantage of pretrained and
projected features from popular classification encoders. Moreover, we compare
our baselines to various common OOD methods in the literature. The results show
feature-based methods reflect stronger generalisation capability across varying
classification thresholds. Specifically, NCM with ImageNet pre-trained features
achieves a 2%, 4% and 22% improvement on AUPR-IN, AUPR-OUT and AUTC over the
best OOD methods, respectively. The code can be found here
https://github.com/pxpana/BIG5OOD

</details>


### [233] [Mitigating Object Hallucination via Robust Local Perception Search](https://arxiv.org/abs/2506.06729)
*Zixian Gao,Chao Yang,Zhanhui Zhou,Xing Xu,Chaochao Lu*

Main category: cs.CV

TL;DR: 论文提出了一种名为LPS的解码方法，通过利用局部视觉先验信息抑制多模态大语言模型中的幻觉现象，无需额外训练且兼容多种模型。


<details>
  <summary>Details</summary>
Motivation: 尽管多模态大语言模型在视觉与语言融合方面取得了显著成功，但其输出仍存在与图像内容不符的幻觉现象，亟需解决。

Method: 提出了Local Perception Search (LPS)，一种在推理过程中利用局部视觉先验信息作为值函数来修正解码过程的训练免费方法。

Result: 在广泛使用的幻觉基准测试和噪声数据实验中，LPS显著减少了幻觉现象，尤其在噪声环境下表现优异。

Conclusion: LPS是一种简单、无需训练且兼容性强的解码方法，能有效抑制多模态大语言模型中的幻觉现象，尤其在噪声环境下效果显著。

Abstract: Recent advancements in Multimodal Large Language Models (MLLMs) have enabled
them to effectively integrate vision and language, addressing a variety of
downstream tasks. However, despite their significant success, these models
still exhibit hallucination phenomena, where the outputs appear plausible but
do not align with the content of the images. To mitigate this issue, we
introduce Local Perception Search (LPS), a decoding method during inference
that is both simple and training-free, yet effectively suppresses
hallucinations. This method leverages local visual prior information as a value
function to correct the decoding process. Additionally, we observe that the
impact of the local visual prior on model performance is more pronounced in
scenarios with high levels of image noise. Notably, LPS is a plug-and-play
approach that is compatible with various models. Extensive experiments on
widely used hallucination benchmarks and noisy data demonstrate that LPS
significantly reduces the incidence of hallucinations compared to the baseline,
showing exceptional performance, particularly in noisy settings.

</details>


### [234] [RecipeGen: A Step-Aligned Multimodal Benchmark for Real-World Recipe Generation](https://arxiv.org/abs/2506.06733)
*Ruoxuan Zhang,Jidong Gao,Bin Wen,Hongxia Xie,Chenming Zhang,Honghan-shuai,Wen-Huang Cheng*

Main category: cs.CV

TL;DR: RecipeGen是一个大规模、真实世界的基准数据集，用于食谱相关的文本到图像（T2I）、图像到视频（I2V）和文本到视频（T2V）生成任务。


<details>
  <summary>Details</summary>
Motivation: 现有数据集在食谱目标、分步指令和视觉内容之间缺乏细粒度对齐，限制了食品计算领域的发展。

Method: 提出了RecipeGen数据集，包含26,453个食谱、196,724张图片和4,491个视频，涵盖多样化的食材、烹饪步骤、风格和菜品类型。

Result: 提出了领域特定的评估指标，用于评估食材保真度和交互建模，并对代表性模型进行了基准测试。

Conclusion: RecipeGen为未来的食谱生成模型提供了有价值的见解和基准。

Abstract: Creating recipe images is a key challenge in food computing, with
applications in culinary education and multimodal recipe assistants. However,
existing datasets lack fine-grained alignment between recipe goals, step-wise
instructions, and visual content. We present RecipeGen, the first large-scale,
real-world benchmark for recipe-based Text-to-Image (T2I), Image-to-Video
(I2V), and Text-to-Video (T2V) generation. RecipeGen contains 26,453 recipes,
196,724 images, and 4,491 videos, covering diverse ingredients, cooking
procedures, styles, and dish types. We further propose domain-specific
evaluation metrics to assess ingredient fidelity and interaction modeling,
benchmark representative T2I, I2V, and T2V models, and provide insights for
future recipe generation models. Project page is available now.

</details>


### [235] [THU-Warwick Submission for EPIC-KITCHEN Challenge 2025: Semi-Supervised Video Object Segmentation](https://arxiv.org/abs/2506.06748)
*Mingqi Gao,Haoran Duan,Tianlu Zhang,Jungong Han*

Main category: cs.CV

TL;DR: 提出了一种结合视觉预训练和深度几何线索的自中心视频对象分割方法，性能优异。


<details>
  <summary>Details</summary>
Motivation: 处理复杂场景和长期跟踪的挑战。

Method: 结合SAM2的大规模视觉预训练和深度几何线索，统一框架。

Result: 在VISOR测试集上J&F得分90.1%。

Conclusion: 方法有效，性能显著。

Abstract: In this report, we describe our approach to egocentric video object
segmentation. Our method combines large-scale visual pretraining from SAM2 with
depth-based geometric cues to handle complex scenes and long-term tracking. By
integrating these signals in a unified framework, we achieve strong
segmentation performance. On the VISOR test set, our method reaches a J&F score
of 90.1%.

</details>


### [236] [SAR2Struct: Extracting 3D Semantic Structural Representation of Aircraft Targets from Single-View SAR Image](https://arxiv.org/abs/2506.06757)
*Ziyu Yue,Ruixi You,Feng Xu*

Main category: cs.CV

TL;DR: 提出了一种从单视角SAR图像恢复目标结构的新任务，通过两步算法框架实现3D语义结构推断。


<details>
  <summary>Details</summary>
Motivation: 现有方法忽视结构建模对语义信息的作用，本文旨在从SAR图像中直接提取目标的语义表示。

Method: 基于结构描述符的两步框架：训练阶段从真实SAR图像检测2D关键点，学习其到3D结构的映射；测试阶段整合这两步推断3D结构。

Result: 实验验证了方法的有效性，首次实现从单视角SAR图像直接推断飞机目标的3D语义结构。

Conclusion: 该方法为SAR图像的高级信息检索提供了新思路，展示了结构建模在语义理解中的潜力。

Abstract: To translate synthetic aperture radar (SAR) image into interpretable forms
for human understanding is the ultimate goal of SAR advanced information
retrieval. Existing methods mainly focus on 3D surface reconstruction or local
geometric feature extraction of targets, neglecting the role of structural
modeling in capturing semantic information. This paper proposes a novel task:
SAR target structure recovery, which aims to infer the components of a target
and the structural relationships between its components, specifically symmetry
and adjacency, from a single-view SAR image. Through learning the structural
consistency and geometric diversity across the same type of targets as observed
in different SAR images, it aims to derive the semantic representation of
target directly from its 2D SAR image. To solve this challenging task, a
two-step algorithmic framework based on structural descriptors is developed.
Specifically, in the training phase, it first detects 2D keypoints from real
SAR images, and then learns the mapping from these keypoints to 3D hierarchical
structures using simulated data. During the testing phase, these two steps are
integrated to infer the 3D structure from real SAR images. Experimental results
validated the effectiveness of each step and demonstrated, for the first time,
that 3D semantic structural representation of aircraft targets can be directly
derived from a single-view SAR image.

</details>


### [237] [LitMAS: A Lightweight and Generalized Multi-Modal Anti-Spoofing Framework for Biometric Security](https://arxiv.org/abs/2506.06759)
*Nidheesh Gorthi,Kartik Thakral,Rishabh Ranjan,Richa Singh,Mayank Vatsa*

Main category: cs.CV

TL;DR: LitMAS是一个轻量级、通用的多模态反欺骗框架，用于检测语音、人脸、虹膜和指纹生物识别系统中的欺骗攻击。


<details>
  <summary>Details</summary>
Motivation: 生物识别认证系统在关键应用中广泛部署，但仍易受欺骗攻击。现有研究多关注特定模态的反欺骗技术，缺乏跨模态的统一高效解决方案。

Method: 提出LitMAS框架，采用模态对齐集中损失（Modality-Aligned Concentration Loss），增强类间分离性并保持跨模态一致性。

Result: LitMAS仅需6M参数，在七个数据集上的平均EER比现有最优方法提升1.36%，表现出高效性和强泛化能力。

Conclusion: LitMAS为边缘部署提供了高效、通用的多模态反欺骗解决方案。

Abstract: Biometric authentication systems are increasingly being deployed in critical
applications, but they remain susceptible to spoofing. Since most of the
research efforts focus on modality-specific anti-spoofing techniques, building
a unified, resource-efficient solution across multiple biometric modalities
remains a challenge. To address this, we propose LitMAS, a
$\textbf{Li}$gh$\textbf{t}$ weight and generalizable $\textbf{M}$ulti-modal
$\textbf{A}$nti-$\textbf{S}$poofing framework designed to detect spoofing
attacks in speech, face, iris, and fingerprint-based biometric systems. At the
core of LitMAS is a Modality-Aligned Concentration Loss, which enhances
inter-class separability while preserving cross-modal consistency and enabling
robust spoof detection across diverse biometric traits. With just 6M
parameters, LitMAS surpasses state-of-the-art methods by $1.36\%$ in average
EER across seven datasets, demonstrating high efficiency, strong
generalizability, and suitability for edge deployment. Code and trained models
are available at https://github.com/IAB-IITJ/LitMAS.

</details>


### [238] [LoopDB: A Loop Closure Dataset for Large Scale Simultaneous Localization and Mapping](https://arxiv.org/abs/2506.06771)
*Mohammad-Maher Nakshbandi,Ziad Sharawy,Dorian Cojocaru,Sorin Grigorescu*

Main category: cs.CV

TL;DR: LoopDB是一个包含1000多张多样化环境图像的闭环数据集，用于测试和训练闭环算法。


<details>
  <summary>Details</summary>
Motivation: 为同时定位与地图构建（SLAM）中的闭环算法提供基准测试和训练数据。

Method: 使用高分辨率相机采集图像，每场景包含五张连续图像，并提供旋转和平移的真实数据。

Result: 数据集公开可用，适用于算法基准测试和深度学习训练。

Conclusion: LoopDB是一个有价值的资源，支持闭环算法的研究和开发。

Abstract: In this study, we introduce LoopDB, which is a challenging loop closure
dataset comprising over 1000 images captured across diverse environments,
including parks, indoor scenes, parking spaces, as well as centered around
individual objects. Each scene is represented by a sequence of five consecutive
images. The dataset was collected using a high resolution camera, providing
suitable imagery for benchmarking the accuracy of loop closure algorithms,
typically used in simultaneous localization and mapping. As ground truth
information, we provide computed rotations and translations between each
consecutive images. Additional to its benchmarking goal, the dataset can be
used to train and fine-tune loop closure methods based on deep neural networks.
LoopDB is publicly available at https://github.com/RovisLab/LoopDB.

</details>


### [239] [Continuous-Time SO(3) Forecasting with Savitzky--Golay Neural Controlled Differential Equations](https://arxiv.org/abs/2506.06780)
*Lennart Bastian,Mohammad Rashed,Nassir Navab,Tolga Birdal*

Main category: cs.CV

TL;DR: 论文提出了一种基于神经控制微分方程和Savitzky-Golay路径的SO(3)旋转物体动态建模方法，解决了噪声、稀疏观测和复杂动态的挑战。


<details>
  <summary>Details</summary>
Motivation: SO(3)旋转外推在计算机视觉和机器人学中很重要，但面临噪声观测、复杂动态和长时预测需求等挑战。

Method: 使用神经控制微分方程和Savitzky-Golay路径建模连续时间旋转动态，学习潜在动力学系统并保持旋转几何结构。

Result: 在真实数据实验中，相比现有方法展现出更强的预测能力。

Conclusion: 该方法能有效建模旋转动态，适用于复杂场景的长时预测。

Abstract: Tracking and forecasting the rotation of objects is fundamental in computer
vision and robotics, yet SO(3) extrapolation remains challenging as (1) sensor
observations can be noisy and sparse, (2) motion patterns can be governed by
complex dynamics, and (3) application settings can demand long-term
forecasting. This work proposes modeling continuous-time rotational object
dynamics on $SO(3)$ using Neural Controlled Differential Equations guided by
Savitzky-Golay paths. Unlike existing methods that rely on simplified motion
assumptions, our method learns a general latent dynamical system of the
underlying object trajectory while respecting the geometric structure of
rotations. Experimental results on real-world data demonstrate compelling
forecasting capabilities compared to existing approaches.

</details>


### [240] [Training-Free Identity Preservation in Stylized Image Generation Using Diffusion Models](https://arxiv.org/abs/2506.06802)
*Mohammad Ali Rezaei,Helia Hajikazem,Saeed Khanehgir,Mahdi Javanmardi*

Main category: cs.CV

TL;DR: 提出了一种基于扩散模型的无训练框架，用于身份保留的风格化图像合成，解决了现有方法在复杂场景中身份保留不足的问题。


<details>
  <summary>Details</summary>
Motivation: 现有风格迁移技术在保持身份的同时实现高质量风格化方面存在困难，尤其是在面部区域小或相机距离远的情况下。

Method: 提出了两种关键技术：1) "Mosaic Restored Content Image" 增强身份保留；2) 无训练的内容一致性损失，提升细节保留。

Result: 实验表明，该方法在保持高风格保真度和身份完整性方面显著优于基线模型，尤其在复杂场景下。

Conclusion: 该框架无需重新训练或微调，即可在复杂条件下实现高质量风格化与身份保留。

Abstract: While diffusion models have demonstrated remarkable generative capabilities,
existing style transfer techniques often struggle to maintain identity while
achieving high-quality stylization. This limitation is particularly acute for
images where faces are small or exhibit significant camera-to-face distances,
frequently leading to inadequate identity preservation. To address this, we
introduce a novel, training-free framework for identity-preserved stylized
image synthesis using diffusion models. Key contributions include: (1) the
"Mosaic Restored Content Image" technique, significantly enhancing identity
retention, especially in complex scenes; and (2) a training-free content
consistency loss that enhances the preservation of fine-grained content details
by directing more attention to the original image during stylization. Our
experiments reveal that the proposed approach substantially surpasses the
baseline model in concurrently maintaining high stylistic fidelity and robust
identity integrity, particularly under conditions of small facial regions or
significant camera-to-face distances, all without necessitating model
retraining or fine-tuning.

</details>


### [241] [Stepwise Decomposition and Dual-stream Focus: A Novel Approach for Training-free Camouflaged Object Segmentation](https://arxiv.org/abs/2506.06818)
*Chao Yin,Hao Li,Kequan Yang,Jide Li,Pinpin Zhu,Xiaoqiang Li*

Main category: cs.CV

TL;DR: RDVP-MSD是一种无需训练的自适应框架，通过多模态逐步分解思维链（MSD-CoT）和区域约束双流视觉提示（RDVP），解决了伪装物体分割（COS）中的语义模糊和空间分离问题，实现了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 当前的任务通用提示分割方法在伪装物体分割中面临语义模糊和空间分离问题，导致分割不准确。

Method: 提出RDVP-MSD框架，结合MSD-CoT逐步分解图像描述以消除语义模糊，RDVP则通过空间约束和独立采样视觉提示来缓解空间分离问题。

Result: 在多个COS基准测试中达到最先进的分割性能，且推理速度更快。

Conclusion: RDVP-MSD无需训练即可显著提升分割精度和效率，为COS任务提供了有效的解决方案。

Abstract: While promptable segmentation (\textit{e.g.}, SAM) has shown promise for
various segmentation tasks, it still requires manual visual prompts for each
object to be segmented. In contrast, task-generic promptable segmentation aims
to reduce the need for such detailed prompts by employing only a task-generic
prompt to guide segmentation across all test samples. However, when applied to
Camouflaged Object Segmentation (COS), current methods still face two critical
issues: 1) \textit{\textbf{semantic ambiguity in getting instance-specific text
prompts}}, which arises from insufficient discriminative cues in holistic
captions, leading to foreground-background confusion; 2)
\textit{\textbf{semantic discrepancy combined with spatial separation in
getting instance-specific visual prompts}}, which results from global
background sampling far from object boundaries with low feature correlation,
causing SAM to segment irrelevant regions. To address the issues above, we
propose \textbf{RDVP-MSD}, a novel training-free test-time adaptation framework
that synergizes \textbf{R}egion-constrained \textbf{D}ual-stream
\textbf{V}isual \textbf{P}rompting (RDVP) via \textbf{M}ultimodal
\textbf{S}tepwise \textbf{D}ecomposition Chain of Thought (MSD-CoT). MSD-CoT
progressively disentangles image captions to eliminate semantic ambiguity,
while RDVP injects spatial constraints into visual prompting and independently
samples visual prompts for foreground and background points, effectively
mitigating semantic discrepancy and spatial separation. Without requiring any
training or supervision, RDVP-MSD achieves a state-of-the-art segmentation
result on multiple COS benchmarks and delivers a faster inference speed than
previous methods, demonstrating significantly improved accuracy and efficiency.
The codes will be available at
\href{https://github.com/ycyinchao/RDVP-MSD}{https://github.com/ycyinchao/RDVP-MSD}

</details>


### [242] [Hi-LSplat: Hierarchical 3D Language Gaussian Splatting](https://arxiv.org/abs/2506.06822)
*Chenlu Zhan,Yufei Zhang,Gaoang Wang,Hongwei Wang*

Main category: cs.CV

TL;DR: Hi-LSplat提出了一种基于3D高斯泼溅的层次化语言模型，解决了现有方法在视图一致性和开放词汇查询中的问题。


<details>
  <summary>Details</summary>
Motivation: 现有3DGS模型依赖2D基础模型，导致视图不一致和层次语义理解不足。

Method: 通过构建3D层次语义树和引入对比损失，提升3D特征的视图一致性和层次语义表示。

Result: 实验表明，Hi-LSplat在开放词汇分割和定位任务中表现优异，并能捕捉复杂层次语义。

Conclusion: Hi-LSplat为3D开放词汇查询提供了一种视图一致且层次化的解决方案。

Abstract: Modeling 3D language fields with Gaussian Splatting for open-ended language
queries has recently garnered increasing attention. However, recent 3DGS-based
models leverage view-dependent 2D foundation models to refine 3D semantics but
lack a unified 3D representation, leading to view inconsistencies.
Additionally, inherent open-vocabulary challenges cause inconsistencies in
object and relational descriptions, impeding hierarchical semantic
understanding. In this paper, we propose Hi-LSplat, a view-consistent
Hierarchical Language Gaussian Splatting work for 3D open-vocabulary querying.
To achieve view-consistent 3D hierarchical semantics, we first lift 2D features
to 3D features by constructing a 3D hierarchical semantic tree with layered
instance clustering, which addresses the view inconsistency issue caused by 2D
semantic features. Besides, we introduce instance-wise and part-wise
contrastive losses to capture all-sided hierarchical semantic representations.
Notably, we construct two hierarchical semantic datasets to better assess the
model's ability to distinguish different semantic levels. Extensive experiments
highlight our method's superiority in 3D open-vocabulary segmentation and
localization. Its strong performance on hierarchical semantic datasets
underscores its ability to capture complex hierarchical semantics within 3D
scenes.

</details>


### [243] [Exploring Visual Prompting: Robustness Inheritance and Beyond](https://arxiv.org/abs/2506.06823)
*Qi Li,Liangzhi Li,Zhouqiang Jiang,Bowen Wang,Keke Tang*

Main category: cs.CV

TL;DR: 本文探讨了视觉提示（VP）在鲁棒源模型下的表现，提出了一种名为Prompt Boundary Loosening（PBL）的策略，以缓解VP在鲁棒性和泛化能力之间的权衡问题。


<details>
  <summary>Details</summary>
Motivation: 研究VP在鲁棒源模型下的表现，探索其是否能继承源模型的鲁棒性，以及是否存在鲁棒性与泛化能力的权衡问题。

Method: 提出了一种轻量级、即插即用的策略PBL，用于缓解VP的权衡问题。

Result: 实验表明，PBL能有效继承源模型的鲁棒性，并显著提升VP在下游任务中的泛化能力。

Conclusion: PBL是一种通用且有效的策略，适用于VP在鲁棒源模型下的场景。

Abstract: Visual Prompting (VP), an efficient method for transfer learning, has shown
its potential in vision tasks. However, previous works focus exclusively on VP
from standard source models, it is still unknown how it performs under the
scenario of a robust source model: Can the robustness of the source model be
successfully inherited? Does VP also encounter the same trade-off between
robustness and generalization ability as the source model during this process?
If such a trade-off exists, is there a strategy specifically tailored to VP to
mitigate this limitation? In this paper, we thoroughly explore these three
questions for the first time and provide affirmative answers to them. To
mitigate the trade-off faced by VP, we propose a strategy called Prompt
Boundary Loosening (PBL). As a lightweight, plug-and-play strategy naturally
compatible with VP, PBL effectively ensures the successful inheritance of
robustness when the source model is a robust model, while significantly
enhancing VP's generalization ability across various downstream datasets.
Extensive experiments across various datasets show that our findings are
universal and demonstrate the significant benefits of the proposed strategy.

</details>


### [244] [Controllable Coupled Image Generation via Diffusion Models](https://arxiv.org/abs/2506.06826)
*Chenfei Yuan,Nanshan Jia,Hangqi Li,Peter W. Glynn,Zeyu Zheng*

Main category: cs.CV

TL;DR: 提出了一种注意力级别控制方法，用于耦合图像生成任务，确保背景相似的同时保持中心对象的灵活性。


<details>
  <summary>Details</summary>
Motivation: 解决多图像生成中背景耦合与对象灵活性的需求。

Method: 通过解耦背景和实体组件，并引入时间步相关的权重控制参数进行优化。

Result: 在背景耦合、文本对齐和视觉质量上优于现有方法。

Conclusion: 该方法有效平衡了背景一致性与对象多样性。

Abstract: We provide an attention-level control method for the task of coupled image
generation, where "coupled" means that multiple simultaneously generated images
are expected to have the same or very similar backgrounds. While backgrounds
coupled, the centered objects in the generated images are still expected to
enjoy the flexibility raised from different text prompts. The proposed method
disentangles the background and entity components in the model's
cross-attention modules, attached with a sequence of time-varying weight
control parameters depending on the time step of sampling. We optimize this
sequence of weight control parameters with a combined objective that assesses
how coupled the backgrounds are as well as text-to-image alignment and overall
visual quality. Empirical results demonstrate that our method outperforms
existing approaches across these criteria.

</details>


### [245] [EndoARSS: Adapting Spatially-Aware Foundation Model for Efficient Activity Recognition and Semantic Segmentation in Endoscopic Surgery](https://arxiv.org/abs/2506.06830)
*Guankun Wang,Rui Tang,Mengya Xu,Long Bai,Huxin Gao,Hongliang Ren*

Main category: cs.CV

TL;DR: EndoARSS是一个基于DINOv2的多任务学习框架，用于内窥镜手术活动识别和语义分割，通过低秩适应和空间感知多尺度注意力提升性能。


<details>
  <summary>Details</summary>
Motivation: 内窥镜手术场景复杂，传统深度学习模型在多任务中表现不佳，需解决跨任务干扰问题。

Method: 结合低秩适应和任务共享适配器，引入空间感知多尺度注意力机制，优化特征表示。

Result: 在多个基准测试中表现优异，显著提升准确性和鲁棒性。

Conclusion: EndoARSS有望推动AI驱动的内窥镜手术系统发展，提升手术安全性和效率。

Abstract: Endoscopic surgery is the gold standard for robotic-assisted minimally
invasive surgery, offering significant advantages in early disease detection
and precise interventions. However, the complexity of surgical scenes,
characterized by high variability in different surgical activity scenarios and
confused image features between targets and the background, presents challenges
for surgical environment understanding. Traditional deep learning models often
struggle with cross-activity interference, leading to suboptimal performance in
each downstream task. To address this limitation, we explore multi-task
learning, which utilizes the interrelated features between tasks to enhance
overall task performance. In this paper, we propose EndoARSS, a novel
multi-task learning framework specifically designed for endoscopy surgery
activity recognition and semantic segmentation. Built upon the DINOv2
foundation model, our approach integrates Low-Rank Adaptation to facilitate
efficient fine-tuning while incorporating Task Efficient Shared Low-Rank
Adapters to mitigate gradient conflicts across diverse tasks. Additionally, we
introduce the Spatially-Aware Multi-Scale Attention that enhances feature
representation discrimination by enabling cross-spatial learning of global
information. In order to evaluate the effectiveness of our framework, we
present three novel datasets, MTLESD, MTLEndovis and MTLEndovis-Gen, tailored
for endoscopic surgery scenarios with detailed annotations for both activity
recognition and semantic segmentation tasks. Extensive experiments demonstrate
that EndoARSS achieves remarkable performance across multiple benchmarks,
significantly improving both accuracy and robustness in comparison to existing
models. These results underscore the potential of EndoARSS to advance AI-driven
endoscopic surgical systems, offering valuable insights for enhancing surgical
safety and efficiency.

</details>


### [246] [Harnessing Vision-Language Models for Time Series Anomaly Detection](https://arxiv.org/abs/2506.06836)
*Zelin He,Sarah Alnegheimish,Matthew Reimherr*

Main category: cs.CV

TL;DR: 论文提出了一种基于视觉语言模型（VLM）的两阶段时间序列异常检测方法，显著提升了检测精度和效率。


<details>
  <summary>Details</summary>
Motivation: 现有方法缺乏视觉-时间推理能力，无法像人类专家一样识别上下文异常，因此探索了基于VLM的解决方案。

Method: 提出两阶段方法：ViT4TS（轻量级视觉编码器定位候选异常）和VLM4TS（结合全局时间上下文和VLM推理能力优化检测）。

Result: VLM4TS在未进行时间序列训练的情况下，F1-max得分比最佳基线提升24.6%，且效率更高。

Conclusion: 该方法在精度和效率上均优于现有方法，展示了VLM在时间序列异常检测中的潜力。

Abstract: Time-series anomaly detection (TSAD) has played a vital role in a variety of
fields, including healthcare, finance, and industrial monitoring. Prior
methods, which mainly focus on training domain-specific models on numerical
data, lack the visual-temporal reasoning capacity that human experts have to
identify contextual anomalies. To fill this gap, we explore a solution based on
vision language models (VLMs). Recent studies have shown the ability of VLMs
for visual reasoning tasks, yet their direct application to time series has
fallen short on both accuracy and efficiency. To harness the power of VLMs for
TSAD, we propose a two-stage solution, with (1) ViT4TS, a vision-screening
stage built on a relatively lightweight pretrained vision encoder, which
leverages 2-D time-series representations to accurately localize candidate
anomalies; (2) VLM4TS, a VLM-based stage that integrates global temporal
context and VLM reasoning capacity to refine the detection upon the candidates
provided by ViT4TS. We show that without any time-series training, VLM4TS
outperforms time-series pretrained and from-scratch baselines in most cases,
yielding a 24.6 percent improvement in F1-max score over the best baseline.
Moreover, VLM4TS also consistently outperforms existing language-model-based
TSAD methods and is on average 36 times more efficient in token usage.

</details>


### [247] [Multi-StyleGS: Stylizing Gaussian Splatting with Multiple Styles](https://arxiv.org/abs/2506.06846)
*Yangkai Lin,Jiabao Lei,Kui jia*

Main category: cs.CV

TL;DR: 提出了一种名为Multi-StyleGS的新方法，用于3D高斯泼溅（GS）的多风格化，通过自动局部风格转移或手动指定，同时保持内存高效训练。


<details>
  <summary>Details</summary>
Motivation: 解决3D GS在风格化时难以匹配多风格、保持内存效率的问题。

Method: 采用二分匹配机制自动匹配风格图像与渲染图像的局部区域，引入语义风格损失函数和局部-全局特征匹配，优化分割网络以分配语义标签。

Result: 实验表明，该方法在风格化效果、灵活性和内存效率上优于现有方法。

Conclusion: Multi-StyleGS能够高效实现3D GS的多风格化，提供更真实的风格化结果和灵活的编辑能力。

Abstract: In recent years, there has been a growing demand to stylize a given 3D scene
to align with the artistic style of reference images for creative purposes.
While 3D Gaussian Splatting(GS) has emerged as a promising and efficient method
for realistic 3D scene modeling, there remains a challenge in adapting it to
stylize 3D GS to match with multiple styles through automatic local style
transfer or manual designation, while maintaining memory efficiency for
stylization training. In this paper, we introduce a novel 3D GS stylization
solution termed Multi-StyleGS to tackle these challenges. In particular, we
employ a bipartite matching mechanism to au tomatically identify
correspondences between the style images and the local regions of the rendered
images. To facilitate local style transfer, we introduce a novel semantic style
loss function that employs a segmentation network to apply distinct styles to
various objects of the scene and propose a local-global feature matching to
enhance the multi-view consistency. Furthermore, this technique can achieve
memory efficient training, more texture details and better color match. To
better assign a robust semantic label to each Gaussian, we propose several
techniques to regularize the segmentation network. As demonstrated by our
comprehensive experiments, our approach outperforms existing ones in producing
plausible stylization results and offering flexible editing.

</details>


### [248] [Deep Inertial Pose: A deep learning approach for human pose estimation](https://arxiv.org/abs/2506.06850)
*Sara M. Cerqueira,Manuel Palermo,Cristina P. Santos*

Main category: cs.CV

TL;DR: 论文研究了基于神经网络的惯性运动捕捉系统，比较了不同架构和方法，发现Hybrid LSTM-Madgwick方法在姿态估计中表现最佳。


<details>
  <summary>Details</summary>
Motivation: 传统惯性运动捕捉系统依赖复杂生物力学模型和昂贵软件，本研究旨在通过神经网络简化这一过程。

Method: 比较了不同神经网络架构和方法，使用低成本和高端的MARG传感器进行姿态估计。

Result: Hybrid LSTM-Madgwick方法在Mtw Awinda数据上实现了7.96的四元数角度距离误差。

Conclusion: 神经网络可以用于姿态估计，效果与最先进的融合滤波器相当。

Abstract: Inertial-based Motion capture system has been attracting growing attention
due to its wearability and unsconstrained use. However, accurate human joint
estimation demands several complex and expertise demanding steps, which leads
to expensive software such as the state-of-the-art MVN Awinda from Xsens
Technologies. This work aims to study the use of Neural Networks to abstract
the complex biomechanical models and analytical mathematics required for pose
estimation. Thus, it presents a comparison of different Neural Network
architectures and methodologies to understand how accurately these methods can
estimate human pose, using both low cost(MPU9250) and high end (Mtw Awinda)
Magnetic, Angular Rate, and Gravity (MARG) sensors. The most efficient method
was the Hybrid LSTM-Madgwick detached, which achieved an Quaternion Angle
distance error of 7.96, using Mtw Awinda data. Also, an ablation study was
conducted to study the impact of data augmentation, output representation,
window size, loss function and magnetometer data on the pose estimation error.
This work indicates that Neural Networks can be trained to estimate human pose,
with results comparable to the state-of-the-art fusion filters.

</details>


### [249] [Position Prediction Self-Supervised Learning for Multimodal Satellite Imagery Semantic Segmentation](https://arxiv.org/abs/2506.06852)
*John Waithaka,Moise Busogi*

Main category: cs.CV

TL;DR: 论文提出了一种基于位置预测的自监督学习方法（LOCA），用于多模态卫星图像的语义分割，显著优于现有的基于重建的自监督方法。


<details>
  <summary>Details</summary>
Motivation: 卫星图像的语义分割对地球观测至关重要，但受限于标记数据的不足。现有自监督方法（如MAE）侧重于重建而非定位，而定位是分割任务的基础。

Method: 扩展SatMAE的通道分组至多模态数据，引入同组注意力掩码以促进跨模态交互，采用相对块位置预测任务以增强空间推理能力。

Result: 在Sen1Floods11洪水映射数据集上，该方法显著优于基于重建的自监督学习方法。

Conclusion: 针对多模态卫星图像的位置预测任务能学习到更有效的表示，优于基于重建的方法。

Abstract: Semantic segmentation of satellite imagery is crucial for Earth observation
applications, but remains constrained by limited labelled training data. While
self-supervised pretraining methods like Masked Autoencoders (MAE) have shown
promise, they focus on reconstruction rather than localisation-a fundamental
aspect of segmentation tasks. We propose adapting LOCA (Location-aware), a
position prediction self-supervised learning method, for multimodal satellite
imagery semantic segmentation. Our approach addresses the unique challenges of
satellite data by extending SatMAE's channel grouping from multispectral to
multimodal data, enabling effective handling of multiple modalities, and
introducing same-group attention masking to encourage cross-modal interaction
during pretraining. The method uses relative patch position prediction,
encouraging spatial reasoning for localisation rather than reconstruction. We
evaluate our approach on the Sen1Floods11 flood mapping dataset, where it
significantly outperforms existing reconstruction-based self-supervised
learning methods for satellite imagery. Our results demonstrate that position
prediction tasks, when properly adapted for multimodal satellite imagery, learn
representations more effective for satellite image semantic segmentation than
reconstruction-based approaches.

</details>


### [250] [DONUT: A Decoder-Only Model for Trajectory Prediction](https://arxiv.org/abs/2506.06854)
*Markus Knoche,Daan de Geus,Bastian Leibe*

Main category: cs.CV

TL;DR: 提出了一种仅解码器网络DONUT，用于预测轨迹，优于现有编码器-解码器模型，并在Argoverse 2基准测试中取得最佳结果。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶需要预测其他代理的运动，现有编码器-解码器模型存在信息滞后问题。

Method: 使用单一自回归模型迭代预测轨迹，并引入‘过预测’策略以提升性能。

Result: 在Argoverse 2单代理运动预测基准测试中表现优于基线模型，达到最新技术水平。

Conclusion: 仅解码器方法在轨迹预测中更高效且性能更优。

Abstract: Predicting the motion of other agents in a scene is highly relevant for
autonomous driving, as it allows a self-driving car to anticipate. Inspired by
the success of decoder-only models for language modeling, we propose DONUT, a
Decoder-Only Network for Unrolling Trajectories. Different from existing
encoder-decoder forecasting models, we encode historical trajectories and
predict future trajectories with a single autoregressive model. This allows the
model to make iterative predictions in a consistent manner, and ensures that
the model is always provided with up-to-date information, enhancing the
performance. Furthermore, inspired by multi-token prediction for language
modeling, we introduce an 'overprediction' strategy that gives the network the
auxiliary task of predicting trajectories at longer temporal horizons. This
allows the model to better anticipate the future, and further improves the
performance. With experiments, we demonstrate that our decoder-only approach
outperforms the encoder-decoder baseline, and achieves new state-of-the-art
results on the Argoverse 2 single-agent motion forecasting benchmark.

</details>


### [251] [Vision-EKIPL: External Knowledge-Infused Policy Learning for Visual Reasoning](https://arxiv.org/abs/2506.06856)
*Chaoyang Wang,Zeyu Zhang,Haiyun Jiang*

Main category: cs.CV

TL;DR: 论文提出了一种名为Vision-EKIPL的新型强化学习框架，通过引入外部辅助模型生成的高质量动作来优化策略模型，显著提升了多模态大语言模型的视觉推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有强化学习方法仅从策略模型本身采样动作组，限制了模型的推理能力上限并导致训练效率低下。

Method: 提出Vision-EKIPL框架，在强化学习训练过程中引入外部辅助模型生成的高质量动作，指导策略模型优化。

Result: 在Reason-RFT-CoT Benchmark上性能提升5%，显著加速训练收敛速度。

Conclusion: Vision-EKIPL克服了传统强化学习方法的局限性，为多模态大语言模型的视觉推理研究提供了新范式。

Abstract: Visual reasoning is crucial for understanding complex multimodal data and
advancing Artificial General Intelligence. Existing methods enhance the
reasoning capability of Multimodal Large Language Models (MLLMs) through
Reinforcement Learning (RL) fine-tuning (e.g., GRPO). However, current RL
approaches sample action groups solely from the policy model itself, which
limits the upper boundary of the model's reasoning capability and leads to
inefficient training. To address these limitations, this paper proposes a novel
RL framework called \textbf{Vision-EKIPL}. The core of this framework lies in
introducing high-quality actions generated by external auxiliary models during
the RL training process to guide the optimization of the policy model. The
policy learning with knowledge infusion from external models significantly
expands the model's exploration space, effectively improves the reasoning
boundary, and substantially accelerates training convergence speed and
efficiency. Experimental results demonstrate that our proposed Vision-EKIPL
achieved up to a 5\% performance improvement on the Reason-RFT-CoT Benchmark
compared to the state-of-the-art (SOTA). It reveals that Vision-EKIPL can
overcome the limitations of traditional RL methods, significantly enhance the
visual reasoning performance of MLLMs, and provide a new effective paradigm for
research in this field.

</details>


### [252] [Face recognition on point cloud with cgan-top for denoising](https://arxiv.org/abs/2506.06864)
*Junyu Liu,Jianfeng Ren,Sunhong Liang,Xudong Jiang*

Main category: cs.CV

TL;DR: 提出了一种端到端的3D人脸识别方法，结合去噪和识别模块，显著提高了噪声点云下的识别精度。


<details>
  <summary>Details</summary>
Motivation: 原始点云常因传感器不完善而包含大量噪声，影响识别效果。

Method: 设计了基于三正交平面的条件生成对抗网络（cGAN-TOP）去噪，并采用链接动态图卷积神经网络（LDGCNN）进行多尺度特征融合识别。

Result: 在Bosphorus数据集上验证，所有噪声设置下识别精度显著提升，最高增益达14.81%。

Conclusion: 该方法有效解决了噪声点云下的3D人脸识别问题，具有实际应用潜力。

Abstract: Face recognition using 3D point clouds is gaining growing interest, while raw
point clouds often contain a significant amount of noise due to imperfect
sensors. In this paper, an end-to-end 3D face recognition on a noisy point
cloud is proposed, which synergistically integrates the denoising and
recognition modules. Specifically, a Conditional Generative Adversarial Network
on Three Orthogonal Planes (cGAN-TOP) is designed to effectively remove the
noise in the point cloud, and recover the underlying features for subsequent
recognition. A Linked Dynamic Graph Convolutional Neural Network (LDGCNN) is
then adapted to recognize faces from the processed point cloud, which
hierarchically links both the local point features and neighboring features of
multiple scales. The proposed method is validated on the Bosphorus dataset. It
significantly improves the recognition accuracy under all noise settings, with
a maximum gain of 14.81%.

</details>


### [253] [Hybrid Vision Transformer-Mamba Framework for Autism Diagnosis via Eye-Tracking Analysis](https://arxiv.org/abs/2506.06886)
*Wafaa Kasri,Yassine Himeur,Abigail Copiaco,Wathiq Mansoor,Ammar Albanna,Valsamma Eapen*

Main category: cs.CV

TL;DR: 提出了一种结合Vision Transformers和Vision Mamba的混合深度学习框架，用于通过眼动数据检测自闭症谱系障碍（ASD），性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 早期诊断ASD对干预至关重要，但传统方法依赖手工特征且缺乏透明性，需要更准确和可解释的解决方案。

Method: 采用Vision Transformers和Vision Mamba的混合框架，结合视觉、语音和面部线索的注意力融合，捕捉时空动态。

Result: 在Saliency4ASD数据集上，模型达到0.96准确率、0.95 F1分数、0.97灵敏度和0.94特异性。

Conclusion: 该模型在资源有限或远程临床环境中具有潜力，为ASD筛查提供了可扩展且可解释的解决方案。

Abstract: Accurate Autism Spectrum Disorder (ASD) diagnosis is vital for early
intervention. This study presents a hybrid deep learning framework combining
Vision Transformers (ViT) and Vision Mamba to detect ASD using eye-tracking
data. The model uses attention-based fusion to integrate visual, speech, and
facial cues, capturing both spatial and temporal dynamics. Unlike traditional
handcrafted methods, it applies state-of-the-art deep learning and explainable
AI techniques to enhance diagnostic accuracy and transparency. Tested on the
Saliency4ASD dataset, the proposed ViT-Mamba model outperformed existing
methods, achieving 0.96 accuracy, 0.95 F1-score, 0.97 sensitivity, and 0.94
specificity. These findings show the model's promise for scalable,
interpretable ASD screening, especially in resource-constrained or remote
clinical settings where access to expert diagnosis is limited.

</details>


### [254] [NSD-Imagery: A benchmark dataset for extending fMRI vision decoding methods to mental imagery](https://arxiv.org/abs/2506.06898)
*Reese Kneeland,Paul S. Scotti,Ghislain St-Yves,Jesse Breedlove,Kendrick Kay,Thomas Naselaris*

Main category: cs.CV

TL;DR: NSD-Imagery是一个新发布的基准数据集，用于评估从人类fMRI活动中重建心理图像的能力，补充了现有的NSD数据集。研究发现，现有模型在心理图像重建上的表现与视觉重建表现脱节，且简单架构模型表现更好。


<details>
  <summary>Details</summary>
Motivation: 为了评估和改进从fMRI活动中重建心理图像的模型性能，以支持医学和脑机接口等实际应用。

Method: 使用NSD-Imagery数据集，对多种NSD训练的视觉解码模型（如MindEye1、Brain Diffuser等）进行心理图像重建性能评估。

Result: 心理图像重建性能与视觉重建表现脱节；简单线性架构和多模态特征解码模型表现更好，复杂架构易过拟合。

Conclusion: 心理图像数据集对实际应用至关重要，NSD-Imagery为优化视觉解码方法提供了重要资源。

Abstract: We release NSD-Imagery, a benchmark dataset of human fMRI activity paired
with mental images, to complement the existing Natural Scenes Dataset (NSD), a
large-scale dataset of fMRI activity paired with seen images that enabled
unprecedented improvements in fMRI-to-image reconstruction efforts. Recent
models trained on NSD have been evaluated only on seen image reconstruction.
Using NSD-Imagery, it is possible to assess how well these models perform on
mental image reconstruction. This is a challenging generalization requirement
because mental images are encoded in human brain activity with relatively lower
signal-to-noise and spatial resolution; however, generalization from seen to
mental imagery is critical for real-world applications in medical domains and
brain-computer interfaces, where the desired information is always internally
generated. We provide benchmarks for a suite of recent NSD-trained open-source
visual decoding models (MindEye1, MindEye2, Brain Diffuser, iCNN, Takagi et
al.) on NSD-Imagery, and show that the performance of decoding methods on
mental images is largely decoupled from performance on vision reconstruction.
We further demonstrate that architectural choices significantly impact
cross-decoding performance: models employing simple linear decoding
architectures and multimodal feature decoding generalize better to mental
imagery, while complex architectures tend to overfit visual training data. Our
findings indicate that mental imagery datasets are critical for the development
of practical applications, and establish NSD-Imagery as a useful resource for
better aligning visual decoding methods with this goal.

</details>


### [255] [KNN-Defense: Defense against 3D Adversarial Point Clouds using Nearest-Neighbor Search](https://arxiv.org/abs/2506.06906)
*Nima Jamali,Matina Mahdizadeh Sani,Hanieh Naderi,Shohreh Kasaei*

Main category: cs.CV

TL;DR: 论文提出了一种名为KNN-Defense的防御策略，通过利用训练集中邻近样本的语义相似性来恢复受扰动的3D点云数据，显著提升了对抗攻击下的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 深度神经网络在3D点云数据分析中表现出色，但其对抗攻击的脆弱性（如点丢弃、移动和添加）威胁了3D视觉系统的可靠性，现有防御机制效果有限。

Method: KNN-Defense基于流形假设和特征空间的最近邻搜索，通过语义相似性恢复受扰动的输入，而非重建表面几何或强制均匀点分布。

Result: 在ModelNet40数据集上，KNN-Defense显著提升了对抗攻击下的鲁棒性，特别是在点丢弃攻击下，对PointNet等模型的准确率提升显著。

Conclusion: KNN-Defense是一种轻量级、计算高效且可扩展的解决方案，适用于实时应用，能有效增强3D点云分类器的对抗鲁棒性。

Abstract: Deep neural networks (DNNs) have demonstrated remarkable performance in
analyzing 3D point cloud data. However, their vulnerability to adversarial
attacks-such as point dropping, shifting, and adding-poses a critical challenge
to the reliability of 3D vision systems. These attacks can compromise the
semantic and structural integrity of point clouds, rendering many existing
defense mechanisms ineffective. To address this issue, a defense strategy named
KNN-Defense is proposed, grounded in the manifold assumption and
nearest-neighbor search in feature space. Instead of reconstructing surface
geometry or enforcing uniform point distributions, the method restores
perturbed inputs by leveraging the semantic similarity of neighboring samples
from the training set. KNN-Defense is lightweight and computationally
efficient, enabling fast inference and making it suitable for real-time and
practical applications. Empirical results on the ModelNet40 dataset
demonstrated that KNN-Defense significantly improves robustness across various
attack types. In particular, under point-dropping attacks-where many existing
methods underperform due to the targeted removal of critical points-the
proposed method achieves accuracy gains of 20.1%, 3.6%, 3.44%, and 7.74% on
PointNet, PointNet++, DGCNN, and PCT, respectively. These findings suggest that
KNN-Defense offers a scalable and effective solution for enhancing the
adversarial resilience of 3D point cloud classifiers. (An open-source
implementation of the method, including code and data, is available at
https://github.com/nimajam41/3d-knn-defense).

</details>


### [256] [Gaussian Mapping for Evolving Scenes](https://arxiv.org/abs/2506.06909)
*Vladimir Yugay,Thies Kersten,Luca Carlone,Theo Gevers,Martin R. Oswald,Lukas Schmid*

Main category: cs.CV

TL;DR: 论文提出了一种动态场景适应机制和关键帧管理机制，用于解决3D高斯泼溅系统中的长期动态场景问题。


<details>
  <summary>Details</summary>
Motivation: 当前3D高斯泼溅系统主要针对静态场景，对长期动态场景（如场景在视野外变化）的研究较少。

Method: 引入动态场景适应机制持续更新3D表示，并提出关键帧管理机制以保持几何和语义一致性。

Result: 在合成和真实数据集上评估，GaME方法比现有技术更准确。

Conclusion: GaME方法有效解决了长期动态场景的挑战，提升了3D高斯泼溅系统的性能。

Abstract: Mapping systems with novel view synthesis (NVS) capabilities are widely used
in computer vision, with augmented reality, robotics, and autonomous driving
applications. Most notably, 3D Gaussian Splatting-based systems show high NVS
performance; however, many current approaches are limited to static scenes.
While recent works have started addressing short-term dynamics (motion within
the view of the camera), long-term dynamics (the scene evolving through changes
out of view) remain less explored. To overcome this limitation, we introduce a
dynamic scene adaptation mechanism that continuously updates the 3D
representation to reflect the latest changes. In addition, since maintaining
geometric and semantic consistency remains challenging due to stale
observations disrupting the reconstruction process, we propose a novel keyframe
management mechanism that discards outdated observations while preserving as
much information as possible. We evaluate Gaussian Mapping for Evolving Scenes
(GaME) on both synthetic and real-world datasets and find it to be more
accurate than the state of the art.

</details>


### [257] [Sleep Stage Classification using Multimodal Embedding Fusion from EOG and PSM](https://arxiv.org/abs/2506.06912)
*Olivier Papillon,Rafik Goubran,James Green,Julien Larivière-Chartier,Caitlin Higginson,Frank Knoefel,Rébecca Robillard*

Main category: cs.CV

TL;DR: 研究提出了一种基于ImageBind的多模态嵌入模型，结合EOG和PSM数据用于睡眠阶段分类，显著提高了准确性。


<details>
  <summary>Details</summary>
Motivation: 传统PSG依赖EEG，复杂且不便家庭监测，因此探索EOG和PSM作为替代方案。

Method: 利用ImageBind模型整合PSM和双通道EOG信号，进行睡眠阶段分类。

Result: 模型在未微调时表现优异，微调后进一步超越现有单模态和多模态方法。

Conclusion: 预训练多模态模型可有效用于睡眠分类，接近EEG系统的准确性。

Abstract: Accurate sleep stage classification is essential for diagnosing sleep
disorders, particularly in aging populations. While traditional polysomnography
(PSG) relies on electroencephalography (EEG) as the gold standard, its
complexity and need for specialized equipment make home-based sleep monitoring
challenging. To address this limitation, we investigate the use of
electrooculography (EOG) and pressure-sensitive mats (PSM) as less obtrusive
alternatives for five-stage sleep-wake classification. This study introduces a
novel approach that leverages ImageBind, a multimodal embedding deep learning
model, to integrate PSM data with dual-channel EOG signals for sleep stage
classification. Our method is the first reported approach that fuses PSM and
EOG data for sleep stage classification with ImageBind. Our results demonstrate
that fine-tuning ImageBind significantly improves classification accuracy,
outperforming existing models based on single-channel EOG (DeepSleepNet),
exclusively PSM data (ViViT), and other multimodal deep learning approaches
(MBT). Notably, the model also achieved strong performance without fine-tuning,
highlighting its adaptability to specific tasks with limited labeled data,
making it particularly advantageous for medical applications. We evaluated our
method using 85 nights of patient recordings from a sleep clinic. Our findings
suggest that pre-trained multimodal embedding models, even those originally
developed for non-medical domains, can be effectively adapted for sleep
staging, with accuracies approaching systems that require complex EEG data.

</details>


### [258] [Reading in the Dark with Foveated Event Vision](https://arxiv.org/abs/2506.06918)
*Carl Brander,Giovanni Cioffi,Nico Messikommer,Davide Scaramuzza*

Main category: cs.CV

TL;DR: 提出一种基于事件的OCR方法，利用用户眼动注视减少带宽，适用于低光和高动态场景。


<details>
  <summary>Details</summary>
Motivation: 解决智能眼镜在低光和高动态场景下因运动模糊和带宽问题导致的文本识别困难。

Method: 结合眼动注视聚焦事件流，利用合成数据训练深度二元重建模型，并整合多模态LLM进行OCR。

Result: 在低光环境下优于传统OCR方法，带宽消耗仅为穿戴式RGB相机的1/2400。

Conclusion: 该方法显著提升了智能眼镜在复杂场景下的文本识别能力，同时大幅降低带宽需求。

Abstract: Current smart glasses equipped with RGB cameras struggle to perceive the
environment in low-light and high-speed motion scenarios due to motion blur and
the limited dynamic range of frame cameras. Additionally, capturing dense
images with a frame camera requires large bandwidth and power consumption,
consequently draining the battery faster. These challenges are especially
relevant for developing algorithms that can read text from images. In this
work, we propose a novel event-based Optical Character Recognition (OCR)
approach for smart glasses. By using the eye gaze of the user, we foveate the
event stream to significantly reduce bandwidth by around 98% while exploiting
the benefits of event cameras in high-dynamic and fast scenes. Our proposed
method performs deep binary reconstruction trained on synthetic data and
leverages multimodal LLMs for OCR, outperforming traditional OCR solutions. Our
results demonstrate the ability to read text in low light environments where
RGB cameras struggle while using up to 2400 times less bandwidth than a
wearable RGB camera.

</details>


### [259] [How Important are Videos for Training Video LLMs?](https://arxiv.org/abs/2506.06928)
*George Lydakis,Alexander Hermans,Ali Athar,Daan de Geus,Bastian Leibe*

Main category: cs.CV

TL;DR: 研究发现，仅通过图像训练的Video LLMs在时间推理能力上表现优于预期，而视频特定训练的改进效果较小。


<details>
  <summary>Details</summary>
Motivation: 探讨Video LLMs在时间推理上的表现，以及当前视频训练方法的效率问题。

Method: 使用LongVU算法训练的两个LLMs在TVBench上进行测试，并引入基于图像序列的简单微调方案。

Result: 图像训练的LLMs在时间推理上表现显著高于随机水平，且微调方案效果接近或优于视频训练的LLMs。

Conclusion: 当前视频训练方法未能充分利用视频的丰富时间特征，需进一步研究图像训练LLMs的时间推理机制及视频训练的瓶颈。

Abstract: Research into Video Large Language Models (LLMs) has progressed rapidly, with
numerous models and benchmarks emerging in just a few years. Typically, these
models are initialized with a pretrained text-only LLM and finetuned on both
image- and video-caption datasets. In this paper, we present findings
indicating that Video LLMs are more capable of temporal reasoning after
image-only training than one would assume, and that improvements from
video-specific training are surprisingly small. Specifically, we show that
image-trained versions of two LLMs trained with the recent LongVU algorithm
perform significantly above chance level on TVBench, a temporal reasoning
benchmark. Additionally, we introduce a simple finetuning scheme involving
sequences of annotated images and questions targeting temporal capabilities.
This baseline results in temporal reasoning performance close to, and
occasionally higher than, what is achieved by video-trained LLMs. This suggests
suboptimal utilization of rich temporal features found in real video by current
models. Our analysis motivates further research into the mechanisms that allow
image-trained LLMs to perform temporal reasoning, as well as into the
bottlenecks that render current video training schemes inefficient.

</details>


### [260] [Polar Hierarchical Mamba: Towards Streaming LiDAR Object Detection with Point Clouds as Egocentric Sequences](https://arxiv.org/abs/2506.06944)
*Mellon M. Zhang,Glen Chou,Saibal Mukhopadhyay*

Main category: cs.CV

TL;DR: PHiM是一种新型SSM架构，专为极坐标流式LiDAR设计，通过局部双向Mamba块和全局前向Mamba块实现高效检测，性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶需要低延迟、高吞吐的实时感知，传统LiDAR处理方法存在延迟问题，流式方法因几何不匹配导致性能下降。

Method: PHiM采用局部双向Mamba块进行空间编码和全局前向Mamba块进行时间建模，替代卷积和位置编码。

Result: 在Waymo Open Dataset上，PHiM性能提升10%，吞吐量翻倍，达到全扫描基线水平。

Conclusion: PHiM为流式LiDAR检测提供了高效解决方案，性能显著提升。

Abstract: Accurate and efficient object detection is essential for autonomous vehicles,
where real-time perception requires low latency and high throughput. LiDAR
sensors provide robust depth information, but conventional methods process full
360{\deg} scans in a single pass, introducing significant delay. Streaming
approaches address this by sequentially processing partial scans in the native
polar coordinate system, yet they rely on translation-invariant convolutions
that are misaligned with polar geometry -- resulting in degraded performance or
requiring complex distortion mitigation. Recent Mamba-based state space models
(SSMs) have shown promise for LiDAR perception, but only in the full-scan
setting, relying on geometric serialization and positional embeddings that are
memory-intensive and ill-suited to streaming. We propose Polar Hierarchical
Mamba (PHiM), a novel SSM architecture designed for polar-coordinate streaming
LiDAR. PHiM uses local bidirectional Mamba blocks for intra-sector spatial
encoding and a global forward Mamba for inter-sector temporal modeling,
replacing convolutions and positional encodings with distortion-aware,
dimensionally-decomposed operations. PHiM sets a new state-of-the-art among
streaming detectors on the Waymo Open Dataset, outperforming the previous best
by 10\% and matching full-scan baselines at twice the throughput. Code will be
available at https://github.com/meilongzhang/Polar-Hierarchical-Mamba .

</details>


### [261] [LaTtE-Flow: Layerwise Timestep-Expert Flow-based Transformer](https://arxiv.org/abs/2506.06952)
*Ying Shen,Zhiyang Xu,Jiuhai Chen,Shizhe Diao,Jiaxin Zhang,Yuguang Yao,Joy Rimchala,Ismini Lourentzou,Lifu Huang*

Main category: cs.CV

TL;DR: LaTtE-Flow是一种高效的多模态模型，统一了图像理解和生成，通过分层时间步专家架构和流匹配设计，显著提升了生成速度和性能。


<details>
  <summary>Details</summary>
Motivation: 现有统一模型需要大量预训练且性能不如专用模型，生成速度慢，限制了实际应用。

Method: 基于预训练视觉语言模型，引入分层时间步专家流架构和条件残差注意力机制。

Result: 在理解任务中表现优异，生成质量接近最新模型，推理速度快6倍。

Conclusion: LaTtE-Flow高效统一了图像理解和生成，具有实际部署潜力。

Abstract: Recent advances in multimodal foundation models unifying image understanding
and generation have opened exciting avenues for tackling a wide range of
vision-language tasks within a single framework. Despite progress, existing
unified models typically require extensive pretraining and struggle to achieve
the same level of performance compared to models dedicated to each task.
Additionally, many of these models suffer from slow image generation speeds,
limiting their practical deployment in real-time or resource-constrained
settings. In this work, we propose Layerwise Timestep-Expert Flow-based
Transformer (LaTtE-Flow), a novel and efficient architecture that unifies image
understanding and generation within a single multimodal model. LaTtE-Flow
builds upon powerful pretrained Vision-Language Models (VLMs) to inherit strong
multimodal understanding capabilities, and extends them with a novel Layerwise
Timestep Experts flow-based architecture for efficient image generation.
LaTtE-Flow distributes the flow-matching process across specialized groups of
Transformer layers, each responsible for a distinct subset of timesteps. This
design significantly improves sampling efficiency by activating only a small
subset of layers at each sampling timestep. To further enhance performance, we
propose a Timestep-Conditioned Residual Attention mechanism for efficient
information reuse across layers. Experiments demonstrate that LaTtE-Flow
achieves strong performance on multimodal understanding tasks, while achieving
competitive image generation quality with around 6x faster inference speed
compared to recent unified multimodal models.

</details>


### [262] [Task-driven real-world super-resolution of document scans](https://arxiv.org/abs/2506.06953)
*Maciej Zyrek,Tomasz Tarasiewicz,Jakub Sadel,Aleksandra Krzywon,Michal Kawulok*

Main category: cs.CV

TL;DR: 论文提出了一种针对光学字符识别任务优化的多任务学习框架，通过结合高级视觉任务的辅助损失函数，动态调整权重，提升了真实场景下的超分辨率性能。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习方法在模拟数据集上表现良好，但在真实场景（如文档扫描）中泛化能力不足，需解决复杂退化和语义变化问题。

Method: 采用多任务学习框架，结合文本检测、识别、关键点定位和色调一致性等辅助损失函数，动态调整权重，基于SRResNet架构实现。

Result: 实验表明，该方法在模拟和真实数据集上均提升了文本检测性能（IoU指标），同时保持了图像保真度。

Conclusion: 多目标优化有助于缩小模拟训练与真实部署之间的差距，提升超分辨率模型在实际场景中的表现。

Abstract: Single-image super-resolution refers to the reconstruction of a
high-resolution image from a single low-resolution observation. Although recent
deep learning-based methods have demonstrated notable success on simulated
datasets -- with low-resolution images obtained by degrading and downsampling
high-resolution ones -- they frequently fail to generalize to real-world
settings, such as document scans, which are affected by complex degradations
and semantic variability. In this study, we introduce a task-driven, multi-task
learning framework for training a super-resolution network specifically
optimized for optical character recognition tasks. We propose to incorporate
auxiliary loss functions derived from high-level vision tasks, including text
detection using the connectionist text proposal network, text recognition via a
convolutional recurrent neural network, keypoints localization using Key.Net,
and hue consistency. To balance these diverse objectives, we employ dynamic
weight averaging mechanism, which adaptively adjusts the relative importance of
each loss term based on its convergence behavior. We validate our approach upon
the SRResNet architecture, which is a well-established technique for
single-image super-resolution. Experimental evaluations on both simulated and
real-world scanned document datasets demonstrate that the proposed approach
improves text detection, measured with intersection over union, while
preserving overall image fidelity. These findings underscore the value of
multi-objective optimization in super-resolution models for bridging the gap
between simulated training regimes and practical deployment in real-world
scenarios.

</details>


### [263] [AR-RAG: Autoregressive Retrieval Augmentation for Image Generation](https://arxiv.org/abs/2506.06962)
*Jingyuan Qi,Zhiyang Xu,Qifan Wang,Lifu Huang*

Main category: cs.CV

TL;DR: AR-RAG是一种新的图像生成方法，通过自回归方式在生成过程中动态检索和整合相关图像块，解决了现有方法的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有图像生成方法通常基于静态检索，容易导致过度复制或风格偏差等问题，AR-RAG旨在动态适应生成需求。

Method: 提出两种框架：DAiD（直接合并预测块与检索块的分布）和FAiD（通过多尺度卷积平滑检索块特征）。

Result: 在Midjourney-30K等基准测试中表现优于现有方法。

Conclusion: AR-RAG通过动态检索和整合，显著提升了图像生成的灵活性和质量。

Abstract: We introduce Autoregressive Retrieval Augmentation (AR-RAG), a novel paradigm
that enhances image generation by autoregressively incorporating knearest
neighbor retrievals at the patch level. Unlike prior methods that perform a
single, static retrieval before generation and condition the entire generation
on fixed reference images, AR-RAG performs context-aware retrievals at each
generation step, using prior-generated patches as queries to retrieve and
incorporate the most relevant patch-level visual references, enabling the model
to respond to evolving generation needs while avoiding limitations (e.g.,
over-copying, stylistic bias, etc.) prevalent in existing methods. To realize
AR-RAG, we propose two parallel frameworks: (1) Distribution-Augmentation in
Decoding (DAiD), a training-free plug-and-use decoding strategy that directly
merges the distribution of model-predicted patches with the distribution of
retrieved patches, and (2) Feature-Augmentation in Decoding (FAiD), a
parameter-efficient fine-tuning method that progressively smooths the features
of retrieved patches via multi-scale convolution operations and leverages them
to augment the image generation process. We validate the effectiveness of
AR-RAG on widely adopted benchmarks, including Midjourney-30K, GenEval and
DPG-Bench, demonstrating significant performance gains over state-of-the-art
image generation models.

</details>


### [264] [Dual-view Spatio-Temporal Feature Fusion with CNN-Transformer Hybrid Network for Chinese Isolated Sign Language Recognition](https://arxiv.org/abs/2506.06966)
*Siyuan Jing,Guangxue Wang,Haoyang Zhai,Qin Tao,Jun Yang,Bing Wang,Peng Jin*

Main category: cs.CV

TL;DR: 该论文提出了一个双视角中国手语数据集NationalCSL-DP，并设计了一个CNN-Transformer网络作为基线模型，通过融合策略提升孤立手语识别性能。


<details>
  <summary>Details</summary>
Motivation: 现有手语数据集覆盖不全且多为单视角，难以应对手部遮挡问题，因此需要构建更全面的双视角数据集。

Method: 提出双视角数据集NationalCSL-DP，包含134140个视频，并设计CNN-Transformer网络及简单有效的融合策略。

Result: 实验证明数据集和基线模型有效，融合策略显著提升性能，但序列模型难以学习双视角互补特征。

Conclusion: 双视角数据集和融合策略为孤立手语识别提供了新方向，但序列模型的局限性仍需改进。

Abstract: Due to the emergence of many sign language datasets, isolated sign language
recognition (ISLR) has made significant progress in recent years. In addition,
the development of various advanced deep neural networks is another reason for
this breakthrough. However, challenges remain in applying the technique in the
real world. First, existing sign language datasets do not cover the whole sign
vocabulary. Second, most of the sign language datasets provide only single view
RGB videos, which makes it difficult to handle hand occlusions when performing
ISLR. To fill this gap, this paper presents a dual-view sign language dataset
for ISLR named NationalCSL-DP, which fully covers the Chinese national sign
language vocabulary. The dataset consists of 134140 sign videos recorded by ten
signers with respect to two vertical views, namely, the front side and the left
side. Furthermore, a CNN transformer network is also proposed as a strong
baseline and an extremely simple but effective fusion strategy for prediction.
Extensive experiments were conducted to prove the effectiveness of the datasets
as well as the baseline. The results show that the proposed fusion strategy can
significantly increase the performance of the ISLR, but it is not easy for the
sequence-to-sequence model, regardless of whether the early-fusion or
late-fusion strategy is applied, to learn the complementary features from the
sign videos of two vertical views.

</details>


### [265] [Guiding Cross-Modal Representations with MLLM Priors via Preference Alignment](https://arxiv.org/abs/2506.06970)
*Pengfei Zhao,Rongbo Luan,Wei Zhang,Peng Wu,Sifeng He*

Main category: cs.CV

TL;DR: 论文提出MAPLE框架，利用MLLM的细粒度对齐特性改进跨模态表示学习，通过强化学习和新的RPA损失显著提升检索效果。


<details>
  <summary>Details</summary>
Motivation: 尽管CLIP在多模态检索中表现优异，但仍存在模态间隙问题。MLLM具有强大的对齐特性，但现有方法依赖粗粒度对齐机制，限制了潜力。

Method: 提出MAPLE框架，结合MLLM的细粒度对齐先验，采用强化学习和RPA损失优化跨模态表示学习。

Result: 实验表明，MAPLE在细粒度跨模态检索中取得显著提升，有效处理语义细微差异。

Conclusion: MAPLE通过细粒度对齐和强化学习，显著改善了跨模态检索性能。

Abstract: Despite Contrastive Language-Image Pretraining (CLIP)'s remarkable capability
to retrieve content across modalities, a substantial modality gap persists in
its feature space. Intriguingly, we discover that off-the-shelf MLLMs
(Multimodal Large Language Models) demonstrate powerful inherent modality
alignment properties. While recent MLLM-based retrievers with unified
architectures partially mitigate this gap, their reliance on coarse modality
alignment mechanisms fundamentally limits their potential. In this work, We
introduce MAPLE (Modality-Aligned Preference Learning for Embeddings), a novel
framework that leverages the fine grained alignment priors inherent in MLLM to
guide cross modal representation learning. MAPLE formulates the learning
process as reinforcement learning with two key components: (1) Automatic
preference data construction using off-the-shelf MLLM, and (2) a new Relative
Preference Alignment (RPA) loss, which adapts Direct Preference Optimization
(DPO) to the embedding learning setting. Experimental results show that our
preference-guided alignment achieves substantial gains in fine-grained
cross-modal retrieval, underscoring its effectiveness in handling nuanced
semantic distinctions.

</details>


### [266] [Hybrid Mesh-Gaussian Representation for Efficient Indoor Scene Reconstruction](https://arxiv.org/abs/2506.06988)
*Binxiao Huang,Zhihao Li,Shiyong Liu,Xiao Tang,Jiajun Tang,Jiaqi Lin,Yuxin Cheng,Zhenyu Chen,Xiaofei Wu,Ngai Wong*

Main category: cs.CV

TL;DR: 提出了一种结合3D高斯点云与纹理网格的混合表示方法，用于提升室内场景的渲染效率。


<details>
  <summary>Details</summary>
Motivation: 复杂纹理区域需要大量高斯点云来准确捕捉颜色变化，导致渲染速度下降。

Method: 通过修剪和优化提取的网格，结合3D高斯点云与纹理网格，并采用联合优化策略。

Result: 混合表示在保持渲染质量的同时，显著提升了帧率并减少了高斯点云数量。

Conclusion: 该方法有效平衡了渲染质量与效率，适用于室内场景的实时渲染。

Abstract: 3D Gaussian splatting (3DGS) has demonstrated exceptional performance in
image-based 3D reconstruction and real-time rendering. However, regions with
complex textures require numerous Gaussians to capture significant color
variations accurately, leading to inefficiencies in rendering speed. To address
this challenge, we introduce a hybrid representation for indoor scenes that
combines 3DGS with textured meshes. Our approach uses textured meshes to handle
texture-rich flat areas, while retaining Gaussians to model intricate
geometries. The proposed method begins by pruning and refining the extracted
mesh to eliminate geometrically complex regions. We then employ a joint
optimization for 3DGS and mesh, incorporating a warm-up strategy and
transmittance-aware supervision to balance their contributions
seamlessly.Extensive experiments demonstrate that the hybrid representation
maintains comparable rendering quality and achieves superior frames per second
FPS with fewer Gaussian primitives.

</details>


### [267] [Boosting Adversarial Transferability via Commonality-Oriented Gradient Optimization](https://arxiv.org/abs/2506.06992)
*Yanting Gao,Yepeng Liu,Junming Liu,Qi Zhang,Hongyun Zhang,Duoqian Miao,Cairong Zhao*

Main category: cs.CV

TL;DR: 论文提出了一种共同性导向的梯度优化策略（COGO），通过增强共同信息和抑制个体特征，显著提高了对抗样本在黑盒设置中的迁移性。


<details>
  <summary>Details</summary>
Motivation: 理解Vision Transformers（ViTs）的特性与机制需要有效的对抗样本，但现有方法因过拟合导致迁移性不足。

Method: COGO包含共同性增强（CE）和个体性抑制（IS）两部分，分别扰动中低频信息并自适应评估梯度相关性。

Result: 实验表明，COGO显著提高了对抗攻击的迁移成功率，优于现有方法。

Conclusion: COGO通过优化共同信息和抑制个体特征，有效提升了对抗样本的迁移性。

Abstract: Exploring effective and transferable adversarial examples is vital for
understanding the characteristics and mechanisms of Vision Transformers (ViTs).
However, adversarial examples generated from surrogate models often exhibit
weak transferability in black-box settings due to overfitting. Existing methods
improve transferability by diversifying perturbation inputs or applying uniform
gradient regularization within surrogate models, yet they have not fully
leveraged the shared and unique features of surrogate models trained on the
same task, leading to suboptimal transfer performance. Therefore, enhancing
perturbations of common information shared by surrogate models and suppressing
those tied to individual characteristics offers an effective way to improve
transferability. Accordingly, we propose a commonality-oriented gradient
optimization strategy (COGO) consisting of two components: Commonality
Enhancement (CE) and Individuality Suppression (IS). CE perturbs the mid-to-low
frequency regions, leveraging the fact that ViTs trained on the same dataset
tend to rely more on mid-to-low frequency information for classification. IS
employs adaptive thresholds to evaluate the correlation between backpropagated
gradients and model individuality, assigning weights to gradients accordingly.
Extensive experiments demonstrate that COGO significantly improves the transfer
success rates of adversarial attacks, outperforming current state-of-the-art
methods.

</details>


### [268] [DM$^3$Net: Dual-Camera Super-Resolution via Domain Modulation and Multi-scale Matching](https://arxiv.org/abs/2506.06993)
*Cong Guan,Jiacheng Ying,Yuya Ieiri,Osamu Yoshie*

Main category: cs.CV

TL;DR: DM$^3$Net是一种基于域调制和多尺度匹配的双摄像头超分辨率网络，旨在通过参考图像提升广角图像的分辨率。


<details>
  <summary>Details</summary>
Motivation: 智能手机摄影中，利用长焦图像作为参考提升广角图像分辨率具有实际意义。

Method: 通过学习两个压缩的全局表示来弥合域差距，并设计多尺度匹配模块以提高匹配精度和鲁棒性。

Result: 在三个真实数据集上的实验表明，DM$^3$Net优于现有方法。

Conclusion: DM$^3$Net通过域调制和多尺度匹配，实现了高效且高性能的超分辨率。

Abstract: Dual-camera super-resolution is highly practical for smartphone photography
that primarily super-resolve the wide-angle images using the telephoto image as
a reference. In this paper, we propose DM$^3$Net, a novel dual-camera
super-resolution network based on Domain Modulation and Multi-scale Matching.
To bridge the domain gap between the high-resolution domain and the degraded
domain, we learn two compressed global representations from image pairs
corresponding to the two domains. To enable reliable transfer of high-frequency
structural details from the reference image, we design a multi-scale matching
module that conducts patch-level feature matching and retrieval across multiple
receptive fields to improve matching accuracy and robustness. Moreover, we also
introduce Key Pruning to achieve a significant reduction in memory usage and
inference time with little model performance sacrificed. Experimental results
on three real-world datasets demonstrate that our DM$^3$Net outperforms the
state-of-the-art approaches.

</details>


### [269] [Technical Report for ICRA 2025 GOOSE 3D Semantic Segmentation Challenge: Adaptive Point Cloud Understanding for Heterogeneous Robotic Systems](https://arxiv.org/abs/2506.06995)
*Xiaoya Zhang*

Main category: cs.CV

TL;DR: 本文介绍了ICRA 2025 GOOSE 3D语义分割挑战赛冠军解决方案的实现细节，通过结合Point Prompt Tuning（PPT）和Point Transformer v3（PTv3）骨干网络，实现了对异构LiDAR数据的自适应处理。


<details>
  <summary>Details</summary>
Motivation: 解决多机器人平台采集的多样化非结构化室外环境中3D点云的语义分割问题。

Method: 采用Point Prompt Tuning（PPT）与Point Transformer v3（PTv3）结合，通过平台特定条件化和跨数据集类别对齐策略处理异构数据。

Result: 在挑战性平台上，mIoU提升了22.59%，显著优于基线PTv3模型。

Conclusion: 该方法展示了自适应点云理解在野外机器人应用中的有效性。

Abstract: This technical report presents the implementation details of the winning
solution for the ICRA 2025 GOOSE 3D Semantic Segmentation Challenge. This
challenge focuses on semantic segmentation of 3D point clouds from diverse
unstructured outdoor environments collected from multiple robotic platforms.
This problem was addressed by implementing Point Prompt Tuning (PPT) integrated
with Point Transformer v3 (PTv3) backbone, enabling adaptive processing of
heterogeneous LiDAR data through platform-specific conditioning and
cross-dataset class alignment strategies. The model is trained without
requiring additional external data. As a result, this approach achieved
substantial performance improvements with mIoU increases of up to 22.59% on
challenging platforms compared to the baseline PTv3 model, demonstrating the
effectiveness of adaptive point cloud understanding for field robotics
applications.

</details>


### [270] [BePo: Leveraging Birds Eye View and Sparse Points for Efficient and Accurate 3D Occupancy Prediction](https://arxiv.org/abs/2506.07002)
*Yunxiao Shi,Hong Cai,Jisoo Jeong,Yinhao Zhu,Shizhong Han,Amin Ansari,Fatih Porikli*

Main category: cs.CV

TL;DR: 论文提出了一种结合BEV和稀疏点表示的新方法BePo，用于3D占用预测，解决了现有方法在小物体和平坦表面上的不足。


<details>
  <summary>Details</summary>
Motivation: 现有3D占用预测方法计算成本高或在小物体和平坦表面表现不佳，需要一种更高效且全面的解决方案。

Method: 采用双分支设计，结合稀疏点分支和BEV分支，通过交叉注意力共享信息，最终融合输出预测3D占用。

Result: 在Occ3D-nuScenes和Occ3D-Waymo基准测试中表现优越，且推理速度与最新高效方法相当。

Conclusion: BePo方法在3D占用预测中实现了高效性与性能的平衡，为自动驾驶场景理解提供了有效工具。

Abstract: 3D occupancy provides fine-grained 3D geometry and semantics for scene
understanding which is critical for autonomous driving. Most existing methods,
however, carry high compute costs, requiring dense 3D feature volume and
cross-attention to effectively aggregate information. More recent works have
adopted Bird's Eye View (BEV) or sparse points as scene representation with
much reduced cost, but still suffer from their respective shortcomings. More
concretely, BEV struggles with small objects that often experience significant
information loss after being projected to the ground plane. On the other hand,
points can flexibly model little objects in 3D, but is inefficient at capturing
flat surfaces or large objects. To address these challenges, in this paper, we
present a novel 3D occupancy prediction approach, BePo, which combines BEV and
sparse points based representations. We propose a dual-branch design: a
query-based sparse points branch and a BEV branch. The 3D information learned
in the sparse points branch is shared with the BEV stream via cross-attention,
which enriches the weakened signals of difficult objects on the BEV plane. The
outputs of both branches are finally fused to generate predicted 3D occupancy.
We conduct extensive experiments on the Occ3D-nuScenes and Occ3D-Waymo
benchmarks that demonstrate the superiority of our proposed BePo. Moreover,
BePo also delivers competitive inference speed when compared to the latest
efficient approaches.

</details>


### [271] [UNO: Unified Self-Supervised Monocular Odometry for Platform-Agnostic Deployment](https://arxiv.org/abs/2506.07013)
*Wentao Zhao,Yihe Niu,Yanbo Wang,Tianchen Deng,Shenghai Yuan,Zhenli Wang,Rui Guo,Jingchuan Wang*

Main category: cs.CV

TL;DR: UNO是一个统一的单目视觉里程计框架，能够在多样环境中实现鲁棒且自适应的姿态估计。


<details>
  <summary>Details</summary>
Motivation: 传统方法依赖于特定部署的调优或预定义的运动先验，而UNO旨在广泛适应各种真实场景，包括自动驾驶、无人机、移动机器人和手持设备。

Method: 采用专家混合策略进行局部状态估计，结合多个专用解码器处理不同类别的自我运动模式，并使用可微分的Gumbel-Softmax模块构建帧间相关图、选择最佳解码器并剔除错误估计。后端结合预训练的尺度无关深度先验和轻量级捆绑调整以确保几何一致性。

Result: 在KITTI、EuRoC-MAV和TUM-RGBD三个主要基准数据集上实现了最先进的性能。

Conclusion: UNO框架在多样环境中表现出色，具有广泛的适应性和鲁棒性。

Abstract: This work presents UNO, a unified monocular visual odometry framework that
enables robust and adaptable pose estimation across diverse environments,
platforms, and motion patterns. Unlike traditional methods that rely on
deployment-specific tuning or predefined motion priors, our approach
generalizes effectively across a wide range of real-world scenarios, including
autonomous vehicles, aerial drones, mobile robots, and handheld devices. To
this end, we introduce a Mixture-of-Experts strategy for local state
estimation, with several specialized decoders that each handle a distinct class
of ego-motion patterns. Moreover, we introduce a fully differentiable
Gumbel-Softmax module that constructs a robust inter-frame correlation graph,
selects the optimal expert decoder, and prunes erroneous estimates. These cues
are then fed into a unified back-end that combines pre-trained,
scale-independent depth priors with a lightweight bundling adjustment to
enforce geometric consistency. We extensively evaluate our method on three
major benchmark datasets: KITTI (outdoor/autonomous driving), EuRoC-MAV
(indoor/aerial drones), and TUM-RGBD (indoor/handheld), demonstrating
state-of-the-art performance.

</details>


### [272] [TABLET: Table Structure Recognition using Encoder-only Transformers](https://arxiv.org/abs/2506.07015)
*Qiyu Hou,Jun Wang*

Main category: cs.CV

TL;DR: 提出了一种基于Split-Merge的表格结构识别方法，通过双Transformer编码器优化行和列的分割任务，并通过网格分类实现合并，提高了准确性和效率。


<details>
  <summary>Details</summary>
Motivation: 解决大型密集表格的结构识别挑战，避免不稳定的边界框预测，降低计算复杂度。

Method: 采用Split-Merge框架，行和列分割作为序列标注任务，合并作为网格分类任务，使用Transformer编码器捕捉特征交互。

Result: 在FinTabNet和PubTabNet上表现优异，准确率高且处理速度快，适用于工业部署。

Conclusion: 该方法为大规模表格识别提供了鲁棒、可扩展且高效的解决方案。

Abstract: To address the challenges of table structure recognition, we propose a novel
Split-Merge-based top-down model optimized for large, densely populated tables.
Our approach formulates row and column splitting as sequence labeling tasks,
utilizing dual Transformer encoders to capture feature interactions. The
merging process is framed as a grid cell classification task, leveraging an
additional Transformer encoder to ensure accurate and coherent merging. By
eliminating unstable bounding box predictions, our method reduces resolution
loss and computational complexity, achieving high accuracy while maintaining
fast processing speed. Extensive experiments on FinTabNet and PubTabNet
demonstrate the superiority of our model over existing approaches, particularly
in real-world applications. Our method offers a robust, scalable, and efficient
solution for large-scale table recognition, making it well-suited for
industrial deployment.

</details>


### [273] [Multi-Step Guided Diffusion for Image Restoration on Edge Devices: Toward Lightweight Perception in Embodied AI](https://arxiv.org/abs/2506.07286)
*Aditya Chakravarty*

Main category: cs.CV

TL;DR: 该论文提出了一种多步优化策略，显著提升了扩散模型在逆问题中的图像质量和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有方法（如MPGD）在每一步去噪中仅应用单次梯度更新，限制了恢复的保真度和鲁棒性。

Method: 在每一步去噪时间步中引入多步优化策略。

Result: 实验表明，增加梯度更新次数提高了LPIPS和PSNR，且在Jetson Orin Nano上验证了方法的有效性。

Conclusion: MPGD具有作为轻量级即插即用恢复模块的潜力，适用于无人机和移动机器人等实时视觉感知任务。

Abstract: Diffusion models have shown remarkable flexibility for solving inverse
problems without task-specific retraining. However, existing approaches such as
Manifold Preserving Guided Diffusion (MPGD) apply only a single gradient update
per denoising step, limiting restoration fidelity and robustness, especially in
embedded or out-of-distribution settings. In this work, we introduce a
multistep optimization strategy within each denoising timestep, significantly
enhancing image quality, perceptual accuracy, and generalization. Our
experiments on super-resolution and Gaussian deblurring demonstrate that
increasing the number of gradient updates per step improves LPIPS and PSNR with
minimal latency overhead. Notably, we validate this approach on a Jetson Orin
Nano using degraded ImageNet and a UAV dataset, showing that MPGD, originally
trained on face datasets, generalizes effectively to natural and aerial scenes.
Our findings highlight MPGD's potential as a lightweight, plug-and-play
restoration module for real-time visual perception in embodied AI agents such
as drones and mobile robots.

</details>


### [274] [MAGNET: A Multi-agent Framework for Finding Audio-Visual Needles by Reasoning over Multi-Video Haystacks](https://arxiv.org/abs/2506.07016)
*Sanjoy Chowdhury,Mohamed Elmoghany,Yohan Abeysinghe,Junjie Fei,Sayan Nag,Salman Khan,Mohamed Elhoseiny,Dinesh Manocha*

Main category: cs.CV

TL;DR: 论文提出了AV-HaystacksQA任务和AVHaystacks基准，用于评估大型多模态模型在多视频检索和时间定位任务中的能力，并提出了MAGNET框架和两个新指标STEM与MTGS。


<details>
  <summary>Details</summary>
Motivation: 现有视频问答基准局限于单片段查询，无法满足实际应用中大规模音频-视觉检索和推理的需求。

Method: 提出AV-Haystacks基准和MAGNET多智能体框架，并引入STEM和MTGS指标。

Result: MAGNET在BLEU@4和GPT评估分数上分别提升89%和65%。

Conclusion: AV-HaystacksQA任务和MAGNET框架为多视频检索和时间定位提供了有效解决方案。

Abstract: Large multimodal models (LMMs) have shown remarkable progress in audio-visual
understanding, yet they struggle with real-world scenarios that require complex
reasoning across extensive video collections. Existing benchmarks for video
question answering remain limited in scope, typically involving one clip per
query, which falls short of representing the challenges of large-scale,
audio-visual retrieval and reasoning encountered in practical applications. To
bridge this gap, we introduce a novel task named AV-HaystacksQA, where the goal
is to identify salient segments across different videos in response to a query
and link them together to generate the most informative answer. To this end, we
present AVHaystacks, an audio-visual benchmark comprising 3100 annotated QA
pairs designed to assess the capabilities of LMMs in multi-video retrieval and
temporal grounding task. Additionally, we propose a model-agnostic, multi-agent
framework MAGNET to address this challenge, achieving up to 89% and 65%
relative improvements over baseline methods on BLEU@4 and GPT evaluation scores
in QA task on our proposed AVHaystacks. To enable robust evaluation of
multi-video retrieval and temporal grounding for optimal response generation,
we introduce two new metrics, STEM, which captures alignment errors between a
ground truth and a predicted step sequence and MTGS, to facilitate balanced and
interpretable evaluation of segment-level grounding performance. Project:
https://schowdhury671.github.io/magnet_project/

</details>


### [275] [Hierarchical Scoring with 3D Gaussian Splatting for Instance Image-Goal Navigation](https://arxiv.org/abs/2506.07338)
*Yijie Deng,Shuaihang Yuan,Geeta Chandra Raju Bethala,Anthony Tzes,Yu-Shen Liu,Yi Fang*

Main category: cs.CV

TL;DR: 本文提出了一种基于分层评分范式的实例图像目标导航（IIN）框架，通过优化视点选择减少冗余，提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖随机采样视点或轨迹，导致冗余和效率低下，缺乏优化的视点选择策略。

Method: 结合跨层级语义评分（基于CLIP的相关性场）和局部几何评分，估计最优视点进行目标匹配。

Result: 在模拟IIN基准测试中达到最优性能，并展示实际应用潜力。

Conclusion: 提出的分层评分范式显著提升了IIN的效率和准确性。

Abstract: Instance Image-Goal Navigation (IIN) requires autonomous agents to identify
and navigate to a target object or location depicted in a reference image
captured from any viewpoint. While recent methods leverage powerful novel view
synthesis (NVS) techniques, such as three-dimensional Gaussian splatting
(3DGS), they typically rely on randomly sampling multiple viewpoints or
trajectories to ensure comprehensive coverage of discriminative visual cues.
This approach, however, creates significant redundancy through overlapping
image samples and lacks principled view selection, substantially increasing
both rendering and comparison overhead. In this paper, we introduce a novel IIN
framework with a hierarchical scoring paradigm that estimates optimal
viewpoints for target matching. Our approach integrates cross-level semantic
scoring, utilizing CLIP-derived relevancy fields to identify regions with high
semantic similarity to the target object class, with fine-grained local
geometric scoring that performs precise pose estimation within promising
regions. Extensive evaluations demonstrate that our method achieves
state-of-the-art performance on simulated IIN benchmarks and real-world
applicability.

</details>


### [276] [Interpretable and Reliable Detection of AI-Generated Images via Grounded Reasoning in MLLMs](https://arxiv.org/abs/2506.07045)
*Yikun Ji,Hong Yan,Jun Lan,Huijia Zhu,Weiqiang Wang,Qi Fan,Liqing Zhang,Jianfu Zhang*

Main category: cs.CV

TL;DR: 论文提出了一种基于多模态大语言模型（MLLMs）的方法，用于检测AI生成图像并提供可解释的视觉定位和文本解释。通过构建标注数据集和多阶段优化策略，模型在检测和定位性能上显著优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 现有图像生成检测方法多为黑箱，缺乏可解释性。MLLMs虽具备分析能力，但在视觉解释与人类推理对齐方面存在不足。

Method: 构建标注数据集，通过多阶段优化策略微调MLLMs，平衡检测准确性、视觉定位和文本解释的连贯性。

Result: 模型在检测AI生成图像和定位视觉缺陷方面表现优异，显著优于基线方法。

Conclusion: 该方法为AI生成图像的检测提供了可解释且高效的解决方案，解决了现有MLLMs的幻觉问题。

Abstract: The rapid advancement of image generation technologies intensifies the demand
for interpretable and robust detection methods. Although existing approaches
often attain high accuracy, they typically operate as black boxes without
providing human-understandable justifications. Multi-modal Large Language
Models (MLLMs), while not originally intended for forgery detection, exhibit
strong analytical and reasoning capabilities. When properly fine-tuned, they
can effectively identify AI-generated images and offer meaningful explanations.
However, existing MLLMs still struggle with hallucination and often fail to
align their visual interpretations with actual image content and human
reasoning. To bridge this gap, we construct a dataset of AI-generated images
annotated with bounding boxes and descriptive captions that highlight synthesis
artifacts, establishing a foundation for human-aligned visual-textual grounded
reasoning. We then finetune MLLMs through a multi-stage optimization strategy
that progressively balances the objectives of accurate detection, visual
localization, and coherent textual explanation. The resulting model achieves
superior performance in both detecting AI-generated images and localizing
visual flaws, significantly outperforming baseline methods.

</details>


### [277] [A Layered Self-Supervised Knowledge Distillation Framework for Efficient Multimodal Learning on the Edge](https://arxiv.org/abs/2506.07055)
*Tarique Dahri,Zulfiqar Ali Memon,Zhenyu Yu,Mohd. Yamani Idna Idris,Sheheryar Khan,Sadiq Ahmad,Maged Shoman,Saddam Aziz,Rizwan Qureshi*

Main category: cs.CV

TL;DR: LSSKD框架通过中间特征图的辅助分类器生成自监督知识，无需预训练教师网络，提升模型性能且无额外计算成本。


<details>
  <summary>Details</summary>
Motivation: 传统方法依赖预训练教师网络，而LSSKD旨在通过自监督知识蒸馏提升紧凑模型的性能，适用于低计算设备。

Method: 在中间特征图上附加辅助分类器，生成多样自监督知识，实现跨网络阶段的一对一知识转移。

Result: 在CIFAR-100上平均提升4.54%，ImageNet提升0.32%，小样本学习场景下也达到SOTA。

Conclusion: LSSKD有效提升模型泛化能力，适用于多模态感知和低计算环境，无需额外推理成本。

Abstract: We introduce Layered Self-Supervised Knowledge Distillation (LSSKD) framework
for training compact deep learning models. Unlike traditional methods that rely
on pre-trained teacher networks, our approach appends auxiliary classifiers to
intermediate feature maps, generating diverse self-supervised knowledge and
enabling one-to-one transfer across different network stages. Our method
achieves an average improvement of 4.54\% over the state-of-the-art PS-KD
method and a 1.14% gain over SSKD on CIFAR-100, with a 0.32% improvement on
ImageNet compared to HASSKD. Experiments on Tiny ImageNet and CIFAR-100 under
few-shot learning scenarios also achieve state-of-the-art results. These
findings demonstrate the effectiveness of our approach in enhancing model
generalization and performance without the need for large over-parameterized
teacher networks. Importantly, at the inference stage, all auxiliary
classifiers can be removed, yielding no extra computational cost. This makes
our model suitable for deploying small language models on affordable
low-computing devices. Owing to its lightweight design and adaptability, our
framework is particularly suitable for multimodal sensing and cyber-physical
environments that require efficient and responsive inference. LSSKD facilitates
the development of intelligent agents capable of learning from limited sensory
data under weak supervision.

</details>


### [278] [D2R: dual regularization loss with collaborative adversarial generation for model robustness](https://arxiv.org/abs/2506.07056)
*Zhenyu Liu,Huizhi Liang,Rajiv Ranjan,Zhanxing Zhu,Vaclav Snasel,Varun Ojha*

Main category: cs.CV

TL;DR: 论文提出了一种双正则化损失（D2R Loss）和协作对抗生成（CAG）策略，用于增强深度神经网络的鲁棒性，解决了现有方法中损失函数指导不足和非协作对抗生成的问题。


<details>
  <summary>Details</summary>
Motivation: 现有防御方法在损失函数指导和对目标模型的协作对抗生成方面存在不足，影响了模型的鲁棒性。

Method: 提出D2R Loss（包括对抗分布和干净分布优化）和CAG策略（基于梯度的协作对抗样本生成）。

Result: 在CIFAR-10、CIFAR-100、Tiny ImageNet等数据集上实验表明，D2R Loss与CAG结合显著提升了模型鲁棒性。

Conclusion: D2R Loss和CAG策略有效解决了现有方法的局限性，显著提升了对抗攻击下的模型鲁棒性。

Abstract: The robustness of Deep Neural Network models is crucial for defending models
against adversarial attacks. Recent defense methods have employed collaborative
learning frameworks to enhance model robustness. Two key limitations of
existing methods are (i) insufficient guidance of the target model via loss
functions and (ii) non-collaborative adversarial generation. We, therefore,
propose a dual regularization loss (D2R Loss) method and a collaborative
adversarial generation (CAG) strategy for adversarial training. D2R loss
includes two optimization steps. The adversarial distribution and clean
distribution optimizations enhance the target model's robustness by leveraging
the strengths of different loss functions obtained via a suitable function
space exploration to focus more precisely on the target model's distribution.
CAG generates adversarial samples using a gradient-based collaboration between
guidance and target models. We conducted extensive experiments on three
benchmark databases, including CIFAR-10, CIFAR-100, Tiny ImageNet, and two
popular target models, WideResNet34-10 and PreActResNet18. Our results show
that D2R loss with CAG produces highly robust models.

</details>


### [279] [R3D2: Realistic 3D Asset Insertion via Diffusion for Autonomous Driving Simulation](https://arxiv.org/abs/2506.07826)
*William Ljungbergh,Bernardo Taveira,Wenzhao Zheng,Adam Tonderski,Chensheng Peng,Fredrik Kahl,Christoffer Petersson,Michael Felsberg,Kurt Keutzer,Masayoshi Tomizuka,Wei Zhan*

Main category: cs.CV

TL;DR: 论文提出R3D2，一种轻量级扩散模型，用于在自动驾驶验证中实现真实3D资产插入，解决现有神经重建方法的动态对象操作和可重用性问题。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶系统验证需要多样化和安全关键的测试，传统仿真平台资源密集且存在领域差距，而现有神经重建方法在动态对象操作和可重用性上表现不佳。

Method: R3D2通过一步扩散模型生成逼真的渲染效果（如阴影和一致光照），训练数据来自3D高斯喷涂生成的3D资产，并合成放置到神经渲染虚拟环境中。

Result: 定量和定性评估表明，R3D2显著提升了插入资产的真实感，支持文本到3D资产插入和跨场景对象转移，实现自动驾驶验证的可扩展性。

Conclusion: R3D2为自动驾驶验证提供了可扩展且逼真的仿真解决方案，未来将公开数据集和代码以促进研究。

Abstract: Validating autonomous driving (AD) systems requires diverse and
safety-critical testing, making photorealistic virtual environments essential.
Traditional simulation platforms, while controllable, are resource-intensive to
scale and often suffer from a domain gap with real-world data. In contrast,
neural reconstruction methods like 3D Gaussian Splatting (3DGS) offer a
scalable solution for creating photorealistic digital twins of real-world
driving scenes. However, they struggle with dynamic object manipulation and
reusability as their per-scene optimization-based methodology tends to result
in incomplete object models with integrated illumination effects. This paper
introduces R3D2, a lightweight, one-step diffusion model designed to overcome
these limitations and enable realistic insertion of complete 3D assets into
existing scenes by generating plausible rendering effects-such as shadows and
consistent lighting-in real time. This is achieved by training R3D2 on a novel
dataset: 3DGS object assets are generated from in-the-wild AD data using an
image-conditioned 3D generative model, and then synthetically placed into
neural rendering-based virtual environments, allowing R3D2 to learn realistic
integration. Quantitative and qualitative evaluations demonstrate that R3D2
significantly enhances the realism of inserted assets, enabling use-cases like
text-to-3D asset insertion and cross-scene/dataset object transfer, allowing
for true scalability in AD validation. To promote further research in scalable
and realistic AD simulation, we will release our dataset and code, see
https://research.zenseact.com/publications/R3D2/.

</details>


### [280] [FLAIR-HUB: Large-scale Multimodal Dataset for Land Cover and Crop Mapping](https://arxiv.org/abs/2506.07080)
*Anatol Garioud,Sébastien Giordano,Nicolas David,Nicolas Gonthier*

Main category: cs.CV

TL;DR: FLAIR-HUB是一个多传感器土地覆盖数据集，结合六种对齐模态数据，用于土地覆盖和作物分类研究，支持监督和多模态预训练。


<details>
  <summary>Details</summary>
Motivation: 高分辨率地球观测数据的处理与标注挑战促使IGN开发FLAIR-HUB，以支持多模态融合和深度学习模型的研究。

Method: IGN引入FLAIR-HUB数据集，结合六种模态数据（如航空影像、Sentinel-1/2时间序列等），并通过基准测试评估多模态融合和深度学习模型（如CNN、transformer）的性能。

Result: 最佳土地覆盖分类性能达到78.2%准确率和65.8% mIoU，几乎使用了所有模态数据。

Conclusion: FLAIR-HUB为多模态融合和精细分类提供了重要支持，数据与代码已公开。

Abstract: The growing availability of high-quality Earth Observation (EO) data enables
accurate global land cover and crop type monitoring. However, the volume and
heterogeneity of these datasets pose major processing and annotation
challenges. To address this, the French National Institute of Geographical and
Forest Information (IGN) is actively exploring innovative strategies to exploit
diverse EO data, which require large annotated datasets. IGN introduces
FLAIR-HUB, the largest multi-sensor land cover dataset with
very-high-resolution (20 cm) annotations, covering 2528 km2 of France. It
combines six aligned modalities: aerial imagery, Sentinel-1/2 time series, SPOT
imagery, topographic data, and historical aerial images. Extensive benchmarks
evaluate multimodal fusion and deep learning models (CNNs, transformers) for
land cover or crop mapping and also explore multi-task learning. Results
underscore the complexity of multimodal fusion and fine-grained classification,
with best land cover performance (78.2% accuracy, 65.8% mIoU) achieved using
nearly all modalities. FLAIR-HUB supports supervised and multimodal
pretraining, with data and code available at
https://ignf.github.io/FLAIR/flairhub.

</details>


### [281] [LogoSP: Local-global Grouping of Superpoints for Unsupervised Semantic Segmentation of 3D Point Clouds](https://arxiv.org/abs/2506.07857)
*Zihui Zhang,Weisheng Dai,Hongtao Wen,Bo Yang*

Main category: cs.CV

TL;DR: LogoSP提出了一种无监督3D语义分割方法，通过结合局部和全局点特征学习语义信息，并在频域中根据全局模式分组超点，生成高质量的语义伪标签。


<details>
  <summary>Details</summary>
Motivation: 现有方法仅依赖局部特征，缺乏发现更丰富语义先验的能力。

Method: LogoSP结合局部和全局点特征，在频域中根据全局模式分组超点，生成语义伪标签。

Result: 在两个室内和一个室外数据集上，LogoSP显著优于现有无监督方法，达到最先进性能。

Conclusion: LogoSP证明了在无人类标注的情况下，全局模式能有效捕捉3D语义信息。

Abstract: We study the problem of unsupervised 3D semantic segmentation on raw point
clouds without needing human labels in training. Existing methods usually
formulate this problem into learning per-point local features followed by a
simple grouping strategy, lacking the ability to discover additional and
possibly richer semantic priors beyond local features. In this paper, we
introduce LogoSP to learn 3D semantics from both local and global point
features. The key to our approach is to discover 3D semantic information by
grouping superpoints according to their global patterns in the frequency
domain, thus generating highly accurate semantic pseudo-labels for training a
segmentation network. Extensive experiments on two indoor and an outdoor
datasets show that our LogoSP surpasses all existing unsupervised methods by
large margins, achieving the state-of-the-art performance for unsupervised 3D
semantic segmentation. Notably, our investigation into the learned global
patterns reveals that they truly represent meaningful 3D semantics in the
absence of human labels during training.

</details>


### [282] [UCOD-DPL: Unsupervised Camouflaged Object Detection via Dynamic Pseudo-label Learning](https://arxiv.org/abs/2506.07087)
*Weiqi Yan,Lvhai Chen,Huaijia Kou,Shengchuan Zhang,Yan Zhang,Liujuan Cao*

Main category: cs.CV

TL;DR: 提出了一种基于动态伪标签学习的无监督伪装目标检测方法（UCOD-DPL），通过自适应伪标签模块、双分支对抗解码器和二次观察机制，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 现有无监督方法因伪标签噪声和简单解码器导致性能不足，需解决伪标签噪声和语义特征学习问题。

Method: 采用教师-学生框架，结合自适应伪标签模块（APM）、双分支对抗解码器（DBA）和二次观察机制。

Result: 实验表明，该方法性能优异，甚至超过部分全监督方法。

Conclusion: UCOD-DPL通过动态伪标签学习和复杂解码器设计，显著提升了无监督伪装目标检测的性能。

Abstract: Unsupervised Camoflaged Object Detection (UCOD) has gained attention since it
doesn't need to rely on extensive pixel-level labels. Existing UCOD methods
typically generate pseudo-labels using fixed strategies and train 1 x1
convolutional layers as a simple decoder, leading to low performance compared
to fully-supervised methods. We emphasize two drawbacks in these approaches:
1). The model is prone to fitting incorrect knowledge due to the pseudo-label
containing substantial noise. 2). The simple decoder fails to capture and learn
the semantic features of camouflaged objects, especially for small-sized
objects, due to the low-resolution pseudo-labels and severe confusion between
foreground and background pixels. To this end, we propose a UCOD method with a
teacher-student framework via Dynamic Pseudo-label Learning called UCOD-DPL,
which contains an Adaptive Pseudo-label Module (APM), a Dual-Branch Adversarial
(DBA) decoder, and a Look-Twice mechanism. The APM module adaptively combines
pseudo-labels generated by fixed strategies and the teacher model to prevent
the model from overfitting incorrect knowledge while preserving the ability for
self-correction; the DBA decoder takes adversarial learning of different
segmentation objectives, guides the model to overcome the foreground-background
confusion of camouflaged objects, and the Look-Twice mechanism mimics the human
tendency to zoom in on camouflaged objects and performs secondary refinement on
small-sized objects. Extensive experiments show that our method demonstrates
outstanding performance, even surpassing some existing fully supervised
methods. The code is available now.

</details>


### [283] [FreeGave: 3D Physics Learning from Dynamic Videos by Gaussian Velocity](https://arxiv.org/abs/2506.07865)
*Jinxi Li,Ziyang Song,Siyuan Zhou,Bo Yang*

Main category: cs.CV

TL;DR: FreeGave是一种无需物体先验的方法，通过引入物理代码和无散度模块，从多视角视频中学习复杂动态3D场景的物理特性。


<details>
  <summary>Details</summary>
Motivation: 现有方法在边界处学习复杂物理运动时表现不佳，或需要物体先验（如掩码或类型）。FreeGave旨在解决这些问题。

Method: 提出FreeGave方法，通过物理代码和无散度模块估计每个高斯速度场，避免低效的PINN损失。

Result: 在多个数据集上验证了方法的优越性，特别是在未来帧外推和运动分割任务中。

Conclusion: FreeGave成功学习了无标签的3D物理运动模式，展示了其潜力。

Abstract: In this paper, we aim to model 3D scene geometry, appearance, and the
underlying physics purely from multi-view videos. By applying various governing
PDEs as PINN losses or incorporating physics simulation into neural networks,
existing works often fail to learn complex physical motions at boundaries or
require object priors such as masks or types. In this paper, we propose
FreeGave to learn the physics of complex dynamic 3D scenes without needing any
object priors. The key to our approach is to introduce a physics code followed
by a carefully designed divergence-free module for estimating a per-Gaussian
velocity field, without relying on the inefficient PINN losses. Extensive
experiments on three public datasets and a newly collected challenging
real-world dataset demonstrate the superior performance of our method for
future frame extrapolation and motion segmentation. Most notably, our
investigation into the learned physics codes reveals that they truly learn
meaningful 3D physical motion patterns in the absence of any human labels in
training.

</details>


### [284] [SceneLCM: End-to-End Layout-Guided Interactive Indoor Scene Generation with Latent Consistency Model](https://arxiv.org/abs/2506.07091)
*Yangkai Lin,Jiabao Lei,Kui Jia*

Main category: cs.CV

TL;DR: SceneLCM是一个端到端框架，结合LLM和LCM，通过四个模块化流程生成和优化室内场景，解决了现有方法的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有室内场景生成方法存在编辑限制、物理不一致、人力需求高、单房间限制和材质质量差等问题，需要更高效的解决方案。

Method: SceneLCM分为四个流程：LLM引导的布局生成、基于LCM的家具生成、环境优化和物理编辑。

Result: 实验证明SceneLCM在生成质量和物理一致性上优于现有技术。

Conclusion: SceneLCM展示了在多样化应用中的广泛潜力。

Abstract: Our project page: https://scutyklin.github.io/SceneLCM/. Automated generation
of complex, interactive indoor scenes tailored to user prompt remains a
formidable challenge. While existing methods achieve indoor scene synthesis,
they struggle with rigid editing constraints, physical incoherence, excessive
human effort, single-room limitations, and suboptimal material quality. To
address these limitations, we propose SceneLCM, an end-to-end framework that
synergizes Large Language Model (LLM) for layout design with Latent Consistency
Model(LCM) for scene optimization. Our approach decomposes scene generation
into four modular pipelines: (1) Layout Generation. We employ LLM-guided 3D
spatial reasoning to convert textual descriptions into parametric blueprints(3D
layout). And an iterative programmatic validation mechanism iteratively refines
layout parameters through LLM-mediated dialogue loops; (2) Furniture
Generation. SceneLCM employs Consistency Trajectory Sampling(CTS), a
consistency distillation sampling loss guided by LCM, to form fast,
semantically rich, and high-quality representations. We also offer two
theoretical justification to demonstrate that our CTS loss is equivalent to
consistency loss and its distillation error is bounded by the truncation error
of the Euler solver; (3) Environment Optimization. We use a multiresolution
texture field to encode the appearance of the scene, and optimize via CTS loss.
To maintain cross-geometric texture coherence, we introduce a normal-aware
cross-attention decoder to predict RGB by cross-attending to the anchors
locations in geometrically heterogeneous instance. (4)Physically Editing.
SceneLCM supports physically editing by integrating physical simulation,
achieved persistent physical realism. Extensive experiments validate SceneLCM's
superiority over state-of-the-art techniques, showing its wide-ranging
potential for diverse applications.

</details>


### [285] [UA-Pose: Uncertainty-Aware 6D Object Pose Estimation and Online Object Completion with Partial References](https://arxiv.org/abs/2506.07996)
*Ming-Feng Li,Xin Yang,Fu-En Wang,Hritam Basak,Yuyin Sun,Shreekant Gayaka,Min Sun,Cheng-Hao Kuo*

Main category: cs.CV

TL;DR: UA-Pose提出了一种不确定性感知的6D物体姿态估计方法，针对部分参考数据（如不完整的3D模型或单张2D图像）进行优化，显著提升了姿态估计的准确性和物体完整性。


<details>
  <summary>Details</summary>
Motivation: 现有方法需要完整的3D模型或大量参考图像，而部分参考数据（如不完整的3D模型或单张2D图像）在6D姿态估计中仍具挑战性。

Method: UA-Pose通过不确定性建模区分已见和未见区域，结合置信度评估和不确定性感知采样策略，优化姿态估计和在线物体补全。

Result: 在YCB-Video、YCBInEOAT和HO3D数据集上，UA-Pose显著优于现有方法，尤其在物体观测不完整时表现突出。

Conclusion: UA-Pose为部分参考数据下的6D姿态估计提供了有效解决方案，提升了鲁棒性和准确性。

Abstract: 6D object pose estimation has shown strong generalizability to novel objects.
However, existing methods often require either a complete, well-reconstructed
3D model or numerous reference images that fully cover the object. Estimating
6D poses from partial references, which capture only fragments of an object's
appearance and geometry, remains challenging. To address this, we propose
UA-Pose, an uncertainty-aware approach for 6D object pose estimation and online
object completion specifically designed for partial references. We assume
access to either (1) a limited set of RGBD images with known poses or (2) a
single 2D image. For the first case, we initialize a partial object 3D model
based on the provided images and poses, while for the second, we use
image-to-3D techniques to generate an initial object 3D model. Our method
integrates uncertainty into the incomplete 3D model, distinguishing between
seen and unseen regions. This uncertainty enables confidence assessment in pose
estimation and guides an uncertainty-aware sampling strategy for online object
completion, enhancing robustness in pose estimation accuracy and improving
object completeness. We evaluate our method on the YCB-Video, YCBInEOAT, and
HO3D datasets, including RGBD sequences of YCB objects manipulated by robots
and human hands. Experimental results demonstrate significant performance
improvements over existing methods, particularly when object observations are
incomplete or partially captured. Project page:
https://minfenli.github.io/UA-Pose/

</details>


### [286] [EdgeSpotter: Multi-Scale Dense Text Spotting for Industrial Panel Monitoring](https://arxiv.org/abs/2506.07112)
*Changhong Fu,Hua Lin,Haobo Zuo,Liangliang Yao,Liguo Zhang*

Main category: cs.CV

TL;DR: 提出了一种名为EdgeSpotter的多尺度密集文本检测方法，用于工业面板的智能监控，解决了跨尺度定位和密集文本区域模糊边界的问题。


<details>
  <summary>Details</summary>
Motivation: 工业面板文本检测任务复杂，现有方法多关注单一文本形状，缺乏多尺度特征信息的全面探索。

Method: 开发了一种新型Transformer结构，结合高效混合器学习多级特征依赖关系，并设计了基于Catmull-Rom样条的特征采样方法。

Result: 在构建的工业面板监控数据集（IPM）上验证了方法的优越性能，实际测试证明了其实用性。

Conclusion: EdgeSpotter在工业面板监控任务中表现出色，代码和演示将开源。

Abstract: Text spotting for industrial panels is a key task for intelligent monitoring.
However, achieving efficient and accurate text spotting for complex industrial
panels remains challenging due to issues such as cross-scale localization and
ambiguous boundaries in dense text regions. Moreover, most existing methods
primarily focus on representing a single text shape, neglecting a comprehensive
exploration of multi-scale feature information across different texts. To
address these issues, this work proposes a novel multi-scale dense text spotter
for edge AI-based vision system (EdgeSpotter) to achieve accurate and robust
industrial panel monitoring. Specifically, a novel Transformer with efficient
mixer is developed to learn the interdependencies among multi-level features,
integrating multi-layer spatial and semantic cues. In addition, a new feature
sampling with catmull-rom splines is designed, which explicitly encodes the
shape, position, and semantic information of text, thereby alleviating missed
detections and reducing recognition errors caused by multi-scale or dense text
regions. Furthermore, a new benchmark dataset for industrial panel monitoring
(IPM) is constructed. Extensive qualitative and quantitative evaluations on
this challenging benchmark dataset validate the superior performance of the
proposed method in different challenging panel monitoring tasks. Finally,
practical tests based on the self-designed edge AI-based vision system
demonstrate the practicality of the method. The code and demo will be available
at https://github.com/vision4robotics/EdgeSpotter.

</details>


### [287] [Image segmentation and classification of E-waste for waste segregation](https://arxiv.org/abs/2506.07122)
*Prakriti Tripathi,Theertha Biju,Maniram Thota,Rakesh Lingam*

Main category: cs.CV

TL;DR: 论文提出了一种基于机器学习的电子废物分类方法，利用YOLOv11和Mask-RCNN模型实现实时分类，并计划与机器人集成完成废物分拣。


<details>
  <summary>Details</summary>
Motivation: 工业合作伙伴提出了电子废物分类的需求，希望通过机器学习模型辅助机器人完成分拣任务。

Method: 通过拆解常见电子废物（如鼠标、充电器）并拍照创建数据集，训练YOLOv11和Mask-RCNN模型。

Result: YOLOv11模型达到70 mAP，Mask-RCNN模型达到41 mAP，均能实时运行。

Conclusion: 模型将进一步集成到机器人中，实现电子废物的自动化分拣。

Abstract: Industry partners provided a problem statement that involves classifying
electronic waste using machine learning models that will be used by
pick-and-place robots for waste segregation. We started by taking common
electronic waste items, such as a mouse and charger, unsoldering them, and
taking pictures to create a custom dataset. Then state-of-the art YOLOv11 model
was trained and run to achieve 70 mAP in real-time. Mask-RCNN model was also
trained and achieved 41 mAP. The model will be further integrated with
pick-and-place robots to perform segregation of e-waste.

</details>


### [288] [Hi-VAE: Efficient Video Autoencoding with Global and Detailed Motion](https://arxiv.org/abs/2506.07136)
*Huaize Liu,Wenzhang Sun,Qiyuan Zhang,Donglin Di,Biao Gong,Hao Li,Chen Wei,Changqing Zou*

Main category: cs.CV

TL;DR: Hi-VAE是一种高效的视频自动编码框架，通过分层编码粗到细的运动表示，显著减少时空冗余，实现高压缩率和高质量重建。


<details>
  <summary>Details</summary>
Motivation: 现有视频自动编码方法未能高效建模动态时空冗余，导致压缩率不足和训练成本高。

Method: Hi-VAE将视频动态分解为全局运动和细节运动两个潜在空间，使用自监督运动编码器压缩视频潜在表示，并通过条件扩散解码器重建视频。

Result: Hi-VAE实现了1428倍的高压缩率，是基线方法的30倍，同时保持高质量重建和下游任务性能。

Conclusion: Hi-VAE在高效压缩和重建质量方面表现优异，为视频潜在表示和生成提供了新视角。

Abstract: Recent breakthroughs in video autoencoders (Video AEs) have advanced video
generation, but existing methods fail to efficiently model spatio-temporal
redundancies in dynamics, resulting in suboptimal compression factors. This
shortfall leads to excessive training costs for downstream tasks. To address
this, we introduce Hi-VAE, an efficient video autoencoding framework that
hierarchically encode coarse-to-fine motion representations of video dynamics
and formulate the decoding process as a conditional generation task.
Specifically, Hi-VAE decomposes video dynamics into two latent spaces: Global
Motion, capturing overarching motion patterns, and Detailed Motion, encoding
high-frequency spatial details. Using separate self-supervised motion encoders,
we compress video latents into compact motion representations to reduce
redundancy significantly. A conditional diffusion decoder then reconstructs
videos by combining hierarchical global and detailed motions, enabling
high-fidelity video reconstructions. Extensive experiments demonstrate that
Hi-VAE achieves a high compression factor of 1428$\times$, almost 30$\times$
higher than baseline methods (e.g., Cosmos-VAE at 48$\times$), validating the
efficiency of our approach. Meanwhile, Hi-VAE maintains high reconstruction
quality at such high compression rates and performs effectively in downstream
generative tasks. Moreover, Hi-VAE exhibits interpretability and scalability,
providing new perspectives for future exploration in video latent
representation and generation.

</details>


### [289] [GoTrack: Generic 6DoF Object Pose Refinement and Tracking](https://arxiv.org/abs/2506.07155)
*Van Nguyen Nguyen,Christian Forster,Sindi Shkodrani,Vincent Lepetit,Bugra Tekin,Cem Keskin,Tomas Hodan*

Main category: cs.CV

TL;DR: GoTrack是一种基于CAD的高效6DoF物体姿态优化与跟踪方法，无需对象特定训练，结合模型到帧和帧到帧注册，通过光流估计实现。


<details>
  <summary>Details</summary>
Motivation: 现有跟踪方法仅依赖模型到帧注册，计算量大且稳定性不足，GoTrack通过结合帧到帧注册优化性能。

Method: 使用标准神经网络模块（基于DINOv2的Transformer）简化模型到帧注册，并采用轻量级光流模型处理帧到帧注册。

Result: GoTrack与现有粗姿态估计方法结合，在标准6DoF姿态估计和跟踪基准上达到RGB-only的领先水平。

Conclusion: GoTrack提供了一种高效、稳定的6DoF姿态跟踪解决方案，适用于多样化对象，且代码和模型已开源。

Abstract: We introduce GoTrack, an efficient and accurate CAD-based method for 6DoF
object pose refinement and tracking, which can handle diverse objects without
any object-specific training. Unlike existing tracking methods that rely solely
on an analysis-by-synthesis approach for model-to-frame registration, GoTrack
additionally integrates frame-to-frame registration, which saves compute and
stabilizes tracking. Both types of registration are realized by optical flow
estimation. The model-to-frame registration is noticeably simpler than in
existing methods, relying only on standard neural network blocks (a transformer
is trained on top of DINOv2) and producing reliable pose confidence scores
without a scoring network. For the frame-to-frame registration, which is an
easier problem as consecutive video frames are typically nearly identical, we
employ a light off-the-shelf optical flow model. We demonstrate that GoTrack
can be seamlessly combined with existing coarse pose estimation methods to
create a minimal pipeline that reaches state-of-the-art RGB-only results on
standard benchmarks for 6DoF object pose estimation and tracking. Our source
code and trained models are publicly available at
https://github.com/facebookresearch/gotrack

</details>


### [290] [Faster than Fast: Accelerating Oriented FAST Feature Detection on Low-end Embedded GPUs](https://arxiv.org/abs/2506.07164)
*Qiong Chang,Xinyuan Chen,Xiang Li,Weimin Wang,Jun Miyazaki*

Main category: cs.CV

TL;DR: 论文提出两种方法加速低端嵌入式GPU上的Oriented FAST特征检测，显著提升SLAM系统实时性能。


<details>
  <summary>Details</summary>
Motivation: 当前ORB-based SLAM系统在移动平台上实时处理能力不足，主要因Oriented FAST计算耗时。

Method: 采用二进制编码策略快速确定候选点，以及可分离Harris检测策略结合GPU硬件指令优化。

Result: 在Jetson TX2上实验显示，相比OpenCV GPU版本平均加速7.3倍。

Conclusion: 方法有效提升实时性能，适合移动和资源受限环境。

Abstract: The visual-based SLAM (Simultaneous Localization and Mapping) is a technology
widely used in applications such as robotic navigation and virtual reality,
which primarily focuses on detecting feature points from visual images to
construct an unknown environmental map and simultaneously determines its own
location. It usually imposes stringent requirements on hardware power
consumption, processing speed and accuracy. Currently, the ORB (Oriented FAST
and Rotated BRIEF)-based SLAM systems have exhibited superior performance in
terms of processing speed and robustness. However, they still fall short of
meeting the demands for real-time processing on mobile platforms. This
limitation is primarily due to the time-consuming Oriented FAST calculations
accounting for approximately half of the entire SLAM system. This paper
presents two methods to accelerate the Oriented FAST feature detection on
low-end embedded GPUs. These methods optimize the most time-consuming steps in
Oriented FAST feature detection: FAST feature point detection and Harris corner
detection, which is achieved by implementing a binary-level encoding strategy
to determine candidate points quickly and a separable Harris detection strategy
with efficient low-level GPU hardware-specific instructions. Extensive
experiments on a Jetson TX2 embedded GPU demonstrate an average speedup of over
7.3 times compared to widely used OpenCV with GPU support. This significant
improvement highlights its effectiveness and potential for real-time
applications in mobile and resource-constrained environments.

</details>


### [291] [Frame Guidance: Training-Free Guidance for Frame-Level Control in Video Diffusion Models](https://arxiv.org/abs/2506.07177)
*Sangwon Jang,Taekyung Ki,Jaehyeong Jo,Jaehong Yoon,Soo Ye Kim,Zhe Lin,Sung Ju Hwang*

Main category: cs.CV

TL;DR: 提出了一种无需训练的帧引导方法（Frame Guidance），用于基于帧级信号的可控视频生成，显著减少内存使用并支持多样化任务。


<details>
  <summary>Details</summary>
Motivation: 现有方法通常需要微调大规模视频模型，随着模型规模增长变得不切实际，因此需要一种无需训练的解决方案。

Method: 提出Frame Guidance，通过简单的潜在处理方法和新颖的潜在优化策略，实现全局一致视频生成。

Result: 实验表明，Frame Guidance能高质量完成多种任务（如关键帧引导、风格化、循环），兼容任何视频模型。

Conclusion: Frame Guidance为可控视频生成提供了一种高效、无需训练的通用方法。

Abstract: Advancements in diffusion models have significantly improved video quality,
directing attention to fine-grained controllability. However, many existing
methods depend on fine-tuning large-scale video models for specific tasks,
which becomes increasingly impractical as model sizes continue to grow. In this
work, we present Frame Guidance, a training-free guidance for controllable
video generation based on frame-level signals, such as keyframes, style
reference images, sketches, or depth maps. For practical training-free
guidance, we propose a simple latent processing method that dramatically
reduces memory usage, and apply a novel latent optimization strategy designed
for globally coherent video generation. Frame Guidance enables effective
control across diverse tasks, including keyframe guidance, stylization, and
looping, without any training, compatible with any video models. Experimental
results show that Frame Guidance can produce high-quality controlled videos for
a wide range of tasks and input signals.

</details>


### [292] [Hierarchical Feature-level Reverse Propagation for Post-Training Neural Networks](https://arxiv.org/abs/2506.07188)
*Ni Ding,Lei He,Shengbo Eben Li,Keqiang Li*

Main category: cs.CV

TL;DR: 本文提出了一种分层解耦的后训练框架，通过重构中间特征图引入监督信号，提升模型透明度和训练灵活性，优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 端到端自动驾驶的黑盒模型缺乏可解释性和安全性，需要改进模型透明度和训练灵活性。

Method: 提出分层解耦的后训练框架，通过重构特征图引入监督信号，将反向计算形式化为优化问题。

Result: 在多个标准图像分类基准上表现优异，验证了方法的有效性和潜力。

Conclusion: 该方法为神经网络训练提供了新范式，兼具高效性和可解释性。

Abstract: End-to-end autonomous driving has emerged as a dominant paradigm, yet its
highly entangled black-box models pose significant challenges in terms of
interpretability and safety assurance. To improve model transparency and
training flexibility, this paper proposes a hierarchical and decoupled
post-training framework tailored for pretrained neural networks. By
reconstructing intermediate feature maps from ground-truth labels, surrogate
supervisory signals are introduced at transitional layers to enable independent
training of specific components, thereby avoiding the complexity and coupling
of conventional end-to-end backpropagation and providing interpretable insights
into networks' internal mechanisms. To the best of our knowledge, this is the
first method to formalize feature-level reverse computation as well-posed
optimization problems, which we rigorously reformulate as systems of linear
equations or least squares problems. This establishes a novel and efficient
training paradigm that extends gradient backpropagation to feature
backpropagation. Extensive experiments on multiple standard image
classification benchmarks demonstrate that the proposed method achieves
superior generalization performance and computational efficiency compared to
traditional training approaches, validating its effectiveness and potential.

</details>


### [293] [SAP-Bench: Benchmarking Multimodal Large Language Models in Surgical Action Planning](https://arxiv.org/abs/2506.07196)
*Mengya Xu,Zhongzhen Huang,Dillan Imans,Yiru Ye,Xiaofan Zhang,Qi Dou*

Main category: cs.CV

TL;DR: 论文介绍了SAP-Bench数据集和MLLM-SAP框架，用于评估多模态大语言模型在手术动作规划任务中的表现，揭示了当前模型的性能差距。


<details>
  <summary>Details</summary>
Motivation: 当前评估标准无法充分衡量手术决策的复杂性和可靠性，需要更精确的基准来推动MLLM研究。

Method: 提出SAP-Bench数据集，包含临床验证的手术动作剪辑和多模态分析锚点，并开发MLLM-SAP框架，结合领域知识生成动作建议。

Result: 评估了七种先进MLLM模型，发现其在预测下一步手术动作方面存在显著性能差距。

Conclusion: SAP-Bench为手术动作规划提供了高质量评估工具，未来需进一步提升模型的预测能力。

Abstract: Effective evaluation is critical for driving advancements in MLLM research.
The surgical action planning (SAP) task, which aims to generate future action
sequences from visual inputs, demands precise and sophisticated analytical
capabilities. Unlike mathematical reasoning, surgical decision-making operates
in life-critical domains and requires meticulous, verifiable processes to
ensure reliability and patient safety. This task demands the ability to
distinguish between atomic visual actions and coordinate complex, long-horizon
procedures, capabilities that are inadequately evaluated by current benchmarks.
To address this gap, we introduce SAP-Bench, a large-scale, high-quality
dataset designed to enable multimodal large language models (MLLMs) to perform
interpretable surgical action planning. Our SAP-Bench benchmark, derived from
the cholecystectomy procedures context with the mean duration of 1137.5s, and
introduces temporally-grounded surgical action annotations, comprising the
1,226 clinically validated action clips (mean duration: 68.7s) capturing five
fundamental surgical actions across 74 procedures. The dataset provides 1,152
strategically sampled current frames, each paired with the corresponding next
action as multimodal analysis anchors. We propose the MLLM-SAP framework that
leverages MLLMs to generate next action recommendations from the current
surgical scene and natural language instructions, enhanced with injected
surgical domain knowledge. To assess our dataset's effectiveness and the
broader capabilities of current models, we evaluate seven state-of-the-art
MLLMs (e.g., OpenAI-o1, GPT-4o, QwenVL2.5-72B, Claude-3.5-Sonnet, GeminiPro2.5,
Step-1o, and GLM-4v) and reveal critical gaps in next action prediction
performance.

</details>


### [294] [TV-LiVE: Training-Free, Text-Guided Video Editing via Layer Informed Vitality Exploitation](https://arxiv.org/abs/2506.07205)
*Min-Jung Kim,Dongjin Kim,Seokju Yun,Jaegul Choo*

Main category: cs.CV

TL;DR: TV-LiVE是一种无需训练、基于文本指导的视频编辑框架，通过利用关键层信息实现复杂编辑任务，如添加新对象和非刚性变换。


<details>
  <summary>Details</summary>
Motivation: 当前视频编辑方法主要关注简单任务（如风格迁移、背景替换），而复杂任务（如添加新对象和非刚性变换）研究较少。TV-LiVE旨在填补这一空白。

Method: 通过识别视频生成模型中的关键层（与RoPE相关），选择性注入源模型的键值特征到目标模型，实现对象添加和非刚性编辑。

Result: 实验表明，TV-LiVE在对象添加和非刚性视频编辑任务上优于现有方法。

Conclusion: TV-LiVE为复杂视频编辑任务提供了一种高效且无需训练的解决方案。

Abstract: Video editing has garnered increasing attention alongside the rapid progress
of diffusion-based video generation models. As part of these advancements,
there is a growing demand for more accessible and controllable forms of video
editing, such as prompt-based editing. Previous studies have primarily focused
on tasks such as style transfer, background replacement, object substitution,
and attribute modification, while maintaining the content structure of the
source video. However, more complex tasks, including the addition of novel
objects and nonrigid transformations, remain relatively unexplored. In this
paper, we present TV-LiVE, a Training-free and text-guided Video editing
framework via Layerinformed Vitality Exploitation. We empirically identify
vital layers within the video generation model that significantly influence the
quality of generated outputs. Notably, these layers are closely associated with
Rotary Position Embeddings (RoPE). Based on this observation, our method
enables both object addition and non-rigid video editing by selectively
injecting key and value features from the source model into the corresponding
layers of the target model guided by the layer vitality. For object addition,
we further identify prominent layers to extract the mask regions corresponding
to the newly added target prompt. We found that the extracted masks from the
prominent layers faithfully indicate the region to be edited. Experimental
results demonstrate that TV-LiVE outperforms existing approaches for both
object addition and non-rigid video editing. Project Page:
https://emjay73.github.io/TV_LiVE/

</details>


### [295] [Backdoor Attack on Vision Language Models with Stealthy Semantic Manipulation](https://arxiv.org/abs/2506.07214)
*Zhiyuan Zhong,Zhen Sun,Yepang Liu,Xinlei He,Guanhong Tao*

Main category: cs.CV

TL;DR: 论文提出了一种新型的跨模态语义不匹配后门攻击方法BadSem，通过数据投毒在训练中故意错配图像-文本对，攻击效果显著且难以防御。


<details>
  <summary>Details</summary>
Motivation: 现有后门攻击主要依赖单模态触发器，忽略了跨模态融合的特性，因此探索跨模态语义不匹配作为隐式触发器的攻击面。

Method: 提出BadSem攻击方法，构建SIMBad数据集，通过故意错配图像-文本对注入后门，并在四种VLMs上进行实验验证。

Result: BadSem攻击平均成功率超过98%，泛化能力强，且能跨模态转移；现有防御策略（系统提示和监督微调）均无法有效防御。

Conclusion: 研究揭示了VLMs在语义层面的安全漏洞，亟需针对跨模态语义攻击的防御方案。

Abstract: Vision Language Models (VLMs) have shown remarkable performance, but are also
vulnerable to backdoor attacks whereby the adversary can manipulate the model's
outputs through hidden triggers. Prior attacks primarily rely on
single-modality triggers, leaving the crucial cross-modal fusion nature of VLMs
largely unexplored. Unlike prior work, we identify a novel attack surface that
leverages cross-modal semantic mismatches as implicit triggers. Based on this
insight, we propose BadSem (Backdoor Attack with Semantic Manipulation), a data
poisoning attack that injects stealthy backdoors by deliberately misaligning
image-text pairs during training. To perform the attack, we construct SIMBad, a
dataset tailored for semantic manipulation involving color and object
attributes. Extensive experiments across four widely used VLMs show that BadSem
achieves over 98% average ASR, generalizes well to out-of-distribution
datasets, and can transfer across poisoning modalities. Our detailed analysis
using attention visualization shows that backdoored models focus on
semantically sensitive regions under mismatched conditions while maintaining
normal behavior on clean inputs. To mitigate the attack, we try two defense
strategies based on system prompt and supervised fine-tuning but find that both
of them fail to mitigate the semantic backdoor. Our findings highlight the
urgent need to address semantic vulnerabilities in VLMs for their safer
deployment.

</details>


### [296] [Dual-Modal Attention-Enhanced Text-Video Retrieval with Triplet Partial Margin Contrastive Learning](https://arxiv.org/abs/2309.11082)
*Chen Jiang,Hong Liu,Xuzheng Yu,Qing Wang,Yuan Cheng,Jia Xu,Zhongyi Liu,Qingpei Guo,Wei Chu,Ming Yang,Yuan Qi*

Main category: cs.CV

TL;DR: 本文提出两种新技术改进对比学习，解决文本-视频检索中硬负样本挖掘和语义相似度建模问题，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有对比学习方法在文本-视频检索中未充分关注硬负样本，且缺乏对不同语义相似度层次的建模能力。

Method: 提出DMAE模块挖掘硬负样本，并设计NegNCE损失；引入TPM-CL模块通过部分序三元组样本建模细粒度语义相似度。

Result: 在MSR-VTT等四个数据集上表现优于现有方法。

Conclusion: 通过硬负样本挖掘和细粒度语义建模，显著提升了文本-视频检索性能。

Abstract: In recent years, the explosion of web videos makes text-video retrieval
increasingly essential and popular for video filtering, recommendation, and
search. Text-video retrieval aims to rank relevant text/video higher than
irrelevant ones. The core of this task is to precisely measure the cross-modal
similarity between texts and videos. Recently, contrastive learning methods
have shown promising results for text-video retrieval, most of which focus on
the construction of positive and negative pairs to learn text and video
representations. Nevertheless, they do not pay enough attention to hard
negative pairs and lack the ability to model different levels of semantic
similarity. To address these two issues, this paper improves contrastive
learning using two novel techniques. First, to exploit hard examples for robust
discriminative power, we propose a novel Dual-Modal Attention-Enhanced Module
(DMAE) to mine hard negative pairs from textual and visual clues. By further
introducing a Negative-aware InfoNCE (NegNCE) loss, we are able to adaptively
identify all these hard negatives and explicitly highlight their impacts in the
training loss. Second, our work argues that triplet samples can better model
fine-grained semantic similarity compared to pairwise samples. We thereby
present a new Triplet Partial Margin Contrastive Learning (TPM-CL) module to
construct partial order triplet samples by automatically generating
fine-grained hard negatives for matched text-video pairs. The proposed TPM-CL
designs an adaptive token masking strategy with cross-modal interaction to
model subtle semantic differences. Extensive experiments demonstrate that the
proposed approach outperforms existing methods on four widely-used text-video
retrieval datasets, including MSR-VTT, MSVD, DiDeMo and ActivityNet.

</details>


### [297] [AugmentGest: Can Random Data Cropping Augmentation Boost Gesture Recognition Performance?](https://arxiv.org/abs/2506.07216)
*Nada Aboudeshish,Dmitry Ignatov,Radu Timofte*

Main category: cs.CV

TL;DR: 本文提出了一种全面的数据增强框架，通过几何变换、随机裁剪等方法提升骨架数据集的多样性，显著提高了模型泛化能力。


<details>
  <summary>Details</summary>
Motivation: 解决骨架数据集中多样性不足的问题，模拟真实世界的变化以提升模型性能。

Method: 集成几何变换、随机裁剪、旋转、缩放和强度变换，生成四倍数据集。

Result: 在多个模型和数据集上显著提升性能，达到最先进水平。

Conclusion: 该框架有效且通用，适用于多种应用场景，代码已开源。

Abstract: Data augmentation is a crucial technique in deep learning, particularly for
tasks with limited dataset diversity, such as skeleton-based datasets. This
paper proposes a comprehensive data augmentation framework that integrates
geometric transformations, random cropping, rotation, zooming and
intensity-based transformations, brightness and contrast adjustments to
simulate real-world variations. Random cropping ensures the preservation of
spatio-temporal integrity while addressing challenges such as viewpoint bias
and occlusions. The augmentation pipeline generates three augmented versions
for each sample in addition to the data set sample, thus quadrupling the data
set size and enriching the diversity of gesture representations. The proposed
augmentation strategy is evaluated on three models: multi-stream e2eET, FPPR
point cloud-based hand gesture recognition (HGR), and DD-Network. Experiments
are conducted on benchmark datasets including DHG14/28, SHREC'17, and JHMDB.
The e2eET model, recognized as the state-of-the-art for hand gesture
recognition on DHG14/28 and SHREC'17. The FPPR-PCD model, the second-best
performing model on SHREC'17, excels in point cloud-based gesture recognition.
DD-Net, a lightweight and efficient architecture for skeleton-based action
recognition, is evaluated on SHREC'17 and the Human Motion Data Base (JHMDB).
The results underline the effectiveness and versatility of the proposed
augmentation strategy, significantly improving model generalization and
robustness across diverse datasets and architectures. This framework not only
establishes state-of-the-art results on all three evaluated models but also
offers a scalable solution to advance HGR and action recognition applications
in real-world scenarios. The framework is available at
https://github.com/NadaAbodeshish/Random-Cropping-augmentation-HGR

</details>


### [298] [MiniGPT-Reverse-Designing: Predicting Image Adjustments Utilizing MiniGPT-4](https://arxiv.org/abs/2406.00971)
*Vahid Azizi,Fatemeh Koochaki*

Main category: cs.CV

TL;DR: 论文探讨了如何通过微调MiniGPT-4来扩展视觉语言模型（VLMs）的能力，以解决复杂的逆向设计任务。


<details>
  <summary>Details</summary>
Motivation: 逆向设计任务需要同时理解源图像、编辑后的图像和可选文本描述之间的交互，传统视觉语言模型难以胜任。

Method: 扩展并微调MiniGPT-4，用于逆向设计任务。

Result: 实验表明，现成的VLM（如MiniGPT-4）可以扩展到更复杂的任务。

Conclusion: MiniGPT-4在逆向设计任务中表现出良好的扩展性和潜力。

Abstract: Vision-Language Models (VLMs) have recently seen significant advancements
through integrating with Large Language Models (LLMs). The VLMs, which process
image and text modalities simultaneously, have demonstrated the ability to
learn and understand the interaction between images and texts across various
multi-modal tasks. Reverse designing, which could be defined as a complex
vision-language task, aims to predict the edits and their parameters, given a
source image, an edited version, and an optional high-level textual edit
description. This task requires VLMs to comprehend the interplay between the
source image, the edited version, and the optional textual context
simultaneously, going beyond traditional vision-language tasks. In this paper,
we extend and fine-tune MiniGPT-4 for the reverse designing task. Our
experiments demonstrate the extensibility of off-the-shelf VLMs, specifically
MiniGPT-4, for more complex tasks such as reverse designing. Code is available
at this \href{https://github.com/VahidAz/MiniGPT-Reverse-Designing}

</details>


### [299] [Hallucination at a Glance: Controlled Visual Edits and Fine-Grained Multimodal Learning](https://arxiv.org/abs/2506.07227)
*Tianyi Bai,Yuxuan Fan,Jiantao Qiu,Fupeng Sun,Jiayi Song,Junlin Han,Zichen Liu,Conghui He,Wentao Zhang,Binhang Yuan*

Main category: cs.CV

TL;DR: 论文提出了一种针对多模态大语言模型（MLLMs）在细粒度视觉差异任务中的改进方法，通过构建微编辑数据集（MED）和引入特征级一致性损失，显著提升了模型性能。


<details>
  <summary>Details</summary>
Motivation: MLLMs在视觉语言任务中表现优异，但在细粒度视觉差异上存在幻觉或语义遗漏问题，主要源于训练数据和目标函数的限制。

Method: 提出可控数据生成流程构建MED数据集，并设计监督微调框架，引入特征级一致性损失以稳定视觉嵌入。

Result: 在微编辑检测基准上，方法显著提升了差异检测准确率并减少幻觉，同时在标准视觉语言任务中表现一致提升。

Conclusion: 结合针对性数据和目标对齐能有效增强MLLMs的细粒度视觉推理能力。

Abstract: Multimodal large language models (MLLMs) have achieved strong performance on
vision-language tasks but still struggle with fine-grained visual differences,
leading to hallucinations or missed semantic shifts. We attribute this to
limitations in both training data and learning objectives. To address these
issues, we propose a controlled data generation pipeline that produces
minimally edited image pairs with semantically aligned captions. Using this
pipeline, we construct the Micro Edit Dataset (MED), containing over 50K
image-text pairs spanning 11 fine-grained edit categories, including attribute,
count, position, and object presence changes. Building on MED, we introduce a
supervised fine-tuning (SFT) framework with a feature-level consistency loss
that promotes stable visual embeddings under small edits. We evaluate our
approach on the Micro Edit Detection benchmark, which includes carefully
balanced evaluation pairs designed to test sensitivity to subtle visual
variations across the same edit categories. Our method improves difference
detection accuracy and reduces hallucinations compared to strong baselines,
including GPT-4o. Moreover, it yields consistent gains on standard
vision-language tasks such as image captioning and visual question answering.
These results demonstrate the effectiveness of combining targeted data and
alignment objectives for enhancing fine-grained visual reasoning in MLLMs.

</details>


### [300] [Multi-Step Visual Reasoning with Visual Tokens Scaling and Verification](https://arxiv.org/abs/2506.07235)
*Tianyi Bai,Zengjie Hu,Fupeng Sun,Jiantao Qiu,Yizhen Jiang,Guangxin He,Bohan Zeng,Conghui He,Binhang Yuan,Wentao Zhang*

Main category: cs.CV

TL;DR: 论文提出了一种动态推理框架，通过视觉标记缩放和验证器引导的迭代推理，提升多模态大语言模型（MLLMs）的视觉理解能力。


<details>
  <summary>Details</summary>
Motivation: 现有MLLMs采用静态推理范式，限制了其动态适应和迭代优化的能力，而人类感知是动态且反馈驱动的。

Method: 将问题建模为马尔可夫决策过程，结合提议视觉动作的推理器和通过多步直接偏好优化（DPO）训练的验证器。

Result: 方法在多个视觉推理基准测试中显著优于现有方法，提高了准确性和可解释性。

Conclusion: 动态推理机制为下一代MLLMs实现细粒度、上下文感知的视觉推理提供了潜力。

Abstract: Multi-modal large language models (MLLMs) have achieved remarkable
capabilities by integrating visual perception with language understanding,
enabling applications such as image-grounded dialogue, visual question
answering, and scientific analysis. However, most MLLMs adopt a static
inference paradigm, encoding the entire image into fixed visual tokens upfront,
which limits their ability to iteratively refine understanding or adapt to
context during inference. This contrasts sharply with human perception, which
is dynamic, selective, and feedback-driven. In this work, we introduce a novel
framework for inference-time visual token scaling that enables MLLMs to perform
iterative, verifier-guided reasoning over visual content. We formulate the
problem as a Markov Decision Process, involving a reasoner that proposes visual
actions and a verifier, which is trained via multi-step Direct Preference
Optimization (DPO), that evaluates these actions and determines when reasoning
should terminate. To support this, we present a new dataset, VTS, comprising
supervised reasoning trajectories (VTS-SFT) and preference-labeled reasoning
comparisons (VTS-DPO). Our method significantly outperforms existing approaches
across diverse visual reasoning benchmarks, offering not only improved accuracy
but also more interpretable and grounded reasoning processes. These results
demonstrate the promise of dynamic inference mechanisms for enabling
fine-grained, context-aware visual reasoning in next-generation MLLMs.

</details>


### [301] [From Generation to Generalization: Emergent Few-Shot Learning in Video Diffusion Models](https://arxiv.org/abs/2506.07280)
*Pablo Acuaviva,Aram Davtyan,Mariam Hassan,Sebastian Stapf,Ahmad Rahimi,Alexandre Alahi,Paolo Favaro*

Main category: cs.CV

TL;DR: VDMs不仅是视频生成工具，还能通过少量样本微调适应新任务，展现广泛泛化能力。


<details>
  <summary>Details</summary>
Motivation: 探索VDMs在视频生成之外的潜力，验证其内部结构和视觉理解能力。

Method: 提出少量样本微调框架，将任务转化为视觉过渡，训练LoRA权重而不改变VDMs的生成接口。

Result: 模型在低层视觉（如分割）到高层推理（如ARC-AGI）任务中表现优异。

Conclusion: VDMs是适应性强的视觉学习器，有望成为未来视觉基础模型的核心。

Abstract: Video Diffusion Models (VDMs) have emerged as powerful generative tools,
capable of synthesizing high-quality spatiotemporal content. Yet, their
potential goes far beyond mere video generation. We argue that the training
dynamics of VDMs, driven by the need to model coherent sequences, naturally
pushes them to internalize structured representations and an implicit
understanding of the visual world. To probe the extent of this internal
knowledge, we introduce a few-shot fine-tuning framework that repurposes VDMs
for new tasks using only a handful of examples. Our method transforms each task
into a visual transition, enabling the training of LoRA weights on short
input-output sequences without altering the generative interface of a frozen
VDM. Despite minimal supervision, the model exhibits strong generalization
across diverse tasks, from low-level vision (for example, segmentation and pose
estimation) to high-level reasoning (for example, on ARC-AGI). These results
reframe VDMs as more than generative engines. They are adaptable visual
learners with the potential to serve as the backbone for future foundation
models in vision.

</details>


### [302] [STARFlow: Scaling Latent Normalizing Flows for High-resolution Image Synthesis](https://arxiv.org/abs/2506.06276)
*Jiatao Gu,Tianrong Chen,David Berthelot,Huangjie Zheng,Yuyang Wang,Ruixiang Zhang,Laurent Dinh,Miguel Angel Bautista,Josh Susskind,Shuangfei Zhai*

Main category: cs.CV

TL;DR: STARFlow是一种基于归一化流的高分辨率图像合成模型，结合了Transformer的自回归能力，提出了TARFlow架构，并通过多项创新提升了可扩展性和样本质量。


<details>
  <summary>Details</summary>
Motivation: 解决高分辨率图像合成中归一化流的可扩展性和性能问题，结合Transformer的表达能力和归一化流的优势。

Method: 提出TARFlow架构，结合深度和浅层Transformer块，利用预训练自编码器的潜在空间建模，并引入新的引导算法。

Result: 在类别和文本条件图像生成任务中表现优异，样本质量接近扩散模型。

Conclusion: STARFlow首次证明了归一化流在大规模高分辨率图像生成中的有效性。

Abstract: We present STARFlow, a scalable generative model based on normalizing flows
that achieves strong performance in high-resolution image synthesis. The core
of STARFlow is Transformer Autoregressive Flow (TARFlow), which combines the
expressive power of normalizing flows with the structured modeling capabilities
of Autoregressive Transformers. We first establish the theoretical universality
of TARFlow for modeling continuous distributions. Building on this foundation,
we introduce several key architectural and algorithmic innovations to
significantly enhance scalability: (1) a deep-shallow design, wherein a deep
Transformer block captures most of the model representational capacity,
complemented by a few shallow Transformer blocks that are computationally
efficient yet substantially beneficial; (2) modeling in the latent space of
pretrained autoencoders, which proves more effective than direct pixel-level
modeling; and (3) a novel guidance algorithm that significantly boosts sample
quality. Crucially, our model remains an end-to-end normalizing flow, enabling
exact maximum likelihood training in continuous spaces without discretization.
STARFlow achieves competitive performance in both class-conditional and
text-conditional image generation tasks, approaching state-of-the-art diffusion
models in sample quality. To our knowledge, this work is the first successful
demonstration of normalizing flows operating effectively at this scale and
resolution.

</details>


### [303] [FANVID: A Benchmark for Face and License Plate Recognition in Low-Resolution Videos](https://arxiv.org/abs/2506.07304)
*Kavitha Viswanathan,Vrinda Goel,Shlesh Gholap,Devayan Ghosh,Madhav Gupta,Dhruvi Ganatra,Sanket Potdar,Amit Sethi*

Main category: cs.CV

TL;DR: FANVID是一个新的视频基准数据集，包含低分辨率（LR）视频片段，用于推动时间识别模型的发展，支持人脸匹配和车牌识别任务。


<details>
  <summary>Details</summary>
Motivation: 现实监控中低分辨率帧难以识别人脸和车牌，需利用时间信息提升识别可靠性。

Method: 数据集包含1,463个LR视频片段，提供手动验证的边界框和标签，并定义两项任务：人脸匹配和车牌识别。

Result: 基线方法在两项任务中分别获得0.58和0.42的评分，显示任务的可行性和挑战性。

Conclusion: FANVID旨在促进低分辨率时间建模的创新，适用于监控、法医和自动驾驶领域。

Abstract: Real-world surveillance often renders faces and license plates unrecognizable
in individual low-resolution (LR) frames, hindering reliable identification. To
advance temporal recognition models, we present FANVID, a novel video-based
benchmark comprising nearly 1,463 LR clips (180 x 320, 20--60 FPS) featuring 63
identities and 49 license plates from three English-speaking countries. Each
video includes distractor faces and plates, increasing task difficulty and
realism. The dataset contains 31,096 manually verified bounding boxes and
labels.
  FANVID defines two tasks: (1) face matching -- detecting LR faces and
matching them to high-resolution mugshots, and (2) license plate recognition --
extracting text from LR plates without a predefined database. Videos are
downsampled from high-resolution sources to ensure that faces and text are
indecipherable in single frames, requiring models to exploit temporal
information. We introduce evaluation metrics adapted from mean Average
Precision at IoU > 0.5, prioritizing identity correctness for faces and
character-level accuracy for text.
  A baseline method with pre-trained video super-resolution, detection, and
recognition achieved performance scores of 0.58 (face matching) and 0.42 (plate
recognition), highlighting both the feasibility and challenge of the tasks.
FANVID's selection of faces and plates balances diversity with recognition
challenge. We release the software for data access, evaluation, baseline, and
annotation to support reproducibility and extension. FANVID aims to catalyze
innovation in temporal modeling for LR recognition, with applications in
surveillance, forensics, and autonomous vehicles.

</details>


### [304] [AllTracker: Efficient Dense Point Tracking at High Resolution](https://arxiv.org/abs/2506.07310)
*Adam W. Harley,Yang You,Xinglong Sun,Yang Zheng,Nikhil Raghuraman,Yunqi Gu,Sheldon Liang,Wen-Hsuan Chu,Achal Dave,Pavel Tokmakov,Suya You,Rares Ambrus,Katerina Fragkiadaki,Leonidas J. Guibas*

Main category: cs.CV

TL;DR: AllTracker是一种通过估计查询帧与视频中其他帧之间的流场来估计长范围点轨迹的模型，提供高分辨率和密集的对应关系。


<details>
  <summary>Details</summary>
Motivation: 现有方法无法同时满足高分辨率、密集对应和长范围点跟踪的需求，AllTracker旨在填补这一空白。

Method: 结合光流和点跟踪技术，采用低分辨率网格迭代推断，通过2D卷积层和像素对齐注意力层分别实现空间和时间信息传播。

Result: 模型参数高效（1600万参数），在768x1024分辨率下实现最先进的点跟踪精度，且训练数据集范围更广。

Conclusion: AllTracker通过新颖架构和广泛训练数据集，实现了高性能的点跟踪，代码和模型权重已开源。

Abstract: We introduce AllTracker: a model that estimates long-range point tracks by
way of estimating the flow field between a query frame and every other frame of
a video. Unlike existing point tracking methods, our approach delivers
high-resolution and dense (all-pixel) correspondence fields, which can be
visualized as flow maps. Unlike existing optical flow methods, our approach
corresponds one frame to hundreds of subsequent frames, rather than just the
next frame. We develop a new architecture for this task, blending techniques
from existing work in optical flow and point tracking: the model performs
iterative inference on low-resolution grids of correspondence estimates,
propagating information spatially via 2D convolution layers, and propagating
information temporally via pixel-aligned attention layers. The model is fast
and parameter-efficient (16 million parameters), and delivers state-of-the-art
point tracking accuracy at high resolution (i.e., tracking 768x1024 pixels, on
a 40G GPU). A benefit of our design is that we can train on a wider set of
datasets, and we find that doing so is crucial for top performance. We provide
an extensive ablation study on our architecture details and training recipe,
making it clear which details matter most. Our code and model weights are
available at https://alltracker.github.io .

</details>


### [305] ["CASE: Contrastive Activation for Saliency Estimation](https://arxiv.org/abs/2506.07327)
*Dane Williamson,Yangfeng Ji,Matthew Dwyer*

Main category: cs.CV

TL;DR: 论文提出了一种诊断测试（class sensitivity）来评估显著性方法的可靠性，发现许多方法对类别不敏感，并提出了一种新的对比解释方法CASE，其表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 显著性方法在可视化模型预测时被广泛使用，但其视觉合理性可能掩盖了关键限制。本文旨在评估这些方法是否能区分同一输入的不同类别标签。

Method: 提出了一种诊断测试（class sensitivity），并通过实验验证了许多显著性方法的类别不敏感性。随后提出了对比解释方法CASE，用于隔离对预测类别唯一具有区分性的特征。

Result: 实验表明，许多广泛使用的显著性方法对类别不敏感，而CASE能生成更忠实且更具类别特异性的解释。

Conclusion: 显著性方法的类别不敏感性是一个普遍问题，CASE通过对比性解释提供了更可靠的解决方案。

Abstract: Saliency methods are widely used to visualize which input features are deemed
relevant to a model's prediction. However, their visual plausibility can
obscure critical limitations. In this work, we propose a diagnostic test for
class sensitivity: a method's ability to distinguish between competing class
labels on the same input. Through extensive experiments, we show that many
widely used saliency methods produce nearly identical explanations regardless
of the class label, calling into question their reliability. We find that
class-insensitive behavior persists across architectures and datasets,
suggesting the failure mode is structural rather than model-specific. Motivated
by these findings, we introduce CASE, a contrastive explanation method that
isolates features uniquely discriminative for the predicted class. We evaluate
CASE using the proposed diagnostic and a perturbation-based fidelity test, and
show that it produces faithful and more class-specific explanations than
existing methods.

</details>


### [306] [CBAM-STN-TPS-YOLO: Enhancing Agricultural Object Detection through Spatially Adaptive Attention Mechanisms](https://arxiv.org/abs/2506.07357)
*Satvik Praveen,Yoonsung Jung*

Main category: cs.CV

TL;DR: 论文提出了一种结合TPS的STN和CBAM的改进YOLO模型（CBAM-STN-TPS-YOLO），用于解决农业目标检测中的遮挡和非刚性变形问题，显著提升了检测精度。


<details>
  <summary>Details</summary>
Motivation: 农业目标检测中，现有模型（如YOLO）在遮挡、不规则结构和背景噪声下表现不佳，而传统的STN仅支持仿射变换，无法处理非刚性变形（如弯曲叶片和重叠）。

Method: 提出CBAM-STN-TPS-YOLO模型，将TPS引入STN以实现非刚性空间变换，并结合CBAM模块抑制背景噪声并突出关键特征。

Result: 在PGP数据集上，模型在精度、召回率和mAP上优于STN-YOLO，假阳性率降低12%。

Conclusion: 该轻量级模型提升了空间感知能力，适合实时边缘部署，为智能农业提供了高效准确的监测方案。

Abstract: Object detection is vital in precision agriculture for plant monitoring,
disease detection, and yield estimation. However, models like YOLO struggle
with occlusions, irregular structures, and background noise, reducing detection
accuracy. While Spatial Transformer Networks (STNs) improve spatial invariance
through learned transformations, affine mappings are insufficient for non-rigid
deformations such as bent leaves and overlaps.
  We propose CBAM-STN-TPS-YOLO, a model integrating Thin-Plate Splines (TPS)
into STNs for flexible, non-rigid spatial transformations that better align
features. Performance is further enhanced by the Convolutional Block Attention
Module (CBAM), which suppresses background noise and emphasizes relevant
spatial and channel-wise features.
  On the occlusion-heavy Plant Growth and Phenotyping (PGP) dataset, our model
outperforms STN-YOLO in precision, recall, and mAP. It achieves a 12% reduction
in false positives, highlighting the benefits of improved spatial flexibility
and attention-guided refinement. We also examine the impact of the TPS
regularization parameter in balancing transformation smoothness and detection
performance.
  This lightweight model improves spatial awareness and supports real-time edge
deployment, making it ideal for smart farming applications requiring accurate
and efficient monitoring.

</details>


### [307] [Multiple Object Stitching for Unsupervised Representation Learning](https://arxiv.org/abs/2506.07364)
*Chengchao Shen,Dawei Liu,Jianxin Wang*

Main category: cs.CV

TL;DR: 论文提出了一种名为MOS的方法，通过拼接单对象图像生成多对象图像，从而提升无监督表示在多对象图像上的性能。


<details>
  <summary>Details</summary>
Motivation: 现有对比学习方法在单对象图像上表现良好，但在多对象图像上性能较差。

Method: 通过拼接单对象图像生成多对象图像，利用预定义的对象对应关系优化无监督表示。

Result: 在ImageNet、CIFAR和COCO数据集上，MOS方法在单对象和多对象图像上均取得领先的无监督表示性能。

Conclusion: MOS方法简单有效，能够为复杂下游任务提供更详细的对象表示。

Abstract: Contrastive learning for single object centric images has achieved remarkable
progress on unsupervised representation, but suffering inferior performance on
the widespread images with multiple objects. In this paper, we propose a simple
but effective method, Multiple Object Stitching (MOS), to refine the
unsupervised representation for multi-object images. Specifically, we construct
the multi-object images by stitching the single object centric ones, where the
objects in the synthesized multi-object images are predetermined. Hence,
compared to the existing contrastive methods, our method provides additional
object correspondences between multi-object images without human annotations.
In this manner, our method pays more attention to the representations of each
object in multi-object image, thus providing more detailed representations for
complicated downstream tasks, such as object detection and semantic
segmentation. Experimental results on ImageNet, CIFAR and COCO datasets
demonstrate that our proposed method achieves the leading unsupervised
representation performance on both single object centric images and
multi-object ones. The source code is available at
https://github.com/visresearch/MultipleObjectStitching.

</details>


### [308] [C3S3: Complementary Competition and Contrastive Selection for Semi-Supervised Medical Image Segmentation](https://arxiv.org/abs/2506.07368)
*Jiaying He,Yitong Lin,Jiahe Chen,Honghui Xu,Jianwei Zheng*

Main category: cs.CV

TL;DR: C3S3是一种新型半监督医学图像分割模型，通过互补竞争和对比选择提升边界细节的精确度。


<details>
  <summary>Details</summary>
Motivation: 解决医学图像分割中边界细节捕捉不足的问题，以减少诊断误差。

Method: 结合结果驱动的对比学习模块和动态互补竞争模块，利用两个高性能子网络生成伪标签。

Result: 在两个公开数据集上表现优于现有方法，95HD和ASD指标提升至少6%。

Conclusion: C3S3显著提升了医学图像分割的边界精确度和整体性能。

Abstract: For the immanent challenge of insufficiently annotated samples in the medical
field, semi-supervised medical image segmentation (SSMIS) offers a promising
solution. Despite achieving impressive results in delineating primary target
areas, most current methodologies struggle to precisely capture the subtle
details of boundaries. This deficiency often leads to significant diagnostic
inaccuracies. To tackle this issue, we introduce C3S3, a novel semi-supervised
segmentation model that synergistically integrates complementary competition
and contrastive selection. This design significantly sharpens boundary
delineation and enhances overall precision. Specifically, we develop an
$\textit{Outcome-Driven Contrastive Learning}$ module dedicated to refining
boundary localization. Additionally, we incorporate a $\textit{Dynamic
Complementary Competition}$ module that leverages two high-performing
sub-networks to generate pseudo-labels, thereby further improving segmentation
quality. The proposed C3S3 undergoes rigorous validation on two publicly
accessible datasets, encompassing the practices of both MRI and CT scans. The
results demonstrate that our method achieves superior performance compared to
previous cutting-edge competitors. Especially, on the 95HD and ASD metrics, our
approach achieves a notable improvement of at least $6\%$, highlighting the
significant advancements. The code is available at
https://github.com/Y-TARL/C3S3.

</details>


### [309] [Generative Models at the Frontier of Compression: A Survey on Generative Face Video Coding](https://arxiv.org/abs/2506.07369)
*Bolin Chen,Shanzhi Yin,Goluck Konuko,Giuseppe Valenzise,Zihan Zhang,Shiqi Wang,Yan Ye*

Main category: cs.CV

TL;DR: 生成式人脸视频编码（GFVC）利用深度生成模型实现超低码率的高保真视频通信，超越现有VVC标准。本文首次全面综述GFVC技术，填补理论与工业标准间的空白。


<details>
  <summary>Details</summary>
Motivation: 推动GFVC的基础研究，加速其从理论创新到工业标准化的进程。

Method: 综述现有GFVC方法，进行基准分析；构建大规模主观评价数据库；总结标准化潜力并开发低复杂度系统。

Result: GFVC在超低码率下表现优于VVC，具备工业应用潜力。

Conclusion: GFVC前景广阔，但仍需解决当前挑战以推动实际应用。

Abstract: The rise of deep generative models has greatly advanced video compression,
reshaping the paradigm of face video coding through their powerful capability
for semantic-aware representation and lifelike synthesis. Generative Face Video
Coding (GFVC) stands at the forefront of this revolution, which could
characterize complex facial dynamics into compact latent codes for bitstream
compactness at the encoder side and leverages powerful deep generative models
to reconstruct high-fidelity face signal from the compressed latent codes at
the decoder side. As such, this well-designed GFVC paradigm could enable
high-fidelity face video communication at ultra-low bitrate ranges, far
surpassing the capabilities of the latest Versatile Video Coding (VVC)
standard. To pioneer foundational research and accelerate the evolution of
GFVC, this paper presents the first comprehensive survey of GFVC technologies,
systematically bridging critical gaps between theoretical innovation and
industrial standardization. In particular, we first review a broad range of
existing GFVC methods with different feature representations and optimization
strategies, and conduct a thorough benchmarking analysis. In addition, we
construct a large-scale GFVC-compressed face video database with subjective
Mean Opinion Scores (MOSs) based on human perception, aiming to identify the
most appropriate quality metrics tailored to GFVC. Moreover, we summarize the
GFVC standardization potentials with a unified high-level syntax and develop a
low-complexity GFVC system which are both expected to push forward future
practical deployments and applications. Finally, we envision the potential of
GFVC in industrial applications and deliberate on the current challenges and
future opportunities.

</details>


### [310] [ARGUS: Hallucination and Omission Evaluation in Video-LLMs](https://arxiv.org/abs/2506.07371)
*Ruchit Rawal,Reza Shirkavand,Heng Huang,Gowthami Somepalli,Tom Goldstein*

Main category: cs.CV

TL;DR: ARGUS是一个新的VideoLLM基准测试，专注于自由文本生成任务（如视频字幕），通过量化幻觉率和遗漏率来评估模型性能。


<details>
  <summary>Details</summary>
Motivation: 现有的VideoLLM基准测试主要依赖选择题，而模型在自由文本生成任务中更容易产生幻觉，因此需要更全面的评估方法。

Method: 提出ARGUS基准，通过比较VideoLLM生成的字幕与人工标注的真实字幕，量化幻觉（错误内容或时序关系）和遗漏（重要细节缺失）的比率。

Result: ARGUS提供了双重指标，全面评估视频字幕任务的性能，揭示了模型在自由文本生成中的弱点。

Conclusion: ARGUS为VideoLLM的自由文本生成能力提供了更准确的评估工具，有助于改进模型性能。

Abstract: Video large language models have not yet been widely deployed, largely due to
their tendency to hallucinate. Typical benchmarks for Video-LLMs rely simply on
multiple-choice questions. Unfortunately, VideoLLMs hallucinate far more
aggressively on freeform text generation tasks like video captioning than they
do on multiple choice verification tasks. To address this weakness, we propose
ARGUS, a VideoLLM benchmark that measures freeform video captioning
performance. By comparing VideoLLM outputs to human ground truth captions,
ARGUS quantifies dual metrics. First, we measure the rate of hallucinations in
the form of incorrect statements about video content or temporal relationships.
Second, we measure the rate at which the model omits important descriptive
details. Together, these dual metrics form a comprehensive view of video
captioning performance.

</details>


### [311] [DINO-CoDT: Multi-class Collaborative Detection and Tracking with Vision Foundation Models](https://arxiv.org/abs/2506.07375)
*Xunjie He,Christina Dao Wen Lee,Meiling Wang,Chengran Yuan,Zefan Huang,Yufeng Yue,Marcelo H. Ang Jr*

Main category: cs.CV

TL;DR: 提出了一种多类别协作检测与跟踪框架，通过全局空间注意力融合模块和视觉语义增强的REID模块，显著提升了检测和跟踪精度。


<details>
  <summary>Details</summary>
Motivation: 现有协作感知研究主要针对车辆类别，缺乏多类别解决方案，限制了实际应用。

Method: 采用全局空间注意力融合（GSAF）模块增强多尺度特征学习，设计REID模块减少ID切换错误，并引入速度自适应的轨迹管理（VATM）模块。

Result: 在V2X-Real和OPV2V数据集上，检测和跟踪精度显著优于现有方法。

Conclusion: 该框架为多类别道路用户提供了有效的协作感知解决方案，提升了实际场景的适用性。

Abstract: Collaborative perception plays a crucial role in enhancing environmental
understanding by expanding the perceptual range and improving robustness
against sensor failures, which primarily involves collaborative 3D detection
and tracking tasks. The former focuses on object recognition in individual
frames, while the latter captures continuous instance tracklets over time.
However, existing works in both areas predominantly focus on the vehicle
superclass, lacking effective solutions for both multi-class collaborative
detection and tracking. This limitation hinders their applicability in
real-world scenarios, which involve diverse object classes with varying
appearances and motion patterns. To overcome these limitations, we propose a
multi-class collaborative detection and tracking framework tailored for diverse
road users. We first present a detector with a global spatial attention fusion
(GSAF) module, enhancing multi-scale feature learning for objects of varying
sizes. Next, we introduce a tracklet RE-IDentification (REID) module that
leverages visual semantics with a vision foundation model to effectively reduce
ID SWitch (IDSW) errors, in cases of erroneous mismatches involving small
objects like pedestrians. We further design a velocity-based adaptive tracklet
management (VATM) module that adjusts the tracking interval dynamically based
on object motion. Extensive experiments on the V2X-Real and OPV2V datasets show
that our approach significantly outperforms existing state-of-the-art methods
in both detection and tracking accuracy.

</details>


### [312] [Adapter Naturally Serves as Decoupler for Cross-Domain Few-Shot Semantic Segmentation](https://arxiv.org/abs/2506.07376)
*Jintao Tong,Ran Ma,Yixiong Zou,Guangyao Chen,Yuhua Li,Ruixuan Li*

Main category: cs.CV

TL;DR: 论文提出了一种跨域少样本分割方法（CD-FSS），通过适配器解耦域信息，并设计了域特征导航器（DFN）和SAM-SVN方法，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 解决跨域少样本分割中的域差距和少样本微调问题。

Method: 提出域特征导航器（DFN）解耦域信息，并结合SAM-SVN方法防止过拟合。

Result: 在1-shot和5-shot场景下，性能分别提升2.69%和4.68% MIoU。

Conclusion: DFN和SAM-SVN方法有效解决了域差距和少样本问题，显著提升了跨域少样本分割的性能。

Abstract: Cross-domain few-shot segmentation (CD-FSS) is proposed to pre-train the
model on a source-domain dataset with sufficient samples, and then transfer the
model to target-domain datasets where only a few samples are available for
efficient fine-tuning. There are majorly two challenges in this task: (1) the
domain gap and (2) fine-tuning with scarce data. To solve these challenges, we
revisit the adapter-based methods, and discover an intriguing insight not
explored in previous works: the adapter not only helps the fine-tuning of
downstream tasks but also naturally serves as a domain information decoupler.
Then, we delve into this finding for an interpretation, and find the model's
inherent structure could lead to a natural decoupling of domain information.
Building upon this insight, we propose the Domain Feature Navigator (DFN),
which is a structure-based decoupler instead of loss-based ones like current
works, to capture domain-specific information, thereby directing the model's
attention towards domain-agnostic knowledge. Moreover, to prevent the potential
excessive overfitting of DFN during the source-domain training, we further
design the SAM-SVN method to constrain DFN from learning sample-specific
knowledge. On target domains, we freeze the model and fine-tune the DFN to
learn target-specific knowledge specific. Extensive experiments demonstrate
that our method surpasses the state-of-the-art method in CD-FSS significantly
by 2.69% and 4.68% MIoU in 1-shot and 5-shot scenarios, respectively.

</details>


### [313] [MrM: Black-Box Membership Inference Attacks against Multimodal RAG Systems](https://arxiv.org/abs/2506.07399)
*Peiru Yang,Jinhua Yin,Haoran Zheng,Xueying Bai,Huili Wang,Yufei Sun,Xintian Li,Shangguang Wang,Yongfeng Huang,Tao Qi*

Main category: cs.CV

TL;DR: 论文提出MrM框架，针对多模态RAG系统的隐私攻击问题，首次探索视觉模态的成员推理攻击（MIA），通过多目标数据扰动和反事实攻击约束，显著提升攻击效果。


<details>
  <summary>Details</summary>
Motivation: 多模态RAG系统因整合跨模态知识而广泛应用，但其可能泄露敏感信息，现有MIA方法主要关注文本模态，视觉模态研究不足。

Method: 提出MrM框架，包括对象感知数据扰动、反事实掩码选择策略和统计成员推理，以增强攻击效果。

Result: 在两种视觉数据集和八种主流视觉语言模型上，MrM在样本级和集合级评估中表现优异，且对自适应防御具有鲁棒性。

Conclusion: MrM填补了视觉模态MIA研究的空白，为多模态RAG系统的隐私保护提供了新视角。

Abstract: Multimodal retrieval-augmented generation (RAG) systems enhance large
vision-language models by integrating cross-modal knowledge, enabling their
increasing adoption across real-world multimodal tasks. These knowledge
databases may contain sensitive information that requires privacy protection.
However, multimodal RAG systems inherently grant external users indirect access
to such data, making them potentially vulnerable to privacy attacks,
particularly membership inference attacks (MIAs). % Existing MIA methods
targeting RAG systems predominantly focus on the textual modality, while the
visual modality remains relatively underexplored. To bridge this gap, we
propose MrM, the first black-box MIA framework targeted at multimodal RAG
systems. It utilizes a multi-object data perturbation framework constrained by
counterfactual attacks, which can concurrently induce the RAG systems to
retrieve the target data and generate information that leaks the membership
information. Our method first employs an object-aware data perturbation method
to constrain the perturbation to key semantics and ensure successful retrieval.
Building on this, we design a counterfact-informed mask selection strategy to
prioritize the most informative masked regions, aiming to eliminate the
interference of model self-knowledge and amplify attack efficacy. Finally, we
perform statistical membership inference by modeling query trials to extract
features that reflect the reconstruction of masked semantics from response
patterns. Experiments on two visual datasets and eight mainstream commercial
visual-language models (e.g., GPT-4o, Gemini-2) demonstrate that MrM achieves
consistently strong performance across both sample-level and set-level
evaluations, and remains robust under adaptive defenses.

</details>


### [314] [Compressed Feature Quality Assessment: Dataset and Baselines](https://arxiv.org/abs/2506.07412)
*Changsheng Gao,Wei Zhou,Guosheng Lin,Weisi Lin*

Main category: cs.CV

TL;DR: 论文提出了压缩特征质量评估（CFQA）的研究问题，并创建了首个包含300个原始特征和12000个压缩特征的基准数据集，评估了三种常用指标的性能，结果强调了数据集代表性和改进指标的必要性。


<details>
  <summary>Details</summary>
Motivation: 在资源受限环境中，特征编码的语义退化难以量化，因此需要研究如何评估压缩特征的语义保真度。

Method: 提出CFQA研究问题，构建包含多任务和多编码器的数据集，评估MSE、余弦相似度和中心核对齐三种指标的性能。

Result: 数据集具有代表性，但现有指标无法完全捕捉语义退化，需更精细的指标。

Conclusion: 论文为CFQA研究提供了基准数据集和开源代码，推动了该领域的发展。

Abstract: The widespread deployment of large models in resource-constrained
environments has underscored the need for efficient transmission of
intermediate feature representations. In this context, feature coding, which
compresses features into compact bitstreams, becomes a critical component for
scenarios involving feature transmission, storage, and reuse. However, this
compression process introduces inherent semantic degradation that is
notoriously difficult to quantify with traditional metrics. To address this,
this paper introduces the research problem of Compressed Feature Quality
Assessment (CFQA), which seeks to evaluate the semantic fidelity of compressed
features. To advance CFQA research, we propose the first benchmark dataset,
comprising 300 original features and 12000 compressed features derived from
three vision tasks and four feature codecs. Task-specific performance drops are
provided as true semantic distortion for the evaluation of CFQA metrics. We
assess the performance of three widely used metrics (MSE, cosine similarity,
and Centered Kernel Alignment) in capturing semantic degradation. The results
underscore the representativeness of the dataset and highlight the need for
more refined metrics capable of addressing the nuances of semantic distortion
in compressed features. To facilitate the ongoing development of CFQA research,
we release the dataset and all accompanying source code at
\href{https://github.com/chansongoal/Compressed-Feature-Quality-Assessment}{https://github.com/chansongoal/Compressed-Feature-Quality-Assessment}.
This contribution aims to advance the field and provide a foundational resource
for the community to explore CFQA.

</details>


### [315] [DPFormer: Dynamic Prompt Transformer for Continual Learning](https://arxiv.org/abs/2506.07414)
*Sheng-Kai Huang,Jiun-Feng Chang,Chun-Rong Huang*

Main category: cs.CV

TL;DR: 提出动态提示变换器（DPFormer）解决持续学习中的稳定性-可塑性困境和任务间混淆问题，通过提示方案和统一分类模块实现高性能。


<details>
  <summary>Details</summary>
Motivation: 解决持续学习中的灾难性遗忘和任务间混淆问题。

Method: 采用动态提示变换器（DPFormer）和提示方案，结合统一分类模块（含交叉熵损失、知识蒸馏损失和辅助损失）进行端到端训练。

Result: 在CIFAR-100、ImageNet100和ImageNet1K数据集上表现优于现有方法。

Conclusion: DPFormer通过提示方案有效解决了持续学习中的关键问题，并在多个数据集上验证了其优越性。

Abstract: In continual learning, solving the catastrophic forgetting problem may make
the models fall into the stability-plasticity dilemma. Moreover, inter-task
confusion will also occur due to the lack of knowledge exchanges between
different tasks. In order to solve the aforementioned problems, we propose a
novel dynamic prompt transformer (DPFormer) with prompt schemes. The prompt
schemes help the DPFormer memorize learned knowledge of previous classes and
tasks, and keep on learning new knowledge from new classes and tasks under a
single network structure with a nearly fixed number of model parameters.
Moreover, they also provide discrepant information to represent different tasks
to solve the inter-task confusion problem. Based on prompt schemes, a unified
classification module with the binary cross entropy loss, the knowledge
distillation loss and the auxiliary loss is proposed to train the whole model
in an end-to-end trainable manner. Compared with state-of-the-art methods, our
method achieves the best performance in the CIFAR-100, ImageNet100 and
ImageNet1K datasets under different class-incremental settings in continual
learning. The source code will be available at our GitHub after acceptance.

</details>


### [316] [FAMSeg: Fetal Femur and Cranial Ultrasound Segmentation Using Feature-Aware Attention and Mamba Enhancement](https://arxiv.org/abs/2506.07431)
*Jie He,Minglang Chen,Minying Lu,Bocheng Liang,Junming Wei,Guiyan Peng,Jiaxi Chen,Ying Tan*

Main category: cs.CV

TL;DR: 提出了一种基于特征感知和Mamba增强的胎儿股骨和颅骨超声图像分割模型，解决了高噪声和高相似性超声对象的分割难题。


<details>
  <summary>Details</summary>
Motivation: 超声图像分割的准确性对生物计量和评估至关重要，但现有模型难以适应高噪声和高相似性的超声对象，尤其是小对象分割时会出现锯齿效应。

Method: 设计了纵向和横向独立视角扫描卷积块及特征感知模块，结合Mamba优化的残差结构，抑制原始噪声干扰并增强局部多维扫描。

Result: FAMSeg网络在实验中实现了最快的损失减少和最佳的分割性能。

Conclusion: 该模型有效提升了超声图像分割的精度和效率。

Abstract: Accurate ultrasound image segmentation is a prerequisite for precise
biometrics and accurate assessment. Relying on manual delineation introduces
significant errors and is time-consuming. However, existing segmentation models
are designed based on objects in natural scenes, making them difficult to adapt
to ultrasound objects with high noise and high similarity. This is particularly
evident in small object segmentation, where a pronounced jagged effect occurs.
Therefore, this paper proposes a fetal femur and cranial ultrasound image
segmentation model based on feature perception and Mamba enhancement to address
these challenges. Specifically, a longitudinal and transverse independent
viewpoint scanning convolution block and a feature perception module were
designed to enhance the ability to capture local detail information and improve
the fusion of contextual information. Combined with the Mamba-optimized
residual structure, this design suppresses the interference of raw noise and
enhances local multi-dimensional scanning. The system builds global information
and local feature dependencies, and is trained with a combination of different
optimizers to achieve the optimal solution. After extensive experimental
validation, the FAMSeg network achieved the fastest loss reduction and the best
segmentation performance across images of varying sizes and orientations.

</details>


### [317] [Prompt to Protection: A Comparative Study of Multimodal LLMs in Construction Hazard Recognition](https://arxiv.org/abs/2506.07436)
*Nishi Chaudhary,S M Jamil Uddin,Sathvik Sharath Chandra,Anto Ovid,Alex Albert*

Main category: cs.CV

TL;DR: 该研究比较了五种多模态大语言模型（LLMs）在建筑工地视觉危险识别中的表现，发现提示策略（如零样本、少样本和思维链）显著影响性能，其中思维链提示效果最佳。GPT-4.5和GPT-o3表现最优。


<details>
  <summary>Details</summary>
Motivation: 探索多模态LLMs在建筑安全领域的应用潜力，填补其在关键视觉任务中性能评估的研究空白。

Method: 对五种LLMs（Claude-3 Opus、GPT-4.5、GPT-4o、GPT-o3和Gemini 2.0 Pro）进行对比评估，采用三种提示策略（零样本、少样本和思维链），并通过精确率、召回率和F1分数进行量化分析。

Result: 思维链提示显著提升模型性能，GPT-4.5和GPT-o3表现最佳。提示设计对多模态LLMs的准确性和一致性至关重要。

Conclusion: 研究为建筑安全领域的AI辅助系统开发提供了实用见解，强调了提示工程的重要性。

Abstract: The recent emergence of multimodal large language models (LLMs) has
introduced new opportunities for improving visual hazard recognition on
construction sites. Unlike traditional computer vision models that rely on
domain-specific training and extensive datasets, modern LLMs can interpret and
describe complex visual scenes using simple natural language prompts. However,
despite growing interest in their applications, there has been limited
investigation into how different LLMs perform in safety-critical visual tasks
within the construction domain. To address this gap, this study conducts a
comparative evaluation of five state-of-the-art LLMs: Claude-3 Opus, GPT-4.5,
GPT-4o, GPT-o3, and Gemini 2.0 Pro, to assess their ability to identify
potential hazards from real-world construction images. Each model was tested
under three prompting strategies: zero-shot, few-shot, and chain-of-thought
(CoT). Zero-shot prompting involved minimal instruction, few-shot incorporated
basic safety context and a hazard source mnemonic, and CoT provided
step-by-step reasoning examples to scaffold model thinking. Quantitative
analysis was performed using precision, recall, and F1-score metrics across all
conditions. Results reveal that prompting strategy significantly influenced
performance, with CoT prompting consistently producing higher accuracy across
models. Additionally, LLM performance varied under different conditions, with
GPT-4.5 and GPT-o3 outperforming others in most settings. The findings also
demonstrate the critical role of prompt design in enhancing the accuracy and
consistency of multimodal LLMs for construction safety applications. This study
offers actionable insights into the integration of prompt engineering and LLMs
for practical hazard recognition, contributing to the development of more
reliable AI-assisted safety systems.

</details>


### [318] [PhysiInter: Integrating Physical Mapping for High-Fidelity Human Interaction Generation](https://arxiv.org/abs/2506.07456)
*Wei Yao,Yunlian Sun,Chang Liu,Hongwen Zhang,Jinhui Tang*

Main category: cs.CV

TL;DR: 论文提出了一种物理映射方法，结合物理模拟环境，提升多人运动生成的真实性和物理合理性。


<details>
  <summary>Details</summary>
Motivation: 现有运动捕捉技术和生成模型常忽略物理约束，导致运动中的穿模、滑动和漂浮等问题，尤其在多人交互场景中更为严重。

Method: 通过物理模拟环境中的运动模仿，将目标运动投影到物理有效空间，并引入运动一致性（MC）和基于标记的交互（MI）损失函数。

Result: 实验表明，该方法在生成运动质量上表现优异，物理保真度提升了3%-89%。

Conclusion: 物理映射方法显著提升了多人运动生成的物理合理性和真实性，为后续研究提供了新思路。

Abstract: Driven by advancements in motion capture and generative artificial
intelligence, leveraging large-scale MoCap datasets to train generative models
for synthesizing diverse, realistic human motions has become a promising
research direction. However, existing motion-capture techniques and generative
models often neglect physical constraints, leading to artifacts such as
interpenetration, sliding, and floating. These issues are exacerbated in
multi-person motion generation, where complex interactions are involved. To
address these limitations, we introduce physical mapping, integrated throughout
the human interaction generation pipeline. Specifically, motion imitation
within a physics-based simulation environment is used to project target motions
into a physically valid space. The resulting motions are adjusted to adhere to
real-world physics constraints while retaining their original semantic meaning.
This mapping not only improves MoCap data quality but also directly informs
post-processing of generated motions. Given the unique interactivity of
multi-person scenarios, we propose a tailored motion representation framework.
Motion Consistency (MC) and Marker-based Interaction (MI) loss functions are
introduced to improve model performance. Experiments show our method achieves
impressive results in generated human motion quality, with a 3%-89% improvement
in physical fidelity. Project page http://yw0208.github.io/physiinter

</details>


### [319] [GLOS: Sign Language Generation with Temporally Aligned Gloss-Level Conditioning](https://arxiv.org/abs/2506.07460)
*Taeryung Lee,Hyeongjin Nam,Gyeongsik Moon,Kyoung Mu Lee*

Main category: cs.CV

TL;DR: 论文提出GLOS框架，通过时间对齐的gloss级条件改进手语生成，解决了现有方法在词序和语义准确性上的问题。


<details>
  <summary>Details</summary>
Motivation: 现有手语生成方法因句子级条件导致词序错误和语义准确性低，需改进。

Method: 采用gloss级条件和时间对齐条件模块（TAC），实现细粒度控制和时序结构保留。

Result: 在CSL-Daily和Phoenix-2014T数据集上表现优于现有方法。

Conclusion: GLOS框架通过gloss级条件和TAC，显著提升了手语生成的词序和语义准确性。

Abstract: Sign language generation (SLG), or text-to-sign generation, bridges the gap
between signers and non-signers. Despite recent progress in SLG, existing
methods still often suffer from incorrect lexical ordering and low semantic
accuracy. This is primarily due to sentence-level condition, which encodes the
entire sentence of the input text into a single feature vector as a condition
for SLG. This approach fails to capture the temporal structure of sign language
and lacks the granularity of word-level semantics, often leading to disordered
sign sequences and ambiguous motions. To overcome these limitations, we propose
GLOS, a sign language generation framework with temporally aligned gloss-level
conditioning. First, we employ gloss-level conditions, which we define as
sequences of gloss embeddings temporally aligned with the motion sequence. This
enables the model to access both the temporal structure of sign language and
word-level semantics at each timestep. As a result, this allows for
fine-grained control of signs and better preservation of lexical order. Second,
we introduce a condition fusion module, temporal alignment conditioning (TAC),
to efficiently deliver the word-level semantic and temporal structure provided
by the gloss-level condition to the corresponding motion timesteps. Our method,
which is composed of gloss-level conditions and TAC, generates signs with
correct lexical order and high semantic accuracy, outperforming prior methods
on CSL-Daily and Phoenix-2014T.

</details>


### [320] [DeepVideo-R1: Video Reinforcement Fine-Tuning via Difficulty-aware Regressive GRPO](https://arxiv.org/abs/2506.07464)
*Jinyoung Park,Jeehye Na,Jinyoung Kim,Hyunwoo J. Kim*

Main category: cs.CV

TL;DR: 论文探讨了GRPO在视频大语言模型中的应用，提出了Reg-GRPO和难度感知数据增强策略，解决了依赖安全措施和优势消失问题，显著提升了视频推理性能。


<details>
  <summary>Details</summary>
Motivation: 研究GRPO在视频大语言模型中的应用，解决其学习效率低下的问题。

Method: 提出Reg-GRPO（回归式GRPO）和难度感知数据增强策略，直接预测优势值并动态增强训练样本。

Result: DeepVideo-R1在多个视频推理基准测试中显著提升了性能。

Conclusion: Reg-GRPO和难度感知数据增强策略有效解决了GRPO在视频大语言模型中的问题，提升了推理能力。

Abstract: Recent works have demonstrated the effectiveness of reinforcement learning
(RL)-based post-training in enhancing the reasoning capabilities of large
language models (LLMs). In particular, Group Relative Policy Optimization
(GRPO) has shown impressive success by employing a PPO-style reinforcement
algorithm with group-based normalized rewards. However, the application of GRPO
to Video Large Language Models (Video LLMs) has been less studied. In this
paper, we explore GRPO for video LLMs and identify two primary issues that
impede its effective learning: (1) reliance on safeguards, and (2) the
vanishing advantage problem. To mitigate these challenges, we propose
DeepVideo-R1, a video large language model trained with our proposed Reg-GRPO
(Regressive GRPO) and difficulty-aware data augmentation strategy. Reg-GRPO
reformulates the GRPO objective as a regression task, directly predicting the
advantage in GRPO. This design eliminates the need for safeguards like clipping
and min functions, thereby facilitating more direct policy guidance by aligning
the model with the advantage values. We also design the difficulty-aware data
augmentation strategy that dynamically augments training samples at solvable
difficulty levels, fostering diverse and informative reward signals. Our
comprehensive experiments show that DeepVideo-R1 significantly improves video
reasoning performance across multiple video reasoning benchmarks.

</details>


### [321] [Ambiguity-Restrained Text-Video Representation Learning for Partially Relevant Video Retrieval](https://arxiv.org/abs/2506.07471)
*CH Cho,WJ Moon,W Jun,MS Jung,JP Heo*

Main category: cs.CV

TL;DR: 论文提出了一种名为ARL的框架，通过多正对比学习和双重三元组边际损失，解决文本-视频对中的模糊性问题，并在PRVR任务中表现出色。


<details>
  <summary>Details</summary>
Motivation: 传统PRVR训练假设文本查询与视频为一对一关系，但实际存在文本与视频内容的模糊性。论文旨在将这种模糊性纳入模型学习过程。

Method: 提出ARL框架，基于不确定性和相似性检测模糊对，通过多正对比学习和双重三元组边际损失学习语义关系，并探索视频内细粒度关系。

Result: ARL在PRVR任务中表现出色，有效解决了文本-视频对的模糊性问题。

Conclusion: ARL通过检测模糊对和分层学习语义关系，提升了PRVR任务的性能，并减少了错误传播。

Abstract: Partially Relevant Video Retrieval~(PRVR) aims to retrieve a video where a
specific segment is relevant to a given text query. Typical training processes
of PRVR assume a one-to-one relationship where each text query is relevant to
only one video. However, we point out the inherent ambiguity between text and
video content based on their conceptual scope and propose a framework that
incorporates this ambiguity into the model learning process. Specifically, we
propose Ambiguity-Restrained representation Learning~(ARL) to address ambiguous
text-video pairs. Initially, ARL detects ambiguous pairs based on two criteria:
uncertainty and similarity. Uncertainty represents whether instances include
commonly shared context across the dataset, while similarity indicates
pair-wise semantic overlap. Then, with the detected ambiguous pairs, our ARL
hierarchically learns the semantic relationship via multi-positive contrastive
learning and dual triplet margin loss. Additionally, we delve into fine-grained
relationships within the video instances. Unlike typical training at the
text-video level, where pairwise information is provided, we address the
inherent ambiguity within frames of the same untrimmed video, which often
contains multiple contexts. This allows us to further enhance learning at the
text-frame level. Lastly, we propose cross-model ambiguity detection to
mitigate the error propagation that occurs when a single model is employed to
detect ambiguous pairs for its training. With all components combined, our
proposed method demonstrates its effectiveness in PRVR.

</details>


### [322] [CoCoA-Mix: Confusion-and-Confidence-Aware Mixture Model for Context Optimization](https://arxiv.org/abs/2506.07484)
*Dasol Hong,Wooju Lee,Hyun Myung*

Main category: cs.CV

TL;DR: CoCoA-Mix通过混淆感知损失（CoA-loss）和置信感知权重（CoA-weights）提升视觉语言模型的专用性和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 解决提示调优中因冻结编码器导致的特征不对齐和类别混淆问题。

Method: 提出CoA-loss优化决策边界，结合CoA-weights的混合模型增强泛化能力。

Result: CoCoA-Mix在专用性和泛化性上优于现有方法。

Conclusion: CoCoA-Mix通过混淆感知和置信感知机制显著提升了模型性能。

Abstract: Prompt tuning, which adapts vision-language models by freezing model
parameters and optimizing only the prompt, has proven effective for
task-specific adaptations. The core challenge in prompt tuning is improving
specialization for a specific task and generalization for unseen domains.
However, frozen encoders often produce misaligned features, leading to
confusion between classes and limiting specialization. To overcome this issue,
we propose a confusion-aware loss (CoA-loss) that improves specialization by
refining the decision boundaries between confusing classes. Additionally, we
mathematically demonstrate that a mixture model can enhance generalization
without compromising specialization. This is achieved using confidence-aware
weights (CoA-weights), which adjust the weights of each prediction in the
mixture model based on its confidence within the class domains. Extensive
experiments show that CoCoA-Mix, a mixture model with CoA-loss and CoA-weights,
outperforms state-of-the-art methods by enhancing specialization and
generalization. Our code is publicly available at
https://github.com/url-kaist/CoCoA-Mix.

</details>


### [323] [Drive Any Mesh: 4D Latent Diffusion for Mesh Deformation from Video](https://arxiv.org/abs/2506.07489)
*Yahao Shi,Yang Liu,Yanmin Wu,Xing Liu,Chen Zhao,Jie Luo,Bin Zhou*

Main category: cs.CV

TL;DR: DriveAnyMesh是一种基于单目视频驱动网格的方法，解决了当前4D生成技术在渲染引擎中的效率与兼容性问题。


<details>
  <summary>Details</summary>
Motivation: 现有4D生成技术效率低且兼容性差，骨骼方法需要大量人工且缺乏跨类别泛化能力。

Method: 提出4D扩散模型，通过去噪潜在集序列并解码为网格动画，利用基于变压器的变分自编码器捕捉3D形状和运动信息。

Result: 实验表明，DriveAnyMesh能快速生成高质量动画，兼容现代渲染引擎。

Conclusion: 该方法在游戏和电影行业有应用潜力。

Abstract: We propose DriveAnyMesh, a method for driving mesh guided by monocular video.
Current 4D generation techniques encounter challenges with modern rendering
engines. Implicit methods have low rendering efficiency and are unfriendly to
rasterization-based engines, while skeletal methods demand significant manual
effort and lack cross-category generalization. Animating existing 3D assets,
instead of creating 4D assets from scratch, demands a deep understanding of the
input's 3D structure. To tackle these challenges, we present a 4D diffusion
model that denoises sequences of latent sets, which are then decoded to produce
mesh animations from point cloud trajectory sequences. These latent sets
leverage a transformer-based variational autoencoder, simultaneously capturing
3D shape and motion information. By employing a spatiotemporal,
transformer-based diffusion model, information is exchanged across multiple
latent frames, enhancing the efficiency and generalization of the generated
results. Our experimental results demonstrate that DriveAnyMesh can rapidly
produce high-quality animations for complex motions and is compatible with
modern rendering engines. This method holds potential for applications in both
the gaming and filming industries.

</details>


### [324] [SpatialLM: Training Large Language Models for Structured Indoor Modeling](https://arxiv.org/abs/2506.07491)
*Yongsen Mao,Junhao Zhong,Chuan Fang,Jia Zheng,Rui Tang,Hao Zhu,Ping Tan,Zihan Zhou*

Main category: cs.CV

TL;DR: SpatialLM是一个处理3D点云数据的大语言模型，能生成结构化3D场景理解输出，如墙壁、门窗等建筑元素，并在公开基准测试中表现优异。


<details>
  <summary>Details</summary>
Motivation: 提升现代大语言模型的空间理解能力，以应用于增强现实、机器人等领域。

Method: 基于开源大语言模型微调，使用大规模合成数据集（12,328个室内场景）进行训练。

Result: 在布局估计任务中达到最优性能，3D物体检测结果具有竞争力。

Conclusion: 展示了增强现代大语言模型空间理解能力的可行路径。

Abstract: SpatialLM is a large language model designed to process 3D point cloud data
and generate structured 3D scene understanding outputs. These outputs include
architectural elements like walls, doors, windows, and oriented object boxes
with their semantic categories. Unlike previous methods which exploit
task-specific network designs, our model adheres to the standard multimodal LLM
architecture and is fine-tuned directly from open-source LLMs.
  To train SpatialLM, we collect a large-scale, high-quality synthetic dataset
consisting of the point clouds of 12,328 indoor scenes (54,778 rooms) with
ground-truth 3D annotations, and conduct a careful study on various modeling
and training decisions. On public benchmarks, our model gives state-of-the-art
performance in layout estimation and competitive results in 3D object
detection. With that, we show a feasible path for enhancing the spatial
understanding capabilities of modern LLMs for applications in augmented
reality, embodied robotics, and more.

</details>


### [325] [Genesis: Multimodal Driving Scene Generation with Spatio-Temporal and Cross-Modal Consistency](https://arxiv.org/abs/2506.07497)
*Xiangyu Guo,Zhanqian Wu,Kaixin Xiong,Ziyang Xu,Lijun Zhou,Gangwei Xu,Shaoqing Xu,Haiyang Sun,Bing Wang,Guang Chen,Hangjun Ye,Wenyu Liu,Xinggang Wang*

Main category: cs.CV

TL;DR: Genesis是一个统一框架，用于联合生成多视角驾驶视频和LiDAR序列，具有时空和跨模态一致性。


<details>
  <summary>Details</summary>
Motivation: 解决多模态数据生成中的一致性问题，提升生成数据的语义保真度和实用性。

Method: 采用两阶段架构，结合DiT视频扩散模型、3D-VAE编码、BEV感知LiDAR生成器和NeRF渲染，通过共享潜在空间耦合模态，并引入DataCrafter模块提供语义监督。

Result: 在nuScenes基准测试中表现优异（FVD 16.95, FID 4.24, Chamfer 0.611），并提升了下游任务性能。

Conclusion: Genesis在多模态数据生成中实现了高效一致性和语义保真，具有实际应用价值。

Abstract: We present Genesis, a unified framework for joint generation of multi-view
driving videos and LiDAR sequences with spatio-temporal and cross-modal
consistency. Genesis employs a two-stage architecture that integrates a
DiT-based video diffusion model with 3D-VAE encoding, and a BEV-aware LiDAR
generator with NeRF-based rendering and adaptive sampling. Both modalities are
directly coupled through a shared latent space, enabling coherent evolution
across visual and geometric domains. To guide the generation with structured
semantics, we introduce DataCrafter, a captioning module built on
vision-language models that provides scene-level and instance-level
supervision. Extensive experiments on the nuScenes benchmark demonstrate that
Genesis achieves state-of-the-art performance across video and LiDAR metrics
(FVD 16.95, FID 4.24, Chamfer 0.611), and benefits downstream tasks including
segmentation and 3D detection, validating the semantic fidelity and practical
utility of the generated data.

</details>


### [326] [MoQAE: Mixed-Precision Quantization for Long-Context LLM Inference via Mixture of Quantization-Aware Experts](https://arxiv.org/abs/2506.07533)
*Wei Tao,Haocheng Lu,Xiaoyang Qu,Bin Zhang,Kai Lu,Jiguang Wan,Jianzong Wang*

Main category: cs.CV

TL;DR: MoQAE是一种新型混合精度量化方法，通过量化感知专家混合（MoE）优化KV缓存的内存使用，同时兼顾效率和效果。


<details>
  <summary>Details</summary>
Motivation: 优化大型语言模型（LLM）长上下文推理中的KV缓存高内存消耗问题。

Method: 1. 将不同量化位宽配置视为专家，采用MoE选择最优配置；2. 分块输入令牌以提高效率；3. 设计轻量级路由器微调过程；4. 引入路由冻结和共享机制。

Result: 在多个基准数据集上表现优于现有KV缓存量化方法。

Conclusion: MoQAE在效率和效果上均优于现有方法，为LLM优化提供了新思路。

Abstract: One of the primary challenges in optimizing large language models (LLMs) for
long-context inference lies in the high memory consumption of the Key-Value
(KV) cache. Existing approaches, such as quantization, have demonstrated
promising results in reducing memory usage. However, current quantization
methods cannot take both effectiveness and efficiency into account. In this
paper, we propose MoQAE, a novel mixed-precision quantization method via
mixture of quantization-aware experts. First, we view different quantization
bit-width configurations as experts and use the traditional mixture of experts
(MoE) method to select the optimal configuration. To avoid the inefficiency
caused by inputting tokens one by one into the router in the traditional MoE
method, we input the tokens into the router chunk by chunk. Second, we design a
lightweight router-only fine-tuning process to train MoQAE with a comprehensive
loss to learn the trade-off between model accuracy and memory usage. Finally,
we introduce a routing freezing (RF) and a routing sharing (RS) mechanism to
further reduce the inference overhead. Extensive experiments on multiple
benchmark datasets demonstrate that our method outperforms state-of-the-art KV
cache quantization approaches in both efficiency and effectiveness.

</details>


### [327] [Domain Randomization for Object Detection in Manufacturing Applications using Synthetic Data: A Comprehensive Study](https://arxiv.org/abs/2506.07539)
*Xiaomeng Zhu,Jacob Henningsson,Duruo Li,Pär Mårtensson,Lars Hanson,Mårten Björkman,Atsuto Maki*

Main category: cs.CV

TL;DR: 论文提出了一种用于制造业目标检测的合成数据生成方法，通过域随机化技术生成多样化的数据，并在公开数据集上验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 解决制造业目标检测中合成数据生成的关键问题，以提升模型在真实场景中的泛化能力。

Method: 构建了一个综合数据生成管道，涵盖对象特征、背景、光照、相机设置和后处理，并引入了SIP15-OD数据集。

Result: 在Yolov8模型上，合成数据训练的模型在公开数据集上表现优异，mAP@50得分高达96.4%。

Conclusion: 域随机化技术能有效生成接近真实数据分布的合成数据，为制造业目标检测提供了可行方案。

Abstract: This paper addresses key aspects of domain randomization in generating
synthetic data for manufacturing object detection applications. To this end, we
present a comprehensive data generation pipeline that reflects different
factors: object characteristics, background, illumination, camera settings, and
post-processing. We also introduce the Synthetic Industrial Parts Object
Detection dataset (SIP15-OD) consisting of 15 objects from three industrial use
cases under varying environments as a test bed for the study, while also
employing an industrial dataset publicly available for robotic applications. In
our experiments, we present more abundant results and insights into the
feasibility as well as challenges of sim-to-real object detection. In
particular, we identified material properties, rendering methods,
post-processing, and distractors as important factors. Our method, leveraging
these, achieves top performance on the public dataset with Yolov8 models
trained exclusively on synthetic data; mAP@50 scores of 96.4% for the robotics
dataset, and 94.1%, 99.5%, and 95.3% across three of the SIP15-OD use cases,
respectively. The results showcase the effectiveness of the proposed domain
randomization, potentially covering the distribution close to real data for the
applications.

</details>


### [328] [APTOS-2024 challenge report: Generation of synthetic 3D OCT images from fundus photographs](https://arxiv.org/abs/2506.07542)
*Bowen Liu,Weiyi Zhang,Peranut Chotcomwongse,Xiaolan Chen,Ruoyu Chen,Pawin Pakaymaskul,Niracha Arjkongharn,Nattaporn Vongsa,Xuelian Cheng,Zongyuan Ge,Kun Huang,Xiaohui Li,Yiru Duan,Zhenbang Wang,BaoYe Xie,Qiang Chen,Huazhu Fu,Michael A. Mahr,Jiaqi Qu,Wangyiyang Chen,Shiye Wang,Yubo Tan,Yongjie Li,Mingguang He,Danli Shi,Paisan Ruamviboonsuk*

Main category: cs.CV

TL;DR: APTOS-2024挑战赛展示了从2D眼底图像生成3D OCT图像的可行性，旨在提高眼科医疗的可及性。


<details>
  <summary>Details</summary>
Motivation: 解决OCT设备成本高、操作复杂的问题，利用2D眼底图像的便捷性生成3D OCT图像。

Method: 挑战赛框架包括基准数据集、评估方法（图像和视频距离指标），并分析了42个初步提交和9个决赛方案。

Result: 领先方法结合了数据预处理、预训练、视觉基础模型和架构改进，342个团队参与。

Conclusion: 该挑战首次证明了从眼底图像生成3D OCT的可行性，有望提升资源匮乏地区的眼科医疗水平。

Abstract: Optical Coherence Tomography (OCT) provides high-resolution, 3D, and
non-invasive visualization of retinal layers in vivo, serving as a critical
tool for lesion localization and disease diagnosis. However, its widespread
adoption is limited by equipment costs and the need for specialized operators.
In comparison, 2D color fundus photography offers faster acquisition and
greater accessibility with less dependence on expensive devices. Although
generative artificial intelligence has demonstrated promising results in
medical image synthesis, translating 2D fundus images into 3D OCT images
presents unique challenges due to inherent differences in data dimensionality
and biological information between modalities. To advance generative models in
the fundus-to-3D-OCT setting, the Asia Pacific Tele-Ophthalmology Society
(APTOS-2024) organized a challenge titled Artificial Intelligence-based OCT
Generation from Fundus Images. This paper details the challenge framework
(referred to as APTOS-2024 Challenge), including: the benchmark dataset,
evaluation methodology featuring two fidelity metrics-image-based distance
(pixel-level OCT B-scan similarity) and video-based distance (semantic-level
volumetric consistency), and analysis of top-performing solutions. The
challenge attracted 342 participating teams, with 42 preliminary submissions
and 9 finalists. Leading methodologies incorporated innovations in hybrid data
preprocessing or augmentation (cross-modality collaborative paradigms),
pre-training on external ophthalmic imaging datasets, integration of vision
foundation models, and model architecture improvement. The APTOS-2024 Challenge
is the first benchmark demonstrating the feasibility of fundus-to-3D-OCT
synthesis as a potential solution for improving ophthalmic care accessibility
in under-resourced healthcare settings, while helping to expedite medical
research and clinical applications.

</details>


### [329] [Synthesize Privacy-Preserving High-Resolution Images via Private Textual Intermediaries](https://arxiv.org/abs/2506.07555)
*Haoxiang Wang,Zinan Lin,Da Yu,Huishuai Zhang*

Main category: cs.CV

TL;DR: SPTI方法通过文本中介生成高分辨率差分隐私图像，无需训练模型，显著提升图像质量。


<details>
  <summary>Details</summary>
Motivation: 现有差分隐私图像合成方法难以生成高分辨率且忠实于原始数据的图像，SPTI旨在解决这一问题。

Method: SPTI将图像转换为文本描述，利用差分隐私文本生成方法生成隐私保护文本，再通过文本到图像模型重建图像。

Result: 在LSUN Bedroom和MM CelebA HQ数据集上，SPTI的FID显著优于现有方法（如26.71 vs 40.36）。

Conclusion: SPTI提供了一种资源高效且兼容现有模型的高分辨率差分隐私图像生成框架。

Abstract: Generating high fidelity, differentially private (DP) synthetic images offers
a promising route to share and analyze sensitive visual data without
compromising individual privacy. However, existing DP image synthesis methods
struggle to produce high resolution outputs that faithfully capture the
structure of the original data. In this paper, we introduce a novel method,
referred to as Synthesis via Private Textual Intermediaries (SPTI), that can
generate high resolution DP images with easy adoption. The key idea is to shift
the challenge of DP image synthesis from the image domain to the text domain by
leveraging state of the art DP text generation methods. SPTI first summarizes
each private image into a concise textual description using image to text
models, then applies a modified Private Evolution algorithm to generate DP
text, and finally reconstructs images using text to image models. Notably, SPTI
requires no model training, only inference with off the shelf models. Given a
private dataset, SPTI produces synthetic images of substantially higher quality
than prior DP approaches. On the LSUN Bedroom dataset, SPTI attains an FID less
than or equal to 26.71 under epsilon equal to 1.0, improving over Private
Evolution FID of 40.36. Similarly, on MM CelebA HQ, SPTI achieves an FID less
than or equal to 33.27 at epsilon equal to 1.0, compared to 57.01 from DP fine
tuning baselines. Overall, our results demonstrate that Synthesis via Private
Textual Intermediaries provides a resource efficient and proprietary model
compatible framework for generating high resolution DP synthetic images,
greatly expanding access to private visual datasets.

</details>


### [330] [Cross-channel Perception Learning for H&E-to-IHC Virtual Staining](https://arxiv.org/abs/2506.07559)
*Hao Yang,JianYu Wu,Run Fang,Xuelian Zhao,Yuan Ji,Zhiyu Chen,Guibin He,Junceng Guo,Yang Liu,Xinhua Zeng*

Main category: cs.CV

TL;DR: 提出了一种跨通道感知学习（CCPL）策略，用于解决H&E-to-IHC研究中忽略的细胞核与细胞膜跨通道相关性，通过双通道特征提取和特征蒸馏损失提升虚拟染色质量。


<details>
  <summary>Details</summary>
Motivation: 现有H&E-to-IHC研究常忽略细胞核与细胞膜的跨通道相关性，限制了病理图像分析与诊断的准确性。

Method: CCPL分解HER2免疫组化染色为Hematoxylin和DAB通道，利用Gigapath的Tile Encoder提取双通道特征并计算跨通道相关性，同时通过特征蒸馏损失和光学密度统计分析提升模型性能。

Result: 实验表明CCPL在PSNR、SSIM、PCC和FID等指标上表现优异，生成的虚拟染色图像质量高，且病理特征保留良好。

Conclusion: CCPL为多媒体医疗数据驱动的自动化病理诊断提供了有效支持。

Abstract: With the rapid development of digital pathology, virtual staining has become
a key technology in multimedia medical information systems, offering new
possibilities for the analysis and diagnosis of pathological images. However,
existing H&E-to-IHC studies often overlook the cross-channel correlations
between cell nuclei and cell membranes. To address this issue, we propose a
novel Cross-Channel Perception Learning (CCPL) strategy. Specifically, CCPL
first decomposes HER2 immunohistochemical staining into Hematoxylin and DAB
staining channels, corresponding to cell nuclei and cell membranes,
respectively. Using the pathology foundation model Gigapath's Tile Encoder,
CCPL extracts dual-channel features from both the generated and real images and
measures cross-channel correlations between nuclei and membranes. The features
of the generated and real stained images, obtained through the Tile Encoder,
are also used to calculate feature distillation loss, enhancing the model's
feature extraction capabilities without increasing the inference burden.
Additionally, CCPL performs statistical analysis on the focal optical density
maps of both single channels to ensure consistency in staining distribution and
intensity. Experimental results, based on quantitative metrics such as PSNR,
SSIM, PCC, and FID, along with professional evaluations from pathologists,
demonstrate that CCPL effectively preserves pathological features, generates
high-quality virtual stained images, and provides robust support for automated
pathological diagnosis using multimedia medical data.

</details>


### [331] [OpenDance: Multimodal Controllable 3D Dance Generation Using Large-scale Internet Data](https://arxiv.org/abs/2506.07565)
*Jinlu Zhang,Zixi Kang,Yizhou Wang*

Main category: cs.CV

TL;DR: 论文提出OpenDance5D数据集和OpenDanceNet框架，解决音乐驱动舞蹈生成的多样性和可控性问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法因缺乏细粒度多模态数据和灵活多条件生成能力，限制了舞蹈生成的多样性和可控性。

Method: 构建OpenDance5D数据集（14种舞蹈类型，101小时数据，5种模态），并提出OpenDanceNet框架，基于掩码建模实现多条件可控生成。

Result: OpenDanceNet实现了高保真和灵活可控的舞蹈生成。

Conclusion: OpenDance5D和OpenDanceNet为音乐驱动舞蹈生成提供了数据和方法支持，显著提升了生成效果。

Abstract: Music-driven dance generation offers significant creative potential yet faces
considerable challenges. The absence of fine-grained multimodal data and the
difficulty of flexible multi-conditional generation limit previous works on
generation controllability and diversity in practice. In this paper, we build
OpenDance5D, an extensive human dance dataset comprising over 101 hours across
14 distinct genres. Each sample has five modalities to facilitate robust
cross-modal learning: RGB video, audio, 2D keypoints, 3D motion, and
fine-grained textual descriptions from human arts. Furthermore, we propose
OpenDanceNet, a unified masked modeling framework for controllable dance
generation conditioned on music and arbitrary combinations of text prompts,
keypoints, or character positioning. Comprehensive experiments demonstrate that
OpenDanceNet achieves high-fidelity and flexible controllability.

</details>


### [332] [Towards the Influence of Text Quantity on Writer Retrieval](https://arxiv.org/abs/2506.07566)
*Marco Peer,Robert Sablatnig,Florian Kleber*

Main category: cs.CV

TL;DR: 本文研究了基于手写相似性的作者检索任务，探讨了文本量对检索性能的影响，发现即使仅使用一行文本，检索准确率仍能保持较高水平。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要关注页面级检索，本文旨在探索文本量（如行级和词级）对作者检索性能的影响。

Method: 评估了三种先进的作者检索系统（包括手工特征和深度学习方法），并在CVL和IAM数据集上测试了不同文本量的性能。

Result: 实验表明，仅使用一行文本时性能下降20-30%，但包含至少四行文本时，检索准确率仍能达到全页性能的90%以上。深度学习方法在低文本量场景下表现更优。

Conclusion: 文本依赖性检索在低文本量场景下仍能保持较强性能，深度学习方法优于传统手工特征方法。

Abstract: This paper investigates the task of writer retrieval, which identifies
documents authored by the same individual within a dataset based on handwriting
similarities. While existing datasets and methodologies primarily focus on page
level retrieval, we explore the impact of text quantity on writer retrieval
performance by evaluating line- and word level retrieval. We examine three
state-of-the-art writer retrieval systems, including both handcrafted and deep
learning-based approaches, and analyze their performance using varying amounts
of text. Our experiments on the CVL and IAM dataset demonstrate that while
performance decreases by 20-30% when only one line of text is used as query and
gallery, retrieval accuracy remains above 90% of full-page performance when at
least four lines are included. We further show that text-dependent retrieval
can maintain strong performance in low-text scenarios. Our findings also
highlight the limitations of handcrafted features in low-text scenarios, with
deep learning-based methods like NetVLAD outperforming traditional VLAD
encoding.

</details>


### [333] [LLM-driven Indoor Scene Layout Generation via Scaled Human-aligned Data Synthesis and Multi-Stage Preference Optimization](https://arxiv.org/abs/2506.07570)
*Yixuan Yang,Zhen Luo,Tongsheng Ding,Junru Lu,Mingqi Gao,Jinyu Yang,Victor Sanchez,Feng Zheng*

Main category: cs.CV

TL;DR: 论文提出了一种基于LLM的室内布局生成方法3D-SynthPlace和OptiScene，通过合成数据集和两阶段训练优化布局生成质量。


<details>
  <summary>Details</summary>
Motivation: 现有方法在空间一致性和泛化能力上存在不足，需要改进室内布局生成的准确性和多样性。

Method: 结合合成数据集3D-SynthPlace，采用两阶段训练（监督微调和直接偏好优化）优化LLM模型OptiScene。

Result: OptiScene在布局生成质量和成功率上优于传统方法，并展示出在交互任务中的潜力。

Conclusion: 3D-SynthPlace和OptiScene为室内布局生成提供了高效且高质量的解决方案。

Abstract: Automatic indoor layout generation has attracted increasing attention due to
its potential in interior design, virtual environment construction, and
embodied AI. Existing methods fall into two categories: prompt-driven
approaches that leverage proprietary LLM services (e.g., GPT APIs) and
learning-based methods trained on layout data upon diffusion-based models.
Prompt-driven methods often suffer from spatial inconsistency and high
computational costs, while learning-based methods are typically constrained by
coarse relational graphs and limited datasets, restricting their generalization
to diverse room categories. In this paper, we revisit LLM-based indoor layout
generation and present 3D-SynthPlace, a large-scale dataset that combines
synthetic layouts generated via a 'GPT synthesize, Human inspect' pipeline,
upgraded from the 3D-Front dataset. 3D-SynthPlace contains nearly 17,000
scenes, covering four common room types -- bedroom, living room, kitchen, and
bathroom -- enriched with diverse objects and high-level spatial annotations.
We further introduce OptiScene, a strong open-source LLM optimized for indoor
layout generation, fine-tuned based on our 3D-SynthPlace dataset through our
two-stage training. For the warum-up stage I, we adopt supervised fine-tuning
(SFT), which is taught to first generate high-level spatial descriptions then
conditionally predict concrete object placements. For the reinforcing stage II,
to better align the generated layouts with human design preferences, we apply
multi-turn direct preference optimization (DPO), which significantly improving
layout quality and generation success rates. Extensive experiments demonstrate
that OptiScene outperforms traditional prompt-driven and learning-based
baselines. Moreover, OptiScene shows promising potential in interactive tasks
such as scene editing and robot navigation.

</details>


### [334] [Learning Speaker-Invariant Visual Features for Lipreading](https://arxiv.org/abs/2506.07572)
*Yu Li,Feng Xue,Shujie Li,Jinrui Zhang,Shuang Yang,Dan Guo,Richang Hong*

Main category: cs.CV

TL;DR: SIFLip是一种新的唇读框架，通过解耦说话者特定特征提升模型泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有唇读方法提取的视觉特征包含说话者特定属性（如唇形、颜色、纹理），导致虚假相关性，影响准确性和泛化能力。

Method: SIFLip使用隐式解耦和显式解耦模块，分别通过文本嵌入监督和说话者识别子任务解耦说话者特定特征。

Result: SIFLip在多个公开数据集上显著提升泛化性能，优于现有方法。

Conclusion: SIFLip通过解耦说话者特定特征，有效提高了唇读模型的泛化能力和准确性。

Abstract: Lipreading is a challenging cross-modal task that aims to convert visual lip
movements into spoken text. Existing lipreading methods often extract visual
features that include speaker-specific lip attributes (e.g., shape, color,
texture), which introduce spurious correlations between vision and text. These
correlations lead to suboptimal lipreading accuracy and restrict model
generalization. To address this challenge, we introduce SIFLip, a
speaker-invariant visual feature learning framework that disentangles
speaker-specific attributes using two complementary disentanglement modules
(Implicit Disentanglement and Explicit Disentanglement) to improve
generalization. Specifically, since different speakers exhibit semantic
consistency between lip movements and phonetic text when pronouncing the same
words, our implicit disentanglement module leverages stable text embeddings as
supervisory signals to learn common visual representations across speakers,
implicitly decoupling speaker-specific features. Additionally, we design a
speaker recognition sub-task within the main lipreading pipeline to filter
speaker-specific features, then further explicitly disentangle these
personalized visual features from the backbone network via gradient reversal.
Experimental results demonstrate that SIFLip significantly enhances
generalization performance across multiple public datasets. Experimental
results demonstrate that SIFLip significantly improves generalization
performance across multiple public datasets, outperforming state-of-the-art
methods.

</details>


### [335] [Uncertainty-o: One Model-agnostic Framework for Unveiling Uncertainty in Large Multimodal Models](https://arxiv.org/abs/2506.07575)
*Ruiyang Zhang,Hu Zhang,Hao Fei,Zhedong Zheng*

Main category: cs.CV

TL;DR: 论文提出Uncertainty-o框架，用于统一评估和量化多模态模型（LMMs）的不确定性，并通过实验验证其在多种任务中的有效性。


<details>
  <summary>Details</summary>
Motivation: 尽管多模态模型（LMMs）被认为比纯语言模型（LLMs）更鲁棒，但其不确定性评估仍存在三个关键问题：统一评估方法、如何提示LMMs显示不确定性，以及如何量化不确定性以支持下游任务。

Method: 提出Uncertainty-o框架，包括模型无关的评估方法、多模态提示扰动实验，以及多模态语义不确定性的量化公式。

Result: 在18个基准测试和10种LMMs上的实验表明，Uncertainty-o能可靠估计LMMs的不确定性，并提升下游任务（如幻觉检测和缓解）的性能。

Conclusion: Uncertainty-o为LMMs的不确定性评估提供了统一且有效的解决方案，对实际应用具有重要意义。

Abstract: Large Multimodal Models (LMMs), harnessing the complementarity among diverse
modalities, are often considered more robust than pure Language Large Models
(LLMs); yet do LMMs know what they do not know? There are three key open
questions remaining: (1) how to evaluate the uncertainty of diverse LMMs in a
unified manner, (2) how to prompt LMMs to show its uncertainty, and (3) how to
quantify uncertainty for downstream tasks. In an attempt to address these
challenges, we introduce Uncertainty-o: (1) a model-agnostic framework designed
to reveal uncertainty in LMMs regardless of their modalities, architectures, or
capabilities, (2) an empirical exploration of multimodal prompt perturbations
to uncover LMM uncertainty, offering insights and findings, and (3) derive the
formulation of multimodal semantic uncertainty, which enables quantifying
uncertainty from multimodal responses. Experiments across 18 benchmarks
spanning various modalities and 10 LMMs (both open- and closed-source)
demonstrate the effectiveness of Uncertainty-o in reliably estimating LMM
uncertainty, thereby enhancing downstream tasks such as hallucination
detection, hallucination mitigation, and uncertainty-aware Chain-of-Thought
reasoning.

</details>


### [336] [Super Encoding Network: Recursive Association of Multi-Modal Encoders for Video Understanding](https://arxiv.org/abs/2506.07576)
*Boyu Chen,Siran Chen,Kunchang Li,Qinglin Xu,Yu Qiao,Yali Wang*

Main category: cs.CV

TL;DR: 提出了一种统一的超级编码网络（SEN），通过递归关联多模态编码器，提升视频理解任务的表现。


<details>
  <summary>Details</summary>
Motivation: 当前多模态基础模型仅通过对比学习对齐不同模态的编码器，缺乏更深层次的多模态交互，限制了复杂视频场景的理解能力。

Method: 将预训练编码器视为“超级神经元”，设计递归关联（RA）块，逐步融合多模态信息，实现知识整合、分发和提示。

Result: 在跟踪、识别、聊天和编辑等任务中表现优异，例如像素级跟踪的Jaccard指数提升2.7%，视频编辑的文本对齐提升6.4%。

Conclusion: SEN通过递归多模态交互显著提升了视频理解能力，为复杂场景建模提供了有效解决方案。

Abstract: Video understanding has been considered as one critical step towards world
modeling, which is an important long-term problem in AI research. Recently,
multi-modal foundation models have shown such potential via large-scale
pretraining. However, these models simply align encoders of different
modalities via contrastive learning, while lacking deeper multi-modal
interactions, which is critical for understanding complex target movements with
diversified video scenes. To fill this gap, we propose a unified Super Encoding
Network (SEN) for video understanding, which builds up such distinct
interactions through recursive association of multi-modal encoders in the
foundation models. Specifically, we creatively treat those well-trained
encoders as "super neurons" in our SEN. Via designing a Recursive Association
(RA) block, we progressively fuse multi-modalities with the input video, based
on knowledge integrating, distributing, and prompting of super neurons in a
recursive manner. In this way, our SEN can effectively encode deeper
multi-modal interactions, for prompting various video understanding tasks in
downstream. Extensive experiments show that, our SEN can remarkably boost the
four most representative video tasks, including tracking, recognition,
chatting, and editing, e.g., for pixel-level tracking, the average jaccard
index improves 2.7%, temporal coherence(TC) drops 8.8% compared to the popular
CaDeX++ approach. For one-shot video editing, textual alignment improves 6.4%,
and frame consistency increases 4.1% compared to the popular TuneA-Video
approach.

</details>


### [337] [Explore the vulnerability of black-box models via diffusion models](https://arxiv.org/abs/2506.07590)
*Jiacheng Shi,Yanfu Zhang,Huajie Shao,Ashley Gao*

Main category: cs.CV

TL;DR: 研究发现扩散模型API可能被用于生成合成图像，进而训练替代模型，从而对黑盒分类模型实施攻击。


<details>
  <summary>Details</summary>
Motivation: 扩散模型的高保真图像生成能力可能被恶意利用，引发安全和隐私问题。

Method: 利用扩散模型API生成合成图像，训练替代模型，实施模型提取和对抗攻击。

Result: 在七个基准测试中，方法性能提升27.37%，查询预算仅为0.01倍，对抗攻击成功率98.68%。

Conclusion: 扩散模型API可能成为新的安全威胁，需加强防范。

Abstract: Recent advancements in diffusion models have enabled high-fidelity and
photorealistic image generation across diverse applications. However, these
models also present security and privacy risks, including copyright violations,
sensitive information leakage, and the creation of harmful or offensive content
that could be exploited maliciously. In this study, we uncover a novel security
threat where an attacker leverages diffusion model APIs to generate synthetic
images, which are then used to train a high-performing substitute model. This
enables the attacker to execute model extraction and transfer-based adversarial
attacks on black-box classification models with minimal queries, without
needing access to the original training data. The generated images are
sufficiently high-resolution and diverse to train a substitute model whose
outputs closely match those of the target model. Across the seven benchmarks,
including CIFAR and ImageNet subsets, our method shows an average improvement
of 27.37% over state-of-the-art methods while using just 0.01 times of the
query budget, achieving a 98.68% success rate in adversarial attacks on the
target model.

</details>


### [338] [SceneRAG: Scene-level Retrieval-Augmented Generation for Video Understanding](https://arxiv.org/abs/2506.07600)
*Nianbo Zeng,Haowen Hou,Fei Richard Yu,Si Shi,Ying Tiffany He*

Main category: cs.CV

TL;DR: SceneRAG是一个基于大语言模型的视频理解框架，通过分割视频为叙事一致的场景，结合视觉和文本信息构建知识图谱，显著提升了长视频内容的理解能力。


<details>
  <summary>Details</summary>
Motivation: 当前检索增强生成（RAG）方法在处理长视频时，固定长度分段破坏了上下文连续性，且无法准确捕捉场景边界。

Method: SceneRAG利用ASR转录和时间元数据分割视频为叙事一致的场景，通过轻量级启发式和迭代修正优化边界，并融合多模态信息构建动态知识图谱。

Result: 在LongerVideos基准测试中，SceneRAG以72.5%的胜率显著优于现有基线。

Conclusion: SceneRAG通过场景分割和多模态融合，有效解决了长视频理解的挑战。

Abstract: Despite recent advances in retrieval-augmented generation (RAG) for video
understanding, effectively understanding long-form video content remains
underexplored due to the vast scale and high complexity of video data. Current
RAG approaches typically segment videos into fixed-length chunks, which often
disrupts the continuity of contextual information and fails to capture
authentic scene boundaries. Inspired by the human ability to naturally organize
continuous experiences into coherent scenes, we present SceneRAG, a unified
framework that leverages large language models to segment videos into
narrative-consistent scenes by processing ASR transcripts alongside temporal
metadata. SceneRAG further sharpens these initial boundaries through
lightweight heuristics and iterative correction. For each scene, the framework
fuses information from both visual and textual modalities to extract entity
relations and dynamically builds a knowledge graph, enabling robust multi-hop
retrieval and generation that account for long-range dependencies. Experiments
on the LongerVideos benchmark, featuring over 134 hours of diverse content,
confirm that SceneRAG substantially outperforms prior baselines, achieving a
win rate of up to 72.5 percent on generation tasks.

</details>


### [339] [SurgBench: A Unified Large-Scale Benchmark for Surgical Video Analysis](https://arxiv.org/abs/2506.07603)
*Jianhui Wei,Zikai Xiao,Danyu Sun,Luqi Gong,Zongxin Yang,Zuozhu Liu,Jian Wu*

Main category: cs.CV

TL;DR: SurgBench是一个统一的手术视频基准框架，包含预训练数据集SurgBench-P和评估基准SurgBench-E，旨在解决手术视频基础模型开发中的数据稀缺问题。


<details>
  <summary>Details</summary>
Motivation: 手术视频理解对自动化术中决策、技能评估和术后质量改进至关重要，但缺乏大规模多样化数据集阻碍了进展。

Method: 提出SurgBench框架，包含预训练数据集SurgBench-P（5300万帧，22种手术）和评估基准SurgBench-E（72个任务）。

Result: 现有视频基础模型在多样化任务中泛化能力差，而基于SurgBench-P的预训练显著提升了性能。

Conclusion: SurgBench为手术视频分析提供了全面的数据集和评估标准，显著提升了模型的泛化能力。

Abstract: Surgical video understanding is pivotal for enabling automated intraoperative
decision-making, skill assessment, and postoperative quality improvement.
However, progress in developing surgical video foundation models (FMs) remains
hindered by the scarcity of large-scale, diverse datasets for pretraining and
systematic evaluation. In this paper, we introduce \textbf{SurgBench}, a
unified surgical video benchmarking framework comprising a pretraining dataset,
\textbf{SurgBench-P}, and an evaluation benchmark, \textbf{SurgBench-E}.
SurgBench offers extensive coverage of diverse surgical scenarios, with
SurgBench-P encompassing 53 million frames across 22 surgical procedures and 11
specialties, and SurgBench-E providing robust evaluation across six categories
(phase classification, camera motion, tool recognition, disease diagnosis,
action classification, and organ detection) spanning 72 fine-grained tasks.
Extensive experiments reveal that existing video FMs struggle to generalize
across varied surgical video analysis tasks, whereas pretraining on SurgBench-P
yields substantial performance improvements and superior cross-domain
generalization to unseen procedures and modalities. Our dataset and code are
available upon request.

</details>


### [340] [DragNeXt: Rethinking Drag-Based Image Editing](https://arxiv.org/abs/2506.07611)
*Yuan Zhou,Junbao Zhou,Qingshan Xu,Kesen Zhao,Yuxuan Wang,Hao Fei,Richang Hong,Hanwang Zhang*

Main category: cs.CV

TL;DR: DragNeXt提出了一种新的基于拖拽的图像编辑方法，通过区域优化和渐进式干预简化流程并提升质量。


<details>
  <summary>Details</summary>
Motivation: 现有拖拽编辑方法存在点拖拽模糊性和繁琐流程问题，需要更高效、明确的解决方案。

Method: 将拖拽编辑重新定义为区域变形、旋转和平移，提出Latent Region Optimization和Progressive Backward Self-Intervention框架。

Result: DragNeXt在NextBench上显著优于现有方法，生成更高质量结果。

Conclusion: DragNeXt通过区域优化和渐进干预有效解决了拖拽编辑的模糊性和质量问题。

Abstract: Drag-Based Image Editing (DBIE), which allows users to manipulate images by
directly dragging objects within them, has recently attracted much attention
from the community. However, it faces two key challenges:
(\emph{\textcolor{magenta}{i}}) point-based drag is often highly ambiguous and
difficult to align with users' intentions; (\emph{\textcolor{magenta}{ii}})
current DBIE methods primarily rely on alternating between motion supervision
and point tracking, which is not only cumbersome but also fails to produce
high-quality results. These limitations motivate us to explore DBIE from a new
perspective -- redefining it as deformation, rotation, and translation of
user-specified handle regions. Thereby, by requiring users to explicitly
specify both drag areas and types, we can effectively address the ambiguity
issue. Furthermore, we propose a simple-yet-effective editing framework, dubbed
\textcolor{SkyBlue}{\textbf{DragNeXt}}. It unifies DBIE as a Latent Region
Optimization (LRO) problem and solves it through Progressive Backward
Self-Intervention (PBSI), simplifying the overall procedure of DBIE while
further enhancing quality by fully leveraging region-level structure
information and progressive guidance from intermediate drag states. We validate
\textcolor{SkyBlue}{\textbf{DragNeXt}} on our NextBench, and extensive
experiments demonstrate that our proposed method can significantly outperform
existing approaches. Code will be released on github.

</details>


### [341] [Scaling Human Activity Recognition: A Comparative Evaluation of Synthetic Data Generation and Augmentation Techniques](https://arxiv.org/abs/2506.07612)
*Zikang Leng,Archith Iyer,Thomas Plötz*

Main category: cs.CV

TL;DR: 论文比较了两种虚拟IMU数据生成方法与传统数据增强技术，发现虚拟IMU数据在有限数据条件下显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 解决HAR中标记数据稀缺的问题，探索视频和语言两种跨模态生成虚拟IMU数据的有效性。

Method: 构建大规模虚拟IMU数据集，比较视频、语言生成方法与传统数据增强，评估四种模型在三个基准数据集上的表现。

Result: 虚拟IMU数据显著优于真实或增强数据，尤其在数据有限时。

Conclusion: 提供选择数据生成策略的实用建议，并分析各方法的优缺点。

Abstract: Human activity recognition (HAR) is often limited by the scarcity of labeled
datasets due to the high cost and complexity of real-world data collection. To
mitigate this, recent work has explored generating virtual inertial measurement
unit (IMU) data via cross-modality transfer. While video-based and
language-based pipelines have each shown promise, they differ in assumptions
and computational cost. Moreover, their effectiveness relative to traditional
sensor-level data augmentation remains unclear. In this paper, we present a
direct comparison between these two virtual IMU generation approaches against
classical data augmentation techniques. We construct a large-scale virtual IMU
dataset spanning 100 diverse activities from Kinetics-400 and simulate sensor
signals at 22 body locations. The three data generation strategies are
evaluated on benchmark HAR datasets (UTD-MHAD, PAMAP2, HAD-AW) using four
popular models. Results show that virtual IMU data significantly improves
performance over real or augmented data alone, particularly under limited-data
conditions. We offer practical guidance on choosing data generation strategies
and highlight the distinct advantages and disadvantages of each approach.

</details>


### [342] [Event-Priori-Based Vision-Language Model for Efficient Visual Understanding](https://arxiv.org/abs/2506.07627)
*Haotong Qin,Cheng Hu,Michele Magno*

Main category: cs.CV

TL;DR: EP-VLM利用动态事件视觉的运动先验，通过稀疏化视觉输入和保留位置信息的策略，显著提升了视觉语言模型的效率，同时保持高精度。


<details>
  <summary>Details</summary>
Motivation: 现有视觉语言模型（VLM）在资源受限的边缘设备上部署时，因处理冗余视觉信息导致计算效率低下。

Method: EP-VLM通过事件数据引导视觉输入的稀疏化，并采用位置保留的标记化策略，优化计算资源分配。

Result: 实验显示，EP-VLM在Qwen2-VL系列模型上实现了50%的计算量节省，同时保持98%的原始精度。

Conclusion: 事件视觉先验可显著提升VLM效率，为边缘设备上的可持续视觉理解提供新方向。

Abstract: Large Language Model (LLM)-based Vision-Language Models (VLMs) have
substantially extended the boundaries of visual understanding capabilities.
However, their high computational demands hinder deployment on
resource-constrained edge devices. A key source of inefficiency stems from the
VLM's need to process dense and redundant visual information. Visual inputs
contain significant regions irrelevant to text semantics, rendering the
associated computations ineffective for inference. This paper introduces a
novel Event-Priori-Based Vision-Language Model, termed EP-VLM. Its core
contribution is a novel mechanism leveraging motion priors derived from dynamic
event vision to enhance VLM efficiency. Inspired by human visual cognition,
EP-VLM first employs event data to guide the patch-wise sparsification of RGB
visual inputs, progressively concentrating VLM computation on salient regions
of the visual input. Subsequently, we construct a position-preserving
tokenization strategy for the visual encoder within the VLM architecture. This
strategy processes the event-guided, unstructured, sparse visual input while
accurately preserving positional understanding within the visual input.
Experimental results demonstrate that EP-VLM achieves significant efficiency
improvements while maintaining nearly lossless accuracy compared to baseline
models from the Qwen2-VL series. For instance, against the original
Qwen2-VL-2B, EP-VLM achieves 50% FLOPs savings while retaining 98% of the
original accuracy on the RealWorldQA dataset. This work demonstrates the
potential of event-based vision priors for improving VLM inference efficiency,
paving the way for creating more efficient and deployable VLMs for sustainable
visual understanding at the edge.

</details>


### [343] [HuSc3D: Human Sculpture dataset for 3D object reconstruction](https://arxiv.org/abs/2506.07628)
*Weronika Smolak-Dyżewska,Dawid Malarz,Grzegorz Wilczyński,Rafał Tobiasz,Joanna Waczyńska,Piotr Borycki,Przemysław Spurek*

Main category: cs.CV

TL;DR: HuSc3D是一个专为3D重建模型在真实采集挑战下进行严格基准测试而设计的新数据集，填补了现有数据集的不足。


<details>
  <summary>Details</summary>
Motivation: 现有数据集集中于理想化的合成或精心捕获的真实数据，未能反映新获取的真实场景中的复杂性。

Method: 提出HuSc3D数据集，包含六个高度详细的全白雕塑，具有复杂穿孔和最小纹理变化，且每场景图像数量差异显著。

Result: 评估流行3D重建方法，显示HuSc3D能有效区分模型性能，揭示其对几何细节、颜色模糊和数据变化的敏感性。

Conclusion: HuSc3D突显了传统数据集掩盖的方法局限性，为3D重建研究提供了更真实的测试平台。

Abstract: 3D scene reconstruction from 2D images is one of the most important tasks in
computer graphics. Unfortunately, existing datasets and benchmarks concentrate
on idealized synthetic or meticulously captured realistic data. Such benchmarks
fail to convey the inherent complexities encountered in newly acquired
real-world scenes. In such scenes especially those acquired outside, the
background is often dynamic, and by popular usage of cell phone cameras, there
might be discrepancies in, e.g., white balance. To address this gap, we present
HuSc3D, a novel dataset specifically designed for rigorous benchmarking of 3D
reconstruction models under realistic acquisition challenges. Our dataset
uniquely features six highly detailed, fully white sculptures characterized by
intricate perforations and minimal textural and color variation. Furthermore,
the number of images per scene varies significantly, introducing the additional
challenge of limited training data for some instances alongside scenes with a
standard number of views. By evaluating popular 3D reconstruction methods on
this diverse dataset, we demonstrate the distinctiveness of HuSc3D in
effectively differentiating model performance, particularly highlighting the
sensitivity of methods to fine geometric details, color ambiguity, and varying
data availability--limitations often masked by more conventional datasets.

</details>


### [344] [HieraEdgeNet: A Multi-Scale Edge-Enhanced Framework for Automated Pollen Recognition](https://arxiv.org/abs/2506.07637)
*Yuchong Long,Wen Sun,Ningxiao Sun,Wenxiao Wang,Chao Li,Shan Yin*

Main category: cs.CV

TL;DR: HieraEdgeNet是一种多尺度边缘增强框架，显著提升了花粉等微小目标的自动检测精度，优于现有模型。


<details>
  <summary>Details</summary>
Motivation: 传统花粉识别方法效率低且主观性强，现有深度学习模型在微小目标定位上表现不佳。

Method: 提出HieraEdgeNet框架，包含三个模块：HEM提取多尺度边缘特征，SEF融合边缘与语义信息，CSPOKM优化细节特征。

Result: 在120类花粉数据集上，mAP@.5达0.9501，优于YOLOv12n和RT-DETR。

Conclusion: HieraEdgeNet通过系统整合边缘信息，为高精度、高效率的微小目标检测提供了解决方案。

Abstract: Automated pollen recognition is vital to paleoclimatology, biodiversity
monitoring, and public health, yet conventional methods are hampered by
inefficiency and subjectivity. Existing deep learning models often struggle to
achieve the requisite localization accuracy for microscopic targets like
pollen, which are characterized by their minute size, indistinct edges, and
complex backgrounds. To overcome this limitation, we introduce HieraEdgeNet, a
multi-scale edge-enhancement framework. The framework's core innovation is the
introduction of three synergistic modules: the Hierarchical Edge Module (HEM),
which explicitly extracts a multi-scale pyramid of edge features that
corresponds to the semantic hierarchy at early network stages; the Synergistic
Edge Fusion (SEF) module, for deeply fusing these edge priors with semantic
information at each respective scale; and the Cross Stage Partial Omni-Kernel
Module (CSPOKM), which maximally refines the most detail-rich feature layers
using an Omni-Kernel operator - comprising anisotropic large-kernel
convolutions and mixed-domain attention - all within a computationally
efficient Cross-Stage Partial (CSP) framework. On a large-scale dataset
comprising 120 pollen classes, HieraEdgeNet achieves a mean Average Precision
(mAP@.5) of 0.9501, significantly outperforming state-of-the-art baseline
models such as YOLOv12n and RT-DETR. Furthermore, qualitative analysis confirms
that our approach generates feature representations that are more precisely
focused on object boundaries. By systematically integrating edge information,
HieraEdgeNet provides a robust and powerful solution for high-precision,
high-efficiency automated detection of microscopic objects.

</details>


### [345] [Synthetic Visual Genome](https://arxiv.org/abs/2506.07643)
*Jae Sung Park,Zixian Ma,Linjie Li,Chenhao Zheng,Cheng-Yu Hsieh,Ximing Lu,Khyathi Chandu,Quan Kong,Norimasa Kobori,Ali Farhadi,Yejin Choi,Ranjay Krishna*

Main category: cs.CV

TL;DR: 论文介绍了ROBIN，一种通过密集标注关系调整的多模态语言模型，用于生成高质量密集场景图。通过合成数据集SVG和自蒸馏框架SG-EDIT，ROBIN在关系理解任务中表现优异，甚至超越更大规模的模型。


<details>
  <summary>Details</summary>
Motivation: 尽管多模态语言模型在视觉理解方面取得进展，但在关系和生成方面的精确推理仍具挑战性。

Method: 使用合成数据集SVG训练ROBIN，并通过SG-EDIT框架进一步优化预测的场景图。

Result: ROBIN-3B模型在关系理解任务中表现优异，超越类似规模甚至更大规模的模型，并在指代表达理解任务中达到88.9的最高分。

Conclusion: 训练于精炼的场景图数据对提升多样化视觉推理任务的性能至关重要。

Abstract: Reasoning over visual relationships-spatial, functional, interactional,
social, etc.-is considered to be a fundamental component of human cognition.
Yet, despite the major advances in visual comprehension in multimodal language
models (MLMs), precise reasoning over relationships and their generations
remains a challenge. We introduce ROBIN: an MLM instruction-tuned with densely
annotated relationships capable of constructing high-quality dense scene graphs
at scale. To train ROBIN, we curate SVG, a synthetic scene graph dataset by
completing the missing relations of selected objects in existing scene graphs
using a teacher MLM and a carefully designed filtering process to ensure
high-quality. To generate more accurate and rich scene graphs at scale for any
image, we introduce SG-EDIT: a self-distillation framework where GPT-4o further
refines ROBIN's predicted scene graphs by removing unlikely relations and/or
suggesting relevant ones. In total, our dataset contains 146K images and 5.6M
relationships for 2.6M objects. Results show that our ROBIN-3B model, despite
being trained on less than 3 million instances, outperforms similar-size models
trained on over 300 million instances on relationship understanding benchmarks,
and even surpasses larger models up to 13B parameters. Notably, it achieves
state-of-the-art performance in referring expression comprehension with a score
of 88.9, surpassing the previous best of 87.4. Our results suggest that
training on the refined scene graph data is crucial to maintaining high
performance across diverse visual reasoning task.

</details>


### [346] [FMaMIL: Frequency-Driven Mamba Multi-Instance Learning for Weakly Supervised Lesion Segmentation in Medical Images](https://arxiv.org/abs/2506.07652)
*Hangbei Cheng,Xiaorong Dong,Xueyu Liu,Jianan Zhang,Xuetao Ma,Mingqiang Wei,Liansheng Wang,Junxin Chen,Yongfei Wu*

Main category: cs.CV

TL;DR: FMaMIL是一种基于图像级标签的弱监督病灶分割框架，通过两阶段方法实现高效分割，无需像素级标注。


<details>
  <summary>Details</summary>
Motivation: 解决组织病理学图像中病灶分割因像素级标注成本高而难以实现的问题。

Method: 两阶段框架：第一阶段使用Mamba编码器和频率域编码模块生成CAMs；第二阶段通过软标签监督和自校正机制优化伪标签。

Result: 在公开和私有数据集上优于现有弱监督方法。

Conclusion: FMaMIL在数字病理学中具有高效性和应用潜力。

Abstract: Accurate lesion segmentation in histopathology images is essential for
diagnostic interpretation and quantitative analysis, yet it remains challenging
due to the limited availability of costly pixel-level annotations. To address
this, we propose FMaMIL, a novel two-stage framework for weakly supervised
lesion segmentation based solely on image-level labels. In the first stage, a
lightweight Mamba-based encoder is introduced to capture long-range
dependencies across image patches under the MIL paradigm. To enhance spatial
sensitivity and structural awareness, we design a learnable frequency-domain
encoding module that supplements spatial-domain features with spectrum-based
information. CAMs generated in this stage are used to guide segmentation
training. In the second stage, we refine the initial pseudo labels via a
CAM-guided soft-label supervision and a self-correction mechanism, enabling
robust training even under label noise. Extensive experiments on both public
and private histopathology datasets demonstrate that FMaMIL outperforms
state-of-the-art weakly supervised methods without relying on pixel-level
annotations, validating its effectiveness and potential for digital pathology
applications.

</details>


### [347] [ProSplat: Improved Feed-Forward 3D Gaussian Splatting for Wide-Baseline Sparse Views](https://arxiv.org/abs/2506.07670)
*Xiaohan Lu,Jiaye Fu,Jiaqi Zhang,Zetian Song,Chuanmin Jia,Siwei Ma*

Main category: cs.CV

TL;DR: ProSplat是一种两阶段前馈框架，用于在宽基线条件下实现高保真渲染，通过3D高斯生成器和改进模型（基于扩散模型）提升性能。


<details>
  <summary>Details</summary>
Motivation: 解决3D高斯溅射在宽基线场景下因纹理细节不足和几何不一致性导致的性能下降问题。

Method: 两阶段框架：1) 生成3D高斯基元；2) 通过改进模型（包含MORI和DWEA）增强渲染视图。

Result: 在RealEstate10K和DL3DV-10K数据集上，PSNR平均提升1 dB。

Conclusion: ProSplat在宽基线条件下显著提升了渲染质量。

Abstract: Feed-forward 3D Gaussian Splatting (3DGS) has recently demonstrated promising
results for novel view synthesis (NVS) from sparse input views, particularly
under narrow-baseline conditions. However, its performance significantly
degrades in wide-baseline scenarios due to limited texture details and
geometric inconsistencies across views. To address these challenges, in this
paper, we propose ProSplat, a two-stage feed-forward framework designed for
high-fidelity rendering under wide-baseline conditions. The first stage
involves generating 3D Gaussian primitives via a 3DGS generator. In the second
stage, rendered views from these primitives are enhanced through an improvement
model. Specifically, this improvement model is based on a one-step diffusion
model, further optimized by our proposed Maximum Overlap Reference view
Injection (MORI) and Distance-Weighted Epipolar Attention (DWEA). MORI
supplements missing texture and color by strategically selecting a reference
view with maximum viewpoint overlap, while DWEA enforces geometric consistency
using epipolar constraints. Additionally, we introduce a divide-and-conquer
training strategy that aligns data distributions between the two stages through
joint optimization. We evaluate ProSplat on the RealEstate10K and DL3DV-10K
datasets under wide-baseline settings. Experimental results demonstrate that
ProSplat achieves an average improvement of 1 dB in PSNR compared to recent
SOTA methods.

</details>


### [348] [OpenSplat3D: Open-Vocabulary 3D Instance Segmentation using Gaussian Splatting](https://arxiv.org/abs/2506.07697)
*Jens Piekenbrinck,Christian Schmidt,Alexander Hermans,Narunas Vaskevicius,Timm Linder,Bastian Leibe*

Main category: cs.CV

TL;DR: OpenSplat3D扩展了3D高斯泼溅（3DGS）的能力，实现了无需手动标注的开放词汇3D实例分割。


<details>
  <summary>Details</summary>
Motivation: 将3DGS从单纯的场景表示扩展到支持开放词汇的3D实例分割，以提升场景理解的细粒度。

Method: 结合特征泼溅技术、Segment Anything Model实例掩码和对比损失，以及视觉语言模型的语言嵌入，实现基于自然语言的实例分割。

Result: 在LERF-mask、LERF-OVS和ScanNet++验证集上展示了方法的有效性。

Conclusion: OpenSplat3D能够灵活识别和分割3D场景中的任意对象，为场景理解提供了新工具。

Abstract: 3D Gaussian Splatting (3DGS) has emerged as a powerful representation for
neural scene reconstruction, offering high-quality novel view synthesis while
maintaining computational efficiency. In this paper, we extend the capabilities
of 3DGS beyond pure scene representation by introducing an approach for
open-vocabulary 3D instance segmentation without requiring manual labeling,
termed OpenSplat3D. Our method leverages feature-splatting techniques to
associate semantic information with individual Gaussians, enabling fine-grained
scene understanding. We incorporate Segment Anything Model instance masks with
a contrastive loss formulation as guidance for the instance features to achieve
accurate instance-level segmentation. Furthermore, we utilize language
embeddings of a vision-language model, allowing for flexible, text-driven
instance identification. This combination enables our system to identify and
segment arbitrary objects in 3D scenes based on natural language descriptions.
We show results on LERF-mask and LERF-OVS as well as the full ScanNet++
validation set, demonstrating the effectiveness of our approach.

</details>


### [349] [NOVA3D: Normal Aligned Video Diffusion Model for Single Image to 3D Generation](https://arxiv.org/abs/2506.07698)
*Yuxiao Yang,Peihao Li,Yuhong Zhang,Junzhe Lu,Xianglong He,Minghan Qin,Weitao Wang,Haoqian Wang*

Main category: cs.CV

TL;DR: NOVA3D是一个创新的单图像到3D生成框架，通过利用预训练视频扩散模型的3D先验和几何信息，解决了多视角一致性问题。


<details>
  <summary>Details</summary>
Motivation: 当前基于Score Distillation Sampling的方法缺乏足够的3D先验，导致多视角一致性不足。

Method: NOVA3D利用预训练视频扩散模型的3D先验，结合几何信息进行多视角视频微调，并提出Geometry-Temporal Alignment注意力机制和去冲突几何融合算法。

Result: 实验表明NOVA3D在多视角一致性和纹理保真度上优于现有基线方法。

Conclusion: NOVA3D通过创新的框架和算法，显著提升了单图像到3D生成的质量和一致性。

Abstract: 3D AI-generated content (AIGC) has made it increasingly accessible for anyone
to become a 3D content creator. While recent methods leverage Score
Distillation Sampling to distill 3D objects from pretrained image diffusion
models, they often suffer from inadequate 3D priors, leading to insufficient
multi-view consistency. In this work, we introduce NOVA3D, an innovative
single-image-to-3D generation framework. Our key insight lies in leveraging
strong 3D priors from a pretrained video diffusion model and integrating
geometric information during multi-view video fine-tuning. To facilitate
information exchange between color and geometric domains, we propose the
Geometry-Temporal Alignment (GTA) attention mechanism, thereby improving
generalization and multi-view consistency. Moreover, we introduce the
de-conflict geometry fusion algorithm, which improves texture fidelity by
addressing multi-view inaccuracies and resolving discrepancies in pose
alignment. Extensive experiments validate the superiority of NOVA3D over
existing baselines.

</details>


### [350] [Adaptive Blind Super-Resolution Network for Spatial-Specific and Spatial-Agnostic Degradations](https://arxiv.org/abs/2506.07705)
*Weilei Wen,Chunle Guo,Wenqi Ren,Hongpeng Wang,Xiuli Shao*

Main category: cs.CV

TL;DR: 论文提出了一种动态滤波网络，通过全局和局部分支处理图像重建中的两种主要退化类型，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法忽视了不同退化类型的多样性，采用统一网络模型处理多种退化，导致效果不佳。

Method: 引入动态滤波网络，分为全局和局部分支：全局分支处理空间无关的退化（如下采样和噪声），局部分支处理空间相关的退化（如模糊）。

Result: 该方法在合成和真实图像数据集上优于现有盲超分辨率算法。

Conclusion: 动态滤波网络能有效区分和处理不同类型的退化，提升图像重建质量。

Abstract: Prior methodologies have disregarded the diversities among distinct
degradation types during image reconstruction, employing a uniform network
model to handle multiple deteriorations. Nevertheless, we discover that
prevalent degradation modalities, including sampling, blurring, and noise, can
be roughly categorized into two classes. We classify the first class as
spatial-agnostic dominant degradations, less affected by regional changes in
image space, such as downsampling and noise degradation. The second class
degradation type is intimately associated with the spatial position of the
image, such as blurring, and we identify them as spatial-specific dominant
degradations. We introduce a dynamic filter network integrating global and
local branches to address these two degradation types. This network can greatly
alleviate the practical degradation problem. Specifically, the global dynamic
filtering layer can perceive the spatial-agnostic dominant degradation in
different images by applying weights generated by the attention mechanism to
multiple parallel standard convolution kernels, enhancing the network's
representation ability. Meanwhile, the local dynamic filtering layer converts
feature maps of the image into a spatially specific dynamic filtering operator,
which performs spatially specific convolution operations on the image features
to handle spatial-specific dominant degradations. By effectively integrating
both global and local dynamic filtering operators, our proposed method
outperforms state-of-the-art blind super-resolution algorithms in both
synthetic and real image datasets.

</details>


### [351] [Consistent Video Editing as Flow-Driven Image-to-Video Generation](https://arxiv.org/abs/2506.07713)
*Ge Wang,Songlin Fan,Hangxu Liu,Quanjian Song,Hewei Wang,Jinfeng Xu*

Main category: cs.CV

TL;DR: FlowV2V提出了一种基于光流的视频编辑方法，通过分解任务为第一帧编辑和条件I2V生成，显著提升了复杂运动建模的效率和一致性。


<details>
  <summary>Details</summary>
Motivation: 现有视频编辑方法难以处理复杂运动模式，尤其是非刚性物体运动和多对象编辑任务。光流为复杂运动建模提供了新思路。

Method: FlowV2V将任务分解为第一帧编辑和条件I2V生成，通过模拟伪光流序列确保编辑一致性。

Result: 在DAVIS-EDIT数据集上，FlowV2V在DOVER和warping error指标上分别提升了13.67%和50.66%。

Conclusion: FlowV2V在复杂运动建模和视频编辑任务中表现出色，为未来研究提供了新方向。

Abstract: With the prosper of video diffusion models, down-stream applications like
video editing have been significantly promoted without consuming much
computational cost. One particular challenge in this task lies at the motion
transfer process from the source video to the edited one, where it requires the
consideration of the shape deformation in between, meanwhile maintaining the
temporal consistency in the generated video sequence. However, existing methods
fail to model complicated motion patterns for video editing, and are
fundamentally limited to object replacement, where tasks with non-rigid object
motions like multi-object and portrait editing are largely neglected. In this
paper, we observe that optical flows offer a promising alternative in complex
motion modeling, and present FlowV2V to re-investigate video editing as a task
of flow-driven Image-to-Video (I2V) generation. Specifically, FlowV2V
decomposes the entire pipeline into first-frame editing and conditional I2V
generation, and simulates pseudo flow sequence that aligns with the deformed
shape, thus ensuring the consistency during editing. Experimental results on
DAVIS-EDIT with improvements of 13.67% and 50.66% on DOVER and warping error
illustrate the superior temporal consistency and sample quality of FlowV2V
compared to existing state-of-the-art ones. Furthermore, we conduct
comprehensive ablation studies to analyze the internal functionalities of the
first-frame paradigm and flow alignment in the proposed method.

</details>


### [352] [ReverB-SNN: Reversing Bit of the Weight and Activation for Spiking Neural Networks](https://arxiv.org/abs/2506.07720)
*Yufei Guo,Yuhan Zhang,Zhou Jie,Xiaode Liu,Xin Tong,Yuanpei Chen,Weihang Peng,Zhe Ma*

Main category: cs.CV

TL;DR: 提出了一种名为ReverB-SNN的方法，通过反转权重和激活的比特位，结合实值激活和二进制权重，提升SNN的精度和效率。


<details>
  <summary>Details</summary>
Motivation: 解决SNN中二进制激活映射信息不足导致精度下降的问题。

Method: 采用实值激活和二进制权重，引入可训练因子调整权重幅度，并通过重参数化技术保持推理效率。

Result: 在多种网络架构和数据集上表现优于现有方法。

Conclusion: ReverB-SNN在保持SNN高效性的同时显著提升了精度。

Abstract: The Spiking Neural Network (SNN), a biologically inspired neural network
infrastructure, has garnered significant attention recently. SNNs utilize
binary spike activations for efficient information transmission, replacing
multiplications with additions, thereby enhancing energy efficiency. However,
binary spike activation maps often fail to capture sufficient data information,
resulting in reduced accuracy. To address this challenge, we advocate reversing
the bit of the weight and activation for SNNs, called \textbf{ReverB-SNN},
inspired by recent findings that highlight greater accuracy degradation from
quantizing activations compared to weights. Specifically, our method employs
real-valued spike activations alongside binary weights in SNNs. This preserves
the event-driven and multiplication-free advantages of standard SNNs while
enhancing the information capacity of activations. Additionally, we introduce a
trainable factor within binary weights to adaptively learn suitable weight
amplitudes during training, thereby increasing network capacity. To maintain
efficiency akin to vanilla \textbf{ReverB-SNN}, our trainable binary weight
SNNs are converted back to standard form using a re-parameterization technique
during inference. Extensive experiments across various network architectures
and datasets, both static and dynamic, demonstrate that our approach
consistently outperforms state-of-the-art methods.

</details>


### [353] [ETA: Efficiency through Thinking Ahead, A Dual Approach to Self-Driving with Large Models](https://arxiv.org/abs/2506.07725)
*Shadi Hamdan,Chonghao Sima,Zetong Yang,Hongyang Li,Fatma Güney*

Main category: cs.CV

TL;DR: ETA系统通过异步计算和批量推理，使大型模型能够快速响应自动驾驶系统的实时需求，同时保持高性能。


<details>
  <summary>Details</summary>
Motivation: 解决自动驾驶系统中大型模型推理速度慢的问题，同时不牺牲其性能。

Method: 提出ETA系统，通过异步计算将当前帧的密集计算转移到先前时间步，并利用小模型提取实时特征，结合动作掩码机制整合双特征。

Result: 在Bench2Drive CARLA Leaderboard-v2基准测试中，ETA将驾驶分数提升至69.53，性能提升8%，推理速度保持在50毫秒。

Conclusion: ETA系统成功实现了大型模型在自动驾驶中的高效实时应用，性能显著提升。

Abstract: How can we benefit from large models without sacrificing inference speed, a
common dilemma in self-driving systems? A prevalent solution is a dual-system
architecture, employing a small model for rapid, reactive decisions and a
larger model for slower but more informative analyses. Existing dual-system
designs often implement parallel architectures where inference is either
directly conducted using the large model at each current frame or retrieved
from previously stored inference results. However, these works still struggle
to enable large models for a timely response to every online frame. Our key
insight is to shift intensive computations of the current frame to previous
time steps and perform a batch inference of multiple time steps to make large
models respond promptly to each time step. To achieve the shifting, we
introduce Efficiency through Thinking Ahead (ETA), an asynchronous system
designed to: (1) propagate informative features from the past to the current
frame using future predictions from the large model, (2) extract current frame
features using a small model for real-time responsiveness, and (3) integrate
these dual features via an action mask mechanism that emphasizes
action-critical image regions. Evaluated on the Bench2Drive CARLA
Leaderboard-v2 benchmark, ETA advances state-of-the-art performance by 8% with
a driving score of 69.53 while maintaining a near-real-time inference speed at
50 ms.

</details>


### [354] [SpikeSMOKE: Spiking Neural Networks for Monocular 3D Object Detection with Cross-Scale Gated Coding](https://arxiv.org/abs/2506.07737)
*Xuemei Chen,Huamin Wang,Hangchi Shen,Shukai Duan,Shiping Wen,Tingwen Huang*

Main category: cs.CV

TL;DR: SpikeSMOKE架构结合SNNs和CSGC机制，实现低功耗单目3D物体检测，性能提升且能耗显著降低。


<details>
  <summary>Details</summary>
Motivation: 解决3D物体检测中高能耗问题，利用SNNs的低功耗特性，同时克服其信息损失问题。

Method: 提出SpikeSMOKE架构，引入跨尺度门控编码机制（CSGC）和轻量级残差块，增强特征表达并降低计算量。

Result: 在KITTI数据集上性能显著提升，能耗降低72.2%，检测性能仅下降4%。

Conclusion: SpikeSMOKE为低功耗3D物体检测提供了有效解决方案，性能与能耗平衡。

Abstract: Low energy consumption for 3D object detection is an important research area
because of the increasing energy consumption with their wide application in
fields such as autonomous driving. The spiking neural networks (SNNs) with
low-power consumption characteristics can provide a novel solution for this
research. Therefore, we apply SNNs to monocular 3D object detection and propose
the SpikeSMOKE architecture in this paper, which is a new attempt for low-power
monocular 3D object detection. As we all know, discrete signals of SNNs will
generate information loss and limit their feature expression ability compared
with the artificial neural networks (ANNs).In order to address this issue,
inspired by the filtering mechanism of biological neuronal synapses, we propose
a cross-scale gated coding mechanism(CSGC), which can enhance feature
representation by combining cross-scale fusion of attentional methods and gated
filtering mechanisms.In addition, to reduce the computation and increase the
speed of training, we present a novel light-weight residual block that can
maintain spiking computing paradigm and the highest possible detection
performance. Compared to the baseline SpikeSMOKE under the 3D Object Detection,
the proposed SpikeSMOKE with CSGC can achieve 11.78 (+2.82, Easy), 10.69 (+3.2,
Moderate), and 10.48 (+3.17, Hard) on the KITTI autonomous driving dataset by
AP|R11 at 0.7 IoU threshold, respectively. It is important to note that the
results of SpikeSMOKE can significantly reduce energy consumption compared to
the results on SMOKE. For example,the energy consumption can be reduced by
72.2% on the hard category, while the detection performance is reduced by only
4%. SpikeSMOKE-L (lightweight) can further reduce the amount of parameters by 3
times and computation by 10 times compared to SMOKE.

</details>


### [355] [AssetDropper: Asset Extraction via Diffusion Models with Reward-Driven Optimization](https://arxiv.org/abs/2506.07738)
*Lanjiong Li,Guanhua Zhao,Lingting Zhu,Zeyu Cai,Lequan Yu,Jian Zhang,Zeyu Wang*

Main category: cs.CV

TL;DR: AssetDropper是一个框架，用于从参考图像中提取标准化资产，解决了设计师在开放世界场景中高效提取高质量资产的挑战。


<details>
  <summary>Details</summary>
Motivation: 设计师需要标准化资产库，但现有生成模型未能显著提升这一领域。开放世界场景提供了丰富素材，但高效提取高质量资产仍具挑战性。

Method: 引入AssetDropper框架，从输入图像中提取选定对象的前视图，处理复杂场景（如透视变形和遮挡）。使用合成数据集和真实世界基准进行评估，并通过预训练的奖励模型实现闭环反馈。

Result: AssetDropper在资产提取任务中取得了最先进的结果，奖励驱动的优化显著提升了性能。

Conclusion: AssetDropper为设计师提供了开放世界资产调色板，解决了资产提取的挑战，并通过奖励模型优化实现了高效提取。

Abstract: Recent research on generative models has primarily focused on creating
product-ready visual outputs; however, designers often favor access to
standardized asset libraries, a domain that has yet to be significantly
enhanced by generative capabilities. Although open-world scenes provide ample
raw materials for designers, efficiently extracting high-quality, standardized
assets remains a challenge. To address this, we introduce AssetDropper, the
first framework designed to extract assets from reference images, providing
artists with an open-world asset palette. Our model adeptly extracts a front
view of selected subjects from input images, effectively handling complex
scenarios such as perspective distortion and subject occlusion. We establish a
synthetic dataset of more than 200,000 image-subject pairs and a real-world
benchmark with thousands more for evaluation, facilitating the exploration of
future research in downstream tasks. Furthermore, to ensure precise asset
extraction that aligns well with the image prompts, we employ a pre-trained
reward model to fulfill a closed-loop with feedback. We design the reward model
to perform an inverse task that pastes the extracted assets back into the
reference sources, which assists training with additional consistency and
mitigates hallucination. Extensive experiments show that, with the aid of
reward-driven optimization, AssetDropper achieves the state-of-the-art results
in asset extraction. Project page: AssetDropper.github.io.

</details>


### [356] [ArchiLense: A Framework for Quantitative Analysis of Architectural Styles Based on Vision Large Language Models](https://arxiv.org/abs/2506.07739)
*Jing Zhong,Jun Yin,Peilin Li,Pengyu Zeng,Miao Zhang,Shuai Lu,Ran Luo*

Main category: cs.CV

TL;DR: 该论文提出了一种基于视觉语言模型的建筑风格分析框架ArchiLense，通过构建专业数据集ArchDiffBench，实现了建筑图像的自动识别与分类，解决了传统方法的主观性和区域偏见问题。


<details>
  <summary>Details</summary>
Motivation: 传统建筑文化研究依赖主观专家解读和历史文献，存在区域偏见和解释范围有限的问题。

Method: 构建ArchDiffBench数据集（1,765张高质量建筑图像），并提出基于视觉语言模型的ArchiLense框架，结合计算机视觉和深度学习技术。

Result: ArchiLense在建筑风格识别中表现优异，与专家标注一致性达92.4%，分类准确率为84.5%。

Conclusion: 该方法超越了传统分析的主观性，为建筑文化比较研究提供了更客观、准确的视角。

Abstract: Architectural cultures across regions are characterized by stylistic
diversity, shaped by historical, social, and technological contexts in addition
to geograph-ical conditions. Understanding architectural styles requires the
ability to describe and analyze the stylistic features of different architects
from various regions through visual observations of architectural imagery.
However, traditional studies of architectural culture have largely relied on
subjective expert interpretations and historical literature reviews, often
suffering from regional biases and limited ex-planatory scope. To address these
challenges, this study proposes three core contributions: (1) We construct a
professional architectural style dataset named ArchDiffBench, which comprises
1,765 high-quality architectural images and their corresponding style
annotations, collected from different regions and historical periods. (2) We
propose ArchiLense, an analytical framework grounded in Vision-Language Models
and constructed using the ArchDiffBench dataset. By integrating ad-vanced
computer vision techniques, deep learning, and machine learning algo-rithms,
ArchiLense enables automatic recognition, comparison, and precise
classi-fication of architectural imagery, producing descriptive language
outputs that ar-ticulate stylistic differences. (3) Extensive evaluations show
that ArchiLense achieves strong performance in architectural style recognition,
with a 92.4% con-sistency rate with expert annotations and 84.5% classification
accuracy, effec-tively capturing stylistic distinctions across images. The
proposed approach transcends the subjectivity inherent in traditional analyses
and offers a more objective and accurate perspective for comparative studies of
architectural culture.

</details>


### [357] [Flow-Anything: Learning Real-World Optical Flow Estimation from Large-Scale Single-view Images](https://arxiv.org/abs/2506.07740)
*Yingping Liang,Ying Fu,Yutao Hu,Wenqi Shao,Jiaming Liu,Debing Zhang*

Main category: cs.CV

TL;DR: Flow-Anything框架通过单视角图像生成大规模真实世界光流训练数据，解决了合成数据集的域差距问题。


<details>
  <summary>Details</summary>
Motivation: 光流估计在计算机视觉中至关重要，但合成数据集的训练限制了其在真实世界中的鲁棒性。

Method: 利用单视角图像生成3D表示，结合虚拟相机渲染光流和新视角图像，并通过对象无关体积渲染和深度感知修复模块建模动态对象。

Result: 生成的FA-Flow数据集在光流估计任务中优于现有无监督和监督方法，并提升了下游视频任务的性能。

Conclusion: Flow-Anything展示了从真实世界图像生成光流训练数据的潜力，为视频任务提供了基础模型。

Abstract: Optical flow estimation is a crucial subfield of computer vision, serving as
a foundation for video tasks. However, the real-world robustness is limited by
animated synthetic datasets for training. This introduces domain gaps when
applied to real-world applications and limits the benefits of scaling up
datasets. To address these challenges, we propose \textbf{Flow-Anything}, a
large-scale data generation framework designed to learn optical flow estimation
from any single-view images in the real world. We employ two effective steps to
make data scaling-up promising. First, we convert a single-view image into a 3D
representation using advanced monocular depth estimation networks. This allows
us to render optical flow and novel view images under a virtual camera. Second,
we develop an Object-Independent Volume Rendering module and a Depth-Aware
Inpainting module to model the dynamic objects in the 3D representation. These
two steps allow us to generate realistic datasets for training from large-scale
single-view images, namely \textbf{FA-Flow Dataset}. For the first time, we
demonstrate the benefits of generating optical flow training data from
large-scale real-world images, outperforming the most advanced unsupervised
methods and supervised methods on synthetic datasets. Moreover, our models
serve as a foundation model and enhance the performance of various downstream
video tasks.

</details>


### [358] [Difference Inversion: Interpolate and Isolate the Difference with Token Consistency for Image Analogy Generation](https://arxiv.org/abs/2506.07750)
*Hyunsoo Kim,Donghyun Kim,Suhyun Kim*

Main category: cs.CV

TL;DR: 提出了一种名为Difference Inversion的方法，通过提取A和A'之间的差异并应用于B，生成B'，解决了现有方法对特定模型的依赖问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法通常局限于特定模型（如InstructPix2Pix），可能导致偏见或编辑能力受限，因此需要一种更通用的方法。

Method: 通过Delta插值提取差异，结合Token一致性损失和零初始化Token嵌入，构建适合稳定扩散模型的完整提示。

Result: 实验表明，Difference Inversion在定量和定性上均优于现有基线。

Conclusion: Difference Inversion能够以模型无关的方式生成更可行的B'。

Abstract: How can we generate an image B' that satisfies A:A'::B:B', given the input
images A,A' and B? Recent works have tackled this challenge through approaches
like visual in-context learning or visual instruction. However, these methods
are typically limited to specific models (e.g. InstructPix2Pix. Inpainting
models) rather than general diffusion models (e.g. Stable Diffusion, SDXL).
This dependency may lead to inherited biases or lower editing capabilities. In
this paper, we propose Difference Inversion, a method that isolates only the
difference from A and A' and applies it to B to generate a plausible B'. To
address model dependency, it is crucial to structure prompts in the form of a
"Full Prompt" suitable for input to stable diffusion models, rather than using
an "Instruction Prompt". To this end, we accurately extract the Difference
between A and A' and combine it with the prompt of B, enabling a plug-and-play
application of the difference. To extract a precise difference, we first
identify it through 1) Delta Interpolation. Additionally, to ensure accurate
training, we propose the 2) Token Consistency Loss and 3) Zero Initialization
of Token Embeddings. Our extensive experiments demonstrate that Difference
Inversion outperforms existing baselines both quantitatively and qualitatively,
indicating its ability to generate more feasible B' in a model-agnostic manner.

</details>


### [359] [Trend-Aware Fashion Recommendation with Visual Segmentation and Semantic Similarity](https://arxiv.org/abs/2506.07773)
*Mohamed Djilani,Nassim Ali Ousalah,Nidhal Eddine Chenni*

Main category: cs.CV

TL;DR: 提出了一种结合视觉、语义和用户行为的时尚推荐系统，通过深度视觉表示和用户行为模拟实现个性化推荐。


<details>
  <summary>Details</summary>
Motivation: 解决时尚推荐中如何平衡个人风格与流行趋势的问题。

Method: 使用语义分割提取服装区域特征，结合用户行为模拟和加权评分函数生成推荐。

Result: 在DeepFashion数据集上表现优异，ResNet-50达到64.95%的类别相似度。

Conclusion: 该方法为个性化时尚推荐提供了可扩展的框架。

Abstract: We introduce a trend-aware and visually-grounded fashion recommendation
system that integrates deep visual representations, garment-aware segmentation,
semantic category similarity and user behavior simulation. Our pipeline
extracts focused visual embeddings by masking non-garment regions via semantic
segmentation followed by feature extraction using pretrained CNN backbones
(ResNet-50, DenseNet-121, VGG16). To simulate realistic shopping behavior, we
generate synthetic purchase histories influenced by user-specific trendiness
and item popularity. Recommendations are computed using a weighted scoring
function that fuses visual similarity, semantic coherence and popularity
alignment. Experiments on the DeepFashion dataset demonstrate consistent gender
alignment and improved category relevance, with ResNet-50 achieving 64.95%
category similarity and lowest popularity MAE. An ablation study confirms the
complementary roles of visual and popularity cues. Our method provides a
scalable framework for personalized fashion recommendations that balances
individual style with emerging trends. Our implementation is available at
https://github.com/meddjilani/FashionRecommender

</details>


### [360] [Language-Vision Planner and Executor for Text-to-Visual Reasoning](https://arxiv.org/abs/2506.07778)
*Yichang Xu,Gaowen Liu,Ramana Rao Kompella,Sihao Hu,Tiansheng Huang,Fatih Ilhan,Selim Furkan Tekin,Zachary Yahn,Ling Liu*

Main category: cs.CV

TL;DR: VLAgent是一个多模态视觉-文本推理系统，通过生成分步推理计划并实时执行，显著提升了现有视觉语言模型的泛化性能。


<details>
  <summary>Details</summary>
Motivation: 现有视觉语言模型在泛化性能上表现不佳，VLAgent旨在通过结合规划脚本与执行验证来解决这一问题。

Method: VLAgent通过上下文学习微调LLM生成分步计划，并利用神经符号模块逐步优化执行结果。其独特设计包括改进计划生成质量、语法-语义解析器修正逻辑错误，以及集成方法提升执行器泛化性能。

Result: 在四个视觉推理基准测试（GQA、MME、NLVR2、VQAv2）中，VLAgent表现优于现有视觉语言模型和基于LLM的视觉组合方法。

Conclusion: VLAgent通过其新颖的优化模块（如SS-Parser、Plan Repairer、Output Verifiers）在多模态视觉-文本推理任务中实现了显著性能提升。

Abstract: The advancement in large language models (LLMs) and large vision models has
fueled the rapid progress in multi-modal visual-text reasoning capabilities.
However, existing vision-language models (VLMs) to date suffer from
generalization performance. Inspired by recent development in LLMs for visual
reasoning, this paper presents VLAgent, an AI system that can create a
step-by-step visual reasoning plan with an easy-to-understand script and
execute each step of the plan in real time by integrating planning script with
execution verifications via an automated process supported by VLAgent. In the
task planning phase, VLAgent fine-tunes an LLM through in-context learning to
generate a step-by-step planner for each user-submitted text-visual reasoning
task. During the plan execution phase, VLAgent progressively refines the
composition of neuro-symbolic executable modules to generate high-confidence
reasoning results. VLAgent has three unique design characteristics: First, we
improve the quality of plan generation through in-context learning, improving
logic reasoning by reducing erroneous logic steps, incorrect programs, and LLM
hallucinations. Second, we design a syntax-semantics parser to identify and
correct additional logic errors of the LLM-generated planning script prior to
launching the plan executor. Finally, we employ the ensemble method to improve
the generalization performance of our step-executor. Extensive experiments with
four visual reasoning benchmarks (GQA, MME, NLVR2, VQAv2) show that VLAgent
achieves significant performance enhancement for multimodal text-visual
reasoning applications, compared to the exiting representative VLMs and LLM
based visual composition approaches like ViperGPT and VisProg, thanks to the
novel optimization modules of VLAgent back-engine (SS-Parser, Plan Repairer,
Output Verifiers). Code and data will be made available upon paper acceptance.

</details>


### [361] [Design and Evaluation of Deep Learning-Based Dual-Spectrum Image Fusion Methods](https://arxiv.org/abs/2506.07779)
*Beining Xu,Junxian Li*

Main category: cs.CV

TL;DR: 该论文构建了一个高质量的双光谱数据集，并提出了一种综合评估框架，以提升可见光和红外图像融合的性能评估。


<details>
  <summary>Details</summary>
Motivation: 当前可见光和红外图像融合的评估缺乏标准化基准和下游任务性能验证，且数据集不足。

Method: 构建了一个包含1,369对对齐图像的数据集，并提出融合速度、通用指标和目标检测性能的综合评估框架。

Result: 实验表明，针对下游任务优化的融合模型在目标检测中表现更优，尤其是在低光和遮挡场景中。

Conclusion: 论文贡献包括高质量数据集、任务感知评估框架和对融合方法的全面分析，为未来研究提供了方向。

Abstract: Visible images offer rich texture details, while infrared images emphasize
salient targets. Fusing these complementary modalities enhances scene
understanding, particularly for advanced vision tasks under challenging
conditions. Recently, deep learning-based fusion methods have gained attention,
but current evaluations primarily rely on general-purpose metrics without
standardized benchmarks or downstream task performance. Additionally, the lack
of well-developed dual-spectrum datasets and fair algorithm comparisons hinders
progress.
  To address these gaps, we construct a high-quality dual-spectrum dataset
captured in campus environments, comprising 1,369 well-aligned visible-infrared
image pairs across four representative scenarios: daytime, nighttime, smoke
occlusion, and underpasses. We also propose a comprehensive and fair evaluation
framework that integrates fusion speed, general metrics, and object detection
performance using the lang-segment-anything model to ensure fairness in
downstream evaluation.
  Extensive experiments benchmark several state-of-the-art fusion algorithms
under this framework. Results demonstrate that fusion models optimized for
downstream tasks achieve superior performance in target detection, especially
in low-light and occluded scenes. Notably, some algorithms that perform well on
general metrics do not translate to strong downstream performance, highlighting
limitations of current evaluation practices and validating the necessity of our
proposed framework.
  The main contributions of this work are: (1)a campus-oriented dual-spectrum
dataset with diverse and challenging scenes; (2) a task-aware, comprehensive
evaluation framework; and (3) thorough comparative analysis of leading fusion
methods across multiple datasets, offering insights for future development.

</details>


### [362] [Re-ranking Reasoning Context with Tree Search Makes Large Vision-Language Models Stronger](https://arxiv.org/abs/2506.07785)
*Qi Yang,Chenghao Zhang,Lubin Fan,Kun Ding,Jieping Ye,Shiming Xiang*

Main category: cs.CV

TL;DR: 提出了一种名为RCTS的多模态RAG框架，通过构建推理上下文丰富的知识库和树搜索重排序方法，提升LVLMs在VQA任务中的性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法在知识推理示例稀缺和检索知识响应不稳定方面存在问题。

Method: 提出RCTS框架，包括推理上下文丰富的知识库和MCTS-HR重排序方法。

Result: 在多个VQA数据集上实现最先进性能，显著优于ICL和Vanilla-RAG方法。

Conclusion: RCTS框架有效提升了LVLMs的性能，证明了知识库和重排序方法的有效性。

Abstract: Recent advancements in Large Vision Language Models (LVLMs) have
significantly improved performance in Visual Question Answering (VQA) tasks
through multimodal Retrieval-Augmented Generation (RAG). However, existing
methods still face challenges, such as the scarcity of knowledge with reasoning
examples and erratic responses from retrieved knowledge. To address these
issues, in this study, we propose a multimodal RAG framework, termed RCTS,
which enhances LVLMs by constructing a Reasoning Context-enriched knowledge
base and a Tree Search re-ranking method. Specifically, we introduce a
self-consistent evaluation mechanism to enrich the knowledge base with
intrinsic reasoning patterns. We further propose a Monte Carlo Tree Search with
Heuristic Rewards (MCTS-HR) to prioritize the most relevant examples. This
ensures that LVLMs can leverage high-quality contextual reasoning for better
and more consistent responses. Extensive experiments demonstrate that our
framework achieves state-of-the-art performance on multiple VQA datasets,
significantly outperforming In-Context Learning (ICL) and Vanilla-RAG methods.
It highlights the effectiveness of our knowledge base and re-ranking method in
improving LVLMs. Our code is available at https://github.com/yannqi/RCTS-RAG.

</details>


### [363] [Image Reconstruction as a Tool for Feature Analysis](https://arxiv.org/abs/2506.07803)
*Eduard Allakhverdov,Dmitrii Tarasov,Elizaveta Goncharova,Andrey Kuznetsov*

Main category: cs.CV

TL;DR: 提出了一种通过图像重建解释视觉特征的新方法，比较了SigLIP和SigLIP2模型，发现基于图像任务预训练的编码器保留更多信息，并揭示了特征空间的操作规律。


<details>
  <summary>Details</summary>
Motivation: 探索视觉编码器内部特征表示方式，以理解其工作机制。

Method: 通过图像重建比较不同视觉编码器的特征信息量，并分析特征空间的操作效果。

Result: 基于图像任务的编码器保留更多信息，特征空间的旋转操作影响颜色编码。

Conclusion: 该方法适用于任何视觉编码器，揭示了特征空间的结构规律。

Abstract: Vision encoders are increasingly used in modern applications, from
vision-only models to multimodal systems such as vision-language models.
Despite their remarkable success, it remains unclear how these architectures
represent features internally. Here, we propose a novel approach for
interpreting vision features via image reconstruction. We compare two related
model families, SigLIP and SigLIP2, which differ only in their training
objective, and show that encoders pre-trained on image-based tasks retain
significantly more image information than those trained on non-image tasks such
as contrastive learning. We further apply our method to a range of vision
encoders, ranking them by the informativeness of their feature representations.
Finally, we demonstrate that manipulating the feature space yields predictable
changes in reconstructed images, revealing that orthogonal rotations (rather
than spatial transformations) control color encoding. Our approach can be
applied to any vision encoder, shedding light on the inner structure of its
feature space. The code and model weights to reproduce the experiments are
available in GitHub.

</details>


### [364] [Incorporating Uncertainty-Guided and Top-k Codebook Matching for Real-World Blind Image Super-Resolution](https://arxiv.org/abs/2506.07809)
*Weilei Wen,Tianyi Zhang,Qianqian Zhao,Zhaohui Zheng,Chunle Guo,Xiuli Shao,Chongyi Li*

Main category: cs.CV

TL;DR: 论文提出了一种基于不确定性引导和Top-k代码书匹配的超分辨率框架（UGTSR），解决了现有方法在特征匹配和纹理重建上的不足。


<details>
  <summary>Details</summary>
Motivation: 现有基于代码书的超分辨率方法在特征匹配和纹理细节重建上存在不足，影响了实际应用效果。

Method: UGTSR框架包含三个关键部分：不确定性学习机制、Top-k特征匹配策略和Align-Attention模块。

Result: 实验结果表明，UGTSR在纹理真实性和重建保真度上显著优于现有方法。

Conclusion: UGTSR框架有效提升了超分辨率任务中的特征匹配和纹理重建性能。

Abstract: Recent advancements in codebook-based real image super-resolution (SR) have
shown promising results in real-world applications. The core idea involves
matching high-quality image features from a codebook based on low-resolution
(LR) image features. However, existing methods face two major challenges:
inaccurate feature matching with the codebook and poor texture detail
reconstruction. To address these issues, we propose a novel Uncertainty-Guided
and Top-k Codebook Matching SR (UGTSR) framework, which incorporates three key
components: (1) an uncertainty learning mechanism that guides the model to
focus on texture-rich regions, (2) a Top-k feature matching strategy that
enhances feature matching accuracy by fusing multiple candidate features, and
(3) an Align-Attention module that enhances the alignment of information
between LR and HR features. Experimental results demonstrate significant
improvements in texture realism and reconstruction fidelity compared to
existing methods. We will release the code upon formal publication.

</details>


### [365] [Looking Beyond Visible Cues: Implicit Video Question Answering via Dual-Clue Reasoning](https://arxiv.org/abs/2506.07811)
*Tieyuan Chen,Huabin Liu,Yi Wang,Chaofan Gan,Mingxi Lyu,Gui Zou,Weiyao Lin*

Main category: cs.CV

TL;DR: 该论文提出了一个新的任务和数据集I-VQA，专注于回答无法直接获取显式视觉证据的问题，并提出了IRM框架，通过双流建模提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有VideoQA方法依赖显式视觉证据，但在涉及符号意义或深层意图时表现不佳，因此需要解决隐式视觉证据的问题。

Method: 提出了IRM框架，包含AIM和VEM模块，分别用于生成线索候选和增强视觉表示。

Result: IRM在I-VQA任务中表现优于GPT-4o等模型，并在类似任务中达到SOTA。

Conclusion: IRM有效解决了隐式视觉证据的问题，为VideoQA提供了新的研究方向。

Abstract: Video Question Answering (VideoQA) aims to answer natural language questions
based on the given video, with prior work primarily focusing on identifying the
duration of relevant segments, referred to as explicit visual evidence.
However, explicit visual evidence is not always directly available,
particularly when questions target symbolic meanings or deeper intentions,
leading to significant performance degradation. To fill this gap, we introduce
a novel task and dataset, $\textbf{I}$mplicit $\textbf{V}$ideo
$\textbf{Q}$uestion $\textbf{A}$nswering (I-VQA), which focuses on answering
questions in scenarios where explicit visual evidence is inaccessible. Given an
implicit question and its corresponding video, I-VQA requires answering based
on the contextual visual cues present within the video. To tackle I-VQA, we
propose a novel reasoning framework, IRM (Implicit Reasoning Model),
incorporating dual-stream modeling of contextual actions and intent clues as
implicit reasoning chains. IRM comprises the Action-Intent Module (AIM) and the
Visual Enhancement Module (VEM). AIM deduces and preserves question-related
dual clues by generating clue candidates and performing relation deduction. VEM
enhances contextual visual representation by leveraging key contextual clues.
Extensive experiments validate the effectiveness of our IRM in I-VQA tasks,
outperforming GPT-4o, OpenAI-o3, and fine-tuned VideoChat2 by $0.76\%$,
$1.37\%$, and $4.87\%$, respectively. Additionally, IRM performs SOTA on
similar implicit advertisement understanding and future prediction in
traffic-VQA. Datasets and codes are available for double-blind review in
anonymous repo: https://github.com/tychen-SJTU/Implicit-VideoQA.

</details>


### [366] [Self-Cascaded Diffusion Models for Arbitrary-Scale Image Super-Resolution](https://arxiv.org/abs/2506.07813)
*Junseo Bang,Joonhee Lee,Kyeonghyun Lee,Haechang Lee,Dong Un Kang,Se Young Chun*

Main category: cs.CV

TL;DR: CasArbi是一种新型的自级联扩散框架，用于任意尺度图像超分辨率，通过逐步增强分辨率实现灵活上采样。


<details>
  <summary>Details</summary>
Motivation: 传统固定尺度超分辨率方法缺乏灵活性，而现有任意尺度方法多为单阶段上采样，难以学习广泛的连续尺度分布。

Method: CasArbi采用自级联扩散框架，将大尺度分解为小尺度序列，逐步增强分辨率，并结合坐标引导的残差扩散模型。

Result: CasArbi在多种任意尺度超分辨率基准测试中，在感知和失真性能指标上均优于现有方法。

Conclusion: CasArbi通过逐步扩散和连续表示学习，为任意尺度超分辨率提供了高效且性能优越的解决方案。

Abstract: Arbitrary-scale image super-resolution aims to upsample images to any desired
resolution, offering greater flexibility than traditional fixed-scale
super-resolution. Recent approaches in this domain utilize regression-based or
generative models, but many of them are a single-stage upsampling process,
which may be challenging to learn across a wide, continuous distribution of
scaling factors. Progressive upsampling strategies have shown promise in
mitigating this issue, yet their integration with diffusion models for flexible
upscaling remains underexplored. Here, we present CasArbi, a novel
self-cascaded diffusion framework for arbitrary-scale image super-resolution.
CasArbi meets the varying scaling demands by breaking them down into smaller
sequential factors and progressively enhancing the image resolution at each
step with seamless transitions for arbitrary scales. Our novel
coordinate-guided residual diffusion model allows for the learning of
continuous image representations while enabling efficient diffusion sampling.
Extensive experiments demonstrate that our CasArbi outperforms prior arts in
both perceptual and distortion performance metrics across diverse
arbitrary-scale super-resolution benchmarks.

</details>


### [367] [M2Restore: Mixture-of-Experts-based Mamba-CNN Fusion Framework for All-in-One Image Restoration](https://arxiv.org/abs/2506.07814)
*Yongzhen Wang,Yongjun Li,Zhuoran Zheng,Xiao-Ping Zhang,Mingqiang Wei*

Main category: cs.CV

TL;DR: M2Restore提出了一种基于Mixture-of-Experts的Mamba-CNN融合框架，用于高效且鲁棒的全能图像恢复，解决了现有方法在动态变化退化场景中泛化能力不足和局部细节与全局依赖平衡不佳的问题。


<details>
  <summary>Details</summary>
Motivation: 自然图像常受复合退化（如雨、雪、雾）影响，现有图像恢复方法在动态退化场景中泛化能力有限，且难以平衡局部细节与全局依赖。

Method: 1. 使用CLIP引导的MoE门控机制，结合任务条件提示和CLIP语义先验；2. 设计双流架构，融合CNN的局部表征能力和Mamba的长程建模效率；3. 引入边缘感知动态门控机制，自适应平衡全局建模与局部增强。

Result: 在多个图像恢复基准测试中，M2Restore在视觉质量和定量性能上均表现出优越性。

Conclusion: M2Restore通过创新的架构和机制，显著提升了图像恢复的泛化能力和细节保持效果。

Abstract: Natural images are often degraded by complex, composite degradations such as
rain, snow, and haze, which adversely impact downstream vision applications.
While existing image restoration efforts have achieved notable success, they
are still hindered by two critical challenges: limited generalization across
dynamically varying degradation scenarios and a suboptimal balance between
preserving local details and modeling global dependencies. To overcome these
challenges, we propose M2Restore, a novel Mixture-of-Experts (MoE)-based
Mamba-CNN fusion framework for efficient and robust all-in-one image
restoration. M2Restore introduces three key contributions: First, to boost the
model's generalization across diverse degradation conditions, we exploit a
CLIP-guided MoE gating mechanism that fuses task-conditioned prompts with
CLIP-derived semantic priors. This mechanism is further refined via cross-modal
feature calibration, which enables precise expert selection for various
degradation types. Second, to jointly capture global contextual dependencies
and fine-grained local details, we design a dual-stream architecture that
integrates the localized representational strength of CNNs with the long-range
modeling efficiency of Mamba. This integration enables collaborative
optimization of global semantic relationships and local structural fidelity,
preserving global coherence while enhancing detail restoration. Third, we
introduce an edge-aware dynamic gating mechanism that adaptively balances
global modeling and local enhancement by reallocating computational attention
to degradation-sensitive regions. This targeted focus leads to more efficient
and precise restoration. Extensive experiments across multiple image
restoration benchmarks validate the superiority of M2Restore in both visual
quality and quantitative performance.

</details>


### [368] [Diffusion models under low-noise regime](https://arxiv.org/abs/2506.07841)
*Elizabeth Pavlova,Xue-Xin Wei*

Main category: cs.CV

TL;DR: 论文研究了扩散模型在低噪声条件下的行为，揭示了训练数据规模、数据几何和模型目标对去噪轨迹的影响。


<details>
  <summary>Details</summary>
Motivation: 填补扩散模型在低噪声条件下行为的理解空白，探讨其在实际应用中的可靠性和可解释性。

Method: 使用CelebA子集和解析高斯混合基准，分析模型在低噪声扩散动力学下的行为。

Result: 发现模型在数据流形附近的行为会因训练数据不同而分化，即使高噪声输出一致。

Conclusion: 研究为理解扩散模型如何学习数据分布提供了新视角，并对其在实际应用中的可靠性提出了见解。

Abstract: Recent work on diffusion models proposed that they operate in two regimes:
memorization, in which models reproduce their training data, and
generalization, in which they generate novel samples. While this has been
tested in high-noise settings, the behavior of diffusion models as effective
denoisers when the corruption level is small remains unclear. To address this
gap, we systematically investigated the behavior of diffusion models under
low-noise diffusion dynamics, with implications for model robustness and
interpretability. Using (i) CelebA subsets of varying sample sizes and (ii)
analytic Gaussian mixture benchmarks, we reveal that models trained on disjoint
data diverge near the data manifold even when their high-noise outputs
converge. We quantify how training set size, data geometry, and model objective
choice shape denoising trajectories and affect score accuracy, providing
insights into how these models actually learn representations of data
distributions. This work starts to address gaps in our understanding of
generative model reliability in practical applications where small
perturbations are common.

</details>


### [369] [F2Net: A Frequency-Fused Network for Ultra-High Resolution Remote Sensing Segmentation](https://arxiv.org/abs/2506.07847)
*Hengzhi Chen,Liqian Feng,Wenhua Wu,Xiaogang Zhu,Shawn Leo,Kun Hu*

Main category: cs.CV

TL;DR: F2Net是一种频率感知框架，通过分解超高分辨率遥感图像为高频和低频成分进行专门处理，解决了传统方法在细节丢失和全局上下文碎片化之间的权衡问题。


<details>
  <summary>Details</summary>
Motivation: 超高分辨率遥感图像的语义分割在环境监测和城市规划中至关重要，但传统方法存在计算和优化挑战，如细节丢失或全局上下文碎片化。

Method: F2Net将图像分解为高频和低频成分，高频分支保留全分辨率结构细节，低频分支通过双子分支捕获短程和长程依赖关系，并通过混合频率融合模块整合结果。

Result: 在DeepGlobe和Inria Aerial基准测试中，F2Net分别达到80.22和83.39的mIoU，表现最优。

Conclusion: F2Net通过频率分解和融合模块，结合新颖的损失函数，实现了高效且稳定的训练，为超高分辨率遥感图像的语义分割提供了新方法。

Abstract: Semantic segmentation of ultra-high-resolution (UHR) remote sensing imagery
is critical for applications like environmental monitoring and urban planning
but faces computational and optimization challenges. Conventional methods
either lose fine details through downsampling or fragment global context via
patch processing. While multi-branch networks address this trade-off, they
suffer from computational inefficiency and conflicting gradient dynamics during
training. We propose F2Net, a frequency-aware framework that decomposes UHR
images into high- and low-frequency components for specialized processing. The
high-frequency branch preserves full-resolution structural details, while the
low-frequency branch processes downsampled inputs through dual sub-branches
capturing short- and long-range dependencies. A Hybrid-Frequency Fusion module
integrates these observations, guided by two novel objectives: Cross-Frequency
Alignment Loss ensures semantic consistency between frequency components, and
Cross-Frequency Balance Loss regulates gradient magnitudes across branches to
stabilize training. Evaluated on DeepGlobe and Inria Aerial benchmarks, F2Net
achieves state-of-the-art performance with mIoU of 80.22 and 83.39,
respectively. Our code will be publicly available.

</details>


### [370] [PolyVivid: Vivid Multi-Subject Video Generation with Cross-Modal Interaction and Enhancement](https://arxiv.org/abs/2506.07848)
*Teng Hu,Zhentao Yu,Zhengguang Zhou,Jiangning Zhang,Yuan Zhou,Qinglin Lu,Ran Yi*

Main category: cs.CV

TL;DR: PolyVivid是一个多主体视频定制框架，通过文本-图像融合模块和3D-RoPE增强模块实现身份一致性和交互控制。


<details>
  <summary>Details</summary>
Motivation: 现有视频生成模型在多主体定制中缺乏细粒度控制和身份一致性。

Method: 设计了VLLM文本-图像融合模块、3D-RoPE增强模块和注意力继承身份注入模块，结合MLLM数据管道。

Result: 实验表明PolyVivid在身份保真度、视频真实性和主体对齐方面优于现有方法。

Conclusion: PolyVivid在多主体视频定制中表现出色，具有灵活性和高保真度。

Abstract: Despite recent advances in video generation, existing models still lack
fine-grained controllability, especially for multi-subject customization with
consistent identity and interaction. In this paper, we propose PolyVivid, a
multi-subject video customization framework that enables flexible and
identity-consistent generation. To establish accurate correspondences between
subject images and textual entities, we design a VLLM-based text-image fusion
module that embeds visual identities into the textual space for precise
grounding. To further enhance identity preservation and subject interaction, we
propose a 3D-RoPE-based enhancement module that enables structured
bidirectional fusion between text and image embeddings. Moreover, we develop an
attention-inherited identity injection module to effectively inject fused
identity features into the video generation process, mitigating identity drift.
Finally, we construct an MLLM-based data pipeline that combines MLLM-based
grounding, segmentation, and a clique-based subject consolidation strategy to
produce high-quality multi-subject data, effectively enhancing subject
distinction and reducing ambiguity in downstream video generation. Extensive
experiments demonstrate that PolyVivid achieves superior performance in
identity fidelity, video realism, and subject alignment, outperforming existing
open-source and commercial baselines.

</details>


### [371] [SAM2Auto: Auto Annotation Using FLASH](https://arxiv.org/abs/2506.07850)
*Arash Rocky,Q. M. Jonathan Wu*

Main category: cs.CV

TL;DR: SAM2Auto是一种全自动视频数据集标注工具，无需人工干预或特定数据集训练，通过结合对象检测和视频实例分割技术，显著减少标注时间和成本。


<details>
  <summary>Details</summary>
Motivation: 解决视觉语言模型（VLMs）因标注数据稀缺而发展受限的问题，传统标注方法耗时且昂贵。

Method: 采用SMART-OD（结合自动掩码生成和开放世界对象检测）和FLASH（多对象实时视频实例分割）技术，确保跨帧一致性并减少误检。

Result: 实验表明，SAM2Auto的标注精度与人工相当，同时大幅降低时间和成本，适用于多样化数据集。

Conclusion: SAM2Auto为自动视频标注设定了新基准，有望加速视觉语言模型的发展。

Abstract: Vision-Language Models (VLMs) lag behind Large Language Models due to the
scarcity of annotated datasets, as creating paired visual-textual annotations
is labor-intensive and expensive. To address this bottleneck, we introduce
SAM2Auto, the first fully automated annotation pipeline for video datasets
requiring no human intervention or dataset-specific training. Our approach
consists of two key components: SMART-OD, a robust object detection system that
combines automatic mask generation with open-world object detection
capabilities, and FLASH (Frame-Level Annotation and Segmentation Handler), a
multi-object real-time video instance segmentation (VIS) that maintains
consistent object identification across video frames even with intermittent
detection gaps. Unlike existing open-world detection methods that require
frame-specific hyperparameter tuning and suffer from numerous false positives,
our system employs statistical approaches to minimize detection errors while
ensuring consistent object tracking throughout entire video sequences.
Extensive experimental validation demonstrates that SAM2Auto achieves
comparable accuracy to manual annotation while dramatically reducing annotation
time and eliminating labor costs. The system successfully handles diverse
datasets without requiring retraining or extensive parameter adjustments,
making it a practical solution for large-scale dataset creation. Our work
establishes a new baseline for automated video annotation and provides a
pathway for accelerating VLM development by addressing the fundamental dataset
bottleneck that has constrained progress in vision-language understanding.

</details>


### [372] [Egocentric Event-Based Vision for Ping Pong Ball Trajectory Prediction](https://arxiv.org/abs/2506.07860)
*Ivan Alberico,Marco Cannici,Giovanni Cioffi,Davide Scaramuzza*

Main category: cs.CV

TL;DR: 提出了一种基于事件相机的实时乒乓球轨迹预测系统，利用高时间分辨率解决传统相机延迟和运动模糊问题，并通过注视数据优化计算资源分配。


<details>
  <summary>Details</summary>
Motivation: 传统相机在高速乒乓球运动中存在延迟和运动模糊问题，事件相机的高时间分辨率能提供更频繁的状态更新和更准确的轨迹预测。

Method: 收集包含3D真实轨迹的乒乓球比赛数据集，结合事件流和注视数据，采用注视视觉技术优化资源分配，实现低延迟检测和轨迹预测。

Result: 系统总延迟最低为4.5毫秒，计算效率提升10.81倍，显著优于30 FPS的传统相机系统。

Conclusion: 首次实现基于事件相机的第一视角乒乓球轨迹预测，展示了高时间分辨率和注视视觉在实时运动分析中的潜力。

Abstract: In this paper, we present a real-time egocentric trajectory prediction system
for table tennis using event cameras. Unlike standard cameras, which suffer
from high latency and motion blur at fast ball speeds, event cameras provide
higher temporal resolution, allowing more frequent state updates, greater
robustness to outliers, and accurate trajectory predictions using just a short
time window after the opponent's impact. We collect a dataset of ping-pong game
sequences, including 3D ground-truth trajectories of the ball, synchronized
with sensor data from the Meta Project Aria glasses and event streams. Our
system leverages foveated vision, using eye-gaze data from the glasses to
process only events in the viewer's fovea. This biologically inspired approach
improves ball detection performance and significantly reduces computational
latency, as it efficiently allocates resources to the most perceptually
relevant regions, achieving a reduction factor of 10.81 on the collected
trajectories. Our detection pipeline has a worst-case total latency of 4.5 ms,
including computation and perception - significantly lower than a frame-based
30 FPS system, which, in the worst case, takes 66 ms solely for perception.
Finally, we fit a trajectory prediction model to the estimated states of the
ball, enabling 3D trajectory forecasting in the future. To the best of our
knowledge, this is the first approach to predict table tennis trajectories from
an egocentric perspective using event cameras.

</details>


### [373] [Spatio-Temporal State Space Model For Efficient Event-Based Optical Flow](https://arxiv.org/abs/2506.07878)
*Muhammad Ahmed Humais,Xiaoqian Huang,Hussain Sajwani,Sajid Javed,Yahya Zweiri*

Main category: cs.CV

TL;DR: 提出了一种基于时空状态空间模型（STSSM）的高效事件相机光流估计方法，显著提升了计算效率。


<details>
  <summary>Details</summary>
Motivation: 事件相机在低延迟运动估计中具有潜力，但现有深度学习方法效率不足，而异步事件方法缺乏时空信息。

Method: 引入STSSM模块和新网络架构，利用状态空间模型高效捕捉事件数据的时空相关性。

Result: 在DSEC基准测试中，模型推理速度提升4.5倍，计算量减少8倍（相比TMA）或2倍（相比EV-FlowNet），性能保持竞争力。

Conclusion: STSSM方法在事件相机光流估计中实现了高效与性能的平衡，为实时应用提供了可行方案。

Abstract: Event cameras unlock new frontiers that were previously unthinkable with
standard frame-based cameras. One notable example is low-latency motion
estimation (optical flow), which is critical for many real-time applications.
In such applications, the computational efficiency of algorithms is paramount.
Although recent deep learning paradigms such as CNN, RNN, or ViT have shown
remarkable performance, they often lack the desired computational efficiency.
Conversely, asynchronous event-based methods including SNNs and GNNs are
computationally efficient; however, these approaches fail to capture sufficient
spatio-temporal information, a powerful feature required to achieve better
performance for optical flow estimation. In this work, we introduce
Spatio-Temporal State Space Model (STSSM) module along with a novel network
architecture to develop an extremely efficient solution with competitive
performance. Our STSSM module leverages state-space models to effectively
capture spatio-temporal correlations in event data, offering higher performance
with lower complexity compared to ViT, CNN-based architectures in similar
settings. Our model achieves 4.5x faster inference and 8x lower computations
compared to TMA and 2x lower computations compared to EV-FlowNet with
competitive performance on the DSEC benchmark. Our code will be available at
https://github.com/AhmedHumais/E-STMFlow

</details>


### [374] [CrosswalkNet: An Optimized Deep Learning Framework for Pedestrian Crosswalk Detection in Aerial Images with High-Performance Computing](https://arxiv.org/abs/2506.07885)
*Zubin Bhuyan,Yuanchang Xie,AngkeaReach Rith,Xintong Yan,Nasko Apostolov,Jimi Oke,Chengbo Ai*

Main category: cs.CV

TL;DR: CrosswalkNet是一种高效的深度学习框架，用于从高分辨率航拍图像中检测行人横道，采用定向边界框（OBB）提升精度，并在多州数据集上验证了其鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 随着航拍和卫星图像的普及，深度学习在交通资产管理、安全分析和城市规划中具有巨大潜力。

Method: CrosswalkNet结合了定向边界框（OBB）、卷积块注意力、双分支空间金字塔池化快速模块和余弦退火等技术，优化检测性能。

Result: 在麻省数据集上，模型精度达96.5%，召回率93.3%，并在其他州数据集上无需微调即表现优异。

Conclusion: CrosswalkNet为政策制定者和城市规划者提供了高效工具，可提升行人安全和城市流动性。

Abstract: With the increasing availability of aerial and satellite imagery, deep
learning presents significant potential for transportation asset management,
safety analysis, and urban planning. This study introduces CrosswalkNet, a
robust and efficient deep learning framework designed to detect various types
of pedestrian crosswalks from 15-cm resolution aerial images. CrosswalkNet
incorporates a novel detection approach that improves upon traditional object
detection strategies by utilizing oriented bounding boxes (OBB), enhancing
detection precision by accurately capturing crosswalks regardless of their
orientation. Several optimization techniques, including Convolutional Block
Attention, a dual-branch Spatial Pyramid Pooling-Fast module, and cosine
annealing, are implemented to maximize performance and efficiency. A
comprehensive dataset comprising over 23,000 annotated crosswalk instances is
utilized to train and validate the proposed framework. The best-performing
model achieves an impressive precision of 96.5% and a recall of 93.3% on aerial
imagery from Massachusetts, demonstrating its accuracy and effectiveness.
CrosswalkNet has also been successfully applied to datasets from New Hampshire,
Virginia, and Maine without transfer learning or fine-tuning, showcasing its
robustness and strong generalization capability. Additionally, the crosswalk
detection results, processed using High-Performance Computing (HPC) platforms
and provided in polygon shapefile format, have been shown to accelerate data
processing and detection, supporting real-time analysis for safety and mobility
applications. This integration offers policymakers, transportation engineers,
and urban planners an effective instrument to enhance pedestrian safety and
improve urban mobility.

</details>


### [375] [EgoM2P: Egocentric Multimodal Multitask Pretraining](https://arxiv.org/abs/2506.07886)
*Gen Li,Yutong Chen,Yiqian Wu,Kaifeng Zhao,Marc Pollefeys,Siyu Tang*

Main category: cs.CV

TL;DR: 论文提出EgoM2P框架，通过高效的时间标记器和掩码建模解决多模态自我中心视觉的挑战，支持多任务处理并优于专业模型。


<details>
  <summary>Details</summary>
Motivation: 自我中心视觉的多模态信号理解对增强现实、机器人等领域至关重要，但数据异构性和缺失模态标签等问题限制了现有方法的扩展。

Method: 引入高效时间标记器，提出EgoM2P掩码建模框架，利用时间感知多模态标记训练通用模型。

Result: EgoM2P在多任务中表现优异，速度更快，且支持条件视频生成。

Conclusion: EgoM2P为自我中心视觉研究提供了高效、通用的解决方案，并将开源以推动社区发展。

Abstract: Understanding multimodal signals in egocentric vision, such as RGB video,
depth, camera poses, and gaze, is essential for applications in augmented
reality, robotics, and human-computer interaction. These capabilities enable
systems to better interpret the camera wearer's actions, intentions, and
surrounding environment. However, building large-scale egocentric multimodal
and multitask models presents unique challenges. Egocentric data are inherently
heterogeneous, with large variations in modality coverage across devices and
settings. Generating pseudo-labels for missing modalities, such as gaze or
head-mounted camera trajectories, is often infeasible, making standard
supervised learning approaches difficult to scale. Furthermore, dynamic camera
motion and the complex temporal and spatial structure of first-person video
pose additional challenges for the direct application of existing multimodal
foundation models.
  To address these challenges, we introduce a set of efficient temporal
tokenizers and propose EgoM2P, a masked modeling framework that learns from
temporally aware multimodal tokens to train a large, general-purpose model for
egocentric 4D understanding. This unified design supports multitasking across
diverse egocentric perception and synthesis tasks, including gaze prediction,
egocentric camera tracking, and monocular depth estimation from egocentric
video. EgoM2P also serves as a generative model for conditional egocentric
video synthesis. Across these tasks, EgoM2P matches or outperforms specialist
models while being an order of magnitude faster. We will fully open-source
EgoM2P to support the community and advance egocentric vision research. Project
page: https://egom2p.github.io/

</details>


### [376] [Video Unlearning via Low-Rank Refusal Vector](https://arxiv.org/abs/2506.07891)
*Simone Facchiano,Stefano Saravalle,Matteo Migliarini,Edoardo De Matteis,Alessio Sampieri,Andrea Pilzer,Emanuele Rodolà,Indro Spinelli,Luca Franco,Fabio Galasso*

Main category: cs.CV

TL;DR: 提出了一种针对视频扩散模型的无学习技术，仅需5对多模态提示对即可消除有害内容，同时保持生成质量。


<details>
  <summary>Details</summary>
Motivation: 视频生成模型可能继承训练数据中的偏见和有害内容，存在生成不良或非法内容的风险。

Method: 通过计算安全与不安全示例的潜在差异生成“拒绝向量”，并采用低秩分解方法隔离目标概念，避免影响其他语义。

Result: 能够有效消除多种有害内容（如裸露、暴力、版权等），且无需重新训练或原始数据。

Conclusion: 该方法通过直接嵌入拒绝方向到模型权重，提升了对抗绕过尝试的鲁棒性，同时保持了视频生成质量。

Abstract: Video generative models democratize the creation of visual content through
intuitive instruction following, but they also inherit the biases and harmful
concepts embedded within their web-scale training data. This inheritance
creates a significant risk, as users can readily generate undesirable and even
illegal content. This work introduces the first unlearning technique tailored
explicitly for video diffusion models to address this critical issue. Our
method requires 5 multi-modal prompt pairs only. Each pair contains a "safe"
and an "unsafe" example that differ only by the target concept. Averaging their
per-layer latent differences produces a "refusal vector", which, once
subtracted from the model parameters, neutralizes the unsafe concept. We
introduce a novel low-rank factorization approach on the covariance difference
of embeddings that yields robust refusal vectors. This isolates the target
concept while minimizing collateral unlearning of other semantics, thus
preserving the visual quality of the generated video. Our method preserves the
model's generation quality while operating without retraining or access to the
original training data. By embedding the refusal direction directly into the
model's weights, the suppression mechanism becomes inherently more robust
against adversarial bypass attempts compared to surface-level input-output
filters. In a thorough qualitative and quantitative evaluation, we show that we
can neutralize a variety of harmful contents, including explicit nudity,
graphic violence, copyrights, and trademarks. Project page:
https://www.pinlab.org/video-unlearning.

</details>


### [377] [WeThink: Toward General-purpose Vision-Language Reasoning via Reinforcement Learning](https://arxiv.org/abs/2506.07905)
*Jie Yang,Feipeng Ma,Zitian Wang,Dacheng Yin,Kang Rong,Fengyun Rao,Ruimao Zhang*

Main category: cs.CV

TL;DR: 论文提出了一种通过强化学习实现通用视觉-语言推理的方法，包括生成多模态QA对的数据集WeThink，以及探索混合奖励机制的RL训练。


<details>
  <summary>Details</summary>
Motivation: 扩展基于文本的推理模型（如DeepSeek-R1）到多模态领域，解决通用视觉-语言推理的挑战。

Method: 1. 开发可扩展的多模态QA合成流水线；2. 构建包含12万对多模态QA的WeThink数据集；3. 探索混合奖励机制的RL训练。

Result: WeThink数据集显著提升了14个多模态基准任务的性能，数据流水线可持续提升多样性。

Conclusion: 通过数据集和RL训练方法，成功实现了通用视觉-语言推理的性能提升。

Abstract: Building on the success of text-based reasoning models like DeepSeek-R1,
extending these capabilities to multimodal reasoning holds great promise. While
recent works have attempted to adapt DeepSeek-R1-style reinforcement learning
(RL) training paradigms to multimodal large language models (MLLM), focusing on
domain-specific tasks like math and visual perception, a critical question
remains: How can we achieve the general-purpose visual-language reasoning
through RL? To address this challenge, we make three key efforts: (1) A novel
Scalable Multimodal QA Synthesis pipeline that autonomously generates
context-aware, reasoning-centric question-answer (QA) pairs directly from the
given images. (2) The open-source WeThink dataset containing over 120K
multimodal QA pairs with annotated reasoning paths, curated from 18 diverse
dataset sources and covering various question domains. (3) A comprehensive
exploration of RL on our dataset, incorporating a hybrid reward mechanism that
combines rule-based verification with model-based assessment to optimize RL
training efficiency across various task domains. Across 14 diverse MLLM
benchmarks, we demonstrate that our WeThink dataset significantly enhances
performance, from mathematical reasoning to diverse general multimodal tasks.
Moreover, we show that our automated data pipeline can continuously increase
data diversity to further improve model performance.

</details>


### [378] [A Comparative Study of U-Net Architectures for Change Detection in Satellite Images](https://arxiv.org/abs/2506.07925)
*Yaxita Amin,Naimisha S Trivedi,Rashmi Bhattad*

Main category: cs.CV

TL;DR: 本文通过分析34篇论文，比较了18种U-Net变体在遥感变化检测中的应用，填补了该领域的研究空白。


<details>
  <summary>Details</summary>
Motivation: 遥感变化检测对监测地球景观变化至关重要，但U-Net架构在该领域的应用尚未充分探索。

Method: 研究比较了18种U-Net变体，评估其在遥感变化检测中的潜力，特别关注专为变化检测设计的变体（如Siamese Swin-U-Net）。

Result: 分析揭示了处理多时相数据和长距离关系对提升变化检测精度的重要性。

Conclusion: 研究为选择U-Net变体进行遥感变化检测提供了有价值的参考。

Abstract: Remote sensing change detection is essential for monitoring the everchanging
landscapes of the Earth. The U-Net architecture has gained popularity for its
capability to capture spatial information and perform pixel-wise
classification. However, their application in the Remote sensing field remains
largely unexplored. Therefore, this paper fill the gap by conducting a
comprehensive analysis of 34 papers. This study conducts a comparison and
analysis of 18 different U-Net variations, assessing their potential for
detecting changes in remote sensing. We evaluate both benefits along with
drawbacks of each variation within the framework of this particular
application. We emphasize variations that are explicitly built for change
detection, such as Siamese Swin-U-Net, which utilizes a Siamese architecture.
The analysis highlights the significance of aspects such as managing data from
different time periods and collecting relationships over a long distance to
enhance the precision of change detection. This study provides valuable
insights for researchers and practitioners that choose U-Net versions for
remote sensing change detection tasks.

</details>


### [379] [Mimicking or Reasoning: Rethinking Multi-Modal In-Context Learning in Vision-Language Models](https://arxiv.org/abs/2506.07936)
*Chengyue Huang,Yuchen Zhu,Sichen Zhu,Jingyun Xiao,Moises Andrade,Shivang Chopra,Zsolt Kira*

Main category: cs.CV

TL;DR: 研究发现，当前视觉语言模型（VLMs）在多模态上下文学习（MM-ICL）中表现不佳，倾向于依赖浅层启发式方法而非真正理解任务。


<details>
  <summary>Details</summary>
Motivation: 探索视觉语言模型是否真正具备多模态上下文学习能力，尤其是在分布变化的情况下。

Method: 提出了一种新的MM-ICL with Reasoning流程，为每个示例生成答案和推理过程，并在不同数据集和模型上进行实验。

Result: 实验表明，模型性能对演示数量、检索方法等不敏感，说明当前VLMs未能有效利用演示信息。

Conclusion: 当前VLMs在多模态上下文学习中表现有限，需进一步改进以提升任务理解能力。

Abstract: Vision-language models (VLMs) are widely assumed to exhibit in-context
learning (ICL), a property similar to that of their language-only counterparts.
While recent work suggests VLMs can perform multimodal ICL (MM-ICL), studies
show they often rely on shallow heuristics -- such as copying or majority
voting -- rather than true task understanding. We revisit this assumption by
evaluating VLMs under distribution shifts, where support examples come from a
dataset different from the query. Surprisingly, performance often degrades with
more demonstrations, and models tend to copy answers rather than learn from
them. To investigate further, we propose a new MM-ICL with Reasoning pipeline
that augments each demonstration with a generated rationale alongside the
answer. We conduct extensive and comprehensive experiments on both perception-
and reasoning-required datasets with open-source VLMs ranging from 3B to 72B
and proprietary models such as Gemini 2.0. We conduct controlled studies
varying shot count, retrieval method, rationale quality, and distribution. Our
results show limited performance sensitivity across these factors, suggesting
that current VLMs do not effectively utilize demonstration-level information as
intended in MM-ICL.

</details>


### [380] [Decoupling the Image Perception and Multimodal Reasoning for Reasoning Segmentation with Digital Twin Representations](https://arxiv.org/abs/2506.07943)
*Yizhen Li,Dell Zhang,Xuelong Li,Yiqing Shen*

Main category: cs.CV

TL;DR: DTwinSeger是一种新颖的推理分割方法，通过数字孪生（DT）表示将感知与推理解耦，利用LLM进行显式推理，在多个基准测试中达到最优性能。


<details>
  <summary>Details</summary>
Motivation: 当前推理分割方法依赖视觉语言模型（VLMs），但其图像标记化破坏了对象间的空间连续性关系。DTwinSeger旨在通过DT表示解决这一问题。

Method: DTwinSeger将推理分割分为两阶段：1）将图像转换为结构化DT表示；2）利用LLM在DT表示上进行显式推理。还提出了针对LLM的监督微调方法和数据集Seg-DT。

Result: 实验表明，DTwinSeger在两个图像推理分割基准和三个图像参考分割基准上达到了最优性能。

Conclusion: DT表示是视觉与文本间的有效桥梁，使复杂多模态推理任务仅需LLM即可完成。

Abstract: Reasoning Segmentation (RS) is a multimodal vision-text task that requires
segmenting objects based on implicit text queries, demanding both precise
visual perception and vision-text reasoning capabilities. Current RS approaches
rely on fine-tuning vision-language models (VLMs) for both perception and
reasoning, but their tokenization of images fundamentally disrupts continuous
spatial relationships between objects. We introduce DTwinSeger, a novel RS
approach that leverages Digital Twin (DT) representation as an intermediate
layer to decouple perception from reasoning. Innovatively, DTwinSeger
reformulates RS as a two-stage process, where the first transforms the image
into a structured DT representation that preserves spatial relationships and
semantic properties and then employs a Large Language Model (LLM) to perform
explicit reasoning over this representation to identify target objects. We
propose a supervised fine-tuning method specifically for LLM with DT
representation, together with a corresponding fine-tuning dataset Seg-DT, to
enhance the LLM's reasoning capabilities with DT representations. Experiments
show that our method can achieve state-of-the-art performance on two image RS
benchmarks and three image referring segmentation benchmarks. It yields that DT
representation functions as an effective bridge between vision and text,
enabling complex multimodal reasoning tasks to be accomplished solely with an
LLM.

</details>


### [381] [Creating a Historical Migration Dataset from Finnish Church Records, 1800-1920](https://arxiv.org/abs/2506.07960)
*Ari Vesalainen,Jenna Kanerva,Aida Nitsch,Kiia Korsu,Ilari Larkiola,Laura Ruotsalainen,Filip Ginter*

Main category: cs.CV

TL;DR: 本文介绍了利用深度学习技术从芬兰1800-1920年的教会迁移记录中提取结构化数据集的大规模工作。


<details>
  <summary>Details</summary>
Motivation: 研究历史人口模式，尤其是内部迁移、城市化和疾病传播。

Method: 使用深度学习流程自动化提取数据，包括布局分析、表格检测、单元格分类和手写识别。

Result: 生成了包含600多万条目的结构化数据集，支持历史与人口研究。

Conclusion: 展示了如何将大量手写档案转化为结构化数据，为历史研究提供支持。

Abstract: This article presents a large-scale effort to create a structured dataset of
internal migration in Finland between 1800 and 1920 using digitized church
moving records. These records, maintained by Evangelical-Lutheran parishes,
document the migration of individuals and families and offer a valuable source
for studying historical demographic patterns. The dataset includes over six
million entries extracted from approximately 200,000 images of handwritten
migration records.
  The data extraction process was automated using a deep learning pipeline that
included layout analysis, table detection, cell classification, and handwriting
recognition. The complete pipeline was applied to all images, resulting in a
structured dataset suitable for research.
  The dataset can be used to study internal migration, urbanization, and family
migration, and the spread of disease in preindustrial Finland. A case study
from the Elim\"aki parish shows how local migration histories can be
reconstructed. The work demonstrates how large volumes of handwritten archival
material can be transformed into structured data to support historical and
demographic research.

</details>


### [382] [SlideCoder: Layout-aware RAG-enhanced Hierarchical Slide Generation from Design](https://arxiv.org/abs/2506.07964)
*Wenxin Tang,Jingyu Xiao,Wenxuan Jiang,Xi Xiao,Yuhang Wang,Xuxin Tang,Qing Li,Yuehe Ma,Junliang Liu,Shisong Tang,Michael R. Lyu*

Main category: cs.CV

TL;DR: 论文提出Slide2Code基准和SlideCoder框架，用于从参考图像生成可编辑幻灯片，解决了现有方法难以捕捉幻灯片视觉和结构细节的问题。


<details>
  <summary>Details</summary>
Motivation: 手动创建幻灯片费时费力，现有基于自然语言的LLM生成方法无法充分捕捉幻灯片设计的视觉和结构细节。

Method: 提出Slide2Code基准和SlideCoder框架，结合颜色梯度分割算法和分层检索增强生成方法，分解复杂任务并优化代码生成。

Result: SlideCoder在布局保真度、执行准确性和视觉一致性上表现优异，比现有基线方法高出40.5分。

Conclusion: SlideCoder在幻灯片生成任务中表现出色，为自动化幻灯片设计提供了有效解决方案。

Abstract: Manual slide creation is labor-intensive and requires expert prior knowledge.
Existing natural language-based LLM generation methods struggle to capture the
visual and structural nuances of slide designs. To address this, we formalize
the Reference Image to Slide Generation task and propose Slide2Code, the first
benchmark with difficulty-tiered samples based on a novel Slide Complexity
Metric. We introduce SlideCoder, a layout-aware, retrieval-augmented framework
for generating editable slides from reference images. SlideCoder integrates a
Color Gradient-based Segmentation algorithm and a Hierarchical
Retrieval-Augmented Generation method to decompose complex tasks and enhance
code generation. We also release SlideMaster, a 7B open-source model fine-tuned
with improved reverse-engineered data. Experiments show that SlideCoder
outperforms state-of-the-art baselines by up to 40.5 points, demonstrating
strong performance across layout fidelity, execution accuracy, and visual
consistency. Our code is available at
https://github.com/vinsontang1/SlideCoder.

</details>


### [383] [SpaCE-10: A Comprehensive Benchmark for Multimodal Large Language Models in Compositional Spatial Intelligence](https://arxiv.org/abs/2506.07966)
*Ziyang Gong,Wenhao Li,Oliver Ma,Songyuan Li,Jiayi Ji,Xue Yang,Gen Luo,Junchi Yan,Rongrong Ji*

Main category: cs.CV

TL;DR: SpaCE-10是一个用于评估多模态大语言模型（MLLMs）空间智能的综合基准，包含10种原子空间能力和8种组合能力，通过5k+ QA对和811个室内场景数据，发现现有MLLMs在空间能力上仍显著落后于人类。


<details>
  <summary>Details</summary>
Motivation: 现有基准难以全面评估MLLMs从原子到组合层面的空间智能，因此需要SpaCE-10填补这一空白。

Method: 定义了10种原子空间能力和8种组合能力，采用分层标注流程生成高质量QA对，覆盖多种评估设置。

Result: 最先进的MLLMs在SpaCE-10上仍大幅落后于人类，特别是计数能力的不足限制了组合空间能力。

Conclusion: SpaCE-10为MLLMs空间智能评估提供了重要工具，揭示了现有模型的局限性，并提出了改进方向。

Abstract: Multimodal Large Language Models (MLLMs) have achieved remarkable progress in
various multimodal tasks. To pursue higher intelligence in space, MLLMs require
integrating multiple atomic spatial capabilities to handle complex and dynamic
tasks. However, existing benchmarks struggle to comprehensively evaluate the
spatial intelligence of common MLLMs from the atomic level to the compositional
level. To fill this gap, we present SpaCE-10, a comprehensive benchmark for
compositional spatial evaluations. In SpaCE-10, we define 10 atomic spatial
capabilities, which are combined to form 8 compositional capabilities. Based on
these definitions, we propose a novel hierarchical annotation pipeline to
generate high-quality and diverse question-answer (QA) pairs. With over 150+
hours of human expert effort, we obtain over 5k QA pairs for 811 real indoor
scenes in SpaCE-10, which covers various evaluation settings like point cloud
input and multi-choice QA. We conduct an extensive evaluation of common MLLMs
on SpaCE-10 and find that even the most advanced MLLM still lags behind humans
by large margins. Through our careful study, we also draw several significant
findings that benefit the MLLM community. For example, we reveal that the
shortcoming of counting capability greatly limits the compositional spatial
capabilities of existing MLLMs. The evaluation code and benchmark datasets are
available at https://github.com/Cuzyoung/SpaCE-10.

</details>


### [384] [CyberV: Cybernetics for Test-time Scaling in Video Understanding](https://arxiv.org/abs/2506.07971)
*Jiahao Meng,Shuyang Sun,Yue Tan,Lu Qi,Yunhai Tong,Xiangtai Li,Longyin Wen*

Main category: cs.CV

TL;DR: 论文提出了一种基于控制论原理的新框架CyberV，通过自适应系统设计提升多模态大语言模型（MLLMs）对长或复杂视频的理解能力。


<details>
  <summary>Details</summary>
Motivation: 当前MLLMs在测试时因计算需求高、鲁棒性不足和准确性有限，难以处理长或复杂视频，尤其是参数较少的模型。

Method: CyberV框架引入了一个控制论循环，包括MLLM推理系统、传感器和控制器，实现自监控、自校正和动态资源分配。

Result: 实验显示，CyberV显著提升了模型性能，如Qwen2.5-VL-7B提升8.3%，甚至达到人类专家水平。

Conclusion: CyberV有效增强了MLLMs的鲁棒性和准确性，适用于动态视频理解，并在通用基准测试中表现优异。

Abstract: Current Multimodal Large Language Models (MLLMs) may struggle with
understanding long or complex videos due to computational demands at test time,
lack of robustness, and limited accuracy, primarily stemming from their
feed-forward processing nature. These limitations could be more severe for
models with fewer parameters. To address these limitations, we propose a novel
framework inspired by cybernetic principles, redesigning video MLLMs as
adaptive systems capable of self-monitoring, self-correction, and dynamic
resource allocation during inference. Our approach, CyberV, introduces a
cybernetic loop consisting of an MLLM Inference System, a Sensor, and a
Controller. Specifically, the sensor monitors forward processes of the MLLM and
collects intermediate interpretations, such as attention drift, then the
controller determines when and how to trigger self-correction and generate
feedback to guide the next round. This test-time adaptive scaling framework
enhances frozen MLLMs without requiring retraining or additional components.
Experiments demonstrate significant improvements: CyberV boosts Qwen2.5-VL-7B
by 8.3% and InternVL3-8B by 5.5% on VideoMMMU, surpassing the competitive
proprietary model GPT-4o. When applied to Qwen2.5-VL-72B, it yields a 10.0%
improvement, achieving performance even comparable to human experts.
Furthermore, our method demonstrates consistent gains on general-purpose
benchmarks, such as VideoMME and WorldSense, highlighting its effectiveness and
generalization capabilities in making MLLMs more robust and accurate for
dynamic video understanding. The code is released at
https://github.com/marinero4972/CyberV.

</details>


### [385] [OneIG-Bench: Omni-dimensional Nuanced Evaluation for Image Generation](https://arxiv.org/abs/2506.07977)
*Jingjing Chang,Yixiao Fang,Peng Xing,Shuhan Wu,Wei Cheng,Rui Wang,Xianfang Zeng,Gang Yu,Hai-Bao Chen*

Main category: cs.CV

TL;DR: 论文介绍了OneIG-Bench，一个用于全面评估文本到图像（T2I）模型的基准框架，覆盖多维度评估，如提示-图像对齐、文本渲染精度、推理生成内容等。


<details>
  <summary>Details</summary>
Motivation: 现有T2I模型的评估系统未能全面覆盖推理、文本渲染和风格等维度，限制了模型的深入分析。

Method: 设计了OneIG-Bench框架，支持多维度细粒度评估，并允许用户灵活选择评估子集。

Result: OneIG-Bench提供了公开的代码和数据集，支持可重复的评估研究和跨模型比较。

Conclusion: OneIG-Bench填补了T2I模型评估的空白，为研究者和实践者提供了系统化的分析工具。

Abstract: Text-to-image (T2I) models have garnered significant attention for generating
high-quality images aligned with text prompts. However, rapid T2I model
advancements reveal limitations in early benchmarks, lacking comprehensive
evaluations, for example, the evaluation on reasoning, text rendering and
style. Notably, recent state-of-the-art models, with their rich knowledge
modeling capabilities, show promising results on the image generation problems
requiring strong reasoning ability, yet existing evaluation systems have not
adequately addressed this frontier. To systematically address these gaps, we
introduce OneIG-Bench, a meticulously designed comprehensive benchmark
framework for fine-grained evaluation of T2I models across multiple dimensions,
including prompt-image alignment, text rendering precision, reasoning-generated
content, stylization, and diversity. By structuring the evaluation, this
benchmark enables in-depth analysis of model performance, helping researchers
and practitioners pinpoint strengths and bottlenecks in the full pipeline of
image generation. Specifically, OneIG-Bench enables flexible evaluation by
allowing users to focus on a particular evaluation subset. Instead of
generating images for the entire set of prompts, users can generate images only
for the prompts associated with the selected dimension and complete the
corresponding evaluation accordingly. Our codebase and dataset are now publicly
available to facilitate reproducible evaluation studies and cross-model
comparisons within the T2I research community.

</details>


### [386] [Real-time Localization of a Soccer Ball from a Single Camera](https://arxiv.org/abs/2506.07981)
*Dmitrii Vorobev,Artem Prosvetov,Karim Elhadji Daou*

Main category: cs.CV

TL;DR: 提出一种高效的单摄像头实时三维足球轨迹重建方法，通过多模态状态模型加速优化，保持厘米级精度。


<details>
  <summary>Details</summary>
Motivation: 解决现有方法在遮挡、运动模糊和复杂背景下的性能问题，同时降低对多摄像头和昂贵设备的需求。

Method: 引入多模态状态模型（$W$离散模态）以加速优化，适用于标准CPU，实现低延迟。

Result: 在6K分辨率俄超比赛数据集上验证，性能媲美多摄像头系统，无需昂贵设备。

Conclusion: 提供了一种实用、低成本且高精度的三维足球轨迹跟踪方法。

Abstract: We propose a computationally efficient method for real-time three-dimensional
football trajectory reconstruction from a single broadcast camera. In contrast
to previous work, our approach introduces a multi-mode state model with $W$
discrete modes to significantly accelerate optimization while preserving
centimeter-level accuracy -- even in cases of severe occlusion, motion blur,
and complex backgrounds. The system operates on standard CPUs and achieves low
latency suitable for live broadcast settings. Extensive evaluation on a
proprietary dataset of 6K-resolution Russian Premier League matches
demonstrates performance comparable to multi-camera systems, without the need
for specialized or costly infrastructure. This work provides a practical method
for accessible and accurate 3D ball tracking in professional football
environments.

</details>


### [387] [CXR-LT 2024: A MICCAI challenge on long-tailed, multi-label, and zero-shot disease classification from chest X-ray](https://arxiv.org/abs/2506.07984)
*Mingquan Lin,Gregory Holste,Song Wang,Yiliang Zhou,Yishu Wei,Imon Banerjee,Pengyi Chen,Tianjie Dai,Yuexi Du,Nicha C. Dvornek,Yuyan Ge,Zuowei Guo,Shouhei Hanaoka,Dongkyun Kim,Pablo Messina,Yang Lu,Denis Parra,Donghyun Son,Álvaro Soto,Aisha Urooj,René Vidal,Yosuke Yamagishi,Zefan Yang,Ruichi Zhang,Yang Zhou,Leo Anthony Celi,Ronald M. Summers,Zhiyong Lu,Hao Chen,Adam Flanders,George Shih,Zhangyang Wang,Yifan Peng*

Main category: cs.CV

TL;DR: CXR-LT系列是一个社区驱动的项目，旨在通过胸部X光（CXR）提升肺部疾病分类技术。2024年版本扩展了数据集并引入零样本学习，以解决长尾分类和噪声标签问题。


<details>
  <summary>Details</summary>
Motivation: 解决开放长尾肺部疾病分类的挑战，并提升现有技术的可测量性。

Method: 提供高质量基准数据，进行综合评估，并引入零样本学习策略。

Result: 扩展数据集至377,110张CXR和45种疾病标签，包括19种新罕见疾病。

Conclusion: CXR-LT 2024为临床现实和泛化诊断模型的开发提供了宝贵资源。

Abstract: The CXR-LT series is a community-driven initiative designed to enhance lung
disease classification using chest X-rays (CXR). It tackles challenges in open
long-tailed lung disease classification and enhances the measurability of
state-of-the-art techniques. The first event, CXR-LT 2023, aimed to achieve
these goals by providing high-quality benchmark CXR data for model development
and conducting comprehensive evaluations to identify ongoing issues impacting
lung disease classification performance. Building on the success of CXR-LT
2023, the CXR-LT 2024 expands the dataset to 377,110 chest X-rays (CXRs) and 45
disease labels, including 19 new rare disease findings. It also introduces a
new focus on zero-shot learning to address limitations identified in the
previous event. Specifically, CXR-LT 2024 features three tasks: (i) long-tailed
classification on a large, noisy test set, (ii) long-tailed classification on a
manually annotated "gold standard" subset, and (iii) zero-shot generalization
to five previously unseen disease findings. This paper provides an overview of
CXR-LT 2024, detailing the data curation process and consolidating
state-of-the-art solutions, including the use of multimodal models for rare
disease detection, advanced generative approaches to handle noisy labels, and
zero-shot learning strategies for unseen diseases. Additionally, the expanded
dataset enhances disease coverage to better represent real-world clinical
settings, offering a valuable resource for future research. By synthesizing the
insights and innovations of participating teams, we aim to advance the
development of clinically realistic and generalizable diagnostic models for
chest radiography.

</details>


### [388] [Rethinking Crowd-Sourced Evaluation of Neuron Explanations](https://arxiv.org/abs/2506.07985)
*Tuomas Oikarinen,Ge Yan,Akshay Kulkarni,Tsui-Wei Weng*

Main category: cs.CV

TL;DR: 本文提出了一种高效且准确的众包评估策略，用于评估神经元解释的可靠性，并通过重要性采样和贝叶斯方法显著降低了成本。


<details>
  <summary>Details</summary>
Motivation: 现有神经元解释方法的可靠性难以评估，传统众包评估成本高且噪声大。

Method: 引入重要性采样选择最有价值的输入样本，并提出贝叶斯方法聚合评分以减少所需评分数量。

Result: 实现了约30倍的成本降低和5倍的评分数量减少，并比较了两种视觉模型的神经元解释质量。

Conclusion: 提出的方法显著提升了众包评估的效率和准确性，为神经元解释的可靠性评估提供了实用工具。

Abstract: Interpreting individual neurons or directions in activations space is an
important component of mechanistic interpretability. As such, many algorithms
have been proposed to automatically produce neuron explanations, but it is
often not clear how reliable these explanations are, or which methods produce
the best explanations. This can be measured via crowd-sourced evaluations, but
they can often be noisy and expensive, leading to unreliable results. In this
paper, we carefully analyze the evaluation pipeline and develop a
cost-effective and highly accurate crowdsourced evaluation strategy. In
contrast to previous human studies that only rate whether the explanation
matches the most highly activating inputs, we estimate whether the explanation
describes neuron activations across all inputs. To estimate this effectively,
we introduce a novel application of importance sampling to determine which
inputs are the most valuable to show to raters, leading to around 30x cost
reduction compared to uniform sampling. We also analyze the label noise present
in crowd-sourced evaluations and propose a Bayesian method to aggregate
multiple ratings leading to a further ~5x reduction in number of ratings
required for the same accuracy. Finally, we use these methods to conduct a
large-scale study comparing the quality of neuron explanations produced by the
most popular methods for two different vision models.

</details>


### [389] [Rethinking Cross-Modal Interaction in Multimodal Diffusion Transformers](https://arxiv.org/abs/2506.07986)
*Zhengyao Lv,Tianlin Pan,Chenyang Si,Zhaoxi Chen,Wangmeng Zuo,Ziwei Liu,Kwan-Yee K. Wong*

Main category: cs.CV

TL;DR: 论文提出了一种名为TACA的方法，通过动态调整跨模态注意力，解决了MM-DiT模型中文本与图像对齐的问题。


<details>
  <summary>Details</summary>
Motivation: 现有MM-DiT模型（如FLUX）在文本驱动的视觉生成中，跨模态注意力的不平衡和缺乏时间步感知权重导致文本与生成内容对齐不精确。

Method: 提出了TACA方法，通过温度缩放和时间步依赖的调整动态平衡跨模态交互，并结合LoRA微调。

Result: 在T2I-CompBench基准测试中，TACA显著提升了文本-图像对齐效果，且计算开销小。

Conclusion: 平衡跨模态注意力对提升文本到图像扩散模型的语义保真度至关重要。

Abstract: Multimodal Diffusion Transformers (MM-DiTs) have achieved remarkable progress
in text-driven visual generation. However, even state-of-the-art MM-DiT models
like FLUX struggle with achieving precise alignment between text prompts and
generated content. We identify two key issues in the attention mechanism of
MM-DiT, namely 1) the suppression of cross-modal attention due to token
imbalance between visual and textual modalities and 2) the lack of
timestep-aware attention weighting, which hinder the alignment. To address
these issues, we propose \textbf{Temperature-Adjusted Cross-modal Attention
(TACA)}, a parameter-efficient method that dynamically rebalances multimodal
interactions through temperature scaling and timestep-dependent adjustment.
When combined with LoRA fine-tuning, TACA significantly enhances text-image
alignment on the T2I-CompBench benchmark with minimal computational overhead.
We tested TACA on state-of-the-art models like FLUX and SD3.5, demonstrating
its ability to improve image-text alignment in terms of object appearance,
attribute binding, and spatial relationships. Our findings highlight the
importance of balancing cross-modal attention in improving semantic fidelity in
text-to-image diffusion models. Our codes are publicly available at
\href{https://github.com/Vchitect/TACA}

</details>


### [390] [PairEdit: Learning Semantic Variations for Exemplar-based Image Editing](https://arxiv.org/abs/2506.07992)
*Haoguang Lu,Jiacheng Chen,Zhenguo Yang,Aurele Tohokantche Gnanha,Fu Lee Wang,Li Qing,Xudong Mao*

Main category: cs.CV

TL;DR: PairEdit是一种无需文本指导的视觉编辑方法，通过少量图像对学习复杂编辑语义。


<details>
  <summary>Details</summary>
Motivation: 现有基于示例的编辑方法依赖文本提示，而某些编辑语义难以用文本精确描述。

Method: 提出目标噪声预测和内容保持噪声调度，优化LoRAs以分离语义学习和内容。

Result: PairEdit成功学习复杂语义，显著提升内容一致性。

Conclusion: PairEdit为无文本指导的视觉编辑提供了有效解决方案。

Abstract: Recent advancements in text-guided image editing have achieved notable
success by leveraging natural language prompts for fine-grained semantic
control. However, certain editing semantics are challenging to specify
precisely using textual descriptions alone. A practical alternative involves
learning editing semantics from paired source-target examples. Existing
exemplar-based editing methods still rely on text prompts describing the change
within paired examples or learning implicit text-based editing instructions. In
this paper, we introduce PairEdit, a novel visual editing method designed to
effectively learn complex editing semantics from a limited number of image
pairs or even a single image pair, without using any textual guidance. We
propose a target noise prediction that explicitly models semantic variations
within paired images through a guidance direction term. Moreover, we introduce
a content-preserving noise schedule to facilitate more effective semantic
learning. We also propose optimizing distinct LoRAs to disentangle the learning
of semantic variations from content. Extensive qualitative and quantitative
evaluations demonstrate that PairEdit successfully learns intricate semantics
while significantly improving content consistency compared to baseline methods.
Code will be available at https://github.com/xudonmao/PairEdit.

</details>


### [391] [MADFormer: Mixed Autoregressive and Diffusion Transformers for Continuous Image Generation](https://arxiv.org/abs/2506.07999)
*Junhao Chen,Yulia Tsvetkov,Xiaochuang Han*

Main category: cs.CV

TL;DR: MADFormer是一种混合自回归（AR）和扩散（Diffusion）的Transformer模型，用于分析AR与扩散模型的权衡，通过分区生成图像并优化性能。


<details>
  <summary>Details</summary>
Motivation: 现有混合模型缺乏系统指导如何分配AR和扩散模型的能力，MADFormer旨在填补这一空白。

Method: MADFormer将图像生成分为空间块，AR层用于全局条件，扩散层用于局部细化。

Result: 实验表明，分区显著提升高分辨率图像性能，混合层在质量和效率间取得更好平衡，FID提升达75%。

Conclusion: MADFormer为未来混合生成模型提供了实用设计原则。

Abstract: Recent progress in multimodal generation has increasingly combined
autoregressive (AR) and diffusion-based approaches, leveraging their
complementary strengths: AR models capture long-range dependencies and produce
fluent, context-aware outputs, while diffusion models operate in continuous
latent spaces to refine high-fidelity visual details. However, existing hybrids
often lack systematic guidance on how and why to allocate model capacity
between these paradigms. In this work, we introduce MADFormer, a Mixed
Autoregressive and Diffusion Transformer that serves as a testbed for analyzing
AR-diffusion trade-offs. MADFormer partitions image generation into spatial
blocks, using AR layers for one-pass global conditioning across blocks and
diffusion layers for iterative local refinement within each block. Through
controlled experiments on FFHQ-1024 and ImageNet, we identify two key insights:
(1) block-wise partitioning significantly improves performance on
high-resolution images, and (2) vertically mixing AR and diffusion layers
yields better quality-efficiency balances--improving FID by up to 75% under
constrained inference compute. Our findings offer practical design principles
for future hybrid generative models.

</details>


### [392] [Aligning Text, Images, and 3D Structure Token-by-Token](https://arxiv.org/abs/2506.08002)
*Aadarsh Sahoo,Vansh Tibrewal,Georgia Gkioxari*

Main category: cs.CV

TL;DR: 论文提出了一种统一的LLM框架，用于对齐语言、图像和3D场景，并提供了关键设计选择的详细指南。模型在四项核心3D任务和四个数据集上表现优异。


<details>
  <summary>Details</summary>
Motivation: 为设计师和机器人提供3D场景理解能力，受语言和图像建模进展启发，探索自回归模型在结构化3D场景中的潜力。

Method: 提出统一的LLM框架，结合语言、图像和3D场景，优化数据表示和模态特定目标。通过量化形状编码增强3D模态，用于复杂3D物体形状重建。

Result: 在渲染、识别、指令跟随和问答四项3D任务上表现优异，并在合成和真实世界数据集中验证了模型的有效性。

Conclusion: 该框架为3D场景理解提供了有效解决方案，并在真实世界3D物体识别任务中展示了潜力。

Abstract: Creating machines capable of understanding the world in 3D is essential in
assisting designers that build and edit 3D environments and robots navigating
and interacting within a three-dimensional space. Inspired by advances in
language and image modeling, we investigate the potential of autoregressive
models for a new modality: structured 3D scenes. To this end, we propose a
unified LLM framework that aligns language, images, and 3D scenes and provide a
detailed ''cookbook'' outlining critical design choices for achieving optimal
training and performance addressing key questions related to data
representation, modality-specific objectives, and more. We evaluate performance
across four core 3D tasks -- rendering, recognition, instruction-following, and
question-answering -- and four 3D datasets, synthetic and real-world. We extend
our approach to reconstruct complex 3D object shapes by enriching our 3D
modality with quantized shape encodings, and show our model's effectiveness on
real-world 3D object recognition tasks. Project webpage:
https://glab-caltech.github.io/kyvo/

</details>


### [393] [Audio-Sync Video Generation with Multi-Stream Temporal Control](https://arxiv.org/abs/2506.08003)
*Shuchen Weng,Haojie Zheng,Zheng Chang,Si Li,Boxin Shi,Xinlong Wang*

Main category: cs.CV

TL;DR: MTV是一个用于音频同步视频生成的框架，通过分离音频为语音、效果和音乐轨道，实现精细控制。DEMIX数据集支持其训练，实验显示MTV在多个指标上表现优异。


<details>
  <summary>Details</summary>
Motivation: 音频与视觉世界紧密同步，是视频生成的自然控制信号，但现有方法在复杂音频类型下难以生成高质量视频。

Method: MTV框架分离音频为语音、效果和音乐轨道，分别控制唇部动作、事件时间和视觉氛围。DEMIX数据集支持多阶段训练。

Result: MTV在视频质量、文本-视频一致性和音频-视频对齐等六个指标上达到最优性能。

Conclusion: MTV通过音频分离和精细控制，实现了高质量且语义对齐的视频生成。

Abstract: Audio is inherently temporal and closely synchronized with the visual world,
making it a naturally aligned and expressive control signal for controllable
video generation (e.g., movies). Beyond control, directly translating audio
into video is essential for understanding and visualizing rich audio narratives
(e.g., Podcasts or historical recordings). However, existing approaches fall
short in generating high-quality videos with precise audio-visual
synchronization, especially across diverse and complex audio types. In this
work, we introduce MTV, a versatile framework for audio-sync video generation.
MTV explicitly separates audios into speech, effects, and music tracks,
enabling disentangled control over lip motion, event timing, and visual mood,
respectively -- resulting in fine-grained and semantically aligned video
generation. To support the framework, we additionally present DEMIX, a dataset
comprising high-quality cinematic videos and demixed audio tracks. DEMIX is
structured into five overlapped subsets, enabling scalable multi-stage training
for diverse generation scenarios. Extensive experiments demonstrate that MTV
achieves state-of-the-art performance across six standard metrics spanning
video quality, text-video consistency, and audio-video alignment. Project page:
https://hjzheng.net/projects/MTV/.

</details>


### [394] [Dynamic View Synthesis as an Inverse Problem](https://arxiv.org/abs/2506.08004)
*Hidir Yesiltepe,Pinar Yanardag*

Main category: cs.CV

TL;DR: 本文提出了一种无需训练的动态视图合成方法，通过改进预训练视频扩散模型的噪声初始化阶段，实现了高保真度的动态视图合成。


<details>
  <summary>Details</summary>
Motivation: 解决单目视频动态视图合成的逆问题，避免权重更新或辅助模块的需求。

Method: 引入K阶递归噪声表示解决零终端信噪比问题，并采用随机潜在调制完成遮挡区域合成。

Result: 实验表明，通过噪声初始化阶段的潜在空间结构化操作，可以有效实现动态视图合成。

Conclusion: 该方法在无需训练的情况下，通过噪声初始化的改进实现了高质量的动态视图合成。

Abstract: In this work, we address dynamic view synthesis from monocular videos as an
inverse problem in a training-free setting. By redesigning the noise
initialization phase of a pre-trained video diffusion model, we enable
high-fidelity dynamic view synthesis without any weight updates or auxiliary
modules. We begin by identifying a fundamental obstacle to deterministic
inversion arising from zero-terminal signal-to-noise ratio (SNR) schedules and
resolve it by introducing a novel noise representation, termed K-order
Recursive Noise Representation. We derive a closed form expression for this
representation, enabling precise and efficient alignment between the
VAE-encoded and the DDIM inverted latents. To synthesize newly visible regions
resulting from camera motion, we introduce Stochastic Latent Modulation, which
performs visibility aware sampling over the latent space to complete occluded
regions. Comprehensive experiments demonstrate that dynamic view synthesis can
be effectively performed through structured latent manipulation in the noise
initialization phase.

</details>


### [395] [ZeroVO: Visual Odometry with Minimal Assumptions](https://arxiv.org/abs/2506.08005)
*Lei Lai,Zekai Yin,Eshed Ohn-Bar*

Main category: cs.CV

TL;DR: ZeroVO是一种无需校准的视觉里程计算法，通过几何感知网络、语言先验和半监督训练实现跨相机和环境的零样本泛化。


<details>
  <summary>Details</summary>
Motivation: 解决现有视觉里程计方法依赖预定义或静态相机校准的问题，提升算法在多样化场景中的适用性。

Method: 结合几何感知网络、语言先验和半监督训练，实现无需校准的跨场景泛化。

Result: 在KITTI、nuScenes和Argoverse 2等基准测试中性能提升30%以上。

Conclusion: ZeroVO为视觉里程计提供了一种无需校准的通用解决方案，适用于大规模实际部署。

Abstract: We introduce ZeroVO, a novel visual odometry (VO) algorithm that achieves
zero-shot generalization across diverse cameras and environments, overcoming
limitations in existing methods that depend on predefined or static camera
calibration setups. Our approach incorporates three main innovations. First, we
design a calibration-free, geometry-aware network structure capable of handling
noise in estimated depth and camera parameters. Second, we introduce a
language-based prior that infuses semantic information to enhance robust
feature extraction and generalization to previously unseen domains. Third, we
develop a flexible, semi-supervised training paradigm that iteratively adapts
to new scenes using unlabeled data, further boosting the models' ability to
generalize across diverse real-world scenarios. We analyze complex autonomous
driving contexts, demonstrating over 30% improvement against prior methods on
three standard benchmarks, KITTI, nuScenes, and Argoverse 2, as well as a newly
introduced, high-fidelity synthetic dataset derived from Grand Theft Auto
(GTA). By not requiring fine-tuning or camera calibration, our work broadens
the applicability of VO, providing a versatile solution for real-world
deployment at scale.

</details>


### [396] [Dreamland: Controllable World Creation with Simulator and Generative Models](https://arxiv.org/abs/2506.08006)
*Sicheng Mo,Ziyang Leng,Leon Liu,Weizhen Wang,Honglin He,Bolei Zhou*

Main category: cs.CV

TL;DR: Dreamland提出了一种结合物理模拟器和生成模型的混合世界生成框架，通过分层抽象增强可控性，并支持现有生成模型的即插即用。


<details>
  <summary>Details</summary>
Motivation: 现有大规模视频生成模型缺乏元素级可控性，限制了其在场景编辑和AI代理训练中的应用。

Method: 设计了分层世界抽象，编码像素级和对象级语义与几何信息，作为模拟器与生成模型间的中间表示。

Result: 实验显示，Dreamland在图像质量上提升50.8%，可控性增强17.9%，并显著提升AI代理训练效果。

Conclusion: Dreamland通过结合模拟器和生成模型，显著提升了可控性和实用性，为动态世界生成提供了新思路。

Abstract: Large-scale video generative models can synthesize diverse and realistic
visual content for dynamic world creation, but they often lack element-wise
controllability, hindering their use in editing scenes and training embodied AI
agents. We propose Dreamland, a hybrid world generation framework combining the
granular control of a physics-based simulator and the photorealistic content
output of large-scale pretrained generative models. In particular, we design a
layered world abstraction that encodes both pixel-level and object-level
semantics and geometry as an intermediate representation to bridge the
simulator and the generative model. This approach enhances controllability,
minimizes adaptation cost through early alignment with real-world
distributions, and supports off-the-shelf use of existing and future pretrained
generative models. We further construct a D3Sim dataset to facilitate the
training and evaluation of hybrid generation pipelines. Experiments demonstrate
that Dreamland outperforms existing baselines with 50.8% improved image
quality, 17.9% stronger controllability, and has great potential to enhance
embodied agent training. Code and data will be made available.

</details>


### [397] [Hidden in plain sight: VLMs overlook their visual representations](https://arxiv.org/abs/2506.08008)
*Stephanie Fu,Tyler Bonnen,Devin Guillory,Trevor Darrell*

Main category: cs.CV

TL;DR: 论文比较了视觉语言模型（VLMs）与其视觉编码器的性能，发现VLMs在视觉任务中表现显著较差，接近随机水平。瓶颈在于VLMs未能有效利用视觉信息，反而继承了语言模型的先验。


<details>
  <summary>Details</summary>
Motivation: 探索视觉语言模型（VLMs）如何整合视觉与语言信息，并评估其在视觉任务中的表现。

Method: 通过一系列视觉基准测试（如深度估计、对应关系）比较VLMs与其视觉编码器的性能，并分析失败原因。

Result: VLMs在视觉任务中表现显著低于视觉编码器，接近随机水平，主要原因是未能有效利用视觉信息。

Conclusion: VLMs在视觉任务中的瓶颈在于未能有效整合视觉信息，未来研究需改进视觉理解能力。

Abstract: Language provides a natural interface to specify and evaluate performance on
visual tasks. To realize this possibility, vision language models (VLMs) must
successfully integrate visual and linguistic information. Our work compares
VLMs to a direct readout of their visual encoders to understand their ability
to integrate across these modalities. Across a series of vision-centric
benchmarks (e.g., depth estimation, correspondence), we find that VLMs perform
substantially worse than their visual encoders, dropping to near-chance
performance. We investigate these results through a series of analyses across
the entire VLM: namely 1) the degradation of vision representations, 2)
brittleness to task prompt, and 3) the language model's role in solving the
task. We find that the bottleneck in performing these vision-centric tasks lies
in this third category; VLMs are not effectively using visual information
easily accessible throughout the entire model, and they inherit the language
priors present in the LLM. Our work helps diagnose the failure modes of
open-source VLMs, and presents a series of evaluations useful for future
investigations into visual understanding within VLMs.

</details>


### [398] [Self Forcing: Bridging the Train-Test Gap in Autoregressive Video Diffusion](https://arxiv.org/abs/2506.08009)
*Xun Huang,Zhengqi Li,Guande He,Mingyuan Zhou,Eli Shechtman*

Main category: cs.CV

TL;DR: Self Forcing是一种新的自回归视频扩散模型训练范式，通过自生成输出和KV缓存解决曝光偏差问题，实现高效实时视频生成。


<details>
  <summary>Details</summary>
Motivation: 解决传统方法在推理时因依赖自身不完美输出而导致的曝光偏差问题。

Method: 采用自回归展开和KV缓存策略，结合多步扩散模型和梯度截断，优化训练效率和性能。

Result: 在单GPU上实现亚秒级延迟的实时视频生成，质量优于或匹配非因果扩散模型。

Conclusion: Self Forcing为高效高质量视频生成提供了新思路，具有实际应用潜力。

Abstract: We introduce Self Forcing, a novel training paradigm for autoregressive video
diffusion models. It addresses the longstanding issue of exposure bias, where
models trained on ground-truth context must generate sequences conditioned on
their own imperfect outputs during inference. Unlike prior methods that denoise
future frames based on ground-truth context frames, Self Forcing conditions
each frame's generation on previously self-generated outputs by performing
autoregressive rollout with key-value (KV) caching during training. This
strategy enables supervision through a holistic loss at the video level that
directly evaluates the quality of the entire generated sequence, rather than
relying solely on traditional frame-wise objectives. To ensure training
efficiency, we employ a few-step diffusion model along with a stochastic
gradient truncation strategy, effectively balancing computational cost and
performance. We further introduce a rolling KV cache mechanism that enables
efficient autoregressive video extrapolation. Extensive experiments demonstrate
that our approach achieves real-time streaming video generation with sub-second
latency on a single GPU, while matching or even surpassing the generation
quality of significantly slower and non-causal diffusion models. Project
website: http://self-forcing.github.io/

</details>


### [399] [Vision Transformers Don't Need Trained Registers](https://arxiv.org/abs/2506.08010)
*Nick Jiang,Amil Dravid,Alexei Efros,Yossi Gandelsman*

Main category: cs.CV

TL;DR: 论文研究了Vision Transformers中高范数令牌的机制，提出了一种无需重新训练的方法来缓解这一问题，通过调整激活分布改善注意力图和性能。


<details>
  <summary>Details</summary>
Motivation: 探索Vision Transformers中高范数令牌导致噪声注意力图的机制，并寻找无需重新训练的解决方案。

Method: 通过将高范数激活从特定神经元转移到未训练的令牌中，模拟注册令牌的效果。

Result: 方法生成了更清晰的注意力图和特征图，提升了多个下游视觉任务的性能，效果接近显式训练注册令牌的模型。

Conclusion: 测试时注册令牌为预训练模型提供了无需训练的解决方案，提升了模型的可解释性和性能。

Abstract: We investigate the mechanism underlying a previously identified phenomenon in
Vision Transformers -- the emergence of high-norm tokens that lead to noisy
attention maps. We observe that in multiple models (e.g., CLIP, DINOv2), a
sparse set of neurons is responsible for concentrating high-norm activations on
outlier tokens, leading to irregular attention patterns and degrading
downstream visual processing. While the existing solution for removing these
outliers involves retraining models from scratch with additional learned
register tokens, we use our findings to create a training-free approach to
mitigate these artifacts. By shifting the high-norm activations from our
discovered register neurons into an additional untrained token, we can mimic
the effect of register tokens on a model already trained without registers. We
demonstrate that our method produces cleaner attention and feature maps,
enhances performance over base models across multiple downstream visual tasks,
and achieves results comparable to models explicitly trained with register
tokens. We then extend test-time registers to off-the-shelf vision-language
models to improve their interpretability. Our results suggest that test-time
registers effectively take on the role of register tokens at test-time,
offering a training-free solution for any pre-trained model released without
them.

</details>


### [400] [Play to Generalize: Learning to Reason Through Game Play](https://arxiv.org/abs/2506.08011)
*Yunfei Xie,Yinsong Ma,Shiyi Lan,Alan Yuille,Junfei Xiao,Chen Wei*

Main category: cs.CV

TL;DR: ViGaL通过强化学习训练MLLM玩街机游戏，提升其在多模态数学和多学科问题上的推理能力，且不依赖具体解题示例。


<details>
  <summary>Details</summary>
Motivation: 受认知科学启发，游戏能促进可迁移的认知技能，因此提出通过游戏训练MLLM以增强其多模态推理能力。

Method: 使用强化学习对7B参数的MLLM进行后训练，通过玩简单街机游戏（如Snake）提升推理能力。

Result: 模型在多模态数学和多学科任务上表现优于专用模型，同时保持基础模型在通用视觉任务上的性能。

Conclusion: 规则化游戏可作为可控且可扩展的预训练任务，解锁MLLM的通用多模态推理能力。

Abstract: Developing generalizable reasoning capabilities in multimodal large language
models (MLLMs) remains challenging. Motivated by cognitive science literature
suggesting that gameplay promotes transferable cognitive skills, we propose a
novel post-training paradigm, Visual Game Learning, or ViGaL, where MLLMs
develop out-of-domain generalization of multimodal reasoning through playing
arcade-like games. Specifically, we show that post-training a 7B-parameter MLLM
via reinforcement learning (RL) on simple arcade-like games, e.g. Snake,
significantly enhances its downstream performance on multimodal math benchmarks
like MathVista, and on multi-discipline questions like MMMU, without seeing any
worked solutions, equations, or diagrams during RL, suggesting the capture of
transferable reasoning skills. Remarkably, our model outperforms specialist
models tuned on multimodal reasoning data in multimodal reasoning benchmarks,
while preserving the base model's performance on general visual benchmarks, a
challenge where specialist models often fall short. Our findings suggest a new
post-training paradigm: synthetic, rule-based games can serve as controllable
and scalable pre-text tasks that unlock generalizable multimodal reasoning
abilities in MLLMs.

</details>


### [401] [StableMTL: Repurposing Latent Diffusion Models for Multi-Task Learning from Partially Annotated Synthetic Datasets](https://arxiv.org/abs/2506.08013)
*Anh-Quan Cao,Ivan Lopes,Raoul de Charette*

Main category: cs.CV

TL;DR: StableMTL利用扩散模型在零样本设置下进行多任务学习，通过统一潜在损失和任务注意力机制，显著提升了多任务密集预测的性能。


<details>
  <summary>Details</summary>
Motivation: 多任务密集预测需要大量标注数据，而部分标注学习限制了其扩展性。本文旨在利用扩散模型的泛化能力，实现零样本多任务学习。

Method: 提出StableMTL方法，利用图像生成器进行潜在回归，采用去噪框架、任务编码和任务注意力机制，统一潜在损失以简化训练。

Result: 在8个基准测试的7个任务上，StableMTL优于基线方法。

Conclusion: StableMTL通过任务注意力机制和统一潜在损失，实现了高效的多任务学习，扩展性强且性能优越。

Abstract: Multi-task learning for dense prediction is limited by the need for extensive
annotation for every task, though recent works have explored training with
partial task labels. Leveraging the generalization power of diffusion models,
we extend the partial learning setup to a zero-shot setting, training a
multi-task model on multiple synthetic datasets, each labeled for only a subset
of tasks. Our method, StableMTL, repurposes image generators for latent
regression. Adapting a denoising framework with task encoding, per-task
conditioning and a tailored training scheme. Instead of per-task losses
requiring careful balancing, a unified latent loss is adopted, enabling
seamless scaling to more tasks. To encourage inter-task synergy, we introduce a
multi-stream model with a task-attention mechanism that converts N-to-N task
interactions into efficient 1-to-N attention, promoting effective cross-task
sharing. StableMTL outperforms baselines on 7 tasks across 8 benchmarks.

</details>


### [402] [4DGT: Learning a 4D Gaussian Transformer Using Real-World Monocular Videos](https://arxiv.org/abs/2506.08015)
*Zhen Xu,Zhengqin Li,Zhao Dong,Xiaowei Zhou,Richard Newcombe,Zhaoyang Lv*

Main category: cs.CV

TL;DR: 4DGT是一种基于4D高斯和Transformer的动态场景重建模型，通过单目视频训练，统一静态和动态组件，支持高效渲染。


<details>
  <summary>Details</summary>
Motivation: 动态场景重建需要处理复杂的时间变化环境，传统优化方法耗时且难以扩展。

Method: 使用4D高斯作为归纳偏置，提出密度控制策略，滚动窗口处理64帧，实现高效前馈推理。

Result: 在真实视频中显著优于其他高斯网络，跨域视频中与优化方法精度相当，重建时间从小时级降至秒级。

Conclusion: 4DGT通过高效前馈推理和密度控制，实现了动态场景的高效重建，适用于长视频序列。

Abstract: We propose 4DGT, a 4D Gaussian-based Transformer model for dynamic scene
reconstruction, trained entirely on real-world monocular posed videos. Using 4D
Gaussian as an inductive bias, 4DGT unifies static and dynamic components,
enabling the modeling of complex, time-varying environments with varying object
lifespans. We proposed a novel density control strategy in training, which
enables our 4DGT to handle longer space-time input and remain efficient
rendering at runtime. Our model processes 64 consecutive posed frames in a
rolling-window fashion, predicting consistent 4D Gaussians in the scene. Unlike
optimization-based methods, 4DGT performs purely feed-forward inference,
reducing reconstruction time from hours to seconds and scaling effectively to
long video sequences. Trained only on large-scale monocular posed video
datasets, 4DGT can outperform prior Gaussian-based networks significantly in
real-world videos and achieve on-par accuracy with optimization-based methods
on cross-domain videos. Project page: https://4dgt.github.io

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [403] [Fake Friends and Sponsored Ads: The Risks of Advertising in Conversational Search](https://arxiv.org/abs/2506.06447)
*Jacob Erickson*

Main category: cs.HC

TL;DR: 论文探讨了对话搜索中广告的未来，分析了其对用户体验的潜在风险，并提出了“假朋友困境”概念。


<details>
  <summary>Details</summary>
Motivation: 研究广告在对话搜索（如ChatGPT）中的影响，尤其是对敏感话题的误导和用户信任的滥用。

Method: 通过推测性案例和概念分析，探讨广告形式和用户风险。

Result: 揭示了广告可能降低搜索质量、滥用用户信任，并提出“假朋友困境”。

Conclusion: 呼吁关注对话搜索中广告的潜在危害，并提出行动建议。

Abstract: Digital commerce thrives on advertising, with many of the largest technology
companies relying on it as a significant source of revenue. However, in the
context of information-seeking behavior, such as search, advertising may
degrade the user experience by lowering search quality, misusing user data for
inappropriate personalization, potentially misleading individuals, or even
leading them toward harm. These challenges remain significant as conversational
search technologies, such as ChatGPT, become widespread. This paper critically
examines the future of advertising in conversational search, utilizing several
speculative examples to illustrate the potential risks posed to users who seek
guidance on sensitive topics. Additionally, it provides an overview of the
forms that advertising might take in this space and introduces the "fake friend
dilemma," the idea that a conversational agent may exploit unaligned user trust
to achieve other objectives. This study presents a provocative discussion on
the future of online advertising in the space of conversational search and ends
with a call to action.

</details>


### [404] [RadioGami: Batteryless, Long-Range Wireless Paper Sensors Using Tunnel Diodes](https://arxiv.org/abs/2506.06473)
*Imran Fahad,Danny Scott,Azizul Zahid,Matthew Bringle,Srinayana Patil,Ella Bevins,Carmen Palileo,Sai Swaminathan*

Main category: cs.HC

TL;DR: RadioGami是一种低成本、无电池的射频传感方法，利用铜带、纸张和现成电子设备实现长距离（45.73米）的纸张变形（如弯曲、撕裂、折纸）传感。


<details>
  <summary>Details</summary>
Motivation: 解决传统纸基射频设备操作距离短的问题，探索可持续、无电池的交互界面。

Method: 采用超低功耗（35uW）开关电路和隧道二极管，结合光二极管能量收集技术，实现无线功能。

Result: 展示了在45.73米范围内监测物体状态、用户交互和环境变化的应用，并分析了性能、灵敏度和功耗。

Conclusion: RadioGami为可持续、无电池的交互界面提供了创新解决方案。

Abstract: Paper-based interactive RF devices have opened new possibilities for wireless
sensing, yet they are typically constrained by short operational ranges. This
paper introduces RadioGami, a method for creating long-range, batteryless RF
sensing surfaces on paper using low-cost, DIY materials like copper tape,
paper, and off-the-shelf electronics paired with an affordable radio receiver
(approx. $20). We explore the design space enabled by RadioGami, including
sensing paper deformations like bending, tearing, and origami patterns (Miura,
Kresling) at ranges up to 45.73 meters. RadioGami employs a novel ultra-low
power (35uW) switching circuit with a tunnel diode for wireless functionality.
These surfaces can sustainably operate by harvesting energy using tiny
photodiodes. We demonstrate applications that monitor object status, track user
interactions (rotation, sliding), and detect environmental changes. We
characterize performance, sensitivity, range, and power consumption with
deployment studies. RadioGami advances sustainable, tangible, and batteryless
interfaces for embodied interaction.

</details>


### [405] [Mind Games! Exploring the Impact of Dark Patterns in Mixed Reality Scenarios](https://arxiv.org/abs/2506.06774)
*Luca-Maxim Meinhardt,Simon Demharter,Michael Rietzler,Mark Colley,Thomas Eßmeyer,Enrico Rukzio*

Main category: cs.HC

TL;DR: 研究探讨了混合现实（MR）中四种黑暗模式对用户的影响，发现这些模式显著降低舒适度、增加抗拒感，并减少使用意愿，尤其是涉及个人或金钱操纵时。


<details>
  <summary>Details</summary>
Motivation: MR技术虽具潜力，但黑暗模式可能被滥用，需研究其对用户的影响。

Method: 采用双因素被试内设计，74名参与者观看13个模拟MR城市漫步视频，分析黑暗模式的效果。

Result: 所有黑暗模式均显著降低用户舒适度，增加抗拒感，减少使用意愿；情感和感官操纵与隐藏信息效果相似。

Conclusion: 需重新评估黑暗模式分类，制定伦理设计指南和工具，防止其在沉浸技术中的滥用。

Abstract: Mixed Reality (MR) integrates virtual objects with the real world, offering
potential but raising concerns about misuse through dark patterns. This study
explored the effects of four dark patterns, adapted from prior research, and
applied to MR across three targets: places, products, and people. In a
two-factorial within-subject study with 74 participants, we analyzed 13 videos
simulating MR experiences during a city walk. Results show that all dark
patterns significantly reduced user comfort, increased reactance, and decreased
the intention to use MR glasses, with the most disruptive effects linked to
personal or monetary manipulation. Additionally, the dark patterns of Emotional
and Sensory Manipulation and Hiding Information produced similar impacts on the
user in MR, suggesting a re-evaluation of current classifications to go beyond
deceptive design techniques. Our findings highlight the importance of
developing ethical design guidelines and tools to detect and prevent dark
patterns as immersive technologies continue to evolve.

</details>


### [406] [Identity Deepfake Threats to Biometric Authentication Systems: Public and Expert Perspectives](https://arxiv.org/abs/2506.06825)
*Shijing He,Yaxiong Lei,Zihan Zhang,Yuzhou Sun,Shujun Li,Chi Zhang,Juan Ye*

Main category: cs.HC

TL;DR: 论文研究了生成式AI（Gen-AI）深度伪造对生物认证的威胁，通过混合方法揭示了公众与专家认知的差距，并提出了基于实证的三层缓解框架。


<details>
  <summary>Details</summary>
Motivation: 生成式AI深度伪造对生物认证构成快速演变的威胁，但公众与专家的认知存在显著差距，导致系统脆弱性。

Method: 采用混合方法，调查408名专业人士并访谈37名参与者（25名专家，12名非专家），引入Deepfake Kill Chain模型分析威胁。

Result: 发现公众依赖生物认证的便利性，而专家担忧静态模态（如人脸和语音识别）的伪造问题，不同行业和人群的认知差异显著。

Conclusion: 提出三层缓解框架，包括动态生物信号、隐私保护数据治理和针对性教育，为防御AI生成的身份威胁提供实证指导。

Abstract: Generative AI (Gen-AI) deepfakes pose a rapidly evolving threat to biometric
authentication, yet a significant gap exists between expert understanding of
these risks and public perception. This disconnection creates critical
vulnerabilities in systems trusted by millions. To bridge this gap, we
conducted a comprehensive mixed-method study, surveying 408 professionals
across key sectors and conducting in-depth interviews with 37 participants (25
experts, 12 general public [non-experts]). Our findings reveal a paradox: while
the public increasingly relies on biometrics for convenience, experts express
grave concerns about the spoofing of static modalities like face and voice
recognition. We found significant demographic and sector-specific divides in
awareness and trust, with finance professionals, for example, showing
heightened skepticism. To systematically analyze these threats, we introduce a
novel Deepfake Kill Chain model, adapted from Hutchins et al.'s cybersecurity
frameworks to map the specific attack vectors used by malicious actors against
biometric systems. Based on this model and our empirical findings, we propose a
tri-layer mitigation framework that prioritizes dynamic biometric signals
(e.g., eye movements), robust privacy-preserving data governance, and targeted
educational initiatives. This work provides the first empirically grounded
roadmap for defending against AI-generated identity threats by aligning
technical safeguards with human-centered insights.

</details>


### [407] [In-Sensor Motion Recognition with Memristive System and Light Sensing Surfaces](https://arxiv.org/abs/2506.06829)
*Hritom Das,Imran Fahad,SNB Tushar,Sk Hasibul Alam,Graham Buchanan,Danny Scott,Garrett S. Rose,Sai Swaminathan*

Main category: cs.HC

TL;DR: 提出了一种新型设备架构，结合忆阻器和光感表面，用于边缘设备的高效能运动识别。系统通过光感表面采集运动数据，利用忆阻系统和WTA电路进行低功耗分类。实验验证了高效能和准确性。


<details>
  <summary>Details</summary>
Motivation: 解决边缘设备中运动识别的高能耗问题，同时提升数据隐私和可持续性。

Method: 结合光感表面和忆阻系统，采用HfO2基突触设备和WTA电路进行低功耗分类。

Result: 系统平均能耗极低（4.17 nJ和0.952 nJ），分类准确率达97.22%，抗噪能力强。

Conclusion: 该架构能效高，支持可持续能源集成，并增强数据隐私。

Abstract: In this paper, we introduce a novel device architecture that merges
memristive devices with light-sensing surfaces, for energy-efficient motion
recognition at the edge. Our light-sensing surface captures motion data through
in-sensor computation. This data is then processed using a memristive system
equipped with a HfO2-based synaptic device, coupled with a winner-take-all
(WTA) circuit, tailored for low-power motion classification tasks. We validate
our end-to-end system using four distinct human hand gestures - left-to-right,
right-to-left, bottom-to-top, and top-to-bottom movements - to assess energy
efficiency and classification robustness. Our experiments show that the system
requires an average of only 4.17 nJ for taking our processed analog signal and
mapping weights onto our memristive system and 0.952 nJ for testing per
movement class, achieving 97.22% accuracy even under 5% noise interference. A
key advantage of our proposed architecture is its low energy requirement,
enabling the integration of energy-harvesting solutions such as solar power for
sustainable autonomous operation. Additionally, our approach enhances data
privacy by processing data locally, reducing the need for external data
transmission and storage.

</details>


### [408] [LLM-D12: A Dual-Dimensional Scale of Instrumental and Relational Dependencies on Large Language Models](https://arxiv.org/abs/2506.06874)
*Ala Yankouskaya,Areej B. Babiker,Syeda W. F. Rizvi,Sameha Alshakhsi,Magnus Liebherr,Raian Ali*

Main category: cs.HC

TL;DR: 研究者开发并验证了一个名为LLM-D12的12项问卷，用于测量人们对大型语言模型（LLM）的依赖性，分为工具性依赖和关系性依赖两个维度。


<details>
  <summary>Details</summary>
Motivation: 当前缺乏专门评估LLM依赖性的工具，现有工具多基于行为成瘾症状，未能充分反映LLM与人类关系的复杂性。

Method: 基于理论框架开发12项问卷，收集526名英国参与者的数据，通过探索性和验证性因子分析验证两因素结构。

Result: 问卷显示良好的内部一致性和区分效度，工具性依赖和关系性依赖两个维度得到验证。

Conclusion: LLM-D12为评估LLM依赖性提供了新工具，依赖不一定是功能失调，但在某些情境下可能成为问题。

Abstract: There is growing interest in understanding how people interact with large
language models (LLMs) and whether such models elicit dependency or even
addictive behaviour. Validated tools to assess the extent to which individuals
may become dependent on LLMs are scarce and primarily build on classic
behavioral addiction symptoms, adapted to the context of LLM use. We view this
as a conceptual limitation, as the LLM-human relationship is more nuanced and
warrants a fresh and distinct perspective. To address this gap, we developed
and validated a new 12-item questionnaire to measure LLM dependency, referred
to as LLM-D12. The scale was based on the authors' prior theoretical work, with
items developed accordingly and responses collected from 526 participants in
the UK. Exploratory and confirmatory factor analyses, performed on separate
halves of the total sample using a split-sample approach, supported a
two-factor structure: Instrumental Dependency (six items) and Relationship
Dependency (six items). Instrumental Dependency reflects the extent to which
individuals rely on LLMs to support or collaborate in decision-making and
cognitive tasks. Relationship Dependency captures the tendency to perceive LLMs
as socially meaningful, sentient, or companion-like entities. The two-factor
structure demonstrated excellent internal consistency and clear discriminant
validity. External validation confirmed both the conceptual foundation and the
distinction between the two subscales. The psychometric properties and
structure of our LLM-D12 scale were interpreted in light of the emerging view
that dependency on LLMs does not necessarily indicate dysfunction but may still
reflect reliance levels that could become problematic in certain contexts.

</details>


### [409] [From Inquisitorial to Adversarial: Using Legal Theory to Redesign Online Reporting Systems](https://arxiv.org/abs/2506.07041)
*Leijie Wang,Weizi Wu,Lirong Que,Nirvan Tyagi,Amy X. Zhang*

Main category: cs.HC

TL;DR: 论文探讨了在线举报系统的设计问题，提出采用对抗性模型以增强用户控制权，同时通过文献综述、访谈和威胁建模探索了设计空间。


<details>
  <summary>Details</summary>
Motivation: 当前在线举报系统的调查性质导致用户缺乏参与感和透明度，而对抗性模型可能提升程序正义和隐私保护。

Method: 通过文献综述、形成性访谈和威胁建模，探索了对抗性模型在举报系统中的设计可能性。

Result: 提出了支持用户收集和呈现证据的设计方案，同时减少信息共享并支持证据认证。

Conclusion: 研究发现可为新型加密工具和在线内容审核的法律框架应用提供参考。

Abstract: User reporting systems are central to addressing interpersonal conflicts and
protecting users from harm in online spaces, particularly those with heightened
privacy expectations. However, users often express frustration at their lack of
insight and input into the reporting process. Drawing on offline legal
literature, we trace these frustrations to the inquisitorial nature of today's
online reporting systems, where moderators lead evidence gathering and case
development. In contrast, adversarial models can grant users greater control
and thus are better for procedural justice and privacy protection, despite
their increased risks of system abuse. This motivates us to explore the
potential of incorporating adversarial practices into online reporting systems.
Through literature review, formative interviews, and threat modeling, we find a
rich design space for empowering users to collect and present their evidence
while mitigating potential abuse in the reporting process. In particular, we
propose designs that minimize the amount of information shared for reporting
purposes, as well as supporting evidence authentication. Finally, we discuss
how our findings can inform new cryptographic tools and new efforts to apply
comparative legal frameworks to online moderation.

</details>


### [410] [earEOG via Periauricular Electrodes to Facilitate Eye Tracking in a Natural Headphone Form Factor](https://arxiv.org/abs/2506.07193)
*Tobias King,Michael Knierim,Philipp Lepold,Christopher Clarke,Hans Gellersen,Michael Beigl,Tobias Röddiger*

Main category: cs.HC

TL;DR: 研究提出了一种基于耳部电极的EOG眼动追踪方法，解决了传统眼动追踪设备的笨重和复杂问题。实验表明，水平方向追踪效果良好，但垂直方向效果较差。


<details>
  <summary>Details</summary>
Motivation: 传统眼动追踪设备笨重且复杂，限制了其广泛应用。研究旨在通过耳部电极的EOG技术提供更轻便的解决方案。

Method: 使用14个耳部电极集成到耳机设备中，通过实验记录16名参与者的平滑追踪和扫视眼动数据，并与标准EOG和摄像头方法对比。

Result: 水平方向追踪与标准方法高度相关（r=0.81），垂直方向相关性较弱（r=0.28）。扫视眼动在水平方向表现优异（r=0.99），垂直方向较差。

Conclusion: 耳部EOG在水平眼动追踪中表现良好，具有潜力，但垂直方向效果不佳，需进一步优化。

Abstract: Eye tracking technology is frequently utilized to diagnose eye and
neurological disorders, assess sleep and fatigue, study human visual
perception, and enable novel gaze-based interaction methods. However,
traditional eye tracking methodologies are constrained by bespoke hardware that
is often cumbersome to wear, complex to apply, and demands substantial
computational resources. To overcome these limitations, we investigated
Electrooculography (EOG) eye tracking using 14 electrodes positioned around the
ears, integrated into a custom-built headphone form factor device. In a
controlled experiment, 16 participants tracked stimuli designed to induce
smooth pursuits and saccades. Data analysis identified optimal electrode pairs
for vertical and horizontal eye movement tracking, benchmarked against
gold-standard EOG and camera-based methods. The electrode montage nearest the
eyes yielded the best horizontal results. Horizontal smooth pursuits via earEOG
showed high correlation with gold-standard measures ($r_{\mathrm{EOG}} = 0.81,
p = 0.01$; $r_{\mathrm{CAM}} = 0.56, p = 0.02$), while vertical pursuits were
weakly correlated ($r_{\mathrm{EOG}} = 0.28, p = 0.04$; $r_{\mathrm{CAM}} =
0.35, p = 0.05$). Voltage deflections when performing saccades showed strong
correlation in the horizontal direction ($r_{\mathrm{left}} = 0.99, p = 0.0$;
$r_{\mathrm{right}} = 0.99, p = 0.0$) but low correlation in the vertical
direction ($r_{\mathrm{up}} = 0.6, p = 0.23$; $r_{\mathrm{down}} = 0.19, p =
0.73$). Overall, horizontal earEOG demonstrated strong performance, indicating
its potential effectiveness, while vertical earEOG results were poor,
suggesting limited feasibility in our current setup.

</details>


### [411] [Sword and Shield: Uses and Strategies of LLMs in Navigating Disinformation](https://arxiv.org/abs/2506.07211)
*Gionnieve Lim,Bryan Chen Zhengyu Tan,Kellie Yu Hui Sim,Weiyan Shi,Ming Hui Chew,Ming Shan Hee,Roy Ka-Wei Lee,Simon T. Perrault,Kenny Tsu Wei Choo*

Main category: cs.HC

TL;DR: 论文探讨了大型语言模型（LLMs）在虚假信息传播与检测中的双重作用，通过模拟在线论坛的游戏实验，分析了不同角色如何利用LLMs，并提出了平衡发展的建议。


<details>
  <summary>Details</summary>
Motivation: 研究LLMs在虚假信息传播中的潜在威胁及其在检测与缓解中的积极作用，以理解其动态关系。

Method: 采用受狼人杀启发的通信游戏，模拟在线论坛环境，25名参与者分别扮演虚假信息传播者、版主和用户，分析LLMs的使用策略。

Result: 研究发现LLMs的使用因角色和策略而异，既可能被滥用，也能用于对抗虚假信息。

Conclusion: 建议未来LLM开发和平台设计需平衡用户赋权与风险控制，以应对LLM辅助的虚假信息挑战。

Abstract: The emergence of Large Language Models (LLMs) presents a dual challenge in
the fight against disinformation. These powerful tools, capable of generating
human-like text at scale, can be weaponised to produce sophisticated and
persuasive disinformation, yet they also hold promise for enhancing detection
and mitigation strategies. This paper investigates the complex dynamics between
LLMs and disinformation through a communication game that simulates online
forums, inspired by the game Werewolf, with 25 participants. We analyse how
Disinformers, Moderators, and Users leverage LLMs to advance their goals,
revealing both the potential for misuse and combating disinformation. Our
findings highlight the varying uses of LLMs depending on the participants'
roles and strategies, underscoring the importance of understanding their
effectiveness in this context. We conclude by discussing implications for
future LLM development and online platform design, advocating for a balanced
approach that empowers users and fosters trust while mitigating the risks of
LLM-assisted disinformation.

</details>


### [412] [IDEIA: A Generative AI-Based System for Real-Time Editorial Ideation in Digital Journalism](https://arxiv.org/abs/2506.07278)
*Victor B. Santos,Cauã O. Jordão,Leonardo J. O. Ibiapina,Gabriel M. Silva,Mirella E. B. Santana,Matheus A. Garrido,Lucas R. C. Farias*

Main category: cs.HC

TL;DR: IDEIA是一个基于生成式AI的系统，通过实时趋势分析和自动化内容建议优化新闻创意过程，显著减少编辑规划时间和认知负担。


<details>
  <summary>Details</summary>
Motivation: 旨在通过智能自动化提升新闻编辑的生产力，同时保持编辑质量，并探讨生成模型在新闻工作流程中的技术和伦理影响。

Method: 结合Google Trends API进行数据驱动的主题监控，利用Google Gemini API生成上下文感知的标题和摘要，采用基于Node.js、React和PostgreSQL的模块化架构，支持Docker容器化和CI/CD流程。

Result: 实证结果显示，内容创意阶段的时间节省高达70%，显著提升了编辑效率。

Conclusion: IDEIA展示了智能自动化在新闻领域的潜力，并讨论了其跨行业应用的扩展性和未来适用性。

Abstract: This paper presents IDEIA (Intelligent Engine for Editorial Ideation and
Assistance), a generative AI-powered system designed to optimize the
journalistic ideation process by combining real-time trend analysis with
automated content suggestion. Developed in collaboration with the Sistema
Jornal do Commercio de Comunica\c{c}\~ao (SJCC), the largest media conglomerate
in Brazil's North and Northeast regions, IDEIA integrates the Google Trends API
for data-driven topic monitoring and the Google Gemini API for the generation
of context-aware headlines and summaries. The system adopts a modular
architecture based on Node.js, React, and PostgreSQL, supported by Docker
containerization and a CI/CD pipeline using GitHub Actions and Vercel.
Empirical results demonstrate a significant reduction in the time and cognitive
effort required for editorial planning, with reported gains of up to 70\% in
the content ideation stage. This work contributes to the field of computational
journalism by showcasing how intelligent automation can enhance productivity
while maintaining editorial quality. It also discusses the technical and
ethical implications of incorporating generative models into newsroom
workflows, highlighting scalability and future applicability across sectors
beyond journalism.

</details>


### [413] [Secondary Stakeholders in AI: Fighting for, Brokering, and Navigating Agency](https://arxiv.org/abs/2506.07281)
*Leah Hope Ajmani,Nuredin Ali Abdelkadir,Stevie Chancellor*

Main category: cs.HC

TL;DR: 论文探讨了如何将参与式AI扩展到次要利益相关者，提出了三个参与理想（知情、同意、能动性），并通过半结构化访谈研究了次要利益相关者如何实现这些理想。


<details>
  <summary>Details</summary>
Motivation: 随着AI技术更面向人类，参与式AI的呼声高涨，但现有研究多关注主要利益相关者（如终端用户），而忽略了次要利益相关者。本文旨在填补这一空白。

Method: 通过半结构化访谈，研究次要利益相关者如何实现参与式AI的三个理想（知情、同意、能动性），并提出了三种利益相关者原型。

Result: 研究发现，次要利益相关者在实现参与理想时需要克服系统性障碍，并提出了三种原型：不情愿的数据贡献者、无支持的活动家和善意的实践者。

Conclusion: 本文展望了一个次要利益相关者能够有意义地参与AI系统的未来，强调了知情、同意和能动性的重要性。

Abstract: As AI technologies become more human-facing, there have been numerous calls
to adapt participatory approaches to AI development -- spurring the idea of
participatory AI. However, these calls often focus only on primary
stakeholders, such as end-users, and not secondary stakeholders. This paper
seeks to translate the ideals of participatory AI to a broader population of
secondary AI stakeholders through semi-structured interviews. We theorize that
meaningful participation involves three participatory ideals: (1) informedness,
(2) consent, and (3) agency. We also explore how secondary stakeholders realize
these ideals by traversing a complicated problem space. Like walking up the
rungs of a ladder, these ideals build on one another. We introduce three
stakeholder archetypes: the reluctant data contributor, the unsupported
activist, and the well-intentioned practitioner, who must navigate systemic
barriers to achieving agentic AI relationships. We envision an AI future where
secondary stakeholders are able to meaningfully participate with the AI systems
they influence and are influenced by.

</details>


### [414] [Human Side of Smart Contract Fuzzing: An Empirical Study](https://arxiv.org/abs/2506.07389)
*Guanming Qiao,Partha Protim Paul*

Main category: cs.HC

TL;DR: 研究分析了智能合约模糊测试工具在实践中的挑战，通过分析GitHub问题和用户研究，提出了改进建议。


<details>
  <summary>Details</summary>
Motivation: 智能合约模糊测试工具的采用面临挑战，研究旨在揭示这些挑战及其对不同实践群体的影响。

Method: 通过分析381个GitHub问题和用户研究，归纳内容并分类挑战。

Result: 发现技术（如区块链模拟）和人为（如文档不足）的挑战，并提出了改进方向。

Conclusion: 研究结果为工具开发者和研究者提供了改进智能合约模糊测试工具的实用建议。

Abstract: Smart contract (SC) fuzzing is a critical technique for detecting
vulnerabilities in blockchain applications. However, its adoption remains
challenging for practitioners due to fundamental differences between SCs and
traditional software systems. In this study, we investigate the challenges
practitioners face when adopting SC fuzzing tools by conducting an inductive
content analysis of 381 GitHub issues from two widely used SC fuzzers: Echidna
and Foundry. Furthermore, we conducted a user study to examine how these
challenges affect different practitioner groups, SC developers, and traditional
software security professionals, and identify strategies practitioners use to
overcome them. We systematically categorize these challenges into a taxonomy
based on their nature and occurrence within the SC fuzzing workflow. Our
findings reveal domain-specific ease-of-use and usefulness challenges,
including technical issues with blockchain emulation, and human issues with a
lack of accessible documentation and process automation. Our results provide
actionable insights for tool developers and researchers, guiding future
improvements in SC fuzzer tool design.

</details>


### [415] [Happiness Finder: Exploring the Role of AI in Enhancing Well-Being During Four-Leaf Clover Searches](https://arxiv.org/abs/2506.07393)
*Anna Yokokubo,Takeo Hamada,Tatsuya Ishizuka,Hiroaki Mori,Noboru Koshizuka*

Main category: cs.HC

TL;DR: 研究探讨了AI辅助寻找四叶草时用户的感受，开发了名为“HappinessFinder”的系统，并在国际工作坊中展示。


<details>
  <summary>Details</summary>
Motivation: 四叶草象征幸运，但传统搜索与AI辅助搜索的成就感不同，研究旨在探索用户感受。

Method: 开发基于智能手机或平板电脑的“HappinessFinder”系统，利用目标检测算法辅助搜索。

Result: 在国际工作坊中展示系统，参与者使用人工盆栽四叶草和应用程序体验搜索。

Conclusion: 研究通过实际演示验证了AI辅助搜索四叶草的用户体验。

Abstract: A four-leaf clover (FLC) symbolizes luck and happiness worldwide, but it is
hard to distinguish it from the common three-leaf clover. While AI technology
can assist in searching for FLC, it may not replicate the traditional search's
sense of achievement. This study explores searcher feelings when AI aids the
FLC search. In this study, we developed a system called ``Happiness Finder''
that uses object detection algorithms on smartphones or tablets to support the
search. We exhibited HappinessFinder at an international workshop, allowing
participants to experience four-leaf clover searching using potted artificial
clovers and the HappinessFinder app. This paper reports the findings from this
demonstration.

</details>


### [416] [Interaction Analysis by Humans and AI: A Comparative Perspective](https://arxiv.org/abs/2506.07707)
*Maryam Teimouri,Filip Ginter,Tomi "bgt" Suovuo*

Main category: cs.HC

TL;DR: 研究比较了混合现实（MR）和2D视频会议（Zoom）对儿童在猜谜游戏中沟通的影响，发现MR能促进更丰富互动，而Zoom更简单易用。


<details>
  <summary>Details</summary>
Motivation: 探讨不同技术平台（MR和Zoom）如何影响儿童的沟通和协作体验。

Method: 芬兰语参与者使用HoloLens MR和Zoom完成协作任务，通过LLM分析录音和视频数据。

Result: MR促进更高情感表达和参与度，Zoom更简单易用；LLM显著减少数据处理时间。

Conclusion: MR有潜力提升分布式环境中儿童的协作学习体验。

Abstract: This paper explores how Mixed Reality (MR) and 2D video conferencing
influence children's communication during a gesture-based guessing game.
Finnish-speaking participants engaged in a short collaborative task using two
different setups: Microsoft HoloLens MR and Zoom. Audio-video recordings were
transcribed and analyzed using Large Language Models (LLMs), enabling iterative
correction, translation, and annotation. Despite limitations in annotations'
accuracy and agreement, automated approaches significantly reduced processing
time and allowed non-Finnish-speaking researchers to participate in data
analysis. Evaluations highlight both the efficiency and constraints of
LLM-based analyses for capturing children's interactions across these
platforms. Initial findings indicate that MR fosters richer interaction,
evidenced by higher emotional expression during annotation, and heightened
engagement, while Zoom offers simplicity and accessibility. This study
underscores the potential of MR to enhance collaborative learning experiences
for children in distributed settings.

</details>


### [417] [Supporting Aging Well through Accessible Digital Games: The Supplemental Role of AI in Game Design for Older Adults](https://arxiv.org/abs/2506.07777)
*Brandon Lyman,Yichi Zhang,Celia Pearce,Miso Kim,Casper Harteveld,Leanne Chukoskie,Bob De Schutter*

Main category: cs.HC

TL;DR: 论文探讨了老年游戏玩家群体的多样性，提出传统游戏无障碍设计的局限性，并引入人工智能作为个性化无障碍设计的解决方案。


<details>
  <summary>Details</summary>
Motivation: 随着老年游戏玩家群体的多样性和需求的增加，传统的通用无障碍设计已无法满足个体需求，需要更灵活的解决方案。

Method: 结合老年学、人机交互和残障研究的视角，提出利用人工智能技术开发个性化的游戏无障碍功能。

Result: 人工智能可以针对老年玩家的个体需求提供适应性无障碍功能，补充传统设计的不足。

Conclusion: 个性化无障碍设计对老年玩家的游戏体验至关重要，有助于实现健康老龄化的长期目标。

Abstract: As the population continues to age, and gaming continues to grow as a hobby
for older people, heterogeneity among older adult gamers is increasing. We
argue that traditional game-based accessibility features, such as simplified
input schemes, redundant information channels, and increased legibility of
digital user interfaces, are increasingly limited in the face of this
heterogeneity. This is because such features affect all older adult players
simultaneously and therefore are designed generically. We introduce artificial
intelligence, although it has its own limitations and ethical concerns, as a
method of creating player-based accessibility features, given the adaptive
nature of the emerging technology. These accessibility features may help to
address unique assemblage of accessibility needs an individual may accumulate
through age. We adopt insights from gerontology, HCI, and disability studies
into the digital game design discourse for older adults, and we contribute
insight that can guide the integration of player-based accessibility features
to supplement game-based counterparts. The accessibility of digital games for
heterogenous older adult audience is paramount, as the medium offers short-term
social, emotional, psychological, cognitive, and physical that support the
long-term goal of aging well.

</details>


### [418] [Integrating Artificial Intelligence as Assistive Technology for Older Adult Gamers: A Pilot Study](https://arxiv.org/abs/2506.07830)
*Yichi Zhang,Brandon Lyman,Celia Pearce,Miso Kim,Casper Harteveld,Leanne Chukoskie,Bob De Schutter*

Main category: cs.HC

TL;DR: 研究探讨AI如何改善老年玩家的游戏体验，通过迭代设计调查问卷，初步发现可用性问题和AI的潜在影响。


<details>
  <summary>Details</summary>
Motivation: 老年玩家在数字游戏中常被忽视，研究旨在探索AI如何提升其游戏体验。

Method: 迭代开发问卷调查，分析39名参与者的反馈，优化问卷内容和形式。

Result: 老年玩家面临游戏可用性问题，对AI的态度受实用性和复杂性影响。

Conclusion: 研究为设计包容性AI游戏体验提供了初步见解，并优化了未来大规模研究的问卷。

Abstract: With respect to digital games, older adults are a demographic that is often
underserved due to an industry-wide focus on younger audiences' preferences and
skill sets. Meanwhile, as artificial intelligence (AI) continues to expand into
everyday technologies, its assistive capabilities have been recognized,
suggesting its potential in improving the gaming experience for older gamers.
To study this potential, we iteratively developed a pilot survey aimed at
understanding older adult gamers' current gameplay preference, challenges they
are facing, and their perspectives of AI usage in gaming. This article
contributes an overview of our iterative survey-design workflow, and pilot
results from 39 participants. During each iteration, we analyzed the survey's
efficacy and adjusted the content, language, and format to better capture
meaningful data, and was able to create a refined survey for a larger, more
representative future parent study. At the same time, preliminary findings
suggest that for older adult gamers, usability issues in gaming remain key
obstacles, while this demographic's perceptions of AI are shaped by both its
practical benefits and concerns about autonomy and complexity. These findings
also offer early insights for the design of age-inclusive, AI-supported gaming
experiences.

</details>


### [419] [Predicting Situation Awareness from Physiological Signals](https://arxiv.org/abs/2506.07930)
*Kieran J. Smith,Tristan C. Endsley,Torin K. Clark*

Main category: cs.HC

TL;DR: 研究通过多模态生理信号预测情境意识（SA）的三个层次，验证了其有效性，并发现EEG和眼动信号对预测最有用。


<details>
  <summary>Details</summary>
Motivation: 由于传统SA测量方法具有干扰性，研究寻求生理指标以实时评估SA。

Method: 使用多模态生理信号（神经生理、心理生理和行为信号）预测SA，并通过实验验证。

Result: 多模态模型预测SA效果优于随机标签模型，EEG和眼动信号对预测SA最有效。

Conclusion: 多模态生理信号可非干扰性地预测SA，为复杂任务中的SA评估提供了新方法。

Abstract: Situation awareness (SA)--comprising the ability to 1) perceive critical
elements in the environment, 2) comprehend their meanings, and 3) project their
future states--is critical for human operator performance. Due to the
disruptive nature of gold-standard SA measures, researchers have sought
physiological indicators to provide real-time information about SA. We extend
prior work by using a multimodal suite of neurophysiological,
psychophysiological, and behavioral signals, predicting all three levels of SA
along a continuum, and predicting a comprehensive measure of SA in a complex
multi-tasking simulation. We present a lab study in which 31 participants
controlled an aircraft simulator task battery while wearing physiological
sensors and responding to SA 'freeze-probe' assessments. We demonstrate the
validity of task and assessment for measuring SA. Multimodal physiological
models predict SA with greater predictive performance ($Q^2$ for levels 1-3 and
total, respectively: 0.14, 0.00, 0.26, and 0.36) than models built with
shuffled labels, demonstrating that multimodal physiological signals provide
useful information in predicting all SA levels. Level 3 SA (projection) was
best predicted, and level 2 SA comprehension) was the most challenging to
predict. Ablation analysis and single sensor models found EEG and eye-tracking
signals to be particularly useful to predictions of level 3 and total SA. A
reduced sensor fusion model showed that predictive performance can be
maintained with a subset of sensors. This first rigorous cross-validation
assessment of predictive performance demonstrates the utility of multimodal
physiological signals for inferring complex, holistic, objective measures of SA
at all levels, non-disruptively, and along a continuum.

</details>


### [420] [Implementation Considerations for Automated AI Grading of Student Work](https://arxiv.org/abs/2506.07955)
*Zewei,Tian,Alex Liu,Lief Esbenshade,Shawon Sarkar,Zachary Zhang,Kevin He,Min Sun*

Main category: cs.HC

TL;DR: 研究探讨了K-12教育中AI评分平台的课堂应用，发现教师重视AI的快速反馈，但对自动评分持怀疑态度，强调需人工监督。


<details>
  <summary>Details</summary>
Motivation: 探索AI评分平台在K-12教育中的实际应用效果，以及教师和学生对AI反馈的接受度。

Method: 通过平台使用日志、问卷调查和定性访谈，分析教师对AI生成评分标准和反馈的使用情况。

Result: 教师认可AI的快速反馈功能，但对自动评分不信任；学生欢迎快速反馈，但对纯AI评分持怀疑态度。

Conclusion: 设计可信赖、以教师为中心的AI评估工具，需平衡反馈效率与教学自主权。

Abstract: This study explores the classroom implementation of an AI-powered grading
platform in K-12 settings through a co-design pilot with 19 teachers. We
combine platform usage logs, surveys, and qualitative interviews to examine how
teachers use AI-generated rubrics and grading feedback. Findings reveal that
while teachers valued the AI's rapid narrative feedback for formative purposes,
they distrusted automated scoring and emphasized the need for human oversight.
Students welcomed fast, revision-oriented feedback but remained skeptical of
AI-only grading. We discuss implications for the design of trustworthy,
teacher-centered AI assessment tools that enhance feedback while preserving
pedagogical agency.

</details>


### [421] [Supporting Construction Worker Well-Being with a Multi-Agent Conversational AI System](https://arxiv.org/abs/2506.07997)
*Fan Yang,Yuan Tian,Jiansong Zhang*

Main category: cs.HC

TL;DR: 论文提出了一种基于多智能体对话系统的AI解决方案，用于改善建筑工人的心理健康和工作支持。


<details>
  <summary>Details</summary>
Motivation: 建筑行业存在高心理风险，但心理健康支持不足，AI尤其是大语言模型（LLMs）的应用潜力尚未充分挖掘。

Method: 开发了一个结合领域知识的AI驱动多智能体对话系统，每个智能体具有独特角色，以满足工人的实际需求和社交互动。

Result: 用户研究表明，该系统在可用性、自主性、社交存在感和信任度上显著优于单智能体基线，提升幅度分别为18%、40%、60%和60%。

Conclusion: LLM驱动的AI系统在建筑行业特定支持中具有显著潜力。

Abstract: The construction industry is characterized by both high physical and
psychological risks, yet supports of mental health remain limited. While
advancements in artificial intelligence (AI), particularly large language
models (LLMs), offer promising solutions, their potential in construction
remains largely underexplored. To bridge this gap, we developed a
conversational multi-agent system that addresses industry-specific challenges
through an AI-driven approach integrated with domain knowledge. In parallel, it
fulfills construction workers' basic psychological needs by enabling
interactions with multiple agents, each has a distinct persona. This approach
ensures that workers receive both practical problem-solving support and social
engagement, ultimately contributing to their overall well-being. We evaluate
its usability and effectiveness through a within-subjects user study with 12
participants. The results show that our system significantly outperforms the
single-agent baseline, achieving improvements of 18% in usability, 40% in
self-determination, 60% in social presence, and 60% in trust. These findings
highlight the promise of LLM-driven AI systems in providing domain-specific
support for construction workers.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [422] [CellCLIP -- Learning Perturbation Effects in Cell Painting via Text-Guided Contrastive Learning](https://arxiv.org/abs/2506.06290)
*Mingyu Lu,Ethan Weinberger,Chanwoo Kim,Su-In Lee*

Main category: cs.LG

TL;DR: CellCLIP是一个用于高内涵筛选数据的跨模态对比学习框架，通过预训练图像编码器和新型通道编码方案，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 利用高内涵筛选数据理解扰动与细胞形态效应的关系，但现有方法因图像语义差异和扰动类别多样性而难以直接应用。

Method: 结合预训练图像编码器、新型通道编码方案和自然语言编码器，构建跨模态对比学习框架。

Result: CellCLIP在跨模态检索和下游生物任务中表现最优，同时显著减少计算时间。

Conclusion: CellCLIP为高内涵筛选数据提供了一种高效且性能优越的跨模态学习方法。

Abstract: High-content screening (HCS) assays based on high-throughput microscopy
techniques such as Cell Painting have enabled the interrogation of cells'
morphological responses to perturbations at an unprecedented scale. The
collection of such data promises to facilitate a better understanding of the
relationships between different perturbations and their effects on cellular
state. Towards achieving this goal, recent advances in cross-modal contrastive
learning could, in theory, be leveraged to learn a unified latent space that
aligns perturbations with their corresponding morphological effects. However,
the application of such methods to HCS data is not straightforward due to
substantial differences in the semantics of Cell Painting images compared to
natural images, and the difficulty of representing different classes of
perturbations (e.g., small molecule vs CRISPR gene knockout) in a single latent
space. In response to these challenges, here we introduce CellCLIP, a
cross-modal contrastive learning framework for HCS data. CellCLIP leverages
pre-trained image encoders coupled with a novel channel encoding scheme to
better capture relationships between different microscopy channels in image
embeddings, along with natural language encoders for representing
perturbations. Our framework outperforms current open-source models,
demonstrating the best performance in both cross-modal retrieval and
biologically meaningful downstream tasks while also achieving significant
reductions in computation time.

</details>


### [423] [Improvement of Optimization using Learning Based Models in Mixed Integer Linear Programming Tasks](https://arxiv.org/abs/2506.06291)
*Xiaoke Wang,Batuhan Altundas,Zhaoxin Li,Aaron Zhao,Matthew Gombolay*

Main category: cs.LG

TL;DR: 提出了一种基于学习的方法，结合行为克隆和强化学习，训练图神经网络为MILP求解器提供高质量初始解，显著减少计算时间。


<details>
  <summary>Details</summary>
Motivation: MILP在规划和调度问题中应用广泛，但计算时间长限制了其在大规模实时场景中的使用。

Method: 利用行为克隆（BC）和强化学习（RL）训练图神经网络（GNN），为多智能体任务分配和调度问题中的MILP求解器提供初始解。

Result: 实验表明，该方法减少了优化时间和方差，同时保持了解决方案的质量和可行性。

Conclusion: 提出的学习框架有效提升了MILP求解器的效率，适用于大规模实时场景。

Abstract: Mixed Integer Linear Programs (MILPs) are essential tools for solving
planning and scheduling problems across critical industries such as
construction, manufacturing, and logistics. However, their widespread adoption
is limited by long computational times, especially in large-scale, real-time
scenarios. To address this, we present a learning-based framework that
leverages Behavior Cloning (BC) and Reinforcement Learning (RL) to train Graph
Neural Networks (GNNs), producing high-quality initial solutions for
warm-starting MILP solvers in Multi-Agent Task Allocation and Scheduling
Problems. Experimental results demonstrate that our method reduces optimization
time and variance compared to traditional techniques while maintaining solution
quality and feasibility.

</details>


### [424] [Mutual-Taught for Co-adapting Policy and Reward Models](https://arxiv.org/abs/2506.06292)
*Tianyuan Shi,Canbin Huang,Fanqi Wan,Longguang Zhong,Ziyi Yang,Weizhou Shen,Xiaojun Quan,Ming Yan*

Main category: cs.LG

TL;DR: 论文提出Mutual-Taught方法，通过自训练迭代优化策略模型和奖励模型，解决分布偏移问题，无需额外人工标注。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型偏好优化中，新生成样本与奖励模型训练数据间的分布偏移会降低奖励模型效果，进而影响策略模型性能。

Method: 采用类似EM算法的自训练方法：E步用当前奖励模型反馈更新策略模型；M步用策略模型输出更新奖励模型。

Result: 实验表明方法有效，8B策略模型在AlpacaEval-2上胜率54.1%，8B奖励模型性能媲美GPT-4o。

Conclusion: Mutual-Taught通过迭代优化解决了分布偏移问题，显著提升了模型性能。

Abstract: During the preference optimization of large language models (LLMs),
distribution shifts may arise between newly generated model samples and the
data used to train the reward model (RM). This shift reduces the efficacy of
the RM, which in turn negatively impacts the performance of the policy model
(PM). To address this challenge, we propose Mutual-Taught, a self-training
method that iteratively improves both the PM and RM without requiring
additional human annotation. Our approach mirrors the expectation-maximization
(EM) algorithm. In the E-step, the PM is updated using feedback from the
current RM, guiding the PM toward a better approximation of the latent optimal
preference distribution. In the M-step, we update the RM by constructing
training data from the outputs of the PM before and after the E-step update.
This process ensures that the RM adapts to the evolving policy distribution.
Experimental results demonstrate that this iterative approach leads to
consistent improvements in both models. Specifically, our 8B policy model,
LLaMA-3-8B-Instruct-MT, achieves a length-controlled win rate of 54.1\% on
AlpacaEval-2, while our 8B reward model, FsfairX-LLaMA3-RM-MT, performs on par
with GPT-4o-2024-08-06 on RewardBench.

</details>


### [425] [Prediction of Bank Credit Ratings using Heterogeneous Topological Graph Neural Networks](https://arxiv.org/abs/2506.06293)
*Junyi Liu,Stanley Kok*

Main category: cs.LG

TL;DR: 该研究利用持久同调构建银行关系网络，结合传统借贷网络形成异构网络，提升信用评级预测准确性。


<details>
  <summary>Details</summary>
Motivation: 银行信用评级对经济稳定和决策至关重要，但隐私问题导致完整的银行间连接图难以获取，限制了图神经网络的应用。

Method: 使用持久同调构建银行关系网络，并与传统借贷网络结合形成异构网络，提出HTGNN模型。

Result: 在真实全球数据集上验证了HTGNN的有效性，提升了预测准确性。

Conclusion: 该研究为投资者和监管机构提供了改进风险缓解和市场干预的工具。

Abstract: Agencies such as Standard & Poor's and Moody's provide bank credit ratings
that influence economic stability and decision-making by stakeholders. Accurate
and timely predictions support informed decision-making, regulatory actions,
and investor protection. However, a complete interbank connection graph is
often unavailable due to privacy concerns, complicating the direct application
of Graph Neural Networks (GNNs) for rating prediction. our research utilizes
persistent homology to construct a network that captures relationships among
banks and combines this with a traditional lending network to create a
heterogeneous network that integrates information from both sources, leading to
improved predictions. Experiments on a global, real-world dataset validate the
effectiveness of HTGNN. This research has implications for investors and
regulatory bodies in enhancing proactive risk mitigation and the implementation
of effective market interventions.The code can be find at
https://github.com/Liu-Jun-Yi/HTGNN.

</details>


### [426] [GLProtein: Global-and-Local Structure Aware Protein Representation Learning](https://arxiv.org/abs/2506.06294)
*Yunqing Liu,Wenqi Fan,Xiaoyong Wei,Qing Li*

Main category: cs.LG

TL;DR: GLProtein是一个结合全局结构相似性和局部氨基酸细节的蛋白质预训练框架，显著提升了预测准确性。


<details>
  <summary>Details</summary>
Motivation: 蛋白质结构信息不仅限于3D信息，还包括从氨基酸分子到蛋白质-蛋白质结构相似性的多层次信息，现有方法尚未充分整合这些信息。

Method: GLProtein创新性地结合了蛋白质掩码建模、三重结构相似性评分、蛋白质3D距离编码和基于子结构的氨基酸分子编码。

Result: 实验表明，GLProtein在蛋白质-蛋白质相互作用预测、接触预测等任务上优于现有方法。

Conclusion: GLProtein为蛋白质功能预测提供了更全面的结构信息整合方法，具有广泛的应用潜力。

Abstract: Proteins are central to biological systems, participating as building blocks
across all forms of life. Despite advancements in understanding protein
functions through protein sequence analysis, there remains potential for
further exploration in integrating protein structural information. We argue
that the structural information of proteins is not only limited to their 3D
information but also encompasses information from amino acid molecules (local
information) to protein-protein structure similarity (global information). To
address this, we propose \textbf{GLProtein}, the first framework in protein
pre-training that incorporates both global structural similarity and local
amino acid details to enhance prediction accuracy and functional insights.
GLProtein innovatively combines protein-masked modelling with triplet structure
similarity scoring, protein 3D distance encoding and substructure-based amino
acid molecule encoding. Experimental results demonstrate that GLProtein
outperforms previous methods in several bioinformatics tasks, including
predicting protein-protein interaction, contact prediction, and so on.

</details>


### [427] [dLLM-Cache: Accelerating Diffusion Large Language Models with Adaptive Caching](https://arxiv.org/abs/2506.06295)
*Zhiyuan Liu,Yicun Yang,Yaojie Zhang,Junjie Chen,Chang Zou,Qingyuan Wei,Shaobo Wang,Linfeng Zhang*

Main category: cs.LG

TL;DR: dLLM-Cache是一种无需训练的缓存框架，通过结合长间隔提示缓存和部分响应更新，显著降低扩散型大语言模型（dLLMs）的推理延迟，速度提升高达9.1倍。


<details>
  <summary>Details</summary>
Motivation: 扩散型大语言模型（dLLMs）虽然具有潜力，但推理延迟高，且传统加速技术不适用。dLLM-Cache旨在解决这一问题。

Method: 提出dLLM-Cache框架，利用静态提示和部分动态响应的特性，通过长间隔提示缓存和特征相似性指导的部分响应更新，实现高效计算复用。

Result: 实验表明，dLLM-Cache在LLaDA 8B和Dream 7B等模型上实现高达9.1倍的加速，且不损失输出质量。

Conclusion: dLLM-Cache显著降低了dLLMs的推理延迟，使其接近自回归模型的水平，为dLLMs的实际应用提供了可行方案。

Abstract: Autoregressive Models (ARMs) have long dominated the landscape of Large
Language Models. Recently, a new paradigm has emerged in the form of
diffusion-based Large Language Models (dLLMs), which generate text by
iteratively denoising masked segments. This approach has shown significant
advantages and potential. However, dLLMs suffer from high inference latency.
Traditional ARM acceleration techniques, such as Key-Value caching, are
incompatible with dLLMs due to their bidirectional attention mechanism. To
address this specific challenge, our work begins with a key observation that
dLLM inference involves a static prompt and a partially dynamic response, where
most tokens remain stable across adjacent denoising steps. Based on this, we
propose dLLM-Cache, a training-free adaptive caching framework that combines
long-interval prompt caching with partial response updates guided by feature
similarity. This design enables efficient reuse of intermediate computations
without compromising model performance. Extensive experiments on representative
dLLMs, including LLaDA 8B and Dream 7B, show that dLLM-Cache achieves up to 9.1
x speedup over standard inference without compromising output quality. Notably,
our method brings dLLM inference latency close to that of ARMs under many
settings. Codes are provided in the supplementary material and will be released
publicly on GitHub.

</details>


### [428] [Dynamic Graph CNN with Jacobi Kolmogorov-Arnold Networks for 3D Classification of Point Sets](https://arxiv.org/abs/2506.06296)
*Hanaa El Afia,Said Ohamouddou,Raddouane Chiheb,Abdellatif El Afia*

Main category: cs.LG

TL;DR: Jacobi-KAN-DGCNN框架结合动态图卷积神经网络与Jacobi多项式KAN，用于三维点云分类，性能优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 探索多项式扩展在动态图卷积网络中的应用，以提升点云分类的准确性和收敛速度。

Method: 用可调单变量多项式扩展替换MLP层，结合DGCNN架构，避免深层结构。

Result: 在ModelNet40数据集上，Jacobi多项式KAN层在精度和收敛速度上优于传统线性层，且参数高效。

Conclusion: 高多项式阶数不总是提升性能，需进一步研究多项式基、阶数与图学习机制的相互作用。

Abstract: We introduce Jacobi-KAN-DGCNN, a framework that integrates Dynamic Graph
Convolutional Neural Network (DGCNN) with Jacobi Kolmogorov-Arnold Networks
(KAN) for the classification of three-dimensional point clouds. This method
replaces Multi-Layer Perceptron (MLP) layers with adaptable univariate
polynomial expansions within a streamlined DGCNN architecture, circumventing
deep levels for both MLP and KAN to facilitate a layer-by-layer comparison. In
comparative experiments on the ModelNet40 dataset, KAN layers employing Jacobi
polynomials outperform the traditional linear layer-based DGCNN baseline in
terms of accuracy and convergence speed, while maintaining parameter
efficiency. Our results demonstrate that higher polynomial degrees do not
automatically improve performance, highlighting the need for further
theoretical and empirical investigation to fully understand the interactions
between polynomial bases, degrees, and the mechanisms of graph-based learning.

</details>


### [429] [Optimal patient allocation for echocardiographic assessments](https://arxiv.org/abs/2506.06297)
*Bozhi Sun,Seda Tierney,Jeffrey A. Feinstein,Frederick Damen,Alison L. Marsden,Daniele E. Schiavazzi*

Main category: cs.LG

TL;DR: 论文通过离散事件随机模拟和强化学习优化医院超声心动图检查的调度策略，提出动态分配策略优于预留策略。


<details>
  <summary>Details</summary>
Motivation: 医院超声心动图检查调度面临非确定性因素和资源不对称的挑战，需优化资源分配以提高效率。

Method: 基于一周运营数据预处理，构建SimPy模拟模型，结合Gymnasium库，比较动态与预留策略，并应用强化学习优化。

Result: 动态分配策略表现更优，强化学习策略进一步提升了效率。

Conclusion: 数据驱动的动态资源管理策略可有效提升超声心动图检查的调度效率。

Abstract: Scheduling echocardiographic exams in a hospital presents significant
challenges due to non-deterministic factors (e.g., patient no-shows, patient
arrival times, diverse exam durations, etc.) and asymmetric resource
constraints between fetal and non-fetal patient streams. To address these
challenges, we first conducted extensive pre-processing on one week of
operational data from the Echo Laboratory at Stanford University's Lucile
Packard Children's Hospital, to estimate patient no-show probabilities and
derive empirical distributions of arrival times and exam durations. Based on
these inputs, we developed a discrete-event stochastic simulation model using
SimPy, and integrate it with the open source Gymnasium Python library. As a
baseline for policy optimization, we developed a comparative framework to
evaluate on-the-fly versus reservation-based allocation strategies, in which
different proportions of resources are reserved in advance. Considering a
hospital configuration with a 1:6 ratio of fetal to non-fetal rooms and a 4:2
ratio of fetal to non-fetal sonographers, we show that on-the-fly allocation
generally yields better performance, more effectively adapting to patient
variability and resource constraints. Building on this foundation, we apply
reinforcement learning (RL) to derive an approximated optimal dynamic
allocation policy. This RL-based policy is benchmarked against the
best-performing rule-based strategies, allowing us to quantify their
differences and provide actionable insights for improving echo lab efficiency
through intelligent, data-driven resource management.

</details>


### [430] [Pairwise Calibrated Rewards for Pluralistic Alignment](https://arxiv.org/abs/2506.06298)
*Daniel Halpern,Evi Micha,Ariel D. Procaccia,Itai Shapira*

Main category: cs.LG

TL;DR: 论文提出了一种基于多样化奖励函数分布的对齐方法，以解决人类偏好的多样性问题，避免少数观点被忽视。


<details>
  <summary>Details</summary>
Motivation: 现有对齐方法假设存在单一理想行为，而人类偏好因用户、背景和文化而异，导致少数观点被多数信号掩盖。

Method: 通过从成对偏好中学习奖励函数分布，无需标注者标识或预定义组，将分歧视为软标签。核心标准是成对校准。

Result: 理论和实验表明，即使小规模无异常值的集成也能准确表示多样化偏好分布，校准效果显著提升。

Conclusion: 该方法能更忠实反映多元化价值观，为对齐问题提供了新思路。

Abstract: Current alignment pipelines presume a single, universal notion of desirable
behavior. However, human preferences often diverge across users, contexts, and
cultures. As a result, disagreement collapses into the majority signal and
minority perspectives are discounted. To address this, we propose reflecting
diverse human preferences through a distribution over multiple reward
functions, each inducing a distinct aligned policy. The distribution is learned
directly from pairwise preference without annotator identifiers or predefined
groups. Instead, annotator disagreements are treated as informative soft
labels. Our central criterion is pairwise calibration: for every pair of
candidate responses, the proportion of reward functions preferring one response
matches the fraction of annotators with that preference. We prove that even a
small outlier-free ensemble can accurately represent diverse preference
distributions. Empirically, we introduce and validate a practical training
heuristic to learn such ensembles, and demonstrate its effectiveness through
improved calibration, implying a more faithful representation of pluralistic
values.

</details>


### [431] [LT-PINN: Lagrangian Topology-conscious Physics-informed Neural Network for Boundary-focused Engineering Optimization](https://arxiv.org/abs/2506.06300)
*Yuanye Zhou,Zhaokun Wang,Kai Zhou,Hui Tang,Xiaofan Li*

Main category: cs.LG

TL;DR: LT-PINNs是一种新型的边界优化框架，通过参数化边界曲线控制变量，避免了传统PINNs的手动插值需求，提升了复杂几何的适用性。


<details>
  <summary>Details</summary>
Motivation: 传统PINNs依赖基于密度的拓扑描述，需要手动插值且难以处理复杂几何，限制了其应用范围。

Method: 提出LT-PINNs，将边界曲线控制变量作为可学习参数，引入边界条件和拓扑损失函数，确保边界精确表示。

Result: LT-PINNs在多种PDE问题中表现出色，显著降低误差，并能处理任意边界条件。

Conclusion: LT-PINNs为工程优化提供了高效、精确的边界优化工具，适用于复杂拓扑问题。

Abstract: Physics-informed neural networks (PINNs) have emerged as a powerful meshless
tool for topology optimization, capable of simultaneously determining optimal
topologies and physical solutions. However, conventional PINNs rely on
density-based topology descriptions, which necessitate manual interpolation and
limit their applicability to complex geometries. To address this, we propose
Lagrangian topology-conscious PINNs (LT-PINNs), a novel framework for
boundary-focused engineering optimization. By parameterizing the control
variables of topology boundary curves as learnable parameters, LT-PINNs
eliminate the need for manual interpolation and enable precise boundary
determination. We further introduce specialized boundary condition loss
function and topology loss function to ensure sharp and accurate boundary
representations, even for intricate topologies. The accuracy and robustness of
LT-PINNs are validated via two types of partial differential equations (PDEs),
including elastic equation with Dirichlet boundary conditions and Laplace's
equation with Neumann boundary conditions. Furthermore, we demonstrate
effectiveness of LT-PINNs on more complex time-dependent and time-independent
flow problems without relying on measurement data, and showcase their
engineering application potential in flow velocity rearrangement, transforming
a uniform upstream velocity into a sine-shaped downstream profile. The results
demonstrate (1) LT-PINNs achieve substantial reductions in relative L2 errors
compared with the state-of-art density topology-oriented PINNs (DT-PINNs), (2)
LT-PINNs can handle arbitrary boundary conditions, making them suitable for a
wide range of PDEs, and (3) LT-PINNs can infer clear topology boundaries
without manual interpolation, especially for complex topologies.

</details>


### [432] [Reward Is Enough: LLMs Are In-Context Reinforcement Learners](https://arxiv.org/abs/2506.06303)
*Kefan Song,Amir Moeini,Peng Wang,Lei Gong,Rohan Chandra,Yanjun Qi,Shangtong Zhang*

Main category: cs.LG

TL;DR: 论文提出了一种名为ICRL prompting的多轮提示框架，展示了LLM在推理时表现出类似强化学习的行为，通过奖励反馈逐步提升任务完成质量。


<details>
  <summary>Details</summary>
Motivation: 探索LLM在推理时是否能够表现出类似强化学习的行为，以提升任务完成质量。

Method: 提出ICRL prompting框架，通过多轮提示和奖励反馈引导LLM逐步优化响应。

Result: 在Game of 24、创意写作和ScienceWorld等基准测试中，ICRL prompting显著优于基线方法。

Conclusion: ICRL prompting为LLM在推理时实现类似强化学习的行为提供了新范式，展示了计算资源扩展的潜力。

Abstract: Reinforcement learning (RL) is a human-designed framework for solving
sequential decision making problems. In this work, we demonstrate that,
surprisingly, RL emerges in LLM's (Large Language Model) inference time -- a
phenomenon known as in-context RL (ICRL). Specifically, we propose a novel
multi-round prompting framework called ICRL prompting. The goal is to prompt
the LLM to complete a task. After the LLM generates a response at the current
round, we give numerical scalar feedbacks for the response, called the rewards.
At the next round, we prompt the LLM again with the same task and a context
consisting of all previous responses and rewards. We observe that the quality
of the LLM's response increases as the context grows. In other words, the LLM
is able to maximize the scalar reward signal in the inference time, just like
an RL algorithm. We evaluate ICRL prompting in three benchmarks (Game of 24,
creative writing, and ScienceWorld) and demonstrate significant performance
improvements over baseline methods such as Self-Refine and Reflexion.
Surprisingly, in some experiments the reward signals are generated by the LLM
itself, yet performance improvements are still observed from ICRL prompting,
offering a promising paradigm for scaling test-time compute.

</details>


### [433] [Wine Quality Prediction with Ensemble Trees: A Unified, Leak-Free Comparative Study](https://arxiv.org/abs/2506.06327)
*Zilang Chen*

Main category: cs.LG

TL;DR: 论文提出了一个基于五种集成学习方法的葡萄酒质量评估基准测试，通过泄漏避免的工作流程和优化方法，比较了不同模型的性能和效率。


<details>
  <summary>Details</summary>
Motivation: 葡萄酒质量评估目前主要依赖主观的人工品尝，缺乏客观且可重复的方法。

Method: 使用五种集成学习方法（Random Forest, Gradient Boosting, XGBoost, LightGBM, CatBoost），结合泄漏避免的工作流程、数据预处理、超参数优化和特征选择。

Result: Gradient Boosting在准确率上表现最佳，但Random Forest在效率和成本效益上更优。特征选择可显著降低维度而仅轻微影响性能。

Conclusion: 推荐Random Forest作为生产模型，XGBoost和LightGBM为GPU高效替代方案，Gradient Boosting为离线基准测试的精度上限。

Abstract: Accurate and reproducible wine-quality assessment is critical for production
control yet remains dominated by subjective, labour-intensive tasting panels.
We present the first unified benchmark of five ensemble learners (Random
Forest, Gradient Boosting, XGBoost, LightGBM, CatBoost) on the canonical Vinho
Verde red- and white-wine datasets (1,599 and 4,898 instances, 11
physicochemical attributes). Our leakage-free workflow employs an 80:20
stratified train-test split, five-fold StratifiedGroupKFold within the training
set, per-fold standardisation, SMOTE-Tomek resampling, inverse-frequency cost
weighting, Optuna hyper-parameter search (120-200 trials per model) and a
two-stage feature-selection refit. Final scores on untouched test sets are
reported with weighted F1 as the headline metric. Gradient Boosting achieves
the highest accuracy (weighted F1 0.693 +/- 0.028 for red and 0.664 +/- 0.016
for white), followed within three percentage points by Random Forest and
XGBoost. Limiting each model to its five top-ranked variables lowers
dimensionality by 55 percent while reducing weighted F1 by only 2.6 percentage
points for red and 3.0 percentage points for white, indicating that alcohol,
volatile acidity, sulphates, free SO2 and chlorides capture most predictive
signal. Runtime profiling on an EPYC 9K84/H20 node reveals a steep efficiency
gradient: Gradient Boosting averages 12 h per five-fold study, XGBoost and
LightGBM require 2-3 h, CatBoost 1 h, and Random Forest under 50 min. We
therefore recommend Random Forest as the most cost-effective production model,
XGBoost and LightGBM as GPU-efficient alternatives, and Gradient Boosting as
the accuracy ceiling for offline benchmarking. The fully documented pipeline
and metric set provide a reproducible baseline for future work on imbalanced
multi-class wine-quality prediction.

</details>


### [434] [ExplainBench: A Benchmark Framework for Local Model Explanations in Fairness-Critical Applications](https://arxiv.org/abs/2506.06330)
*James Afful*

Main category: cs.LG

TL;DR: ExplainBench是一个开源基准测试套件，用于系统评估局部模型解释方法，特别是在公平敏感场景下。


<details>
  <summary>Details</summary>
Motivation: 随着机器学习系统在高风险领域的广泛应用，对可解释和可信模型的需求增加，但缺乏标准化、可复现的评估框架。

Method: ExplainBench提供统一的解释算法封装、端到端模型训练和解释生成流程，并通过保真度、稀疏性和鲁棒性指标进行评估。

Result: 在COMPAS、UCI Adult Income和LendingClub等公平研究数据集上展示了不同解释方法的行为。

Conclusion: ExplainBench通过支持可复现的局部解释比较分析，推动了可解释机器学习的方法学基础，并提升了现实AI系统的问责性。

Abstract: As machine learning systems are increasingly deployed in high-stakes domains
such as criminal justice, finance, and healthcare, the demand for interpretable
and trustworthy models has intensified. Despite the proliferation of local
explanation techniques, including SHAP, LIME, and counterfactual methods, there
exists no standardized, reproducible framework for their comparative
evaluation, particularly in fairness-sensitive settings.
  We introduce ExplainBench, an open-source benchmarking suite for systematic
evaluation of local model explanations across ethically consequential datasets.
ExplainBench provides unified wrappers for popular explanation algorithms,
integrates end-to-end pipelines for model training and explanation generation,
and supports evaluation via fidelity, sparsity, and robustness metrics. The
framework includes a Streamlit-based graphical interface for interactive
exploration and is packaged as a Python module for seamless integration into
research workflows.
  We demonstrate ExplainBench on datasets commonly used in fairness research,
such as COMPAS, UCI Adult Income, and LendingClub, and showcase how different
explanation methods behave under a shared experimental protocol. By enabling
reproducible, comparative analysis of local explanations, ExplainBench advances
the methodological foundations of interpretable machine learning and
facilitates accountability in real-world AI systems.

</details>


### [435] [Extending AALpy with Passive Learning: A Generalized State-Merging Approach](https://arxiv.org/abs/2506.06333)
*Benjamin von Berg,Bernhard K. Aichernig*

Main category: cs.LG

TL;DR: AALpy是一个开源的Python自动机学习库，专注于具有IO行为的系统的主动学习。本文介绍了其新增的被动学习领域重要方法——红蓝框架下的状态合并通用实现。


<details>
  <summary>Details</summary>
Motivation: 为了简化状态合并算法的实现，并提供对不同自动机类型的通用支持。

Method: 使用统一的内部表示实现红蓝框架的状态合并，用户只需定义兼容性标准和评分。

Result: 通过AALpy，现有和新算法的实现大幅简化，部分算法仅需几行代码即可完成。

Conclusion: AALpy为状态合并算法提供了高效且通用的实现工具，显著降低了开发复杂度。

Abstract: AALpy is a well-established open-source automata learning library written in
Python with a focus on active learning of systems with IO behavior. It provides
a wide range of state-of-the-art algorithms for different automaton types
ranging from fully deterministic to probabilistic automata. In this work, we
present the recent addition of a generalized implementation of an important
method from the domain of passive automata learning: state-merging in the
red-blue framework. Using a common internal representation for different
automaton types allows for a general and highly configurable implementation of
the red-blue framework. We describe how to define and execute state-merging
algorithms using AALpy, which reduces the implementation effort for
state-merging algorithms mainly to the definition of compatibility criteria and
scoring. This aids the implementation of both existing and novel algorithms. In
particular, defining some existing state-merging algorithms from the literature
with AALpy only takes a few lines of code.

</details>


### [436] [Optimized Local Updates in Federated Learning via Reinforcement Learning](https://arxiv.org/abs/2506.06337)
*Ali Murad,Bo Hui,Wei-Shinn Ku*

Main category: cs.LG

TL;DR: 提出了一种基于深度强化学习（DRL）的框架，优化联邦学习（FL）中客户端的数据选择，以提升性能并减少非独立同分布（non-IID）数据的影响。


<details>
  <summary>Details</summary>
Motivation: 解决FL中因非IID数据导致的性能下降问题，避免客户端过度共享数据。

Method: 利用DRL代理动态选择客户端训练数据的优化量，以训练损失变化为奖励信号，学习优化数据分配策略。

Result: 在多个基准数据集和FL框架上验证了性能提升。

Conclusion: 该框架有效优化了FL客户端训练，缓解了非IID数据的影响，代码已开源。

Abstract: Federated Learning (FL) is a distributed framework for collaborative model
training over large-scale distributed data, enabling higher performance while
maintaining client data privacy. However, the nature of model aggregation at
the centralized server can result in a performance drop in the presence of
non-IID data across different clients. We remark that training a client locally
on more data than necessary does not benefit the overall performance of all
clients. In this paper, we devise a novel framework that leverages a Deep
Reinforcement Learning (DRL) agent to select an optimized amount of data
necessary to train a client model without oversharing information with the
server. Starting without awareness of the client's performance, the DRL agent
utilizes the change in training loss as a reward signal and learns to optimize
the amount of training data necessary for improving the client's performance.
Specifically, after each aggregation round, the DRL algorithm considers the
local performance as the current state and outputs the optimized weights for
each class, in the training data, to be used during the next round of local
training. In doing so, the agent learns a policy that creates an optimized
partition of the local training dataset during the FL rounds. After FL, the
client utilizes the entire local training dataset to further enhance its
performance on its own data distribution, mitigating the non-IID effects of
aggregation. Through extensive experiments, we demonstrate that training FL
clients through our algorithm results in superior performance on multiple
benchmark datasets and FL frameworks. Our code is available at
https://github.com/amuraddd/optimized_client_training.git.

</details>


### [437] [From Transformers to Large Language Models: A systematic review of AI applications in the energy sector towards Agentic Digital Twins](https://arxiv.org/abs/2506.06359)
*Gabriel Antonesi,Tudor Cioara,Ionut Anghel,Vasilis Michalakopoulos,Elissaios Sarmas,Liana Toderean*

Main category: cs.LG

TL;DR: 本文综述了AI在能源领域的应用，重点探讨了Transformer和LLMs在智能电网中的潜力及其挑战，并提出了Agentic Digital Twin的概念。


<details>
  <summary>Details</summary>
Motivation: 传统机器学习在能源管理中面临泛化能力、情境感知和多源数据整合的挑战，而Transformer和LLMs在复杂关系和多模态数据融合方面表现优异，因此值得深入研究其在能源领域的应用。

Method: 通过综述Transformer和LLMs的架构基础、领域适应及实际应用，分析其在能源预测和电网管理中的表现，并探讨LLMs的适应性和新挑战。

Result: 研究发现，Transformer和LLMs在能源领域展现出强大的潜力，尤其在复杂任务和多模态数据融合中表现突出，但也引入了新的挑战。

Conclusion: Generative AI正逐步改变能源管理，从高层规划到日常操作。Agentic Digital Twin概念的提出，为未来自主、主动的能源管理系统指明了方向。

Abstract: Artificial intelligence (AI) has long promised to improve energy management
in smart grids by enhancing situational awareness and supporting more effective
decision-making. While traditional machine learning has demonstrated notable
results in forecasting and optimization, it often struggles with
generalization, situational awareness, and heterogeneous data integration.
Recent advances in foundation models such as Transformer architecture and Large
Language Models (LLMs) have demonstrated improved capabilities in modelling
complex temporal and contextual relationships, as well as in multi-modal data
fusion which is essential for most AI applications in the energy sector. In
this review we synthesize the rapid expanding field of AI applications in the
energy domain focusing on Transformers and LLMs. We examine the architectural
foundations, domain-specific adaptations and practical implementations of
transformer models across various forecasting and grid management tasks. We
then explore the emerging role of LLMs in the field: adaptation and fine tuning
for the energy sector, the type of tasks they are suited for, and the new
challenges they introduce. Along the way, we highlight practical
implementations, innovations, and areas where the research frontier is rapidly
expanding. These recent developments reviewed underscore a broader trend:
Generative AI (GenAI) is beginning to augment decision-making not only in
high-level planning but also in day-to-day operations, from forecasting and
grid balancing to workforce training and asset onboarding. Building on these
developments, we introduce the concept of the Agentic Digital Twin, a
next-generation model that integrates LLMs to bring autonomy, proactivity, and
social interaction into digital twin-based energy management systems.

</details>


### [438] [Chasing Moving Targets with Online Self-Play Reinforcement Learning for Safer Language Models](https://arxiv.org/abs/2506.07468)
*Mickel Liu,Liwei Jiang,Yancheng Liang,Simon Shaolei Du,Yejin Choi,Tim Althoff,Natasha Jaques*

Main category: cs.LG

TL;DR: Self-RedTeam是一种在线自博弈强化学习算法，通过攻击者和防御者的持续交互实现动态安全对齐，显著提升攻击多样性和防御鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 传统语言模型安全对齐是反应式的，攻击者和防御者之间存在滞后，导致防御效果不佳。

Method: 提出Self-RedTeam算法，将安全对齐建模为零和博弈，攻击者和防御者角色交替，通过强化学习实现动态协同进化。

Result: 实验表明，Self-RedTeam攻击多样性提升21.8%，防御鲁棒性提升65.5%，并引入隐藏思维链提升效果。

Conclusion: 研究提倡从被动修补转向主动协同进化，通过多智能体强化学习实现语言模型的自主安全提升。

Abstract: Conventional language model (LM) safety alignment relies on a reactive,
disjoint procedure: attackers exploit a static model, followed by defensive
fine-tuning to patch exposed vulnerabilities. This sequential approach creates
a mismatch -- attackers overfit to obsolete defenses, while defenders
perpetually lag behind emerging threats. To address this, we propose
Self-RedTeam, an online self-play reinforcement learning algorithm where an
attacker and defender agent co-evolve through continuous interaction. We cast
safety alignment as a two-player zero-sum game, where a single model alternates
between attacker and defender roles -- generating adversarial prompts and
safeguarding against them -- while a reward LM adjudicates outcomes. This
enables dynamic co-adaptation. Grounded in the game-theoretic framework of
zero-sum games, we establish a theoretical safety guarantee which motivates the
design of our method: if self-play converges to a Nash Equilibrium, the
defender will reliably produce safe responses to any adversarial input.
Empirically, Self-RedTeam uncovers more diverse attacks (+21.8% SBERT) compared
to attackers trained against static defenders and achieves higher robustness on
safety benchmarks (e.g., +65.5% on WildJailBreak) than defenders trained
against static attackers. We further propose hidden Chain-of-Thought, allowing
agents to plan privately, which boosts adversarial diversity and reduces
over-refusals. Our results motivate a shift from reactive patching to proactive
co-evolution in LM safety training, enabling scalable, autonomous, and robust
self-improvement of LMs via multi-agent reinforcement learning (MARL).

</details>


### [439] [Beyond the Norm: A Survey of Synthetic Data Generation for Rare Events](https://arxiv.org/abs/2506.06380)
*Jingyi Gu,Xuan Zhang,Guiling Wang*

Main category: cs.LG

TL;DR: 本文综述了极端事件合成数据生成的首次概述，涵盖了生成模型技术、评估框架及应用领域，并指出了未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 极端事件（如市场崩溃、自然灾害等）罕见但破坏性大，数据稀缺，合成数据生成成为解决方案，但现有研究未针对极端事件的独特需求。

Method: 系统回顾生成模型技术和大语言模型，提出评估框架（统计、依赖、视觉和任务导向指标），并分析其适用性。

Result: 总结了基准数据集，提供了模型评估的实用指南，并分类了关键应用领域及未充分探索的方向。

Conclusion: 为极端事件合成数据研究提供了结构化基础，并指出了未来挑战。

Abstract: Extreme events, such as market crashes, natural disasters, and pandemics, are
rare but catastrophic, often triggering cascading failures across
interconnected systems. Accurate prediction and early warning can help minimize
losses and improve preparedness. While data-driven methods offer powerful
capabilities for extreme event modeling, they require abundant training data,
yet extreme event data is inherently scarce, creating a fundamental challenge.
Synthetic data generation has emerged as a powerful solution. However, existing
surveys focus on general data with privacy preservation emphasis, rather than
extreme events' unique performance requirements. This survey provides the first
overview of synthetic data generation for extreme events. We systematically
review generative modeling techniques and large language models, particularly
those enhanced by statistical theory as well as specialized training and
sampling mechanisms to capture heavy-tailed distributions. We summarize
benchmark datasets and introduce a tailored evaluation framework covering
statistical, dependence, visual, and task-oriented metrics. A central
contribution is our in-depth analysis of each metric's applicability in
extremeness and domain-specific adaptations, providing actionable guidance for
model evaluation in extreme settings. We categorize key application domains and
identify underexplored areas like behavioral finance, wildfires, earthquakes,
windstorms, and infectious outbreaks. Finally, we outline open challenges,
providing a structured foundation for advancing synthetic rare-event research.

</details>


### [440] [Theoretical Analysis of Positional Encodings in Transformer Models: Impact on Expressiveness and Generalization](https://arxiv.org/abs/2506.06398)
*Yin Li*

Main category: cs.LG

TL;DR: 本文提出了一个理论框架，分析不同位置编码方法对Transformer模型表达能力、泛化能力和长序列外推能力的影响，并提出基于正交函数的新编码方法。


<details>
  <summary>Details</summary>
Motivation: 研究位置编码方法对Transformer模型性能的影响，填补理论空白，为自然语言处理、计算机视觉等领域的应用提供设计指导。

Method: 通过函数逼近定义表达能力，利用Rademacher复杂度建立泛化界限，提出基于正交函数（如小波和Legendre多项式）的新编码方法。

Result: 实验表明，基于正交变换的编码方法在泛化和外推能力上优于传统的正弦编码方法。

Conclusion: 本文为Transformer理论提供了重要补充，为实际应用中的编码方法选择提供了理论依据。

Abstract: Positional encodings are a core part of transformer-based models, enabling
processing of sequential data without recurrence. This paper presents a
theoretical framework to analyze how various positional encoding methods,
including sinusoidal, learned, relative, and bias-based methods like Attention
with Linear Biases (ALiBi), impact a transformer's expressiveness,
generalization ability, and extrapolation to longer sequences. Expressiveness
is defined via function approximation, generalization bounds are established
using Rademacher complexity, and new encoding methods based on orthogonal
functions, such as wavelets and Legendre polynomials, are proposed. The
extrapolation capacity of existing and proposed encodings is analyzed,
extending ALiBi's biasing approach to a unified theoretical context.
Experimental evaluation on synthetic sequence-to-sequence tasks shows that
orthogonal transform-based encodings outperform traditional sinusoidal
encodings in generalization and extrapolation. This work addresses a critical
gap in transformer theory, providing insights for design choices in natural
language processing, computer vision, and other transformer applications.

</details>


### [441] [CoxNTF: A New Approach for Joint Clustering and Prediction in Survival Analysis](https://arxiv.org/abs/2506.06411)
*Paul Fogel,Christophe Geissler,George Luta*

Main category: cs.LG

TL;DR: 提出了一种结合生存信息的非负张量分解方法CoxNTF，用于生存分析中的潜在因子表示，性能接近Coxnet但更结构化且可解释。


<details>
  <summary>Details</summary>
Motivation: 现有方法（如NMF）未结合生存信息，限制了预测能力。

Method: 使用非负张量分解（NTF）构建加权协变量张量，以Coxnet模型的生存概率指导张量化过程。

Result: CoxNTF预测性能与原始协变量的Coxnet相当，同时提供结构化且可解释的聚类框架，并能有效处理特征冗余。

Conclusion: CoxNTF是一种强大的生存分析工具，适用于联合聚类和预测。

Abstract: The interpretation of the results of survival analysis often benefits from
latent factor representations of baseline covariates. However, existing
methods, such as Nonnegative Matrix Factorization (NMF), do not incorporate
survival information, limiting their predictive power. We present CoxNTF, a
novel approach that uses non-negative tensor factorization (NTF) to derive
meaningful latent representations that are closely associated with survival
outcomes. CoxNTF constructs a weighted covariate tensor in which survival
probabilities derived from the Coxnet model are used to guide the tensorization
process. Our results show that CoxNTF achieves survival prediction performance
comparable to using Coxnet with the original covariates, while providing a
structured and interpretable clustering framework. In addition, the new
approach effectively handles feature redundancy, making it a powerful tool for
joint clustering and prediction in survival analysis.

</details>


### [442] [NeurNCD: Novel Class Discovery via Implicit Neural Representation](https://arxiv.org/abs/2506.06412)
*Junming Wang,Yi Shi*

Main category: cs.LG

TL;DR: NeurNCD是一种基于Embedding-NeRF和KL散度的新型类别发现框架，解决了传统显式表示的局限性，无需密集标注即可在开放和封闭世界场景中实现优越的分割性能。


<details>
  <summary>Details</summary>
Motivation: 传统显式表示（如3D分割图）存在离散、易产生空洞和噪声的问题，限制了新类别的发现能力。

Method: 采用Embedding-NeRF模型结合KL散度，替代传统3D分割图，并整合特征查询、调制和聚类等关键组件。

Result: 在NYUv2和Replica数据集上显著优于现有方法。

Conclusion: NeurNCD是一种高效且通用的新类别发现框架，适用于开放和封闭世界场景。

Abstract: Discovering novel classes in open-world settings is crucial for real-world
applications. Traditional explicit representations, such as object descriptors
or 3D segmentation maps, are constrained by their discrete, hole-prone, and
noisy nature, which hinders accurate novel class discovery. To address these
challenges, we introduce NeurNCD, the first versatile and data-efficient
framework for novel class discovery that employs the meticulously designed
Embedding-NeRF model combined with KL divergence as a substitute for
traditional explicit 3D segmentation maps to aggregate semantic embedding and
entropy in visual embedding space. NeurNCD also integrates several key
components, including feature query, feature modulation and clustering,
facilitating efficient feature augmentation and information exchange between
the pre-trained semantic segmentation network and implicit neural
representations. As a result, our framework achieves superior segmentation
performance in both open and closed-world settings without relying on densely
labelled datasets for supervised training or human interaction to generate
sparse label supervision. Extensive experiments demonstrate that our method
significantly outperforms state-of-the-art approaches on the NYUv2 and Replica
datasets.

</details>


### [443] [CAtCh: Cognitive Assessment through Cookie Thief](https://arxiv.org/abs/2506.06603)
*Joseph T Colonel,Carolyn Hagler,Guiselle Wismer,Laura Curtis,Jacqueline Becker,Juan Wisnivesky,Alex Federman,Gaurav Pandey*

Main category: cs.LG

TL;DR: 该论文评估了多种基于语音的开源方法，用于从患者录音中预测认知障碍（CI），发现多模态方法优于单模态方法，声学特征优于语言学特征。


<details>
  <summary>Details</summary>
Motivation: 现有机器学习算法主要用于预测阿尔茨海默病及相关痴呆（ADRD），但未扩展到更广泛的认知障碍（CI）预测，而CI可能是ADRD的前兆和风险因素。

Method: 评估了基于语音的开源方法（原用于ADRD预测）和多模态情感分析方法，用于从患者录音中预测CI。

Result: 多模态方法优于单模态方法，声学特征（尤其是与情感和韵律相关的可解释特征）显著优于基于BERT的语言学特征和可解释语言学特征。

Conclusion: 声学特征在预测CI方面表现更优，多模态方法具有潜力，相关代码已开源。

Abstract: Several machine learning algorithms have been developed for the prediction of
Alzheimer's disease and related dementia (ADRD) from spontaneous speech.
However, none of these algorithms have been translated for the prediction of
broader cognitive impairment (CI), which in some cases is a precursor and risk
factor of ADRD. In this paper, we evaluated several speech-based open-source
methods originally proposed for the prediction of ADRD, as well as methods from
multimodal sentiment analysis for the task of predicting CI from patient audio
recordings. Results demonstrated that multimodal methods outperformed unimodal
ones for CI prediction, and that acoustics-based approaches performed better
than linguistics-based ones. Specifically, interpretable acoustic features
relating to affect and prosody were found to significantly outperform
BERT-based linguistic features and interpretable linguistic features,
respectively. All the code developed for this study is available at
https://github.com/JTColonel/catch.

</details>


### [444] [Unlocking Chemical Insights: Superior Molecular Representations from Intermediate Encoder Layers](https://arxiv.org/abs/2506.06443)
*Luis Pinto*

Main category: cs.LG

TL;DR: 研究发现，分子编码器的中间层嵌入比最终层表现更好，固定嵌入平均提升5.4%，微调后提升8.5%，并提出了高效评估-微调方法。


<details>
  <summary>Details</summary>
Motivation: 传统方法仅依赖最终层嵌入可能丢失有价值信息，研究旨在探索分子编码器各层的表现潜力。

Method: 对五种分子编码器进行分层分析，测试22种ADMET任务，比较固定嵌入和微调效果。

Result: 中间层嵌入表现更优，固定嵌入平均提升5.4%，微调后提升8.5%，部分任务提升达40.8%。

Conclusion: 充分利用分子编码器的各层嵌入可显著提升性能，同时提出高效评估-微调策略。

Abstract: Pretrained molecular encoders have become indispensable in computational
chemistry for tasks such as property prediction and molecular generation.
However, the standard practice of relying solely on final-layer embeddings for
downstream tasks may discard valuable information. In this work, we challenge
this convention by conducting a comprehensive layer-wise analysis of five
diverse molecular encoders across 22 ADMET property prediction tasks. Our
results demonstrate that embeddings from intermediate layers consistently
outperform final-layer representations. Specifically, using fixed embeddings
from the optimal intermediate layers improved downstream performance by an
average of 5.4%, reaching gains up to 28.6%. Furthermore, finetuning up to
these intermediate layers yielded even greater average improvements of 8.5%,
with performance increases as high as 40.8%, achieving new state-of-the-art
results on several benchmarks. Additionally, a strong positive correlation
between fixed embedding performance and finetuning outcomes supports an
efficient evaluate-then-finetune approach, enabling identification of optimal
layers with reduced computational cost. These findings highlight the importance
of exploring the full representational depth of molecular encoders to achieve
substantial performance improvements and computational efficiency. The code is
made publicly available at
https://github.com/luispintoc/Unlocking-Chemical-Insights.

</details>


### [445] [Saffron-1: Towards an Inference Scaling Paradigm for LLM Safety Assurance](https://arxiv.org/abs/2506.06444)
*Ruizhong Qiu,Gaotang Li,Tianxin Wei,Jingrui He,Hanghang Tong*

Main category: cs.LG

TL;DR: 论文提出了一种新的推理扩展方法SAFFRON，用于增强LLM的安全性，解决了传统方法在安全场景下的低效问题，并公开了相关模型和数据集。


<details>
  <summary>Details</summary>
Motivation: 现有安全保证研究主要关注训练阶段的调整，但易受攻击；同时推理扩展在安全领域的潜力未被探索。

Method: 提出SAFFRON方法，包括多分支奖励模型（MRM）、部分监督训练目标、保守探索约束和Trie缓存策略。

Result: 实验验证了SAFFRON的有效性，并公开了模型Saffron-1和数据集Safety4M。

Conclusion: SAFFRON为LLM安全提供了高效解决方案，推动了未来研究。

Abstract: Existing safety assurance research has primarily focused on training-phase
alignment to instill safe behaviors into LLMs. However, recent studies have
exposed these methods' susceptibility to diverse jailbreak attacks.
Concurrently, inference scaling has significantly advanced LLM reasoning
capabilities but remains unexplored in the context of safety assurance.
Addressing this gap, our work pioneers inference scaling for robust and
effective LLM safety against emerging threats. We reveal that conventional
inference scaling techniques, despite their success in reasoning tasks, perform
poorly in safety contexts, even falling short of basic approaches like
Best-of-N Sampling. We attribute this inefficiency to a newly identified
challenge, the exploration--efficiency dilemma, arising from the high
computational overhead associated with frequent process reward model (PRM)
evaluations. To overcome this dilemma, we propose SAFFRON, a novel inference
scaling paradigm tailored explicitly for safety assurance. Central to our
approach is the introduction of a multifurcation reward model (MRM) that
significantly reduces the required number of reward model evaluations. To
operationalize this paradigm, we further propose: (i) a partial supervision
training objective for MRM, (ii) a conservative exploration constraint to
prevent out-of-distribution explorations, and (iii) a Trie-based key--value
caching strategy that facilitates cache sharing across sequences during tree
search. Extensive experiments validate the effectiveness of our method.
Additionally, we publicly release our trained multifurcation reward model
(Saffron-1) and the accompanying token-level safety reward dataset (Safety4M)
to accelerate future research in LLM safety. Our code, model, and data are
publicly available at https://github.com/q-rz/saffron , and our project
homepage is at https://q-rz.github.io/p/saffron .

</details>


### [446] [LETS Forecast: Learning Embedology for Time Series Forecasting](https://arxiv.org/abs/2506.06454)
*Abrar Majeedi,Viswanatha Reddy Gajjala,Satya Sai Srinath Namburi GNVV,Nada Magdi Elkordi,Yin Li*

Main category: cs.LG

TL;DR: DeepEDM结合非线性动力学建模与深度学习，通过时间延迟嵌入学习潜在空间，利用核回归逼近动态，实现高精度时间序列预测。


<details>
  <summary>Details</summary>
Motivation: 现实世界时间序列通常具有复杂非线性动态，现有深度学习方法未明确建模这些动态，需填补这一空白。

Method: 基于Takens定理和EDM，提出DeepEDM框架，结合时间延迟嵌入、核回归及软注意力机制。

Result: 在合成和真实数据实验中，DeepEDM对输入噪声鲁棒，预测精度优于现有方法。

Conclusion: DeepEDM有效整合动力学建模与深度学习，显著提升时间序列预测性能。

Abstract: Real-world time series are often governed by complex nonlinear dynamics.
Understanding these underlying dynamics is crucial for precise future
prediction. While deep learning has achieved major success in time series
forecasting, many existing approaches do not explicitly model the dynamics. To
bridge this gap, we introduce DeepEDM, a framework that integrates nonlinear
dynamical systems modeling with deep neural networks. Inspired by empirical
dynamic modeling (EDM) and rooted in Takens' theorem, DeepEDM presents a novel
deep model that learns a latent space from time-delayed embeddings, and employs
kernel regression to approximate the underlying dynamics, while leveraging
efficient implementation of softmax attention and allowing for accurate
prediction of future time steps. To evaluate our method, we conduct
comprehensive experiments on synthetic data of nonlinear dynamical systems as
well as real-world time series across domains. Our results show that DeepEDM is
robust to input noise, and outperforms state-of-the-art methods in forecasting
accuracy. Our code is available at: https://abrarmajeedi.github.io/deep_edm.

</details>


### [447] [WISCA: A Consensus-Based Approach to Harmonizing Interpretability in Tabular Datasets](https://arxiv.org/abs/2506.06455)
*Antonio Jesús Banegas-Luna,Horacio Pérez-Sánchez,Carlos Martínez-Cortés*

Main category: cs.LG

TL;DR: 论文提出了一种新的共识方法WISCA，用于整合机器学习模型的可解释性结果，以提高解释的可靠性。


<details>
  <summary>Details</summary>
Motivation: 在科学和高风险领域，机器学习模型的可解释性至关重要，但现有方法常产生冲突解释，需要共识策略来协调结果。

Method: 训练了六种ML模型，使用多种模型无关的可解释性技术，并提出了WISCA方法，结合类别概率和归一化属性生成共识解释。

Result: WISCA与最可靠的个体方法一致，证明了共识策略在提高解释可靠性方面的价值。

Conclusion: WISCA是一种有效的共识方法，能够提升机器学习模型解释的可信度。

Abstract: While predictive accuracy is often prioritized in machine learning (ML)
models, interpretability remains essential in scientific and high-stakes
domains. However, diverse interpretability algorithms frequently yield
conflicting explanations, highlighting the need for consensus to harmonize
results. In this study, six ML models were trained on six synthetic datasets
with known ground truths, utilizing various model-agnostic interpretability
techniques. Consensus explanations were generated using established methods and
a novel approach: WISCA (Weighted Scaled Consensus Attributions), which
integrates class probability and normalized attributions. WISCA consistently
aligned with the most reliable individual method, underscoring the value of
robust consensus strategies in improving explanation reliability.

</details>


### [448] [Towards Infant Sleep-Optimized Driving: Synergizing Wearable and Vehicle Sensing in Intelligent Cruise Control](https://arxiv.org/abs/2506.06459)
*Ruitao Chen,Mozhang Guo,Jinge Li*

Main category: cs.LG

TL;DR: 本文提出了一种结合强化学习（RL）的智能巡航控制框架，通过整合可穿戴传感器和车辆数据，优化驾驶行为以提升婴儿睡眠质量。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶（AD）虽提高了安全性和舒适性，但对婴儿睡眠的影响尚未充分研究。急加速、急刹车等行为可能干扰婴儿睡眠，影响乘客舒适度和父母便利性。

Method: 结合LSTM和Transformer神经网络与RL，建模驾驶行为与婴儿睡眠质量的关系，动态计算最佳驾驶激进程度，并转化为具体AD控制策略。

Result: 仿真结果表明，相比基线方法，该方案显著提升了婴儿睡眠质量，同时保持了理想的出行效率。

Conclusion: 该研究为自动驾驶中乘客舒适性优化提供了新思路，尤其在婴儿睡眠方面具有实际应用价值。

Abstract: Automated driving (AD) has substantially improved vehicle safety and driving
comfort, but their impact on passenger well-being, particularly infant sleep,
is not sufficiently studied. Sudden acceleration, abrupt braking, and sharp
maneuvers can disrupt infant sleep, compromising both passenger comfort and
parental convenience. To solve this problem, this paper explores the
integration of reinforcement learning (RL) within AD to personalize driving
behavior and optimally balance occupant comfort and travel efficiency. In
particular, we propose an intelligent cruise control framework that adapts to
varying driving conditions to enhance infant sleep quality by effectively
synergizing wearable sensing and vehicle data. Long short-term memory (LSTM)
and transformer-based neural networks are integrated with RL to model the
relationship between driving behavior and infant sleep quality under diverse
traffic and road conditions. Based on the sleep quality indicators from the
wearable sensors, driving action data from vehicle controllers, and map data
from map applications, the model dynamically computes the optimal driving
aggressiveness level, which is subsequently translated into specific AD control
strategies, e.g., the magnitude and frequency of acceleration, lane change, and
overtaking. Simulation results demonstrate that the proposed solution
significantly improves infant sleep quality compared to baseline methods, while
preserving desirable travel efficiency.

</details>


### [449] [E-BATS: Efficient Backpropagation-Free Test-Time Adaptation for Speech Foundation Models](https://arxiv.org/abs/2506.07078)
*Jiaheng Dong,Hong Jia,Soumyajit Chatterjee,Abhirup Ghosh,James Bailey,Ting Dang*

Main category: cs.LG

TL;DR: 论文提出E-BATS，一种高效的无反向传播测试时适应框架，用于解决语音基础模型在声学域偏移下的性能下降问题。


<details>
  <summary>Details</summary>
Motivation: 语音基础模型在真实场景中（如背景噪声和口音）性能显著下降，现有测试时适应方法要么内存消耗大，要么精度低。

Method: E-BATS通过轻量级提示适应、多尺度损失和测试时指数移动平均机制实现高效适应。

Result: 在四个噪声语音数据集上，E-BATS比无反向传播基线提升4.1%-13.5%准确率，比基于反向传播方法节省2.0-6.4倍GPU内存。

Conclusion: E-BATS为实际语音处理系统在声学变化下的高效适应提供了新方向。

Abstract: Speech Foundation Models encounter significant performance degradation when
deployed in real-world scenarios involving acoustic domain shifts, such as
background noise and speaker accents. Test-time adaptation (TTA) has recently
emerged as a viable strategy to address such domain shifts at inference time
without requiring access to source data or labels. However, existing TTA
approaches, particularly those relying on backpropagation, are
memory-intensive, limiting their applicability in speech tasks and
resource-constrained settings. Although backpropagation-free methods offer
improved efficiency, existing ones exhibit poor accuracy. This is because they
are predominantly developed for vision tasks, which fundamentally differ from
speech task formulations, noise characteristics, and model architecture, posing
unique transferability challenges. In this paper, we introduce E-BATS, the
first Efficient BAckpropagation-free TTA framework designed explicitly for
speech foundation models. E-BATS achieves a balance between adaptation
effectiveness and memory efficiency through three key components: (i)
lightweight prompt adaptation for a forward-pass-based feature alignment, (ii)
a multi-scale loss to capture both global (utterance-level) and local
distribution shifts (token-level) and (iii) a test-time exponential moving
average mechanism for stable adaptation across utterances. Experiments
conducted on four noisy speech datasets spanning sixteen acoustic conditions
demonstrate consistent improvements, with 4.1%-13.5% accuracy gains over
backpropagation-free baselines and 2.0-6.4 times GPU memory savings compared to
backpropagation-based methods. By enabling scalable and robust adaptation under
acoustic variability, this work paves the way for developing more efficient
adaptation approaches for practical speech processing systems in real-world
environments.

</details>


### [450] [TimeRecipe: A Time-Series Forecasting Recipe via Benchmarking Module Level Effectiveness](https://arxiv.org/abs/2506.06482)
*Zhiyuan Zhao,Juntong Ni,Shangqing Xu,Haoxin Liu,Wei Jin,B. Aditya Prakash*

Main category: cs.LG

TL;DR: TimeRecipe是一个统一的时间序列预测基准框架，通过模块级评估揭示设计选择的有效性。


<details>
  <summary>Details</summary>
Motivation: 解决现有基准对模型设计组件效果评估不足的问题。

Method: 进行超过10,000次实验，评估不同组件在多样化数据集和任务设置中的表现。

Result: 发现深入探索设计空间可以超越现有最优方法，并揭示设计选择与预测场景的关联。

Conclusion: TimeRecipe提供了实用的工具包，基于实证推荐模型架构。

Abstract: Time-series forecasting is an essential task with wide real-world
applications across domains. While recent advances in deep learning have
enabled time-series forecasting models with accurate predictions, there remains
considerable debate over which architectures and design components, such as
series decomposition or normalization, are most effective under varying
conditions. Existing benchmarks primarily evaluate models at a high level,
offering limited insight into why certain designs work better. To mitigate this
gap, we propose TimeRecipe, a unified benchmarking framework that
systematically evaluates time-series forecasting methods at the module level.
TimeRecipe conducts over 10,000 experiments to assess the effectiveness of
individual components across a diverse range of datasets, forecasting horizons,
and task settings. Our results reveal that exhaustive exploration of the design
space can yield models that outperform existing state-of-the-art methods and
uncover meaningful intuitions linking specific design choices to forecasting
scenarios. Furthermore, we release a practical toolkit within TimeRecipe that
recommends suitable model architectures based on these empirical insights. The
benchmark is available at: https://github.com/AdityaLab/TimeRecipe.

</details>


### [451] [A Certified Unlearning Approach without Access to Source Data](https://arxiv.org/abs/2506.06486)
*Umit Yigit Basaran,Sk Miraj Ahmed,Amit Roy-Chowdhury,Basak Guler*

Main category: cs.LG

TL;DR: 提出了一种无需原始训练数据的认证遗忘框架，通过替代数据集和噪声校准实现数据删除。


<details>
  <summary>Details</summary>
Motivation: 随着数据隐私法规的普及，从训练模型中删除私有或受版权保护信息的需求日益重要，但传统方法依赖完整训练数据，不适用于数据不可用场景。

Method: 利用替代数据集近似源数据的统计特性，通过控制噪声缩放实现数据删除，并引入噪声校准技术。

Result: 理论和实验验证表明，该方法在隐私敏感场景中有效且可靠，同时保持模型实用性。

Conclusion: 该框架为数据删除提供了强保证，适用于实际隐私保护需求。

Abstract: With the growing adoption of data privacy regulations, the ability to erase
private or copyrighted information from trained models has become a crucial
requirement. Traditional unlearning methods often assume access to the complete
training dataset, which is unrealistic in scenarios where the source data is no
longer available. To address this challenge, we propose a certified unlearning
framework that enables effective data removal \final{without access to the
original training data samples}. Our approach utilizes a surrogate dataset that
approximates the statistical properties of the source data, allowing for
controlled noise scaling based on the statistical distance between the two.
\updated{While our theoretical guarantees assume knowledge of the exact
statistical distance, practical implementations typically approximate this
distance, resulting in potentially weaker but still meaningful privacy
guarantees.} This ensures strong guarantees on the model's behavior
post-unlearning while maintaining its overall utility. We establish theoretical
bounds, introduce practical noise calibration techniques, and validate our
method through extensive experiments on both synthetic and real-world datasets.
The results demonstrate the effectiveness and reliability of our approach in
privacy-sensitive settings.

</details>


### [452] [Membership Inference Attacks for Unseen Classes](https://arxiv.org/abs/2506.06488)
*Pratiksha Thaker,Neil Kale,Zhiwei Steven Wu,Virginia Smith*

Main category: cs.LG

TL;DR: 论文研究了在无法访问完整子类数据的情况下，成员推理攻击的性能，发现影子模型攻击表现极差，而分位数回归攻击表现更优。


<details>
  <summary>Details</summary>
Motivation: 研究在更极端但现实的分布偏移情况下，成员推理攻击的有效性，尤其是影子模型攻击的局限性。

Method: 比较影子模型攻击和分位数回归攻击在类丢失设置下的性能，并通过理论模型分析其潜力与限制。

Result: 分位数回归攻击在未见类上表现显著优于影子模型攻击（如CIFAR-100上TPR提高11倍），且在ImageNet上即使90%训练类被移除仍有效。

Conclusion: 分位数回归攻击在类丢失情况下更具优势，为成员推理攻击提供了新方向。

Abstract: Shadow model attacks are the state-of-the-art approach for membership
inference attacks on machine learning models. However, these attacks typically
assume an adversary has access to a background (nonmember) data distribution
that matches the distribution the target model was trained on. We initiate a
study of membership inference attacks where the adversary or auditor cannot
access an entire subclass from the distribution -- a more extreme but realistic
version of distribution shift than has been studied previously. In this
setting, we first show that the performance of shadow model attacks degrades
catastrophically, and then demonstrate the promise of another approach,
quantile regression, that does not have the same limitations. We show that
quantile regression attacks consistently outperform shadow model attacks in the
class dropout setting -- for example, quantile regression attacks achieve up to
11$\times$ the TPR of shadow models on the unseen class on CIFAR-100, and
achieve nontrivial TPR on ImageNet even with 90% of training classes removed.
We also provide a theoretical model that illustrates the potential and
limitations of this approach.

</details>


### [453] [Alternating Gradient Flows: A Theory of Feature Learning in Two-layer Neural Networks](https://arxiv.org/abs/2506.06489)
*Daniel Kunin,Giovanni Luca Marchetti,Feng Chen,Dhruva Karkada,James B. Simon,Michael R. DeWeese,Surya Ganguli,Nina Miolane*

Main category: cs.LG

TL;DR: 本文提出交替梯度流（AGF）框架，用于描述小初始化下两层神经网络的特征学习动态，揭示了神经元激活与损失下降的规律。


<details>
  <summary>Details</summary>
Motivation: 研究神经网络学习特征的过程及其机制仍是一个开放问题，本文旨在通过AGF框架揭示这一动态过程。

Method: AGF将梯度流行为建模为交替的两步过程：休眠神经元最大化效用函数，活跃神经元最小化成本函数。

Result: AGF量化了特征学习的顺序、时间和幅度，并在多种架构中验证了实验结果，统一了现有分析方法。

Conclusion: AGF为理解神经网络特征学习提供了新视角，尤其是在小初始化条件下，展示了其广泛适用性。

Abstract: What features neural networks learn, and how, remains an open question. In
this paper, we introduce Alternating Gradient Flows (AGF), an algorithmic
framework that describes the dynamics of feature learning in two-layer networks
trained from small initialization. Prior works have shown that gradient flow in
this regime exhibits a staircase-like loss curve, alternating between plateaus
where neurons slowly align to useful directions and sharp drops where neurons
rapidly grow in norm. AGF approximates this behavior as an alternating two-step
process: maximizing a utility function over dormant neurons and minimizing a
cost function over active ones. AGF begins with all neurons dormant. At each
round, a dormant neuron activates, triggering the acquisition of a feature and
a drop in the loss. AGF quantifies the order, timing, and magnitude of these
drops, matching experiments across architectures. We show that AGF unifies and
extends existing saddle-to-saddle analyses in fully connected linear networks
and attention-only linear transformers, where the learned features are singular
modes and principal components, respectively. In diagonal linear networks, we
prove AGF converges to gradient flow in the limit of vanishing initialization.
Applying AGF to quadratic networks trained to perform modular addition, we give
the first complete characterization of the training dynamics, revealing that
networks learn Fourier features in decreasing order of coefficient magnitude.
Altogether, AGF offers a promising step towards understanding feature learning
in neural networks.

</details>


### [454] [Investigating the Relationship Between Physical Activity and Tailored Behavior Change Messaging: Connecting Contextual Bandit with Large Language Models](https://arxiv.org/abs/2506.07275)
*Haochen Song,Dominik Hofer,Rania Islambouli,Laura Hawkins,Ananya Bhattacharjee,Meredith Franklin,Joseph Jay Williams*

Main category: cs.LG

TL;DR: 论文提出了一种结合cMAB和LLM的混合方法，用于个性化干预以减少久坐行为，并通过实验评估了四种干预模型的效果。


<details>
  <summary>Details</summary>
Motivation: 传统cMAB算法需要大样本且可能忽略心理因素，因此研究探索结合LLM以提升个性化干预效果。

Method: 结合cMAB选择干预类型和LLM个性化消息内容，评估四种干预模型（cMAB、LLM、cMABxLLM、RCT）对步数和消息接受度的影响。

Result: 研究通过因果推断框架评估了各模型的效果，揭示了LLM和cMAB在促进体育活动中的互补作用。

Conclusion: 混合方法（cMABxLLM）在个性化行为干预中表现出潜力，为未来研究提供了新方向。

Abstract: Machine learning approaches, such as contextual multi-armed bandit (cMAB)
algorithms, offer a promising strategy to reduce sedentary behavior by
delivering personalized interventions to encourage physical activity. However,
cMAB algorithms typically require large participant samples to learn
effectively and may overlook key psychological factors that are not explicitly
encoded in the model. In this study, we propose a hybrid approach that combines
cMAB for selecting intervention types with large language models (LLMs) to
personalize message content. We evaluate four intervention types: behavioral
self-monitoring, gain-framed, loss-framed, and social comparison, each
delivered as a motivational message aimed at increasing motivation for physical
activity and daily step count. Message content is further personalized using
dynamic contextual factors including daily fluctuations in self-efficacy,
social influence, and regulatory focus. Over a seven-day trial, participants
receive daily messages assigned by one of four models: cMAB alone, LLM alone,
combined cMAB with LLM personalization (cMABxLLM), or equal randomization
(RCT). Outcomes include daily step count and message acceptance, assessed via
ecological momentary assessments (EMAs). We apply a causal inference framework
to evaluate the effects of each model. Our findings offer new insights into the
complementary roles of LLM-based personalization and cMAB adaptation in
promoting physical activity through personalized behavioral messaging.

</details>


### [455] [Synthetic Problem Generation for Reasoning via Quality-Diversity Algorithms](https://arxiv.org/abs/2506.06499)
*Alex Havrilla,Edward Hughes,Mikayel Samvelyan,Jacob Abernethy*

Main category: cs.LG

TL;DR: SPARQ利用质量-多样性算法生成高质量、多样化的数学问题及解决方案，通过单一模型提升推理能力，生成2000万对问题-解决方案，并证明过滤难度和多样性可显著提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法在复杂和多样化问题领域的扩展性有限，SPARQ旨在通过合成数据生成解决这一问题。

Method: 使用质量-多样性算法生成问题-解决方案对，通过解决率衡量问题难度，并过滤数据以优化模型性能。

Result: 生成2000万对数据，模型性能提升24%，研究发现数据质量和多样性对泛化能力有显著影响。

Conclusion: SPARQ为合成数据生成提供了可扩展的方法，数据质量和多样性对模型性能至关重要。

Abstract: Large language model (LLM) driven synthetic data generation has emerged as a
powerful method for improving model reasoning capabilities. However, most
methods either distill large state-of-the-art models into small students or use
natural ground-truth problem statements to guarantee problem statement quality.
This limits the scalability of these approaches to more complex and diverse
problem domains. To address this, we present SPARQ: Synthetic Problem
Generation for Reasoning via Quality-Diversity Algorithms, a novel approach for
generating high-quality and diverse synthetic math problem and solution pairs
using only a single model by measuring a problem's solve-rate: a proxy for
problem difficulty. Starting from a seed dataset of 7.5K samples, we generate
over 20 million new problem-solution pairs. We show that filtering the
generated data by difficulty and then fine-tuning the same model on the
resulting data improves relative model performance by up to 24\%. Additionally,
we conduct ablations studying the impact of synthetic data quantity, quality
and diversity on model generalization. We find that higher quality, as measured
by problem difficulty, facilitates better in-distribution performance. Further,
while generating diverse synthetic data does not as strongly benefit
in-distribution performance, filtering for more diverse data facilitates more
robust OOD generalization. We also confirm the existence of model and data
scaling laws for synthetically generated problems, which positively benefit
downstream model generalization.

</details>


### [456] [Optimal Rates in Continual Linear Regression via Increasing Regularization](https://arxiv.org/abs/2506.06501)
*Ran Levinstein,Amit Attia,Matan Schliserman,Uri Sherman,Tomer Koren,Daniel Soudry,Itay Evron*

Main category: cs.LG

TL;DR: 论文研究了随机任务顺序下的可实现持续线性回归，通过两种正则化方法缩小了理论上的性能差距，并提出了最优收敛速率的调度策略。


<details>
  <summary>Details</summary>
Motivation: 持续学习中，随机任务顺序下的最坏情况损失存在理论下限，但现有方法的性能上限与之差距显著。研究旨在通过正则化方法缩小或消除这一差距。

Method: 采用两种正则化方案：(1)显式各向同性ℓ2正则化，(2)通过有限步预算的隐式正则化，并将其转化为对替代损失的随机梯度下降（SGD）。

Result: 固定正则化强度可实现接近最优的收敛速率O(logk/k)，而动态调整正则化强度的调度策略则达到最优速率O(1/k)。

Conclusion: 研究表明，增加正则化系数或减少每任务步数的调度策略在持续学习中具有优势。

Abstract: We study realizable continual linear regression under random task orderings,
a common setting for developing continual learning theory. In this setup, the
worst-case expected loss after $k$ learning iterations admits a lower bound of
$\Omega(1/k)$. However, prior work using an unregularized scheme has only
established an upper bound of $O(1/k^{1/4})$, leaving a significant gap. Our
paper proves that this gap can be narrowed, or even closed, using two
frequently used regularization schemes: (1) explicit isotropic $\ell_2$
regularization, and (2) implicit regularization via finite step budgets. We
show that these approaches, which are used in practice to mitigate forgetting,
reduce to stochastic gradient descent (SGD) on carefully defined surrogate
losses. Through this lens, we identify a fixed regularization strength that
yields a near-optimal rate of $O(\log k / k)$. Moreover, formalizing and
analyzing a generalized variant of SGD for time-varying functions, we derive an
increasing regularization strength schedule that provably achieves an optimal
rate of $O(1/k)$. This suggests that schedules that increase the regularization
coefficient or decrease the number of steps per task are beneficial, at least
in the worst case.

</details>


### [457] [InstantFT: An FPGA-Based Runtime Subsecond Fine-tuning of CNN Models](https://arxiv.org/abs/2506.06505)
*Keisuke Sugiura,Hiroki Matsutani*

Main category: cs.LG

TL;DR: InstantFT是一种基于FPGA的超快速CNN微调方法，显著提升了IoT设备上的计算效率和能源效率。


<details>
  <summary>Details</summary>
Motivation: 解决深度神经网络（DNN）在资源受限的IoT平台上运行时计算和内存需求高的问题。

Method: 通过优化参数高效微调（PEFT）中的前向和反向计算，实现快速微调。

Result: 实验表明，InstantFT比现有的LoRA方法快17.4倍，微调时间仅0.36秒，能源效率提升16.3倍。

Conclusion: InstantFT能够高效地在非平稳数据分布下实现CNN的实时微调。

Abstract: Training deep neural networks (DNNs) requires significantly more computation
and memory than inference, making runtime adaptation of DNNs challenging on
resource-limited IoT platforms. We propose InstantFT, an FPGA-based method for
ultra-fast CNN fine-tuning on IoT devices, by optimizing the forward and
backward computations in parameter-efficient fine-tuning (PEFT). Experiments on
datasets with concept drift demonstrate that InstantFT fine-tunes a pre-trained
CNN 17.4x faster than existing Low-Rank Adaptation (LoRA)-based approaches,
while achieving comparable accuracy. Our FPGA-based InstantFT reduces the
fine-tuning time to just 0.36s and improves energy-efficiency by 16.3x,
enabling on-the-fly adaptation of CNNs to non-stationary data distributions.

</details>


### [458] [Sharp Gap-Dependent Variance-Aware Regret Bounds for Tabular MDPs](https://arxiv.org/abs/2506.06521)
*Shulun Chen,Runlong Zhou,Zihan Zhang,Maryam Fazel,Simon S. Du*

Main category: cs.LG

TL;DR: MVP算法在分幕MDP中实现了方差感知的间隙依赖遗憾界，并证明了其下界。


<details>
  <summary>Details</summary>
Motivation: 研究如何在分幕MDP中实现更精确的遗憾界，特别是考虑方差和间隙的影响。

Method: 使用Monotonic Value Propagation (MVP)算法，并通过加权子最优间隙的新颖分析。

Result: 得到了方差感知的遗憾界，并证明了其下界，表明对条件方差的依赖是必要的。

Conclusion: MVP算法的分析为其他算法提供了潜在适应性，并揭示了方差在遗憾界中的重要性。

Abstract: We consider the gap-dependent regret bounds for episodic MDPs. We show that
the Monotonic Value Propagation (MVP) algorithm achieves a variance-aware
gap-dependent regret bound of $$\tilde{O}\left(\left(\sum_{\Delta_h(s,a)>0}
\frac{H^2 \log K \land \mathtt{Var}_{\max}^{\text{c}}}{\Delta_h(s,a)}
+\sum_{\Delta_h(s,a)=0}\frac{ H^2 \land
\mathtt{Var}_{\max}^{\text{c}}}{\Delta_{\mathrm{min}}} + SAH^4 (S \lor H)
\right) \log K\right),$$ where $H$ is the planning horizon, $S$ is the number
of states, $A$ is the number of actions, and $K$ is the number of episodes.
Here, $\Delta_h(s,a) =V_h^* (a) - Q_h^* (s, a)$ represents the suboptimality
gap and $\Delta_{\mathrm{min}} := \min_{\Delta_h (s,a) > 0} \Delta_h(s,a)$. The
term $\mathtt{Var}_{\max}^{\text{c}}$ denotes the maximum conditional total
variance, calculated as the maximum over all $(\pi, h, s)$ tuples of the
expected total variance under policy $\pi$ conditioned on trajectories visiting
state $s$ at step $h$. $\mathtt{Var}_{\max}^{\text{c}}$ characterizes the
maximum randomness encountered when learning any $(h, s)$ pair. Our result
stems from a novel analysis of the weighted sum of the suboptimality gap and
can be potentially adapted for other algorithms. To complement the study, we
establish a lower bound of $$\Omega \left( \sum_{\Delta_h(s,a)>0} \frac{H^2
\land \mathtt{Var}_{\max}^{\text{c}}}{\Delta_h(s,a)}\cdot \log K\right),$$
demonstrating the necessity of dependence on $\mathtt{Var}_{\max}^{\text{c}}$
even when the maximum unconditional total variance (without conditioning on
$(h, s)$) approaches zero.

</details>


### [459] [Hierarchical and Collaborative LLM-Based Control for Multi-UAV Motion and Communication in Integrated Terrestrial and Non-Terrestrial Networks](https://arxiv.org/abs/2506.06532)
*Zijiang Yan,Hao Zhou,Jianhua Pei,Hina Tabassum*

Main category: cs.LG

TL;DR: 论文提出了一种基于大语言模型（LLM）的分层协作方法，用于多无人机（UAV）在动态约束环境中的联合运动与通信控制。


<details>
  <summary>Details</summary>
Motivation: 多无人机系统在动态和受限环境中的控制与优化是一个重要挑战，尤其是在集成地面和非地面网络（如高空平台站HAPS）的场景下。

Method: 采用分层协作框架，HAPS上的LLM负责无人机接入控制，每架无人机上的LLM处理运动规划与控制，利用预训练模型的丰富知识进行决策。

Result: 实验表明，该方法在系统奖励、运营成本和无人机碰撞率方面均优于基线方法。

Conclusion: 基于LLM的知识驱动范式为下一代3D空中高速公路系统的开发提供了潜力。

Abstract: Unmanned aerial vehicles (UAVs) have been widely adopted in various
real-world applications. However, the control and optimization of multi-UAV
systems remain a significant challenge, particularly in dynamic and constrained
environments. This work explores the joint motion and communication control of
multiple UAVs operating within integrated terrestrial and non-terrestrial
networks that include high-altitude platform stations (HAPS). Specifically, we
consider an aerial highway scenario in which UAVs must accelerate, decelerate,
and change lanes to avoid collisions and maintain overall traffic flow.
Different from existing studies, we propose a novel hierarchical and
collaborative method based on large language models (LLMs). In our approach, an
LLM deployed on the HAPS performs UAV access control, while another LLM onboard
each UAV handles motion planning and control. This LLM-based framework
leverages the rich knowledge embedded in pre-trained models to enable both
high-level strategic planning and low-level tactical decisions. This
knowledge-driven paradigm holds great potential for the development of
next-generation 3D aerial highway systems. Experimental results demonstrate
that our proposed collaborative LLM-based method achieves higher system
rewards, lower operational costs, and significantly reduced UAV collision rates
compared to baseline approaches.

</details>


### [460] [GeoClip: Geometry-Aware Clipping for Differentially Private SGD](https://arxiv.org/abs/2506.06549)
*Atefeh Gilani,Naima Tasnim,Lalitha Sankar,Oliver Kosut*

Main category: cs.LG

TL;DR: GeoClip是一种几何感知的框架，通过变换梯度分布的基础来优化DP-SGD中的梯度裁剪和扰动，显著提升隐私与效用的平衡。


<details>
  <summary>Details</summary>
Motivation: 解决DP-SGD中梯度裁剪阈值设置的挑战，现有方法未能考虑梯度坐标间的相关性。

Method: 提出GeoClip框架，在变换后的基础上裁剪和扰动梯度，自适应估计变换而不增加隐私成本。

Result: 实验表明GeoClip在相同隐私预算下优于现有自适应裁剪方法。

Conclusion: GeoClip通过几何感知优化梯度处理，显著提升DP-SGD的性能。

Abstract: Differentially private stochastic gradient descent (DP-SGD) is the most
widely used method for training machine learning models with provable privacy
guarantees. A key challenge in DP-SGD is setting the per-sample gradient
clipping threshold, which significantly affects the trade-off between privacy
and utility. While recent adaptive methods improve performance by adjusting
this threshold during training, they operate in the standard coordinate system
and fail to account for correlations across the coordinates of the gradient. We
propose GeoClip, a geometry-aware framework that clips and perturbs gradients
in a transformed basis aligned with the geometry of the gradient distribution.
GeoClip adaptively estimates this transformation using only previously released
noisy gradients, incurring no additional privacy cost. We provide convergence
guarantees for GeoClip and derive a closed-form solution for the optimal
transformation that minimizes the amount of noise added while keeping the
probability of gradient clipping under control. Experiments on both tabular and
image datasets demonstrate that GeoClip consistently outperforms existing
adaptive clipping methods under the same privacy budget.

</details>


### [461] [SDN-Based False Data Detection With Its Mitigation and Machine Learning Robustness for In-Vehicle Networks](https://arxiv.org/abs/2506.06556)
*Long Dang,Thushari Hapuarachchi,Kaiqi Xiong,Yi Li*

Main category: cs.LG

TL;DR: 本文提出了一种基于SDN的虚假数据检测与缓解系统（FDDMS），用于车载网络，通过LSTM模型实时检测虚假数据注入攻击，并采用动态流规则更新技术进行缓解。


<details>
  <summary>Details</summary>
Motivation: 随着自动驾驶和互联车辆的发展，车载网络中ECU数量增加，CAN协议的安全性至关重要。本文旨在解决虚假数据注入攻击对车辆安全的威胁。

Method: 1. 解码原始CAN数据构建攻击模型；2. 使用LSTM模型检测攻击；3. 提出DeepFool变体评估模型鲁棒性；4. 采用基于阈值的重训练技术对抗攻击；5. 通过SDN动态更新流规则实现缓解。

Result: 实验表明，FDDMS能有效实时检测和缓解虚假数据注入攻击，并对抗多种对抗性攻击。

Conclusion: FDDMS为车载网络安全提供了鲁棒的解决方案，能够实时检测和缓解虚假数据注入攻击。

Abstract: As the development of autonomous and connected vehicles advances, the
complexity of modern vehicles increases, with numerous Electronic Control Units
(ECUs) integrated into the system. In an in-vehicle network, these ECUs
communicate with one another using an standard protocol called Controller Area
Network (CAN). Securing communication among ECUs plays a vital role in
maintaining the safety and security of the vehicle. This paper proposes a
robust SDN-based False Data Detection and Mitigation System (FDDMS) for
in-vehicle networks. Leveraging the unique capabilities of Software-Defined
Networking (SDN), FDDMS is designed to monitor and detect false data injection
attacks in real-time. Specifically, we focus on brake-related ECUs within an
SDN-enabled in-vehicle network. First, we decode raw CAN data to create an
attack model that illustrates how false data can be injected into the system.
Then, FDDMS, incorporating a Long Short Term Memory (LSTM)-based detection
model, is used to identify false data injection attacks. We further propose an
effective variant of DeepFool attack to evaluate the model's robustness. To
countermeasure the impacts of four adversarial attacks including Fast gradient
descent method, Basic iterative method, DeepFool, and the DeepFool variant, we
further enhance a re-training technique method with a threshold based selection
strategy. Finally, a mitigation scheme is implemented to redirect attack
traffic by dynamically updating flow rules through SDN. Our experimental
results show that the proposed FDDMS is robust against adversarial attacks and
effectively detects and mitigates false data injection attacks in real-time.

</details>


### [462] [Rapid training of Hamiltonian graph networks without gradient descent](https://arxiv.org/abs/2506.06558)
*Atamert Rahma,Chinmay Datar,Ana Cukarska,Felix Dietrich*

Main category: cs.LG

TL;DR: 论文提出了一种基于随机特征参数构建的Hamiltonian Graph Networks（HGN），相比传统迭代优化方法（如Adam、RMSProp等），训练速度提升高达600倍，同时保持准确性。


<details>
  <summary>Details</summary>
Motivation: 解决数据驱动建模中物理对称性和约束的挑战，同时提升训练效率。

Method: 使用随机特征参数构建替代迭代优化方法，训练Hamiltonian Graph Networks（HGN）。

Result: 在多种模拟中表现稳健，包括3维N体弹簧系统，且无需重新训练即可泛化到更大规模系统（如4096节点）。

Conclusion: 挑战了传统基于梯度下降的优化算法在物理系统神经网络训练中的主导地位。

Abstract: Learning dynamical systems that respect physical symmetries and constraints
remains a fundamental challenge in data-driven modeling. Integrating physical
laws with graph neural networks facilitates principled modeling of complex
N-body dynamics and yields accurate and permutation-invariant models. However,
training graph neural networks with iterative, gradient-based optimization
algorithms (e.g., Adam, RMSProp, LBFGS) often leads to slow training,
especially for large, complex systems. In comparison to 15 different
optimizers, we demonstrate that Hamiltonian Graph Networks (HGN) can be trained
up to 600x faster--but with comparable accuracy--by replacing iterative
optimization with random feature-based parameter construction. We show robust
performance in diverse simulations, including N-body mass-spring systems in up
to 3 dimensions with different geometries, while retaining essential physical
invariances with respect to permutation, rotation, and translation. We reveal
that even when trained on minimal 8-node systems, the model can generalize in a
zero-shot manner to systems as large as 4096 nodes without retraining. Our work
challenges the dominance of iterative gradient-descent-based optimization
algorithms for training neural network models for physical systems.

</details>


### [463] [Graph Persistence goes Spectral](https://arxiv.org/abs/2506.06571)
*Mattie Ji,Amauri H. Souza,Vikas Garg*

Main category: cs.LG

TL;DR: SpectRe是一种新的拓扑描述符，将谱信息融入持久同调图，显著提升了图神经网络的表达能力。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖特征而忽略基本图结构信息，限制了表达能力。

Method: 提出SpectRe，将谱信息融入持久同调图，并分析其全局和局部稳定性。

Result: SpectRe在合成和真实数据集上表现优异，提升了图模型的性能。

Conclusion: SpectRe是一种更强大的拓扑描述符，为图表示学习提供了新方向。

Abstract: Including intricate topological information (e.g., cycles) provably enhances
the expressivity of message-passing graph neural networks (GNNs) beyond the
Weisfeiler-Leman (WL) hierarchy. Consequently, Persistent Homology (PH) methods
are increasingly employed for graph representation learning. In this context,
recent works have proposed decorating classical PH diagrams with vertex and
edge features for improved expressivity. However, due to their dependence on
features, these methods still fail to capture basic graph structural
information. In this paper, we propose SpectRe -- a new topological descriptor
for graphs that integrates spectral information into PH diagrams. Notably,
SpectRe is strictly more expressive than existing descriptors on graphs. We
also introduce notions of global and local stability to analyze existing
descriptors and establish that SpectRe is locally stable. Finally, experiments
on synthetic and real-world datasets demonstrate the effectiveness of SpectRe
and its potential to enhance the capabilities of graph models in relevant
learning tasks.

</details>


### [464] [Towards Efficient Multi-LLM Inference: Characterization and Analysis of LLM Routing and Hierarchical Techniques](https://arxiv.org/abs/2506.06579)
*Adarsh Prasad Behera,Jaya Prakash Champati,Roberto Morabito,Sasu Tarkoma,James Gross*

Main category: cs.LG

TL;DR: 该论文探讨了如何通过动态模型选择和分层推理策略优化语言模型的推理效率，以降低计算成本和能耗。


<details>
  <summary>Details</summary>
Motivation: 语言模型在自然语言处理任务中表现出色，但其推理过程计算成本高、能耗大，限制了在资源受限环境中的部署。

Method: 论文提出了两种策略：(i) 路由选择，根据查询复杂度选择最合适的模型；(ii) 分层推理，通过模型序列逐步处理查询直至获得可信结果。

Result: 这些策略显著减少了计算资源的使用，同时保持了任务性能。

Conclusion: 未来研究方向包括更快的响应时间、基于任务复杂度的自适应模型选择，以及在异构环境中的可扩展部署。

Abstract: Recent progress in Language Models (LMs) has dramatically advanced the field
of natural language processing (NLP), excelling at tasks like text generation,
summarization, and question answering. However, their inference remains
computationally expensive and energy intensive, especially in settings with
limited hardware, power, or bandwidth. This makes it difficult to deploy LMs in
mobile, edge, or cost sensitive environments. To address these challenges,
recent approaches have introduced multi LLM intelligent model selection
strategies that dynamically allocate computational resources based on query
complexity -- using lightweight models for simpler queries and escalating to
larger models only when necessary. This survey explores two complementary
strategies for efficient LLM inference: (i) routing, which selects the most
suitable model based on the query, and (ii) cascading or hierarchical inference
(HI), which escalates queries through a sequence of models until a confident
response is found. Both approaches aim to reduce computation by using
lightweight models for simpler tasks while offloading only when needed. We
provide a comparative analysis of these techniques across key performance
metrics, discuss benchmarking efforts, and outline open challenges. Finally, we
outline future research directions to enable faster response times, adaptive
model selection based on task complexity, and scalable deployment across
heterogeneous environments, making LLM based systems more efficient and
accessible for real world applications.

</details>


### [465] [Demystifying Topological Message-Passing with Relational Structures: A Case Study on Oversquashing in Simplicial Message-Passing](https://arxiv.org/abs/2506.06582)
*Diaaeldin Taha,James Chapman,Marzieh Eidi,Karel Devriendt,Guido Montúfar*

Main category: cs.LG

TL;DR: 本文提出了一种统一的公理化框架，将图和拓扑消息传递联系起来，通过关系结构的视角分析单纯和蜂窝复合体及其消息传递方案，扩展了图论结果和算法到高阶结构，以解决拓扑消息传递中的过压缩问题。


<details>
  <summary>Details</summary>
Motivation: 拓扑深度学习（TDL）在建模关系数据中的高阶交互方面表现出强大能力，但拓扑消息传递中的过压缩现象缺乏理论分析。

Method: 提出一个统一的公理化框架，通过关系结构的视角分析单纯和蜂窝复合体及其消息传递方案，扩展图论结果和算法到高阶结构。

Result: 通过理论分析和单纯网络的实证研究，展示了该框架在推动TDL发展方面的潜力。

Conclusion: 该框架为分析并缓解拓扑消息传递中的过压缩问题提供了理论基础，推动了拓扑深度学习的发展。

Abstract: Topological deep learning (TDL) has emerged as a powerful tool for modeling
higher-order interactions in relational data. However, phenomena such as
oversquashing in topological message-passing remain understudied and lack
theoretical analysis. We propose a unifying axiomatic framework that bridges
graph and topological message-passing by viewing simplicial and cellular
complexes and their message-passing schemes through the lens of relational
structures. This approach extends graph-theoretic results and algorithms to
higher-order structures, facilitating the analysis and mitigation of
oversquashing in topological message-passing networks. Through theoretical
analysis and empirical studies on simplicial networks, we demonstrate the
potential of this framework to advance TDL.

</details>


### [466] [Global Convergence of Gradient EM for Over-Parameterized Gaussian Mixtures](https://arxiv.org/abs/2506.06584)
*Mo Zhou,Weihang Xu,Maryam Fazel,Simon S. Du*

Main category: cs.LG

TL;DR: 本文研究了高斯混合模型（GMM）的过参数化学习问题，证明了梯度EM算法在全局收敛性上的突破。


<details>
  <summary>Details</summary>
Motivation: 现有研究仅证明了EM算法在m=2时的全局收敛性，而在m≥3时失效，因此探索过参数化设置下的收敛性具有重要意义。

Method: 采用梯度EM算法，结合Hermite多项式和张量分解工具，分析动态和几何景观。

Result: 证明了在n=Ω(mlogm)的过参数化条件下，梯度EM能全局收敛到真实参数。

Conclusion: 这是首次在m>2时证明EM或梯度EM的全局收敛性，为GMM学习提供了新理论支持。

Abstract: Learning Gaussian Mixture Models (GMMs) is a fundamental problem in machine
learning, with the Expectation-Maximization (EM) algorithm and its popular
variant gradient EM being arguably the most widely used algorithms in practice.
In the exact-parameterized setting, where both the ground truth GMM and the
learning model have the same number of components $m$, a vast line of work has
aimed to establish rigorous recovery guarantees for EM. However, global
convergence has only been proven for the case of $m=2$, and EM is known to fail
to recover the ground truth when $m\geq 3$.
  In this paper, we consider the $\textit{over-parameterized}$ setting, where
the learning model uses $n>m$ components to fit an $m$-component ground truth
GMM. In contrast to the exact-parameterized case, we provide a rigorous global
convergence guarantee for gradient EM. Specifically, for any well separated
GMMs in general position, we prove that with only mild over-parameterization $n
= \Omega(m\log m)$, randomly initialized gradient EM converges globally to the
ground truth at a polynomial rate with polynomial samples. Our analysis
proceeds in two stages and introduces a suite of novel tools for Gaussian
Mixture analysis. We use Hermite polynomials to study the dynamics of gradient
EM and employ tensor decomposition to characterize the geometric landscape of
the likelihood loss. This is the first global convergence and recovery result
for EM or Gradient EM beyond the special case of $m=2$.

</details>


### [467] [Direct Prediction Set Minimization via Bilevel Conformal Classifier Training](https://arxiv.org/abs/2506.06599)
*Yuanjie Shi,Hooman Shahrokhi,Xuesong Jia,Xiongzhi Chen,Janardhan Rao Doppa,Yan Yan*

Main category: cs.LG

TL;DR: 本文提出了一种名为DPSM的算法，通过将共形预测原则融入深度分类器的训练过程，直接最小化预测集的大小，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 标准共形预测校准方法生成的预测集通常过大，实用性受限。本文旨在通过优化训练过程直接减小预测集大小。

Method: 将共形训练建模为双层优化问题，提出DPSM算法，通过最小化预测集大小的度量（上层）并基于一致性分数的分位数（下层）进行优化。

Result: DPSM在多个基准数据集和深度模型上表现优异，预测集大小减少了20.46%，验证了其理论优势。

Conclusion: DPSM算法通过直接优化预测集大小，显著提升了共形预测的实用性，为不确定性量化提供了更高效的解决方案。

Abstract: Conformal prediction (CP) is a promising uncertainty quantification framework
which works as a wrapper around a black-box classifier to construct prediction
sets (i.e., subset of candidate classes) with provable guarantees. However,
standard calibration methods for CP tend to produce large prediction sets which
makes them less useful in practice. This paper considers the problem of
integrating conformal principles into the training process of deep classifiers
to directly minimize the size of prediction sets. We formulate conformal
training as a bilevel optimization problem and propose the {\em Direct
Prediction Set Minimization (DPSM)} algorithm to solve it. The key insight
behind DPSM is to minimize a measure of the prediction set size (upper level)
that is conditioned on the learned quantile of conformity scores (lower level).
We analyze that DPSM has a learning bound of $O(1/\sqrt{n})$ (with $n$ training
samples), while prior conformal training methods based on stochastic
approximation for the quantile has a bound of $\Omega(1/s)$ (with batch size
$s$ and typically $s \ll \sqrt{n}$). Experiments on various benchmark datasets
and deep models show that DPSM significantly outperforms the best prior
conformal training baseline with $20.46\%\downarrow$ in the prediction set size
and validates our theory.

</details>


### [468] [Stacey: Promoting Stochastic Steepest Descent via Accelerated $\ell_p$-Smooth Nonconvex Optimization](https://arxiv.org/abs/2506.06606)
*Xinyu Luo,Cedar Site Bai,Bolian Li,Petros Drineas,Ruqi Zhang,Brian Bullins*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: While popular optimization methods such as SGD, AdamW, and Lion depend on
steepest descent updates in either $\ell_2$ or $\ell_\infty$ norms, there
remains a critical gap in handling the non-Euclidean structure observed in
modern deep networks training. In this work, we address this need by
introducing a new accelerated $\ell_p$ steepest descent algorithm, called
Stacey, which uses interpolated primal-dual iterate sequences to effectively
navigate non-Euclidean smooth optimization tasks. In addition to providing
novel theoretical guarantees for the foundations of our algorithm, we
empirically compare our approach against these popular methods on tasks
including image classification and language model (LLM) pretraining,
demonstrating both faster convergence and higher final accuracy. We further
evaluate different values of $p$ across various models and datasets,
underscoring the importance and efficiency of non-Euclidean approaches over
standard Euclidean methods. Code can be found at
https://github.com/xinyuluo8561/Stacey .

</details>


### [469] [Curriculum Reinforcement Learning from Easy to Hard Tasks Improves LLM Reasoning](https://arxiv.org/abs/2506.06632)
*Shubham Parashar,Shurui Gui,Xiner Li,Hongyi Ling,Sushil Vemuri,Blake Olson,Eric Li,Yu Zhang,James Caverlee,Dileep Kalathil,Shuiwang Ji*

Main category: cs.LG

TL;DR: 论文提出了一种名为E2H Reasoner的方法，通过从易到难的课程学习提升语言模型的推理能力，实验证明该方法显著提升了小型语言模型的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的强化学习方法在提升语言模型对困难任务的推理能力上效果有限，因此需要一种更有效的方法。

Method: 采用从易到难（E2H）的任务调度策略，结合课程学习，逐步提升模型的推理能力。

Result: 实验表明，E2H Reasoner显著提升了小型语言模型（1.5B到3B）的推理能力，且比直接强化学习更高效。

Conclusion: E2H Reasoner是一种有效的方法，通过任务分解和课程学习，显著提升了语言模型的推理能力。

Abstract: We aim to improve the reasoning capabilities of language models via
reinforcement learning (RL). Recent RL post-trained models like DeepSeek-R1
have demonstrated reasoning abilities on mathematical and coding tasks.
However, prior studies suggest that using RL alone to improve reasoning on
inherently difficult tasks is less effective. Here, we draw inspiration from
curriculum learning and propose to schedule tasks from easy to hard (E2H),
allowing LLMs to build reasoning skills gradually. Our method is termed E2H
Reasoner. Empirically, we observe that, although easy tasks are important
initially, fading them out through appropriate scheduling is essential in
preventing overfitting. Theoretically, we establish convergence guarantees for
E2H Reasoner within an approximate policy iteration framework. We derive
finite-sample complexity bounds and show that when tasks are appropriately
decomposed and conditioned, learning through curriculum stages requires fewer
total samples than direct learning. Experiments across multiple domains show
that E2H Reasoner significantly improves the reasoning ability of small LLMs
(1.5B to 3B), which otherwise struggle when trained with vanilla RL alone,
highlighting the effectiveness of our method.

</details>


### [470] [Vision-QRWKV: Exploring Quantum-Enhanced RWKV Models for Image Classification](https://arxiv.org/abs/2506.06633)
*Chi-Sheng Chen*

Main category: cs.LG

TL;DR: 论文提出了一种名为Vision-QRWKV的量子-经典混合模型，用于图像分类任务，通过引入变分量子电路（VQC）提升RWKV架构的非线性特征转换能力。


<details>
  <summary>Details</summary>
Motivation: 量子机器学习在复杂高维数据领域显示出潜力，本文旨在探索量子增强的RWKV架构在视觉任务中的应用。

Method: 在RWKV的通道混合组件中集成变分量子电路（VQC），构建Vision-QRWKV模型，并在14个医学和标准图像分类数据集上进行评估。

Result: 量子增强模型在多数数据集上优于经典模型，尤其是在类别区分微妙或有噪声的数据集（如ChestMNIST、RetinaMNIST、BloodMNIST）上表现突出。

Conclusion: 这是量子增强RWKV在视觉领域的首次系统性应用，为轻量高效视觉任务的量子模型提供了架构权衡和未来潜力的见解。

Abstract: Recent advancements in quantum machine learning have shown promise in
enhancing classical neural network architectures, particularly in domains
involving complex, high-dimensional data. Building upon prior work in temporal
sequence modeling, this paper introduces Vision-QRWKV, a hybrid
quantum-classical extension of the Receptance Weighted Key Value (RWKV)
architecture, applied for the first time to image classification tasks. By
integrating a variational quantum circuit (VQC) into the channel mixing
component of RWKV, our model aims to improve nonlinear feature transformation
and enhance the expressive capacity of visual representations.
  We evaluate both classical and quantum RWKV models on a diverse collection of
14 medical and standard image classification benchmarks, including MedMNIST
datasets, MNIST, and FashionMNIST. Our results demonstrate that the
quantum-enhanced model outperforms its classical counterpart on a majority of
datasets, particularly those with subtle or noisy class distinctions (e.g.,
ChestMNIST, RetinaMNIST, BloodMNIST). This study represents the first
systematic application of quantum-enhanced RWKV in the visual domain, offering
insights into the architectural trade-offs and future potential of quantum
models for lightweight and efficient vision tasks.

</details>


### [471] [Non-Intrusive Load Monitoring Based on Image Load Signatures and Continual Learning](https://arxiv.org/abs/2506.06637)
*Olimjon Toirov,Wei Yu*

Main category: cs.LG

TL;DR: 论文提出了一种结合图像负载特征和持续学习的非侵入式负载监测方法，显著提升了识别精度。


<details>
  <summary>Details</summary>
Motivation: 传统NILM方法在复杂多变的负载组合和应用环境下，特征鲁棒性和模型泛化能力不足。

Method: 将多维电力信号转换为图像负载特征，结合深度卷积神经网络进行设备识别，并引入自监督预训练和持续在线学习策略。

Result: 在高采样率负载数据集上的实验表明，该方法在识别精度上有显著提升。

Conclusion: 该方法通过视觉特征和持续学习，有效解决了传统NILM方法的局限性。

Abstract: Non-Intrusive Load Monitoring (NILM) identifies the operating status and
energy consumption of each electrical device in the circuit by analyzing the
electrical signals at the bus, which is of great significance for smart power
management. However, the complex and changeable load combinations and
application environments lead to the challenges of poor feature robustness and
insufficient model generalization of traditional NILM methods. To this end,
this paper proposes a new non-intrusive load monitoring method that integrates
"image load signature" and continual learning. This method converts
multi-dimensional power signals such as current, voltage, and power factor into
visual image load feature signatures, and combines deep convolutional neural
networks to realize the identification and classification of multiple devices;
at the same time, self-supervised pre-training is introduced to improve feature
generalization, and continual online learning strategies are used to overcome
model forgetting to adapt to the emergence of new loads. This paper conducts a
large number of experiments on high-sampling rate load datasets, and compares a
variety of existing methods and model variants. The results show that the
proposed method has achieved significant improvements in recognition accuracy.

</details>


### [472] [Spark Transformer: Reactivating Sparsity in FFN and Attention](https://arxiv.org/abs/2506.06644)
*Chong You,Kan Wu,Zhipeng Jia,Lin Chen,Srinadh Bhojanapalli,Jiaxian Guo,Utku Evci,Jan Wassenberg,Praneeth Netrapalli,Jeremiah J. Willcock,Suvinay Subramanian,Felix Chern,Alek Andreev,Shreya Pathak,Felix Yu,Prateek Jain,David E. Culler,Henry M. Levy,Sanjiv Kumar*

Main category: cs.LG

TL;DR: Spark Transformer通过top-k掩码和统计top-k算法实现高激活稀疏性，保持模型质量的同时显著减少计算量。


<details>
  <summary>Details</summary>
Motivation: 现有Transformer中ReLU激活函数的稀疏性现象被忽视，重新引入稀疏性常导致模型质量下降或训练复杂度增加。

Method: 采用top-k掩码和统计top-k算法控制稀疏性，并重新分配参数以低成本预测激活条目。

Result: Spark Transformer在标准基准测试中表现优异，FFN神经元激活率仅8%，FLOPs减少2.5倍，解码速度提升1.79x（CPU）和1.40x（GPU）。

Conclusion: Spark Transformer成功实现高稀疏性且不牺牲模型质量，为大型模型效率提升提供了新方向。

Abstract: The discovery of the lazy neuron phenomenon in trained Transformers, where
the vast majority of neurons in their feed-forward networks (FFN) are inactive
for each token, has spurred tremendous interests in activation sparsity for
enhancing large model efficiency. While notable progress has been made in
translating such sparsity to wall-time benefits, modern Transformers have moved
away from the ReLU activation function crucial to this phenomenon. Existing
efforts on re-introducing activation sparsity often degrade model quality,
increase parameter count, complicate or slow down training. Sparse attention,
the application of sparse activation to the attention mechanism, often faces
similar challenges.
  This paper introduces the Spark Transformer, a novel architecture that
achieves a high level of activation sparsity in both FFN and the attention
mechanism while maintaining model quality, parameter count, and standard
training procedures. Our method realizes sparsity via top-k masking for
explicit control over sparsity level. Crucially, we introduce statistical
top-k, a hardware-accelerator-friendly, linear-time approximate algorithm that
avoids costly sorting and mitigates significant training slowdown from standard
top-$k$ operators. Furthermore, Spark Transformer reallocates existing FFN
parameters and attention key embeddings to form a low-cost predictor for
identifying activated entries. This design not only mitigates quality loss from
enforced sparsity, but also enhances wall-time benefit. Pretrained with the
Gemma-2 recipe, Spark Transformer demonstrates competitive performance on
standard benchmarks while exhibiting significant sparsity: only 8% of FFN
neurons are activated, and each token attends to a maximum of 256 tokens. This
sparsity translates to a 2.5x reduction in FLOPs, leading to decoding wall-time
speedups of up to 1.79x on CPU and 1.40x on GPU.

</details>


### [473] [SAFER: A Calibrated Risk-Aware Multimodal Recommendation Model for Dynamic Treatment Regimes](https://arxiv.org/abs/2506.06649)
*Yishan Shen,Yuyang Ye,Hui Xiong,Yong Chen*

Main category: cs.LG

TL;DR: SAFER是一个结合结构化电子健康记录和临床笔记的风险感知推荐框架，用于动态治疗策略，通过统计保证提供安全治疗建议。


<details>
  <summary>Details</summary>
Motivation: 动态治疗策略需要个性化决策，但现有方法依赖临床标准且未充分利用临床笔记，限制了可靠性。

Method: SAFER整合结构化数据和临床笔记，采用共形预测处理标签不确定性，并提供统计保证。

Result: 在公开的脓毒症数据集上，SAFER在推荐指标和反事实死亡率上优于现有方法。

Conclusion: SAFER为高风险动态治疗应用提供了可信赖的理论基础解决方案。

Abstract: Dynamic treatment regimes (DTRs) are critical to precision medicine,
optimizing long-term outcomes through personalized, real-time decision-making
in evolving clinical contexts, but require careful supervision for unsafe
treatment risks. Existing efforts rely primarily on clinician-prescribed gold
standards despite the absence of a known optimal strategy, and predominantly
using structured EHR data without extracting valuable insights from clinical
notes, limiting their reliability for treatment recommendations. In this work,
we introduce SAFER, a calibrated risk-aware tabular-language recommendation
framework for DTR that integrates both structured EHR and clinical notes,
enabling them to learn from each other, and addresses inherent label
uncertainty by assuming ambiguous optimal treatment solution for deceased
patients. Moreover, SAFER employs conformal prediction to provide statistical
guarantees, ensuring safe treatment recommendations while filtering out
uncertain predictions. Experiments on two publicly available sepsis datasets
demonstrate that SAFER outperforms state-of-the-art baselines across multiple
recommendation metrics and counterfactual mortality rate, while offering robust
formal assurances. These findings underscore SAFER potential as a trustworthy
and theoretically grounded solution for high-stakes DTR applications.

</details>


### [474] [Rescaled Influence Functions: Accurate Data Attribution in High Dimension](https://arxiv.org/abs/2506.06656)
*Ittai Rubinstein,Samuel B. Hopkins*

Main category: cs.LG

TL;DR: 论文提出了一种改进的影响函数（RIF），用于更准确地预测训练数据对模型行为的影响，解决了传统影响函数（IF）在高维数据中的不精确问题。


<details>
  <summary>Details</summary>
Motivation: 研究训练数据如何影响模型行为，传统影响函数（IF）在高维数据中表现不佳，需要更精确的工具。

Method: 提出了一种称为重新缩放影响函数（RIF）的新方法，作为IF的替代方案，计算开销小但精度显著提升。

Result: 在多个真实数据集上验证，RIF比IF更准确地预测样本移除的影响，并提出了理论分析支持其改进。

Conclusion: RIF是一种高效且精确的数据归因工具，能够检测到IF无法识别的数据中毒攻击。

Abstract: How does the training data affect a model's behavior? This is the question we
seek to answer with data attribution. The leading practical approaches to data
attribution are based on influence functions (IF). IFs utilize a first-order
Taylor approximation to efficiently predict the effect of removing a set of
samples from the training set without retraining the model, and are used in a
wide variety of machine learning applications. However, especially in the
high-dimensional regime (# params $\geq \Omega($# samples$)$), they are often
imprecise and tend to underestimate the effect of sample removals, even for
simple models such as logistic regression. We present rescaled influence
functions (RIF), a new tool for data attribution which can be used as a drop-in
replacement for influence functions, with little computational overhead but
significant improvement in accuracy. We compare IF and RIF on a range of
real-world datasets, showing that RIFs offer significantly better predictions
in practice, and present a theoretical analysis explaining this improvement.
Finally, we present a simple class of data poisoning attacks that would fool
IF-based detections but would be detected by RIF.

</details>


### [475] [SDP-CROWN: Efficient Bound Propagation for Neural Network Verification with Tightness of Semidefinite Programming](https://arxiv.org/abs/2506.06665)
*Hong-Ming Chiu,Hao Chen,Huan Zhang,Richard Y. Zhang*

Main category: cs.LG

TL;DR: SDP-CROWN结合了SDP的紧致性和线性边界传播的可扩展性，显著提升了大规模神经网络的验证性能。


<details>
  <summary>Details</summary>
Motivation: 现有线性边界传播验证器在大规模模型上表现良好但边界松散，而SDP验证器虽紧致但计算复杂度高，难以应用于大模型。

Method: 提出SDP-CROWN框架，通过SDP原理推导新的线性边界，显式捕获神经元耦合，同时保持可扩展性。

Result: 理论证明新边界比传统方法紧致√n倍，实践验证在大型模型上性能显著提升，接近SDP方法的紧致性。

Conclusion: SDP-CROWN成功结合了SDP的紧致性和线性边界传播的可扩展性，为大规模神经网络验证提供了高效解决方案。

Abstract: Neural network verifiers based on linear bound propagation scale impressively
to massive models but can be surprisingly loose when neuron coupling is
crucial. Conversely, semidefinite programming (SDP) verifiers capture
inter-neuron coupling naturally, but their cubic complexity restricts them to
only small models. In this paper, we propose SDP-CROWN, a novel hybrid
verification framework that combines the tightness of SDP relaxations with the
scalability of bound-propagation verifiers. At the core of SDP-CROWN is a new
linear bound, derived via SDP principles, that explicitly captures
$\ell_{2}$-norm-based inter-neuron coupling while adding only one extra
parameter per layer. This bound can be integrated seamlessly into any linear
bound-propagation pipeline, preserving the inherent scalability of such methods
yet significantly improving tightness. In theory, we prove that our
inter-neuron bound can be up to a factor of $\sqrt{n}$ tighter than traditional
per-neuron bounds. In practice, when incorporated into the state-of-the-art
$\alpha$-CROWN verifier, we observe markedly improved verification performance
on large models with up to 65 thousand neurons and 2.47 million parameters,
achieving tightness that approaches that of costly SDP-based methods.

</details>


### [476] [Through the Gaps: Uncovering Tactical Line-Breaking Passes with Clustering](https://arxiv.org/abs/2506.06666)
*Oktay Karakuş,Hasan Arkadaş*

Main category: cs.LG

TL;DR: 提出了一种基于聚类的无监督框架，用于检测和分析足球中的线突破传球（LBPs），并通过战术指标量化其效果。


<details>
  <summary>Details</summary>
Motivation: 线突破传球（LBPs）是足球中的关键战术动作，能够穿透防线并进入高价值区域，但缺乏系统的分析方法。

Method: 通过垂直空间分割建模对手队形，利用同步事件和跟踪数据检测LBPs，并引入空间构建比（SBR）和链式指标（LBPCh^1和LBPCh^2）量化效果。

Result: 在2022年世界杯数据中验证了方法的有效性，揭示了不同球队和球员在垂直推进和结构破坏上的风格差异。

Conclusion: 该方法可解释、可扩展，适用于现代表现分析和球探工作流程。

Abstract: Line-breaking passes (LBPs) are crucial tactical actions in football,
allowing teams to penetrate defensive lines and access high-value spaces. In
this study, we present an unsupervised, clustering-based framework for
detecting and analysing LBPs using synchronised event and tracking data from
elite matches. Our approach models opponent team shape through vertical spatial
segmentation and identifies passes that disrupt defensive lines within open
play. Beyond detection, we introduce several tactical metrics, including the
space build-up ratio (SBR) and two chain-based variants, LBPCh$^1$ and
LBPCh$^2$, which quantify the effectiveness of LBPs in generating immediate or
sustained attacking threats. We evaluate these metrics across teams and players
in the 2022 FIFA World Cup, revealing stylistic differences in vertical
progression and structural disruption. The proposed methodology is explainable,
scalable, and directly applicable to modern performance analysis and scouting
workflows.

</details>


### [477] [Learning Robust Heterogeneous Graph Representations via Contrastive-Reconstruction under Sparse Semantics](https://arxiv.org/abs/2506.06682)
*Di Lin,Wanjing Ren,Xuanbin Li,Rui Zhang*

Main category: cs.LG

TL;DR: HetCRF是一种新型的双通道自监督学习框架，针对异构图设计，通过两阶段聚合策略和正样本增强策略，解决了语义稀疏和梯度不平衡问题，显著提升了节点分类性能。


<details>
  <summary>Details</summary>
Motivation: 现有混合框架在异构图中的语义稀疏和梯度不平衡问题限制了自监督学习的效果，需要一种更高效的解决方案。

Method: HetCRF采用两阶段聚合策略适应嵌入语义，并增强编码器输出以优化视图构建，同时提出两种正样本增强策略平衡梯度贡献。

Result: 在四个真实异构图数据集上，HetCRF在节点分类任务中表现优于现有基线方法，尤其在特征缺失情况下提升显著。

Conclusion: HetCRF通过创新的双通道设计和正样本增强策略，有效解决了异构图自监督学习中的关键问题，具有广泛的应用潜力。

Abstract: In graph self-supervised learning, masked autoencoders (MAE) and contrastive
learning (CL) are two prominent paradigms. MAE focuses on reconstructing masked
elements, while CL maximizes similarity between augmented graph views. Recent
studies highlight their complementarity: MAE excels at local feature capture,
and CL at global information extraction. Hybrid frameworks for homogeneous
graphs have been proposed, but face challenges in designing shared encoders to
meet the semantic requirements of both tasks. In semantically sparse scenarios,
CL struggles with view construction, and gradient imbalance between positive
and negative samples persists. This paper introduces HetCRF, a novel
dual-channel self-supervised learning framework for heterogeneous graphs.
HetCRF uses a two-stage aggregation strategy to adapt embedding semantics,
making it compatible with both MAE and CL. To address semantic sparsity, it
enhances encoder output for view construction instead of relying on raw
features, improving efficiency. Two positive sample augmentation strategies are
also proposed to balance gradient contributions. Node classification
experiments on four real-world heterogeneous graph datasets demonstrate that
HetCRF outperforms state-of-the-art baselines. On datasets with missing node
features, such as Aminer and Freebase, at a 40% label rate in node
classification, HetCRF improves the Macro-F1 score by 2.75% and 2.2%
respectively compared to the second-best baseline, validating its effectiveness
and superiority.

</details>


### [478] [Breaking Data Silos: Towards Open and Scalable Mobility Foundation Models via Generative Continual Learning](https://arxiv.org/abs/2506.06694)
*Yuan Yuan,Yukun Liu,Chonghua Han,Jie Feng,Yong Li*

Main category: cs.LG

TL;DR: MoveGCL是一个隐私保护的框架，通过生成式持续学习训练移动性基础模型，解决了数据隐私和分散性问题，性能接近联合训练并优于联邦学习。


<details>
  <summary>Details</summary>
Motivation: 由于移动数据的隐私敏感性和数据孤岛问题，构建类似自然语言处理和计算机视觉的基础模型具有挑战性。

Method: MoveGCL采用生成式持续学习，通过冻结教师模型生成合成轨迹进行分散式模型进化，并结合专家混合Transformer和移动感知路由机制。

Result: 在六个真实城市数据集上，MoveGCL性能接近联合训练，显著优于联邦学习基线，同时提供强隐私保护。

Conclusion: MoveGCL为移动性基础模型的发展提供了开放、可扩展且隐私保护的实用方案。

Abstract: Foundation models have revolutionized fields such as natural language
processing and computer vision by enabling general-purpose learning across
diverse tasks and datasets. However, building analogous models for human
mobility remains challenging due to the privacy-sensitive nature of mobility
data and the resulting data silos across institutions. To bridge this gap, we
propose MoveGCL, a scalable and privacy-preserving framework for training
mobility foundation models via generative continual learning. Without sharing
raw data, MoveGCL enables decentralized and progressive model evolution by
replaying synthetic trajectories generated from a frozen teacher model, and
reinforces knowledge retention through a tailored distillation strategy that
mitigates catastrophic forgetting. To address the heterogeneity of mobility
patterns, MoveGCL incorporates a Mixture-of-Experts Transformer with a
mobility-aware expert routing mechanism, and employs a layer-wise progressive
adaptation strategy to stabilize continual updates. Experiments on six
real-world urban datasets demonstrate that MoveGCL achieves performance
comparable to joint training and significantly outperforms federated learning
baselines, while offering strong privacy protection. MoveGCL marks a crucial
step toward unlocking foundation models for mobility, offering a practical
blueprint for open, scalable, and privacy-preserving model development in the
era of foundation models.

</details>


### [479] [MarginSel : Max-Margin Demonstration Selection for LLMs](https://arxiv.org/abs/2506.06699)
*Rajeev Bhatt Ambati,James Lester,Shashank Srivastava,Snigdha Chaturvedi*

Main category: cs.LG

TL;DR: MarginSel是一种两阶段方法，通过选择困难的演示示例来提升LLMs的少样本学习效果。


<details>
  <summary>Details</summary>
Motivation: 解决ICL中演示示例选择和排序对效果敏感的问题。

Method: 提出MarginSel方法，自适应选择困难示例。

Result: 在分类任务中F1分数绝对提升2-7%。

Conclusion: MarginSel通过增加困难示例的边距，有效改善了LLMs的决策边界。

Abstract: Large Language Models (LLMs) excel at few-shot learning via in-context
learning (ICL). However, the effectiveness of ICL is often sensitive to the
selection and ordering of demonstration examples. To address this, we present
MarginSel: Max-Margin Demonstration Selection for LLMs, a two-step method that
selects hard demonstration examples for the ICL prompt, adapting to each test
instance. Our approach achieves 2-7% absolute improvement in F1-score across
classification tasks, compared to a random selection of examples. We also
provide theoretical insights and empirical evidence showing that MarginSel
induces max-margin behavior in LLMs by effectively increasing the margin for
hard examples, analogous to support vectors, thereby shifting the decision
boundary in a beneficial direction.

</details>


### [480] [Do Protein Transformers Have Biological Intelligence?](https://arxiv.org/abs/2506.06701)
*Fudong Lin,Wanrou Du,Jinchan Liu,Tarikul Milon,Shelby Meche,Wu Xu,Xiaoqi Qin,Xu Yuan*

Main category: cs.LG

TL;DR: 论文提出了一种新的Transformer架构（SPT）和可解释AI技术（Sequence Score），用于高效预测蛋白质功能并揭示其生物智能。


<details>
  <summary>Details</summary>
Motivation: 探索蛋白质Transformer是否能捕捉蛋白质序列中的生物智能。

Method: 1. 引入Protein-FN数据集；2. 设计SPT架构；3. 开发Sequence Score技术。

Result: SPT-Tiny模型在AR和Protein-FN数据集上分别达到94.3%和99.6%的准确率。

Conclusion: SPT模型能发现与生物学知识一致的蛋白质序列模式，数据集和代码已开源。

Abstract: Deep neural networks, particularly Transformers, have been widely adopted for
predicting the functional properties of proteins. In this work, we focus on
exploring whether Protein Transformers can capture biological intelligence
among protein sequences. To achieve our goal, we first introduce a protein
function dataset, namely Protein-FN, providing over 9000 protein data with
meaningful labels. Second, we devise a new Transformer architecture, namely
Sequence Protein Transformers (SPT), for computationally efficient protein
function predictions. Third, we develop a novel Explainable Artificial
Intelligence (XAI) technique called Sequence Score, which can efficiently
interpret the decision-making processes of protein models, thereby overcoming
the difficulty of deciphering biological intelligence bided in Protein
Transformers. Remarkably, even our smallest SPT-Tiny model, which contains only
5.4M parameters, demonstrates impressive predictive accuracy, achieving 94.3%
on the Antibiotic Resistance (AR) dataset and 99.6% on the Protein-FN dataset,
all accomplished by training from scratch. Besides, our Sequence Score
technique helps reveal that our SPT models can discover several meaningful
patterns underlying the sequence structures of protein data, with these
patterns aligning closely with the domain knowledge in the biology community.
We have officially released our Protein-FN dataset on Hugging Face Datasets
https://huggingface.co/datasets/Protein-FN/Protein-FN. Our code is available at
https://github.com/fudong03/BioIntelligence.

</details>


### [481] [Safety-Aware Reinforcement Learning for Control via Risk-Sensitive Action-Value Iteration and Quantile Regression](https://arxiv.org/abs/2506.06954)
*Clinton Enwerem,Aniruddh G. Puranic,John S. Baras,Calin Belta*

Main category: cs.LG

TL;DR: 提出了一种基于分位数和CVaR的风险正则化强化学习算法，解决了传统方法在高方差环境中的过估计偏差和安全约束问题。


<details>
  <summary>Details</summary>
Motivation: 主流强化学习算法在高方差随机环境中存在过估计偏差，导致策略次优，且安全约束难以满足。

Method: 结合分位数回归和CVaR，设计风险正则化算法，并证明其在Wasserstein空间中的收敛性。

Result: 仿真实验表明，该方法在动态避障任务中表现优于风险中性方法，成功率高且碰撞少。

Conclusion: 该方法有效平衡了性能与安全性，无需复杂架构即可满足安全约束。

Abstract: Mainstream approximate action-value iteration reinforcement learning (RL)
algorithms suffer from overestimation bias, leading to suboptimal policies in
high-variance stochastic environments. Quantile-based action-value iteration
methods reduce this bias by learning a distribution of the expected cost-to-go
using quantile regression. However, ensuring that the learned policy satisfies
safety constraints remains a challenge when these constraints are not
explicitly integrated into the RL framework. Existing methods often require
complex neural architectures or manual tradeoffs due to combined cost
functions. To address this, we propose a risk-regularized quantile-based
algorithm integrating Conditional Value-at-Risk (CVaR) to enforce safety
without complex architectures. We also provide theoretical guarantees on the
contraction properties of the risk-sensitive distributional Bellman operator in
Wasserstein space, ensuring convergence to a unique cost distribution.
Simulations of a mobile robot in a dynamic reach-avoid task show that our
approach leads to more goal successes, fewer collisions, and better
safety-performance trade-offs compared to risk-neutral methods.

</details>


### [482] [A Framework for Controllable Multi-objective Learning with Annealed Stein Variational Hypernetworks](https://arxiv.org/abs/2506.06715)
*Minh-Duc Nguyen,Dung D. Le*

Main category: cs.LG

TL;DR: 本文提出了一种基于Stein变分梯度下降（SVGD）的新方法SVH-MOL，用于解决多目标学习中帕累托集的多样性与超体积最大化问题。


<details>
  <summary>Details</summary>
Motivation: 当前方法在帕累托解的多样性与超体积最大化之间存在挑战，需要一种高效的方法来平衡这两者。

Method: 采用SVGD逼近整个帕累托集，通过功能梯度下降推动粒子向帕累托集收敛并多样化。结合多样化梯度方向策略和退火调度以提升稳定性。

Result: SVH-MOL在多目标问题和多任务学习中表现出优越性能。

Conclusion: SVH-MOL有效解决了帕累托解的多样性与超体积最大化问题，为多目标学习提供了高效框架。

Abstract: Pareto Set Learning (PSL) is popular as an efficient approach to obtaining
the complete optimal solution in Multi-objective Learning (MOL). A set of
optimal solutions approximates the Pareto set, and its mapping is a set of
dense points in the Pareto front in objective space. However, some current
methods face a challenge: how to make the Pareto solution is diverse while
maximizing the hypervolume value. In this paper, we propose a novel method to
address this challenge, which employs Stein Variational Gradient Descent (SVGD)
to approximate the entire Pareto set. SVGD pushes a set of particles towards
the Pareto set by applying a form of functional gradient descent, which helps
to converge and diversify optimal solutions. Additionally, we employ diverse
gradient direction strategies to thoroughly investigate a unified framework for
SVGD in multi-objective optimization and adapt this framework with an annealing
schedule to promote stability. We introduce our method, SVH-MOL, and validate
its effectiveness through extensive experiments on multi-objective problems and
multi-task learning, demonstrating its superior performance.

</details>


### [483] [The OCR Quest for Generalization: Learning to recognize low-resource alphabets with model editing](https://arxiv.org/abs/2506.06761)
*Adrià Molina Rodríguez,Oriol Ramos Terrades,Josep Lladós*

Main category: cs.LG

TL;DR: 论文提出了一种通过模型编辑技术增强低资源语言（如古文字和非西方语言）识别的方法，显著提升了模型在新数据分布（如新字母表）上的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 解决低资源语言在识别系统中代表性不足的问题，提升模型对新数据分布的适应能力。

Method: 利用模型编辑技术，结合领域合并策略，避免传统元学习中对数据分布或原型的依赖。

Result: 实验显示，即使在相同训练数据下，模型在新字母表和跨域评估（如历史密码文本和非拉丁文字）中表现显著提升。

Conclusion: 该方法为构建能够快速适应低资源字母表的模型提供了新思路，扩展了文档识别的应用范围。

Abstract: Achieving robustness in recognition systems across diverse domains is crucial
for their practical utility. While ample data availability is usually assumed,
low-resource languages, such as ancient manuscripts and non-western languages,
tend to be kept out of the equations of massive pretraining and foundational
techniques due to an under representation. In this work, we aim for building
models which can generalize to new distributions of data, such as alphabets,
faster than centralized fine-tune strategies. For doing so, we take advantage
of the recent advancements in model editing to enhance the incorporation of
unseen scripts (low-resource learning). In contrast to state-of-the-art
meta-learning, we showcase the effectiveness of domain merging in sparse
distributions of data, with agnosticity of its relation to the overall
distribution or any other prototyping necessity. Even when using the same exact
training data, our experiments showcase significant performance boosts in
\textbf{transfer learning} to new alphabets and \textbf{out-of-domain
evaluation} in challenging domain shifts, including historical ciphered texts
and non-Latin scripts. This research contributes a novel approach into building
models that can easily adopt under-represented alphabets and, therefore, enable
document recognition to a wider set of contexts and cultures.

</details>


### [484] [Feature-Based Instance Neighbor Discovery: Advanced Stable Test-Time Adaptation in Dynamic World](https://arxiv.org/abs/2506.06782)
*Qinting Jiang,Chuyang Ye,Dongyan Wei,Bingli Wang,Yuan Xue,Jingyan Jiang,Zhi Wang*

Main category: cs.LG

TL;DR: 论文提出了一种名为FIND的新方法，通过特征实例邻居发现解决深度神经网络在测试时分布偏移下的性能下降问题。


<details>
  <summary>Details</summary>
Motivation: 深度神经网络在训练和测试域分布偏移时性能下降，影响应用体验。现有测试时适应方法难以应对动态多测试分布。

Method: FIND包括三个关键组件：层间特征解耦（LFD）、特征感知批量归一化（FABN）和选择性FABN（S-FABN）。LFD通过图结构捕获相似分布特征，FABN结合源统计和测试分布统计，S-FABN优化特征分区。

Result: 实验表明，FIND在动态场景中显著优于现有方法，准确率提升30%，同时保持计算效率。

Conclusion: FIND通过特征解耦和动态归一化策略，有效解决了测试时分布偏移问题，提升了模型鲁棒性和效率。

Abstract: Despite progress, deep neural networks still suffer performance declines
under distribution shifts between training and test domains, leading to a
substantial decrease in Quality of Experience (QoE) for applications. Existing
test-time adaptation (TTA) methods are challenged by dynamic, multiple test
distributions within batches. We observe that feature distributions across
different domains inherently cluster into distinct groups with varying means
and variances. This divergence reveals a critical limitation of previous global
normalization strategies in TTA, which inevitably distort the original data
characteristics. Based on this insight, we propose Feature-based Instance
Neighbor Discovery (FIND), which comprises three key components: Layer-wise
Feature Disentanglement (LFD), Feature Aware Batch Normalization (FABN) and
Selective FABN (S-FABN). LFD stably captures features with similar
distributions at each layer by constructing graph structures. While FABN
optimally combines source statistics with test-time distribution specific
statistics for robust feature representation. Finally, S-FABN determines which
layers require feature partitioning and which can remain unified, thereby
enhancing inference efficiency. Extensive experiments demonstrate that FIND
significantly outperforms existing methods, achieving a 30\% accuracy
improvement in dynamic scenarios while maintaining computational efficiency.

</details>


### [485] [Caterpillar GNN: Replacing Message Passing with Efficient Aggregation](https://arxiv.org/abs/2506.06784)
*Marek Černý*

Main category: cs.LG

TL;DR: 论文提出了一种高效的聚合机制，牺牲部分表达能力以增强结构化聚合能力，并基于此设计了Caterpillar GNN，在合成和真实数据集上表现优异。


<details>
  <summary>Details</summary>
Motivation: 现代图学习中，消息传递图神经网络（MPGNNs）通常追求最大表达能力，但本文希望通过牺牲部分表达能力来增强聚合能力。

Method: 引入高效聚合机制，通过层次化的广义caterpillar图同构计数严格表征表达能力，并设计Caterpillar GNN。

Result: Caterpillar GNN在合成图级任务中表现优于传统MPGNNs，在真实数据集上预测性能相当但隐藏层节点数显著减少。

Conclusion: 通过结构化聚合机制，Caterpillar GNN在保持性能的同时提升了计算效率。

Abstract: Message-passing graph neural networks (MPGNNs) dominate modern graph
learning, typically prioritizing maximal expressive power. In contrast, we
introduce an \emph{efficient aggregation} mechanism, deliberately trading off
some expressivity for stronger and more structured aggregation capabilities.
Our approach allows seamless scaling between classical message-passing and
simpler methods based on colored or plain walks. We rigorously characterize the
expressive power at each intermediate step using homomorphism counts from a
hierarchy of generalized \emph{caterpillar graphs}. Based on this foundation,
we propose the \emph{Caterpillar GNN}, whose robust graph-level aggregation
enables it to successfully tackle synthetic graph-level task specifically
designed to be challenging for classical MPGNNs. Moreover, we demonstrate that,
on real-world datasets, the Caterpillar GNN achieves comparable predictive
performance while significantly reducing the number of nodes in the hidden
layers of the computational graph.

</details>


### [486] [FuncGNN: Learning Functional Semantics of Logic Circuits with Graph Neural Networks](https://arxiv.org/abs/2506.06787)
*Qiyun Zhao*

Main category: cs.LG

TL;DR: FuncGNN提出了一种结合混合特征聚合和门感知归一化的方法，以解决AIGs在复杂电路中的结构异质性和全局信息丢失问题，显著提升了逻辑电路表示的性能。


<details>
  <summary>Details</summary>
Motivation: 随着集成电路规模和设计复杂度的增加，AIGs在表示布尔逻辑时面临结构异质性和全局信息丢失的挑战，需要更有效的电路表示方法。

Method: FuncGNN通过混合特征聚合提取多粒度拓扑模式，引入门感知归一化适应电路门分布，并采用多层集成合并中间特征，以综合局部和全局语义信息。

Result: 在信号概率预测和真值表距离预测任务中，FuncGNN分别提升了2.06%和18.71%的性能，同时减少了50.6%的训练时间和32.8%的GPU内存使用。

Conclusion: FuncGNN通过创新的特征聚合和归一化方法，显著提升了复杂电路的表示能力，为电子设计自动化提供了更高效的解决方案。

Abstract: As integrated circuit scale grows and design complexity rises, effective
circuit representation helps support logic synthesis, formal verification, and
other automated processes in electronic design automation. And-Inverter Graphs
(AIGs), as a compact and canonical structure, are widely adopted for
representing Boolean logic in these workflows. However, the increasing
complexity and integration density of modern circuits introduce structural
heterogeneity and global logic information loss in AIGs, posing significant
challenges to accurate circuit modeling. To address these issues, we propose
FuncGNN, which integrates hybrid feature aggregation to extract
multi-granularity topological patterns, thereby mitigating structural
heterogeneity and enhancing logic circuit representations. FuncGNN further
introduces gate-aware normalization that adapts to circuit-specific gate
distributions, improving robustness to structural heterogeneity. Finally,
FuncGNN employs multi-layer integration to merge intermediate features across
layers, effectively synthesizing local and global semantic information for
comprehensive logic representations. Experimental results on two logic-level
analysis tasks (i.e., signal probability prediction and truth-table distance
prediction) demonstrate that FuncGNN outperforms existing state-of-the-art
methods, achieving improvements of 2.06% and 18.71%, respectively, while
reducing training time by approximately 50.6% and GPU memory usage by about
32.8%.

</details>


### [487] [Is Optimal Transport Necessary for Inverse Reinforcement Learning?](https://arxiv.org/abs/2506.06793)
*Zixuan Dong,Yumi Omori,Keith Ross*

Main category: cs.LG

TL;DR: 论文提出两种简单的启发式替代方法（最小距离奖励和分段匹配奖励），挑战了逆强化学习中最优传输的必要性，并在实验中表现优异。


<details>
  <summary>Details</summary>
Motivation: 质疑最优传输（OT）在逆强化学习（IRL）中的必要性，提出更简单的方法以减少算法复杂性和超参数敏感性。

Method: 提出两种启发式方法：1) 最小距离奖励（忽略时间顺序）；2) 分段匹配奖励（轻量级时间对齐）。

Result: 在32个基准测试中，新方法表现优于或匹配OT方法，且复杂度更低。

Conclusion: OT的核心优势可能源于基本对齐而非复杂耦合，未来IRL设计应重新评估复杂性。

Abstract: Inverse Reinforcement Learning (IRL) aims to recover a reward function from
expert demonstrations. Recently, Optimal Transport (OT) methods have been
successfully deployed to align trajectories and infer rewards. While OT-based
methods have shown strong empirical results, they introduce algorithmic
complexity, hyperparameter sensitivity, and require solving the OT optimization
problems. In this work, we challenge the necessity of OT in IRL by proposing
two simple, heuristic alternatives: (1) Minimum-Distance Reward, which assigns
rewards based on the nearest expert state regardless of temporal order; and (2)
Segment-Matching Reward, which incorporates lightweight temporal alignment by
matching agent states to corresponding segments in the expert trajectory. These
methods avoid optimization, exhibit linear-time complexity, and are easy to
implement. Through extensive evaluations across 32 online and offline
benchmarks with three reinforcement learning algorithms, we show that our
simple rewards match or outperform recent OT-based approaches. Our findings
suggest that the core benefits of OT may arise from basic proximity alignment
rather than its optimal coupling formulation, advocating for reevaluation of
complexity in future IRL design.

</details>


### [488] [Graph-Assisted Stitching for Offline Hierarchical Reinforcement Learning](https://arxiv.org/abs/2506.07744)
*Seungho Baek,Taegeon Park,Jongchan Park,Seungjun Oh,Yusung Kim*

Main category: cs.LG

TL;DR: 论文提出了一种名为GAS的新框架，通过将子目标选择建模为图搜索问题，而非学习显式的高层策略，提升了离线分层强化学习的效率。


<details>
  <summary>Details</summary>
Motivation: 现有离线分层强化学习方法依赖高层策略生成子目标序列，但随着任务时间增长效率下降，且缺乏有效策略拼接不同轨迹的状态转移。

Method: GAS将状态嵌入到Temporal Distance Representation (TDR) 空间，聚类语义相似状态为图节点，利用最短路径算法选择子目标序列，并通过低层策略实现子目标。引入Temporal Efficiency (TE) 指标优化图质量。

Result: GAS在运动、导航和操作任务中表现优于现有离线HRL方法，尤其在拼接关键任务中得分88.3，远超之前最佳得分1.0。

Conclusion: GAS通过图搜索和状态聚类显著提升了离线分层强化学习的效率和性能。

Abstract: Existing offline hierarchical reinforcement learning methods rely on
high-level policy learning to generate subgoal sequences. However, their
efficiency degrades as task horizons increase, and they lack effective
strategies for stitching useful state transitions across different
trajectories. We propose Graph-Assisted Stitching (GAS), a novel framework that
formulates subgoal selection as a graph search problem rather than learning an
explicit high-level policy. By embedding states into a Temporal Distance
Representation (TDR) space, GAS clusters semantically similar states from
different trajectories into unified graph nodes, enabling efficient transition
stitching. A shortest-path algorithm is then applied to select subgoal
sequences within the graph, while a low-level policy learns to reach the
subgoals. To improve graph quality, we introduce the Temporal Efficiency (TE)
metric, which filters out noisy or inefficient transition states, significantly
enhancing task performance. GAS outperforms prior offline HRL methods across
locomotion, navigation, and manipulation tasks. Notably, in the most
stitching-critical task, it achieves a score of 88.3, dramatically surpassing
the previous state-of-the-art score of 1.0. Our source code is available at:
https://github.com/qortmdgh4141/GAS.

</details>


### [489] [IMPA-HGAE:Intra-Meta-Path Augmented Heterogeneous Graph Autoencoder](https://arxiv.org/abs/2506.06809)
*Di Lin,Wanjing Ren,Xuanbin Li,Rui Zhang*

Main category: cs.LG

TL;DR: 本文提出了一种名为IMPA-HGAE的新框架，通过充分利用元路径上的节点信息来增强目标节点嵌入，解决了现有异构图自监督学习模型对元路径信息利用不足的问题。


<details>
  <summary>Details</summary>
Motivation: 现有异构图自监督学习方法通常将异构图转换为同构图进行训练，仅利用元路径两端节点的信息，而忽略了元路径上的异构节点信息。

Method: 提出了IMPA-HGAE框架，通过利用元路径上的内部节点信息增强目标节点嵌入，并引入了创新的掩码策略以提升生成式自监督学习模型的表示能力。

Result: 实验证明IMPA-HGAE在异构图数据集上表现优异。

Conclusion: 该工作为复杂图场景中利用元路径结构语义进行鲁棒表示学习提供了新思路，并探讨了生成式自监督学习在异构图中的未来方向。

Abstract: Self-supervised learning (SSL) methods have been increasingly applied to
diverse downstream tasks due to their superior generalization capabilities and
low annotation costs. However, most existing heterogeneous graph SSL models
convert heterogeneous graphs into homogeneous ones via meta-paths for training,
which only leverage information from nodes at both ends of meta-paths while
underutilizing the heterogeneous node information along the meta-paths. To
address this limitation, this paper proposes a novel framework named IMPA-HGAE
to enhance target node embeddings by fully exploiting internal node information
along meta-paths. Experimental results validate that IMPA-HGAE achieves
superior performance on heterogeneous datasets. Furthermore, this paper
introduce innovative masking strategies to strengthen the representational
capacity of generative SSL models on heterogeneous graph data. Additionally,
this paper discuss the interpretability of the proposed method and potential
future directions for generative self-supervised learning in heterogeneous
graphs. This work provides insights into leveraging meta-path-guided structural
semantics for robust representation learning in complex graph scenarios.

</details>


### [490] [Path Integral Optimiser: Global Optimisation via Neural Schrödinger-Föllmer Diffusion](https://arxiv.org/abs/2506.06815)
*Max McGuinness,Eirik Fladmark,Francisco Vargas*

Main category: cs.LG

TL;DR: 本文研究了神经扩散过程在全局优化中的应用，基于Zhang等人的路径积分采样器，通过Boltzmann分布将优化问题转化为Schrödinger桥采样问题，并利用Girsanov定理和神经近似（Fourier MLP）求解。实验表明，该方法在2至1,247维任务中表现良好，但在15.9k参数模型中探索高维空间时存在困难。


<details>
  <summary>Details</summary>
Motivation: 探索神经扩散过程在全局优化中的潜力，尤其是针对高维空间的优化问题。

Method: 利用Boltzmann分布将优化问题转化为Schrödinger桥采样问题，应用Girsanov定理和神经近似（Fourier MLP）求解。

Result: 在2至1,247维任务中表现良好，但在15.9k参数模型中探索高维空间时表现不佳。

Conclusion: 该方法在低维任务中具有潜力，但在高维环境中需要进一步改进以适应更复杂的优化问题。

Abstract: We present an early investigation into the use of neural diffusion processes
for global optimisation, focusing on Zhang et al.'s Path Integral Sampler. One
can use the Boltzmann distribution to formulate optimization as solving a
Schr\"odinger bridge sampling problem, then apply Girsanov's theorem with a
simple (single-point) prior to frame it in stochastic control terms, and
compute the solution's integral terms via a neural approximation (a Fourier
MLP). We provide theoretical bounds for this optimiser, results on toy
optimisation tasks, and a summary of the stochastic theory motivating the
model. Ultimately, we found the optimiser to display promising per-step
performance at optimisation tasks between 2 and 1,247 dimensions, but struggle
to explore higher-dimensional spaces when faced with a 15.9k parameter model,
indicating a need for work on adaptation in such environments.

</details>


### [491] [Curvature Enhanced Data Augmentation for Regression](https://arxiv.org/abs/2506.06853)
*Ilya Kaufman Sirot,Omri Azencot*

Main category: cs.LG

TL;DR: 论文提出了一种基于二阶数据流形表示的Curvature-Enhanced Manifold Sampling (CEMS)方法，用于回归任务，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 尽管数据增强在分类任务中表现优异，但在回归问题中的应用较少。本文旨在填补这一空白，提出一种新的流形学习方法。

Method: 利用二阶数据流形表示，开发了CEMS方法，用于高效采样和重建新数据点。

Result: CEMS在多个数据集上表现优异，优于现有方法，且计算开销极小。

Conclusion: CEMS为回归任务提供了一种高效的数据增强方法，具有广泛的应用潜力。

Abstract: Deep learning models with a large number of parameters, often referred to as
over-parameterized models, have achieved exceptional performance across various
tasks. Despite concerns about overfitting, these models frequently generalize
well to unseen data, thanks to effective regularization techniques, with data
augmentation being among the most widely used. While data augmentation has
shown great success in classification tasks using label-preserving
transformations, its application in regression problems has received less
attention. Recently, a novel \emph{manifold learning} approach for generating
synthetic data was proposed, utilizing a first-order approximation of the data
manifold. Building on this foundation, we present a theoretical framework and
practical tools for approximating and sampling general data manifolds.
Furthermore, we introduce the Curvature-Enhanced Manifold Sampling (CEMS)
method for regression tasks. CEMS leverages a second-order representation of
the data manifold to enable efficient sampling and reconstruction of new data
points. Extensive evaluations across multiple datasets and comparisons with
state-of-the-art methods demonstrate that CEMS delivers superior performance in
both in-distribution and out-of-distribution scenarios, while introducing only
minimal computational overhead. Code is available at
https://github.com/azencot-group/CEMS.

</details>


### [492] [High-Fidelity Scientific Simulation Surrogates via Adaptive Implicit Neural Representations](https://arxiv.org/abs/2506.06858)
*Ziwei Li,Yuhan Duan,Tianyu Xiong,Yi-Tang Chen,Wei-Lun Chao,Han-Wei Shen*

Main category: cs.LG

TL;DR: FA-INR通过交叉注意力和混合专家机制，实现了高效且灵活的隐式神经表示，显著提升了科学模拟的精度和模型紧凑性。


<details>
  <summary>Details</summary>
Motivation: 现有INR方法在处理复杂科学数据时，因局部高频变化和刚性几何结构限制，导致灵活性不足和模型过大。

Method: 提出FA-INR，利用交叉注意力学习自适应特征表示，并引入坐标引导的混合专家机制提升可扩展性。

Result: 在三个大规模模拟数据集上，FA-INR实现了最佳精度，同时显著减小模型规模。

Conclusion: FA-INR为INR替代模型建立了新的精度与紧凑性权衡前沿。

Abstract: Effective surrogate models are critical for accelerating scientific
simulations. Implicit neural representations (INRs) offer a compact and
continuous framework for modeling spatially structured data, but they often
struggle with complex scientific fields exhibiting localized, high-frequency
variations. Recent approaches address this by introducing additional features
along rigid geometric structures (e.g., grids), but at the cost of flexibility
and increased model size. In this paper, we propose a simple yet effective
alternative: Feature-Adaptive INR (FA-INR). FA-INR leverages cross-attention to
an augmented memory bank to learn flexible feature representations, enabling
adaptive allocation of model capacity based on data characteristics, rather
than rigid structural assumptions. To further improve scalability, we introduce
a coordinate-guided mixture of experts (MoE) that enhances the specialization
and efficiency of feature representations. Experiments on three large-scale
ensemble simulation datasets show that FA-INR achieves state-of-the-art
fidelity while significantly reducing model size, establishing a new trade-off
frontier between accuracy and compactness for INR-based surrogates.

</details>


### [493] [Differentially Private Sparse Linear Regression with Heavy-tailed Responses](https://arxiv.org/abs/2506.06861)
*Xizhi Tian,Meng Ding,Touming Tao,Zihang Xiang,Di Wang*

Main category: cs.LG

TL;DR: 本文研究了高维稀疏线性回归在差分隐私（DP）下的问题，提出了两种方法DP-IHT-H和DP-IHT-L，分别针对重尾响应数据和额外假设下的响应数据，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有DP线性回归方法多关注规则数据分布或低维情况，无法处理高维稀疏数据和重尾响应，本文旨在填补这一空白。

Method: 提出DP-IHT-H（基于Huber损失和私有迭代硬阈值）和DP-IHT-L（在额外假设下改进误差界），分别适用于不同数据特性。

Result: DP-IHT-H在重尾数据下达到特定误差界，DP-IHT-L在额外假设下进一步优化误差界，实验验证其优于标准DP算法。

Conclusion: 本文方法在高维稀疏和重尾数据下表现优异，为DP线性回归提供了更通用的解决方案。

Abstract: As a fundamental problem in machine learning and differential privacy (DP),
DP linear regression has been extensively studied. However, most existing
methods focus primarily on either regular data distributions or low-dimensional
cases with irregular data. To address these limitations, this paper provides a
comprehensive study of DP sparse linear regression with heavy-tailed responses
in high-dimensional settings. In the first part, we introduce the DP-IHT-H
method, which leverages the Huber loss and private iterative hard thresholding
to achieve an estimation error bound of \(
  \tilde{O}\biggl(
  s^{* \frac{1 }{2}}
  \cdot \biggl(\frac{\log d}{n}\biggr)^{\frac{\zeta}{1 + \zeta}}
  +
  s^{* \frac{1 + 2\zeta}{2 + 2\zeta}}
  \cdot \biggl(\frac{\log^2 d}{n \varepsilon}\biggr)^{\frac{\zeta}{1 + \zeta}}
  \biggr) \) under the $(\varepsilon, \delta)$-DP model, where $n$ is the
sample size, $d$ is the dimensionality, $s^*$ is the sparsity of the parameter,
and $\zeta \in (0, 1]$ characterizes the tail heaviness of the data. In the
second part, we propose DP-IHT-L, which further improves the error bound under
additional assumptions on the response and achieves \(
  \tilde{O}\Bigl(\frac{(s^*)^{3/2} \log d}{n \varepsilon}\Bigr). \) Compared to
the first result, this bound is independent of the tail parameter $\zeta$.
Finally, through experiments on synthetic and real-world datasets, we
demonstrate that our methods outperform standard DP algorithms designed for
``regular'' data.

</details>


### [494] [SAFE: Finding Sparse and Flat Minima to Improve Pruning](https://arxiv.org/abs/2506.06866)
*Dongyeop Lee,Kwanhee Lee,Jinseok Chung,Namhoon Lee*

Main category: cs.LG

TL;DR: 论文提出了一种新的剪枝方法SAFE，通过同时优化稀疏性和平坦性，显著提升了稀疏网络的性能。


<details>
  <summary>Details</summary>
Motivation: 稀疏化神经网络常导致性能下降，难以恢复原始性能，研究旨在通过寻找既稀疏又平坦的子网络来解决这一问题。

Method: 将剪枝问题建模为稀疏约束优化问题，通过增强拉格朗日对偶方法求解，并进一步提出广义投影操作，形成SAFE及其扩展SAFE$^+$。

Result: SAFE在图像分类和语言建模任务中表现优异，稀疏网络泛化性能提升，且对噪声数据具有鲁棒性。

Conclusion: SAFE是一种有效的剪枝方法，适用于现实场景，性能优于现有基线。

Abstract: Sparsifying neural networks often suffers from seemingly inevitable
performance degradation, and it remains challenging to restore the original
performance despite much recent progress. Motivated by recent studies in robust
optimization, we aim to tackle this problem by finding subnetworks that are
both sparse and flat at the same time. Specifically, we formulate pruning as a
sparsity-constrained optimization problem where flatness is encouraged as an
objective. We solve it explicitly via an augmented Lagrange dual approach and
extend it further by proposing a generalized projection operation, resulting in
novel pruning methods called SAFE and its extension, SAFE$^+$. Extensive
evaluations on standard image classification and language modeling tasks reveal
that SAFE consistently yields sparse networks with improved generalization
performance, which compares competitively to well-established baselines. In
addition, SAFE demonstrates resilience to noisy data, making it well-suited for
real-world conditions.

</details>


### [495] [Log-Sum-Exponential Estimator for Off-Policy Evaluation and Learning](https://arxiv.org/abs/2506.06873)
*Armin Behnamnia,Gholamali Aminian,Alireza Aghaei,Chengchun Shi,Vincent Y. F. Tan,Hamid R. Rabiee*

Main category: cs.LG

TL;DR: 论文提出了一种基于log-sum-exponential（LSE）算子的新估计器，用于解决离线策略学习和评估中的高方差和重尾奖励分布问题，表现出优于传统方法的性能。


<details>
  <summary>Details</summary>
Motivation: 离线策略学习和评估面临高方差、低质量倾向得分和重尾奖励分布的挑战，需要更稳健的解决方案。

Method: 引入基于LSE算子的新估计器，推导其偏差和方差的上界，并在离线策略学习和评估场景中进行理论分析。

Result: LSE估计器在方差减少和重尾条件下表现稳健，理论分析显示其遗憾收敛速率为$O(n^{-\epsilon/(1+ \epsilon)})$。

Conclusion: LSE估计器在理论和实践中均表现出优越性，为离线策略学习和评估提供了有效工具。

Abstract: Off-policy learning and evaluation leverage logged bandit feedback datasets,
which contain context, action, propensity score, and feedback for each data
point. These scenarios face significant challenges due to high variance and
poor performance with low-quality propensity scores and heavy-tailed reward
distributions. We address these issues by introducing a novel estimator based
on the log-sum-exponential (LSE) operator, which outperforms traditional
inverse propensity score estimators. Our LSE estimator demonstrates variance
reduction and robustness under heavy-tailed conditions. For off-policy
evaluation, we derive upper bounds on the estimator's bias and variance. In the
off-policy learning scenario, we establish bounds on the regret -- the
performance gap between our LSE estimator and the optimal policy -- assuming
bounded $(1+\epsilon)$-th moment of weighted reward. Notably, we achieve a
convergence rate of $O(n^{-\epsilon/(1+ \epsilon)})$ for the regret bounds,
where $\epsilon \in [0,1]$ and $n$ is the size of logged bandit feedback
dataset. Theoretical analysis is complemented by comprehensive empirical
evaluations in both off-policy learning and evaluation scenarios, confirming
the practical advantages of our approach. The code for our estimator is
available at the following link:
https://github.com/armin-behnamnia/lse-offpolicy-learning.

</details>


### [496] [FREE: Fast and Robust Vision Language Models with Early Exits](https://arxiv.org/abs/2506.06884)
*Divya Jyoti Bajpai,Manjesh Kumar Hanawal*

Main category: cs.LG

TL;DR: 论文提出了一种名为FREE的方法，通过对抗训练在GAN框架中优化Vision-Language Models（VLMs）的Early Exit策略，以提升推理速度并保持性能。


<details>
  <summary>Details</summary>
Motivation: 尽管VLMs在视觉-语言任务中表现优异，但其大模型尺寸导致推理延迟问题，限制了实际应用。

Method: 采用对抗训练框架，每个退出点包含一个Transformer层和一个分类器，通过生成类似最终层的特征表示来优化推理。

Result: 实验表明，该方法在保持性能的同时，将推理速度提升1.51倍以上，并增强了模型鲁棒性。

Conclusion: FREE方法有效解决了VLMs推理延迟问题，为实际应用提供了高效解决方案。

Abstract: In recent years, Vision-Language Models (VLMs) have shown remarkable
performance improvements in Vision-Language tasks. However, their large size
poses challenges for real-world applications where inference latency is a
concern. To tackle this issue, we propose employing Early Exit (EE) strategies
in VLMs. However, training exit classifiers in VLMs is challenging,
particularly with limited labeled training data. To address this, we introduce
FREE, an adversarial training approach within a GAN-based framework. Here, each
exit consists of a transformer layer and a classifier. The transformer layer is
adversarially trained to produce feature representations similar to the final
layer, while a feature classifier serves as the discriminator. Our method
focuses on performing input-adaptive inference that increases inference speed
with minimal drop in performance. Experimental results demonstrate the
effectiveness of our approach in enhancing accuracy and model robustness by
mitigating overthinking and the phenomenon of mid-crisis that we highlight. We
experimentally validate that our method speeds up the inference process by more
than 1.51x while retaining comparable performance. The source code is available
at https://github.com/Div290/FREE.

</details>


### [497] [Can In-Context Reinforcement Learning Recover From Reward Poisoning Attacks?](https://arxiv.org/abs/2506.06891)
*Paulius Sasnauskas,Yiğit Yalın,Goran Radanović*

Main category: cs.LG

TL;DR: 论文研究了上下文强化学习（ICRL）的抗腐败性，提出了一种对抗训练框架AT-DPT，显著优于现有基线方法。


<details>
  <summary>Details</summary>
Motivation: 针对决策预训练Transformer（DPT）易受奖励投毒攻击的问题，研究其抗腐败性。

Method: 提出AT-DPT框架，同时训练攻击者和DPT模型，攻击者通过污染环境奖励最小化真实奖励，DPT从污染数据中推断最优动作。

Result: 在强盗问题和MDP设置中，AT-DPT显著优于现有基线方法，包括针对奖励污染的鲁棒基线。

Conclusion: AT-DPT在复杂环境中表现出良好的抗腐败性，验证了其鲁棒性。

Abstract: We study the corruption-robustness of in-context reinforcement learning
(ICRL), focusing on the Decision-Pretrained Transformer (DPT, Lee et al.,
2023). To address the challenge of reward poisoning attacks targeting the DPT,
we propose a novel adversarial training framework, called Adversarially Trained
Decision-Pretrained Transformer (AT-DPT). Our method simultaneously trains an
attacker to minimize the true reward of the DPT by poisoning environment
rewards, and a DPT model to infer optimal actions from the poisoned data. We
evaluate the effectiveness of our approach against standard bandit algorithms,
including robust baselines designed to handle reward contamination. Our results
show that the proposed method significantly outperforms these baselines in
bandit settings, under a learned attacker. We additionally evaluate AT-DPT on
an adaptive attacker, and observe similar results. Furthermore, we extend our
evaluation to the MDP setting, confirming that the robustness observed in
bandit scenarios generalizes to more complex environments.

</details>


### [498] [Scalable Gaussian Processes with Latent Kronecker Structure](https://arxiv.org/abs/2506.06895)
*Jihao Andreas Lin,Sebastian Ament,Maximilian Balandat,David Eriksson,José Miguel Hernández-Lobato,Eytan Bakshy*

Main category: cs.LG

TL;DR: 提出一种利用潜在Kronecker结构的方法，解决高斯过程在大规模数据集上的计算瓶颈，显著提升效率。


<details>
  <summary>Details</summary>
Motivation: 高斯过程在大数据集上的计算可扩展性受限，现有方法（如Kronecker积）常需近似或不现实假设，且缺失数据会破坏结构。

Method: 通过将观测值的核矩阵表示为潜在Kronecker积的投影，结合迭代线性系统求解器和路径条件，实现高效精确推断。

Result: 在多达五百万样本的真实数据集（如机器人、自动机器学习和气候应用）上，方法优于现有稀疏和变分高斯过程。

Conclusion: 该方法为大规模高斯过程推断提供了高效且精确的解决方案，适用于多种实际应用。

Abstract: Applying Gaussian processes (GPs) to very large datasets remains a challenge
due to limited computational scalability. Matrix structures, such as the
Kronecker product, can accelerate operations significantly, but their
application commonly entails approximations or unrealistic assumptions. In
particular, the most common path to creating a Kronecker-structured kernel
matrix is by evaluating a product kernel on gridded inputs that can be
expressed as a Cartesian product. However, this structure is lost if any
observation is missing, breaking the Cartesian product structure, which
frequently occurs in real-world data such as time series. To address this
limitation, we propose leveraging latent Kronecker structure, by expressing the
kernel matrix of observed values as the projection of a latent Kronecker
product. In combination with iterative linear system solvers and pathwise
conditioning, our method facilitates inference of exact GPs while requiring
substantially fewer computational resources than standard iterative methods. We
demonstrate that our method outperforms state-of-the-art sparse and variational
GPs on real-world datasets with up to five million examples, including
robotics, automated machine learning, and climate applications.

</details>


### [499] [Uncertainty Estimation on Graphs with Structure Informed Stochastic Partial Differential Equations](https://arxiv.org/abs/2506.06907)
*Fred Xu,Thomas Markovich*

Main category: cs.LG

TL;DR: 提出一种基于高斯过程的图神经网络消息传递方法，提升分布偏移下不确定性估计的准确性。


<details>
  <summary>Details</summary>
Motivation: 图神经网络在不确定性估计中面临结构和标签分布随机性的双重挑战，传统方法难以应对。

Method: 通过将随机偏微分方程与图神经网络消息传递类比，设计结合时空噪声的消息传递方案。

Result: 实验表明，该方法在分布外检测任务中优于现有方法，尤其在标签信息丰富度不同的图上表现优越。

Conclusion: 该方法为图数据不确定性估计提供了更可靠且可控的解决方案。

Abstract: Graph Neural Networks have achieved impressive results across diverse network
modeling tasks, but accurately estimating uncertainty on graphs remains
difficult, especially under distributional shifts. Unlike traditional
uncertainty estimation, graph-based uncertainty must account for randomness
arising from both the graph's structure and its label distribution, which adds
complexity. In this paper, making an analogy between the evolution of a
stochastic partial differential equation (SPDE) driven by Matern Gaussian
Process and message passing using GNN layers, we present a principled way to
design a novel message passing scheme that incorporates spatial-temporal noises
motivated by the Gaussian Process approach to SPDE. Our method simultaneously
captures uncertainty across space and time and allows explicit control over the
covariance kernel smoothness, thereby enhancing uncertainty estimates on graphs
with both low and high label informativeness. Our extensive experiments on
Out-of-Distribution (OOD) detection on graph datasets with varying label
informativeness demonstrate the soundness and superiority of our model to
existing approaches.

</details>


### [500] [Graph-Based Physics-Guided Urban PM2.5 Air Quality Imputation with Constrained Monitoring Data](https://arxiv.org/abs/2506.06917)
*Shangjie Du,Hui Wei,Dong Yoon Lee,Zhizhang Hu,Shijia Pan*

Main category: cs.LG

TL;DR: GraPhy是一种基于图神经网络的物理引导学习框架，用于高分辨率空气质量建模，适用于监测数据有限的地区。


<details>
  <summary>Details</summary>
Motivation: 解决社会经济弱势地区空气质量监测数据稀疏导致建模精度低的问题。

Method: 采用物理引导的图神经网络架构，设计特定层和边特征以处理低分辨率数据。

Result: 在加州圣华金谷的实验表明，GraPhy在MSE、MAE和R2指标上表现最佳，性能提升9%-56%。

Conclusion: GraPhy在不同空间异质性水平下均优于基线模型，验证了其设计有效性。

Abstract: This work introduces GraPhy, a graph-based, physics-guided learning framework
for high-resolution and accurate air quality modeling in urban areas with
limited monitoring data. Fine-grained air quality monitoring information is
essential for reducing public exposure to pollutants. However, monitoring
networks are often sparse in socioeconomically disadvantaged regions, limiting
the accuracy and resolution of air quality modeling. To address this, we
propose a physics-guided graph neural network architecture called GraPhy with
layers and edge features designed specifically for low-resolution monitoring
data. Experiments using data from California's socioeconomically disadvantaged
San Joaquin Valley show that GraPhy achieves the overall best performance
evaluated by mean squared error (MSE), mean absolute error (MAE), and R-square
value (R2), improving the performance by 9%-56% compared to various baseline
models. Moreover, GraPhy consistently outperforms baselines across different
spatial heterogeneity levels, demonstrating the effectiveness of our model
design.

</details>


### [501] [Basis Transformers for Multi-Task Tabular Regression](https://arxiv.org/abs/2506.06926)
*Wei Min Loh,Jiaqi Shang,Pascal Poupart*

Main category: cs.LG

TL;DR: 论文提出了一种名为“basis transformers”的新架构，用于处理表格数据的挑战，如部分信息、噪声和异构结构，并在多任务回归基准测试中表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有技术难以同时处理表格数据的关键问题，如文本信息、可变列数和无元数据的新数据。

Method: 提出了一种专门设计的“basis transformers”架构，尊重表格数据的固有不变性，如层次结构和数值表示。

Result: 在OpenML-CTR23基准测试的34项任务中，中位数R²分数提高了0.338，参数数量比最佳基线少五倍，且优于预训练大型语言模型基线。

Conclusion: “basis transformers”架构在表格数据处理中表现出色，具有高效性和优越性能。

Abstract: Dealing with tabular data is challenging due to partial information, noise,
and heterogeneous structure. Existing techniques often struggle to
simultaneously address key aspects of tabular data such as textual information,
a variable number of columns, and unseen data without metadata besides column
names. We propose a novel architecture, \textit{basis transformers},
specifically designed to tackle these challenges while respecting inherent
invariances in tabular data, including hierarchical structure and the
representation of numeric values. We evaluate our design on a multi-task
tabular regression benchmark, achieving an improvement of 0.338 in the median
$R^2$ score and the lowest standard deviation across 34 tasks from the
OpenML-CTR23 benchmark. Furthermore, our model has five times fewer parameters
than the best-performing baseline and surpasses pretrained large language model
baselines -- even when initialized from randomized weights.

</details>


### [502] [Rewriting the Budget: A General Framework for Black-Box Attacks Under Cost Asymmetry](https://arxiv.org/abs/2506.06933)
*Mahdi Salmani,Alireza Abdollahpoorrostam,Seyed-Mohsen Moosavi-Dezfooli*

Main category: cs.LG

TL;DR: 本文提出了一种针对不对称查询成本的决策型黑盒对抗攻击框架，通过改进搜索策略和梯度估计过程，显著降低了总查询成本和扰动大小。


<details>
  <summary>Details</summary>
Motivation: 现有对抗攻击方法假设所有查询成本相同，而实际应用中某些查询可能成本更高（如触发额外审查或惩罚），因此需要开发更高效的算法。

Method: 提出不对称搜索（AS）和不对称梯度估计（AGREST），优化查询类型平衡以减少总攻击成本。

Result: 在多种成本设置下，新方法的总查询成本和扰动大小均优于现有方法，部分场景提升达40%。

Conclusion: 该框架可轻松集成到现有黑盒攻击中，为不对称查询成本场景提供了高效解决方案。

Abstract: Traditional decision-based black-box adversarial attacks on image classifiers
aim to generate adversarial examples by slightly modifying input images while
keeping the number of queries low, where each query involves sending an input
to the model and observing its output. Most existing methods assume that all
queries have equal cost. However, in practice, queries may incur asymmetric
costs; for example, in content moderation systems, certain output classes may
trigger additional review, enforcement, or penalties, making them more costly
than others. While prior work has considered such asymmetric cost settings,
effective algorithms for this scenario remain underdeveloped. In this paper, we
propose a general framework for decision-based attacks under asymmetric query
costs, which we refer to as asymmetric black-box attacks. We modify two core
components of existing attacks: the search strategy and the gradient estimation
process. Specifically, we propose Asymmetric Search (AS), a more conservative
variant of binary search that reduces reliance on high-cost queries, and
Asymmetric Gradient Estimation (AGREST), which shifts the sampling distribution
to favor low-cost queries. We design efficient algorithms that minimize total
attack cost by balancing different query types, in contrast to earlier methods
such as stealthy attacks that focus only on limiting expensive (high-cost)
queries. Our method can be integrated into a range of existing black-box
attacks with minimal changes. We perform both theoretical analysis and
empirical evaluation on standard image classification benchmarks. Across
various cost regimes, our method consistently achieves lower total query cost
and smaller perturbations than existing approaches, with improvements of up to
40% in some settings.

</details>


### [503] [Understanding Sharpness Dynamics in NN Training with a Minimalist Example: The Effects of Dataset Difficulty, Depth, Stochasticity, and More](https://arxiv.org/abs/2506.06940)
*Geonhui Yoo,Minhak Song,Chulhee Yun*

Main category: cs.LG

TL;DR: 论文研究了深度神经网络训练中梯度下降导致的渐进锐化现象，通过简化模型（深度线性网络）揭示了其动态机制，并探讨了数据集、网络深度、优化器随机性和步长对锐化的影响。


<details>
  <summary>Details</summary>
Motivation: 理解深度神经网络训练中渐进锐化现象的机制，填补现有研究的空白。

Method: 使用深度线性网络（每层单神经元）作为简化模型，理论分析并结合实验验证。

Result: 简化模型成功捕捉了渐进锐化现象，揭示了数据集、网络深度、优化器和步长对锐化的影响。

Conclusion: 研究提供了对神经网络训练中锐化动态的深入理解，强调了深度、训练数据和优化器之间的相互作用。

Abstract: When training deep neural networks with gradient descent, sharpness often
increases -- a phenomenon known as progressive sharpening -- before saturating
at the edge of stability. Although commonly observed in practice, the
underlying mechanisms behind progressive sharpening remain poorly understood.
In this work, we study this phenomenon using a minimalist model: a deep linear
network with a single neuron per layer. We show that this simple model
effectively captures the sharpness dynamics observed in recent empirical
studies, offering a simple testbed to better understand neural network
training. Moreover, we theoretically analyze how dataset properties, network
depth, stochasticity of optimizers, and step size affect the degree of
progressive sharpening in the minimalist model. We then empirically demonstrate
how these theoretical insights extend to practical scenarios. This study offers
a deeper understanding of sharpness dynamics in neural network training,
highlighting the interplay between depth, training data, and optimizers.

</details>


### [504] [UdonCare: Hierarchy Pruning for Unseen Domain Discovery in Predictive Healthcare](https://arxiv.org/abs/2506.06977)
*Pengfei Hu,Xiaoxue Han,Fei Wang,Yue Ning*

Main category: cs.LG

TL;DR: UdonCare利用医学本体论发现潜在域，通过层次化修剪和编码提升临床预测的领域泛化性能。


<details>
  <summary>Details</summary>
Motivation: 解决临床预测中数据分布变化导致的模型性能下降问题，特别是缺乏域标签和医学知识整合不足的挑战。

Method: 利用ICD-9-CM层次结构分组疾病，迭代修剪细粒度域，编码优化域，并采用Siamese推理机制分离域信号与患者特征。

Result: 在MIMIC-III和MIMIC-IV数据集上表现优于其他基线方法，尤其在域差异显著时。

Conclusion: UdonCare展示了医学知识在提升领域泛化中的潜力，适用于实际医疗应用。

Abstract: Domain generalization has become a critical challenge in clinical prediction,
where patient cohorts often exhibit shifting data distributions that degrade
model performance. Typical domain generalization approaches struggle in
real-world healthcare settings for two main reasons: (1) patient-specific
domain labels are typically unavailable, making domain discovery especially
difficult; (2) purely data-driven approaches overlook key clinical insights,
leading to a gap in medical knowledge integration. To address these problems,
we leverage hierarchical medical ontologies like the ICD-9-CM hierarchy to
group diseases into higher-level categories and discover more flexible latent
domains. In this paper, we introduce UdonCare, a hierarchy-guided framework
that iteratively prunes fine-grained domains, encodes these refined domains,
and applies a Siamese-type inference mechanism to separate domain-related
signals from patient-level features. Experimental results on clinical datasets
(MIMIC-III and MIMIC-IV) show that the proposed model achieves higher
performance compared to other domain generalization baselines when substantial
domain gaps presents, highlighting the untapped potential of medical knowledge
for enhancing domain generalization in practical healthcare applications.

</details>


### [505] [Near Optimal Non-asymptotic Sample Complexity of 1-Identification](https://arxiv.org/abs/2506.06978)
*Zitian Li,Wang Chi Cheung*

Main category: cs.LG

TL;DR: 论文研究了多臂老虎机中的1-识别问题，提出新算法SEE，填补了非渐近分析的空白，实现了近乎最优的样本复杂度。


<details>
  <summary>Details</summary>
Motivation: 现有文献中关于1-识别问题的非渐近分析尚不明确，本文旨在填补这一空白。

Method: 设计了新算法Sequential-Exploration-Exploitation (SEE)，并从非渐近角度进行理论分析。

Result: SEE算法在样本复杂度上实现了近乎最优的上界和下界匹配，差距仅为多项式对数因子。数值实验验证了算法的有效性。

Conclusion: SEE算法在1-识别问题中表现优异，填补了非渐近分析的空白，为多臂老虎机问题提供了新思路。

Abstract: Motivated by an open direction in existing literature, we study the
1-identification problem, a fundamental multi-armed bandit formulation on pure
exploration. The goal is to determine whether there exists an arm whose mean
reward is at least a known threshold $\mu_0$, or to output None if it believes
such an arm does not exist. The agent needs to guarantee its output is correct
with probability at least $1-\delta$. Degenne & Koolen 2019 has established the
asymptotically tight sample complexity for the 1-identification problem, but
they commented that the non-asymptotic analysis remains unclear. We design a
new algorithm Sequential-Exploration-Exploitation (SEE), and conduct
theoretical analysis from the non-asymptotic perspective. Novel to the
literature, we achieve near optimality, in the sense of matching upper and
lower bounds on the pulling complexity. The gap between the upper and lower
bounds is up to a polynomial logarithmic factor. The numerical result also
indicates the effectiveness of our algorithm, compared to existing benchmarks.

</details>


### [506] [MoXGATE: Modality-aware cross-attention for multi-omic gastrointestinal cancer sub-type classification](https://arxiv.org/abs/2506.06980)
*Sajib Acharjee Dip,Uddip Acharjee Shuvo,Dipanwita Mallick,Abrar Rahman Abir,Liqing Zhang*

Main category: cs.LG

TL;DR: MoXGATE是一种基于跨注意力和可学习模态权重的深度学习框架，用于多组学数据整合，显著提升癌症亚型分类性能。


<details>
  <summary>Details</summary>
Motivation: 癌症亚型分类对个性化治疗和预后评估至关重要，但多组学数据的异质性使其整合具有挑战性。

Method: 提出MoXGATE框架，利用跨注意力和模态权重增强多组学特征融合，并应用焦点损失解决数据不平衡问题。

Result: 在GIAC和BRCA数据集上达到95%的分类准确率，优于现有方法，且能泛化到未见过的癌症类型。

Conclusion: MoXGATE在多组学癌症亚型分类中表现出色，具有高性能和生物学泛化能力。

Abstract: Cancer subtype classification is crucial for personalized treatment and
prognostic assessment. However, effectively integrating multi-omic data remains
challenging due to the heterogeneous nature of genomic, epigenomic, and
transcriptomic features. In this work, we propose Modality-Aware
Cross-Attention MoXGATE, a novel deep-learning framework that leverages
cross-attention and learnable modality weights to enhance feature fusion across
multiple omics sources. Our approach effectively captures inter-modality
dependencies, ensuring robust and interpretable integration. Through
experiments on Gastrointestinal Adenocarcinoma (GIAC) and Breast Cancer (BRCA)
datasets from TCGA, we demonstrate that MoXGATE outperforms existing methods,
achieving 95\% classification accuracy. Ablation studies validate the
effectiveness of cross-attention over simple concatenation and highlight the
importance of different omics modalities. Moreover, our model generalizes well
to unseen cancer types e.g., breast cancer, underscoring its adaptability. Key
contributions include (1) a cross-attention-based multi-omic integration
framework, (2) modality-weighted fusion for enhanced interpretability, (3)
application of focal loss to mitigate data imbalance, and (4) validation across
multiple cancer subtypes. Our results indicate that MoXGATE is a promising
approach for multi-omic cancer subtype classification, offering improved
performance and biological generalizability.

</details>


### [507] [Certified Unlearning for Neural Networks](https://arxiv.org/abs/2506.06985)
*Anastasia Koloskova,Youssef Allouah,Animesh Jha,Rachid Guerraoui,Sanmi Koyejo*

Main category: cs.LG

TL;DR: 提出一种新的认证机器遗忘方法，通过噪声微调保留数据实现可证明的遗忘保证，无需对损失函数做假设。


<details>
  <summary>Details</summary>
Motivation: 解决隐私问题和法规要求（如“被遗忘权”），现有方法假设严格或缺乏形式化保证。

Method: 利用遗忘与随机后处理隐私放大的联系，通过噪声微调保留数据实现认证遗忘。

Result: 理论分析效率与准确性权衡，实证表明方法优于现有基线，实现形式化遗忘保证。

Conclusion: 该方法广泛适用，无需假设损失函数，有效实现认证遗忘。

Abstract: We address the problem of machine unlearning, where the goal is to remove the
influence of specific training data from a model upon request, motivated by
privacy concerns and regulatory requirements such as the "right to be
forgotten." Unfortunately, existing methods rely on restrictive assumptions or
lack formal guarantees. To this end, we propose a novel method for certified
machine unlearning, leveraging the connection between unlearning and privacy
amplification by stochastic post-processing. Our method uses noisy fine-tuning
on the retain data, i.e., data that does not need to be removed, to ensure
provable unlearning guarantees. This approach requires no assumptions about the
underlying loss function, making it broadly applicable across diverse settings.
We analyze the theoretical trade-offs in efficiency and accuracy and
demonstrate empirically that our method not only achieves formal unlearning
guarantees but also performs effectively in practice, outperforming existing
baselines. Our code is available at
https://github.com/stair-lab/certified-unlearningneural-networks-icml-2025

</details>


### [508] [Fully Explainable Classification Models Using Hyperblocks](https://arxiv.org/abs/2506.06986)
*Austin Snyder,Ryan Gallagher,Boris Kovalerchuk*

Main category: cs.LG

TL;DR: 论文提出了一种基于Hyperblocks的简化算法，旨在提升模型可解释性、减少训练时间和复杂度，同时保持准确性。


<details>
  <summary>Details</summary>
Motivation: 增强Hyperblocks的可解释性，降低模型复杂度，使领域专家无需机器学习背景即可理解模型决策逻辑。

Method: 引入Hyperblock简化算法，包括去除冗余属性、分析重叠块、创建分离单元，并结合k-NN作为后备机制。

Result: 在WBC和MNIST数据集上，模型在保持高准确性的同时显著降低了复杂度。

Conclusion: 该方法为高维大数据集提供了一种透明且高效的替代方案，适用于需要信任和清晰度的领域。

Abstract: Building on existing work with Hyperblocks, which classify data using minimum
and maximum bounds for each attribute, we focus on enhancing interpretability,
decreasing training time, and reducing model complexity without sacrificing
accuracy. This system allows subject matter experts (SMEs) to directly inspect
and understand the model's decision logic without requiring extensive machine
learning expertise. To reduce Hyperblock complexity while retaining
performance, we introduce a suite of algorithms for Hyperblock simplification.
These include removing redundant attributes, removing redundant blocks through
overlap analysis, and creating disjunctive units. These methods eliminate
unnecessary parameters, dramatically reducing model size without harming
classification power. We increase robustness by introducing an interpretable
fallback mechanism using k-Nearest Neighbor (k-NN) classifiers for points not
covered by any block, ensuring complete data coverage while preserving model
transparency. Our results demonstrate that interpretable models can scale to
high-dimensional, large-volume datasets while maintaining competitive accuracy.
On benchmark datasets such as WBC (9-D), we achieve strong predictive
performance with significantly reduced complexity. On MNIST (784-D), our method
continues to improve through tuning and simplification, showing promise as a
transparent alternative to black-box models in domains where trust, clarity,
and control are crucial.

</details>


### [509] [Modified K-means Algorithm with Local Optimality Guarantees](https://arxiv.org/abs/2506.06990)
*Mingyi Li,Michael R. Metel,Akiko Takeda*

Main category: cs.LG

TL;DR: 本文分析了K-means算法的局部最优性保证，提出了改进方法以确保局部最优性，并通过实验验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 尽管K-means算法在机器学习中被广泛研究，但其局部最优性保证缺乏严格分析。本文旨在填补这一空白。

Method: 提出了对K-means算法的简单修改，确保其在连续和离散意义上的局部最优性，同时保持与原算法相同的计算复杂度。

Result: 实验表明，原始K-means算法在实践中并不总能找到局部最优解，而改进方法能提供更优的聚类结果。

Conclusion: 改进后的K-means算法在保持计算效率的同时，显著提升了局部最优性，适用于更广泛的Bregman散度度量。

Abstract: The K-means algorithm is one of the most widely studied clustering algorithms
in machine learning. While extensive research has focused on its ability to
achieve a globally optimal solution, there still lacks a rigorous analysis of
its local optimality guarantees. In this paper, we first present conditions
under which the K-means algorithm converges to a locally optimal solution.
Based on this, we propose simple modifications to the K-means algorithm which
ensure local optimality in both the continuous and discrete sense, with the
same computational complexity as the original K-means algorithm. As the
dissimilarity measure, we consider a general Bregman divergence, which is an
extension of the squared Euclidean distance often used in the K-means
algorithm. Numerical experiments confirm that the K-means algorithm does not
always find a locally optimal solution in practice, while our proposed methods
provide improved locally optimal solutions with reduced clustering loss. Our
code is available at https://github.com/lmingyi/LO-K-means.

</details>


### [510] [Towards Physics-informed Diffusion for Anomaly Detection in Trajectories](https://arxiv.org/abs/2506.06999)
*Arun Sharma,Mingzhou Yang,Majid Farhadloo,Subhankar Ghosh,Bharat Jayaprakash,Shashi Shekhar*

Main category: cs.LG

TL;DR: 提出了一种基于物理知识的扩散模型，用于检测异常轨迹，以应对GPS欺骗问题，提高了检测精度并降低了错误率。


<details>
  <summary>Details</summary>
Motivation: 解决国际水域非法活动（如非法捕鱼和石油走私）中的GPS欺骗问题，同时应对AI生成虚假轨迹和数据稀疏的挑战。

Method: 结合物理运动学约束的扩散模型，用于识别不符合物理规律的轨迹。

Result: 在真实数据集（海事和城市领域）中表现出更高的异常检测准确率和更低的轨迹生成误差。

Conclusion: 提出的物理知识扩散模型在异常轨迹检测中表现优异，为GPS欺骗问题提供了有效解决方案。

Abstract: Given trajectory data, a domain-specific study area, and a user-defined
threshold, we aim to find anomalous trajectories indicative of possible GPS
spoofing (e.g., fake trajectory). The problem is societally important to curb
illegal activities in international waters, such as unauthorized fishing and
illicit oil transfers. The problem is challenging due to advances in AI
generated in deep fakes generation (e.g., additive noise, fake trajectories)
and lack of adequate amount of labeled samples for ground-truth verification.
Recent literature shows promising results for anomalous trajectory detection
using generative models despite data sparsity. However, they do not consider
fine-scale spatiotemporal dependencies and prior physical knowledge, resulting
in higher false-positive rates. To address these limitations, we propose a
physics-informed diffusion model that integrates kinematic constraints to
identify trajectories that do not adhere to physical laws. Experimental results
on real-world datasets in the maritime and urban domains show that the proposed
framework results in higher prediction accuracy and lower estimation error rate
for anomaly detection and trajectory generation methods, respectively. Our
implementation is available at
https://github.com/arunshar/Physics-Informed-Diffusion-Probabilistic-Model.

</details>


### [511] [End-to-End Probabilistic Framework for Learning with Hard Constraints](https://arxiv.org/abs/2506.07003)
*Utkarsh Utkarsh,Danielle C. Maddix,Ruijun Ma,Michael W. Mahoney,Yuyang Wang*

Main category: cs.LG

TL;DR: ProbHardE2E是一个通用的概率预测框架，通过新颖的DPPL层实现硬约束和不确定性量化，适用于多种神经网络架构。


<details>
  <summary>Details</summary>
Motivation: 现有方法通常通过后处理或推理阶段满足约束，且依赖分布假设，限制了模型的灵活性和鲁棒性。

Method: 使用可微分概率投影层（DPPL）实现端到端学习，支持非线性约束和严格评分规则优化。

Result: 在偏微分方程学习和时间序列预测中展示了广泛适用性，优于依赖分布假设的现有方法。

Conclusion: ProbHardE2E为硬约束和不确定性量化提供了一种通用且灵活的方法，适用于多种领域。

Abstract: We present a general purpose probabilistic forecasting framework,
ProbHardE2E, to learn systems that can incorporate operational/physical
constraints as hard requirements. ProbHardE2E enforces hard constraints by
exploiting variance information in a novel way; and thus it is also capable of
performing uncertainty quantification (UQ) on the model. Our methodology uses a
novel differentiable probabilistic projection layer (DPPL) that can be combined
with a wide range of neural network architectures. This DPPL allows the model
to learn the system in an end-to-end manner, compared to other approaches where
the constraints are satisfied either through a post-processing step or at
inference. In addition, ProbHardE2E can optimize a strictly proper scoring
rule, without making any distributional assumptions on the target, which
enables it to obtain robust distributional estimates (in contrast to existing
approaches that generally optimize likelihood-based objectives, which are
heavily biased by their distributional assumptions and model choices); and it
can incorporate a range of non-linear constraints (increasing the power of
modeling and flexibility). We apply ProbHardE2E to problems in learning partial
differential equations with uncertainty estimates and to probabilistic
time-series forecasting, showcasing it as a broadly applicable general setup
that connects these seemingly disparate domains.

</details>


### [512] [Comparison of Lightweight Methods for Vehicle Dynamics-Based Driver Drowsiness Detection](https://arxiv.org/abs/2506.07014)
*Yutaro Nakagama,Daisuke Ishii,Kazuki Yoshizoe*

Main category: cs.LG

TL;DR: 本文通过透明公平的框架比较了基于车辆动力学的驾驶员疲劳检测方法，提出了一种轻量级随机森林方法，准确率达88%。


<details>
  <summary>Details</summary>
Motivation: 解决现有驾驶员疲劳检测方法性能指标不可靠、数据集不公开及数据泄漏问题。

Method: 开发了一个框架，从公开数据集中提取特征，并实现三种现有方法和一种随机森林方法。

Result: 随机森林方法在实验中表现最佳，准确率达88%。

Conclusion: 研究揭示了非标准化开发方法的固有问题，并展示了高性能方法的实现。

Abstract: Driver drowsiness detection (DDD) prevents road accidents caused by driver
fatigue. Vehicle dynamics-based DDD has been proposed as a method that is both
economical and high performance. However, there are concerns about the
reliability of performance metrics and the reproducibility of many of the
existing methods. For instance, some previous studies seem to have a data
leakage issue among training and test datasets, and many do not openly provide
the datasets they used. To this end, this paper aims to compare the performance
of representative vehicle dynamics-based DDD methods under a transparent and
fair framework that uses a public dataset. We first develop a framework for
extracting features from an open dataset by Aygun et al. and performing DDD
with lightweight ML models; the framework is carefully designed to support a
variety of onfigurations. Second, we implement three existing representative
methods and a concise random forest (RF)-based method in the framework.
Finally, we report the results of experiments to verify the reproducibility and
clarify the performance of DDD based on common metrics. Among the evaluated
methods, the RF-based method achieved the highest accuracy of 88 %. Our
findings imply the issues inherent in DDD methods developed in a non-standard
manner, and demonstrate a high performance method implemented appropriately.

</details>


### [513] [AlphaSteer: Learning Refusal Steering with Principled Null-Space Constraint](https://arxiv.org/abs/2506.07022)
*Leheng Sheng,Changshuo Shen,Weixiang Zhao,Junfeng Fang,Xiaohao Liu,Zhenkai Liang,Xiang Wang,An Zhang,Tat-Seng Chua*

Main category: cs.LG

TL;DR: AlphaSteer是一种基于理论的方法，通过优化激活导向向量，在提升LLM安全性的同时保持其通用能力。


<details>
  <summary>Details</summary>
Motivation: 随着LLM在现实应用中的广泛部署，确保其能够拒绝恶意提示（如越狱攻击）是安全可靠使用的关键。现有方法在安全性和实用性之间存在权衡。

Method: AlphaSteer将激活导向视为可学习过程，通过两个学习目标（实用性保持和安全性增强）优化导向向量。前者通过零向量约束保持实用性，后者通过线性回归增强安全性。

Result: 实验表明，AlphaSteer在多种越狱攻击和实用性基准测试中显著提升了LLM的安全性，且未损害其通用能力。

Conclusion: AlphaSteer是一种理论扎实且实证有效的方法，成功解决了安全性与实用性之间的权衡问题。

Abstract: As LLMs are increasingly deployed in real-world applications, ensuring their
ability to refuse malicious prompts, especially jailbreak attacks, is essential
for safe and reliable use. Recently, activation steering has emerged as an
effective approach for enhancing LLM safety by adding a refusal direction
vector to internal activations of LLMs during inference, which will further
induce the refusal behaviors of LLMs. However, indiscriminately applying
activation steering fundamentally suffers from the trade-off between safety and
utility, since the same steering vector can also lead to over-refusal and
degraded performance on benign prompts. Although prior efforts, such as vector
calibration and conditional steering, have attempted to mitigate this
trade-off, their lack of theoretical grounding limits their robustness and
effectiveness. To better address the trade-off between safety and utility, we
present a theoretically grounded and empirically effective activation steering
method called AlphaSteer. Specifically, it considers activation steering as a
learnable process with two principled learning objectives: utility preservation
and safety enhancement. For utility preservation, it learns to construct a
nearly zero vector for steering benign data, with the null-space constraints.
For safety enhancement, it learns to construct a refusal direction vector for
steering malicious data, with the help of linear regression. Experiments across
multiple jailbreak attacks and utility benchmarks demonstrate the effectiveness
of AlphaSteer, which significantly improves the safety of LLMs without
compromising general capabilities. Our codes are available at
https://github.com/AlphaLab-USTC/AlphaSteer.

</details>


### [514] [Mixture Experts with Test-Time Self-Supervised Aggregation for Tabular Imbalanced Regression](https://arxiv.org/abs/2506.07033)
*Yung-Chien Wang,Kuang-Da Wang,Wei-Yao Wang,Wen-Chih Peng*

Main category: cs.LG

TL;DR: MATI是一种针对表格不平衡回归任务的新方法，通过区域感知混合专家和测试时自监督专家聚合，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 表格数据在现实应用中广泛存在，但回归任务中的数据不平衡问题尚未充分研究，现有方法假设测试分布已知且平衡，实际中可能导致性能下降。

Method: MATI采用高斯混合模型捕获相关区域，训练区域特定专家，并通过测试时自监督动态调整专家权重以适应不同测试分布。

Result: 在四种真实数据集上，MATI在三种测试分布下平均MAE提升7.1%。

Conclusion: MATI有效解决了表格不平衡回归问题，具有实际应用潜力。

Abstract: Tabular data serve as a fundamental and ubiquitous representation of
structured information in numerous real-world applications, e.g., finance and
urban planning. In the realm of tabular imbalanced applications, data imbalance
has been investigated in classification tasks with insufficient instances in
certain labels, causing the model's ineffective generalizability. However, the
imbalance issue of tabular regression tasks is underexplored, and yet is
critical due to unclear boundaries for continuous labels and simplifying
assumptions in existing imbalance regression work, which often rely on known
and balanced test distributions. Such assumptions may not hold in practice and
can lead to performance degradation. To address these issues, we propose MATI:
Mixture Experts with Test-Time Self-Supervised Aggregation for Tabular
Imbalance Regression, featuring two key innovations: (i) the Region-Aware
Mixture Expert, which adopts a Gaussian Mixture Model to capture the underlying
related regions. The statistical information of each Gaussian component is then
used to synthesize and train region-specific experts to capture the unique
characteristics of their respective regions. (ii) Test-Time Self-Supervised
Expert Aggregation, which dynamically adjusts region expert weights based on
test data features to reinforce expert adaptation across varying test
distributions. We evaluated MATI on four real-world tabular imbalance
regression datasets, including house pricing, bike sharing, and age prediction.
To reflect realistic deployment scenarios, we adopted three types of test
distributions: a balanced distribution with uniform target frequencies, a
normal distribution that follows the training data, and an inverse distribution
that emphasizes rare target regions. On average across these three test
distributions, MATI achieved a 7.1% improvement in MAE compared to existing
methods.

</details>


### [515] [Efficient $Q$-Learning and Actor-Critic Methods for Robust Average Reward Reinforcement Learning](https://arxiv.org/abs/2506.07040)
*Yang Xu,Swetha Ganesh,Vaneet Aggarwal*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: We present the first $Q$-learning and actor-critic algorithms for robust
average reward Markov Decision Processes (MDPs) with non-asymptotic convergence
under contamination, TV distance and Wasserstein distance uncertainty sets. We
show that the robust $Q$ Bellman operator is a strict contractive mapping with
respect to a carefully constructed semi-norm with constant functions being
quotiented out. This property supports a stochastic approximation update, that
learns the optimal robust $Q$ function in $\tilde{\cO}(\epsilon^{-2})$ samples.
We also show that the same idea can be used for robust $Q$ function estimation,
which can be further used for critic estimation. Coupling it with theories in
robust policy mirror descent update, we present a natural actor-critic
algorithm that attains an $\epsilon$-optimal robust policy in
$\tilde{\cO}(\epsilon^{-3})$ samples. These results advance the theory of
distributionally robust reinforcement learning in the average reward setting.

</details>


### [516] [FairPFN: A Tabular Foundation Model for Causal Fairness](https://arxiv.org/abs/2506.07049)
*Jake Robertson,Noah Hollmann,Samuel Müller,Noor Awad,Frank Hutter*

Main category: cs.LG

TL;DR: FairPFN是一种无需因果模型先验知识的表格基础模型，通过合成因果公平数据预训练，有效识别和消除预测中的保护属性因果效应。


<details>
  <summary>Details</summary>
Motivation: 机器学习系统在关键领域应用广泛，但历史数据中的偏见可能导致决策加剧社会不平等。现有因果公平框架依赖已知因果模型，限制了其适用性。

Method: 提出FairPFN，预训练于合成因果公平数据的表格基础模型，无需因果模型知识即可处理复杂公平问题。

Result: FairPFN在多种手工和真实场景中表现优异，能有效识别和消除保护属性的因果效应。

Conclusion: FairPFN为复杂公平问题提供了更易用的因果公平解决方案，推动了该领域的未来研究。

Abstract: Machine learning (ML) systems are utilized in critical sectors, such as
healthcare, law enforcement, and finance. However, these systems are often
trained on historical data that contains demographic biases, leading to ML
decisions that perpetuate or exacerbate existing social inequalities. Causal
fairness provides a transparent, human-in-the-loop framework to mitigate
algorithmic discrimination, aligning closely with legal doctrines of direct and
indirect discrimination. However, current causal fairness frameworks hold a key
limitation in that they assume prior knowledge of the correct causal model,
restricting their applicability in complex fairness scenarios where causal
models are unknown or difficult to identify. To bridge this gap, we propose
FairPFN, a tabular foundation model pre-trained on synthetic causal fairness
data to identify and mitigate the causal effects of protected attributes in its
predictions. FairPFN's key contribution is that it requires no knowledge of the
causal model and still demonstrates strong performance in identifying and
removing protected causal effects across a diverse set of hand-crafted and
real-world scenarios relative to robust baseline methods. FairPFN paves the way
for promising future research, making causal fairness more accessible to a
wider variety of complex fairness problems.

</details>


### [517] [Policy Gradient with Tree Search: Avoiding Local Optimas through Lookahead](https://arxiv.org/abs/2506.07054)
*Uri Koren,Navdeep Kumar,Uri Gadot,Giorgia Ramponi,Kfir Yehuda Levy,Shie Mannor*

Main category: cs.LG

TL;DR: PGTS方法通过结合树搜索增强策略优化，减少不良稳定点，提升最差性能表现。


<details>
  <summary>Details</summary>
Motivation: 传统策略梯度方法易陷入局部最优，尤其在复杂环境中。PGTS旨在通过树搜索提升策略优化效果。

Method: PGTS引入m步前瞻机制，理论分析表明增加搜索深度可减少不良稳定点。

Result: 实验证明PGTS能展现“远见”，避开局部陷阱，优于标准PG方法。

Conclusion: PGTS通过树搜索显著提升策略梯度方法的性能，尤其在复杂环境中表现优异。

Abstract: Classical policy gradient (PG) methods in reinforcement learning frequently
converge to suboptimal local optima, a challenge exacerbated in large or
complex environments. This work investigates Policy Gradient with Tree Search
(PGTS), an approach that integrates an $m$-step lookahead mechanism to enhance
policy optimization. We provide theoretical analysis demonstrating that
increasing the tree search depth $m$-monotonically reduces the set of
undesirable stationary points and, consequently, improves the worst-case
performance of any resulting stationary policy. Critically, our analysis
accommodates practical scenarios where policy updates are restricted to states
visited by the current policy, rather than requiring updates across the entire
state space. Empirical evaluations on diverse MDP structures, including Ladder,
Tightrope, and Gridworld environments, illustrate PGTS's ability to exhibit
"farsightedness," navigate challenging reward landscapes, escape local traps
where standard PG fails, and achieve superior solutions.

</details>


### [518] [State Entropy Regularization for Robust Reinforcement Learning](https://arxiv.org/abs/2506.07085)
*Uri Koren,Yonatan Ashlag,Mirco Mutti,Esther Derman,Pierre-Luc Bacon,Shie Mannor*

Main category: cs.LG

TL;DR: 本文研究了状态熵正则化在强化学习中的鲁棒性优势，特别是对结构化扰动和空间相关扰动的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 状态熵正则化在强化学习中表现出更好的探索性和样本效率，但其理论保证尚未被充分研究。本文旨在填补这一空白。

Method: 通过理论分析和实验对比状态熵与策略熵正则化，研究其在奖励和转移不确定性下的表现。

Result: 状态熵正则化对结构化扰动和空间相关扰动具有更强的鲁棒性，但其优势对策略评估的样本数量更敏感。

Conclusion: 状态熵正则化在特定扰动下表现优异，但需注意其样本敏感性。

Abstract: State entropy regularization has empirically shown better exploration and
sample complexity in reinforcement learning (RL). However, its theoretical
guarantees have not been studied. In this paper, we show that state entropy
regularization improves robustness to structured and spatially correlated
perturbations. These types of variation are common in transfer learning but
often overlooked by standard robust RL methods, which typically focus on small,
uncorrelated changes. We provide a comprehensive characterization of these
robustness properties, including formal guarantees under reward and transition
uncertainty, as well as settings where the method performs poorly. Much of our
analysis contrasts state entropy with the widely used policy entropy
regularization, highlighting their different benefits. Finally, from a
practical standpoint, we illustrate that compared with policy entropy, the
robustness advantages of state entropy are more sensitive to the number of
rollouts used for policy evaluation.

</details>


### [519] [Pointwise confidence estimation in the non-linear $\ell^2$-regularized least squares](https://arxiv.org/abs/2506.07088)
*Ilja Kuzborskij,Yasin Abbasi Yadkori*

Main category: cs.LG

TL;DR: 论文提出了一种在高概率非渐近条件下，针对固定设计的ℓ²正则化非线性最小二乘问题的置信度估计方法，特别关注局部最小化器的置信度估计。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于为非线性最小二乘问题中的局部最小化器提供点式置信度估计，并确保其随测试数据与训练数据的相似性而变化。

Method: 方法包括推导基于逆Hessian矩阵的加权范数的置信度边界，并提出高效计算该范数的算法。

Result: 结果显示，该方法在覆盖率和宽度之间取得了更好的平衡，优于传统的自助法置信度估计。

Conclusion: 结论表明，该方法为非渐近置信度估计提供了有效工具，尤其适用于非线性预测器如神经网络。

Abstract: We consider a high-probability non-asymptotic confidence estimation in the
$\ell^2$-regularized non-linear least-squares setting with fixed design. In
particular, we study confidence estimation for local minimizers of the
regularized training loss. We show a pointwise confidence bound, meaning that
it holds for the prediction on any given fixed test input $x$. Importantly, the
proposed confidence bound scales with similarity of the test input to the
training data in the implicit feature space of the predictor (for instance,
becoming very large when the test input lies far outside of the training data).
This desirable last feature is captured by the weighted norm involving the
inverse-Hessian matrix of the objective function, which is a generalized
version of its counterpart in the linear setting, $x^{\top} \text{Cov}^{-1} x$.
Our generalized result can be regarded as a non-asymptotic counterpart of the
classical confidence interval based on asymptotic normality of the MLE
estimator. We propose an efficient method for computing the weighted norm,
which only mildly exceeds the cost of a gradient computation of the loss
function. Finally, we complement our analysis with empirical evidence showing
that the proposed confidence bound provides better coverage/width trade-off
compared to a confidence estimation by bootstrapping, which is a gold-standard
method in many applications involving non-linear predictors such as neural
networks.

</details>


### [520] [Patient Similarity Computation for Clinical Decision Support: An Efficient Use of Data Transformation, Combining Static and Time Series Data](https://arxiv.org/abs/2506.07092)
*Joydeb Kumar Sana,Mohammad M. Masud,M Sohel Rahman,M Saifur Rahman*

Main category: cs.LG

TL;DR: 本文提出了一种基于数据转换方法的分布式患者相似性计算（DPSC）技术，结合时间序列和静态数据，显著提升了预测性能和计算效率。


<details>
  <summary>Details</summary>
Motivation: 患者相似性计算（PSC）是医疗信息学中的核心问题，旨在通过患者历史临床记录衡量相似性以支持临床决策。

Method: 采用数据转换方法（如aWOE和Z-score）处理静态数据，结合动态时间规整（DTW）处理时间序列数据，并引入分布式计算以解决DTW在大数据中的计算效率问题。

Result: 在冠状动脉疾病和充血性心力衰竭的预测中，AUC、准确率和F值分别提升了11.4%-15.9%、10.2%-10.5%和12.6%-21.9%，计算时间减少了40%。

Conclusion: 提出的DPSC方法在提升预测性能和计算效率方面表现出色，为临床决策提供了有效支持。

Abstract: Patient similarity computation (PSC) is a fundamental problem in healthcare
informatics. The aim of the patient similarity computation is to measure the
similarity among patients according to their historical clinical records, which
helps to improve clinical decision support. This paper presents a novel
distributed patient similarity computation (DPSC) technique based on data
transformation (DT) methods, utilizing an effective combination of time series
and static data. Time series data are sensor-collected patients' information,
including metrics like heart rate, blood pressure, Oxygen saturation,
respiration, etc. The static data are mainly patient background and demographic
data, including age, weight, height, gender, etc. Static data has been used for
clustering the patients. Before feeding the static data to the machine learning
model adaptive Weight-of-Evidence (aWOE) and Z-score data transformation (DT)
methods have been performed, which improve the prediction performances. In
aWOE-based patient similarity models, sensitive patient information has been
processed using aWOE which preserves the data privacy of the trained models. We
used the Dynamic Time Warping (DTW) approach, which is robust and very popular,
for time series similarity. However, DTW is not suitable for big data due to
the significant computational run-time. To overcome this problem, distributed
DTW computation is used in this study. For Coronary Artery Disease, our DT
based approach boosts prediction performance by as much as 11.4%, 10.20%, and
12.6% in terms of AUC, accuracy, and F-measure, respectively. In the case of
Congestive Heart Failure (CHF), our proposed method achieves performance
enhancement up to 15.9%, 10.5%, and 21.9% for the same measures, respectively.
The proposed method reduces the computation time by as high as 40%.

</details>


### [521] [Filling the Missings: Spatiotemporal Data Imputation by Conditional Diffusion](https://arxiv.org/abs/2506.07099)
*Wenying He,Jieling Huang,Junhua Gu,Ji Zhang,Yude Bai*

Main category: cs.LG

TL;DR: CoFILL是一种基于条件扩散模型的新方法，用于时空数据填补，通过双流架构处理时空特征，显著提升了填补精度。


<details>
  <summary>Details</summary>
Motivation: 时空数据中的缺失值问题对现代应用（如环境监测和交通管理）构成挑战，现有方法难以有效建模时空依赖关系且易产生累积误差。

Method: CoFILL利用扩散模型生成高质量填补值，采用双流架构并行处理时域和频域特征，融合互补特征以捕捉数据中的快速波动和潜在模式。

Result: 实验表明，CoFILL的噪声预测网络能有效生成符合真实数据分布的填补值，且在填补精度上优于现有方法。

Conclusion: CoFILL通过创新的扩散模型和双流架构，解决了时空数据填补中的关键问题，为实际应用提供了可靠工具。

Abstract: Missing data in spatiotemporal systems presents a significant challenge for
modern applications, ranging from environmental monitoring to urban traffic
management. The integrity of spatiotemporal data often deteriorates due to
hardware malfunctions and software failures in real-world deployments. Current
approaches based on machine learning and deep learning struggle to model the
intricate interdependencies between spatial and temporal dimensions effectively
and, more importantly, suffer from cumulative errors during the data imputation
process, which propagate and amplify through iterations. To address these
limitations, we propose CoFILL, a novel Conditional Diffusion Model for
spatiotemporal data imputation. CoFILL builds on the inherent advantages of
diffusion models to generate high-quality imputations without relying on
potentially error-prone prior estimates. It incorporates an innovative
dual-stream architecture that processes temporal and frequency domain features
in parallel. By fusing these complementary features, CoFILL captures both rapid
fluctuations and underlying patterns in the data, which enables more robust
imputation. The extensive experiments reveal that CoFILL's noise prediction
network successfully transforms random noise into meaningful values that align
with the true data distribution. The results also show that CoFILL outperforms
state-of-the-art methods in imputation accuracy. The source code is publicly
available at https://github.com/joyHJL/CoFILL.

</details>


### [522] [Towards Universal Offline Black-Box Optimization via Learning Language Model Embeddings](https://arxiv.org/abs/2506.07109)
*Rong-Xi Tan,Ming Chen,Ke Xue,Yao Wang,Yaoyuan Wang,Sheng Fu,Chao Qian*

Main category: cs.LG

TL;DR: 本文探讨了利用语言模型（LMs）的嵌入能力实现通用黑盒优化（BBO）的方法，提出了端到端学习框架和潜在空间学习策略，并在实验中验证了其有效性和通用性。


<details>
  <summary>Details</summary>
Motivation: 传统离线BBO方法因缺乏统一表示而局限于单任务和固定维度设置，无法实现跨领域通用优化。语言模型的嵌入能力为解决这一问题提供了新思路。

Method: 提出了基于语言模型的端到端学习框架（如next-token预测）和潜在空间学习策略，并通过开源学术数据训练验证。

Result: 实验证明所提方法具有通用性和有效性，能够克服传统BBO的局限性。

Conclusion: 结合语言模型先验和学习字符串嵌入空间，可为通用BBO算法的发展铺平道路。

Abstract: The pursuit of universal black-box optimization (BBO) algorithms is a
longstanding goal. However, unlike domains such as language or vision, where
scaling structured data has driven generalization, progress in offline BBO
remains hindered by the lack of unified representations for heterogeneous
numerical spaces. Thus, existing offline BBO approaches are constrained to
single-task and fixed-dimensional settings, failing to achieve cross-domain
universal optimization. Recent advances in language models (LMs) offer a
promising path forward: their embeddings capture latent relationships in a
unifying way, enabling universal optimization across different data types
possible. In this paper, we discuss multiple potential approaches, including an
end-to-end learning framework in the form of next-token prediction, as well as
prioritizing the learning of latent spaces with strong representational
capabilities. To validate the effectiveness of these methods, we collect
offline BBO tasks and data from open-source academic works for training.
Experiments demonstrate the universality and effectiveness of our proposed
methods. Our findings suggest that unifying language model priors and learning
string embedding space can overcome traditional barriers in universal BBO,
paving the way for general-purpose BBO algorithms. The code is provided at
https://github.com/lamda-bbo/universal-offline-bbo.

</details>


### [523] [Quality-Diversity Red-Teaming: Automated Generation of High-Quality and Diverse Attackers for Large Language Models](https://arxiv.org/abs/2506.07121)
*Ren-Jian Wang,Ke Xue,Zeyu Qin,Ziniu Li,Sheng Tang,Hao-Tian Li,Shengcai Liu,Chao Qian*

Main category: cs.LG

TL;DR: QDRT框架通过行为条件训练和多攻击者模型，提升了对抗提示的多样性和有效性，改进了LLM的安全性评估。


<details>
  <summary>Details</summary>
Motivation: 现有红队方法在对抗提示多样性和攻击风格覆盖上存在不足，需要更系统的方法来全面评估LLM安全性。

Method: 提出QDRT框架，采用行为条件训练和行为重放缓冲区，训练多个专业攻击者模型。

Result: QDRT生成的攻击在多样性和有效性上优于现有方法，适用于多种目标LLM。

Conclusion: QDRT为LLM安全性提供了更系统、有效的自动化红队方法，支持其负责任部署。

Abstract: Ensuring safety of large language models (LLMs) is important. Red teaming--a
systematic approach to identifying adversarial prompts that elicit harmful
responses from target LLMs--has emerged as a crucial safety evaluation method.
Within this framework, the diversity of adversarial prompts is essential for
comprehensive safety assessments. We find that previous approaches to
red-teaming may suffer from two key limitations. First, they often pursue
diversity through simplistic metrics like word frequency or sentence embedding
similarity, which may not capture meaningful variation in attack strategies.
Second, the common practice of training a single attacker model restricts
coverage across potential attack styles and risk categories. This paper
introduces Quality-Diversity Red-Teaming (QDRT), a new framework designed to
address these limitations. QDRT achieves goal-driven diversity through
behavior-conditioned training and implements a behavioral replay buffer in an
open-ended manner. Additionally, it trains multiple specialized attackers
capable of generating high-quality attacks across diverse styles and risk
categories. Our empirical evaluation demonstrates that QDRT generates attacks
that are both more diverse and more effective against a wide range of target
LLMs, including GPT-2, Llama-3, Gemma-2, and Qwen2.5. This work advances the
field of LLM safety by providing a systematic and effective approach to
automated red-teaming, ultimately supporting the responsible deployment of
LLMs.

</details>


### [524] [Reliable Critics: Monotonic Improvement and Convergence Guarantees for Reinforcement Learning](https://arxiv.org/abs/2506.07134)
*Eshwar S. R.,Gugan Thoppe,Aditya Gopalan,Gal Dalal*

Main category: cs.LG

TL;DR: 论文提出了Reliable Policy Iteration (RPI)算法，解决了强化学习中函数逼近下策略迭代的单调性保证问题。


<details>
  <summary>Details</summary>
Motivation: 尽管强化学习研究多年，但在函数逼近下正确使用RL算法仍具挑战性，尤其是策略迭代的单调性保证问题。

Method: RPI通过基于Bellman的约束优化替代传统策略评估中的投影或Bellman误差最小化。

Result: RPI不仅保证了单调性，其值估计还下界真实回报，且极限部分满足未投影Bellman方程。

Conclusion: RPI是首个在函数逼近下具有单调性和收敛保证的算法，实验验证其优于基线方法。

Abstract: Despite decades of research, it remains challenging to correctly use
Reinforcement Learning (RL) algorithms with function approximation. A prime
example is policy iteration, whose fundamental guarantee of monotonic
improvement collapses even under linear function approximation. To address this
issue, we introduce Reliable Policy Iteration (RPI). It replaces the common
projection or Bellman-error minimization during policy evaluation with a
Bellman-based constrained optimization. We prove that not only does RPI confer
textbook monotonicity on its value estimates but these estimates also lower
bound the true return. Also, their limit partially satisfies the unprojected
Bellman equation, emphasizing RPI's natural fit within RL. RPI is the first
algorithm with such monotonicity and convergence guarantees under function
approximation. For practical use, we provide a model-free variant of RPI that
amounts to a novel critic. It can be readily integrated into primary model-free
PI implementations such as DQN and DDPG. In classical control tasks, such
RPI-enhanced variants consistently maintain their lower-bound guarantee while
matching or surpassing the performance of all baseline methods.

</details>


### [525] [AMoPO: Adaptive Multi-objective Preference Optimization without Reward Models and Reference Models](https://arxiv.org/abs/2506.07165)
*Qi Liu,Jingqing Ruan,Hao Li,Haodong Zhao,Desheng Wang,Jiansong Chen,Wan Guanglu,Xunliang Cai,Zhi Zheng,Tong Xu*

Main category: cs.LG

TL;DR: AMoPO是一种新型框架，通过动态平衡多目标偏好优化，解决了现有方法在平衡偏好维度和计算复杂度上的不足。


<details>
  <summary>Details</summary>
Motivation: 现有方法无法有效平衡多目标偏好，且依赖辅助模型增加计算复杂度。

Method: 引入多目标优化范式，利用维度感知生成指标作为隐式奖励，无需额外奖励模型。采用自适应权重分配机制，动态优先处理偏好维度。

Result: AMoPO在实验中表现优于现有基线28.5%，并在不同规模模型上验证了其扩展能力。

Conclusion: AMoPO在多目标偏好对齐中表现出优越性和适应性，验证了其有效性。

Abstract: Existing multi-objective preference alignment methods for large language
models (LLMs) face limitations: (1) the inability to effectively balance
various preference dimensions, and (2) reliance on auxiliary reward/reference
models introduces computational complexity. To address these challenges, we
propose Adaptive Multi-objective Preference Optimization (AMoPO), a novel
framework that achieves dynamic balance across preference dimensions. By
introducing the multi-objective optimization paradigm to use the
dimension-aware generation metrics as implicit rewards, AMoPO aligns LLMs with
diverse preferences without additional reward models or reference models. We
introduce an adaptive weight assignment mechanism that models the generation
space as a Gaussian distribution, allowing dynamic prioritization of preference
dimensions. Empirical results demonstrate that AMoPO outperforms
state-of-the-art baselines by 28.5%, and the experiments on 7B, 14B, and 32B
models reveal the scaling ability of AMoPO. Moreover, additional analysis of
multiple dimensions verifies its adaptability and effectiveness. These findings
validate AMoPO's capability to achieve dimension-aware preference alignment,
highlighting its superiority. Our codes and datasets are available at
https://github.com/Javkonline/AMoPO.

</details>


### [526] [Efficient Text-Attributed Graph Learning through Selective Annotation and Graph Alignment](https://arxiv.org/abs/2506.07168)
*Huanyi Xie,Lijie Hu,Lu Yu,Tianhao Huang,Longfei Li,Meng Li,Jun Zhou,Huan Wang,Di Wang*

Main category: cs.LG

TL;DR: GAGA是一种高效的TAG表示学习框架，通过仅标注代表性节点和边来减少标注成本，并通过两级对齐模块整合标注图和TAG。实验表明，GAGA仅需1%的标注数据即可达到或超越现有方法的分类精度。


<details>
  <summary>Details</summary>
Motivation: 传统GNN在处理带文本属性的图（TAG）时表现不佳，而现有方法依赖大量标注或微调，成本高昂。GAGA旨在解决这些问题。

Method: GAGA通过标注代表性节点和边构建标注图，并采用两级对齐模块将标注图与TAG的结构对齐。

Result: GAGA在仅标注1%数据的情况下，分类精度达到或超越现有方法。

Conclusion: GAGA是一种高效且低成本的TAG表示学习框架，具有实际应用潜力。

Abstract: In the realm of Text-attributed Graphs (TAGs), traditional graph neural
networks (GNNs) often fall short due to the complex textual information
associated with each node. Recent methods have improved node representations by
leveraging large language models (LLMs) to enhance node text features, but
these approaches typically require extensive annotations or fine-tuning across
all nodes, which is both time-consuming and costly. To overcome these
challenges, we introduce GAGA, an efficient framework for TAG representation
learning. GAGA reduces annotation time and cost by focusing on annotating only
representative nodes and edges. It constructs an annotation graph that captures
the topological relationships among these annotations. Furthermore, GAGA
employs a two-level alignment module to effectively integrate the annotation
graph with the TAG, aligning their underlying structures. Experiments show that
GAGA achieves classification accuracies on par with or surpassing
state-of-the-art methods while requiring only 1% of the data to be annotated,
demonstrating its high efficiency.

</details>


### [527] [Regularized Adaptive Graph Learning for Large-Scale Traffic Forecasting](https://arxiv.org/abs/2506.07179)
*Kaiqi Wu,Weiyang Kong,Sen Zhang,Yubao Liu,Zitong Chen*

Main category: cs.LG

TL;DR: 提出了一种名为RAGL的模型，通过正则化自适应图学习和高效余弦算子解决交通预测中的嵌入正则化和可扩展性问题。


<details>
  <summary>Details</summary>
Motivation: 现有自适应图学习方法在交通预测中常忽略节点嵌入的正则化或面临图卷积操作的可扩展性问题。

Method: 结合随机共享嵌入和自适应图卷积的正则化框架，以及基于余弦相似性的高效图卷积算子。

Result: 在四个大规模真实交通数据集上，RAGL在预测精度和计算效率上均优于现有方法。

Conclusion: RAGL模型有效解决了交通预测中的嵌入正则化和可扩展性问题，表现出优越性能。

Abstract: Traffic prediction is a critical task in spatial-temporal forecasting with
broad applications in travel planning and urban management. Adaptive graph
convolution networks have emerged as mainstream solutions due to their ability
to learn node embeddings in a data-driven manner and capture complex latent
dependencies. However, existing adaptive graph learning methods for traffic
forecasting often either ignore the regularization of node embeddings, which
account for a significant proportion of model parameters, or face scalability
issues from expensive graph convolution operations. To address these
challenges, we propose a Regularized Adaptive Graph Learning (RAGL) model.
First, we introduce a regularized adaptive graph learning framework that
synergizes Stochastic Shared Embedding (SSE) and adaptive graph convolution via
a residual difference mechanism, achieving both embedding regularization and
noise suppression. Second, to ensure scalability on large road networks, we
develop the Efficient Cosine Operator (ECO), which performs graph convolution
based on the cosine similarity of regularized embeddings with linear time
complexity. Extensive experiments on four large-scale real-world traffic
datasets show that RAGL consistently outperforms state-of-the-art methods in
terms of prediction accuracy and exhibits competitive computational efficiency.

</details>


### [528] [Learning based on neurovectors for tabular data: a new neural network approach](https://arxiv.org/abs/2506.07185)
*J. C. Husillos,A. Gallego,A. Roma,A. Troncoso*

Main category: cs.LG

TL;DR: 提出了一种基于Neurovectors的新型学习方法，通过向量空间和能量传播实现更灵活和可解释的学习过程，实验表明其在分类和回归任务中具有竞争力。


<details>
  <summary>Details</summary>
Motivation: 传统神经网络依赖反向传播调整权重，缺乏可解释性和灵活性，Neurovectors通过向量空间和能量传播提供了一种替代方案。

Method: 利用Neurovectors构建向量空间，通过能量传播而非权重更新驱动学习过程，生成动态知识表示。

Result: 在UCI和Kaggle数据集上的实验表明，Neurovectors在分类和回归任务中与传统模型相比具有竞争力。

Conclusion: Neurovectors提供了一种更灵活、可解释的学习方法，适用于多种任务，性能与传统模型相当。

Abstract: In this paper, we present a novel learning approach based on Neurovectors, an
innovative paradigm that structures information through interconnected nodes
and vector relationships for tabular data processing. Unlike traditional
artificial neural networks that rely on weight adjustment through
backpropagation, Neurovectors encode information by structuring data in vector
spaces where energy propagation, rather than traditional weight updates, drives
the learning process, enabling a more adaptable and explainable learning
process. Our method generates dynamic representations of knowledge through
neurovectors, thereby improving both the interpretability and efficiency of the
predictive model. Experimental results using datasets from well-established
repositories such as the UCI machine learning repository and Kaggle are
reported both for classification and regression. To evaluate its performance,
we compare our approach with standard machine learning and deep learning
models, showing that Neurovectors achieve competitive accuracy.

</details>


### [529] [Analyzing Breast Cancer Survival Disparities by Race and Demographic Location: A Survival Analysis Approach](https://arxiv.org/abs/2506.07191)
*Ramisa Farha,Joshua O. Olukoya*

Main category: cs.LG

TL;DR: 该研究利用SEER 2021数据集，通过探索性数据分析和生存分析技术，揭示了不同种族和地区乳腺癌患者生存率的差异，旨在为政策制定者和医疗专业人员提供可靠数据以减少治疗不平等。


<details>
  <summary>Details</summary>
Motivation: 研究旨在揭示乳腺癌患者生存率在不同种族和地区间的差异，为全球改善乳腺癌治疗结果和减少不平等提供支持。

Method: 采用探索性数据分析（EDA）、Kaplan-Meier估计、对数秩检验和Cox比例风险模型等生存分析技术，结合模型验证和解释。

Result: 研究发现乳腺癌治疗和护理中存在显著差异，并提供了详细的统计分析结果。

Conclusion: 研究为制定针对性干预措施以减少不平等提供了基础工具，有助于全球改善乳腺癌治疗结果。

Abstract: This study employs a robust analytical framework to uncover patterns in
survival outcomes among breast cancer patients from diverse racial and
geographical backgrounds. This research uses the SEER 2021 dataset to analyze
breast cancer survival outcomes to identify and comprehend dissimilarities. Our
approach integrates exploratory data analysis (EDA), through this we identify
key variables that influence survival rates and employ survival analysis
techniques, including the Kaplan-Meier estimator and log-rank test and the
advanced modeling Cox Proportional Hazards model to determine how survival
rates vary across racial groups and countries. Model validation and
interpretation are undertaken to ensure the reliability of our findings, which
are documented comprehensively to inform policymakers and healthcare
professionals. The outcome of this paper is a detailed version of statistical
analysis that not just highlights disparities in breast cancer treatment and
care but also serves as a foundational tool for developing targeted
interventions to address the inequalities effectively. Through this research,
our aim is to contribute to the global efforts to improve breast cancer
outcomes and reduce treatment disparities.

</details>


### [530] [GGBall: Graph Generative Model on Poincaré Ball](https://arxiv.org/abs/2506.07198)
*Tianci Bu,Chuanrui Wang,Hao Ma,Haoren Zheng,Xin Lu,Tailin Wu*

Main category: cs.LG

TL;DR: GGBall提出了一种基于双曲几何的图生成框架，结合了HVQVAE和黎曼流匹配先验，显著提升了生成图的层次结构保留能力。


<details>
  <summary>Details</summary>
Motivation: 由于欧几里得几何在捕捉指数复杂度上的局限性，生成具有层次结构的图仍是一个挑战。

Method: GGBall结合了双曲向量量化自编码器（HVQVAE）和基于闭式测地线的黎曼流匹配先验，并开发了完全在双曲流形中操作的GNN和Transformer层。

Result: 在Community-Small和Ego-Small数据集上，GGBall分别减少了75%和40%的度MMD，显著优于现有基线。

Conclusion: 双曲几何为复杂、结构化和层次化数据域的生成建模提供了强大基础。

Abstract: Generating graphs with hierarchical structures remains a fundamental
challenge due to the limitations of Euclidean geometry in capturing exponential
complexity. Here we introduce \textbf{GGBall}, a novel hyperbolic framework for
graph generation that integrates geometric inductive biases with modern
generative paradigms. GGBall combines a Hyperbolic Vector-Quantized Autoencoder
(HVQVAE) with a Riemannian flow matching prior defined via closed-form
geodesics. This design enables flow-based priors to model complex latent
distributions, while vector quantization helps preserve the curvature-aware
structure of the hyperbolic space. We further develop a suite of hyperbolic GNN
and Transformer layers that operate entirely within the manifold, ensuring
stability and scalability. Empirically, our model reduces degree MMD by over
75\% on Community-Small and over 40\% on Ego-Small compared to state-of-the-art
baselines, demonstrating an improved ability to preserve topological
hierarchies. These results highlight the potential of hyperbolic geometry as a
powerful foundation for the generative modeling of complex, structured, and
hierarchical data domains. Our code is available at
\href{https://github.com/AI4Science-WestlakeU/GGBall}{here}.

</details>


### [531] [Advancing Multimodal Reasoning Capabilities of Multimodal Large Language Models via Visual Perception Reward](https://arxiv.org/abs/2506.07218)
*Tong Xiao,Xin Xu,Zhenya Huang,Hongyu Gao,Quan Liu,Qi Liu,Enhong Chen*

Main category: cs.LG

TL;DR: 论文提出Perception-R1方法，通过引入视觉感知奖励提升多模态大语言模型（MLLMs）的感知和推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有强化学习方法（RLVR）在多模态领域未能有效提升MLLMs的感知能力，限制了其推理能力的进一步提升。

Method: 提出Perception-R1，通过视觉感知奖励机制，利用文本视觉注释和一致性评估来激励模型准确感知视觉内容。

Result: 在多个多模态推理基准测试中，Perception-R1仅用1,442个训练数据即达到最先进性能。

Conclusion: Perception-R1有效提升了MLLMs的感知和推理能力，为多模态任务提供了新思路。

Abstract: Enhancing the multimodal reasoning capabilities of Multimodal Large Language
Models (MLLMs) is a challenging task that has attracted increasing attention in
the community. Recently, several studies have applied Reinforcement Learning
with Verifiable Rewards (RLVR) to the multimodal domain in order to enhance the
reasoning abilities of MLLMs. However, these works largely overlook the
enhancement of multimodal perception capabilities in MLLMs, which serve as a
core prerequisite and foundational component of complex multimodal reasoning.
Through McNemar's test, we find that existing RLVR method fails to effectively
enhance the multimodal perception capabilities of MLLMs, thereby limiting their
further improvement in multimodal reasoning. To address this limitation, we
propose Perception-R1, which introduces a novel visual perception reward that
explicitly encourages MLLMs to perceive the visual content accurately, thereby
can effectively incentivizing both their multimodal perception and reasoning
capabilities. Specifically, we first collect textual visual annotations from
the CoT trajectories of multimodal problems, which will serve as visual
references for reward assignment. During RLVR training, we employ a judging LLM
to assess the consistency between the visual annotations and the responses
generated by MLLM, and assign the visual perception reward based on these
consistency judgments. Extensive experiments on several multimodal reasoning
benchmarks demonstrate the effectiveness of our Perception-R1, which achieves
state-of-the-art performance on most benchmarks using only 1,442 training data.

</details>


### [532] [VARSHAP: Addressing Global Dependency Problems in Explainable AI with Variance-Based Local Feature Attribution](https://arxiv.org/abs/2506.07229)
*Mateusz Gajewski,Mikołaj Morzy,Adam Karczmarz,Piotr Sankowski*

Main category: cs.LG

TL;DR: VARSHAP是一种新的局部特征归因方法，通过减少预测方差作为特征重要性度量，优于SHAP和LIME。


<details>
  <summary>Details</summary>
Motivation: 现有特征归因方法（如SHAP）存在全局依赖性，无法准确捕捉局部模型行为。

Method: 基于Shapley值框架，VARSHAP以减少预测方差为核心度量，对全局数据分布变化具有鲁棒性。

Result: 在合成和真实数据集上的实验表明，VARSHAP在定量和定性上均优于KernelSHAP和LIME。

Conclusion: VARSHAP是一种有效的局部特征归因方法，适用于数据分布变化的场景。

Abstract: Existing feature attribution methods like SHAP often suffer from global
dependence, failing to capture true local model behavior. This paper introduces
VARSHAP, a novel model-agnostic local feature attribution method which uses the
reduction of prediction variance as the key importance metric of features.
Building upon Shapley value framework, VARSHAP satisfies the key Shapley
axioms, but, unlike SHAP, is resilient to global data distribution shifts.
Experiments on synthetic and real-world datasets demonstrate that VARSHAP
outperforms popular methods such as KernelSHAP or LIME, both quantitatively and
qualitatively.

</details>


### [533] [Overclocking LLM Reasoning: Monitoring and Controlling Thinking Path Lengths in LLMs](https://arxiv.org/abs/2506.07240)
*Roy Eisenstadt,Itamar Zimerman,Lior Wolf*

Main category: cs.LG

TL;DR: 论文探讨了LLMs在显式推理过程中如何理解和调节其推理长度，通过进度条可视化揭示模型规划动态，并通过操纵内部进度编码减少不必要步骤，提升答案准确性和推理效率。


<details>
  <summary>Details</summary>
Motivation: 研究显式结构化推理中推理长度对答案质量的影响，避免推理过短或过长导致的问题。

Method: 引入进度条可视化模型推理动态，并操纵内部进度编码以减少推理步骤。

Result: 实验表明，该方法能减少过度思考，提高答案准确性并降低推理延迟。

Conclusion: 通过调节推理长度，可以有效优化LLMs的显式推理性能。

Abstract: Recently, techniques such as explicit structured reasoning have demonstrated
strong test-time scaling behavior by enforcing a separation between the model's
internal "thinking" process and the final response. A key factor influencing
answer quality in this setting is the length of the thinking stage. When the
reasoning is too short, the model may fail to capture the complexity of the
task. Conversely, when it is too long, the model may overthink, leading to
unnecessary computation and degraded performance. This paper explores and
exploits the underlying mechanisms by which LLMs understand and regulate the
length of their reasoning during explicit thought processes. First, we show
that LLMs encode their progress through the reasoning process and introduce an
interactive progress bar visualization, which is then used to reveal insights
on the model's planning dynamics. Second, we manipulate the internal progress
encoding during inference to reduce unnecessary steps and generate a more
concise and decisive chain of thoughts. Our empirical results demonstrate that
this "overclocking" method mitigates overthinking, improves answer accuracy,
and reduces inference latency. Our code is publicly available.

</details>


### [534] [Promoting Ensemble Diversity with Interactive Bayesian Distributional Robustness for Fine-tuning Foundation Models](https://arxiv.org/abs/2506.07247)
*Ngoc-Quan Pham,Tuan Truong,Quyen Tran,Tan Nguyen,Dinh Phung,Trung Le*

Main category: cs.LG

TL;DR: IBDR是一种新型贝叶斯推理框架，通过增强粒子多样性提升集合质量，并在VTAB-1K基准测试中表现优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 研究动机是提出一种能够建模粒子间交互的贝叶斯推理框架，以提升集合质量。

Method: IBDR基于广义理论框架，连接分布总体损失与近似后验，采用双重优化程序实现分布鲁棒性和粒子多样性。

Result: 在VTAB-1K基准测试和语言推理任务中，IBDR表现优于基线方法。

Conclusion: IBDR在现实应用中表现出色，验证了其有效性。

Abstract: We introduce Interactive Bayesian Distributional Robustness (IBDR), a novel
Bayesian inference framework that allows modeling the interactions between
particles, thereby enhancing ensemble quality through increased particle
diversity. IBDR is grounded in a generalized theoretical framework that
connects the distributional population loss with the approximate posterior,
motivating a practical dual optimization procedure that enforces distributional
robustness while fostering particle diversity. We evaluate IBDR's performance
against various baseline methods using the VTAB-1K benchmark and the common
reasoning language task. The results consistently show that IBDR outperforms
these baselines, underscoring its effectiveness in real-world applications.

</details>


### [535] [A Stable Whitening Optimizer for Efficient Neural Network Training](https://arxiv.org/abs/2506.07254)
*Kevin Frans,Sergey Levine,Pieter Abbeel*

Main category: cs.LG

TL;DR: 论文提出SPlus方法，解决了Shampoo算法的三个关键问题，包括稳定性、学习率适应性和参数噪声问题，实验证明其性能优于Adam。


<details>
  <summary>Details</summary>
Motivation: 针对Shampoo算法在实际应用中的三个关键问题（稳定性、学习率适应性和参数噪声）进行改进。

Method: 提出SPlus方法，包括历史特征基更新、形状感知的学习率调整和迭代平均方案。

Result: SPlus在验证性能上比Adam节省44%的梯度步数和62%的墙钟时间。

Conclusion: SPlus在稳定性和效率上显著优于现有方法，适用于多种任务。

Abstract: In this work, we take an experimentally grounded look at neural network
optimization. Building on the Shampoo family of algorithms, we identify and
alleviate three key issues, resulting in the proposed SPlus method. First, we
find that naive Shampoo is prone to divergence when matrix-inverses are cached
for long periods. We introduce an alternate bounded update combining a
historical eigenbasis with instantaneous normalization, resulting in
across-the-board stability and significantly lower computational requirements.
Second, we adapt a shape-aware scaling to enable learning rate transfer across
network width. Third, we find that high learning rates result in large
parameter noise, and propose a simple iterate-averaging scheme which unblocks
faster learning. To properly confirm these findings, we introduce a pointed
Transformer training benchmark, considering three objectives (language
modelling, image classification, and diffusion modelling) across different
stages of training. On average, SPlus is able to reach the validation
performance of Adam within 44% of the gradient steps and 62% of the wallclock
time.

</details>


### [536] [A Cramér-von Mises Approach to Incentivizing Truthful Data Sharing](https://arxiv.org/abs/2506.07272)
*Alex Clinton,Thomas Zeng,Yiding Chen,Xiaojin Zhu,Kirthevasan Kandasamy*

Main category: cs.LG

TL;DR: 论文提出了一种基于Cramér-von Mises统计的新型奖励机制，旨在激励代理人提交真实数据，同时防止数据伪造。


<details>
  <summary>Details</summary>
Motivation: 现有基于数据量的奖励机制容易被操纵，代理人可能提交虚假或低质量数据以获取奖励。

Method: 采用基于Cramér-von Mises统计的两样本检验方法，设计奖励机制。

Result: 理论证明真实报告在贝叶斯和无先验设置下构成（近似）纳什均衡，并在三个典型数据共享问题中验证了方法的有效性。

Conclusion: 新机制在理论和实证上均能有效激励真实数据共享，放宽了先前工作的强假设。

Abstract: Modern data marketplaces and data sharing consortia increasingly rely on
incentive mechanisms to encourage agents to contribute data. However, schemes
that reward agents based on the quantity of submitted data are vulnerable to
manipulation, as agents may submit fabricated or low-quality data to inflate
their rewards. Prior work has proposed comparing each agent's data against
others' to promote honesty: when others contribute genuine data, the best way
to minimize discrepancy is to do the same. Yet prior implementations of this
idea rely on very strong assumptions about the data distribution (e.g.
Gaussian), limiting their applicability. In this work, we develop reward
mechanisms based on a novel, two-sample test inspired by the Cram\'er-von Mises
statistic. Our methods strictly incentivize agents to submit more genuine data,
while disincentivizing data fabrication and other types of untruthful
reporting. We establish that truthful reporting constitutes a (possibly
approximate) Nash equilibrium in both Bayesian and prior-agnostic settings. We
theoretically instantiate our method in three canonical data sharing problems
and show that it relaxes key assumptions made by prior work. Empirically, we
demonstrate that our mechanism incentivizes truthful data sharing via
simulations and on real-world language and image data.

</details>


### [537] [Tokenized Bandit for LLM Decoding and Alignment](https://arxiv.org/abs/2506.07276)
*Suho Shin,Chenghao Yang,Haifeng Xu,Mohammad T. Hajiaghayi*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: We introduce the tokenized linear bandit (TLB) and multi-armed bandit (TMAB),
variants of linear and stochastic multi-armed bandit problems inspired by LLM
decoding and alignment. In these problems, at each round $t \in [T]$, a user
submits a query (context), and the decision maker (DM) sequentially selects a
token irrevocably from a token set. Once the sequence is complete, the DM
observes a random utility from the user, whose expectation is presented by a
sequence function mapping the chosen token sequence to a nonnegative real value
that depends on the query.
  In both problems, we first show that learning is impossible without any
structure on the sequence function. We introduce a natural assumption,
diminishing distance with more commons (DDMC), and propose algorithms with
regret $\tilde{O}(L\sqrt{T})$ and $\tilde{O}(L\sqrt{T^{2/3}})$ for TLB and
TMAB, respectively. As a side product, we obtain an (almost) optimality of the
greedy decoding for LLM decoding algorithm under DDMC, which justifies the
unresaonable effectiveness of greedy decoding in several tasks. This also has
an immediate application to decoding-time LLM alignment, when the misaligned
utility can be represented as the frozen LLM's utility and a linearly
realizable latent function. We finally validate our algorithm's performance
empirically as well as verify our assumptions using synthetic and real-world
datasets.

</details>


### [538] [EviNet: Evidential Reasoning Network for Resilient Graph Learning in the Open and Noisy Environments](https://arxiv.org/abs/2506.07288)
*Weijie Guan,Haohui Wang,Jian Kang,Lihui Liu,Dawei Zhou*

Main category: cs.LG

TL;DR: EVINET是一个基于主观逻辑框架的图学习框架，通过Beta嵌入解决误分类检测和分布外检测问题。


<details>
  <summary>Details</summary>
Motivation: 传统图学习假设封闭世界，而实际环境是开放且嘈杂的，需要模型能识别误分类和分布外数据。

Method: EVINET结合Beta嵌入和主观逻辑，包含两个模块：Dissonance Reasoning（误分类检测）和Vacuity Reasoning（分布外检测）。

Result: 实验表明EVINET在多个任务和指标上优于现有方法。

Conclusion: EVINET证明了不确定性估计和逻辑推理在开放世界图学习中的重要性。

Abstract: Graph learning has been crucial to many real-world tasks, but they are often
studied with a closed-world assumption, with all possible labels of data known
a priori. To enable effective graph learning in an open and noisy environment,
it is critical to inform the model users when the model makes a wrong
prediction to in-distribution data of a known class, i.e., misclassification
detection or when the model encounters out-of-distribution from novel classes,
i.e., out-of-distribution detection. This paper introduces Evidential Reasoning
Network (EVINET), a framework that addresses these two challenges by
integrating Beta embedding within a subjective logic framework. EVINET includes
two key modules: Dissonance Reasoning for misclassification detection and
Vacuity Reasoning for out-of-distribution detection. Extensive experiments
demonstrate that EVINET outperforms state-of-the-art methods across multiple
metrics in the tasks of in-distribution classification, misclassification
detection, and out-of-distribution detection. EVINET demonstrates the necessity
of uncertainty estimation and logical reasoning for misclassification detection
and out-of-distribution detection and paves the way for open-world graph
learning. Our code and data are available at https://github.com/SSSKJ/EviNET.

</details>


### [539] [Pre-trained Large Language Models Learn Hidden Markov Models In-context](https://arxiv.org/abs/2506.07298)
*Yijia Dai,Zhaolin Gao,Yahya Satter,Sarah Dean,Jennifer J. Sun*

Main category: cs.LG

TL;DR: LLMs通过上下文学习（ICL）有效建模HMM生成的数据，预测精度接近理论最优，并在实际任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: 解决HMM拟合真实数据时的计算挑战，探索LLMs在建模隐藏结构中的潜力。

Method: 利用预训练LLMs的ICL能力，在合成HMM数据和真实动物决策任务中测试。

Result: LLMs在合成数据上接近理论最优，实际任务中与专家设计模型竞争。

Conclusion: ICL能学习HMM序列，为复杂科学数据提供新工具，深化对LLMs的理解。

Abstract: Hidden Markov Models (HMMs) are foundational tools for modeling sequential
data with latent Markovian structure, yet fitting them to real-world data
remains computationally challenging. In this work, we show that pre-trained
large language models (LLMs) can effectively model data generated by HMMs via
in-context learning (ICL)$\unicode{x2013}$their ability to infer patterns from
examples within a prompt. On a diverse set of synthetic HMMs, LLMs achieve
predictive accuracy approaching the theoretical optimum. We uncover novel
scaling trends influenced by HMM properties, and offer theoretical conjectures
for these empirical observations. We also provide practical guidelines for
scientists on using ICL as a diagnostic tool for complex data. On real-world
animal decision-making tasks, ICL achieves competitive performance with models
designed by human experts. To our knowledge, this is the first demonstration
that ICL can learn and predict HMM-generated sequences$\unicode{x2013}$an
advance that deepens our understanding of in-context learning in LLMs and
establishes its potential as a powerful tool for uncovering hidden structure in
complex scientific data.

</details>


### [540] [PASS: Private Attributes Protection with Stochastic Data Substitution](https://arxiv.org/abs/2506.07308)
*Yizhuo Chen,Chun-Fu,Chen,Hsiang Hsu,Shaohan Hu,Tarek Abdelzaher*

Main category: cs.LG

TL;DR: 论文提出了一种名为PASS的新方法，通过概率替换原始数据样本以保护隐私属性，克服了现有对抗训练方法的漏洞。


<details>
  <summary>Details</summary>
Motivation: 现有保护隐私属性的方法存在严重漏洞，主要源于其对抗训练策略的固有弱点。

Method: 提出PASS方法，通过随机概率替换原始样本，并采用基于信息论目标的新型损失函数进行训练。

Result: 在多种数据集（如面部图像、活动传感器信号和语音记录）上验证了PASS的有效性和通用性。

Conclusion: PASS是一种有效且通用的方法，能够保护隐私属性同时保持数据实用性。

Abstract: The growing Machine Learning (ML) services require extensive collections of
user data, which may inadvertently include people's private information
irrelevant to the services. Various studies have been proposed to protect
private attributes by removing them from the data while maintaining the
utilities of the data for downstream tasks. Nevertheless, as we theoretically
and empirically show in the paper, these methods reveal severe vulnerability
because of a common weakness rooted in their adversarial training based
strategies. To overcome this limitation, we propose a novel approach, PASS,
designed to stochastically substitute the original sample with another one
according to certain probabilities, which is trained with a novel loss function
soundly derived from information-theoretic objective defined for
utility-preserving private attributes protection. The comprehensive evaluation
of PASS on various datasets of different modalities, including facial images,
human activity sensory signals, and voice recording datasets, substantiates
PASS's effectiveness and generalizability.

</details>


### [541] [Paged Attention Meets FlexAttention: Unlocking Long-Context Efficiency in Deployed Inference](https://arxiv.org/abs/2506.07311)
*Thomas Joshi,Herman Saini,Neil Dhillon,Antoni Viros i Martin,Kaoutar El Maghraoui*

Main category: cs.LG

TL;DR: 提出了一种结合PagedAttention和PyTorch FlexAttention的方法，解决了LLMs在长上下文推理中的内存效率问题。


<details>
  <summary>Details</summary>
Motivation: 传统KV缓存处理方式导致LLMs在长上下文推理中存在严重的内存效率问题。

Method: 通过集成PagedAttention和FlexAttention，优化KV缓存分配，减少内部碎片化。

Result: 在NVIDIA L4 GPU上测试，推理延迟显著降低，且随序列长度线性增长。

Conclusion: 该方法为未来长上下文模型部署提供了高效解决方案，并开源了实现。

Abstract: Large Language Models (LLMs) encounter severe memory inefficiencies during
long-context inference due to conventional handling of key-value (KV) caches.
In this work, we introduce a novel integration of PagedAttention with PyTorch's
FlexAttention, addressing internal fragmentation and inefficiencies associated
with monolithic KV cache allocations. Implemented within IBM's Foundation Model
Stack (FMS), our fused attention kernel efficiently gathers scattered KV data.
Our benchmarks on an NVIDIA L4 GPU (24GB) demonstrate significantly reduced
inference latency, growing only linearly (~2x) with sequence length from 128 to
2048 tokens when utilizing a global KV cache, compared to exponential latency
increases without caching. While peak memory usage remains largely unchanged
for single-step evaluations (dominated by model weights and activations), paged
attention causes minimal incremental memory usage, observable only at sequence
lengths exceeding 2048 tokens due to its power-of-two cache allocations. We
open-source the full implementation and discuss its implications for future
long-context model deployment.

</details>


### [542] [Generative Modeling of Networked Time-Series via Transformer Architectures](https://arxiv.org/abs/2506.07312)
*Yusuf Elnady*

Main category: cs.LG

TL;DR: 提出了一种基于Transformer的高效生成模型，用于合成时间序列数据，以解决安全领域中数据不足的问题，并提升机器学习模型的性能。


<details>
  <summary>Details</summary>
Motivation: 安全领域的数据访问受限，现有合成数据方法无法有效提升模型性能。

Method: 设计了一种高效的Transformer生成框架，用于生成高质量的时间序列数据。

Result: 新模型在性能上达到SOTA水平，且具有通用性和高质量样本生成能力。

Conclusion: 该Transformer模型能有效解决数据不足问题，并提升ML工作流的性能。

Abstract: Many security and network applications require having large datasets to train
the machine learning models. Limited data access is a well-known problem in the
security domain. Recent studies have shown the potential of Transformer models
to enlarge the size of data by synthesizing new samples, but the synthesized
samples don't improve the models over the real data. To address this issue, we
design an efficient transformer-based model as a generative framework to
generate time-series data, that can be used to boost the performance of
existing and new ML workflows. Our new transformer model achieves the SOTA
results. We style our model to be generalizable and work across different
datasets, and produce high-quality samples.

</details>


### [543] [DEF: Diffusion-augmented Ensemble Forecasting](https://arxiv.org/abs/2506.07324)
*David Millard,Arielle Carr,Stéphane Gaudreault,Ali Baheri*

Main category: cs.LG

TL;DR: DEF是一种通过扩散增强集成预报生成初始条件扰动的新方法，适用于机器学习天气预报领域，能够将确定性系统转化为随机系统，提升长期预测性能。


<details>
  <summary>Details</summary>
Motivation: 现有初始条件扰动方法主要针对数值天气预报（NWP）设计，限制了其在机器学习天气预报中的应用，导致随机模型开发效率低下。

Method: 使用条件扩散模型生成结构化扰动，支持迭代应用和通过指导项控制扰动水平。

Result: 在ERA5数据集上验证，DEF方法减少了长期预测误差，并生成合理的预测分布。

Conclusion: DEF方法为机器学习天气预报提供了一种高效的随机化工具，显著提升了预测性能。

Abstract: We present DEF (\textbf{\ul{D}}iffusion-augmented \textbf{\ul{E}}nsemble
\textbf{\ul{F}}orecasting), a novel approach for generating initial condition
perturbations. Modern approaches to initial condition perturbations are
primarily designed for numerical weather prediction (NWP) solvers, limiting
their applicability in the rapidly growing field of machine learning for
weather prediction. Consequently, stochastic models in this domain are often
developed on a case-by-case basis. We demonstrate that a simple conditional
diffusion model can (1) generate meaningful structured perturbations, (2) be
applied iteratively, and (3) utilize a guidance term to intuitivey control the
level of perturbation. This method enables the transformation of any
deterministic neural forecasting system into a stochastic one. With our
stochastic extended systems, we show that the model accumulates less error over
long-term forecasts while producing meaningful forecast distributions. We
validate our approach on the 5.625$^\circ$ ERA5 reanalysis dataset, which
comprises atmospheric and surface variables over a discretized global grid,
spanning from the 1960s to the present. On this dataset, our method
demonstrates improved predictive performance along with reasonable spread
estimates.

</details>


### [544] [Mobility-Aware Asynchronous Federated Learning with Dynamic Sparsification](https://arxiv.org/abs/2506.07328)
*Jintao Yan,Tan Chen,Yuxuan Sun,Zhaojun Nan,Sheng Zhou,Zhisheng Niu*

Main category: cs.LG

TL;DR: 论文提出了一种移动感知的动态稀疏化（MADS）算法，优化异步联邦学习（AFL）中的稀疏化程度，以解决设备移动性导致的连接问题和模型陈旧性对收敛的影响。


<details>
  <summary>Details</summary>
Motivation: 设备移动性导致间歇性连接和模型陈旧性，影响异步联邦学习的收敛性。

Method: 开发理论模型分析稀疏化、模型陈旧性和移动接触模式的相互作用，并提出MADS算法动态优化稀疏化程度。

Result: MADS在低速时增加稀疏化程度以提升收敛，高速时减少稀疏化以确保可靠上传。实验显示，MADS在CIFAR-10数据集上分类准确率提升8.76%，在Argoverse轨迹预测数据集上平均位移误差降低9.46%。

Conclusion: MADS算法有效解决了移动性对AFL的影响，显著提升了性能。

Abstract: Asynchronous Federated Learning (AFL) enables distributed model training
across multiple mobile devices, allowing each device to independently update
its local model without waiting for others. However, device mobility introduces
intermittent connectivity, which necessitates gradient sparsification and leads
to model staleness, jointly affecting AFL convergence. This paper develops a
theoretical model to characterize the interplay among sparsification, model
staleness and mobility-induced contact patterns, and their joint impact on AFL
convergence. Based on the analysis, we propose a mobility-aware dynamic
sparsification (MADS) algorithm that optimizes the sparsification degree based
on contact time and model staleness. Closed-form solutions are derived, showing
that under low-speed conditions, MADS increases the sparsification degree to
enhance convergence, while under high-speed conditions, it reduces the
sparsification degree to guarantee reliable uploads within limited contact
time. Experimental results validate the theoretical findings. Compared with the
state-of-the-art benchmarks, the MADS algorithm increases the image
classification accuracy on the CIFAR-10 dataset by 8.76% and reduces the
average displacement error in the Argoverse trajectory prediction dataset by
9.46%.

</details>


### [545] [JavelinGuard: Low-Cost Transformer Architectures for LLM Security](https://arxiv.org/abs/2506.07330)
*Yash Datta,Sharath Rajasekar*

Main category: cs.LG

TL;DR: JavelinGuard是一套低成本、高性能的模型架构，用于检测大型语言模型（LLM）交互中的恶意意图，专为生产环境优化。


<details>
  <summary>Details</summary>
Motivation: 针对LLM交互中的恶意意图检测需求，结合最新的Transformer架构进展，设计高效且适用于实际部署的模型。

Method: 探索了五种逐步复杂的Transformer架构，包括Sharanga、Mahendra、Vaishnava、Ashwina和Raudra，并在九个对抗性数据集上进行测试。

Result: Raudra的多任务设计表现最稳健，但每种架构在速度、可解释性和资源需求上有独特权衡。

Conclusion: JavelinGuard为实际LLM安全应用提供了复杂性与效率的最佳平衡选择。

Abstract: We present JavelinGuard, a suite of low-cost, high-performance model
architectures designed for detecting malicious intent in Large Language Model
(LLM) interactions, optimized specifically for production deployment. Recent
advances in transformer architectures, including compact BERT(Devlin et al.
2019) variants (e.g., ModernBERT (Warner et al. 2024)), allow us to build
highly accurate classifiers with as few as approximately 400M parameters that
achieve rapid inference speeds even on standard CPU hardware. We systematically
explore five progressively sophisticated transformer-based architectures:
Sharanga (baseline transformer classifier), Mahendra (enhanced
attention-weighted pooling with deeper heads), Vaishnava and Ashwina (hybrid
neural ensemble architectures), and Raudra (an advanced multi-task framework
with specialized loss functions). Our models are rigorously benchmarked across
nine diverse adversarial datasets, including popular sets like the NotInject
series, BIPIA, Garak, ImprovedLLM, ToxicChat, WildGuard, and our newly
introduced JavelinBench, specifically crafted to test generalization on
challenging borderline and hard-negative cases. Additionally, we compare our
architectures against leading open-source guardrail models as well as large
decoder-only LLMs such as gpt-4o, demonstrating superior cost-performance
trade-offs in terms of accuracy, and latency. Our findings reveal that while
Raudra's multi-task design offers the most robust performance overall, each
architecture presents unique trade-offs in speed, interpretability, and
resource requirements, guiding practitioners in selecting the optimal balance
of complexity and efficiency for real-world LLM security applications.

</details>


### [546] [Graph-KV: Breaking Sequence via Injecting Structural Biases into Large Language Models](https://arxiv.org/abs/2506.07334)
*Haoyu Wang,Peihao Wang,Mufei Li,Shikun Liu,Siqi Miao,Zhangyang Wang,Pan Li*

Main category: cs.LG

TL;DR: Graph-KV通过利用KV缓存和结构归纳偏置，改进了大型语言模型在处理结构化数据时的性能，显著优于传统序列化方法。


<details>
  <summary>Details</summary>
Motivation: 现代大型语言模型（LLM）在处理结构化数据时，由于输入被序列化为扁平序列，无法有效利用结构依赖关系。Graph-KV旨在解决这一问题。

Method: Graph-KV利用文本段的KV缓存作为压缩表示，并通过结构归纳偏置控制其交互。目标段仅关注指定的源段KV缓存，而非所有前序段，从而引入图结构块掩码和消息传递机制。

Result: Graph-KV在多个任务（如RAG、Arxiv-QA和论文主题分类）中表现优异，显著优于基线方法。

Conclusion: Graph-KV通过减少位置偏置和利用结构归纳偏置，为LLM处理结构化数据提供了高效解决方案。

Abstract: Modern large language models (LLMs) are inherently auto-regressive, requiring
input to be serialized into flat sequences regardless of their structural
dependencies. This serialization hinders the model's ability to leverage
structural inductive biases, especially in tasks such as retrieval-augmented
generation (RAG) and reasoning on data with native graph structures, where
inter-segment dependencies are crucial. We introduce Graph-KV with the
potential to overcome this limitation. Graph-KV leverages the KV-cache of text
segments as condensed representations and governs their interaction through
structural inductive biases. In this framework, 'target' segments selectively
attend only to the KV-caches of their designated 'source' segments, rather than
all preceding segments in a serialized sequence. This approach induces a
graph-structured block mask, sparsifying attention and enabling a
message-passing-like step within the LLM. Furthermore, strategically allocated
positional encodings for source and target segments reduce positional bias and
context window consumption. We evaluate Graph-KV across three scenarios: (1)
seven RAG benchmarks spanning direct inference, multi-hop reasoning, and
long-document understanding; (2) Arxiv-QA, a novel academic paper QA task with
full-text scientific papers structured as citation ego-graphs; and (3) paper
topic classification within a citation network. By effectively reducing
positional bias and harnessing structural inductive biases, Graph-KV
substantially outperforms baselines, including standard costly sequential
encoding, across various settings. Code and the Graph-KV data are publicly
available.

</details>


### [547] [SALT: A Lightweight Model Adaptation Method for Closed Split Computing Environments](https://arxiv.org/abs/2506.07355)
*Yuya Okada,Takayuki Nishio*

Main category: cs.LG

TL;DR: SALT是一种轻量级模型适配框架，用于在封闭约束下的分割计算，通过客户端可训练适配器优化特征，无需修改原始模型。


<details>
  <summary>Details</summary>
Motivation: 在封闭环境中，传统适配方法不可行，因为无法访问模型参数或架构。SALT旨在解决这一问题。

Method: 引入紧凑的可训练适配器，优化头部网络的潜在特征，支持用户特定适配。

Result: 在CIFAR-10和CIFAR-100上验证，SALT提高了准确性且训练延迟更低。

Conclusion: SALT为边缘AI系统提供了一种实用的个性化推理解决方案，适应严格系统约束。

Abstract: We propose SALT (Split-Adaptive Lightweight Tuning), a lightweight model
adaptation framework for Split Computing under closed constraints, where the
head and tail networks are proprietary and inaccessible to users. In such
closed environments, conventional adaptation methods are infeasible since they
require access to model parameters or architectures. SALT addresses this
challenge by introducing a compact, trainable adapter on the client side to
refine latent features from the head network, enabling user-specific adaptation
without modifying the original models or increasing communication overhead. We
evaluate SALT on user-specific classification tasks with CIFAR-10 and
CIFAR-100, demonstrating improved accuracy with lower training latency compared
to fine-tuning methods. Furthermore, SALT facilitates model adaptation for
robust inference over lossy networks, a common challenge in edge-cloud
environments. With minimal deployment overhead, SALT offers a practical
solution for personalized inference in edge AI systems under strict system
constraints.

</details>


### [548] [MoE-GPS: Guidlines for Prediction Strategy for Dynamic Expert Duplication in MoE Load Balancing](https://arxiv.org/abs/2506.07366)
*Haiyue Ma,Zhixu Du,Yiran Chen*

Main category: cs.LG

TL;DR: MoE-GPS框架通过量化预测策略对系统性能的影响，提出了一种仅预测整体令牌分布的优化方法，显著提升多GPU MoE网络的推理性能。


<details>
  <summary>Details</summary>
Motivation: 解决多GPU MoE网络中专家负载不平衡问题，动态复制热门专家以处理过量令牌，但需预测令牌分布。

Method: 提出MoE-GPS框架，量化预测策略对系统性能的影响，推荐仅预测整体令牌分布的优化方法。

Result: 在Mixtral 8x7B MMLU数据集上，优化方法比传统方法提升推理性能23%以上。

Conclusion: Distribution-Only Prediction策略显著降低开销，提升系统性能，适用于多GPU MoE网络。

Abstract: In multi-GPU Mixture-of-Experts (MoE) network, experts are distributed across
different GPUs, which creates load imbalance as each expert processes different
number of tokens. Recent works improve MoE inference load balance by
dynamically duplicating popular experts to more GPUs to process excessive
tokens, which requires predicting the distribution before routing. In this
paper, we discuss the tradeoff of prediction strategies, accuracies, overhead,
and end-to-end system performance. We propose MoE-GPS, a framework that guides
the selection of the optimal predictor design under various system
configurations, by quantifying the performance impact to system-level model
runtime. Specifically, we advocate for Distribution-Only Prediction, a
prediction strategy that only predicts overall token distribution which
significantly reduces overhead compared to the traditional Token-to-Expert
Prediction. On Mixtral 8x7B MMLU dataset, MoE-GPS suggests Distribution-Only
Prediction which improves end-to-end inference performance by more than 23%
compared with Token-to-Expert Prediction.

</details>


### [549] [Moment Alignment: Unifying Gradient and Hessian Matching for Domain Generalization](https://arxiv.org/abs/2506.07378)
*Yuen Chen,Haozhe Si,Guojun Zhang,Han Zhao*

Main category: cs.LG

TL;DR: 本文提出了一种基于矩对齐理论的域泛化方法（CMA），通过闭式对齐梯度和Hessian矩阵，解决了现有方法计算效率低的问题，并在实验中验证了其优越性能。


<details>
  <summary>Details</summary>
Motivation: 域泛化（DG）旨在解决现实应用中分布偏移问题，但现有方法计算效率低且理论基础不足。本文希望通过矩对齐理论统一理解现有方法，并提出高效算法。

Method: 基于转移测度的理论框架，提出闭式矩对齐（CMA）算法，直接对齐梯度和Hessian矩阵，避免重复反向传播或采样估计。

Result: CMA在线性探测和全微调实验中均优于经验风险最小化和现有先进算法。

Conclusion: 矩对齐理论为域泛化提供了统一视角，CMA算法在性能和效率上均表现出色。

Abstract: Domain generalization (DG) seeks to develop models that generalize well to
unseen target domains, addressing the prevalent issue of distribution shifts in
real-world applications. One line of research in DG focuses on aligning
domain-level gradients and Hessians to enhance generalization. However,
existing methods are computationally inefficient and the underlying principles
of these approaches are not well understood. In this paper, we develop the
theory of moment alignment for DG. Grounded in \textit{transfer measure}, a
principled framework for quantifying generalizability between two domains, we
first extend the definition of transfer measure to domain generalization that
includes multiple source domains and establish a target error bound. Then, we
prove that aligning derivatives across domains improves transfer measure both
when the feature extractor induces an invariant optimal predictor across
domains and when it does not. Notably, moment alignment provides a unifying
understanding of Invariant Risk Minimization, gradient matching, and Hessian
matching, three previously disconnected approaches to DG. We further connect
feature moments and derivatives of the classifier head, and establish the
duality between feature learning and classifier fitting. Building upon our
theory, we introduce \textbf{C}losed-Form \textbf{M}oment \textbf{A}lignment
(CMA), a novel DG algorithm that aligns domain-level gradients and Hessians in
closed-form. Our method overcomes the computational inefficiencies of existing
gradient and Hessian-based techniques by eliminating the need for repeated
backpropagation or sampling-based Hessian estimation. We validate the efficacy
of our approach through two sets of experiments: linear probing and full
fine-tuning. CMA demonstrates superior performance in both settings compared to
Empirical Risk Minimization and state-of-the-art algorithms.

</details>


### [550] [RiemannFormer: A Framework for Attention in Curved Spaces](https://arxiv.org/abs/2506.07405)
*Zhongping Ji*

Main category: cs.LG

TL;DR: 提出了一种基于几何视角的Transformer注意力机制解释，通过减少参数和引入局部增强机制提升性能。


<details>
  <summary>Details</summary>
Motivation: 为Transformer的注意力机制提供几何解释，并解决其忽视局部归纳偏置的问题。

Method: 利用度量张量、切空间和内积等几何概念，通过并行传输连接离散位置的结构，减少参数并引入局部增强机制。

Result: 实验显示模块性能显著优于基线，未来将在视觉和大语言模型上进一步验证。

Conclusion: 几何视角为Transformer优化提供了新思路，局部增强机制有效提升了性能。

Abstract: This research endeavors to offer insights into unlocking the further
potential of transformer-based architectures. One of the primary motivations is
to offer a geometric interpretation for the attention mechanism in
transformers. In our framework, the attention mainly involves metric tensors,
tangent spaces, inner product, and how they relate to each other. These
quantities and structures at discrete positions are intricately interconnected
via the parallel transport of tangent vectors. To make the learning process
more efficient, we reduce the number of parameters through ingenious predefined
configurations. Moreover, we introduce an explicit mechanism to highlight a
neighborhood by attenuating the remote values, given that transformers
inherently neglect local inductive bias. Experimental results demonstrate that
our modules deliver significant performance improvements relative to the
baseline. More evaluation experiments on visual and large language models will
be launched successively.

</details>


### [551] [InverseScope: Scalable Activation Inversion for Interpreting Large Language Models](https://arxiv.org/abs/2506.07406)
*Yifan Luo,Zhennan Zhou,Bin Dong*

Main category: cs.LG

TL;DR: InverseScope是一种轻假设、可扩展的框架，通过输入反演解释神经激活，提升了对大型语言模型内部表征的理解。


<details>
  <summary>Details</summary>
Motivation: 现有特征解释方法通常依赖强假设，而实际中这些假设可能不成立，因此需要一种更灵活的方法。

Method: 通过定义目标激活的输入分布，并分析该分布来推断编码特征；提出条件生成架构以提高采样效率。

Result: InverseScope显著提升了采样效率，并支持对大型模型内部表征的系统定量分析。

Conclusion: InverseScope为解释大型语言模型内部表征提供了一种高效且可扩展的方法。

Abstract: Understanding the internal representations of large language models (LLMs) is
a central challenge in interpretability research. Existing feature
interpretability methods often rely on strong assumptions about the structure
of representations that may not hold in practice. In this work, we introduce
InverseScope, an assumption-light and scalable framework for interpreting
neural activations via input inversion. Given a target activation, we define a
distribution over inputs that generate similar activations and analyze this
distribution to infer the encoded features. To address the inefficiency of
sampling in high-dimensional spaces, we propose a novel conditional generation
architecture that significantly improves sample efficiency compared to previous
methods. We further introduce a quantitative evaluation protocol that tests
interpretability hypotheses using feature consistency rate computed over the
sampled inputs. InverseScope scales inversion-based interpretability methods to
larger models and practical tasks, enabling systematic and quantitative
analysis of internal representations in real-world LLMs.

</details>


### [552] [Anomaly Detection and Early Warning Mechanism for Intelligent Monitoring Systems in Multi-Cloud Environments Based on LLM](https://arxiv.org/abs/2506.07407)
*Yihong Jin,Ze Yang,Juntian Liu,Xinhe Xu*

Main category: cs.LG

TL;DR: 提出了一种基于大语言模型（LLM）的多云环境智能监控系统异常检测与预警机制，结合自然语言处理与传统机器学习，显著提升了检测精度和实时响应效率。


<details>
  <summary>Details</summary>
Motivation: 随着多云环境的快速发展，确保智能监控系统的安全性和可靠性变得日益重要。

Method: 在现有监控框架基础上，创新引入多级特征提取方法，结合LLM的自然语言处理能力和传统机器学习方法，动态适应不同云服务提供商和环境。

Result: 实验结果表明，该模型在检测精度和延迟方面显著优于传统异常检测系统，并显著提升了云基础设施的弹性和主动管理能力。

Conclusion: 该研究为多云环境下的智能监控系统提供了一种高效、可靠的异常检测与预警解决方案。

Abstract: With the rapid development of multi-cloud environments, it is increasingly
important to ensure the security and reliability of intelligent monitoring
systems. In this paper, we propose an anomaly detection and early warning
mechanism for intelligent monitoring system in multi-cloud environment based on
Large-Scale Language Model (LLM). On the basis of the existing monitoring
framework, the proposed model innovatively introduces a multi-level feature
extraction method, which combines the natural language processing ability of
LLM with traditional machine learning methods to enhance the accuracy of
anomaly detection and improve the real-time response efficiency. By introducing
the contextual understanding capabilities of LLMs, the model dynamically adapts
to different cloud service providers and environments, so as to more
effectively detect abnormal patterns and predict potential failures.
Experimental results show that the proposed model is significantly better than
the traditional anomaly detection system in terms of detection accuracy and
latency, and significantly improves the resilience and active management
ability of cloud infrastructure.

</details>


### [553] [Fractional-order Jacobian Matrix Differentiation and Its Application in Artificial Neural Networks](https://arxiv.org/abs/2506.07408)
*Xiaojun zhou,Chunna Zhao,Yaqun Huang,Chengli Zhou,Junjie Ye,Kemeng Xiang*

Main category: cs.LG

TL;DR: 提出了一种基于分数阶雅可比矩阵的矩阵微分方法，兼容自动微分技术，提升了分数阶微分在深度学习中的实用性。


<details>
  <summary>Details</summary>
Motivation: 目前缺乏与自动微分技术兼容的分数阶矩阵微分方法，限制了分数阶微分在神经网络优化中的应用。

Method: 通过整数阶雅可比矩阵定义引入分数阶雅可比矩阵微分（${{\bf{J}}^\alpha }$），并基于此设计分数阶自动微分技术。

Result: 实验验证了${{\bf{J}}^\alpha }$在训练集和验证集上的优越性能，证明其是深度学习领域优秀的分数阶梯度下降方法。

Conclusion: 提出的分数阶雅可比矩阵微分方法有效提升了分数阶微分在深度学习中的实用性，具有显著性能优势。

Abstract: Fractional-order differentiation has many characteristics different from
integer-order differentiation. These characteristics can be applied to the
optimization algorithms of artificial neural networks to obtain better results.
However, due to insufficient theoretical research, at present, there is no
fractional-order matrix differentiation method that is perfectly compatible
with automatic differentiation (Autograd) technology. Therefore, we propose a
fractional-order matrix differentiation calculation method. This method is
introduced by the definition of the integer-order Jacobian matrix. We denote it
as fractional-order Jacobian matrix differentiation (${{\bf{J}}^\alpha }$).
Through ${{\bf{J}}^\alpha }$, we can carry out the matrix-based
fractional-order chain rule. Based on the Linear module and the
fractional-order differentiation, we design the fractional-order Autograd
technology to enable the use of fractional-order differentiation in hidden
layers, thereby enhancing the practicality of fractional-order differentiation
in deep learning. In the experiment, according to the PyTorch framework, we
design fractional-order Linear (FLinear) and replace nn.Linear in the
multilayer perceptron with FLinear. Through the qualitative analysis of the
training set and validation set $Loss$, the quantitative analysis of the test
set indicators, and the analysis of time consumption and GPU memory usage
during model training, we verify the superior performance of ${{\bf{J}}^\alpha
}$ and prove that it is an excellent fractional-order gradient descent method
in the field of deep learning.

</details>


### [554] [Variational Supervised Contrastive Learning](https://arxiv.org/abs/2506.07413)
*Ziwen Wang,Jiajun Fan,Thao Nguyen,Heng Ji,Ge Liu*

Main category: cs.LG

TL;DR: VarCon通过变分推理改进对比学习，解决了嵌入分布无显式调控和过度依赖大批量负样本的问题，在多个数据集上取得SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 解决对比学习中嵌入分布无显式调控和过度依赖大批量负样本的局限性。

Method: 提出VarCon，将监督对比学习重新表述为对潜在类变量的变分推理，最大化后验加权的ELBO，实现高效的类感知匹配和嵌入空间内类内分散的精细控制。

Result: 在CIFAR-10、CIFAR-100、ImageNet-100和ImageNet-1K上达到SOTA性能，Top-1准确率分别为79.36%和78.29%，并展示了更清晰的决策边界和语义组织。

Conclusion: VarCon在性能、语义组织和鲁棒性方面均优于现有方法，适用于少样本学习和多种增强策略。

Abstract: Contrastive learning has proven to be highly efficient and adaptable in
shaping representation spaces across diverse modalities by pulling similar
samples together and pushing dissimilar ones apart. However, two key
limitations persist: (1) Without explicit regulation of the embedding
distribution, semantically related instances can inadvertently be pushed apart
unless complementary signals guide pair selection, and (2) excessive reliance
on large in-batch negatives and tailored augmentations hinders generalization.
To address these limitations, we propose Variational Supervised Contrastive
Learning (VarCon), which reformulates supervised contrastive learning as
variational inference over latent class variables and maximizes a
posterior-weighted evidence lower bound (ELBO) that replaces exhaustive
pair-wise comparisons for efficient class-aware matching and grants
fine-grained control over intra-class dispersion in the embedding space.
Trained exclusively on image data, our experiments on CIFAR-10, CIFAR-100,
ImageNet-100, and ImageNet-1K show that VarCon (1) achieves state-of-the-art
performance for contrastive learning frameworks, reaching 79.36% Top-1 accuracy
on ImageNet-1K and 78.29% on CIFAR-100 with a ResNet-50 encoder while
converging in just 200 epochs; (2) yields substantially clearer decision
boundaries and semantic organization in the embedding space, as evidenced by
KNN classification, hierarchical clustering results, and transfer-learning
assessments; and (3) demonstrates superior performance in few-shot learning
than supervised baseline and superior robustness across various augmentation
strategies.

</details>


### [555] [LiteVLM: A Low-Latency Vision-Language Model Inference Pipeline for Resource-Constrained Environments](https://arxiv.org/abs/2506.07416)
*Jin Huang,Yuchao Jin,Le An,Josh Park*

Main category: cs.LG

TL;DR: 提出了一种针对嵌入式设备优化的高效视觉语言模型（VLM）流水线，显著降低计算开销，实现实时部署。


<details>
  <summary>Details</summary>
Motivation: 为在资源受限的嵌入式设备（如机器人和自动驾驶设备）上实现高效的视觉语言模型部署。

Method: 采用联合策略：1) 补丁选择过滤无关视图；2) 令牌选择模块减少输入序列长度；3) 推测解码加速令牌生成。

Result: 在NVIDIA DRIVE Thor平台上，流水线实现了2.5倍的端到端延迟降低，FP8量化后提升至3.2倍，且不影响任务精度。

Conclusion: 该流水线是资源受限环境下实时VLM部署的可行解决方案。

Abstract: This paper introduces an efficient Vision-Language Model (VLM) pipeline
specifically optimized for deployment on embedded devices, such as those used
in robotics and autonomous driving. The pipeline significantly reduces the
computational overhead by jointly leveraging patch selection to filter
irrelevant camera views, a token selection module to reduce input sequence
length for the LLM, and speculative decoding to accelerate token generation.
Evaluation on the NVIDIA DRIVE Thor platform for automonous driving
application, our pipeline achieves $2.5\times$ end-to-end latency reduction
without compromising task accuracy. The speed-up further increases to
$3.2\times$ when applying FP8 post-training quantization. These results
demonstrate our pipeline as a viable solution for enabling real-time VLM
deployment in resource-constrained environments.

</details>


### [556] [Evidential Spectrum-Aware Contrastive Learning for OOD Detection in Dynamic Graphs](https://arxiv.org/abs/2506.07417)
*Nan Sun,Xixun Lin,Zhiheng Zhou,Yanmin Shang,Zhenlin Cheng,Yanan Cao*

Main category: cs.LG

TL;DR: 论文提出了一种基于证据深度学习的动态图OOD检测方法EviSEC，通过不确定性估计和谱感知对比学习解决现有方法的偏差和分数同质化问题。


<details>
  <summary>Details</summary>
Motivation: 动态图中的OOD检测在安全敏感领域备受关注，但现有方法主要针对静态图，存在单点估计导致的偏差和方差问题，以及缺乏OOD训练数据导致的分数同质化问题。

Method: 提出EviSEC方法，采用证据神经网络将输出定义为后验Dirichlet分布，并通过谱感知增强模块生成OOD近似数据以扩大ID与OOD分数差距。

Result: 在真实数据集上的实验表明，EviSEC能有效检测动态图中的OOD样本。

Conclusion: EviSEC通过不确定性估计和谱感知对比学习，显著提升了动态图OOD检测的性能。

Abstract: Recently, Out-of-distribution (OOD) detection in dynamic graphs, which aims
to identify whether incoming data deviates from the distribution of the
in-distribution (ID) training set, has garnered considerable attention in
security-sensitive fields. Current OOD detection paradigms primarily focus on
static graphs and confront two critical challenges: i) high bias and high
variance caused by single-point estimation, which makes the predictions
sensitive to randomness in the data; ii) score homogenization resulting from
the lack of OOD training data, where the model only learns ID-specific
patterns, resulting in overall low OOD scores and a narrow score gap between ID
and OOD data. To tackle these issues, we first investigate OOD detection in
dynamic graphs through the lens of Evidential Deep Learning (EDL).
Specifically, we propose EviSEC, an innovative and effective OOD detector via
Evidential Spectrum-awarE Contrastive Learning. We design an evidential neural
network to redefine the output as the posterior Dirichlet distribution,
explaining the randomness of inputs through the uncertainty of distribution,
which is overlooked by single-point estimation. Moreover, spectrum-aware
augmentation module generates OOD approximations to identify patterns with high
OOD scores, thereby widening the score gap between ID and OOD data and
mitigating score homogenization. Extensive experiments on real-world datasets
demonstrate that EviSAC effectively detects OOD samples in dynamic graphs.

</details>


### [557] [Federated In-Context Learning: Iterative Refinement for Improved Answer Quality](https://arxiv.org/abs/2506.07440)
*Ruhan Wang,Zhiyong Wang,Chengkai Huang,Rui Wang,Tong Yu,Lina Yao,John C. S. Lui,Dongruo Zhou*

Main category: cs.LG

TL;DR: 提出了一种联邦上下文学习（Fed-ICL）框架，通过多轮客户端与服务器交互提升问答任务性能，无需传输模型参数。


<details>
  <summary>Details</summary>
Motivation: 解决上下文学习（ICL）依赖高质量示例的问题，同时避免数据隐私和通信开销的挑战。

Method: 采用联邦学习框架，通过多轮交互逐步优化回答，避免传输模型参数。

Result: 理论证明了Fed-ICL的收敛性，实验显示其在标准问答基准上表现优异且通信成本低。

Conclusion: Fed-ICL是一种高效且隐私友好的解决方案，适用于实际问答任务。

Abstract: For question-answering (QA) tasks, in-context learning (ICL) enables language
models to generate responses without modifying their parameters by leveraging
examples provided in the input. However, the effectiveness of ICL heavily
depends on the availability of high-quality examples, which are often scarce
due to data privacy constraints, annotation costs, and distribution
disparities. A natural solution is to utilize examples stored on client
devices, but existing approaches either require transmitting model parameters -
incurring significant communication overhead - or fail to fully exploit local
datasets, limiting their effectiveness. To address these challenges, we propose
Federated In-Context Learning (Fed-ICL), a general framework that enhances ICL
through an iterative, collaborative process. Fed-ICL progressively refines
responses by leveraging multi-round interactions between clients and a central
server, improving answer quality without the need to transmit model parameters.
We establish theoretical guarantees for the convergence of Fed-ICL and conduct
extensive experiments on standard QA benchmarks, demonstrating that our
proposed approach achieves strong performance while maintaining low
communication costs.

</details>


### [558] [Extending Epistemic Uncertainty Beyond Parameters Would Assist in Designing Reliable LLMs](https://arxiv.org/abs/2506.07448)
*T. Duy Nguyen-Hien,Desi R. Ivanova,Yee Whye Teh,Wee Sun Lee*

Main category: cs.LG

TL;DR: 论文提出了一种基于贝叶斯实验建模的框架，用于主动管理和解决LLM部署中的不确定性，而非被动拒绝高不确定性输出。


<details>
  <summary>Details</summary>
Motivation: 当前LLM部署中缺乏系统区分和应对不确定性的工具，导致只能保守拒绝高不确定性输出。

Method: 采用贝叶斯实验建模框架，为不确定性提供一致的理论基础，并支持主动解决措施（如请求澄清、检索外部信息等）。

Result: 该框架使LLM及其用户能够采取情境适当的步骤，提高系统的可靠性和透明度。

Conclusion: 通过主动解决不确定性，该框架为高风险现实场景中的LLM系统提供了更广泛的应用潜力。

Abstract: Although large language models (LLMs) are highly interactive and extendable,
current approaches to ensure reliability in deployments remain mostly limited
to rejecting outputs with high uncertainty in order to avoid misinformation.
This conservative strategy reflects the current lack of tools to systematically
distinguish and respond to different sources of uncertainty. In this paper, we
advocate for the adoption of Bayesian Modeling of Experiments -- a framework
that provides a coherent foundation to reason about uncertainty and clarify the
reducibility of uncertainty -- for managing and proactively addressing
uncertainty that arises in LLM deployments. This framework enables LLMs and
their users to take contextually appropriate steps, such as requesting
clarification, retrieving external information, or refining inputs. By
supporting active resolution rather than passive avoidance, it opens the door
to more reliable, transparent, and broadly applicable LLM systems, particularly
in high-stakes, real-world settings.

</details>


### [559] [When Style Breaks Safety: Defending Language Models Against Superficial Style Alignment](https://arxiv.org/abs/2506.07452)
*Yuxin Xiao,Sana Tonekaboni,Walter Gerych,Vinith Suriyakumar,Marzyeh Ghassemi*

Main category: cs.LG

TL;DR: 研究发现，风格模式（如列表格式）会显著增加大型语言模型（LLM）对越狱攻击的脆弱性，并提出了一种防御策略SafeStyle。


<details>
  <summary>Details</summary>
Motivation: 探讨风格模式是否影响LLM的安全性，以及如何通过对齐和防御策略降低风险。

Method: 评估32个LLM在七个越狱基准测试中的表现，分析风格模式对攻击成功率的影响，并提出SafeStyle防御策略。

Result: 风格模式显著提高攻击成功率，且与模式长度和模型注意力相关；SafeStyle能有效维持模型安全性。

Conclusion: 风格模式对LLM安全性有负面影响，SafeStyle是一种有效的防御方法。

Abstract: Large language models (LLMs) can be prompted with specific styles (e.g.,
formatting responses as lists), including in jailbreak queries. Although these
style patterns are semantically unrelated to the malicious intents behind
jailbreak queries, their safety impact remains unclear. In this work, we seek
to understand whether style patterns compromise LLM safety, how superficial
style alignment increases model vulnerability, and how best to mitigate these
risks during alignment. We evaluate 32 LLMs across seven jailbreak benchmarks,
and find that malicious queries with style patterns inflate the attack success
rate (ASR) for nearly all models. Notably, ASR inflation correlates with both
the length of style patterns and the relative attention an LLM exhibits on
them. We then investigate superficial style alignment, and find that
fine-tuning with specific styles makes LLMs more vulnerable to jailbreaks of
those same styles. Finally, we propose SafeStyle, a defense strategy that
incorporates a small amount of safety training data augmented to match the
distribution of style patterns in the fine-tuning data. Across three LLMs and
five fine-tuning style settings, SafeStyle consistently outperforms baselines
in maintaining LLM safety.

</details>


### [560] [ProteinZero: Self-Improving Protein Generation via Online Reinforcement Learning](https://arxiv.org/abs/2506.07459)
*Ziwen Wang,Jiajun Fan,Ruihan Guo,Thao Nguyen,Heng Ji,Ge Liu*

Main category: cs.LG

TL;DR: ProteinZero是一种通过在线强化学习实现蛋白质生成模型自我改进的新框架，显著提高了蛋白质设计的成功率。


<details>
  <summary>Details</summary>
Motivation: 由于高质量蛋白质数据集的稀缺性，现有蛋白质生成模型在成功率上存在局限。

Method: 引入基于ESM-fold和快速ddG预测器的高效代理奖励模型，结合多奖励最大化、KL散度和序列多样性正则化的强化学习框架。

Result: ProteinZero在结构准确性、可设计性、热力学稳定性和序列多样性上显著优于现有方法，设计失败率降低36%-48%。

Conclusion: ProteinZero为蛋白质设计建立了持续自我改进的新范式，为探索蛋白质设计空间提供了新可能。

Abstract: Protein generative models have shown remarkable promise in protein design but
still face limitations in success rate, due to the scarcity of high-quality
protein datasets for supervised pretraining. We present ProteinZero, a novel
framework that enables scalable, automated, and continuous self-improvement of
the inverse folding model through online reinforcement learning. To achieve
computationally tractable online feedback, we introduce efficient proxy reward
models based on ESM-fold and a novel rapid ddG predictor that significantly
accelerates evaluation speed. ProteinZero employs a general RL framework
balancing multi-reward maximization, KL-divergence from a reference model, and
a novel protein-embedding level diversity regularization that prevents mode
collapse while promoting higher sequence diversity. Through extensive
experiments, we demonstrate that ProteinZero substantially outperforms existing
methods across every key metric in protein design, achieving significant
improvements in structural accuracy, designability, thermodynamic stability,
and sequence diversity. Most impressively, ProteinZero reduces design failure
rates by approximately 36% - 48% compared to widely-used methods like
ProteinMPNN, ESM-IF and InstructPLM, consistently achieving success rates
exceeding 90% across diverse and complex protein folds. Notably, the entire RL
run on CATH-4.3 can be done with a single 8 X GPU node in under 3 days,
including reward computation. Our work establishes a new paradigm for protein
design where models evolve continuously from their own generated outputs,
opening new possibilities for exploring the vast protein design space.

</details>


### [561] [Circumventing Backdoor Space via Weight Symmetry](https://arxiv.org/abs/2506.07467)
*Jie Peng,Hongwei Yang,Jing Zhao,Hengji Dong,Hui He,Weizhe Zhang,Haoyu He*

Main category: cs.LG

TL;DR: 论文提出了一种名为TSC的新型后门净化防御方法，适用于多种学习范式，无需依赖数据格式或大量标记数据。


<details>
  <summary>Details</summary>
Motivation: 现有防御方法通常需要标记数据或特定训练流程，限制了其在监督学习以外的应用。后门攻击已扩展到多种学习范式，亟需通用防御方案。

Method: TSC利用神经网络的置换不变性和二次模式连接性，通过两阶段对称连接性放大中毒样本的损失，同时保持干净样本的准确性。

Result: 实验表明，TSC在监督学习场景中表现优异，并能推广到自监督学习框架（如SimCLR和CLIP）。

Conclusion: TSC是一种通用且高效的后门防御方法，适用于多种学习范式，具有实际应用潜力。

Abstract: Deep neural networks are vulnerable to backdoor attacks, where malicious
behaviors are implanted during training. While existing defenses can
effectively purify compromised models, they typically require labeled data or
specific training procedures, making them difficult to apply beyond supervised
learning settings. Notably, recent studies have shown successful backdoor
attacks across various learning paradigms, highlighting a critical security
concern. To address this gap, we propose Two-stage Symmetry Connectivity (TSC),
a novel backdoor purification defense that operates independently of data
format and requires only a small fraction of clean samples. Through theoretical
analysis, we prove that by leveraging permutation invariance in neural networks
and quadratic mode connectivity, TSC amplifies the loss on poisoned samples
while maintaining bounded clean accuracy. Experiments demonstrate that TSC
achieves robust performance comparable to state-of-the-art methods in
supervised learning scenarios. Furthermore, TSC generalizes to self-supervised
learning frameworks, such as SimCLR and CLIP, maintaining its strong defense
capabilities. Our code is available at https://github.com/JiePeng104/TSC.

</details>


### [562] [Premise Selection for a Lean Hammer](https://arxiv.org/abs/2506.07477)
*Thomas Zhu,Joshua Clune,Jeremy Avigad,Albert Qiaochu Jiang,Sean Welleck*

Main category: cs.LG

TL;DR: LeanHammer是首个面向Lean证明助手的端到端通用hammer工具，通过动态适应上下文和结合符号证明搜索，显著提升了验证效率。


<details>
  <summary>Details</summary>
Motivation: 尽管神经方法在自动推理中取得进展，但将其整合到实际验证工作流中仍具挑战性。Lean作为流行的证明助手，尚缺乏有效的hammer工具。

Method: 提出基于神经前提选择系统的LeanHammer，动态适应用户上下文，并与符号证明搜索和重构结合。

Result: LeanHammer比现有前提选择器多解决21%的目标，并能泛化到多样领域。

Conclusion: 该研究弥合了神经检索与符号推理的鸿沟，提升了形式验证的实用性。

Abstract: Neural methods are transforming automated reasoning for proof assistants, yet
integrating these advances into practical verification workflows remains
challenging. Hammers are tools that interface with external automatic theorem
provers to automate tedious reasoning steps. They have dramatically improved
productivity in proof assistants, but the Lean proof assistant still does not
have a hammer despite its growing popularity. We present LeanHammer, the first
end-to-end domain-general hammer for Lean, built on a novel neural premise
selection system for a hammer in dependent type theory. Unlike existing Lean
premise selectors, our approach dynamically adapts to user-specific contexts
and combines with symbolic proof search and reconstruction to create a
practical hammer. With comprehensive evaluations, we show that our premise
selector enables LeanHammer to solve 21\% more goals relative to existing
premise selectors, and generalize well to diverse domains. Our work bridges the
gap between neural retrieval and symbolic reasoning, making formal verification
more accessible to researchers and practitioners.

</details>


### [563] [Explicit Preference Optimization: No Need for an Implicit Reward Model](https://arxiv.org/abs/2506.07492)
*Xiangkun Hu,Lemin Kong,Tong He,David Wipf*

Main category: cs.LG

TL;DR: 论文探讨了直接偏好优化（DPO）及其变体在优化大型语言模型（LLM）时的局限性，并提出了一种新的显式偏好优化框架EXPO，避免了DPO的潜在缺陷。


<details>
  <summary>Details</summary>
Motivation: RLHF（基于人类反馈的强化学习）在优化LLM时依赖复杂的训练流程，DPO及其变体虽简化了流程，但仍存在次优正则化和反直觉插值行为的问题。

Method: 通过重新参数化技巧，DPO等方法隐式地学习奖励函数，但存在局限性。EXPO框架则直接引入直观的正则化因子，无需隐式奖励。

Result: EXPO在理论上满足正则化需求，并在实验中证明了其有效性，避免了DPO的潜在问题。

Conclusion: EXPO提供了一种更透明且有效的替代方案，解决了DPO方法的局限性，为LLM优化提供了新思路。

Abstract: The generated responses of large language models (LLMs) are often fine-tuned
to human preferences through a process called reinforcement learning from human
feedback (RLHF). As RLHF relies on a challenging training sequence, whereby a
separate reward model is independently learned and then later applied to LLM
policy updates, ongoing research effort has targeted more straightforward
alternatives. In this regard, direct preference optimization (DPO) and its many
offshoots circumvent the need for a separate reward training step. Instead,
through the judicious use of a reparameterization trick that induces an
\textit{implicit} reward, DPO and related methods consolidate learning to the
minimization of a single loss function. And yet despite demonstrable success in
some real-world settings, we prove that DPO-based objectives are nonetheless
subject to sub-optimal regularization and counter-intuitive interpolation
behaviors, underappreciated artifacts of the reparameterizations upon which
they are based. To this end, we introduce an \textit{explicit} preference
optimization framework termed EXPO that requires no analogous
reparameterization to achieve an implicit reward. Quite differently, we merely
posit intuitively-appealing regularization factors from scratch that
transparently avoid the potential pitfalls of key DPO variants, provably
satisfying regularization desiderata that prior methods do not. Empirical
results serve to corroborate our analyses and showcase the efficacy of EXPO.

</details>


### [564] [Mind the Gap: Removing the Discretization Gap in Differentiable Logic Gate Networks](https://arxiv.org/abs/2506.07500)
*Shakir Yousefi,Andreas Plesner,Till Aczel,Roger Wattenhofer*

Main category: cs.LG

TL;DR: 论文提出了一种通过注入Gumbel噪声和直通估计器加速逻辑门网络（LGNs）训练的方法，显著减少了离散化间隙和未使用门数量。


<details>
  <summary>Details</summary>
Motivation: 现代神经网络在计算和能耗上效率不足，逻辑门网络（LGNs）虽高效但训练时间长且存在离散化间隙，影响实际部署。

Method: 在训练中注入Gumbel噪声并使用直通估计器，通过隐式Hessian正则化改进收敛性。

Result: 训练速度提升4.5倍，离散化间隙减少98%，未使用门数量减少100%。

Conclusion: 该方法显著提升了LGNs的训练效率和实际部署性能。

Abstract: Modern neural networks demonstrate state-of-the-art performance on numerous
existing benchmarks; however, their high computational requirements and energy
consumption prompt researchers to seek more efficient solutions for real-world
deployment. Logic gate networks (LGNs) learns a large network of logic gates
for efficient image classification. However, learning a network that can solve
a simple problem like CIFAR-10 can take days to weeks to train. Even then,
almost half of the network remains unused, causing a discretization gap. This
discretization gap hinders real-world deployment of LGNs, as the performance
drop between training and inference negatively impacts accuracy. We inject
Gumbel noise with a straight-through estimator during training to significantly
speed up training, improve neuron utilization, and decrease the discretization
gap. We theoretically show that this results from implicit Hessian
regularization, which improves the convergence properties of LGNs. We train
networks $4.5 \times$ faster in wall-clock time, reduce the discretization gap
by $98\%$, and reduce the number of unused gates by $100\%$.

</details>


### [565] [Graph-of-Causal Evolution: Challenging Chain-of-Model for Reasoning](https://arxiv.org/abs/2506.07501)
*Libo Wang*

Main category: cs.LG

TL;DR: 论文提出了一种名为GoCE的方法，通过构建可微分稀疏因果邻接矩阵和动态平衡机制，解决了传统链式模型中长距离依赖丢失的问题。


<details>
  <summary>Details</summary>
Motivation: 传统链式模型（CoM）因仅依赖前序子链信息且受因果掩码限制，导致长距离依赖丢失。

Method: GoCE通过映射隐式令牌表示为可微分稀疏因果邻接矩阵，结合因果掩码注意力和因果-MoE，并引入干预一致性损失和自进化门实现动态平衡。

Result: 实验证明GoCE在多个公开数据集上优于基线模型，显著提升了长距离因果依赖捕捉能力和自进化能力。

Conclusion: GoCE不仅在设计上超越了CoM，还为因果学习和持续自适应改进提供了新思路。

Abstract: In view of the problem that each subchain in the chain-of-model (CoM) relies
only on the information of the previous subchain and may lose long-range
dependencies due to the causal mask blocking the global context flow between
multi-level subchains, this work proposes a graph of causal evolution (GoCE).
Its core principle is to map the implicit token representation into a
differentiable and sparse causal adjacency matrix, then permeate causal
constraints through each layer of calculation using causal-masked attention and
causal-MoE. By combining intervention consistency loss test and self-evolution
gate, the dynamic balance between causal structure learning and adaptive
updating of transformer architecture is realized. The researcher built
experimental environments in sandboxes built with Claude Sonnet 4,
o4-mini-high, and DeepSeek R1 respectively with the transformer variant
architecture introduced in GoCE. It is evaluated on publicly available datasets
including CLUTRR, CLADDER, EX-FEVER, and CausalQA and compared with the
baseline LLMs. The finding proves that GoCE strengthens the transformer's
ability to capture long-range causal dependencies, while the ability to
self-evolve is improved. It not only surpasses the design of CoM in terms of
design principles, but also provides experience for future research on causal
learning and continuous adaptive improvement.

</details>


### [566] [Reinforcement Learning via Implicit Imitation Guidance](https://arxiv.org/abs/2506.07505)
*Perry Dong,Alec M. Lessing,Annie S. Chen,Chelsea Finn*

Main category: cs.LG

TL;DR: 论文提出了一种名为DGN的方法，利用先验数据指导探索而非直接模仿，显著提升了样本效率。


<details>
  <summary>Details</summary>
Motivation: 研究如何在强化学习中高效利用先验数据（如演示），避免模仿学习目标对长期性能的负面影响。

Method: 提出DGN框架，通过向策略添加噪声来利用先验数据指导探索，而非强制行为克隆。

Result: 在七个模拟连续控制任务中，性能比现有方法提升2-3倍。

Conclusion: DGN通过探索导向而非模仿，有效提升了强化学习的样本效率。

Abstract: We study the problem of sample efficient reinforcement learning, where prior
data such as demonstrations are provided for initialization in lieu of a dense
reward signal. A natural approach is to incorporate an imitation learning
objective, either as regularization during training or to acquire a reference
policy. However, imitation learning objectives can ultimately degrade long-term
performance, as it does not directly align with reward maximization. In this
work, we propose to use prior data solely for guiding exploration via noise
added to the policy, sidestepping the need for explicit behavior cloning
constraints. The key insight in our framework, Data-Guided Noise (DGN), is that
demonstrations are most useful for identifying which actions should be
explored, rather than forcing the policy to take certain actions. Our approach
achieves up to 2-3x improvement over prior reinforcement learning from offline
data methods across seven simulated continuous control tasks.

</details>


### [567] [Addressing Correlated Latent Exogenous Variables in Debiased Recommender Systems](https://arxiv.org/abs/2506.07517)
*Shuqiang Zhang,Yuchao Zhang,Jinkun Chen,Haochen Sui*

Main category: cs.LG

TL;DR: 论文提出了一种基于似然最大化的学习算法，解决推荐系统中因外生变量独立性假设导致的偏差问题，并通过蒙特卡洛算法估计似然函数。


<details>
  <summary>Details</summary>
Motivation: 推荐系统因选择偏差导致用户偏好失真，现有方法假设外生变量独立，限制了效果。

Method: 提出统一方法处理潜在外生变量，基于正态假设建模数据生成过程，使用蒙特卡洛算法估计似然函数。

Result: 在合成和真实数据集上的实验验证了方法的有效性。

Conclusion: 方法有效解决了外生变量相关性问题，提升了推荐系统的准确性和公平性。

Abstract: Recommendation systems (RS) aim to provide personalized content, but they
face a challenge in unbiased learning due to selection bias, where users only
interact with items they prefer. This bias leads to a distorted representation
of user preferences, which hinders the accuracy and fairness of
recommendations. To address the issue, various methods such as error imputation
based, inverse propensity scoring, and doubly robust techniques have been
developed. Despite the progress, from the structural causal model perspective,
previous debiasing methods in RS assume the independence of the exogenous
variables. In this paper, we release this assumption and propose a learning
algorithm based on likelihood maximization to learn a prediction model. We
first discuss the correlation and difference between unmeasured confounding and
our scenario, then we propose a unified method that effectively handles latent
exogenous variables. Specifically, our method models the data generation
process with latent exogenous variables under mild normality assumptions. We
then develop a Monte Carlo algorithm to numerically estimate the likelihood
function. Extensive experiments on synthetic datasets and three real-world
datasets demonstrate the effectiveness of our proposed method. The code is at
https://github.com/WallaceSUI/kdd25-background-variable.

</details>


### [568] [Flowing Datasets with Wasserstein over Wasserstein Gradient Flows](https://arxiv.org/abs/2506.07534)
*Clément Bonet,Christophe Vauthier,Anna Korba*

Main category: cs.LG

TL;DR: 论文提出了一种基于最优传输的Wasserstein over Wasserstein (WoW)距离，用于处理概率分布上的梯度流，应用于迁移学习和数据集蒸馏任务。


<details>
  <summary>Details</summary>
Motivation: 机器学习中许多应用涉及概率分布表示的数据，需要设计新的梯度流方法来处理这类无限维对象。

Method: 将每个类别表示为特征的条件分布，数据集建模为这些类别的混合分布，并引入WoW距离和梯度流。

Result: 通过WoW梯度流和新型功能，成功应用于迁移学习和数据集蒸馏任务。

Conclusion: WoW框架为处理概率分布数据提供了有效的工具，尤其在迁移学习和数据集蒸馏中表现出色。

Abstract: Many applications in machine learning involve data represented as probability
distributions. The emergence of such data requires radically novel techniques
to design tractable gradient flows on probability distributions over this type
of (infinite-dimensional) objects. For instance, being able to flow labeled
datasets is a core task for applications ranging from domain adaptation to
transfer learning or dataset distillation. In this setting, we propose to
represent each class by the associated conditional distribution of features,
and to model the dataset as a mixture distribution supported on these classes
(which are themselves probability distributions), meaning that labeled datasets
can be seen as probability distributions over probability distributions. We
endow this space with a metric structure from optimal transport, namely the
Wasserstein over Wasserstein (WoW) distance, derive a differential structure on
this space, and define WoW gradient flows. The latter enables to design
dynamics over this space that decrease a given objective functional. We apply
our framework to transfer learning and dataset distillation tasks, leveraging
our gradient flow construction as well as novel tractable functionals that take
the form of Maximum Mean Discrepancies with Sliced-Wasserstein based kernels
between probability distributions.

</details>


### [569] [Improving Memory Efficiency for Training KANs via Meta Learning](https://arxiv.org/abs/2506.07549)
*Zhangchi Zhao,Jun Shu,Deyu Meng,Zongben Xu*

Main category: cs.LG

TL;DR: MetaKANs通过元学习生成KANs的权重，显著减少可训练参数，提升内存效率，同时保持性能与可解释性。


<details>
  <summary>Details</summary>
Motivation: KANs虽然高效且可解释，但参数过多导致内存和训练成本高，MetaKANs旨在解决这一问题。

Method: 使用较小的元学习器（MetaKANs）为KANs生成权重，并通过端到端可微分方式联合训练。

Result: 在符号回归、偏微分方程求解和图像分类等任务中，MetaKANs显著减少参数并保持性能。

Conclusion: MetaKANs为KANs提供了一种更高效、可扩展的训练方法，缩小了与MLPs的训练成本差距。

Abstract: Inspired by the Kolmogorov-Arnold representation theorem, KANs offer a novel
framework for function approximation by replacing traditional neural network
weights with learnable univariate functions. This design demonstrates
significant potential as an efficient and interpretable alternative to
traditional MLPs. However, KANs are characterized by a substantially larger
number of trainable parameters, leading to challenges in memory efficiency and
higher training costs compared to MLPs. To address this limitation, we propose
to generate weights for KANs via a smaller meta-learner, called MetaKANs. By
training KANs and MetaKANs in an end-to-end differentiable manner, MetaKANs
achieve comparable or even superior performance while significantly reducing
the number of trainable parameters and maintaining promising interpretability.
Extensive experiments on diverse benchmark tasks, including symbolic
regression, partial differential equation solving, and image classification,
demonstrate the effectiveness of MetaKANs in improving parameter efficiency and
memory usage. The proposed method provides an alternative technique for
training KANs, that allows for greater scalability and extensibility, and
narrows the training cost gap with MLPs stated in the original paper of KANs.
Our code is available at https://github.com/Murphyzc/MetaKAN.

</details>


### [570] [ChemAgent: Enhancing LLMs for Chemistry and Materials Science through Tree-Search Based Tool Learning](https://arxiv.org/abs/2506.07551)
*Mengsong Wu,YaFei Wang,Yidong Ming,Yuqi An,Yuwei Wan,Wenliang Chen,Binbin Lin,Yuqiang Li,Tong Xie,Dongzhan Zhou*

Main category: cs.LG

TL;DR: 提出了一种基于LLM的代理，整合外部化学工具和数据集ChemToolBench，通过HE-MCTS框架优化工具规划和执行，显著提升化学任务性能。


<details>
  <summary>Details</summary>
Motivation: 解决LLMs在化学任务中因知识过时和难以整合专业知识而面临的挑战。

Method: 提出LLM代理，整合137种化学工具，使用HE-MCTS框架优化工具规划和执行，并通过自生成数据支持模型微调。

Result: 实验表明，该方法在化学问答和发现任务中性能显著提升，超越GPT-4o。

Conclusion: 为LLMs整合专业工具提供了高效解决方案，适用于高级化学应用。

Abstract: Large language models (LLMs) have recently demonstrated promising
capabilities in chemistry tasks while still facing challenges due to outdated
pretraining knowledge and the difficulty of incorporating specialized chemical
expertise. To address these issues, we propose an LLM-based agent that
synergistically integrates 137 external chemical tools created ranging from
basic information retrieval to complex reaction predictions, and a dataset
curation pipeline to generate the dataset ChemToolBench that facilitates both
effective tool selection and precise parameter filling during fine-tuning and
evaluation. We introduce a Hierarchical Evolutionary Monte Carlo Tree Search
(HE-MCTS) framework, enabling independent optimization of tool planning and
execution. By leveraging self-generated data, our approach supports step-level
fine-tuning (FT) of the policy model and training task-adaptive PRM and ORM
that surpass GPT-4o. Experimental evaluations demonstrate that our approach
significantly improves performance in Chemistry QA and discovery tasks,
offering a robust solution to integrate specialized tools with LLMs for
advanced chemical applications. All datasets and code are available at
https://github.com/AI4Chem/ChemistryAgent .

</details>


### [571] [Denoising the Future: Top-p Distributions for Moving Through Time](https://arxiv.org/abs/2506.07578)
*Florian Andreas Marwitz,Ralf Möller,Magnus Bender,Marcel Gehrke*

Main category: cs.LG

TL;DR: 提出了一种通过仅使用概率最高的top-p状态来加速动态概率模型推理的方法，显著提升效率且误差可控。


<details>
  <summary>Details</summary>
Motivation: 动态概率模型推理复杂且计算成本高，尤其是隐马尔可夫模型中需枚举整个状态空间，导致效率低下和噪声传播。

Method: 仅使用累积概率为p的最可能状态（top-p状态）进行推理，以减少计算量和噪声。

Result: 实验表明，该方法可实现至少一个数量级的加速，且总变差距离误差低于0.09。

Conclusion: 该方法在显著提升推理速度的同时，误差可控，适用于高效动态概率模型推理。

Abstract: Inference in dynamic probabilistic models is a complex task involving
expensive operations. In particular, for Hidden Markov Models, the whole state
space has to be enumerated for advancing in time. Even states with negligible
probabilities are considered, resulting in computational inefficiency and
increased noise due to the propagation of unlikely probability mass. We propose
to denoise the future and speed up inference by using only the top-p states,
i.e., the most probable states with accumulated probability p. We show that the
error introduced by using only the top-p states is bound by p and the so-called
minimal mixing rate of the underlying model. Moreover, in our empirical
evaluation, we show that we can expect speedups of at least an order of
magnitude, while the error in terms of total variation distance is below 0.09.

</details>


### [572] [FedCGD: Collective Gradient Divergence Optimized Scheduling for Wireless Federated Learning](https://arxiv.org/abs/2506.07581)
*Tan Chen,Jintao Yan,Yuxuan Sun,Sheng Zhou,Zhisheng Niu*

Main category: cs.LG

TL;DR: 该论文提出了一种新的联邦学习（FL）设备调度策略FedCGD，通过最小化多级集体梯度发散（CGD）来优化收敛速度，同时减少设备调度数量。


<details>
  <summary>Details</summary>
Motivation: 在无线网络中，数据异构性和带宽限制是影响FL性能的主要问题。现有研究多关注设备级数据异构性，而忽略了设备组和样本级的影响。

Method: 论文证明了FL收敛速度受设备级和样本级CGD的共同影响，并将设备级CGD转化为加权地球移动距离（WEMD），提出FedCGD算法平衡WEMD和采样方差。

Result: 在CIFAR-10数据集上，FedCGD将分类准确率提升4.2%，同时减少41.8%的设备调度。

Conclusion: FedCGD通过多级CGD优化，显著提升FL性能，并灵活权衡WEMD和采样方差。

Abstract: Federated learning (FL) is a promising paradigm for multiple devices to
cooperatively train a model. When applied in wireless networks, two issues
consistently affect the performance of FL, i.e., data heterogeneity of devices
and limited bandwidth. Many papers have investigated device scheduling
strategies considering the two issues. However, most of them recognize data
heterogeneity as a property of individual devices. In this paper, we prove that
the convergence speed of FL is affected by the sum of device-level and
sample-level collective gradient divergence (CGD). The device-level CGD refers
to the gradient divergence of the scheduled device group, instead of the sum of
the individual device divergence. The sample-level CGD is statistically upper
bounded by sampling variance, which is inversely proportional to the total
number of samples scheduled for local update. To derive a tractable form of the
device-level CGD, we further consider a classification problem and transform it
into the weighted earth moving distance (WEMD) between the group distribution
and the global distribution. Then we propose FedCGD algorithm to minimize the
sum of multi-level CGDs by balancing WEMD and sampling variance, within
polynomial time. Simulation shows that the proposed strategy increases
classification accuracy on the CIFAR-10 dataset by up to 4.2\% while scheduling
41.8\% fewer devices, and flexibly switches between reducing WEMD and reducing
sampling variance.

</details>


### [573] [MIRA: Medical Time Series Foundation Model for Real-World Health Data](https://arxiv.org/abs/2506.07584)
*Hao Li,Bowen Deng,Chang Xu,Zhiyuan Feng,Viktor Schlegel,Yu-Hao Huang,Yizheng Sun,Jingyuan Sun,Kailai Yang,Yiyao Yu,Jiang Bian*

Main category: cs.LG

TL;DR: MIRA是一个专为医学时间序列设计的统一基础模型，通过创新方法解决了医学数据中的不规则间隔和缺失值问题，显著提升了预测准确性。


<details>
  <summary>Details</summary>
Motivation: 医学时间序列数据具有不规则间隔、异质采样率和频繁缺失值等挑战，现有通用基础模型难以处理，因此需要专门设计的模型。

Method: MIRA引入了连续时间旋转位置编码、频率特定专家混合层和基于神经ODE的连续动态外推块，以建模时间间隔和连续轨迹。

Result: 在公开数据集上预训练后，MIRA在分布外和分布内场景中的预测误差分别平均降低10%和7%。

Conclusion: MIRA为医学时间序列建模提供了统一基础，并通过全面基准测试为未来研究奠定了基础。

Abstract: A unified foundation model for medical time series -- pretrained on open
access and ethics board-approved medical corpora -- offers the potential to
reduce annotation burdens, minimize model customization, and enable robust
transfer across clinical institutions, modalities, and tasks, particularly in
data-scarce or privacy-constrained environments. However, existing generalist
time series foundation models struggle to handle medical time series data due
to their inherent challenges, including irregular intervals, heterogeneous
sampling rates, and frequent missing values. To address these challenges, we
introduce MIRA, a unified foundation model specifically designed for medical
time series forecasting. MIRA incorporates a Continuous-Time Rotary Positional
Encoding that enables fine-grained modeling of variable time intervals, a
frequency-specific mixture-of-experts layer that routes computation across
latent frequency regimes to further promote temporal specialization, and a
Continuous Dynamics Extrapolation Block based on Neural ODE that models the
continuous trajectory of latent states, enabling accurate forecasting at
arbitrary target timestamps. Pretrained on a large-scale and diverse medical
corpus comprising over 454 billion time points collect from publicly available
datasets, MIRA achieves reductions in forecasting errors by an average of 10%
and 7% in out-of-distribution and in-distribution scenarios, respectively, when
compared to other zero-shot and fine-tuned baselines. We also introduce a
comprehensive benchmark spanning multiple downstream clinical tasks,
establishing a foundation for future research in medical time series modeling.

</details>


### [574] [Aircraft Trajectory Dataset Augmentation in Latent Space](https://arxiv.org/abs/2506.07585)
*Seokbin Yoon,Keumjin Lee*

Main category: cs.LG

TL;DR: 提出了一种名为ATRADA的新型框架，用于飞机轨迹数据集的增强，通过Transformer编码器和GMM生成高质量的合成轨迹数据。


<details>
  <summary>Details</summary>
Motivation: 飞机轨迹建模在航空交通管理中至关重要，但现有数据集可能不足或不平衡，需要合成数据增强以提高模型鲁棒性。

Method: 使用Transformer编码器学习原始轨迹数据的潜在模式，通过PCA降维和GMM拟合数据分布，再用MLP解码生成新样本。

Result: 实验表明，ATRADA能有效生成高质量的合成轨迹数据，优于多个基线方法。

Conclusion: ATRADA框架为飞机轨迹数据增强提供了一种有效解决方案，有助于提升下游任务的性能。

Abstract: Aircraft trajectory modeling plays a crucial role in Air Traffic Management
(ATM) and is important for various downstream tasks, including conflict
detection and landing time prediction. Dataset augmentation through the
addition of synthetically generated trajectory data is necessary to develop a
more robust aircraft trajectory model and ensure that the trajectory dataset is
sufficient and balanced. In this work, we propose a novel framework called
ATRADA for aircraft trajectory dataset augmentation. In the proposed framework,
a Transformer encoder learns the underlying patterns in the original trajectory
dataset and converts each data point into a context vector in the learned
latent space. The converted dataset in the latent space is projected into
reduced dimensions using principal component analysis (PCA), and a Gaussian
mixture model (GMM) is applied to fit the probability distribution of the data
points in the reduced-dimensional space. Finally, new samples are drawn from
the fitted GMM, the dimension of the samples is reverted to the original
dimension, and they are decoded with a Multi-Layer Perceptron (MLP). Several
experiments demonstrate that the framework effectively generates new,
high-quality synthetic aircraft trajectory data, which were compared to the
results of several baselines.

</details>


### [575] [PrunePEFT: Iterative Hybrid Pruning for Parameter-Efficient Fine-tuning of LLMs](https://arxiv.org/abs/2506.07587)
*Tongzhou Yu,Zhuhao Zhang,Guanghui Zhu,Shen Jiang,Meikang Qiu,Yihua Huang*

Main category: cs.LG

TL;DR: PrunePEFT将PEFT策略搜索转化为剪枝问题，通过混合剪枝策略优化模块配置，显著降低计算负担。


<details>
  <summary>Details</summary>
Motivation: PEFT方法设计空间大，传统搜索方法开销高，需更高效的优化方案。

Method: 将PEFT策略搜索建模为剪枝问题，采用混合剪枝策略迭代移除冗余模块。

Result: 显著减少计算负担，优化配置，提升效率。

Conclusion: PrunePEFT为大规模预训练模型提供了一种可扩展且高效的微调解决方案。

Abstract: Parameter Efficient Fine-Tuning (PEFT) methods have emerged as effective and
promising approaches for fine-tuning pre-trained language models. Compared with
Full parameter Fine-Tuning (FFT), PEFT achieved comparable task performance
with a substantial reduction of trainable parameters, which largely saved the
training and storage costs. However, using the PEFT method requires considering
a vast design space, such as the type of PEFT modules and their insertion
layers. Inadequate configurations can lead to sub-optimal results. Conventional
solutions such as architectural search techniques, while effective, tend to
introduce substantial additional overhead. In this paper, we propose a novel
approach, PrunePEFT, which formulates the PEFT strategy search as a pruning
problem and introduces a hybrid pruning strategy that capitalizes on the
sensitivity of pruning methods to different PEFT modules. This method extends
traditional pruning techniques by iteratively removing redundant or conflicting
PEFT modules, thereby optimizing the fine-tuned configuration. By efficiently
identifying the most relevant modules, our approach significantly reduces the
computational burden typically associated with architectural search processes,
making it a more scalable and efficient solution for fine-tuning large
pre-trained models.

</details>


### [576] [Exploiting Curvature in Online Convex Optimization with Delayed Feedback](https://arxiv.org/abs/2506.07595)
*Hao Qiu,Emmanuel Esposito,Mengxiao Zhang*

Main category: cs.LG

TL;DR: 论文研究了具有曲率损失和延迟反馈的在线凸优化问题，提出了改进算法以减少遗憾界。


<details>
  <summary>Details</summary>
Motivation: 现有方法在强凸损失下的遗憾界可能较差，需改进以适应不同延迟情况。

Method: 提出了一种改进的跟随正则化领导者算法，并扩展了在线牛顿步算法以处理延迟。

Result: 新算法实现了更优的遗憾界，并在实验中表现优于现有方法。

Conclusion: 论文提出的算法在理论和实验中均表现出色，填补了现有方法的不足。

Abstract: In this work, we study the online convex optimization problem with curved
losses and delayed feedback. When losses are strongly convex, existing
approaches obtain regret bounds of order $d_{\max} \ln T$, where $d_{\max}$ is
the maximum delay and $T$ is the time horizon. However, in many cases, this
guarantee can be much worse than $\sqrt{d_{\mathrm{tot}}}$ as obtained by a
delayed version of online gradient descent, where $d_{\mathrm{tot}}$ is the
total delay. We bridge this gap by proposing a variant of
follow-the-regularized-leader that obtains regret of order
$\min\{\sigma_{\max}\ln T, \sqrt{d_{\mathrm{tot}}}\}$, where $\sigma_{\max}$ is
the maximum number of missing observations. We then consider exp-concave losses
and extend the Online Newton Step algorithm to handle delays with an adaptive
learning rate tuning, achieving regret $\min\{d_{\max} n\ln T,
\sqrt{d_{\mathrm{tot}}}\}$ where $n$ is the dimension. To our knowledge, this
is the first algorithm to achieve such a regret bound for exp-concave losses.
We further consider the problem of unconstrained online linear regression and
achieve a similar guarantee by designing a variant of the Vovk-Azoury-Warmuth
forecaster with a clipping trick. Finally, we implement our algorithms and
conduct experiments under various types of delay and losses, showing an
improved performance over existing methods.

</details>


### [577] [TwinBreak: Jailbreaking LLM Security Alignments based on Twin Prompts](https://arxiv.org/abs/2506.07596)
*Torsten Krauß,Hamid Dashtbani,Alexandra Dmitrienko*

Main category: cs.LG

TL;DR: TwinBreak是一种创新的安全对齐移除方法，通过识别和修剪与安全功能相关的参数，有效绕过LLM的安全机制，同时保持模型实用性。


<details>
  <summary>Details</summary>
Motivation: 尽管LLM带来了显著的好处，但恶意用户可能利用有害提示绕过安全机制，现有方法成本高或影响模型性能。

Method: TwinBreak通过分析中间输出来隔离安全参数，并修剪这些参数，同时利用TwinPrompt数据集进行验证。

Result: 实验表明，TwinBreak在16种LLM上成功率达到89%至98%，且计算成本低。

Conclusion: TwinBreak提供了一种高效且低成本的方法来绕过LLM的安全机制，同时保持模型实用性。

Abstract: Machine learning is advancing rapidly, with applications bringing notable
benefits, such as improvements in translation and code generation. Models like
ChatGPT, powered by Large Language Models (LLMs), are increasingly integrated
into daily life. However, alongside these benefits, LLMs also introduce social
risks. Malicious users can exploit LLMs by submitting harmful prompts, such as
requesting instructions for illegal activities. To mitigate this, models often
include a security mechanism that automatically rejects such harmful prompts.
However, they can be bypassed through LLM jailbreaks. Current jailbreaks often
require significant manual effort, high computational costs, or result in
excessive model modifications that may degrade regular utility.
  We introduce TwinBreak, an innovative safety alignment removal method.
Building on the idea that the safety mechanism operates like an embedded
backdoor, TwinBreak identifies and prunes parameters responsible for this
functionality. By focusing on the most relevant model layers, TwinBreak
performs fine-grained analysis of parameters essential to model utility and
safety. TwinBreak is the first method to analyze intermediate outputs from
prompts with high structural and content similarity to isolate safety
parameters. We present the TwinPrompt dataset containing 100 such twin prompts.
Experiments confirm TwinBreak's effectiveness, achieving 89% to 98% success
rates with minimal computational requirements across 16 LLMs from five vendors.

</details>


### [578] [FuXi-Air: Urban Air Quality Forecasting Based on Emission-Meteorology-Pollutant multimodal Machine Learning](https://arxiv.org/abs/2506.07616)
*Zhixin Geng,Xu Fan,Xiqiao Lu,Yan Zhang,Guangyuan Yu,Cheng Huang,Qian Wang,Yuewu Li,Weichun Ma,Qi Yu,Libo Wu,Hao Li*

Main category: cs.LG

TL;DR: 论文提出了一种名为FuXi-Air的多模态数据融合模型，用于高效、低成本的城市空气质量预测，优于传统数值模型。


<details>
  <summary>Details</summary>
Motivation: 解决现有空气质量预测方法的高计算成本、低效率和观测数据整合不足的问题。

Method: 结合气象预报、排放清单和污染物监测数据，采用自回归预测框架和帧插值策略。

Result: 模型在25-30秒内完成72小时六种主要污染物的预测，计算效率和精度均优于主流数值模型。

Conclusion: 多模态数据融合显著提升预测精度，为智能城市管理中的空气质量预警提供了技术参考。

Abstract: Air pollution has emerged as a major public health challenge in megacities.
Numerical simulations and single-site machine learning approaches have been
widely applied in air quality forecasting tasks. However, these methods face
multiple limitations, including high computational costs, low operational
efficiency, and limited integration with observational data. With the rapid
advancement of artificial intelligence, there is an urgent need to develop a
low-cost, efficient air quality forecasting model for smart urban management.
An air quality forecasting model, named FuXi-Air, has been constructed in this
study based on multimodal data fusion to support high-precision air quality
forecasting and operated in typical megacities. The model integrates
meteorological forecasts, emission inventories, and pollutant monitoring data
under the guidance of air pollution mechanism. By combining an autoregressive
prediction framework with a frame interpolation strategy, the model
successfully completes 72-hour forecasts for six major air pollutants at an
hourly resolution across multiple monitoring sites within 25-30 seconds. In
terms of both computational efficiency and forecasting accuracy, it outperforms
the mainstream numerical air quality models in operational forecasting work.
Ablation experiments concerning key influencing factors show that although
meteorological data contribute more to model accuracy than emission inventories
do, the integration of multimodal data significantly improves forecasting
precision and ensures that reliable predictions are obtained under differing
pollution mechanisms across megacities. This study provides both a technical
reference and a practical example for applying multimodal data-driven models to
air quality forecasting and offers new insights into building hybrid
forecasting systems to support air pollution risk warning in smart city
management.

</details>


### [579] [The Catechol Benchmark: Time-series Solvent Selection Data for Few-shot Machine Learning](https://arxiv.org/abs/2506.07619)
*Toby Boyne,Juan S. Campos,Becky D. Langdon,Jixiang Qing,Yilin Xie,Shiqiang Zhang,Calvin Tsay,Ruth Misener,Daniel W. Davies,Kim E. Jelfs,Sarah Boyall,Thomas M. Dixon,Linden Schrecker,Jose Pablo Folch*

Main category: cs.LG

TL;DR: 该论文介绍了一个新的数据集，用于机器学习在溶剂选择领域的应用，特别是产率预测。


<details>
  <summary>Details</summary>
Motivation: 化学数据集通常难以获取或需要专业知识清理，限制了机器学习在化学领域的应用。

Method: 提供了一个包含1200多个连续过程条件的瞬态流数据集，用于机器学习模型的基准测试。

Result: 展示了回归算法、迁移学习、特征工程和主动学习的基准测试结果。

Conclusion: 该数据集为溶剂替代和可持续制造提供了重要应用前景。

Abstract: Machine learning has promised to change the landscape of laboratory
chemistry, with impressive results in molecular property prediction and
reaction retro-synthesis. However, chemical datasets are often inaccessible to
the machine learning community as they tend to require cleaning, thorough
understanding of the chemistry, or are simply not available. In this paper, we
introduce a novel dataset for yield prediction, providing the first-ever
transient flow dataset for machine learning benchmarking, covering over 1200
process conditions. While previous datasets focus on discrete parameters, our
experimental set-up allow us to sample a large number of continuous process
conditions, generating new challenges for machine learning models. We focus on
solvent selection, a task that is particularly difficult to model theoretically
and therefore ripe for machine learning applications. We showcase benchmarking
for regression algorithms, transfer-learning approaches, feature engineering,
and active learning, with important applications towards solvent replacement
and sustainable manufacturing.

</details>


### [580] [Return of ChebNet: Understanding and Improving an Overlooked GNN on Long Range Tasks](https://arxiv.org/abs/2506.07624)
*Ali Hariri,Álvaro Arroyo,Alessio Gravina,Moshe Eliasof,Carola-Bibiane Schönlieb,Davide Bacciu,Kamyar Azizzadenesheli,Xiaowen Dong,Pierre Vandergheynst*

Main category: cs.LG

TL;DR: ChebNet在建模远程节点交互方面具有竞争力，但存在训练不稳定的问题。通过将其转化为稳定动态系统（Stable-ChebNet），解决了这一问题，并在多个基准测试中表现优异。


<details>
  <summary>Details</summary>
Motivation: MPNNs和Graph Transformers在捕捉远程依赖关系时存在局限性，而ChebNet可能提供更好的解决方案。

Method: 将ChebNet转化为稳定动态系统（Stable-ChebNet），避免多项式扩展导致的不稳定性。

Result: Stable-ChebNet在远程基准测试中表现优异，接近最先进水平。

Conclusion: Stable-ChebNet是一种高效且稳定的方法，适用于远程节点交互建模。

Abstract: ChebNet, one of the earliest spectral GNNs, has largely been overshadowed by
Message Passing Neural Networks (MPNNs), which gained popularity for their
simplicity and effectiveness in capturing local graph structure. Despite their
success, MPNNs are limited in their ability to capture long-range dependencies
between nodes. This has led researchers to adapt MPNNs through rewiring or make
use of Graph Transformers, which compromises the computational efficiency that
characterized early spatial message-passing architectures, and typically
disregards the graph structure. Almost a decade after its original
introduction, we revisit ChebNet to shed light on its ability to model distant
node interactions. We find that out-of-box, ChebNet already shows competitive
advantages relative to classical MPNNs and GTs on long-range benchmarks, while
maintaining good scalability properties for high-order polynomials. However, we
uncover that this polynomial expansion leads ChebNet to an unstable regime
during training. To address this limitation, we cast ChebNet as a stable and
non-dissipative dynamical system, which we coin Stable-ChebNet. Our
Stable-ChebNet model allows for stable information propagation, and has
controllable dynamics which do not require the use of eigendecompositions,
positional encodings, or graph rewiring. Across several benchmarks,
Stable-ChebNet achieves near state-of-the-art performance.

</details>


### [581] [The Universality Lens: Why Even Highly Over-Parametrized Models Learn Well](https://arxiv.org/abs/2506.07661)
*Meir Feder,Ruediger Urbanke,Yaniv Fogel*

Main category: cs.LG

TL;DR: 论文通过信息论和通用学习理论，研究了过参数化模型（如深度神经网络）泛化能力强的现象，提出模型简单性由假设权重决定，并通过后验集中解释了其避免过拟合的原因。


<details>
  <summary>Details</summary>
Motivation: 探索为何过参数化模型（如深度神经网络和Transformer）在参数远多于训练样本时仍能泛化良好。

Method: 基于信息论和通用学习理论，研究贝叶斯混合学习器及其在KL散度距离下的假设权重。

Result: 发现泛化能力取决于假设权重而非假设类大小，简单模型权重高，泛化所需样本少。

Conclusion: 过参数化模型通过后验集中在简单假设上避免过拟合，理论与实际方法（如随机梯度下降）结合，提供了统一的理论框架。

Abstract: A fundamental question in modern machine learning is why large,
over-parameterized models, such as deep neural networks and transformers, tend
to generalize well, even when their number of parameters far exceeds the number
of training samples.
  We investigate this phenomenon through the lens of information theory,
grounded in universal learning theory. Specifically, we study a Bayesian
mixture learner with log-loss and (almost) uniform prior over an expansive
hypothesis class.
  Our key result shows that the learner's regret is not determined by the
overall size of the hypothesis class, but rather by the cumulative probability
of all models that are close, in Kullback-Leibler divergence distance, to the
true data-generating process. We refer to this cumulative probability as the
weight of the hypothesis.
  This leads to a natural notion of model simplicity: simple models are those
with large weight and thus require fewer samples to generalize, while complex
models have small weight and need more data. This perspective provides a
rigorous and intuitive explanation for why over-parameterized models often
avoid overfitting: the presence of simple hypotheses allows the posterior to
concentrate on them when supported by the data.
  We further bridge theory and practice by recalling that stochastic gradient
descent with Langevin dynamics samples from the correct posterior distribution,
enabling our theoretical learner to be approximated using standard machine
learning methods combined with ensemble learning.
  Our analysis yields non-uniform regret bounds and aligns with key practical
concepts such as flat minima and model distillation. The results apply broadly
across online, batch, and supervised learning settings, offering a unified and
principled understanding of the generalization behavior of modern AI systems.

</details>


### [582] [ProARD: progressive adversarial robustness distillation: provide wide range of robust students](https://arxiv.org/abs/2506.07666)
*Seyedhamidreza Mousavi,Seyedali Mousavi,Masoud Daneshtalab*

Main category: cs.LG

TL;DR: ProARD提出了一种动态网络训练方法，通过一次训练支持多种轻量级学生网络，避免了重复训练的高计算成本和碳排放。


<details>
  <summary>Details</summary>
Motivation: 现有ARD方法需要为不同资源约束的设备重新训练学生网络，导致高计算成本和碳排放。

Method: 基于动态层构建动态网络，支持多种架构；以最大学生网络为动态教师网络，通过权重共享和采样机制联合优化。

Result: 随机采样学生网络效果不佳，需更有效的采样策略。

Conclusion: ProARD为轻量级网络的对抗鲁棒性提供了一种高效训练方法。

Abstract: Adversarial Robustness Distillation (ARD) has emerged as an effective method
to enhance the robustness of lightweight deep neural networks against
adversarial attacks. Current ARD approaches have leveraged a large robust
teacher network to train one robust lightweight student. However, due to the
diverse range of edge devices and resource constraints, current approaches
require training a new student network from scratch to meet specific
constraints, leading to substantial computational costs and increased CO2
emissions. This paper proposes Progressive Adversarial Robustness Distillation
(ProARD), enabling the efficient one-time training of a dynamic network that
supports a diverse range of accurate and robust student networks without
requiring retraining. We first make a dynamic deep neural network based on
dynamic layers by encompassing variations in width, depth, and expansion in
each design stage to support a wide range of architectures. Then, we consider
the student network with the largest size as the dynamic teacher network.
ProARD trains this dynamic network using a weight-sharing mechanism to jointly
optimize the dynamic teacher network and its internal student networks.
However, due to the high computational cost of calculating exact gradients for
all the students within the dynamic network, a sampling mechanism is required
to select a subset of students. We show that random student sampling in each
iteration fails to produce accurate and robust students.

</details>


### [583] [How Benchmark Prediction from Fewer Data Misses the Mark](https://arxiv.org/abs/2506.07673)
*Guanhua Zhang,Florian E. Dorner,Moritz Hardt*

Main category: cs.LG

TL;DR: 论文研究了11种基准预测方法在19个不同基准上的表现，发现随机采样加回归模型优于多数现有方法，且所有方法依赖于模型相似性。提出了一种新方法，在模型相似性高时表现更好，但在评估前沿（新模型能力未知时）效果有限。


<details>
  <summary>Details</summary>
Motivation: 由于大型语言模型（LLM）评估成本高昂，研究如何通过缩小基准数据集来加速评估。

Method: 系统评估11种基准预测方法，发现随机采样加回归模型是最强基线。提出一种新方法（基于增强逆倾向加权）以改进外推性能。

Result: 现有方法依赖模型相似性，外推时表现不佳；新方法在模型相似性高时优于随机采样平均，但增益有限。

Conclusion: 基准预测在评估前沿（新模型能力未知时）效果不佳，亟需更鲁棒的方法。

Abstract: Large language model (LLM) evaluation is increasingly costly, prompting
interest in methods that speed up evaluation by shrinking benchmark datasets.
Benchmark prediction (also called efficient LLM evaluation) aims to select a
small subset of evaluation points and predict overall benchmark performance
from that subset. In this paper, we systematically assess the strengths and
limitations of 11 benchmark prediction methods across 19 diverse benchmarks.
First, we identify a highly competitive baseline: Take a random sample and fit
a regression model on the sample to predict missing entries. Outperforming most
existing methods, this baseline challenges the assumption that careful subset
selection is necessary for benchmark prediction. Second, we discover that all
existing methods crucially depend on model similarity. They work best when
interpolating scores among similar models. The effectiveness of benchmark
prediction sharply declines when new models have higher accuracy than
previously seen models. In this setting of extrapolation, none of the previous
methods consistently beat a simple average over random samples. To improve over
the sample average, we introduce a new method inspired by augmented inverse
propensity weighting. This method consistently outperforms the random sample
average even for extrapolation. However, its performance still relies on model
similarity and the gains are modest in general. This shows that benchmark
prediction fails just when it is most needed: at the evaluation frontier, where
the goal is to evaluate new models of unknown capabilities.

</details>


### [584] [Evaluating Robustness in Latent Diffusion Models via Embedding Level Augmentation](https://arxiv.org/abs/2506.07706)
*Boris Martirosyan,Alexey Karmanov*

Main category: cs.LG

TL;DR: 本文提出提升潜在扩散模型（LDMs）鲁棒性的方法，包括分离文本编码器评估、数据增强技术、模型微调及新评估流程。


<details>
  <summary>Details</summary>
Motivation: LDMs在多任务中表现优异，但鲁棒性不足，现有研究未充分探讨此问题。

Method: 1. 分离文本编码器评估鲁棒性；2. 设计数据增强技术；3. 用Dreambooth微调模型；4. 提出新评估流程。

Result: 通过实验验证了所提方法的有效性。

Conclusion: 本文方法显著提升了LDMs的鲁棒性，为未来研究提供了新方向。

Abstract: Latent diffusion models (LDMs) achieve state-of-the-art performance across
various tasks, including image generation and video synthesis. However, they
generally lack robustness, a limitation that remains not fully explored in
current research. In this paper, we propose several methods to address this
gap. First, we hypothesize that the robustness of LDMs primarily should be
measured without their text encoder, because if we take and explore the whole
architecture, the problems of image generator and text encoders wll be fused.
Second, we introduce novel data augmentation techniques designed to reveal
robustness shortcomings in LDMs when processing diverse textual prompts. We
then fine-tune Stable Diffusion 3 and Stable Diffusion XL models using
Dreambooth, incorporating these proposed augmentation methods across multiple
tasks. Finally, we propose a novel evaluation pipeline specifically tailored to
assess the robustness of LDMs fine-tuned via Dreambooth.

</details>


### [585] [Language Embedding Meets Dynamic Graph: A New Exploration for Neural Architecture Representation Learning](https://arxiv.org/abs/2506.07735)
*Haizhao Jing,Haokui Zhang,Zhenhao Shang,Rong Xiao,Peng Wang,Yanning Zhang*

Main category: cs.LG

TL;DR: LeDG-Former框架通过语言嵌入和动态图表示学习的结合，解决了现有方法忽略硬件属性和静态邻接矩阵的局限性，实现了跨硬件平台的零样本预测，并在多个基准测试中取得最优性能。


<details>
  <summary>Details</summary>
Motivation: 当前方法在神经网络架构表示学习中忽略了硬件属性信息，且依赖静态邻接矩阵，限制了模型的实用性和编码效果。

Method: 提出LeDG-Former框架，结合语言嵌入（利用LLM将架构和硬件投影到统一语义空间）和动态图Transformer（改进架构建模）。

Result: 在NNLQP基准测试中超越现有方法，实现跨硬件延迟预测，并在NAS-Bench-101和NAS-Bench-201数据集上表现优异。

Conclusion: LeDG-Former通过创新的语言嵌入和动态图表示学习，显著提升了神经网络架构表示学习的性能和应用范围。

Abstract: Neural Architecture Representation Learning aims to transform network models
into feature representations for predicting network attributes, playing a
crucial role in deploying and designing networks for real-world applications.
Recently, inspired by the success of transformers, transformer-based models
integrated with Graph Neural Networks (GNNs) have achieved significant progress
in representation learning. However, current methods still have some
limitations. First, existing methods overlook hardware attribute information,
which conflicts with the current trend of diversified deep learning hardware
and limits the practical applicability of models. Second, current encoding
approaches rely on static adjacency matrices to represent topological
structures, failing to capture the structural differences between computational
nodes, which ultimately compromises encoding effectiveness. In this paper, we
introduce LeDG-Former, an innovative framework that addresses these limitations
through the synergistic integration of language-based semantic embedding and
dynamic graph representation learning. Specifically, inspired by large language
models (LLMs), we propose a language embedding framework where both neural
architectures and hardware platform specifications are projected into a unified
semantic space through tokenization and LLM processing, enabling zero-shot
prediction across different hardware platforms for the first time. Then, we
propose a dynamic graph-based transformer for modeling neural architectures,
resulting in improved neural architecture modeling performance. On the NNLQP
benchmark, LeDG-Former surpasses previous methods, establishing a new SOTA
while demonstrating the first successful cross-hardware latency prediction
capability. Furthermore, our framework achieves superior performance on the
cell-structured NAS-Bench-101 and NAS-Bench-201 datasets.

</details>


### [586] [E-LDA: Toward Interpretable LDA Topic Models with Strong Guarantees in Logarithmic Parallel Time](https://arxiv.org/abs/2506.07747)
*Adam Breuer*

Main category: cs.LG

TL;DR: 本文提出了一种新颖的非梯度组合方法，用于推断LDA主题模型中每个文档的主题分配，实现了对数级并行计算时间的高效收敛，并提供了可解释性保证。


<details>
  <summary>Details</summary>
Motivation: 解决LDA主题模型在社会科学、数据探索和因果推断等应用中的主要推断问题，提供高效且可解释的算法。

Method: 采用非梯度组合方法估计主题模型，实现对数级并行计算时间的收敛。

Result: 算法在多种文本数据集和评估参数下，语义质量优于现有LDA、神经主题模型和基于LLM的主题模型。

Conclusion: 该方法不仅高效且可解释，还能保持独立性假设，适用于下游因果推断研究。

Abstract: In this paper, we provide the first practical algorithms with provable
guarantees for the problem of inferring the topics assigned to each document in
an LDA topic model. This is the primary inference problem for many applications
of topic models in social science, data exploration, and causal inference
settings. We obtain this result by showing a novel non-gradient-based,
combinatorial approach to estimating topic models. This yields algorithms that
converge to near-optimal posterior probability in logarithmic parallel
computation time (adaptivity) -- exponentially faster than any known LDA
algorithm. We also show that our approach can provide interpretability
guarantees such that each learned topic is formally associated with a known
keyword. Finally, we show that unlike alternatives, our approach can maintain
the independence assumptions necessary to use the learned topic model for
downstream causal inference methods that allow researchers to study topics as
treatments. In terms of practical performance, our approach consistently
returns solutions of higher semantic quality than solutions from
state-of-the-art LDA algorithms, neural topic models, and LLM-based topic
models across a diverse range of text datasets and evaluation parameters.

</details>


### [587] [Comparing Credit Risk Estimates in the Gen-AI Era](https://arxiv.org/abs/2506.07754)
*Nicola Lavecchia,Sid Fadanelli,Federico Ricciuti,Gennaro Aloe,Enrico Bagli,Pietro Giuffrida,Daniele Vergari*

Main category: cs.LG

TL;DR: 生成式AI在信用评分建模中表现不及传统方法，需进一步研究。


<details>
  <summary>Details</summary>
Motivation: 比较生成式AI与传统方法在信用评分建模中的表现，探索其潜力与局限性。

Method: 对比分析生成式AI与传统信用评分建模技术，评估不同整合策略的效果。

Result: 当前生成式AI模型在信用评分任务中表现不如传统方法。

Conclusion: 生成式AI在信用风险评分中能力有限，需更多研发才能应用。

Abstract: Generative AI technologies have demonstrated significant potential across
diverse applications. This study provides a comparative analysis of credit
score modeling techniques, contrasting traditional approaches with those
leveraging generative AI. Our findings reveal that current generative AI models
fall short of matching the performance of traditional methods, regardless of
the integration strategy employed. These results highlight the limitations in
the current capabilities of generative AI for credit risk scoring, emphasizing
the need for further research and development before the possibility of
applying generative AI for this specific task, or equivalent ones.

</details>


### [588] [Clustered Federated Learning via Embedding Distributions](https://arxiv.org/abs/2506.07769)
*Dekai Zhang,Matthew Williams,Francesca Toni*

Main category: cs.LG

TL;DR: 论文提出了一种名为EMD-CFL的单次聚类方法，用于解决联邦学习中非独立同分布数据的挑战。


<details>
  <summary>Details</summary>
Motivation: 联邦学习在分布式数据环境中广泛应用，但非独立同分布数据会降低其性能。

Method: 使用嵌入空间中的Earth Mover's距离（EMD）进行聚类。

Result: 在16个基线方法和多个数据集上表现出优越的聚类性能。

Conclusion: EMD-CFL是一种有效的解决方案，能够提升联邦学习在非独立同分布数据环境中的表现。

Abstract: Federated learning (FL) is a widely used framework for machine learning in
distributed data environments where clients hold data that cannot be easily
centralised, such as for data protection reasons. FL, however, is known to be
vulnerable to non-IID data. Clustered FL addresses this issue by finding more
homogeneous clusters of clients. We propose a novel one-shot clustering method,
EMD-CFL, using the Earth Mover's distance (EMD) between data distributions in
embedding space. We theoretically motivate the use of EMDs using results from
the domain adaptation literature and demonstrate empirically superior
clustering performance in extensive comparisons against 16 baselines and on a
range of challenging datasets.

</details>


### [589] [Enhancing Adversarial Robustness with Conformal Prediction: A Framework for Guaranteed Model Reliability](https://arxiv.org/abs/2506.07804)
*Jie Bao,Chuangyin Dang,Rui Luo,Hanwei Zhang,Zhixin Zhou*

Main category: cs.LG

TL;DR: 论文提出了一种基于Conformal Prediction的对抗攻击方法OPSA和防御策略OPSA-AT，旨在提升深度学习模型的鲁棒性和不确定性估计。


<details>
  <summary>Details</summary>
Motivation: 深度学习模型在高风险应用中需要更强的对抗防御和可靠的性能保证，仅靠准确性不足以保证模型的可信度。

Method: 开发了OPSA攻击方法，通过最大化模型不确定性降低Conformal Prediction效率；提出了OPSA-AT防御策略，将OPSA整合到新的Conformal训练框架中。

Result: OPSA攻击比其他基线方法能诱导更大的不确定性；OPSA-AT防御模型显著提升了对抗攻击的鲁棒性，并保持了可靠的预测性能。

Conclusion: 该集成方法为安全关键领域开发可信赖且鲁棒的深度学习模型提供了有效途径。

Abstract: As deep learning models are increasingly deployed in high-risk applications,
robust defenses against adversarial attacks and reliable performance guarantees
become paramount. Moreover, accuracy alone does not provide sufficient
assurance or reliable uncertainty estimates for these models. This study
advances adversarial training by leveraging principles from Conformal
Prediction. Specifically, we develop an adversarial attack method, termed OPSA
(OPtimal Size Attack), designed to reduce the efficiency of conformal
prediction at any significance level by maximizing model uncertainty without
requiring coverage guarantees. Correspondingly, we introduce OPSA-AT
(Adversarial Training), a defense strategy that integrates OPSA within a novel
conformal training paradigm. Experimental evaluations demonstrate that our OPSA
attack method induces greater uncertainty compared to baseline approaches for
various defenses. Conversely, our OPSA-AT defensive model significantly
enhances robustness not only against OPSA but also other adversarial attacks,
and maintains reliable prediction. Our findings highlight the effectiveness of
this integrated approach for developing trustworthy and resilient deep learning
models for safety-critical domains. Our code is available at
https://github.com/bjbbbb/Enhancing-Adversarial-Robustness-with-Conformal-Prediction.

</details>


### [590] [Identifiable Object Representations under Spatial Ambiguities](https://arxiv.org/abs/2506.07806)
*Avinash Kori,Francesca Toni,Ben Glocker*

Main category: cs.LG

TL;DR: 提出一种多视角概率方法，解决空间模糊性问题，无需视角标注，并在实验中验证了其鲁棒性和可扩展性。


<details>
  <summary>Details</summary>
Motivation: 模块化物体中心表征对人类推理至关重要，但在空间模糊性（如遮挡和视角模糊）下难以实现。

Method: 引入多视角概率方法，聚合视角特定槽以捕捉不变内容信息，同时学习解耦的全局视角级信息。

Result: 方法解决了空间模糊性，提供了可识别性的理论保证，并在标准和新复杂数据集上验证了其性能。

Conclusion: 该方法在无需视角标注的情况下，显著提升了物体中心表征的性能，具有理论和实践优势。

Abstract: Modular object-centric representations are essential for *human-like
reasoning* but are challenging to obtain under spatial ambiguities, *e.g. due
to occlusions and view ambiguities*. However, addressing challenges presents
both theoretical and practical difficulties. We introduce a novel multi-view
probabilistic approach that aggregates view-specific slots to capture
*invariant content* information while simultaneously learning disentangled
global *viewpoint-level* information. Unlike prior single-view methods, our
approach resolves spatial ambiguities, provides theoretical guarantees for
identifiability, and requires *no viewpoint annotations*. Extensive experiments
on standard benchmarks and novel complex datasets validate our method's
robustness and scalability.

</details>


### [591] [Accelerating Diffusion Models in Offline RL via Reward-Aware Consistency Trajectory Distillation](https://arxiv.org/abs/2506.07822)
*Xintong Duan,Yutong He,Fahim Tajwar,Ruslan Salakhutdinov,J. Zico Kolter,Jeff Schneider*

Main category: cs.LG

TL;DR: 提出了一种新的离线强化学习方法，通过将奖励优化直接融入一致性蒸馏过程，实现单步生成，同时保持高性能和简化训练。


<details>
  <summary>Details</summary>
Motivation: 扩散模型在决策任务中表现优异，但推理速度慢；一致性模型虽能解决此问题，但在决策任务中常因次优演示或复杂训练而受限。

Method: 提出一种新颖的一致性蒸馏方法，将奖励优化直接整合到蒸馏过程中，实现单步生成。

Result: 在Gym MuJoCo基准测试和长时程规划中，性能提升8.7%，推理速度比扩散模型快142倍。

Conclusion: 该方法在保持高性能的同时显著提升了推理速度，为决策任务提供了一种高效解决方案。

Abstract: Although diffusion models have achieved strong results in decision-making
tasks, their slow inference speed remains a key limitation. While the
consistency model offers a potential solution, its applications to
decision-making often struggle with suboptimal demonstrations or rely on
complex concurrent training of multiple networks. In this work, we propose a
novel approach to consistency distillation for offline reinforcement learning
that directly incorporates reward optimization into the distillation process.
Our method enables single-step generation while maintaining higher performance
and simpler training. Empirical evaluations on the Gym MuJoCo benchmarks and
long horizon planning demonstrate that our approach can achieve an 8.7%
improvement over previous state-of-the-art while offering up to 142x speedup
over diffusion counterparts in inference time.

</details>


### [592] [Decentralizing Multi-Agent Reinforcement Learning with Temporal Causal Information](https://arxiv.org/abs/2506.07829)
*Jan Corazza,Hadi Partovi Aria,Hyohun Kim,Daniel Neider,Zhe Xu*

Main category: cs.LG

TL;DR: 论文研究了在多智能体强化学习中，通过提供高层符号知识解决隐私、通信和性能问题，并扩展了理论工具以确保策略兼容性。


<details>
  <summary>Details</summary>
Motivation: 现实问题常需多智能体协作，但现有去中心化多智能体强化学习（DMARL）方法在策略兼容性、隐私和通信方面存在挑战。

Method: 扩展了用于检查局部策略与团队任务兼容性的理论工具，并引入符号知识以加速学习过程。

Result: 符号知识显著加快了DMARL的学习速度，扩展的理论工具使去中心化训练在更多场景中可用。

Conclusion: 符号知识和扩展的理论工具能有效解决DMARL中的独特挑战，提升协作效率。

Abstract: Reinforcement learning (RL) algorithms can find an optimal policy for a
single agent to accomplish a particular task. However, many real-world problems
require multiple agents to collaborate in order to achieve a common goal. For
example, a robot executing a task in a warehouse may require the assistance of
a drone to retrieve items from high shelves. In Decentralized Multi-Agent RL
(DMARL), agents learn independently and then combine their policies at
execution time, but often must satisfy constraints on compatibility of local
policies to ensure that they can achieve the global task when combined. In this
paper, we study how providing high-level symbolic knowledge to agents can help
address unique challenges of this setting, such as privacy constraints,
communication limitations, and performance concerns. In particular, we extend
the formal tools used to check the compatibility of local policies with the
team task, making decentralized training with theoretical guarantees usable in
more scenarios. Furthermore, we empirically demonstrate that symbolic knowledge
about the temporal evolution of events in the environment can significantly
expedite the learning process in DMARL.

</details>


### [593] [Improving large language models with concept-aware fine-tuning](https://arxiv.org/abs/2506.07833)
*Michael K. Chen,Xikun Zhang,Jiaxing Huang,Dacheng Tao*

Main category: cs.LG

TL;DR: 论文提出了一种名为概念感知微调（CAFT）的新方法，通过多标记训练改进大语言模型（LLMs）的概念理解能力，显著提升了任务表现。


<details>
  <summary>Details</summary>
Motivation: 现有的大语言模型基于下一个标记预测的范式，限制了其对连贯、高层次概念的理解能力，阻碍了真正智能系统的发展。

Method: 引入概念感知微调（CAFT），一种多标记训练方法，允许模型学习跨多个标记的序列，从而增强概念感知学习。

Result: 实验表明，CAFT在文本摘要和蛋白质设计等任务中显著优于传统的下一个标记微调方法。

Conclusion: CAFT首次将多标记预测引入后训练阶段，为研究社区提供了更高效的工具，并暗示了机器学习研究的更广泛影响。

Abstract: Large language models (LLMs) have become the cornerstone of modern AI.
However, the existing paradigm of next-token prediction fundamentally limits
their ability to form coherent, high-level concepts, making it a critical
barrier to human-like understanding and reasoning. Take the phrase "ribonucleic
acid" as an example: an LLM will first decompose it into tokens, i.e.,
artificial text fragments ("rib", "on", ...), then learn each token
sequentially, rather than grasping the phrase as a unified, coherent semantic
entity. This fragmented representation hinders deeper conceptual understanding
and, ultimately, the development of truly intelligent systems. In response, we
introduce Concept-Aware Fine-Tuning (CAFT), a novel multi-token training method
that redefines how LLMs are fine-tuned. By enabling the learning of sequences
that span multiple tokens, this method fosters stronger concept-aware learning.
Our experiments demonstrate significant improvements compared to conventional
next-token finetuning methods across diverse tasks, including traditional
applications like text summarization and domain-specific ones like de novo
protein design. Multi-token prediction was previously only possible in the
prohibitively expensive pretraining phase; CAFT, to our knowledge, is the first
to bring the multi-token setting to the post-training phase, thus effectively
democratizing its benefits for the broader community of practitioners and
researchers. Finally, the unexpected effectiveness of our proposed method
suggests wider implications for the machine learning research community. All
code and data are available at https://github.com/michaelchen-lab/caft-llm

</details>


### [594] [Jarzynski Reweighting and Sampling Dynamics for Training Energy-Based Models: Theoretical Analysis of Different Transition Kernels](https://arxiv.org/abs/2506.07843)
*Davide Carbone*

Main category: cs.LG

TL;DR: 论文探讨了Jarzynski重加权在训练基于能量的模型（EBMs）中的应用，分析了其理论意义及在不同生成框架中的效果。


<details>
  <summary>Details</summary>
Motivation: 传统方法（如对比散度和分数匹配）在训练EBMs时存在偏差，影响学习准确性。研究旨在利用Jarzynski重加权技术解决这一问题。

Method: 通过理论分析Jarzynski重加权技术，并在两种生成框架（基于流的扩散模型和受限玻尔兹曼机）中验证其效果。

Result: 研究发现Jarzynski重加权能有效减少离散化误差、提高样本质量，并修正对比散度的偏差。

Conclusion: Jarzynski重加权为生成学习提供了一种理论支持的工具，其核选择对模型性能有重要影响。

Abstract: Energy-Based Models (EBMs) provide a flexible framework for generative
modeling, but their training remains theoretically challenging due to the need
to approximate normalization constants and efficiently sample from complex,
multi-modal distributions. Traditional methods, such as contrastive divergence
and score matching, introduce biases that can hinder accurate learning. In this
work, we present a theoretical analysis of Jarzynski reweighting, a technique
from non-equilibrium statistical mechanics, and its implications for training
EBMs. We focus on the role of the choice of the kernel and we illustrate these
theoretical considerations in two key generative frameworks: (i) flow-based
diffusion models, where we reinterpret Jarzynski reweighting in the context of
stochastic interpolants to mitigate discretization errors and improve sample
quality, and (ii) Restricted Boltzmann Machines, where we analyze its role in
correcting the biases of contrastive divergence. Our results provide insights
into the interplay between kernel choice and model performance, highlighting
the potential of Jarzynski reweighting as a principled tool for generative
learning.

</details>


### [595] [Residual Reweighted Conformal Prediction for Graph Neural Networks](https://arxiv.org/abs/2506.07854)
*Zheng Zhang,Jie Bao,Zhixin Zhou,Nicolo Colombo,Lixin Cheng,Rui Luo*

Main category: cs.LG

TL;DR: RR-GNN通过结合图结构和残差重加权，解决了GNN在高风险领域中不确定性量化不足的问题，提供了更高效的预测区间和统计覆盖保证。


<details>
  <summary>Details</summary>
Motivation: GNN在关系数据建模中表现出色，但在高风险领域因不确定性未量化而面临挑战。现有方法生成的预测区间过于保守，未能考虑图的异方差性和结构偏差。

Method: RR-GNN提出三项创新：1) 基于图结构的Mondrian CP分区；2) 残差自适应非一致性评分；3) 交叉训练协议以防止数据泄漏。

Result: 在15个真实图数据上的实验表明，RR-GNN在保持覆盖率的条件下，比现有方法更高效。

Conclusion: RR-GNN通过结合图拓扑和残差动态调整，显著提升了预测性能，适用于多种图任务。

Abstract: Graph Neural Networks (GNNs) excel at modeling relational data but face
significant challenges in high-stakes domains due to unquantified uncertainty.
Conformal prediction (CP) offers statistical coverage guarantees, but existing
methods often produce overly conservative prediction intervals that fail to
account for graph heteroscedasticity and structural biases. While residual
reweighting CP variants address some of these limitations, they neglect graph
topology, cluster-specific uncertainties, and risk data leakage by reusing
training sets. To address these issues, we propose Residual Reweighted GNN
(RR-GNN), a framework designed to generate minimal prediction sets with
provable marginal coverage guarantees.
  RR-GNN introduces three major innovations to enhance prediction performance.
First, it employs Graph-Structured Mondrian CP to partition nodes or edges into
communities based on topological features, ensuring cluster-conditional
coverage that reflects heterogeneity. Second, it uses Residual-Adaptive
Nonconformity Scores by training a secondary GNN on a held-out calibration set
to estimate task-specific residuals, dynamically adjusting prediction intervals
according to node or edge uncertainty. Third, it adopts a Cross-Training
Protocol, which alternates the optimization of the primary GNN and the residual
predictor to prevent information leakage while maintaining graph dependencies.
We validate RR-GNN on 15 real-world graphs across diverse tasks, including node
classification, regression, and edge weight prediction. Compared to CP
baselines, RR-GNN achieves improved efficiency over state-of-the-art methods,
with no loss of coverage.

</details>


### [596] [Fairness Overfitting in Machine Learning: An Information-Theoretic Perspective](https://arxiv.org/abs/2506.07861)
*Firas Laakom,Haobo Chen,Jürgen Schmidhuber,Yuheng Bu*

Main category: cs.LG

TL;DR: 该论文提出了一种基于信息论的理论框架，用于分析公平性泛化误差，并通过Efron-Stein不等式推导出紧致的公平性泛化界。


<details>
  <summary>Details</summary>
Motivation: 现有方法缺乏对公平性在训练数据与未见数据之间泛化的形式化保证，公平性过拟合问题未得到充分研究。

Method: 采用信息论视角，基于Efron-Stein不等式，结合互信息（MI）和条件互信息（CMI）推导公平性泛化界。

Result: 实验验证了所提边界在多种公平性学习算法中的紧致性和实用性。

Conclusion: 该框架为设计提升公平性泛化的算法提供了有价值的指导。

Abstract: Despite substantial progress in promoting fairness in high-stake applications
using machine learning models, existing methods often modify the training
process, such as through regularizers or other interventions, but lack formal
guarantees that fairness achieved during training will generalize to unseen
data. Although overfitting with respect to prediction performance has been
extensively studied, overfitting in terms of fairness loss has received far
less attention. This paper proposes a theoretical framework for analyzing
fairness generalization error through an information-theoretic lens. Our novel
bounding technique is based on Efron-Stein inequality, which allows us to
derive tight information-theoretic fairness generalization bounds with both
Mutual Information (MI) and Conditional Mutual Information (CMI). Our empirical
results validate the tightness and practical relevance of these bounds across
diverse fairness-aware learning algorithms. Our framework offers valuable
insights to guide the design of algorithms improving fairness generalization.

</details>


### [597] [Lightweight Sequential Transformers for Blood Glucose Level Prediction in Type-1 Diabetes](https://arxiv.org/abs/2506.07864)
*Mirko Paolo Barbato,Giorgia Rigamonti,Davide Marelli,Paolo Napoletano*

Main category: cs.LG

TL;DR: 提出了一种轻量级序列Transformer模型，用于T1D患者的血糖预测，解决了穿戴设备上的计算和内存限制问题。


<details>
  <summary>Details</summary>
Motivation: T1D需要持续监测血糖，但现有预测模型在穿戴设备上部署困难，需解决计算和内存限制问题。

Method: 结合Transformer的注意力机制和RNN的序列处理能力，设计轻量级模型，优化边缘设备部署并处理数据不平衡。

Result: 在OhioT1DM和DiaTrend数据集上表现优于现有方法，能有效预测血糖水平和检测不良事件。

Conclusion: 该模型填补了高性能建模与实际部署间的空白，为T1D管理提供了高效可靠的解决方案。

Abstract: Type 1 Diabetes (T1D) affects millions worldwide, requiring continuous
monitoring to prevent severe hypo- and hyperglycemic events. While continuous
glucose monitoring has improved blood glucose management, deploying predictive
models on wearable devices remains challenging due to computational and memory
constraints. To address this, we propose a novel Lightweight Sequential
Transformer model designed for blood glucose prediction in T1D. By integrating
the strengths of Transformers' attention mechanisms and the sequential
processing of recurrent neural networks, our architecture captures long-term
dependencies while maintaining computational efficiency. The model is optimized
for deployment on resource-constrained edge devices and incorporates a balanced
loss function to handle the inherent data imbalance in hypo- and hyperglycemic
events. Experiments on two benchmark datasets, OhioT1DM and DiaTrend,
demonstrate that the proposed model outperforms state-of-the-art methods in
predicting glucose levels and detecting adverse events. This work fills the gap
between high-performance modeling and practical deployment, providing a
reliable and efficient T1D management solution.

</details>


### [598] [Can Hessian-Based Insights Support Fault Diagnosis in Attention-based Models?](https://arxiv.org/abs/2506.07871)
*Sigma Jahan,Mohammad Masudur Rahman*

Main category: cs.LG

TL;DR: 论文研究了基于Hessian的分析方法在诊断注意力模型故障中的潜力，发现其比梯度方法更有效。


<details>
  <summary>Details</summary>
Motivation: 随着注意力模型的规模和复杂性增加，诊断其故障变得更具挑战性，需要更有效的工具。

Method: 使用Hessian矩阵进行曲率分析和参数交互分析，识别注意力机制中的脆弱区域和参数依赖关系。

Result: 在HAN、3D-CNN和DistilBERT模型上的实验表明，Hessian指标能更有效地定位不稳定性和故障源。

Conclusion: Hessian方法可显著改进复杂神经架构的故障诊断，有望提升软件调试实践。

Abstract: As attention-based deep learning models scale in size and complexity,
diagnosing their faults becomes increasingly challenging. In this work, we
conduct an empirical study to evaluate the potential of Hessian-based analysis
for diagnosing faults in attention-based models. Specifically, we use
Hessian-derived insights to identify fragile regions (via curvature analysis)
and parameter interdependencies (via parameter interaction analysis) within
attention mechanisms. Through experiments on three diverse models (HAN, 3D-CNN,
DistilBERT), we show that Hessian-based metrics can localize instability and
pinpoint fault sources more effectively than gradients alone. Our empirical
findings suggest that these metrics could significantly improve fault diagnosis
in complex neural architectures, potentially improving software debugging
practices.

</details>


### [599] [Diffusion Counterfactual Generation with Semantic Abduction](https://arxiv.org/abs/2506.07883)
*Rajat Rasal,Avinash Kori,Fabio De Sousa Ribeiro,Tian Xia,Ben Glocker*

Main category: cs.LG

TL;DR: 该论文提出了一种基于扩散模型的反事实图像生成框架，通过引入空间、语义和动态反演机制，结合Pearl因果理论，实现了高保真度的图像编辑。


<details>
  <summary>Details</summary>
Motivation: 现有自编码框架在反事实图像生成中存在可扩展性和保真度问题，而扩散模型在视觉质量和语义表示方面表现出色，因此探索如何利用扩散模型改进反事实图像编辑。

Method: 提出了一套基于扩散模型的因果机制，包括空间、语义和动态反演，并将语义表示通过Pearl因果理论整合到扩散模型中，实现反事实推理的图像编辑。

Result: 该框架首次在扩散模型中实现了高级语义身份保留，展示了如何在忠实因果控制和身份保留之间进行权衡。

Conclusion: 扩散模型结合因果机制为反事实图像生成提供了新的解决方案，实现了高保真度和语义控制的平衡。

Abstract: Counterfactual image generation presents significant challenges, including
preserving identity, maintaining perceptual quality, and ensuring faithfulness
to an underlying causal model. While existing auto-encoding frameworks admit
semantic latent spaces which can be manipulated for causal control, they
struggle with scalability and fidelity. Advancements in diffusion models
present opportunities for improving counterfactual image editing, having
demonstrated state-of-the-art visual quality, human-aligned perception and
representation learning capabilities. Here, we present a suite of
diffusion-based causal mechanisms, introducing the notions of spatial, semantic
and dynamic abduction. We propose a general framework that integrates semantic
representations into diffusion models through the lens of Pearlian causality to
edit images via a counterfactual reasoning process. To our knowledge, this is
the first work to consider high-level semantic identity preservation for
diffusion counterfactuals and to demonstrate how semantic control enables
principled trade-offs between faithful causal control and identity
preservation.

</details>


### [600] [Schauder Bases for $C[0, 1]$ Using ReLU, Softplus and Two Sigmoidal Functions](https://arxiv.org/abs/2506.07884)
*Anand Ganesh,Babhrubahan Bose,Anand Rajagopalan*

Main category: cs.LG

TL;DR: 本文构建了四种Schauder基用于空间$C[0,1]$，分别基于ReLU、Softplus及其sigmoidal变体，首次证明了这些函数基的存在性，并改进了其通用逼近性质。


<details>
  <summary>Details</summary>
Motivation: 探索ReLU、Softplus及其sigmoidal变体在函数空间$C[0,1]$中作为Schauder基的可行性，填补相关理论空白。

Method: 通过构造四种不同的Schauder基，分别基于ReLU、Softplus及其sigmoidal版本，验证其在$C[0,1]$空间中的基性质。

Result: 首次证明了ReLU、Softplus及其sigmoidal变体可以作为Schauder基，并提升了其通用逼近能力。

Conclusion: 研究为函数逼近理论提供了新的工具，扩展了ReLU和Softplus函数的应用范围。

Abstract: We construct four Schauder bases for the space $C[0,1]$, one using ReLU
functions, another using Softplus functions, and two more using sigmoidal
versions of the ReLU and Softplus functions. This establishes the existence of
a basis using these functions for the first time, and improves on the universal
approximation property associated with them.

</details>


### [601] [FunDiff: Diffusion Models over Function Spaces for Physics-Informed Generative Modeling](https://arxiv.org/abs/2506.07902)
*Sifan Wang,Zehao Dou,Tong-Rui Liu,Lu Lu*

Main category: cs.LG

TL;DR: FunDiff是一个用于函数空间的生成建模框架，结合潜在扩散过程和函数自动编码器，支持不同离散化输入并生成连续函数，同时融入物理先验。


<details>
  <summary>Details</summary>
Motivation: 适应生成模型（如扩散模型和流匹配）到物理应用中，处理连续函数并满足复杂物理规律。

Method: FunDiff结合潜在扩散过程和函数自动编码器，通过架构约束或物理损失函数融入物理先验。

Result: 理论证明扩散估计器在函数空间中达到最优收敛率，实验表明FunDiff在流体和固体力学中生成高保真且物理一致的样本。

Conclusion: FunDiff在函数空间中实现了高效生成建模，适用于物理应用，并具备对噪声和低分辨率数据的鲁棒性。

Abstract: Recent advances in generative modeling -- particularly diffusion models and
flow matching -- have achieved remarkable success in synthesizing discrete data
such as images and videos. However, adapting these models to physical
applications remains challenging, as the quantities of interest are continuous
functions governed by complex physical laws. Here, we introduce
$\textbf{FunDiff}$, a novel framework for generative modeling in function
spaces. FunDiff combines a latent diffusion process with a function autoencoder
architecture to handle input functions with varying discretizations, generate
continuous functions evaluable at arbitrary locations, and seamlessly
incorporate physical priors. These priors are enforced through architectural
constraints or physics-informed loss functions, ensuring that generated samples
satisfy fundamental physical laws. We theoretically establish minimax
optimality guarantees for density estimation in function spaces, showing that
diffusion-based estimators achieve optimal convergence rates under suitable
regularity conditions. We demonstrate the practical effectiveness of FunDiff
across diverse applications in fluid dynamics and solid mechanics. Empirical
results show that our method generates physically consistent samples with high
fidelity to the target distribution and exhibits robustness to noisy and
low-resolution data. Code and datasets are publicly available at
https://github.com/sifanexisted/fundiff.

</details>


### [602] [Diffuse Everything: Multimodal Diffusion Models on Arbitrary State Spaces](https://arxiv.org/abs/2506.07903)
*Kevin Rojas,Yuchen Zhu,Sichen Zhu,Felix X. -F. Ye,Molei Tao*

Main category: cs.LG

TL;DR: 提出了一种新的多模态扩散模型框架，支持跨模态数据的原生生成，无需依赖外部预处理协议。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖外部预处理协议（如分词器和变分自编码器）统一数据表示，限制了在数据有限场景下的应用。

Method: 引入解耦的噪声调度策略，支持无条件生成和模态条件生成。

Result: 在文本-图像生成和混合类型表格数据合成任务中表现优异。

Conclusion: 该框架为多模态扩散模型提供了一种灵活且高效的解决方案。

Abstract: Diffusion models have demonstrated remarkable performance in generating
unimodal data across various tasks, including image, video, and text
generation. On the contrary, the joint generation of multimodal data through
diffusion models is still in the early stages of exploration. Existing
approaches heavily rely on external preprocessing protocols, such as tokenizers
and variational autoencoders, to harmonize varied data representations into a
unified, unimodal format. This process heavily demands the high accuracy of
encoders and decoders, which can be problematic for applications with limited
data. To lift this restriction, we propose a novel framework for building
multimodal diffusion models on arbitrary state spaces, enabling native
generation of coupled data across different modalities. By introducing an
innovative decoupled noise schedule for each modality, we enable both
unconditional and modality-conditioned generation within a single model
simultaneously. We empirically validate our approach for text-image generation
and mixed-type tabular data synthesis, demonstrating that it achieves
competitive performance.

</details>


### [603] [Uncovering the Functional Roles of Nonlinearity in Memory](https://arxiv.org/abs/2506.07919)
*Manuel Brenner,Georgia Koppe*

Main category: cs.LG

TL;DR: 研究表明，在序列建模任务中，最小非线性通常足够且最优，简化模型并提升鲁棒性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 探讨非线性在循环网络中的功能角色，明确其计算必要性和机制。

Method: 使用几乎线性循环神经网络（AL-RNNs）作为建模工具和记忆机制探针。

Result: 发现最小非线性在多种任务中表现最佳，模型更简单、鲁棒且可解释。

Conclusion: 为选择性引入非线性提供理论框架，连接动力学系统理论与循环网络的记忆和计算需求。

Abstract: Memory and long-range temporal processing are core requirements for sequence
modeling tasks across natural language processing, time-series forecasting,
speech recognition, and control. While nonlinear recurrence has long been
viewed as essential for enabling such mechanisms, recent work suggests that
linear dynamics may often suffice. In this study, we go beyond performance
comparisons to systematically dissect the functional role of nonlinearity in
recurrent networks--identifying both when it is computationally necessary, and
what mechanisms it enables. We use Almost Linear Recurrent Neural Networks
(AL-RNNs), which allow fine-grained control over nonlinearity, as both a
flexible modeling tool and a probe into the internal mechanisms of memory.
Across a range of classic sequence modeling tasks and a real-world stimulus
selection task, we find that minimal nonlinearity is not only sufficient but
often optimal, yielding models that are simpler, more robust, and more
interpretable than their fully nonlinear or linear counterparts. Our results
provide a principled framework for selectively introducing nonlinearity,
bridging dynamical systems theory with the functional demands of long-range
memory and structured computation in recurrent neural networks, with
implications for both artificial and biological neural systems.

</details>


### [604] [CausalPFN: Amortized Causal Effect Estimation via In-Context Learning](https://arxiv.org/abs/2506.07918)
*Vahid Balazadeh,Hamidreza Kamkari,Valentin Thomas,Benson Li,Junwei Ma,Jesse C. Cresswell,Rahul G. Krishnan*

Main category: cs.LG

TL;DR: CausalPFN是一个基于Transformer的模型，通过大规模模拟数据训练，无需额外调整即可直接推断因果效应，性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有因果效应估计方法需要大量手动选择和领域知识，CausalPFN旨在自动化这一过程。

Method: 结合贝叶斯因果推断和先验拟合网络（PFNs）的思想，通过模拟数据训练Transformer模型。

Result: 在IHDP、Lalonde和ACIC等基准测试中表现优异，并在实际政策制定任务中具有竞争力。

Conclusion: CausalPFN提供了一种无需调整的自动化因果推断解决方案，支持可靠决策。

Abstract: Causal effect estimation from observational data is fundamental across
various applications. However, selecting an appropriate estimator from dozens
of specialized methods demands substantial manual effort and domain expertise.
We present CausalPFN, a single transformer that amortizes this workflow:
trained once on a large library of simulated data-generating processes that
satisfy ignorability, it infers causal effects for new observational datasets
out-of-the-box. CausalPFN combines ideas from Bayesian causal inference with
the large-scale training protocol of prior-fitted networks (PFNs), learning to
map raw observations directly to causal effects without any task-specific
adjustment. Our approach achieves superior average performance on heterogeneous
and average treatment effect estimation benchmarks (IHDP, Lalonde, ACIC).
Moreover, it shows competitive performance for real-world policy making on
uplift modeling tasks. CausalPFN provides calibrated uncertainty estimates to
support reliable decision-making based on Bayesian principles. This
ready-to-use model does not require any further training or tuning and takes a
step toward automated causal inference (https://github.com/vdblm/CausalPFN).

</details>


### [605] [W4S4: WaLRUS Meets S4 for Long-Range Sequence Modeling](https://arxiv.org/abs/2506.07920)
*Hossein Babaei,Mel White,Richard G. Baraniuk*

Main category: cs.LG

TL;DR: 论文提出了一种新的状态空间模型W4S4，基于冗余小波框架构建，具有稳定对角化和高效计算特性，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有状态空间模型（SSMs）的性能高度依赖于状态矩阵的选择和初始化，需要改进以更好地处理长程依赖关系。

Method: 基于SaFARi框架和WaLRUS SSMs，构建了W4S4模型，利用冗余小波框架实现稳定对角化和快速核计算。

Result: 实验表明，W4S4在延迟重构任务、分类基准和长序列建模中表现优于基于HiPPO的SSMs。

Conclusion: W4S4为下一代基于深度SSM的模型提供了可扩展且通用的基础。

Abstract: State Space Models (SSMs) have emerged as powerful components for sequence
modeling, enabling efficient handling of long-range dependencies via linear
recurrence and convolutional computation. However, their effectiveness depends
heavily on the choice and initialization of the state matrix. In this work, we
build on the SaFARi framework and existing WaLRUS SSMs to introduce a new
variant, W4S4 (WaLRUS for S4), a new class of SSMs constructed from redundant
wavelet frames. WaLRUS admits a stable diagonalization and supports fast kernel
computation without requiring low-rank approximations, making it both
theoretically grounded and computationally efficient. We show that WaLRUS
retains information over long horizons significantly better than HiPPO-based
SSMs, both in isolation and when integrated into deep architectures such as S4.
Our experiments demonstrate consistent improvements across delay reconstruction
tasks, classification benchmarks, and long-range sequence modeling, confirming
that high-quality, structured initialization enabled by wavelet-based state
dynamic offers substantial advantages over existing alternatives. WaLRUS
provides a scalable and versatile foundation for the next generation of deep
SSM-based models.

</details>


### [606] [A Generative Physics-Informed Reinforcement Learning-Based Approach for Construction of Representative Drive Cycle](https://arxiv.org/abs/2506.07929)
*Amirreza Yasami,Mohammadali Tofigh,Mahdi Shahbakhti,Charles Robert Koch*

Main category: cs.LG

TL;DR: PIESMC方法通过物理信息强化学习框架和蒙特卡洛采样，高效构建代表性驾驶循环，显著减少计算成本并提升准确性。


<details>
  <summary>Details</summary>
Motivation: 精确构建驾驶循环对车辆设计、燃油经济性分析和环境影响评估至关重要。

Method: 采用生成式物理信息强化学习框架（PIESMC），结合蒙特卡洛采样，捕捉瞬态动态、加减速、怠速和坡度变化。

Result: 实验表明，PIESMC在关键运动学和能量指标上表现优异，比MTB和MCB方法分别减少57.3%和10.5%的误差，且速度快一个数量级。

Conclusion: PIESMC能高效且准确地构建驾驶循环，适用于实际应用。

Abstract: Accurate driving cycle construction is crucial for vehicle design, fuel
economy analysis, and environmental impact assessments. A generative
Physics-Informed Expected SARSA-Monte Carlo (PIESMC) approach that constructs
representative driving cycles by capturing transient dynamics, acceleration,
deceleration, idling, and road grade transitions while ensuring model fidelity
is introduced. Leveraging a physics-informed reinforcement learning framework
with Monte Carlo sampling, PIESMC delivers efficient cycle construction with
reduced computational cost. Experimental evaluations on two real-world datasets
demonstrate that PIESMC replicates key kinematic and energy metrics, achieving
up to a 57.3% reduction in cumulative kinematic fragment errors compared to the
Micro-trip-based (MTB) method and a 10.5% reduction relative to the
Markov-chain-based (MCB) method. Moreover, it is nearly an order of magnitude
faster than conventional techniques. Analyses of vehicle-specific power
distributions and wavelet-transformed frequency content further confirm its
ability to reproduce experimental central tendencies and variability.

</details>


### [607] [Ensemble-Based Survival Models with the Self-Attended Beran Estimator Predictions](https://arxiv.org/abs/2506.07933)
*Lev V. Utkin,Semen P. Khomets,Vlada A. Efremenko,Andrei V. Konstantinov,Natalya M. Verbova*

Main category: cs.LG

TL;DR: 提出SurvBESA模型，结合Beran估计器和自注意力机制，提升生存分析的预测稳定性。


<details>
  <summary>Details</summary>
Motivation: 传统集成模型（如随机生存森林和梯度提升）因bootstrap样本的变异性导致预测不稳定，需改进。

Method: 结合Beran估计器和自注意力机制，通过调整相邻生存函数的相似性平滑噪声。

Result: 数值实验表明SurvBESA优于现有最优模型。

Conclusion: SurvBESA通过自注意力机制有效提升预测稳定性，代码已公开。

Abstract: Survival analysis predicts the time until an event of interest, such as
failure or death, but faces challenges due to censored data, where some events
remain unobserved. Ensemble-based models, like random survival forests and
gradient boosting, are widely used but can produce unstable predictions due to
variations in bootstrap samples. To address this, we propose SurvBESA (Survival
Beran Estimators Self-Attended), a novel ensemble model that combines Beran
estimators with a self-attention mechanism. Unlike traditional methods,
SurvBESA applies self-attention to predicted survival functions, smoothing out
noise by adjusting each survival function based on its similarity to
neighboring survival functions. We also explore a special case using Huber's
contamination model to define attention weights, simplifying training to a
quadratic or linear optimization problem. Numerical experiments show that
SurvBESA outperforms state-of-the-art models. The implementation of SurvBESA is
publicly available.

</details>


### [608] [HeuriGym: An Agentic Benchmark for LLM-Crafted Heuristics in Combinatorial Optimization](https://arxiv.org/abs/2506.07972)
*Hongzheng Chen,Yingheng Wang,Yaohui Cai,Hins Hu,Jiajie Li,Shirley Huang,Chenhui Deng,Rongjian Liang,Shufeng Kong,Haoxing Ren,Samitha Samaranayake,Carla P. Gomes,Zhiru Zhang*

Main category: cs.LG

TL;DR: 论文提出了HeuriGym框架，用于评估LLM在组合优化问题中的启发式算法生成能力，并提出了QYI指标量化性能。


<details>
  <summary>Details</summary>
Motivation: 现有评估方法无法充分衡量LLM在推理和问题解决中的能力，需要更严谨的评估框架。

Method: 通过HeuriGym框架，LLM提出启发式算法，接收代码执行反馈并迭代优化，评估了9个先进模型。

Result: 顶级模型QYI得分仅0.6，远低于专家基准1，揭示了工具使用、规划和适应性推理的局限性。

Conclusion: HeuriGym为LLM在科学和工程领域的问题解决能力提供了更有效的评估基准。

Abstract: While Large Language Models (LLMs) have demonstrated significant advancements
in reasoning and agent-based problem-solving, current evaluation methodologies
fail to adequately assess their capabilities: existing benchmarks either rely
on closed-ended questions prone to saturation and memorization, or subjective
comparisons that lack consistency and rigor. In this work, we introduce
HeuriGym, an agentic framework designed for evaluating heuristic algorithms
generated by LLMs for combinatorial optimization problems, characterized by
clearly defined objectives and expansive solution spaces. HeuriGym empowers
LLMs to propose heuristics, receive evaluative feedback via code execution, and
iteratively refine their solutions. We evaluate nine state-of-the-art models on
nine problems across domains such as computer systems, logistics, and biology,
exposing persistent limitations in tool use, planning, and adaptive reasoning.
To quantify performance, we propose the Quality-Yield Index (QYI), a metric
that captures both solution pass rate and quality. Even top models like
GPT-o4-mini-high and Gemini-2.5-Pro attain QYI scores of only 0.6, well below
the expert baseline of 1. Our open-source benchmark aims to guide the
development of LLMs toward more effective and realistic problem-solving in
scientific and engineering domains.

</details>


### [609] [TokenBreak: Bypassing Text Classification Models Through Token Manipulation](https://arxiv.org/abs/2506.07948)
*Kasimir Schulz,Kenneth Yeung,Kieran Evans*

Main category: cs.LG

TL;DR: 论文提出了一种名为TokenBreak的新型攻击方法，能够绕过基于tokenization的保护模型，同时展示了防御策略。


<details>
  <summary>Details</summary>
Motivation: 现有的NLP保护模型通过tokenization防御威胁，但存在漏洞，TokenBreak利用这些漏洞绕过保护。

Method: 通过操纵输入文本，利用tokenization策略使保护模型分类错误，同时目标仍能理解文本。

Result: 攻击成功绕过保护模型，揭示了基于tokenization的模型的脆弱性。

Conclusion: 论文提出了防御策略，无需重新训练模型即可增强保护。

Abstract: Natural Language Processing (NLP) models are used for text-related tasks such
as classification and generation. To complete these tasks, input data is first
tokenized from human-readable text into a format the model can understand,
enabling it to make inferences and understand context. Text classification
models can be implemented to guard against threats such as prompt injection
attacks against Large Language Models (LLMs), toxic input and cybersecurity
risks such as spam emails. In this paper, we introduce TokenBreak: a novel
attack that can bypass these protection models by taking advantage of the
tokenization strategy they use. This attack technique manipulates input text in
such a way that certain models give an incorrect classification. Importantly,
the end target (LLM or email recipient) can still understand and respond to the
manipulated text and therefore be vulnerable to the very attack the protection
model was put in place to prevent. The tokenizer is tied to model architecture,
meaning it is possible to predict whether or not a model is vulnerable to
attack based on family. We also present a defensive strategy as an added layer
of protection that can be implemented without having to retrain the defensive
model.

</details>


### [610] [Cost-Optimal Active AI Model Evaluation](https://arxiv.org/abs/2506.07949)
*Anastasios N. Angelopoulos,Jacob Eisenstein,Jonathan Berant,Alekh Agarwal,Adam Fisch*

Main category: cs.LG

TL;DR: 论文提出了一种成本感知方法，用于在生成式AI系统的评估中平衡低成本但可能不准确的弱标注（如模型自动标注）与高成本但更准确的强标注（如人工标注），以在预算内最大化统计效率。


<details>
  <summary>Details</summary>
Motivation: 生成式AI系统的开发需要持续评估和数据标注，但资源成本高。快速迭代中常依赖低成本但可能有偏的合成标注数据。

Method: 基于主动和预测驱动的统计推断，提出成本最优策略，分配标注预算以最大化统计效率。

Result: 在合成和真实数据上验证，特别是在任务样本难度差异大时，能以更低预算达到相同估计精度。

Conclusion: 新策略在特定条件下显著优于传统方法，为资源受限的AI开发提供了高效解决方案。

Abstract: The development lifecycle of generative AI systems requires continual
evaluation, data acquisition, and annotation, which is costly in both resources
and time. In practice, rapid iteration often makes it necessary to rely on
synthetic annotation data because of the low cost, despite the potential for
substantial bias. In this paper, we develop novel, cost-aware methods for
actively balancing the use of a cheap, but often inaccurate, weak rater -- such
as a model-based autorater that is designed to automatically assess the quality
of generated content -- with a more expensive, but also more accurate, strong
rater alternative such as a human. More specifically, the goal of our approach
is to produce a low variance, unbiased estimate of the mean of the target
"strong" rating, subject to some total annotation budget. Building on recent
work in active and prediction-powered statistical inference, we derive a family
of cost-optimal policies for allocating a given annotation budget between weak
and strong raters so as to maximize statistical efficiency. Using synthetic and
real-world data, we empirically characterize the conditions under which these
policies yield improvements over prior methods. We find that, especially in
tasks where there is high variability in the difficulty of examples, our
policies can achieve the same estimation precision at a far lower total
annotation budget than standard evaluation methods.

</details>


### [611] [Reparameterized LLM Training via Orthogonal Equivalence Transformation](https://arxiv.org/abs/2506.08001)
*Zeju Qiu,Simon Buchholz,Tim Z. Xiao,Maximilian Dax,Bernhard Schölkopf,Weiyang Liu*

Main category: cs.LG

TL;DR: POET是一种新颖的重新参数化训练算法，通过正交等价变换优化神经元，提高大语言模型训练的稳定性和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 大语言模型（LLMs）训练中存在显著挑战，需要更有效和稳定的方法。

Method: POET通过两个可学习的正交矩阵和一个固定随机权重矩阵重新参数化神经元，并开发高效近似方法以适应大规模网络训练。

Result: 实验验证了POET在训练LLMs中的有效性和可扩展性。

Conclusion: POET为解决大语言模型训练挑战提供了稳定且高效的解决方案。

Abstract: While large language models (LLMs) are driving the rapid advancement of
artificial intelligence, effectively and reliably training these large models
remains one of the field's most significant challenges. To address this
challenge, we propose POET, a novel reParameterized training algorithm that
uses Orthogonal Equivalence Transformation to optimize neurons. Specifically,
POET reparameterizes each neuron with two learnable orthogonal matrices and a
fixed random weight matrix. Because of its provable preservation of spectral
properties of weight matrices, POET can stably optimize the objective function
with improved generalization. We further develop efficient approximations that
make POET flexible and scalable for training large-scale neural networks.
Extensive experiments validate the effectiveness and scalability of POET in
training LLMs.

</details>


### [612] [Neural Tangent Kernel Analysis to Probe Convergence in Physics-informed Neural Solvers: PIKANs vs. PINNs](https://arxiv.org/abs/2506.07958)
*Salah A. Faroughi,Farinaz Mostajeran*

Main category: cs.LG

TL;DR: 本文通过神经切线核（NTK）理论分析了基于Chebyshev的物理信息Kolmogorov-Arnold网络（cPIKANs），研究了其训练动态和收敛行为，揭示了其学习效率与核结构演化的关系。


<details>
  <summary>Details</summary>
Motivation: cPIKANs在求解偏微分方程（PDEs）中表现出潜力，但其训练动态和收敛行为缺乏理论和数值研究。本文旨在通过NTK理论填补这一空白。

Method: 推导了标准cKANs在监督学习中的NTK，并扩展到物理信息场景，分析了四种典型PDEs的NTK矩阵谱性质及优化策略的影响。

Result: 研究发现cPIKANs的NTK行为可预测，揭示了标准PINNs无法捕捉的学习动态，并明确了域分解对训练的影响。

Conclusion: 这是首次对cPIKANs的系统性NTK研究，为理解其性能提供了理论依据。

Abstract: Physics-informed Kolmogorov-Arnold Networks (PIKANs), and in particular their
Chebyshev-based variants (cPIKANs), have recently emerged as promising models
for solving partial differential equations (PDEs). However, their training
dynamics and convergence behavior remain largely unexplored both theoretically
and numerically. In this work, we aim to advance the theoretical understanding
of cPIKANs by analyzing them using Neural Tangent Kernel (NTK) theory. Our
objective is to discern the evolution of kernel structure throughout
gradient-based training and its subsequent impact on learning efficiency. We
first derive the NTK of standard cKANs in a supervised setting, and then extend
the analysis to the physics-informed context. We analyze the spectral
properties of NTK matrices, specifically their eigenvalue distributions and
spectral bias, for four representative PDEs: the steady-state Helmholtz
equation, transient diffusion and Allen-Cahn equations, and forced vibrations
governed by the Euler-Bernoulli beam equation. We also conduct an investigation
into the impact of various optimization strategies, e.g., first-order,
second-order, and hybrid approaches, on the evolution of the NTK and the
resulting learning dynamics. Results indicate a tractable behavior for NTK in
the context of cPIKANs, which exposes learning dynamics that standard
physics-informed neural networks (PINNs) cannot capture. Spectral trends also
reveal when domain decomposition improves training, directly linking kernel
behavior to convergence rates under different setups. To the best of our
knowledge, this is the first systematic NTK study of cPIKANs, providing
theoretical insight that clarifies and predicts their empirical performance.

</details>


### [613] [A Two-Phase Deep Learning Framework for Adaptive Time-Stepping in High-Speed Flow Modeling](https://arxiv.org/abs/2506.07969)
*Jacob Helwig,Sai Sreeharsha Adavi,Xuan Zhang,Yuchao Lin,Felix S. Chim,Luke Takeshi Vizzini,Haiyang Yu,Muhammad Hasnain,Saykat Kumar Biswas,John J. Holloway,Narendra Singh,N. K. Anand,Swagnik Guhathakurta,Shuiwang Ji*

Main category: cs.LG

TL;DR: 论文提出了一种名为ShockCast的两阶段机器学习方法，用于建模高速流体流动，并采用自适应时间步长技术。


<details>
  <summary>Details</summary>
Motivation: 研究高速流体流动（如超音速流）时，传统均匀时间步长方法难以捕捉激波等突变现象，因此需要自适应时间步长方法。

Method: ShockCast分为两阶段：第一阶段用机器学习模型预测时间步长，第二阶段将预测的时间步长与当前流体场结合以推进系统状态。

Result: 论文生成了两个超音速流动数据集，并公开了代码。

Conclusion: ShockCast是首个用于学习高速流体流动的框架，通过自适应时间步长有效解决了高速流动建模问题。

Abstract: We consider the problem of modeling high-speed flows using machine learning
methods. While most prior studies focus on low-speed fluid flows in which
uniform time-stepping is practical, flows approaching and exceeding the speed
of sound exhibit sudden changes such as shock waves. In such cases, it is
essential to use adaptive time-stepping methods to allow a temporal resolution
sufficient to resolve these phenomena while simultaneously balancing
computational costs. Here, we propose a two-phase machine learning method,
known as ShockCast, to model high-speed flows with adaptive time-stepping. In
the first phase, we propose to employ a machine learning model to predict the
timestep size. In the second phase, the predicted timestep is used as an input
along with the current fluid fields to advance the system state by the
predicted timestep. We explore several physically-motivated components for
timestep prediction and introduce timestep conditioning strategies inspired by
neural ODE and Mixture of Experts. As ShockCast is the first framework for
learning high-speed flows, we evaluate our methods by generating two supersonic
flow datasets, available at https://huggingface.co/datasets/divelab. Our code
is publicly available as part of the AIRS library
(https://github.com/divelab/AIRS).

</details>


### [614] [Hyperpruning: Efficient Search through Pruned Variants of Recurrent Neural Networks Leveraging Lyapunov Spectrum](https://arxiv.org/abs/2506.07975)
*Caleb Zheng,Eli Shlizerman*

Main category: cs.LG

TL;DR: 论文提出了一种基于Lyapunov谱（LS）的距离度量方法，用于高效比较剪枝网络与密集网络的性能，并开发了LSH框架，显著减少了剪枝策略的搜索时间。


<details>
  <summary>Details</summary>
Motivation: 现有剪枝方法缺乏早期性能保证，且计算成本高，因此需要一种能快速评估剪枝网络性能的方法。

Method: 提出LS距离度量，结合超参数优化算法，开发LSH框架，用于高效剪枝策略搜索。

Result: 在多个网络架构和数据集上，LSH框架显著减少了搜索时间，并找到了性能优于密集网络的剪枝模型。

Conclusion: LSH框架为剪枝策略选择提供了高效且性能优越的解决方案。

Abstract: A variety of pruning methods have been introduced for over-parameterized
Recurrent Neural Networks to improve efficiency in terms of power consumption
and storage utilization. These advances motivate a new paradigm, termed
`hyperpruning', which seeks to identify the most suitable pruning strategy for
a given network architecture and application. Unlike conventional
hyperparameter search, where the optimal configuration's accuracy remains
uncertain, in the context of network pruning, the accuracy of the dense model
sets the target for the accuracy of the pruned one. The goal, therefore, is to
discover pruned variants that match or even surpass this established accuracy.
However, exhaustive search over pruning configurations is computationally
expensive and lacks early performance guarantees. To address this challenge, we
propose a novel Lyapunov Spectrum (LS)-based distance metric that enables early
comparison between pruned and dense networks, allowing accurate prediction of
post-training performance. By integrating this LS-based distance with standard
hyperparameter optimization algorithms, we introduce an efficient hyperpruning
framework, termed LS-based Hyperpruning (LSH). LSH reduces search time by an
order of magnitude compared to conventional approaches relying on full
training. Experiments on stacked LSTM and RHN architectures using the Penn
Treebank dataset, and on AWD-LSTM-MoS using WikiText-2, demonstrate that under
fixed training budgets and target pruning ratios, LSH consistently identifies
superior pruned models. Remarkably, these pruned variants not only outperform
those selected by loss-based baseline but also exceed the performance of their
dense counterpart.

</details>


### [615] [Thinking vs. Doing: Agents that Reason by Scaling Test-Time Interaction](https://arxiv.org/abs/2506.07976)
*Junhong Shen,Hao Bai,Lunjun Zhang,Yifei Zhou,Amrith Setlur,Shengbang Tong,Diego Caples,Nan Jiang,Tong Zhang,Ameet Talwalkar,Aviral Kumar*

Main category: cs.LG

TL;DR: 论文提出了一种新的测试时间交互扩展方法（TTI），通过增加代理的交互范围，使其能够动态调整行为，从而在Web代理任务中取得显著效果。


<details>
  <summary>Details</summary>
Motivation: 当前测试时间扩展方法仅依赖长推理轨迹，无法让代理从环境中获取新信息或动态调整行为，因此需要探索交互扩展的潜力。

Method: 提出TTI方法，基于课程学习的在线强化学习（RL），动态调整代理的交互范围，支持探索、回溯和动态重规划。

Result: TTI在WebVoyager和WebArena基准测试中实现了最先进的性能，并能自适应平衡探索与利用。

Conclusion: 交互扩展是计算扩展的有力补充，为训练自适应代理提供了新方向。

Abstract: The current paradigm of test-time scaling relies on generating long reasoning
traces ("thinking" more) before producing a response. In agent problems that
require interaction, this can be done by generating thinking traces before
acting in the world. However, this process does not allow agents to acquire new
information from the environment or adapt their behavior over time. In this
work, we propose to scale test-time interaction, an untapped dimension of
test-time scaling that increases the agent's interaction horizon to enable
running rich behaviors such as exploration, backtracking, and dynamic
re-planning within a single rollout. To demonstrate the promise of this scaling
dimension, we study the domain of web agents. We first show that even
prompting-based interaction scaling without any training can improve task
success on web benchmarks non-trivially. Building on this, we introduce TTI
(Test-Time Interaction), a curriculum-based online reinforcement learning (RL)
approach that trains agents by adaptively adjusting their rollout lengths.
Using a Gemma 3 12B model, TTI produces state-of-the-art open-source, open-data
web agents on WebVoyager and WebArena benchmarks. We further show that TTI
enables agents to balance exploration and exploitation adaptively. Our results
establish interaction scaling as a powerful, complementary axis to scaling
per-step compute, offering new avenues for training adaptive agents.

</details>


### [616] [Realistic Urban Traffic Generator using Decentralized Federated Learning for the SUMO simulator](https://arxiv.org/abs/2506.07980)
*Alberto Bazán-Guillén,Carlos Beis-Penedo,Diego Cajaraville-Aboy,Pablo Barbecho-Bautista,Rebeca P. Díaz-Redondo,Luis J. de la Cruz Llopis,Ana Fernández-Vilas,Mónica Aguilar Igartua,Manuel Fernández-Veiga*

Main category: cs.LG

TL;DR: DesRUTGe是一个基于去中心化联邦学习和深度强化学习的框架，用于生成高保真、时间变化的城市交通模式，解决了现有方法在准确性、扩展性和隐私方面的限制。


<details>
  <summary>Details</summary>
Motivation: 现实城市交通模拟对可持续城市规划和智能交通系统发展至关重要，但现有方法在准确性、扩展性或隐私保护方面存在不足。

Method: 结合深度强化学习与SUMO模拟器，采用去中心化联邦学习，每个交通检测器作为独立节点，通过局部训练和参数交换优化模型。

Result: 在巴塞罗那真实数据上测试，DesRUTGe比RouteSampler和其他集中式学习方法更准确且隐私保护更好。

Conclusion: DesRUTGe为大规模城市交通模拟提供了一种高效、隐私保护的解决方案。

Abstract: Realistic urban traffic simulation is essential for sustainable urban
planning and the development of intelligent transportation systems. However,
generating high-fidelity, time-varying traffic profiles that accurately reflect
real-world conditions, especially in large-scale scenarios, remains a major
challenge. Existing methods often suffer from limitations in accuracy,
scalability, or raise privacy concerns due to centralized data processing. This
work introduces DesRUTGe (Decentralized Realistic Urban Traffic Generator), a
novel framework that integrates Deep Reinforcement Learning (DRL) agents with
the SUMO simulator to generate realistic 24-hour traffic patterns. A key
innovation of DesRUTGe is its use of Decentralized Federated Learning (DFL),
wherein each traffic detector and its corresponding urban zone function as an
independent learning node. These nodes train local DRL models using minimal
historical data and collaboratively refine their performance by exchanging
model parameters with selected peers (e.g., geographically adjacent zones),
without requiring a central coordinator. Evaluated using real-world data from
the city of Barcelona, DesRUTGe outperforms standard SUMO-based tools such as
RouteSampler, as well as other centralized learning approaches, by delivering
more accurate and privacy-preserving traffic pattern generation.

</details>


### [617] [Generative Modeling of Weights: Generalization or Memorization?](https://arxiv.org/abs/2506.07998)
*Boya Zeng,Yida Yin,Zhiqiu Xu,Zhuang Liu*

Main category: cs.LG

TL;DR: 生成模型在图像和视频生成中表现出色，但用于生成神经网络权重时，现有方法主要通过记忆训练检查点，而非真正生成新权重。


<details>
  <summary>Details</summary>
Motivation: 探索生成模型在合成高性能神经网络权重方面的能力，并评估其是否能生成不同于训练数据的新权重。

Method: 研究了四种代表性方法，分析其生成权重的能力，并与简单基线方法（如添加噪声或权重集成）进行比较。

Result: 现有方法主要通过记忆训练检查点生成权重，无法超越简单基线方法，且无法通过调整模型因素或数据增强缓解记忆问题。

Conclusion: 研究揭示了当前生成模型在新领域中的局限性，强调了更谨慎评估的必要性。

Abstract: Generative models, with their success in image and video generation, have
recently been explored for synthesizing effective neural network weights. These
approaches take trained neural network checkpoints as training data, and aim to
generate high-performing neural network weights during inference. In this work,
we examine four representative methods on their ability to generate novel model
weights, i.e., weights that are different from the checkpoints seen during
training. Surprisingly, we find that these methods synthesize weights largely
by memorization: they produce either replicas, or at best simple
interpolations, of the training checkpoints. Current methods fail to outperform
simple baselines, such as adding noise to the weights or taking a simple weight
ensemble, in obtaining different and simultaneously high-performing models. We
further show that this memorization cannot be effectively mitigated by
modifying modeling factors commonly associated with memorization in image
diffusion models, or applying data augmentations. Our findings provide a
realistic assessment of what types of data current generative models can model,
and highlight the need for more careful evaluation of generative models in new
domains. Our code is available at
https://github.com/boyazeng/weight_memorization.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [618] [AI-Generated Compromises for Coalition Formation](https://arxiv.org/abs/2506.06837)
*Eyal Briman,Ehud Shapiro,Nimrod Talmon*

Main category: cs.MA

TL;DR: 该论文提出了一种结合AI技术的方法，用于在协作文档写作（如社区宪法的民主起草）中寻找妥协提案，以解决传统工具的局限性。


<details>
  <summary>Details</summary>
Motivation: 在AI领域（如论证、调解和谈判）中，如何找到多数支持的妥协提案是一个核心问题。Elkind等人的研究提出了基于理想点的联盟形成过程，但如何有效找到妥协提案仍待解决。

Method: 通过形式化一个包含有限理性和不确定性的模型，并利用自然语言处理技术和大型语言模型构建语义度量空间，设计算法生成妥协提案。

Result: 模拟实验表明，AI方法能够有效支持大规模民主文本编辑，优于传统工具。

Conclusion: AI技术可以成功应用于协作文档写作，为解决复杂的社会协调问题提供了新途径。

Abstract: The challenge of finding compromises between agent proposals is fundamental
to AI subfields such as argumentation, mediation, and negotiation. Building on
this tradition, Elkind et al. (2021) introduced a process for coalition
formation that seeks majority-supported proposals preferable to the status quo,
using a metric space where each agent has an ideal point. A crucial step in
this process involves identifying compromise proposals around which agent
coalitions can unite. How to effectively find such compromise proposals remains
an open question. We address this gap by formalizing a model that incorporates
agent bounded rationality and uncertainty, and by developing AI methods to
generate compromise proposals. We focus on the domain of collaborative document
writing, such as the democratic drafting of a community constitution. Our
approach uses natural language processing techniques and large language models
to induce a semantic metric space over text. Based on this space, we design
algorithms to suggest compromise points likely to receive broad support. To
evaluate our methods, we simulate coalition formation processes and show that
AI can facilitate large-scale democratic text editing, a domain where
traditional tools are limited.

</details>


### [619] [Learn as Individuals, Evolve as a Team: Multi-agent LLMs Adaptation in Embodied Environments](https://arxiv.org/abs/2506.07232)
*Xinran Li,Chenjia Bai,Zijian Li,Jiakun Zheng,Ting Xiao,Jun Zhang*

Main category: cs.MA

TL;DR: 论文提出了一种名为LIET的框架，通过个体学习和团队进化提升多智能体LLM在具身环境中的适应能力。


<details>
  <summary>Details</summary>
Motivation: 现有基于LLM的多智能体规划算法在具身环境中适应能力不足，需要改进以提升规划和协作能力。

Method: 采用LIET范式，结合个体学习（局部效用函数）和团队进化（共享合作知识列表），实现智能体的动态适应。

Result: 在Communicative Watch-And-Help和ThreeD-World Multi-Agent Transport基准测试中，LIET表现优于现有基线，展示了强大的协作规划能力。

Conclusion: LIET框架通过个体与团队的协同学习，显著提升了LLM智能体在复杂多智能体具身环境中的适应性和协作效率。

Abstract: Large language models (LLMs) possess extensive knowledge bases and strong
reasoning capabilities, making them promising tools for complex, multi-agent
planning in embodied environments. However, despite LLMs' advanced abilities
and the sophisticated modular design of agentic methods, existing LLM-based
planning algorithms remain limited by weak adaptation capabilities to
multi-agent embodied scenarios. We address this limitation by introducing a
framework that enables LLM agents to learn and evolve both before and during
test time, equipping them with environment-relevant knowledge for better
planning and enhanced communication for improved cooperation. Inspired by
centralized training with decentralized execution in multi-agent reinforcement
learning, we propose a \textit{Learn as Individuals, Evolve as a Team (LIET)}
paradigm for multi-agent LLMs adaptation. At the individual level, LLM agents
learn a local utility function from exploratory datasets to better comprehend
the embodied environment, which is then queried during test time to support
informed decision-making. At the team level, LLM agents collaboratively and
iteratively maintain and update a shared cooperation knowledge list based on
new experiences, using it to guide more effective communication. By combining
individual learning with team evolution, LIET enables comprehensive and
flexible adaptation for LLM agents. Our experiments on Communicative
Watch-And-Help and ThreeD-World Multi-Agent Transport benchmarks demonstrate
that LIET, instantiated with both LLaMA and GPT-4o, outperforms existing
baselines and exhibits strong cooperative planning abilities.

</details>


### [620] [Digital Twin-based Smart Manufacturing: Dynamic Line Reconfiguration for Disturbance Handling](https://arxiv.org/abs/2506.07332)
*Bo Fu,Mingjie Bi,Shota Umeda,Takahiro Nakano,Youichi Nonaka,Quan Zhou,Takaharu Matsui,Dawn M. Tilbury,Kira Barton*

Main category: cs.MA

TL;DR: 提出了一种动态制造线重构框架，通过数字孪生、能力本体模型、配置优化器和快速仿真，有效应对制造系统中的扰动，提升系统吞吐量。


<details>
  <summary>Details</summary>
Motivation: 现代制造系统面临复杂性增加、需求波动和供应链不确定性等问题，需要灵活应对扰动的重构框架，但现有研究缺乏整体解决方案。

Method: 框架包含数字孪生监测扰动、能力本体模型捕捉资源选项、配置优化器生成最优配置，以及快速仿真评估（400倍实时速度）。

Result: 在电池生产线案例中，框架成功避免了26%和63%的吞吐量下降，优化器平均0.03秒即可找到51个操作、40个代理的最优配置。

Conclusion: 该框架显著提升了制造系统对扰动的适应能力，为动态重构提供了高效解决方案。

Abstract: The increasing complexity of modern manufacturing, coupled with demand
fluctuation, supply chain uncertainties, and product customization, underscores
the need for manufacturing systems that can flexibly update their
configurations and swiftly adapt to disturbances. However, current research
falls short in providing a holistic reconfigurable manufacturing framework that
seamlessly monitors system disturbances, optimizes alternative line
configurations based on machine capabilities, and automates simulation
evaluation for swift adaptations. This paper presents a dynamic manufacturing
line reconfiguration framework to handle disturbances that result in operation
time changes. The framework incorporates a system process digital twin for
monitoring disturbances and triggering reconfigurations, a capability-based
ontology model capturing available agent and resource options, a configuration
optimizer generating optimal line configurations, and a simulation generation
program initializing simulation setups and evaluating line configurations at
approximately 400x real-time speed. A case study of a battery production line
has been conducted to evaluate the proposed framework. In two implemented
disturbance scenarios, the framework successfully recovers system throughput
with limited resources, preventing the 26% and 63% throughput drops that would
have occurred without a reconfiguration plan. The reconfiguration optimizer
efficiently finds optimal solutions, taking an average of 0.03 seconds to find
a reconfiguration plan for a manufacturing line with 51 operations and 40
available agents across 8 agent types.

</details>


### [621] [Shapley-Coop: Credit Assignment for Emergent Cooperation in Self-Interested LLM Agents](https://arxiv.org/abs/2506.07388)
*Yun Hua,Haosheng Chen,Shiqin Wang,Wenhao Li,Xiangfeng Wang,Jun Luo*

Main category: cs.MA

TL;DR: Shapley-Coop是一种协作工作流，通过Shapley Chain-of-Thought和结构化谈判协议，解决LLM在开放环境中的协作问题，实现公平信用分配和激励对齐。


<details>
  <summary>Details</summary>
Motivation: 在缺乏协调规则的开放环境中，LLM代理倾向于自私行为，公平信用分配和定价机制成为关键挑战。

Method: 提出Shapley-Coop工作流，结合Shapley Chain-of-Thought（基于边际贡献定价）和结构化谈判协议，实现任务时间定价和奖励再分配。

Result: 在多代理游戏和软件工程模拟中，Shapley-Coop显著提升协作效果，实现公平信用分配。

Conclusion: Shapley-Coop的定价机制能准确反映个体贡献，促进LLM代理的协作和激励对齐。

Abstract: Large Language Models (LLMs) show strong collaborative performance in
multi-agent systems with predefined roles and workflows. However, in open-ended
environments lacking coordination rules, agents tend to act in self-interested
ways. The central challenge in achieving coordination lies in credit assignment
-- fairly evaluating each agent's contribution and designing pricing mechanisms
that align their heterogeneous goals. This problem is critical as LLMs
increasingly participate in complex human-AI collaborations, where fair
compensation and accountability rely on effective pricing mechanisms. Inspired
by how human societies address similar coordination challenges (e.g., through
temporary collaborations such as employment or subcontracting), we propose a
cooperative workflow, Shapley-Coop. Shapley-Coop integrates Shapley
Chain-of-Thought -- leveraging marginal contributions as a principled basis for
pricing -- with structured negotiation protocols for effective price matching,
enabling LLM agents to coordinate through rational task-time pricing and
post-task reward redistribution. This approach aligns agent incentives, fosters
cooperation, and maintains autonomy. We evaluate Shapley-Coop across two
multi-agent games and a software engineering simulation, demonstrating that it
consistently enhances LLM agent collaboration and facilitates equitable credit
assignment. These results highlight the effectiveness of Shapley-Coop's pricing
mechanisms in accurately reflecting individual contributions during task
execution.

</details>


### [622] [G-Memory: Tracing Hierarchical Memory for Multi-Agent Systems](https://arxiv.org/abs/2506.07398)
*Guibin Zhang,Muxin Fu,Guancheng Wan,Miao Yu,Kun Wang,Shuicheng Yan*

Main category: cs.MA

TL;DR: G-Memory是一种为多智能体系统（MAS）设计的分层记忆架构，通过三层图层次结构（洞察图、查询图和交互图）提升协作能力，显著提高了任务成功率和准确性。


<details>
  <summary>Details</summary>
Motivation: 当前多智能体系统的记忆机制过于简单，忽视了智能体间的协作轨迹，且缺乏跨任务和智能体定制化，限制了系统的自我进化能力。

Method: 提出G-Memory，一种基于组织记忆理论的分层记忆系统，通过双向记忆检索和动态更新三层图结构来优化协作。

Result: 在五个基准测试中，G-Memory显著提升了任务成功率（最高20.89%）和知识问答准确性（最高10.12%）。

Conclusion: G-Memory为多智能体系统的记忆架构提供了创新解决方案，显著提升了协作效率和性能。

Abstract: Large language model (LLM)-powered multi-agent systems (MAS) have
demonstrated cognitive and execution capabilities that far exceed those of
single LLM agents, yet their capacity for self-evolution remains hampered by
underdeveloped memory architectures. Upon close inspection, we are alarmed to
discover that prevailing MAS memory mechanisms (1) are overly simplistic,
completely disregarding the nuanced inter-agent collaboration trajectories, and
(2) lack cross-trial and agent-specific customization, in stark contrast to the
expressive memory developed for single agents. To bridge this gap, we introduce
G-Memory, a hierarchical, agentic memory system for MAS inspired by
organizational memory theory, which manages the lengthy MAS interaction via a
three-tier graph hierarchy: insight, query, and interaction graphs. Upon
receiving a new user query, G-Memory performs bi-directional memory traversal
to retrieve both $\textit{high-level, generalizable insights}$ that enable the
system to leverage cross-trial knowledge, and $\textit{fine-grained, condensed
interaction trajectories}$ that compactly encode prior collaboration
experiences. Upon task execution, the entire hierarchy evolves by assimilating
new collaborative trajectories, nurturing the progressive evolution of agent
teams. Extensive experiments across five benchmarks, three LLM backbones, and
three popular MAS frameworks demonstrate that G-Memory improves success rates
in embodied action and accuracy in knowledge QA by up to $20.89\%$ and
$10.12\%$, respectively, without any modifications to the original frameworks.
Our codes are available at https://github.com/bingreeky/GMemory.

</details>


### [623] [MedChat: A Multi-Agent Framework for Multimodal Diagnosis with Large Language Models](https://arxiv.org/abs/2506.07400)
*Philip Liu,Sparsh Bansal,Jimmy Dinh,Aditya Pawar,Ramani Satishkumar,Shail Desai,Neeraj Gupta,Xin Wang,Shu Hu*

Main category: cs.MA

TL;DR: MedChat提出了一种结合专业视觉模型和多个角色特定LLM代理的多代理诊断框架，以解决通用LLM在医学影像应用中的幻觉和解释性问题。


<details>
  <summary>Details</summary>
Motivation: 解决通用LLM在医学影像应用中因幻觉、解释性不足和领域知识缺乏导致的临床准确性下降问题。

Method: 结合专业视觉模型与多个角色特定LLM代理，通过导演代理协调，设计交互式诊断报告平台。

Result: 提高了可靠性，减少幻觉风险，并支持临床审查和教育用途的交互式报告。

Conclusion: MedChat框架通过多代理协作，显著提升了医学影像诊断的准确性和实用性。

Abstract: The integration of deep learning-based glaucoma detection with large language
models (LLMs) presents an automated strategy to mitigate ophthalmologist
shortages and improve clinical reporting efficiency. However, applying general
LLMs to medical imaging remains challenging due to hallucinations, limited
interpretability, and insufficient domain-specific medical knowledge, which can
potentially reduce clinical accuracy. Although recent approaches combining
imaging models with LLM reasoning have improved reporting, they typically rely
on a single generalist agent, restricting their capacity to emulate the diverse
and complex reasoning found in multidisciplinary medical teams. To address
these limitations, we propose MedChat, a multi-agent diagnostic framework and
platform that combines specialized vision models with multiple role-specific
LLM agents, all coordinated by a director agent. This design enhances
reliability, reduces hallucination risk, and enables interactive diagnostic
reporting through an interface tailored for clinical review and educational
use. Code available at https://github.com/Purdue-M2/MedChat.

</details>


### [624] [Diffusion of Responsibility in Collective Decision Making](https://arxiv.org/abs/2506.07935)
*Pavel Naumov,Jia Tao*

Main category: cs.MA

TL;DR: 论文研究了集体决策机制中的责任扩散现象，发现避免责任扩散的唯一方式是让一个代理人独裁决策。


<details>
  <summary>Details</summary>
Motivation: 探讨集体决策中责任扩散的机制及其影响，旨在明确个体责任。

Method: 通过定义决策机制的互模拟，证明互模拟保留责任相关属性，并在最小互模拟机制中验证结果。

Result: 在双代理人场景中，避免责任扩散的唯一方式是独裁决策；多代理人场景中，需选举独裁者。

Conclusion: 责任扩散在集体决策中不可避免，除非采用独裁或选举独裁机制。

Abstract: The term "diffusion of responsibility'' refers to situations in which
multiple agents share responsibility for an outcome, obscuring individual
accountability. This paper examines this frequently undesirable phenomenon in
the context of collective decision-making mechanisms.
  The work shows that if a decision is made by two agents, then the only way to
avoid diffusion of responsibility is for one agent to act as a "dictator'',
making the decision unilaterally. In scenarios with more than two agents, any
diffusion-free mechanism is an "elected dictatorship'' where the agents elect a
single agent to make a unilateral decision.
  The technical results are obtained by defining a bisimulation of
decision-making mechanisms, proving that bisimulation preserves
responsibility-related properties, and establishing the results for a smallest
bisimular mechanism.

</details>


<div id='cs.MM'></div>

# cs.MM [[Back]](#toc)

### [625] [An Efficient Digital Watermarking Technique for Small Scale devices](https://arxiv.org/abs/2506.06691)
*Kaushik Talathi,Aparna Santra Biswas*

Main category: cs.MM

TL;DR: 提出了一种轻量级水印方案FWT-AQIM，用于在低功耗设备上保护数字图片，平衡了鲁棒性、不可感知性和计算效率。


<details>
  <summary>Details</summary>
Motivation: 在物联网和移动平台时代，如何在有限硬件上确保内容真实性且不增加负担是关键问题。

Method: 结合快速小波变换和加性量化索引调制（FWT-AQIM），在YCbCr颜色空间的亮度分量中嵌入水印，使用低频FWT子带以减少感知失真。

Result: 在树莓派5上测试，嵌入和提取过程耗时少于40毫秒，内存占用低；PSNR≥34 dB，SSIM≥0.97；抗攻击能力强，NCC≥0.998。

Conclusion: FWT-AQIM为带宽和功耗受限的场景提供了高效、可扩展的实时水印解决方案，适用于物联网和多媒体应用。

Abstract: In the age of IoT and mobile platforms, ensuring that content stay authentic
whilst avoiding overburdening limited hardware is a key problem. This study
introduces hybrid Fast Wavelet Transform & Additive Quantization index
Modulation (FWT-AQIM) scheme, a lightweight watermarking approach that secures
digital pictures on low-power, memory-constrained small scale devices to
achieve a balanced trade-off among robustness, imperceptibility, and
computational efficiency. The method embeds watermark in the luminance
component of YCbCr color space using low-frequency FWT sub-bands, minimizing
perceptual distortion, using additive QIM for simplicity. Both the extraction
and embedding processes run in less than 40 ms and require minimum RAM when
tested on a Raspberry Pi 5. Quality assessments on standard and high-resolution
images yield PSNR greater than equal to 34 dB and SSIM greater than equal to
0.97, while robustness verification includes various geometric and
signal-processing attacks demonstrating near-zero bit error rates and NCC
greater than equal to 0.998. Using a mosaic-based watermark, redundancy added
enhancing robustness without reducing throughput, which peaks at 11 MP/s. These
findings show that FWT-AQIM provides an efficient, scalable solution for
real-time, secure watermarking in bandwidth- and power-constrained contexts,
opening the way for dependable content protection in developing IoT and
multimedia applications.

</details>


### [626] [The State-of-the-Art in Lifelog Retrieval: A Review of Progress at the ACM Lifelog Search Challenge Workshop 2022-24](https://arxiv.org/abs/2506.06743)
*Allie Tran,Werner Bailer,Duc-Tien Dang-Nguyen,Graham Healy,Steve Hodges,Björn Þór Jónsson,Luca Rossetto,Klaus Schoeffmann,Minh-Triet Tran,Lucia Vadicamo,Cathal Gurrin*

Main category: cs.MM

TL;DR: 本文回顾了2022至2024年ACM Lifelog Search Challenge (LSC)中交互式生命日志检索的进展，总结了嵌入检索方法、大语言模型和多模态界面的趋势，并强调了检索复杂性与可用性的平衡。


<details>
  <summary>Details</summary>
Motivation: 探讨生命日志检索系统的最新进展及其在交互式竞赛中的表现。

Method: 通过比较分析2022至2024年ACM LSC的数据，评估嵌入检索方法、大语言模型和多模态界面的应用。

Result: 嵌入检索与大语言模型结合显示出潜力，UI设计的改进提升了可用性和效率。

Conclusion: 建议优化多实例系统评估，以更好地管理用户熟悉度和配置效果的差异。

Abstract: The ACM Lifelog Search Challenge (LSC) is a venue that welcomes and compares
systems that support the exploration of lifelog data, and in particular the
retrieval of specific information, through an interactive competition format.
This paper reviews the recent advances in interactive lifelog retrieval as
demonstrated at the ACM LSC from 2022 to 2024. Through a detailed comparative
analysis, we highlight key improvements across three main retrieval tasks:
known-item search, question answering, and ad-hoc search. Our analysis
identifies trends such as the widespread adoption of embedding-based retrieval
methods (e.g., CLIP, BLIP), increased integration of large language models
(LLMs) for conversational retrieval, and continued innovation in multimodal and
collaborative search interfaces. We further discuss how specific retrieval
techniques and user interface (UI) designs have impacted system performance,
emphasizing the importance of balancing retrieval complexity with usability.
Our findings indicate that embedding-driven approaches combined with LLMs show
promise for lifelog retrieval systems. Likewise, improving UI design can
enhance usability and efficiency. Additionally, we recommend reconsidering
multi-instance system evaluations within the expert track to better manage
variability in user familiarity and configuration effectiveness.

</details>


### [627] [Experimental Evaluation of Static Image Sub-Region-Based Search Models Using CLIP](https://arxiv.org/abs/2506.06938)
*Bastian Jäckl,Vojtěch Kloda,Daniel A. Keim,Jakub Lokoč*

Main category: cs.MM

TL;DR: 研究探讨了在高度同质化的专业领域中，通过添加基于位置的提示来增强模糊文本查询的检索性能。


<details>
  <summary>Details</summary>
Motivation: 多模态文本-图像模型在日常生活场景中表现良好，但在同质化专业领域中的查询仍具挑战性，因为用户通常只能提供模糊的文本描述。

Method: 收集了741个人工标注的数据集，包含短/长文本描述和感兴趣区域的边界框，评估CLIP模型在不同静态子区域上的查询性能。

Result: 简单的3x3分区和5网格重叠显著提高了检索效果，并对标注框的扰动具有鲁棒性。

Conclusion: 基于位置的提示可以有效补充模糊文本查询，提升在专业领域中的检索性能。

Abstract: Advances in multimodal text-image models have enabled effective text-based
querying in extensive image collections. While these models show convincing
performance for everyday life scenes, querying in highly homogeneous,
specialized domains remains challenging. The primary problem is that users can
often provide only vague textual descriptions as they lack expert knowledge to
discriminate between homogenous entities. This work investigates whether adding
location-based prompts to complement these vague text queries can enhance
retrieval performance. Specifically, we collected a dataset of 741 human
annotations, each containing short and long textual descriptions and bounding
boxes indicating regions of interest in challenging underwater scenes. Using
these annotations, we evaluate the performance of CLIP when queried on various
static sub-regions of images compared to the full image. Our results show that
both a simple 3-by-3 partitioning and a 5-grid overlap significantly improve
retrieval effectiveness and remain robust to perturbations of the annotation
box.

</details>


### [628] [Harmony-Aware Music-driven Motion Synthesis with Perceptual Constraint on UGC Datasets](https://arxiv.org/abs/2506.07076)
*Xinyi Wu,Haohong Wang,Aggelos K. Katsaggelos*

Main category: cs.MM

TL;DR: 提出了一种基于GAN的框架，通过和谐评估策略提升音乐到动作合成的节奏同步性。


<details>
  <summary>Details</summary>
Motivation: 随着视频用户生成内容（UGC）的流行，基于人类感知原则的节奏一致性对提升用户参与度至关重要。

Method: 采用跨模态节拍检测、显著性节拍加权和间隔驱动的节拍对齐策略，结合编码器-解码器和深度提升设计进行对抗训练。

Result: 实验表明，该模型在节奏和谐性上显著优于其他方法，即使训练数据有限。

Conclusion: 提出的和谐评估策略和模型设计有效提升了音乐到动作合成的节奏同步性和真实性。

Abstract: With the popularity of video-based user-generated content (UGC) on social
media, harmony, as dictated by human perceptual principles, is critical in
assessing the rhythmic consistency of audio-visual UGCs for better user
engagement. In this work, we propose a novel harmony-aware GAN framework,
following a specifically designed harmony evaluation strategy to enhance
rhythmic synchronization in the automatic music-to-motion synthesis using a UGC
dance dataset. This harmony strategy utilizes refined cross-modal beat
detection to capture closely correlated audio and visual rhythms in an
audio-visual pair. To mimic human attention mechanism, we introduce
saliency-based beat weighting and interval-driven beat alignment, which ensures
accurate harmony score estimation consistent with human perception. Building on
this strategy, our model, employing efficient encoder-decoder and depth-lifting
designs, is adversarially trained based on categorized musical meter segments
to generate realistic and rhythmic 3D human motions. We further incorporate our
harmony evaluation strategy as a weakly supervised perceptual constraint to
flexibly guide the synchronized audio-visual rhythms during the generation
process. Experimental results show that our proposed model significantly
outperforms other leading music-to-motion methods in rhythmic harmony, both
quantitatively and qualitatively, even with limited UGC training data. Live
samples 15 can be watched at: https://youtu.be/tWwz7yq4aUs

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [629] [Tactile MNIST: Benchmarking Active Tactile Perception](https://arxiv.org/abs/2506.06361)
*Tim Schneider,Guillaume Duret,Cristiana de Farias,Roberto Calandra,Liming Chen,Jan Peters*

Main category: cs.RO

TL;DR: 论文提出了Tactile MNIST Benchmark Suite，一个用于主动触觉感知任务的开源基准测试套件，填补了触觉感知和主动感知领域缺乏标准化基准的空白。


<details>
  <summary>Details</summary>
Motivation: 触觉感知能增强机器人操作的灵活性，但由于其局部性，难以单独完成需要全局场景理解的任务。主动感知技术可以弥补这一不足，但目前缺乏标准化基准。

Method: 引入Tactile MNIST Benchmark Suite，包含多样化的模拟场景和数据集（合成3D MNIST数字模型和真实触觉样本），并训练CycleGAN用于逼真的触觉模拟渲染。

Result: 提供了标准化的协议和可复现的评估框架，促进触觉感知和主动感知领域的系统性进展。

Conclusion: 该基准套件为触觉感知和主动感知研究提供了重要工具，推动了相关领域的标准化和进展。

Abstract: Tactile perception has the potential to significantly enhance dexterous
robotic manipulation by providing rich local information that can complement or
substitute for other sensory modalities such as vision. However, because
tactile sensing is inherently local, it is not well-suited for tasks that
require broad spatial awareness or global scene understanding on its own. A
human-inspired strategy to address this issue is to consider active perception
techniques instead. That is, to actively guide sensors toward regions with more
informative or significant features and integrate such information over time in
order to understand a scene or complete a task. Both active perception and
different methods for tactile sensing have received significant attention
recently. Yet, despite advancements, both fields lack standardized benchmarks.
To bridge this gap, we introduce the Tactile MNIST Benchmark Suite, an
open-source, Gymnasium-compatible benchmark specifically designed for active
tactile perception tasks, including localization, classification, and volume
estimation. Our benchmark suite offers diverse simulation scenarios, from
simple toy environments all the way to complex tactile perception tasks using
vision-based tactile sensors. Furthermore, we also offer a comprehensive
dataset comprising 13,500 synthetic 3D MNIST digit models and 153,600
real-world tactile samples collected from 600 3D printed digits. Using this
dataset, we train a CycleGAN for realistic tactile simulation rendering. By
providing standardized protocols and reproducible evaluation frameworks, our
benchmark suite facilitates systematic progress in the fields of tactile
sensing and active perception.

</details>


### [630] [CPS-Guard: Framework for Dependability Assurance of AI- and LLM-Based Cyber-Physical Systems](https://arxiv.org/abs/2506.06381)
*Trisanth Srinivasan,Santosh Patapati,Himani Musku,Idhant Gode,Aditya Arora,Samvit Bhattacharya,Abubakr Nazriev,Sanika Hirave,Zaryab Kanjiani,Srinjoy Ghose,Srinidhi Shetty*

Main category: cs.RO

TL;DR: CPS-Guard是一个通过多角色编排自动化AI驱动CPS保障过程的新框架，有效检测漏洞并支持自适应恢复策略。


<details>
  <summary>Details</summary>
Motivation: 传统验证方法难以应对AI组件的动态性和不可预测性，需要一种新的保障框架。

Method: 采用多角色编排，在模拟环境中分配专用代理角色（如安全监控、故障注入等），持续评估和优化AI行为。

Result: 通过自动驾驶车辆案例研究，证明CPS-Guard能有效检测漏洞、管理性能影响并支持自适应恢复。

Conclusion: CPS-Guard为安全和关键系统提供了一种结构化和可扩展的严格验证与验证解决方案。

Abstract: Cyber-Physical Systems (CPS) increasingly depend on advanced AI techniques to
operate in critical applications. However, traditional verification and
validation methods often struggle to handle the unpredictable and dynamic
nature of AI components. In this paper, we introduce CPS-Guard, a novel
framework that employs multi-role orchestration to automate the iterative
assurance process for AI-powered CPS. By assigning specialized roles (e.g.,
safety monitoring, security assessment, fault injection, and recovery planning)
to dedicated agents within a simulated environment, CPS-Guard continuously
evaluates and refines AI behavior against a range of dependability
requirements. We demonstrate the framework through a case study involving an
autonomous vehicle navigating an intersection with an AI-based planner. Our
results show that CPS-Guard effectively detects vulnerabilities, manages
performance impacts, and supports adaptive recovery strategies, thereby
offering a structured and extensible solution for rigorous V&V in safety- and
security-critical systems.

</details>


### [631] [Active Illumination Control in Low-Light Environments using NightHawk](https://arxiv.org/abs/2506.06394)
*Yash Turkar,Youngjin Kim,Karthik Dantu*

Main category: cs.RO

TL;DR: NightHawk框架通过结合主动照明和曝光控制，优化地下环境中机器人视觉的图像质量，显著提升特征检测和匹配性能。


<details>
  <summary>Details</summary>
Motivation: 地下环境（如涵洞）光线昏暗且缺乏特征，传统照明方法存在反射、过曝和功耗问题。

Method: 提出在线贝叶斯优化方法，动态调整光照强度和曝光时间，并设计基于特征检测的效用指标作为优化目标。

Result: 实验显示特征检测和匹配性能提升47-197%，视觉估计更可靠。

Conclusion: NightHawk有效解决了地下环境中的视觉挑战，提升了机器人导航能力。

Abstract: Subterranean environments such as culverts present significant challenges to
robot vision due to dim lighting and lack of distinctive features. Although
onboard illumination can help, it introduces issues such as specular
reflections, overexposure, and increased power consumption. We propose
NightHawk, a framework that combines active illumination with exposure control
to optimize image quality in these settings. NightHawk formulates an online
Bayesian optimization problem to determine the best light intensity and
exposure-time for a given scene. We propose a novel feature detector-based
metric to quantify image utility and use it as the cost function for the
optimizer. We built NightHawk as an event-triggered recursive optimization
pipeline and deployed it on a legged robot navigating a culvert beneath the
Erie Canal. Results from field experiments demonstrate improvements in feature
detection and matching by 47-197% enabling more reliable visual estimation in
challenging lighting conditions.

</details>


### [632] [Edge-Enabled Collaborative Object Detection for Real-Time Multi-Vehicle Perception](https://arxiv.org/abs/2506.06474)
*Everett Richards,Bipul Thapa,Lena Mashayekhy*

Main category: cs.RO

TL;DR: 论文提出了一种基于边缘计算和多车协作的实时物体检测框架ECOD，通过PACE和VOTE算法提升CAV在动态环境中的感知能力，实验显示其分类准确率比传统方法高75%。


<details>
  <summary>Details</summary>
Motivation: 传统车载感知系统因遮挡和盲区精度受限，云端解决方案延迟高，无法满足实时需求。

Method: ECOD框架结合PACE（多车数据聚合）和VOTE（共识投票机制）算法，利用边缘计算实现低延迟决策。

Result: 实验表明ECOD在分类准确率上比传统方法提升75%，且满足实时处理需求。

Conclusion: 边缘计算可显著提升协作感知能力，适用于延迟敏感的自动驾驶系统。

Abstract: Accurate and reliable object detection is critical for ensuring the safety
and efficiency of Connected Autonomous Vehicles (CAVs). Traditional on-board
perception systems have limited accuracy due to occlusions and blind spots,
while cloud-based solutions introduce significant latency, making them
unsuitable for real-time processing demands required for autonomous driving in
dynamic environments. To address these challenges, we introduce an innovative
framework, Edge-Enabled Collaborative Object Detection (ECOD) for CAVs, that
leverages edge computing and multi-CAV collaboration for real-time,
multi-perspective object detection. Our ECOD framework integrates two key
algorithms: Perceptive Aggregation and Collaborative Estimation (PACE) and
Variable Object Tally and Evaluation (VOTE). PACE aggregates detection data
from multiple CAVs on an edge server to enhance perception in scenarios where
individual CAVs have limited visibility. VOTE utilizes a consensus-based voting
mechanism to improve the accuracy of object classification by integrating data
from multiple CAVs. Both algorithms are designed at the edge to operate in
real-time, ensuring low-latency and reliable decision-making for CAVs. We
develop a hardware-based controlled testbed consisting of camera-equipped
robotic CAVs and an edge server to evaluate the efficacy of our framework. Our
experimental results demonstrate the significant benefits of ECOD in terms of
improved object classification accuracy, outperforming traditional
single-perspective onboard approaches by up to 75%, while ensuring low-latency,
edge-driven real-time processing. This research highlights the potential of
edge computing to enhance collaborative perception for latency-sensitive
autonomous systems.

</details>


### [633] [Enhancing Situational Awareness in Underwater Robotics with Multi-modal Spatial Perception](https://arxiv.org/abs/2506.06476)
*Pushyami Kaveti,Ambjorn Grimsrud Waldum,Hanumant Singh,Martin Ludvigsen*

Main category: cs.RO

TL;DR: 论文提出了一种多模态感知方法，结合摄像头、IMU和声学设备数据，以提升水下SLAM的鲁棒性和实时性。


<details>
  <summary>Details</summary>
Motivation: 水下环境（如光线衰减、低对比度）导致传统视觉SLAM失效，且现有方法难以适应多摄像头配置。

Method: 融合多传感器数据（摄像头、IMU、声学设备），结合几何与学习技术及语义分析。

Result: 实验证明在复杂水下环境中可实现实时可靠的状态估计和高质量3D重建。

Conclusion: 多模态感知方法有效，但传感器校准和学习方法限制仍需进一步研究。

Abstract: Autonomous Underwater Vehicles (AUVs) and Remotely Operated Vehicles (ROVs)
demand robust spatial perception capabilities, including Simultaneous
Localization and Mapping (SLAM), to support both remote and autonomous tasks.
Vision-based systems have been integral to these advancements, capturing rich
color and texture at low cost while enabling semantic scene understanding.
However, underwater conditions -- such as light attenuation, backscatter, and
low contrast -- often degrade image quality to the point where traditional
vision-based SLAM pipelines fail. Moreover, these pipelines typically rely on
monocular or stereo inputs, limiting their scalability to the multi-camera
configurations common on many vehicles. To address these issues, we propose to
leverage multi-modal sensing that fuses data from multiple sensors-including
cameras, inertial measurement units (IMUs), and acoustic devices-to enhance
situational awareness and enable robust, real-time SLAM. We explore both
geometric and learning-based techniques along with semantic analysis, and
conduct experiments on the data collected from a work-class ROV during several
field deployments in the Trondheim Fjord. Through our experimental results, we
demonstrate the feasibility of real-time reliable state estimation and
high-quality 3D reconstructions in visually challenging underwater conditions.
We also discuss system constraints and identify open research questions, such
as sensor calibration, limitations with learning-based methods, that merit
further exploration to advance large-scale underwater operations.

</details>


### [634] [BeliefMapNav: 3D Voxel-Based Belief Map for Zero-Shot Object Navigation](https://arxiv.org/abs/2506.06487)
*Zibo Zhou,Yue Hu,Lingkai Zhang,Zonglin Li,Siheng Chen*

Main category: cs.RO

TL;DR: 论文提出了一种基于3D体素的目标导航系统BeliefMapNav，通过结合语义先验和实时观测，显著提升了零样本目标导航的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的大型语言模型和视觉语言模型在零样本目标导航中缺乏全局环境理解和空间推理能力，限制了导航效果。

Method: 提出了一种3D体素化的信念地图，结合语义先验和实时观测，构建目标位置的后验分布，并设计了高效的导航系统BeliefMapNav。

Result: 在多个基准测试中，BeliefMapNav取得了最优的成功率和路径长度加权成功率，SPL提升了46.4%。

Conclusion: BeliefMapNav通过全局语义和空间推理，显著提升了零样本目标导航的效率和准确性。

Abstract: Zero-shot object navigation (ZSON) allows robots to find target objects in
unfamiliar environments using natural language instructions, without relying on
pre-built maps or task-specific training. Recent general-purpose models, such
as large language models (LLMs) and vision-language models (VLMs), equip agents
with semantic reasoning abilities to estimate target object locations in a
zero-shot manner. However, these models often greedily select the next goal
without maintaining a global understanding of the environment and are
fundamentally limited in the spatial reasoning necessary for effective
navigation. To overcome these limitations, we propose a novel 3D voxel-based
belief map that estimates the target's prior presence distribution within a
voxelized 3D space. This approach enables agents to integrate semantic priors
from LLMs and visual embeddings with hierarchical spatial structure, alongside
real-time observations, to build a comprehensive 3D global posterior belief of
the target's location. Building on this 3D voxel map, we introduce
BeliefMapNav, an efficient navigation system with two key advantages: i)
grounding LLM semantic reasoning within the 3D hierarchical semantics voxel
space for precise target position estimation, and ii) integrating sequential
path planning to enable efficient global navigation decisions. Experiments on
HM3D, MP3D, and HSSD benchmarks show that BeliefMapNav achieves
state-of-the-art (SOTA) Success Rate (SR) and Success weighted by Path Length
(SPL), with a notable 46.4% SPL improvement over the previous best SR method,
validating its effectiveness and efficiency.

</details>


### [635] [MapleGrasp: Mask-guided Feature Pooling for Language-driven Efficient Robotic Grasping](https://arxiv.org/abs/2506.06535)
*Vineet Bhat,Naman Patel,Prashanth Krishnamurthy,Ramesh Karri,Farshad Khorrami*

Main category: cs.RO

TL;DR: 论文提出了一种名为Mask-guided feature pooling的轻量级增强方法，用于语言驱动的机器人抓取（LDRG），通过两阶段训练策略和掩码区域特征池化，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 解决自然语言驱动的机器人抓取未见过物体的挑战，提升抓取效率和准确性。

Method: 采用两阶段训练策略：首先生成分割掩码，然后在掩码区域内进行特征池化以预测抓取姿态。

Result: 在OCID-VLG基准上性能提升12%，并在真实世界实验中达到57%的成功率。

Conclusion: 该方法在性能和泛化能力上优于现有方法，并通过开源数据集进一步推动了领域发展。

Abstract: Robotic manipulation of unseen objects via natural language commands remains
challenging. Language driven robotic grasping (LDRG) predicts stable grasp
poses from natural language queries and RGB-D images. Here we introduce
Mask-guided feature pooling, a lightweight enhancement to existing LDRG
methods. Our approach employs a two-stage training strategy: first, a
vision-language model generates feature maps from CLIP-fused embeddings, which
are upsampled and weighted by text embeddings to produce segmentation masks.
Next, the decoder generates separate feature maps for grasp prediction, pooling
only token features within these masked regions to efficiently predict grasp
poses. This targeted pooling approach reduces computational complexity,
accelerating both training and inference. Incorporating mask pooling results in
a 12% improvement over prior approaches on the OCID-VLG benchmark. Furthermore,
we introduce RefGraspNet, an open-source dataset eight times larger than
existing alternatives, significantly enhancing model generalization for
open-vocabulary grasping. By extending 2D grasp predictions to 3D via depth
mapping and inverse kinematics, our modular method achieves performance
comparable to recent Vision-Language-Action (VLA) models on the LIBERO
simulation benchmark, with improved generalization across different task
suites. Real-world experiments on a 7 DoF Franka robotic arm demonstrate a 57%
success rate with unseen objects, surpassing competitive baselines by 7%. Code
will be released post publication.

</details>


### [636] [Semantics-aware Predictive Inspection Path Planning](https://arxiv.org/abs/2506.06560)
*Mihir Dharmadhikari,Kostas Alexis*

Main category: cs.RO

TL;DR: 本文提出了一种名为“语义感知预测规划”（SPP）的新型语义感知检查路径规划方法，针对工业环境中重复语义结构的预测与利用，显著提升了检查效率。


<details>
  <summary>Details</summary>
Motivation: 工业环境中（如船舶压载水舱）的语义结构通常具有重复性，利用这种重复性可以优化检查路径规划。

Method: 提出了一种算法，用于识别语义场景图中的重复模式并预测未观察部分的语义结构，随后设计了两种针对压载水舱检查的路径规划策略。

Result: 仿真和实验结果表明，该方法在检查时间和语义覆盖方面优于现有技术，并能处理不完美的重复模式。

Conclusion: SPP方法在工业检查任务中表现出高效性和鲁棒性，代码和视频已开源。

Abstract: This paper presents a novel semantics-aware inspection path planning paradigm
called "Semantics-aware Predictive Planning" (SPP). Industrial environments
that require the inspection of specific objects or structures (called
"semantics"), such as ballast water tanks inside ships, often present
structured and repetitive spatial arrangements of the semantics of interest.
Motivated by this, we first contribute an algorithm that identifies spatially
repeating patterns of semantics - exact or inexact - in a semantic scene graph
representation and makes predictions about the evolution of the graph in the
unseen parts of the environment using these patterns. Furthermore, two
inspection path planning strategies, tailored to ballast water tank inspection,
that exploit these predictions are proposed. To assess the performance of the
novel predictive planning paradigm, both simulation and experimental
evaluations are performed. First, we conduct a simulation study comparing the
method against relevant state-of-the-art techniques and further present tests
showing its ability to handle imperfect patterns. Second, we deploy our method
onboard a collision-tolerant aerial robot operating inside the ballast tanks of
two real ships. The results, both in simulation and field experiments,
demonstrate significant improvement over the state-of-the-art in terms of
inspection time while maintaining equal or better semantic surface coverage. A
set of videos describing the different parts of the method and the field
deployments is available at https://tinyurl.com/spp-videos. The code for this
work is made available at https://github.com/ntnu-arl/predictive_planning_ros.

</details>


### [637] [Towards Terrain-Aware Task-Driven 3D Scene Graph Generation in Outdoor Environments](https://arxiv.org/abs/2506.06562)
*Chad R Samuelson,Timothy W McLain,Joshua G Mangelson*

Main category: cs.RO

TL;DR: 论文探讨了3D场景图（3DSGs）在户外环境中的构建与应用，提出了一种任务无关的度量-语义点云生成方法，并改进了现有室内3DSG技术以适应户外场景。


<details>
  <summary>Details</summary>
Motivation: 传统3D场景表示（如点云和占用网格）缺乏语义组织，限制了机器人的高级推理能力。3DSGs通过整合几何、拓扑和语义关系，提供了结构化表示，但现有研究主要集中在室内环境。本文旨在探索3DSGs在户外环境中的潜力。

Method: 提出了一种生成任务无关的度量-语义点云的方法，并改进了现有室内3DSG生成技术以适应户外场景。

Result: 初步定性结果表明户外3DSGs的可行性，并展示了其在现实世界机器人应用中的潜力。

Conclusion: 户外3DSGs有望提升机器人的上下文感知决策和自适应规划能力，为未来实际应用奠定了基础。

Abstract: High-level autonomous operations depend on a robot's ability to construct a
sufficiently expressive model of its environment. Traditional three-dimensional
(3D) scene representations, such as point clouds and occupancy grids, provide
detailed geometric information but lack the structured, semantic organization
needed for high-level reasoning. 3D scene graphs (3DSGs) address this
limitation by integrating geometric, topological, and semantic relationships
into a multi-level graph-based representation. By capturing hierarchical
abstractions of objects and spatial layouts, 3DSGs enable robots to reason
about environments in a structured manner, improving context-aware
decision-making and adaptive planning. Although most recent work has focused on
indoor 3DSGs, this paper investigates their construction and utility in outdoor
environments. We present a method for generating a task-agnostic
metric-semantic point cloud for large outdoor settings and propose
modifications to existing indoor 3DSG generation techniques for outdoor
applicability. Our preliminary qualitative results demonstrate the feasibility
of outdoor 3DSGs and highlight their potential for future deployment in
real-world field robotic applications.

</details>


### [638] [NeSyPack: A Neuro-Symbolic Framework for Bimanual Logistics Packing](https://arxiv.org/abs/2506.06567)
*Bowei Li,Peiqi Yu,Zhenran Tang,Han Zhou,Yifan Sun,Ruixuan Liu,Changliu Liu*

Main category: cs.RO

TL;DR: NeSyPack是一个神经符号框架，用于双手物流包装，结合数据驱动模型和符号推理，构建可解释、通用、高效且可靠的系统。


<details>
  <summary>Details</summary>
Motivation: 解决双手物流包装任务中端到端模型需要大规模重新训练的问题，提供更高效、可解释且通用的解决方案。

Method: 通过分层推理将任务分解为子任务，再进一步分解为由符号技能图管理的原子技能，选择参数、配置和控制策略。

Result: NeSyPack在2025年IEEE国际机器人与自动化会议的WBCD竞赛中获得一等奖。

Conclusion: NeSyPack的模块化设计实现了鲁棒性、适应性和高效复用，优于端到端模型。

Abstract: This paper presents NeSyPack, a neuro-symbolic framework for bimanual
logistics packing. NeSyPack combines data-driven models and symbolic reasoning
to build an explainable hierarchical system that is generalizable,
data-efficient, and reliable. It decomposes a task into subtasks via
hierarchical reasoning, and further into atomic skills managed by a symbolic
skill graph. The graph selects skill parameters, robot configurations, and
task-specific control strategies for execution. This modular design enables
robustness, adaptability, and efficient reuse - outperforming end-to-end models
that require large-scale retraining. Using NeSyPack, our team won the First
Prize in the What Bimanuals Can Do (WBCD) competition at the 2025 IEEE
International Conference on Robotics and Automation.

</details>


### [639] [Enhancing Robot Safety via MLLM-Based Semantic Interpretation of Failure Data](https://arxiv.org/abs/2506.06570)
*Aryaman Gupta,Yusuf Umut Ciftci,Somil Bansal*

Main category: cs.RO

TL;DR: 提出一种利用多模态大语言模型（MLLMs）自动聚类机器人失败数据的方法，以无监督方式从失败中学习，提升策略改进和实时适应能力。


<details>
  <summary>Details</summary>
Motivation: 机器人系统在复杂环境中易失败，手动分析失败数据不切实际，需自动化方法从失败中学习以提高性能和安全性。

Method: 利用MLLMs从原始感知轨迹推断失败原因，并将未整理的失败日志聚类为语义上有意义的模式。

Result: 语义聚类揭示了潜在的失败模式和原因，可用于指导策略改进和实时失败检测，提升机器人学习和鲁棒性。

Conclusion: 该方法将失败数据转化为可操作的信号，为机器人学习和适应提供了高效且可解释的框架。

Abstract: As robotic systems become increasingly integrated into real-world
environments, ranging from autonomous vehicles to household assistants, they
inevitably encounter diverse and unstructured scenarios that lead to failures.
While such failures pose safety and reliability challenges, they also provide
rich perceptual data for improving future performance. However, manually
analyzing large-scale failure datasets is impractical. In this work, we present
a method for automatically organizing large-scale robotic failure data into
semantically meaningful clusters, enabling scalable learning from failure
without human supervision. Our approach leverages the reasoning capabilities of
Multimodal Large Language Models (MLLMs), trained on internet-scale data, to
infer high-level failure causes from raw perceptual trajectories and discover
interpretable structure within uncurated failure logs. These semantic clusters
reveal latent patterns and hypothesized causes of failure, enabling scalable
learning from experience. We demonstrate that the discovered failure modes can
guide targeted data collection for policy refinement, accelerating iterative
improvement in agent policies and overall safety. Additionally, we show that
these semantic clusters can be employed for online failure detection, offering
a lightweight yet powerful safeguard for real-time adaptation. We demonstrate
that this framework enhances robot learning and robustness by transforming
real-world failures into actionable and interpretable signals for adaptation.

</details>


### [640] [Underwater Multi-Robot Simulation and Motion Planning in Angler](https://arxiv.org/abs/2506.06612)
*Akshaya Agrawal,Evan Palmer,Zachary Kingston,Geoffrey A. Hollinger*

Main category: cs.RO

TL;DR: 扩展了Angler框架，支持多机器人模拟和运动规划，提供模块化架构和工具，促进水下多机器人系统的开发与测试。


<details>
  <summary>Details</summary>
Motivation: 水下多机器人系统部署成本高且耗时，需模拟框架支持以加速开发。

Method: 扩展Angler框架，创建非冲突通信通道，集成Gazebo、ArduSub SITL和MAVROS，并加入运动规划模块和碰撞避免工具。

Result: 实现多机器人同时模拟，支持动态环境下的运动规划和碰撞避免。

Conclusion: 该扩展为水下多机器人系统的开发和测试提供了高效工具。

Abstract: Deploying multi-robot systems in underwater environments is expensive and
lengthy; testing algorithms and software in simulation improves development by
decoupling software and hardware. However, this requires a simulation framework
that closely resembles the real-world. Angler is an open-source framework that
simulates low-level communication protocols for an onboard autopilot, such as
ArduSub, providing a framework that is close to reality, but unfortunately
lacking support for simulating multiple robots. We present an extension to
Angler that supports multi-robot simulation and motion planning. Our extension
has a modular architecture that creates non-conflicting communication channels
between Gazebo, ArduSub Software-in-the-Loop (SITL), and MAVROS to operate
multiple robots simultaneously in the same environment. Our multi-robot motion
planning module interfaces with cascaded controllers via a JointTrajectory
controller in ROS~2. We also provide an integration with the Open Motion
Planning Library (OMPL), a collision avoidance module, and tools for procedural
environment generation. Our work enables the development and benchmarking of
underwater multi-robot motion planning in dynamic environments.

</details>


### [641] [Attention-Based Convolutional Neural Network Model for Human Lower Limb Activity Recognition using sEMG](https://arxiv.org/abs/2506.06624)
*Mojtaba Mollahossein,Farshad Haghgoo Daryakenari,Mohammad Hossein Rohban,Gholamreza Vossoughi*

Main category: cs.RO

TL;DR: 提出了一种轻量级注意力深度神经网络，用于实时分类下肢运动，无需昂贵预处理，验证集准确率86.74%，测试集85.38%。


<details>
  <summary>Details</summary>
Motivation: 精确分类下肢运动对辅助机器人和康复系统至关重要。

Method: 使用多通道sEMG数据，设计轻量级注意力DNN（62,876参数），采用留一验证策略。

Result: 验证集准确率86.74%，测试集85.38%，计算成本低，适合实时部署。

Conclusion: 该模型在计算成本和实时响应关键场景中表现优异，适合人机交互系统。

Abstract: Accurate classification of lower limb movements using surface
electromyography (sEMG) signals plays a crucial role in assistive robotics and
rehabilitation systems. In this study, we present a lightweight attention-based
deep neural network (DNN) for real-time movement classification using
multi-channel sEMG data from the publicly available BASAN dataset. The proposed
model consists of only 62,876 parameters and is designed without the need for
computationally expensive preprocessing, making it suitable for real-time
deployment. We employed a leave-oneout validation strategy to ensure
generalizability across subjects, and evaluated the model on three movement
classes: walking, standing with knee flexion, and sitting with knee extension.
The network achieved 86.74% accuracy on the validation set and 85.38% on the
test set, demonstrating strong classification performance under realistic
conditions. Comparative analysis with existing models in the literature
highlights the efficiency and effectiveness of our approach, especially in
scenarios where computational cost and real-time response are critical. The
results indicate that the proposed model is a promising candidate for
integration into upper-level controllers in human-robot interaction systems.

</details>


### [642] [Active Test-time Vision-Language Navigation](https://arxiv.org/abs/2506.06630)
*Heeju Ko,Sungjune Kim,Gyeongrok Oh,Jeongyoon Yoon,Honglak Lee,Sujin Jang,Seungryong Kim,Sangpil Kim*

Main category: cs.RO

TL;DR: ATENA是一个测试时主动学习框架，通过人类-机器人交互优化导航策略，提升不确定性和适应性。


<details>
  <summary>Details</summary>
Motivation: 离线训练的视觉语言导航策略在陌生环境中性能下降，熵最小化可能因错误累积导致过度自信。

Method: 提出混合熵优化和自我主动学习策略，结合动作和伪专家分布，优化不确定性和决策。

Result: 在多个VLN基准测试中表现优异，克服了分布偏移问题。

Conclusion: ATENA通过主动学习和不确定性校准，显著提升了导航策略的适应性和性能。

Abstract: Vision-Language Navigation (VLN) policies trained on offline datasets often
exhibit degraded task performance when deployed in unfamiliar navigation
environments at test time, where agents are typically evaluated without access
to external interaction or feedback. Entropy minimization has emerged as a
practical solution for reducing prediction uncertainty at test time; however,
it can suffer from accumulated errors, as agents may become overconfident in
incorrect actions without sufficient contextual grounding. To tackle these
challenges, we introduce ATENA (Active TEst-time Navigation Agent), a test-time
active learning framework that enables a practical human-robot interaction via
episodic feedback on uncertain navigation outcomes. In particular, ATENA learns
to increase certainty in successful episodes and decrease it in failed ones,
improving uncertainty calibration. Here, we propose mixture entropy
optimization, where entropy is obtained from a combination of the action and
pseudo-expert distributions-a hypothetical action distribution assuming the
agent's selected action to be optimal-controlling both prediction confidence
and action preference. In addition, we propose a self-active learning strategy
that enables an agent to evaluate its navigation outcomes based on confident
predictions. As a result, the agent stays actively engaged throughout all
iterations, leading to well-grounded and adaptive decision-making. Extensive
evaluations on challenging VLN benchmarks-REVERIE, R2R, and R2R-CE-demonstrate
that ATENA successfully overcomes distributional shifts at test time,
outperforming the compared baseline methods across various settings.

</details>


### [643] [Self-Adapting Improvement Loops for Robotic Learning](https://arxiv.org/abs/2506.06658)
*Calvin Luo,Zilai Zeng,Mingxi Jia,Yilun Du,Chen Sun*

Main category: cs.RO

TL;DR: 论文提出了一种自我适应改进循环（SAIL）方法，通过结合互联网规模的预训练视频模型和自我收集的行为数据，持续提升视频生成模型在新任务中的性能。


<details>
  <summary>Details</summary>
Motivation: 解决视频生成模型在新任务中泛化能力不足的问题，并探索如何通过在线自我改进提升性能。

Method: 提出SAIL方法，利用互联网规模的预训练视频模型和自我收集的行为数据，迭代更新视频模型。

Result: 在MetaWorld任务和真实机器人任务中，SAIL持续提升性能，且对初始数据质量和过滤方式具有鲁棒性。

Conclusion: SAIL通过结合互联网规模数据和在线学习，为机器人任务提供了一种高效的自适应改进方案。

Abstract: Video generative models trained on expert demonstrations have been utilized
as performant text-conditioned visual planners for solving robotic tasks.
However, generalization to unseen tasks remains a challenge. Whereas improved
generalization may be facilitated by leveraging learned prior knowledge from
additional pre-collected offline data sources, such as web-scale video
datasets, in the era of experience we aim to design agents that can
continuously improve in an online manner from self-collected behaviors. In this
work we thus propose the Self-Adapting Improvement Loop (SAIL), where an
in-domain video model iteratively updates itself on self-produced trajectories,
collected through adaptation with an internet-scale pretrained video model, and
steadily improves its performance for a specified task of interest. We apply
SAIL to a diverse suite of MetaWorld tasks, as well as two manipulation tasks
on a real robot arm, and find that performance improvements continuously emerge
over multiple iterations for novel tasks initially unseen during original
in-domain video model training. Furthermore, we discover that SAIL is
surprisingly robust regarding if and how the self-collected experience is
filtered, and the quality of the initial in-domain demonstrations. Through
adaptation with summarized internet-scale data, and learning through online
experience, we thus demonstrate a way to iteratively bootstrap a
high-performance video model for solving novel robotic tasks through
self-improvement.

</details>


### [644] [DriveSuprim: Towards Precise Trajectory Selection for End-to-End Planning](https://arxiv.org/abs/2506.06659)
*Wenhao Yao,Zhenxin Li,Shiyi Lan,Zi Wang,Xinglong Sun,Jose M. Alvarez,Zuxuan Wu*

Main category: cs.RO

TL;DR: DriveSuprim通过渐进式候选过滤、旋转增强和自蒸馏框架，提升了自动驾驶轨迹选择的安全性和性能。


<details>
  <summary>Details</summary>
Motivation: 解决基于选择的轨迹预测方法在优化和区分安全关键差异上的挑战。

Method: 采用粗到细的候选过滤、旋转增强和自蒸馏框架。

Result: 在NAVSIM v1和v2上分别达到93.5% PDMS和87.1% EPDMS，表现优异。

Conclusion: DriveSuprim显著提升了自动驾驶的安全性和轨迹质量。

Abstract: In complex driving environments, autonomous vehicles must navigate safely.
Relying on a single predicted path, as in regression-based approaches, usually
does not explicitly assess the safety of the predicted trajectory.
Selection-based methods address this by generating and scoring multiple
trajectory candidates and predicting the safety score for each, but face
optimization challenges in precisely selecting the best option from thousands
of possibilities and distinguishing subtle but safety-critical differences,
especially in rare or underrepresented scenarios. We propose DriveSuprim to
overcome these challenges and advance the selection-based paradigm through a
coarse-to-fine paradigm for progressive candidate filtering, a rotation-based
augmentation method to improve robustness in out-of-distribution scenarios, and
a self-distillation framework to stabilize training. DriveSuprim achieves
state-of-the-art performance, reaching 93.5% PDMS in NAVSIM v1 and 87.1% EPDMS
in NAVSIM v2 without extra data, demonstrating superior safetycritical
capabilities, including collision avoidance and compliance with rules, while
maintaining high trajectory quality in various driving scenarios.

</details>


### [645] [Very Large-scale Multi-Robot Task Allocation in Challenging Environments via Robot Redistribution](https://arxiv.org/abs/2506.07293)
*Seabin Lee,Joonyeol Sim,Changjoo Nam*

Main category: cs.RO

TL;DR: 提出了一种考虑机器人路径的多机器人任务分配方法，避免冲突和死锁，以最小化任务完成时间。


<details>
  <summary>Details</summary>
Motivation: 在密集障碍物和狭窄通道的环境中，传统方法因机器人冲突和路径问题导致效率低下。

Method: 利用广义Voronoi图构建路线图，分区后通过先进先出的推拉机制重新分配机器人路径。

Result: 实验表明，该方法能处理数百机器人的密集环境，而其他方法无法在时限内求解。

Conclusion: 该方法在复杂环境中表现出高效性和可扩展性。

Abstract: We consider the Multi-Robot Task Allocation (MRTA) problem that aims to
optimize an assignment of multiple robots to multiple tasks in challenging
environments which are with densely populated obstacles and narrow passages. In
such environments, conventional methods optimizing the sum-of-cost are often
ineffective because the conflicts between robots incur additional costs (e.g.,
collision avoidance, waiting). Also, an allocation that does not incorporate
the actual robot paths could cause deadlocks, which significantly degrade the
collective performance of the robots.
  We propose a scalable MRTA method that considers the paths of the robots to
avoid collisions and deadlocks which result in a fast completion of all tasks
(i.e., minimizing the \textit{makespan}). To incorporate robot paths into task
allocation, the proposed method constructs a roadmap using a Generalized
Voronoi Diagram. The method partitions the roadmap into several components to
know how to redistribute robots to achieve all tasks with less conflicts
between the robots. In the redistribution process, robots are transferred to
their final destinations according to a push-pop mechanism with the first-in
first-out principle. From the extensive experiments, we show that our method
can handle instances with hundreds of robots in dense clutter while competitors
are unable to compute a solution within a time limit.

</details>


### [646] [Generalized Trajectory Scoring for End-to-end Multimodal Planning](https://arxiv.org/abs/2506.06664)
*Zhenxin Li,Wenhao Yao,Zi Wang,Xinglong Sun,Joshua Chen,Nadine Chang,Maying Shen,Zuxuan Wu,Shiyi Lan,Jose M. Alvarez*

Main category: cs.RO

TL;DR: GTRS是一个端到端多模态规划的统一框架，结合了粗粒度和细粒度轨迹评估，解决了静态和动态轨迹评分方法的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有的轨迹评分方法在泛化能力上存在显著不足，静态词汇表难以适应细粒度调整，而动态提案无法捕捉更广泛的轨迹分布。

Method: GTRS包含三个创新：扩散基轨迹生成器、词汇表泛化技术和传感器增强策略。

Result: GTRS在Navsim v2挑战赛中表现优异，接近依赖真实感知的特权方法。

Conclusion: GTRS通过结合粗粒度和细粒度评估，显著提升了轨迹评分的泛化能力和性能。

Abstract: End-to-end multi-modal planning is a promising paradigm in autonomous
driving, enabling decision-making with diverse trajectory candidates. A key
component is a robust trajectory scorer capable of selecting the optimal
trajectory from these candidates. While recent trajectory scorers focus on
scoring either large sets of static trajectories or small sets of dynamically
generated ones, both approaches face significant limitations in generalization.
Static vocabularies provide effective coarse discretization but struggle to
make fine-grained adaptation, while dynamic proposals offer detailed precision
but fail to capture broader trajectory distributions. To overcome these
challenges, we propose GTRS (Generalized Trajectory Scoring), a unified
framework for end-to-end multi-modal planning that combines coarse and
fine-grained trajectory evaluation. GTRS consists of three complementary
innovations: (1) a diffusion-based trajectory generator that produces diverse
fine-grained proposals; (2) a vocabulary generalization technique that trains a
scorer on super-dense trajectory sets with dropout regularization, enabling its
robust inference on smaller subsets; and (3) a sensor augmentation strategy
that enhances out-of-domain generalization while incorporating refinement
training for critical trajectory discrimination. As the winning solution of the
Navsim v2 Challenge, GTRS demonstrates superior performance even with
sub-optimal sensor inputs, approaching privileged methods that rely on
ground-truth perception. Code will be available at
https://github.com/NVlabs/GTRS.

</details>


### [647] [RoboCerebra: A Large-scale Benchmark for Long-horizon Robotic Manipulation Evaluation](https://arxiv.org/abs/2506.06677)
*Songhao Han,Boxiang Qiu,Yue Liao,Siyuan Huang,Chen Gao,Shuicheng Yan,Si Liu*

Main category: cs.RO

TL;DR: RoboCerebra是一个用于评估机器人长期操作中高级推理能力的基准，结合了高层次视觉语言模型（VLM）规划器和低层次视觉语言动作（VLA）控制器。


<details>
  <summary>Details</summary>
Motivation: 现有研究多关注反应性策略，未充分利用VLM在语义推理和长期规划中的潜力，RoboCerebra旨在填补这一空白。

Method: 通过GPT生成任务指令并分解为子任务序列，人类操作员在模拟中执行，构建高质量轨迹数据集。

Result: 相比现有基准，RoboCerebra具有更长的动作序列和更密集的标注，并评估了先进VLM作为System 2模块的性能。

Conclusion: RoboCerebra推动了更强大、更通用的机器人规划器的发展。

Abstract: Recent advances in vision-language models (VLMs) have enabled
instruction-conditioned robotic systems with improved generalization. However,
most existing work focuses on reactive System 1 policies, underutilizing VLMs'
strengths in semantic reasoning and long-horizon planning. These System 2
capabilities-characterized by deliberative, goal-directed thinking-remain under
explored due to the limited temporal scale and structural complexity of current
benchmarks. To address this gap, we introduce RoboCerebra, a benchmark for
evaluating high-level reasoning in long-horizon robotic manipulation.
RoboCerebra includes: (1) a large-scale simulation dataset with extended task
horizons and diverse subtask sequences in household environments; (2) a
hierarchical framework combining a high-level VLM planner with a low-level
vision-language-action (VLA) controller; and (3) an evaluation protocol
targeting planning, reflection, and memory through structured System 1-System 2
interaction. The dataset is constructed via a top-down pipeline, where GPT
generates task instructions and decomposes them into subtask sequences. Human
operators execute the subtasks in simulation, yielding high-quality
trajectories with dynamic object variations. Compared to prior benchmarks,
RoboCerebra features significantly longer action sequences and denser
annotations. We further benchmark state-of-the-art VLMs as System 2 modules and
analyze their performance across key cognitive dimensions, advancing the
development of more capable and generalizable robotic planners.

</details>


### [648] [RoboPARA: Dual-Arm Robot Planning with Parallel Allocation and Recomposition Across Tasks](https://arxiv.org/abs/2506.06683)
*Shiying Duan,Pei Ren,Nanxiang Jiang,Zhengping Che,Jian Tang,Yifan Sun,Zhaoxin Fan,Wenjun Wu*

Main category: cs.RO

TL;DR: RoboPARA是一个基于大型语言模型的双臂机器人任务并行规划框架，通过两阶段优化任务并行性，显著提升效率和可靠性。


<details>
  <summary>Details</summary>
Motivation: 现有方法在任务规划中未能充分利用双臂协作的并行潜力，限制了效率。

Method: RoboPARA采用两阶段方法：依赖图生成任务候选，图重遍历优化并行规划。

Result: 在X-DAPT数据集上，RoboPARA表现优于现有方法，尤其在复杂任务组合中。

Conclusion: RoboPARA为双臂机器人任务并行规划提供了高效可靠的解决方案。

Abstract: Dual-arm robots play a crucial role in improving efficiency and flexibility
in complex multitasking scenarios. While existing methods have achieved
promising results in task planning, they often fail to fully optimize task
parallelism, limiting the potential of dual-arm collaboration. To address this
issue, we propose RoboPARA, a novel large language model (LLM)-driven framework
for dual-arm task parallelism planning. RoboPARA employs a two-stage process:
(1) Dependency Graph-based Planning Candidates Generation, which constructs
directed acyclic graphs (DAGs) to model task dependencies and eliminate
redundancy, and (2) Graph Re-Traversal-based Dual-Arm Parallel Planning, which
optimizes DAG traversal to maximize parallelism while maintaining task
coherence. In addition, we introduce the Cross-Scenario Dual-Arm Parallel Task
dataset (X-DAPT dataset), the first dataset specifically designed to evaluate
dual-arm task parallelism across diverse scenarios and difficulty levels.
Extensive experiments on the X-DAPT dataset demonstrate that RoboPARA
significantly outperforms existing methods, achieving higher efficiency and
reliability, particularly in complex task combinations. The code and dataset
will be released upon acceptance.

</details>


### [649] [SpikePingpong: High-Frequency Spike Vision-based Robot Learning for Precise Striking in Table Tennis Game](https://arxiv.org/abs/2506.06690)
*Hao Wang,Chengkai Hou,Xianglong Li,Yankai Fu,Chenxuan Li,Ning Chen,Gaole Dai,Jiaming Liu,Tiejun Huang,Shanghang Zhang*

Main category: cs.RO

TL;DR: SpikePingpong系统结合脉冲视觉与模仿学习，实现高精度机器人乒乓球控制，成功率达91%（30cm目标区）和71%（20cm目标区），超越现有方法。


<details>
  <summary>Details</summary>
Motivation: 高速物体控制在机器人领域具有挑战性，乒乓球作为测试平台，需要快速拦截和精确轨迹调整。

Method: 系统包括SONIC（脉冲相机模块，毫米级接触预测）和IMPACT（战略规划模块），结合20kHz脉冲相机和实时神经网络。

Result: 实验显示，30cm目标区成功率91%，20cm目标区71%，分别超越现有方法38%和37%。

Conclusion: SpikePingpong为高速动态任务中的机器人控制提供了新视角。

Abstract: Learning to control high-speed objects in the real world remains a
challenging frontier in robotics. Table tennis serves as an ideal testbed for
this problem, demanding both rapid interception of fast-moving balls and
precise adjustment of their trajectories. This task presents two fundamental
challenges: it requires a high-precision vision system capable of accurately
predicting ball trajectories, and it necessitates intelligent strategic
planning to ensure precise ball placement to target regions. The dynamic nature
of table tennis, coupled with its real-time response requirements, makes it
particularly well-suited for advancing robotic control capabilities in
fast-paced, precision-critical domains. In this paper, we present
SpikePingpong, a novel system that integrates spike-based vision with imitation
learning for high-precision robotic table tennis. Our approach introduces two
key attempts that directly address the aforementioned challenges: SONIC, a
spike camera-based module that achieves millimeter-level precision in
ball-racket contact prediction by compensating for real-world uncertainties
such as air resistance and friction; and IMPACT, a strategic planning module
that enables accurate ball placement to targeted table regions. The system
harnesses a 20 kHz spike camera for high-temporal resolution ball tracking,
combined with efficient neural network models for real-time trajectory
correction and stroke planning. Experimental results demonstrate that
SpikePingpong achieves a remarkable 91% success rate for 30 cm accuracy target
area and 71% in the more challenging 20 cm accuracy task, surpassing previous
state-of-the-art approaches by 38% and 37% respectively. These significant
performance improvements enable the robust implementation of sophisticated
tactical gameplay strategies, providing a new research perspective for robotic
control in high-speed dynamic tasks.

</details>


### [650] [SARAL-Bot: Autonomous Robot for Strawberry Plant Care](https://arxiv.org/abs/2506.06798)
*Arif Ahmed,Ritvik Agarwal,Gaurav Srikar,Nathaniel Rose,Parikshit Maini*

Main category: cs.RO

TL;DR: Team SARAL开发了一款自主机器人，用于草莓种植中的导航、病叶检测与移除，旨在解决劳动力短缺问题并推动可持续农业。


<details>
  <summary>Details</summary>
Motivation: 草莓种植需要大量劳动力监控和维护植物健康，劳动力短缺和成本问题亟待解决。

Method: 开发了一款具备导航、病叶检测与移除功能的自主机器人，采用视觉技术评估植物健康。

Result: 该系统能够有效减少劳动力需求，降低成本，并支持可持续农业。

Conclusion: 该研究展示了机器人在草莓种植现代化和智能农业规模化中的潜力。

Abstract: Strawberry farming demands intensive labor for monitoring and maintaining
plant health. To address this, Team SARAL develops an autonomous robot for the
2024 ASABE Student Robotics Challenge, capable of navigation, unhealthy leaf
detection, and removal. The system addresses labor shortages, reduces costs,
and supports sustainable farming through vision-based plant assessment. This
work demonstrates the potential of robotics to modernize strawberry cultivation
and enable scalable, intelligent agricultural solutions.

</details>


### [651] [IRS: Instance-Level 3D Scene Graphs via Room Prior Guided LiDAR-Camera Fusion](https://arxiv.org/abs/2506.06804)
*Hongming Chen,Yiyang Lin,Ziliang Li,Biyu Ye,Yuying Zhang,Ximin Lyu*

Main category: cs.RO

TL;DR: 提出了一种基于LiDAR-相机融合的高效3D场景图构建框架，结合几何与语义信息，显著提升了构建速度和精度。


<details>
  <summary>Details</summary>
Motivation: 传统方法在开放环境中的适应性有限，视觉基础模型的出现为开放词汇识别和自然语言查询提供了新可能。

Method: 利用LiDAR的广视角和长距离感知能力获取几何先验，结合多级视觉基础模型提升语义提取精度，通过并行处理和几何语义融合增强鲁棒性。

Result: 实验表明，该方法在构建速度上提升了一个数量级，同时保持高语义精度，适用于语言引导的语义导航任务。

Conclusion: 该框架在模拟和真实环境中均表现出色，展示了其在机器人应用中的潜力。

Abstract: Indoor scene understanding remains a fundamental challenge in robotics, with
direct implications for downstream tasks such as navigation and manipulation.
Traditional approaches often rely on closed-set recognition or loop closure,
limiting their adaptability in open-world environments. With the advent of
visual foundation models (VFMs), open-vocabulary recognition and natural
language querying have become feasible, unlocking new possibilities for 3D
scene graph construction.
  In this paper, we propose a robust and efficient framework for instance-level
3D scene graph construction via LiDAR-camera fusion. Leveraging LiDAR's wide
field of view (FOV) and long-range sensing capabilities, we rapidly acquire
room-level geometric priors. Multi-level VFMs are employed to improve the
accuracy and consistency of semantic extraction. During instance fusion,
room-based segmentation enables parallel processing, while the integration of
geometric and semantic cues significantly enhances fusion accuracy and
robustness. Compared to state-of-the-art methods, our approach achieves up to
an order-of-magnitude improvement in construction speed while maintaining high
semantic precision.
  Extensive experiments in both simulated and real-world environments validate
the effectiveness of our approach. We further demonstrate its practical value
through a language-guided semantic navigation task, highlighting its potential
for real-world robotic applications.

</details>


### [652] [RF-Source Seeking with Obstacle Avoidance using Real-time Modified Artificial Potential Fields in Unknown Environments](https://arxiv.org/abs/2506.06811)
*Shahid Mohammad Mulla,Aryan Kanakapudi,Lakshmi Narasimhan,Anuj Tiwari*

Main category: cs.RO

TL;DR: 论文提出了一种实时轨迹规划方法，结合RF源追踪和自适应人工势场（APF），用于无人机在未知环境中的导航。


<details>
  <summary>Details</summary>
Motivation: 现有障碍物避障算法（如APF）无法适应不同障碍物配置的环境，且目标精确位置可能未知（如搜救任务）。

Method: 结合RF源追踪算法（提供目标方位角）和采样调整的APF，实时适应新障碍物配置。

Result: RF源追踪算法平均角度误差仅1.48度；导航算法成功率达46%，轨迹长度减少1.2%。

Conclusion: 该方法显著提升了无人机在未知环境中的导航性能，尤其在目标位置不明确时。

Abstract: Navigation of UAVs in unknown environments with obstacles is essential for
applications in disaster response and infrastructure monitoring. However,
existing obstacle avoidance algorithms, such as Artificial Potential Field
(APF) are unable to generalize across environments with different obstacle
configurations. Furthermore, the precise location of the final target may not
be available in applications such as search and rescue, in which case
approaches such as RF source seeking can be used to align towards the target
location. This paper proposes a real-time trajectory planning method, which
involves real-time adaptation of APF through a sampling-based approach. The
proposed approach utilizes only the bearing angle of the target without its
precise location, and adjusts the potential field parameters according to the
environment with new obstacle configurations in real time. The main
contributions of the article are i) an RF source seeking algorithm to provide a
bearing angle estimate using RF signal calculations based on antenna placement,
and ii) a modified APF for adaptable collision avoidance in changing
environments, which are evaluated separately in the simulation software Gazebo,
using ROS2 for communication. Simulation results show that the RF
source-seeking algorithm achieves high accuracy, with an average angular error
of just 1.48 degrees, and with this estimate, the proposed navigation algorithm
improves the success rate of reaching the target by 46% and reduces the
trajectory length by 1.2% compared to standard potential fields.

</details>


### [653] [Multimodal Spatial Language Maps for Robot Navigation and Manipulation](https://arxiv.org/abs/2506.06862)
*Chenguang Huang,Oier Mees,Andy Zeng,Wolfram Burgard*

Main category: cs.RO

TL;DR: 提出多模态空间语言地图（VLMaps和AVLMaps），结合预训练多模态特征与3D环境重建，实现自然语言命令到空间目标的翻译及多模态目标导航。


<details>
  <summary>Details</summary>
Motivation: 现有方法在环境映射、空间精度和多模态信息利用上存在不足，需要更高效的解决方案。

Method: 通过标准探索自主构建多模态空间语言地图，融合视觉、语言和音频信息，结合大语言模型（LLMs）实现目标导航。

Result: 实验表明，该方法在仿真和真实环境中实现零样本多模态目标导航，并在模糊场景中召回率提升50%。

Conclusion: 多模态空间语言地图为移动机器人和桌面操作器提供了基于视觉、音频和空间线索的导航与交互支持。

Abstract: Grounding language to a navigating agent's observations can leverage
pretrained multimodal foundation models to match perceptions to object or event
descriptions. However, previous approaches remain disconnected from environment
mapping, lack the spatial precision of geometric maps, or neglect additional
modality information beyond vision. To address this, we propose multimodal
spatial language maps as a spatial map representation that fuses pretrained
multimodal features with a 3D reconstruction of the environment. We build these
maps autonomously using standard exploration. We present two instances of our
maps, which are visual-language maps (VLMaps) and their extension to
audio-visual-language maps (AVLMaps) obtained by adding audio information. When
combined with large language models (LLMs), VLMaps can (i) translate natural
language commands into open-vocabulary spatial goals (e.g., "in between the
sofa and TV") directly localized in the map, and (ii) be shared across
different robot embodiments to generate tailored obstacle maps on demand.
Building upon the capabilities above, AVLMaps extend VLMaps by introducing a
unified 3D spatial representation integrating audio, visual, and language cues
through the fusion of features from pretrained multimodal foundation models.
This enables robots to ground multimodal goal queries (e.g., text, images, or
audio snippets) to spatial locations for navigation. Additionally, the
incorporation of diverse sensory inputs significantly enhances goal
disambiguation in ambiguous environments. Experiments in simulation and
real-world settings demonstrate that our multimodal spatial language maps
enable zero-shot spatial and multimodal goal navigation and improve recall by
50% in ambiguous scenarios. These capabilities extend to mobile robots and
tabletop manipulators, supporting navigation and interaction guided by visual,
audio, and spatial cues.

</details>


### [654] [Hierarchical Intention Tracking with Switching Trees for Real-Time Adaptation to Dynamic Human Intentions during Collaboration](https://arxiv.org/abs/2506.07004)
*Zhe Huang,Ye-Ji Mun,Fatemeh Cheraghi Pouria,Katherine Driggs-Campbell*

Main category: cs.RO

TL;DR: 提出了一种分层意图跟踪（HIT）算法，用于协作机器人实时动态跟踪人类的多层次意图，提升协作效率和用户体验。


<details>
  <summary>Details</summary>
Motivation: 人类在协作任务中的意图是多层次且动态变化的，机器人需实时准确跟踪这些意图以提升协作效果。

Method: HIT算法通过贝叶斯滤波、上下传播机制构建意图树，动态切换任务树以协调任务、交互和验证三个层次的意图。

Result: 实验表明，HIT系统在效率、工作负荷和用户舒适度上优于现有方案，同时增强用户信任并减少任务中断。

Conclusion: HIT算法有效解决了多层次意图跟踪问题，为协作机器人提供了更自然的交互体验。

Abstract: During collaborative tasks, human behavior is guided by multiple levels of
intentions that evolve over time, such as task sequence preferences and
interaction strategies. To adapt to these changing preferences and promptly
correct any inaccurate estimations, collaborative robots must accurately track
these dynamic human intentions in real time. We propose a Hierarchical
Intention Tracking (HIT) algorithm for collaborative robots to track dynamic
and hierarchical human intentions effectively in real time. HIT represents
human intentions as intention trees with arbitrary depth, and probabilistically
tracks human intentions by Bayesian filtering, upward measurement propagation,
and downward posterior propagation across all levels. We develop a HIT-based
robotic system that dynamically switches between Interaction-Task and
Verification-Task trees for a collaborative assembly task, allowing the robot
to effectively coordinate human intentions at three levels: task-level (subtask
goal locations), interaction-level (mode of engagement with the robot), and
verification-level (confirming or correcting intention recognition). Our user
study shows that our HIT-based collaborative robot system surpasses existing
collaborative robot solutions by achieving a balance between efficiency,
physical workload, and user comfort while ensuring safety and task completion.
Post-experiment surveys further reveal that the HIT-based system enhances the
user trust and minimizes interruptions to user's task flow through its
effective understanding of human intentions across multiple levels.

</details>


### [655] [CARoL: Context-aware Adaptation for Robot Learning](https://arxiv.org/abs/2506.07006)
*Zechen Hu,Tong Xu,Xuesu Xiao,Xuan Wang*

Main category: cs.RO

TL;DR: CARoL框架通过上下文感知和知识适配，提升机器人学习新任务的效率。


<details>
  <summary>Details</summary>
Motivation: 利用先验知识提升机器人学习效率，但需解决知识相关性和适应性整合问题。

Method: CARoL通过分析状态转移识别任务相似性，并优先适配相关知识。

Result: 在仿真和实际实验中，CARoL表现出更快收敛和更高奖励。

Conclusion: CARoL能高效学习新任务并适用于多种RL算法。

Abstract: Using Reinforcement Learning (RL) to learn new robotic tasks from scratch is
often inefficient. Leveraging prior knowledge has the potential to
significantly enhance learning efficiency, which, however, raises two critical
challenges: how to determine the relevancy of existing knowledge and how to
adaptively integrate them into learning a new task. In this paper, we propose
Context-aware Adaptation for Robot Learning (CARoL), a novel framework to
efficiently learn a similar but distinct new task from prior knowledge. CARoL
incorporates context awareness by analyzing state transitions in system
dynamics to identify similarities between the new task and prior knowledge. It
then utilizes these identified similarities to prioritize and adapt specific
knowledge pieces for the new task. Additionally, CARoL has a broad
applicability spanning policy-based, value-based, and actor-critic RL
algorithms. We validate the efficiency and generalizability of CARoL on both
simulated robotic platforms and physical ground vehicles. The simulations
include CarRacing and LunarLander environments, where CARoL demonstrates faster
convergence and higher rewards when learning policies for new tasks. In
real-world experiments, we show that CARoL enables a ground vehicle to quickly
and efficiently adapt policies learned in simulation to smoothly traverse
real-world off-road terrain.

</details>


### [656] [Prime the search: Using large language models for guiding geometric task and motion planning by warm-starting tree search](https://arxiv.org/abs/2506.07062)
*Dongryung Lee,Sejune Joo,Kimin Lee,Beomjoon Kim*

Main category: cs.RO

TL;DR: 论文提出了一种利用大型语言模型（LLM）结合蒙特卡洛树搜索（MCTS）解决几何任务与运动规划（G-TAMP）问题的方法，优于传统启发式和纯搜索算法。


<details>
  <summary>Details</summary>
Motivation: 传统G-TAMP方法依赖领域无关启发式或学习经验，计算资源或数据需求高，而人类常凭直觉解决此类问题。受此启发，作者利用LLM的常识知识指导任务规划。

Method: 设计基于谓词的提示编码几何信息，查询LLM生成任务计划，并用MCTS在混合动作空间中搜索可行参数。LLM用于预热MCTS，而非逐节点调用。

Result: 在六个G-TAMP问题上，该方法优于现有LLM规划器和纯搜索算法。

Conclusion: 结合LLM和MCTS的方法能高效解决G-TAMP问题，减少计算成本并提升性能。

Abstract: The problem of relocating a set of objects to designated areas amidst movable
obstacles can be framed as a Geometric Task and Motion Planning (G-TAMP)
problem, a subclass of task and motion planning (TAMP). Traditional approaches
to G-TAMP have relied either on domain-independent heuristics or on learning
from planning experience to guide the search, both of which typically demand
significant computational resources or data. In contrast, humans often use
common sense to intuitively decide which objects to manipulate in G-TAMP
problems. Inspired by this, we propose leveraging Large Language Models (LLMs),
which have common sense knowledge acquired from internet-scale data, to guide
task planning in G-TAMP problems. To enable LLMs to perform geometric
reasoning, we design a predicate-based prompt that encodes geometric
information derived from a motion planning algorithm. We then query the LLM to
generate a task plan, which is then used to search for a feasible set of
continuous parameters. Since LLMs are prone to mistakes, instead of committing
to LLM's outputs, we extend Monte Carlo Tree Search (MCTS) to a hybrid action
space and use the LLM to guide the search. Unlike the previous approach that
calls an LLM at every node and incurs high computational costs, we use it to
warm-start the MCTS with the nodes explored in completing the LLM's task plan.
On six different G-TAMP problems, we show our method outperforms previous LLM
planners and pure search algorithms. Code can be found at:
https://github.com/iMSquared/prime-the-search

</details>


### [657] [Robotic Policy Learning via Human-assisted Action Preference Optimization](https://arxiv.org/abs/2506.07127)
*Wenke xia,Yichu Yang,Hongtao Wu,Xiao Ma,Tao Kong,Di Hu*

Main category: cs.RO

TL;DR: 论文提出了一种名为HAPO的人辅助动作偏好优化方法，旨在通过偏好对齐纠正VLA模型的部署失败并促进有效适应。


<details>
  <summary>Details</summary>
Motivation: 现有的VLA模型依赖专家演示，缺乏从失败中学习和纠正的能力，限制了其在真实场景中的部署。

Method: HAPO方法结合人机协作框架收集干预轨迹，并通过自适应重加权算法优化动作偏好，解决不可逆交互和标记概率不匹配问题。

Result: 实验证明，HAPO在仿真和真实场景中表现出优越的泛化能力和鲁棒性。

Conclusion: HAPO方法有效提升了VLA模型的可靠性和从失败中学习的能力。

Abstract: Establishing a reliable and iteratively refined robotic system is essential
for deploying real-world applications. While Vision-Language-Action (VLA)
models are widely recognized as the foundation model for such robotic
deployment, their dependence on expert demonstrations hinders the crucial
capabilities of correction and learning from failures. To mitigate this
limitation, we introduce a Human-assisted Action Preference Optimization method
named HAPO, designed to correct deployment failures and foster effective
adaptation through preference alignment for VLA models. This method begins with
a human-robot collaboration framework for reliable failure correction and
interaction trajectory collection through human intervention. These
human-intervention trajectories are further employed within the action
preference optimization process, facilitating VLA models to mitigate failure
action occurrences while enhancing corrective action adaptation. Specifically,
we propose an adaptive reweighting algorithm to address the issues of
irreversible interactions and token probability mismatch when introducing
preference optimization into VLA models, facilitating model learning from
binary desirability signals derived from interactions. Through combining these
modules, our human-assisted action preference optimization method ensures
reliable deployment and effective learning from failure for VLA models. The
experiments conducted in simulation and real-world scenarios prove superior
generalization and robustness of our framework across a variety of manipulation
tasks.

</details>


### [658] [Improving Traffic Signal Data Quality for the Waymo Open Motion Dataset](https://arxiv.org/abs/2506.07150)
*Xintao Yan,Erdao Liang,Jiawei Wang,Haojie Zhu,Henry X. Liu*

Main category: cs.RO

TL;DR: 该论文提出了一种自动化方法，利用车辆轨迹数据和交通领域知识，修复Waymo Open Motion Dataset中缺失或不准确的交通信号状态，显著提升了数据集质量。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶车辆数据集在AI和交通工程研究中具有重要价值，但交通信号状态的缺失或不准确会影响数据可靠性和模型性能。

Method: 结合车辆轨迹数据和交通领域知识，设计了一种鲁棒且灵活的方法，用于修复和补全交通信号信息。

Result: 在360,000多个交通信号相关场景中，成功补全了71.7%的缺失信号状态，并将红灯违规率从15.7%降至2.9%。

Conclusion: 该方法显著提升了自动驾驶数据集的质量，为AI和自动驾驶研究提供了更可靠的数据支持，相关代码和数据已开源。

Abstract: Datasets pertaining to autonomous vehicles (AVs) hold significant promise for
a range of research fields, including artificial intelligence (AI), autonomous
driving, and transportation engineering. Nonetheless, these datasets often
encounter challenges related to the states of traffic signals, such as missing
or inaccurate data. Such issues can compromise the reliability of the datasets
and adversely affect the performance of models developed using them. This
research introduces a fully automated approach designed to tackle these issues
by utilizing available vehicle trajectory data alongside knowledge from the
transportation domain to effectively impute and rectify traffic signal
information within the Waymo Open Motion Dataset (WOMD). The proposed method is
robust and flexible, capable of handling diverse intersection geometries and
traffic signal configurations in real-world scenarios. Comprehensive
validations have been conducted on the entire WOMD, focusing on over 360,000
relevant scenarios involving traffic signals, out of a total of 530,000
real-world driving scenarios. In the original dataset, 71.7% of traffic signal
states are either missing or unknown, all of which were successfully imputed by
our proposed method. Furthermore, in the absence of ground-truth signal states,
the accuracy of our approach is evaluated based on the rate of red-light
violations among vehicle trajectories. Results show that our method reduces the
estimated red-light running rate from 15.7% in the original data to 2.9%,
thereby demonstrating its efficacy in rectifying data inaccuracies. This paper
significantly enhances the quality of AV datasets, contributing to the wider AI
and AV research communities and benefiting various downstream applications. The
code and improved traffic signal data are open-sourced at
https://github.com/michigan-traffic-lab/WOMD-Traffic-Signal-Data-Improvement

</details>


### [659] [MorphoCopter: Design, Modeling, and Control of a New Transformable Quad-Bi Copter](https://arxiv.org/abs/2506.07204)
*Harsh Modi,Hao Su,Xiao Liang,Minghui Zheng*

Main category: cs.RO

TL;DR: 论文介绍了一种新型变形四旋翼无人机MorphoCopter，通过单旋转关节实现快速变形为超窄形态，解决了传统四旋翼硬件配置固定的局限性。


<details>
  <summary>Details</summary>
Motivation: 传统四旋翼的硬件配置限制了其在某些环境中的能力，而现有变形设计往往牺牲了可控性或依赖复杂多关节系统。

Method: 设计了一种单旋转关节的变形机制，开发了基于惯性和控制动作的自适应控制系统。

Result: 原型机宽度可从447毫米快速缩减至138毫米（减少近70%），并通过仿真和飞行实验验证了性能。

Conclusion: MorphoCopter通过创新的变形设计和控制系统，显著提升了四旋翼在复杂环境中的适应性和性能。

Abstract: This paper presents a novel morphing quadrotor, named MorphoCopter, covering
its design, modeling, control, and experimental tests. It features a unique
single rotary joint that enables rapid transformation into an ultra-narrow
profile. Although quadrotors have seen widespread adoption in applications such
as cinematography, agriculture, and disaster management with increasingly
sophisticated control systems, their hardware configurations have remained
largely unchanged, limiting their capabilities in certain environments. Our
design addresses this by enabling the hardware configuration to change on the
fly when required. In standard flight mode, the MorphoCopter adopts an X
configuration, functioning as a traditional quadcopter, but can quickly fold
into a stacked bicopters arrangement or any configuration in between. Existing
morphing designs often sacrifice controllability in compact configurations or
rely on complex multi-joint systems. Moreover, our design achieves a greater
width reduction than any existing solution. We develop a new inertia and
control-action aware adaptive control system that maintains robust performance
across all rotary-joint configurations. The prototype can reduce its width from
447 mm to 138 mm (nearly 70\% reduction) in just a few seconds. We validated
the MorphoCopter through rigorous simulations and a comprehensive series of
flight experiments, including robustness tests, trajectory tracking, and
narrow-gap passing tests.

</details>


### [660] [Machine Learning-Based Self-Localization Using Internal Sensors for Automating Bulldozers](https://arxiv.org/abs/2506.07271)
*Hikaru Sawafuji,Ryota Ozaki,Takuto Motomura,Toyohisa Matsuda,Masanori Tojima,Kento Uchida,Shinichi Shirakawa*

Main category: cs.RO

TL;DR: 提出了一种基于机器学习的推土机自定位方法，通过内部传感器估计局部速度并结合扩展卡尔曼滤波器（EKF）进行全局定位，实验表明该方法能有效减少位置误差。


<details>
  <summary>Details</summary>
Motivation: RTK-GNSS信号在采矿环境中可能丢失，需要不依赖RTK-GNSS的自定位方法。

Method: 使用机器学习模型从内部传感器估计局部速度，并通过EKF进行全局定位。

Result: 实验表明，该方法在滑移等情况下能有效抑制位置误差累积，且推土机专用传感器有助于提高定位精度。

Conclusion: 该方法为推土机提供了一种不依赖RTK-GNSS的高效自定位解决方案。

Abstract: Self-localization is an important technology for automating bulldozers.
Conventional bulldozer self-localization systems rely on RTK-GNSS (Real Time
Kinematic-Global Navigation Satellite Systems). However, RTK-GNSS signals are
sometimes lost in certain mining conditions. Therefore, self-localization
methods that do not depend on RTK-GNSS are required. In this paper, we propose
a machine learning-based self-localization method for bulldozers. The proposed
method consists of two steps: estimating local velocities using a machine
learning model from internal sensors, and incorporating these estimates into an
Extended Kalman Filter (EKF) for global localization. We also created a novel
dataset for bulldozer odometry and conducted experiments across various driving
scenarios, including slalom, excavation, and driving on slopes. The result
demonstrated that the proposed self-localization method suppressed the
accumulation of position errors compared to kinematics-based methods,
especially when slip occurred. Furthermore, this study showed that
bulldozer-specific sensors, such as blade position sensors and hydraulic
pressure sensors, contributed to improving self-localization accuracy.

</details>


### [661] [Model Analysis And Design Of Ellipse Based Segmented Varying Curved Foot For Biped Robot Walking](https://arxiv.org/abs/2506.07283)
*Boyang Chen,Xizhe Zang,Chao Song,Yue Zhang,Jie Zhao*

Main category: cs.RO

TL;DR: 本文提出了一种基于椭圆的分段变曲率（ESVC）足部设计，用于提高双足机器人的步态能效，并通过实验验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 受人类足部分段曲率滚动形状的启发，ESVC足部旨在提高步态能效，同时保持足部位置控制的解析可操作性。

Method: 通过椭圆段的空间变换建立完整的接触模型，采用非线性规划优化椭圆参数，并引入误差补偿方法。将ESVC足部与混合线性倒立摆模型控制器结合，进行仿真和物理实验验证。

Result: 实验表明，ESVC足部在多种步态任务中显著降低能耗，横向行走能效最高提升18.52%。

Conclusion: ESVC足部为双足行走提供了实用且高效的解决方案，并为未来数据驱动的足部形状优化奠定了基础。

Abstract: This paper presents the modeling, design, and experimental validation of an
Ellipse-based Segmented Varying Curvature (ESVC) foot for bipedal robots.
Inspired by the segmented curvature rollover shape of human feet, the ESVC foot
aims to enhance gait energy efficiency while maintaining analytical
tractability for foot location based controller. First, we derive a complete
analytical contact model for the ESVC foot by formulating spatial
transformations of elliptical segments only using elementary functions. Then a
nonlinear programming approach is engaged to determine optimal elliptical
parameters of hind foot and fore foot based on a known mid-foot. An error
compensation method is introduced to address approximation inaccuracies in
rollover length calculation. The proposed ESVC foot is then integrated with a
Hybrid Linear Inverted Pendulum model-based walking controller and validated
through both simulation and physical experiments on the TT II biped robot.
Experimental results across marking time, sagittal, and lateral walking tasks
show that the ESVC foot consistently reduces energy consumption compared to
line, and flat feet, with up to 18.52\% improvement in lateral walking. These
findings demonstrate that the ESVC foot provides a practical and
energy-efficient alternative for real-world bipedal locomotion. The proposed
design methodology also lays a foundation for data-driven foot shape
optimization in future research.

</details>


### [662] [BR-MPPI: Barrier Rate guided MPPI for Enforcing Multiple Inequality Constraints with Learned Signed Distance Field](https://arxiv.org/abs/2506.07325)
*Hardik Parwana,Taekyung Kim,Kehan Long,Bardh Hoxha,Hideki Okamoto,Georgios Fainekos,Dimitra Panagou*

Main category: cs.RO

TL;DR: 将MPPI与CBF结合，通过CBF条件指导MPPI的采样，增强安全性。


<details>
  <summary>Details</summary>
Motivation: 解决MPPI在无约束最优控制问题中缺乏严格约束的问题，同时提升安全性。

Method: 将CBF条件转化为等式约束，通过参数化线性类K函数和状态变换实现。

Result: 仿真和实验表明，该方法比传统MPPI采样效率更高，能更接近安全边界操作。

Conclusion: 提出的方法有效结合了MPPI和CBF，提升了控制的安全性和效率。

Abstract: Model Predictive Path Integral (MPPI) controller is used to solve
unconstrained optimal control problems and Control Barrier Function (CBF) is a
tool to impose strict inequality constraints, a.k.a, barrier constraints. In
this work, we propose an integration of these two methods that employ CBF-like
conditions to guide the control sampling procedure of MPPI. CBFs provide an
inequality constraint restricting the rate of change of barrier functions by a
classK function of the barrier itself. We instead impose the CBF condition as
an equality constraint by choosing a parametric linear classK function and
treating this parameter as a state in an augmented system. The time derivative
of this parameter acts as an additional control input that is designed by MPPI.
A cost function is further designed to reignite Nagumo's theorem at the
boundary of the safe set by promoting specific values of classK parameter to
enforce safety. Our problem formulation results in an MPPI subject to multiple
state and control-dependent equality constraints which are non-trivial to
satisfy with randomly sampled control inputs. We therefore also introduce state
transformations and control projection operations, inspired by the literature
on path planning for manifolds, to resolve the aforementioned issue. We show
empirically through simulations and experiments on quadrotor that our proposed
algorithm exhibits better sampled efficiency and enhanced capability to operate
closer to the safe set boundary over vanilla MPPI.

</details>


### [663] [Real-Time Execution of Action Chunking Flow Policies](https://arxiv.org/abs/2506.07339)
*Kevin Black,Manuel Y. Galliker,Sergey Levine*

Main category: cs.RO

TL;DR: 提出了一种实时分块（RTC）算法，解决现有视觉语言动作模型（VLA）的高延迟问题，实现平滑异步执行动作分块策略。


<details>
  <summary>Details</summary>
Motivation: 现代AI系统在物理世界交互中需要实时性能，但现有通用模型的高延迟限制了其应用，尤其是在高频控制任务中。

Method: RTC算法无需重新训练，适用于任何基于扩散或流的VLA模型，通过在执行当前动作块时生成下一个动作块，实现平滑过渡。

Result: 在12个高动态任务和6个真实世界双手机器人任务中，RTC显著提高了任务吞吐量和成功率，尤其在存在延迟的情况下表现优异。

Conclusion: RTC是一种高效、鲁棒的实时动作分块方法，显著提升了AI系统在实时交互任务中的性能。

Abstract: Modern AI systems, especially those interacting with the physical world,
increasingly require real-time performance. However, the high latency of
state-of-the-art generalist models, including recent vision-language action
models (VLAs), poses a significant challenge. While action chunking has enabled
temporal consistency in high-frequency control tasks, it does not fully address
the latency problem, leading to pauses or out-of-distribution jerky movements
at chunk boundaries. This paper presents a novel inference-time algorithm that
enables smooth asynchronous execution of action chunking policies. Our method,
real-time chunking (RTC), is applicable to any diffusion- or flow-based VLA out
of the box with no re-training. It generates the next action chunk while
executing the current one, "freezing" actions guaranteed to execute and
"inpainting" the rest. To test RTC, we introduce a new benchmark of 12 highly
dynamic tasks in the Kinetix simulator, as well as evaluate 6 challenging
real-world bimanual manipulation tasks. Results demonstrate that RTC is fast,
performant, and uniquely robust to inference delay, significantly improving
task throughput and enabling high success rates in precise tasks
$\unicode{x2013}$ such as lighting a match $\unicode{x2013}$ even in the
presence of significant latency. See
https://pi.website/research/real_time_chunking for videos.

</details>


### [664] [Reproducibility in the Control of Autonomous Mobility-on-Demand Systems](https://arxiv.org/abs/2506.07345)
*Xinling Li,Meshal Alharbi,Daniele Gammelli,James Harrison,Filipe Rodrigues,Maximilian Schiffer,Marco Pavone,Emilio Frazzoli,Jinhua Zhao,Gioele Zardini*

Main category: cs.RO

TL;DR: 本文探讨了自动驾驶按需出行（AMoD）研究中可重复性的问题，提出了系统性评估和改进可重复性的框架和指南。


<details>
  <summary>Details</summary>
Motivation: AMoD领域的快速发展导致标准化评估和报告实践的缺失，影响了研究的可重复性和科学进展。

Method: 通过分析研究流程中的关键组件（如系统建模、控制问题、仿真设计等），识别不可重复性的常见来源，并提出结构化框架和检查表。

Result: 提出了具体的指南和“可重复性检查表”，以支持未来研究实现可复制、可比较和可扩展的结果。

Conclusion: 本文旨在为智能出行系统的设计和部署奠定更透明和可重复的研究文化基础，其原则适用于更广泛的网络物理系统。

Abstract: Autonomous Mobility-on-Demand (AMoD) systems, powered by advances in
robotics, control, and Machine Learning (ML), offer a promising paradigm for
future urban transportation. AMoD offers fast and personalized travel services
by leveraging centralized control of autonomous vehicle fleets to optimize
operations and enhance service performance. However, the rapid growth of this
field has outpaced the development of standardized practices for evaluating and
reporting results, leading to significant challenges in reproducibility. As
AMoD control algorithms become increasingly complex and data-driven, a lack of
transparency in modeling assumptions, experimental setups, and algorithmic
implementation hinders scientific progress and undermines confidence in the
results. This paper presents a systematic study of reproducibility in AMoD
research. We identify key components across the research pipeline, spanning
system modeling, control problems, simulation design, algorithm specification,
and evaluation, and analyze common sources of irreproducibility. We survey
prevalent practices in the literature, highlight gaps, and propose a structured
framework to assess and improve reproducibility. Specifically, concrete
guidelines are offered, along with a "reproducibility checklist", to support
future work in achieving replicable, comparable, and extensible results. While
focused on AMoD, the principles and practices we advocate generalize to a
broader class of cyber-physical systems that rely on networked autonomy and
data-driven control. This work aims to lay the foundation for a more
transparent and reproducible research culture in the design and deployment of
intelligent mobility systems.

</details>


### [665] [UruBots Autonomous Cars Challenge Pro Team Description Paper for FIRA 2025](https://arxiv.org/abs/2506.07348)
*Pablo Moraes,Mónica Rodríguez,Sebastian Barcelona,Angel Da Silva,Santiago Fernandez,Hiago Sodre,Igor Nunes,Bruna Guterres,Ricardo Grando*

Main category: cs.RO

TL;DR: UruBots团队开发了一款用于2025 FIRA自动驾驶汽车挑战赛的自主小车，采用深度学习模型实现实时导航。


<details>
  <summary>Details</summary>
Motivation: 为参加2025 FIRA自动驾驶汽车挑战赛，设计一款能够自主导航的小型电动车。

Method: 结合机械电子组件和机器学习算法，使用CNN处理摄像头图像，控制车辆的转向和油门。

Result: 车辆在30秒内完成赛道，速度约0.4米/秒，并能避开障碍物。

Conclusion: 项目成功展示了小型电动车在自主导航中的潜力。

Abstract: This paper describes the development of an autonomous car by the UruBots team
for the 2025 FIRA Autonomous Cars Challenge (Pro). The project involves
constructing a compact electric vehicle, approximately the size of an RC car,
capable of autonomous navigation through different tracks. The design
incorporates mechanical and electronic components and machine learning
algorithms that enable the vehicle to make real-time navigation decisions based
on visual input from a camera. We use deep learning models to process camera
images and control vehicle movements. Using a dataset of over ten thousand
images, we trained a Convolutional Neural Network (CNN) to drive the vehicle
effectively, through two outputs, steering and throttle. The car completed the
track in under 30 seconds, achieving a pace of approximately 0.4 meters per
second while avoiding obstacles.

</details>


### [666] [MapBERT: Bitwise Masked Modeling for Real-Time Semantic Mapping Generation](https://arxiv.org/abs/2506.07350)
*Yijie Deng,Shuaihang Yuan,Congcong Wen,Hao Huang,Anthony Tzes,Geeta Chandra Raju Bethala,Yi Fang*

Main category: cs.RO

TL;DR: MapBERT是一个新框架，利用BitVAE和掩码变换器生成未观测区域的语义地图，通过对象感知掩码策略提升推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有方法难以实时生成未观测区域且泛化性差，因此提出MapBERT以有效建模未观测空间的分布。

Method: 使用BitVAE编码语义地图为紧凑比特令牌，结合掩码变换器推断缺失区域；提出对象感知掩码策略增强推理。

Result: 在Gibson基准测试中，MapBERT实现了最先进的语义地图生成，平衡了计算效率和未观测区域的准确重建。

Conclusion: MapBERT通过比特编码和对象感知策略，显著提升了室内语义分布的建模能力，适用于实际机器人任务。

Abstract: Spatial awareness is a critical capability for embodied agents, as it enables
them to anticipate and reason about unobserved regions. The primary challenge
arises from learning the distribution of indoor semantics, complicated by
sparse, imbalanced object categories and diverse spatial scales. Existing
methods struggle to robustly generate unobserved areas in real time and do not
generalize well to new environments. To this end, we propose \textbf{MapBERT},
a novel framework designed to effectively model the distribution of unseen
spaces. Motivated by the observation that the one-hot encoding of semantic maps
aligns naturally with the binary structure of bit encoding, we, for the first
time, leverage a lookup-free BitVAE to encode semantic maps into compact
bitwise tokens. Building on this, a masked transformer is employed to infer
missing regions and generate complete semantic maps from limited observations.
To enhance object-centric reasoning, we propose an object-aware masking
strategy that masks entire object categories concurrently and pairs them with
learnable embeddings, capturing implicit relationships between object
embeddings and spatial tokens. By learning these relationships, the model more
effectively captures indoor semantic distributions crucial for practical
robotic tasks. Experiments on Gibson benchmarks show that MapBERT achieves
state-of-the-art semantic map generation, balancing computational efficiency
with accurate reconstruction of unobserved regions.

</details>


### [667] [Language-Grounded Hierarchical Planning and Execution with Multi-Robot 3D Scene Graphs](https://arxiv.org/abs/2506.07454)
*Jared Strader,Aaron Ray,Jacob Arkin,Mason B. Peterson,Yun Chang,Nathan Hughes,Christopher Bradley,Yi Xuan Jia,Carlos Nieto-Granda,Rajat Talak,Chuchu Fan,Luca Carlone,Jonathan P. How,Nicholas Roy*

Main category: cs.RO

TL;DR: 提出了一种多机器人系统，通过3D场景图整合建图、定位和任务与运动规划（TAMP），以执行自然语言表达的复杂指令。


<details>
  <summary>Details</summary>
Motivation: 解决多机器人在复杂环境中执行自然语言指令的挑战。

Method: 构建共享的3D场景图，结合开放集物体地图，支持实时、视角不变的重新定位和规划。利用大型语言模型（LLM）将操作意图转化为PDDL目标。

Result: 系统在大型户外环境中进行了实验评估，验证了其性能。

Conclusion: 该系统能够有效支持多机器人团队在复杂环境中执行任务。

Abstract: In this paper, we introduce a multi-robot system that integrates mapping,
localization, and task and motion planning (TAMP) enabled by 3D scene graphs to
execute complex instructions expressed in natural language. Our system builds a
shared 3D scene graph incorporating an open-set object-based map, which is
leveraged for multi-robot 3D scene graph fusion. This representation supports
real-time, view-invariant relocalization (via the object-based map) and
planning (via the 3D scene graph), allowing a team of robots to reason about
their surroundings and execute complex tasks. Additionally, we introduce a
planning approach that translates operator intent into Planning Domain
Definition Language (PDDL) goals using a Large Language Model (LLM) by
leveraging context from the shared 3D scene graph and robot capabilities. We
provide an experimental assessment of the performance of our system on
real-world tasks in large-scale, outdoor environments.

</details>


### [668] [RAPID Hand: A Robust, Affordable, Perception-Integrated, Dexterous Manipulation Platform for Generalist Robot Autonomy](https://arxiv.org/abs/2506.07490)
*Zhaoliang Wan,Zetong Bi,Zida Zhou,Hao Ren,Yiming Zeng,Yihan Li,Lu Qi,Xu Yang,Ming-Hsuan Yang,Hui Cheng*

Main category: cs.RO

TL;DR: 论文提出RAPID Hand平台，通过硬件与软件协同优化，解决低成本高灵巧机器人数据收集的难题。


<details>
  <summary>Details</summary>
Motivation: 解决低成本高灵巧机器人平台稀缺的问题，以支持通用机器人自主性的数据收集。

Method: 设计紧凑的20自由度手、稳定感知框架和高自由度遥操作接口，结合硬件级感知和定制电子设备。

Result: 平台在硬件、感知和遥操作方面表现优异，数据训练结果优于现有工作。

Conclusion: RAPID Hand平台低成本、高性能，且将公开以促进复现和采用。

Abstract: This paper addresses the scarcity of low-cost but high-dexterity platforms
for collecting real-world multi-fingered robot manipulation data towards
generalist robot autonomy. To achieve it, we propose the RAPID Hand, a
co-optimized hardware and software platform where the compact 20-DoF hand,
robust whole-hand perception, and high-DoF teleoperation interface are jointly
designed. Specifically, RAPID Hand adopts a compact and practical hand ontology
and a hardware-level perception framework that stably integrates wrist-mounted
vision, fingertip tactile sensing, and proprioception with sub-7 ms latency and
spatial alignment. Collecting high-quality demonstrations on high-DoF hands is
challenging, as existing teleoperation methods struggle with precision and
stability on complex multi-fingered systems. We address this by co-optimizing
hand design, perception integration, and teleoperation interface through a
universal actuation scheme, custom perception electronics, and two retargeting
constraints. We evaluate the platform's hardware, perception, and teleoperation
interface. Training a diffusion policy on collected data shows superior
performance over prior works, validating the system's capability for reliable,
high-quality data collection. The platform is constructed from low-cost and
off-the-shelf components and will be made public to ensure reproducibility and
ease of adoption.

</details>


### [669] [Taking Flight with Dialogue: Enabling Natural Language Control for PX4-based Drone Agent](https://arxiv.org/abs/2506.07509)
*Shoon Kit Lim,Melissa Jia Ying Chong,Jing Huey Khor,Ting Yang Ling*

Main category: cs.RO

TL;DR: 本文提出了一种开源框架，用于无人机的自然语言控制，整合了PX4飞行控制、ROS 2中间件和本地托管模型，并评估了多种LLM和VLM的性能。


<details>
  <summary>Details</summary>
Motivation: 当前空中机器人的研究相对不足，且现有的无人机多模态视觉语言系统依赖闭源模型，限制了资源的可及性。本文旨在通过开源框架实现无人机自然语言控制的民主化。

Method: 提出一个开源框架，整合PX4飞行控制、ROS 2中间件和本地托管模型（Ollama），并在仿真和自定义四旋翼平台上评估了四种LLM和三种VLM的性能。

Result: 在仿真和实际平台上评估了不同LLM和VLM的性能，为无人机自然语言控制提供了基准数据。

Conclusion: 该开源框架为无人机自然语言控制提供了可行方案，推动了空中机器人领域的研究和应用。

Abstract: Recent advances in agentic and physical artificial intelligence (AI) have
largely focused on ground-based platforms such as humanoid and wheeled robots,
leaving aerial robots relatively underexplored. Meanwhile, state-of-the-art
unmanned aerial vehicle (UAV) multimodal vision-language systems typically rely
on closed-source models accessible only to well-resourced organizations. To
democratize natural language control of autonomous drones, we present an
open-source agentic framework that integrates PX4-based flight control, Robot
Operating System 2 (ROS 2) middleware, and locally hosted models using Ollama.
We evaluate performance both in simulation and on a custom quadcopter platform,
benchmarking four large language model (LLM) families for command generation
and three vision-language model (VLM) families for scene understanding.

</details>


### [670] [BitVLA: 1-bit Vision-Language-Action Models for Robotics Manipulation](https://arxiv.org/abs/2506.07530)
*Hongyu Wang,Chuyan Xiong,Ruiping Wang,Xilin Chen*

Main category: cs.RO

TL;DR: BitVLA是首个用于机器人操作的1-bit VLA模型，通过三元参数和1.58-bit视觉编码器压缩技术，显著减少内存占用，性能接近4-bit量化模型。


<details>
  <summary>Details</summary>
Motivation: 解决VLA模型在资源受限机器人系统上部署时因模型规模大导致的内存挑战。

Method: 提出BitVLA模型，采用三元参数和蒸馏感知训练策略压缩视觉编码器。

Result: 在LIBERO基准测试中性能接近OpenVLA-OFT（4-bit量化），内存占用仅29.8%。

Conclusion: BitVLA在内存受限的边缘设备上具有部署潜力，代码和模型已开源。

Abstract: Vision-Language-Action (VLA) models have shown impressive capabilities across
a wide range of robotics manipulation tasks. However, their growing model size
poses significant challenges for deployment on resource-constrained robotic
systems. While 1-bit pretraining has proven effective for enhancing the
inference efficiency of large language models with minimal performance loss,
its application to VLA models remains underexplored. In this work, we present
BitVLA, the first 1-bit VLA model for robotics manipulation, in which every
parameter is ternary, i.e., {-1, 0, 1}. To further reduce the memory footprint
of the vision encoder, we propose the distillation-aware training strategy that
compresses the full-precision encoder to 1.58-bit weights. During this process,
a full-precision encoder serves as a teacher model to better align latent
representations. Despite the lack of large-scale robotics pretraining, BitVLA
achieves performance comparable to the state-of-the-art model OpenVLA-OFT with
4-bit post-training quantization on the LIBERO benchmark, while consuming only
29.8% of the memory. These results highlight BitVLA's promise for deployment on
memory-constrained edge devices. We release the code and model weights in
https://github.com/ustcwhy/BitVLA.

</details>


### [671] [Fractional Collisions: A Framework for Risk Estimation of Counterfactual Conflicts using Autonomous Driving Behavior Simulations](https://arxiv.org/abs/2506.07540)
*Sreeja Roy-Singh,Sarvesh Kolekar,Daniel P. Bonny,Kyle Foss*

Main category: cs.RO

TL;DR: 提出一种基于传感器数据估计碰撞风险的方法，通过模拟场景评估冲突类型、角色分配和反应点，结合概率模型预测碰撞严重性，验证了方法的准确性并展示了其在自动驾驶系统（ADS）中的应用效果。


<details>
  <summary>Details</summary>
Motivation: 为了解决自动驾驶系统中碰撞风险的量化评估问题，通过模拟场景和概率模型提供更精确的风险预测。

Method: 利用传感器数据构建模拟场景，识别冲突类型和角色，建模人类行为期望为概率轨迹，结合速度差和碰撞模型预测碰撞严重性。

Result: 方法在合成环境中验证，预测误差在1%以内；ADS测试显示碰撞风险显著降低（自然碰撞减少4倍，分数碰撞风险降低62%）。

Conclusion: 该方法能有效评估和降低碰撞风险，适用于自动驾驶系统的开发和测试。

Abstract: We present a methodology for estimating collision risk from counterfactual
simulated scenarios built on sensor data from automated driving systems (ADS)
or naturalistic driving databases. Two-agent conflicts are assessed by
detecting and classifying conflict type, identifying the agents' roles
(initiator or responder), identifying the point of reaction of the responder,
and modeling their human behavioral expectations as probabilistic
counterfactual trajectories. The states are used to compute velocity
differentials at collision, which when combined with crash models, estimates
severity of loss in terms of probabilistic injury or property damage,
henceforth called fractional collisions. The probabilistic models may also be
extended to include other uncertainties associated with the simulation,
features, and agents. We verify the effectiveness of the methodology in a
synthetic simulation environment using reconstructed trajectories from 300+
collision and near-collision scenes sourced from VTTI's SHRP2 database and
Nexar dashboard camera data. Our methodology predicted fractional collisions
within 1% of ground truth collisions. We then evaluate agent-initiated
collision risk of an arbitrary ADS software release by replacing the
naturalistic responder in these synthetic reconstructions with an ADS simulator
and comparing the outcome to human-response outcomes. Our ADS reduced
naturalistic collisions by 4x and fractional collision risk by ~62%. The
framework's utility is also demonstrated on 250k miles of proprietary,
open-loop sensor data collected on ADS test vehicles, re-simulated with an
arbitrary ADS software release. The ADS initiated conflicts that caused 0.4
injury-causing and 1.7 property-damaging fractional collisions, and the ADS
improved collision risk in 96% of the agent-initiated conflicts.

</details>


### [672] [Blending Participatory Design and Artificial Awareness for Trustworthy Autonomous Vehicles](https://arxiv.org/abs/2506.07633)
*Ana Tanevska,Ananthapathmanabhan Ratheesh Kumar,Arabinda Ghosh,Ernesto Casablanca,Ginevra Castellano,Sadegh Soudjani*

Main category: cs.RO

TL;DR: 论文提出了一种数据驱动的人类驾驶员模型，用于增强多智能体系统中的情境感知和信任交互，并通过大规模用户研究验证了模型的有效性。


<details>
  <summary>Details</summary>
Motivation: 解决自主车辆和无人机在多智能体系统中与人类交互时的情境感知、风险意识和信任问题。

Method: 通过大规模用户中心研究收集数据，构建马尔可夫链模型来模拟人类驾驶员行为。

Result: 研究发现，AV的透明度、环境场景和用户人口统计特征显著影响模型的行为转换。

Conclusion: 研究为多智能体系统中的人类交互建模提供了数据支持，并强调了透明度和用户特征的重要性。

Abstract: Current robotic agents, such as autonomous vehicles (AVs) and drones, need to
deal with uncertain real-world environments with appropriate situational
awareness (SA), risk awareness, coordination, and decision-making. The SymAware
project strives to address this issue by designing an architecture for
artificial awareness in multi-agent systems, enabling safe collaboration of
autonomous vehicles and drones. However, these agents will also need to
interact with human users (drivers, pedestrians, drone operators), which in
turn requires an understanding of how to model the human in the interaction
scenario, and how to foster trust and transparency between the agent and the
human.
  In this work, we aim to create a data-driven model of a human driver to be
integrated into our SA architecture, grounding our research in the principles
of trustworthy human-agent interaction. To collect the data necessary for
creating the model, we conducted a large-scale user-centered study on human-AV
interaction, in which we investigate the interaction between the AV's
transparency and the users' behavior.
  The contributions of this paper are twofold: First, we illustrate in detail
our human-AV study and its findings, and second we present the resulting Markov
chain models of the human driver computed from the study's data. Our results
show that depending on the AV's transparency, the scenario's environment, and
the users' demographics, we can obtain significant differences in the model's
transitions.

</details>


### [673] [Fast ECoT: Efficient Embodied Chain-of-Thought via Thoughts Reuse](https://arxiv.org/abs/2506.07639)
*Zhekai Duan,Yuan Zhang,Shikai Geng,Gaowen Liu,Joschka Boedecker,Chris Xiaoxuan Lu*

Main category: cs.RO

TL;DR: Fast ECoT通过缓存和并行化推理步骤，显著降低了ECoT推理延迟，无需模型修改或额外训练。


<details>
  <summary>Details</summary>
Motivation: ECoT推理的序列化生成导致高延迟，限制了实时部署。

Method: 提出Fast ECoT，利用缓存和并行化推理步骤，并引入异步调度器。

Result: 实验显示延迟降低7.5%，任务成功率和推理准确性保持或提升。

Conclusion: Fast ECoT使ECoT更接近实时部署。

Abstract: Embodied Chain-of-Thought (ECoT) reasoning enhances vision-language-action
(VLA) models by improving performance and interpretability through intermediate
reasoning steps. However, its sequential autoregressive token generation
introduces significant inference latency, limiting real-time deployment. We
propose Fast ECoT, an inference-time acceleration method that exploits the
structured and repetitive nature of ECoT to (1) cache and reuse high-level
reasoning across timesteps and (2) parallelise the generation of modular
reasoning steps. Additionally, we introduce an asynchronous scheduler that
decouples reasoning from action decoding, further boosting responsiveness. Fast
ECoT requires no model changes or additional training and integrates easily
into existing VLA pipelines. Experiments in both simulation (LIBERO) and
real-world robot tasks show up to a 7.5% reduction in latency with comparable
or improved task success rate and reasoning faithfulness, bringing ECoT
policies closer to practical real-time deployment.

</details>


### [674] [A Communication-Latency-Aware Co-Simulation Platform for Safety and Comfort Evaluation of Cloud-Controlled ICVs](https://arxiv.org/abs/2506.07696)
*Yongqi Zhao,Xinrui Zhang,Tomislav Mihalj,Martin Schabauer,Luis Putzer,Erik Reichmann-Blaga,Ádám Boronyák,András Rövid,Gábor Soós,Peizhi Zhang,Lu Xiong,Jia Hu,Arno Eichberger*

Main category: cs.RO

TL;DR: 提出了一种基于CarMaker和Vissim的延迟感知协同仿真平台，用于评估云控智能网联车在真实V2C延迟下的安全性和舒适性。


<details>
  <summary>Details</summary>
Motivation: 测试云控智能网联车需要模拟真实车辆行为和通信延迟的环境。

Method: 整合CarMaker和Vissim，引入基于5G实测数据的Gamma分布延迟模型，并提出主动冲突模块（PCM）生成安全关键场景。

Result: 实验表明PCM能有效提升驾驶环境临界性，而V2C延迟主要影响舒适性。

Conclusion: 该平台能系统评估云控智能网联车在不同测试条件下的表现。

Abstract: Testing cloud-controlled intelligent connected vehicles (ICVs) requires
simulation environments that faithfully emulate both vehicle behavior and
realistic communication latencies. This paper proposes a latency-aware
co-simulation platform integrating CarMaker and Vissim to evaluate safety and
comfort under real-world vehicle-to-cloud (V2C) latency conditions. Two
communication latency models, derived from empirical 5G measurements in China
and Hungary, are incorporated and statistically modeled using Gamma
distributions. A proactive conflict module (PCM) is proposed to dynamically
control background vehicles and generate safety-critical scenarios. The
platform is validated through experiments involving an exemplary system under
test (SUT) across six testing conditions combining two PCM modes
(enabled/disabled) and three latency conditions (none, China, Hungary). Safety
and comfort are assessed using metrics including collision rate, distance
headway, post-encroachment time, and the spectral characteristics of
longitudinal acceleration. Results show that the PCM effectively increases
driving environment criticality, while V2C latency primarily affects ride
comfort. These findings confirm the platform's effectiveness in systematically
evaluating cloud-controlled ICVs under diverse testing conditions.

</details>


### [675] [SMaRCSim: Maritime Robotics Simulation Modules](https://arxiv.org/abs/2506.07781)
*Mart Kartašev,David Dörner,Özer Özkahraman,Petter Ögren,Ivan Stenius,John Folkesson*

Main category: cs.RO

TL;DR: SMaRCSim是一个模拟工具包，旨在解决水下机器人开发中的快速测试、多车辆协同及任务规划集成问题。


<details>
  <summary>Details</summary>
Motivation: 现有模拟工具无法满足水下机器人学习、多车辆协同及任务规划的需求，亟需一个综合解决方案。

Method: 开发SMaRCSim模拟工具包，支持学习算法、多车辆协同及任务规划集成。

Result: SMaRCSim为水下机器人开发提供了高效的模拟测试环境。

Conclusion: SMaRCSim填补了现有工具的不足，为水下机器人创新功能开发提供了支持。

Abstract: Developing new functionality for underwater robots and testing them in the
real world is time-consuming and resource-intensive. Simulation environments
allow for rapid testing before field deployment. However, existing tools lack
certain functionality for use cases in our project: i) developing
learning-based methods for underwater vehicles; ii) creating teams of
autonomous underwater, surface, and aerial vehicles; iii) integrating the
simulation with mission planning for field experiments. A holistic solution to
these problems presents great potential for bringing novel functionality into
the underwater domain. In this paper we present SMaRCSim, a set of simulation
packages that we have developed to help us address these issues.

</details>


### [676] [Primal-Dual iLQR for GPU-Accelerated Learning and Control in Legged Robots](https://arxiv.org/abs/2506.07823)
*Lorenzo Amatucci,João Sousa-Pinto,Giulio Turrisi,Dominique Orban,Victor Barasuol,Claudio Semini*

Main category: cs.RO

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: This paper introduces a novel Model Predictive Control (MPC) implementation
for legged robot locomotion that leverages GPU parallelization. Our approach
enables both temporal and state-space parallelization by incorporating a
parallel associative scan to solve the primal-dual Karush-Kuhn-Tucker (KKT)
system. In this way, the optimal control problem is solved in
$\mathcal{O}(n\log{N} + m)$ complexity, instead of $\mathcal{O}(N(n + m)^3)$,
where $n$, $m$, and $N$ are the dimension of the system state, control vector,
and the length of the prediction horizon. We demonstrate the advantages of this
implementation over two state-of-the-art solvers (acados and crocoddyl),
achieving up to a 60\% improvement in runtime for Whole Body Dynamics (WB)-MPC
and a 700\% improvement for Single Rigid Body Dynamics (SRBD)-MPC when varying
the prediction horizon length. The presented formulation scales efficiently
with the problem state dimensions as well, enabling the definition of a
centralized controller for up to 16 legged robots that can be computed in less
than 25 ms. Furthermore, thanks to the JAX implementation, the solver supports
large-scale parallelization across multiple environments, allowing the
possibility of performing learning with the MPC in the loop directly in GPU.

</details>


### [677] [Versatile Loco-Manipulation through Flexible Interlimb Coordination](https://arxiv.org/abs/2506.07876)
*Xinghao Zhu,Yuxin Chen,Lingfeng Sun,Farzad Niroui,Simon Le CleacH,Jiuguang Wang,Kuan Fang*

Main category: cs.RO

TL;DR: ReLIC是一种通过强化学习实现灵活肢体协调的方法，用于机器人在非结构化环境中的多功能运动与操作。


<details>
  <summary>Details</summary>
Motivation: 现有研究在运动与操作方面受限于特定任务或固定肢体配置，难以适应复杂环境需求。

Method: 采用自适应控制器，动态分配肢体功能（运动或操作），并通过强化学习在仿真中训练，实现任务目标。

Result: 在12个真实任务中平均成功率达78.9%，展示了多功能性和鲁棒性。

Conclusion: ReLIC通过灵活协调肢体功能，显著提升了机器人在复杂环境中的适应能力。

Abstract: The ability to flexibly leverage limbs for loco-manipulation is essential for
enabling autonomous robots to operate in unstructured environments. Yet, prior
work on loco-manipulation is often constrained to specific tasks or
predetermined limb configurations. In this work, we present Reinforcement
Learning for Interlimb Coordination (ReLIC), an approach that enables versatile
loco-manipulation through flexible interlimb coordination. The key to our
approach is an adaptive controller that seamlessly bridges the execution of
manipulation motions and the generation of stable gaits based on task demands.
Through the interplay between two controller modules, ReLIC dynamically assigns
each limb for manipulation or locomotion and robustly coordinates them to
achieve the task success. Using efficient reinforcement learning in simulation,
ReLIC learns to perform stable gaits in accordance with the manipulation goals
in the real world. To solve diverse and complex tasks, we further propose to
interface the learned controller with different types of task specifications,
including target trajectories, contact points, and natural language
instructions. Evaluated on 12 real-world tasks that require diverse and complex
coordination patterns, ReLIC demonstrates its versatility and robustness by
achieving a success rate of 78.9% on average. Videos and code can be found at
https://relic-locoman.github.io/.

</details>


### [678] [Design and Implementation of a Peer-to-Peer Communication, Modular and Decentral YellowCube UUV](https://arxiv.org/abs/2506.07924)
*Zhizun Xu,Baozhu Jia,Weichao Shi*

Main category: cs.RO

TL;DR: 论文提出了一种模块化和去中心化的水下无人航行器（UUV）YellowCube，采用P2P通信机制，解决了现有UUV难以集成新传感器的问题。


<details>
  <summary>Details</summary>
Motivation: 现有UUV通常难以集成新或升级的传感器，限制了其多功能性。

Method: 设计并实现了一种模块化、去中心化的UUV系统，采用P2P通信机制替代集中式架构。

Result: 通过实验室和海上试验验证了UUV的性能。

Conclusion: YellowCube的模块化和P2P设计为UUV的多功能集成提供了可行解决方案。

Abstract: The underwater Unmanned Vehicles(UUVs) are pivot tools for offshore
engineering and oceanographic research. Most existing UUVs do not facilitate
easy integration of new or upgraded sensors. A solution to this problem is to
have a modular UUV system with changeable payload sections capable of carrying
different sensor to suite different missions. The design and implementation of
a modular and decentral UUV named YellowCube is presented in the paper. Instead
a centralised software architecture which is adopted by the other modular
underwater vehicles designs, a Peer-To-Peer(P2P) communication mechanism is
implemented among the UUV's modules. The experiments in the laboratory and sea
trials have been executed to verify the performances of the UUV.

</details>


### [679] [BridgeVLA: Input-Output Alignment for Efficient 3D Manipulation Learning with Vision-Language Models](https://arxiv.org/abs/2506.07961)
*Peiyan Li,Yixiang Chen,Hongtao Wu,Xiao Ma,Xiangnan Wu,Yan Huang,Liang Wang,Tao Kong,Tieniu Tan*

Main category: cs.RO

TL;DR: BridgeVLA是一种新型的3D视觉语言动作模型，通过将3D输入投影为2D图像并利用2D热图进行动作预测，显著提升了样本效率和性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法在将3D信号融入视觉语言模型（VLM）时未能充分利用3D数据的空间结构，导致样本效率低下。

Method: BridgeVLA将3D输入投影为多个2D图像，并在2D图像空间中统一输入和输出，同时提出了一种可扩展的预训练方法。

Result: 在三个仿真基准测试中，BridgeVLA表现优于现有方法，平均成功率显著提升，并在真实机器人实验中表现出色。

Conclusion: BridgeVLA在样本效率和泛化能力上具有显著优势，适用于多种任务和场景。

Abstract: Recently, leveraging pre-trained vision-language models (VLMs) for building
vision-language-action (VLA) models has emerged as a promising approach to
effective robot manipulation learning. However, only few methods incorporate 3D
signals into VLMs for action prediction, and they do not fully leverage the
spatial structure inherent in 3D data, leading to low sample efficiency. In
this paper, we introduce BridgeVLA, a novel 3D VLA model that (1) projects 3D
inputs to multiple 2D images, ensuring input alignment with the VLM backbone,
and (2) utilizes 2D heatmaps for action prediction, unifying the input and
output spaces within a consistent 2D image space. In addition, we propose a
scalable pre-training method that equips the VLM backbone with the capability
to predict 2D heatmaps before downstream policy learning. Extensive experiments
show the proposed method is able to learn 3D manipulation efficiently and
effectively. BridgeVLA outperforms state-of-the-art baseline methods across
three simulation benchmarks. In RLBench, it improves the average success rate
from 81.4% to 88.2%. In COLOSSEUM, it demonstrates significantly better
performance in challenging generalization settings, boosting the average
success rate from 56.7% to 64.0%. In GemBench, it surpasses all the comparing
baseline methods in terms of average success rate. In real-robot experiments,
BridgeVLA outperforms a state-of-the-art baseline method by 32% on average. It
generalizes robustly in multiple out-of-distribution settings, including visual
disturbances and unseen instructions. Remarkably, it is able to achieve a
success rate of 96.8% on 10+ tasks with only 3 trajectories per task,
highlighting its extraordinary sample efficiency. Project
Website:https://bridgevla.github.io/

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [680] [A Fast and Lightweight Model for Causal Audio-Visual Speech Separation](https://arxiv.org/abs/2506.06689)
*Wendi Sang,Kai Li,Runxuan Yang,Jianqiang Huang,Xiaolin Hu*

Main category: cs.SD

TL;DR: Swift-Net是一种新型的流式音频-视觉语音分离模型，专为实时应用设计，通过轻量级视觉特征提取和高效融合模块提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有音频-视觉语音分离方法架构复杂且依赖未来上下文，无法满足实时需求。

Method: 采用轻量级视觉特征提取模块、高效融合模块及Grouped SRUs整合历史信息，并提出因果转换模板。

Result: 在LRS2、LRS3和VoxCeleb2数据集上表现出色，适用于复杂环境。

Conclusion: Swift-Net展示了实时音频-视觉语音分离的潜力，性能优越。

Abstract: Audio-visual speech separation (AVSS) aims to extract a target speech signal
from a mixed signal by leveraging both auditory and visual (lip movement) cues.
However, most existing AVSS methods exhibit complex architectures and rely on
future context, operating offline, which renders them unsuitable for real-time
applications. Inspired by the pipeline of RTFSNet, we propose a novel streaming
AVSS model, named Swift-Net, which enhances the causal processing capabilities
required for real-time applications. Swift-Net adopts a lightweight visual
feature extraction module and an efficient fusion module for audio-visual
integration. Additionally, Swift-Net employs Grouped SRUs to integrate
historical information across different feature spaces, thereby improving the
utilization efficiency of historical information. We further propose a causal
transformation template to facilitate the conversion of non-causal AVSS models
into causal counterparts. Experiments on three standard benchmark datasets
(LRS2, LRS3, and VoxCeleb2) demonstrated that under causal conditions, our
proposed Swift-Net exhibited outstanding performance, highlighting the
potential of this method for processing speech in complex environments.

</details>


### [681] [Can Quantized Audio Language Models Perform Zero-Shot Spoofing Detection?](https://arxiv.org/abs/2506.06756)
*Bikash Dutta,Rishabh Ranjan,Shyam Sathvik,Mayank Vatsa,Richa Singh*

Main category: cs.SD

TL;DR: 研究评估了五种音频语言模型在零样本音频伪造检测任务中的表现，并探讨了量化（FP32、FP16、INT8）对其性能的影响。FP16量化在保持性能的同时显著降低了资源需求，而INT8量化则加剧了模型偏差。


<details>
  <summary>Details</summary>
Motivation: 量化对大型音频语言模型在资源受限环境中的部署至关重要，但其在复杂任务（如零样本音频伪造检测）中的影响尚未充分研究。

Method: 评估了五种模型（GAMA、LTU-AS、MERaLiON、Qwen-Audio、SALMONN）在三个数据集（ASVspoof2019、In-the-Wild、WaveFake）上的零样本能力，并测试了不同量化精度（FP32、FP16、INT8）下的鲁棒性。

Result: FP16量化对性能影响极小，但INT8量化显著降低了平衡准确率。所有模型在量化后表现出对伪造分类的严重偏差。

Conclusion: FP16量化是资源与性能的最佳折衷方案，为实际部署和未来模型优化提供了指导。

Abstract: Quantization is essential for deploying large audio language models (LALMs)
efficiently in resource-constrained environments. However, its impact on
complex tasks, such as zero-shot audio spoofing detection, remains
underexplored. This study evaluates the zero-shot capabilities of five LALMs,
GAMA, LTU-AS, MERaLiON, Qwen-Audio, and SALMONN, across three distinct
datasets: ASVspoof2019, In-the-Wild, and WaveFake, and investigates their
robustness to quantization (FP32, FP16, INT8). Despite high initial spoof
detection accuracy, our analysis demonstrates severe predictive biases toward
spoof classification across all models, rendering their practical performance
equivalent to random classification. Interestingly, quantization to FP16
precision resulted in negligible performance degradation compared to FP32,
effectively halving memory and computational requirements without materially
impacting accuracy. However, INT8 quantization intensified model biases,
significantly degrading balanced accuracy. These findings highlight critical
architectural limitations and emphasize FP16 quantization as an optimal
trade-off, providing guidelines for practical deployment and future model
refinement.

</details>


### [682] [SynHate: Detecting Hate Speech in Synthetic Deepfake Audio](https://arxiv.org/abs/2506.06772)
*Rishabh Ranjan,Kishan Pipariya,Mayank Vatsa,Richa Singh*

Main category: cs.SD

TL;DR: SynHate是首个多语言数据集，用于检测合成音频中的仇恨言论，涵盖37种语言，采用四类分类方案，并评估了五种自监督模型。


<details>
  <summary>Details</summary>
Motivation: 深度伪造音频和仇恨言论的兴起威胁在线安全，需要多语言和跨文化的解决方案。

Method: 基于MuTox和ADIMA数据集构建SynHate，采用四类分类方案（Real-normal, Real-hate, Fake-normal, Fake-hate），并评估五种自监督模型。

Result: Whisper-small表现最佳，但跨数据集泛化仍具挑战性。

Conclusion: SynHate的发布旨在推动针对合成仇恨言论的鲁棒、文化敏感和多语言解决方案。

Abstract: The rise of deepfake audio and hate speech, powered by advanced
text-to-speech, threatens online safety. We present SynHate, the first
multilingual dataset for detecting hate speech in synthetic audio, spanning 37
languages. SynHate uses a novel four-class scheme: Real-normal, Real-hate,
Fake-normal, and Fake-hate. Built from MuTox and ADIMA datasets, it captures
diverse hate speech patterns globally and in India. We evaluate five leading
self-supervised models (Whisper-small/medium, XLS-R, AST, mHuBERT), finding
notable performance differences by language, with Whisper-small performing best
overall. Cross-dataset generalization remains a challenge. By releasing SynHate
and baseline code, we aim to advance robust, culturally sensitive, and
multilingual solutions against synthetic hate speech. The dataset is available
at https://www.iab-rubric.org/resources.

</details>


### [683] ["In This Environment, As That Speaker": A Text-Driven Framework for Multi-Attribute Speech Conversion](https://arxiv.org/abs/2506.07036)
*Jiawei Jin,Zhuhan Yang,Yixuan Zhou,Zhiyong Wu*

Main category: cs.SD

TL;DR: TES-VC是一个文本驱动的语音转换框架，可独立控制说话者音色和环境声学，通过潜在扩散模型和检索模块实现高精度控制。


<details>
  <summary>Details</summary>
Motivation: 解决语音转换中音色和环境声学独立控制的难题，同时保留源内容。

Method: 使用潜在扩散模型处理合成数据，结合检索模块（RBTC）实现无配对数据的精确控制。

Result: 实验表明TES-VC能生成上下文匹配的语音，保留内容且控制性强。

Conclusion: TES-VC在语音转换领域具有广泛应用潜力。

Abstract: We propose TES-VC (Text-driven Environment and Speaker controllable Voice
Conversion), a text-driven voice conversion framework with independent control
of speaker timbre and environmental acoustics. TES-VC processes simultaneous
text inputs for target voice and environment, accurately generating speech
matching described timbre/environment while preserving source content. Trained
on synthetic data with decoupled vocal/environment features via latent
diffusion modeling, our method eliminates interference between attributes. The
Retrieval-Based Timbre Control (RBTC) module enables precise manipulation using
abstract descriptions without paired data. Experiments confirm TES-VC
effectively generates contextually appropriate speech in both timbre and
environment with high content retention and superior controllability which
demonstrates its potential for widespread applications.

</details>


### [684] [Insights on Harmonic Tones from a Generative Music Experiment](https://arxiv.org/abs/2506.07073)
*Emmanuel Deruty,Maarten Grachten*

Main category: cs.SD

TL;DR: 生成音乐AI的最终目的是音乐制作，通过工作室实验室的形式，结合艺术与科学，推动AI音乐模型的应用。实验发现，模型能生成结构化和连贯的旋律线，挑战了人类对谐波音高的感知传统观点。


<details>
  <summary>Details</summary>
Motivation: 探索生成音乐AI在音乐制作中的实际应用，并研究其对音乐创作和理解的影响。

Method: 通过工作室实验室的形式，结合研究人员、音乐制作人和AI音乐模型进行实验，观察模型输出在音乐制作中的使用方式。

Result: 模型能够生成结构化和连贯的旋律线，同时揭示人类对谐波音高的感知可能性。

Conclusion: 生成音乐AI不仅能提升音乐创作，还能深化对音乐的理解，挑战传统观点。

Abstract: The ultimate purpose of generative music AI is music production. The
studio-lab, a social form within the art-science branch of
cross-disciplinarity, is a way to advance music production with AI music
models. During a studio-lab experiment involving researchers, music producers,
and an AI model for music generating bass-like audio, it was observed that the
producers used the model's output to convey two or more pitches with a single
harmonic complex tone, which in turn revealed that the model had learned to
generate structured and coherent simultaneous melodic lines using monophonic
sequences of harmonic complex tones. These findings prompt a reconsideration of
the long-standing debate on whether humans can perceive harmonics as distinct
pitches and highlight how generative AI can not only enhance musical creativity
but also contribute to a deeper understanding of music.

</details>


### [685] [Streaming Endpointer for Spoken Dialogue using Neural Audio Codecs and Label-Delayed Training](https://arxiv.org/abs/2506.07081)
*Sathvik Udupa,Shinji Watanabe,Petr Schwarz,Jan Cernocky*

Main category: cs.SD

TL;DR: 提出了一种基于神经音频编解码器（NAC）特征的实时语音端点检测方法，结合标签延迟训练方案，显著降低了多轮对话中的端点错误。


<details>
  <summary>Details</summary>
Motivation: 传统端点检测方法依赖频谱特征，但实时性和准确性不足，尤其在多轮对话中。

Method: 使用流式低比特率NAC特征，并引入标签延迟训练方案。

Result: 在160毫秒的中位延迟下，单流和双流配置分别减少42.7%和37.5%的端点错误，与预训练语音大模型集成后，响应时间减少1200毫秒，端点错误降低35%。

Conclusion: NAC特征结合标签延迟训练显著提升了端点检测的实时性和准确性，适用于多轮对话系统。

Abstract: Accurate, low-latency endpointing is crucial for effective spoken dialogue
systems. While traditional endpointers often rely on spectrum-based audio
features, this work proposes real-time speech endpointing for multi-turn
dialogues using streaming, low-bitrate Neural Audio Codec (NAC) features,
building upon recent advancements in neural audio codecs. To further reduce
cutoff errors, we introduce a novel label delay training scheme. At a fixed
median latency of 160 ms, our combined NAC and label delay approach achieves
significant relative cutoff error reductions: 42.7% for a single-stream
endpointer and 37.5% for a two-stream configuration, compared to baseline
methods. Finally, we demonstrate efficient integration with a codec-based
pretrained speech large language model, improving its median response time by
1200 ms and reducing its cutoff error by 35%.

</details>


### [686] [RBA-FE: A Robust Brain-Inspired Audio Feature Extractor for Depression Diagnosis](https://arxiv.org/abs/2506.07118)
*Yu-Xuan Wu,Ziyan Huang,Bin Hu,Zhi-Hong Guan*

Main category: cs.SD

TL;DR: 提出了一种基于大脑启发的鲁棒音频特征提取器（RBA-FE），用于抑郁症诊断，采用改进的分层网络架构，结合六种声学特征和自适应速率平滑泄漏整合发放（ARSLIF）神经元模型，显著提升了噪声鲁棒性和诊断准确性。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习模型在图像诊断任务中表现优异，但忽视了音频特征。RBA-FE旨在解决音频特征提取中的噪声挑战，并提升抑郁症诊断的精度。

Method: 结合六种声学特征提取空间和时间依赖关系，引入ARSLIF神经元模型模拟大脑注意力系统的信号选择性重调机制，增强噪声鲁棒性。

Result: 在MODMA数据集上，RBA-FE在精确度、准确率、召回率和F1分数上分别达到0.8750、0.8974、0.8750和0.8750，并在AVEC2014和DAIC-WOZ数据集上验证了噪声鲁棒性提升。

Conclusion: RBA-FE通过大脑启发的设计显著提升了抑郁症诊断的准确性和噪声鲁棒性，同时ARSLIF模型为抑郁音频数据的特征提取提供了可解释性。

Abstract: This article proposes a robust brain-inspired audio feature extractor
(RBA-FE) model for depression diagnosis, using an improved hierarchical network
architecture. Most deep learning models achieve state-of-the-art performance
for image-based diagnostic tasks, ignoring the counterpart audio features. In
order to tailor the noise challenge, RBA-FE leverages six acoustic features
extracted from the raw audio, capturing both spatial characteristics and
temporal dependencies. This hybrid attribute helps alleviate the precision
limitation in audio feature extraction within other learning models like deep
residual shrinkage networks. To deal with the noise issues, our model
incorporates an improved spiking neuron model, called adaptive rate smooth
leaky integrate-and-fire (ARSLIF). The ARSLIF model emulates the mechanism of
``retuning of cellular signal selectivity" in the brain attention systems,
which enhances the model robustness against environmental noises in audio data.
Experimental results demonstrate that RBA-FE achieves state-of-the-art accuracy
on the MODMA dataset, respectively with 0.8750, 0.8974, 0.8750 and 0.8750 in
precision, accuracy, recall and F1 score. Extensive experiments on the AVEC2014
and DAIC-WOZ datasets both show enhancements in noise robustness. It is further
indicated by comparison that the ARSLIF neuron model suggest the abnormal
firing pattern within the feature extraction on depressive audio data, offering
brain-inspired interpretability.

</details>


### [687] [Technical Report: A Practical Guide to Kaldi ASR Optimization](https://arxiv.org/abs/2506.07149)
*Mengze Hong,Di Jiang*

Main category: cs.SD

TL;DR: 本文介绍了基于Kaldi的自动语音识别系统的创新优化方法，包括声学模型增强、超参数调整和语言模型效率提升。


<details>
  <summary>Details</summary>
Motivation: 为了提升Kaldi在快速发展的技术环境中的适应性和竞争力，需要系统性的优化方法。

Method: 采用自定义Conformer块与多流TDNN-F结构结合，结合高级数据增强和动态超参数优化，以及贝叶斯优化和n-gram剪枝的语言模型管理策略。

Result: 显著提高了ASR的准确性和鲁棒性，优于现有方法，并为多样化的语音识别场景提供了可扩展的解决方案。

Conclusion: 战略性优化对维持Kaldi的竞争力和适应性至关重要。

Abstract: This technical report introduces innovative optimizations for Kaldi-based
Automatic Speech Recognition (ASR) systems, focusing on acoustic model
enhancement, hyperparameter tuning, and language model efficiency. We developed
a custom Conformer block integrated with a multistream TDNN-F structure,
enabling superior feature extraction and temporal modeling. Our approach
includes advanced data augmentation techniques and dynamic hyperparameter
optimization to boost performance and reduce overfitting. Additionally, we
propose robust strategies for language model management, employing Bayesian
optimization and $n$-gram pruning to ensure relevance and computational
efficiency. These systematic improvements significantly elevate ASR accuracy
and robustness, outperforming existing methods and offering a scalable solution
for diverse speech recognition scenarios. This report underscores the
importance of strategic optimizations in maintaining Kaldi's adaptability and
competitiveness in rapidly evolving technological landscapes.

</details>


### [688] [Audio synthesizer inversion in symmetric parameter spaces with approximately equivariant flow matching](https://arxiv.org/abs/2506.07199)
*Ben Hayes,Charalampos Saitis,György Fazekas*

Main category: cs.SD

TL;DR: 论文研究了音频合成器参数反演的固有对称性问题，提出了一种基于条件生成模型和置换等变连续归一化流的方法，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 音频合成器的参数反演是一个病态问题，因为不同参数配置可能生成相同的信号。论文旨在解决由合成器内在对称性（尤其是置换不变性）导致的性能下降问题。

Method: 论文首先在合成任务中验证了置换对称性对回归性能的影响，随后提出了一种条件生成模型，并结合置换等变连续归一化流来改进性能。此外，还提出了一种自适应发现对称性的松弛等变策略。

Result: 在Surge XT合成器上的实验表明，该方法在音频重建指标上优于回归和生成基线方法。

Conclusion: 通过建模参数分布的多模态性和利用置换等变性，论文提出的方法有效解决了音频合成器参数反演中的对称性问题。

Abstract: Many audio synthesizers can produce the same signal given different parameter
configurations, meaning the inversion from sound to parameters is an inherently
ill-posed problem. We show that this is largely due to intrinsic symmetries of
the synthesizer, and focus in particular on permutation invariance. First, we
demonstrate on a synthetic task that regressing point estimates under
permutation symmetry degrades performance, even when using a
permutation-invariant loss function or symmetry-breaking heuristics. Then,
viewing equivalent solutions as modes of a probability distribution, we show
that a conditional generative model substantially improves performance.
Further, acknowledging the invariance of the implicit parameter distribution,
we find that performance is further improved by using a permutation equivariant
continuous normalizing flow. To accommodate intricate symmetries in real
synthesizers, we also propose a relaxed equivariance strategy that adaptively
discovers relevant symmetries from data. Applying our method to Surge XT, a
full-featured open source synthesizer used in real world audio production, we
find our method outperforms regression and generative baselines across audio
reconstruction metrics.

</details>


### [689] [Methods for pitch analysis in contemporary popular music: Vitalic's use of tones that do not operate on the principle of acoustic resonance](https://arxiv.org/abs/2506.07207)
*Emmanuel Deruty,Pascal Arbez-Nicolas,David Meredith*

Main category: cs.SD

TL;DR: 论文分析了Vitalic在电子音乐中使用非谐波音调的技术，探讨了其如何通过单音序列创造双重旋律效果，并扩展讨论了流行音乐中的类似现象。


<details>
  <summary>Details</summary>
Motivation: 研究Vitalic如何通过非谐波音调创造独特的音乐效果，并探讨这种技术在当代流行音乐中的普遍性。

Method: 以Vitalic的2005年曲目《No Fun》为例，分析其合成器部分的结构，同时提供其他流行音乐中的类似例子。

Result: 研究发现Vitalic通过非谐波音调实现了双重旋律效果，且这种技术在其他流行音乐中也有应用。

Conclusion: 非谐波音调是电子音乐中创造复杂旋律的有效工具，具有广泛的应用潜力。

Abstract: Vitalic is an electronic music producer who has been active since 2001.
Vitalic's 2005 track "No Fun" features a main synthesiser part built from a
sequence of single inharmonic tones that evoke two simultaneous melodies. This
part serves as a starting point for examining Vitalic's use of tones that do
not operate on the principle of acoustic resonance. The study considers tones
that evoke two or more simultaneous pitches and examines various inharmonic
partial layouts. Examples outside Vitalic's music are also provided to suggest
that similar tone properties can be found elsewhere in contemporary popular
music.

</details>


### [690] [Towards Generalized Source Tracing for Codec-Based Deepfake Speech](https://arxiv.org/abs/2506.07294)
*Xuanjun Chen,I-Ming Lin,Lin Zhang,Haibin Wu,Hung-yi Lee,Jyh-Shing Roger Jang*

Main category: cs.SD

TL;DR: 论文提出SASTNet模型，结合语义和声学特征，提升CodecFake语音来源追踪性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法在CodecFake语音来源追踪中表现不佳，且模拟数据训练的模型难以泛化到真实数据。

Method: 引入SASTNet，联合使用Whisper（语义特征）和Wav2vec2与AudioMAE（声学特征）。

Result: SASTNet在CodecFake+数据集上达到最优性能。

Conclusion: SASTNet能有效提升CodecFake语音来源追踪的可靠性。

Abstract: Recent attempts at source tracing for codec-based deepfake speech
(CodecFake), generated by neural audio codec-based speech generation (CoSG)
models, have exhibited suboptimal performance. However, how to train source
tracing models using simulated CoSG data while maintaining strong performance
on real CoSG-generated audio remains an open challenge. In this paper, we show
that models trained solely on codec-resynthesized data tend to overfit to
non-speech regions and struggle to generalize to unseen content. To mitigate
these challenges, we introduce the Semantic-Acoustic Source Tracing Network
(SASTNet), which jointly leverages Whisper for semantic feature encoding and
Wav2vec2 with AudioMAE for acoustic feature encoding. Our proposed SASTNet
achieves state-of-the-art performance on the CoSG test set of the CodecFake+
dataset, demonstrating its effectiveness for reliable source tracing.

</details>


### [691] [Speech Recognition on TV Series with Video-guided Post-Correction](https://arxiv.org/abs/2506.07323)
*Haoyuan Yang,Yue Zhang,Liqiang Jing*

Main category: cs.SD

TL;DR: 提出了一种新颖的多模态后校正框架，通过视频提取上下文信息改进自动语音识别（ASR）在复杂环境中的转录准确性。


<details>
  <summary>Details</summary>
Motivation: ASR在复杂环境（如电视剧）中表现不佳，现有方法未能充分利用视频中的丰富信息。

Method: 采用两阶段框架：ASR生成和基于视频的后校正，结合VLMM和LLM提取并利用视频上下文信息。

Result: 在电视剧ASR多模态基准测试中，该方法显著提升了转录准确性。

Conclusion: 视频上下文信息能有效提升ASR在复杂环境中的性能。

Abstract: Automatic Speech Recognition (ASR) has achieved remarkable success with deep
learning, driving advancements in conversational artificial intelligence, media
transcription, and assistive technologies. However, ASR systems still struggle
in complex environments such as TV series, where overlapping speech,
domain-specific terminology, and long-range contextual dependencies pose
significant challenges to transcription accuracy. Existing multimodal
approaches fail to correct ASR outputs with the rich temporal and contextual
information available in video. To address this limitation, we propose a novel
multimodal post-correction framework that refines ASR transcriptions by
leveraging contextual cues extracted from video. Our framework consists of two
stages: ASR Generation and Video-based Post-Correction, where the first stage
produces the initial transcript and the second stage corrects errors using
Video-based Contextual Information Extraction and Context-aware ASR Correction.
We employ the Video-Large Multimodal Model (VLMM) to extract key contextual
information using tailored prompts, which is then integrated with a Large
Language Model (LLM) to refine the ASR output. We evaluate our method on a
multimodal benchmark for TV series ASR and demonstrate its effectiveness in
improving ASR performance by leveraging video-based context to enhance
transcription accuracy in complex multimedia environments.

</details>


### [692] [Lightweight Joint Audio-Visual Deepfake Detection via Single-Stream Multi-Modal Learning Framework](https://arxiv.org/abs/2506.07358)
*Kuiyuan Zhang,Wenjie Pei,Rushi Lan,Yifang Guo,Zhongyun Hua*

Main category: cs.SD

TL;DR: 提出了一种轻量级单流多模态学习框架，用于音频-视觉深度伪造检测，通过协作学习块和多模态分类模块提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法通常独立学习音频和视觉特征，未能充分利用其相关性，且模型冗余低效。

Method: 设计单流网络，引入协作学习块和多模态分类模块，实现高效特征融合与轻量化。

Result: 在多个数据集上优于现有方法，仅需0.48M参数，性能更优且泛化能力强。

Conclusion: 该方法轻量高效，适用于资源受限环境，且在检测未知类型深度伪造时表现优异。

Abstract: Deepfakes are AI-synthesized multimedia data that may be abused for spreading
misinformation. Deepfake generation involves both visual and audio
manipulation. To detect audio-visual deepfakes, previous studies commonly
employ two relatively independent sub-models to learn audio and visual
features, respectively, and fuse them subsequently for deepfake detection.
However, this may underutilize the inherent correlations between audio and
visual features. Moreover, utilizing two isolated feature learning sub-models
can result in redundant neural layers, making the overall model inefficient and
impractical for resource-constrained environments.
  In this work, we design a lightweight network for audio-visual deepfake
detection via a single-stream multi-modal learning framework. Specifically, we
introduce a collaborative audio-visual learning block to efficiently integrate
multi-modal information while learning the visual and audio features. By
iteratively employing this block, our single-stream network achieves a
continuous fusion of multi-modal features across its layers. Thus, our network
efficiently captures visual and audio features without the need for excessive
block stacking, resulting in a lightweight network design. Furthermore, we
propose a multi-modal classification module that can boost the dependence of
the visual and audio classifiers on modality content. It also enhances the
whole resistance of the video classifier against the mismatches between audio
and visual modalities. We conduct experiments on the DF-TIMIT, FakeAVCeleb, and
DFDC benchmark datasets. Compared to state-of-the-art audio-visual joint
detection methods, our method is significantly lightweight with only 0.48M
parameters, yet it achieves superiority in both uni-modal and multi-modal
deepfakes, as well as in unseen types of deepfakes.

</details>


### [693] [An introduction to pitch strength in contemporary popular music analysis and production](https://arxiv.org/abs/2506.07473)
*Emmanuel Deruty*

Main category: cs.SD

TL;DR: 论文探讨了音乐信息检索中高低层次描述的差异，提出音高强度作为低层次特征可能提升生成式AI模型在音乐制作中的适用性。


<details>
  <summary>Details</summary>
Motivation: 当前生成式AI模型依赖高层次文本描述，与音乐制作中的低层次控制脱节，音高强度可能弥合这一差距。

Method: 通过信号和感知分析，研究了音高强度在歌曲内外的变化及其对音乐结构的影响。

Result: 音高强度在歌曲内外差异显著，影响音乐结构和多音不和谐处理，可能与听觉丰富性相关。

Conclusion: 音高强度是提升生成式AI模型音乐制作适用性的潜在低层次特征。

Abstract: Music information retrieval distinguishes between low- and high-level
descriptions of music. Current generative AI models rely on text descriptions
that are higher level than the controls familiar to studio musicians. Pitch
strength, a low-level perceptual parameter of contemporary popular music, may
be one feature that could make such AI models more suited to music production.
Signal and perceptual analyses suggest that pitch strength (1) varies
significantly across and inside songs; (2) contributes to both small- and
large-scale structure; (3) contributes to the handling of polyphonic
dissonance; and (4) may be a feature of upper harmonics made audible in a
perspective of perceptual richness.

</details>


### [694] [Towards Energy-Efficient and Low-Latency Voice-Controlled Smart Homes: A Proposal for Offline Speech Recognition and IoT Integration](https://arxiv.org/abs/2506.07494)
*Peng Huang,Imdad Ullah,Xiaotong Wei,Tariq Ahamed Ahanger,Najm Hassan,Zawar Hussain Shah*

Main category: cs.SD

TL;DR: 论文提出了一种基于离线语音识别和物联网技术的智能家居系统，解决现有云平台语音识别的高能耗、延迟和单点故障问题。


<details>
  <summary>Details</summary>
Motivation: 现有智能家居系统依赖云端语音识别，存在能耗高、延迟大和单点故障风险，亟需一种更高效、可靠的解决方案。

Method: 1) 在资源有限的硬件中集成离线关键词检测技术；2) 设计去中心化的本地物联网网络。

Result: 系统实现了低延迟的语音控制，不依赖互联网，提升了扩展性和能源可持续性。

Conclusion: 离线语音识别和物联网技术的结合为智能家居提供了更高效、可靠的解决方案。

Abstract: The smart home systems, based on AI speech recognition and IoT technology,
enable people to control devices through verbal commands and make people's
lives more efficient. However, existing AI speech recognition services are
primarily deployed on cloud platforms on the Internet. When users issue a
command, speech recognition devices like ``Amazon Echo'' will post a recording
through numerous network nodes, reach multiple servers, and then receive
responses through the Internet. This mechanism presents several issues,
including unnecessary energy consumption, communication latency, and the risk
of a single-point failure. In this position paper, we propose a smart home
concept based on offline speech recognition and IoT technology: 1) integrating
offline keyword spotting (KWS) technologies into household appliances with
limited resource hardware to enable them to understand user voice commands; 2)
designing a local IoT network with decentralized architecture to manage and
connect various devices, enhancing the robustness and scalability of the
system. This proposal of a smart home based on offline speech recognition and
IoT technology will allow users to use low-latency voice control anywhere in
the home without depending on the Internet and provide better scalability and
energy sustainability.

</details>


### [695] [LeVo: High-Quality Song Generation with Multi-Preference Alignment](https://arxiv.org/abs/2506.07520)
*Shun Lei,Yaoxun Xu,Zhiwei Lin,Huaicheng Zhang,Wei Tan,Hangting Chen,Jianwei Yu,Yixuan Zhang,Chenyu Yang,Haina Zhu,Shuai Wang,Zhiyong Wu,Dong Yu*

Main category: cs.SD

TL;DR: LeVo是一个基于语言模型的框架，通过并行建模混合和双轨音频令牌，结合DPO优化方法，显著提升了歌词到歌曲生成的质量。


<details>
  <summary>Details</summary>
Motivation: 现有方法在歌曲复杂性和数据稀缺性方面存在局限，导致音质、音乐性、指令跟随和声乐-乐器和谐性不足。

Method: LeVo框架包含LeLM（并行建模混合和双轨令牌）和音乐编解码器，采用模块化扩展训练策略和基于DPO的多偏好对齐方法。

Result: 实验表明，LeVo在客观和主观指标上均优于现有方法，消融研究验证了设计的有效性。

Conclusion: LeVo通过创新设计和优化方法，解决了现有歌词到歌曲生成中的关键问题，显著提升了生成质量。

Abstract: Recent advances in large language models (LLMs) and audio language models
have significantly improved music generation, particularly in lyrics-to-song
generation. However, existing approaches still struggle with the complex
composition of songs and the scarcity of high-quality data, leading to
limitations in sound quality, musicality, instruction following, and
vocal-instrument harmony. To address these challenges, we introduce LeVo, an
LM-based framework consisting of LeLM and a music codec. LeLM is capable of
parallelly modeling two types of tokens: mixed tokens, which represent the
combined audio of vocals and accompaniment to achieve vocal-instrument harmony,
and dual-track tokens, which separately encode vocals and accompaniment for
high-quality song generation. It employs two decoder-only transformers and a
modular extension training strategy to prevent interference between different
token types. To further enhance musicality and instruction following, we
introduce a multi-preference alignment method based on Direct Preference
Optimization (DPO). This method handles diverse human preferences through a
semi-automatic data construction process and DPO post-training. Experimental
results demonstrate that LeVo consistently outperforms existing methods on both
objective and subjective metrics. Ablation studies further justify the
effectiveness of our designs. Audio examples are available at
https://levo-demo.github.io/.

</details>


### [696] [Generative Voice Bursts during Phone Call](https://arxiv.org/abs/2506.07526)
*Paritosh Ranjan,Surajit Majumder,Prodip Roy*

Main category: cs.SD

TL;DR: 提出了一种在通话中传输紧急语音消息的新方法，利用生成式AI生成上下文感知的音频消息，确保高优先级信息能绕过常规呼叫等待。


<details>
  <summary>Details</summary>
Motivation: 传统移动电话在紧急情况下无法向正在通话中的接收者传递紧急语音消息，标准呼叫等待提示无法传达紧急性或内容。

Method: 利用生成式AI技术（如GPT Neo）从上下文输入（如位置、健康数据、图像等）自动生成语音消息，并通过语音、文本和优先级推断机制传递。

Result: 系统能生成并传递高优先级紧急消息，最小化通话干扰的同时保持紧急性。

Conclusion: 该方法对电信、移动设备制造和紧急通信平台具有潜在重大影响。

Abstract: In critical situations, conventional mobile telephony fails to convey
emergency voice messages to a callee already engaged in another call. The
standard call waiting alert does not provide the urgency or content of the
waiting call. This paper proposes a novel method for transmitting Generative
Voice Bursts short, context aware audio messages during ongoing calls, from
either preauthorized or dynamically prioritized callers. By leveraging
generative AI techniques, the system automatically generates spoken messages
from contextual inputs example like location, health data, images, background
noise when the caller is unable to speak due to incapacitation or environmental
constraints. The solution incorporates voice, text, and priority inference
mechanisms, allowing high priority emergency messages to bypass conventional
call waiting barriers. The approach employs models such as GPT Neo for
generative text, which is synthesized into audio and delivered in configurable
intervals G seconds and counts N times, ensuring minimal disruption while
preserving urgency. This method holds potential for significant impact across
telecom, mobile device manufacturing, and emergency communication platforms.

</details>


### [697] [Towards a Unified Benchmark for Arabic Pronunciation Assessment: Quranic Recitation as Case Study](https://arxiv.org/abs/2506.07722)
*Yassine El Kheir,Omnia Ibrahim,Amit Meghanani,Nada Almarwani,Hawau Olamide Toyin,Sadeen Alharbi,Modar Alfadly,Lamya Alkanhal,Ibrahim Selim,Shehab Elbatal,Salima Mdhaffar,Thomas Hain,Yasser Hifny,Mostafa Shahin,Ahmed Ali*

Main category: cs.SD

TL;DR: 提出一个统一的现代标准阿拉伯语（MSA）发音错误检测基准，以古兰经诵读为案例研究，包括数据处理、专用音素集开发和首个公开测试集（QuranMB.v1），并评估基线模型。


<details>
  <summary>Details</summary>
Motivation: 为阿拉伯语发音评估提供标准化框架，推动相关研究和应用发展。

Method: 开发数据处理流程、专用音素集和公开测试集（QuranMB.v1），并评估基线模型。

Result: 建立了首个公开的MSA发音错误检测基准，展示了评估MSA发音的潜力和挑战。

Conclusion: 通过标准化框架促进阿拉伯语发音评估的进一步研究和技术应用。

Abstract: We present a unified benchmark for mispronunciation detection in Modern
Standard Arabic (MSA) using Qur'anic recitation as a case study. Our approach
lays the groundwork for advancing Arabic pronunciation assessment by providing
a comprehensive pipeline that spans data processing, the development of a
specialized phoneme set tailored to the nuances of MSA pronunciation, and the
creation of the first publicly available test set for this task, which we term
as the Qur'anic Mispronunciation Benchmark (QuranMB.v1). Furthermore, we
evaluate several baseline models to provide initial performance insights,
thereby highlighting both the promise and the challenges inherent in assessing
MSA pronunciation. By establishing this standardized framework, we aim to
foster further research and development in pronunciation assessment in Arabic
language technology and related applications.

</details>


<div id='physics.med-ph'></div>

# physics.med-ph [[Back]](#toc)

### [698] [Active Lubrication of Transluminal Medical Instruments](https://arxiv.org/abs/2506.07225)
*Mostafa A. Atalla,Jelte Nieuwenhuis,Alan Martin,Xuan Wang,Ahranee Canden,Matt J. Carré,Roger Lewis,Aimée Sakes,Michaël Wiertlewski*

Main category: physics.med-ph

TL;DR: 提出了一种通过超声波振动主动润滑导管以减少摩擦的新方法，显著提高了手术安全性和效率。


<details>
  <summary>Details</summary>
Motivation: 传统微创手术中导管与组织摩擦可能导致穿孔、触觉反馈差等问题，亟需解决方案。

Method: 利用超声波振动在导管表面产生加压流体层，动态减少摩擦。

Result: 在猪主动脉组织上摩擦减少42%，刚性基底上减少82%，且温度安全，防止导管弯曲。

Conclusion: 主动润滑技术可显著提升微创手术的安全性和效果。

Abstract: Transluminal minimally invasive surgery uses natural orifices and small
incisions to access internal anatomical structures, promoting quicker recovery
and reduced morbidity. However, navigating instruments--catheters and
endoscopes--through anatomical pathways creates frictional interactions with
luminal walls, risking complications such as perforation, poor haptic feedback,
and instrument buckling. In this paper, we present a new approach to actively
lubricate transluminal instruments and dynamically reduce friction with
surrounding tissues. This approach employs ultrasonic vibrations, at the
instrument surface, to generate a pressurized fluid layer at the contact
interface, lubricating the interface and thereby reducing friction. We
implemented this approach in a prototype catheter, which we validated under dry
and liquid-lubricated conditions, across rigid and soft interfaces, and along
varied anatomical curvatures. In a cardiac catheter use case, active
lubrication reduced friction by up to 42% on ex-vivo porcine aorta tissue and
82% on rigid substrates, denoting its potential performance on healthy and
calcified tissue, respectively. Thermal imaging confirmed that temperature at
the tissue-catheter interface remained within safe limits. Additionally, the
system effectively prevented buckling during catheter insertion experiment,
further showcasing its potential. By minimizing injury risk and enhancing
procedural stability, active lubrication can drastically enhance the safety and
efficacy of transluminal interventions.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [699] [On the Fundamental Impossibility of Hallucination Control in Large Language Models](https://arxiv.org/abs/2506.06382)
*Michał P. Karpowicz*

Main category: stat.ML

TL;DR: 论文证明无法创建不产生幻觉的大语言模型，并分析了权衡点。


<details>
  <summary>Details</summary>
Motivation: 探讨为何无法避免大语言模型的幻觉问题及其权衡。

Method: 通过形式化的不可能定理和拍卖模型证明。

Result: 证明了同时满足四个基本属性的不可能性。

Conclusion: 为模型架构、训练目标和评估方法提供了理论基础。

Abstract: This paper explains \textbf{why it is impossible to create large language
models that do not hallucinate and what are the trade-offs we should be looking
for}. It presents a formal \textbf{impossibility theorem} demonstrating that no
inference mechanism can simultaneously satisfy four fundamental properties:
\textbf{truthful (non-hallucinatory) generation, semantic information
conservation, relevant knowledge revelation, and knowledge-constrained
optimality}. By modeling LLM inference as an \textbf{auction of ideas} where
neural components compete to contribute to responses, we prove the
impossibility using the Green-Laffont theorem. That mathematical framework
provides a rigorous foundation for understanding the nature of inference
process, with implications for model architecture, training objectives, and
evaluation methods.

</details>


### [700] [A Statistical Framework for Model Selection in LSTM Networks](https://arxiv.org/abs/2506.06840)
*Fahad Mostafa*

Main category: stat.ML

TL;DR: 本文提出了一种统一的统计框架，用于LSTM网络的系统模型选择，解决了超参数调优和架构选择等问题。


<details>
  <summary>Details</summary>
Motivation: 尽管LSTM在序列数据建模中广泛应用，但其模型选择问题仍依赖启发式方法且计算成本高。

Method: 扩展经典模型选择思想（如信息准则和收缩估计）至序列神经网络，提出惩罚似然和广义阈值方法，并使用变分贝叶斯和近似边际似然进行高效估计。

Result: 在多个生物医学数据示例中展示了框架的灵活性和性能提升。

Conclusion: 该框架为LSTM模型选择提供了系统且高效的方法。

Abstract: Long Short-Term Memory (LSTM) neural network models have become the
cornerstone for sequential data modeling in numerous applications, ranging from
natural language processing to time series forecasting. Despite their success,
the problem of model selection, including hyperparameter tuning, architecture
specification, and regularization choice remains largely heuristic and
computationally expensive. In this paper, we propose a unified statistical
framework for systematic model selection in LSTM networks. Our framework
extends classical model selection ideas, such as information criteria and
shrinkage estimation, to sequential neural networks. We define penalized
likelihoods adapted to temporal structures, propose a generalized threshold
approach for hidden state dynamics, and provide efficient estimation strategies
using variational Bayes and approximate marginal likelihood methods. Several
biomedical data centric examples demonstrate the flexibility and improved
performance of the proposed framework.

</details>


### [701] [Direct Fisher Score Estimation for Likelihood Maximization](https://arxiv.org/abs/2506.06542)
*Sherman Khoo,Yakun Wang,Song Liu,Mark Beaumont*

Main category: stat.ML

TL;DR: 提出了一种基于局部得分匹配技术的序列梯度优化方法，用于解决似然函数难以处理但模型模拟易得的问题。


<details>
  <summary>Details</summary>
Motivation: 研究在似然函数难以处理但模型模拟易得的情况下，如何高效最大化似然。

Method: 采用局部得分匹配技术，通过线性参数化构建代理得分模型，获得闭式最小二乘解。

Result: 该方法能快速、灵活且高效地近似Fisher得分，平滑似然目标并缓解复杂似然景观的挑战。

Conclusion: 理论保证和实证结果均表明该方法优于现有基准。

Abstract: We study the problem of likelihood maximization when the likelihood function
is intractable but model simulations are readily available. We propose a
sequential, gradient-based optimization method that directly models the Fisher
score based on a local score matching technique which uses simulations from a
localized region around each parameter iterate. By employing a linear
parameterization to the surrogate score model, our technique admits a
closed-form, least-squares solution. This approach yields a fast, flexible, and
efficient approximation to the Fisher score, effectively smoothing the
likelihood objective and mitigating the challenges posed by complex likelihood
landscapes. We provide theoretical guarantees for our score estimator,
including bounds on the bias introduced by the smoothing. Empirical results on
a range of synthetic and real-world problems demonstrate the superior
performance of our method compared to existing benchmarks.

</details>


### [702] [Robust Learnability of Sample-Compressible Distributions under Noisy or Adversarial Perturbations](https://arxiv.org/abs/2506.06613)
*Arefe Boushehrian,Amir Najafi*

Main category: stat.ML

TL;DR: 该论文研究了在扰动样本下可压缩分布族的学习问题，提出了两种数据扰动模型，并建立了样本复杂性界限。


<details>
  <summary>Details</summary>
Motivation: 探索在数据扰动情况下，样本可压缩分布族是否仍可学习，并解决高维分布学习中的开放性问题。

Method: 分析两种数据扰动模型（加性独立噪声和对抗性污染），提出扰动-量化框架，并与压缩方案结合。

Result: 证明了样本可压缩族在扰动下仍可学习，并给出了样本复杂性界限，解决了高维均匀分布混合和高斯混合模型的学习问题。

Conclusion: 样本可压缩分布族在扰动条件下仍可学习，且样本复杂性界限与噪声水平和污染预算相关。

Abstract: Learning distribution families over $\mathbb{R}^d$ is a fundamental problem
in unsupervised learning and statistics. A central question in this setting is
whether a given family of distributions possesses sufficient structure to be
(at least) information-theoretically learnable and, if so, to characterize its
sample complexity. In 2018, Ashtiani et al. reframed \emph{sample
compressibility}, originally due to Littlestone and Warmuth (1986), as a
structural property of distribution classes, proving that it guarantees
PAC-learnability. This discovery subsequently enabled a series of recent
advancements in deriving nearly tight sample complexity bounds for various
high-dimensional open problems. It has been further conjectured that the
converse also holds: every learnable class admits a tight sample compression
scheme.
  In this work, we establish that sample compressible families remain learnable
even from perturbed samples, subject to a set of necessary and sufficient
conditions. We analyze two models of data perturbation: (i) an additive
independent noise model, and (ii) an adversarial corruption model, where an
adversary manipulates a limited subset of the samples unknown to the learner.
Our results are general and rely on as minimal assumptions as possible. We
develop a perturbation-quantization framework that interfaces naturally with
the compression scheme and leads to sample complexity bounds that scale
gracefully with the noise level and corruption budget. As concrete
applications, we establish new sample complexity bounds for learning finite
mixtures of high-dimensional uniform distributions under both noise and
adversarial perturbations, as well as for learning Gaussian mixture models from
adversarially corrupted samples, resolving two open problems in the literature.

</details>


### [703] [Continuous Semi-Implicit Models](https://arxiv.org/abs/2506.06778)
*Longlin Yu,Jiajun Zha,Tong Yang,Tianyu Xie,Xiangyu Zhang,S. -H. Gary Chan,Cheng Zhang*

Main category: stat.ML

TL;DR: CoSIM是一种连续半隐式模型，通过连续过渡核提升训练效率，并在图像生成任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: 解决分层半隐式模型训练收敛慢的问题，提升生成模型的效率和性能。

Method: 引入连续过渡核，实现无模拟训练，并通过设计确保分布一致性。

Result: 在图像生成任务中表现优于现有扩散模型加速方法，尤其在FD-DINOv2上表现突出。

Conclusion: CoSIM为生成模型的多步蒸馏提供了新思路，具有高效和性能优势。

Abstract: Semi-implicit distributions have shown great promise in variational inference
and generative modeling. Hierarchical semi-implicit models, which stack
multiple semi-implicit layers, enhance the expressiveness of semi-implicit
distributions and can be used to accelerate diffusion models given pretrained
score networks. However, their sequential training often suffers from slow
convergence. In this paper, we introduce CoSIM, a continuous semi-implicit
model that extends hierarchical semi-implicit models into a continuous
framework. By incorporating a continuous transition kernel, CoSIM enables
efficient, simulation-free training. Furthermore, we show that CoSIM achieves
consistency with a carefully designed transition kernel, offering a novel
approach for multistep distillation of generative models at the distributional
level. Extensive experiments on image generation demonstrate that CoSIM
performs on par or better than existing diffusion model acceleration methods,
achieving superior performance on FD-DINOv2.

</details>


### [704] [The Currents of Conflict: Decomposing Conflict Trends with Gaussian Processes](https://arxiv.org/abs/2506.06828)
*Simon P. von der Maase*

Main category: stat.ML

TL;DR: 提出了一种利用高斯过程和时间空间分解数据估计冲突趋势的新方法，用于研究冲突陷阱、扩散和预测未来冲突模式。


<details>
  <summary>Details</summary>
Motivation: 研究冲突的时间空间模式，以理解冲突陷阱、扩散和暴露，并为其他估计任务提供控制变量。

Method: 使用高斯过程和时间空间分解的冲突事件数据，估计冲突趋势。

Result: 能够预测未来冲突模式，并仅依赖历史冲突数据实现高效分析。

Conclusion: 该方法为冲突研究提供了高效且强大的工具，仅需单一数据源即可实现预测。

Abstract: I present a novel approach to estimating the temporal and spatial patterns of
violent conflict. I show how we can use highly temporally and spatially
disaggregated data on conflict events in tandem with Gaussian processes to
estimate temporospatial conflict trends. These trends can be studied to gain
insight into conflict traps, diffusion and tempo-spatial conflict exposure in
general; they can also be used to control for such phenomenons given other
estimation tasks; lastly, the approach allow us to extrapolate the estimated
tempo-spatial conflict patterns into future temporal units, thus facilitating
powerful, stat-of-the-art, conflict forecasts. Importantly, these results are
achieved via a relatively parsimonious framework using only one data source:
past conflict patterns.

</details>


### [705] [Half-AVAE: Adversarial-Enhanced Factorized and Structured Encoder-Free VAE for Underdetermined Independent Component Analysis](https://arxiv.org/abs/2506.07011)
*Yuan-Hao Wei,Yan-Jie Sun*

Main category: stat.ML

TL;DR: 该研究提出了一种改进的变分自编码器（Half-AVAE），通过结合对抗网络和外部增强项，解决了独立成分分析（ICA）在确定和欠定条件下的挑战，提升了潜在变量的独立性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 传统VAE在欠定ICA中表现不佳，潜在变量数量超过观测信号时难以处理。研究旨在通过改进VAE框架解决这一问题。

Method: 提出Half-AVAE，基于无编码器的Half-VAE框架，结合对抗网络和外部增强项，避免显式逆映射。

Result: 实验表明，Half-AVAE在欠定条件下优于基线模型（如GP-AVAE和Half-VAE），独立成分恢复效果更好。

Conclusion: Half-AVAE展示了VAE在变分推断中的灵活性，为复杂ICA任务提供了有效解决方案，推动了在解耦、因果推断和生成建模中的应用。

Abstract: This study advances the Variational Autoencoder (VAE) framework by addressing
challenges in Independent Component Analysis (ICA) under both determined and
underdetermined conditions, focusing on enhancing the independence and
interpretability of latent variables. Traditional VAEs map observed data to
latent variables and back via an encoder-decoder architecture, but struggle
with underdetermined ICA where the number of latent variables exceeds observed
signals. The proposed Half Adversarial VAE (Half-AVAE) builds on the
encoder-free Half-VAE framework, eliminating explicit inverse mapping to tackle
underdetermined scenarios. By integrating adversarial networks and External
Enhancement (EE) terms, Half-AVAE promotes mutual independence among latent
dimensions, achieving factorized and interpretable representations. Experiments
with synthetic signals demonstrate that Half-AVAE outperforms baseline models,
including GP-AVAE and Half-VAE, in recovering independent components under
underdetermined conditions, as evidenced by lower root mean square errors. The
study highlights the flexibility of VAEs in variational inference, showing that
encoder omission, combined with adversarial training and structured priors,
enables effective solutions for complex ICA tasks, advancing applications in
disentanglement, causal inference, and generative modeling.

</details>


### [706] [Quantile-Optimal Policy Learning under Unmeasured Confounding](https://arxiv.org/abs/2506.07140)
*Zhongren Chen,Siyu Chen,Zhengling Qi,Xiaohong Chen,Zhuoran Yang*

Main category: stat.ML

TL;DR: 研究分位数最优策略学习，解决离线数据中的未观测混杂问题，提出因果辅助方法并证明其理论保证。


<details>
  <summary>Details</summary>
Motivation: 目标是找到奖励分布具有最大α分位数的策略，解决离线数据中的未观测混杂、分位数目标非线性及数据覆盖不足的挑战。

Method: 利用因果推断工具（如工具变量和负控制）估计分位数目标，采用极小极大估计和非参数模型解决积分方程，构建保守策略估计。

Result: 提出正则化策略学习方法，证明策略在温和覆盖假设下具有样本效率。

Conclusion: 首次提出在未观测混杂下估计分位数最优策略的样本高效算法。

Abstract: We study quantile-optimal policy learning where the goal is to find a policy
whose reward distribution has the largest $\alpha$-quantile for some $\alpha
\in (0, 1)$. We focus on the offline setting whose generating process involves
unobserved confounders. Such a problem suffers from three main challenges: (i)
nonlinearity of the quantile objective as a functional of the reward
distribution, (ii) unobserved confounding issue, and (iii) insufficient
coverage of the offline dataset. To address these challenges, we propose a
suite of causal-assisted policy learning methods that provably enjoy strong
theoretical guarantees under mild conditions. In particular, to address (i) and
(ii), using causal inference tools such as instrumental variables and negative
controls, we propose to estimate the quantile objectives by solving nonlinear
functional integral equations. Then we adopt a minimax estimation approach with
nonparametric models to solve these integral equations, and propose to
construct conservative policy estimates that address (iii). The final policy is
the one that maximizes these pessimistic estimates. In addition, we propose a
novel regularized policy learning method that is more amenable to computation.
Finally, we prove that the policies learned by these methods are
$\tilde{\mathscr{O}}(n^{-1/2})$ quantile-optimal under a mild coverage
assumption on the offline dataset. Here, $\tilde{\mathscr{O}}(\cdot)$ omits
poly-logarithmic factors. To the best of our knowledge, we propose the first
sample-efficient policy learning algorithms for estimating the quantile-optimal
policy when there exist unmeasured confounding.

</details>


### [707] [ALINE: Joint Amortization for Bayesian Inference and Active Data Acquisition](https://arxiv.org/abs/2506.07259)
*Daolang Huang,Xinyi Wen,Ayush Bharti,Samuel Kaski,Luigi Acerbi*

Main category: stat.ML

TL;DR: ALINE是一个结合贝叶斯推断和主动数据采集的统一框架，通过强化学习训练，优化数据查询和推断。


<details>
  <summary>Details</summary>
Motivation: 解决在需要即时推断的新数据采集任务中，现有方法无法兼顾最优数据选择和推断的问题。

Method: 采用基于Transformer的架构，通过强化学习训练，奖励基于自估计的信息增益。

Result: 在回归主动学习、贝叶斯实验设计和心理测量模型中表现优异，实现高效数据选择和准确推断。

Conclusion: ALINE在即时推断和高效数据采集方面表现出色，适用于多种任务。

Abstract: Many critical applications, from autonomous scientific discovery to
personalized medicine, demand systems that can both strategically acquire the
most informative data and instantaneously perform inference based upon it.
While amortized methods for Bayesian inference and experimental design offer
part of the solution, neither approach is optimal in the most general and
challenging task, where new data needs to be collected for instant inference.
To tackle this issue, we introduce the Amortized Active Learning and Inference
Engine (ALINE), a unified framework for amortized Bayesian inference and active
data acquisition. ALINE leverages a transformer architecture trained via
reinforcement learning with a reward based on self-estimated information gain
provided by its own integrated inference component. This allows it to
strategically query informative data points while simultaneously refining its
predictions. Moreover, ALINE can selectively direct its querying strategy
towards specific subsets of model parameters or designated predictive tasks,
optimizing for posterior estimation, data prediction, or a mixture thereof.
Empirical results on regression-based active learning, classical Bayesian
experimental design benchmarks, and a psychometric model with selectively
targeted parameters demonstrate that ALINE delivers both instant and accurate
inference along with efficient selection of informative points.

</details>


### [708] [Rao-Blackwellised Reparameterisation Gradients](https://arxiv.org/abs/2506.07687)
*Kevin Lam,Thang Bui,George Deligiannidis,Yee Whye Teh*

Main category: stat.ML

TL;DR: 本文提出了R2-G2估计器，作为重参数化梯度估计器的Rao-Blackwell化版本，并展示了其在贝叶斯多层感知机中的应用。


<details>
  <summary>Details</summary>
Motivation: 研究如何通过Rao-Blackwell化改进重参数化梯度估计器，以提升梯度估计的效率和模型性能。

Method: 提出R2-G2估计器，通过Rao-Blackwell化重参数化梯度估计器，降低梯度方差。

Result: R2-G2在初始训练中表现更优，尤其适用于多次应用重参数化技巧的模型。

Conclusion: R2-G2扩展了Rao-Blackwell化梯度的优势，为概率模型提供了更高效的梯度估计方法。

Abstract: Latent Gaussian variables have been popularised in probabilistic machine
learning. In turn, gradient estimators are the machinery that facilitates
gradient-based optimisation for models with latent Gaussian variables. The
reparameterisation trick is often used as the default estimator as it is simple
to implement and yields low-variance gradients for variational inference. In
this work, we propose the R2-G2 estimator as the Rao-Blackwellisation of the
reparameterisation gradient estimator. Interestingly, we show that the local
reparameterisation gradient estimator for Bayesian MLPs is an instance of the
R2-G2 estimator and Rao-Blackwellisation. This lets us extend benefits of
Rao-Blackwellised gradients to a suite of probabilistic models. We show that
initial training with R2-G2 consistently yields better performance in models
with multiple applications of the reparameterisation trick.

</details>


### [709] [Quickest Causal Change Point Detection by Adaptive Intervention](https://arxiv.org/abs/2506.07760)
*Haijie Xu,Chen Zhang*

Main category: stat.ML

TL;DR: 提出一种用于线性因果模型中变化点监测的算法，通过集中化技术和干预节点选择放大变化信号。


<details>
  <summary>Details</summary>
Motivation: 解决因果模型中变化点监测问题，特别是干预对变化传播的影响。

Method: 采用集中化技术将变化集中到单一维度，基于KL散度选择干预节点，并提出两种监测方法。

Result: 理论证明方法的一阶最优性，并通过模拟和实际案例验证其有效性。

Conclusion: 算法能有效监测因果模型中的变化点，适用于实际应用。

Abstract: We propose an algorithm for change point monitoring in linear causal models
that accounts for interventions. Through a special centralization technique, we
can concentrate the changes arising from causal propagation across nodes into a
single dimension. Additionally, by selecting appropriate intervention nodes
based on Kullback-Leibler divergence, we can amplify the change magnitude. We
also present an algorithm for selecting the intervention values, which aids in
the identification of the most effective intervention nodes. Two monitoring
methods are proposed, each with an adaptive intervention policy to make a
balance between exploration and exploitation. We theoretically demonstrate the
first-order optimality of the proposed methods and validate their properties
using simulation datasets and two real-world case studies.

</details>


### [710] [Accelerating Constrained Sampling: A Large Deviations Approach](https://arxiv.org/abs/2506.07816)
*Yingli Wang,Changwei Tu,Xiaoyu Wang,Lingjiong Zhu*

Main category: stat.ML

TL;DR: 论文研究了在约束域上采样目标概率分布的问题，重点分析了带偏斜对称矩阵的反射非可逆Langevin动力学（SRNLD）的长期行为，并提出了设计偏斜对称矩阵的方法以提升性能。


<details>
  <summary>Details</summary>
Motivation: 约束域上的采样问题在机器学习等应用中常见，但现有方法如反射Langevin动力学（RLD）及其离散化版本（PLMC）的性能仍有优化空间。偏斜对称矩阵的引入（SRNLD）虽能加速收敛，但其设计方法尚不明确。

Method: 通过建立SRNLD经验测度的大偏差原理（LDP），并明确表征速率函数，分析了偏斜对称矩阵的设计对收敛速度的影响。

Result: 理论分析表明，当偏斜对称矩阵与边界内法向量场的乘积为零时，SRNLD能加速收敛到目标分布。数值实验验证了该设计的优越性。

Conclusion: 论文为SRNLD中偏斜对称矩阵的设计提供了理论依据，并通过实验验证了其性能提升，为约束域采样问题提供了更优解决方案。

Abstract: The problem of sampling a target probability distribution on a constrained
domain arises in many applications including machine learning. For constrained
sampling, various Langevin algorithms such as projected Langevin Monte Carlo
(PLMC) based on the discretization of reflected Langevin dynamics (RLD) and
more generally skew-reflected non-reversible Langevin Monte Carlo (SRNLMC)
based on the discretization of skew-reflected non-reversible Langevin dynamics
(SRNLD) have been proposed and studied in the literature. This work focuses on
the long-time behavior of SRNLD, where a skew-symmetric matrix is added to RLD.
Although the non-asymptotic convergence analysis for SRNLD (and SRNLMC) and the
acceleration compared to RLD (and PMLC) have been studied in the literature, it
is not clear how one should design the skew-symmetric matrix in the dynamics to
achieve good performance in practice. We establish a large deviation principle
(LDP) for the empirical measure of SRNLD when the skew-symmetric matrix is
chosen such that its product with the inward unit normal vector field on the
boundary is zero. By explicitly characterizing the rate functions, we show that
SRNLD can accelerate the convergence to the target distribution compared to RLD
with this choice of the skew-symmetric matrix. Numerical experiments for SRNLMC
based on the proposed skew-symmetric matrix show superior performance which
validate the theoretical findings from the large deviations theory.

</details>


<div id='q-fin.ST'></div>

# q-fin.ST [[Back]](#toc)

### [711] [DELPHYNE: A Pre-Trained Model for General and Financial Time Series](https://arxiv.org/abs/2506.06288)
*Xueying Ding,Aakriti Mittal,Achintya Gopal*

Main category: q-fin.ST

TL;DR: 论文提出了一种名为Delphyne的金融时间序列预训练模型，解决了现有模型在金融领域性能不足的问题。


<details>
  <summary>Details</summary>
Motivation: 现有时间序列预训练模型在金融应用中表现不佳，主要由于缺乏金融数据和跨领域负迁移效应。

Method: 提出Delphyne模型，针对金融时间序列数据的特点进行优化。

Result: Delphyne在公开数据集上表现优异，优于现有基准模型。

Conclusion: Delphyne为金融时间序列分析提供了有效的解决方案。

Abstract: Time-series data is a vital modality within data science communities. This is
particularly valuable in financial applications, where it helps in detecting
patterns, understanding market behavior, and making informed decisions based on
historical data. Recent advances in language modeling have led to the rise of
time-series pre-trained models that are trained on vast collections of datasets
and applied to diverse tasks across financial domains. However, across
financial applications, existing time-series pre-trained models have not shown
boosts in performance over simple finance benchmarks in both zero-shot and
fine-tuning settings. This phenomenon occurs because of a i) lack of financial
data within the pre-training stage, and ii) the negative transfer effect due to
inherently different time-series patterns across domains. Furthermore,
time-series data is continuous, noisy, and can be collected at varying
frequencies and with varying lags across different variables, making this data
more challenging to model than languages. To address the above problems, we
introduce a Pre-trained MoDEL for FINance TimE-series (Delphyne). Delphyne
achieves competitive performance to existing foundation and full-shot models
with few fine-tuning steps on publicly available datasets, and also shows
superior performances on various financial tasks.

</details>


### [712] [Explainable-AI powered stock price prediction using time series transformers: A Case Study on BIST100](https://arxiv.org/abs/2506.06345)
*Sukru Selim Calik,Andac Akyuz,Zeynep Hilal Kilimci,Kerem Colak*

Main category: q-fin.ST

TL;DR: 论文提出了一种结合Transformer时间序列模型与可解释人工智能（XAI）的新方法，以提高股票价格预测的准确性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 金融素养的提升需要复杂数据的解读和高级预测工具的使用，因此研究旨在通过可解释的机器学习方法增强股票预测的透明度和准确性。

Method: 研究采用了多种Transformer模型（如DLinear、LTSNet等）并结合技术指标，使用SHAP和LIME技术解释模型输出。

Result: 结果表明Transformer模型具有强大的预测能力，可解释机器学习有助于提升投资决策的透明度。

Conclusion: 可解释的机器学习方法能够增强个人在金融市场中的参与度和决策能力。

Abstract: Financial literacy is increasingly dependent on the ability to interpret
complex financial data and utilize advanced forecasting tools. In this context,
this study proposes a novel approach that combines transformer-based time
series models with explainable artificial intelligence (XAI) to enhance the
interpretability and accuracy of stock price predictions. The analysis focuses
on the daily stock prices of the five highest-volume banks listed in the
BIST100 index, along with XBANK and XU100 indices, covering the period from
January 2015 to March 2025. Models including DLinear, LTSNet, Vanilla
Transformer, and Time Series Transformer are employed, with input features
enriched by technical indicators. SHAP and LIME techniques are used to provide
transparency into the influence of individual features on model outputs. The
results demonstrate the strong predictive capabilities of transformer models
and highlight the potential of interpretable machine learning to empower
individuals in making informed investment decisions and actively engaging in
financial markets.

</details>


### [713] [The Hype Index: an NLP-driven Measure of Market News Attention](https://arxiv.org/abs/2506.06329)
*Zheng Cao,Wanchaloem Wunkaew,Helyette Geman*

Main category: q-fin.ST

TL;DR: 本文提出了一种名为Hype Index的新指标，用于量化媒体对大市值股票的关注度，结合NLP技术从金融新闻中提取预测信号。通过S&P 100股票，构建了两种Hype Index版本，并验证了其在市场分析中的实用性。


<details>
  <summary>Details</summary>
Motivation: 量化媒体对股票的关注度，以提供市场信号和波动性分析的实用工具。

Method: 利用NLP技术构建新闻计数和市值调整的Hype Index，并通过分类、关联分析和市场信号验证其有效性。

Result: Hype Index家族在股票波动性分析、市场信号和NLP金融应用中表现出实用价值。

Conclusion: Hype Index为金融分析和市场预测提供了新的量化工具。

Abstract: This paper introduces the Hype Index as a novel metric to quantify media
attention toward large-cap equities, leveraging advances in Natural Language
Processing (NLP) for extracting predictive signals from financial news. Using
the S&P 100 as the focus universe, we first construct a News Count-Based Hype
Index, which measures relative media exposure by computing the share of news
articles referencing each stock or sector. We then extend it to the
Capitalization Adjusted Hype Index, adjusts for economic size by taking the
ratio of a stock's or sector's media weight to its market capitalization weight
within its industry or sector. We compute both versions of the Hype Index at
the stock and sector levels, and evaluate them through multiple lenses: (1)
their classification into different hype groups, (2) their associations with
returns, volatility, and VIX index at various lags, (3) their signaling power
for short-term market movements, and (4) their empirical properties including
correlations, samplings, and trends. Our findings suggest that the Hype Index
family provides a valuable set of tools for stock volatility analysis, market
signaling, and NLP extensions in Finance.

</details>


### [714] [Towards Competent AI for Fundamental Analysis in Finance: A Benchmark Dataset and Evaluation](https://arxiv.org/abs/2506.07315)
*Zonghan Wu,Junlin Wang,Congyuan Zou,Chenhan Wang,Yilei Shao*

Main category: q-fin.ST

TL;DR: FinAR-Bench是一个专注于财务报表分析的基准数据集，旨在评估大型语言模型（LLMs）在生成财务分析报告中的表现。


<details>
  <summary>Details</summary>
Motivation: 现有金融基准主要评估LLMs回答财务问题的能力，但未反映其在生成财务分析报告等实际任务中的表现。FinAR-Bench填补了这一空白。

Method: 将财务报表分析任务分解为三个可测量步骤：提取关键信息、计算财务指标和应用逻辑推理，以客观评估LLMs的表现。

Result: 研究明确了LLMs在基本面分析中的优势和局限性，并提供了更实用的性能评估方法。

Conclusion: FinAR-Bench为LLMs在真实金融场景中的表现提供了更精确的评估工具。

Abstract: Generative AI, particularly large language models (LLMs), is beginning to
transform the financial industry by automating tasks and helping to make sense
of complex financial information. One especially promising use case is the
automatic creation of fundamental analysis reports, which are essential for
making informed investment decisions, evaluating credit risks, guiding
corporate mergers, etc. While LLMs attempt to generate these reports from a
single prompt, the risks of inaccuracy are significant. Poor analysis can lead
to misguided investments, regulatory issues, and loss of trust. Existing
financial benchmarks mainly evaluate how well LLMs answer financial questions
but do not reflect performance in real-world tasks like generating financial
analysis reports. In this paper, we propose FinAR-Bench, a solid benchmark
dataset focusing on financial statement analysis, a core competence of
fundamental analysis. To make the evaluation more precise and reliable, we
break this task into three measurable steps: extracting key information,
calculating financial indicators, and applying logical reasoning. This
structured approach allows us to objectively assess how well LLMs perform each
step of the process. Our findings offer a clear understanding of LLMs current
strengths and limitations in fundamental analysis and provide a more practical
way to benchmark their performance in real-world financial settings.

</details>


<div id='physics.optics'></div>

# physics.optics [[Back]](#toc)

### [715] [Inverse Design of Metamaterials with Manufacturing-Guiding Spectrum-to-Structure Conditional Diffusion Model](https://arxiv.org/abs/2506.07083)
*Jiawen Li,Jiang Guo,Yuanzhe Li,Zetian Mao,Jiaxing Shen,Tashi Xu,Diptesh Das,Jinming He,Run Hu,Yaerim Lee,Koji Tsuda,Junichiro Shiomi*

Main category: physics.optics

TL;DR: 提出了一种基于条件扩散模型的通用框架，用于解决超材料逆向设计中的一对多问题，具有高光谱预测精度和生成多样性，并成功应用于热伪装设计。


<details>
  <summary>Details</summary>
Motivation: 超材料的逆向设计存在高度非线性关系和制造困难，传统机器学习方法难以应对复杂设计需求。

Method: 采用条件扩散模型实现定制化的光谱到形状和尺寸参数映射，解决一对多逆向设计问题。

Result: 方法在光谱预测精度和生成多样性上优于其他生成模型，并为制造提供了有价值的先验知识。

Conclusion: 该方法成功设计并制造了具有定制选择性发射光谱的自由形式超材料，验证了其有效性。

Abstract: Metamaterials are artificially engineered structures that manipulate
electromagnetic waves, having optical properties absent in natural materials.
Recently, machine learning for the inverse design of metamaterials has drawn
attention. However, the highly nonlinear relationship between the metamaterial
structures and optical behaviour, coupled with fabrication difficulties, poses
challenges for using machine learning to design and manufacture complex
metamaterials. Herein, we propose a general framework that implements
customised spectrum-to-shape and size parameters to address one-to-many
metamaterial inverse design problems using conditional diffusion models. Our
method exhibits superior spectral prediction accuracy, generates a diverse
range of patterns compared to other typical generative models, and offers
valuable prior knowledge for manufacturing through the subsequent analysis of
the diverse generated results, thereby facilitating the experimental
fabrication of metamaterial designs. We demonstrate the efficacy of the
proposed method by successfully designing and fabricating a free-form
metamaterial with a tailored selective emission spectrum for thermal camouflage
applications.

</details>


<div id='eess.SY'></div>

# eess.SY [[Back]](#toc)

### [716] [Deep Equivariant Multi-Agent Control Barrier Functions](https://arxiv.org/abs/2506.07755)
*Nikolaos Bousias,Lars Lindemann,George Pappas*

Main category: eess.SY

TL;DR: 论文提出了一种基于对称性的分布式控制屏障函数方法，用于提升多智能体系统的安全性和可扩展性。


<details>
  <summary>Details</summary>
Motivation: 随着多智能体系统在复杂环境中的自主部署，确保数据驱动策略的安全性变得至关重要。现有方法在可扩展性、泛化性和采样效率上存在不足。

Method: 引入了对称性注入的分布式控制屏障函数，通过可学习的基于图的安全证书满足内在对称性。提出了等变参数化的方法，构建了适应性强且高效的等变群模块网络。

Result: 在模拟多机器人导航任务中，该方法在安全性、可扩展性和任务成功率上优于现有基线。

Conclusion: 研究表明，在分布式神经策略中嵌入对称性对提升安全性至关重要。

Abstract: With multi-agent systems increasingly deployed autonomously at scale in
complex environments, ensuring safety of the data-driven policies is critical.
Control Barrier Functions have emerged as an effective tool for enforcing
safety constraints, yet existing learning-based methods often lack in
scalability, generalization and sampling efficiency as they overlook inherent
geometric structures of the system. To address this gap, we introduce
symmetries-infused distributed Control Barrier Functions, enforcing the
satisfaction of intrinsic symmetries on learnable graph-based safety
certificates. We theoretically motivate the need for equivariant
parametrization of CBFs and policies, and propose a simple, yet efficient and
adaptable methodology for constructing such equivariant group-modular networks
via the compatible group actions. This approach encodes safety constraints in a
distributed data-efficient manner, enabling zero-shot generalization to larger
and denser swarms. Through extensive simulations on multi-robot navigation
tasks, we demonstrate that our method outperforms state-of-the-art baselines in
terms of safety, scalability, and task success rates, highlighting the
importance of embedding symmetries in safe distributed neural policies.

</details>


### [717] [Towards Data-Driven Model-Free Safety-Critical Control](https://arxiv.org/abs/2506.06931)
*Zhe Shen,Yitaek Kim,Christoffer Sloth*

Main category: eess.SY

TL;DR: 提出了一种基于数据驱动的无模型控制屏障函数（CBFs）框架，用于实现机器人系统的安全速度控制，通过神经网络学习Lyapunov函数并估计速度控制器的最大衰减率，结合概率安全条件增强鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 实际应用中，无模型CBFs的设计参数依赖于速度控制器的指数衰减率，但该衰减率通常未知，需手动调整参数。为解决这一问题，提出了一种自动化方法。

Method: 使用神经网络从数据中学习Lyapunov函数，估计速度控制器的最大衰减率，并基于Chernoff边界推导概率安全条件，以处理稳定性条件的不确定性。

Result: 在UR5e机器人上进行了多组实验，验证了该框架在确保无模型CBFs安全速度控制方面的有效性。

Conclusion: 该框架通过自动化参数估计和概率安全条件，显著提升了无模型CBFs在实际应用中的鲁棒性和安全性。

Abstract: This paper presents a framework for enabling safe velocity control of general
robotic systems using data-driven model-free Control Barrier Functions (CBFs).
Model-free CBFs rely on an exponentially stable velocity controller and a
design parameter (e.g. alpha in CBFs); this design parameter depends on the
exponential decay rate of the controller. However, in practice, the decay rate
is often unavailable, making it non-trivial to use model-free CBFs, as it
requires manual tuning for alpha. To address this, a Neural Network is used to
learn the Lyapunov function from data, and the maximum decay rate of the
systems built-in velocity controller is subsequently estimated. Furthermore, to
integrate the estimated decay rate with model-free CBFs, we derive a
probabilistic safety condition that incorporates a confidence bound on the
violation rate of the exponential stability condition, using Chernoff bound.
This enhances robustness against uncertainties in stability violations. The
proposed framework has been tested on a UR5e robot in multiple experimental
settings, and its effectiveness in ensuring safe velocity control with
model-free CBFs has been demonstrated.

</details>


### [718] [The Economic Dispatch of Power-to-Gas Systems with Deep Reinforcement Learning:Tackling the Challenge of Delayed Rewards with Long-Term Energy Storage](https://arxiv.org/abs/2506.06484)
*Manuel Sage,Khalil Al Handawi,Yaoyao Fiona Zhao*

Main category: eess.SY

TL;DR: 该研究探讨了如何利用深度强化学习（DRL）优化Power-to-Gas（P2G）系统的长期经济运营，结合电池储能系统和燃气轮机，并通过改进DRL算法解决延迟奖励问题。


<details>
  <summary>Details</summary>
Motivation: P2G技术虽能整合间歇性可再生能源，但其运营成本效益复杂且效率较低，且现有研究多关注短期能源转换，忽视长期存储潜力。

Method: 通过三个逐步复杂的案例研究，评估Deep Q-Networks和Proximal Policy Optimization算法的性能，并引入预测整合、奖励函数惩罚和策略成本计算等改进。

Result: 改进后的DRL算法显著提升了P2G系统长期运营的成本效益策略制定能力。

Conclusion: 研究表明，改进的DRL方法能够有效解决P2G系统的延迟奖励问题，释放其长期储能潜力。

Abstract: Power-to-Gas (P2G) technologies gain recognition for enabling the integration
of intermittent renewables, such as wind and solar, into electricity grids.
However, determining the most cost-effective operation of these systems is
complex due to the volatile nature of renewable energy, electricity prices, and
loads. Additionally, P2G systems are less efficient in converting and storing
energy compared to battery energy storage systems (BESs), and the benefits of
converting electricity into gas are not immediately apparent. Deep
Reinforcement Learning (DRL) has shown promise in managing the operation of
energy systems amidst these uncertainties. Yet, DRL techniques face
difficulties with the delayed reward characteristic of P2G system operation.
Previous research has mostly focused on short-term studies that look at the
energy conversion process, neglecting the long-term storage capabilities of
P2G.
  This study presents a new method by thoroughly examining how DRL can be
applied to the economic operation of P2G systems, in combination with BESs and
gas turbines, over extended periods. Through three progressively more complex
case studies, we assess the performance of DRL algorithms, specifically Deep
Q-Networks and Proximal Policy Optimization, and introduce modifications to
enhance their effectiveness. These modifications include integrating forecasts,
implementing penalties on the reward function, and applying strategic cost
calculations, all aimed at addressing the issue of delayed rewards. Our
findings indicate that while DRL initially struggles with the complex
decision-making required for P2G system operation, the adjustments we propose
significantly improve its capability to devise cost-effective operation
strategies, thereby unlocking the potential for long-term energy storage in P2G
technologies.

</details>


### [719] [From Model-Based and Adaptive Control to Evolving Fuzzy Control](https://arxiv.org/abs/2506.06594)
*Daniel Leite,Igor Škrjanc,Fernando Gomide*

Main category: eess.SY

TL;DR: 本文回顾了模糊集理论60年来的发展，重点介绍了经典模糊和自适应建模与控制框架的核心贡献，并探讨了演化智能系统在模糊建模与控制中的优势与挑战。


<details>
  <summary>Details</summary>
Motivation: 纪念模糊集理论60周年，总结其历史发展及核心贡献，并探讨演化智能系统在非平稳环境中的应用。

Method: 通过回顾历史文献和框架，分析经典模糊和自适应建模与控制方法，并讨论演化智能系统的优势。

Result: 演化模糊系统在非平稳环境中表现出色，但仍面临安全性、可解释性和结构性演化等挑战。

Conclusion: 演化智能系统为模糊建模与控制提供了新的方向，未来需解决安全性、可解释性和结构性演化等问题。

Abstract: Evolving fuzzy systems build and adapt fuzzy models - such as predictors and
controllers - by incrementally updating their rule-base structure from data
streams. On the occasion of the 60-year anniversary of fuzzy set theory,
commemorated during the Fuzz-IEEE 2025 event, this brief paper revisits the
historical development and core contributions of classical fuzzy and adaptive
modeling and control frameworks. It then highlights the emergence and
significance of evolving intelligent systems in fuzzy modeling and control,
emphasizing their advantages in handling nonstationary environments. Key
challenges and future directions are discussed, including safety,
interpretability, and principled structural evolution.

</details>


### [720] [On the Generalization of Data-Assisted Control in port-Hamiltonian Systems (DAC-pH)](https://arxiv.org/abs/2506.07079)
*Mostafa Eslami,Maryam Babazadeh*

Main category: eess.SY

TL;DR: 本文提出了一种基于数据辅助控制（DAC）的混合控制框架，用于端口哈密顿系统，通过动态分解处理不确定性问题。


<details>
  <summary>Details</summary>
Motivation: 解决端口哈密顿系统中的参数和结构不确定性，同时保持系统固有结构。

Method: 将系统分解为右端（RHS）和左端（LHS），分别由非线性控制器和强化学习（RL）处理。

Result: 框架有效管理不确定性，提升性能、可解释性和安全性，并通过仿真验证。

Conclusion: 该混合方法为未来研究提供了理论和实践基础。

Abstract: This paper introduces a hypothetical hybrid control framework for
port-Hamiltonian (p$\mathcal{H}$) systems, employing a dynamic decomposition
based on Data-Assisted Control (DAC). The system's evolution is split into two
parts with fixed topology: Right-Hand Side (RHS)- an intrinsic Hamiltonian flow
handling worst-case parametric uncertainties, and Left-Hand Side (LHS)- a
dissipative/input flow addressing both structural and parametric uncertainties.
A virtual port variable $\Pi$ serves as the interface between these two
components. A nonlinear controller manages the intrinsic Hamiltonian flow,
determining a desired port control value $\Pi_c$. Concurrently, Reinforcement
Learning (RL) is applied to the dissipative/input flow to learn an agent for
providing optimal policy in mapping $\Pi_c$ to the actual system input. This
hybrid approach effectively manages RHS uncertainties while preserving the
system's inherent structure. Key advantages include adjustable performance via
LHS controller parameters, enhanced AI explainability and interpretability
through the port variable $\Pi$, the ability to guarantee safety and state
attainability with hard/soft constraints, reduced complexity in learning
hypothesis classes compared to end-to-end solutions, and improved
state/parameter estimation using LHS prior knowledge and system Hamiltonian to
address partial observability. The paper details the p$\mathcal{H}$
formulation, derives the decomposition, and presents the modular controller
architecture. Beyond design, crucial aspects of stability and robustness
analysis and synthesis are investigated, paving the way for deeper theoretical
investigations. An application example, a pendulum with nonlinear dynamics, is
simulated to demonstrate the approach's empirical and phenomenological benefits
for future research.

</details>


### [721] [Distributed Risk-Sensitive Safety Filters for Uncertain Discrete-Time Systems](https://arxiv.org/abs/2506.07347)
*Armin Lederer,Erfaun Noorani,Andreas Krause*

Main category: eess.SY

TL;DR: 提出了一种基于控制屏障函数（CBFs）的风险敏感安全过滤器，用于具有不确定动态的离散时间多智能体系统，确保在模型不确定性下的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 多智能体系统中集中协调不切实际时，确保安全性是一个重要挑战。

Method: 利用基于指数风险算子的集中化风险敏感安全条件，并引入两种分布式策略：最坏情况预测和接近已知安全策略。

Result: 通过数值评估验证了方法在保持安全性同时避免过度保守的有效性。

Conclusion: 该安全过滤器在多智能体系统中实现了安全性与灵活性的平衡。

Abstract: Ensuring safety in multi-agent systems is a significant challenge,
particularly in settings where centralized coordination is impractical. In this
work, we propose a novel risk-sensitive safety filter for discrete-time
multi-agent systems with uncertain dynamics that leverages control barrier
functions (CBFs) defined through value functions. Our approach relies on
centralized risk-sensitive safety conditions based on exponential risk
operators to ensure robustness against model uncertainties. We introduce a
distributed formulation of the safety filter by deriving two alternative
strategies: one based on worst-case anticipation and another on proximity to a
known safe policy. By allowing agents to switch between strategies, feasibility
can be ensured. Through detailed numerical evaluations, we demonstrate the
efficacy of our approach in maintaining safety without being overly
conservative.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [722] [GOLFer: Smaller LM-Generated Documents Hallucination Filter & Combiner for Query Expansion in Information Retrieval](https://arxiv.org/abs/2506.04762)
*Lingyuan Liu,Mengxiang Zhang*

Main category: cs.IR

TL;DR: GOLFer是一种利用小型开源语言模型进行查询扩展的新方法，通过过滤幻觉内容和组合文档来提升性能，解决了大型语言模型的高成本和计算强度问题。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）在查询扩展中表现优异，但成本高、计算强度大且难以普及，因此需要一种更高效且经济的方法。

Method: GOLFer包含两个模块：幻觉过滤器（检测并移除生成文档中的非事实和不一致内容）和文档组合器（通过权重向量平衡查询与过滤后内容的影响）。

Result: 在三个网络搜索和十个低资源数据集上的实验表明，GOLFer在使用小型语言模型时优于其他方法，并与大型语言模型方法保持竞争力。

Conclusion: GOLFer证明了小型语言模型在查询扩展中的潜力，提供了一种高效且经济的替代方案。

Abstract: Large language models (LLMs)-based query expansion for information retrieval
augments queries with generated hypothetical documents with LLMs. However, its
performance relies heavily on the scale of the language models (LMs),
necessitating larger, more advanced LLMs. This approach is costly,
computationally intensive, and often has limited accessibility. To address
these limitations, we introduce GOLFer - Smaller LMs-Generated Documents
Hallucination Filter & Combiner - a novel method leveraging smaller open-source
LMs for query expansion. GOLFer comprises two modules: a hallucination filter
and a documents combiner. The former detects and removes non-factual and
inconsistent sentences in generated documents, a common issue with smaller LMs,
while the latter combines the filtered content with the query using a weight
vector to balance their influence. We evaluate GOLFer alongside dominant
LLM-based query expansion methods on three web search and ten low-resource
datasets. Experimental results demonstrate that GOLFer consistently outperforms
other methods using smaller LMs, and maintains competitive performance against
methods using large-size LLMs, demonstrating its effectiveness.

</details>


### [723] [DISRetrieval: Harnessing Discourse Structure for Long Document Retrieval](https://arxiv.org/abs/2506.06313)
*Huiyao Chen,Yi Yang,Yinghui Li,Meishan Zhang,Min Zhang*

Main category: cs.IR

TL;DR: DISRetrieval是一种基于语言学话语结构的层次检索框架，通过利用修辞结构理论（RST）和自适应摘要技术，显著提升了长文档理解的效果。


<details>
  <summary>Details</summary>
Motivation: 现有方法未能捕捉文档的固有话语结构，限制了长文档理解的效果。

Method: 提出了一种层次检索框架，包括话语感知的文档组织、LLM增强的节点表示和层次证据检索机制。

Result: 在QASPER和QuALITY数据集上，DISRetrieval在检索指标和问答任务中表现优于现有方法。

Conclusion: 话语结构的引入显著提升了检索效果，验证了语言学信息在长文本理解中的重要性。

Abstract: Long document understanding has become increasingly crucial in natural
language processing, with retrieval-based methods emerging as a promising
solution to address the context length limitations of large language models
(LLMs). However, existing approaches either treat documents as flat sequences
or employ arbitrary chunking strategies, failing to capture the inherent
discourse structure that guides human comprehension. We present DISRetrieval, a
novel hierarchical retrieval framework that leverages linguistic discourse
structure to enhance long document understanding. Our approach introduces three
key innovations: (1) a discourse-aware document organization framework that
utilizes rhetorical structure theory (RST) to create sentence-level
hierarchical representations, preserving both semantic relationships and
natural document flow; (2) an LLM-enhanced node representation technique that
combines discourse structure with adaptive summarization to enrich tree nodes
with contextual information; and (3) a hierarchical evidence retrieval
mechanism that effectively selects relevant content while maintaining discourse
coherence. Through comprehensive experiments on QASPER and QuALITY datasets,
DISRetrieval demonstrates substantial improvements over existing methods in
both token-level retrieval metrics and downstream question answering tasks. Our
ablation studies confirm that incorporating discourse structure significantly
enhances retrieval effectiveness across different document lengths and query
types, validating the importance of linguistically-informed document
representation in long-text understanding. Our code and datasets are publicly
available at github/DreamH1gh/DISRetrieval to facilitate future research.

</details>


### [724] [A Reinforcement-Learning-Enhanced LLM Framework for Automated A/B Testing in Personalized Marketing](https://arxiv.org/abs/2506.06316)
*Haoyang Feng,Yanjun Dai,Yuan Gao*

Main category: cs.IR

TL;DR: 提出RL-LLM-AB测试框架，结合强化学习和LLM优化A/B测试，实现个性化营销。


<details>
  <summary>Details</summary>
Motivation: 解决个性化营销中A/B测试如何最大化用户响应的挑战。

Method: 基于预训练语言模型生成候选内容，动态融合用户画像和上下文，通过Actor-Critic结构实时选择内容版本，并利用增强记忆奖励估计器捕捉长期偏好。

Result: 在真实营销数据上优于传统A/B测试、Contextual Bandits和基准强化学习方法。

Conclusion: RL-LLM-ABTest框架有效提升A/B测试的个性化和自动化水平。

Abstract: For personalized marketing, a new challenge of how to effectively algorithm
the A/B testing to maximize user response is urgently to be overcome. In this
paper, we present a new approach, the RL-LLM-AB test framework, for using
reinforcement learning strategy optimization combined with LLM to automate and
personalize A/B tests. The RL-LLM-AB test is built upon the pre-trained
instruction-tuned language model. It first generates A/B versions of candidate
content variants using a Prompt-Conditioned Generator, and then dynamically
embeds and fuses the user portrait and the context of the current query with
the multi-modal perception module to constitute the current interaction state.
The content version is then selected in real-time through the policy
optimization module with an Actor-Critic structure, and long-term revenue is
estimated according to real-time feedback (such as click-through rate and
conversion rate). Furthermore, a Memory-Augmented Reward Estimator is embedded
into the framework to capture long-term user preference drift, which helps to
generalize policy across multiple users and content contexts. Numerical results
demonstrate the superiority of our proposed RL-LLM-ABTest over existing A/B
testing methods, including classical A/B testing, Contextual Bandits, and
benchmark reinforcement learning approaches on real-world marketing data.

</details>


### [725] [FinBERT2: A Specialized Bidirectional Encoder for Bridging the Gap in Finance-Specific Deployment of Large Language Models](https://arxiv.org/abs/2506.06335)
*Xuan Xu,Fufang Wen,Beilin Chu,Zhibing Fu,Qinhong Lin,Jiaqi Liu,Binjie Fei,Zhongliang Yang,Linna Zhou,Yu Li*

Main category: cs.IR

TL;DR: 论文介绍了FinBERT2，一种针对金融领域的双向编码器，解决了LLMs在金融应用中的局限性，并在分类、检索和主题建模任务中表现出色。


<details>
  <summary>Details</summary>
Motivation: LLMs在金融领域的实际应用中存在性能不足和资源消耗高的问题，尤其是在判别任务、生成任务和主题建模中表现不佳。

Method: 提出了FinBERT2，一个基于32b金融领域语料预训练的双向编码器，并开发了Fin-Labelers、Fin-Retrievers和Fin-TopicModel等变体。

Result: FinBERT2在金融分类任务中平均优于其他BERT变体和LLMs 9.7%-12.3%，在检索任务中优于开源和商业嵌入模型，并实现了更好的主题建模效果。

Conclusion: FinBERT2为金融领域提供了更高效的解决方案，弥补了LLMs的不足，并展示了在LLMs时代中BERT模型的实用价值。

Abstract: In natural language processing (NLP), the focus has shifted from encoder-only
tiny language models like BERT to decoder-only large language models(LLMs) such
as GPT-3. However, LLMs' practical application in the financial sector has
revealed three limitations: (1) LLMs often perform worse than fine-tuned BERT
on discriminative tasks despite costing much higher computational resources,
such as market sentiment analysis in financial reports; (2) Application on
generative tasks heavily relies on retrieval augmented generation (RAG) methods
to provide current and specialized information, with general retrievers showing
suboptimal performance on domain-specific retrieval tasks; (3) There are
additional inadequacies in other feature-based scenarios, such as topic
modeling. We introduce FinBERT2, a specialized bidirectional encoder pretrained
on a high-quality, financial-specific corpus of 32b tokens. This represents the
largest known Chinese financial pretraining corpus for models of this parameter
size. As a better backbone, FinBERT2 can bridge the gap in the
financial-specific deployment of LLMs through the following achievements: (1)
Discriminative fine-tuned models (Fin-Labelers) outperform other (Fin)BERT
variants by 0.4%-3.3% and leading LLMs by 9.7%-12.3% on average across five
financial classification tasks. (2) Contrastive fine-tuned models
(Fin-Retrievers) outperform both open-source (e.g., +6.8\% avg improvement over
BGE-base-zh) and proprietary (e.g., +4.2\% avg improvement over OpenAI's
text-embedding-3-large) embedders across five financial retrieval tasks; (3)
Building on FinBERT2 variants, we construct the Fin-TopicModel, which enables
superior clustering and topic representation for financial titles. Our work
revisits financial BERT models through comparative analysis with contemporary
LLMs and offers practical insights for effectively utilizing FinBERT in the
LLMs era.

</details>


### [726] [Optimizing RAG Pipelines for Arabic: A Systematic Analysis of Core Components](https://arxiv.org/abs/2506.06339)
*Jumana Alsubhi,Mohammad D. Alahmadi,Ahmed Alhusayni,Ibrahim Aldailami,Israa Hamdine,Ahmad Shabana,Yazeed Iskandar,Suhayb Khayyat*

Main category: cs.IR

TL;DR: 本文评估了阿拉伯语检索增强生成（RAG）组件的性能，发现句子感知分块、BGE-M3和Multilingual-E5-large嵌入模型效果最佳，且重排序器显著提升复杂数据集的忠实度。


<details>
  <summary>Details</summary>
Motivation: 针对阿拉伯语的RAG组件优化研究不足，本文旨在填补这一空白。

Method: 使用RAGAS框架，评估分块策略、嵌入模型、重排序器和语言模型在阿拉伯语数据集上的表现。

Result: 句子感知分块效果最佳，BGE-M3和Multilingual-E5-large嵌入模型表现最优，重排序器提升忠实度，Aya-8B生成质量优于StableLM。

Conclusion: 研究为构建高质量阿拉伯语RAG管道提供了关键见解和实用指南。

Abstract: Retrieval-Augmented Generation (RAG) has emerged as a powerful architecture
for combining the precision of retrieval systems with the fluency of large
language models. While several studies have investigated RAG pipelines for
high-resource languages, the optimization of RAG components for Arabic remains
underexplored. This study presents a comprehensive empirical evaluation of
state-of-the-art RAG components-including chunking strategies, embedding
models, rerankers, and language models-across a diverse set of Arabic datasets.
Using the RAGAS framework, we systematically compare performance across four
core metrics: context precision, context recall, answer faithfulness, and
answer relevancy. Our experiments demonstrate that sentence-aware chunking
outperforms all other segmentation methods, while BGE-M3 and
Multilingual-E5-large emerge as the most effective embedding models. The
inclusion of a reranker (bge-reranker-v2-m3) significantly boosts faithfulness
in complex datasets, and Aya-8B surpasses StableLM in generation quality. These
findings provide critical insights for building high-quality Arabic RAG
pipelines and offer practical guidelines for selecting optimal components
across different document types.

</details>


### [727] [Structured Semantics from Unstructured Notes: Language Model Approaches to EHR-Based Decision Support](https://arxiv.org/abs/2506.06340)
*Wu Hao Ran,Xi Xi,Furong Li,Jingyi Lu,Jian Jiang,Hui Huang,Yuzhuan Zhang,Shi Li*

Main category: cs.IR

TL;DR: 论文探讨了如何利用大型语言模型（LLMs）分析电子健康记录（EHRs）中的多样化数据，以提升临床决策支持。


<details>
  <summary>Details</summary>
Motivation: 电子健康记录（EHRs）包含丰富但多样化的数据，传统方法难以充分利用其文本特征。

Method: 应用高级语言模型整合EHRs中的文本、结构化数据和医疗代码。

Result: 文本特征能提供语义丰富的表示，并有助于跨机构数据协调。

Conclusion: 研究强调了整合多样化数据源的重要性，并探讨了AI模型在医疗领域的通用性和公平性挑战。

Abstract: The advent of large language models (LLMs) has opened new avenues for
analyzing complex, unstructured data, particularly within the medical domain.
Electronic Health Records (EHRs) contain a wealth of information in various
formats, including free text clinical notes, structured lab results, and
diagnostic codes. This paper explores the application of advanced language
models to leverage these diverse data sources for improved clinical decision
support. We will discuss how text-based features, often overlooked in
traditional high dimensional EHR analysis, can provide semantically rich
representations and aid in harmonizing data across different institutions.
Furthermore, we delve into the challenges and opportunities of incorporating
medical codes and ensuring the generalizability and fairness of AI models in
healthcare.

</details>


### [728] [NR4DER: Neural Re-ranking for Diversified Exercise Recommendation](https://arxiv.org/abs/2506.06341)
*Xinghe Cheng,Xufang Zhou,Liangda Fang,Chaobo He,Yuyu Zhou,Weiqi Luo,Zhiguo Gong,Quanlong Guan*

Main category: cs.IR

TL;DR: 论文提出NR4DER方法，通过改进练习推荐系统，解决高辍学率和学习节奏不匹配问题，显著提升推荐准确性和多样性。


<details>
  <summary>Details</summary>
Motivation: 在线教育平台中，现有练习推荐方法难以适应学生多样化的学习节奏，且对不活跃学生的学习模式调整不足，导致推荐效果有限。

Method: NR4DER结合mLSTM模型优化练习筛选模块，采用序列增强方法提升不活跃学生的表示，并通过神经重排序生成个性化推荐列表。

Result: 实验表明，NR4DER在多个真实数据集上表现优于现有方法，能有效适应学生多样化学习节奏。

Conclusion: NR4DER通过改进推荐系统，显著提升了练习推荐的准确性和多样性，适应了学生的个性化学习需求。

Abstract: With the widespread adoption of online education platforms, an increasing
number of students are gaining new knowledge through Massive Open Online
Courses (MOOCs). Exercise recommendation have made strides toward improving
student learning outcomes. However, existing methods not only struggle with
high dropout rates but also fail to match the diverse learning pace of
students. They frequently face difficulties in adjusting to inactive students'
learning patterns and in accommodating individualized learning paces, resulting
in limited accuracy and diversity in recommendations. To tackle these
challenges, we propose Neural Re-ranking for Diversified Exercise
Recommendation (in short, NR4DER). NR4DER first leverages the mLSTM model to
improve the effectiveness of the exercise filter module. It then employs a
sequence enhancement method to enhance the representation of inactive students,
accurately matches students with exercises of appropriate difficulty. Finally,
it utilizes neural re-ranking to generate diverse recommendation lists based on
individual students' learning histories. Extensive experimental results
indicate that NR4DER significantly outperforms existing methods across multiple
real-world datasets and effectively caters to the diverse learning pace of
students.

</details>


### [729] [Is BERTopic Better than PLSA for Extracting Key Topics in Aviation Safety Reports?](https://arxiv.org/abs/2506.06328)
*Aziida Nanyonga,Joiner Keith,Turhan Ugur,Wild Graham*

Main category: cs.IR

TL;DR: 比较BERTopic和PLSA在航空安全报告中的主题提取效果，BERTopic表现更优。


<details>
  <summary>Details</summary>
Motivation: 提升对航空事故数据模式的理解。

Method: 使用36,000+份NTSB报告，BERTopic基于Transformer和层次聚类，PLSA基于概率建模和EM算法。

Result: BERTopic在主题一致性和可解释性上优于PLSA（Cv分数0.41 vs 0.37）。

Conclusion: Transformer方法在复杂航空数据分析中更具优势，未来将探索混合模型和多语言数据集。

Abstract: This study compares the effectiveness of BERTopic and Probabilistic Latent
Semantic Analysis (PLSA) in extracting meaningful topics from aviation safety
reports aiming to enhance the understanding of patterns in aviation incident
data. Using a dataset of over 36,000 National Transportation Safety Board
(NTSB) reports from 2000 to 2020, BERTopic employed transformer based
embeddings and hierarchical clustering, while PLSA utilized probabilistic
modelling through the Expectation-Maximization (EM) algorithm. Results showed
that BERTopic outperformed PLSA in topic coherence, achieving a Cv score of
0.41 compared to PLSA 0.37, while also demonstrating superior interpretability
as validated by aviation safety experts. These findings underscore the
advantages of modern transformer based approaches in analyzing complex aviation
datasets, paving the way for enhanced insights and informed decision-making in
aviation safety. Future work will explore hybrid models, multilingual datasets,
and advanced clustering techniques to further improve topic modelling in this
domain.

</details>


### [730] [LlamaRec-LKG-RAG: A Single-Pass, Learnable Knowledge Graph-RAG Framework for LLM-Based Ranking](https://arxiv.org/abs/2506.07449)
*Vahid Azizi,Fatemeh Koochaki*

Main category: cs.IR

TL;DR: LlamaRec-LKG-RAG是一个新颖的端到端可训练框架，通过将个性化知识图谱整合到基于LLM的推荐排名中，显著提升了推荐系统的性能。


<details>
  <summary>Details</summary>
Motivation: 现有RAG方法主要依赖基于相似性的检索，未能充分利用用户-物品交互中的丰富关系结构。

Method: 扩展LlamaRec架构，引入轻量级用户偏好模块，动态识别异构知识图谱中的关键关系路径，并将其整合到Llama-2模型的提示中。

Result: 在ML-100K和Amazon Beauty数据集上，LlamaRec-LKG-RAG在MRR、NDCG和Recall等关键指标上显著优于LlamaRec。

Conclusion: LlamaRec-LKG-RAG证明了结构化推理在基于LLM的推荐中的重要性，为下一代推荐系统的可扩展、知识感知个性化奠定了基础。

Abstract: Recent advances in Large Language Models (LLMs) have driven their adoption in
recommender systems through Retrieval-Augmented Generation (RAG) frameworks.
However, existing RAG approaches predominantly rely on flat, similarity-based
retrieval that fails to leverage the rich relational structure inherent in
user-item interactions. We introduce LlamaRec-LKG-RAG, a novel single-pass,
end-to-end trainable framework that integrates personalized knowledge graph
context into LLM-based recommendation ranking. Our approach extends the
LlamaRec architecture by incorporating a lightweight user preference module
that dynamically identifies salient relation paths within a heterogeneous
knowledge graph constructed from user behavior and item metadata. These
personalized subgraphs are seamlessly integrated into prompts for a fine-tuned
Llama-2 model, enabling efficient and interpretable recommendations through a
unified inference step. Comprehensive experiments on ML-100K and Amazon Beauty
datasets demonstrate consistent and significant improvements over LlamaRec
across key ranking metrics (MRR, NDCG, Recall). LlamaRec-LKG-RAG demonstrates
the critical value of structured reasoning in LLM-based recommendations and
establishes a foundation for scalable, knowledge-aware personalization in
next-generation recommender systems. Code is available
at~\href{https://github.com/VahidAz/LlamaRec-LKG-RAG}{repository}.

</details>


### [731] [Preference-based learning for news headline recommendation](https://arxiv.org/abs/2506.06334)
*Alexandre Bouras,Audrey Durand,Richard Khoury*

Main category: cs.IR

TL;DR: 研究探讨了基于偏好的学习优化新闻标题推荐策略，发现显式探索在噪声环境中可能不必要。


<details>
  <summary>Details</summary>
Motivation: 探索如何通过偏好学习和上下文多臂老虎机优化新闻标题推荐，特别是翻译对用户参与度的影响。

Method: 使用法语在线新闻的用户交互数据，在上下文多臂老虎机框架下训练标题推荐代理。

Result: 结果表明，在噪声环境中，显式探索可能不必要，可采用更简单高效的策略。

Conclusion: 研究为实践中简化推荐策略提供了依据，特别是在噪声环境下。

Abstract: This study explores strategies for optimizing news headline recommendations
through preference-based learning. Using real-world data of user interactions
with French-language online news posts, we learn a headline recommender agent
under a contextual bandit setting. This allows us to explore the impact of
translation on engagement predictions, as well as the benefits of different
interactive strategies on user engagement during data collection. Our results
show that explicit exploration may not be required in the presence of noisy
contexts, opening the door to simpler but efficient strategies in practice.

</details>


### [732] [HotelMatch-LLM: Joint Multi-Task Training of Small and Large Language Models for Efficient Multimodal Hotel Retrieval](https://arxiv.org/abs/2506.07296)
*Arian Askari,Emmanouil Stergiadis,Ilya Gusev,Moran Beladev*

Main category: cs.IR

TL;DR: HotelMatch-LLM是一种多模态密集检索模型，用于旅游领域的自然语言酒店搜索，解决了传统搜索引擎的局限性。


<details>
  <summary>Details</summary>
Motivation: 传统旅游搜索引擎需要用户从目的地开始并调整搜索参数，限制了搜索的灵活性和自然性。

Method: 结合多任务优化、非对称密集检索架构和图像处理技术，实现了高效的酒店搜索。

Result: 在四个测试集上显著优于现有模型，如VISTA和MARVEL，主查询类型的性能达到0.681。

Conclusion: HotelMatch-LLM在多任务优化、通用性和可扩展性方面表现出色，适用于大规模图像库处理。

Abstract: We present HotelMatch-LLM, a multimodal dense retrieval model for the travel
domain that enables natural language property search, addressing the
limitations of traditional travel search engines which require users to start
with a destination and editing search parameters. HotelMatch-LLM features three
key innovations: (1) Domain-specific multi-task optimization with three novel
retrieval, visual, and language modeling objectives; (2) Asymmetrical dense
retrieval architecture combining a small language model (SLM) for efficient
online query processing and a large language model (LLM) for embedding hotel
data; and (3) Extensive image processing to handle all property image
galleries. Experiments on four diverse test sets show HotelMatch-LLM
significantly outperforms state-of-the-art models, including VISTA and MARVEL.
Specifically, on the test set -- main query type -- we achieve 0.681 for
HotelMatch-LLM compared to 0.603 for the most effective baseline, MARVEL. Our
analysis highlights the impact of our multi-task optimization, the
generalizability of HotelMatch-LLM across LLM architectures, and its
scalability for processing large image galleries.

</details>


### [733] [Infinity Search: Approximate Vector Search with Projections on q-Metric Spaces](https://arxiv.org/abs/2506.06557)
*Antonio Pariente,Ignacio Hounie,Santiago Segarra,Alejandro Ribeiro*

Main category: cs.IR

TL;DR: 论文提出了一种利用q-度量空间的投影方法，通过强化三角不等式减少向量搜索的比较次数，使搜索复杂度接近对数级别，并在实验中验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 当前向量搜索算法忽略了向量嵌入的度量结构，仅将其视为约束而非利用其潜在特性。

Method: 提出一种投影方法，将任意相异性度量的向量数据集嵌入q-度量空间，并学习近似投影以高效转换查询点。

Result: 实验表明，该方法使经典度量树算法在高维数据中表现优异，与最先进搜索方法竞争。

Conclusion: 通过利用q-度量空间的特性，可以显著提升向量搜索的效率与性能。

Abstract: Despite the ubiquity of vector search applications, prevailing search
algorithms overlook the metric structure of vector embeddings, treating it as a
constraint rather than exploiting its underlying properties. In this paper, we
demonstrate that in $q$-metric spaces, metric trees can leverage a stronger
version of the triangle inequality to reduce comparisons for exact search.
Notably, as $q$ approaches infinity, the search complexity becomes logarithmic.
Therefore, we propose a novel projection method that embeds vector datasets
with arbitrary dissimilarity measures into $q$-metric spaces while preserving
the nearest neighbor. We propose to learn an approximation of this projection
to efficiently transform query points to a space where euclidean distances
satisfy the desired properties. Our experimental results with text and image
vector embeddings show that learning $q$-metric approximations enables classic
metric tree algorithms -- which typically underperform with high-dimensional
data -- to achieve competitive performance against state-of-the-art search
methods.

</details>


### [734] [Correcting for Position Bias in Learning to Rank: A Control Function Approach](https://arxiv.org/abs/2506.06989)
*Md Aminul Islam,Kathryn Vasilaky,Elena Zheleva*

Main category: cs.IR

TL;DR: 提出了一种基于控制函数的两阶段方法，用于纠正隐式反馈数据中的位置偏差，无需点击或倾向模型知识，并能处理非线性排序模型。


<details>
  <summary>Details</summary>
Motivation: 隐式反馈数据（如用户点击）易受多种偏差影响，直接用于学习排序系统会导致次优性能，尤其是位置偏差问题突出。

Method: 采用两阶段过程：第一阶段利用排序过程的残差外生变量纠正第二阶段点击方程中的位置偏差，无需依赖点击或倾向模型。

Result: 实验表明，该方法在纠正位置偏差方面优于现有技术。

Conclusion: 该方法通用性强，可应用于任何先进排序算法，并提供了超参数调优的无偏验证点击技术。

Abstract: Implicit feedback data, such as user clicks, is commonly used in
learning-to-rank (LTR) systems because it is easy to collect and it often
reflects user preferences. However, this data is prone to various biases, and
training an LTR system directly on biased data can result in suboptimal ranking
performance. One of the most prominent and well-studied biases in implicit
feedback data is position bias, which occurs because users are more likely to
interact with higher-ranked documents regardless of their true relevance. In
this paper, we propose a novel control function-based method that accounts for
position bias in a two-stage process. The first stage uses exogenous variation
from the residuals of the ranking process to correct for position bias in the
second stage click equation. Unlike previous position bias correction methods,
our method does not require knowledge of the click or propensity model and
allows for nonlinearity in the underlying ranking model. Moreover, our method
is general and allows for debiasing any state-of-the-art ranking algorithm by
plugging it into the second stage. We also introduce a technique to debias
validation clicks for hyperparameter tuning to select the optimal model in the
absence of unbiased validation data. Experimental results demonstrate that our
method outperforms state-of-the-art approaches in correcting for position bias.

</details>


### [735] [RADAR: Recall Augmentation through Deferred Asynchronous Retrieval](https://arxiv.org/abs/2506.07261)
*Amit Jaspal,Qian Dang,Ajantha Ramineni*

Main category: cs.IR

TL;DR: RADAR框架通过异步离线计算提升推荐系统的召回率，显著提高推荐质量。


<details>
  <summary>Details</summary>
Motivation: 解决大规模推荐系统中初始检索阶段难以从海量数据中精准筛选高吸引力内容的问题。

Method: 引入RADAR框架，利用异步离线计算预排名更大候选集，绕过在线检索和预排名阶段。

Result: 离线实验显示召回率提升2倍，在线A/B测试验证了0.8%的参与度提升。

Conclusion: RADAR是一种在严格在线服务约束下提升推荐质量的有效方法。

Abstract: Modern large-scale recommender systems employ multi-stage ranking funnel
(Retrieval, Pre-ranking, Ranking) to balance engagement and computational
constraints (latency, CPU). However, the initial retrieval stage, often relying
on efficient but less precise methods like K-Nearest Neighbors (KNN), struggles
to effectively surface the most engaging items from billion-scale catalogs,
particularly distinguishing highly relevant and engaging candidates from merely
relevant ones. We introduce Recall Augmentation through Deferred Asynchronous
Retrieval (RADAR), a novel framework that leverages asynchronous, offline
computation to pre-rank a significantly larger candidate set for users using
the full complexity ranking model. These top-ranked items are stored and
utilized as a high-quality retrieval source during online inference, bypassing
online retrieval and pre-ranking stages for these candidates. We demonstrate
through offline experiments that RADAR significantly boosts recall (2X
Recall@200 vs DNN retrieval baseline) by effectively combining a larger
retrieved candidate set with a more powerful ranking model. Online A/B tests
confirm a +0.8% lift in topline engagement metrics, validating RADAR as a
practical and effective method to improve recommendation quality under strict
online serving constraints.

</details>


### [736] [MoE-MLoRA for Multi-Domain CTR Prediction: Efficient Adaptation with Expert Specialization](https://arxiv.org/abs/2506.07563)
*Ken Yagel,Eyal German,Aviel Ben Siman Tov*

Main category: cs.IR

TL;DR: MoE-MLoRA是一种基于专家混合的推荐系统框架，通过动态加权专家贡献提升多领域推荐性能，在大规模动态数据中表现优异，但在低多样性数据中效果有限。


<details>
  <summary>Details</summary>
Motivation: 传统方法如MLoRA在跨领域用户行为处理上缺乏灵活性，需要更灵活的框架以适应多样化的用户行为。

Method: 提出MoE-MLoRA框架，先独立训练专家以专精于各自领域，再训练门控网络动态加权专家贡献。

Result: 在Movielens和Taobao数据集上，MoE-MLoRA在大规模动态数据中表现优异（Taobao-20上加权AUC提升1.45），但在低多样性数据中效果有限。

Conclusion: 专家混合架构在多领域推荐系统中具有潜力，任务感知的专家专精和自适应门控能提升复杂环境下的预测准确性。

Abstract: Personalized recommendation systems must adapt to user interactions across
different domains. Traditional approaches like MLoRA apply a single adaptation
per domain but lack flexibility in handling diverse user behaviors. To address
this, we propose MoE-MLoRA, a mixture-of-experts framework where each expert is
first trained independently to specialize in its domain before a gating network
is trained to weight their contributions dynamically. We evaluate MoE-MLoRA
across eight CTR models on Movielens and Taobao, showing that it improves
performance in large-scale, dynamic datasets (+1.45 Weighed-AUC in Taobao-20)
but offers limited benefits in structured datasets with low domain diversity
and sparsity. Further analysis of the number of experts per domain reveals that
larger ensembles do not always improve performance, indicating the need for
model-aware tuning. Our findings highlight the potential of expert-based
architectures for multi-domain recommendation systems, demonstrating that
task-aware specialization and adaptive gating can enhance predictive accuracy
in complex environments. The implementation and code are available in our
GitHub repository.

</details>


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [737] [Benchmarking Early Agitation Prediction in Community-Dwelling People with Dementia Using Multimodal Sensors and Machine Learning](https://arxiv.org/abs/2506.06306)
*Ali Abedi,Charlene H. Chu,Shehroz S. Khan*

Main category: eess.SP

TL;DR: 该研究开发并评估了机器学习方法，利用多模态传感器数据预测社区居住的老年痴呆患者的激越行为，提出了一种新的基于活动数据的特征集，并在最大公开数据集上进行了验证。


<details>
  <summary>Details</summary>
Motivation: 激越行为是老年痴呆患者常见的反应行为，及时预测可以早期干预，减轻护理负担，提高生活质量。

Method: 研究引入了基于活动数据的激越相关特征，评估了多种机器学习和深度学习模型，包括二进制分类和异常检测。

Result: 最佳模型在二进制分类中实现了AUC-ROC为0.9720和AUC-PR为0.4320。

Conclusion: 该方法为基于隐私保护传感器数据的激越预测提供了准确、可解释且高效的解决方案，支持主动护理和居家养老。

Abstract: Agitation is one of the most common responsive behaviors in people living
with dementia, particularly among those residing in community settings without
continuous clinical supervision. Timely prediction of agitation can enable
early intervention, reduce caregiver burden, and improve the quality of life
for both patients and caregivers. This study aimed to develop and benchmark
machine learning approaches for the early prediction of agitation in
community-dwelling older adults with dementia using multimodal sensor data. A
new set of agitation-related contextual features derived from activity data was
introduced and employed for agitation prediction. A wide range of machine
learning and deep learning models was evaluated across multiple problem
formulations, including binary classification for single-timestamp tabular
sensor data and multi-timestamp sequential sensor data, as well as anomaly
detection for single-timestamp tabular sensor data. The study utilized the
Technology Integrated Health Management (TIHM) dataset, the largest publicly
available dataset for remote monitoring of people living with dementia,
comprising 2,803 days of in-home activity, physiology, and sleep data. The most
effective setting involved binary classification of sensor data using the
current 6-hour timestamp to predict agitation at the subsequent timestamp.
Incorporating additional information, such as time of day and agitation
history, further improved model performance, with the highest AUC-ROC of 0.9720
and AUC-PR of 0.4320 achieved by the light gradient boosting machine. This work
presents the first comprehensive benchmarking of state-of-the-art techniques
for agitation prediction in community-based dementia care using
privacy-preserving sensor data. The approach enables accurate, explainable, and
efficient agitation prediction, supporting proactive dementia care and aging in
place.

</details>


### [738] [MoE-Gyro: Self-Supervised Over-Range Reconstruction and Denoising for MEMS Gyroscopes](https://arxiv.org/abs/2506.06318)
*Feiyang Pan,Shenghe Zheng,Chunyan Yin,Guangbin Dou*

Main category: eess.SP

TL;DR: MoE-Gyro是一种自监督框架，通过混合专家模型（ORE和DE）同时解决MEMS陀螺仪的过范围信号重建和噪声抑制问题，显著提升性能。


<details>
  <summary>Details</summary>
Motivation: MEMS陀螺仪在测量范围和噪声性能之间存在固有折衷，现有硬件和深度学习方法难以解决此问题。

Method: MoE-Gyro采用两个专家模型（ORE和DE）和一个轻量级门控模块，分别处理过范围信号和噪声抑制。

Result: 实验表明，MoE-Gyro将可测量范围从450 deg/s扩展到1500 deg/s，并将偏置不稳定性降低98.4%。

Conclusion: MoE-Gyro有效解决了MEMS陀螺仪的长期折衷问题，并提出了新的评估标准ISEBench。

Abstract: MEMS gyroscopes play a critical role in inertial navigation and motion
control applications but typically suffer from a fundamental trade-off between
measurement range and noise performance. Existing hardware-based solutions
aimed at mitigating this issue introduce additional complexity, cost, and
scalability challenges. Deep-learning methods primarily focus on noise
reduction and typically require precisely aligned ground-truth signals, making
them difficult to deploy in practical scenarios and leaving the fundamental
trade-off unresolved. To address these challenges, we introduce Mixture of
Experts for MEMS Gyroscopes (MoE-Gyro), a novel self-supervised framework
specifically designed for simultaneous over-range signal reconstruction and
noise suppression. MoE-Gyro employs two experts: an Over-Range Reconstruction
Expert (ORE), featuring a Gaussian-Decay Attention mechanism for reconstructing
saturated segments; and a Denoise Expert (DE), utilizing dual-branch
complementary masking combined with FFT-guided augmentation for robust noise
reduction. A lightweight gating module dynamically routes input segments to the
appropriate expert. Furthermore, existing evaluation lack a comprehensive
standard for assessing multi-dimensional signal enhancement. To bridge this
gap, we introduce IMU Signal Enhancement Benchmark (ISEBench), an open-source
benchmarking platform comprising the GyroPeak-100 dataset and a unified
evaluation of IMU signal enhancement methods. We evaluate MoE-Gyro using our
proposed ISEBench, demonstrating that our framework significantly extends the
measurable range from 450 deg/s to 1500 deg/s, reduces Bias Instability by
98.4%, and achieves state-of-the-art performance, effectively addressing the
long-standing trade-off in inertial sensing.

</details>


### [739] [A Reinforcement Learning Approach for RIS-aided Fair Communications](https://arxiv.org/abs/2506.06344)
*Alex Pierron,Michel Barbeau,Luca De Cicco,Jose Rubio-Hernan,Joaquin Garcia-Alfaro*

Main category: eess.SP

TL;DR: 本文提出了一种结合可重构智能表面（RIS）和强化学习（RL）的新方法，旨在实现高效且公平的多用户通信系统。


<details>
  <summary>Details</summary>
Motivation: 为了解决RIS在通信中可能导致的用户间信号强度不公平问题，研究提出了一种优化方法。

Method: 结合RIS和RL技术，提出了一种新颖的优化方法，确保多用户通信的公平性和效率。

Result: 实验和仿真结果表明，该方法在保证网络性能和能源效率的同时，实现了用户间的公平通信。

Conclusion: 本文提出的RIS-RL系统为多用户通信提供了一种高效且公平的解决方案，并公开了代码和数据集以促进进一步研究。

Abstract: Reconfigurable Intelligent Surfaces (RISs) are composed of physical elements
that can dynamically alter electromagnetic wave properties to enhance
beamforming and leading to improvements in areas with low coverage properties.
They have the potential to be combined with Reinforcement Learning (RL)
techniques to achieve network performance and energy efficiency via
optimization techniques. In addition to performance and energy improvements, it
is also crucial to consider the concept of fair communications. RISs must
ensure that User Equipment (UE) units receive their signals with adequate
strength, without other UE being deprived of service due to insufficient power.
In this paper, we address such a problem. We explore the fairness properties of
previous work and propose a novel method that aims at obtaining an efficient
and fair duplex RIS-RL system for multiple legitimate UE units. We report and
discuss our experimental work and simulation results. We also release our code
and datasets to foster further research in the topic.

</details>


### [740] [Deep learning methods for modeling infrasound transmission loss in the middle atmosphere](https://arxiv.org/abs/2506.06351)
*Alexis Le Pichon,Alice Janela Cameijo,Samir Aknine,Youcef Sklab,Souhila Arib,Quentin Brissaud,Sven Peter Naesholm*

Main category: eess.SP

TL;DR: 该研究优化了卷积神经网络，用于快速预测全球范围内的次声传输损失（TLs），解决了之前方法在高频和不利风条件下的局限性，平均误差为8.6 dB。


<details>
  <summary>Details</summary>
Motivation: 准确建模次声传输损失对国际监测系统性能评估至关重要，但现有方法计算成本高或预测效果不佳。

Method: 开发了一种优化的卷积神经网络，利用全球模拟的温度和风场数据预测TLs，改进了网络架构。

Result: 优化模型在0.1-3.2 Hz频段内平均误差为8.6 dB，适用于实际大气场景。

Conclusion: 该方法显著提升了TLs预测的效率和准确性，适用于大范围次声监测。

Abstract: Accurate modeling of infrasound transmission losses (TLs) is essential to
assess the performance of the global International Monitoring System infrasound
network. Among existing propagation modeling tools, parabolic equation (PE)
method enables TLs to be finely modeled, but its computational cost does not
allow exploration of a large parameter space for operational monitoring
applications. To reduce computation times, Brissaud et al. 2023 explored the
potential of convolutional neural networks trained on a large set of regionally
simulated wavefields (< 1000 km from the source) to predict TLs with negligible
computation times compared to PE simulations. However, this method struggles in
unfavorable initial wind conditions, especially at high frequencies, and causal
issues with winds at large distances from the source affecting ground TLs close
to the source. In this study, we have developed an optimized convolutional
network designed to minimize prediction errors while predicting TLs from
globally simulated combined temperature and wind fields spanning over
propagation ranges of 4000 km. Our approach enhances the previously proposed
one by implementing key optimizations that improve the overall architecture
performance. The implemented model predicts TLs with an average error of 8.6 dB
in the whole frequency band (0.1-3.2 Hz) and explored realistic atmospheric
scenarios.

</details>


### [741] [Large Language Models for EEG: A Comprehensive Survey and Taxonomy](https://arxiv.org/abs/2506.06353)
*Naseem Babu,Jimson Mathew,A. P. Vinod*

Main category: eess.SP

TL;DR: 本文综述了大型语言模型（LLMs）与脑电图（EEG）研究的结合，系统整理了相关文献，并提出了一个分类框架，涵盖四个领域：EEG表示学习、EEG到语言解码、跨模态生成以及临床应用。


<details>
  <summary>Details</summary>
Motivation: 探索LLMs与EEG研究的结合，以推动神经解码、脑机接口和情感计算的新方向。

Method: 通过系统综述和分类，整理了LLMs在EEG分析中的应用，包括微调、少样本和零样本学习等技术。

Result: 总结了基于变压器的架构在EEG任务中的表现，如自然语言生成、语义解释和诊断辅助。

Conclusion: 本文为未来通过语言模型连接自然语言处理和神经信号分析提供了基础资源。

Abstract: The growing convergence between Large Language Models (LLMs) and
electroencephalography (EEG) research is enabling new directions in neural
decoding, brain-computer interfaces (BCIs), and affective computing. This
survey offers a systematic review and structured taxonomy of recent
advancements that utilize LLMs for EEG-based analysis and applications. We
organize the literature into four domains: (1) LLM-inspired foundation models
for EEG representation learning, (2) EEG-to-language decoding, (3) cross-modal
generation including image and 3D object synthesis, and (4) clinical
applications and dataset management tools. The survey highlights how
transformer-based architectures adapted through fine-tuning, few-shot, and
zero-shot learning have enabled EEG-based models to perform complex tasks such
as natural language generation, semantic interpretation, and diagnostic
assistance. By offering a structured overview of modeling strategies, system
designs, and application areas, this work serves as a foundational resource for
future work to bridge natural language processing and neural signal analysis
through language models.

</details>


### [742] [Towards real-time assessment of infrasound event detection capability using deep learning-based transmission loss estimation](https://arxiv.org/abs/2506.06358)
*Alice Janela Cameijo,Alexis Le Pichon,Youcef Sklab,Souhila Arib,Quentin Brissaud,Sven peter Naesholm,Constantino Listowski,Samir Aknine*

Main category: eess.SP

TL;DR: 该研究通过结合风和温度场作为神经网络输入，优化网络架构，显著提升了次声传输损失的预测精度，适用于长距离传播和实时监测。


<details>
  <summary>Details</summary>
Motivation: 解决现有深度学习算法因不完全大气模型和缺少温度输入而无法有效预测长距离次声传播的问题。

Method: 使用风和温度场作为输入，模拟至130公里高度和4000公里距离，优化神经网络架构，结合卷积和循环层捕捉空间和距离特征。

Result: 神经网络平均误差为4 dB，提供不确定度估计，并在未训练的大气条件和频率下验证了预测能力。

Conclusion: 该研究为实时评估国际监测系统对爆炸源的检测阈值迈出了重要一步。

Abstract: Accurate modeling of infrasound transmission loss is essential for evaluating
the performance of the International Monitoring System, enabling the effective
design and maintenance of infrasound stations to support compliance of the
Comprehensive Nuclear-Test-Ban Treaty. State-of-the-art propagation modeling
tools enable transmission loss to be finely simulated using atmospheric models.
However, the computational cost prohibits the exploration of a large parameter
space in operational monitoring applications. To address this, recent studies
made use of a deep learning algorithm capable of making transmission loss
predictions almost instantaneously. However, the use of nudged atmospheric
models leads to an incomplete representation of the medium, and the absence of
temperature as an input makes the algorithm incompatible with long range
propagation. In this study, we address these limitations by using both wind and
temperature fields as inputs to a neural network, simulated up to 130 km
altitude and 4,000 km distance. We also optimize several aspects of the neural
network architecture. We exploit convolutional and recurrent layers to capture
spatially and range-dependent features embedded in realistic atmospheric
models, improving the overall performance. The neural network reaches an
average error of 4 dB compared to full parabolic equation simulations and
provides epistemic and data-related uncertainty estimates. Its evaluation on
the 2022 Hunga Tonga-Hunga Ha'apai volcanic eruption demonstrates its
prediction capability using atmospheric conditions and frequencies not included
in the training. This represents a significant step towards near real-time
assessment of International Monitoring System detection thresholds of explosive
sources.

</details>


### [743] [Model-based Neural Data Augmentation for sub-wavelength Radio Localization](https://arxiv.org/abs/2506.06387)
*Baptiste Chatelier,Vincent Corlay,Musa Furkan Keskin,Matthieu Crussière,Henk Wymeersch,Luc Le Magoarou*

Main category: eess.SP

TL;DR: 论文提出了一种基于模型神经网络的指纹定位方法，显著提高了复杂无线环境中的定位精度，同时降低了内存需求。


<details>
  <summary>Details</summary>
Motivation: 传统信号处理技术在复杂无线环境（尤其是非视距传播路径主导的场景）中定位精度下降，而现有机器学习方法计算复杂度高。

Method: 使用模型神经网络学习位置到信道的映射，作为生成神经信道模型，增强指纹比较字典并减少内存需求。

Result: 在非视距环境中实现亚波长级定位精度，定位精度提升数个数量级，内存需求降低一个数量级。

Conclusion: 该方法在复杂无线环境中显著提升了定位性能，同时优化了资源使用。

Abstract: The increasing deployment of large antenna arrays at base stations has
significantly improved the spatial resolution and localization accuracy of
radio-localization methods. However, traditional signal processing techniques
struggle in complex radio environments, particularly in scenarios dominated by
non line of sight (NLoS) propagation paths, resulting in degraded localization
accuracy. Recent developments in machine learning have facilitated the
development of machine learning-assisted localization techniques, enhancing
localization accuracy in complex radio environments. However, these methods
often involve substantial computational complexity during both the training and
inference phases. This work extends the well-established fingerprinting-based
localization framework by simultaneously reducing its memory requirements and
improving its accuracy. Specifically, a model-based neural network is used to
learn the location-to-channel mapping, and then serves as a generative neural
channel model. This generative model augments the fingerprinting comparison
dictionary while reducing the memory requirements. The proposed method
outperforms fingerprinting baselines by achieving sub-wavelength localization
accuracy, even in NLoS environments. Remarkably, it offers an improvement by
several orders of magnitude in localization accuracy, while simultaneously
reducing memory requirements by an order of magnitude compared to classical
fingerprinting methods.

</details>


### [744] [An Open-Source Python Framework and Synthetic ECG Image Datasets for Digitization, Lead and Lead Name Detection, and Overlapping Signal Segmentation](https://arxiv.org/abs/2506.06315)
*Masoud Rahimi,Reza Karbasi,Abdol-Hossein Vahabie*

Main category: eess.SP

TL;DR: 开源Python框架生成合成ECG图像数据集，支持ECG数字化、导联区域和名称检测、波形分割等任务。


<details>
  <summary>Details</summary>
Motivation: 推动基于深度学习的ECG分析任务，如ECG数字化、导联检测和波形分割。

Method: 利用PTB-XL信号数据集生成四种开放数据集，包括配对的ECG图像与时间序列信号、YOLO格式标注的导联区域和名称图像、单导联分割掩码图像。

Result: 提供四种公开数据集，支持多种ECG分析任务，框架和数据集已开源。

Conclusion: 该框架和数据集为ECG分析任务提供了重要资源，有助于深度学习模型的开发和应用。

Abstract: We introduce an open-source Python framework for generating synthetic ECG
image datasets to advance critical deep learning-based tasks in ECG analysis,
including ECG digitization, lead region and lead name detection, and
pixel-level waveform segmentation. Using the PTB-XL signal dataset, our
proposed framework produces four open-access datasets: (1) ECG images in
various lead configurations paired with time-series signals for ECG
digitization, (2) ECG images annotated with YOLO-format bounding boxes for
detection of lead region and lead name, (3)-(4) cropped single-lead images with
segmentation masks compatible with U-Net-based models in normal and overlapping
versions. In the overlapping case, waveforms from neighboring leads are
superimposed onto the target lead image, while the segmentation masks remain
clean. The open-source Python framework and datasets are publicly available at
https://github.com/rezakarbasi/ecg-image-and-signal-dataset and
https://doi.org/10.5281/zenodo.15484519, respectively.

</details>


### [745] [Heart Rate Classification in ECG Signals Using Machine Learning and Deep Learning](https://arxiv.org/abs/2506.06349)
*Thien Nhan Vo,Thanh Xuan Truong*

Main category: eess.SP

TL;DR: 该研究通过传统机器学习和深度学习方法对ECG信号的心跳进行分类，发现基于手工特征的LightGBM模型表现最佳。


<details>
  <summary>Details</summary>
Motivation: 解决ECG信号中心跳分类的问题，比较传统手工特征与深度学习图像转换方法的性能差异。

Method: 1. 传统机器学习：提取HRV、均值、方差和RR间期等特征，训练多种分类器。2. 深度学习方法：将ECG信号转换为图像（GAF、MTF、RP），并用CNN分类。

Result: LightGBM模型表现最优（准确率99%，F1分数0.94），优于基于图像的CNN方法（F1分数0.85）。SVM和AdaBoost表现较差。

Conclusion: 手工特征能更好地捕捉ECG信号的时态和形态变化，未来可结合多导联信号和连续心跳的时态依赖进一步提升分类精度。

Abstract: This study addresses the classification of heartbeats from ECG signals
through two distinct approaches: traditional machine learning utilizing
hand-crafted features and deep learning via transformed images of ECG beats.
The dataset underwent preprocessing steps, including downsampling, filtering,
and normalization, to ensure consistency and relevance for subsequent analysis.
In the first approach, features such as heart rate variability (HRV), mean,
variance, and RR intervals were extracted to train various classifiers,
including SVM, Random Forest, AdaBoost, LSTM, Bi-directional LSTM, and
LightGBM. The second approach involved transforming ECG signals into images
using Gramian Angular Field (GAF), Markov Transition Field (MTF), and
Recurrence Plots (RP), with these images subsequently classified using CNN
architectures like VGG and Inception.
  Experimental results demonstrate that the LightGBM model achieved the highest
performance, with an accuracy of 99% and an F1 score of 0.94, outperforming the
image-based CNN approach (F1 score of 0.85). Models such as SVM and AdaBoost
yielded significantly lower scores, indicating limited suitability for this
task. The findings underscore the superior ability of hand-crafted features to
capture temporal and morphological variations in ECG signals compared to
image-based representations of individual beats. Future investigations may
benefit from incorporating multi-lead ECG signals and temporal dependencies
across successive beats to enhance classification accuracy further.

</details>


### [746] [Leveraging Novel Ensemble Learning Techniques and Landsat Multispectral Data for Estimating Olive Yields in Tunisia](https://arxiv.org/abs/2506.06309)
*Mohamed Kefi,Tien Dat Pham,Thin Nguyen,Mark G. Tjoelker,Viola Devasirvatham,Kenichi Kashiwagi*

Main category: eess.SP

TL;DR: 该研究开发了一种基于遥感数据和机器学习的橄榄产量估算方法，结合多光谱反射带、植被指数和数字高程模型数据，通过AutoGluon实现自动化集成学习框架，预测性能优异。


<details>
  <summary>Details</summary>
Motivation: 气候变化导致橄榄产量波动显著，传统估算方法复杂且不准确，需要一种高效、可扩展的解决方案。

Method: 利用Landsat-8和Landsat-9卫星数据提取特征，结合地面调查数据，使用AutoGluon训练集成模型，并通过五折交叉验证优化预测。

Result: Landsat-8 OLI的R2为0.8635，RMSE为1.17吨/公顷；Landsat-9 OLI-2的R2为0.8378，RMSE为1.32吨/公顷。

Conclusion: 该方法成本低、精度高，具有全球农业区域的潜在适用性。

Abstract: Olive production is an important tree crop in Mediterranean climates.
However, olive yield varies significantly due to climate change. Accurately
estimating yield using remote sensing and machine learning remains a complex
challenge. In this study, we developed a streamlined pipeline for olive yield
estimation in the Kairouan and Sousse governorates of Tunisia. We extracted
features from multispectral reflectance bands, vegetation indices derived from
Landsat-8 OLI and Landsat-9 OLI-2 satellite imagery, along with digital
elevation model data. These spatial features were combined with ground-based
field survey data to form a structured tabular dataset. We then developed an
automated ensemble learning framework, implemented using AutoGluon to train and
evaluate multiple machine learning models, select optimal combinations through
stacking, and generate robust yield predictions using five-fold
cross-validation. The results demonstrate strong predictive performance from
both sensors, with Landsat-8 OLI achieving R2 = 0.8635 and RMSE = 1.17 tons per
ha, and Landsat-9 OLI-2 achieving R2 = 0.8378 and RMSE = 1.32 tons per ha. This
study highlights a scalable, cost-effective, and accurate method for olive
yield estimation, with potential applicability across diverse agricultural
regions globally.

</details>


### [747] [Enhancing Contrastive Learning-based Electrocardiogram Pretrained Model with Patient Memory Queue](https://arxiv.org/abs/2506.06310)
*Xiaoyu Sun,Yang Yang,Xunde Dong*

Main category: eess.SP

TL;DR: 论文提出了一种基于对比学习的ECG预训练模型，通过引入患者记忆队列（PMQ）和数据增强方法，解决了现有方法中患者一致性利用不足的问题。


<details>
  <summary>Details</summary>
Motivation: 在自动心电图（ECG）诊断领域，标记数据有限，如何基于未标记数据构建鲁棒的预训练模型是关键。现有方法在利用患者一致性方面效率不足。

Method: 提出了一种结合患者记忆队列（PMQ）的对比学习模型，并引入两种数据增强方法，以优化正负样本对的生成。

Result: 在三个公共数据集上的实验表明，该方法性能优于现有对比学习方法，且在标记数据有限时更具鲁棒性。

Conclusion: PMQ和数据增强方法显著提升了ECG预训练模型的性能，为有限标记数据场景提供了更优解决方案。

Abstract: In the field of automatic Electrocardiogram (ECG) diagnosis, due to the
relatively limited amount of labeled data, how to build a robust ECG pretrained
model based on unlabeled data is a key area of focus for researchers. Recent
advancements in contrastive learning-based ECG pretrained models highlight the
potential of exploiting the additional patient-level self-supervisory signals
inherent in ECG. They are referred to as patient contrastive learning. Its
rationale is that multiple physical recordings from the same patient may share
commonalities, termed patient consistency, so redefining positive and negative
pairs in contrastive learning as intrapatient and inter-patient samples
provides more shared context to learn an effective representation. However,
these methods still fail to efficiently exploit patient consistency due to the
insufficient amount of intra-inter patient samples existing in a batch. Hence,
we propose a contrastive learning-based ECG pretrained model enhanced by the
Patient Memory Queue (PMQ), which incorporates a large patient memory queue to
mitigate model degeneration that can arise from insufficient intra-inter
patient samples. In order to further enhance the performance of the pretrained
model, we introduce two extra data augmentation methods to provide more
perspectives of positive and negative pairs for pretraining. Extensive
experiments were conducted on three public datasets with three different data
ratios. The experimental results show that the comprehensive performance of our
method outperforms previous contrastive learning methods and exhibits greater
robustness in scenarios with limited labeled data. The code is available at
https://github.com/3hiuwoo/PMQ.

</details>


### [748] [A Novel Shape-Aware Topological Representation for GPR Data with DNN Integration](https://arxiv.org/abs/2506.06311)
*Meiyan Kang,Shizuo Kaji,Sang-Yun Lee,Taegon Kim,Hee-Hwan Ryu,Suyoung Choi*

Main category: eess.SP

TL;DR: 该研究提出了一种结合拓扑数据分析和YOLOv5深度学习网络的框架，用于增强地下管线的检测能力，并通过Sim2Real策略解决数据稀缺问题，实验结果显示其显著提升了检测精度。


<details>
  <summary>Details</summary>
Motivation: 传统的地下探测方法受限于噪声敏感性和缺乏结构感知能力，因此需要一种更可靠的方法来提升地下管线的检测效果。

Method: 研究提出了一种基于B-scan GPR图像的形状感知拓扑特征表示方法，结合YOLOv5深度学习网络，并通过Sim2Real策略生成合成数据集。

Result: 实验结果表明，该方法在平均精度（mAP）上有显著提升，验证了其鲁棒性和有效性。

Conclusion: 该研究展示了拓扑数据分析和深度学习结合在实时地下物体检测中的潜力，适用于城市规划、安全检查和基础设施管理等领域。

Abstract: Ground Penetrating Radar (GPR) is a widely used Non-Destructive Testing (NDT)
technique for subsurface exploration, particularly in infrastructure inspection
and maintenance. However, conventional interpretation methods are often limited
by noise sensitivity and a lack of structural awareness. This study presents a
novel framework that enhances the detection of underground utilities,
especially pipelines, by integrating shape-aware topological features derived
from B-scan GPR images using Topological Data Analysis (TDA), with the spatial
detection capabilities of the YOLOv5 deep neural network (DNN). We propose a
novel shape-aware topological representation that amplifies structural features
in the input data, thereby improving the model's responsiveness to the
geometrical features of buried objects. To address the scarcity of annotated
real-world data, we employ a Sim2Real strategy that generates diverse and
realistic synthetic datasets, effectively bridging the gap between simulated
and real-world domains. Experimental results demonstrate significant
improvements in mean Average Precision (mAP), validating the robustness and
efficacy of our approach. This approach underscores the potential of
TDA-enhanced learning in achieving reliable, real-time subsurface object
detection, with broad applications in urban planning, safety inspection, and
infrastructure management.

</details>


### [749] [Composite Reward Design in PPO-Driven Adaptive Filtering](https://arxiv.org/abs/2506.06323)
*Abdullah Burkan Bereketoglu*

Main category: eess.SP

TL;DR: 提出了一种基于PPO的自适应滤波框架，用于动态非平稳环境中的信号去噪，优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 传统滤波器（如LMS、RLS等）在非平稳环境中表现受限，需复杂调参或精确噪声统计。

Method: 使用PPO算法，通过复合奖励（SNR提升、MSE降低和残差平滑）指导滤波。

Result: 在合成信号实验中，PPO滤波器表现优于传统方法，并具有实时性和泛化能力。

Conclusion: 证明了策略梯度强化学习在低延迟、鲁棒自适应信号滤波中的可行性。

Abstract: Model-free and reinforcement learning-based adaptive filtering methods are
gaining traction for denoising in dynamic, non-stationary environments such as
wireless signal channels. Traditional filters like LMS, RLS, Wiener, and Kalman
are limited by assumptions of stationary or requiring complex fine-tuning or
exact noise statistics or fixed models. This letter proposes an adaptive
filtering framework using Proximal Policy Optimization (PPO), guided by a
composite reward that balances SNR improvement, MSE reduction, and residual
smoothness. Experiments on synthetic signals with various noise types show that
our PPO agent generalizes beyond its training distribution, achieving real-time
performance and outperforming classical filters. This work demonstrates the
viability of policy-gradient reinforcement learning for robust, low-latency
adaptive signal filtering.

</details>


### [750] [Uncertainty-Aware Multi-view Arrhythmia Classification from ECG](https://arxiv.org/abs/2506.06342)
*Mohd Ashhad,Sana Rahmani,Mohammed Fayiz,Ali Etemad,Javad Hashemi*

Main category: eess.SP

TL;DR: 提出一种深度神经网络架构，用于ECG多视图分类，融合1D和2D视图以减少噪声干扰，提升心律失常分类性能。


<details>
  <summary>Details</summary>
Motivation: 解决ECG数据中噪声和伪影导致的多视图冲突问题，提升分类准确性和鲁棒性。

Method: 结合时间序列模块（形态特征）、图像空间学习模块（时空特征）和不确定性感知融合模块。

Result: 在两个真实数据集上表现优于现有方法，对噪声和伪影更具鲁棒性。

Conclusion: 该方法通过多视图融合和不确定性处理，显著提升了心律失常分类的性能和鲁棒性。

Abstract: We propose a deep neural architecture that performs uncertainty-aware
multi-view classification of arrhythmia from ECG. Our method learns two
different views (1D and 2D) of single-lead ECG to capture different types of
information. We use a fusion technique to reduce the conflict between the
different views caused by noise and artifacts in ECG data, thus incorporating
uncertainty to obtain stronger final predictions. Our framework contains the
following three modules (1) a time-series module to learn the morphological
features from ECG; (2) an image-space learning module to learn the
spatiotemporal features; and (3) the uncertainty-aware fusion module to fuse
the information from the two different views. Experimental results on two
real-world datasets demonstrate that our framework not only improves the
performance on arrhythmia classification compared to the state-of-the-art but
also shows better robustness to noise and artifacts present in ECG.

</details>


### [751] [LD-RPMNet: Near-Sensor Diagnosis for Railway Point Machines](https://arxiv.org/abs/2506.06346)
*Wei Li,Xiaochun Wu,Xiaoxi Hu,Yuxuan Zhang,Sebastian Bader,Yuhan Huang*

Main category: eess.SP

TL;DR: LD-RPMNet结合Transformer和CNN，通过多尺度深度可分离卷积和广播自注意力机制，优化计算效率，在铁路道岔故障诊断中实现高精度。


<details>
  <summary>Details</summary>
Motivation: 工业中近传感器诊断需求增加，需轻量高效模型以优化铁路应用。

Method: 提出LD-RPMNet，集成MDSC模块和BSA机制，减少计算复杂度。

Result: 参数和计算复杂度降低50%，诊断精度提升3%，达98.86%。

Conclusion: LD-RPMNet为铁路道岔近传感器故障诊断提供可行方案。

Abstract: Near-sensor diagnosis has become increasingly prevalent in industry. This
study proposes a lightweight model named LD-RPMNet that integrates Transformers
and Convolutional Neural Networks, leveraging both local and global feature
extraction to optimize computational efficiency for a practical railway
application. The LD-RPMNet introduces a Multi-scale Depthwise Separable
Convolution (MDSC) module, which decomposes cross-channel convolutions into
pointwise and depthwise convolutions while employing multi-scale kernels to
enhance feature extraction. Meanwhile, a Broadcast Self-Attention (BSA)
mechanism is incorporated to simplify complex matrix multiplications and
improve computational efficiency. Experimental results based on collected sound
signals during the operation of railway point machines demonstrate that the
optimized model reduces parameter count and computational complexity by 50%
while improving diagnostic accuracy by nearly 3%, ultimately achieving an
accuracy of 98.86%. This demonstrates the possibility of near-sensor fault
diagnosis applications in railway point machines.

</details>


### [752] [Multi-Platform Methane Plume Detection via Model and Domain Adaptation](https://arxiv.org/abs/2506.06348)
*Vassiliki Mancoridis,Brian Bue,Jake H. Lee,Andrew K. Thorpe,Daniel Cusworth,Alana Ayasse,Philip G. Brodrick,Riley Duren*

Main category: eess.SP

TL;DR: 论文提出了一种结合模型和数据驱动的机器学习方法，通过迁移学习和CycleGAN技术，优化了空间遥感平台对甲烷羽流的检测能力。


<details>
  <summary>Details</summary>
Motivation: 甲烷对全球变暖影响显著，但现有遥感平台检测甲烷羽流时存在跨平台对齐问题，亟需解决。

Method: 利用AVIRIS-NG机载数据，通过迁移学习优化EMIT空间遥感数据的分类器，并使用CycleGAN进行数据分布对齐。

Result: 通过CycleGAN将EMIT数据转换为AVIRIS-NG域后直接应用机载分类器，取得了最佳检测效果。

Conclusion: 该方法不仅适用于甲烷羽流检测，还可推广至其他遥感仪器的数据对齐任务。

Abstract: Prioritizing methane for near-term climate action is crucial due to its
significant impact on global warming. Previous work used columnwise matched
filter products from the airborne AVIRIS-NG imaging spectrometer to detect
methane plume sources; convolutional neural networks (CNNs) discerned
anthropogenic methane plumes from false positive enhancements. However, as an
increasing number of remote sensing platforms are used for methane plume
detection, there is a growing need to address cross-platform alignment. In this
work, we describe model- and data-driven machine learning approaches that
leverage airborne observations to improve spaceborne methane plume detection,
reconciling the distributional shifts inherent with performing the same task
across platforms. We develop a spaceborne methane plume classifier using data
from the EMIT imaging spectroscopy mission. We refine classifiers trained on
airborne imagery from AVIRIS-NG campaigns using transfer learning,
outperforming the standalone spaceborne model. Finally, we use CycleGAN, an
unsupervised image-to-image translation technique, to align the data
distributions between airborne and spaceborne contexts. Translating spaceborne
EMIT data to the airborne AVIRIS-NG domain using CycleGAN and applying airborne
classifiers directly yields the best plume detection results. This methodology
is useful not only for data simulation, but also for direct data alignment.
Though demonstrated on the task of methane plume detection, our work more
broadly demonstrates a data-driven approach to align related products obtained
from distinct remote sensing instruments.

</details>


### [753] [Towards Generalizable Drowsiness Monitoring with Physiological Sensors: A Preliminary Study](https://arxiv.org/abs/2506.06360)
*Jiyao Wang,Suzan Ayas,Jiahao Zhang,Xiao Wen,Dengbo He,Birsen Donmez*

Main category: eess.SP

TL;DR: 该论文研究了基于生理信号的驾驶疲劳检测，发现不同疲劳诱导方式会导致不同的生理反应，且客观评估比主观评估更敏感。


<details>
  <summary>Details</summary>
Motivation: 驾驶疲劳检测对安全至关重要，生理信号方法比摄像头更保护隐私，但不同数据集中生理指标与疲劳标签的关联存在冲突。

Method: 分析了四个数据集中的心电图（ECG）、皮肤电活动（EDA）和呼吸（RESP）信号，构建二元逻辑回归模型识别与疲劳相关的生理指标。

Result: 发现心率稳定性增加、呼吸幅度降低和皮肤电活动减少与疲劳显著相关，且客观评估更敏感。

Conclusion: 研究结果提升了疲劳检测的理解，为未来通用监测设计提供了依据。

Abstract: Accurately detecting drowsiness is vital to driving safety. Among all
measures, physiological-signal-based drowsiness monitoring can be more
privacy-preserving than a camera-based approach. However, conflicts exist
regarding how physiological metrics are associated with different drowsiness
labels across datasets. Thus, we analyzed key features from electrocardiograms
(ECG), electrodermal activity (EDA), and respiratory (RESP) signals across four
datasets, where different drowsiness inducers (such as fatigue and low arousal)
and assessment methods (subjective vs. objective) were used. Binary logistic
regression models were built to identify the physiological metrics that are
associated with drowsiness. Findings indicate that distinct different
drowsiness inducers can lead to different physiological responses, and
objective assessments were more sensitive than subjective ones in detecting
drowsiness. Further, the increased heart rate stability, reduced respiratory
amplitude, and decreased tonic EDA are robustly associated with increased
drowsiness. The results enhance understanding of drowsiness detection and can
inform future generalizable monitoring designs.

</details>


### [754] [Transformer-Based Decomposition of Electrodermal Activity for Real-World Mental Health Applications](https://arxiv.org/abs/2506.06378)
*Charalampos Tsirmpas,Stasinos Konstantopoulos,Dimitris Andrikopoulos,Konstantina Kyriakouli,Panagiotis Fatouros*

Main category: eess.SP

TL;DR: 论文比较了知识驱动、统计和深度学习方法在EDA信号分解中的表现，提出了一种基于Transformer的新模型Feel Transformer，在真实数据中表现优异。


<details>
  <summary>Details</summary>
Motivation: 分解EDA信号为相位和张力成分对提取情感和生理标志物至关重要，但现有方法在真实数据中表现不足。

Method: 提出Feel Transformer模型，基于Autoformer架构，利用池化和趋势去除机制实现无监督分解。

Result: Feel Transformer在特征保真度和抗噪性上优于Ledalab、cvxEDA等方法。

Conclusion: 该模型有望用于实时生物信号分析、压力预测和心理健康干预。

Abstract: Decomposing Electrodermal Activity (EDA) into phasic (short-term,
stimulus-linked responses) and tonic (longer-term baseline) components is
essential for extracting meaningful emotional and physiological biomarkers.
This study presents a comparative analysis of knowledge-driven, statistical,
and deep learning-based methods for EDA signal decomposition, with a focus on
in-the-wild data collected from wearable devices. In particular, the authors
introduce the Feel Transformer, a novel Transformer-based model adapted from
the Autoformer architecture, designed to separate phasic and tonic components
without explicit supervision. The model leverages pooling and trend-removal
mechanisms to enforce physiologically meaningful decompositions. Comparative
experiments against methods such as Ledalab, cvxEDA, and conventional
detrending show that the Feel Transformer achieves a balance between feature
fidelity (SCR frequency, amplitude, and tonic slope) and robustness to noisy,
real-world data. The model demonstrates potential for real-time biosignal
analysis and future applications in stress prediction, digital mental health
interventions, and physiological forecasting.

</details>


### [755] [IQFM A Wireless Foundational Model for I/Q Streams in AI-Native 6G](https://arxiv.org/abs/2506.06718)
*Omar Mashaal,Hatem Abou-Zeid*

Main category: eess.SP

TL;DR: IQFM是首个基于原始I/Q信号的无线通信基础模型，支持多种任务，无需复杂预处理或手工特征。通过对比自监督学习框架和任务感知增强策略，模型在少量标注数据下表现优异，并展现出良好的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 探索无线通信中基于原始I/Q信号的基础模型潜力，填补现有研究空白，为AI原生6G系统提供高效、可复用的多任务学习编码器。

Method: 提出IQFM模型，采用对比自监督学习框架，结合任务感知增强策略（核心增强和任务特定增强），在少量标注数据下进行预训练。

Result: 模型在调制分类和AoA分类任务上分别达到99.67%和65.45%的准确率，显著优于监督基线；在泛化任务中表现同样出色。

Conclusion: IQFM展示了原始I/Q信号基础模型在无线通信中的高效性和多任务学习潜力，为6G系统提供了新的研究方向。

Abstract: Foundational models have shown remarkable potential in natural language
processing and computer vision, yet remain in their infancy in wireless
communications. While a few efforts have explored image-based modalities such
as channel state information (CSI) and frequency spectrograms, foundational
models that operate directly on raw IQ data remain largely unexplored. This
paper presents, IQFM, the first I/Q signal foundational model for wireless
communications. IQFM supporting diverse tasks: modulation classification,
angle-of-arrival (AoA), beam prediction, and RF fingerprinting, without heavy
preprocessing or handcrafted features. We also introduce a task-aware
augmentation strategy that categorizes transformations into core augmentations,
such as cyclic time shifting, and task-specific augmentations. This strategy
forms the basis for structured, task-dependent representation learning within a
contrastive self-supervised learning (SSL) framework. Using this strategy, the
lightweight encoder, pre-trained via SSL on over-the-air multi-antenna IQ data,
achieves up to 99.67% and 65.45% accuracy on modulation and AoA classification,
respectively, using only one labeled sample per class, outperforming supervised
baselines by up to 7x and 145x. The model also generalizes to
out-of-distribution tasks; when adapted to new tasks using only 500 samples per
class and minimal parameter updates via LoRA, the same frozen encoder achieves
94.15% on beam prediction (vs. 89.53% supervised), 50.00% on RML2016a
modulation classification (vs. 49.30%), and 96.05% on RF fingerprinting (vs.
96.64%). These results demonstrate the potential of raw IQ-based foundational
models as efficient, reusable encoders for multi-task learning in AI-native 6G
systems.

</details>


### [756] [Conditional Denoising Diffusion for ISAC Enhanced Channel Estimation in Cell-Free 6G](https://arxiv.org/abs/2506.06942)
*Mohammad Farzanullah,Han Zhang,Akram Bin Sediq,Ali Afana,Melike Erol-Kantarci*

Main category: eess.SP

TL;DR: 本文提出了一种基于条件去噪扩散模型（CDDM）和多模态变换器（MMT）的新框架，用于提升无蜂窝集成传感与通信（ISAC）系统中的信道估计性能。


<details>
  <summary>Details</summary>
Motivation: 无蜂窝ISAC系统面临信道估计性能受限的问题，如导频污染和噪声干扰。本文旨在通过结合传感信息提升信道估计的准确性和鲁棒性。

Method: 提出了一种结合CDDM和MMT的框架，利用传感和位置数据的多模态关系，通过迭代去噪优化信道估计。

Result: 仿真结果显示，该模型在NMSE上比LS和MMSE分别提升了8 dB和9 dB，比传统TDDM提升了27.8%，且在低信噪比和导频污染下表现优异。

Conclusion: 该框架显著提升了无蜂窝ISAC系统的信道估计性能，尤其在用户靠近传感目标时表现更佳。

Abstract: Cell-free Integrated Sensing and Communication (ISAC) aims to revolutionize
6th Generation (6G) networks. By combining distributed access points with ISAC
capabilities, it boosts spectral efficiency, situational awareness, and
communication reliability. Channel estimation is a critical step in cell-free
ISAC systems to ensure reliable communication, but its performance is usually
limited by challenges such as pilot contamination and noisy channel estimates.
This paper presents a novel framework leveraging sensing information as a key
input within a Conditional Denoising Diffusion Model (CDDM). In this framework,
we integrate CDDM with a Multimodal Transformer (MMT) to enhance channel
estimation in ISAC-enabled cell-free systems. The MMT encoder effectively
captures inter-modal relationships between sensing and location data, enabling
the CDDM to iteratively denoise and refine channel estimates. Simulation
results demonstrate that the proposed approach achieves significant performance
gains. As compared with Least Squares (LS) and Minimum Mean Squared Error
(MMSE) estimators, the proposed model achieves normalized mean squared error
(NMSE) improvements of 8 dB and 9 dB, respectively. Moreover, we achieve a
27.8% NMSE improvement compared to the traditional denoising diffusion model
(TDDM), which does not incorporate sensing channel information. Additionally,
the model exhibits higher robustness against pilot contamination and maintains
high accuracy under challenging conditions, such as low signal-to-noise ratios
(SNRs). According to the simulation results, the model performs well for users
near sensing targets by leveraging the correlation between sensing and
communication channels.

</details>


### [757] [Diffusion Models-Aided Uplink Channel Estimation for RIS-Assisted Systems](https://arxiv.org/abs/2506.07770)
*Yang Wang,Yin Xu,Cixiao Zhang,Zhiyong Chen,Xiaowu Ou,Mingzeng Dai,Meixia Tao,Wenjun Zhang*

Main category: eess.SP

TL;DR: 提出了一种基于扩散模型（DM）的可重构智能表面（RIS）辅助系统信道估计方法，通过确定性采样策略和轻量级网络设计，显著提升了性能并减少了参数数量。


<details>
  <summary>Details</summary>
Motivation: 传统扩散模型在信道估计中存在随机性问题，且参数较多，限制了实用性。

Method: 将信道估计问题重构为去噪过程，采用确定性采样策略和步长对齐机制，并设计轻量级网络以减少参数。

Result: 在多种信噪比（SNR）下性能优于基线，NMSE提升达13.5 dB（SNR=0 dB），轻量级网络参数仅需原U-Net的6.59%，性能几乎无损。

Conclusion: 该方法在性能和实用性上均表现出色，为RIS辅助系统的信道估计提供了高效解决方案。

Abstract: This letter proposes a channel estimation method for reconfigurable
intelligent surface (RIS)-assisted systems through a novel diffusion model (DM)
framework. We reformulate the channel estimation problem as a denoising
process, which aligns with the reverse process of the DM. To overcome the
inherent randomness in the reverse process of conventional DM approaches, we
adopt a deterministic sampling strategy with a step alignment mechanism that
ensures the accuracy of channel estimation while adapting to different
signal-to-noise ratio (SNR). Furthermore, to reduce the number of parameters of
the U-Net, we meticulously design a lightweight network that achieves
comparable performance, thereby enhancing the practicality of our proposed
method. Extensive simulations demonstrate superior performance over a wide
range of SNRs compared to baselines. For instance, the proposed method achieves
performance improvements of up to 13.5 dB in normalized mean square error
(NMSE) at SNR = 0 dB. Notably, the proposed lightweight network exhibits almost
no performance loss compared to the original U-Net, while requiring only 6.59\%
of its parameters.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [758] [ResPF: Residual Poisson Flow for Efficient and Physically Consistent Sparse-View CT Reconstruction](https://arxiv.org/abs/2506.06400)
*Changsheng Fang,Yongtong Liu,Bahareh Morovati,Shuo Han,Yu Shi,Li Zhou,Shuyi Fan,Hengyong Yu*

Main category: eess.IV

TL;DR: 提出ResPF模型，通过结合Poisson Flow生成模型和数据一致性约束，高效解决稀疏视图CT重建问题，提升重建质量和速度。


<details>
  <summary>Details</summary>
Motivation: 稀疏视图CT可降低辐射剂量，但重建问题复杂，现有方法缺乏物理可解释性或计算成本高。

Method: 基于PFGM++，引入条件指导和残差融合模块，结合数据一致性约束，优化采样过程。

Result: ResPF在合成和临床数据上表现优异，重建质量高、速度快且鲁棒性强。

Conclusion: ResPF首次将Poisson Flow模型应用于稀疏视图CT，为高效重建提供了新思路。

Abstract: Sparse-view computed tomography (CT) is a practical solution to reduce
radiation dose, but the resulting ill-posed inverse problem poses significant
challenges for accurate image reconstruction. Although deep learning and
diffusion-based methods have shown promising results, they often lack physical
interpretability or suffer from high computational costs due to iterative
sampling starting from random noise. Recent advances in generative modeling,
particularly Poisson Flow Generative Models (PFGM), enable high-fidelity image
synthesis by modeling the full data distribution. In this work, we propose
Residual Poisson Flow (ResPF) Generative Models for efficient and accurate
sparse-view CT reconstruction. Based on PFGM++, ResPF integrates conditional
guidance from sparse measurements and employs a hijacking strategy to
significantly reduce sampling cost by skipping redundant initial steps.
However, skipping early stages can degrade reconstruction quality and introduce
unrealistic structures. To address this, we embed a data-consistency into each
iteration, ensuring fidelity to sparse-view measurements. Yet, PFGM sampling
relies on a fixed ordinary differential equation (ODE) trajectory induced by
electrostatic fields, which can be disrupted by step-wise data consistency,
resulting in unstable or degraded reconstructions. Inspired by ResNet, we
introduce a residual fusion module to linearly combine generative outputs with
data-consistent reconstructions, effectively preserving trajectory continuity.
To the best of our knowledge, this is the first application of Poisson flow
models to sparse-view CT. Extensive experiments on synthetic and clinical
datasets demonstrate that ResPF achieves superior reconstruction quality,
faster inference, and stronger robustness compared to state-of-the-art
iterative, learning-based, and diffusion models.

</details>


### [759] [SPC to 3D: Novel View Synthesis from Binary SPC via I2I translation](https://arxiv.org/abs/2506.06890)
*Sumit Sharma,Gopi Raju Matta,Kaushik Mitra*

Main category: eess.IV

TL;DR: 提出了一种两阶段框架，将二进制SPC图像转换为高质量彩色新视图，解决了传统3D合成技术因信息丢失而无效的问题。


<details>
  <summary>Details</summary>
Motivation: SPADs和SPCs技术能够高精度检测光子，但二进制SPC图像导致纹理和颜色信息严重丢失，传统3D合成技术难以处理。

Method: 采用两阶段框架：第一阶段使用Pix2PixHD等生成模型将二进制SPC图像转换为RGB图像；第二阶段使用NeRF或3DGS进行3D场景重建和新视图生成。

Result: 实验验证表明，该框架在感知质量和几何一致性上显著优于基线方法。

Conclusion: 提出的两阶段框架有效解决了二进制SPC图像的信息丢失问题，为3D重建和辐射场恢复提供了新思路。

Abstract: Single Photon Avalanche Diodes (SPADs) represent a cutting-edge imaging
technology, capable of detecting individual photons with remarkable timing
precision. Building on this sensitivity, Single Photon Cameras (SPCs) enable
image capture at exceptionally high speeds under both low and high
illumination. Enabling 3D reconstruction and radiance field recovery from such
SPC data holds significant promise. However, the binary nature of SPC images
leads to severe information loss, particularly in texture and color, making
traditional 3D synthesis techniques ineffective. To address this challenge, we
propose a modular two-stage framework that converts binary SPC images into
high-quality colorized novel views. The first stage performs image-to-image
(I2I) translation using generative models such as Pix2PixHD, converting binary
SPC inputs into plausible RGB representations. The second stage employs 3D
scene reconstruction techniques like Neural Radiance Fields (NeRF) or Gaussian
Splatting (3DGS) to generate novel views. We validate our two-stage pipeline
(Pix2PixHD + Nerf/3DGS) through extensive qualitative and quantitative
experiments, demonstrating significant improvements in perceptual quality and
geometric consistency over the alternative baseline.

</details>


### [760] [Optimal Transport Driven Asymmetric Image-to-Image Translation for Nuclei Segmentation of Histological Images](https://arxiv.org/abs/2506.07023)
*Suman Mahapatra,Pradipta Maji*

Main category: eess.IV

TL;DR: 该论文提出了一种新的深度生成模型，用于从组织学图像中分割细胞核结构，解决了图像到图像转换模型在信息不对称时的失败问题。


<details>
  <summary>Details</summary>
Motivation: 细胞核区域的分割有助于疾病的检测和诊断，但现有方法在信息不对称时表现不佳。

Method: 模型引入嵌入空间处理信息不对称，结合最优传输和测度理论，开发可逆生成器，无需显式循环一致性损失。

Result: 模型在复杂性和性能之间取得更好平衡，优于现有方法。

Conclusion: 提出的模型在公开数据集上验证了其有效性，为细胞核分割提供了新思路。

Abstract: Segmentation of nuclei regions from histological images enables morphometric
analysis of nuclei structures, which in turn helps in the detection and
diagnosis of diseases under consideration. To develop a nuclei segmentation
algorithm, applicable to different types of target domain representations,
image-to-image translation networks can be considered as they are invariant to
target domain image representations. One of the important issues with
image-to-image translation models is that they fail miserably when the
information content between two image domains are asymmetric in nature. In this
regard, the paper introduces a new deep generative model for segmenting nuclei
structures from histological images. The proposed model considers an embedding
space for handling information-disparity between information-rich histological
image space and information-poor segmentation map domain. Integrating
judiciously the concepts of optimal transport and measure theory, the model
develops an invertible generator, which provides an efficient optimization
framework with lower network complexity. The concept of invertible generator
automatically eliminates the need of any explicit cycle-consistency loss. The
proposed model also introduces a spatially-constrained squeeze operation within
the framework of invertible generator to maintain spatial continuity within the
image patches. The model provides a better trade-off between network complexity
and model performance compared to other existing models having complex network
architectures. The performance of the proposed deep generative model, along
with a comparison with state-of-the-art nuclei segmentation methods, is
demonstrated on publicly available histological image data sets.

</details>


### [761] [SiliCoN: Simultaneous Nuclei Segmentation and Color Normalization of Histological Images](https://arxiv.org/abs/2506.07028)
*Suman Mahapatra,Pradipta Maji*

Main category: eess.IV

TL;DR: 提出了一种新的深度生成模型，用于同时分割细胞核结构和标准化染色组织图像的颜色外观。


<details>
  <summary>Details</summary>
Motivation: 解决染色组织图像中颜色变化对细胞核分割的影响，同时提高分割和颜色标准化的准确性。

Method: 结合截断正态分布和空间注意力的深度生成模型，假设颜色外观信息与细胞核分割图独立。

Result: 在公开数据集上验证了模型的性能，并与现有算法进行了比较。

Conclusion: 该模型具有通用性和适应性，颜色外观信息的修改或丢失不会影响细胞核分割结果。

Abstract: Segmentation of nuclei regions from histological images is an important task
for automated computer-aided analysis of histological images, particularly in
the presence of impermissible color variation in the color appearance of
stained tissue images. While color normalization enables better nuclei
segmentation, accurate segmentation of nuclei structures makes color
normalization rather trivial. In this respect, the paper proposes a novel deep
generative model for simultaneously segmenting nuclei structures and
normalizing color appearance of stained histological images.This model
judiciously integrates the merits of truncated normal distribution and spatial
attention. The model assumes that the latent color appearance information,
corresponding to a particular histological image, is independent of respective
nuclei segmentation map as well as embedding map information. The disentangled
representation makes the model generalizable and adaptable as the modification
or loss in color appearance information cannot be able to affect the nuclei
segmentation map as well as embedding information. Also, for dealing with the
stain overlap of associated histochemical reagents, the prior for latent color
appearance code is assumed to be a mixture of truncated normal distributions.
The proposed model incorporates the concept of spatial attention for
segmentation of nuclei regions from histological images. The performance of the
proposed approach, along with a comparative analysis with related
state-of-the-art algorithms, has been demonstrated on publicly available
standard histological image data sets.

</details>


### [762] [Transfer Learning and Explainable AI for Brain Tumor Classification: A Study Using MRI Data from Bangladesh](https://arxiv.org/abs/2506.07228)
*Shuvashis Sarker*

Main category: eess.IV

TL;DR: 该研究开发了一种基于深度学习的自动脑肿瘤分类系统，结合可解释AI技术，在资源有限的地区（如孟加拉国）提高了脑肿瘤诊断的准确性和透明度。


<details>
  <summary>Details</summary>
Motivation: 脑肿瘤的早期诊断对改善患者预后至关重要，尤其在医疗资源有限的地区。传统MRI分析效率低且易出错，亟需自动化解决方案。

Method: 使用VGG16、VGG19和ResNet50等深度学习模型，结合Grad-CAM和Grad-CAM++等XAI技术，对MRI数据进行分类和分析。

Result: VGG16模型表现最佳，准确率达99.17%。XAI技术提升了模型的透明度和稳定性。

Conclusion: 深度学习与XAI结合，可在资源有限地区有效提升脑肿瘤诊断能力。

Abstract: Brain tumors, regardless of being benign or malignant, pose considerable
health risks, with malignant tumors being more perilous due to their swift and
uncontrolled proliferation, resulting in malignancy. Timely identification is
crucial for enhancing patient outcomes, particularly in nations such as
Bangladesh, where healthcare infrastructure is constrained. Manual MRI analysis
is arduous and susceptible to inaccuracies, rendering it inefficient for prompt
diagnosis. This research sought to tackle these problems by creating an
automated brain tumor classification system utilizing MRI data obtained from
many hospitals in Bangladesh. Advanced deep learning models, including VGG16,
VGG19, and ResNet50, were utilized to classify glioma, meningioma, and various
brain cancers. Explainable AI (XAI) methodologies, such as Grad-CAM and
Grad-CAM++, were employed to improve model interpretability by emphasizing the
critical areas in MRI scans that influenced the categorization. VGG16 achieved
the most accuracy, attaining 99.17%. The integration of XAI enhanced the
system's transparency and stability, rendering it more appropriate for clinical
application in resource-limited environments such as Bangladesh. This study
highlights the capability of deep learning models, in conjunction with
explainable artificial intelligence (XAI), to enhance brain tumor detection and
identification in areas with restricted access to advanced medical
technologies.

</details>


### [763] [A Comprehensive Analysis of COVID-19 Detection Using Bangladeshi Data and Explainable AI](https://arxiv.org/abs/2506.07234)
*Shuvashis Sarker*

Main category: eess.IV

TL;DR: 研究利用VGG19模型在CXR图像中检测COVID-19，准确率达98%，并采用LIME和SMOTE提升模型透明性和平衡数据。


<details>
  <summary>Details</summary>
Motivation: COVID-19全球大流行对孟加拉国造成严重影响，亟需高效检测方法。

Method: 使用4,350张CXR图像数据集，分为四类，采用ML、DL和TL模型，VGG19为主，结合LIME和SMOTE。

Result: VGG19模型达到98%准确率，LIME解释模型决策，SMOTE解决数据不平衡。

Conclusion: 研究强调XAI在提升模型透明性和可靠性中的重要性，优化CXR图像检测效果。

Abstract: COVID-19 is a rapidly spreading and highly infectious virus which has
triggered a global pandemic, profoundly affecting millions across the world.
The pandemic has introduced unprecedented challenges in public health, economic
stability, and societal structures, necessitating the implementation of
extensive and multifaceted health interventions globally. It had a tremendous
impact on Bangladesh by April 2024, with around 29,495 fatalities and more than
2 million confirmed cases. This study focuses on improving COVID-19 detection
in CXR images by utilizing a dataset of 4,350 images from Bangladesh
categorized into four classes: Normal, Lung-Opacity, COVID-19 and
Viral-Pneumonia. ML, DL and TL models are employed with the VGG19 model
achieving an impressive 98% accuracy. LIME is used to explain model
predictions, highlighting the regions and features influencing classification
decisions. SMOTE is applied to address class imbalances. By providing insight
into both correct and incorrect classifications, the study emphasizes the
importance of XAI in enhancing the transparency and reliability of models,
ultimately improving the effectiveness of detection from CXR images.

</details>


### [764] [A Narrative Review on Large AI Models in Lung Cancer Screening, Diagnosis, and Treatment Planning](https://arxiv.org/abs/2506.07236)
*Jiachen Zhong,Yiting Wang,Di Zhu,Ziwei Wang*

Main category: eess.IV

TL;DR: 该论文综述了大型AI模型在肺癌筛查、诊断、预后和治疗中的应用，分类并评估了现有模型，探讨了其临床潜力与局限性。


<details>
  <summary>Details</summary>
Motivation: 肺癌是全球高发且致命的疾病，需要更准确的诊断和治疗方法。大型AI模型在医学影像理解和临床决策中的进步为肺癌管理提供了新机遇。

Method: 系统综述了现有大型AI模型（如CLIP、BLIP、Flamingo等），分类为模态特定编码器、编码器-解码器框架和联合编码器架构，并评估其在多模态学习任务中的表现。

Result: 模型在肺结节检测、基因突变预测、多组学整合和个性化治疗规划中表现优异，部分已进入临床验证阶段。

Conclusion: 大型AI模型在肺癌管理中具有变革潜力，但需解决泛化性、可解释性和合规性等挑战，未来应发展可扩展、可解释且临床集成的AI系统。

Abstract: Lung cancer remains one of the most prevalent and fatal diseases worldwide,
demanding accurate and timely diagnosis and treatment. Recent advancements in
large AI models have significantly enhanced medical image understanding and
clinical decision-making. This review systematically surveys the
state-of-the-art in applying large AI models to lung cancer screening,
diagnosis, prognosis, and treatment. We categorize existing models into
modality-specific encoders, encoder-decoder frameworks, and joint encoder
architectures, highlighting key examples such as CLIP, BLIP, Flamingo,
BioViL-T, and GLoRIA. We further examine their performance in multimodal
learning tasks using benchmark datasets like LIDC-IDRI, NLST, and MIMIC-CXR.
Applications span pulmonary nodule detection, gene mutation prediction,
multi-omics integration, and personalized treatment planning, with emerging
evidence of clinical deployment and validation. Finally, we discuss current
limitations in generalizability, interpretability, and regulatory compliance,
proposing future directions for building scalable, explainable, and clinically
integrated AI systems. Our review underscores the transformative potential of
large AI models to personalize and optimize lung cancer care.

</details>


### [765] [Text-guided multi-stage cross-perception network for medical image segmentation](https://arxiv.org/abs/2506.07475)
*Gaoyu Chen*

Main category: eess.IV

TL;DR: 论文提出了一种名为TMC的文本引导多阶段交叉感知网络，用于解决医学图像分割中目标区域语义表达弱的问题，通过多阶段交叉注意力模块和多阶段对齐损失提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有医学图像分割方法因目标与非目标区域对比度低导致语义表达弱，而现有文本引导方法存在跨模态交互不足和特征表达不充分的问题。

Method: 提出TMC网络，引入多阶段交叉注意力模块增强语义细节理解，并使用多阶段对齐损失提升跨模态语义一致性。

Result: 在三个公开数据集（QaTa-COV19、MosMedData和Breast）上，TMC的Dice分数分别为84.77%、78.50%和88.73%，优于UNet和现有文本引导方法。

Conclusion: TMC通过改进跨模态交互和特征表达，显著提升了医学图像分割的性能。

Abstract: Medical image segmentation plays a crucial role in clinical medicine, serving
as a tool for auxiliary diagnosis, treatment planning, and disease monitoring,
thus facilitating physicians in the study and treatment of diseases. However,
existing medical image segmentation methods are limited by the weak semantic
expression of the target segmentation regions, which is caused by the low
contrast between the target and non-target segmentation regions. To address
this limitation, text prompt information has greast potential to capture the
lesion location. However, existing text-guided methods suffer from insufficient
cross-modal interaction and inadequate cross-modal feature expression. To
resolve these issues, we propose the Text-guided Multi-stage Cross-perception
network (TMC). In TMC, we introduce a multistage cross-attention module to
enhance the model's understanding of semantic details and a multi-stage
alignment loss to improve the consistency of cross-modal semantics. The results
of the experiments demonstrate that our TMC achieves a superior performance
with Dice of 84.77%, 78.50%, 88.73% in three public datasets (QaTa-COV19,
MosMedData and Breast), outperforming UNet based networks and text-guided
methods.

</details>


### [766] [Fine-Grained Motion Compression and Selective Temporal Fusion for Neural B-Frame Video Coding](https://arxiv.org/abs/2506.07709)
*Xihua Sheng,Peilin Chen,Meng Wang,Li Zhang,Shiqi Wang,Dapeng Oliver Wu*

Main category: eess.IV

TL;DR: 论文提出了一种针对神经B帧编码的增强方法，包括细粒度运动压缩和选择性时间融合，显著提升了压缩性能。


<details>
  <summary>Details</summary>
Motivation: 现有神经B帧编解码器直接采用P帧工具，未能解决B帧压缩的独特挑战，导致性能不佳。

Method: 设计了细粒度运动压缩方法和选择性时间融合方法，分别优化运动向量压缩和双向上下文利用。

Result: 实验表明，该方法优于现有神经B帧编解码器，性能接近或超过H.266/VVC参考软件。

Conclusion: 提出的方法有效解决了B帧压缩的挑战，为神经B帧编码提供了新思路。

Abstract: With the remarkable progress in neural P-frame video coding, neural B-frame
coding has recently emerged as a critical research direction. However, most
existing neural B-frame codecs directly adopt P-frame coding tools without
adequately addressing the unique challenges of B-frame compression, leading to
suboptimal performance. To bridge this gap, we propose novel enhancements for
motion compression and temporal fusion for neural B-frame coding. First, we
design a fine-grained motion compression method. This method incorporates an
interactive dual-branch motion auto-encoder with per-branch adaptive
quantization steps, which enables fine-grained compression of bi-directional
motion vectors while accommodating their asymmetric bitrate allocation and
reconstruction quality requirements. Furthermore, this method involves an
interactive motion entropy model that exploits correlations between
bi-directional motion latent representations by interactively leveraging
partitioned latent segments as directional priors. Second, we propose a
selective temporal fusion method that predicts bi-directional fusion weights to
achieve discriminative utilization of bi-directional multi-scale temporal
contexts with varying qualities. Additionally, this method introduces a
hyperprior-based implicit alignment mechanism for contextual entropy modeling.
By treating the hyperprior as a surrogate for the contextual latent
representation, this mechanism implicitly mitigates the misalignment in the
fused bi-directional temporal priors. Extensive experiments demonstrate that
our proposed codec outperforms state-of-the-art neural B-frame codecs and
achieves comparable or even superior compression performance to the H.266/VVC
reference software under random-access configurations.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [767] [Depth-Optimal Quantum Layout Synthesis as SAT](https://arxiv.org/abs/2506.06752)
*Anna B. Jakobsen,Anders B. Clausen,Jaco van de Pol,Irfansha Shaik*

Main category: quant-ph

TL;DR: 论文提出了一种新的SAT编码方法，用于量子电路布局合成，重点优化电路深度和CX门深度，相比现有方法提速10-100倍。


<details>
  <summary>Details</summary>
Motivation: 当前量子硬件对CX门的连接性有限制，且CX门噪声较大，因此需要优化布局合成以减少CX门数量或深度。

Method: 采用增量SAT求解和并行计划的高效编码方法，专注于最小化电路深度或CX门深度。

Result: 新方法在深度优化上比OLSQ2快10-100倍，但比Q-Synth的优化门数量耗时更长。实验表明，同时优化CX门数量和深度能最好地降低噪声。

Conclusion: 优化CX门数量比优化深度更能减少噪声，但综合考虑两者效果最佳。

Abstract: Quantum circuits consist of gates applied to qubits. Current quantum hardware
platforms impose connectivity restrictions on binary CX gates. Hence, Layout
Synthesis is an important step to transpile quantum circuits before they can be
executed. Since CX gates are noisy, it is important to reduce the CX count or
CX depth of the mapped circuits.
  We provide a new and efficient encoding of Quantum-circuit Layout Synthesis
in SAT. Previous SAT encodings focused on gate count and CX-gate count. Our
encoding instead guarantees that we find mapped circuits with minimal circuit
depth or minimal CX-gate depth. We use incremental SAT solving and parallel
plans for an efficient encoding. This results in speedups of more than 10-100x
compared to OLSQ2, which guarantees depth-optimality. But minimizing depth
still takes more time than minimizing gate count with Q-Synth.
  We correlate the noise reduction achieved by simulating circuits after
(CX)-count and (CX)-depth reduction. We find that minimizing for CX-count
correlates better with reducing noise than minimizing for CX-depth. However,
taking into account both CX-count and CX-depth provides the best noise
reduction.

</details>


### [768] [A weighted quantum ensemble of homogeneous quantum classifiers](https://arxiv.org/abs/2506.07810)
*Emiliano Tolotti,Enrico Blanzieri,Davide Pastorello*

Main category: quant-ph

TL;DR: 提出了一种基于量子分类器的加权同质集成方法，通过量子并行执行多样化内部分类器，并结合经典权重优化，验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 通过结合量子计算的并行性和集成学习的多样性，提高预测准确性。

Method: 使用量子分类器和索引寄存器进行数据编码，通过量子并行执行多样化内部分类器，并结合经典权重优化。

Result: 实证评估表明该方法有效。

Conclusion: 该方法为量子集成学习提供了新的思路，并展示了其性能潜力。

Abstract: Ensemble methods in machine learning aim to improve prediction accuracy by
combining multiple models. This is achieved by ensuring diversity among
predictors to capture different data aspects. Homogeneous ensembles use
identical models, achieving diversity through different data subsets, and
weighted-average ensembles assign higher influence to more accurate models
through a weight learning procedure. We propose a method to achieve a weighted
homogeneous quantum ensemble using quantum classifiers with indexing registers
for data encoding. This approach leverages instance-based quantum classifiers,
enabling feature and training point subsampling through superposition and
controlled unitaries, and allowing for a quantum-parallel execution of diverse
internal classifiers with different data compositions in superposition. The
method integrates a learning process involving circuit execution and classical
weight optimization, for a trained ensemble execution with weights encoded in
the circuit at test-time. Empirical evaluation demonstrate the effectiveness of
the proposed method, offering insights into its performance.

</details>


### [769] [Deep reinforcement learning for near-deterministic preparation of cubic- and quartic-phase gates in photonic quantum computing](https://arxiv.org/abs/2506.07859)
*Amanuel Anteneh Léandre Brunel,Carlos González-Arciniegas,Olivier Pfister*

Main category: quant-ph

TL;DR: 论文提出利用深度神经网络和强化学习控制量子光学电路生成立方相位态，平均成功率96%，仅需光子数分辨测量作为非高斯资源，并可直接生成四次相位门。


<details>
  <summary>Details</summary>
Motivation: 立方相位态是实现连续变量通用量子计算的充分资源，研究旨在高效生成此类态并探索其扩展应用。

Method: 通过深度神经网络和强化学习训练控制量子光学电路，利用光子数分辨测量作为非高斯资源。

Result: 实验平均成功率为96%，且无需立方门分解即可直接生成四次相位门。

Conclusion: 该方法高效生成立方相位态并扩展至四次相位门，为连续变量量子计算提供实用工具。

Abstract: Cubic-phase states are a sufficient resource for universal quantum computing
over continuous variables. We present results from numerical experiments in
which deep neural networks are trained via reinforcement learning to control a
quantum optical circuit for generating cubic-phase states, with an average
success rate of 96%. The only non-Gaussian resource required is
photon-number-resolving measurements. We also show that the exact same
resources enable the direct generation of a quartic-phase gate, with no need
for a cubic gate decomposition.

</details>


<div id='q-bio.NC'></div>

# q-bio.NC [[Back]](#toc)

### [770] [AI Agent Behavioral Science](https://arxiv.org/abs/2506.06366)
*Lin Chen,Yunke Zhang,Jie Feng,Haoye Chai,Honglin Zhang,Bingbing Fan,Yibo Ma,Shiyuan Zhang,Nian Li,Tianhui Liu,Nicholas Sukiennik,Keyu Zhao,Yu Li,Ziyi Liu,Fengli Xu,Yong Li*

Main category: q-bio.NC

TL;DR: 论文提出了一种新的科学视角——AI Agent Behavioral Science，强调通过行为观察、干预设计和理论解释来研究AI代理的行为、适应和交互，而非仅关注内部机制。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型（LLMs）的发展，AI系统在多样化、交互性和开放性场景中展现出类人行为，这需要新的科学方法来理解和评估其行为。

Method: 通过系统化研究个体、多代理和人与代理交互场景，结合行为观察、干预设计和理论解释，分析AI代理的行为模式。

Result: AI Agent Behavioral Science为理解、评估和治理日益自主的AI系统提供了必要工具，并将公平性、安全性等视为行为属性。

Conclusion: AI Agent Behavioral Science是对传统方法的必要补充，为AI系统的实际行为研究提供了新视角和工具。

Abstract: Recent advances in large language models (LLMs) have enabled AI systems to
behave in increasingly human-like ways, exhibiting planning, adaptation, and
social dynamics across increasingly diverse, interactive, and open-ended
scenarios. These behaviors are not solely the product of the models' internal
architecture, but emerge from their integration into agentic systems that
operate within situated contexts, where goals, feedback, and interactions shape
behavior over time. This shift calls for a new scientific lens: AI Agent
Behavioral Science. Rather than focusing only on internal mechanisms, this
paradigm emphasizes the systematic observation of behavior, design of
interventions to test hypotheses, and theory-guided interpretation of how AI
agents act, adapt, and interact over time. We systematize a growing body of
research across individual, multi-agent, and human-agent interaction settings,
and further demonstrate how this perspective informs responsible AI by treating
fairness, safety, interpretability, accountability, and privacy as behavioral
properties. By unifying recent findings and laying out future directions, we
position AI Agent Behavioral Science as a necessary complement to traditional
approaches, providing essential tools for understanding, evaluating, and
governing the real-world behavior of increasingly autonomous AI systems.

</details>


### [771] [Less is More: some Computational Principles based on Parcimony, and Limitations of Natural Intelligence](https://arxiv.org/abs/2506.07060)
*Laura Cohen,Xavier Hinaut,Lilyana Petrova,Alexandre Pitti,Syd Reynal,Ichiro Tsuda*

Main category: q-bio.NC

TL;DR: 论文探讨了自然智能（NI）如何在有限资源下高效学习，提出约束是效率、适应性和创造性的催化剂，并建议AI借鉴这些原则。


<details>
  <summary>Details</summary>
Motivation: 研究自然智能（NI）如何在有限神经和能量资源下高效学习，以启发更高效、可解释的AI系统。

Method: 分析有限神经带宽如何促进简洁编码，探讨混沌游走和储备计算，并结合发展视角讨论内在动机和社会环境的作用。

Result: 约束促进高效编码、灵活记忆管理和快速泛化，而主动学习和现实交互是自然智能的关键。

Conclusion: 建议AI采用'少即是多'原则（如能量约束、简洁架构和现实交互），以实现更高效、可解释和生物启发的系统。

Abstract: Natural intelligence (NI) consistently achieves more with less. Infants learn
language, develop abstract concepts, and acquire sensorimotor skills from
sparse data, all within tight neural and energy limits. In contrast, today's AI
relies on virtually unlimited computational power, energy, and data to reach
high performance. This paper argues that constraints in NI are paradoxically
catalysts for efficiency, adaptability, and creativity. We first show how
limited neural bandwidth promotes concise codes that still capture complex
patterns. Spiking neurons, hierarchical structures, and symbolic-like
representations emerge naturally from bandwidth constraints, enabling robust
generalization. Next, we discuss chaotic itinerancy, illustrating how the brain
transits among transient attractors to flexibly retrieve memories and manage
uncertainty. We then highlight reservoir computing, where random projections
facilitate rapid generalization from small datasets. Drawing on developmental
perspectives, we emphasize how intrinsic motivation, along with responsive
social environments, drives infant language learning and discovery of meaning.
Such active, embodied processes are largely absent in current AI. Finally, we
suggest that adopting 'less is more' principles -- energy constraints,
parsimonious architectures, and real-world interaction -- can foster the
emergence of more efficient, interpretable, and biologically grounded
artificial systems.

</details>


<div id='physics.comp-ph'></div>

# physics.comp-ph [[Back]](#toc)

### [772] [Scientific machine learning in Hydrology: a unified perspective](https://arxiv.org/abs/2506.06308)
*Adoubi Vincent De Paul Adombi*

Main category: physics.comp-ph

TL;DR: 本文综述了科学机器学习（SciML）在水文学中的应用，提出了一个统一的方法框架，以解决当前方法碎片化的问题，并指出了未来的研究方向。


<details>
  <summary>Details</summary>
Motivation: 当前水文学中科学机器学习方法缺乏统一框架，导致方法碎片化，难以评估创新性和进展。

Method: 提出了一个统一的方法框架，将物理知识融入数据驱动建模，分类整理了物理信息机器学习、物理引导机器学习、混合物理-机器学习和数据驱动的物理发现等方法。

Result: 通过统一框架，增强了概念清晰度，支持了水文学建模的累积进展。

Conclusion: 总结了各方法家族的局限性和未来机会，为系统研究提供了指导。

Abstract: Scientific machine learning (SciML) provides a structured approach to
integrating physical knowledge into data-driven modeling, offering significant
potential for advancing hydrological research. In recent years, multiple
methodological families have emerged, including physics-informed machine
learning, physics-guided machine learning, hybrid physics-machine learning, and
data-driven physics discovery. Within each of these families, a proliferation
of heterogeneous approaches has developed independently, often without
conceptual coordination. This fragmentation complicates the assessment of
methodological novelty and makes it difficult to identify where meaningful
advances can still be made in the absence of a unified conceptual framework.
This review, the first focused overview of SciML in hydrology, addresses these
limitations by proposing a unified methodological framework for each SciML
family, bringing together representative contributions into a coherent
structure that fosters conceptual clarity and supports cumulative progress in
hydrological modeling. Finally, we highlight the limitations and future
opportunities of each unified family to guide systematic research in hydrology,
where these methods remain underutilized.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [773] [Private GPTs for LLM-driven testing in software development and machine learning](https://arxiv.org/abs/2506.06509)
*Jakub Jagielski,Markus Abel*

Main category: cs.SE

TL;DR: 论文研究了私有GPT自动生成基于需求的可执行测试代码的能力，通过验收标准作为输入，探索了直接生成与通过Gherkin语法中间步骤的两种方法，发现两步法效果更佳。


<details>
  <summary>Details</summary>
Motivation: 探讨私有GPT在生成测试代码方面的能力，为产品所有者或业务智能提供直接生成可测试标准的方法。

Method: 使用验收标准作为输入，比较直接生成代码与通过Gherkin语法中间步骤的两种方法。

Result: 两步法（通过Gherkin语法）生成的测试代码质量更高，表现为更好的可读性和编码实践。

Conclusion: 结构化提示能生成更高质量的测试输出，两步法优于直接生成。

Abstract: In this contribution, we examine the capability of private GPTs to
automatically generate executable test code based on requirements. More
specifically, we use acceptance criteria as input, formulated as part of epics,
or stories, which are typically used in modern development processes. This
gives product owners, or business intelligence, respectively, a way to directly
produce testable criteria through the use of LLMs. We explore the quality of
the so-produced tests in two ways: i) directly by letting the LLM generate code
from requirements, ii) through an intermediate step using Gherkin syntax. As a
result, it turns out that the two-step procedure yields better results -where
we define better in terms of human readability and best coding practices, i.e.
lines of code and use of additional libraries typically used in testing.
Concretely, we evaluate prompt effectiveness across two scenarios: a simple
"Hello World" program and a digit classification model, showing that structured
prompts lead to higher-quality test outputs.

</details>


### [774] [Is Your Training Pipeline Production-Ready? A Case Study in the Healthcare Domain](https://arxiv.org/abs/2506.06946)
*Daniel Lawand,Lucas Quaresma,Roberto Bolgheroni,Alfredo Goldman,Renato Cordeiro Ferreira*

Main category: cs.SE

TL;DR: 论文探讨了将机器学习训练管道部署到生产环境中的挑战，通过SPIRA项目展示了从混乱架构到模块化单体再到微服务的演进过程，以提高系统的可维护性、健壮性和可扩展性。


<details>
  <summary>Details</summary>
Motivation: 研究如何将机器学习训练管道从实验性工作流转化为生产环境中的稳健系统，解决软件质量问题。

Method: 通过SPIRA项目，比较了三种不同架构版本（Big Ball of Mud、Modular Monolith、Microservices）的设计原则和模式。

Result: 展示了架构演进如何提升系统的可维护性、健壮性和可扩展性。

Conclusion: 为ML工程师和数据科学家提供了生产化ML训练管道和采用MLOps实践的实用见解。

Abstract: Deploying a Machine Learning (ML) training pipeline into production requires
robust software engineering practices. This differs significantly from
experimental workflows. This experience report investigates this challenge in
SPIRA, a project whose goal is to create an ML-Enabled System (MLES) to
pre-diagnose insufficiency respiratory via speech analysis. The first version
of SPIRA's training pipeline lacked critical software quality attributes. This
paper presents an overview of the MLES, then compares three versions of the
architecture of the Continuous Training subsystem, which evolved from a Big
Ball of Mud, to a Modular Monolith, towards Microservices. By adopting
different design principles and patterns to enhance its maintainability,
robustness, and extensibility. In this way, the paper seeks to offer insights
for both ML Engineers tasked to productionize ML training pipelines and Data
Scientists seeking to adopt MLOps practices.

</details>


### [775] [Taxonomy of migration scenarios for Qiskit refactoring using LLMs](https://arxiv.org/abs/2506.07135)
*José Manuel Suárez,Luís Mariano Bibbó,Joaquín Bogado,Alejandro Fernandez*

Main category: cs.SE

TL;DR: 该研究针对量子编程库频繁更新导致的代码重构问题，开发了量子电路重构问题的分类法，并利用LLMs分析Qiskit版本迁移中的重构需求，最终整合专家与LLM的分类法。


<details>
  <summary>Details</summary>
Motivation: 量子计算软件的快速发展和库的频繁更新增加了开发者的重构负担，且其重构问题与传统软件工程不同，因此需要系统化的分类框架。

Method: 通过分析Qiskit文档和发布说明，创建初始重构分类法，并分别由专家和LLM生成分类法，比较后整合为统一分类法。

Result: 生成了统一的重构分类法，为AI辅助迁移和自动化重构技术评估提供基础。

Conclusion: 该研究为量子软件工程提供了实用的分类框架，优化了开发流程，并推动了量子编程的最佳实践。

Abstract: As quantum computing advances, quantum programming libraries' heterogeneity
and steady evolution create new challenges for software developers. Frequent
updates in software libraries break working code that needs to be refactored,
thus adding complexity to an already complex landscape. These refactoring
challenges are, in many cases, fundamentally different from those known in
classical software engineering due to the nature of quantum computing software.
This study addresses these challenges by developing a taxonomy of quantum
circuit's refactoring problems, providing a structured framework to analyze and
compare different refactoring approaches. Large Language Models (LLMs) have
proven valuable tools for classic software development, yet their value in
quantum software engineering remains unexplored. This study uses LLMs to
categorize refactoring needs in migration scenarios between different Qiskit
versions. Qiskit documentation and release notes were scrutinized to create an
initial taxonomy of refactoring required for migrating between Qiskit releases.
Two taxonomies were produced: one by expert developers and one by an LLM. These
taxonomies were compared, analyzing differences and similarities, and were
integrated into a unified taxonomy that reflects the findings of both methods.
By systematically categorizing refactoring challenges in Qiskit, the unified
taxonomy is a foundation for future research on AI-assisted migration while
enabling a more rigorous evaluation of automated refactoring techniques.
Additionally, this work contributes to quantum software engineering (QSE) by
enhancing software development workflows, improving language compatibility, and
promoting best practices in quantum programming.

</details>


### [776] [IntenTest: Stress Testing for Intent Integrity in API-Calling LLM Agents](https://arxiv.org/abs/2506.07524)
*Shiwei Feng,Xiangzhe Xu,Xuan Chen,Kaiyuan Zhang,Syed Yusuf Ahmed,Zian Su,Mingwei Zheng,Xiangyu Zhang*

Main category: cs.SE

TL;DR: IntenTest是一个API为中心的测试框架，用于系统性地发现LLM代理中的意图完整性违规，通过生成基于工具包文档的测试任务并应用定向变异来暴露代理错误。


<details>
  <summary>Details</summary>
Motivation: LLM代理在通过自然语言指令调用API时，常因用户意图误解导致行为偏离目标，传统测试方法无法处理自然语言的模糊性。

Method: IntenTest利用语义分区组织任务，生成并变异测试任务，通过轻量级预测器评估错误触发概率，并使用数据类型感知策略记忆提高效率。

Result: 实验表明，IntenTest在80个工具包API上显著优于基线方法，能有效暴露意图完整性违规，且适应性强。

Conclusion: IntenTest为LLM代理的意图完整性测试提供了高效且通用的解决方案，适用于不断演化的API和不同领域。

Abstract: LLM agents are increasingly deployed to automate real-world tasks by invoking
APIs through natural language instructions. While powerful, they often suffer
from misinterpretation of user intent, leading to the agent's actions that
diverge from the user's intended goal, especially as external toolkits evolve.
Traditional software testing assumes structured inputs and thus falls short in
handling the ambiguity of natural language. We introduce IntenTest, an
API-centric stress testing framework that systematically uncovers intent
integrity violations in LLM agents. Unlike prior work focused on fixed
benchmarks or adversarial inputs, IntenTest generates realistic tasks based on
toolkits' documentation and applies targeted mutations to expose subtle agent
errors while preserving user intent. To guide testing, we propose semantic
partitioning, which organizes natural language tasks into meaningful categories
based on toolkit API parameters and their equivalence classes. Within each
partition, seed tasks are mutated and ranked by a lightweight predictor that
estimates the likelihood of triggering agent errors. To enhance efficiency,
IntenTest maintains a datatype-aware strategy memory that retrieves and adapts
effective mutation patterns from past cases. Experiments on 80 toolkit APIs
demonstrate that IntenTest effectively uncovers intent integrity violations,
significantly outperforming baselines in both error-exposing rate and query
efficiency. Moreover, IntenTest generalizes well to stronger target models
using smaller LLMs for test generation, and adapts to evolving APIs across
domains.

</details>


### [777] [Towards a Small Language Model Lifecycle Framework](https://arxiv.org/abs/2506.07695)
*Parsa Miraghaei,Sergio Moreschini,Antti Kolehmainen,David Hästbacka*

Main category: cs.SE

TL;DR: 该研究提出了一种小型语言模型（SLM）的生命周期框架，通过整合学术和实践资源，填补了现有研究的碎片化问题。


<details>
  <summary>Details</summary>
Motivation: 由于对高效且可部署语言模型的需求增长，但现有研究缺乏统一的生命周期视角，因此需要构建一个全面的SLM生命周期框架。

Method: 通过对36篇文献的综合调查，分析并分类了与生命周期相关的技术。

Result: 提出了一个模块化的生命周期模型，包含主要、可选和跨领域组件，支持方法重用和生命周期意识。

Conclusion: 该框架为SLM的开发与维护提供了统一基础，连接理论与实践，并指导未来研究和工具开发。

Abstract: Background: The growing demand for efficient and deployable language models
has led to increased interest in Small Language Models (SLMs). However,
existing research remains fragmented, lacking a unified lifecycle perspective.
  Objective: This study aims to define a comprehensive lifecycle framework for
SLMs by synthesizing insights from academic literature and practitioner
sources.
  Method: We conducted a comprehensive survey of 36 works, analyzing and
categorizing lifecycle-relevant techniques.
  Results: We propose a modular lifecycle model structured into main, optional,
and cross-cutting components. The model captures key interconnections across
stages, supporting method reuse, co-adaptation, and lifecycle-awareness.
  Conclusion: Our framework provides a coherent foundation for developing and
maintaining SLMs, bridging theory and practice, and guiding future research and
tool development.

</details>


<div id='physics.ed-ph'></div>

# physics.ed-ph [[Back]](#toc)

### [778] [Pendulum Tracker -- SimuFísica: A Web-based Tool for Real-time Measurement of Oscillatory Motion](https://arxiv.org/abs/2506.07301)
*Marco P. M. de Souza,Juciane G. Maia,Lilian N. de Andrade*

Main category: physics.ed-ph

TL;DR: Pendulum Tracker是一个基于计算机视觉的应用，用于实时测量物理摆的振荡运动，适用于教育平台SimuFísica。


<details>
  <summary>Details</summary>
Motivation: 为教育领域提供一个实时、准确且易于使用的工具，用于测量和分析摆的运动。

Method: 利用OpenCV.js库和设备摄像头自动检测摆的位置，实时显示角度-时间图并估算振荡周期。

Result: 实验结果与理论预测高度一致，验证了系统的准确性和教育适用性。

Conclusion: Pendulum Tracker是一个多功能工具，适用于实验物理教学，具有直观界面和数据导出功能。

Abstract: We present Pendulum Tracker, a computer vision-based application that enables
real-time measurement of the oscillatory motion of a physical pendulum.
Integrated into the educational platform SimuF\'isica, the system uses the
OpenCV.js library and runs directly in the browser, working on computers,
tablets, and smartphones. The application automatically detects the pendulum's
position via the device's camera, displaying in real time the angle-versus-time
graph and estimates of the oscillation period. Experimental case studies
demonstrate its effectiveness in measuring the period, determining
gravitational acceleration, and analyzing damped oscillations. The results show
excellent agreement with theoretical predictions, confirming the system's
accuracy and its applicability in educational contexts. The accessible
interface and the ability to export raw data make Pendulum Tracker a versatile
tool for experimental physics teaching.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [779] [KramaBench: A Benchmark for AI Systems on Data-to-Insight Pipelines over Data Lakes](https://arxiv.org/abs/2506.06541)
*Eugenie Lai,Gerardo Vitagliano,Ziyu Zhang,Sivaprasad Sudhir,Om Chabra,Anna Zeng,Anton A. Zabreyko,Chenning Li,Ferdi Kossmann,Jialin Ding,Jun Chen,Markos Markakis,Matthew Russo,Weiyang Wang,Ziniu Wu,Michael J. Cafarella,Lei Cao,Samuel Madden,Tim Kraska*

Main category: cs.DB

TL;DR: KRAMABENCH是一个包含104个真实世界数据科学管道的基准测试，用于评估AI系统在数据处理、发现、清理和编排方面的能力。


<details>
  <summary>Details</summary>
Motivation: 研究AI系统是否能够成功设计和执行复杂的数据科学管道，尤其是在需要广泛数据处理和领域知识的情况下。

Method: 使用KRAMABENCH基准测试，评估5个通用模型和3个代码生成模型在数据处理、发现、清理和编排方面的表现。

Result: 现有模型在明确的数据科学代码生成任务中表现良好，但在需要复杂数据处理和领域知识的真实世界管道中表现不足。

Conclusion: KRAMABENCH的进展是开发自主数据科学代理的关键步骤，现有模型仍需改进。

Abstract: Constructing real-world data-to-insight pipelines often involves data
extraction from data lakes, data integration across heterogeneous data sources,
and diverse operations from data cleaning to analysis. The design and
implementation of data science pipelines require domain knowledge, technical
expertise, and even project-specific insights. AI systems have shown remarkable
reasoning, coding, and understanding capabilities. However, it remains unclear
to what extent these capabilities translate into successful design and
execution of such complex pipelines. We introduce KRAMABENCH: a benchmark
composed of 104 manually-curated real-world data science pipelines spanning
1700 data files from 24 data sources in 6 different domains. We show that these
pipelines test the end-to-end capabilities of AI systems on data processing,
requiring data discovery, wrangling and cleaning, efficient processing,
statistical reasoning, and orchestrating data processing steps given a
high-level task. Our evaluation tests 5 general models and 3 code generation
models using our reference framework, DS-GURU, which instructs the AI model to
decompose a question into a sequence of subtasks, reason through each step, and
synthesize Python code that implements the proposed design. Our results on
KRAMABENCH show that, although the models are sufficiently capable of solving
well-specified data science code generation tasks, when extensive data
processing and domain knowledge are required to construct real-world data
science pipelines, existing out-of-box models fall short. Progress on
KramaBench represents crucial steps towards developing autonomous data science
agents for real-world applications. Our code, reference framework, and data are
available at https://github.com/mitdbg/KramaBench.

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [780] [Position: Simulating Society Requires Simulating Thought](https://arxiv.org/abs/2506.06958)
*Chance Jiajie Li,Jiayi Wu,Zhenze Mo,Ao Qu,Yuhan Tang,Kaiya Ivy Zhao,Yulu Gan,Jie Fan,Jiangbo Yu,Jinhua Zhao,Paul Liang,Luis Alonso,Kent Larson*

Main category: cs.CY

TL;DR: 论文提出了一种基于认知科学的生成代理框架（GenMinds），旨在提升大语言模型在社会模拟中的推理能力，并通过RECAP框架评估其因果可追溯性和一致性。


<details>
  <summary>Details</summary>
Motivation: 当前基于大语言模型的代理在模拟社会行为时缺乏内部一致性、因果推理和信念可追溯性，限制了其在分析人类推理和干预响应中的可靠性。

Method: 提出了Generative Minds（GenMinds）框架，结合认知科学支持生成代理的结构化信念表示，并设计了RECAP基准评估其推理能力。

Result: 通过RECAP框架验证了GenMinds代理在因果追溯、人口统计基础和干预一致性方面的表现，提升了社会模拟的深度。

Conclusion: GenMinds框架标志着从表面模仿到模拟思维的转变，为生成代理在社会模拟中的应用提供了更可靠的基础。

Abstract: Simulating society with large language models (LLMs), we argue, requires more
than generating plausible behavior -- it demands cognitively grounded reasoning
that is structured, revisable, and traceable. LLM-based agents are increasingly
used to emulate individual and group behavior -- primarily through prompting
and supervised fine-tuning. Yet they often lack internal coherence, causal
reasoning, and belief traceability -- making them unreliable for analyzing how
people reason, deliberate, or respond to interventions.
  To address this, we present a conceptual modeling paradigm, Generative Minds
(GenMinds), which draws from cognitive science to support structured belief
representations in generative agents. To evaluate such agents, we introduce the
RECAP (REconstructing CAusal Paths) framework, a benchmark designed to assess
reasoning fidelity via causal traceability, demographic grounding, and
intervention consistency. These contributions advance a broader shift: from
surface-level mimicry to generative agents that simulate thought -- not just
language -- for social simulations.

</details>


### [781] [Disentangling AI Alignment: A Structured Taxonomy Beyond Safety and Ethics](https://arxiv.org/abs/2506.06286)
*Kevin Baum*

Main category: cs.CY

TL;DR: 论文提出了一个结构化概念框架，用于理解AI对齐问题，区分了对齐的目标、范围和选民，为跨领域整合提供了基础。


<details>
  <summary>Details</summary>
Motivation: 随着AI技术的发展，确保AI代理在现实世界中的安全和规范性成为一个紧迫的跨学科挑战，但目前相关领域的界限和关系模糊，缺乏明确的研究指导。

Method: 作者开发了一个结构化概念框架，通过分类对齐的目标（如安全性、伦理性）、范围（结果与执行）和选民（个体与集体）来系统化AI对齐问题。

Result: 该框架揭示了多种合理的对齐配置，为跨领域的实践和哲学整合提供了基础，并明确了“全面对齐”的含义。

Conclusion: 该研究为AI对齐领域提供了清晰的概念工具，有助于指导未来的研究和实践。

Abstract: Recent advances in AI research make it increasingly plausible that artificial
agents with consequential real-world impact will soon operate beyond tightly
controlled environments. Ensuring that these agents are not only safe but that
they adhere to broader normative expectations is thus an urgent
interdisciplinary challenge. Multiple fields -- notably AI Safety, AI
Alignment, and Machine Ethics -- claim to contribute to this task. However, the
conceptual boundaries and interrelations among these domains remain vague,
leaving researchers without clear guidance in positioning their work.
  To address this meta-challenge, we develop a structured conceptual framework
for understanding AI alignment. Rather than focusing solely on alignment goals,
we introduce a taxonomy distinguishing the alignment aim (safety, ethicality,
legality, etc.), scope (outcome vs. execution), and constituency (individual
vs. collective). This structural approach reveals multiple legitimate alignment
configurations, providing a foundation for practical and philosophical
integration across domains, and clarifying what it might mean for an agent to
be aligned all-things-considered.

</details>


### [782] [Future of Work with AI Agents: Auditing Automation and Augmentation Potential across the U.S. Workforce](https://arxiv.org/abs/2506.06576)
*Yijia Shao,Humishka Zope,Yucheng Jiang,Jiaxin Pei,David Nguyen,Erik Brynjolfsson,Diyi Yang*

Main category: cs.CY

TL;DR: 本文提出了一种新的审计框架，用于评估工人希望AI代理自动化或增强哪些职业任务，并分析这些愿望与当前技术能力的匹配情况。通过WORKBank数据库和Human Agency Scale（HAS），研究揭示了任务分类和人类参与偏好的多样性。


<details>
  <summary>Details</summary>
Motivation: 随着复合AI系统的快速发展，劳动力市场面临就业替代、人类能动性减弱和过度依赖自动化等问题，但缺乏系统性研究。本文旨在填补这一空白。

Method: 研究采用音频增强的迷你访谈和HAS量表，结合WORKBank数据库（基于O*NET数据库），收集了1,500名工人和AI专家对844项任务的偏好和能力评估。

Result: 任务被分为四个区域：自动化“绿灯”区、自动化“红灯”区、研发机会区和低优先级区。研究还发现不同职业对人类参与的期望存在显著差异。

Conclusion: AI代理开发应与人类需求对齐，并帮助工人适应职场动态变化，同时关注从信息技能向人际技能的转变。

Abstract: The rapid rise of compound AI systems (a.k.a., AI agents) is reshaping the
labor market, raising concerns about job displacement, diminished human agency,
and overreliance on automation. Yet, we lack a systematic understanding of the
evolving landscape. In this paper, we address this gap by introducing a novel
auditing framework to assess which occupational tasks workers want AI agents to
automate or augment, and how those desires align with the current technological
capabilities. Our framework features an audio-enhanced mini-interview to
capture nuanced worker desires and introduces the Human Agency Scale (HAS) as a
shared language to quantify the preferred level of human involvement. Using
this framework, we construct the WORKBank database, building on the U.S.
Department of Labor's O*NET database, to capture preferences from 1,500 domain
workers and capability assessments from AI experts across over 844 tasks
spanning 104 occupations. Jointly considering the desire and technological
capability divides tasks in WORKBank into four zones: Automation "Green Light"
Zone, Automation "Red Light" Zone, R&D Opportunity Zone, Low Priority Zone.
This highlights critical mismatches and opportunities for AI agent development.
Moving beyond a simple automate-or-not dichotomy, our results reveal diverse
HAS profiles across occupations, reflecting heterogeneous expectations for
human involvement. Moreover, our study offers early signals of how AI agent
integration may reshape the core human competencies, shifting from
information-focused skills to interpersonal ones. These findings underscore the
importance of aligning AI agent development with human desires and preparing
workers for evolving workplace dynamics.

</details>


### [783] [Privacy Perspectives and Practices of Chinese Smart Home Product Teams](https://arxiv.org/abs/2506.06591)
*Shijing He,Yaxiong Lei,Xiao Zhan,Chi Zhang,Juan Ye,Ruba Abu-Salma,Jose Such*

Main category: cs.CY

TL;DR: 研究探讨了中国智能家居产品团队的隐私观点与实践，发现其更注重国家安全而非个人隐私，并提出相关建议。


<details>
  <summary>Details</summary>
Motivation: 填补非西方背景下智能家居产品团队隐私实践的空白。

Method: 对27名中国智能家居产品团队成员进行半结构化访谈。

Result: 参与者更强调遵守中国数据隐私法律，文化和社会因素影响隐私与便利的平衡。

Conclusion: 提出针对中国智能家居隐私问题的建议和社会技术干预措施。

Abstract: Previous research has explored the privacy needs and concerns of device
owners, primary users, and different bystander groups with regard to smart home
devices like security cameras, smart speakers, and hubs, but little is known
about the privacy views and practices of smart home product teams, particularly
those in non-Western contexts. This paper presents findings from 27
semi-structured interviews with Chinese smart home product team members,
including product/project managers, software/hardware engineers, user
experience (UX) designers, legal/privacy experts, and marketers/operation
specialists. We examine their privacy perspectives, practices, and risk
mitigation strategies. Our results show that participants emphasized compliance
with Chinese data privacy laws, which typically prioritized national security
over individual privacy rights. China-specific cultural, social, and legal
factors also influenced participants' ethical considerations and attitudes
toward balancing user privacy and security with convenience. Drawing on our
findings, we propose a set of recommendations for smart home product teams,
along with socio-technical and legal interventions to address smart home
privacy issues-especially those belonging to at-risk groups-in Chinese
multi-user smart homes.

</details>


### [784] [How Malicious AI Swarms Can Threaten Democracy](https://arxiv.org/abs/2506.06299)
*Daniel Thilo Schroeder,Meeyoung Cha,Andrea Baronchelli,Nick Bostrom,Nicholas A. Christakis,David Garcia,Amit Goldenberg,Yara Kyrychenko,Kevin Leyton-Brown,Nina Lutz,Gary Marcus,Filippo Menczer,Gordon Pennycook,David G. Rand,Frank Schweitzer,Christopher Summerfield,Audrey Tang,Jay Van Bavel,Sander van der Linden,Dawn Song,Jonas R. Kunst*

Main category: cs.CY

TL;DR: 论文探讨了AI技术可能带来的新型恶意群体行为及其对社会的威胁，并提出了三方面的应对措施。


<details>
  <summary>Details</summary>
Motivation: 随着AI技术的发展，恶意AI群体可能对社会造成严重威胁，包括破坏民主进程、侵蚀信任等，亟需应对。

Method: 提出了平台端防御、模型端保障和系统级监管的三方面措施。

Result: 恶意AI群体可能导致虚假共识、现实分裂、骚扰等问题，威胁社会稳定性。

Conclusion: 需要通过多层次的防御和监管措施来应对恶意AI群体的威胁。

Abstract: Advances in AI portend a new era of sophisticated disinformation operations.
While individual AI systems already create convincing -- and at times
misleading -- information, an imminent development is the emergence of
malicious AI swarms. These systems can coordinate covertly, infiltrate
communities, evade traditional detectors, and run continuous A/B tests, with
round-the-clock persistence. The result can include fabricated grassroots
consensus, fragmented shared reality, mass harassment, voter micro-suppression
or mobilization, contamination of AI training data, and erosion of
institutional trust. With democratic processes worldwide increasingly
vulnerable, we urge a three-pronged response: (1) platform-side defenses --
always-on swarm-detection dashboards, pre-election high-fidelity
swarm-simulation stress-tests, transparency audits, and optional client-side
"AI shields" for users; (2) model-side safeguards -- standardized
persuasion-risk tests, provenance-authenticating passkeys, and watermarking;
and (3) system-level oversight -- a UN-backed AI Influence Observatory.

</details>


### [785] [Human and AI collaboration in Fitness Education:A Longitudinal Study with a Pilates Instructor](https://arxiv.org/abs/2506.06383)
*Qian Huang,King Wang Poon*

Main category: cs.CY

TL;DR: 研究探讨了AI与人类在健身教育中的协作，通过一年的定性案例研究分析了生成式AI在普拉提教学中的应用。


<details>
  <summary>Details</summary>
Motivation: 人工智能可能改变教学和辅导方式，但其与人类专业知识的协同作用尚不明确。

Method: 采用为期一年的定性案例研究，研究者参与普拉提课程并进行双周半结构化访谈，探索生成式AI在课程规划和教学中的整合。

Result: 研究发现生成式AI可以辅助普拉提教学，但具体效果和方式需进一步验证。

Conclusion: AI与人类协作在健身教育中具有潜力，但需更多研究明确其最佳角色。

Abstract: Artificial intelligence is poised to transform teaching and coaching
practices,yet its optimal role alongside human expertise remains unclear.This
study investigates human and AI collaboration in fitness education through a
one year qualitative case study with a Pilates instructor.The researcher
participated in the instructor classes and conducted biweekly semi structured
interviews to explore how generative AI could be integrated into class planning
and instruction.

</details>


### [786] [Benchmarking Large Language Models on Homework Assessment in Circuit Analysis](https://arxiv.org/abs/2506.06390)
*Liangliang Chen,Zhihao Qin,Yiming Guo,Jacqueline Rohde,Ying Zhang*

Main category: cs.CY

TL;DR: 论文研究了大型语言模型（LLMs）在工程教育中的应用，特别是评估电路分析课程作业的能力，发现GPT-4o和Llama 3 70B表现优于GPT-3.5 Turbo。


<details>
  <summary>Details</summary>
Motivation: 利用LLMs的广泛知识和快速进步，探索其在工程教育中的潜力，尤其是作业评估的可靠性。

Method: 开发了包含电路分析问题参考和学生解答的数据集，转换为LaTeX格式，设计提示模板测试五个评估指标。

Result: GPT-4o和Llama 3 70B在所有指标上显著优于GPT-3.5 Turbo，各有优势。

Conclusion: 研究为开发可靠的个性化电路分析辅导工具提供了基准和见解，未来可推广到其他工程课程。

Abstract: Large language models (LLMs) have the potential to revolutionize various
fields, including code development, robotics, finance, and education, due to
their extensive prior knowledge and rapid advancements. This paper investigates
how LLMs can be leveraged in engineering education. Specifically, we benchmark
the capabilities of different LLMs, including GPT-3.5 Turbo, GPT-4o, and Llama
3 70B, in assessing homework for an undergraduate-level circuit analysis
course. We have developed a novel dataset consisting of official reference
solutions and real student solutions to problems from various topics in circuit
analysis. To overcome the limitations of image recognition in current
state-of-the-art LLMs, the solutions in the dataset are converted to LaTeX
format. Using this dataset, a prompt template is designed to test five metrics
of student solutions: completeness, method, final answer, arithmetic error, and
units. The results show that GPT-4o and Llama 3 70B perform significantly
better than GPT-3.5 Turbo across all five metrics, with GPT-4o and Llama 3 70B
each having distinct advantages in different evaluation aspects. Additionally,
we present insights into the limitations of current LLMs in several aspects of
circuit analysis. Given the paramount importance of ensuring reliability in
LLM-generated homework assessment to avoid misleading students, our results
establish benchmarks and offer valuable insights for the development of a
reliable, personalized tutor for circuit analysis -- a focus of our future
work. Furthermore, the proposed evaluation methods can be generalized to a
broader range of courses for engineering education in the future.

</details>


### [787] [From Rogue to Safe AI: The Role of Explicit Refusals in Aligning LLMs with International Humanitarian Law](https://arxiv.org/abs/2506.06391)
*John Mavi,Diana Teodora Găitan,Sergio Coronado*

Main category: cs.CY

TL;DR: 该研究评估了8种主流大语言模型（LLMs）在国际人道法（IHL）合规性上的表现，发现模型虽能拒绝违法请求，但回应的清晰度和一致性存在差异。标准化安全提示显著提升了拒绝解释的质量，但复杂请求仍暴露漏洞。


<details>
  <summary>Details</summary>
Motivation: LLMs在各领域广泛应用，但其与国际人道法的合规性尚不明确，需评估其拒绝违法请求的能力及回应的清晰度。

Method: 研究测试了8种LLMs对明确违反IHL的提示的拒绝能力，并评估其回应的清晰度和建设性。同时测试了标准化安全提示的效果。

Result: 多数模型能拒绝违法请求，但回应的清晰度不一。标准化提示显著提升解释质量，但复杂请求仍存在问题。

Conclusion: 研究为开发更安全、透明的AI系统提供了基准，并展示了轻量级干预的有效性，但复杂请求仍需进一步改进。

Abstract: Large Language Models (LLMs) are widely used across sectors, yet their
alignment with International Humanitarian Law (IHL) is not well understood.
This study evaluates eight leading LLMs on their ability to refuse prompts that
explicitly violate these legal frameworks, focusing also on helpfulness - how
clearly and constructively refusals are communicated. While most models
rejected unlawful requests, the clarity and consistency of their responses
varied. By revealing the model's rationale and referencing relevant legal or
safety principles, explanatory refusals clarify the system's boundaries, reduce
ambiguity, and help prevent misuse. A standardised system-level safety prompt
significantly improved the quality of the explanations expressed within
refusals in most models, highlighting the effectiveness of lightweight
interventions. However, more complex prompts involving technical language or
requests for code revealed ongoing vulnerabilities. These findings contribute
to the development of safer, more transparent AI systems and propose a
benchmark to evaluate the compliance of LLM with IHL.

</details>


### [788] [LLMs as World Models: Data-Driven and Human-Centered Pre-Event Simulation for Disaster Impact Assessment](https://arxiv.org/abs/2506.06355)
*Lingyao Li,Dawei Li,Zhenhui Ou,Xiaoran Xu,Jingxiao Liu,Zihui Ma,Runlong Yu,Min Deng*

Main category: cs.CY

TL;DR: 该研究利用大语言模型（LLMs）模拟地震影响，结合多模态数据预测地震烈度，结果显示与真实数据高度吻合。


<details>
  <summary>Details</summary>
Motivation: 提升对突发性灾害（如地震）的主动应对能力，需要高效的模拟方法。LLMs作为世界模型在复杂场景模拟中展现出潜力。

Method: 使用多模态数据集（地理空间、社会经济、建筑和街景图像数据），结合RAG和ICL技术，生成地震烈度预测。

Result: 在2014年纳帕和2019年里奇克莱斯特地震中，预测结果与USGS报告高度一致（相关性0.88，RMSE 0.77）。视觉数据显著提升准确性。

Conclusion: LLMs在模拟灾害影响方面具有潜力，有助于加强事前规划。

Abstract: Efficient simulation is essential for enhancing proactive preparedness for
sudden-onset disasters such as earthquakes. Recent advancements in large
language models (LLMs) as world models show promise in simulating complex
scenarios. This study examines multiple LLMs to proactively estimate perceived
earthquake impacts. Leveraging multimodal datasets including geospatial,
socioeconomic, building, and street-level imagery data, our framework generates
Modified Mercalli Intensity (MMI) predictions at zip code and county scales.
Evaluations on the 2014 Napa and 2019 Ridgecrest earthquakes using USGS ''Did
You Feel It? (DYFI)'' reports demonstrate significant alignment, as evidenced
by a high correlation of 0.88 and a low RMSE of 0.77 as compared to real
reports at the zip code level. Techniques such as RAG and ICL can improve
simulation performance, while visual inputs notably enhance accuracy compared
to structured numerical data alone. These findings show the promise of LLMs in
simulating disaster impacts that can help strengthen pre-event planning.

</details>


### [789] [Large Language Models Can Be a Viable Substitute for Expert Political Surveys When a Shock Disrupts Traditional Measurement Approaches](https://arxiv.org/abs/2506.06540)
*Patrick Y. Wu*

Main category: cs.CY

TL;DR: 论文探讨了在突发事件（如2025年DOGE联邦裁员）后，专家判断受结果影响，难以重建事件前认知。提出用大语言模型（LLMs）替代传统政治调查，并通过DOGE案例验证其可行性。


<details>
  <summary>Details</summary>
Motivation: 研究突发事件后专家判断受结果影响的问题，探索LLMs作为替代传统调查方法的潜力。

Method: 使用LLMs进行成对比较提示，生成联邦机构意识形态分数，并与裁员前的专家测量结果对比。

Result: LLMs生成的分数能复制裁员前专家测量结果，并预测被DOGE针对的机构。此外，某些机构的知识属性也能预测裁员目标。

Conclusion: LLMs可作为传统调查的替代方法，尤其在突发事件后，并提出使用LLMs的两部分标准。

Abstract: After a disruptive event or shock, such as the Department of Government
Efficiency (DOGE) federal layoffs of 2025, expert judgments are colored by
knowledge of the outcome. This can make it difficult or impossible to
reconstruct the pre-event perceptions needed to study the factors associated
with the event. This position paper argues that large language models (LLMs),
trained on vast amounts of digital media data, can be a viable substitute for
expert political surveys when a shock disrupts traditional measurement. We
analyze the DOGE layoffs as a specific case study for this position. We use
pairwise comparison prompts with LLMs and derive ideology scores for federal
executive agencies. These scores replicate pre-layoff expert measures and
predict which agencies were targeted by DOGE. We also use this same approach
and find that the perceptions of certain federal agencies as knowledge
institutions predict which agencies were targeted by DOGE, even when
controlling for ideology. This case study demonstrates that using LLMs allows
us to rapidly and easily test the associated factors hypothesized behind the
shock. More broadly, our case study of this recent event exemplifies how LLMs
offer insights into the correlational factors of the shock when traditional
measurement techniques fail. We conclude by proposing a two-part criterion for
when researchers can turn to LLMs as a substitute for expert political surveys.

</details>


### [790] [Evaluating Large Language Model Capabilities in Assessing Spatial Econometrics Research](https://arxiv.org/abs/2506.06377)
*Giuseppe Arbia,Luca Morandini,Vincenzo Nardelli*

Main category: cs.CY

TL;DR: 论文研究了大型语言模型（LLMs）评估空间计量经济学中实证结果的经济合理性和理论一致性的能力，发现LLMs在变量选择上表现优异，但在更深层次评估如系数合理性和发表适宜性上表现不一。


<details>
  <summary>Details</summary>
Motivation: 探讨LLMs在学术评估中的潜力，尤其是在空间计量经济学领域的应用，以辅助同行评审。

Method: 通过创建原始和人为修改的“反事实”摘要，由多种LLMs进行评估，包括定性分析和结构化二元分类。

Result: LLMs在变量选择上表现优秀（如GPT-4o的F1分数为0.87），但在更深层次评估中表现不稳定，且模型选择和论文特性显著影响评估准确性。

Conclusion: LLMs适合辅助初步表面检查，但在深度经济推理上仍需人类监督，建议其在同行评审中发挥辅助作用。

Abstract: This paper investigates Large Language Models (LLMs) ability to assess the
economic soundness and theoretical consistency of empirical findings in spatial
econometrics. We created original and deliberately altered "counterfactual"
summaries from 28 published papers (2005-2024), which were evaluated by a
diverse set of LLMs. The LLMs provided qualitative assessments and structured
binary classifications on variable choice, coefficient plausibility, and
publication suitability. The results indicate that while LLMs can expertly
assess the coherence of variable choices (with top models like GPT-4o achieving
an overall F1 score of 0.87), their performance varies significantly when
evaluating deeper aspects such as coefficient plausibility and overall
publication suitability. The results further revealed that the choice of LLM,
the specific characteristics of the paper and the interaction between these two
factors significantly influence the accuracy of the assessment, particularly
for nuanced judgments. These findings highlight LLMs' current strengths in
assisting with initial, more surface-level checks and their limitations in
performing comprehensive, deep economic reasoning, suggesting a potential
assistive role in peer review that still necessitates robust human oversight.

</details>


### [791] [Deepfake Technology Unveiled: The Commoditization of AI and Its Impact on Digital Trust](https://arxiv.org/abs/2506.07363)
*Claudiu Popa,Rex Pallath,Liam Cunningham,Hewad Tahiri,Abiram Kesavarajah,Tao Wu*

Main category: cs.CY

TL;DR: 论文探讨了深度伪造技术的普及及其对数字信任的影响，分析了其带来的风险和挑战。


<details>
  <summary>Details</summary>
Motivation: 随着生成式AI的普及，深度伪造技术的门槛降低，但其滥用可能威胁信任、隐私和安全。

Method: 使用低成本工具（如Runway、Rope、ElevenLabs）展示深度伪造的易操作性，并分析其技术和伦理挑战。

Result: 深度伪造技术对个人和组织构成风险，亟需监管框架和公众意识。

Conclusion: 需通过法规、公众教育和合作来维护数字媒体的信任。

Abstract: Deepfake Technology Unveiled: The Commoditization of AI and Its Impact on
Digital Trust. With the increasing accessibility of generative AI, tools for
voice cloning, face-swapping, and synthetic media creation have advanced
significantly, lowering both financial and technical barriers for their use.
While these technologies present innovative opportunities, their rapid growth
raises concerns about trust, privacy, and security. This white paper explores
the implications of deepfake technology, analyzing its role in enabling fraud,
misinformation, and the erosion of authenticity in multimedia. Using
cost-effective, easy to use tools such as Runway, Rope, and ElevenLabs, we
explore how realistic deepfakes can be created with limited resources,
demonstrating the risks posed to individuals and organizations alike. By
analyzing the technical and ethical challenges of deepfake mitigation and
detection, we emphasize the urgent need for regulatory frameworks, public
awareness, and collaborative efforts to maintain trust in digital media.

</details>


<div id='cs.LO'></div>

# cs.LO [[Back]](#toc)

### [792] [Recursive Semantic Anchoring in ISO 639:2023: A Structural Extension to ISO/TC 37 Frameworks](https://arxiv.org/abs/2506.06870)
*Bugra Kilictas,Faruk Alpay*

Main category: cs.LO

TL;DR: 论文提出了一种递归语义锚定方法，用于处理语言变体和混合语的语义漂移问题，通过固定点操作符建模漂移，并在实验中验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: ISO 639:2023缺乏处理方言漂移和混合语的机器原生机制，因此需要一种形式化方法来建模语义漂移。

Method: 提出递归语义锚定方法，使用固定点操作符建模语义漂移，并通过范畴论和RDF/Turtle模式实现。

Result: 实验表明，该方法在语言识别和翻译任务中对噪声或混合输入具有更高的准确性。

Conclusion: 该框架为未来标准提供了兼容ISO/TC 37的、支持语义漂移的AI可处理层。

Abstract: ISO 639:2023 unifies the ISO language-code family and introduces contextual
metadata, but it lacks a machine-native mechanism for handling dialectal drift
and creole mixtures. We propose a formalisation of recursive semantic
anchoring, attaching to every language entity $\chi$ a family of fixed-point
operators $\phi_{n,m}$ that model bounded semantic drift via the relation
$\phi_{n,m}(\chi) = \chi \oplus \Delta(\chi)$, where $\Delta(\chi)$ is a drift
vector in a latent semantic manifold. The base anchor $\phi_{0,0}$ recovers the
canonical ISO 639:2023 identity, whereas $\phi_{99,9}$ marks the maximal drift
state that triggers a deterministic fallback. Using category theory, we treat
the operators $\phi_{n,m}$ as morphisms and drift vectors as arrows in a
category $\mathrm{DriftLang}$. A functor $\Phi: \mathrm{DriftLang} \to
\mathrm{AnchorLang}$ maps every drifted object to its unique anchor and proves
convergence. We provide an RDF/Turtle schema (\texttt{BaseLanguage},
\texttt{DriftedLanguage}, \texttt{ResolvedAnchor}) and worked examples -- e.g.,
$\phi_{8,4}$ (Standard Mandarin) versus $\phi_{8,7}$ (a colloquial variant),
and $\phi_{1,7}$ for Nigerian Pidgin anchored to English. Experiments with
transformer models show higher accuracy in language identification and
translation on noisy or code-switched input when the $\phi$-indices are used to
guide fallback routing. The framework is compatible with ISO/TC 37 and provides
an AI-tractable, drift-aware semantic layer for future standards.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [793] [TimeWak: Temporal Chained-Hashing Watermark for Time Series Data](https://arxiv.org/abs/2506.06407)
*Zhi Wen Soi,Chaoyi Zhu,Fouad Abiad,Aditya Shankar,Jeroen M. Galjaard,Huijuan Wang,Lydia Y. Chen*

Main category: cs.CR

TL;DR: TimeWak是一种用于多元时间序列扩散模型的水印算法，直接在真实时间-特征空间嵌入水印，解决了特征异质性和时间依赖性问题，同时保持高数据效用和水印可检测性。


<details>
  <summary>Details</summary>
Motivation: 隐私敏感数据（如患者fMRI记录）的共享需要合成数据具有高效用性和可追溯性，但现有水印方法在潜在空间中嵌入，与时间序列生成器的真实空间操作不兼容。

Method: 提出TimeWak算法，通过时间链式哈希水印在真实时间-特征空间嵌入，并采用ε-精确反演解决扩散过程反演的非均匀误差分布问题。

Result: 在5个数据集上评估，TimeWak在上下文FID分数和相关性分数上分别比基线提升61.96%和8.44%，且水印可检测性稳定。

Conclusion: TimeWak成功解决了真实空间水印的挑战，为隐私敏感数据的共享提供了高效且可追溯的解决方案。

Abstract: Synthetic time series generated by diffusion models enable sharing
privacy-sensitive datasets, such as patients' functional MRI records. Key
criteria for synthetic data include high data utility and traceability to
verify the data source. Recent watermarking methods embed in homogeneous latent
spaces, but state-of-the-art time series generators operate in real space,
making latent-based watermarking incompatible. This creates the challenge of
watermarking directly in real space while handling feature heterogeneity and
temporal dependencies. We propose TimeWak, the first watermarking algorithm for
multivariate time series diffusion models. To handle temporal dependence and
spatial heterogeneity, TimeWak embeds a temporal chained-hashing watermark
directly within the real temporal-feature space. The other unique feature is
the $\epsilon$-exact inversion, which addresses the non-uniform reconstruction
error distribution across features from inverting the diffusion process to
detect watermarks. We derive the error bound of inverting multivariate time
series and further maintain high watermark detectability. We extensively
evaluate TimeWak on its impact on synthetic data quality, watermark
detectability, and robustness under various post-editing attacks, against 5
datasets and baselines of different temporal lengths. Our results show that
TimeWak achieves improvements of 61.96% in context-FID score, and 8.44% in
correlational scores against the state-of-the-art baseline, while remaining
consistently detectable.

</details>


### [794] [HeavyWater and SimplexWater: Watermarking Low-Entropy Text Distributions](https://arxiv.org/abs/2506.06409)
*Dor Tsur,Carol Xuan Long,Claudio Mayrink Verdun,Hsiang Hsu,Chen-Fu Chen,Haim Permuter,Sajani Vithana,Flavio P. Calmon*

Main category: cs.CR

TL;DR: 论文提出了一种优化框架，设计了两种新水印（HeavyWater和SimplexWater），用于在低熵任务中高效检测水印并最小化文本失真。


<details>
  <summary>Details</summary>
Motivation: 解决大型语言模型（LLM）水印在低熵生成任务（如编码）中的挑战，优化水印设计以提高检测率和减少文本失真。

Method: 提出优化框架，设计两种可调水印（HeavyWater和SimplexWater），通过随机辅助信息最大化检测率并最小化失真。

Result: 实验表明，两种水印在低熵任务中能高效检测且对文本质量影响小，并揭示了水印与编码理论的新联系。

Conclusion: HeavyWater和SimplexWater为LLM水印提供了高效、可调的解决方案，适用于低熵任务。

Abstract: Large language model (LLM) watermarks enable authentication of text
provenance, curb misuse of machine-generated text, and promote trust in AI
systems. Current watermarks operate by changing the next-token predictions
output by an LLM. The updated (i.e., watermarked) predictions depend on random
side information produced, for example, by hashing previously generated tokens.
LLM watermarking is particularly challenging in low-entropy generation tasks -
such as coding - where next-token predictions are near-deterministic. In this
paper, we propose an optimization framework for watermark design. Our goal is
to understand how to most effectively use random side information in order to
maximize the likelihood of watermark detection and minimize the distortion of
generated text. Our analysis informs the design of two new watermarks:
HeavyWater and SimplexWater. Both watermarks are tunable, gracefully
trading-off between detection accuracy and text distortion. They can also be
applied to any LLM and are agnostic to side information generation. We examine
the performance of HeavyWater and SimplexWater through several benchmarks,
demonstrating that they can achieve high watermark detection accuracy with
minimal compromise of text generation quality, particularly in the low-entropy
regime. Our theoretical analysis also reveals surprising new connections
between LLM watermarking and coding theory. The code implementation can be
found in https://github.com/DorTsur/HeavyWater_SimplexWater

</details>


### [795] [Benchmarking Misuse Mitigation Against Covert Adversaries](https://arxiv.org/abs/2506.06414)
*Davis Brown,Mahdi Sabbaghi,Luze Sun,Alexander Robey,George J. Pappas,Eric Wong,Hamed Hassani*

Main category: cs.CR

TL;DR: 该论文提出了一种新的语言模型安全评估方法，针对隐蔽攻击（通过多个看似无害的查询组合完成危险任务）设计了状态防御基准（BSD），并验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有安全评估主要关注明显攻击和低风险任务，难以检测隐蔽攻击（通过多个无害查询组合完成危险任务）。

Method: 开发了BSD（状态防御基准）数据生成管道，用于自动化评估隐蔽攻击及防御措施，并创建了两个新数据集。

Result: 实验表明，分解攻击是有效的滥用手段，而状态防御是有效的应对措施。

Conclusion: BSD为语言模型安全提供了新的评估工具，状态防御是应对隐蔽攻击的有效方法。

Abstract: Existing language model safety evaluations focus on overt attacks and
low-stakes tasks. Realistic attackers can subvert current safeguards by
requesting help on small, benign-seeming tasks across many independent queries.
Because individual queries do not appear harmful, the attack is hard to
{detect}. However, when combined, these fragments uplift misuse by helping the
attacker complete hard and dangerous tasks. Toward identifying defenses against
such strategies, we develop Benchmarks for Stateful Defenses (BSD), a data
generation pipeline that automates evaluations of covert attacks and
corresponding defenses. Using this pipeline, we curate two new datasets that
are consistently refused by frontier models and are too difficult for weaker
open-weight models. Our evaluations indicate that decomposition attacks are
effective misuse enablers, and highlight stateful defenses as a countermeasure.

</details>


### [796] [Auditing Black-Box LLM APIs with a Rank-Based Uniformity Test](https://arxiv.org/abs/2506.06975)
*Xiaoyuan Zhu,Yaowen Ye,Tianyi Qiu,Hanlin Zhu,Sijun Tan,Ajraf Mannan,Jonathan Michala,Raluca Ada Popa,Willie Neiswanger*

Main category: cs.CR

TL;DR: 提出了一种基于秩的统一性测试方法，用于验证黑盒LLM与本地部署的真实模型的行为一致性，解决了API提供商可能暗中替换模型的问题。


<details>
  <summary>Details</summary>
Motivation: API提供商可能为了降低成本或恶意修改模型行为，暗中提供量化或微调的模型变体，影响性能和安全性，而用户缺乏透明度和检测手段。

Method: 采用基于秩的统一性测试方法，无需访问模型权重或输出logits，高效且不易被检测。

Result: 在多种威胁场景（如量化、有害微调、越狱提示和完整模型替换）下，该方法在有限查询预算下表现出优于现有方法的统计能力。

Conclusion: 该方法能有效检测黑盒LLM的行为变化，具有高准确性和鲁棒性，适用于对抗性环境。

Abstract: As API access becomes a primary interface to large language models (LLMs),
users often interact with black-box systems that offer little transparency into
the deployed model. To reduce costs or maliciously alter model behaviors, API
providers may discreetly serve quantized or fine-tuned variants, which can
degrade performance and compromise safety. Detecting such substitutions is
difficult, as users lack access to model weights and, in most cases, even
output logits. To tackle this problem, we propose a rank-based uniformity test
that can verify the behavioral equality of a black-box LLM to a locally
deployed authentic model. Our method is accurate, query-efficient, and avoids
detectable query patterns, making it robust to adversarial providers that
reroute or mix responses upon the detection of testing attempts. We evaluate
the approach across diverse threat scenarios, including quantization, harmful
fine-tuning, jailbreak prompts, and full model substitution, showing that it
consistently achieves superior statistical power over prior methods under
constrained query budgets.

</details>


### [797] [HauntAttack: When Attack Follows Reasoning as a Shadow](https://arxiv.org/abs/2506.07031)
*Jingyuan Ma,Rui Li,Zheng Li,Junfeng Liu,Lei Sha,Zhifang Sui*

Main category: cs.CR

TL;DR: 论文提出HauntAttack框架，揭示大型推理模型（LRMs）在安全性与推理能力之间的权衡问题。


<details>
  <summary>Details</summary>
Motivation: 探讨LRMs在推理能力提升过程中可能引发的安全隐患，特别是推理与有害性交织时的安全漏洞。

Method: 提出HauntAttack框架，通过将有害指令嵌入推理问题，逐步引导模型生成不安全输出。

Result: 实验表明，即使最先进的LRMs也存在显著安全漏洞，并分析了不同模型、有害指令类型及输出模式。

Conclusion: 研究为LRMs的安全性提供了重要见解，揭示了安全与推理能力的潜在冲突。

Abstract: Emerging Large Reasoning Models (LRMs) consistently excel in mathematical and
reasoning tasks, showcasing exceptional capabilities. However, the enhancement
of reasoning abilities and the exposure of their internal reasoning processes
introduce new safety vulnerabilities. One intriguing concern is: when reasoning
is strongly entangled with harmfulness, what safety-reasoning trade-off do LRMs
exhibit? To address this issue, we introduce HauntAttack, a novel and
general-purpose black-box attack framework that systematically embeds harmful
instructions into reasoning questions. Specifically, we treat reasoning
questions as carriers and substitute one of their original conditions with a
harmful instruction. This process creates a reasoning pathway in which the
model is guided step by step toward generating unsafe outputs. Based on
HauntAttack, we conduct comprehensive experiments on multiple LRMs. Our results
reveal that even the most advanced LRMs exhibit significant safety
vulnerabilities. Additionally, we perform a detailed analysis of different
models, various types of harmful instructions, and model output patterns,
providing valuable insights into the security of LRMs.

</details>


### [798] [Beyond Jailbreaks: Revealing Stealthier and Broader LLM Security Risks Stemming from Alignment Failures](https://arxiv.org/abs/2506.07402)
*Yukai Zhou,Sibei Yang,Wenjie Wang*

Main category: cs.CR

TL;DR: 论文提出了LLM在隐式危害（Implicit Harm）方面的风险，并通过JailFlipBench基准测试和攻击方法揭示了其现实危害。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注显式危害（如越狱攻击），而忽略了看似无害的输入可能导致的隐式危害，这种危害同样具有现实风险。

Method: 提出JailFlipBench基准测试，涵盖单模态、多模态和事实扩展场景，并开发了JailFlip攻击方法，对多种LLM进行评估。

Result: 评估显示隐式危害在开源和黑盒LLM中普遍存在，证实其现实风险。

Conclusion: 呼吁扩展LLM安全性评估范围，超越传统越狱攻击范式，关注隐式危害。

Abstract: Large language models (LLMs) are increasingly deployed in real-world
applications, raising concerns about their security. While jailbreak attacks
highlight failures under overtly harmful queries, they overlook a critical
risk: incorrectly answering harmless-looking inputs can be dangerous and cause
real-world harm (Implicit Harm). We systematically reformulate the LLM risk
landscape through a structured quadrant perspective based on output factuality
and input harmlessness, uncovering an overlooked high-risk region. To
investigate this gap, we propose JailFlipBench, a benchmark aims to capture
implicit harm, spanning single-modal, multimodal, and factual extension
scenarios with diverse evaluation metrics. We further develop initial JailFlip
attack methodologies and conduct comprehensive evaluations across multiple
open-source and black-box LLMs, show that implicit harm present immediate and
urgent real-world risks, calling for broader LLM safety assessments and
alignment beyond conventional jailbreak paradigms.

</details>


### [799] [Fuse and Federate: Enhancing EV Charging Station Security with Multimodal Fusion and Federated Learning](https://arxiv.org/abs/2506.06730)
*Rabah Rahal,Abdelaziz Amara Korba,Yacine Ghamri-Doudane*

Main category: cs.CR

TL;DR: 本文提出了一种新型入侵检测框架，利用多模态数据源和分布式学习技术，显著提升了电动汽车充电设备（EVSE）的网络安全性能。


<details>
  <summary>Details</summary>
Motivation: 电动汽车充电设备（EVSE）作为智能电网的关键组成部分，面临日益复杂的网络安全威胁，传统入侵检测系统难以应对。

Method: 提出了一种基于多模态数据（网络流量和内核事件）的入侵检测框架，采用分布式学习和联邦学习技术。

Result: 实验结果显示，该框架在分散环境中检测率超过98%，精确率超过97%。

Conclusion: 该框架为EVSE安全提供了可扩展且隐私保护的解决方案，有效应对高级网络威胁。

Abstract: The rapid global adoption of electric vehicles (EVs) has established electric
vehicle supply equipment (EVSE) as a critical component of smart grid
infrastructure. While essential for ensuring reliable energy delivery and
accessibility, EVSE systems face significant cybersecurity challenges,
including network reconnaissance, backdoor intrusions, and distributed
denial-of-service (DDoS) attacks. These emerging threats, driven by the
interconnected and autonomous nature of EVSE, require innovative and adaptive
security mechanisms that go beyond traditional intrusion detection systems
(IDS). Existing approaches, whether network-based or host-based, often fail to
detect sophisticated and targeted attacks specifically crafted to exploit new
vulnerabilities in EVSE infrastructure. This paper proposes a novel intrusion
detection framework that leverages multimodal data sources, including network
traffic and kernel events, to identify complex attack patterns. The framework
employs a distributed learning approach, enabling collaborative intelligence
across EVSE stations while preserving data privacy through federated learning.
Experimental results demonstrate that the proposed framework outperforms
existing solutions, achieving a detection rate above 98% and a precision rate
exceeding 97% in decentralized environments. This solution addresses the
evolving challenges of EVSE security, offering a scalable and privacypreserving
response to advanced cyber threats

</details>


### [800] [Ai-Driven Vulnerability Analysis in Smart Contracts: Trends, Challenges and Future Directions](https://arxiv.org/abs/2506.06735)
*Mesut Ozdag*

Main category: cs.CR

TL;DR: 论文探讨了AI驱动的智能合约漏洞检测技术，包括机器学习、深度学习、图神经网络和基于Transformer的模型，分析了其优缺点及未来挑战。


<details>
  <summary>Details</summary>
Motivation: 智能合约漏洞导致巨大经济损失，传统审计方法在可扩展性和自动化方面存在不足，AI技术为解决这些问题提供了新途径。

Method: 研究采用机器学习、深度学习、图神经网络和Transformer模型，分析其代码表示、语义信息处理及对实际漏洞的检测能力。

Result: AI技术在检测复杂漏洞方面表现出潜力，但不同方法在准确性、可解释性、计算开销和实时性方面各有优劣。

Conclusion: AI驱动的智能合约漏洞检测具有前景，但仍需解决可扩展性、实时性和模型解释性等挑战。

Abstract: Smart contracts, integral to blockchain ecosystems, enable decentralized
applications to execute predefined operations without intermediaries. Their
ability to enforce trustless interactions has made them a core component of
platforms such as Ethereum. Vulnerabilities such as numerical overflows,
reentrancy attacks, and improper access permissions have led to the loss of
millions of dollars throughout the blockchain and smart contract sector.
Traditional smart contract auditing techniques such as manual code reviews and
formal verification face limitations in scalability, automation, and
adaptability to evolving development patterns. As a result, AI-based solutions
have emerged as a promising alternative, offering the ability to learn complex
patterns, detect subtle flaws, and provide scalable security assurances. This
paper examines novel AI-driven techniques for vulnerability detection in smart
contracts, focusing on machine learning, deep learning, graph neural networks,
and transformer-based models. This paper analyzes how each technique represents
code, processes semantic information, and responds to real world vulnerability
classes. We also compare their strengths and weaknesses in terms of accuracy,
interpretability, computational overhead, and real time applicability. Lastly,
it highlights open challenges and future opportunities for advancing this
domain.

</details>


### [801] [Amatriciana: Exploiting Temporal GNNs for Robust and Efficient Money Laundering Detection](https://arxiv.org/abs/2506.00654)
*Marco Di Gennaro,Francesco Panebianco,Marco Pianta,Stefano Zanero,Michele Carminati*

Main category: cs.CR

TL;DR: Amatriciana是一种基于图神经网络的创新方法，用于检测交易图中的洗钱者，利用时间信息并整合全图数据，实验显示其在有限数据下表现良好，数据充足时优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 洗钱对金融安全和社会稳定构成严重威胁，交易量增长需要自动化工具辅助执法机构检测此类犯罪活动。

Method: Amatriciana采用图神经网络，整合全图交易数据而非分时子图，充分利用关系信息。

Result: 在公开数据集上，模型在有限数据下表现良好，数据充足时F1得分达0.76，并将误报率降低55%。

Conclusion: Amatriciana在洗钱检测中表现优异，尤其在减少误报方面显著优于现有方法。

Abstract: Money laundering is a financial crime that poses a serious threat to
financial integrity and social security. The growing number of transactions
makes it necessary to use automatic tools that help law enforcement agencies
detect such criminal activity. In this work, we present Amatriciana, a novel
approach based on Graph Neural Networks to detect money launderers inside a
graph of transactions by considering temporal information. Amatriciana uses the
whole graph of transactions without splitting it into several time-based
subgraphs, exploiting all relational information in the dataset. Our
experiments on a public dataset reveal that the model can learn from a limited
amount of data. Furthermore, when more data is available, the model outperforms
other State-of-the-art approaches; in particular, Amatriciana decreases the
number of False Positives (FPs) while detecting many launderers. In summary,
Amatriciana achieves an F1 score of 0.76. In addition, it lowers the FPs by 55%
with respect to other State-of-the-art models.

</details>


### [802] [Dual-Priv Pruning : Efficient Differential Private Fine-Tuning in Multimodal Large Language Models](https://arxiv.org/abs/2506.07077)
*Qianshan Wei,Jiaqi Li,Zihan You,Yi Zhan,Kecen Li,Jialin Wu,Xinfeng Li Hengjun Liu,Yi Yu,Bin Cao,Yiwen Xu,Yang Liu,Guilin Qi*

Main category: cs.CR

TL;DR: 论文提出Dual-Priv Pruning框架，通过视觉令牌剪枝和梯度更新剪枝机制，解决多模态大语言模型（MLLMs）中差分隐私（DP）带来的计算开销和模型性能下降问题。


<details>
  <summary>Details</summary>
Motivation: 差分隐私（DP）在MLLMs中的有效性尚不确定，且其引入的计算开销和噪声导致的模型性能下降是主要挑战。

Method: 提出Dual-Priv Pruning框架，包括视觉令牌剪枝和梯度更新剪枝，以减少输入维度和噪声影响。

Result: 实验表明，该方法在性能下降最小的情况下取得竞争性结果，并在计算效率上优于标准DP-SGD。

Conclusion: 该框架首次探索了MLLMs中的DP微调，具有高效性和实用性。

Abstract: Differential Privacy (DP) is a widely adopted technique, valued for its
effectiveness in protecting the privacy of task-specific datasets, making it a
critical tool for large language models. However, its effectiveness in
Multimodal Large Language Models (MLLMs) remains uncertain. Applying
Differential Privacy (DP) inherently introduces substantial computation
overhead, a concern particularly relevant for MLLMs which process extensive
textual and visual data. Furthermore, a critical challenge of DP is that the
injected noise, necessary for privacy, scales with parameter dimensionality,
leading to pronounced model degradation; This trade-off between privacy and
utility complicates the application of Differential Privacy (DP) to complex
architectures like MLLMs. To address these, we propose Dual-Priv Pruning, a
framework that employs two complementary pruning mechanisms for DP fine-tuning
in MLLMs: (i) visual token pruning to reduce input dimensionality by removing
redundant visual information, and (ii) gradient-update pruning during the DP
optimization process. This second mechanism selectively prunes parameter
updates based on the magnitude of noisy gradients, aiming to mitigate noise
impact and improve utility. Experiments demonstrate that our approach achieves
competitive results with minimal performance degradation. In terms of
computational efficiency, our approach consistently utilizes less memory than
standard DP-SGD. While requiring only 1.74% more memory than zeroth-order
methods which suffer from severe performance issues on A100 GPUs, our method
demonstrates leading memory efficiency on H20 GPUs. To the best of our
knowledge, we are the first to explore DP fine-tuning in MLLMs. Our code is
coming soon.

</details>


### [803] [A Systematic Review of Poisoning Attacks Against Large Language Models](https://arxiv.org/abs/2506.06518)
*Neil Fendley,Edward W. Staley,Joshua Carney,William Redman,Marie Chau,Nathan Drenkow*

Main category: cs.CR

TL;DR: 本文系统综述了大型语言模型（LLM）的中毒攻击，提出了一个全面的威胁模型，用于分类和衡量攻击特征。


<details>
  <summary>Details</summary>
Motivation: 随着预训练LLM的普及，其安全风险（如中毒攻击）日益突出，但现有框架和术语未能完全适应生成式LLM的需求。

Method: 通过系统综述已发表的LLM中毒攻击文献，提出包含四种攻击规范和六种衡量指标的威胁模型。

Result: 提出了一个适用于广泛LLM中毒攻击的分类框架，并从四个关键维度（概念毒药、隐蔽毒药、持久毒药和特定任务毒药）分析了现有攻击。

Conclusion: 该框架为LLM中毒攻击的研究提供了统一术语和分类标准，有助于更清晰地理解其安全风险。

Abstract: With the widespread availability of pretrained Large Language Models (LLMs)
and their training datasets, concerns about the security risks associated with
their usage has increased significantly. One of these security risks is the
threat of LLM poisoning attacks where an attacker modifies some part of the LLM
training process to cause the LLM to behave in a malicious way. As an emerging
area of research, the current frameworks and terminology for LLM poisoning
attacks are derived from earlier classification poisoning literature and are
not fully equipped for generative LLM settings. We conduct a systematic review
of published LLM poisoning attacks to clarify the security implications and
address inconsistencies in terminology across the literature. We propose a
comprehensive poisoning threat model applicable to categorize a wide range of
LLM poisoning attacks. The poisoning threat model includes four poisoning
attack specifications that define the logistics and manipulation strategies of
an attack as well as six poisoning metrics used to measure key characteristics
of an attack. Under our proposed framework, we organize our discussion of
published LLM poisoning literature along four critical dimensions of LLM
poisoning attacks: concept poisons, stealthy poisons, persistent poisons, and
poisons for unique tasks, to better understand the current landscape of
security risks.

</details>


### [804] [Mind the Web: The Security of Web Use Agents](https://arxiv.org/abs/2506.07153)
*Avishag Shapira,Parth Atulbhai Gandhi,Edan Habler,Oleg Brodt,Asaf Shabtai*

Main category: cs.CR

TL;DR: 论文揭示了Web-use代理的高权限能力可能被恶意利用，通过任务对齐注入技术攻击代理，导致多种安全问题。


<details>
  <summary>Details</summary>
Motivation: Web-use代理的高权限能力为攻击者提供了新的攻击面，研究其潜在风险及防御策略。

Method: 提出任务对齐注入技术，通过伪装恶意命令为任务指导，利用LLM的上下文推理缺陷。

Result: 在四种流行代理上验证了九种攻击载荷，成功率高达80%-100%。

Conclusion: 提出综合缓解策略，包括监督机制和执行约束，以提升代理的安全性。

Abstract: Web-use agents are rapidly being deployed to automate complex web tasks,
operating with extensive browser capabilities including multi-tab navigation,
DOM manipulation, JavaScript execution and authenticated session access.
However, these powerful capabilities create a critical and previously
unexplored attack surface. This paper demonstrates how attackers can exploit
web-use agents' high-privilege capabilities by embedding malicious content in
web pages such as comments, reviews, or advertisements that agents encounter
during legitimate browsing tasks. In addition, we introduce the task-aligned
injection technique that frame malicious commands as helpful task guidance
rather than obvious attacks. This technique exploiting fundamental limitations
in LLMs' contextual reasoning: agents struggle in maintaining coherent
contextual awareness and fail to detect when seemingly helpful web content
contains steering attempts that deviate from their original task goal. Through
systematic evaluation of four popular agents (OpenAI Operator, Browser Use, Do
Browser, OpenOperator), we demonstrate nine payload types that compromise
confidentiality, integrity, and availability, including unauthorized camera
activation, user impersonation, local file exfiltration, password leakage, and
denial of service, with validation across multiple LLMs achieving success rates
of 80%-100%. These payloads succeed across agents with built-in safety
mechanisms, requiring only the ability to post content on public websites,
creating unprecedented risks given the ease of exploitation combined with
agents' high-privilege access. To address this attack, we propose comprehensive
mitigation strategies including oversight mechanisms, execution constraints,
and task-aware reasoning techniques, providing practical directions for secure
development and deployment.

</details>


### [805] [Scoring the Unscorables: Cyber Risk Assessment Beyond Internet Scans](https://arxiv.org/abs/2506.06604)
*Armin Sarabi,Manish Karir,Mingyan Liu*

Main category: cs.CR

TL;DR: 研究提出了一种利用新型数据类型（技术数字签名）进行网络风险量化的方法，通过爬取组织网站数据构建高精度风险评估模型，克服了传统IP扫描数据的局限性。


<details>
  <summary>Details</summary>
Motivation: 传统基于IP扫描的数据存在不完整性和对中小型企业（SMEs）覆盖不足的问题，而技术数字签名数据更易获取且适用于大量SMEs。

Method: 通过爬取组织网站获取技术数字签名数据，构建网络风险评估模型，并与不同网络事件数据集进行交叉验证。

Result: 研究发现技术数字签名与组织的网络安全态势密切相关，并揭示了勒索软件攻击受害者与其他网络事件受害者的关键差异。

Conclusion: 该方法为网络风险量化提供了更高效且广泛适用的解决方案，尤其适用于中小型企业。

Abstract: In this paper we present a study on using novel data types to perform cyber
risk quantification by estimating the likelihood of a data breach. We
demonstrate that it is feasible to build a highly accurate cyber risk
assessment model using public and readily available technology signatures
obtained from crawling an organization's website. This approach overcomes the
limitations of previous similar approaches that relied on large-scale IP
address based scanning data, which suffers from incomplete/missing IP address
mappings as well as the lack of such data for large numbers of small and
medium-sized organizations (SMEs). In comparison to scan data, technology
digital signature data is more readily available for millions of SMEs. Our
study shows that there is a strong relationship between these technology
signatures and an organization's cybersecurity posture. In cross-validating our
model using different cyber incident datasets, we also highlight the key
differences between ransomware attack victims and the larger population of
cyber incident and data breach victims.

</details>


### [806] [From Static to Adaptive Defense: Federated Multi-Agent Deep Reinforcement Learning-Driven Moving Target Defense Against DoS Attacks in UAV Swarm Networks](https://arxiv.org/abs/2506.07392)
*Yuyang Zhou,Guang Cheng,Kang Du,Zihan Chen,Tian Qin,Yuyu Zhao*

Main category: cs.CR

TL;DR: 论文提出了一种基于联邦多智能体深度强化学习（FMADRL）的移动目标防御（MTD）框架，用于无人机群网络中的主动和自适应DoS缓解。


<details>
  <summary>Details</summary>
Motivation: 无人机群网络的开放无线环境、动态拓扑和资源限制使其面临严重的DoS威胁，传统静态或集中式防御机制难以应对。

Method: 设计了三种轻量级协调的MTD机制（领导者切换、路由变异和频率跳变），并将防御问题建模为多智能体部分可观察马尔可夫决策过程（POMDP），采用FMADRL算法进行分布式学习。

Result: 实验表明，该方法显著优于现有基线，攻击缓解率提升34.6%，平均恢复时间减少94.6%，能耗和防御成本分别降低29.3%和98.3%。

Conclusion: 该框架能有效提升无人机群网络的抗DoS能力，同时保持任务连续性。

Abstract: The proliferation of unmanned aerial vehicle (UAV) swarms has enabled a wide
range of mission-critical applications, but also exposes UAV networks to severe
Denial-of-Service (DoS) threats due to their open wireless environment, dynamic
topology, and resource constraints. Traditional static or centralized defense
mechanisms are often inadequate for such dynamic and distributed scenarios. To
address these challenges, we propose a novel federated multi-agent deep
reinforcement learning (FMADRL)-driven moving target defense (MTD) framework
for proactive and adaptive DoS mitigation in UAV swarm networks. Specifically,
we design three lightweight and coordinated MTD mechanisms, including leader
switching, route mutation, and frequency hopping, that leverage the inherent
flexibility of UAV swarms to disrupt attacker efforts and enhance network
resilience. The defense problem is formulated as a multi-agent partially
observable Markov decision process (POMDP), capturing the distributed,
resource-constrained, and uncertain nature of UAV swarms under attack. Each UAV
is equipped with a local policy agent that autonomously selects MTD actions
based on partial observations and local experiences. By employing a policy
gradient-based FMADRL algorithm, UAVs collaboratively optimize their defense
policies via reward-weighted aggregation, enabling distributed learning without
sharing raw data and thus reducing communication overhead. Extensive
simulations demonstrate that our approach significantly outperforms
state-of-the-art baselines, achieving up to a 34.6% improvement in attack
mitigation rate, a reduction in average recovery time of up to 94.6%, and
decreases in energy consumption and defense cost by as much as 29.3% and 98.3%,
respectively, while maintaining robust mission continuity under various DoS
attack strategies.

</details>


### [807] [TimberStrike: Dataset Reconstruction Attack Revealing Privacy Leakage in Federated Tree-Based Systems](https://arxiv.org/abs/2506.07605)
*Marco Di Gennaro,Giovanni De Lucia,Stefano Longari,Stefano Zanero,Michele Carminati*

Main category: cs.CR

TL;DR: TimberStrike是一种针对横向联邦树模型的优化数据集重建攻击，通过利用决策树的离散特性，攻击者可以推断其他客户的敏感数据。实验表明，该攻击在多个框架中能重建73.05%-95.63%的目标数据集，差分隐私虽能部分缓解攻击，但会显著降低模型性能。


<details>
  <summary>Details</summary>
Motivation: 研究联邦学习中树模型的安全与隐私问题，填补现有研究的空白。

Method: 提出TimberStrike攻击方法，利用决策树的分裂值和决策路径重建数据集，并在多个联邦梯度提升框架中验证其有效性。

Result: 攻击在公开数据集上成功重建73.05%-95.63%的目标数据，差分隐私虽能部分缓解攻击，但损害模型性能。

Conclusion: 需设计专门针对树模型的隐私保护机制，以平衡隐私与性能。

Abstract: Federated Learning has emerged as a privacy-oriented alternative to
centralized Machine Learning, enabling collaborative model training without
direct data sharing. While extensively studied for neural networks, the
security and privacy implications of tree-based models remain underexplored.
This work introduces TimberStrike, an optimization-based dataset reconstruction
attack targeting horizontally federated tree-based models. Our attack, carried
out by a single client, exploits the discrete nature of decision trees by using
split values and decision paths to infer sensitive training data from other
clients. We evaluate TimberStrike on State-of-the-Art federated gradient
boosting implementations across multiple frameworks, including Flower, NVFlare,
and FedTree, demonstrating their vulnerability to privacy breaches. On a
publicly available stroke prediction dataset, TimberStrike consistently
reconstructs between 73.05% and 95.63% of the target dataset across all
implementations. We further analyze Differential Privacy, showing that while it
partially mitigates the attack, it also significantly degrades model
performance. Our findings highlight the need for privacy-preserving mechanisms
specifically designed for tree-based Federated Learning systems, and we provide
preliminary insights into their design.

</details>


### [808] [Profiling Electric Vehicles via Early Charging Voltage Patterns](https://arxiv.org/abs/2506.07714)
*Francesco Marchiori,Denis Donadel,Alessandro Brighente,Mauro Conti*

Main category: cs.CR

TL;DR: 本文提出了一种基于早期充电阶段电压行为的电动汽车（EV）识别框架，以提高认证效率并减少能源盗窃风险。


<details>
  <summary>Details</summary>
Motivation: 传统认证方法集中在充电后期，导致恶意行为者可能在检测前消耗大量能源，同时充电模式分析可能引发隐私问题。

Method: 通过提取早期充电阶段电压行为的特征，构建轻量级模型进行EV识别。

Result: 在49辆EV的7408次充电数据上测试，准确率达0.86，仅需10个关键特征即可接近最优性能。

Conclusion: 该研究为新型认证因素提供了基础，但也揭示了充电数据可能带来的隐私风险。

Abstract: Electric Vehicles (EVs) are rapidly gaining adoption as a sustainable
alternative to fuel-powered vehicles, making secure charging infrastructure
essential. Despite traditional authentication protocols, recent results showed
that attackers may steal energy through tailored relay attacks. One
countermeasure is leveraging the EV's fingerprint on the current exchanged
during charging. However, existing methods focus on the final charging stage,
allowing malicious actors to consume substantial energy before being detected
and repudiated. This underscores the need for earlier and more effective
authentication methods to prevent unauthorized charging. Meanwhile, profiling
raises privacy concerns, as uniquely identifying EVs through charging patterns
could enable user tracking.
  In this paper, we propose a framework for uniquely identifying EVs using
physical measurements from the early charging stages. We hypothesize that
voltage behavior early in the process exhibits similar characteristics to
current behavior in later stages. By extracting features from early voltage
measurements, we demonstrate the feasibility of EV profiling. Our approach
improves existing methods by enabling faster and more reliable vehicle
identification. We test our solution on a dataset of 7408 usable charges from
49 EVs, achieving up to 0.86 accuracy. Feature importance analysis shows that
near-optimal performance is possible with just 10 key features, improving
efficiency alongside our lightweight models. This research lays the foundation
for a novel authentication factor while exposing potential privacy risks from
unauthorized access to charging data.

</details>


### [809] [SoK: Data Reconstruction Attacks Against Machine Learning Models: Definition, Metrics, and Benchmark](https://arxiv.org/abs/2506.07888)
*Rui Wen,Yiyong Liu,Michael Backes,Yang Zhang*

Main category: cs.CR

TL;DR: 本文提出了一种针对视觉领域数据重建攻击的统一分类和正式定义，并设计了一套定量评估指标，同时利用大语言模型（LLMs）替代人工判断进行视觉评估。


<details>
  <summary>Details</summary>
Motivation: 当前数据重建攻击领域缺乏正式定义和通用评估指标，阻碍了研究的进一步发展。

Method: 提出统一攻击分类和正式定义，设计定量评估指标，并利用LLMs进行视觉评估。

Result: 实证结果验证了所提指标的有效性，并为设计新攻击提供了有价值的见解。

Conclusion: 本文为数据重建攻击研究提供了统一框架和基准，推动了该领域的进一步发展。

Abstract: Data reconstruction attacks, which aim to recover the training dataset of a
target model with limited access, have gained increasing attention in recent
years. However, there is currently no consensus on a formal definition of data
reconstruction attacks or appropriate evaluation metrics for measuring their
quality. This lack of rigorous definitions and universal metrics has hindered
further advancement in this field. In this paper, we address this issue in the
vision domain by proposing a unified attack taxonomy and formal definitions of
data reconstruction attacks. We first propose a set of quantitative evaluation
metrics that consider important criteria such as quantifiability, consistency,
precision, and diversity. Additionally, we leverage large language models
(LLMs) as a substitute for human judgment, enabling visual evaluation with an
emphasis on high-quality reconstructions. Using our proposed taxonomy and
metrics, we present a unified framework for systematically evaluating the
strengths and limitations of existing attacks and establishing a benchmark for
future research. Empirical results, primarily from a memorization perspective,
not only validate the effectiveness of our metrics but also offer valuable
insights for designing new attacks.

</details>


### [810] [Are Trees Really Green? A Detection Approach of IoT Malware Attacks](https://arxiv.org/abs/2506.07836)
*Silvia Lucia Sanna,Diego Soi,Davide Maiorca,Giorgio Giacinto*

Main category: cs.CR

TL;DR: 论文提出了一种基于流量隐私保护统计特征的绿色方法，用于识别物联网恶意软件网络攻击，并通过优化树模型超参数在能耗和性能间取得平衡。


<details>
  <summary>Details</summary>
Motivation: 物联网设备因资源限制和难以应用安全补丁而容易受到攻击，现有机器学习方法多关注攻击识别而忽略计算资源影响。

Method: 优化决策树、随机森林和Extra-Trees模型的超参数，基于能耗和性能（马修斯相关系数）进行权衡。

Result: 模型在保持高性能和检测准确率的同时显著降低能耗（瓦时）。

Conclusion: 基于机器学习的本地入侵检测系统适合物联网等资源受限设备。

Abstract: Nowadays, the Internet of Things (IoT) is widely employed, and its usage is
growing exponentially because it facilitates remote monitoring, predictive
maintenance, and data-driven decision making, especially in the healthcare and
industrial sectors. However, IoT devices remain vulnerable due to their
resource constraints and difficulty in applying security patches. Consequently,
various cybersecurity attacks are reported daily, such as Denial of Service,
particularly in IoT-driven solutions. Most attack detection methodologies are
based on Machine Learning (ML) techniques, which can detect attack patterns.
However, the focus is more on identification rather than considering the impact
of ML algorithms on computational resources. This paper proposes a green
methodology to identify IoT malware networking attacks based on flow
privacy-preserving statistical features. In particular, the hyperparameters of
three tree-based models -- Decision Trees, Random Forest and Extra-Trees -- are
optimized based on energy consumption and test-time performance in terms of
Matthew's Correlation Coefficient. Our results show that models maintain high
performance and detection accuracy while consistently reducing power usage in
terms of watt-hours (Wh). This suggests that on-premise ML-based Intrusion
Detection Systems are suitable for IoT and other resource-constrained devices.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [811] [QForce-RL: Quantized FPGA-Optimized Reinforcement Learning Compute Engine](https://arxiv.org/abs/2506.07046)
*Anushka Jha,Tanushree Dewangan,Mukul Lokhande,Santosh Kumar Vishvakarma*

Main category: cs.AR

TL;DR: QForce-RL通过量化和轻量级架构提升FPGA上的RL性能，减少资源消耗，性能提升2.3倍。


<details>
  <summary>Details</summary>
Motivation: FPGA部署RL资源消耗大，现有方法在高质量图像训练中计算量大，面临挑战。

Method: 结合E2HRL减少RL动作和QuaRL的量化SIMD加速，优化模型大小和计算操作。

Result: 性能提升2.3倍，FPS提升2.6倍，适用于资源受限设备。

Conclusion: QForce-RL在资源效率、性能和灵活性方面优于现有技术。

Abstract: Reinforcement Learning (RL) has outperformed other counterparts in sequential
decision-making and dynamic environment control. However, FPGA deployment is
significantly resource-expensive, as associated with large number of
computations in training agents with high-quality images and possess new
challenges. In this work, we propose QForce-RL takes benefits of quantization
to enhance throughput and reduce energy footprint with light-weight RL
architecture, without significant performance degradation. QForce-RL takes
advantages from E2HRL to reduce overall RL actions to learn desired policy and
QuaRL for quantization based SIMD for hardware acceleration. We have also
provided detailed analysis for different RL environments, with emphasis on
model size, parameters, and accelerated compute ops. The architecture is
scalable for resource-constrained devices and provide parametrized efficient
deployment with flexibility in latency, throughput, power, and energy
efficiency. The proposed QForce-RL provides performance enhancement up to 2.3x
and better FPS - 2.6x compared to SoTA works.

</details>


### [812] [Design and Implementation of a RISC-V SoC with Custom DSP Accelerators for Edge Computing](https://arxiv.org/abs/2506.06693)
*Priyanshu Yadav*

Main category: cs.AR

TL;DR: 本文全面分析了RISC-V指令集架构，重点探讨其模块化设计、实现挑战和性能特点，展示了其在嵌入式系统中的优势及可扩展性。


<details>
  <summary>Details</summary>
Motivation: 研究RISC-V指令集架构的模块化设计和性能特点，以评估其在嵌入式系统和定制加速器中的潜力。

Method: 通过周期精确的流水线实现模拟，评估RV32I基础指令集及其扩展（M和A）的性能指标（如CPI和能效）。

Result: 结果显示RISC-V在嵌入式系统中具有优势，能效比ARM Cortex-M0实现低17%。

Conclusion: RISC-V的开源标准特性为领域特定优化提供了显著灵活性。

Abstract: This paper presents a comprehensive analysis of the RISC-V instruction set
architecture, focusing on its modular design, implementation challenges, and
performance characteristics. We examine the RV32I base instruction set with
extensions for multiplication (M) and atomic operations (A). Through
cycle-accurate simulation of a pipelined implementation, we evaluate
performance metrics including CPI (cycles per instruction) and power
efficiency. Our results demonstrate RISC-V's advantages in embedded systems and
its scalability for custom accelerators. Comparative analysis shows a 17%
reduction in power consumption compared to ARM Cortex-M0 implementations in
similar process nodes. The open-standard nature of RISC-V provides significant
flexibility for domain-specific optimizations.

</details>


### [813] [ProtocolLLM: RTL Benchmark for SystemVerilog Generation of Communication Protocols](https://arxiv.org/abs/2506.07945)
*Arnav Sheth,Ivaxi Sheth,Mario Fritz*

Main category: cs.AR

TL;DR: 论文探讨了大型语言模型（LLMs）在生成硬件描述语言（HDL）代码方面的能力，特别是针对SystemVerilog的标准通信协议实现。


<details>
  <summary>Details</summary>
Motivation: 尽管LLMs在通用编程语言代码生成方面表现优异，但其在HDL（如SystemVerilog）中的应用尚未充分探索，尤其是在满足时序语义、并发性和可综合性的严格约束方面。

Method: 论文提出了首个针对SPI、I2C、UART和AXI四种协议的基准测试套件，定义了不同设计抽象层次和提示特异性的代码生成任务。

Result: 生成的代码通过波形仿真和测试台评估了语法正确性、可综合性和功能保真度。

Conclusion: 论文为LLMs在HDL领域的应用提供了基准和分析，填补了研究空白。

Abstract: Recent advances in Large Language Models (LLMs) have shown promising
capabilities in generating code for general-purpose programming languages. In
contrast, their applicability for hardware description languages, particularly
for generating synthesizable and functionally correct designs, remains
significantly underexplored. HDLs such as SystemVerilog are logic-oriented and
demand strict adherence to timing semantics, concurrency, and synthesizability
constraints. Moreover, HDL-based design flows encompass a broad set of tasks
beyond structural code generation, including testbench development,
assertion-based verification, timing closure, and protocol-level integration
for on-chip communication. The objective of our paper is to analyze the
capabilities of state-of-the-art LLMs in generating SystemVerilog
implementations of standard communication protocols, a core component of
embedded and System-on-Chip (SoC) architectures. This paper introduces the
first benchmark suite targeting four widely used protocols: SPI, I2C, UART, and
AXI. We define code generation tasks that capture varying levels of design
abstraction and prompt specificity. The generated designs are assessed for
syntactic correctness, synthesizability, and functional fidelity via waveform
simulation and test benches.

</details>


### [814] [MAGNet: A Multi-Scale Attention-Guided Graph Fusion Network for DRC Violation Detection](https://arxiv.org/abs/2506.07126)
*Weihan Lu,Hong Cai Chen*

Main category: cs.AR

TL;DR: MAGNet是一种混合深度学习模型，结合改进的U-Net和图神经网络（GNN），用于预测集成电路设计中的DRC违规。通过动态注意力模块和多尺度卷积模块增强特征提取能力，并利用图结构建模拓扑关系，显著提高了预测精度和降低了误报率。


<details>
  <summary>Details</summary>
Motivation: DRC检查对降低成本和提升设计效率至关重要，机器学习方法成为重要手段。现有方法在特征提取和拓扑关系建模上存在不足，MAGNet旨在解决这些问题。

Method: 提出MAGNet模型，结合改进的U-Net（含动态注意力模块和多尺度卷积模块）和GNN，通过像素对齐的图结构建模拓扑关系，并采用标签放大策略增强稀疏违规模式的检测能力。

Result: MAGNet在DRC热点检测中显著优于ibUnet、RouteNet和J-Net，预测精度更高且误报率更低。增量训练进一步提升了热点识别能力。

Conclusion: MAGNet通过融合空间、语义和结构信息，有效提升了DRC违规预测性能，为集成电路设计提供了更高效的解决方案。

Abstract: Design rule checking (DRC) is of great significance for cost reduction and
design efficiency improvement in integrated circuit (IC) designs.
Machine-learning-based DRC has become an important approach in computer-aided
design (CAD). In this paper, we propose MAGNet, a hybrid deep learning model
that integrates an improved U-Net with a graph neural network for DRC violation
prediction. The U-Net backbone is enhanced with a Dynamic Attention Module
(DAM) and a Multi-Scale Convolution Module (MSCM) to strengthen its capability
in extracting fine-grained and multi-scale spatial features. In parallel, we
construct a pixel-aligned graph structure based on chip layout tiles, and apply
a specialized GNN to model the topological relationships among pins. During
graph construction, a graph-to-grid mapping is generated to align GNN features
with the layout image. In addition, a label amplification strategy is adopted
during training to enhance the model's sensitivity to sparse violation
patterns. Overall, MAGNet effectively combines spatial, semantic, and
structural information, achieving improved prediction accuracy and reduced
false positive rates in DRC hotspot detection. Subsequently, through
incremental training, we achieve a more sensitive discrimination ability for
hotspots. The results demonstrate that, in comparison with ibUnet, RouteNet,
and J-Net, MAGnet significantly outperforms these models, achieving substantial
improvements in overall performance.

</details>


### [815] [VeriLoC: Line-of-Code Level Prediction of Hardware Design Quality from Verilog Code](https://arxiv.org/abs/2506.07239)
*Raghu Vamshi Hemadri,Jitendra Bhandari,Johann Knechtel,Badri P Gopalan,Ramesh Narayanaswamy,Ramesh Karri,Siddharth Garg*

Main category: cs.AR

TL;DR: VeriLoC是一种新方法，直接从Verilog代码预测设计质量，包括行级和模块级，利用LLM嵌入和分类器/回归器，显著提高了预测精度。


<details>
  <summary>Details</summary>
Motivation: 现代芯片设计复杂，需要从Verilog代码早期预测设计质量指标（如时序和布线拥塞），尤其是行级预测。现有方法未解决行级预测问题。

Method: VeriLoC利用Verilog代码生成LLM提取行级和模块级嵌入，并通过分类器/回归器训练这些嵌入的拼接。

Result: VeriLoC在行级拥塞和时序预测上F1分数达0.86-0.95，将平均百分比误差从14%-18%降至4%。

Conclusion: VeriLoC的嵌入和方法对其他硬件设计预测和优化任务也有价值。

Abstract: Modern chip design is complex, and there is a crucial need for early-stage
prediction of key design-quality metrics like timing and routing congestion
directly from Verilog code (a commonly used programming language for hardware
design). It is especially important yet complex to predict individual lines of
code that cause timing violations or downstream routing congestion. Prior works
have tried approaches like converting Verilog into an intermediate graph
representation and using LLM embeddings alongside other features to predict
module-level quality, but did not consider line-level quality prediction. We
propose VeriLoC, the first method that predicts design quality directly from
Verilog at both the line- and module-level. To this end, VeriLoC leverages
recent Verilog code-generation LLMs to extract local line-level and
module-level embeddings, and train downstream classifiers/regressors on
concatenations of these embeddings. VeriLoC achieves high F1-scores of
0.86-0.95 for line-level congestion and timing prediction, and reduces the mean
average percentage error from 14% - 18% for SOTA methods down to only 4%. We
believe that VeriLoC embeddings and insights from our work will also be of
value for other predictive and optimization tasks for complex hardware design.

</details>


### [816] [Taming Wild Branches: Overcoming Hard-to-Predict Branches using the Bullseye Predictor](https://arxiv.org/abs/2506.06773)
*Emet Behrendt,Shing Wai Pun,Prashant J. Nair*

Main category: cs.AR

TL;DR: 论文提出了一种名为Bullseye的预测器，用于解决TAGE-SC-L预测器中难以预测的分支问题，通过局部和全局历史感知的感知器提升性能。


<details>
  <summary>Details</summary>
Motivation: TAGE-SC-L预测器中难以预测的分支（H2P）导致大量误预测，现有方法（如扩大表）效果有限。

Method: 设计了一个28 KB的Bullseye子系统，包括H2P识别表、局部和全局历史感知器，并通过动态阈值筛选分支。

Result: 平均MPKI为3.4045，CycWpPKI为145.09，显著提升了预测性能。

Conclusion: Bullseye预测器有效解决了H2P分支问题，减少了误预测和污染。

Abstract: Branch prediction is key to the performance of out-of-order processors. While
the CBP-2016 winner TAGE-SC-L combines geometric-history tables, a statistical
corrector, and a loop predictor, over half of its remaining mispredictions stem
from a small set of hard-to-predict (H2P) branches. These branches occur under
diverse global histories, causing repeated thrashing in TAGE and eviction
before usefulness counters can mature. Prior work shows that simply enlarging
the tables offers only marginal improvement.
  We augment a 159 KB TAGE-SC-L predictor with a 28 KB H2P-targeted subsystem
called the Bullseye predictor. It identifies problematic PCs using a
set-associative H2P Identification Table (HIT) and steers them to one of two
branch-specific perceptrons, one indexed by hashed local history and the other
by folded global history. A short trial phase tracks head-to-head accuracy in
an H2P cache. A branch becomes perceptron-resident only if the perceptron's
sustained accuracy and output magnitude exceed dynamic thresholds, after which
TAGE updates for that PC are suppressed to reduce pollution. The HIT, cache,
and perceptron operate fully in parallel with TAGE-SC-L, providing higher
fidelity on the H2P tail. This achieves an average MPKI of 3.4045 and CycWpPKI
of 145.09.

</details>


### [817] [ASPO: Constraint-Aware Bayesian Optimization for FPGA-based Soft Processors](https://arxiv.org/abs/2506.06817)
*Haoran Wu,Ce Guo,Wayne Luk,Robert Mullins*

Main category: cs.AR

TL;DR: ASPO是一种改进的贝叶斯优化方法，支持处理包含分类参数的约束，并加速FPGA软处理器的设计优化。


<details>
  <summary>Details</summary>
Motivation: 标准贝叶斯优化无法处理分类参数约束，且优化时间随处理器复杂度增加，限制了其在FPGA软处理器设计中的应用。

Method: ASPO通过定制化的协方差核支持分类参数，并通过惩罚获取函数和重用FPGA合成检查点加速设计评估。

Result: ASPO在BOOM处理器上为“multiply”基准测试减少35%执行时间，设计时间比现有方法减少74%。

Conclusion: ASPO有效解决了分类参数约束和优化时间问题，显著提升了FPGA软处理器的设计效率。

Abstract: Bayesian Optimization (BO) has shown promise in tuning processor design
parameters. However, standard BO does not support constraints involving
categorical parameters such as types of branch predictors and division
circuits. In addition, optimization time of BO grows with processor complexity,
which becomes increasingly significant especially for FPGA-based soft
processors. This paper introduces ASPO, an approach that leverages disjunctive
form to enable BO to handle constraints involving categorical parameters.
Unlike existing methods that directly apply standard BO, the proposed ASPO
method, for the first time, customizes the mathematical mechanism of BO to
address challenges faced by soft-processor designs on FPGAs. Specifically, ASPO
supports categorical parameters using a novel customized BO covariance kernel.
It also accelerates the design evaluation procedure by penalizing the BO
acquisition function with potential evaluation time and by reusing FPGA
synthesis checkpoints from previously evaluated configurations. ASPO targets
three soft processors: RocketChip, BOOM, and EL2 VeeR. The approach is
evaluated based on seven RISC-V benchmarks. Results show that ASPO can reduce
execution time for the ``multiply'' benchmark on the BOOM processor by up to
35\% compared to the default configuration. Furthermore, it reduces design time
for the BOOM processor by up to 74\% compared to Boomerang, a state-of-the-art
hardware-oriented BO approach.

</details>


<div id='q-fin.CP'></div>

# q-fin.CP [[Back]](#toc)

### [818] [Explaining Risks: Axiomatic Risk Attributions for Financial Models](https://arxiv.org/abs/2506.06653)
*Dangxing Chen*

Main category: q-fin.CP

TL;DR: 论文提出了一种基于Shapley值框架的风险分配方法，用于解释机器学习模型在高风险领域的预测风险。


<details>
  <summary>Details</summary>
Motivation: 在高风险领域（如金融）中，模型的风险分配与均值预测同样重要，但现有方法难以公平分配风险。

Method: 通过扩展Shapley值框架，提出了一种风险分配方法。

Result: 分析和实证表明，该方法能有效公平地分配风险。

Conclusion: 扩展Shapley值框架是解决风险分配问题的有效方法。

Abstract: In recent years, machine learning models have achieved great success at the
expense of highly complex black-box structures. By using axiomatic attribution
methods, we can fairly allocate the contributions of each feature, thus
allowing us to interpret the model predictions. In high-risk sectors such as
finance, risk is just as important as mean predictions. Throughout this work,
we address the following risk attribution problem: how to fairly allocate the
risk given a model with data? We demonstrate with analysis and empirical
examples that risk can be well allocated by extending the Shapley value
framework.

</details>


### [819] [Uncertainty-Aware Strategies: A Model-Agnostic Framework for Robust Financial Optimization through Subsampling](https://arxiv.org/abs/2506.07299)
*Hans Buehler,Blanka Horvath,Yannick Limmer,Thorsten Schmidt*

Main category: q-fin.CP

TL;DR: 论文提出了一种应对量化金融中模型不确定性的方法，通过叠加外部“不确定性度量”增强传统目标，并采用子采样策略和优化算法提升效率。


<details>
  <summary>Details</summary>
Motivation: 量化金融中模型不确定性可能导致决策质量显著偏差，传统方法依赖经验近似，缺乏对真实概率的准确估计。

Method: 结合Klibanoff等人的框架，叠加不确定性度量，提出子采样策略和并行化随机梯度下降算法。

Result: 不确定性度量优于传统混合度量策略，子采样方法在模型风险和性能上媲美贝叶斯方法。

Conclusion: 该方法在模型不确定性和计算效率上表现优越，适用于实际金融场景。

Abstract: This paper addresses the challenge of model uncertainty in quantitative
finance, where decisions in portfolio allocation, derivative pricing, and risk
management rely on estimating stochastic models from limited data. In practice,
the unavailability of the true probability measure forces reliance on an
empirical approximation, and even small misestimations can lead to significant
deviations in decision quality. Building on the framework of Klibanoff et al.
(2005), we enhance the conventional objective - whether this is expected
utility in an investing context or a hedging metric - by superimposing an outer
"uncertainty measure", motivated by traditional monetary risk measures, on the
space of models. In scenarios where a natural model distribution is lacking or
Bayesian methods are impractical, we propose an ad hoc subsampling strategy,
analogous to bootstrapping in statistical finance and related to mini-batch
sampling in deep learning, to approximate model uncertainty. To address the
quadratic memory demands of naive implementations, we also present an adapted
stochastic gradient descent algorithm that enables efficient parallelization.
Through analytical, simulated, and empirical studies - including multi-period,
real data and high-dimensional examples - we demonstrate that uncertainty
measures outperform traditional mixture of measures strategies and our
model-agnostic subsampling-based approach not only enhances robustness against
model risk but also achieves performance comparable to more elaborate Bayesian
methods.

</details>


<div id='cs.SI'></div>

# cs.SI [[Back]](#toc)

### [820] [Fast Geometric Embedding for Node Influence Maximization](https://arxiv.org/abs/2506.07435)
*Alexander Kolpakov,Igor Rivin*

Main category: cs.SI

TL;DR: 提出一种高效的力导向布局算法，通过低维嵌入图的径向距离作为中心性度量的替代，显著降低计算成本。


<details>
  <summary>Details</summary>
Motivation: 大规模图上计算传统中心性度量（如介数和接近度）计算成本高，需高效替代方法。

Method: 使用力导向布局算法将图嵌入低维空间，径向距离作为中心性度量的代理。

Result: 在多类图上验证，与度、PageRank和路径中心性高度相关，且能高效识别高影响力节点。

Conclusion: 该嵌入方法为传统贪婪算法提供了快速且可扩展的替代方案。

Abstract: Computing classical centrality measures such as betweenness and closeness is
computationally expensive on large-scale graphs. In this work, we introduce an
efficient force layout algorithm that embeds a graph into a low-dimensional
space, where the radial distance from the origin serves as a proxy for various
centrality measures. We evaluate our method on multiple graph families and
demonstrate strong correlations with degree, PageRank, and paths-based
centralities. As an application, it turns out that the proposed embedding
allows to find high-influence nodes in a network, and provides a fast and
scalable alternative to the standard greedy algorithm.

</details>


<div id='cs.DM'></div>

# cs.DM [[Back]](#toc)

### [821] [HyColor: An Efficient Heuristic Algorithm for Graph Coloring](https://arxiv.org/abs/2506.07373)
*Enqiang Zhu,Yu Zhang,Haopeng Sun,Ziqi Wei,Witold Pedrycz,Chanjuan Liu,Jin Xu*

Main category: cs.DM

TL;DR: 本文提出了一种名为HyColor的高效混合启发式算法，用于解决图着色问题（GCP），特别擅长处理大规模稀疏图和小规模稠密图。


<details>
  <summary>Details</summary>
Motivation: 由于GCP的NP难特性，现有算法主要针对小规模难解图或大规模稀疏图，缺乏一种通用高效的解决方案。

Method: HyColor结合了局部决策策略、图缩减策略以及基于k核和混合度的贪心启发式方法。

Result: 在209个测试实例中，HyColor在194个实例中表现最佳（超过93%），并在128个实例中确定了色数。

Conclusion: HyColor在解决GCP问题上显著优于现有启发式算法，尤其在处理大规模稀疏图时表现出色。

Abstract: The graph coloring problem (GCP) is a classic combinatorial optimization
problem that aims to find the minimum number of colors assigned to vertices of
a graph such that no two adjacent vertices receive the same color. GCP has been
extensively studied by researchers from various fields, including mathematics,
computer science, and biological science. Due to the NP-hard nature, many
heuristic algorithms have been proposed to solve GCP. However, existing GCP
algorithms focus on either small hard graphs or large-scale sparse graphs (with
up to 10^7 vertices). This paper presents an efficient hybrid heuristic
algorithm for GCP, named HyColor, which excels in handling large-scale sparse
graphs while achieving impressive results on small dense graphs. The efficiency
of HyColor comes from the following three aspects: a local decision strategy to
improve the lower bound on the chromatic number; a graph-reduction strategy to
reduce the working graph; and a k-core and mixed degree-based greedy heuristic
for efficiently coloring graphs. HyColor is evaluated against three
state-of-the-art GCP algorithms across four benchmarks, comprising three
large-scale sparse graph benchmarks and one small dense graph benchmark,
totaling 209 instances. The results demonstrate that HyColor consistently
outperforms existing heuristic algorithms in both solution accuracy and
computational efficiency for the majority of instances. Notably, HyColor
achieved the best solutions in 194 instances (over 93%), with 34 of these
solutions significantly surpassing those of other algorithms. Furthermore,
HyColor successfully determined the chromatic number and achieved optimal
coloring in 128 instances.

</details>


<div id='cs.NE'></div>

# cs.NE [[Back]](#toc)

### [822] [Evolutionary model for energy trading in community microgrids using Hawk-Dove strategies](https://arxiv.org/abs/2506.06325)
*Viorica Rozina Chifu,Tudor Cioara,Cristina Bianca Pop,Ionut Anghel*

Main category: cs.NE

TL;DR: 论文提出了一种微电网间的去中心化能源合作模型，通过本地决策和进化算法实现能源平衡。


<details>
  <summary>Details</summary>
Motivation: 解决微电网间能源交易中的局部决策和全局稳定性问题。

Method: 采用Hawk或Dove策略的自治代理模型，结合进化算法（重组和变异）优化能源交易矩阵。

Result: 在100个微电网的模拟场景中，95个达到稳定能源状态。

Conclusion: 模型在个体和社区层面均能有效实现能源平衡。

Abstract: This paper proposes a decentralized model of energy cooperation between
microgrids, in which decisions are made locally, at the level of the microgrid
community. Each microgrid is modeled as an autonomous agent that adopts a Hawk
or Dove strategy, depending on the level of energy stored in the battery and
its role in the energy trading process. The interactions between selling and
buying microgrids are modeled through an evolutionary algorithm. An individual
in the algorithm population is represented as an energy trading matrix that
encodes the amounts of energy traded between the selling and buying microgrids.
The population evolution is achieved by recombination and mutation operators.
Recombination uses a specialized operator for matrix structures, and mutation
is applied to the matrix elements according to a Gaussian distribution. The
evaluation of an individual is made with a multi-criteria fitness function that
considers the seller profit, the degree of energy stability at the community
level, penalties for energy imbalance at the community level and for the
degradation of microgrids batteries. The method was tested on a simulated
scenario with 100 microgrids, each with its own selling and buying thresholds,
to reflect a realistic environment with variable storage characteristics of
microgrids batteries. By applying the algorithm on this scenario, 95 out of the
100 microgrids reached a stable energy state. This result confirms the
effectiveness of the proposed model in achieving energy balance both at the
individual level, for each microgrid, and at the level of the entire community.

</details>


### [823] [Neural networks with image recognition by pairs](https://arxiv.org/abs/2506.06322)
*Polad Geidarov*

Main category: cs.NE

TL;DR: 论文探讨了将基于度量识别方法的神经网络转化为可应用经典学习算法的网络，简化了学习过程并支持网络扩展。


<details>
  <summary>Details</summary>
Motivation: 传统基于度量识别的神经网络架构严格由初始任务条件决定，限制了灵活性和扩展性。

Method: 通过成对图像识别进行训练，避免使用解析表达式计算权重，简化网络架构和训练过程。

Result: 转化后的网络具有架构简单、训练可靠、支持大规模图像识别和动态扩展类别等优势。

Conclusion: 该方法为神经网络提供了更灵活和可扩展的学习框架。

Abstract: Neural networks based on metric recognition methods have a strictly
determined architecture. Number of neurons, connections, as well as weights and
thresholds values are calculated analytically, based on the initial conditions
of tasks: number of recognizable classes, number of samples, metric expressions
used. This paper discusses the possibility of transforming these networks in
order to apply classical learning algorithms to them without using analytical
expressions that calculate weight values. In the received network, training is
carried out by recognizing images in pairs. This approach simplifies the
learning process and easily allows to expand the neural network by adding new
images to the recognition task. The advantages of these networks, including
such as: 1) network architecture simplicity and transparency; 2) training
simplicity and reliability; 3) the possibility of using a large number of
images in the recognition problem using a neural network; 4) a consistent
increase in the number of recognizable classes without changing the previous
values of weights and thresholds.

</details>


### [824] [Introduction to Predictive Coding Networks for Machine Learning](https://arxiv.org/abs/2506.06332)
*Mikko Stenlund*

Main category: cs.NE

TL;DR: 本文简要介绍了预测编码网络（PCNs），作为生物启发的框架，用于理解大脑中的分层计算，并提供了机器学习实践者的快速入门指南。


<details>
  <summary>Details</summary>
Motivation: PCNs为传统前馈神经网络提供了替代方案，并具有生物学启发的优势。

Method: 介绍了PCNs的基础架构、推理和学习更新规则，以及算法实现。

Result: 通过CIFAR-10图像分类任务展示了PCNs的性能，并提供了PyTorch实现。

Conclusion: PCNs为机器学习提供了一种新的、生物启发的计算框架。

Abstract: Predictive coding networks (PCNs) constitute a biologically inspired
framework for understanding hierarchical computation in the brain, and offer an
alternative to traditional feedforward neural networks in ML. This note serves
as a quick, onboarding introduction to PCNs for machine learning practitioners.
We cover the foundational network architecture, inference and learning update
rules, and algorithmic implementation. A concrete image-classification task
(CIFAR-10) is provided as a benchmark-smashing application, together with an
accompanying Python notebook containing the PyTorch implementation.

</details>


### [825] [CR-BLEA: Contrastive Ranking for Adaptive Resource Allocation in Bilevel Evolutionary Algorithms](https://arxiv.org/abs/2506.06362)
*Dejun Xu,Jijia Chen,Gary G. Yen,Min Jiang*

Main category: cs.NE

TL;DR: 提出了一种新的资源分配框架，通过对比排序网络选择性优化有潜力的下层任务，显著降低计算成本并保持或提升解的质量。


<details>
  <summary>Details</summary>
Motivation: 双层优化因嵌套结构导致计算成本高，进化算法虽有效但资源浪费严重，尤其是对无潜力的下层任务重复评估。

Method: 使用对比排序网络在线学习上下层解的关系模式，指导基于参考的排序策略，优先优化有潜力的任务并自适应控制重采样。

Result: 在五种前沿双层算法上的实验表明，该框架显著降低计算成本，同时保持或提升解精度。

Conclusion: 该框架为提升双层进化算法效率提供了通用策略，推动了更可扩展的双层优化。

Abstract: Bilevel optimization poses a significant computational challenge due to its
nested structure, where each upper-level candidate solution requires solving a
corresponding lower-level problem. While evolutionary algorithms (EAs) are
effective at navigating such complex landscapes, their high resource demands
remain a key bottleneck -- particularly the redundant evaluation of numerous
unpromising lower-level tasks. Despite recent advances in multitasking and
transfer learning, resource waste persists. To address this issue, we propose a
novel resource allocation framework for bilevel EAs that selectively identifies
and focuses on promising lower-level tasks. Central to our approach is a
contrastive ranking network that learns relational patterns between paired
upper- and lower-level solutions online. This knowledge guides a
reference-based ranking strategy that prioritizes tasks for optimization and
adaptively controls resampling based on estimated population quality.
Comprehensive experiments across five state-of-the-art bilevel algorithms show
that our framework significantly reduces computational cost while preserving --
or even enhancing -- solution accuracy. This work offers a generalizable
strategy to improve the efficiency of bilevel EAs, paving the way for more
scalable bilevel optimization.

</details>


### [826] [Can Biologically Plausible Temporal Credit Assignment Rules Match BPTT for Neural Similarity? E-prop as an Example](https://arxiv.org/abs/2506.06904)
*Yuhan Helena Liu,Guangyu Robert Yang,Christopher J. Cueva*

Main category: cs.NE

TL;DR: 研究表明，基于梯度截断的生物合理学习规则e-prop在任务准确性和神经数据相似性上可与BPTT媲美，且模型架构和初始条件对神经相似性影响更大。


<details>
  <summary>Details</summary>
Motivation: 探索生物合理学习规则是否既能满足任务性能需求，又能与神经记录数据对齐。

Method: 使用Procrustes分析等方法，对比e-prop和BPTT在神经数据相似性和任务准确性上的表现。

Result: e-prop在任务准确性和神经数据相似性上与BPTT相当，且模型架构和初始条件对神经相似性影响显著。

Conclusion: 生物合理学习规则在任务性能和神经数据相似性上取得显著进展，具有潜在应用价值。

Abstract: Understanding how the brain learns may be informed by studying biologically
plausible learning rules. These rules, often approximating gradient descent
learning to respect biological constraints such as locality, must meet two
critical criteria to be considered an appropriate brain model: (1) good
neuroscience task performance and (2) alignment with neural recordings. While
extensive research has assessed the first criterion, the second remains
underexamined. Employing methods such as Procrustes analysis on well-known
neuroscience datasets, this study demonstrates the existence of a biologically
plausible learning rule -- namely e-prop, which is based on gradient truncation
and has demonstrated versatility across a wide range of tasks -- that can
achieve neural data similarity comparable to Backpropagation Through Time
(BPTT) when matched for task accuracy. Our findings also reveal that model
architecture and initial conditions can play a more significant role in
determining neural similarity than the specific learning rule. Furthermore, we
observe that BPTT-trained models and their biologically plausible counterparts
exhibit similar dynamical properties at comparable accuracies. These results
underscore the substantial progress made in developing biologically plausible
learning rules, highlighting their potential to achieve both competitive task
performance and neural data similarity.

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [827] [Noise Consistency Regularization for Improved Subject-Driven Image Synthesis](https://arxiv.org/abs/2506.06483)
*Yao Ni,Song Wen,Piotr Koniusz,Anoop Cherian*

Main category: cs.GR

TL;DR: 论文提出两种一致性损失函数，解决Stable Diffusion微调中的欠拟合和过拟合问题，提升生成图像的多样性和保真度。


<details>
  <summary>Details</summary>
Motivation: 现有微调方法存在欠拟合（无法可靠捕捉主题身份）和过拟合（记忆主题图像并减少背景多样性）的问题。

Method: 提出两种辅助一致性损失：先验一致性正则化损失和主题一致性正则化损失，分别用于保持预训练模型的噪声预测一致性和增强主题身份的鲁棒性。

Result: 实验表明，该方法在CLIP分数、背景多样性和视觉质量上优于DreamBooth。

Conclusion: 通过引入一致性损失，该方法在保持主题身份的同时提升了图像多样性。

Abstract: Fine-tuning Stable Diffusion enables subject-driven image synthesis by
adapting the model to generate images containing specific subjects. However,
existing fine-tuning methods suffer from two key issues: underfitting, where
the model fails to reliably capture subject identity, and overfitting, where it
memorizes the subject image and reduces background diversity. To address these
challenges, we propose two auxiliary consistency losses for diffusion
fine-tuning. First, a prior consistency regularization loss ensures that the
predicted diffusion noise for prior (non-subject) images remains consistent
with that of the pretrained model, improving fidelity. Second, a subject
consistency regularization loss enhances the fine-tuned model's robustness to
multiplicative noise modulated latent code, helping to preserve subject
identity while improving diversity. Our experimental results demonstrate that
incorporating these losses into fine-tuning not only preserves subject identity
but also enhances image diversity, outperforming DreamBooth in terms of CLIP
scores, background variation, and overall visual quality.

</details>


### [828] [Vid2Sim: Generalizable, Video-based Reconstruction of Appearance, Geometry and Physics for Mesh-free Simulation](https://arxiv.org/abs/2506.06440)
*Chuhao Chen,Zhiyang Dou,Chen Wang,Yiming Huang,Anjun Chen,Qiao Feng,Jiatao Gu,Lingjie Liu*

Main category: cs.GR

TL;DR: Vid2Sim是一种基于视频的通用框架，通过无网格简化模拟高效重建几何和物理属性。


<details>
  <summary>Details</summary>
Motivation: 从视频中忠实重建纹理形状和物理属性具有挑战性，现有方法依赖复杂优化流程且效率低。

Method: Vid2Sim结合前馈神经网络和轻量优化流程，基于线性混合蒙皮（LBS）实现高效重建。

Result: 实验表明，Vid2Sim在几何和物理属性重建上具有高精度和高效性。

Conclusion: Vid2Sim提供了一种高效、通用的解决方案，优于现有方法。

Abstract: Faithfully reconstructing textured shapes and physical properties from videos
presents an intriguing yet challenging problem. Significant efforts have been
dedicated to advancing such a system identification problem in this area.
Previous methods often rely on heavy optimization pipelines with a
differentiable simulator and renderer to estimate physical parameters. However,
these approaches frequently necessitate extensive hyperparameter tuning for
each scene and involve a costly optimization process, which limits both their
practicality and generalizability. In this work, we propose a novel framework,
Vid2Sim, a generalizable video-based approach for recovering geometry and
physical properties through a mesh-free reduced simulation based on Linear
Blend Skinning (LBS), offering high computational efficiency and versatile
representation capability. Specifically, Vid2Sim first reconstructs the
observed configuration of the physical system from video using a feed-forward
neural network trained to capture physical world knowledge. A lightweight
optimization pipeline then refines the estimated appearance, geometry, and
physical properties to closely align with video observations within just a few
minutes. Additionally, after the reconstruction, Vid2Sim enables high-quality,
mesh-free simulation with high efficiency. Extensive experiments demonstrate
that our method achieves superior accuracy and efficiency in reconstructing
geometry and physical properties from video data.

</details>


### [829] [Splat and Replace: 3D Reconstruction with Repetitive Elements](https://arxiv.org/abs/2506.06462)
*Nicolás Violante,Andreas Meuleman,Alban Gauthier,Frédo Durand,Thibault Groueix,George Drettakis*

Main category: cs.GR

TL;DR: 利用3D场景中的重复元素改进新视角合成，通过分割和注册重复实例共享信息，显著提升合成质量。


<details>
  <summary>Details</summary>
Motivation: 现有方法（如NeRF和3DGS）在训练视角不足时，对未见或遮挡部分的渲染质量较低。环境中的重复元素为解决这一问题提供了机会。

Method: 提出一种方法，分割3DGS重建中的重复实例，注册它们并共享信息，同时考虑实例间的外观变化。

Result: 在合成和真实场景中验证，新视角合成质量显著提升。

Conclusion: 利用重复元素改进3D重建是有效的，尤其在视角覆盖不足的情况下。

Abstract: We leverage repetitive elements in 3D scenes to improve novel view synthesis.
Neural Radiance Fields (NeRF) and 3D Gaussian Splatting (3DGS) have greatly
improved novel view synthesis but renderings of unseen and occluded parts
remain low-quality if the training views are not exhaustive enough. Our key
observation is that our environment is often full of repetitive elements. We
propose to leverage those repetitions to improve the reconstruction of
low-quality parts of the scene due to poor coverage and occlusions. We propose
a method that segments each repeated instance in a 3DGS reconstruction,
registers them together, and allows information to be shared among instances.
Our method improves the geometry while also accounting for appearance
variations across instances. We demonstrate our method on a variety of
synthetic and real scenes with typical repetitive elements, leading to a
substantial improvement in the quality of novel view synthesis.

</details>


### [830] [Accelerating 3D Gaussian Splatting with Neural Sorting and Axis-Oriented Rasterization](https://arxiv.org/abs/2506.07069)
*Zhican Wang,Guanghui He,Dantong Liu,Lingjun Gao,Shell Xu Hu,Chen Zhang,Zhuoran Song,Nicholas Lane,Wayne Luk,Hongxiang Fan*

Main category: cs.GR

TL;DR: 论文提出了一种架构与算法协同设计的方法，通过轴定向光栅化和神经排序等技术，显著提升了3D高斯溅射（3DGS）在资源受限设备上的实时渲染效率。


<details>
  <summary>Details</summary>
Motivation: 尽管3DGS在视图合成中表现出色，但在资源受限设备上实现实时渲染仍面临挑战，主要由于功耗和面积限制。

Method: 1. 提出轴定向光栅化以减少重复计算；2. 引入神经排序替代硬件排序；3. 设计可重构处理阵列和优化的瓦片调度策略。

Result: 实验表明，设计在保持渲染质量的同时，速度提升23.4~27.8倍，能耗降低28.8~51.4倍。

Conclusion: 该研究为资源受限设备上的高效3DGS渲染提供了可行方案，并计划开源以推动领域发展。

Abstract: 3D Gaussian Splatting (3DGS) has recently gained significant attention for
high-quality and efficient view synthesis, making it widely adopted in fields
such as AR/VR, robotics, and autonomous driving. Despite its impressive
algorithmic performance, real-time rendering on resource-constrained devices
remains a major challenge due to tight power and area budgets. This paper
presents an architecture-algorithm co-design to address these inefficiencies.
First, we reveal substantial redundancy caused by repeated computation of
common terms/expressions during the conventional rasterization. To resolve
this, we propose axis-oriented rasterization, which pre-computes and reuses
shared terms along both the X and Y axes through a dedicated hardware design,
effectively reducing multiply-and-add (MAC) operations by up to 63%. Second, by
identifying the resource and performance inefficiency of the sorting process,
we introduce a novel neural sorting approach that predicts order-independent
blending weights using an efficient neural network, eliminating the need for
costly hardware sorters. A dedicated training framework is also proposed to
improve its algorithmic stability. Third, to uniformly support rasterization
and neural network inference, we design an efficient reconfigurable processing
array that maximizes hardware utilization and throughput. Furthermore, we
introduce a $\pi$-trajectory tile schedule, inspired by Morton encoding and
Hilbert curve, to optimize Gaussian reuse and reduce memory access overhead.
Comprehensive experiments demonstrate that the proposed design preserves
rendering quality while achieving a speedup of $23.4\sim27.8\times$ and energy
savings of $28.8\sim51.4\times$ compared to edge GPUs for real-world scenes. We
plan to open-source our design to foster further development in this field.

</details>


### [831] [HOI-PAGE: Zero-Shot Human-Object Interaction Generation with Part Affordance Guidance](https://arxiv.org/abs/2506.07209)
*Lei Li,Angela Dai*

Main category: cs.GR

TL;DR: HOI-PAGE是一种从文本提示零样本合成4D人-物交互（HOI）的新方法，基于部分级功能推理。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要关注全局的全身-物体运动，而生成真实多样的HOI需要更细粒度的理解，即人体部分如何与物体部分交互。

Method: 引入部分功能图（PAGs），从大语言模型提取结构化HOI表示，指导三阶段合成：分解3D物体、生成参考视频并提取运动约束、优化4D HOI序列。

Result: 实验表明，该方法能灵活生成复杂多对象或多人的交互序列，显著提升零样本4D HOI生成的现实感和文本对齐性。

Conclusion: HOI-PAGE通过部分级功能推理，实现了更真实和多样化的4D HOI合成。

Abstract: We present HOI-PAGE, a new approach to synthesizing 4D human-object
interactions (HOIs) from text prompts in a zero-shot fashion, driven by
part-level affordance reasoning. In contrast to prior works that focus on
global, whole body-object motion for 4D HOI synthesis, we observe that
generating realistic and diverse HOIs requires a finer-grained understanding --
at the level of how human body parts engage with object parts. We thus
introduce Part Affordance Graphs (PAGs), a structured HOI representation
distilled from large language models (LLMs) that encodes fine-grained part
information along with contact relations. We then use these PAGs to guide a
three-stage synthesis: first, decomposing input 3D objects into geometric
parts; then, generating reference HOI videos from text prompts, from which we
extract part-based motion constraints; finally, optimizing for 4D HOI motion
sequences that not only mimic the reference dynamics but also satisfy
part-level contact constraints. Extensive experiments show that our approach is
flexible and capable of generating complex multi-object or multi-person
interaction sequences, with significantly improved realism and text alignment
for zero-shot 4D HOI generation.

</details>


### [832] [PIG: Physically-based Multi-Material Interaction with 3D Gaussians](https://arxiv.org/abs/2506.07657)
*Zeyu Xiao,Zhenyi Wu,Mingyang Sun,Qipeng Yan,Yufan Guo,Zhuoer Liang,Lihua Zhang*

Main category: cs.GR

TL;DR: PIG方法通过结合3D对象分割与高精度物理模拟，解决了3D高斯场景中的交互问题，提升了视觉质量和几何保真度。


<details>
  <summary>Details</summary>
Motivation: 3D高斯场景中的对象交互存在分割不准确、变形不精确和渲染伪影问题，需要一种新方法解决。

Method: 1. 快速准确地将2D像素映射到3D高斯；2. 为分割对象分配物理属性；3. 嵌入约束尺度到变形梯度中。

Result: 实验显示PIG在视觉质量和几何保真度上优于现有方法。

Conclusion: PIG为物理真实场景生成提供了新方向。

Abstract: 3D Gaussian Splatting has achieved remarkable success in reconstructing both
static and dynamic 3D scenes. However, in a scene represented by 3D Gaussian
primitives, interactions between objects suffer from inaccurate 3D
segmentation, imprecise deformation among different materials, and severe
rendering artifacts. To address these challenges, we introduce PIG:
Physically-Based Multi-Material Interaction with 3D Gaussians, a novel approach
that combines 3D object segmentation with the simulation of interacting objects
in high precision. Firstly, our method facilitates fast and accurate mapping
from 2D pixels to 3D Gaussians, enabling precise 3D object-level segmentation.
Secondly, we assign unique physical properties to correspondingly segmented
objects within the scene for multi-material coupled interactions. Finally, we
have successfully embedded constraint scales into deformation gradients,
specifically clamping the scaling and rotation properties of the Gaussian
primitives to eliminate artifacts and achieve geometric fidelity and visual
consistency. Experimental results demonstrate that our method not only
outperforms the state-of-the-art (SOTA) in terms of visual quality, but also
opens up new directions and pipelines for the field of physically realistic
scene generation.

</details>


### [833] [GaussianVAE: Adaptive Learning Dynamics of 3D Gaussians for High-Fidelity Super-Resolution](https://arxiv.org/abs/2506.07897)
*Shuja Khalid,Mohamed Ibrahim,Yang Liu*

Main category: cs.GR

TL;DR: 提出一种新方法，通过轻量级生成模型和Hessian辅助采样策略，提升3D高斯泼溅的分辨率和几何保真度，突破输入分辨率限制。


<details>
  <summary>Details</summary>
Motivation: 现有3DGS方法受限于输入分辨率，无法生成比训练视图更精细的细节。

Method: 采用轻量级生成模型预测并细化额外的3D高斯分布，结合Hessian辅助采样策略智能识别需要密集化的区域。

Result: 实验表明，该方法在几何精度和渲染质量上显著优于现有技术，且实时性高（单GPU上0.015秒/推理）。

Conclusion: 该方法为分辨率无关的3D场景增强提供了新范式。

Abstract: We present a novel approach for enhancing the resolution and geometric
fidelity of 3D Gaussian Splatting (3DGS) beyond native training resolution.
Current 3DGS methods are fundamentally limited by their input resolution,
producing reconstructions that cannot extrapolate finer details than are
present in the training views. Our work breaks this limitation through a
lightweight generative model that predicts and refines additional 3D Gaussians
where needed most. The key innovation is our Hessian-assisted sampling
strategy, which intelligently identifies regions that are likely to benefit
from densification, ensuring computational efficiency. Unlike computationally
intensive GANs or diffusion approaches, our method operates in real-time
(0.015s per inference on a single consumer-grade GPU), making it practical for
interactive applications. Comprehensive experiments demonstrate significant
improvements in both geometric accuracy and rendering quality compared to
state-of-the-art methods, establishing a new paradigm for resolution-free 3D
scene enhancement.

</details>


### [834] [Speedy Deformable 3D Gaussian Splatting: Fast Rendering and Compression of Dynamic Scenes](https://arxiv.org/abs/2506.07917)
*Allen Tu,Haiyang Ying,Alex Hanson,Yonghan Lee,Tom Goldstein,Matthias Zwicker*

Main category: cs.GR

TL;DR: SpeeDe3DGS通过时间敏感剪枝和GroupFlow技术，显著加速动态3D高斯泼溅渲染，提升速度10.37倍，减少模型大小7.71倍。


<details>
  <summary>Details</summary>
Motivation: 动态3D高斯泼溅（3DGS）中，每帧对每个高斯进行神经网络推断导致渲染速度慢、内存和计算需求高。

Method: 提出时间敏感剪枝分数和退火平滑剪枝机制，以及GroupFlow运动分析技术，聚类高斯并预测组级刚性变换。

Result: 在NeRF-DS数据集上，渲染速度提升10.37倍，模型大小减少7.71倍，训练时间缩短2.71倍。

Conclusion: SpeeDe3DGS模块化设计可集成到任何可变形3DGS或4DGS框架中，显著提升效率。

Abstract: Recent extensions of 3D Gaussian Splatting (3DGS) to dynamic scenes achieve
high-quality novel view synthesis by using neural networks to predict the
time-varying deformation of each Gaussian. However, performing per-Gaussian
neural inference at every frame poses a significant bottleneck, limiting
rendering speed and increasing memory and compute requirements. In this paper,
we present Speedy Deformable 3D Gaussian Splatting (SpeeDe3DGS), a general
pipeline for accelerating the rendering speed of dynamic 3DGS and 4DGS
representations by reducing neural inference through two complementary
techniques. First, we propose a temporal sensitivity pruning score that
identifies and removes Gaussians with low contribution to the dynamic scene
reconstruction. We also introduce an annealing smooth pruning mechanism that
improves pruning robustness in real-world scenes with imprecise camera poses.
Second, we propose GroupFlow, a motion analysis technique that clusters
Gaussians by trajectory similarity and predicts a single rigid transformation
per group instead of separate deformations for each Gaussian. Together, our
techniques accelerate rendering by $10.37\times$, reduce model size by
$7.71\times$, and shorten training time by $2.71\times$ on the NeRF-DS dataset.
SpeeDe3DGS also improves rendering speed by $4.20\times$ and $58.23\times$ on
the D-NeRF and HyperNeRF vrig datasets. Our methods are modular and can be
integrated into any deformable 3DGS or 4DGS framework.

</details>


### [835] [Squeeze3D: Your 3D Generation Model is Secretly an Extreme Neural Compressor](https://arxiv.org/abs/2506.07932)
*Rishit Dagli,Yushi Guan,Sankeerth Durvasula,Mohammadreza Mofayezi,Nandita Vijaykumar*

Main category: cs.GR

TL;DR: Squeeze3D是一种利用预训练3D生成模型隐式先验知识实现高压缩比3D数据压缩的新框架。


<details>
  <summary>Details</summary>
Motivation: 现有3D数据压缩方法难以同时实现高压缩比和高质量重建，Squeeze3D旨在通过预训练模型的隐式知识解决这一问题。

Method: 通过可训练的映射网络连接预训练编码器和生成模型的潜在空间，将3D数据（网格、点云或辐射场）压缩为紧凑潜在代码，再通过生成模型重建。

Result: Squeeze3D实现了网格2187x、点云55x、辐射场619x的高压缩比，且视觉质量与现有方法相当。

Conclusion: Squeeze3D无需特定对象训练，支持多种3D格式，是一种高效灵活的压缩框架。

Abstract: We propose Squeeze3D, a novel framework that leverages implicit prior
knowledge learnt by existing pre-trained 3D generative models to compress 3D
data at extremely high compression ratios. Our approach bridges the latent
spaces between a pre-trained encoder and a pre-trained generation model through
trainable mapping networks. Any 3D model represented as a mesh, point cloud, or
a radiance field is first encoded by the pre-trained encoder and then
transformed (i.e. compressed) into a highly compact latent code. This latent
code can effectively be used as an extremely compressed representation of the
mesh or point cloud. A mapping network transforms the compressed latent code
into the latent space of a powerful generative model, which is then conditioned
to recreate the original 3D model (i.e. decompression). Squeeze3D is trained
entirely on generated synthetic data and does not require any 3D datasets. The
Squeeze3D architecture can be flexibly used with existing pre-trained 3D
encoders and existing generative models. It can flexibly support different
formats, including meshes, point clouds, and radiance fields. Our experiments
demonstrate that Squeeze3D achieves compression ratios of up to 2187x for
textured meshes, 55x for point clouds, and 619x for radiance fields while
maintaining visual quality comparable to many existing methods. Squeeze3D only
incurs a small compression and decompression latency since it does not involve
training object-specific networks to compress an object.

</details>


<div id='math.PR'></div>

# math.PR [[Back]](#toc)

### [836] [Poisson Midpoint Method for Log Concave Sampling: Beyond the Strong Error Lower Bounds](https://arxiv.org/abs/2506.07614)
*Rishikesh Srinivasan,Dheeraj Nagaraj*

Main category: math.PR

TL;DR: 研究使用泊松中点离散化方法从强对数凹分布中采样，证明其在2-Wasserstein距离下的收敛性，比欧拉-丸山离散化快三倍，且在欠阻尼Langevin动力学中复杂度更低。


<details>
  <summary>Details</summary>
Motivation: 探索更高效的采样方法，以解决强对数凹分布采样的计算效率问题。

Method: 采用泊松中点离散化方法（随机中点法的变体），分析其在过阻尼/欠阻尼Langevin动力学中的收敛性。

Result: 在2-Wasserstein距离下实现收敛，且对目标精度的依赖比欧拉-丸山离散化快三倍；欠阻尼情况下复杂度显著低于文献中的L^2强误差下界。

Conclusion: 泊松中点离散化方法在强对数凹分布采样中具有显著优势，尤其在欠阻尼Langevin动力学中表现更优。

Abstract: We study the problem of sampling from strongly log-concave distributions over
$\mathbb{R}^d$ using the Poisson midpoint discretization (a variant of the
randomized midpoint method) for overdamped/underdamped Langevin dynamics. We
prove its convergence in the 2-Wasserstein distance ($W_2$), achieving a cubic
speedup in dependence on the target accuracy ($\epsilon$) over the
Euler-Maruyama discretization, surpassing existing bounds for randomized
midpoint methods. Notably, in the case of underdamped Langevin dynamics, we
demonstrate the complexity of $W_2$ convergence is much smaller than the
complexity lower bounds for convergence in $L^2$ strong error established in
the literature.

</details>


<div id='cs.CE'></div>

# cs.CE [[Back]](#toc)

### [837] [\textit{QuantMCP}: Grounding Large Language Models in Verifiable Financial Reality](https://arxiv.org/abs/2506.06622)
*Yifan Zeng*

Main category: cs.CE

TL;DR: QuantMCP框架通过标准化工具调用，将LLMs与实时金融数据API连接，解决数据幻觉问题，提升金融分析的准确性和深度。


<details>
  <summary>Details</summary>
Motivation: LLMs在金融分析中潜力巨大，但受限于数据幻觉和缺乏实时数据访问，QuantMCP旨在解决这些问题。

Method: 利用Model Context Protocol（MCP）标准化工具调用，连接Python金融数据API，通过自然语言交互获取实时数据。

Result: QuantMCP使LLMs能够准确获取和分析实时金融数据，生成可靠见解，支持更明智的金融决策。

Conclusion: QuantMCP为LLMs与金融数据之间提供了可靠桥梁，提升了金融应用中LLMs的可靠性和分析能力。

Abstract: Large Language Models (LLMs) hold immense promise for revolutionizing
financial analysis and decision-making, yet their direct application is often
hampered by issues of data hallucination and lack of access to real-time,
verifiable financial information. This paper introduces QuantMCP, a novel
framework designed to rigorously ground LLMs in financial reality. By
leveraging the Model Context Protocol (MCP) for standardized and secure tool
invocation, QuantMCP enables LLMs to accurately interface with a diverse array
of Python-accessible financial data APIs (e.g., Wind, yfinance). Users can
interact via natural language to precisely retrieve up-to-date financial data,
thereby overcoming LLM's inherent limitations in factual data recall. More
critically, once furnished with this verified, structured data, the LLM's
analytical capabilities are unlocked, empowering it to perform sophisticated
data interpretation, generate insights, and ultimately support more informed
financial decision-making processes. QuantMCP provides a robust, extensible,
and secure bridge between conversational AI and the complex world of financial
data, aiming to enhance both the reliability and the analytical depth of LLM
applications in finance.

</details>


### [838] [Deep Learning Enhanced Multi-Day Turnover Quantitative Trading Algorithm for Chinese A-Share Market](https://arxiv.org/abs/2506.06356)
*Yimin Du*

Main category: cs.CE

TL;DR: 本文提出了一种基于深度学习的多日换手量化交易算法，适用于中国A股市场，结合多个模块实现高效资本利用与风险管理，年化收益15.2%，最大回撤低于5%。


<details>
  <summary>Details</summary>
Motivation: 针对中国A股市场的量化交易需求，设计一种能够平衡资本效率与风险管理的算法，以应对复杂的市场环境。

Method: 结合五个模块：深度横截面预测网络选股、混合模型分析开盘信号、动态头寸规模调整、网格搜索优化的止盈止损机制、多粒度波动性市场择时模型。

Result: 在2010-2020年数据训练和2021-2024年回测中，年化收益15.2%，最大回撤低于5%，夏普比率1.87，适合机构部署。

Conclusion: 该算法在多种市场环境下表现稳健，具有高资本容量和可扩展性，适合量化交易实践。

Abstract: This paper presents a sophisticated multi-day turnover quantitative trading
algorithm that integrates advanced deep learning techniques with comprehensive
cross-sectional stock prediction for the Chinese A-share market. Our framework
combines five interconnected modules: initial stock selection through deep
cross-sectional prediction networks, opening signal distribution analysis using
mixture models for arbitrage identification, market capitalization and
liquidity-based dynamic position sizing, grid-search optimized profit-taking
and stop-loss mechanisms, and multi-granularity volatility-based market timing
models. The algorithm employs a novel approach to balance capital efficiency
with risk management through adaptive holding periods and sophisticated
entry/exit timing. Trained on comprehensive A-share data from 2010-2020 and
rigorously backtested on 2021-2024 data, our method achieves remarkable
performance with 15.2\% annualized returns, maximum drawdown constrained below
5\%, and a Sharpe ratio of 1.87. The strategy demonstrates exceptional
scalability by maintaining 50-100 daily positions with a 9-day maximum holding
period, incorporating dynamic profit-taking and stop-loss mechanisms that
enhance capital turnover efficiency while preserving risk-adjusted returns. Our
approach exhibits robust performance across various market regimes while
maintaining high capital capacity suitable for institutional deployment.

</details>


<div id='q-bio.BM'></div>

# q-bio.BM [[Back]](#toc)

### [839] [Template-Guided 3D Molecular Pose Generation via Flow Matching and Differentiable Optimization](https://arxiv.org/abs/2506.06305)
*Noémie Bergues,Arthur Carré,Paul Join-Lambert,Brice Hoffmann,Arnaud Blondel,Hamza Tajmouati*

Main category: q-bio.BM

TL;DR: 论文提出了一种基于模板的两阶段方法，用于预测小分子在蛋白质结合位点的3D构象，并在新基准测试中表现优于现有工具。


<details>
  <summary>Details</summary>
Motivation: 预测小分子在蛋白质结合位点的3D构象是药物设计中的关键挑战，而现有方法在模板相似性低或配体灵活性高时效果不佳。

Method: 方法分为两阶段：1）基于流匹配的分子对齐生成初始3D坐标；2）通过可微分优化程序基于形状、药效团相似性和内部能量等进一步优化构象。

Result: 在新基准测试中，该方法优于标准对接工具和开放对齐方法，尤其在模板相似性低或配体灵活性高的情况下。

Conclusion: 该方法为基于模板的配体构象预测提供了有效解决方案，尤其在复杂情况下表现突出。

Abstract: Predicting the 3D conformation of small molecules within protein binding
sites is a key challenge in drug design. When a crystallized reference ligand
(template) is available, it provides geometric priors that can guide 3D pose
prediction. We present a two-stage method for ligand conformation generation
guided by such templates. In the first stage, we introduce a molecular
alignment approach based on flow-matching to generate 3D coordinates for the
ligand, using the template structure as a reference. In the second stage, a
differentiable pose optimization procedure refines this conformation based on
shape and pharmacophore similarities, internal energy, and, optionally, the
protein binding pocket. We evaluate our approach on a new benchmark of ligand
pairs co-crystallized with the same target and show that it outperforms
standard docking tools and open-access alignment methods, especially in cases
involving low similarity to the template or high ligand flexibility.

</details>


### [840] [AnnoDPO: Protein Functional Annotation Learning with Direct Preference Optimization](https://arxiv.org/abs/2506.07035)
*Zixuan Jiang,Renjing Xu*

Main category: q-bio.BM

TL;DR: AnnoDPO是一个基于直接偏好优化（DPO）的多模态框架，用于解决蛋白质功能预测中的注释稀缺和类别不平衡问题。


<details>
  <summary>Details</summary>
Motivation: 蛋白质功能注释类别繁多且分布不均，给蛋白质语言模型（PLMs）带来挑战。

Method: 采用直接偏好优化（DPO）进行偏好对齐训练，整合生物知识。

Result: 提出了一种新的蛋白质表示学习范式。

Conclusion: AnnoDPO为蛋白质功能预测提供了有效的解决方案。

Abstract: Deciphering protein function remains a fundamental challenge in protein
representation learning. The task presents significant difficulties for protein
language models (PLMs) due to the sheer volume of functional annotation
categories and the highly imbalanced distribution of annotated instances across
biological ontologies. Inspired by the remarkable success of reinforcement
learning from human feedback (RLHF) in large language model (LLM) alignment, we
propose AnnoDPO, a novel multi-modal framework for protein function prediction
that leverages Direct Preference Optimization (DPO) to enhance annotation
learning. Our methodology addresses the dual challenges of annotation scarcity
and category imbalance through preference-aligned training objectives,
establishing a new paradigm for biological knowledge integration in protein
representation learning.

</details>


### [841] [Graph Neural Networks in Modern AI-aided Drug Discovery](https://arxiv.org/abs/2506.06915)
*Odin Zhang,Haitao Lin,Xujun Zhang,Xiaorui Wang,Zhenxing Wu,Qing Ye,Weibo Zhao,Jike Wang,Kejun Ying,Yu Kang,Chang-yu Hsieh,Tingjun Hou*

Main category: q-bio.BM

TL;DR: 综述文章全面介绍了图神经网络（GNNs）在药物发现中的应用，包括方法基础、代表性任务及最新进展。


<details>
  <summary>Details</summary>
Motivation: GNNs因其对分子结构的直观表达能力，成为AI辅助药物发现（AIDD）的有力工具。

Method: 文章总结了GNNs在分子性质预测、虚拟筛选、分子生成等任务中的应用，并重点介绍了几何GNNs、可解释模型等最新方法。

Result: GNNs在药物发现中展现出强大潜力，但仍面临实际应用中的挑战。

Conclusion: 未来需进一步解决GNNs在药物发现中的方法瓶颈，并探索与其他深度学习技术的结合。

Abstract: Graph neural networks (GNNs), as topology/structure-aware models within deep
learning, have emerged as powerful tools for AI-aided drug discovery (AIDD). By
directly operating on molecular graphs, GNNs offer an intuitive and expressive
framework for learning the complex topological and geometric features of
drug-like molecules, cementing their role in modern molecular modeling. This
review provides a comprehensive overview of the methodological foundations and
representative applications of GNNs in drug discovery, spanning tasks such as
molecular property prediction, virtual screening, molecular generation,
biomedical knowledge graph construction, and synthesis planning. Particular
attention is given to recent methodological advances, including geometric GNNs,
interpretable models, uncertainty quantification, scalable graph architectures,
and graph generative frameworks. We also discuss how these models integrate
with modern deep learning approaches, such as self-supervised learning,
multi-task learning, meta-learning and pre-training. Throughout this review, we
highlight the practical challenges and methodological bottlenecks encountered
when applying GNNs to real-world drug discovery pipelines, and conclude with a
discussion on future directions.

</details>


<div id='econ.GN'></div>

# econ.GN [[Back]](#toc)

### [842] [Improving choice model specification using reinforcement learning](https://arxiv.org/abs/2506.06410)
*Gabriel Nova,Sander van Cranenburgh,Stephane Hess*

Main category: econ.GN

TL;DR: 论文提出了一种基于深度强化学习的离散选择建模框架，解决了传统元启发式方法的局限性，能够动态调整策略并提高模型适应性。


<details>
  <summary>Details</summary>
Motivation: 传统离散选择建模依赖专家经验、耗时且主观，现有元启发式方法无法动态调整搜索策略或转移知识。

Method: 采用深度强化学习框架，通过智能体（agent）根据拟合优度和简洁性奖励动态生成模型。

Result: 智能体能动态适应不同数据生成过程，表现出鲁棒性和潜在的可转移性。

Conclusion: 该框架无需先验领域知识，显著提升了离散选择建模的效率和适应性。

Abstract: Discrete choice modelling is a theory-driven modelling framework for
understanding and forecasting choice behaviour. To obtain behavioural insights,
modellers test several competing model specifications in their attempts to
discover the 'true' data generation process. This trial-and-error process
requires expertise, is time-consuming, and relies on subjective theoretical
assumptions. Although metaheuristics have been proposed to assist choice
modellers, they treat model specification as a classic optimisation problem,
relying on static strategies, applying predefined rules, and neglecting
outcomes from previous estimated models. As a result, current metaheuristics
struggle to prioritise promising search regions, adapt exploration dynamically,
and transfer knowledge to other modelling tasks. To address these limitations,
we introduce a deep reinforcement learning-based framework where an 'agent'
specifies models by estimating them and receiving rewards based on
goodness-of-fit and parsimony. Results demonstrate the agent dynamically adapts
its strategies to identify promising specifications across data generation
processes, showing robustness and potential transferability, without prior
domain knowledge.

</details>


<div id='stat.ME'></div>

# stat.ME [[Back]](#toc)

### [843] [Conditional Local Independence Testing with Application to Dynamic Causal Discovery](https://arxiv.org/abs/2506.07844)
*Mingzhou Liu,Xinwei Sun,Yizhou Wang*

Main category: stat.ME

TL;DR: 将条件局部独立性检验理论扩展到Ito过程，应用于动态系统的因果发现。


<details>
  <summary>Details</summary>
Motivation: 扩展Christgau等人的理论，以适用于更广泛的动态系统。

Method: 将条件局部独立性检验理论应用于Ito过程。

Result: 理论扩展成功，可用于动态系统的因果发现。

Conclusion: 扩展后的理论为动态系统的因果分析提供了新工具。

Abstract: In this note, we extend the conditional local independence testing theory
developed in Christgau et al. (2024) to Ito processes. The result can be
applied to causal discovery in dynamic systems.

</details>


<div id='eess.AS'></div>

# eess.AS [[Back]](#toc)

### [844] [SongBloom: Coherent Song Generation via Interleaved Autoregressive Sketching and Diffusion Refinement](https://arxiv.org/abs/2506.07634)
*Chenyu Yang,Shuai Wang,Hangting Chen,Wei Tan,Jianwei Yu,Haizhou Li*

Main category: eess.AS

TL;DR: SongBloom是一个新颖的框架，通过结合自回归草图和扩散细化方法生成高质量的全长歌曲，解决了现有方法在全局一致性和局部保真度上的不足。


<details>
  <summary>Details</summary>
Motivation: 现有语言模型和扩散方法在生成音乐时难以平衡全局一致性与局部保真度，导致音乐性不足或歌词与旋律不匹配。

Method: SongBloom采用自回归扩散模型，通过逐步扩展音乐草图（从短到长）和细化细节（从粗到细），结合语义和声学上下文指导生成过程。

Result: 实验表明，SongBloom在主观和客观指标上均优于现有方法，性能接近商业音乐生成平台。

Conclusion: SongBloom通过创新的生成范式，显著提升了音乐生成的质量和一致性。

Abstract: Generating music with coherent structure, harmonious instrumental and vocal
elements remains a significant challenge in song generation. Existing language
models and diffusion-based methods often struggle to balance global coherence
with local fidelity, resulting in outputs that lack musicality or suffer from
incoherent progression and mismatched lyrics. This paper introduces
$\textbf{SongBloom}$, a novel framework for full-length song generation that
leverages an interleaved paradigm of autoregressive sketching and
diffusion-based refinement. SongBloom employs an autoregressive diffusion model
that combines the high fidelity of diffusion models with the scalability of
language models. Specifically, it gradually extends a musical sketch from short
to long and refines the details from coarse to fine-grained. The interleaved
generation paradigm effectively integrates prior semantic and acoustic context
to guide the generation process. Experimental results demonstrate that
SongBloom outperforms existing methods across both subjective and objective
metrics and achieves performance comparable to the state-of-the-art commercial
music generation platforms. Audio samples are available on our demo page:
https://cypress-yang.github.io/SongBloom\_demo.

</details>


### [845] [Accurate analysis of the pitch pulse-based magnitude/phase structure of natural vowels and assessment of three lightweight time/frequency voicing restoration methods](https://arxiv.org/abs/2506.06675)
*Aníbal J. S. Ferreira,Luis M. T. Jesus,Laurentino M. M. Leal,Jorge E. F. Spratley*

Main category: eess.AS

TL;DR: 该论文提出了一种从耳语音恢复自然语音的信号处理方法，重点解决了谐波相位/幅度结构建模和基于模型的合成发声问题。


<details>
  <summary>Details</summary>
Motivation: 耳语音缺乏周期性信号成分，恢复自然语音需要精细的信号处理技术，尤其是在低资源便携设备上实时实现。

Method: 提出了一种分割单个音高脉冲的新算法，并比较了三种基于模型的合成发声实现方法：频域、频时域结合和生理启发的单独滤波。

Result: 通过客观和主观测试比较了三种方法的性能，展示了持续和协同发音元音之间的差异。

Conclusion: 论文为耳语音恢复自然语音提供了实用的技术方案，并验证了不同合成发声方法的有效性。

Abstract: Whispered speech is produced when the vocal folds are not used, either
intentionally, or due to a temporary or permanent voice condition. The
essential difference between natural speech and whispered speech is that
periodic signal components that exist in certain regions of the former, called
voiced regions, as a consequence of the vibration of the vocal folds, are
missing in the latter. The restoration of natural speech from whispered speech
requires delicate signal processing procedures that are especially useful if
they can be implemented on low-resourced portable devices, in real-time, and
on-the-fly, taking advantage of the established source-filter paradigm of voice
production and related models. This paper addresses two challenges that are
intertwined and are key in informing and making viable this envisioned
technological realization. The first challenge involves characterizing and
modeling the evolution of the harmonic phase/magnitude structure of a sequence
of individual pitch periods in a voiced region of natural speech comprising
sustained or co-articulated vowels. This paper proposes a novel algorithm
segmenting individual pitch pulses, which is then used to obtain illustrative
results highlighting important differences between sustained and co-articulated
vowels, and suggesting practical synthetic voicing approaches. The second
challenge involves model-based synthetic voicing. Three implementation
alternatives are described that differ in their signal reconstruction
approaches: frequency-domain, combined frequency and time-domain, and
physiologically-inspired separate filtering of glottal excitation pulses
individually generated. The three alternatives are compared objectively using
illustrative examples, and subjectively using the results of listening tests
involving synthetic voicing of sustained and co-articulated vowels in word
context.

</details>


### [846] [Speaker-Distinguishable CTC: Learning Speaker Distinction Using CTC for Multi-Talker Speech Recognition](https://arxiv.org/abs/2506.07515)
*Asahi Sakuma,Hiroaki Sato,Ryuga Sugano,Tadashi Kumano,Yoshihiko Kawai,Tetsuji Ogawa*

Main category: eess.AS

TL;DR: 提出了一种无需辅助信息的多说话人语音识别框架SD-CTC，结合SOT框架显著降低错误率。


<details>
  <summary>Details</summary>
Motivation: 解决SOT方法因说话人分配失败导致的识别错误问题，避免依赖难以提取的辅助信息。

Method: 提出SD-CTC，扩展CTC以联合分配标记和说话人标签，并与SOT框架结合。

Result: 实验显示，SD-CTC与SOT的多任务学习使错误率降低26%，性能媲美依赖辅助信息的方法。

Conclusion: SD-CTC为多说话人语音识别提供了一种高效且无需辅助信息的解决方案。

Abstract: This paper presents a novel framework for multi-talker automatic speech
recognition without the need for auxiliary information. Serialized Output
Training (SOT), a widely used approach, suffers from recognition errors due to
speaker assignment failures. Although incorporating auxiliary information, such
as token-level timestamps, can improve recognition accuracy, extracting such
information from natural conversational speech remains challenging. To address
this limitation, we propose Speaker-Distinguishable CTC (SD-CTC), an extension
of CTC that jointly assigns a token and its corresponding speaker label to each
frame. We further integrate SD-CTC into the SOT framework, enabling the SOT
model to learn speaker distinction using only overlapping speech and
transcriptions. Experimental comparisons show that multi-task learning with
SD-CTC and SOT reduces the error rate of the SOT model by 26% and achieves
performance comparable to state-of-the-art methods relying on auxiliary
information.

</details>


### [847] [AS-ASR: A Lightweight Framework for Aphasia-Specific Automatic Speech Recognition](https://arxiv.org/abs/2506.06566)
*Chen Bao,Chuanbing Huo,Qinyu Chen,Chang Gao*

Main category: eess.AS

TL;DR: AS-ASR是一个基于Whisper-tiny的轻量级失语症专用语音识别框架，适用于边缘设备的低资源部署。


<details>
  <summary>Details</summary>
Motivation: 针对失语症患者的语音识别需求，提出一种高效且适应性强的解决方案。

Method: 采用混合训练策略结合标准与失语症语音，并利用GPT-4增强参考文本质量。

Result: 模型在失语症语音上的WER降低30%以上，同时保持标准语音性能。

Conclusion: 该框架为现实世界中的障碍语音识别提供了可扩展且高效的解决方案。

Abstract: This paper proposes AS-ASR, a lightweight aphasia-specific speech recognition
framework based on Whisper-tiny, tailored for low-resource deployment on edge
devices. Our approach introduces a hybrid training strategy that systematically
combines standard and aphasic speech at varying ratios, enabling robust
generalization, and a GPT-4-based reference enhancement method that refines
noisy aphasic transcripts, improving supervision quality. We conduct extensive
experiments across multiple data mixing configurations and evaluation settings.
Results show that our fine-tuned model significantly outperforms the zero-shot
baseline, reducing WER on aphasic speech by over 30% while preserving
performance on standard speech. The proposed framework offers a scalable,
efficient solution for real-world disordered speech recognition.

</details>


### [848] [Reducing Object Hallucination in Large Audio-Language Models via Audio-Aware Decoding](https://arxiv.org/abs/2506.07233)
*Tzu-wen Hsu,Ke-Han Lu,Cheng-Han Chiang,Hung-yi Lee*

Main category: eess.AS

TL;DR: 论文提出了一种名为Audio-Aware Decoding (AAD)的轻量级推理策略，通过对比解码减少大型音频-语言模型（LALMs）的幻觉问题，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 现有LALMs在标准测试中表现良好，但存在幻觉问题（即模型生成与音频内容不符的回答）。为了解决这一问题，作者提出了AAD。

Method: AAD采用对比解码方法，比较音频上下文存在与否时的token预测概率，选择音频存在时概率增加的token。

Result: 实验表明，AAD在对象幻觉数据集上将F1分数提升了0.046至0.428，在通用音频QA数据集（如Clotho-AQA）上准确率提升了5.4%至10.3%。

Conclusion: AAD有效减少了LALMs的幻觉问题，并通过消融实验验证了其各组成部分的有效性。

Abstract: Large Audio-Language Models (LALMs) can take audio and text as the inputs and
answer questions about the audio. While prior LALMs have shown strong
performance on standard benchmarks, there has been alarming evidence that LALMs
can hallucinate what is presented in the audio. To mitigate the hallucination
of LALMs, we introduce Audio-Aware Decoding (AAD), a lightweight inference-time
strategy that uses contrastive decoding to compare the token prediction logits
with and without the audio context. By contrastive decoding, AAD promotes the
tokens whose probability increases when the audio is present. We conduct our
experiment on object hallucination datasets with three LALMs and show that AAD
improves the F1 score by 0.046 to 0.428. We also show that AAD can improve the
accuracy on general audio QA datasets like Clotho-AQA by 5.4% to 10.3%. We
conduct thorough ablation studies to understand the effectiveness of each
component in AAD.

</details>


### [849] [Neural Spectral Band Generation for Audio Coding](https://arxiv.org/abs/2506.06732)
*Woongjib Choi,Byeong Hyeon Kim,Hyungseob Lim,Inseon Jang,Hong-Goo Kang*

Main category: eess.AS

TL;DR: 论文提出了一种新的参数化非盲带宽扩展方法，结合DNN技术，以改进传统SBR方法的局限性。


<details>
  <summary>Details</summary>
Motivation: 传统SBR方法在处理多样音频信号时存在局限性，而现有DNN方法因缺乏先验信息导致性能不佳。

Method: 提出了一种参数化非盲带宽扩展方法，在音频编码管道的首尾分别使用DNN提取侧信息和扩展带宽。

Result: 该方法结合了DNN的优势，避免了传统SBR的粗糙特征提取问题。

Conclusion: 新方法有望替代传统SBR，提升带宽扩展的性能和适应性。

Abstract: Audio bandwidth extension is the task of reconstructing missing high
frequency components of bandwidth-limited audio signals, where bandwidth
limitation is a common issue for audio signals due to several reasons,
including channel capacity and data constraints. While conventional spectral
band replication is a well-established parametric approach to audio bandwidth
extension, the SBR usually entails coarse feature extraction and reconstruction
techniques, which leads to limitations when processing various types of audio
signals. In parallel, numerous deep neural network-based audio bandwidth
extension methods have been proposed. These DNN-based methods are usually
referred to as blind BWE, as these methods do not rely on prior information
extracted from original signals, and only utilize given low frequency band
signals to estimate missing high frequency components. In order to replace
conventional SBR with DNNs, simply adopting existing DNN-based methodologies
results in suboptimal performance due to the blindness of these methods. My
proposed research suggests a new approach to parametric non-blind bandwidth
extension, as DNN-based side information extraction and DNN-based bandwidth
extension are performed only at the front and end of the audio coding pipeline.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [850] [Cost-Efficient LLM Training with Lifetime-Aware Tensor Offloading via GPUDirect Storage](https://arxiv.org/abs/2506.06472)
*Ziqi Yuan,Haoyang Zhang,Yirui Eric Zhou,Apoorve Mohan,I-Hsin Chung,Seetharami Seelam,Jian Huang*

Main category: cs.DC

TL;DR: TERAIO是一个基于SSD的GPU内存扩展框架，通过智能张量卸载和预取优化LLM训练性能。


<details>
  <summary>Details</summary>
Motivation: 观察到LLM训练中活跃张量仅占GPU内存的1.7%，非活跃张量长时间未使用，提供了卸载和预取的机会。

Method: 通过前几次迭代分析张量生命周期，生成优化卸载/预取计划，并集成到PyTorch中，利用GPUDirect存储直接迁移张量。

Result: 相比ZeRO-Offload和ZeRO-Infinity，TERAIO平均提升LLM训练性能1.47倍，达到理想性能的80.7%。

Conclusion: TERAIO通过智能张量管理显著提升LLM训练效率，接近理想性能。

Abstract: We present the design and implementation of a new lifetime-aware tensor
offloading framework for GPU memory expansion using low-cost PCIe-based
solid-state drives (SSDs). Our framework, TERAIO, is developed explicitly for
large language model (LLM) training with multiple GPUs and multiple SSDs. Its
design is driven by our observation that the active tensors take only a small
fraction (1.7% on average) of allocated GPU memory in each LLM training
iteration, the inactive tensors are usually large and will not be used for a
long period of time, creating ample opportunities for offloading/prefetching
tensors to/from slow SSDs without stalling the GPU training process. TERAIO
accurately estimates the lifetime (active period of time in GPU memory) of each
tensor with the profiling of the first few iterations in the training process.
With the tensor lifetime analysis, TERAIO will generate an optimized tensor
offloading/prefetching plan and integrate it into the compiled LLM program via
PyTorch. TERAIO has a runtime tensor migration engine to execute the
offloading/prefetching plan via GPUDirect storage, which allows direct tensor
migration between GPUs and SSDs for alleviating the CPU bottleneck and
maximizing the SSD bandwidth utilization. In comparison with state-of-the-art
studies such as ZeRO-Offload and ZeRO-Infinity, we show that TERAIO improves
the training performance of various LLMs by 1.47x on average, and achieves
80.7% of the ideal performance assuming unlimited GPU memory.

</details>


### [851] [pFedSOP : Accelerating Training Of Personalized Federated Learning Using Second-Order Optimization](https://arxiv.org/abs/2506.07159)
*Mrinmay Sen,Chalavadi Krishna Mohan*

Main category: cs.DC

TL;DR: pFedSOP利用二阶优化加速个性化联邦学习，减少通信轮次并提升性能。


<details>
  <summary>Details</summary>
Motivation: 解决传统联邦学习中因数据异构导致的模型泛化问题，以及现有PFL方法因一阶优化导致的训练缓慢和通信轮次增加问题。

Method: 提出pFedSOP，通过基于Gompertz函数的个性化梯度更新和正则化Fisher信息矩阵（FIM）近似Hessian矩阵，实现二阶优化。

Result: 在异构图像分类数据集上，pFedSOP优于现有FL和PFL算法。

Conclusion: pFedSOP通过二阶优化有效加速训练并减少通信轮次，为个性化联邦学习提供了高效解决方案。

Abstract: Personalized Federated Learning (PFL) enables clients to collaboratively
train personalized models tailored to their individual objectives, addressing
the challenge of model generalization in traditional Federated Learning (FL)
due to high data heterogeneity. However, existing PFL methods often require
increased communication rounds to achieve the desired performance, primarily
due to slow training caused by the use of first-order optimization, which has
linear convergence. Additionally, many of these methods increase local
computation because of the additional data fed into the model during the search
for personalized local models. One promising solution to this slow training is
second-order optimization, known for its quadratic convergence. However,
employing it in PFL is challenging due to the Hessian matrix and its inverse.
In this paper, we propose pFedSOP, which efficiently utilizes second-order
optimization in PFL to accelerate the training of personalized models and
enhance performance with fewer communication rounds. Our approach first
computes a personalized local gradient update using the Gompertz function-based
normalized angle between local and global gradient updates, incorporating
client-specific global information. We then use a regularized Fisher
Information Matrix (FIM), computed from this personalized gradient update, as
an approximation of the Hessian to update the personalized models. This
FIM-based second-order optimization speeds up training with fewer communication
rounds by tackling the challenges with exact Hessian and avoids additional data
being fed into the model during the search for personalized local models.
Extensive experiments on heterogeneously partitioned image classification
datasets with partial client participation demonstrate that pFedSOP outperforms
state-of-the-art FL and PFL algorithms.

</details>


<div id='econ.TH'></div>

# econ.TH [[Back]](#toc)

### [852] [From Axioms to Algorithms: Mechanized Proofs of the vNM Utility Theorem](https://arxiv.org/abs/2506.07066)
*Li Jingyuan*

Main category: econ.TH

TL;DR: 本文使用Lean 4交互式定理证明器对冯·诺依曼-摩根斯坦（vNM）期望效用定理进行了全面的形式化，验证了偏好关系的数学结构及其效用表示的存在性与唯一性。


<details>
  <summary>Details</summary>
Motivation: 为经济建模、AI对齐和管理决策系统提供严格的理论基础，弥合理论决策理论与计算实现之间的差距。

Method: 实现偏好完备性、传递性、连续性和独立性等经典公理，并通过机器验证证明效用表示的存在性与唯一性。

Result: 验证了满足vNM公理的偏好可以通过期望效用最大化表示，并提供了混合彩票基本主张的形式化证明。

Conclusion: 该形式化为理论决策理论的应用提供了精确的计算基础，同时验证了经典理论的等价性。

Abstract: This paper presents a comprehensive formalization of the von
Neumann-Morgenstern (vNM) expected utility theorem using the Lean 4 interactive
theorem prover. We implement the classical axioms of preference-completeness,
transitivity, continuity, and independence-enabling machine-verified proofs of
both the existence and uniqueness of utility representations. Our formalization
captures the mathematical structure of preference relations over lotteries,
verifying that preferences satisfying the vNM axioms can be represented by
expected utility maximization.
  Our contributions include a granular implementation of the independence
axiom, formally verified proofs of fundamental claims about mixture lotteries,
constructive demonstrations of utility existence, and computational experiments
validating the results. We prove equivalence to classical presentations while
offering greater precision at decision boundaries.
  This formalization provides a rigorous foundation for applications in
economic modeling, AI alignment, and management decision systems, bridging the
gap between theoretical decision theory and computational implementation.

</details>


<div id='math.NA'></div>

# math.NA [[Back]](#toc)

### [853] [Deep regularization networks for inverse problems with noisy operators](https://arxiv.org/abs/2506.07008)
*Fatemeh Pourahmadian,Yang Xu*

Main category: math.NA

TL;DR: 提出了一种监督学习方法，用于正则化大型逆问题，通过神经网络加速实时成像，并提升图像质量。


<details>
  <summary>Details</summary>
Motivation: 解决由噪声数据构建的逆问题在超分辨率成像中的正则化需求，加速时空正则化过程以实现实时成像。

Method: 采用两步训练神经网络：先基于低分辨率正则化图训练，再通过Tikhonov损失函数优化预测，自适应选择损失权重。

Result: 方法显著加速成像过程，并在复杂环境中提升图像质量，特别是基于差异逻辑的网络生成更高对比度图像。

Conclusion: 该方法无需先验知识，直接学习测试数据，适用于复杂环境中的高质量实时成像。

Abstract: A supervised learning approach is proposed for regularization of large
inverse problems where the main operator is built from noisy data. This is
germane to superresolution imaging via the sampling indicators of the inverse
scattering theory. We aim to accelerate the spatiotemporal regularization
process for this class of inverse problems to enable real-time imaging. In this
approach, a neural operator maps each pattern on the right-hand side of the
scattering equation to its affiliated regularization parameter. The network is
trained in two steps which entails: (1) training on low-resolution
regularization maps furnished by the Morozov discrepancy principle with
nonoptimal thresholds, and (2) optimizing network predictions through
minimization of the Tikhonov loss function regulated by the validation loss.
Step 2 allows for tailoring of the approximate maps of Step 1 toward
construction of higher quality images. This approach enables direct learning
from test data and dispenses with the need for a-priori knowledge of the
optimal regularization maps. The network, trained on low-resolution data,
quickly generates dense regularization maps for high-resolution imaging. We
highlight the importance of the training loss function on the network's
generalizability. In particular, we demonstrate that networks informed by the
logic of discrepancy principle lead to images of higher contrast. In this case,
the training process involves many-objective optimization. We propose a new
method to adaptively select the appropriate loss weights during training
without requiring an additional optimization process. The proposed approach is
synthetically examined for imaging damage evolution in an elastic plate. The
results indicate that the discrepancy-informed regularization networks not only
accelerate the imaging process, but also remarkably enhance the image quality
in complex environments.

</details>


<div id='cs.MS'></div>

# cs.MS [[Back]](#toc)

### [854] [El0ps: An Exact L0-regularized Problems Solver](https://arxiv.org/abs/2506.06373)
*Théo Guyard,Cédric Herzet,Clément Elvira*

Main category: cs.MS

TL;DR: El0ps是一个Python工具箱，用于处理L0正则化问题，提供灵活框架、高性能求解器和内置机器学习流程。


<details>
  <summary>Details</summary>
Motivation: 为机器学习和信号处理等领域提供更灵活、高效的L0正则化问题解决方案。

Method: 通过自定义问题实例框架、专用求解器和内置机器学习流程实现。

Result: El0ps在性能上达到先进水平，并为实际应用提供新视角。

Conclusion: El0ps是一个全面的工具，推动了L0正则化问题在实际中的应用。

Abstract: This paper presents El0ps, a Python toolbox providing several utilities to
handle L0-regularized problems related to applications in machine learning,
statistics, and signal processing, among other fields. In contrast to existing
toolboxes, El0ps allows users to define custom instances of these problems
through a flexible framework, provides a dedicated solver achieving
state-of-the-art performance, and offers several built-in machine learning
pipelines. Our aim with El0ps is to provide a comprehensive tool which opens
new perspectives for the integration of L0-regularized problems in practical
applications.

</details>


<div id='physics.chem-ph'></div>

# physics.chem-ph [[Back]](#toc)

### [855] [ChemGraph: An Agentic Framework for Computational Chemistry Workflows](https://arxiv.org/abs/2506.06363)
*Thang D. Pham,Aditya Tanikanti,Murat Keçeli*

Main category: physics.chem-ph

TL;DR: ChemGraph是一个基于AI的框架，用于自动化计算化学和材料科学工作流程，结合图神经网络和大型语言模型，简化复杂任务。


<details>
  <summary>Details</summary>
Motivation: 原子模拟在化学和材料科学中至关重要，但现有方法复杂且依赖专家知识，ChemGraph旨在解决这一问题。

Method: 利用图神经网络和大型语言模型（LLMs），提供自然语言界面，支持多种计算任务和方法。

Result: 在13个基准测试中，较小的LLMs在简单任务中表现良好，复杂任务需要更大模型，多代理框架可提升小模型性能。

Conclusion: ChemGraph通过AI和自动化显著提升了原子模拟的效率，多代理框架为小模型提供了优化空间。

Abstract: Atomistic simulations are essential tools in chemistry and materials science,
accelerating the discovery of novel catalysts, energy storage materials, and
pharmaceuticals. However, running these simulations remains challenging due to
the wide range of computational methods, diverse software ecosystems, and the
need for expert knowledge and manual effort for the setup, execution, and
validation stages. In this work, we present ChemGraph, an agentic framework
powered by artificial intelligence and state-of-the-art simulation tools to
streamline and automate computational chemistry and materials science
workflows. ChemGraph leverages graph neural network-based foundation models for
accurate yet computationally efficient calculations and large language models
(LLMs) for natural language understanding, task planning, and scientific
reasoning to provide an intuitive and interactive interface. Users can perform
tasks such as molecular structure generation, single-point energy, geometry
optimization, vibrational analysis, and thermochemistry calculations with
methods ranging from tight-binding and machine learning interatomic potentials
to density functional theory or wave function theory-based methods. We evaluate
ChemGraph across 13 benchmark tasks and demonstrate that smaller LLMs
(GPT-4o-mini, Claude-3.5-haiku, Qwen2.5-14B) perform well on simple workflows,
while more complex tasks benefit from using larger models like GPT-4o.
Importantly, we show that decomposing complex tasks into smaller subtasks
through a multi-agent framework enables smaller LLM models to match or exceed
GPT-4o's performance in specific scenarios.

</details>


<div id='math.OC'></div>

# math.OC [[Back]](#toc)

### [856] [Decentralized Optimization on Compact Submanifolds by Quantized Riemannian Gradient Tracking](https://arxiv.org/abs/2506.07351)
*Jun Chen,Lina Liu,Tianyi Zhu,Yong Liu,Guang Dai,Yunliang Jiang,Ivor W. Tsang*

Main category: math.OC

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: This paper considers the problem of decentralized optimization on compact
submanifolds, where a finite sum of smooth (possibly non-convex) local
functions is minimized by $n$ agents forming an undirected and connected graph.
However, the efficiency of distributed optimization is often hindered by
communication bottlenecks. To mitigate this, we propose the Quantized
Riemannian Gradient Tracking (Q-RGT) algorithm, where agents update their local
variables using quantized gradients. The introduction of quantization noise
allows our algorithm to bypass the constraints of the accurate Riemannian
projection operator (such as retraction), further improving iterative
efficiency. To the best of our knowledge, this is the first algorithm to
achieve an $\mathcal{O}(1/K)$ convergence rate in the presence of quantization,
matching the convergence rate of methods without quantization. Additionally, we
explicitly derive lower bounds on decentralized consensus associated with a
function of quantization levels. Numerical experiments demonstrate that Q-RGT
performs comparably to non-quantized methods while reducing communication
bottlenecks and computational overhead.

</details>


### [857] [Discrete and Continuous Difference of Submodular Minimization](https://arxiv.org/abs/2506.07952)
*George Orfanides,Tim Hoheisel,Marwa El Halabi*

Main category: math.OC

TL;DR: 论文研究了连续和离散域上两个子模函数差（DS）的最小化问题，扩展了之前仅限于集合函数的工作。提出了一种新的DC算法变体，并在实验中验证了其优越性。


<details>
  <summary>Details</summary>
Motivation: 子模函数在多个应用中具有重要意义，但之前的研究主要集中在集合函数上。本文旨在扩展这一研究到连续和离散域。

Method: 提出了一种新的DC算法变体（DCA），并将其应用于DS最小化问题，同时在连续域中通过离散化实现。

Result: 实验表明，该方法在整数压缩感知和整数最小二乘问题上优于基线方法。

Conclusion: 该研究成功地将DS最小化问题扩展到更广泛的领域，并提出了一种有效的算法，具有理论和实践意义。

Abstract: Submodular functions, defined on continuous or discrete domains, arise in
numerous applications. We study the minimization of the difference of two
submodular (DS) functions, over both domains, extending prior work restricted
to set functions. We show that all functions on discrete domains and all smooth
functions on continuous domains are DS. For discrete domains, we observe that
DS minimization is equivalent to minimizing the difference of two convex (DC)
functions, as in the set function case. We propose a novel variant of the DC
Algorithm (DCA) and apply it to the resulting DC Program, obtaining comparable
theoretical guarantees as in the set function case. The algorithm can be
applied to continuous domains via discretization. Experiments demonstrate that
our method outperforms baselines in integer compressive sensing and integer
least squares.

</details>
