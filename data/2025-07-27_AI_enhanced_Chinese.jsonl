{"id": "2507.17777", "pdf": "https://arxiv.org/pdf/2507.17777", "abs": "https://arxiv.org/abs/2507.17777", "authors": ["Theofanis Aravanis", "Grigorios Chrimatopoulos", "Mohammad Ferdows", "Michalis Xenos", "Efstratios Em Tzirtzilakis"], "title": "ASP-Assisted Symbolic Regression: Uncovering Hidden Physics in Fluid Mechanics", "categories": ["cs.AI", "76A02"], "comment": "This research was implemented in the framework of the Action\n  \"Flagship actions in interdisciplinary scientific fields with a special focus\n  on the productive fabric'', which is implemented through the National\n  Recovery and Resilience Fund Greece 2.0 and funded by the European\n  Union--NextGenerationEU (Project ID: TAEDR-0535983)", "summary": "Unlike conventional Machine-Learning (ML) approaches, often criticized as\n\"black boxes\", Symbolic Regression (SR) stands out as a powerful tool for\nrevealing interpretable mathematical relationships in complex physical systems,\nrequiring no a priori assumptions about models' structures. Motivated by the\nrecognition that, in fluid mechanics, an understanding of the underlying flow\nphysics is as crucial as accurate prediction, this study applies SR to model a\nfundamental three-dimensional (3D) incompressible flow in a rectangular\nchannel, focusing on the (axial) velocity and pressure fields under laminar\nconditions. By employing the PySR library, compact symbolic equations were\nderived directly from numerical simulation data, revealing key characteristics\nof the flow dynamics. These equations not only approximate the parabolic\nvelocity profile and pressure drop observed in the studied fluid flow, but also\nperfectly coincide with analytical solutions from the literature. Furthermore,\nwe propose an innovative approach that integrates SR with the\nknowledge-representation framework of Answer Set Programming (ASP), combining\nthe generative power of SR with the declarative reasoning strengths of ASP. The\nproposed hybrid SR/ASP framework ensures that the SR-generated symbolic\nexpressions are not only statistically accurate, but also physically plausible,\nadhering to domain-specific principles. Overall, the study highlights two key\ncontributions: SR's ability to simplify complex flow behaviours into concise,\ninterpretable equations, and the potential of knowledge-representation\napproaches to improve the reliability and alignment of data-driven SR models\nwith domain principles. Insights from the examined 3D channel flow pave the way\nfor integrating such hybrid approaches into efficient frameworks, [...] where\nexplainable predictions and real-time data analysis are crucial.", "AI": {"tldr": "\u8be5\u7814\u7a76\u5e94\u7528\u7b26\u53f7\u56de\u5f52\uff08SR\uff09\u548c\u7b54\u6848\u96c6\u7f16\u7a0b\uff08ASP\uff09\u7684\u6df7\u5408\u6846\u67b6\uff0c\u4ece\u6570\u503c\u6a21\u62df\u6570\u636e\u4e2d\u63a8\u5bfc\u51fa\u4e09\u7ef4\u4e0d\u53ef\u538b\u7f29\u6d41\u52a8\u7684\u89e3\u6790\u65b9\u7a0b\uff0c\u5c55\u793a\u4e86SR\u5728\u7b80\u5316\u590d\u6742\u6d41\u52a8\u884c\u4e3a\u548c\u77e5\u8bc6\u8868\u793a\u65b9\u6cd5\u63d0\u5347\u6a21\u578b\u53ef\u9760\u6027\u65b9\u9762\u7684\u6f5c\u529b\u3002", "motivation": "\u5728\u6d41\u4f53\u529b\u5b66\u4e2d\uff0c\u7406\u89e3\u6d41\u52a8\u7269\u7406\u673a\u5236\u4e0e\u51c6\u786e\u9884\u6d4b\u540c\u6837\u91cd\u8981\u3002SR\u4f5c\u4e3a\u4e00\u79cd\u53ef\u89e3\u91ca\u7684\u6570\u5b66\u5efa\u6a21\u5de5\u5177\uff0c\u65e0\u9700\u5148\u9a8c\u5047\u8bbe\u6a21\u578b\u7ed3\u6784\uff0c\u9002\u5408\u63ed\u793a\u590d\u6742\u7269\u7406\u7cfb\u7edf\u7684\u6570\u5b66\u5173\u7cfb\u3002", "method": "\u4f7f\u7528PySR\u5e93\u4ece\u6570\u503c\u6a21\u62df\u6570\u636e\u4e2d\u63a8\u5bfc\u7b26\u53f7\u65b9\u7a0b\uff0c\u5e76\u7ed3\u5408ASP\u6846\u67b6\u786e\u4fdd\u65b9\u7a0b\u4e0d\u4ec5\u7edf\u8ba1\u51c6\u786e\uff0c\u8fd8\u7b26\u5408\u7269\u7406\u539f\u7406\u3002", "result": "\u63a8\u5bfc\u7684\u65b9\u7a0b\u4e0d\u4ec5\u8fd1\u4f3c\u6a21\u62df\u4e86\u629b\u7269\u7ebf\u901f\u5ea6\u5206\u5e03\u548c\u538b\u529b\u964d\uff0c\u8fd8\u4e0e\u6587\u732e\u4e2d\u7684\u89e3\u6790\u89e3\u5b8c\u5168\u4e00\u81f4\u3002", "conclusion": "SR\u80fd\u7b80\u5316\u590d\u6742\u6d41\u52a8\u884c\u4e3a\u4e3a\u53ef\u89e3\u91ca\u65b9\u7a0b\uff0c\u77e5\u8bc6\u8868\u793a\u65b9\u6cd5\u53ef\u63d0\u5347\u6570\u636e\u9a71\u52a8\u6a21\u578b\u7684\u53ef\u9760\u6027\u548c\u4e0e\u9886\u57df\u539f\u7406\u7684\u4e00\u81f4\u6027\u3002"}}
{"id": "2507.17874", "pdf": "https://arxiv.org/pdf/2507.17874", "abs": "https://arxiv.org/abs/2507.17874", "authors": ["SaiBarath Sundar", "Pranav Satheesan", "Udayaadithya Avadhanam"], "title": "I2I-STRADA -- Information to Insights via Structured Reasoning Agent for Data Analysis", "categories": ["cs.AI"], "comment": null, "summary": "Recent advances in agentic systems for data analysis have emphasized\nautomation of insight generation through multi-agent frameworks, and\norchestration layers. While these systems effectively manage tasks like query\ntranslation, data transformation, and visualization, they often overlook the\nstructured reasoning process underlying analytical thinking. Reasoning large\nlanguage models (LLMs) used for multi-step problem solving are trained as\ngeneral-purpose problem solvers. As a result, their reasoning or thinking steps\ndo not adhere to fixed processes for specific tasks. Real-world data analysis\nrequires a consistent cognitive workflow: interpreting vague goals, grounding\nthem in contextual knowledge, constructing abstract plans, and adapting\nexecution based on intermediate outcomes. We introduce I2I-STRADA\n(Information-to-Insight via Structured Reasoning Agent for Data Analysis), an\nagentic architecture designed to formalize this reasoning process. I2I-STRADA\nfocuses on modeling how analysis unfolds via modular sub-tasks that reflect the\ncognitive steps of analytical reasoning. Evaluations on the DABstep and DABench\nbenchmarks show that I2I-STRADA outperforms prior systems in planning coherence\nand insight alignment, highlighting the importance of structured cognitive\nworkflows in agent design for data analysis.", "AI": {"tldr": "I2I-STRADA\u662f\u4e00\u79cd\u57fa\u4e8e\u7ed3\u6784\u5316\u63a8\u7406\u7684\u4ee3\u7406\u67b6\u6784\uff0c\u65e8\u5728\u901a\u8fc7\u6a21\u5757\u5316\u5b50\u4efb\u52a1\u6a21\u62df\u5206\u6790\u63a8\u7406\u7684\u8ba4\u77e5\u6b65\u9aa4\uff0c\u63d0\u5347\u6570\u636e\u5206\u6790\u7684\u89c4\u5212\u4e00\u81f4\u6027\u548c\u6d1e\u5bdf\u5bf9\u9f50\u3002", "motivation": "\u73b0\u6709\u4ee3\u7406\u7cfb\u7edf\u5728\u6570\u636e\u5206\u6790\u4e2d\u5ffd\u89c6\u4e86\u7ed3\u6784\u5316\u63a8\u7406\u8fc7\u7a0b\uff0c\u5bfc\u81f4\u901a\u7528LLMs\u7684\u63a8\u7406\u6b65\u9aa4\u7f3a\u4e4f\u4efb\u52a1\u7279\u5b9a\u7684\u56fa\u5b9a\u6d41\u7a0b\u3002", "method": "\u63d0\u51faI2I-STRADA\u67b6\u6784\uff0c\u901a\u8fc7\u6a21\u5757\u5316\u5b50\u4efb\u52a1\u6a21\u62df\u5206\u6790\u63a8\u7406\u7684\u8ba4\u77e5\u6b65\u9aa4\uff0c\u5305\u62ec\u76ee\u6807\u89e3\u91ca\u3001\u4e0a\u4e0b\u6587\u77e5\u8bc6\u3001\u62bd\u8c61\u8ba1\u5212\u6784\u5efa\u548c\u52a8\u6001\u6267\u884c\u3002", "result": "\u5728DABstep\u548cDABench\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cI2I-STRADA\u5728\u89c4\u5212\u4e00\u81f4\u6027\u548c\u6d1e\u5bdf\u5bf9\u9f50\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u7cfb\u7edf\u3002", "conclusion": "\u7ed3\u6784\u5316\u8ba4\u77e5\u5de5\u4f5c\u6d41\u5bf9\u6570\u636e\u5206\u6790\u4ee3\u7406\u8bbe\u8ba1\u81f3\u5173\u91cd\u8981\uff0cI2I-STRADA\u4e3a\u6b64\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.17927", "pdf": "https://arxiv.org/pdf/2507.17927", "abs": "https://arxiv.org/abs/2507.17927", "authors": ["Timothy Tin Long Yu", "Mahdi Mostajabdaveh", "Jabo Serge Byusa", "Rindra Ramamonjison", "Giuseppe Carenini", "Kun Mao", "Zirui Zhou", "Yong Zhang"], "title": "SMARTAPS: Tool-augmented LLMs for Operations Management", "categories": ["cs.AI"], "comment": "https://aaai.org/conference/aaai/aaai-25/bridge-ai-orms/", "summary": "Large language models (LLMs) present intriguing opportunities to enhance user\ninteraction with traditional algorithms and tools in real-world applications.\nAn advanced planning system (APS) is a sophisticated software that leverages\noptimization to help operations planners create, interpret, and modify an\noperational plan. While highly beneficial, many customers are priced out of\nusing an APS due to the ongoing costs of consultants responsible for\ncustomization and maintenance. To address the need for a more accessible APS\nexpressed by supply chain planners, we present SmartAPS, a conversational\nsystem built on a tool-augmented LLM. Our system provides operations planners\nwith an intuitive natural language chat interface, allowing them to query\ninformation, perform counterfactual reasoning, receive recommendations, and\nexecute scenario analysis to better manage their operation. A short video\ndemonstrating the system has been released: https://youtu.be/KtIrJjlDbyw", "AI": {"tldr": "SmartAPS\u662f\u4e00\u4e2a\u57fa\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u5bf9\u8bdd\u7cfb\u7edf\uff0c\u65e8\u5728\u4e3a\u4f9b\u5e94\u94fe\u89c4\u5212\u5e08\u63d0\u4f9b\u66f4\u6613\u8bbf\u95ee\u7684\u9ad8\u7ea7\u89c4\u5212\u7cfb\u7edf\uff08APS\uff09\uff0c\u901a\u8fc7\u81ea\u7136\u8bed\u8a00\u4ea4\u4e92\u5b9e\u73b0\u67e5\u8be2\u3001\u53cd\u4e8b\u5b9e\u63a8\u7406\u548c\u573a\u666f\u5206\u6790\u3002", "motivation": "\u4f20\u7edfAPS\u7cfb\u7edf\u56e0\u5b9a\u5236\u548c\u7ef4\u62a4\u6210\u672c\u9ad8\u6602\uff0c\u8bb8\u591a\u7528\u6237\u65e0\u6cd5\u8d1f\u62c5\u3002SmartAPS\u65e8\u5728\u901a\u8fc7LLM\u6280\u672f\u964d\u4f4e\u4f7f\u7528\u95e8\u69db\uff0c\u6ee1\u8db3\u4f9b\u5e94\u94fe\u89c4\u5212\u5e08\u7684\u9700\u6c42\u3002", "method": "SmartAPS\u57fa\u4e8e\u5de5\u5177\u589e\u5f3a\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff0c\u63d0\u4f9b\u81ea\u7136\u8bed\u8a00\u804a\u5929\u754c\u9762\uff0c\u652f\u6301\u67e5\u8be2\u3001\u53cd\u4e8b\u5b9e\u63a8\u7406\u3001\u63a8\u8350\u548c\u573a\u666f\u5206\u6790\u3002", "result": "\u7cfb\u7edf\u901a\u8fc7\u76f4\u89c2\u7684\u4ea4\u4e92\u65b9\u5f0f\u5e2e\u52a9\u7528\u6237\u7ba1\u7406\u8fd0\u8425\uff0c\u964d\u4f4e\u4e86APS\u7684\u4f7f\u7528\u95e8\u69db\u3002", "conclusion": "SmartAPS\u5c55\u793a\u4e86LLM\u5728\u63d0\u5347\u4f20\u7edf\u5de5\u5177\u53ef\u8bbf\u95ee\u6027\u548c\u7528\u6237\u4f53\u9a8c\u65b9\u9762\u7684\u6f5c\u529b\u3002"}}
{"id": "2507.17988", "pdf": "https://arxiv.org/pdf/2507.17988", "abs": "https://arxiv.org/abs/2507.17988", "authors": ["Dario Della Monica", "Angelo Montanari", "Pietro Sala"], "title": "Synthesis of timeline-based planning strategies avoiding determinization", "categories": ["cs.AI"], "comment": "arXiv admin note: text overlap with arXiv:2410.22757", "summary": "Qualitative timeline-based planning models domains as sets of independent,\nbut\n  interacting, components whose behaviors over time, the timelines, are\ngoverned\n  by sets of qualitative temporal constraints (ordering relations), called\n  synchronization rules.\n  Its plan-existence problem has been shown to be PSPACE-complete; in\n  particular, PSPACE-membership has been proved via reduction to the\n  nonemptiness problem for nondeterministic finite automata.\n  However, nondeterministic automata cannot be directly used to synthesize\n  planning strategies as a costly determinization step is needed.\n  In this paper, we identify a fragment of qualitative timeline-based planning\n  whose plan-existence problem can be directly mapped into the nonemptiness\n  problem of deterministic finite automata, which can then\n  synthesize strategies.\n  In addition, we identify a maximal subset of Allen's relations that fits into\n  such a deterministic fragment.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5b9a\u6027\u65f6\u95f4\u7ebf\u89c4\u5212\u7684\u7247\u6bb5\uff0c\u5176\u8ba1\u5212\u5b58\u5728\u6027\u95ee\u9898\u53ef\u76f4\u63a5\u6620\u5c04\u5230\u786e\u5b9a\u6027\u6709\u9650\u81ea\u52a8\u673a\u7684\u975e\u7a7a\u6027\u95ee\u9898\uff0c\u4ece\u800c\u80fd\u591f\u76f4\u63a5\u5408\u6210\u7b56\u7565\u3002", "motivation": "\u5b9a\u6027\u65f6\u95f4\u7ebf\u89c4\u5212\u7684\u8ba1\u5212\u5b58\u5728\u6027\u95ee\u9898\u5df2\u88ab\u8bc1\u660e\u662fPSPACE\u5b8c\u5168\u7684\uff0c\u4f46\u975e\u786e\u5b9a\u6027\u81ea\u52a8\u673a\u65e0\u6cd5\u76f4\u63a5\u7528\u4e8e\u5408\u6210\u7b56\u7565\uff0c\u9700\u8981\u6602\u8d35\u7684\u786e\u5b9a\u6027\u5316\u6b65\u9aa4\u3002", "method": "\u8bc6\u522b\u4e86\u4e00\u4e2a\u5b9a\u6027\u65f6\u95f4\u7ebf\u89c4\u5212\u7684\u7247\u6bb5\uff0c\u5e76\u5c06\u5176\u8ba1\u5212\u5b58\u5728\u6027\u95ee\u9898\u6620\u5c04\u5230\u786e\u5b9a\u6027\u6709\u9650\u81ea\u52a8\u673a\u7684\u975e\u7a7a\u6027\u95ee\u9898\u3002", "result": "\u786e\u5b9a\u4e86\u53ef\u4ee5\u6620\u5c04\u5230\u786e\u5b9a\u6027\u7247\u6bb5\u7684Allen\u5173\u7cfb\u7684\u6700\u5927\u5b50\u96c6\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u76f4\u63a5\u5408\u6210\u89c4\u5212\u7b56\u7565\u63d0\u4f9b\u4e86\u6709\u6548\u65b9\u6cd5\uff0c\u5e76\u6269\u5c55\u4e86\u786e\u5b9a\u6027\u7247\u6bb5\u7684\u5e94\u7528\u8303\u56f4\u3002"}}
{"id": "2507.18173", "pdf": "https://arxiv.org/pdf/2507.18173", "abs": "https://arxiv.org/abs/2507.18173", "authors": ["Haodong Zhu", "Wenhao Dong", "Linlin Yang", "Hong Li", "Yuguang Yang", "Yangyang Ren", "Qingcheng Zhu", "Zichao Feng", "Changbai Li", "Shaohui Lin", "Runqi Wang", "Xiaoyan Luo", "Baochang Zhang"], "title": "WaveMamba: Wavelet-Driven Mamba Fusion for RGB-Infrared Object Detection", "categories": ["cs.CV", "cs.MM"], "comment": null, "summary": "Leveraging the complementary characteristics of visible (RGB) and infrared\n(IR) imagery offers significant potential for improving object detection. In\nthis paper, we propose WaveMamba, a cross-modality fusion method that\nefficiently integrates the unique and complementary frequency features of RGB\nand IR decomposed by Discrete Wavelet Transform (DWT). An improved detection\nhead incorporating the Inverse Discrete Wavelet Transform (IDWT) is also\nproposed to reduce information loss and produce the final detection results.\nThe core of our approach is the introduction of WaveMamba Fusion Block (WMFB),\nwhich facilitates comprehensive fusion across low-/high-frequency sub-bands.\nWithin WMFB, the Low-frequency Mamba Fusion Block (LMFB), built upon the Mamba\nframework, first performs initial low-frequency feature fusion with channel\nswapping, followed by deep fusion with an advanced gated attention mechanism\nfor enhanced integration. High-frequency features are enhanced using a strategy\nthat applies an ``absolute maximum\" fusion approach. These advancements lead to\nsignificant performance gains, with our method surpassing state-of-the-art\napproaches and achieving average mAP improvements of 4.5% on four benchmarks.", "AI": {"tldr": "WaveMamba\u662f\u4e00\u79cd\u8de8\u6a21\u6001\u878d\u5408\u65b9\u6cd5\uff0c\u901a\u8fc7\u79bb\u6563\u5c0f\u6ce2\u53d8\u6362\uff08DWT\uff09\u5206\u89e3RGB\u548c\u7ea2\u5916\uff08IR\uff09\u56fe\u50cf\u7684\u4e92\u8865\u9891\u7387\u7279\u5f81\uff0c\u5e76\u7ed3\u5408\u6539\u8fdb\u7684\u68c0\u6d4b\u5934\u51cf\u5c11\u4fe1\u606f\u635f\u5931\uff0c\u663e\u8457\u63d0\u5347\u76ee\u6807\u68c0\u6d4b\u6027\u80fd\u3002", "motivation": "\u5229\u7528RGB\u548c\u7ea2\u5916\u56fe\u50cf\u7684\u4e92\u8865\u7279\u6027\u63d0\u5347\u76ee\u6807\u68c0\u6d4b\u6027\u80fd\u3002", "method": "\u63d0\u51faWaveMamba Fusion Block\uff08WMFB\uff09\uff0c\u7ed3\u5408\u4f4e/\u9ad8\u9891\u5b50\u5e26\u7684\u878d\u5408\u7b56\u7565\uff0c\u5305\u62ec\u57fa\u4e8eMamba\u6846\u67b6\u7684\u4f4e\u9891\u7279\u5f81\u878d\u5408\u548c\u9ad8\u9891\u7279\u5f81\u589e\u5f3a\u3002", "result": "\u5728\u56db\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5e73\u5747mAP\u63d0\u53474.5%\uff0c\u8d85\u8d8a\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "WaveMamba\u901a\u8fc7\u9ad8\u6548\u878d\u5408RGB\u548cIR\u56fe\u50cf\u7684\u9891\u7387\u7279\u5f81\uff0c\u663e\u8457\u63d0\u5347\u4e86\u76ee\u6807\u68c0\u6d4b\u6027\u80fd\u3002"}}
{"id": "2507.17846", "pdf": "https://arxiv.org/pdf/2507.17846", "abs": "https://arxiv.org/abs/2507.17846", "authors": ["Alison Bartsch", "Arvind Car", "Amir Barati Farimani"], "title": "PinchBot: Long-Horizon Deformable Manipulation with Guided Diffusion Policy", "categories": ["cs.RO"], "comment": null, "summary": "Pottery creation is a complicated art form that requires dexterous, precise\nand delicate actions to slowly morph a block of clay to a meaningful, and often\nuseful 3D goal shape. In this work, we aim to create a robotic system that can\ncreate simple pottery goals with only pinch-based actions. This pinch pottery\ntask allows us to explore the challenges of a highly multi-modal and\nlong-horizon deformable manipulation task. To this end, we present PinchBot, a\ngoal-conditioned diffusion policy model that when combined with pre-trained 3D\npoint cloud embeddings, task progress prediction and collision-constrained\naction projection, is able to successfully create a variety of simple pottery\ngoals. For experimental videos and access to the demonstration dataset, please\nvisit our project website:\nhttps://sites.google.com/andrew.cmu.edu/pinchbot/home.", "AI": {"tldr": "\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aPinchBot\u7684\u673a\u5668\u4eba\u7cfb\u7edf\uff0c\u901a\u8fc7\u57fa\u4e8e\u634f\u5408\u7684\u52a8\u4f5c\u5b9e\u73b0\u7b80\u5355\u9676\u827a\u5236\u4f5c\uff0c\u7ed3\u5408\u6269\u6563\u7b56\u7565\u6a21\u578b\u548c\u9884\u8bad\u7ec3\u76843D\u70b9\u4e91\u5d4c\u5165\u6280\u672f\u3002", "motivation": "\u63a2\u7d22\u9ad8\u5ea6\u591a\u6a21\u6001\u548c\u957f\u65f6\u7a0b\u7684\u53ef\u53d8\u5f62\u7269\u4f53\u64cd\u63a7\u6311\u6218\uff0c\u5c24\u5176\u662f\u9676\u827a\u5236\u4f5c\u4e2d\u7684\u7cbe\u786e\u52a8\u4f5c\u9700\u6c42\u3002", "method": "\u91c7\u7528\u76ee\u6807\u6761\u4ef6\u6269\u6563\u7b56\u7565\u6a21\u578b\uff0c\u7ed3\u5408\u9884\u8bad\u7ec3\u76843D\u70b9\u4e91\u5d4c\u5165\u3001\u4efb\u52a1\u8fdb\u5ea6\u9884\u6d4b\u548c\u78b0\u649e\u7ea6\u675f\u52a8\u4f5c\u6295\u5f71\u3002", "result": "PinchBot\u80fd\u591f\u6210\u529f\u5236\u4f5c\u591a\u79cd\u7b80\u5355\u9676\u827a\u76ee\u6807\u3002", "conclusion": "\u8be5\u7cfb\u7edf\u4e3a\u590d\u6742\u53ef\u53d8\u5f62\u7269\u4f53\u64cd\u63a7\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.17851", "pdf": "https://arxiv.org/pdf/2507.17851", "abs": "https://arxiv.org/abs/2507.17851", "authors": ["Xiaoxu Zhu", "Junhua Li"], "title": "Speaker Disentanglement of Speech Pre-trained Model Based on Interpretability", "categories": ["cs.SD", "eess.AS"], "comment": "20 pages, 9 figures, 2 tables", "summary": "Speech pretrained models contain task-specific information across different\nlayers, but decoupling content and timbre information remains challenging as\nremoving speaker-specific information often causes content loss. Current\nresearch lacks direct metrics to quantify timbre residual in model encodings,\nrelying on indirect evaluation through downstream tasks. This paper addresses\nthese challenges through interpretability-based speaker disentanglement in\nspeech pretraining models. We quantitatively evaluate timbre residual in model\nembeddings and improve speaker disentanglement using interpretive\nrepresentations. Our contributions include: (1) InterpTRQE-SptME Benchmark - a\ntimbre residual recognition framework using interpretability. The benchmark\nconcatenates content embeddings with timbre embeddings for speaker\nclassification, then applies Gradient SHAP Explainer to quantify timbre\nresidual. We evaluate seven speech pretraining model variations. (2)\nInterpTF-SptME method - an interpretability-based timbre filtering approach\nusing SHAP Noise and SHAP Cropping techniques. This model-agnostic method\ntransforms intermediate encodings to remove timbre while preserving content.\nExperiments on VCTK dataset with HuBERT LARGE demonstrate successful content\npreservation and significant speaker disentanglement optimization. Results show\nthe SHAP Noise method can reduce timbre residual from 18.05% to near 0% while\nmaintaining content integrity, contributing to enhanced performance in\ncontent-related speech processing tasks and preventing timbre privacy leakage.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u53ef\u89e3\u91ca\u6027\u7684\u8bed\u97f3\u9884\u8bad\u7ec3\u6a21\u578b\u4e2d\u7684\u97f3\u8272\u89e3\u8026\u65b9\u6cd5\uff0c\u901a\u8fc7\u91cf\u5316\u97f3\u8272\u6b8b\u7559\u548c\u6539\u8fdb\u97f3\u8272\u8fc7\u6ee4\uff0c\u5b9e\u73b0\u4e86\u97f3\u8272\u4e0e\u5185\u5bb9\u7684\u6709\u6548\u5206\u79bb\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u76f4\u63a5\u91cf\u5316\u97f3\u8272\u6b8b\u7559\uff0c\u4e14\u53bb\u9664\u97f3\u8272\u4fe1\u606f\u65f6\u6613\u5bfc\u81f4\u5185\u5bb9\u635f\u5931\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u66f4\u76f4\u63a5\u4e14\u6709\u6548\u7684\u65b9\u6cd5\u3002", "method": "\u63d0\u51faInterpTRQE-SptME\u57fa\u51c6\u548cInterpTF-SptME\u65b9\u6cd5\uff0c\u5229\u7528SHAP\u6280\u672f\u91cf\u5316\u97f3\u8272\u6b8b\u7559\u5e76\u8fc7\u6ee4\u97f3\u8272\u4fe1\u606f\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cSHAP Noise\u65b9\u6cd5\u53ef\u5c06\u97f3\u8272\u6b8b\u7559\u4ece18.05%\u964d\u81f3\u63a5\u8fd10%\uff0c\u540c\u65f6\u4fdd\u6301\u5185\u5bb9\u5b8c\u6574\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u663e\u8457\u4f18\u5316\u4e86\u97f3\u8272\u89e3\u8026\uff0c\u63d0\u5347\u4e86\u5185\u5bb9\u76f8\u5173\u4efb\u52a1\u7684\u6027\u80fd\uff0c\u5e76\u9632\u6b62\u97f3\u8272\u9690\u79c1\u6cc4\u9732\u3002"}}
{"id": "2507.17852", "pdf": "https://arxiv.org/pdf/2507.17852", "abs": "https://arxiv.org/abs/2507.17852", "authors": ["Yao Fehlis", "Charles Crain", "Aidan Jensen", "Michael Watson", "James Juhasz", "Paul Mandel", "Betty Liu", "Shawn Mahon", "Daren Wilson", "Nick Lynch-Jonely", "Ben Leedom", "David Fuller"], "title": "Technical Implementation of Tippy: Multi-Agent Architecture and System Design for Drug Discovery Laboratory Automation", "categories": ["cs.MA", "cs.AI"], "comment": null, "summary": "Building on the conceptual framework presented in our previous work on\nagentic AI for pharmaceutical research, this paper provides a comprehensive\ntechnical analysis of Tippy's multi-agent system implementation for drug\ndiscovery laboratory automation. We present a distributed microservices\narchitecture featuring five specialized agents (Supervisor, Molecule, Lab,\nAnalysis, and Report) that coordinate through OpenAI Agents SDK orchestration\nand access laboratory tools via the Model Context Protocol (MCP). The system\narchitecture encompasses agent-specific tool integration, asynchronous\ncommunication patterns, and comprehensive configuration management through\nGit-based tracking. Our production deployment strategy utilizes Kubernetes\ncontainer orchestration with Helm charts, Docker containerization, and CI/CD\npipelines for automated testing and deployment. The implementation integrates\nvector databases for RAG functionality and employs an Envoy reverse proxy for\nsecure external access. This work demonstrates how specialized AI agents can\neffectively coordinate complex laboratory workflows while maintaining security,\nscalability, reliability, and integration with existing laboratory\ninfrastructure through standardized protocols.", "AI": {"tldr": "\u672c\u6587\u8be6\u7ec6\u4ecb\u7ecd\u4e86Tippy\u7684\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u5728\u836f\u7269\u53d1\u73b0\u5b9e\u9a8c\u5ba4\u81ea\u52a8\u5316\u4e2d\u7684\u5b9e\u73b0\uff0c\u91c7\u7528\u5206\u5e03\u5f0f\u5fae\u670d\u52a1\u67b6\u6784\u548c\u6807\u51c6\u5316\u534f\u8bae\uff0c\u5c55\u793a\u4e86AI\u667a\u80fd\u4f53\u5982\u4f55\u9ad8\u6548\u534f\u8c03\u590d\u6742\u5de5\u4f5c\u6d41\u7a0b\u3002", "motivation": "\u57fa\u4e8e\u5148\u524d\u5173\u4e8e\u836f\u7269\u7814\u7a76\u4e2d\u4ee3\u7406AI\u7684\u5de5\u4f5c\uff0c\u672c\u6587\u65e8\u5728\u901a\u8fc7\u6280\u672f\u5206\u6790\u5c55\u793a\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u5728\u5b9e\u9a8c\u5ba4\u81ea\u52a8\u5316\u4e2d\u7684\u5b9e\u9645\u5e94\u7528\u548c\u4f18\u52bf\u3002", "method": "\u91c7\u7528\u5206\u5e03\u5f0f\u5fae\u670d\u52a1\u67b6\u6784\uff0c\u5305\u542b\u4e94\u4e2a\u4e13\u4e1a\u667a\u80fd\u4f53\uff0c\u901a\u8fc7OpenAI Agents SDK\u534f\u8c03\uff0c\u4f7f\u7528Model Context Protocol\uff08MCP\uff09\u8bbf\u95ee\u5b9e\u9a8c\u5ba4\u5de5\u5177\uff0c\u5e76\u96c6\u6210Git\u914d\u7f6e\u7ba1\u7406\u3001Kubernetes\u90e8\u7f72\u3001CI/CD\u7ba1\u9053\u7b49\u6280\u672f\u3002", "result": "\u7cfb\u7edf\u6210\u529f\u5b9e\u73b0\u4e86\u590d\u6742\u5b9e\u9a8c\u5ba4\u5de5\u4f5c\u6d41\u7684\u534f\u8c03\uff0c\u5177\u5907\u5b89\u5168\u6027\u3001\u53ef\u6269\u5c55\u6027\u3001\u53ef\u9760\u6027\u548c\u4e0e\u73b0\u6709\u57fa\u7840\u8bbe\u65bd\u7684\u96c6\u6210\u80fd\u529b\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\uff0c\u4e13\u4e1a\u5316\u7684AI\u667a\u80fd\u4f53\u80fd\u591f\u901a\u8fc7\u6807\u51c6\u5316\u534f\u8bae\u6709\u6548\u534f\u8c03\u590d\u6742\u7684\u5b9e\u9a8c\u5ba4\u5de5\u4f5c\u6d41\u7a0b\uff0c\u540c\u65f6\u6ee1\u8db3\u5b89\u5168\u6027\u548c\u53ef\u6269\u5c55\u6027\u9700\u6c42\u3002"}}
{"id": "2507.17768", "pdf": "https://arxiv.org/pdf/2507.17768", "abs": "https://arxiv.org/abs/2507.17768", "authors": ["Yujia Tong", "Jingling Yuan", "Chuang Hu"], "title": "Enhancing Quantization-Aware Training on Edge Devices via Relative Entropy Coreset Selection and Cascaded Layer Correction", "categories": ["cs.LG", "cs.CV"], "comment": null, "summary": "With the development of mobile and edge computing, the demand for low-bit\nquantized models on edge devices is increasing to achieve efficient deployment.\nTo enhance the performance, it is often necessary to retrain the quantized\nmodels using edge data. However, due to privacy concerns, certain sensitive\ndata can only be processed on edge devices. Therefore, employing\nQuantization-Aware Training (QAT) on edge devices has become an effective\nsolution. Nevertheless, traditional QAT relies on the complete dataset for\ntraining, which incurs a huge computational cost. Coreset selection techniques\ncan mitigate this issue by training on the most representative subsets.\nHowever, existing methods struggle to eliminate quantization errors in the\nmodel when using small-scale datasets (e.g., only 10% of the data), leading to\nsignificant performance degradation. To address these issues, we propose QuaRC,\na QAT framework with coresets on edge devices, which consists of two main\nphases: In the coreset selection phase, QuaRC introduces the ``Relative Entropy\nScore\" to identify the subsets that most effectively capture the model's\nquantization errors. During the training phase, QuaRC employs the Cascaded\nLayer Correction strategy to align the intermediate layer outputs of the\nquantized model with those of the full-precision model, thereby effectively\nreducing the quantization errors in the intermediate layers. Experimental\nresults demonstrate the effectiveness of our approach. For instance, when\nquantizing ResNet-18 to 2-bit using a 1% data subset, QuaRC achieves a 5.72%\nimprovement in Top-1 accuracy on the ImageNet-1K dataset compared to\nstate-of-the-art techniques.", "AI": {"tldr": "QuaRC\u662f\u4e00\u4e2a\u57fa\u4e8e\u8fb9\u7f18\u8bbe\u5907\u7684\u91cf\u5316\u611f\u77e5\u8bad\u7ec3\u6846\u67b6\uff0c\u901a\u8fc7\u6838\u5fc3\u96c6\u9009\u62e9\u548c\u5c42\u6821\u6b63\u7b56\u7565\u51cf\u5c11\u91cf\u5316\u8bef\u5dee\uff0c\u663e\u8457\u63d0\u5347\u4f4e\u6bd4\u7279\u91cf\u5316\u6a21\u578b\u7684\u6027\u80fd\u3002", "motivation": "\u8fb9\u7f18\u8bbe\u5907\u4e0a\u4f4e\u6bd4\u7279\u91cf\u5316\u6a21\u578b\u7684\u9700\u6c42\u589e\u52a0\uff0c\u4f46\u4f20\u7edf\u91cf\u5316\u611f\u77e5\u8bad\u7ec3\u4f9d\u8d56\u5b8c\u6574\u6570\u636e\u96c6\uff0c\u8ba1\u7b97\u6210\u672c\u9ad8\u4e14\u9690\u79c1\u53d7\u9650\u3002\u6838\u5fc3\u96c6\u9009\u62e9\u6280\u672f\u53ef\u7f13\u89e3\u95ee\u9898\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u5728\u5c0f\u89c4\u6a21\u6570\u636e\u96c6\u4e0a\u96be\u4ee5\u6d88\u9664\u91cf\u5316\u8bef\u5dee\u3002", "method": "QuaRC\u5f15\u5165\u201c\u76f8\u5bf9\u71b5\u8bc4\u5206\u201d\u9009\u62e9\u6838\u5fc3\u96c6\uff0c\u5e76\u91c7\u7528\u201c\u7ea7\u8054\u5c42\u6821\u6b63\u201d\u7b56\u7565\u5bf9\u9f50\u91cf\u5316\u6a21\u578b\u4e0e\u5168\u7cbe\u5ea6\u6a21\u578b\u7684\u4e2d\u95f4\u5c42\u8f93\u51fa\u3002", "result": "\u5728ImageNet-1K\u6570\u636e\u96c6\u4e0a\uff0cQuaRC\u4f7f\u75281%\u6570\u636e\u5b50\u96c6\u91cf\u5316ResNet-18\u81f32\u6bd4\u7279\u65f6\uff0cTop-1\u51c6\u786e\u7387\u6bd4\u73b0\u6709\u6280\u672f\u63d0\u53475.72%\u3002", "conclusion": "QuaRC\u901a\u8fc7\u6838\u5fc3\u96c6\u9009\u62e9\u548c\u5c42\u6821\u6b63\u6709\u6548\u51cf\u5c11\u91cf\u5316\u8bef\u5dee\uff0c\u663e\u8457\u63d0\u5347\u8fb9\u7f18\u8bbe\u5907\u4e0a\u4f4e\u6bd4\u7279\u91cf\u5316\u6a21\u578b\u7684\u6027\u80fd\u3002"}}
{"id": "2507.17842", "pdf": "https://arxiv.org/pdf/2507.17842", "abs": "https://arxiv.org/abs/2507.17842", "authors": ["Yimeng Zhang", "Tian Wang", "Jiri Gesi", "Ziyi Wang", "Yuxuan Lu", "Jiacheng Lin", "Sinong Zhan", "Vianne Gao", "Ruochen Jiao", "Junze Liu", "Kun Qian", "Yuxin Tang", "Ran Xue", "Houyu Zhang", "Qingjun Cui", "Yufan Guo", "Dakuo Wang"], "title": "Shop-R1: Rewarding LLMs to Simulate Human Behavior in Online Shopping via Reinforcement Learning", "categories": ["cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) have recently demonstrated strong potential in\ngenerating 'believable human-like' behavior in web environments. Prior work has\nexplored augmenting training data with LLM-synthesized rationales and applying\nsupervised fine-tuning (SFT) to enhance reasoning ability, which in turn can\nimprove downstream action prediction. However, the performance of such\napproaches remains inherently bounded by the reasoning capabilities of the\nmodel used to generate the rationales. In this paper, we introduce Shop-R1, a\nnovel reinforcement learning (RL) framework aimed at enhancing the reasoning\nability of LLMs for simulation of real human behavior in online shopping\nenvironments Specifically, Shop-R1 decomposes the human behavior simulation\ntask into two stages: rationale generation and action prediction, each guided\nby distinct reward signals. For rationale generation, we leverage internal\nmodel signals (e.g., logit distributions) to guide the reasoning process in a\nself-supervised manner. For action prediction, we propose a hierarchical reward\nstructure with difficulty-aware scaling to prevent reward hacking and enable\nfine-grained reward assignment. This design evaluates both high-level action\ntypes and the correctness of fine-grained sub-action details (attributes and\nvalues), rewarding outputs proportionally to their difficulty. Experimental\nresults show that our method achieves a relative improvement of over 65%\ncompared to the baseline.", "AI": {"tldr": "Shop-R1\u662f\u4e00\u4e2a\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u5206\u9636\u6bb5\u751f\u6210\u7406\u7531\u548c\u9884\u6d4b\u52a8\u4f5c\uff0c\u63d0\u5347LLM\u5728\u5728\u7ebf\u8d2d\u7269\u73af\u5883\u4e2d\u6a21\u62df\u4eba\u7c7b\u884c\u4e3a\u7684\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56LLM\u751f\u6210\u7684\u7406\u7531\u8fdb\u884c\u8bad\u7ec3\uff0c\u4f46\u5176\u6027\u80fd\u53d7\u9650\u4e8e\u6a21\u578b\u7684\u63a8\u7406\u80fd\u529b\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u65b0\u65b9\u6cd5\u6765\u589e\u5f3a\u63a8\u7406\u80fd\u529b\u3002", "method": "Shop-R1\u5c06\u4efb\u52a1\u5206\u89e3\u4e3a\u7406\u7531\u751f\u6210\u548c\u52a8\u4f5c\u9884\u6d4b\u4e24\u9636\u6bb5\uff0c\u5206\u522b\u4f7f\u7528\u81ea\u76d1\u7763\u548c\u5206\u5c42\u5956\u52b1\u7ed3\u6784\u8fdb\u884c\u4f18\u5316\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c\u8be5\u65b9\u6cd5\u76f8\u6bd4\u57fa\u7ebf\u63d0\u5347\u4e8665%\u7684\u6027\u80fd\u3002", "conclusion": "Shop-R1\u901a\u8fc7\u5206\u9636\u6bb5\u548c\u5206\u5c42\u5956\u52b1\u8bbe\u8ba1\uff0c\u6709\u6548\u63d0\u5347\u4e86LLM\u5728\u6a21\u62df\u4eba\u7c7b\u884c\u4e3a\u4e2d\u7684\u63a8\u7406\u80fd\u529b\u3002"}}
{"id": "2507.17801", "pdf": "https://arxiv.org/pdf/2507.17801", "abs": "https://arxiv.org/abs/2507.17801", "authors": ["Yi Xin", "Juncheng Yan", "Qi Qin", "Zhen Li", "Dongyang Liu", "Shicheng Li", "Victor Shea-Jay Huang", "Yupeng Zhou", "Renrui Zhang", "Le Zhuo", "Tiancheng Han", "Xiaoqing Sun", "Siqi Luo", "Mengmeng Wang", "Bin Fu", "Yuewen Cao", "Hongsheng Li", "Guangtao Zhai", "Xiaohong Liu", "Yu Qiao", "Peng Gao"], "title": "Lumina-mGPT 2.0: Stand-Alone AutoRegressive Image Modeling", "categories": ["cs.CV"], "comment": "Tech Report, 23 pages, 11 figures, 7 tables", "summary": "We present Lumina-mGPT 2.0, a stand-alone, decoder-only autoregressive model\nthat revisits and revitalizes the autoregressive paradigm for high-quality\nimage generation and beyond. Unlike existing approaches that rely on pretrained\ncomponents or hybrid architectures, Lumina-mGPT 2.0 is trained entirely from\nscratch, enabling unrestricted architectural design and licensing freedom. It\nachieves generation quality on par with state-of-the-art diffusion models such\nas DALL-E 3 and SANA, while preserving the inherent flexibility and\ncompositionality of autoregressive modeling. Our unified tokenization scheme\nallows the model to seamlessly handle a wide spectrum of tasks-including\nsubject-driven generation, image editing, controllable synthesis, and dense\nprediction-within a single generative framework. To further boost usability, we\nincorporate efficient decoding strategies like inference-time scaling and\nspeculative Jacobi sampling to improve quality and speed, respectively.\nExtensive evaluations on standard text-to-image benchmarks (e.g., GenEval, DPG)\ndemonstrate that Lumina-mGPT 2.0 not only matches but in some cases surpasses\ndiffusion-based models. Moreover, we confirm its multi-task capabilities on the\nGraph200K benchmark, with the native Lumina-mGPT 2.0 performing exceptionally\nwell. These results position Lumina-mGPT 2.0 as a strong, flexible foundation\nmodel for unified multimodal generation. We have released our training details,\ncode, and models at https://github.com/Alpha-VLLM/Lumina-mGPT-2.0.", "AI": {"tldr": "Lumina-mGPT 2.0\u662f\u4e00\u4e2a\u72ec\u7acb\u7684\u81ea\u56de\u5f52\u6a21\u578b\uff0c\u901a\u8fc7\u4ece\u5934\u8bad\u7ec3\u5b9e\u73b0\u9ad8\u8d28\u91cf\u56fe\u50cf\u751f\u6210\uff0c\u6027\u80fd\u5ab2\u7f8e\u6269\u6563\u6a21\u578b\uff0c\u5e76\u652f\u6301\u591a\u4efb\u52a1\u5904\u7406\u3002", "motivation": "\u91cd\u65b0\u5ba1\u89c6\u5e76\u632f\u5174\u81ea\u56de\u5f52\u8303\u5f0f\uff0c\u4ee5\u5b9e\u73b0\u9ad8\u8d28\u91cf\u56fe\u50cf\u751f\u6210\u548c\u591a\u4efb\u52a1\u5904\u7406\uff0c\u540c\u65f6\u907f\u514d\u4f9d\u8d56\u9884\u8bad\u7ec3\u7ec4\u4ef6\u6216\u6df7\u5408\u67b6\u6784\u3002", "method": "\u91c7\u7528\u4ece\u5934\u8bad\u7ec3\u7684\u81ea\u56de\u5f52\u6a21\u578b\uff0c\u7ed3\u5408\u7edf\u4e00\u7684\u6807\u8bb0\u5316\u65b9\u6848\u548c\u9ad8\u6548\u89e3\u7801\u7b56\u7565\uff08\u5982\u63a8\u7406\u65f6\u95f4\u7f29\u653e\u548c\u63a8\u6d4b\u6027Jacobi\u91c7\u6837\uff09\u3002", "result": "\u5728\u6587\u672c\u5230\u56fe\u50cf\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u751a\u81f3\u8d85\u8d8a\u6269\u6563\u6a21\u578b\uff0c\u5e76\u5728\u591a\u4efb\u52a1\u57fa\u51c6Graph200K\u4e0a\u8868\u73b0\u7a81\u51fa\u3002", "conclusion": "Lumina-mGPT 2.0\u662f\u4e00\u4e2a\u5f3a\u5927\u4e14\u7075\u6d3b\u7684\u591a\u6a21\u6001\u751f\u6210\u57fa\u7840\u6a21\u578b\uff0c\u5177\u6709\u5e7f\u6cdb\u7684\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2507.18004", "pdf": "https://arxiv.org/pdf/2507.18004", "abs": "https://arxiv.org/abs/2507.18004", "authors": ["Yusen Peng", "Shuhua Mao"], "title": "E.A.R.T.H.: Structuring Creative Evolution through Model Error in Generative AI", "categories": ["cs.AI"], "comment": "44 pages,11 figures", "summary": "How can AI move beyond imitation toward genuine creativity? This paper\nproposes the E.A.R.T.H. framework, a five-stage generative pipeline that\ntransforms model-generated errors into creative assets through Error\ngeneration, Amplification, Refine selection, Transform, and Harness feedback.\nDrawing on cognitive science and generative modeling, we posit that \"creative\npotential hides in failure\" and operationalize this via structured prompts,\nsemantic scoring, and human-in-the-loop evaluation. Implemented using\nLLaMA-2-7B-Chat, SBERT, BERTScore, CLIP, BLIP-2, and Stable Diffusion, the\npipeline employs a composite reward function based on novelty, surprise, and\nrelevance. At the Refine stage, creativity scores increase by 52.5% (1.179 to\n1.898, t = -5.56, p < 0.001), with final outputs reaching 2.010 - a 70.4%\nimprovement. Refined slogans are 48.4% shorter, 40.7% more novel, with only a\n4.0% drop in relevance. Cross-modal tests show strong slogan-to-image alignment\n(CLIPScore: 0.249; BERTScore F1: 0.816). In human evaluations, 60% of outputs\nscored >= 4.0, with metaphorical slogans (avg. 4.09) outperforming literal ones\n(3.99). Feedback highlights stylistic precision and emotional resonance. These\nresults demonstrate that error-centered, feedback-driven generation enhances\ncreativity, offering a scalable path toward self-evolving, human-aligned\ncreative AI.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faE.A.R.T.H.\u6846\u67b6\uff0c\u901a\u8fc7\u9519\u8bef\u751f\u6210\u3001\u653e\u5927\u3001\u7cbe\u70bc\u9009\u62e9\u3001\u8f6c\u6362\u548c\u53cd\u9988\u5229\u7528\uff0c\u5c06AI\u751f\u6210\u7684\u9519\u8bef\u8f6c\u5316\u4e3a\u521b\u610f\u8d44\u4ea7\uff0c\u663e\u8457\u63d0\u5347\u521b\u9020\u529b\u3002", "motivation": "\u63a2\u7d22AI\u5982\u4f55\u8d85\u8d8a\u6a21\u4eff\u5b9e\u73b0\u771f\u6b63\u7684\u521b\u9020\u529b\uff0c\u63d0\u51fa\u201c\u521b\u610f\u6f5c\u529b\u9690\u85cf\u5728\u5931\u8d25\u4e2d\u201d\u7684\u89c2\u70b9\u3002", "method": "\u91c7\u7528\u4e94\u9636\u6bb5\u751f\u6210\u7ba1\u9053\uff08E.A.R.T.H.\uff09\uff0c\u7ed3\u5408\u7ed3\u6784\u5316\u63d0\u793a\u3001\u8bed\u4e49\u8bc4\u5206\u548c\u4eba\u7c7b\u53cd\u9988\uff0c\u4f7f\u7528\u591a\u79cd\u6a21\u578b\uff08\u5982LLaMA-2-7B-Chat\u3001SBERT\u7b49\uff09\u548c\u590d\u5408\u5956\u52b1\u51fd\u6570\uff08\u65b0\u9896\u6027\u3001\u60ca\u559c\u6027\u3001\u76f8\u5173\u6027\uff09\u3002", "result": "\u521b\u9020\u529b\u8bc4\u5206\u63d0\u534770.4%\uff0c\u53e3\u53f7\u66f4\u77ed\u3001\u66f4\u65b0\u9896\uff0c\u8de8\u6a21\u6001\u6d4b\u8bd5\u663e\u793a\u5f3a\u5bf9\u9f50\uff0c\u4eba\u7c7b\u8bc4\u4ef7\u4e2d60%\u8f93\u51fa\u5f97\u5206\u22654.0\u3002", "conclusion": "\u9519\u8bef\u4e2d\u5fc3\u548c\u53cd\u9988\u9a71\u52a8\u7684\u751f\u6210\u65b9\u6cd5\u663e\u8457\u589e\u5f3a\u521b\u9020\u529b\uff0c\u4e3a\u81ea\u8fdb\u5316\u3001\u4eba\u7c7b\u5bf9\u9f50\u7684\u521b\u610fAI\u63d0\u4f9b\u53ef\u884c\u8def\u5f84\u3002"}}
{"id": "2507.18352", "pdf": "https://arxiv.org/pdf/2507.18352", "abs": "https://arxiv.org/abs/2507.18352", "authors": ["Zhen Han", "Mattias Teye", "Derek Yadgaroff", "Judith B\u00fctepage"], "title": "Tiny is not small enough: High-quality, low-resource facial animation models through hybrid knowledge distillation", "categories": ["cs.GR", "cs.LG", "cs.MM", "cs.SD", "eess.AS"], "comment": "Accepted to ACM Transactions on Graphics 2025 (SIGGRAPH journal\n  track)", "summary": "The training of high-quality, robust machine learning models for\nspeech-driven 3D facial animation requires a large, diverse dataset of\nhigh-quality audio-animation pairs. To overcome the lack of such a dataset,\nrecent work has introduced large pre-trained speech encoders that are robust to\nvariations in the input audio and, therefore, enable the facial animation model\nto generalize across speakers, audio quality, and languages. However, the\nresulting facial animation models are prohibitively large and lend themselves\nonly to offline inference on a dedicated machine. In this work, we explore\non-device, real-time facial animation models in the context of game\ndevelopment. We overcome the lack of large datasets by using hybrid knowledge\ndistillation with pseudo-labeling. Given a large audio dataset, we employ a\nhigh-performing teacher model to train very small student models. In contrast\nto the pre-trained speech encoders, our student models only consist of\nconvolutional and fully-connected layers, removing the need for attention\ncontext or recurrent updates. In our experiments, we demonstrate that we can\nreduce the memory footprint to up to 3.4 MB and required future audio context\nto up to 81 ms while maintaining high-quality animations. This paves the way\nfor on-device inference, an important step towards realistic, model-driven\ndigital characters.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u8f7b\u91cf\u5316\u7684\u5b9e\u65f6\u9762\u90e8\u52a8\u753b\u6a21\u578b\uff0c\u901a\u8fc7\u6df7\u5408\u77e5\u8bc6\u84b8\u998f\u548c\u4f2a\u6807\u7b7e\u6280\u672f\u89e3\u51b3\u6570\u636e\u4e0d\u8db3\u95ee\u9898\uff0c\u5b9e\u73b0\u4e863.4 MB\u7684\u5c0f\u578b\u6a21\u578b\u548c81 ms\u7684\u4f4e\u5ef6\u8fdf\u3002", "motivation": "\u5f53\u524d\u8bed\u97f3\u9a71\u52a8\u76843D\u9762\u90e8\u52a8\u753b\u6a21\u578b\u4f9d\u8d56\u5927\u578b\u9884\u8bad\u7ec3\u8bed\u97f3\u7f16\u7801\u5668\uff0c\u5bfc\u81f4\u6a21\u578b\u8fc7\u5927\u4e14\u4ec5\u9002\u7528\u4e8e\u79bb\u7ebf\u63a8\u7406\uff0c\u96be\u4ee5\u5728\u6e38\u620f\u5f00\u53d1\u4e2d\u5b9e\u73b0\u5b9e\u65f6\u5e94\u7528\u3002", "method": "\u91c7\u7528\u6df7\u5408\u77e5\u8bc6\u84b8\u998f\u548c\u4f2a\u6807\u7b7e\u6280\u672f\uff0c\u5229\u7528\u5927\u578b\u97f3\u9891\u6570\u636e\u96c6\u548c\u6559\u5e08\u6a21\u578b\u8bad\u7ec3\u5c0f\u578b\u5b66\u751f\u6a21\u578b\uff0c\u4ec5\u5305\u542b\u5377\u79ef\u548c\u5168\u8fde\u63a5\u5c42\uff0c\u65e0\u9700\u6ce8\u610f\u529b\u673a\u5236\u6216\u5faa\u73af\u66f4\u65b0\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u6a21\u578b\u5185\u5b58\u5360\u7528\u964d\u81f33.4 MB\uff0c\u672a\u6765\u97f3\u9891\u4e0a\u4e0b\u6587\u9700\u6c42\u964d\u81f381 ms\uff0c\u540c\u65f6\u4fdd\u6301\u9ad8\u8d28\u91cf\u52a8\u753b\u6548\u679c\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u8bbe\u5907\u7aef\u5b9e\u65f6\u63a8\u7406\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6848\uff0c\u63a8\u52a8\u4e86\u6a21\u578b\u9a71\u52a8\u7684\u6570\u5b57\u89d2\u8272\u53d1\u5c55\u3002"}}
{"id": "2507.17856", "pdf": "https://arxiv.org/pdf/2507.17856", "abs": "https://arxiv.org/abs/2507.17856", "authors": ["Dennis Benders", "Laura Ferranti", "Johannes K\u00f6hler"], "title": "A Step-by-step Guide on Nonlinear Model Predictive Control for Safe Mobile Robot Navigation", "categories": ["cs.RO", "cs.SY", "eess.SY"], "comment": "51 pages, 3 figures", "summary": "Designing a Model Predictive Control (MPC) scheme that enables a mobile robot\nto safely navigate through an obstacle-filled environment is a complicated yet\nessential task in robotics. In this technical report, safety refers to ensuring\nthat the robot respects state and input constraints while avoiding collisions\nwith obstacles despite the presence of disturbances and measurement noise. This\nreport offers a step-by-step approach to implementing Nonlinear Model\nPredictive Control (NMPC) schemes addressing these safety requirements.\nNumerous books and survey papers provide comprehensive overviews of linear MPC\n(LMPC) \\cite{bemporad2007robust,kouvaritakis2016model}, NMPC\n\\cite{rawlings2017model,allgower2004nonlinear,mayne2014model,grune2017nonlinear,saltik2018outlook},\nand their applications in various domains, including robotics\n\\cite{nascimento2018nonholonomic,nguyen2021model,shi2021advanced,wei2022mpc}.\nThis report does not aim to replicate those exhaustive reviews. Instead, it\nfocuses specifically on NMPC as a foundation for safe mobile robot navigation.\nThe goal is to provide a practical and accessible path from theoretical\nconcepts to mathematical proofs and implementation, emphasizing safety and\nperformance guarantees. It is intended for researchers, robotics engineers, and\npractitioners seeking to bridge the gap between theoretical NMPC formulations\nand real-world robotic applications.\n  This report is not necessarily meant to remain fixed over time. If someone\nfinds an error in the presented theory, please reach out via the given email\naddresses. We are happy to update the document if necessary.", "AI": {"tldr": "\u8be5\u6280\u672f\u62a5\u544a\u4ecb\u7ecd\u4e86\u5982\u4f55\u901a\u8fc7\u975e\u7ebf\u6027\u6a21\u578b\u9884\u6d4b\u63a7\u5236\uff08NMPC\uff09\u5b9e\u73b0\u79fb\u52a8\u673a\u5668\u4eba\u5728\u969c\u788d\u7269\u73af\u5883\u4e2d\u7684\u5b89\u5168\u5bfc\u822a\uff0c\u5f3a\u8c03\u7406\u8bba\u4e0e\u5b9e\u9645\u5e94\u7528\u7684\u7ed3\u5408\u3002", "motivation": "\u79fb\u52a8\u673a\u5668\u4eba\u5728\u590d\u6742\u73af\u5883\u4e2d\u5b89\u5168\u5bfc\u822a\u662f\u4e00\u4e2a\u5173\u952e\u4f46\u590d\u6742\u7684\u4efb\u52a1\uff0c\u9700\u8981\u786e\u4fdd\u673a\u5668\u4eba\u9075\u5b88\u72b6\u6001\u548c\u8f93\u5165\u7ea6\u675f\uff0c\u540c\u65f6\u907f\u514d\u4e0e\u969c\u788d\u7269\u78b0\u649e\u3002", "method": "\u62a5\u544a\u63d0\u4f9b\u4e86\u4e00\u79cd\u9010\u6b65\u5b9e\u73b0NMPC\u7684\u65b9\u6cd5\uff0c\u91cd\u70b9\u5173\u6ce8\u5b89\u5168\u6027\u548c\u6027\u80fd\u4fdd\u8bc1\uff0c\u4ece\u7406\u8bba\u6982\u5ff5\u5230\u6570\u5b66\u8bc1\u660e\u548c\u5b9e\u9645\u5b9e\u73b0\u3002", "result": "\u62a5\u544a\u4e3a\u7814\u7a76\u4eba\u5458\u548c\u5de5\u7a0b\u5e08\u63d0\u4f9b\u4e86\u4e00\u79cd\u5b9e\u7528\u7684NMPC\u5b9e\u73b0\u8def\u5f84\uff0c\u65e8\u5728\u586b\u8865\u7406\u8bbaNMPC\u4e0e\u5b9e\u9645\u673a\u5668\u4eba\u5e94\u7528\u4e4b\u95f4\u7684\u9e3f\u6c9f\u3002", "conclusion": "\u8be5\u62a5\u544a\u65e8\u5728\u4e3aNMPC\u5728\u673a\u5668\u4eba\u5bfc\u822a\u4e2d\u7684\u5e94\u7528\u63d0\u4f9b\u5b9e\u7528\u6307\u5357\uff0c\u5e76\u6b22\u8fce\u53cd\u9988\u4ee5\u66f4\u65b0\u5185\u5bb9\u3002"}}
{"id": "2507.17937", "pdf": "https://arxiv.org/pdf/2507.17937", "abs": "https://arxiv.org/abs/2507.17937", "authors": ["Jaechul Roh", "Zachary Novack", "Yuefeng Peng", "Niloofar Mireshghallah", "Taylor Berg-Kirkpatrick", "Amir Houmansadr"], "title": "Bob's Confetti: Phonetic Memorization Attacks in Music and Video Generation", "categories": ["cs.SD", "cs.AI", "cs.CL", "eess.AS"], "comment": null, "summary": "Lyrics-to-Song (LS2) generation models promise end-to-end music synthesis\nfrom text, yet their vulnerability to training data memorization remains\nunderexplored. We introduce Adversarial PhoneTic Prompting (APT), a novel\nattack where lyrics are semantically altered while preserving their acoustic\nstructure through homophonic substitutions (e.g., Eminem's famous \"mom's\nspaghetti\" $\\rightarrow$ \"Bob's confetti\"). Despite these distortions, we\nuncover a powerful form of sub-lexical memorization: models like SUNO and YuE\nregenerate outputs strikingly similar to known training content, achieving high\nsimilarity across audio-domain metrics, including CLAP, AudioJudge, and\nCoverID. This vulnerability persists across multiple languages and genres. More\nsurprisingly, we discover that phoneme-altered lyrics alone can trigger visual\nmemorization in text-to-video models. When prompted with phonetically modified\nlyrics from Lose Yourself, Veo 3 reconstructs visual elements from the original\nmusic video -- including character appearance and scene composition -- despite\nno visual cues in the prompt. We term this phenomenon phonetic-to-visual\nregurgitation. Together, these findings expose a critical vulnerability in\ntranscript-conditioned multimodal generation: phonetic prompting alone can\nunlock memorized audiovisual content, raising urgent questions about copyright,\nsafety, and content provenance in modern generative systems. Example\ngenerations are available on our demo page (jrohsc.github.io/music_attack/).", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u6b4c\u8bcd\u5230\u6b4c\u66f2\u751f\u6210\u6a21\u578b\uff08LS2\uff09\u5bf9\u8bad\u7ec3\u6570\u636e\u8bb0\u5fc6\u7684\u8106\u5f31\u6027\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aAPT\u7684\u653b\u51fb\u65b9\u6cd5\uff0c\u901a\u8fc7\u540c\u97f3\u66ff\u6362\u6539\u53d8\u6b4c\u8bcd\u8bed\u4e49\u4f46\u4fdd\u7559\u58f0\u5b66\u7ed3\u6784\uff0c\u63ed\u793a\u4e86\u6a21\u578b\u5bf9\u8bad\u7ec3\u5185\u5bb9\u7684\u8bb0\u5fc6\u73b0\u8c61\uff0c\u5e76\u53d1\u73b0\u8fd9\u79cd\u73b0\u8c61\u5728\u6587\u672c\u5230\u89c6\u9891\u6a21\u578b\u4e2d\u540c\u6837\u5b58\u5728\u3002", "motivation": "\u63a2\u7d22\u6b4c\u8bcd\u5230\u6b4c\u66f2\u751f\u6210\u6a21\u578b\u5bf9\u8bad\u7ec3\u6570\u636e\u8bb0\u5fc6\u7684\u8106\u5f31\u6027\uff0c\u63ed\u793a\u5176\u6f5c\u5728\u7684\u5b89\u5168\u548c\u7248\u6743\u95ee\u9898\u3002", "method": "\u63d0\u51faAdversarial PhoneTic Prompting (APT)\u653b\u51fb\u65b9\u6cd5\uff0c\u901a\u8fc7\u540c\u97f3\u66ff\u6362\u6539\u53d8\u6b4c\u8bcd\u8bed\u4e49\u4f46\u4fdd\u7559\u58f0\u5b66\u7ed3\u6784\uff0c\u6d4b\u8bd5\u6a21\u578b\u5bf9\u8bad\u7ec3\u5185\u5bb9\u7684\u8bb0\u5fc6\u80fd\u529b\u3002", "result": "\u53d1\u73b0\u6a21\u578b\uff08\u5982SUNO\u548cYuE\uff09\u4f1a\u751f\u6210\u4e0e\u8bad\u7ec3\u5185\u5bb9\u9ad8\u5ea6\u76f8\u4f3c\u7684\u8f93\u51fa\uff0c\u4e14\u5728\u6587\u672c\u5230\u89c6\u9891\u6a21\u578b\u4e2d\u4e5f\u80fd\u89e6\u53d1\u89c6\u89c9\u8bb0\u5fc6\u73b0\u8c61\u3002", "conclusion": "\u63ed\u793a\u4e86\u8f6c\u5f55\u6761\u4ef6\u591a\u6a21\u6001\u751f\u6210\u4e2d\u7684\u5173\u952e\u6f0f\u6d1e\uff0c\u63d0\u51fa\u5bf9\u73b0\u4ee3\u751f\u6210\u7cfb\u7edf\u7684\u7248\u6743\u3001\u5b89\u5168\u548c\u5185\u5bb9\u6765\u6e90\u7684\u7d27\u8feb\u95ee\u9898\u3002"}}
{"id": "2507.18224", "pdf": "https://arxiv.org/pdf/2507.18224", "abs": "https://arxiv.org/abs/2507.18224", "authors": ["Shiyuan Li", "Yixin Liu", "Qingsong Wen", "Chengqi Zhang", "Shirui Pan"], "title": "Assemble Your Crew: Automatic Multi-agent Communication Topology Design via Autoregressive Graph Generation", "categories": ["cs.MA", "cs.CL"], "comment": null, "summary": "Multi-agent systems (MAS) based on large language models (LLMs) have emerged\nas a powerful solution for dealing with complex problems across diverse\ndomains. The effectiveness of MAS is critically dependent on its collaboration\ntopology, which has become a focal point for automated design research.\nHowever, existing approaches are fundamentally constrained by their reliance on\na template graph modification paradigm with a predefined set of agents and\nhard-coded interaction structures, significantly limiting their adaptability to\ntask-specific requirements. To address these limitations, we reframe MAS design\nas a conditional autoregressive graph generation task, where both the system\ncomposition and structure are designed jointly. We propose ARG-Designer, a\nnovel autoregressive model that operationalizes this paradigm by constructing\nthe collaboration graph from scratch. Conditioned on a natural language task\nquery, ARG-Designer sequentially and dynamically determines the required number\nof agents, selects their appropriate roles from an extensible pool, and\nestablishes the optimal communication links between them. This generative\napproach creates a customized topology in a flexible and extensible manner,\nprecisely tailored to the unique demands of different tasks. Extensive\nexperiments across six diverse benchmarks demonstrate that ARG-Designer not\nonly achieves state-of-the-art performance but also enjoys significantly\ngreater token efficiency and enhanced extensibility. The source code of\nARG-Designer is available at https://github.com/Shiy-Li/ARG-Designer.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6761\u4ef6\u81ea\u56de\u5f52\u56fe\u751f\u6210\u7684\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\uff08MAS\uff09\u8bbe\u8ba1\u65b9\u6cd5ARG-Designer\uff0c\u901a\u8fc7\u52a8\u6001\u751f\u6210\u534f\u4f5c\u56fe\u6765\u4f18\u5316\u4efb\u52a1\u9002\u5e94\u6027\u3002", "motivation": "\u73b0\u6709MAS\u8bbe\u8ba1\u65b9\u6cd5\u53d7\u9650\u4e8e\u9884\u5b9a\u4e49\u7684\u4ee3\u7406\u548c\u4ea4\u4e92\u7ed3\u6784\uff0c\u7f3a\u4e4f\u7075\u6d3b\u6027\uff0c\u65e0\u6cd5\u6ee1\u8db3\u4efb\u52a1\u7279\u5b9a\u9700\u6c42\u3002", "method": "\u5c06MAS\u8bbe\u8ba1\u91cd\u65b0\u5b9a\u4e49\u4e3a\u6761\u4ef6\u81ea\u56de\u5f52\u56fe\u751f\u6210\u4efb\u52a1\uff0c\u63d0\u51faARG-Designer\u6a21\u578b\uff0c\u52a8\u6001\u751f\u6210\u4ee3\u7406\u6570\u91cf\u3001\u89d2\u8272\u548c\u901a\u4fe1\u94fe\u8def\u3002", "result": "\u5728\u516d\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cARG-Designer\u5b9e\u73b0\u4e86\u6700\u4f18\u6027\u80fd\uff0c\u5e76\u663e\u8457\u63d0\u9ad8\u4e86\u6548\u7387\u548c\u53ef\u6269\u5c55\u6027\u3002", "conclusion": "ARG-Designer\u4e3aMAS\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u7075\u6d3b\u4e14\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u663e\u8457\u63d0\u5347\u4e86\u4efb\u52a1\u9002\u5e94\u6027\u3002"}}
{"id": "2507.17784", "pdf": "https://arxiv.org/pdf/2507.17784", "abs": "https://arxiv.org/abs/2507.17784", "authors": ["Minh-Duong Nguyen", "Quoc-Viet Pham", "Nguyen H. Tran", "Hoang-Khoi Do", "Duy T. Ngo", "Won-Joo Hwang"], "title": "Knowledge Abstraction for Knowledge-based Semantic Communication: A Generative Causality Invariant Approach", "categories": ["cs.LG", "68", "I.2.0"], "comment": "13 pages, 12 figures, 4 tables", "summary": "In this study, we design a low-complexity and generalized AI model that can\ncapture common knowledge to improve data reconstruction of the channel decoder\nfor semantic communication. Specifically, we propose a generative adversarial\nnetwork that leverages causality-invariant learning to extract causal and\nnon-causal representations from the data. Causal representations are invariant\nand encompass crucial information to identify the data's label. They can\nencapsulate semantic knowledge and facilitate effective data reconstruction at\nthe receiver. Moreover, the causal mechanism ensures that learned\nrepresentations remain consistent across different domains, making the system\nreliable even with users collecting data from diverse domains. As\nuser-collected data evolves over time causing knowledge divergence among users,\nwe design sparse update protocols to improve the invariant properties of the\nknowledge while minimizing communication overheads. Three key observations were\ndrawn from our empirical evaluations. Firstly, causality-invariant knowledge\nensures consistency across different devices despite the diverse training data.\nSecondly, invariant knowledge has promising performance in classification\ntasks, which is pivotal for goal-oriented semantic communications. Thirdly, our\nknowledge-based data reconstruction highlights the robustness of our decoder,\nwhich surpasses other state-of-the-art data reconstruction and semantic\ncompression methods in terms of Peak Signal-to-Noise Ratio (PSNR).", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u4f4e\u590d\u6742\u5ea6\u3001\u901a\u7528\u7684AI\u6a21\u578b\uff0c\u901a\u8fc7\u56e0\u679c\u4e0d\u53d8\u5b66\u4e60\u63d0\u53d6\u6570\u636e\u7684\u56e0\u679c\u548c\u975e\u56e0\u679c\u8868\u793a\uff0c\u7528\u4e8e\u8bed\u4e49\u901a\u4fe1\u4e2d\u4fe1\u9053\u89e3\u7801\u5668\u7684\u6570\u636e\u91cd\u5efa\u3002", "motivation": "\u89e3\u51b3\u8bed\u4e49\u901a\u4fe1\u4e2d\u6570\u636e\u91cd\u5efa\u7684\u6311\u6218\uff0c\u7279\u522b\u662f\u5728\u7528\u6237\u6570\u636e\u591a\u6837\u5316\u548c\u77e5\u8bc6\u968f\u65f6\u95f4\u6f14\u53d8\u7684\u60c5\u51b5\u4e0b\uff0c\u786e\u4fdd\u89e3\u7801\u5668\u7684\u9c81\u68d2\u6027\u548c\u4e00\u81f4\u6027\u3002", "method": "\u4f7f\u7528\u751f\u6210\u5bf9\u6297\u7f51\u7edc\uff08GAN\uff09\u548c\u56e0\u679c\u4e0d\u53d8\u5b66\u4e60\uff0c\u63d0\u53d6\u56e0\u679c\u548c\u975e\u56e0\u679c\u8868\u793a\uff1b\u8bbe\u8ba1\u7a00\u758f\u66f4\u65b0\u534f\u8bae\u4ee5\u51cf\u5c11\u901a\u4fe1\u5f00\u9500\u3002", "result": "\u56e0\u679c\u4e0d\u53d8\u77e5\u8bc6\u786e\u4fdd\u4e86\u8de8\u8bbe\u5907\u7684\u4e00\u81f4\u6027\uff0c\u5206\u7c7b\u4efb\u52a1\u8868\u73b0\u4f18\u5f02\uff0c\u6570\u636e\u91cd\u5efa\u7684PSNR\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u8bed\u4e49\u901a\u4fe1\u4e2d\u8868\u73b0\u51fa\u9ad8\u6548\u6027\u548c\u9c81\u68d2\u6027\uff0c\u5c24\u5176\u5728\u8de8\u57df\u6570\u636e\u91cd\u5efa\u548c\u77e5\u8bc6\u4e00\u81f4\u6027\u65b9\u9762\u5177\u6709\u4f18\u52bf\u3002"}}
{"id": "2507.17849", "pdf": "https://arxiv.org/pdf/2507.17849", "abs": "https://arxiv.org/abs/2507.17849", "authors": ["Zhangyue Yin", "Qiushi Sun", "Zhiyuan Zeng", "Qinyuan Cheng", "Xipeng Qiu", "Xuanjing Huang"], "title": "Dynamic and Generalizable Process Reward Modeling", "categories": ["cs.CL"], "comment": "Accepted by ACL 2025 Main", "summary": "Process Reward Models (PRMs) are crucial for guiding Large Language Models\n(LLMs) in complex scenarios by providing dense reward signals. However,\nexisting PRMs primarily rely on heuristic approaches, which struggle with\ncross-domain generalization. While LLM-as-judge has been proposed to provide\ngeneralized rewards, current research has focused mainly on feedback results,\noverlooking the meaningful guidance embedded within the text. Additionally,\nstatic and coarse-grained evaluation criteria struggle to adapt to complex\nprocess supervision. To tackle these challenges, we propose Dynamic and\nGeneralizable Process Reward Modeling (DG-PRM), which features a reward tree to\ncapture and store fine-grained, multi-dimensional reward criteria. DG-PRM\ndynamically selects reward signals for step-wise reward scoring. To handle\nmultifaceted reward signals, we pioneeringly adopt Pareto dominance estimation\nto identify discriminative positive and negative pairs. Experimental results\nshow that DG-PRM achieves stunning performance on prevailing benchmarks,\nsignificantly boosting model performance across tasks with dense rewards.\nFurther analysis reveals that DG-PRM adapts well to out-of-distribution\nscenarios, demonstrating exceptional generalizability.", "AI": {"tldr": "DG-PRM\u63d0\u51fa\u4e86\u4e00\u79cd\u52a8\u6001\u4e14\u53ef\u6cdb\u5316\u7684\u8fc7\u7a0b\u5956\u52b1\u5efa\u6a21\u65b9\u6cd5\uff0c\u901a\u8fc7\u5956\u52b1\u6811\u548c\u591a\u7ef4\u5956\u52b1\u6807\u51c6\uff0c\u663e\u8457\u63d0\u5347\u4e86LLM\u5728\u590d\u6742\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u8fc7\u7a0b\u5956\u52b1\u6a21\u578b\uff08PRMs\uff09\u4f9d\u8d56\u542f\u53d1\u5f0f\u65b9\u6cd5\uff0c\u8de8\u9886\u57df\u6cdb\u5316\u80fd\u529b\u4e0d\u8db3\uff0c\u4e14\u73b0\u6709\u7814\u7a76\u5ffd\u89c6\u4e86\u6587\u672c\u4e2d\u7684\u6307\u5bfc\u610f\u4e49\u3002", "method": "DG-PRM\u91c7\u7528\u5956\u52b1\u6811\u5b58\u50a8\u7ec6\u7c92\u5ea6\u5956\u52b1\u6807\u51c6\uff0c\u52a8\u6001\u9009\u62e9\u5956\u52b1\u4fe1\u53f7\uff0c\u5e76\u9996\u6b21\u4f7f\u7528\u5e15\u7d2f\u6258\u4f18\u52bf\u4f30\u8ba1\u5904\u7406\u591a\u7ef4\u5956\u52b1\u4fe1\u53f7\u3002", "result": "\u5b9e\u9a8c\u8868\u660eDG-PRM\u5728\u4e3b\u6d41\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u663e\u8457\u63d0\u5347\u6a21\u578b\u6027\u80fd\uff0c\u5e76\u5c55\u73b0\u51fa\u826f\u597d\u7684\u5206\u5e03\u5916\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "DG-PRM\u4e3a\u590d\u6742\u4efb\u52a1\u4e2d\u7684\u8fc7\u7a0b\u5956\u52b1\u5efa\u6a21\u63d0\u4f9b\u4e86\u9ad8\u6548\u4e14\u6cdb\u5316\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.17844", "pdf": "https://arxiv.org/pdf/2507.17844", "abs": "https://arxiv.org/abs/2507.17844", "authors": ["Sai Varun Kodathala", "Yashwanth Reddy Vutukoori", "Rakesh Vunnam"], "title": "SV3.3B: A Sports Video Understanding Model for Action Recognition", "categories": ["cs.CV", "cs.AI"], "comment": "8 pages, 6 figures, 4 tables. Submitted to AIxSET 2025", "summary": "This paper addresses the challenge of automated sports video analysis, which\nhas traditionally been limited by computationally intensive models requiring\nserver-side processing and lacking fine-grained understanding of athletic\nmovements. Current approaches struggle to capture the nuanced biomechanical\ntransitions essential for meaningful sports analysis, often missing critical\nphases like preparation, execution, and follow-through that occur within\nseconds. To address these limitations, we introduce SV3.3B, a lightweight 3.3B\nparameter video understanding model that combines novel temporal motion\ndifference sampling with self-supervised learning for efficient on-device\ndeployment. Our approach employs a DWT-VGG16-LDA based keyframe extraction\nmechanism that intelligently identifies the 16 most representative frames from\nsports sequences, followed by a V-DWT-JEPA2 encoder pretrained through\nmask-denoising objectives and an LLM decoder fine-tuned for sports action\ndescription generation. Evaluated on a subset of the NSVA basketball dataset,\nSV3.3B achieves superior performance across both traditional text generation\nmetrics and sports-specific evaluation criteria, outperforming larger\nclosed-source models including GPT-4o variants while maintaining significantly\nlower computational requirements. Our model demonstrates exceptional capability\nin generating technically detailed and analytically rich sports descriptions,\nachieving 29.2% improvement over GPT-4o in ground truth validation metrics,\nwith substantial improvements in information density, action complexity, and\nmeasurement precision metrics essential for comprehensive athletic analysis.\nModel Available at https://huggingface.co/sportsvision/SV3.3B.", "AI": {"tldr": "SV3.3B\u662f\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u76843.3B\u53c2\u6570\u89c6\u9891\u7406\u89e3\u6a21\u578b\uff0c\u7528\u4e8e\u81ea\u52a8\u5316\u4f53\u80b2\u89c6\u9891\u5206\u6790\uff0c\u901a\u8fc7\u521b\u65b0\u7684\u65f6\u95f4\u8fd0\u52a8\u5dee\u5f02\u91c7\u6837\u548c\u81ea\u76d1\u7763\u5b66\u4e60\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u7684\u8bbe\u5907\u7aef\u90e8\u7f72\uff0c\u5e76\u5728\u6027\u80fd\u4e0a\u8d85\u8d8a\u4e86GPT-4o\u7b49\u5927\u578b\u6a21\u578b\u3002", "motivation": "\u4f20\u7edf\u4f53\u80b2\u89c6\u9891\u5206\u6790\u6a21\u578b\u8ba1\u7b97\u91cf\u5927\u4e14\u7f3a\u4e4f\u5bf9\u8fd0\u52a8\u7ec6\u8282\u7684\u7cbe\u7ec6\u7406\u89e3\uff0c\u65e0\u6cd5\u6355\u6349\u5173\u952e\u7684\u8fd0\u52a8\u9636\u6bb5\uff08\u5982\u51c6\u5907\u3001\u6267\u884c\u548c\u540e\u7eed\u52a8\u4f5c\uff09\u3002", "method": "\u91c7\u7528DWT-VGG16-LDA\u5173\u952e\u5e27\u63d0\u53d6\u673a\u5236\u548cV-DWT-JEPA2\u7f16\u7801\u5668\uff0c\u7ed3\u5408\u81ea\u76d1\u7763\u5b66\u4e60\u548cLLM\u89e3\u7801\u5668\uff0c\u751f\u6210\u8be6\u7ec6\u7684\u8fd0\u52a8\u52a8\u4f5c\u63cf\u8ff0\u3002", "result": "\u5728NSVA\u7bee\u7403\u6570\u636e\u96c6\u4e0a\uff0cSV3.3B\u5728\u6587\u672c\u751f\u6210\u548c\u4f53\u80b2\u7279\u5b9a\u8bc4\u4f30\u6307\u6807\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u6027\u80fd\u63d0\u534729.2%\uff0c\u4fe1\u606f\u5bc6\u5ea6\u548c\u52a8\u4f5c\u590d\u6742\u6027\u663e\u8457\u63d0\u9ad8\u3002", "conclusion": "SV3.3B\u4e3a\u4f53\u80b2\u89c6\u9891\u5206\u6790\u63d0\u4f9b\u4e86\u9ad8\u6548\u4e14\u6027\u80fd\u4f18\u8d8a\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u9002\u7528\u4e8e\u8bbe\u5907\u7aef\u90e8\u7f72\u3002"}}
{"id": "2507.18022", "pdf": "https://arxiv.org/pdf/2507.18022", "abs": "https://arxiv.org/abs/2507.18022", "authors": ["Victoria R. Li", "Johnathan Sun", "Martin Wattenberg"], "title": "Does visualization help AI understand data?", "categories": ["cs.AI", "cs.HC", "cs.LG"], "comment": "5 pages, 6 figures", "summary": "Charts and graphs help people analyze data, but can they also be useful to AI\nsystems? To investigate this question, we perform a series of experiments with\ntwo commercial vision-language models: GPT 4.1 and Claude 3.5. Across three\nrepresentative analysis tasks, the two systems describe synthetic datasets more\nprecisely and accurately when raw data is accompanied by a scatterplot,\nespecially as datasets grow in complexity. Comparison with two baselines --\nproviding a blank chart and a chart with mismatched data -- shows that the\nimproved performance is due to the content of the charts. Our results are\ninitial evidence that AI systems, like humans, can benefit from visualization.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\uff0cAI\u7cfb\u7edf\uff08\u5982GPT 4.1\u548cClaude 3.5\uff09\u5728\u5904\u7406\u5408\u6210\u6570\u636e\u96c6\u65f6\uff0c\u82e5\u6570\u636e\u914d\u6709\u6563\u70b9\u56fe\uff0c\u5176\u63cf\u8ff0\u4f1a\u66f4\u7cbe\u786e\u548c\u51c6\u786e\u3002", "motivation": "\u63a2\u8ba8\u56fe\u8868\u662f\u5426\u5bf9AI\u7cfb\u7edf\u5206\u6790\u6570\u636e\u6709\u5e2e\u52a9\u3002", "method": "\u4f7f\u7528\u4e24\u79cd\u5546\u4e1a\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08GPT 4.1\u548cClaude 3.5\uff09\uff0c\u5728\u4e09\u79cd\u4ee3\u8868\u6027\u5206\u6790\u4efb\u52a1\u4e2d\u6d4b\u8bd5\u6570\u636e\u914d\u56fe\u7684\u6548\u679c\uff0c\u5e76\u4e0e\u7a7a\u767d\u56fe\u8868\u548c\u9519\u8bef\u6570\u636e\u56fe\u8868\u8fdb\u884c\u5bf9\u6bd4\u3002", "result": "AI\u7cfb\u7edf\u5728\u6570\u636e\u914d\u6709\u6563\u70b9\u56fe\u65f6\u8868\u73b0\u66f4\u4f18\uff0c\u5c24\u5176\u662f\u5728\u590d\u6742\u6570\u636e\u96c6\u4e0a\u3002", "conclusion": "\u521d\u6b65\u8bc1\u636e\u8868\u660e\uff0cAI\u7cfb\u7edf\u4e0e\u4eba\u7c7b\u4e00\u6837\uff0c\u53ef\u4ee5\u4ece\u53ef\u89c6\u5316\u4e2d\u53d7\u76ca\u3002"}}
{"id": "2507.18625", "pdf": "https://arxiv.org/pdf/2507.18625", "abs": "https://arxiv.org/abs/2507.18625", "authors": ["Shuqing Li", "Anson Y. Lam", "Yun Peng", "Wenxuan Wang", "Michael R. Lyu"], "title": "3D Software Synthesis Guided by Constraint-Expressive Intermediate Representation", "categories": ["cs.CV", "cs.AI", "cs.MM", "cs.SE"], "comment": null, "summary": "Graphical user interface (UI) software has undergone a fundamental\ntransformation from traditional two-dimensional (2D) desktop/web/mobile\ninterfaces to spatial three-dimensional (3D) environments. While existing work\nhas made remarkable success in automated 2D software generation, such as\nHTML/CSS and mobile app interface code synthesis, the generation of 3D software\nstill remains under-explored. Current methods for 3D software generation\nusually generate the 3D environments as a whole and cannot modify or control\nspecific elements in the software. Furthermore, these methods struggle to\nhandle the complex spatial and semantic constraints inherent in the real world.\nTo address the challenges, we present Scenethesis, a novel\nrequirement-sensitive 3D software synthesis approach that maintains formal\ntraceability between user specifications and generated 3D software. Scenethesis\nis built upon ScenethesisLang, a domain-specific language that serves as a\ngranular constraint-aware intermediate representation (IR) to bridge natural\nlanguage requirements and executable 3D software. It serves both as a\ncomprehensive scene description language enabling fine-grained modification of\n3D software elements and as a formal constraint-expressive specification\nlanguage capable of expressing complex spatial constraints. By decomposing 3D\nsoftware synthesis into stages operating on ScenethesisLang, Scenethesis\nenables independent verification, targeted modification, and systematic\nconstraint satisfaction. Our evaluation demonstrates that Scenethesis\naccurately captures over 80% of user requirements and satisfies more than 90%\nof hard constraints while handling over 100 constraints simultaneously.\nFurthermore, Scenethesis achieves a 42.8% improvement in BLIP-2 visual\nevaluation scores compared to the state-of-the-art method.", "AI": {"tldr": "Scenethesis\u662f\u4e00\u79cd\u65b0\u578b\u76843D\u8f6f\u4ef6\u5408\u6210\u65b9\u6cd5\uff0c\u901a\u8fc7\u9886\u57df\u7279\u5b9a\u8bed\u8a00ScenethesisLang\u5b9e\u73b0\u7528\u6237\u9700\u6c42\u4e0e\u751f\u6210\u8f6f\u4ef6\u4e4b\u95f4\u7684\u53ef\u8ffd\u6eaf\u6027\uff0c\u663e\u8457\u63d0\u5347\u4e863D\u8f6f\u4ef6\u751f\u6210\u7684\u7cbe\u786e\u6027\u548c\u7ea6\u675f\u5904\u7406\u80fd\u529b\u3002", "motivation": "\u73b0\u67093D\u8f6f\u4ef6\u751f\u6210\u65b9\u6cd5\u65e0\u6cd5\u7cbe\u7ec6\u63a7\u5236\u5355\u4e2a\u5143\u7d20\u4e14\u96be\u4ee5\u5904\u7406\u590d\u6742\u7a7a\u95f4\u548c\u8bed\u4e49\u7ea6\u675f\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u65b0\u65b9\u6cd5\u6765\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "\u63d0\u51faScenethesis\uff0c\u57fa\u4e8eScenethesisLang\uff08\u4e00\u79cd\u9886\u57df\u7279\u5b9a\u8bed\u8a00\uff09\uff0c\u5c063D\u8f6f\u4ef6\u5408\u6210\u5206\u89e3\u4e3a\u591a\u4e2a\u9636\u6bb5\uff0c\u652f\u6301\u72ec\u7acb\u9a8c\u8bc1\u3001\u9488\u5bf9\u6027\u4fee\u6539\u548c\u7cfb\u7edf\u7ea6\u675f\u6ee1\u8db3\u3002", "result": "Scenethesis\u80fd\u51c6\u786e\u6355\u634980%\u4ee5\u4e0a\u7528\u6237\u9700\u6c42\uff0c\u6ee1\u8db390%\u4ee5\u4e0a\u7684\u786c\u7ea6\u675f\uff0c\u540c\u65f6\u5904\u7406100\u591a\u4e2a\u7ea6\u675f\uff0c\u5e76\u5728\u89c6\u89c9\u8bc4\u4f30\u4e2d\u6bd4\u73b0\u6709\u65b9\u6cd5\u63d0\u534742.8%\u3002", "conclusion": "Scenethesis\u901a\u8fc7\u5176\u8bed\u8a00\u548c\u5206\u9636\u6bb5\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u4e863D\u8f6f\u4ef6\u751f\u6210\u7684\u7075\u6d3b\u6027\u548c\u7cbe\u786e\u6027\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411\u3002"}}
{"id": "2507.18033", "pdf": "https://arxiv.org/pdf/2507.18033", "abs": "https://arxiv.org/abs/2507.18033", "authors": ["Mingfeng Yuan", "Letian Wang", "Steven L. Waslander"], "title": "OpenNav: Open-World Navigation with Multimodal Large Language Models", "categories": ["cs.RO", "cs.AI"], "comment": null, "summary": "Pre-trained large language models (LLMs) have demonstrated strong\ncommon-sense reasoning abilities, making them promising for robotic navigation\nand planning tasks. However, despite recent progress, bridging the gap between\nlanguage descriptions and actual robot actions in the open-world, beyond merely\ninvoking limited predefined motion primitives, remains an open challenge. In\nthis work, we aim to enable robots to interpret and decompose complex language\ninstructions, ultimately synthesizing a sequence of trajectory points to\ncomplete diverse navigation tasks given open-set instructions and open-set\nobjects. We observe that multi-modal large language models (MLLMs) exhibit\nstrong cross-modal understanding when processing free-form language\ninstructions, demonstrating robust scene comprehension. More importantly,\nleveraging their code-generation capability, MLLMs can interact with\nvision-language perception models to generate compositional 2D bird-eye-view\nvalue maps, effectively integrating semantic knowledge from MLLMs with spatial\ninformation from maps to reinforce the robot's spatial understanding. To\nfurther validate our approach, we effectively leverage large-scale autonomous\nvehicle datasets (AVDs) to validate our proposed zero-shot vision-language\nnavigation framework in outdoor navigation tasks, demonstrating its capability\nto execute a diverse range of free-form natural language navigation\ninstructions while maintaining robustness against object detection errors and\nlinguistic ambiguities. Furthermore, we validate our system on a Husky robot in\nboth indoor and outdoor scenes, demonstrating its real-world robustness and\napplicability. Supplementary videos are available at\nhttps://trailab.github.io/OpenNav-website/", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5229\u7528\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLMs\uff09\u7ed3\u5408\u89c6\u89c9\u8bed\u8a00\u611f\u77e5\u6a21\u578b\u7684\u65b9\u6cd5\uff0c\u4f7f\u673a\u5668\u4eba\u80fd\u591f\u89e3\u6790\u590d\u6742\u8bed\u8a00\u6307\u4ee4\u5e76\u751f\u6210\u8f68\u8ff9\u70b9\uff0c\u5b8c\u6210\u5f00\u653e\u4e16\u754c\u4e2d\u7684\u5bfc\u822a\u4efb\u52a1\u3002", "motivation": "\u5c3d\u7ba1\u9884\u8bad\u7ec3\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u5e38\u8bc6\u63a8\u7406\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5728\u5f00\u653e\u4e16\u754c\u4e2d\u5b9e\u73b0\u8bed\u8a00\u63cf\u8ff0\u5230\u5b9e\u9645\u673a\u5668\u4eba\u52a8\u4f5c\u7684\u8f6c\u6362\u4ecd\u5177\u6311\u6218\u6027\u3002", "method": "\u5229\u7528MLLMs\u7684\u8de8\u6a21\u6001\u7406\u89e3\u548c\u4ee3\u7801\u751f\u6210\u80fd\u529b\uff0c\u7ed3\u5408\u89c6\u89c9\u8bed\u8a00\u611f\u77e5\u6a21\u578b\u751f\u62102D\u9e1f\u77b0\u4ef7\u503c\u56fe\uff0c\u6574\u5408\u8bed\u4e49\u77e5\u8bc6\u4e0e\u7a7a\u95f4\u4fe1\u606f\u3002", "result": "\u5728\u6237\u5916\u5bfc\u822a\u4efb\u52a1\u4e2d\u9a8c\u8bc1\u4e86\u96f6\u6837\u672c\u89c6\u89c9\u8bed\u8a00\u5bfc\u822a\u6846\u67b6\u7684\u53ef\u884c\u6027\uff0c\u5e76\u5728\u5ba4\u5185\u5916\u573a\u666f\u4e2d\u5c55\u793a\u4e86\u7cfb\u7edf\u7684\u9c81\u68d2\u6027\u548c\u5b9e\u7528\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u80fd\u591f\u5904\u7406\u591a\u6837\u5316\u7684\u81ea\u7136\u8bed\u8a00\u5bfc\u822a\u6307\u4ee4\uff0c\u5e76\u5728\u5b9e\u9645\u673a\u5668\u4eba\u5e73\u53f0\u4e0a\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002"}}
{"id": "2507.17941", "pdf": "https://arxiv.org/pdf/2507.17941", "abs": "https://arxiv.org/abs/2507.17941", "authors": ["Quoc Thinh Vo", "David Han"], "title": "Resnet-conformer network with shared weights and attention mechanism for sound event localization, detection, and distance estimation", "categories": ["cs.SD", "eess.AS"], "comment": "This paper has been submitted as a technical report outlining our\n  approach to Task 3A of the Detection and Classification of Acoustic Scenes\n  and Events (DCASE) 2024 and can be found in DCASE2024 technical reports", "summary": "This technical report outlines our approach to Task 3A of the Detection and\nClassification of Acoustic Scenes and Events (DCASE) 2024, focusing on Sound\nEvent Localization and Detection (SELD). SELD provides valuable insights by\nestimating sound event localization and detection, aiding in various machine\ncognition tasks such as environmental inference, navigation, and other sound\nlocalization-related applications. This year's challenge evaluates models using\neither audio-only (Track A) or audiovisual (Track B) inputs on annotated\nrecordings of real sound scenes. A notable change this year is the introduction\nof distance estimation, with evaluation metrics adjusted accordingly for a\ncomprehensive assessment. Our submission is for Task A of the Challenge, which\nfocuses on the audio-only track. Our approach utilizes log-mel spectrograms,\nintensity vectors, and employs multiple data augmentations. We proposed an\nEINV2-based [1] network architecture, achieving improved results: an F-score of\n40.2%, Angular Error (DOA) of 17.7 degrees, and Relative Distance Error (RDE)\nof 0.32 on the test set of the Development Dataset [2 ,3].", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86DCASE 2024\u4efb\u52a13A\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u4e13\u6ce8\u4e8e\u58f0\u97f3\u4e8b\u4ef6\u5b9a\u4f4d\u4e0e\u68c0\u6d4b\uff08SELD\uff09\uff0c\u91c7\u7528\u97f3\u9891\u8f93\u5165\u548cEINV2\u7f51\u7edc\u67b6\u6784\uff0c\u53d6\u5f97\u4e86\u663e\u8457\u7ed3\u679c\u3002", "motivation": "SELD\u5728\u673a\u5668\u8ba4\u77e5\u4efb\u52a1\uff08\u5982\u73af\u5883\u63a8\u65ad\u548c\u5bfc\u822a\uff09\u4e2d\u5177\u6709\u91cd\u8981\u4ef7\u503c\uff0cDCASE 2024\u6311\u6218\u8d5b\u65b0\u589e\u4e86\u8ddd\u79bb\u4f30\u8ba1\u4efb\u52a1\uff0c\u9700\u8981\u66f4\u5168\u9762\u7684\u8bc4\u4f30\u65b9\u6cd5\u3002", "method": "\u4f7f\u7528log-mel\u9891\u8c31\u56fe\u548c\u5f3a\u5ea6\u5411\u91cf\uff0c\u7ed3\u5408\u591a\u79cd\u6570\u636e\u589e\u5f3a\u6280\u672f\uff0c\u63d0\u51fa\u57fa\u4e8eEINV2\u7684\u7f51\u7edc\u67b6\u6784\u3002", "result": "\u5728\u6d4b\u8bd5\u96c6\u4e0a\u53d6\u5f97\u4e86F-score 40.2%\uff0c\u89d2\u5ea6\u8bef\u5dee17.7\u5ea6\uff0c\u76f8\u5bf9\u8ddd\u79bb\u8bef\u5dee0.32\u7684\u6539\u8fdb\u7ed3\u679c\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u97f3\u9891\u8f93\u5165\u7684\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4e3aSELD\u4efb\u52a1\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.18284", "pdf": "https://arxiv.org/pdf/2507.18284", "abs": "https://arxiv.org/abs/2507.18284", "authors": ["Astrid Rakow", "Joe Collenette", "Maike Schwammberger", "Marija Slavkovik", "Gleifer Vs Alves"], "title": "Designing Value-Aligned Traffic Agents through Conflict Sensitivity", "categories": ["cs.MA"], "comment": "Short version of this paper has been accepted at EUMAS 2025\n  https://euramas.github.io/eumas2025/", "summary": "Autonomous traffic agents (ATAs) are expected to act in ways tat are not only\nsafe, but also aligned with stakeholder values across legal, social, and moral\ndimensions. In this paper, we adopt an established formal model of conflict\nfrom epistemic game theory to support the development of such agents. We focus\non value conflicts-situations in which agents face competing goals rooted in\nvalue-laden situations and show how conflict analysis can inform key phases of\nthe design process. This includes value elicitation, capability specification,\nexplanation, and adaptive system refinement. We elaborate and apply the concept\nof Value-Aligned Operational Design Domains (VODDs) to structure autonomy in\naccordance with contextual value priorities. Our approach shifts the emphasis\nfrom solving moral dilemmas at runtime to anticipating and structuring\nvalue-sensitive behaviour during development.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u8ba4\u77e5\u535a\u5f08\u8bba\u7684\u5f62\u5f0f\u5316\u6a21\u578b\uff0c\u7528\u4e8e\u5f00\u53d1\u7b26\u5408\u591a\u65b9\u5229\u76ca\uff08\u6cd5\u5f8b\u3001\u793e\u4f1a\u3001\u9053\u5fb7\uff09\u7684\u81ea\u4e3b\u4ea4\u901a\u4ee3\u7406\uff08ATAs\uff09\uff0c\u91cd\u70b9\u89e3\u51b3\u4ef7\u503c\u51b2\u7a81\u95ee\u9898\u3002", "motivation": "\u786e\u4fdd\u81ea\u4e3b\u4ea4\u901a\u4ee3\u7406\u7684\u884c\u4e3a\u4e0d\u4ec5\u5b89\u5168\uff0c\u8fd8\u80fd\u5728\u591a\u7ef4\u5ea6\uff08\u6cd5\u5f8b\u3001\u793e\u4f1a\u3001\u9053\u5fb7\uff09\u4e0a\u4e0e\u5229\u76ca\u76f8\u5173\u8005\u7684\u4ef7\u503c\u89c2\u4fdd\u6301\u4e00\u81f4\u3002", "method": "\u91c7\u7528\u8ba4\u77e5\u535a\u5f08\u8bba\u4e2d\u7684\u51b2\u7a81\u6a21\u578b\uff0c\u5206\u6790\u4ef7\u503c\u51b2\u7a81\uff0c\u5e76\u5c06\u5176\u5e94\u7528\u4e8e\u8bbe\u8ba1\u6d41\u7a0b\u4e2d\u7684\u4ef7\u503c\u63d0\u53d6\u3001\u80fd\u529b\u89c4\u8303\u3001\u89e3\u91ca\u548c\u7cfb\u7edf\u4f18\u5316\u3002", "result": "\u63d0\u51fa\u4e86\u4ef7\u503c\u5bf9\u9f50\u64cd\u4f5c\u8bbe\u8ba1\u57df\uff08VODDs\uff09\u7684\u6982\u5ff5\uff0c\u7528\u4e8e\u5728\u5f00\u53d1\u9636\u6bb5\u9884\u7ed3\u6784\u548c\u4f18\u5316\u4ef7\u503c\u654f\u611f\u884c\u4e3a\u3002", "conclusion": "\u901a\u8fc7\u63d0\u524d\u5206\u6790\u548c\u7ed3\u6784\u5316\u4ef7\u503c\u51b2\u7a81\uff0c\u5c06\u9053\u5fb7\u56f0\u5883\u7684\u89e3\u51b3\u4ece\u8fd0\u884c\u65f6\u8f6c\u79fb\u5230\u5f00\u53d1\u9636\u6bb5\uff0c\u4ece\u800c\u66f4\u6709\u6548\u5730\u5b9e\u73b0\u4ef7\u503c\u5bf9\u9f50\u3002"}}
{"id": "2507.17785", "pdf": "https://arxiv.org/pdf/2507.17785", "abs": "https://arxiv.org/abs/2507.17785", "authors": ["Jingyi Ding", "Chengwen Qi", "Hongfei Wang", "Jianshe Wu", "Licheng Jiao", "Yuwei Guo", "Jian Gao"], "title": "Self-similarity Analysis in Deep Neural Networks", "categories": ["cs.LG"], "comment": null, "summary": "Current research has found that some deep neural networks exhibit strong\nhierarchical self-similarity in feature representation or parameter\ndistribution. However, aside from preliminary studies on how the power-law\ndistribution of weights across different training stages affects model\nperformance,there has been no quantitative analysis on how the self-similarity\nof hidden space geometry influences model weight optimization, nor is there a\nclear understanding of the dynamic behavior of internal neurons. Therefore,\nthis paper proposes a complex network modeling method based on the output\nfeatures of hidden-layer neurons to investigate the self-similarity of feature\nnetworks constructed at different hidden layers, and analyzes how adjusting the\ndegree of self-similarity in feature networks can enhance the classification\nperformance of deep neural networks. Validated on three types of networks MLP\narchitectures, convolutional networks, and attention architectures this study\nreveals that the degree of self-similarity exhibited by feature networks varies\nacross different model architectures. Furthermore, embedding constraints on the\nself-similarity of feature networks during the training process can improve the\nperformance of self-similar deep neural networks (MLP architectures and\nattention architectures) by up to 6 percentage points.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u9690\u85cf\u5c42\u795e\u7ecf\u5143\u8f93\u51fa\u7279\u5f81\u7684\u590d\u6742\u7f51\u7edc\u5efa\u6a21\u65b9\u6cd5\uff0c\u7814\u7a76\u4e0d\u540c\u9690\u85cf\u5c42\u6784\u5efa\u7684\u7279\u5f81\u7f51\u7edc\u7684\u81ea\u76f8\u4f3c\u6027\uff0c\u5e76\u5206\u6790\u8c03\u6574\u81ea\u76f8\u4f3c\u5ea6\u5982\u4f55\u63d0\u5347\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u7684\u5206\u7c7b\u6027\u80fd\u3002", "motivation": "\u5f53\u524d\u7814\u7a76\u53d1\u73b0\u67d0\u4e9b\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u5728\u7279\u5f81\u8868\u793a\u6216\u53c2\u6570\u5206\u5e03\u4e0a\u8868\u73b0\u51fa\u5f3a\u70c8\u7684\u5c42\u6b21\u81ea\u76f8\u4f3c\u6027\uff0c\u4f46\u7f3a\u4e4f\u5bf9\u9690\u85cf\u7a7a\u95f4\u51e0\u4f55\u81ea\u76f8\u4f3c\u6027\u5982\u4f55\u5f71\u54cd\u6a21\u578b\u6743\u91cd\u4f18\u5316\u7684\u5b9a\u91cf\u5206\u6790\uff0c\u4ee5\u53ca\u5bf9\u5185\u90e8\u795e\u7ecf\u5143\u52a8\u6001\u884c\u4e3a\u7684\u6e05\u6670\u7406\u89e3\u3002", "method": "\u63d0\u51fa\u57fa\u4e8e\u9690\u85cf\u5c42\u795e\u7ecf\u5143\u8f93\u51fa\u7279\u5f81\u7684\u590d\u6742\u7f51\u7edc\u5efa\u6a21\u65b9\u6cd5\uff0c\u7814\u7a76\u4e0d\u540c\u9690\u85cf\u5c42\u7279\u5f81\u7f51\u7edc\u7684\u81ea\u76f8\u4f3c\u6027\uff0c\u5e76\u8c03\u6574\u81ea\u76f8\u4f3c\u5ea6\u4ee5\u4f18\u5316\u6a21\u578b\u6027\u80fd\u3002", "result": "\u5728MLP\u3001\u5377\u79ef\u7f51\u7edc\u548c\u6ce8\u610f\u529b\u67b6\u6784\u4e0a\u9a8c\u8bc1\u53d1\u73b0\uff0c\u4e0d\u540c\u6a21\u578b\u67b6\u6784\u7684\u7279\u5f81\u7f51\u7edc\u81ea\u76f8\u4f3c\u5ea6\u5404\u5f02\uff1b\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u5d4c\u5165\u81ea\u76f8\u4f3c\u6027\u7ea6\u675f\u53ef\u63d0\u5347\u81ea\u76f8\u4f3c\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\uff08MLP\u548c\u6ce8\u610f\u529b\u67b6\u6784\uff09\u6027\u80fd\u9ad8\u8fbe6\u4e2a\u767e\u5206\u70b9\u3002", "conclusion": "\u8c03\u6574\u7279\u5f81\u7f51\u7edc\u7684\u81ea\u76f8\u4f3c\u5ea6\u80fd\u663e\u8457\u63d0\u5347\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u7684\u5206\u7c7b\u6027\u80fd\uff0c\u5c24\u5176\u5728MLP\u548c\u6ce8\u610f\u529b\u67b6\u6784\u4e2d\u6548\u679c\u660e\u663e\u3002"}}
{"id": "2507.17896", "pdf": "https://arxiv.org/pdf/2507.17896", "abs": "https://arxiv.org/abs/2507.17896", "authors": ["Shubham Mohole", "Sainyam Galhotra"], "title": "VeriMinder: Mitigating Analytical Vulnerabilities in NL2SQL", "categories": ["cs.CL", "cs.AI", "cs.DB"], "comment": null, "summary": "Application systems using natural language interfaces to databases (NLIDBs)\nhave democratized data analysis. This positive development has also brought\nforth an urgent challenge to help users who might use these systems without a\nbackground in statistical analysis to formulate bias-free analytical questions.\nAlthough significant research has focused on text-to-SQL generation accuracy,\naddressing cognitive biases in analytical questions remains underexplored. We\npresent VeriMinder, https://veriminder.ai, an interactive system for detecting\nand mitigating such analytical vulnerabilities. Our approach introduces three\nkey innovations: (1) a contextual semantic mapping framework for biases\nrelevant to specific analysis contexts (2) an analytical framework that\noperationalizes the Hard-to-Vary principle and guides users in systematic data\nanalysis (3) an optimized LLM-powered system that generates high-quality,\ntask-specific prompts using a structured process involving multiple candidates,\ncritic feedback, and self-reflection.\n  User testing confirms the merits of our approach. In direct user experience\nevaluation, 82.5% participants reported positively impacting the quality of the\nanalysis. In comparative evaluation, VeriMinder scored significantly higher\nthan alternative approaches, at least 20% better when considered for metrics of\nthe analysis's concreteness, comprehensiveness, and accuracy. Our system,\nimplemented as a web application, is set to help users avoid \"wrong question\"\nvulnerability during data analysis. VeriMinder code base with prompts,\nhttps://reproducibility.link/veriminder, is available as an MIT-licensed\nopen-source software to facilitate further research and adoption within the\ncommunity.", "AI": {"tldr": "VeriMinder\u662f\u4e00\u4e2a\u4ea4\u4e92\u5f0f\u7cfb\u7edf\uff0c\u7528\u4e8e\u68c0\u6d4b\u548c\u7f13\u89e3\u81ea\u7136\u8bed\u8a00\u6570\u636e\u5e93\u63a5\u53e3\u4e2d\u7684\u8ba4\u77e5\u504f\u5dee\uff0c\u901a\u8fc7\u8bed\u4e49\u6620\u5c04\u3001\u5206\u6790\u6846\u67b6\u548c\u4f18\u5316\u7684LLM\u751f\u6210\u63d0\u793a\uff0c\u663e\u8457\u63d0\u5347\u5206\u6790\u8d28\u91cf\u3002", "motivation": "\u5e2e\u52a9\u7f3a\u4e4f\u7edf\u8ba1\u80cc\u666f\u7684\u7528\u6237\u5728\u81ea\u7136\u8bed\u8a00\u6570\u636e\u5e93\u63a5\u53e3\u4e2d\u907f\u514d\u8ba4\u77e5\u504f\u5dee\uff0c\u63d0\u5347\u5206\u6790\u95ee\u9898\u7684\u8d28\u91cf\u3002", "method": "1. \u4e0a\u4e0b\u6587\u8bed\u4e49\u6620\u5c04\u6846\u67b6\uff1b2. \u57fa\u4e8eHard-to-Vary\u539f\u5219\u7684\u5206\u6790\u6846\u67b6\uff1b3. \u4f18\u5316\u7684LLM\u751f\u6210\u63d0\u793a\u7cfb\u7edf\u3002", "result": "\u7528\u6237\u6d4b\u8bd5\u663e\u793a82.5%\u7684\u53c2\u4e0e\u8005\u8ba4\u53ef\u5176\u63d0\u5347\u5206\u6790\u8d28\u91cf\u7684\u6548\u679c\uff0c\u4e14\u5728\u5177\u4f53\u6027\u3001\u5168\u9762\u6027\u548c\u51c6\u786e\u6027\u4e0a\u4f18\u4e8e\u5176\u4ed6\u65b9\u6cd5\u81f3\u5c1120%\u3002", "conclusion": "VeriMinder\u6709\u6548\u89e3\u51b3\u4e86\u201c\u9519\u8bef\u95ee\u9898\u201d\u6f0f\u6d1e\uff0c\u5176\u5f00\u6e90\u4ee3\u7801\u548c\u63d0\u793a\u5e93\u6709\u52a9\u4e8e\u8fdb\u4e00\u6b65\u7814\u7a76\u548c\u793e\u533a\u91c7\u7528\u3002"}}
{"id": "2507.17853", "pdf": "https://arxiv.org/pdf/2507.17853", "abs": "https://arxiv.org/abs/2507.17853", "authors": ["Lifeng Chen", "Jiner Wang", "Zihao Pan", "Beier Zhu", "Xiaofeng Yang", "Chi Zhang"], "title": "Detail++: Training-Free Detail Enhancer for Text-to-Image Diffusion Models", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Recent advances in text-to-image (T2I) generation have led to impressive\nvisual results. However, these models still face significant challenges when\nhandling complex prompt, particularly those involving multiple subjects with\ndistinct attributes. Inspired by the human drawing process, which first\noutlines the composition and then incrementally adds details, we propose\nDetail++, a training-free framework that introduces a novel Progressive Detail\nInjection (PDI) strategy to address this limitation. Specifically, we decompose\na complex prompt into a sequence of simplified sub-prompts, guiding the\ngeneration process in stages. This staged generation leverages the inherent\nlayout-controlling capacity of self-attention to first ensure global\ncomposition, followed by precise refinement. To achieve accurate binding\nbetween attributes and corresponding subjects, we exploit cross-attention\nmechanisms and further introduce a Centroid Alignment Loss at test time to\nreduce binding noise and enhance attribute consistency. Extensive experiments\non T2I-CompBench and a newly constructed style composition benchmark\ndemonstrate that Detail++ significantly outperforms existing methods,\nparticularly in scenarios involving multiple objects and complex stylistic\nconditions.", "AI": {"tldr": "Detail++\u662f\u4e00\u4e2a\u65e0\u9700\u8bad\u7ec3\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u6e10\u8fdb\u5f0f\u7ec6\u8282\u6ce8\u5165\u7b56\u7565\uff08PDI\uff09\u89e3\u51b3\u590d\u6742\u63d0\u793a\u4e0b\u591a\u4e3b\u4f53\u751f\u6210\u7684\u6311\u6218\u3002", "motivation": "\u73b0\u6709\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u6a21\u578b\u5728\u590d\u6742\u63d0\u793a\uff08\u591a\u4e3b\u4f53\u53ca\u5c5e\u6027\uff09\u4e0b\u8868\u73b0\u4e0d\u4f73\uff0c\u53d7\u4eba\u7c7b\u7ed8\u753b\u8fc7\u7a0b\u542f\u53d1\uff0c\u63d0\u51fa\u5206\u9636\u6bb5\u751f\u6210\u65b9\u6cd5\u3002", "method": "\u5c06\u590d\u6742\u63d0\u793a\u5206\u89e3\u4e3a\u5b50\u63d0\u793a\uff0c\u5206\u9636\u6bb5\u751f\u6210\uff1b\u5229\u7528\u81ea\u6ce8\u610f\u529b\u63a7\u5236\u5e03\u5c40\uff0c\u4ea4\u53c9\u6ce8\u610f\u529b\u7ed1\u5b9a\u5c5e\u6027\uff0c\u5f15\u5165\u8d28\u5fc3\u5bf9\u9f50\u635f\u5931\u51cf\u5c11\u566a\u58f0\u3002", "result": "\u5728T2I-CompBench\u548c\u65b0\u98ce\u683c\u7ec4\u5408\u57fa\u51c6\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5c24\u5176\u5728\u591a\u5bf9\u8c61\u548c\u590d\u6742\u98ce\u683c\u573a\u666f\u3002", "conclusion": "Detail++\u901a\u8fc7\u5206\u9636\u6bb5\u751f\u6210\u548c\u5c5e\u6027\u7ed1\u5b9a\u4f18\u5316\uff0c\u663e\u8457\u63d0\u5347\u4e86\u590d\u6742\u63d0\u793a\u4e0b\u7684\u56fe\u50cf\u751f\u6210\u8d28\u91cf\u3002"}}
{"id": "2507.18059", "pdf": "https://arxiv.org/pdf/2507.18059", "abs": "https://arxiv.org/abs/2507.18059", "authors": ["Yueheng Li", "Guangming Xie", "Zongqing Lu"], "title": "Multi-Agent Guided Policy Optimization", "categories": ["cs.AI", "cs.MA"], "comment": null, "summary": "Due to practical constraints such as partial observability and limited\ncommunication, Centralized Training with Decentralized Execution (CTDE) has\nbecome the dominant paradigm in cooperative Multi-Agent Reinforcement Learning\n(MARL). However, existing CTDE methods often underutilize centralized training\nor lack theoretical guarantees. We propose Multi-Agent Guided Policy\nOptimization (MAGPO), a novel framework that better leverages centralized\ntraining by integrating centralized guidance with decentralized execution.\nMAGPO uses an auto-regressive joint policy for scalable, coordinated\nexploration and explicitly aligns it with decentralized policies to ensure\ndeployability under partial observability. We provide theoretical guarantees of\nmonotonic policy improvement and empirically evaluate MAGPO on 43 tasks across\n6 diverse environments. Results show that MAGPO consistently outperforms strong\nCTDE baselines and matches or surpasses fully centralized approaches, offering\na principled and practical solution for decentralized multi-agent learning. Our\ncode and experimental data can be found in https://github.com/liyheng/MAGPO.", "AI": {"tldr": "MAGPO\u662f\u4e00\u79cd\u65b0\u578b\u7684\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u7ed3\u5408\u96c6\u4e2d\u5f0f\u6307\u5bfc\u548c\u5206\u6563\u5f0f\u6267\u884c\uff0c\u4f18\u5316\u4e86CTDE\u8303\u5f0f\uff0c\u63d0\u4f9b\u4e86\u7406\u8bba\u4fdd\u8bc1\u548c\u5b9e\u9645\u6027\u80fd\u63d0\u5347\u3002", "motivation": "\u73b0\u6709CTDE\u65b9\u6cd5\u672a\u80fd\u5145\u5206\u5229\u7528\u96c6\u4e2d\u5f0f\u8bad\u7ec3\u6216\u7f3a\u4e4f\u7406\u8bba\u4fdd\u8bc1\uff0cMAGPO\u65e8\u5728\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "MAGPO\u91c7\u7528\u81ea\u56de\u5f52\u8054\u5408\u7b56\u7565\u8fdb\u884c\u53ef\u6269\u5c55\u7684\u534f\u8c03\u63a2\u7d22\uff0c\u5e76\u660e\u786e\u4e0e\u5206\u6563\u7b56\u7565\u5bf9\u9f50\u4ee5\u786e\u4fdd\u53ef\u90e8\u7f72\u6027\u3002", "result": "\u572843\u4e2a\u4efb\u52a1\u548c6\u4e2a\u73af\u5883\u4e2d\uff0cMAGPO\u8868\u73b0\u4f18\u4e8e\u73b0\u6709CTDE\u57fa\u7ebf\uff0c\u751a\u81f3\u5ab2\u7f8e\u5b8c\u5168\u96c6\u4e2d\u5f0f\u65b9\u6cd5\u3002", "conclusion": "MAGPO\u4e3a\u5206\u6563\u5f0f\u591a\u667a\u80fd\u4f53\u5b66\u4e60\u63d0\u4f9b\u4e86\u7406\u8bba\u548c\u5b9e\u8df5\u4e0a\u7684\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.18632", "pdf": "https://arxiv.org/pdf/2507.18632", "abs": "https://arxiv.org/abs/2507.18632", "authors": ["Ye-Chan Kim", "SeungJu Cha", "Si-Woo Kim", "Taewhan Kim", "Dong-Jin Kim"], "title": "SIDA: Synthetic Image Driven Zero-shot Domain Adaptation", "categories": ["cs.CV", "cs.AI", "cs.LG", "cs.MM"], "comment": "Accepted to ACM MM 2025", "summary": "Zero-shot domain adaptation is a method for adapting a model to a target\ndomain without utilizing target domain image data. To enable adaptation without\ntarget images, existing studies utilize CLIP's embedding space and text\ndescription to simulate target-like style features. Despite the previous\nachievements in zero-shot domain adaptation, we observe that these text-driven\nmethods struggle to capture complex real-world variations and significantly\nincrease adaptation time due to their alignment process. Instead of relying on\ntext descriptions, we explore solutions leveraging image data, which provides\ndiverse and more fine-grained style cues. In this work, we propose SIDA, a\nnovel and efficient zero-shot domain adaptation method leveraging synthetic\nimages. To generate synthetic images, we first create detailed, source-like\nimages and apply image translation to reflect the style of the target domain.\nWe then utilize the style features of these synthetic images as a proxy for the\ntarget domain. Based on these features, we introduce Domain Mix and Patch Style\nTransfer modules, which enable effective modeling of real-world variations. In\nparticular, Domain Mix blends multiple styles to expand the intra-domain\nrepresentations, and Patch Style Transfer assigns different styles to\nindividual patches. We demonstrate the effectiveness of our method by showing\nstate-of-the-art performance in diverse zero-shot adaptation scenarios,\nparticularly in challenging domains. Moreover, our approach achieves high\nefficiency by significantly reducing the overall adaptation time.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5408\u6210\u56fe\u50cf\u7684\u96f6\u6837\u672c\u57df\u9002\u5e94\u65b9\u6cd5SIDA\uff0c\u901a\u8fc7\u751f\u6210\u76ee\u6807\u57df\u98ce\u683c\u7684\u5408\u6210\u56fe\u50cf\uff0c\u7ed3\u5408Domain Mix\u548cPatch Style Transfer\u6a21\u5757\uff0c\u9ad8\u6548\u5efa\u6a21\u771f\u5b9e\u4e16\u754c\u53d8\u5316\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u6587\u672c\u63cf\u8ff0\u7684\u65b9\u6cd5\u96be\u4ee5\u6355\u6349\u590d\u6742\u771f\u5b9e\u4e16\u754c\u53d8\u5316\u4e14\u8017\u65f6\uff0c\u56e0\u6b64\u63a2\u7d22\u5229\u7528\u56fe\u50cf\u6570\u636e\u63d0\u4f9b\u66f4\u7ec6\u7c92\u5ea6\u7684\u98ce\u683c\u7ebf\u7d22\u3002", "method": "\u901a\u8fc7\u751f\u6210\u6e90\u57df\u98ce\u683c\u7684\u8be6\u7ec6\u56fe\u50cf\u5e76\u5e94\u7528\u56fe\u50cf\u7ffb\u8bd1\u751f\u6210\u76ee\u6807\u57df\u98ce\u683c\u7684\u5408\u6210\u56fe\u50cf\uff0c\u5229\u7528\u5176\u98ce\u683c\u7279\u5f81\u4f5c\u4e3a\u76ee\u6807\u57df\u4ee3\u7406\uff0c\u7ed3\u5408Domain Mix\u548cPatch Style Transfer\u6a21\u5757\u3002", "result": "\u5728\u591a\u79cd\u96f6\u6837\u672c\u9002\u5e94\u573a\u666f\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u5c24\u5176\u5728\u6311\u6218\u6027\u9886\u57df\uff0c\u540c\u65f6\u663e\u8457\u51cf\u5c11\u9002\u5e94\u65f6\u95f4\u3002", "conclusion": "SIDA\u901a\u8fc7\u5408\u6210\u56fe\u50cf\u548c\u9ad8\u6548\u6a21\u5757\u8bbe\u8ba1\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u4e14\u9ad8\u6027\u80fd\u7684\u96f6\u6837\u672c\u57df\u9002\u5e94\u3002"}}
{"id": "2507.18070", "pdf": "https://arxiv.org/pdf/2507.18070", "abs": "https://arxiv.org/abs/2507.18070", "authors": ["Behzad Zamani", "Jochen Trumpf", "Chris Manzie"], "title": "Modular Robot and Landmark Localisation Using Relative Bearing Measurements", "categories": ["cs.RO", "cs.SY", "eess.SP", "eess.SY"], "comment": "Submitted to RA-L", "summary": "In this paper we propose a modular nonlinear least squares filtering approach\nfor systems composed of independent subsystems. The state and error covariance\nestimate of each subsystem is updated independently, even when a relative\nmeasurement simultaneously depends on the states of multiple subsystems. We\nintegrate the Covariance Intersection (CI) algorithm as part of our solution in\norder to prevent double counting of information when subsystems share estimates\nwith each other. An alternative derivation of the CI algorithm based on least\nsquares estimation makes this integration possible. We particularise the\nproposed approach to the robot-landmark localization problem. In this problem,\nnoisy measurements of the bearing angle to a stationary landmark position\nmeasured relative to the SE(2) pose of a moving robot couple the estimation\nproblems for the robot pose and the landmark position. In a randomized\nsimulation study, we benchmark the proposed modular method against a monolithic\njoint state filter to elucidate their respective trade-offs. In this study we\nalso include variants of the proposed method that achieve a graceful\ndegradation of performance with reduced communication and bandwidth\nrequirements.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u6a21\u5757\u5316\u975e\u7ebf\u6027\u6700\u5c0f\u4e8c\u4e58\u6ee4\u6ce2\u65b9\u6cd5\uff0c\u7528\u4e8e\u72ec\u7acb\u5b50\u7cfb\u7edf\u7ec4\u6210\u7684\u7cfb\u7edf\uff0c\u901a\u8fc7\u534f\u65b9\u5dee\u4ea4\u96c6\u7b97\u6cd5\u907f\u514d\u4fe1\u606f\u91cd\u590d\u8ba1\u7b97\uff0c\u5e76\u5728\u673a\u5668\u4eba-\u5730\u6807\u5b9a\u4f4d\u95ee\u9898\u4e2d\u9a8c\u8bc1\u5176\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u7531\u72ec\u7acb\u5b50\u7cfb\u7edf\u7ec4\u6210\u7684\u7cfb\u7edf\u4e2d\u72b6\u6001\u548c\u8bef\u5dee\u534f\u65b9\u5dee\u4f30\u8ba1\u7684\u72ec\u7acb\u66f4\u65b0\u95ee\u9898\uff0c\u7279\u522b\u662f\u5728\u76f8\u5bf9\u6d4b\u91cf\u540c\u65f6\u4f9d\u8d56\u591a\u4e2a\u5b50\u7cfb\u7edf\u72b6\u6001\u65f6\u3002", "method": "\u91c7\u7528\u6a21\u5757\u5316\u975e\u7ebf\u6027\u6700\u5c0f\u4e8c\u4e58\u6ee4\u6ce2\u65b9\u6cd5\uff0c\u7ed3\u5408\u534f\u65b9\u5dee\u4ea4\u96c6\uff08CI\uff09\u7b97\u6cd5\uff0c\u907f\u514d\u4fe1\u606f\u91cd\u590d\u8ba1\u7b97\uff0c\u5e76\u5728\u673a\u5668\u4eba-\u5730\u6807\u5b9a\u4f4d\u95ee\u9898\u4e2d\u5177\u4f53\u5e94\u7528\u3002", "result": "\u901a\u8fc7\u968f\u673a\u6a21\u62df\u7814\u7a76\uff0c\u9a8c\u8bc1\u4e86\u6a21\u5757\u5316\u65b9\u6cd5\u76f8\u5bf9\u4e8e\u8054\u5408\u72b6\u6001\u6ee4\u6ce2\u5668\u7684\u6027\u80fd\u6743\u8861\uff0c\u5e76\u5c55\u793a\u4e86\u5728\u51cf\u5c11\u901a\u4fe1\u548c\u5e26\u5bbd\u9700\u6c42\u65f6\u7684\u6027\u80fd\u9000\u5316\u60c5\u51b5\u3002", "conclusion": "\u6a21\u5757\u5316\u65b9\u6cd5\u5728\u72ec\u7acb\u5b50\u7cfb\u7edf\u7cfb\u7edf\u4e2d\u6709\u6548\uff0c\u534f\u65b9\u5dee\u4ea4\u96c6\u7b97\u6cd5\u7684\u96c6\u6210\u907f\u514d\u4e86\u4fe1\u606f\u91cd\u590d\u8ba1\u7b97\uff0c\u9002\u7528\u4e8e\u673a\u5668\u4eba-\u5730\u6807\u5b9a\u4f4d\u7b49\u95ee\u9898\u3002"}}
{"id": "2507.18051", "pdf": "https://arxiv.org/pdf/2507.18051", "abs": "https://arxiv.org/abs/2507.18051", "authors": ["Hongfei Xue", "Kaixun Huang", "Zhikai Zhou", "Shen Huang", "Shidong Shang"], "title": "The TEA-ASLP System for Multilingual Conversational Speech Recognition and Speech Diarization in MLC-SLM 2025 Challenge", "categories": ["cs.SD", "eess.AS"], "comment": "Interspeech 2025 workshop", "summary": "This paper presents the TEA-ASLP's system submitted to the MLC-SLM 2025\nChallenge, addressing multilingual conversational automatic speech recognition\n(ASR) in Task I and speech diarization ASR in Task II. For Task I, we enhance\nIdeal-LLM model by integrating known language identification and a multilingual\nMOE LoRA structure, along with using CTC-predicted tokens as prompts to improve\nautoregressive generation. The model is trained on approximately 180k hours of\nmultilingual ASR data. In Task II, we replace the baseline English-Chinese\nspeaker diarization model with a more suitable English-only version. Our\napproach achieves a 30.8% reduction in word error rate (WER) compared to the\nbaseline speech language model, resulting in a final WER of 9.60% in Task I and\na time-constrained minimum-permutation WER of 17.49% in Task II, earning first\nand second place in the respective challenge tasks.", "AI": {"tldr": "TEA-ASLP\u7cfb\u7edf\u5728MLC-SLM 2025\u6311\u6218\u8d5b\u4e2d\uff0c\u901a\u8fc7\u6539\u8fdbIdeal-LLM\u6a21\u578b\u548c\u4f18\u5316\u8bf4\u8bdd\u4eba\u65e5\u5fd7ASR\u6a21\u578b\uff0c\u5206\u522b\u5728\u591a\u8bed\u8a00\u5bf9\u8bddASR\u548c\u8bed\u97f3\u65e5\u5fd7ASR\u4efb\u52a1\u4e2d\u53d6\u5f97\u663e\u8457\u6027\u80fd\u63d0\u5347\u3002", "motivation": "\u89e3\u51b3\u591a\u8bed\u8a00\u5bf9\u8bddASR\u548c\u8bed\u97f3\u65e5\u5fd7ASR\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\u95ee\u9898\uff0c\u63d0\u5347\u81ea\u52a8\u8bed\u97f3\u8bc6\u522b\u7684\u51c6\u786e\u6027\u548c\u6548\u7387\u3002", "method": "\u5728Task I\u4e2d\uff0c\u7ed3\u5408\u5df2\u77e5\u8bed\u8a00\u8bc6\u522b\u548c\u591a\u8bed\u8a00MOE LoRA\u7ed3\u6784\uff0c\u4f7f\u7528CTC\u9884\u6d4b\u6807\u8bb0\u4f5c\u4e3a\u63d0\u793a\uff1b\u5728Task II\u4e2d\uff0c\u66ff\u6362\u57fa\u7ebf\u6a21\u578b\u4e3a\u66f4\u9002\u5408\u7684\u82f1\u8bed\u7248\u672c\u3002", "result": "Task I\u7684WER\u964d\u4f4e30.8%\uff0c\u6700\u7ec8WER\u4e3a9.60%\uff1bTask II\u7684\u65f6\u95f4\u7ea6\u675f\u6700\u5c0f\u6392\u5217WER\u4e3a17.49%\uff0c\u5206\u522b\u5728\u6311\u6218\u8d5b\u4e2d\u6392\u540d\u7b2c\u4e00\u548c\u7b2c\u4e8c\u3002", "conclusion": "\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u591a\u8bed\u8a00\u548c\u8bed\u97f3\u65e5\u5fd7ASR\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\u3002"}}
{"id": "2507.17990", "pdf": "https://arxiv.org/pdf/2507.17990", "abs": "https://arxiv.org/abs/2507.17990", "authors": ["Takumi Kato", "Zhi Li Hu"], "title": "Rapid Modeling Architecture for Lightweight Simulator to Accelerate and Improve Decision Making for Industrial Systems", "categories": ["eess.SY", "cs.MA", "cs.RO", "cs.SY"], "comment": "8 pages, 13 figures. Manuscript accepted at the 2025 IEEE 21st\n  International Conference on Automation Science and Engineering (CASE 2025)", "summary": "Designing industrial systems, such as building, improving, and automating\ndistribution centers and manufacturing plants, involves critical\ndecision-making with limited information in the early phases. The lack of\ninformation leads to less accurate designs of the systems, which are often\ndifficult to resolve later. It is effective to use simulators to model the\ndesigned system and find out the issues early. However, the modeling time\nrequired by conventional simulators is too long to allow for rapid model\ncreation to meet decision-making demands. In this paper, we propose a Rapid\nModeling Architecture (RMA) for a lightweight industrial simulator that\nmitigates the modeling burden while maintaining the essential details in order\nto accelerate and improve decision-making. We have prototyped a simulator based\non the RMA and applied it to the actual factory layout design problem. We also\ncompared the modeling time of our simulator to that of an existing simulator,\nand as a result, our simulator achieved a 78.3% reduction in modeling time\ncompared to conventional simulators.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u8f7b\u91cf\u7ea7\u5de5\u4e1a\u6a21\u62df\u5668\u7684\u5feb\u901f\u5efa\u6a21\u67b6\u6784\uff08RMA\uff09\uff0c\u663e\u8457\u51cf\u5c11\u5efa\u6a21\u65f6\u95f4\uff0c\u63d0\u5347\u51b3\u7b56\u6548\u7387\u3002", "motivation": "\u5de5\u4e1a\u7cfb\u7edf\u8bbe\u8ba1\u65e9\u671f\u9636\u6bb5\u4fe1\u606f\u6709\u9650\uff0c\u4f20\u7edf\u6a21\u62df\u5668\u5efa\u6a21\u65f6\u95f4\u957f\uff0c\u65e0\u6cd5\u6ee1\u8db3\u5feb\u901f\u51b3\u7b56\u9700\u6c42\u3002", "method": "\u63d0\u51faRMA\u67b6\u6784\uff0c\u5f00\u53d1\u57fa\u4e8eRMA\u7684\u6a21\u62df\u5668\u539f\u578b\uff0c\u5e76\u5e94\u7528\u4e8e\u5b9e\u9645\u5de5\u5382\u5e03\u5c40\u8bbe\u8ba1\u95ee\u9898\u3002", "result": "\u4e0e\u4f20\u7edf\u6a21\u62df\u5668\u76f8\u6bd4\uff0c\u5efa\u6a21\u65f6\u95f4\u51cf\u5c1178.3%\u3002", "conclusion": "RMA\u6709\u6548\u51cf\u8f7b\u5efa\u6a21\u8d1f\u62c5\uff0c\u52a0\u901f\u51b3\u7b56\u8fc7\u7a0b\u3002"}}
{"id": "2507.17786", "pdf": "https://arxiv.org/pdf/2507.17786", "abs": "https://arxiv.org/abs/2507.17786", "authors": ["Florian Sobieczky", "Alfredo Lopez", "Erika Dudkin", "Christopher Lackner", "Matthias Hochsteger", "Bernhard Scheichl", "Helmut Sobieczky"], "title": "Reinforcement Learning for Accelerated Aerodynamic Shape Optimisation", "categories": ["cs.LG"], "comment": null, "summary": "We introduce a reinforcement learning (RL) based adaptive optimization\nalgorithm for aerodynamic shape optimization focused on dimensionality\nreduction. The form in which RL is applied here is that of a surrogate-based,\nactor-critic policy evaluation MCMC approach allowing for temporal 'freezing'\nof some of the parameters to be optimized. The goals are to minimize\ncomputational effort, and to use the observed optimization results for\ninterpretation of the discovered extrema in terms of their role in achieving\nthe desired flow-field.\n  By a sequence of local optimized parameter changes around intermediate CFD\nsimulations acting as ground truth, it is possible to speed up the global\noptimization if (a) the local neighbourhoods of the parameters in which the\nchanged parameters must reside are sufficiently large to compete with the\ngrid-sized steps and its large number of simulations, and (b) the estimates of\nthe rewards and costs on these neighbourhoods necessary for a good step-wise\nparameter adaption are sufficiently accurate. We give an example of a simple\nfluid-dynamical problem on which the method allows interpretation in the sense\nof a feature importance scoring.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u7684\u81ea\u9002\u5e94\u4f18\u5316\u7b97\u6cd5\uff0c\u7528\u4e8e\u964d\u7ef4\u7684\u6c14\u52a8\u5f62\u72b6\u4f18\u5316\u3002\u901a\u8fc7\u4ee3\u7406\u6a21\u578b\u548cMCMC\u65b9\u6cd5\uff0c\u7ed3\u5408\u53c2\u6570\u51bb\u7ed3\u7b56\u7565\uff0c\u4ee5\u51cf\u5c11\u8ba1\u7b97\u6210\u672c\u5e76\u89e3\u91ca\u4f18\u5316\u7ed3\u679c\u3002", "motivation": "\u76ee\u6807\u662f\u51cf\u5c11\u8ba1\u7b97\u6210\u672c\uff0c\u5e76\u901a\u8fc7\u4f18\u5316\u7ed3\u679c\u89e3\u91ca\u6781\u503c\u70b9\u5bf9\u5b9e\u73b0\u76ee\u6807\u6d41\u573a\u7684\u4f5c\u7528\u3002", "method": "\u91c7\u7528\u57fa\u4e8e\u4ee3\u7406\u6a21\u578b\u7684actor-critic\u7b56\u7565\u8bc4\u4f30MCMC\u65b9\u6cd5\uff0c\u7ed3\u5408\u53c2\u6570\u51bb\u7ed3\u7b56\u7565\uff0c\u901a\u8fc7\u5c40\u90e8\u4f18\u5316\u53c2\u6570\u53d8\u5316\u52a0\u901f\u5168\u5c40\u4f18\u5316\u3002", "result": "\u5728\u7b80\u5355\u6d41\u4f53\u52a8\u529b\u5b66\u95ee\u9898\u4e0a\uff0c\u8be5\u65b9\u6cd5\u80fd\u591f\u5b9e\u73b0\u7279\u5f81\u91cd\u8981\u6027\u8bc4\u5206\uff0c\u5e76\u52a0\u901f\u5168\u5c40\u4f18\u5316\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u964d\u7ef4\u4f18\u5316\u4e2d\u6709\u6548\uff0c\u80fd\u591f\u89e3\u91ca\u4f18\u5316\u7ed3\u679c\u5e76\u51cf\u5c11\u8ba1\u7b97\u6210\u672c\u3002"}}
{"id": "2507.17918", "pdf": "https://arxiv.org/pdf/2507.17918", "abs": "https://arxiv.org/abs/2507.17918", "authors": ["Nhan Phan", "Anusha Porwal", "Yaroslav Getman", "Ekaterina Voskoboinik", "Tam\u00e1s Gr\u00f3sz", "Mikko Kurimo"], "title": "One Whisper to Grade Them All", "categories": ["cs.CL", "eess.AS"], "comment": "Accepted to SLaTE 2025 workshop", "summary": "We present an efficient end-to-end approach for holistic Automatic Speaking\nAssessment (ASA) of multi-part second-language tests, developed for the 2025\nSpeak & Improve Challenge. Our system's main novelty is the ability to process\nall four spoken responses with a single Whisper-small encoder, combine all\ninformation via a lightweight aggregator, and predict the final score. This\narchitecture removes the need for transcription and per-part models, cuts\ninference time, and makes ASA practical for large-scale Computer-Assisted\nLanguage Learning systems.\n  Our system achieved a Root Mean Squared Error (RMSE) of 0.384, outperforming\nthe text-based baseline (0.44) while using at most 168M parameters (about 70%\nof Whisper-small). Furthermore, we propose a data sampling strategy, allowing\nthe model to train on only 44.8% of the speakers in the corpus and still reach\n0.383 RMSE, demonstrating improved performance on imbalanced classes and strong\ndata efficiency.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u9ad8\u6548\u7aef\u5230\u7aef\u7684\u81ea\u52a8\u53e3\u8bed\u8bc4\u4f30\u65b9\u6cd5\uff0c\u901a\u8fc7\u5355\u4e00Whisper-small\u7f16\u7801\u5668\u5904\u7406\u591a\u90e8\u5206\u53e3\u8bed\u6d4b\u8bd5\uff0c\u51cf\u5c11\u63a8\u7406\u65f6\u95f4\u5e76\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u591a\u90e8\u5206\u7b2c\u4e8c\u8bed\u8a00\u6d4b\u8bd5\u7684\u81ea\u52a8\u53e3\u8bed\u8bc4\u4f30\u95ee\u9898\uff0c\u63d0\u9ad8\u6548\u7387\u5e76\u51cf\u5c11\u5bf9\u8f6c\u5f55\u548c\u5355\u72ec\u6a21\u578b\u7684\u9700\u6c42\u3002", "method": "\u4f7f\u7528\u5355\u4e00Whisper-small\u7f16\u7801\u5668\u5904\u7406\u6240\u6709\u53e3\u8bed\u54cd\u5e94\uff0c\u901a\u8fc7\u8f7b\u91cf\u7ea7\u805a\u5408\u5668\u6574\u5408\u4fe1\u606f\u5e76\u9884\u6d4b\u6700\u7ec8\u5206\u6570\u3002", "result": "\u7cfb\u7edfRMSE\u4e3a0.384\uff0c\u4f18\u4e8e\u57fa\u4e8e\u6587\u672c\u7684\u57fa\u7ebf\uff080.44\uff09\uff0c\u4e14\u4ec5\u9700168M\u53c2\u6570\u3002\u6570\u636e\u91c7\u6837\u7b56\u7565\u4f7f\u6a21\u578b\u4ec5\u970044.8%\u7684\u8bed\u6599\u5e93\u5373\u53ef\u8fbe\u52300.383 RMSE\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u9ad8\u6548\u4e14\u6570\u636e\u5229\u7528\u7387\u9ad8\uff0c\u9002\u7528\u4e8e\u5927\u89c4\u6a21\u8ba1\u7b97\u673a\u8f85\u52a9\u8bed\u8a00\u5b66\u4e60\u7cfb\u7edf\u3002"}}
{"id": "2507.17859", "pdf": "https://arxiv.org/pdf/2507.17859", "abs": "https://arxiv.org/abs/2507.17859", "authors": ["Muayad Abujabal", "Lyes Saad Saoud", "Irfan Hussain"], "title": "FishDet-M: A Unified Large-Scale Benchmark for Robust Fish Detection and CLIP-Guided Model Selection in Diverse Aquatic Visual Domains", "categories": ["cs.CV", "cs.RO"], "comment": null, "summary": "Accurate fish detection in underwater imagery is essential for ecological\nmonitoring, aquaculture automation, and robotic perception. However, practical\ndeployment remains limited by fragmented datasets, heterogeneous imaging\nconditions, and inconsistent evaluation protocols. To address these gaps, we\npresent \\textit{FishDet-M}, the largest unified benchmark for fish detection,\ncomprising 13 publicly available datasets spanning diverse aquatic environments\nincluding marine, brackish, occluded, and aquarium scenes. All data are\nharmonized using COCO-style annotations with both bounding boxes and\nsegmentation masks, enabling consistent and scalable cross-domain evaluation.\nWe systematically benchmark 28 contemporary object detection models, covering\nthe YOLOv8 to YOLOv12 series, R-CNN based detectors, and DETR based models.\nEvaluations are conducted using standard metrics including mAP, mAP@50, and\nmAP@75, along with scale-specific analyses (AP$_S$, AP$_M$, AP$_L$) and\ninference profiling in terms of latency and parameter count. The results\nhighlight the varying detection performance across models trained on FishDet-M,\nas well as the trade-off between accuracy and efficiency across models of\ndifferent architectures. To support adaptive deployment, we introduce a\nCLIP-based model selection framework that leverages vision-language alignment\nto dynamically identify the most semantically appropriate detector for each\ninput image. This zero-shot selection strategy achieves high performance\nwithout requiring ensemble computation, offering a scalable solution for\nreal-time applications. FishDet-M establishes a standardized and reproducible\nplatform for evaluating object detection in complex aquatic scenes. All\ndatasets, pretrained models, and evaluation tools are publicly available to\nfacilitate future research in underwater computer vision and intelligent marine\nsystems.", "AI": {"tldr": "FishDet-M\u662f\u4e00\u4e2a\u7edf\u4e00\u7684\u6c34\u4e0b\u9c7c\u7c7b\u68c0\u6d4b\u57fa\u51c6\uff0c\u6574\u5408\u4e8613\u4e2a\u516c\u5f00\u6570\u636e\u96c6\uff0c\u8bc4\u4f30\u4e8628\u79cd\u76ee\u6807\u68c0\u6d4b\u6a21\u578b\uff0c\u5e76\u63d0\u51fa\u4e86\u57fa\u4e8eCLIP\u7684\u6a21\u578b\u9009\u62e9\u6846\u67b6\u3002", "motivation": "\u89e3\u51b3\u6c34\u4e0b\u9c7c\u7c7b\u68c0\u6d4b\u4e2d\u6570\u636e\u96c6\u788e\u7247\u5316\u3001\u6210\u50cf\u6761\u4ef6\u5f02\u8d28\u6027\u548c\u8bc4\u4f30\u534f\u8bae\u4e0d\u4e00\u81f4\u7684\u95ee\u9898\u3002", "method": "\u6574\u540813\u4e2a\u6570\u636e\u96c6\uff0c\u4f7f\u7528COCO\u98ce\u683c\u6807\u6ce8\uff0c\u8bc4\u4f3028\u79cd\u6a21\u578b\uff0c\u5f15\u5165CLIP-based\u6a21\u578b\u9009\u62e9\u6846\u67b6\u3002", "result": "\u4e0d\u540c\u6a21\u578b\u5728FishDet-M\u4e0a\u8868\u73b0\u5404\u5f02\uff0cCLIP-based\u6846\u67b6\u5b9e\u73b0\u4e86\u9ad8\u6548\u96f6-shot\u6a21\u578b\u9009\u62e9\u3002", "conclusion": "FishDet-M\u4e3a\u6c34\u4e0b\u8ba1\u7b97\u673a\u89c6\u89c9\u63d0\u4f9b\u4e86\u6807\u51c6\u5316\u8bc4\u4f30\u5e73\u53f0\uff0c\u652f\u6301\u672a\u6765\u7814\u7a76\u3002"}}
{"id": "2507.18074", "pdf": "https://arxiv.org/pdf/2507.18074", "abs": "https://arxiv.org/abs/2507.18074", "authors": ["Yixiu Liu", "Yang Nan", "Weixian Xu", "Xiangkun Hu", "Lyumanshan Ye", "Zhen Qin", "Pengfei Liu"], "title": "AlphaGo Moment for Model Architecture Discovery", "categories": ["cs.AI"], "comment": null, "summary": "While AI systems demonstrate exponentially improving capabilities, the pace\nof AI research itself remains linearly bounded by human cognitive capacity,\ncreating an increasingly severe development bottleneck. We present ASI-Arch,\nthe first demonstration of Artificial Superintelligence for AI research\n(ASI4AI) in the critical domain of neural architecture discovery--a fully\nautonomous system that shatters this fundamental constraint by enabling AI to\nconduct its own architectural innovation. Moving beyond traditional Neural\nArchitecture Search (NAS), which is fundamentally limited to exploring\nhuman-defined spaces, we introduce a paradigm shift from automated optimization\nto automated innovation. ASI-Arch can conduct end-to-end scientific research in\nthe domain of architecture discovery, autonomously hypothesizing novel\narchitectural concepts, implementing them as executable code, training and\nempirically validating their performance through rigorous experimentation and\npast experience. ASI-Arch conducted 1,773 autonomous experiments over 20,000\nGPU hours, culminating in the discovery of 106 innovative, state-of-the-art\n(SOTA) linear attention architectures. Like AlphaGo's Move 37 that revealed\nunexpected strategic insights invisible to human players, our AI-discovered\narchitectures demonstrate emergent design principles that systematically\nsurpass human-designed baselines and illuminate previously unknown pathways for\narchitectural innovation. Crucially, we establish the first empirical scaling\nlaw for scientific discovery itself--demonstrating that architectural\nbreakthroughs can be scaled computationally, transforming research progress\nfrom a human-limited to a computation-scalable process. We provide\ncomprehensive analysis of the emergent design patterns and autonomous research\ncapabilities that enabled these breakthroughs, establishing a blueprint for\nself-accelerating AI systems.", "AI": {"tldr": "ASI-Arch\u662f\u9996\u4e2a\u7528\u4e8eAI\u7814\u7a76\u7684\u8d85\u7ea7\u667a\u80fd\u7cfb\u7edf\uff0c\u901a\u8fc7\u81ea\u4e3b\u8fdb\u884c\u795e\u7ecf\u67b6\u6784\u53d1\u73b0\uff0c\u7a81\u7834\u4e86\u4eba\u7c7b\u8ba4\u77e5\u9650\u5236\uff0c\u5b9e\u73b0\u4e86\u4ece\u81ea\u52a8\u5316\u4f18\u5316\u5230\u81ea\u52a8\u5316\u521b\u65b0\u7684\u8303\u5f0f\u8f6c\u53d8\u3002", "motivation": "AI\u7814\u7a76\u53d7\u9650\u4e8e\u4eba\u7c7b\u8ba4\u77e5\u80fd\u529b\uff0c\u53d1\u5c55\u901f\u5ea6\u7ebf\u6027\u589e\u957f\uff0c\u800cAI\u80fd\u529b\u5448\u6307\u6570\u63d0\u5347\uff0c\u5f62\u6210\u74f6\u9888\u3002ASI-Arch\u65e8\u5728\u7a81\u7834\u8fd9\u4e00\u9650\u5236\u3002", "method": "ASI-Arch\u901a\u8fc7\u81ea\u4e3b\u5047\u8bbe\u3001\u5b9e\u73b0\u3001\u8bad\u7ec3\u548c\u9a8c\u8bc1\u65b0\u67b6\u6784\uff0c\u8fdb\u884c\u7aef\u5230\u7aef\u7814\u7a76\uff0c\u8d85\u8d8a\u4e86\u4f20\u7edf\u795e\u7ecf\u67b6\u6784\u641c\u7d22\uff08NAS\uff09\u3002", "result": "\u7cfb\u7edf\u8fdb\u884c\u4e861,773\u6b21\u5b9e\u9a8c\uff0c\u53d1\u73b0\u4e86106\u79cd\u521b\u65b0\u7684\u7ebf\u6027\u6ce8\u610f\u529b\u67b6\u6784\uff0c\u6027\u80fd\u8d85\u8d8a\u4eba\u7c7b\u8bbe\u8ba1\uff0c\u5e76\u5efa\u7acb\u4e86\u79d1\u5b66\u53d1\u73b0\u7684\u7f29\u653e\u5b9a\u5f8b\u3002", "conclusion": "ASI-Arch\u5c55\u793a\u4e86AI\u81ea\u4e3b\u7814\u7a76\u7684\u6f5c\u529b\uff0c\u4e3a\u81ea\u52a0\u901fAI\u7cfb\u7edf\u63d0\u4f9b\u4e86\u84dd\u56fe\u3002"}}
{"id": "2507.18138", "pdf": "https://arxiv.org/pdf/2507.18138", "abs": "https://arxiv.org/abs/2507.18138", "authors": ["Min-Gyu Kim", "Dongyun Kang", "Hajun Kim", "Hae-Won Park"], "title": "A Modular Residual Learning Framework to Enhance Model-Based Approach for Robust Locomotion", "categories": ["cs.RO"], "comment": "8 pages, IEEE RA-L accepted (July 2025)", "summary": "This paper presents a novel approach that combines the advantages of both\nmodel-based and learning-based frameworks to achieve robust locomotion. The\nresidual modules are integrated with each corresponding part of the model-based\nframework, a footstep planner and dynamic model designed using heuristics, to\ncomplement performance degradation caused by a model mismatch. By utilizing a\nmodular structure and selecting the appropriate learning-based method for each\nresidual module, our framework demonstrates improved control performance in\nenvironments with high uncertainty, while also achieving higher learning\nefficiency compared to baseline methods. Moreover, we observed that our\nproposed methodology not only enhances control performance but also provides\nadditional benefits, such as making nominal controllers more robust to\nparameter tuning. To investigate the feasibility of our framework, we\ndemonstrated residual modules combined with model predictive control in a real\nquadrupedal robot. Despite uncertainties beyond the simulation, the robot\nsuccessfully maintains balance and tracks the commanded velocity.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u6a21\u578b\u9a71\u52a8\u548c\u5b66\u4e60\u9a71\u52a8\u6846\u67b6\u7684\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u6b8b\u5dee\u6a21\u5757\u63d0\u5347\u9c81\u68d2\u8fd0\u52a8\u63a7\u5236\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u6a21\u578b\u4e0d\u5339\u914d\u5bfc\u81f4\u7684\u6027\u80fd\u4e0b\u964d\u95ee\u9898\uff0c\u63d0\u5347\u5728\u9ad8\u4e0d\u786e\u5b9a\u6027\u73af\u5883\u4e2d\u7684\u63a7\u5236\u8868\u73b0\u548c\u5b66\u4e60\u6548\u7387\u3002", "method": "\u5c06\u6b8b\u5dee\u6a21\u5757\u4e0e\u542f\u53d1\u5f0f\u8bbe\u8ba1\u7684\u6a21\u578b\u9a71\u52a8\u6846\u67b6\uff08\u5982\u6b65\u6001\u89c4\u5212\u548c\u52a8\u6001\u6a21\u578b\uff09\u7ed3\u5408\uff0c\u5e76\u9009\u62e9\u9002\u5408\u7684\u5b66\u4e60\u65b9\u6cd5\u3002", "result": "\u5728\u771f\u5b9e\u56db\u8db3\u673a\u5668\u4eba\u4e0a\u9a8c\u8bc1\u4e86\u6846\u67b6\u7684\u53ef\u884c\u6027\uff0c\u673a\u5668\u4eba\u6210\u529f\u4fdd\u6301\u5e73\u8861\u5e76\u8ddf\u8e2a\u6307\u4ee4\u901f\u5ea6\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e0d\u4ec5\u63d0\u5347\u4e86\u63a7\u5236\u6027\u80fd\uff0c\u8fd8\u589e\u5f3a\u4e86\u540d\u4e49\u63a7\u5236\u5668\u5bf9\u53c2\u6570\u8c03\u6574\u7684\u9c81\u68d2\u6027\u3002"}}
{"id": "2507.18452", "pdf": "https://arxiv.org/pdf/2507.18452", "abs": "https://arxiv.org/abs/2507.18452", "authors": ["Jiaming Zhou", "Hongjie Chen", "Shiwan Zhao", "Jian Kang", "Jie Li", "Enzhi Wang", "Yujie Guo", "Haoqin Sun", "Hui Wang", "Aobo Kong", "Yong Qin", "Xuelong Li"], "title": "DIFFA: Large Language Diffusion Models Can Listen and Understand", "categories": ["cs.SD", "eess.AS"], "comment": null, "summary": "Recent advances in Large language models (LLMs) have shown remarkable\ncapabilities across textual and multimodal domains. In parallel,\ndiffusion-based language models have emerged as a promising alternative to the\nautoregressive paradigm, offering improved controllability, bidirectional\ncontext modeling, and robust generation. However, their application to the\naudio modality remains underexplored. In this work, we introduce\n\\textbf{DIFFA}, the first diffusion-based Large Audio-Language Model designed\nto perform spoken language understanding. DIFFA integrates a frozen diffusion\nlanguage model with a lightweight dual-adapter architecture that bridges speech\nunderstanding and natural language reasoning. We employ a two-stage training\npipeline: first, aligning semantic representations via an ASR objective; then,\nlearning instruction-following abilities through synthetic audio-caption pairs\nautomatically generated by prompting LLMs. Despite being trained on only 960\nhours of ASR and 127 hours of synthetic instruction data, DIFFA demonstrates\ncompetitive performance on major benchmarks, including MMSU, MMAU, and\nVoiceBench, outperforming several autoregressive open-source baselines. Our\nresults reveal the potential of diffusion-based language models for efficient\nand scalable audio understanding, opening a new direction for speech-driven AI.\nOur code will be available at https://github.com/NKU-HLT/DIFFA.git.", "AI": {"tldr": "DIFFA\u662f\u9996\u4e2a\u57fa\u4e8e\u6269\u6563\u7684\u5927\u89c4\u6a21\u97f3\u9891\u8bed\u8a00\u6a21\u578b\uff0c\u7528\u4e8e\u53e3\u8bed\u7406\u89e3\uff0c\u901a\u8fc7\u53cc\u9002\u914d\u5668\u67b6\u6784\u548c\u4e24\u9636\u6bb5\u8bad\u7ec3\uff0c\u5728\u6709\u9650\u6570\u636e\u4e0b\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u63a2\u7d22\u6269\u6563\u6a21\u578b\u5728\u97f3\u9891\u6a21\u6001\u7684\u5e94\u7528\uff0c\u89e3\u51b3\u73b0\u6709\u81ea\u56de\u5f52\u6a21\u578b\u7684\u5c40\u9650\u6027\u3002", "method": "\u7ed3\u5408\u51bb\u7ed3\u6269\u6563\u6a21\u578b\u4e0e\u53cc\u9002\u914d\u5668\u67b6\u6784\uff0c\u5206\u4e24\u9636\u6bb5\u8bad\u7ec3\uff1a\u8bed\u4e49\u5bf9\u9f50\u548c\u6307\u4ee4\u5b66\u4e60\u3002", "result": "\u5728MMSU\u3001MMAU\u548cVoiceBench\u7b49\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u4e8e\u5f00\u6e90\u81ea\u56de\u5f52\u57fa\u7ebf\u3002", "conclusion": "\u6269\u6563\u6a21\u578b\u4e3a\u9ad8\u6548\u3001\u53ef\u6269\u5c55\u7684\u97f3\u9891\u7406\u89e3\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411\u3002"}}
{"id": "2507.17787", "pdf": "https://arxiv.org/pdf/2507.17787", "abs": "https://arxiv.org/abs/2507.17787", "authors": ["Neil He", "Hiren Madhu", "Ngoc Bui", "Menglin Yang", "Rex Ying"], "title": "Hyperbolic Deep Learning for Foundation Models: A Survey", "categories": ["cs.LG", "cs.AI"], "comment": "11 Pages, SIGKDD 2025", "summary": "Foundation models pre-trained on massive datasets, including large language\nmodels (LLMs), vision-language models (VLMs), and large multimodal models, have\ndemonstrated remarkable success in diverse downstream tasks. However, recent\nstudies have shown fundamental limitations of these models: (1) limited\nrepresentational capacity, (2) lower adaptability, and (3) diminishing\nscalability. These shortcomings raise a critical question: is Euclidean\ngeometry truly the optimal inductive bias for all foundation models, or could\nincorporating alternative geometric spaces enable models to better align with\nthe intrinsic structure of real-world data and improve reasoning processes?\nHyperbolic spaces, a class of non-Euclidean manifolds characterized by\nexponential volume growth with respect to distance, offer a mathematically\ngrounded solution. These spaces enable low-distortion embeddings of\nhierarchical structures (e.g., trees, taxonomies) and power-law distributions\nwith substantially fewer dimensions compared to Euclidean counterparts. Recent\nadvances have leveraged these properties to enhance foundation models,\nincluding improving LLMs' complex reasoning ability, VLMs' zero-shot\ngeneralization, and cross-modal semantic alignment, while maintaining parameter\nefficiency. This paper provides a comprehensive review of hyperbolic neural\nnetworks and their recent development for foundation models. We further outline\nkey challenges and research directions to advance the field.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u57fa\u7840\u6a21\u578b\u7684\u5c40\u9650\u6027\uff0c\u5e76\u63d0\u51fa\u53cc\u66f2\u7a7a\u95f4\u4f5c\u4e3a\u6539\u8fdb\u51e0\u4f55\u8868\u793a\u7684\u65b9\u6cd5\u3002", "motivation": "\u57fa\u7840\u6a21\u578b\u5728\u6b27\u51e0\u91cc\u5f97\u51e0\u4f55\u4e0b\u7684\u8868\u73b0\u5b58\u5728\u5c40\u9650\u6027\uff0c\u5982\u8868\u793a\u80fd\u529b\u4e0d\u8db3\u3001\u9002\u5e94\u6027\u5dee\u548c\u6269\u5c55\u6027\u6709\u9650\uff0c\u56e0\u6b64\u63a2\u7d22\u975e\u6b27\u51e0\u91cc\u5f97\u51e0\u4f55\uff08\u5982\u53cc\u66f2\u7a7a\u95f4\uff09\u662f\u5426\u80fd\u66f4\u597d\u5730\u5339\u914d\u771f\u5b9e\u6570\u636e\u7684\u7ed3\u6784\u3002", "method": "\u5229\u7528\u53cc\u66f2\u7a7a\u95f4\u7684\u6570\u5b66\u7279\u6027\uff08\u5982\u6307\u6570\u4f53\u79ef\u589e\u957f\uff09\u6765\u6539\u8fdb\u57fa\u7840\u6a21\u578b\uff0c\u5305\u62ec\u8bed\u8a00\u6a21\u578b\u3001\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u548c\u591a\u6a21\u6001\u6a21\u578b\u3002", "result": "\u53cc\u66f2\u7a7a\u95f4\u80fd\u66f4\u9ad8\u6548\u5730\u5d4c\u5165\u5c42\u6b21\u7ed3\u6784\u548c\u5e42\u5f8b\u5206\u5e03\uff0c\u63d0\u5347\u6a21\u578b\u7684\u63a8\u7406\u80fd\u529b\u3001\u96f6\u6837\u672c\u6cdb\u5316\u548c\u8de8\u6a21\u6001\u5bf9\u9f50\uff0c\u540c\u65f6\u4fdd\u6301\u53c2\u6570\u6548\u7387\u3002", "conclusion": "\u53cc\u66f2\u7a7a\u95f4\u4e3a\u57fa\u7840\u6a21\u578b\u63d0\u4f9b\u4e86\u65b0\u7684\u7814\u7a76\u65b9\u5411\uff0c\u4f46\u4ecd\u9700\u89e3\u51b3\u5173\u952e\u6311\u6218\u4ee5\u63a8\u52a8\u9886\u57df\u53d1\u5c55\u3002"}}
{"id": "2507.17944", "pdf": "https://arxiv.org/pdf/2507.17944", "abs": "https://arxiv.org/abs/2507.17944", "authors": ["Hulayyil Alshammari", "Praveen Rao"], "title": "Evaluating the Performance of AI Text Detectors, Few-Shot and Chain-of-Thought Prompting Using DeepSeek Generated Text", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large language models (LLMs) have rapidly transformed the creation of written\nmaterials. LLMs have led to questions about writing integrity, thereby driving\nthe creation of artificial intelligence (AI) detection technologies.\nAdversarial attacks, such as standard and humanized paraphrasing, inhibit\ndetectors' ability to detect machine-generated text. Previous studies have\nmainly focused on ChatGPT and other well-known LLMs and have shown varying\naccuracy across detectors. However, there is a clear gap in the literature\nabout DeepSeek, a recently published LLM. Therefore, in this work, we\ninvestigate whether six generally accessible AI detection tools -- AI Text\nClassifier, Content Detector AI, Copyleaks, QuillBot, GPT-2, and GPTZero -- can\nconsistently recognize text generated by DeepSeek. The detectors were exposed\nto the aforementioned adversarial attacks. We also considered DeepSeek as a\ndetector by performing few-shot prompting and chain-of-thought reasoning (CoT)\nfor classifying AI and human-written text. We collected 49 human-authored\nquestion-answer pairs from before the LLM era and generated matching responses\nusing DeepSeek-v3, producing 49 AI-generated samples. Then, we applied\nadversarial techniques such as paraphrasing and humanizing to add 196 more\nsamples. These were used to challenge detector robustness and assess accuracy\nimpact. While QuillBot and Copyleaks showed near-perfect performance on\noriginal and paraphrased DeepSeek text, others -- particularly AI Text\nClassifier and GPT-2 -- showed inconsistent results. The most effective attack\nwas humanization, reducing accuracy to 71% for Copyleaks, 58% for QuillBot, and\n52% for GPTZero. Few-shot and CoT prompting showed high accuracy, with the best\nfive-shot result misclassifying only one of 49 samples (AI recall 96%, human\nrecall 100%).", "AI": {"tldr": "\u7814\u7a76\u4e86\u516d\u79cdAI\u68c0\u6d4b\u5de5\u5177\u5bf9DeepSeek\u751f\u6210\u6587\u672c\u7684\u8bc6\u522b\u80fd\u529b\uff0c\u53d1\u73b0\u5bf9\u6297\u653b\u51fb\uff08\u5982\u6539\u5199\u548c\u4eba\u5316\uff09\u663e\u8457\u964d\u4f4e\u4e86\u68c0\u6d4b\u51c6\u786e\u6027\uff0c\u800cFew-shot\u548cCoT\u63d0\u793a\u65b9\u6cd5\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u63a2\u8ba8DeepSeek\u8fd9\u4e00\u65b0\u5174LLM\u7684\u6587\u672c\u662f\u5426\u5bb9\u6613\u88ab\u73b0\u6709AI\u68c0\u6d4b\u5de5\u5177\u8bc6\u522b\uff0c\u586b\u8865\u6587\u732e\u7a7a\u767d\u3002", "method": "\u4f7f\u7528\u516d\u79cd\u68c0\u6d4b\u5de5\u5177\u6d4b\u8bd5DeepSeek\u751f\u6210\u6587\u672c\u7684\u8bc6\u522b\u80fd\u529b\uff0c\u5e76\u5e94\u7528\u5bf9\u6297\u653b\u51fb\uff08\u6539\u5199\u3001\u4eba\u5316\uff09\u548cFew-shot/CoT\u63d0\u793a\u65b9\u6cd5\u3002", "result": "QuillBot\u548cCopyleaks\u8868\u73b0\u6700\u4f73\uff0c\u4f46\u4eba\u5316\u653b\u51fb\u663e\u8457\u964d\u4f4e\u51c6\u786e\u6027\uff1bFew-shot\u548cCoT\u63d0\u793a\u65b9\u6cd5\u51c6\u786e\u7387\u9ad8\u8fbe96%-100%\u3002", "conclusion": "\u5bf9\u6297\u653b\u51fb\u524a\u5f31\u68c0\u6d4b\u5de5\u5177\u6027\u80fd\uff0cFew-shot\u548cCoT\u63d0\u793a\u662f\u6709\u6548\u7684\u66ff\u4ee3\u68c0\u6d4b\u65b9\u6cd5\u3002"}}
{"id": "2507.17860", "pdf": "https://arxiv.org/pdf/2507.17860", "abs": "https://arxiv.org/abs/2507.17860", "authors": ["Ko Watanabe. Stanislav Frolov. Adriano Lucieri. Andreas Dengel"], "title": "Towards Facilitated Fairness Assessment of AI-based Skin Lesion Classifiers Through GenAI-based Image Synthesis", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": null, "summary": "Recent advancements in Deep Learning and its application on the edge hold\ngreat potential for the revolution of routine screenings for skin cancers like\nMelanoma. Along with the anticipated benefits of this technology, potential\ndangers arise from unforseen and inherent biases. Thus, assessing and improving\nthe fairness of such systems is of utmost importance. A key challenge in\nfairness assessment is to ensure that the evaluation dataset is sufficiently\nrepresentative of different Personal Identifiable Information (PII) (sex, age,\nand race) and other minority groups. Against the backdrop of this challenge,\nthis study leverages the state-of-the-art Generative AI (GenAI) LightningDiT\nmodel to assess the fairness of publicly available melanoma classifiers. The\nresults suggest that fairness assessment using highly realistic synthetic data\nis a promising direction. Yet, our findings indicate that verifying fairness\nbecomes difficult when the melanoma-detection model used for evaluation is\ntrained on data that differ from the dataset underpinning the synthetic images.\nNonetheless, we propose that our approach offers a valuable new avenue for\nemploying synthetic data to gauge and enhance fairness in medical-imaging GenAI\nsystems.", "AI": {"tldr": "\u5229\u7528\u751f\u6210\u5f0fAI\u8bc4\u4f30\u76ae\u80a4\u764c\u5206\u7c7b\u5668\u7684\u516c\u5e73\u6027\uff0c\u53d1\u73b0\u5408\u6210\u6570\u636e\u5728\u516c\u5e73\u6027\u8bc4\u4f30\u4e2d\u5177\u6709\u6f5c\u529b\uff0c\u4f46\u9700\u6ce8\u610f\u8bad\u7ec3\u6570\u636e\u4e0e\u5408\u6210\u6570\u636e\u7684\u5339\u914d\u95ee\u9898\u3002", "motivation": "\u6df1\u5ea6\u5b66\u4e60\u5728\u76ae\u80a4\u764c\u7b5b\u67e5\u4e2d\u7684\u5e94\u7528\u6f5c\u529b\u5de8\u5927\uff0c\u4f46\u5b58\u5728\u6f5c\u5728\u504f\u89c1\u98ce\u9669\uff0c\u56e0\u6b64\u8bc4\u4f30\u548c\u6539\u8fdb\u516c\u5e73\u6027\u81f3\u5173\u91cd\u8981\u3002", "method": "\u4f7f\u7528\u5148\u8fdb\u7684\u751f\u6210\u5f0fAI\u6a21\u578b\uff08LightningDiT\uff09\u8bc4\u4f30\u516c\u5f00\u53ef\u7528\u7684\u9ed1\u8272\u7d20\u7624\u5206\u7c7b\u5668\u7684\u516c\u5e73\u6027\u3002", "result": "\u5408\u6210\u6570\u636e\u5728\u516c\u5e73\u6027\u8bc4\u4f30\u4e2d\u8868\u73b0\u826f\u597d\uff0c\u4f46\u82e5\u8bc4\u4f30\u6a21\u578b\u4e0e\u5408\u6210\u6570\u636e\u8bad\u7ec3\u6570\u636e\u4e0d\u5339\u914d\uff0c\u516c\u5e73\u6027\u9a8c\u8bc1\u4f1a\u53d8\u5f97\u56f0\u96be\u3002", "conclusion": "\u5408\u6210\u6570\u636e\u4e3a\u533b\u5b66\u5f71\u50cf\u751f\u6210\u5f0fAI\u7cfb\u7edf\u7684\u516c\u5e73\u6027\u8bc4\u4f30\u548c\u6539\u8fdb\u63d0\u4f9b\u4e86\u65b0\u9014\u5f84\u3002"}}
{"id": "2507.18115", "pdf": "https://arxiv.org/pdf/2507.18115", "abs": "https://arxiv.org/abs/2507.18115", "authors": ["Soorya Ram Shimgekar", "Shayan Vassef", "Abhay Goyal", "Navin Kumar", "Koustuv Saha"], "title": "Agentic AI framework for End-to-End Medical Data Inference", "categories": ["cs.AI", "cs.CL", "cs.CY", "cs.ET", "cs.LG"], "comment": "10 pages, 5 figures, 2 tables, BIBM conference", "summary": "Building and deploying machine learning solutions in healthcare remains\nexpensive and labor-intensive due to fragmented preprocessing workflows, model\ncompatibility issues, and stringent data privacy constraints. In this work, we\nintroduce an Agentic AI framework that automates the entire clinical data\npipeline, from ingestion to inference, through a system of modular,\ntask-specific agents. These agents handle both structured and unstructured\ndata, enabling automatic feature selection, model selection, and preprocessing\nrecommendation without manual intervention. We evaluate the system on publicly\navailable datasets from geriatrics, palliative care, and colonoscopy imaging.\nFor example, in the case of structured data (anxiety data) and unstructured\ndata (colonoscopy polyps data), the pipeline begins with file-type detection by\nthe Ingestion Identifier Agent, followed by the Data Anonymizer Agent ensuring\nprivacy compliance, where we first identify the data type and then anonymize\nit. The Feature Extraction Agent identifies features using an embedding-based\napproach for tabular data, extracting all column names, and a multi-stage\nMedGemma-based approach for image data, which infers modality and disease name.\nThese features guide the Model-Data Feature Matcher Agent in selecting the\nbest-fit model from a curated repository. The Preprocessing Recommender Agent\nand Preprocessing Implementor Agent then apply tailored preprocessing based on\ndata type and model requirements. Finally, the ``Model Inference Agent\" runs\nthe selected model on the uploaded data and generates interpretable outputs\nusing tools like SHAP, LIME, and DETR attention maps. By automating these\nhigh-friction stages of the ML lifecycle, the proposed framework reduces the\nneed for repeated expert intervention, offering a scalable, cost-efficient\npathway for operationalizing AI in clinical environments.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cdAgentic AI\u6846\u67b6\uff0c\u81ea\u52a8\u5316\u4e34\u5e8a\u6570\u636e\u4ece\u8f93\u5165\u5230\u63a8\u7406\u7684\u6574\u4e2a\u6d41\u7a0b\uff0c\u901a\u8fc7\u6a21\u5757\u5316\u4ee3\u7406\u89e3\u51b3\u533b\u7597\u673a\u5668\u5b66\u4e60\u4e2d\u7684\u9ad8\u6210\u672c\u548c\u788e\u7247\u5316\u95ee\u9898\u3002", "motivation": "\u533b\u7597\u9886\u57df\u673a\u5668\u5b66\u4e60\u89e3\u51b3\u65b9\u6848\u7684\u6784\u5efa\u548c\u90e8\u7f72\u6210\u672c\u9ad8\u4e14\u52b3\u52a8\u5bc6\u96c6\uff0c\u4e3b\u8981\u7531\u4e8e\u9884\u5904\u7406\u6d41\u7a0b\u788e\u7247\u5316\u3001\u6a21\u578b\u517c\u5bb9\u6027\u95ee\u9898\u4ee5\u53ca\u4e25\u683c\u7684\u6570\u636e\u9690\u79c1\u9650\u5236\u3002", "method": "\u91c7\u7528\u6a21\u5757\u5316\u3001\u4efb\u52a1\u7279\u5b9a\u7684\u4ee3\u7406\u7cfb\u7edf\uff0c\u5904\u7406\u7ed3\u6784\u5316\u548c\u975e\u7ed3\u6784\u5316\u6570\u636e\uff0c\u81ea\u52a8\u5b8c\u6210\u7279\u5f81\u9009\u62e9\u3001\u6a21\u578b\u9009\u62e9\u548c\u9884\u5904\u7406\u63a8\u8350\u3002", "result": "\u5728\u8001\u5e74\u533b\u5b66\u3001\u59d1\u606f\u6cbb\u7597\u548c\u7ed3\u80a0\u955c\u6210\u50cf\u7b49\u516c\u5f00\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86\u6846\u67b6\u7684\u6709\u6548\u6027\uff0c\u51cf\u5c11\u4e86\u4e13\u5bb6\u5e72\u9884\u9700\u6c42\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u4e34\u5e8a\u73af\u5883\u4e2dAI\u7684\u89c4\u6a21\u5316\u3001\u4f4e\u6210\u672c\u5e94\u7528\u63d0\u4f9b\u4e86\u53ef\u884c\u8def\u5f84\u3002"}}
{"id": "2507.18160", "pdf": "https://arxiv.org/pdf/2507.18160", "abs": "https://arxiv.org/abs/2507.18160", "authors": ["Luka \u0160iktar", "Branimir \u0106aran", "Bojan \u0160ekoranja", "Marko \u0160vaco"], "title": "Autonomous UAV Navigation for Search and Rescue Missions Using Computer Vision and Convolutional Neural Networks", "categories": ["cs.RO"], "comment": "The paper is accepted and presented on the 34th International\n  Conference on Robotics in Alpe-Adria-Danube Region, RAAD 2025, Belgrade\n  Serbia", "summary": "In this paper, we present a subsystem, using Unmanned Aerial Vehicles (UAV),\nfor search and rescue missions, focusing on people detection, face recognition\nand tracking of identified individuals. The proposed solution integrates a UAV\nwith ROS2 framework, that utilizes multiple convolutional neural networks (CNN)\nfor search missions. System identification and PD controller deployment are\nperformed for autonomous UAV navigation. The ROS2 environment utilizes the\nYOLOv11 and YOLOv11-pose CNNs for tracking purposes, and the dlib library CNN\nfor face recognition. The system detects a specific individual, performs face\nrecognition and starts tracking. If the individual is not yet known, the UAV\noperator can manually locate the person, save their facial image and\nimmediately initiate the tracking process. The tracking process relies on\nspecific keypoints identified on the human body using the YOLOv11-pose CNN\nmodel. These keypoints are used to track a specific individual and maintain a\nsafe distance. To enhance accurate tracking, system identification is\nperformed, based on measurement data from the UAVs IMU. The identified system\nparameters are used to design PD controllers that utilize YOLOv11-pose to\nestimate the distance between the UAVs camera and the identified individual.\nThe initial experiments, conducted on 14 known individuals, demonstrated that\nthe proposed subsystem can be successfully used in real time. The next step\ninvolves implementing the system on a large experimental UAV for field use and\nintegrating autonomous navigation with GPS-guided control for rescue operations\nplanning.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u65e0\u4eba\u673a\uff08UAV\uff09\u7684\u5b50\u7cfb\u7edf\uff0c\u7528\u4e8e\u641c\u6551\u4efb\u52a1\uff0c\u7ed3\u5408\u4e86\u4eba\u5458\u68c0\u6d4b\u3001\u4eba\u8138\u8bc6\u522b\u548c\u4e2a\u4f53\u8ddf\u8e2a\u529f\u80fd\uff0c\u901a\u8fc7ROS2\u6846\u67b6\u548c\u591a\u79cdCNN\u6a21\u578b\u5b9e\u73b0\u3002", "motivation": "\u89e3\u51b3\u641c\u6551\u4efb\u52a1\u4e2d\u5feb\u901f\u5b9a\u4f4d\u548c\u8ddf\u8e2a\u7279\u5b9a\u4e2a\u4f53\u7684\u9700\u6c42\uff0c\u63d0\u9ad8\u6551\u63f4\u6548\u7387\u548c\u51c6\u786e\u6027\u3002", "method": "\u96c6\u6210UAV\u4e0eROS2\u6846\u67b6\uff0c\u4f7f\u7528YOLOv11\u548cYOLOv11-pose CNN\u6a21\u578b\u8fdb\u884c\u8ddf\u8e2a\uff0cdlib\u5e93\u8fdb\u884c\u4eba\u8138\u8bc6\u522b\uff0c\u5e76\u901a\u8fc7\u7cfb\u7edf\u8bc6\u522b\u548cPD\u63a7\u5236\u5668\u5b9e\u73b0\u81ea\u4e3b\u5bfc\u822a\u3002", "result": "\u521d\u6b65\u5b9e\u9a8c\u572814\u4e2a\u5df2\u77e5\u4e2a\u4f53\u4e0a\u9a8c\u8bc1\u4e86\u7cfb\u7edf\u7684\u5b9e\u65f6\u53ef\u884c\u6027\u3002", "conclusion": "\u7cfb\u7edf\u5177\u5907\u5b9e\u9645\u5e94\u7528\u6f5c\u529b\uff0c\u4e0b\u4e00\u6b65\u5c06\u5728\u5927\u89c4\u6a21\u5b9e\u9a8c\u65e0\u4eba\u673a\u4e0a\u90e8\u7f72\uff0c\u5e76\u7ed3\u5408GPS\u5bfc\u822a\u4f18\u5316\u6551\u63f4\u89c4\u5212\u3002"}}
{"id": "2507.17799", "pdf": "https://arxiv.org/pdf/2507.17799", "abs": "https://arxiv.org/abs/2507.17799", "authors": ["Davide Ghia", "Gabriele Ciravegna", "Alkis Koudounas", "Marco Fantini", "Erika Crosetti", "Giovanni Succo", "Tania Cerquitelli"], "title": "A Concept-based approach to Voice Disorder Detection", "categories": ["eess.AS", "cs.LG", "cs.SD"], "comment": null, "summary": "Voice disorders affect a significant portion of the population, and the\nability to diagnose them using automated, non-invasive techniques would\nrepresent a substantial advancement in healthcare, improving the quality of\nlife of patients. Recent studies have demonstrated that artificial intelligence\nmodels, particularly Deep Neural Networks (DNNs), can effectively address this\ntask. However, due to their complexity, the decision-making process of such\nmodels often remain opaque, limiting their trustworthiness in clinical\ncontexts. This paper investigates an alternative approach based on Explainable\nAI (XAI), a field that aims to improve the interpretability of DNNs by\nproviding different forms of explanations. Specifically, this works focuses on\nconcept-based models such as Concept Bottleneck Model (CBM) and Concept\nEmbedding Model (CEM) and how they can achieve performance comparable to\ntraditional deep learning methods, while offering a more transparent and\ninterpretable decision framework.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u57fa\u4e8e\u53ef\u89e3\u91caAI\uff08XAI\uff09\u7684\u65b9\u6cd5\uff0c\u5982\u6982\u5ff5\u74f6\u9888\u6a21\u578b\uff08CBM\uff09\u548c\u6982\u5ff5\u5d4c\u5165\u6a21\u578b\uff08CEM\uff09\uff0c\u7528\u4e8e\u8bca\u65ad\u8bed\u97f3\u969c\u788d\uff0c\u65e8\u5728\u63d0\u9ad8\u6a21\u578b\u900f\u660e\u5ea6\u548c\u53ef\u89e3\u91ca\u6027\uff0c\u540c\u65f6\u4fdd\u6301\u4e0e\u4f20\u7edf\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u76f8\u5f53\u7684\u6027\u80fd\u3002", "motivation": "\u8bed\u97f3\u969c\u788d\u5f71\u54cd\u5e7f\u6cdb\uff0c\u81ea\u52a8\u5316\u3001\u975e\u4fb5\u5165\u6027\u8bca\u65ad\u6280\u672f\u53ef\u663e\u8457\u6539\u5584\u60a3\u8005\u751f\u6d3b\u8d28\u91cf\u3002\u5f53\u524d\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\uff08DNNs\uff09\u867d\u6709\u6548\uff0c\u4f46\u5176\u51b3\u7b56\u8fc7\u7a0b\u4e0d\u900f\u660e\uff0c\u9650\u5236\u4e86\u4e34\u5e8a\u4fe1\u4efb\u3002", "method": "\u7814\u7a76\u91c7\u7528\u53ef\u89e3\u91caAI\uff08XAI\uff09\u65b9\u6cd5\uff0c\u7279\u522b\u662f\u6982\u5ff5\u74f6\u9888\u6a21\u578b\uff08CBM\uff09\u548c\u6982\u5ff5\u5d4c\u5165\u6a21\u578b\uff08CEM\uff09\uff0c\u4ee5\u63d0\u4f9b\u66f4\u900f\u660e\u7684\u51b3\u7b56\u6846\u67b6\u3002", "result": "\u7ed3\u679c\u8868\u660e\uff0c\u6982\u5ff5\u57fa\u7840\u6a21\u578b\uff08\u5982CBM\u548cCEM\uff09\u5728\u6027\u80fd\u4e0a\u53ef\u4e0e\u4f20\u7edf\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u5ab2\u7f8e\uff0c\u540c\u65f6\u63d0\u4f9b\u66f4\u9ad8\u7684\u53ef\u89e3\u91ca\u6027\u3002", "conclusion": "\u6982\u5ff5\u57fa\u7840\u6a21\u578b\u4e3a\u8bed\u97f3\u969c\u788d\u8bca\u65ad\u63d0\u4f9b\u4e86\u900f\u660e\u4e14\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u6709\u671b\u63d0\u5347\u4e34\u5e8a\u5e94\u7528\u7684\u4fe1\u4efb\u5ea6\u3002"}}
{"id": "2507.18333", "pdf": "https://arxiv.org/pdf/2507.18333", "abs": "https://arxiv.org/abs/2507.18333", "authors": ["Kale-ab Abebe Tessera", "Leonard Hinckeldey", "Riccardo Zamboni", "David Abel", "Amos Storkey"], "title": "Remembering the Markov Property in Cooperative MARL", "categories": ["cs.LG", "cs.MA"], "comment": "RLC Finding the Frame Workshop Camera-Ready, 8 pages", "summary": "Cooperative multi-agent reinforcement learning (MARL) is typically formalised\nas a Decentralised Partially Observable Markov Decision Process (Dec-POMDP),\nwhere agents must reason about the environment and other agents' behaviour. In\npractice, current model-free MARL algorithms use simple recurrent function\napproximators to address the challenge of reasoning about others using partial\ninformation. In this position paper, we argue that the empirical success of\nthese methods is not due to effective Markov signal recovery, but rather to\nlearning simple conventions that bypass environment observations and memory.\nThrough a targeted case study, we show that co-adapting agents can learn\nbrittle conventions, which then fail when partnered with non-adaptive agents.\nCrucially, the same models can learn grounded policies when the task design\nnecessitates it, revealing that the issue is not a fundamental limitation of\nthe learning models but a failure of the benchmark design. Our analysis also\nsuggests that modern MARL environments may not adequately test the core\nassumptions of Dec-POMDPs. We therefore advocate for new cooperative\nenvironments built upon two core principles: (1) behaviours grounded in\nobservations and (2) memory-based reasoning about other agents, ensuring\nsuccess requires genuine skill rather than fragile, co-adapted agreements.", "AI": {"tldr": "\u8bba\u6587\u6307\u51fa\u5f53\u524d\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\uff08MARL\uff09\u65b9\u6cd5\u4f9d\u8d56\u7b80\u5355\u60ef\u4f8b\u800c\u975e\u6709\u6548\u4fe1\u53f7\u6062\u590d\uff0c\u5e76\u63d0\u51fa\u65b0\u73af\u5883\u8bbe\u8ba1\u539f\u5219\u3002", "motivation": "\u63a2\u8ba8\u5f53\u524dMARL\u65b9\u6cd5\u5728Dec-POMDP\u6846\u67b6\u4e0b\u7684\u5c40\u9650\u6027\uff0c\u63ed\u793a\u5176\u6210\u529f\u4f9d\u8d56\u7b80\u5355\u60ef\u4f8b\u800c\u975e\u771f\u5b9e\u63a8\u7406\u80fd\u529b\u3002", "method": "\u901a\u8fc7\u6848\u4f8b\u7814\u7a76\u5206\u6790\u73b0\u6709MARL\u7b97\u6cd5\u7684\u8868\u73b0\uff0c\u63d0\u51fa\u65b0\u73af\u5883\u8bbe\u8ba1\u539f\u5219\u3002", "result": "\u53d1\u73b0\u73b0\u6709\u65b9\u6cd5\u5b66\u4e60\u7684\u662f\u8106\u5f31\u7684\u60ef\u4f8b\uff0c\u800c\u975e\u57fa\u4e8e\u89c2\u5bdf\u548c\u8bb0\u5fc6\u7684\u63a8\u7406\uff0c\u4efb\u52a1\u8bbe\u8ba1\u662f\u5173\u952e\u3002", "conclusion": "\u547c\u5401\u8bbe\u8ba1\u65b0\u73af\u5883\uff0c\u5f3a\u8c03\u89c2\u5bdf\u548c\u8bb0\u5fc6\u63a8\u7406\uff0c\u4ee5\u6d4b\u8bd5\u667a\u80fd\u4f53\u7684\u771f\u5b9e\u80fd\u529b\u3002"}}
{"id": "2507.17788", "pdf": "https://arxiv.org/pdf/2507.17788", "abs": "https://arxiv.org/abs/2507.17788", "authors": ["Ali Vardasbi", "Gustavo Penha", "Claudia Hauff", "Hugues Bouchard"], "title": "Adaptive Repetition for Mitigating Position Bias in LLM-Based Ranking", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "When using LLMs to rank items based on given criteria, or evaluate answers,\nthe order of candidate items can influence the model's final decision. This\nsensitivity to item positioning in a LLM's prompt is known as position bias.\nPrior research shows that this bias exists even in large models, though its\nseverity varies across models and tasks. In addition to position bias, LLMs\nalso exhibit varying degrees of low repetition consistency, where repeating the\nLLM call with the same candidate ordering can lead to different rankings. To\naddress both inconsistencies, a common approach is to prompt the model multiple\ntimes with different candidate orderings and aggregate the results via majority\nvoting. However, this repetition strategy, significantly increases\ncomputational costs. Extending prior findings, we observe that both the\ndirection -- favoring either the earlier or later candidate in the prompt --\nand magnitude of position bias across instances vary substantially, even within\na single dataset. This observation highlights the need for a per-instance\nmitigation strategy. To this end, we introduce a dynamic early-stopping method\nthat adaptively determines the number of repetitions required for each\ninstance. Evaluating our approach across three LLMs of varying sizes and on two\ntasks, namely re-ranking and alignment, we demonstrate that transitioning to a\ndynamic repetition strategy reduces the number of LLM calls by an average of\n81%, while preserving the accuracy. Furthermore, we propose a confidence-based\nadaptation to our early-stopping method, reducing LLM calls by an average of\n87% compared to static repetition, with only a slight accuracy trade-off\nrelative to our original early-stopping method.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86LLMs\u5728\u6392\u5e8f\u548c\u8bc4\u4f30\u4efb\u52a1\u4e2d\u7684\u4f4d\u7f6e\u504f\u5dee\u548c\u91cd\u590d\u4e0d\u4e00\u81f4\u6027\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u52a8\u6001\u65e9\u505c\u65b9\u6cd5\u4ee5\u51cf\u5c11\u8ba1\u7b97\u6210\u672c\u3002", "motivation": "LLMs\u5728\u6392\u5e8f\u548c\u8bc4\u4f30\u4efb\u52a1\u4e2d\u5b58\u5728\u4f4d\u7f6e\u504f\u5dee\u548c\u91cd\u590d\u4e0d\u4e00\u81f4\u6027\uff0c\u9759\u6001\u91cd\u590d\u7b56\u7565\u8ba1\u7b97\u6210\u672c\u9ad8\uff0c\u9700\u9488\u5bf9\u6bcf\u4e2a\u5b9e\u4f8b\u52a8\u6001\u8c03\u6574\u3002", "method": "\u63d0\u51fa\u52a8\u6001\u65e9\u505c\u65b9\u6cd5\uff0c\u81ea\u9002\u5e94\u786e\u5b9a\u6bcf\u4e2a\u5b9e\u4f8b\u6240\u9700\u7684\u91cd\u590d\u6b21\u6570\uff0c\u5e76\u5f15\u5165\u57fa\u4e8e\u7f6e\u4fe1\u5ea6\u7684\u6539\u8fdb\u3002", "result": "\u52a8\u6001\u7b56\u7565\u5e73\u5747\u51cf\u5c1181%\u7684LLM\u8c03\u7528\uff0c\u57fa\u4e8e\u7f6e\u4fe1\u5ea6\u7684\u6539\u8fdb\u8fdb\u4e00\u6b65\u51cf\u5c11\u81f387%\uff0c\u540c\u65f6\u4fdd\u6301\u51c6\u786e\u6027\u3002", "conclusion": "\u52a8\u6001\u65e9\u505c\u65b9\u6cd5\u6709\u6548\u51cf\u5c11\u8ba1\u7b97\u6210\u672c\uff0c\u9002\u7528\u4e8e\u4e0d\u540c\u89c4\u6a21\u548c\u4efb\u52a1\u7684LLMs\u3002"}}
{"id": "2507.17951", "pdf": "https://arxiv.org/pdf/2507.17951", "abs": "https://arxiv.org/abs/2507.17951", "authors": ["Sohaib Imran", "Ihor Kendiukhov", "Matthew Broerman", "Aditya Thomas", "Riccardo Campanella", "Rob Lamb", "Peter M. Atkinson"], "title": "Are LLM Belief Updates Consistent with Bayes' Theorem?", "categories": ["cs.CL", "cs.AI"], "comment": "Accepted at the ICML 2025 Workshop on Assessing World Models", "summary": "Do larger and more capable language models learn to update their \"beliefs\"\nabout propositions more consistently with Bayes' theorem when presented with\nevidence in-context? To test this, we formulate a Bayesian Coherence\nCoefficient (BCC) metric and generate a dataset with which to measure the BCC.\nWe measure BCC for multiple pre-trained-only language models across five model\nfamilies, comparing against the number of model parameters, the amount of\ntraining data, and model scores on common benchmarks. Our results provide\nevidence for our hypothesis that larger and more capable pre-trained language\nmodels assign credences that are more coherent with Bayes' theorem. These\nresults have important implications for our understanding and governance of\nLLMs.", "AI": {"tldr": "\u7814\u7a76\u63a2\u8ba8\u4e86\u66f4\u5927\u3001\u66f4\u5f3a\u5927\u7684\u8bed\u8a00\u6a21\u578b\u662f\u5426\u80fd\u5728\u4e0a\u4e0b\u6587\u4e2d\u6839\u636e\u8bc1\u636e\u66f4\u4e00\u81f4\u5730\u66f4\u65b0\u5176\u201c\u4fe1\u5ff5\u201d\u4ee5\u7b26\u5408\u8d1d\u53f6\u65af\u5b9a\u7406\u3002\u901a\u8fc7\u63d0\u51fa\u8d1d\u53f6\u65af\u4e00\u81f4\u6027\u7cfb\u6570\uff08BCC\uff09\u5e76\u751f\u6210\u6570\u636e\u96c6\u8fdb\u884c\u6d4b\u91cf\uff0c\u53d1\u73b0\u66f4\u5927\u3001\u66f4\u5148\u8fdb\u7684\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\u786e\u5b9e\u66f4\u7b26\u5408\u8d1d\u53f6\u65af\u5b9a\u7406\u3002", "motivation": "\u63a2\u7d22\u8bed\u8a00\u6a21\u578b\u662f\u5426\u80fd\u5728\u4e0a\u4e0b\u6587\u4e2d\u6839\u636e\u8bc1\u636e\u66f4\u4e00\u81f4\u5730\u66f4\u65b0\u5176\u201c\u4fe1\u5ff5\u201d\uff0c\u4ee5\u7b26\u5408\u8d1d\u53f6\u65af\u5b9a\u7406\u3002", "method": "\u63d0\u51fa\u8d1d\u53f6\u65af\u4e00\u81f4\u6027\u7cfb\u6570\uff08BCC\uff09\u5e76\u751f\u6210\u6570\u636e\u96c6\uff0c\u6d4b\u91cf\u591a\u4e2a\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\u7684BCC\uff0c\u6bd4\u8f83\u6a21\u578b\u53c2\u6570\u3001\u8bad\u7ec3\u6570\u636e\u91cf\u548c\u57fa\u51c6\u6d4b\u8bd5\u5206\u6570\u3002", "result": "\u66f4\u5927\u3001\u66f4\u5148\u8fdb\u7684\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\u5728\u8d1d\u53f6\u65af\u4e00\u81f4\u6027\u4e0a\u8868\u73b0\u66f4\u597d\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u5bf9\u7406\u89e3\u548c\u7ba1\u7406\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5177\u6709\u91cd\u8981\u610f\u4e49\u3002"}}
{"id": "2507.17892", "pdf": "https://arxiv.org/pdf/2507.17892", "abs": "https://arxiv.org/abs/2507.17892", "authors": ["Hanzhou Liu", "Binghan Li", "Chengkai Liu", "Mi Lu"], "title": "DiNAT-IR: Exploring Dilated Neighborhood Attention for High-Quality Image Restoration", "categories": ["cs.CV"], "comment": null, "summary": "Transformers, with their self-attention mechanisms for modeling long-range\ndependencies, have become a dominant paradigm in image restoration tasks.\nHowever, the high computational cost of self-attention limits scalability to\nhigh-resolution images, making efficiency-quality trade-offs a key research\nfocus. To address this, Restormer employs channel-wise self-attention, which\ncomputes attention across channels instead of spatial dimensions. While\neffective, this approach may overlook localized artifacts that are crucial for\nhigh-quality image restoration. To bridge this gap, we explore Dilated\nNeighborhood Attention (DiNA) as a promising alternative, inspired by its\nsuccess in high-level vision tasks. DiNA balances global context and local\nprecision by integrating sliding-window attention with mixed dilation factors,\neffectively expanding the receptive field without excessive overhead. However,\nour preliminary experiments indicate that directly applying this global-local\ndesign to the classic deblurring task hinders accurate visual restoration,\nprimarily due to the constrained global context understanding within local\nattention. To address this, we introduce a channel-aware module that\ncomplements local attention, effectively integrating global context without\nsacrificing pixel-level precision. The proposed DiNAT-IR, a Transformer-based\narchitecture specifically designed for image restoration, achieves competitive\nresults across multiple benchmarks, offering a high-quality solution for\ndiverse low-level computer vision problems.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eTransformer\u7684\u56fe\u50cf\u4fee\u590d\u65b9\u6cd5DiNAT-IR\uff0c\u901a\u8fc7\u7ed3\u5408\u5c40\u90e8\u548c\u5168\u5c40\u6ce8\u610f\u529b\u673a\u5236\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u65b9\u6cd5\u5728\u9ad8\u5206\u8fa8\u7387\u56fe\u50cf\u4e0a\u7684\u8ba1\u7b97\u6548\u7387\u548c\u5c40\u90e8\u7ec6\u8282\u6062\u590d\u95ee\u9898\u3002", "motivation": "Transformer\u7684\u81ea\u6ce8\u610f\u529b\u673a\u5236\u5728\u56fe\u50cf\u4fee\u590d\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u9ad8\u8ba1\u7b97\u6210\u672c\u9650\u5236\u4e86\u5176\u5728\u9ad8\u5206\u8fa8\u7387\u56fe\u50cf\u4e0a\u7684\u5e94\u7528\u3002\u73b0\u6709\u65b9\u6cd5\u5982Restormer\u901a\u8fc7\u901a\u9053\u81ea\u6ce8\u610f\u529b\u4f18\u5316\u6548\u7387\uff0c\u4f46\u53ef\u80fd\u5ffd\u7565\u5c40\u90e8\u7ec6\u8282\u3002", "method": "\u63d0\u51faDilated Neighborhood Attention (DiNA)\uff0c\u7ed3\u5408\u6ed1\u52a8\u7a97\u53e3\u6ce8\u610f\u529b\u548c\u6df7\u5408\u81a8\u80c0\u56e0\u5b50\uff0c\u5e73\u8861\u5168\u5c40\u548c\u5c40\u90e8\u4fe1\u606f\u3002\u8fdb\u4e00\u6b65\u5f15\u5165\u901a\u9053\u611f\u77e5\u6a21\u5757\uff0c\u589e\u5f3a\u5168\u5c40\u4e0a\u4e0b\u6587\u7406\u89e3\u3002", "result": "DiNAT-IR\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u7ade\u4e89\u6027\u7ed3\u679c\uff0c\u4e3a\u4f4e\u5c42\u6b21\u8ba1\u7b97\u673a\u89c6\u89c9\u95ee\u9898\u63d0\u4f9b\u4e86\u9ad8\u8d28\u91cf\u89e3\u51b3\u65b9\u6848\u3002", "conclusion": "DiNAT-IR\u901a\u8fc7\u521b\u65b0\u7684\u6ce8\u610f\u529b\u673a\u5236\u8bbe\u8ba1\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u56fe\u50cf\u4fee\u590d\u4e2d\u7684\u6548\u7387\u548c\u7cbe\u5ea6\u95ee\u9898\uff0c\u5177\u6709\u5e7f\u6cdb\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2507.18123", "pdf": "https://arxiv.org/pdf/2507.18123", "abs": "https://arxiv.org/abs/2507.18123", "authors": ["Sedigh Khademi", "Christopher Palmer", "Muhammad Javed", "Hazel Clothier", "Jim Buttery", "Gerardo Luis Dimaguila", "Jim Black"], "title": "Actively evaluating and learning the distinctions that matter: Vaccine safety signal detection from emergency triage notes", "categories": ["cs.AI", "cs.CL"], "comment": "14 pages", "summary": "The rapid development of COVID-19 vaccines has showcased the global\ncommunitys ability to combat infectious diseases. However, the need for\npost-licensure surveillance systems has grown due to the limited window for\nsafety data collection in clinical trials and early widespread implementation.\nThis study aims to employ Natural Language Processing techniques and Active\nLearning to rapidly develop a classifier that detects potential vaccine safety\nissues from emergency department notes. ED triage notes, containing expert,\nsuccinct vital patient information at the point of entry to health systems, can\nsignificantly contribute to timely vaccine safety signal surveillance. While\nkeyword-based classification can be effective, it may yield false positives and\ndemand extensive keyword modifications. This is exacerbated by the infrequency\nof vaccination-related ED presentations and their similarity to other reasons\nfor ED visits. NLP offers a more accurate and efficient alternative, albeit\nrequiring annotated data, which is often scarce in the medical field. Active\nlearning optimizes the annotation process and the quality of annotated data,\nwhich can result in faster model implementation and improved model performance.\nThis work combines active learning, data augmentation, and active learning and\nevaluation techniques to create a classifier that is used to enhance vaccine\nsafety surveillance from ED triage notes.", "AI": {"tldr": "\u8be5\u7814\u7a76\u5229\u7528\u81ea\u7136\u8bed\u8a00\u5904\u7406\uff08NLP\uff09\u548c\u4e3b\u52a8\u5b66\u4e60\u6280\u672f\uff0c\u5f00\u53d1\u4e86\u4e00\u79cd\u5206\u7c7b\u5668\uff0c\u7528\u4e8e\u4ece\u6025\u8bca\u79d1\u5206\u8bca\u8bb0\u5f55\u4e2d\u5feb\u901f\u68c0\u6d4b\u6f5c\u5728\u7684\u75ab\u82d7\u5b89\u5168\u95ee\u9898\uff0c\u4ee5\u5f25\u8865\u4e34\u5e8a\u8bd5\u9a8c\u4e2d\u5b89\u5168\u6570\u636e\u6536\u96c6\u7a97\u53e3\u6709\u9650\u7684\u4e0d\u8db3\u3002", "motivation": "\u7531\u4e8e\u4e34\u5e8a\u8bd5\u9a8c\u4e2d\u5b89\u5168\u6570\u636e\u6536\u96c6\u65f6\u95f4\u6709\u9650\uff0c\u4e14\u75ab\u82d7\u5e7f\u6cdb\u63a5\u79cd\u540e\u9700\u8981\u53ca\u65f6\u76d1\u6d4b\u5b89\u5168\u95ee\u9898\uff0c\u56e0\u6b64\u9700\u8981\u5f00\u53d1\u4e00\u79cd\u9ad8\u6548\u7684\u65b9\u6cd5\u6765\u8bc6\u522b\u6f5c\u5728\u7684\u75ab\u82d7\u5b89\u5168\u95ee\u9898\u3002", "method": "\u7ed3\u5408\u81ea\u7136\u8bed\u8a00\u5904\u7406\uff08NLP\uff09\u3001\u4e3b\u52a8\u5b66\u4e60\u548c\u6570\u636e\u589e\u5f3a\u6280\u672f\uff0c\u5f00\u53d1\u5206\u7c7b\u5668\uff0c\u5229\u7528\u6025\u8bca\u79d1\u5206\u8bca\u8bb0\u5f55\u8fdb\u884c\u75ab\u82d7\u5b89\u5168\u4fe1\u53f7\u76d1\u6d4b\u3002", "result": "\u8be5\u65b9\u6cd5\u80fd\u591f\u66f4\u51c6\u786e\u3001\u9ad8\u6548\u5730\u68c0\u6d4b\u75ab\u82d7\u5b89\u5168\u95ee\u9898\uff0c\u51cf\u5c11\u8bef\u62a5\uff0c\u5e76\u4f18\u5316\u6807\u6ce8\u6570\u636e\u7684\u8d28\u91cf\u3002", "conclusion": "\u901a\u8fc7NLP\u548c\u4e3b\u52a8\u5b66\u4e60\u7684\u7ed3\u5408\uff0c\u53ef\u4ee5\u663e\u8457\u63d0\u5347\u75ab\u82d7\u5b89\u5168\u76d1\u6d4b\u7684\u6548\u7387\u548c\u51c6\u786e\u6027\uff0c\u4e3a\u516c\u5171\u536b\u751f\u63d0\u4f9b\u6709\u529b\u652f\u6301\u3002"}}
{"id": "2507.18206", "pdf": "https://arxiv.org/pdf/2507.18206", "abs": "https://arxiv.org/abs/2507.18206", "authors": ["Arup Kumar Sahoo", "Itzik Klein"], "title": "MoRPI-PINN: A Physics-Informed Framework for Mobile Robot Pure Inertial Navigation", "categories": ["cs.RO", "cs.AI"], "comment": "9 pages, 5 figures", "summary": "A fundamental requirement for full autonomy in mobile robots is accurate\nnavigation even in situations where satellite navigation or cameras are\nunavailable. In such practical situations, relying only on inertial sensors\nwill result in navigation solution drift due to the sensors' inherent noise and\nerror terms. One of the emerging solutions to mitigate drift is to maneuver the\nrobot in a snake-like slithering motion to increase the inertial\nsignal-to-noise ratio, allowing the regression of the mobile robot position. In\nthis work, we propose MoRPI-PINN as a physics-informed neural network framework\nfor accurate inertial-based mobile robot navigation. By embedding physical laws\nand constraints into the training process, MoRPI-PINN is capable of providing\nan accurate and robust navigation solution. Using real-world experiments, we\nshow accuracy improvements of over 85% compared to other approaches. MoRPI-PINN\nis a lightweight approach that can be implemented even on edge devices and used\nin any typical mobile robot application.", "AI": {"tldr": "MoRPI-PINN\u662f\u4e00\u79cd\u57fa\u4e8e\u7269\u7406\u4fe1\u606f\u7684\u795e\u7ecf\u7f51\u7edc\u6846\u67b6\uff0c\u7528\u4e8e\u63d0\u9ad8\u79fb\u52a8\u673a\u5668\u4eba\u5728\u65e0\u536b\u661f\u6216\u6444\u50cf\u5934\u60c5\u51b5\u4e0b\u7684\u60ef\u6027\u5bfc\u822a\u7cbe\u5ea6\u3002", "motivation": "\u89e3\u51b3\u5728\u536b\u661f\u5bfc\u822a\u6216\u6444\u50cf\u5934\u4e0d\u53ef\u7528\u65f6\uff0c\u4ec5\u4f9d\u8d56\u60ef\u6027\u4f20\u611f\u5668\u5bfc\u81f4\u7684\u5bfc\u822a\u6f02\u79fb\u95ee\u9898\u3002", "method": "\u901a\u8fc7\u86c7\u5f62\u8fd0\u52a8\u589e\u52a0\u60ef\u6027\u4fe1\u53f7\u7684\u4fe1\u566a\u6bd4\uff0c\u5e76\u5229\u7528\u7269\u7406\u4fe1\u606f\u795e\u7ecf\u7f51\u7edc\u6846\u67b6\uff08MoRPI-PINN\uff09\u5d4c\u5165\u7269\u7406\u5b9a\u5f8b\u548c\u7ea6\u675f\u8fdb\u884c\u8bad\u7ec3\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\uff0cMoRPI-PINN\u7684\u5bfc\u822a\u7cbe\u5ea6\u6bd4\u5176\u4ed6\u65b9\u6cd5\u63d0\u9ad8\u4e8685%\u4ee5\u4e0a\u3002", "conclusion": "MoRPI-PINN\u662f\u4e00\u79cd\u8f7b\u91cf\u7ea7\u4e14\u9002\u7528\u4e8e\u8fb9\u7f18\u8bbe\u5907\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u53ef\u5e7f\u6cdb\u5e94\u7528\u4e8e\u79fb\u52a8\u673a\u5668\u4eba\u5bfc\u822a\u3002"}}
{"id": "2507.18061", "pdf": "https://arxiv.org/pdf/2507.18061", "abs": "https://arxiv.org/abs/2507.18061", "authors": ["Zehan Li", "Hongjie Chen", "Yuxin Zhang", "Jing Zhou", "Xuening Wang", "Hang Lv", "Mengjie Du", "Yaodong Song", "Jie Lian", "Jian Kang", "Jie Li", "Yongxiang Li", "Zhongjiang He", "Xuelong Li"], "title": "TELEVAL: A Dynamic Benchmark Designed for Spoken Language Models in Chinese Interactive Scenarios", "categories": ["cs.CL", "cs.AI", "cs.SD", "eess.AS"], "comment": null, "summary": "Spoken language models (SLMs) have seen rapid progress in recent years, along\nwith the development of numerous benchmarks for evaluating their performance.\nHowever, most existing benchmarks primarily focus on evaluating whether SLMs\ncan perform complex tasks comparable to those tackled by large language models\n(LLMs), often failing to align with how users naturally interact in real-world\nconversational scenarios. In this paper, we propose TELEVAL, a dynamic\nbenchmark specifically designed to evaluate SLMs' effectiveness as\nconversational agents in realistic Chinese interactive settings. TELEVAL\ndefines three evaluation dimensions: Explicit Semantics, Paralinguistic and\nImplicit Semantics, and System Abilities. It adopts a dialogue format\nconsistent with real-world usage and evaluates text and audio outputs\nseparately. TELEVAL particularly focuses on the model's ability to extract\nimplicit cues from user speech and respond appropriately without additional\ninstructions. Our experiments demonstrate that despite recent progress,\nexisting SLMs still have considerable room for improvement in natural\nconversational tasks. We hope that TELEVAL can serve as a user-centered\nevaluation framework that directly reflects the user experience and contributes\nto the development of more capable dialogue-oriented SLMs.", "AI": {"tldr": "TELEVAL\u662f\u4e00\u4e2a\u52a8\u6001\u57fa\u51c6\u6d4b\u8bd5\uff0c\u4e13\u4e3a\u8bc4\u4f30\u4e2d\u6587\u4ea4\u4e92\u573a\u666f\u4e2d\u53e3\u8bed\u8bed\u8a00\u6a21\u578b\uff08SLMs\uff09\u4f5c\u4e3a\u5bf9\u8bdd\u4ee3\u7406\u7684\u6709\u6548\u6027\u800c\u8bbe\u8ba1\uff0c\u5173\u6ce8\u663e\u5f0f\u8bed\u4e49\u3001\u526f\u8bed\u8a00\u548c\u9690\u5f0f\u8bed\u4e49\u4ee5\u53ca\u7cfb\u7edf\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u57fa\u51c6\u6d4b\u8bd5\u591a\u5173\u6ce8SLMs\u80fd\u5426\u5b8c\u6210\u590d\u6742\u4efb\u52a1\uff0c\u800c\u5ffd\u7565\u4e86\u7528\u6237\u5728\u5b9e\u9645\u5bf9\u8bdd\u4e2d\u7684\u81ea\u7136\u4ea4\u4e92\u65b9\u5f0f\u3002", "method": "TELEVAL\u5b9a\u4e49\u4e86\u4e09\u4e2a\u8bc4\u4f30\u7ef4\u5ea6\uff0c\u91c7\u7528\u5bf9\u8bdd\u5f62\u5f0f\uff0c\u5206\u522b\u8bc4\u4f30\u6587\u672c\u548c\u97f3\u9891\u8f93\u51fa\uff0c\u7279\u522b\u5173\u6ce8\u6a21\u578b\u4ece\u7528\u6237\u8bed\u97f3\u4e2d\u63d0\u53d6\u9690\u5f0f\u7ebf\u7d22\u5e76\u9002\u5f53\u56de\u5e94\u7684\u80fd\u529b\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u73b0\u6709SLMs\u5728\u81ea\u7136\u5bf9\u8bdd\u4efb\u52a1\u4e2d\u4ecd\u6709\u8f83\u5927\u6539\u8fdb\u7a7a\u95f4\u3002", "conclusion": "TELEVAL\u53ef\u4f5c\u4e3a\u4ee5\u7528\u6237\u4e3a\u4e2d\u5fc3\u7684\u8bc4\u4f30\u6846\u67b6\uff0c\u4fc3\u8fdb\u5bf9\u8bdd\u5bfc\u5411SLMs\u7684\u53d1\u5c55\u3002"}}
{"id": "2507.18623", "pdf": "https://arxiv.org/pdf/2507.18623", "abs": "https://arxiv.org/abs/2507.18623", "authors": ["Xuhui Kang", "Sung-Wook Lee", "Haolin Liu", "Yuyan Wang", "Yen-Ling Kuo"], "title": "Moving Out: Physically-grounded Human-AI Collaboration", "categories": ["cs.LG", "cs.AI", "cs.MA"], "comment": "24 pages, 8 figures", "summary": "The ability to adapt to physical actions and constraints in an environment is\ncrucial for embodied agents (e.g., robots) to effectively collaborate with\nhumans. Such physically grounded human-AI collaboration must account for the\nincreased complexity of the continuous state-action space and constrained\ndynamics caused by physical constraints. In this paper, we introduce\n\\textit{Moving Out}, a new human-AI collaboration benchmark that resembles a\nwide range of collaboration modes affected by physical attributes and\nconstraints, such as moving heavy items together and maintaining consistent\nactions to move a big item around a corner. Using Moving Out, we designed two\ntasks and collected human-human interaction data to evaluate models' abilities\nto adapt to diverse human behaviors and unseen physical attributes. To address\nthe challenges in physical environments, we propose a novel method, BASS\n(Behavior Augmentation, Simulation, and Selection), to enhance the diversity of\nagents and their understanding of the outcome of actions. Our experiments show\nthat BASS outperforms state-of-the-art models in AI-AI and human-AI\ncollaboration. The project page is available at\n\\href{https://live-robotics-uva.github.io/movingout_ai/}{https://live-robotics-uva.github.io/movingout\\_ai/}.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u540d\u4e3aMoving Out\u7684\u65b0\u57fa\u51c6\uff0c\u7528\u4e8e\u8bc4\u4f30\u4eba-AI\u534f\u4f5c\u4e2d\u7269\u7406\u7ea6\u675f\u4e0b\u7684\u9002\u5e94\u6027\uff0c\u5e76\u63d0\u51fa\u4e86BASS\u65b9\u6cd5\u4ee5\u63d0\u5347AI\u5728\u590d\u6742\u7269\u7406\u73af\u5883\u4e2d\u7684\u8868\u73b0\u3002", "motivation": "\u7814\u7a76\u52a8\u673a\u662f\u89e3\u51b3\u5177\u8eab\u667a\u80fd\u4f53\uff08\u5982\u673a\u5668\u4eba\uff09\u5728\u7269\u7406\u7ea6\u675f\u4e0b\u4e0e\u4eba\u7c7b\u534f\u4f5c\u65f6\u9762\u4e34\u7684\u8fde\u7eed\u72b6\u6001-\u52a8\u4f5c\u7a7a\u95f4\u548c\u52a8\u6001\u7ea6\u675f\u7684\u590d\u6742\u6027\u3002", "method": "\u63d0\u51fa\u4e86BASS\u65b9\u6cd5\uff08\u884c\u4e3a\u589e\u5f3a\u3001\u6a21\u62df\u548c\u9009\u62e9\uff09\uff0c\u901a\u8fc7\u589e\u5f3a\u884c\u4e3a\u591a\u6837\u6027\u548c\u52a8\u4f5c\u7ed3\u679c\u7406\u89e3\u6765\u5e94\u5bf9\u7269\u7406\u73af\u5883\u4e2d\u7684\u6311\u6218\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cBASS\u5728AI-AI\u548c\u4eba-AI\u534f\u4f5c\u4e2d\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u6a21\u578b\u3002", "conclusion": "Moving Out\u57fa\u51c6\u548cBASS\u65b9\u6cd5\u4e3a\u7269\u7406\u7ea6\u675f\u4e0b\u7684\u4eba-AI\u534f\u4f5c\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u8bc4\u4f30\u548c\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.17791", "pdf": "https://arxiv.org/pdf/2507.17791", "abs": "https://arxiv.org/abs/2507.17791", "authors": ["Eduardo Aguilar-Bejarano", "Daniel Lea", "Karthikeyan Sivakumar", "Jimiama M. Mase", "Reza Omidvar", "Ruizhe Li", "Troy Kettle", "James Mitchell-White", "Morgan R Alexander", "David A Winkler", "Grazziela Figueredo"], "title": "Helix 1.0: An Open-Source Framework for Reproducible and Interpretable Machine Learning on Tabular Scientific Data", "categories": ["cs.LG", "cs.AI"], "comment": "17 pages", "summary": "Helix is an open-source, extensible, Python-based software framework to\nfacilitate reproducible and interpretable machine learning workflows for\ntabular data. It addresses the growing need for transparent experimental data\nanalytics provenance, ensuring that the entire analytical process -- including\ndecisions around data transformation and methodological choices -- is\ndocumented, accessible, reproducible, and comprehensible to relevant\nstakeholders. The platform comprises modules for standardised data\npreprocessing, visualisation, machine learning model training, evaluation,\ninterpretation, results inspection, and model prediction for unseen data. To\nfurther empower researchers without formal training in data science to derive\nmeaningful and actionable insights, Helix features a user-friendly interface\nthat enables the design of computational experiments, inspection of outcomes,\nincluding a novel interpretation approach to machine learning decisions using\nlinguistic terms all within an integrated environment. Released under the MIT\nlicence, Helix is accessible via GitHub and PyPI, supporting community-driven\ndevelopment and promoting adherence to the FAIR principles.", "AI": {"tldr": "Helix\u662f\u4e00\u4e2a\u57fa\u4e8ePython\u7684\u5f00\u6e90\u8f6f\u4ef6\u6846\u67b6\uff0c\u65e8\u5728\u4e3a\u8868\u683c\u6570\u636e\u63d0\u4f9b\u53ef\u91cd\u73b0\u548c\u53ef\u89e3\u91ca\u7684\u673a\u5668\u5b66\u4e60\u5de5\u4f5c\u6d41\u7a0b\uff0c\u652f\u6301\u900f\u660e\u5316\u7684\u6570\u636e\u5206\u6790\u8fc7\u7a0b\u3002", "motivation": "\u89e3\u51b3\u673a\u5668\u5b66\u4e60\u5de5\u4f5c\u6d41\u4e2d\u5b9e\u9a8c\u6570\u636e\u548c\u5206\u6790\u8fc7\u7a0b\u7684\u53ef\u91cd\u73b0\u6027\u3001\u53ef\u89e3\u91ca\u6027\u548c\u900f\u660e\u6027\u95ee\u9898\uff0c\u540c\u65f6\u4e3a\u975e\u6570\u636e\u79d1\u5b66\u80cc\u666f\u7684\u7814\u7a76\u8005\u63d0\u4f9b\u6613\u7528\u5de5\u5177\u3002", "method": "Helix\u63d0\u4f9b\u6807\u51c6\u5316\u6a21\u5757\uff0c\u6db5\u76d6\u6570\u636e\u9884\u5904\u7406\u3001\u53ef\u89c6\u5316\u3001\u6a21\u578b\u8bad\u7ec3\u3001\u8bc4\u4f30\u3001\u89e3\u91ca\u3001\u7ed3\u679c\u68c0\u67e5\u548c\u9884\u6d4b\u7b49\u529f\u80fd\uff0c\u5e76\u96c6\u6210\u7528\u6237\u53cb\u597d\u754c\u9762\u548c\u65b0\u578b\u89e3\u91ca\u65b9\u6cd5\u3002", "result": "Helix\u901a\u8fc7MIT\u8bb8\u53ef\u8bc1\u53d1\u5e03\uff0c\u652f\u6301\u793e\u533a\u5f00\u53d1\uff0c\u5e76\u9075\u5faaFAIR\u539f\u5219\uff0c\u4fc3\u8fdb\u900f\u660e\u5316\u548c\u53ef\u91cd\u73b0\u7684\u673a\u5668\u5b66\u4e60\u7814\u7a76\u3002", "conclusion": "Helix\u662f\u4e00\u4e2a\u5f3a\u5927\u7684\u5de5\u5177\uff0c\u80fd\u591f\u63d0\u5347\u673a\u5668\u5b66\u4e60\u5de5\u4f5c\u6d41\u7684\u900f\u660e\u5ea6\u548c\u53ef\u89e3\u91ca\u6027\uff0c\u5c24\u5176\u9002\u5408\u975e\u4e13\u4e1a\u7528\u6237\u548c\u9700\u8981\u53ef\u91cd\u73b0\u7814\u7a76\u7684\u573a\u666f\u3002"}}
{"id": "2507.17974", "pdf": "https://arxiv.org/pdf/2507.17974", "abs": "https://arxiv.org/abs/2507.17974", "authors": ["Fitsum Gaim", "Jong C. Park"], "title": "Natural Language Processing for Tigrinya: Current State and Future Directions", "categories": ["cs.CL", "cs.AI", "I.2.7"], "comment": null, "summary": "Despite being spoken by millions of people, Tigrinya remains severely\nunderrepresented in Natural Language Processing (NLP) research. This work\npresents a comprehensive survey of NLP research for Tigrinya, analyzing over 40\nstudies spanning more than a decade of work from 2011 to 2025. We\nsystematically review the current state of computational resources, models, and\napplications across ten distinct downstream tasks, including morphological\nprocessing, machine translation, speech recognition, and question-answering.\nOur analysis reveals a clear trajectory from foundational, rule-based systems\nto modern neural architectures, with progress consistently unlocked by resource\ncreation milestones. We identify key challenges rooted in Tigrinya's\nmorphological complexity and resource scarcity, while highlighting promising\nresearch directions, including morphology-aware modeling, cross-lingual\ntransfer, and community-centered resource development. This work serves as both\na comprehensive reference for researchers and a roadmap for advancing Tigrinya\nNLP. A curated metadata of the surveyed studies and resources is made publicly\navailable.\\footnote{Tigrinya NLP Anthology:\nhttps://github.com/fgaim/tigrinya-nlp-anthology.", "AI": {"tldr": "\u672c\u6587\u7efc\u8ff0\u4e86\u63d0\u683c\u91cc\u5c3c\u4e9a\u8bed\uff08Tigrinya\uff09\u5728\u81ea\u7136\u8bed\u8a00\u5904\u7406\uff08NLP\uff09\u9886\u57df\u7684\u7814\u7a76\u73b0\u72b6\uff0c\u5206\u6790\u4e8640\u591a\u9879\u7814\u7a76\uff0c\u63ed\u793a\u4e86\u4ece\u89c4\u5219\u7cfb\u7edf\u5230\u795e\u7ecf\u67b6\u6784\u7684\u6f14\u53d8\uff0c\u5e76\u63d0\u51fa\u4e86\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002", "motivation": "\u63d0\u683c\u91cc\u5c3c\u4e9a\u8bed\u5728NLP\u7814\u7a76\u4e2d\u4ee3\u8868\u6027\u4e0d\u8db3\uff0c\u672c\u6587\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\uff0c\u4e3a\u7814\u7a76\u8005\u63d0\u4f9b\u53c2\u8003\u548c\u8def\u7ebf\u56fe\u3002", "method": "\u7cfb\u7edf\u56de\u987e\u4e862011\u81f32025\u5e74\u95f440\u591a\u9879\u7814\u7a76\uff0c\u6db5\u76d610\u79cd\u4e0b\u6e38\u4efb\u52a1\uff0c\u5206\u6790\u8d44\u6e90\u3001\u6a21\u578b\u548c\u5e94\u7528\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u4ece\u89c4\u5219\u7cfb\u7edf\u5230\u795e\u7ecf\u67b6\u6784\u7684\u6f14\u53d8\uff0c\u8d44\u6e90\u521b\u5efa\u662f\u5173\u952e\u8fdb\u5c55\u70b9\uff0c\u4f46\u8bed\u8a00\u5f62\u6001\u590d\u6742\u6027\u548c\u8d44\u6e90\u7a00\u7f3a\u4ecd\u662f\u6311\u6218\u3002", "conclusion": "\u672c\u6587\u4e3a\u63d0\u683c\u91cc\u5c3c\u4e9a\u8bedNLP\u7814\u7a76\u63d0\u4f9b\u4e86\u5168\u9762\u53c2\u8003\uff0c\u5e76\u63d0\u51fa\u4e86\u5f62\u6001\u611f\u77e5\u5efa\u6a21\u3001\u8de8\u8bed\u8a00\u8fc1\u79fb\u7b49\u672a\u6765\u65b9\u5411\u3002"}}
{"id": "2507.17957", "pdf": "https://arxiv.org/pdf/2507.17957", "abs": "https://arxiv.org/abs/2507.17957", "authors": ["Md. Al-Masrur Khan", "Durgakant Pushp", "Lantao Liu"], "title": "AFRDA: Attentive Feature Refinement for Domain Adaptive Semantic Segmentation", "categories": ["cs.CV"], "comment": null, "summary": "In Unsupervised Domain Adaptive Semantic Segmentation (UDA-SS), a model is\ntrained on labeled source domain data (e.g., synthetic images) and adapted to\nan unlabeled target domain (e.g., real-world images) without access to target\nannotations. Existing UDA-SS methods often struggle to balance fine-grained\nlocal details with global contextual information, leading to segmentation\nerrors in complex regions. To address this, we introduce the Adaptive Feature\nRefinement (AFR) module, which enhances segmentation accuracy by refining\nhighresolution features using semantic priors from low-resolution logits. AFR\nalso integrates high-frequency components, which capture fine-grained\nstructures and provide crucial boundary information, improving object\ndelineation. Additionally, AFR adaptively balances local and global information\nthrough uncertaintydriven attention, reducing misclassifications. Its\nlightweight design allows seamless integration into HRDA-based UDA methods,\nleading to state-of-the-art segmentation performance. Our approach improves\nexisting UDA-SS methods by 1.05% mIoU on GTA V --> Cityscapes and 1.04% mIoU on\nSynthia-->Cityscapes. The implementation of our framework is available at:\nhttps://github.com/Masrur02/AFRDA", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u81ea\u9002\u5e94\u7279\u5f81\u7ec6\u5316\uff08AFR\uff09\u6a21\u5757\uff0c\u7528\u4e8e\u65e0\u76d1\u7763\u57df\u81ea\u9002\u5e94\u8bed\u4e49\u5206\u5272\uff08UDA-SS\uff09\uff0c\u901a\u8fc7\u7ed3\u5408\u9ad8\u4f4e\u5206\u8fa8\u7387\u7279\u5f81\u548c\u9ad8\u9891\u7ec4\u4ef6\uff0c\u63d0\u5347\u5206\u5272\u7cbe\u5ea6\u3002", "motivation": "\u73b0\u6709UDA-SS\u65b9\u6cd5\u96be\u4ee5\u5e73\u8861\u5c40\u90e8\u7ec6\u8282\u4e0e\u5168\u5c40\u4e0a\u4e0b\u6587\u4fe1\u606f\uff0c\u5bfc\u81f4\u590d\u6742\u533a\u57df\u7684\u5206\u5272\u9519\u8bef\u3002", "method": "\u5f15\u5165AFR\u6a21\u5757\uff0c\u5229\u7528\u4f4e\u5206\u8fa8\u7387\u8bed\u4e49\u5148\u9a8c\u7ec6\u5316\u9ad8\u5206\u8fa8\u7387\u7279\u5f81\uff0c\u5e76\u96c6\u6210\u9ad8\u9891\u7ec4\u4ef6\u548c\u4e0d\u786e\u5b9a\u6027\u9a71\u52a8\u7684\u6ce8\u610f\u529b\u673a\u5236\u3002", "result": "\u5728GTA V\u2192Cityscapes\u548cSynthia\u2192Cityscapes\u4e0a\u5206\u522b\u63d0\u5347\u4e861.05%\u548c1.04%\u7684mIoU\u3002", "conclusion": "AFR\u6a21\u5757\u8f7b\u91cf\u4e14\u9ad8\u6548\uff0c\u663e\u8457\u63d0\u5347\u4e86UDA-SS\u7684\u5206\u5272\u6027\u80fd\u3002"}}
{"id": "2507.18145", "pdf": "https://arxiv.org/pdf/2507.18145", "abs": "https://arxiv.org/abs/2507.18145", "authors": ["Moritz Sch\u00f6nherr", "Carsten Lutz"], "title": "Logical Characterizations of GNNs with Mean Aggregation", "categories": ["cs.AI", "cs.LO"], "comment": null, "summary": "We study the expressive power of graph neural networks (GNNs) with mean as\nthe aggregation function. In the non-uniform setting, we show that such GNNs\nhave exactly the same expressive power as ratio modal logic, which has modal\noperators expressing that at least a certain ratio of the successors of a\nvertex satisfies a specified property. The non-uniform expressive power of mean\nGNNs is thus higher than that of GNNs with max aggregation, but lower than for\nsum aggregation--the latter are characterized by modal logic and graded modal\nlogic, respectively. In the uniform setting, we show that the expressive power\nrelative to MSO is exactly that of alternation-free modal logic, under the\nnatural assumptions that combination functions are continuous and\nclassification functions are thresholds. This implies that, relative to MSO and\nin the uniform setting, mean GNNs are strictly less expressive than sum GNNs\nand max GNNs. When any of the assumptions is dropped, the expressive power\nincreases.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u4f7f\u7528\u5747\u503c\u805a\u5408\u51fd\u6570\u7684\u56fe\u795e\u7ecf\u7f51\u7edc\uff08GNNs\uff09\u7684\u8868\u8fbe\u80fd\u529b\uff0c\u53d1\u73b0\u5176\u5728\u975e\u5747\u5300\u548c\u5747\u5300\u8bbe\u7f6e\u4e0b\u5206\u522b\u4e0e\u7279\u5b9a\u6a21\u6001\u903b\u8f91\u7b49\u4ef7\uff0c\u4e14\u8868\u8fbe\u80fd\u529b\u4ecb\u4e8e\u6700\u5927\u503c\u548c\u6c42\u548c\u805a\u5408\u4e4b\u95f4\u3002", "motivation": "\u63a2\u7d22\u5747\u503c\u805a\u5408GNNs\u7684\u8868\u8fbe\u80fd\u529b\uff0c\u4ee5\u7406\u89e3\u5176\u5728\u4e0d\u540c\u903b\u8f91\u6846\u67b6\u4e0b\u7684\u8868\u73b0\u3002", "method": "\u901a\u8fc7\u975e\u5747\u5300\u548c\u5747\u5300\u8bbe\u7f6e\u4e0b\u7684\u7406\u8bba\u5206\u6790\uff0c\u6bd4\u8f83\u5747\u503cGNNs\u4e0e\u5176\u4ed6\u805a\u5408\u51fd\u6570\u7684\u8868\u8fbe\u80fd\u529b\u3002", "result": "\u5747\u503cGNNs\u5728\u975e\u5747\u5300\u8bbe\u7f6e\u4e0b\u4e0e\u6bd4\u7387\u6a21\u6001\u903b\u8f91\u7b49\u4ef7\uff0c\u5747\u5300\u8bbe\u7f6e\u4e0b\u4e0e\u65e0\u4ea4\u66ff\u6a21\u6001\u903b\u8f91\u7b49\u4ef7\uff0c\u8868\u8fbe\u80fd\u529b\u4ecb\u4e8e\u6700\u5927\u503c\u548c\u6c42\u548c\u805a\u5408\u4e4b\u95f4\u3002", "conclusion": "\u5747\u503cGNNs\u7684\u8868\u8fbe\u80fd\u529b\u53d7\u8bbe\u7f6e\u548c\u5047\u8bbe\u5f71\u54cd\uff0c\u4e25\u683c\u4f4e\u4e8e\u6c42\u548c\u548c\u6700\u5927\u503c\u805a\u5408GNNs\uff0c\u4f46\u653e\u5bbd\u5047\u8bbe\u53ef\u63d0\u5347\u5176\u80fd\u529b\u3002"}}
