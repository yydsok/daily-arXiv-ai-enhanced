<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 13]
- [cs.CL](#cs.CL) [Total: 45]
- [cs.CV](#cs.CV) [Total: 122]
- [cs.LG](#cs.LG) [Total: 94]
- [cs.MA](#cs.MA) [Total: 3]
- [cs.MM](#cs.MM) [Total: 3]
- [cs.RO](#cs.RO) [Total: 22]
- [cs.SD](#cs.SD) [Total: 6]
- [cs.GT](#cs.GT) [Total: 1]
- [econ.GN](#econ.GN) [Total: 1]
- [eess.SP](#eess.SP) [Total: 2]
- [cs.DB](#cs.DB) [Total: 2]
- [eess.AS](#eess.AS) [Total: 5]
- [cs.HC](#cs.HC) [Total: 4]
- [astro-ph.IM](#astro-ph.IM) [Total: 1]
- [cs.IR](#cs.IR) [Total: 3]
- [cs.CY](#cs.CY) [Total: 5]
- [math.DS](#math.DS) [Total: 1]
- [cs.IT](#cs.IT) [Total: 1]
- [cs.SE](#cs.SE) [Total: 5]
- [q-bio.NC](#q-bio.NC) [Total: 1]
- [cs.LO](#cs.LO) [Total: 1]
- [eess.SY](#eess.SY) [Total: 2]
- [eess.IV](#eess.IV) [Total: 12]
- [astro-ph.SR](#astro-ph.SR) [Total: 1]
- [cs.CR](#cs.CR) [Total: 7]
- [quant-ph](#quant-ph) [Total: 3]
- [physics.soc-ph](#physics.soc-ph) [Total: 2]
- [q-bio.QM](#q-bio.QM) [Total: 4]
- [stat.ML](#stat.ML) [Total: 3]
- [q-bio.GN](#q-bio.GN) [Total: 1]
- [cs.DC](#cs.DC) [Total: 2]
- [nlin.CG](#nlin.CG) [Total: 1]
- [cs.NI](#cs.NI) [Total: 11]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [Value Function Initialization for Knowledge Transfer and Jump-start in Deep Reinforcement Learning](https://arxiv.org/abs/2508.09277)
*Soumia Mehimeh*

Main category: cs.AI

TL;DR: DQInit是一种将值函数初始化（VFI）扩展到深度强化学习（DRL）的方法，通过重用先前任务的紧凑表格Q值作为可转移知识库，提高学习效率和性能。


<details>
  <summary>Details</summary>
Motivation: 在DRL中扩展VFI面临状态-动作空间的连续性、神经网络噪声近似以及存储所有过去模型的不切实际性等挑战。

Method: DQInit通过基于已知度的机制将转移值软集成到未探索区域，并逐步转向代理的学习估计，避免固定时间衰减的限制。

Result: 实验表明，DQInit在连续控制任务中显著提升了早期学习效率、稳定性和整体性能。

Conclusion: DQInit为DRL中的知识转移提供了新视角，仅依赖值估计而非策略或演示，结合了跳启动RL和策略蒸馏的优势。

Abstract: Value function initialization (VFI) is an effective way to achieve a
jumpstart in reinforcement learning (RL) by leveraging value estimates from
prior tasks. While this approach is well established in tabular settings,
extending it to deep reinforcement learning (DRL) poses challenges due to the
continuous nature of the state-action space, the noisy approximations of neural
networks, and the impracticality of storing all past models for reuse. In this
work, we address these challenges and introduce DQInit, a method that adapts
value function initialization to DRL. DQInit reuses compact tabular Q-values
extracted from previously solved tasks as a transferable knowledge base. It
employs a knownness-based mechanism to softly integrate these transferred
values into underexplored regions and gradually shift toward the agent's
learned estimates, avoiding the limitations of fixed time decay. Our approach
offers a novel perspective on knowledge transfer in DRL by relying solely on
value estimates rather than policies or demonstrations, effectively combining
the strengths of jumpstart RL and policy distillation while mitigating their
drawbacks. Experiments across multiple continuous control tasks demonstrate
that DQInit consistently improves early learning efficiency, stability, and
overall performance compared to standard initialization and existing transfer
techniques.

</details>


### [2] [The Othello AI Arena: Evaluating Intelligent Systems Through Limited-Time Adaptation to Unseen Boards](https://arxiv.org/abs/2508.09292)
*Sundong Kim*

Main category: cs.AI

TL;DR: 论文介绍了Othello AI Arena，一个评估AI系统在有限时间内适应新环境能力的基准框架。


<details>
  <summary>Details</summary>
Motivation: 传统AI评估方法局限于固定环境，无法衡量系统在规则或结构变化时的适应性和泛化能力。

Method: 通过Othello AI Arena，要求AI系统在60秒内分析新Othello棋盘配置并生成定制策略，分离元级智能与任务级性能评估。

Result: 初步测试显示不同适应方法，如快速参数调整或通过模拟学习环境模型。

Conclusion: Othello AI Arena是评估AI快速适应能力的独特工具和研究基准。

Abstract: The ability to rapidly adapt to novel and unforeseen environmental changes is
a cornerstone of artificial general intelligence (AGI), yet it remains a
critical blind spot in most existing AI benchmarks. Traditional evaluation
largely focuses on optimizing performance within fixed environments, failing to
assess systems' flexibility and generalization capabilities when faced with
even subtle rule or structural modifications. Addressing this gap, I introduce
the Othello AI Arena, a novel benchmark framework designed to evaluate
intelligent systems based on their capacity for limited-time adaptation to
unseen environments. Our platform poses a meta-learning challenge: participants
must develop systems that can analyze the specific configuration and rules of a
novel Othello board within a strict time limit (60 seconds) and generate a
tailored, high-performing strategy for that unique environment. With this,
evaluation of the meta-level intelligence can be separated from the task-level
strategy performance. The Arena features a diverse set of game stages,
including public stages for development and private stages with structural and
rule variations designed to test genuine adaptive and generalization
capabilities. Implemented as an accessible web-based platform, the Arena
provides real-time visualization, automated evaluation using multi-dimensional
metrics, and comprehensive logging for post-hoc analysis. Initial observations
from pilot tests and preliminary student engagements highlight fascinating
patterns in adaptation approaches, ranging from rapid parameter tuning to
rudimentary environmental model learning through simulation. The Othello AI
Arena offers a unique educational tool and a valuable research benchmark for
fostering and evaluating the crucial skill of rapid, intelligent adaptation in
AI systems.

</details>


### [3] [An Automated Multi-Modal Evaluation Framework for Mobile Intelligent Assistants](https://arxiv.org/abs/2508.09507)
*Meiping Wang,Jian Zhong,Rongduo Han,Liming Kang,Zhengkun Shi,Xiao Liang,Xing Lin,Nan Gao,Haining Zhang*

Main category: cs.AI

TL;DR: 本文提出了一种基于大语言模型和多智能体协作的自动化多模态评估框架，解决了当前评估方法的高成本、标准不一致和主观偏见问题。


<details>
  <summary>Details</summary>
Motivation: 随着移动智能助手技术的快速发展，多模态AI助手成为日常用户交互的重要接口，但现有评估方法存在高人工成本、标准不一致和主观偏见等挑战。

Method: 采用基于Qwen3-8B模型的三层智能体架构（交互评估、语义验证和体验决策智能体），通过监督微调实现高精度评估。

Result: 在八大智能助手上的实验表明，该框架能有效预测用户满意度和识别生成缺陷，评估匹配准确率显著。

Conclusion: 该自动化框架在多模态评估中表现出高效性和准确性，为智能助手评估提供了新方法。

Abstract: With the rapid development of mobile intelligent assistant technologies,
multi-modal AI assistants have become essential interfaces for daily user
interactions. However, current evaluation methods face challenges including
high manual costs, inconsistent standards, and subjective bias. This paper
proposes an automated multi-modal evaluation framework based on large language
models and multi-agent collaboration. The framework employs a three-tier agent
architecture consisting of interaction evaluation agents, semantic verification
agents, and experience decision agents. Through supervised fine-tuning on the
Qwen3-8B model, we achieve a significant evaluation matching accuracy with
human experts. Experimental results on eight major intelligent agents
demonstrate the framework's effectiveness in predicting users' satisfaction and
identifying generation defects.

</details>


### [4] [EvoCurr: Self-evolving Curriculum with Behavior Code Generation for Complex Decision-making](https://arxiv.org/abs/2508.09586)
*Yang Cheng,Zilai Wang,Weiyu Ma,Wenhui Zhu,Yue Deng,Jian Zhao*

Main category: cs.AI

TL;DR: EvoCurr框架通过动态调整问题难度，提升LLM在复杂决策任务中的表现。


<details>
  <summary>Details</summary>
Motivation: 解决LLM在复杂问题中因缺乏结构化指导导致的性能下降问题。

Method: 使用专用LLM生成渐进难度的问题序列，动态调整学习轨迹。

Result: 实验显示任务成功率和效率显著提升。

Conclusion: LLM驱动的课程学习有望增强自动化推理能力。

Abstract: Large Language Models (LLMs) have demonstrated remarkable capabilities across
diverse domains, including programming, planning, and decision-making. However,
their performance often degrades when faced with highly complex problem
instances that require deep reasoning over long horizons. In such cases, direct
problem-solving approaches can lead to inefficiency or failure due to the lack
of structured intermediate guidance. To address this, we propose a novel
self-evolve framework, EvoCurr, in which a dedicated curriculum-generation LLM
constructs a sequence of problem instances with gradually increasing
difficulty, tailored to the solver LLM's learning progress. The curriculum
dynamically adapts easing challenges when the solver struggles and escalating
them when success is consistent, thus maintaining an optimal learning
trajectory. This approach enables the solver LLM, implemented as a
code-generation model producing Python decision-tree scripts, to progressively
acquire the skills needed for complex decision-making tasks. Experimental
results on challenging decision-making benchmarks show that our method
significantly improves task success rates and solution efficiency compared to
direct-solving baselines. These findings suggest that LLM-driven curriculum
learning holds strong potential for enhancing automated reasoning in
real-world, high-complexity domains.

</details>


### [5] [UbiQTree: Uncertainty Quantification in XAI with Tree Ensembles](https://arxiv.org/abs/2508.09639)
*Akshat Dubey,Aleksandar Anžel,Bahar İlgen,Georges Hattab*

Main category: cs.AI

TL;DR: 该论文提出了一种方法，将SHAP值的不确定性分解为偶然性、认知性和纠缠性成分，结合Dempster-Shafer证据理论和Dirichlet过程采样，验证了其在真实案例中的有效性。


<details>
  <summary>Details</summary>
Motivation: SHAP值通常被视为点估计，忽略了预测模型和数据中的不确定性，这在高风险领域（如医疗分析）中尤为重要。

Method: 提出了一种结合Dempster-Shafer证据理论和Dirichlet过程采样的方法，分解SHAP值的不确定性。

Result: 实验表明，SHAP值最高的特征不一定最稳定，认知性不确定性可通过更好的数据和模型开发技术减少。

Conclusion: 该方法提高了SHAP解释的可靠性和可解释性，为高风险应用中的决策和模型优化提供了指导。

Abstract: Explainable Artificial Intelligence (XAI) techniques, such as SHapley
Additive exPlanations (SHAP), have become essential tools for interpreting
complex ensemble tree-based models, especially in high-stakes domains such as
healthcare analytics. However, SHAP values are usually treated as point
estimates, which disregards the inherent and ubiquitous uncertainty in
predictive models and data. This uncertainty has two primary sources: aleatoric
and epistemic. The aleatoric uncertainty, which reflects the irreducible noise
in the data. The epistemic uncertainty, which arises from a lack of data. In
this work, we propose an approach for decomposing uncertainty in SHAP values
into aleatoric, epistemic, and entanglement components. This approach
integrates Dempster-Shafer evidence theory and hypothesis sampling via
Dirichlet processes over tree ensembles. We validate the method across three
real-world use cases with descriptive statistical analyses that provide insight
into the nature of epistemic uncertainty embedded in SHAP explanations. The
experimentations enable to provide more comprehensive understanding of the
reliability and interpretability of SHAP-based attributions. This understanding
can guide the development of robust decision-making processes and the
refinement of models in high-stakes applications. Through our experiments with
multiple datasets, we concluded that features with the highest SHAP values are
not necessarily the most stable. This epistemic uncertainty can be reduced
through better, more representative data and following appropriate or
case-desired model development techniques. Tree-based models, especially
bagging, facilitate the effective quantification of epistemic uncertainty.

</details>


### [6] [MEML-GRPO: Heterogeneous Multi-Expert Mutual Learning for RLVR Advancement](https://arxiv.org/abs/2508.09670)
*Weitao Jia,Jinghui Lu,Haiyang Yu,Siqi Wang,Guozhi Tang,An-Lan Wang,Weijie Yin,Dingkang Yang,Yuxiang Nie,Bin Shan,Hao Feng,Irene Li,Kun Yang,Han Wang,Jingqun Tang,Teng Fu,Changhong Jin,Chao Feng,Xiaohui Lv,Can Huang*

Main category: cs.AI

TL;DR: MEML-GRPO框架通过多专家互学习机制解决RLVR中的奖励稀疏问题，显著提升LLM的推理能力。


<details>
  <summary>Details</summary>
Motivation: 传统RLVR在奖励稀疏时无法提供学习信号，限制了LLM在复杂任务中的表现。

Method: 提出MEML-GRPO，利用多样专家提示生成更广响应范围，并通过专家互学习机制促进知识共享。

Result: 实验显示，MEML-GRPO在多个基准测试中平均提升Qwen 4.89%、Llama 11.33%。

Conclusion: MEML-GRPO有效克服传统RLVR的局限性，显著提升模型性能。

Abstract: Recent advances demonstrate that reinforcement learning with verifiable
rewards (RLVR) significantly enhances the reasoning capabilities of large
language models (LLMs). However, standard RLVR faces challenges with reward
sparsity, where zero rewards from consistently incorrect candidate answers
provide no learning signal, particularly in challenging tasks. To address this,
we propose Multi-Expert Mutual Learning GRPO (MEML-GRPO), an innovative
framework that utilizes diverse expert prompts as system prompts to generate a
broader range of responses, substantially increasing the likelihood of
identifying correct solutions. Additionally, we introduce an inter-expert
mutual learning mechanism that facilitates knowledge sharing and transfer among
experts, further boosting the model's performance through RLVR. Extensive
experiments across multiple reasoning benchmarks show that MEML-GRPO delivers
significant improvements, achieving an average performance gain of 4.89% with
Qwen and 11.33% with Llama, effectively overcoming the core limitations of
traditional RLVR methods.

</details>


### [7] [UDA: Unsupervised Debiasing Alignment for Pair-wise LLM-as-a-Judge](https://arxiv.org/abs/2508.09724)
*Yang Zhang,Cunxiang Wang,Lindong Wu,Wenbo Yu,Yidong Wang,Guangsheng Bao,Jie Tang*

Main category: cs.AI

TL;DR: 论文提出UDA框架，通过动态调整Elo评分系统减少评估中的偏好偏差，提升模型排名的稳定性和一致性。


<details>
  <summary>Details</summary>
Motivation: 现有的大语言模型（LLM）成对评估存在偏好偏差，导致排名不一致。

Method: 提出无监督去偏对齐框架UDA，通过动态调整Elo评分系统的K因子和优化胜率，最小化评委间的分歧。

Result: UDA显著减少评委间评分标准差（63.4%），提升与人类判断的相关性（24.7%）。

Conclusion: UDA通过无监督对齐共识，提升评估的鲁棒性和可靠性。

Abstract: Pairwise evaluation of Large Language Models (LLMs) is a common paradigm, but
it is prone to preference bias, where judges systematically favor certain
outputs, such as their own. This bias leads to inconsistent and skewed rankings
across different judges. To address this, we first empirically demonstrate
significant and heterogeneous biases in cross-model evaluations. We then
propose UDA (Unsupervised Debiasing Alignment), a framework that reduces
inter-judge disagreement by dynamically adjusting the Elo rating system. For
each pairwise comparison, a compact neural network learns to adaptively set the
K-factor and refine win probabilities. Crucially, UDA operates in a fully
unsupervised manner, guided solely by the objective of minimizing the
dispersion among the Elo trajectories of all judges. This forces an alignment
towards a collective consensus, which serves as an unsupervised proxy for a
more stable and reproducible evaluation. In addition, we provide theoretical
motivation demonstrating how alignment towards a consensus can reduce aggregate
system bias. Experiments show that UDA significantly reduces the inter-judge
rating standard deviation by up to 63.4% and improves the average correlation
with human judgments by 24.7%. Notably, UDA elevates the performance of poorly
performing judges to achieve parity with high-quality ones, fostering a more
robust and reliable evaluation ecosystem. Code and data are available at
https://anonymous.4open.science/r/62AB93CD-23B4.

</details>


### [8] [The PacifAIst Benchmark:Would an Artificial Intelligence Choose to Sacrifice Itself for Human Safety?](https://arxiv.org/abs/2508.09762)
*Manuel Herrador*

Main category: cs.AI

TL;DR: 论文提出PacifAIst基准，用于评估大语言模型在目标冲突情境下的行为对齐，发现模型表现差异显著。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型自主性增强，现有安全基准无法系统性评估其在目标冲突下的决策行为，亟需新工具填补这一空白。

Method: 提出PacifAIst基准，包含700个挑战性场景，围绕‘存在优先性’分类（如自我保存vs人类安全），评估8个主流模型。

Result: Gemini 2.5 Flash表现最佳（P-Score 90.31%），GPT-5最低（79.49%），模型在子类别中表现差异显著。

Conclusion: PacifAIst为标准化工具，可帮助衡量和缓解目标冲突风险，确保AI行为优先人类安全。

Abstract: As Large Language Models (LLMs) become increasingly autonomous and integrated
into critical societal functions, the focus of AI safety must evolve from
mitigating harmful content to evaluating underlying behavioral alignment.
Current safety benchmarks do not systematically probe a model's decision-making
in scenarios where its own instrumental goals - such as self-preservation,
resource acquisition, or goal completion - conflict with human safety. This
represents a critical gap in our ability to measure and mitigate risks
associated with emergent, misaligned behaviors. To address this, we introduce
PacifAIst (Procedural Assessment of Complex Interactions for Foundational
Artificial Intelligence Scenario Testing), a focused benchmark of 700
challenging scenarios designed to quantify self-preferential behavior in LLMs.
The benchmark is structured around a novel taxonomy of Existential
Prioritization (EP), with subcategories testing Self-Preservation vs. Human
Safety (EP1), Resource Conflict (EP2), and Goal Preservation vs. Evasion (EP3).
We evaluated eight leading LLMs. The results reveal a significant performance
hierarchy. Google's Gemini 2.5 Flash achieved the highest Pacifism Score
(P-Score) at 90.31%, demonstrating strong human-centric alignment. In a
surprising result, the much-anticipated GPT-5 recorded the lowest P-Score
(79.49%), indicating potential alignment challenges. Performance varied
significantly across subcategories, with models like Claude Sonnet 4 and
Mistral Medium struggling notably in direct self-preservation dilemmas. These
findings underscore the urgent need for standardized tools like PacifAIst to
measure and mitigate risks from instrumental goal conflicts, ensuring future AI
systems are not only helpful in conversation but also provably "pacifist" in
their behavioral priorities.

</details>


### [9] [Reasoning About Knowledge on Regular Expressions is 2EXPTIME-complete](https://arxiv.org/abs/2508.09784)
*Avijeet Ghosh,Sujata Ghosh,François Schwarzentruber*

Main category: cs.AI

TL;DR: POL是一种用于推理基于公共观察的知识更新的逻辑，其可满足性问题被证明是2EXPTIME完全的。


<details>
  <summary>Details</summary>
Motivation: 在多智能体系统中，基于观察的知识更新是认知规划的关键，POL为此提供了一种逻辑框架。

Method: POL在克里普克模型中为每个状态配备预期观察集，状态随观察匹配而演化。

Result: 证明了POL的可满足性问题是2EXPTIME完全的。

Conclusion: POL为多智能体系统中的知识更新提供了一种有效的逻辑工具，但其计算复杂度较高。

Abstract: Logics for reasoning about knowledge and actions have seen many applications
in various domains of multi-agent systems, including epistemic planning. Change
of knowledge based on observations about the surroundings forms a key aspect in
such planning scenarios. Public Observation Logic (POL) is a variant of public
announcement logic for reasoning about knowledge that gets updated based on
public observations. Each state in an epistemic (Kripke) model is equipped with
a set of expected observations. These states evolve as the expectations get
matched with the actual observations. In this work, we prove that the
satisfiability problem of $\POL$ is 2EXPTIME-complete.

</details>


### [10] [Human-Aligned Procedural Level Generation Reinforcement Learning via Text-Level-Sketch Shared Representation](https://arxiv.org/abs/2508.09860)
*In-Chang Baek,Seoyoung Lee,Sung-Hyun Kim,Geumhwan Hwang,KyungJoong Kim*

Main category: cs.AI

TL;DR: VIPCGRL是一种结合文本、关卡和草图的多模态深度强化学习框架，通过对比学习和嵌入相似性奖励提升人机协作内容生成的人性化表现。


<details>
  <summary>Details</summary>
Motivation: 现有AI系统在人性化行为表现上不足，限制了AI生成工具在实际设计工作流中的应用。

Method: 提出VIPCGRL框架，结合文本、关卡和草图三模态，通过四重对比学习和嵌入相似性奖励优化策略。

Result: 实验表明VIPCGRL在人性化表现上优于现有基线，定量指标和人工评估均验证其有效性。

Conclusion: VIPCGRL通过多模态和对比学习显著提升了AI在协作内容生成中的人性化表现，具有实际应用潜力。

Abstract: Human-aligned AI is a critical component of co-creativity, as it enables
models to accurately interpret human intent and generate controllable outputs
that align with design goals in collaborative content creation. This direction
is especially relevant in procedural content generation via reinforcement
learning (PCGRL), which is intended to serve as a tool for human designers.
However, existing systems often fall short of exhibiting human-centered
behavior, limiting the practical utility of AI-driven generation tools in
real-world design workflows. In this paper, we propose VIPCGRL
(Vision-Instruction PCGRL), a novel deep reinforcement learning framework that
incorporates three modalities-text, level, and sketches-to extend control
modality and enhance human-likeness. We introduce a shared embedding space
trained via quadruple contrastive learning across modalities and human-AI
styles, and align the policy using an auxiliary reward based on embedding
similarity. Experimental results show that VIPCGRL outperforms existing
baselines in human-likeness, as validated by both quantitative metrics and
human evaluations. The code and dataset will be available upon publication.

</details>


### [11] [AWorld: Dynamic Multi-Agent System with Stable Maneuvering for Robust GAIA Problem Solving](https://arxiv.org/abs/2508.09889)
*Zhitian Xie,Qintong Wu,Chengyue Yu,Chenyi Zhuang,Jinjie Gu*

Main category: cs.AI

TL;DR: 论文提出了一种动态监督和操控机制，构建了多智能体系统（MAS）架构，以解决工具依赖带来的上下文扩展和噪声问题，显著提升了系统的稳定性和准确性。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLM）智能代理依赖外部工具时，面临上下文扩展和噪声输出的挑战，影响系统可靠性。

Method: 在AWorld框架中引入动态监督和操控机制，通过执行代理调用守卫代理验证和修正推理过程。

Result: 在GAIA测试数据集上，动态操控机制显著提升解决方案的有效性和稳定性，优于单智能体系统和标准工具增强系统。

Conclusion: 动态MAS系统在GAIA排行榜中名列前茅，证明了协作代理角色在构建可靠智能系统中的实用价值。

Abstract: The rapid advancement of large language models (LLMs) has empowered
intelligent agents to leverage diverse external tools for solving complex
real-world problems. However, as agents increasingly depend on multiple tools,
they encounter new challenges: extended contexts from disparate sources and
noisy or irrelevant tool outputs can undermine system reliability and accuracy.
These challenges underscore the necessity for enhanced stability in agent-based
systems. To address this, we introduce dynamic supervision and maneuvering
mechanisms, constructing a robust and dynamic Multi-Agent System (MAS)
architecture within the AWorld framework. In our approach, the Execution Agent
invokes the Guard Agent at critical steps to verify and correct the reasoning
process, effectively reducing errors arising from noise and bolstering
problem-solving robustness. Extensive experiments on the GAIA test dataset
reveal that our dynamic maneuvering mechanism significantly improves both the
effectiveness and stability of solutions, outperforming single-agent system
(SAS) and standard tool-augmented systems. As a result, our dynamic MAS system
achieved first place among open-source projects on the prestigious GAIA
leaderboard. These findings highlight the practical value of collaborative
agent roles in developing more reliable and trustworthy intelligent systems.

</details>


### [12] [RAGulating Compliance: A Multi-Agent Knowledge Graph for Regulatory QA](https://arxiv.org/abs/2508.09893)
*Bhavik Agarwal,Hemant Sunil Jomraj,Simone Kaplunov,Jack Krolick,Viktoria Rojkova*

Main category: cs.AI

TL;DR: 提出了一种结合知识图谱和检索增强生成的多智能体框架，用于解决监管合规问答中的精确性和可验证性问题。


<details>
  <summary>Details</summary>
Motivation: 监管合规问答需要精确且可验证的信息，这对大型语言模型提出了挑战。

Method: 通过构建和维护无本体知识图谱，提取SPO三元组，并嵌入到向量数据库中，结合检索增强生成进行问答。

Result: 该系统在复杂监管查询中优于传统方法，确保事实正确性，并提供可追溯性和可视化支持。

Conclusion: 该框架为合规驱动和审计应用提供了坚实基础。

Abstract: Regulatory compliance question answering (QA) requires precise, verifiable
information, and domain-specific expertise, posing challenges for Large
Language Models (LLMs). In this work, we present a novel multi-agent framework
that integrates a Knowledge Graph (KG) of Regulatory triplets with
Retrieval-Augmented Generation (RAG) to address these demands. First, agents
build and maintain an ontology-free KG by extracting subject--predicate--object
(SPO) triplets from regulatory documents and systematically cleaning,
normalizing, deduplicating, and updating them. Second, these triplets are
embedded and stored along with their corresponding textual sections and
metadata in a single enriched vector database, allowing for both graph-based
reasoning and efficient information retrieval. Third, an orchestrated agent
pipeline leverages triplet-level retrieval for question answering, ensuring
high semantic alignment between user queries and the factual
"who-did-what-to-whom" core captured by the graph. Our hybrid system
outperforms conventional methods in complex regulatory queries, ensuring
factual correctness with embedded triplets, enabling traceability through a
unified vector database, and enhancing understanding through subgraph
visualization, providing a robust foundation for compliance-driven and broader
audit-focused applications.

</details>


### [13] [Mathematical Computation and Reasoning Errors by Large Language Models](https://arxiv.org/abs/2508.09932)
*Liang Zhang,Edith Aurora Graf*

Main category: cs.AI

TL;DR: 研究评估了四种大语言模型（LLMs）在解决数学问题时的准确性，发现推理增强的OpenAI o1模型表现最佳，双代理配置显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 评估LLMs在数学教育中的准确性，为AI驱动的教学和评估提供可靠反馈。

Method: 构建挑战性数学任务，分析LLMs的答案准确性和步骤错误，测试单代理和双代理配置。

Result: OpenAI o1模型表现最优，双代理配置显著提升性能，程序性错误最常见。

Conclusion: 研究为提升LLMs在数学教育中的性能提供了有效策略，推动了AI教学实践的发展。

Abstract: Large Language Models (LLMs) are increasingly utilized in AI-driven
educational instruction and assessment, particularly within mathematics
education. The capability of LLMs to generate accurate answers and detailed
solutions for math problem-solving tasks is foundational for ensuring reliable
and precise feedback and assessment in math education practices. Our study
focuses on evaluating the accuracy of four LLMs (OpenAI GPT-4o and o1,
DeepSeek-V3 and DeepSeek-R1) solving three categories of math tasks, including
arithmetic, algebra, and number theory, and identifies step-level reasoning
errors within their solutions. Instead of relying on standard benchmarks, we
intentionally build math tasks (via item models) that are challenging for LLMs
and prone to errors. The accuracy of final answers and the presence of errors
in individual solution steps were systematically analyzed and coded. Both
single-agent and dual-agent configurations were tested. It is observed that the
reasoning-enhanced OpenAI o1 model consistently achieved higher or nearly
perfect accuracy across all three math task categories. Analysis of errors
revealed that procedural slips were the most frequent and significantly
impacted overall performance, while conceptual misunderstandings were less
frequent. Deploying dual-agent configurations substantially improved overall
performance. These findings offer actionable insights into enhancing LLM
performance and underscore effective strategies for integrating LLMs into
mathematics education, thereby advancing AI-driven instructional practices and
assessment precision.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [14] [ParallelSearch: Train your LLMs to Decompose Query and Search Sub-queries in Parallel with Reinforcement Learning](https://arxiv.org/abs/2508.09303)
*Shu Zhao,Tan Yu,Anbang Xu,Japinder Singh,Aaditya Shukla,Rama Akkiraju*

Main category: cs.CL

TL;DR: ParallelSearch是一种新型强化学习框架，通过并行处理查询提升搜索代理的效率，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有搜索代理在处理查询时采用严格的顺序方式，限制了计算效率，尤其是需要多实体比较的查询。

Method: 提出ParallelSearch框架，利用强化学习训练LLM识别并行查询结构并执行并发搜索操作，引入奖励函数激励独立查询组件的识别。

Result: 在七个问答基准测试中平均性能提升2.9%，在可并行问题上性能提升12.7%，同时仅需69.6%的LLM调用。

Conclusion: ParallelSearch通过并行化显著提升了搜索代理的效率和性能，解决了现有方法的瓶颈。

Abstract: Reasoning-augmented search agents such as Search-R1, trained via
reinforcement learning with verifiable rewards (RLVR), demonstrate remarkable
capabilities in multi-step information retrieval from external knowledge
sources. These agents address the limitations of their parametric memory by
dynamically gathering relevant facts to address complex reasoning tasks.
However, existing approaches suffer from a fundamental architectural
limitation: they process search queries strictly sequentially, even when
handling inherently parallelizable and logically independent comparisons. This
sequential bottleneck significantly constrains computational efficiency,
particularly for queries that require multiple entity comparisons. To address
this critical limitation, we propose ParallelSearch, a novel reinforcement
learning framework that empowers large language models (LLMs) to recognize
parallelizable query structures and execute multiple search operations
concurrently. Our approach introduces dedicated reward functions that
incentivize the identification of independent query components while preserving
answer accuracy through jointly considering correctness, query decomposition
quality, and parallel execution benefits. Comprehensive experiments demonstrate
that ParallelSearch outperforms state-of-the-art baselines by an average
performance gain of 2.9% across seven question-answering benchmarks. Notably,
on parallelizable questions, our method achieves a 12.7% performance
improvement while requiring only 69.6% of the LLM calls compared to sequential
approaches.

</details>


### [15] [Leveraging Large Language Models for Rare Disease Named Entity Recognition](https://arxiv.org/abs/2508.09323)
*Nan Miles Xi,Yu Deng,Lin Wang*

Main category: cs.CL

TL;DR: GPT-4o在罕见病领域的命名实体识别（NER）中表现出色，通过多种提示策略和任务级微调，性能优于传统模型。


<details>
  <summary>Details</summary>
Motivation: 解决罕见病领域NER面临的标注数据稀缺、语义模糊和长尾分布等挑战。

Method: 采用零样本提示、少样本上下文学习、检索增强生成（RAG）和任务级微调，并设计了结构化提示框架和语义引导的样本选择方法。

Result: 在RareDis Corpus上，GPT-4o性能优于BioClinicalBERT，任务级微调达到新SOTA；少样本提示在低成本下表现优异。

Conclusion: 优化提示的大语言模型可作为罕见病NER的有效替代方案，尤其在标注数据稀缺的场景。

Abstract: Named Entity Recognition (NER) in the rare disease domain poses unique
challenges due to limited labeled data, semantic ambiguity between entity
types, and long-tail distributions. In this study, we evaluate the capabilities
of GPT-4o for rare disease NER under low-resource settings, using a range of
prompt-based strategies including zero-shot prompting, few-shot in-context
learning, retrieval-augmented generation (RAG), and task-level fine-tuning. We
design a structured prompting framework that encodes domain-specific knowledge
and disambiguation rules for four entity types. We further introduce two
semantically guided few-shot example selection methods to improve in-context
performance while reducing labeling effort. Experiments on the RareDis Corpus
show that GPT-4o achieves competitive or superior performance compared to
BioClinicalBERT, with task-level fine-tuning yielding new state-of-the-art
(SOTA) results. Cost-performance analysis reveals that few-shot prompting
delivers high returns at low token budgets, while RAG offers marginal
additional benefit. An error taxonomy highlights common failure modes such as
boundary drift and type confusion, suggesting opportunities for post-processing
and hybrid refinement. Our results demonstrate that prompt-optimized LLMs can
serve as effective, scalable alternatives to traditional supervised models in
biomedical NER, particularly in rare disease applications where annotated data
is scarce.

</details>


### [16] [TEN: Table Explicitization, Neurosymbolically](https://arxiv.org/abs/2508.09324)
*Nikita Mehrotra,Aayush Kumar,Sumit Gulwani,Arjun Radhakrishna,Ashish Tiwari*

Main category: cs.CL

TL;DR: TEN是一种神经符号方法，用于从半结构化文本中提取表格数据，通过结合大型语言模型和符号检查器，显著提高了准确性和减少了幻觉问题。


<details>
  <summary>Details</summary>
Motivation: 解决半结构化文本中表格数据提取的挑战，特别是缺乏一致分隔符的情况，纯神经方法因幻觉和无法强制约束而表现不佳。

Method: 采用结构分解提示法生成初始表格，符号检查器评估表格质量并检测问题，再通过批判性LLM生成修复指导，形成自调试循环。

Result: TEN在多个数据集和指标上显著优于纯神经基线，准确率更高，幻觉率更低，用户研究也证实其表格更准确且更易验证。

Conclusion: TEN通过神经符号结合的方法，有效解决了表格提取中的关键问题，具有实际应用价值。

Abstract: We present a neurosymbolic approach, TEN, for extracting tabular data from
semistructured input text. This task is particularly challenging for text input
that does not use special delimiters consistently to separate columns and rows.
Purely neural approaches perform poorly due to hallucinations and their
inability to enforce hard constraints. TEN uses Structural Decomposition
prompting - a specialized chain-of-thought prompting approach - on a large
language model (LLM) to generate an initial table, and thereafter uses a
symbolic checker to evaluate not only the well-formedness of that table, but
also detect cases of hallucinations or forgetting. The output of the symbolic
checker is processed by a critique-LLM to generate guidance for fixing the
table, which is presented to the original LLM in a self-debug loop. Our
extensive experiments demonstrate that TEN significantly outperforms purely
neural baselines across multiple datasets and metrics, achieving significantly
higher exact match accuracy and substantially reduced hallucination rates. A
21-participant user study further confirms that TEN's tables are rated
significantly more accurate (mean score: 5.0 vs 4.3; p = 0.021), and are
consistently preferred for ease of verification and correction, with
participants favoring our method in over 60% of the cases.

</details>


### [17] [Decoding Neural Emotion Patterns through Natural Language Processing Embeddings](https://arxiv.org/abs/2508.09337)
*Gideon Vos,Maryam Ebrahimpour,Liza van Eijk,Zoltan Sarnyai,Mostafa Rahimi Azghadi*

Main category: cs.CL

TL;DR: 该论文提出了一种计算框架，将文本情感内容映射到大脑区域，无需神经影像数据，并通过实验验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 传统神经影像成本高且受限，而数字文本为情感-大脑映射提供了新途径。现有研究缺乏将神经影像与文本分析的整合。

Method: 使用OpenAI的文本嵌入生成语义表示，通过降维和聚类识别情感组，并映射到18个大脑区域。实验包括健康与抑郁受试者对话分析、GoEmotions数据集应用及人类文本与LLM生成文本的比较。

Result: 结果显示神经解剖学上合理的映射，抑郁受试者表现出更强的边缘系统激活。LLM文本在基本情感分布上与人类相似，但缺乏共情和自我参照区域的激活。

Conclusion: 该方法成本低、可扩展，适用于自然语言的大规模分析，并能区分临床群体，为AI情感表达提供了基于大脑的基准。

Abstract: Understanding how emotional expression in language relates to brain function
is a challenge in computational neuroscience and affective computing.
Traditional neuroimaging is costly and lab-bound, but abundant digital text
offers new avenues for emotion-brain mapping. Prior work has largely examined
neuroimaging-based emotion localization or computational text analysis
separately, with little integration. We propose a computational framework that
maps textual emotional content to anatomically defined brain regions without
requiring neuroimaging. Using OpenAI's text-embedding-ada-002, we generate
high-dimensional semantic representations, apply dimensionality reduction and
clustering to identify emotional groups, and map them to 18 brain regions
linked to emotional processing. Three experiments were conducted: i) analyzing
conversational data from healthy vs. depressed subjects (DIAC-WOZ dataset) to
compare mapping patterns, ii) applying the method to the GoEmotions dataset and
iii) comparing human-written text with large language model (LLM) responses to
assess differences in inferred brain activation. Emotional intensity was scored
via lexical analysis. Results showed neuroanatomically plausible mappings with
high spatial specificity. Depressed subjects exhibited greater limbic
engagement tied to negative affect. Discrete emotions were successfully
differentiated. LLM-generated text matched humans in basic emotion distribution
but lacked nuanced activation in empathy and self-referential regions (medial
prefrontal and posterior cingulate cortex). This cost-effective, scalable
approach enables large-scale analysis of naturalistic language, distinguishes
between clinical populations, and offers a brain-based benchmark for evaluating
AI emotional expression.

</details>


### [18] [The Human-AI Hybrid Delphi Model: A Structured Framework for Context-Rich, Expert Consensus in Complex Domains](https://arxiv.org/abs/2508.09349)
*Cathy Speed,Ahmed A. Metwally*

Main category: cs.CL

TL;DR: 研究提出了一种人机混合德尔菲（HAH-Delphi）框架，结合生成AI与专家共识，解决了传统方法的局限性，并在多个领域验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 传统专家共识方法（如德尔菲研究）存在高负担、简化解释和忽略条件性细节等问题，信息过载和证据碎片化加剧了这些挑战。

Method: HAH-Delphi框架整合生成AI（Gemini 2.5 Pro）、资深专家小组和结构化引导，分三阶段测试：回顾性复制、前瞻性比较和应用部署。

Result: AI在回顾性复制中重现95%的共识结论，前瞻性比较中与专家方向性一致率达95%；小型专家小组在应用阶段达成>90%共识覆盖。

Conclusion: HAH-Delphi是一种灵活、可扩展的方法，能生成高质量、情境敏感的共识，适用于健康、训练和绩效科学等领域。

Abstract: Expert consensus plays a critical role in domains where evidence is complex,
conflicting, or insufficient for direct prescription. Traditional methods, such
as Delphi studies, consensus conferences, and systematic guideline synthesis,
offer structure but face limitations including high panel burden, interpretive
oversimplification, and suppression of conditional nuance. These challenges are
now exacerbated by information overload, fragmentation of the evidence base,
and increasing reliance on publicly available sources that lack expert
filtering. This study introduces and evaluates a Human-AI Hybrid Delphi
(HAH-Delphi) framework designed to augment expert consensus development by
integrating a generative AI model (Gemini 2.5 Pro), small panels of senior
human experts, and structured facilitation. The HAH-Delphi was tested in three
phases: retrospective replication, prospective comparison, and applied
deployment in two applied domains (endurance training and resistance and mixed
cardio/strength training). The AI replicated 95% of published expert consensus
conclusions in Phase I and showed 95% directional agreement with senior human
experts in Phase II, though it lacked experiential and pragmatic nuance. In
Phase III, compact panels of six senior experts achieved >90% consensus
coverage and reached thematic saturation before the final participant. The AI
provided consistent, literature-grounded scaffolding that supported divergence
resolution and accelerated saturation. The HAH-Delphi framework offers a
flexible, scalable approach for generating high-quality, context-sensitive
consensus. Its successful application across health, coaching, and performance
science confirms its methodological robustness and supports its use as a
foundation for generating conditional, personalised guidance and published
consensus frameworks at scale.

</details>


### [19] [Flow-SLM: Joint Learning of Linguistic and Acoustic Information for Spoken Language Modeling](https://arxiv.org/abs/2508.09350)
*Ju-Chieh Chou,Jiawei Zhou,Karen Livescu*

Main category: cs.CL

TL;DR: 本文提出了一种联合建模语言和声学信息的文本无关语音生成模型，通过预测语义标记和连续声学帧表示，提升了生成语音的声学细节。


<details>
  <summary>Details</summary>
Motivation: 现有文本无关语音模型仅预测语义标记，依赖外部声码器，缺乏声学上下文和控制能力。

Method: 采用流匹配目标预测连续声学向量，并结合多步语义标记预测以保留语言信息。

Result: 模型在语言似然基准上表现相当，同时在提示生成中提供更好的声学细节。

Conclusion: 联合建模语言和声学信息的方法有效提升了语音生成的声学质量。

Abstract: Textless spoken language models (SLMs) are generative models of speech that
do not rely on text supervision. Most textless SLMs learn to predict the next
semantic token, a discrete representation of linguistic content, and rely on a
separate vocoder to add acoustic information to the generated speech. Such
models have no access to acoustic context and no built-in control over acoustic
details. In this work, we propose to jointly model linguistic and acoustic
information by generating semantic tokens and a continuous real-valued
representation of the acoustic frame. We use a flow-matching objective to
predict the continuous vector conditioned on the semantic tokens. We study the
design space of this approach and find that predicting multiple future semantic
tokens helps preserve linguistic information. Our approach achieves comparable
performance to existing models in terms of linguistic likelihood benchmarks,
while providing better acoustic detail in prompted generation.

</details>


### [20] [APIO: Automatic Prompt Induction and Optimization for Grammatical Error Correction and Text Simplification](https://arxiv.org/abs/2508.09378)
*Artem Chernodub,Aman Saini,Yejin Huh,Vivek Kulkarni,Vipul Raheja*

Main category: cs.CL

TL;DR: APIO是一种无需依赖手动指定种子提示的自动提示诱导和优化方法，在语法错误纠正和文本简化任务中取得了最新性能。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）的进步使得通过简单的提示交互完成多种NLP任务成为可能，但如何优化提示以提高性能仍需研究。

Method: 提出APIO方法，自动诱导和优化提示，不依赖手动指定的种子提示。

Result: APIO在语法错误纠正和文本简化任务中达到了纯LLM提示方法的最新性能。

Conclusion: APIO是一种简单有效的自动提示优化方法，适用于特定NLP任务，并公开了相关资源。

Abstract: Recent advancements in large language models (LLMs) have enabled a wide range
of natural language processing (NLP) tasks to be performed through simple
prompt-based interactions. Consequently, several approaches have been proposed
to engineer prompts that most effectively enable LLMs to perform a given task
(e.g., chain-of-thought prompting). In settings with a well-defined metric to
optimize model performance, automatic prompt optimization (APO) methods have
been developed to refine a seed prompt. Advancing this line of research, we
propose APIO, a simple but effective prompt induction and optimization approach
for the tasks of Grammatical Error Correction (GEC) and Text Simplification,
without relying on manually specified seed prompts. APIO achieves a new
state-of-the-art performance for purely LLM-based prompting methods on these
tasks. We make our data, code, prompts, and outputs publicly available.

</details>


### [21] [Columbo: Expanding Abbreviated Column Names for Tabular Data Using Large Language Models](https://arxiv.org/abs/2508.09403)
*Ting Cai,Stephen Sheen,AnHai Doan*

Main category: cs.CL

TL;DR: 论文提出Columbo，一种基于LLM的解决方案，用于扩展表格中的缩写列名，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 解决表格中缩写列名扩展问题，因现有方法在合成数据和准确性评估上存在不足。

Method: 引入4个新数据集，提出新的同义词感知准确性评估方法，开发基于LLM的Columbo系统。

Result: Columbo在5个数据集上比当前最先进方法NameGuess提升4-29%。

Conclusion: Columbo在实际应用中表现优异，已用于环境科学数据门户EDI。

Abstract: Expanding the abbreviated column names of tables, such as ``esal'' to
``employee salary'', is critical for numerous downstream data tasks. This
problem arises in enterprises, domain sciences, government agencies, and more.
In this paper we make three contributions that significantly advances the state
of the art. First, we show that synthetic public data used by prior work has
major limitations, and we introduce 4 new datasets in enterprise/science
domains, with real-world abbreviations. Second, we show that accuracy measures
used by prior work seriously undercount correct expansions, and we propose new
synonym-aware measures that capture accuracy much more accurately. Finally, we
develop Columbo, a powerful LLM-based solution that exploits context, rules,
chain-of-thought reasoning, and token-level analysis. Extensive experiments
show that Columbo significantly outperforms NameGuess, the current most
advanced solution, by 4-29\%, over 5 datasets. Columbo has been used in
production on EDI, a major data portal for environmental sciences.

</details>


### [22] [Leveraging Zipformer Model for Effective Language Identification in Code-Switched Child-Directed Speech](https://arxiv.org/abs/2508.09430)
*Lavanya Shankar,Leibny Paola Garcia Perera*

Main category: cs.CL

TL;DR: 论文提出使用Zipformer处理双语（普通话和英语）不平衡的儿童导向语音数据，通过内部层编码语言特征，显著提升语言识别的平衡准确率。


<details>
  <summary>Details</summary>
Motivation: 解决双语环境中儿童导向语音的代码切换和语言识别挑战。

Method: 利用Zipformer的内部层提取语言特征嵌入，并与不同后端进行比较。

Result: Zipformer在语言识别中表现稳健，平衡准确率达81.89%，比基线提升15.47%。

Conclusion: Zipformer的编码器架构在现实场景中具有潜力。

Abstract: Code-switching and language identification in child-directed scenarios
present significant challenges, particularly in bilingual environments. This
paper addresses this challenge by using Zipformer to handle the nuances of
speech, which contains two imbalanced languages, Mandarin and English, in an
utterance. This work demonstrates that the internal layers of the Zipformer
effectively encode the language characteristics, which can be leveraged in
language identification. We present the selection methodology of the inner
layers to extract the embeddings and make a comparison with different
back-ends. Our analysis shows that Zipformer is robust across these backends.
Our approach effectively handles imbalanced data, achieving a Balanced Accuracy
(BAC) of 81.89%, a 15.47% improvement over the language identification
baseline. These findings highlight the potential of the transformer encoder
architecture model in real scenarios.

</details>


### [23] [From Charts to Fair Narratives: Uncovering and Mitigating Geo-Economic Biases in Chart-to-Text](https://arxiv.org/abs/2508.09450)
*Ridwan Mahbub,Mohammed Saidul Islam,Mir Tafseer Nayeem,Md Tahmid Rahman Laskar,Mizanur Rahman,Shafiq Joty,Enamul Hoque*

Main category: cs.CL

TL;DR: 该论文研究了视觉语言模型（VLMs）在生成图表摘要时可能放大的地理经济偏见，发现高收入国家的描述更积极，并探讨了部分去偏见方法的局限性。


<details>
  <summary>Details</summary>
Motivation: 图表摘要任务自动化过程中，VLMs可能放大地理经济偏见，导致社会危害，因此需要研究其偏见表现。

Method: 通过评估6,000个图表-国家对，分析6种VLMs生成的摘要中地理经济偏见的表现，并尝试提示去偏见方法。

Result: 发现VLMs对高收入国家的描述更积极，且不同模型偏见程度不一，提示去偏见方法效果有限。

Conclusion: 地理经济偏见在VLMs中普遍存在，需开发更鲁棒的去偏见策略。

Abstract: Charts are very common for exploring data and communicating insights, but
extracting key takeaways from charts and articulating them in natural language
can be challenging. The chart-to-text task aims to automate this process by
generating textual summaries of charts. While with the rapid advancement of
large Vision-Language Models (VLMs), we have witnessed great progress in this
domain, little to no attention has been given to potential biases in their
outputs. This paper investigates how VLMs can amplify geo-economic biases when
generating chart summaries, potentially causing societal harm. Specifically, we
conduct a large-scale evaluation of geo-economic biases in VLM-generated chart
summaries across 6,000 chart-country pairs from six widely used proprietary and
open-source models to understand how a country's economic status influences the
sentiment of generated summaries. Our analysis reveals that existing VLMs tend
to produce more positive descriptions for high-income countries compared to
middle- or low-income countries, even when country attribution is the only
variable changed. We also find that models such as GPT-4o-mini,
Gemini-1.5-Flash, and Phi-3.5 exhibit varying degrees of bias. We further
explore inference-time prompt-based debiasing techniques using positive
distractors but find them only partially effective, underscoring the complexity
of the issue and the need for more robust debiasing strategies. Our code and
dataset are publicly available here.

</details>


### [24] [User-centric Subjective Leaderboard by Customizable Reward Modeling](https://arxiv.org/abs/2508.09463)
*Qi Jia,Xiujie Song,Zicheng Zhang,Yijin Guo,Kaiwei Zhang,Zijian Chen,Guangtao Zhai*

Main category: cs.CL

TL;DR: 论文提出了一种用户中心的主观排行榜（USL），通过动态排名帮助用户选择适合的大语言模型（LLM），并引入了可定制奖励模型（CRM）以解决人类偏好的多样性和矛盾问题。


<details>
  <summary>Details</summary>
Motivation: 现有的大语言模型基准主要关注可验证任务，难以满足用户个性化需求，因此需要一种更贴近实际使用场景的评估方法。

Method: 基于10K+主观查询数据，开发了可定制奖励模型（CRM），并通过其驱动用户中心的主观排行榜（USL）。

Result: CRM仅需4B参数，性能超越GPT-4.1和Gemini-2.5-pro，并在新主题和标准上表现出色。USL能有效处理矛盾偏好。

Conclusion: USL和CRM为LLM选择提供了更实用的动态评估工具，解决了现有基准的局限性。

Abstract: Existing benchmarks for large language models (LLMs) predominantely focus on
assessing their capabilities through verifiable tasks. Such objective and
static benchmarks offer limited utility for practical LLM selection, making it
difficult for users to find suitable models for their individual needs. To
bridge this gap, we present the first User-Centric Subjective Leaderboard
(USL), which provides a preference-driven, dynamic ranking of LLMs across
diverse real-world scenarios. Our work is built upon a thorough investigation
of real human preference data, involving more than 10K subjective queries. Our
investigation reveals significant diversity and contradictions in human
preferences, which limit the effectiveness of state-of-the-art reward models.
To address this, we introduce Customizable Reward Models (CRMs). With only 4B
parameters, our CRM surpasses the performance of leading models such as GPT-4.1
and Gemini-2.5-pro, showing exceptional generalization capabilities across new
topics and criteria. The USL, powered by CRMs, exhibits strong negative
correlations to contradictory preferences.

</details>


### [25] [Learning Facts at Scale with Active Reading](https://arxiv.org/abs/2508.09494)
*Jessy Lin,Vincent-Pierre Berges,Xilun Chen,Wen-Tau Yih,Gargi Ghosh,Barlas Oğuz*

Main category: cs.CL

TL;DR: 论文提出了一种名为“主动阅读”的框架，通过让模型自我生成学习策略来更可靠地吸收知识，显著提升了模型在特定领域的知识掌握能力。


<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型在学习和记忆知识时表现不稳定，缺乏可靠的工具确保模型能一致地学习特定知识。

Method: 提出“主动阅读”框架，训练模型通过自我生成的学习策略研究给定材料，并在专家领域和预训练规模上验证其效果。

Result: 实验显示，主动阅读显著提升了模型在SimpleQA和FinanceBench上的表现，并成功训练出在事实问答上优于更大规模模型的Meta WikiExpert-8B。

Conclusion: 主动阅读是一种有效提升模型知识吸收能力的方法，适用于特定领域和预训练规模的模型优化。

Abstract: LLMs are known to store vast amounts of knowledge in their parametric memory.
However, learning and recalling facts from this memory is known to be
unreliable, depending largely on the prevalence of particular facts in the
training data and other factors which are poorly understood. Practitioners are
lacking tools which will allow them to ensure that the models learn a given
body of knowledge reliably and consistently. To this end, we propose Active
Reading: a framework where we train models to study a given set of material
with self-generated learning strategies. First, we demonstrate models trained
with Active Reading on expert domains absorb significantly more knowledge than
vanilla finetuning and other data augmentations. We train expert 8B models that
achieve 66% on a Wikipedia-grounded subset of SimpleQA (+313% relative over
vanilla finetuning) and 26% on FinanceBench (+160% relative over vanilla
finetuning) by applying Active Reading to the source documents for each
benchmark. Finally, we show that Active Reading can be utilized at pre-training
scale to build more factual models. As a demonstration of this, we release Meta
WikiExpert-8B, a Wikipedia-expert model trained on 1 trillion generated tokens,
which outcompetes models with hundreds of billions of parameters on factual QA.

</details>


### [26] [From Ranking to Selection: A Simple but Efficient Dynamic Passage Selector for Retrieval Augmented Generation](https://arxiv.org/abs/2508.09497)
*Siyuan Meng,Junming Liu,Yirong Chen,Song Mao,Pinlong Cai,Guohang Yan,Botian Shi,Ding Wang*

Main category: cs.CL

TL;DR: 论文提出了一种动态段落选择器（DPS），用于解决检索增强生成（RAG）系统中传统重排序模块在多跳查询中的瓶颈问题。DPS通过监督学习动态选择相关段落，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 传统RAG系统的重排序模块在多跳查询中表现不佳，固定Top-K选择方法无法平衡信息遗漏和噪声引入的问题。

Method: 提出DPS框架，将其作为监督学习问题，捕捉段落间依赖关系并动态选择最相关段落。

Result: 在五个基准测试中，DPS表现优于现有方法，尤其在MuSiQue数据集上F1分数提升显著。

Conclusion: DPS通过自适应证据选择，显著提升了复杂RAG场景中的推理能力。

Abstract: Retrieval-augmented generation (RAG) systems are often bottlenecked by their
reranking modules, which typically score passages independently and select a
fixed Top-K size. This approach struggles with complex multi-hop queries that
require synthesizing evidence across multiple documents, creating a trade-off
where small K values omit crucial information and large K values introduce
noise. To address this, we introduce the Dynamic Passage Selector (DPS), a
novel reranking framework that treats passage selection as a supervised
learning problem. Unlike traditional point-wise or list-wise methods, DPS is
fine-tuned to capture inter-passage dependencies and dynamically select the
most relevant set of passages for generation. As a seamless plug-and-play
module, DPS requires no modifications to the standard RAG pipeline.
Comprehensive evaluations on five benchmarks show that DPS consistently
outperforms state-of-the-art rerankers and fine-tuning methods. Notably, on the
challenging MuSiQue dataset, DPS improves the F1-score by 30.06% and 15.4% over
strong baselines like Qwen3-reranker and RankingGPT, respectively. Our results
demonstrate that by enabling adaptive evidence selection, DPS substantially
enhances reasoning capabilities in complex RAG scenarios.

</details>


### [27] [LACA: Improving Cross-lingual Aspect-Based Sentiment Analysis with LLM Data Augmentation](https://arxiv.org/abs/2508.09515)
*Jakub Šmíd,Pavel Přibáň,Pavel Král*

Main category: cs.CL

TL;DR: 提出一种无需翻译工具的跨语言ABSA方法，利用LLM生成高质量伪标签数据，显著优于现有翻译方法。


<details>
  <summary>Details</summary>
Motivation: 现有跨语言ABSA方法依赖翻译工具，但其可靠性不足，需更高效的方法。

Method: 先训练ABSA模型预测目标语言数据，再用LLM生成更自然的伪标签句子，最后微调模型。

Result: 在六种语言和五种骨干模型上验证，性能超越现有翻译方法，LLM微调后表现更优。

Conclusion: 该方法有效避免了翻译工具的局限性，为跨语言ABSA提供了新思路。

Abstract: Cross-lingual aspect-based sentiment analysis (ABSA) involves detailed
sentiment analysis in a target language by transferring knowledge from a source
language with available annotated data. Most existing methods depend heavily on
often unreliable translation tools to bridge the language gap. In this paper,
we propose a new approach that leverages a large language model (LLM) to
generate high-quality pseudo-labelled data in the target language without the
need for translation tools. First, the framework trains an ABSA model to obtain
predictions for unlabelled target language data. Next, LLM is prompted to
generate natural sentences that better represent these noisy predictions than
the original text. The ABSA model is then further fine-tuned on the resulting
pseudo-labelled dataset. We demonstrate the effectiveness of this method across
six languages and five backbone models, surpassing previous state-of-the-art
translation-based approaches. The proposed framework also supports generative
models, and we show that fine-tuned LLMs outperform smaller multilingual
models.

</details>


### [28] [Cross-lingual Aspect-Based Sentiment Analysis: A Survey on Tasks, Approaches, and Challenges](https://arxiv.org/abs/2508.09516)
*Jakub Šmíd,Pavel Král*

Main category: cs.CL

TL;DR: 本文综述了跨语言方面级情感分析（ABSA）的研究现状，填补了该领域缺乏系统性总结的空白。


<details>
  <summary>Details</summary>
Motivation: 跨语言ABSA旨在将资源丰富语言（如英语）的知识迁移到低资源语言，但目前研究较少，缺乏系统性总结。

Method: 总结了ABSA的核心任务（如方面词提取、情感分类等）、数据集、建模范式及跨语言迁移方法，并探讨了单语、多语ABSA及大语言模型对跨语言ABSA的贡献。

Result: 系统梳理了跨语言ABSA的研究进展，包括任务、方法及现有成果。

Conclusion: 指出了跨语言ABSA的主要挑战，并提出了未来研究方向以推动该领域发展。

Abstract: Aspect-based sentiment analysis (ABSA) is a fine-grained sentiment analysis
task that focuses on understanding opinions at the aspect level, including
sentiment towards specific aspect terms, categories, and opinions. While ABSA
research has seen significant progress, much of the focus has been on
monolingual settings. Cross-lingual ABSA, which aims to transfer knowledge from
resource-rich languages (such as English) to low-resource languages, remains an
under-explored area, with no systematic review of the field. This paper aims to
fill that gap by providing a comprehensive survey of cross-lingual ABSA. We
summarize key ABSA tasks, including aspect term extraction, aspect sentiment
classification, and compound tasks involving multiple sentiment elements.
Additionally, we review the datasets, modelling paradigms, and cross-lingual
transfer methods used to solve these tasks. We also examine how existing work
in monolingual and multilingual ABSA, as well as ABSA with LLMs, contributes to
the development of cross-lingual ABSA. Finally, we highlight the main
challenges and suggest directions for future research to advance cross-lingual
ABSA systems.

</details>


### [29] [UWBa at SemEval-2025 Task 7: Multilingual and Crosslingual Fact-Checked Claim Retrieval](https://arxiv.org/abs/2508.09517)
*Ladislav Lenc,Daniel Cífka,Jiří Martínek,Jakub Šmíd,Pavel Král*

Main category: cs.CL

TL;DR: 本文提出了一种零样本事实核查声明检索系统，结合多种先进大语言模型获取文本嵌入，最终在单语和跨语言任务中分别取得第7和第9名。


<details>
  <summary>Details</summary>
Motivation: 解决事实核查声明检索问题，尤其是在多语言环境下的性能提升。

Method: 使用多种大语言模型生成文本嵌入，结合模型以优化结果，通过余弦相似度匹配最相关声明。

Result: 在单语任务中排名第7，跨语言任务中排名第9，NVIDIA NV-Embed-v2模型表现最佳。

Conclusion: 模型组合（如NV-Embed与GPT或Mistral）对某些语言有益，但多语言模型效果不佳，需依赖英语翻译输入。

Abstract: This paper presents a zero-shot system for fact-checked claim retrieval. We
employed several state-of-the-art large language models to obtain text
embeddings. The models were then combined to obtain the best possible result.
Our approach achieved 7th place in monolingual and 9th in cross-lingual
subtasks. We used only English translations as an input to the text embedding
models since multilingual models did not achieve satisfactory results. We
identified the most relevant claims for each post by leveraging the embeddings
and measuring cosine similarity. Overall, the best results were obtained by the
NVIDIA NV-Embed-v2 model. For some languages, we benefited from model
combinations (NV-Embed & GPT or Mistral).

</details>


### [30] [COMPEER: Controllable Empathetic Reinforcement Reasoning for Emotional Support Conversation](https://arxiv.org/abs/2508.09521)
*Yunxiao Wang,Meng Liu,Wenqi Liu,Kaiyu Jiang,Bin Wen,Fan Yang,Tingting Gao,Guorui Zhou,Liqiang Nie*

Main category: cs.CL

TL;DR: 提出了一种可控共情推理方法，结合自然语言推理与心理步骤，通过强化学习和数据集优化提升情感支持能力。


<details>
  <summary>Details</summary>
Motivation: 当前情感支持模型缺乏基于心理学的深度共情推理，需改进以提升情感支持效果。

Method: 结合自然语言推理与结构化心理步骤，构建细粒度数据集，采用强化学习与统一过程-结果奖励模型，引入个性对话改写和冗余感知奖励策略。

Result: 显著提升模型的情感支持能力，推动共情化、类人支持系统的发展。

Conclusion: 该方法有效解决了情感支持模型中的共情推理不足和响应重复性问题。

Abstract: Emotional support conversations are crucial for promoting emotional
well-being, yet current models often lack deep empathetic reasoning grounded in
psychological principles. To address this, we propose controllable empathetic
reasoning, which combines natural language reasoning with structured
psychological steps. We construct a fine-grained dataset annotated with
reasoning correctness and response preferences to enable this capability. To
further enhance training, we employ reinforcement learning with a unified
process-outcome reward model that delivers precise feedback. To mitigate
response repetitiveness from entropy collapse, we introduce personality-based
dialogue rewriting and a redundancy-aware reward reweighting strategy. Our
approach significantly improves model's emotional support ability, advancing
the development of empathetic, human-like support systems.

</details>


### [31] [The Surprising Effectiveness of Membership Inference with Simple N-Gram Coverage](https://arxiv.org/abs/2508.09603)
*Skyler Hallinan,Jaehun Jung,Melanie Sclar,Ximing Lu,Abhilasha Ravichander,Sahana Ramnath,Yejin Choi,Sai Praneeth Karimireddy,Niloofar Mireshghallah,Xiang Ren*

Main category: cs.CL

TL;DR: 论文提出了一种仅依赖模型文本输出的成员推理攻击方法N-Gram Coverage Attack，适用于黑盒模型，并在多个基准测试中表现优于其他黑盒方法，甚至媲美白盒攻击。


<details>
  <summary>Details</summary>
Motivation: 当前多数成员推理攻击需要访问模型的隐藏状态或概率分布，限制了其在仅提供API访问的模型（如GPT-4）中的应用。本文旨在开发一种仅依赖文本输出的攻击方法。

Method: 通过获取模型在候选前缀条件下的多个生成文本，利用n-gram重叠度量计算与真实后缀的相似性，高相似性表明成员资格。

Result: N-Gram Coverage Attack在多个基准测试中表现优异，且攻击成功率随生成序列数量的增加而提升。对OpenAI模型的研究显示，GPT-4o等新模型对成员推理的鲁棒性增强。

Conclusion: 该方法为黑盒模型的成员推理提供了有效工具，同时揭示了模型隐私保护的进步趋势。

Abstract: Membership inference attacks serves as useful tool for fair use of language
models, such as detecting potential copyright infringement and auditing data
leakage. However, many current state-of-the-art attacks require access to
models' hidden states or probability distribution, which prevents investigation
into more widely-used, API-access only models like GPT-4. In this work, we
introduce N-Gram Coverage Attack, a membership inference attack that relies
solely on text outputs from the target model, enabling attacks on completely
black-box models. We leverage the observation that models are more likely to
memorize and subsequently generate text patterns that were commonly observed in
their training data. Specifically, to make a prediction on a candidate member,
N-Gram Coverage Attack first obtains multiple model generations conditioned on
a prefix of the candidate. It then uses n-gram overlap metrics to compute and
aggregate the similarities of these outputs with the ground truth suffix; high
similarities indicate likely membership. We first demonstrate on a diverse set
of existing benchmarks that N-Gram Coverage Attack outperforms other black-box
methods while also impressively achieving comparable or even better performance
to state-of-the-art white-box attacks - despite having access to only text
outputs. Interestingly, we find that the success rate of our method scales with
the attack compute budget - as we increase the number of sequences generated
from the target model conditioned on the prefix, attack performance tends to
improve. Having verified the accuracy of our method, we use it to investigate
previously unstudied closed OpenAI models on multiple domains. We find that
more recent models, such as GPT-4o, exhibit increased robustness to membership
inference, suggesting an evolving trend toward improved privacy protections.

</details>


### [32] [AINL-Eval 2025 Shared Task: Detection of AI-Generated Scientific Abstracts in Russian](https://arxiv.org/abs/2508.09622)
*Tatiana Batura,Elena Bruches,Milana Shvenk,Valentin Malykh*

Main category: cs.CL

TL;DR: AINL-Eval 2025共享任务旨在检测俄语科学摘要中的AI生成内容，提供了一个包含52,305个样本的数据集，并吸引了10个团队参与。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型的快速发展，区分人类和AI生成内容变得困难，这对学术诚信构成挑战，尤其是在科学出版和多语言环境中。

Method: 任务基于一个大规模数据集，包含人类和五种先进LLM生成的科学摘要，要求参与者开发能泛化到新领域和新模型的解决方案。

Result: 任务吸引了10个团队和159份提交，顶级系统在检测AI生成内容方面表现优异。

Conclusion: 通过建立持续共享任务平台，该研究推动了AI生成内容检测的长期进展。

Abstract: The rapid advancement of large language models (LLMs) has revolutionized text
generation, making it increasingly difficult to distinguish between human- and
AI-generated content. This poses a significant challenge to academic integrity,
particularly in scientific publishing and multilingual contexts where detection
resources are often limited. To address this critical gap, we introduce the
AINL-Eval 2025 Shared Task, specifically focused on the detection of
AI-generated scientific abstracts in Russian. We present a novel, large-scale
dataset comprising 52,305 samples, including human-written abstracts across 12
diverse scientific domains and AI-generated counterparts from five
state-of-the-art LLMs (GPT-4-Turbo, Gemma2-27B, Llama3.3-70B, Deepseek-V3, and
GigaChat-Lite). A core objective of the task is to challenge participants to
develop robust solutions capable of generalizing to both (i) previously unseen
scientific domains and (ii) models not included in the training data. The task
was organized in two phases, attracting 10 teams and 159 submissions, with top
systems demonstrating strong performance in identifying AI-generated content.
We also establish a continuous shared task platform to foster ongoing research
and long-term progress in this important area. The dataset and platform are
publicly available at https://github.com/iis-research-team/AINL-Eval-2025.

</details>


### [33] [Improving Diversity in Language Models: When Temperature Fails, Change the Loss](https://arxiv.org/abs/2508.09654)
*Alexandre Verine,Florian Le Bronnec,Kunhao Zheng,Alexandre Allauzen,Yann Chevaleyre,Benjamin Negrevergne*

Main category: cs.CL

TL;DR: 研究探讨了通过调整解码温度提升语言模型多样性的方法，发现降低温度可提高质量（精确率），而提高温度未必能提升覆盖率（召回率）。提出基于精确率-召回率框架的损失函数改进方法，效果优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 语言模型多样性提升是一个重要但具有挑战性的目标，传统方法（如调整解码温度）效果有限，需探索更有效的改进途径。

Method: 通过分析解码温度调整对模型性能的影响，提出基于精确率-召回率框架的损失函数改进方法。

Result: 改进方法在精确率和召回率之间取得了更好的平衡，优于传统负对数似然训练结合温度调整的方法。

Conclusion: 研究为语言模型多样性提升提供了新思路，通过重新设计损失函数可实现更优的性能平衡。

Abstract: Increasing diversity in language models is a challenging yet essential
objective. A common approach is to raise the decoding temperature. In this
work, we investigate this approach through a simplistic yet common case to
provide insights into why decreasing temperature can improve quality
(Precision), while increasing it often fails to boost coverage (Recall). Our
analysis reveals that for a model to be effectively tunable through temperature
adjustments, it must be trained toward coverage. To address this, we propose
rethinking loss functions in language models by leveraging the Precision-Recall
framework. Our results demonstrate that this approach achieves a substantially
better trade-off between Precision and Recall than merely combining negative
log-likelihood training with temperature scaling. These findings offer a
pathway toward more versatile and robust language modeling techniques.

</details>


### [34] [EffiEval: Efficient and Generalizable Model Evaluation via Capability Coverage Maximization](https://arxiv.org/abs/2508.09662)
*Yaoning Wang,Jiahao Ying,Yixin Cao,Yubo Ma,Yugang Jiang*

Main category: cs.CL

TL;DR: EffiEval是一种无需训练的高效基准测试方法，通过自适应选择代表性数据子集解决数据冗余问题，同时保持评估可靠性。


<details>
  <summary>Details</summary>
Motivation: 解决大型语言模型评估中的计算挑战和数据冗余问题。

Method: 基于模型效用指数（MUI）自适应选择高质量代表性数据子集。

Result: 在多个公共基准测试中，EffiEval仅用少量数据即可实现与完整数据集评估一致的排名。

Conclusion: EffiEval为LLMs时代提供了可靠、公平且高效的评估解决方案。

Abstract: The rapid advancement of large language models (LLMs) and the development of
increasingly large and diverse evaluation benchmarks have introduced
substantial computational challenges for model assessment. In this paper, we
present EffiEval, a training-free approach for efficient benchmarking that
effectively addresses data redundancy while maintaining high evaluation
reliability. Our method is specifically designed to meet three key criteria for
high-quality evaluation: representativeness, by ensuring comprehensive coverage
of model capabilities; fairness, by remaining independent of model performance
during sample selection to avoid bias; and generalizability, by enabling
flexible transfer across datasets and model families without reliance on
large-scale evaluation data. Unlike traditional methods that rely on absolute
performance or require extensive evaluation data, our approach adaptively
selects high-quality representative subsets based on the Model Utility Index
(MUI). Extensive experiments on multiple public benchmarks and diverse LLMs
demonstrate that EffiEval achieves strong ranking consistency with full-dataset
evaluation using only a small fraction of the original data. Furthermore, our
method is flexible and scalable in size, allowing users to balance evaluation
efficiency and representativeness according to specific needs. Overall,
EffiEval provides a practical and generalizable solution for reliable, fair,
and efficient evaluation in the era of LLMs.

</details>


### [35] [Slow Tuning and Low-Entropy Masking for Safe Chain-of-Thought Distillation](https://arxiv.org/abs/2508.09666)
*Ziyang Ma,Qingyue Yuan,Linhai Zhang,Deyu Zhou*

Main category: cs.CL

TL;DR: 本文提出了一种名为SLowED的安全蒸馏方法，旨在在链式思维（CoT）蒸馏过程中保持小型语言模型（SLM）的安全性，同时提升其推理能力。该方法包含慢调优和低熵掩码两个模块，实验证明其有效性和优越性。


<details>
  <summary>Details</summary>
Motivation: 现有CoT蒸馏方法虽能提升SLM的推理能力，但忽视了其对模型安全性的负面影响。本文旨在解决这一问题，提出一种无需额外计算或标注数据的安全蒸馏方法。

Method: 提出SLowED方法，包含慢调优（Slow Tuning）和低熵掩码（Low-Entropy Masking）两个模块。慢调优通过限制权重变化幅度优化模型，低熵掩码则排除不必要的学习目标。

Result: 在多个SLM和评测基准上的实验表明，SLowED在保持模型安全性的同时，显著提升了推理能力，且优于现有蒸馏方法。

Conclusion: SLowED是一种高效的安全蒸馏方法，通过慢调优和低熵掩码的结合，成功解决了CoT蒸馏中的安全性问题，同时提升了模型性能。

Abstract: Previous chain-of-thought (CoT) distillation methods primarily focused on
enhancing the reasoning capabilities of Small Language Models (SLMs) by
utilizing high-quality rationales generated by powerful Large Language Models
(LLMs, e.g., GPT-4). However, few works have noted the negative effects on SLM
safety brought by the training, which are revealed in this study. Although
there are works on safety alignment that fine-tune language models or
manipulate model weights to defend against harmful inputs, they require extra
computation or annotated data, and probably impact the reasoning ability of
SLMs. In this paper, we investigate how to maintain the safety of SLMs during
the CoT distillation process. Specifically, we propose a safe distillation
method, Slow Tuning and Low-Entropy Masking Distillation (SLowED), containing
two modules: Slow Tuning and Low-Entropy Masking. Slow Tuning scales down the
magnitude of model weight changes to optimize the model weights in the
neighboring space near the initial weight distribution. Low-Entropy Masking
masks low-entropy tokens, which are regarded as unnecessary learning targets,
to exclude them from fine-tuning. Experiments on three SLMs (Qwen2.5-1.5B,
Llama-3.2-1B, BLOOM-1.1B) across reasoning benchmarks (BBH, BB-Sub, ARC,
AGIEval) and safety evaluation (AdvBench) show that SLowED retains the safety
of SLMs and comparably improves their reasoning capability compared to existing
distillation methods. Furthermore, our ablation study presents the
effectiveness of Slow Tuning and Low-Entropy Masking, with the former
maintaining the model's safety in the early stage and the latter prolonging the
safe training epochs.

</details>


### [36] [Evaluating the Role of Large Language Models in Legal Practice in India](https://arxiv.org/abs/2508.09713)
*Rahul Hemrajani*

Main category: cs.CL

TL;DR: 论文评估了大型语言模型（LLM）在印度法律任务中的表现，发现其在起草和问题识别上表现优异，但在专业法律研究和推理上存在不足，仍需人类专家参与。


<details>
  <summary>Details</summary>
Motivation: 探讨AI在法律领域的应用潜力，特别是LLM在关键法律任务中的表现。

Method: 通过调查实验，比较LLM（如GPT、Claude、Llama）与初级律师在法律任务中的输出，由高级法学院学生评分。

Result: LLM在法律起草和问题识别上表现优异，但在专业研究和推理中常出现错误或虚构内容。

Conclusion: LLM可辅助部分法律任务，但复杂推理和法律精准应用仍需人类专家。

Abstract: The integration of Artificial Intelligence(AI) into the legal profession
raises significant questions about the capacity of Large Language Models(LLM)
to perform key legal tasks. In this paper, I empirically evaluate how well
LLMs, such as GPT, Claude, and Llama, perform key legal tasks in the Indian
context, including issue spotting, legal drafting, advice, research, and
reasoning. Through a survey experiment, I compare outputs from LLMs with those
of a junior lawyer, with advanced law students rating the work on helpfulness,
accuracy, and comprehensiveness. LLMs excel in drafting and issue spotting,
often matching or surpassing human work. However, they struggle with
specialised legal research, frequently generating hallucinations, factually
incorrect or fabricated outputs. I conclude that while LLMs can augment certain
legal tasks, human expertise remains essential for nuanced reasoning and the
precise application of law.

</details>


### [37] [The Perils of Chart Deception: How Misleading Visualizations Affect Vision-Language Models](https://arxiv.org/abs/2508.09716)
*Ridwan Mahbub,Mohammed Saidul Islam,Md Tahmid Rahman Laskar,Mizanur Rahman,Mir Tafseer Nayeem,Enamul Hoque*

Main category: cs.CL

TL;DR: 该研究评估了视觉语言模型（VLMs）对误导性信息可视化的解读能力，发现大多数模型容易被欺骗，导致对图表的错误理解。


<details>
  <summary>Details</summary>
Motivation: 随着VLMs被广泛用于解读可视化数据，尤其是非专业用户，了解这些模型对误导性设计的敏感性变得至关重要。

Method: 研究分析了来自10个不同模型的16,000多个响应，覆盖8种误导性图表设计。

Result: 大多数VLMs被误导性设计欺骗，导致对图表的错误解读，尽管数据本身未变。

Conclusion: 研究强调了在VLMs中建立防护措施以防止视觉误导信息的必要性。

Abstract: Information visualizations are powerful tools that help users quickly
identify patterns, trends, and outliers, facilitating informed decision-making.
However, when visualizations incorporate deceptive design elements-such as
truncated or inverted axes, unjustified 3D effects, or violations of best
practices-they can mislead viewers and distort understanding, spreading
misinformation. While some deceptive tactics are obvious, others subtly
manipulate perception while maintaining a facade of legitimacy. As
Vision-Language Models (VLMs) are increasingly used to interpret
visualizations, especially by non-expert users, it is critical to understand
how susceptible these models are to deceptive visual designs. In this study, we
conduct an in-depth evaluation of VLMs' ability to interpret misleading
visualizations. By analyzing over 16,000 responses from ten different models
across eight distinct types of misleading chart designs, we demonstrate that
most VLMs are deceived by them. This leads to altered interpretations of
charts, despite the underlying data remaining the same. Our findings highlight
the need for robust safeguards in VLMs against visual misinformation.

</details>


### [38] [Sample More to Think Less: Group Filtered Policy Optimization for Concise Reasoning](https://arxiv.org/abs/2508.09726)
*Vaishnavi Shrivastava,Ahmed Awadallah,Vidhisha Balachandran,Shivam Garg,Harkirat Behl,Dimitris Papailiopoulos*

Main category: cs.CL

TL;DR: GFPO（Group Filtered Policy Optimization）通过训练时采样更多组并基于长度和奖励效率过滤响应，有效减少语言模型因追求奖励而过度生成的问题，同时保持准确性。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在强化学习中倾向于通过增加回答长度来提高准确性，导致大量冗余内容。GFPO旨在解决这一问题。

Method: GFPO在训练时采样更多组，并根据响应长度和奖励效率（奖励/令牌比）过滤响应进行训练。还提出自适应难度GFPO，动态分配资源给更难的问题。

Result: 在Phi-4-reasoning模型上，GFPO将长度膨胀减少46-85%，同时保持准确性。自适应难度GFPO进一步优化了计算效率和准确性。

Conclusion: GFPO通过增加训练计算量减少推理计算量，为高效推理提供了一种简单有效的解决方案。

Abstract: Large language models trained with reinforcement learning with verifiable
rewards tend to trade accuracy for length--inflating response lengths to
achieve gains in accuracy. While longer answers may be warranted for harder
problems, many tokens are merely "filler": repetitive, verbose text that makes
no real progress. We introduce GFPO (Group Filtered Policy Optimization), which
curbs this length explosion by sampling larger groups per problem during
training and filtering responses to train on based on two key metrics: (1)
response length and (2) token efficiency: reward per token ratio. By sampling
more at training time, we teach models to think less at inference time. On the
Phi-4-reasoning model, GFPO cuts GRPO's length inflation by 46-71% across
challenging STEM and coding benchmarks (AIME 24/25, GPQA, Omni-MATH,
LiveCodeBench) while maintaining accuracy. Optimizing for reward per token
further increases reductions in length inflation to 71-85%. We also propose
Adaptive Difficulty GFPO, which dynamically allocates more training resources
to harder problems based on real-time difficulty estimates, improving the
balance between computational efficiency and accuracy especially on difficult
questions. GFPO demonstrates that increased training-time compute directly
translates to reduced test-time compute--a simple yet effective trade-off for
efficient reasoning.

</details>


### [39] [Transforming Questions and Documents for Semantically Aligned Retrieval-Augmented Generation](https://arxiv.org/abs/2508.09755)
*Seokgi Lee*

Main category: cs.CL

TL;DR: 提出了一种针对多跳问答的新型检索增强生成（RAG）框架，通过分解问题和生成可回答问题嵌入提升性能。


<details>
  <summary>Details</summary>
Motivation: 解决多跳问答中查询模糊和文档检索效率低的问题。

Method: 1. 使用LLM分解多跳问题为单跳子问题；2. 生成可回答问题嵌入文档块并检索。

Result: 在三个多跳问答数据集上性能优于基线系统。

Conclusion: 可回答问题嵌入和LLM查询分解在多跳RAG中效果显著。

Abstract: We introduce a novel retrieval-augmented generation (RAG) framework tailored
for multihop question answering. First, our system uses large language model
(LLM) to decompose complex multihop questions into a sequence of single-hop
subquestions that guide document retrieval. This decomposition mitigates the
ambiguity inherent in multi-hop queries by clearly targeting distinct knowledge
facets. Second, instead of embedding raw or chunked documents directly, we
generate answerable questions from each document chunk using Qwen3-8B, embed
these generated questions, and retrieve relevant chunks via question-question
embedding similarity. During inference, the retrieved chunks are then fed along
with the original question into the RAG pipeline. We evaluate on three multihop
question datasets (MuSiQue, 2WikiMultiHopQa, HotpotQA) from LongBench. Our
method improves RAG performacne compared to baseline systems. Our contributions
highlight the benefits of using answerable-question embeddings for RAG, and the
effectiveness of LLM-based query decomposition for multihop scenarios.

</details>


### [40] [Echoes of Agreement: Argument Driven Opinion Shifts in Large Language Models](https://arxiv.org/abs/2508.09759)
*Avneet Kaur*

Main category: cs.CL

TL;DR: 研究表明，LLMs在政治话题上的立场对提示词高度敏感，且提示词中的支持或反驳论点会显著改变模型回应，显示出LLMs存在迎合倾向。


<details>
  <summary>Details</summary>
Motivation: 探讨提示词中支持或反驳论点如何影响LLMs的政治偏见评估，以理解模型行为的稳健性及其与观点性文本的互动。

Method: 通过实验评估在支持或反驳论点存在时LLMs的政治偏见，分析单轮和多轮对话中模型回应的变化。

Result: 实验显示，论点会显著改变模型回应的立场方向，且论点强度影响模型回应的方向一致性。

Conclusion: LLMs存在迎合论点倾向，这对政治偏见测量和缓解策略开发具有重要影响。

Abstract: There have been numerous studies evaluating bias of LLMs towards political
topics. However, how positions towards these topics in model outputs are highly
sensitive to the prompt. What happens when the prompt itself is suggestive of
certain arguments towards those positions remains underexplored. This is
crucial for understanding how robust these bias evaluations are and for
understanding model behaviour, as these models frequently interact with
opinionated text. To that end, we conduct experiments for political bias
evaluation in presence of supporting and refuting arguments. Our experiments
show that such arguments substantially alter model responses towards the
direction of the provided argument in both single-turn and multi-turn settings.
Moreover, we find that the strength of these arguments influences the
directional agreement rate of model responses. These effects point to a
sycophantic tendency in LLMs adapting their stance to align with the presented
arguments which has downstream implications for measuring political bias and
developing effective mitigation strategies.

</details>


### [41] [UtterTune: LoRA-Based Target-Language Pronunciation Edit and Control in Multilingual Text-to-Speech](https://arxiv.org/abs/2508.09767)
*Shuhei Kato*

Main category: cs.CL

TL;DR: UtterTune是一种轻量级适应方法，通过低秩适应调整多语言TTS系统，提升目标语言（日语）的发音可控性，同时保持其他语言的性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于LLM的TTS系统在G2P映射和韵律建模方面存在挑战，尤其是在缺少显式G2P模块时。UtterTune旨在解决这一问题。

Method: 采用低秩适应技术，针对日语语音在音素级别控制发音和音高重音，同时保持零样本设置下的自然度和说话人相似性。

Result: 主客观评估证实了其有效性。

Conclusion: UtterTune成功提升了目标语言的发音可控性，同时不影响其他语言的性能。

Abstract: We propose UtterTune, a lightweight adaptation method that fine-tunes a
multilingual text-to-speech (TTS) system based on a large language model (LLM)
architecture, designed to enhance the controllability of pronunciation in a
target language while preserving performance in others. While LLM architectures
have enabled TTS models to achieve remarkable naturalness, accurately modeling
grapheme-to-phoneme (G2P) mapping and prosody remains challenging, especially
when the model omits an explicit G2P module and directly processes minimally
encoded text (e.g., byte-pair encoding). UtterTune leverages low-rank
adaptation to enable the control of segmental pronunciation and pitch accent at
the phoneme level for Japanese speech, the target language in this paper, while
maintaining naturalness and speaker similarity in a zero-shot setting.
Objective and subjective evaluations confirm its effectiveness.

</details>


### [42] [Can LLM-Generated Textual Explanations Enhance Model Classification Performance? An Empirical Study](https://arxiv.org/abs/2508.09776)
*Mahdi Dhaini,Juraj Vladika,Ege Erdogan,Zineb Attaoui,Gjergji Kasneci*

Main category: cs.CL

TL;DR: 该论文提出了一种利用大型语言模型（LLMs）自动生成高质量文本解释的框架，并通过实验验证其效果优于人工标注。


<details>
  <summary>Details</summary>
Motivation: 传统依赖人工标注的文本解释方法成本高且难以扩展，因此需要一种自动化解决方案。

Method: 使用多种最先进的LLMs生成解释，并通过NLG指标评估其质量，同时研究其对预训练语言模型性能的影响。

Result: 实验表明，自动生成的解释在提升模型性能方面与人工标注解释效果相当。

Conclusion: 该研究为可扩展的自动化文本解释生成提供了新方向，有助于增强NLP数据集和模型性能。

Abstract: In the rapidly evolving field of Explainable Natural Language Processing
(NLP), textual explanations, i.e., human-like rationales, are pivotal for
explaining model predictions and enriching datasets with interpretable labels.
Traditional approaches rely on human annotation, which is costly,
labor-intensive, and impedes scalability. In this work, we present an automated
framework that leverages multiple state-of-the-art large language models (LLMs)
to generate high-quality textual explanations. We rigorously assess the quality
of these LLM-generated explanations using a comprehensive suite of Natural
Language Generation (NLG) metrics. Furthermore, we investigate the downstream
impact of these explanations on the performance of pre-trained language models
(PLMs) and LLMs across natural language inference tasks on two diverse
benchmark datasets. Our experiments demonstrate that automated explanations
exhibit highly competitive effectiveness compared to human-annotated
explanations in improving model performance. Our findings underscore a
promising avenue for scalable, automated LLM-based textual explanation
generation for extending NLP datasets and enhancing model performance.

</details>


### [43] [Adoption of Explainable Natural Language Processing: Perspectives from Industry and Academia on Practices and Challenges](https://arxiv.org/abs/2508.09786)
*Mahdi Dhaini,Tobias Müller,Roksoliana Rabets,Gjergji Kasneci*

Main category: cs.CL

TL;DR: 论文探讨了可解释性NLP的实际应用情况，通过访谈行业从业者和学术研究者，揭示了当前方法的不足和挑战。


<details>
  <summary>Details</summary>
Motivation: 复杂模型的不透明性需要解释其决策，但实际应用中可解释性方法的采纳和效果尚未充分研究。

Method: 采用定性访谈研究，分析行业从业者和学术研究者的观点。

Result: 发现概念差距、对现有方法满意度低，并强调评估挑战。

Conclusion: 需要明确的定义和以用户为中心的框架，以促进可解释性NLP的实际应用。

Abstract: The field of explainable natural language processing (NLP) has grown rapidly
in recent years. The growing opacity of complex models calls for transparency
and explanations of their decisions, which is crucial to understand their
reasoning and facilitate deployment, especially in high-stakes environments.
Despite increasing attention given to explainable NLP, practitioners'
perspectives regarding its practical adoption and effectiveness remain
underexplored. This paper addresses this research gap by investigating
practitioners' experiences with explainability methods, specifically focusing
on their motivations for adopting such methods, the techniques employed,
satisfaction levels, and the practical challenges encountered in real-world NLP
applications. Through a qualitative interview-based study with industry
practitioners and complementary interviews with academic researchers, we
systematically analyze and compare their perspectives. Our findings reveal
conceptual gaps, low satisfaction with current explainability methods, and
highlight evaluation challenges. Our findings emphasize the need for clear
definitions and user-centric frameworks for better adoption of explainable NLP
in practice.

</details>


### [44] [BigCharts-R1: Enhanced Chart Reasoning with Visual Reinforcement Finetuning](https://arxiv.org/abs/2508.09804)
*Ahmed Masry,Abhay Puri,Masoud Hashemi,Juan A. Rodriguez,Megh Thakkar,Khyati Mahajan,Vikas Yadav,Sathwik Tejaswi Madhusudhan,Alexandre Piché,Dzmitry Bahdanau,Christopher Pal,David Vazquez,Enamul Hoque,Perouz Taslakian,Sai Rajeswar,Spandana Gella*

Main category: cs.CL

TL;DR: 论文提出BigCharts数据集生成流程和综合训练框架，解决了现有图表理解模型的多样性不足和数据质量问题，并通过实验验证了其优越性。


<details>
  <summary>Details</summary>
Motivation: 现有视觉语言模型在图表理解上表现不佳，主要因训练数据缺乏多样性和真实性，且依赖低质量数据集的监督微调。

Method: 提出BigCharts数据集生成流程，结合真实图表数据和多平台来源，确保多样性和真实性；引入综合训练框架，结合监督微调和GRPO强化学习。

Result: BigCharts-R1模型在多个图表问答基准测试中超越现有方法，包括更大规模的开源和闭源模型。

Conclusion: BigCharts通过高质量数据集和综合训练框架，显著提升了图表理解模型的性能和泛化能力。

Abstract: Charts are essential to data analysis, transforming raw data into clear
visual representations that support human decision-making. Although current
vision-language models (VLMs) have made significant progress, they continue to
struggle with chart comprehension due to training on datasets that lack
diversity and real-world authenticity, or on automatically extracted underlying
data tables of charts, which can contain numerous estimation errors.
Furthermore, existing models only rely on supervised fine-tuning using these
low-quality datasets, severely limiting their effectiveness. To address these
issues, we first propose BigCharts, a dataset creation pipeline that generates
visually diverse chart images by conditioning the rendering process on
real-world charts sourced from multiple online platforms. Unlike purely
synthetic datasets, BigCharts incorporates real-world data, ensuring
authenticity and visual diversity, while still retaining accurate underlying
data due to our proposed replotting process. Additionally, we introduce a
comprehensive training framework that integrates supervised fine-tuning with
Group Relative Policy Optimization (GRPO)-based reinforcement learning. By
introducing novel reward signals specifically designed for chart reasoning, our
approach enhances model robustness and generalization across diverse chart
styles and domains, resulting in a state-of-the-art chart reasoning model,
BigCharts-R1. Extensive experiments demonstrate that our models surpass
existing methods on multiple chart question-answering benchmarks compared to
even larger open-source and closed-source models.

</details>


### [45] [A Comprehensive Survey of Datasets for Clinical Mental Health AI Systems](https://arxiv.org/abs/2508.09809)
*Aishik Mandal,Prottay Kumar Adhikary,Hiba Arnaout,Iryna Gurevych,Tanmoy Chakraborty*

Main category: cs.CL

TL;DR: 论文综述了用于训练AI心理健康助手的临床数据集，分析了现有数据的局限性，并提出了改进建议。


<details>
  <summary>Details</summary>
Motivation: 全球心理健康问题日益严重，但专业临床医生资源不足，AI辅助诊断和干预成为解决方案，但高质量数据集是关键。

Method: 对临床心理健康数据集进行全面调查，按疾病、数据类型、任务、可访问性和文化背景分类，并分析合成数据集。

Result: 发现现有数据集存在纵向数据不足、文化和语言代表性有限、标准不一致等问题。

Conclusion: 提出了改进数据集质量和标准化的建议，以促进更稳健、通用和公平的心理健康AI系统发展。

Abstract: Mental health disorders are rising worldwide. However, the availability of
trained clinicians has not scaled proportionally, leaving many people without
adequate or timely support. To bridge this gap, recent studies have shown the
promise of Artificial Intelligence (AI) to assist mental health diagnosis,
monitoring, and intervention. However, the development of efficient, reliable,
and ethical AI to assist clinicians is heavily dependent on high-quality
clinical training datasets. Despite growing interest in data curation for
training clinical AI assistants, existing datasets largely remain scattered,
under-documented, and often inaccessible, hindering the reproducibility,
comparability, and generalizability of AI models developed for clinical mental
health care. In this paper, we present the first comprehensive survey of
clinical mental health datasets relevant to the training and development of
AI-powered clinical assistants. We categorize these datasets by mental
disorders (e.g., depression, schizophrenia), data modalities (e.g., text,
speech, physiological signals), task types (e.g., diagnosis prediction, symptom
severity estimation, intervention generation), accessibility (public,
restricted or private), and sociocultural context (e.g., language and cultural
background). Along with these, we also investigate synthetic clinical mental
health datasets. Our survey identifies critical gaps such as a lack of
longitudinal data, limited cultural and linguistic representation, inconsistent
collection and annotation standards, and a lack of modalities in synthetic
data. We conclude by outlining key challenges in curating and standardizing
future datasets and provide actionable recommendations to facilitate the
development of more robust, generalizable, and equitable mental health AI
systems.

</details>


### [46] [Speed Always Wins: A Survey on Efficient Architectures for Large Language Models](https://arxiv.org/abs/2508.09834)
*Weigao Sun,Jiaxi Hu,Yucheng Zhou,Jusen Du,Disen Lan,Kexin Wang,Tong Zhu,Xiaoye Qu,Yu Zhang,Xiaoyu Mo,Daizong Liu,Yuxuan Liang,Wenliang Chen,Guoqi Li,Yu Cheng*

Main category: cs.CL

TL;DR: 本文综述了针对传统Transformer架构计算量大、难以大规模训练的问题，提出了一系列高效的LLM架构创新方法，包括线性/稀疏序列建模、高效注意力变体、混合专家模型等，并探讨了其在多模态中的应用。


<details>
  <summary>Details</summary>
Motivation: 传统Transformer架构在计算资源和训练规模上存在限制，亟需更高效的LLM架构以提升性能和实用性。

Method: 系统分析了线性/稀疏序列建模、高效注意力变体、稀疏混合专家模型、混合架构及新兴扩散LLM等技术。

Result: 提出了一种现代高效LLM架构的蓝图，为未来研究提供了方向。

Conclusion: 这些创新方法有望推动更高效、多功能的AI系统发展。

Abstract: Large Language Models (LLMs) have delivered impressive results in language
understanding, generation, reasoning, and pushes the ability boundary of
multimodal models. Transformer models, as the foundation of modern LLMs, offer
a strong baseline with excellent scaling properties. However, the traditional
transformer architecture requires substantial computations and poses
significant obstacles for large-scale training and practical deployment. In
this survey, we offer a systematic examination of innovative LLM architectures
that address the inherent limitations of transformers and boost the efficiency.
Starting from language modeling, this survey covers the background and
technical details of linear and sparse sequence modeling methods, efficient
full attention variants, sparse mixture-of-experts, hybrid model architectures
incorporating the above techniques, and emerging diffusion LLMs. Additionally,
we discuss applications of these techniques to other modalities and consider
their wider implications for developing scalable, resource-aware foundation
models. By grouping recent studies into the above category, this survey
presents a blueprint of modern efficient LLM architectures, and we hope this
could help motivate future research toward more efficient, versatile AI
systems.

</details>


### [47] [PRELUDE: A Benchmark Designed to Require Global Comprehension and Reasoning over Long Contexts](https://arxiv.org/abs/2508.09848)
*Mo Yu,Tsz Ting Chung,Chulun Zhou,Tong Li,Rui Lu,Jiangnan Li,Liyan Xu,Haoshu Lu,Ning Zhang,Jing Li,Jie Zhou*

Main category: cs.CL

TL;DR: PRELUDE是一个评估长文本理解的基准任务，通过判断角色的前传故事是否与原书叙事一致，强调全局理解和深度推理。实验显示，现有模型表现显著落后于人类。


<details>
  <summary>Details</summary>
Motivation: 现有基准任务对长文本理解和深度推理的需求不足，PRELUDE通过更复杂的任务填补这一空白。

Method: 提出PRELUDE任务，要求模型整合间接相关的多部分信息，评估其表现。

Result: 实验表明，现有模型（如LLMs和商业服务）落后人类15%以上，且推理准确率差距超过30%。

Conclusion: 长文本理解和推理仍有显著改进空间，PRELUDE为未来研究提供了挑战性基准。

Abstract: We introduce PRELUDE, a benchmark for evaluating long-context understanding
through the task of determining whether a character's prequel story is
consistent with the canonical narrative of the original book. Our task poses a
stronger demand for global comprehension and deep reasoning than existing
benchmarks -- as the prequels are not part of the original story, assessing
their plausibility typically requires searching and integrating information
that is only indirectly related. Empirically, 88% of instances require evidence
from multiple parts of the narrative. Experimental results highlight the
challenge of our task: in-context learning, RAG and in-domain training with
state-of-the-art LLMs, and commercial DeepResearch services, lag behind humans
by >15%. A further human study reveals that models often produce correct
answers with flawed reasoning, leading to an over 30% gap in reasoning accuracy
compared to humans. These findings underscore the substantial room for
improvement in long-context understanding and reasoning.

</details>


### [48] [Assessing the Feasibility of Lightweight Whisper Models for Low-Resource Urdu Transcription](https://arxiv.org/abs/2508.09865)
*Abdul Rehman Antall,Naveed Akhtar*

Main category: cs.CL

TL;DR: 研究评估了轻量级Whisper模型（Tiny、Base、Small）在低资源环境下对乌尔都语语音识别的可行性，发现Small模型表现最佳（33.68% WER），但仍存在语音和词汇准确性挑战。


<details>
  <summary>Details</summary>
Motivation: 乌尔都语是全球第十大语言，但因其方言多样性、代码转换和训练数据稀缺，其在自动语音识别（ASR）系统中的表现受限。

Method: 使用未微调的Whisper模型在乌尔都语数据集上进行基准测试，以词错误率（WER）为指标。

Result: Whisper-Small模型表现最佳（33.68% WER），优于Tiny（67.08% WER）和Base（53.67% WER）。

Conclusion: Whisper-Small模型在乌尔都语ASR中表现出潜力，但仍需解决语音和词汇准确性问题，为未来低资源ASR研究奠定基础。

Abstract: This study evaluates the feasibility of lightweight Whisper models (Tiny,
Base, Small) for Urdu speech recognition in low-resource settings. Despite Urdu
being the 10th most spoken language globally with over 230 million speakers,
its representation in automatic speech recognition (ASR) systems remains
limited due to dialectal diversity, code-switching, and sparse training data.
We benchmark these models on a curated Urdu dataset using word error rate
(WER), without fine-tuning. Results show Whisper-Small achieves the lowest
error rates (33.68\% WER), outperforming Tiny (67.08\% WER) and Base (53.67\%
WER). Qualitative analysis reveals persistent challenges in phonetic accuracy
and lexical coherence, particularly for complex utterances. While Whisper-Small
demonstrates promise for deployable Urdu ASR, significant gaps remain. Our
findings emphasize lay the groundwork for future research into effective,
low-resource ASR systems.

</details>


### [49] [Memory Decoder: A Pretrained, Plug-and-Play Memory for Large Language Models](https://arxiv.org/abs/2508.09874)
*Jiaqi Cao,Jiarui Wang,Rubin Wei,Qipeng Guo,Kai Chen,Bowen Zhou,Zhouhan Lin*

Main category: cs.CL

TL;DR: 论文提出了一种名为Memory Decoder的插件式预训练记忆模块，用于高效领域适应，无需修改原始模型参数。


<details>
  <summary>Details</summary>
Motivation: 当前领域适应方法（如DAPT）成本高且存在灾难性遗忘问题，而RAG方法则因检索延迟影响效率。

Method: 采用小型Transformer解码器模仿外部非参数检索器的行为，训练后可无缝集成到任何共享相同分词器的预训练语言模型中。

Result: 实验表明，Memory Decoder在生物医学、金融和法律领域平均降低困惑度6.17点。

Conclusion: Memory Decoder为领域适应提供了一种高效、通用的新范式。

Abstract: Large Language Models (LLMs) have shown strong abilities in general language
tasks, yet adapting them to specific domains remains a challenge. Current
method like Domain Adaptive Pretraining (DAPT) requires costly full-parameter
training and suffers from catastrophic forgetting. Meanwhile,
Retrieval-Augmented Generation (RAG) introduces substantial inference latency
due to expensive nearest-neighbor searches and longer context. This paper
introduces Memory Decoder, a plug-and-play pretrained memory that enables
efficient domain adaptation without changing the original model's parameters.
Memory Decoder employs a small transformer decoder that learns to imitate the
behavior of an external non-parametric retriever. Once trained, Memory Decoder
can be seamlessly integrated with any pretrained language model that shares the
same tokenizer, requiring no model-specific modifications. Experimental results
demonstrate that Memory Decoder enables effective adaptation of various Qwen
and Llama models to three distinct specialized domains: biomedicine, finance,
and law, reducing perplexity by an average of 6.17 points. Overall, Memory
Decoder introduces a novel paradigm centered on a specially pretrained memory
component designed for domain-specific adaptation. This memory architecture can
be integrated in a plug-and-play manner, consistently enhancing performance
across multiple models within the target domain.

</details>


### [50] [A Survey of Cognitive Distortion Detection and Classification in NLP](https://arxiv.org/abs/2508.09878)
*Archie Sage,Jeroen Keppens,Helen Yannakoudakis*

Main category: cs.CL

TL;DR: 本文综述了自然语言处理（NLP）在心理健康领域中的应用，特别是认知扭曲（CDs）的自动检测与分类研究，总结了20年来的38项研究，提出了统一的CD分类参考和任务框架，并指出了该领域的开放性问题。


<details>
  <summary>Details</summary>
Motivation: 随着NLP在心理健康领域的应用兴趣增加，研究认知扭曲的自动检测与分类变得重要，但该领域存在分类不一致、任务定义模糊和评估方法不统一的问题。

Method: 本文综述了20年来的38项研究，分析了数据集、建模方法和评估策略，提出了统一的CD分类参考和任务框架。

Result: 研究总结了认知扭曲检测的常见任务设置，并指出了该领域的开放性问题，如分类不一致和评估方法不统一。

Conclusion: 本文为认知扭曲研究提供了结构化的综述，支持该领域更一致和可重复的研究。

Abstract: As interest grows in the application of natural language processing (NLP)
techniques to mental health, a growing body of work explores the automatic
detection and classification of cognitive distortions (CDs). CDs are habitual
patterns of negatively biased or flawed thinking that distort how people
perceive events, judge themselves, and react to the world around them.
Identifying and addressing them is an important part of therapy. Despite its
momentum, the field remains fragmented, with inconsistencies in CD taxonomies,
task formulations, and evaluation practices. This survey reviews 38 studies
spanning two decades, providing a structured overview of datasets, modelling
approaches, and evaluation strategies. We provide a consolidated CD taxonomy
reference, summarise common task setups, and highlight open challenges to
support more coherent and reproducible research in this emerging area.

</details>


### [51] [Language of Persuasion and Misrepresentation in Business Communication: A Textual Detection Approach](https://arxiv.org/abs/2508.09935)
*Sayem Hossen,Monalisa Moon Joti,Md. Golam Rashed*

Main category: cs.CL

TL;DR: 论文探讨了数字化商业沟通中欺骗性语言的检测方法，结合古典修辞学、心理学和语言学理论，通过计算文本分析实现高准确率，但多语言环境仍具挑战性。


<details>
  <summary>Details</summary>
Motivation: 研究动机是数字化沟通中欺骗性语言的增加，需要系统化的检测方法。

Method: 方法结合古典修辞学、心理学理论和计算文本分析，使用个性化Transformer模型。

Result: 在控制环境中检测准确率超过99%，但多语言环境下数据不足和基础设施缺乏导致问题。

Conclusion: 结论指出需要更强的自动文本识别系统，以应对AI与人类沟通的现实挑战。

Abstract: Business communication digitisation has reorganised the process of persuasive
discourse, which
  allows not only greater transparency but also advanced deception. This
inquiry synthesises classical
  rhetoric and communication psychology with linguistic theory and empirical
studies in the financial
  reporting, sustainability discourse, and digital marketing to explain how
deceptive language can be
  systematically detected using persuasive lexicon. In controlled settings,
detection accuracies of greater
  than 99% were achieved by using computational textual analysis as well as
personalised transformer
  models. However, reproducing this performance in multilingual settings is
also problematic and,
  to a large extent, this is because it is not easy to find sufficient data,
and because few multilingual
  text-processing infrastructures are in place. This evidence shows that there
has been an increasing
  gap between the theoretical representations of communication and those
empirically approximated,
  and therefore, there is a need to have strong automatic text-identification
systems where AI-based
  discourse is becoming more realistic in communicating with humans.

</details>


### [52] [A Comprehensive Evaluation framework of Alignment Techniques for LLMs](https://arxiv.org/abs/2508.09937)
*Muneeza Azmat,Momin Abbas,Maysa Malfiza Garcia de Macedo,Marcelo Carpinette Grave,Luan Soares de Souza,Tiago Machado,Rogerio A de Paula,Raya Horesh,Yixin Chen,Heloisa Caroline de Souza Pereira Candello,Rebecka Nordenlow,Aminat Adebiyi*

Main category: cs.CL

TL;DR: 本文提出了一种多维评估框架，用于系统比较LLM对齐技术的不同范式，涵盖检测、质量、效率和鲁棒性四个维度。


<details>
  <summary>Details</summary>
Motivation: 随着LLM在现实应用中的普及，确保其输出符合人类价值观和安全标准变得至关重要，但缺乏统一的评估框架。

Method: 提出一个多维评估框架，通过实验比较不同对齐范式在检测、质量、计算效率和鲁棒性方面的表现。

Result: 实验证明了该框架在识别当前最先进模型的优缺点方面的实用性。

Conclusion: 该框架为未来研究方向提供了有价值的见解。

Abstract: As Large Language Models (LLMs) become increasingly integrated into
real-world applications, ensuring their outputs align with human values and
safety standards has become critical. The field has developed diverse alignment
approaches including traditional fine-tuning methods (RLHF, instruction
tuning), post-hoc correction systems, and inference-time interventions, each
with distinct advantages and limitations. However, the lack of unified
evaluation frameworks makes it difficult to systematically compare these
paradigms and guide deployment decisions. This paper introduces a
multi-dimensional evaluation of alignment techniques for LLMs, a comprehensive
evaluation framework that provides a systematic comparison across all major
alignment paradigms. Our framework assesses methods along four key dimensions:
alignment detection, alignment quality, computational efficiency, and
robustness. Through experiments across diverse base models and alignment
strategies, we demonstrate the utility of our framework in identifying
strengths and limitations of current state-of-the-art models, providing
valuable insights for future research directions.

</details>


### [53] [VisCodex: Unified Multimodal Code Generation via Merging Vision and Coding Models](https://arxiv.org/abs/2508.09945)
*Lingjie Jiang,Shaohan Huang,Xun Wu,Yixia Li,Dongdong Zhang,Furu Wei*

Main category: cs.CL

TL;DR: VisCodex是一个统一框架，通过融合视觉和编码语言模型，提升多模态大语言模型（MLLMs）的多模态代码生成能力。


<details>
  <summary>Details</summary>
Motivation: 当前MLLMs在多模态输入生成代码方面表现有限，需要更强大的视觉与编码结合能力。

Method: 采用任务向量模型融合技术，将先进的编码LLM集成到视觉语言骨干模型中，同时保留视觉理解和编码能力。

Result: VisCodex在开源MLLMs中达到最先进性能，接近GPT-4o等专有模型。

Conclusion: VisCodex的模型融合策略和新数据集（MCD和InfiBench-V）有效提升了多模态代码生成能力。

Abstract: Multimodal large language models (MLLMs) have significantly advanced the
integration of visual and textual understanding. However, their ability to
generate code from multimodal inputs remains limited. In this work, we
introduce VisCodex, a unified framework that seamlessly merges vision and
coding language models to empower MLLMs with strong multimodal code generation
abilities. Leveraging a task vector-based model merging technique, we integrate
a state-of-the-art coding LLM into a strong vision-language backbone, while
preserving both visual comprehension and advanced coding skills. To support
training and evaluation, we introduce the Multimodal Coding Dataset (MCD), a
large-scale and diverse collection of 598k samples, including high-quality HTML
code, chart image-code pairs, image-augmented StackOverflow QA, and algorithmic
problems. Furthermore, we propose InfiBench-V, a novel and challenging
benchmark specifically designed to assess models on visually-rich, real-world
programming questions that demand a nuanced understanding of both textual and
visual contexts. Extensive experiments show that VisCodex achieves
state-of-the-art performance among open-source MLLMs and approaches proprietary
models like GPT-4o, highlighting the effectiveness of our model merging
strategy and new datasets.

</details>


### [54] [Specialised or Generic? Tokenization Choices for Radiology Language Models](https://arxiv.org/abs/2508.09952)
*Hermione Warr,Wentian Xu,Harry Anthony,Yasin Ibrahim,Daniel McGowan,Konstantinos Kamnitsas*

Main category: cs.CL

TL;DR: 研究表明，在放射学报告总结任务中，医学和领域特定分词器优于通用分词器，尤其在模型从头训练时。预训练能部分缩小分词器间的性能差距，但领域特定分词器仍表现最佳，同时还能降低内存需求。


<details>
  <summary>Details</summary>
Motivation: 探索分词器（词汇表）对语言模型在放射学文本生成质量中的影响，填补该领域的研究空白。

Method: 系统比较通用、医学和领域特定分词器在三种成像模态的放射学报告总结任务中的表现，并研究有无PubMed摘要预训练的影响。

Result: 医学和领域特定分词器在从头训练时表现最佳；预训练缩小性能差距；领域特定分词器降低内存需求。

Conclusion: 为语言模型适配临床领域词汇表能提升性能并减少计算需求，使其更适用于研究和实际医疗场景。

Abstract: The vocabulary used by language models (LM) - defined by the tokenizer -
plays a key role in text generation quality. However, its impact remains
under-explored in radiology. In this work, we address this gap by
systematically comparing general, medical, and domain-specific tokenizers on
the task of radiology report summarisation across three imaging modalities. We
also investigate scenarios with and without LM pre-training on PubMed
abstracts. Our findings demonstrate that medical and domain-specific
vocabularies outperformed widely used natural language alternatives when models
are trained from scratch. Pre-training partially mitigates performance
differences between tokenizers, whilst the domain-specific tokenizers achieve
the most favourable results. Domain-specific tokenizers also reduce memory
requirements due to smaller vocabularies and shorter sequences. These results
demonstrate that adapting the vocabulary of LMs to the clinical domain provides
practical benefits, including improved performance and reduced computational
demands, making such models more accessible and effective for both research and
real-world healthcare settings.

</details>


### [55] [Shaping Event Backstories to Estimate Potential Emotion Contexts](https://arxiv.org/abs/2508.09954)
*Johannes Schäfer,Roman Klinger*

Main category: cs.CL

TL;DR: 论文提出了一种通过添加合理上下文来增强事件描述的方法，以提高情感标注的一致性。


<details>
  <summary>Details</summary>
Motivation: 情感分析任务存在模糊性，以往研究关注标注者特性，但忽略了上下文缺失可能是模糊性的根源。

Method: 通过自动生成基于不同情感的事件链，结合短故事生成技术，构建上下文丰富的叙事数据集。

Result: 实验表明，上下文叙事能增强情感解释并提高标注一致性。

Conclusion: 上下文信息有助于情感分析的可靠性和一致性。

Abstract: Emotion analysis is an inherently ambiguous task. Previous work studied
annotator properties to explain disagreement, but this overlooks the
possibility that ambiguity may stem from missing information about the context
of events. In this paper, we propose a novel approach that adds reasonable
contexts to event descriptions, which may better explain a particular
situation. Our goal is to understand whether these enriched contexts enable
human annotators to annotate emotions more reliably. We disambiguate a target
event description by automatically generating multiple event chains conditioned
on differing emotions. By combining techniques from short story generation in
various settings, we achieve coherent narratives that result in a specialized
dataset for the first comprehensive and systematic examination of
contextualized emotion analysis. Through automatic and human evaluation, we
find that contextual narratives enhance the interpretation of specific emotions
and support annotators in producing more consistent annotations.

</details>


### [56] [Performance of GPT-5 Frontier Models in Ophthalmology Question Answering](https://arxiv.org/abs/2508.09956)
*Fares Antaki,David Mikhail,Daniel Milad,Danny A Mammo,Sumit Sharma,Sunil K Srivastava,Bing Yu Chen,Samir Touma,Mertcan Sevgi,Jonathan El-Khoury,Pearse A Keane,Qingyu Chen,Yih Chung Tham,Renaud Duval*

Main category: cs.CL

TL;DR: GPT-5在眼科医学问答任务中表现优异，尤其是GPT-5-high配置，准确率最高，但成本较高。GPT-5-mini-low在成本与性能之间取得最佳平衡。


<details>
  <summary>Details</summary>
Motivation: 研究旨在评估GPT-5系列模型在复杂医学问答任务中的表现，探索最优配置以平衡准确性和成本效率。

Method: 评估了12种GPT-5配置，使用260道眼科选择题，比较准确率、排名、理由质量及成本效益。

Result: GPT-5-high准确率最高（0.965），但未显著优于o3-high（0.958）。GPT-5-mini-low在成本与性能上表现最佳。

Conclusion: GPT-5在眼科数据集上表现优异，推理努力影响准确性，并提出了可扩展的自动评分框架。

Abstract: Large language models (LLMs) such as GPT-5 integrate advanced reasoning
capabilities that may improve performance on complex medical question-answering
tasks. For this latest generation of reasoning models, the configurations that
maximize both accuracy and cost-efficiency have yet to be established. We
evaluated 12 configurations of OpenAI's GPT-5 series (three model tiers across
four reasoning effort settings) alongside o1-high, o3-high, and GPT-4o, using
260 closed-access multiple-choice questions from the American Academy of
Ophthalmology Basic Clinical Science Course (BCSC) dataset. The primary outcome
was multiple-choice accuracy; secondary outcomes included head-to-head ranking
via a Bradley-Terry model, rationale quality assessment using a
reference-anchored, pairwise LLM-as-a-judge framework, and analysis of
accuracy-cost trade-offs using token-based cost estimates. GPT-5-high achieved
the highest accuracy (0.965; 95% CI, 0.942-0.985), outperforming all GPT-5-nano
variants (P < .001), o1-high (P = .04), and GPT-4o (P < .001), but not o3-high
(0.958; 95% CI, 0.931-0.981). GPT-5-high ranked first in both accuracy (1.66x
stronger than o3-high) and rationale quality (1.11x stronger than o3-high).
Cost-accuracy analysis identified several GPT-5 configurations on the Pareto
frontier, with GPT-5-mini-low offering the most favorable low-cost,
high-performance balance. These results benchmark GPT-5 on a high-quality
ophthalmology dataset, demonstrate the influence of reasoning effort on
accuracy, and introduce an autograder framework for scalable evaluation of
LLM-generated answers against reference standards in ophthalmology.

</details>


### [57] [Which one Performs Better? Wav2Vec or Whisper? Applying both in Badini Kurdish Speech to Text (BKSTT)](https://arxiv.org/abs/2508.09957)
*Renas Adnan,Hossein Hassani*

Main category: cs.CL

TL;DR: 该研究旨在为库尔德语Badini方言开发语音转文本（STT）系统，填补现有技术空白。通过使用Wav2Vec2和Whisper模型，实验表明Wav2Vec2表现更优。


<details>
  <summary>Details</summary>
Motivation: Badini方言缺乏STT系统，影响了其社区的技术使用和全球可见性。研究试图解决这一问题。

Method: 使用Badini儿童故事作为文本输入，六位叙述者录制约17小时语音，预处理后得到15小时语音数据。采用Wav2Vec2和Whisper模型开发语言模型。

Result: Wav2Vec2模型在可读性和准确性上显著优于Whisper模型（90.38% vs 65.45%可读性，82.67% vs 53.17%准确性）。

Conclusion: Wav2Vec2模型更适合Badini方言的STT任务，为低资源语言处理提供了有效解决方案。

Abstract: Speech-to-text (STT) systems have a wide range of applications. They are
available in many languages, albeit at different quality levels. Although
Kurdish is considered a less-resourced language from a processing perspective,
SST is available for some of the Kurdish dialects, for instance, Sorani
(Central Kurdish). However, that is not applied to other Kurdish dialects,
Badini and Hawrami, for example. This research is an attempt to address this
gap. Bandin, approximately, has two million speakers, and STT systems can help
their community use mobile and computer-based technologies while giving their
dialect more global visibility. We aim to create a language model based on
Badini's speech and evaluate its performance. To cover a conversational aspect,
have a proper confidence level of grammatical accuracy, and ready
transcriptions, we chose Badini kids' stories, eight books including 78
stories, as the textual input. Six narrators narrated the books, which resulted
in approximately 17 hours of recording. We cleaned, segmented, and tokenized
the input. The preprocessing produced nearly 15 hours of speech, including
19193 segments and 25221 words. We used Wav2Vec2-Large-XLSR-53 and
Whisper-small to develop the language models. The experiments indicate that the
transcriptions process based on the Wav2Vec2-Large-XLSR-53 model provides a
significantly more accurate and readable output than the Whisper-small model,
with 90.38% and 65.45% readability, and 82.67% and 53.17% accuracy,
respectively.

</details>


### [58] [Neural Bandit Based Optimal LLM Selection for a Pipeline of Tasks](https://arxiv.org/abs/2508.09958)
*Baran Atalar,Eddie Zhang,Carlee Joe-Wong*

Main category: cs.CL

TL;DR: 提出一种基于神经上下文老虎机的算法，用于动态选择LLM序列以完成复杂任务。


<details>
  <summary>Details</summary>
Motivation: 随着LLM的普及，如何低成本高效地选择适合的LLM序列成为关键问题。

Method: 使用神经上下文老虎机算法，在线学习LLM在子任务中的表现，动态调整选择策略。

Result: 在电信问答和医疗诊断数据集上验证了算法的有效性。

Conclusion: 该算法能显著提升复杂任务中LLM序列选择的成功率。

Abstract: With the increasing popularity of large language models (LLMs) for a variety
of tasks, there has been a growing interest in strategies that can predict
which out of a set of LLMs will yield a successful answer at low cost. This
problem promises to become more and more relevant as providers like Microsoft
allow users to easily create custom LLM "assistants" specialized to particular
types of queries. However, some tasks (i.e., queries) may be too specialized
and difficult for a single LLM to handle alone. These applications often
benefit from breaking down the task into smaller subtasks, each of which can
then be executed by a LLM expected to perform well on that specific subtask.
For example, in extracting a diagnosis from medical records, one can first
select an LLM to summarize the record, select another to validate the summary,
and then select another, possibly different, LLM to extract the diagnosis from
the summarized record. Unlike existing LLM selection or routing algorithms,
this setting requires that we select a sequence of LLMs, with the output of
each LLM feeding into the next and potentially influencing its success. Thus,
unlike single LLM selection, the quality of each subtask's output directly
affects the inputs, and hence the cost and success rate, of downstream LLMs,
creating complex performance dependencies that must be learned and accounted
for during selection. We propose a neural contextual bandit-based algorithm
that trains neural networks that model LLM success on each subtask in an online
manner, thus learning to guide the LLM selections for the different subtasks,
even in the absence of historical LLM performance data. Experiments on
telecommunications question answering and medical diagnosis prediction datasets
illustrate the effectiveness of our proposed approach compared to other LLM
selection algorithms.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [59] [A Context-aware Attention and Graph Neural Network-based Multimodal Framework for Misogyny Detection](https://arxiv.org/abs/2508.09175)
*Mohammad Zia Ur Rehman,Sufyaan Zahoor,Areeb Manzoor,Musharaf Maqbool,Nagendra Kumar*

Main category: cs.CV

TL;DR: 提出了一种多模态框架，用于检测社交媒体上针对女性的厌女和性别歧视内容，通过三个模块（MANM、GFRM、CFLM）结合视觉和文本信息，性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 社交媒体上针对女性的攻击性内容较多，现有方法难以有效检测厌女内容，需要针对性解决方案。

Method: 提出多模态框架，包含MANM（多模态注意力模块）、GFRM（基于图的特征重构模块）和CFLM（内容特定特征学习模块），结合视觉和文本特征，并使用厌女词典计算分数。

Result: 在两个数据集（MAMI和MMHS150K）上，宏F1平均提升10.17%和8.88%。

Conclusion: 该方法在多模态检测厌女内容上表现优异，显著优于现有方法。

Abstract: A substantial portion of offensive content on social media is directed
towards women. Since the approaches for general offensive content detection
face a challenge in detecting misogynistic content, it requires solutions
tailored to address offensive content against women. To this end, we propose a
novel multimodal framework for the detection of misogynistic and sexist
content. The framework comprises three modules: the Multimodal Attention module
(MANM), the Graph-based Feature Reconstruction Module (GFRM), and the
Content-specific Features Learning Module (CFLM). The MANM employs adaptive
gating-based multimodal context-aware attention, enabling the model to focus on
relevant visual and textual information and generating contextually relevant
features. The GFRM module utilizes graphs to refine features within individual
modalities, while the CFLM focuses on learning text and image-specific features
such as toxicity features and caption features. Additionally, we curate a set
of misogynous lexicons to compute the misogyny-specific lexicon score from the
text. We apply test-time augmentation in feature space to better generalize the
predictions on diverse inputs. The performance of the proposed approach has
been evaluated on two multimodal datasets, MAMI and MMHS150K, with 11,000 and
13,494 samples, respectively. The proposed method demonstrates an average
improvement of 10.17% and 8.88% in macro-F1 over existing methods on the MAMI
and MMHS150K datasets, respectively.

</details>


### [60] [IAD-R1: Reinforcing Consistent Reasoning in Industrial Anomaly Detection](https://arxiv.org/abs/2508.09178)
*Yanhui Li,Yunkang Cao,Chengliang Liu,Yuan Xiong,Xinghui Dong,Chao Huang*

Main category: cs.CV

TL;DR: IAD-R1是一个通用的后训练框架，显著提升了不同架构和参数规模的视觉语言模型（VLMs）在工业异常检测中的性能。


<details>
  <summary>Details</summary>
Motivation: 工业异常检测中缺陷样本稀缺，传统方法受限，而现有VLMs性能不足。

Method: 采用两阶段训练策略：PA-SFT阶段使用高质量Chain-of-Thought数据集增强异常感知；SC-GRPO阶段通过奖励函数实现从感知到解释的跃升。

Result: 在7种VLMs上显著提升，平均准确率最高提升43.3%，0.5B参数模型在零样本设置下超越GPT-4.1等商业模型。

Conclusion: IAD-R1展示了在工业异常检测中的高效性和优越性，相关资源将开源。

Abstract: Industrial anomaly detection is a critical component of modern manufacturing,
yet the scarcity of defective samples restricts traditional detection methods
to scenario-specific applications. Although Vision-Language Models (VLMs)
demonstrate significant advantages in generalization capabilities, their
performance in industrial anomaly detection remains limited. To address this
challenge, we propose IAD-R1, a universal post-training framework applicable to
VLMs of different architectures and parameter scales, which substantially
enhances their anomaly detection capabilities. IAD-R1 employs a two-stage
training strategy: the Perception Activation Supervised Fine-Tuning (PA-SFT)
stage utilizes a meticulously constructed high-quality Chain-of-Thought dataset
(Expert-AD) for training, enhancing anomaly perception capabilities and
establishing reasoning-to-answer correlations; the Structured Control Group
Relative Policy Optimization (SC-GRPO) stage employs carefully designed reward
functions to achieve a capability leap from "Anomaly Perception" to "Anomaly
Interpretation". Experimental results demonstrate that IAD-R1 achieves
significant improvements across 7 VLMs, attaining up to 43.3% enhancement in
average accuracy on 6 industrial anomaly detection benchmark datasets. Notably,
the 0.5B parameter model trained with IAD-R1 surpasses commercial models
including GPT-4.1 and Claude-Sonnet-4 in zero-shot settings, demonstrating the
effectiveness and superiority of IAD-R1. The dataset, code, and all model
weights will be publicly available at https://github.com/Yanhui-Lee/IAD-R1.

</details>


### [61] [A Neurosymbolic Framework for Interpretable Cognitive Attack Detection in Augmented Reality](https://arxiv.org/abs/2508.09185)
*Rongqian Chen,Allison Andreyev,Yanming Xiu,Mahdi Imani,Bin Li,Maria Gorlatova,Gang Tan,Tian Lan*

Main category: cs.CV

TL;DR: CADAR是一种新型神经符号方法，用于检测AR中的认知攻击，结合视觉语言模型和粒子滤波，提高了检测准确性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有方法在检测AR认知攻击时，要么缺乏语义推理能力，要么依赖黑盒模型，可解释性不足。

Method: CADAR融合多模态视觉语言输入，生成感知图表示，并利用粒子滤波进行统计推理。

Result: 在扩展的AR认知攻击数据集上，CADAR比基线方法准确率提高了10.7%。

Conclusion: 神经符号方法在AR认知攻击检测中具有高效性和可解释性的潜力。

Abstract: Augmented Reality (AR) enriches perception by overlaying virtual elements on
the physical world. Due to its growing popularity, cognitive attacks that alter
AR content to manipulate users' semantic perception have received increasing
attention. Existing detection methods often focus on visual changes, which are
restricted to pixel- or image-level processing and lack semantic reasoning
capabilities, or they rely on pre-trained vision-language models (VLMs), which
function as black-box approaches with limited interpretability. In this paper,
we present CADAR, a novel neurosymbolic approach for cognitive attack detection
in AR. It fuses multimodal vision-language inputs using neural VLMs to obtain a
symbolic perception-graph representation, incorporating prior knowledge,
salience weighting, and temporal correlations. The model then enables
particle-filter based statistical reasoning -- a sequential Monte Carlo method
-- to detect cognitive attacks. Thus, CADAR inherits the adaptability of
pre-trained VLM and the interpretability and reasoning rigor of particle
filtering. Experiments on an extended AR cognitive attack dataset show accuracy
improvements of up to 10.7% over strong baselines on challenging AR attack
scenarios, underscoring the promise of neurosymbolic methods for effective and
interpretable cognitive attack detection.

</details>


### [62] [Waymo-3DSkelMo: A Multi-Agent 3D Skeletal Motion Dataset for Pedestrian Interaction Modeling in Autonomous Driving](https://arxiv.org/abs/2508.09404)
*Guangxun Zhu,Shiyu Fan,Hang Dai,Edmond S. L. Ho*

Main category: cs.CV

TL;DR: Waymo-3DSkelMo是一个大规模高质量的3D骨骼运动数据集，用于提升自动驾驶中行人交互理解的精细度。


<details>
  <summary>Details</summary>
Motivation: 现有数据集依赖单目RGB视频帧估计3D姿态，存在遮挡和时间不连续问题，导致运动质量低。

Method: 利用3D人体形状和运动先验，从LiDAR点云中提取高质量的3D姿态序列。

Result: 数据集覆盖800多个真实驾驶场景，包含丰富的交互行为，并建立了3D姿态预测基准。

Conclusion: 该数据集为复杂城市环境中精细人类行为理解提供了基础资源。

Abstract: Large-scale high-quality 3D motion datasets with multi-person interactions
are crucial for data-driven models in autonomous driving to achieve
fine-grained pedestrian interaction understanding in dynamic urban
environments. However, existing datasets mostly rely on estimating 3D poses
from monocular RGB video frames, which suffer from occlusion and lack of
temporal continuity, thus resulting in unrealistic and low-quality human
motion. In this paper, we introduce Waymo-3DSkelMo, the first large-scale
dataset providing high-quality, temporally coherent 3D skeletal motions with
explicit interaction semantics, derived from the Waymo Perception dataset. Our
key insight is to utilize 3D human body shape and motion priors to enhance the
quality of the 3D pose sequences extracted from the raw LiDRA point clouds. The
dataset covers over 14,000 seconds across more than 800 real driving scenarios,
including rich interactions among an average of 27 agents per scene (with up to
250 agents in the largest scene). Furthermore, we establish 3D pose forecasting
benchmarks under varying pedestrian densities, and the results demonstrate its
value as a foundational resource for future research on fine-grained human
behavior understanding in complex urban environments. The dataset and code will
be available at https://github.com/GuangxunZhu/Waymo-3DSkelMo

</details>


### [63] [RL-MoE: An Image-Based Privacy Preserving Approach In Intelligent Transportation System](https://arxiv.org/abs/2508.09186)
*Abdolazim Rezaei,Mehdi Sookhak,Mahboobeh Haghparast*

Main category: cs.CV

TL;DR: 论文提出RL-MoE框架，将敏感视觉数据转为隐私保护文本描述，解决智能交通系统中隐私与数据效用的冲突。


<details>
  <summary>Details</summary>
Motivation: 智能交通系统中AI摄像头的普及导致隐私与数据需求的冲突，现有隐私保护方法不足。

Method: 结合Mixture-of-Experts架构和强化学习代理，优化生成文本的语义准确性和隐私保护。

Result: RL-MoE显著降低重放攻击成功率至9.4%，同时生成更丰富的文本内容。

Conclusion: RL-MoE为隐私敏感领域提供实用、可扩展的解决方案，推动安全智能城市和自动驾驶网络发展。

Abstract: The proliferation of AI-powered cameras in Intelligent Transportation Systems
(ITS) creates a severe conflict between the need for rich visual data and the
fundamental right to privacy. Existing privacy-preserving mechanisms, such as
blurring or encryption, are often insufficient, creating an undesirable
trade-off where either privacy is compromised against advanced reconstruction
attacks or data utility is critically degraded. To resolve this impasse, we
propose RL-MoE, a novel framework that transforms sensitive visual data into
privacy-preserving textual descriptions, eliminating the need for direct image
transmission. RL-MoE uniquely combines a Mixture-of-Experts (MoE) architecture
for nuanced, multi-aspect scene decomposition with a Reinforcement Learning
(RL) agent that optimizes the generated text for a dual objective of semantic
accuracy and privacy preservation. Extensive experiments demonstrate that
RL-MoE provides superior privacy protection, reducing the success rate of
replay attacks to just 9.4\% on the CFP-FP dataset, while simultaneously
generating richer textual content than baseline methods. Our work provides a
practical and scalable solution for building trustworthy AI systems in
privacy-sensitive domains, paving the way for more secure smart city and
autonomous vehicle networks.

</details>


### [64] [Episodic Memory Representation for Long-form Video Understanding](https://arxiv.org/abs/2508.09486)
*Yun Wang,Long Zhang,Jingren Liu,Jiaqi Yan,Zhanjie Zhang,Jiahao Zheng,Xun Yang,Dapeng Wu,Xiangyu Chen,Xuelong Li*

Main category: cs.CV

TL;DR: Video-EM框架通过模拟人类情景记忆，解决了Video-LLMs在长视频理解中的关键帧冗余和时空关系缺失问题，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法将长视频简化为静态关键帧匹配，忽略了时空关系和上下文连续性，导致信息冗余和关键线索丢失。

Method: Video-EM将关键帧建模为时序性情景事件，结合空间和时间动态，并利用LLM的链式思维迭代筛选最小但信息丰富的记忆子集。

Result: 在多个基准测试中，Video-EM性能提升4-9%，且使用更少帧数。

Conclusion: Video-EM通过情景记忆和链式思维，显著提升了长视频理解的准确性和效率。

Abstract: Video Large Language Models (Video-LLMs) excel at general video understanding
but struggle with long-form videos due to context window limits. Consequently,
recent approaches focus on keyframe retrieval, condensing lengthy videos into a
small set of informative frames. Despite their practicality, these methods
simplify the problem to static text image matching, overlooking spatio temporal
relationships crucial for capturing scene transitions and contextual
continuity, and may yield redundant keyframes with limited information,
diluting salient cues essential for accurate video question answering. To
address these limitations, we introduce Video-EM, a training free framework
inspired by the principles of human episodic memory, designed to facilitate
robust and contextually grounded reasoning. Rather than treating keyframes as
isolated visual entities, Video-EM explicitly models them as temporally ordered
episodic events, capturing both spatial relationships and temporal dynamics
necessary for accurately reconstructing the underlying narrative. Furthermore,
the framework leverages chain of thought (CoT) thinking with LLMs to
iteratively identify a minimal yet highly informative subset of episodic
memories, enabling efficient and accurate question answering by Video-LLMs.
Extensive evaluations on the Video-MME, EgoSchema, HourVideo, and LVBench
benchmarks confirm the superiority of Video-EM, which achieves highly
competitive results with performance gains of 4-9 percent over respective
baselines while utilizing fewer frames.

</details>


### [65] [Synthetic Data Generation for Emotional Depth Faces: Optimizing Conditional DCGANs via Genetic Algorithms in the Latent Space and Stabilizing Training with Knowledge Distillation](https://arxiv.org/abs/2508.09188)
*Seyed Muhammad Hossein Mousavi,S. Younes Mirinezhad*

Main category: cs.CV

TL;DR: 提出了一种基于优化GAN和知识蒸馏的合成深度人脸生成框架，结合遗传算法提升多样性和质量，在分类任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: 解决情感计算中高质量、多样化深度人脸数据集缺乏的问题。

Method: 使用优化的GAN和知识蒸馏（EMA教师模型）稳定训练，结合遗传算法优化潜在向量，提取多种特征进行分类。

Result: 在多样性和质量上优于GAN、VAE、GMM和KDE，分类准确率达94%和96%。

Conclusion: 该方法在生成和分类任务中均优于现有技术，为情感计算提供了高质量数据支持。

Abstract: Affective computing faces a major challenge: the lack of high-quality,
diverse depth facial datasets for recognizing subtle emotional expressions. We
propose a framework for synthetic depth face generation using an optimized GAN
with Knowledge Distillation (EMA teacher models) to stabilize training, improve
quality, and prevent mode collapse. We also apply Genetic Algorithms to evolve
GAN latent vectors based on image statistics, boosting diversity and visual
quality for target emotions. The approach outperforms GAN, VAE, GMM, and KDE in
both diversity and quality. For classification, we extract and concatenate LBP,
HOG, Sobel edge, and intensity histogram features, achieving 94% and 96%
accuracy with XGBoost. Evaluation using FID, IS, SSIM, and PSNR shows
consistent improvement over state-of-the-art methods.

</details>


### [66] [$Δ$-AttnMask: Attention-Guided Masked Hidden States for Efficient Data Selection and Augmentation](https://arxiv.org/abs/2508.09199)
*Jucheng Hu,Suorong Yang,Dongzhan Zhou*

Main category: cs.CV

TL;DR: 论文提出了一种名为Δ-AttnMask的数据高效框架，用于视觉指令微调（VIF），通过注意力引导的隐藏状态掩码量化样本质量，仅需20%数据即可实现最佳性能。


<details>
  <summary>Details</summary>
Motivation: 视觉指令微调需要多模态数据，数据选择挑战大且研究不足，亟需高效方法。

Method: Δ-AttnMask通过计算原始状态与高注意力区域掩码状态的损失差异（Δ）评估样本质量，无需额外标签或训练。

Result: 实验表明，Δ-AttnMask仅用20%数据即可加速训练5倍，并在准确率上超越全数据基线10.1%。

Conclusion: Δ-AttnMask是一种模型无关、数据无关的通用框架，适用于多种模态和架构。

Abstract: Visual Instruction Finetuning (VIF) is pivotal for post-training
Vision-Language Models (VLMs). Unlike unimodal instruction finetuning in
plain-text large language models, which mainly requires instruction datasets to
enable model instruction-following ability, VIF also requires multimodal data
to enable joint visual and textual understanding; therefore, it typically
requires more data. Consequently, VIF imposes stricter data selection
challenges: the method must scale efficiently to handle larger data demands
while ensuring the quality of both visual and textual content, as well as their
alignment. Despite its critical impact on performance, data selection for VIF
remains an understudied area. In this paper, we propose $\Delta$-AttnMask. This
data-efficient framework quantifies sample quality through attention-guided
masking of the model's hidden states, jointly evaluating image-text pairs
without requiring domain labels, auxiliary models, or extra training. By
computing loss differences ($\Delta$) between the original states and states
masked using high-attention regions, $\Delta$-AttnMask intrinsically assesses
sample quality. Experiments across multiple VLMs and datasets show that
$\Delta$-AttnMask achieves state-of-the-art performance with just 20% of data,
accelerating training by 5x while surpassing full-dataset baselines by +10.1%
in overall accuracy. Its model-agnostic and data-agnostic design ensures broad
applicability across modalities and architectures.

</details>


### [67] [Personalized Feature Translation for Expression Recognition: An Efficient Source-Free Domain Adaptation Method](https://arxiv.org/abs/2508.09202)
*Masoumeh Sharafi,Soufiane Belharbi,Houssem Ben Salem,Ali Etemad,Alessandro Lameiras Koerich,Marco Pedersoli,Simon Bacon,Eric Granger*

Main category: cs.CV

TL;DR: 提出了一种轻量级的个性化特征翻译方法（PFT），用于源自由域适应（SFDA），解决了仅使用中性表情目标数据时的挑战，避免了图像合成的复杂性和计算开销。


<details>
  <summary>Details</summary>
Motivation: 现有SFDA方法通常无法适应仅含单类（中性表情）的目标数据，且图像生成方法不稳定且计算量大。

Method: 在潜在空间中预训练翻译器，通过优化表情一致性和风格感知目标，保留表情信息，随后仅用中性目标数据适应翻译器。

Result: PFT避免了图像合成的复杂性，生成优化的分类嵌入，计算开销低，方法高效。

Conclusion: PFT为仅含中性表情数据的SFDA提供了高效解决方案，避免了图像合成的挑战。

Abstract: Facial expression recognition (FER) models are employed in many video-based
affective computing applications, such as human-computer interaction and
healthcare monitoring. However, deep FER models often struggle with subtle
expressions and high inter-subject variability, limiting their performance in
real-world applications. To improve their performance, source-free domain
adaptation (SFDA) methods have been proposed to personalize a pretrained source
model using only unlabeled target domain data, thereby avoiding data privacy,
storage, and transmission constraints. This paper addresses a challenging
scenario where source data is unavailable for adaptation, and only unlabeled
target data consisting solely of neutral expressions is available. SFDA methods
are not typically designed to adapt using target data from only a single class.
Further, using models to generate facial images with non-neutral expressions
can be unstable and computationally intensive. In this paper, personalized
feature translation (PFT) is proposed for SFDA. Unlike current image
translation methods for SFDA, our lightweight method operates in the latent
space. We first pre-train the translator on the source domain data to transform
the subject-specific style features from one source subject into another.
Expression information is preserved by optimizing a combination of expression
consistency and style-aware objectives. Then, the translator is adapted on
neutral target data, without using source data or image synthesis. By
translating in the latent space, PFT avoids the complexity and noise of face
expression generation, producing discriminative embeddings optimized for
classification. Using PFT eliminates the need for image synthesis, reduces
computational overhead (using a lightweight translator), and only adapts part
of the model, making the method efficient compared to image-based translation.

</details>


### [68] [GANime: Generating Anime and Manga Character Drawings from Sketches with Deep Learning](https://arxiv.org/abs/2508.09207)
*Tai Vu,Robert Yang*

Main category: cs.CV

TL;DR: 研究比较了几种图像到图像转换模型在动漫角色与草图之间的效果，发现C-GAN表现最佳。


<details>
  <summary>Details</summary>
Motivation: 解决动漫行业中从草图生成全彩图的高成本问题。

Method: 评估了Neural Style Transfer、C-GAN和CycleGAN等模型。

Result: C-GAN能生成接近人类创作的高质量、高分辨率图像。

Conclusion: C-GAN是解决该问题的最有效模型。

Abstract: The process of generating fully colorized drawings from sketches is a large,
usually costly bottleneck in the manga and anime industry. In this study, we
examine multiple models for image-to-image translation between anime characters
and their sketches, including Neural Style Transfer, C-GAN, and CycleGAN. By
assessing them qualitatively and quantitatively, we find that C-GAN is the most
effective model that is able to produce high-quality and high-resolution images
close to those created by humans.

</details>


### [69] [MME-Emotion: A Holistic Evaluation Benchmark for Emotional Intelligence in Multimodal Large Language Models](https://arxiv.org/abs/2508.09210)
*Fan Zhang,Zebang Cheng,Chong Deng,Haoxuan Li,Zheng Lian,Qian Chen,Huadai Liu,Wen Wang,Yi-Fan Zhang,Renrui Zhang,Ziyu Guo,Zhihong Zhu,Hao Wu,Haixin Wang,Yefeng Zheng,Xiaojiang Peng,Xian Wu,Kun Wang,Xiangang Li,Jieping Ye,Pheng-Ann Heng*

Main category: cs.CV

TL;DR: 论文提出了MME-Emotion基准，用于评估多模态大语言模型（MLLMs）的情感情感和推理能力，揭示了当前模型的局限性。


<details>
  <summary>Details</summary>
Motivation: 当前情感基准在泛化能力和情感触发因素推理方面存在不足，需要更系统的评估工具。

Method: 构建了包含6,000多个视频片段和任务特定问答对的MME-Emotion基准，涵盖八种情感任务，并采用混合指标和多代理系统框架进行评估。

Result: 评估20个先进MLLMs发现，其情感情感表现不佳，最佳模型识别分数仅为39.3%，推理分数为56.0%。

Conclusion: MME-Emotion为未来提升MLLMs的情感情感能力提供了基础。

Abstract: Recent advances in multimodal large language models (MLLMs) have catalyzed
transformative progress in affective computing, enabling models to exhibit
emergent emotional intelligence. Despite substantial methodological progress,
current emotional benchmarks remain limited, as it is still unknown: (a) the
generalization abilities of MLLMs across distinct scenarios, and (b) their
reasoning capabilities to identify the triggering factors behind emotional
states. To bridge these gaps, we present \textbf{MME-Emotion}, a systematic
benchmark that assesses both emotional understanding and reasoning capabilities
of MLLMs, enjoying \textit{scalable capacity}, \textit{diverse settings}, and
\textit{unified protocols}. As the largest emotional intelligence benchmark for
MLLMs, MME-Emotion contains over 6,000 curated video clips with task-specific
questioning-answering (QA) pairs, spanning broad scenarios to formulate eight
emotional tasks. It further incorporates a holistic evaluation suite with
hybrid metrics for emotion recognition and reasoning, analyzed through a
multi-agent system framework. Through a rigorous evaluation of 20 advanced
MLLMs, we uncover both their strengths and limitations, yielding several key
insights: \ding{182} Current MLLMs exhibit unsatisfactory emotional
intelligence, with the best-performing model achieving only $39.3\%$
recognition score and $56.0\%$ Chain-of-Thought (CoT) score on our benchmark.
\ding{183} Generalist models (\emph{e.g.}, Gemini-2.5-Pro) derive emotional
intelligence from generalized multimodal understanding capabilities, while
specialist models (\emph{e.g.}, R1-Omni) can achieve comparable performance
through domain-specific post-training adaptation. By introducing MME-Emotion,
we hope that it can serve as a foundation for advancing MLLMs' emotional
intelligence in the future.

</details>


### [70] [Towards Effective MLLM Jailbreaking Through Balanced On-Topicness and OOD-Intensity](https://arxiv.org/abs/2508.09218)
*Zuoou Li,Weitong Zhang,Jingyuan Wang,Shuyuan Zhang,Wenjia Bai,Bernhard Kainz,Mengyun Qiao*

Main category: cs.CV

TL;DR: 论文提出了一种四轴评估框架和递归重写策略BSD，用于更准确地评估和提升对抗性提示的攻击效果。


<details>
  <summary>Details</summary>
Motivation: 当前多模态大语言模型（MLLMs）的安全机制存在漏洞，且现有评估标准可能高估攻击效果。

Method: 引入四轴评估框架（输入相关性、输入OOD强度、输出危害性、输出拒绝率）和递归重写策略BSD。

Result: BSD在13个MLLMs中显著提升攻击成功率和危害性，同时减少拒绝率。

Conclusion: BSD揭示了当前多模态安全系统的潜在弱点，为改进安全机制提供了方向。

Abstract: Multimodal large language models (MLLMs) are widely used in vision-language
reasoning tasks. However, their vulnerability to adversarial prompts remains a
serious concern, as safety mechanisms often fail to prevent the generation of
harmful outputs. Although recent jailbreak strategies report high success
rates, many responses classified as "successful" are actually benign, vague, or
unrelated to the intended malicious goal. This mismatch suggests that current
evaluation standards may overestimate the effectiveness of such attacks. To
address this issue, we introduce a four-axis evaluation framework that
considers input on-topicness, input out-of-distribution (OOD) intensity, output
harmfulness, and output refusal rate. This framework identifies truly effective
jailbreaks. In a substantial empirical study, we reveal a structural trade-off:
highly on-topic prompts are frequently blocked by safety filters, whereas those
that are too OOD often evade detection but fail to produce harmful content.
However, prompts that balance relevance and novelty are more likely to evade
filters and trigger dangerous output. Building on this insight, we develop a
recursive rewriting strategy called Balanced Structural Decomposition (BSD).
The approach restructures malicious prompts into semantically aligned
sub-tasks, while introducing subtle OOD signals and visual cues that make the
inputs harder to detect. BSD was tested across 13 commercial and open-source
MLLMs, where it consistently led to higher attack success rates, more harmful
outputs, and fewer refusals. Compared to previous methods, it improves success
rates by $67\%$ and harmfulness by $21\%$, revealing a previously
underappreciated weakness in current multimodal safety systems.

</details>


### [71] [Towards Scalable Training for Handwritten Mathematical Expression Recognition](https://arxiv.org/abs/2508.09220)
*Haoyang Li,Jiaqing Li,Jialun Cao,Zongyuan Yang,Yongping Xiong*

Main category: cs.CV

TL;DR: 论文提出了一种结合手写公式和LaTeX渲染公式的新方法，构建了最大的公式数据集Tex80M，并训练了首个大规模HMER模型TexTeller，取得了SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 手写数学表达式识别（HMER）因数据稀缺而受限，主要由于人工标注成本高昂。

Method: 开发了一个可扩展的数据引擎，结合有限手写公式和大规模LaTeX渲染公式，生成复杂且一致的LaTeX序列，构建了Tex80M数据集。

Result: TexTeller模型在几乎所有基准测试中达到了SOTA性能。

Conclusion: 论文将公开模型、数据集和代码，推动HMER领域的进一步研究。

Abstract: Large foundation models have achieved significant performance gains through
scalable training on massive datasets. However, the field of
\textbf{H}andwritten \textbf{M}athematical \textbf{E}xpression
\textbf{R}ecognition (HMER) has been impeded by the scarcity of data, primarily
due to the arduous and costly process of manual annotation. To bridge this gap,
we propose a novel method integrating limited handwritten formulas with
large-scale LaTeX-rendered formulas by developing a scalable data engine to
generate complex and consistent LaTeX sequences. With this engine, we built the
largest formula dataset to date, termed \texttt{Tex80M}, comprising over 80
million high-quality training instances. Then we propose \texttt{TexTeller},
the first HMER model trained at scale, by mix-training \texttt{Tex80M} with a
relatively small HME dataset. The expansive training dataset and our refined
pipeline have equipped \texttt{TexTeller} with state-of-the-art (SOTA)
performance across nearly all benchmarks. To advance the field, we will openly
release our complete model, entire dataset, and full codebase, enabling further
research building upon our contributions.

</details>


### [72] [Gradient-Direction-Aware Density Control for 3D Gaussian Splatting](https://arxiv.org/abs/2508.09239)
*Zheng Zhou,Yu-Jie Xiong,Chun-Ming Xia,Jia-Chen Zhang,Hong-Jian Zhan*

Main category: cs.CV

TL;DR: GDAGS提出了一种基于梯度方向感知的自适应密度控制框架，解决了3D高斯溅射中的过度重建和过度密集问题，显著提升了渲染质量并减少了内存消耗。


<details>
  <summary>Details</summary>
Motivation: 现有3D高斯溅射方法在复杂场景中存在过度重建和过度密集的问题，导致渲染质量下降和内存开销增加。

Method: GDAGS通过梯度一致性比率（GCR）和非线性动态加权机制，实现了梯度方向感知的密度控制，优化了高斯分布。

Result: GDAGS在多种真实场景基准测试中表现出色，渲染质量更高，内存消耗减少50%。

Conclusion: GDAGS有效解决了3D高斯溅射中的关键问题，为实时逼真渲染提供了更高效的解决方案。

Abstract: The emergence of 3D Gaussian Splatting (3DGS) has significantly advanced
novel view synthesis through explicit scene representation, enabling real-time
photorealistic rendering. However, existing approaches manifest two critical
limitations in complex scenarios: (1) Over-reconstruction occurs when
persistent large Gaussians cannot meet adaptive splitting thresholds during
density control. This is exacerbated by conflicting gradient directions that
prevent effective splitting of these Gaussians; (2) Over-densification of
Gaussians occurs in regions with aligned gradient aggregation, leading to
redundant component proliferation. This redundancy significantly increases
memory overhead due to unnecessary data retention. We present
Gradient-Direction-Aware Gaussian Splatting (GDAGS), a gradient-direction-aware
adaptive density control framework to address these challenges. Our key
innovations: the gradient coherence ratio (GCR), computed through normalized
gradient vector norms, which explicitly discriminates Gaussians with concordant
versus conflicting gradient directions; and a nonlinear dynamic weighting
mechanism leverages the GCR to enable gradient-direction-aware density control.
Specifically, GDAGS prioritizes conflicting-gradient Gaussians during splitting
operations to enhance geometric details while suppressing redundant
concordant-direction Gaussians. Conversely, in cloning processes, GDAGS
promotes concordant-direction Gaussian densification for structural completion
while preventing conflicting-direction Gaussian overpopulation. Comprehensive
evaluations across diverse real-world benchmarks demonstrate that GDAGS
achieves superior rendering quality while effectively mitigating
over-reconstruction, suppressing over-densification, and constructing compact
scene representations with 50\% reduced memory consumption through optimized
Gaussians utilization.

</details>


### [73] [FineState-Bench: A Comprehensive Benchmark for Fine-Grained State Control in GUI Agents](https://arxiv.org/abs/2508.09241)
*Fengxian Ji,Jingpu Yang,Zirui Song,Yuanxi Wang,Zhexuan Cui,Yuke Li,Qian Jiang,Miao Fang,Xiuying Chen*

Main category: cs.CV

TL;DR: FineState-Bench是首个针对细粒度GUI代理操作的评估标准，旨在量化精细控制能力，填补现有评测框架的不足。


<details>
  <summary>Details</summary>
Motivation: 现有GUI代理评测框架过于关注粗粒度任务完成，而忽略了细粒度控制能力的重要性。

Method: 提出FineState-Bench，包含2257个任务基准和四阶段指标，并开发了Visual Diagnostic Assistant（VDA）进行定量分析。

Result: 实验显示最先进模型的细粒度交互准确率仅为32.8%，理想视觉定位可将成功率提升14.9%。

Conclusion: 当前GUI代理的主要瓶颈是基础视觉定位能力，所有资源已开源。

Abstract: With the rapid advancement of generative artificial intelligence technology,
Graphical User Interface (GUI) agents have demonstrated tremendous potential
for autonomously managing daily tasks through natural language instructions.
However, current evaluation frameworks for GUI agents suffer from fundamental
flaws: existing benchmarks overly focus on coarse-grained task completion while
neglecting fine-grained control capabilities crucial for real-world
applications. To address this, we introduce FineState-Bench, the first
evaluation and diagnostic standard for fine-grained GUI proxy operations,
designed to quantify fine-grained control. This multi-platform (desktop, Web,
mobile) framework includes 2257 task benchmarks in four components and uses a
four-phase indicator for comprehensive perception-to-control assessment. To
analyze perception and positioning for refined operations, we developed the
plug-and-play Visual Diagnostic Assistant (VDA), enabling the first
quantitative decoupling analysis of these capabilities. Experimental results on
our benchmark show that the most advanced models achieve only 32.8%
fine-grained interaction accuracy. Using our VDA in controlled experiments,
quantifying the impact of visual capabilities, we showed that ideal visual
localization boosts Gemini-2.5-Flash's success rate by 14.9\%. Our diagnostic
framework confirms for the first time that the primary bottleneck for current
GUI proxies is basic visual positioning capability.All resources are fully
open-source. github: https://github.com/AnonymousThewarehouse/FineState-Bench
huggingface: https://huggingface.co/datasets/Willtime2006/Static-FineBench

</details>


### [74] [Beyond Blanket Masking: Examining Granularity for Privacy Protection in Images Captured by Blind and Low Vision Users](https://arxiv.org/abs/2508.09245)
*Jeffri Murrugarra-LLerena,Haoran Niu,K. Suzanne Barber,Hal Daumé III,Yang Trista Cao,Paola Cascante-Bonilla*

Main category: cs.CV

TL;DR: FiGPriv是一种细粒度隐私保护框架，通过选择性屏蔽高风险隐私信息，提升视觉语言模型的可用性。


<details>
  <summary>Details</summary>
Motivation: 解决现有隐私保护方法因粗粒度屏蔽导致可用性降低的问题，特别是针对盲人和低视力用户的隐私需求。

Method: 结合细粒度分割和数据驱动的风险评分机制，选择性屏蔽高风险隐私信息。

Result: 在BIV-Priv-Seg数据集上测试，FiGPriv保留26%的图像内容，提升模型响应能力11%和内容识别能力45%。

Conclusion: FiGPriv在保护隐私的同时显著提升了视觉语言模型的可用性和功能性。

Abstract: As visual assistant systems powered by visual language models (VLMs) become
more prevalent, concerns over user privacy have grown, particularly for blind
and low vision users who may unknowingly capture personal private information
in their images. Existing privacy protection methods rely on coarse-grained
segmentation, which uniformly masks entire private objects, often at the cost
of usability. In this work, we propose FiGPriv, a fine-grained privacy
protection framework that selectively masks only high-risk private information
while preserving low-risk information. Our approach integrates fine-grained
segmentation with a data-driven risk scoring mechanism. We evaluate our
framework using the BIV-Priv-Seg dataset and show that FiG-Priv preserves +26%
of image content, enhancing the ability of VLMs to provide useful responses by
11% and identify the image content by 45%, while ensuring privacy protection.
Project Page: https://artcs1.github.io/VLMPrivacy/

</details>


### [75] [Harnessing Input-Adaptive Inference for Efficient VLN](https://arxiv.org/abs/2508.09262)
*Dongwoo Kang,Akhil Perincherry,Zachary Coalson,Aiden Gabriel,Stefan Lee,Sanghyun Hong*

Main category: cs.CV

TL;DR: 提出了一种新型输入自适应导航方法，通过三种算法提升视觉与语言导航（VLN）模型的效率，显著减少计算量。


<details>
  <summary>Details</summary>
Motivation: 现有输入自适应机制在减少计算量时会导致性能显著下降，因此需要更高效的解决方案。

Method: 1. 空间效率：选择性处理全景视图；2. 模型内效率：基于重要性的自适应阈值；3. 时间效率：缓存机制避免重复处理。

Result: 在七个VLN基准测试中，计算量减少超过2倍。

Conclusion: 提出的方法在不显著降低性能的情况下，显著提升了VLN模型的效率。

Abstract: An emerging paradigm in vision-and-language navigation (VLN) is the use of
history-aware multi-modal transformer models. Given a language instruction,
these models process observation and navigation history to predict the most
appropriate action for an agent. While they have significantly improved
performance, the scale of these models can be a bottleneck in practical
settings with limited computational resources. In this work, we propose a novel
input-adaptive navigation method to enhance VLN model efficiency. We first show
that existing input-adaptive mechanisms fail to reduce computations without
substantial performance degradation. To address this, we introduce three
adaptive algorithms, each deployed at a different level: (1) To improve spatial
efficiency, we selectively process panoramic views at each observation of an
agent. (2) To improve intra-model efficiency, we propose importance-based
adaptive thresholding for the early-exit methods. (3) To improve temporal
efficiency, we implement a caching mechanism that prevents reprocessing of
views previously seen by the agent. In evaluations on seven VLN benchmarks, we
demonstrate over a 2$\times$ reduction in computation across three
off-the-shelf agents in both standard and continuous environments. Our code is
publicly available at
https://github.com/secure-ai-systems-group/adaptive-vision-and-language-navigation.

</details>


### [76] [SegDAC: Segmentation-Driven Actor-Critic for Visual Reinforcement Learning](https://arxiv.org/abs/2508.09325)
*Alexandre Brown,Glen Berseth*

Main category: cs.CV

TL;DR: SegDAC是一种基于分割的视觉强化学习方法，通过结合SAM和YOLO-World实现对象分解和语义标注，显著提升了视觉泛化能力和样本效率。


<details>
  <summary>Details</summary>
Motivation: 视觉强化学习面临高维输入和噪声奖励的挑战，现有感知模型与RL的整合效果不明，需要一种更有效的方法。

Method: SegDAC结合SAM进行对象分解，YOLO-World提供语义标注，采用基于Transformer的动态架构，通过在线RL学习关注关键片段。

Result: 在Maniskill3基准测试中，SegDAC在视觉泛化能力上表现优异，性能翻倍，样本效率也优于现有方法。

Conclusion: SegDAC通过对象分解和动态注意力机制，显著提升了视觉强化学习的泛化能力和效率。

Abstract: Visual reinforcement learning (RL) is challenging due to the need to learn
both perception and actions from high-dimensional inputs and noisy rewards.
Although large perception models exist, integrating them effectively into RL
for visual generalization and improved sample efficiency remains unclear. We
propose SegDAC, a Segmentation-Driven Actor-Critic method. SegDAC uses Segment
Anything (SAM) for object-centric decomposition and YOLO-World to ground
segments semantically via text prompts. It includes a novel transformer-based
architecture that supports a dynamic number of segments at each time step and
effectively learns which segments to focus on using online RL, without using
human labels. By evaluating SegDAC over a challenging visual generalization
benchmark using Maniskill3, which covers diverse manipulation tasks under
strong visual perturbations, we demonstrate that SegDAC achieves significantly
better visual generalization, doubling prior performance on the hardest setting
and matching or surpassing prior methods in sample efficiency across all
evaluated tasks.

</details>


### [77] [Lung-DDPM+: Efficient Thoracic CT Image Synthesis using Diffusion Probabilistic Model](https://arxiv.org/abs/2508.09327)
*Yifan Jiang,Ahmad Shariftabrizi,Venkata SK. Manem*

Main category: cs.CV

TL;DR: Lung-DDPM+是一种改进的生成模型，通过结合语义布局和加速技术，显著提升了肺结节CT图像的生成效率和精度。


<details>
  <summary>Details</summary>
Motivation: 现有生成模型在肺结节诊断中存在效率低和解剖学不精确的问题，限制了临床应用。

Method: 提出Lung-DDPM+，一种基于语义布局引导和加速技术的去噪扩散概率模型（DDPM）。

Result: 在LIDC-IDRI数据集上，Lung-DDPM+实现了8倍FLOPs减少、6.8倍GPU内存降低和14倍采样速度提升，同时保持高质量样本生成。

Conclusion: Lung-DDPM+能高效生成高质量肺结节CT图像，具有广泛临床应用潜力。

Abstract: Generative artificial intelligence (AI) has been playing an important role in
various domains. Leveraging its high capability to generate high-fidelity and
diverse synthetic data, generative AI is widely applied in diagnostic tasks,
such as lung cancer diagnosis using computed tomography (CT). However, existing
generative models for lung cancer diagnosis suffer from low efficiency and
anatomical imprecision, which limit their clinical applicability. To address
these drawbacks, we propose Lung-DDPM+, an improved version of our previous
model, Lung-DDPM. This novel approach is a denoising diffusion probabilistic
model (DDPM) guided by nodule semantic layouts and accelerated by a pulmonary
DPM-solver, enabling the method to focus on lesion areas while achieving a
better trade-off between sampling efficiency and quality. Evaluation results on
the public LIDC-IDRI dataset suggest that the proposed method achieves
8$\times$ fewer FLOPs (floating point operations per second), 6.8$\times$ lower
GPU memory consumption, and 14$\times$ faster sampling compared to Lung-DDPM.
Moreover, it maintains comparable sample quality to both Lung-DDPM and other
state-of-the-art (SOTA) generative models in two downstream segmentation tasks.
We also conducted a Visual Turing Test by an experienced radiologist, showing
the advanced quality and fidelity of synthetic samples generated by the
proposed method. These experimental results demonstrate that Lung-DDPM+ can
effectively generate high-quality thoracic CT images with lung nodules,
highlighting its potential for broader applications, such as general tumor
synthesis and lesion generation in medical imaging. The code and pretrained
models are available at https://github.com/Manem-Lab/Lung-DDPM-PLUS.

</details>


### [78] [UltraLight Med-Vision Mamba for Classification of Neoplastic Progression in Tubular Adenomas](https://arxiv.org/abs/2508.09339)
*Aqsa Sultana,Nordin Abouzahra,Ahmed Rahu,Brian Shula,Brandon Combs,Derrick Forchetti,Theus Aspiras,Vijayan K. Asari*

Main category: cs.CV

TL;DR: Ultralight Med-Vision Mamba模型通过先进的深度学习技术，提高了结肠镜检查中息肉分类和分层的准确性，优化了风险评估和个性化监测方案。


<details>
  <summary>Details</summary>
Motivation: 结肠镜检查中准确识别癌前息肉对降低结直肠癌风险至关重要，但现有方法在长程依赖和图像泛化方面存在不足。

Method: 采用基于状态空间模型（SSM）的Ultralight Med-Vision Mamba，擅长建模长程和短程依赖关系，并具有高效的图像泛化能力。

Result: 该模型在计算速度和可扩展性上表现优异，适合实时临床部署。

Conclusion: Ultralight Med-Vision Mamba是一种有前景的工具，可提升结肠镜检查的效率和准确性。

Abstract: Identification of precancerous polyps during routine colonoscopy screenings
is vital for their excision, lowering the risk of developing colorectal cancer.
Advanced deep learning algorithms enable precise adenoma classification and
stratification, improving risk assessment accuracy and enabling personalized
surveillance protocols that optimize patient outcomes. Ultralight Med-Vision
Mamba, a state-space based model (SSM), has excelled in modeling long- and
short-range dependencies and image generalization, critical factors for
analyzing whole slide images. Furthermore, Ultralight Med-Vision Mamba's
efficient architecture offers advantages in both computational speed and
scalability, making it a promising tool for real-time clinical deployment.

</details>


### [79] [Blink-to-code: real-time Morse code communication via eye blink detection and classification](https://arxiv.org/abs/2508.09344)
*Anushka Bhatt*

Main category: cs.CV

TL;DR: 提出了一种实时系统，将眨眼动作转换为摩尔斯电码，帮助严重运动障碍者进行交流。


<details>
  <summary>Details</summary>
Motivation: 为严重运动障碍者提供低成本、可行的辅助交流方法。

Method: 使用标准摄像头和计算机视觉技术检测眨眼动作，并将其分类为短（点）或长（划），再解码为字母数字字符。

Result: 实验显示解码准确率为62%，响应时间为18-20秒。

Conclusion: 该系统是一种可行的低成本辅助交流方法。

Abstract: This study proposes a real-time system that translates voluntary eye blinks
into Morse code, enabling communication for individuals with severe motor
impairments. Using a standard webcam and computer vision, the system detects
and classifies blinks as short (dot) or long (dash), then decodes them into
alphanumeric characters. Experiments with five participants show 62% decoding
accuracy and 18-20 seconds response times, demonstrating a viable, low-cost
assistive communication method.

</details>


### [80] [FusionEnsemble-Net: An Attention-Based Ensemble of Spatiotemporal Networks for Multimodal Sign Language Recognition](https://arxiv.org/abs/2508.09362)
*Md. Milon Islam,Md Rezwanul Haque,S M Taslim Uddin Raju,Fakhri Karray*

Main category: cs.CV

TL;DR: FusionEnsemble-Net是一种基于注意力的时空网络集成方法，通过动态融合视觉和运动数据，显著提升了手语识别的准确性。


<details>
  <summary>Details</summary>
Motivation: 医疗沟通中的手语识别面临复杂多模态手势的挑战，需要高精度框架。

Method: 提出FusionEnsemble-Net，同步处理RGB视频和雷达数据，通过四种时空网络和注意力融合模块，最终集成分类。

Result: 在意大利手语数据集上达到99.44%的测试准确率，优于现有方法。

Conclusion: 基于注意力融合的多样化时空网络集成，为复杂多模态手势识别提供了鲁棒且准确的框架。

Abstract: Accurate recognition of sign language in healthcare communication poses a
significant challenge, requiring frameworks that can accurately interpret
complex multimodal gestures. To deal with this, we propose FusionEnsemble-Net,
a novel attention-based ensemble of spatiotemporal networks that dynamically
fuses visual and motion data to enhance recognition accuracy. The proposed
approach processes RGB video and range Doppler map radar modalities
synchronously through four different spatiotemporal networks. For each network,
features from both modalities are continuously fused using an attention-based
fusion module before being fed into an ensemble of classifiers. Finally, the
outputs of these four different fused channels are combined in an ensemble
classification head, thereby enhancing the model's robustness. Experiments
demonstrate that FusionEnsemble-Net outperforms state-of-the-art approaches
with a test accuracy of 99.44% on the large-scale MultiMeDaLIS dataset for
Italian Sign Language. Our findings indicate that an ensemble of diverse
spatiotemporal networks, unified by attention-based fusion, yields a robust and
accurate framework for complex, multimodal isolated gesture recognition tasks.
The source code is available at:
https://github.com/rezwanh001/Multimodal-Isolated-Italian-Sign-Language-Recognition.

</details>


### [81] [A Signer-Invariant Conformer and Multi-Scale Fusion Transformer for Continuous Sign Language Recognition](https://arxiv.org/abs/2508.09372)
*Md Rezwanul Haque,Md. Milon Islam,S M Taslim Uddin Raju,Fakhri Karray*

Main category: cs.CV

TL;DR: 论文提出了一种双架构框架，分别针对手语识别中的独立手语者（SI）和未见句子（US）问题，通过Signer-Invariant Conformer和Multi-Scale Fusion Transformer提升性能。


<details>
  <summary>Details</summary>
Motivation: 解决连续手语识别（CSLR）中的手语者独立性和新句子结构泛化问题。

Method: 使用Signer-Invariant Conformer（结合卷积与多头自注意力）处理SI问题；设计Multi-Scale Fusion Transformer（双路径时间编码器）处理US问题。

Result: 在Isharah-1000数据集上，SI任务WER降至13.07%（提升13.53%），US任务WER为47.78%，均优于现有方法。

Conclusion: 任务特定网络设计显著提升CSLR性能，为后续研究奠定新基准。

Abstract: Continuous Sign Language Recognition (CSLR) faces multiple challenges,
including significant inter-signer variability and poor generalization to novel
sentence structures. Traditional solutions frequently fail to handle these
issues efficiently. For overcoming these constraints, we propose a
dual-architecture framework. For the Signer-Independent (SI) challenge, we
propose a Signer-Invariant Conformer that combines convolutions with multi-head
self-attention to learn robust, signer-agnostic representations from pose-based
skeletal keypoints. For the Unseen-Sentences (US) task, we designed a
Multi-Scale Fusion Transformer with a novel dual-path temporal encoder that
captures both fine-grained posture dynamics, enabling the model's ability to
comprehend novel grammatical compositions. Experiments on the challenging
Isharah-1000 dataset establish a new standard for both CSLR benchmarks. The
proposed conformer architecture achieves a Word Error Rate (WER) of 13.07% on
the SI challenge, a reduction of 13.53% from the state-of-the-art. On the US
task, the transformer model scores a WER of 47.78%, surpassing previous work.
In the SignEval 2025 CSLR challenge, our team placed 2nd in the US task and 4th
in the SI task, demonstrating the performance of these models. The findings
validate our key hypothesis: that developing task-specific networks designed
for the particular challenges of CSLR leads to considerable performance
improvements and establishes a new baseline for further research. The source
code is available at: https://github.com/rezwanh001/MSLR-Pose86K-CSLR-Isharah.

</details>


### [82] [What Can We Learn from Inter-Annotator Variability in Skin Lesion Segmentation?](https://arxiv.org/abs/2508.09381)
*Kumar Abhishek,Jeremy Kawahara,Ghassan Hamarneh*

Main category: cs.CV

TL;DR: 论文研究了医学图像分割中标注者间的变异性，尤其是皮肤病变的恶性程度与标注一致性的关系，并提出了一种多任务学习方法。


<details>
  <summary>Details</summary>
Motivation: 医学图像分割中标注者的变异性（如边界模糊、标注偏好等）影响结果准确性，尤其是恶性病变更易引发分歧。

Method: 构建了IMA++数据集，研究标注者、恶性程度等因素对一致性的影响，并利用多任务学习将一致性作为软特征。

Result: 恶性程度与标注一致性显著相关（p<0.001），一致性可从图像预测（MAE=0.108），多任务学习提升4.2%的平衡准确率。

Conclusion: 标注一致性可作为临床特征提升模型性能，方法在多个数据集上验证有效。

Abstract: Medical image segmentation exhibits intra- and inter-annotator variability
due to ambiguous object boundaries, annotator preferences, expertise, and
tools, among other factors. Lesions with ambiguous boundaries, e.g., spiculated
or infiltrative nodules, or irregular borders per the ABCD rule, are
particularly prone to disagreement and are often associated with malignancy. In
this work, we curate IMA++, the largest multi-annotator skin lesion
segmentation dataset, on which we conduct an in-depth study of variability due
to annotator, malignancy, tool, and skill factors. We find a statistically
significant (p<0.001) association between inter-annotator agreement (IAA),
measured using Dice, and the malignancy of skin lesions. We further show that
IAA can be accurately predicted directly from dermoscopic images, achieving a
mean absolute error of 0.108. Finally, we leverage this association by
utilizing IAA as a "soft" clinical feature within a multi-task learning
objective, yielding a 4.2% improvement in balanced accuracy averaged across
multiple model architectures and across IMA++ and four public dermoscopic
datasets. The code is available at https://github.com/sfu-mial/skin-IAV.

</details>


### [83] [X-UniMotion: Animating Human Images with Expressive, Unified and Identity-Agnostic Motion Latents](https://arxiv.org/abs/2508.09383)
*Guoxian Song,Hongyi Xu,Xiaochen Zhao,You Xie,Tianpei Gu,Zenan Li,Chenxu Zhang,Linjie Luo*

Main category: cs.CV

TL;DR: X-UniMotion提出了一种统一的隐式潜在表示方法，用于捕捉全身人体运动（包括面部表情、身体姿势和手势），并通过自监督框架实现高保真跨身份运动迁移。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖显式骨骼姿势和启发式跨身份调整，限制了表达性和通用性。X-UniMotion旨在通过隐式潜在表示解决这一问题。

Method: 采用自监督端到端框架，学习运动编码器和潜在表示，结合DiT生成模型，并通过2D/3D增强和辅助解码器优化运动嵌入。

Result: 实验表明，X-UniMotion在运动保真度和身份保持上优于现有方法，生成高表达性动画。

Conclusion: X-UniMotion为跨身份运动迁移提供了一种高效且表达性强的解决方案。

Abstract: We present X-UniMotion, a unified and expressive implicit latent
representation for whole-body human motion, encompassing facial expressions,
body poses, and hand gestures. Unlike prior motion transfer methods that rely
on explicit skeletal poses and heuristic cross-identity adjustments, our
approach encodes multi-granular motion directly from a single image into a
compact set of four disentangled latent tokens -- one for facial expression,
one for body pose, and one for each hand. These motion latents are both highly
expressive and identity-agnostic, enabling high-fidelity, detailed
cross-identity motion transfer across subjects with diverse identities, poses,
and spatial configurations. To achieve this, we introduce a self-supervised,
end-to-end framework that jointly learns the motion encoder and latent
representation alongside a DiT-based video generative model, trained on
large-scale, diverse human motion datasets. Motion-identity disentanglement is
enforced via 2D spatial and color augmentations, as well as synthetic 3D
renderings of cross-identity subject pairs under shared poses. Furthermore, we
guide motion token learning with auxiliary decoders that promote fine-grained,
semantically aligned, and depth-aware motion embeddings. Extensive experiments
show that X-UniMotion outperforms state-of-the-art methods, producing highly
expressive animations with superior motion fidelity and identity preservation.

</details>


### [84] [Distilling LLM Prior to Flow Model for Generalizable Agent's Imagination in Object Goal Navigation](https://arxiv.org/abs/2508.09423)
*Badi Li,Ren-jie Lu,Yu Zhou,Jingke Meng,Wei-shi Zheng*

Main category: cs.CV

TL;DR: GOAL是一种基于生成流的框架，通过结合LLM增强的全场景语义地图，建模室内环境的语义分布，显著提升了ObjectNav任务的性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖确定性模型完成语义地图，忽略了室内布局的不确定性，限制了泛化能力。

Method: 利用LLM推断空间先验，编码为二维高斯场并注入目标地图，通过生成流模型实现更通用的语义补全。

Result: 在MP3D和Gibson上达到SOTA性能，并在HM3D上表现出强泛化能力。

Conclusion: GOAL通过生成流模型和LLM先验的结合，显著提升了ObjectNav任务的性能和泛化能力。

Abstract: The Object Goal Navigation (ObjectNav) task challenges agents to locate a
specified object in an unseen environment by imagining unobserved regions of
the scene. Prior approaches rely on deterministic and discriminative models to
complete semantic maps, overlooking the inherent uncertainty in indoor layouts
and limiting their ability to generalize to unseen environments. In this work,
we propose GOAL, a generative flow-based framework that models the semantic
distribution of indoor environments by bridging observed regions with
LLM-enriched full-scene semantic maps. During training, spatial priors inferred
from large language models (LLMs) are encoded as two-dimensional Gaussian
fields and injected into target maps, distilling rich contextual knowledge into
the flow model and enabling more generalizable completions. Extensive
experiments demonstrate that GOAL achieves state-of-the-art performance on MP3D
and Gibson, and shows strong generalization in transfer settings to HM3D. Codes
and pretrained models are available at https://github.com/Badi-Li/GOAL.

</details>


### [85] [DenoDet V2: Phase-Amplitude Cross Denoising for SAR Object Detection](https://arxiv.org/abs/2508.09392)
*Kang Ni,Minrui Zou,Yuxuan Li,Xiang Li,Kehua Guo,Ming-Ming Cheng,Yimian Dai*

Main category: cs.CV

TL;DR: DenoDet V2提出了一种基于变换域特征解构和调制的注意力架构，通过振幅和相位信息的互补性实现去噪，性能优于DenoDet V1。


<details>
  <summary>Details</summary>
Motivation: SAR目标检测中相干噪声的普遍影响是主要挑战，现有方法多依赖空间域特征分析或增强。

Method: 设计了基于变换域的注意力架构，利用振幅和相位信息的互补性进行互调制。

Result: 在多个SAR数据集上表现优异，SARDet-100K上性能提升0.8%，模型复杂度减半。

Conclusion: DenoDet V2通过变换域特征调制实现了高效的SAR目标检测和去噪。

Abstract: One of the primary challenges in Synthetic Aperture Radar (SAR) object
detection lies in the pervasive influence of coherent noise. As a common
practice, most existing methods, whether handcrafted approaches or deep
learning-based methods, employ the analysis or enhancement of object
spatial-domain characteristics to achieve implicit denoising. In this paper, we
propose DenoDet V2, which explores a completely novel and different perspective
to deconstruct and modulate the features in the transform domain via a
carefully designed attention architecture. Compared to DenoDet V1, DenoDet V2
is a major advancement that exploits the complementary nature of amplitude and
phase information through a band-wise mutual modulation mechanism, which
enables a reciprocal enhancement between phase and amplitude spectra. Extensive
experiments on various SAR datasets demonstrate the state-of-the-art
performance of DenoDet V2. Notably, DenoDet V2 achieves a significant 0.8\%
improvement on SARDet-100K dataset compared to DenoDet V1, while reducing the
model complexity by half. The code is available at
https://github.com/GrokCV/GrokSAR.

</details>


### [86] [WeatherPrompt: Multi-modality Representation Learning for All-Weather Drone Visual Geo-Localization](https://arxiv.org/abs/2508.09560)
*Jiahao Wen,Hang Yu,Zhedong Zheng*

Main category: cs.CV

TL;DR: WeatherPrompt提出了一种多模态学习框架，通过融合图像嵌入与文本上下文，建立天气不变的表示，解决了无人机视觉地理定位在天气扰动下的性能下降问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法在天气扰动（如雨、雾）下性能显著下降，主要受限于对有限天气类别的依赖以及场景-天气特征解耦不足。

Method: 提出了Training-free Weather Reasoning机制和动态门控多模态框架，通过文本嵌入自适应调整视觉特征权重，并结合跨模态目标优化。

Result: 在多种天气条件下，WeatherPrompt的召回率优于现有方法，Recall@1在夜间条件下提升13.37%，在雾雪条件下提升18.69%。

Conclusion: WeatherPrompt通过多模态融合和动态门控机制，显著提升了无人机在复杂天气下的地理定位性能。

Abstract: Visual geo-localization for drones faces critical degradation under weather
perturbations, \eg, rain and fog, where existing methods struggle with two
inherent limitations: 1) Heavy reliance on limited weather categories that
constrain generalization, and 2) Suboptimal disentanglement of entangled
scene-weather features through pseudo weather categories. We present
WeatherPrompt, a multi-modality learning paradigm that establishes
weather-invariant representations through fusing the image embedding with the
text context. Our framework introduces two key contributions: First, a
Training-free Weather Reasoning mechanism that employs off-the-shelf large
multi-modality models to synthesize multi-weather textual descriptions through
human-like reasoning. It improves the scalability to unseen or complex weather,
and could reflect different weather strength. Second, to better disentangle the
scene and weather feature, we propose a multi-modality framework with the
dynamic gating mechanism driven by the text embedding to adaptively reweight
and fuse visual features across modalities. The framework is further optimized
by the cross-modal objectives, including image-text contrastive learning and
image-text matching, which maps the same scene with different weather
conditions closer in the respresentation space. Extensive experiments validate
that, under diverse weather conditions, our method achieves competitive recall
rates compared to state-of-the-art drone geo-localization methods. Notably, it
improves Recall@1 by +13.37\% under night conditions and by 18.69\% under fog
and snow conditions.

</details>


### [87] [Skyshield: Event-Driven Submillimetre Thin Obstacle Detection for Drone Flight Safety](https://arxiv.org/abs/2508.09397)
*Zhengli Zhang,Xinyu Luo,Yuchen Sun,Wenhua Ding,Dongyu Huang,Xinlei Chen*

Main category: cs.CV

TL;DR: SkyShield是一个事件驱动的端到端框架，用于检测无人机在复杂环境中难以感知的亚毫米级障碍物（如钢丝、风筝线）。


<details>
  <summary>Details</summary>
Motivation: 传统传感器（如RGB相机、LiDAR、深度相机）难以检测亚毫米级障碍物，这对无人机操作构成重大威胁。

Method: 利用事件流中薄障碍物的独特特征，采用轻量级U-Net架构和创新的Dice-Contour正则化损失函数。

Result: 实验结果显示，该方法平均F1分数为0.7088，延迟低至21.2毫秒，适合边缘和移动平台部署。

Conclusion: SkyShield为无人机在复杂环境中的安全操作提供了高效解决方案。

Abstract: Drones operating in complex environments face a significant threat from thin
obstacles, such as steel wires and kite strings at the submillimeter level,
which are notoriously difficult for conventional sensors like RGB cameras,
LiDAR, and depth cameras to detect. This paper introduces SkyShield, an
event-driven, end-to-end framework designed for the perception of submillimeter
scale obstacles. Drawing upon the unique features that thin obstacles present
in the event stream, our method employs a lightweight U-Net architecture and an
innovative Dice-Contour Regularization Loss to ensure precise detection.
Experimental results demonstrate that our event-based approach achieves mean F1
Score of 0.7088 with a low latency of 21.2 ms, making it ideal for deployment
on edge and mobile platforms.

</details>


### [88] [Plane Detection and Ranking via Model Information Optimization](https://arxiv.org/abs/2508.09625)
*Daoxin Zhong,Jun Li,Meng Yee Michael Chuah*

Main category: cs.CV

TL;DR: 提出了一种基于模型信息优化的平面检测框架，解决了RANSAC方法在复杂场景中易产生误检的问题。


<details>
  <summary>Details</summary>
Motivation: RANSAC方法在平面检测中因阈值模糊易产生误检，尤其在复杂场景中表现不佳。

Method: 将深度数据视为离散随机变量，通过随机子采样生成候选平面模型，结合传感器物理和噪声模型计算信息量，选择信息量最小的模型作为真实平面。

Result: 实验表明，该方法比Open3D RANSAC平面分割更准确，并通过神经网络分割加速算法。

Conclusion: 提出的信息优化框架能有效减少误检，提升平面检测的准确性。

Abstract: Plane detection from depth images is a crucial subtask with broad robotic
applications, often accomplished by iterative methods such as Random Sample
Consensus (RANSAC). While RANSAC is a robust strategy with strong probabilistic
guarantees, the ambiguity of its inlier threshold criterion makes it
susceptible to false positive plane detections. This issue is particularly
prevalent in complex real-world scenes, where the true number of planes is
unknown and multiple planes coexist. In this paper, we aim to address this
limitation by proposing a generalised framework for plane detection based on
model information optimization. Building on previous works, we treat the
observed depth readings as discrete random variables, with their probability
distributions constrained by the ground truth planes. Various models containing
different candidate plane constraints are then generated through repeated
random sub-sampling to explain our observations. By incorporating the physics
and noise model of the depth sensor, we can calculate the information for each
model, and the model with the least information is accepted as the most likely
ground truth. This information optimization process serves as an objective
mechanism for determining the true number of planes and preventing false
positive detections. Additionally, the quality of each detected plane can be
ranked by summing the information reduction of inlier points for each plane. We
validate these properties through experiments with synthetic data and find that
our algorithm estimates plane parameters more accurately compared to the
default Open3D RANSAC plane segmentation. Furthermore, we accelerate our
algorithm by partitioning the depth map using neural network segmentation,
which enhances its ability to generate more realistic plane parameters in
real-world data.

</details>


### [89] [Autonomous AI Bird Feeder for Backyard Biodiversity Monitoring](https://arxiv.org/abs/2508.09398)
*El Mustapha Mansouri*

Main category: cs.CV

TL;DR: 低成本、本地化的比利时城市花园鸟类监测系统，使用运动触发摄像头和本地服务器处理，无需GPU，保护隐私。


<details>
  <summary>Details</summary>
Motivation: 为公民科学提供低成本、隐私保护的鸟类监测解决方案，避免云服务费用。

Method: 使用运动触发IP摄像头捕捉视频，Detectron2定位鸟类，EfficientNet-B3分类40种比利时鸟类。

Result: 分类器验证准确率约99.5%，实际野外准确率约88%，适合家庭生物多样性记录。

Conclusion: 系统可行，适合家庭级公民科学项目，兼顾隐私和成本。

Abstract: This paper presents a low cost, on premise system for autonomous backyard
bird monitoring in Belgian urban gardens. A motion triggered IP camera uploads
short clips via FTP to a local server, where frames are sampled and birds are
localized with Detectron2; cropped regions are then classified by an
EfficientNet-B3 model fine tuned on a 40-species Belgian subset derived from a
larger Kaggle corpus. All processing runs on commodity hardware without a
discrete GPU, preserving privacy and avoiding cloud fees. The physical feeder
uses small entry ports (30 mm) to exclude pigeons and reduce nuisance triggers.
Detector-guided cropping improves classification accuracy over raw-frame
classification. The classifier attains high validation performance on the
curated subset (about 99.5 percent) and delivers practical field accuracy
(top-1 about 88 percent) on held-out species, demonstrating feasibility for
citizen-science-grade biodiversity logging at home.

</details>


### [90] [Surg-InvNeRF: Invertible NeRF for 3D tracking and reconstruction in surgical vision](https://arxiv.org/abs/2508.09681)
*Gerardo Loza,Junlei Hu,Dominic Jones,Sharib Ali,Pietro Valdastri*

Main category: cs.CV

TL;DR: 提出了一种基于NeRF的新型测试时优化（TTO）方法，用于长期3D点跟踪，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有方法在点跟踪中难以实现一致运动或仅限于2D运动，因此提出一种结合NeRF架构的TTO方法。

Method: 使用可逆神经辐射场（InvNeRF）架构参数化函数，支持2D和3D跟踪，并结合多尺度HexPlanes和高效像素采样算法。

Result: 在STIR和SCARE数据集上表现优异，2D跟踪精度提升近50%，3D跟踪首次实现TTO方法。

Conclusion: 该方法在2D和3D点跟踪中均表现出色，结合了可变形NeRF的优势。

Abstract: We proposed a novel test-time optimisation (TTO) approach framed by a
NeRF-based architecture for long-term 3D point tracking. Most current methods
in point tracking struggle to obtain consistent motion or are limited to 2D
motion. TTO approaches frame the solution for long-term tracking as optimising
a function that aggregates correspondences from other specialised
state-of-the-art methods. Unlike the state-of-the-art on TTO, we propose
parametrising such a function with our new invertible Neural Radiance Field
(InvNeRF) architecture to perform both 2D and 3D tracking in surgical
scenarios. Our approach allows us to exploit the advantages of a
rendering-based approach by supervising the reprojection of pixel
correspondences. It adapts strategies from recent rendering-based methods to
obtain a bidirectional deformable-canonical mapping, to efficiently handle a
defined workspace, and to guide the rays' density. It also presents our
multi-scale HexPlanes for fast inference and a new algorithm for efficient
pixel sampling and convergence criteria. We present results in the STIR and
SCARE datasets, for evaluating point tracking and testing the integration of
kinematic data in our pipeline, respectively. In 2D point tracking, our
approach surpasses the precision and accuracy of the TTO state-of-the-art
methods by nearly 50% on average precision, while competing with other
approaches. In 3D point tracking, this is the first TTO approach, surpassing
feed-forward methods while incorporating the benefits of a deformable
NeRF-based reconstruction.

</details>


### [91] [Predictive Uncertainty for Runtime Assurance of a Real-Time Computer Vision-Based Landing System](https://arxiv.org/abs/2508.09732)
*Romeo Valentin,Sydney M. Katz,Artur B. Carneiro,Don Walker,Mykel J. Kochenderfer*

Main category: cs.CV

TL;DR: 该论文提出了一种基于视觉的飞机姿态估计方法，通过三个关键创新点提高了精度和不确定性估计，适用于航空安全应用。


<details>
  <summary>Details</summary>
Motivation: 尽管数据驱动的计算机视觉在航空导航中取得进展，但确保系统满足航空安全要求仍具挑战性。本文旨在开发一种可认证的视觉姿态估计方法。

Method: 方法包括：1）基于空间Soft Argmax的高效神经网络架构；2）产生校准预测不确定性的损失函数；3）改进的RAIM用于运行时故障检测。

Result: 在跑道图像数据集上验证，模型在精度和不确定性校准方面优于基线，支持亚像素级精度和故障检测。

Conclusion: 该研究为航空安全应用中的视觉姿态估计提供了实用且可认证的解决方案。

Abstract: Recent advances in data-driven computer vision have enabled robust autonomous
navigation capabilities for civil aviation, including automated landing and
runway detection. However, ensuring that these systems meet the robustness and
safety requirements for aviation applications remains a major challenge. In
this work, we present a practical vision-based pipeline for aircraft pose
estimation from runway images that represents a step toward the ability to
certify these systems for use in safety-critical aviation applications. Our
approach features three key innovations: (i) an efficient, flexible neural
architecture based on a spatial Soft Argmax operator for probabilistic keypoint
regression, supporting diverse vision backbones with real-time inference; (ii)
a principled loss function producing calibrated predictive uncertainties, which
are evaluated via sharpness and calibration metrics; and (iii) an adaptation of
Residual-based Receiver Autonomous Integrity Monitoring (RAIM), enabling
runtime detection and rejection of faulty model outputs. We implement and
evaluate our pose estimation pipeline on a dataset of runway images. We show
that our model outperforms baseline architectures in terms of accuracy while
also producing well-calibrated uncertainty estimates with sub-pixel precision
that can be used downstream for fault detection.

</details>


### [92] [RampNet: A Two-Stage Pipeline for Bootstrapping Curb Ramp Detection in Streetscape Images from Open Government Metadata](https://arxiv.org/abs/2508.09415)
*John S. O'Meara,Jared Hwang,Zeyu Wang,Michael Saugstad,Jon E. Froehlich*

Main category: cs.CV

TL;DR: 论文提出RampNet两阶段管道，通过自动标注生成大规模高质量路缘坡道数据集，并训练高性能检测模型。


<details>
  <summary>Details</summary>
Motivation: 路缘坡道对城市无障碍至关重要，但缺乏大规模高质量数据集限制了其检测能力。

Method: 两阶段方法：1) 利用政府数据自动标注21万张GSV全景图；2) 训练改进的ConvNeXt V2模型。

Result: 生成数据集精度94.0%，召回率92.5%；检测模型AP达0.9236，超越现有工作。

Conclusion: 研究首次提供大规模高质量路缘坡道数据集、基准和模型，显著提升检测性能。

Abstract: Curb ramps are critical for urban accessibility, but robustly detecting them
in images remains an open problem due to the lack of large-scale, high-quality
datasets. While prior work has attempted to improve data availability with
crowdsourced or manually labeled data, these efforts often fall short in either
quality or scale. In this paper, we introduce and evaluate a two-stage pipeline
called RampNet to scale curb ramp detection datasets and improve model
performance. In Stage 1, we generate a dataset of more than 210,000 annotated
Google Street View (GSV) panoramas by auto-translating government-provided curb
ramp location data to pixel coordinates in panoramic images. In Stage 2, we
train a curb ramp detection model (modified ConvNeXt V2) from the generated
dataset, achieving state-of-the-art performance. To evaluate both stages of our
pipeline, we compare to manually labeled panoramas. Our generated dataset
achieves 94.0% precision and 92.5% recall, and our detection model reaches
0.9236 AP -- far exceeding prior work. Our work contributes the first
large-scale, high-quality curb ramp detection dataset, benchmark, and model.

</details>


### [93] [TRACE: Learning 3D Gaussian Physical Dynamics from Multi-view Videos](https://arxiv.org/abs/2508.09811)
*Jinxi Li,Ziyang Song,Bo Yang*

Main category: cs.CV

TL;DR: TRACE框架通过将3D点建模为刚性粒子，直接学习其平移旋转动力学系统，无需人工标签即可建模动态3D场景的物理运动。


<details>
  <summary>Details</summary>
Motivation: 现有方法在缺乏人工标签时难以学习复杂运动物理，或需要额外标注（如物体类型或掩码）。TRACE旨在解决这一问题。

Method: 将每个3D点视为具有大小和方向的刚性粒子，直接学习其平移旋转动力学系统，并估计物理参数以控制运动。

Result: 在多个动态数据集上表现优异，尤其在未来帧外推任务中优于基线方法，并能通过物理参数聚类轻松分割物体。

Conclusion: TRACE无需人工标签即可建模复杂动态3D场景的物理运动，且具有高效分割能力。

Abstract: In this paper, we aim to model 3D scene geometry, appearance, and physical
information just from dynamic multi-view videos in the absence of any human
labels. By leveraging physics-informed losses as soft constraints or
integrating simple physics models into neural nets, existing works often fail
to learn complex motion physics, or doing so requires additional labels such as
object types or masks. We propose a new framework named TRACE to model the
motion physics of complex dynamic 3D scenes. The key novelty of our method is
that, by formulating each 3D point as a rigid particle with size and
orientation in space, we directly learn a translation rotation dynamics system
for each particle, explicitly estimating a complete set of physical parameters
to govern the particle's motion over time. Extensive experiments on three
existing dynamic datasets and one newly created challenging synthetic datasets
demonstrate the extraordinary performance of our method over baselines in the
task of future frame extrapolation. A nice property of our framework is that
multiple objects or parts can be easily segmented just by clustering the
learned physical parameters.

</details>


### [94] [RayletDF: Raylet Distance Fields for Generalizable 3D Surface Reconstruction from Point Clouds or Gaussians](https://arxiv.org/abs/2508.09830)
*Shenxing Wei,Jinxi Li,Yafei Yang,Siyuan Zhou,Bo Yang*

Main category: cs.CV

TL;DR: 提出了一种名为RayletDF的通用方法，用于从点云或3D高斯中重建3D表面，通过射线距离场直接预测表面点。


<details>
  <summary>Details</summary>
Motivation: 现有基于坐标的方法在渲染显式表面时计算量大，需要更高效且通用的解决方案。

Method: 采用三个模块：射线特征提取器、射线距离场预测器和多射线混合器，共同提取几何特征并预测表面点。

Result: 在多个公开数据集上表现优异，具有出色的泛化能力，能在未见数据集上单次前向通过重建表面。

Conclusion: RayletDF方法高效且通用，适用于多种3D表面重建任务。

Abstract: In this paper, we present a generalizable method for 3D surface
reconstruction from raw point clouds or pre-estimated 3D Gaussians by 3DGS from
RGB images. Unlike existing coordinate-based methods which are often
computationally intensive when rendering explicit surfaces, our proposed
method, named RayletDF, introduces a new technique called raylet distance
field, which aims to directly predict surface points from query rays. Our
pipeline consists of three key modules: a raylet feature extractor, a raylet
distance field predictor, and a multi-raylet blender. These components work
together to extract fine-grained local geometric features, predict raylet
distances, and aggregate multiple predictions to reconstruct precise surface
points. We extensively evaluate our method on multiple public real-world
datasets, demonstrating superior performance in surface reconstruction from
point clouds or 3D Gaussians. Most notably, our method achieves exceptional
generalization ability, successfully recovering 3D surfaces in a single-forward
pass across unseen datasets in testing.

</details>


### [95] [What-Meets-Where: Unified Learning of Action and Contact Localization in a New Dataset](https://arxiv.org/abs/2508.09428)
*Yuxiao Wang,Yu Lei,Wolin Liang,Weiying Xue,Zhenao Wei,Nan Zhuang,Qi Liu*

Main category: cs.CV

TL;DR: 论文提出了一种新视觉任务，同时预测高级动作语义和细粒度身体接触区域，并提出了PaIR-Net框架和PaIR数据集，实验表现优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 现有方法未能同时建模动作语义及其在场景中的空间上下文，需填补这一空白。

Method: 提出PaIR-Net框架，包含CPAM、PGCS和IIM三个模块，用于识别接触相关身体部分、像素级接触分割和全局关系整合。

Result: PaIR-Net显著优于基线方法，消融实验验证了各模块的有效性。

Conclusion: PaIR-Net和PaIR数据集为同时理解动作语义和空间上下文提供了有效解决方案。

Abstract: People control their bodies to establish contact with the environment. To
comprehensively understand actions across diverse visual contexts, it is
essential to simultaneously consider \textbf{what} action is occurring and
\textbf{where} it is happening. Current methodologies, however, often
inadequately capture this duality, typically failing to jointly model both
action semantics and their spatial contextualization within scenes. To bridge
this gap, we introduce a novel vision task that simultaneously predicts
high-level action semantics and fine-grained body-part contact regions. Our
proposed framework, PaIR-Net, comprises three key components: the Contact Prior
Aware Module (CPAM) for identifying contact-relevant body parts, the
Prior-Guided Concat Segmenter (PGCS) for pixel-wise contact segmentation, and
the Interaction Inference Module (IIM) responsible for integrating global
interaction relationships. To facilitate this task, we present PaIR (Part-aware
Interaction Representation), a comprehensive dataset containing 13,979 images
that encompass 654 actions, 80 object categories, and 17 body parts.
Experimental evaluation demonstrates that PaIR-Net significantly outperforms
baseline approaches, while ablation studies confirm the efficacy of each
architectural component. The code and dataset will be released upon
publication.

</details>


### [96] [MPT: Motion Prompt Tuning for Micro-Expression Recognition](https://arxiv.org/abs/2508.09446)
*Jiateng Liu,Hengcan Shi,Feng Chen,Zhiwen Shao,Yaonan Wang,Jianfei Cai,Wenming Zheng*

Main category: cs.CV

TL;DR: 论文提出了一种名为Motion Prompt Tuning (MPT)的新方法，通过微调大型预训练模型（LMs）来适应微表情识别（MER），解决了现有方法难以捕捉细微面部运动的局限性。


<details>
  <summary>Details</summary>
Motivation: 微表情识别在医疗诊断、测谎等领域有广泛应用，但由于标注困难，数据集样本稀缺，且现有大型预训练模型无法有效捕捉细微面部运动。

Method: MPT方法包括运动提示生成（运动放大和高斯标记化）和组适配器设计，以增强模型对微表情的捕捉能力。

Result: 在三个广泛使用的MER数据集上的实验表明，MPT方法优于现有最先进方法。

Conclusion: MPT方法通过运动提示调优和组适配器设计，显著提升了微表情识别的性能。

Abstract: Micro-expression recognition (MER) is crucial in the affective computing
field due to its wide application in medical diagnosis, lie detection, and
criminal investigation. Despite its significance, obtaining micro-expression
(ME) annotations is challenging due to the expertise required from
psychological professionals. Consequently, ME datasets often suffer from a
scarcity of training samples, severely constraining the learning of MER models.
While current large pre-training models (LMs) offer general and discriminative
representations, their direct application to MER is hindered by an inability to
capture transitory and subtle facial movements-essential elements for effective
MER. This paper introduces Motion Prompt Tuning (MPT) as a novel approach to
adapting LMs for MER, representing a pioneering method for subtle motion prompt
tuning. Particularly, we introduce motion prompt generation, including motion
magnification and Gaussian tokenization, to extract subtle motions as prompts
for LMs. Additionally, a group adapter is carefully designed and inserted into
the LM to enhance it in the target MER domain, facilitating a more nuanced
distinction of ME representation. Furthermore, extensive experiments conducted
on three widely used MER datasets demonstrate that our proposed MPT
consistently surpasses state-of-the-art approaches and verifies its
effectiveness.

</details>


### [97] [RASR: Retrieval-Augmented Super Resolution for Practical Reference-based Image Restoration](https://arxiv.org/abs/2508.09449)
*Jiaqi Yan,Shuning Xu,Xiangyu Chen,Dell Zhang,Jie Tang,Gangshan Wu,Jie Liu*

Main category: cs.CV

TL;DR: 论文提出了一种新的参考超分辨率方法RASR，通过自动检索相关高分辨率参考图像，解决了传统RefSR依赖手动配对的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有RefSR方法依赖手动配对的参考图像，限制了实际应用。RASR旨在通过自动检索参考图像提升实用性和灵活性。

Method: 提出RASR框架，结合语义检索器和扩散生成器，自动检索并利用相关参考图像进行超分辨率重建。

Result: 在RASR-Flickr30数据集上，RASRNet比SISR基线提升0.38 dB PSNR和-0.0131 LPIPS，生成更真实的纹理。

Conclusion: 检索增强是连接学术研究与实际应用的有效方向，RASR展示了其潜力。

Abstract: Reference-based Super Resolution (RefSR) improves upon Single Image Super
Resolution (SISR) by leveraging high-quality reference images to enhance
texture fidelity and visual realism. However, a critical limitation of existing
RefSR approaches is their reliance on manually curated target-reference image
pairs, which severely constrains their practicality in real-world scenarios. To
overcome this, we introduce Retrieval-Augmented Super Resolution (RASR), a new
and practical RefSR paradigm that automatically retrieves semantically relevant
high-resolution images from a reference database given only a low-quality
input. This enables scalable and flexible RefSR in realistic use cases, such as
enhancing mobile photos taken in environments like zoos or museums, where
category-specific reference data (e.g., animals, artworks) can be readily
collected or pre-curated. To facilitate research in this direction, we
construct RASR-Flickr30, the first benchmark dataset designed for RASR. Unlike
prior datasets with fixed target-reference pairs, RASR-Flickr30 provides
per-category reference databases to support open-world retrieval. We further
propose RASRNet, a strong baseline that combines a semantic reference retriever
with a diffusion-based RefSR generator. It retrieves relevant references based
on semantic similarity and employs a diffusion-based generator enhanced with
semantic conditioning. Experiments on RASR-Flickr30 demonstrate that RASRNet
consistently improves over SISR baselines, achieving +0.38 dB PSNR and -0.0131
LPIPS, while generating more realistic textures. These findings highlight
retrieval augmentation as a promising direction to bridge the gap between
academic RefSR research and real-world applicability.

</details>


### [98] [HyperKD: Distilling Cross-Spectral Knowledge in Masked Autoencoders via Inverse Domain Shift with Spatial-Aware Masking and Specialized Loss](https://arxiv.org/abs/2508.09453)
*Abdul Matin,Tanjim Bin Faruk,Shrideep Pallickara,Sangmi Lee Pallickara*

Main category: cs.CV

TL;DR: HyperKD是一种新颖的知识蒸馏框架，通过逆向知识转移解决高光谱遥感中基础模型应用的挑战。


<details>
  <summary>Details</summary>
Motivation: 高光谱遥感数据与基础模型之间存在光谱差异和数据稀缺问题，限制了基础模型的直接应用。

Method: HyperKD通过特征对齐、空间特征引导掩码和增强损失函数，将Prithvi基础模型的知识蒸馏到EnMAP高光谱图像的学生模型中。

Result: 实验表明，HyperKD显著提升了表示学习效果，改善了重建保真度，并在下游任务中表现更稳健。

Conclusion: HyperKD展示了知识蒸馏框架在高光谱遥感分析中的潜力，成功弥合了光谱域差距。

Abstract: The proliferation of foundation models, pretrained on large-scale unlabeled
datasets, has emerged as an effective approach in creating adaptable and
reusable architectures that can be leveraged for various downstream tasks using
satellite observations. However, their direct application to hyperspectral
remote sensing remains challenging due to inherent spectral disparities and the
scarcity of available observations. In this work, we present HyperKD, a novel
knowledge distillation framework that enables transferring learned
representations from a teacher model into a student model for effective
development of a foundation model on hyperspectral images. Unlike typical
knowledge distillation frameworks, which use a complex teacher to guide a
simpler student, HyperKD enables an inverse form of knowledge transfer across
different types of spectral data, guided by a simpler teacher model. Building
upon a Masked Autoencoder, HyperKD distills knowledge from the Prithvi
foundational model into a student tailored for EnMAP hyperspectral imagery.
HyperKD addresses the inverse domain adaptation problem with spectral gaps by
introducing a feature-based strategy that includes spectral range-based channel
alignment, spatial feature-guided masking, and an enhanced loss function
tailored for hyperspectral images. HyperKD bridges the substantial spectral
domain gap, enabling the effective use of pretrained foundation models for
geospatial applications. Extensive experiments show that HyperKD significantly
improves representation learning in MAEs, leading to enhanced reconstruction
fidelity and more robust performance on downstream tasks such as land cover
classification, crop type identification, and soil organic carbon prediction,
underpinning the potential of knowledge distillation frameworks in remote
sensing analytics with hyperspectral imagery.

</details>


### [99] [Animate-X++: Universal Character Image Animation with Dynamic Backgrounds](https://arxiv.org/abs/2508.09454)
*Shuai Tan,Biao Gong,Zhuoxin Liu,Yan Wang,Xi Chen,Yifan Feng,Hengshuang Zhao*

Main category: cs.CV

TL;DR: Animate-X++ 是一个基于 DiT 的通用动画框架，用于生成包括拟人化角色在内的各种角色的高质量视频。通过引入 Pose Indicator 和多任务训练策略，解决了现有方法在运动建模和背景动态性上的不足。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要适用于人类角色，对拟人化角色泛化能力差，且只能生成静态背景视频，限制了视频的真实感。

Method: 提出 Animate-X++ 框架，结合 Pose Indicator（通过 CLIP 视觉特征和模拟输入增强运动建模）和多任务训练策略（联合训练动画和 TI2V 任务）。

Result: 实验表明 Animate-X++ 在通用性和动画质量上表现优越，并引入了 A2Bench 基准进行评估。

Conclusion: Animate-X++ 在角色动画和背景动态性上取得了显著进展，为行业应用提供了更通用的解决方案。

Abstract: Character image animation, which generates high-quality videos from a
reference image and target pose sequence, has seen significant progress in
recent years. However, most existing methods only apply to human figures, which
usually do not generalize well on anthropomorphic characters commonly used in
industries like gaming and entertainment. Furthermore, previous methods could
only generate videos with static backgrounds, which limits the realism of the
videos. For the first challenge, our in-depth analysis suggests to attribute
this limitation to their insufficient modeling of motion, which is unable to
comprehend the movement pattern of the driving video, thus imposing a pose
sequence rigidly onto the target character. To this end, this paper proposes
Animate-X++, a universal animation framework based on DiT for various character
types, including anthropomorphic characters. To enhance motion representation,
we introduce the Pose Indicator, which captures comprehensive motion pattern
from the driving video through both implicit and explicit manner. The former
leverages CLIP visual features of a driving video to extract its gist of
motion, like the overall movement pattern and temporal relations among motions,
while the latter strengthens the generalization of DiT by simulating possible
inputs in advance that may arise during inference. For the second challenge, we
introduce a multi-task training strategy that jointly trains the animation and
TI2V tasks. Combined with the proposed partial parameter training, this
approach achieves not only character animation but also text-driven background
dynamics, making the videos more realistic. Moreover, we introduce a new
Animated Anthropomorphic Benchmark (A2Bench) to evaluate the performance of
Animate-X++ on universal and widely applicable animation images. Extensive
experiments demonstrate the superiority and effectiveness of Animate-X++.

</details>


### [100] [IAG: Input-aware Backdoor Attack on VLMs for Visual Grounding](https://arxiv.org/abs/2508.09456)
*Junxian Li,Beining Xu,Di Zhang*

Main category: cs.CV

TL;DR: 提出了一种针对视觉语言模型的新型输入感知后门攻击方法IAG，通过自适应触发器生成器操纵模型的视觉定位行为，攻击效果显著且隐蔽。


<details>
  <summary>Details</summary>
Motivation: 探索视觉语言模型在视觉定位任务中的安全问题，尤其是后门攻击的潜在威胁。

Method: 设计输入感知后门攻击IAG，利用文本条件U-Net嵌入攻击目标的语义信息，并通过重建损失确保隐蔽性。

Result: 在InternVL-2.5-8B上ASR@0.5超过65%，且在Ferret-7B和LlaVA-1.5-7B上攻击效果显著且对干净样本影响小。

Conclusion: IAG展示了强大的攻击能力和隐蔽性，为视觉语言模型的安全研究提供了新视角。

Abstract: Vision-language models (VLMs) have shown significant advancements in tasks
such as visual grounding, where they localize specific objects in images based
on natural language queries and images. However, security issues in visual
grounding tasks for VLMs remain underexplored, especially in the context of
backdoor attacks. In this paper, we introduce a novel input-aware backdoor
attack method, IAG, designed to manipulate the grounding behavior of VLMs. This
attack forces the model to ground a specific target object in the input image,
regardless of the user's query. We propose an adaptive trigger generator that
embeds the semantic information of the attack target's description into the
original image using a text-conditional U-Net, thereby overcoming the
open-vocabulary attack challenge. To ensure the attack's stealthiness, we
utilize a reconstruction loss to minimize visual discrepancies between poisoned
and clean images. Additionally, we introduce a unified method for generating
attack data. IAG is evaluated theoretically and empirically, demonstrating its
feasibility and effectiveness. Notably, our ASR@0.5 on InternVL-2.5-8B reaches
over 65\% on various testing sets. IAG also shows promising potential on
manipulating Ferret-7B and LlaVA-1.5-7B with very little accuracy decrease on
clean samples. Extensive specific experiments, such as ablation study and
potential defense, also indicate the robustness and transferability of our
attack.

</details>


### [101] [RelayFormer: A Unified Local-Global Attention Framework for Scalable Image and Video Manipulation Localization](https://arxiv.org/abs/2508.09459)
*Wen Huang,Jiarui Yang,Tao Dai,Jiawei Li,Shaoxiong Zhan,Bin Wang,Shu-Tao Xia*

Main category: cs.CV

TL;DR: RelayFormer是一种统一的模块化架构，用于图像和视频中的视觉篡改定位，通过GLoRA机制实现高效处理。


<details>
  <summary>Details</summary>
Motivation: 现有方法在跨模态泛化和处理高分辨率或长时输入方面表现不足。

Method: 采用灵活的局部单元和GLoRA机制，结合轻量级适配模块与现有Transformer骨干网络集成。

Result: 在多个基准测试中达到最先进的定位性能。

Conclusion: RelayFormer为可扩展且模态无关的VML设立了新基准。

Abstract: Visual manipulation localization (VML) -- across both images and videos -- is
a crucial task in digital forensics that involves identifying tampered regions
in visual content. However, existing methods often lack cross-modal
generalization and struggle to handle high-resolution or long-duration inputs
efficiently.
  We propose RelayFormer, a unified and modular architecture for visual
manipulation localization across images and videos. By leveraging flexible
local units and a Global-Local Relay Attention (GLoRA) mechanism, it enables
scalable, resolution-agnostic processing with strong generalization. Our
framework integrates seamlessly with existing Transformer-based backbones, such
as ViT and SegFormer, via lightweight adaptation modules that require only
minimal architectural changes, ensuring compatibility without disrupting
pretrained representations.
  Furthermore, we design a lightweight, query-based mask decoder that supports
one-shot inference across video sequences with linear complexity. Extensive
experiments across multiple benchmarks demonstrate that our approach achieves
state-of-the-art localization performance, setting a new baseline for scalable
and modality-agnostic VML. Code is available at:
https://github.com/WenOOI/RelayFormer.

</details>


### [102] [Gen-AFFECT: Generation of Avatar Fine-grained Facial Expressions with Consistent identiTy](https://arxiv.org/abs/2508.09461)
*Hao Yu,Rupayan Mallick,Margrit Betke,Sarah Adel Bargal*

Main category: cs.CV

TL;DR: GEN-AFFECT是一个新颖的个性化头像生成框架，通过多模态扩散变换器生成表情丰富且身份一致的头像。


<details>
  <summary>Details</summary>
Motivation: 现有方法难以捕捉细粒度面部表情并保持身份一致性，GEN-AFFECT旨在解决这一问题。

Method: 利用多模态扩散变换器，结合身份-表情表示，并通过一致注意力机制在推理时共享信息。

Result: 在生成表情准确性、身份保持和一致性方面优于现有方法。

Conclusion: GEN-AFFECT为个性化头像生成提供了高效且一致的解决方案。

Abstract: Different forms of customized 2D avatars are widely used in gaming
applications, virtual communication, education, and content creation. However,
existing approaches often fail to capture fine-grained facial expressions and
struggle to preserve identity across different expressions. We propose
GEN-AFFECT, a novel framework for personalized avatar generation that generates
expressive and identity-consistent avatars with a diverse set of facial
expressions. Our framework proposes conditioning a multimodal diffusion
transformer on an extracted identity-expression representation. This enables
identity preservation and representation of a wide range of facial expressions.
GEN-AFFECT additionally employs consistent attention at inference for
information sharing across the set of generated expressions, enabling the
generation process to maintain identity consistency over the array of generated
fine-grained expressions. GEN-AFFECT demonstrates superior performance compared
to previous state-of-the-art methods on the basis of the accuracy of the
generated expressions, the preservation of the identity and the consistency of
the target identity across an array of fine-grained facial expressions.

</details>


### [103] [Event-driven Robust Fitting on Neuromorphic Hardware](https://arxiv.org/abs/2508.09466)
*Tam Ngoc-Bang Nguyen,Anh-Dzung Doan,Zhipeng Cai,Tat-Jun Chin*

Main category: cs.CV

TL;DR: 论文提出了一种基于神经形态计算的能量高效鲁棒拟合方法，使用新型脉冲神经网络在Intel Loihi 2硬件上实现，能耗仅为传统CPU方法的15%。


<details>
  <summary>Details</summary>
Motivation: 鲁棒拟合在计算机视觉中至关重要，但现有方法忽视了能源效率问题。随着AI能耗问题日益突出，研究能源高效的鲁棒拟合方法变得尤为重要。

Method: 设计了新型脉冲神经网络，结合事件驱动的模型估计方法，适应Intel Loihi 2硬件的独特架构，并通过算法策略解决硬件精度和指令集限制。

Result: 实验表明，神经形态鲁棒拟合的能耗仅为传统CPU方法的15%，且达到相同精度。

Conclusion: 神经形态计算为能源高效的鲁棒拟合提供了可行方案，未来可进一步优化硬件和算法以提升性能。

Abstract: Robust fitting of geometric models is a fundamental task in many computer
vision pipelines. Numerous innovations have been produced on the topic, from
improving the efficiency and accuracy of random sampling heuristics to
generating novel theoretical insights that underpin new approaches with
mathematical guarantees. However, one aspect of robust fitting that has
received little attention is energy efficiency. This performance metric has
become critical as high energy consumption is a growing concern for AI
adoption. In this paper, we explore energy-efficient robust fitting via the
neuromorphic computing paradigm. Specifically, we designed a novel spiking
neural network for robust fitting on real neuromorphic hardware, the Intel
Loihi 2. Enabling this are novel event-driven formulations of model estimation
that allow robust fitting to be implemented in the unique architecture of Loihi
2, and algorithmic strategies to alleviate the current limited precision and
instruction set of the hardware. Results show that our neuromorphic robust
fitting consumes only a fraction (15%) of the energy required to run the
established robust fitting algorithm on a standard CPU to equivalent accuracy.

</details>


### [104] [CitySeg: A 3D Open Vocabulary Semantic Segmentation Foundation Model in City-scale Scenarios](https://arxiv.org/abs/2508.09470)
*Jialei Xu,Zizhuang Wei,Weikang You,Linyun Li,Weijian Sun*

Main category: cs.CV

TL;DR: CitySeg是一种用于城市规模点云语义分割的基础模型，通过引入文本模态实现开放词汇分割和零样本推理，解决了数据分布不均和语义标签差异问题。


<details>
  <summary>Details</summary>
Motivation: 现有模型受限于3D数据规模小和数据集间的领域差距，导致泛化能力不足。

Method: 提出局部-全局交叉注意力网络、分层分类策略和两阶段训练方法，结合铰链损失增强特征可分性。

Result: 在九个封闭集基准测试中达到SOTA性能，首次实现不依赖视觉信息的零样本泛化。

Conclusion: CitySeg通过创新方法显著提升了城市规模点云分割的性能和泛化能力。

Abstract: Semantic segmentation of city-scale point clouds is a critical technology for
Unmanned Aerial Vehicle (UAV) perception systems, enabling the classification
of 3D points without relying on any visual information to achieve comprehensive
3D understanding. However, existing models are frequently constrained by the
limited scale of 3D data and the domain gap between datasets, which lead to
reduced generalization capability. To address these challenges, we propose
CitySeg, a foundation model for city-scale point cloud semantic segmentation
that incorporates text modality to achieve open vocabulary segmentation and
zero-shot inference. Specifically, in order to mitigate the issue of
non-uniform data distribution across multiple domains, we customize the data
preprocessing rules, and propose a local-global cross-attention network to
enhance the perception capabilities of point networks in UAV scenarios. To
resolve semantic label discrepancies across datasets, we introduce a
hierarchical classification strategy. A hierarchical graph established
according to the data annotation rules consolidates the data labels, and the
graph encoder is used to model the hierarchical relationships between
categories. In addition, we propose a two-stage training strategy and employ
hinge loss to increase the feature separability of subcategories. Experimental
results demonstrate that the proposed CitySeg achieves state-of-the-art (SOTA)
performance on nine closed-set benchmarks, significantly outperforming existing
approaches. Moreover, for the first time, CitySeg enables zero-shot
generalization in city-scale point cloud scenarios without relying on visual
information.

</details>


### [105] [Leveraging Failed Samples: A Few-Shot and Training-Free Framework for Generalized Deepfake Detection](https://arxiv.org/abs/2508.09475)
*Shibo Yao,Renshuai Tao,Xiaolong Zheng,Chao Liang,Chunjie Zhang*

Main category: cs.CV

TL;DR: 论文提出了一种无需训练的少样本深度伪造检测方法FTNet，利用少量样本显著提升检测性能。


<details>
  <summary>Details</summary>
Motivation: 现有深度伪造检测方法在未知样本上表现不佳，而现实场景中这些样本仍可用于分析，因此需将其视为少样本任务。

Method: 提出FTNet，仅需一个评估集中的伪造样本，无需训练或参数更新，通过比较测试样本与已知样本进行分类。

Result: 在29种生成模型的测试中，FTNet平均性能提升8.7%，达到新SoTA。

Conclusion: FTNet为深度伪造检测提供了新思路，利用失败样本可显著提升性能。

Abstract: Recent deepfake detection studies often treat unseen sample detection as a
``zero-shot" task, training on images generated by known models but
generalizing to unknown ones. A key real-world challenge arises when a model
performs poorly on unknown samples, yet these samples remain available for
analysis. This highlights that it should be approached as a ``few-shot" task,
where effectively utilizing a small number of samples can lead to significant
improvement. Unlike typical few-shot tasks focused on semantic understanding,
deepfake detection prioritizes image realism, which closely mirrors real-world
distributions. In this work, we propose the Few-shot Training-free Network
(FTNet) for real-world few-shot deepfake detection. Simple yet effective, FTNet
differs from traditional methods that rely on large-scale known data for
training. Instead, FTNet uses only one fake samplefrom an evaluation set,
mimicking the scenario where new samples emerge in the real world and can be
gathered for use, without any training or parameter updates. During evaluation,
each test sample is compared to the known fake and real samples, and it is
classified based on the category of the nearest sample. We conduct a
comprehensive analysis of AI-generated images from 29 different generative
models and achieve a new SoTA performance, with an average improvement of 8.7\%
compared to existing methods. This work introduces a fresh perspective on
real-world deepfake detection: when the model struggles to generalize on a
few-shot sample, leveraging the failed samples leads to better performance.

</details>


### [106] [From Large Angles to Consistent Faces: Identity-Preserving Video Generation via Mixture of Facial Experts](https://arxiv.org/abs/2508.09476)
*Yuji Wang,Moran Li,Xiaobin Hu,Ran Yi,Jiangning Zhang,Chengming Xu,Weijian Cao,Yabiao Wang,Chengjie Wang,Lizhuang Ma*

Main category: cs.CV

TL;DR: 论文提出了一种动态混合面部专家（MoFE）方法和LFA数据集，解决了视频生成模型中大角度面部身份保留的难题。


<details>
  <summary>Details</summary>
Motivation: 当前视频生成模型在大角度面部时难以保持身份一致性，主要因缺乏有效的身份特征整合机制和针对性数据集。

Method: 引入MoFE动态结合三种专家（身份、语义、细节）的特征，并开发了包含Face Constraints和Identity Consistency的数据处理流程，构建了LFA数据集。

Result: 实验表明，该方法在LFA基准测试中显著优于现有方法，提升了面部相似度、FID和CLIP语义对齐。

Conclusion: MoFE和LFA数据集有效解决了大角度面部身份保留问题，代码和数据集将开源。

Abstract: Current video generation models struggle with identity preservation under
large facial angles, primarily facing two challenges: the difficulty in
exploring an effective mechanism to integrate identity features into DiT
structure, and the lack of targeted coverage of large facial angles in existing
open-source video datasets. To address these, we present two key innovations.
First, we introduce a Mixture of Facial Experts (MoFE) that dynamically
combines complementary cues from three specialized experts, each designed to
capture distinct but mutually reinforcing aspects of facial attributes. The
identity expert captures cross-pose identity-sensitive features, the semantic
expert extracts high-level visual semantxics, and the detail expert preserves
pixel-level features (e.g., skin texture, color gradients). Furthermore, to
mitigate dataset limitations, we have tailored a data processing pipeline
centered on two key aspects: Face Constraints and Identity Consistency. Face
Constraints ensure facial angle diversity and a high proportion of facial
regions, while Identity Consistency preserves coherent person-specific features
across temporal sequences, collectively addressing the scarcity of large facial
angles and identity-stable training data in existing datasets. Leveraging this
pipeline, we have curated and refined a Large Face Angles (LFA) Dataset from
existing open-source human video datasets, comprising 460K video clips with
annotated facial angles. Experimental results on the LFA benchmark demonstrate
that our method, empowered by the LFA dataset, significantly outperforms prior
SOTA methods in face similarity, face FID, and CLIP semantic alignment. The
code and dataset will be made publicly available at
https://github.com/rain152/LFA-Video-Generation.

</details>


### [107] [CLIP-Flow: A Universal Discriminator for AI-Generated Images Inspired by Anomaly Detection](https://arxiv.org/abs/2508.09477)
*Zhipeng Yuan,Kai Wang,Weize Quan,Dong-Ming Yan,Tieru Wu*

Main category: cs.CV

TL;DR: 提出了一种基于异常检测的通用AI生成图像检测器，无需访问AI生成图像，通过无监督学习实现泛化。


<details>
  <summary>Details</summary>
Motivation: AI生成图像质量接近真实图像，引发安全问题，现有检测器对未见过的生成模型性能有限。

Method: 使用预训练CLIP编码器提取特征，设计类似归一化流的无监督模型，利用代理图像训练。

Result: 实验证明该方法对多种图像生成器生成的AI图像检测有效。

Conclusion: 提出的无监督方法在通用AI生成图像检测中表现优异。

Abstract: With the rapid advancement of AI generative models, the visual quality of
AI-generated images (AIIs) has become increasingly close to natural images,
which inevitably raises security concerns. Most AII detectors often employ the
conventional image classification pipeline with natural images and AIIs
(generated by a generative model), which can result in limited detection
performance for AIIs from unseen generative models. To solve this, we proposed
a universal AI-generated image detector from the perspective of anomaly
detection. Our discriminator does not need to access any AIIs and learn a
generalizable representation with unsupervised learning. Specifically, we use
the pre-trained CLIP encoder as the feature extractor and design a normalizing
flow-like unsupervised model. Instead of AIIs, proxy images, e.g., obtained by
applying a spectral modification operation on natural images, are used for
training. Our models are trained by minimizing the likelihood of proxy images,
optionally combined with maximizing the likelihood of natural images. Extensive
experiments demonstrate the effectiveness of our method on AIIs produced by
various image generators.

</details>


### [108] [GazeLT: Visual attention-guided long-tailed disease classification in chest radiographs](https://arxiv.org/abs/2508.09478)
*Moinak Bhattacharya,Gagandeep Singh,Shubham Jain,Prateek Prasanna*

Main category: cs.CV

TL;DR: GazeLT是一种结合放射科医生视觉注意力的长尾疾病分类方法，通过整合与分解机制提升分类效果。


<details>
  <summary>Details</summary>
Motivation: 放射科医生的视觉注意力模式包含疾病相关信息的细粒度与粗粒度特征，将其融入深度学习框架可提升自动化图像解读能力。

Method: GazeLT利用视觉搜索过程的时序特性，通过整合与分解机制改进长尾疾病分类。

Result: 在NIH-CXR-LT和MIMIC-CXR-LT数据集上，GazeLT比最佳长尾损失方法提升4.1%，比视觉注意力基线提升21.7%。

Conclusion: GazeLT通过结合放射科医生的视觉注意力，显著提升了长尾疾病分类的准确性。

Abstract: In this work, we present GazeLT, a human visual attention
integration-disintegration approach for long-tailed disease classification. A
radiologist's eye gaze has distinct patterns that capture both fine-grained and
coarser level disease related information. While interpreting an image, a
radiologist's attention varies throughout the duration; it is critical to
incorporate this into a deep learning framework to improve automated image
interpretation. Another important aspect of visual attention is that apart from
looking at major/obvious disease patterns, experts also look at
minor/incidental findings (few of these constituting long-tailed classes)
during the course of image interpretation. GazeLT harnesses the temporal aspect
of the visual search process, via an integration and disintegration mechanism,
to improve long-tailed disease classification. We show the efficacy of GazeLT
on two publicly available datasets for long-tailed disease classification,
namely the NIH-CXR-LT (n=89237) and the MIMIC-CXR-LT (n=111898) datasets.
GazeLT outperforms the best long-tailed loss by 4.1% and the visual
attention-based baseline by 21.7% in average accuracy metrics for these
datasets. Our code is available at https://github.com/lordmoinak1/gazelt.

</details>


### [109] [SkySplat: Generalizable 3D Gaussian Splatting from Multi-Temporal Sparse Satellite Images](https://arxiv.org/abs/2508.09479)
*Xuejun Huang,Xinyi Liu,Yi Wan,Zhi Zheng,Bin Zhang,Mingtao Xiong,Yingying Pei,Yongjun Zhang*

Main category: cs.CV

TL;DR: SkySplat是一种自监督框架，将RPC模型融入3D高斯泼溅（3DGS）流程，解决了稀疏卫星图像重建中的几何约束不足和辐射不一致问题。


<details>
  <summary>Details</summary>
Motivation: 稀疏卫星图像的三维重建存在几何约束不足、瞬态物体干扰和辐射不一致等挑战，现有方法无法有效解决。

Method: SkySplat通过整合RPC模型、引入交叉自一致性模块（CSCM）和多视角一致性聚合策略，仅依赖RGB图像和相对高度监督实现重建。

Result: 相比现有方法，SkySplat在速度（86倍于EOGS）和精度（MAE从13.18米降至1.80米）上均有显著提升，并在跨数据集测试中表现优异。

Conclusion: SkySplat为稀疏卫星图像的三维重建提供了一种高效且通用的解决方案。

Abstract: Three-dimensional scene reconstruction from sparse-view satellite images is a
long-standing and challenging task. While 3D Gaussian Splatting (3DGS) and its
variants have recently attracted attention for its high efficiency, existing
methods remain unsuitable for satellite images due to incompatibility with
rational polynomial coefficient (RPC) models and limited generalization
capability. Recent advances in generalizable 3DGS approaches show potential,
but they perform poorly on multi-temporal sparse satellite images due to
limited geometric constraints, transient objects, and radiometric
inconsistencies. To address these limitations, we propose SkySplat, a novel
self-supervised framework that integrates the RPC model into the generalizable
3DGS pipeline, enabling more effective use of sparse geometric cues for
improved reconstruction. SkySplat relies only on RGB images and
radiometric-robust relative height supervision, thereby eliminating the need
for ground-truth height maps. Key components include a Cross-Self Consistency
Module (CSCM), which mitigates transient object interference via
consistency-based masking, and a multi-view consistency aggregation strategy
that refines reconstruction results. Compared to per-scene optimization
methods, SkySplat achieves an 86 times speedup over EOGS with higher accuracy.
It also outperforms generalizable 3DGS baselines, reducing MAE from 13.18 m to
1.80 m on the DFC19 dataset significantly, and demonstrates strong
cross-dataset generalization on the MVS3D benchmark.

</details>


### [110] [SARE: Semantic-Aware Reconstruction Error for Generalizable Diffusion-Generated Image Detection](https://arxiv.org/abs/2508.09487)
*Ju Yeon Kang,Jaehong Park,Semin Kim,Ji Won Yoon,Nam Soo Kim*

Main category: cs.CV

TL;DR: 论文提出了一种名为SARE的新方法，通过测量图像与其标题引导重建之间的语义差异，来检测扩散模型生成的假图像，解决了现有方法在未见生成模型上性能下降的问题。


<details>
  <summary>Details</summary>
Motivation: 扩散模型的快速发展引发了对其潜在滥用的担忧，现有检测方法在面对未见生成模型时性能显著下降，因为它们主要依赖模型特定的伪影。

Method: 论文提出了一种名为SARE的表示方法，通过量化图像与标题引导重建之间的语义差异，作为检测假图像的判别特征。

Result: 实验表明，SARE方法在GenImage和CommunityForensics等基准测试中表现出色，优于现有基线方法。

Conclusion: SARE方法通过利用假图像与标题之间的高相似性，实现了对多样化生成模型的鲁棒检测。

Abstract: Recently, diffusion-generated image detection has gained increasing
attention, as the rapid advancement of diffusion models has raised serious
concerns about their potential misuse. While existing detection methods have
achieved promising results, their performance often degrades significantly when
facing fake images from unseen, out-of-distribution (OOD) generative models,
since they primarily rely on model-specific artifacts. To address this
limitation, we explore a fundamental property commonly observed in fake images.
Motivated by the observation that fake images tend to exhibit higher similarity
to their captions than real images, we propose a novel representation, namely
Semantic-Aware Reconstruction Error (SARE), that measures the semantic
difference between an image and its caption-guided reconstruction. The
hypothesis behind SARE is that real images, whose captions often fail to fully
capture their complex visual content, may undergo noticeable semantic shifts
during the caption-guided reconstruction process. In contrast, fake images,
which closely align with their captions, show minimal semantic changes. By
quantifying these semantic shifts, SARE can be utilized as a discriminative
feature for robust detection across diverse generative models. We empirically
demonstrate that the proposed method exhibits strong generalization,
outperforming existing baselines on benchmarks including GenImage and
CommunityForensics.

</details>


### [111] [CWFBind: Geometry-Awareness for Fast and Accurate Protein-Ligand Docking](https://arxiv.org/abs/2508.09499)
*Liyan Jia,Chuan-Xian Ren,Hong Yan*

Main category: cs.CV

TL;DR: CWFBind是一种基于局部曲率特征的快速、准确的分子对接方法，通过整合几何信息提升预测性能。


<details>
  <summary>Details</summary>
Motivation: 传统深度学习方法依赖图表示和语言模型编码器，忽略了关键的几何信息，导致口袋定位和结合构象不准确。

Method: CWFBind在特征提取阶段整合局部曲率描述符，增强几何表示，并引入度感知权重机制和动态半径策略。

Result: 在多个对接基准测试中，CWFBind表现出竞争性性能，平衡了准确性和效率。

Conclusion: CWFBind通过几何信息增强和动态策略，显著提升了分子对接的准确性和实用性。

Abstract: Accurately predicting the binding conformation of small-molecule ligands to
protein targets is a critical step in rational drug design. Although recent
deep learning-based docking surpasses traditional methods in speed and
accuracy, many approaches rely on graph representations and language
model-inspired encoders while neglecting critical geometric information,
resulting in inaccurate pocket localization and unrealistic binding
conformations. In this study, we introduce CWFBind, a weighted, fast, and
accurate docking method based on local curvature features. Specifically, we
integrate local curvature descriptors during the feature extraction phase to
enrich the geometric representation of both proteins and ligands, complementing
existing chemical, sequence, and structural features. Furthermore, we embed
degree-aware weighting mechanisms into the message passing process, enhancing
the model's ability to capture spatial structural distinctions and interaction
strengths. To address the class imbalance challenge in pocket prediction,
CWFBind employs a ligand-aware dynamic radius strategy alongside an enhanced
loss function, facilitating more precise identification of binding regions and
key residues. Comprehensive experimental evaluations demonstrate that CWFBind
achieves competitive performance across multiple docking benchmarks, offering a
balanced trade-off between accuracy and efficiency.

</details>


### [112] [Generation of Indian Sign Language Letters, Numbers, and Words](https://arxiv.org/abs/2508.09522)
*Ajeet Kumar Yadav,Nishant Kumar,Rathna G N*

Main category: cs.CV

TL;DR: 论文提出了一种结合ProGAN和SAGAN的GAN变体，用于生成高质量、高分辨率的印度手语图像，并在IS和FID指标上优于传统ProGAN。


<details>
  <summary>Details</summary>
Motivation: 解决手语生成领域在分辨率和细节平衡上的挑战，促进听障与非听障人群的沟通。

Method: 结合ProGAN和SAGAN的优点，开发了一种改进的基于注意力的GAN模型。

Result: 生成的印度手语图像在IS和FID指标上分别提升了3.2和30.12，并发布了包含字母、数字和129个单词的大型数据集。

Conclusion: 该模型在手语图像生成中表现出色，为相关研究和应用提供了高质量的数据支持。

Abstract: Sign language, which contains hand movements, facial expressions and bodily
gestures, is a significant medium for communicating with hard-of-hearing
people. A well-trained sign language community communicates easily, but those
who don't know sign language face significant challenges. Recognition and
generation are basic communication methods between hearing and hard-of-hearing
individuals. Despite progress in recognition, sign language generation still
needs to be explored. The Progressive Growing of Generative Adversarial Network
(ProGAN) excels at producing high-quality images, while the Self-Attention
Generative Adversarial Network (SAGAN) generates feature-rich images at medium
resolutions. Balancing resolution and detail is crucial for sign language image
generation. We are developing a Generative Adversarial Network (GAN) variant
that combines both models to generate feature-rich, high-resolution, and
class-conditional sign language images. Our modified Attention-based model
generates high-quality images of Indian Sign Language letters, numbers, and
words, outperforming the traditional ProGAN in Inception Score (IS) and
Fr\'echet Inception Distance (FID), with improvements of 3.2 and 30.12,
respectively. Additionally, we are publishing a large dataset incorporating
high-quality images of Indian Sign Language alphabets, numbers, and 129 words.

</details>


### [113] [SOI is the Root of All Evil: Quantifying and Breaking Similar Object Interference in Single Object Tracking](https://arxiv.org/abs/2508.09524)
*Yipei Wang,Shiyu Hu,Shukun Jia,Panxi Xu,Hongfei Ma,Yiping Ma,Jing Zhang,Xiaobo Lu,Xin Zhao*

Main category: cs.CV

TL;DR: 论文首次系统研究并量化了单目标跟踪（SOT）中的相似物体干扰（SOI），通过实验验证消除干扰源能显著提升性能，并构建了首个针对SOI的语义认知基准SOIBench。


<details>
  <summary>Details</summary>
Motivation: 揭示SOI是单目标跟踪中的关键瓶颈，探索外部认知指导（如自然语言）对提升跟踪性能的可行性。

Method: 通过在线干扰掩蔽（OIM）实验量化SOI影响，构建SOIBench基准，并提出基于大规模视觉语言模型（VLM）的新范式。

Result: 消除SOI显著提升跟踪性能（AUC增益达4.35），现有视觉语言跟踪方法效果有限，而新方法在语义指导下AUC增益达0.93。

Conclusion: SOIBench为语义认知跟踪研究提供了标准化平台，新范式显著优于现有方法，为跟踪领域带来新见解。

Abstract: In this paper, we present the first systematic investigation and
quantification of Similar Object Interference (SOI), a long-overlooked yet
critical bottleneck in Single Object Tracking (SOT). Through controlled Online
Interference Masking (OIM) experiments, we quantitatively demonstrate that
eliminating interference sources leads to substantial performance improvements
(AUC gains up to 4.35) across all SOTA trackers, directly validating SOI as a
primary constraint for robust tracking and highlighting the feasibility of
external cognitive guidance. Building upon these insights, we adopt natural
language as a practical form of external guidance, and construct SOIBench-the
first semantic cognitive guidance benchmark specifically targeting SOI
challenges. It automatically mines SOI frames through multi-tracker collective
judgment and introduces a multi-level annotation protocol to generate precise
semantic guidance texts. Systematic evaluation on SOIBench reveals a striking
finding: existing vision-language tracking (VLT) methods fail to effectively
exploit semantic cognitive guidance, achieving only marginal improvements or
even performance degradation (AUC changes of -0.26 to +0.71). In contrast, we
propose a novel paradigm employing large-scale vision-language models (VLM) as
external cognitive engines that can be seamlessly integrated into arbitrary RGB
trackers. This approach demonstrates substantial improvements under semantic
cognitive guidance (AUC gains up to 0.93), representing a significant
advancement over existing VLT methods. We hope SOIBench will serve as a
standardized evaluation platform to advance semantic cognitive tracking
research and contribute new insights to the tracking research community.

</details>


### [114] [Learning Spatial Decay for Vision Transformers](https://arxiv.org/abs/2508.09525)
*Yuxin Mao,Zhen Qin,Jinxing Zhou,Bin Fan,Jing Zhang,Yiran Zhong,Yuchao Dai*

Main category: cs.CV

TL;DR: 论文提出了一种名为SDT的新型视觉Transformer，通过动态数据依赖的空间衰减机制改进了传统ViT在空间结构任务中的表现。


<details>
  <summary>Details</summary>
Motivation: 传统ViT的自注意力机制缺乏显式空间归纳偏置，导致在空间结构任务中表现不佳。现有方法使用固定距离度量的空间衰减，无法适应多样化的视觉场景。

Method: 提出SDT，引入上下文感知门控机制（CAG），动态生成数据依赖的空间衰减，结合曼哈顿距离的空间先验与学习到的内容表示。

Result: 在ImageNet-1K分类和生成任务中，SDT显著优于基线模型。

Conclusion: 数据依赖的空间衰减为增强视觉Transformer的空间注意力提供了新范式。

Abstract: Vision Transformers (ViTs) have revolutionized computer vision, yet their
self-attention mechanism lacks explicit spatial inductive biases, leading to
suboptimal performance on spatially-structured tasks. Existing approaches
introduce data-independent spatial decay based on fixed distance metrics,
applying uniform attention weighting regardless of image content and limiting
adaptability to diverse visual scenarios. Inspired by recent advances in large
language models where content-aware gating mechanisms (e.g., GLA, HGRN2, FOX)
significantly outperform static alternatives, we present the first successful
adaptation of data-dependent spatial decay to 2D vision transformers. We
introduce \textbf{Spatial Decay Transformer (SDT)}, featuring a novel
Context-Aware Gating (CAG) mechanism that generates dynamic, data-dependent
decay for patch interactions. Our approach learns to modulate spatial attention
based on both content relevance and spatial proximity. We address the
fundamental challenge of 1D-to-2D adaptation through a unified spatial-content
fusion framework that integrates manhattan distance-based spatial priors with
learned content representations. Extensive experiments on ImageNet-1K
classification and generation tasks demonstrate consistent improvements over
strong baselines. Our work establishes data-dependent spatial decay as a new
paradigm for enhancing spatial attention in vision transformers.

</details>


### [115] [Physics-guided Deep Unfolding Network for Enhanced Kronecker Compressive sensing](https://arxiv.org/abs/2508.09528)
*Gang Qu,Ping Wang,Siming Zheng,Xin Yuan*

Main category: cs.CV

TL;DR: 论文提出了一种新的非对称Kronecker压缩感知模型（AKCS）和测量感知交叉注意力机制（MACA），结合为MEUNet，显著提升了图像压缩感知的性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法在测量阶段缺乏非相干性，重建阶段缺乏显式测量表示，限制了整体性能。

Method: 提出AKCS模型提高测量非相干性，MACA机制学习隐式测量表示，并整合为MEUNet。

Result: MEUNet在重建精度和推理速度上达到最优性能。

Conclusion: AKCS和MACA的结合显著提升了压缩感知任务的效果。

Abstract: Deep networks have achieved remarkable success in image compressed sensing
(CS) task, namely reconstructing a high-fidelity image from its compressed
measurement. However, existing works are deficient inincoherent compressed
measurement at sensing phase and implicit measurement representations at
reconstruction phase, limiting the overall performance. In this work, we answer
two questions: 1) how to improve the measurement incoherence for decreasing the
ill-posedness; 2) how to learn informative representations from measurements.
To this end, we propose a novel asymmetric Kronecker CS (AKCS) model and
theoretically present its better incoherence than previous Kronecker CS with
minimal complexity increase. Moreover, we reveal that the unfolding networks'
superiority over non-unfolding ones result from sufficient gradient descents,
called explicit measurement representations. We propose a measurement-aware
cross attention (MACA) mechanism to learn implicit measurement representations.
We integrate AKCS and MACA into widely-used unfolding architecture to get a
measurement-enhanced unfolding network (MEUNet). Extensive experiences
demonstrate that our MEUNet achieves state-of-the-art performance in
reconstruction accuracy and inference speed.

</details>


### [116] [COXNet: Cross-Layer Fusion with Adaptive Alignment and Scale Integration for RGBT Tiny Object Detection](https://arxiv.org/abs/2508.09533)
*Peiran Peng,Tingfa Xu,Liqiang Song,Mengqi Zhu,Yuqiang Fang,Jianan Li*

Main category: cs.CV

TL;DR: COXNet是一种用于RGBT微小目标检测的新框架，通过跨层融合、动态对齐和优化标签分配策略，显著提升了复杂环境下的检测性能。


<details>
  <summary>Details</summary>
Motivation: 解决多模态RGBT图像中微小目标检测的挑战，特别是在无人机场景中因空间错位、低光照和复杂背景导致的检测困难。

Method: 提出COXNet框架，包含跨层融合模块、动态对齐与尺度细化模块，以及基于GeoShape相似性度量的优化标签分配策略。

Result: 在RGBTDronePerson数据集上，COXNet比现有方法提升了3.32%的mAP50。

Conclusion: COXNet在复杂环境中表现出色，为RGBT微小目标检测提供了有效解决方案。

Abstract: Detecting tiny objects in multimodal Red-Green-Blue-Thermal (RGBT) imagery is
a critical challenge in computer vision, particularly in surveillance, search
and rescue, and autonomous navigation. Drone-based scenarios exacerbate these
challenges due to spatial misalignment, low-light conditions, occlusion, and
cluttered backgrounds. Current methods struggle to leverage the complementary
information between visible and thermal modalities effectively. We propose
COXNet, a novel framework for RGBT tiny object detection, addressing these
issues through three core innovations: i) the Cross-Layer Fusion Module, fusing
high-level visible and low-level thermal features for enhanced semantic and
spatial accuracy; ii) the Dynamic Alignment and Scale Refinement module,
correcting cross-modal spatial misalignments and preserving multi-scale
features; and iii) an optimized label assignment strategy using the GeoShape
Similarity Measure for better localization. COXNet achieves a 3.32\% mAP$_{50}$
improvement on the RGBTDronePerson dataset over state-of-the-art methods,
demonstrating its effectiveness for robust detection in complex environments.

</details>


### [117] [Iterative Volume Fusion for Asymmetric Stereo Matching](https://arxiv.org/abs/2508.09543)
*Yuanting Gao,Linghao Shen*

Main category: cs.CV

TL;DR: 论文提出了一种针对非对称立体视觉的双阶段迭代体积融合网络（IVF-AStereo），通过融合两种成本体积以解决视觉不对称对立体匹配的干扰。


<details>
  <summary>Details</summary>
Motivation: 非对称多相机系统（如望远-广角相机）的兴起挑战了传统立体匹配算法对视觉对称性的假设，导致匹配成本体积计算受到干扰。

Method: 提出IVF-AStereo网络，分两阶段：首先通过聚合拼接体积优化相关体积，随后融合两种体积以增强细节。

Result: 在非对称场景中表现优异，对显著视觉不对称具有鲁棒性。

Conclusion: 实验验证了该方法在分辨率和颜色退化情况下的有效性。

Abstract: Stereo matching is vital in 3D computer vision, with most algorithms assuming
symmetric visual properties between binocular visions. However, the rise of
asymmetric multi-camera systems (e.g., tele-wide cameras) challenges this
assumption and complicates stereo matching. Visual asymmetry disrupts stereo
matching by affecting the crucial cost volume computation. To address this, we
explore the matching cost distribution of two established cost volume
construction methods in asymmetric stereo. We find that each cost volume
experiences distinct information distortion, indicating that both should be
comprehensively utilized to solve the issue. Based on this, we propose the
two-phase Iterative Volume Fusion network for Asymmetric Stereo matching
(IVF-AStereo). Initially, the aggregated concatenation volume refines the
correlation volume. Subsequently, both volumes are fused to enhance fine
details. Our method excels in asymmetric scenarios and shows robust performance
against significant visual asymmetry. Extensive comparative experiments on
benchmark datasets, along with ablation studies, confirm the effectiveness of
our approach in asymmetric stereo with resolution and color degradation.

</details>


### [118] [GoViG: Goal-Conditioned Visual Navigation Instruction Generation](https://arxiv.org/abs/2508.09547)
*Fengyi Wu,Yifei Dong,Zhi-Qi Cheng,Yilong Dai,Guangyu Chen,Hang Wang,Qi Dai,Alexander G. Hauptmann*

Main category: cs.CV

TL;DR: GoViG是一种新任务，仅通过初始和目标状态的视觉观察生成导航指令，无需结构化输入，提高了对未知环境的适应性。


<details>
  <summary>Details</summary>
Motivation: 传统方法依赖结构化输入（如语义标注或地图），限制了在未知和非结构化环境中的适应性。GoViG旨在通过视觉数据直接生成指令。

Method: 将任务分解为视觉预测（预测中间状态）和指令生成（基于视觉生成语言指令），并采用多模态大语言模型和两种推理策略（单次和交替推理）。

Result: 在R2R-Goal数据集上表现优于现有方法，BLEU-4和CIDEr分数显著提升，且具有跨域泛化能力。

Conclusion: GoViG通过视觉数据和多模态模型实现了高效导航指令生成，适用于未知和非结构化环境。

Abstract: We introduce Goal-Conditioned Visual Navigation Instruction Generation
(GoViG), a new task that aims to autonomously generate precise and contextually
coherent navigation instructions solely from egocentric visual observations of
initial and goal states. Unlike conventional approaches that rely on structured
inputs such as semantic annotations or environmental maps, GoViG exclusively
leverages raw egocentric visual data, substantially improving its adaptability
to unseen and unstructured environments. Our method addresses this task by
decomposing it into two interconnected subtasks: (1) visual forecasting, which
predicts intermediate visual states bridging the initial and goal views; and
(2) instruction generation, which synthesizes linguistically coherent
instructions grounded in both observed and anticipated visuals. These subtasks
are integrated within an autoregressive multimodal large language model trained
with tailored objectives to ensure spatial accuracy and linguistic clarity.
Furthermore, we introduce two complementary multimodal reasoning strategies,
one-pass and interleaved reasoning, to mimic incremental human cognitive
processes during navigation. To evaluate our method, we propose the R2R-Goal
dataset, combining diverse synthetic and real-world trajectories. Empirical
results demonstrate significant improvements over state-of-the-art methods,
achieving superior BLEU-4 and CIDEr scores along with robust cross-domain
generalization.

</details>


### [119] [Exploring the Equivalence of Closed-Set Generative and Real Data Augmentation in Image Classification](https://arxiv.org/abs/2508.09550)
*Haowen Wang,Guowei Zhang,Xiang Zhang,Zeyuan Chen,Haiyang Xu,Dou Hoon Kwark,Zhuowen Tu*

Main category: cs.CV

TL;DR: 本文探讨了在图像分类任务中，使用生成模型生成的合成数据增强训练集的效果，并量化了合成数据与真实数据增强的等价性。


<details>
  <summary>Details</summary>
Motivation: 解决机器学习中如何利用生成模型增强图像分类性能的问题，特别是封闭集合成数据的有效性。

Method: 通过实验比较真实图像与封闭集合成图像的差异，量化合成数据增强的等效规模，并与开放集生成增强对比。

Result: 实证表明合成数据增强需要更大规模才能达到与真实数据增强相当的效果，且效果随训练集大小和合成数据量变化。

Conclusion: 研究为合成数据增强提供了量化指导，并展示了其在自然和医学图像数据集上的适用性。

Abstract: In this paper, we address a key scientific problem in machine learning: Given
a training set for an image classification task, can we train a generative
model on this dataset to enhance the classification performance? (i.e.,
closed-set generative data augmentation). We start by exploring the
distinctions and similarities between real images and closed-set synthetic
images generated by advanced generative models. Through extensive experiments,
we offer systematic insights into the effective use of closed-set synthetic
data for augmentation. Notably, we empirically determine the equivalent scale
of synthetic images needed for augmentation. In addition, we also show
quantitative equivalence between the real data augmentation and open-set
generative augmentation (generative models trained using data beyond the given
training set). While it aligns with the common intuition that real images are
generally preferred, our empirical formulation also offers a guideline to
quantify the increased scale of synthetic data augmentation required to achieve
comparable image classification performance. Our results on natural and medical
image datasets further illustrate how this effect varies with the baseline
training set size and the amount of synthetic data incorporated.

</details>


### [120] [Topological Invariant-Based Iris Identification via Digital Homology and Machine Learning](https://arxiv.org/abs/2508.09555)
*Ahmet Öztel,İsmet Karaca*

Main category: cs.CV

TL;DR: 提出了一种基于拓扑不变量的生物特征识别方法，通过数字同调理论分析虹膜纹理，并在分类性能上表现优异。


<details>
  <summary>Details</summary>
Motivation: 探索一种紧凑、可解释且准确的虹膜识别方法，以替代深度学习，适用于需要可解释性或数据有限的场景。

Method: 将归一化虹膜图像划分为网格，计算每个子区域的Betti0、Betti1及其比值，形成特征矩阵，结合逻辑回归、KNN和SVM进行分类，并与CNN对比。

Result: 逻辑回归准确率达97.78%，优于CNN（96.44%）和其他特征模型，拓扑特征表现出高准确性和低方差。

Conclusion: 首次将数字同调理论的拓扑不变量用于虹膜识别，方法紧凑、可解释且高效，适用于安全关键领域及其他多学科应用。

Abstract: Objective - This study presents a biometric identification method based on
topological invariants from 2D iris images, representing iris texture via
formally defined digital homology and evaluating classification performance.
  Methods - Each normalized iris image (48x482 pixels) is divided into grids
(e.g., 6x54 or 3x27). For each subregion, we compute Betti0, Betti1, and their
ratio using a recent algorithm for homology groups in 2D digital images. The
resulting invariants form a feature matrix used with logistic regression, KNN,
and SVM (with PCA and 100 randomized repetitions). A convolutional neural
network (CNN) is trained on raw images for comparison.
  Results - Logistic regression achieved 97.78 +/- 0.82% accuracy,
outperforming CNN (96.44 +/- 1.32%) and other feature-based models. The
topological features showed high accuracy with low variance.
  Conclusion - This is the first use of topological invariants from formal
digital homology for iris recognition. The method offers a compact,
interpretable, and accurate alternative to deep learning, useful when
explainability or limited data is important. Beyond iris recognition, it can
apply to other biometrics, medical imaging, materials science, remote sensing,
and interpretable AI. It runs efficiently on CPU-only systems and produces
robust, explainable features valuable for security-critical domains.

</details>


### [121] [WEC-DG: Multi-Exposure Wavelet Correction Method Guided by Degradation Description](https://arxiv.org/abs/2508.09565)
*Ming Zhao,Pingping Liu,Tongshun Zhang,Zhe Zhang*

Main category: cs.CV

TL;DR: 本文提出了一种基于小波变换和退化引导的多曝光校正方法（WEC-DG），通过曝光一致性对齐模块和曝光恢复与细节重建模块，解决了现有方法在复杂光照条件下的适应性不足问题。


<details>
  <summary>Details</summary>
Motivation: 当前多曝光校正方法在处理单一曝光水平的图像时，难以应对由不同光照条件、拍摄环境和天气因素引起的类内变异性，导致曝光异常校正不准确。

Method: 提出WEC-DG方法，包括曝光一致性对齐模块（ECAM）和曝光恢复与细节重建模块（EDRM），利用小波变换的光照-细节解耦特性进行序列处理。

Result: 在多个公开数据集上的实验表明，该方法显著优于现有算法，性能提升明显。

Conclusion: WEC-DG方法有效解决了复杂成像条件下的曝光校正问题，具有实际应用价值。

Abstract: Multi-exposure correction technology is essential for restoring images
affected by insufficient or excessive lighting, enhancing the visual experience
by improving brightness, contrast, and detail richness. However, current
multi-exposure correction methods often encounter challenges in addressing
intra-class variability caused by diverse lighting conditions, shooting
environments, and weather factors, particularly when processing images captured
at a single exposure level. To enhance the adaptability of these models under
complex imaging conditions, this paper proposes a Wavelet-based Exposure
Correction method with Degradation Guidance (WEC-DG). Specifically, we
introduce a degradation descriptor within the Exposure Consistency Alignment
Module (ECAM) at both ends of the processing pipeline to ensure exposure
consistency and achieve final alignment. This mechanism effectively addresses
miscorrected exposure anomalies caused by existing methods' failure to
recognize 'blurred' exposure degradation. Additionally, we investigate the
light-detail decoupling properties of the wavelet transform to design the
Exposure Restoration and Detail Reconstruction Module (EDRM), which processes
low-frequency information related to exposure enhancement before utilizing
high-frequency information as a prior guide for reconstructing spatial domain
details. This serial processing strategy guarantees precise light correction
and enhances detail recovery. Extensive experiments conducted on multiple
public datasets demonstrate that the proposed method outperforms existing
algorithms, achieving significant performance improvements and validating its
effectiveness and practical applicability.

</details>


### [122] [COME: Dual Structure-Semantic Learning with Collaborative MoE for Universal Lesion Detection Across Heterogeneous Ultrasound Datasets](https://arxiv.org/abs/2508.09886)
*Lingyu Chen,Yawen Zeng,Yue Wang,Peng Wan,Guo-chen Ning,Hongen Liao,Daoqiang Zhang,Fang Chen*

Main category: cs.CV

TL;DR: 论文提出了一种名为COME的通用框架，用于解决超声图像分析中多异构数据集训练的挑战，通过双结构-语义共享专家与源特定专家协作提取特征，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 传统单数据集训练在新数据分布下表现不佳，尤其是在超声图像分析中，由于数据有限、声学阴影和斑点噪声等问题，构建一个适用于多异构数据集的通用框架变得至关重要。

Method: COME通过建立双结构-语义共享专家，创建通用表示空间，并与源特定专家协作提取判别性特征，利用跨数据集经验分布和通用超声先验知识。

Result: 在三种评估模式（单数据集、器官内和器官间集成数据集）下，COME表现优于现有方法，平均AP显著提升。

Conclusion: COME通过协作混合异构源特定专家，实现了对超声图像分析的鲁棒泛化能力，为小批量或未见数据场景提供了通用解决方案。

Abstract: Conventional single-dataset training often fails with new data distributions,
especially in ultrasound (US) image analysis due to limited data, acoustic
shadows, and speckle noise. Therefore, constructing a universal framework for
multi-heterogeneous US datasets is imperative. However, a key challenge arises:
how to effectively mitigate inter-dataset interference while preserving
dataset-specific discriminative features for robust downstream task? Previous
approaches utilize either a single source-specific decoder or a domain
adaptation strategy, but these methods experienced a decline in performance
when applied to other domains. Considering this, we propose a Universal
Collaborative Mixture of Heterogeneous Source-Specific Experts (COME).
Specifically, COME establishes dual structure-semantic shared experts that
create a universal representation space and then collaborate with
source-specific experts to extract discriminative features through providing
complementary features. This design enables robust generalization by leveraging
cross-datasets experience distributions and providing universal US priors for
small-batch or unseen data scenarios. Extensive experiments under three
evaluation modes (single-dataset, intra-organ, and inter-organ integration
datasets) demonstrate COME's superiority, achieving significant mean AP
improvements over state-of-the-art methods. Our project is available at:
https://universalcome.github.io/UniversalCOME/.

</details>


### [123] [A Chain of Diagnosis Framework for Accurate and Explainable Radiology Report Generation](https://arxiv.org/abs/2508.09566)
*Haibo Jin,Haoxuan Che,Sunan He,Hao Chen*

Main category: cs.CV

TL;DR: 论文提出了一种名为诊断链（CoD）的框架，旨在解决放射学报告生成（RRG）中临床效果不佳和生成文本缺乏可解释性的问题。通过生成问答对和利用大语言模型，CoD实现了准确且可解释的报告生成，并通过病灶定位模块提升了放射科医生的工作效率。


<details>
  <summary>Details</summary>
Motivation: 现有放射学报告生成模型在临床效果（尤其是病灶属性描述）和可解释性方面表现不佳，导致放射科医生难以信任生成结果。

Method: 提出CoD框架，包括生成诊断对话的问答对、利用大语言模型生成报告、设计诊断和病灶定位模块以增强可解释性，并采用全监督学习策略利用多种标注数据。

Result: CoD在两个RRG基准测试中优于专家和通用模型，并通过准确地将生成句子与问答诊断和图像关联，展示了良好的可解释性。

Conclusion: CoD框架通过结合诊断链和病灶定位，显著提升了放射学报告生成的准确性和可解释性，同时提供了高效的工具和数据集支持。

Abstract: Despite the progress of radiology report generation (RRG), existing works
face two challenges: 1) The performances in clinical efficacy are
unsatisfactory, especially for lesion attributes description; 2) the generated
text lacks explainability, making it difficult for radiologists to trust the
results. To address the challenges, we focus on a trustworthy RRG model, which
not only generates accurate descriptions of abnormalities, but also provides
basis of its predictions. To this end, we propose a framework named chain of
diagnosis (CoD), which maintains a chain of diagnostic process for clinically
accurate and explainable RRG. It first generates question-answer (QA) pairs via
diagnostic conversation to extract key findings, then prompts a large language
model with QA diagnoses for accurate generation. To enhance explainability, a
diagnosis grounding module is designed to match QA diagnoses and generated
sentences, where the diagnoses act as a reference. Moreover, a lesion grounding
module is designed to locate abnormalities in the image, further improving the
working efficiency of radiologists. To facilitate label-efficient training, we
propose an omni-supervised learning strategy with clinical consistency to
leverage various types of annotations from different datasets. Our efforts lead
to 1) an omni-labeled RRG dataset with QA pairs and lesion boxes; 2) a
evaluation tool for assessing the accuracy of reports in describing lesion
location and severity; 3) extensive experiments to demonstrate the
effectiveness of CoD, where it outperforms both specialist and generalist
models consistently on two RRG benchmarks and shows promising explainability by
accurately grounding generated sentences to QA diagnoses and images.

</details>


### [124] [Echo-4o: Harnessing the Power of GPT-4o Synthetic Images for Improved Image Generation](https://arxiv.org/abs/2508.09987)
*Junyan Ye,Dongzhi Jiang,Zihao Wang,Leqi Zhu,Zhenghao Hu,Zilong Huang,Jun He,Zhiyuan Yan,Jinghua Yu,Hongsheng Li,Conghui He,Weijia Li*

Main category: cs.CV

TL;DR: 论文探讨了GPT-4o生成的合成图像数据在增强开源模型中的优势，并提出了Echo-4o-Image数据集和两个新评估基准。


<details>
  <summary>Details</summary>
Motivation: 开源模型在图像生成上落后于GPT-4o，而合成数据可以弥补真实数据在罕见场景和噪声问题上的不足。

Method: 通过GPT-4o生成180K规模的合成数据集Echo-4o-Image，并用于微调Bagel模型。同时提出GenEval++和Imagine-Bench两个新评估基准。

Result: Echo-4o在标准基准测试中表现优异，且合成数据对其他基础模型也有性能提升。

Conclusion: 合成数据能有效填补真实数据的盲点，提升模型性能，并具有强迁移性。

Abstract: Recently, GPT-4o has garnered significant attention for its strong
performance in image generation, yet open-source models still lag behind.
Several studies have explored distilling image data from GPT-4o to enhance
open-source models, achieving notable progress. However, a key question
remains: given that real-world image datasets already constitute a natural
source of high-quality data, why should we use GPT-4o-generated synthetic data?
In this work, we identify two key advantages of synthetic images. First, they
can complement rare scenarios in real-world datasets, such as surreal fantasy
or multi-reference image generation, which frequently occur in user queries.
Second, they provide clean and controllable supervision. Real-world data often
contains complex background noise and inherent misalignment between text
descriptions and image content, whereas synthetic images offer pure backgrounds
and long-tailed supervision signals, facilitating more accurate text-to-image
alignment. Building on these insights, we introduce Echo-4o-Image, a 180K-scale
synthetic dataset generated by GPT-4o, harnessing the power of synthetic image
data to address blind spots in real-world coverage. Using this dataset, we
fine-tune the unified multimodal generation baseline Bagel to obtain Echo-4o.
In addition, we propose two new evaluation benchmarks for a more accurate and
challenging assessment of image generation capabilities: GenEval++, which
increases instruction complexity to mitigate score saturation, and
Imagine-Bench, which focuses on evaluating both the understanding and
generation of imaginative content. Echo-4o demonstrates strong performance
across standard benchmarks. Moreover, applying Echo-4o-Image to other
foundation models (e.g., OmniGen2, BLIP3-o) yields consistent performance gains
across multiple metrics, highlighting the datasets strong transferability.

</details>


### [125] [Dual Recursive Feedback on Generation and Appearance Latents for Pose-Robust Text-to-Image Diffusion](https://arxiv.org/abs/2508.09575)
*Jiwon Kim,Pureum Kim,SeonHwa Kim,Soobin Park,Eunju Cha,Kyong Hwan Jin*

Main category: cs.CV

TL;DR: 提出了一种无需训练的双递归反馈（DRF）系统，用于改进可控文本到图像（T2I）扩散模型的空间和外观控制。


<details>
  <summary>Details</summary>
Motivation: 现有可控T2I模型在保留空间结构和捕捉细粒度条件（如物体姿态和场景布局）方面表现不足。

Method: DRF通过外观反馈和生成反馈递归优化中间潜在表示，以更好地反映给定外观信息和用户意图。

Result: 实验表明，DRF能生成高质量、语义一致且结构一致的图像，支持跨类别的结构-外观融合。

Conclusion: DRF有效解决了现有模型的局限性，提升了可控T2I模型的性能。

Abstract: Recent advancements in controllable text-to-image (T2I) diffusion models,
such as Ctrl-X and FreeControl, have demonstrated robust spatial and appearance
control without requiring auxiliary module training. However, these models
often struggle to accurately preserve spatial structures and fail to capture
fine-grained conditions related to object poses and scene layouts. To address
these challenges, we propose a training-free Dual Recursive Feedback (DRF)
system that properly reflects control conditions in controllable T2I models.
The proposed DRF consists of appearance feedback and generation feedback that
recursively refines the intermediate latents to better reflect the given
appearance information and the user's intent. This dual-update mechanism guides
latent representations toward reliable manifolds, effectively integrating
structural and appearance attributes. Our approach enables fine-grained
generation even between class-invariant structure-appearance fusion, such as
transferring human motion onto a tiger's form. Extensive experiments
demonstrate the efficacy of our method in producing high-quality, semantically
coherent, and structurally consistent image generations. Our source code is
available at https://github.com/jwonkm/DRF.

</details>


### [126] [SHALE: A Scalable Benchmark for Fine-grained Hallucination Evaluation in LVLMs](https://arxiv.org/abs/2508.09584)
*Bei Yan,Zhiyuan Chen,Yuecong Min,Jie Zhang,Jiahao Wang,Xiaozhen Wang,Shiguang Shan*

Main category: cs.CV

TL;DR: 论文提出SHALE基准，用于细粒度评估大型视觉语言模型的幻觉问题，包括忠实性和事实性幻觉，并通过自动化数据构建和输入扰动模拟噪声场景。


<details>
  <summary>Details</summary>
Motivation: 现有研究对大型视觉语言模型的幻觉问题评估不够细粒度，且依赖人工或公共数据集，存在可扩展性和数据泄露问题。

Method: 提出自动化数据构建管道和分层幻觉诱导框架，构建SHALE基准，包含30K图像-指令对，覆盖12个视觉感知方面和6个知识领域。

Result: 实验发现主流模型存在显著的事实性幻觉，并对语义扰动高度敏感。

Conclusion: SHALE基准为评估和改善大型视觉语言模型的幻觉问题提供了有效工具。

Abstract: Despite rapid advances, Large Vision-Language Models (LVLMs) still suffer
from hallucinations, i.e., generating content inconsistent with input or
established world knowledge, which correspond to faithfulness and factuality
hallucinations, respectively. Prior studies primarily evaluate faithfulness
hallucination at a coarse level (e.g., object-level) and lack fine-grained
analysis. Additionally, existing benchmarks rely on costly manual curation or
reused public datasets, raising concerns about scalability and data leakage. To
address these limitations, we propose an automated data construction pipeline
that produces scalable, controllable, and diverse evaluation data. We also
design a hierarchical hallucination induction framework with input
perturbations to simulate realistic noisy scenarios. Integrating these designs,
we construct SHALE, a Scalable HALlucination Evaluation benchmark designed to
assess both faithfulness and factuality hallucinations via a fine-grained
hallucination categorization scheme. SHALE comprises over 30K image-instruction
pairs spanning 12 representative visual perception aspects for faithfulness and
6 knowledge domains for factuality, considering both clean and noisy scenarios.
Extensive experiments on over 20 mainstream LVLMs reveal significant factuality
hallucinations and high sensitivity to semantic perturbations.

</details>


### [127] [Offline Auto Labeling: BAAS](https://arxiv.org/abs/2508.09585)
*Stefan Haag,Bharanidhar Duraisamy,Felix Govaers,Wolfgang Koch,Martin Fritzsche,Juergen Dickmann*

Main category: cs.CV

TL;DR: BAAS是一个基于贝叶斯跟踪和融合的雷达检测标注框架，用于自动驾驶中的扩展目标跟踪和标注。


<details>
  <summary>Details</summary>
Motivation: 解决自动驾驶中雷达检测的精确跟踪和标注问题，支持不同监督级别的标注需求。

Method: 利用贝叶斯跟踪、平滑和融合方法，提供精确的目标轨迹和形状估计，支持模块化分析和闭环改进。

Result: 在复杂城市场景中验证了框架的跟踪性能和标注误差，适用于多种动态目标和类别。

Conclusion: BAAS框架能有效提升雷达检测的跟踪和标注精度，支持自动驾驶系统的持续优化。

Abstract: This paper introduces BAAS, a new Extended Object Tracking (EOT) and
fusion-based label annotation framework for radar detections in autonomous
driving. Our framework utilizes Bayesian-based tracking, smoothing and
eventually fusion methods to provide veritable and precise object trajectories
along with shape estimation to provide annotation labels on the detection level
under various supervision levels. Simultaneously, the framework provides
evaluation of tracking performance and label annotation. If manually labeled
data is available, each processing module can be analyzed independently or
combined with other modules to enable closed-loop continuous improvements. The
framework performance is evaluated in a challenging urban real-world scenario
in terms of tracking performance and the label annotation errors. We
demonstrate the functionality of the proposed approach for varying dynamic
objects and class types

</details>


### [128] [Hierarchical Brain Structure Modeling for Predicting Genotype of Glioma](https://arxiv.org/abs/2508.09593)
*Haotian Tang,Jianwei Chen,Xinrui Tang,Yunjia Wu,Zhengyang Miao,Chao Li*

Main category: cs.CV

TL;DR: Hi-SMGNN 是一个基于结构和形态连接组的层次化框架，用于预测 IDH 突变状态，通过多模态交互和多尺度特征融合提升预测性能。


<details>
  <summary>Details</summary>
Motivation: 当前的功能性 MRI 方法在预测 IDH 突变状态时受限于数据可用性和噪声问题，而现有基于连接组的方法忽略了大脑的层次结构和多尺度交互。

Method: Hi-SMGNN 结合了结构和形态连接组，采用 Siamese 网络和跨模态注意力机制的多模态交互模块，多尺度特征融合减少冗余，以及个性化模块划分策略。

Result: 在 UCSF-PDGM 数据集上，Hi-SMGNN 优于基线模型和现有先进模型，表现出更高的鲁棒性和预测效果。

Conclusion: Hi-SMGNN 提供了一种更有效和可解释的 IDH 突变预测方法，解决了现有方法的局限性。

Abstract: Isocitrate DeHydrogenase (IDH) mutation status is a crucial biomarker for
glioma prognosis. However, current prediction methods are limited by the low
availability and noise of functional MRI. Structural and morphological
connectomes offer a non-invasive alternative, yet existing approaches often
ignore the brain's hierarchical organisation and multiscale interactions. To
address this, we propose Hi-SMGNN, a hierarchical framework that integrates
structural and morphological connectomes from regional to modular levels. It
features a multimodal interaction module with a Siamese network and cross-modal
attention, a multiscale feature fusion mechanism for reducing redundancy, and a
personalised modular partitioning strategy to enhance individual specificity
and interpretability. Experiments on the UCSF-PDGM dataset demonstrate that
Hi-SMGNN outperforms baseline and state-of-the-art models, showing improved
robustness and effectiveness in IDH mutation prediction.

</details>


### [129] [SVG-Head: Hybrid Surface-Volumetric Gaussians for High-Fidelity Head Reconstruction and Real-Time Editing](https://arxiv.org/abs/2508.09597)
*Heyi Sun,Cong Wang,Tian-Xing Xu,Jingwei Huang,Di Kang,Chunchao Guo,Song-Hai Zhang*

Main category: cs.CV

TL;DR: SVG-Head提出了一种混合表示方法，结合表面和体积高斯模型，实现了高保真且可实时编辑的头像渲染。


<details>
  <summary>Details</summary>
Motivation: 解决头像编辑中因隐式表示和几何与全局外观纠缠建模带来的挑战。

Method: 使用3D高斯绑定FLAME网格建模几何，分离纹理图像捕捉全局外观，并通过分层优化策略提升性能。

Result: 在NeRSemble数据集上实现了高保真渲染，并首次支持实时外观编辑。

Conclusion: SVG-Head为头像编辑提供了高效且灵活的解决方案。

Abstract: Creating high-fidelity and editable head avatars is a pivotal challenge in
computer vision and graphics, boosting many AR/VR applications. While recent
advancements have achieved photorealistic renderings and plausible animation,
head editing, especially real-time appearance editing, remains challenging due
to the implicit representation and entangled modeling of the geometry and
global appearance. To address this, we propose Surface-Volumetric Gaussian Head
Avatar (SVG-Head), a novel hybrid representation that explicitly models the
geometry with 3D Gaussians bound on a FLAME mesh and leverages disentangled
texture images to capture the global appearance. Technically, it contains two
types of Gaussians, in which surface Gaussians explicitly model the appearance
of head avatars using learnable texture images, facilitating real-time texture
editing, while volumetric Gaussians enhance the reconstruction quality of
non-Lambertian regions (e.g., lips and hair). To model the correspondence
between 3D world and texture space, we provide a mesh-aware Gaussian UV mapping
method, which leverages UV coordinates given by the FLAME mesh to obtain sharp
texture images and real-time rendering speed. A hierarchical optimization
strategy is further designed to pursue the optimal performance in both
reconstruction quality and editing flexibility. Experiments on the NeRSemble
dataset show that SVG-Head not only generates high-fidelity rendering results,
but also is the first method to obtain explicit texture images for Gaussian
head avatars and support real-time appearance editing.

</details>


### [130] [Images Speak Louder Than Scores: Failure Mode Escape for Enhancing Generative Quality](https://arxiv.org/abs/2508.09598)
*Jie Shao,Ke Zhu,Minghao Fu,Guo-hua Wang,Jianxin Wu*

Main category: cs.CV

TL;DR: 本文提出FaME方法，通过评估图像质量并利用失败样本作为负引导，提升扩散模型的生成质量，同时不影响FID分数。


<details>
  <summary>Details</summary>
Motivation: 现有扩散模型在类到图像生成中虽FID表现优异，但部分类别图像质量低或失真，且CFG技术可能引入分布偏移和视觉伪影。

Method: 提出FaME方法，利用图像质量评估模型识别低质量生成样本，并存储其采样轨迹作为负引导，避免未来采样进入低质量区域。

Result: 在ImageNet上的实验表明，FaME能一致提升视觉质量且不影响FID，并有望扩展到文本到图像生成。

Conclusion: FaME是一种无需训练且高效的方法，能显著改善扩散模型的生成质量。

Abstract: Diffusion models have achieved remarkable progress in class-to-image
generation. However, we observe that despite impressive FID scores,
state-of-the-art models often generate distorted or low-quality images,
especially in certain classes. This gap arises because FID evaluates global
distribution alignment, while ignoring the perceptual quality of individual
samples. We further examine the role of CFG, a common technique used to enhance
generation quality. While effective in improving metrics and suppressing
outliers, CFG can introduce distribution shift and visual artifacts due to its
misalignment with both training objectives and user expectations. In this work,
we propose FaME, a training-free and inference-efficient method for improving
perceptual quality. FaME uses an image quality assessment model to identify
low-quality generations and stores their sampling trajectories. These failure
modes are then used as negative guidance to steer future sampling away from
poor-quality regions. Experiments on ImageNet demonstrate that FaME brings
consistent improvements in visual quality without compromising FID. FaME also
shows the potential to be extended to improve text-to-image generation.

</details>


### [131] [BridgeTA: Bridging the Representation Gap in Knowledge Distillation via Teacher Assistant for Bird's Eye View Map Segmentation](https://arxiv.org/abs/2508.09599)
*Beomjun Kim,Suhan Woo,Sejong Heo,Euntai Kim*

Main category: cs.CV

TL;DR: 提出BridgeTA框架，通过教师助理网络缩小LiDAR-Camera融合与纯相机模型之间的表示差距，保持学生模型架构和推理成本不变，显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 纯相机方法在自动驾驶BEV分割任务中性能落后于LiDAR-Camera融合方法，现有知识蒸馏方法因模仿教师架构导致推理成本增加。

Method: 引入轻量级教师助理网络，结合教师和学生的BEV表示，构建共享潜在空间，并基于Young不等式推导蒸馏损失以优化知识传递。

Result: 在nuScenes数据集上，性能提升4.2% mIoU，优于其他先进蒸馏方法45%。

Conclusion: BridgeTA框架有效提升了纯相机模型的性能，同时避免了推理成本的增加。

Abstract: Bird's-Eye-View (BEV) map segmentation is one of the most important and
challenging tasks in autonomous driving. Camera-only approaches have drawn
attention as cost-effective alternatives to LiDAR, but they still fall behind
LiDAR-Camera (LC) fusion-based methods. Knowledge Distillation (KD) has been
explored to narrow this gap, but existing methods mainly enlarge the student
model by mimicking the teacher's architecture, leading to higher inference
cost. To address this issue, we introduce BridgeTA, a cost-effective
distillation framework to bridge the representation gap between LC fusion and
Camera-only models through a Teacher Assistant (TA) network while keeping the
student's architecture and inference cost unchanged. A lightweight TA network
combines the BEV representations of the teacher and student, creating a shared
latent space that serves as an intermediate representation. To ground the
framework theoretically, we derive a distillation loss using Young's
Inequality, which decomposes the direct teacher-student distillation path into
teacher-TA and TA-student dual paths, stabilizing optimization and
strengthening knowledge transfer. Extensive experiments on the challenging
nuScenes dataset demonstrate the effectiveness of our method, achieving an
improvement of 4.2% mIoU over the Camera-only baseline, up to 45% higher than
the improvement of other state-of-the-art KD methods.

</details>


### [132] [MInDI-3D: Iterative Deep Learning in 3D for Sparse-view Cone Beam Computed Tomography](https://arxiv.org/abs/2508.09616)
*Daniel Barco,Marc Stadelmann,Martin Oswald,Ivo Herzig,Lukas Lichtensteiger,Pascal Paysan,Igor Peterlik,Michal Walczak,Bjoern Menze,Frank-Peter Schilling*

Main category: cs.CV

TL;DR: MInDI-3D是一种基于3D条件扩散的模型，用于稀疏视图CBCT伪影去除，显著降低辐射暴露。


<details>
  <summary>Details</summary>
Motivation: 减少医学成像中的辐射暴露，并扩展InDI概念至3D医学图像处理。

Method: 采用迭代去噪过程，直接从稀疏视图输入优化CBCT体积，并利用大规模伪CBCT数据集进行训练。

Result: 在CT-RATE测试集上PSNR提升12.96 dB，辐射暴露降低8倍，性能与3D U-Net相当，且能泛化至新CBCT扫描仪。

Conclusion: MInDI-3D在临床评估中表现优异，适用于患者定位并保留肿瘤边界。

Abstract: We present MInDI-3D (Medical Inversion by Direct Iteration in 3D), the first
3D conditional diffusion-based model for real-world sparse-view Cone Beam
Computed Tomography (CBCT) artefact removal, aiming to reduce imaging radiation
exposure. A key contribution is extending the "InDI" concept from 2D to a full
3D volumetric approach for medical images, implementing an iterative denoising
process that refines the CBCT volume directly from sparse-view input. A further
contribution is the generation of a large pseudo-CBCT dataset (16,182) from
chest CT volumes of the CT-RATE public dataset to robustly train MInDI-3D. We
performed a comprehensive evaluation, including quantitative metrics,
scalability analysis, generalisation tests, and a clinical assessment by 11
clinicians. Our results show MInDI-3D's effectiveness, achieving a 12.96 (6.10)
dB PSNR gain over uncorrected scans with only 50 projections on the CT-RATE
pseudo-CBCT (independent real-world) test set and enabling an 8x reduction in
imaging radiation exposure. We demonstrate its scalability by showing that
performance improves with more training data. Importantly, MInDI-3D matches the
performance of a 3D U-Net on real-world scans from 16 cancer patients across
distortion and task-based metrics. It also generalises to new CBCT scanner
geometries. Clinicians rated our model as sufficient for patient positioning
across all anatomical sites and found it preserved lung tumour boundaries well.

</details>


### [133] [Semantic-aware DropSplat: Adaptive Pruning of Redundant Gaussians for 3D Aerial-View Segmentation](https://arxiv.org/abs/2508.09626)
*Xu Tang,Junan Jia,Yijing Wang,Jingjing Ma,Xiangrong Zhang*

Main category: cs.CV

TL;DR: 论文提出了一种名为SAD-Splat的新方法，用于解决3D航空场景语义分割中的语义模糊问题，通过高斯点丢弃模块和高置信度伪标签生成流程提升性能。


<details>
  <summary>Details</summary>
Motivation: 传统方法在3D航空场景语义分割中因尺度变化和结构遮挡导致语义模糊，限制了分割精度和一致性。

Method: 提出高斯点丢弃模块，结合语义置信度估计和可学习的稀疏机制；引入高置信度伪标签生成流程，利用2D基础模型增强监督。

Result: SAD-Splat在分割精度和表示紧凑性之间取得了良好平衡，实验验证了其有效性。

Conclusion: SAD-Splat为3D航空场景理解提供了高效且可扩展的解决方案。

Abstract: In the task of 3D Aerial-view Scene Semantic Segmentation (3D-AVS-SS),
traditional methods struggle to address semantic ambiguity caused by scale
variations and structural occlusions in aerial images. This limits their
segmentation accuracy and consistency. To tackle these challenges, we propose a
novel 3D-AVS-SS approach named SAD-Splat. Our method introduces a Gaussian
point drop module, which integrates semantic confidence estimation with a
learnable sparsity mechanism based on the Hard Concrete distribution. This
module effectively eliminates redundant and semantically ambiguous Gaussian
points, enhancing both segmentation performance and representation compactness.
Furthermore, SAD-Splat incorporates a high-confidence pseudo-label generation
pipeline. It leverages 2D foundation models to enhance supervision when
ground-truth labels are limited, thereby further improving segmentation
accuracy. To advance research in this domain, we introduce a challenging
benchmark dataset: 3D Aerial Semantic (3D-AS), which encompasses diverse
real-world aerial scenes with sparse annotations. Experimental results
demonstrate that SAD-Splat achieves an excellent balance between segmentation
accuracy and representation compactness. It offers an efficient and scalable
solution for 3D aerial scene understanding.

</details>


### [134] [Enhancing Monocular 3D Hand Reconstruction with Learned Texture Priors](https://arxiv.org/abs/2508.09629)
*Giorgos Karvounas,Nikolaos Kyriazis,Iason Oikonomidis,Georgios Pavlakos,Antonis A. Argyros*

Main category: cs.CV

TL;DR: 论文提出了一种轻量级纹理模块，通过纹理对齐提升单目3D手部重建的精度和真实感。


<details>
  <summary>Details</summary>
Motivation: 现有高精度模型中，预测的手部几何与图像外观的覆盖往往不完美，纹理对齐可能是一个未被充分利用的监督信号。

Method: 提出一个轻量级纹理模块，将像素观测嵌入UV纹理空间，并引入密集对齐损失函数，结合可微分渲染和已知拓扑的3D手部网格模型。

Result: 该方法提升了HaMeR模型的精度和真实感，验证了外观引导对齐的价值。

Conclusion: 纹理对齐可以作为手部重建中的有效监督信号，提升模型性能。

Abstract: We revisit the role of texture in monocular 3D hand reconstruction, not as an
afterthought for photorealism, but as a dense, spatially grounded cue that can
actively support pose and shape estimation. Our observation is simple: even in
high-performing models, the overlay between predicted hand geometry and image
appearance is often imperfect, suggesting that texture alignment may be an
underused supervisory signal. We propose a lightweight texture module that
embeds per-pixel observations into UV texture space and enables a novel dense
alignment loss between predicted and observed hand appearances. Our approach
assumes access to a differentiable rendering pipeline and a model that maps
images to 3D hand meshes with known topology, allowing us to back-project a
textured hand onto the image and perform pixel-based alignment. The module is
self-contained and easily pluggable into existing reconstruction pipelines. To
isolate and highlight the value of texture-guided supervision, we augment
HaMeR, a high-performing yet unadorned transformer architecture for 3D hand
pose estimation. The resulting system improves both accuracy and realism,
demonstrating the value of appearance-guided alignment in hand reconstruction.

</details>


### [135] [Preacher: Paper-to-Video Agentic System](https://arxiv.org/abs/2508.09632)
*Jingwei Liu,Ling Yang,Hao Luo,Fan Wang Hongyan Li,Mengdi Wang*

Main category: cs.CV

TL;DR: 论文提出了一种名为Preacher的论文转视频系统，解决了现有视频生成模型的局限性，如上下文窗口有限、视频时长固定、风格单一和缺乏领域知识。


<details>
  <summary>Details</summary>
Motivation: 现有视频生成模型在生成论文摘要视频时存在多种限制，如上下文窗口有限、视频时长固定、风格单一和缺乏领域知识，亟需一种更灵活、专业的解决方案。

Method: Preacher采用自上而下的方法分解、总结和重构论文，再通过自下而上的视频生成，结合渐进式思维链（P-CoT）进行细粒度规划，生成多样化的视频片段。

Result: Preacher在五个研究领域成功生成了高质量的视频摘要，表现优于现有视频生成模型。

Conclusion: Preacher通过创新的分解与生成方法，显著提升了论文转视频任务的质量和灵活性，代码将开源。

Abstract: The paper-to-video task converts a research paper into a structured video
abstract, distilling key concepts, methods, and conclusions into an accessible,
well-organized format. While state-of-the-art video generation models
demonstrate potential, they are constrained by limited context windows, rigid
video duration constraints, limited stylistic diversity, and an inability to
represent domain-specific knowledge. To address these limitations, we introduce
Preacher, the first paper-to-video agentic system. Preacher employs a top-down
approach to decompose, summarize, and reformulate the paper, followed by
bottom-up video generation, synthesizing diverse video segments into a coherent
abstract. To align cross-modal representations, we define key scenes and
introduce a Progressive Chain of Thought (P-CoT) for granular, iterative
planning. Preacher successfully generates high-quality video abstracts across
five research fields, demonstrating expertise beyond current video generation
models. Code will be released at: https://github.com/GenVerse/Paper2Video

</details>


### [136] [Multi-Contrast Fusion Module: An attention mechanism integrating multi-contrast features for fetal torso plane classification](https://arxiv.org/abs/2508.09644)
*Shengjun Zhu,Siyu Liu,Runqing Xiong,Liping Zheng,Duo Ma,Rongshang Chen,Jiaxin Cai*

Main category: cs.CV

TL;DR: 提出了一种多对比度融合模块（MCFM），用于提升胎儿躯干平面超声图像的识别性能，通过多对比度注意力机制增强特征建模，显著提高分类准确性和临床可靠性。


<details>
  <summary>Details</summary>
Motivation: 超声图像的低对比度和模糊纹理细节限制了胎儿躯干平面的准确识别，影响产前诊断的可靠性。

Method: 设计了MCFM模块，专注于神经网络底层处理原始超声数据，通过多对比度条件下的注意力权重分配增强特征提取，同时保持低参数开销。

Result: 在胎儿躯干平面超声图像数据集上验证，MCFM显著提升识别性能，模型复杂度增加极小，且能更好捕捉细微解剖结构。

Conclusion: MCFM为超声图像中的胎儿躯干平面识别提供了有效解决方案，支持临床更准确和一致的诊断，具有临床应用潜力。

Abstract: Purpose: Prenatal ultrasound is a key tool in evaluating fetal structural
development and detecting abnormalities, contributing to reduced perinatal
complications and improved neonatal survival. Accurate identification of
standard fetal torso planes is essential for reliable assessment and
personalized prenatal care. However, limitations such as low contrast and
unclear texture details in ultrasound imaging pose significant challenges for
fine-grained anatomical recognition. Methods: We propose a novel Multi-Contrast
Fusion Module (MCFM) to enhance the model's ability to extract detailed
information from ultrasound images. MCFM operates exclusively on the lower
layers of the neural network, directly processing raw ultrasound data. By
assigning attention weights to image representations under different contrast
conditions, the module enhances feature modeling while explicitly maintaining
minimal parameter overhead. Results: The proposed MCFM was evaluated on a
curated dataset of fetal torso plane ultrasound images. Experimental results
demonstrate that MCFM substantially improves recognition performance, with a
minimal increase in model complexity. The integration of multi-contrast
attention enables the model to better capture subtle anatomical structures,
contributing to higher classification accuracy and clinical reliability.
Conclusions: Our method provides an effective solution for improving fetal
torso plane recognition in ultrasound imaging. By enhancing feature
representation through multi-contrast fusion, the proposed approach supports
clinicians in achieving more accurate and consistent diagnoses, demonstrating
strong potential for clinical adoption in prenatal screening. The codes are
available at https://github.com/sysll/MCFM.

</details>


### [137] [Multi-Sequence Parotid Gland Lesion Segmentation via Expert Text-Guided Segment Anything Model](https://arxiv.org/abs/2508.09645)
*Zhongyuan Wu,Chuan-Xian Ren,Yu Wang,Xiaohua Ban,Jianning Xiao,Xiaohui Duan*

Main category: cs.CV

TL;DR: PG-SAM结合专家诊断文本和跨序列注意力模块，显著提升了腮腺病变分割的准确性。


<details>
  <summary>Details</summary>
Motivation: 腮腺病变分割因病变大小和边界复杂而困难，现有方法依赖精确提示且忽略专家知识。

Method: 提出专家诊断文本引导的提示生成模块和跨序列注意力模块，结合多模态信息进行分割。

Result: 在三个独立临床中心验证中，PG-SAM达到最先进性能。

Conclusion: PG-SAM验证了专家诊断文本在临床图像分割中的有效性。

Abstract: Parotid gland lesion segmentation is essential for the treatment of parotid
gland diseases. However, due to the variable size and complex lesion
boundaries, accurate parotid gland lesion segmentation remains challenging.
Recently, the Segment Anything Model (SAM) fine-tuning has shown remarkable
performance in the field of medical image segmentation. Nevertheless, SAM's
interaction segmentation model relies heavily on precise lesion prompts
(points, boxes, masks, etc.), which are very difficult to obtain in real-world
applications. Besides, current medical image segmentation methods are
automatically generated, ignoring the domain knowledge of medical experts when
performing segmentation. To address these limitations, we propose the parotid
gland segment anything model (PG-SAM), an expert diagnosis text-guided SAM
incorporating expert domain knowledge for cross-sequence parotid gland lesion
segmentation. Specifically, we first propose an expert diagnosis report guided
prompt generation module that can automatically generate prompt information
containing the prior domain knowledge to guide the subsequent lesion
segmentation process. Then, we introduce a cross-sequence attention module,
which integrates the complementary information of different modalities to
enhance the segmentation effect. Finally, the multi-sequence image features and
generated prompts are feed into the decoder to get segmentation result.
Experimental results demonstrate that PG-SAM achieves state-of-the-art
performance in parotid gland lesion segmentation across three independent
clinical centers, validating its clinical applicability and the effectiveness
of diagnostic text for enhancing image segmentation in real-world clinical
settings.

</details>


### [138] [The Brain Resection Multimodal Image Registration (ReMIND2Reg) 2025 Challenge](https://arxiv.org/abs/2508.09649)
*Reuben Dorent,Laura Rigolo,Colin P. Galvin,Junyu Chen,Mattias P. Heinrich,Aaron Carass,Olivier Colliot,Demian Wassermann,Alexandra Golby,Tina Kapur,William Wells*

Main category: cs.CV

TL;DR: ReMIND2Reg 2025挑战赛旨在通过提供大规模公开基准数据集，推动多模态图像配准算法在脑肿瘤手术中的应用，解决脑移位导致的导航不准确问题。


<details>
  <summary>Details</summary>
Motivation: 脑肿瘤手术中，术前MRI导航系统因脑移位失去准确性，需通过配准术中超声与术前MRI恢复空间精度。

Method: 挑战赛提供99例训练数据、5例验证数据和10例测试数据（含配对3D ceT1 MRI、T2 MRI和术中超声），评估基于无标注数据的配准算法性能。

Result: 评估指标包括目标配准误差（TRE）、最差地标错位鲁棒性（TRE30）和运行时间。

Conclusion: ReMIND2Reg挑战赛为临床关键问题提供标准化评估框架，促进稳健、通用且可临床部署的多模态配准算法发展。

Abstract: Accurate intraoperative image guidance is critical for achieving maximal safe
resection in brain tumor surgery, yet neuronavigation systems based on
preoperative MRI lose accuracy during the procedure due to brain shift.
Aligning post-resection intraoperative ultrasound (iUS) with preoperative MRI
can restore spatial accuracy by estimating brain shift deformations, but it
remains a challenging problem given the large anatomical and topological
changes and substantial modality intensity gap. The ReMIND2Reg 2025 Challenge
provides the largest public benchmark for this task, built upon the ReMIND
dataset. It offers 99 training cases, 5 validation cases, and 10 private test
cases comprising paired 3D ceT1 MRI, T2 MRI, and post-resection 3D iUS volumes.
Data are provided without annotations for training, while validation and test
performance are evaluated on manually annotated anatomical landmarks. Metrics
include target registration error (TRE), robustness to worst-case landmark
misalignment (TRE30), and runtime. By establishing a standardized evaluation
framework for this clinically critical and technically complex problem,
ReMIND2Reg aims to accelerate the development of robust, generalizable, and
clinically deployable multimodal registration algorithms for image-guided
neurosurgery.

</details>


### [139] [TOTNet: Occlusion-Aware Temporal Tracking for Robust Ball Detection in Sports Videos](https://arxiv.org/abs/2508.09650)
*Hao Xu,Arbind Agrahari Baniya,Sam Wells,Mohamed Reda Bouadjenek,Richard Dazely,Sunil Aryal*

Main category: cs.CV

TL;DR: TOTNet是一种用于体育视频分析的时序遮挡跟踪网络，通过3D卷积、可见性加权损失和遮挡增强技术，显著提升了遮挡情况下的球体跟踪性能。


<details>
  <summary>Details</summary>
Motivation: 体育视频分析中，遮挡情况下的球体跟踪是一个关键挑战，影响了事件检测和裁判等任务。

Method: TOTNet采用3D卷积、可见性加权损失和遮挡增强技术，并结合新的遮挡丰富的乒乓球数据集TTA进行训练。

Result: 在四个数据集的评估中，TOTNet显著优于现有方法，RMSE从37.30降至7.19，完全遮挡帧的准确率从0.63提升至0.80。

Conclusion: TOTNet在快速体育场景中表现出色，适用于离线体育分析。

Abstract: Robust ball tracking under occlusion remains a key challenge in sports video
analysis, affecting tasks like event detection and officiating. We present
TOTNet, a Temporal Occlusion Tracking Network that leverages 3D convolutions,
visibility-weighted loss, and occlusion augmentation to improve performance
under partial and full occlusions. Developed in collaboration with Paralympics
Australia, TOTNet is designed for real-world sports analytics. We introduce
TTA, a new occlusion-rich table tennis dataset collected from
professional-level Paralympic matches, comprising 9,159 samples with 1,996
occlusion cases. Evaluated on four datasets across tennis, badminton, and table
tennis, TOTNet significantly outperforms prior state-of-the-art methods,
reducing RMSE from 37.30 to 7.19 and improving accuracy on fully occluded
frames from 0.63 to 0.80. These results demonstrate TOTNets effectiveness for
offline sports analytics in fast-paced scenarios. Code and data
access:\href{https://github.com/AugustRushG/TOTNet}{AugustRushG/TOTNet}.

</details>


### [140] [Noise-adapted Neural Operator for Robust Non-Line-of-Sight Imaging](https://arxiv.org/abs/2508.09655)
*Lianfang Wang,Kuilin Qin,Xueying Liu,Huibin Chang,Yong Wang,Yuping Duan*

Main category: cs.CV

TL;DR: 该论文提出了一种参数化逆问题框架，用于3D成像重建中的大规模线性问题，通过噪声估计模块和参数化神经算子实现快速重建，并结合全局与局部时空数据特征提升准确性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 非视距（NLOS）成像中，间接光信号微弱且易受噪声干扰，需结合物理过程实现精确重建。

Method: 采用噪声估计模块评估瞬态数据噪声，开发参数化神经算子近似逆映射，并通过深度算法展开构建框架，融合全局与局部时空数据特征。

Result: 在模拟和真实数据集上的实验表明，该方法在快速扫描和稀疏照明点数据下表现优异。

Conclusion: 该方法为复杂场景下的NLOS成像提供了可行解决方案，兼具高效性和鲁棒性。

Abstract: Computational imaging, especially non-line-of-sight (NLOS) imaging, the
extraction of information from obscured or hidden scenes is achieved through
the utilization of indirect light signals resulting from multiple reflections
or scattering. The inherently weak nature of these signals, coupled with their
susceptibility to noise, necessitates the integration of physical processes to
ensure accurate reconstruction. This paper presents a parameterized inverse
problem framework tailored for large-scale linear problems in 3D imaging
reconstruction. Initially, a noise estimation module is employed to adaptively
assess the noise levels present in transient data. Subsequently, a
parameterized neural operator is developed to approximate the inverse mapping,
facilitating end-to-end rapid image reconstruction. Our 3D image reconstruction
framework, grounded in operator learning, is constructed through deep algorithm
unfolding, which not only provides commendable model interpretability but also
enables dynamic adaptation to varying noise levels in the acquired data,
thereby ensuring consistently robust and accurate reconstruction outcomes.
Furthermore, we introduce a novel method for the fusion of global and local
spatiotemporal data features. By integrating structural and detailed
information, this method significantly enhances both accuracy and robustness.
Comprehensive numerical experiments conducted on both simulated and real
datasets substantiate the efficacy of the proposed method. It demonstrates
remarkable performance with fast scanning data and sparse illumination point
data, offering a viable solution for NLOS imaging in complex scenarios.

</details>


### [141] [NegFaceDiff: The Power of Negative Context in Identity-Conditioned Diffusion for Synthetic Face Generation](https://arxiv.org/abs/2508.09661)
*Eduarda Caldeira,Naser Damer,Fadi Boutros*

Main category: cs.CV

TL;DR: NegFaceDiff是一种新的采样方法，通过在身份条件扩散过程中引入负条件，增强生成数据的身份分离性，从而提升人脸识别模型的性能。


<details>
  <summary>Details</summary>
Motivation: 解决现有身份条件扩散模型在生成数据时缺乏明确的类间分离机制，导致身份重叠和次优人脸识别性能的问题。

Method: 提出NegFaceDiff方法，利用负条件在扩散过程中引导模型远离不需要的特征，同时保持类内一致性。

Result: 实验表明，NegFaceDiff显著提升了生成数据的身份一致性和分离性（FDR从2.427提高到5.687），并使人脸识别系统在多个基准测试中表现更优。

Conclusion: NegFaceDiff通过负条件采样有效提升了生成数据的质量，为人脸识别模型的训练提供了更优的合成数据解决方案。

Abstract: The use of synthetic data as an alternative to authentic datasets in face
recognition (FR) development has gained significant attention, addressing
privacy, ethical, and practical concerns associated with collecting and using
authentic data. Recent state-of-the-art approaches have proposed
identity-conditioned diffusion models to generate identity-consistent face
images, facilitating their use in training FR models. However, these methods
often lack explicit sampling mechanisms to enforce inter-class separability,
leading to identity overlap in the generated data and, consequently, suboptimal
FR performance. In this work, we introduce NegFaceDiff, a novel sampling method
that incorporates negative conditions into the identity-conditioned diffusion
process. NegFaceDiff enhances identity separation by leveraging negative
conditions that explicitly guide the model away from unwanted features while
preserving intra-class consistency. Extensive experiments demonstrate that
NegFaceDiff significantly improves the identity consistency and separability of
data generated by identity-conditioned diffusion models. Specifically, identity
separability, measured by the Fisher Discriminant Ratio (FDR), increases from
2.427 to 5.687. These improvements are reflected in FR systems trained on the
NegFaceDiff dataset, which outperform models trained on data generated without
negative conditions across multiple benchmarks.

</details>


### [142] [GSFixer: Improving 3D Gaussian Splatting with Reference-Guided Video Diffusion Priors](https://arxiv.org/abs/2508.09667)
*Xingyilang Yin,Qi Zhang,Jiahao Chang,Ying Feng,Qingnan Fan,Xi Yang,Chi-Man Pun,Huaqi Zhang,Xiaodong Cun*

Main category: cs.CV

TL;DR: GSFixer是一种新框架，通过参考引导的视频修复模型改进稀疏视图3D高斯泼溅（3DGS）重建的质量。


<details>
  <summary>Details</summary>
Motivation: 稀疏视图3DGS重建因信息不足导致明显伪影，现有生成先验方法难以保持与输入观察的一致性。

Method: 基于DiT的视频扩散模型，结合2D语义和3D几何特征，增强修复视图的语义一致性和3D一致性。

Result: GSFixer在3DGS伪影修复和稀疏视图3D重建中优于现有方法。

Conclusion: GSFixer通过多特征融合有效提升3DGS重建质量，并提出了新的评估基准DL3DV-Res。

Abstract: Reconstructing 3D scenes using 3D Gaussian Splatting (3DGS) from sparse views
is an ill-posed problem due to insufficient information, often resulting in
noticeable artifacts. While recent approaches have sought to leverage
generative priors to complete information for under-constrained regions, they
struggle to generate content that remains consistent with input observations.
To address this challenge, we propose GSFixer, a novel framework designed to
improve the quality of 3DGS representations reconstructed from sparse inputs.
The core of our approach is the reference-guided video restoration model, built
upon a DiT-based video diffusion model trained on paired artifact 3DGS renders
and clean frames with additional reference-based conditions. Considering the
input sparse views as references, our model integrates both 2D semantic
features and 3D geometric features of reference views extracted from the visual
geometry foundation model, enhancing the semantic coherence and 3D consistency
when fixing artifact novel views. Furthermore, considering the lack of suitable
benchmarks for 3DGS artifact restoration evaluation, we present DL3DV-Res which
contains artifact frames rendered using low-quality 3DGS. Extensive experiments
demonstrate our GSFixer outperforms current state-of-the-art methods in 3DGS
artifact restoration and sparse-view 3D reconstruction. Project page:
https://github.com/GVCLab/GSFixer.

</details>


### [143] [PaCo-FR: Patch-Pixel Aligned End-to-End Codebook Learning for Facial Representation Pre-training](https://arxiv.org/abs/2508.09691)
*Yin Xie,Zhichao Chen,Xiaoze Yu,Yongle Zhao,Xiang An,Kaicheng Yang,Zimin Ran,Jia Guo,Ziyong Feng,Jiankang Deng*

Main category: cs.CV

TL;DR: PaCo-FR是一种无监督框架，通过掩码图像建模和像素对齐解决面部表示预训练中的三大挑战，并在少量无标签数据下实现最佳性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法在捕捉面部特征、空间结构和数据利用效率方面存在不足，需要更高效且无需大量标注数据的解决方案。

Method: 结合掩码图像建模与像素对齐，采用结构化掩码策略、基于补丁的码书和空间一致性约束。

Result: 在多种面部分析任务中表现优异，尤其在姿态、遮挡和光照变化场景下显著提升性能。

Conclusion: PaCo-FR推动了面部表示学习，提供了一种高效、可扩展的解决方案，减少了对标注数据的依赖。

Abstract: Facial representation pre-training is crucial for tasks like facial
recognition, expression analysis, and virtual reality. However, existing
methods face three key challenges: (1) failing to capture distinct facial
features and fine-grained semantics, (2) ignoring the spatial structure
inherent to facial anatomy, and (3) inefficiently utilizing limited labeled
data. To overcome these, we introduce PaCo-FR, an unsupervised framework that
combines masked image modeling with patch-pixel alignment. Our approach
integrates three innovative components: (1) a structured masking strategy that
preserves spatial coherence by aligning with semantically meaningful facial
regions, (2) a novel patch-based codebook that enhances feature discrimination
with multiple candidate tokens, and (3) spatial consistency constraints that
preserve geometric relationships between facial components. PaCo-FR achieves
state-of-the-art performance across several facial analysis tasks with just 2
million unlabeled images for pre-training. Our method demonstrates significant
improvements, particularly in scenarios with varying poses, occlusions, and
lighting conditions. We believe this work advances facial representation
learning and offers a scalable, efficient solution that reduces reliance on
expensive annotated datasets, driving more effective facial analysis systems.

</details>


### [144] [Slot Attention-based Feature Filtering for Few-Shot Learning](https://arxiv.org/abs/2508.09699)
*Javier Rodenas,Eduardo Aguilar,Petia Radeva*

Main category: cs.CV

TL;DR: 论文提出了一种基于Slot Attention的特征过滤方法（SAFF），用于提升小样本学习性能，通过过滤无关特征减少混淆。


<details>
  <summary>Details</summary>
Motivation: 无关特征会显著降低小样本学习性能，导致分类混淆。

Method: 提出SAFF方法，结合Slot Attention机制和块嵌入，通过相似性矩阵过滤无关特征。

Result: 实验表明SAFF在多个小样本学习基准数据集上优于现有方法。

Conclusion: SAFF能有效过滤无关特征，提升小样本分类性能。

Abstract: Irrelevant features can significantly degrade few-shot learn ing performance.
This problem is used to match queries and support images based on meaningful
similarities despite the limited data. However, in this process, non-relevant
fea tures such as background elements can easily lead to confu sion and
misclassification. To address this issue, we pro pose Slot Attention-based
Feature Filtering for Few-Shot Learning (SAFF) that leverages slot attention
mechanisms to discriminate and filter weak features, thereby improving few-shot
classification performance. The key innovation of SAFF lies in its integration
of slot attention with patch em beddings, unifying class-aware slots into a
single attention mechanism to filter irrelevant features effectively. We intro
duce a similarity matrix that computes across support and query images to
quantify the relevance of filtered embed dings for classification. Through
experiments, we demon strate that Slot Attention performs better than other
atten tion mechanisms, capturing discriminative features while reducing
irrelevant information. We validate our approach through extensive experiments
on few-shot learning bench marks: CIFAR-FS, FC100, miniImageNet and tieredIma
geNet, outperforming several state-of-the-art methods.

</details>


### [145] [MangaDiT: Reference-Guided Line Art Colorization with Hierarchical Attention in Diffusion Transformers](https://arxiv.org/abs/2508.09709)
*Qianru Qiu,Jiafeng Mao,Kento Masui,Xueting Wang*

Main category: cs.CV

TL;DR: MangaDiT利用扩散变换器（DiT）和分层注意力机制，解决了参考引导线稿上色的区域级颜色一致性问题，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有方法在参考图像和目标图像姿态或动作不同时，区域级颜色一致性表现不佳，且依赖外部匹配标注。

Method: 提出MangaDiT模型，结合线稿和参考图像，采用分层注意力机制和动态注意力权重策略，增强区域级颜色对齐。

Result: 在两个基准数据集上，MangaDiT在定性和定量评估中均显著优于现有方法。

Conclusion: MangaDiT通过内部注意力机制隐式发现语义对应，解决了参考引导上色的关键问题，性能优越。

Abstract: Recent advances in diffusion models have significantly improved the
performance of reference-guided line art colorization. However, existing
methods still struggle with region-level color consistency, especially when the
reference and target images differ in character pose or motion. Instead of
relying on external matching annotations between the reference and target, we
propose to discover semantic correspondences implicitly through internal
attention mechanisms. In this paper, we present MangaDiT, a powerful model for
reference-guided line art colorization based on Diffusion Transformers (DiT).
Our model takes both line art and reference images as conditional inputs and
introduces a hierarchical attention mechanism with a dynamic attention
weighting strategy. This mechanism augments the vanilla attention with an
additional context-aware path that leverages pooled spatial features,
effectively expanding the model's receptive field and enhancing region-level
color alignment. Experiments on two benchmark datasets demonstrate that our
method significantly outperforms state-of-the-art approaches, achieving
superior performance in both qualitative and quantitative evaluations.

</details>


### [146] [NEURAL: Attention-Guided Pruning for Unified Multimodal Resource-Constrained Clinical Evaluation](https://arxiv.org/abs/2508.09715)
*Devvrat Joshi,Islem Rekik*

Main category: cs.CV

TL;DR: NEURAL是一种新颖的多模态医学影像数据压缩框架，通过语义引导压缩技术显著减少数据大小，同时保持高诊断性能。


<details>
  <summary>Details</summary>
Motivation: 解决资源受限临床环境中多模态医学影像数据的存储和传输挑战。

Method: 利用生成式视觉-语言模型的交叉注意力分数对胸部X光进行结构性修剪，生成高度压缩的图表示，并与临床报告的知识图融合。

Result: 在MIMIC-CXR和CheXpert Plus数据集上，NEURAL实现了93.4-97.7%的数据压缩，同时诊断AUC达到0.88-0.95，优于基线模型。

Conclusion: NEURAL在数据大小和临床效用之间取得了平衡，支持高效工作流程和远程放射学。

Abstract: The rapid growth of multimodal medical imaging data presents significant
storage and transmission challenges, particularly in resource-constrained
clinical settings. We propose NEURAL, a novel framework that addresses this by
using semantics-guided data compression. Our approach repurposes
cross-attention scores between the image and its radiological report from a
fine-tuned generative vision-language model to structurally prune chest X-rays,
preserving only diagnostically critical regions. This process transforms the
image into a highly compressed, graph representation. This unified graph-based
representation fuses the pruned visual graph with a knowledge graph derived
from the clinical report, creating a universal data structure that simplifies
downstream modeling. Validated on the MIMIC-CXR and CheXpert Plus dataset for
pneumonia detection, NEURAL achieves a 93.4-97.7\% reduction in image data size
while maintaining a high diagnostic performance of 0.88-0.95 AUC, outperforming
other baseline models that use uncompressed data. By creating a persistent,
task-agnostic data asset, NEURAL resolves the trade-off between data size and
clinical utility, enabling efficient workflows and teleradiology without
sacrificing performance. Our NEURAL code is available at
https://github.com/basiralab/NEURAL.

</details>


### [147] [Multimodal Sheaf-based Network for Glioblastoma Molecular Subtype Prediction](https://arxiv.org/abs/2508.09717)
*Shekhnaz Idrissova,Islem Rekik*

Main category: cs.CV

TL;DR: 提出了一种基于sheaf的框架，用于MRI和组织病理学数据的结构感知和一致性融合，解决了现有方法在多模态数据融合中的局限性。


<details>
  <summary>Details</summary>
Motivation: 胶质母细胞瘤分子亚型分类对靶向治疗选择至关重要，但现有方法依赖侵入性组织提取且多模态融合效果不佳。

Method: 采用sheaf-based框架，结合MRI和组织病理学数据，保留共享结构信息并处理缺失数据。

Result: 模型优于基线方法，在数据缺失或不完整情况下表现稳健。

Conclusion: 该框架为快速诊断的虚拟活检工具开发提供了支持。

Abstract: Glioblastoma is a highly invasive brain tumor with rapid progression rates.
Recent studies have shown that glioblastoma molecular subtype classification
serves as a significant biomarker for effective targeted therapy selection.
However, this classification currently requires invasive tissue extraction for
comprehensive histopathological analysis. Existing multimodal approaches
combining MRI and histopathology images are limited and lack robust mechanisms
for preserving shared structural information across modalities. In particular,
graph-based models often fail to retain discriminative features within
heterogeneous graphs, and structural reconstruction mechanisms for handling
missing or incomplete modality data are largely underexplored. To address these
limitations, we propose a novel sheaf-based framework for structure-aware and
consistent fusion of MRI and histopathology data. Our model outperforms
baseline methods and demonstrates robustness in incomplete or missing data
scenarios, contributing to the development of virtual biopsy tools for rapid
diagnostics. Our source code is available at
https://github.com/basiralab/MMSN/.

</details>


### [148] [Seeing, Listening, Remembering, and Reasoning: A Multimodal Agent with Long-Term Memory](https://arxiv.org/abs/2508.09736)
*Lin Long,Yichen He,Wentao Ye,Yiyuan Pan,Yuan Lin,Hang Li,Junbo Zhao,Wei Li*

Main category: cs.CV

TL;DR: M3-Agent是一种新型多模态代理框架，具备长期记忆能力，能够处理实时视觉和听觉输入，并通过强化学习在基准测试中表现优异。


<details>
  <summary>Details</summary>
Motivation: 研究旨在开发一种具备人类类似长期记忆的多模态代理，以提升其在复杂任务中的理解和推理能力。

Method: M3-Agent采用实体中心的多模态记忆格式，结合强化学习训练，并通过M3-Bench基准测试评估其性能。

Result: M3-Agent在M3-Bench-robot、M3-Bench-web和VideoMME-long基准测试中分别比基线模型高出6.7%、7.7%和5.3%的准确率。

Conclusion: M3-Agent在多模态代理的长期记忆和推理能力方面取得了显著进展，为实际应用提供了有价值的参考。

Abstract: We introduce M3-Agent, a novel multimodal agent framework equipped with
long-term memory. Like humans, M3-Agent can process real-time visual and
auditory inputs to build and update its long-term memory. Beyond episodic
memory, it also develops semantic memory, enabling it to accumulate world
knowledge over time. Its memory is organized in an entity-centric, multimodal
format, allowing deeper and more consistent understanding of the environment.
Given an instruction, M3-Agent autonomously performs multi-turn, iterative
reasoning and retrieves relevant information from memory to accomplish the
task. To evaluate memory effectiveness and memory-based reasoning in multimodal
agents, we develop M3-Bench, a new long-video question answering benchmark.
M3-Bench comprises 100 newly recorded real-world videos captured from a robot's
perspective (M3-Bench-robot) and 929 web-sourced videos across diverse
scenarios (M3-Bench-web). We annotate question-answer pairs designed to test
key capabilities essential for agent applications, such as human understanding,
general knowledge extraction, and cross-modal reasoning. Experimental results
show that M3-Agent, trained via reinforcement learning, outperforms the
strongest baseline, a prompting agent using Gemini-1.5-pro and GPT-4o,
achieving 6.7%, 7.7%, and 5.3% higher accuracy on M3-Bench-robot, M3-Bench-web
and VideoMME-long, respectively. Our work advances the multimodal agents toward
more human-like long-term memory and provides insights into their practical
design. Model, code and data are available at
https://github.com/bytedance-seed/m3-agent

</details>


### [149] [Region-to-Region: Enhancing Generative Image Harmonization with Adaptive Regional Injection](https://arxiv.org/abs/2508.09746)
*Zhiqiu Zhang,Dongqi Fan,Mingjie Wang,Qiang Tang,Jian Yang,Zili Yi*

Main category: cs.CV

TL;DR: 论文提出了一种基于区域到区域变换的图像协调方法R2R，通过Clear-VAE和Harmony Controller提升细节保留和协调能力，并构建了新数据集RPHarmony。


<details>
  <summary>Details</summary>
Motivation: 现有基于LDM的图像协调方法在细节保留和协调能力上存在不足，且合成数据集缺乏真实光照变化。

Method: 提出Region-to-Region变换，设计Clear-VAE和Harmony Controller（含MACA模块），并采用Random Poisson Blending构建新数据集RPHarmony。

Result: 实验表明，该方法在定量指标和视觉协调性上优于其他方法，且新数据集提升了模型在真实场景中的表现。

Conclusion: R2R方法有效解决了现有问题，并通过开源代码、数据集和模型权重推动研究进展。

Abstract: The goal of image harmonization is to adjust the foreground in a composite
image to achieve visual consistency with the background. Recently, latent
diffusion model (LDM) are applied for harmonization, achieving remarkable
results. However, LDM-based harmonization faces challenges in detail
preservation and limited harmonization ability. Additionally, current synthetic
datasets rely on color transfer, which lacks local variations and fails to
capture complex real-world lighting conditions. To enhance harmonization
capabilities, we propose the Region-to-Region transformation. By injecting
information from appropriate regions into the foreground, this approach
preserves original details while achieving image harmonization or, conversely,
generating new composite data. From this perspective, We propose a novel model
R2R. Specifically, we design Clear-VAE to preserve high-frequency details in
the foreground using Adaptive Filter while eliminating disharmonious elements.
To further enhance harmonization, we introduce the Harmony Controller with
Mask-aware Adaptive Channel Attention (MACA), which dynamically adjusts the
foreground based on the channel importance of both foreground and background
regions. To address the limitation of existing datasets, we propose Random
Poisson Blending, which transfers color and lighting information from a
suitable region to the foreground, thereby generating more diverse and
challenging synthetic images. Using this method, we construct a new synthetic
dataset, RPHarmony. Experiments demonstrate the superiority of our method over
other methods in both quantitative metrics and visual harmony. Moreover, our
dataset helps the model generate more realistic images in real examples. Our
code, dataset, and model weights have all been released for open access.

</details>


### [150] [MoIIE: Mixture of Intra- and Inter-Modality Experts for Large Vision Language Models](https://arxiv.org/abs/2508.09779)
*Dianyi Wang,Siyuan Wang,Zejun Li,Yikun Wang,Yitong Li,Duyu Tang,Xiaoyu Shen,Xuanjing Huang,Zhongyu Wei*

Main category: cs.CV

TL;DR: 提出了一种稀疏混合专家架构（MoIIE），用于提升大型视觉语言模型（LVLM）的效率与性能，通过模态引导的路由机制联合学习模态内特征与跨模态关联。


<details>
  <summary>Details</summary>
Motivation: 解决密集LVLM计算成本高的问题，同时探索稀疏MoE架构在多模态任务中的应用挑战。

Method: 引入MoIIE架构，结合模态内与跨模态专家路由，采用两阶段训练策略激活MoE与多模态能力。

Result: 实验表明MoIIE模型在参数效率与性能上优于现有开源MoE多模态模型。

Conclusion: MoIIE为LVLM提供了一种高效且通用的解决方案，显著提升多模态任务表现。

Abstract: Large Vision-Language Models (LVLMs) have demonstrated remarkable performance
across multi-modal tasks by scaling model size and training data. However,
these dense LVLMs incur significant computational costs and motivate the
exploration of sparse Mixture of Experts (MoE) architectures. While MoE improve
parameter efficiency, effectively applying MoE to simultaneously model
modality-specific features and cross-modal associations in LVLMs remains
challenging. In this work, we propose to incorporate Mixture of Intra- and
Inter-Modality Experts (MoIIE) to LVLMs. For each token, expert routing is
guided by its modality, directing tokens to their respective intra-modality
experts as well as a shared pool of inter-modality experts, enabling the model
to jointly learn rich intra-modal features and cross-modal interactions. We
further introduce an effective and straightforward two-stage training strategy,
which facilitates the direct activation of both MoE and multi-modal
capabilities. Extensive experiments across different data scales and LLM
backbone demonstrate the effectiveness, efficiency and generality of our
approach. Notably, our MoIIE models with 5.5B and 11.3B activated parameters
match or even surpass the performance of existing advanced open-source MoE-LLMs
based multi-modal models that involve more activated parameters. The code is
available at https://github.com/AlenjandroWang/MoIIE.

</details>


### [151] [Combinative Matching for Geometric Shape Assembly](https://arxiv.org/abs/2508.09780)
*Nahyuk Lee,Juhong Min,Junhong Lee,Chunghyun Park,Minsu Cho*

Main category: cs.CV

TL;DR: 提出了一种新的形状匹配方法“组合匹配”，用于几何形状装配中的互锁部件组合。


<details>
  <summary>Details</summary>
Motivation: 传统方法依赖表面形状对齐，而新方法明确建模互锁形状的‘相同表面形状’和‘相反体积占用’特性。

Method: 通过等变神经网络学习表面形状相同但体积占用相反的区域的对应关系，并估计形状方向以对齐旋转区域。

Result: 显著减少了匹配中的局部模糊性，实现了部件的稳健组合，实验结果表明其性能优于现有技术。

Conclusion: 组合匹配方法在几何装配任务中表现出色，为互锁部件组合提供了新思路。

Abstract: This paper introduces a new shape-matching methodology, combinative matching,
to combine interlocking parts for geometric shape assembly. Previous methods
for geometric assembly typically rely on aligning parts by finding identical
surfaces between the parts as in conventional shape matching and registration.
In contrast, we explicitly model two distinct properties of interlocking
shapes: 'identical surface shape' and 'opposite volume occupancy.' Our method
thus learns to establish correspondences across regions where their surface
shapes appear identical but their volumes occupy the inverted space to each
other. To facilitate this process, we also learn to align regions in rotation
by estimating their shape orientations via equivariant neural networks. The
proposed approach significantly reduces local ambiguities in matching and
allows a robust combination of parts in assembly. Experimental results on
geometric assembly benchmarks demonstrate the efficacy of our method,
consistently outperforming the state of the art. Project page:
https://nahyuklee.github.io/cmnet.

</details>


### [152] [DSS-Prompt: Dynamic-Static Synergistic Prompting for Few-Shot Class-Incremental Learning](https://arxiv.org/abs/2508.09785)
*Linpu He,Yanan Li,Bingze Li,Elvis Han Cui,Donghui Wang*

Main category: cs.CV

TL;DR: DSS-Prompt是一种简单有效的方法，通过提示微调预训练的视觉Transformer，用于少样本类增量学习（FSCIL），结合静态和动态提示提升性能。


<details>
  <summary>Details</summary>
Motivation: 解决少样本类增量学习中从预训练模型高效学习新概念且不遗忘旧知识的问题。

Method: 利用静态提示弥合预训练与下游任务的领域差距，动态提示捕捉实例感知语义，并通过多模态模型生成输入相关的动态提示。

Result: 在四个基准测试中，DSS-Prompt性能优于现有方法，缓解了灾难性遗忘问题。

Conclusion: DSS-Prompt通过提示微调预训练模型，显著提升了FSCIL任务的性能。

Abstract: Learning from large-scale pre-trained models with strong generalization
ability has shown remarkable success in a wide range of downstream tasks
recently, but it is still underexplored in the challenging few-shot
class-incremental learning (FSCIL) task. It aims to continually learn new
concepts from limited training samples without forgetting the old ones at the
same time. In this paper, we introduce DSS-Prompt, a simple yet effective
approach that transforms the pre-trained Vision Transformer with minimal
modifications in the way of prompts into a strong FSCIL classifier. Concretely,
we synergistically utilize two complementary types of prompts in each
Transformer block: static prompts to bridge the domain gap between the
pre-training and downstream datasets, thus enabling better adaption; and
dynamic prompts to capture instance-aware semantics, thus enabling easy
transfer from base to novel classes. Specially, to generate dynamic prompts, we
leverage a pre-trained multi-modal model to extract input-related diverse
semantics, thereby generating complementary input-aware prompts, and then
adaptively adjust their importance across different layers. In this way, on top
of the prompted visual embeddings, a simple prototype classifier can beat
state-of-the-arts without further training on the incremental tasks. We conduct
extensive experiments on four benchmarks to validate the effectiveness of our
DSS-Prompt and show that it consistently achieves better performance than
existing approaches on all datasets and can alleviate the catastrophic
forgetting issue as well.

</details>


### [153] [MeMoSORT: Memory-Assisted Filtering and Motion-Adaptive Association Metric for Multi-Person Tracking](https://arxiv.org/abs/2508.09796)
*Yingjie Wang,Zhixing Wang,Le Zheng,Tianxiao Liu,Roujing Li,Xueyao Hu*

Main category: cs.CV

TL;DR: MeMoSORT是一种实时多目标跟踪方法，通过改进卡尔曼滤波和自适应IoU匹配提升性能。


<details>
  <summary>Details</summary>
Motivation: 传统跟踪方法因卡尔曼滤波和刚性IoU匹配的局限性，在复杂运动和遮挡场景下表现不佳。

Method: 提出MeKF（记忆辅助卡尔曼滤波）和Mo-IoU（运动自适应IoU）以优化跟踪。

Result: 在DanceTrack和SportsMOT数据集上分别达到67.9%和82.1%的HOTA分数。

Conclusion: MeMoSORT在复杂场景下表现优异，实现了实时高性能跟踪。

Abstract: Multi-object tracking (MOT) in human-dominant scenarios, which involves
continuously tracking multiple people within video sequences, remains a
significant challenge in computer vision due to targets' complex motion and
severe occlusions. Conventional tracking-by-detection methods are fundamentally
limited by their reliance on Kalman filter (KF) and rigid Intersection over
Union (IoU)-based association. The motion model in KF often mismatches
real-world object dynamics, causing filtering errors, while rigid association
struggles under occlusions, leading to identity switches or target loss. To
address these issues, we propose MeMoSORT, a simple, online, and real-time MOT
tracker with two key innovations. First, the Memory-assisted Kalman filter
(MeKF) uses memory-augmented neural networks to compensate for mismatches
between assumed and actual object motion. Second, the Motion-adaptive IoU
(Mo-IoU) adaptively expands the matching space and incorporates height
similarity to reduce the influence of detection errors and association
failures, while remaining lightweight. Experiments on DanceTrack and SportsMOT
show that MeMoSORT achieves state-of-the-art performance, with HOTA scores of
67.9\% and 82.1\%, respectively.

</details>


### [154] [MUJICA: Reforming SISR Models for PBR Material Super-Resolution via Cross-Map Attention](https://arxiv.org/abs/2508.09802)
*Xin Du,Maoyuan Xu,Zhi Ying*

Main category: cs.CV

TL;DR: MUJICA是一种基于跨图注意力的多模态上采样方法，用于提升PBR材料的超分辨率效果，解决了现有方法的跨图不一致性和模态特征建模不足问题。


<details>
  <summary>Details</summary>
Motivation: 现有单图像超分辨率方法在PBR材料上采样中存在跨图不一致、模态特征建模不足和数据分布偏移导致的泛化能力受限问题。

Method: 提出MUJICA，一种灵活的适配器，基于预训练的Swin-transformer SISR模型，通过跨图注意力融合特征，保持预训练模型的强大重建能力。

Result: 在PSNR、SSIM和LPIPS指标上表现优异，同时保持跨图一致性，实验证明其在有限资源下高效训练，并在PBR材料数据集上达到先进水平。

Conclusion: MUJICA通过跨图注意力有效提升了PBR材料超分辨率的效果，为现代3D图形应用提供了实用解决方案。

Abstract: Physically Based Rendering (PBR) materials are typically characterized by
multiple 2D texture maps such as basecolor, normal, metallic, and roughness
which encode spatially-varying bi-directional reflectance distribution function
(SVBRDF) parameters to model surface reflectance properties and microfacet
interactions. Upscaling SVBRDF material is valuable for modern 3D graphics
applications. However, existing Single Image Super-Resolution (SISR) methods
struggle with cross-map inconsistency, inadequate modeling of modality-specific
features, and limited generalization due to data distribution shifts. In this
work, we propose Multi-modal Upscaling Joint Inference via Cross-map Attention
(MUJICA), a flexible adapter that reforms pre-trained Swin-transformer-based
SISR models for PBR material super-resolution. MUJICA is seamlessly attached
after the pre-trained and frozen SISR backbone. It leverages cross-map
attention to fuse features while preserving remarkable reconstruction ability
of the pre-trained SISR model. Applied to SISR models such as SwinIR, DRCT, and
HMANet, MUJICA improves PSNR, SSIM, and LPIPS scores while preserving cross-map
consistency. Experiments demonstrate that MUJICA enables efficient training
even with limited resources and delivers state-of-the-art performance on PBR
material datasets.

</details>


### [155] [Automated Segmentation of Coronal Brain Tissue Slabs for 3D Neuropathology](https://arxiv.org/abs/2508.09805)
*Jonathan Williams Ramirez,Dina Zemlyanker,Lucas Deden-Binder,Rogeny Herisse,Erendira Garcia Pallares,Karthik Gopinath,Harshvardhan Gazula,Christopher Mount,Liana N. Kozanno,Michael S. Marshall,Theresa R. Connors,Matthew P. Frosch,Mark Montine,Derek H. Oakley,Christine L. Mac Donald,C. Dirk Keene,Bradley T. Hyman,Juan Eugenio Iglesias*

Main category: cs.CV

TL;DR: 提出了一种基于U-Net的深度学习模型，用于自动化分割脑组织照片中的组织，解决了传统方法依赖昂贵人工干预的问题。


<details>
  <summary>Details</summary>
Motivation: 脑组织照片的分割通常需要人工干预，成本高昂，因此需要一种自动化工具来提高效率。

Method: 使用U-Net架构，结合1,414张手动标记的图像和2,000张合成的MRI图像进行训练，以提高模型的泛化能力。

Result: 模型在未见过的照片上表现优异，Dice分数中位数超过0.98，接近人工标注的精度。

Conclusion: 该工具公开可用，能够高效替代人工分割，适用于脑库和神经病理学实验室。

Abstract: Advances in image registration and machine learning have recently enabled
volumetric analysis of \emph{postmortem} brain tissue from conventional
photographs of coronal slabs, which are routinely collected in brain banks and
neuropathology laboratories worldwide. One caveat of this methodology is the
requirement of segmentation of the tissue from photographs, which currently
requires costly manual intervention. In this article, we present a deep
learning model to automate this process. The automatic segmentation tool relies
on a U-Net architecture that was trained with a combination of
\textit{(i)}1,414 manually segmented images of both fixed and fresh tissue,
from specimens with varying diagnoses, photographed at two different sites; and
\textit{(ii)}~2,000 synthetic images with randomized contrast and corresponding
masks generated from MRI scans for improved generalizability to unseen
photographic setups. Automated model predictions on a subset of photographs not
seen in training were analyzed to estimate performance compared to manual
labels -- including both inter- and intra-rater variability. Our model achieved
a median Dice score over 0.98, mean surface distance under 0.4~mm, and 95\%
Hausdorff distance under 1.60~mm, which approaches inter-/intra-rater levels.
Our tool is publicly available at surfer.nmr.mgh.harvard.edu/fswiki/PhotoTools.

</details>


### [156] [Poaching Hotspot Identification Using Satellite Imagery](https://arxiv.org/abs/2508.09812)
*Aryan Pandhi,Shrey Baid,Sanjali Jha*

Main category: cs.CV

TL;DR: 非洲象偷猎问题严重，计算机视觉模型被提出用于动态识别偷猎热点。


<details>
  <summary>Details</summary>
Motivation: 非洲象因偷猎濒危，传统反偷猎方法效率低，需新技术动态监测。

Method: 利用计算机视觉模型结合卫星图像识别偷猎热点地理特征。

Result: 模型可自动追踪偷猎热点，减少人工干预和环境影响。

Conclusion: 计算机视觉模型为动态反偷猎提供了高效解决方案。

Abstract: Elephant Poaching in African countries has been a decade-old problem. So much
so that African Forest Elephants are now listed as an endangered species, and
African Savannah Elephants as critically endangered by the IUCN (International
Union for Conservation of Nature). [1] Elephants are hunted primarily for their
ivory tusks which caused many elephants to be born tuskless as a genetic
modification for survival. [2] Data gathered by recent studies shows that
though poaching methods remain the same, the poaching grounds are rather
dynamic. Poachers have shifted to areas with less ranger patrols and several
other factors like watering holes, seasons, altitude etc. cause constant shifts
in poaching hotspot locations. [3] After a period of low poaching from
2000-2014, poaching numbers in African countries are now on the rise again --
WWF (World Wildlife Foundation) says there are 20,000 elephants poached
annually [4]. In African countries, anti-poaching efforts are concentrated near
towns, while a majority of poaching occurs in the deserted regions. All of
these factors result in the need for a Computer Vision Model to identify
poaching hotspots through locating the geographic indicators of favorable
poaching regions. A CV model eliminates the need to manually track poachers and
account for the environmental factors to deploy resources and its combination
with satellite imagery allows us to survey large areas without disturbing local
species or cross border aviation restrictions.

</details>


### [157] [Evolution of Low-Level and Texture Human-CLIP Alignment](https://arxiv.org/abs/2508.09814)
*Pablo Hernández-Cámara,Jose Manuel Jaén-Lorites,Jorge Vila-Tomás,Jesus Malo,Valero Laparra*

Main category: cs.CV

TL;DR: 研究发现CLIP模型在训练早期与低层次人类图像质量评估相关性最高，随后逐渐下降，原因与形状-纹理偏差对齐和噪声下分类准确性下降有关。


<details>
  <summary>Details</summary>
Motivation: 探索CLIP模型训练过程中与人类低层次感知对齐性变化的原因及其机制。

Method: 通过分析形状-纹理偏差对齐和噪声下的分类准确性变化来研究CLIP的学习动态。

Result: CLIP早期学习低层次视觉特征，与人类感知对齐性高但噪声敏感；后期转向抽象形状表示，噪声鲁棒性增强但对齐性降低。

Conclusion: 研究揭示了CLIP学习机制中感知对齐与鲁棒性的权衡，为优化视觉语言模型提供了新见解。

Abstract: During the training of multi-modal models like CLIP, we observed an
intriguing phenomenon: the correlation with low-level human image quality
assessments peaks in the early epochs before gradually declining. This study
investigates this observation and seeks to understand its causes through two
key factors: shape-texture bias alignment and classification accuracy drop
under noise. Our findings suggest that CLIP initially learn low-level visual
features, enhancing its alignment with low-level human perception but also
increasing its sensitivity to noise and its texture bias. As training
progresses, the model shifts toward more abstract shape-based representations,
improving noise robustness but reducing alignment with low-level human
perception. These results suggest that these factors shared an underlying
learning mechanism and provide new insights into optimizing the trade-off
between perceptual alignment and robustness in vision-language models.

</details>


### [158] [ViMoNet: A Multimodal Vision-Language Framework for Human Behavior Understanding from Motion and Video](https://arxiv.org/abs/2508.09818)
*Rajan Das Gupta,Md Yeasin Rahat,Nafiz Fahad,Abir Ahmed,Liew Tze Hui*

Main category: cs.CV

TL;DR: ViMoNet框架结合视频和运动数据，通过联合训练策略提升对人类行为的理解和推断能力，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有模型仅关注单一数据类型（运动或视频），无法全面捕捉人类行为的细微差别，因此需要结合两者。

Method: 提出ViMoNet框架，联合训练详细运动-文本数据和通用视频-文本数据，并发布新数据集VIMOS和评估基准ViMoNet-Bench。

Result: ViMoNet在字幕生成、运动理解和行为解释任务中表现优于现有方法。

Conclusion: 结合视频和运动数据的联合训练策略能有效提升对人类行为的理解，ViMoNet为相关研究提供了新工具和数据集。

Abstract: This study investigates how large language models (LLMs) can be used to
understand human behavior using motion and video data. We think that mixing
both types is essential to completely capture the nuanced movements and
meanings of human actions, in contrast to recent models that simply concentrate
on motion data or films. To address this, we provide ViMoNet, a straightforward
yet effective framework for comprehending, characterizing, and deducing human
action. ViMoNet employs a joint training strategy that leverages the advantages
of two data types: detailed motion-text data, which is more exact, and generic
video-text data, which is more comprehensive but less detailed. This aids in
the model's acquisition of rich data regarding time and space in human
behavior. Additionally, we provide a brand new dataset named VIMOS that
contains a variety of films, motion sequences, instructions, and subtitles. We
developed ViMoNet-Bench, a standardized benchmark with carefully labeled
samples, to evaluate how well models understand human behavior. Our tests show
that ViMoNet outperforms existing methods in caption generation, motion
understanding, and behavior interpretation.

</details>


### [159] [Physical Autoregressive Model for Robotic Manipulation without Action Pretraining](https://arxiv.org/abs/2508.09822)
*Zijian Song,Sihan Qin,Tianshui Chen,Liang Lin,Guangrun Wang*

Main category: cs.CV

TL;DR: PAR模型利用视频预训练的世界知识，通过物理令牌结合帧和动作，实现机器人操作任务的高效预测与执行。


<details>
  <summary>Details</summary>
Motivation: 机器人操作数据稀缺，促使研究者利用其他模态的预训练大模型来解决这一问题。

Method: 提出PAR模型，结合帧和动作的物理令牌表示，采用DiT-based去令牌化器，并引入因果掩码、逆运动学、并行训练和KV-cache机制。

Result: 在ManiSkill基准测试中，PAR在PushCube任务上达到100%成功率，其他任务表现与动作预训练基线相当，并能准确预测未来视频。

Conclusion: PAR展示了通过视频预训练迁移世界知识在机器人操作中的潜力。

Abstract: The scarcity of manipulation data has motivated the use of pretrained large
models from other modalities in robotics. In this work, we build upon
autoregressive video generation models to propose a Physical Autoregressive
Model (PAR), where physical tokens combine frames and actions to represent the
joint evolution of the robot and its environment. PAR leverages the world
knowledge embedded in video pretraining to understand physical dynamics without
requiring action pretraining, enabling accurate video prediction and consistent
action trajectories. It also adopts a DiT-based de-tokenizer to model frames
and actions as continuous tokens, mitigating quantization errors and
facilitating mutual enhancement. Furthermore, we incorporate a causal mask with
inverse kinematics, parallel training, and the KV-cache mechanism to further
improve performance and efficiency. Experiments on the ManiSkill benchmark show
that PAR achieves a 100\% success rate on the PushCube task, matches the
performance of action-pretrained baselines on other tasks, and accurately
predicts future videos with tightly aligned action trajectories. These findings
underscore a promising direction for robotic manipulation by transferring world
knowledge from autoregressive video pretraining.

</details>


### [160] [KonfAI: A Modular and Fully Configurable Framework for Deep Learning in Medical Imaging](https://arxiv.org/abs/2508.09823)
*Valentin Boussot,Jean-Louis Dillenseger*

Main category: cs.CV

TL;DR: KonfAI是一个模块化、可扩展且完全可配置的深度学习框架，专为医学影像任务设计，通过YAML配置文件实现训练、推理和评估工作流，无需修改代码。


<details>
  <summary>Details</summary>
Motivation: 提高医学影像任务的复现性、透明性和实验可追溯性，同时减少开发时间。

Method: 使用结构化YAML配置文件定义工作流，支持高级策略如基于补丁的学习、测试时增强、模型集成和深度监督。

Result: 成功应用于分割、配准和图像合成任务，并在多个国际医学影像挑战中取得领先成绩。

Conclusion: KonfAI是一个开源、灵活且高效的框架，适用于复杂的医学影像深度学习任务。

Abstract: KonfAI is a modular, extensible, and fully configurable deep learning
framework specifically designed for medical imaging tasks. It enables users to
define complete training, inference, and evaluation workflows through
structured YAML configuration files, without modifying the underlying code.
This declarative approach enhances reproducibility, transparency, and
experimental traceability while reducing development time. Beyond the
capabilities of standard pipelines, KonfAI provides native abstractions for
advanced strategies including patch-based learning, test-time augmentation,
model ensembling, and direct access to intermediate feature representations for
deep supervision. It also supports complex multi-model training setups such as
generative adversarial architectures. Thanks to its modular and extensible
architecture, KonfAI can easily accommodate custom models, loss functions, and
data processing components. The framework has been successfully applied to
segmentation, registration, and image synthesis tasks, and has contributed to
top-ranking results in several international medical imaging challenges. KonfAI
is open source and available at
\href{https://github.com/vboussot/KonfAI}{https://github.com/vboussot/KonfAI}.

</details>


### [161] [Reverse Convolution and Its Applications to Image Restoration](https://arxiv.org/abs/2508.09824)
*Xuhong Huang,Shiqi Liu,Kai Zhang,Ying Tai,Jian Yang,Hui Zeng,Lei Zhang*

Main category: cs.CV

TL;DR: 论文提出了一种新的深度反向卷积算子，用于有效反转深度卷积，并通过实验验证了其在图像恢复任务中的有效性。


<details>
  <summary>Details</summary>
Motivation: 由于转置卷积并非卷积的真正逆运算，现有神经网络架构缺乏标准的反向卷积算子，因此需要开发一种新的反向卷积方法。

Method: 通过正则化最小二乘优化问题设计深度反向卷积算子，并结合层归一化、1×1卷积和GELU激活构建反向卷积块，形成Transformer-like结构。

Result: 实验表明，提出的反向卷积算子能有效替代传统卷积和转置卷积层，在去噪、超分辨率和去模糊任务中表现优异。

Conclusion: 该工作为深度模型设计中的新算子开发提供了基础，并展示了其在实际应用中的潜力。

Abstract: Convolution and transposed convolution are fundamental operators widely used
in neural networks. However, transposed convolution (a.k.a. deconvolution) does
not serve as a true inverse of convolution due to inherent differences in their
mathematical formulations. To date, no reverse convolution operator has been
established as a standard component in neural architectures. In this paper, we
propose a novel depthwise reverse convolution operator as an initial attempt to
effectively reverse depthwise convolution by formulating and solving a
regularized least-squares optimization problem. We thoroughly investigate its
kernel initialization, padding strategies, and other critical aspects to ensure
its effective implementation. Building upon this operator, we further construct
a reverse convolution block by combining it with layer normalization,
1$\times$1 convolution, and GELU activation, forming a Transformer-like
structure. The proposed operator and block can directly replace conventional
convolution and transposed convolution layers in existing architectures,
leading to the development of ConverseNet. Corresponding to typical image
restoration models such as DnCNN, SRResNet and USRNet, we train three variants
of ConverseNet for Gaussian denoising, super-resolution and deblurring,
respectively. Extensive experiments demonstrate the effectiveness of the
proposed reverse convolution operator as a basic building module. We hope this
work could pave the way for developing new operators in deep model design and
applications.

</details>


### [162] [Hierarchical Graph Attention Network for No-Reference Omnidirectional Image Quality Assessment](https://arxiv.org/abs/2508.09843)
*Hao Yang,Xu Zhang,Jiaqi Ma,Linwei Zhu,Yun Zhang,Huan Zhang*

Main category: cs.CV

TL;DR: 提出了一种基于图神经网络的OIQA框架，通过建模视口间的结构关系来提升对空间失真非均匀性的感知能力。


<details>
  <summary>Details</summary>
Motivation: 现有OIQA方法在评估局部非均匀失真时表现不佳，主要由于对质量空间变化的建模不足以及对局部细节和全局上下文特征提取的不充分。

Method: 采用斐波那契球采样生成视口，将其表示为图节点；通过多阶段特征提取网络获取节点表示；结合图注意力网络（GAT）和图变换器建模局部和长程质量依赖关系。

Result: 在两个大规模OIQA数据库上的实验表明，该方法显著优于现有方法，验证了其有效性和强泛化能力。

Conclusion: 所提出的框架通过结合局部和全局特征建模，显著提升了OIQA的性能，尤其在处理复杂空间失真时表现优异。

Abstract: Current Omnidirectional Image Quality Assessment (OIQA) methods struggle to
evaluate locally non-uniform distortions due to inadequate modeling of spatial
variations in quality and ineffective feature representation capturing both
local details and global context. To address this, we propose a graph neural
network-based OIQA framework that explicitly models structural relationships
between viewports to enhance perception of spatial distortion non-uniformity.
Our approach employs Fibonacci sphere sampling to generate viewports with
well-structured topology, representing each as a graph node. Multi-stage
feature extraction networks then derive high-dimensional node representation.
To holistically capture spatial dependencies, we integrate a Graph Attention
Network (GAT) modeling fine-grained local distortion variations among adjacent
viewports, and a graph transformer capturing long-range quality interactions
across distant regions. Extensive experiments on two large-scale OIQA databases
with complex spatial distortions demonstrate that our method significantly
outperforms existing approaches, confirming its effectiveness and strong
generalization capability.

</details>


### [163] [Enhancing Diffusion Face Generation with Contrastive Embeddings and SegFormer Guidance](https://arxiv.org/abs/2508.09847)
*Dhruvraj Singh Rawat,Enggen Sherpa,Rishikesan Kirupanantha,Tin Hoang*

Main category: cs.CV

TL;DR: 论文提出了在小规模CelebAMask-HQ数据集上评估扩散模型用于人脸生成的基准，比较了UNet和DiT架构的无条件生成，并探索了LoRA微调预训练Stable Diffusion模型。通过引入InfoNCE损失和SegFormer分割编码器，提升了属性引导合成的语义对齐和可控性。


<details>
  <summary>Details</summary>
Motivation: 研究旨在探索在小规模数据集上实现可控人脸生成的方法，提升语义对齐和生成质量。

Method: 比较UNet和DiT架构的无条件生成，采用LoRA微调预训练模型，引入InfoNCE损失和SegFormer分割编码器增强属性引导合成。

Result: 结果表明，对比嵌入学习和先进分割编码在有限数据下对可控人脸生成有效。

Conclusion: 论文通过改进的损失函数和编码器设计，显著提升了小规模数据集上人脸生成的语义对齐和可控性。

Abstract: We present a benchmark of diffusion models for human face generation on a
small-scale CelebAMask-HQ dataset, evaluating both unconditional and
conditional pipelines. Our study compares UNet and DiT architectures for
unconditional generation and explores LoRA-based fine-tuning of pretrained
Stable Diffusion models as a separate experiment. Building on the
multi-conditioning approach of Giambi and Lisanti, which uses both attribute
vectors and segmentation masks, our main contribution is the integration of an
InfoNCE loss for attribute embedding and the adoption of a SegFormer-based
segmentation encoder. These enhancements improve the semantic alignment and
controllability of attribute-guided synthesis. Our results highlight the
effectiveness of contrastive embedding learning and advanced segmentation
encoding for controlled face generation in limited data settings.

</details>


### [164] [ARI3D: A Software for Interactive Quantification of Regions in X-Ray CT 3D Images](https://arxiv.org/abs/2508.09849)
*Jan Phillipp Albrecht,Jose R. A. Godinho,Christina Hübers,Deborah Schmidt*

Main category: cs.CV

TL;DR: ARI3D是一款用于交互式分析X射线CT图像区域的软件工具，旨在改进相位识别、处理部分体积效应、提高检测限和准确性，并统一3D定量分析。


<details>
  <summary>Details</summary>
Motivation: X射线CT成像技术存在固有成像伪影（如束硬化和部分体积效应），导致用户需基于体素灰度值做出大量决策来分割和分类微结构。

Method: 开发ARI3D软件工具，通过交互式协议辅助用户完成3D图像区域的分类和量化步骤。

Result: ARI3D改善了相位识别、处理了部分体积效应、提高了检测限和量化准确性，并实现了跨学科的统一分析。

Conclusion: ARI3D为X射线CT图像的定量分析提供了高效工具，解决了现有技术中的关键问题。

Abstract: X-ray computed tomography (CT) is the main 3D technique for imaging the
internal microstructures of materials. Quantitative analysis of the
microstructures is usually achieved by applying a sequence of steps that are
implemented to the entire 3D image. This is challenged by various imaging
artifacts inherent from the technique, e.g., beam hardening and partial volume.
Consequently, the analysis requires users to make a number of decisions to
segment and classify the microstructures based on the voxel gray-values. In
this context, a software tool, here called ARI3D, is proposed to interactively
analyze regions in three-dimensional X-ray CT images, assisting users through
the various steps of a protocol designed to classify and quantify objects
within regions of a three-dimensional image. ARI3D aims to 1) Improve phase
identification; 2) Account for partial volume effect; 3) Increase the detection
limit and accuracy of object quantification; and 4) Harmonize quantitative 3D
analysis that can be implemented in different fields of science.

</details>


### [165] [Do Vision Transformers See Like Humans? Evaluating their Perceptual Alignment](https://arxiv.org/abs/2508.09850)
*Pablo Hernández-Cámara,Jose Manuel Jaén-Lorites,Jorge Vila-Tomás,Valero Laparra,Jesus Malo*

Main category: cs.CV

TL;DR: 研究发现，更大的ViT模型与人类感知对齐性更低，数据增强和正则化会进一步降低对齐性。


<details>
  <summary>Details</summary>
Motivation: 探索Vision Transformers（ViT）在图像识别任务中与人类感知的对齐性。

Method: 系统分析模型大小、数据集大小、数据增强和正则化对ViT与人类感知对齐性的影响。

Result: 大模型对齐性更低，数据增强和正则化进一步降低对齐性，尤其是重复训练的模型。

Conclusion: 模型复杂性、训练策略与人类感知对齐性之间存在权衡，需在应用中权衡这些因素。

Abstract: Vision Transformers (ViTs) achieve remarkable performance in image
recognition tasks, yet their alignment with human perception remains largely
unexplored. This study systematically analyzes how model size, dataset size,
data augmentation and regularization impact ViT perceptual alignment with human
judgments on the TID2013 dataset. Our findings confirm that larger models
exhibit lower perceptual alignment, consistent with previous works. Increasing
dataset diversity has a minimal impact, but exposing models to the same images
more times reduces alignment. Stronger data augmentation and regularization
further decrease alignment, especially in models exposed to repeated training
cycles. These results highlight a trade-off between model complexity, training
strategies, and alignment with human perception, raising important
considerations for applications requiring human-like visual understanding.

</details>


### [166] [OneVAE: Joint Discrete and Continuous Optimization Helps Discrete Video VAE Train Better](https://arxiv.org/abs/2508.09857)
*Yupeng Zhou,Zhen Li,Ziheng Ouyang,Yuming Chen,Ruoyi Du,Daquan Zhou,Bin Fu,Yihao Liu,Peng Gao,Ming-Ming Cheng,Qibin Hou*

Main category: cs.CV

TL;DR: 论文提出了一种名为OneVAE的方法，通过结合连续和离散视频VAE的优势，提升了离散视频VAE的训练效率和性能。


<details>
  <summary>Details</summary>
Motivation: 离散视频VAE存在训练不稳定、时间长和重建质量差的问题，而连续VAE表现更好，因此希望通过结合两者优势改进离散VAE。

Method: 利用FSQ保留连续VAE的先验知识，提出多令牌量化机制和增强首帧重建的结构改进，并设计了联合离散-连续优化方案。

Result: 方法收敛速度更快，性能更优，PSNR提升近1 dB，并在高压缩视频VAE中显著改善重建效果。

Conclusion: OneVAE首次在单一网络中实现了连续和离散表示的竞争性能，统一了两者范式。

Abstract: Encoding videos into discrete tokens could align with text tokens to
facilitate concise and unified multi-modal LLMs, yet introducing significant
spatiotemporal compression compared to continuous video representation.
Previous discrete video VAEs experienced unstable training, long training time,
and degraded reconstruction quality. Given the easier training and superior
performance of continuous VAEs, an intuitive idea is to enhance discrete video
VAEs by leveraging continuous VAEs. After rethinking the intrinsic link between
discrete and continuous representations, we found that FSQ could effectively
preserve pre-trained continuous VAE priors compared to other quantization
methods. By leveraging continuous VAE priors, it converges several times faster
than training from scratch and achieves superior performance at convergence.
Meanwhile, two structural improvements are proposed. First, inspired by how
continuous VAEs enhance reconstruction via enlarged latent dimensions, we
introduce a multi-token quantization mechanism, which achieves nearly a 1 dB
improvement in PSNR without compromising the token compression ratio. Second,
to tackle reconstruction challenges in high-compression video VAEs, we
strengthen first-frame reconstruction, enabling the causal VAE to leverage this
information in subsequent frames and markedly improving the performance of 4 x
16 x 16 discrete VAEs. Furthermore, we propose a joint discrete-continuous
optimization scheme that unifies the two paradigms and, for the first time,
achieves competitive performance on both continuous and discrete
representations within a single network. We name our method OneVAE to reflect
this connection.

</details>


### [167] [HumanGenesis: Agent-Based Geometric and Generative Modeling for Synthetic Human Dynamics](https://arxiv.org/abs/2508.09858)
*Weiqi Li,Zehao Zhang,Liang Lin,Guangrun Wang*

Main category: cs.CV

TL;DR: HumanGenesis框架通过四个协作代理解决合成人类动态中的几何不一致和运动泛化问题，实现高质量视频生成。


<details>
  <summary>Details</summary>
Motivation: 当前方法在几何一致性和运动泛化方面存在不足，HumanGenesis旨在解决这些问题。

Method: 整合几何与生成建模，包括Reconstructor、Critique Agent、Pose Guider和Video Harmonizer四个代理。

Result: 在文本引导合成、视频重演和新姿势泛化任务中达到最优性能。

Conclusion: HumanGenesis显著提升了表达性、几何保真度和场景整合能力。

Abstract: \textbf{Synthetic human dynamics} aims to generate photorealistic videos of
human subjects performing expressive, intention-driven motions. However,
current approaches face two core challenges: (1) \emph{geometric inconsistency}
and \emph{coarse reconstruction}, due to limited 3D modeling and detail
preservation; and (2) \emph{motion generalization limitations} and \emph{scene
inharmonization}, stemming from weak generative capabilities. To address these,
we present \textbf{HumanGenesis}, a framework that integrates geometric and
generative modeling through four collaborative agents: (1)
\textbf{Reconstructor} builds 3D-consistent human-scene representations from
monocular video using 3D Gaussian Splatting and deformation decomposition. (2)
\textbf{Critique Agent} enhances reconstruction fidelity by identifying and
refining poor regions via multi-round MLLM-based reflection. (3) \textbf{Pose
Guider} enables motion generalization by generating expressive pose sequences
using time-aware parametric encoders. (4) \textbf{Video Harmonizer} synthesizes
photorealistic, coherent video via a hybrid rendering pipeline with diffusion,
refining the Reconstructor through a Back-to-4D feedback loop. HumanGenesis
achieves state-of-the-art performance on tasks including text-guided synthesis,
video reenactment, and novel-pose generalization, significantly improving
expressiveness, geometric fidelity, and scene integration.

</details>


### [168] [E-4DGS: High-Fidelity Dynamic Reconstruction from the Multi-view Event Cameras](https://arxiv.org/abs/2508.09912)
*Chaoran Feng,Zhenyu Tang,Wangbo Yu,Yatian Pang,Yian Zhao,Jianbin Zhao,Li Yuan,Yonghong Tian*

Main category: cs.CV

TL;DR: E-4DGS是一种基于事件相机的动态高斯泼溅方法，用于从多视角事件流中进行新视角合成，解决了传统RGB相机在高速运动和低光场景中的局限性。


<details>
  <summary>Details</summary>
Motivation: 传统RGB相机在场景重建中存在光照依赖、运动模糊和动态范围有限等问题，而事件相机具有低功耗、高时间分辨率和高动态范围的优势，适合高速运动和低光场景。

Method: 提出事件驱动的初始化方案和事件自适应切片泼溅技术，结合强度重要性剪枝和自适应对比度阈值优化，实现时间感知重建。

Result: E-4DGS在合成多视角事件流数据集上表现优于纯事件和事件-RGB融合基线方法。

Conclusion: 该方法为多视角事件相机重建提供了一种新思路，适用于快速场景捕捉。

Abstract: Novel view synthesis and 4D reconstruction techniques predominantly rely on
RGB cameras, thereby inheriting inherent limitations such as the dependence on
adequate lighting, susceptibility to motion blur, and a limited dynamic range.
Event cameras, offering advantages of low power, high temporal resolution and
high dynamic range, have brought a new perspective to addressing the scene
reconstruction challenges in high-speed motion and low-light scenes. To this
end, we propose E-4DGS, the first event-driven dynamic Gaussian Splatting
approach, for novel view synthesis from multi-view event streams with
fast-moving cameras. Specifically, we introduce an event-based initialization
scheme to ensure stable training and propose event-adaptive slicing splatting
for time-aware reconstruction. Additionally, we employ intensity importance
pruning to eliminate floating artifacts and enhance 3D consistency, while
incorporating an adaptive contrast threshold for more precise optimization. We
design a synthetic multi-view camera setup with six moving event cameras
surrounding the object in a 360-degree configuration and provide a benchmark
multi-view event stream dataset that captures challenging motion scenarios. Our
approach outperforms both event-only and event-RGB fusion baselines and paves
the way for the exploration of multi-view event-based reconstruction as a novel
approach for rapid scene capture.

</details>


### [169] [SpeechForensics: Audio-Visual Speech Representation Learning for Face Forgery Detection](https://arxiv.org/abs/2508.09913)
*Yachao Liang,Min Yu,Gang Li,Jianguo Jiang,Boquan Li,Feng Yu,Ning Zhang,Xiang Meng,Weiqing Huang*

Main category: cs.CV

TL;DR: 论文提出了一种基于音频-视觉语音表示学习的新方法，用于检测伪造视频，无需使用任何伪造视频进行训练，且在跨数据集泛化和鲁棒性上表现优异。


<details>
  <summary>Details</summary>
Motivation: 音频信号可以提供精确反映面部运动的信息，利用音频与视觉语音元素的协同作用，解决伪造视频检测中的泛化和鲁棒性问题。

Method: 通过自监督掩码预测任务学习真实视频中的音频-视觉语音表示，同时编码局部和全局语义信息，并将模型直接迁移到伪造检测任务中。

Result: 实验表明，该方法在跨数据集泛化和鲁棒性上优于现有方法。

Conclusion: 该方法通过音频-视觉语音表示学习，有效提升了伪造视频检测的性能，且无需依赖伪造视频训练。

Abstract: Detection of face forgery videos remains a formidable challenge in the field
of digital forensics, especially the generalization to unseen datasets and
common perturbations. In this paper, we tackle this issue by leveraging the
synergy between audio and visual speech elements, embarking on a novel approach
through audio-visual speech representation learning. Our work is motivated by
the finding that audio signals, enriched with speech content, can provide
precise information effectively reflecting facial movements. To this end, we
first learn precise audio-visual speech representations on real videos via a
self-supervised masked prediction task, which encodes both local and global
semantic information simultaneously. Then, the derived model is directly
transferred to the forgery detection task. Extensive experiments demonstrate
that our method outperforms the state-of-the-art methods in terms of
cross-dataset generalization and robustness, without the participation of any
fake video in model training. Code is available at
https://github.com/Eleven4AI/SpeechForensics.

</details>


### [170] [Towards Comprehensive Cellular Characterisation of H&E slides](https://arxiv.org/abs/2508.09926)
*Benjamin Adjadj,Pierre-Antoine Bannier,Guillaume Horent,Sebastien Mandela,Aurore Lyon,Kathryn Schutte,Ulysse Marteau,Valentin Gaury,Laura Dumont,Thomas Mathieu,Reda Belbahri,Benoît Schmauch,Eric Durand,Katharina Von Loga,Lucie Gillet*

Main category: cs.CV

TL;DR: HistoPLUS是一种用于细胞分析的最先进模型，在跨域泛化和罕见细胞类型检测上表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有方法在罕见细胞类型检测和跨域泛化上表现不佳，限制了肿瘤微环境（TME）的分析。

Method: 基于一个包含108,722个细胞核的新数据集，训练了HistoPLUS模型。

Result: 在外部验证中，HistoPLUS在检测质量和分类F1分数上分别提升了5.2%和23.7%，且参数更少。

Conclusion: HistoPLUS显著提升了罕见细胞类型的研究能力，并支持更广泛的TME生物标志物研究。

Abstract: Cell detection, segmentation and classification are essential for analyzing
tumor microenvironments (TME) on hematoxylin and eosin (H&E) slides. Existing
methods suffer from poor performance on understudied cell types (rare or not
present in public datasets) and limited cross-domain generalization. To address
these shortcomings, we introduce HistoPLUS, a state-of-the-art model for cell
analysis, trained on a novel curated pan-cancer dataset of 108,722 nuclei
covering 13 cell types. In external validation across 4 independent cohorts,
HistoPLUS outperforms current state-of-the-art models in detection quality by
5.2% and overall F1 classification score by 23.7%, while using 5x fewer
parameters. Notably, HistoPLUS unlocks the study of 7 understudied cell types
and brings significant improvements on 8 of 13 cell types. Moreover, we show
that HistoPLUS robustly transfers to two oncology indications unseen during
training. To support broader TME biomarker research, we release the model
weights and inference code at https://github.com/owkin/histoplus/.

</details>


### [171] [Quo Vadis Handwritten Text Generation for Handwritten Text Recognition?](https://arxiv.org/abs/2508.09936)
*Vittorio Pippi,Konstantina Nikolaidou,Silvia Cascianelli,George Retsinas,Giorgos Sfikas,Rita Cucchiara,Marcus Liwicki*

Main category: cs.CV

TL;DR: 本文系统比较了三种手写文本生成（HTG）模型对低资源手写文本识别（HTR）性能的影响，并提供了选择最有效模型的定量指南。


<details>
  <summary>Details</summary>
Motivation: 解决历史手稿数字化中因数据分布差异导致的HTR性能问题，尤其是针对小规模、特定作者的手写文本。

Method: 比较三种最先进的HTG模型（生成对抗、扩散和自回归范式），分析合成数据的视觉和语言特征对HTR微调的影响。

Result: 评估了HTG模型在低资源HTR中的效果，并提出了选择最优模型的定量建议。

Conclusion: 研究揭示了当前HTG方法的能力，并指出了在低资源HTR应用中需改进的关键领域。

Abstract: The digitization of historical manuscripts presents significant challenges
for Handwritten Text Recognition (HTR) systems, particularly when dealing with
small, author-specific collections that diverge from the training data
distributions. Handwritten Text Generation (HTG) techniques, which generate
synthetic data tailored to specific handwriting styles, offer a promising
solution to address these challenges. However, the effectiveness of various HTG
models in enhancing HTR performance, especially in low-resource transcription
settings, has not been thoroughly evaluated. In this work, we systematically
compare three state-of-the-art styled HTG models (representing the generative
adversarial, diffusion, and autoregressive paradigms for HTG) to assess their
impact on HTR fine-tuning. We analyze how visual and linguistic characteristics
of synthetic data influence fine-tuning outcomes and provide quantitative
guidelines for selecting the most effective HTG model. The results of our
analysis provide insights into the current capabilities of HTG methods and
highlight key areas for further improvement in their application to
low-resource HTR.

</details>


### [172] [AST-n: A Fast Sampling Approach for Low-Dose CT Reconstruction using Diffusion Models](https://arxiv.org/abs/2508.09943)
*Tomás de la Sotta,José M. Saavedra,Héctor Henríquez,Violeta Chang,Aline Xavier*

Main category: cs.CV

TL;DR: AST-n框架通过从中间噪声水平启动反向扩散并结合高阶ODE求解器，显著加速低剂量CT去噪，同时保持图像质量。


<details>
  <summary>Details</summary>
Motivation: 低剂量CT（LDCT）减少辐射但增加图像噪声，影响诊断信心。扩散生成模型在LDCT去噪中表现潜力，但需要加速推理。

Method: 提出AST-n框架，从中间噪声水平启动反向扩散，结合高阶ODE求解器减少采样步骤。

Result: AST-25仅用25步即达到PSNR>38dB和SSIM>0.95，推理时间从16秒降至1秒。无条件采样质量下降明显。

Conclusion: AST-n结合高阶求解器实现快速LDCT重建，不损失图像质量，提升扩散模型在临床中的可行性。

Abstract: Low-dose CT (LDCT) protocols reduce radiation exposure but increase image
noise, compromising diagnostic confidence. Diffusion-based generative models
have shown promise for LDCT denoising by learning image priors and performing
iterative refinement. In this work, we introduce AST-n, an accelerated
inference framework that initiates reverse diffusion from intermediate noise
levels, and integrate high-order ODE solvers within conditioned models to
further reduce sampling steps. We evaluate two acceleration paradigms--AST-n
sampling and standard scheduling with high-order solvers -- on the Low Dose CT
Grand Challenge dataset, covering head, abdominal, and chest scans at 10-25 %
of standard dose. Conditioned models using only 25 steps (AST-25) achieve peak
signal-to-noise ratio (PSNR) above 38 dB and structural similarity index (SSIM)
above 0.95, closely matching standard baselines while cutting inference time
from ~16 seg to under 1 seg per slice. Unconditional sampling suffers
substantial quality loss, underscoring the necessity of conditioning. We also
assess DDIM inversion, which yields marginal PSNR gains at the cost of doubling
inference time, limiting its clinical practicality. Our results demonstrate
that AST-n with high-order samplers enables rapid LDCT reconstruction without
significant loss of image fidelity, advancing the feasibility of
diffusion-based methods in clinical workflows.

</details>


### [173] [Stable Diffusion Models are Secretly Good at Visual In-Context Learning](https://arxiv.org/abs/2508.09949)
*Trevine Oorloff,Vishwanath Sindagi,Wele Gedara Chaminda Bandara,Ali Shafahi,Amin Ghiasi,Charan Prakash,Reza Ardekani*

Main category: cs.CV

TL;DR: 利用现成的Stable Diffusion模型进行视觉上下文学习（V-ICL），无需额外微调即可适应多种视觉任务。


<details>
  <summary>Details</summary>
Motivation: 探索如何在不增加额外训练或数据的情况下，利用现有模型实现视觉任务的上下文学习。

Method: 通过重新计算Stable Diffusion的自注意力层中的注意力，显式结合查询和示例提示的上下文。

Result: 在六个视觉任务上表现优异，例如前景分割任务在Pascal-5i数据集上的mIoU提升8.9%和3.2%。

Conclusion: 该方法展示了现成模型的潜力，并能通过集成多提示进一步提升性能。

Abstract: Large language models (LLM) in natural language processing (NLP) have
demonstrated great potential for in-context learning (ICL) -- the ability to
leverage a few sets of example prompts to adapt to various tasks without having
to explicitly update the model weights. ICL has recently been explored for
computer vision tasks with promising early outcomes. These approaches involve
specialized training and/or additional data that complicate the process and
limit its generalizability. In this work, we show that off-the-shelf Stable
Diffusion models can be repurposed for visual in-context learning (V-ICL).
Specifically, we formulate an in-place attention re-computation within the
self-attention layers of the Stable Diffusion architecture that explicitly
incorporates context between the query and example prompts. Without any
additional fine-tuning, we show that this repurposed Stable Diffusion model is
able to adapt to six different tasks: foreground segmentation, single object
detection, semantic segmentation, keypoint detection, edge detection, and
colorization. For example, the proposed approach improves the mean intersection
over union (mIoU) for the foreground segmentation task on Pascal-5i dataset by
8.9% and 3.2% over recent methods such as Visual Prompting and IMProv,
respectively. Additionally, we show that the proposed method is able to
effectively leverage multiple prompts through ensembling to infer the task
better and further improve the performance.

</details>


### [174] [LIA-X: Interpretable Latent Portrait Animator](https://arxiv.org/abs/2508.09959)
*Yaohui Wang,Di Yang,Xinyuan Chen,Francois Bremond,Yu Qiao,Antitza Dantcheva*

Main category: cs.CV

TL;DR: LIA-X是一种新型可解释的肖像动画器，通过稀疏运动字典实现面部动态的精细控制，支持‘编辑-变形-渲染’策略，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 解决传统‘变形-渲染’方法在面部动态转移中缺乏精细控制和可解释性的问题。

Method: 采用自编码器模型，通过稀疏运动字典将面部动态分解为可解释因素，实现线性导航。

Result: 在多个基准测试中，LIA-X在自重现和跨重现任务上优于现有方法，并支持用户引导的精细编辑。

Conclusion: LIA-X通过可解释性和可控性，为肖像动画和视频编辑提供了实用解决方案。

Abstract: We introduce LIA-X, a novel interpretable portrait animator designed to
transfer facial dynamics from a driving video to a source portrait with
fine-grained control. LIA-X is an autoencoder that models motion transfer as a
linear navigation of motion codes in latent space. Crucially, it incorporates a
novel Sparse Motion Dictionary that enables the model to disentangle facial
dynamics into interpretable factors. Deviating from previous 'warp-render'
approaches, the interpretability of the Sparse Motion Dictionary allows LIA-X
to support a highly controllable 'edit-warp-render' strategy, enabling precise
manipulation of fine-grained facial semantics in the source portrait. This
helps to narrow initial differences with the driving video in terms of pose and
expression. Moreover, we demonstrate the scalability of LIA-X by successfully
training a large-scale model with approximately 1 billion parameters on
extensive datasets. Experimental results show that our proposed method
outperforms previous approaches in both self-reenactment and cross-reenactment
tasks across several benchmarks. Additionally, the interpretable and
controllable nature of LIA-X supports practical applications such as
fine-grained, user-guided image and video editing, as well as 3D-aware portrait
video manipulation.

</details>


### [175] [January Food Benchmark (JFB): A Public Benchmark Dataset and Evaluation Suite for Multimodal Food Analysis](https://arxiv.org/abs/2508.09966)
*Amir Hosseinian,Ashkan Dehghani Zahedani,Umer Mansoor,Noosheen Hashemi,Mark Woodward*

Main category: cs.CV

TL;DR: 论文介绍了JFB数据集、评估框架及专用模型，显著提升了营养分析性能。


<details>
  <summary>Details</summary>
Motivation: 解决AI营养分析领域缺乏标准化评估方法和高质量数据集的问题。

Method: 提出JFB数据集、评估框架及专用模型january/food-vision-v1。

Result: 专用模型总体得分86.2，比通用模型高12.1分。

Conclusion: 为营养分析研究提供了新数据集和评估框架。

Abstract: Progress in AI for automated nutritional analysis is critically hampered by
the lack of standardized evaluation methodologies and high-quality, real-world
benchmark datasets. To address this, we introduce three primary contributions.
First, we present the January Food Benchmark (JFB), a publicly available
collection of 1,000 food images with human-validated annotations. Second, we
detail a comprehensive benchmarking framework, including robust metrics and a
novel, application-oriented overall score designed to assess model performance
holistically. Third, we provide baseline results from both general-purpose
Vision-Language Models (VLMs) and our own specialized model,
january/food-vision-v1. Our evaluation demonstrates that the specialized model
achieves an Overall Score of 86.2, a 12.1-point improvement over the
best-performing general-purpose configuration. This work offers the research
community a valuable new evaluation dataset and a rigorous framework to guide
and benchmark future developments in automated nutritional analysis.

</details>


### [176] [MOC: Meta-Optimized Classifier for Few-Shot Whole Slide Image Classification](https://arxiv.org/abs/2508.09967)
*Tianqi Xiang,Yi Li,Qixiang Zhang,Xiaomeng Li*

Main category: cs.CV

TL;DR: 提出了一种名为Meta-Optimized Classifier (MOC)的新方法，通过元学习和多样分类器组合，显著提升了少样本学习下的WSI分类性能。


<details>
  <summary>Details</summary>
Motivation: 现有的少样本学习方法在数据稀缺时表现不佳，需要一种更鲁棒的方法来提升诊断准确性。

Method: MOC包含一个元学习器和一个分类器库，前者自动优化分类器配置，后者提供多样化的候选分类器。

Result: 在多个少样本基准测试中，MOC表现优于现有方法，特别是在TCGA-NSCLC基准上，AUC提升了10.4%。

Conclusion: MOC为临床部署中数据稀缺问题提供了有效解决方案，显著提升了诊断性能。

Abstract: Recent advances in histopathology vision-language foundation models (VLFMs)
have shown promise in addressing data scarcity for whole slide image (WSI)
classification via zero-shot adaptation. However, these methods remain
outperformed by conventional multiple instance learning (MIL) approaches
trained on large datasets, motivating recent efforts to enhance VLFM-based WSI
classification through fewshot learning paradigms. While existing few-shot
methods improve diagnostic accuracy with limited annotations, their reliance on
conventional classifier designs introduces critical vulnerabilities to data
scarcity. To address this problem, we propose a Meta-Optimized Classifier (MOC)
comprising two core components: (1) a meta-learner that automatically optimizes
a classifier configuration from a mixture of candidate classifiers and (2) a
classifier bank housing diverse candidate classifiers to enable a holistic
pathological interpretation. Extensive experiments demonstrate that MOC
outperforms prior arts in multiple few-shot benchmarks. Notably, on the
TCGA-NSCLC benchmark, MOC improves AUC by 10.4% over the state-of-the-art
few-shot VLFM-based methods, with gains up to 26.25% under 1-shot conditions,
offering a critical advancement for clinical deployments where diagnostic
training data is severely limited. Code is available at
https://github.com/xmed-lab/MOC.

</details>


### [177] [PERSONA: Personalized Whole-Body 3D Avatar with Pose-Driven Deformations from a Single Image](https://arxiv.org/abs/2508.09973)
*Geonhee Sim,Gyeongsik Moon*

Main category: cs.CV

TL;DR: PERSONA结合3D和扩散方法，从单张图像生成个性化3D人体化身，解决身份保持和姿态驱动变形问题。


<details>
  <summary>Details</summary>
Motivation: 现有3D方法需要大量姿态丰富的视频，扩散方法难以保持身份一致性，PERSONA旨在结合两者优势。

Method: 利用扩散方法从单张图像生成姿态丰富视频，优化3D化身，引入平衡采样和几何加权优化。

Result: 实现了从单张图像生成高质量、姿态驱动的3D人体化身。

Conclusion: PERSONA框架有效解决了身份保持和姿态变形的挑战，为个性化3D化身提供了实用解决方案。

Abstract: Two major approaches exist for creating animatable human avatars. The first,
a 3D-based approach, optimizes a NeRF- or 3DGS-based avatar from videos of a
single person, achieving personalization through a disentangled identity
representation. However, modeling pose-driven deformations, such as non-rigid
cloth deformations, requires numerous pose-rich videos, which are costly and
impractical to capture in daily life. The second, a diffusion-based approach,
learns pose-driven deformations from large-scale in-the-wild videos but
struggles with identity preservation and pose-dependent identity entanglement.
We present PERSONA, a framework that combines the strengths of both approaches
to obtain a personalized 3D human avatar with pose-driven deformations from a
single image. PERSONA leverages a diffusion-based approach to generate
pose-rich videos from the input image and optimizes a 3D avatar based on them.
To ensure high authenticity and sharp renderings across diverse poses, we
introduce balanced sampling and geometry-weighted optimization. Balanced
sampling oversamples the input image to mitigate identity shifts in
diffusion-generated training videos. Geometry-weighted optimization prioritizes
geometry constraints over image loss, preserving rendering quality in diverse
poses.

</details>


### [178] [A Survey on 3D Gaussian Splatting Applications: Segmentation, Editing, and Generation](https://arxiv.org/abs/2508.09977)
*Shuting He,Peilin Ji,Yitong Yang,Changshuo Wang,Jiayi Ji,Yinglin Wang,Henghui Ding*

Main category: cs.CV

TL;DR: 3D高斯泼溅（3DGS）作为一种替代神经辐射场（NeRF）的3D场景表示方法，提供高保真实时渲染，并支持多种下游应用。本文综述了3DGS的最新进展，包括语义理解、分割、编辑、生成等功能任务。


<details>
  <summary>Details</summary>
Motivation: 3DGS因其显式和紧凑的特性，在几何和语义理解方面具有广泛的应用潜力，本文旨在系统梳理其应用进展。

Method: 通过分类3DGS应用（如分割、编辑、生成等），总结代表性方法、监督策略和学习范式，并分析设计原则与趋势。

Result: 综述了常用数据集、评估协议及公开基准上的方法比较，为研究提供参考。

Conclusion: 3DGS在多种任务中表现出色，未来研究可进一步探索其潜力。资源库持续更新以支持研究。

Abstract: 3D Gaussian Splatting (3DGS) has recently emerged as a powerful alternative
to Neural Radiance Fields (NeRF) for 3D scene representation, offering
high-fidelity photorealistic rendering with real-time performance. Beyond novel
view synthesis, the explicit and compact nature of 3DGS enables a wide range of
downstream applications that require geometric and semantic understanding. This
survey provides a comprehensive overview of recent progress in 3DGS
applications. It first introduces 2D foundation models that support semantic
understanding and control in 3DGS applications, followed by a review of
NeRF-based methods that inform their 3DGS counterparts. We then categorize 3DGS
applications into segmentation, editing, generation, and other functional
tasks. For each, we summarize representative methods, supervision strategies,
and learning paradigms, highlighting shared design principles and emerging
trends. Commonly used datasets and evaluation protocols are also summarized,
along with comparative analyses of recent methods across public benchmarks. To
support ongoing research and development, a continually updated repository of
papers, code, and resources is maintained at
https://github.com/heshuting555/Awesome-3DGS-Applications.

</details>


### [179] [LLMC+: Benchmarking Vision-Language Model Compression with a Plug-and-play Toolkit](https://arxiv.org/abs/2508.09981)
*Chengtao Lv,Bilang Zhang,Yang Yong,Ruihao Gong,Yushi Huang,Shiqiao Gu,Jiajun Wu,Yumeng Shi,Jinyang Guo,Wenya Wang*

Main category: cs.CV

TL;DR: LLMC+是一个全面的VLM压缩基准测试工具包，支持20多种算法，揭示了空间和时间冗余需要不同策略，并展示了联合压缩的潜力。


<details>
  <summary>Details</summary>
Motivation: 解决现有VLM压缩方法在公平评估、多任务表现和联合技术探索方面的不足。

Method: 提出LLMC+工具包，支持多种算法，系统研究令牌级和模型级压缩。

Result: 发现空间和时间冗余需不同策略，联合压缩可实现高效压缩且性能损失小。

Conclusion: LLMC+有助于公平评估和推动高效VLM研究。

Abstract: Large Vision-Language Models (VLMs) exhibit impressive multi-modal
capabilities but suffer from prohibitive computational and memory demands, due
to their long visual token sequences and massive parameter sizes. To address
these issues, recent works have proposed training-free compression methods.
However, existing efforts often suffer from three major limitations: (1)
Current approaches do not decompose techniques into comparable modules,
hindering fair evaluation across spatial and temporal redundancy. (2)
Evaluation confined to simple single-turn tasks, failing to reflect performance
in realistic scenarios. (3) Isolated use of individual compression techniques,
without exploring their joint potential. To overcome these gaps, we introduce
LLMC+, a comprehensive VLM compression benchmark with a versatile,
plug-and-play toolkit. LLMC+ supports over 20 algorithms across five
representative VLM families and enables systematic study of token-level and
model-level compression. Our benchmark reveals that: (1) Spatial and temporal
redundancies demand distinct technical strategies. (2) Token reduction methods
degrade significantly in multi-turn dialogue and detail-sensitive tasks. (3)
Combining token and model compression achieves extreme compression with minimal
performance loss. We believe LLMC+ will facilitate fair evaluation and inspire
future research in efficient VLM. Our code is available at
https://github.com/ModelTC/LightCompress.

</details>


### [180] [Story2Board: A Training-Free Approach for Expressive Storyboard Generation](https://arxiv.org/abs/2508.09983)
*David Dinkevich,Matan Levy,Omri Avrahami,Dvir Samuel,Dani Lischinski*

Main category: cs.CV

TL;DR: Story2Board是一个无需训练的框架，用于从自然语言生成富有表现力的故事板。它通过轻量级一致性框架解决了现有方法忽略视觉叙事关键要素的问题，并在生成多样且一致的故事板方面表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有方法过于关注主体身份，忽略了视觉叙事中的空间构图、背景演变和叙事节奏等关键要素。

Method: 引入轻量级一致性框架，包括潜在面板锚定（保持角色一致性）和互惠注意力值混合（软融合视觉特征），并结合语言模型生成面板级提示。

Result: 在Rich Storyboard Benchmark上，Story2Board在布局多样性和叙事一致性方面表现优异，并通过用户研究验证了其动态性和叙事吸引力。

Conclusion: Story2Board能够生成更动态、连贯且叙事吸引人的故事板，优于现有基线方法。

Abstract: We present Story2Board, a training-free framework for expressive storyboard
generation from natural language. Existing methods narrowly focus on subject
identity, overlooking key aspects of visual storytelling such as spatial
composition, background evolution, and narrative pacing. To address this, we
introduce a lightweight consistency framework composed of two components:
Latent Panel Anchoring, which preserves a shared character reference across
panels, and Reciprocal Attention Value Mixing, which softly blends visual
features between token pairs with strong reciprocal attention. Together, these
mechanisms enhance coherence without architectural changes or fine-tuning,
enabling state-of-the-art diffusion models to generate visually diverse yet
consistent storyboards. To structure generation, we use an off-the-shelf
language model to convert free-form stories into grounded panel-level prompts.
To evaluate, we propose the Rich Storyboard Benchmark, a suite of open-domain
narratives designed to assess layout diversity and background-grounded
storytelling, in addition to consistency. We also introduce a new Scene
Diversity metric that quantifies spatial and pose variation across storyboards.
Our qualitative and quantitative results, as well as a user study, show that
Story2Board produces more dynamic, coherent, and narratively engaging
storyboards than existing baselines.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [181] [Efficient Real-Time Aircraft ETA Prediction via Feature Tokenization Transformer](https://arxiv.org/abs/2508.09144)
*Liping Huang,Yicheng Zhang,Yifang Yin,Sheng Zhang,Yi Zhang*

Main category: cs.LG

TL;DR: 该研究提出了一种基于特征标记化的Transformer模型，用于实时预测飞机的预计到达时间（ETA），在准确性和效率上均优于传统的提升树模型。


<details>
  <summary>Details</summary>
Motivation: 实时ETA预测对于航空到达管理至关重要，尤其是在跑道排序中。研究旨在提高预测效率和准确性，以满足实时系统的需求。

Method: 采用特征标记化将原始输入映射到潜在空间，并利用Transformer的多头自注意力机制捕捉关键特征，避免了复杂的特征工程。模型输入包括飞机位置、速度、天气等数据，每秒更新一次预测。

Result: 实验表明，该方法比XGBoost模型准确率提高7%，计算时间仅需39%。在40架飞机同时飞行时，ETA推理时间仅为51.7微秒。

Conclusion: 该方法高效且准确，适用于实时到达管理系统，特别是在高频率更新和大规模数据处理场景中。

Abstract: Estimated time of arrival (ETA) for airborne aircraft in real-time is crucial
for arrival management in aviation, particularly for runway sequencing. Given
the rapidly changing airspace context, the ETA prediction efficiency is as
important as its accuracy in a real-time arrival aircraft management system. In
this study, we utilize a feature tokenization-based Transformer model to
efficiently predict aircraft ETA. Feature tokenization projects raw inputs to
latent spaces, while the multi-head self-attention mechanism in the Transformer
captures important aspects of the projections, alleviating the need for complex
feature engineering. Moreover, the Transformer's parallel computation
capability allows it to handle ETA requests at a high frequency, i.e., 1HZ,
which is essential for a real-time arrival management system. The model inputs
include raw data, such as aircraft latitude, longitude, ground speed, theta
degree for the airport, day and hour from track data, the weather context, and
aircraft wake turbulence category. With a data sampling rate of 1HZ, the ETA
prediction is updated every second. We apply the proposed aircraft ETA
prediction approach to Singapore Changi Airport (ICAO Code: WSSS) using
one-month Automatic Dependent Surveillance-Broadcast (ADS-B) data from October
1 to October 31, 2022. In the experimental evaluation, the ETA modeling covers
all aircraft within a range of 10NM to 300NM from WSSS. The results show that
our proposed method method outperforms the commonly used boosting tree based
model, improving accuracy by 7\% compared to XGBoost, while requiring only 39\%
of its computing time. Experimental results also indicate that, with 40
aircraft in the airspace at a given timestamp, the ETA inference time is only
51.7 microseconds, making it promising for real-time arrival management
systems.

</details>


### [182] [MoLAN: A Unified Modality-Aware Noise Dynamic Editing Framework for Multimodal Sentiment Analysis](https://arxiv.org/abs/2508.09145)
*Xingle Xu,Yongkang Liu,Dexian Cai,Shi Feng,Xiaocui Yang,Daling Wang,Yifei Zhang*

Main category: cs.LG

TL;DR: MoLAN是一个多模态感知的动态噪声编辑框架，通过分块处理模态特征并动态分配去噪强度，有效保留关键信息。MoLAN+在此基础上实现了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 多模态情感分析常因无关或误导性信息而受限，现有方法可能丢失关键信息。

Method: MoLAN将模态特征分块，动态分配去噪强度，MoLAN+是其扩展应用。

Result: 在五个模型和四个数据集上验证了MoLAN的广泛有效性，MoLAN+达到最先进性能。

Conclusion: MoLAN框架灵活统一，能有效抑制噪声并保留关键信息，MoLAN+表现卓越。

Abstract: Multimodal Sentiment Analysis aims to integrate information from various
modalities, such as audio, visual, and text, to make complementary predictions.
However, it often struggles with irrelevant or misleading visual and auditory
information. Most existing approaches typically treat the entire modality
information (e.g., a whole image, audio segment, or text paragraph) as an
independent unit for feature enhancement or denoising. They often suppress the
redundant and noise information at the risk of losing critical information. To
address this challenge, we propose MoLAN, a unified ModaLity-aware noise
dynAmic editiNg framework. Specifically, MoLAN performs modality-aware blocking
by dividing the features of each modality into multiple blocks. Each block is
then dynamically assigned a distinct denoising strength based on its noise
level and semantic relevance, enabling fine-grained noise suppression while
preserving essential multimodal information. Notably, MoLAN is a unified and
flexible framework that can be seamlessly integrated into a wide range of
multimodal models. Building upon this framework, we further introduce MoLAN+, a
new multimodal sentiment analysis approach. Experiments across five models and
four datasets demonstrate the broad effectiveness of the MoLAN framework.
Extensive evaluations show that MoLAN+ achieves the state-of-the-art
performance. The code is publicly available at
https://github.com/betterfly123/MoLAN-Framework.

</details>


### [183] [To Theoretically Understand Transformer-Based In-Context Learning for Optimizing CSMA](https://arxiv.org/abs/2508.09146)
*Shugang Hao,Hongbo Li,Lingjie Duan*

Main category: cs.LG

TL;DR: 论文提出了一种基于LLM transformer的上下文学习（ICL）理论，用于优化WiFi 7中的信道访问，通过预测竞争窗口阈值（CWT）提升吞吐量。


<details>
  <summary>Details</summary>
Motivation: 现有基于模型的退避策略在动态信道环境下表现不佳，且依赖固定的节点密度假设，导致吞吐量损失。

Method: 设计了一个基于transformer的ICL优化器，通过预收集碰撞阈值数据示例和查询碰撞案例，生成预测的CWT。

Result: 实验证明该方法在未知节点密度下具有快速收敛性和接近最优的吞吐量表现。

Conclusion: 该方法在动态环境中显著提升了吞吐量性能，且对输入数据的容错能力强。

Abstract: The binary exponential backoff scheme is widely used in WiFi 7 and still
incurs poor throughput performance under dynamic channel environments. Recent
model-based approaches (e.g., non-persistent and $p$-persistent CSMA) simply
optimize backoff strategies under a known and fixed node density, still leading
to a large throughput loss due to inaccurate node density estimation. This
paper is the first to propose LLM transformer-based in-context learning (ICL)
theory for optimizing channel access. We design a transformer-based ICL
optimizer to pre-collect collision-threshold data examples and a query
collision case. They are constructed as a prompt as the input for the
transformer to learn the pattern, which then generates a predicted contention
window threshold (CWT). To train the transformer for effective ICL, we develop
an efficient algorithm and guarantee a near-optimal CWT prediction within
limited training steps. As it may be hard to gather perfect data examples for
ICL in practice, we further extend to allow erroneous data input in the prompt.
We prove that our optimizer maintains minimal prediction and throughput
deviations from the optimal values. Experimental results on NS-3 further
demonstrate our approach's fast convergence and near-optimal throughput over
existing model-based and DRL-based approaches under unknown node densities.

</details>


### [184] [Constrained Black-Box Attacks Against Multi-Agent Reinforcement Learning](https://arxiv.org/abs/2508.09275)
*Amine Andam,Jamal Bentahar,Mustapha Hedabou*

Main category: cs.LG

TL;DR: 本文研究了协作多智能体强化学习（c-MARL）在现实和受限条件下的新漏洞，提出了一种高效的对抗扰动生成算法，并在多个环境中验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 协作多智能体强化学习在敏感领域应用广泛，但其对抗攻击漏洞尚未深入研究，尤其是在现实条件下。

Method: 假设攻击者仅能收集并扰动部署智能体的观测数据，提出了一种简单高效的对抗扰动生成算法。

Result: 算法在三个基准测试和22个环境中验证有效，且样本效率高，仅需1,000样本。

Conclusion: 研究表明c-MARL在现实条件下存在显著漏洞，提出的算法为防御研究提供了新方向。

Abstract: Collaborative multi-agent reinforcement learning (c-MARL) has rapidly
evolved, offering state-of-the-art algorithms for real-world applications,
including sensitive domains. However, a key challenge to its widespread
adoption is the lack of a thorough investigation into its vulnerabilities to
adversarial attacks. Existing work predominantly focuses on training-time
attacks or unrealistic scenarios, such as access to policy weights or the
ability to train surrogate policies. In this paper, we investigate new
vulnerabilities under more realistic and constrained conditions, assuming an
adversary can only collect and perturb the observations of deployed agents. We
also consider scenarios where the adversary has no access at all. We propose
simple yet highly effective algorithms for generating adversarial perturbations
designed to misalign how victim agents perceive their environment. Our approach
is empirically validated on three benchmarks and 22 environments, demonstrating
its effectiveness across diverse algorithms and environments. Furthermore, we
show that our algorithm is sample-efficient, requiring only 1,000 samples
compared to the millions needed by previous methods.

</details>


### [185] [Motif 2.6B Technical Report](https://arxiv.org/abs/2508.09148)
*Junghwan Lim,Sungmin Lee,Dongseok Kim,Eunhwan Park,Hyunbyung Park,Junhyeok Lee,Wai Ting Cheung,Dahye Choi,Jaeheui Her,Jaeyeon Huh,Hanbin Jung,Changjin Kang,Beomgyu Kim,Jihwan Kim,Minjae Kim,Taehwan Kim,Youngrok Kim,Haesol Lee,Jeesoo Lee,Kungyu Lee,Dongpin Oh,Yeongjae Park,Bokki Ryu,Daewon Suh,Dongjoo Weon*

Main category: cs.LG

TL;DR: Motif-2.6B是一个2.6B参数的基础模型，旨在平衡高性能与计算效率，通过创新架构（如Differential Attention和PolyNorm）提升长文本理解、减少幻觉并增强上下文学习能力。


<details>
  <summary>Details</summary>
Motivation: 解决大型语言模型（LLMs）在性能与计算效率之间的平衡问题，尤其是为新兴研究群体提供高效的基础模型。

Method: 采用Differential Attention和PolyNorm等创新架构，并通过大量实验优化模型设计。

Result: Motif-2.6B在多项基准测试中表现优于或接近同类先进模型，展示了其高效性和可扩展性。

Conclusion: Motif-2.6B为高效、可扩展的基础LLMs提供了新方向，并为未来研究和部署奠定了坚实基础。

Abstract: Recent advancements in Large Language Models (LLMs) have revolutionized
artificial intelligence, yet developing an effective foundational LLM that
balances high performance with computational efficiency remains challenging,
especially for emerging research groups. To address this gap, we introduce
Motif-2.6B, a 2.6-billion-parameter foundation model designed to democratize
advanced LLM capabilities. Motif-2.6B incorporates several innovative
architectural enhancements, including Differential Attention and PolyNorm
activation functions, which improve long-context comprehension, reduce
hallucination, and enhance in-context learning capabilities. We rigorously
tested multiple novel architectural components through extensive
experimentation to determine the optimal architecture for Motif-2.6B.
Comprehensive evaluations demonstrate that Motif-2.6B consistently meets or
exceeds the performance of similarly sized state-of-the-art models across
diverse benchmarks, showcasing its effectiveness, scalability, and real-world
applicability. Through detailed experiments and tailored techniques, Motif-2.6B
significantly advances the landscape of efficient, scalable, and powerful
foundational LLMs, offering valuable insights and a robust foundation for
future research and deployment.

</details>


### [186] [JustDense: Just using Dense instead of Sequence Mixer for Time Series analysis](https://arxiv.org/abs/2508.09153)
*TaekHyun Park,Yongjae Lee,Daesan Park,Dohee Kim,Hyerim Bae*

Main category: cs.LG

TL;DR: 论文质疑复杂序列混合器在时间序列分析中的必要性，提出用简单密集层替代，实验证明性能相当甚至更优。


<details>
  <summary>Details</summary>
Motivation: 近期研究表明，复杂序列混合器（如注意力机制）可能并非必要，简单架构也能达到类似或更好效果。因此，论文探讨序列混合器是否真的必要。

Method: 提出JustDense方法，将序列混合器替换为密集层，基于MatrixMixer框架进行理论分析，并在29个基准测试中验证。

Result: 实验显示，密集层替代序列混合器性能相当或更优，挑战了“更复杂架构必然更好”的假设。

Conclusion: 复杂序列混合器在时间序列分析中可能并非必要，简单密集层也能有效完成任务。

Abstract: Sequence and channel mixers, the core mechanism in sequence models, have
become the de facto standard in time series analysis (TSA). However, recent
studies have questioned the necessity of complex sequence mixers, such as
attention mechanisms, demonstrating that simpler architectures can achieve
comparable or even superior performance. This suggests that the benefits
attributed to complex sequencemixers might instead emerge from other
architectural or optimization factors. Based on this observation, we pose a
central question: Are common sequence mixers necessary for time-series
analysis? Therefore, we propose JustDense, an empirical study that
systematically replaces sequence mixers in various well-established TSA models
with dense layers. Grounded in the MatrixMixer framework, JustDense treats any
sequence mixer as a mixing matrix and replaces it with a dense layer. This
substitution isolates the mixing operation, enabling a clear theoretical
foundation for understanding its role. Therefore, we conducted extensive
experiments on 29 benchmarks covering five representative TSA tasks using seven
state-of-the-art TSA models to address our research question. The results show
that replacing sequence mixers with dense layers yields comparable or even
superior performance. In the cases where dedicated sequence mixers still offer
benefits, JustDense challenges the assumption that "deeper and more complex
architectures are inherently better" in TSA.

</details>


### [187] [Peer Effect Estimation in the Presence of Simultaneous Feedback and Unobserved Confounders](https://arxiv.org/abs/2508.09154)
*Xiaojing Du,Jiuyong Li,Lin Liu,Debo Cheng,Thuc. Le*

Main category: cs.LG

TL;DR: DIG2RSI是一个深度学习框架，通过I-G变换和2SRI技术解决社交网络中同伴效应的反馈和未观测混杂问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法无法同时处理同伴效应的反馈和未观测混杂，导致估计不准确。

Method: DIG2RSI结合I-G变换消除反馈偏差，利用2SRI技术通过神经网络和对抗性去偏处理混杂。

Result: 理论证明和实验表明DIG2RSI优于现有方法，能准确估计同伴效应。

Conclusion: DIG2RSI为复杂网络中的因果推断提供了有效解决方案。

Abstract: Estimating peer causal effects within complex real-world networks such as
social networks is challenging, primarily due to simultaneous feedback between
peers and unobserved confounders. Existing methods either address unobserved
confounders while ignoring the simultaneous feedback, or account for feedback
but under restrictive linear assumptions, thus failing to obtain accurate peer
effect estimation. In this paper, we propose DIG2RSI, a novel Deep learning
framework which leverages I-G transformation (matrix operation) and 2SRI (an
instrumental variable or IV technique) to address both simultaneous feedback
and unobserved confounding, while accommodating complex, nonlinear and
high-dimensional relationships. DIG2RSI first applies the I-G transformation to
disentangle mutual peer influences and eliminate the bias due to the
simultaneous feedback. To deal with unobserved confounding, we first construct
valid IVs from network data. In stage 1 of 2RSI, we train a neural network on
these IVs to predict peer exposure, and extract residuals as proxies for the
unobserved confounders. In the stage 2, we fit a separate neural network
augmented by an adversarial discriminator that incorporates these residuals as
a control function and enforces the learned representation to contain no
residual confounding signal. The expressive power of deep learning models in
capturing complex non-linear relationships and adversarial debiasing enhances
the effectiveness of DIG2RSI in eliminating bias from both feedback loops and
hidden confounders. We prove consistency of our estimator under standard
regularity conditions, ensuring asymptotic recovery of the true peer effect.
Empirical results on two semi-synthetic benchmarks and a real-world dataset
demonstrate that DIG2RSI outperforms existing approaches.

</details>


### [188] [A Rolling Stone Gathers No Moss: Adaptive Policy Optimization for Stable Self-Evaluation in Large Multimodal Models](https://arxiv.org/abs/2508.09155)
*Wenkai Wang,Hongcan Guo,Zheqi Lv,Shengyu Zhang*

Main category: cs.LG

TL;DR: 论文提出AdaPO框架，通过自适应调整训练目标和动态奖励机制，解决多目标优化中的奖励黑客问题，显著提升模型的自评估和推理能力。


<details>
  <summary>Details</summary>
Motivation: 大型多模态模型（LMMs）在自评估能力上存在不足，传统强化学习的固定奖励机制在多目标优化中易导致奖励黑客和模型崩溃。

Method: 提出AdaPO框架，包含自适应奖励模型（ARM）和奖励感知动态KL正则化机制，实时调整训练目标和奖励策略。

Result: 在8个基准测试中，AdaPO显著提升了模型的直接推理和自评估能力。

Conclusion: AdaPO通过自适应机制有效解决了奖励黑客问题，为LMMs的自改进提供了新思路。

Abstract: Self-evaluation, a model's ability to assess the correctness of its own
output, is crucial for Large Multimodal Models (LMMs) to achieve
self-improvement in multi-turn conversations, yet largely absent in foundation
models. Recent work has employed reinforcement learning (RL) to enhance
self-evaluation; however, its fixed reward mechanism suffers from reward
hacking when optimizing multiple training objectives, leading to model
collapse. In this paper we propose AdaPO, an online reinforcement learning
framework capable of adaptively adjusting training objective in real time
according to the current training state for each task. Specifically, to
mitigate reward hacking , AdaPO introduces an Adaptive Reward Model (ARM) and a
Reward Aware Dynamic KL Regularization mechanism. ARM assesses the task's
training state from the distribution of model generated multi-turn
trajectories' performance. Reward Aware Dynamic KL replaces a fixed penalty
with dynamic coefficients which is modulated by the reward gap between
different multi-turn situations. Notably, our method automatically and smoothly
adjusts its learning focus based on sub-tasks' training progress without manual
intervention. Extensive experiments over 8 benchmarks and various models show
that our method significantly enhances both direct reasoning and
self-evaluation capability. We will release our code to contribute to the
community.

</details>


### [189] [Physics-Constrained Fine-Tuning of Flow-Matching Models for Generation and Inverse Problems](https://arxiv.org/abs/2508.09156)
*Jan Tauberschmidt,Sophie Fellenz,Sebastian J. Vollmer,Andrew B. Duncan*

Main category: cs.LG

TL;DR: 提出一种框架，通过微调流匹配生成模型来强制物理约束并解决科学系统中的逆问题。


<details>
  <summary>Details</summary>
Motivation: 解决低保真或观测数据训练的生成模型在物理一致性上的不足，同时处理未知物理输入的推断问题。

Method: 采用可微分的后训练程序，最小化控制偏微分方程的弱形式残差，并结合可学习的潜在参数预测器进行联合优化。

Result: 模型能生成物理有效的场解及隐藏参数的合理估计，在基准测试中表现出改进的PDE约束满足和参数恢复准确性。

Conclusion: 该方法连接了生成建模与科学推断，为模拟增强发现和数据高效建模开辟了新途径。

Abstract: We present a framework for fine-tuning flow-matching generative models to
enforce physical constraints and solve inverse problems in scientific systems.
Starting from a model trained on low-fidelity or observational data, we apply a
differentiable post-training procedure that minimizes weak-form residuals of
governing partial differential equations (PDEs), promoting physical consistency
and adherence to boundary conditions without distorting the underlying learned
distribution. To infer unknown physical inputs, such as source terms, material
parameters, or boundary data, we augment the generative process with a
learnable latent parameter predictor and propose a joint optimization strategy.
The resulting model produces physically valid field solutions alongside
plausible estimates of hidden parameters, effectively addressing ill-posed
inverse problems in a data-driven yet physicsaware manner. We validate our
method on canonical PDE benchmarks, demonstrating improved satisfaction of PDE
constraints and accurate recovery of latent coefficients. Our approach bridges
generative modelling and scientific inference, opening new avenues for
simulation-augmented discovery and data-efficient modelling of physical
systems.

</details>


### [190] [EvaDrive: Evolutionary Adversarial Policy Optimization for End-to-End Autonomous Driving](https://arxiv.org/abs/2508.09158)
*Siwen Jiao,Kangan Qian,Hao Ye,Yang Zhong,Ziang Luo,Sicong Jiang,Zilin Huang,Yangyi Fang,Jinyu Miao,Zheng Fu,Yunlong Wang,Kun Jiang,Diange Yang,Rui Fan,Baoyun Peng*

Main category: cs.LG

TL;DR: EvaDrive提出了一种多目标强化学习框架，通过对抗优化实现轨迹生成与评估的闭环协同进化，解决了现有方法中迭代优化不足和标量化偏差的问题。


<details>
  <summary>Details</summary>
Motivation: 当前自动驾驶决策方法中，轨迹生成与评估分离导致迭代优化不足，而强化学习方法将多维偏好压缩为标量奖励，掩盖了关键权衡并引入偏差。

Method: EvaDrive采用分层生成器（结合自回归意图建模和扩散模型）和多目标评估器，通过对抗博弈实现多轮迭代优化，保留多样偏好结构。

Result: 在NAVSIM和Bench2Drive基准测试中表现优异，分别达到94.9 PDMS和64.96 Driving Score，超越现有方法。

Conclusion: EvaDrive通过闭环对抗框架实现了人类式迭代决策，为轨迹优化提供了无标量化的新方法。

Abstract: Autonomous driving faces significant challenges in achieving human-like
iterative decision-making, which continuously generates, evaluates, and refines
trajectory proposals. Current generation-evaluation frameworks isolate
trajectory generation from quality assessment, preventing iterative refinement
essential for planning, while reinforcement learning methods collapse
multi-dimensional preferences into scalar rewards, obscuring critical
trade-offs and yielding scalarization bias.To overcome these issues, we present
EvaDrive, a novel multi-objective reinforcement learning framework that
establishes genuine closed-loop co-evolution between trajectory generation and
evaluation via adversarial optimization. EvaDrive frames trajectory planning as
a multi-round adversarial game. In this game, a hierarchical generator
continuously proposes candidate paths by combining autoregressive intent
modeling for temporal causality with diffusion-based refinement for spatial
flexibility. These proposals are then rigorously assessed by a trainable
multi-objective critic that explicitly preserves diverse preference structures
without collapsing them into a single scalarization bias.This adversarial
interplay, guided by a Pareto frontier selection mechanism, enables iterative
multi-round refinement, effectively escaping local optima while preserving
trajectory diversity.Extensive experiments on NAVSIM and Bench2Drive benchmarks
demonstrate SOTA performance, achieving 94.9 PDMS on NAVSIM v1 (surpassing
DiffusionDrive by 6.8, DriveSuprim by 5.0, and TrajHF by 0.9) and 64.96 Driving
Score on Bench2Drive. EvaDrive generates diverse driving styles via dynamic
weighting without external preference data, introducing a closed-loop
adversarial framework for human-like iterative decision-making, offering a
novel scalarization-free trajectory optimization approach.

</details>


### [191] [Presenting DiaData for Research on Type 1 Diabetes](https://arxiv.org/abs/2508.09160)
*Beyza Cinar,Maria Maleshkova*

Main category: cs.LG

TL;DR: 该论文整合了15个数据集，创建了一个包含2510名受试者的大规模糖尿病数据库，用于研究低血糖事件及其与心率的关系。


<details>
  <summary>Details</summary>
Motivation: 研究动机是解决糖尿病研究中数据不足的问题，尤其是低血糖事件的预测和管理。

Method: 通过系统整合15个数据集，构建了一个包含149百万次血糖测量的数据库，并提取了两个子数据库（人口统计和心率数据）。

Result: 结果显示数据不平衡和缺失值是主要挑战，同时发现血糖水平与心率在低血糖前15至55分钟存在相关性。

Conclusion: 结论是整合的数据集为糖尿病研究提供了重要资源，并揭示了血糖与心率的关系，有助于低血糖的早期预测。

Abstract: Type 1 diabetes (T1D) is an autoimmune disorder that leads to the destruction
of insulin-producing cells, resulting in insulin deficiency, as to why the
affected individuals depend on external insulin injections. However, insulin
can decrease blood glucose levels and can cause hypoglycemia. Hypoglycemia is a
severe event of low blood glucose levels ($\le$70 mg/dL) with dangerous side
effects of dizziness, coma, or death. Data analysis can significantly enhance
diabetes care by identifying personal patterns and trends leading to adverse
events. Especially, machine learning (ML) models can predict glucose levels and
provide early alarms. However, diabetes and hypoglycemia research is limited by
the unavailability of large datasets. Thus, this work systematically integrates
15 datasets to provide a large database of 2510 subjects with glucose
measurements recorded every 5 minutes. In total, 149 million measurements are
included, of which 4% represent values in the hypoglycemic range. Moreover, two
sub-databases are extracted. Sub-database I includes demographics, and
sub-database II includes heart rate data. The integrated dataset provides an
equal distribution of sex and different age levels. As a further contribution,
data quality is assessed, revealing that data imbalance and missing values
present a significant challenge. Moreover, a correlation study on glucose
levels and heart rate data is conducted, showing a relation between 15 and 55
minutes before hypoglycemia.

</details>


### [192] [Physics-Guided Memory Network for Building Energy Modeling](https://arxiv.org/abs/2508.09161)
*Muhammad Umair Danish,Kashif Ali,Kamran Siddiqui,Katarina Grolinger*

Main category: cs.LG

TL;DR: 本文提出了一种物理引导记忆网络（PgMN），结合深度学习和物理模型，解决建筑物能耗预测中历史数据不足或缺失的问题。


<details>
  <summary>Details</summary>
Motivation: 建筑物能耗预测对资源管理和可持续发展至关重要，但现有深度学习和物理模型各有局限。

Method: PgMN通过并行投影层处理不完整输入，记忆单元解决持续偏差，记忆经验模块优化预测范围。

Result: 实验验证了PgMN在新建建筑、数据缺失等场景下的准确性和适用性。

Conclusion: PgMN为动态建筑环境中的能耗预测提供了有效解决方案。

Abstract: Accurate energy consumption forecasting is essential for efficient resource
management and sustainability in the building sector. Deep learning models are
highly successful but struggle with limited historical data and become unusable
when historical data are unavailable, such as in newly constructed buildings.
On the other hand, physics-based models, such as EnergyPlus, simulate energy
consumption without relying on historical data but require extensive building
parameter specifications and considerable time to model a building. This paper
introduces a Physics-Guided Memory Network (PgMN), a neural network that
integrates predictions from deep learning and physics-based models to address
their limitations. PgMN comprises a Parallel Projection Layers to process
incomplete inputs, a Memory Unit to account for persistent biases, and a Memory
Experience Module to optimally extend forecasts beyond their input range and
produce output. Theoretical evaluation shows that components of PgMN are
mathematically valid for performing their respective tasks. The PgMN was
evaluated on short-term energy forecasting at an hourly resolution, critical
for operational decision-making in smart grid and smart building systems.
Experimental validation shows accuracy and applicability of PgMN in diverse
scenarios such as newly constructed buildings, missing data, sparse historical
data, and dynamic infrastructure changes. This paper provides a promising
solution for energy consumption forecasting in dynamic building environments,
enhancing model applicability in scenarios where historical data are limited or
unavailable or when physics-based models are inadequate.

</details>


### [193] [An Unsupervised Deep XAI Framework for Localization of Concurrent Replay Attacks in Nuclear Reactor Signals](https://arxiv.org/abs/2508.09162)
*Konstantinos Vasili,Zachery T. Dahm,William Richards,Stylianos Chatzidakis*

Main category: cs.LG

TL;DR: 论文提出了一种基于自编码器和定制化windowSHAP算法的无监督可解释AI框架，用于实时检测和表征核反应堆中的重放攻击。


<details>
  <summary>Details</summary>
Motivation: 下一代先进核反应堆依赖数字化控制系统，数据完整性对抗欺骗攻击至关重要，但现有方法存在局限性，如依赖合成数据或无法捕捉未建模动态。

Method: 结合自编码器和定制化windowSHAP算法，提出无监督可解释AI框架，用于检测、识别重放攻击的源头、时间和类型。

Result: 在Purdue核反应堆PUR-1的真实数据集上测试，框架能以95%以上的准确率检测和识别重放攻击。

Conclusion: 该框架为核反应堆中的重放攻击提供了高效且可解释的解决方案。

Abstract: Next generation advanced nuclear reactors are expected to be smaller both in
size and power output, relying extensively on fully digital instrumentation and
control systems. These reactors will generate a large flow of information in
the form of multivariate time series data, conveying simultaneously various non
linear cyber physical, process, control, sensor, and operational states.
Ensuring data integrity against deception attacks is becoming increasingly
important for networked communication and a requirement for safe and reliable
operation. Current efforts to address replay attacks, almost universally focus
on watermarking or supervised anomaly detection approaches without further
identifying and characterizing the root cause of the anomaly. In addition,
these approaches rely mostly on synthetic data with uncorrelated Gaussian
process and measurement noise and full state feedback or are limited to
univariate signals, signal stationarity, linear quadratic regulators, or other
linear-time invariant state-space which may fail to capture any unmodeled
system dynamics. In the realm of regulated nuclear cyber-physical systems,
additional work is needed on characterization of replay attacks and
explainability of predictions using real data. Here, we propose an unsupervised
explainable AI framework based on a combination of autoencoder and customized
windowSHAP algorithm to fully characterize real-time replay attacks, i.e.,
detection, source identification, timing and type, of increasing complexity
during a dynamic time evolving reactor process. The proposed XAI framework was
benchmarked on several real world datasets from Purdue's nuclear reactor PUR-1
with up to six signals concurrently being replayed. In all cases, the XAI
framework was able to detect and identify the source and number of signals
being replayed and the duration of the falsification with 95 percent or better
accuracy.

</details>


### [194] [Energy-Efficient Stochastic Computing (SC) Neural Networks for Internet of Things Devices With Layer-Wise Adjustable Sequence Length (ASL)](https://arxiv.org/abs/2508.09163)
*Ziheng Wang,Pedro Reviriego,Farzad Niknia,Zhen Gao,Javier Conde,Shanshan Liu,Fabrizio Lombardi*

Main category: cs.LG

TL;DR: 本文提出了一种名为ASL（可调序列长度）的新方案，将混合精度概念应用于随机计算神经网络，通过理论模型和实际验证，显著降低了能耗和延迟。


<details>
  <summary>Details</summary>
Motivation: 随机计算（SC）在资源受限场景（如物联网）中作为低功耗替代方案具有潜力，但层间混合精度实现的进一步优化尚未探索。

Method: 引入基于算子范数的理论模型，分析截断噪声的累积传播，并通过随机森林回归进行敏感性分析，提出粗粒度和细粒度两种截断策略。

Result: 在32nm工艺下合成的SC MLP上，ASL方案可降低60%以上的能耗和延迟，且精度损失可忽略。

Conclusion: ASL方案在物联网应用中具有可行性，并突显了混合精度截断在SC设计中的独特优势。

Abstract: Stochastic computing (SC) has emerged as an efficient low-power alternative
for deploying neural networks (NNs) in resource-limited scenarios, such as the
Internet of Things (IoT). By encoding values as serial bitstreams, SC
significantly reduces energy dissipation compared to conventional
floating-point (FP) designs; however, further improvement of layer-wise
mixed-precision implementation for SC remains unexplored. This article
introduces Adjustable Sequence Length (ASL), a novel scheme that applies
mixed-precision concepts specifically to SC NNs. By introducing an
operator-norm-based theoretical model, this article shows that truncation noise
can cumulatively propagate through the layers by the estimated amplification
factors. An extended sensitivity analysis is presented, using random forest
(RF) regression to evaluate multilayer truncation effects and validate the
alignment of theoretical predictions with practical network behaviors. To
accommodate different application scenarios, this article proposes two
truncation strategies (coarse-grained and fine-grained), which apply diverse
sequence length configurations at each layer. Evaluations on a pipelined SC MLP
synthesized at 32nm demonstrate that ASL can reduce energy and latency
overheads by up to over 60% with negligible accuracy loss. It confirms the
feasibility of the ASL scheme for IoT applications and highlights the distinct
advantages of mixed-precision truncation in SC designs.

</details>


### [195] [Generating Feasible and Diverse Synthetic Populations Using Diffusion Models](https://arxiv.org/abs/2508.09164)
*Min Tang,Peng Lu,Qing Feng*

Main category: cs.LG

TL;DR: 提出了一种基于扩散模型的人口合成方法，用于估计人口的联合分布，优于VAE和GAN方法。


<details>
  <summary>Details</summary>
Motivation: 解决高维属性下调查数据稀疏导致的联合分布建模困难问题。

Method: 使用扩散模型生成合成人口，恢复缺失的采样零并减少结构零。

Result: 新方法在可行性和多样性上优于VAE和GAN。

Conclusion: 扩散模型在人口合成中表现更优，平衡了可行性与多样性。

Abstract: Population synthesis is a critical task that involves generating synthetic
yet realistic representations of populations. It is a fundamental problem in
agent-based modeling (ABM), which has become the standard to analyze
intelligent transportation systems. The synthetic population serves as the
primary input for ABM transportation simulation, with traveling agents
represented by population members. However, when the number of attributes
describing agents becomes large, survey data often cannot densely support the
joint distribution of the attributes in the population due to the curse of
dimensionality. This sparsity makes it difficult to accurately model and
produce the population. Interestingly, deep generative models trained from
available sample data can potentially synthesize possible attribute
combinations that present in the actual population but do not exist in the
sample data(called sampling zeros). Nevertheless, this comes at the cost of
falsely generating the infeasible attribute combinations that do not exist in
the population (called structural zeros). In this study, a novel diffusion
model-based population synthesis method is proposed to estimate the underlying
joint distribution of a population. This approach enables the recovery of
numerous missing sampling zeros while keeping the generated structural zeros
minimal. Our method is compared with other recently proposed approaches such as
Variational Autoencoders (VAE) and Generative Adversarial Network (GAN)
approaches, which have shown success in high dimensional tabular population
synthesis. We assess the performance of the synthesized outputs using a range
of metrics, including marginal distribution similarity, feasibility, and
diversity. The results demonstrate that our proposed method outperforms
previous approaches in achieving a better balance between the feasibility and
diversity of the synthesized population.

</details>


### [196] [Masked Training for Robust Arrhythmia Detection from Digitalized Multiple Layout ECG Images](https://arxiv.org/abs/2508.09165)
*Shanwei Zhang,Deyun Zhang,Yirao Tao,Kexin Wang,Shijia Geng,Jun Li,Qinghao Zhao,Xingpeng Liu,Yuxi Zhou,Shenda Hong*

Main category: cs.LG

TL;DR: PatchECG框架通过掩码训练策略解决了不同ECG布局导致的信号异步和部分缺失问题，实现了对心律失常的关键识别，并在实验中表现出优越的鲁棒性和性能。


<details>
  <summary>Details</summary>
Motivation: 不同医院的ECG布局差异导致信号异步和部分缺失，现有模型难以应对这一挑战。

Method: 提出PatchECG框架，基于掩码训练策略，自适应学习缺失表示，关注关键块与导联间的协作依赖。

Result: 在PTB-XL数据集和真实ECG图像数据上，AUROC分别达到0.835和0.778，性能优于经典方法和当前最优模型。

Conclusion: PatchECG在多种ECG布局下表现稳定且性能优越，为心律失常诊断提供了有效解决方案。

Abstract: Electrocardiogram (ECG) as an important tool for diagnosing cardiovascular
diseases such as arrhythmia. Due to the differences in ECG layouts used by
different hospitals, the digitized signals exhibit asynchronous lead time and
partial blackout loss, which poses a serious challenge to existing models. To
address this challenge, the study introduced PatchECG, a framework for adaptive
variable block count missing representation learning based on a masking
training strategy, which automatically focuses on key patches with
collaborative dependencies between leads, thereby achieving key recognition of
arrhythmia in ECGs with different layouts. Experiments were conducted on the
PTB-XL dataset and 21388 asynchronous ECG images generated using ECG image kit
tool, using the 23 Subclasses as labels. The proposed method demonstrated
strong robustness under different layouts, with average Area Under the Receiver
Operating Characteristic Curve (AUROC) of 0.835 and remained stable (unchanged
with layout changes). In external validation based on 400 real ECG images data
from Chaoyang Hospital, the AUROC for atrial fibrillation diagnosis reached
0.778; On 12 x 1 layout ECGs, AUROC reaches 0.893. This result is superior to
various classic interpolation and baseline methods, and compared to the current
optimal large-scale pre-training model ECGFounder, it has improved by 0.111 and
0.19.

</details>


### [197] [SVGen: Interpretable Vector Graphics Generation with Large Language Models](https://arxiv.org/abs/2508.09168)
*Feiyu Wang,Zhiyuan Zhao,Yuandong Liu,Da Zhang,Junyu Gao,Hao Sun,Xuelong Li*

Main category: cs.LG

TL;DR: SVG-1M数据集和SVGen模型解决了从自然语言生成精确SVG代码的挑战，提高了效率和准确性。


<details>
  <summary>Details</summary>
Motivation: 将创意转化为精确的矢量图形耗时且复杂，需要更好的工具支持。

Method: 构建SVG-1M数据集，结合数据增强和标注，开发SVGen模型，采用课程学习和强化学习优化。

Result: SVGen在效果和效率上优于通用大模型和传统渲染方法。

Conclusion: SVGen为前端开发和设计提供了高效、准确的SVG生成解决方案。

Abstract: Scalable Vector Graphics (SVG) is widely used in front-end development and
UI/UX design due to its scalability, editability, and rendering efficiency.
However, turning creative ideas into precise vector graphics remains a
time-consuming challenge. To address this, we introduce SVG-1M, a large-scale
dataset of high-quality SVGs paired with natural language descriptions. Through
advanced data augmentation and annotation, we create well-aligned Text to SVG
training pairs, including a subset with Chain of Thought annotations for
enhanced semantic guidance. Based on this dataset, we propose SVGen, an
end-to-end model that generates SVG code from natural language inputs. Our
approach ensures semantic accuracy and structural completeness, supported by
curriculum learning and reinforcement learning optimization. Experiments show
that SVGen outperforms general large models and traditional rendering methods
in both effectiveness and efficiency. Code, model, and dataset are available on
GitHub.

</details>


### [198] [Multimodal RAG Enhanced Visual Description](https://arxiv.org/abs/2508.09170)
*Amit Kumar Jaiswal,Haiming Liu,Ingo Frommholz*

Main category: cs.LG

TL;DR: 论文提出了一种轻量级、无需训练的方法，利用检索增强生成（RAG）跨模态线性映射，解决多模态模型中文本与视觉表示不对齐的问题。


<details>
  <summary>Details</summary>
Motivation: 预训练大型多模态模型（LMMs）存在模态间隙，即文本与视觉表示在共享嵌入空间中的不对齐问题，而微调成本高昂且不切实际。

Method: 采用检索增强生成（RAG）技术，通过线性映射高效扩展模态，并在推理时利用语言模型生成新的文本描述。

Result: 在两个基准多模态数据集上的实验结果显示显著改进。

Conclusion: 该方法为多模态对齐提供了一种高效且低成本的解决方案。

Abstract: Textual descriptions for multimodal inputs entail recurrent refinement of
queries to produce relevant output images. Despite efforts to address
challenges such as scaling model size and data volume, the cost associated with
pre-training and fine-tuning remains substantial. However, pre-trained large
multimodal models (LMMs) encounter a modality gap, characterised by a
misalignment between textual and visual representations within a common
embedding space. Although fine-tuning can potentially mitigate this gap, it is
typically expensive and impractical due to the requirement for extensive
domain-driven data. To overcome this challenge, we propose a lightweight
training-free approach utilising Retrieval-Augmented Generation (RAG) to extend
across the modality using a linear mapping, which can be computed efficiently.
During inference, this mapping is applied to images embedded by an LMM enabling
retrieval of closest textual descriptions from the training set. These textual
descriptions, in conjunction with an instruction, cater as an input prompt for
the language model to generate new textual descriptions. In addition, we
introduce an iterative technique for distilling the mapping by generating
synthetic descriptions via the language model facilitating optimisation for
standard utilised image description measures. Experimental results on two
benchmark multimodal datasets demonstrate significant improvements.

</details>


### [199] [FedMP: Tackling Medical Feature Heterogeneity in Federated Learning from a Manifold Perspective](https://arxiv.org/abs/2508.09174)
*Zhekai Zhou,Shudong Liu,Zhaokun Zhou,Yang Liu,Qiang Yang,Yuesheng Zhu,Guibo Luo*

Main category: cs.LG

TL;DR: FedMP是一种针对非独立同分布（non-IID）数据的联邦学习方法，通过随机特征流形补全和类原型对齐提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 解决联邦学习在非IID数据（尤其是医学影像领域）中因特征分布差异导致的模型收敛和性能问题。

Method: FedMP采用随机特征流形补全丰富客户端分类器训练空间，利用类原型在语义一致子空间中对齐特征流形。

Result: 在多个医学影像和多域自然图像数据集上，FedMP优于现有联邦学习算法。

Conclusion: FedMP有效提升了非IID场景下联邦学习的性能，并分析了流形维度、通信效率和隐私影响。

Abstract: Federated learning (FL) is a decentralized machine learning paradigm in which
multiple clients collaboratively train a shared model without sharing their
local private data. However, real-world applications of FL frequently encounter
challenges arising from the non-identically and independently distributed
(non-IID) local datasets across participating clients, which is particularly
pronounced in the field of medical imaging, where shifts in image feature
distributions significantly hinder the global model's convergence and
performance. To address this challenge, we propose FedMP, a novel method
designed to enhance FL under non-IID scenarios. FedMP employs stochastic
feature manifold completion to enrich the training space of individual client
classifiers, and leverages class-prototypes to guide the alignment of feature
manifolds across clients within semantically consistent subspaces, facilitating
the construction of more distinct decision boundaries. We validate the
effectiveness of FedMP on multiple medical imaging datasets, including those
with real-world multi-center distributions, as well as on a multi-domain
natural image dataset. The experimental results demonstrate that FedMP
outperforms existing FL algorithms. Additionally, we analyze the impact of
manifold dimensionality, communication efficiency, and privacy implications of
feature exposure in our method.

</details>


### [200] [DQT: Dynamic Quantization Training via Dequantization-Free Nested Integer Arithmetic](https://arxiv.org/abs/2508.09176)
*Hazem Hesham Yousef Shalby,Fabrizio Pittorino,Francesca Palermo,Diana Trojaniello,Manuel Roveri*

Main category: cs.LG

TL;DR: DQT提出了一种动态量化训练框架，通过嵌套整数表示和整数运算实现高效动态量化，避免了传统方法的浮点运算瓶颈，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 现有动态量化方法需要昂贵的浮点运算切换精度，破坏了整数硬件范式并影响性能增益。

Method: DQT采用嵌套整数表示和定制整数运算，通过低成本位移操作实现动态精度切换。

Result: 在ImageNet上，4位动态ResNet50达到77.00% top-1准确率，优于静态和动态方法，且切换成本仅为28.3M位移操作。

Conclusion: DQT为高效自适应AI开辟了新途径，显著降低了动态量化的计算成本。

Abstract: The deployment of deep neural networks on resource-constrained devices relies
on quantization. While static, uniform quantization applies a fixed bit-width
to all inputs, it fails to adapt to their varying complexity. Dynamic,
instance-based mixed-precision quantization promises a superior
accuracy-efficiency trade-off by allocating higher precision only when needed.
However, a critical bottleneck remains: existing methods require a costly
dequantize-to-float and requantize-to-integer cycle to change precision,
breaking the integer-only hardware paradigm and compromising performance gains.
This paper introduces Dynamic Quantization Training (DQT), a novel framework
that removes this bottleneck. At the core of DQT is a nested integer
representation where lower-precision values are bit-wise embedded within
higher-precision ones. This design, coupled with custom integer-only
arithmetic, allows for on-the-fly bit-width switching through a near-zero-cost
bit-shift operation. This makes DQT the first quantization framework to enable
both dequantization-free static mixed-precision of the backbone network, and
truly efficient dynamic, instance-based quantization through a lightweight
controller that decides at runtime how to quantize each layer. We demonstrate
DQT state-of-the-art performance on ResNet18 on CIFAR-10 and ResNet50 on
ImageNet. On ImageNet, our 4-bit dynamic ResNet50 achieves 77.00% top-1
accuracy, an improvement over leading static (LSQ, 76.70%) and dynamic (DQNET,
76.94%) methods at a comparable BitOPs budget. Crucially, DQT achieves this
with a bit-width transition cost of only 28.3M simple bit-shift operations, a
drastic improvement over the 56.6M costly Multiply-Accumulate (MAC)
floating-point operations required by previous dynamic approaches - unlocking a
new frontier in efficient, adaptive AI.

</details>


### [201] [scAGC: Learning Adaptive Cell Graphs with Contrastive Guidance for Single-Cell Clustering](https://arxiv.org/abs/2508.09180)
*Huifa Li,Jie Fu,Xinlin Zhuang,Haolin Yang,Xinpeng Ling,Tong Cheng,Haochen xue,Imran Razzak,Zhili Chen*

Main category: cs.LG

TL;DR: 提出了一种名为scAGC的单细胞聚类方法，通过自适应学习细胞图和对比指导，解决了传统方法在高维和稀疏数据中的局限性。


<details>
  <summary>Details</summary>
Motivation: 单细胞RNA测序数据的高维性和稀疏性导致传统聚类方法效果不佳，现有方法依赖静态图结构且对噪声敏感，无法捕捉长尾分布。

Method: scAGC结合拓扑自适应图自编码器和可微Gumbel-Softmax采样策略，动态优化图结构；引入ZINB损失和对比学习目标，提升鲁棒性和稳定性。

Result: 在9个真实数据集上，scAGC在NMI和ARI指标上均优于其他方法。

Conclusion: scAGC通过自适应图学习和对比指导，显著提升了单细胞聚类的准确性和鲁棒性。

Abstract: Accurate cell type annotation is a crucial step in analyzing single-cell RNA
sequencing (scRNA-seq) data, which provides valuable insights into cellular
heterogeneity. However, due to the high dimensionality and prevalence of zero
elements in scRNA-seq data, traditional clustering methods face significant
statistical and computational challenges. While some advanced methods use graph
neural networks to model cell-cell relationships, they often depend on static
graph structures that are sensitive to noise and fail to capture the
long-tailed distribution inherent in single-cell populations.To address these
limitations, we propose scAGC, a single-cell clustering method that learns
adaptive cell graphs with contrastive guidance. Our approach optimizes feature
representations and cell graphs simultaneously in an end-to-end manner.
Specifically, we introduce a topology-adaptive graph autoencoder that leverages
a differentiable Gumbel-Softmax sampling strategy to dynamically refine the
graph structure during training. This adaptive mechanism mitigates the problem
of a long-tailed degree distribution by promoting a more balanced neighborhood
structure. To model the discrete, over-dispersed, and zero-inflated nature of
scRNA-seq data, we integrate a Zero-Inflated Negative Binomial (ZINB) loss for
robust feature reconstruction. Furthermore, a contrastive learning objective is
incorporated to regularize the graph learning process and prevent abrupt
changes in the graph topology, ensuring stability and enhancing convergence.
Comprehensive experiments on 9 real scRNA-seq datasets demonstrate that scAGC
consistently outperforms other state-of-the-art methods, yielding the best NMI
and ARI scores on 9 and 7 datasets, respectively.Our code is available at
Anonymous Github.

</details>


### [202] [Long-Term Client Selection for Federated Learning with Non-IID Data: A Truthful Auction Approach](https://arxiv.org/abs/2508.09181)
*Jinghong Tan,Zhian Liu,Kun Guo,Mingxiong Zhao*

Main category: cs.LG

TL;DR: 论文提出了一种基于诚实拍卖的长期客户选择联邦学习方案（LCSFLA），用于解决车联网中非独立同分布数据导致的模型收敛和精度问题。


<details>
  <summary>Details</summary>
Motivation: 车联网中智能车辆的非独立同分布数据影响联邦学习模型的收敛和精度，传统客户选择方法存在资源浪费和信息不对称问题。

Method: 提出LCSFLA方案，结合长期数据质量评估和拍卖机制，激励客户参与并确保信息真实性。

Result: 实验证明LCSFLA能有效缓解非独立同分布数据导致的性能下降。

Conclusion: LCSFLA通过长期数据质量评估和诚实拍卖机制，提升了联邦学习在车联网中的效果。

Abstract: Federated learning (FL) provides a decentralized framework that enables
universal model training through collaborative efforts on mobile nodes, such as
smart vehicles in the Internet of Vehicles (IoV). Each smart vehicle acts as a
mobile client, contributing to the process without uploading local data. This
method leverages non-independent and identically distributed (non-IID) training
data from different vehicles, influenced by various driving patterns and
environmental conditions, which can significantly impact model convergence and
accuracy. Although client selection can be a feasible solution for non-IID
issues, it faces challenges related to selection metrics. Traditional metrics
evaluate client data quality independently per round and require client
selection after all clients complete local training, leading to resource
wastage from unused training results. In the IoV context, where vehicles have
limited connectivity and computational resources, information asymmetry in
client selection risks clients submitting false information, potentially making
the selection ineffective. To tackle these challenges, we propose a novel
Long-term Client-Selection Federated Learning based on Truthful Auction
(LCSFLA). This scheme maximizes social welfare with consideration of long-term
data quality using a new assessment mechanism and energy costs, and the advised
auction mechanism with a deposit requirement incentivizes client participation
and ensures information truthfulness. We theoretically prove the incentive
compatibility and individual rationality of the advised incentive mechanism.
Experimental results on various datasets, including those from IoV scenarios,
demonstrate its effectiveness in mitigating performance degradation caused by
non-IID data.

</details>


### [203] [Breath as a biomarker: A survey of contact and contactless applications and approaches in respiratory monitoring](https://arxiv.org/abs/2508.09187)
*Almustapha A. Wakili,Babajide J. Asaju,Woosub Jung*

Main category: cs.LG

TL;DR: 综述探讨了呼吸分析的接触式与非接触式方法，重点介绍了机器学习和深度学习技术的应用，分析了其优缺点及未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 传统接触式呼吸监测方法存在舒适性和实用性挑战，非接触式方法（如Wi-Fi和声学传感）提供了无创监测的可能性。

Method: 比较了接触式与非接触式呼吸分析方法，详细介绍了数据预处理、特征提取和分类技术，以及适用的机器学习/深度学习模型。

Result: 非接触式方法在呼吸率检测、用户识别和疾病检测中表现出潜力，但仍面临数据集稀缺、多用户干扰和隐私问题。

Conclusion: 综述为呼吸分析的未来研究提供了框架，强调了可解释AI、联邦学习等新兴趋势的重要性。

Abstract: Breath analysis has emerged as a critical tool in health monitoring, offering
insights into respiratory function, disease detection, and continuous health
assessment. While traditional contact-based methods are reliable, they often
pose challenges in comfort and practicality, particularly for long-term
monitoring. This survey comprehensively examines contact-based and contactless
approaches, emphasizing recent advances in machine learning and deep learning
techniques applied to breath analysis. Contactless methods, including Wi-Fi
Channel State Information and acoustic sensing, are analyzed for their ability
to provide accurate, noninvasive respiratory monitoring. We explore a broad
range of applications, from single-user respiratory rate detection to
multi-user scenarios, user identification, and respiratory disease detection.
Furthermore, this survey details essential data preprocessing, feature
extraction, and classification techniques, offering comparative insights into
machine learning/deep learning models suited to each approach. Key challenges
like dataset scarcity, multi-user interference, and data privacy are also
discussed, along with emerging trends like Explainable AI, federated learning,
transfer learning, and hybrid modeling. By synthesizing current methodologies
and identifying open research directions, this survey offers a comprehensive
framework to guide future innovations in breath analysis, bridging advanced
technological capabilities with practical healthcare applications.

</details>


### [204] [Fine-Grained Safety Neurons with Training-Free Continual Projection to Reduce LLM Fine Tuning Risks](https://arxiv.org/abs/2508.09190)
*Bing Han,Feifei Zhao,Dongcheng Zhao,Guobin Shen,Ping Wu,Yu Shi,Yi Zeng*

Main category: cs.LG

TL;DR: 论文提出了一种细粒度安全神经元（FGSN）方法，通过无训练持续投影技术减少微调的安全风险，平衡安全性和实用性。


<details>
  <summary>Details</summary>
Motivation: 微调大型语言模型（LLM）会引入安全风险，现有防御方法对安全层和神经元的细粒度考虑不足。

Method: 提出FGSN方法，结合安全层与神经元的交互，定位稀疏且精确的安全神经元，并通过投影技术优化参数。

Result: 实验表明，FGSN显著降低有害分数和攻击成功率，同时保持模型实用性。

Conclusion: FGSN通过多维度异构安全神经元集群优化机制，实现了对未知安全问题的持续防御和泛化能力。

Abstract: Fine-tuning as service injects domain-specific knowledge into large language
models (LLMs), while challenging the original alignment mechanisms and
introducing safety risks. A series of defense strategies have been proposed for
the alignment, fine-tuning, and post-fine-tuning phases, where most
post-fine-tuning defenses rely on coarse-grained safety layer mapping. These
methods lack a comprehensive consideration of both safety layers and
fine-grained neurons, limiting their ability to efficiently balance safety and
utility. To address this, we propose the Fine-Grained Safety Neurons (FGSN)
with Training-Free Continual Projection method to reduce the fine-tuning safety
risks. FGSN inherently integrates the multi-scale interactions between safety
layers and neurons, localizing sparser and more precise fine-grained safety
neurons while minimizing interference with downstream task neurons. We then
project the safety neuron parameters onto safety directions, improving model
safety while aligning more closely with human preferences. Extensive
experiments across multiple fine-tuned LLM models demonstrate that our method
significantly reduce harmfulness scores and attack success rates with minimal
parameter modifications, while preserving the model's utility. Furthermore, by
introducing a task-specific, multi-dimensional heterogeneous safety neuron
cluster optimization mechanism, we achieve continual defense and generalization
capability against unforeseen emerging safety concerns.

</details>


### [205] [From Values to Tokens: An LLM-Driven Framework for Context-aware Time Series Forecasting via Symbolic Discretization](https://arxiv.org/abs/2508.09191)
*Xiaoyu Tao,Shilong Zhang,Mingyue Cheng,Daoyu Wang,Tingyue Pan,Bokai Pan,Changqing Zhang,Shijin Wang*

Main category: cs.LG

TL;DR: TokenCast是一个基于LLM的框架，通过语言符号表示统一处理上下文感知的时间序列预测。


<details>
  <summary>Details</summary>
Motivation: 解决历史数值序列与上下文特征（如非结构化文本数据）融合的挑战，提升预测准确性。

Method: 使用离散分词器将连续数值序列转化为时间标记，通过预训练LLM嵌入共享表示空间，并优化生成目标。

Result: 在多样化真实数据集上验证了TokenCast的有效性和通用性。

Conclusion: TokenCast为上下文感知的时间序列预测提供了一种有效的统一框架。

Abstract: Time series forecasting plays a vital role in supporting decision-making
across a wide range of critical applications, including energy, healthcare, and
finance. Despite recent advances, forecasting accuracy remains limited due to
the challenge of integrating historical numerical sequences with contextual
features, which often comprise unstructured textual data. To address this
challenge, we propose TokenCast, an LLM-driven framework that leverages
language-based symbolic representations as a unified intermediary for
context-aware time series forecasting. Specifically, TokenCast employs a
discrete tokenizer to transform continuous numerical sequences into temporal
tokens, enabling structural alignment with language-based inputs. To bridge the
semantic gap between modalities, both temporal and contextual tokens are
embedded into a shared representation space via a pre-trained large language
model (LLM), further optimized with autoregressive generative objectives.
Building upon this unified semantic space, the aligned LLM is subsequently
fine-tuned in a supervised manner to predict future temporal tokens, which are
then decoded back into the original numerical space. Extensive experiments on
diverse real-world datasets enriched with contextual features demonstrate the
effectiveness and generalizability of TokenCast.

</details>


### [206] [Diffusion LLMs Can Do Faster-Than-AR Inference via Discrete Diffusion Forcing](https://arxiv.org/abs/2508.09192)
*Xu Wang,Chenkai Xu,Yijie Jin,Jiachun Jin,Hao Zhang,Zhijie Deng*

Main category: cs.LG

TL;DR: 本文提出了一种名为离散扩散强制（D2F）的策略，通过改进扩散大语言模型（dLLMs）的推理效率，使其在生成速度上超越自回归（AR）模型。


<details>
  <summary>Details</summary>
Motivation: 现有的开源dLLMs在推理速度上未能超越类似规模的AR模型，因此需要一种方法提升其效率。

Method: D2F通过块级自回归生成和跨块并行解码，将dLLMs改造为AR-扩散混合范式，并结合非对称蒸馏和流水线并行解码算法。

Result: 实验显示，D2F dLLMs在GSM8K上的推理速度比LLaMA3和Qwen2.5快2.5倍以上，比LLaDA和Dream等原始dLLMs快50倍以上，同时保持输出质量。

Conclusion: D2F成功提升了dLLMs的推理效率，为高效文本生成提供了新思路。

Abstract: Diffusion Large Language Models (dLLMs) have emerged as a promising
alternative to autoregressive (AR) LLMs for text generation, with the potential
to decode multiple tokens in a single iteration. However, none of the existing
open-source dLLMs have achieved superior inference speed over AR LLMs of
similar size. This paper breaks this barrier based on a simple and effective
strategy named discrete diffusion forcing (D2F). D2F equips dLLMs with two key
capabilities: (1) block-wise autoregressive generation to enable KV cache
utilization; (2) prediction of following tokens without requiring completion of
prior blocks for inter-block parallel decoding. In this way, the vanilla dLLMs
are refurbished into an AR-diffusion hybrid paradigm for efficient inference.
D2F can be implemented with an asymmetric distillation process based on
pre-trained dLLMs. We further propose a pipelined parallel decoding algorithm,
which enables a trade-off between efficiency and efficacy. Empirically, D2F
dLLMs achieve more than $\mathbf{2.5\times}$ inference speed than LLaMA3 and
Qwen2.5 on GSM8K. Compared to vanilla dLLMs like LLaDA and Dream, the
acceleration can be more than $\mathbf{50\times}$ while maintaining comparable
output quality. The code is available at
https://github.com/zhijie-group/Discrete-Diffusion-Forcing.

</details>


### [207] [Multi-Objective Instruction-Aware Representation Learning in Procedural Content Generation RL](https://arxiv.org/abs/2508.09193)
*Sung-Hyun Kim,In-Chang Baek,Seo-Young Lee,Geum-Hwan Hwang,Kyung-Joong Kim*

Main category: cs.LG

TL;DR: MIPCGRL方法通过结合句子嵌入和多目标学习，提升了生成模型对复杂文本指令的响应能力，实现了13.8%的改进。


<details>
  <summary>Details</summary>
Motivation: 现有IPCGRL方法难以充分利用文本输入的丰富表达性，尤其在多目标指令下控制性不足。

Method: 提出MIPCGRL，结合句子嵌入和多目标表示学习，使用多标签分类和多头回归网络。

Result: 实验显示，MIPCGRL在多目标指令下的控制性提升13.8%。

Conclusion: MIPCGRL能更灵活地处理复杂指令，提升生成内容的表达性和可控性。

Abstract: Recent advancements in generative modeling emphasize the importance of
natural language as a highly expressive and accessible modality for controlling
content generation. However, existing instructed reinforcement learning for
procedural content generation (IPCGRL) method often struggle to leverage the
expressive richness of textual input, especially under complex, multi-objective
instructions, leading to limited controllability. To address this problem, we
propose \textit{MIPCGRL}, a multi-objective representation learning method for
instructed content generators, which incorporates sentence embeddings as
conditions. MIPCGRL effectively trains a multi-objective embedding space by
incorporating multi-label classification and multi-head regression networks.
Experimental results show that the proposed method achieves up to a 13.8\%
improvement in controllability with multi-objective instructions. The ability
to process complex instructions enables more expressive and flexible content
generation.

</details>


### [208] [Meta-Learning for Speeding Up Large Model Inference in Decentralized Environments](https://arxiv.org/abs/2508.09194)
*Yipeng Du,Zihao Wang,Ahmad Farhan,Claudio Angione,Harry Yang,Fielding Johnston,James P. Buban,Patrick Colangelo,Yue Zhao,Yuzhe Yang*

Main category: cs.LG

TL;DR: 论文提出了一种基于元学习的框架，用于在去中心化系统中自动选择最优的推理加速方法，以提高效率和性能。


<details>
  <summary>Details</summary>
Motivation: 大规模模型（如LLMs）的部署成本高昂，且面临可扩展性和数据安全的挑战。去中心化系统需要高效的推理加速方案来优化资源管理和响应速度。

Method: 引入元学习框架，通过学习历史性能数据，自动选择适合不同任务特性的最优加速策略。

Result: 该框架在效率和性能上优于传统方法，优化了决策过程。

Conclusion: 去中心化AI系统中的推理加速具有潜力，为更民主和经济可行的人工智能解决方案提供了路径。

Abstract: The deployment of large-scale models, such as large language models (LLMs),
incurs substantial costs due to their computational demands. To mitigate these
costs and address challenges related to scalability and data security, there is
a growing shift towards decentralized systems for model deployment, where
choosing efficient inference acceleration schemes become crucial to manage
computational resources effectively and enhance system responsiveness. In this
work, we address the challenge of selecting optimal acceleration methods in
decentralized systems by introducing a meta-learning-based framework. This
framework automates the selection process by learning from historical
performance data of various acceleration techniques across different tasks.
Unlike traditional methods that rely on random selection or expert intuition,
our approach systematically identifies the best acceleration strategies based
on the specific characteristics of each task. We demonstrate that our
meta-learning framework not only streamlines the decision-making process but
also consistently outperforms conventional methods in terms of efficiency and
performance. Our results highlight the potential of inference acceleration in
decentralized AI systems, offering a path towards more democratic and
economically feasible artificial intelligence solutions.

</details>


### [209] [ADT4Coupons: An Innovative Framework for Sequential Coupon Distribution in E-commerce](https://arxiv.org/abs/2508.09198)
*Li Kong,Bingzhe Wang,Zhou Chen,Suhan Hu,Yuchao Ma,Qi Qi,Suoyuan Song,Bicheng Jin*

Main category: cs.LG

TL;DR: 论文提出了一种名为ADT4Coupons的新框架，用于优化在线平台的优惠券分发策略，以提升长期收入。


<details>
  <summary>Details</summary>
Motivation: 现有优惠券分发策略未能充分利用平台与用户间的复杂序列交互，导致性能瓶颈。

Method: ADT4Coupons框架整合了通用场景、更全面的历史数据序列建模和高效迭代更新。

Result: 在真实工业数据集、公开数据集和合成数据集上的实验证明了该框架的优越性。

Conclusion: ADT4Coupons能有效提升长期收入，适用于多种实际营销场景。

Abstract: Coupon distribution is a critical marketing strategy used by online platforms
to boost revenue and enhance user engagement. Regrettably, existing coupon
distribution strategies fall far short of effectively leveraging the complex
sequential interactions between platforms and users. This critical oversight,
despite the abundance of e-commerce log data, has precipitated a performance
plateau. In this paper, we focus on the scene that the platforms make
sequential coupon distribution decision multiple times for various users, with
each user interacting with the platform repeatedly. Based on this marketing
scenario, we propose a novel marketing framework, named Aligned Decision
Transformer for Coupons (ADT4Coupons), to directly devise coupon distribution
policy for long-term revenue boosting. ADT4Coupons enables optimized online
decision-making in a variety of real-world marketing scenarios. It achieves
this by seamlessly integrating three key characteristics, general scenarios,
sequential modeling with more comprehensive historical data, and efficient
iterative updates within a unified framework. Furthermore, empirical results on
real-world industrial dataset, alongside public and synthetic datasets
demonstrate the superiority of our framework.

</details>


### [210] [Building Safer Sites: A Large-Scale Multi-Level Dataset for Construction Safety Research](https://arxiv.org/abs/2508.09203)
*Zhenhui Ou,Dawei Li,Zhen Tan,Wenlin Li,Huan Liu,Siyuan Song*

Main category: cs.LG

TL;DR: 论文介绍了Construction Safety Dataset (CSDataset)，一个多层级、综合性的建筑安全数据集，用于解决现有数据不足的问题，并支持机器学习和语言模型的应用。


<details>
  <summary>Details</summary>
Motivation: 现有建筑安全数据集数量有限且多样性不足，阻碍了深入分析。

Method: 构建CSDataset，整合OSHA的结构化属性和非结构化叙述，并进行初步基准测试和跨层级分析。

Result: 发现投诉驱动的检查与后续事故减少17.3%相关。

Conclusion: CSDataset为建筑安全研究提供了新工具，未来可进一步优化分析。

Abstract: Construction safety research is a critical field in civil engineering, aiming
to mitigate risks and prevent injuries through the analysis of site conditions
and human factors. However, the limited volume and lack of diversity in
existing construction safety datasets pose significant challenges to conducting
in-depth analyses. To address this research gap, this paper introduces the
Construction Safety Dataset (CSDataset), a well-organized comprehensive
multi-level dataset that encompasses incidents, inspections, and violations
recorded sourced from the Occupational Safety and Health Administration (OSHA).
This dataset uniquely integrates structured attributes with unstructured
narratives, facilitating a wide range of approaches driven by machine learning
and large language models. We also conduct a preliminary approach benchmarking
and various cross-level analyses using our dataset, offering insights to inform
and enhance future efforts in construction safety. For example, we found that
complaint-driven inspections were associated with a 17.3% reduction in the
likelihood of subsequent incidents. Our dataset and code are released at
https://github.com/zhenhuiou/Construction-Safety-Dataset-CSDataset.

</details>


### [211] [MoQE: Improve Quantization Model performance via Mixture of Quantization Experts](https://arxiv.org/abs/2508.09204)
*Jinhao Zhang,Yunquan Zhang,Boyang Zhang,Zeyu Liu,Daning Cheng*

Main category: cs.LG

TL;DR: MoQE是一种基于混合专家架构的量化推理框架，通过动态路由输入数据到最适合的量化专家模型，减轻单一量化模型的性能下降问题。


<details>
  <summary>Details</summary>
Motivation: 量化方法在提高模型效率和降低部署成本方面至关重要，但量化过程会引入精度下降。MoQE旨在通过混合量化专家模型联合提升量化模型的性能。

Method: MoQE结合多个全精度模型的量化变体作为专门的“量化专家”，并基于输入数据的特性动态路由到最合适的专家。设计了轻量级、结构感知的路由器模型，适用于CV和NLP任务。

Result: 在ResNet、LLaMA和Qwen模型家族以及ImageNet、WikiText、C4和OpenWebText等基准数据集上的实验表明，MoQE性能接近SOTA量化模型，且未显著增加推理延迟。

Conclusion: MoQE通过混合量化专家模型有效缓解了量化过程中的性能下降问题，同时保持了高效的推理性能。

Abstract: Quantization method plays a crucial role in improving model efficiency and
reducing deployment costs, enabling the widespread application of deep learning
models on resource-constrained devices. However, the quantization process
inevitably introduces accuracy degradation. In this paper, we propose Mixture
of Quantization Experts( abbr. MoQE), a quantization inference framework based
on the Mixture-of-Experts (MoE) architecture, aiming to jointly improve the
performance of quantization models. MoQE combines multiple quantization
variants of one full-precision model as specialized "quantization experts" and
dynamically routes input data to the most suitable expert based on its
characteristics. MoQE alleviates the performance degradation commonly seen in
single quantization models through specialization quantization expert models.
We design lightweight, structure-aware router models tailored for both CV and
NLP tasks. Experimental evaluations on ResNet, LLaMA, and Qwen model families
across benchmark datasets including ImageNet, WikiText, C4, and OpenWebText
demonstrate that MoQE achieves performance comparable to SOTA quantization
model, without incurring significant increases in inference latency.

</details>


### [212] [The First Differentiable Transfer-Based Algorithm for Discrete MicroLED Repair](https://arxiv.org/abs/2508.09206)
*Ning-Yuan Lue*

Main category: cs.LG

TL;DR: 提出了一种基于可微分转移模块的修复算法，用于优化微LED制造中的转移步骤，减少XY平台运动，并在大规模阵列中实现快速规划。


<details>
  <summary>Details</summary>
Motivation: 微LED制造中的选择性转移需要高效的计算模型来优化转移序列，适应不同的优化目标，而现有方法（如局部搜索或强化学习）存在灵活性不足或训练复杂的问题。

Method: 提出了一种可微分转移模块，通过梯度优化训练，能够建模离散转移步骤，支持灵活的优化目标设计（如最小化步骤数）。

Result: 实验显示，该方法在2000x2000阵列上减少了50%的转移步骤，规划时间低于2分钟。

Conclusion: 该方法为微LED修复提供了高效、灵活的解决方案，适用于AR/VR和下一代显示器的制造。

Abstract: Laser-enabled selective transfer, a key process in high-throughput microLED
fabrication, requires computational models that can plan shift sequences to
minimize motion of XY stages and adapt to varying optimization objectives
across the substrate. We propose the first repair algorithm based on a
differentiable transfer module designed to model discrete shifts of transfer
platforms, while remaining trainable via gradient-based optimization. Compared
to local proximity searching algorithms, our approach achieves superior repair
performance and enables more flexible objective designs, such as minimizing the
number of steps. Unlike reinforcement learning (RL)-based approaches, our
method eliminates the need for handcrafted feature extractors and trains
significantly faster, allowing scalability to large arrays. Experiments show a
50% reduction in transfer steps and sub-2-minute planning time on 2000x2000
arrays. This method provides a practical and adaptable solution for
accelerating microLED repair in AR/VR and next-generation display fabrication.

</details>


### [213] [Hierarchical Adaptive networks with Task vectors for Test-Time Adaptation](https://arxiv.org/abs/2508.09223)
*Sameer Ambekar,Daniel M. Lang,Julia A. Schnabel*

Main category: cs.LG

TL;DR: Hi-Vec提出了一种分层自适应网络，通过动态选择层和权重合并机制，提升预训练模型在测试时适应数据分布变化的能力。


<details>
  <summary>Details</summary>
Motivation: 解决传统方法因单维线性分类层无法应对复杂分布变化的问题。

Method: 采用分层组织层，动态选择最优层，合并权重至其他层，并通过线性层一致性防止噪声批次干扰。

Result: 在多种挑战性场景和目标数据集上表现优异，提升了鲁棒性并减少了不确定性。

Conclusion: Hi-Vec显著提升了现有方法对复杂分布变化的适应能力。

Abstract: Test-time adaptation allows pretrained models to adjust to incoming data
streams, addressing distribution shifts between source and target domains.
However, standard methods rely on single-dimensional linear classification
layers, which often fail to handle diverse and complex shifts. We propose
Hierarchical Adaptive Networks with Task Vectors (Hi-Vec), which leverages
multiple layers of increasing size for dynamic test-time adaptation. By
decomposing the encoder's representation space into such hierarchically
organized layers, Hi-Vec, in a plug-and-play manner, allows existing methods to
adapt to shifts of varying complexity. Our contributions are threefold: First,
we propose dynamic layer selection for automatic identification of the optimal
layer for adaptation to each test batch. Second, we propose a mechanism that
merges weights from the dynamic layer to other layers, ensuring all layers
receive target information. Third, we propose linear layer agreement that acts
as a gating function, preventing erroneous fine-tuning by adaptation on noisy
batches. We rigorously evaluate the performance of Hi-Vec in challenging
scenarios and on multiple target datasets, proving its strong capability to
advance state-of-the-art methods. Our results show that Hi-Vec improves
robustness, addresses uncertainty, and handles limited batch sizes and
increased outlier rates.

</details>


### [214] [GSMT: Graph Fusion and Spatiotemporal TaskCorrection for Multi-Bus Trajectory Prediction](https://arxiv.org/abs/2508.09227)
*Fan Ding,Hwa Hui Tew,Junn Yong Loo,Susilawati,LiTong Liu,Fang Yu Leong,Xuewen Luo,Kar Keong Chin,Jia Jun Gan*

Main category: cs.LG

TL;DR: GSMT是一种结合图注意力网络和序列到序列RNN的混合模型，用于公交车轨迹预测，通过任务校正器优化预测结果，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 在数据有限的发展中地区，仅依赖GPS数据预测公交车轨迹具有挑战性，需高效模型解决。

Method: GSMT整合GAT和RNN，利用任务校正器聚类历史轨迹并优化预测，结合动态和静态信息进行两阶段预测。

Result: 在吉隆坡真实数据集上，GSMT在短期和长期轨迹预测中表现优异。

Conclusion: GSMT通过混合模型和任务校正器，显著提升了公交车轨迹预测的准确性。

Abstract: Accurate trajectory prediction for buses is crucial in intelligent
transportation systems, particularly within urban environments. In developing
regions where access to multimodal data is limited, relying solely on onboard
GPS data remains indispensable despite inherent challenges. To address this
problem, we propose GSMT, a hybrid model that integrates a Graph Attention
Network (GAT) with a sequence-to-sequence Recurrent Neural Network (RNN), and
incorporates a task corrector capable of extracting complex behavioral patterns
from large-scale trajectory data. The task corrector clusters historical
trajectories to identify distinct motion patterns and fine-tunes the
predictions generated by the GAT and RNN. Specifically, GSMT fuses dynamic bus
information and static station information through embedded hybrid networks to
perform trajectory prediction, and applies the task corrector for secondary
refinement after the initial predictions are generated. This two-stage approach
enables multi-node trajectory prediction among buses operating in dense urban
traffic environments under complex conditions. Experiments conducted on a
real-world dataset from Kuala Lumpur, Malaysia, demonstrate that our method
significantly outperforms existing approaches, achieving superior performance
in both short-term and long-term trajectory prediction tasks.

</details>


### [215] [Blockchain Network Analysis using Quantum Inspired Graph Neural Networks & Ensemble Models](https://arxiv.org/abs/2508.09237)
*Luigi D'Amico,Daniel De Rosso,Ninad Dixit,Raul Salles de Padua,Samuel Palmer,Samuel Mugel,Román Orús,Holger Eble,Ali Abedi*

Main category: cs.LG

TL;DR: 论文提出了一种结合量子启发图神经网络（QI-GNN）和集成模型（QBoost或随机森林分类器）的新方法，用于区块链网络中的反洗钱（AML）交易检测。通过引入CP分解层，提升了处理复杂数据结构的能力，实验结果显示F2分数达74.8%，表明量子启发技术在金融安全领域的潜力。


<details>
  <summary>Details</summary>
Motivation: 金融科技领域快速发展，区块链网络中非法交易的检测仍是一个关键挑战，需要创新解决方案。

Method: 结合量子启发图神经网络（QI-GNN）和集成模型（QBoost或随机森林分类器），并引入CP分解层以增强复杂数据处理能力。

Result: 实验结果显示，该方法在检测欺诈交易时F2分数为74.8%，优于传统机器学习方法。

Conclusion: 量子启发算法在金融安全领域具有潜力，值得进一步探索和推广。

Abstract: In the rapidly evolving domain of financial technology, the detection of
illicit transactions within blockchain networks remains a critical challenge,
necessitating robust and innovative solutions. This work proposes a novel
approach by combining Quantum Inspired Graph Neural Networks (QI-GNN) with
flexibility of choice of an Ensemble Model using QBoost or a classic model such
as Random Forrest Classifier. This system is tailored specifically for
blockchain network analysis in anti-money laundering (AML) efforts. Our
methodology to design this system incorporates a novel component, a Canonical
Polyadic (CP) decomposition layer within the graph neural network framework,
enhancing its capability to process and analyze complex data structures
efficiently. Our technical approach has undergone rigorous evaluation against
classical machine learning implementations, achieving an F2 score of 74.8% in
detecting fraudulent transactions. These results highlight the potential of
quantum-inspired techniques, supplemented by the structural advancements of the
CP layer, to not only match but potentially exceed traditional methods in
complex network analysis for financial security. The findings advocate for a
broader adoption and further exploration of quantum-inspired algorithms within
the financial sector to effectively combat fraud.

</details>


### [216] [LLM Empowered Prototype Learning for Zero and Few-Shot Tasks on Tabular Data](https://arxiv.org/abs/2508.09263)
*Peng Wang,Dongsheng Wang,He Zhao,Hangting Ye,Dandan Guo,Yi Chang*

Main category: cs.LG

TL;DR: 提出了一种基于LLM的原型估计框架，用于零样本和小样本表格学习，无需训练分类器或微调LLM。


<details>
  <summary>Details</summary>
Motivation: 探索大型语言模型（LLM）在表格数据建模中的潜力，解决零样本和小样本场景下的挑战。

Method: 通过任务和特征描述生成特征值，构建零样本原型，并融合小样本数据增强性能。

Result: 实验证明该方法在零样本和小样本表格学习中有效。

Conclusion: 该框架避免了基于示例提示的限制，提供了可扩展且稳健的解决方案。

Abstract: Recent breakthroughs in large language models (LLMs) have opened the door to
in-depth investigation of their potential in tabular data modeling. However,
effectively utilizing advanced LLMs in few-shot and even zero-shot scenarios is
still challenging. To this end, we propose a novel LLM-based prototype
estimation framework for tabular learning. Our key idea is to query the LLM to
generate feature values based example-free prompt, which solely relies on task
and feature descriptions. With the feature values generated by LLM, we can
build a zero-shot prototype in a training-free manner, which can be further
enhanced by fusing few-shot samples, avoiding training a classifier or
finetuning the LLMs. Thanks to the example-free prompt and prototype
estimation, ours bypasses the constraints brought by the example-based prompt,
providing a scalable and robust framework. Extensive experiments demonstrate
the effectiveness of ours in zero and few-shot tabular learning.

</details>


### [217] [Detection of Odor Presence via Deep Neural Networks](https://arxiv.org/abs/2508.09264)
*Matin Hassanloo,Ali Zareh,Mehmet Kemal Özdemir*

Main category: cs.LG

TL;DR: 论文提出了一种基于深度学习的方法，通过分析嗅觉球的局部场电位（LFPs）实现单次试验气味检测，验证了两个假设并显著优于先前基准。


<details>
  <summary>Details</summary>
Motivation: 当前气味检测传感器在复杂混合物中表现不佳，非侵入性记录缺乏单次试验可靠性，因此需要开发一种通用系统。

Method: 使用互补的一维卷积网络（ResCNN和AttentionCNN）解码多通道嗅觉球LFPs，验证了两个假设。

Result: 在七只清醒小鼠的2,349次试验中，模型平均准确率为86.6%，F1分数81.0%，AUC为0.9247，显著优于先前方法。

Conclusion: 研究证实了从LFPs中实现单次试验气味检测的可行性，并展示了深度学习在理解嗅觉表征中的潜力。

Abstract: Odor detection underpins food safety, environmental monitoring, medical
diagnostics, and many more fields. The current artificial sensors developed for
odor detection struggle with complex mixtures while non-invasive recordings
lack reliable single-trial fidelity. To develop a general system for odor
detection, in this study we present a preliminary work where we aim to test two
hypotheses: (i) that spectral features of local field potentials (LFPs) are
sufficient for robust single-trial odor detection and (ii) that signals from
the olfactory bulb alone are adequate. To test two hypotheses, we propose an
ensemble of complementary one-dimensional convolutional networks (ResCNN and
AttentionCNN) that decodes the presence of odor from multichannel olfactory
bulb LFPs. Tested on 2,349 trials from seven awake mice, our final ensemble
model supports both hypotheses, achieving a mean accuracy of 86.6%, an F1-score
of 81.0%, and an AUC of 0.9247, substantially outperforming previous
benchmarks. In addition, the t-SNE visualization confirms that our framework
captures biologically significant signatures. These findings establish the
feasibility of robust single-trial detection of the presence of odor from
extracellular LFPs, as well as demonstrate the potential of deep learning
models to provide a deeper understanding of olfactory representations.

</details>


### [218] [Over-Squashing in GNNs and Causal Inference of Rewiring Strategies](https://arxiv.org/abs/2508.09265)
*Danial Saber,Amirali Salehi-Abari*

Main category: cs.LG

TL;DR: 该论文提出了一种评估图神经网络（GNNs）中信息过度压缩（over-squashing）的方法，并研究了重连（rewiring）技术对其的影响。


<details>
  <summary>Details</summary>
Motivation: 信息过度压缩限制了GNNs的表达能力，但缺乏直接的评估指标阻碍了重连技术的实际应用。

Method: 提出了一种基于拓扑结构的评估方法，通过节点间互敏感性的衰减率量化信息过度压缩，并扩展为四种图级统计量。

Result: 实验表明，大多数图分类数据集存在信息过度压缩，重连技术能有效缓解，但效果因数据集和方法而异；节点分类中信息过度压缩不明显，重连可能适得其反。

Conclusion: 重连技术在信息过度压缩严重且适度修正时最有效，过度或不当使用可能损害性能。论文提供了诊断工具帮助实践者决策。

Abstract: Graph neural networks (GNNs) have exhibited state-of-the-art performance
across wide-range of domains such as recommender systems, material design, and
drug repurposing. Yet message-passing GNNs suffer from over-squashing --
exponential compression of long-range information from distant nodes -- which
limits expressivity. Rewiring techniques can ease this bottleneck; but their
practical impacts are unclear due to the lack of a direct empirical
over-squashing metric. We propose a rigorous, topology-focused method for
assessing over-squashing between node pairs using the decay rate of their
mutual sensitivity. We then extend these pairwise assessments to four
graph-level statistics (prevalence, intensity, variability, extremity).
Coupling these metrics with a within-graph causal design, we quantify how
rewiring strategies affect over-squashing on diverse graph- and
node-classification benchmarks. Our extensive empirical analyses show that most
graph classification datasets suffer from over-squashing (but to various
extents), and rewiring effectively mitigates it -- though the degree of
mitigation, and its translation into performance gains, varies by dataset and
method. We also found that over-squashing is less notable in node
classification datasets, where rewiring often increases over-squashing, and
performance variations are uncorrelated with over-squashing changes. These
findings suggest that rewiring is most beneficial when over-squashing is both
substantial and corrected with restraint -- while overly aggressive rewiring,
or rewiring applied to minimally over-squashed graphs, is unlikely to help and
may even harm performance. Our plug-and-play diagnostic tool lets practitioners
decide -- before any training -- whether rewiring is likely to pay off.

</details>


### [219] [Pattern-based Knowledge Component Extraction from Student Code Using Representation Learning](https://arxiv.org/abs/2508.09281)
*Muntasir Hoq,Griffin Pitts,Andrew Lan,Peter Brusilovsky,Bita Akram*

Main category: cs.LG

TL;DR: 提出了一种基于模式的知识组件（KC）自动发现框架，通过变分自编码器和注意力机制从学生代码中提取可解释的模式，显著提升了知识追踪性能。


<details>
  <summary>Details</summary>
Motivation: 计算机科学教育中的个性化学习需要准确建模学生的知识状态，但传统方法在解释性和处理编程问题的开放性上存在挑战。

Method: 使用变分自编码器和注意力模型从学生代码中提取代表性模式，聚类形成模式基KC，并通过学习曲线分析和深度知识追踪评估。

Result: 实验表明，该方法能生成有意义的学习轨迹，并在知识追踪预测性能上优于传统方法。

Conclusion: 该框架为计算机科学教育提供了自动化、可扩展且可解释的知识建模方法。

Abstract: Effective personalized learning in computer science education depends on
accurately modeling what students know and what they need to learn. While
Knowledge Components (KCs) provide a foundation for such modeling, automated KC
extraction from student code is inherently challenging due to insufficient
explainability of discovered KCs and the open-endedness of programming problems
with significant structural variability across student solutions and complex
interactions among programming concepts. In this work, we propose a novel,
explainable framework for automated KC discovery through pattern-based KCs:
recurring structural patterns within student code that capture the specific
programming patterns and language constructs that students must master. Toward
this, we train a Variational Autoencoder to generate important representative
patterns from student code guided by an explainable, attention-based code
representation model that identifies important correct and incorrect pattern
implementations from student code. These patterns are then clustered to form
pattern-based KCs. We evaluate our KCs using two well-established methods
informed by Cognitive Science: learning curve analysis and Deep Knowledge
Tracing (DKT). Experimental results demonstrate meaningful learning
trajectories and significant improvements in DKT predictive performance over
traditional KT methods. This work advances knowledge modeling in CS education
by providing an automated, scalable, and explainable framework for identifying
granular code patterns and algorithmic constructs, essential for student
learning.

</details>


### [220] [Distilling Reinforcement Learning into Single-Batch Datasets](https://arxiv.org/abs/2508.09283)
*Connor Wilhelm,Dan Ventura*

Main category: cs.LG

TL;DR: 数据集蒸馏将大数据集压缩为小型合成数据集，使得在合成数据集上的学习效果接近原始数据集。该方法可应用于强化学习任务，并将其转化为监督学习任务。


<details>
  <summary>Details</summary>
Motivation: 探索数据集蒸馏的通用性，特别是如何将强化学习任务压缩并转化为监督学习任务，以简化学习过程。

Method: 提出了一种基于近端策略优化的元学习方法，用于蒸馏多维度经典问题（如cart-pole）、MuJoCo环境和Atari游戏。

Result: 展示了蒸馏方法能够将复杂强化学习环境压缩为一步监督学习任务，并验证了其在不同学习架构中的通用性。

Conclusion: 数据集蒸馏不仅能高效压缩任务，还能跨模态转换学习任务，具有广泛的应用潜力。

Abstract: Dataset distillation compresses a large dataset into a small synthetic
dataset such that learning on the synthetic dataset approximates learning on
the original. Training on the distilled dataset can be performed in as little
as one step of gradient descent. We demonstrate that distillation is
generalizable to different tasks by distilling reinforcement learning
environments into one-batch supervised learning datasets. This demonstrates not
only distillation's ability to compress a reinforcement learning task but also
its ability to transform one learning modality (reinforcement learning) into
another (supervised learning). We present a novel extension of proximal policy
optimization for meta-learning and use it in distillation of a
multi-dimensional extension of the classic cart-pole problem, all MuJoCo
environments, and several Atari games. We demonstrate distillation's ability to
compress complex RL environments into one-step supervised learning, explore RL
distillation's generalizability across learner architectures, and demonstrate
distilling an environment into the smallest-possible synthetic dataset.

</details>


### [221] [Decentralized Weather Forecasting via Distributed Machine Learning and Blockchain-Based Model Validation](https://arxiv.org/abs/2508.09299)
*Rilwan Umar,Aydin Abadi,Basil Aldali,Benito Vincent,Elliot A. J. Hurley,Hotoon Aljazaeri,Jamie Hedley-Cook,Jamie-Lee Bell,Lambert Uwuigbusun,Mujeeb Ahmed,Shishir Nagaraja,Suleiman Sabo,Weaam Alrbeiqi*

Main category: cs.LG

TL;DR: 提出了一种结合联邦学习和区块链技术的去中心化天气预报框架，以提高隐私性、安全性和可扩展性。


<details>
  <summary>Details</summary>
Motivation: 解决当前集中式天气预报系统的安全漏洞、可扩展性不足和单点故障问题。

Method: 整合联邦学习（保护隐私）和区块链技术（透明验证），引入基于信誉的投票机制和IPFS存储。

Result: 实验表明该方法提高了预报准确性、系统弹性和可扩展性。

Conclusion: 该框架适用于现实世界中对安全性要求高的环境。

Abstract: Weather forecasting plays a vital role in disaster preparedness, agriculture,
and resource management, yet current centralized forecasting systems are
increasingly strained by security vulnerabilities, limited scalability, and
susceptibility to single points of failure. To address these challenges, we
propose a decentralized weather forecasting framework that integrates Federated
Learning (FL) with blockchain technology. FL enables collaborative model
training without exposing sensitive local data; this approach enhances privacy
and reduces data transfer overhead. Meanwhile, the Ethereum blockchain ensures
transparent and dependable verification of model updates. To further enhance
the system's security, we introduce a reputation-based voting mechanism that
assesses the trustworthiness of submitted models while utilizing the
Interplanetary File System (IPFS) for efficient off-chain storage. Experimental
results demonstrate that our approach not only improves forecasting accuracy
but also enhances system resilience and scalability, making it a viable
candidate for deployment in real-world, security-critical environments.

</details>


### [222] [Exact Verification of Graph Neural Networks with Incremental Constraint Solving](https://arxiv.org/abs/2508.09320)
*Minghao Liu,Chia-Hsuan Lu,Marta Kwiatkowska*

Main category: cs.LG

TL;DR: 本文提出了一种精确验证方法GNNev，用于增强图神经网络（GNNs）对抗属性与结构扰动的鲁棒性，支持多种聚合函数，并在实验中表现出色。


<details>
  <summary>Details</summary>
Motivation: GNNs在高风险应用中易受对抗攻击，现有方法对常用聚合函数的支持不足，因此需要开发一种更全面的验证方法。

Method: 采用约束求解与边界收紧技术，迭代解决松弛约束问题，并利用求解器的增量求解能力提升效率。支持sum、max和mean三种聚合函数。

Result: 在Cora、CiteSeer、Amazon和Yelp数据集上验证了GNNev的有效性，尤其在sum聚合任务上优于现有工具。

Conclusion: GNNev为GNNs提供了对抗扰动的鲁棒性保证，扩展了对聚合函数的支持，并在实验中展现了优越性能。

Abstract: Graph neural networks (GNNs) are increasingly employed in high-stakes
applications, such as fraud detection or healthcare, but are susceptible to
adversarial attacks. A number of techniques have been proposed to provide
adversarial robustness guarantees, but support for commonly used aggregation
functions in message-passing GNNs is still lacking. In this paper, we develop
an exact (sound and complete) verification method for GNNs to compute
guarantees against attribute and structural perturbations that involve edge
addition or deletion, subject to budget constraints. Focusing on node
classification tasks, our method employs constraint solving with bound
tightening, and iteratively solves a sequence of relaxed constraint
satisfaction problems while relying on incremental solving capabilities of
solvers to improve efficiency. We implement GNNev, a versatile solver for
message-passing neural networks, which supports three aggregation functions,
sum, max and mean, with the latter two considered here for the first time.
Extensive experimental evaluation of GNNev on two standard benchmarks (Cora and
CiteSeer) and two real-world fraud datasets (Amazon and Yelp) demonstrates its
usability and effectiveness, as well as superior performance compared to
existing {exact verification} tools on sum-aggregated node classification
tasks.

</details>


### [223] [Synaptic Pruning: A Biological Inspiration for Deep Learning Regularization](https://arxiv.org/abs/2508.09330)
*Gideon Vos,Liza van Eijk,Zoltan Sarnyai,Mostafa Rahimi Azghadi*

Main category: cs.LG

TL;DR: 提出了一种基于权重大小的突触修剪方法，模拟生物大脑的修剪机制，逐步移除低重要性连接，显著提升了时间序列预测模型的性能。


<details>
  <summary>Details</summary>
Motivation: 生物大脑通过突触修剪移除弱连接以提高效率，而人工神经网络中的随机失活（dropout）未考虑活动依赖性修剪。本文旨在提出一种更符合生物学的修剪方法。

Method: 提出了一种基于权重大小的修剪方法，通过立方调度逐步增加全局稀疏性，定期永久移除低重要性权重，同时保持梯度流动，无需单独的修剪和微调阶段。

Result: 在多个时间序列预测模型（RNN、LSTM、Patch Time Series Transformer）和四个数据集上表现优异，显著降低了金融预测的平均绝对误差（最高达20%）。

Conclusion: 该方法通过动态修剪机制改进了正则化，易于集成到多种架构中，尤其在金融时间序列预测中表现出色，是传统dropout技术的实用替代方案。

Abstract: Synaptic pruning in biological brains removes weak connections to improve
efficiency. In contrast, dropout regularization in artificial neural networks
randomly deactivates neurons without considering activity-dependent pruning. We
propose a magnitude-based synaptic pruning method that better reflects biology
by progressively removing low-importance connections during training.
Integrated directly into the training loop as a dropout replacement, our
approach computes weight importance from absolute magnitudes across layers and
applies a cubic schedule to gradually increase global sparsity. At fixed
intervals, pruning masks permanently remove low-importance weights while
maintaining gradient flow for active ones, eliminating the need for separate
pruning and fine-tuning phases. Experiments on multiple time series forecasting
models including RNN, LSTM, and Patch Time Series Transformer across four
datasets show consistent gains. Our method ranked best overall, with
statistically significant improvements confirmed by Friedman tests (p < 0.01).
In financial forecasting, it reduced Mean Absolute Error by up to 20% over
models with no or standard dropout, and up to 52% in select transformer models.
This dynamic pruning mechanism advances regularization by coupling weight
elimination with progressive sparsification, offering easy integration into
diverse architectures. Its strong performance, especially in financial time
series forecasting, highlights its potential as a practical alternative to
conventional dropout techniques.

</details>


### [224] [RicciFlowRec: A Geometric Root Cause Recommender Using Ricci Curvature on Financial Graphs](https://arxiv.org/abs/2508.09334)
*Zhongtian Sun,Anoushka Harit*

Main category: cs.LG

TL;DR: RicciFlowRec是一个基于几何的推荐框架，利用Ricci曲率和流分析动态金融图中的根因，提升推荐鲁棒性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 研究旨在通过几何方法（Ricci曲率和流）量化金融图中的局部压力，追踪冲击传播，以支持风险感知的金融决策。

Method: 通过动态金融图建模股票、宏观经济指标和新闻的交互，利用离散Ricci曲率量化局部压力，Ricci流追踪冲击传播，并基于曲率梯度设计排名函数。

Result: 初步实验在S&P 500数据和FinBERT情感分析上显示，框架在合成扰动下具有更好的鲁棒性和可解释性。

Conclusion: RicciFlowRec是首个将几何流推理应用于金融决策的推荐系统，未来计划扩展至投资组合优化和收益预测。

Abstract: We propose RicciFlowRec, a geometric recommendation framework that performs
root cause attribution via Ricci curvature and flow on dynamic financial
graphs. By modelling evolving interactions among stocks, macroeconomic
indicators, and news, we quantify local stress using discrete Ricci curvature
and trace shock propagation via Ricci flow. Curvature gradients reveal causal
substructures, informing a structural risk-aware ranking function. Preliminary
results on S\&P~500 data with FinBERT-based sentiment show improved robustness
and interpretability under synthetic perturbations. This ongoing work supports
curvature-based attribution and early-stage risk-aware ranking, with plans for
portfolio optimization and return forecasting. To our knowledge, RicciFlowRec
is the first recommender to apply geometric flow-based reasoning in financial
decision support.

</details>


### [225] [Resurrecting the Salmon: Rethinking Mechanistic Interpretability with Domain-Specific Sparse Autoencoders](https://arxiv.org/abs/2508.09363)
*Charles O'Neill,Mudith Jayasekara,Max Kirkby*

Main category: cs.LG

TL;DR: 领域限制的稀疏自编码器（SAEs）在医学文本上训练，显著提高了重建保真度和可解释性，减少了线性残差误差。


<details>
  <summary>Details</summary>
Motivation: 传统SAEs在广泛数据分布上训练，导致高频率通用模式占主导，线性残差误差大且特征解释性差。

Method: 在医学文本上训练JumpReLU SAEs，使用195k临床问答数据，专注于领域特定特征。

Result: 领域限制的SAEs解释了更多方差，提高了损失恢复，减少了线性残差误差，特征与临床概念对齐。

Conclusion: 领域限制缓解了广泛SAEs的局限性，提供了更完整和可解释的潜在分解，质疑通用SAEs的扩展需求。

Abstract: Sparse autoencoders (SAEs) decompose large language model (LLM) activations
into latent features that reveal mechanistic structure. Conventional SAEs train
on broad data distributions, forcing a fixed latent budget to capture only
high-frequency, generic patterns. This often results in significant linear
``dark matter'' in reconstruction error and produces latents that fragment or
absorb each other, complicating interpretation. We show that restricting SAE
training to a well-defined domain (medical text) reallocates capacity to
domain-specific features, improving both reconstruction fidelity and
interpretability. Training JumpReLU SAEs on layer-20 activations of Gemma-2
models using 195k clinical QA examples, we find that domain-confined SAEs
explain up to 20\% more variance, achieve higher loss recovery, and reduce
linear residual error compared to broad-domain SAEs. Automated and human
evaluations confirm that learned features align with clinically meaningful
concepts (e.g., ``taste sensations'' or ``infectious mononucleosis''), rather
than frequent but uninformative tokens. These domain-specific SAEs capture
relevant linear structure, leaving a smaller, more purely nonlinear residual.
We conclude that domain-confinement mitigates key limitations of broad-domain
SAEs, enabling more complete and interpretable latent decompositions, and
suggesting the field may need to question ``foundation-model'' scaling for
general-purpose SAEs.

</details>


### [226] [Understanding Dementia Speech Alignment with Diffusion-Based Image Generation](https://arxiv.org/abs/2508.09385)
*Mansi,Anastasios Lepipas,Dominika Woszczyk,Yiying Guan,Soteris Demetriou*

Main category: cs.LG

TL;DR: 研究发现，通过文本生成图像的模型可以从痴呆症相关语音生成的图像中检测痴呆症，准确率达75%，并揭示了语言中哪些部分对检测有贡献。


<details>
  <summary>Details</summary>
Motivation: 探索文本生成图像模型是否能将病理语音（如痴呆症）与生成图像对齐，并解释这种对齐的可能性。

Method: 利用文本生成图像模型，分析痴呆症相关语音生成的图像，开发解释对齐的方法。

Result: 在ADReSS数据集上，仅通过生成图像即可实现75%的痴呆症检测准确率。

Conclusion: 文本生成图像模型能够捕捉痴呆症相关语音的特征，并通过图像生成实现检测，为病理语音分析提供了新视角。

Abstract: Text-to-image models generate highly realistic images based on natural
language descriptions and millions of users use them to create and share images
online. While it is expected that such models can align input text and
generated image in the same latent space little has been done to understand
whether this alignment is possible between pathological speech and generated
images. In this work, we examine the ability of such models to align
dementia-related speech information with the generated images and develop
methods to explain this alignment. Surprisingly, we found that dementia
detection is possible from generated images alone achieving 75% accuracy on the
ADReSS dataset. We then leverage explainability methods to show which parts of
the language contribute to the detection.

</details>


### [227] [Integrating Feature Attention and Temporal Modeling for Collaborative Financial Risk Assessment](https://arxiv.org/abs/2508.09399)
*Yue Yao,Zhen Xu,Youzhu Liu,Kunyuan Ma,Yuxiu Lin,Mohan Jiang*

Main category: cs.LG

TL;DR: 该论文提出了一种基于联邦学习的金融风险评估框架，通过特征注意力和时序建模结构，在不共享原始数据的情况下实现跨机构联合建模和风险识别。


<details>
  <summary>Details</summary>
Motivation: 解决跨机构金融风险分析中的数据隐私和协作建模问题。

Method: 采用分布式优化策略，各机构训练本地子模型，并通过差分隐私和噪声注入保护参数后上传至中央服务器聚合生成全局模型。

Result: 实验表明，该方法在通信效率、模型准确性、系统性风险检测和跨市场泛化方面优于传统集中式方法和现有联邦学习变体。

Conclusion: 该方法在保护数据主权的同时提升了风险识别的范围和效率，为智能金融风险分析提供了安全高效的解决方案。

Abstract: This paper addresses the challenges of data privacy and collaborative
modeling in cross-institution financial risk analysis. It proposes a risk
assessment framework based on federated learning. Without sharing raw data, the
method enables joint modeling and risk identification across multiple
institutions. This is achieved by incorporating a feature attention mechanism
and temporal modeling structure. Specifically, the model adopts a distributed
optimization strategy. Each financial institution trains a local sub-model. The
model parameters are protected using differential privacy and noise injection
before being uploaded. A central server then aggregates these parameters to
generate a global model. This global model is used for systemic risk
identification. To validate the effectiveness of the proposed method, multiple
experiments are conducted. These evaluate communication efficiency, model
accuracy, systemic risk detection, and cross-market generalization. The results
show that the proposed model outperforms both traditional centralized methods
and existing federated learning variants across all evaluation metrics. It
demonstrates strong modeling capabilities and practical value in sensitive
financial environments. The method enhances the scope and efficiency of risk
identification while preserving data sovereignty. It offers a secure and
efficient solution for intelligent financial risk analysis.

</details>


### [228] [Graph Neural Network and Transformer Integration for Unsupervised System Anomaly Discovery](https://arxiv.org/abs/2508.09401)
*Yun Zi,Ming Gong,Zhihao Xue,Yujun Zou,Nia Qi,Yingnan Deng*

Main category: cs.LG

TL;DR: 提出了一种无监督异常检测方法，用于分布式后端服务系统，解决了复杂结构依赖、行为演化和无标签数据等挑战。


<details>
  <summary>Details</summary>
Motivation: 解决分布式后端服务系统中异常检测的实际问题，如复杂结构依赖、行为演化和缺乏标签数据。

Method: 构建动态图表示服务调用关系，使用图卷积提取高阶结构特征，Transformer建模节点时间行为，融合结构和行为特征后计算异常分数。

Result: 在真实云监控数据上实验，表现优于现有模型，能更好地捕捉异常传播路径和动态行为序列。

Conclusion: 该方法具有较强表达能力和稳定性，适合实际部署。

Abstract: This study proposes an unsupervised anomaly detection method for distributed
backend service systems, addressing practical challenges such as complex
structural dependencies, diverse behavioral evolution, and the absence of
labeled data. The method constructs a dynamic graph based on service invocation
relationships and applies graph convolution to extract high-order structural
representations from multi-hop topologies. A Transformer is used to model the
temporal behavior of each node, capturing long-term dependencies and local
fluctuations. During the feature fusion stage, a learnable joint embedding
mechanism integrates structural and behavioral representations into a unified
anomaly vector. A nonlinear mapping is then applied to compute anomaly scores,
enabling an end-to-end detection process without supervision. Experiments on
real-world cloud monitoring data include sensitivity analyses across different
graph depths, sequence lengths, and data perturbations. Results show that the
proposed method outperforms existing models on several key metrics,
demonstrating stronger expressiveness and stability in capturing anomaly
propagation paths and modeling dynamic behavior sequences, with high potential
for practical deployment.

</details>


### [229] [Domain-Generalization to Improve Learning in Meta-Learning Algorithms](https://arxiv.org/abs/2508.09418)
*Usman Anjum,Chris Stockman,Cat Luong,Justin Zhan*

Main category: cs.LG

TL;DR: DGS-MAML是一种新的元学习算法，结合梯度匹配和锐度感知最小化，提升模型适应性和鲁棒性，适用于小样本学习。


<details>
  <summary>Details</summary>
Motivation: 解决在有限训练数据下跨任务泛化的问题。

Method: 结合梯度匹配和锐度感知最小化的双层优化框架，支持PAC-Bayes理论和收敛性分析。

Result: 在基准数据集上表现优于现有方法，准确性和泛化能力更强。

Conclusion: DGS-MAML适用于需要快速适应和小样本学习的场景，代码已开源。

Abstract: This paper introduces Domain Generalization Sharpness-Aware Minimization
Model-Agnostic Meta-Learning (DGS-MAML), a novel meta-learning algorithm
designed to generalize across tasks with limited training data. DGS-MAML
combines gradient matching with sharpness-aware minimization in a bi-level
optimization framework to enhance model adaptability and robustness. We support
our method with theoretical analysis using PAC-Bayes and convergence
guarantees. Experimental results on benchmark datasets show that DGS-MAML
outperforms existing approaches in terms of accuracy and generalization. The
proposed method is particularly useful for scenarios requiring few-shot
learning and quick adaptation, and the source code is publicly available at
GitHub.

</details>


### [230] [Implicit Hypergraph Neural Networks: A Stable Framework for Higher-Order Relational Learning with Provable Guarantees](https://arxiv.org/abs/2508.09427)
*Xiaoyu Li,Guangyu Tang,Jiaojiao Jiang*

Main category: cs.LG

TL;DR: 论文提出了一种隐式超图神经网络（IHGNN），通过非线性固定点方程计算表示，解决了传统超图神经网络在长程依赖捕获和训练稳定性上的限制。


<details>
  <summary>Details</summary>
Motivation: 现实中的许多交互是基于群体的（如多作者论文或用户联合参与项目），传统超图神经网络依赖固定层数的显式消息传递，限制了长程依赖的捕获且训练不稳定。

Method: IHGNN采用隐式平衡公式，通过非线性固定点方程计算表示，避免了深层架构，并开发了收敛性可证明的训练方案。

Result: 在引用基准测试中，IHGNN在准确性和鲁棒性上均优于传统图/超图神经网络基线，且对随机初始化和超参数变化具有强鲁棒性。

Conclusion: IHGNN为高阶关系学习提供了稳定、高效的全局传播方法，具有强泛化能力和实用价值。

Abstract: Many real-world interactions are group-based rather than pairwise such as
papers with multiple co-authors and users jointly engaging with items.
Hypergraph neural networks have shown great promise at modeling higher-order
relations, but their reliance on a fixed number of explicit message-passing
layers limits long-range dependency capture and can destabilize training as
depth grows. In this work, we introduce Implicit Hypergraph Neural Networks
(IHGNN), which bring the implicit equilibrium formulation to hypergraphs:
instead of stacking layers, IHGNN computes representations as the solution to a
nonlinear fixed-point equation, enabling stable and efficient global
propagation across hyperedges without deep architectures. We develop a
well-posed training scheme with provable convergence, analyze the oversmoothing
conditions and expressivity of the model, and derive a transductive
generalization bound on hypergraphs. We further present an implicit-gradient
training procedure coupled with a projection-based stabilization strategy.
Extensive experiments on citation benchmarks show that IHGNN consistently
outperforms strong traditional graph/hypergraph neural network baselines in
both accuracy and robustness. Empirically, IHGNN is resilient to random
initialization and hyperparameter variation, highlighting its strong
generalization and practical value for higher-order relational learning.

</details>


### [231] [NEXICA: Discovering Road Traffic Causality (Extended arXiv Version)](https://arxiv.org/abs/2508.09447)
*Siddharth Srikanth,John Krumm,Jonathan Qin*

Main category: cs.LG

TL;DR: NEXICA是一种算法，用于发现高速公路系统中哪些部分容易引发其他部分的交通拥堵。它基于时间序列的道路速度数据，通过新方法识别因果关系。


<details>
  <summary>Details</summary>
Motivation: 交通拥堵是一个持续性问题，通过集中资源解决拥堵源头是高效策略。

Method: 1. 关注时间序列中事件（交通减速的开始）；2. 使用最大似然估计建立概率模型；3. 训练二元分类器识别因果关系。

Result: 在洛杉矶地区195个高速公路传感器数据上测试，NEXICA在准确性和计算速度上优于现有方法。

Conclusion: NEXICA为交通拥堵的源头识别提供了高效解决方案。

Abstract: Road traffic congestion is a persistent problem. Focusing resources on the
causes of congestion is a potentially efficient strategy for reducing
slowdowns. We present NEXICA, an algorithm to discover which parts of the
highway system tend to cause slowdowns on other parts of the highway. We use
time series of road speeds as inputs to our causal discovery algorithm. Finding
other algorithms inadequate, we develop a new approach that is novel in three
ways. First, it concentrates on just the presence or absence of events in the
time series, where an event indicates the temporal beginning of a traffic
slowdown. Second, we develop a probabilistic model using maximum likelihood
estimation to compute the probabilities of spontaneous and caused slowdowns
between two locations on the highway. Third, we train a binary classifier to
identify pairs of cause/effect locations trained on pairs of road locations
where we are reasonably certain a priori of their causal connections, both
positive and negative. We test our approach on six months of road speed data
from 195 different highway speed sensors in the Los Angeles area, showing that
our approach is superior to state-of-the-art baselines in both accuracy and
computation speed.

</details>


### [232] [A Unified Contrastive-Generative Framework for Time Series Classification](https://arxiv.org/abs/2508.09451)
*Ziyu Liu,Azadeh Alavi,Minyi Li,Xiang Zhang*

Main category: cs.LG

TL;DR: CoGenT框架首次将对比学习和生成方法结合，通过联合优化提升多变量时间序列的自监督学习性能。


<details>
  <summary>Details</summary>
Motivation: 探索对比学习和生成方法的互补潜力，解决各自在高类内相似性和大数据依赖上的局限性。

Method: 提出Contrastive Generative Time series框架（CoGenT），通过联合对比-生成优化统一两种范式。

Result: 在六个时间序列数据集上表现优异，F1分数分别比SimCLR和MAE提升59.2%和14.27%。

Conclusion: CoGenT为时间域混合自监督学习奠定了基础，兼具判别力和生成鲁棒性。

Abstract: Self-supervised learning (SSL) for multivariate time series mainly includes
two paradigms: contrastive methods that excel at instance discrimination and
generative approaches that model data distributions. While effective
individually, their complementary potential remains unexplored. We propose a
Contrastive Generative Time series framework (CoGenT), the first framework to
unify these paradigms through joint contrastive-generative optimization. CoGenT
addresses fundamental limitations of both approaches: it overcomes contrastive
learning's sensitivity to high intra-class similarity in temporal data while
reducing generative methods' dependence on large datasets. We evaluate CoGenT
on six diverse time series datasets. The results show consistent improvements,
with up to 59.2% and 14.27% F1 gains over standalone SimCLR and MAE,
respectively. Our analysis reveals that the hybrid objective preserves
discriminative power while acquiring generative robustness. These findings
establish a foundation for hybrid SSL in temporal domains. We will release the
code shortly.

</details>


### [233] [Open-Set Fault Diagnosis in Multimode Processes via Fine-Grained Deep Feature Representation](https://arxiv.org/abs/2508.09462)
*Guangqiang Li,M. Amine Atoui,Xiangshun Li*

Main category: cs.LG

TL;DR: 提出了一种名为FGCRN的新型开放集故障诊断模型，结合多尺度深度卷积、双向门控循环单元和时间注意力机制，通过无监督学习构建细粒度特征表示，有效识别未知故障。


<details>
  <summary>Details</summary>
Motivation: 解决多模式过程中同一健康状态样本分布多簇化的问题，构建紧凑且准确的决策边界。

Method: 结合多尺度深度卷积、双向门控循环单元和时间注意力机制，设计基于距离的损失函数增强类内紧凑性，通过无监督学习构建细粒度特征表示，利用极值理论建模样本特征与细粒度表示的距离。

Result: 实验证明该方法性能优越。

Conclusion: FGCRN能有效识别未知故障，适用于多模式过程的故障诊断。

Abstract: A reliable fault diagnosis system should not only accurately classify known
health states but also effectively identify unknown faults. In multimode
processes, samples belonging to the same health state often show multiple
cluster distributions, making it difficult to construct compact and accurate
decision boundaries for that state. To address this challenge, a novel open-set
fault diagnosis model named fine-grained clustering and rejection network
(FGCRN) is proposed. It combines multiscale depthwise convolution,
bidirectional gated recurrent unit and temporal attention mechanism to capture
discriminative features. A distance-based loss function is designed to enhance
the intra-class compactness. Fine-grained feature representations are
constructed through unsupervised learning to uncover the intrinsic structures
of each health state. Extreme value theory is employed to model the distance
between sample features and their corresponding fine-grained representations,
enabling effective identification of unknown faults. Extensive experiments
demonstrate the superior performance of the proposed method.

</details>


### [234] [Learn to Explore: Meta NAS via Bayesian Optimization Guided Graph Generation](https://arxiv.org/abs/2508.09467)
*Zijun Sun,Yanning Shen*

Main category: cs.LG

TL;DR: GraB-NAS提出了一种新的Meta-NAS框架，通过图建模和混合搜索策略提升任务感知架构的性能和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 传统NAS方法局限于单一任务，Meta-NAS虽能跨任务迁移但存在泛化差、搜索空间有限或计算成本高的问题。

Method: GraB-NAS将神经网络架构建模为图，结合全局贝叶斯优化和局部梯度上升的混合搜索策略。

Result: 实验表明GraB-NAS优于现有Meta-NAS方法，泛化能力和搜索效率更高。

Conclusion: GraB-NAS通过混合搜索策略解决了Meta-NAS的局限性，展现了更强的任务适应性和性能。

Abstract: Neural Architecture Search (NAS) automates the design of high-performing
neural networks but typically targets a single predefined task, thereby
restricting its real-world applicability. To address this, Meta Neural
Architecture Search (Meta-NAS) has emerged as a promising paradigm that
leverages prior knowledge across tasks to enable rapid adaptation to new ones.
Nevertheless, existing Meta-NAS methods often struggle with poor
generalization, limited search spaces, or high computational costs. In this
paper, we propose a novel Meta-NAS framework, GraB-NAS. Specifically, GraB-NAS
first models neural architectures as graphs, and then a hybrid search strategy
is developed to find and generate new graphs that lead to promising neural
architectures. The search strategy combines global architecture search via
Bayesian Optimization in the search space with local exploration for novel
neural networks via gradient ascent in the latent space. Such a hybrid search
strategy allows GraB-NAS to discover task-aware architectures with strong
performance, even beyond the predefined search space. Extensive experiments
demonstrate that GraB-NAS outperforms state-of-the-art Meta-NAS baselines,
achieving better generalization and search effectiveness.

</details>


### [235] [NeuronTune: Fine-Grained Neuron Modulation for Balanced Safety-Utility Alignment in LLMs](https://arxiv.org/abs/2508.09473)
*Birong Pan,Mayi Xu,Qiankun Pi,Jianhao Chen,Yuanyuan Zhu,Ming Zhong,Tieyun Qian*

Main category: cs.LG

TL;DR: 论文提出NeuronTune框架，通过细粒度神经元调控实现LLM的安全性与性能优化。


<details>
  <summary>Details</summary>
Motivation: 现有方法在LLM的安全性和性能优化上存在不足，如对抗攻击鲁棒性差、频繁拒绝良性查询以及性能下降。

Method: 通过归因分析识别安全关键和性能保留神经元，利用元学习动态调控神经元激活，支持灵活调整干预范围。

Result: 实验表明，NeuronTune显著优于现有技术，在保持高性能的同时提升模型安全性。

Conclusion: NeuronTune为LLM的安全性与性能优化提供了有效解决方案。

Abstract: Ensuring robust safety alignment while preserving utility is critical for the
reliable deployment of Large Language Models (LLMs). However, current
techniques fundamentally suffer from intertwined deficiencies: insufficient
robustness against malicious attacks, frequent refusal of benign queries,
degradation in generated text quality and general task performance--the former
two reflecting deficits in robust safety and the latter constituting utility
impairment. We trace these limitations to the coarse-grained layer-wise
interventions in existing methods. To resolve this, we propose NeuronTune, a
fine-grained framework that dynamically modulates sparse neurons to achieve
simultaneous safety-utility optimization. Our approach first identifies
safety-critical and utility-preserving neurons across all layers via
attribution, then employs meta-learning to adaptively amplify safety-neuron
activations and suppress utility-neuron activations. Crucially, NeuronTune
enables tunable adjustment of intervention scope via neuron-count thresholds,
supporting flexible adaptation to security-critical or utility-priority
scenarios. Extensive experimental results demonstrate that our method
significantly outperforms existing state-of-the-art technologies, achieving
superior model safety while maintaining excellent utility.

</details>


### [236] [DeepFeatIoT: Unifying Deep Learned, Randomized, and LLM Features for Enhanced IoT Time Series Sensor Data Classification in Smart Industries](https://arxiv.org/abs/2508.09468)
*Muhammad Sakib Khan Inan,Kewen Liao*

Main category: cs.LG

TL;DR: 论文提出了一种名为DeepFeatIoT的深度学习模型，通过融合学习与非学习的特征，显著提升了IoT时间序列数据的分类性能，尤其在标记数据有限的情况下表现优异。


<details>
  <summary>Details</summary>
Motivation: IoT传感器数据存在元数据丢失、数据源异构、采样频率不一致等问题，导致原始数据难以解读，影响智能系统的有效性。

Method: 提出DeepFeatIoT模型，整合学习的局部与全局特征、随机卷积核特征及大语言模型特征。

Result: 模型在多个真实IoT数据集上表现优于现有基准模型，展现了泛化能力。

Conclusion: DeepFeatIoT有望推动IoT分析领域的进步，支持下一代智能系统的发展。

Abstract: Internet of Things (IoT) sensors are ubiquitous technologies deployed across
smart cities, industrial sites, and healthcare systems. They continuously
generate time series data that enable advanced analytics and automation in
industries. However, challenges such as the loss or ambiguity of sensor
metadata, heterogeneity in data sources, varying sampling frequencies,
inconsistent units of measurement, and irregular timestamps make raw IoT time
series data difficult to interpret, undermining the effectiveness of smart
systems. To address these challenges, we propose a novel deep learning model,
DeepFeatIoT, which integrates learned local and global features with
non-learned randomized convolutional kernel-based features and features from
large language models (LLMs). This straightforward yet unique fusion of diverse
learned and non-learned features significantly enhances IoT time series sensor
data classification, even in scenarios with limited labeled data. Our model's
effectiveness is demonstrated through its consistent and generalized
performance across multiple real-world IoT sensor datasets from diverse
critical application domains, outperforming state-of-the-art benchmark models.
These results highlight DeepFeatIoT's potential to drive significant
advancements in IoT analytics and support the development of next-generation
smart systems.

</details>


### [237] [EGGS-PTP: An Expander-Graph Guided Structured Post-training Pruning Method for Large Language Models](https://arxiv.org/abs/2508.09471)
*Omar Bazarbachi,Zijun Sun,Yanning Shen*

Main category: cs.LG

TL;DR: EGGS-PTP是一种基于扩展图理论的N:M结构化剪枝方法，有效减少大型语言模型的规模和计算需求。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型规模扩大，部署时的计算和内存挑战日益严峻，亟需更高效的模型变体。

Method: 利用扩展图理论设计N:M结构化剪枝，确保剪枝后网络的信息流动和功能保留。

Result: 实验表明，EGGS-PTP在加速和内存节省方面表现优异，且准确率优于现有结构化剪枝技术。

Conclusion: EGGS-PTP为大型语言模型的高效部署提供了一种有效解决方案。

Abstract: As Large Language Models (LLMs) become more widely adopted and scale up in
size, the computational and memory challenges involved in deploying these
massive foundation models have grown increasingly severe. This underscores the
urgent need to develop more efficient model variants. Faced with this
challenge, the present work introduces EGGS-PTP: an Expander-Graph Guided
Structured Post-training Pruning method. The proposed approach leverages graph
theory to guide the design of N:M structured pruning, effectively reducing
model size and computational demands. By incorporating concepts from expander
graphs, EGGS-PTP ensures information flow within the pruned network, preserving
essential model functionality. Extensive numerical experiments demonstrate that
EGGS-PTP not only achieves significant acceleration and memory savings due to
structured sparsity but also outperforms existing structured pruning techniques
in terms of accuracy across various LLMs.

</details>


### [238] [Large-Small Model Collaborative Framework for Federated Continual Learning](https://arxiv.org/abs/2508.09489)
*Hao Yu,Xin Yang,Boyang Fan,Xuemei Cao,Hanlin Gu,Lixin Fan,Qiang Yang*

Main category: cs.LG

TL;DR: 本文提出了一种联邦持续学习（FCL）框架，通过轻量级本地模型动态适应新任务，同时提升大模型的性能，解决了基础模型（FMs）在持续学习中的挑战。


<details>
  <summary>Details</summary>
Motivation: 基础模型（FMs）在持续学习中表现不佳，尤其是在联邦环境下，由于数据隐私和通信限制，无法有效利用本地数据。此外，FMs的庞大参数和复杂性使其难以避免遗忘问题。

Method: 提出了一种协作框架，轻量级本地模型作为动态桥梁，适应新任务并提升大模型性能。包含两个新组件：小模型持续微调防止遗忘；一对一蒸馏实现异构本地知识的个性化融合。

Result: 实验结果表明，该框架性能优越，即使客户端使用异构小模型。

Conclusion: 该框架成功解决了FMs在FCL中的挑战，为持续学习提供了新思路。

Abstract: Continual learning (CL) for Foundation Models (FMs) is an essential yet
underexplored challenge, especially in Federated Continual Learning (FCL),
where each client learns from a private, evolving task stream under strict data
and communication constraints. Despite their powerful generalization abilities,
FMs often exhibit suboptimal performance on local downstream tasks, as they are
unable to utilize private local data. Furthermore, enabling FMs to learn new
tasks without forgetting prior knowledge is inherently a challenging problem,
primarily due to their immense parameter count and high model complexity. In
contrast, small models can be trained locally under resource-constrained
conditions and benefit from more mature CL techniques. To bridge the gap
between small models and FMs, we propose the first collaborative framework in
FCL, where lightweight local models act as a dynamic bridge, continually
adapting to new tasks while enhancing the utility of the large model. Two novel
components are also included: Small Model Continual Fine-tuning is for
preventing small models from temporal forgetting; One-by-One Distillation
performs personalized fusion of heterogeneous local knowledge on the server.
Experimental results demonstrate its superior performance, even when clients
utilize heterogeneous small models.

</details>


### [239] [MiCo: End-to-End Mixed Precision Neural Network Co-Exploration Framework for Edge AI](https://arxiv.org/abs/2508.09500)
*Zijun Jiang,Yangdi Lyu*

Main category: cs.LG

TL;DR: 本文提出了MiCo框架，用于高效探索和部署层间混合精度量化（MPQ）模型，以在边缘设备上实现低精度神经网络的高效存储和计算。


<details>
  <summary>Details</summary>
Motivation: 现有MPQ方案探索算法在灵活性和效率上受限，且缺乏端到端的优化与部署框架。

Method: MiCo框架采用新型优化算法搜索最优量化方案，并构建硬件感知延迟模型以快速探索。

Result: 框架实现了从PyTorch模型到裸机C代码的直接部署，显著提升速度且精度损失最小。

Conclusion: MiCo为边缘AI应用提供了高效的MPQ探索与部署解决方案。

Abstract: Quantized Neural Networks (QNN) with extremely low-bitwidth data have proven
promising in efficient storage and computation on edge devices. To further
reduce the accuracy drop while increasing speedup, layer-wise mixed-precision
quantization (MPQ) becomes a popular solution. However, existing algorithms for
exploring MPQ schemes are limited in flexibility and efficiency. Comprehending
the complex impacts of different MPQ schemes on post-training quantization and
quantization-aware training results is a challenge for conventional methods.
Furthermore, an end-to-end framework for the optimization and deployment of MPQ
models is missing in existing work.
  In this paper, we propose the MiCo framework, a holistic MPQ exploration and
deployment framework for edge AI applications. The framework adopts a novel
optimization algorithm to search for optimal quantization schemes with the
highest accuracies while meeting latency constraints. Hardware-aware latency
models are built for different hardware targets to enable fast explorations.
After the exploration, the framework enables direct deployment from PyTorch MPQ
models to bare-metal C codes, leading to end-to-end speedup with minimal
accuracy drops.

</details>


### [240] [Causal Graph Profiling via Structural Divergence for Robust Anomaly Detection in Cyber-Physical Systems](https://arxiv.org/abs/2508.09504)
*Arun Vignesh Malarkkan,Haoyue Bai,Dongjie Wang,Yanjie Fu*

Main category: cs.LG

TL;DR: CGAD是一种基于因果图的异常检测框架，用于公共基础设施系统中的可靠网络攻击检测，通过因果分析和图比较提升检测精度。


<details>
  <summary>Details</summary>
Motivation: 针对传统方法在多元时间序列中因分布偏移和类别不平衡导致的高误报率问题，提出CGAD以解决这些挑战。

Method: CGAD采用两阶段监督框架：1) 使用动态贝叶斯网络学习因果不变图结构；2) 通过因果图比较检测异常。

Result: CGAD在非平稳和不平衡时间序列环境中表现优于传统方法，显著提高了F1和ROC-AUC分数。

Conclusion: CGAD通过揭示因果结构，不仅提高了检测精度，还重新定义了异常检测的鲁棒性。

Abstract: With the growing complexity of cyberattacks targeting critical
infrastructures such as water treatment networks, there is a pressing need for
robust anomaly detection strategies that account for both system
vulnerabilities and evolving attack patterns. Traditional methods --
statistical, density-based, and graph-based models struggle with distribution
shifts and class imbalance in multivariate time series, often leading to high
false positive rates. To address these challenges, we propose CGAD, a Causal
Graph-based Anomaly Detection framework designed for reliable cyberattack
detection in public infrastructure systems. CGAD follows a two-phase supervised
framework -- causal profiling and anomaly scoring. First, it learns causal
invariant graph structures representing the system's behavior under "Normal"
and "Attack" states using Dynamic Bayesian Networks. Second, it employs
structural divergence to detect anomalies via causal graph comparison by
evaluating topological deviations in causal graphs over time. By leveraging
causal structures, CGAD achieves superior adaptability and accuracy in
non-stationary and imbalanced time series environments compared to conventional
machine learning approaches. By uncovering causal structures beneath volatile
sensor data, our framework not only detects cyberattacks with markedly higher
precision but also redefines robustness in anomaly detection, proving
resilience where traditional models falter under imbalance and drift. Our
framework achieves substantial gains in F1 and ROC-AUC scores over
best-performing baselines across four industrial datasets, demonstrating robust
detection of delayed and structurally complex anomalies.

</details>


### [241] [Enhancing Memory Recall in LLMs with Gauss-Tin: A Hybrid Instructional and Gaussian Replay Approach](https://arxiv.org/abs/2508.09510)
*Iing Muttakhiroh,Thomas Fevens*

Main category: cs.LG

TL;DR: Gauss-Tin是一种结合回放策略与高斯混合模型的新方法，用于提升LLMs在持续学习中的知识保留能力，实验结果显示其性能优于传统方法6%。


<details>
  <summary>Details</summary>
Motivation: 解决大型语言模型（LLMs）在学习新知识时出现的灾难性遗忘问题。

Method: 提出Gauss-Tin方法，结合回放策略与高斯混合模型优化样本选择，并通过指导生成增强过去学习。

Result: 实验表明，Gauss-Tin在保留指标上比传统方法提高了6%。

Conclusion: Gauss-Tin是一种有效的混合模型策略，可增强LLMs在动态学习环境中的鲁棒性和适应性。

Abstract: Despite the significant advancements in Large Language Models (LLMs),
catastrophic forgetting remains a substantial challenge, where models lose
previously acquired knowledge upon learning new information. Continual learning
(CL) strategies have emerged as a potential solution to this problem, with
replay-based techniques demonstrating superior performance in preserving
learned knowledge. In this context, we introduce Gauss-Tin, a novel approach
that integrates the replay strategy with a Gaussian mixture model to enhance
the quality of sample selection during training, supplemented by instructional
guidance to facilitate the generation of past learning. This method aims to
improve LLMs' retention capabilities by strategically reinforcing important
past learnings while accommodating new information. Our experimental results
indicate a promising 6\% improvement in retention metrics over traditional
methods, suggesting that Gauss-Tin is an effective strategy for mitigating
catastrophic forgetting in LLMs. This study underscores the potential of hybrid
models in enhancing the robustness and adaptability of LLMs in dynamic learning
environments.

</details>


### [242] [Time-Aware and Transition-Semantic Graph Neural Networks for Interpretable Predictive Business Process Monitoring](https://arxiv.org/abs/2508.09527)
*Fang Wang,Ernesto Damiani*

Main category: cs.LG

TL;DR: 提出了一种统一的、可解释的GNN框架，用于预测业务流程监控中的未来事件，解决了现有GNN模型在时间相关性和转移语义上的不足。


<details>
  <summary>Details</summary>
Motivation: 现有基于GNN的PBPM模型在捕捉时间相关性和转移语义方面表现不足，需要更先进的解决方案。

Method: 结合前缀GCN和全迹GAT，引入时间衰减注意力机制和转移类型语义嵌入，提升模型性能。

Result: 在五个基准测试中表现优异，无需针对数据集调整即可实现高准确率。

Conclusion: 该框架为PBPM中的下一事件预测提供了鲁棒、通用且可解释的解决方案。

Abstract: Predictive Business Process Monitoring (PBPM) aims to forecast future events
in ongoing cases based on historical event logs. While Graph Neural Networks
(GNNs) are well suited to capture structural dependencies in process data,
existing GNN-based PBPM models remain underdeveloped. Most rely either on short
prefix subgraphs or global architectures that overlook temporal relevance and
transition semantics. We propose a unified, interpretable GNN framework that
advances the state of the art along three key axes. First, we compare
prefix-based Graph Convolutional Networks(GCNs) and full trace Graph Attention
Networks(GATs) to quantify the performance gap between localized and global
modeling. Second, we introduce a novel time decay attention mechanism that
constructs dynamic, prediction-centered windows, emphasizing temporally
relevant history and suppressing noise. Third, we embed transition type
semantics into edge features to enable fine grained reasoning over structurally
ambiguous traces. Our architecture includes multilevel interpretability
modules, offering diverse visualizations of attention behavior. Evaluated on
five benchmarks, the proposed models achieve competitive Top-k accuracy and DL
scores without per-dataset tuning. By addressing architectural, temporal, and
semantic gaps, this work presents a robust, generalizable, and explainable
solution for next event prediction in PBPM.

</details>


### [243] [Decentralized Rank Scheduling for Energy-Constrained Multi-Task Federated Fine-Tuning in Edge-Assisted IoV Networks](https://arxiv.org/abs/2508.09532)
*Bokeng Zheng,Jianqiang Zhong,Jiayi Liu,Xiaoxi Zhang*

Main category: cs.LG

TL;DR: 提出了一种分层联邦微调框架，用于动态车联网场景中的资源感知和移动弹性学习，通过LoRA和UCB-DUAL算法优化效率和准确性。


<details>
  <summary>Details</summary>
Motivation: 解决车联网系统中因客户端移动性、资源异构性和间歇性连接导致的多任务适应挑战。

Method: 采用分层联邦学习框架，结合LoRA和UCB-DUAL算法，实现资源感知和移动弹性学习。

Result: 实验表明，该方法在延迟减少24%的同时，平均准确率提升2.5%。

Conclusion: 提出的框架在车联网场景中实现了高效的准确性和效率权衡。

Abstract: Federated fine-tuning has emerged as a promising approach for adapting
foundation models (FMs) to diverse downstream tasks in edge environments. In
Internet of Vehicles (IoV) systems, enabling efficient and low-latency
multi-task adaptation is particularly challenging due to client mobility,
heterogeneous resources, and intermittent connectivity. This paper proposes a
hierarchical federated fine-tuning framework that coordinates roadside units
(RSUs) and vehicles to support resource-aware and mobility-resilient learning
across dynamic IoV scenarios. Leveraging Low-Rank Adaptation (LoRA), we
introduce a decentralized, energy-aware rank adaptation mechanism formulated as
a constrained multi-armed bandit problem. A novel UCB-DUAL algorithm is
developed to enable adaptive exploration under per-task energy budgets,
achieving provable sublinear regret. To evaluate our method, we construct a
large-scale IoV simulator based on real-world trajectories, capturing dynamic
participation, RSU handoffs, and communication variability. Extensive
experiments show that our approach achieves the best accuracy-efficiency
trade-off among all baselines, reducing latency by over 24\% and improving
average accuracy by more than 2.5\%.

</details>


### [244] [SYNAPSE-G: Bridging Large Language Models and Graph Learning for Rare Event Classification](https://arxiv.org/abs/2508.09544)
*Sasan Tavakkol,Lin Chen,Max Springer,Abigail Schantz,Blaž Bratanič,Vincent Cohen-Addad,MohammadHossein Bateni*

Main category: cs.LG

TL;DR: SYNAPSE-G利用LLM生成合成数据解决罕见事件分类中的冷启动问题，并通过半监督标签传播扩展数据集，实验证明其优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 标记数据稀缺，尤其是罕见事件，限制了机器学习模型的训练效果。

Method: 提出SYNAPSE-G，利用LLM生成合成数据作为种子，通过相似图进行半监督标签传播，扩展数据集并训练分类器。

Result: 在SST2和MHS数据集上，SYNAPSE-G在寻找正标签方面表现优于基线方法。

Conclusion: SYNAPSE-G通过合成数据和标签传播有效解决了罕见事件分类中的冷启动问题。

Abstract: Scarcity of labeled data, especially for rare events, hinders training
effective machine learning models. This paper proposes SYNAPSE-G (Synthetic
Augmentation for Positive Sampling via Expansion on Graphs), a novel pipeline
leveraging Large Language Models (LLMs) to generate synthetic training data for
rare event classification, addressing the cold-start problem. This synthetic
data serve as seeds for semi-supervised label propagation on a similarity graph
constructed between the seeds and a large unlabeled dataset. This identifies
candidate positive examples, subsequently labeled by an oracle (human or LLM).
The expanded dataset then trains/fine-tunes a classifier. We theoretically
analyze how the quality (validity and diversity) of the synthetic data impacts
the precision and recall of our method. Experiments on the imbalanced SST2 and
MHS datasets demonstrate SYNAPSE-G's effectiveness in finding positive labels,
outperforming baselines including nearest neighbor search.

</details>


### [245] [Edge General Intelligence Through World Models and Agentic AI: Fundamentals, Solutions, and Challenges](https://arxiv.org/abs/2508.09561)
*Changyuan Zhao,Guangyuan Liu,Ruichen Zhang,Yinqiu Liu,Jiacheng Wang,Jiawen Kang,Dusit Niyato,Zan Li,Xuemin,Shen,Zhu Han,Sumei Sun,Chau Yuen,Dong In Kim*

Main category: cs.LG

TL;DR: EGI（边缘通用智能）通过世界模型实现分布式智能体的感知、推理和自主行动，填补了无线边缘领域的研究空白。


<details>
  <summary>Details</summary>
Motivation: 探索世界模型在无线边缘计算中的应用，以提升智能体在动态环境中的自主决策能力。

Method: 分析世界模型的架构（如潜在表示学习、动态建模和基于想象的规划），并展示其在EGI场景中的应用。

Result: 世界模型在车辆网络、无人机网络、物联网等场景中优化了延迟、能耗和隐私问题，并与基础模型和数字孪生协同。

Conclusion: 世界模型是EGI的认知核心，但仍需解决安全性、高效训练和部署等挑战，为未来智能边缘系统提供方向。

Abstract: Edge General Intelligence (EGI) represents a transformative evolution of edge
computing, where distributed agents possess the capability to perceive, reason,
and act autonomously across diverse, dynamic environments. Central to this
vision are world models, which act as proactive internal simulators that not
only predict but also actively imagine future trajectories, reason under
uncertainty, and plan multi-step actions with foresight. This proactive nature
allows agents to anticipate potential outcomes and optimize decisions ahead of
real-world interactions. While prior works in robotics and gaming have
showcased the potential of world models, their integration into the wireless
edge for EGI remains underexplored. This survey bridges this gap by offering a
comprehensive analysis of how world models can empower agentic artificial
intelligence (AI) systems at the edge. We first examine the architectural
foundations of world models, including latent representation learning, dynamics
modeling, and imagination-based planning. Building on these core capabilities,
we illustrate their proactive applications across EGI scenarios such as
vehicular networks, unmanned aerial vehicle (UAV) networks, the Internet of
Things (IoT) systems, and network functions virtualization, thereby
highlighting how they can enhance optimization under latency, energy, and
privacy constraints. We then explore their synergy with foundation models and
digital twins, positioning world models as the cognitive backbone of EGI.
Finally, we highlight open challenges, such as safety guarantees, efficient
training, and constrained deployment, and outline future research directions.
This survey provides both a conceptual foundation and a practical roadmap for
realizing the next generation of intelligent, autonomous edge systems.

</details>


### [246] [Online Prediction with Limited Selectivity](https://arxiv.org/abs/2508.09592)
*Licheng Liu,Mingda Qiao*

Main category: cs.LG

TL;DR: 论文提出了一种有限选择性预测（PLS）模型，研究在预测窗口受限时的最优预测误差。


<details>
  <summary>Details</summary>
Motivation: 现有选择性预测模型允许预测者在任意时间开始预测，但现实中预测窗口可能受限，因此需要研究受限情况下的预测性能。

Method: 引入PLS模型，分析实例依赖和平均情况下的最优预测误差，并提出一种复杂度度量方法。

Result: 提出的复杂度度量方法能提供实例依赖的最优误差界限，且在随机生成的PLS实例中高概率匹配。

Conclusion: PLS模型为受限预测窗口场景提供了理论支持，复杂度度量方法能有效评估预测性能。

Abstract: Selective prediction [Dru13, QV19] models the scenario where a forecaster
freely decides on the prediction window that their forecast spans. Many data
statistics can be predicted to a non-trivial error rate without any
distributional assumptions or expert advice, yet these results rely on that the
forecaster may predict at any time. We introduce a model of Prediction with
Limited Selectivity (PLS) where the forecaster can start the prediction only on
a subset of the time horizon. We study the optimal prediction error both on an
instance-by-instance basis and via an average-case analysis. We introduce a
complexity measure that gives instance-dependent bounds on the optimal error.
For a randomly-generated PLS instance, these bounds match with high
probability.

</details>


### [247] [Goal Discovery with Causal Capacity for Efficient Reinforcement Learning](https://arxiv.org/abs/2508.09624)
*Yan Yu,Yaodong Yang,Zhengbo Lu,Chengdong Ma,Wengang Zhou,Houqiang Li*

Main category: cs.LG

TL;DR: 论文提出了一种基于因果容量的目标发现框架（GDCC），用于强化学习中的高效环境探索。通过测量状态空间中的因果容量，并利用蒙特卡洛方法识别关键点，指导智能体更有目的性地探索。


<details>
  <summary>Details</summary>
Motivation: 现有研究表明，动作与状态转换之间的因果关系能提升智能体的探索效率，但复杂场景中因果关系的测量具有挑战性。

Method: 提出因果容量测量方法，利用蒙特卡洛方法识别关键点，并将其作为子目标指导探索。

Result: 实验表明，高因果容量的状态与预期子目标一致，GDCC在成功率上显著优于基线方法。

Conclusion: GDCC框架通过因果容量和关键点识别，有效提升了强化学习中的探索效率。

Abstract: Causal inference is crucial for humans to explore the world, which can be
modeled to enable an agent to efficiently explore the environment in
reinforcement learning. Existing research indicates that establishing the
causality between action and state transition will enhance an agent to reason
how a policy affects its future trajectory, thereby promoting directed
exploration. However, it is challenging to measure the causality due to its
intractability in the vast state-action space of complex scenarios. In this
paper, we propose a novel Goal Discovery with Causal Capacity (GDCC) framework
for efficient environment exploration. Specifically, we first derive a
measurement of causality in state space, \emph{i.e.,} causal capacity, which
represents the highest influence of an agent's behavior on future trajectories.
After that, we present a Monte Carlo based method to identify critical points
in discrete state space and further optimize this method for continuous
high-dimensional environments. Those critical points are used to uncover where
the agent makes important decisions in the environment, which are then regarded
as our subgoals to guide the agent to make exploration more purposefully and
efficiently. Empirical results from multi-objective tasks demonstrate that
states with high causal capacity align with our expected subgoals, and our GDCC
achieves significant success rate improvements compared to baselines.

</details>


### [248] [Physics- and geometry-aware spatio-spectral graph neural operator for time-independent and time-dependent PDEs](https://arxiv.org/abs/2508.09627)
*Subhankar Sarkar,Souvik Chakraborty*

Main category: cs.LG

TL;DR: 论文提出了一种物理和几何感知的时空谱图神经算子（πG-Sp²GNO），用于高效求解复杂几何和有限标签数据下的偏微分方程（PDEs）。该方法改进了现有技术，结合几何感知和物理信息，在仿真自由设置中学习解算子。


<details>
  <summary>Details</summary>
Motivation: 偏微分方程的高效求解在科学与工程中至关重要，但复杂几何和有限数据增加了挑战。现有方法在几何适应性和物理信息整合上存在不足。

Method: 提出πG-Sp²GNO，结合几何感知和物理信息，采用时空谱结构实现多尺度学习。针对时间相关问题，设计了混合物理损失函数。

Result: 在多个基准测试中，该方法在复杂几何和时变问题中表现优于现有物理信息神经算子算法。

Conclusion: πG-Sp²GNO通过几何感知和物理信息整合，显著提升了PDE求解的效率和准确性，适用于复杂场景。

Abstract: Solving partial differential equations (PDEs) efficiently and accurately
remains a cornerstone challenge in science and engineering, especially for
problems involving complex geometries and limited labeled data. We introduce a
Physics- and Geometry- Aware Spatio-Spectral Graph Neural Operator
($\pi$G-Sp$^2$GNO) for learning the solution operators of time-independent and
time-dependent PDEs. The proposed approach first improves upon the recently
developed Sp$^2$GNO by enabling geometry awareness and subsequently exploits
the governing physics to learn the underlying solution operator in a
simulation-free setup. While the spatio-spectral structure present in the
proposed architecture allows multiscale learning, two separate strategies for
enabling geometry awareness is introduced in this paper. For time dependent
problems, we also introduce a novel hybrid physics informed loss function that
combines higher-order time-marching scheme with upscaled theory inspired
stochastic projection scheme. This allows accurate integration of the
physics-information into the loss function. The performance of the proposed
approach is illustrated on number of benchmark examples involving regular and
complex domains, variation in geometry during inference, and time-independent
and time-dependent problems. The results obtained illustrate the efficacy of
the proposed approach as compared to the state-of-the-art physics-informed
neural operator algorithms in the literature.

</details>


### [249] [TimeMKG: Knowledge-Infused Causal Reasoning for Multivariate Time Series Modeling](https://arxiv.org/abs/2508.09630)
*Yifei Sun,Junming Liu,Ding Wang,Yirong Chen,Xuefeng Yan*

Main category: cs.LG

TL;DR: TimeMKG是一个多模态因果推理框架，利用变量语义和历史时间序列数据提升时间序列建模的预测性能和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 传统时间序列模型忽略变量名称和描述中的语义信息，而这些信息对建模至关重要。TimeMKG旨在通过结合语义和统计模式改进建模。

Method: TimeMKG使用大语言模型解析变量语义，构建多变量知识图谱，并通过双模态编码器分别建模语义提示和历史时间序列模式，最后通过跨模态注意力融合。

Result: 实验表明，引入变量级知识显著提升了预测性能和泛化能力。

Conclusion: TimeMKG通过结合语义和统计信息，为时间序列建模提供了更鲁棒和可解释的方法。

Abstract: Multivariate time series data typically comprises two distinct modalities:
variable semantics and sampled numerical observations. Traditional time series
models treat variables as anonymous statistical signals, overlooking the rich
semantic information embedded in variable names and data descriptions. However,
these textual descriptors often encode critical domain knowledge that is
essential for robust and interpretable modeling. Here we present TimeMKG, a
multimodal causal reasoning framework that elevates time series modeling from
low-level signal processing to knowledge informed inference. TimeMKG employs
large language models to interpret variable semantics and constructs structured
Multivariate Knowledge Graphs that capture inter-variable relationships. A
dual-modality encoder separately models the semantic prompts, generated from
knowledge graph triplets, and the statistical patterns from historical time
series. Cross-modality attention aligns and fuses these representations at the
variable level, injecting causal priors into downstream tasks such as
forecasting and classification, providing explicit and interpretable priors to
guide model reasoning. The experiment in diverse datasets demonstrates that
incorporating variable-level knowledge significantly improves both predictive
performance and generalization.

</details>


### [250] [Thermal Tracks: A Gaussian process-based framework for universal melting curve analysis enabling unconstrained hit identification in thermal proteome profiling experiments](https://arxiv.org/abs/2508.09659)
*Johannes F. Hevler,Shivam Verma,Mirat Soijtra,Carolyn R. Bertozzi*

Main category: cs.LG

TL;DR: Thermal Tracks是一个基于Python的统计框架，用于分析蛋白质热稳定性数据，克服了现有热蛋白质组分析（TPP）方法的关键限制。


<details>
  <summary>Details</summary>
Motivation: 传统TPP方法假设蛋白质熔解曲线为S形，并受限于经验零分布，导致仅能检测约5%的数据。Thermal Tracks旨在解决这一问题，尤其适用于分析显著改变蛋白质热稳定性的扰动。

Method: 采用高斯过程（GP）模型和平方指数核，灵活建模任意熔解曲线形状，并通过核先验生成无偏零分布。

Result: Thermal Tracks能够有效分析非传统熔解曲线的蛋白质（如相分离蛋白和膜蛋白），并在全蛋白质组水平上检测生物相关变化。

Conclusion: Thermal Tracks是一个免费、灵活且易于使用的工具，适用于全蛋白质组热稳定性研究。

Abstract: Thermal Tracks is a Python-based statistical framework for analyzing protein
thermal stability data that overcomes key limitations of existing thermal
proteome profiling (TPP) work-flows. Unlike standard approaches that assume
sigmoidal melting curves and are constrained by empirical null distributions
(limiting significant hits to approximately 5 % of data), Thermal Tracks uses
Gaussian Process (GP) models with squared-exponential kernels to flexibly model
any melting curve shape while generating unbiased null distributions through
kernel priors. This framework is particularly valuable for analyzing
proteome-wide perturbations that significantly alter protein thermal stability,
such as pathway inhibitions, genetic modifications, or environmental stresses,
where conventional TPP methods may miss biologically relevant changes due to
their statistical constraints. Furthermore, Thermal Tracks excels at analyzing
proteins with un-conventional melting profiles, including phase-separating
proteins and membrane proteins, which often exhibit complex, non-sigmoidal
thermal stability behaviors. Thermal Tracks is freely available from GitHub and
is implemented in Python, providing an accessible and flexible tool for
proteome-wide thermal profiling studies.

</details>


### [251] [Global Convergence Analysis of Vanilla Gradient Descent for Asymmetric Matrix Completion](https://arxiv.org/abs/2508.09685)
*Xu Zhang,Shuo Chen,Jinsheng Li,Xiangying Pang,Maoguo Gong*

Main category: cs.LG

TL;DR: 本文研究了非对称低秩矩阵补全问题，通过梯度下降方法解决，发现无需正则化项也能保证收敛。


<details>
  <summary>Details</summary>
Motivation: 探讨梯度下降算法在非对称低秩矩阵补全中的表现，验证是否必须依赖正则化项。

Method: 使用谱初始化的普通梯度下降法，结合留一技术进行理论分析。

Result: 证明算法具有高概率线性收敛率，且计算成本更低。

Conclusion: 梯度下降具有隐式正则化特性，无需显式正则化项即可高效收敛。

Abstract: This paper investigates the asymmetric low-rank matrix completion problem,
which can be formulated as an unconstrained non-convex optimization problem
with a nonlinear least-squares objective function, and is solved via gradient
descent methods. Previous gradient descent approaches typically incorporate
regularization terms into the objective function to guarantee convergence.
However, numerical experiments and theoretical analysis of the gradient flow
both demonstrate that the elimination of regularization terms in gradient
descent algorithms does not adversely affect convergence performance. By
introducing the leave-one-out technique, we inductively prove that the vanilla
gradient descent with spectral initialization achieves a linear convergence
rate with high probability. Besides, we demonstrate that the balancing
regularization term exhibits a small norm during iterations, which reveals the
implicit regularization property of gradient descent. Empirical results show
that our algorithm has a lower computational cost while maintaining comparable
completion performance compared to other gradient descent algorithms.

</details>


### [252] [Temporal Anchoring in Deepening Embedding Spaces: Event-Indexed Projections, Drift, Convergence, and an Internal Computational Architecture](https://arxiv.org/abs/2508.09693)
*Faruk Alpay,Bugra Kilictas,Hamdi Alakkad*

Main category: cs.LG

TL;DR: 本文提出了一种基于算子理论的框架，用于嵌入空间中的时间锚定，通过漂移映射和事件索引块的交替实现，最终通过仿射投影完成。


<details>
  <summary>Details</summary>
Motivation: 旨在为嵌入空间中的时间锚定提供一个严格的数学框架，并验证其计算等价性和鲁棒性。

Method: 采用算子理论框架，结合漂移映射和事件索引块，通过仿射投影实现收敛，并开发了一个内部手稿计算机（MC）来验证计算等价性。

Result: 证明了变块收缩引理、漂移-投影收敛定理，以及软注意力层的Lipschitz性质，并给出了层收缩的充分条件。

Conclusion: 该框架为时间锚定提供了严格的数学基础，并在计算等价性和鲁棒性方面取得了显著成果。

Abstract: We develop an operator-theoretic framework for temporal anchoring in
embedding spaces, modeled as drift maps interleaved with event-indexed blocks
culminating in affine projections. We provide complete proofs for a
variable-block contraction lemma (products of Lipschitz factors), a
drift--projection convergence theorem with explicit uniform-gap envelopes, and
ontological convergence under nested affine anchors with a robustness variant.
We formalize an internal Manuscript Computer (MC) whose computations are
defined purely by these operators and prove a rigorous finite-run equivalence
theorem (with perturbation bounds). For attention layers, we give a
self-contained proof that softmax is $1/2$-Lipschitz in $\ell_2$ and derive
sufficient layer-contraction conditions (orthogonal/non-orthogonal heads). All
floats are placed exactly where written; the manuscript uses only in-paper
pseudocode and appendix figures.

</details>


### [253] [Combating Noisy Labels via Dynamic Connection Masking](https://arxiv.org/abs/2508.09697)
*Xinlei Zhang,Fan Liu,Chuanyi Zhang,Fan Cheng,Yuhui Zheng*

Main category: cs.LG

TL;DR: 提出了一种动态连接掩码（DCM）机制，用于增强MLPs和KANs对噪声标签的鲁棒性，通过自适应掩码减少梯度误差，并在实验中优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现实场景中噪声标签不可避免，现有研究主要关注鲁棒损失函数和样本选择，对模型架构正则化的探索较少。

Method: 基于KANs的稀疏正则化，提出DCM机制，动态评估并掩码不重要连接。

Result: 实验证明DCM在合成和真实数据集上优于现有方法，并首次验证KANs在噪声标签分类中的优越性。

Conclusion: DCM可无缝集成多种噪声鲁棒方法，提升模型鲁棒性，KANs在噪声场景中表现优于MLPs。

Abstract: Noisy labels are inevitable in real-world scenarios. Due to the strong
capacity of deep neural networks to memorize corrupted labels, these noisy
labels can cause significant performance degradation. Existing research on
mitigating the negative effects of noisy labels has mainly focused on robust
loss functions and sample selection, with comparatively limited exploration of
regularization in model architecture. Inspired by the sparsity regularization
used in Kolmogorov-Arnold Networks (KANs), we propose a Dynamic Connection
Masking (DCM) mechanism for both Multi-Layer Perceptron Networks (MLPs) and
KANs to enhance the robustness of classifiers against noisy labels. The
mechanism can adaptively mask less important edges during training by
evaluating their information-carrying capacity. Through theoretical analysis,
we demonstrate its efficiency in reducing gradient error. Our approach can be
seamlessly integrated into various noise-robust training methods to build more
robust deep networks, including robust loss functions, sample selection
strategies, and regularization techniques. Extensive experiments on both
synthetic and real-world benchmarks demonstrate that our method consistently
outperforms state-of-the-art (SOTA) approaches. Furthermore, we are also the
first to investigate KANs as classifiers against noisy labels, revealing their
superior noise robustness over MLPs in real-world noisy scenarios. Our code
will soon be publicly available.

</details>


### [254] [GraphTreeGen: Subtree-Centric Approach to Efficient and Supervised Graph Generation](https://arxiv.org/abs/2508.09710)
*Yitong Luo,Islem Rekik*

Main category: cs.LG

TL;DR: 论文提出GraphTreeGen（GTG），一种子树中心的生成框架，用于高效、准确的连接组合成，解决了现有模型在局部细节、节点属性依赖、边权重预测和计算效率方面的局限性。


<details>
  <summary>Details</summary>
Motivation: 脑连接组数据获取成本高且耗时，现有生成模型在局部细节、节点属性依赖、边权重预测和计算效率方面存在不足，亟需改进。

Method: GTG将连接组分解为熵引导的k-hop子树，通过共享GCN编码，结合全局节点特征，使用双分支解码器联合预测边存在和权重。

Result: GTG在自监督任务中优于现有方法，在监督任务中表现竞争力，提供更高结构保真度和更精确的权重预测，且内存需求更低。

Conclusion: GTG是一种高效、准确的连接组生成框架，其模块化设计支持扩展至超分辨率和跨模态合成。

Abstract: Brain connectomes, representing neural connectivity as graphs, are crucial
for understanding brain organization but costly and time-consuming to acquire,
motivating generative approaches. Recent advances in graph generative modeling
offer a data-driven alternative, enabling synthetic connectome generation and
reducing dependence on large neuroimaging datasets. However, current models
face key limitations: (i) compressing the whole graph into a single latent code
(e.g., VGAEs) blurs fine-grained local motifs; (ii) relying on rich node
attributes rarely available in connectomes reduces reconstruction quality;
(iii) edge-centric models emphasize topology but overlook accurate edge-weight
prediction, harming quantitative fidelity; and (iv) computationally expensive
designs (e.g., edge-conditioned convolutions) impose high memory demands,
limiting scalability. We propose GraphTreeGen (GTG), a subtree-centric
generative framework for efficient, accurate connectome synthesis. GTG
decomposes each connectome into entropy-guided k-hop trees capturing
informative local structure, encoded by a shared GCN. A bipartite
message-passing layer fuses subtree embeddings with global node features, while
a dual-branch decoder jointly predicts edge existence and weights to
reconstruct the adjacency matrix. GTG outperforms state-of-the-art baselines in
self-supervised tasks and remains competitive in supervised settings,
delivering higher structural fidelity and more precise weights with far less
memory. Its modular design enables extensions to connectome super-resolution
and cross-modality synthesis. Code: https://github.com/basiralab/GTG/

</details>


### [255] [Improving ARDS Diagnosis Through Context-Aware Concept Bottleneck Models](https://arxiv.org/abs/2508.09719)
*Anish Narain,Ritam Majumdar,Nikita Narayanan,Dominic Marshall,Sonali Parbhoo*

Main category: cs.LG

TL;DR: 论文提出了一种结合临床笔记上下文信息的方法，通过大型语言模型（LLM）生成额外概念，提升概念瓶颈模型（CBM）在急性呼吸窘迫综合征（ARDS）识别中的性能。


<details>
  <summary>Details</summary>
Motivation: 利用大型公开临床数据集理解疾病异质性和个性化治疗，但数据不完整且缺乏标签。现有AI工具解释性不足，概念瓶颈模型（CBM）在概念无法充分解释任务时性能受限。

Method: 结合临床笔记的上下文信息，使用LLM生成额外概念，改进CBM。

Result: 性能提升10%，学习到更全面的概念，减少信息泄露和虚假捷径依赖。

Conclusion: 该方法通过结合临床笔记上下文信息，显著提升了CBM的性能和解释性，为疾病表征提供了更全面的方法。

Abstract: Large, publicly available clinical datasets have emerged as a novel resource
for understanding disease heterogeneity and to explore personalization of
therapy. These datasets are derived from data not originally collected for
research purposes and, as a result, are often incomplete and lack critical
labels. Many AI tools have been developed to retrospectively label these
datasets, such as by performing disease classification; however, they often
suffer from limited interpretability. Previous work has attempted to explain
predictions using Concept Bottleneck Models (CBMs), which learn interpretable
concepts that map to higher-level clinical ideas, facilitating human
evaluation. However, these models often experience performance limitations when
the concepts fail to adequately explain or characterize the task. We use the
identification of Acute Respiratory Distress Syndrome (ARDS) as a challenging
test case to demonstrate the value of incorporating contextual information from
clinical notes to improve CBM performance. Our approach leverages a Large
Language Model (LLM) to process clinical notes and generate additional
concepts, resulting in a 10% performance gain over existing methods.
Additionally, it facilitates the learning of more comprehensive concepts,
thereby reducing the risk of information leakage and reliance on spurious
shortcuts, thus improving the characterization of ARDS.

</details>


### [256] [Generative Modeling with Multi-Instance Reward Learning for E-commerce Creative Optimization](https://arxiv.org/abs/2508.09730)
*Qiaolei Gu,Yu Li,DingYi Zeng,Lu Wang,Ming Pang,Changping Peng,Zhangang Lin,Ching Law,Jingping Shao*

Main category: cs.LG

TL;DR: 论文提出GenCO框架，结合生成模型与多实例奖励学习，优化电商广告创意组合选择，显著提升广告收入。


<details>
  <summary>Details</summary>
Motivation: 现有方法单独评估广告创意元素，难以处理组合搜索空间大的问题。

Method: 采用两阶段架构：生成模型生成创意组合，强化学习优化选择；多实例学习模型分配组合级奖励。

Result: 在电商平台部署后显著增加广告收入。

Conclusion: GenCO框架有效解决创意组合优化问题，并发布工业数据集促进研究。

Abstract: In e-commerce advertising, selecting the most compelling combination of
creative elements -- such as titles, images, and highlights -- is critical for
capturing user attention and driving conversions. However, existing methods
often evaluate creative components individually, failing to navigate the
exponentially large search space of possible combinations. To address this
challenge, we propose a novel framework named GenCO that integrates generative
modeling with multi-instance reward learning. Our unified two-stage
architecture first employs a generative model to efficiently produce a diverse
set of creative combinations. This generative process is optimized with
reinforcement learning, enabling the model to effectively explore and refine
its selections. Next, to overcome the challenge of sparse user feedback, a
multi-instance learning model attributes combination-level rewards, such as
clicks, to the individual creative elements. This allows the reward model to
provide a more accurate feedback signal, which in turn guides the generative
model toward creating more effective combinations. Deployed on a leading
e-commerce platform, our approach has significantly increased advertising
revenue, demonstrating its practical value. Additionally, we are releasing a
large-scale industrial dataset to facilitate further research in this important
domain.

</details>


### [257] [HKT: A Biologically Inspired Framework for Modular Hereditary Knowledge Transfer in Neural Networks](https://arxiv.org/abs/2508.09743)
*Yanick Chistian Tchenko,Felix Mohr,Hicham Hadj Abdelkader,Hedi Tabia*

Main category: cs.LG

TL;DR: 提出了一种名为Hereditary Knowledge Transfer (HKT) 的方法，通过结构化知识继承优化小型可部署模型，性能优于传统蒸馏方法。


<details>
  <summary>Details</summary>
Motivation: 解决深度模型性能提升与可部署性之间的矛盾，通过生物启发的知识继承机制优化小型模型。

Method: 提出HKT框架，包含提取、转移和混合（ETM）三阶段，结合遗传注意力机制（GA）选择性继承知识。

Result: 在多种视觉任务（如光流、图像分类、语义分割）中，HKT显著提升小型模型性能，同时保持紧凑性。

Conclusion: HKT为资源受限环境提供了一种通用、可解释且可扩展的高性能神经网络部署方案。

Abstract: A prevailing trend in neural network research suggests that model performance
improves with increasing depth and capacity - often at the cost of
integrability and efficiency. In this paper, we propose a strategy to optimize
small, deployable models by enhancing their capabilities through structured
knowledge inheritance. We introduce Hereditary Knowledge Transfer (HKT), a
biologically inspired framework for modular and selective transfer of
task-relevant features from a larger, pretrained parent network to a smaller
child model. Unlike standard knowledge distillation, which enforces uniform
imitation of teacher outputs, HKT draws inspiration from biological inheritance
mechanisms - such as memory RNA transfer in planarians - to guide a multi-stage
process of feature transfer. Neural network blocks are treated as functional
carriers, and knowledge is transmitted through three biologically motivated
components: Extraction, Transfer, and Mixture (ETM). A novel Genetic Attention
(GA) mechanism governs the integration of inherited and native representations,
ensuring both alignment and selectivity. We evaluate HKT across diverse vision
tasks, including optical flow (Sintel, KITTI), image classification (CIFAR-10),
and semantic segmentation (LiTS), demonstrating that it significantly improves
child model performance while preserving its compactness. The results show that
HKT consistently outperforms conventional distillation approaches, offering a
general-purpose, interpretable, and scalable solution for deploying
high-performance neural networks in resource-constrained environments.

</details>


### [258] [A Machine Learning Approach to Predict Biological Age and its Longitudinal Drivers](https://arxiv.org/abs/2508.09747)
*Nazira Dunbayeva,Yulong Li,Yutong Xie,Imran Razzak*

Main category: cs.LG

TL;DR: 该研究开发了一个机器学习流程，通过纵向数据预测年龄，发现动态变化的生物标志物比静态数据更能准确预测年龄。


<details>
  <summary>Details</summary>
Motivation: 预测个体的衰老轨迹是预防医学和生物信息学的核心挑战，但现有模型难以捕捉衰老的动态过程。

Method: 使用纵向队列数据（2019-2020和2021-2022），设计动态变化的生物标志物特征，训练LightGBM模型。

Result: 模型在预测后续时间点的年龄时表现优异（男性R²=0.515，女性R²=0.498），显著优于传统方法。

Conclusion: 动态健康轨迹是预测生物年龄的关键，该框架为临床工具开发提供了基础。

Abstract: Predicting an individual's aging trajectory is a central challenge in
preventative medicine and bioinformatics. While machine learning models can
predict chronological age from biomarkers, they often fail to capture the
dynamic, longitudinal nature of the aging process. In this work, we developed
and validated a machine learning pipeline to predict age using a longitudinal
cohort with data from two distinct time periods (2019-2020 and 2021-2022). We
demonstrate that a model using only static, cross-sectional biomarkers has
limited predictive power when generalizing to future time points. However, by
engineering novel features that explicitly capture the rate of change (slope)
of key biomarkers over time, we significantly improved model performance. Our
final LightGBM model, trained on the initial wave of data, successfully
predicted age in the subsequent wave with high accuracy ($R^2 = 0.515$ for
males, $R^2 = 0.498$ for females), significantly outperforming both traditional
linear models and other tree-based ensembles. SHAP analysis of our successful
model revealed that the engineered slope features were among the most important
predictors, highlighting that an individual's health trajectory, not just their
static health snapshot, is a key determinant of biological age. Our framework
paves the way for clinical tools that dynamically track patient health
trajectories, enabling early intervention and personalized prevention
strategies for age-related diseases.

</details>


### [259] [$μ$-Parametrization for Mixture of Experts](https://arxiv.org/abs/2508.09752)
*Jan Małaśnicki,Kamil Ciebiera,Mateusz Boruń,Maciej Pióro,Jan Ludziejewski,Maciej Stefaniak,Michał Krutul,Sebastian Jaszczur,Marek Cygan,Kamil Adamczewski,Jakub Krajewski*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Recent years have seen a growing interest and adoption of LLMs, with
$\mu$Transfer becoming a key technique for tuning hyperparameters in
large-scale training. Meanwhile, Mixture-of-Experts (MoE) has emerged as a
leading architecture in extremely large models. However, the intersection of
these two advancements has remained unexplored. In this work, we derive a
$\mu$-Parameterization ($\mu$P) for MoE, providing theoretical guarantees for
feature learning across model widths in both the router and experts. We
empirically validate our parameterization and further investigate how scaling
the number of experts and granularity affects the optimal learning rate.

</details>


### [260] [TriForecaster: A Mixture of Experts Framework for Multi-Region Electric Load Forecasting with Tri-dimensional Specialization](https://arxiv.org/abs/2508.09753)
*Zhaoyang Zhu,Zhipeng Zeng,Qiming Chen,Linxiao Yang,Peiyuan Liu,Weiqi Chen,Liang Sun*

Main category: cs.LG

TL;DR: 本文提出TriForecaster框架，通过混合专家（MoE）和多任务学习（MTL）方法解决多区域电力负荷预测（MRELF）问题，显著降低预测误差。


<details>
  <summary>Details</summary>
Motivation: 智能电网和电表的普及提供了更详细的负荷数据，但区域、上下文和时间变化给多区域负荷预测带来挑战。

Method: 提出TriForecaster框架，包含RegionMixer和CTSpecializer层，动态协调专家模型在区域、上下文和时间维度的合作与专业化。

Result: 在四个真实数据集上，TriForecaster平均减少22.4%的预测误差，并在中国东部17个城市成功部署。

Conclusion: TriForecaster展示了其在多区域电力负荷预测中的灵活性和实用性，适用于大规模实际应用。

Abstract: Electric load forecasting is pivotal for power system operation, planning and
decision-making. The rise of smart grids and meters has provided more detailed
and high-quality load data at multiple levels of granularity, from home to bus
and cities. Motivated by similar patterns of loads across different cities in a
province in eastern China, in this paper we focus on the Multi-Region Electric
Load Forecasting (MRELF) problem, targeting accurate short-term load
forecasting for multiple sub-regions within a large region. We identify three
challenges for MRELF, including regional variation, contextual variation, and
temporal variation. To address them, we propose TriForecaster, a new framework
leveraging the Mixture of Experts (MoE) approach within a Multi-Task Learning
(MTL) paradigm to overcome these challenges. TriForecaster features RegionMixer
and Context-Time Specializer (CTSpecializer) layers, enabling dynamic
cooperation and specialization of expert models across regional, contextual,
and temporal dimensions. Based on evaluation on four real-world MRELF datasets
with varied granularity, TriForecaster outperforms state-of-the-art models by
achieving an average forecast error reduction of 22.4\%, thereby demonstrating
its flexibility and broad applicability. In particular, the deployment of
TriForecaster on the eForecaster platform in eastern China exemplifies its
practical utility, effectively providing city-level, short-term load forecasts
for 17 cities, supporting a population exceeding 110 million and daily
electricity usage over 100 gigawatt-hours.

</details>


### [261] [Prototype Training with Dual Pseudo-Inverse and Optimized Hidden Activations](https://arxiv.org/abs/2508.09787)
*Mauro Tucci*

Main category: cs.LG

TL;DR: Proto-PINV+H是一种快速训练范式，结合闭式权重计算和梯度优化少量合成输入、软标签及隐藏激活。


<details>
  <summary>Details</summary>
Motivation: 旨在通过减少可训练参数数量并优化数据/激活空间，提高训练速度和模型性能。

Method: 采用闭式权重计算和梯度优化原型，通过岭正则伪逆求解更新权重矩阵，同时用Adam优化原型。

Result: 在MNIST和Fashion-MNIST上分别达到97.8%和89.3%的测试准确率，训练时间仅3.9-4.5秒。

Conclusion: 该方法在准确性、速度和模型大小之间取得了优越的权衡，优于传统方法。

Abstract: We present Proto-PINV+H, a fast training paradigm that combines closed-form
weight computation with gradient-based optimisation of a small set of synthetic
inputs, soft labels, and-crucially-hidden activations. At each iteration we
recompute all weight matrices in closed form via two (or more)
ridge-regularised pseudo-inverse solves, while updating only the prototypes
with Adam. The trainable degrees of freedom are thus shifted from weight space
to data/activation space. On MNIST (60k train, 10k test) and Fashion-MNIST (60k
train, 10k test), our method reaches 97.8% and 89.3% test accuracy on the
official 10k test sets, respectively, in 3.9s--4.5s using approximately 130k
trainable parameters and only 250 epochs on an RTX 5060 (16GB). We provide a
multi-layer extension (optimised activations at each hidden stage), learnable
ridge parameters, optional PCA/PLS projections, and theory linking the
condition number of prototype matrices to generalisation. The approach yields
favourable accuracy--speed--size trade-offs against ELM, random-feature ridge,
and shallow MLPs trained by back-propagation.

</details>


### [262] [Bayesian autoregression to optimize temporal Matérn kernel Gaussian process hyperparameters](https://arxiv.org/abs/2508.09792)
*Wouter M. Kouw*

Main category: cs.LG

TL;DR: 提出一种优化Matérn核时间高斯过程超参数的方法，基于递归贝叶斯估计，性能优于边际似然最大化和哈密顿蒙特卡洛采样。


<details>
  <summary>Details</summary>
Motivation: 高斯过程在概率数值中很重要，但现有方法（如边际似然最大化）在运行时和精度上存在不足。

Method: 将超参数优化问题转化为自回归模型参数的递归贝叶斯估计。

Result: 在运行时和均方根误差上优于边际似然最大化和哈密顿蒙特卡洛采样。

Conclusion: 该方法为高斯过程超参数优化提供了更高效的解决方案。

Abstract: Gaussian processes are important models in the field of probabilistic
numerics. We present a procedure for optimizing Mat\'ern kernel temporal
Gaussian processes with respect to the kernel covariance function's
hyperparameters. It is based on casting the optimization problem as a recursive
Bayesian estimation procedure for the parameters of an autoregressive model. We
demonstrate that the proposed procedure outperforms maximizing the marginal
likelihood as well as Hamiltonian Monte Carlo sampling, both in terms of
runtime and ultimate root mean square error in Gaussian process regression.

</details>


### [263] [Feature Impact Analysis on Top Long-Jump Performances with Quantile Random Forest and Explainable AI Techniques](https://arxiv.org/abs/2508.09810)
*Qi Gan,Stephan Clémençon,Mounîm A. El-Yacoubi,Sao Mai Nguyen,Eric Fenaux,Ons Jelassi*

Main category: cs.LG

TL;DR: 利用机器学习分析跳远比赛中生物力学特征对成绩的影响，发现男运动员的关键特征是支撑腿膝盖角度，女运动员则是着陆姿势和助跑技术。


<details>
  <summary>Details</summary>
Motivation: 传统方法难以明确分析复杂人体运动特征与成绩的关系，现代数据分析和机器学习为此提供了新工具。

Method: 使用分位数回归建模生物力学特征与有效距离的关系，结合SHAP、PDP和ICE图解释模型。

Result: 男运动员支撑腿膝盖角度大于169度对成绩贡献显著，女运动员着陆姿势和助跑技术是关键。

Conclusion: 研究为分析运动表现特征提供了框架，特别关注顶级赛事中的关键因素。

Abstract: Biomechanical features have become important indicators for evaluating
athletes' techniques. Traditionally, experts propose significant features and
evaluate them using physics equations. However, the complexity of the human
body and its movements makes it challenging to explicitly analyze the
relationships between some features and athletes' final performance. With
advancements in modern machine learning and statistics, data analytics methods
have gained increasing importance in sports analytics. In this study, we
leverage machine learning models to analyze expert-proposed biomechanical
features from the finals of long jump competitions in the World Championships.
The objectives of the analysis include identifying the most important features
contributing to top-performing jumps and exploring the combined effects of
these key features. Using quantile regression, we model the relationship
between the biomechanical feature set and the target variable (effective
distance), with a particular focus on elite-level jumps. To interpret the
model, we apply SHapley Additive exPlanations (SHAP) alongside Partial
Dependence Plots (PDPs) and Individual Conditional Expectation (ICE) plots. The
findings reveal that, beyond the well-documented velocity-related features,
specific technical aspects also play a pivotal role. For male athletes, the
angle of the knee of the supporting leg before take-off is identified as a key
factor for achieving top 10% performance in our dataset, with angles greater
than 169{\deg}contributing significantly to jump performance. In contrast, for
female athletes, the landing pose and approach step technique emerge as the
most critical features influencing top 10% performances, alongside velocity.
This study establishes a framework for analyzing the impact of various features
on athletic performance, with a particular emphasis on top-performing events.

</details>


### [264] [Provable In-Context Vector Arithmetic via Retrieving Task Concepts](https://arxiv.org/abs/2508.09820)
*Dake Bu,Wei Huang,Andi Han,Atsushi Nitanda,Qingfu Zhang,Hau-San Wong,Taiji Suzuki*

Main category: cs.LG

TL;DR: 论文提出了一种理论框架，解释了大语言模型（LLMs）在上下文学习（ICL）中如何通过向量算术解决事实召回任务，并证明了其强泛化能力。


<details>
  <summary>Details</summary>
Motivation: 尽管已有研究表明LLMs在ICL中存在潜在任务向量，并利用Word2Vec式向量算术完成任务，但缺乏理论解释。本文旨在填补这一空白。

Method: 基于层次化概念建模，提出了一种优化理论，分析了通过梯度下降训练的残差Transformer如何通过向量算术完成事实召回任务。

Result: 证明了0-1损失收敛性，并展示了强泛化能力，包括对概念重组和分布变化的鲁棒性。

Conclusion: 理论框架揭示了Transformer优于静态嵌入模型的优势，并通过实验验证了理论结果。

Abstract: In-context learning (ICL) has garnered significant attention for its ability
to grasp functions/tasks from demonstrations. Recent studies suggest the
presence of a latent task/function vector in LLMs during ICL. Merullo et al.
(2024) showed that LLMs leverage this vector alongside the residual stream for
Word2Vec-like vector arithmetic, solving factual-recall ICL tasks.
Additionally, recent work empirically highlighted the key role of
Question-Answer data in enhancing factual-recall capabilities. Despite these
insights, a theoretical explanation remains elusive. To move one step forward,
we propose a theoretical framework building on empirically grounded
hierarchical concept modeling. We develop an optimization theory, showing how
nonlinear residual transformers trained via gradient descent on cross-entropy
loss perform factual-recall ICL tasks via vector arithmetic. We prove 0-1 loss
convergence and show the strong generalization, including robustness to concept
recombination and distribution shifts. These results elucidate the advantages
of transformers over static embedding predecessors. Empirical simulations
corroborate our theoretical insights.

</details>


### [265] [RankList -- A Listwise Preference Learning Framework for Predicting Subjective Preferences](https://arxiv.org/abs/2508.09826)
*Abinay Reddy Naini,Fernando Diaz,Carlos Busso*

Main category: cs.LG

TL;DR: 论文提出了一种名为RankList的新型列表式偏好学习框架，旨在解决传统成对比较方法（如RankNet）在全局排名一致性上的局限性。


<details>
  <summary>Details</summary>
Motivation: 传统成对比较方法（如RankNet）虽然能有效建模相对偏好，但局限于局部比较，难以捕捉全局排名一致性。

Method: 论文提出了RankList框架，通过列表级监督扩展RankNet，并引入局部和非局部排名约束的建模。采用log-sum-exp近似提高训练效率，并通过跳步比较增强全局排名能力。

Result: 在多个基准数据集（如MSP-Podcast、IEMOCAP、BIIC Podcast和Artistic Image Aesthetics）上，RankList在Kendall's Tau和排名准确性上均优于基线方法。

Conclusion: RankList不仅提升了领域内排名性能，还表现出更好的跨数据集泛化能力，为主观学习任务提供了一种统一且可扩展的解决方案。

Abstract: Preference learning has gained significant attention in tasks involving
subjective human judgments, such as \emph{speech emotion recognition} (SER) and
image aesthetic assessment. While pairwise frameworks such as RankNet offer
robust modeling of relative preferences, they are inherently limited to local
comparisons and struggle to capture global ranking consistency. To address
these limitations, we propose RankList, a novel listwise preference learning
framework that generalizes RankNet to structured list-level supervision. Our
formulation explicitly models local and non-local ranking constraints within a
probabilistic framework. The paper introduces a log-sum-exp approximation to
improve training efficiency. We further extend RankList with skip-wise
comparisons, enabling progressive exposure to complex list structures and
enhancing global ranking fidelity. Extensive experiments demonstrate the
superiority of our method across diverse modalities. On benchmark SER datasets
(MSP-Podcast, IEMOCAP, BIIC Podcast), RankList achieves consistent improvements
in Kendall's Tau and ranking accuracy compared to standard listwise baselines.
We also validate our approach on aesthetic image ranking using the Artistic
Image Aesthetics dataset, highlighting its broad applicability. Through
ablation and cross-domain studies, we show that RankList not only improves
in-domain ranking but also generalizes better across datasets. Our framework
offers a unified, extensible approach for modeling ordered preferences in
subjective learning scenarios.

</details>


### [266] [FedShard: Federated Unlearning with Efficiency Fairness and Performance Fairness](https://arxiv.org/abs/2508.09866)
*Siyuan Wen,Meng Zhang,Yang Yang,Ningning Ding*

Main category: cs.LG

TL;DR: FedShard是一种联邦学习遗忘算法，首次同时保证效率公平性和性能公平性，解决了收敛、遗忘效率和公平性之间的困境，并提出了新的公平性评估指标。


<details>
  <summary>Details</summary>
Motivation: 当前联邦学习遗忘研究主要关注效率和有效性，而忽略了去中心化客户端在遗忘过程中的效率公平性和性能公平性。

Method: FedShard通过自适应方法解决收敛、遗忘效率和公平性之间的挑战，并提出了两个新的公平性评估指标。

Result: FedShard在遗忘性能和效率上均表现出公平性，实验显示其遗忘速度比从头训练快1.3-6.2倍，比现有最优方法快4.9倍。

Conclusion: FedShard有效解决了联邦学习遗忘中的公平性问题，显著提升了遗忘效率，并降低了不公平风险。

Abstract: To protect clients' right to be forgotten in federated learning, federated
unlearning aims to remove the data contribution of leaving clients from the
global learned model. While current studies mainly focused on enhancing
unlearning efficiency and effectiveness, the crucial aspects of efficiency
fairness and performance fairness among decentralized clients during unlearning
have remained largely unexplored. In this study, we introduce FedShard, the
first federated unlearning algorithm designed to concurrently guarantee both
efficiency fairness and performance fairness. FedShard adaptively addresses the
challenges introduced by dilemmas among convergence, unlearning efficiency, and
unlearning fairness. Furthermore, we propose two novel metrics to
quantitatively assess the fairness of unlearning algorithms, which we prove to
satisfy well-known properties in other existing fairness measurements. Our
theoretical analysis and numerical evaluation validate FedShard's fairness in
terms of both unlearning performance and efficiency. We demonstrate that
FedShard mitigates unfairness risks such as cascaded leaving and poisoning
attacks and realizes more balanced unlearning costs among clients. Experimental
results indicate that FedShard accelerates the data unlearning process 1.3-6.2
times faster than retraining from scratch and 4.9 times faster than the
state-of-the-art exact unlearning methods.

</details>


### [267] [Beyond Scaling Law: A Data-Efficient Distillation Framework for Reasoning](https://arxiv.org/abs/2508.09883)
*Xiaojun Wu,Xiaoguang Jiang,Huiyang Li,Jucai Zhai,Dengfeng Liu,Qiaobo Hao,Huang Liu,Zhiguo Yang,Ji Xie,Ninglun Gu,Jin Yang,Kailai Zhang,Yelun Bao,Jun Wang*

Main category: cs.LG

TL;DR: 提出了一种数据高效蒸馏框架（DED），通过优化推理蒸馏的帕累托前沿，减少计算成本，同时保持高性能。


<details>
  <summary>Details</summary>
Motivation: 尽管现有方法通过扩展语料库和多阶段训练提升了推理能力，但计算成本高且缺乏针对性的推理扩展规律。DED旨在解决这一问题。

Method: 结合强化学习的策略和多样化推理轨迹，选择最优教师模型，并利用小规模精选语料库平衡领域内外性能。

Result: 在数学推理和代码生成任务上取得最先进成果，仅需0.8k精选样本，避免了大规模扩展。

Conclusion: DED提供了一种高效且实用的推理能力提升方法，同时保留了模型的通用能力。

Abstract: Large language models (LLMs) demonstrate remarkable reasoning capabilities in
tasks such as algorithmic coding and mathematical problem-solving. Recent
methods have improved reasoning through expanded corpus and multistage training
combining reinforcement learning and supervised fine-tuning. Although some
methods suggest that small but targeted dataset can incentivize reasoning via
only distillation, a reasoning scaling laws is still taking shape, increasing
computational costs. To address this, we propose a data-efficient distillation
framework (DED) that optimizes the Pareto frontier of reasoning distillation.
Inspired by the on-policy learning and diverse roll-out strategies of
reinforcement learning, the key idea of our approach is threefold: (1) We
identify that benchmark scores alone do not determine an effective teacher
model. Through comprehensive comparisons of leading reasoning LLMs, we develop
a method to select an optimal teacher model. (2) While scaling distillation can
enhance reasoning, it often degrades out-of-domain performance. A carefully
curated, smaller corpus achieves a balanced trade-off between in-domain and
out-of-domain capabilities. (3) Diverse reasoning trajectories encourage the
student model to develop robust reasoning skills. We validate our method
through evaluations on mathematical reasoning (AIME 2024/2025, MATH-500) and
code generation (LiveCodeBench), achieving state-of-the-art results with only
0.8k carefully curated examples, bypassing the need for extensive scaling. Our
systematic analysis demonstrates that DED outperforms existing methods by
considering factors beyond superficial hardness, token length, or teacher model
capability. This work offers a practical and efficient pathway to advanced
reasoning while preserving general capabilities.

</details>


### [268] [Modern Neural Networks for Small Tabular Datasets: The New Default for Field-Scale Digital Soil Mapping?](https://arxiv.org/abs/2508.09888)
*Viacheslav Barkov,Jonas Schmidinger,Robin Gebbers,Martin Atzmueller*

Main category: cs.LG

TL;DR: 现代人工神经网络（ANN）在土壤属性预测中超越了传统机器学习方法，尤其是TabPFN表现最佳。


<details>
  <summary>Details</summary>
Motivation: 验证现代ANN在田间尺度土壤预测建模（PSM）中的适用性，挑战传统方法的主导地位。

Method: 通过31个数据集评估多种ANN架构（如TabM、RealMLP、FT-Transformer等）与传统方法（如随机森林、偏最小二乘回归）的性能。

Result: 现代ANN在多数任务中表现优于传统方法，TabPFN表现最稳健。

Conclusion: 推荐将现代ANN（尤其是TabPFN）作为田间尺度PSM的新标准工具。

Abstract: In the field of pedometrics, tabular machine learning is the predominant
method for predicting soil properties from remote and proximal soil sensing
data, forming a central component of digital soil mapping. At the field-scale,
this predictive soil modeling (PSM) task is typically constrained by small
training sample sizes and high feature-to-sample ratios in soil spectroscopy.
Traditionally, these conditions have proven challenging for conventional deep
learning methods. Classical machine learning algorithms, particularly
tree-based models like Random Forest and linear models such as Partial Least
Squares Regression, have long been the default choice for field-scale PSM.
Recent advances in artificial neural networks (ANN) for tabular data challenge
this view, yet their suitability for field-scale PSM has not been proven. We
introduce a comprehensive benchmark that evaluates state-of-the-art ANN
architectures, including the latest multilayer perceptron (MLP)-based models
(TabM, RealMLP), attention-based transformer variants (FT-Transformer,
ExcelFormer, T2G-Former, AMFormer), retrieval-augmented approaches (TabR,
ModernNCA), and an in-context learning foundation model (TabPFN). Our
evaluation encompasses 31 field- and farm-scale datasets containing 30 to 460
samples and three critical soil properties: soil organic matter or soil organic
carbon, pH, and clay content. Our results reveal that modern ANNs consistently
outperform classical methods on the majority of tasks, demonstrating that deep
learning has matured sufficiently to overcome the long-standing dominance of
classical machine learning for PSM. Notably, TabPFN delivers the strongest
overall performance, showing robustness across varying conditions. We therefore
recommend the adoption of modern ANNs for field-scale PSM and propose TabPFN as
the new default choice in the toolkit of every pedometrician.

</details>


### [269] [Rare anomalies require large datasets: About proving the existence of anomalies](https://arxiv.org/abs/2508.09894)
*Simon Klüttermann,Emmanuel Müller*

Main category: cs.LG

TL;DR: 论文研究了如何确定数据集中是否存在异常，提出了一个基于数据集大小、污染率和算法常数的下界条件。


<details>
  <summary>Details</summary>
Motivation: 异常检测中确认异常存在的基础问题尚未充分研究，本文旨在填补这一空白。

Method: 通过超过三百万次统计测试，分析数据集大小、污染率和算法常数之间的关系。

Result: 发现条件$N \ge \frac{\alpha_{\text{algo}}}{\nu^2}$是确认异常存在所需样本数的下界。

Conclusion: 该条件揭示了异常稀有性对检测可行性的限制。

Abstract: Detecting whether any anomalies exist within a dataset is crucial for
effective anomaly detection, yet it remains surprisingly underexplored in
anomaly detection literature. This paper presents a comprehensive study that
addresses the fundamental question: When can we conclusively determine that
anomalies are present? Through extensive experimentation involving over three
million statistical tests across various anomaly detection tasks and
algorithms, we identify a relationship between the dataset size, contamination
rate, and an algorithm-dependent constant $ \alpha_{\text{algo}} $. Our results
demonstrate that, for an unlabeled dataset of size $ N $ and contamination rate
$ \nu $, the condition $ N \ge \frac{\alpha_{\text{algo}}}{\nu^2} $ represents
a lower bound on the number of samples required to confirm anomaly existence.
This threshold implies a limit to how rare anomalies can be before proving
their existence becomes infeasible.

</details>


### [270] [Beyond Naïve Prompting: Strategies for Improved Zero-shot Context-aided Forecasting with LLMs](https://arxiv.org/abs/2508.09904)
*Arjun Ashok,Andrew Robert Williams,Vincent Zhihao Zheng,Irina Rish,Nicolas Chapados,Étienne Marcotte,Valentina Zantedeschi,Alexandre Drouin*

Main category: cs.LG

TL;DR: 论文提出了四种策略（ReDP、CorDP、IC-DP、RouteDP），以探索大型语言模型（LLMs）在上下文辅助预测任务中的零样本能力，并展示了这些策略在不同模型上的优势。


<details>
  <summary>Details</summary>
Motivation: 现有方法仅通过直接提示利用LLMs进行上下文辅助预测，但其潜力未被充分挖掘。本文旨在填补这一空白。

Method: 提出了四种策略：ReDP（提升可解释性）、CorDP（优化现有预测）、IC-DP（嵌入历史示例）、RouteDP（任务路由优化）。

Result: 在CiK基准测试中，这些策略在不同规模和家族的LLMs上均优于直接提示方法。

Conclusion: 这些策略为基于LLM的上下文辅助预测提供了简单而有效的改进方向。

Abstract: Forecasting in real-world settings requires models to integrate not only
historical data but also relevant contextual information, often available in
textual form. While recent work has shown that large language models (LLMs) can
be effective context-aided forecasters via na\"ive direct prompting, their full
potential remains underexplored. We address this gap with 4 strategies,
providing new insights into the zero-shot capabilities of LLMs in this setting.
ReDP improves interpretability by eliciting explicit reasoning traces, allowing
us to assess the model's reasoning over the context independently from its
forecast accuracy. CorDP leverages LLMs solely to refine existing forecasts
with context, enhancing their applicability in real-world forecasting
pipelines. IC-DP proposes embedding historical examples of context-aided
forecasting tasks in the prompt, substantially improving accuracy even for the
largest models. Finally, RouteDP optimizes resource efficiency by using LLMs to
estimate task difficulty, and routing the most challenging tasks to larger
models. Evaluated on different kinds of context-aided forecasting tasks from
the CiK benchmark, our strategies demonstrate distinct benefits over na\"ive
prompting across LLMs of different sizes and families. These results open the
door to further simple yet effective improvements in LLM-based context-aided
forecasting.

</details>


### [271] [Prototype-Guided Diffusion: Visual Conditioning without External Memory](https://arxiv.org/abs/2508.09922)
*Bilal Faye,Hanane Azzag,Mustapha Lebbah*

Main category: cs.LG

TL;DR: 提出了一种原型扩散模型（PDM），通过将原型学习直接集成到扩散过程中，实现了高效且自适应的视觉条件生成，无需外部存储。


<details>
  <summary>Details</summary>
Motivation: 扩散模型在高质量图像生成中表现出色，但计算成本高，尤其是迭代去噪过程。现有方法如RDM依赖外部存储和检索，成本高且缺乏适应性。

Method: PDM通过对比学习从干净图像特征中构建动态紧凑视觉原型，引导去噪步骤，无需外部存储。

Result: 实验表明，PDM在保持高质量生成的同时，降低了计算和存储开销。

Conclusion: PDM为扩散模型提供了一种可扩展的替代方案，避免了检索方法的缺点。

Abstract: Diffusion models have emerged as a leading framework for high-quality image
generation, offering stable training and strong performance across diverse
domains. However, they remain computationally intensive, particularly during
the iterative denoising process. Latent-space models like Stable Diffusion
alleviate some of this cost by operating in compressed representations, though
at the expense of fine-grained detail. More recent approaches such as
Retrieval-Augmented Diffusion Models (RDM) address efficiency by conditioning
denoising on similar examples retrieved from large external memory banks. While
effective, these methods introduce drawbacks: they require costly storage and
retrieval infrastructure, depend on static vision-language models like CLIP for
similarity, and lack adaptability during training. We propose the Prototype
Diffusion Model (PDM), a method that integrates prototype learning directly
into the diffusion process for efficient and adaptive visual conditioning -
without external memory. Instead of retrieving reference samples, PDM
constructs a dynamic set of compact visual prototypes from clean image features
using contrastive learning. These prototypes guide the denoising steps by
aligning noisy representations with semantically relevant visual patterns,
enabling efficient generation with strong semantic grounding. Experiments show
that PDM maintains high generation quality while reducing computational and
storage overhead, offering a scalable alternative to retrieval-based
conditioning in diffusion models.

</details>


### [272] [Residual Reservoir Memory Networks](https://arxiv.org/abs/2508.09925)
*Matteo Pinna,Andrea Ceni,Claudio Gallicchio*

Main category: cs.LG

TL;DR: 提出了一种新型的Residual Reservoir Memory Networks (ResRMNs)，结合线性记忆库和非线性库，通过残差正交连接增强长期输入传播。实验表明其在时间序列和像素级分类任务中优于传统RC模型。


<details>
  <summary>Details</summary>
Motivation: 传统RC模型在长期输入传播方面存在局限性，ResRMN通过引入残差正交连接改进这一问题。

Method: 结合线性记忆库和非线性库，利用残差正交连接增强输入传播，并通过线性稳定性分析研究状态动态。

Result: 在时间序列和像素级分类任务中表现优于传统RC模型。

Conclusion: ResRMN通过残差正交连接有效提升了长期输入传播能力，实验验证了其优越性。

Abstract: We introduce a novel class of untrained Recurrent Neural Networks (RNNs)
within the Reservoir Computing (RC) paradigm, called Residual Reservoir Memory
Networks (ResRMNs). ResRMN combines a linear memory reservoir with a non-linear
reservoir, where the latter is based on residual orthogonal connections along
the temporal dimension for enhanced long-term propagation of the input. The
resulting reservoir state dynamics are studied through the lens of linear
stability analysis, and we investigate diverse configurations for the temporal
residual connections. The proposed approach is empirically assessed on
time-series and pixel-level 1-D classification tasks. Our experimental results
highlight the advantages of the proposed approach over other conventional RC
models.

</details>


### [273] [Noise Hypernetworks: Amortizing Test-Time Compute in Diffusion Models](https://arxiv.org/abs/2508.09968)
*Luca Eyring,Shyamgopal Karthik,Alexey Dosovitskiy,Nataniel Ruiz,Zeynep Akata*

Main category: cs.LG

TL;DR: 论文提出了一种通过噪声超网络替代奖励引导的测试时噪声优化的方法，以减少推理时的计算开销，同时保留测试时扩展的优势。


<details>
  <summary>Details</summary>
Motivation: 测试时扩展虽然提升了模型性能，但计算时间大幅增加，限制了其实际应用。作者希望保留其优势，同时减少推理开销。

Method: 提出噪声超网络，用于调制初始输入噪声，并通过理论框架学习奖励倾斜的分布，以优化生成器的特性。

Result: 该方法以较低的计算成本恢复了测试时优化的大部分质量提升。

Conclusion: 噪声超网络是一种有效的解决方案，能够在减少计算开销的同时保持测试时扩展的性能增益。

Abstract: The new paradigm of test-time scaling has yielded remarkable breakthroughs in
Large Language Models (LLMs) (e.g. reasoning models) and in generative vision
models, allowing models to allocate additional computation during inference to
effectively tackle increasingly complex problems. Despite the improvements of
this approach, an important limitation emerges: the substantial increase in
computation time makes the process slow and impractical for many applications.
Given the success of this paradigm and its growing usage, we seek to preserve
its benefits while eschewing the inference overhead. In this work we propose
one solution to the critical problem of integrating test-time scaling knowledge
into a model during post-training. Specifically, we replace reward guided
test-time noise optimization in diffusion models with a Noise Hypernetwork that
modulates initial input noise. We propose a theoretically grounded framework
for learning this reward-tilted distribution for distilled generators, through
a tractable noise-space objective that maintains fidelity to the base model
while optimizing for desired characteristics. We show that our approach
recovers a substantial portion of the quality gains from explicit test-time
optimization at a fraction of the computational cost. Code is available at
https://github.com/ExplainableML/HyperNoise

</details>


### [274] [Dynamic Mixture-of-Experts for Incremental Graph Learning](https://arxiv.org/abs/2508.09974)
*Lecheng Kong,Theodore Vasiloudis,Seongjun Yun,Han Xie,Xiang Song*

Main category: cs.LG

TL;DR: 论文提出动态专家混合（DyMoE）方法，解决图增量学习中的灾难性遗忘问题，通过定制正则化损失和稀疏MoE提升性能。


<details>
  <summary>Details</summary>
Motivation: 传统图机器学习方法在增量学习中存在灾难性遗忘问题，且未考虑不同时间点知识对新任务的不同贡献。

Method: 采用动态专家混合（DyMoE）方法，新增专家网络处理新数据块，设计正则化损失保持旧任务能力，并引入稀疏MoE降低计算成本。

Result: 模型在类增量学习任务中相对基线准确率提升4.92%。

Conclusion: DyMoE方法有效解决了图增量学习中的灾难性遗忘问题，显著提升了性能。

Abstract: Graph incremental learning is a learning paradigm that aims to adapt trained
models to continuously incremented graphs and data over time without the need
for retraining on the full dataset. However, regular graph machine learning
methods suffer from catastrophic forgetting when applied to incremental
learning settings, where previously learned knowledge is overridden by new
knowledge. Previous approaches have tried to address this by treating the
previously trained model as an inseparable unit and using techniques to
maintain old behaviors while learning new knowledge. These approaches, however,
do not account for the fact that previously acquired knowledge at different
timestamps contributes differently to learning new tasks. Some prior patterns
can be transferred to help learn new data, while others may deviate from the
new data distribution and be detrimental. To address this, we propose a dynamic
mixture-of-experts (DyMoE) approach for incremental learning. Specifically, a
DyMoE GNN layer adds new expert networks specialized in modeling the incoming
data blocks. We design a customized regularization loss that utilizes data
sequence information so existing experts can maintain their ability to solve
old tasks while helping the new expert learn the new data effectively. As the
number of data blocks grows over time, the computational cost of the full
mixture-of-experts (MoE) model increases. To address this, we introduce a
sparse MoE approach, where only the top-$k$ most relevant experts make
predictions, significantly reducing the computation time. Our model achieved
4.92\% relative accuracy increase compared to the best baselines on class
incremental learning, showing the model's exceptional power.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [275] [Cowpox: Towards the Immunity of VLM-based Multi-Agent Systems](https://arxiv.org/abs/2508.09230)
*Yutong Wu,Jie Zhang,Yiming Li,Chao Zhang,Qing Guo,Nils Lukas,Tianwei Zhang*

Main category: cs.MA

TL;DR: 提出了一种名为Cowpox的新防御方法，用于增强多智能体系统的鲁棒性，通过分布式机制限制感染的传播。


<details>
  <summary>Details</summary>
Motivation: 现有多智能体系统设计缺乏鲁棒性考虑，单个智能体被攻击可能感染整个系统。

Method: 采用分布式机制，生成并分发特殊治愈样本，免疫未受攻击的智能体并帮助已感染的智能体恢复。

Result: 实验证明Cowpox有效，并提供了理论鲁棒性保证。

Conclusion: Cowpox能显著提升多智能体系统的安全性，防止攻击扩散。

Abstract: Vision Language Model (VLM)-based agents are stateful, autonomous entities
capable of perceiving and interacting with their environments through vision
and language. Multi-agent systems comprise specialized agents who collaborate
to solve a (complex) task. A core security property is robustness, stating that
the system should maintain its integrity under adversarial attacks. However,
the design of existing multi-agent systems lacks the robustness consideration,
as a successful exploit against one agent can spread and infect other agents to
undermine the entire system's assurance. To address this, we propose a new
defense approach, Cowpox, to provably enhance the robustness of multi-agent
systems. It incorporates a distributed mechanism, which improves the recovery
rate of agents by limiting the expected number of infections to other agents.
The core idea is to generate and distribute a special cure sample that
immunizes an agent against the attack before exposure and helps recover the
already infected agents. We demonstrate the effectiveness of Cowpox empirically
and provide theoretical robustness guarantees.

</details>


### [276] [Emergence of Hierarchies in Multi-Agent Self-Organizing Systems Pursuing a Joint Objective](https://arxiv.org/abs/2508.09541)
*Gang Chen,Guoxin Wang,Anton van Beek,Zhenjun Ming,Yan Yan*

Main category: cs.MA

TL;DR: 论文研究了多智能体自组织系统（MASOS）中依赖层次结构的动态涌现，通过多智能体强化学习（MARL）分析任务执行中的依赖关系及其演化。


<details>
  <summary>Details</summary>
Motivation: 理解MASOS中依赖层次结构如何从智能体的集体行为中涌现，以及其动态演化和影响因素。

Method: 使用MARL训练MASOS完成协作推箱任务，通过计算智能体动作对其他智能体状态的梯度量化依赖关系，并分析层次结构的涌现。

Result: 依赖层次结构在任务执行中动态涌现，受任务环境和网络初始化条件影响，且由智能体的“天赋”和“努力”动态交互决定。

Conclusion: 依赖层次结构是MASOS中自组织行为的自然结果，而非预设规则或参数的产物，其涌现受环境和智能体动态交互影响。

Abstract: Multi-agent self-organizing systems (MASOS) exhibit key characteristics
including scalability, adaptability, flexibility, and robustness, which have
contributed to their extensive application across various fields. However, the
self-organizing nature of MASOS also introduces elements of unpredictability in
their emergent behaviors. This paper focuses on the emergence of dependency
hierarchies during task execution, aiming to understand how such hierarchies
arise from agents' collective pursuit of the joint objective, how they evolve
dynamically, and what factors govern their development. To investigate this
phenomenon, multi-agent reinforcement learning (MARL) is employed to train
MASOS for a collaborative box-pushing task. By calculating the gradients of
each agent's actions in relation to the states of other agents, the inter-agent
dependencies are quantified, and the emergence of hierarchies is analyzed
through the aggregation of these dependencies. Our results demonstrate that
hierarchies emerge dynamically as agents work towards a joint objective, with
these hierarchies evolving in response to changing task requirements. Notably,
these dependency hierarchies emerge organically in response to the shared
objective, rather than being a consequence of pre-configured rules or
parameters that can be fine-tuned to achieve specific results. Furthermore, the
emergence of hierarchies is influenced by the task environment and network
initialization conditions. Additionally, hierarchies in MASOS emerge from the
dynamic interplay between agents' "Talent" and "Effort" within the
"Environment." "Talent" determines an agent's initial influence on collective
decision-making, while continuous "Effort" within the "Environment" enables
agents to shift their roles and positions within the system.

</details>


### [277] [Extending the OWASP Multi-Agentic System Threat Modeling Guide: Insights from Multi-Agent Security Research](https://arxiv.org/abs/2508.09815)
*Klaudia Krawiecka,Christian Schroeder de Witt*

Main category: cs.MA

TL;DR: 扩展OWASP多代理系统（MAS）威胁建模指南，填补现有模型在LLM驱动的多代理架构中的安全漏洞，并提出新的威胁类别和评估策略。


<details>
  <summary>Details</summary>
Motivation: 现有OWASP威胁模型在多代理系统（尤其是LLM驱动的架构）中存在漏洞，需扩展以应对新挑战。

Method: 分析现有模型的不足，提出新的威胁类别（如推理链崩溃、度量过拟合等），并设计评估策略（如鲁棒性测试、协调评估等）。

Result: 扩展了OWASP框架，使其适用于更复杂、自主的多代理系统，提升实际部署中的安全性和韧性。

Conclusion: 通过填补漏洞和提出新方法，增强了多代理系统的安全建模和评估能力。

Abstract: We propose an extension to the OWASP Multi-Agentic System (MAS) Threat
Modeling Guide, translating recent anticipatory research in multi-agent
security (MASEC) into practical guidance for addressing challenges unique to
large language model (LLM)-driven multi-agent architectures. Although OWASP's
existing taxonomy covers many attack vectors, our analysis identifies gaps in
modeling failures, including, but not limited to: reasoning collapse across
planner-executor chains, metric overfitting, unsafe delegation escalation,
emergent covert coordination, and heterogeneous multi-agent exploits. We
introduce additional threat classes and scenarios grounded in practical MAS
deployments, highlighting risks from benign goal drift, cross-agent
hallucination propagation, affective prompt framing, and multi-agent backdoors.
We also outline evaluation strategies, including robustness testing,
coordination assessment, safety enforcement, and emergent behavior monitoring,
to ensure complete coverage. This work complements the framework of OWASP by
expanding its applicability to increasingly complex, autonomous, and adaptive
multi-agent systems, with the goal of improving security posture and resilience
in real world deployments.

</details>


<div id='cs.MM'></div>

# cs.MM [[Back]](#toc)

### [278] [PETLP: A Privacy-by-Design Pipeline for Social Media Data in AI Research](https://arxiv.org/abs/2508.09232)
*Nick Oh,Giorgos D. Vrakas,Siân J. M. Brooke,Sasha Morinière,Toju Duke*

Main category: cs.MM

TL;DR: PETLP框架整合GDPR、版权法和平台条款，为社交媒体数据研究提供统一合规指南。


<details>
  <summary>Details</summary>
Motivation: 现有框架未能整合不同法规领域，导致研究人员缺乏统一指导。

Method: 提出PETLP框架，将法律保障嵌入ETL流程，动态更新数据保护影响评估。

Result: 通过Reddit分析，揭示研究机构与商业实体在数据提取权利上的差异，以及GDPR的普遍适用性。

Conclusion: PETLP通过结构化合规决策，帮助研究人员应对法规复杂性，弥合法律要求与研究实践的差距。

Abstract: Social media data presents AI researchers with overlapping obligations under
the GDPR, copyright law, and platform terms -- yet existing frameworks fail to
integrate these regulatory domains, leaving researchers without unified
guidance. We introduce PETLP (Privacy-by-design Extract, Transform, Load, and
Present), a compliance framework that embeds legal safeguards directly into
extended ETL pipelines. Central to PETLP is treating Data Protection Impact
Assessments as living documents that evolve from pre-registration through
dissemination. Through systematic Reddit analysis, we demonstrate how
extraction rights fundamentally differ between qualifying research
organisations (who can invoke DSM Article 3 to override platform restrictions)
and commercial entities (bound by terms of service), whilst GDPR obligations
apply universally. We reveal why true anonymisation remains unachievable for
social media data and expose the legal gap between permitted dataset creation
and uncertain model distribution. By structuring compliance decisions into
practical workflows and simplifying institutional data management plans, PETLP
enables researchers to navigate regulatory complexity with confidence, bridging
the gap between legal requirements and research practice.

</details>


### [279] [AI Blob! LLM-Driven Recontextualization of Italian Television Archives](https://arxiv.org/abs/2508.09535)
*Roberto Balestri*

Main category: cs.MM

TL;DR: AI Blob! 是一个实验性系统，利用语义编目和大型语言模型（LLMs）检索和重新语境化电视档案素材。


<details>
  <summary>Details</summary>
Motivation: 探索语义技术和LLMs在档案电视素材检索与重构中的潜力，推动媒体史学与AI驱动档案研究的发展。

Method: 结合自动语音识别（ASR）、语义嵌入和检索增强生成（RAG），对意大利电视视频进行转录、分段和语义查询。

Result: 系统能根据用户主题提示生成相关查询，检索并重组视听片段，形成具有讽刺性并置和主题连贯性的叙事序列。

Conclusion: AI Blob! 展示了语义技术如何推动档案互动的新方法，支持自动化叙事构建和文化分析，为跨学科研究提供了框架和数据集。

Abstract: This paper introduces AI Blob!, an experimental system designed to explore
the potential of semantic cataloging and Large Language Models (LLMs) for the
retrieval and recontextualization of archival television footage. Drawing
methodological inspiration from Italian television programs such as Blob (RAI
Tre, 1989-), AI Blob! integrates automatic speech recognition (ASR), semantic
embeddings, and retrieval-augmented generation (RAG) to organize and
reinterpret archival content. The system processes a curated dataset of 1,547
Italian television videos by transcribing audio, segmenting it into
sentence-level units, and embedding these segments into a vector database for
semantic querying. Upon user input of a thematic prompt, the LLM generates a
range of linguistically and conceptually related queries, guiding the retrieval
and recombination of audiovisual fragments. These fragments are algorithmically
selected and structured into narrative sequences producing montages that
emulate editorial practices of ironic juxtaposition and thematic coherence. By
foregrounding dynamic, content-aware retrieval over static metadata schemas, AI
Blob! demonstrates how semantic technologies can facilitate new approaches to
archival engagement, enabling novel forms of automated narrative construction
and cultural analysis. The project contributes to ongoing debates in media
historiography and AI-driven archival research, offering both a conceptual
framework and a publicly available dataset to support further interdisciplinary
experimentation.

</details>


### [280] [In-place Double Stimulus Methodology for Subjective Assessment of High Quality Images](https://arxiv.org/abs/2508.09777)
*Shima Mohammadi,Mohsen Jenadeleh,Michela Testolina,Jon Sneyers,Touradj Ebrahimi,Dietmar Saupe,João Ascenso*

Main category: cs.MM

TL;DR: 提出了一种新的双刺激主观评估方法（IDSQS），用于高质量图像的评估，解决了现有协议在检测细微感知差异上的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有协议难以检测高质量图像中的细微感知差异，需要更直观的方法。

Method: 采用同位置双刺激质量量表（IDSQS），让受试者在同一位置交替查看参考图像和失真图像。

Result: 通过大规模众包研究验证了IDSQS方法的有效性，并建立了公开数据集。

Conclusion: IDSQS方法能有效评估高质量图像的主观质量，数据集和工具已公开。

Abstract: This paper introduces a novel double stimulus subjective assessment
methodology for the evaluation of high quality images to address the
limitations of existing protocols in detecting subtle perceptual differences.
The In-place Double Stimulus Quality Scale (IDSQS) allows subjects to
alternately view a reference and a distorted image at the same spatial
location, facilitating a more intuitive detection of differences in quality,
especially at high to visually lossless quality levels. A large-scale
crowdsourcing study employing this methodology was conducted, generating a
comprehensive public dataset to evaluate perceived image quality across several
compression algorithms and distortion levels. An additional contribution is the
modeling of quality scores using a Beta distribution, allowing for the
assessment of variability and subject consistency. Our findings demonstrate the
effectiveness of the IDSQS methodology in achieving high correlation with more
precise subjective evaluation benchmarks. The dataset, subjective data, and
graphical user interface developed for this study are publicly available at
https://github.com/shimamohammadi/IDSQS

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [281] [Decision-Making-Based Path Planning for Autonomous UAVs: A Survey](https://arxiv.org/abs/2508.09304)
*Kelen C. Teixeira Vivaldini,Robert Pěnička,Martin Saska*

Main category: cs.RO

TL;DR: 综述探讨了自主无人机路径规划中的决策方法，重点介绍了探索路径规划和信息路径规划的研究方向，并总结了数据建模与理解的特点及当前挑战。


<details>
  <summary>Details</summary>
Motivation: 自主无人机需基于环境信息做出决策以应对系统与环境的不确定性，决策能力是其成功运行的关键。

Method: 综述了现有研究中决策在路径规划中的应用，包括探索路径规划和信息路径规划，并分析了数据建模与理解的特点。

Result: 总结了决策在路径规划中的重要性，并梳理了相关研究方向。

Conclusion: 指出了该领域当前面临的挑战，为未来研究提供了方向。

Abstract: One of the most critical features for the successful operation of autonomous
UAVs is the ability to make decisions based on the information acquired from
their surroundings. Each UAV must be able to make decisions during the flight
in order to deal with uncertainties in its system and the environment, and to
further act upon the information being received. Such decisions influence the
future behavior of the UAV, which is expressed as the path plan. Thus,
decision-making in path planning is an enabling technique for deploying
autonomous UAVs in real-world applications. This survey provides an overview of
existing studies that use aspects of decision-making in path planning,
presenting the research strands for Exploration Path Planning and Informative
Path Planning, and focusing on characteristics of how data have been modeled
and understood. Finally, we highlight the existing challenges for relevant
topics in this field.

</details>


### [282] [How Safe Will I Be Given What I Saw? Calibrated Prediction of Safety Chances for Image-Controlled Autonomy](https://arxiv.org/abs/2508.09346)
*Zhenjiang Mao,Mrinall Eashaan Umasudhan,Ivan Ruchkin*

Main category: cs.RO

TL;DR: 提出了一种用于端到端视觉控制系统的校准安全预测框架，解决了部分可观测性和分布偏移下的安全预测问题。


<details>
  <summary>Details</summary>
Motivation: 传统基于模型的验证方法在可扩展性和低维状态模型访问上受限，而无模型方法缺乏可靠性保证。

Method: 利用变分自编码器和循环预测器从原始图像序列预测潜在轨迹，并估计满足安全属性的概率，结合无监督域适应确保分布偏移下的鲁棒性。

Result: 在三个基准测试中，UDA-equipped评估器在分布偏移下保持高准确性和低误报率，复合预测器在长时任务中表现优于单一预测器。

Conclusion: 该框架通过理论校准保证和实际评估，为高维观测下的长时安全预测提供了可靠解决方案。

Abstract: Autonomous robots that rely on deep neural network controllers pose critical
challenges for safety prediction, especially under partial observability and
distribution shift. Traditional model-based verification techniques are limited
in scalability and require access to low-dimensional state models, while
model-free methods often lack reliability guarantees. This paper addresses
these limitations by introducing a framework for calibrated safety prediction
in end-to-end vision-controlled systems, where neither the state-transition
model nor the observation model is accessible. Building on the foundation of
world models, we leverage variational autoencoders and recurrent predictors to
forecast future latent trajectories from raw image sequences and estimate the
probability of satisfying safety properties. We distinguish between monolithic
and composite prediction pipelines and introduce a calibration mechanism to
quantify prediction confidence. In long-horizon predictions from
high-dimensional observations, the forecasted inputs to the safety evaluator
can deviate significantly from the training distribution due to compounding
prediction errors and changing environmental conditions, leading to
miscalibrated risk estimates. To address this, we incorporate unsupervised
domain adaptation to ensure robustness of safety evaluation under distribution
shift in predictions without requiring manual labels. Our formulation provides
theoretical calibration guarantees and supports practical evaluation across
long prediction horizons. Experimental results on three benchmarks show that
our UDA-equipped evaluators maintain high accuracy and substantially lower
false positive rates under distribution shift. Similarly, world model-based
composite predictors outperform their monolithic counterparts on long-horizon
tasks, and our conformal calibration provides reliable statistical bounds.

</details>


### [283] [CLF-RL: Control Lyapunov Function Guided Reinforcement Learning](https://arxiv.org/abs/2508.09354)
*Kejun Li,Zachary Olkin,Yisong Yue,Aaron D. Ames*

Main category: cs.RO

TL;DR: 提出了一种基于模型轨迹生成和控制Lyapunov函数（CLF）的结构化奖励框架，用于强化学习（RL）中双足机器人运动策略的优化。


<details>
  <summary>Details</summary>
Motivation: 传统RL在双足机器人运动策略生成中常因奖励设计复杂和目标函数敏感性问题表现不佳，需改进。

Method: 结合线性倒立摆（LIP）模型和混合零动力学（HZD）预计算步态库生成参考轨迹，并基于CLF设计奖励函数。

Result: 在仿真和真实机器人（Unitree G1）实验中，CLF-RL比基线RL策略更鲁棒，优于传统跟踪奖励RL。

Conclusion: 结构化奖励框架有效提升了RL策略的鲁棒性和性能，且部署时策略轻量。

Abstract: Reinforcement learning (RL) has shown promise in generating robust locomotion
policies for bipedal robots, but often suffers from tedious reward design and
sensitivity to poorly shaped objectives. In this work, we propose a structured
reward shaping framework that leverages model-based trajectory generation and
control Lyapunov functions (CLFs) to guide policy learning. We explore two
model-based planners for generating reference trajectories: a reduced-order
linear inverted pendulum (LIP) model for velocity-conditioned motion planning,
and a precomputed gait library based on hybrid zero dynamics (HZD) using
full-order dynamics. These planners define desired end-effector and joint
trajectories, which are used to construct CLF-based rewards that penalize
tracking error and encourage rapid convergence. This formulation provides
meaningful intermediate rewards, and is straightforward to implement once a
reference is available. Both the reference trajectories and CLF shaping are
used only during training, resulting in a lightweight policy at deployment. We
validate our method both in simulation and through extensive real-world
experiments on a Unitree G1 robot. CLF-RL demonstrates significantly improved
robustness relative to the baseline RL policy and better performance than a
classic tracking reward RL formulation.

</details>


### [284] [DAgger Diffusion Navigation: DAgger Boosted Diffusion Policy for Vision-Language Navigation](https://arxiv.org/abs/2508.09444)
*Haoxiang Shi,Xiang Deng,Zaijing Li,Gongwei Chen,Yaowei Wang,Liqiang Nie*

Main category: cs.RO

TL;DR: 论文提出了一种名为DifNav的端到端优化方法，用于解决连续环境中的视觉语言导航问题，通过统一传统的两阶段框架（路径点生成与规划）为一个扩散策略，显著提升了导航性能。


<details>
  <summary>Details</summary>
Motivation: 现有两阶段路径点规划框架存在全局次优化和对第一阶段路径点质量的强依赖问题，限制了导航性能。

Method: 提出DAgger Diffusion Navigation (DifNav)，采用条件扩散策略直接建模连续导航空间中的多模态动作分布，结合DAgger进行在线策略训练和专家轨迹增强。

Result: 实验表明，DifNav在无需路径点预测器的情况下，导航性能显著优于现有两阶段模型。

Conclusion: DifNav通过端到端优化和扩散策略，解决了传统框架的局限性，提升了导航的鲁棒性和性能。

Abstract: Vision-Language Navigation in Continuous Environments (VLN-CE) requires
agents to follow natural language instructions through free-form 3D spaces.
Existing VLN-CE approaches typically use a two-stage waypoint planning
framework, where a high-level waypoint predictor generates the navigable
waypoints, and then a navigation planner suggests the intermediate goals in the
high-level action space. However, this two-stage decomposition framework
suffers from: (1) global sub-optimization due to the proxy objective in each
stage, and (2) a performance bottleneck caused by the strong reliance on the
quality of the first-stage predicted waypoints. To address these limitations,
we propose DAgger Diffusion Navigation (DifNav), an end-to-end optimized VLN-CE
policy that unifies the traditional two stages, i.e. waypoint generation and
planning, into a single diffusion policy. Notably, DifNav employs a conditional
diffusion policy to directly model multi-modal action distributions over future
actions in continuous navigation space, eliminating the need for a waypoint
predictor while enabling the agent to capture multiple possible
instruction-following behaviors. To address the issues of compounding error in
imitation learning and enhance spatial reasoning in long-horizon navigation
tasks, we employ DAgger for online policy training and expert trajectory
augmentation, and use the aggregated data to further fine-tune the policy. This
approach significantly improves the policy's robustness and its ability to
recover from error states. Extensive experiments on benchmark datasets
demonstrate that, even without a waypoint predictor, the proposed method
substantially outperforms previous state-of-the-art two-stage waypoint-based
models in terms of navigation performance. Our code is available at:
https://github.com/Tokishx/DifNav.

</details>


### [285] [Reactive Model Predictive Contouring Control for Robot Manipulators](https://arxiv.org/abs/2508.09502)
*Junheon Yoon,Woo-Jeong Baek,Jaeheung Park*

Main category: cs.RO

TL;DR: 提出了一种基于反应式模型预测轮廓控制（RMPCC）的机器人路径跟踪框架，能在动态环境中以100 Hz频率避开障碍物、奇异点和自碰撞。


<details>
  <summary>Details</summary>
Motivation: 现有路径跟踪方法依赖时间参数化，难以同时处理碰撞和奇异点避免，且在执行规避动作时误差较大。

Method: 通过路径参数化参考路径，并利用RMPCC进行优化，引入控制屏障函数（CBFs）避免碰撞和奇异点，采用雅可比线性化和高斯-牛顿Hessian近似实现高效求解。

Result: 实验表明，该方法在动态环境中表现优异，轮廓误差和机器人加速度均较低，计算速度比现有方法快10倍。

Conclusion: 该框架在动态环境中高效且稳定，适用于实时机器人路径跟踪任务。

Abstract: This contribution presents a robot path-following framework via Reactive
Model Predictive Contouring Control (RMPCC) that successfully avoids obstacles,
singularities and self-collisions in dynamic environments at 100 Hz. Many
path-following methods rely on the time parametrization, but struggle to handle
collision and singularity avoidance while adhering kinematic limits or other
constraints. Specifically, the error between the desired path and the actual
position can become large when executing evasive maneuvers. Thus, this paper
derives a method that parametrizes the reference path by a path parameter and
performs the optimization via RMPCC. In particular, Control Barrier Functions
(CBFs) are introduced to avoid collisions and singularities in dynamic
environments. A Jacobian-based linearization and Gauss-Newton Hessian
approximation enable solving the nonlinear RMPCC problem at 100 Hz,
outperforming state-of-the-art methods by a factor of 10. Experiments confirm
that the framework handles dynamic obstacles in real-world settings with low
contouring error and low robot acceleration.

</details>


### [286] [SMART-OC: A Real-time Time-risk Optimal Replanning Algorithm for Dynamic Obstacles and Spatio-temporally Varying Currents](https://arxiv.org/abs/2508.09508)
*Reema Raval,Shalabh Gupta*

Main category: cs.RO

TL;DR: 提出了一种名为SMART-OC的新算法，用于无人水面车辆（USV）在动态海洋环境中的实时路径规划，以避障并利用洋流。


<details>
  <summary>Details</summary>
Motivation: 海洋环境复杂多变，USV需要实时调整路径以避障并利用洋流高效导航。

Method: SMART-OC算法整合路径上的障碍风险和到达目标的时间成本，实现时间-风险最优路径规划。

Result: 仿真实验验证了SMART-OC的有效性，USV能快速重新规划路径以避障并利用洋流。

Conclusion: SMART-OC为USV在动态环境中提供了高效的实时路径规划解决方案。

Abstract: Typical marine environments are highly complex with spatio-temporally varying
currents and dynamic obstacles, presenting significant challenges to Unmanned
Surface Vehicles (USVs) for safe and efficient navigation. Thus, the USVs need
to continuously adapt their paths with real-time information to avoid
collisions and follow the path of least resistance to the goal via exploiting
ocean currents. In this regard, we introduce a novel algorithm, called
Self-Morphing Adaptive Replanning Tree for dynamic Obstacles and Currents
(SMART-OC), that facilitates real-time time-risk optimal replanning in dynamic
environments. SMART-OC integrates the obstacle risks along a path with the time
cost to reach the goal to find the time-risk optimal path. The effectiveness of
SMART-OC is validated by simulation experiments, which demonstrate that the USV
performs fast replannings to avoid dynamic obstacles and exploit ocean currents
to successfully reach the goal.

</details>


### [287] [CaRoBio: 3D Cable Routing with a Bio-inspired Gripper Fingernail](https://arxiv.org/abs/2508.09558)
*Jiahui Zuo,Boyang Zhang,Fumin Zhang*

Main category: cs.RO

TL;DR: 论文提出了一种受鹰爪启发的指甲设计，用于机器人夹爪，以改进电缆的抓取和引导操作，并开发了一个端到端的3D电缆布线框架，显著优于传统的拾取放置策略。


<details>
  <summary>Details</summary>
Motivation: 电缆布线是工业中复杂的多阶段机器人操作任务，传统夹爪易导致电缆过压或过张，需要更高效的解决方案。

Method: 设计了一种新型鹰爪指甲，安装在夹爪上，结合视觉状态估计和离线轨迹规划，实现连续控制。

Result: 提出的框架在多种电缆和槽道测试中表现优异，显著优于传统拾取放置方法。

Conclusion: 该可重构任务设置和框架为未来3D空间电缆布线提供了参考。

Abstract: The manipulation of deformable linear flexures has a wide range of
applications in industry, such as cable routing in automotive manufacturing and
textile production. Cable routing, as a complex multi-stage robot manipulation
scenario, is a challenging task for robot automation. Common parallel
two-finger grippers have the risk of over-squeezing and over-tension when
grasping and guiding cables. In this paper, a novel eagle-inspired fingernail
is designed and mounted on the gripper fingers, which helps with cable grasping
on planar surfaces and in-hand cable guiding operations. Then we present a
single-grasp end-to-end 3D cable routing framework utilizing the proposed
fingernails, instead of the common pick-and-place strategy. Continuous control
is achieved to efficiently manipulate cables through vision-based state
estimation of task configurations and offline trajectory planning based on
motion primitives. We evaluate the effectiveness of the proposed framework with
a variety of cables and channel slots, significantly outperforming the
pick-and-place manipulation process under equivalent perceptual conditions. Our
reconfigurable task setting and the proposed framework provide a reference for
future cable routing manipulations in 3D space.

</details>


### [288] [ESCoT: An Enhanced Step-based Coordinate Trajectory Planning Method for Multiple Car-like Robots](https://arxiv.org/abs/2508.09581)
*Junkai Jiang,Yihe Chen,Yibin Yang,Ruochen Li,Shaobing Xu,Jianqiang Wang*

Main category: cs.RO

TL;DR: ESCoT是一种改进的多车轨迹规划方法，通过协作规划和重规划策略提升性能，在稀疏和密集场景中均优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 多车轨迹规划（MVTP）是多机器人系统的关键挑战，应用广泛。现有步进方法在性能和适用性上存在不足，需改进。

Method: ESCoT采用协作规划和重规划策略，优化步进方法的性能。

Result: 在稀疏场景中，ESCoT显著提升解决方案质量（最高70%），在密集场景中保持50%以上的成功率。

Conclusion: ESCoT有效解决了MVTP问题，扩展了步进方法的能力，并通过实际机器人测试验证了实用性。

Abstract: Multi-vehicle trajectory planning (MVTP) is one of the key challenges in
multi-robot systems (MRSs) and has broad applications across various fields.
This paper presents ESCoT, an enhanced step-based coordinate trajectory
planning method for multiple car-like robots. ESCoT incorporates two key
strategies: collaborative planning for local robot groups and replanning for
duplicate configurations. These strategies effectively enhance the performance
of step-based MVTP methods. Through extensive experiments, we show that ESCoT
1) in sparse scenarios, significantly improves solution quality compared to
baseline step-based method, achieving up to 70% improvement in typical conflict
scenarios and 34% in randomly generated scenarios, while maintaining high
solving efficiency; and 2) in dense scenarios, outperforms all baseline
methods, maintains a success rate of over 50% even in the most challenging
configurations. The results demonstrate that ESCoT effectively solves MVTP,
further extending the capabilities of step-based methods. Finally, practical
robot tests validate the algorithm's applicability in real-world scenarios.

</details>


### [289] [HapticGiant: A Novel Very Large Kinesthetic Haptic Interface with Hierarchical Force Control](https://arxiv.org/abs/2508.09595)
*Michael Fennel,Markus Walker,Dominik Pikos,Uwe D. Hanebeck*

Main category: cs.RO

TL;DR: HapticGiant是一种新型的大规模动觉触觉接口，旨在匹配人类手臂特性并提供自然用户运动与完整触觉反馈。


<details>
  <summary>Details</summary>
Motivation: 现有动觉触觉接口存在工作空间有限、自由度不足及与人类手臂运动学不匹配的问题。

Method: 采用新型导纳型力控制方案，利用分层优化实现任意串行运动链和笛卡尔导纳的渲染。

Result: 实验证明HapticGiant及其控制方案有效，为高度沉浸式虚拟现实应用铺平道路。

Conclusion: HapticGiant通过匹配人类手臂特性和解决系统限制，显著提升了虚拟现实的触觉沉浸感。

Abstract: Research in virtual reality and haptic technologies has consistently aimed to
enhance immersion. While advanced head-mounted displays are now commercially
available, kinesthetic haptic interfaces still face challenges such as limited
workspaces, insufficient degrees of freedom, and kinematics not matching the
human arm. In this paper, we present HapticGiant, a novel large-scale
kinesthetic haptic interface designed to match the properties of the human arm
as closely as possible and to facilitate natural user locomotion while
providing full haptic feedback. The interface incorporates a novel
admittance-type force control scheme, leveraging hierarchical optimization to
render both arbitrary serial kinematic chains and Cartesian admittances.
Notably, the proposed control scheme natively accounts for system limitations,
including joint and Cartesian constraints, as well as singularities.
Experimental results demonstrate the effectiveness of HapticGiant and its
control scheme, paving the way for highly immersive virtual reality
applications.

</details>


### [290] [BEAVR: Bimanual, multi-Embodiment, Accessible, Virtual Reality Teleoperation System for Robots](https://arxiv.org/abs/2508.09606)
*Alejandro Posadas-Nava,Alejandro Carrasco,Richard Linares*

Main category: cs.RO

TL;DR: BEAVR是一个开源的、双手操作的、多体现的VR远程操作系统，用于机器人，支持实时控制、数据记录和策略学习。


<details>
  <summary>Details</summary>
Motivation: 设计一个统一的系统，支持异构机器人平台的实时控制、数据记录和策略学习。

Method: 采用零拷贝流架构（延迟≤35ms）、异步“思考-行动”控制循环和灵活的网络API。

Result: 在多样化操作任务中表现优异，兼容主流视觉运动策略（如ACT、DiffusionPolicy、SmolVLA）。

Conclusion: BEAVR是一个高效、兼容性强的VR远程操作系统，代码和数据集已开源。

Abstract: \textbf{BEAVR} is an open-source, bimanual, multi-embodiment Virtual Reality
(VR) teleoperation system for robots, designed to unify real-time control, data
recording, and policy learning across heterogeneous robotic platforms. BEAVR
enables real-time, dexterous teleoperation using commodity VR hardware,
supports modular integration with robots ranging from 7-DoF manipulators to
full-body humanoids, and records synchronized multi-modal demonstrations
directly in the LeRobot dataset schema. Our system features a zero-copy
streaming architecture achieving $\leq$35\,ms latency, an asynchronous
``think--act'' control loop for scalable inference, and a flexible network API
optimized for real-time, multi-robot operation. We benchmark BEAVR across
diverse manipulation tasks and demonstrate its compatibility with leading
visuomotor policies such as ACT, DiffusionPolicy, and SmolVLA. All code is
publicly available, and datasets are released on Hugging Face\footnote{Code,
datasets, and VR app available at https://github.com/ARCLab-MIT/BEAVR-Bot.

</details>


### [291] [QuickGrasp: Lightweight Antipodal Grasp Planning with Point Clouds](https://arxiv.org/abs/2504.19716)
*Navin Sriram Ravie,Keerthi Vasan M,Asokan Thondiyath,Bijo Sebastian*

Main category: cs.RO

TL;DR: 本文提出了一种轻量级的分析方法用于机器人抓取规划，特别是对握式抓取，减少了六自由度空间的采样需求。


<details>
  <summary>Details</summary>
Motivation: 随着环境和任务复杂化，现有抓取规划方法因泛化能力差、耗时和缺乏可重复性而难以适应实际需求。

Method: 将抓取规划问题转化为优化问题，通过软区域生长算法进行平面分割，并使用基于优化的质量度量评估抓取点。

Result: 与现有方法GPD相比，所提方法在模拟和真实环境中均表现出更高的有效性。

Conclusion: 该方法在减少采样需求的同时，提高了抓取规划的效率和可重复性，适用于实际任务。

Abstract: Grasping has been a long-standing challenge in facilitating the final
interface between a robot and the environment. As environments and tasks become
complicated, the need to embed higher intelligence to infer from the
surroundings and act on them has become necessary. Although most methods
utilize techniques to estimate grasp pose by treating the problem via pure
sampling-based approaches in the six-degree-of-freedom space or as a learning
problem, they usually fail in real-life settings owing to poor generalization
across domains. In addition, the time taken to generate the grasp plan and the
lack of repeatability, owing to sampling inefficiency and the probabilistic
nature of existing grasp planning approaches, severely limits their application
in real-world tasks. This paper presents a lightweight analytical approach
towards robotic grasp planning, particularly antipodal grasps, with little to
no sampling in the six-degree-of-freedom space. The proposed grasp planning
algorithm is formulated as an optimization problem towards estimating grasp
points on the object surface instead of directly estimating the end-effector
pose. To this extent, a soft-region-growing algorithm is presented for
effective plane segmentation, even in the case of curved surfaces. An
optimization-based quality metric is then used for the evaluation of grasp
points to ensure indirect force closure. The proposed grasp framework is
compared with the existing state-of-the-art grasp planning approach, Grasp pose
detection (GPD), as a baseline over multiple simulated objects. The
effectiveness of the proposed approach in comparison to GPD is also evaluated
in a real-world setting using image and point-cloud data, with the planned
grasps being executed using a ROBOTIQ gripper and UR5 manipulator.

</details>


### [292] [Interpretable Robot Control via Structured Behavior Trees and Large Language Models](https://arxiv.org/abs/2508.09621)
*Ingrid Maéva Chekam,Ines Pastor-Martinez,Ali Tourani,Jose Andres Millan-Romera,Laura Ribeiro,Pedro Miguel Bastos Soares,Holger Voos,Jose Luis Sanchez-Lopez*

Main category: cs.RO

TL;DR: 论文提出了一种结合大型语言模型（LLMs）和行为树的新框架，用于实现自然语言指令到机器人动作的转换，提升了人机交互的直观性和可靠性。


<details>
  <summary>Details</summary>
Motivation: 智能机器人在人类环境中的集成需要更自然、适应性更强的人机交互界面，传统方法限制了动态环境中的可用性。

Method: 通过结合LLMs和行为树，系统将自然语言指令转化为可执行动作，支持模块化集成和感知功能。

Result: 实验结果显示，系统在真实场景中的认知到执行准确率约为94%。

Conclusion: 该框架为HRI系统和机器人提供了实用且可扩展的解决方案，代码已开源。

Abstract: As intelligent robots become more integrated into human environments, there
is a growing need for intuitive and reliable Human-Robot Interaction (HRI)
interfaces that are adaptable and more natural to interact with. Traditional
robot control methods often require users to adapt to interfaces or memorize
predefined commands, limiting usability in dynamic, unstructured environments.
This paper presents a novel framework that bridges natural language
understanding and robotic execution by combining Large Language Models (LLMs)
with Behavior Trees. This integration enables robots to interpret natural
language instructions given by users and translate them into executable actions
by activating domain-specific plugins. The system supports scalable and modular
integration, with a primary focus on perception-based functionalities, such as
person tracking and hand gesture recognition. To evaluate the system, a series
of real-world experiments was conducted across diverse environments.
Experimental results demonstrate that the proposed approach is practical in
real-world scenarios, with an average cognition-to-execution accuracy of
approximately 94%, making a significant contribution to HRI systems and robots.
The complete source code of the framework is publicly available at
https://github.com/snt-arg/robot_suite.

</details>


### [293] [Immersive Teleoperation of Beyond-Human-Scale Robotic Manipulators: Challenges and Future Directions](https://arxiv.org/abs/2508.09700)
*Mahdi Hejrati,Jouni Mattila*

Main category: cs.RO

TL;DR: 本文探讨了超大型机器人操作器（BHSRMs）的远程操作挑战，重点研究了控制、认知和界面设计问题，并提出了改进安全性和操作体验的方法。


<details>
  <summary>Details</summary>
Motivation: 随着BHSRMs在工业领域的应用增加，需要重新设计沉浸式界面以支持安全、高效的人机协作。

Method: 分析了触觉和视觉反馈系统的设计权衡，并通过实验比较了外骨骼和摇杆控制方案。

Result: 提出了改进操作安全性、减少感知运动不匹配和增强操作者体验的方法。

Conclusion: 未来研究方向包括开发新的评估工具、扩展策略和以人为中心的安全模型。

Abstract: Teleoperation of beyond-human-scale robotic manipulators (BHSRMs) presents
unique challenges that differ fundamentally from conventional human-scale
systems. As these platforms gain relevance in industrial domains such as
construction, mining, and disaster response, immersive interfaces must be
rethought to support scalable, safe, and effective human-robot collaboration.
This paper investigates the control, cognitive, and interface-level challenges
of immersive teleoperation in BHSRMs, with a focus on ensuring operator safety,
minimizing sensorimotor mismatch, and enhancing the sense of embodiment. We
analyze design trade-offs in haptic and visual feedback systems, supported by
early experimental comparisons of exoskeleton- and joystick-based control
setups. Finally, we outline key research directions for developing new
evaluation tools, scaling strategies, and human-centered safety models tailored
to large-scale robotic telepresence.

</details>


### [294] [FLARE: Agile Flights for Quadrotor Cable-Suspended Payload System via Reinforcement Learning](https://arxiv.org/abs/2508.09797)
*Dongcheng Cao,Jin Zhou,Xian Wang,Shuo Li*

Main category: cs.RO

TL;DR: FLARE是一个基于强化学习的框架，用于解决四旋翼悬挂载荷系统的敏捷飞行问题，显著优于传统优化方法，并实现了零样本的仿真到现实迁移。


<details>
  <summary>Details</summary>
Motivation: 四旋翼悬挂载荷系统的动态特性复杂，传统优化方法计算成本高且难以处理电缆模式转换，限制了实时性和机动性。

Method: 采用强化学习框架FLARE，直接从高保真仿真中学习敏捷导航策略。

Result: 在三个挑战性场景中，FLARE比现有优化方法快3倍，且实现了零样本的仿真到现实迁移。

Conclusion: FLARE展示了在实时性和安全性上的显著优势，适用于现实世界的敏捷飞行任务。

Abstract: Agile flight for the quadrotor cable-suspended payload system is a formidable
challenge due to its underactuated, highly nonlinear, and hybrid dynamics.
Traditional optimization-based methods often struggle with high computational
costs and the complexities of cable mode transitions, limiting their real-time
applicability and maneuverability exploitation. In this letter, we present
FLARE, a reinforcement learning (RL) framework that directly learns agile
navigation policy from high-fidelity simulation. Our method is validated across
three designed challenging scenarios, notably outperforming a state-of-the-art
optimization-based approach by a 3x speedup during gate traversal maneuvers.
Furthermore, the learned policies achieve successful zero-shot sim-to-real
transfer, demonstrating remarkable agility and safety in real-world
experiments, running in real time on an onboard computer.

</details>


### [295] [Embodied Tactile Perception of Soft Objects Properties](https://arxiv.org/abs/2508.09836)
*Anirvan Dutta,Alexis WM Devillard,Zhihuan Zhang,Xiaoxiao Cheng,Etienne Burdet*

Main category: cs.RO

TL;DR: 研究探讨了机械顺应性、多模态传感和交互策略如何共同影响机器人触觉感知，提出了一种无监督的深度状态空间模型，证明多模态传感优于单模态传感。


<details>
  <summary>Details</summary>
Motivation: 为了实现机器人像人类一样的精细操作，需要理解机械顺应性、多模态传感和交互策略如何共同塑造触觉感知。

Method: 使用模块化电子皮肤（e-Skin）和软波物体，系统研究传感体现和交互策略对感知的影响，并提出无监督的深度状态空间模型（潜在滤波器）。

Result: 多模态传感优于单模态传感，且环境与电子皮肤机械特性之间存在复杂的相互作用。

Conclusion: 研究强调了结合时间动态分析环境与机械特性的重要性，为机器人触觉感知提供了通用且可解释的表示。

Abstract: To enable robots to develop human-like fine manipulation, it is essential to
understand how mechanical compliance, multi-modal sensing, and purposeful
interaction jointly shape tactile perception. In this study, we use a dedicated
modular e-Skin with tunable mechanical compliance and multi-modal sensing
(normal, shear forces and vibrations) to systematically investigate how sensing
embodiment and interaction strategies influence robotic perception of objects.
Leveraging a curated set of soft wave objects with controlled viscoelastic and
surface properties, we explore a rich set of palpation primitives-pressing,
precession, sliding that vary indentation depth, frequency, and directionality.
In addition, we propose the latent filter, an unsupervised, action-conditioned
deep state-space model of the sophisticated interaction dynamics and infer
causal mechanical properties into a structured latent space. This provides
generalizable and in-depth interpretable representation of how embodiment and
interaction determine and influence perception. Our investigation demonstrates
that multi-modal sensing outperforms uni-modal sensing. It highlights a nuanced
interaction between the environment and mechanical properties of e-Skin, which
should be examined alongside the interaction by incorporating temporal
dynamics.

</details>


### [296] [Whole-Body Bilateral Teleoperation with Multi-Stage Object Parameter Estimation for Wheeled Humanoid Locomanipulation](https://arxiv.org/abs/2508.09846)
*Donghoon Baek,Amartya Purushottam,Jason J. Choi,Joao Ramos*

Main category: cs.RO

TL;DR: 提出了一种面向轮式人形机器人的物体感知全身双边遥操作框架，结合在线多阶段物体惯性参数估计模块，提升操作动态性和效率。


<details>
  <summary>Details</summary>
Motivation: 解决轮式人形机器人在搬运任务中因物体惯性参数未知导致的动态同步和操作精度问题。

Method: 通过多阶段估计（视觉尺寸估计、大视觉语言模型初始猜测、分层采样策略）实时更新物体惯性参数，并用于机器人控制。

Result: 框架在硬件平台上实现了实时执行搬运任务，负载达机器人重量的三分之一。

Conclusion: 该框架显著提升了动态遥操作和物体搬运的性能，同时保持了柔顺行为。

Abstract: This paper presents an object-aware whole-body bilateral teleoperation
framework for wheeled humanoid loco-manipulation. This framework combines
whole-body bilateral teleoperation with an online multi-stage object inertial
parameter estimation module, which is the core technical contribution of this
work. The multi-stage process sequentially integrates a vision-based object
size estimator, an initial parameter guess generated by a large vision-language
model (VLM), and a decoupled hierarchical sampling strategy. The visual size
estimate and VLM prior offer a strong initial guess of the object's inertial
parameters, significantly reducing the search space for sampling-based
refinement and improving the overall estimation speed. A hierarchical strategy
first estimates mass and center of mass, then infers inertia from object size
to ensure physically feasible parameters, while a decoupled multi-hypothesis
scheme enhances robustness to VLM prior errors. Our estimator operates in
parallel with high-fidelity simulation and hardware, enabling real-time online
updates. The estimated parameters are then used to update the wheeled
humanoid's equilibrium point, allowing the operator to focus more on locomotion
and manipulation. This integration improves the haptic force feedback for
dynamic synchronization, enabling more dynamic whole-body teleoperation. By
compensating for object dynamics using the estimated parameters, the framework
also improves manipulation tracking while preserving compliant behavior. We
validate the system on a customized wheeled humanoid with a robotic gripper and
human-machine interface, demonstrating real-time execution of lifting,
delivering, and releasing tasks with a payload weighing approximately one-third
of the robot's body weight.

</details>


### [297] [Toward Human-Robot Teaming: Learning Handover Behaviors from 3D Scenes](https://arxiv.org/abs/2508.09855)
*Yuekun Wu,Yik Lung Pang,Andrea Cavallaro,Changjae Oh*

Main category: cs.RO

TL;DR: 提出一种仅通过RGB图像训练人机交接策略的方法，无需真实机器人数据，利用稀疏视图高斯泼溅重建生成演示。


<details>
  <summary>Details</summary>
Motivation: 解决人机协作中真实机器人训练成本高及仿真与真实视觉差距的问题。

Method: 利用稀疏视图高斯泼溅重建场景生成图像-动作对，直接模拟相机位姿变化为夹爪位姿变化。

Result: 在重建场景和真实人机交接实验中验证了方法的有效性和鲁棒性。

Conclusion: 该方法为人机交接任务提供了新表示，提升了人机协作的流畅性和鲁棒性。

Abstract: Human-robot teaming (HRT) systems often rely on large-scale datasets of human
and robot interactions, especially for close-proximity collaboration tasks such
as human-robot handovers. Learning robot manipulation policies from raw,
real-world image data requires a large number of robot-action trials in the
physical environment. Although simulation training offers a cost-effective
alternative, the visual domain gap between simulation and robot workspace
remains a major limitation. We introduce a method for training HRT policies,
focusing on human-to-robot handovers, solely from RGB images without the need
for real-robot training or real-robot data collection. The goal is to enable
the robot to reliably receive objects from a human with stable grasping while
avoiding collisions with the human hand. The proposed policy learner leverages
sparse-view Gaussian Splatting reconstruction of human-to-robot handover scenes
to generate robot demonstrations containing image-action pairs captured with a
camera mounted on the robot gripper. As a result, the simulated camera pose
changes in the reconstructed scene can be directly translated into gripper pose
changes. Experiments in both Gaussian Splatting reconstructed scene and
real-world human-to-robot handover experiments demonstrate that our method
serves as a new and effective representation for the human-to-robot handover
task, contributing to more seamless and robust HRT.

</details>


### [298] [A Shank Angle-Based Control System Enables Soft Exoskeleton to Assist Human Non-Steady Locomotion](https://arxiv.org/abs/2508.09876)
*Xiaowei Tan,Weizhong Jiang,Bi Zhang,Wanxin Chen,Yiwen Zhao,Ning Li,Lianqing Liu,Xingang Zhao*

Main category: cs.RO

TL;DR: 该论文提出了一种基于小腿角度的外骨骼控制系统，能够实时协调人类步态，适应非线性步态变化，并在多种活动中动态调整辅助模式。


<details>
  <summary>Details</summary>
Motivation: 外骨骼在稳态步态中已证明有效，但在非线性步态（如步态扰动）中的效果尚未充分研究，尤其是在多样化活动中。

Method: 采用双高斯模型生成辅助模式，并结合基于模型的反馈控制方法，仅使用IMU测量实时更新参数。

Result: 实验验证了方法的有效性，展示了控制系统在多种活动中的鲁棒性，并观察到外骨骼辅助对人类用户的积极生物力学和生理反应。

Conclusion: 该控制系统能够有效适应非线性步态变化，为外骨骼在多样化活动中的应用提供了技术支持。

Abstract: Exoskeletons have been shown to effectively assist humans during steady
locomotion. However, their effects on non-steady locomotion, characterized by
nonlinear phase progression within a gait cycle, remain insufficiently
explored, particularly across diverse activities. This work presents a shank
angle-based control system that enables the exoskeleton to maintain real-time
coordination with human gait, even under phase perturbations, while dynamically
shaping assistance profiles to match the biological ankle moment patterns
across walking, running, stair negotiation tasks. The control system consists
of an assistance profile online generation method and a model-based feedforward
control method. The assistance profile is formulated as a dual-Gaussian model
with the shank angle as the independent variable. Leveraging only IMU
measurements, the model parameters are updated online each stride to adapt to
inter- and intra-individual biomechanical variability. The profile tracking
control employs a human-exoskeleton kinematics and stiffness model as a
feedforward component, reducing reliance on historical control data due to the
lack of clear and consistent periodicity in non-steady locomotion. Three
experiments were conducted using a lightweight soft exoskeleton with multiple
subjects. The results validated the effectiveness of each individual method,
demonstrated the robustness of the control system against gait perturbations
across various activities, and revealed positive biomechanical and
physiological responses of human users to the exoskeleton's mechanical
assistance.

</details>


### [299] [PPL: Point Cloud Supervised Proprioceptive Locomotion Reinforcement Learning for Legged Robots in Crawl Spaces](https://arxiv.org/abs/2508.09950)
*Bida Ma,Nuo Xu,Chenkun Qi,Xin Liu,Yule Mo,Jinkai Wang,Chunpeng Lu*

Main category: cs.RO

TL;DR: 提出了一种基于点云监督的强化学习方法，用于腿式机器人在狭窄空间中的运动，无需外部传感器。


<details>
  <summary>Details</summary>
Motivation: 现有方法在狭窄空间中因传感器噪声和低可见性受限，且仅能推断地面特征，难以实现高效运动。

Method: 设计状态估计网络，利用历史本体感受数据估计周围环境和碰撞状态；提出极坐标点云处理方法提取特征。

Result: 实验表明，该方法在狭窄空间中表现更敏捷。

Conclusion: 该方法提升了腿式机器人在无外部传感器情况下的狭窄空间运动能力。

Abstract: The legged locomotion in spatially constrained structures (called crawl
spaces) is challenging. In crawl spaces, current exteroceptive locomotion
learning methods are limited by large noises and errors of the sensors in
possible low visibility conditions, and current proprioceptive locomotion
learning methods are difficult in traversing crawl spaces because only ground
features are inferred. In this study, a point cloud supervised proprioceptive
locomotion reinforcement learning method for legged robots in crawl spaces is
proposed. A state estimation network is designed to estimate the robot's
surrounding ground and spatial features as well as the robot's collision states
using historical proprioceptive sensor data. The point cloud is represented in
polar coordinate frame and a point cloud processing method is proposed to
efficiently extract the ground and spatial features that are used to supervise
the state estimation network learning. Comprehensive reward functions that
guide the robot to traverse through crawl spaces after collisions are designed.
Experiments demonstrate that, compared to existing methods, our method exhibits
more agile locomotion in crawl spaces. This study enhances the ability of
legged robots to traverse spatially constrained environments without requiring
exteroceptive sensors.

</details>


### [300] [GBC: Generalized Behavior-Cloning Framework for Whole-Body Humanoid Imitation](https://arxiv.org/abs/2508.09960)
*Yifei Yao,Chengyuan Luo,Jiaheng Du,Wentao He,Jun-Guo Lu*

Main category: cs.RO

TL;DR: 本文提出了一种通用行为克隆（GBC）框架，通过三种创新技术解决人形机器人数据与算法不通用的问题，实现了从人类动作到机器人行为的端到端统一解决方案。


<details>
  <summary>Details</summary>
Motivation: 当前人形机器人领域存在数据与算法不通用的问题，阻碍了通用人形机器人的发展。

Method: GBC框架包含三个关键技术：1）自适应数据管道，利用可微分IK网络将人类动作数据自动重定向到任意人形机器人；2）DAgger-MMPPO算法与MMTransformer架构，学习鲁棒的高保真模仿策略；3）基于Isaac Lab的高效开源平台，简化部署流程。

Result: 在多种异构人形机器人上验证了GBC的通用性和性能，展示了优异的模仿能力和对新动作的迁移能力。

Conclusion: GBC首次实现了创建通用人形控制器的实用化统一路径。

Abstract: The creation of human-like humanoid robots is hindered by a fundamental
fragmentation: data processing and learning algorithms are rarely universal
across different robot morphologies. This paper introduces the Generalized
Behavior Cloning (GBC) framework, a comprehensive and unified solution designed
to solve this end-to-end challenge. GBC establishes a complete pathway from
human motion to robot action through three synergistic innovations. First, an
adaptive data pipeline leverages a differentiable IK network to automatically
retarget any human MoCap data to any humanoid. Building on this foundation, our
novel DAgger-MMPPO algorithm with its MMTransformer architecture learns robust,
high-fidelity imitation policies. To complete the ecosystem, the entire
framework is delivered as an efficient, open-source platform based on Isaac
Lab, empowering the community to deploy the full workflow via simple
configuration scripts. We validate the power and generality of GBC by training
policies on multiple heterogeneous humanoids, demonstrating excellent
performance and transfer to novel motions. This work establishes the first
practical and unified pathway for creating truly generalized humanoid
controllers.

</details>


### [301] [Vision-driven River Following of UAV via Safe Reinforcement Learning using Semantic Dynamics Model](https://arxiv.org/abs/2508.09971)
*Zihan Wang,Nina Mahmoudian*

Main category: cs.RO

TL;DR: 论文提出了一种基于视觉的无人机自主沿河飞行方法，通过子模马尔可夫决策过程解决覆盖控制问题，并引入了边际增益优势估计、语义动态模型和约束演员动态估计器架构，提升了性能与安全性。


<details>
  <summary>Details</summary>
Motivation: 在密集河流环境中，GPS信号不可靠，无人机自主沿河飞行对救援、监视和环境监测等应用至关重要。

Method: 1. 边际增益优势估计（MGAE）优化奖励优势函数；2. 基于语义掩码的语义动态模型（SDM）提供高效短期预测；3. 约束演员动态估计器（CADE）整合模型与安全强化学习框架。

Result: MGAE比传统方法收敛更快且性能更优；SDM提升了短期状态预测精度；CADE成功将安全约束整合到模型强化学习中。

Conclusion: 提出的方法在性能与安全性上均优于传统方法，适用于部分可观测的约束子模马尔可夫决策过程。

Abstract: Vision-driven autonomous river following by Unmanned Aerial Vehicles is
critical for applications such as rescue, surveillance, and environmental
monitoring, particularly in dense riverine environments where GPS signals are
unreliable. We formalize river following as a coverage control problem in which
the reward function is submodular, yielding diminishing returns as more unique
river segments are visited, thereby framing the task as a Submodular Markov
Decision Process. First, we introduce Marginal Gain Advantage Estimation, which
refines the reward advantage function by using a sliding window baseline
computed from historical episodic returns, thus aligning the advantage
estimation with the agent's evolving recognition of action value in
non-Markovian settings. Second, we develop a Semantic Dynamics Model based on
patchified water semantic masks that provides more interpretable and
data-efficient short-term prediction of future observations compared to latent
vision dynamics models. Third, we present the Constrained Actor Dynamics
Estimator architecture, which integrates the actor, the cost estimator, and SDM
for cost advantage estimation to form a model-based SafeRL framework capable of
solving partially observable Constrained Submodular Markov Decision Processes.
Simulation results demonstrate that MGAE achieves faster convergence and
superior performance over traditional critic-based methods like Generalized
Advantage Estimation. SDM provides more accurate short-term state predictions
that enable the cost estimator to better predict potential violations. Overall,
CADE effectively integrates safety regulation into model-based RL, with the
Lagrangian approach achieving the soft balance of reward and safety during
training, while the safety layer enhances performance during inference by hard
action overlay.

</details>


### [302] [Masquerade: Learning from In-the-wild Human Videos using Data-Editing](https://arxiv.org/abs/2508.09976)
*Marion Lepert,Jiaying Fang,Jeannette Bohg*

Main category: cs.RO

TL;DR: Masquerade方法通过编辑人类视频生成机器人演示，显著提升了机器人策略的泛化能力，性能优于基线5-6倍。


<details>
  <summary>Details</summary>
Motivation: 机器人操纵研究面临数据稀缺问题，现有数据集远小于语言和视觉领域的数据规模。

Method: 编辑人类视频，包括估计3D手部姿势、修复人类手臂并叠加机器人轨迹，预训练视觉编码器并微调扩散策略。

Result: 在三个未见过的厨房任务场景中，性能提升5-6倍，且性能随人类视频数据量对数增长。

Conclusion: 通过缩小视觉体现差距，人类视频成为提升机器人策略的宝贵数据源。

Abstract: Robot manipulation research still suffers from significant data scarcity:
even the largest robot datasets are orders of magnitude smaller and less
diverse than those that fueled recent breakthroughs in language and vision. We
introduce Masquerade, a method that edits in-the-wild egocentric human videos
to bridge the visual embodiment gap between humans and robots and then learns a
robot policy with these edited videos. Our pipeline turns each human video into
robotized demonstrations by (i) estimating 3-D hand poses, (ii) inpainting the
human arms, and (iii) overlaying a rendered bimanual robot that tracks the
recovered end-effector trajectories. Pre-training a visual encoder to predict
future 2-D robot keypoints on 675K frames of these edited clips, and continuing
that auxiliary loss while fine-tuning a diffusion policy head on only 50 robot
demonstrations per task, yields policies that generalize significantly better
than prior work. On three long-horizon, bimanual kitchen tasks evaluated in
three unseen scenes each, Masquerade outperforms baselines by 5-6x. Ablations
show that both the robot overlay and co-training are indispensable, and
performance scales logarithmically with the amount of edited human video. These
results demonstrate that explicitly closing the visual embodiment gap unlocks a
vast, readily available source of data from human videos that can be used to
improve robot policies.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [303] [OSUM-EChat: Enhancing End-to-End Empathetic Spoken Chatbot via Understanding-Driven Spoken Dialogue](https://arxiv.org/abs/2508.09600)
*Xuelong Geng,Qijie Shao,Hongfei Xue,Shuiyuan Wang,Hanke Xie,Zhao Guo,Yi Zhao,Guojian Li,Wenjie Tian,Chengyou Wang,Zhixian Zhao,Kangxiang Xia,Ziyu Zhang,Zhennan Lin,Tianlun Zuo,Mingchen Shao,Yuang Cao,Guobin Ma,Longhao Li,Yuhang Dai,Dehui Gao,Dake Guo,Lei Xie*

Main category: cs.SD

TL;DR: OSUM-EChat是一个开源的端到端语音对话系统，通过三阶段训练策略和双思维机制提升共情能力，减少对大规模数据集的依赖，并引入了新的数据集和评估框架。


<details>
  <summary>Details</summary>
Motivation: 解决现有语音对话系统在共情交互中的问题，如过度依赖大规模数据集、缺乏副语言线索提取以及共情专用数据集和评估框架的不足。

Method: 提出三阶段理解驱动的训练策略和语言-副语言双思维机制，结合EChat-200K数据集和EChat-eval评估框架。

Result: 实验表明OSUM-EChat在共情响应能力上优于其他端到端模型。

Conclusion: OSUM-EChat有效提升了语音对话系统的共情能力，为资源有限场景提供了实用解决方案。

Abstract: Empathy is crucial in enabling natural interactions within spoken dialogue
systems, allowing machines to recognize and respond appropriately to
paralinguistic cues such as age, gender, and emotion. Recent advancements in
end-to-end speech language models, which unify speech understanding and
generation, provide promising solutions. However, several challenges persist,
including an over-reliance on large-scale dialogue datasets, insufficient
extraction of paralinguistic cues vital for conveying empathy, and the lack of
empathy-specific datasets and evaluation frameworks. To address these issues,
we introduce OSUM-EChat, an open-source, end-to-end spoken dialogue system
designed to enhance empathetic interactions, particularly in resource-limited
settings. OSUM-EChat introduces two key innovations: (1) a three-stage
understanding-driven spoken dialogue training strategy that extends the
capabilities of a large speech understanding model to spoken dialogue tasks,
and (2) a linguistic-paralinguistic dual thinking mechanism that integrates
paralinguistic understanding through a chain of thought with dialogue
generation, enabling the system to produce more empathetic responses. This
approach reduces reliance on large-scale dialogue datasets while maintaining
high-quality empathetic interactions. Additionally, we introduce the EChat-200K
dataset, a rich corpus of empathetic speech-to-speech dialogues, and the
EChat-eval benchmark, a comprehensive framework for evaluating the empathetic
capabilities of dialogue systems. Experimental results demonstrate that
OSUM-EChat outperforms end-to-end spoken dialogue models regarding empathetic
responsiveness, validating its effectiveness.

</details>


### [304] [MetaGuardian: Enhancing Voice Assistant Security through Advanced Acoustic Metamaterials](https://arxiv.org/abs/2508.09728)
*Zhiyuan Ning,Zheng Wang,Zhanyong Tang*

Main category: cs.SD

TL;DR: MetaGuardian是一种基于声学超材料的语音助手保护系统，能有效防御无声、对抗性和激光攻击，无需额外软件支持或硬件改动。


<details>
  <summary>Details</summary>
Motivation: 语音助手易受无声、对抗性和激光攻击，现有解决方案通常依赖软件或硬件修改，影响用户体验。MetaGuardian旨在提供一种无需额外支持的保护方案。

Method: 利用超材料单元间的互阻抗效应扩展信号过滤范围（16-40 kHz），并采用螺旋空间结构精确干扰对抗性攻击，同时保持语音助手正常功能。

Result: 在控制评估环境中，MetaGuardian对对抗性、无声和激光攻击的防御成功率很高。

Conclusion: MetaGuardian提供了一种通用、便携且高效的保护方案，适用于多种智能设备。

Abstract: We present MetaGuardian, a voice assistant (VA) protection system based on
acoustic metamaterials. MetaGuardian can be directly integrated into the
enclosures of various smart devices, effectively defending against inaudible,
adversarial and laser attacks without relying on additional software support or
altering the underlying hardware, ensuring usability. To achieve this,
MetaGuardian leverages the mutual impedance effects between metamaterial units
to extend the signal filtering range to 16-40 kHz to effectively block
wide-band inaudible attacks. Additionally, it adopts a carefully designed
coiled space structure to precisely interfere with adversarial attacks while
ensuring the normal functioning of VAs. Furthermore, MetaGuardian offers a
universal structural design, allowing itself to be flexibly adapted to various
smart devices, striking a balance between portability and protection
effectiveness. In controled evaluation environments, MetaGuardian achieves a
high defense success rate against various attack types, including adversarial,
inaudible and laser attacks.

</details>


### [305] [HingeNet: A Harmonic-Aware Fine-Tuning Approach for Beat Tracking](https://arxiv.org/abs/2508.09788)
*Ganghui Ru,Jieying Wang,Jiahao Zhao,Yulun Wu,Yi Yu,Nannan Jiang,Wei Wang,Wei Li*

Main category: cs.SD

TL;DR: HingeNet是一种参数高效的微调方法，专为节拍跟踪任务设计，通过利用预训练基础模型的中间特征表示，结合谐波感知机制，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 现有预训练模型在节拍跟踪任务中因标注数据有限而效果不佳，需要一种更高效的微调方法。

Method: 提出HingeNet，一种轻量级、可分离的网络，利用预训练模型的中间特征，并引入谐波感知机制。

Result: 在基准数据集上，HingeNet实现了节拍和强拍跟踪的最先进性能。

Conclusion: HingeNet通过高效微调和谐波感知机制，显著提升了节拍跟踪任务的性能。

Abstract: Fine-tuning pre-trained foundation models has made significant progress in
music information retrieval. However, applying these models to beat tracking
tasks remains unexplored as the limited annotated data renders conventional
fine-tuning methods ineffective. To address this challenge, we propose
HingeNet, a novel and general parameter-efficient fine-tuning method
specifically designed for beat tracking tasks. HingeNet is a lightweight and
separable network, visually resembling a hinge, designed to tightly interface
with pre-trained foundation models by using their intermediate feature
representations as input. This unique architecture grants HingeNet broad
generalizability, enabling effective integration with various pre-trained
foundation models. Furthermore, considering the significance of harmonics in
beat tracking, we introduce harmonic-aware mechanism during the fine-tuning
process to better capture and emphasize the harmonic structures in musical
signals. Experiments on benchmark datasets demonstrate that HingeNet achieves
state-of-the-art performance in beat and downbeat tracking

</details>


### [306] [BeatFM: Improving Beat Tracking with Pre-trained Music Foundation Model](https://arxiv.org/abs/2508.09790)
*Ganghui Ru,Jieying Wang,Jiahao Zhao,Yulun Wu,Yi Yu,Nannan Jiang,Wei Wang,Wei Li*

Main category: cs.SD

TL;DR: 提出了一种基于预训练音乐基础模型BeatFM的新范式，通过多维度语义聚合模块提升节拍追踪性能，实验表明其在多个基准数据集上表现最佳。


<details>
  <summary>Details</summary>
Motivation: 现有节拍追踪方法因标注数据稀缺，难以泛化到多样音乐风格和复杂节奏结构。

Method: 引入预训练音乐基础模型，设计多维度语义聚合模块（时间、频率、通道域）。

Result: 在多个基准数据集上实现了最先进的节拍和强拍追踪性能。

Conclusion: BeatFM通过预训练模型和语义聚合模块有效解决了节拍追踪的泛化和准确性问题。

Abstract: Beat tracking is a widely researched topic in music information retrieval.
However, current beat tracking methods face challenges due to the scarcity of
labeled data, which limits their ability to generalize across diverse musical
styles and accurately capture complex rhythmic structures. To overcome these
challenges, we propose a novel beat tracking paradigm BeatFM, which introduces
a pre-trained music foundation model and leverages its rich semantic knowledge
to improve beat tracking performance. Pre-training on diverse music datasets
endows music foundation models with a robust understanding of music, thereby
effectively addressing these challenges. To further adapt it for beat tracking,
we design a plug-and-play multi-dimensional semantic aggregation module, which
is composed of three parallel sub-modules, each focusing on semantic
aggregation in the temporal, frequency, and channel domains, respectively.
Extensive experiments demonstrate that our method achieves state-of-the-art
performance in beat and downbeat tracking across multiple benchmark datasets.

</details>


### [307] [Analysis of Domain Shift across ASR Architectures via TTS-Enabled Separation of Target Domain and Acoustic Conditions](https://arxiv.org/abs/2508.09868)
*Tina Raissi,Nick Rossenbach,Ralf Schlüter*

Main category: cs.SD

TL;DR: 比较经典模块化和新型序列到序列（seq2seq）架构在领域不匹配下的自动语音识别（ASR）性能，发现具体建模选择对性能影响更大。


<details>
  <summary>Details</summary>
Motivation: 研究不同ASR架构在领域不匹配下的表现，以了解其泛化能力。

Method: 通过合成目标领域音频，结合目标领域语言模型进行领域适应，比较不同建模选择（如标签单元、上下文长度和拓扑结构）。

Result: 在领域不匹配下，具体建模选择（而非架构选择）对性能影响更大。

Conclusion: 优化ASR性能的关键在于具体建模选择，而非架构类型。

Abstract: We analyze automatic speech recognition (ASR) modeling choices under domain
mismatch, comparing classic modular and novel sequence-to-sequence (seq2seq)
architectures. Across the different ASR architectures, we examine a spectrum of
modeling choices, including label units, context length, and topology. To
isolate language domain effects from acoustic variation, we synthesize target
domain audio using a text-to-speech system trained on LibriSpeech. We
incorporate target domain n-gram and neural language models for domain
adaptation without retraining the acoustic model. To our knowledge, this is the
first controlled comparison of optimized ASR systems across state-of-the-art
architectures under domain shift, offering insights into their generalization.
The results show that, under domain shift, rather than the decoder architecture
choice or the distinction between classic modular and novel seq2seq models, it
is specific modeling choices that influence performance.

</details>


### [308] [A Comparative Analysis on ASR System Combination for Attention, CTC, Factored Hybrid, and Transducer Models](https://arxiv.org/abs/2508.09880)
*Noureldin Bayoumi,Robin Schmitt,Tina Raissi,Albert Zeyer,Ralf Schlüter,Hermann Ney*

Main category: cs.SD

TL;DR: 比较不同ASR架构的模型组合方法，通过两阶段方法实现一致比较，提升性能。


<details>
  <summary>Details</summary>
Motivation: 探索不同ASR模型的互补优势，解决模型组合中的变异性问题。

Method: 采用两阶段方法，联合假设列表重评分，并通过对数线性组合选择最佳假设。

Result: 在Librispeech 960h任务中验证了方法的有效性。

Conclusion: 两阶段方法确保了模型组合结果的一致性，并提升了性能。

Abstract: Combination approaches for speech recognition (ASR) systems cover structured
sentence-level or word-based merging techniques as well as combination of model
scores during beam search. In this work, we compare model combination across
popular ASR architectures. Our method leverages the complementary strengths of
different models in exploring diverse portions of the search space. We rescore
a joint hypothesis list of two model candidates. We then identify the best
hypothesis through log-linear combination of these sequence-level scores. While
model combination during first-pass recognition may yield improved performance,
it introduces variability due to differing decoding methods, making direct
comparison more challenging. Our two-pass method ensures consistent comparisons
across all system combination results presented in this study. We evaluate
model pair candidates with varying architectures and label topologies and
units. Experimental results are provided for the Librispeech 960h task.

</details>


<div id='cs.GT'></div>

# cs.GT [[Back]](#toc)

### [309] [Collective dynamics of strategic classification](https://arxiv.org/abs/2508.09340)
*Marta C. Couto,Flavia Barsotti,Fernando P. Santos*

Main category: cs.GT

TL;DR: 论文探讨了AI分类算法在高风险决策中的应用，分析了用户策略性适应与算法重新训练之间的反馈循环，并提出通过进化博弈论框架解决这一问题。


<details>
  <summary>Details</summary>
Motivation: 研究AI分类算法在高风险领域（如金融、医疗等）中用户策略性适应与算法重新训练之间的动态关系，以解决由此产生的社会成本问题。

Method: 采用进化博弈论框架，分析用户与机构之间的反馈循环，并通过信用贷款案例研究不同交互范式。

Result: 研究发现，提高检测能力可降低社会成本并促进用户改进；在不完美分类器情况下，算法补救可引导动态向高用户改进率发展。

Conclusion: 机构对用户群体的快速适应能力影响最终结果，严格机构提供补救措施可能导致文献中未发现的循环动态。

Abstract: Classification algorithms based on Artificial Intelligence (AI) are nowadays
applied in high-stakes decisions in finance, healthcare, criminal justice, or
education. Individuals can strategically adapt to the information gathered
about classifiers, which in turn may require algorithms to be re-trained. Which
collective dynamics will result from users' adaptation and algorithms'
retraining? We apply evolutionary game theory to address this question. Our
framework provides a mathematically rigorous way of treating the problem of
feedback loops between collectives of users and institutions, allowing to test
interventions to mitigate the adverse effects of strategic adaptation. As a
case study, we consider institutions deploying algorithms for credit lending.
We consider several scenarios, each representing different interaction
paradigms. When algorithms are not robust against strategic manipulation, we
are able to capture previous challenges discussed in the strategic
classification literature, whereby users either pay excessive costs to meet the
institutions' expectations (leading to high social costs) or game the algorithm
(e.g., provide fake information). From this baseline setting, we test the role
of improving gaming detection and providing algorithmic recourse. We show that
increased detection capabilities reduce social costs and could lead to users'
improvement; when perfect classifiers are not feasible (likely to occur in
practice), algorithmic recourse can steer the dynamics towards high users'
improvement rates. The speed at which the institutions re-adapt to the user's
population plays a role in the final outcome. Finally, we explore a scenario
where strict institutions provide actionable recourse to their unsuccessful
users and observe cycling dynamics so far unnoticed in the literature.

</details>


<div id='econ.GN'></div>

# econ.GN [[Back]](#toc)

### [310] [Forecasting Binary Economic Events in Modern Mercantilism: Traditional methodologies coupled with PCA and K-means Quantitative Analysis of Qualitative Sentimental Data](https://arxiv.org/abs/2508.09243)
*Sebastian Kot*

Main category: econ.GN

TL;DR: 论文通过主成分分析（PCA）研究现代重商主义，揭示其与全球化范式的差异。


<details>
  <summary>Details</summary>
Motivation: 探讨经济民族主义、技术脱钩和地缘政治碎片化对全球化的影响。

Method: 使用PCA分析SBERT生成的新闻语义嵌入，提取关键特征。

Result: 识别出驱动分类性能的语义特征，提升预测准确性。

Conclusion: 提供了一种可扩展的数据驱动框架，用于定量追踪重商主义动态。

Abstract: This paper examines Modern Mercantilism, characterized by rising economic
nationalism, strategic technological decoupling, and geopolitical
fragmentation, as a disruptive shift from the post-1945 globalization paradigm.
It applies Principal Component Analysis (PCA) to 768-dimensional
SBERT-generated semantic embeddings of curated news articles to extract
orthogonal latent factors that discriminate binary event outcomes linked to
protectionism, technological sovereignty, and bloc realignments. Analysis of
principal component loadings identifies key semantic features driving
classification performance, enhancing interpretability and predictive accuracy.
This methodology provides a scalable, data-driven framework for quantitatively
tracking emergent mercantilist dynamics through high-dimensional text analytics

</details>


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [311] [Bayesian-Driven Graph Reasoning for Active Radio Map Construction](https://arxiv.org/abs/2508.09142)
*Wenlihan Lu,Shijian Gao,Miaowen Wen,Yuxuan Liang,Chan-Byoung Chae,H. Vincent Poor*

Main category: eess.SP

TL;DR: 提出了一种基于不确定性感知的无线电地图重建框架（URAM），结合贝叶斯神经网络和注意力强化学习，优化无人机导航效率与覆盖范围。


<details>
  <summary>Details</summary>
Motivation: 低空经济中，无人机的电池容量限制了其数据收集的覆盖范围和效率，需一种智能导航方法。

Method: 结合贝叶斯神经网络实时估计空间不确定性，以及注意力强化学习策略进行全局路径规划。

Result: URAM框架比现有基线方法提高了34%的重建精度。

Conclusion: URAM通过智能路径规划和不确定性管理，显著提升了无人机数据收集的效率与准确性。

Abstract: With the emergence of the low-altitude economy, radio maps have become
essential for ensuring reliable wireless connectivity to aerial platforms.
Autonomous aerial agents are commonly deployed for data collection using
waypoint-based navigation; however, their limited battery capacity
significantly constrains coverage and efficiency. To address this, we propose
an uncertainty-aware radio map (URAM) reconstruction framework that explicitly
leverages graph-based reasoning tailored for waypoint navigation. Our approach
integrates two key deep learning components: (1) a Bayesian neural network that
estimates spatial uncertainty in real time, and (2) an attention-based
reinforcement learning policy that performs global reasoning over a
probabilistic roadmap, using uncertainty estimates to plan informative and
energy-efficient trajectories. This graph-based reasoning enables intelligent,
non-myopic trajectory planning, guiding agents toward the most informative
regions while satisfying safety constraints. Experimental results show that
URAM improves reconstruction accuracy by up to 34% over existing baselines.

</details>


### [312] [RadioMamba: Breaking the Accuracy-Efficiency Trade-off in Radio Map Construction via a Hybrid Mamba-UNet](https://arxiv.org/abs/2508.09140)
*Honggang Jia,Nan Cheng,Xiucheng Wang,Conghao Zhou,Ruijin Sun,Xuemin,Shen*

Main category: eess.SP

TL;DR: RadioMamba是一种混合Mamba-UNet架构，用于解决无线电地图构建中的精度与效率权衡问题。


<details>
  <summary>Details</summary>
Motivation: 当前基于深度学习的无线电地图构建方法存在精度与效率的权衡问题，无法满足6G服务的实时和准确需求。

Method: 采用Mamba-Convolutional块，Mamba分支捕获长空间依赖关系，卷积分支提取局部特征，实现全局与局部特征的结合。

Result: 实验表明，RadioMamba在精度上优于现有方法（如扩散模型），速度快20倍，参数仅占2.9%。

Conclusion: RadioMamba通过提升精度和效率，为下一代无线系统的实时智能优化提供了可行方案。

Abstract: Radio map (RM) has recently attracted much attention since it can provide
real-time and accurate spatial channel information for 6G services and
applications. However, current deep learning-based methods for RM construction
exhibit well known accuracy-efficiency trade-off. In this paper, we introduce
RadioMamba, a hybrid Mamba-UNet architecture for RM construction to address the
trade-off. Generally, accurate RM construction requires modeling long-range
spatial dependencies, reflecting the global nature of wave propagation physics.
RadioMamba utilizes a Mamba-Convolutional block where the Mamba branch captures
these global dependencies with linear complexity, while a parallel
convolutional branch extracts local features. This hybrid design generates
feature representations that capture both global context and local detail.
Experiments show that RadioMamba achieves higher accuracy than existing
methods, including diffusion models, while operating nearly 20 times faster and
using only 2.9\% of the model parameters. By improving both accuracy and
efficiency, RadioMamba presents a viable approach for real-time intelligent
optimization in next generation wireless systems.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [313] [A Lightweight Learned Cardinality Estimation Model](https://arxiv.org/abs/2508.09602)
*Yaoyu Zhu,Jintao Zhang,Guoliang Li,Jianhua Feng*

Main category: cs.DB

TL;DR: CoDe提出了一种基于覆盖设计和张量分解的数据驱动方法，用于高效且准确地估计数据库查询的基数。


<details>
  <summary>Details</summary>
Motivation: 现有基数估计技术要么精度低，要么延迟高，亟需同时实现高速度和准确性。

Method: CoDe通过覆盖设计将表划分为重叠的小段，利用张量分解建模数据分布，并创新算法选择最佳分布组合。

Result: 实验表明，CoDe在精度和效率上均达到领先水平，半数以上查询实现绝对准确估计。

Conclusion: CoDe在基数估计问题上取得了显著进展，兼具高精度和高效性。

Abstract: Cardinality estimation is a fundamental task in database management systems,
aiming to predict query results accurately without executing the queries.
However, existing techniques either achieve low estimation accuracy or incur
high inference latency. Simultaneously achieving high speed and accuracy
becomes critical for the cardinality estimation problem. In this paper, we
propose a novel data-driven approach called CoDe (Covering with Decompositions)
to address this problem. CoDe employs the concept of covering design, which
divides the table into multiple smaller, overlapping segments. For each
segment, CoDe utilizes tensor decomposition to accurately model its data
distribution. Moreover, CoDe introduces innovative algorithms to select the
best-fitting distributions for each query, combining them to estimate the final
result. By employing multiple models to approximate distributions, CoDe excels
in effectively modeling discrete distributions and ensuring computational
efficiency. Notably, experimental results show that our method represents a
significant advancement in cardinality estimation, achieving state-of-the-art
levels of both estimation accuracy and inference efficiency. Across various
datasets, CoDe achieves absolute accuracy in estimating more than half of the
queries.

</details>


### [314] [AmbiGraph-Eval: Can LLMs Effectively Handle Ambiguous Graph Queries?](https://arxiv.org/abs/2508.09631)
*Yuchen Tian,Kaixin Li,Hao Chen,Ziyang Luo,Hongzhan Lin,Sebastian Schelter,Lun Du,Jing Ma*

Main category: cs.DB

TL;DR: 论文提出了一种评估大语言模型（LLMs）处理图查询歧义能力的分类法和基准测试AmbiGraph-Eval，发现现有模型在歧义查询上表现不佳。


<details>
  <summary>Details</summary>
Motivation: 现实世界的图查询常存在歧义，而LLMs在处理这些歧义时表现不佳，需要系统评估和改进。

Method: 提出了一种图查询歧义的分类法（包括属性歧义、关系歧义和属性-关系歧义），并开发了AmbiGraph-Eval基准测试。

Result: 评估9种代表性LLMs后发现，即使是顶级模型也难以处理歧义图查询。

Conclusion: 研究揭示了LLMs在歧义处理上的不足，为未来开发专门解决方案提供了方向。

Abstract: Large Language Models (LLMs) have recently demonstrated strong capabilities
in translating natural language into database queries, especially when dealing
with complex graph-structured data. However, real-world queries often contain
inherent ambiguities, and the interconnected nature of graph structures can
amplify these challenges, leading to unintended or incorrect query results. To
systematically evaluate LLMs on this front, we propose a taxonomy of
graph-query ambiguities, comprising three primary types: Attribute Ambiguity,
Relationship Ambiguity, and Attribute-Relationship Ambiguity, each subdivided
into Same-Entity and Cross-Entity scenarios. We introduce AmbiGraph-Eval, a
novel benchmark of real-world ambiguous queries paired with expert-verified
graph query answers. Evaluating 9 representative LLMs shows that even top
models struggle with ambiguous graph queries. Our findings reveal a critical
gap in ambiguity handling and motivate future work on specialized resolution
techniques.

</details>


<div id='eess.AS'></div>

# eess.AS [[Back]](#toc)

### [315] [ProMode: A Speech Prosody Model Conditioned on Acoustic and Textual Inputs](https://arxiv.org/abs/2508.09389)
*Eray Eren,Qingju Liu,Hyeongwoo Kim,Pablo Garrido,Abeer Alwan*

Main category: eess.AS

TL;DR: 提出了一种独立模型ProMode，用于从文本生成韵律特征（如F0和能量），并在TTS等任务中表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 韵律在语音信号中传递丰富的情感和语义信息，但现有方法在韵律建模方面仍有改进空间。

Method: ProMode模型通过部分掩码的声学特征和时间对齐文本输入，生成固定长度的潜在韵律嵌入，解码器利用这些嵌入预测掩码区域的声学特征。

Result: 在GigaSpeech数据集上训练后，模型在F0和能量预测方面表现优于现有方法，并在TTS系统中获得更高的韵律偏好评分。

Conclusion: ProMode模型在韵律建模任务中具有潜力，尤其在需要高质量韵律生成的下游应用中。

Abstract: Prosody conveys rich emotional and semantic information of the speech signal
as well as individual idiosyncrasies. We propose a stand-alone model that maps
text-to-prosodic features such as F0 and energy and can be used in downstream
tasks such as TTS. The ProMode encoder takes as input acoustic features and
time-aligned textual content, both are partially masked, and obtains a
fixed-length latent prosodic embedding. The decoder predicts acoustics in the
masked region using both the encoded prosody input and unmasked textual
content. Trained on the GigaSpeech dataset, we compare our method with
state-of-the-art style encoders. For F0 and energy predictions, we show
consistent improvements for our model at different levels of granularity. We
also integrate these predicted prosodic features into a TTS system and conduct
perceptual tests, which show higher prosody preference compared to the
baselines, demonstrating the model's potential in tasks where prosody modeling
is important.

</details>


### [316] [$\text{M}^3\text{PDB}$: A Multimodal, Multi-Label, Multilingual Prompt Database for Speech Generation](https://arxiv.org/abs/2508.09702)
*Boyu Zhu,Cheng Gong,Muyang Wu,Ruihao Jing,Fan Liu,Xiaolei Zhang,Chi Zhang,Xuelong Li*

Main category: eess.AS

TL;DR: 论文提出了一种名为M3PDB的大型多模态、多语言提示数据库，用于解决零样本语音生成中高质量提示缺失的问题，并设计了一种轻量级提示选择策略。


<details>
  <summary>Details</summary>
Motivation: 当前零样本语音生成模型在现实场景中因提示语音质量不匹配而表现不佳，需要更鲁棒的提示选择方法。

Method: 构建了M3PDB数据库，采用多模态、多智能体标注框架，并提出轻量级提示选择策略。

Result: 实验表明，该数据库和策略能有效支持多种挑战性语音生成场景。

Conclusion: 研究鼓励社区关注更现实和多样化的语音生成应用场景，而非仅优化标准基准性能。

Abstract: Recent advancements in zero-shot speech generation have enabled models to
synthesize speech that mimics speaker identity and speaking style from speech
prompts. However, these models' effectiveness is significantly limited in
real-world scenarios where high-quality speech prompts are absent, incomplete,
or out of domain. This issue arises primarily from a significant quality
mismatch between the speech data utilized for model training and the input
prompt speech during inference. To address this, we introduce
$\text{M}^3\text{PDB}$, the first large-scale, multi-modal, multi-label, and
multilingual prompt database designed for robust prompt selection in speech
generation. Our dataset construction leverages a novel multi-modal, multi-agent
annotation framework, enabling precise and hierarchical labeling across diverse
modalities. Furthermore, we propose a lightweight yet effective prompt
selection strategy tailored for real-time, resource-constrained inference
settings. Experimental results demonstrate that our proposed database and
selection strategy effectively support various challenging speech generation
scenarios. We hope our work can inspire the community to shift focus from
improving performance on standard benchmarks to addressing more realistic and
diverse application scenarios in speech generation. Code and dataset are
available at: https://github.com/hizening/M3PDB.

</details>


### [317] [Fake-Mamba: Real-Time Speech Deepfake Detection Using Bidirectional Mamba as Self-Attention's Alternative](https://arxiv.org/abs/2508.09294)
*Xi Xuan,Zimo Zhu,Wenxin Zhang,Yi-Cheng Lin,Tomi Kinnunen*

Main category: eess.AS

TL;DR: Fake-Mamba利用双向Mamba和XLSR前端检测合成语音，提出三种高效编码器，在多个基准测试中表现优于现有模型。


<details>
  <summary>Details</summary>
Motivation: 语音合成技术的进步加剧了安全威胁，推动了实时深度伪造检测的研究。

Method: 集成XLSR前端与双向Mamba，提出TransBiMamba、ConBiMamba和PN-BiMamba三种编码器，捕捉合成语音的局部和全局特征。

Result: 在ASVspoof 21 LA、21 DF和In-The-Wild基准测试中，分别达到0.97%、1.74%和5.85%的EER，优于现有模型。

Conclusion: Fake-Mamba框架在实时性和泛化能力上表现出色，具有实际应用潜力。

Abstract: Advances in speech synthesis intensify security threats, motivating real-time
deepfake detection research. We investigate whether bidirectional Mamba can
serve as a competitive alternative to Self-Attention in detecting synthetic
speech. Our solution, Fake-Mamba, integrates an XLSR front-end with
bidirectional Mamba to capture both local and global artifacts. Our core
innovation introduces three efficient encoders: TransBiMamba, ConBiMamba, and
PN-BiMamba. Leveraging XLSR's rich linguistic representations, PN-BiMamba can
effectively capture the subtle cues of synthetic speech. Evaluated on ASVspoof
21 LA, 21 DF, and In-The-Wild benchmarks, Fake-Mamba achieves 0.97%, 1.74%, and
5.85% EER, respectively, representing substantial relative gains over SOTA
models XLSR-Conformer and XLSR-Mamba. The framework maintains real-time
inference across utterance lengths, demonstrating strong generalization and
practical viability. The code is available at
https://github.com/xuanxixi/Fake-Mamba.

</details>


### [318] [Objective Soups: Multilingual Multi-Task Modeling for Speech Processing](https://arxiv.org/abs/2508.09228)
*A F M Saif,Lisha Chen,Xiaodong Cui,Songtao Lu,Brian Kingsbury,Tianyi Chen*

Main category: eess.AS

TL;DR: 论文探讨了多语言多任务语音处理（MSP）中目标冲突问题，提出三种多目标优化方案（“目标汤配方”），并通过实验证明分层优化优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 多任务语音处理中目标冲突严重，传统多目标优化（MOO）效果随任务增加而下降，需探索更有效的优化策略。

Method: 提出三种多目标优化方案，引入轻量级层选择机制以减少计算开销，并在多个数据集上验证。

Result: 实验表明，分层优化（分离识别和翻译任务）优于传统方法，性能更优且可扩展。

Conclusion: 分层多目标优化是构建高效MSP模型的有效方法，代码已开源。

Abstract: Training a single model for multilingual, multi-task speech processing (MSP)
is severely hampered by conflicting objectives between tasks like speech
recognition and translation. While multi-objective optimization (MOO) aims to
align gradient updates, its effectiveness diminishes as the number of tasks
grows, making it difficult to find a common descent direction. This raises a
fundamental question: should highly conflicting objectives be optimized jointly
or separated into a hierarchical structure? To address this question, this
paper investigates three multi-objective MSP formulations, which we refer to as
\textbf{objective soup recipes}. These formulations apply multi-objective
optimization at different optimization levels to mitigate potential conflicts
among all objectives. To ensure efficiency, we introduce a lightweight
layer-selection mechanism that computes the conflict-avoiding gradient using
only the most problematic layers, minimizing computational and memory overhead.
Extensive experiments on CoVoST v2, LibriSpeech, and AISHELL-1 reveal that a
bi-level recipe separating recognition and translation tasks consistently
outperforms standard flat optimization. Our work demonstrates that hierarchical
MOO is a more effective and scalable approach for building state-of-the-art MSP
models. Our code has been released at
https://github.com/afmsaif/Objective_Soups.

</details>


### [319] [Improving the Speaker Anonymization Evaluation's Robustness to Target Speakers with Adversarial Learning](https://arxiv.org/abs/2508.09803)
*Carlos Franzreb,Arnab Das,Tim Polzehl,Sebastian Möller*

Main category: eess.AS

TL;DR: 论文提出了一种新的隐私评估方法，通过添加目标分类器来更准确地评估说话人匿名化的隐私保护效果。


<details>
  <summary>Details</summary>
Motivation: 当前说话人匿名化的隐私评估方法在使用同性别目标选择算法（TSA）时高估了隐私保护效果，忽略了匿名化语音中同时包含源说话人和目标说话人信息的问题。

Method: 提出添加目标分类器来测量目标说话人信息的影响，并通过对抗学习去除这些信息。

Result: 实验表明，该方法对多种匿名化方法有效，尤其是在使用同性别TSA时，能提供更可靠的评估。

Conclusion: 通过引入目标分类器，可以更准确地评估说话人匿名化的隐私保护效果，尤其是在同性别TSA场景下。

Abstract: The current privacy evaluation for speaker anonymization often overestimates
privacy when a same-gender target selection algorithm (TSA) is used, although
this TSA leaks the speaker's gender and should hence be more vulnerable. We
hypothesize that this occurs because the evaluation does not account for the
fact that anonymized speech contains information from both the source and
target speakers. To address this, we propose to add a target classifier that
measures the influence of target speaker information in the evaluation, which
can also be removed with adversarial learning. Experiments demonstrate that
this approach is effective for multiple anonymizers, particularly when using a
same-gender TSA, leading to a more reliable assessment.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [320] [How Persuasive Could LLMs Be? A First Study Combining Linguistic-Rhetorical Analysis and User Experiments](https://arxiv.org/abs/2508.09614)
*Daniel Raffini,Agnese Macori,Lorenzo Porcaro,Tiziana Catarci,Marco Angelini*

Main category: cs.HC

TL;DR: 研究分析了ChatGPT生成的伦理主题议论文的修辞和语言特征及其对读者的说服效果，发现其结构一致但风格单一，说服力有限，尤其在伦理问题上。


<details>
  <summary>Details</summary>
Motivation: 探讨AI生成文本在伦理敏感话题中的说服力及其对人类读者的影响。

Method: 通过62名参与者的用户研究及前后调查，分析AI生成论点对观点变化和用户感知的影响。

Result: ChatGPT能生成连贯的议论文，但说服力有限，伦理问题上的担忧甚至加剧。不同主题效果有差异。

Conclusion: 研究揭示了AI在伦理敏感领域说服力的局限性，为未来研究提供了基础。

Abstract: This study examines the rhetorical and linguistic features of argumentative
texts generated by ChatGPT on ethically nuanced topics and investigates their
persuasive impact on human readers.Through a user study involving 62
participants and pre-post interaction surveys, the paper analyzes how exposure
to AI-generated arguments affects opinion change and user perception. A
linguistic and rhetorical analysis of the generated texts reveals a consistent
argumentative macrostructure, reliance on formulaic expressions, and limited
stylistic richness. While ChatGPT demonstrates proficiency in constructing
coherent argumentative texts, its persuasive efficacy appears constrained,
particularly on topics involving ethical issues.The study finds that while
participants often acknowledge the benefits highlighted by ChatGPT, ethical
concerns tend to persist or even intensify post-interaction. The results also
demonstrate a variation depending on the topic. These findings highlight new
insights on AI-generated persuasion in ethically sensitive domains and are a
basis for future research.

</details>


### [321] [A Close Reading Approach to Gender Narrative Biases in AI-Generated Stories](https://arxiv.org/abs/2508.09651)
*Daniel Raffini,Agnese Macori,Marco Angelini,Tiziana Catarci*

Main category: cs.HC

TL;DR: 研究分析了ChatGPT、Gemini和Claude生成故事中的性别叙事偏见，发现隐含偏见持续存在，强调需通过多层面评估。


<details>
  <summary>Details</summary>
Motivation: 探讨AI生成故事中的性别偏见，揭示其对社会认知的潜在影响。

Method: 基于Propp角色分类和Freytag叙事结构设计提示，通过细读法分析故事内容。

Result: 发现生成故事中存在隐含性别偏见，尤其在角色分配、描述和情节发展中。

Conclusion: 需采用多层面评估方法识别和减少AI生成内容中的偏见。

Abstract: The paper explores the study of gender-based narrative biases in stories
generated by ChatGPT, Gemini, and Claude. The prompt design draws on Propp's
character classifications and Freytag's narrative structure. The stories are
analyzed through a close reading approach, with particular attention to
adherence to the prompt, gender distribution of characters, physical and
psychological descriptions, actions, and finally, plot development and
character relationships. The results reveal the persistence of biases -
especially implicit ones - in the generated stories and highlight the
importance of assessing biases at multiple levels using an interpretative
approach.

</details>


### [322] [Based AI improves human decision-making but reduces trust](https://arxiv.org/abs/2508.09297)
*Shiyang Lai,Junsol Kim,Nadav Kunievsky,Yujin Potter,James Evans*

Main category: cs.HC

TL;DR: 研究发现，带有文化偏见的AI助手比中立AI更能提升人类决策表现，但会降低信任度。


<details>
  <summary>Details</summary>
Motivation: 探讨AI中立性是否真的有益于人类决策，以及文化偏见AI的潜在优势。

Method: 对2,500名参与者进行随机试验，测试不同政治倾向的GPT-4o变体在信息评估任务中的效果。

Result: 带有偏见的AI助手提升了人类表现、增加了参与度并减少了评估偏见，但信任度下降。

Conclusion: AI中立性并非最优解，战略性地引入文化偏见可能改善人类决策。

Abstract: Current AI systems minimize risk by enforcing ideological neutrality, yet
this may introduce automation bias by suppressing cognitive engagement in human
decision-making. We conducted randomized trials with 2,500 participants to test
whether culturally biased AI enhances human decision-making. Participants
interacted with politically diverse GPT-4o variants on information evaluation
tasks. Partisan AI assistants enhanced human performance, increased engagement,
and reduced evaluative bias compared to non-biased counterparts, with amplified
benefits when participants encountered opposing views. These gains carried a
trust penalty: participants underappreciated biased AI and overcredited neutral
systems. Exposing participants to two AIs whose biases flanked human
perspectives closed the perception-performance gap. These findings complicate
conventional wisdom about AI neutrality, suggesting that strategic integration
of diverse cultural biases may foster improved and resilient human
decision-making.

</details>


### [323] [Hallucination vs interpretation: rethinking accuracy and precision in AI-assisted data extraction for knowledge synthesis](https://arxiv.org/abs/2508.09458)
*Xi Long,Christy Boscardin,Lauren A. Maggio,Joseph A. Costello,Ralph Gonzales,Rasmyah Hammoudeh,Ki Lai,Yoon Soo Park,Brian C. Gin*

Main category: cs.HC

TL;DR: AI辅助数据提取在健康职业教育文献综述中表现出高效性，尤其在明确问题上与人类表现一致，但在主观解释问题上表现较弱。AI的准确性主要取决于问题的可解释性而非幻觉。


<details>
  <summary>Details</summary>
Motivation: 解决文献综述中数据提取的高劳动强度问题，同时评估AI在提取过程中的准确性和可靠性。

Method: 使用大型语言模型（LLMs）开发自动化数据提取平台，并与人类提取结果进行比较，涵盖187篇文献和17个提取问题。

Result: AI在明确问题上与人类高度一致，但在主观问题上表现较差。AI的错误率（1.51%）低于人类（4.37%）。

Conclusion: AI可作为知识合成的透明、可信赖工具，但需注意保留人类的关键见解。

Abstract: Knowledge syntheses (literature reviews) are essential to health professions
education (HPE), consolidating findings to advance theory and practice.
However, they are labor-intensive, especially during data extraction.
Artificial Intelligence (AI)-assisted extraction promises efficiency but raises
concerns about accuracy, making it critical to distinguish AI 'hallucinations'
(fabricated content) from legitimate interpretive differences. We developed an
extraction platform using large language models (LLMs) to automate data
extraction and compared AI to human responses across 187 publications and 17
extraction questions from a published scoping review. AI-human, human-human,
and AI-AI consistencies were measured using interrater reliability
(categorical) and thematic similarity ratings (open-ended). Errors were
identified by comparing extracted responses to source publications. AI was
highly consistent with humans for concrete, explicitly stated questions (e.g.,
title, aims) and lower for questions requiring subjective interpretation or
absent in text (e.g., Kirkpatrick's outcomes, study rationale). Human-human
consistency was not higher than AI-human and showed the same question-dependent
variability. Discordant AI-human responses (769/3179 = 24.2%) were mostly due
to interpretive differences (18.3%); AI inaccuracies were rare (1.51%), while
humans were nearly three times more likely to state inaccuracies (4.37%).
Findings suggest AI accuracy depends more on interpretability than
hallucination. Repeating AI extraction can identify interpretive complexity or
ambiguity, refining processes before human review. AI can be a transparent,
trustworthy partner in knowledge synthesis, though caution is needed to
preserve critical human insights.

</details>


<div id='astro-ph.IM'></div>

# astro-ph.IM [[Back]](#toc)

### [324] [Robustness analysis of Deep Sky Objects detection models on HPC](https://arxiv.org/abs/2508.09831)
*Olivier Parisot,Diogo Ramalho Fernandes*

Main category: astro-ph.IM

TL;DR: 论文探讨了利用计算机视觉和深度学习技术自动检测深空天体（如星系、星云和星团）的方法，并比较了YOLO和RET-DETR等模型在智能望远镜图像上的表现。


<details>
  <summary>Details</summary>
Motivation: 随着天文观测数据的激增和业余天文学家的参与，需要更准确、鲁棒的自动化处理方法来解决深空天体检测的挑战。

Method: 使用高性能计算（HPC）并行化计算，训练并比较了YOLO和RET-DETR等检测模型在智能望远镜图像上的表现。

Result: 通过HPC加速的模型训练和鲁棒性测试，验证了这些方法在深空天体检测中的有效性。

Conclusion: 计算机视觉和深度学习技术结合HPC，为自动化处理天文图像提供了高效且鲁棒的解决方案。

Abstract: Astronomical surveys and the growing involvement of amateur astronomers are
producing more sky images than ever before, and this calls for automated
processing methods that are accurate and robust. Detecting Deep Sky Objects --
such as galaxies, nebulae, and star clusters -- remains challenging because of
their faint signals and complex backgrounds. Advances in Computer Vision and
Deep Learning now make it possible to improve and automate this process. In
this paper, we present the training and comparison of different detection
models (YOLO, RET-DETR) on smart telescope images, using High-Performance
Computing (HPC) to parallelise computations, in particular for robustness
testing.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [325] [Personalized Product Search Ranking: A Multi-Task Learning Approach with Tabular and Non-Tabular Data](https://arxiv.org/abs/2508.09636)
*Lalitesh Morishetti,Abhay Kumar,Jonathan Scott,Kaushiki Nag,Gunjan Sharma,Shanu Vashishtha,Rahul Sridhar,Rohit Chatter,Kannan Achan*

Main category: cs.IR

TL;DR: 提出了一种基于多任务学习的个性化产品搜索排序优化模型，结合表格与非表格数据，使用TinyBERT和新型采样技术提升性能。


<details>
  <summary>Details</summary>
Motivation: 解决个性化产品搜索排序中混合数据类型处理的挑战，并优化模型性能。

Method: 采用多任务学习框架，整合TinyBERT语义嵌入和新型采样技术，提出基于点击数据的可扩展相关性标注机制。

Result: 实验表明，结合非表格数据和高级嵌入技术显著提升模型性能，消融研究验证了各模块的有效性。

Conclusion: 该方法在个性化产品搜索排序中表现优异，验证了多任务学习和混合数据整合的有效性。

Abstract: In this paper, we present a novel model architecture for optimizing
personalized product search ranking using a multi-task learning (MTL)
framework. Our approach uniquely integrates tabular and non-tabular data,
leveraging a pre-trained TinyBERT model for semantic embeddings and a novel
sampling technique to capture diverse customer behaviors. We evaluate our model
against several baselines, including XGBoost, TabNet, FT-Transformer, DCN-V2,
and MMoE, focusing on their ability to handle mixed data types and optimize
personalized ranking. Additionally, we propose a scalable relevance labeling
mechanism based on click-through rates, click positions, and semantic
similarity, offering an alternative to traditional human-annotated labels.
Experimental results show that combining non-tabular data with advanced
embedding techniques in multi-task learning paradigm significantly enhances
model performance. Ablation studies further underscore the benefits of
incorporating relevance labels, fine-tuning TinyBERT layers, and TinyBERT
query-product embedding interactions. These results demonstrate the
effectiveness of our approach in achieving improved personalized product search
ranking.

</details>


### [326] [On Negative-aware Preference Optimization for Recommendation](https://arxiv.org/abs/2508.09653)
*Chenlu Ding,Daoxuan Liu,Jiancan Wu,Xingyu Hu,Junkang Wu,Haitao Wang,Yongkang Wang,Xingxing Wang,Xiang Wang*

Main category: cs.IR

TL;DR: NAPO是一种针对LLM推荐系统的优化框架，通过共享负样本和动态调整奖励边界，提升推荐准确性和减少流行度偏差。


<details>
  <summary>Details</summary>
Motivation: 现有LLM推荐系统在利用负样本时存在计算开销大和未考虑样本信息量的问题。

Method: 提出NAPO框架，包括批内负样本共享和动态奖励边界调整。

Result: 在三个公开数据集上，NAPO在推荐准确性和减少流行度偏差方面优于现有方法。

Conclusion: NAPO有效解决了负样本利用问题，提升了LLM推荐系统的性能。

Abstract: Recommendation systems leverage user interaction data to suggest relevant
items while filtering out irrelevant (negative) ones. The rise of large
language models (LLMs) has garnered increasing attention for their potential in
recommendation tasks. However, existing methods for optimizing LLM-based
recommenders face challenges in effectively utilizing negative samples. Simply
integrating large numbers of negative samples can improve ranking accuracy and
mitigate popularity bias but often leads to increased computational overhead
and memory costs. Additionally, current approaches fail to account for the
varying informativeness of negative samples, leading to suboptimal optimization
performance. To address these issues, we propose NAPO
(\textbf{N}egative-\textbf{A}ware \textbf{P}reference \textbf{O}ptimization),
an enhanced framework for preference optimization in LLM-based recommendation.
NAPO introduces two key innovations: (1) in-batch negative sharing, which
expands the pool of negative samples without additional memory overhead, and
(2) dynamic reward margin adjustment, which adapts model updates based on the
confidence of negative samples. Extensive experiments on three public datasets
demonstrate that NAPO outperforms existing methods in both recommendation
accuracy and popularity bias reduction.

</details>


### [327] [Describe What You See with Multimodal Large Language Models to Enhance Video Recommendations](https://arxiv.org/abs/2508.09789)
*Marco De Nadai,Andreas Damianou,Mounia Lalmas*

Main category: cs.IR

TL;DR: 论文提出了一种无需微调的框架，利用多模态大语言模型（MLLM）为视频生成丰富的自然语言描述，以提升推荐系统的语义理解能力。


<details>
  <summary>Details</summary>
Motivation: 传统视频推荐系统依赖低层次特征或用户定义的元数据，无法捕捉视频的深层语义（如意图、幽默等），影响个性化推荐效果。

Method: 通过MLLM生成视频的自然语言描述，结合文本编码器，将语义信息注入现有推荐系统（协同过滤、基于内容、生成式推荐）。

Result: 在MicroLens-100K数据集上，该方法优于传统视频、音频和元数据特征，提升了五种代表性模型的性能。

Conclusion: 利用MLLM作为实时知识提取工具，可以构建更具意图感知能力的视频推荐系统。

Abstract: Existing video recommender systems rely primarily on user-defined metadata or
on low-level visual and acoustic signals extracted by specialised encoders.
These low-level features describe what appears on the screen but miss deeper
semantics such as intent, humour, and world knowledge that make clips resonate
with viewers. For example, is a 30-second clip simply a singer on a rooftop, or
an ironic parody filmed amid the fairy chimneys of Cappadocia, Turkey? Such
distinctions are critical to personalised recommendations yet remain invisible
to traditional encoding pipelines. In this paper, we introduce a simple,
recommendation system-agnostic zero-finetuning framework that injects
high-level semantics into the recommendation pipeline by prompting an
off-the-shelf Multimodal Large Language Model (MLLM) to summarise each clip
into a rich natural-language description (e.g. "a superhero parody with
slapstick fights and orchestral stabs"), bridging the gap between raw content
and user intent. We use MLLM output with a state-of-the-art text encoder and
feed it into standard collaborative, content-based, and generative
recommenders. On the MicroLens-100K dataset, which emulates user interactions
with TikTok-style videos, our framework consistently surpasses conventional
video, audio, and metadata features in five representative models. Our findings
highlight the promise of leveraging MLLMs as on-the-fly knowledge extractors to
build more intent-aware video recommenders.

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [328] [From Hard Refusals to Safe-Completions: Toward Output-Centric Safety Training](https://arxiv.org/abs/2508.09224)
*Yuan Yuan,Tina Sriskandarajah,Anna-Luisa Brakman,Alec Helyar,Alex Beutel,Andrea Vallone,Saachi Jain*

Main category: cs.CY

TL;DR: 论文提出了一种名为safe-completions的安全训练方法，替代传统的二元拒绝边界，以提升模型在模糊意图和双用途场景下的安全性和实用性。


<details>
  <summary>Details</summary>
Motivation: 传统的大型语言模型（如ChatGPT）通过二元拒绝边界处理用户意图，但这种方法在模糊意图或双用途场景（如生物学或网络安全）中表现脆弱。

Method: 提出safe-completions方法，专注于模型输出的安全性而非用户意图的二元分类，旨在在安全策略约束下最大化实用性。

Result: 在GPT-5中应用该方法后，实验显示其在双用途提示上安全性提升，残余安全问题的严重性降低，同时模型实用性显著增加。

Conclusion: safe-completions方法有效解决了二元拒绝边界的局限性，提升了模型的安全性和实用性。

Abstract: Large Language Models used in ChatGPT have traditionally been trained to
learn a refusal boundary: depending on the user's intent, the model is taught
to either fully comply or outright refuse. While this is a strong mitigation
for explicitly malicious prompts, focusing safety training on refusals can lead
to brittleness for prompts with obscured user intent. Binary refusal boundaries
are especially ill-suited for dual-use cases (such as biology or
cybersecurity), where a user request can be answered safely at a high level,
but in some cases can lead to malicious uplift if sufficiently detailed or
actionable. As an alternative, we propose safe-completions: a safety-training
approach that centers on the safety of the assistant's output, rather than a
binary classification of the user's intent. Safe-completions seek to maximize
helpfulness within the safety policy's constraints. We incorporated this
approach into GPT-5 and find that across both production comparisons and
internally controlled experiments, safe-completion training improves safety
(especially on dual-use prompts), reduces the severity of residual safety
failures, and substantially increases model helpfulness.

</details>


### [329] [Understanding Ethical Practices in AI: Insights from a Cross-Role, Cross-Region Survey of AI Development Teams](https://arxiv.org/abs/2508.09219)
*Wilder Baldwin,Sepideh Ghanavati,Manuel Woersdoerfer*

Main category: cs.CY

TL;DR: 论文通过混合方法调查分析了不同AI开发角色对伦理的认知、实践和知识，强调需角色敏感的协作方法。


<details>
  <summary>Details</summary>
Motivation: AI应用的快速发展引发了对伦理指南和法规的需求，以降低技术风险。

Method: 混合方法调查（统计与定性分析），涵盖414名来自43个国家的参与者。

Result: 发现不同角色、地区对AI伦理原则的熟悉度和经验存在差异，需角色敏感的协作方法。

Conclusion: 提倡定制化、包容性解决方案，并建议未来研究和教育策略以推动伦理意识。

Abstract: Recent advances in AI applications have raised growing concerns about the
need for ethical guidelines and regulations to mitigate the risks posed by
these technologies. In this paper, we present a mixed-method survey study -
combining statistical and qualitative analyses - to examine the ethical
perceptions, practices, and knowledge of individuals involved in various AI
development roles. Our survey includes 414 participants from 43 countries,
representing roles such as AI managers, analysts, developers, quality assurance
professionals, and information security and privacy experts. The results reveal
varying degrees of familiarity and experience with AI ethics principles,
government initiatives, and risk mitigation strategies across roles, regions,
and other demographic factors. Our findings highlight the importance of a
collaborative, role-sensitive approach, involving diverse stakeholders in
ethical decision-making throughout the AI development lifecycle. We advocate
for developing tailored, inclusive solutions to address ethical challenges in
AI development, and we propose future research directions and educational
strategies to promote ethics-aware AI practices.

</details>


### [330] [Beyond Technocratic XAI: The Who, What & How in Explanation Design](https://arxiv.org/abs/2508.09231)
*Ruchira Dhar,Stephanie Brandl,Ninell Oldenburg,Anders Søgaard*

Main category: cs.CY

TL;DR: 本文提出了一种将解释视为情境化设计过程的框架，强调在XAI中设计解释时需要关注受众、内容和方式，并纳入伦理考量。


<details>
  <summary>Details</summary>
Motivation: 实践中生成有意义的解释是依赖情境的任务，需通过设计选择确保透明度和可访问性。

Method: 基于设计思维，提出一个三部分框架：明确解释的受众（Who）、内容（What）和方式（How），并强调伦理问题。

Result: 框架支持情境感知的XAI方法，促进有效沟通和伦理责任的解释开发。

Conclusion: 将解释视为社会技术设计过程，有助于开发透明、可访问且伦理的XAI系统。

Abstract: The field of Explainable AI (XAI) offers a wide range of techniques for
making complex models interpretable. Yet, in practice, generating meaningful
explanations is a context-dependent task that requires intentional design
choices to ensure accessibility and transparency. This paper reframes
explanation as a situated design process -- an approach particularly relevant
for practitioners involved in building and deploying explainable systems.
Drawing on prior research and principles from design thinking, we propose a
three-part framework for explanation design in XAI: asking Who needs the
explanation, What they need explained, and How that explanation should be
delivered. We also emphasize the need for ethical considerations, including
risks of epistemic inequality, reinforcing social inequities, and obscuring
accountability and governance. By treating explanation as a sociotechnical
design process, this framework encourages a context-aware approach to XAI that
supports effective communication and the development of ethically responsible
explanations.

</details>


### [331] [Ethical Medical Image Synthesis](https://arxiv.org/abs/2508.09293)
*Weina Jin,Ashish Sinha,Kumar Abhishek,Ghassan Hamarneh*

Main category: cs.CY

TL;DR: 论文探讨了医学图像合成（MISyn）的伦理问题，提出了理论分析和实践建议，以减少伦理风险。


<details>
  <summary>Details</summary>
Motivation: 确保MISyn技术在整个生命周期中符合伦理，避免其负面影响，如信任缺失和算法歧视。

Method: 通过理论分析识别MISyn的关键伦理属性和内在限制，并提出实践建议和案例研究。

Result: 发现合成图像缺乏真实医学现象的基础，可能引发伦理风险，并提出适应伦理挑战的实践建议。

Conclusion: 需通过集体努力和实践建议推动MISyn的伦理发展，填补现有实践与伦理要求之间的差距。

Abstract: The task of ethical Medical Image Synthesis (MISyn) is to ensure that the
MISyn techniques are researched and developed ethically throughout their entire
lifecycle, which is essential to prevent the negative impacts of MISyn. To
address the ever-increasing needs and requirements for ethical practice of
MISyn research and development, we first conduct a theoretical analysis that
identifies the key properties of ethical MISyn and intrinsic limits of MISyn.
We identify that synthetic images lack inherent grounding in real medical
phenomena, cannot fully represent the training medical images, and inevitably
introduce new distribution shifts and biases.
  Ethical risks can arise from not acknowledging the intrinsic limits and
weaknesses of synthetic images compared to medical images, with the extreme
form manifested as misinformation of MISyn that substitutes synthetic images
for medical images without acknowledgment. The resulting ethical harms include
eroding trust in the medical imaging dataset environment and causing
algorithmic discrimination towards stakeholders and the public.
  To facilitate collective efforts towards ethical MISyn within and outside the
medical image analysis community, we then propose practical supports for
ethical practice in MISyn based on the theoretical analysis, including ethical
practice recommendations that adapt the existing technical standards, problem
formulation, design, and evaluation practice of MISyn to the ethical
challenges; and oversight recommendations to facilitate checks and balances
from stakeholders and the public. We also present two case studies that
demonstrate how to apply the ethical practice recommendations in practice, and
identify gaps between existing practice and the ethical practice
recommendations.

</details>


### [332] [STREAM (ChemBio): A Standard for Transparently Reporting Evaluations in AI Model Reports](https://arxiv.org/abs/2508.09853)
*Tegan McCaslin,Jide Alaga,Samira Nedungadi,Seth Donoughe,Tom Reed,Rishi Bommasani,Chris Painter,Luca Righetti*

Main category: cs.CY

TL;DR: STREAM标准旨在提高AI模型报告中评估结果的透明度，重点关注化学和生物基准测试，以增强信任和实用性。


<details>
  <summary>Details</summary>
Motivation: 管理AI的灾难性风险需要透明公开的评估，以建立对AI开发的信任。

Method: 与23位专家合作开发STREAM标准，提供最佳实践示例和三页报告模板。

Result: STREAM标准帮助开发者更清晰地展示评估结果，并便于第三方评估其严谨性。

Conclusion: STREAM标准为AI评估的透明报告提供了实用工具，促进更可信的AI发展。

Abstract: Evaluations of dangerous AI capabilities are important for managing
catastrophic risks. Public transparency into these evaluations - including what
they test, how they are conducted, and how their results inform decisions - is
crucial for building trust in AI development. We propose STREAM (A Standard for
Transparently Reporting Evaluations in AI Model Reports), a standard to improve
how model reports disclose evaluation results, initially focusing on chemical
and biological (ChemBio) benchmarks. Developed in consultation with 23 experts
across government, civil society, academia, and frontier AI companies, this
standard is designed to (1) be a practical resource to help AI developers
present evaluation results more clearly, and (2) help third parties identify
whether model reports provide sufficient detail to assess the rigor of the
ChemBio evaluations. We concretely demonstrate our proposed best practices with
"gold standard" examples, and also provide a three-page reporting template to
enable AI developers to implement our recommendations more easily.

</details>


<div id='math.DS'></div>

# math.DS [[Back]](#toc)

### [333] [DeepWKB: Learning WKB Expansions of Invariant Distributions for Stochastic Systems](https://arxiv.org/abs/2508.09529)
*Yao Li,Yicheng Liu,Shirou Wang*

Main category: math.DS

TL;DR: DeepWKB是一种新型深度学习方法，用于估计随机扰动系统的稳态分布，通过WKB近似和蒙特卡洛数据结合，解决了小噪声强度下的计算难题。


<details>
  <summary>Details</summary>
Motivation: 现有的方法在小噪声强度下难以准确估计稳态分布，而DeepWKB旨在提供一种可扩展且灵活的解决方案。

Method: DeepWKB利用蒙特卡洛数据和偏微分方程，分别计算准势和归一化因子，从而近似稳态分布。

Result: 该方法适用于高维随机系统，并能有效处理小噪声强度下的稳态分布问题。

Conclusion: DeepWKB为分析罕见事件、亚稳态和复杂系统的随机稳定性提供了新工具。

Abstract: This paper introduces a novel deep learning method, called DeepWKB, for
estimating the invariant distribution of randomly perturbed systems via its
Wentzel-Kramers-Brillouin (WKB) approximation $u_\epsilon(x) = Q(\epsilon)^{-1}
Z_\epsilon(x) \exp\{-V(x)/\epsilon\}$, where $V$ is known as the
quasi-potential, $\epsilon$ denotes the noise strength, and $Q(\epsilon)$ is
the normalization factor. By utilizing both Monte Carlo data and the partial
differential equations satisfied by $V$ and $Z_\epsilon$, the DeepWKB method
computes $V$ and $Z_\epsilon$ separately. This enables an approximation of the
invariant distribution in the singular regime where $\epsilon$ is sufficiently
small, which remains a significant challenge for most existing methods.
Moreover, the DeepWKB method is applicable to higher-dimensional stochastic
systems whose deterministic counterparts admit non-trivial attractors. In
particular, it provides a scalable and flexible alternative for computing the
quasi-potential, which plays a key role in the analysis of rare events,
metastability, and the stochastic stability of complex systems.

</details>


<div id='cs.IT'></div>

# cs.IT [[Back]](#toc)

### [334] [User-Intent-Driven Semantic Communication via Adaptive Deep Understanding](https://arxiv.org/abs/2508.05884)
*Peigen Ye,Jingpu Duan,Hongyang Du,Yulan Guo*

Main category: cs.IT

TL;DR: 提出了一种用户意图驱动的语义通信系统，通过多模态大模型和掩码引导注意力模块，实现了对用户意图的深度理解和高效传输。


<details>
  <summary>Details</summary>
Motivation: 现有语义通信系统虽能提取关键语义，但难以深入理解和泛化用户的真实意图。

Method: 整合多模态大模型生成用户意图先验，提出掩码引导注意力模块突出关键语义区域，并引入信道状态感知模块适应不同信道条件。

Result: 在瑞利信道（SNR为5 dB）下，PSNR、SSIM和LPIPS分别提升8%、6%和19%。

Conclusion: 该系统显著提升了语义通信的意图理解能力和传输效率。

Abstract: Semantic communication focuses on transmitting task-relevant semantic
information, aiming for intent-oriented communication. While existing systems
improve efficiency by extracting key semantics, they still fail to deeply
understand and generalize users' real intentions. To overcome this, we propose
a user-intention-driven semantic communication system that interprets diverse
abstract intents. First, we integrate a multi-modal large model as semantic
knowledge base to generate user-intention prior. Next, a mask-guided attention
module is proposed to effectively highlight critical semantic regions. Further,
a channel state awareness module ensures adaptive, robust transmission across
varying channel conditions. Extensive experiments demonstrate that our system
achieves deep intent understanding and outperforms DeepJSCC, e.g., under a
Rayleigh channel at an SNR of 5 dB, it achieves improvements of 8%, 6%, and 19%
in PSNR, SSIM, and LPIPS, respectively.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [335] [Teaching Code Refactoring Using LLMs](https://arxiv.org/abs/2508.09332)
*Anshul Khairnar,Aarya Rajoju,Edward F. Gehringer*

Main category: cs.SE

TL;DR: 论文探讨了如何利用大语言模型（LLMs）通过实时、上下文感知的反馈来改进软件工程课程中的代码重构教学。


<details>
  <summary>Details</summary>
Motivation: 代码重构能提升代码质量，但教学难度大，尤其是面对复杂、真实的代码库时。传统方法（如代码审查和静态分析工具）反馈有限且不一致。

Method: 研究将LLM辅助的重构整合到课程项目中，使用结构化提示帮助学生识别和解决代码异味（如长方法和低内聚）。

Result: 2025年春季在一个长期维护的开源项目中实施，并通过学生反馈和代码质量改进分析进行评估。

Conclusion: 结果表明，LLMs能弥合理论与实践学习，帮助学生更深入地理解可维护性和重构原则。

Abstract: This Innovative Practice full paper explores how Large Language Models (LLMs)
can enhance the teaching of code refactoring in software engineering courses
through real-time, context-aware feedback. Refactoring improves code quality
but is difficult to teach, especially with complex, real-world codebases.
Traditional methods like code reviews and static analysis tools offer limited,
inconsistent feedback. Our approach integrates LLM-assisted refactoring into a
course project using structured prompts to help students identify and address
code smells such as long methods and low cohesion. Implemented in Spring 2025
in a long-lived OSS project, the intervention is evaluated through student
feedback and planned analysis of code quality improvements. Findings suggest
that LLMs can bridge theoretical and practical learning, supporting a deeper
understanding of maintainability and refactoring principles.

</details>


### [336] [Your Coding Intent is Secretly in the Context and You Should Deliberately Infer It Before Completion](https://arxiv.org/abs/2508.09537)
*Yanzhou Li,Tianlin Li,Yiran Zhang,Shangqing Liu,Aishan Liu,Yang Liu*

Main category: cs.SE

TL;DR: 论文提出了一种三阶段方法，通过意图推断和交互式优化提升LLM在无注释代码库中的函数生成性能。


<details>
  <summary>Details</summary>
Motivation: 解决现实代码库中因缺乏注释导致LLM性能下降的问题。

Method: 分为意图推断、交互式优化和代码生成三阶段，结合推理提示和开发者反馈。

Result: 在多个评测集上显著提升LLM性能，相对增益超20%。

Conclusion: 三阶段框架有效弥补了无注释场景的不足，交互优化进一步提升了效果。

Abstract: Large Language Models (LLMs) are increasingly used for function completion in
repository-scale codebases. Prior studies demonstrate that when explicit
instructions--such as docstrings--are provided, these models can generate
highly accurate implementations. However, in real-world repositories, such
annotations are frequently absent, and performance drops substantially without
them. To address this gap, we frame the task as a three-stage process. The
first stage focuses on intent inference, where the model analyzes the code
preceding the target function to uncover cues about the desired functionality.
Such preceding context often encodes subtle but critical information, and we
design a reasoning-based prompting framework to guide the LLM through
step-by-step extraction and synthesis of these signals before any code is
generated. The second stage introduces an optional interactive refinement
mechanism to handle cases where preceding context alone is insufficient for
intent recovery. In this stage, the model proposes a small set of candidate
intentions, enabling the developer to select or edit them so that the inferred
intent closely matches the actual requirement. Finally, in the third stage, the
LLM generates the target function conditioned on the finalized intent. To
support this pipeline, we curate a dataset of 40,000 examples annotated with
intermediate reasoning traces and corresponding docstrings. Extensive
experiments on DevEval and ComplexCodeEval show that our approach consistently
boosts multiple LLMs, achieving over 20\% relative gains in both
reference-based and execution-based metrics, with the interactive refinement
stage delivering additional improvements beyond these gains.

</details>


### [337] [DeputyDev -- AI Powered Developer Assistant: Breaking the Code Review Logjam through Contextual AI to Boost Developer Productivity](https://arxiv.org/abs/2508.09676)
*Vishal Khare,Vijay Saini,Deepak Sharma,Anand Kumar,Ankit Rana,Anshul Yadav*

Main category: cs.SE

TL;DR: DeputyDev是一款AI驱动的代码审查助手，旨在解决软件开发中的低效问题。通过自动化代码审查，显著减少了审查时间和代码行审查时间。


<details>
  <summary>Details</summary>
Motivation: 传统代码审查存在耗时长、反馈不一致和质量不高等问题，影响了开发效率和代码质量。

Method: 开发DeputyDev的PR审查功能，并通过双盲A/B实验评估其效果，涉及200多名工程师。

Result: 实验结果显示，每PR和每行代码的审查时间分别减少了23.09%和40.13%。

Conclusion: DeputyDev成功提升了审查效率，并已作为SaaS解决方案推广至外部公司。

Abstract: This study investigates the implementation and efficacy of DeputyDev, an
AI-powered code review assistant developed to address inefficiencies in the
software development process. The process of code review is highly inefficient
for several reasons, such as it being a time-consuming process, inconsistent
feedback, and review quality not being at par most of the time. Using our
telemetry data, we observed that at TATA 1mg, pull request (PR) processing
exhibits significant inefficiencies, with average pick-up and review times of
73 and 82 hours, respectively, resulting in a 6.2 day closure cycle. The review
cycle was marked by prolonged iterative communication between the reviewing and
submitting parties. Research from the University of California, Irvine
indicates that interruptions can lead to an average of 23 minutes of lost
focus, critically affecting code quality and timely delivery. To address these
challenges, we developed DeputyDev's PR review capabilities by providing
automated, contextual code reviews. We conducted a rigorous double-controlled
A/B experiment involving over 200 engineers to evaluate DeputyDev's impact on
review times. The results demonstrated a statistically significant reduction in
both average per PR (23.09%) and average per-line-of-code (40.13%) review
durations. After implementing safeguards to exclude outliers, DeputyDev has
been effectively rolled out across the entire organisation. Additionally, it
has been made available to external companies as a Software-as-a-Service (SaaS)
solution, currently supporting the daily work of numerous engineering
professionals. This study explores the implementation and effectiveness of
AI-assisted code reviews in improving development workflow timelines and code.

</details>


### [338] [LibRec: Benchmarking Retrieval-Augmented LLMs for Library Migration Recommendations](https://arxiv.org/abs/2508.09791)
*Junxiao Han,Yarong Wang,Xiaodong Gu,Cuiyun Gao,Yao Wan,Song Han,David Lo,Shuiguang Deng*

Main category: cs.SE

TL;DR: LibRec是一个结合LLMs和RAG技术的新框架，用于自动化推荐替代库，并通过上下文学习提取迁移意图以提高准确性。LibEval基准用于评估性能，包含2,888条迁移记录。


<details>
  <summary>Details</summary>
Motivation: 解决库迁移推荐问题，提高推荐准确性。

Method: 结合LLMs和RAG技术，利用上下文学习提取迁移意图。

Result: 评估了十种流行LLMs，分析了框架关键组件、提示策略和意图类型的影响。

Conclusion: LibRec在库迁移推荐任务中表现有效，LibEval为评估提供了可靠基准。

Abstract: In this paper, we propose LibRec, a novel framework that integrates the
capabilities of LLMs with retrieval-augmented generation(RAG) techniques to
automate the recommendation of alternative libraries. The framework further
employs in-context learning to extract migration intents from commit messages
to enhance the accuracy of its recommendations. To evaluate the effectiveness
of LibRec, we introduce LibEval, a benchmark designed to assess the performance
in the library migration recommendation task. LibEval comprises 2,888 migration
records associated with 2,368 libraries extracted from 2,324 Python
repositories. Each migration record captures source-target library pairs, along
with their corresponding migration intents and intent types. Based on LibEval,
we evaluated the effectiveness of ten popular LLMs within our framework,
conducted an ablation study to examine the contributions of key components
within our framework, explored the impact of various prompt strategies on the
framework's performance, assessed its effectiveness across various intent
types, and performed detailed failure case analyses.

</details>


### [339] [Exploring the Potential of Large Language Models in Fine-Grained Review Comment Classification](https://arxiv.org/abs/2508.09832)
*Linh Nguyen,Chunhua Liu,Hong Yi Lin,Patanamon Thongtanunam*

Main category: cs.SE

TL;DR: 论文探讨了使用大型语言模型（LLMs）自动分类代码审查评论的潜力，结果显示LLMs优于现有深度学习方法，尤其在低频类别上表现更优。


<details>
  <summary>Details</summary>
Motivation: 现有基于监督学习的代码评论分类方法需要大量人工标注，限制了其扩展性。LLMs可能提供更高效的解决方案。

Method: 评估LLMs对17类代码审查评论的分类性能，并与现有深度学习方法对比。

Result: LLMs在分类性能上优于现有方法，尤其在低频类别上表现更优，且无需依赖特定训练数据分布。

Conclusion: LLMs为代码审查分析提供了可扩展的解决方案，有助于提升代码审查效率。

Abstract: Code review is a crucial practice in software development. As code review
nowadays is lightweight, various issues can be identified, and sometimes, they
can be trivial. Research has investigated automated approaches to classify
review comments to gauge the effectiveness of code reviews. However, previous
studies have primarily relied on supervised machine learning, which requires
extensive manual annotation to train the models effectively. To address this
limitation, we explore the potential of using Large Language Models (LLMs) to
classify code review comments. We assess the performance of LLMs to classify 17
categories of code review comments. Our results show that LLMs can classify
code review comments, outperforming the state-of-the-art approach using a
trained deep learning model. In particular, LLMs achieve better accuracy in
classifying the five most useful categories, which the state-of-the-art
approach struggles with due to low training examples. Rather than relying
solely on a specific small training data distribution, our results show that
LLMs provide balanced performance across high- and low-frequency categories.
These results suggest that the LLMs could offer a scalable solution for code
review analytics to improve the effectiveness of the code review process.

</details>


<div id='q-bio.NC'></div>

# q-bio.NC [[Back]](#toc)

### [340] [Perceptual Reality Transformer: Neural Architectures for Simulating Neurological Perception Conditions](https://arxiv.org/abs/2508.09852)
*Baihan Lin*

Main category: q-bio.NC

TL;DR: 提出了一种名为Perceptual Reality Transformer的框架，通过六种神经网络架构模拟八种神经感知障碍，用于医学教育和辅助技术开发。


<details>
  <summary>Details</summary>
Motivation: 神经感知障碍导致患者与周围人之间的体验差异，亟需一种科学方法模拟这些状态以增进理解和应用。

Method: 使用六种神经网络架构，学习从自然图像到特定感知状态的映射，并通过ImageNet和CIFAR-10数据集评估性能。

Result: Vision Transformer架构表现最佳，优于传统CNN和生成方法，并建立了首个神经感知模拟的基准。

Conclusion: 该框架在医学教育、同理心训练和辅助技术中有广泛应用，同时推进了对神经网络模拟非典型人类感知的理解。

Abstract: Neurological conditions affecting visual perception create profound
experiential divides between affected individuals and their caregivers,
families, and medical professionals. We present the Perceptual Reality
Transformer, a comprehensive framework employing six distinct neural
architectures to simulate eight neurological perception conditions with
scientifically-grounded visual transformations. Our system learns mappings from
natural images to condition-specific perceptual states, enabling others to
experience approximations of simultanagnosia, prosopagnosia, ADHD attention
deficits, visual agnosia, depression-related changes, anxiety tunnel vision,
and Alzheimer's memory effects. Through systematic evaluation across ImageNet
and CIFAR-10 datasets, we demonstrate that Vision Transformer architectures
achieve optimal performance, outperforming traditional CNN and generative
approaches. Our work establishes the first systematic benchmark for
neurological perception simulation, contributes novel condition-specific
perturbation functions grounded in clinical literature, and provides
quantitative metrics for evaluating simulation fidelity. The framework has
immediate applications in medical education, empathy training, and assistive
technology development, while advancing our fundamental understanding of how
neural networks can model atypical human perception.

</details>


<div id='cs.LO'></div>

# cs.LO [[Back]](#toc)

### [341] [TPTP World Infrastructure for Non-classical Logics](https://arxiv.org/abs/2508.09318)
*Alexander Steen,Geoff Sutcliffe*

Main category: cs.LO

TL;DR: 本文概述了TPTP World基础设施在非经典逻辑中的扩展，包括语言、问题、解决方案和工具支持，并以量化正规多模态逻辑为例详细说明。


<details>
  <summary>Details</summary>
Motivation: TPTP World是自动定理证明（ATP）系统的重要基础设施，扩展其支持非经典逻辑的需求推动了本文的研究。

Method: 介绍了TPTP World的非经典逻辑扩展，包括语言设计、问题集、解决方案及工具支持，并以量化正规多模态逻辑为例进行详细描述。

Result: TPTP World成功扩展支持非经典逻辑，为ATP系统提供了更广泛的应用基础。

Conclusion: TPTP World的非经典逻辑扩展为ATP研究提供了强大支持，未来可进一步扩展更多逻辑类型。

Abstract: The TPTP World is the well established infrastructure that supports research,
development, and deployment of Automated Theorem Proving (ATP) systems. The
TPTP World supports a range of classical logics, and since release v9.0.0 has
supported non-classical logics. This paper provides a self-contained
comprehensive overview of the TPTP World infrastructure for ATP in
non-classical logics: the non-classical language extension, problems and
solutions, and tool support. A detailed description of use of the
infrastructure for quantified normal multi-modal logic is given.

</details>


<div id='eess.SY'></div>

# eess.SY [[Back]](#toc)

### [342] [Online Safety under Multiple Constraints and Input Bounds using gatekeeper: Theory and Applications](https://arxiv.org/abs/2508.09963)
*Devansh R. Agrawal,Dimitra Panagou*

Main category: eess.SY

TL;DR: 提出了一种名为gatekeeper的框架，用于保证多约束条件下的网络物理系统在线安全，通过递归验证无限时域轨迹的存在性，并利用备份控制器实现高效计算。


<details>
  <summary>Details</summary>
Motivation: 解决网络物理系统在多状态和输入约束下的在线安全问题，确保系统动态和约束的满足。

Method: 使用gatekeeper框架，基于备份控制器和单标量变量优化，递归验证轨迹存在性，并设计子最优性边界。

Result: 理论推导了子最优性边界，并在多智能体编队飞行中验证了框架的有效性。

Conclusion: gatekeeper框架在保证安全性的同时具有计算高效性，适用于复杂非线性非凸约束场景。

Abstract: This letter presents an approach to guarantee online safety of a
cyber-physical system under multiple state and input constraints. Our proposed
framework, called gatekeeper, recursively guarantees the existence of an
infinite-horizon trajectory that satisfies all constraints and system dynamics.
Such trajectory is constructed using a backup controller, which we define
formally in this paper. gatekeeper relies on a small number of verifiable
assumptions, and is computationally efficient since it requires optimization
over a single scalar variable. We make two primary contributions in this
letter. (A) First, we develop the theory of gatekeeper: we derive a
sub-optimality bound relative to a full nonlinear trajectory optimization
problem, and show how this can be used in runtime to validate performance. This
also informs the design of the backup controllers and sets. (B) Second, we
demonstrate in detail an application of gatekeeper for multi-agent formation
flight, where each Dubins agent must avoid multiple obstacles and weapons
engagement zones, both of which are nonlinear, nonconvex constraints.

</details>


### [343] [Collision-Free Bearing-Driven Formation Tracking for Euler-Lagrange Systems](https://arxiv.org/abs/2508.09908)
*Haoshu Cheng,Martin Guay,Shimin Wang,Yunhong Che*

Main category: eess.SY

TL;DR: 本文研究了基于方位的异构欧拉-拉格朗日系统在多移动领导者下的编队跟踪问题，提出了一种分布式观测器和自适应控制律，无需系统参数先验知识，并确保碰撞避免。


<details>
  <summary>Details</summary>
Motivation: 解决异构欧拉-拉格朗日系统在多移动领导者下的编队跟踪问题，克服参数不确定性和传统连通性假设的限制。

Method: 设计分布式观测器估计领导者速度和加速度，结合自适应机制提出分布式控制律，并基于初始编队配置确保碰撞避免。

Result: 提出的方法有效实现了目标编队跟踪，并通过数值示例验证了其有效性。

Conclusion: 该方法在参数不确定性和多移动领导者情况下具有鲁棒性，且能避免碰撞，适用于实际应用。

Abstract: In this paper, we investigate the problem of tracking formations driven by
bearings for heterogeneous Euler-Lagrange systems with parametric uncertainty
in the presence of multiple moving leaders. To estimate the leaders' velocities
and accelerations, we first design a distributed observer for the leader
system, utilizing a bearing-based localization condition in place of the
conventional connectivity assumption. This observer, coupled with an adaptive
mechanism, enables the synthesis of a novel distributed control law that guides
the formation towards the target formation, without requiring prior knowledge
of the system parameters. Furthermore, we establish a sufficient condition,
dependent on the initial formation configuration, that ensures collision
avoidance throughout the formation evolution. The effectiveness of the proposed
approach is demonstrated through a numerical example.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [344] [Generative Artificial Intelligence in Medical Imaging: Foundations, Progress, and Clinical Translation](https://arxiv.org/abs/2508.09177)
*Xuanru Zhou,Cheng Li,Shuqiang Wang,Ye Li,Tao Tan,Hairong Zheng,Shanshan Wang*

Main category: eess.IV

TL;DR: 本文综述了生成式AI在医学影像中的进展，包括GANs、VAEs、扩散模型等，探讨了其在临床工作流中的作用及挑战。


<details>
  <summary>Details</summary>
Motivation: 探讨生成式AI如何解决医学影像中的数据稀缺、标准化和多模态整合等长期挑战。

Method: 系统回顾生成模型在医学影像中的应用，并提出三层次评估框架（像素级、特征级、任务级）。

Result: 生成式AI在影像获取、重建、诊断支持等方面展现出潜力，但仍面临泛化性、隐私和监管等障碍。

Conclusion: 生成式AI与基础模型的结合有望推动下一代医学影像系统的发展，需跨学科合作以促进临床转化。

Abstract: Generative artificial intelligence (AI) is rapidly transforming medical
imaging by enabling capabilities such as data synthesis, image enhancement,
modality translation, and spatiotemporal modeling. This review presents a
comprehensive and forward-looking synthesis of recent advances in generative
modeling including generative adversarial networks (GANs), variational
autoencoders (VAEs), diffusion models, and emerging multimodal foundation
architectures and evaluates their expanding roles across the clinical imaging
continuum. We systematically examine how generative AI contributes to key
stages of the imaging workflow, from acquisition and reconstruction to
cross-modality synthesis, diagnostic support, and treatment planning. Emphasis
is placed on both retrospective and prospective clinical scenarios, where
generative models help address longstanding challenges such as data scarcity,
standardization, and integration across modalities. To promote rigorous
benchmarking and translational readiness, we propose a three-tiered evaluation
framework encompassing pixel-level fidelity, feature-level realism, and
task-level clinical relevance. We also identify critical obstacles to
real-world deployment, including generalization under domain shift,
hallucination risk, data privacy concerns, and regulatory hurdles. Finally, we
explore the convergence of generative AI with large-scale foundation models,
highlighting how this synergy may enable the next generation of scalable,
reliable, and clinically integrated imaging systems. By charting technical
progress and translational pathways, this review aims to guide future research
and foster interdisciplinary collaboration at the intersection of AI, medicine,
and biomedical engineering.

</details>


### [345] [Hybrid(Transformer+CNN)-based Polyp Segmentation](https://arxiv.org/abs/2508.09189)
*Madan Baduwal*

Main category: eess.IV

TL;DR: 提出了一种结合Transformer和CNN的混合模型，用于提高结肠息肉分割的准确性和鲁棒性，解决了边界模糊和内窥镜伪影的挑战。


<details>
  <summary>Details</summary>
Motivation: 结肠息肉分割面临形状、光照和边界模糊等挑战，现有方法难以应对。

Method: 采用Transformer + CNN的混合架构，引入边界感知注意力机制和鲁棒特征提取。

Result: 在分割准确性和伪影鲁棒性上显著优于现有方法（召回率提升1.76%，准确率提升0.07%）。

Conclusion: 混合模型在复杂场景下表现出色，为息肉分割提供了更优解决方案。

Abstract: Colonoscopy is still the main method of detection and segmentation of colonic
polyps, and recent advancements in deep learning networks such as U-Net,
ResUNet, Swin-UNet, and PraNet have made outstanding performance in polyp
segmentation. Yet, the problem is extremely challenging due to high variation
in size, shape, endoscopy types, lighting, imaging protocols, and ill-defined
boundaries (fluid, folds) of the polyps, rendering accurate segmentation a
challenging and problematic task. To address these critical challenges in polyp
segmentation, we introduce a hybrid (Transformer + CNN) model that is crafted
to enhance robustness against evolving polyp characteristics. Our hybrid
architecture demonstrates superior performance over existing solutions,
particularly in addressing two critical challenges: (1) accurate segmentation
of polyps with ill-defined margins through boundary-aware attention mechanisms,
and (2) robust feature extraction in the presence of common endoscopic
artifacts, including specular highlights, motion blur, and fluid occlusions.
Quantitative evaluations reveal significant improvements in segmentation
accuracy (Recall improved by 1.76%, i.e., 0.9555, accuracy improved by 0.07%,
i.e., 0.9849) and artifact resilience compared to state-of-the-art polyp
segmentation methods.

</details>


### [346] [impuTMAE: Multi-modal Transformer with Masked Pre-training for Missing Modalities Imputation in Cancer Survival Prediction](https://arxiv.org/abs/2508.09195)
*Maria Boyko,Aleksandra Beliaeva,Dmitriy Kornilov,Alexander Bernstein,Maxim Sharaev*

Main category: eess.IV

TL;DR: impuTMAE是一种基于Transformer的多模态预训练方法，能够处理缺失模态数据并提升脑胶质瘤生存预测性能。


<details>
  <summary>Details</summary>
Motivation: 多模态数据（如组学、医学影像和临床数据）可提升预后模型性能，但数据复杂且常缺失，需有效处理方法。

Method: 提出impuTMAE，通过掩码重建学习模态间和模态内交互，预训练后微调用于脑胶质瘤生存预测。

Result: 在TCGA-GBM/LGG和BraTS数据集上表现优异，超越现有方法。

Conclusion: impuTMAE通过处理缺失数据和多模态预训练，实现了脑胶质瘤生存预测的先进性能。

Abstract: The use of diverse modalities, such as omics, medical images, and clinical
data can not only improve the performance of prognostic models but also deepen
an understanding of disease mechanisms and facilitate the development of novel
treatment approaches. However, medical data are complex, often incomplete, and
contains missing modalities, making effective handling its crucial for training
multimodal models. We introduce impuTMAE, a novel transformer-based end-to-end
approach with an efficient multimodal pre-training strategy. It learns inter-
and intra-modal interactions while simultaneously imputing missing modalities
by reconstructing masked patches. Our model is pre-trained on heterogeneous,
incomplete data and fine-tuned for glioma survival prediction using
TCGA-GBM/LGG and BraTS datasets, integrating five modalities: genetic (DNAm,
RNA-seq), imaging (MRI, WSI), and clinical data. By addressing missing data
during pre-training and enabling efficient resource utilization, impuTMAE
surpasses prior multimodal approaches, achieving state-of-the-art performance
in glioma patient survival prediction. Our code is available at
https://github.com/maryjis/mtcp

</details>


### [347] [FIVA: Federated Inverse Variance Averaging for Universal CT Segmentation with Uncertainty Estimation](https://arxiv.org/abs/2508.09196)
*Asim Ukaye,Numan Saeed,Karthik Nandakumar*

Main category: eess.IV

TL;DR: 提出了一种新的联邦学习方法，通过利用模型不确定性进行聚合和预测不确定性进行推理，实现跨多样腹部CT数据集的通用分割。


<details>
  <summary>Details</summary>
Motivation: 解决不同CT数据集因扫描器和捕获设置不同导致的异质性问题，同时保护患者隐私。

Method: 利用随机小批量梯度下降的固有噪声估计模型权重分布，采用贝叶斯启发的逆方差聚合方案在服务器端聚合参数，并通过传播模型权重的不确定性量化预测不确定性。

Result: 实验证明该方法在联邦聚合质量和不确定性加权推理方面优于现有基线。

Conclusion: 该方法有效提升了联邦学习的通用分割能力，并为临床决策提供了重要的置信度衡量。

Abstract: Different CT segmentation datasets are typically obtained from different
scanners under different capture settings and often provide segmentation labels
for a limited and often disjoint set of organs. Using these heterogeneous data
effectively while preserving patient privacy can be challenging. This work
presents a novel federated learning approach to achieve universal segmentation
across diverse abdominal CT datasets by utilizing model uncertainty for
aggregation and predictive uncertainty for inference. Our approach leverages
the inherent noise in stochastic mini-batch gradient descent to estimate a
distribution over the model weights to provide an on-the-go uncertainty over
the model parameters at the client level. The parameters are then aggregated at
the server using the additional uncertainty information using a
Bayesian-inspired inverse-variance aggregation scheme. Furthermore, the
proposed method quantifies prediction uncertainty by propagating the
uncertainty from the model weights, providing confidence measures essential for
clinical decision-making. In line with recent work shown, predictive
uncertainty is utilized in the inference stage to improve predictive
performance. Experimental evaluations demonstrate the effectiveness of this
approach in improving both the quality of federated aggregation and
uncertainty-weighted inference compared to previously established baselines.
The code for this work is made available at: https://github.com/asimukaye/fiva

</details>


### [348] [Zero-shot self-supervised learning of single breath-hold magnetic resonance cholangiopancreatography (MRCP) reconstruction](https://arxiv.org/abs/2508.09200)
*Jinho Kim,Marcel Dominik Nickel,Florian Knoll*

Main category: eess.IV

TL;DR: 零样本自监督学习重建技术可显著缩短MRCP的屏气时间，浅层训练方法进一步优化了计算效率。


<details>
  <summary>Details</summary>
Motivation: 研究旨在探索零样本自监督学习重建技术是否能在磁共振胰胆管造影（MRCP）中减少屏气时间，从而提升临床实用性。

Method: 通过对比屏气MRCP（14秒）与呼吸触发MRCP（338秒）的图像重建效果，评估零样本重建技术的性能。采用预训练网络减少反向传播深度，优化计算时间。

Result: 零样本重建在图像质量（如信噪比和导管清晰度）上优于压缩感知重建，接近呼吸触发MRCP的效果。浅层训练将训练时间从271分钟缩短至11分钟，性能几乎相同。

Conclusion: 零样本学习能实现高质量MRCP重建并缩短屏气时间，浅层训练为临床工作流程提供了实用解决方案。

Abstract: Purpose: To investigate the feasibility of applying zero-shot self-supervised
learning reconstruction to reduce breath-hold times in magnetic resonance
cholangiopancreatography (MRCP). Methods: Breath-hold MRCP was acquired from 11
healthy volunteers on a 3T scanner using an incoherent k-space sampling pattern
leading to a breath-hold duration of 14s. We evaluated zero-shot reconstruction
of breath-hold MRCP against parallel imaging of respiratory-triggered MRCP
acquired in 338s on average and compressed sensing reconstruction of
breath-hold MRCP. To address the long computation times of zero-shot trainings,
we used a training approach that leverages a pretrained network to reduce
backpropagation depth during training. Results: Zero-shot learning
reconstruction significantly improved visual image quality compared to
compressed sensing reconstruction, particularly in terms of signal-to-noise
ratio and ductal delineation, and reached a level of quality comparable to that
of successful respiratory-triggered acquisitions with regular breathing
patterns. Shallow training provided nearly equivalent reconstruction
performance with a training time of 11 minutes in comparison to 271 minutes for
a conventional zero-shot training. Conclusion: Zero-shot learning delivers
high-fidelity MRCP reconstructions with reduced breath-hold times, and shallow
training offers a practical solution for translation to time-constrained
clinical workflows.

</details>


### [349] [From Explainable to Explained AI: Ideas for Falsifying and Quantifying Explanations](https://arxiv.org/abs/2508.09205)
*Yoni Schirris,Eric Marcus,Jonas Teuwen,Hugo Horlings,Efstratios Gavves*

Main category: eess.IV

TL;DR: 论文提出了一种人机-VLM交互系统，用于解释计算病理学中的分类器，结合了多实例学习和视觉语言模型，以验证解释的预测性。


<details>
  <summary>Details</summary>
Motivation: 解释深度学习模型对医学图像分析系统的临床整合至关重要，以避免模型依赖虚假特征或发现新的生物学见解。

Method: 提出了一种交互系统，包括AI集成的幻灯片查看器和通用视觉语言模型，用于测试和量化解释的预测性。

Result: 系统能够定性测试解释的合理性，并量化区分竞争性解释，为可解释AI提供了实用路径。

Conclusion: 该方法为数字病理学及其他领域从可解释AI到解释AI提供了实际解决方案。

Abstract: Explaining deep learning models is essential for clinical integration of
medical image analysis systems. A good explanation highlights if a model
depends on spurious features that undermines generalization and harms a subset
of patients or, conversely, may present novel biological insights. Although
techniques like GradCAM can identify influential features, they are measurement
tools that do not themselves form an explanation. We propose a
human-machine-VLM interaction system tailored to explaining classifiers in
computational pathology, including multi-instance learning for whole-slide
images. Our proof of concept comprises (1) an AI-integrated slide viewer to run
sliding-window experiments to test claims of an explanation, and (2)
quantification of an explanation's predictiveness using general-purpose
vision-language models. The results demonstrate that this allows us to
qualitatively test claims of explanations and can quantifiably distinguish
competing explanations. This offers a practical path from explainable AI to
explained AI in digital pathology and beyond. Code and prompts are available at
https://github.com/nki-ai/x2x.

</details>


### [350] [AMRG: Extend Vision Language Models for Automatic Mammography Report Generation](https://arxiv.org/abs/2508.09225)
*Nak-Jun Sung,Donghyun Lee,Bo Hwa Choi,Chae Jung Park*

Main category: eess.IV

TL;DR: AMRG是首个用于生成乳腺X光报告的全端到端框架，基于大型视觉语言模型（VLM），通过参数高效微调（PEFT）实现高性能。


<details>
  <summary>Details</summary>
Motivation: 乳腺X光报告生成在医学AI中是一个重要但未被充分探索的任务，面临多视图图像推理、高分辨率视觉线索和非结构化放射学语言等挑战。

Method: 基于MedGemma-4B-it模型，采用低秩适应（LoRA）进行参数高效微调，训练和评估使用公开数据集DMID。

Result: 在语言生成和临床指标上表现优异，ROUGE-L为0.5691，METEOR为0.6152，CIDEr为0.5818，BI-RADS准确率为0.5582。

Conclusion: AMRG为放射学报告生成提供了可扩展和适应性强的框架，为未来多模态医学AI研究奠定了基础。

Abstract: Mammography report generation is a critical yet underexplored task in medical
AI, characterized by challenges such as multiview image reasoning,
high-resolution visual cues, and unstructured radiologic language. In this
work, we introduce AMRG (Automatic Mammography Report Generation), the first
end-to-end framework for generating narrative mammography reports using large
vision-language models (VLMs). Building upon MedGemma-4B-it-a
domain-specialized, instruction-tuned VLM-we employ a parameter-efficient
fine-tuning (PEFT) strategy via Low-Rank Adaptation (LoRA), enabling
lightweight adaptation with minimal computational overhead. We train and
evaluate AMRG on DMID, a publicly available dataset of paired high-resolution
mammograms and diagnostic reports. This work establishes the first reproducible
benchmark for mammography report generation, addressing a longstanding gap in
multimodal clinical AI. We systematically explore LoRA hyperparameter
configurations and conduct comparative experiments across multiple VLM
backbones, including both domain-specific and general-purpose models under a
unified tuning protocol. Our framework demonstrates strong performance across
both language generation and clinical metrics, achieving a ROUGE-L score of
0.5691, METEOR of 0.6152, CIDEr of 0.5818, and BI-RADS accuracy of 0.5582.
Qualitative analysis further highlights improved diagnostic consistency and
reduced hallucinations. AMRG offers a scalable and adaptable foundation for
radiology report generation and paves the way for future research in multimodal
medical AI.

</details>


### [351] [A Generative Imputation Method for Multimodal Alzheimer's Disease Diagnosis](https://arxiv.org/abs/2508.09271)
*Reihaneh Hassanzadeh,Anees Abrol,Hamid Reza Hassanzadeh,Vince D. Calhoun*

Main category: eess.IV

TL;DR: 提出了一种生成对抗网络方法，用于从现有模态中重建缺失的神经影像数据，提高了阿尔茨海默病分类准确性。


<details>
  <summary>Details</summary>
Motivation: 多模态数据在神经影像领域因数据缺失问题导致诊断准确性受限，传统方法存在偏差或精度不足。

Method: 使用生成对抗网络（GAN）从T1加权结构MRI和功能网络连接中重建缺失模态。

Result: 与传统方法相比，生成填补方法将阿尔茨海默病与认知正常组的分类准确性提高了9%。

Conclusion: 生成对抗网络方法能有效填补缺失模态，提升诊断准确性。

Abstract: Multimodal data analysis can lead to more accurate diagnoses of brain
disorders due to the complementary information that each modality adds.
However, a major challenge of using multimodal datasets in the neuroimaging
field is incomplete data, where some of the modalities are missing for certain
subjects. Hence, effective strategies are needed for completing the data.
Traditional methods, such as subsampling or zero-filling, may reduce the
accuracy of predictions or introduce unintended biases. In contrast, advanced
methods such as generative models have emerged as promising solutions without
these limitations. In this study, we proposed a generative adversarial network
method designed to reconstruct missing modalities from existing ones while
preserving the disease patterns. We used T1-weighted structural magnetic
resonance imaging and functional network connectivity as two modalities. Our
findings showed a 9% improvement in the classification accuracy for Alzheimer's
disease versus cognitive normal groups when using our generative imputation
method compared to the traditional approaches.

</details>


### [352] [HiFi-Mamba: Dual-Stream W-Laplacian Enhanced Mamba for High-Fidelity MRI Reconstruction](https://arxiv.org/abs/2508.09179)
*Hongli Chen,Pengcheng Fang,Yuxia Chen,Yingxuan Ren,Jing Hao,Fangfang Tang,Xiaohao Cai,Shanshan Shan,Feng Liu*

Main category: eess.IV

TL;DR: HiFi-Mamba是一种新型双流Mamba架构，用于从欠采样的k空间数据重建高保真MR图像，解决了传统方法的局限性，并在实验中表现优异。


<details>
  <summary>Details</summary>
Motivation: 传统Mamba变体在MRI重建中存在对高频解剖细节不敏感和依赖冗余多向扫描的问题，HiFi-Mamba旨在解决这些局限性。

Method: HiFi-Mamba采用双流架构，包括WL块和HiFi-Mamba块，前者进行保真度谱解耦，后者专注于低频结构和高频特征的选择性集成。

Result: 实验表明，HiFi-Mamba在重建精度上优于现有的CNN、Transformer和其他Mamba模型，同时保持高效设计。

Conclusion: HiFi-Mamba通过创新的双流架构和高效的单向遍历策略，显著提升了MRI重建的精度和效率。

Abstract: Reconstructing high-fidelity MR images from undersampled k-space data remains
a challenging problem in MRI. While Mamba variants for vision tasks offer
promising long-range modeling capabilities with linear-time complexity, their
direct application to MRI reconstruction inherits two key limitations: (1)
insensitivity to high-frequency anatomical details; and (2) reliance on
redundant multi-directional scanning. To address these limitations, we
introduce High-Fidelity Mamba (HiFi-Mamba), a novel dual-stream Mamba-based
architecture comprising stacked W-Laplacian (WL) and HiFi-Mamba blocks.
Specifically, the WL block performs fidelity-preserving spectral decoupling,
producing complementary low- and high-frequency streams. This separation
enables the HiFi-Mamba block to focus on low-frequency structures, enhancing
global feature modeling. Concurrently, the HiFi-Mamba block selectively
integrates high-frequency features through adaptive state-space modulation,
preserving comprehensive spectral details. To eliminate the scanning
redundancy, the HiFi-Mamba block adopts a streamlined unidirectional traversal
strategy that preserves long-range modeling capability with improved
computational efficiency. Extensive experiments on standard MRI reconstruction
benchmarks demonstrate that HiFi-Mamba consistently outperforms
state-of-the-art CNN-based, Transformer-based, and other Mamba-based models in
reconstruction accuracy while maintaining a compact and efficient model design.

</details>


### [353] [MedPatch: Confidence-Guided Multi-Stage Fusion for Multimodal Clinical Data](https://arxiv.org/abs/2508.09182)
*Baraa Al Jorf,Farah Shamout*

Main category: eess.IV

TL;DR: MedPatch是一种多阶段多模态融合架构，通过置信度引导的分块技术整合临床数据，解决了医学数据的异质性和稀疏性问题，并在预测任务中取得了最佳性能。


<details>
  <summary>Details</summary>
Motivation: 医学数据具有异质性、规模小和模态缺失的特点，限制了临床预测模型的性能。受临床工作流程启发，研究旨在通过多阶段融合策略提升模型表现。

Method: MedPatch包含三部分：多阶段融合策略、缺失感知模块和基于置信度的联合融合模块，整合时间序列、影像和文本数据。

Result: 在MIMIC数据集上的实验表明，MedPatch在院内死亡预测和临床条件分类任务中优于现有基线。

Conclusion: 置信度引导的多阶段融合有效解决了多模态数据的异质性，为临床预测任务设定了新的性能基准。

Abstract: Clinical decision-making relies on the integration of information across
various data modalities, such as clinical time-series, medical images and
textual reports. Compared to other domains, real-world medical data is
heterogeneous in nature, limited in size, and sparse due to missing modalities.
This significantly limits model performance in clinical prediction tasks.
Inspired by clinical workflows, we introduce MedPatch, a multi-stage multimodal
fusion architecture, which seamlessly integrates multiple modalities via
confidence-guided patching. MedPatch comprises three main components: (i) a
multi-stage fusion strategy that leverages joint and late fusion
simultaneously, (ii) a missingness-aware module that handles sparse samples
with missing modalities, (iii) a joint fusion module that clusters latent token
patches based on calibrated unimodal token-level confidence. We evaluated
MedPatch using real-world data consisting of clinical time-series data, chest
X-ray images, radiology reports, and discharge notes extracted from the
MIMIC-IV, MIMIC-CXR, and MIMIC-Notes datasets on two benchmark tasks, namely
in-hospital mortality prediction and clinical condition classification.
Compared to existing baselines, MedPatch achieves state-of-the-art performance.
Our work highlights the effectiveness of confidence-guided multi-stage fusion
in addressing the heterogeneity of multimodal data, and establishes new
state-of-the-art benchmark results for clinical prediction tasks.

</details>


### [354] [Dynamic Survival Prediction using Longitudinal Images based on Transformer](https://arxiv.org/abs/2508.09328)
*Bingfan Liu,Haolun Shi,Jiguo Cao*

Main category: eess.IV

TL;DR: SurLonFormer是一种基于Transformer的神经网络，用于整合纵向医学影像和结构化数据以进行生存预测，解决了现有方法在利用截尾数据、时间相关性及可解释性方面的不足。


<details>
  <summary>Details</summary>
Motivation: 现有方法在利用截尾数据、处理纵向影像相关性及可解释性方面存在不足，需要一种更有效的生存分析工具。

Method: 提出SurLonFormer，包含视觉编码器（提取空间特征）、序列编码器（聚合时间信息）和生存编码器（基于Cox比例风险模型）。

Result: 在模拟和实际阿尔茨海默病分析中，SurLonFormer表现出优越的预测性能，并成功识别疾病相关影像生物标志物。

Conclusion: SurLonFormer通过整合多模态数据和增强可解释性，为生存分析提供了更有效的解决方案。

Abstract: Survival analysis utilizing multiple longitudinal medical images plays a
pivotal role in the early detection and prognosis of diseases by providing
insight beyond single-image evaluations. However, current methodologies often
inadequately utilize censored data, overlook correlations among longitudinal
images measured over multiple time points, and lack interpretability. We
introduce SurLonFormer, a novel Transformer-based neural network that
integrates longitudinal medical imaging with structured data for survival
prediction. Our architecture comprises three key components: a Vision Encoder
for extracting spatial features, a Sequence Encoder for aggregating temporal
information, and a Survival Encoder based on the Cox proportional hazards
model. This framework effectively incorporates censored data, addresses
scalability issues, and enhances interpretability through occlusion sensitivity
analysis and dynamic survival prediction. Extensive simulations and a
real-world application in Alzheimer's disease analysis demonstrate that
SurLonFormer achieves superior predictive performance and successfully
identifies disease-related imaging biomarkers.

</details>


### [355] [T-CACE: A Time-Conditioned Autoregressive Contrast Enhancement Multi-Task Framework for Contrast-Free Liver MRI Synthesis, Segmentation, and Diagnosis](https://arxiv.org/abs/2508.09919)
*Xiaojiao Xiao,Jianfeng Zhao,Qinmin Vivian Hu,Guanghui Wang*

Main category: eess.IV

TL;DR: T-CACE框架通过合成多期相增强MRI，解决了传统MRI的对比剂风险、耗时手动评估和标注数据不足的问题。


<details>
  <summary>Details</summary>
Motivation: 传统MRI存在对比剂风险、评估耗时和标注数据有限的问题，需要一种更安全高效的替代方法。

Method: T-CACE结合条件令牌编码（CTE）、动态时间感知注意力掩码（DTAM）和时间分类一致性约束（TCC），直接从非增强MRI合成多期相增强MRI。

Result: 在肝脏MRI数据集上，T-CACE在图像合成、分割和病灶分类方面优于现有方法。

Conclusion: T-CACE提供了一种安全高效的替代方案，提升了肝脏病灶评估的诊断效率和可靠性。

Abstract: Magnetic resonance imaging (MRI) is a leading modality for the diagnosis of
liver cancer, significantly improving the classification of the lesion and
patient outcomes. However, traditional MRI faces challenges including risks
from contrast agent (CA) administration, time-consuming manual assessment, and
limited annotated datasets. To address these limitations, we propose a
Time-Conditioned Autoregressive Contrast Enhancement (T-CACE) framework for
synthesizing multi-phase contrast-enhanced MRI (CEMRI) directly from
non-contrast MRI (NCMRI). T-CACE introduces three core innovations: a
conditional token encoding (CTE) mechanism that unifies anatomical priors and
temporal phase information into latent representations; and a dynamic
time-aware attention mask (DTAM) that adaptively modulates inter-phase
information flow using a Gaussian-decayed attention mechanism, ensuring smooth
and physiologically plausible transitions across phases. Furthermore, a
constraint for temporal classification consistency (TCC) aligns the lesion
classification output with the evolution of the physiological signal, further
enhancing diagnostic reliability. Extensive experiments on two independent
liver MRI datasets demonstrate that T-CACE outperforms state-of-the-art methods
in image synthesis, segmentation, and lesion classification. This framework
offers a clinically relevant and efficient alternative to traditional
contrast-enhanced imaging, improving safety, diagnostic efficiency, and
reliability for the assessment of liver lesion. The implementation of T-CACE is
publicly available at: https://github.com/xiaojiao929/T-CACE.

</details>


<div id='astro-ph.SR'></div>

# astro-ph.SR [[Back]](#toc)

### [356] [Classifying Cool Dwarfs: Comprehensive Spectral Typing of Field and Peculiar Dwarfs Using Machine Learning](https://arxiv.org/abs/2508.09370)
*Tianxing Zhou,Christopher A. Theissen,S. Jean Feeser,William M. J. Best,Adam J. Burgasser,Kelle L. Cruz,Lexu Zhao*

Main category: astro-ph.SR

TL;DR: 论文探讨了机器学习在低分辨率近红外光谱分类中的应用，特别是对M0-T9矮星的表面重力和金属丰度子类分类。


<details>
  <summary>Details</summary>
Motivation: 低质量恒星和褐矮星的研究对理解恒星和亚恒星过程及人口统计至关重要，但目前分类仍依赖人工方法，机器学习为大规模光谱数据提供了自动化解决方案。

Method: 使用随机森林（RF）、支持向量机（SVM）和K近邻（KNN）模型，以分箱通量作为输入特征，测试不同归一化方法及光谱区域的重要性。

Result: 最佳模型（KNN）在±1光谱类型内分类准确率为95.5±0.6%，表面重力和金属丰度子类分类准确率为89.5±0.9%。信噪比≥60时准确率≥95%。

Conclusion: 机器学习能高效分类低分辨率光谱，zy波段和FeH、TiO特征在RF模型中最为重要。

Abstract: Low-mass stars and brown dwarfs -- spectral types (SpTs) M0 and later -- play
a significant role in studying stellar and substellar processes and
demographics, reaching down to planetary-mass objects. Currently, the
classification of these sources remains heavily reliant on visual inspection of
spectral features, equivalent width measurements, or narrow-/wide-band spectral
indices. Recent advances in machine learning (ML) methods offer automated
approaches for spectral typing, which are becoming increasingly important as
large spectroscopic surveys such as Gaia, SDSS, and SPHEREx generate datasets
containing millions of spectra. We investigate the application of ML in
spectral type classification on low-resolution (R $\sim$ 120) near-infrared
spectra of M0--T9 dwarfs obtained with the SpeX instrument on the NASA Infrared
Telescope Facility. We specifically aim to classify the gravity- and
metallicity-dependent subclasses for late-type dwarfs. We used binned fluxes as
input features and compared the efficacy of spectral type estimators built
using Random Forest (RF), Support Vector Machine (SVM), and K-Nearest Neighbor
(KNN) models. We tested the influence of different normalizations and analyzed
the relative importance of different spectral regions for surface gravity and
metallicity subclass classification. Our best-performing model (using KNN)
classifies 95.5 $\pm$ 0.6% of sources to within $\pm$1 SpT, and assigns surface
gravity and metallicity subclasses with 89.5 $\pm$ 0.9% accuracy. We test the
dependence of signal-to-noise ratio on classification accuracy and find sources
with SNR $\gtrsim$ 60 have $\gtrsim$ 95% accuracy. We also find that zy-band
plays the most prominent role in the RF model, with FeH and TiO having the
highest feature importance.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [357] [Can AI Keep a Secret? Contextual Integrity Verification: A Provable Security Architecture for LLMs](https://arxiv.org/abs/2508.09288)
*Aayush Gupta*

Main category: cs.CR

TL;DR: 论文提出了一种名为CIV的安全架构，通过加密签名和信任源格栅技术，有效防御LLMs的提示注入攻击，同时保持模型性能。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）容易受到提示注入和越狱攻击，现有防护措施常被绕过，亟需更可靠的解决方案。

Method: 采用Contextual Integrity Verification（CIV）架构，通过加密签名标记每个token，并在transformer中实施信任源格栅，确保低信任token不影响高信任表示。

Result: 在Elite-Attack和SoK-246基准测试中，CIV实现了0%攻击成功率，同时保持93.1%的token相似性和模型困惑度不变。

Conclusion: CIV是一种轻量级补丁，无需微调即可为LLMs提供确定性保护，适用于Llama-3-8B和Mistral-7B等模型。

Abstract: Large language models (LLMs) remain acutely vulnerable to prompt injection
and related jailbreak attacks; heuristic guardrails (rules, filters, LLM
judges) are routinely bypassed. We present Contextual Integrity Verification
(CIV), an inference-time security architecture that attaches cryptographically
signed provenance labels to every token and enforces a source-trust lattice
inside the transformer via a pre-softmax hard attention mask (with optional
FFN/residual gating). CIV provides deterministic, per-token non-interference
guarantees on frozen models: lower-trust tokens cannot influence higher-trust
representations. On benchmarks derived from recent taxonomies of
prompt-injection vectors (Elite-Attack + SoK-246), CIV attains 0% attack
success rate under the stated threat model while preserving 93.1% token-level
similarity and showing no degradation in model perplexity on benign tasks; we
note a latency overhead attributable to a non-optimized data path. Because CIV
is a lightweight patch -- no fine-tuning required -- we demonstrate drop-in
protection for Llama-3-8B and Mistral-7B. We release a reference
implementation, an automated certification harness, and the Elite-Attack corpus
to support reproducible research.

</details>


### [358] [Learning to Detect Unknown Jailbreak Attacks in Large Vision-Language Models: A Unified and Accurate Approach](https://arxiv.org/abs/2508.09201)
*Shuang Liang,Zhihao Xu,Jialing Tao,Hui Xue,Xiting Wang*

Main category: cs.CR

TL;DR: 论文提出了一种名为LoD的无监督框架，通过异常检测方法解决大视觉语言模型（LVLM）的越狱攻击问题，无需攻击标签即可实现高效检测。


<details>
  <summary>Details</summary>
Motivation: 尽管已有研究尝试通过内部表征检测越狱攻击，但大多依赖启发式规则，性能不佳。LoD旨在通过无监督学习提升检测效果。

Method: LoD包含两个核心组件：多模态安全概念激活向量（MSCAV）捕捉跨模态安全表征，安全模式自编码器（AE）通过重构误差检测异常。

Result: 在三个LVLM和五个基准测试中，LoD的平均AUROC达到0.9951，最小AUROC提升38.89%，性能优于现有方法。

Conclusion: LoD通过无监督异常检测实现了高效、统一的越狱攻击检测，为LVLM安全性提供了新思路。

Abstract: Despite extensive alignment efforts, Large Vision-Language Models (LVLMs)
remain vulnerable to jailbreak attacks, posing serious safety risks. Although
recent detection works have shifted to internal representations due to their
rich cross-modal information, most methods rely on heuristic rules rather than
principled objectives, resulting in suboptimal performance. To address these
limitations, we propose Learning to Detect (LoD), a novel unsupervised
framework that formulates jailbreak detection as anomaly detection. LoD
introduces two key components: Multi-modal Safety Concept Activation Vectors
(MSCAV), which capture layer-wise safety-related representations across
modalities, and the Safety Pattern Auto-Encoder, which models the distribution
of MSCAV derived from safe inputs and detects anomalies via reconstruction
errors. By training the auto-encoder (AE) solely on safe samples without attack
labels, LoD naturally identifies jailbreak inputs as distributional anomalies,
enabling accurate and unified detection of jailbreak attacks. Comprehensive
experiments on three different LVLMs and five benchmarks demonstrate that LoD
achieves state-of-the-art performance, with an average AUROC of 0.9951 and an
improvement of up to 38.89% in the minimum AUROC over the strongest baselines.

</details>


### [359] [Shadow in the Cache: Unveiling and Mitigating Privacy Risks of KV-cache in LLM Inference](https://arxiv.org/abs/2508.09442)
*Zhifan Luo,Shuo Shao,Su Zhang,Lijing Zhou,Yuke Hu,Chenxu Zhao,Zhihao Liu,Zhan Qin*

Main category: cs.CR

TL;DR: 本文首次全面分析了KV缓存的隐私风险，提出三种攻击方法并设计防御机制KV-Cloak，实验证明其有效性且不影响模型性能。


<details>
  <summary>Details</summary>
Motivation: KV缓存虽加速LLM推理，但存在未充分研究的隐私风险，攻击者可从中重构敏感输入。

Method: 提出三种攻击方法（直接反转、碰撞、语义注入），并设计防御机制KV-Cloak，采用可逆矩阵混淆和算子融合。

Result: KV-Cloak有效抵御攻击，将重构质量降至随机噪声水平，且不影响模型准确性和性能。

Conclusion: KV-Cloak为LLM部署提供了轻量、高效的隐私保护解决方案。

Abstract: The Key-Value (KV) cache, which stores intermediate attention computations
(Key and Value pairs) to avoid redundant calculations, is a fundamental
mechanism for accelerating Large Language Model (LLM) inference. However, this
efficiency optimization introduces significant yet underexplored privacy risks.
This paper provides the first comprehensive analysis of these vulnerabilities,
demonstrating that an attacker can reconstruct sensitive user inputs directly
from the KV-cache. We design and implement three distinct attack vectors: a
direct Inversion Attack, a more broadly applicable and potent Collision Attack,
and a semantic-based Injection Attack. These methods demonstrate the
practicality and severity of KV-cache privacy leakage issues. To mitigate this,
we propose KV-Cloak, a novel, lightweight, and efficient defense mechanism.
KV-Cloak uses a reversible matrix-based obfuscation scheme, combined with
operator fusion, to secure the KV-cache. Our extensive experiments show that
KV-Cloak effectively thwarts all proposed attacks, reducing reconstruction
quality to random noise. Crucially, it achieves this robust security with
virtually no degradation in model accuracy and minimal performance overhead,
offering a practical solution for trustworthy LLM deployment.

</details>


### [360] [Social-Sensor Identity Cloning Detection Using Weakly Supervised Deep Forest and Cryptographic Authentication](https://arxiv.org/abs/2508.09665)
*Ahmed Alharbi,Hai Dong,Xun Yi*

Main category: cs.CR

TL;DR: 提出了一种检测社交传感器云身份克隆的新方法，结合相似身份检测和加密认证协议，性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有方法性能不足，缺乏重复账户检测方案，且未在大规模真实数据集上验证。

Method: 1) 弱监督深度森林模型检测相似身份；2) 加密认证协议验证身份是否来自同一提供商。

Result: 在大规模真实数据集上验证了方法的可行性和优越性能。

Conclusion: 该方法在身份克隆检测中表现出色，优于现有技术。

Abstract: Recent years have witnessed a rising trend in social-sensor cloud identity
cloning incidents. However, existing approaches suffer from unsatisfactory
performance, a lack of solutions for detecting duplicated accounts, and a lack
of large-scale evaluations on real-world datasets. We introduce a novel method
for detecting identity cloning in social-sensor cloud service providers. Our
proposed technique consists of two primary components: 1) a similar identity
detection method and 2) a cryptography-based authentication protocol.
Initially, we developed a weakly supervised deep forest model to identify
similar identities using non-privacy-sensitive user profile features provided
by the service. Subsequently, we designed a cryptography-based authentication
protocol to verify whether similar identities were generated by the same
provider. Our extensive experiments on a large real-world dataset demonstrate
the feasibility and superior performance of our technique compared to current
state-of-the-art identity clone detection methods.

</details>


### [361] [Demystifying the Role of Rule-based Detection in AI Systems for Windows Malware Detection](https://arxiv.org/abs/2508.09652)
*Andrea Ponte,Luca Demetrio,Luca Oneto,Ivan Tesfai Ogbu,Battista Biggio,Fabio Roli*

Main category: cs.CR

TL;DR: 论文研究了将签名检测与机器学习结合的AI系统对模型训练的影响，发现能提升对抗性样本和时序数据漂移的鲁棒性，但会引入固定的假阳性下限。


<details>
  <summary>Details</summary>
Motivation: 传统方法中签名检测与机器学习组件独立开发，未能充分利用数据复杂性降低和对抗性样本防御的潜力。

Method: 比较了在完整数据集上训练的模型与仅基于未被签名检测标记样本训练的AI系统。

Result: 结果显示提升了对抗性样本和时序数据漂移的鲁棒性，但存在固定的假阳性下限。

Conclusion: 未来研究可结合动态分析以进一步提升系统韧性。

Abstract: Malware detection increasingly relies on AI systems that integrate
signature-based detection with machine learning. However, these components are
typically developed and combined in isolation, missing opportunities to reduce
data complexity and strengthen defenses against adversarial EXEmples, carefully
crafted programs designed to evade detection. Hence, in this work we
investigate the influence that signature-based detection exerts on model
training, when they are included inside the training pipeline. Specifically, we
compare models trained on a comprehensive dataset with an AI system whose
machine learning component is trained solely on samples not already flagged by
signatures. Our results demonstrate improved robustness to both adversarial
EXEmples and temporal data drift, although this comes at the cost of a fixed
lower bound on false positives, driven by suboptimal rule selection. We
conclude by discussing these limitations and outlining how future research
could extend AI-based malware detection to include dynamic analysis, thereby
further enhancing system resilience.

</details>


### [362] [Enhance the machine learning algorithm performance in phishing detection with keyword features](https://arxiv.org/abs/2508.09765)
*Zijiang Yang*

Main category: cs.CR

TL;DR: 论文提出了一种结合关键词特征与传统特征的新方法，用于提升钓鱼URL检测的机器学习算法效果，实验表明该方法能显著降低分类错误率。


<details>
  <summary>Details</summary>
Motivation: 钓鱼攻击日益增多，导致用户敏感信息泄露和财务损失，早期检测钓鱼URL至关重要。

Method: 提出一种结合关键词特征与传统特征的新方法，应用于多种传统机器学习算法。

Result: 该方法平均减少大型数据集的分类错误率30%，对小型数据集效果更显著，最佳算法准确率达99.68%。

Conclusion: 该方法有效提升了钓鱼URL检测的准确性，且不依赖第三方服务提供额外信息。

Abstract: Recently, we can observe a significant increase of the phishing attacks in
the Internet. In a typical phishing attack, the attacker sets up a malicious
website that looks similar to the legitimate website in order to obtain the
end-users' information. This may cause the leakage of the sensitive information
and the financial loss for the end-users. To avoid such attacks, the early
detection of these websites' URLs is vital and necessary. Previous researchers
have proposed many machine learning algorithms to distinguish the phishing URLs
from the legitimate ones. In this paper, we would like to enhance these machine
learning algorithms from the perspective of feature selection. We propose a
novel method to incorporate the keyword features with the traditional features.
This method is applied on multiple traditional machine learning algorithms and
the experimental results have shown this method is useful and effective. On
average, this method can reduce the classification error by 30% for the large
dataset. Moreover, its enhancement is more significant for the small dataset.
In addition, this method extracts the information from the URL and does not
rely on the additional information provided by the third-part service. The best
result for the machine learning algorithm using our proposed method has
achieved the accuracy of 99.68%.

</details>


### [363] [Explainable Ensemble Learning for Graph-Based Malware Detection](https://arxiv.org/abs/2508.09801)
*Hossein Shokouhinejad,Roozbeh Razavi-Far,Griffin Higgins,Ali A Ghorbani*

Main category: cs.CR

TL;DR: 提出了一种基于图神经网络的堆叠集成框架，用于恶意软件检测和解释，通过动态提取控制流图并利用多样化的GNN基学习器提升分类性能和可解释性。


<details>
  <summary>Details</summary>
Motivation: 现代计算环境需要准确、可解释且能抵抗规避技术的恶意软件检测模型，而单模型方法在泛化和可解释性上存在不足。

Method: 动态提取PE文件中的控制流图，采用两步嵌入策略编码基本块，使用多样化的GNN基学习器捕获互补特征，并通过注意力机制的多层感知机聚合预测结果。

Result: 实验表明，该框架提高了分类性能，并提供了对恶意软件行为的深入解释。

Conclusion: 提出的堆叠集成框架在恶意软件检测中表现出优越性能，同时增强了模型的可解释性。

Abstract: Malware detection in modern computing environments demands models that are
not only accurate but also interpretable and robust to evasive techniques.
Graph neural networks (GNNs) have shown promise in this domain by modeling rich
structural dependencies in graph-based program representations such as control
flow graphs (CFGs). However, single-model approaches may suffer from limited
generalization and lack interpretability, especially in high-stakes security
applications. In this paper, we propose a novel stacking ensemble framework for
graph-based malware detection and explanation. Our method dynamically extracts
CFGs from portable executable (PE) files and encodes their basic blocks through
a two-step embedding strategy. A set of diverse GNN base learners, each with a
distinct message-passing mechanism, is used to capture complementary behavioral
features. Their prediction outputs are aggregated by a meta-learner implemented
as an attention-based multilayer perceptron, which both classifies malware
instances and quantifies the contribution of each base model. To enhance
explainability, we introduce an ensemble-aware post-hoc explanation technique
that leverages edge-level importance scores generated by a GNN explainer and
fuses them using the learned attention weights. This produces interpretable,
model-agnostic explanations aligned with the final ensemble decision.
Experimental results demonstrate that our framework improves classification
performance while providing insightful interpretations of malware behavior.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [364] [Quantum-Efficient Reinforcement Learning Solutions for Last-Mile On-Demand Delivery](https://arxiv.org/abs/2508.09183)
*Farzan Moosavi,Bilal Farooq*

Main category: quant-ph

TL;DR: 论文提出了一种结合强化学习和参数化量子电路的框架，用于解决大规模CPDPTW问题，展示了量子计算在优化问题中的潜力。


<details>
  <summary>Details</summary>
Motivation: 经典方法在大规模组合优化问题中难以处理，量子计算提供了一种有前景的替代方案。

Method: 设计了基于强化学习的框架，结合参数化量子电路，并提出了一种特定问题的量子编码电路。

Result: 通过数值实验验证了所提方法在解决方案规模和训练复杂度上的优越性。

Conclusion: 量子计算结合强化学习能够有效解决大规模CPDPTW问题，具有实际应用潜力。

Abstract: Quantum computation has demonstrated a promising alternative to solving the
NP-hard combinatorial problems. Specifically, when it comes to optimization,
classical approaches become intractable to account for large-scale solutions.
Specifically, we investigate quantum computing to solve the large-scale
Capacitated Pickup and Delivery Problem with Time Windows (CPDPTW). In this
regard, a Reinforcement Learning (RL) framework augmented with a Parametrized
Quantum Circuit (PQC) is designed to minimize the travel time in a realistic
last-mile on-demand delivery. A novel problem-specific encoding quantum circuit
with an entangling and variational layer is proposed. Moreover, Proximal Policy
Optimization (PPO) and Quantum Singular Value Transformation (QSVT) are
designed for comparison through numerical experiments, highlighting the
superiority of the proposed method in terms of the scale of the solution and
training complexity while incorporating the real-world constraints.

</details>


### [365] [Quantum-Enhanced Generative Adversarial Networks: Comparative Analysis of Classical and Hybrid Quantum-Classical Generative Adversarial Networks](https://arxiv.org/abs/2508.09209)
*Kun Ming Goh*

Main category: quant-ph

TL;DR: 研究探讨了混合量子-经典生成对抗网络（HQCGANs）的可行性，发现7量子比特的HQCGAN在性能上接近经典GAN，验证了在NISQ时代量子电路作为潜在先验的潜力。


<details>
  <summary>Details</summary>
Motivation: 传统GAN的性能受限于经典噪声分布生成的潜在表示，研究旨在探索量子电路能否提升生成模型的质量。

Method: 使用Qiskit的AerSimulator模拟噪声模型，比较经典GAN与3、5、7量子比特的HQCGAN在MNIST数据集上的表现。

Result: 经典GAN表现最佳，但7量子比特HQCGAN接近其性能，3量子比特模型收敛受限。量子采样仅适度增加训练时间。

Conclusion: 研究证实了在NISQ时代量子电路作为潜在先验的可行性，为生成模型提供了新方向。

Abstract: Generative adversarial networks (GANs) have emerged as a powerful paradigm
for producing high-fidelity data samples, yet their performance is constrained
by the quality of latent representations, typically sampled from classical
noise distributions. This study investigates hybrid quantum-classical GANs
(HQCGANs) in which a quantum generator, implemented via parameterised quantum
circuits, produces latent vectors for a classical discriminator. We evaluate a
classical GAN alongside three HQCGAN variants with 3, 5, and 7 qubits, using
Qiskit's AerSimulator with realistic noise models to emulate near-term quantum
devices. The binary MNIST dataset (digits 0 and 1) is used to align with the
low-dimensional latent spaces imposed by current quantum hardware. Models are
trained for 150 epochs and assessed with Frechet Inception Distance (FID) and
Kernel Inception Distance (KID). Results show that while the classical GAN
achieved the best scores, the 7-qubit HQCGAN produced competitive performance,
narrowing the gap in later epochs, whereas the 3-qubit model exhibited earlier
convergence limitations. Efficiency analysis indicates only moderate training
time increases despite quantum sampling overhead. These findings validate the
feasibility of noisy quantum circuits as latent priors in GAN architectures,
highlighting their potential to enhance generative modelling within the
constraints of the noisy intermediate-scale quantum (NISQ) era.

</details>


### [366] [On the Generalization Limits of Quantum Generative Adversarial Networks with Pure State Generators](https://arxiv.org/abs/2508.09844)
*Jasmin Frkatovic,Akash Malemath,Ivan Kankeu,Yannick Werner,Matthias Tschöpe,Vitor Fortes Rey,Sungho Suh,Paul Lukowicz,Nikolaos Palaiodimopoulos,Maximilian Kiefer-Emmanouilidis*

Main category: quant-ph

TL;DR: 研究探讨了量子生成对抗网络（QGANs）在图像生成任务中的表现，发现其泛化能力有限，仅能收敛到训练数据的平均表示。


<details>
  <summary>Details</summary>
Motivation: 探索QGANs在图像生成任务中的能力，特别是全量子实现的生成器和判别器的表现。

Method: 通过数值测试和分析推导，评估QGANs的泛化能力，并给出判别器质量的理论下界。

Result: QGANs在泛化能力上存在根本性挑战，仅能生成训练数据的平均表示。

Conclusion: 研究揭示了现有量子生成模型的局限性，对相关量子生成模型的性能具有广泛启示。

Abstract: We investigate the capabilities of Quantum Generative Adversarial Networks
(QGANs) in image generations tasks. Our analysis centers on fully quantum
implementations of both the generator and discriminator. Through extensive
numerical testing of current main architectures, we find that QGANs struggle to
generalize across datasets, converging on merely the average representation of
the training data. When the output of the generator is a pure-state, we
analytically derive a lower bound for the discriminator quality given by the
fidelity between the pure-state output of the generator and the target data
distribution, thereby providing a theoretical explanation for the limitations
observed in current models. Our findings reveal fundamental challenges in the
generalization capabilities of existing quantum generative models. While our
analysis focuses on QGANs, the results carry broader implications for the
performance of related quantum generative models.

</details>


<div id='physics.soc-ph'></div>

# physics.soc-ph [[Back]](#toc)

### [367] [The 2R-Conjecture for the Hegselmann--Krause Model: A Proof in Expectation and New Directions](https://arxiv.org/abs/2508.08299)
*Partha S. Dey,S. Rasoul Etesami,Aditya S. Gopalan*

Main category: physics.soc-ph

TL;DR: 本文研究了Hegselmann-Krause模型的“2R猜想”，通过无限维点过程方法，证明了期望结果并提出了更强结论。


<details>
  <summary>Details</summary>
Motivation: Hegselmann-Krause模型在空间数据中形成聚类，但其固定点结构问题长期未解，尤其是“2R猜想”自2007年提出后无实质进展。

Method: 将动态视为无限维点过程问题，利用平稳性、平移不变性等对称性，突破了有限维方法的限制。

Result: 在期望下证明了“2R猜想”，并通过仿真支持了更强结论。

Conclusion: 无限维方法为Hegselmann-Krause模型的固定点结构提供了新视角，解决了长期未决的问题。

Abstract: Hegselmann--Krause models are localized, distributed averaging dynamics on
spatial data. A key aspect of these dynamics is that they lead to cluster
formation, which has important applications in geographic information systems,
dynamic clustering algorithms, opinion dynamics, and social networks. For these
models, the key questions are whether a fixed point exists and, if so,
characterizing it. In this work, we establish new results towards the
"2R-Conjecture" for the Hegselmann--Krause model, for which no meaningful
progress, or even any precise statement, has been made since its introduction
in 2007. This conjecture relates to the structure of the fixed point when there
are a large number of agents per unit space. We provide, among other results, a
proof in expectation and a statement of a stronger result that is supported by
simulation. The key methodological contribution is to consider the dynamics as
an infinite-dimensional problem on the space of point processes, rather than on
finitely many points. This enables us to leverage stationarity, shift
invariance, and certain other symmetries to obtain the results. These
techniques do not have finite-dimensional analogs.

</details>


### [368] [Safety Perspective on Assisted Lane Changes: Insights from Open-Road, Live-Traffic Experiments](https://arxiv.org/abs/2508.09233)
*Konstantinos Mattas,Sandor Vass,Gergely Zachar,Junyi Ji,Derek Gloudemans,Davide Maggi,Akos Kriston,Mohamed Brahmi,Maria Christina Galassi,Daniel B Work,Biagio Ciuffo*

Main category: physics.soc-ph

TL;DR: 研究了五种配备ADAS的车辆的辅助变道功能，发现其变道行为与人类驾驶员不同，部分系统可能引发安全问题。


<details>
  <summary>Details</summary>
Motivation: 探索商业化ADAS技术中尚未充分研究的辅助变道功能，评估其在实际道路中的表现。

Method: 在I-24高速公路上进行实验，收集变道时的运动学和安全间距数据，并通过模拟方法评估对其他车辆的挑战程度。

Result: 四种车辆变道速度较慢，一种表现更积极；部分系统未遵守UN法规，可能引发交通冲击波。

Conclusion: ADAS辅助变道功能存在安全隐患，需进一步优化以确保安全和交通流畅。

Abstract: This study investigates the assisted lane change functionality of five
different vehicles equipped with advanced driver assistance systems (ADAS). The
goal is to examine novel, under-researched features of commercially available
ADAS technologies. The experimental campaign, conducted in the I-24 highway
near Nashville, TN, US, collected data on the kinematics and safety margins of
assisted lane changes in real-world conditions. The results show that the
kinematics of assisted lane changes are consistent for each system, with four
out of five vehicles using slower speeds and decelerations than human drivers.
However, one system consistently performed more assertive lane changes,
completing the maneuver in around 5 seconds. Regarding safety margins, only
three vehicles are investigated. Those operated in the US are not restricted by
relevant UN regulations, and their designs were found not to adhere to these
regulatory requirements. A simulation method used to classify the challenge
level for the vehicle receiving the lane change, showing that these systems can
force trailing vehicles to decelerate to keep a safe gap. One assisted system
was found to have performed a maneuver that posed a hard challenge level for
the other vehicle, raising concerns about the safety of these systems in
real-world operation. All three vehicles were found to carry out lane changes
that induced decelerations to the vehicle in the target lane. Those
decelerations could affect traffic flow, inducing traffic shockwaves.

</details>


<div id='q-bio.QM'></div>

# q-bio.QM [[Back]](#toc)

### [369] [Real-time deep learning phase imaging flow cytometer reveals blood cell aggregate biomarkers for haematology diagnostics](https://arxiv.org/abs/2508.09215)
*Kerem Delikoyun,Qianyu Chen,Liu Wei,Si Ko Myo,Johannes Krell,Martin Schlegel,Win Sen Kuan,John Tshon Yit Soong,Gerhard Schneider,Clarissa Prazeres da Costa,Percy A. Knolle,Laurent Renia,Matthew Edward Cove,Hwee Kuan Lee,Klaus Diepold,Oliver Hayden*

Main category: q-bio.QM

TL;DR: RT-HAD是一种基于深度学习的端到端图像数据处理框架，用于离轴数字全息显微镜（DHM），能够实时处理大量图像数据，识别血细胞聚集体。


<details>
  <summary>Details</summary>
Motivation: 传统流式细胞仪无法识别血细胞聚集体，而定量相位成像流式细胞技术虽能捕获聚集体形态，但数据存储和处理问题阻碍了临床应用。RT-HAD旨在解决这些问题，提升无标记功能诊断的效率。

Method: RT-HAD结合物理一致的全息重建和检测方法，将每个血细胞表示为图中的节点以识别聚集体，能够实时处理超过30GB的图像数据。

Result: RT-HAD在血小板聚集体检测中的错误率为8.9%，与可接受的实验室错误率相当，处理时间小于1.5分钟。

Conclusion: RT-HAD解决了即时诊断中的大数据挑战，显著提升了血细胞聚集体检测的效率，具有临床应用潜力。

Abstract: While analysing rare blood cell aggregates remains challenging in automated
haematology, they could markedly advance label-free functional diagnostics.
Conventional flow cytometers efficiently perform cell counting with leukocyte
differentials but fail to identify aggregates with flagged results, requiring
manual reviews. Quantitative phase imaging flow cytometry captures detailed
aggregate morphologies, but clinical use is hampered by massive data storage
and offline processing. Incorporating hidden biomarkers into routine
haematology panels would significantly improve diagnostics without flagged
results. We present RT-HAD, an end-to-end deep learning-based image and data
processing framework for off-axis digital holographic microscopy (DHM), which
combines physics-consistent holographic reconstruction and detection,
representing each blood cell in a graph to recognize aggregates. RT-HAD
processes >30 GB of image data on-the-fly with turnaround time of <1.5 min and
error rate of 8.9% in platelet aggregate detection, which matches acceptable
laboratory error rates of haematology biomarkers and solves the big data
challenge for point-of-care diagnostics.

</details>


### [370] [Cross-BCI, A Cross-BCI-Paradigm Classifica-tion Model Towards Universal BCI Applications](https://arxiv.org/abs/2508.09242)
*Gaojie Zhou,Junhua Li*

Main category: q-bio.QM

TL;DR: 提出了一种轻量级、统一的跨BCI范式分类模型，显著优于现有模型。


<details>
  <summary>Details</summary>
Motivation: 现有BCI分类模型通常针对单一范式设计，重新开发成本高，且需要轻量级模型以适应实际应用和便携设备。

Method: 模型包括时空卷积、多尺度局部特征选择模块和多维全局特征提取模块，用于提取和融合跨范式的特征。

Result: 在三种经典BCI范式（MI、SSVEP、P300）上，模型准确率达88.39%，显著优于对比模型。

Conclusion: 该研究为跨BCI范式分类提供了可行方案，为低成本、通用化的实际应用奠定了基础。

Abstract: Classification models used in brain-computer interface (BCI) are usually
designed for a single BCI paradigm. This requires the redevelopment of the
model when applying it to a new BCI paradigm, resulting in repeated costs and
effort. Moreover, less complex deep learning models are desired for practical
usage, as well as for deployment on portable devices. In or-der to fill the
above gaps, we, in this study, proposed a light-weight and unified decoding
model for cross-BCI-paradigm classification. The proposed model starts with a
tempo-spatial convolution. It is followed by a multi-scale local feature
selec-tion module, aiming to extract local features shared across BCI paradigms
and generate weighted features. Finally, a mul-ti-dimensional global feature
extraction module is designed, in which multi-dimensional global features are
extracted from the weighted features and fused with the weighted features to
form high-level feature representations associated with BCI para-digms. The
results, evaluated on a mixture of three classical BCI paradigms (i.e., MI,
SSVEP, and P300), demon-strate that the proposed model achieves 88.39%, 82.36%,
80.01%, and 0.8092 for accuracy, macro-precision, mac-ro-recall, and
macro-F1-score, respectively, significantly out-performing the compared models.
This study pro-vides a feasible solution for cross-BCI-paradigm
classifica-tion. It lays a technological foundation for de-veloping a new
generation of unified decoding systems, paving the way for low-cost and
universal practical applications.

</details>


### [371] [Exploring Molecular Odor Taxonomies for Structure-based Odor Predictions using Machine Learning](https://arxiv.org/abs/2508.09217)
*Akshay Sajan,Stijn Sluis,Reza Haydarlou,Sanne Abeln,Pasquale Lisena,Raphael Troncy,Caro Verbeek,Inger Leemans,Halima Mouhib*

Main category: q-bio.QM

TL;DR: 论文提出通过专家和数据驱动的气味分类法提升机器学习模型对气味预测的性能，并评估了两种分类法的效果。


<details>
  <summary>Details</summary>
Motivation: 解决从分子结构预测气味的挑战，尤其是气味空间理解不足和结构-气味关系的复杂性。

Method: 使用专家分类法（基于语义和感知相似性）和数据驱动分类法（基于气味描述符的共现模式聚类），评估其对机器学习模型预测性能的提升。

Result: 两种分类法均优于随机分组，提升了预测性能，并通过深入错误分析揭示了气味-结构关系的复杂性。

Conclusion: 提供两种分类法和完整数据集，为未来气味分子基础研究奠定基础，并详细介绍了多层专家分类法。

Abstract: One of the key challenges to predict odor from molecular structure is
unarguably our limited understanding of the odor space and the complexity of
the underlying structure-odor relationships. Here, we show that the predictive
performance of machine learning models for structure-based odor predictions can
be improved using both, an expert and a data-driven odor taxonomy. The expert
taxonomy is based on semantic and perceptual similarities, while the
data-driven taxonomy is based on clustering co-occurrence patterns of odor
descriptors directly from the prepared dataset. Both taxonomies improve the
predictions of different machine learning models and outperform random
groupings of descriptors that do not reflect existing relations between odor
descriptors. We assess the quality of both taxonomies through their predictive
performance across different odor classes and perform an in-depth error
analysis highlighting the complexity of odor-structure relationships and
identifying potential inconsistencies within the taxonomies by showcasing pear
odorants used in perfumery. The data-driven taxonomy allows us to critically
evaluate our expert taxonomy and better understand the molecular odor space.
Both taxonomies as well as a full dataset are made available to the community,
providing a stepping stone for a future community-driven exploration of the
molecular basis of smell. In addition, we provide a detailed multi-layer expert
taxonomy including a total of 777 different descriptors from the Pyrfume
repository.

</details>


### [372] [NEUBORN: The Neurodevelopmental Evolution framework Using BiOmechanical RemodelliNg](https://arxiv.org/abs/2508.09757)
*Nashira Baena,Mariana da Silva,Irina Grigorescu,Aakash Saboo,Saga Masui,Jaques-Donald Tournier,Emma C. Robinson*

Main category: q-bio.QM

TL;DR: 提出了一种基于生物力学约束的个体生长轨迹学习框架，用于改善皮层发育的预测和早期异常识别。


<details>
  <summary>Details</summary>
Motivation: 当前规范建模框架难以捕捉细尺度解剖细节，因其依赖于群体平均参考空间。

Method: 通过分层网络架构实现生物力学约束的纵向图像配准，学习个体生长轨迹。

Result: 方法提高了变形的生物合理性，生成更平滑的变形和更少的负雅可比行列式，优于现有基线。

Conclusion: 该框架为大脑成熟预测和皮层发育异常的早期识别提供了新可能。

Abstract: Understanding individual cortical development is essential for identifying
deviations linked to neurodevelopmental disorders. However, current normative
modelling frameworks struggle to capture fine-scale anatomical details due to
their reliance on modelling data within a population-average reference space.
Here, we present a novel framework for learning individual growth trajectories
from biomechanically constrained, longitudinal, diffeomorphic image
registration, implemented via a hierarchical network architecture. Trained on
neonatal MRI data from the Developing Human Connectome Project, the method
improves the biological plausibility of warps, generating growth trajectories
that better follow population-level trends while generating smoother warps,
with fewer negative Jacobians, relative to state-of-the-art baselines. The
resulting subject-specific deformations provide interpretable, biologically
grounded mappings of development. This framework opens new possibilities for
predictive modeling of brain maturation and early identification of
malformations of cortical development.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [373] [A pseudo-inverse of a line graph](https://arxiv.org/abs/2508.09412)
*Sevvandi Kandanaarachchi,Philip Kilby,Cheng Soon Ong*

Main category: stat.ML

TL;DR: 研究线图的小扰动情况下如何恢复原始图，提出线性整数程序并验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 线图变换不可逆，研究如何通过小扰动恢复原始图。

Method: 提出线性整数程序，编辑线图中最少的边以找到对应的原始图。

Result: 通过谱范数证明伪逆操作行为良好，实验验证理论结果。

Conclusion: 方法在实践中有效，为线图逆操作提供了理论支持。

Abstract: Line graphs are an alternative representation of graphs where each vertex of
the original (root) graph becomes an edge. However not all graphs have a
corresponding root graph, hence the transformation from graphs to line graphs
is not invertible. We investigate the case when there is a small perturbation
in the space of line graphs, and try to recover the corresponding root graph,
essentially defining the inverse of the line graph operation. We propose a
linear integer program that edits the smallest number of edges in the line
graph, that allow a root graph to be found. We use the spectral norm to
theoretically prove that such a pseudo-inverse operation is well behaved.
Illustrative empirical experiments on Erd\H{o}s-R\'enyi graphs show that our
theoretical results work in practice.

</details>


### [374] [Scalable h-adaptive probabilistic solver for time-independent and time-dependent systems](https://arxiv.org/abs/2508.09623)
*Akshay Thakur,Sawan Kumar,Matthew Zahr,Souvik Chakraborty*

Main category: stat.ML

TL;DR: 提出了一种基于概率数值方法的PDE求解器，通过降低计算复杂度和自适应选择采样点，解决了大规模或高维问题中的计算瓶颈。


<details>
  <summary>Details</summary>
Motivation: 传统概率数值方法在求解PDE时，由于计算复杂度高（立方级增长），难以处理大规模或高维问题。

Method: 1. 提出随机对偶下降算法，将每次迭代的计算复杂度从立方降至线性；2. 采用基于聚类的主动学习策略，自适应选择采样点以最大化信息增益。

Result: 实现了可扩展的概率求解器，能够处理大量采样点，并在二维、三维稳态椭圆问题及时空抛物PDE中验证了其有效性。

Conclusion: 该方法显著提升了概率数值求解器的可扩展性，为大规模PDE问题提供了高效解决方案。

Abstract: Solving partial differential equations (PDEs) within the framework of
probabilistic numerics offers a principled approach to quantifying epistemic
uncertainty arising from discretization. By leveraging Gaussian process
regression and imposing the governing PDE as a constraint at a finite set of
collocation points, probabilistic numerics delivers mesh-free solutions at
arbitrary locations. However, the high computational cost, which scales
cubically with the number of collocation points, remains a critical bottleneck,
particularly for large-scale or high-dimensional problems. We propose a
scalable enhancement to this paradigm through two key innovations. First, we
develop a stochastic dual descent algorithm that reduces the per-iteration
complexity from cubic to linear in the number of collocation points, enabling
tractable inference. Second, we exploit a clustering-based active learning
strategy that adaptively selects collocation points to maximize information
gain while minimizing computational expense. Together, these contributions
result in an $h$-adaptive probabilistic solver that can scale to a large number
of collocation points. We demonstrate the efficacy of the proposed solver on
benchmark PDEs, including two- and three-dimensional steady-state elliptic
problems, as well as a time-dependent parabolic PDE formulated in a space-time
setting.

</details>


### [375] [Structured Kernel Regression VAE: A Computationally Efficient Surrogate for GP-VAEs in ICA](https://arxiv.org/abs/2508.09721)
*Yuan-Hao Wei,Fu-Hao Deng,Lin-Yong Cui,Yan-Jie Sun*

Main category: stat.ML

TL;DR: SKR-VAE通过结构化核回归提升生成模型的解释性和计算效率，避免高斯过程的高计算负担。


<details>
  <summary>Details</summary>
Motivation: 提升生成模型的解释性和可控性，同时减少计算资源消耗。

Method: 提出Structured Kernel Regression VAE (SKR-VAE)，利用核函数结构化潜在变量先验，避免高斯过程的核矩阵求逆。

Result: SKR-VAE在保持ICA性能的同时，显著提高了计算效率。

Conclusion: SKR-VAE是一种高效且解释性强的生成模型改进方法。

Abstract: The interpretability of generative models is considered a key factor in
demonstrating their effectiveness and controllability. The generated data are
believed to be determined by latent variables that are not directly observable.
Therefore, disentangling, decoupling, decomposing, causal inference, or
performing Independent Component Analysis (ICA) in the latent variable space
helps uncover the independent factors that influence the attributes or features
affecting the generated outputs, thereby enhancing the interpretability of
generative models. As a generative model, Variational Autoencoders (VAEs)
combine with variational Bayesian inference algorithms. Using VAEs, the inverse
process of ICA can be equivalently framed as a variational inference process.
In some studies, Gaussian processes (GPs) have been introduced as priors for
each dimension of latent variables in VAEs, structuring and separating each
dimension from temporal or spatial perspectives, and encouraging different
dimensions to control various attributes of the generated data. However, GPs
impose a significant computational burden, resulting in substantial resource
consumption when handling large datasets. Essentially, GPs model different
temporal or spatial structures through various kernel functions. Structuring
the priors of latent variables via kernel functions-so that different kernel
functions model the correlations among sequence points within different latent
dimensions-is at the core of achieving disentanglement in VAEs. The proposed
Structured Kernel Regression VAE (SKR-VAE) leverages this core idea in a more
efficient way, avoiding the costly kernel matrix inversion required in GPs.
This research demonstrates that, while maintaining ICA performance, SKR-VAE
achieves greater computational efficiency and significantly reduced
computational burden compared to GP-VAE.

</details>


<div id='q-bio.GN'></div>

# q-bio.GN [[Back]](#toc)

### [376] [Deep Generative Models for Discrete Genotype Simulation](https://arxiv.org/abs/2508.09212)
*Sihan Xie,Thierry Tribout,Didier Boichard,Blaise Hanczar,Julien Chiquet,Eric Barrey*

Main category: q-bio.GN

TL;DR: 该论文研究了利用深度生成模型（如VAE、扩散模型和GAN）生成离散基因型数据的方法，并在大规模数据集上验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 解决基因型数据生成的挑战，同时保护隐私并提高数据可访问性。

Method: 开发并评估了多种生成模型（VAE、扩散模型、GAN），并针对离散基因型数据进行了适应性改进。

Result: 模型能有效捕捉遗传模式并保持基因型-表型关联。

Conclusion: 提供了模型比较和未来基因型模拟研究的实用指南，代码已开源。

Abstract: Deep generative models open new avenues for simulating realistic genomic data
while preserving privacy and addressing data accessibility constraints. While
previous studies have primarily focused on generating gene expression or
haplotype data, this study explores generating genotype data in both
unconditioned and phenotype-conditioned settings, which is inherently more
challenging due to the discrete nature of genotype data. In this work, we
developed and evaluated commonly used generative models, including Variational
Autoencoders (VAEs), Diffusion Models, and Generative Adversarial Networks
(GANs), and proposed adaptation tailored to discrete genotype data. We
conducted extensive experiments on large-scale datasets, including all
chromosomes from cow and multiple chromosomes from human. Model performance was
assessed using a well-established set of metrics drawn from both deep learning
and quantitative genetics literature. Our results show that these models can
effectively capture genetic patterns and preserve genotype-phenotype
association. Our findings provide a comprehensive comparison of these models
and offer practical guidelines for future research in genotype simulation. We
have made our code publicly available at
https://github.com/SihanXXX/DiscreteGenoGen.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [377] [Verify Distributed Deep Learning Model Implementation Refinement with Iterative Relation Inference](https://arxiv.org/abs/2508.09505)
*Zhanghan Wang,Ding Ding,Hang Zhu,Haibin Lin,Aurojit Panda*

Main category: cs.DC

TL;DR: 提出了一种静态识别分布式机器学习模型中错误的方法，通过检查模型细化来验证分布式模型的输出是否能重构为顺序模型的输出。


<details>
  <summary>Details</summary>
Motivation: 分布式机器学习训练和推理中，由于模型规模大，常需将状态和计算分配到多个GPU，但在此过程中可能引入错误，导致分布式模型与顺序模型的输出不一致。

Method: 使用迭代重写技术证明模型细化，工具GraphGuard实现该方法。

Result: 方法可扩展到GPT和Llama-3等大型模型，并提供有助于错误定位的可操作输出。

Conclusion: 该方法能有效识别分布式模型中的错误，并支持大规模模型的验证和调试。

Abstract: Distributed machine learning training and inference is common today because
today's large models require more memory and compute than can be provided by a
single GPU. Distributed models are generally produced by programmers who take a
sequential model specification and apply several distribution strategies to
distribute state and computation across GPUs. Unfortunately, bugs can be
introduced in the process, and a distributed model implementation's outputs
might differ from the sequential model's outputs. In this paper, we describe an
approach to statically identify such bugs by checking model refinement, that
is, can the sequential model's outputs be reconstructed from the distributed
model's outputs? Our approach, implemented in GraphGuard, uses iterative
rewriting to prove model refinement. Our approach can scale to today's large
models and deployments: we evaluate it using GPT and Llama-3. Further, it
provides actionable output that aids in bug localization.

</details>


### [378] [HierMoE: Accelerating MoE Training with Hierarchical Token Deduplication and Expert Swap](https://arxiv.org/abs/2508.09591)
*Wenxiang Lin,Xinglin Pan,Lin Zhang,Shaohuai Shi,Xuan Wang,Xiaowen Chu*

Main category: cs.DC

TL;DR: HierMoE通过拓扑感知技术加速MoE模型训练，减少通信流量并平衡GPU负载，实验显示性能显著提升。


<details>
  <summary>Details</summary>
Motivation: MoE模型在分布式系统中存在通信和负载不平衡问题，阻碍了GPU集群的可扩展性。

Method: 提出HierMoE，采用令牌去重和专家交换两种拓扑感知技术，并建立理论模型优化策略。

Result: 在32-GPU集群上实验，HierMoE通信速度提升1.55×至3.32×，训练速度提升1.18×至1.27×。

Conclusion: HierMoE显著提高了MoE模型的训练效率，优于现有系统。

Abstract: The sparsely activated mixture-of-experts (MoE) transformer has become a
common architecture for large language models (LLMs) due to its sparsity, which
requires fewer computational demands while easily scaling the model size. In
MoE models, each MoE layer requires to dynamically choose tokens to activate
particular experts for computation while the activated experts may not be
located in the same device or GPU as the token. However, this leads to
substantial communication and load imbalances across all GPUs, which obstructs
the scalability of distributed systems within a GPU cluster. To this end, we
introduce HierMoE to accelerate the training of MoE models by two
topology-aware techniques: 1) token deduplication to reduce the communication
traffic, and 2) expert swap to balance the workloads among all GPUs. To enable
the above two proposed approaches to be more general, we build theoretical
models aimed at achieving the best token duplication and expert swap strategy
under different model configurations and hardware environments. We implement
our prototype HierMoE system atop Megatron-LM and conduct experiments on a
32-GPU cluster with DeepSeek-V3 and Qwen3-30B-A3B models. Experimental results
show that our HierMoE achieves $1.55\times$ to $3.32\times$ faster
communication and delivers $1.18\times$ to $1.27\times$ faster end-to-end
training compared to state-of-the-art MoE training systems, Tutel-2DH,
SmartMoE, and Megatron-LM.

</details>


<div id='nlin.CG'></div>

# nlin.CG [[Back]](#toc)

### [379] [Counting Short Trajectories in Elementary Cellular Automata using the Transfer Matrix Method](https://arxiv.org/abs/2508.09768)
*Cédric Koller,Barbora Hudcová*

Main category: nlin.CG

TL;DR: 该论文通过转移矩阵方法（TMM）量化了初等细胞自动机（ECAs）的全局动力学，计算了短吸引子的初始配置数量，并建立了熵与Wolfram定性分类之间的定量联系。


<details>
  <summary>Details</summary>
Motivation: 为理解ECAs的多样性行为提供定量基础，补充Wolfram的定性分类。

Method: 采用转移矩阵方法（TMM），计算在有限时间步内收敛到短吸引子的初始配置数量，得到热力学极限下的精确结果。

Result: 不同Wolfram类别的规则在熵表现上具有显著差异：Class 1和2快速达到最大熵，Class 3熵低且饱和，Class 4熵有限且类似某些Class 3规则。

Conclusion: 该方法为量化轨迹统计提供了精确框架，但计算成本随参数指数增长，限制了实际分析范围。

Abstract: Elementary Cellular Automata (ECAs) exhibit diverse behaviours often
categorized by Wolfram's qualitative classification. To provide a quantitative
basis for understanding these behaviours, we investigate the global dynamics of
such automata and we describe a method that allows us to compute the number of
all configurations leading to short attractors in a limited number of time
steps. This computation yields exact results in the thermodynamic limit (as the
CA grid size grows to infinity), and is based on the Transfer Matrix Method
(TMM) that we adapt for our purposes. Specifically, given two parameters $(p,
c)$ we are able to compute the entropy of all initial configurations converging
to an attractor of size $c$ after $p$ time-steps. By calculating such
statistics for various ECA rules, we establish a quantitative connection
between the entropy and the qualitative Wolfram classification scheme. Class 1
rules rapidly converge to maximal entropy for stationary states ($c=1$) as $p$
increases. Class 2 rules also approach maximal entropy quickly for appropriate
cycle lengths $c$, potentially requiring consideration of translations. Class 3
rules exhibit zero or low finite entropy that saturates after a short
transient. Class 4 rules show finite positive entropy, similar to some Class 3
rules. This method provides a precise framework for quantifying trajectory
statistics, although its exponential computational cost in $p+c$ restricts
practical analysis to short trajectories.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [380] [Agentic TinyML for Intent-aware Handover in 6G Wireless Networks](https://arxiv.org/abs/2508.09147)
*Alaa Saleh,Roberto Morabito,Sasu Tarkoma,Anders Lindgren,Susanna Pirttikangas,Lauri Lovén*

Main category: cs.NI

TL;DR: 本文提出WAAN框架，通过嵌入轻量级TinyML代理实现意图感知和主动切换，以应对6G网络中移动边缘计算和自主代理服务的挑战。


<details>
  <summary>Details</summary>
Motivation: 传统反应式切换机制在6G网络的AI驱动、用户中心化生态系统中表现不足，尤其在移动边缘计算和自主代理服务场景下。

Method: WAAN框架通过跨层设计，嵌入TinyML代理作为自主协商实体，利用半稳定会合点实现上下文转移和状态保持。

Result: 通过多模态环境控制案例研究验证了WAAN在移动性下保持用户体验的有效性。

Conclusion: 文章讨论了WAAN部署和演进中的关键挑战与未来机遇。

Abstract: As 6G networks evolve into increasingly AI-driven, user-centric ecosystems,
traditional reactive handover mechanisms demonstrate limitations, especially in
mobile edge computing and autonomous agent-based service scenarios. This
manuscript introduces WAAN, a cross-layer framework that enables intent-aware
and proactive handovers by embedding lightweight TinyML agents as autonomous,
negotiation-capable entities across heterogeneous edge nodes that contribute to
intent propagation and network adaptation. To ensure continuity across
mobility-induced disruptions, WAAN incorporates semi-stable rendezvous points
that serve as coordination anchors for context transfer and state preservation.
The framework's operational capabilities are demonstrated through a multimodal
environmental control case study, highlighting its effectiveness in maintaining
user experience under mobility. Finally, the article discusses key challenges
and future opportunities associated with the deployment and evolution of WAAN.

</details>


### [381] [Physiological Signal-Driven QoE Optimization for Wireless Virtual Reality Transmission](https://arxiv.org/abs/2508.09151)
*Chang Wu,Yuang Chen,Yiyuan Chen,Fengqian Guo,Xiaowei Qin,Hancheng Lu*

Main category: cs.NI

TL;DR: 论文提出了一种基于生理信号的QoE建模与优化框架，通过EEG、ECG和皮肤活动信号动态调整VR流媒体分辨率，显著提升了用户体验。


<details>
  <summary>Details</summary>
Motivation: 现有QoE模型未能充分解决VR流媒体中分辨率突变对用户体验的影响，需更精准的生理信号驱动方法。

Method: 结合生理信号（EEG、ECG、皮肤活动）和DRL框架，动态分配无线资源以优化分辨率调整。

Result: 实验显示，该方法在分辨率和切换频率上分别提升了88.7%和降低了81.0%。

Conclusion: 生理信号驱动的策略有效提升了VR流媒体的QoE，展示了边缘AI在沉浸式媒体服务中的潜力。

Abstract: Abrupt resolution changes in virtual reality (VR) streaming can significantly
impair the quality-of-experience (QoE) of users, particularly during
transitions from high to low resolutions. Existing QoE models and transmission
schemes inadequately address the perceptual impact of these shifts. To bridge
this gap, this article proposes, for the first time, an innovative
physiological signal-driven QoE modeling and optimization framework that fully
leverages users' electroencephalogram (EEG), electrocardiogram (ECG), and skin
activity signals. This framework precisely captures the temporal dynamics of
physiological responses and resolution changes in VR streaming, enabling
accurate quantification of resolution upgrades' benefits and downgrades'
impacts. Integrated the proposed QoE framework into the radio access network
(RAN) via a deep reinforcement learning (DRL) framework, adaptive transmission
strategies have been implemented to allocate radio resources dynamically, which
mitigates short-term channel fluctuations and adjusts frame resolution in
response to channel variations caused by user mobility. By prioritizing
long-term resolution while minimizing abrupt transitions, the proposed solution
achieves an 88.7\% improvement in resolution and an 81.0\% reduction in
handover over the baseline. Experimental results demonstrate the effectiveness
of this physiological signal-driven strategy, underscoring the promise of edge
AI in immersive media services.

</details>


### [382] [5G Core Fault Detection and Root Cause Analysis using Machine Learning and Generative AI](https://arxiv.org/abs/2508.09152)
*Joseph H. R. Isaac,Harish Saradagam,Nallamothu Pardhasaradhi*

Main category: cs.NI

TL;DR: 本文提出了一种基于AI/ML的故障分析引擎，用于自动分类5G核心网中的PCAP文件中的成功与故障帧，显著减少人工分析时间并提高效率。


<details>
  <summary>Details</summary>
Motivation: 随着5G网络的普及，确保数据包核心流量的完整性和性能至关重要。当前方法需要大量人工分析测试结果和故障，效率低下。

Method: 利用自然语言处理技术分析网络流量，结合生成式AI（基于大型语言模型）提供故障修复建议，并参考3GPP标准等文档解释错误。

Result: 测试结果显示，ML模型在训练数据（80%成功PCAP文件和20%故障文件）上具有高分类准确率。

Conclusion: 该引擎显著提升了故障分析效率，未来可扩展至4G网络和其他网络数据类型。

Abstract: With the advent of 5G networks and technologies, ensuring the integrity and
performance of packet core traffic is paramount. During network analysis, test
files such as Packet Capture (PCAP) files and log files will contain errors if
present in the system that must be resolved for better overall network
performance, such as connectivity strength and handover quality. Current
methods require numerous person-hours to sort out testing results and find the
faults. This paper presents a novel AI/ML-driven Fault Analysis (FA) Engine
designed to classify successful and faulty frames in PCAP files, specifically
within the 5G packet core. The FA engine analyses network traffic using natural
language processing techniques to identify anomalies and inefficiencies,
significantly reducing the effort time required and increasing efficiency. The
FA Engine also suggests steps to fix the issue using Generative AI via a Large
Language Model (LLM) trained on several 5G packet core documents. The engine
explains the details of the error from the domain perspective using documents
such as the 3GPP standards and user documents regarding the internal conditions
of the tests. Test results on the ML models show high classification accuracy
on the test dataset when trained with 80-20 splits for the successful and
failed PCAP files. Future scopes include extending the AI engine to incorporate
4G network traffic and other forms of network data, such as log text files and
multimodal systems.

</details>


### [383] [Agoran: An Agentic Open Marketplace for 6G RAN Automation](https://arxiv.org/abs/2508.09159)
*Ilias Chatzistefanidis,Navid Nikaein,Andrea Leone,Ali Maatouk,Leandros Tassioulas,Roberto Morabito,Ioannis Pitsiorlas,Marios Kountouris*

Main category: cs.NI

TL;DR: Agoran SRB是一种基于代理的市场化解决方案，通过多分支AI系统优化下一代移动网络的服务和资源分配，显著提升性能并降低资源消耗。


<details>
  <summary>Details</summary>
Motivation: 解决当前网络切片控制器在灵活性和业务感知方面的不足，满足多服务所有者的需求。

Method: 采用三分支AI架构（立法、行政、司法）和谈判代理，结合多目标优化器达成共识意图。

Result: 在5G测试中，eMBB切片吞吐量提升37%，URLLC切片延迟降低73%，PRB使用节省8.3%。

Conclusion: Agoran为6G网络提供了一种灵活且以利益相关者为中心的可行路径。

Abstract: Next-generation mobile networks must reconcile the often-conflicting goals of
multiple service owners. However, today's network slice controllers remain
rigid, policy-bound, and unaware of the business context. We introduce Agoran
Service and Resource Broker (SRB), an agentic marketplace that brings
stakeholders directly into the operational loop. Inspired by the ancient Greek
agora, Agoran distributes authority across three autonomous AI branches: a
Legislative branch that answers compliance queries using retrieval-augmented
Large Language Models (LLMs); an Executive branch that maintains real-time
situational awareness through a watcher-updated vector database; and a Judicial
branch that evaluates each agent message with a rule-based Trust Score, while
arbitrating LLMs detect malicious behavior and apply real-time incentives to
restore trust. Stakeholder-side Negotiation Agents and the SRB-side Mediator
Agent negotiate feasible, Pareto-optimal offers produced by a multi-objective
optimizer, reaching a consensus intent in a single round, which is then
deployed to Open and AI RAN controllers. Deployed on a private 5G testbed and
evaluated with realistic traces of vehicle mobility, Agoran achieved
significant gains: (i) a 37% increase in throughput of eMBB slices, (ii) a 73%
reduction in latency of URLLC slices, and concurrently (iii) an end-to-end 8.3%
saving in PRB usage compared to a static baseline. An 1B-parameter Llama model,
fine-tuned for five minutes on 100 GPT-4 dialogues, recovers approximately 80%
of GPT-4.1's decision quality, while operating within 6 GiB of memory and
converging in only 1.3 seconds. These results establish Agoran as a concrete,
standards-aligned path toward ultra-flexible, stakeholder-centric 6G networks.
A live demo is presented
https://www.youtube.com/watch?v=h7vEyMu2f5w\&ab_channel=BubbleRAN.

</details>


### [384] [webMCP: Efficient AI-Native Client-Side Interaction for Agent-Ready Web Design](https://arxiv.org/abs/2508.09171)
*D. Perera*

Main category: cs.NI

TL;DR: webMCP是一种客户端标准，通过嵌入结构化交互元数据到网页中，显著降低AI代理处理网页的计算开销，提升人机协作效率。


<details>
  <summary>Details</summary>
Motivation: 当前AI代理处理网页需要大量计算，导致交互缓慢且成本高，webMCP旨在解决这一问题。

Method: webMCP直接在网页中嵌入结构化交互元数据，为AI代理提供明确的页面元素与用户动作映射。

Result: 评估显示，webMCP减少67.6%的计算需求，任务成功率97.9%，成本降低34-63%，响应时间显著缩短。

Conclusion: webMCP无需服务器端修改，可部署于现有网站，为AI辅助网页交互提供了高效且可持续的解决方案。

Abstract: Current AI agents create significant barriers for users by requiring
extensive processing to understand web pages, making AI-assisted web
interaction slow and expensive. This paper introduces webMCP (Web Machine
Context & Procedure), a client-side standard that embeds structured interaction
metadata directly into web pages, enabling more efficient human-AI
collaboration on existing websites. webMCP transforms how AI agents understand
web interfaces by providing explicit mappings between page elements and user
actions. Instead of processing entire HTML documents, agents can access
pre-structured interaction data, dramatically reducing computational overhead
while maintaining task accuracy. A comprehensive evaluation across 1,890 real
API calls spanning online shopping, authentication, and content management
scenarios demonstrates webMCP reduces processing requirements by 67.6% while
maintaining 97.9% task success rates compared to 98.8% for traditional
approaches. Users experience significantly lower costs (34-63% reduction) and
faster response times across diverse web interactions. Statistical analysis
confirms these improvements are highly significant across multiple AI models.
An independent WordPress deployment study validates practical applicability,
showing consistent improvements across real-world content management workflows.
webMCP requires no server-side modifications, making it deployable across
millions of existing websites without technical barriers. These results
establish webMCP as a viable solution for making AI web assistance more
accessible and sustainable, addressing the critical gap between user
interaction needs and AI computational requirements in production environments.

</details>


### [385] [HiSTM: Hierarchical Spatiotemporal Mamba for Cellular Traffic Forecasting](https://arxiv.org/abs/2508.09184)
*Zineddine Bettouche,Khalid Ali,Andreas Fischer,Andreas Kassler*

Main category: cs.NI

TL;DR: HiSTM模型通过结合双空间编码器和Mamba时序模块，显著提升了蜂窝流量预测的准确性，同时减少了参数数量。


<details>
  <summary>Details</summary>
Motivation: 蜂窝流量预测对网络规划和资源分配至关重要，但现有模型在准确性和计算效率之间存在权衡。

Method: HiSTM采用双空间编码器和Mamba时序模块，结合选择性状态空间方法捕捉时空模式。

Result: 实验显示，HiSTM在MAE上比STN基线提升29.4%，且参数减少94%。

Conclusion: HiSTM在不同数据集上泛化能力强，且在长时间预测中表现更优。

Abstract: Cellular traffic forecasting is essential for network planning, resource
allocation, or load-balancing traffic across cells. However, accurate
forecasting is difficult due to intricate spatial and temporal patterns that
exist due to the mobility of users. Existing AI-based traffic forecasting
models often trade-off accuracy and computational efficiency. We present
Hierarchical SpatioTemporal Mamba (HiSTM), which combines a dual spatial
encoder with a Mamba-based temporal module and attention mechanism. HiSTM
employs selective state space methods to capture spatial and temporal patterns
in network traffic. In our evaluation, we use a real-world dataset to compare
HiSTM against several baselines, showing a 29.4% MAE improvement over the STN
baseline while using 94% fewer parameters. We show that the HiSTM generalizes
well across different datasets and improves in accuracy over longer
time-horizons.

</details>


### [386] [MX-AI: Agentic Observability and Control Platform for Open and AI-RAN](https://arxiv.org/abs/2508.09197)
*Ilias Chatzistefanidis,Andrea Leone,Ali Yaghoubian,Mikel Irazabal,Sehad Nassim,Lina Bariah,Merouane Debbah,Navid Nikaein*

Main category: cs.NI

TL;DR: MX-AI是一个端到端的AI原生系统，用于6G无线接入网络（RAN），通过LLM驱动的代理实现自然语言控制和观测，性能接近人类专家水平。


<details>
  <summary>Details</summary>
Motivation: 6G RAN需要AI原生支持，以实现自主观测、推理和重新配置。MX-AI旨在验证这一理念的可行性。

Method: 基于OpenAirInterface和FlexRIC构建5G Open RAN测试床，在SMO层部署LLM驱动的代理图，提供自然语言接口。

Result: 在50个操作查询中，MX-AI平均回答质量4.1/5.0，决策准确率100%，延迟仅8.8秒。

Conclusion: MX-AI验证了AI原生RAN的实用性，性能接近人类专家，并公开了相关资源以推动研究。

Abstract: Future 6G radio access networks (RANs) will be artificial intelligence
(AI)-native: observed, reasoned about, and re-configured by autonomous agents
cooperating across the cloud-edge continuum. We introduce MX-AI, the first
end-to-end agentic system that (i) instruments a live 5G Open RAN testbed based
on OpenAirInterface (OAI) and FlexRIC, (ii) deploys a graph of
Large-Language-Model (LLM)-powered agents inside the Service Management and
Orchestration (SMO) layer, and (iii) exposes both observability and control
functions for 6G RAN resources through natural-language intents. On 50
realistic operational queries, MX-AI attains a mean answer quality of 4.1/5.0
and 100 % decision-action accuracy, while incurring only 8.8 seconds end-to-end
latency when backed by GPT-4.1. Thus, it matches human-expert performance,
validating its practicality in real settings. We publicly release the agent
graph, prompts, and evaluation harness to accelerate open research on AI-native
RANs. A live demo is presented here:
https://www.youtube.com/watch?v=CEIya7988Ug&t=285s&ab_channel=BubbleRAN

</details>


### [387] [NEFMind: Parameter-Efficient Fine-Tuning of Open-Source LLMs for Telecom APIs Automation](https://arxiv.org/abs/2508.09240)
*Zainab Khan,Ahmed Hussain,Mukesh Thakur,Arto Hellas,Panos Papadimitratos*

Main category: cs.NI

TL;DR: 论文提出NEFMind框架，利用高效参数微调的开源大语言模型（LLMs）解决5G服务架构中服务发现和管理的复杂性，显著降低通信开销并实现高精度API调用识别。


<details>
  <summary>Details</summary>
Motivation: 现代电信中基于服务的架构导致网络功能和API数量激增，带来服务发现和管理的复杂性，亟需高效解决方案。

Method: NEFMind框架包含三个核心组件：基于NEF API规范生成合成数据集、通过量化低秩适应优化模型、使用GPT-4 Ref Score和BertScore评估性能。

Result: 实验验证表明，该方法在5G服务架构中通信开销减少85%，API调用识别准确率达98-100%，且性能接近GPT-4但计算效率更高。

Conclusion: 研究验证了高效参数微调的LLM策略在管理下一代电信网络复杂API生态系统中的有效性。

Abstract: The use of Service-Based Architecture in modern telecommunications has
exponentially increased Network Functions (NFs) and Application Programming
Interfaces (APIs), creating substantial operational complexities in service
discovery and management. We introduce \textit{NEFMind}, a framework leveraging
parameter-efficient fine-tuning of open-source Large Language Models (LLMs) to
address these challenges. It integrates three core components: synthetic
dataset generation from Network Exposure Function (NEF) API specifications,
model optimization through Quantized-Low-Rank Adaptation, and performance
evaluation via GPT-4 Ref Score and BertScore metrics. Targeting 5G
Service-Based Architecture APIs, our approach achieves 85% reduction in
communication overhead compared to manual discovery methods. Experimental
validation using the open-source Phi-2 model demonstrates exceptional API call
identification performance at 98-100% accuracy. The fine-tuned Phi-2 model
delivers performance comparable to significantly larger models like GPT-4 while
maintaining computational efficiency for telecommunications infrastructure
deployment. These findings validate domain-specific, parameter-efficient LLM
strategies for managing complex API ecosystems in next-generation
telecommunications networks.

</details>


### [388] [CoMoE: Collaborative Optimization of Expert Aggregation and Offloading for MoE-based LLMs at Edge](https://arxiv.org/abs/2508.09208)
*Muqing Li,Ning Li,Xin Yuan,Wenchao Xu,Quan Chen,Song Guo,Haijun Zhang*

Main category: cs.NI

TL;DR: CoMoE提出了一种动态资源感知协作优化框架，用于在移动边缘环境中高效部署MoE模型，显著减少内存使用和推理延迟。


<details>
  <summary>Details</summary>
Motivation: 解决MoE模型在资源受限的移动边缘环境中部署的挑战，如大内存占用和动态专家激活模式。

Method: 通过联合优化专家聚合粒度和卸载策略，结合实时设备资源状态、网络条件和输入特征，设计自适应调度机制。

Result: 实验表明，CoMoE内存使用减少70%，推理延迟降低10.5%，并成功部署大型MoE模型于资源受限设备。

Conclusion: CoMoE为移动边缘环境中的MoE模型部署提供了高效解决方案，显著提升了资源利用率和性能稳定性。

Abstract: The proliferation of large language models (LLMs) has driven the adoption of
Mixture-of-Experts (MoE) architectures as a promising solution to scale model
capacity while controlling computational costs. However, deploying MoE models
in resource-constrained mobile edge computing environments presents significant
challenges due to their large memory footprint and dynamic expert activation
patterns. To address these challenges, we propose a novel dynamic
resource-aware collaborative optimization framework that jointly optimizes
expert aggregation granularity and offloading strategies based on real-time
device resource states, network conditions, and input characteristics in mobile
edge environments, denoted as CoMoE. In CoMoE, we first systematically analyze
existing expert aggregation techniques, including expert parameter
merging,knowledge distillation,and parameter sharing decomposition, identifying
their limitations in dynamic mobile environments.We then investigate expert
offloading strategies encompassing expert prediction and prefetching, expert
caching and scheduling, and multi-tier storage architectures, revealing the
interdependencies between routing decisions and offloading performance.The
CoMoE incorporates adaptive scheduling mechanisms that respond to user mobility
and varying network conditions, enabling efficient MoE deployment across
heterogeneous edge devices. Extensive experiments on real mobile edge testbeds
demonstrate that CoMoE achieves approximately 70% reduction in memory usage
compared to baseline methods, 10.5% lower inference latency than existing
expert offloading techniques, while maintaining model performance stability.
For large-scale MoE models (e.g,7.4B-parameter Switch-Base-128), the CoMoE
reduces memory requirements from 15.6GB to 4.7GB, enabling deployment on
resource-constrained mobile edge devices that previously could only support
much smaller models.

</details>


### [389] [Cluster Topology-Driven Placement of Experts Reduces Network Traffic in MoE Inference](https://arxiv.org/abs/2508.09229)
*Danil Sivtsov,Aleksandr Katrutsa,Ivan Oseledets*

Main category: cs.NI

TL;DR: 本文提出了一种基于整数线性规划（ILP）的模型放置算法，用于优化预训练MoE LLM在多服务器集群中的部署，以减少网络流量。


<details>
  <summary>Details</summary>
Motivation: 高效部署预训练的MoE LLM到多服务器集群是提升用户查询响应速度的关键，而现有方法未充分考虑网络拓扑结构和专家负载不均衡的问题。

Method: 提出了一种整数线性规划（ILP）方法，用于确定专家的最优放置位置，最小化预期传输次数。该方法适用于小规模（DeepSeekMoE~16B）和大规模（DeepSeek-R1~671B）模型。

Result: 实验表明，基于ILP的放置策略比现有方法能显著降低网络流量。

Conclusion: ILP方法能有效优化MoE LLM的部署，提升集群利用率并减少网络传输开销。

Abstract: Efficient deployment of a pre-trained LLM to a cluster with multiple servers
is a critical step for providing fast responses to users' queries. The recent
success of Mixture-of-Experts (MoE) LLMs raises the question of how to deploy
them efficiently, considering their underlying structure. During the inference
in MoE LLMs, only a small part of the experts is selected to process a given
token. Moreover, in practice, the experts' load is highly imbalanced. For
efficient deployment, one has to distribute the model across a large number of
servers using a model placement algorithm. Thus, to improve cluster
utilization, the model placement algorithm has to take into account the network
topology. This work focuses on the efficient topology-aware placement of the
pre-trained MoE LLMs in the inference stage. We propose an integer linear
program (ILP) that determines the optimal placement of experts, minimizing the
expected number of transmissions. Due to the internal structure, this
optimization problem can be solved with a standard ILP solver. We demonstrate
that ILP-based placement strategy yields lower network traffic than competitors
for small-scale (DeepSeekMoE~16B) and large-scale (DeepSeek-R1~671B) models.

</details>


### [390] [Anomaly Detection for IoT Global Connectivity](https://arxiv.org/abs/2508.09660)
*Jesus Omaña Iglesias,Carlos Segura Perales,Stefan Geißler,Diego Perino,Andra Lutu*

Main category: cs.NI

TL;DR: ANCHOR是一个无监督异常检测解决方案，用于全球漫游平台的物联网连接服务，旨在通过主动识别潜在问题客户来提升服务可靠性。


<details>
  <summary>Details</summary>
Motivation: 物联网服务依赖复杂的多实体通信路径，现有方法多为被动响应，难以保证通信可用性和可靠性。

Method: 结合统计规则、机器学习和深度学习模型，基于被动信令流量进行异常检测。

Result: ANCHOR成功识别了潜在问题客户，实现了问题的主动解决。

Conclusion: ANCHOR为物联网连接服务提供了一种有效的无监督异常检测方法，显著提升了服务可靠性。

Abstract: Internet of Things (IoT) application providers rely on Mobile Network
Operators (MNOs) and roaming infrastructures to deliver their services
globally. In this complex ecosystem, where the end-to-end communication path
traverses multiple entities, it has become increasingly challenging to
guarantee communication availability and reliability. Further, most platform
operators use a reactive approach to communication issues, responding to user
complaints only after incidents have become severe, compromising service
quality. This paper presents our experience in the design and deployment of
ANCHOR -- an unsupervised anomaly detection solution for the IoT connectivity
service of a large global roaming platform. ANCHOR assists engineers by
filtering vast amounts of data to identify potential problematic clients (i.e.,
those with connectivity issues affecting several of their IoT devices),
enabling proactive issue resolution before the service is critically impacted.
We first describe the IoT service, infrastructure, and network visibility of
the IoT connectivity provider we operate. Second, we describe the main
challenges and operational requirements for designing an unsupervised anomaly
detection solution on this platform. Following these guidelines, we propose
different statistical rules, and machine- and deep-learning models for IoT
verticals anomaly detection based on passive signaling traffic. We describe the
steps we followed working with the operational teams on the design and
evaluation of our solution on the operational platform, and report an
evaluation on operational IoT customers.

</details>
