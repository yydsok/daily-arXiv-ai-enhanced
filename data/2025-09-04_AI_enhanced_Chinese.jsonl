{"id": "2509.02650", "pdf": "https://arxiv.org/pdf/2509.02650", "abs": "https://arxiv.org/abs/2509.02650", "authors": ["Henrique Correia da Fonseca", "Ant\u00f3nio Fernandes", "Zhao Song", "Theodor Cimpeanu", "Nataliya Balabanova", "Adeela Bashir", "Paolo Bova", "Alessio Buscemi", "Alessandro Di Stefano", "Manh Hong Duong", "Elias Fernandez Domingos", "Ndidi Bianca Ogbo", "Simon T. Powers", "Daniele Proverbio", "Zia Ush Shamszaman", "Fernando P. Santos", "The Anh Han", "Marcus Krellner"], "title": "Can Media Act as a Soft Regulator of Safe AI Development? A Game Theoretical Analysis", "categories": ["cs.AI", "cs.GT", "q-bio.PE"], "comment": "10 Pages, 7 Figures, accepted in the ALIFE 2025 Conference", "summary": "When developers of artificial intelligence (AI) products need to decide\nbetween profit and safety for the users, they likely choose profit.\nUntrustworthy AI technology must come packaged with tangible negative\nconsequences. Here, we envisage those consequences as the loss of reputation\ncaused by media coverage of their misdeeds, disseminated to the public. We\nexplore whether media coverage has the potential to push AI creators into the\nproduction of safe products, enabling widespread adoption of AI technology. We\ncreated artificial populations of self-interested creators and users and\nstudied them through the lens of evolutionary game theory. Our results reveal\nthat media is indeed able to foster cooperation between creators and users, but\nnot always. Cooperation does not evolve if the quality of the information\nprovided by the media is not reliable enough, or if the costs of either\naccessing media or ensuring safety are too high. By shaping public perception\nand holding developers accountable, media emerges as a powerful soft regulator\n-- guiding AI safety even in the absence of formal government oversight.", "AI": {"tldr": "\u7a0b\u5e8f\u5458\u5728\u5229\u6da6\u548c\u5b89\u5168\u4e4b\u95f4\u9009\u62e9\u65f6\u5e38\u9009\u62e9\u5229\u6da6\uff0c\u5a92\u4f53\u62a5\u9053\u53ef\u901a\u8fc7\u6350\u5931\u58f0\u8a89\u6765\u4fc3\u4f7fAI\u521b\u9020\u8005\u91c7\u53d6\u5b89\u5168\u63aa\u65bd\uff0c\u4f46\u9700\u8981\u9ad8\u8d28\u91cf\u4fe1\u606f\u548c\u5408\u7406\u6210\u672c\u6761\u4ef6\u3002", "motivation": "\u63a2\u7d22\u5a92\u4f53\u62a5\u9053\u662f\u5426\u80fd\u591f\u4fc3\u4f7fAI\u521b\u9020\u8005\u91c7\u53d6\u5b89\u5168\u63aa\u65bd\uff0c\u4ee5\u5b9e\u73b0AI\u6280\u672f\u7684\u5e7f\u6cdb\u91c7\u7528\u3002", "method": "\u901a\u8fc7\u8fdb\u5316\u6e38\u620f\u7406\u8bba\u7814\u7a76\u81ea\u5229\u7684\u521b\u9020\u8005\u548c\u7528\u6237\u7fa4\u4f53\uff0c\u5206\u6790\u5a92\u4f53\u62a5\u9053\u5bf9\u5408\u4f5c\u884c\u4e3a\u7684\u5f71\u54cd\u3002", "result": "\u5a92\u4f53\u80fd\u591f\u4fc3\u8fdb\u5408\u4f5c\uff0c\u4f46\u9700\u8981\u9ad8\u8d28\u91cf\u4fe1\u606f\u548c\u5408\u7406\u6210\u672c\u6761\u4ef6\uff1b\u5982\u679c\u4fe1\u606f\u4e0d\u53ef\u9760\u6216\u6210\u672c\u8fc7\u9ad8\uff0c\u5408\u4f5c\u5c31\u65e0\u6cd5\u5f62\u6210\u3002", "conclusion": "\u5a92\u4f53\u4f5c\u4e3a\u4e00\u79cd\u8f6f\u6027\u76d1\u7ba1\u673a\u5236\uff0c\u53ef\u4ee5\u5728\u6ca1\u6709\u653f\u5e9c\u76d1\u7ba1\u7684\u60c5\u51b5\u4e0b\u6307\u5bfcAI\u5b89\u5168\u53d1\u5c55\u3002"}}
{"id": "2509.02661", "pdf": "https://arxiv.org/pdf/2509.02661", "abs": "https://arxiv.org/abs/2509.02661", "authors": ["Andrew Ferguson", "Marisa LaFleur", "Lars Ruthotto", "Jesse Thaler", "Yuan-Sen Ting", "Pratyush Tiwary", "Soledad Villar", "E. Paulo Alves", "Jeremy Avigad", "Simon Billinge", "Camille Bilodeau", "Keith Brown", "Emmanuel Candes", "Arghya Chattopadhyay", "Bingqing Cheng", "Jonathan Clausen", "Connor Coley", "Andrew Connolly", "Fred Daum", "Sijia Dong", "Chrisy Xiyu Du", "Cora Dvorkin", "Cristiano Fanelli", "Eric B. Ford", "Luis Manuel Frutos", "Nicol\u00e1s Garc\u00eda Trillos", "Cecilia Garraffo", "Robert Ghrist", "Rafael Gomez-Bombarelli", "Gianluca Guadagni", "Sreelekha Guggilam", "Sergei Gukov", "Juan B. Guti\u00e9rrez", "Salman Habib", "Johannes Hachmann", "Boris Hanin", "Philip Harris", "Murray Holland", "Elizabeth Holm", "Hsin-Yuan Huang", "Shih-Chieh Hsu", "Nick Jackson", "Olexandr Isayev", "Heng Ji", "Aggelos Katsaggelos", "Jeremy Kepner", "Yannis Kevrekidis", "Michelle Kuchera", "J. Nathan Kutz", "Branislava Lalic", "Ann Lee", "Matt LeBlanc", "Josiah Lim", "Rebecca Lindsey", "Yongmin Liu", "Peter Y. Lu", "Sudhir Malik", "Vuk Mandic", "Vidya Manian", "Emeka P. Mazi", "Pankaj Mehta", "Peter Melchior", "Brice M\u00e9nard", "Jennifer Ngadiuba", "Stella Offner", "Elsa Olivetti", "Shyue Ping Ong", "Christopher Rackauckas", "Philippe Rigollet", "Chad Risko", "Philip Romero", "Grant Rotskoff", "Brett Savoie", "Uros Seljak", "David Shih", "Gary Shiu", "Dima Shlyakhtenko", "Eva Silverstein", "Taylor Sparks", "Thomas Strohmer", "Christopher Stubbs", "Stephen Thomas", "Suriyanarayanan Vaikuntanathan", "Rene Vidal", "Francisco Villaescusa-Navarro", "Gregory Voth", "Benjamin Wandelt", "Rachel Ward", "Melanie Weber", "Risa Wechsler", "Stephen Whitelam", "Olaf Wiest", "Mike Williams", "Zhuoran Yang", "Yaroslava G. Yingling", "Bin Yu", "Shuwen Yue", "Ann Zabludoff", "Huimin Zhao", "Tong Zhang"], "title": "The Future of Artificial Intelligence and the Mathematical and Physical Sciences (AI+MPS)", "categories": ["cs.AI", "astro-ph.IM", "cond-mat.mtrl-sci", "cs.LG", "physics.data-an"], "comment": "Community Paper from the Future of NSF AI+MPS Workshop, Cambridge,\n  Massachusetts, March 24-26, 2025, supported by NSF Award Number 2512945", "summary": "This community paper developed out of the NSF Workshop on the Future of\nArtificial Intelligence (AI) and the Mathematical and Physics Sciences (MPS),\nwhich was held in March 2025 with the goal of understanding how the MPS domains\n(Astronomy, Chemistry, Materials Research, Mathematical Sciences, and Physics)\ncan best capitalize on, and contribute to, the future of AI. We present here a\nsummary and snapshot of the MPS community's perspective, as of Spring/Summer\n2025, in a rapidly developing field. The link between AI and MPS is becoming\nincreasingly inextricable; now is a crucial moment to strengthen the link\nbetween AI and Science by pursuing a strategy that proactively and thoughtfully\nleverages the potential of AI for scientific discovery and optimizes\nopportunities to impact the development of AI by applying concepts from\nfundamental science. To achieve this, we propose activities and strategic\npriorities that: (1) enable AI+MPS research in both directions; (2) build up an\ninterdisciplinary community of AI+MPS researchers; and (3) foster education and\nworkforce development in AI for MPS researchers and students. We conclude with\na summary of suggested priorities for funding agencies, educational\ninstitutions, and individual researchers to help position the MPS community to\nbe a leader in, and take full advantage of, the transformative potential of\nAI+MPS.", "AI": {"tldr": "NSF\u7814\u8ba8\u4f1a\u62a5\u544a\uff0c\u63a2\u8ba8AI\u4e0e\u6570\u5b66\u7269\u7406\u79d1\u5b66\uff08MPS\uff09\u9886\u57df\u7684\u53cc\u5411\u878d\u5408\u7b56\u7565\uff0c\u5305\u62ec\u4fc3\u8fdb\u4ea4\u53c9\u7814\u7a76\u3001\u5efa\u8bbe\u8de8\u5b66\u79d1\u793e\u533a\u3001\u52a0\u5f3a\u6559\u80b2\u57f9\u8bad\uff0c\u5e76\u63d0\u51fa\u8d44\u52a9\u673a\u6784\u3001\u6559\u80b2\u673a\u6784\u548c\u7814\u7a76\u4eba\u5458\u7684\u4f18\u5148\u4e8b\u9879\u5efa\u8bae\u3002", "motivation": "\u7406\u89e3\u6570\u5b66\u7269\u7406\u79d1\u5b66\u9886\u57df\uff08\u5929\u6587\u5b66\u3001\u5316\u5b66\u3001\u6750\u6599\u7814\u7a76\u3001\u6570\u5b66\u79d1\u5b66\u3001\u7269\u7406\u5b66\uff09\u5982\u4f55\u6700\u597d\u5730\u5229\u7528AI\u7684\u672a\u6765\u53d1\u5c55\u5e76\u4e3a\u5176\u505a\u51fa\u8d21\u732e\uff0c\u5728AI\u5feb\u901f\u53d1\u5c55\u7684\u5173\u952e\u65f6\u523b\u52a0\u5f3aAI\u4e0e\u79d1\u5b66\u7684\u8054\u7cfb\u3002", "method": "\u901a\u8fc7NSF\u7814\u8ba8\u4f1a\u5f62\u6210\u793e\u533a\u5171\u8bc6\uff0c\u63d0\u51fa\u4e09\u65b9\u9762\u6218\u7565\uff1a1\uff09\u53cc\u5411\u4fc3\u8fdbAI+MPS\u7814\u7a76\uff1b2\uff09\u5efa\u8bbe\u8de8\u5b66\u79d1\u7814\u7a76\u793e\u533a\uff1b3\uff09\u52a0\u5f3aMPS\u7814\u7a76\u4eba\u5458\u548c\u5b66\u751f\u7684AI\u6559\u80b2\u57f9\u8bad\u3002", "result": "\u5f62\u6210\u4e86MPS\u793e\u533a\u57282025\u5e74\u6625\u5b63/\u590f\u5b63\u7684\u89c6\u89d2\u603b\u7ed3\uff0c\u63d0\u51fa\u4e86\u5177\u4f53\u7684\u6d3b\u52a8\u5efa\u8bae\u548c\u6218\u7565\u4f18\u5148\u4e8b\u9879\uff0c\u65e8\u5728\u4f7fMPS\u793e\u533a\u6210\u4e3aAI+MPS\u53d8\u9769\u6f5c\u529b\u7684\u9886\u5bfc\u8005\u548c\u5145\u5206\u53d7\u76ca\u8005\u3002", "conclusion": "\u9700\u8981\u91c7\u53d6\u79ef\u6781\u4e3b\u52a8\u7684\u6218\u7565\u6765\u52a0\u5f3aAI\u4e0e\u57fa\u7840\u79d1\u5b66\u7684\u8054\u7cfb\uff0c\u901a\u8fc7\u53cc\u5411\u7814\u7a76\u3001\u793e\u533a\u5efa\u8bbe\u548c\u6559\u80b2\u57f9\u8bad\uff0c\u4f7fMPS\u9886\u57df\u80fd\u591f\u5728AI\u53d1\u5c55\u4e2d\u53d1\u6325\u9886\u5bfc\u4f5c\u7528\u5e76\u5145\u5206\u5229\u7528\u5176\u53d8\u9769\u6f5c\u529b\u3002"}}
{"id": "2509.02722", "pdf": "https://arxiv.org/pdf/2509.02722", "abs": "https://arxiv.org/abs/2509.02722", "authors": ["Delong Chen", "Theo Moutakanni", "Willy Chung", "Yejin Bang", "Ziwei Ji", "Allen Bolourchi", "Pascale Fung"], "title": "Planning with Reasoning using Vision Language World Model", "categories": ["cs.AI"], "comment": null, "summary": "Effective planning requires strong world models, but high-level world models\nthat can understand and reason about actions with semantic and temporal\nabstraction remain largely underdeveloped. We introduce the Vision Language\nWorld Model (VLWM), a foundation model trained for language-based world\nmodeling on natural videos. Given visual observations, the VLWM first infers\nthe overall goal achievements then predicts a trajectory composed of\ninterleaved actions and world state changes. Those targets are extracted by\niterative LLM Self-Refine conditioned on compressed future observations\nrepresented by Tree of Captions. The VLWM learns both an action policy and a\ndynamics model, which respectively facilitates reactive system-1 plan decoding\nand reflective system-2 planning via cost minimization. The cost evaluates the\nsemantic distance between the hypothetical future states given by VLWM\nroll-outs and the expected goal state, and is measured by a critic model that\nwe trained in a self-supervised manner. The VLWM achieves state-of-the-art\nVisual Planning for Assistance (VPA) performance on both benchmark evaluations\nand our proposed PlannerArena human evaluations, where system-2 improves the\nElo score by +27% upon system-1. The VLWM models also outperforms strong VLM\nbaselines on RoboVQA and WorldPrediction benchmark.", "AI": {"tldr": "VLWM\u662f\u4e00\u4e2a\u57fa\u4e8e\u89c6\u89c9\u8bed\u8a00\u7684\u4e16\u754c\u6a21\u578b\uff0c\u901a\u8fc7\u8bed\u8a00\u5efa\u6a21\u548c\u53cc\u7cfb\u7edf\u89c4\u5212\uff08\u7cfb\u7edf1\u53cd\u5e94\u5f0f\u89e3\u7801\u548c\u7cfb\u7edf2\u53cd\u601d\u5f0f\u6210\u672c\u6700\u5c0f\u5316\u89c4\u5212\uff09\u5728\u81ea\u7136\u89c6\u9891\u4e0a\u5b9e\u73b0\u8bed\u4e49\u548c\u65f6\u95f4\u62bd\u8c61\u63a8\u7406\uff0c\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\u3002", "motivation": "\u5f53\u524d\u9ad8\u5c42\u6b21\u4e16\u754c\u6a21\u578b\u5728\u7406\u89e3\u548c\u63a8\u7406\u5177\u6709\u8bed\u4e49\u548c\u65f6\u95f4\u62bd\u8c61\u7684\u52a8\u4f5c\u65b9\u9762\u4ecd\u7136\u4e0d\u8db3\uff0c\u9700\u8981\u5f00\u53d1\u80fd\u591f\u8fdb\u884c\u6709\u6548\u89c4\u5212\u7684\u4e16\u754c\u6a21\u578b\u3002", "method": "\u4f7f\u7528LLM\u81ea\u4f18\u5316\u65b9\u6cd5\u4ece\u538b\u7f29\u7684\u672a\u6765\u89c2\u5bdf\u4e2d\u63d0\u53d6\u76ee\u6807\uff0c\u5b66\u4e60\u52a8\u4f5c\u7b56\u7565\u548c\u52a8\u6001\u6a21\u578b\uff0c\u901a\u8fc7\u7cfb\u7edf1\u53cd\u5e94\u5f0f\u89e3\u7801\u548c\u7cfb\u7edf2\u57fa\u4e8e\u6210\u672c\u6700\u5c0f\u5316\u7684\u53cd\u601d\u5f0f\u89c4\u5212\uff0c\u6210\u672c\u7531\u81ea\u76d1\u7763\u8bad\u7ec3\u7684\u8bc4\u8bba\u5bb6\u6a21\u578b\u8bc4\u4f30\u8bed\u4e49\u8ddd\u79bb\u3002", "result": "\u5728VPA\u57fa\u51c6\u8bc4\u4f30\u548cPlannerArena\u4eba\u7c7b\u8bc4\u4f30\u4e2d\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\uff0c\u7cfb\u7edf2\u6bd4\u7cfb\u7edf1Elo\u5206\u6570\u63d0\u534727%\uff0c\u5728RoboVQA\u548cWorldPrediction\u57fa\u51c6\u4e0a\u4e5f\u4f18\u4e8e\u5f3aVLM\u57fa\u7ebf\u3002", "conclusion": "VLWM\u901a\u8fc7\u8bed\u8a00\u57fa\u7840\u7684\u4e16\u754c\u5efa\u6a21\u548c\u53cc\u7cfb\u7edf\u89c4\u5212\u6846\u67b6\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u9ad8\u5c42\u6b21\u4e16\u754c\u5efa\u6a21\u7684\u6311\u6218\uff0c\u4e3a\u89c6\u89c9\u89c4\u5212\u4efb\u52a1\u63d0\u4f9b\u4e86\u5f3a\u5927\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.02924", "pdf": "https://arxiv.org/pdf/2509.02924", "abs": "https://arxiv.org/abs/2509.02924", "authors": ["Nefeli Manoudaki", "Mert Toka", "Iason Paterakis", "Diarmid Flatley"], "title": "Simulacra Naturae: Generative Ecosystem driven by Agent-Based Simulations and Brain Organoid Collective Intelligence", "categories": ["cs.MM", "cs.AI", "cs.HC"], "comment": "to be published in IEEE VISAP 2025", "summary": "Simulacra Naturae is a data-driven media installation that explores\ncollective care through the entanglement of biological computation, material\necologies, and generative systems. The work translates pre-recorded neural\nactivity from brain organoids, lab-grown three-dimensional clusters of neurons,\ninto a multi-sensory environment composed of generative visuals, spatial audio,\nliving plants, and fabricated clay artifacts. These biosignals, streamed\nthrough a real-time system, modulate emergent agent behaviors inspired by\nnatural systems such as termite colonies and slime molds. Rather than using\nbiosignals as direct control inputs, Simulacra Naturae treats organoid activity\nas a co-creative force, allowing neural rhythms to guide the growth, form, and\natmosphere of a generative ecosystem. The installation features computationally\nfabricated clay prints embedded with solenoids, adding physical sound\nresonances to the generative surround composition. The spatial environment,\nfilled with live tropical plants and a floor-level projection layer featuring\nreal-time generative AI visuals, invites participants into a sensory field\nshaped by nonhuman cognition. By grounding abstract data in living materials\nand embodied experience, Simulacra Naturae reimagines visualization as a\npractice of care, one that decentralizes human agency and opens new spaces for\nethics, empathy, and ecological attunement within hybrid computational systems.", "AI": {"tldr": "Simulacra Naturae\u662f\u4e00\u4e2a\u6570\u636e\u9a71\u52a8\u7684\u5a92\u4f53\u88c5\u7f6e\uff0c\u901a\u8fc7\u751f\u7269\u8ba1\u7b97\u3001\u6750\u6599\u751f\u6001\u548c\u751f\u6210\u7cfb\u7edf\u7684\u878d\u5408\u63a2\u7d22\u96c6\u4f53\u5173\u6000\uff0c\u5c06\u8111\u7c7b\u5668\u5b98\u7684\u795e\u7ecf\u6d3b\u52a8\u8f6c\u5316\u4e3a\u591a\u611f\u5b98\u73af\u5883\u3002", "motivation": "\u63a2\u7d22\u901a\u8fc7\u975e\u4eba\u7c7b\u8ba4\u77e5\u5851\u9020\u7684\u611f\u5b98\u573a\u57df\uff0c\u91cd\u65b0\u6784\u60f3\u53ef\u89c6\u5316\u4f5c\u4e3a\u5173\u6000\u5b9e\u8df5\uff0c\u5728\u6df7\u5408\u8ba1\u7b97\u7cfb\u7edf\u4e2d\u5f00\u8f9f\u4f26\u7406\u3001\u5171\u60c5\u548c\u751f\u6001\u534f\u8c03\u7684\u65b0\u7a7a\u95f4\u3002", "method": "\u4f7f\u7528\u9884\u8bb0\u5f55\u7684\u8111\u7c7b\u5668\u5b98\u795e\u7ecf\u6d3b\u52a8\uff0c\u901a\u8fc7\u5b9e\u65f6\u7cfb\u7edf\u8f6c\u5316\u4e3a\u751f\u6210\u89c6\u89c9\u3001\u7a7a\u95f4\u97f3\u9891\u3001\u6d3b\u4f53\u690d\u7269\u548c\u5236\u9020\u7c98\u571f\u6587\u7269\u7684\u591a\u611f\u5b98\u73af\u5883\uff0c\u91c7\u7528\u53d7\u81ea\u7136\u7cfb\u7edf\u542f\u53d1\u7684\u6d8c\u73b0\u4ee3\u7406\u884c\u4e3a\u3002", "result": "\u521b\u5efa\u4e86\u4e00\u4e2a\u7531\u975e\u4eba\u7c7b\u8ba4\u77e5\u5851\u9020\u7684\u611f\u5b98\u73af\u5883\uff0c\u5c06\u62bd\u8c61\u6570\u636e\u951a\u5b9a\u5728\u6d3b\u4f53\u6750\u6599\u548c\u5177\u8eab\u4f53\u9a8c\u4e2d\uff0c\u5b9e\u73b0\u4e86\u53bb\u4e2d\u5fc3\u5316\u7684\u4eba\u7c7b\u4ee3\u7406\u3002", "conclusion": "\u8be5\u4f5c\u54c1\u901a\u8fc7\u751f\u7269\u4fe1\u53f7\u4f5c\u4e3a\u5171\u521b\u529b\u91cf\u800c\u975e\u76f4\u63a5\u63a7\u5236\u8f93\u5165\uff0c\u4e3a\u6df7\u5408\u8ba1\u7b97\u7cfb\u7edf\u4e2d\u7684\u4f26\u7406\u548c\u751f\u6001\u534f\u8c03\u5f00\u8f9f\u4e86\u65b0\u9014\u5f84\uff0c\u91cd\u65b0\u5b9a\u4e49\u4e86\u53ef\u89c6\u5316\u4f5c\u4e3a\u5173\u6000\u5b9e\u8df5\u7684\u610f\u4e49\u3002"}}
{"id": "2509.02751", "pdf": "https://arxiv.org/pdf/2509.02751", "abs": "https://arxiv.org/abs/2509.02751", "authors": ["Matthew Russo", "Tim Kraska"], "title": "Deep Research is the New Analytics System: Towards Building the Runtime for AI-Driven Analytics", "categories": ["cs.AI", "cs.DB", "cs.LG", "cs.MA", "I.2.1; H.3.3; H.2.4"], "comment": "6 pages, 2 figures, submitted to CIDR'26", "summary": "With advances in large language models (LLMs), researchers are creating new\nsystems that can perform AI-driven analytics over large unstructured datasets.\nRecent work has explored executing such analytics queries using semantic\noperators -- a declarative set of AI-powered data transformations with natural\nlanguage specifications. However, even when optimized, these operators can be\nexpensive to execute on millions of records and their iterator execution\nsemantics make them ill-suited for interactive data analytics tasks. In another\nline of work, Deep Research systems have demonstrated an ability to answer\nnatural language question(s) over large datasets. These systems use one or more\nLLM agent(s) to plan their execution, process the dataset(s), and iteratively\nrefine their answer. However, these systems do not explicitly optimize their\nquery plans which can lead to poor plan execution. In order for AI-driven\nanalytics to excel, we need a runtime which combines the optimized execution of\nsemantic operators with the flexibility and more dynamic execution of Deep\nResearch systems. As a first step towards this vision, we build a prototype\nwhich enables Deep Research agents to write and execute optimized semantic\noperator programs. We evaluate our prototype and demonstrate that it can\noutperform a handcrafted semantic operator program and open Deep Research\nsystems on two basic queries. Compared to a standard open Deep Research agent,\nour prototype achieves up to 1.95x better F1-score. Furthermore, even if we\ngive the agent access to semantic operators as tools, our prototype still\nachieves cost and runtime savings of up to 76.8% and 72.7% thanks to its\noptimized execution.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684AI\u9a71\u52a8\u5206\u6790\u8fd0\u884c\u65f6\uff0c\u7ed3\u5408\u4e86\u4f18\u5316\u6267\u884c\u7684\u8bed\u4e49\u64cd\u4f5c\u7b26\u548c\u6df1\u5ea6\u7814\u7a76\u7cfb\u7edf\u7684\u7075\u6d3b\u6027\uff0c\u901a\u8fc7\u8ba9LLM\u7ec4\u4ef6\u7f16\u5199\u548c\u6267\u884c\u4f18\u5316\u7684\u8bed\u4e49\u64cd\u4f5c\u7b26\u7a0b\u5e8f\uff0c\u5b9e\u73b0\u4e86\u66f4\u9ad8\u7684\u6027\u80fd\u548c\u66f4\u4f4e\u7684\u6210\u672c\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u8bed\u4e49\u64cd\u4f5c\u7b26\u5728\u5927\u89c4\u6a21\u6570\u636e\u96c6\u4e0a\u6267\u884c\u6210\u672c\u9ad8\u3001\u4e0d\u9002\u5408\u4ea4\u4e92\u5f0f\u5206\u6790\u7684\u95ee\u9898\uff0c\u4ee5\u53ca\u6df1\u5ea6\u7814\u7a76\u7cfb\u7edf\u7f3a\u4e4f\u67e5\u8be2\u8ba1\u5212\u4f18\u5316\u5bfc\u81f4\u6267\u884c\u6548\u7387\u4f4e\u7684\u95ee\u9898\u3002", "method": "\u6784\u5efa\u4e86\u4e00\u4e2a\u539f\u578b\u7cfb\u7edf\uff0c\u8ba9\u6df1\u5ea6\u7814\u7a76\u7cfb\u7edf\u4e2d\u7684LLM\u7ec4\u4ef6\u80fd\u591f\u7f16\u5199\u548c\u6267\u884c\u7ecf\u8fc7\u4f18\u5316\u7684\u8bed\u4e49\u64cd\u4f5c\u7b26\u7a0b\u5e8f\uff0c\u7ed3\u5408\u4e86\u4e24\u79cd\u65b9\u6cd5\u7684\u4f18\u52bf\u3002", "result": "\u5728\u4e24\u4e2a\u57fa\u7840\u67e5\u8be2\u4e0a\uff0c\u8be5\u539f\u578b\u7cfb\u7edf\u8d85\u8fc7\u4e86\u624b\u5de5\u7f16\u5199\u7684\u8bed\u4e49\u64cd\u4f5c\u7b26\u7a0b\u5e8f\u548c\u5f00\u653e\u6df1\u5ea6\u7814\u7a76\u7cfb\u7edf\uff0c\u4e0e\u6807\u51c6\u6df1\u5ea6\u7814\u7a76\u7ec4\u4ef6\u76f8\u6bd4F1\u5206\u6570\u63d0\u9ad8\u4e861.95\u500d\uff0c\u6210\u672c\u548c\u8fd0\u884c\u65f6\u95f4\u8282\u7701\u4e86\u670076.8%\u548c72.7%\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3aAI\u9a71\u52a8\u5206\u6790\u9886\u57df\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u524d\u666f\u7684\u65b9\u5411\uff0c\u901a\u8fc7\u7ed3\u5408\u4f18\u5316\u6267\u884c\u548c\u7075\u6d3b\u52a8\u6001\u6267\u884c\u7684\u4f18\u52bf\uff0c\u80fd\u591f\u5728\u4fdd\u6301\u9ad8\u51c6\u786e\u6027\u7684\u540c\u65f6\u663e\u8457\u964d\u4f4e\u8fd0\u884c\u6210\u672c\u3002"}}
{"id": "2509.02990", "pdf": "https://arxiv.org/pdf/2509.02990", "abs": "https://arxiv.org/abs/2509.02990", "authors": ["Liang Xie", "Wenke Huang"], "title": "Automatically Generating High-Precision Simulated Road Networking in Traffic Scenario", "categories": ["cs.MM"], "comment": "7 pages,11 figures", "summary": "Existing lane-level simulation road network generation is labor-intensive,\nresource-demanding, and costly due to the need for large-scale data collection\nand manual post-editing. To overcome these limitations, we propose\nautomatically generating high-precision simulated road networks in traffic\nscenario, an efficient and fully automated solution. Initially, real-world road\nstreet view data is collected through open-source street view map platforms,\nand a large-scale street view lane line dataset is constructed to provide a\nrobust foundation for subsequent analysis. Next, an end-to-end lane line\ndetection approach based on deep learning is designed, where a neural network\nmodel is trained to accurately detect the number and spatial distribution of\nlane lines in street view images, enabling automated extraction of lane\ninformation. Subsequently, by integrating coordinate transformation and map\nmatching algorithms, the extracted lane information from street views is fused\nwith the foundational road topology obtained from open-source map service\nplatforms, resulting in the generation of a high-precision lane-level\nsimulation road network. This method significantly reduces the costs associated\nwith data collection and manual editing while enhancing the efficiency and\naccuracy of simulation road network generation. It provides reliable data\nsupport for urban traffic simulation, autonomous driving navigation, and the\ndevelopment of intelligent transportation systems, offering a novel technical\npathway for the automated modeling of large-scale urban road networks.", "AI": {"tldr": "\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u548c\u5f00\u6e90\u5730\u56fe\u6570\u636e\u7684\u5168\u81ea\u52a8\u9ad8\u7cbe\u5ea6\u8f66\u9053\u7ea7\u6a21\u62df\u8def\u7f51\u751f\u6210\u65b9\u6cd5", "motivation": "\u73b0\u6709\u8f66\u9053\u7ea7\u6a21\u62df\u8def\u7f51\u751f\u6210\u65b9\u6cd5\u9700\u8981\u5927\u89c4\u6a21\u6570\u636e\u6536\u96c6\u548c\u624b\u5de5\u540e\u671f\u7f16\u8f91\uff0c\u82af\u7247\u5f3a\u5ea6\u9ad8\u3001\u6210\u672c\u8f83\u5927", "method": "\u6536\u96c6\u5f00\u6e90\u8857\u666f\u6570\u636e\u6784\u5efa\u8f66\u9053\u7ebf\u6570\u636e\u96c6\uff0c\u8bbe\u8ba1\u7aef\u5230\u7aef\u6df1\u5ea6\u5b66\u4e60\u8f66\u9053\u7ebf\u68c0\u6d4b\u6a21\u578b\uff0c\u7ed3\u5408\u5750\u6807\u53d8\u6362\u548c\u5730\u56fe\u5339\u914d\u7b97\u6cd5\u878d\u5408\u8f66\u9053\u4fe1\u606f\u4e0e\u8def\u7f51\u62d3\u6251\u7ed3\u6784", "result": "\u663e\u8457\u964d\u4f4e\u4e86\u6570\u636e\u6536\u96c6\u548c\u624b\u5de5\u7f16\u8f91\u6210\u672c\uff0c\u63d0\u9ad8\u4e86\u6a21\u62df\u8def\u7f51\u751f\u6210\u7684\u6548\u7387\u548c\u51c6\u786e\u6027", "conclusion": "\u4e3a\u57ce\u5e02\u4ea4\u901a\u6a21\u62df\u3001\u81ea\u4e3b\u9a7e\u9a76\u5bfc\u822a\u548c\u667a\u80fd\u4ea4\u901a\u7cfb\u7edf\u63d0\u4f9b\u4e86\u53ef\u9760\u6570\u636e\u652f\u6301\uff0c\u4e3a\u5927\u89c4\u6a21\u57ce\u5e02\u8def\u7f51\u81ea\u52a8\u5efa\u6a21\u63d0\u4f9b\u4e86\u65b0\u6280\u672f\u8def\u5f84"}}
{"id": "2509.02605", "pdf": "https://arxiv.org/pdf/2509.02605", "abs": "https://arxiv.org/abs/2509.02605", "authors": ["Jorn K. Teutloff"], "title": "Synthetic Founders: AI-Generated Social Simulations for Startup Validation Research in Computational Social Science", "categories": ["cs.MA", "cs.AI", "cs.CY", "68T42 (Primary) 91F99, 92D50 (Secondary)", "I.6.3; I.2.11; J.4"], "comment": "Manuscript submitted to the Journal of Artificial Societies and\n  Social Simulation (JASSS). 21 pages, 1 table", "summary": "We present a comparative docking experiment that aligns human-subject\ninterview data with large language model (LLM)-driven synthetic personas to\nevaluate fidelity, divergence, and blind spots in AI-enabled simulation.\nFifteen early-stage startup founders were interviewed about their hopes and\nconcerns regarding AI-powered validation, and the same protocol was replicated\nwith AI-generated founder and investor personas. A structured thematic\nsynthesis revealed four categories of outcomes: (1) Convergent themes -\ncommitment-based demand signals, black-box trust barriers, and efficiency gains\nwere consistently emphasized across both datasets; (2) Partial overlaps -\nfounders worried about outliers being averaged away and the stress of real\ncustomer validation, while synthetic personas highlighted irrational blind\nspots and framed AI as a psychological buffer; (3) Human-only themes -\nrelational and advocacy value from early customer engagement and skepticism\ntoward moonshot markets; and (4) Synthetic-only themes - amplified false\npositives and trauma blind spots, where AI may overstate adoption potential by\nmissing negative historical experiences.\n  We interpret this comparative framework as evidence that LLM-driven personas\nconstitute a form of hybrid social simulation: more linguistically expressive\nand adaptable than traditional rule-based agents, yet bounded by the absence of\nlived history and relational consequence. Rather than replacing empirical\nstudies, we argue they function as a complementary simulation category -\ncapable of extending hypothesis space, accelerating exploratory validation, and\nclarifying the boundaries of cognitive realism in computational social science.", "AI": {"tldr": "\u8fd9\u662f\u4e00\u4e2a\u6bd4\u8f83\u6027\u5bf9\u63a5\u5b9e\u9a8c\uff0c\u5c06\u4eba\u7c7b\u8bbf\u8c08\u6570\u636e\u4e0eLLM\u9a71\u52a8\u7684\u5408\u6210\u4eba\u8bbe\u8fdb\u884c\u5bf9\u6bd4\uff0c\u8bc4\u4f30AI\u6a21\u62df\u7684\u4fdd\u771f\u5ea6\u3001\u5206\u5f02\u548c\u76f2\u70b9\u3002", "motivation": "\u8bc4\u4f30\u5927\u8bed\u8a00\u6a21\u578b\u9a71\u52a8\u7684\u5408\u6210\u4eba\u8bbe\u5728AI\u6a21\u62df\u4e2d\u7684\u4fdd\u771f\u5ea6\u548c\u5c40\u9650\u6027\uff0c\u63a2\u7d22\u5176\u4f5c\u4e3a\u8ba4\u77e5\u793e\u4f1a\u79d1\u5b66\u5de5\u5177\u7684\u6f5c\u529b\u4e0e\u8fb9\u754c\u3002", "method": "\u91c7\u8bbf15\u540d\u521d\u521b\u516c\u53f8\u521b\u59cb\u4eba\u5e76\u4e0eAI\u751f\u6210\u7684\u521b\u59cb\u4eba\u548c\u6295\u8d44\u8005\u4eba\u8bbe\u8fdb\u884c\u540c\u6837\u534f\u8bae\u7684\u5bf9\u8bdd\u5b9e\u9a8c\uff0c\u901a\u8fc7\u7ed3\u6784\u5316\u4e3b\u9898\u7efc\u5408\u5206\u6790\u6570\u636e\u3002", "result": "\u53d1\u73b0\u56db\u7c7b\u7ed3\u679c\uff1a\u6536\u655b\u4e3b\u9898\u3001\u90e8\u5206\u91cd\u53e0\u3001\u4ec5\u4eba\u7c7b\u4e3b\u9898\u548c\u4ec5\u5408\u6210\u4eba\u8bbe\u4e3b\u9898\uff0c\u663e\u793aLLM\u4eba\u8bbe\u66f4\u8bed\u8a00\u8868\u8fbe\u6027\u5f3a\u4f46\u7f3a\u4e4f\u751f\u6d3b\u7ecf\u9a8c\u3002", "conclusion": "LLM\u9a71\u52a8\u4eba\u8bbe\u6784\u6210\u4e86\u4e00\u79cd\u6df7\u5408\u793e\u4f1a\u6a21\u62df\u5f62\u5f0f\uff0c\u4f5c\u4e3a\u5b8c\u5584\u5b9e\u8bc1\u7814\u7a76\u7684\u8865\u5145\u5de5\u5177\uff0c\u80fd\u6269\u5c55\u5047\u8bbe\u7a7a\u95f4\u3001\u52a0\u901f\u63a2\u7d22\u6027\u9a8c\u8bc1\u5e76\u660e\u786e\u8ba4\u77e5\u73b0\u5b9e\u4e3b\u4e49\u7684\u8fb9\u754c\u3002"}}
{"id": "2509.02771", "pdf": "https://arxiv.org/pdf/2509.02771", "abs": "https://arxiv.org/abs/2509.02771", "authors": ["Nirmalya Mallick Thakur", "Jia Qi Yip", "Eng Siong Chng"], "title": "Analysis of Speaker Verification Performance Trade-offs with Neural Audio Codec Transmission", "categories": ["cs.SD", "eess.AS"], "comment": "Accepted by APSIPA ASC 2025", "summary": "Neural audio codecs (NACs) have made significant advancements in recent years\nand are rapidly being adopted in many audio processing pipelines. However, they\ncan introduce audio distortions which degrade speaker verification (SV)\nperformance. This study investigates the impact of both traditional and neural\naudio codecs at varying bitrates on three state of-the-art SV models evaluated\non the VoxCeleb1 dataset. Our findings reveal a consistent degradation in SV\nperformance across all models and codecs as bitrates decrease. Notably, NACs do\nnot fundamentally break SV performance when compared to traditional codecs.\nThey outperform Opus by 6-8% at low-bitrates (< 12 kbps) and remain marginally\nbehind at higher bitrates ($\\approx$ 24 kbps), with an EER increase of only\n0.4-0.7%. The disparity at higher bitrates is likely due to the primary\noptimization of NACs for perceptual quality, which can inadvertently discard\ncritical speaker-discriminative features, unlike Opus which was designed to\npreserve vocal characteristics. Our investigation suggests that NACs are a\nfeasible alternative to traditional codecs, especially under bandwidth\nlimitations. To bridge the gap at higher bitrates, future work should focus on\ndeveloping speaker-aware NACs or retraining and adapting SV models.", "AI": {"tldr": "\u795e\u7ecf\u97f3\u9891\u7f16\u89e3\u7801\u5668(NACs)\u5728\u4f4e\u6bd4\u7279\u7387(<12kbps)\u4e0b\u6bd4\u4f20\u7edf\u7f16\u89e3\u7801\u5668Opus\u6027\u80fd\u597d6-8%\uff0c\u4f46\u5728\u9ad8\u6bd4\u7279\u7387(\u224824kbps)\u4e0b\u7565\u5dee\uff0c\u4ec5\u5bfc\u81f4EER\u589e\u52a00.4-0.7%\u3002NACs\u4e3b\u8981\u4f18\u5316\u611f\u77e5\u8d28\u91cf\uff0c\u53ef\u80fd\u4e22\u5f03\u8bf4\u8bdd\u4eba\u533a\u5206\u7279\u5f81\uff0c\u4f46\u4ecd\u662f\u5e26\u5bbd\u53d7\u9650\u60c5\u51b5\u4e0b\u7684\u53ef\u884c\u66ff\u4ee3\u65b9\u6848\u3002", "motivation": "\u7814\u7a76\u795e\u7ecf\u97f3\u9891\u7f16\u89e3\u7801\u5668(NACs)\u5bf9\u8bf4\u8bdd\u4eba\u9a8c\u8bc1(SV)\u6027\u80fd\u7684\u5f71\u54cd\uff0c\u56e0\u4e3aNACs\u867d\u7136\u8fd1\u5e74\u6765\u53d6\u5f97\u663e\u8457\u8fdb\u5c55\u5e76\u88ab\u5e7f\u6cdb\u91c7\u7528\uff0c\u4f46\u53ef\u80fd\u5f15\u5165\u97f3\u9891\u5931\u771f\u4ece\u800c\u964d\u4f4eSV\u6027\u80fd\u3002", "method": "\u5728VoxCeleb1\u6570\u636e\u96c6\u4e0a\u8bc4\u4f30\u4e09\u79cd\u6700\u5148\u8fdb\u7684SV\u6a21\u578b\uff0c\u6bd4\u8f83\u4f20\u7edf\u548c\u795e\u7ecf\u97f3\u9891\u7f16\u89e3\u7801\u5668\u5728\u4e0d\u540c\u6bd4\u7279\u7387\u4e0b\u7684\u6027\u80fd\u8868\u73b0\u3002", "result": "\u968f\u7740\u6bd4\u7279\u7387\u964d\u4f4e\uff0c\u6240\u6709\u6a21\u578b\u548c\u7f16\u89e3\u7801\u5668\u7684SV\u6027\u80fd\u90fd\u4e00\u81f4\u4e0b\u964d\u3002NACs\u5728\u4f4e\u6bd4\u7279\u7387(<12kbps)\u4e0b\u6bd4Opus\u6027\u80fd\u597d6-8%\uff0c\u5728\u9ad8\u6bd4\u7279\u7387(\u224824kbps)\u4e0b\u7565\u5dee\uff0cEER\u4ec5\u589e\u52a00.4-0.7%\u3002", "conclusion": "NACs\u662f\u4f20\u7edf\u7f16\u89e3\u7801\u5668\u7684\u53ef\u884c\u66ff\u4ee3\u65b9\u6848\uff0c\u7279\u522b\u662f\u5728\u5e26\u5bbd\u53d7\u9650\u60c5\u51b5\u4e0b\u3002\u672a\u6765\u5de5\u4f5c\u9700\u8981\u5f00\u53d1\u8bf4\u8bdd\u4eba\u611f\u77e5\u7684NACs\u6216\u91cd\u65b0\u8bad\u7ec3\u9002\u5e94SV\u6a21\u578b\u6765\u5f25\u5408\u9ad8\u6bd4\u7279\u7387\u4e0b\u7684\u5dee\u8ddd\u3002"}}
{"id": "2509.02785", "pdf": "https://arxiv.org/pdf/2509.02785", "abs": "https://arxiv.org/abs/2509.02785", "authors": ["Jusheng Zhang", "Yijia Fan", "Kaitong Cai", "Zimeng Huang", "Xiaofei Sun", "Jian Wang", "Chengpei Tang", "Keze Wang"], "title": "DrDiff: Dynamic Routing Diffusion with Hierarchical Attention for Breaking the Efficiency-Quality Trade-off", "categories": ["cs.CL", "cs.AI"], "comment": "Accepted 2025 EMNLP (MainConference)", "summary": "This paper introduces DrDiff, a novel framework for long-text generation that\novercomes the efficiency-quality trade-off through three core technologies.\nFirst, we design a dynamic expert scheduling mechanism that intelligently\nallocates computational resources during the diffusion process based on text\ncomplexity, enabling more efficient handling of text generation tasks of\nvarying difficulty. Second, we introduce a Hierarchical Sparse Attention (HSA)\nmechanism that adaptively adjusts attention patterns according to a variety of\ninput lengths, reducing computational complexity from O($n^2$) to O($n$) while\nmaintaining model performance. Finally, we propose a soft absorption guidance\noptimization strategy that combines with DPM-solver++ to reduce diffusion\nsteps, significantly improving generation speed. Comprehensive experiments on\nvarious long-text generation benchmarks demonstrate the superiority of our\nDrDiff over the existing SOTA methods.", "AI": {"tldr": "DrDiff\u662f\u4e00\u4e2a\u65b0\u9896\u7684\u957f\u6587\u672c\u751f\u6210\u6846\u67b6\uff0c\u901a\u8fc7\u52a8\u6001\u4e13\u5bb6\u8c03\u5ea6\u3001\u5206\u5c42\u7a00\u758f\u6ce8\u610f\u529b\u548c\u8f6f\u5438\u6536\u5f15\u5bfc\u4f18\u5316\u4e09\u9879\u6838\u5fc3\u6280\u672f\uff0c\u89e3\u51b3\u4e86\u6548\u7387\u4e0e\u8d28\u91cf\u4e4b\u95f4\u7684\u6743\u8861\u95ee\u9898\u3002", "motivation": "\u89e3\u51b3\u957f\u6587\u672c\u751f\u6210\u4e2d\u6548\u7387\u4e0e\u8d28\u91cf\u4e4b\u95f4\u7684\u6743\u8861\u95ee\u9898\uff0c\u4f20\u7edf\u65b9\u6cd5\u5728\u5904\u7406\u4e0d\u540c\u96be\u5ea6\u548c\u957f\u5ea6\u7684\u6587\u672c\u65f6\u8ba1\u7b97\u6548\u7387\u4f4e\u4e0b\u3002", "method": "1) \u52a8\u6001\u4e13\u5bb6\u8c03\u5ea6\u673a\u5236\uff1a\u6839\u636e\u6587\u672c\u590d\u6742\u5ea6\u667a\u80fd\u5206\u914d\u8ba1\u7b97\u8d44\u6e90\uff1b2) \u5206\u5c42\u7a00\u758f\u6ce8\u610f\u529b\u673a\u5236\uff1a\u5c06\u8ba1\u7b97\u590d\u6742\u5ea6\u4eceO(n\u00b2)\u964d\u4f4e\u5230O(n)\uff1b3) \u8f6f\u5438\u6536\u5f15\u5bfc\u4f18\u5316\u7b56\u7565\uff1a\u7ed3\u5408DPM-solver++\u51cf\u5c11\u6269\u6563\u6b65\u9aa4\u3002", "result": "\u5728\u5404\u79cd\u957f\u6587\u672c\u751f\u6210\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cDrDiff\u8868\u73b0\u51fa\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u65b9\u6cd5\u7684\u6027\u80fd\u3002", "conclusion": "DrDiff\u6846\u67b6\u901a\u8fc7\u4e09\u9879\u521b\u65b0\u6280\u672f\u6709\u6548\u89e3\u51b3\u4e86\u957f\u6587\u672c\u751f\u6210\u7684\u6548\u7387-\u8d28\u91cf\u6743\u8861\u95ee\u9898\uff0c\u5728\u4fdd\u6301\u6a21\u578b\u6027\u80fd\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u4e86\u751f\u6210\u901f\u5ea6\u3002"}}
{"id": "2509.02727", "pdf": "https://arxiv.org/pdf/2509.02727", "abs": "https://arxiv.org/abs/2509.02727", "authors": ["Guillaume Gagn\u00e9-Labelle", "Vassil Atanassov", "Ioannis Havoutis"], "title": "Acrobotics: A Generalist Approahc To Quadrupedal Robots' Parkour", "categories": ["cs.RO"], "comment": "Supplementary material can be found here:\n  https://drive.google.com/drive/folders/18h25azbCFfPF4fhSsRfxKrnZo3dPKs_j?usp=sharing", "summary": "Climbing, crouching, bridging gaps, and walking up stairs are just a few of\nthe advantages that quadruped robots have over wheeled robots, making them more\nsuitable for navigating rough and unstructured terrain. However, executing such\nmanoeuvres requires precise temporal coordination and complex agent-environment\ninteractions. Moreover, legged locomotion is inherently more prone to slippage\nand tripping, and the classical approach of modeling such cases to design a\nrobust controller thus quickly becomes impractical. In contrast, reinforcement\nlearning offers a compelling solution by enabling optimal control through trial\nand error. We present a generalist reinforcement learning algorithm for\nquadrupedal agents in dynamic motion scenarios. The learned policy rivals\nstate-of-the-art specialist policies trained using a mixture of experts\napproach, while using only 25% as many agents during training. Our experiments\nalso highlight the key components of the generalist locomotion policy and the\nprimary factors contributing to its success.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u7528\u7684\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\u7528\u4e8e\u56db\u8db3\u673a\u5668\u4eba\u7684\u52a8\u6001\u8fd0\u52a8\u63a7\u5236\uff0c\u8be5\u7b97\u6cd5\u5728\u8bad\u7ec3\u65f6\u4ec5\u970025%\u7684\u667a\u80fd\u4f53\u6570\u91cf\u5c31\u80fd\u8fbe\u5230\u4e13\u5bb6\u6df7\u5408\u65b9\u6cd5\u7684\u6027\u80fd\u6c34\u5e73\u3002", "motivation": "\u56db\u8db3\u673a\u5668\u4eba\u76f8\u6bd4\u8f6e\u5f0f\u673a\u5668\u4eba\u5728\u590d\u6742\u5730\u5f62\u5bfc\u822a\u65b9\u9762\u5177\u6709\u4f18\u52bf\uff0c\u4f46\u4f20\u7edf\u5efa\u6a21\u65b9\u6cd5\u96be\u4ee5\u5904\u7406\u6ed1\u5012\u548c\u7eca\u5012\u7b49\u590d\u6742\u60c5\u51b5\uff0c\u800c\u5f3a\u5316\u5b66\u4e60\u901a\u8fc7\u8bd5\u9519\u5b66\u4e60\u80fd\u591f\u5b9e\u73b0\u6700\u4f18\u63a7\u5236\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u79cd\u901a\u7528\u7684\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\uff0c\u7528\u4e8e\u8bad\u7ec3\u56db\u8db3\u673a\u5668\u4eba\u5728\u52a8\u6001\u8fd0\u52a8\u573a\u666f\u4e2d\u7684\u63a7\u5236\u7b56\u7565\uff0c\u907f\u514d\u4e86\u590d\u6742\u7684\u4e13\u5bb6\u6df7\u5408\u65b9\u6cd5\u3002", "result": "\u5b66\u4e60\u5230\u7684\u7b56\u7565\u5728\u6027\u80fd\u4e0a\u53ef\u4e0e\u6700\u5148\u8fdb\u7684\u4e13\u5bb6\u6df7\u5408\u65b9\u6cd5\u76f8\u5ab2\u7f8e\uff0c\u540c\u65f6\u8bad\u7ec3\u6240\u9700\u7684\u667a\u80fd\u4f53\u6570\u91cf\u51cf\u5c11\u4e8675%\u3002", "conclusion": "\u8be5\u7814\u7a76\u5c55\u793a\u4e86\u901a\u7528\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\u5728\u56db\u8db3\u673a\u5668\u4eba\u8fd0\u52a8\u63a7\u5236\u4e2d\u7684\u6709\u6548\u6027\uff0c\u5e76\u8bc6\u522b\u4e86\u5173\u952e\u6210\u529f\u56e0\u7d20\u548c\u7b56\u7565\u7ec4\u4ef6\u3002"}}
{"id": "2509.02659", "pdf": "https://arxiv.org/pdf/2509.02659", "abs": "https://arxiv.org/abs/2509.02659", "authors": ["Zilong Guo", "Yi Luo", "Long Sha", "Dongxu Wang", "Panqu Wang", "Chenyang Xu", "Yi Yang"], "title": "2nd Place Solution for CVPR2024 E2E Challenge: End-to-End Autonomous Driving Using Vision Language Model", "categories": ["cs.CV", "cs.RO"], "comment": "2nd place in CVPR 2024 End-to-End Driving at Scale Challenge", "summary": "End-to-end autonomous driving has drawn tremendous attention recently. Many\nworks focus on using modular deep neural networks to construct the end-to-end\narchi-tecture. However, whether using powerful large language models (LLM),\nespecially multi-modality Vision Language Models (VLM) could benefit the\nend-to-end driving tasks remain a question. In our work, we demonstrate that\ncombining end-to-end architectural design and knowledgeable VLMs yield\nimpressive performance on the driving tasks. It is worth noting that our method\nonly uses a single camera and is the best camera-only solution across the\nleaderboard, demonstrating the effectiveness of vision-based driving approach\nand the potential for end-to-end driving tasks.", "AI": {"tldr": "\u57fa\u4e8e\u591a\u6a21\u6001\u89c6\u89c9\u8bed\u8a00\u6a21\u578b(VLM)\u7684\u7aef\u5230\u7aef\u81ea\u4e3b\u9a7e\u9a76\u65b9\u6848\uff0c\u4ec5\u4f7f\u7528\u5355\u76ee\u6444\u50cf\u5934\u5373\u53d6\u5f97\u9886\u5148\u6027\u80fd", "motivation": "\u63a2\u7d22\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u548c\u591a\u6a21\u6001\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\u662f\u5426\u80fd\u591f\u63d0\u5347\u7aef\u5230\u7aef\u81ea\u4e3b\u9a7e\u9a76\u4efb\u52a1\u7684\u6027\u80fd", "method": "\u7ed3\u5408\u7aef\u5230\u7aef\u67b6\u6784\u8bbe\u8ba1\u548c\u77e5\u8bc6\u4e30\u5bcc\u7684VLM\u6a21\u578b", "result": "\u5728\u9a7e\u9a76\u4efb\u52a1\u4e0a\u53d6\u5f97\u4e86\u5370\u8c61\u6df1\u523b\u7684\u6027\u80fd\uff0c\u662f\u76ee\u524d\u6700\u4f73\u7684\u5355\u76ee\u6444\u50cf\u5934\u89e3\u51b3\u65b9\u6848", "conclusion": "\u8bc1\u660e\u4e86\u57fa\u4e8e\u89c6\u89c9\u7684\u9a7e\u9a76\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u5c55\u793a\u4e86\u7aef\u5230\u7aef\u9a7e\u9a76\u4efb\u52a1\u7684\u5f88\u5927\u6f5c\u529b"}}
{"id": "2509.02575", "pdf": "https://arxiv.org/pdf/2509.02575", "abs": "https://arxiv.org/abs/2509.02575", "authors": ["Zichuan Yang"], "title": "The Lifecycle Principle: Stabilizing Dynamic Neural Networks with State Memory", "categories": ["cs.LG", "cs.AI"], "comment": "8 pages, 1 figure", "summary": "I investigate a stronger form of regularization by deactivating neurons for\nextended periods, a departure from the temporary changes of methods like\nDropout. However, this long-term dynamism introduces a critical challenge:\nsevere training instability when neurons are revived with random weights. To\nsolve this, I propose the Lifecycle (LC) principle, a regularization mechanism\ncentered on a key innovation: state memory. Instead of re-initializing a\nrevived neuron, my method restores its parameters to their last known effective\nstate. This process preserves learned knowledge and avoids destructive\noptimization shocks. My theoretical analysis reveals that the LC principle\nsmooths the loss landscape, guiding optimization towards flatter minima\nassociated with better generalization. Experiments on image classification\nbenchmarks demonstrate that my method improves generalization and robustness.\nCrucially, ablation studies confirm that state memory is essential for\nachieving these gains.", "AI": {"tldr": "\u901a\u8fc7\u957f\u671f\u795e\u7ecf\u5143\u505c\u7528\u548c\u72ec\u521b\u7684\u72b6\u6001\u8bb0\u5fc6\u673a\u5236\uff0c\u8be5\u65b9\u6cd5\u63d0\u51fa\u4e86\u4e00\u79cd\u66f4\u5f3a\u7684\u6b63\u5219\u5316\u65b9\u5f0f\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u795e\u7ecf\u5143\u590d\u6d3b\u65f6\u7684\u8bad\u7ec3\u4e0d\u7a33\u5b9a\u95ee\u9898\uff0c\u63d0\u5347\u4e86\u6a21\u578b\u7684\u666e\u901a\u5316\u80fd\u529b\u548c\u7a33\u5065\u6027\u3002", "motivation": "\u4f20\u7edf\u6b63\u5219\u5316\u65b9\u6cd5\u5982Dropout\u4ec5\u4f5c\u77ed\u6682\u7684\u795e\u7ecf\u5143\u53d8\u5316\uff0c\u672c\u7814\u7a76\u63a2\u7d22\u957f\u671f\u795e\u7ecf\u5143\u505c\u7528\u7684\u66f4\u5f3a\u6b63\u5219\u5316\u6548\u679c\uff0c\u4f46\u8fd9\u79cd\u65b9\u6cd5\u4f1a\u5bfc\u81f4\u795e\u7ecf\u5143\u590d\u6d3b\u65f6\u7684\u4e25\u91cd\u8bad\u7ec3\u4e0d\u7a33\u5b9a\u95ee\u9898\u3002", "method": "\u63d0\u51faLifecycle\uff08LC\uff09\u539f\u5219\uff0c\u6838\u5fc3\u662f\u72b6\u6001\u8bb0\u5fc6\u673a\u5236\u3002\u5f53\u795e\u7ecf\u5143\u88ab\u590d\u6d3b\u65f6\uff0c\u4e0d\u662f\u91cd\u65b0\u521d\u59cb\u5316\u6743\u91cd\uff0c\u800c\u662f\u6062\u590d\u5230\u5176\u6700\u540e\u4e00\u6b21\u6709\u6548\u8fd0\u884c\u7684\u53c2\u6570\u72b6\u6001\uff0c\u4fdd\u7559\u5df2\u5b66\u4e60\u77e5\u8bc6\u5e76\u907f\u514d\u7834\u574f\u6027\u7684\u4f18\u5316\u51b2\u51fb\u3002", "result": "\u7406\u8bba\u5206\u6790\u663e\u793aLifecycle\u539f\u5219\u80fd\u591f\u5e73\u6ed1\u635f\u5931\u5730\u5f62\uff0c\u5bfc\u5411\u66f4\u5e73\u5766\u7684\u6700\u5c0f\u503c\uff0c\u8fd9\u4e9b\u6700\u5c0f\u503c\u901a\u5e38\u4e0e\u66f4\u597d\u7684\u666e\u901a\u5316\u80fd\u529b\u76f8\u5173\u3002\u56fe\u50cf\u5206\u7c7b\u5b9e\u9a8c\u8868\u660e\u8be5\u65b9\u6cd5\u80fd\u591f\u63d0\u5347\u666e\u901a\u5316\u80fd\u529b\u548c\u7a33\u5065\u6027\u3002", "conclusion": "\u72b6\u6001\u8bb0\u5fc6\u673a\u5236\u662f\u5b9e\u73b0\u6027\u80fd\u63d0\u5347\u7684\u5173\u952e\u8981\u7d20\uff0c\u901a\u8fc7\u4fdd\u7559\u795e\u7ecf\u5143\u5386\u53f2\u6709\u6548\u72b6\u6001\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u957f\u671f\u795e\u7ecf\u5143\u505c\u7528\u5e26\u6765\u7684\u8bad\u7ec3\u4e0d\u7a33\u5b9a\u95ee\u9898\uff0c\u4e3a\u6df1\u5ea6\u5b66\u4e60\u6b63\u5219\u5316\u63d0\u4f9b\u4e86\u65b0\u7684\u89c6\u89d2\u3002"}}
{"id": "2509.02754", "pdf": "https://arxiv.org/pdf/2509.02754", "abs": "https://arxiv.org/abs/2509.02754", "authors": ["Mingyi Wang", "Jingke Wang", "Tengju Ye", "Junbo Chen", "Kaicheng Yu"], "title": "Do LLM Modules Generalize? A Study on Motion Generation for Autonomous Driving", "categories": ["cs.AI"], "comment": "CoRL 2025", "summary": "Recent breakthroughs in large language models (LLMs) have not only advanced\nnatural language processing but also inspired their application in domains with\nstructurally similar problems--most notably, autonomous driving motion\ngeneration. Both domains involve autoregressive sequence modeling, token-based\nrepresentations, and context-aware decision making, making the transfer of LLM\ncomponents a natural and increasingly common practice. However, despite\npromising early attempts, a systematic understanding of which LLM modules are\ntruly transferable remains lacking. In this paper, we present a comprehensive\nevaluation of five key LLM modules--tokenizer design, positional embedding,\npre-training paradigms, post-training strategies, and test-time\ncomputation--within the context of motion generation for autonomous driving.\nThrough extensive experiments on the Waymo Sim Agents benchmark, we demonstrate\nthat, when appropriately adapted, these modules can significantly improve\nperformance for autonomous driving motion generation. In addition, we identify\nwhich techniques can be effectively transferred, analyze the potential reasons\nfor the failure of others, and discuss the specific adaptations needed for\nautonomous driving scenarios. We evaluate our method on the Sim Agents task and\nachieve competitive results.", "AI": {"tldr": "\u672c\u6587\u7cfb\u7edf\u8bc4\u4f30\u4e865\u4e2a\u5173\u952eLLM\u6a21\u5757\u5728\u81ea\u52a8\u9a7e\u9a76\u8fd0\u52a8\u751f\u6210\u4e2d\u7684\u53ef\u8fc1\u79fb\u6027\uff0c\u5305\u62ec\u5206\u8bcd\u5668\u8bbe\u8ba1\u3001\u4f4d\u7f6e\u7f16\u7801\u3001\u9884\u8bad\u7ec3\u8303\u5f0f\u3001\u540e\u8bad\u7ec3\u7b56\u7565\u548c\u6d4b\u8bd5\u65f6\u8ba1\u7b97\uff0c\u5728Waymo\u57fa\u51c6\u4e0a\u53d6\u5f97\u4e86\u6709\u7ade\u4e89\u529b\u7684\u7ed3\u679c\u3002", "motivation": "\u867d\u7136LLM\u5728\u81ea\u7136\u8bed\u8a00\u5904\u7406\u9886\u57df\u53d6\u5f97\u7a81\u7834\uff0c\u5176\u7ec4\u4ef6\u5728\u7ed3\u6784\u76f8\u4f3c\u7684\u81ea\u52a8\u9a7e\u9a76\u8fd0\u52a8\u751f\u6210\u4efb\u52a1\u4e2d\u4e5f\u6709\u5e94\u7528\u6f5c\u529b\uff0c\u4f46\u7f3a\u4e4f\u5bf9\u54ea\u4e9bLLM\u6a21\u5757\u771f\u6b63\u53ef\u8fc1\u79fb\u7684\u7cfb\u7edf\u6027\u7406\u89e3\u3002", "method": "\u901a\u8fc7Waymo Sim Agents\u57fa\u51c6\u8fdb\u884c\u5e7f\u6cdb\u5b9e\u9a8c\uff0c\u8bc4\u4f30\u4e94\u4e2a\u5173\u952eLLM\u6a21\u5757\uff08\u5206\u8bcd\u5668\u8bbe\u8ba1\u3001\u4f4d\u7f6e\u7f16\u7801\u3001\u9884\u8bad\u7ec3\u8303\u5f0f\u3001\u540e\u8bad\u7ec3\u7b56\u7565\u3001\u6d4b\u8bd5\u65f6\u8ba1\u7b97\uff09\u5728\u81ea\u52a8\u9a7e\u9a76\u8fd0\u52a8\u751f\u6210\u4e2d\u7684\u9002\u5e94\u6027\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u7ecf\u8fc7\u9002\u5f53\u9002\u914d\u540e\uff0c\u8fd9\u4e9bLLM\u6a21\u5757\u80fd\u663e\u8457\u63d0\u5347\u81ea\u52a8\u9a7e\u9a76\u8fd0\u52a8\u751f\u6210\u7684\u6027\u80fd\uff0c\u5e76\u5728Sim Agents\u4efb\u52a1\u4e0a\u53d6\u5f97\u4e86\u6709\u7ade\u4e89\u529b\u7684\u7ed3\u679c\u3002", "conclusion": "\u7814\u7a76\u786e\u5b9a\u4e86\u53ef\u6709\u6548\u8fc1\u79fb\u7684\u6280\u672f\uff0c\u5206\u6790\u4e86\u5176\u4ed6\u6280\u672f\u5931\u8d25\u7684\u539f\u56e0\uff0c\u5e76\u8ba8\u8bba\u4e86\u81ea\u52a8\u9a7e\u9a76\u573a\u666f\u6240\u9700\u7684\u5177\u4f53\u9002\u914d\u65b9\u6cd5\uff0c\u4e3aLLM\u7ec4\u4ef6\u5728\u76f8\u5173\u9886\u57df\u7684\u5e94\u7528\u63d0\u4f9b\u4e86\u7cfb\u7edf\u6307\u5bfc\u3002"}}
{"id": "2509.02969", "pdf": "https://arxiv.org/pdf/2509.02969", "abs": "https://arxiv.org/abs/2509.02969", "authors": ["Dasong Li", "Sizhuo Ma", "Hang Hua", "Wenjie Li", "Jian Wang", "Chris Wei Zhou", "Fengbin Guan", "Xin Li", "Zihao Yu", "Yiting Lu", "Ru-Ling Liao", "Yan Ye", "Zhibo Chen", "Wei Sun", "Linhan Cao", "Yuqin Cao", "Weixia Zhang", "Wen Wen", "Kaiwei Zhang", "Zijian Chen", "Fangfang Lu", "Xiongkuo Min", "Guangtao Zhai", "Erjia Xiao", "Lingfeng Zhang", "Zhenjie Su", "Hao Cheng", "Yu Liu", "Renjing Xu", "Long Chen", "Xiaoshuai Hao", "Zhenpeng Zeng", "Jianqin Wu", "Xuxu Wang", "Qian Yu", "Bo Hu", "Weiwei Wang", "Pinxin Liu", "Yunlong Tang", "Luchuan Song", "Jinxi He", "Jiaru Wu", "Hanjia Lyu"], "title": "VQualA 2025 Challenge on Engagement Prediction for Short Videos: Methods and Results", "categories": ["cs.CV", "cs.MM", "cs.SI"], "comment": "ICCV 2025 VQualA workshop EVQA track", "summary": "This paper presents an overview of the VQualA 2025 Challenge on Engagement\nPrediction for Short Videos, held in conjunction with ICCV 2025. The challenge\nfocuses on understanding and modeling the popularity of user-generated content\n(UGC) short videos on social media platforms. To support this goal, the\nchallenge uses a new short-form UGC dataset featuring engagement metrics\nderived from real-world user interactions. This objective of the Challenge is\nto promote robust modeling strategies that capture the complex factors\ninfluencing user engagement. Participants explored a variety of multi-modal\nfeatures, including visual content, audio, and metadata provided by creators.\nThe challenge attracted 97 participants and received 15 valid test submissions,\ncontributing significantly to progress in short-form UGC video engagement\nprediction.", "AI": {"tldr": "VQualA 2025\u6311\u6218\u8d5b\u4e13\u6ce8\u4e8e\u77edUGC\u89c6\u9891\u7684\u53c2\u4e0e\u5ea6\u9884\u6d4b\uff0c\u4f7f\u7528\u771f\u5b9e\u7528\u6237\u4ea4\u4e92\u6570\u636e\uff0c\u5438\u5f15\u4e8697\u540d\u53c2\u4e0e\u8005\u63d0\u4ea415\u4efd\u6709\u6548\u6d4b\u8bd5\u65b9\u6848\uff0c\u63a8\u52a8\u4e86\u591a\u6a21\u6001\u7279\u5f81\u5efa\u6a21\u7684\u53d1\u5c55\u3002", "motivation": "\u7406\u89e3\u548c\u5efa\u6a21\u793e\u4ea4\u5a92\u4f53\u5e73\u53f0\u4e0a\u7528\u6237\u751f\u6210\u77ed\u89c6\u9891\u7684\u53d7\u6b22\u8fce\u7a0b\u5ea6\uff0c\u4fc3\u8fdb\u80fd\u591f\u6355\u6349\u5f71\u54cd\u7528\u6237\u53c2\u4e0e\u5ea6\u590d\u6742\u56e0\u7d20\u7684\u7a33\u5065\u5efa\u6a21\u7b56\u7565\u3002", "method": "\u4f7f\u7528\u5305\u542b\u771f\u5b9e\u7528\u6237\u4ea4\u4e92\u53c2\u4e0e\u5ea6\u6307\u6807\u7684\u65b0\u77ed\u683c\u5f0fUGC\u6570\u636e\u96c6\uff0c\u53c2\u4e0e\u8005\u63a2\u7d22\u4e86\u5305\u62ec\u89c6\u89c9\u5185\u5bb9\u3001\u97f3\u9891\u548c\u521b\u4f5c\u8005\u63d0\u4f9b\u7684\u5143\u6570\u636e\u5728\u5185\u7684\u591a\u6a21\u6001\u7279\u5f81\u3002", "result": "\u6311\u6218\u8d5b\u5438\u5f15\u4e8697\u540d\u53c2\u4e0e\u8005\uff0c\u6536\u5230\u4e8615\u4efd\u6709\u6548\u7684\u6d4b\u8bd5\u63d0\u4ea4\uff0c\u5728\u77ed\u683c\u5f0fUGC\u89c6\u9891\u53c2\u4e0e\u5ea6\u9884\u6d4b\u65b9\u9762\u53d6\u5f97\u4e86\u663e\u8457\u8fdb\u5c55\u3002", "conclusion": "\u8be5\u6311\u6218\u8d5b\u6210\u529f\u63a8\u52a8\u4e86\u77ed\u89c6\u9891\u53c2\u4e0e\u5ea6\u9884\u6d4b\u9886\u57df\u7684\u53d1\u5c55\uff0c\u4e3a\u7406\u89e3\u7528\u6237\u751f\u6210\u5185\u5bb9\u7684\u591a\u6a21\u6001\u7279\u5f81\u5efa\u6a21\u63d0\u4f9b\u4e86\u91cd\u8981\u8d21\u732e\u3002"}}
{"id": "2509.03303", "pdf": "https://arxiv.org/pdf/2509.03303", "abs": "https://arxiv.org/abs/2509.03303", "authors": ["Arnau Quera-Bofarull", "Nicholas Bishop", "Joel Dyer", "Daniel Jarne Ornia", "Anisoara Calinescu", "Doyne Farmer", "Michael Wooldridge"], "title": "Automatic Differentiation of Agent-Based Models", "categories": ["cs.MA", "cs.AI", "cs.CE", "cs.LG"], "comment": null, "summary": "Agent-based models (ABMs) simulate complex systems by capturing the bottom-up\ninteractions of individual agents comprising the system. Many complex systems\nof interest, such as epidemics or financial markets, involve thousands or even\nmillions of agents. Consequently, ABMs often become computationally demanding\nand rely on the calibration of numerous free parameters, which has\nsignificantly hindered their widespread adoption. In this paper, we demonstrate\nthat automatic differentiation (AD) techniques can effectively alleviate these\ncomputational burdens. By applying AD to ABMs, the gradients of the simulator\nbecome readily available, greatly facilitating essential tasks such as\ncalibration and sensitivity analysis. Specifically, we show how AD enables\nvariational inference (VI) techniques for efficient parameter calibration. Our\nexperiments demonstrate substantial performance improvements and computational\nsavings using VI on three prominent ABMs: Axtell's model of firms; Sugarscape;\nand the SIR epidemiological model. Our approach thus significantly enhances the\npracticality and scalability of ABMs for studying complex systems.", "AI": {"tldr": "\u672c\u6587\u5c55\u793a\u4e86\u81ea\u52a8\u5fae\u5206\u6280\u672f\u5982\u4f55\u6709\u6548\u51cf\u8f7b\u57fa\u4e8e\u4ee3\u7406\u6a21\u578b\u7684\u8ba1\u7b97\u8d1f\u62c5\uff0c\u901a\u8fc7\u53d8\u5206\u63a8\u7406\u5b9e\u73b0\u9ad8\u6548\u53c2\u6570\u6821\u51c6\uff0c\u663e\u8457\u63d0\u5347\u4e86ABMs\u7684\u5b9e\u7528\u6027\u548c\u53ef\u6269\u5c55\u6027", "motivation": "\u57fa\u4e8e\u4ee3\u7406\u6a21\u578b\u5728\u6a21\u62df\u590d\u6742\u7cfb\u7edf\u65f6\u9762\u4e34\u8ba1\u7b97\u91cf\u5927\u3001\u53c2\u6570\u6821\u51c6\u56f0\u96be\u7684\u95ee\u9898\uff0c\u8fd9\u9650\u5236\u4e86\u5176\u5e7f\u6cdb\u5e94\u7528", "method": "\u5c06\u81ea\u52a8\u5fae\u5206\u6280\u672f\u5e94\u7528\u4e8e\u57fa\u4e8e\u4ee3\u7406\u6a21\u578b\uff0c\u4f7f\u6a21\u62df\u5668\u7684\u68af\u5ea6\u6613\u4e8e\u83b7\u53d6\uff0c\u5e76\u91c7\u7528\u53d8\u5206\u63a8\u7406\u6280\u672f\u8fdb\u884c\u53c2\u6570\u6821\u51c6", "result": "\u5728\u4e09\u4e2a\u8457\u540dABMs\u4e0a\u7684\u5b9e\u9a8c\u663e\u793a\uff0c\u4f7f\u7528\u53d8\u5206\u63a8\u7406\u83b7\u5f97\u4e86\u663e\u8457\u7684\u6027\u80fd\u63d0\u5347\u548c\u8ba1\u7b97\u8282\u7701", "conclusion": "\u8be5\u65b9\u6cd5\u663e\u8457\u589e\u5f3a\u4e86\u57fa\u4e8e\u4ee3\u7406\u6a21\u578b\u5728\u7814\u7a76\u590d\u6742\u7cfb\u7edf\u65f6\u7684\u5b9e\u7528\u6027\u548c\u53ef\u6269\u5c55\u6027"}}
{"id": "2509.02859", "pdf": "https://arxiv.org/pdf/2509.02859", "abs": "https://arxiv.org/abs/2509.02859", "authors": ["Sandipana Dowerah", "Atharva Kulkarni", "Ajinkya Kulkarni", "Hoan My Tran", "Joonas Kalda", "Artem Fedorchenko", "Benoit Fauve", "Damien Lolive", "Tanel Alum\u00e4e", "Matthew Magimai Doss"], "title": "Speech DF Arena: A Leaderboard for Speech DeepFake Detection Models", "categories": ["cs.SD", "cs.CL", "eess.AS"], "comment": null, "summary": "Parallel to the development of advanced deepfake audio generation, audio\ndeepfake detection has also seen significant progress. However, a standardized\nand comprehensive benchmark is still missing. To address this, we introduce\nSpeech DeepFake (DF) Arena, the first comprehensive benchmark for audio\ndeepfake detection. Speech DF Arena provides a toolkit to uniformly evaluate\ndetection systems, currently across 14 diverse datasets and attack scenarios,\nstandardized evaluation metrics and protocols for reproducibility and\ntransparency. It also includes a leaderboard to compare and rank the systems to\nhelp researchers and developers enhance their reliability and robustness. We\ninclude 14 evaluation sets, 12 state-of-the-art open-source and 3 proprietary\ndetection systems. Our study presents many systems exhibiting high EER in\nout-of-domain scenarios, highlighting the need for extensive cross-domain\nevaluation. The leaderboard is hosted on Huggingface1 and a toolkit for\nreproducing results across the listed datasets is available on GitHub.", "AI": {"tldr": "Speech DF Arena\u662f\u9996\u4e2a\u5168\u9762\u7684\u97f3\u9891\u6df1\u5ea6\u4f2a\u9020\u68c0\u6d4b\u57fa\u51c6\uff0c\u63d0\u4f9b\u6807\u51c6\u5316\u8bc4\u4f30\u5de5\u5177\u5305\u300114\u4e2a\u6570\u636e\u96c6\u3001\u6807\u51c6\u5316\u6307\u6807\u548c\u534f\u8bae\uff0c\u4ee5\u53ca\u7cfb\u7edf\u6392\u540d\u699c\u5355\uff0c\u65e8\u5728\u63d0\u9ad8\u68c0\u6d4b\u7cfb\u7edf\u7684\u53ef\u9760\u6027\u548c\u9c81\u68d2\u6027\u3002", "motivation": "\u968f\u7740\u6df1\u5ea6\u4f2a\u9020\u97f3\u9891\u751f\u6210\u6280\u672f\u7684\u53d1\u5c55\uff0c\u97f3\u9891\u6df1\u5ea6\u4f2a\u9020\u68c0\u6d4b\u4e5f\u53d6\u5f97\u4e86\u663e\u8457\u8fdb\u5c55\uff0c\u4f46\u7f3a\u4e4f\u6807\u51c6\u5316\u548c\u5168\u9762\u7684\u57fa\u51c6\u6d4b\u8bd5\u3002", "method": "\u5f15\u5165Speech DF Arena\u57fa\u51c6\uff0c\u63d0\u4f9b\u7edf\u4e00\u8bc4\u4f30\u5de5\u5177\u5305\uff0c\u6db5\u76d614\u4e2a\u4e0d\u540c\u6570\u636e\u96c6\u548c\u653b\u51fb\u573a\u666f\uff0c\u91c7\u7528\u6807\u51c6\u5316\u8bc4\u4f30\u6307\u6807\u548c\u534f\u8bae\uff0c\u5305\u542b12\u4e2a\u5f00\u6e90\u548c3\u4e2a\u4e13\u6709\u68c0\u6d4b\u7cfb\u7edf\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u8bb8\u591a\u7cfb\u7edf\u5728\u8de8\u57df\u573a\u666f\u4e2d\u8868\u73b0\u51fa\u9ad8EER\uff08\u7b49\u9519\u8bef\u7387\uff09\uff0c\u51f8\u663e\u4e86\u5e7f\u6cdb\u8de8\u57df\u8bc4\u4f30\u7684\u5fc5\u8981\u6027\u3002", "conclusion": "Speech DF Arena\u4e3a\u97f3\u9891\u6df1\u5ea6\u4f2a\u9020\u68c0\u6d4b\u63d0\u4f9b\u4e86\u9996\u4e2a\u5168\u9762\u57fa\u51c6\uff0c\u6709\u52a9\u4e8e\u7814\u7a76\u4eba\u5458\u548c\u5f00\u53d1\u8005\u63d0\u5347\u68c0\u6d4b\u7cfb\u7edf\u7684\u53ef\u9760\u6027\u548c\u9c81\u68d2\u6027\uff0c\u5de5\u5177\u5305\u5df2\u5728Huggingface\u548cGitHub\u4e0a\u63d0\u4f9b\u3002"}}
{"id": "2509.02830", "pdf": "https://arxiv.org/pdf/2509.02830", "abs": "https://arxiv.org/abs/2509.02830", "authors": ["Pu Wang", "Shinji Watanabe", "Hugo Van hamme"], "title": "SSVD: Structured SVD for Parameter-Efficient Fine-Tuning and Benchmarking under Domain Shift in ASR", "categories": ["cs.CL", "eess.AS"], "comment": "Accepted by IEEE ASRU 2025", "summary": "Parameter-efficient fine-tuning (PEFT) has emerged as a scalable solution for\nadapting large foundation models. While low-rank adaptation (LoRA) is widely\nused in speech applications, its state-of-the-art variants, e.g., VeRA, DoRA,\nPiSSA, and SVFT, are developed mainly for language and vision tasks, with\nlimited validation in speech. This work presents the first comprehensive\nintegration and benchmarking of these PEFT methods within ESPnet. We further\nintroduce structured SVD-guided (SSVD) fine-tuning, which selectively rotates\ninput-associated right singular vectors while keeping output-associated vectors\nfixed to preserve semantic mappings. This design enables robust domain\nadaptation with minimal trainable parameters and improved efficiency. We\nevaluate all methods on domain-shifted speech recognition tasks, including\nchild speech and dialectal variation, across model scales from 0.1B to 2B. All\nimplementations are released in ESPnet to support reproducibility and future\nwork.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u7efc\u8ff0\u4e86\u53c2\u6570\u9ad8\u6548\u5fae\u8c03\u65b9\u6cd5\u5728\u8bed\u97f3\u8bc6\u522b\u4e2d\u7684\u5e94\u7528\uff0c\u63d0\u51fa\u4e86\u7ed3\u6784\u5316SVD\u5bfc\u5411\u5fae\u8c03\u65b9\u6cd5\uff0c\u5e76\u5728ESPnet\u4e2d\u5b9e\u73b0\u4e86\u591a\u79cdPEFT\u65b9\u6cd5\u7684\u7efc\u5408\u6027\u80fd\u6d4b\u8bc4\u3002", "motivation": "\u867d\u7136LoRA\u5728\u8bed\u97f3\u5e94\u7528\u4e2d\u5e7f\u6cdb\u4f7f\u7528\uff0c\u4f46\u6700\u65b0\u7684PEFT\u53d8\u4f53\u5982VeRA\u3001DoRA\u3001PiSSA\u548cSVFT\u4e3b\u8981\u4e3a\u8bed\u8a00\u548c\u89c6\u89c9\u4efb\u52a1\u800c\u5f00\u53d1\uff0c\u5728\u8bed\u97f3\u9886\u57df\u7f3a\u4e4f\u5145\u5206\u9a8c\u8bc1\u3002\u9700\u8981\u5728\u8bed\u97f3\u8bc6\u522b\u4efb\u52a1\u4e2d\u7cfb\u7edf\u6027\u5730\u96c6\u6210\u548c\u6d4b\u8bc4\u8fd9\u4e9b\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u4e86\u7ed3\u6784\u5316SVD\u5bfc\u5411(SSVD)\u5fae\u8c03\u65b9\u6cd5\uff0c\u901a\u8fc7\u9009\u62e9\u6027\u65cb\u8f6c\u8f93\u5165\u76f8\u5173\u7684\u53f3\u5947\u5f02\u5411\u91cf\u800c\u4fdd\u6301\u8f93\u51fa\u76f8\u5173\u5411\u91cf\u56fa\u5b9a\uff0c\u4ee5\u4fdd\u7559\u8bed\u4e49\u6620\u5c04\u3002\u5728ESPnet\u4e2d\u96c6\u6210\u4e86\u591a\u79cdPEFT\u65b9\u6cd5\u5e76\u8fdb\u884c\u7efc\u5408\u6027\u80fd\u6d4b\u8bc4\u3002", "result": "\u5728\u57df\u5047\u79fb\u8bed\u97f3\u8bc6\u522b\u4efb\u52a1\u4e0a\u8fdb\u884c\u4e86\u8bc4\u4f30\uff0c\u5305\u62ec\u513f\u7ae5\u8bed\u97f3\u548c\u65b9\u8a00\u53d8\u4f53\uff0c\u6a21\u578b\u89c4\u6a21\u4ece0.1B\u52302B\u3002SSVD\u65b9\u6cd5\u80fd\u591f\u4ee5\u6700\u5c11\u7684\u53ef\u8bad\u7ec3\u53c2\u6570\u5b9e\u73b0\u7a33\u5065\u7684\u57df\u9002\u5e94\uff0c\u5e76\u63d0\u9ad8\u6548\u7387\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u8bed\u97f3\u8bc6\u522b\u9886\u57df\u63d0\u4f9b\u4e86\u5168\u9762\u7684PEFT\u65b9\u6cd5\u96c6\u6210\u548c\u6027\u80fd\u5206\u6790\uff0cSSVD\u65b9\u6cd5\u663e\u793a\u4e86\u5728\u57df\u9002\u5e94\u4e2d\u7684\u4f18\u52bf\u3002\u6240\u6709\u5b9e\u73b0\u5df2\u5728ESPnet\u4e2d\u5f00\u6e90\u53d1\u5e03\uff0c\u652f\u6301\u53ef\u590d\u73b0\u6027\u548c\u672a\u6765\u7814\u7a76\u3002"}}
{"id": "2509.02749", "pdf": "https://arxiv.org/pdf/2509.02749", "abs": "https://arxiv.org/abs/2509.02749", "authors": ["Giorgia Buracchio", "Ariele Callegari", "Massimo Donini", "Cristina Gena", "Antonio Lieto", "Alberto Lillo", "Claudio Mattutino", "Alessandro Mazzei", "Linda Pigureddu", "Manuel Striani", "Fabiana Vernero"], "title": "The Impact of Adaptive Emotional Alignment on Mental State Attribution and User Empathy in HRI", "categories": ["cs.RO"], "comment": "autohor copy of the paper accepted at ROMAN2025", "summary": "The paper presents an experiment on the effects of adaptive emotional\nalignment between agents, considered a prerequisite for empathic communication,\nin Human-Robot Interaction (HRI). Using the NAO robot, we investigate the\nimpact of an emotionally aligned, empathic, dialogue on these aspects: (i) the\nrobot's persuasive effectiveness, (ii) the user's communication style, and\n(iii) the attribution of mental states and empathy to the robot. In an\nexperiment with 42 participants, two conditions were compared: one with neutral\ncommunication and another where the robot provided responses adapted to the\nemotions expressed by the users. The results show that emotional alignment does\nnot influence users' communication styles or have a persuasive effect. However,\nit significantly influences attribution of mental states to the robot and its\nperceived empathy", "AI": {"tldr": "\u60c5\u611f\u5bf9\u9f50\u5728HRI\u4e2d\u5bf9\u673a\u5668\u4eba\u8bf4\u670d\u529b\u548c\u7528\u6237\u6c9f\u901a\u98ce\u683c\u65e0\u663e\u8457\u5f71\u54cd\uff0c\u4f46\u663e\u8457\u63d0\u5347\u7528\u6237\u5bf9\u673a\u5668\u4eba\u5fc3\u7406\u72b6\u6001\u5f52\u56e0\u548c\u5171\u60c5\u611f\u77e5", "motivation": "\u7814\u7a76\u81ea\u9002\u5e94\u60c5\u611f\u5bf9\u9f50\u4f5c\u4e3a\u5171\u60c5\u6c9f\u901a\u524d\u63d0\u5728\u4eba\u7c7b-\u673a\u5668\u4eba\u4ea4\u4e92\u4e2d\u7684\u4f5c\u7528\uff0c\u63a2\u7d22\u60c5\u611f\u5bf9\u9f50\u5bf9\u8bdd\u5bf9\u673a\u5668\u4eba\u8bf4\u670d\u529b\u3001\u7528\u6237\u6c9f\u901a\u98ce\u683c\u548c\u5fc3\u7406\u72b6\u6001\u5f52\u56e0\u7684\u5f71\u54cd", "method": "\u4f7f\u7528NAO\u673a\u5668\u4eba\u8fdb\u884c\u5b9e\u9a8c\uff0c42\u540d\u53c2\u4e0e\u8005\u5206\u4e3a\u4e24\u7ec4\uff1a\u4e2d\u6027\u6c9f\u901a\u7ec4\u548c\u60c5\u611f\u81ea\u9002\u5e94\u54cd\u5e94\u7ec4\uff0c\u6bd4\u8f83\u4e24\u79cd\u6761\u4ef6\u4e0b\u7684\u6548\u679c\u5dee\u5f02", "result": "\u60c5\u611f\u5bf9\u9f50\u4e0d\u5f71\u54cd\u7528\u6237\u6c9f\u901a\u98ce\u683c\u548c\u673a\u5668\u4eba\u8bf4\u670d\u6548\u679c\uff0c\u4f46\u663e\u8457\u5f71\u54cd\u7528\u6237\u5bf9\u673a\u5668\u4eba\u5fc3\u7406\u72b6\u6001\u7684\u5f52\u56e0\u548c\u611f\u77e5\u5230\u7684\u5171\u60c5\u80fd\u529b", "conclusion": "\u60c5\u611f\u5bf9\u9f50\u867d\u7136\u4e0d\u80fd\u63d0\u5347\u8bf4\u670d\u6548\u679c\u6216\u6539\u53d8\u6c9f\u901a\u98ce\u683c\uff0c\u4f46\u80fd\u6709\u6548\u589e\u5f3a\u7528\u6237\u5bf9\u673a\u5668\u4eba\u5fc3\u7406\u80fd\u529b\u548c\u5171\u60c5\u80fd\u529b\u7684\u611f\u77e5"}}
{"id": "2509.02807", "pdf": "https://arxiv.org/pdf/2509.02807", "abs": "https://arxiv.org/abs/2509.02807", "authors": ["Mennatullah Siam"], "title": "PixFoundation 2.0: Do Video Multi-Modal LLMs Use Motion in Visual Grounding?", "categories": ["cs.CV"], "comment": "Work under review in NeurIPS 2025 with the title \"Are we using Motion\n  in Referring Segmentation? A Motion-Centric Evaluation\"", "summary": "Multi-modal large language models (MLLMs) have shown impressive\ngeneralization across tasks using images and text modalities. While their\nextension to video has enabled tasks such as video question answering and video\ncaptioning, their pixel-level visual grounding abilities are less studied. In\nthis work, we raise the pertinent question of whether motion is used in\npixel-level visual grounding and whether video MLLMs can segment objects based\non natural language expressions describing their motion patterns. We identify\nthe shortcomings in the current benchmarks, where we show that a single frame\ncan often suffice for capturing the motion referring expression without any\ntemporal reasoning. To address this, we introduce four motion-centric probing\ntechniques, particularly designed for the visual grounding task, to study video\nMLLMs' ability to identify true motion from a fake one and their ability to\ngrasp the motion order. Consequently, we provide a motion-centric benchmark,\nMoCentric-Bench. It ensures that video MLLMs are evaluated towards leveraging\nthe interaction between motion and language rather than being dominated by\nstatic appearance cues emphasized in existing visual grounding datasets. We\nfurther establish strong single-image baselines that are on par with or\noutperform prior methods. Finally, we explore simple motion-centric adaptation\ntechniques that provide state-of-the-art performance on our MoCentric-Bench.\nOur motion-centric benchmark, evaluation and findings challenge future models\nto improve dense spatiotemporal grounding and pixel-level understanding within\nvideos. Code and datasets will be made publicly available at\nhttps://github.com/MSiam/PixFoundation-2.0.git.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86MoCentric-Bench\u57fa\u51c6\u6d4b\u8bd5\uff0c\u7528\u4e8e\u8bc4\u4f30\u89c6\u9891\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u50cf\u7d20\u7ea7\u89c6\u89c9\u5b9a\u4f4d\u4e2d\u5bf9\u8fd0\u52a8\u4fe1\u606f\u7684\u7406\u89e3\u80fd\u529b\uff0c\u53d1\u73b0\u73b0\u6709\u6a21\u578b\u8fc7\u5ea6\u4f9d\u8d56\u9759\u6001\u5916\u89c2\u7ebf\u7d22\u800c\u975e\u771f\u6b63\u7684\u65f6\u5e8f\u63a8\u7406\u3002", "motivation": "\u5f53\u524d\u89c6\u9891MLLMs\u5728\u50cf\u7d20\u7ea7\u89c6\u89c9\u5b9a\u4f4d\u65b9\u9762\u7684\u80fd\u529b\u7814\u7a76\u4e0d\u8db3\uff0c\u73b0\u6709\u57fa\u51c6\u6d4b\u8bd5\u5b58\u5728\u7f3a\u9677\uff0c\u5355\u5e27\u56fe\u50cf\u5f80\u5f80\u5c31\u80fd\u6ee1\u8db3\u8fd0\u52a8\u6307\u4ee3\u8868\u8fbe\u5f0f\u7684\u9700\u6c42\uff0c\u65e0\u6cd5\u771f\u6b63\u6d4b\u8bd5\u6a21\u578b\u5bf9\u8fd0\u52a8\u4fe1\u606f\u7684\u7406\u89e3\u3002", "method": "\u63d0\u51fa\u4e86\u56db\u79cd\u8fd0\u52a8\u4e2d\u5fc3\u63a2\u6d4b\u6280\u672f\uff0c\u6784\u5efa\u4e86MoCentric-Bench\u57fa\u51c6\u6d4b\u8bd5\uff0c\u786e\u4fdd\u8bc4\u4f30\u6a21\u578b\u5bf9\u8fd0\u52a8\u4e0e\u8bed\u8a00\u4ea4\u4e92\u7684\u7406\u89e3\uff0c\u800c\u975e\u4ec5\u4f9d\u8d56\u9759\u6001\u5916\u89c2\u7ebf\u7d22\u3002\u8fd8\u5efa\u7acb\u4e86\u5f3a\u5355\u5e27\u57fa\u7ebf\u5e76\u63a2\u7d22\u4e86\u8fd0\u52a8\u4e2d\u5fc3\u9002\u5e94\u6280\u672f\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u73b0\u6709\u89c6\u9891MLLMs\u5728\u8fd0\u52a8\u7406\u89e3\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\uff0c\u5f3a\u5355\u5e27\u57fa\u7ebf\u65b9\u6cd5\u5728\u6027\u80fd\u4e0a\u4e0e\u5148\u524d\u65b9\u6cd5\u76f8\u5f53\u6216\u66f4\u4f18\uff0c\u63d0\u51fa\u7684\u8fd0\u52a8\u4e2d\u5fc3\u9002\u5e94\u6280\u672f\u5728\u65b0\u57fa\u51c6\u4e0a\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u6027\u80fd\u3002", "conclusion": "\u8be5\u7814\u7a76\u6311\u6218\u672a\u6765\u6a21\u578b\u6539\u8fdb\u5bc6\u96c6\u65f6\u7a7a\u5b9a\u4f4d\u548c\u50cf\u7d20\u7ea7\u89c6\u9891\u7406\u89e3\u80fd\u529b\uff0c\u5f3a\u8c03\u4e86\u771f\u6b63\u8fd0\u52a8\u7406\u89e3\u5728\u89c6\u9891MLLMs\u4e2d\u7684\u91cd\u8981\u6027\u3002"}}
{"id": "2509.02579", "pdf": "https://arxiv.org/pdf/2509.02579", "abs": "https://arxiv.org/abs/2509.02579", "authors": ["Mazyar Taghavi", "Rahman Farnoosh"], "title": "Latent Variable Modeling in Multi-Agent Reinforcement Learning via Expectation-Maximization for UAV-Based Wildlife Protection", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Protecting endangered wildlife from illegal poaching presents a critical\nchallenge, particularly in vast and partially observable environments where\nreal-time response is essential. This paper introduces a novel\nExpectation-Maximization (EM) based latent variable modeling approach in the\ncontext of Multi-Agent Reinforcement Learning (MARL) for Unmanned Aerial\nVehicle (UAV) coordination in wildlife protection. By modeling hidden\nenvironmental factors and inter-agent dynamics through latent variables, our\nmethod enhances exploration and coordination under uncertainty.We implement and\nevaluate our EM-MARL framework using a custom simulation involving 10 UAVs\ntasked with patrolling protected habitats of the endangered Iranian leopard.\nExtensive experimental results demonstrate superior performance in detection\naccuracy, adaptability, and policy convergence when compared to standard\nalgorithms such as Proximal Policy Optimization (PPO) and Deep Deterministic\nPolicy Gradient (DDPG). Our findings underscore the potential of combining EM\ninference with MARL to improve decentralized decisionmaking in complex,\nhigh-stakes conservation scenarios. The full implementation, simulation\nenvironment, and training scripts are publicly available on GitHub.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u671f\u671b\u6700\u5927\u5316(EM)\u7684\u9690\u53d8\u91cf\u5efa\u6a21\u65b9\u6cd5\uff0c\u7ed3\u5408\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60(MARL)\u5b9e\u73b0\u65e0\u4eba\u673a\u534f\u540c\u4fdd\u62a4\u6fd2\u5371\u91ce\u751f\u52a8\u7269\uff0c\u5728\u4f0a\u6717\u8c79\u4fdd\u62a4\u573a\u666f\u4e2d\u8868\u73b0\u4f18\u4e8ePPO\u548cDDPG\u7b97\u6cd5\u3002", "motivation": "\u89e3\u51b3\u5728\u5e7f\u9614\u4e14\u90e8\u5206\u53ef\u89c2\u6d4b\u73af\u5883\u4e2d\u5b9e\u65f6\u54cd\u5e94\u975e\u6cd5\u76d7\u730e\u7684\u6311\u6218\uff0c\u4fdd\u62a4\u6fd2\u5371\u91ce\u751f\u52a8\u7269\u9700\u8981\u9ad8\u6548\u7684\u65e0\u4eba\u673a\u534f\u540c\u5de1\u903b\u7b56\u7565\u3002", "method": "\u91c7\u7528\u671f\u671b\u6700\u5927\u5316(EM)\u7b97\u6cd5\u5efa\u7acb\u9690\u53d8\u91cf\u6a21\u578b\uff0c\u5904\u7406\u9690\u85cf\u73af\u5883\u56e0\u7d20\u548c\u667a\u80fd\u4f53\u95f4\u52a8\u6001\u5173\u7cfb\uff0c\u901a\u8fc7\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\u5b9e\u73b010\u67b6\u65e0\u4eba\u673a\u7684\u534f\u540c\u51b3\u7b56\u3002", "result": "\u5728\u81ea\u5b9a\u4e49\u4eff\u771f\u73af\u5883\u4e2d\uff0c\u76f8\u6bd4PPO\u548cDDPG\u7b49\u6807\u51c6\u7b97\u6cd5\uff0cEM-MARL\u6846\u67b6\u5728\u68c0\u6d4b\u7cbe\u5ea6\u3001\u9002\u5e94\u6027\u548c\u7b56\u7565\u6536\u655b\u6027\u65b9\u9762\u8868\u73b0\u51fa\u663e\u8457\u4f18\u52bf\u3002", "conclusion": "EM\u63a8\u7406\u4e0eMARL\u7684\u7ed3\u5408\u80fd\u591f\u6709\u6548\u63d0\u5347\u590d\u6742\u9ad8\u98ce\u9669\u4fdd\u62a4\u573a\u666f\u4e2d\u7684\u5206\u6563\u51b3\u7b56\u80fd\u529b\uff0c\u4e3a\u91ce\u751f\u52a8\u7269\u4fdd\u62a4\u63d0\u4f9b\u4e86\u6709\u524d\u666f\u7684\u6280\u672f\u65b9\u6848\u3002"}}
{"id": "2509.02761", "pdf": "https://arxiv.org/pdf/2509.02761", "abs": "https://arxiv.org/abs/2509.02761", "authors": ["Ananth Hariharan", "Vardhan Dongre", "Dilek Hakkani-T\u00fcr", "Gokhan Tur"], "title": "Plan Verification for LLM-Based Embodied Task Completion Agents", "categories": ["cs.AI"], "comment": null, "summary": "Large language model (LLM) based task plans and corresponding human\ndemonstrations for embodied AI may be noisy, with unnecessary actions,\nredundant navigation, and logical errors that reduce policy quality. We propose\nan iterative verification framework in which a Judge LLM critiques action\nsequences and a Planner LLM applies the revisions, yielding progressively\ncleaner and more spatially coherent trajectories. Unlike rule-based approaches,\nour method relies on natural language prompting, enabling broad generalization\nacross error types including irrelevant actions, contradictions, and missing\nsteps. On a set of manually annotated actions from the TEACh embodied AI\ndataset, our framework achieves up to 90% recall and 100% precision across four\nstate-of-the-art LLMs (GPT o4-mini, DeepSeek-R1, Gemini 2.5, LLaMA 4 Scout).\nThe refinement loop converges quickly, with 96.5% of sequences requiring at\nmost three iterations, while improving both temporal efficiency and spatial\naction organization. Crucially, the method preserves human error-recovery\npatterns rather than collapsing them, supporting future work on robust\ncorrective behavior. By establishing plan verification as a reliable LLM\ncapability for spatial planning and action refinement, we provide a scalable\npath to higher-quality training data for imitation learning in embodied AI.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8eLLM\u7684\u8fed\u4ee3\u9a8c\u8bc1\u6846\u67b6\uff0c\u901a\u8fc7Judge LLM\u6279\u8bc4\u548cPlanner LLM\u4fee\u8ba2\uff0c\u9010\u6b65\u4f18\u5316\u5177\u8eabAI\u4efb\u52a1\u8ba1\u5212\uff0c\u63d0\u9ad8\u8f68\u8ff9\u8d28\u91cf\u548c\u7a7a\u95f4\u4e00\u81f4\u6027", "motivation": "LLM\u751f\u6210\u7684\u4efb\u52a1\u8ba1\u5212\u548c\u4eba\u7c7b\u6f14\u793a\u53ef\u80fd\u5b58\u5728\u566a\u58f0\u52a8\u4f5c\u3001\u5197\u4f59\u5bfc\u822a\u548c\u903b\u8f91\u9519\u8bef\uff0c\u5f71\u54cd\u7b56\u7565\u8d28\u91cf\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u591f\u6cdb\u5316\u5904\u7406\u5404\u7c7b\u9519\u8bef\u7684\u9a8c\u8bc1\u65b9\u6cd5", "method": "\u4f7f\u7528\u81ea\u7136\u8bed\u8a00\u63d0\u793a\u7684\u8fed\u4ee3\u9a8c\u8bc1\u6846\u67b6\uff1aJudge LLM\u6279\u8bc4\u52a8\u4f5c\u5e8f\u5217\uff0cPlanner LLM\u5e94\u7528\u4fee\u8ba2\uff0c\u9010\u6b65\u6e05\u7406\u548c\u4f18\u5316\u8f68\u8ff9", "result": "\u5728TEACh\u6570\u636e\u96c6\u4e0a\u8fbe\u523090%\u53ec\u56de\u7387\u548c100%\u7cbe\u786e\u5ea6\uff0c96.5%\u7684\u5e8f\u5217\u6700\u591a\u9700\u89813\u6b21\u8fed\u4ee3\u6536\u655b\uff0c\u540c\u65f6\u6539\u5584\u65f6\u95f4\u6548\u7387\u548c\u7a7a\u95f4\u52a8\u4f5c\u7ec4\u7ec7", "conclusion": "\u8be5\u65b9\u6cd5\u5c06\u8ba1\u5212\u9a8c\u8bc1\u786e\u7acb\u4e3aLLM\u5728\u7a7a\u95f4\u89c4\u5212\u548c\u52a8\u4f5c\u4f18\u5316\u4e2d\u7684\u53ef\u9760\u80fd\u529b\uff0c\u4e3a\u5177\u8eabAI\u7684\u6a21\u4eff\u5b66\u4e60\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u7684\u9ad8\u8d28\u91cf\u8bad\u7ec3\u6570\u636e\u751f\u6210\u8def\u5f84"}}
{"id": "2509.03409", "pdf": "https://arxiv.org/pdf/2509.03409", "abs": "https://arxiv.org/abs/2509.03409", "authors": ["Hoan My Tran", "Damien Lolive", "Aghilas Sini", "Arnaud Delhay", "Pierre-Fran\u00e7ois Marteau", "David Guennec"], "title": "Multi-level SSL Feature Gating for Audio Deepfake Detection", "categories": ["cs.SD", "cs.AI", "cs.MM", "I.2.7"], "comment": "This paper has been accepted by ACM MM 2025", "summary": "Recent advancements in generative AI, particularly in speech synthesis, have\nenabled the generation of highly natural-sounding synthetic speech that closely\nmimics human voices. While these innovations hold promise for applications like\nassistive technologies, they also pose significant risks, including misuse for\nfraudulent activities, identity theft, and security threats. Current research\non spoofing detection countermeasures remains limited by generalization to\nunseen deepfake attacks and languages. To address this, we propose a gating\nmechanism extracting relevant feature from the speech foundation XLS-R model as\na front-end feature extractor. For downstream back-end classifier, we employ\nMulti-kernel gated Convolution (MultiConv) to capture both local and global\nspeech artifacts. Additionally, we introduce Centered Kernel Alignment (CKA) as\na similarity metric to enforce diversity in learned features across different\nMultiConv layers. By integrating CKA with our gating mechanism, we hypothesize\nthat each component helps improving the learning of distinct synthetic speech\npatterns. Experimental results demonstrate that our approach achieves\nstate-of-the-art performance on in-domain benchmarks while generalizing\nrobustly to out-of-domain datasets, including multilingual speech samples. This\nunderscores its potential as a versatile solution for detecting evolving speech\ndeepfake threats.", "AI": {"tldr": "\u901a\u8fc7\u7ed3\u5408XLS-R\u57fa\u7840\u6a21\u578b\u7684\u95e8\u63a7\u673a\u5236\u548c\u591a\u5185\u6838\u5377\u79ef\u5206\u6790\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u80fd\u591f\u68c0\u6d4b\u591a\u8bed\u8a00\u6df1\u5ea6\u4f2a\u9020\u8bed\u97f3\u7684\u65b0\u65b9\u6cd5\uff0c\u5728\u57df\u5185\u5488\u57df\u5916\u90fd\u53d6\u5f97\u4e86\u6700\u4f73\u6027\u80fd\u3002", "motivation": "\u968f\u7740\u8bed\u97f3\u5408\u6210\u6280\u672f\u7684\u53d1\u5c55\uff0c\u9ad8\u8d28\u91cf\u7684\u5408\u6210\u8bed\u97f3\u53ef\u80fd\u88ab\u6b3a\u8bc8\u6d3b\u52a8\u3001\u8eab\u4efd\u76d7\u7528\u7b49\u6076\u610f\u4f7f\u7528\uff0c\u5f53\u524d\u7684\u68c0\u6d4b\u65b9\u6cd5\u5728\u5e94\u5bf9\u672a\u89c1\u653b\u51fb\u5488\u591a\u8bed\u8a00\u573a\u666f\u65f6\u5b58\u5728\u4e0d\u8db3\u3002", "method": "\u4f7f\u7528XLS-R\u57fa\u7840\u6a21\u578b\u4f5c\u4e3a\u524d\u7aef\u7279\u5f81\u63d0\u53d6\u5668\uff0c\u901a\u8fc7\u95e8\u63a7\u673a\u5236\u63d0\u53d6\u5173\u952e\u7279\u5f81\uff1b\u91c7\u7528\u591a\u5185\u6838\u95e8\u63a7\u5377\u79ef\uff08MultiConv\uff09\u4f5c\u4e3a\u540e\u7aef\u5206\u7c7b\u5668\uff0c\u6355\u6349\u5c40\u90e8\u548c\u5168\u5c40\u8bed\u97f3\u4f2a\u9020\u7279\u5f81\uff1b\u5f15\u5165\u4e2d\u5fc3\u5185\u6838\u5bf9\u9f50\uff08CKA\uff09\u4f5c\u4e3a\u76f8\u4f3c\u6027\u6307\u6807\uff0c\u786e\u4fdd\u4e0d\u540c\u5c42\u5b66\u5230\u591a\u6837\u5316\u7684\u7279\u5f81\u3002", "result": "\u8be5\u65b9\u6cd5\u5728\u57df\u5185\u6d4b\u8bd5\u96c6\u4e0a\u8fbe\u5230\u4e86\u6700\u4f73\u6027\u80fd\uff0c\u540c\u65f6\u5728\u57df\u5916\u6570\u636e\u96c6\uff08\u5305\u62ec\u591a\u8bed\u8a00\u8bed\u97f3\u6837\u672c\uff09\u4e0a\u4e5f\u8868\u73b0\u51fa\u4e86\u5f3a\u5927\u7684\u6f14\u5316\u80fd\u529b\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u4f9b\u4e86\u4e00\u79cd\u80fd\u591f\u6709\u6548\u68c0\u6d4b\u4e0d\u65ad\u53d1\u5c55\u7684\u8bed\u97f3\u6df1\u5ea6\u4f2a\u9020\u5a01\u80c1\u7684\u591a\u7528\u9014\u89e3\u51b3\u65b9\u6848\uff0c\u5177\u6709\u826f\u597d\u7684\u901a\u7528\u6027\u548c\u6f14\u5316\u6027\u3002"}}
{"id": "2509.02834", "pdf": "https://arxiv.org/pdf/2509.02834", "abs": "https://arxiv.org/abs/2509.02834", "authors": ["Gustavo Bonil", "Jo\u00e3o Gondim", "Marina dos Santos", "Simone Hashiguti", "Helena Maia", "Nadia Silva", "Helio Pedrini", "Sandra Avila"], "title": "Clustering Discourses: Racial Biases in Short Stories about Women Generated by Large Language Models", "categories": ["cs.CL", "cs.AI"], "comment": "12 pages, 3 figures. Accepted at STIL @ BRACIS 2025", "summary": "This study investigates how large language models, in particular LLaMA\n3.2-3B, construct narratives about Black and white women in short stories\ngenerated in Portuguese. From 2100 texts, we applied computational methods to\ngroup semantically similar stories, allowing a selection for qualitative\nanalysis. Three main discursive representations emerge: social overcoming,\nancestral mythification and subjective self-realization. The analysis uncovers\nhow grammatically coherent, seemingly neutral texts materialize a crystallized,\ncolonially structured framing of the female body, reinforcing historical\ninequalities. The study proposes an integrated approach, that combines machine\nlearning techniques with qualitative, manual discourse analysis.", "AI": {"tldr": "\u672c\u7814\u7a76\u5206\u6790LLaMA 3.2-3B\u751f\u6210\u8461\u8404\u7259\u8bed\u77ed\u7bc7\u6545\u4e8b\u4e2d\u5bf9\u9ed1\u4eba\u548c\u767d\u4eba\u5973\u6027\u7684\u53d9\u4e8b\u5efa\u6784\uff0c\u53d1\u73b0\u5b58\u5728\u4e09\u79cd\u4e3b\u8981\u8bdd\u8bed\u8868\u5f81\uff0c\u63ed\u793a\u4e86\u770b\u4f3c\u4e2d\u7acb\u7684\u6587\u672c\u5b9e\u5219\u5f3a\u5316\u6b96\u6c11\u7ed3\u6784\u4e0b\u7684\u6027\u522b\u4e0d\u5e73\u7b49\u3002", "motivation": "\u63a2\u7a76\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u751f\u6210\u6587\u672c\u4e2d\u5982\u4f55\u518d\u73b0\u548c\u5f3a\u5316\u5173\u4e8e\u79cd\u65cf\u548c\u6027\u522b\u7684\u5386\u53f2\u4e0d\u5e73\u7b49\u7ed3\u6784\uff0c\u7279\u522b\u5173\u6ce8\u5bf9\u9ed1\u4eba\u548c\u767d\u4eba\u5973\u6027\u7684\u53d9\u4e8b\u5efa\u6784\u3002", "method": "\u4ece2100\u4e2a\u751f\u6210\u6587\u672c\u4e2d\u5e94\u7528\u8ba1\u7b97\u65b9\u6cd5\u8fdb\u884c\u8bed\u4e49\u805a\u7c7b\uff0c\u9009\u62e9\u4ee3\u8868\u6027\u6837\u672c\u8fdb\u884c\u8d28\u6027\u8bdd\u8bed\u5206\u6790\uff0c\u7ed3\u5408\u673a\u5668\u5b66\u4e60\u6280\u672f\u548c\u4eba\u5de5\u5206\u6790\u3002", "result": "\u8bc6\u522b\u51fa\u4e09\u79cd\u4e3b\u8981\u8bdd\u8bed\u8868\u5f81\uff1a\u793e\u4f1a\u8d85\u8d8a\u3001\u7956\u5148\u795e\u8bdd\u5316\u548c\u4e3b\u89c2\u81ea\u6211\u5b9e\u73b0\uff0c\u53d1\u73b0\u8bed\u6cd5\u8fde\u8d2f\u7684\u6587\u672c\u5b9e\u9645\u4e0a\u56fa\u5316\u6b96\u6c11\u7ed3\u6784\u4e0b\u7684\u5973\u6027\u8eab\u4f53\u6846\u67b6\u3002", "conclusion": "\u7814\u7a76\u63d0\u51fa\u4e86\u7ed3\u5408\u673a\u5668\u5b66\u4e60\u4e0e\u8d28\u6027\u8bdd\u8bed\u5206\u6790\u7684\u6574\u5408\u65b9\u6cd5\uff0c\u63ed\u793a\u4e86AI\u751f\u6210\u6587\u672c\u4e2d\u9690\u85cf\u7684\u6b96\u6c11\u6027\u522b\u4e0d\u5e73\u7b49\u7ed3\u6784\uff0c\u5f3a\u8c03\u9700\u8981\u6279\u5224\u6027\u5ba1\u89c6\u8bed\u8a00\u6a21\u578b\u7684\u8f93\u51fa\u3002"}}
{"id": "2509.02760", "pdf": "https://arxiv.org/pdf/2509.02760", "abs": "https://arxiv.org/abs/2509.02760", "authors": ["Maximilian Neidhardt", "Ludwig Bosse", "Vidas Raudonis", "Kristina Allgoewer", "Axel Heinemann", "Benjamin Ondruschka", "Alexander Schlaefer"], "title": "A Digital Twin for Robotic Post Mortem Tissue Sampling using Virtual Reality", "categories": ["cs.RO"], "comment": null, "summary": "Studying tissue samples obtained during autopsies is the gold standard when\ndiagnosing the cause of death and for understanding disease pathophysiology.\nRecently, the interest in post mortem minimally invasive biopsies has grown\nwhich is a less destructive approach in comparison to an open autopsy and\nreduces the risk of infection. While manual biopsies under ultrasound guidance\nare more widely performed, robotic post mortem biopsies have been recently\nproposed. This approach can further reduce the risk of infection for\nphysicians. However, planning of the procedure and control of the robot need to\nbe efficient and usable. We explore a virtual reality setup with a digital twin\nto realize fully remote planning and control of robotic post mortem biopsies.\nThe setup is evaluated with forensic pathologists in a usability study for\nthree interaction methods. Furthermore, we evaluate clinical feasibility and\nevaluate the system with three human cadavers. Overall, 132 needle insertions\nwere performed with an off-axis needle placement error of 5.30+-3.25 mm. Tissue\nsamples were successfully biopsied and histopathologically verified. Users\nreported a very intuitive needle placement approach, indicating that the system\nis a promising, precise, and low-risk alternative to conventional approaches.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u63a2\u7d22\u4e86\u4f7f\u7528\u865a\u62df\u73b0\u5b9e\u548c\u6570\u5b57\u53cc\u751f\u6280\u672f\u6765\u5b9e\u73b0\u5168\u7a0b\u8fdc\u7a0b\u89c4\u5212\u548c\u63a7\u5236\u6c7d\u8f66\u4eba\u5316\u67d0\u4e8b\u540e\u5c11\u4fb5\u5165\u6027\u68c0\u67e5\u7684\u65b0\u65b9\u6cd5\uff0c\u4ee5\u964d\u4f4e\u533b\u751f\u611f\u67d3\u98ce\u9669\u5e76\u63d0\u9ad8\u6548\u7387\u3002", "motivation": "\u4f20\u7edf\u7684\u5f00\u653e\u5f0f\u67d0\u4e8b\u540e\u68c0\u67e5\u5bf9\u533b\u751f\u6784\u6210\u611f\u67d3\u98ce\u9669\uff0c\u800c\u624b\u52a8\u8d85\u58f0\u5f15\u5bfc\u68c0\u67e5\u65b9\u6cd5\u5b58\u5728\u9650\u5236\u3002\u9700\u8981\u4e00\u79cd\u66f4\u5b89\u5168\u3001\u6548\u7387\u66f4\u9ad8\u7684\u65b0\u6280\u672f\u6765\u8fdb\u884c\u67d0\u4e8b\u540e\u5c11\u4fb5\u5165\u6027\u68c0\u67e5\u3002", "method": "\u7814\u7a76\u4eba\u5458\u5f00\u53d1\u4e86\u4e00\u79cd\u865a\u62df\u73b0\u5b9e\u8bbe\u5907\uff0c\u7edf\u4e86\u6570\u5b57\u53cc\u751f\u6280\u672f\uff0c\u5b9e\u73b0\u5168\u7a0b\u8fdc\u7a0b\u89c4\u5212\u548c\u63a7\u5236\u6c7d\u8f66\u4eba\u5316\u67d0\u4e8b\u540e\u68c0\u67e5\u3002\u8fdb\u884c\u4e86\u4e09\u79cd\u4ea4\u4e92\u65b9\u6cd5\u7684\u53ef\u7528\u6027\u7814\u7a76\uff0c\u5e76\u5728\u4e09\u4f53\u4eba\u7c7b\u5c38\u4f53\u4e0a\u8bc4\u4f30\u4e34\u5e8a\u53ef\u884c\u6027\u3002", "result": "\u8fdb\u884c\u4e86132\u6b21\u9488\u63d2\u5165\u64cd\u4f5c\uff0c\u9488\u5934\u504f\u79fb\u8bef\u5dee\u4e3a5.30\u00b13.25 mm\u3002\u6210\u529f\u83b7\u53d6\u4e86\u7ec4\u7ec7\u6837\u672c\u5e76\u7ecf\u5386\u53f2\u75c5\u7406\u5b66\u9a8c\u8bc1\u3002\u7528\u6237\u53cd\u9988\u8be5\u7cfb\u7edf\u7684\u9488\u5934\u653e\u7f6e\u65b9\u6cd5\u975e\u5e38\u76f4\u89c2\u3002", "conclusion": "\u8be5\u7cfb\u7edf\u662f\u4e00\u79cd\u6709\u524d\u666f\u7684\u3001\u7cbe\u786e\u7684\u4f4e\u98ce\u9669\u66ff\u4ee3\u65b9\u6848\uff0c\u53ef\u4ee5\u4f5c\u4e3a\u4f20\u7edf\u67d0\u4e8b\u540e\u68c0\u67e5\u65b9\u6cd5\u7684\u66ff\u4ee3\u9009\u62e9\u3002"}}
{"id": "2509.02851", "pdf": "https://arxiv.org/pdf/2509.02851", "abs": "https://arxiv.org/abs/2509.02851", "authors": ["Sadra Saremi", "Amirhossein Ahmadkhan Kordbacheh"], "title": "Multi-Scale Deep Learning for Colon Histopathology: A Hybrid Graph-Transformer Approach", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "Colon cancer also known as Colorectal cancer, is one of the most malignant\ntypes of cancer worldwide. Early-stage detection of colon cancer is highly\ncrucial to prevent its deterioration. This research presents a hybrid\nmulti-scale deep learning architecture that synergizes capsule networks, graph\nattention mechanisms, transformer modules, and residual learning to advance\ncolon cancer classification on the Lung and Colon Cancer Histopathological\nImage Dataset (LC25000) dataset. The proposed model in this paper utilizes the\nHG-TNet model that introduces a hybrid architecture that joins strength points\nin transformers and convolutional neural networks to capture multi-scale\nfeatures in histopathological images. Mainly, a transformer branch extracts\nglobal contextual bonds by partitioning the image into patches by\nconvolution-based patch embedding and then processing these patches through a\ntransformer encoder. Analogously, a dedicated CNN branch captures fine-grained,\nlocal details through successive Incorporation these diverse features, combined\nwith a self-supervised rotation prediction objective, produce a robust\ndiagnostic representation that surpasses standard architectures in performance.\nResults show better performance not only in accuracy or loss function but also\nin these algorithms by utilizing capsule networks to preserve spatial orders\nand realize how each element individually combines and forms whole structures.", "AI": {"tldr": "\u57fa\u4e8eTransformer\u5488CNN\u7684\u6df7\u5408\u6df1\u5ea6\u5b66\u4e60\u6a21\u578bHG-TNet\uff0c\u901a\u8fc7\u591a\u5c3a\u5ea6\u7279\u5f81\u63d0\u53d6\u548c\u81ea\u76d1\u7763\u5b66\u4e60\uff0c\u5728\u7ed3\u80a0\u764c\u75c5\u7406\u56fe\u50cf\u5206\u6790\u4e2d\u5b9e\u73b0\u4e86\u66f4\u9ad8\u7684\u8bc6\u522b\u51c6\u786e\u7387\u3002", "motivation": "\u65e9\u671f\u68c0\u6d4b\u7ed3\u80a0\u764c\u5bf9\u9884\u9632\u75c5\u60c5\u6076\u5316\u81f3\u5173\u91cd\u8981\uff0c\u9700\u8981\u66f4\u51c6\u786e\u7684\u75c5\u7406\u56fe\u50cf\u5206\u6790\u6280\u672f\u3002", "method": "\u63d0\u51faHG-TNet\u6df7\u5408\u6a21\u578b\uff0c\u7ed3\u5408Transformer\u63d0\u53d6\u5168\u5c40\u4e0a\u4e0b\u6587\u5173\u7cfb\u5488CNN\u6293\u53d6\u5c40\u90e8\u7ec6\u8282\u7279\u5f81\uff0c\u4f7f\u7528\u80ce\u56ca\u7f51\u7edc\u4fdd\u6301\u7a7a\u95f4\u7ed3\u6784\u4fe1\u606f\uff0c\u52a0\u4e0a\u81ea\u76d1\u7763\u5b66\u4e60\u65cb\u8f6c\u9884\u6d4b\u4efb\u52a1\u3002", "result": "\u5728LC25000\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u8d85\u8fc7\u6807\u51c6\u67b6\u6784\uff0c\u5728\u51c6\u786e\u7387\u5488\u635f\u5931\u51fd\u6570\u65b9\u9762\u90fd\u53d6\u5f97\u66f4\u597d\u7ed3\u679c\u3002", "conclusion": "\u8be5\u6df7\u5408\u65b9\u6cd5\u901a\u8fc7\u591a\u6a21\u6001\u7279\u5f81\u63d0\u53d6\u5488\u81ea\u76d1\u7763\u5b66\u4e60\uff0c\u4e3a\u7ed3\u80a0\u764c\u75c5\u7406\u56fe\u50cf\u5206\u6790\u63d0\u4f9b\u4e86\u66f4\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.02592", "pdf": "https://arxiv.org/pdf/2509.02592", "abs": "https://arxiv.org/abs/2509.02592", "authors": ["Hunter Gittlin"], "title": "Beyond Synthetic Augmentation: Group-Aware Threshold Calibration for Robust Balanced Accuracy in Imbalanced Learning", "categories": ["cs.LG", "cs.AI"], "comment": "Accepted to the AIDEM'25 conference at ECML; to be published in\n  Springer (LNCS)", "summary": "Class imbalance remains a fundamental challenge in machine learning, with\ntraditional solutions often creating as many problems as they solve. We\ndemonstrate that group-aware threshold calibration--setting different decision\nthresholds for different demographic groups--provides superior robustness\ncompared to synthetic data generation methods. Through extensive experiments,\nwe show that group-specific thresholds achieve 1.5-4% higher balanced accuracy\nthan SMOTE and CT-GAN augmented models while improving worst-group balanced\naccuracy. Unlike single-threshold approaches that apply one cutoff across all\ngroups, our group-aware method optimizes the Pareto frontier between balanced\naccuracy and worst-group balanced accuracy, enabling fine-grained control over\ngroup-level performance. Critically, we find that applying group thresholds to\nsynthetically augmented data yields minimal additional benefit, suggesting\nthese approaches are fundamentally redundant. Our results span seven model\nfamilies including linear, tree-based, instance-based, and boosting methods,\nconfirming that group-aware threshold calibration offers a simpler, more\ninterpretable, and more effective solution to class imbalance.", "AI": {"tldr": "\u9488\u5bf9\u7c7b\u522b\u4e0d\u5e73\u8861\u95ee\u9898\uff0c\u672c\u6587\u63d0\u51fa\u7fa4\u4f53\u611f\u77e5\u9608\u503c\u6821\u51c6\u65b9\u6cd5\uff0c\u901a\u8fc7\u4e3a\u4e0d\u540c\u4eba\u53e3\u7edf\u8ba1\u7fa4\u4f53\u8bbe\u7f6e\u4e0d\u540c\u51b3\u7b56\u9608\u503c\uff0c\u76f8\u6bd4\u4f20\u7edf\u5408\u6210\u6570\u636e\u751f\u6210\u65b9\u6cd5\uff08\u5982SMOTE\u548cCT-GAN\uff09\u5728\u5e73\u8861\u51c6\u786e\u7387\u548c\u6700\u5dee\u7fa4\u4f53\u8868\u73b0\u65b9\u9762\u90fd\u53d6\u5f97\u66f4\u597d\u6548\u679c\u3002", "motivation": "\u4f20\u7edf\u89e3\u51b3\u7c7b\u522b\u4e0d\u5e73\u8861\u7684\u65b9\u6cd5\uff08\u5982\u5408\u6210\u6570\u636e\u751f\u6210\uff09\u5f80\u5f80\u5e26\u6765\u65b0\u95ee\u9898\uff0c\u9700\u8981\u5bfb\u627e\u66f4\u7b80\u5355\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002\u7fa4\u4f53\u95f4\u6027\u80fd\u5dee\u5f02\u663e\u8457\uff0c\u5355\u4e00\u9608\u503c\u65b9\u6cd5\u65e0\u6cd5\u6ee1\u8db3\u4e0d\u540c\u7fa4\u4f53\u7684\u6700\u4f18\u6027\u80fd\u9700\u6c42\u3002", "method": "\u91c7\u7528\u7fa4\u4f53\u611f\u77e5\u9608\u503c\u6821\u51c6\u7b56\u7565\uff0c\u4e3a\u4e0d\u540c\u4eba\u53e3\u7edf\u8ba1\u7fa4\u4f53\u5206\u522b\u8bbe\u7f6e\u6700\u4f18\u51b3\u7b56\u9608\u503c\uff0c\u800c\u4e0d\u662f\u4f7f\u7528\u7edf\u4e00\u7684\u5168\u5c40\u9608\u503c\u3002\u5728\u4e03\u4e2a\u6a21\u578b\u5bb6\u65cf\uff08\u7ebf\u6027\u3001\u6811\u57fa\u3001\u5b9e\u4f8b\u57fa\u548c\u63d0\u5347\u65b9\u6cd5\uff09\u4e0a\u8fdb\u884c\u5e7f\u6cdb\u5b9e\u9a8c\u9a8c\u8bc1\u3002", "result": "\u7fa4\u4f53\u7279\u5b9a\u9608\u503c\u65b9\u6cd5\u6bd4SMOTE\u548cCT-GAN\u589e\u5f3a\u6a21\u578b\u83b7\u5f971.5-4%\u66f4\u9ad8\u7684\u5e73\u8861\u51c6\u786e\u7387\uff0c\u540c\u65f6\u6539\u5584\u4e86\u6700\u5dee\u7fa4\u4f53\u7684\u5e73\u8861\u51c6\u786e\u7387\u3002\u8be5\u65b9\u6cd5\u5728\u5e73\u8861\u51c6\u786e\u7387\u548c\u6700\u5dee\u7fa4\u4f53\u6027\u80fd\u4e4b\u95f4\u5b9e\u73b0\u4e86\u5e15\u7d2f\u6258\u6700\u4f18\u3002", "conclusion": "\u7fa4\u4f53\u611f\u77e5\u9608\u503c\u6821\u51c6\u63d0\u4f9b\u4e86\u4e00\u79cd\u66f4\u7b80\u5355\u3001\u53ef\u89e3\u91ca\u6027\u66f4\u5f3a\u4e14\u66f4\u6709\u6548\u7684\u7c7b\u522b\u4e0d\u5e73\u8861\u89e3\u51b3\u65b9\u6848\uff0c\u76f8\u6bd4\u5408\u6210\u6570\u636e\u751f\u6210\u65b9\u6cd5\u5177\u6709\u660e\u663e\u4f18\u52bf\uff0c\u4e14\u4e24\u79cd\u65b9\u6cd5\u7ed3\u5408\u4f7f\u7528\u6536\u76ca\u6709\u9650\u3002"}}
{"id": "2509.02782", "pdf": "https://arxiv.org/pdf/2509.02782", "abs": "https://arxiv.org/abs/2509.02782", "authors": ["V\u00e1clav Sobotka", "Lucas Kletzander", "Nysret Musliu", "Hana Rudov\u00e1"], "title": "Key Principles in Cross-Domain Hyper-Heuristic Performance", "categories": ["cs.AI"], "comment": null, "summary": "Cross-domain selection hyper-heuristics aim to distill decades of research on\nproblem-specific heuristic search algorithms into adaptable general-purpose\nsearch strategies. In this respect, existing selection hyper-heuristics\nprimarily focus on an adaptive selection of low-level heuristics (LLHs) from a\npredefined set. In contrast, we concentrate on the composition of this set and\nits strategic transformations. We systematically analyze transformations based\non three key principles: solution acceptance, LLH repetitions, and perturbation\nintensity, i.e., the proportion of a solution affected by a perturbative LLH.\nWe demonstrate the raw effects of our transformations on a trivial unbiased\nrandom selection mechanism. With an appropriately constructed transformation,\nthis trivial method outperforms all available state-of-the-art hyper-heuristics\non three challenging real-world domains and finds 11 new best-known solutions.\nThe same method is competitive with the winner of the CHeSC competition,\ncommonly used as the standard cross-domain benchmark. Moreover, we accompany\nseveral recent hyper-heuristics with such strategic transformations. Using this\napproach, we outperform the current state-of-the-art methods on both the CHeSC\nbenchmark and real-world domains while often simplifying their designs.", "AI": {"tldr": "\u901a\u8fc7\u7cfb\u7edf\u5206\u6790\u4f4e\u7ea7\u8d4b\u503c\u7b56\u7565\u7684\u4e09\u4e2a\u5173\u952e\u8f6c\u6362\u539f\u5219\uff08\u89e3\u51b3\u65b9\u6848\u63a5\u53d7\u3001\u91cd\u590d\u6267\u884c\u548c\u6270\u52a8\u5f3a\u5ea6\uff09\uff0c\u6784\u5efa\u4e86\u4e00\u79cd\u7b80\u5355\u4f46\u9ad8\u6548\u7684\u8de8\u57df\u9009\u62e9\u8d85\u7ea7\u8d4b\u503c\u7b56\u7565\uff0c\u5728\u591a\u4e2a\u5b9e\u9645\u5e94\u7528\u9886\u57df\u8d85\u8d8a\u4e86\u73b0\u6709\u6700\u5148\u8fdb\u65b9\u6cd5\u5e76\u53d1\u73b011\u4e2a\u65b0\u7684\u6700\u4f73\u89e3\u3002", "motivation": "\u73b0\u6709\u7684\u9009\u62e9\u8d85\u7ea7\u8d4b\u503c\u7b56\u7565\u4e3b\u8981\u5173\u6ce8\u4e8e\u4ece\u9884\u5b9a\u4e49\u96c6\u5408\u4e2d\u9002\u5e94\u6027\u9009\u62e9\u4f4e\u7ea7\u8d4b\u503c\uff0c\u800c\u672c\u6587\u91cd\u70b9\u7814\u7a76\u5982\u4f55\u6784\u9020\u548c\u8f6c\u6362\u8fd9\u4e2a\u96c6\u5408\u7684\u7b56\u7565\u6027\u8bbe\u8ba1\u3002", "method": "\u7cfb\u7edf\u5206\u6790\u4e86\u57fa\u4e8e\u4e09\u4e2a\u5173\u952e\u539f\u5219\u7684\u8f6c\u6362\u65b9\u6cd5\uff1a\u89e3\u51b3\u65b9\u6848\u63a5\u53d7\u3001\u4f4e\u7ea7\u8d4b\u503c\u91cd\u590d\u6267\u884c\u6b21\u6570\u3001\u4ee5\u53ca\u6270\u52a8\u5f3a\u5ea6\uff08\u5373\u6270\u52a8\u6027\u8d4b\u503c\u5f71\u54cd\u89e3\u51b3\u65b9\u6848\u7684\u6bd4\u4f8b\uff09\u3002\u5c06\u8fd9\u4e9b\u8f6c\u6362\u5e94\u7528\u4e8e\u7b80\u5355\u7684\u968f\u673a\u9009\u62e9\u673a\u5236\u6765\u9a8c\u8bc1\u5176\u6548\u679c\u3002", "result": "\u901a\u8fc7\u5408\u9002\u7684\u8f6c\u6362\u6784\u9020\uff0c\u7b80\u5355\u7684\u968f\u673a\u9009\u62e9\u673a\u5236\u5728\u4e09\u4e2a\u5177\u6709\u6311\u6218\u6027\u7684\u5b9e\u9645\u9886\u57df\u4e0a\u8d85\u8d8a\u4e86\u6240\u6709\u73b0\u6709\u7684\u6700\u5148\u8fdb\u8d85\u7ea7\u8d4b\u503c\u7b56\u7565\uff0c\u53d1\u73b0\u4e8611\u4e2a\u65b0\u7684\u6700\u4f73\u89e3\u3002\u8be5\u65b9\u6cd5\u5728CHeSC\u7ade\u8d5b\u6807\u51c6\u6d4b\u8bd5\u4e2d\u4e0e\u51a0\u519b\u65b9\u6cd5\u4e0d\u76f8\u4e0a\u4e0b\u3002\u5c06\u8fd9\u4e9b\u8f6c\u6362\u4e0e\u6700\u65b0\u7684\u8d85\u7ea7\u8d4b\u503c\u7b56\u7565\u7ed3\u5408\u540e\uff0c\u5728\u6807\u51c6\u6d4b\u8bd5\u548c\u5b9e\u9645\u9886\u57df\u4e0a\u90fd\u8d85\u8d8a\u4e86\u5f53\u524d\u6700\u5148\u8fdb\u65b9\u6cd5\u3002", "conclusion": "\u901a\u8fc7\u7b56\u7565\u6027\u8f6c\u6362\u4f4e\u7ea7\u8d4b\u503c\u96c6\u5408\u7684\u6784\u9020\u65b9\u6cd5\uff0c\u53ef\u4ee5\u5f88\u5927\u7a0b\u5ea6\u63d0\u5347\u8d85\u7ea7\u8d4b\u503c\u7b56\u7565\u7684\u6027\u80fd\uff0c\u751a\u81f3\u80fd\u8ba9\u7b80\u5355\u7684\u968f\u673a\u9009\u62e9\u673a\u5236\u8fbe\u5230\u9886\u5148\u6c34\u5e73\uff0c\u540c\u65f6\u8fd8\u80fd\u7b80\u5316\u590d\u6742\u7b97\u6cd5\u7684\u8bbe\u8ba1\u3002"}}
{"id": "2509.02800", "pdf": "https://arxiv.org/pdf/2509.02800", "abs": "https://arxiv.org/abs/2509.02800", "authors": ["Niuniu Zhang"], "title": "Too Noisy to Collude? Algorithmic Collusion Under Laplacian Noise", "categories": ["econ.GN", "cs.GT", "cs.MA", "q-fin.EC"], "comment": null, "summary": "The rise of autonomous pricing systems has sparked growing concern over\nalgorithmic collusion in markets from retail to housing. This paper examines\ncontrolled information quality as an ex ante policy lever: by reducing the\nfidelity of data that pricing algorithms draw on, regulators can frustrate\ncollusion before supracompetitive prices emerge. We show, first, that\ninformation quality is the central driver of competitive outcomes, shaping\nprices, profits, and consumer welfare. Second, we demonstrate that collusion\ncan be slowed or destabilized by injecting carefully calibrated noise into\npooled market data, yielding a feasibility region where intervention disrupts\ncartels without undermining legitimate pricing. Together, these results\nhighlight information control as a lightweight yet practical lever to blunt\ndigital collusion at its source.", "AI": {"tldr": "\u901a\u8fc7\u63a7\u5236\u4fe1\u606f\u8d28\u91cf\u548c\u6ce8\u5165\u566a\u58f0\u6765\u963b\u6b62\u7b97\u6cd5\u5408\u4f19\uff0c\u4f7f\u5176\u5728\u5408\u7406\u5b9a\u4ef7\u8303\u56f4\u5185\u7a33\u5b9a", "motivation": "\u81ea\u4e3b\u5b9a\u4ef7\u7cfb\u7edf\u7684\u5174\u8d77\u5f15\u53d1\u5bf9\u7b97\u6cd5\u5408\u4f19\u7684\u62c5\u5fe7\uff0c\u9700\u8981\u4e8b\u524d\u653e\u7f6e\u653f\u7b56\u6765\u9632\u8303\u8d85\u8fc7\u7ade\u4e89\u6c34\u5e73\u7684\u4ef7\u683c", "method": "\u901a\u8fc7\u51cf\u5c11\u5b9a\u4ef7\u7b97\u6cd5\u6240\u4f9d\u8d56\u6570\u636e\u7684\u4fdd\u771f\u5ea6\uff0c\u5e76\u5728\u6c60\u5316\u5e02\u573a\u6570\u636e\u4e2d\u6ce8\u5165\u7cbe\u51c6\u68c0\u5b9a\u7684\u566a\u58f0", "result": "\u4fe1\u606f\u8d28\u91cf\u662f\u7ade\u4e89\u7ed3\u679c\u7684\u6838\u5fc3\u9a71\u52a8\u56e0\u7d20\uff0c\u901a\u8fc7\u566a\u58f0\u6ce8\u5165\u53ef\u4ee5\u51cf\u7f13\u6216\u7834\u574f\u5408\u4f19\uff0c\u5f62\u6210\u4e00\u4e2a\u53ef\u884c\u5e72\u9884\u533a\u57df", "conclusion": "\u4fe1\u606f\u63a7\u5236\u662f\u4e00\u79cd\u8f7b\u91cf\u7ea7\u4f46\u5b9e\u7528\u7684\u653f\u7b56\u624b\u6bb5\uff0c\u53ef\u4ee5\u4ece\u6e90\u5934\u51b2\u51fb\u6570\u5b57\u5408\u4f19"}}
{"id": "2509.02571", "pdf": "https://arxiv.org/pdf/2509.02571", "abs": "https://arxiv.org/abs/2509.02571", "authors": ["Diego Di Carlo", "Koyama Shoichi", "Nugraha Aditya Arie", "Fontaine Mathieu", "Bando Yoshiaki", "Yoshii Kazuyoshi"], "title": "Gaussian Process Regression of Steering Vectors With Physics-Aware Deep Composite Kernels for Augmented Listening", "categories": ["eess.AS", "cs.LG", "cs.SD", "eess.SP"], "comment": null, "summary": "This paper investigates continuous representations of steering vectors over\nfrequency and position of microphone and source for augmented listening (e.g.,\nspatial filtering and binaural rendering) with precise control of the sound\nfield perceived by the user. Steering vectors have typically been used for\nrepresenting the spatial characteristics of the sound field as a function of\nthe listening position. The basic algebraic representation of steering vectors\nassuming an idealized environment cannot deal with the scattering effect of the\nsound field. One may thus collect a discrete set of real steering vectors\nmeasured in dedicated facilities and super-resolve (i.e., upsample) them.\nRecently, physics-aware deep learning methods have been effectively used for\nthis purpose. Such deterministic super-resolution, however, suffers from the\noverfitting problem due to the non-uniform uncertainty over the measurement\nspace. To solve this problem, we integrate an expressive representation based\non the neural field (NF) into the principled probabilistic framework based on\nthe Gaussian process (GP). Specifically, we propose a physics-aware composite\nkernel that model the directional incoming waves and the subsequent scattering\neffect. Our comprehensive comparative experiment showed the effectiveness of\nthe proposed method under data insufficiency conditions. In downstream tasks\nsuch as speech enhancement and binaural rendering using the simulated data of\nthe SPEAR challenge, the oracle performances were attained with less than ten\ntimes fewer measurements.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u795e\u7ecf\u573a\u548c\u9ad8\u65af\u8fc7\u7a0b\u7684\u7269\u7406\u611f\u77e5\u590d\u5408\u6838\u65b9\u6cd5\uff0c\u7528\u4e8e\u8fde\u7eed\u8868\u793a\u58f0\u573a\u5bfc\u5411\u5411\u91cf\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u786e\u5b9a\u6027\u8d85\u5206\u8fa8\u7387\u65b9\u6cd5\u5728\u6d4b\u91cf\u7a7a\u95f4\u975e\u5747\u5300\u4e0d\u786e\u5b9a\u6027\u4e0b\u7684\u8fc7\u62df\u5408\u95ee\u9898\u3002", "motivation": "\u4f20\u7edf\u5bfc\u5411\u5411\u91cf\u8868\u793a\u65b9\u6cd5\u65e0\u6cd5\u5904\u7406\u58f0\u573a\u7684\u6563\u5c04\u6548\u5e94\uff0c\u800c\u73b0\u6709\u7684\u7269\u7406\u611f\u77e5\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u867d\u7136\u6709\u6548\uff0c\u4f46\u5728\u6d4b\u91cf\u7a7a\u95f4\u975e\u5747\u5300\u4e0d\u786e\u5b9a\u6027\u4e0b\u5bb9\u6613\u8fc7\u62df\u5408\uff0c\u9700\u8981\u66f4\u597d\u7684\u6982\u7387\u6846\u67b6\u6765\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\u3002", "method": "\u5c06\u795e\u7ecf\u573a\uff08NF\uff09\u7684\u8868\u8fbe\u6027\u8868\u793a\u96c6\u6210\u5230\u57fa\u4e8e\u9ad8\u65af\u8fc7\u7a0b\uff08GP\uff09\u7684\u6982\u7387\u6846\u67b6\u4e2d\uff0c\u63d0\u51fa\u7269\u7406\u611f\u77e5\u590d\u5408\u6838\u6765\u5efa\u6a21\u65b9\u5411\u6027\u5165\u5c04\u6ce2\u548c\u540e\u7eed\u6563\u5c04\u6548\u5e94\u3002", "result": "\u5728\u6570\u636e\u4e0d\u8db3\u6761\u4ef6\u4e0b\uff0c\u8be5\u65b9\u6cd5\u8868\u73b0\u51fa\u6709\u6548\u6027\u3002\u5728\u4e0b\u6e38\u4efb\u52a1\u5982\u8bed\u97f3\u589e\u5f3a\u548c\u53cc\u8033\u6e32\u67d3\u4e2d\uff0c\u4f7f\u7528SPEAR\u6311\u6218\u8d5b\u7684\u6a21\u62df\u6570\u636e\uff0c\u4ec5\u9700\u4e0d\u5230\u5341\u5206\u4e4b\u4e00\u7684\u6d4b\u91cf\u6b21\u6570\u5c31\u80fd\u8fbe\u5230oracle\u6027\u80fd\u3002", "conclusion": "\u63d0\u51fa\u7684NF-GP\u590d\u5408\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u89e3\u51b3\u58f0\u573a\u5bfc\u5411\u5411\u91cf\u8fde\u7eed\u8868\u793a\u4e2d\u7684\u8fc7\u62df\u5408\u95ee\u9898\uff0c\u5728\u6570\u636e\u7a00\u7f3a\u6761\u4ef6\u4e0b\u4ecd\u80fd\u4fdd\u6301\u4f18\u5f02\u6027\u80fd\uff0c\u4e3a\u589e\u5f3a\u542c\u89c9\u5e94\u7528\u63d0\u4f9b\u4e86\u7cbe\u786e\u7684\u58f0\u573a\u63a7\u5236\u80fd\u529b\u3002"}}
{"id": "2509.02855", "pdf": "https://arxiv.org/pdf/2509.02855", "abs": "https://arxiv.org/abs/2509.02855", "authors": ["Hyunji Nam", "Lucia Langlois", "James Malamut", "Mei Tan", "Dorottya Demszky"], "title": "IDEAlign: Comparing Large Language Models to Human Experts in Open-ended Interpretive Annotations", "categories": ["cs.CL", "cs.CY"], "comment": "10 pages, 9 pages for appendix", "summary": "Large language models (LLMs) are increasingly applied to open-ended,\ninterpretive annotation tasks, such as thematic analysis by researchers or\ngenerating feedback on student work by teachers. These tasks involve free-text\nannotations requiring expert-level judgments grounded in specific objectives\n(e.g., research questions or instructional goals). Evaluating whether\nLLM-generated annotations align with those generated by expert humans is\nchallenging to do at scale, and currently, no validated, scalable measure of\nsimilarity in ideas exists. In this paper, we (i) introduce the scalable\nevaluation of interpretive annotation by LLMs as a critical and understudied\ntask, (ii) propose IDEAlgin, an intuitive benchmarking paradigm for capturing\nexpert similarity ratings via a \"pick-the-odd-one-out\" triplet judgment task,\nand (iii) evaluate various similarity metrics, including vector-based ones\n(topic models, embeddings) and LLM-as-a-judge via IDEAlgin, against these human\nbenchmarks. Applying this approach to two real-world educational datasets\n(interpretive analysis and feedback generation), we find that vector-based\nmetrics largely fail to capture the nuanced dimensions of similarity meaningful\nto experts. Prompting LLMs via IDEAlgin significantly improves alignment with\nexpert judgments (9-30% increase) compared to traditional lexical and\nvector-based metrics. These results establish IDEAlgin as a promising paradigm\nfor evaluating LLMs against open-ended expert annotations at scale, informing\nresponsible deployment of LLMs in education and beyond.", "AI": {"tldr": "\u63d0\u51fa\u4e86IDEAlgin\u8bc4\u4f30\u6846\u67b6\uff0c\u901a\u8fc7\"\u4e09\u9009\u4e00\"\u4efb\u52a1\u6765\u8bc4\u4f30LLM\u751f\u6210\u89e3\u91ca\u6027\u6807\u6ce8\u4e0e\u4e13\u5bb6\u6807\u6ce8\u7684\u76f8\u4f3c\u6027\uff0c\u53d1\u73b0\u57fa\u4e8e\u5411\u91cf\u7684\u6307\u6807\u6548\u679c\u4e0d\u4f73\uff0c\u800cLLM\u4f5c\u4e3a\u8bc4\u5224\u8005\u80fd\u663e\u8457\u63d0\u5347\u4e0e\u4e13\u5bb6\u5224\u65ad\u7684\u4e00\u81f4\u6027\u3002", "motivation": "LLM\u8d8a\u6765\u8d8a\u591a\u5730\u5e94\u7528\u4e8e\u5f00\u653e\u5f0f\u89e3\u91ca\u6027\u6807\u6ce8\u4efb\u52a1\uff0c\u4f46\u7f3a\u4e4f\u53ef\u6269\u5c55\u7684\u65b9\u6cd5\u6765\u8bc4\u4f30LLM\u751f\u6210\u6807\u6ce8\u4e0e\u4e13\u5bb6\u6807\u6ce8\u7684\u76f8\u4f3c\u6027\uff0c\u7279\u522b\u662f\u5728\u6559\u80b2\u548c\u7814\u7a76\u9886\u57df\u3002", "method": "\u63d0\u51faIDEAlgin\u57fa\u51c6\u6d4b\u8bd5\u8303\u5f0f\uff0c\u4f7f\u7528\"\u4e09\u9009\u4e00\"\u4e09\u5143\u7ec4\u5224\u65ad\u4efb\u52a1\u6536\u96c6\u4e13\u5bb6\u76f8\u4f3c\u6027\u8bc4\u5206\uff0c\u5e76\u6bd4\u8f83\u5404\u79cd\u76f8\u4f3c\u6027\u6307\u6807\uff08\u57fa\u4e8e\u5411\u91cf\u7684\u65b9\u6cd5\u548cLLM\u4f5c\u4e3a\u8bc4\u5224\u8005\uff09\u4e0e\u4eba\u7c7b\u57fa\u51c6\u7684\u4e00\u81f4\u6027\u3002", "result": "\u57fa\u4e8e\u5411\u91cf\u7684\u6307\u6807\u65e0\u6cd5\u6355\u6349\u4e13\u5bb6\u5173\u6ce8\u7684\u7ec6\u5fae\u76f8\u4f3c\u6027\u7ef4\u5ea6\uff0c\u800c\u901a\u8fc7IDEAlgin\u63d0\u793a\u7684LLM\u76f8\u6bd4\u4f20\u7edf\u8bcd\u6c47\u548c\u5411\u91cf\u6307\u6807\uff0c\u4e0e\u4e13\u5bb6\u5224\u65ad\u7684\u4e00\u81f4\u6027\u63d0\u9ad8\u4e869-30%\u3002", "conclusion": "IDEAlgin\u4e3a\u5927\u89c4\u6a21\u8bc4\u4f30LLM\u5728\u5f00\u653e\u5f0f\u4e13\u5bb6\u6807\u6ce8\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\u63d0\u4f9b\u4e86\u6709\u524d\u666f\u7684\u8303\u5f0f\uff0c\u6709\u52a9\u4e8e\u5728\u6559\u80b2\u7b49\u9886\u57df\u8d1f\u8d23\u4efb\u5730\u90e8\u7f72LLM\u3002"}}
{"id": "2509.02808", "pdf": "https://arxiv.org/pdf/2509.02808", "abs": "https://arxiv.org/abs/2509.02808", "authors": ["Isaac Ronald Ward", "Mark Paral", "Kristopher Riordan", "Mykel J. Kochenderfer"], "title": "Improving the Resilience of Quadrotors in Underground Environments by Combining Learning-based and Safety Controllers", "categories": ["cs.RO", "cs.AI", "cs.SY", "eess.SY"], "comment": "Accepted and awarded best paper at the 11th International Conference\n  on Control, Decision and Information Technologies (CoDIT 2025 -\n  https://codit2025.org/)", "summary": "Autonomously controlling quadrotors in large-scale subterranean environments\nis applicable to many areas such as environmental surveying, mining operations,\nand search and rescue. Learning-based controllers represent an appealing\napproach to autonomy, but are known to not generalize well to\n`out-of-distribution' environments not encountered during training. In this\nwork, we train a normalizing flow-based prior over the environment, which\nprovides a measure of how far out-of-distribution the quadrotor is at any given\ntime. We use this measure as a runtime monitor, allowing us to switch between a\nlearning-based controller and a safe controller when we are sufficiently\nout-of-distribution. Our methods are benchmarked on a point-to-point navigation\ntask in a simulated 3D cave environment based on real-world point cloud data\nfrom the DARPA Subterranean Challenge Final Event Dataset. Our experimental\nresults show that our combined controller simultaneously possesses the liveness\nof the learning-based controller (completing the task quickly) and the safety\nof the safety controller (avoiding collision).", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5f52\u4e00\u5316\u6d41\u7684\u8fd0\u884c\u65f6\u76d1\u63a7\u65b9\u6cd5\uff0c\u7528\u4e8e\u5728\u56db\u65cb\u7ffc\u65e0\u4eba\u673a\u5bfc\u822a\u4e2d\u5207\u6362\u5b66\u4e60\u63a7\u5236\u5668\u548c\u5b89\u5168\u63a7\u5236\u5668\uff0c\u4ee5\u540c\u65f6\u4fdd\u8bc1\u4efb\u52a1\u6548\u7387\u548c\u5b89\u5168\u6027\u3002", "motivation": "\u5b66\u4e60\u578b\u63a7\u5236\u5668\u5728\u5927\u89c4\u6a21\u5730\u4e0b\u73af\u5883\u81ea\u4e3b\u63a7\u5236\u4e2d\u5177\u6709\u5e94\u7528\u6f5c\u529b\uff0c\u4f46\u5b58\u5728\u6cdb\u5316\u6027\u95ee\u9898\uff0c\u5728\u8bad\u7ec3\u672a\u89c1\u7684\u73af\u5883\u5206\u5e03\u5916\u533a\u57df\u8868\u73b0\u4e0d\u4f73\uff0c\u9700\u8981\u4fdd\u8bc1\u5b89\u5168\u6027\u3002", "method": "\u8bad\u7ec3\u57fa\u4e8e\u5f52\u4e00\u5316\u6d41\u7684\u73af\u5883\u5148\u9a8c\u5206\u5e03\uff0c\u4f5c\u4e3a\u8fd0\u884c\u65f6\u76d1\u63a7\u6307\u6807\u6765\u8bc4\u4f30\u65e0\u4eba\u673a\u662f\u5426\u5904\u4e8e\u5206\u5e03\u5916\u72b6\u6001\uff0c\u5e76\u636e\u6b64\u5728\u5b66\u4e60\u63a7\u5236\u5668\u548c\u5b89\u5168\u63a7\u5236\u5668\u4e4b\u95f4\u5207\u6362\u3002", "result": "\u5728\u57fa\u4e8e\u771f\u5b9e\u70b9\u4e91\u6570\u636e\u7684\u6a21\u62df3D\u6d1e\u7a74\u73af\u5883\u4e2d\u8fdb\u884c\u70b9\u5bf9\u70b9\u5bfc\u822a\u6d4b\u8bd5\uff0c\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\u7ec4\u5408\u63a7\u5236\u5668\u540c\u65f6\u5177\u5907\u5b66\u4e60\u63a7\u5236\u5668\u7684\u5feb\u901f\u4efb\u52a1\u5b8c\u6210\u80fd\u529b\u548c\u5b89\u5168\u63a7\u5236\u5668\u7684\u907f\u78b0\u5b89\u5168\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u5b66\u4e60\u63a7\u5236\u5668\u5728\u5206\u5e03\u5916\u73af\u5883\u4e2d\u7684\u5b89\u5168\u6027\u95ee\u9898\uff0c\u4e3a\u5730\u4e0b\u73af\u5883\u81ea\u4e3b\u5bfc\u822a\u63d0\u4f9b\u4e86\u53ef\u9760\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.02898", "pdf": "https://arxiv.org/pdf/2509.02898", "abs": "https://arxiv.org/abs/2509.02898", "authors": ["Armin Saadat", "Nima Hashemi", "Hooman Vaseli", "Michael Y. Tsang", "Christina Luong", "Michiel Van de Panne", "Teresa S. M. Tsang", "Purang Abolmaesumi"], "title": "PRECISE-AS: Personalized Reinforcement Learning for Efficient Point-of-Care Echocardiography in Aortic Stenosis Diagnosis", "categories": ["cs.CV"], "comment": "To be published in MICCAI 2025", "summary": "Aortic stenosis (AS) is a life-threatening condition caused by a narrowing of\nthe aortic valve, leading to impaired blood flow. Despite its high prevalence,\naccess to echocardiography (echo), the gold-standard diagnostic tool, is often\nlimited due to resource constraints, particularly in rural and underserved\nareas. Point-of-care ultrasound (POCUS) offers a more accessible alternative\nbut is restricted by operator expertise and the challenge of selecting the most\nrelevant imaging views. To address this, we propose a reinforcement learning\n(RL)-driven active video acquisition framework that dynamically selects each\npatient's most informative echo videos. Unlike traditional methods that rely on\na fixed set of videos, our approach continuously evaluates whether additional\nimaging is needed, optimizing both accuracy and efficiency. Tested on data from\n2,572 patients, our method achieves 80.6% classification accuracy while using\nonly 47% of the echo videos compared to a full acquisition. These results\ndemonstrate the potential of active feature acquisition to enhance AS\ndiagnosis, making echocardiographic assessments more efficient, scalable, and\npersonalized. Our source code is available at:\nhttps://github.com/Armin-Saadat/PRECISE-AS.", "AI": {"tldr": "\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u4e3b\u52a8\u89c6\u9891\u91c7\u96c6\u6846\u67b6\uff0c\u7528\u4e8e\u4f18\u5316\u4e3b\u52a8\u8109\u72ed\u7a84\u8bca\u65ad\u7684\u8d85\u58f0\u5fc3\u52a8\u56fe\u89c6\u9891\u9009\u62e9\uff0c\u5728\u51cf\u5c1147%\u89c6\u9891\u91c7\u96c6\u91cf\u7684\u60c5\u51b5\u4e0b\u8fbe\u523080.6%\u7684\u5206\u7c7b\u51c6\u786e\u7387", "motivation": "\u89e3\u51b3\u4e3b\u52a8\u8109\u72ed\u7a84\u8bca\u65ad\u4e2d\u8d85\u58f0\u5fc3\u52a8\u56fe\u8d44\u6e90\u53d7\u9650\u7684\u95ee\u9898\uff0c\u7279\u522b\u662f\u5728\u519c\u6751\u548c\u670d\u52a1\u4e0d\u8db3\u5730\u533a\uff0c\u4f20\u7edf\u65b9\u6cd5\u9700\u8981\u56fa\u5b9a\u89c6\u9891\u96c6\u4e14\u4f9d\u8d56\u64cd\u4f5c\u8005\u4e13\u4e1a\u77e5\u8bc6", "method": "\u63d0\u51fa\u5f3a\u5316\u5b66\u4e60\u9a71\u52a8\u7684\u4e3b\u52a8\u89c6\u9891\u91c7\u96c6\u6846\u67b6\uff0c\u52a8\u6001\u9009\u62e9\u6bcf\u4f4d\u60a3\u8005\u6700\u5177\u4fe1\u606f\u91cf\u7684\u8d85\u58f0\u5fc3\u52a8\u56fe\u89c6\u9891\uff0c\u6301\u7eed\u8bc4\u4f30\u662f\u5426\u9700\u8981\u989d\u5916\u6210\u50cf", "result": "\u57282,572\u540d\u60a3\u8005\u6570\u636e\u4e0a\u6d4b\u8bd5\uff0c\u4ec5\u4f7f\u752847%\u7684\u89c6\u9891\u91cf\u5c31\u5b9e\u73b0\u4e8680.6%\u7684\u5206\u7c7b\u51c6\u786e\u7387", "conclusion": "\u8be5\u65b9\u6cd5\u80fd\u591f\u63d0\u9ad8\u4e3b\u52a8\u8109\u72ed\u7a84\u8bca\u65ad\u7684\u6548\u7387\u3001\u53ef\u6269\u5c55\u6027\u548c\u4e2a\u6027\u5316\u7a0b\u5ea6\uff0c\u4f7f\u8d85\u58f0\u5fc3\u52a8\u56fe\u8bc4\u4f30\u66f4\u52a0\u9ad8\u6548"}}
{"id": "2509.02709", "pdf": "https://arxiv.org/pdf/2509.02709", "abs": "https://arxiv.org/abs/2509.02709", "authors": ["Cheol Woo Kim", "Shresth Verma", "Mauricio Tec", "Milind Tambe"], "title": "Preference Robustness for DPO with Applications to Public Health", "categories": ["cs.LG"], "comment": null, "summary": "We study an LLM fine-tuning task for designing reward functions for\nsequential resource allocation problems in public health, guided by human\npreferences expressed in natural language. This setting presents a challenging\ntestbed for alignment due to complex and ambiguous objectives and limited data\navailability. We propose DPO-PRO, a robust fine-tuning algorithm based on\nDirect Preference Optimization (DPO), which accounts for uncertainty in the\npreference distribution using a lightweight Distributionally Robust\nOptimization (DRO) formulation. Unlike prior DRO-based DPO methods, DPO-PRO is\nsignificantly less conservative. We evaluate DPO-PRO on a real-world maternal\nmobile health program operated by the non-profit organization ARMMAN, as well\nas on standard alignment benchmarks. Experimental results demonstrate that our\nmethod consistently improves robustness to noisy preference signals compared to\nexisting DPO variants. Moreover, DPO-PRO achieves comparable performance to\nprior self-reflection-based baseline for reward function design, while\nrequiring significantly lower inference-time cost.", "AI": {"tldr": "\u57fa\u4e8eDPO\u7684\u5065\u58c1\u7cbe\u7c92\u5ea6\u8c03\u4f18\u7b97\u6cd5DPO-PRO\uff0c\u901a\u8fc7\u5206\u5e03\u5065\u58c1\u4f18\u5316\u5904\u7406\u4eba\u7c7b\u504f\u597d\u4e0d\u786e\u5b9a\u6027\uff0c\u5728\u516c\u5171\u5065\u5eb7\u8d44\u6e90\u5206\u914d\u4efb\u52a1\u4e2d\u663e\u8457\u63d0\u5347\u4e86\u5bf9\u566a\u58f0\u504f\u597d\u4fe1\u53f7\u7684\u5065\u58c1\u6027\uff0c\u4e14\u63a8\u7406\u6210\u672c\u66f4\u4f4e\u3002", "motivation": "\u516c\u5171\u5065\u5eb7\u9886\u57df\u7684\u8fde\u7eed\u8d44\u6e90\u5206\u914d\u95ee\u9898\u5177\u6709\u590d\u6742\u548c\u6a21\u7cca\u7684\u76ee\u6807\uff0c\u52a0\u4e4b\u6570\u636e\u6709\u9650\uff0c\u4f7f\u5f97\u57fa\u4e8e\u4eba\u7c7b\u504f\u597d\u7684LLM\u5fae\u8c03\u5bf9\u51c6\u5f88\u5177\u6311\u6218\u3002\u9700\u8981\u4e00\u79cd\u80fd\u591f\u5904\u7406\u504f\u597d\u5206\u5e03\u4e0d\u786e\u5b9a\u6027\u7684\u5065\u58c1\u7b97\u6cd5\u3002", "method": "\u63d0\u51faDPO-PRO\u7b97\u6cd5\uff0c\u57fa\u4e8e\u76f4\u63a5\u504f\u597d\u4f18\u5316(DPO)\uff0c\u91c7\u7528\u8f7b\u91cf\u7ea7\u7684\u5206\u5e03\u5065\u58c1\u4f18\u5316(DRO)\u6765\u5904\u7406\u504f\u597d\u5206\u5e03\u7684\u4e0d\u786e\u5b9a\u6027\u3002\u4e0e\u4ee5\u524d\u7684DRO-DPO\u65b9\u6cd5\u76f8\u6bd4\uff0cDPO-PRO\u66f4\u4e0d\u4fdd\u5b88\u3002", "result": "\u5728ARMMAN\u6bcd\u5a74\u79fb\u52a8\u5065\u5eb7\u9879\u76ee\u548c\u6807\u51c6\u5bf9\u51c6\u6d4b\u8bd5\u96c6\u4e0a\u8bc4\u4f30\uff0cDPO-PRO\u5728\u566a\u58f0\u504f\u597d\u4fe1\u53f7\u4e0b\u663e\u8457\u63d0\u5347\u4e86\u5065\u58c1\u6027\uff0c\u4e14\u4e0e\u81ea\u53cd\u601d\u57fa\u7ebf\u65b9\u6cd5\u76f8\u6bd4\u6709\u76f8\u4f3c\u6027\u80fd\u4f46\u63a8\u7406\u6210\u672c\u66f4\u4f4e\u3002", "conclusion": "DPO-PRO\u662f\u4e00\u79cd\u9ad8\u6548\u7684\u5065\u58c1\u5fae\u8c03\u65b9\u6cd5\uff0c\u80fd\u591f\u5728\u590d\u6742\u516c\u5171\u5065\u5eb7\u573a\u666f\u4e2d\u5904\u7406\u4eba\u7c7b\u504f\u597d\u7684\u4e0d\u786e\u5b9a\u6027\uff0c\u540c\u65f6\u4fdd\u6301\u4f4e\u8ba1\u7b97\u6210\u672c\u3002"}}
{"id": "2509.02794", "pdf": "https://arxiv.org/pdf/2509.02794", "abs": "https://arxiv.org/abs/2509.02794", "authors": ["Blai Bonet", "Hector Geffner"], "title": "Learning General Policies From Examples", "categories": ["cs.AI"], "comment": null, "summary": "Combinatorial methods for learning general policies that solve large\ncollections of planning problems have been recently developed. One of their\nstrengths, in relation to deep learning approaches, is that the resulting\npolicies can be understood and shown to be correct. A weakness is that the\nmethods do not scale up and learn only from small training instances and\nfeature pools that contain a few hundreds of states and features at most. In\nthis work, we propose a new symbolic method for learning policies based on the\ngeneralization of sampled plans that ensures structural termination and hence\nacyclicity. The proposed learning approach is not based on SAT/ASP, as previous\nsymbolic methods, but on a hitting set algorithm that can effectively handle\nproblems with millions of states, and pools with hundreds of thousands of\nfeatures. The formal properties of the approach are analyzed, and its\nscalability is tested on a number of benchmarks.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u91c7\u6837\u8ba1\u5212\u6cdb\u5316\u7684\u7b26\u53f7\u5316\u7b56\u7565\u5b66\u4e60\u65b9\u6cd5\uff0c\u4f7f\u7528\u547d\u4e2d\u96c6\u7b97\u6cd5\u66ff\u4ee3\u4f20\u7edfSAT/ASP\u65b9\u6cd5\uff0c\u53ef\u5904\u7406\u767e\u4e07\u7ea7\u72b6\u6001\u548c\u6570\u5341\u4e07\u7279\u5f81\u7684\u5927\u89c4\u6a21\u95ee\u9898", "motivation": "\u73b0\u6709\u7ec4\u5408\u5b66\u4e60\u65b9\u6cd5\u867d\u7136\u53ef\u751f\u6210\u53ef\u7406\u89e3\u4e14\u6b63\u786e\u7684\u7b56\u7565\uff0c\u4f46\u65e0\u6cd5\u6269\u5c55\u5230\u5927\u89c4\u6a21\u95ee\u9898\uff0c\u53ea\u80fd\u5904\u7406\u5c0f\u89c4\u6a21\u8bad\u7ec3\u5b9e\u4f8b\u548c\u7279\u5f81\u6c60", "method": "\u57fa\u4e8e\u91c7\u6837\u8ba1\u5212\u6cdb\u5316\u7684\u7b26\u53f7\u5316\u65b9\u6cd5\uff0c\u4f7f\u7528\u547d\u4e2d\u96c6\u7b97\u6cd5\u786e\u4fdd\u7ed3\u6784\u7ec8\u6b62\u548c\u65e0\u73af\u6027\uff0c\u66ff\u4ee3\u4f20\u7edf\u7684SAT/ASP\u65b9\u6cd5", "result": "\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u5904\u7406\u5305\u542b\u6570\u767e\u4e07\u72b6\u6001\u548c\u6570\u5341\u4e07\u7279\u5f81\u7684\u5927\u89c4\u6a21\u95ee\u9898\uff0c\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5c55\u73b0\u51fa\u826f\u597d\u7684\u53ef\u6269\u5c55\u6027", "conclusion": "\u8be5\u65b9\u6cd5\u89e3\u51b3\u4e86\u7b26\u53f7\u5316\u7b56\u7565\u5b66\u4e60\u65b9\u6cd5\u7684\u53ef\u6269\u5c55\u6027\u95ee\u9898\uff0c\u4e3a\u5927\u89c4\u6a21\u89c4\u5212\u95ee\u9898\u7684\u7b56\u7565\u5b66\u4e60\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848"}}
{"id": "2509.02922", "pdf": "https://arxiv.org/pdf/2509.02922", "abs": "https://arxiv.org/abs/2509.02922", "authors": ["Shahbaz P Qadri Syed", "He Bai"], "title": "Approximate constrained stochastic optimal control via parameterized input inference", "categories": ["eess.SY", "cs.MA", "cs.RO", "cs.SY", "math.OC"], "comment": null, "summary": "Approximate methods to solve stochastic optimal control (SOC) problems have\nreceived significant interest from researchers in the past decade.\nProbabilistic inference approaches to SOC have been developed to solve\nnonlinear quadratic Gaussian problems. In this work, we propose an\nExpectation-Maximization (EM) based inference procedure to generate\nstate-feedback controls for constrained SOC problems. We consider the\ninequality constraints for the state and controls and also the structural\nconstraints for the controls. We employ barrier functions to address state and\ncontrol constraints. We show that the expectation step leads to smoothing of\nthe state-control pair while the the maximization step on the non-zero subsets\nof the control parameters allows inference of structured stochastic optimal\ncontrollers. We demonstrate the effectiveness of the algorithm on unicycle\nobstacle avoidance, four-unicycle formation control, and quadcopter navigation\nin windy environment examples. In these examples, we perform an empirical study\non the parametric effect of barrier functions on the state constraint\nsatisfaction. We also present a comparative study of smoothing algorithms on\nthe performance of the proposed approach.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u671f\u671b\u6700\u5927\u5316(EM)\u7684\u63a8\u7406\u65b9\u6cd5\u89e3\u51b3\u5e26\u7ea6\u675f\u7684\u968f\u673a\u6700\u4f18\u63a7\u5236\u95ee\u9898\uff0c\u4f7f\u7528\u969c\u788d\u51fd\u6570\u5904\u7406\u72b6\u6001\u548c\u63a7\u5236\u7ea6\u675f\uff0c\u5728\u591a\u79cd\u673a\u5668\u4eba\u63a7\u5236\u573a\u666f\u4e2d\u9a8c\u8bc1\u6709\u6548\u6027", "motivation": "\u8fd1\u4f3c\u65b9\u6cd5\u89e3\u51b3\u968f\u673a\u6700\u4f18\u63a7\u5236\u95ee\u9898\u5728\u8fc7\u53bb\u5341\u5e74\u53d7\u5230\u5e7f\u6cdb\u5173\u6ce8\uff0c\u4f46\u73b0\u6709\u6982\u7387\u63a8\u7406\u65b9\u6cd5\u4e3b\u8981\u9488\u5bf9\u975e\u7ebf\u6027\u4e8c\u6b21\u9ad8\u65af\u95ee\u9898\uff0c\u9700\u8981\u5f00\u53d1\u80fd\u591f\u5904\u7406\u72b6\u6001\u548c\u63a7\u5236\u7ea6\u675f\u7684\u901a\u7528\u65b9\u6cd5", "method": "\u57fa\u4e8eEM\u6846\u67b6\u7684\u63a8\u7406\u8fc7\u7a0b\uff0c\u4f7f\u7528\u969c\u788d\u51fd\u6570\u5904\u7406\u72b6\u6001\u548c\u63a7\u5236\u7684\u4e0d\u7b49\u5f0f\u7ea6\u675f\uff0cE\u6b65\u5e73\u6ed1\u72b6\u6001-\u63a7\u5236\u5bf9\uff0cM\u6b65\u5728\u63a7\u5236\u53c2\u6570\u7684\u975e\u96f6\u5b50\u96c6\u4e0a\u63a8\u65ad\u7ed3\u6784\u5316\u968f\u673a\u6700\u4f18\u63a7\u5236\u5668", "result": "\u5728\u5355\u673a\u5668\u4eba\u907f\u969c\u3001\u591a\u673a\u5668\u4eba\u7f16\u961f\u63a7\u5236\u548c\u56db\u65cb\u7ffc\u98ce\u529b\u73af\u5883\u5bfc\u822a\u7b49\u793a\u4f8b\u4e2d\u9a8c\u8bc1\u4e86\u7b97\u6cd5\u6709\u6548\u6027\uff0c\u8fdb\u884c\u4e86\u969c\u788d\u51fd\u6570\u53c2\u6570\u5bf9\u72b6\u6001\u7ea6\u675f\u6ee1\u8db3\u7684\u5b9e\u8bc1\u7814\u7a76\u548c\u5e73\u6ed1\u7b97\u6cd5\u7684\u6bd4\u8f83\u7814\u7a76", "conclusion": "\u63d0\u51fa\u7684EM-based\u63a8\u7406\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u89e3\u51b3\u5e26\u7ea6\u675f\u7684\u968f\u673a\u6700\u4f18\u63a7\u5236\u95ee\u9898\uff0c\u969c\u788d\u51fd\u6570\u65b9\u6cd5\u6210\u529f\u5904\u7406\u4e86\u72b6\u6001\u548c\u63a7\u5236\u7ea6\u675f\uff0c\u5728\u591a\u79cd\u5b9e\u9645\u63a7\u5236\u573a\u666f\u4e2d\u8868\u73b0\u51fa\u826f\u597d\u6027\u80fd"}}
{"id": "2509.02622", "pdf": "https://arxiv.org/pdf/2509.02622", "abs": "https://arxiv.org/abs/2509.02622", "authors": ["Berger Cl\u00e9mentine", "Stamadiatis Paraskevas", "Badeau Roland", "Essid Slim"], "title": "IS${}^3$ : Generic Impulsive--Stationary Sound Separation in Acoustic Scenes using Deep Filtering", "categories": ["eess.AS", "cs.AI", "cs.SD", "eess.SP"], "comment": null, "summary": "We are interested in audio systems capable of performing a differentiated\nprocessing of stationary backgrounds and isolated acoustic events within an\nacoustic scene, whether for applying specific processing methods to each part\nor for focusing solely on one while ignoring the other. Such systems have\napplications in real-world scenarios, including robust adaptive audio rendering\nsystems (e.g., EQ or compression), plosive attenuation in voice mixing, noise\nsuppression or reduction, robust acoustic event classification or even\nbioacoustics. To this end, we introduce IS${}^3$, a neural network designed for\nImpulsive--Stationary Sound Separation, that isolates impulsive acoustic events\nfrom the stationary background using a deep filtering approach, that can act as\na pre-processing stage for the above-mentioned tasks. To ensure optimal\ntraining, we propose a sophisticated data generation pipeline that curates and\nadapts existing datasets for this task. We demonstrate that a learning-based\napproach, build on a relatively lightweight neural architecture and trained\nwith well-designed and varied data, is successful in this previously\nunaddressed task, outperforming the Harmonic--Percussive Sound Separation\nmasking method, adapted from music signal processing research, and wavelet\nfiltering on objective separation metrics.", "AI": {"tldr": "IS\u00b3\u795e\u7ecf\u7f51\u7edc\u7528\u4e8e\u5206\u79bb\u97f3\u9891\u4e2d\u7684\u8109\u51b2\u58f0\u548c\u7a33\u6001\u80cc\u666f\u58f0\uff0c\u5728\u591a\u79cd\u97f3\u9891\u5904\u7406\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5", "motivation": "\u5f00\u53d1\u80fd\u591f\u533a\u5206\u5904\u7406\u7a33\u6001\u80cc\u666f\u548c\u5b64\u7acb\u58f0\u5b66\u4e8b\u4ef6\u7684\u97f3\u9891\u7cfb\u7edf\uff0c\u7528\u4e8e\u81ea\u9002\u5e94\u97f3\u9891\u6e32\u67d3\u3001\u566a\u58f0\u6291\u5236\u3001\u58f0\u5b66\u4e8b\u4ef6\u5206\u7c7b\u7b49\u5b9e\u9645\u5e94\u7528\u573a\u666f", "method": "\u4f7f\u7528\u6df1\u5ea6\u6ee4\u6ce2\u65b9\u6cd5\u7684\u795e\u7ecf\u7f51\u7edcIS\u00b3\uff0c\u914d\u5408\u7cbe\u5fc3\u8bbe\u8ba1\u7684\u6570\u636e\u751f\u6210\u6d41\u7a0b\u6765\u8bad\u7ec3\u6a21\u578b\u8fdb\u884c\u8109\u51b2-\u7a33\u6001\u58f0\u97f3\u5206\u79bb", "result": "\u57fa\u4e8e\u76f8\u5bf9\u8f7b\u91cf\u7ea7\u795e\u7ecf\u7f51\u7edc\u67b6\u6784\u7684\u5b66\u4e60\u65b9\u6cd5\u5728\u8be5\u4efb\u52a1\u4e2d\u53d6\u5f97\u6210\u529f\uff0c\u5728\u5ba2\u89c2\u5206\u79bb\u6307\u6807\u4e0a\u4f18\u4e8e\u8c10\u6ce2-\u6253\u51fb\u4e50\u5206\u79bb\u63a9\u853d\u65b9\u6cd5\u548c\u5c0f\u6ce2\u6ee4\u6ce2", "conclusion": "\u5b66\u4e60\u578b\u65b9\u6cd5\u7ed3\u5408\u826f\u597d\u8bbe\u8ba1\u7684\u6570\u636e\u96c6\u80fd\u591f\u6709\u6548\u89e3\u51b3\u6b64\u524d\u672a\u89e3\u51b3\u7684\u8109\u51b2-\u7a33\u6001\u58f0\u97f3\u5206\u79bb\u95ee\u9898\uff0c\u4e3a\u5404\u79cd\u97f3\u9891\u5904\u7406\u5e94\u7528\u63d0\u4f9b\u6709\u6548\u7684\u9884\u5904\u7406\u9636\u6bb5"}}
{"id": "2509.02864", "pdf": "https://arxiv.org/pdf/2509.02864", "abs": "https://arxiv.org/abs/2509.02864", "authors": ["Kesen Wang", "Daulet Toibazar", "Pedro J. Moreno"], "title": "A-SEA3L-QA: A Fully Automated Self-Evolving, Adversarial Workflow for Arabic Long-Context Question-Answer Generation", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "We present an end-to-end, self-evolving adversarial workflow for long-context\nQuestion-Answer (QA) Generation in Arabic. By orchestrating multiple\nspecialized LVLMs: a question generator, an evaluator, and a swarm of answer\ngenerators, our system iteratively refines its own performance without any\nhuman intervention. Starting from raw, multi-page Arabic documents across\ndiverse domains, the question generator produces fine-grained, context-aware\nqueries to be tackled by the answer generator swarm, and the evaluator assesses\nand feeds back quality metrics. This closed-loop cycle enables continuous\nlearning: low-confidence outputs trigger automated re-generation and model\nupdates, progressively enhancing question difficulty and relevance. Moreover,\nwe set the quality metrics as a tunable hyperparameter, enabling question\ngeneration at controllable and customizable difficulty levels. We release\nAraLongBench, a large-scale Arabic benchmark of single- and multi-page\nchallenges spanning hundreds of pages, and demonstrate that our self-evolving\nworkflow substantially outperform static pipelines, markedly boosting the\nlong-context comprehension capabilities of leading Arabic Large Vision Language\nModels (LVLMs). Lastly, we also meticulously architect a fully automated\nagentic workflow for long-context Arabic document collection.", "AI": {"tldr": "\u4e00\u79cd\u9632\u5fa1\u6027\u81ea\u6210\u957f\u4e0a\u4e0b\u6587\u963f\u62c9\u4f2f\u8bed\u95ee\u7b54\u751f\u6210\u5de5\u4f5c\u6d41\uff0c\u901a\u8fc7\u591a\u4e2a\u4e13\u95e8\u7684\u5927\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u534f\u540c\u5de5\u4f5c\uff0c\u5b9e\u73b0\u65e0\u4eba\u5e72\u9884\u7684\u8fed\u4ee3\u7cbe\u70bc\u548c\u6301\u7eed\u5b66\u4e60\u3002", "motivation": "\u89e3\u51b3\u963f\u62c9\u4f2f\u8bed\u957f\u4e0a\u4e0b\u6587\u95ee\u7b54\u751f\u6210\u7684\u6311\u6218\uff0c\u63d0\u9ad8\u963f\u62c9\u4f2f\u8bed\u5927\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u957f\u6587\u6863\u7406\u89e3\u80fd\u529b\u3002", "method": "\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u7ed3\u6784\u5316\u7684\u5bf9\u6297\u6027\u5de5\u4f5c\u6d41\uff0c\u5305\u542b\u95ee\u9898\u751f\u6210\u5668\u3001\u8bc4\u4f30\u5668\u548c\u7b54\u6848\u751f\u6210\u5668\u7fa4\uff0c\u901a\u8fc7\u95ed\u73af\u5faa\u73af\u8fdb\u884c\u8fed\u4ee3\u7cbe\u70bc\u3002", "result": "\u7cfb\u7edf\u663e\u8457\u8d85\u8d8a\u4e86\u9759\u6001\u6d41\u6c34\u7ebf\uff0c\u5927\u5e45\u63d0\u5347\u4e86\u963f\u62c9\u4f2f\u8bedLVLM\u7684\u957f\u4e0a\u4e0b\u6587\u7406\u89e3\u80fd\u529b\uff0c\u5e76\u53d1\u5e03\u4e86AraLongBench\u5927\u89c4\u6a21\u6d4b\u8bd5\u96c6\u3002", "conclusion": "\u8be5\u81ea\u6210\u957f\u5de5\u4f5c\u6d41\u4e3a\u963f\u62c9\u4f2f\u8bed\u957f\u6587\u6863\u7406\u89e3\u5e26\u6765\u4e86\u91cd\u5927\u8fdb\u6b65\uff0c\u901a\u8fc7\u81ea\u52a8\u5316\u5bf9\u6297\u8fc7\u7a0b\u5b9e\u73b0\u4e86\u6301\u7eed\u6027\u80fd\u63d0\u5347\u3002"}}
{"id": "2509.02815", "pdf": "https://arxiv.org/pdf/2509.02815", "abs": "https://arxiv.org/abs/2509.02815", "authors": ["Nico Bohlinger", "Jan Peters"], "title": "Multi-Embodiment Locomotion at Scale with extreme Embodiment Randomization", "categories": ["cs.RO", "cs.LG"], "comment": null, "summary": "We present a single, general locomotion policy trained on a diverse\ncollection of 50 legged robots. By combining an improved embodiment-aware\narchitecture (URMAv2) with a performance-based curriculum for extreme\nEmbodiment Randomization, our policy learns to control millions of\nmorphological variations. Our policy achieves zero-shot transfer to unseen\nreal-world humanoid and quadruped robots.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u901a\u7528\u7684\u8fd0\u52a8\u63a7\u5236\u7b56\u7565\uff0c\u901a\u8fc7\u572850\u79cd\u4e0d\u540c\u817f\u5f0f\u673a\u5668\u4eba\u4e0a\u8fdb\u884c\u8bad\u7ec3\uff0c\u7ed3\u5408\u6539\u8fdb\u7684URMAv2\u67b6\u6784\u548c\u6027\u80fd\u5bfc\u5411\u8bfe\u7a0b\u5b66\u4e60\uff0c\u5b9e\u73b0\u4e86\u5bf9\u767e\u4e07\u7ea7\u5f62\u6001\u53d8\u5316\u7684\u63a7\u5236\uff0c\u5e76\u5728\u771f\u5b9e\u4eba\u5f62\u548c\u56db\u8db3\u673a\u5668\u4eba\u4e0a\u5b9e\u73b0\u96f6\u6837\u672c\u8fc1\u79fb\u3002", "motivation": "\u89e3\u51b3\u4f20\u7edf\u8fd0\u52a8\u63a7\u5236\u7b56\u7565\u5bf9\u7279\u5b9a\u673a\u5668\u4eba\u5f62\u6001\u7684\u4f9d\u8d56\u6027\uff0c\u5f00\u53d1\u80fd\u591f\u9002\u5e94\u591a\u79cd\u4e0d\u540c\u5f62\u6001\u817f\u5f0f\u673a\u5668\u4eba\u7684\u901a\u7528\u63a7\u5236\u65b9\u6cd5\u3002", "method": "\u4f7f\u7528\u6539\u8fdb\u7684URMAv2\uff08\u4f53\u73b0\u611f\u77e5\u67b6\u6784\uff09\u7ed3\u5408\u6027\u80fd\u5bfc\u5411\u7684\u6781\u7aef\u4f53\u73b0\u968f\u673a\u5316\u8bfe\u7a0b\u5b66\u4e60\uff0c\u572850\u79cd\u817f\u5f0f\u673a\u5668\u4eba\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u8bad\u7ec3\u3002", "result": "\u7b56\u7565\u5b66\u4f1a\u4e86\u63a7\u5236\u6570\u767e\u4e07\u79cd\u5f62\u6001\u53d8\u5316\uff0c\u5e76\u5728\u672a\u89c1\u8fc7\u7684\u771f\u5b9e\u4eba\u5f62\u548c\u56db\u8db3\u673a\u5668\u4eba\u4e0a\u5b9e\u73b0\u4e86\u96f6\u6837\u672c\u8fc1\u79fb\u6027\u80fd\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u8bc1\u660e\u4e86\u901a\u8fc7\u5927\u89c4\u6a21\u4f53\u73b0\u968f\u673a\u5316\u548c\u9002\u5f53\u7684\u67b6\u6784\u8bbe\u8ba1\uff0c\u53ef\u4ee5\u5f00\u53d1\u51fa\u5177\u6709\u5f3a\u5927\u6cdb\u5316\u80fd\u529b\u7684\u901a\u7528\u8fd0\u52a8\u63a7\u5236\u7b56\u7565\u3002"}}
{"id": "2509.02902", "pdf": "https://arxiv.org/pdf/2509.02902", "abs": "https://arxiv.org/abs/2509.02902", "authors": ["Muhammad Shahbaz", "Shaurya Agarwal"], "title": "LiGuard: A Streamlined Open-Source Framework for Rapid & Interactive Lidar Research", "categories": ["cs.CV"], "comment": null, "summary": "There is a growing interest in the development of lidar-based autonomous\nmobility and Intelligent Transportation Systems (ITS). To operate and research\non lidar data, researchers often develop code specific to application niche.\nThis approach leads to duplication of efforts across studies that, in many\ncases, share multiple methodological steps such as data input/output (I/O),\npre/post processing, and common algorithms in multi-stage solutions. Moreover,\nslight changes in data, algorithms, and/or research focus may force major\nrevisions in the code. To address these challenges, we present LiGuard, an\nopen-source software framework that allows researchers to: 1) rapidly develop\ncode for their lidar-based projects by providing built-in support for data I/O,\npre/post processing, and commonly used algorithms, 2) interactively\nadd/remove/reorder custom algorithms and adjust their parameters, and 3)\nvisualize results for classification, detection, segmentation, and tracking\ntasks. Moreover, because it creates all the code files in structured\ndirectories, it allows easy sharing of entire projects or even the individual\ncomponents to be reused by other researchers. The effectiveness of LiGuard is\ndemonstrated via case studies.", "AI": {"tldr": "LiGuard\u662f\u4e00\u4e2a\u5f00\u6e90\u7684\u6fc0\u5149\u96f7\u8fbe\u6570\u636e\u5904\u7406\u6846\u67b6\uff0c\u65e8\u5728\u89e3\u51b3\u7814\u7a76\u4e2d\u7684\u4ee3\u7801\u91cd\u590d\u95ee\u9898\uff0c\u63d0\u4f9b\u6570\u636eI/O\u3001\u9884\u5904\u7406/\u540e\u5904\u7406\u548c\u5e38\u7528\u7b97\u6cd5\u7684\u5185\u7f6e\u652f\u6301\uff0c\u652f\u6301\u5feb\u901f\u5f00\u53d1\u548c\u7ed3\u679c\u53ef\u89c6\u5316\u3002", "motivation": "\u6fc0\u5149\u96f7\u8fbe\u81ea\u4e3b\u79fb\u52a8\u548c\u667a\u80fd\u4ea4\u901a\u7cfb\u7edf\u7814\u7a76\u4e2d\u5b58\u5728\u4ee3\u7801\u91cd\u590d\u5f00\u53d1\u95ee\u9898\uff0c\u4e0d\u540c\u7814\u7a76\u5171\u4eab\u591a\u4e2a\u65b9\u6cd5\u6b65\u9aa4\u4f46\u9700\u8981\u91cd\u590d\u5b9e\u73b0\uff0c\u6570\u636e\u6216\u7b97\u6cd5\u7684\u5c0f\u53d8\u5316\u53ef\u80fd\u5bfc\u81f4\u4ee3\u7801\u5927\u4fee\u6539\u3002", "method": "\u5f00\u53d1LiGuard\u5f00\u6e90\u8f6f\u4ef6\u6846\u67b6\uff0c\u63d0\u4f9b\u6570\u636eI/O\u3001\u9884\u5904\u7406/\u540e\u5904\u7406\u3001\u5e38\u7528\u7b97\u6cd5\u7684\u5185\u7f6e\u652f\u6301\uff0c\u652f\u6301\u4ea4\u4e92\u5f0f\u7b97\u6cd5\u6dfb\u52a0/\u5220\u9664/\u91cd\u6392\u5e8f\u548c\u53c2\u6570\u8c03\u6574\uff0c\u4ee5\u53ca\u5206\u7c7b\u3001\u68c0\u6d4b\u3001\u5206\u5272\u548c\u8ddf\u8e2a\u4efb\u52a1\u7684\u7ed3\u679c\u53ef\u89c6\u5316\u3002", "result": "\u901a\u8fc7\u6848\u4f8b\u7814\u7a76\u8bc1\u660e\u4e86LiGuard\u7684\u6709\u6548\u6027\uff0c\u6846\u67b6\u80fd\u591f\u751f\u6210\u7ed3\u6784\u5316\u76ee\u5f55\u7684\u4ee3\u7801\u6587\u4ef6\uff0c\u4fbf\u4e8e\u9879\u76ee\u6574\u4f53\u6216\u5355\u4e2a\u7ec4\u4ef6\u7684\u5171\u4eab\u548c\u91cd\u7528\u3002", "conclusion": "LiGuard\u6846\u67b6\u6210\u529f\u89e3\u51b3\u4e86\u6fc0\u5149\u96f7\u8fbe\u7814\u7a76\u4e2d\u4ee3\u7801\u91cd\u590d\u5f00\u53d1\u7684\u95ee\u9898\uff0c\u63d0\u9ad8\u4e86\u7814\u7a76\u6548\u7387\uff0c\u4fc3\u8fdb\u4e86\u4ee3\u7801\u5171\u4eab\u548c\u91cd\u7528\u3002"}}
{"id": "2509.02737", "pdf": "https://arxiv.org/pdf/2509.02737", "abs": "https://arxiv.org/abs/2509.02737", "authors": ["Zhongzhu Zhou", "Yibo Yang", "Ziyan Chen", "Fengxiang Bie", "Haojun Xia", "Xiaoxia Wu", "Robert Wu", "Ben Athiwaratkun", "Bernard Ghanem", "Shuaiwen Leon Song"], "title": "Imitate Optimal Policy: Prevail and Induce Action Collapse in Policy Gradient", "categories": ["cs.LG"], "comment": "18 pages, 4 figures, 2 tables; includes supplementary material;\n  preprint", "summary": "Policy gradient (PG) methods in reinforcement learning frequently utilize\ndeep neural networks (DNNs) to learn a shared backbone of feature\nrepresentations used to compute likelihoods in an action selection layer.\nNumerous studies have been conducted on the convergence and global optima of\npolicy networks, but few have analyzed representational structures of those\nunderlying networks. While training an optimal policy DNN, we observed that\nunder certain constraints, a gentle structure resembling neural collapse, which\nwe refer to as Action Collapse (AC), emerges. This suggests that 1) the\nstate-action activations (i.e. last-layer features) sharing the same optimal\nactions collapse towards those optimal actions respective mean activations; 2)\nthe variability of activations sharing the same optimal actions converges to\nzero; 3) the weights of action selection layer and the mean activations\ncollapse to a simplex equiangular tight frame (ETF). Our early work showed\nthose aforementioned constraints to be necessary for these observations. Since\nthe collapsed ETF of optimal policy DNNs maximally separates the pair-wise\nangles of all actions in the state-action space, we naturally raise a question:\ncan we learn an optimal policy using an ETF structure as a (fixed) target\nconfiguration in the action selection layer? Our analytical proof shows that\nlearning activations with a fixed ETF as action selection layer naturally leads\nto the AC. We thus propose the Action Collapse Policy Gradient (ACPG) method,\nwhich accordingly affixes a synthetic ETF as our action selection layer. ACPG\ninduces the policy DNN to produce such an ideal configuration in the action\nselection layer while remaining optimal. Our experiments across various OpenAI\nGym environments demonstrate that our technique can be integrated into any\ndiscrete PG methods and lead to favorable reward improvements more quickly and\nrobustly.", "AI": {"tldr": "\u6539\u8fdb\u7684\u7b56\u7565\u68af\u5ea6\u65b9\u6cd5ACPG\uff0c\u901a\u8fc7\u56fa\u5b9aETF\u7ed3\u6784\u4f5c\u4e3a\u52a8\u4f5c\u9009\u62e9\u5c42\u76ee\u6807\uff0c\u4fc3\u8fdb\u52a8\u4f5c\u51b2\u51fb\u73b0\u8c61\u7684\u53d1\u751f\uff0c\u63d0\u5347\u653b\u51fb\u6027\u80fd\u548c\u7a33\u5b9a\u6027", "motivation": "\u89c2\u5bdf\u5230\u5728\u67d0\u4e9b\u7ea6\u675f\u6761\u4ef6\u4e0b\uff0c\u4f18\u5316\u7b56\u7565\u795e\u7ecf\u7f51\u7edc\u4f1a\u51fa\u73b0\u7c7b\u4f3c\u795e\u7ecf\u51b2\u51fb\u7684\"\u52a8\u4f5c\u51b2\u51fb\"\u73b0\u8c61\uff0c\u8fd9\u79cd\u7ed3\u6784\u80fd\u6700\u5927\u5316\u5206\u79bb\u4e0d\u540c\u52a8\u4f5c\u95f4\u7684\u89d2\u5ea6", "method": "\u63d0\u51faACPG\u65b9\u6cd5\uff0c\u5728\u52a8\u4f5c\u9009\u62e9\u5c42\u4e2d\u56fa\u5b9a\u4e00\u4e2a\u5408\u6210\u7684\u7b80\u5355\u7b49\u89d2\u7d27\u51d1\u6846\u67b6\uff08ETF\uff09\u4f5c\u4e3a\u76ee\u6807\u914d\u7f6e\uff0c\u4fc3\u4f7f\u7b56\u7565\u7f51\u7edc\u4ea7\u751f\u7406\u60f3\u7684\u914d\u7f6e", "result": "\u5728\u591a\u4e2aOpenAI Gym\u73af\u5883\u4e2d\u8bc1\u660e\uff0cACPG\u65b9\u6cd5\u53ef\u4ee5\u96c6\u6210\u5230\u4efb\u4f55\u79bb\u6563\u7b56\u7565\u68af\u5ea6\u65b9\u6cd5\u4e2d\uff0c\u66f4\u5feb\u901f\u3001\u66f4\u7a33\u5065\u5730\u63d0\u5347\u5956\u52b1\u6027\u80fd", "conclusion": "\u901a\u8fc7\u56fa\u5b9aETF\u7ed3\u6784\u4f5c\u4e3a\u52a8\u4f5c\u9009\u62e9\u5c42\u7684\u76ee\u6807\u914d\u7f6e\uff0c\u53ef\u4ee5\u6709\u6548\u5730\u4fc3\u8fdb\u52a8\u4f5c\u51b2\u51fb\u73b0\u8c61\u7684\u53d1\u751f\uff0c\u4ece\u800c\u5b66\u4e60\u5230\u4f18\u5316\u7b56\u7565\u4e14\u4fdd\u6301\u7406\u60f3\u7684\u7a7a\u95f4\u914d\u7f6e"}}
