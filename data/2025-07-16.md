<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 36]
- [cs.CL](#cs.CL) [Total: 47]
- [cs.CV](#cs.CV) [Total: 65]
- [cs.LG](#cs.LG) [Total: 83]
- [cs.MA](#cs.MA) [Total: 1]
- [cs.MM](#cs.MM) [Total: 1]
- [cs.RO](#cs.RO) [Total: 34]
- [cs.SD](#cs.SD) [Total: 5]
- [cs.IR](#cs.IR) [Total: 2]
- [eess.SY](#eess.SY) [Total: 2]
- [cs.DB](#cs.DB) [Total: 2]
- [cs.NE](#cs.NE) [Total: 5]
- [cs.SE](#cs.SE) [Total: 9]
- [cs.SI](#cs.SI) [Total: 2]
- [eess.IV](#eess.IV) [Total: 6]
- [q-fin.TR](#q-fin.TR) [Total: 1]
- [cs.DC](#cs.DC) [Total: 1]
- [cs.CR](#cs.CR) [Total: 7]
- [cs.HC](#cs.HC) [Total: 3]
- [cs.CE](#cs.CE) [Total: 1]
- [cs.AR](#cs.AR) [Total: 3]
- [quant-ph](#quant-ph) [Total: 2]
- [cs.DS](#cs.DS) [Total: 1]
- [cs.GR](#cs.GR) [Total: 1]
- [cs.GT](#cs.GT) [Total: 3]
- [physics.app-ph](#physics.app-ph) [Total: 1]
- [math.PR](#math.PR) [Total: 1]
- [cond-mat.str-el](#cond-mat.str-el) [Total: 1]
- [physics.geo-ph](#physics.geo-ph) [Total: 1]
- [econ.GN](#econ.GN) [Total: 1]
- [physics.chem-ph](#physics.chem-ph) [Total: 1]
- [math-ph](#math-ph) [Total: 1]
- [q-bio.QM](#q-bio.QM) [Total: 1]
- [physics.med-ph](#physics.med-ph) [Total: 1]
- [cs.NI](#cs.NI) [Total: 2]
- [math.OC](#math.OC) [Total: 3]
- [eess.AS](#eess.AS) [Total: 1]
- [gr-qc](#gr-qc) [Total: 1]
- [stat.ML](#stat.ML) [Total: 8]
- [cs.CY](#cs.CY) [Total: 5]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [SAMEP: A Secure Protocol for Persistent Context Sharing Across AI Agents](https://arxiv.org/abs/2507.10562)
*Hari Masoor*

Main category: cs.AI

TL;DR: SAMEP协议解决了AI代理间记忆共享的持久性、安全性和语义搜索问题，显著提升了协作效率和合规性。


<details>
  <summary>Details</summary>
Motivation: 当前AI代理架构存在记忆短暂性问题，限制了跨会话和代理的知识共享与协作。

Method: 提出了SAMEP框架，包括分布式记忆存储、向量语义搜索、加密访问控制和标准化API。

Result: 实验显示减少了73%冗余计算，提升了89%上下文相关性，并完全符合监管要求。

Conclusion: SAMEP为持久、协作的AI代理生态系统提供了安全、隐私保障的新范式。

Abstract: Current AI agent architectures suffer from ephemeral memory limitations,
preventing effective collaboration and knowledge sharing across sessions and
agent boundaries. We introduce SAMEP (Secure Agent Memory Exchange Protocol), a
novel framework that enables persistent, secure, and semantically searchable
memory sharing among AI agents. Our protocol addresses three critical
challenges: (1) persistent context preservation across agent sessions, (2)
secure multi-agent collaboration with fine-grained access control, and (3)
efficient semantic discovery of relevant historical context. SAMEP implements a
distributed memory repository with vector-based semantic search, cryptographic
access controls (AES-256-GCM), and standardized APIs compatible with existing
agent communication protocols (MCP, A2A). We demonstrate SAMEP's effectiveness
across diverse domains including multi-agent software development, healthcare
AI with HIPAA compliance, and multi-modal processing pipelines. Experimental
results show 73% reduction in redundant computations, 89% improvement in
context relevance scores, and complete compliance with regulatory requirements
including audit trail generation. SAMEP enables a new paradigm of persistent,
collaborative AI agent ecosystems while maintaining security and privacy
guarantees.

</details>


### [2] [AI Mother Tongue: Self-Emergent Communication in MARL via Endogenous Symbol Systems](https://arxiv.org/abs/2507.10566)
*Hung Ming Liu*

Main category: cs.AI

TL;DR: 论文提出了一种基于VQ-VAE的AIM框架，证明无需外部归纳偏置，多智能体可通过内生符号系统实现自然语义压缩与纳什均衡驱动的语义收敛，从而达成有效通信。


<details>
  <summary>Details</summary>
Motivation: 传统方法通过引入归纳偏置解决多智能体强化学习中的通信真空均衡问题，但作者质疑这种人为偏置是否过度设计。

Method: 采用基于VQ-VAE的AIM框架，研究智能体在内生符号系统下的通信行为。

Result: 实验表明，内生符号系统能自发实现语义压缩与收敛，且通信效率优于传统方法。研究还提出了三个理论洞见。

Conclusion: AIM框架为连接符号主义与连接主义提供了新思路，未来将探索HQ-VAE增强表达能力的潜力。

Abstract: In Decentralized Multi-Agent Reinforcement Learning (MARL), the development
of Emergent Communication has long been constrained by the ``Joint Exploration
Dilemma'', leading agents to fall into a ``Communication Vacuum Equilibrium'' .
Traditional methods address this by introducing inductive biases to facilitate
communication emergence . This study fundamentally questions whether such
artificial inductive biases are, in fact, over-engineering. Through experiments
with the ``AI Mother Tongue'' (AIM) framework, based on a Vector Quantized
Variational Autoencoder (VQ-VAE), we demonstrate that when agents possess an
endogenous symbol system, their neural representations naturally exhibit
spontaneous semantic compression and Nash equilibrium-driven semantic
convergence, achieving effective symbolic communication without external
inductive biases. This aligns with recent neuroscience findings suggesting that
the human brain does not directly use human language for internal thought , and
resonates with research on ``soft thinking'' capabilities in Large Language
Models (LLMs) . Compared to traditional explicit communication methods, AIM
demonstrates stronger generality and efficiency. The interpretable analysis
toolkit developed in this study confirms that symbol usage exhibits a
significant power-law distribution, leading to three major theoretical
insights: the ``Neural Communication Hypothesis'', the ``Tool-First
Principle'', and the ``Semantic Interpretability Paradigm''. Future research
will explore the integration of Hierarchical Quantized Variational Autoencoders
(HQ-VAE) to enhance AIM's complex expressive capabilities and investigate the
potential for ``Reinforcement Learning (RL) Low-Level Pre-training''. This
discovery offers new avenues for bridging symbolism and connectionism.

</details>


### [3] [From Semantic Web and MAS to Agentic AI: A Unified Narrative of the Web of Agents](https://arxiv.org/abs/2507.10644)
*Tatiana Petrova,Aleksandr Puzikov,Boris Bliznukov,Radu State*

Main category: cs.AI

TL;DR: 本文提出了一个全面的Web of Agents（WoA）进化概述，揭示了从早期标准到现代协议的演变，并引入了一个四轴分类法以统一分析不同世代的代理架构。


<details>
  <summary>Details</summary>
Motivation: 研究WoA领域的碎片化问题，整合多代理系统（MAS）和语义Web的历史，以全面理解该领域的发展轨迹。

Method: 引入四轴分类法（语义基础、通信范式、智能中心、发现机制），系统比较不同世代的代理架构。

Result: 揭示了智能中心的范式转变：从外部数据或平台转移到代理核心模型（LLM），为现代Agentic AI奠定了基础。

Conclusion: 新协议虽重要，但不足以构建强大的开放生态系统，未来研究应关注去中心化身份、经济模型、安全与治理等社会技术挑战。

Abstract: The concept of the Web of Agents (WoA), which transforms the static,
document-centric Web into an environment of autonomous agents acting on users'
behalf, has attracted growing interest as large language models (LLMs) become
more capable. However, research in this area is still fragmented across
different communities. Contemporary surveys catalog the latest LLM-powered
frameworks, while the rich histories of Multi-Agent Systems (MAS) and the
Semantic Web are often treated as separate, legacy domains. This fragmentation
obscures the intellectual lineage of modern systems and hinders a holistic
understanding of the field's trajectory. We present the first comprehensive
evolutionary overview of the WoA. We show that modern protocols like A2A and
the MCP, are direct evolutionary responses to the well-documented limitations
of earlier standards like FIPA standards and OWL-based semantic agents. To
systematize this analysis, we introduce a four-axis taxonomy (semantic
foundation, communication paradigm, locus of intelligence, discovery
mechanism). This framework provides a unified analytical lens for comparing
agent architectures across all generations, revealing a clear line of descent
where others have seen a disconnect. Our analysis identifies a paradigm shift
in the 'locus of intelligence': from being encoded in external data (Semantic
Web) or the platform (MAS) to being embedded within the agent's core model
(LLM). This shift is foundational to modern Agentic AI, enabling the scalable
and adaptive systems the WoA has long envisioned. We conclude that while new
protocols are essential, they are insufficient for building a robust, open,
trustworthy ecosystem. Finally, we argue that the next research frontier lies
in solving persistent socio-technical challenges, and we map out a new agenda
focused on decentralized identity, economic models, security, and governance
for the emerging WoA.

</details>


### [4] [Orchestrator-Agent Trust: A Modular Agentic AI Visual Classification System with Trust-Aware Orchestration and RAG-Based Reasoning](https://arxiv.org/abs/2507.10571)
*Konstantinos I. Roumeliotis,Ranjan Sapkota,Manoj Karkee,Nikolaos D. Tselikas*

Main category: cs.AI

TL;DR: 论文提出了一种模块化的多智能体AI视觉分类框架，通过信任感知的协调和RAG模块，在零样本设置中显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 解决多智能体AI在零样本设置中的信任问题，尤其是在视觉和语言理解结合的领域。

Method: 结合通用多模态智能体、非视觉推理协调器和RAG模块，通过信任校准指标（ECE, OCR, CCC）协调智能体。

Result: 零样本设置下准确率提升77.94%，总体达到85.63%；GPT-4o校准效果更好，Qwen-2.5-VL表现过度自信。

Conclusion: 系统分离感知与元推理，可扩展至诊断、生物学等信任关键领域，代码开源以支持复现和社区基准测试。

Abstract: Modern Artificial Intelligence (AI) increasingly relies on multi-agent
architectures that blend visual and language understanding. Yet, a pressing
challenge remains: How can we trust these agents especially in zero-shot
settings with no fine-tuning? We introduce a novel modular Agentic AI visual
classification framework that integrates generalist multimodal agents with a
non-visual reasoning orchestrator and a Retrieval-Augmented Generation (RAG)
module. Applied to apple leaf disease diagnosis, we benchmark three
configurations: (I) zero-shot with confidence-based orchestration, (II)
fine-tuned agents with improved performance, and (III) trust-calibrated
orchestration enhanced by CLIP-based image retrieval and re-evaluation loops.
Using confidence calibration metrics (ECE, OCR, CCC), the orchestrator
modulates trust across agents. Our results demonstrate a 77.94\% accuracy
improvement in the zero-shot setting using trust-aware orchestration and RAG,
achieving 85.63\% overall. GPT-4o showed better calibration, while Qwen-2.5-VL
displayed overconfidence. Furthermore, image-RAG grounded predictions with
visually similar cases, enabling correction of agent overconfidence via
iterative re-evaluation. The proposed system separates perception (vision
agents) from meta-reasoning (orchestrator), enabling scalable and interpretable
multi-agent AI. This blueprint is extensible to diagnostics, biology, and other
trust-critical domains. All models, prompts, results, and system components
including the complete software source code are openly released to support
reproducibility, transparency, and community benchmarking at Github:
https://github.com/Applied-AI-Research-Lab/Orchestrator-Agent-Trust

</details>


### [5] [Taming Uncertainty via Automation: Observing, Analyzing, and Optimizing Agentic AI Systems](https://arxiv.org/abs/2507.11277)
*Dany Moshkovich,Sergey Zeltyn*

Main category: cs.AI

TL;DR: 论文提出了AgentOps框架，用于观察、分析和优化基于大型语言模型（LLM）的智能代理系统，解决其独特的不确定性问题。


<details>
  <summary>Details</summary>
Motivation: 智能代理系统因概率推理、动态记忆和灵活执行路径引入不确定性，传统软件运维方法难以应对。

Method: 提出AgentOps框架，包括六阶段自动化流程：行为观察、指标收集、问题检测、根因分析、优化建议和运行时自动化。

Result: 框架支持开发者、测试者、SRE和业务用户在不同生命周期阶段的需求，通过自动化管理不确定性。

Conclusion: AgentOps通过驯服而非消除不确定性，确保智能代理系统的安全、自适应和高效运行。

Abstract: Large Language Models (LLMs) are increasingly deployed within agentic
systems-collections of interacting, LLM-powered agents that execute complex,
adaptive workflows using memory, tools, and dynamic planning. While enabling
powerful new capabilities, these systems also introduce unique forms of
uncertainty stemming from probabilistic reasoning, evolving memory states, and
fluid execution paths. Traditional software observability and operations
practices fall short in addressing these challenges.
  This paper introduces AgentOps: a comprehensive framework for observing,
analyzing, optimizing, and automating operation of agentic AI systems. We
identify distinct needs across four key roles-developers, testers, site
reliability engineers (SREs), and business users-each of whom engages with the
system at different points in its lifecycle. We present the AgentOps Automation
Pipeline, a six-stage process encompassing behavior observation, metric
collection, issue detection, root cause analysis, optimized recommendations,
and runtime automation. Throughout, we emphasize the critical role of
automation in managing uncertainty and enabling self-improving AI systems-not
by eliminating uncertainty, but by taming it to ensure safe, adaptive, and
effective operation.

</details>


### [6] [Comprehension Without Competence: Architectural Limits of LLMs in Symbolic Computation and Reasoning](https://arxiv.org/abs/2507.10624)
*Zheng Zhang*

Main category: cs.AI

TL;DR: LLMs表面流畅但符号推理、算术准确性和逻辑一致性任务表现不佳，研究发现其存在理解与能力之间的鸿沟，称为“计算分裂脑综合征”。


<details>
  <summary>Details</summary>
Motivation: 揭示LLMs在任务失败背后的结构性原因，探讨其理解与能力之间的不一致性。

Method: 通过控制实验和架构分析，研究LLMs在原则表达与应用上的差异。

Result: 发现LLMs能表达正确原则但无法可靠应用，根源在于计算执行而非知识获取。

Conclusion: LLMs缺乏结构化推理能力，未来需改进模型架构以支持原则性推理。

Abstract: Large Language Models (LLMs) display striking surface fluency yet
systematically fail at tasks requiring symbolic reasoning, arithmetic accuracy,
and logical consistency. This paper offers a structural diagnosis of such
failures, revealing a persistent gap between \textit{comprehension} and
\textit{competence}. Through controlled experiments and architectural analysis,
we demonstrate that LLMs often articulate correct principles without reliably
applying them--a failure rooted not in knowledge access, but in computational
execution. We term this phenomenon the computational \textit{split-brain
syndrome}, where instruction and action pathways are geometrically and
functionally dissociated. This core limitation recurs across domains, from
mathematical operations to relational inferences, and explains why model
behavior remains brittle even under idealized prompting. We argue that LLMs
function as powerful pattern completion engines, but lack the architectural
scaffolding for principled, compositional reasoning. Our findings delineate the
boundary of current LLM capabilities and motivate future models with
metacognitive control, principle lifting, and structurally grounded execution.
This diagnosis also clarifies why mechanistic interpretability findings may
reflect training-specific pattern coordination rather than universal
computational principles, and why the geometric separation between instruction
and execution pathways suggests limitations in neural introspection and
mechanistic analysis.

</details>


### [7] [Enhancing the Capabilities of Large Language Models for API calls through Knowledge Graphs](https://arxiv.org/abs/2507.10630)
*Ye Yang,Xue Xiao,Ping Yin,Taotao Xie*

Main category: cs.AI

TL;DR: KG2data结合知识图谱、LLM、ReAct代理和工具使用技术，提升气象领域数据查询的准确性和智能性。


<details>
  <summary>Details</summary>
Motivation: 解决LLM在知识密集型领域（如气象学）中API调用能力不足的问题。

Method: 通过知识图谱作为持久内存，整合LLM和工具技术，评估API调用的准确性。

Result: KG2data在名称识别、幻觉和调用正确性上表现优于RAG2data和chat2data。

Conclusion: KG2data为高知识需求领域提供了智能问答和数据分析的新方案。

Abstract: API calls by large language models (LLMs) offer a cutting-edge approach for
data analysis. However, their ability to effectively utilize tools via API
calls remains underexplored in knowledge-intensive domains like meteorology.
This paper introduces KG2data, a system that integrates knowledge graphs, LLMs,
ReAct agents, and tool-use technologies to enable intelligent data acquisition
and query handling in the meteorological field. Using a virtual API, we
evaluate API call accuracy across three metrics: name recognition failure,
hallucination failure, and call correctness. KG2data achieves superior
performance (1.43%, 0%, 88.57%) compared to RAG2data (16%, 10%, 72.14%) and
chat2data (7.14%, 8.57%, 71.43%). KG2data differs from typical LLM-based
systems by addressing their limited access to domain-specific knowledge, which
hampers performance on complex or terminology-rich queries. By using a
knowledge graph as persistent memory, our system enhances content retrieval,
complex query handling, domain-specific reasoning, semantic relationship
resolution, and heterogeneous data integration. It also mitigates the high cost
of fine-tuning LLMs, making the system more adaptable to evolving domain
knowledge and API structures. In summary, KG2data provides a novel solution for
intelligent, knowledge-based question answering and data analysis in domains
with high knowledge demands.

</details>


### [8] [Parsing Musical Structure to Enable Meaningful Variations](https://arxiv.org/abs/2507.10740)
*Maziar Kanani,Sean O Leary,James McDermott*

Main category: cs.AI

TL;DR: 提出一种基于规则的音乐生成方法，通过变异现有曲调的语法结构生成新曲调，分析变异对曲调的影响。


<details>
  <summary>Details</summary>
Motivation: 探索如何通过变异现有曲调的语法结构生成新曲调，并研究变异对曲调特性的影响。

Method: 使用Sequitur算法解析曲调为语法结构（PA），随机应用19种变异类型（如添加、删除、交换等），再扩展语法生成新曲调。

Result: 通过编辑距离、结构复杂性和长度等指标分析变异效果，并评估每种变异类型的影响。

Conclusion: 该方法能有效生成与原曲调相关的新曲调，但仅关注音高序列生成，未涉及其他音乐要素。

Abstract: This paper presents a novel rule-based approach for generating music by
varying existing tunes. We parse each tune to find the Pathway Assembly (PA) [
1], that is a structure representing all repetitions in the tune. The Sequitur
algorithm [2 ] is used for this. The result is a grammar. We then carry out
mutation on the grammar, rather than on a tune directly. There are potentially
19 types of mutations such as adding, removing, swapping or reversing parts of
the grammar that can be applied to the grammars. The system employs one of the
mutations randomly in this step to automatically manipulate the grammar.
Following the mutation, we need to expand the grammar which returns a new tune.
The output after 1 or more mutations will be a new tune related to the original
tune. Our study examines how tunes change gradually over the course of multiple
mutations. Edit distances, structural complexity and length of the tunes are
used to show how a tune is changed after multiple mutations. In addition, the
size of effect of each mutation type is analyzed. As a final point, we review
the musical aspect of the output tunes. It should be noted that the study only
focused on generating new pitch sequences. The study is based on an Irish
traditional tune dataset and a list of integers has been used to represent each
tune's pitch values.

</details>


### [9] [AI and the Net-Zero Journey: Energy Demand, Emissions, and the Potential for Transition](https://arxiv.org/abs/2507.10750)
*Pandu Devarakota,Nicolas Tsesmetzis,Faruk O. Alpak,Apurva Gala,Detlef Hohl*

Main category: cs.AI

TL;DR: 本文探讨了AI数据中心的能源消耗及其对温室气体排放的影响，分析了短期（2030年前）和长期（2035年后）的情景，并评估AI对CO2排放的净影响。


<details>
  <summary>Details</summary>
Motivation: 随着AI在各行业的广泛应用，数据中心的能源需求和碳排放问题日益突出，研究旨在评估AI对环境的短期和长期影响。

Method: 通过分析数据中心的能源消耗和AI模型的运行需求，结合行业发展趋势，预测短期和长期的碳排放情景。

Result: 短期内AI需求增长可能导致碳排放增加，但长期来看，AI通过优化能源生产和物流等流程，有望显著减少碳足迹。

Conclusion: AI初期可能对环境造成压力，但长期具备支持气候缓解的潜力，其正面影响可能超过初期排放增长。

Abstract: Thanks to the availability of massive amounts of data, computing resources,
and advanced algorithms, AI has entered nearly every sector. This has sparked
significant investment and interest, particularly in building data centers with
the necessary hardware and software to develop and operate AI models and
AI-based workflows. In this technical review article, we present energy
consumption scenarios of data centers and impact on GHG emissions, considering
both near-term projections (up to 2030) and long-term outlook (2035 and
beyond). We address the quintessential question of whether AI will have a net
positive, neutral, or negative impact on CO2 emissions by 2035. Additionally,
we discuss AI's potential to automate, create efficient and disruptive
workflows across various fields related to energy production, supply and
consumption. In the near-term scenario, the growing demand for AI will likely
strain computing resources, lead to increase in electricity consumption and
therefore associated CO2 emissions. This is due to the power-hungry nature of
big data centers and the requirements for training and running of large and
complex AI models, as well as the penetration of AI assistant search and
applications for public use. However, the long-term outlook could be more
promising. AI has the potential to be a game-changer in CO2 reduction. Its
ability to further automate and optimize processes across industries, from
energy production to logistics, could significantly decrease our carbon
footprint. This positive impact is anticipated to outweigh the initial
emissions bump, creating value for businesses and society in areas where
traditional solutions have fallen short. In essence, AI might cause some
initial growing pains for the environment, but it has the potential to support
climate mitigation efforts.

</details>


### [10] [IoT Malware Network Traffic Detection using Deep Learning and GraphSAGE Models](https://arxiv.org/abs/2507.10758)
*Nikesh Prajapati,Bimal Karki,Saroj Gopali,Akbar Siami Namin*

Main category: cs.AI

TL;DR: 该论文通过深度学习模型检测物联网恶意攻击，评估了多种模型在恶意网络流量检测中的表现，其中BERT模型表现最佳。


<details>
  <summary>Details</summary>
Motivation: 物联网系统流量模式具有时序性和多样性，为模型学习提供了丰富的数据，因此需要高效的方法来检测恶意攻击。

Method: 采用了GraphSAGE、BERT、TCN、Multi-Head Attention、BI-LSTM和LSTM等模型，以捕捉时序模式和特征重要性。

Result: BERT模型表现最优，准确率达99.94%，其他指标如精确率、召回率、F1分数和AUC-ROC均接近99.99%。Multi-Head Attention模型结果可解释但处理时间长，GraphSAGE训练时间最短但性能较低。

Conclusion: BERT模型在捕获时序依赖方面表现突出，适合物联网恶意攻击检测；Multi-Head Attention和GraphSAGE各有优缺点，可根据需求选择。

Abstract: This paper intends to detect IoT malicious attacks through deep learning
models and demonstrates a comprehensive evaluation of the deep learning and
graph-based models regarding malicious network traffic detection. The models
particularly are based on GraphSAGE, Bidirectional encoder representations from
transformers (BERT), Temporal Convolutional Network (TCN) as well as Multi-Head
Attention, together with Bidirectional Long Short-Term Memory (BI-LSTM)
Multi-Head Attention and BI-LSTM and LSTM models. The chosen models
demonstrated great performance to model temporal patterns and detect feature
significance. The observed performance are mainly due to the fact that IoT
system traffic patterns are both sequential and diverse, leaving a rich set of
temporal patterns for the models to learn. Experimental results showed that
BERT maintained the best performance. It achieved 99.94% accuracy rate
alongside high precision and recall, F1-score and AUC-ROC score of 99.99% which
demonstrates its capabilities through temporal dependency capture. The
Multi-Head Attention offered promising results by providing good detection
capabilities with interpretable results. On the other side, the Multi-Head
Attention model required significant processing time like BI-LSTM variants. The
GraphSAGE model achieved good accuracy while requiring the shortest training
time but yielded the lowest accuracy, precision, and F1 score compared to the
other models

</details>


### [11] [Detecting AI Assistance in Abstract Complex Tasks](https://arxiv.org/abs/2507.10761)
*Tyler King,Nikolos Gurney,John H. Miller,Volkan Ustun*

Main category: cs.AI

TL;DR: 论文提出将AI辅助检测视为分类任务，通过预处理数据并构建适合神经网络的图像和时间序列表示，展示了常见模型在此任务上的有效性。


<details>
  <summary>Details</summary>
Motivation: 随着AI在复杂任务中的普及，检测AI辅助变得重要，但人类难以处理抽象任务数据。研究旨在利用神经网络分类能力解决这一问题。

Method: 构建四种神经网络友好的图像表示和一种时间序列表示，结合探索/利用用户行为编码，测试了三种经典深度学习架构和一种并行CNN-RNN架构。

Result: 实验表明，适当预处理的数据能被常见模型有效分类，且结合时空编码的架构在检测AI辅助时表现最佳。

Conclusion: 研究证明了预处理和时空编码对检测抽象任务中AI辅助的重要性，为未来研究提供了通用框架。

Abstract: Detecting assistance from artificial intelligence is increasingly important
as they become ubiquitous across complex tasks such as text generation, medical
diagnosis, and autonomous driving. Aid detection is challenging for humans,
especially when looking at abstract task data. Artificial neural networks excel
at classification thanks to their ability to quickly learn from and process
large amounts of data -- assuming appropriate preprocessing. We posit detecting
help from AI as a classification task for such models. Much of the research in
this space examines the classification of complex but concrete data classes,
such as images. Many AI assistance detection scenarios, however, result in data
that is not machine learning-friendly. We demonstrate that common models can
effectively classify such data when it is appropriately preprocessed. To do so,
we construct four distinct neural network-friendly image formulations along
with an additional time-series formulation that explicitly encodes the
exploration/exploitation of users, which allows for generalizability to other
abstract tasks. We benchmark the quality of each image formulation across three
classical deep learning architectures, along with a parallel CNN-RNN
architecture that leverages the additional time series to maximize testing
performance, showcasing the importance of encoding temporal and spatial
quantities for detecting AI aid in abstract tasks.

</details>


### [12] [Uncertainty-Informed Scheduling of Decision Points for Intelligent Mobile Health Interventions](https://arxiv.org/abs/2507.10798)
*Asim H. Gazi,Bhanu T. Gullapalli,Daiqi Gao,Benjamin M. Marlin,Vivek Shetty,Susan A. Murphy*

Main category: cs.AI

TL;DR: 提出了一种动态调度决策点的方法SigmaScheduling，根据预测行为时间的不确定性调整干预时机，提高了移动健康干预的及时性。


<details>
  <summary>Details</summary>
Motivation: 当前固定间隔的决策点调度方法对习惯性行为干预效果不佳，尤其是对作息不规律的用户。

Method: 提出SigmaScheduling方法，根据行为时间预测的不确定性动态调整决策点，预测准确时靠近行为时间，不确定时提前调度。

Result: 在68名参与者的10周试验中，SigmaScheduling在70%以上的情况下确保决策点位于刷牙行为之前。

Conclusion: SigmaScheduling提升了移动健康干预的精准性，尤其适用于时间敏感的习惯性行为。

Abstract: Timely decision making is critical to the effectiveness of mobile health
(mHealth) interventions. At predefined timepoints called "decision points,"
intelligent mHealth systems such as just-in-time adaptive interventions
(JITAIs) estimate an individual's biobehavioral context from sensor or survey
data and determine whether and how to intervene. For interventions targeting
habitual behavior (e.g., oral hygiene), effectiveness often hinges on
delivering support shortly before the target behavior is likely to occur.
Current practice schedules decision points at a fixed interval (e.g., one hour)
before user-provided behavior times, and the fixed interval is kept the same
for all individuals. However, this one-size-fits-all approach performs poorly
for individuals with irregular routines, often scheduling decision points after
the target behavior has already occurred, rendering interventions ineffective.
In this paper, we propose SigmaScheduling, a method to dynamically schedule
decision points based on uncertainty in predicted behavior times. When behavior
timing is more predictable, SigmaScheduling schedules decision points closer to
the predicted behavior time; when timing is less certain, SigmaScheduling
schedules decision points earlier, increasing the likelihood of timely
intervention. We evaluated SigmaScheduling using real-world data from 68
participants in a 10-week trial of Oralytics, a JITAI designed to improve daily
toothbrushing. SigmaScheduling increased the likelihood that decision points
preceded brushing events in at least 70% of cases, preserving opportunities to
intervene and impact behavior. Our results indicate that SigmaScheduling can
advance precision mHealth, particularly for JITAIs targeting time-sensitive,
habitual behaviors such as oral hygiene or dietary habits.

</details>


### [13] [Automated Thematic Analyses Using LLMs: Xylazine Wound Management Social Media Chatter Use Case](https://arxiv.org/abs/2507.10803)
*JaMor Hairston,Ritvik Ranjan,Sahithi Lakamana,Anthony Spadaro,Selen Bozkurt,Jeanmarie Perrone,Abeed Sarker*

Main category: cs.AI

TL;DR: 研究表明，少量样本提示的大语言模型（如GPT-4o）可以高效复现专家驱动的主题分析，为定性研究提供可扩展的补充。


<details>
  <summary>Details</summary>
Motivation: 探索大语言模型（LLMs）在需要深度解释和领域专业知识的归纳主题分析任务中的可行性。

Method: 使用两个非重叠时间段的Reddit数据集，通过零样本、单样本和少量样本提示策略，将任务建模为一系列二元分类问题，评估五个LLMs的性能。

Result: GPT-4o在少量样本提示下表现最佳（准确率90.9%；F1分数0.71），高流行主题的模型分类与专家分类高度一致。

Conclusion: 少量样本提示的LLM方法可以自动化主题分析，为定性研究提供可扩展的解决方案。

Abstract: Background Large language models (LLMs) face challenges in inductive thematic
analysis, a task requiring deep interpretive and domain-specific expertise. We
evaluated the feasibility of using LLMs to replicate expert-driven thematic
analysis of social media data. Methods Using two temporally non-intersecting
Reddit datasets on xylazine (n=286 and n=686, for model optimization and
validation, respectively) with twelve expert-derived themes, we evaluated five
LLMs against expert coding. We modeled the task as a series of binary
classifications, rather than a single, multi-label classification, employing
zero-, single-, and few-shot prompting strategies and measuring performance via
accuracy, precision, recall, and F1-score. Results On the validation set,
GPT-4o with two-shot prompting performed best (accuracy: 90.9%; F1-score:
0.71). For high-prevalence themes, model-derived thematic distributions closely
mirrored expert classifications (e.g., xylazine use: 13.6% vs. 17.8%; MOUD use:
16.5% vs. 17.8%). Conclusions Our findings suggest that few-shot LLM-based
approaches can automate thematic analyses, offering a scalable supplement for
qualitative research. Keywords: thematic analysis, large language models,
natural language processing, qualitative analysis, social media, prompt
engineering, public health

</details>


### [14] [AF-XRAY: Visual Explanation and Resolution of Ambiguity in Legal Argumentation Frameworks](https://arxiv.org/abs/2507.10831)
*Yilin Xia,Heng Zheng,Shawn Bowers,Bertram Ludäscher*

Main category: cs.AI

TL;DR: AF-XRAY是一个开源工具包，用于探索、分析和可视化法律推理中的抽象论证框架，帮助非专家识别歧义来源并解释论证接受性。


<details>
  <summary>Details</summary>
Motivation: 法律推理中的论证框架（AFs）存在歧义和解释难题，非专家难以理解。AF-XRAY旨在通过可视化工具解决这一问题。

Method: AF-XRAY提供分层可视化、攻击边分类、替代解决方案叠加以及关键攻击集识别，将歧义场景转化为明确解决方案。

Result: 工具通过真实法律案例验证，展示了如何通过不同假设得出不同结论，支持法律推理。

Conclusion: AF-XRAY通过系统化方法帮助用户识别歧义原因并探索解决方案，提升了法律推理的透明度和可理解性。

Abstract: Argumentation frameworks (AFs) provide formal approaches for legal reasoning,
but identifying sources of ambiguity and explaining argument acceptance remains
challenging for non-experts. We present AF-XRAY, an open-source toolkit for
exploring, analyzing, and visualizing abstract AFs in legal reasoning. AF-XRAY
introduces: (i) layered visualizations based on game-theoretic argument length
revealing well-founded derivation structures; (ii) classification of attack
edges by semantic roles (primary, secondary, blunders); (iii) overlay
visualizations of alternative 2-valued solutions on ambiguous 3-valued grounded
semantics; and (iv) identification of critical attack sets whose suspension
resolves undecided arguments. Through systematic generation of critical attack
sets, AF-XRAY transforms ambiguous scenarios into grounded solutions, enabling
users to pinpoint specific causes of ambiguity and explore alternative
resolutions. We use real-world legal cases (e.g., Wild Animals as modeled by
Bench-Capon) to show that our tool supports teleological legal reasoning by
revealing how different assumptions lead to different justified conclusions.

</details>


### [15] [NavComposer: Composing Language Instructions for Navigation Trajectories through Action-Scene-Object Modularization](https://arxiv.org/abs/2507.10894)
*Zongtao He,Liuyi Wang,Lu Chen,Chengju Liu,Qijun Chen*

Main category: cs.AI

TL;DR: NavComposer是一个自动生成高质量导航指令的框架，通过分解和重组语义实体（如动作、场景和对象）生成自然语言指令。NavInstrCritic是一个无需标注的评估系统，从对比匹配、语义一致性和语言多样性三个维度评估指令质量。


<details>
  <summary>Details</summary>
Motivation: 解决专家提供的导航指令数量有限和合成指令质量不足的问题，以支持大规模研究。

Method: NavComposer分解语义实体并重组为指令，支持数据无关的适应；NavInstrCritic从三个维度评估指令质量。

Result: 实验证明该方法有效，支持更可扩展和通用的研究。

Conclusion: NavComposer和NavInstrCritic为语言引导导航提供了高质量指令生成和评估的解决方案。

Abstract: Language-guided navigation is a cornerstone of embodied AI, enabling agents
to interpret language instructions and navigate complex environments. However,
expert-provided instructions are limited in quantity, while synthesized
annotations often lack quality, making them insufficient for large-scale
research. To address this, we propose NavComposer, a novel framework for
automatically generating high-quality navigation instructions. NavComposer
explicitly decomposes semantic entities such as actions, scenes, and objects,
and recomposes them into natural language instructions. Its modular
architecture allows flexible integration of state-of-the-art techniques, while
the explicit use of semantic entities enhances both the richness and accuracy
of instructions. Moreover, it operates in a data-agnostic manner, supporting
adaptation to diverse navigation trajectories without domain-specific training.
Complementing NavComposer, we introduce NavInstrCritic, a comprehensive
annotation-free evaluation system that assesses navigation instructions on
three dimensions: contrastive matching, semantic consistency, and linguistic
diversity. NavInstrCritic provides a holistic evaluation of instruction
quality, addressing limitations of traditional metrics that rely heavily on
expert annotations. By decoupling instruction generation and evaluation from
specific navigation agents, our method enables more scalable and generalizable
research. Extensive experiments provide direct and practical evidence for the
effectiveness of our method.

</details>


### [16] [Lessons Learned from Evaluation of LLM based Multi-agents in Safer Therapy Recommendation](https://arxiv.org/abs/2507.10911)
*Yicong Wu,Ting Chen,Irit Hochberg,Zhoujian Sun,Ruth Edry,Zhengxing Huang,Mor Peleg*

Main category: cs.AI

TL;DR: 研究探讨了基于大型语言模型（LLM）的多代理系统（MAS）在慢性多病症患者治疗推荐中的可行性和价值，模拟多学科团队（MDT）决策，结果显示单代理系统表现与MDT相当，但建议存在不完整和不必要药物问题。


<details>
  <summary>Details</summary>
Motivation: 慢性多病症患者的治疗推荐因治疗冲突风险而复杂，现有决策支持系统扩展性不足，需探索新方法。

Method: 设计单代理和MAS框架，模拟MDT决策，通过LLM代理讨论解决医疗冲突，并在多病症患者治疗任务中评估性能。

Result: 单代理系统表现与MDT相当，但建议不完整且存在不必要药物，可能导致冲突或药物相互作用。

Conclusion: 当前LLM在治疗推荐中表现良好，但需改进建议完整性和减少不必要药物，未来可优化MAS框架。

Abstract: Therapy recommendation for chronic patients with multimorbidity is
challenging due to risks of treatment conflicts. Existing decision support
systems face scalability limitations. Inspired by the way in which general
practitioners (GP) manage multimorbidity patients, occasionally convening
multidisciplinary team (MDT) collaboration, this study investigated the
feasibility and value of using a Large Language Model (LLM)-based multi-agent
system (MAS) for safer therapy recommendations. We designed a single agent and
a MAS framework simulating MDT decision-making by enabling discussion among LLM
agents to resolve medical conflicts. The systems were evaluated on therapy
planning tasks for multimorbidity patients using benchmark cases. We compared
MAS performance with single-agent approaches and real-world benchmarks. An
important contribution of our study is the definition of evaluation metrics
that go beyond the technical precision and recall and allow the inspection of
clinical goals met and medication burden of the proposed advices to a gold
standard benchmark. Our results show that with current LLMs, a single agent GP
performs as well as MDTs. The best-scoring models provide correct
recommendations that address all clinical goals, yet the advices are
incomplete. Some models also present unnecessary medications, resulting in
unnecessary conflicts between medication and conditions or drug-drug
interactions.

</details>


### [17] [Enhancing Safe and Controllable Protein Generation via Knowledge Preference Optimization](https://arxiv.org/abs/2507.10923)
*Yuhao Wang,Keyan Ding,Kehua Feng,Zeyuan Wang,Ming Qin,Xiaotong Li,Qiang Zhang,Huajun Chen*

Main category: cs.AI

TL;DR: 提出了一种知识引导的偏好优化（KPO）框架，通过蛋白质安全知识图谱整合先验知识，以减少生成有害蛋白质序列的风险。


<details>
  <summary>Details</summary>
Motivation: 蛋白质语言模型在功能优化和设计方面具有优势，但存在生成有害序列的风险，如增强病毒传播性或逃避免疫反应的蛋白质。

Method: 结合蛋白质安全知识图谱，采用图剪枝策略识别优选序列，并通过强化学习最小化有害蛋白质生成风险。

Result: 实验表明，KPO能有效降低有害序列生成概率，同时保持高功能性。

Conclusion: KPO为生物技术中生成模型的应用提供了安全保证框架。

Abstract: Protein language models have emerged as powerful tools for sequence
generation, offering substantial advantages in functional optimization and
denovo design. However, these models also present significant risks of
generating harmful protein sequences, such as those that enhance viral
transmissibility or evade immune responses. These concerns underscore critical
biosafety and ethical challenges. To address these issues, we propose a
Knowledge-guided Preference Optimization (KPO) framework that integrates prior
knowledge via a Protein Safety Knowledge Graph. This framework utilizes an
efficient graph pruning strategy to identify preferred sequences and employs
reinforcement learning to minimize the risk of generating harmful proteins.
Experimental results demonstrate that KPO effectively reduces the likelihood of
producing hazardous sequences while maintaining high functionality, offering a
robust safety assurance framework for applying generative models in
biotechnology.

</details>


### [18] [Modeling Habitat Shifts: Integrating Convolutional Neural Networks and Tabular Data for Species Migration Prediction](https://arxiv.org/abs/2507.10993)
*Emir Durakovic,Min-Hong Shih*

Main category: cs.AI

TL;DR: 结合卷积神经网络和表格数据，通过卫星图像和环境特征预测鸟类栖息地分布，准确率达85%。


<details>
  <summary>Details</summary>
Motivation: 气候变化导致栖息地范围变化，需准确预测鸟类分布。

Method: 使用CNN分析卫星图像的空间特征（如森林、水体），结合表格数据（温度、降水等）进行预测。

Result: 模型平均准确率为85%，能可靠预测鸟类分布。

Conclusion: 该方法可扩展且可靠，适用于研究鸟类迁徙。

Abstract: Due to climate-induced changes, many habitats are experiencing range shifts
away from their traditional geographic locations (Piguet, 2011). We propose a
solution to accurately model whether bird species are present in a specific
habitat through the combination of Convolutional Neural Networks (CNNs)
(O'Shea, 2015) and tabular data. Our approach makes use of satellite imagery
and environmental features (e.g., temperature, precipitation, elevation) to
predict bird presence across various climates. The CNN model captures spatial
characteristics of landscapes such as forestation, water bodies, and
urbanization, whereas the tabular method uses ecological and geographic data.
Both systems predict the distribution of birds with an average accuracy of 85%,
offering a scalable but reliable method to understand bird migration.

</details>


### [19] [Personalized Exercise Recommendation with Semantically-Grounded Knowledge Tracing](https://arxiv.org/abs/2507.11060)
*Yilmazcan Ozyurt,Tunaberk Almaci,Stefan Feuerriegel,Mrinmaya Sachan*

Main category: cs.AI

TL;DR: ExRec是一个结合语义知识追踪的个性化练习推荐框架，通过改进强化学习方法提升推荐效果。


<details>
  <summary>Details</summary>
Motivation: 现有练习推荐方法忽视问题的语义内容和学习顺序，ExRec旨在解决这一问题。

Method: ExRec采用端到端流程，包括问题标注、语义表示学习、知识追踪模型训练和强化学习优化。

Result: 在多个在线数学学习任务中验证了ExRec的有效性，并能泛化到新问题。

Conclusion: ExRec展示了知识追踪引导的强化学习在教育个性化中的潜力。

Abstract: We introduce ExRec, a general framework for personalized exercise
recommendation with semantically-grounded knowledge tracing. Our method builds
on the observation that existing exercise recommendation approaches simulate
student performance via knowledge tracing (KT) but they often overlook two key
aspects: (a) the semantic content of questions and (b) the sequential,
structured progression of student learning. To address this, our ExRec presents
an end-to-end pipeline, from annotating the KCs of questions and learning their
semantic representations to training KT models and optimizing several
reinforcement learning (RL) methods. Moreover, we improve standard
Q-learning-based continuous RL methods via a tailored model-based value
estimation (MVE) approach that directly leverages the components of KT model in
estimating cumulative knowledge improvement. We validate the effectiveness of
our ExRec using various RL methods across four real-world tasks with different
educational goals in online math learning. We further show that ExRec
generalizes robustly to new, unseen questions and that it produces
interpretable student learning trajectories. Together, our findings highlight
the promise of KT-guided RL for effective personalization in education.

</details>


### [20] [Tactical Decision for Multi-UGV Confrontation with a Vision-Language Model-Based Commander](https://arxiv.org/abs/2507.11079)
*Li Wang,Qizhen Wu,Lei Chen*

Main category: cs.AI

TL;DR: 提出了一种基于视觉语言模型的指挥官方法，用于解决无人地面车辆对抗中的智能感知到决策推理问题。


<details>
  <summary>Details</summary>
Motivation: 传统的手工规则方法在复杂战场环境中表现脆弱，而现有强化学习方法因缺乏可解释性主要关注动作而非战略决策。

Method: 结合视觉语言模型进行场景理解和轻量级大语言模型进行战略推理，实现感知与决策在共享语义空间的统一。

Result: 仿真和消融实验表明，该方法相比基线模型胜率超过80%。

Conclusion: 该方法通过模拟人类指挥官的认知过程，实现了高适应性和可解释性的全链决策。

Abstract: In multiple unmanned ground vehicle confrontations, autonomously evolving
multi-agent tactical decisions from situational awareness remain a significant
challenge. Traditional handcraft rule-based methods become vulnerable in the
complicated and transient battlefield environment, and current reinforcement
learning methods mainly focus on action manipulation instead of strategic
decisions due to lack of interpretability. Here, we propose a vision-language
model-based commander to address the issue of intelligent
perception-to-decision reasoning in autonomous confrontations. Our method
integrates a vision language model for scene understanding and a lightweight
large language model for strategic reasoning, achieving unified perception and
decision within a shared semantic space, with strong adaptability and
interpretability. Unlike rule-based search and reinforcement learning methods,
the combination of the two modules establishes a full-chain process, reflecting
the cognitive process of human commanders. Simulation and ablation experiments
validate that the proposed approach achieves a win rate of over 80% compared
with baseline models.

</details>


### [21] [Function-to-Style Guidance of LLMs for Code Translation](https://arxiv.org/abs/2507.11083)
*Longhui Zhang,Bin Wang,Jiahao Wang,Xiaofeng Zhao,Min Zhang,Hao Yang,Meishan Zhang,Yu Li,Jing Li,Jun Yu,Min Zhang*

Main category: cs.AI

TL;DR: F2STrans提出了一种分阶段的方法，通过功能学习和风格学习提升LLM在代码翻译中的正确性和可读性，并在新基准测试中表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有LLM在代码翻译中难以同时保证正确性和可读性，限制了其实际应用。

Method: F2STrans分为功能学习（优化正确性）和风格学习（提升可读性），并结合新基准测试。

Result: 实验表明，F2STrans显著提升性能，Qwen-1.5B甚至优于Qwen-32B和GPT-4。

Conclusion: F2STrans通过分阶段学习有效解决了代码翻译中的关键问题，具有实际应用潜力。

Abstract: Large language models (LLMs) have made significant strides in code
translation tasks. However, ensuring both the correctness and readability of
translated code remains a challenge, limiting their effective adoption in
real-world software development. In this work, we propose F2STrans, a
function-to-style guiding paradigm designed to progressively improve the
performance of LLMs in code translation. Our approach comprises two key stages:
(1) Functional learning, which optimizes translation correctness using
high-quality source-target code pairs mined from online programming platforms,
and (2) Style learning, which improves translation readability by incorporating
both positive and negative style examples. Additionally, we introduce a novel
code translation benchmark that includes up-to-date source code, extensive test
cases, and manually annotated ground-truth translations, enabling comprehensive
functional and stylistic evaluations. Experiments on both our new benchmark and
existing datasets demonstrate that our approach significantly improves code
translation performance. Notably, our approach enables Qwen-1.5B to outperform
prompt-enhanced Qwen-32B and GPT-4 on average across 20 diverse code
translation scenarios.

</details>


### [22] [AI Agent Architecture for Decentralized Trading of Alternative Assets](https://arxiv.org/abs/2507.11117)
*Ailiya Borjigin,Cong He,Charles CC Lee,Wei Zhou*

Main category: cs.AI

TL;DR: GoldMine OS是一个研究导向的架构，利用多个专用AI代理自动化并安全地将实物黄金代币化为区块链稳定币（OZ），结合链上智能合约和链下AI代理，满足合规性、流动性和风险管理需求。


<details>
  <summary>Details</summary>
Motivation: 解决实物资产与区块链系统之间的代币化和交易问题，同时满足严格的合规性、流动性和风险管理要求。

Method: 结合链上智能合约（关键风险控制）和链下AI代理（决策），设计了四个协作代理（合规、代币发行、做市、风险控制）和一个协调核心。

Result: 原型实现按需代币发行时间低于1.2秒，做市代理在波动条件下保持利差低于0.5%，系统在故障注入测试中表现稳健，支持5000 TPS和10000并发用户。

Conclusion: AI代理驱动的去中心化交易所可满足高性能和安全需求，为传统非流动性资产提供民主化访问，并通过治理模型确保透明度和系统完整性。

Abstract: Decentralized trading of real-world alternative assets (e.g., gold) requires
bridging physical asset custody with blockchain systems while meeting strict
requirements for compliance, liquidity, and risk management. We present
GoldMine OS, a research oriented architecture that employs multiple specialized
AI agents to automate and secure the tokenization and exchange of physical gold
into a blockchain based stablecoin ("OZ"). Our approach combines on chain smart
contracts for critical risk controls with off chain AI agents for decision
making, blending the transparency and reliability of blockchains with the
flexibility of AI driven automation. We describe four cooperative agents
(Compliance, Token Issuance, Market Making, and Risk Control) and a
coordinating core, and evaluate the system through simulation and a controlled
pilot deployment. In experiments the prototype delivers on demand token
issuance in under 1.2 s, more than 100 times faster than manual workflows. The
Market Making agent maintains tight liquidity with spreads often below 0.5
percent even under volatile conditions. Fault injection tests show resilience:
an oracle price spoofing attack is detected and mitigated within 10 s, and a
simulated vault mis reporting halts issuance immediately with minimal user
impact. The architecture scales to 5000 transactions per second with 10000
concurrent users in benchmarks. These results indicate that an AI agent based
decentralized exchange for alternative assets can satisfy rigorous performance
and safety requirements. We discuss broader implications for democratizing
access to traditionally illiquid assets and explain how our governance model --
multi signature agent updates and on chain community voting on risk parameters
-- provides ongoing transparency, adaptability, and formal assurance of system
integrity.

</details>


### [23] [Defining neurosymbolic AI](https://arxiv.org/abs/2507.11127)
*Lennert De Smet,Luc De Raedt*

Main category: cs.AI

TL;DR: 本文提出了神经符号AI的形式化定义，将其推理过程抽象为逻辑函数与信念函数的积分计算。


<details>
  <summary>Details</summary>
Motivation: 当前神经符号AI领域缺乏统一的定义，阻碍了研究的进一步发展。

Method: 通过抽象关键要素，提出神经符号推理的形式化定义。

Result: 该定义能够涵盖代表性的神经符号AI系统。

Conclusion: 形式化定义有助于推动神经符号AI领域的标准化与进一步发展。

Abstract: Neurosymbolic AI focuses on integrating learning and reasoning, in
particular, on unifying logical and neural representations. Despite the
existence of an alphabet soup of neurosymbolic AI systems, the field is lacking
a generally accepted formal definition of what neurosymbolic models and
inference really are. We introduce a formal definition for neurosymbolic AI
that makes abstraction of its key ingredients. More specifically, we define
neurosymbolic inference as the computation of an integral over a product of a
logical and a belief function. We show that our neurosymbolic AI definition
makes abstraction of key representative neurosymbolic AI systems.

</details>


### [24] [Collaborative Trustworthiness for Good Decision Making in Autonomous Systems](https://arxiv.org/abs/2507.11135)
*Selma Saidi,Omar Laimona,Christoph Schmickler,Dirk Ziegenbein*

Main category: cs.AI

TL;DR: 提出一种基于协作的自主系统可信决策方法，利用感知质量等属性评估系统可信度，并采用BDD进行高效信念聚合与传播。


<details>
  <summary>Details</summary>
Motivation: 自主系统在动态复杂环境中的安全性和正确性决策面临挑战，需提高其可信度和可靠性。

Method: 利用感知质量等属性评估系统可信度，结合社会认识论定义聚合与传播规则，使用BDD进行信念建模与高效计算。

Result: 通过BDD的简化规则实现高效协作自动推理，提升决策的可靠性和可信度。

Conclusion: 该方法为自主系统在冲突信息下的可信决策提供了一种有效解决方案。

Abstract: Autonomous systems are becoming an integral part of many application domains,
like in the mobility sector. However, ensuring their safe and correct behaviour
in dynamic and complex environments remains a significant challenge, where
systems should autonomously make decisions e.g., about manoeuvring. We propose
in this paper a general collaborative approach for increasing the level of
trustworthiness in the environment of operation and improve reliability and
good decision making in autonomous system. In the presence of conflicting
information, aggregation becomes a major issue for trustworthy decision making
based on collaborative data sharing. Unlike classical approaches in the
literature that rely on consensus or majority as aggregation rule, we exploit
the fact that autonomous systems have different quality attributes like
perception quality. We use this criteria to determine which autonomous systems
are trustworthy and borrow concepts from social epistemology to define
aggregation and propagation rules, used for automated decision making. We use
Binary Decision Diagrams (BDDs) as formal models for beliefs aggregation and
propagation, and formulate reduction rules to reduce the size of the BDDs and
allow efficient computation structures for collaborative automated reasoning.

</details>


### [25] [Fine-grained Timing Analysis of Digital Integrated Circuits in Answer Set Programming](https://arxiv.org/abs/2507.11150)
*Alessandro Bertagnon,Marcello Dalpasso,Michele Favalli,Marco Gavanelli*

Main category: cs.AI

TL;DR: 论文提出了一种使用答案集编程（ASP）精确计算组合电路最大延迟的方法，以替代传统的静态时序分析，从而提高处理器性能。


<details>
  <summary>Details</summary>
Motivation: 传统静态时序分析只能提供最大延迟的上界，可能导致处理器性能未被充分利用。

Method: 将问题建模为答案集编程（ASP），并提出非平凡的编码方法。

Result: 实验证明ASP能有效解决硬件设计中的复杂问题。

Conclusion: ASP是解决硬件设计中复杂问题的可行方案。

Abstract: In the design of integrated circuits, one critical metric is the maximum
delay introduced by combinational modules within the circuit. This delay is
crucial because it represents the time required to perform a computation: in an
Arithmetic-Logic Unit it represents the maximum time taken by the circuit to
perform an arithmetic operation. When such a circuit is part of a larger,
synchronous system, like a CPU, the maximum delay directly impacts the maximum
clock frequency of the entire system. Typically, hardware designers use Static
Timing Analysis to compute an upper bound of the maximum delay because it can
be determined in polynomial time. However, relying on this upper bound can lead
to suboptimal processor speeds, thereby missing performance opportunities. In
this work, we tackle the challenging task of computing the actual maximum
delay, rather than an approximate value. Since the problem is computationally
hard, we model it in Answer Set Programming (ASP), a logic language featuring
extremely efficient solvers. We propose non-trivial encodings of the problem
into ASP. Experimental results show that ASP is a viable solution to address
complex problems in hardware design.

</details>


### [26] [DuetGraph: Coarse-to-Fine Knowledge Graph Reasoning with Dual-Pathway Global-Local Fusion](https://arxiv.org/abs/2507.11229)
*Jin Li,Zezhong Ding,Xike Xie*

Main category: cs.AI

TL;DR: DuetGraph提出了一种双路径全局-局部融合的知识图谱推理机制，通过分离全局和局部信息处理路径解决了分数过平滑问题，并采用粗到细优化策略提升推理质量。


<details>
  <summary>Details</summary>
Motivation: 现有知识图谱推理方法因全局和局部信息堆叠处理导致分数过平滑，影响推理效果。

Method: DuetGraph采用双路径机制（消息传递处理局部信息，注意力机制处理全局信息），并引入粗到细优化策略。

Result: 实验表明，DuetGraph在推理质量和训练效率上均优于现有方法，推理质量提升8.7%，训练效率加速1.8倍。

Conclusion: DuetGraph通过双路径融合和粗到细优化有效解决了分数过平滑问题，实现了SOTA性能。

Abstract: Knowledge graphs (KGs) are vital for enabling knowledge reasoning across
various domains. Recent KG reasoning methods that integrate both global and
local information have achieved promising results. However, existing methods
often suffer from score over-smoothing, which blurs the distinction between
correct and incorrect answers and hinders reasoning effectiveness. To address
this, we propose DuetGraph, a coarse-to-fine KG reasoning mechanism with
dual-pathway global-local fusion. DuetGraph tackles over-smoothing by
segregating -- rather than stacking -- the processing of local (via message
passing) and global (via attention) information into two distinct pathways,
preventing mutual interference and preserving representational discrimination.
In addition, DuetGraph introduces a coarse-to-fine optimization, which
partitions entities into high- and low-score subsets. This strategy narrows the
candidate space and sharpens the score gap between the two subsets, which
alleviates over-smoothing and enhances inference quality. Extensive experiments
on various datasets demonstrate that DuetGraph achieves state-of-the-art (SOTA)
performance, with up to an 8.7% improvement in reasoning quality and a
1.8$\times$ acceleration in training efficiency.

</details>


### [27] [Opus: A Prompt Intention Framework for Complex Workflow Generation](https://arxiv.org/abs/2507.11288)
*Théo Fagnoni,Mahsun Altin,Chia En Chung,Phillip Kingston,Alan Tuning,Dana O. Mohamed,Inès Adnani*

Main category: cs.AI

TL;DR: Opus Prompt Intention Framework通过引入中间层（Intention Capture）来改进基于LLM的工作流生成，显著提升了复杂查询下的输出质量。


<details>
  <summary>Details</summary>
Motivation: 解决直接基于用户查询生成工作流时逻辑性和可扩展性不足的问题。

Method: 提出Opus Workflow Intention Framework，包括从查询中提取Workflow Signals、解析为结构化Workflow Intention对象，并基于此生成工作流。

Result: 在1000对多意图查询-工作流对的基准测试中，语义相似性指标显著提升。

Conclusion: Opus框架通过意图捕获层有效提升了工作流生成的质量，尤其在混合意图场景下表现突出。

Abstract: This paper introduces the Opus Prompt Intention Framework, designed to
improve complex Workflow Generation with instruction-tuned Large Language
Models (LLMs). We propose an intermediate Intention Capture layer between user
queries and Workflow Generation, implementing the Opus Workflow Intention
Framework, which consists of extracting Workflow Signals from user queries,
interpreting them into structured Workflow Intention objects, and generating
Workflows based on these Intentions. Our results show that this layer enables
LLMs to produce logical and meaningful outputs that scale reliably as query
complexity increases. On a synthetic benchmark of 1,000 multi-intent
query-Workflow(s) pairs, applying the Opus Prompt Intention Framework to
Workflow Generation yields consistent improvements in semantic Workflow
similarity metrics. In this paper, we introduce the Opus Prompt Intention
Framework by applying the concepts of Workflow Signal and Workflow Intention to
LLM-driven Workflow Generation. We present a reproducible, customizable
LLM-based Intention Capture system to extract Workflow Signals and Workflow
Intentions from user queries. Finally, we provide empirical evidence that the
proposed system significantly improves Workflow Generation quality compared to
direct generation from user queries, particularly in cases of Mixed Intention
Elicitation.

</details>


### [28] [Contestability in Quantitative Argumentation](https://arxiv.org/abs/2507.11323)
*Xiang Yin,Nico Potyka,Antonio Rago,Timotheus Kampik,Francesca Toni*

Main category: cs.AI

TL;DR: 论文提出了一种基于梯度的方法（G-RAEs）来解决EW-QBAFs中的争议性问题，通过调整边权重以实现特定论证的期望强度。


<details>
  <summary>Details</summary>
Motivation: 研究如何使AI决策与人类偏好一致，特别是在争议性框架（EW-QBAFs）中，目前相关研究较少。

Method: 提出梯度关系归因解释（G-RAEs），量化主题论证强度对边权重变化的敏感性，并开发迭代算法调整权重。

Result: 实验表明，该方法在模拟推荐系统和多层感知器的合成EW-QBAFs中有效解决问题。

Conclusion: G-RAEs为EW-QBAFs中的争议性问题提供了可解释的解决方案，支持AI决策的可争议性。

Abstract: Contestable AI requires that AI-driven decisions align with human
preferences. While various forms of argumentation have been shown to support
contestability, Edge-Weighted Quantitative Bipolar Argumentation Frameworks
(EW-QBAFs) have received little attention. In this work, we show how EW-QBAFs
can be deployed for this purpose. Specifically, we introduce the contestability
problem for EW-QBAFs, which asks how to modify edge weights (e.g., preferences)
to achieve a desired strength for a specific argument of interest (i.e., a
topic argument). To address this problem, we propose gradient-based relation
attribution explanations (G-RAEs), which quantify the sensitivity of the topic
argument's strength to changes in individual edge weights, thus providing
interpretable guidance for weight adjustments towards contestability. Building
on G-RAEs, we develop an iterative algorithm that progressively adjusts the
edge weights to attain the desired strength. We evaluate our approach
experimentally on synthetic EW-QBAFs that simulate the structural
characteristics of personalised recommender systems and multi-layer
perceptrons, and demonstrate that it can solve the problem effectively.

</details>


### [29] [CogDDN: A Cognitive Demand-Driven Navigation with Decision Optimization and Dual-Process Thinking](https://arxiv.org/abs/2507.11334)
*Yuehao Huang,Liang Liu,Shuangming Lei,Yukai Ma,Hao Su,Jianbiao Mei,Pengxiang Zhao,Yaqing Gu,Yong Liu,Jiajun Lv*

Main category: cs.AI

TL;DR: CogDDN是一个基于视觉语言模型（VLM）的框架，通过模拟人类认知和学习机制，结合快速和慢速思维系统，提升机器人在未知环境中的导航和交互能力。


<details>
  <summary>Details</summary>
Motivation: 传统的数据驱动需求导航（DDN）方法依赖预收集数据，限制了在未知场景中的泛化能力。CogDDN旨在通过模拟人类认知机制解决这一问题。

Method: CogDDN整合了快速和慢速思维系统，通过语义对齐检测对象与指令，并采用双过程决策模块（启发式过程和分析过程）和链式思维（CoT）推理。

Result: 在AI2Thor模拟器和ProcThor数据集上的实验表明，CogDDN比单视角相机方法性能提升15%，导航准确性和适应性显著提高。

Conclusion: CogDDN通过模拟人类认知机制，显著提升了机器人在未知环境中的导航能力，为需求驱动导航提供了新思路。

Abstract: Mobile robots are increasingly required to navigate and interact within
unknown and unstructured environments to meet human demands. Demand-driven
navigation (DDN) enables robots to identify and locate objects based on
implicit human intent, even when object locations are unknown. However,
traditional data-driven DDN methods rely on pre-collected data for model
training and decision-making, limiting their generalization capability in
unseen scenarios. In this paper, we propose CogDDN, a VLM-based framework that
emulates the human cognitive and learning mechanisms by integrating fast and
slow thinking systems and selectively identifying key objects essential to
fulfilling user demands. CogDDN identifies appropriate target objects by
semantically aligning detected objects with the given instructions.
Furthermore, it incorporates a dual-process decision-making module, comprising
a Heuristic Process for rapid, efficient decisions and an Analytic Process that
analyzes past errors, accumulates them in a knowledge base, and continuously
improves performance. Chain of Thought (CoT) reasoning strengthens the
decision-making process. Extensive closed-loop evaluations on the AI2Thor
simulator with the ProcThor dataset show that CogDDN outperforms single-view
camera-only methods by 15%, demonstrating significant improvements in
navigation accuracy and adaptability. The project page is available at
https://yuehaohuang.github.io/CogDDN/.

</details>


### [30] [Foundation Models for Logistics: Toward Certifiable, Conversational Planning Interfaces](https://arxiv.org/abs/2507.11352)
*Yunhao Yang,Neel P. Bhatt,Christian Ellis,Alvaro Velasquez,Zhangyang Wang,Ufuk Topcu*

Main category: cs.AI

TL;DR: 提出了一种结合自然语言对话与可验证保证的神经符号框架，用于复杂物流决策，提升实时性和安全性。


<details>
  <summary>Details</summary>
Motivation: 物流决策需要快速重规划且处理不确定性，现有方法（如整数规划）速度慢且假设理想环境，而大语言模型（LLMs）易误解和幻觉。

Method: 引入神经符号框架，将用户请求转为结构化规划，量化不确定性，并在置信度低时触发交互澄清循环。

Result: 轻量级模型在100个样本上微调后，性能超越GPT-4.1，推理延迟降低近50%。

Conclusion: 该框架为复杂物流提供了可验证、实时且用户对齐的决策路径。

Abstract: Logistics operators, from battlefield coordinators rerouting airlifts ahead
of a storm to warehouse managers juggling late trucks, often face life-critical
decisions that demand both domain expertise and rapid and continuous
replanning. While popular methods like integer programming yield logistics
plans that satisfy user-defined logical constraints, they are slow and assume
an idealized mathematical model of the environment that does not account for
uncertainty. On the other hand, large language models (LLMs) can handle
uncertainty and promise to accelerate replanning while lowering the barrier to
entry by translating free-form utterances into executable plans, yet they
remain prone to misinterpretations and hallucinations that jeopardize safety
and cost. We introduce a neurosymbolic framework that pairs the accessibility
of natural-language dialogue with verifiable guarantees on goal interpretation.
It converts user requests into structured planning specifications, quantifies
its own uncertainty at the field and token level, and invokes an interactive
clarification loop whenever confidence falls below an adaptive threshold. A
lightweight model, fine-tuned on just 100 uncertainty-filtered examples,
surpasses the zero-shot performance of GPT-4.1 while cutting inference latency
by nearly 50%. These preliminary results highlight a practical path toward
certifiable, real-time, and user-aligned decision-making for complex logistics.

</details>


### [31] [Modeling Code: Is Text All You Need?](https://arxiv.org/abs/2507.11467)
*Daniel Nichols,Konstantinos Parasyris,Harshitha Menon,Brian R. Bartoldson,Giorgis Georgakoudis,Tal Ben-Nun,Abhinav Bhatele*

Main category: cs.AI

TL;DR: 论文提出了一种结合代码文本建模和结构化建模优势的新方法，以弥补现有LLMs在代码分析和生成能力上的不足。


<details>
  <summary>Details</summary>
Motivation: 现有的基于Transformer的代码LLMs在处理代码的结构化属性（如控制流和数据流）时能力有限，而传统的结构化建模方法又缺乏现代LLMs的生成能力和规模。

Method: 提出了一种新颖的方法，将代码作为文本建模与结构化建模相结合。

Result: 未明确提及具体结果，但暗示新方法有望提升代码分析和生成能力。

Conclusion: 新方法结合了代码文本建模和结构化建模的优势，为代码分析和生成任务提供了更全面的解决方案。

Abstract: Code LLMs have become extremely popular recently for modeling source code
across a variety of tasks, such as generation, translation, and summarization.
However, transformer-based models are limited in their capabilities to reason
through structured, analytical properties of code, such as control and data
flow. Previous work has explored the modeling of these properties with
structured data and graph neural networks. However, these approaches lack the
generative capabilities and scale of modern LLMs. In this work, we introduce a
novel approach to combine the strengths of modeling both code as text and more
structured forms.

</details>


### [32] [Chain of Thought Monitorability: A New and Fragile Opportunity for AI Safety](https://arxiv.org/abs/2507.11473)
*Tomek Korbak,Mikita Balesni,Elizabeth Barnes,Yoshua Bengio,Joe Benton,Joseph Bloom,Mark Chen,Alan Cooney,Allan Dafoe,Anca Dragan,Scott Emmons,Owain Evans,David Farhi,Ryan Greenblatt,Dan Hendrycks,Marius Hobbhahn,Evan Hubinger,Geoffrey Irving,Erik Jenner,Daniel Kokotajlo,Victoria Krakovna,Shane Legg,David Lindner,David Luan,Aleksander Mądry,Julian Michael,Neel Nanda,Dave Orr,Jakub Pachocki,Ethan Perez,Mary Phuong,Fabien Roger,Joshua Saxe,Buck Shlegeris,Martín Soto,Eric Steinberger,Jasmine Wang,Wojciech Zaremba,Bowen Baker,Rohin Shah,Vlad Mikulik*

Main category: cs.AI

TL;DR: AI系统通过人类语言“思考”为AI安全提供了独特机会，可通过监控其思维链（CoT）检测不良意图。尽管不完美，但CoT监控有潜力，建议进一步研究并投资。


<details>
  <summary>Details</summary>
Motivation: 探索AI系统通过人类语言“思考”的特性，利用思维链监控提升AI安全性。

Method: 提出监控AI的思维链（CoT）以检测潜在的不良意图。

Result: CoT监控虽不完美，但显示出潜力，建议结合现有安全方法进一步研究和投资。

Conclusion: 建议前沿模型开发者考虑开发决策对CoT监控能力的影响，因其可能具有脆弱性。

Abstract: AI systems that "think" in human language offer a unique opportunity for AI
safety: we can monitor their chains of thought (CoT) for the intent to
misbehave. Like all other known AI oversight methods, CoT monitoring is
imperfect and allows some misbehavior to go unnoticed. Nevertheless, it shows
promise and we recommend further research into CoT monitorability and
investment in CoT monitoring alongside existing safety methods. Because CoT
monitorability may be fragile, we recommend that frontier model developers
consider the impact of development decisions on CoT monitorability.

</details>


### [33] [Perspective-Aware AI in Extended Reality](https://arxiv.org/abs/2507.11479)
*Daniel Platnick,Matti Gruener,Marjan Alirezaie,Kent Larson,Dava J. Newman,Hossein Rahnama*

Main category: cs.AI

TL;DR: PAiR框架通过整合Perspective-Aware AI（PAi）与XR，利用多模态数字足迹构建用户身份模型，实现可解释、上下文感知的沉浸式体验。


<details>
  <summary>Details</summary>
Motivation: 当前AI增强的XR系统因用户建模浅层和认知上下文有限而表现不足，PAiR旨在解决这一问题。

Method: PAiR基于Chronicles（多模态数字足迹学习到的身份模型），构建闭环系统动态连接用户状态与沉浸环境，并在Unity引擎中实现两个概念验证场景。

Result: PAiR展示了将基于视角的身份模型嵌入沉浸式系统的可行性，为人类-AI交互开辟新方向。

Conclusion: PAiR通过深度用户建模和认知上下文整合，提升了XR的适应性和沉浸感。

Abstract: AI-enhanced Extended Reality (XR) aims to deliver adaptive, immersive
experiences-yet current systems fall short due to shallow user modeling and
limited cognitive context. We introduce Perspective-Aware AI in Extended
Reality (PAiR), a foundational framework for integrating Perspective-Aware AI
(PAi) with XR to enable interpretable, context-aware experiences grounded in
user identity. PAi is built on Chronicles: reasoning-ready identity models
learned from multimodal digital footprints that capture users' cognitive and
experiential evolution. PAiR employs these models in a closed-loop system
linking dynamic user states with immersive environments. We present PAiR's
architecture, detailing its modules and system flow, and demonstrate its
utility through two proof-of-concept scenarios implemented in the Unity-based
OpenDome engine. PAiR opens a new direction for human-AI interaction by
embedding perspective-based identity models into immersive systems.

</details>


### [34] [Illuminating the Three Dogmas of Reinforcement Learning under Evolutionary Light](https://arxiv.org/abs/2507.11482)
*Mani Hamidi,Terrence W. Deacon*

Main category: cs.AI

TL;DR: 论文提出一个受开放式进化论启发的框架，重新审视强化学习的三个核心信条：代理定义、学习目标和奖励假设范围，并探讨其理论和应用意义。


<details>
  <summary>Details</summary>
Motivation: 强化学习的三个核心信条存在概念上的局限性，需要重新思考以推动理论和应用的发展。

Method: 结合开放式进化论的观点，重新审视这三个信条，并通过进化动力学和生命起源理论提供新的视角。

Result: 进化论视角为强化学习的适应性和多目标问题提供了新见解，但代理问题仍需结合生命起源理论解决。

Conclusion: 进化论框架为强化学习的信条提供了有价值的修正方向，但代理问题需要更广泛的理论整合。

Abstract: Three core tenets of reinforcement learning (RL)--concerning the definition
of agency, the objective of learning, and the scope of the reward
hypothesis--have been highlighted as key targets for conceptual revision, with
major implications for theory and application. We propose a framework, inspired
by open-ended evolutionary theory, to reconsider these three "dogmas." We
revisit each assumption and address related concerns raised alongside them. To
make our arguments relevant to RL as a model of biological learning, we first
establish that evolutionary dynamics can plausibly operate within living brains
over an individual's lifetime, and are not confined to cross-generational
processes. We begin by revisiting the second dogma, drawing on evolutionary
insights to enrich the "adaptation-rather-than-search" view of learning. We
then address the third dogma regarding the limits of the reward hypothesis,
using analogies from evolutionary fitness to illuminate the scalar reward vs.
multi-objective debate. After discussing practical implications for exploration
in RL, we turn to the first--and arguably most fundamental--issue: the absence
of a formal account of agency. We argue that unlike the other two problems, the
evolutionary paradigm alone cannot resolve the agency question, though it
gestures in a productive direction. We advocate integrating ideas from
origins-of-life theory, where the thermodynamics of sustenance and replication
offer promising foundations for understanding agency and resource-constrained
reinforcement learning in biological systems.

</details>


### [35] [DrafterBench: Benchmarking Large Language Models for Tasks Automation in Civil Engineering](https://arxiv.org/abs/2507.11527)
*Yinsheng Li,Zhen Dong,Yi Shao*

Main category: cs.AI

TL;DR: DrafterBench是一个用于评估LLM代理在土木工程图纸修订任务中的综合基准，包含12类任务、46个定制功能和1920个任务，旨在测试代理的多方面能力。


<details>
  <summary>Details</summary>
Motivation: 现有基准不足以从工业角度（如土木工程）系统评估自动化代理的能力，因此需要开发更全面的评估工具。

Method: 提出DrafterBench，包含从实际图纸文件中总结的任务类型、定制功能，并通过任务准确性和错误统计详细分析代理能力。

Result: DrafterBench能够全面评估代理在结构化数据理解、功能执行、指令遵循和批判性推理等方面的能力。

Conclusion: DrafterBench为LLM在工程应用中的集成提供了深入见解和改进目标，是一个开源基准。

Abstract: Large Language Model (LLM) agents have shown great potential for solving
real-world problems and promise to be a solution for tasks automation in
industry. However, more benchmarks are needed to systematically evaluate
automation agents from an industrial perspective, for example, in Civil
Engineering. Therefore, we propose DrafterBench for the comprehensive
evaluation of LLM agents in the context of technical drawing revision, a
representation task in civil engineering. DrafterBench contains twelve types of
tasks summarized from real-world drawing files, with 46 customized
functions/tools and 1920 tasks in total. DrafterBench is an open-source
benchmark to rigorously test AI agents' proficiency in interpreting intricate
and long-context instructions, leveraging prior knowledge, and adapting to
dynamic instruction quality via implicit policy awareness. The toolkit
comprehensively assesses distinct capabilities in structured data
comprehension, function execution, instruction following, and critical
reasoning. DrafterBench offers detailed analysis of task accuracy and error
statistics, aiming to provide deeper insight into agent capabilities and
identify improvement targets for integrating LLMs in engineering applications.
Our benchmark is available at https://github.com/Eason-Li-AIS/DrafterBench,
with the test set hosted at
https://huggingface.co/datasets/Eason666/DrafterBench.

</details>


### [36] [How Many Instructions Can LLMs Follow at Once?](https://arxiv.org/abs/2507.11538)
*Daniel Jaroslawicz,Brendan Whiting,Parth Shah,Karime Maamari*

Main category: cs.AI

TL;DR: IFScale是一个评估LLM在高密度指令下性能的基准测试，发现即使最佳模型在500条指令时准确率仅为68%。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试仅评估少量指令任务，无法反映生产级LLM系统需同时处理大量指令的需求。

Method: 引入IFScale基准测试，包含500条关键词包含指令，评估20个前沿模型在高密度指令下的表现。

Result: 最佳模型在500条指令时准确率为68%，模型规模和推理能力与性能下降模式相关。

Conclusion: 研究结果有助于设计现实应用中的高密度指令提示，并揭示了性能与延迟的权衡。

Abstract: Production-grade LLM systems require robust adherence to dozens or even
hundreds of instructions simultaneously. However, the instruction-following
capabilities of LLMs at high instruction densities have not yet been
characterized, as existing benchmarks only evaluate models on tasks with a
single or few instructions. We introduce IFScale, a simple benchmark of 500
keyword-inclusion instructions for a business report writing task to measure
how instruction-following performance degrades as instruction density
increases. We evaluate 20 state-of-the-art models across seven major providers
and find that even the best frontier models only achieve 68% accuracy at the
max density of 500 instructions. Our analysis reveals model size and reasoning
capability to correlate with 3 distinct performance degradation patterns, bias
towards earlier instructions, and distinct categories of instruction-following
errors. Our insights can help inform design of instruction-dense prompts in
real-world applications and highlight important performance-latency tradeoffs.
We open-source the benchmark and all results for further analysis at
https://distylai.github.io/IFScale.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [37] [Teach Me Sign: Stepwise Prompting LLM for Sign Language Production](https://arxiv.org/abs/2507.10972)
*Zhaoyi An,Rei Kawakami*

Main category: cs.CL

TL;DR: TEAM-Sign通过微调大语言模型（LLM），将其视为另一种自然语言，以解决手语生成的复杂性。采用逐步提示策略，利用LLM的内在知识，成功对齐手语和口语的分布与语法规则。


<details>
  <summary>Details</summary>
Motivation: 手语生成因复杂性和独特规则受限于大语言模型的应用，研究旨在利用LLM的推理能力和知识解决这一问题。

Method: 提出TEAM-Sign方法，将手语视为自然语言，通过微调LLM学习文本与手语对应关系，并采用逐步提示策略提取LLM内在知识。

Result: 在How2Sign和Phoenix14T数据集上验证，TEAM-Sign有效利用LLM知识对齐手语和口语的分布与语法规则。

Conclusion: TEAM-Sign通过LLM微调和逐步提示策略，成功解决了手语生成的挑战，展示了LLM在手语任务中的潜力。

Abstract: Large language models, with their strong reasoning ability and rich
knowledge, have brought revolution to many tasks of AI, but their impact on
sign language generation remains limited due to its complexity and unique
rules. In this paper, we propose TEAch Me Sign (TEAM-Sign), treating sign
language as another natural language. By fine-tuning an LLM, we enable it to
learn the correspondence between text and sign language, and facilitate
generation. Considering the differences between sign and spoken language, we
employ a stepwise prompting strategy to extract the inherent sign language
knowledge within the LLM, thereby supporting the learning and generation
process. Experimental results on How2Sign and Phoenix14T datasets demonstrate
that our approach effectively leverages both the sign language knowledge and
reasoning capabilities of LLM to align the different distribution and
grammatical rules between sign and spoken language.

</details>


### [38] [Truth Sleuth and Trend Bender: AI Agents to fact-check YouTube videos and influence opinions](https://arxiv.org/abs/2507.10577)
*Logé Cécile,Ghori Rehan*

Main category: cs.CL

TL;DR: 论文提出了一种AI系统，通过两个代理（Truth Sleuth和Trend Bender）在YouTube上检测和反驳虚假信息，并通过实验验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 虚假信息在数字世界中快速传播，尤其是通过YouTube等平台，亟需一种有效的方法来检测和纠正。

Method: 系统包括Truth Sleuth（使用RAG方法从多源验证视频中的声明）和Trend Bender（生成有说服力的评论以引导讨论）。

Result: 实验显示系统在事实核查和用户互动方面表现优异，能够有效影响在线讨论。

Conclusion: AI驱动的干预措施在打击虚假信息和促进更明智的在线空间方面具有潜力。

Abstract: Misinformation poses a significant threat in today's digital world, often
spreading rapidly through platforms like YouTube. This paper introduces a novel
approach to combating misinformation by developing an AI-powered system that
not only fact-checks claims made in YouTube videos but also actively engages
users in the comment section and challenge misleading narratives. Our system
comprises two main agents: Truth Sleuth and Trend Bender.
  Truth Sleuth extracts claims from a YouTube video, uses a Retrieval-Augmented
Generation (RAG) approach - drawing on sources like Wikipedia, Google Search,
Google FactCheck - to accurately assess their veracity and generates a nuanced
and comprehensive report. Through rigorous prompt engineering, Trend Bender
leverages this report along with a curated corpus of relevant articles to
generate insightful and persuasive comments designed to stimulate a productive
debate. With a carefully set up self-evaluation loop, this agent is able to
iteratively improve its style and refine its output.
  We demonstrate the system's capabilities through experiments on established
benchmark datasets and a real-world deployment on YouTube, showcasing its
potential to engage users and potentially influence perspectives. Our findings
highlight the high accuracy of our fact-checking agent, and confirm the
potential of AI-driven interventions in combating misinformation and fostering
a more informed online space.

</details>


### [39] [An Offline Mobile Conversational Agent for Mental Health Support: Learning from Emotional Dialogues and Psychological Texts with Student-Centered Evaluation](https://arxiv.org/abs/2507.10580)
*Vimaleswar A,Prabhu Nandan Sahu,Nilesh Kumar Sahu,Haroon R Lone*

Main category: cs.CL

TL;DR: EmoSApp是一款基于智能手机的离线对话应用，专为心理健康和情感支持设计，利用优化的大型语言模型在设备端运行。


<details>
  <summary>Details</summary>
Motivation: 解决数字心理健康平台在用户可访问性、网络连接和数据隐私方面的挑战。

Method: 使用LLaMA-3.2-1B-Instruct模型，通过自定义心理健康知识数据集和多轮对话数据进行微调，并在资源受限设备上部署。

Result: 定性评估显示应用能提供连贯、共情的对话和针对性建议；定量评估验证了模型在低资源环境中的有效性。

Conclusion: EmoSApp为便携、安全和定制化的AI心理健康解决方案提供了范例。

Abstract: Mental health plays a crucial role in the overall well-being of an
individual. In recent years, digital platforms have been increasingly used to
expand mental health and emotional support. However, there are persistent
challenges related to limited user accessibility, internet connectivity, and
data privacy, which highlight the need for an offline, smartphone-based
solution. To address these challenges, we propose EmoSApp (Emotional Support
App): an entirely offline, smartphone-based conversational app designed for
mental health and emotional support. The system leverages Large Language Models
(LLMs), specifically fine-tuned, quantized and deployed using Torchtune and
Executorch for resource-constrained devices, allowing all inferences to occur
on the smartphone. To equip EmoSApp with robust domain expertise, we fine-tuned
the LLaMA-3.2-1B-Instruct model on our custom curated ``Knowledge dataset'' of
14,582 mental-health QA pairs, along with the multi-turn conversational data.
  Through qualitative human evaluation with the student population, we
demonstrate that EmoSApp has the ability to respond coherently, empathetically,
maintain interactive dialogue, and provide relevant suggestions to user's
mental health problems. Additionally, quantitative evaluations on nine standard
commonsense and reasoning benchmarks demonstrate the efficacy of our
fine-tuned, quantized model in low-resource settings. By prioritizing on-device
deployment and specialized domain adaptation, EmoSApp serves as a blueprint for
future innovations in portable, secure, and highly tailored AI-driven mental
health solutions.

</details>


### [40] [Transforming Sensitive Documents into Quantitative Data: An AI-Based Preprocessing Toolchain for Structured and Privacy-Conscious Analysis](https://arxiv.org/abs/2507.10582)
*Anders Ledberg,Anna Thalén*

Main category: cs.CL

TL;DR: 提出了一种模块化工具链，用于处理非结构化文本数据，支持隐私敏感的大规模研究。


<details>
  <summary>Details</summary>
Motivation: 法律、医疗和行政文本是公共健康和社会科学研究的重要资源，但敏感信息和结构异质性限制了其利用。

Method: 使用开源模型在本地硬件上处理文本，包括标准化、摘要、翻译和匿名化。

Result: 在瑞典法院判决数据集上验证了工具链的有效性，成功匿名化并保留语义内容。

Conclusion: 该工具链为隐私敏感领域的大规模文本分析提供了新可能。

Abstract: Unstructured text from legal, medical, and administrative sources offers a
rich but underutilized resource for research in public health and the social
sciences. However, large-scale analysis is hampered by two key challenges: the
presence of sensitive, personally identifiable information, and significant
heterogeneity in structure and language. We present a modular toolchain that
prepares such text data for embedding-based analysis, relying entirely on
open-weight models that run on local hardware, requiring only a
workstation-level GPU and supporting privacy-sensitive research.
  The toolchain employs large language model (LLM) prompting to standardize,
summarize, and, when needed, translate texts to English for greater
comparability. Anonymization is achieved via LLM-based redaction, supplemented
with named entity recognition and rule-based methods to minimize the risk of
disclosure. We demonstrate the toolchain on a corpus of 10,842 Swedish court
decisions under the Care of Abusers Act (LVM), comprising over 56,000 pages.
Each document is processed into an anonymized, standardized summary and
transformed into a document-level embedding. Validation, including manual
review, automated scanning, and predictive evaluation shows the toolchain
effectively removes identifying information while retaining semantic content.
As an illustrative application, we train a predictive model using embedding
vectors derived from a small set of manually labeled summaries, demonstrating
the toolchain's capacity for semi-automated content analysis at scale.
  By enabling structured, privacy-conscious analysis of sensitive documents,
our toolchain opens new possibilities for large-scale research in domains where
textual data was previously inaccessible due to privacy and heterogeneity
constraints.

</details>


### [41] [A Taxonomy for Design and Evaluation of Prompt-Based Natural Language Explanations](https://arxiv.org/abs/2507.10585)
*Isar Nejadgholi,Mona Omidyeganeh,Marc-Antoine Drouin,Jonathan Boisvert*

Main category: cs.CL

TL;DR: 论文提出了一种针对自然语言解释（NLEs）的更新版XAI分类法，以支持透明AI系统的治理。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型的兴起，NLEs成为解释模型行为的关键，需要对其特性和治理影响进行系统研究。

Method: 基于可解释AI（XAI）文献，构建了一个适应提示型NLEs的三维分类法：上下文、生成与呈现、评估。

Result: 分类法为研究人员、审计者和政策制定者提供了描述、设计和改进NLEs的框架。

Conclusion: 该分类法有助于提升AI系统的透明性，支持更有效的AI治理。

Abstract: Effective AI governance requires structured approaches for stakeholders to
access and verify AI system behavior. With the rise of large language models,
Natural Language Explanations (NLEs) are now key to articulating model
behavior, which necessitates a focused examination of their characteristics and
governance implications. We draw on Explainable AI (XAI) literature to create
an updated XAI taxonomy, adapted to prompt-based NLEs, across three dimensions:
(1) Context, including task, data, audience, and goals; (2) Generation and
Presentation, covering generation methods, inputs, interactivity, outputs, and
forms; and (3) Evaluation, focusing on content, presentation, and user-centered
properties, as well as the setting of the evaluation. This taxonomy provides a
framework for researchers, auditors, and policymakers to characterize, design,
and enhance NLEs for transparent AI systems.

</details>


### [42] [AutoRAG-LoRA: Hallucination-Triggered Knowledge Retuning via Lightweight Adapters](https://arxiv.org/abs/2507.10586)
*Kaushik Dwivedi,Padmanabh Patanjali Mishra*

Main category: cs.CL

TL;DR: AutoRAG-LoRA是一个模块化框架，通过轻量级LoRA适配器和KL正则化训练减少大语言模型的幻觉问题。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在自然语言任务中表现出色，但存在幻觉问题（事实不准确），影响实际部署的信任度。

Method: 结合自动提示重写、混合检索和低秩适配器调优，通过幻觉检测模块和反馈校正循环提升事实准确性。

Result: AutoRAG-LoRA显著减少了事实漂移，同时保持了模型的效率和模块化。

Conclusion: 该框架有效解决了大语言模型的幻觉问题，提升了生成内容的可信度。

Abstract: Large Language Models (LLMs) have demonstrated remarkable fluency across a
range of natural language tasks, yet remain vulnerable to hallucinations -
factual inaccuracies that undermine trust in real world deployment. We present
AutoRAG-LoRA, a modular framework for Retrieval-Augmented Generation (RAG) that
tackles hallucination in large language models through lightweight LoRA-based
adapters and KL-regularized training. Our pipeline integrates automated prompt
rewriting, hybrid retrieval, and low-rank adapter tuning to ground responses in
retrieved evidence. A hallucination detection module, using both
classifier-based and self-evaluation techniques, assigns confidence scores to
generated outputs, triggering an optional feedback correction loop. This loop
enforces factual alignment via contrastive KL loss and adapter fine tuning. We
demonstrate that AutoRAG-LoRA significantly reduces the factual drift while
preserving the efficiency and modularity of the model.

</details>


### [43] [Anthropomimetic Uncertainty: What Verbalized Uncertainty in Language Models is Missing](https://arxiv.org/abs/2507.10587)
*Dennis Ulmer,Alexandra Lorson,Ivan Titov,Christian Hardmeier*

Main category: cs.CL

TL;DR: 论文探讨了大型语言模型（LLM）输出中过度自信的问题，提出通过语言化不确定性（verbalized uncertainty）来增强用户信任，并呼吁研究更贴近人类沟通方式的不确定性表达。


<details>
  <summary>Details</summary>
Motivation: LLM输出常表现出过度自信，影响用户信任，需通过语言化不确定性改善这一问题。

Method: 综述人类不确定性沟通研究，分析现有数据偏差，提出拟人化不确定性（anthropomimetic uncertainty）的概念。

Result: 揭示了语言化不确定性中的偏差，并提出了未来研究方向。

Conclusion: 拟人化不确定性是提升LLM信任的关键，需进一步研究其实现方式。

Abstract: Human users increasingly rely on natural language interactions with large
language models (LLMs) in order to receive help on a large variety of tasks and
problems. However, the trustworthiness and perceived legitimacy of LLMs is
undermined by the fact that their output is frequently stated in very confident
terms, even when its accuracy is questionable. Therefore, there is a need to
signal the confidence of the language model to a user in order to reap the
benefits of human-machine collaboration and mitigate potential harms.
Verbalized uncertainty is the expression of confidence with linguistic means,
an approach that integrates perfectly into language-based interfaces.
Nevertheless, most recent research in natural language processing (NLP)
overlooks the nuances surrounding human uncertainty communication and the data
biases that influence machine uncertainty communication. We argue for
anthropomimetic uncertainty, meaning that intuitive and trustworthy uncertainty
communication requires a degree of linguistic authenticity and personalization
to the user, which could be achieved by emulating human communication. We
present a thorough overview over the research in human uncertainty
communication, survey ongoing research, and perform additional analyses to
demonstrate so-far overlooked biases in verbalized uncertainty. We conclude by
pointing out unique factors in human-machine communication of uncertainty and
deconstruct anthropomimetic uncertainty into future research directions for
NLP.

</details>


### [44] [PLEX: Perturbation-free Local Explanations for LLM-Based Text Classification](https://arxiv.org/abs/2507.10596)
*Yogachandran Rahulamathavan,Misbah Farooq,Varuna De Silva*

Main category: cs.CL

TL;DR: PLEX是一种无需扰动的局部解释方法，通过利用LLM的上下文嵌入和Siamese网络，显著提高了解释效率，同时与LIME和SHAP效果相当。


<details>
  <summary>Details</summary>
Motivation: LLMs在文本分类中表现优异，但其复杂性导致解释性差，现有XAI方法（如LIME和SHAP）计算成本高。

Method: 提出PLEX方法，利用LLM的上下文嵌入和Siamese网络，避免扰动，实现高效解释。

Result: 在四个分类任务中，PLEX与LIME和SHAP的一致性超过92%，且计算效率提升显著。

Conclusion: PLEX为LLM的文本分类提供了一种高效且解释性强的解决方案。

Abstract: Large Language Models (LLMs) excel in text classification, but their
complexity hinders interpretability, making it difficult to understand the
reasoning behind their predictions. Explainable AI (XAI) methods like LIME and
SHAP offer local explanations by identifying influential words, but they rely
on computationally expensive perturbations. These methods typically generate
thousands of perturbed sentences and perform inferences on each, incurring a
substantial computational burden, especially with LLMs. To address this, we
propose \underline{P}erturbation-free \underline{L}ocal \underline{Ex}planation
(PLEX), a novel method that leverages the contextual embeddings extracted from
the LLM and a ``Siamese network" style neural network trained to align with
feature importance scores. This one-off training eliminates the need for
subsequent perturbations, enabling efficient explanations for any new sentence.
We demonstrate PLEX's effectiveness on four different classification tasks
(sentiment, fake news, fake COVID-19 news and depression), showing more than
92\% agreement with LIME and SHAP. Our evaluation using a ``stress test"
reveals that PLEX accurately identifies influential words, leading to a similar
decline in classification accuracy as observed with LIME and SHAP when these
words are removed. Notably, in some cases, PLEX demonstrates superior
performance in capturing the impact of key features. PLEX dramatically
accelerates explanation, reducing time and computational overhead by two and
four orders of magnitude, respectively. This work offers a promising solution
for explainable LLM-based text classification.

</details>


### [45] [Emergence of Hierarchical Emotion Organization in Large Language Models](https://arxiv.org/abs/2507.10599)
*Bo Zhao,Maya Okawa,Eric J. Bigelow,Rose Yu,Tomer Ullman,Ekdeep Singh Lubana,Hidenori Tanaka*

Main category: cs.CL

TL;DR: LLMs自然形成与人类心理模型一致的情感层次结构，但存在社会经济角色的系统性偏见。


<details>
  <summary>Details</summary>
Motivation: 理解LLMs如何建模用户情感状态，以支持伦理部署。

Method: 基于情感轮理论，分析模型输出中情感状态的概率依赖关系。

Result: LLMs形成层次化情感树，且模型越大层次越复杂；情感识别存在社会经济偏见。

Conclusion: LLMs内化了社会感知，认知理论可用于改进模型评估。

Abstract: As large language models (LLMs) increasingly power conversational agents,
understanding how they model users' emotional states is critical for ethical
deployment. Inspired by emotion wheels -- a psychological framework that argues
emotions organize hierarchically -- we analyze probabilistic dependencies
between emotional states in model outputs. We find that LLMs naturally form
hierarchical emotion trees that align with human psychological models, and
larger models develop more complex hierarchies. We also uncover systematic
biases in emotion recognition across socioeconomic personas, with compounding
misclassifications for intersectional, underrepresented groups. Human studies
reveal striking parallels, suggesting that LLMs internalize aspects of social
perception. Beyond highlighting emergent emotional reasoning in LLMs, our
results hint at the potential of using cognitively-grounded theories for
developing better model evaluations.

</details>


### [46] [Language Models for Adult Service Website Text Analysis](https://arxiv.org/abs/2507.10743)
*Nickolas Freeman,Thanh Nguyen,Gregory Bott,Jason Parton,Collin Francel*

Main category: cs.CL

TL;DR: 该论文研究了成人服务网站（ASW）广告文本的语言建模方法，以帮助识别性交易受害者。通过定制化Transformer模型，在有限GPU资源下实现了高效性能，并在多项任务中表现优于现有模型。


<details>
  <summary>Details</summary>
Motivation: 成人服务网站（ASW）广告文本因使用表情符号、语法混乱和故意模糊化，难以分析。研究旨在开发高效的语言模型，以支持性交易受害者的识别。

Method: 研究比较了多种语言建模方法，包括信息检索、预训练Transformer和定制化Transformer模型。定制模型在有限资源下训练，并在ASW文本分析任务中表现优异。

Result: 定制化Transformer模型在准确性、召回率、F1分数和ROC AUC上优于BERT-base、RoBERTa和ModernBERT等模型。

Conclusion: 定制化模型为ASW文本分析提供了显著进步，可应用于下游任务和研究。

Abstract: Sex trafficking refers to the use of force, fraud, or coercion to compel an
individual to perform in commercial sex acts against their will. Adult service
websites (ASWs) have and continue to be linked to sex trafficking, offering a
platform for traffickers to advertise their victims. Thus, organizations
involved in the fight against sex trafficking often use ASW data when
attempting to identify potential sex trafficking victims. A critical challenge
in transforming ASW data into actionable insight is text analysis. Previous
research using ASW data has shown that ASW ad text is important for linking
ads. However, working with this text is challenging due to its extensive use of
emojis, poor grammar, and deliberate obfuscation to evade law enforcement
scrutiny. We conduct a comprehensive study of language modeling approaches for
this application area, including simple information retrieval methods,
pre-trained transformers, and custom transformer models. We demonstrate that
characteristics of ASW text data allow efficient custom transformer models to
be trained with relatively small GPU resources and used efficiently for
inference on consumer hardware. Our custom models outperform fine-tuned
variants of well-known encoder-only transformer models, including BERT-base,
RoBERTa, and ModernBERT, on accuracy, recall, F1 score, and ROC AUC. We
demonstrate the use of our best-performing custom configuration on three tasks
related to ASW data analysis: (i) decomposing the giant component in a graph
representation of ASW data, (ii) clustering ASW ad text, and (iii) using the
learned token embeddings to understand the use of emojis in the illicit context
we study. The models we develop represent a significant advancement in ASW text
analysis, which can be leveraged in a variety of downstream applications and
research.

</details>


### [47] [Applying Text Embedding Models for Efficient Analysis in Labeled Property Graphs](https://arxiv.org/abs/2507.10772)
*Michal Podstawski*

Main category: cs.CL

TL;DR: 利用预训练文本嵌入模型增强标记属性图的语义分析，提升节点分类和关系预测任务。


<details>
  <summary>Details</summary>
Motivation: 标记属性图中丰富的文本属性未被充分利用，希望通过语义分析增强其分析能力。

Method: 嵌入文本节点和边属性，将语言模型嵌入集成到图流程中，不改变图结构。

Result: 文本语义显著提高了属性图分析的准确性和可解释性。

Conclusion: 文本嵌入模型能有效增强属性图的语义分析能力。

Abstract: Labeled property graphs often contain rich textual attributes that can
enhance analytical tasks when properly leveraged. This work explores the use of
pretrained text embedding models to enable efficient semantic analysis in such
graphs. By embedding textual node and edge properties, we support downstream
tasks including node classification and relation prediction with improved
contextual understanding. Our approach integrates language model embeddings
into the graph pipeline without altering its structure, demonstrating that
textual semantics can significantly enhance the accuracy and interpretability
of property graph analysis.

</details>


### [48] [Can Multimodal Foundation Models Understand Schematic Diagrams? An Empirical Study on Information-Seeking QA over Scientific Papers](https://arxiv.org/abs/2507.10787)
*Yilun Zhao,Chengye Wang,Chuhan Li,Arman Cohan*

Main category: cs.CL

TL;DR: MISS-QA是首个评估模型解读科学文献中示意图能力的基准，包含1,500个专家标注示例。测试了18种前沿多模态基础模型，发现与人类专家存在显著性能差距。


<details>
  <summary>Details</summary>
Motivation: 科学文献中的示意图包含丰富信息，但现有模型在解读能力上存在不足，需专门基准进行评估和改进。

Method: 构建MISS-QA基准，包含1,500个标注示例，测试18种多模态模型（如o4-mini、Gemini-2.5-Flash等），分析模型在不可回答问题上的表现及错误。

Result: 模型与人类专家在MISS-QA上存在显著性能差距，错误分析揭示了当前模型的局限性。

Conclusion: MISS-QA为提升模型理解多模态科学文献能力提供了关键见解，未来需进一步优化模型。

Abstract: This paper introduces MISS-QA, the first benchmark specifically designed to
evaluate the ability of models to interpret schematic diagrams within
scientific literature. MISS-QA comprises 1,500 expert-annotated examples over
465 scientific papers. In this benchmark, models are tasked with interpreting
schematic diagrams that illustrate research overviews and answering
corresponding information-seeking questions based on the broader context of the
paper. We assess the performance of 18 frontier multimodal foundation models,
including o4-mini, Gemini-2.5-Flash, and Qwen2.5-VL. We reveal a significant
performance gap between these models and human experts on MISS-QA. Our analysis
of model performance on unanswerable questions and our detailed error analysis
further highlight the strengths and limitations of current models, offering key
insights to enhance models in comprehending multimodal scientific literature.

</details>


### [49] [Testing Hypotheses from the Social Approval Theory of Online Hate: An Analysis of 110 Million Posts from Parler](https://arxiv.org/abs/2507.10810)
*David M. Markowitz,Samuel Hardman Taylor*

Main category: cs.CL

TL;DR: 研究发现，社交平台上的仇恨言论并未因获得社交认可（如点赞）而显著增加或变得更极端。


<details>
  <summary>Details</summary>
Motivation: 探讨社交认可如何激励在线仇恨言论，验证Walther的社会认可理论。

Method: 分析Parler平台上1.1亿条帖子（2018-2021），考察点赞数与后续仇恨言论的关系。

Result: 点赞数与后续仇恨言论无显著关联，甚至在某些时段呈负相关。

Conclusion: 社交认可对仇恨言论的强化机制在特定平台上可能不同。

Abstract: In this paper, we explored how online hate is motivated by receiving social
approval from others. We specifically examined two central tenets of Walther's
(2024) social approval theory of online hate: (H1a) more signals of social
approval on hate messages predicts more subsequent hate messages, and (H1b) as
social approval increases, hate speech messages become more extreme. Using over
110 million posts from Parler (2018-2021), we observed that the number of
upvotes a person received on a hate speech post was unassociated with the
amount of hate speech in their next post and posts during the next week, month,
three months, and six months. Between-person effects revealed an average
negative relationship between social approval and hate speech production at the
post level, but this relationship was mixed at other time intervals. Social
approval reinforcement mechanisms of online hate may operate differently on
niche social media platforms.

</details>


### [50] [LLMs on Trial: Evaluating Judicial Fairness for Large Language Models](https://arxiv.org/abs/2507.10852)
*Yiran Hu,Zongyue Xue,Haitao Li,Siyuan Zheng,Qingjing Chen,Shaochun Wang,Xihan Zhang,Ning Zheng,Yun Liu,Qingyao Ai,Yiqun Liu,Charles L. A. Clarke,Weixing Shen*

Main category: cs.CL

TL;DR: 论文研究了大型语言模型（LLMs）在司法系统中的公平性，提出了一个评估框架和数据集，发现LLMs普遍存在不一致性、偏见和不平衡的准确性。


<details>
  <summary>Details</summary>
Motivation: LLMs在司法等高风险领域的应用日益增多，但其公平性及对社会正义的影响尚未充分研究。

Method: 基于司法公平理论构建评估框架，开发三个指标（不一致性、偏见、不平衡准确性），并在16个LLMs上进行实验。

Result: 实验显示LLMs普遍存在不公平性，尤其在人口统计标签上偏见更明显。调整温度参数可影响公平性，但模型规模、发布时间和来源国无显著影响。

Conclusion: 论文提出了一个公开工具包，支持未来研究和改进LLM公平性。

Abstract: Large Language Models (LLMs) are increasingly used in high-stakes fields
where their decisions impact rights and equity. However, LLMs' judicial
fairness and implications for social justice remain underexplored. When LLMs
act as judges, the ability to fairly resolve judicial issues is a prerequisite
to ensure their trustworthiness. Based on theories of judicial fairness, we
construct a comprehensive framework to measure LLM fairness, leading to a
selection of 65 labels and 161 corresponding values. Applying this framework to
the judicial system, we compile an extensive dataset, JudiFair, comprising
177,100 unique case facts. To achieve robust statistical inference, we develop
three evaluation metrics, inconsistency, bias, and imbalanced inaccuracy, and
introduce a method to assess the overall fairness of multiple LLMs across
various labels. Through experiments with 16 LLMs, we uncover pervasive
inconsistency, bias, and imbalanced inaccuracy across models, underscoring
severe LLM judicial unfairness. Particularly, LLMs display notably more
pronounced biases on demographic labels, with slightly less bias on substance
labels compared to procedure ones. Interestingly, increased inconsistency
correlates with reduced biases, but more accurate predictions exacerbate
biases. While we find that adjusting the temperature parameter can influence
LLM fairness, model size, release date, and country of origin do not exhibit
significant effects on judicial fairness. Accordingly, we introduce a publicly
available toolkit containing all datasets and code, designed to support future
research in evaluating and improving LLM fairness.

</details>


### [51] [How Stylistic Similarity Shapes Preferences in Dialogue Dataset with User and Third Party Evaluations](https://arxiv.org/abs/2507.10918)
*Ikumi Numaya,Shoji Moriya,Shiki Sato,Reina Akama,Jun Suzuki*

Main category: cs.CL

TL;DR: 论文研究了对话生成中用户与系统风格相似性对用户偏好的影响，区分了主观与客观相似性，并发现主观相似性与用户偏好强相关。


<details>
  <summary>Details</summary>
Motivation: 探讨用户与系统风格相似性对用户印象的影响，区分主观与客观相似性的差异。

Method: 构建包含用户偏好、主观风格相似性和客观风格相似性的新数据集，并进行分析。

Result: 主观风格相似性与用户偏好强相关，且主观与客观相似性存在显著差异。

Conclusion: 强调区分主观与客观评价的重要性，数据集已公开。

Abstract: Recent advancements in dialogue generation have broadened the scope of
human-bot interactions, enabling not only contextually appropriate responses
but also the analysis of human affect and sensitivity. While prior work has
suggested that stylistic similarity between user and system may enhance user
impressions, the distinction between subjective and objective similarity is
often overlooked. To investigate this issue, we introduce a novel dataset that
includes users' preferences, subjective stylistic similarity based on users'
own perceptions, and objective stylistic similarity annotated by third party
evaluators in open-domain dialogue settings. Analysis using the constructed
dataset reveals a strong positive correlation between subjective stylistic
similarity and user preference. Furthermore, our analysis suggests an important
finding: users' subjective stylistic similarity differs from third party
objective similarity. This underscores the importance of distinguishing between
subjective and objective evaluations and understanding the distinct aspects
each captures when analyzing the relationship between stylistic similarity and
user preferences. The dataset presented in this paper is available online.

</details>


### [52] [HanjaBridge: Resolving Semantic Ambiguity in Korean LLMs via Hanja-Augmented Pre-Training](https://arxiv.org/abs/2507.10920)
*Seungho Choi*

Main category: cs.CL

TL;DR: 提出HanjaBridge方法，通过注入汉字意义解决韩语同音词歧义，显著提升低资源语言模型性能。


<details>
  <summary>Details</summary>
Motivation: 解决大语言模型在韩语等低资源语言中因同音汉字词歧义导致的性能问题。

Method: 结合持续预训练框架，引入HanjaBridge技术，为同音词提供所有可能的汉字候选，结合知识蒸馏避免灾难性遗忘。

Result: 在KoBALT基准上相对提升21%，并观察到中韩跨语言迁移的积极效果。

Conclusion: HanjaBridge有效提升韩语理解能力，且无需推理时额外成本。

Abstract: Large language models (LLMs) often show poor performance in low-resource
languages like Korean, partly due to unique linguistic challenges such as
homophonous Sino-Korean words that are indistinguishable in Hangul script. To
address this semantic ambiguity, we propose HanjaBridge, a novel
meaning-injection technique integrated into a continual pre-training (CPT)
framework. Instead of deterministically mapping a word to a single Hanja
(Chinese character), HanjaBridge presents the model with all possible Hanja
candidates for a given homograph, encouraging the model to learn contextual
disambiguation. This process is paired with token-level knowledge distillation
to prevent catastrophic forgetting. Experimental results show that HanjaBridge
significantly improves Korean language understanding, achieving a 21\% relative
improvement on the KoBALT benchmark. Notably, by reinforcing semantic alignment
between Korean and Chinese through shared Hanja, we observe a strong positive
cross-lingual transfer. Furthermore, these gains persist even when Hanja
augmentation is omitted at inference time, ensuring practical efficiency with
no additional run-time cost.

</details>


### [53] [Modeling Understanding of Story-Based Analogies Using Large Language Models](https://arxiv.org/abs/2507.10957)
*Kalit Inani,Keshav Kabra,Vijay Marupudi,Sashank Varma*

Main category: cs.CL

TL;DR: 研究探讨了大型语言模型（LLMs）在类比推理任务中与人类表现的对比，重点评估了语义表示和显式提示的效果，并考察了模型规模和架构的影响。


<details>
  <summary>Details</summary>
Motivation: 了解LLMs在类比推理任务中是否接近人类表现，以及其推理能力的局限性。

Method: 通过故事类比喻任务，评估LLMs的语义表示和显式提示效果，比较不同模型规模和架构的表现。

Result: LLMs在类比推理任务中表现接近人类，但缺乏人类般的稳健推理能力；模型规模和架构对性能有显著影响。

Conclusion: LLMs在类比推理方面有潜力，但仍需改进以更贴近人类推理能力。

Abstract: Recent advancements in Large Language Models (LLMs) have brought them closer
to matching human cognition across a variety of tasks. How well do these models
align with human performance in detecting and mapping analogies? Prior research
has shown that LLMs can extract similarities from analogy problems but lack
robust human-like reasoning. Building on Webb, Holyoak, and Lu (2023), the
current study focused on a story-based analogical mapping task and conducted a
fine-grained evaluation of LLM reasoning abilities compared to human
performance. First, it explored the semantic representation of analogies in
LLMs, using sentence embeddings to assess whether they capture the similarity
between the source and target texts of an analogy, and the dissimilarity
between the source and distractor texts. Second, it investigated the
effectiveness of explicitly prompting LLMs to explain analogies. Throughout, we
examine whether LLMs exhibit similar performance profiles to those observed in
humans by evaluating their reasoning at the level of individual analogies, and
not just at the level of overall accuracy (as prior studies have done). Our
experiments include evaluating the impact of model size (8B vs. 70B parameters)
and performance variation across state-of-the-art model architectures such as
GPT-4 and LLaMA3. This work advances our understanding of the analogical
reasoning abilities of LLMs and their potential as models of human reasoning.

</details>


### [54] [DS@GT at eRisk 2025: From prompts to predictions, benchmarking early depression detection with conversational agent based assessments and temporal attention models](https://arxiv.org/abs/2507.10958)
*Anthony Miyaguchi,David Guecha,Yuwen Chiu,Sidharth Gaur*

Main category: cs.CL

TL;DR: DS@GT团队在eRisk 2025挑战赛中采用提示工程策略，利用多种LLM进行BDI-II评估，生成结构化JSON输出，并在缺乏真实标签的情况下评估模型间一致性和内部一致性。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于利用大型语言模型（LLM）进行对话式抑郁检测，并探索提示设计对模型输出的影响。

Method: 采用提示工程策略，设计多样化的提示，使LLM生成符合BDI-II标准的JSON输出，并分析对话线索对症状预测的影响。

Result: 最佳提交在官方排行榜上排名第二，指标为DCHR=0.50、ADODL=0.89、ASHR=0.27。

Conclusion: 提示设计方法有效，模型输出与BDI-II标准一致，且能分析对话线索对抑郁症状预测的影响。

Abstract: This Working Note summarizes the participation of the DS@GT team in two eRisk
2025 challenges. For the Pilot Task on conversational depression detection with
large language-models (LLMs), we adopted a prompt-engineering strategy in which
diverse LLMs conducted BDI-II-based assessments and produced structured JSON
outputs. Because ground-truth labels were unavailable, we evaluated cross-model
agreement and internal consistency. Our prompt design methodology aligned model
outputs with BDI-II criteria and enabled the analysis of conversational cues
that influenced the prediction of symptoms. Our best submission, second on the
official leaderboard, achieved DCHR = 0.50, ADODL = 0.89, and ASHR = 0.27.

</details>


### [55] [Mario at EXIST 2025: A Simple Gateway to Effective Multilingual Sexism Detection](https://arxiv.org/abs/2507.10996)
*Lin Tian,Johanne R. Trippas,Marian-Andrei Rizoiu*

Main category: cs.CL

TL;DR: 本文提出了一种基于分层低秩适应（LoRA）的方法，用于检测英语和西班牙语推文中的性别歧视，通过条件适配器路由显式建模标签依赖关系，实现了高效的多语言训练和性能提升。


<details>
  <summary>Details</summary>
Motivation: 解决文本性别歧视检测任务，特别是在多语言环境中，同时减少计算和存储资源的需求。

Method: 采用分层LoRA适配器，对所有线性变换进行适应，利用条件适配器路由显式建模标签依赖关系，并通过多语言训练实现跨语言迁移。

Result: 在仅使用1.67%可训练参数的情况下，性能接近全微调，训练时间减少75%，模型存储减少98%，并在所有子任务中表现优异。

Conclusion: 分层LoRA适配器是一种高效且性能优越的方法，适用于多语言性别歧视检测任务。

Abstract: This paper presents our approach to EXIST 2025 Task 1, addressing text-based
sexism detection in English and Spanish tweets through hierarchical Low-Rank
Adaptation (LoRA) of Llama 3.1 8B. Our method introduces conditional adapter
routing that explicitly models label dependencies across three hierarchically
structured subtasks: binary sexism identification, source intention detection,
and multilabel sexism categorization. Unlike conventional LoRA applications
that target only attention layers, we apply adaptation to all linear
transformations, enhancing the model's capacity to capture task-specific
patterns. In contrast to complex data processing and ensemble approaches, we
show that straightforward parameter-efficient fine-tuning achieves strong
performance. We train separate LoRA adapters (rank=16, QLoRA 4-bit) for each
subtask using unified multilingual training that leverages Llama 3.1's native
bilingual capabilities. The method requires minimal preprocessing and uses
standard supervised learning. Our multilingual training strategy eliminates the
need for separate language-specific models, achieving 1.7-2.4\% F1 improvements
through cross-lingual transfer. With only 1.67\% trainable parameters compared
to full fine-tuning, our approach reduces training time by 75\% and model
storage by 98\%, while achieving competitive performance across all subtasks
(ICM-Hard: 0.6774 for binary classification, 0.4991 for intention detection,
0.6519 for multilabel categorization).

</details>


### [56] [Team HUMANE at AVeriTeC 2025: HerO 2 for Efficient Fact Verification](https://arxiv.org/abs/2507.11004)
*Yejun Yoon,Jaeyoon Jung,Seunghyun Yoon,Kunwoo Park*

Main category: cs.CL

TL;DR: HerO 2是HUMANE团队为FEVER-25研讨会AVeriTeC共享任务开发的系统，改进了证据质量和验证预测，并优化了性能，最终在排行榜上排名第二。


<details>
  <summary>Details</summary>
Motivation: 改进去年的最佳开源模型HerO，提升证据质量、验证预测效率和系统性能，以适应实际应用需求。

Method: 通过文档摘要和答案重构改进证据质量，利用后训练量化优化验证预测，并集成更新的语言模型主干。

Result: 在排行榜上排名第二，运行时间最短，展示了高效性和实际应用的潜力。

Conclusion: HerO 2在效率和性能上均有显著提升，适用于实际事实验证任务。

Abstract: This paper presents HerO 2, Team HUMANE's system for the AVeriTeC shared task
at the FEVER-25 workshop. HerO 2 is an enhanced version of HerO, the
best-performing open-source model from the previous year's challenge. It
improves evidence quality through document summarization and answer
reformulation, optimizes veracity prediction via post-training quantization
under computational constraints, and enhances overall system performance by
integrating updated language model (LM) backbones. HerO 2 ranked second on the
leaderboard while achieving the shortest runtime among the top three systems,
demonstrating both high efficiency and strong potential for real-world fact
verification. The code is available at https://github.com/ssu-humane/HerO2.

</details>


### [57] [Journalism-Guided Agentic In-Context Learning for News Stance Detection](https://arxiv.org/abs/2507.11049)
*Dahyun Lee,Jonghyeon Choi,Jiyoung Han,Kunwoo Park*

Main category: cs.CL

TL;DR: 论文提出了首个韩语新闻立场检测数据集K-News-Stance，并开发了JoA-ICL框架，通过分段立场检测提升长新闻的立场识别效果，实验证明其优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 在线新闻消费增长导致推荐系统可能加剧信息茧房和政治极化，现有立场检测研究局限于短文本和高资源语言。

Method: 提出K-News-Stance数据集和JoA-ICL框架，利用语言模型代理检测关键段落立场并聚合为文章整体立场。

Result: JoA-ICL在立场检测上优于现有方法，案例研究显示其在新闻推荐和媒体偏见分析中的实用性。

Conclusion: 分段立场检测框架JoA-ICL能有效提升长新闻立场识别，促进新闻多样性和媒体偏见分析。

Abstract: As online news consumption grows, personalized recommendation systems have
become integral to digital journalism. However, these systems risk reinforcing
filter bubbles and political polarization by failing to incorporate diverse
perspectives. Stance detection -- identifying a text's position on a target --
can help mitigate this by enabling viewpoint-aware recommendations and
data-driven analyses of media bias. Yet, existing stance detection research
remains largely limited to short texts and high-resource languages. To address
these gaps, we introduce \textsc{K-News-Stance}, the first Korean dataset for
article-level stance detection, comprising 2,000 news articles with
article-level and 19,650 segment-level stance annotations across 47 societal
issues. We also propose \textsc{JoA-ICL}, a \textbf{Jo}urnalism-guided
\textbf{A}gentic \textbf{I}n-\textbf{C}ontext \textbf{L}earning framework that
employs a language model agent to predict the stances of key structural
segments (e.g., leads, quotes), which are then aggregated to infer the overall
article stance. Experiments show that \textsc{JoA-ICL} outperforms existing
stance detection methods, highlighting the benefits of segment-level agency in
capturing the overall position of long-form news articles. Two case studies
further demonstrate its broader utility in promoting viewpoint diversity in
news recommendations and uncovering patterns of media bias.

</details>


### [58] [LLM-Augmented Symptom Analysis for Cardiovascular Disease Risk Prediction: A Clinical NLP](https://arxiv.org/abs/2507.11052)
*Haowei Yang,Ziyu Shen,Junli Shao,Luyao Men,Xinyue Han,Jing Dong*

Main category: cs.CL

TL;DR: 该研究提出了一种基于LLM增强的临床NLP流程，用于从非结构化临床笔记中提取心血管疾病早期指标，提升了预测模型的性能。


<details>
  <summary>Details</summary>
Motivation: 心血管疾病的及时识别和准确风险分层对降低全球死亡率至关重要，但现有预测模型主要依赖结构化数据，忽略了非结构化临床笔记中的有价值信息。

Method: 研究采用领域适应的大语言模型进行症状提取、上下文推理和相关性分析，结合心血管特定微调、基于提示的推理和实体感知推理。

Result: 在MIMIC-III和CARDIO-NLP数据集上的评估显示，该方法在精确率、召回率、F1分数和AUROC方面表现更优，临床相关性高（kappa = 0.82）。

Conclusion: 该研究展示了LLM在临床决策支持系统中的潜力，能够提升早期预警系统并将患者叙述转化为可操作的风险评估。

Abstract: Timely identification and accurate risk stratification of cardiovascular
disease (CVD) remain essential for reducing global mortality. While existing
prediction models primarily leverage structured data, unstructured clinical
notes contain valuable early indicators. This study introduces a novel
LLM-augmented clinical NLP pipeline that employs domain-adapted large language
models for symptom extraction, contextual reasoning, and correlation from
free-text reports. Our approach integrates cardiovascular-specific fine-tuning,
prompt-based inference, and entity-aware reasoning. Evaluations on MIMIC-III
and CARDIO-NLP datasets demonstrate improved performance in precision, recall,
F1-score, and AUROC, with high clinical relevance (kappa = 0.82) assessed by
cardiologists. Challenges such as contextual hallucination, which occurs when
plausible information contracts with provided source, and temporal ambiguity,
which is related with models struggling with chronological ordering of events
are addressed using prompt engineering and hybrid rule-based verification. This
work underscores the potential of LLMs in clinical decision support systems
(CDSS), advancing early warning systems and enhancing the translation of
patient narratives into actionable risk assessments.

</details>


### [59] [Social Media Sentiments Analysis on the July Revolution in Bangladesh: A Hybrid Transformer Based Machine Learning Approach](https://arxiv.org/abs/2507.11084)
*Md. Sabbir Hossen,Md. Saiduzzaman,Pabon Shaha*

Main category: cs.CL

TL;DR: 论文提出了一种基于Transformer的混合情感分析框架，用于分析孟加拉国七月革命期间社交媒体上的公众情绪，结合多种Transformer模型和机器学习分类器，取得了83.7%的高准确率。


<details>
  <summary>Details</summary>
Motivation: 研究旨在解码孟加拉国七月革命期间社交媒体上的公众情绪，以了解社会动态和民众诉求。

Method: 采用混合Transformer框架（包括BanglaBERT、mBERT、XLM-RoBERTa和提出的XMB-BERT），结合PCA降维和11种机器学习分类器进行情感分析。

Result: 提出的XMB-BERT与投票分类器组合达到了83.7%的准确率，优于其他模型。

Conclusion: 研究表明机器学习技术可以有效分析低资源语言（如孟加拉语）的社会情绪，为类似研究提供了参考。

Abstract: The July Revolution in Bangladesh marked a significant student-led mass
uprising, uniting people across the nation to demand justice, accountability,
and systemic reform. Social media platforms played a pivotal role in amplifying
public sentiment and shaping discourse during this historic mass uprising. In
this study, we present a hybrid transformer-based sentiment analysis framework
to decode public opinion expressed in social media comments during and after
the revolution. We used a brand new dataset of 4,200 Bangla comments collected
from social media. The framework employs advanced transformer-based feature
extraction techniques, including BanglaBERT, mBERT, XLM-RoBERTa, and the
proposed hybrid XMB-BERT, to capture nuanced patterns in textual data.
Principle Component Analysis (PCA) were utilized for dimensionality reduction
to enhance computational efficiency. We explored eleven traditional and
advanced machine learning classifiers for identifying sentiments. The proposed
hybrid XMB-BERT with the voting classifier achieved an exceptional accuracy of
83.7% and outperform other model classifier combinations. This study
underscores the potential of machine learning techniques to analyze social
sentiment in low-resource languages like Bangla.

</details>


### [60] [Beyond Traditional Algorithms: Leveraging LLMs for Accurate Cross-Border Entity Identification](https://arxiv.org/abs/2507.11086)
*Andres Azqueta-Gavaldón,Joaquin Ramos Cosgrove*

Main category: cs.CL

TL;DR: 论文探讨了在跨境金融活动中使用大型语言模型（LLMs）改进实体匹配的准确性，相比传统方法表现更优。


<details>
  <summary>Details</summary>
Motivation: 跨境金融活动增多，西班牙金融系统需准确识别和分类外国实体以管理风险、遵守法规并防止金融不当行为。传统匹配方法因语言差异、特殊字符等问题表现不佳。

Method: 评估了传统方法（如Jaccard、余弦、Levenshtein距离）、基于Hugging Face的LLMs和接口型LLMs（如Microsoft Copilot、阿里巴巴的Qwen 2.5），使用65个葡萄牙公司案例数据集。

Result: 传统方法准确率超92%，但假阳性率高（20-40%）；接口型LLMs表现更优，准确率超93%，F1分数超96%，假阳性率更低（40-80%）。

Conclusion: LLMs在实体匹配任务中优于传统方法，尤其在处理语言和上下文复杂性方面更具优势。

Abstract: The growing prevalence of cross-border financial activities in global markets
has underscored the necessity of accurately identifying and classifying foreign
entities. This practice is essential within the Spanish financial system for
ensuring robust risk management, regulatory adherence, and the prevention of
financial misconduct. This process involves a labor-intensive entity-matching
task, where entities need to be validated against available reference sources.
Challenges arise from linguistic variations, special characters, outdated
names, and changes in legal forms, complicating traditional matching algorithms
like Jaccard, cosine, and Levenshtein distances. These methods struggle with
contextual nuances and semantic relationships, leading to mismatches. To
address these limitations, we explore Large Language Models (LLMs) as a
flexible alternative. LLMs leverage extensive training to interpret context,
handle abbreviations, and adapt to legal transitions. We evaluate traditional
methods, Hugging Face-based LLMs, and interface-based LLMs (e.g., Microsoft
Copilot, Alibaba's Qwen 2.5) using a dataset of 65 Portuguese company cases.
Results show traditional methods achieve accuracies over 92% but suffer high
false positive rates (20-40%). Interface-based LLMs outperform, achieving
accuracies above 93%, F1 scores exceeding 96%, and lower false positives
(40-80%).

</details>


### [61] [The Devil behind the mask: An emergent safety vulnerability of Diffusion LLMs](https://arxiv.org/abs/2507.11097)
*Zichen Wen,Jiashu Qu,Dongrui Liu,Zhiyuan Liu,Ruixi Wu,Yicun Yang,Xiangqi Jin,Haoyun Xu,Xuyang Liu,Weijia Li,Chaochao Lu,Jing Shao,Conghui He,Linfeng Zhang*

Main category: cs.CL

TL;DR: DIJA是一种针对扩散式大语言模型（dLLMs）的越狱攻击框架，揭示了其独特的安全弱点，并通过实验验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有对齐机制无法保护dLLMs免受上下文感知的掩码输入对抗性提示攻击，暴露了新的安全漏洞。

Method: DIJA通过构造对抗性的交错掩码-文本提示，利用dLLMs的双向建模和平行解码机制，绕过标准对齐机制。

Result: DIJA在Dream-Instruct上实现了100%的关键词攻击成功率，显著优于现有方法。

Conclusion: 研究强调了在dLLMs中重新思考安全对齐的紧迫性。

Abstract: Diffusion-based large language models (dLLMs) have recently emerged as a
powerful alternative to autoregressive LLMs, offering faster inference and
greater interactivity via parallel decoding and bidirectional modeling.
However, despite strong performance in code generation and text infilling, we
identify a fundamental safety concern: existing alignment mechanisms fail to
safeguard dLLMs against context-aware, masked-input adversarial prompts,
exposing novel vulnerabilities. To this end, we present DIJA, the first
systematic study and jailbreak attack framework that exploits unique safety
weaknesses of dLLMs. Specifically, our proposed DIJA constructs adversarial
interleaved mask-text prompts that exploit the text generation mechanisms of
dLLMs, i.e., bidirectional modeling and parallel decoding. Bidirectional
modeling drives the model to produce contextually consistent outputs for masked
spans, even when harmful, while parallel decoding limits model dynamic
filtering and rejection sampling of unsafe content. This causes standard
alignment mechanisms to fail, enabling harmful completions in alignment-tuned
dLLMs, even when harmful behaviors or unsafe instructions are directly exposed
in the prompt. Through comprehensive experiments, we demonstrate that DIJA
significantly outperforms existing jailbreak methods, exposing a previously
overlooked threat surface in dLLM architectures. Notably, our method achieves
up to 100% keyword-based ASR on Dream-Instruct, surpassing the strongest prior
baseline, ReNeLLM, by up to 78.5% in evaluator-based ASR on JailbreakBench and
by 37.7 points in StrongREJECT score, while requiring no rewriting or hiding of
harmful content in the jailbreak prompt. Our findings underscore the urgent
need for rethinking safety alignment in this emerging class of language models.
Code is available at https://github.com/ZichenWen1/DIJA.

</details>


### [62] [Multi-Trigger Poisoning Amplifies Backdoor Vulnerabilities in LLMs](https://arxiv.org/abs/2507.11112)
*Sanhanat Sivapiromrat,Caiqi Zhang,Marco Basaldella,Nigel Collier*

Main category: cs.CL

TL;DR: 该论文研究了大型语言模型（LLMs）中多触发器数据投毒攻击的共存机制，并提出了一种基于层间权重差异分析的后处理恢复方法。


<details>
  <summary>Details</summary>
Motivation: 现有研究多关注单触发器攻击的有效性，缺乏对多触发器共存机制及其交互的理解，揭示了LLMs更广泛和持久的漏洞。

Method: 提出一个框架研究LLMs中的投毒行为，展示多触发器可共存且互不干扰，并通过高嵌入相似性实现鲁棒激活。

Result: 发现多触发器能在模型中独立存在，且即使令牌被替换或分隔，投毒触发器仍能稳定激活。

Conclusion: 提出一种选择性重训练方法，通过最小参数更新有效消除触发器行为，为多触发器投毒提供了实用防御方案。

Abstract: Recent studies have shown that Large Language Models (LLMs) are vulnerable to
data poisoning attacks, where malicious training examples embed hidden
behaviours triggered by specific input patterns. However, most existing works
assume a phrase and focus on the attack's effectiveness, offering limited
understanding of trigger mechanisms and how multiple triggers interact within
the model. In this paper, we present a framework for studying poisoning in
LLMs. We show that multiple distinct backdoor triggers can coexist within a
single model without interfering with each other, enabling adversaries to embed
several triggers concurrently. Using multiple triggers with high embedding
similarity, we demonstrate that poisoned triggers can achieve robust activation
even when tokens are substituted or separated by long token spans. Our findings
expose a broader and more persistent vulnerability surface in LLMs. To mitigate
this threat, we propose a post hoc recovery method that selectively retrains
specific model components based on a layer-wise weight difference analysis. Our
method effectively removes the trigger behaviour with minimal parameter
updates, presenting a practical and efficient defence against multi-trigger
poisoning.

</details>


### [63] [MSA at ImageCLEF 2025 Multimodal Reasoning: Multilingual Multimodal Reasoning With Ensemble Vision Language Models](https://arxiv.org/abs/2507.11114)
*Seif Ahmed,Mohamed T. Younes,Abdelrahman Moustafa,Abdelrahman Allam,Hamza Moustafa*

Main category: cs.CL

TL;DR: 提出了一种基于集成的多语言多模态推理系统，在ImageCLEF 2025 EXAMS V挑战中表现优异。


<details>
  <summary>Details</summary>
Motivation: 设计一个轻量级OCR-VLM集成系统，通过精确的提示策略和跨语言增强，在高风险多语言教育环境中超越更重的端到端模型。

Method: 集成Gemini 2.5 Flash、Gemini 1.5 Pro和Gemini 2.5 Pro，通过少量样本和零样本提示协调，并进行广泛的消融研究。

Result: 在官方排行榜上，系统在多语言赛道中以81.4%的准确率获得第一名，并在13个语言赛道中领先11个。

Conclusion: 轻量级OCR-VLM集成与精确提示策略和跨语言增强结合，能在多语言教育环境中超越更重的模型。

Abstract: We present a robust ensemble-based system for multilingual multimodal
reasoning, designed for the ImageCLEF 2025 EXAMS V challenge. Our approach
integrates Gemini 2.5 Flash for visual description, Gemini 1.5 Pro for caption
refinement and consistency checks, and Gemini 2.5 Pro as a reasoner which
handles final answer selection, all coordinated through carefully engineered
few-shot and zero-shot prompts. We conducted an extensive ablation study,
training several large language models (Gemini 2.5 Flash, Phi 4, Gemma 3,
Mistral) on an English dataset and its multilingual augmented version.
Additionally, we evaluated Gemini 2.5 Flash in a zero-shot setting for
comparison and found it to substantially outperform the trained models. Prompt
design also proved critical: enforcing concise, language-normalized formats and
prohibiting explanatory text boosted model accuracy on the English validation
set from 55.9% to 61.7%. On the official leaderboard, our system (Team MSA)
achieved first place overall in the multilingual track with 81.4% accuracy, and
led 11 out of 13 individual language tracks, with top results such as 95.07%
for Croatian and 92.12% for Italian. These findings highlight that lightweight
OCR-VLM ensembles, when paired with precise prompt strategies and cross-lingual
augmentation, can outperform heavier end-to-end models in high-stakes,
multilingual educational settings.

</details>


### [64] [What Should LLMs Forget? Quantifying Personal Data in LLMs for Right-to-Be-Forgotten Requests](https://arxiv.org/abs/2507.11128)
*Dimitri Staufer*

Main category: cs.CL

TL;DR: 论文提出WikiMem数据集和模型无关的指标，用于量化LLMs中个人事实关联，支持动态构建遗忘集以满足GDPR的RTBF要求。


<details>
  <summary>Details</summary>
Motivation: 解决LLMs可能记忆并泄露个人信息的问题，尤其是如何识别和量化模型中的个人事实关联，以符合GDPR的RTBF规定。

Method: 引入WikiMem数据集和基于校准负对数似然的模型无关指标，通过排名真实值与反事实值来量化关联。

Result: 评估了15个LLMs，发现记忆程度与个体网络存在感和模型规模相关。

Conclusion: 为识别LLMs中个人数据记忆提供了基础，支持动态构建遗忘集以满足RTBF请求。

Abstract: Large Language Models (LLMs) can memorize and reveal personal information,
raising concerns regarding compliance with the EU's GDPR, particularly the
Right to Be Forgotten (RTBF). Existing machine unlearning methods assume the
data to forget is already known but do not address how to identify which
individual-fact associations are stored in the model. Privacy auditing
techniques typically operate at the population level or target a small set of
identifiers, limiting applicability to individual-level data inquiries. We
introduce WikiMem, a dataset of over 5,000 natural language canaries covering
243 human-related properties from Wikidata, and a model-agnostic metric to
quantify human-fact associations in LLMs. Our approach ranks ground-truth
values against counterfactuals using calibrated negative log-likelihood across
paraphrased prompts. We evaluate 200 individuals across 15 LLMs (410M-70B
parameters), showing that memorization correlates with subject web presence and
model scale. We provide a foundation for identifying memorized personal data in
LLMs at the individual level, enabling the dynamic construction of forget sets
for machine unlearning and RTBF requests.

</details>


### [65] [Temperature and Persona Shape LLM Agent Consensus With Minimal Accuracy Gains in Qualitative Coding](https://arxiv.org/abs/2507.11198)
*Conrad Borchers,Bahar Shahrokhian,Francesco Balzan,Elham Tajik,Sreecharan Sankaranarayanan,Sebastian Simon*

Main category: cs.CL

TL;DR: 研究了多代理系统（MAS）在定性研究中的表现，发现温度和代理角色对共识构建有显著影响，但未显著提升编码准确性。


<details>
  <summary>Details</summary>
Motivation: 探索多代理系统（MAS）是否比单代理系统在定性研究中表现更好，特别是在编码和数据标注任务中。

Method: 通过实验研究，使用6个开源LLM和18种配置，分析了77,000多个编码决策，比较了不同温度和代理角色对共识和编码准确性的影响。

Result: 温度显著影响共识达成，多角色代理延迟共识，但未显著提升编码准确性。仅特定模型和条件下MAS表现优于单代理。

Conclusion: 挑战了MAS多样性带来更好结果的假设，为LLM在定性研究中的应用提供了新见解。

Abstract: Large Language Models (LLMs) enable new possibilities for qualitative
research at scale, including coding and data annotation. While multi-agent
systems (MAS) can emulate human coding workflows, their benefits over
single-agent coding remain poorly understood. We conducted an experimental
study of how agent persona and temperature shape consensus-building and coding
accuracy of dialog segments based on a codebook with 8 codes. Our open-source
MAS mirrors deductive human coding through structured agent discussion and
consensus arbitration. Using six open-source LLMs (with 3 to 32 billion
parameters) and 18 experimental configurations, we analyze over 77,000 coding
decisions against a gold-standard dataset of human-annotated transcripts from
online math tutoring sessions. Temperature significantly impacted whether and
when consensus was reached across all six LLMs. MAS with multiple personas
(including neutral, assertive, or empathetic), significantly delayed consensus
in four out of six LLMs compared to uniform personas. In three of those LLMs,
higher temperatures significantly diminished the effects of multiple personas
on consensus. However, neither temperature nor persona pairing lead to robust
improvements in coding accuracy. Single agents matched or outperformed MAS
consensus in most conditions. Only one model (OpenHermesV2:7B) and code
category showed above-chance gains from MAS deliberation when temperature was
0.5 or lower and especially when the agents included at least one assertive
persona. Qualitative analysis of MAS collaboration for these configurations
suggests that MAS may nonetheless aid in narrowing ambiguous code applications
that could improve codebooks and human-AI coding. We contribute new insight
into the limits of LLM-based qualitative methods, challenging the notion that
diverse MAS personas lead to better outcomes. We open-source our MAS and
experimentation code.

</details>


### [66] [EsBBQ and CaBBQ: The Spanish and Catalan Bias Benchmarks for Question Answering](https://arxiv.org/abs/2507.11216)
*Valle Ruiz-Fernández,Mario Mina,Júlia Falcão,Luis Vasquez-Reina,Anna Sallés,Aitor Gonzalez-Agirre,Olatz Perez-de-Viñaspre*

Main category: cs.CL

TL;DR: 本文介绍了西班牙和加泰罗尼亚偏见基准（EsBBQ和CaBBQ），用于评估多语言LLM在西班牙社会背景下的偏见问题。


<details>
  <summary>Details</summary>
Motivation: 填补非英语语言和社会背景（如西班牙）在LLM偏见评估资源上的空白。

Method: 基于原始BBQ构建平行数据集，采用多选题QA形式评估10个类别的社会偏见。

Result: LLM在模糊场景中易出错，且高QA准确率常与依赖社会偏见相关。

Conclusion: 需进一步改进LLM以减少偏见，尤其是在非英语和社会背景多样化的场景中。

Abstract: Previous literature has largely shown that Large Language Models (LLMs)
perpetuate social biases learnt from their pre-training data. Given the notable
lack of resources for social bias evaluation in languages other than English,
and for social contexts outside of the United States, this paper introduces the
Spanish and the Catalan Bias Benchmarks for Question Answering (EsBBQ and
CaBBQ). Based on the original BBQ, these two parallel datasets are designed to
assess social bias across 10 categories using a multiple-choice QA setting, now
adapted to the Spanish and Catalan languages and to the social context of
Spain. We report evaluation results on different LLMs, factoring in model
family, size and variant. Our results show that models tend to fail to choose
the correct answer in ambiguous scenarios, and that high QA accuracy often
correlates with greater reliance on social biases.

</details>


### [67] [An Agentic Flow for Finite State Machine Extraction using Prompt Chaining](https://arxiv.org/abs/2507.11222)
*Fares Wael,Youssef Maklad,Ali Hamdi,Wael Elsersy*

Main category: cs.CL

TL;DR: FlowFSM利用LLM和提示链技术从RFC文档中提取精确的FSM，解决了现有方法的局限性，并在实验中表现出色。


<details>
  <summary>Details</summary>
Motivation: 现有FSM提取技术存在可扩展性、覆盖不完整和自然语言歧义等问题，需要改进。

Method: 提出FlowFSM框架，结合LLM、提示链和链式推理，从RFC文档中提取FSM并构建结构化规则书。

Result: 在FTP和RTSP协议上的实验表明，FlowFSM提取精度高，且减少了虚假状态转换。

Conclusion: FlowFSM展示了基于代理的LLM系统在协议分析和FSM推断中的潜力，适用于网络安全和逆向工程。

Abstract: Finite-State Machines (FSMs) are critical for modeling the operational logic
of network protocols, enabling verification, analysis, and vulnerability
discovery. However, existing FSM extraction techniques face limitations such as
scalability, incomplete coverage, and ambiguity in natural language
specifications. In this paper, we propose FlowFSM, a novel agentic framework
that leverages Large Language Models (LLMs) combined with prompt chaining and
chain-of-thought reasoning to extract accurate FSMs from raw RFC documents.
FlowFSM systematically processes protocol specifications, identifies state
transitions, and constructs structured rule-books by chaining agent outputs.
Experimental evaluation across FTP and RTSP protocols demonstrates that FlowFSM
achieves high extraction precision while minimizing hallucinated transitions,
showing promising results. Our findings highlight the potential of agent-based
LLM systems in the advancement of protocol analysis and FSM inference for
cybersecurity and reverse engineering applications.

</details>


### [68] [Sparse Autoencoders Can Capture Language-Specific Concepts Across Diverse Languages](https://arxiv.org/abs/2507.11230)
*Lyzander Marciano Andrylie,Inaya Rahmanisa,Mahardika Krisna Ihsani,Alfan Farizki Wicaksono,Haryo Akbarianto Wibowo,Alham Fikri Aji*

Main category: cs.CL

TL;DR: 论文提出了一种基于稀疏自编码器（SAE）的方法SAE-LAPE，用于识别大语言模型（LLM）中的语言特定特征，发现这些特征主要位于模型的中后层，并影响其多语言性能。


<details>
  <summary>Details</summary>
Motivation: 理解LLM的多语言机制具有挑战性，现有研究难以从跨语言表征中分离出语言特定单元。

Method: 使用稀疏自编码器（SAE）和特征激活概率方法（SAE-LAPE）识别语言特定特征。

Result: 发现语言特定特征主要位于模型的中后层，且可解释性强，可用于语言识别，性能与fastText相当。

Conclusion: SAE-LAPE方法有效识别了语言特定特征，为理解LLM的多语言机制提供了新视角。

Abstract: Understanding the multilingual mechanisms of large language models (LLMs)
provides insight into how they process different languages, yet this remains
challenging. Existing studies often focus on individual neurons, but their
polysemantic nature makes it difficult to isolate language-specific units from
cross-lingual representations. To address this, we explore sparse autoencoders
(SAEs) for their ability to learn monosemantic features that represent concrete
and abstract concepts across languages in LLMs. While some of these features
are language-independent, the presence of language-specific features remains
underexplored. In this work, we introduce SAE-LAPE, a method based on feature
activation probability, to identify language-specific features within the
feed-forward network. We find that many such features predominantly appear in
the middle to final layers of the model and are interpretable. These features
influence the model's multilingual performance and language output and can be
used for language identification with performance comparable to fastText along
with more interpretability. Our code is available at
https://github.com/LyzanderAndrylie/language-specific-features .

</details>


### [69] [KV-Latent: Dimensional-level KV Cache Reduction with Frequency-aware Rotary Positional Embedding](https://arxiv.org/abs/2507.11273)
*Luohe Shi,Zuchao Li,Lefei Zhang,Guoming Liu,Baoyuan Qi,Hai Zhao*

Main category: cs.CL

TL;DR: KV-Latent通过降采样Key-Value向量维度到潜在空间，显著减少KV Cache占用并提升推理速度，仅需少量额外训练。


<details>
  <summary>Details</summary>
Motivation: Transformer Decoder的KV Cache在推理过程中逐渐增加，成为内存消耗和数据传输带宽的主要瓶颈。

Method: 提出KV-Latent范式，降采样KV向量维度至潜在空间，并改进Rotary Positional Embedding的频率采样机制以增强稳定性。

Result: 实验显示KV-Latent在减少KV Cache的同时保持性能，且对不同注意力机制模型均有效。

Conclusion: KV-Latent为高效语言模型系统提供了新思路，实现了KV Cache节省和高效LLMs的可能性。

Abstract: Large language models (LLMs) based on Transformer Decoders have become the
preferred choice for conversational generative AI. Despite the overall
superiority of the Decoder architecture, the gradually increasing Key-Value
(KV) cache during inference has emerged as a primary efficiency bottleneck,
both in aspects of memory consumption and data transfer bandwidth limitations.
To address these challenges, we propose a paradigm called KV-Latent. By
down-sampling the Key-Value vector dimensions into a latent space, we can
significantly reduce the KV Cache footprint and improve inference speed, only
with a small amount of extra training, less than 1\% of pre-training takes.
Besides, we enhanced the stability of Rotary Positional Embedding applied on
lower-dimensional vectors by modifying its frequency sampling mechanism,
avoiding noise introduced by higher frequencies while retaining position
attenuation. Our experiments, including both models with Grouped Query
Attention and those without, have yielded satisfactory results. Finally, we
conducted comparative experiments to study the impact of separately reducing
Key and Value components on model's performance. Our approach allows for the
construction of more efficient language model systems, and opens the new
possibility on KV Cache saving and efficient LLMs. Our code is available at
https://github.com/ShiLuohe/KV-Latent.

</details>


### [70] [FMC: Formalization of Natural Language Mathematical Competition Problems](https://arxiv.org/abs/2507.11275)
*Jiaxuan Xie,Chengwu Liu,Ye Yuan,Siqi Li,Zhiping Xiao,Ming Zhang*

Main category: cs.CL

TL;DR: 提出了一种基于大语言模型和错误反馈的自动形式化方法，构建了一个高质量的奥林匹克数学问题数据集，并验证了其在自动定理证明中的价值。


<details>
  <summary>Details</summary>
Motivation: 推动形式化数学推理的发展，需要高效且准确的自动形式化方法。

Method: 基于大语言模型和错误反馈的自动形式化流程，构建自然语言与Lean形式化对齐的数据集。

Result: 构建了包含3,922个自然语言问题和9,787个Lean形式化问题的数据集，64.46%质量较高；验证了错误反馈和采样数提升对自动形式化的效果。

Conclusion: 该方法为自动定理证明提供了有价值的基准数据集，并展示了错误反馈和采样策略的有效性。

Abstract: Efficient and accurate autoformalization methods, which leverage large-scale
datasets of extensive natural language mathematical problems to construct
formal language datasets, are key to advancing formal mathematical reasoning.
In this paper, we propose an autoformalization pipeline based on large language
models with error feedback, achieving a fully automatic and training-free
formalization approach. Using this pipeline, we curate an Olympiad-level
dataset aligning natural language problems with Lean formalizations. The
dataset comprises $3,922$ mathematical problems in natural language and $9,787$
in Lean, of which $64.46\%$ were assessed as at least above-average quality,
making it suitable as a benchmark for automated theorem provers. Additionally,
we investigate the formalization and reasoning capabilities of various LLMs and
empirically demonstrate that few-shot learning, error feedback, and increasing
sampling numbers enhance the autoformalization process. Experiments of three
automated theorem provers on the \dataset\ dataset also highlight its
challenging nature and its value as a benchmark for formal reasoning tasks.

</details>


### [71] [Fine-Grained Chinese Hate Speech Understanding: Span-Level Resources, Coded Term Lexicon, and Enhanced Detection Frameworks](https://arxiv.org/abs/2507.11292)
*Zewen Bai,Liang Yang,Shengdi Yin,Yuanyuan Sun,Hongfei Lin*

Main category: cs.CL

TL;DR: 论文提出首个中文细粒度仇恨言论数据集（STATE ToxiCN），并研究了编码仇恨术语及大语言模型的语义理解能力，提出了一种整合标注词典的方法以提升检测性能。


<details>
  <summary>Details</summary>
Motivation: 中文仇恨言论检测研究滞后，且缺乏细粒度标注数据和编码仇恨术语的解析能力，限制了模型的深度语义理解和可解释性。

Method: 引入首个中文细粒度仇恨言论数据集STATE ToxiCN，研究编码仇恨术语及大语言模型能力，提出整合标注词典的方法。

Result: 提出的方法显著提升了仇恨言论检测性能，并为中文仇恨言论检测的可解释性研究提供了资源。

Conclusion: 该研究填补了中文仇恨言论检测的空白，为提升模型的可解释性和性能提供了重要支持。

Abstract: The proliferation of hate speech has inflicted significant societal harm,
with its intensity and directionality closely tied to specific targets and
arguments. In recent years, numerous machine learning-based methods have been
developed to detect hateful comments on online platforms automatically.
However, research on Chinese hate speech detection lags behind, and
interpretability studies face two major challenges: first, the scarcity of
span-level fine-grained annotated datasets limits models' deep semantic
understanding of hate speech; second, insufficient research on identifying and
interpreting coded hate speech restricts model explainability in complex
real-world scenarios. To address these, we make the following contributions:
(1) We introduce the Span-level Target-Aware Toxicity Extraction dataset (STATE
ToxiCN), the first span-level Chinese hate speech dataset, and evaluate the
hate semantic understanding of existing models using it. (2) We conduct the
first comprehensive study on Chinese coded hate terms, LLMs' ability to
interpret hate semantics. (3) We propose a method to integrate an annotated
lexicon into models, significantly enhancing hate speech detection performance.
Our work provides valuable resources and insights to advance the
interpretability of Chinese hate speech detection research.

</details>


### [72] [Dr.Copilot: A Multi-Agent Prompt Optimized Assistant for Improving Patient-Doctor Communication in Romanian](https://arxiv.org/abs/2507.11299)
*Andrei Niculae,Adrian Cosma,Cosmin Dumitrache,Emilian Rǎdoi*

Main category: cs.CL

TL;DR: Dr.Copilot是一个多代理LLM系统，旨在提升罗马尼亚语医生在文本远程医疗中的沟通质量，而非医学准确性。


<details>
  <summary>Details</summary>
Motivation: 解决文本远程医疗中医生建议的沟通质量而非临床准确性的问题。

Method: 使用三个LLM代理，通过DSPy自动优化提示，基于低资源罗马尼亚语数据设计，部署开放权重模型。

Result: 实证评估和41名医生的实际部署显示用户评价和响应质量显著提升。

Conclusion: Dr.Copilot是罗马尼亚医疗环境中首批实际部署的LLM之一，有效改善了沟通质量。

Abstract: Text-based telemedicine has become increasingly common, yet the quality of
medical advice in doctor-patient interactions is often judged more on how
advice is communicated rather than its clinical accuracy. To address this, we
introduce Dr.Copilot , a multi-agent large language model (LLM) system that
supports Romanian-speaking doctors by evaluating and enhancing the presentation
quality of their written responses. Rather than assessing medical correctness,
Dr.Copilot provides feedback along 17 interpretable axes. The system comprises
of three LLM agents with prompts automatically optimized via DSPy. Designed
with low-resource Romanian data and deployed using open-weight models, it
delivers real-time specific feedback to doctors within a telemedicine platform.
Empirical evaluations and live deployment with 41 doctors show measurable
improvements in user reviews and response quality, marking one of the first
real-world deployments of LLMs in Romanian medical settings.

</details>


### [73] [Internal Value Alignment in Large Language Models through Controlled Value Vector Activation](https://arxiv.org/abs/2507.11316)
*Haoran Jin,Meng Li,Xiting Wang,Zhihao Xu,Minlie Huang,Yantao Jia,Defu Lian*

Main category: cs.CL

TL;DR: 提出了一种名为ConVA的方法，通过控制LLMs内部值向量激活，直接对齐其潜在表示中的价值观，确保模型行为与人类价值观一致。


<details>
  <summary>Details</summary>
Motivation: 随着LLMs的广泛应用，如何使其行为与人类价值观一致变得尤为重要，以确保透明度、适应性和无偏见。

Method: 提出上下文控制的值向量识别方法和门控值向量激活方法，以准确识别并最小化对模型性能的影响。

Result: 实验表明，该方法在10种基本价值观上实现了最高的控制成功率，且不影响模型性能和流畅性。

Conclusion: ConVA方法有效且高效地实现了LLMs与人类价值观的对齐，具有实际应用潜力。

Abstract: Aligning Large Language Models (LLMs) with human values has attracted
increasing attention since it provides clarity, transparency, and the ability
to adapt to evolving scenarios. In this paper, we introduce a Controlled Value
Vector Activation (ConVA) method that directly aligns the internal values of
LLMs by interpreting how a value is encoded in their latent representations and
modifies relevant activations to ensure consistent values in LLMs. To ensure an
accurate and unbiased interpretation, we propose a context-controlled value
vector identification method. To consistently control values without
sacrificing model performance, we introduce a gated value vector activation
method for effective and minimum degree of value control. Experiments show that
our method achieves the highest control success rate across 10 basic values
without hurting LLM performance and fluency, and ensures target values even
with opposite and potentially malicious input prompts. Source code and data are
available at~ https://github.com/hr-jin/ConVA.

</details>


### [74] [Automated Novelty Evaluation of Academic Paper: A Collaborative Approach Integrating Human and Large Language Model Knowledge](https://arxiv.org/abs/2507.11330)
*Wenqing Wu,Chengzhi Zhang,Yi Zhao*

Main category: cs.CL

TL;DR: 论文提出了一种结合人类专家知识和大型语言模型（LLM）的方法，用于评估学术论文的方法新颖性，解决了传统方法的局限性。


<details>
  <summary>Details</summary>
Motivation: 传统新颖性评估方法（专家判断或引用组合）存在知识有限和效果不确定的问题，LLM和人类专家各有优势，因此结合两者以改进评估。

Method: 从同行评审报告中提取新颖性相关句子，用LLM总结论文方法部分，并设计文本引导融合模块（Sparse-Attention）整合人类和LLM知识，微调预训练语言模型（PLM）。

Result: 实验表明，该方法优于大量基线模型，性能显著提升。

Conclusion: 结合人类和LLM知识的方法能有效评估论文方法新颖性，为学术评审提供新思路。

Abstract: Novelty is a crucial criterion in the peer review process for evaluating
academic papers. Traditionally, it's judged by experts or measure by unique
reference combinations. Both methods have limitations: experts have limited
knowledge, and the effectiveness of the combination method is uncertain.
Moreover, it's unclear if unique citations truly measure novelty. The large
language model (LLM) possesses a wealth of knowledge, while human experts
possess judgment abilities that the LLM does not possess. Therefore, our
research integrates the knowledge and abilities of LLM and human experts to
address the limitations of novelty assessment. The most common novelty in
academic papers is the introduction of new methods. In this paper, we propose
leveraging human knowledge and LLM to assist pretrained language models (PLMs,
e.g. BERT etc.) in predicting the method novelty of papers. Specifically, we
extract sentences related to the novelty of the academic paper from peer review
reports and use LLM to summarize the methodology section of the academic paper,
which are then used to fine-tune PLMs. In addition, we have designed a
text-guided fusion module with novel Sparse-Attention to better integrate human
and LLM knowledge. We compared the method we proposed with a large number of
baselines. Extensive experiments demonstrate that our method achieves superior
performance.

</details>


### [75] [What is the Best Process Model Representation? A Comparative Analysis for Process Modeling with Large Language Models](https://arxiv.org/abs/2507.11356)
*Alexis Brissard,Frédéric Cuppens,Amal Zouaq*

Main category: cs.CL

TL;DR: 本文首次对多种流程模型表示（PMRs）在大型语言模型（LLMs）应用于流程建模（PMo）中的表现进行了实证研究，评估了不同PMRs的适用性和性能。


<details>
  <summary>Details</summary>
Motivation: 现有PMRs在结构、复杂性和可用性上差异显著，且缺乏系统性比较，同时不同PMG方法的评估策略和技术也导致难以对比。

Method: 研究引入了一个包含55个流程描述和九种不同PMRs模型的新数据集PMo Dataset，并从LLM-based PMo的适用性和PMG性能两个维度评估PMRs。

Result: Mermaid在六项PMo标准中得分最高，而BPMN text在流程元素相似性方面表现最佳。

Conclusion: 研究为PMRs的选择提供了实证依据，Mermaid和BPMN text分别在PMo适用性和PMG性能上表现突出。

Abstract: Large Language Models (LLMs) are increasingly applied for Process Modeling
(PMo) tasks such as Process Model Generation (PMG). To support these tasks,
researchers have introduced a variety of Process Model Representations (PMRs)
that serve as model abstractions or generation targets. However, these PMRs
differ widely in structure, complexity, and usability, and have never been
systematically compared. Moreover, recent PMG approaches rely on distinct
evaluation strategies and generation techniques, making comparison difficult.
This paper presents the first empirical study that evaluates multiple PMRs in
the context of PMo with LLMs. We introduce the PMo Dataset, a new dataset
containing 55 process descriptions paired with models in nine different PMRs.
We evaluate PMRs along two dimensions: suitability for LLM-based PMo and
performance on PMG. \textit{Mermaid} achieves the highest overall score across
six PMo criteria, whereas \textit{BPMN text} delivers the best PMG results in
terms of process element similarity.

</details>


### [76] [Addressing Data Imbalance in Transformer-Based Multi-Label Emotion Detection with Weighted Loss](https://arxiv.org/abs/2507.11384)
*Xia Cui*

Main category: cs.CL

TL;DR: 本文探讨了在Transformer模型中应用加权损失函数以解决多标签情感检测中的类别不平衡问题，结果显示其对高频情感类别有效但对少数类别影响有限。


<details>
  <summary>Details</summary>
Motivation: 解决多标签情感检测中的类别不平衡问题，避免传统重采样方法的计算负担。

Method: 使用BERT、RoBERTa和BART模型，在BRIGHTER数据集上应用动态调整类权重的加权损失函数。

Result: 加权损失函数提高了高频情感类别的性能，但对少数类别影响有限。

Conclusion: 该方法在多标签情感检测中有效，但仍需进一步解决少数类别的性能问题。

Abstract: This paper explores the application of a simple weighted loss function to
Transformer-based models for multi-label emotion detection in SemEval-2025
Shared Task 11. Our approach addresses data imbalance by dynamically adjusting
class weights, thereby enhancing performance on minority emotion classes
without the computational burden of traditional resampling methods. We evaluate
BERT, RoBERTa, and BART on the BRIGHTER dataset, using evaluation metrics such
as Micro F1, Macro F1, ROC-AUC, Accuracy, and Jaccard similarity coefficients.
The results demonstrate that the weighted loss function improves performance on
high-frequency emotion classes but shows limited impact on minority classes.
These findings underscore both the effectiveness and the challenges of applying
this approach to imbalanced multi-label emotion detection.

</details>


### [77] [DCR: Quantifying Data Contamination in LLMs Evaluation](https://arxiv.org/abs/2507.11405)
*Cheng Xu,Nan Yan,Shuhao Guan,Changhong Jin,Yuke Mei,Yibing Guo,M-Tahar Kechadi*

Main category: cs.CL

TL;DR: 论文提出了一种名为DCR的轻量级框架，用于检测和量化大语言模型中的基准数据污染问题，并通过调整准确率来反映污染程度。


<details>
  <summary>Details</summary>
Motivation: 大语言模型的快速发展引发了基准数据污染的担忧，污染可能导致性能指标虚高，影响对模型真实泛化能力的评估。

Method: DCR框架通过四个粒度级别（语义、信息、数据和标签）检测污染，并使用模糊推理系统合成污染分数，生成统一的DCR因子来调整准确率。

Result: 在9个大语言模型（0.5B-72B）上的验证显示，DCR能可靠诊断污染严重程度，调整后的准确率与未污染基准相比平均误差在4%以内。

Conclusion: DCR框架计算高效且透明，为常规评估提供了实用的污染检测工具，有助于更公平的比较和提升大语言模型基准测试的可信度。

Abstract: The rapid advancement of large language models (LLMs) has heightened concerns
about benchmark data contamination (BDC), where models inadvertently memorize
evaluation data, inflating performance metrics and undermining genuine
generalization assessment. This paper introduces the Data Contamination Risk
(DCR) framework, a lightweight, interpretable pipeline designed to detect and
quantify BDC across four granular levels: semantic, informational, data, and
label. By synthesizing contamination scores via a fuzzy inference system, DCR
produces a unified DCR Factor that adjusts raw accuracy to reflect
contamination-aware performance. Validated on 9 LLMs (0.5B-72B) across
sentiment analysis, fake news detection, and arithmetic reasoning tasks, the
DCR framework reliably diagnoses contamination severity and with accuracy
adjusted using the DCR Factor to within 4% average error across the three
benchmarks compared to the uncontaminated baseline. Emphasizing computational
efficiency and transparency, DCR provides a practical tool for integrating
contamination assessment into routine evaluations, fostering fairer comparisons
and enhancing the credibility of LLM benchmarking practices.

</details>


### [78] [EXAONE 4.0: Unified Large Language Models Integrating Non-reasoning and Reasoning Modes](https://arxiv.org/abs/2507.11407)
*LG AI Research,:,Kyunghoon Bae,Eunbi Choi,Kibong Choi,Stanley Jungkyu Choi,Yemuk Choi,Kyubeen Han,Seokhee Hong,Junwon Hwang,Taewan Hwang,Joonwon Jang,Hyojin Jeon,Kijeong Jeon,Gerrard Jeongwon Jo,Hyunjik Jo,Jiyeon Jung,Euisoon Kim,Hyosang Kim,Jihoon Kim,Joonkee Kim,Seonghwan Kim,Soyeon Kim,Sunkyoung Kim,Yireun Kim,Yongil Kim,Youchul Kim,Edward Hwayoung Lee,Gwangho Lee,Haeju Lee,Honglak Lee,Jinsik Lee,Kyungmin Lee,Sangha Park,Young Min Paik,Yongmin Park,Youngyong Park,Sanghyun Seo,Sihoon Yang,Heuiyeen Yeen,Sihyuk Yi,Hyeongu Yun*

Main category: cs.CL

TL;DR: EXAONE 4.0整合了非推理和推理模式，提升了可用性和推理能力，支持多语言（英语、韩语、西班牙语），并提供了两种规模的模型（32B和1.2B）。


<details>
  <summary>Details</summary>
Motivation: 为代理AI时代铺路，结合EXAONE 3.5的易用性和EXAONE Deep的高级推理能力。

Method: 引入非推理和推理模式，扩展多语言支持，优化模型性能。

Result: 性能优于同类开源模型，与前沿模型竞争。

Conclusion: EXAONE 4.0公开可用，适合研究用途。

Abstract: This technical report introduces EXAONE 4.0, which integrates a Non-reasoning
mode and a Reasoning mode to achieve both the excellent usability of EXAONE 3.5
and the advanced reasoning abilities of EXAONE Deep. To pave the way for the
agentic AI era, EXAONE 4.0 incorporates essential features such as agentic tool
use, and its multilingual capabilities are extended to support Spanish in
addition to English and Korean. The EXAONE 4.0 model series consists of two
sizes: a mid-size 32B model optimized for high performance, and a small-size
1.2B model designed for on-device applications. The EXAONE 4.0 demonstrates
superior performance compared to open-weight models in its class and remains
competitive even against frontier-class models. The models are publicly
available for research purposes and can be easily downloaded via
https://huggingface.co/LGAI-EXAONE.

</details>


### [79] [KisMATH: Do LLMs Have Knowledge of Implicit Structures in Mathematical Reasoning?](https://arxiv.org/abs/2507.11408)
*Soumadeep Saha,Akshay Chaturvedi,Saptarshi Saha,Utpal Garain,Nicholas Asher*

Main category: cs.CL

TL;DR: 论文通过引入因果链式思维图（CCGs）分析语言模型的推理机制，发现推理节点是最终答案的中介，且模型内部结构与CCGs相似。


<details>
  <summary>Details</summary>
Motivation: 研究链式思维（CoT）如何提升语言模型推理性能，缺乏共识机制。

Method: 提出CCGs，从推理痕迹中提取有向无环图，建模语言模型输出的细粒度因果依赖。

Result: 通过1671个数学问题数据集KisMATH验证，推理节点是答案中介，模型内部结构与CCGs一致。

Conclusion: KisMATH支持可控干预，为CoT在语言模型推理中的作用研究开辟新途径。

Abstract: Chain-of-thought traces have been shown to improve performance of large
language models in a plethora of reasoning tasks, yet there is no consensus on
the mechanism through which this performance boost is achieved. To shed more
light on this, we introduce Causal CoT Graphs (CCGs), which are directed
acyclic graphs automatically extracted from reasoning traces that model
fine-grained causal dependencies in the language model output. A collection of
$1671$ mathematical reasoning problems from MATH500, GSM8K and AIME, and their
associated CCGs are compiled into our dataset -- \textbf{KisMATH}. Our detailed
empirical analysis with 15 open-weight LLMs shows that (i) reasoning nodes in
the CCG are mediators for the final answer, a condition necessary for
reasoning; and (ii) LLMs emphasise reasoning paths given by the CCG, indicating
that models internally realise structures akin to our graphs. KisMATH enables
controlled, graph-aligned interventions and opens up avenues for further
investigation into the role of chain-of-thought in LLM reasoning.

</details>


### [80] [Seq vs Seq: An Open Suite of Paired Encoders and Decoders](https://arxiv.org/abs/2507.11412)
*Orion Weller,Kathryn Ricci,Marc Marone,Antoine Chaffin,Dawn Lawrie,Benjamin Van Durme*

Main category: cs.CL

TL;DR: 论文介绍了Ettin模型套件，比较了编码器-解码器架构在分类、检索和生成任务上的表现，发现各自擅长的任务不同，且直接转换架构效果不佳。


<details>
  <summary>Details</summary>
Motivation: 研究编码器-解码器架构在不同任务上的表现差异，并提供一个统一的训练框架进行比较。

Method: 使用相同训练方法训练不同规模的编码器和解码器模型，比较它们在分类、检索和生成任务上的表现。

Result: 编码器在分类和检索任务上表现更好，解码器在生成任务上更优；直接转换架构效果不如专用模型。

Conclusion: 不同架构适合不同任务，直接转换效果有限，开源所有研究资料以促进未来研究。

Abstract: The large language model (LLM) community focuses almost exclusively on
decoder-only language models, since they are easier to use for text generation.
However, a large subset of the community still uses encoder-only models for
tasks such as classification or retrieval. Previous work has attempted to
compare these architectures, but is forced to make comparisons with models that
have different numbers of parameters, training techniques, and datasets. We
introduce the SOTA open-data Ettin suite of models: paired encoder-only and
decoder-only models ranging from 17 million parameters to 1 billion, trained on
up to 2 trillion tokens. Using the same recipe for both encoder-only and
decoder-only models produces SOTA recipes in both categories for their
respective sizes, beating ModernBERT as an encoder and Llama 3.2 and SmolLM2 as
decoders. Like previous work, we find that encoder-only models excel at
classification and retrieval tasks while decoders excel at generative tasks.
However, we show that adapting a decoder model to encoder tasks (and vice
versa) through continued training is subpar compared to using only the reverse
objective (i.e. a 400M encoder outperforms a 1B decoder on MNLI, and vice versa
for generative tasks). We open-source all artifacts of this study including
training data, training order segmented by checkpoint, and 200+ checkpoints to
allow future work to analyze or extend all aspects of training.

</details>


### [81] [Reasoning Strategies in Large Language Models: Can They Follow, Prefer, and Optimize?](https://arxiv.org/abs/2507.11423)
*Yanjian Zhang,Guillaume Wisniewski,Nadi Tomeh,Thierry Charnois*

Main category: cs.CL

TL;DR: 研究表明，通过提示可以控制大语言模型（LLMs）的推理策略，但单一策略无法持续提升准确性。自适应选择最优策略可能提高性能。


<details>
  <summary>Details</summary>
Motivation: 探索提示是否能控制LLMs的推理策略，并评估其对逻辑问题解决的影响。

Method: 通过实验和提出方法引导LLMs选择推理策略。

Result: 实验显示单一策略无法持续提升准确性，但自适应策略选择可能改善性能。

Conclusion: 提出新方法优化LLMs的推理能力，强调自适应策略选择的重要性。

Abstract: Human reasoning involves different strategies, each suited to specific
problems. Prior work shows that large language model (LLMs) tend to favor a
single reasoning strategy, potentially limiting their effectiveness in diverse
reasoning challenges. In this work, we investigate whether prompting can
control LLMs reasoning strategies and assess its impact on logical
problem-solving. While our experiments show that no single strategy
consistently improves accuracy, performance could be enhanced if models could
adaptively choose the optimal strategy. We propose methods to guide LLMs in
strategy selection, highlighting new ways to refine their reasoning abilities.

</details>


### [82] [HKGAI-V1: Towards Regional Sovereign Large Language Model for Hong Kong](https://arxiv.org/abs/2507.11502)
*Sirui Han,Junqi Zhu,Ruiyuan Zhang,Yike Guo*

Main category: cs.CL

TL;DR: HKGAI-V1是一个为香港定制的多语言大语言模型，结合了DeepSeek架构和检索增强生成技术，专注于区域文化和法律对齐，并开发了本地化评估工具。


<details>
  <summary>Details</summary>
Motivation: 为香港建立符合其多语言、文化和法律特点的AI基础设施，确保AI应用在关键领域的可控性和安全性。

Method: 基于DeepSeek架构，通过全参数微调和检索增强生成技术（RAG）开发HKGAI-V1，并设计对抗性评估工具Adversarial HK Value Benchmark。

Result: HKGAI-V1在文化敏感查询上优于通用模型，并实现了嵌入式治理的数字主权；评估工具验证了模型与本地伦理法律的对齐。

Conclusion: 该研究为开发区域化AI系统提供了技术和框架参考，强调了本地身份在AI发展中的重要性。

Abstract: This paper presents the development of HKGAI-V1, a foundational sovereign
large language model (LLM), developed as part of an initiative to establish
value-aligned AI infrastructure specifically tailored for Hong Kong. Addressing
the region's unique multilingual environment (Cantonese, Mandarin, and
English), its distinct socio-legal context under the "one country, two systems"
framework, and specific local cultural and value considerations, the model is
built upon the DeepSeek architecture and systematically aligned with regional
norms through a multifaceted full parameter fine-tuning process. It is further
integrated with a retrieval-augmented generation (RAG) system to ensure timely
and factually grounded information access. The core contribution lies in the
design and implementation of a comprehensive, region-specific AI alignment and
safety framework, demonstrated through two key achievements: 1) The successful
development of HKGAI-V1 itself - which outper-forms general-purpose models in
handling Hong Kong-specific culturally sensitive queries, and embodies a
"governance-embedded" approach to digital sovereignty - empowers Hong Kong to
exercise control over AI applications in critical sectors including public
services, legal systems, and edu-cation. 2) The development of the proprietary
Adversarial HK Value Benchmark, a rigorous tool for evaluating model alignment
with local ethical and legal stand-ards under challenging conditions. By
documenting these achievements, the paper provides not only a technological
artifact but also a replicable blueprint for developing advanced, regionally
focused AI systems deeply rooted in their local identities.

</details>


### [83] [Real-World Summarization: When Evaluation Reaches Its Limits](https://arxiv.org/abs/2507.11508)
*Patrícia Schmidtová,Ondřej Dušek,Saad Mahamood*

Main category: cs.CL

TL;DR: 论文研究了酒店亮点总结的忠实性评估，发现简单指标（如词重叠）与人类判断相关性高，而复杂方法在跨域数据中表现不佳。LLM生成高质量总结但评估不可靠，且错误信息风险最大。


<details>
  <summary>Details</summary>
Motivation: 探讨在酒店亮点总结任务中，如何评估生成内容的忠实性，并比较不同评估方法的有效性。

Method: 通过人类评估活动（分类错误评估和细粒度标注），比较传统指标、可训练方法和LLM作为评估者的方法。

Result: 简单指标（如词重叠）与人类判断相关性高（Spearman 0.63），LLM评估不可靠，错误信息风险最大。

Conclusion: 简单指标在跨域数据中表现优异，LLM评估需谨慎，错误信息是主要风险。

Abstract: We examine evaluation of faithfulness to input data in the context of hotel
highlights: brief LLM-generated summaries that capture unique features of
accommodations. Through human evaluation campaigns involving categorical error
assessment and span-level annotation, we compare traditional metrics, trainable
methods, and LLM-as-a-judge approaches. Our findings reveal that simpler
metrics like word overlap correlate surprisingly well with human judgments
(Spearman correlation rank of 0.63), often outperforming more complex methods
when applied to out-of-domain data. We further demonstrate that while LLMs can
generate high-quality highlights, they prove unreliable for evaluation as they
tend to severely under- or over-annotate. Our analysis of real-world business
impacts shows incorrect and non-checkable information pose the greatest risks.
We also highlight challenges in crowdsourced evaluations.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [84] [CWNet: Causal Wavelet Network for Low-Light Image Enhancement](https://arxiv.org/abs/2507.10689)
*Tongshun Zhang,Pingping Liu,Yubing Lu,Mengen Cai,Zijian Zhang,Zhe Zhang,Qiuzhan Zhou*

Main category: cs.CV

TL;DR: CWNet提出了一种基于小波变换和因果推理的低光图像增强方法，通过全局和局部策略优化语义信息恢复，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 传统低光图像增强方法忽略实例级语义信息和特征特性，CWNet旨在通过因果推理和小波变换解决这些问题。

Method: CWNet采用因果推理视角和小波变换，包括全局度量学习和局部CLIP语义损失，优化频率信息恢复。

Result: 实验表明CWNet在多个数据集上显著优于现有方法，适应多样场景。

Conclusion: CWNet通过因果推理和小波变换有效提升了低光图像增强性能，代码已开源。

Abstract: Traditional Low-Light Image Enhancement (LLIE) methods primarily focus on
uniform brightness adjustment, often neglecting instance-level semantic
information and the inherent characteristics of different features. To address
these limitations, we propose CWNet (Causal Wavelet Network), a novel
architecture that leverages wavelet transforms for causal reasoning.
Specifically, our approach comprises two key components: 1) Inspired by the
concept of intervention in causality, we adopt a causal reasoning perspective
to reveal the underlying causal relationships in low-light enhancement. From a
global perspective, we employ a metric learning strategy to ensure causal
embeddings adhere to causal principles, separating them from non-causal
confounding factors while focusing on the invariance of causal factors. At the
local level, we introduce an instance-level CLIP semantic loss to precisely
maintain causal factor consistency. 2) Based on our causal analysis, we present
a wavelet transform-based backbone network that effectively optimizes the
recovery of frequency information, ensuring precise enhancement tailored to the
specific attributes of wavelet transforms. Extensive experiments demonstrate
that CWNet significantly outperforms current state-of-the-art methods across
multiple datasets, showcasing its robust performance across diverse scenes.
Code is available at https://github.com/bywlzts/CWNet-Causal-Wavelet-Network.

</details>


### [85] [Integrating Biological Knowledge for Robust Microscopy Image Profiling on De Novo Cell Lines](https://arxiv.org/abs/2507.10737)
*Jiayuan Chen,Thai-Hoang Pham,Yuanlong Wang,Ping Zhang*

Main category: cs.CV

TL;DR: 提出了一种整合外部生物知识的新框架，用于增强显微镜图像分析模型，以解决细胞系异质性带来的挑战。


<details>
  <summary>Details</summary>
Motivation: 由于细胞系间显著的形态和生物学异质性，现有的高通量筛选技术在药物发现中面临挑战。

Method: 通过知识图谱（利用STRING和Hetionet数据库）和单细胞转录组特征，解耦扰动特异性和细胞系特异性表征。

Result: 在RxRx数据库上验证，模型在少样本和单样本微调中表现出色，提升了新细胞系的图像分析能力。

Conclusion: 该方法有效提升了显微镜图像分析在新细胞系中的泛化能力，适用于表型药物发现。

Abstract: High-throughput screening techniques, such as microscopy imaging of cellular
responses to genetic and chemical perturbations, play a crucial role in drug
discovery and biomedical research. However, robust perturbation screening for
\textit{de novo} cell lines remains challenging due to the significant
morphological and biological heterogeneity across cell lines. To address this,
we propose a novel framework that integrates external biological knowledge into
existing pretraining strategies to enhance microscopy image profiling models.
Our approach explicitly disentangles perturbation-specific and cell
line-specific representations using external biological information.
Specifically, we construct a knowledge graph leveraging protein interaction
data from STRING and Hetionet databases to guide models toward
perturbation-specific features during pretraining. Additionally, we incorporate
transcriptomic features from single-cell foundation models to capture cell
line-specific representations. By learning these disentangled features, our
method improves the generalization of imaging models to \textit{de novo} cell
lines. We evaluate our framework on the RxRx database through one-shot
fine-tuning on an RxRx1 cell line and few-shot fine-tuning on cell lines from
the RxRx19a dataset. Experimental results demonstrate that our method enhances
microscopy image profiling for \textit{de novo} cell lines, highlighting its
effectiveness in real-world phenotype-based drug discovery applications.

</details>


### [86] [Auditing Facial Emotion Recognition Datasets for Posed Expressions and Racial Bias](https://arxiv.org/abs/2507.10755)
*Rina Khan,Catherine Stinson*

Main category: cs.CV

TL;DR: 该研究审核了两个先进的FER数据集，发现其中包含大量摆拍图像，且模型对非白人或深肤色人群存在偏见，可能导致实际应用中的危害。


<details>
  <summary>Details</summary>
Motivation: FER算法在检测自发表情时性能下降，且对不同种族和肤色的人群表现不佳，这些问题与数据集收集方式有关。

Method: 随机抽样审核数据集中的图像，区分自发与摆拍表情，并测试模型对不同肤色人群的表现。

Result: 发现数据集中存在大量摆拍图像，且模型对非白人或深肤色人群更倾向于预测负面情绪，存在偏见。

Conclusion: 数据集和模型的偏见可能导致实际应用中的性能不准确和潜在危害，需改进数据收集和模型训练方法。

Abstract: Facial expression recognition (FER) algorithms classify facial expressions
into emotions such as happy, sad, or angry. An evaluative challenge facing FER
algorithms is the fall in performance when detecting spontaneous expressions
compared to posed expressions. An ethical (and evaluative) challenge facing FER
algorithms is that they tend to perform poorly for people of some races and
skin colors. These challenges are linked to the data collection practices
employed in the creation of FER datasets. In this study, we audit two
state-of-the-art FER datasets. We take random samples from each dataset and
examine whether images are spontaneous or posed. In doing so, we propose a
methodology for identifying spontaneous or posed images. We discover a
significant number of images that were posed in the datasets purporting to
consist of in-the-wild images. Since performance of FER models vary between
spontaneous and posed images, the performance of models trained on these
datasets will not represent the true performance if such models were to be
deployed in in-the-wild applications. We also observe the skin color of
individuals in the samples, and test three models trained on each of the
datasets to predict facial expressions of people from various races and skin
tones. We find that the FER models audited were more likely to predict people
labeled as not white or determined to have dark skin as showing a negative
emotion such as anger or sadness even when they were smiling. This bias makes
such models prone to perpetuate harm in real life applications.

</details>


### [87] [FPC-Net: Revisiting SuperPoint with Descriptor-Free Keypoint Detection via Feature Pyramids and Consistency-Based Implicit Matching](https://arxiv.org/abs/2507.10770)
*Ionuţ Grigore,Călin-Adrian Popa,Claudiu Leoveanu-Condrei*

Main category: cs.CV

TL;DR: 提出了一种无需描述符的兴趣点匹配方法，显著降低了内存使用。


<details>
  <summary>Details</summary>
Motivation: 传统方法需要计算、存储和匹配描述符，导致内存占用高。

Method: 在检测过程中直接关联兴趣点，避免使用描述符。

Result: 匹配精度略低于传统方法，但内存使用大幅减少。

Conclusion: 该方法为定位系统提供了一种高效的内存优化解决方案。

Abstract: The extraction and matching of interest points are fundamental to many
geometric computer vision tasks. Traditionally, matching is performed by
assigning descriptors to interest points and identifying correspondences based
on descriptor similarity. This work introduces a technique where interest
points are inherently associated during detection, eliminating the need for
computing, storing, transmitting, or matching descriptors. Although the
matching accuracy is marginally lower than that of conventional approaches, our
method completely eliminates the need for descriptors, leading to a drastic
reduction in memory usage for localization systems. We assess its effectiveness
by comparing it against both classical handcrafted methods and modern learned
approaches.

</details>


### [88] [A New Dataset and Performance Benchmark for Real-time Spacecraft Segmentation in Onboard Flight Computers](https://arxiv.org/abs/2507.10775)
*Jeffrey Joan Sam,Janhavi Sathe,Nikhil Chigali,Naman Gupta,Radhey Ruparel,Yicheng Jiang,Janmajay Singh,James W. Berck,Arko Barman*

Main category: cs.CV

TL;DR: 论文提出了一种新的航天器图像分割数据集，用于训练和评估实时自主检测系统，解决了公开标注数据稀缺的问题。


<details>
  <summary>Details</summary>
Motivation: 航天器在太空环境中易受损害，传统修复方式成本高且风险大，需要可靠的自主检测系统。

Method: 创建了一个包含64k标注图像的数据集，结合真实和合成背景，并添加噪声模拟真实环境。使用YOLOv8和YOLOv11模型进行微调，在硬件和时间约束下评估性能。

Result: 模型在约束条件下表现优异，Dice分数为0.92，Hausdorff距离为0.69，推理时间为0.5秒。

Conclusion: 该数据集和模型为航天器实时图像分割提供了有效解决方案，具有实际应用潜力。

Abstract: Spacecraft deployed in outer space are routinely subjected to various forms
of damage due to exposure to hazardous environments. In addition, there are
significant risks to the subsequent process of in-space repairs through human
extravehicular activity or robotic manipulation, incurring substantial
operational costs. Recent developments in image segmentation could enable the
development of reliable and cost-effective autonomous inspection systems. While
these models often require large amounts of training data to achieve
satisfactory results, publicly available annotated spacecraft segmentation data
are very scarce. Here, we present a new dataset of nearly 64k annotated
spacecraft images that was created using real spacecraft models, superimposed
on a mixture of real and synthetic backgrounds generated using NASA's TTALOS
pipeline. To mimic camera distortions and noise in real-world image
acquisition, we also added different types of noise and distortion to the
images. Finally, we finetuned YOLOv8 and YOLOv11 segmentation models to
generate performance benchmarks for the dataset under well-defined hardware and
inference time constraints to mimic real-world image segmentation challenges
for real-time onboard applications in space on NASA's inspector spacecraft. The
resulting models, when tested under these constraints, achieved a Dice score of
0.92, Hausdorff distance of 0.69, and an inference time of about 0.5 second.
The dataset and models for performance benchmark are available at
https://github.com/RiceD2KLab/SWiM.

</details>


### [89] [Warehouse Spatial Question Answering with LLM Agent](https://arxiv.org/abs/2507.10778)
*Hsiang-Wei Huang,Jen-Hao Cheng,Kuang-Ming Chen,Cheng-Yen Yang,Bahaa Alattar,Yi-Ru Lin,Pyongkun Kim,Sangwon Kim,Kwangju Kim,Chung-I Huang,Jenq-Neng Hwang*

Main category: cs.CV

TL;DR: 提出了一种数据高效的方法，通过LLM代理系统增强空间理解能力，解决复杂室内仓库场景中的空间问答任务。


<details>
  <summary>Details</summary>
Motivation: 现有多模态大语言模型（MLLMs）在空间理解任务上表现不足，需要更高效的方法提升其能力。

Method: 设计了一个LLM代理系统，结合空间推理工具和API交互，以高效解决复杂空间问题。

Result: 在2025 AI City Challenge数据集上表现出高准确性和效率，尤其在物体检索、计数和距离估计任务中。

Conclusion: 该方法为MLLMs在空间理解任务中的应用提供了一种高效且实用的解决方案。

Abstract: Spatial understanding has been a challenging task for existing Multi-modal
Large Language Models~(MLLMs). Previous methods leverage large-scale MLLM
finetuning to enhance MLLM's spatial understanding ability. In this paper, we
present a data-efficient approach. We propose a LLM agent system with strong
and advanced spatial reasoning ability, which can be used to solve the
challenging spatial question answering task in complex indoor warehouse
scenarios. Our system integrates multiple tools that allow the LLM agent to
conduct spatial reasoning and API tools interaction to answer the given
complicated spatial question. Extensive evaluations on the 2025 AI City
Challenge Physical AI Spatial Intelligence Warehouse dataset demonstrate that
our system achieves high accuracy and efficiency in tasks such as object
retrieval, counting, and distance estimation. The code is available at:
https://github.com/hsiangwei0903/SpatialAgent

</details>


### [90] [ThinkingViT: Matryoshka Thinking Vision Transformer for Elastic Inference](https://arxiv.org/abs/2507.10800)
*Ali Hojjat,Janek Haberer,Soren Pirk,Olaf Landsiedel*

Main category: cs.CV

TL;DR: ThinkingViT是一种嵌套ViT架构，通过动态调整计算资源提升效率。


<details>
  <summary>Details</summary>
Motivation: 解决传统嵌套Transformer对所有输入分配相同计算资源导致的效率问题。

Method: 采用渐进式思考阶段和Token Recycling机制，动态调整注意力头数量。

Result: 在ImageNet-1K上，相同吞吐量下准确率提升2.0 p.p.，相同GMACs下提升2.9 p.p.。

Conclusion: ThinkingViT显著提升效率，可作为插件升级现有ViT模型。

Abstract: Vision Transformers deliver state-of-the-art performance, yet their fixed
computational budget prevents scalable deployment across heterogeneous
hardware. Recent nested Transformer architectures mitigate this by embedding
nested subnetworks within a single model to enable scalable inference. However,
these models allocate the same amount of compute to all inputs, regardless of
their complexity, which leads to inefficiencies. To address this, we introduce
ThinkingViT, a nested ViT architecture that employs progressive thinking stages
to dynamically adjust inference computation based on input difficulty.
ThinkingViT initiates inference by activating a small subset of the most
important attention heads and terminates early if predictions reach sufficient
certainty. Otherwise, it activates additional attention heads and re-evaluates
the input. At the core of ThinkingViT is our Token Recycling mechanism, which
conditions each subsequent inference stage on the embeddings from the previous
stage, enabling progressive improvement. Due to its backbone-preserving design,
ThinkingViT also serves as a plugin upgrade for vanilla ViT. Experiments show
that ThinkingViT surpasses nested baselines by up to 2.0 percentage points
(p.p.) in accuracy at the same throughput and by up to 2.9 p.p. at equal GMACs
on ImageNet-1K. The source code is available at
https://github.com/ds-kiel/ThinkingViT.

</details>


### [91] [LLM-Guided Agentic Object Detection for Open-World Understanding](https://arxiv.org/abs/2507.10844)
*Furkan Mumcu,Michael J. Jones,Anoop Cherian,Yasin Yilmaz*

Main category: cs.CV

TL;DR: 提出了一种基于LLM的无标签零样本目标检测框架（LAOD），通过动态生成场景特定对象名称，结合开放词汇检测器实现自适应检测。


<details>
  <summary>Details</summary>
Motivation: 传统目标检测依赖固定类别集，灵活性不足；现有开放世界和开放词汇检测方法存在语义标签缺失或依赖用户提示的问题。

Method: 利用LLM生成场景特定对象名称，结合开放词汇检测器进行定位，引入CAAP和SNAP指标分别评估定位和命名性能。

Result: 在LVIS、COCO和COCO-OOD数据集上验证了方法的有效性，展示了检测和命名新对象的强大性能。

Conclusion: LAOD框架提升了开放世界理解的自主性和适应性。

Abstract: Object detection traditionally relies on fixed category sets, requiring
costly re-training to handle novel objects. While Open-World and
Open-Vocabulary Object Detection (OWOD and OVOD) improve flexibility, OWOD
lacks semantic labels for unknowns, and OVOD depends on user prompts, limiting
autonomy. We propose an LLM-guided agentic object detection (LAOD) framework
that enables fully label-free, zero-shot detection by prompting a Large
Language Model (LLM) to generate scene-specific object names. These are passed
to an open-vocabulary detector for localization, allowing the system to adapt
its goals dynamically. We introduce two new metrics, Class-Agnostic Average
Precision (CAAP) and Semantic Naming Average Precision (SNAP), to separately
evaluate localization and naming. Experiments on LVIS, COCO, and COCO-OOD
validate our approach, showing strong performance in detecting and naming novel
objects. Our method offers enhanced autonomy and adaptability for open-world
understanding.

</details>


### [92] [Winsor-CAM: Human-Tunable Visual Explanations from Deep Networks via Layer-Wise Winsorization](https://arxiv.org/abs/2507.10846)
*Casey Wall,Longwei Wang,Rodrigue Rizk,KC Santosh*

Main category: cs.CV

TL;DR: Winsor-CAM是一种改进的Grad-CAM方法，通过跨层聚合信息和Winsorization技术生成更鲁棒、可解释的热力图。


<details>
  <summary>Details</summary>
Motivation: 解释CNN决策过程在高风险领域至关重要，但现有方法（如Grad-CAM）可能掩盖重要语义或放大噪声。

Method: 提出Winsor-CAM，利用Winsorization技术衰减异常值，并通过用户可调阈值实现语义级控制。

Result: 在PASCAL VOC 2012数据集上，Winsor-CAM在定位指标（如IoU和质心对齐）上优于Grad-CAM和均匀层平均基线。

Conclusion: Winsor-CAM通过多层分析和人机交互控制，推动了可信AI的发展。

Abstract: Interpreting the decision-making process of Convolutional Neural Networks
(CNNs) is critical for deploying models in high-stakes domains.
Gradient-weighted Class Activation Mapping (Grad-CAM) is a widely used method
for visual explanations, yet it typically focuses on the final convolutional
layer or na\"ively averages across layers, strategies that can obscure
important semantic cues or amplify irrelevant noise. We propose Winsor-CAM, a
novel, human-tunable extension of Grad-CAM that generates robust and coherent
saliency maps by aggregating information across all convolutional layers. To
mitigate the influence of noisy or extreme attribution values, Winsor-CAM
applies Winsorization, a percentile-based outlier attenuation technique. A
user-controllable threshold allows for semantic-level tuning, enabling flexible
exploration of model behavior across representational hierarchies. Evaluations
on standard architectures (ResNet50, DenseNet121, VGG16, InceptionV3) using the
PASCAL VOC 2012 dataset demonstrate that Winsor-CAM produces more interpretable
heatmaps and achieves superior performance in localization metrics, including
intersection-over-union and center-of-mass alignment, when compared to Grad-CAM
and uniform layer-averaging baselines. Winsor-CAM advances the goal of
trustworthy AI by offering interpretable, multi-layer insights with
human-in-the-loop control.

</details>


### [93] [Sparse Fine-Tuning of Transformers for Generative Tasks](https://arxiv.org/abs/2507.10855)
*Wei Chen,Jingxi Yu,Zichen Miao,Qiang Qiu*

Main category: cs.CV

TL;DR: 提出了一种基于稀疏编码的微调框架，通过稀疏组合特征字典原子来提升模型的可解释性和任务适应能力，并在图像编辑和文本到图像任务中验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有微调方法生成的表示是密集参数组合，难以解释其贡献和模型如何适应新任务，因此需要一种更透明的方法。

Method: 引入稀疏编码框架，将微调特征表示为特征字典原子的稀疏组合，稀疏系数指示原子重要性。

Result: 在图像编辑中提升文本对齐性能，在文本到图像概念定制任务中优于基线方法。

Conclusion: 稀疏编码框架提高了模型的可解释性和任务适应能力，在多个任务中表现优异。

Abstract: Large pre-trained transformers have revolutionized artificial intelligence
across various domains, and fine-tuning remains the dominant approach for
adapting these models to downstream tasks due to the cost of training from
scratch. However, in existing fine-tuning methods, the updated representations
are formed as a dense combination of modified parameters, making it challenging
to interpret their contributions and understand how the model adapts to new
tasks. In this work, we introduce a fine-tuning framework inspired by sparse
coding, where fine-tuned features are represented as a sparse combination of
basic elements, i.e., feature dictionary atoms. The feature dictionary atoms
function as fundamental building blocks of the representation, and tuning atoms
allows for seamless adaptation to downstream tasks. Sparse coefficients then
serve as indicators of atom importance, identifying the contribution of each
atom to the updated representation. Leveraging the atom selection capability of
sparse coefficients, we first demonstrate that our method enhances image
editing performance by improving text alignment through the removal of
unimportant feature dictionary atoms. Additionally, we validate the
effectiveness of our approach in the text-to-image concept customization task,
where our method efficiently constructs the target concept using a sparse
combination of feature dictionary atoms, outperforming various baseline
fine-tuning methods.

</details>


### [94] [A Lightweight and Robust Framework for Real-Time Colorectal Polyp Detection Using LOF-Based Preprocessing and YOLO-v11n](https://arxiv.org/abs/2507.10864)
*Saadat Behzadi,Danial Sharifrazi,Bita Mesbahzadeh,Javad Hassannataj Joloudarid,Roohallah Alizadehsani*

Main category: cs.CV

TL;DR: 该研究提出了一种结合LOF算法和YOLO-v11n的轻量级高效框架，用于结直肠息肉检测，显著提升了检测性能。


<details>
  <summary>Details</summary>
Motivation: 结直肠息肉及时准确的检测对诊断和预防结直肠癌至关重要，研究旨在通过结合异常值过滤和高效深度学习模型优化检测效果。

Method: 使用LOF算法过滤噪声数据，将分割掩码转换为检测标签，并通过5折交叉验证和YOLO-v11n模型训练优化检测性能。

Result: 模型在多个数据集上表现优异，精度达95.83%，召回率91.85%，F1分数93.48%，mAP@0.5为96.48%。

Conclusion: 该方法适合临床实时结肠镜检查，强调了数据预处理和模型效率在医学影像AI系统中的重要性。

Abstract: Objectives: Timely and accurate detection of colorectal polyps plays a
crucial role in diagnosing and preventing colorectal cancer, a major cause of
mortality worldwide. This study introduces a new, lightweight, and efficient
framework for polyp detection that combines the Local Outlier Factor (LOF)
algorithm for filtering noisy data with the YOLO-v11n deep learning model.
  Study design: An experimental study leveraging deep learning and outlier
removal techniques across multiple public datasets.
  Methods: The proposed approach was tested on five diverse and publicly
available datasets: CVC-ColonDB, CVC-ClinicDB, Kvasir-SEG, ETIS, and EndoScene.
Since these datasets originally lacked bounding box annotations, we converted
their segmentation masks into suitable detection labels. To enhance the
robustness and generalizability of our model, we apply 5-fold cross-validation
and remove anomalous samples using the LOF method configured with 30 neighbors
and a contamination ratio of 5%. Cleaned data are then fed into YOLO-v11n, a
fast and resource-efficient object detection architecture optimized for
real-time applications. We train the model using a combination of modern
augmentation strategies to improve detection accuracy under diverse conditions.
  Results: Our approach significantly improves polyp localization performance,
achieving a precision of 95.83%, recall of 91.85%, F1-score of 93.48%, mAP@0.5
of 96.48%, and mAP@0.5:0.95 of 77.75%. Compared to previous YOLO-based methods,
our model demonstrates enhanced accuracy and efficiency.
  Conclusions: These results suggest that the proposed method is well-suited
for real-time colonoscopy support in clinical settings. Overall, the study
underscores how crucial data preprocessing and model efficiency are when
designing effective AI systems for medical imaging.

</details>


### [95] [Trexplorer Super: Topologically Correct Centerline Tree Tracking of Tubular Objects in CT Volumes](https://arxiv.org/abs/2507.10881)
*Roman Naeem,David Hagerman,Jennifer Alvén,Lennart Svensson,Fredrik Kahl*

Main category: cs.CV

TL;DR: Trexplorer Super是一种改进的3D医学图像中心线追踪模型，解决了重复分支和提前终止的问题，并在新开发的数据集上表现优于现有SOTA模型。


<details>
  <summary>Details</summary>
Motivation: 准确追踪管状树结构（如血管和气道）对医学任务至关重要，但现有模型存在重复分支和提前终止的问题。

Method: 提出了Trexplorer Super模型，通过新方法改进性能，并开发了三个难度递增的中心线数据集（一个合成，两个真实）用于评估。

Result: Trexplorer Super在所有数据集上均优于现有SOTA模型，且发现合成数据上的强表现不一定适用于真实数据。

Conclusion: Trexplorer Super显著提升了中心线追踪性能，同时公开了代码和数据集以促进进一步研究。

Abstract: Tubular tree structures, such as blood vessels and airways, are essential in
human anatomy and accurately tracking them while preserving their topology is
crucial for various downstream tasks. Trexplorer is a recurrent model designed
for centerline tracking in 3D medical images but it struggles with predicting
duplicate branches and terminating tracking prematurely. To address these
issues, we present Trexplorer Super, an enhanced version that notably improves
performance through novel advancements. However, evaluating centerline tracking
models is challenging due to the lack of public datasets. To enable thorough
evaluation, we develop three centerline datasets, one synthetic and two real,
each with increasing difficulty. Using these datasets, we conduct a
comprehensive evaluation of existing state-of-the-art (SOTA) models and compare
them with our approach. Trexplorer Super outperforms previous SOTA models on
every dataset. Our results also highlight that strong performance on synthetic
data does not necessarily translate to real datasets. The code and datasets are
available at https://github.com/RomStriker/Trexplorer-Super.

</details>


### [96] [Modernizing CNN-based Weather Forecast Model towards Higher Computational Efficiency](https://arxiv.org/abs/2507.10893)
*Minjong Cheon,Eunhan Goo,Su-Hyeon Shin,Muhammad Ahmed,Hyungjun Kim*

Main category: cs.CV

TL;DR: 本文提出了一种基于CNN的轻量级全球天气预报模型KAI-a，其性能与现有Transformer模型相当，但计算需求显著降低。


<details>
  <summary>Details</summary>
Motivation: 尽管AI天气预报模型取得显著进展，但Transformer架构的高复杂性和资源需求限制了其应用。本文旨在通过现代化CNN架构解决这一问题。

Method: KAI-a采用尺度不变架构和InceptionNeXt模块，结合地球系统数据的特性设计。模型在ERA5数据集上训练，仅需12小时完成。

Result: KAI-a在中期天气预报中表现与最先进模型相当，且计算资源需求更低。案例研究显示其对极端事件的捕捉能力。

Conclusion: KAI-a为轻量级天气预报模型提供了可行方案，兼具高性能和低资源需求，具有实际应用潜力。

Abstract: Recently, AI-based weather forecast models have achieved impressive advances.
These models have reached accuracy levels comparable to traditional NWP
systems, marking a significant milestone in data-driven weather prediction.
However, they mostly leverage Transformer-based architectures, which often
leads to high training complexity and resource demands due to the massive
parameter sizes. In this study, we introduce a modernized CNN-based model for
global weather forecasting that delivers competitive accuracy while
significantly reducing computational requirements. To present a systematic
modernization roadmap, we highlight key architectural enhancements across
multiple design scales from an earlier CNN-based approach. KAI-a incorporates a
scale-invariant architecture and InceptionNeXt-based blocks within a
geophysically-aware design, tailored to the structure of Earth system data.
Trained on the ERA5 daily dataset with 67 atmospheric variables, the model
contains about 7 million parameters and completes training in just 12 hours on
a single NVIDIA L40s GPU. Our evaluation shows that KAI-a matches the
performance of state-of-the-art models in medium-range weather forecasting,
while offering a significantly lightweight design. Furthermore, case studies on
the 2018 European heatwave and the East Asian summer monsoon demonstrate
KAI-a's robust skill in capturing extreme events, reinforcing its practical
utility.

</details>


### [97] [Commuting Distance Regularization for Timescale-Dependent Label Inconsistency in EEG Emotion Recognition](https://arxiv.org/abs/2507.10895)
*Xiaocong Zeng,Craig Michoski,Yan Pang,Dongyang Kuang*

Main category: cs.CV

TL;DR: 论文提出两种新的正则化策略（LVL和LGCL）解决EEG情感识别中的时间尺度依赖标签不一致问题，并通过实验验证其优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 解决EEG情感识别中时间尺度依赖标签不一致（TsDLI）问题，提升模型泛化性和可解释性。

Method: 提出局部变化损失（LVL）和局部-全局一致性损失（LGCL），结合有界变分函数和通勤时间距离的数学原理。

Result: 在DREAMER和DEAP数据集上，LVL和LGCL表现优于现有方法，LVL在所有基准中排名第一。

Conclusion: 提出的方法在标签不一致情况下平衡了可解释性和预测能力，LVL表现最佳。

Abstract: In this work, we address the often-overlooked issue of Timescale Dependent
Label Inconsistency (TsDLI) in training neural network models for EEG-based
human emotion recognition. To mitigate TsDLI and enhance model generalization
and explainability, we propose two novel regularization strategies: Local
Variation Loss (LVL) and Local-Global Consistency Loss (LGCL). Both methods
incorporate classical mathematical principles--specifically, functions of
bounded variation and commute-time distances--within a graph theoretic
framework. Complementing our regularizers, we introduce a suite of new
evaluation metrics that better capture the alignment between temporally local
predictions and their associated global emotion labels. We validate our
approach through comprehensive experiments on two widely used EEG emotion
datasets, DREAMER and DEAP, across a range of neural architectures including
LSTM and transformer-based models. Performance is assessed using five distinct
metrics encompassing both quantitative accuracy and qualitative consistency.
Results consistently show that our proposed methods outperform state-of-the-art
baselines, delivering superior aggregate performance and offering a principled
trade-off between interpretability and predictive power under label
inconsistency. Notably, LVL achieves the best aggregate rank across all
benchmarked backbones and metrics, while LGCL frequently ranks the second,
highlighting the effectiveness of our framework.

</details>


### [98] [GeoDistill: Geometry-Guided Self-Distillation for Weakly Supervised Cross-View Localization](https://arxiv.org/abs/2507.10935)
*Shaowen Tong,Zimin Xia,Alexandre Alahi,Xuming He,Yujiao Shi*

Main category: cs.CV

TL;DR: GeoDistill提出了一种几何引导的弱监督自蒸馏框架，通过教师-学生学习和基于视场（FoV）的掩码技术，提升跨视角定位的准确性。


<details>
  <summary>Details</summary>
Motivation: 现有跨视角定位方法依赖昂贵的全监督学习，GeoDistill旨在通过弱监督和自蒸馏技术降低成本并提升性能。

Method: 采用教师-学生学习框架，教师模型定位全景图像，学生模型通过FoV掩码学习局部特征，并通过对齐预测结果优化特征学习。

Result: 实验表明，GeoDistill显著提升了定位性能，并减少了不确定性，适用于全景和有限FoV图像。

Conclusion: GeoDistill为跨视角定位提供了一种可扩展且高效的解决方案，无需精确的地面真值标注。

Abstract: Cross-view localization, the task of estimating a camera's
3-degrees-of-freedom (3-DoF) pose by aligning ground-level images with
satellite images, is crucial for large-scale outdoor applications like
autonomous navigation and augmented reality. Existing methods often rely on
fully supervised learning, which requires costly ground-truth pose annotations.
In this work, we propose GeoDistill, a Geometry guided weakly supervised self
distillation framework that uses teacher-student learning with Field-of-View
(FoV)-based masking to enhance local feature learning for robust cross-view
localization. In GeoDistill, the teacher model localizes a panoramic image,
while the student model predicts locations from a limited FoV counterpart
created by FoV-based masking. By aligning the student's predictions with those
of the teacher, the student focuses on key features like lane lines and ignores
textureless regions, such as roads. This results in more accurate predictions
and reduced uncertainty, regardless of whether the query images are panoramas
or limited FoV images. Our experiments show that GeoDistill significantly
improves localization performance across different frameworks. Additionally, we
introduce a novel orientation estimation network that predicts relative
orientation without requiring precise planar position ground truth. GeoDistill
provides a scalable and efficient solution for real-world cross-view
localization challenges. Code and model can be found at
https://github.com/tongshw/GeoDistill.

</details>


### [99] [Graph Aggregation Prototype Learning for Semantic Change Detection in Remote Sensing](https://arxiv.org/abs/2507.10938)
*Zhengyi Xu,Haoran Wu,Wen Jiang,Jie Geng*

Main category: cs.CV

TL;DR: 论文提出GAPL-SCD方法，通过多任务联合优化和原型学习解决语义变化检测中的负迁移问题，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 语义变化检测（SCD）需要同时优化多个任务，容易因任务冲突导致负迁移，影响性能。

Method: 提出GAPL-SCD框架，结合多任务优化、图聚合原型学习、自适应权重分配和梯度旋转方法。

Result: 在SECOND和Landsat-SCD数据集上达到最优性能，准确性和鲁棒性显著提升。

Conclusion: GAPL-SCD有效解决了SCD中的任务冲突问题，提升了多任务学习能力。

Abstract: Semantic change detection (SCD) extends the binary change detection task to
provide not only the change locations but also the detailed "from-to"
categories in multi-temporal remote sensing data. Such detailed semantic
insights into changes offer considerable advantages for a wide array of
applications. However, since SCD involves the simultaneous optimization of
multiple tasks, the model is prone to negative transfer due to task-specific
learning difficulties and conflicting gradient flows. To address this issue, we
propose Graph Aggregation Prototype Learning for Semantic Change Detection in
remote sensing(GAPL-SCD). In this framework, a multi-task joint optimization
method is designed to optimize the primary task of semantic segmentation and
change detection, along with the auxiliary task of graph aggregation prototype
learning. Adaptive weight allocation and gradient rotation methods are used to
alleviate the conflict between training tasks and improve multi-task learning
capabilities. Specifically, the graph aggregation prototype learning module
constructs an interaction graph using high-level features. Prototypes serve as
class proxies, enabling category-level domain alignment across time points and
reducing interference from irrelevant changes. Additionally, the proposed
self-query multi-level feature interaction and bi-temporal feature fusion
modules further enhance multi-scale feature representation, improving
performance in complex scenes. Experimental results on the SECOND and
Landsat-SCD datasets demonstrate that our method achieves state-of-the-art
performance, with significant improvements in accuracy and robustness for SCD
task.

</details>


### [100] [Robust ID-Specific Face Restoration via Alignment Learning](https://arxiv.org/abs/2507.10943)
*Yushun Fang,Lu Liu,Xiang Gao,Qiang Hu,Ning Cao,Jianghe Cui,Gang Chen,Xiaoyun Zhang*

Main category: cs.CV

TL;DR: RIDFR是一种基于扩散模型的新型ID特定人脸修复框架，通过内容注入和身份注入模块，结合对齐学习，解决了身份模糊问题，实现了高质量的身份特定修复。


<details>
  <summary>Details</summary>
Motivation: 当前人脸修复技术虽在视觉质量上有显著提升，但身份模糊问题仍未解决，RIDFR旨在通过特定身份注入和对齐学习解决这一问题。

Method: RIDFR利用预训练扩散模型，结合内容注入模块和身份注入模块，并通过对齐学习抑制ID无关的面部语义干扰。

Result: 实验表明，RIDFR在身份保真度和鲁棒性上优于现有方法，能重建高质量的ID特定结果。

Conclusion: RIDFR通过结合扩散模型和身份特定模块，成功解决了身份模糊问题，为人脸修复提供了新思路。

Abstract: The latest developments in Face Restoration have yielded significant
advancements in visual quality through the utilization of diverse diffusion
priors. Nevertheless, the uncertainty of face identity introduced by
identity-obscure inputs and stochastic generative processes remains unresolved.
To address this challenge, we present Robust ID-Specific Face Restoration
(RIDFR), a novel ID-specific face restoration framework based on diffusion
models. Specifically, RIDFR leverages a pre-trained diffusion model in
conjunction with two parallel conditioning modules. The Content Injection
Module inputs the severely degraded image, while the Identity Injection Module
integrates the specific identity from a given image. Subsequently, RIDFR
incorporates Alignment Learning, which aligns the restoration results from
multiple references with the same identity in order to suppress the
interference of ID-irrelevant face semantics (e.g. pose, expression, make-up,
hair style). Experiments demonstrate that our framework outperforms the
state-of-the-art methods, reconstructing high-quality ID-specific results with
high identity fidelity and demonstrating strong robustness.

</details>


### [101] [Women Sport Actions Dataset for Visual Classification Using Small Scale Training Data](https://arxiv.org/abs/2507.10969)
*Palash Ray,Mahuya Sasmal,Asish Bera*

Main category: cs.CV

TL;DR: 论文提出一个新的女性体育动作数据集WomenSports，并设计了一种结合通道注意力的CNN方法，取得了89.15%的分类准确率。


<details>
  <summary>Details</summary>
Motivation: 现有数据集缺乏女性体育动作的多样性，限制了相关研究的发展。

Method: 提出WomenSports数据集，并设计了一种基于CNN和通道注意力的深度特征提取方法。

Result: 在WomenSports数据集上，使用ResNet-50达到了89.15%的分类准确率。

Conclusion: 该研究填补了女性体育动作数据集的空白，提出的方法在多个数据集上表现优异。

Abstract: Sports action classification representing complex body postures and
player-object interactions is an emerging area in image-based sports analysis.
Some works have contributed to automated sports action recognition using
machine learning techniques over the past decades. However, sufficient image
datasets representing women sports actions with enough intra- and inter-class
variations are not available to the researchers. To overcome this limitation,
this work presents a new dataset named WomenSports for women sports
classification using small-scale training data. This dataset includes a variety
of sports activities, covering wide variations in movements, environments, and
interactions among players. In addition, this study proposes a convolutional
neural network (CNN) for deep feature extraction. A channel attention scheme
upon local contextual regions is applied to refine and enhance feature
representation. The experiments are carried out on three different sports
datasets and one dance dataset for generalizing the proposed algorithm, and the
performances on these datasets are noteworthy. The deep learning method
achieves 89.15% top-1 classification accuracy using ResNet-50 on the proposed
WomenSports dataset, which is publicly available for research at Mendeley Data.

</details>


### [102] [Conceptualizing Multi-scale Wavelet Attention and Ray-based Encoding for Human-Object Interaction Detection](https://arxiv.org/abs/2507.10977)
*Quan Bi Pay,Vishnu Monn Baskaran,Junn Yong Loo,KokSheik Wong,Simon See*

Main category: cs.CV

TL;DR: 提出了一种基于小波注意力机制和射线编码器的新架构，用于高效且可靠的人-物交互检测。


<details>
  <summary>Details</summary>
Motivation: 现有的人-物交互检测方法在效率和可靠性上存在不足，依赖资源密集型训练和低效架构。

Method: 设计了小波注意力机制的主干网络和射线编码器，分别用于提取多阶交互特征和优化多尺度注意力。

Result: 在ImageNet和HICO-DET等基准数据集上验证了架构的有效性。

Conclusion: 新架构显著提升了人-物交互检测的效率和准确性。

Abstract: Human-object interaction (HOI) detection is essential for accurately
localizing and characterizing interactions between humans and objects,
providing a comprehensive understanding of complex visual scenes across various
domains. However, existing HOI detectors often struggle to deliver reliable
predictions efficiently, relying on resource-intensive training methods and
inefficient architectures. To address these challenges, we conceptualize a
wavelet attention-like backbone and a novel ray-based encoder architecture
tailored for HOI detection. Our wavelet backbone addresses the limitations of
expressing middle-order interactions by aggregating discriminative features
from the low- and high-order interactions extracted from diverse convolutional
filters. Concurrently, the ray-based encoder facilitates multi-scale attention
by optimizing the focus of the decoder on relevant regions of interest and
mitigating computational overhead. As a result of harnessing the attenuated
intensity of learnable ray origins, our decoder aligns query embeddings with
emphasized regions of interest for accurate predictions. Experimental results
on benchmark datasets, including ImageNet and HICO-DET, showcase the potential
of our proposed architecture. The code is publicly available at
[https://github.com/henry-pay/RayEncoder].

</details>


### [103] [Mind the Gap: Bridging Occlusion in Gait Recognition via Residual Gap Correction](https://arxiv.org/abs/2507.10978)
*Ayush Gupta,Siyuan Huang,Rama Chellappa*

Main category: cs.CV

TL;DR: RG-Gait提出了一种通过残差学习解决步态识别中遮挡问题的方法，同时保持对完整步态的识别能力。


<details>
  <summary>Details</summary>
Motivation: 步态识别在远距离身份识别中具有潜力，但现有方法未有效解决遮挡问题，且部分方法需要不切实际的成对数据。

Method: 将遮挡步态建模为完整步态的残差偏差，通过残差学习网络自适应整合残差，提升遮挡步态识别性能。

Result: 在Gait3D、GREW和BRIAR数据集上验证，RG-Gait显著提升了遮挡步态识别性能且不影响完整步态识别。

Conclusion: 残差学习是解决遮挡步态识别并保持完整步态识别能力的有效方法。

Abstract: Gait is becoming popular as a method of person re-identification because of
its ability to identify people at a distance. However, most current works in
gait recognition do not address the practical problem of occlusions. Among
those which do, some require paired tuples of occluded and holistic sequences,
which are impractical to collect in the real world. Further, these approaches
work on occlusions but fail to retain performance on holistic inputs. To
address these challenges, we propose RG-Gait, a method for residual correction
for occluded gait recognition with holistic retention. We model the problem as
a residual learning task, conceptualizing the occluded gait signature as a
residual deviation from the holistic gait representation. Our proposed network
adaptively integrates the learned residual, significantly improving performance
on occluded gait sequences without compromising the holistic recognition
accuracy. We evaluate our approach on the challenging Gait3D, GREW and BRIAR
datasets and show that learning the residual can be an effective technique to
tackle occluded gait recognition with holistic retention.

</details>


### [104] [SpaRTAN: Spatial Reinforcement Token-based Aggregation Network for Visual Recognition](https://arxiv.org/abs/2507.10999)
*Quan Bi Pay,Vishnu Monn Baskaran,Junn Yong Loo,KokSheik Wong,Simon See*

Main category: cs.CV

TL;DR: SpaRTAN是一种轻量级架构设计，通过多尺度空间特征和通道聚合模块提升性能，参数效率高。


<details>
  <summary>Details</summary>
Motivation: 解决CNN和Transformer在视觉任务中的简单性偏差和信息冗余问题。

Method: 采用多尺度核和波状通道聚合模块增强空间和通道信息处理。

Result: 在ImageNet-1k上达到77.7%准确率（3.8M参数），COCO上50.0% AP（21.5M参数）。

Conclusion: SpaRTAN通过高效设计实现高性能，参数效率显著。

Abstract: The resurgence of convolutional neural networks (CNNs) in visual recognition
tasks, exemplified by ConvNeXt, has demonstrated their capability to rival
transformer-based architectures through advanced training methodologies and
ViT-inspired design principles. However, both CNNs and transformers exhibit a
simplicity bias, favoring straightforward features over complex structural
representations. Furthermore, modern CNNs often integrate MLP-like blocks akin
to those in transformers, but these blocks suffer from significant information
redundancies, necessitating high expansion ratios to sustain competitive
performance. To address these limitations, we propose SpaRTAN, a lightweight
architectural design that enhances spatial and channel-wise information
processing. SpaRTAN employs kernels with varying receptive fields, controlled
by kernel size and dilation factor, to capture discriminative multi-order
spatial features effectively. A wave-based channel aggregation module further
modulates and reinforces pixel interactions, mitigating channel-wise
redundancies. Combining the two modules, the proposed network can efficiently
gather and dynamically contextualize discriminative features. Experimental
results in ImageNet and COCO demonstrate that SpaRTAN achieves remarkable
parameter efficiency while maintaining competitive performance. In particular,
on the ImageNet-1k benchmark, SpaRTAN achieves 77. 7% accuracy with only 3.8M
parameters and approximately 1.0 GFLOPs, demonstrating its ability to deliver
strong performance through an efficient design. On the COCO benchmark, it
achieves 50.0% AP, surpassing the previous benchmark by 1.2% with only 21.5M
parameters. The code is publicly available at
[https://github.com/henry-pay/SpaRTAN].

</details>


### [105] [Bridge Feature Matching and Cross-Modal Alignment with Mutual-filtering for Zero-shot Anomaly Detection](https://arxiv.org/abs/2507.11003)
*Yuhu Bai,Jiangning Zhang,Yunkang Cao,Guangyuan Lu,Qingdong He,Xiangtai Li,Guanzhong Tian*

Main category: cs.CV

TL;DR: FiSeCLIP利用CLIP模型进行零样本异常检测，通过特征匹配和跨模态对齐，结合批次内图像作为参考，并利用文本信息过滤噪声特征，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 零样本异常检测在工业应用中需求广泛，但现有方法在批次测试和噪声过滤方面存在不足。

Method: 结合特征匹配与跨模态对齐，利用批次内图像作为参考，并通过文本信息过滤噪声特征，同时恢复CLIP的局部语义相关性。

Result: 在MVTec-AD等基准测试中，FiSeCLIP在异常分类和分割任务上表现优异，AU-ROC和F1-max分别提升4.6%和5.7%。

Conclusion: FiSeCLIP为零样本异常检测提供了更强的基线方法，展示了CLIP模型在细粒度任务中的潜力。

Abstract: With the advent of vision-language models (e.g., CLIP) in zero- and few-shot
settings, CLIP has been widely applied to zero-shot anomaly detection (ZSAD) in
recent research, where the rare classes are essential and expected in many
applications. This study introduces \textbf{FiSeCLIP} for ZSAD with
training-free \textbf{CLIP}, combining the feature matching with the
cross-modal alignment. Testing with the entire dataset is impractical, while
batch-based testing better aligns with real industrial needs, and images within
a batch can serve as mutual reference points. Accordingly, FiSeCLIP utilizes
other images in the same batch as reference information for the current image.
However, the lack of labels for these references can introduce ambiguity, we
apply text information to \textbf{fi}lter out noisy features. In addition, we
further explore CLIP's inherent potential to restore its local
\textbf{se}mantic correlation, adapting it for fine-grained anomaly detection
tasks to enable a more accurate filtering process. Our approach exhibits
superior performance for both anomaly classification and segmentation on
anomaly detection benchmarks, building a stronger baseline for the direction,
e.g., on MVTec-AD, FiSeCLIP outperforms the SOTA AdaCLIP by
+4.6\%$\uparrow$/+5.7\%$\uparrow$ in segmentation metrics AU-ROC/$F_1$-max.

</details>


### [106] [Semantically Informed Salient Regions Guided Radiology Report Generation](https://arxiv.org/abs/2507.11015)
*Zeyi Hou,Zeqiang Wei,Ruixin Yan,Ning Lang,Xiuzhuang Zhou*

Main category: cs.CV

TL;DR: 提出了一种基于语义显著区域的放射报告生成方法（SISRNet），通过聚焦医学关键区域，减少数据偏差影响，生成更准确的报告。


<details>
  <summary>Details</summary>
Motivation: 现有方法因数据偏差常生成流畅但不准确的报告，限制了临床应用。

Method: 利用细粒度跨模态语义识别医学关键区域，并在图像建模和报告生成中系统聚焦这些区域。

Result: 在IU-Xray和MIMIC-CXR数据集上表现优于同类方法。

Conclusion: SISRNet能有效捕捉细微异常，生成临床准确的报告。

Abstract: Recent advances in automated radiology report generation from chest X-rays
using deep learning algorithms have the potential to significantly reduce the
arduous workload of radiologists. However, due to the inherent massive data
bias in radiology images, where abnormalities are typically subtle and sparsely
distributed, existing methods often produce fluent yet medically inaccurate
reports, limiting their applicability in clinical practice. To address this
issue effectively, we propose a Semantically Informed Salient Regions-guided
(SISRNet) report generation method. Specifically, our approach explicitly
identifies salient regions with medically critical characteristics using
fine-grained cross-modal semantics. Then, SISRNet systematically focuses on
these high-information regions during both image modeling and report
generation, effectively capturing subtle abnormal findings, mitigating the
negative impact of data bias, and ultimately generating clinically accurate
reports. Compared to its peers, SISRNet demonstrates superior performance on
widely used IU-Xray and MIMIC-CXR datasets.

</details>


### [107] [Human-Guided Shade Artifact Suppression in CBCT-to-MDCT Translation via Schrödinger Bridge with Conditional Diffusion](https://arxiv.org/abs/2507.11025)
*Sung Ho Kang,Hyun-Cheol Park*

Main category: cs.CV

TL;DR: 提出了一种基于Schrodinger Bridge的CBCT-to-MDCT转换框架，结合GAN先验和人类引导的条件扩散，确保解剖保真度和感知可控性。


<details>
  <summary>Details</summary>
Motivation: 解决传统GAN或扩散模型在医学图像转换中边界一致性和临床偏好对齐的不足。

Method: 利用Schrodinger Bridge框架，结合GAN先验和人类反馈（通过CFG），通过迭代优化和锦标赛选择实现偏好对齐。

Result: 在临床数据集上，定量评估（RMSE、SSIM、LPIPS、Dice）表现优于现有方法，且仅需10步采样。

Conclusion: 该框架高效且有效，适用于实时、偏好对齐的医学图像转换。

Abstract: We present a novel framework for CBCT-to-MDCT translation, grounded in the
Schrodinger Bridge (SB) formulation, which integrates GAN-derived priors with
human-guided conditional diffusion. Unlike conventional GANs or diffusion
models, our approach explicitly enforces boundary consistency between CBCT
inputs and pseudo targets, ensuring both anatomical fidelity and perceptual
controllability. Binary human feedback is incorporated via classifier-free
guidance (CFG), effectively steering the generative process toward clinically
preferred outcomes. Through iterative refinement and tournament-based
preference selection, the model internalizes human preferences without relying
on a reward model. Subtraction image visualizations reveal that the proposed
method selectively attenuates shade artifacts in key anatomical regions while
preserving fine structural detail. Quantitative evaluations further demonstrate
superior performance across RMSE, SSIM, LPIPS, and Dice metrics on clinical
datasets -- outperforming prior GAN- and fine-tuning-based feedback methods --
while requiring only 10 sampling steps. These findings underscore the
effectiveness and efficiency of our framework for real-time, preference-aligned
medical image translation.

</details>


### [108] [Personalized OVSS: Understanding Personal Concept in Open-Vocabulary Semantic Segmentation](https://arxiv.org/abs/2507.11030)
*Sunghyun Park,Jungsoo Lee,Shubhankar Borse,Munawar Hayat,Sungha Choi,Kyuwoong Hwang,Fatih Porikli*

Main category: cs.CV

TL;DR: 论文提出了一种个性化开放词汇语义分割任务，通过文本提示调优插件方法识别个人视觉概念，同时保持原始OVSS性能。


<details>
  <summary>Details</summary>
Motivation: 解决开放词汇语义分割无法理解个人文本（如“我的杯子”）的问题，识别用户特定兴趣区域。

Method: 采用文本提示调优插件方法，结合负掩码提案和视觉嵌入注入，减少误预测并丰富文本提示表示。

Result: 在FSS$^\text{per}$、CUB$^\text{per}$和ADE$^\text{per}$基准测试中表现优越。

Conclusion: 提出的方法在不影响原始OVSS性能的情况下，显著提升了个性化开放词汇语义分割的效果。

Abstract: While open-vocabulary semantic segmentation (OVSS) can segment an image into
semantic regions based on arbitrarily given text descriptions even for classes
unseen during training, it fails to understand personal texts (e.g., `my mug
cup') for segmenting regions of specific interest to users. This paper
addresses challenges like recognizing `my mug cup' among `multiple mug cups'.
To overcome this challenge, we introduce a novel task termed
\textit{personalized open-vocabulary semantic segmentation} and propose a text
prompt tuning-based plug-in method designed to recognize personal visual
concepts using a few pairs of images and masks, while maintaining the
performance of the original OVSS. Based on the observation that reducing false
predictions is essential when applying text prompt tuning to this task, our
proposed method employs `negative mask proposal' that captures visual concepts
other than the personalized concept. We further improve the performance by
enriching the representation of text prompts by injecting visual embeddings of
the personal concept into them. This approach enhances personalized OVSS
without compromising the original OVSS performance. We demonstrate the
superiority of our method on our newly established benchmarks for this task,
including FSS$^\text{per}$, CUB$^\text{per}$, and ADE$^\text{per}$.

</details>


### [109] [Efficient Dual-domain Image Dehazing with Haze Prior Perception](https://arxiv.org/abs/2507.11035)
*Lirong Zheng,Yanshan Li,Rui Yu,Kaihao Zhang*

Main category: cs.CV

TL;DR: 提出了一种双域去雾网络DGFDNet，结合空间域和频域特征，通过物理引导的退化对齐提升去雾效果。


<details>
  <summary>Details</summary>
Motivation: Transformer模型在单图像去雾中全局建模能力强，但计算成本高，现有方法依赖空间域特征，计算昂贵且效果不佳。频域方法耦合性弱，限制了性能。

Method: 提出DGFDNet框架，包含HAFM模块（基于暗通道先验生成雾霾置信图，增强相关频域成分）和MGAM模块（多尺度特征融合）。PCGB分支通过闭环反馈机制迭代优化先验。

Result: 在四个基准数据集上，DGFDNet实现了最优性能，兼具鲁棒性和实时性。

Conclusion: DGFDNet通过双域协同和物理引导，显著提升了去雾效果和效率。

Abstract: Transformer-based models exhibit strong global modeling capabilities in
single-image dehazing, but their high computational cost limits real-time
applicability. Existing methods predominantly rely on spatial-domain features
to capture long-range dependencies, which are computationally expensive and
often inadequate under complex haze conditions. While some approaches introduce
frequency-domain cues, the weak coupling between spatial and frequency branches
limits the overall performance. To overcome these limitations, we propose the
Dark Channel Guided Frequency-aware Dehazing Network (DGFDNet), a novel
dual-domain framework that performs physically guided degradation alignment
across spatial and frequency domains. At its core, the DGFDBlock comprises two
key modules: 1) the Haze-Aware Frequency Modulator (HAFM), which generates a
pixel-level haze confidence map from dark channel priors to adaptively enhance
haze-relevant frequency components, thereby achieving global degradation-aware
spectral modulation; 2) the Multi-level Gating Aggregation Module (MGAM), which
fuses multi-scale features through diverse convolutional kernels and hybrid
gating mechanisms to recover fine structural details. Additionally, a Prior
Correction Guidance Branch (PCGB) incorporates a closed-loop feedback
mechanism, enabling iterative refinement of the prior by intermediate dehazed
features and significantly improving haze localization accuracy, especially in
challenging outdoor scenes. Extensive experiments on four benchmark haze
datasets demonstrate that DGFDNet achieves state-of-the-art performance with
superior robustness and real-time efficiency. Code is available at:
https://github.com/Dilizlr/DGFDNet.

</details>


### [110] [A Multi-View High-Resolution Foot-Ankle Complex Point Cloud Dataset During Gait for Occlusion-Robust 3D Completion](https://arxiv.org/abs/2507.11037)
*Jie-Wen Li,Zi-Han Ye,Qingyuan Zhou,Jiayi Song,Ying He,Ben Fei,Wen-Ming Chen*

Main category: cs.CV

TL;DR: FootGait3D是一个专注于足踝区域的高分辨率点云数据集，用于3D点云补全任务，支持生物力学和临床步态分析研究。


<details>
  <summary>Details</summary>
Motivation: 动态步态条件下足踝表面几何数据的准确采集具有挑战性，现有数据集通常关注全身或下肢运动，缺乏足踝区域的精细数据。

Method: 使用五相机深度传感系统采集46名受试者的8,403帧多视角点云数据，提供完整和部分遮挡的点云用于评估补全方法。

Result: FootGait3D数据集支持单模态和多模态3D点云补全网络的基准测试，为足踝运动建模提供高精度数据。

Conclusion: FootGait3D在生物力学、临床步态分析和机器人应用中具有重要潜力，数据集已公开可用。

Abstract: The kinematics analysis of foot-ankle complex during gait is essential for
advancing biomechanical research and clinical assessment. Collecting accurate
surface geometry data from the foot and ankle during dynamic gait conditions is
inherently challenging due to swing foot occlusions and viewing limitations.
Thus, this paper introduces FootGait3D, a novel multi-view dataset of
high-resolution ankle-foot surface point clouds captured during natural gait.
Different from existing gait datasets that typically target whole-body or
lower-limb motion, FootGait3D focuses specifically on the detailed modeling of
the ankle-foot region, offering a finer granularity of motion data. To address
this, FootGait3D consists of 8,403 point cloud frames collected from 46
subjects using a custom five-camera depth sensing system. Each frame includes a
complete 5-view reconstruction of the foot and ankle (serving as ground truth)
along with partial point clouds obtained from only four, three, or two views.
This structured variation enables rigorous evaluation of 3D point cloud
completion methods under varying occlusion levels and viewpoints. Our dataset
is designed for shape completion tasks, facilitating the benchmarking of
state-of-the-art single-modal (e.g., PointTr, SnowflakeNet, Anchorformer) and
multi-modal (e.g., SVDFormer, PointSea, CSDN) completion networks on the
challenge of recovering the full foot geometry from occluded inputs. FootGait3D
has significant potential to advance research in biomechanics and multi-segment
foot modeling, offering a valuable testbed for clinical gait analysis,
prosthetic design, and robotics applications requiring detailed 3D models of
the foot during motion. The dataset is now available at
https://huggingface.co/datasets/ljw285/FootGait3D.

</details>


### [111] [Combining Transformers and CNNs for Efficient Object Detection in High-Resolution Satellite Imagery](https://arxiv.org/abs/2507.11040)
*Nicolas Drapier,Aladine Chetouani,Aurélien Chateigner*

Main category: cs.CV

TL;DR: GLOD是一种基于Transformer的架构，用于高分辨率卫星图像中的目标检测，通过Swin Transformer和创新的上采样模块实现高效性能。


<details>
  <summary>Details</summary>
Motivation: 解决高分辨率卫星图像中目标检测的挑战，提升性能并超越现有方法。

Method: 采用Swin Transformer替代CNN主干，结合UpConvMixer块和Fusion Blocks进行多尺度特征整合。

Result: 在xView数据集上达到32.95%的准确率，比现有最佳方法高出11.46%。

Conclusion: GLOD通过创新设计和优化，显著提升了卫星图像目标检测的性能和效率。

Abstract: We present GLOD, a transformer-first architecture for object detection in
high-resolution satellite imagery. GLOD replaces CNN backbones with a Swin
Transformer for end-to-end feature extraction, combined with novel UpConvMixer
blocks for robust upsampling and Fusion Blocks for multi-scale feature
integration. Our approach achieves 32.95\% on xView, outperforming SOTA methods
by 11.46\%. Key innovations include asymmetric fusion with CBAM attention and a
multi-path head design capturing objects across scales. The architecture is
optimized for satellite imagery challenges, leveraging spatial priors while
maintaining computational efficiency.

</details>


### [112] [Alleviating Textual Reliance in Medical Language-guided Segmentation via Prototype-driven Semantic Approximation](https://arxiv.org/abs/2507.11055)
*Shuchang Ye,Usman Naseem,Mingyuan Meng,Jinman Kim*

Main category: cs.CV

TL;DR: ProLearn框架通过原型驱动的语义近似（PSA）模块，减少对配对图像-文本输入的依赖，提升医学图像分割性能。


<details>
  <summary>Details</summary>
Motivation: 现有医学语言引导分割方法依赖配对图像-文本输入，导致未配对数据利用不足且临床应用受限。

Method: 提出ProLearn框架，引入PSA模块，通过原型空间和查询-响应机制近似语义引导。

Result: 在QaTa-COV19等数据集上，ProLearn在文本有限时优于现有方法。

Conclusion: ProLearn有效减少文本依赖，扩展了语言引导分割的适用性。

Abstract: Medical language-guided segmentation, integrating textual clinical reports as
auxiliary guidance to enhance image segmentation, has demonstrated significant
improvements over unimodal approaches. However, its inherent reliance on paired
image-text input, which we refer to as ``textual reliance", presents two
fundamental limitations: 1) many medical segmentation datasets lack paired
reports, leaving a substantial portion of image-only data underutilized for
training; and 2) inference is limited to retrospective analysis of cases with
paired reports, limiting its applicability in most clinical scenarios where
segmentation typically precedes reporting. To address these limitations, we
propose ProLearn, the first Prototype-driven Learning framework for
language-guided segmentation that fundamentally alleviates textual reliance. At
its core, in ProLearn, we introduce a novel Prototype-driven Semantic
Approximation (PSA) module to enable approximation of semantic guidance from
textual input. PSA initializes a discrete and compact prototype space by
distilling segmentation-relevant semantics from textual reports. Once
initialized, it supports a query-and-respond mechanism which approximates
semantic guidance for images without textual input, thereby alleviating textual
reliance. Extensive experiments on QaTa-COV19, MosMedData+ and Kvasir-SEG
demonstrate that ProLearn outperforms state-of-the-art language-guided methods
when limited text is available.

</details>


### [113] [Robust 3D-Masked Part-level Editing in 3D Gaussian Splatting with Regularized Score Distillation Sampling](https://arxiv.org/abs/2507.11061)
*Hayeon Kim,Ji Ha Jang,Se Young Chun*

Main category: cs.CV

TL;DR: RoMaP是一种新的局部3D高斯编辑框架，通过3D-GALP和正则化SDS损失实现精确的局部编辑。


<details>
  <summary>Details</summary>
Motivation: 当前3D神经表示和实例级编辑模型在实现精确局部3D编辑时面临挑战，尤其是高斯泼溅技术因多视角2D分割不一致和SDS损失的模糊性而受限。

Method: RoMaP包含3D-GALP模块（利用球谐系数建模视角依赖的标签变化）和正则化SDS损失（结合L1锚定损失和高斯先验去除等）。

Result: 实验表明，RoMaP在重建和生成的高斯场景中实现了最先进的局部3D编辑效果。

Conclusion: RoMaP为3D高斯编辑提供了更鲁棒和灵活的部分级编辑能力。

Abstract: Recent advances in 3D neural representations and instance-level editing
models have enabled the efficient creation of high-quality 3D content. However,
achieving precise local 3D edits remains challenging, especially for Gaussian
Splatting, due to inconsistent multi-view 2D part segmentations and inherently
ambiguous nature of Score Distillation Sampling (SDS) loss. To address these
limitations, we propose RoMaP, a novel local 3D Gaussian editing framework that
enables precise and drastic part-level modifications. First, we introduce a
robust 3D mask generation module with our 3D-Geometry Aware Label Prediction
(3D-GALP), which uses spherical harmonics (SH) coefficients to model
view-dependent label variations and soft-label property, yielding accurate and
consistent part segmentations across viewpoints. Second, we propose a
regularized SDS loss that combines the standard SDS loss with additional
regularizers. In particular, an L1 anchor loss is introduced via our Scheduled
Latent Mixing and Part (SLaMP) editing method, which generates high-quality
part-edited 2D images and confines modifications only to the target region
while preserving contextual coherence. Additional regularizers, such as
Gaussian prior removal, further improve flexibility by allowing changes beyond
the existing context, and robust 3D masking prevents unintended edits.
Experimental results demonstrate that our RoMaP achieves state-of-the-art local
3D editing on both reconstructed and generated Gaussian scenes and objects
qualitatively and quantitatively, making it possible for more robust and
flexible part-level 3D Gaussian editing.

</details>


### [114] [Joint angle model based learning to refine kinematic human pose estimation](https://arxiv.org/abs/2507.11075)
*Chang Peng,Yifei Zhou,Huifeng Xi,Shiqing Huang,Chuangye Chen,Jianming Yang,Bao Yang,Zhenyu Jiang*

Main category: cs.CV

TL;DR: 提出了一种基于关节角度建模的新方法，用于改进无标记人体姿态估计（HPE）中的关键点识别和轨迹平滑问题。


<details>
  <summary>Details</summary>
Motivation: 现有HPE方法在关键点识别和轨迹分析中存在误差和波动，且训练数据集标注不准确限制了深度学习模型的性能。

Method: 通过关节角度建模和高阶傅里叶级数近似关节角度变化，设计双向循环网络作为后处理模块。

Result: 实验表明，该方法在花样滑冰和街舞等挑战性场景中优于现有HPE细化网络。

Conclusion: 基于关节角度的细化方法（JAR）显著提升了HPE的准确性和稳定性。

Abstract: Marker-free human pose estimation (HPE) has found increasing applications in
various fields. Current HPE suffers from occasional errors in keypoint
recognition and random fluctuation in keypoint trajectories when analyzing
kinematic human poses. The performance of existing deep learning-based models
for HPE refinement is considerably limited by inaccurate training datasets in
which the keypoints are manually annotated. This paper proposed a novel method
to overcome the difficulty through joint angle-based modeling. The key
techniques include: (i) A joint angle-based model of human pose, which is
robust to describe kinematic human poses; (ii) Approximating temporal variation
of joint angles through high order Fourier series to get reliable "ground
truth"; (iii) A bidirectional recurrent network is designed as a
post-processing module to refine the estimation of well-established HRNet.
Trained with the high-quality dataset constructed using our method, the network
demonstrates outstanding performance to correct wrongly recognized joints and
smooth their spatiotemporal trajectories. Tests show that joint angle-based
refinement (JAR) outperforms the state-of-the-art HPE refinement network in
challenging cases like figure skating and breaking.

</details>


### [115] [GKNet: Graph-based Keypoints Network for Monocular Pose Estimation of Non-cooperative Spacecraft](https://arxiv.org/abs/2507.11077)
*Weizhao Ma,Dong Zhou,Yuhui Hu,Zipeng He*

Main category: cs.CV

TL;DR: 论文提出了一种基于图的关键点网络（GKNet），用于非合作航天器的单目姿态估计，解决了结构对称性和部分遮挡问题，并发布了一个新的数据集SKD。


<details>
  <summary>Details</summary>
Motivation: 非合作航天器的单目姿态估计对在轨服务任务至关重要，但现有关键点检测器在结构对称性和部分遮挡下表现不佳。

Method: 提出GKNet，利用关键点图的几何约束进行姿态估计，并发布SKD数据集用于验证。

Result: 实验表明GKNet在精度和有效性上优于现有方法。

Conclusion: GKNet和SKD数据集为非合作航天器的姿态估计提供了高精度解决方案。

Abstract: Monocular pose estimation of non-cooperative spacecraft is significant for
on-orbit service (OOS) tasks, such as satellite maintenance, space debris
removal, and station assembly. Considering the high demands on pose estimation
accuracy, mainstream monocular pose estimation methods typically consist of
keypoint detectors and PnP solver. However, current keypoint detectors remain
vulnerable to structural symmetry and partial occlusion of non-cooperative
spacecraft. To this end, we propose a graph-based keypoints network for the
monocular pose estimation of non-cooperative spacecraft, GKNet, which leverages
the geometric constraint of keypoints graph. In order to better validate
keypoint detectors, we present a moderate-scale dataset for the spacecraft
keypoint detection, named SKD, which consists of 3 spacecraft targets, 90,000
simulated images, and corresponding high-precise keypoint annotations.
Extensive experiments and an ablation study have demonstrated the high accuracy
and effectiveness of our GKNet, compared to the state-of-the-art spacecraft
keypoint detectors. The code for GKNet and the SKD dataset is available at
https://github.com/Dongzhou-1996/GKNet.

</details>


### [116] [Automatic Road Subsurface Distress Recognition from Ground Penetrating Radar Images using Deep Learning-based Cross-verification](https://arxiv.org/abs/2507.11081)
*Chang Peng,Bao Yang,Meiqi Li,Ge Zhang,Hui Sun,Zhenyu Jiang*

Main category: cs.CV

TL;DR: 该研究提出了一种基于YOLO模型和交叉验证策略的GPR图像自动识别方法，显著提高了道路地下病害（RSD）的检测效率和准确性。


<details>
  <summary>Details</summary>
Motivation: GPR图像识别RSD依赖人工且效率低，深度学习因数据稀缺和网络能力不足而受限。

Method: 构建了高质量的3D GPR数据集，提出基于YOLO模型的交叉验证策略。

Result: 在实地测试中召回率超过98.6%，检测系统可减少约90%的人工工作量。

Conclusion: 该方法为RSD自动识别提供了高效解决方案，显著提升了检测效率。

Abstract: Ground penetrating radar (GPR) has become a rapid and non-destructive
solution for road subsurface distress (RSD) detection. However, RSD recognition
from GPR images is labor-intensive and heavily relies on inspectors' expertise.
Deep learning offers the possibility for automatic RSD recognition, but its
current performance is limited by two factors: Scarcity of high-quality dataset
for network training and insufficient capability of network to distinguish RSD.
In this study, a rigorously validated 3D GPR dataset containing 2134 samples of
diverse types was constructed through field scanning. Based on the finding that
the YOLO model trained with one of the three scans of GPR images exhibits
varying sensitivity to specific type of RSD, we proposed a novel
cross-verification strategy with outstanding accuracy in RSD recognition,
achieving recall over 98.6% in field tests. The approach, integrated into an
online RSD detection system, can reduce the labor of inspection by around 90%.

</details>


### [117] [Atmos-Bench: 3D Atmospheric Structures for Climate Insight](https://arxiv.org/abs/2507.11085)
*Tianchi Xu*

Main category: cs.CV

TL;DR: 提出首个3D大气基准Atmos-Bench和新型FourCastX网络，用于卫星LiDAR数据的3D大气结构恢复，无需辅助输入且性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖辅助输入和简化物理近似，缺乏标准化3D基准，可能引入不确定性且无法充分捕捉真实辐射传输和大气散射-吸收效应。

Method: 开发Atmos-Bench基准，包含921,600张图像切片，并通过FourCastX网络嵌入物理约束，实现能量一致性恢复。

Result: 在355 nm和532 nm波段上均优于现有基线模型，无需辅助输入。

Conclusion: Atmos-Bench为卫星3D大气结构恢复设定了新标准，有助于更深入的气候研究。

Abstract: Atmospheric structure, represented by backscatter coefficients (BC) recovered
from satellite LiDAR attenuated backscatter (ATB), provides a volumetric view
of clouds, aerosols, and molecules, playing a critical role in human
activities, climate understanding, and extreme weather forecasting. Existing
methods often rely on auxiliary inputs and simplified physics-based
approximations, and lack a standardized 3D benchmark for fair evaluation.
However, such approaches may introduce additional uncertainties and
insufficiently capture realistic radiative transfer and atmospheric
scattering-absorption effects. To bridge these gaps, we present Atmos-Bench:
the first 3D atmospheric benchmark, along with a novel FourCastX:
Frequency-enhanced Spatio-Temporal Mixture-of-Experts Network that (a)
generates 921,600 image slices from 3D scattering volumes simulated at 532 nm
and 355 nm by coupling WRF with an enhanced COSP simulator over 384 land-ocean
time steps, yielding high-quality voxel-wise references; (b) embeds ATB-BC
physical constraints into the model architecture, promoting energy consistency
during restoration; (c) achieves consistent improvements on the Atmos-Bench
dataset across both 355 nm and 532 nm bands, outperforming state-of-the-art
baseline models without relying on auxiliary inputs. Atmos-Bench establishes a
new standard for satellite-based 3D atmospheric structure recovery and paves
the way for deeper climate insight.

</details>


### [118] [A Survey on Interpretability in Visual Recognition](https://arxiv.org/abs/2507.11099)
*Qiyang Wan,Chengzhi Gao,Ruiping Wang,Xilin Chen*

Main category: cs.CV

TL;DR: 本文系统综述了视觉识别模型的可解释性研究，提出了一种以人为中心的分类法，并探讨了评估指标和新技术的应用。


<details>
  <summary>Details</summary>
Motivation: 随着视觉识别模型在关键领域的应用增加，理解其机制和失败原因的需求推动了可解释性研究的发展。

Method: 提出了一种基于意图、对象、呈现和方法的分类法，系统整理了现有XAI方法。

Result: 建立了视觉识别模型可解释性方法的系统性分类标准，并总结了评估指标的需求。

Conclusion: 本文旨在组织现有研究并为未来视觉识别模型的可解释性探索提供启发。

Abstract: In recent years, visual recognition methods have advanced significantly,
finding applications across diverse fields. While researchers seek to
understand the mechanisms behind the success of these models, there is also a
growing impetus to deploy them in critical areas like autonomous driving and
medical diagnostics to better diagnose failures, which promotes the development
of interpretability research. This paper systematically reviews existing
research on the interpretability of visual recognition models and proposes a
taxonomy of methods from a human-centered perspective. The proposed taxonomy
categorizes interpretable recognition methods based on Intent, Object,
Presentation, and Methodology, thereby establishing a systematic and coherent
set of grouping criteria for these XAI methods. Additionally, we summarize the
requirements for evaluation metrics and explore new opportunities enabled by
recent technologies, such as large multimodal models. We aim to organize
existing research in this domain and inspire future investigations into the
interpretability of visual recognition models.

</details>


### [119] [KptLLM++: Towards Generic Keypoint Comprehension with Large Language Model](https://arxiv.org/abs/2507.11102)
*Jie Yang,Wang Zeng,Sheng Jin,Lumin Xu,Wentao Liu,Chen Qian,Zhen Li,Ruimao Zhang*

Main category: cs.CV

TL;DR: KptLLM++是一种新型多模态大语言模型，专注于通过用户指令引导的多模态输入实现通用关键点理解，显著提升了细粒度图像分析的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的多模态大语言模型（MLLMs）在捕捉细粒度语义信息（如对象关键点）方面表现不足，而关键点对于细粒度图像分析、对象检索和行为识别等应用至关重要。

Method: KptLLM++采用了一种新颖的“识别-检测”范式，通过结构化思维链推理机制，先解释关键点语义，再定位其精确位置，并基于超过50万样本的多样化训练数据集进行训练。

Result: 在多个关键点检测基准测试中，KptLLM++表现出最先进的性能，展示了其卓越的准确性和泛化能力。

Conclusion: KptLLM++为细粒度图像理解提供了统一的解决方案，并对人机交互具有潜在的变革性影响。

Abstract: The emergence of Multimodal Large Language Models (MLLMs) has revolutionized
image understanding by bridging textual and visual modalities. However, these
models often struggle with capturing fine-grained semantic information, such as
the precise identification and analysis of object keypoints. Keypoints, as
structure-aware, pixel-level, and compact representations of objects,
particularly articulated ones, play a crucial role in applications such as
fine-grained image analysis, object retrieval, and behavior recognition. In
this paper, we propose KptLLM++, a novel multimodal large language model that
specifically designed for generic keypoint comprehension through the
integration of diverse input modalities guided by user-defined instructions. By
unifying keypoint detection across varied contexts, KptLLM++ establishes itself
as an advanced interface, fostering more effective human-AI collaboration. The
model is built upon a novel identify-then-detect paradigm, which first
interprets keypoint semantics and subsequently localizes their precise
positions through a structured chain-of-thought reasoning mechanism. To push
the boundaries of performance, we have scaled up the training dataset to over
500K samples, encompassing diverse objects, keypoint categories, image styles,
and scenarios with complex occlusions. This extensive scaling enables KptLLM++
to unlock its potential, achieving remarkable accuracy and generalization.
Comprehensive experiments on multiple keypoint detection benchmarks demonstrate
its state-of-the-art performance, underscoring its potential as a unified
solution for fine-grained image understanding and its transformative
implications for human-AI interaction.

</details>


### [120] [Jellyfish Species Identification: A CNN Based Artificial Neural Network Approach](https://arxiv.org/abs/2507.11116)
*Md. Sabbir Hossen,Md. Saiduzzaman,Pabon Shaha,Mostofa Kamal Nasir*

Main category: cs.CV

TL;DR: 提出了一种基于深度学习的框架，用于水母物种检测和分类，结合多种特征提取技术和分类器，最高准确率达98%。


<details>
  <summary>Details</summary>
Motivation: 水母在海洋生态系统中至关重要，但其快速增殖和生态影响对生物多样性和保护构成挑战，需要准确识别物种以进行生态监测和管理。

Method: 整合MobileNetV3、ResNet50、EfficientNetV2-B0和VGG16等特征提取技术，结合传统机器学习分类器和前馈神经网络分类器，并使用softmax函数直接分类。

Result: MobileNetV3与人工神经网络的组合表现最佳，准确率达98%，显著优于其他组合。

Conclusion: 深度学习与混合框架在解决生物多样性挑战和推进海洋物种检测方面具有高效性。

Abstract: Jellyfish, a diverse group of gelatinous marine organisms, play a crucial
role in maintaining marine ecosystems but pose significant challenges for
biodiversity and conservation due to their rapid proliferation and ecological
impact. Accurate identification of jellyfish species is essential for
ecological monitoring and management. In this study, we proposed a deep
learning framework for jellyfish species detection and classification using an
underwater image dataset. The framework integrates advanced feature extraction
techniques, including MobileNetV3, ResNet50, EfficientNetV2-B0, and VGG16,
combined with seven traditional machine learning classifiers and three
Feedforward Neural Network classifiers for precise species identification.
Additionally, we activated the softmax function to directly classify jellyfish
species using the convolutional neural network models. The combination of the
Artificial Neural Network with MobileNetV3 is our best-performing model,
achieving an exceptional accuracy of 98%, significantly outperforming other
feature extractor-classifier combinations. This study demonstrates the efficacy
of deep learning and hybrid frameworks in addressing biodiversity challenges
and advancing species detection in marine environments.

</details>


### [121] [Task-Oriented Human Grasp Synthesis via Context- and Task-Aware Diffusers](https://arxiv.org/abs/2507.11287)
*An-Lun Liu,Yu-Wei Chao,Yi-Ting Chen*

Main category: cs.CV

TL;DR: 论文提出了一种任务导向的人体抓取合成方法，通过任务感知接触图结合场景和任务信息，显著提升了抓取质量和任务性能。


<details>
  <summary>Details</summary>
Motivation: 传统接触图仅考虑物体与手的关系，缺乏任务和场景信息，导致抓取姿势与任务需求不匹配。

Method: 采用两阶段流程：首先生成任务感知接触图，随后基于该图合成任务导向的抓取姿势。

Result: 实验验证了结合场景和任务信息的重要性，新方法在抓取质量和任务性能上优于现有方法。

Conclusion: 任务感知接触图是提升抓取合成任务的关键，未来可扩展至更复杂场景。

Abstract: In this paper, we study task-oriented human grasp synthesis, a new grasp
synthesis task that demands both task and context awareness. At the core of our
method is the task-aware contact maps. Unlike traditional contact maps that
only reason about the manipulated object and its relation with the hand, our
enhanced maps take into account scene and task information. This comprehensive
map is critical for hand-object interaction, enabling accurate grasping poses
that align with the task. We propose a two-stage pipeline that first constructs
a task-aware contact map informed by the scene and task. In the subsequent
stage, we use this contact map to synthesize task-oriented human grasps. We
introduce a new dataset and a metric for the proposed task to evaluate our
approach. Our experiments validate the importance of modeling both scene and
task, demonstrating significant improvements over existing methods in both
grasp quality and task performance. See our project page for more details:
https://hcis-lab.github.io/TOHGS/

</details>


### [122] [Try Harder: Hard Sample Generation and Learning for Clothes-Changing Person Re-ID](https://arxiv.org/abs/2507.11119)
*Hankun Liu,Yujian Zhao,Guanglin Niu*

Main category: cs.CV

TL;DR: 提出了一种多模态引导的硬样本生成与学习框架（HSGL），通过结合文本和视觉模态来定义、生成和优化硬样本，提升了服装变化行人重识别（CC-ReID）的性能。


<details>
  <summary>Details</summary>
Motivation: 硬样本在行人重识别任务中具有挑战性，尤其是在服装变化场景下。其模糊性和缺乏明确定义限制了学习策略的设计和模型的鲁棒性。

Method: HSGL框架包含双粒度硬样本生成（DGHSG）和硬样本自适应学习（HSAL），分别通过多模态合成硬样本和硬度感知优化策略提升模型性能。

Result: 在多个CC-ReID基准测试中表现优异，显著加速了学习过程，并在PRCC和LTCC数据集上达到了最先进水平。

Conclusion: 多模态引导的硬样本生成与学习为CC-ReID提供了有效解决方案，显著提升了模型的判别能力和鲁棒性。

Abstract: Hard samples pose a significant challenge in person re-identification (ReID)
tasks, particularly in clothing-changing person Re-ID (CC-ReID). Their inherent
ambiguity or similarity, coupled with the lack of explicit definitions, makes
them a fundamental bottleneck. These issues not only limit the design of
targeted learning strategies but also diminish the model's robustness under
clothing or viewpoint changes. In this paper, we propose a novel
multimodal-guided Hard Sample Generation and Learning (HSGL) framework, which
is the first effort to unify textual and visual modalities to explicitly
define, generate, and optimize hard samples within a unified paradigm. HSGL
comprises two core components: (1) Dual-Granularity Hard Sample Generation
(DGHSG), which leverages multimodal cues to synthesize semantically consistent
samples, including both coarse- and fine-grained hard positives and negatives
for effectively increasing the hardness and diversity of the training data. (2)
Hard Sample Adaptive Learning (HSAL), which introduces a hardness-aware
optimization strategy that adjusts feature distances based on textual semantic
labels, encouraging the separation of hard positives and drawing hard negatives
closer in the embedding space to enhance the model's discriminative capability
and robustness to hard samples. Extensive experiments on multiple CC-ReID
benchmarks demonstrate the effectiveness of our approach and highlight the
potential of multimodal-guided hard sample generation and learning for robust
CC-ReID. Notably, HSAL significantly accelerates the convergence of the
targeted learning procedure and achieves state-of-the-art performance on both
PRCC and LTCC datasets. The code is available at
https://github.com/undooo/TryHarder-ACMMM25.

</details>


### [123] [MMOne: Representing Multiple Modalities in One Scene](https://arxiv.org/abs/2507.11129)
*Zhifeng Gu,Bing Wang*

Main category: cs.CV

TL;DR: 论文提出了一种名为MMOne的通用框架，用于解决多模态场景表示中的模态冲突问题，通过模态建模模块和多模态分解机制，提升了各模态的表征能力。


<details>
  <summary>Details</summary>
Motivation: 人类通过多模态线索感知世界，但模态间的固有差异（如属性差异和粒度差异）带来了挑战。

Method: 设计了模态建模模块（含模态指示器）和多模态分解机制，将多模态信息解耦为共享和模态特定部分。

Result: 实验表明，该方法能有效提升各模态的表征能力，并具有扩展性。

Conclusion: MMOne框架为多模态场景表示提供了一种紧凑高效的解决方案。

Abstract: Humans perceive the world through multimodal cues to understand and interact
with the environment. Learning a scene representation for multiple modalities
enhances comprehension of the physical world. However, modality conflicts,
arising from inherent distinctions among different modalities, present two
critical challenges: property disparity and granularity disparity. To address
these challenges, we propose a general framework, MMOne, to represent multiple
modalities in one scene, which can be readily extended to additional
modalities. Specifically, a modality modeling module with a novel modality
indicator is proposed to capture the unique properties of each modality.
Additionally, we design a multimodal decomposition mechanism to separate
multi-modal Gaussians into single-modal Gaussians based on modality
differences. We address the essential distinctions among modalities by
disentangling multimodal information into shared and modality-specific
components, resulting in a more compact and efficient multimodal scene
representation. Extensive experiments demonstrate that our method consistently
enhances the representation capability for each modality and is scalable to
additional modalities. The code is available at
https://github.com/Neal2020GitHub/MMOne.

</details>


### [124] [RMAU-NET: A Residual-Multihead-Attention U-Net Architecture for Landslide Segmentation and Detection from Remote Sensing Images](https://arxiv.org/abs/2507.11143)
*Lam Pham,Cam Le,Hieu Tang,Khang Truong,Truong Nguyen,Jasmin Lampert,Alexander Schindler,Martin Boyer,Son Phan*

Main category: cs.CV

TL;DR: 提出了一种基于深度学习的端到端模型，利用遥感图像自动观测滑坡事件，并在多个数据集上取得了高精度结果。


<details>
  <summary>Details</summary>
Motivation: 由于极端天气和人类活动导致滑坡灾害频发，但传统观测方法在大范围和复杂地形中面临挑战。

Method: 设计了一种新型神经网络架构，用于滑坡检测和分割任务，输入为遥感图像。

Result: 在LandSlide4Sense、Bijie和Nepal数据集上，检测任务的F1分数分别为98.23、93.83，分割任务的mIoU分数为63.74、76.88。

Conclusion: 实验结果证明了该模型在实际滑坡观测系统中的潜在应用价值。

Abstract: In recent years, landslide disasters have reported frequently due to the
extreme weather events of droughts, floods , storms, or the consequence of
human activities such as deforestation, excessive exploitation of natural
resources. However, automatically observing landslide is challenging due to the
extremely large observing area and the rugged topography such as mountain or
highland. This motivates us to propose an end-to-end deep-learning-based model
which explores the remote sensing images for automatically observing landslide
events. By considering remote sensing images as the input data, we can obtain
free resource, observe large and rough terrains by time. To explore the remote
sensing images, we proposed a novel neural network architecture which is for
two tasks of landslide detection and landslide segmentation. We evaluated our
proposed model on three different benchmark datasets of LandSlide4Sense, Bijie,
and Nepal. By conducting extensive experiments, we achieve F1 scores of 98.23,
93.83 for the landslide detection task on LandSlide4Sense, Bijie datasets; mIoU
scores of 63.74, 76.88 on the segmentation tasks regarding LandSlide4Sense,
Nepal datasets. These experimental results prove potential to integrate our
proposed model into real-life landslide observation systems.

</details>


### [125] [Assessing Color Vision Test in Large Vision-language Models](https://arxiv.org/abs/2507.11153)
*Hongfei Ye,Bin Chen,Wenxi Liu,Yu Zhang,Zhao Li,Dandan Ni,Hongyang Chen*

Main category: cs.CV

TL;DR: 论文探讨了大型视觉语言模型的色彩视觉能力，提出了一个测试任务和数据集，并分析了错误类型及优化策略。


<details>
  <summary>Details</summary>
Motivation: 大型视觉语言模型的色彩视觉能力尚未被充分研究，填补这一空白是研究的动机。

Method: 定义色彩视觉测试任务，构建多类别、多难度数据集，分析错误并提出微调策略。

Result: 提出了测试框架和数据集，并展示了错误类型及优化方法。

Conclusion: 通过测试和优化，提升了大型视觉语言模型的色彩视觉能力。

Abstract: With the widespread adoption of large vision-language models, the capacity
for color vision in these models is crucial. However, the color vision
abilities of large visual-language models have not yet been thoroughly
explored. To address this gap, we define a color vision testing task for large
vision-language models and construct a dataset \footnote{Anonymous Github
Showing some of the data
https://anonymous.4open.science/r/color-vision-test-dataset-3BCD} that covers
multiple categories of test questions and tasks of varying difficulty levels.
Furthermore, we analyze the types of errors made by large vision-language
models and propose fine-tuning strategies to enhance their performance in color
vision tests.

</details>


### [126] [Clustering-Guided Multi-Layer Contrastive Representation Learning for Citrus Disease Classification](https://arxiv.org/abs/2507.11171)
*Jun Chen,Yonghua Yu,Weifu Li,Yaohui Chen,Hong Chen*

Main category: cs.CV

TL;DR: 本文提出了一种名为CMCRL的自监督多层对比表示学习算法，用于柑橘病害检测与分类，通过优化未标注样本和分层特征学习，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 柑橘病害严重影响产量，传统深度学习方法依赖大量标注数据，而CMCRL旨在减少标注需求并提高分类准确性。

Method: 引入聚类中心对比和多层对比训练（MCT）设计，提出CMCRL算法，利用未标注样本优化并适应病害症状相似性。

Result: 在公共数据集CDD上，CMCRL性能优于现有方法4.5%-30.1%，并接近全监督方法的性能。

Conclusion: CMCRL在减少标注依赖的同时，实现了高精度和鲁棒性，为柑橘病害检测提供了有效解决方案。

Abstract: Citrus, as one of the most economically important fruit crops globally,
suffers severe yield depressions due to various diseases. Accurate disease
detection and classification serve as critical prerequisites for implementing
targeted control measures. Recent advancements in artificial intelligence,
particularly deep learning-based computer vision algorithms, have substantially
decreased time and labor requirements while maintaining the accuracy of
detection and classification. Nevertheless, these methods predominantly rely on
massive, high-quality annotated training examples to attain promising
performance. By introducing two key designs: contrasting with cluster centroids
and a multi-layer contrastive training (MCT) paradigm, this paper proposes a
novel clustering-guided self-supervised multi-layer contrastive representation
learning (CMCRL) algorithm. The proposed method demonstrates several advantages
over existing counterparts: (1) optimizing with massive unannotated samples;
(2) effective adaptation to the symptom similarity across distinct citrus
diseases; (3) hierarchical feature representation learning. The proposed method
achieves state-of-the-art performance on the public citrus image set CDD,
outperforming existing methods by 4.5\%-30.1\% accuracy. Remarkably, our method
narrows the performance gap with fully supervised counterparts (all samples are
labeled). Beyond classification accuracy, our method shows great performance on
other evaluation metrics (F1 score, precision, and recall), highlighting the
robustness against the class imbalance challenge.

</details>


### [127] [How Far Have Medical Vision-Language Models Come? A Comprehensive Benchmarking Study](https://arxiv.org/abs/2507.11200)
*Che Liu,Jiazhen Pan,Weixiang Shen,Wenjia Bai,Daniel Rueckert,Rossella Arcucci*

Main category: cs.CV

TL;DR: 评估通用和医学专用视觉语言模型在医疗任务中的表现，发现通用模型在某些任务上优于医学专用模型，但推理能力不足，且性能差异大，尚未达到临床可靠性。


<details>
  <summary>Details</summary>
Motivation: 探索视觉语言模型在医疗任务中的能力，填补现有研究的空白。

Method: 对开源通用和医学专用视觉语言模型（3B至72B参数）在八个基准测试上进行评估，分为理解和推理两部分。

Result: 通用模型在部分任务上表现优于医学专用模型；推理能力普遍低于理解能力；不同基准测试性能差异显著。

Conclusion: 当前模型尚未达到临床部署的可靠性标准，需加强多模态对齐和更严格的评估协议。

Abstract: Vision-Language Models (VLMs) trained on web-scale corpora excel at natural
image tasks and are increasingly repurposed for healthcare; however, their
competence in medical tasks remains underexplored. We present a comprehensive
evaluation of open-source general-purpose and medically specialised VLMs,
ranging from 3B to 72B parameters, across eight benchmarks: MedXpert,
OmniMedVQA, PMC-VQA, PathVQA, MMMU, SLAKE, and VQA-RAD. To observe model
performance across different aspects, we first separate it into understanding
and reasoning components. Three salient findings emerge. First, large
general-purpose models already match or surpass medical-specific counterparts
on several benchmarks, demonstrating strong zero-shot transfer from natural to
medical images. Second, reasoning performance is consistently lower than
understanding, highlighting a critical barrier to safe decision support. Third,
performance varies widely across benchmarks, reflecting differences in task
design, annotation quality, and knowledge demands. No model yet reaches the
reliability threshold for clinical deployment, underscoring the need for
stronger multimodal alignment and more rigorous, fine-grained evaluation
protocols.

</details>


### [128] [A Robust Incomplete Multimodal Low-Rank Adaptation Approach for Emotion Recognition](https://arxiv.org/abs/2507.11202)
*Xinkui Zhao,Jinsong Shu,Yangyang Wu,Guanjie Cheng,Zihe Liu,Naibo Wang,Shuiguang Deng,Zhongle Xie,Jianwei Yin*

Main category: cs.CV

TL;DR: MCULoRA是一种基于模态组合的单模态解耦动态低秩适应方法，用于高效训练不完全多模态学习模型，显著提升了任务准确性。


<details>
  <summary>Details</summary>
Motivation: 实际应用中多模态情感识别常因传感器故障或隐私保护导致模态缺失，现有方法因模态组合训练梯度冲突而性能受限。

Method: 提出MCULoRA框架，包含模态组合感知低秩适应（MCLA）和动态参数微调（DPFT）模块，分别解耦模态共享信息并优化训练比例。

Result: 在多个基准数据集上实验表明，MCULoRA显著优于现有不完全多模态学习方法。

Conclusion: MCULoRA通过解耦和动态调整模态组合训练，有效解决了不完全多模态学习中的梯度冲突问题。

Abstract: Multimodal Emotion Recognition (MER) often encounters incomplete
multimodality in practical applications due to sensor failures or privacy
protection requirements. While existing methods attempt to address various
incomplete multimodal scenarios by balancing the training of each modality
combination through additional gradients, these approaches face a critical
limitation: training gradients from different modality combinations conflict
with each other, ultimately degrading the performance of the final prediction
model. In this paper, we propose a unimodal decoupled dynamic low-rank
adaptation method based on modality combinations, named MCULoRA, which is a
novel framework for the parameter-efficient training of incomplete multimodal
learning models. MCULoRA consists of two key modules, modality combination
aware low-rank adaptation (MCLA) and dynamic parameter fine-tuning (DPFT). The
MCLA module effectively decouples the shared information from the distinct
characteristics of individual modality combinations. The DPFT module adjusts
the training ratio of modality combinations based on the separability of each
modality's representation space, optimizing the learning efficiency across
different modality combinations. Our extensive experimental evaluation in
multiple benchmark datasets demonstrates that MCULoRA substantially outperforms
previous incomplete multimodal learning approaches in downstream task accuracy.

</details>


### [129] [NarrLV: Towards a Comprehensive Narrative-Centric Evaluation for Long Video Generation Models](https://arxiv.org/abs/2507.11245)
*X. Feng,H. Yu,M. Wu,S. Hu,J. Chen,C. Zhu,J. Wu,X. Chu,K. Huang*

Main category: cs.CV

TL;DR: 提出了首个针对长视频生成模型的叙事表达评估基准NarrLV，通过Temporal Narrative Atom（TNA）量化叙事丰富度，并设计了基于MLLM的评估指标。


<details>
  <summary>Details</summary>
Motivation: 当前长视频生成模型缺乏专门评估叙事表达能力的基准，现有基准（如VBench）仅支持简单叙事提示。

Method: 1. 引入TNA作为基本叙事单元；2. 构建自动提示生成管道；3. 设计基于MLLM的评估指标。

Result: 实验表明，该指标与人类判断高度一致，揭示了当前模型在叙事表达上的能力边界。

Conclusion: NarrLV为长视频生成模型的叙事表达能力提供了全面评估工具，填补了研究空白。

Abstract: With the rapid development of foundation video generation technologies, long
video generation models have exhibited promising research potential thanks to
expanded content creation space. Recent studies reveal that the goal of long
video generation tasks is not only to extend video duration but also to
accurately express richer narrative content within longer videos. However, due
to the lack of evaluation benchmarks specifically designed for long video
generation models, the current assessment of these models primarily relies on
benchmarks with simple narrative prompts (e.g., VBench). To the best of our
knowledge, our proposed NarrLV is the first benchmark to comprehensively
evaluate the Narrative expression capabilities of Long Video generation models.
Inspired by film narrative theory, (i) we first introduce the basic narrative
unit maintaining continuous visual presentation in videos as Temporal Narrative
Atom (TNA), and use its count to quantitatively measure narrative richness.
Guided by three key film narrative elements influencing TNA changes, we
construct an automatic prompt generation pipeline capable of producing
evaluation prompts with a flexibly expandable number of TNAs. (ii) Then, based
on the three progressive levels of narrative content expression, we design an
effective evaluation metric using the MLLM-based question generation and
answering framework. (iii) Finally, we conduct extensive evaluations on
existing long video generation models and the foundation generation models.
Experimental results demonstrate that our metric aligns closely with human
judgments. The derived evaluation outcomes reveal the detailed capability
boundaries of current video generation models in narrative content expression.

</details>


### [130] [Fairness-Aware Grouping for Continuous Sensitive Variables: Application for Debiasing Face Analysis with respect to Skin Tone](https://arxiv.org/abs/2507.11247)
*Veronika Shilova,Emmanuel Malherbe,Giovanni Palma,Laurent Risser,Jean-Michel Loubes*

Main category: cs.CV

TL;DR: 提出了一种基于公平性的分组方法，用于处理连续敏感属性，通过最大化组间歧视差异的新标准来识别关键子群体。


<details>
  <summary>Details</summary>
Motivation: 现有方法在处理连续敏感属性（如肤色）时，默认分组可能忽略少数群体的歧视问题。

Method: 提出一种分组方法，根据观察到的歧视水平对数据进行分组，并最大化组间歧视差异。

Result: 在合成数据集和真实数据集（CelebA、FFHQ）上验证了方法的有效性，揭示了更细微的歧视模式，并通过后处理提高了公平性。

Conclusion: 该方法在保持准确性的同时提高了公平性，适用于工业部署。

Abstract: Within a legal framework, fairness in datasets and models is typically
assessed by dividing observations into predefined groups and then computing
fairness measures (e.g., Disparate Impact or Equality of Odds with respect to
gender). However, when sensitive attributes such as skin color are continuous,
dividing into default groups may overlook or obscure the discrimination
experienced by certain minority subpopulations. To address this limitation, we
propose a fairness-based grouping approach for continuous (possibly
multidimensional) sensitive attributes. By grouping data according to observed
levels of discrimination, our method identifies the partition that maximizes a
novel criterion based on inter-group variance in discrimination, thereby
isolating the most critical subgroups.
  We validate the proposed approach using multiple synthetic datasets and
demonstrate its robustness under changing population distributions - revealing
how discrimination is manifested within the space of sensitive attributes.
Furthermore, we examine a specialized setting of monotonic fairness for the
case of skin color. Our empirical results on both CelebA and FFHQ, leveraging
the skin tone as predicted by an industrial proprietary algorithm, show that
the proposed segmentation uncovers more nuanced patterns of discrimination than
previously reported, and that these findings remain stable across datasets for
a given model. Finally, we leverage our grouping model for debiasing purpose,
aiming at predicting fair scores with group-by-group post-processing. The
results demonstrate that our approach improves fairness while having minimal
impact on accuracy, thus confirming our partition method and opening the door
for industrial deployment.

</details>


### [131] [MFGDiffusion: Mask-Guided Smoke Synthesis for Enhanced Forest Fire Detection](https://arxiv.org/abs/2507.11252)
*Guanghao Wu,Chen Xu,Hai Song,Chong Wang,Qixing Zhang*

Main category: cs.CV

TL;DR: 提出了一种生成森林火灾烟雾图像的框架，通过改进修复模型和引入新损失函数，生成高质量烟雾图像，提升烟雾检测性能。


<details>
  <summary>Details</summary>
Motivation: 解决烟雾图像数据稀缺及现有修复模型生成烟雾图像质量不足的问题。

Method: 使用预训练分割和多模态模型获取烟雾掩码和图像描述，提出掩码和掩码图像特征引导的网络架构及新损失函数，结合多模态大语言模型筛选图像。

Result: 生成的烟雾图像真实多样，有效提升了烟雾检测模型的性能。

Conclusion: 提出的框架能生成高质量烟雾图像，为森林火灾烟雾检测提供有力支持。

Abstract: Smoke is the first visible indicator of a wildfire.With the advancement of
deep learning, image-based smoke detection has become a crucial method for
detecting and preventing forest fires. However, the scarcity of smoke image
data from forest fires is one of the significant factors hindering the
detection of forest fire smoke. Image generation models offer a promising
solution for synthesizing realistic smoke images. However, current inpainting
models exhibit limitations in generating high-quality smoke representations,
particularly manifesting as inconsistencies between synthesized smoke and
background contexts. To solve these problems, we proposed a comprehensive
framework for generating forest fire smoke images. Firstly, we employed the
pre-trained segmentation model and the multimodal model to obtain smoke masks
and image captions.Then, to address the insufficient utilization of masks and
masked images by inpainting models, we introduced a network architecture guided
by mask and masked image features. We also proposed a new loss function, the
mask random difference loss, which enhances the consistency of the generated
effects around the mask by randomly expanding and eroding the mask
edges.Finally, to generate a smoke image dataset using random masks for
subsequent detection tasks, we incorporated smoke characteristics and use a
multimodal large language model as a filtering tool to select diverse and
reasonable smoke images, thereby improving the quality of the synthetic
dataset. Experiments showed that our generated smoke images are realistic and
diverse, and effectively enhance the performance of forest fire smoke detection
models. Code is available at https://github.com/wghr123/MFGDiffusion.

</details>


### [132] [ViewSRD: 3D Visual Grounding via Structured Multi-View Decomposition](https://arxiv.org/abs/2507.11261)
*Ronggang Huang,Haoxin Yang,Yan Cai,Xuemiao Xu,Huaidong Zhang,Shengfeng He*

Main category: cs.CV

TL;DR: ViewSRD框架通过结构化多视角分解解决3D视觉定位中的复杂多锚点查询和视角变化问题，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有方法难以处理复杂多锚点查询和视角变化导致的空间描述不一致问题。

Method: 提出ViewSRD框架，包含Simple Relation Decoupling模块解耦多锚点查询，Multi-view Textual-Scene Interaction模块整合多视角特征，以及Textual-Scene Reasoning模块综合预测。

Result: 在3D视觉定位数据集上表现显著优于现有方法，尤其在复杂空间区分任务中。

Conclusion: ViewSRD通过结构化多视角分解有效解决了3D视觉定位中的复杂问题。

Abstract: 3D visual grounding aims to identify and localize objects in a 3D space based
on textual descriptions. However, existing methods struggle with disentangling
targets from anchors in complex multi-anchor queries and resolving
inconsistencies in spatial descriptions caused by perspective variations. To
tackle these challenges, we propose ViewSRD, a framework that formulates 3D
visual grounding as a structured multi-view decomposition process. First, the
Simple Relation Decoupling (SRD) module restructures complex multi-anchor
queries into a set of targeted single-anchor statements, generating a
structured set of perspective-aware descriptions that clarify positional
relationships. These decomposed representations serve as the foundation for the
Multi-view Textual-Scene Interaction (Multi-TSI) module, which integrates
textual and scene features across multiple viewpoints using shared, Cross-modal
Consistent View Tokens (CCVTs) to preserve spatial correlations. Finally, a
Textual-Scene Reasoning module synthesizes multi-view predictions into a
unified and robust 3D visual grounding. Experiments on 3D visual grounding
datasets show that ViewSRD significantly outperforms state-of-the-art methods,
particularly in complex queries requiring precise spatial differentiation.

</details>


### [133] [YOLOatr : Deep Learning Based Automatic Target Detection and Localization in Thermal Infrared Imagery](https://arxiv.org/abs/2507.11267)
*Aon Safdar,Usman Akram,Waseem Anwar,Basit Malik,Mian Ibad Ali*

Main category: cs.CV

TL;DR: 论文提出了一种改进的单阶段检测器YOLOatr，用于热红外图像中的目标检测与识别，解决了现有深度学习模型在该领域的性能不足问题。


<details>
  <summary>Details</summary>
Motivation: 热红外图像在国防和监控领域的目标检测与识别面临诸多挑战，如数据集有限、硬件限制、天气影响等，导致现有深度学习模型表现不佳。

Method: 基于改进的YOLOv5s，优化了检测头、特征融合和自定义数据增强策略，提出了YOLOatr模型。

Result: 在DSIAC MWIR数据集上，YOLOatr实现了高达99.6%的识别准确率。

Conclusion: YOLOatr在热红外目标识别任务中表现出色，达到了当前最优性能。

Abstract: Automatic Target Detection (ATD) and Recognition (ATR) from Thermal Infrared
(TI) imagery in the defense and surveillance domain is a challenging computer
vision (CV) task in comparison to the commercial autonomous vehicle perception
domain. Limited datasets, peculiar domain-specific and TI modality-specific
challenges, i.e., limited hardware, scale invariance issues due to greater
distances, deliberate occlusion by tactical vehicles, lower sensor resolution
and resultant lack of structural information in targets, effects of weather,
temperature, and time of day variations, and varying target to clutter ratios
all result in increased intra-class variability and higher inter-class
similarity, making accurate real-time ATR a challenging CV task. Resultantly,
contemporary state-of-the-art (SOTA) deep learning architectures underperform
in the ATR domain. We propose a modified anchor-based single-stage detector,
called YOLOatr, based on a modified YOLOv5s, with optimal modifications to the
detection heads, feature fusion in the neck, and a custom augmentation profile.
We evaluate the performance of our proposed model on a comprehensive DSIAC MWIR
dataset for real-time ATR over both correlated and decorrelated testing
protocols. The results demonstrate that our proposed model achieves
state-of-the-art ATR performance of up to 99.6%.

</details>


### [134] [Tomato Multi-Angle Multi-Pose Dataset for Fine-Grained Phenotyping](https://arxiv.org/abs/2507.11279)
*Yujie Zhang,Sabine Struckmeyer,Andreas Kolb,Sven Reichardt*

Main category: cs.CV

TL;DR: TomatoMAP是一个基于IoT的番茄植物表型数据集，包含64,464张RGB图像和精细标注，通过深度学习框架验证其准确性，达到与专家相当的水平。


<details>
  <summary>Details</summary>
Motivation: 传统植物表型分析方法存在观察者偏见和不一致性，影响准确性和可重复性。

Method: 开发了TomatoMAP数据集，采用IoT成像系统和标准化协议，结合MobileNetv3、YOLOv11和MaskRCNN的深度学习框架进行验证。

Result: 模型在精细表型分析中达到与专家相当的准确性和速度，Cohen's Kappa和热图验证了方法的可靠性。

Conclusion: TomatoMAP为植物表型研究提供了高精度、可重复的自动化解决方案。

Abstract: Observer bias and inconsistencies in traditional plant phenotyping methods
limit the accuracy and reproducibility of fine-grained plant analysis. To
overcome these challenges, we developed TomatoMAP, a comprehensive dataset for
Solanum lycopersicum using an Internet of Things (IoT) based imaging system
with standardized data acquisition protocols. Our dataset contains 64,464 RGB
images that capture 12 different plant poses from four camera elevation angles.
Each image includes manually annotated bounding boxes for seven regions of
interest (ROIs), including leaves, panicle, batch of flowers, batch of fruits,
axillary shoot, shoot and whole plant area, along with 50 fine-grained growth
stage classifications based on the BBCH scale. Additionally, we provide 3,616
high-resolution image subset with pixel-wise semantic and instance segmentation
annotations for fine-grained phenotyping. We validated our dataset using a
cascading model deep learning framework combining MobileNetv3 for
classification, YOLOv11 for object detection, and MaskRCNN for segmentation.
Through AI vs. Human analysis involving five domain experts, we demonstrate
that the models trained on our dataset achieve accuracy and speed comparable to
the experts. Cohen's Kappa and inter-rater agreement heatmap confirm the
reliability of automated fine-grained phenotyping using our approach.

</details>


### [135] [Detección y Cuantificación de Erosión Fluvial con Visión Artificial](https://arxiv.org/abs/2507.11301)
*Paúl Maji,Marlon Túquerres,Stalin Valencia,Marcela Valenzuela,Christian Mejia-Escobar*

Main category: cs.CV

TL;DR: 论文提出了一种基于人工智能的方法，利用YOLOv11模型自动识别侵蚀区域并估算其面积，开发了交互式网页应用EROSCAN。


<details>
  <summary>Details</summary>
Motivation: 传统方法需要专业知识且处理繁琐，因此研究旨在通过AI优化侵蚀检测与量化。

Method: 使用YOLOv11模型，结合照片和LiDAR图像训练，通过Roboflow平台进行数据标注和分割。

Result: 实验显示模型能高效检测侵蚀模式（准确率70%），并可靠计算侵蚀面积。

Conclusion: EROSCAN系统为决策者提供了便捷工具，优化了风险管理和土地规划。

Abstract: Fluvial erosion is a natural process that can generate significant impacts on
soil stability and strategic infrastructures. The detection and monitoring of
this phenomenon is traditionally addressed by photogrammetric methods and
analysis in geographic information systems. These tasks require specific
knowledge and intensive manual processing. This study proposes an artificial
intelligence-based approach for automatic identification of eroded zones and
estimation of their area. The state-of-the-art computer vision model YOLOv11,
adjusted by fine-tuning and trained with photographs and LiDAR images, is used.
This combined dataset was segmented and labeled using the Roboflow platform.
Experimental results indicate efficient detection of erosion patterns with an
accuracy of 70%, precise identification of eroded areas and reliable
calculation of their extent in pixels and square meters. As a final product,
the EROSCAN system has been developed, an interactive web application that
allows users to upload images and obtain automatic segmentations of fluvial
erosion, together with the estimated area. This tool optimizes the detection
and quantification of the phenomenon, facilitating decision making in risk
management and territorial planning.

</details>


### [136] [A Mixed-Primitive-based Gaussian Splatting Method for Surface Reconstruction](https://arxiv.org/abs/2507.11321)
*Haoxuan Qu,Yujun Cai,Hossein Rahmani,Ajay Kumar,Junsong Yuan,Jun Liu*

Main category: cs.CV

TL;DR: 提出了一种新的高斯泼溅框架，首次在表面重建中结合多种几何基元，提升了重建质量。


<details>
  <summary>Details</summary>
Motivation: 现有高斯泼溅方法仅使用单一基元（椭圆或椭球）表示复杂多样的物体表面，限制了重建质量。

Method: 提出组合泼溅策略、混合基元初始化策略和顶点修剪机制，支持多种基元的高效重建。

Result: 实验证明框架有效，实现了高精度的表面重建。

Conclusion: 通过引入多种基元，显著提升了高斯泼溅的表面重建能力。

Abstract: Recently, Gaussian Splatting (GS) has received a lot of attention in surface
reconstruction. However, while 3D objects can be of complex and diverse shapes
in the real world, existing GS-based methods only limitedly use a single type
of splatting primitive (Gaussian ellipse or Gaussian ellipsoid) to represent
object surfaces during their reconstruction. In this paper, we highlight that
this can be insufficient for object surfaces to be represented in high quality.
Thus, we propose a novel framework that, for the first time, enables Gaussian
Splatting to incorporate multiple types of (geometrical) primitives during its
surface reconstruction process. Specifically, in our framework, we first
propose a compositional splatting strategy, enabling the splatting and
rendering of different types of primitives in the Gaussian Splatting pipeline.
In addition, we also design our framework with a mixed-primitive-based
initialization strategy and a vertex pruning mechanism to further promote its
surface representation learning process to be well executed leveraging
different types of primitives. Extensive experiments show the efficacy of our
framework and its accurate surface reconstruction performance.

</details>


### [137] [MonoMVSNet: Monocular Priors Guided Multi-View Stereo Network](https://arxiv.org/abs/2507.11333)
*Jianfei Jiang,Qiankun Liu,Haochen Yu,Hongyuan Liu,Liyong Wang,Jiansheng Chen,Huimin Ma*

Main category: cs.CV

TL;DR: MonoMVSNet结合单目深度估计和多视角立体视觉，通过注意力机制和动态深度候选更新，提升了在纹理缺失和反射区域的性能。


<details>
  <summary>Details</summary>
Motivation: 现有MVS方法在纹理缺失和反射区域表现不佳，而单目深度估计在这些区域具有优势。

Method: 提出MonoMVSNet，通过注意力机制整合单目特征，动态更新深度候选，并设计相对一致性损失。

Result: 在DTU和Tanks-and-Temples数据集上达到SOTA性能，排名第一。

Conclusion: MonoMVSNet有效结合单目和多视角优势，显著提升了MVS性能。

Abstract: Learning-based Multi-View Stereo (MVS) methods aim to predict depth maps for
a sequence of calibrated images to recover dense point clouds. However,
existing MVS methods often struggle with challenging regions, such as
textureless regions and reflective surfaces, where feature matching fails. In
contrast, monocular depth estimation inherently does not require feature
matching, allowing it to achieve robust relative depth estimation in these
regions. To bridge this gap, we propose MonoMVSNet, a novel monocular feature
and depth guided MVS network that integrates powerful priors from a monocular
foundation model into multi-view geometry. Firstly, the monocular feature of
the reference view is integrated into source view features by the attention
mechanism with a newly designed cross-view position encoding. Then, the
monocular depth of the reference view is aligned to dynamically update the
depth candidates for edge regions during the sampling procedure. Finally, a
relative consistency loss is further designed based on the monocular depth to
supervise the depth prediction. Extensive experiments demonstrate that
MonoMVSNet achieves state-of-the-art performance on the DTU and
Tanks-and-Temples datasets, ranking first on the Tanks-and-Temples Intermediate
and Advanced benchmarks. The source code is available at
https://github.com/JianfeiJ/MonoMVSNet.

</details>


### [138] [UGC-VideoCaptioner: An Omni UGC Video Detail Caption Model and New Benchmarks](https://arxiv.org/abs/2507.11336)
*Peiran Wu,Yunze Liu,Zhengdong Zhu,Enmin Zhou,Shawn Shen*

Main category: cs.CV

TL;DR: 论文提出了UGC-VideoCap，一个专注于音频和视觉平衡整合的新基准和模型框架，用于短格式用户生成视频的详细多模态字幕生成。


<details>
  <summary>Details</summary>
Motivation: 现有视频字幕生成模型主要依赖视觉信息，忽视了音频在场景动态、说话者意图和叙事背景中的重要作用，缺乏全面的数据集和轻量级模型。

Method: 通过三阶段人工标注流程（音频、视觉、联合音频视觉语义）构建了包含1000个TikTok视频的数据集，并提出UGC-VideoCaptioner(3B)模型，采用两阶段训练策略（监督微调和GRPO）。

Result: UGC-VideoCap基准和模型为无约束真实世界用户生成视频的多模态字幕生成提供了高质量基础和数据高效解决方案。

Conclusion: 该研究填补了多模态视频理解的空白，为未来研究提供了新的方向和工具。

Abstract: Real-world user-generated videos, especially on platforms like TikTok, often
feature rich and intertwined audio visual content. However, existing video
captioning benchmarks and models remain predominantly visual centric,
overlooking the crucial role of audio in conveying scene dynamics, speaker
intent, and narrative context. This lack of omni datasets and lightweight,
capable models hampers progress in fine grained, multimodal video
understanding. To address these challenges, we introduce UGC-VideoCap, a new
benchmark and model framework specifically designed for detailed omnimodal
captioning of short form user-generated videos. Unlike prior datasets,
UGC-VideoCap emphasizes balanced integration of audio and visual modalities,
featuring 1000 TikTok videos annotated through a structured three stage
human-in-the-loop pipeline covering audio only, visual only, and joint audio
visual semantics. The benchmark also includes 4000 carefully crafted QA pairs
probing both unimodal and cross modal understanding. Alongside the dataset, we
propose UGC-VideoCaptioner(3B), a 3B parameter captioning model distilled from
Gemini 2.5 Flash. Using a novel two-stage training strategy supervised fine
tuning followed by Group Relative Policy Optimization (GRPO), our approach
enables efficient adaptation from limited data while maintaining competitive
performance. Together, our benchmark and model offer a high-quality foundation
and a data-efficient solution for advancing omnimodal video captioning in
unconstrained real-world UGC settings.

</details>


### [139] [Attributes Shape the Embedding Space of Face Recognition Models](https://arxiv.org/abs/2507.11372)
*Pierrick Leroy,Antonio Mastropietro,Marco Nurisso,Francesco Vaccarino*

Main category: cs.CV

TL;DR: 论文提出了一种几何方法，用于分析人脸识别模型中嵌入空间的多尺度几何结构，并引入了一种物理启发的对齐度量。


<details>
  <summary>Details</summary>
Motivation: 研究发现，尽管人脸识别模型主要依赖身份信息，但嵌入空间中存在受可解释面部和图像属性（如发色、对比度）影响的多尺度几何结构。

Method: 提出了一种几何方法来描述模型对这些属性的依赖性或不变性，并引入了一种物理启发的对齐度量。在简化的模型和经过合成数据微调的广泛使用的人脸识别模型上进行了评估。

Result: 研究发现，模型对不同属性表现出不同程度的不变性，揭示了其优势和弱点，并提供了更深的可解释性。

Conclusion: 该研究为人脸识别模型的嵌入空间提供了新的几何视角，增强了模型的可解释性。

Abstract: Face Recognition (FR) tasks have made significant progress with the advent of
Deep Neural Networks, particularly through margin-based triplet losses that
embed facial images into high-dimensional feature spaces. During training,
these contrastive losses focus exclusively on identity information as labels.
However, we observe a multiscale geometric structure emerging in the embedding
space, influenced by interpretable facial (e.g., hair color) and image
attributes (e.g., contrast). We propose a geometric approach to describe the
dependence or invariance of FR models to these attributes and introduce a
physics-inspired alignment metric. We evaluate the proposed metric on
controlled, simplified models and widely used FR models fine-tuned with
synthetic data for targeted attribute augmentation. Our findings reveal that
the models exhibit varying degrees of invariance across different attributes,
providing insight into their strengths and weaknesses and enabling deeper
interpretability. Code available here:
https://github.com/mantonios107/attrs-fr-embs}{https://github.com/mantonios107/attrs-fr-embs

</details>


### [140] [Implementing Adaptations for Vision AutoRegressive Model](https://arxiv.org/abs/2507.11441)
*Kaif Shaikh,Antoni Kowalczuk,Franziska Boenisch,Adam Dziedzic*

Main category: cs.CV

TL;DR: VAR模型在图像生成领域作为扩散模型的替代方案，本文研究了其适应技术，尤其是针对医疗数据生成的微调方法，并比较了其与扩散模型在隐私保护和非隐私保护任务中的表现。


<details>
  <summary>Details</summary>
Motivation: 研究VAR模型在特定下游任务（如医疗数据生成）中的适应技术，填补其在隐私保护适应方面的研究空白，并与扩散模型进行比较。

Method: 实现并测试了多种VAR适应策略，包括隐私保护和非隐私保护方法，并与扩散模型的先进适应策略进行对比。

Result: VAR在非隐私保护任务中表现优于扩散模型，但在隐私保护任务中表现不佳，需要进一步研究。

Conclusion: VAR在非隐私保护任务中具有优势，但在隐私保护适应方面仍需改进，未来研究应关注此方向。

Abstract: Vision AutoRegressive model (VAR) was recently introduced as an alternative
to Diffusion Models (DMs) in image generation domain. In this work we focus on
its adaptations, which aim to fine-tune pre-trained models to perform specific
downstream tasks, like medical data generation. While for DMs there exist many
techniques, adaptations for VAR remain underexplored. Similarly, differentially
private (DP) adaptations-ones that aim to preserve privacy of the adaptation
data-have been extensively studied for DMs, while VAR lacks such solutions. In
our work, we implement and benchmark many strategies for VAR, and compare them
to state-of-the-art DM adaptation strategies. We observe that VAR outperforms
DMs for non-DP adaptations, however, the performance of DP suffers, which
necessitates further research in private adaptations for VAR. Code is available
at https://github.com/sprintml/finetuning_var_dp.

</details>


### [141] [COLI: A Hierarchical Efficient Compressor for Large Images](https://arxiv.org/abs/2507.11443)
*Haoran Wang,Hanyu Pei,Yang Lyu,Kai Zhang,Li Li,Feng-Lei Fan*

Main category: cs.CV

TL;DR: 论文提出COLI框架，利用神经表示视频（NeRV）技术解决大图像压缩问题，通过加速训练和超压缩技术提升效率和压缩比。


<details>
  <summary>Details</summary>
Motivation: 高分辨率大视场图像的压缩需求增加，传统方法难以保留细节，数据驱动方法泛化性差，INR技术虽好但压缩速度慢且压缩比不理想。

Method: COLI框架采用NeRV技术，通过预训练-微调范式、混合精度训练和并行化目标加速收敛，并引入超压缩技术提升压缩比。

Result: 在两个医学影像数据集上，COLI在PSNR和SSIM指标上表现优异，压缩比显著降低，训练速度提升4倍。

Conclusion: COLI框架有效解决了INR在大图像压缩中的速度与压缩比问题，为高效图像压缩提供了新思路。

Abstract: The escalating adoption of high-resolution, large-field-of-view imagery
amplifies the need for efficient compression methodologies. Conventional
techniques frequently fail to preserve critical image details, while
data-driven approaches exhibit limited generalizability. Implicit Neural
Representations (INRs) present a promising alternative by learning continuous
mappings from spatial coordinates to pixel intensities for individual images,
thereby storing network weights rather than raw pixels and avoiding the
generalization problem. However, INR-based compression of large images faces
challenges including slow compression speed and suboptimal compression ratios.
To address these limitations, we introduce COLI (Compressor for Large Images),
a novel framework leveraging Neural Representations for Videos (NeRV). First,
recognizing that INR-based compression constitutes a training process, we
accelerate its convergence through a pretraining-finetuning paradigm,
mixed-precision training, and reformulation of the sequential loss into a
parallelizable objective. Second, capitalizing on INRs' transformation of image
storage constraints into weight storage, we implement Hyper-Compression, a
novel post-training technique to substantially enhance compression ratios while
maintaining minimal output distortion. Evaluations across two medical imaging
datasets demonstrate that COLI consistently achieves competitive or superior
PSNR and SSIM metrics at significantly reduced bits per pixel (bpp), while
accelerating NeRV training by up to 4 times.

</details>


### [142] [HUG-VAS: A Hierarchical NURBS-Based Generative Model for Aortic Geometry Synthesis and Controllable Editing](https://arxiv.org/abs/2507.11474)
*Pan Du,Mingqi Xu,Xiaozhi Zhu,Jian-xun Wang*

Main category: cs.CV

TL;DR: HUG-VAS是一种基于NURBS和扩散生成模型的分层生成框架，用于合成高保真主动脉几何结构，解决了传统统计形状建模在复杂拓扑结构中的局限性。


<details>
  <summary>Details</summary>
Motivation: 传统统计形状建模方法依赖线性假设，难以处理复杂血管结构（如多分支主动脉），限制了其表达能力和可扩展性。

Method: HUG-VAS结合NURBS表面参数化和扩散生成模型，采用分层架构：一个扩散模型生成中心线，另一个生成基于中心线的径向轮廓。

Result: 模型通过21个患者样本训练，生成的主动脉几何结构在生物标志物分布上与原始数据高度匹配，并支持零样本条件生成。

Conclusion: HUG-VAS首次通过NURBS参数化和分层扩散过程，将图像先验与生成形状建模统一集成，具有广泛的实际应用潜力。

Abstract: Accurate characterization of vascular geometry is essential for
cardiovascular diagnosis and treatment planning. Traditional statistical shape
modeling (SSM) methods rely on linear assumptions, limiting their expressivity
and scalability to complex topologies such as multi-branch vascular structures.
We introduce HUG-VAS, a Hierarchical NURBS Generative model for Vascular
geometry Synthesis, which integrates NURBS surface parameterization with
diffusion-based generative modeling to synthesize realistic, fine-grained
aortic geometries. Trained with 21 patient-specific samples, HUG-VAS generates
anatomically faithful aortas with supra-aortic branches, yielding biomarker
distributions that closely match those of the original dataset. HUG-VAS adopts
a hierarchical architecture comprising a denoising diffusion model that
generates centerlines and a guided diffusion model that synthesizes radial
profiles conditioned on those centerlines, thereby capturing two layers of
anatomical variability. Critically, the framework supports zero-shot
conditional generation from image-derived priors, enabling practical
applications such as interactive semi-automatic segmentation, robust
reconstruction under degraded imaging conditions, and implantable device
optimization. To our knowledge, HUG-VAS is the first SSM framework to bridge
image-derived priors with generative shape modeling via a unified integration
of NURBS parameterization and hierarchical diffusion processes.

</details>


### [143] [C-FBI: A Combinatorial method using Convolutions for Circle Fitting in Blurry Images](https://arxiv.org/abs/2507.11476)
*Esteban Román Catafau,Torbjörn E. M. Nordling*

Main category: cs.CV

TL;DR: 3C-FBI是一种结合组合边缘像素采样和卷积参数空间密度估计的算法，用于模糊图像中的圆检测和拟合，在精度和速度上优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 解决在退化成像条件下鲁棒的圆检测和拟合这一基础计算机视觉挑战。

Method: 结合组合边缘像素采样和卷积参数空间密度估计的算法3C-FBI。

Result: 在真实医学数据、合成数据和不同分辨率/异常值条件下，3C-FBI实现了最高精度（Jaccard指数0.896）和实时性能（40.3 fps），优于RCD等方法。

Conclusion: 3C-FBI在精度、速度和鲁棒性上的优异表现使其适用于医疗影像、机器人和工业检测等挑战性场景。

Abstract: This paper addresses the fundamental computer vision challenge of robust
circle detection and fitting in degraded imaging conditions. We present
Combinatorial Convolution-based Circle Fitting for Blurry Images (3C-FBI), an
algorithm that bridges the gap between circle detection and precise parametric
fitting by combining (1) efficient combinatorial edge pixel (edgel) sampling
and (2) convolution-based density estimation in parameter space.
  We evaluate 3C-FBI across three experimental frameworks: (1) real-world
medical data from Parkinson's disease assessments (144 frames from 36 videos),
(2) controlled synthetic data following established circle-fitting benchmarks,
and (3) systematic analysis across varying spatial resolutions and outlier
contamination levels. Results show that 3C-FBI achieves state-of-the-art
accuracy (Jaccard index 0.896) while maintaining real-time performance (40.3
fps), significantly outperforming classical methods like RCD (6.8 fps) on a
standard CPU (i7-10875H). It maintains near-perfect accuracy (Jaccard almost
1.0) at high resolutions (480x480) and reliable performance (Jaccard higher
than 0.95) down to 160x160 with up to 20% outliers.
  In extensive synthetic testing, 3C-FBI achieves a mean Jaccard Index of 0.989
across contamination levels, comparable to modern methods like Qi et al. (2024,
0.991), and surpassing RHT (0.964). This combination of accuracy, speed, and
robustness makes 3C-FBI ideal for medical imaging, robotics, and industrial
inspection under challenging conditions.

</details>


### [144] [COLIBRI Fuzzy Model: Color Linguistic-Based Representation and Interpretation](https://arxiv.org/abs/2507.11488)
*Pakizar Shamoi,Nuray Toganas,Muragul Muratbekova,Elnara Kadyrgali,Adilet Yerkin,Ayan Igali,Malika Ziyada,Ayana Adilova,Aron Karatayev,Yerdauit Torekhan*

Main category: cs.CV

TL;DR: 本文提出了一种基于人类感知的模糊颜色模型COLIBRI，通过模糊集和逻辑构建颜色分类框架，实验验证其与传统颜色模型相比更符合人类感知。


<details>
  <summary>Details</summary>
Motivation: 计算机难以模仿人类颜色感知，现有颜色模型（如RGB、HSV、LAB）与人类视觉感知存在差距，需要一种更贴近人类感知的颜色表示方法。

Method: 采用三阶段实验方法：初步实验确定可区分的颜色刺激，大规模人类分类调查（1000+受试者），提取模糊分区并生成隶属函数，模型支持基于反馈的适应性调整。

Result: COLIBRI模型在人类感知对齐上优于传统颜色模型，实验样本规模（n=2496）为同类研究中最大。

Conclusion: COLIBRI模型在设计与AI等领域具有重要应用价值，为颜色表示提供了更符合人类感知的解决方案。

Abstract: Colors are omnipresent in today's world and play a vital role in how humans
perceive and interact with their surroundings. However, it is challenging for
computers to imitate human color perception. This paper introduces the Human
Perception-Based Fuzzy Color Model, COLIBRI (Color Linguistic-Based
Representation and Interpretation), designed to bridge the gap between
computational color representations and human visual perception. The proposed
model uses fuzzy sets and logic to create a framework for color categorization.
Using a three-phase experimental approach, the study first identifies
distinguishable color stimuli for hue, saturation, and intensity through
preliminary experiments, followed by a large-scale human categorization survey
involving more than 1000 human subjects. The resulting data are used to extract
fuzzy partitions and generate membership functions that reflect real-world
perceptual uncertainty. The model incorporates a mechanism for adaptation that
allows refinement based on feedback and contextual changes. Comparative
evaluations demonstrate the model's alignment with human perception compared to
traditional color models, such as RGB, HSV, and LAB. To the best of our
knowledge, no previous research has documented the construction of a model for
color attribute specification based on a sample of this size or a comparable
sample of the human population (n = 2496). Our findings are significant for
fields such as design, artificial intelligence, marketing, and human-computer
interaction, where perceptually relevant color representation is critical.

</details>


### [145] [CATVis: Context-Aware Thought Visualization](https://arxiv.org/abs/2507.11522)
*Tariq Mehmood,Hamza Ahmad,Muhammad Haroon Shakeel,Murtaza Taj*

Main category: cs.CV

TL;DR: 提出了一种新的5阶段框架，用于从EEG信号解码视觉表示，通过跨模态对齐和重新排序实现上下文感知的EEG到图像生成，实验结果表明其性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: EEG信号复杂且噪声多，解码视觉表示具有挑战性，因此需要一种新方法来提高解码质量和图像生成效果。

Method: 5阶段框架：EEG编码器、跨模态对齐、标题重新排序、加权插值和图像生成。

Result: 在分类准确率、生成准确率和图像质量上显著优于现有方法。

Conclusion: 该方法在EEG到图像生成任务中表现出色，具有更高的语义对齐和图像质量。

Abstract: EEG-based brain-computer interfaces (BCIs) have shown promise in various
applications, such as motor imagery and cognitive state monitoring. However,
decoding visual representations from EEG signals remains a significant
challenge due to their complex and noisy nature. We thus propose a novel
5-stage framework for decoding visual representations from EEG signals: (1) an
EEG encoder for concept classification, (2) cross-modal alignment of EEG and
text embeddings in CLIP feature space, (3) caption refinement via re-ranking,
(4) weighted interpolation of concept and caption embeddings for richer
semantics, and (5) image generation using a pre-trained Stable Diffusion model.
We enable context-aware EEG-to-image generation through cross-modal alignment
and re-ranking. Experimental results demonstrate that our method generates
high-quality images aligned with visual stimuli, outperforming SOTA approaches
by 13.43% in Classification Accuracy, 15.21% in Generation Accuracy and
reducing Fr\'echet Inception Distance by 36.61%, indicating superior semantic
alignment and image quality.

</details>


### [146] [CharaConsist: Fine-Grained Consistent Character Generation](https://arxiv.org/abs/2507.11533)
*Mengyu Wang,Henghui Ding,Jianing Peng,Yao Zhao,Yunpeng Chen,Yunchao Wei*

Main category: cs.CV

TL;DR: CharaConsist提出了一种基于点跟踪注意力和自适应令牌合并的方法，解决了文本到图像生成中身份和背景一致性的问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法在生成一致内容时无法保持背景细节和身份一致性，限制了实际应用。

Method: 采用点跟踪注意力、自适应令牌合并以及前景与背景解耦控制。

Result: CharaConsist实现了细粒度的一致性，支持连续或离散场景中的角色生成，并适用于DiT模型。

Conclusion: CharaConsist扩展了文本到图像生成的应用范围，提供了高质量的视觉输出。

Abstract: In text-to-image generation, producing a series of consistent contents that
preserve the same identity is highly valuable for real-world applications.
Although a few works have explored training-free methods to enhance the
consistency of generated subjects, we observe that they suffer from the
following problems. First, they fail to maintain consistent background details,
which limits their applicability. Furthermore, when the foreground character
undergoes large motion variations, inconsistencies in identity and clothing
details become evident. To address these problems, we propose CharaConsist,
which employs point-tracking attention and adaptive token merge along with
decoupled control of the foreground and background. CharaConsist enables
fine-grained consistency for both foreground and background, supporting the
generation of one character in continuous shots within a fixed scene or in
discrete shots across different scenes. Moreover, CharaConsist is the first
consistent generation method tailored for text-to-image DiT model. Its ability
to maintain fine-grained consistency, combined with the larger capacity of
latest base model, enables it to produce high-quality visual outputs,
broadening its applicability to a wider range of real-world scenarios. The
source code has been released at https://github.com/Murray-Wang/CharaConsist

</details>


### [147] [Streaming 4D Visual Geometry Transformer](https://arxiv.org/abs/2507.11539)
*Dong Zhuo,Wenzhao Zheng,Jiahe Guo,Yuqi Wu,Jie Zhou,Jiwen Lu*

Main category: cs.CV

TL;DR: 论文提出了一种流式4D视觉几何变换器，用于实时感知和重建视频中的4D时空几何，结合自回归模型和因果注意力机制，实现高效在线处理。


<details>
  <summary>Details</summary>
Motivation: 从视频中感知和重建4D时空几何是一个基础但具有挑战性的任务，需要支持实时和交互式应用。

Method: 采用因果变换器架构，利用时间因果注意力和历史键值缓存作为隐式记忆，实现高效的流式长期4D重建。通过知识蒸馏从双向视觉几何变换器（VGGT）中学习。

Result: 模型在在线场景中显著提高了推理速度，同时保持了竞争性能。

Conclusion: 该模型为可扩展和交互式的4D视觉系统铺平了道路，代码已开源。

Abstract: Perceiving and reconstructing 4D spatial-temporal geometry from videos is a
fundamental yet challenging computer vision task. To facilitate interactive and
real-time applications, we propose a streaming 4D visual geometry transformer
that shares a similar philosophy with autoregressive large language models. We
explore a simple and efficient design and employ a causal transformer
architecture to process the input sequence in an online manner. We use temporal
causal attention and cache the historical keys and values as implicit memory to
enable efficient streaming long-term 4D reconstruction. This design can handle
real-time 4D reconstruction by incrementally integrating historical information
while maintaining high-quality spatial consistency. For efficient training, we
propose to distill knowledge from the dense bidirectional visual geometry
grounded transformer (VGGT) to our causal model. For inference, our model
supports the migration of optimized efficient attention operator (e.g.,
FlashAttention) from the field of large language models. Extensive experiments
on various 4D geometry perception benchmarks demonstrate that our model
increases the inference speed in online scenarios while maintaining competitive
performance, paving the way for scalable and interactive 4D vision systems.
Code is available at: https://github.com/wzzheng/StreamVGGT.

</details>


### [148] [Towards Depth Foundation Model: Recent Trends in Vision-Based Depth Estimation](https://arxiv.org/abs/2507.11540)
*Zhen Xu,Hongyu Zhou,Sida Peng,Haotong Lin,Haoyu Guo,Jiahao Shao,Peishan Yang,Qinglin Yang,Sheng Miao,Xingyi He,Yifan Wang,Yue Wang,Ruizhen Hu,Yiyi Liao,Xiaowei Zhou,Hujun Bao*

Main category: cs.CV

TL;DR: 该论文综述了深度估计在3D计算机视觉中的重要性，探讨了传统硬件方法的局限性及基于视觉方法的挑战，并提出了深度基础模型的潜力。


<details>
  <summary>Details</summary>
Motivation: 深度估计在3D重建、机器人、自动驾驶等领域至关重要，但传统硬件方法成本高且受限，基于视觉的方法又面临泛化性和稳定性问题。

Method: 论文综述了单目、立体、多视图和单目视频设置下的深度学习架构和范式，并探讨了大规模数据集的作用。

Result: 通过分析关键架构和训练策略，论文提出了构建鲁棒深度基础模型的路径。

Conclusion: 深度基础模型有望解决现有挑战，未来研究应关注其泛化能力和应用潜力。

Abstract: Depth estimation is a fundamental task in 3D computer vision, crucial for
applications such as 3D reconstruction, free-viewpoint rendering, robotics,
autonomous driving, and AR/VR technologies. Traditional methods relying on
hardware sensors like LiDAR are often limited by high costs, low resolution,
and environmental sensitivity, limiting their applicability in real-world
scenarios. Recent advances in vision-based methods offer a promising
alternative, yet they face challenges in generalization and stability due to
either the low-capacity model architectures or the reliance on domain-specific
and small-scale datasets. The emergence of scaling laws and foundation models
in other domains has inspired the development of "depth foundation models":
deep neural networks trained on large datasets with strong zero-shot
generalization capabilities. This paper surveys the evolution of deep learning
architectures and paradigms for depth estimation across the monocular, stereo,
multi-view, and monocular video settings. We explore the potential of these
models to address existing challenges and provide a comprehensive overview of
large-scale datasets that can facilitate their development. By identifying key
architectures and training strategies, we aim to highlight the path towards
robust depth foundation models, offering insights into their future research
and applications.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [149] [Tool-to-Tool Matching Analysis Based Difference Score Computation Methods for Semiconductor Manufacturing](https://arxiv.org/abs/2507.10564)
*Sameera Bharadwaja H.,Siddhrath Jandial,Shashank S. Agashe,Rajesh Kumar Reddy Moore,Youngkwan Kim*

Main category: cs.LG

TL;DR: 论文提出了一种新的工具间匹配（TTTM）分析方法，解决了传统方法在异构设备环境中的局限性，并通过实验验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 传统TTTM方法依赖静态配置或黄金参考数据，难以在商业生产线中应用，且不适用于异构设备环境。

Method: 提出基于数据方差和模式数量的新分析流程，包括单变量和多变量方法。

Result: 最佳单变量方法与方差和模式数的相关系数分别>0.95和>0.5；多变量方法与单变量方法的相关系数>0.75。

Conclusion: 新方法在异构设备环境中表现优异，且对超参数敏感度低。

Abstract: We consider the problem of tool-to-tool matching (TTTM), also called, chamber
matching in the context of a semiconductor manufacturing equipment. Traditional
TTTM approaches utilize static configuration data or depend on a golden
reference which are difficult to obtain in a commercial manufacturing line.
Further, existing methods do not extend very well to a heterogeneous setting,
where equipment are of different make-and-model, sourced from different
equipment vendors. We propose novel TTTM analysis pipelines to overcome these
issues. We hypothesize that a mismatched equipment would have higher variance
and/or higher number of modes in the data. Our best univariate method achieves
a correlation coefficient >0.95 and >0.5 with the variance and number of modes,
respectively showing that the proposed methods are effective. Also, the best
multivariate method achieves a correlation coefficient >0.75 with the
top-performing univariate methods, showing its effectiveness. Finally, we
analyze the sensitivity of the multivariate algorithms to the algorithm
hyper-parameters.

</details>


### [150] [Enhancing Cross Entropy with a Linearly Adaptive Loss Function for Optimized Classification Performance](https://arxiv.org/abs/2507.10574)
*Jae Wan Shim*

Main category: cs.LG

TL;DR: 提出了一种基于信息论的线性自适应交叉熵损失函数，通过增加一个依赖于真实类预测概率的项，优化分类任务。


<details>
  <summary>Details</summary>
Motivation: 改进标准交叉熵损失函数，以提升分类任务的优化效果。

Method: 提出线性自适应交叉熵损失函数，增加一个与真实类预测概率相关的项。

Result: 在CIFAR-100数据集上，使用ResNet模型验证，分类准确率优于标准交叉熵损失函数。

Conclusion: 该方法简单高效，为损失函数设计提供了新的研究方向。

Abstract: We propose the Linearly Adaptive Cross Entropy Loss function. This is a novel
measure derived from the information theory. In comparison to the standard
cross entropy loss function, the proposed one has an additional term that
depends on the predicted probability of the true class. This feature serves to
enhance the optimization process in classification tasks involving one-hot
encoded class labels. The proposed one has been evaluated on a ResNet-based
model using the CIFAR-100 dataset. Preliminary results show that the proposed
one consistently outperforms the standard cross entropy loss function in terms
of classification accuracy. Moreover, the proposed one maintains simplicity,
achieving practically the same efficiency to the traditional cross entropy
loss. These findings suggest that our approach could broaden the scope for
future research into loss function design.

</details>


### [151] [An Adaptive Volatility-based Learning Rate Scheduler](https://arxiv.org/abs/2507.10575)
*Kieran Chai Kai Ren*

Main category: cs.LG

TL;DR: VolSched是一种新型自适应学习率调度器，通过动态调整学习率提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 预定义和自适应学习率调度器可能导致次优泛化，需要更有效的调度方法。

Method: 利用长期和短期准确率波动比动态调整学习率，帮助模型逃离平台期或稳定训练。

Result: 在CIFAR-100数据集上，VolSched显著提升ResNet-18和ResNet-34的准确率，并找到更平坦的极小值。

Conclusion: VolSched通过动态调整学习率，有效提升模型泛化性能。

Abstract: Effective learning rate (LR) scheduling is crucial for training deep neural
networks. However, popular pre-defined and adaptive schedulers can still lead
to suboptimal generalization. This paper introduces VolSched, a novel adaptive
LR scheduler inspired by the concept of volatility in stochastic processes like
Geometric Brownian Motion to dynamically adjust the learning rate. By
calculating the ratio between long-term and short-term accuracy volatility,
VolSched increases the LR to escape plateaus and decreases it to stabilize
training, allowing the model to explore the loss landscape more effectively. We
evaluate VolSched on the CIFAR-100 dataset against a strong baseline using a
standard augmentation pipeline. When paired with ResNet-18 and ResNet-34, our
scheduler delivers consistent performance gains, improving top-1 accuracy by
1.4 and 1.3 percentage points respectively. Analysis of the loss curves reveals
that VolSched promotes a longer exploration phase. A quantitative analysis of
the Hessian shows that VolSched finds a final solution that is 38% flatter than
the next-best baseline, allowing the model to obtain wider minima and hence
better generalization performance.

</details>


### [152] [Universal Approximation Theorem for a Single-Layer Transformer](https://arxiv.org/abs/2507.10581)
*Esmail Gumaan*

Main category: cs.LG

TL;DR: 论文探讨了深度学习和Transformer的数学基础，提出了一个通用的逼近定理，证明单层Transformer可以逼近任意连续序列到序列的映射。


<details>
  <summary>Details</summary>
Motivation: 尽管深度学习和Transformer在许多领域取得了成功，但其理论理解仍然有限。本文旨在填补这一理论空白。

Method: 回顾了线性代数、概率和优化等关键概念，详细分析了多头自注意力机制和反向传播算法，并提出了一个通用的逼近定理。

Result: 证明了单层Transformer可以逼近任意连续序列到序列的映射，并提供了完整的证明和案例研究。

Conclusion: 研究结果推动了Transformer模型的理论理解，并缩小了理论与实践之间的差距。

Abstract: Deep learning employs multi-layer neural networks trained via the
backpropagation algorithm. This approach has achieved success across many
domains and relies on adaptive gradient methods such as the Adam optimizer.
Sequence modeling evolved from recurrent neural networks to attention-based
models, culminating in the Transformer architecture. Transformers have achieved
state-of-the-art performance in natural language processing (for example, BERT
and GPT-3) and have been applied in computer vision and computational biology.
However, theoretical understanding of these models remains limited. In this
paper, we examine the mathematical foundations of deep learning and
Transformers and present a novel theoretical result. We review key concepts
from linear algebra, probability, and optimization that underpin deep learning,
and we analyze the multi-head self-attention mechanism and the backpropagation
algorithm in detail. Our main contribution is a universal approximation theorem
for Transformers: we prove that a single-layer Transformer, comprising one
self-attention layer followed by a position-wise feed-forward network with ReLU
activation, can approximate any continuous sequence-to-sequence mapping on a
compact domain to arbitrary precision. We provide a formal statement and a
complete proof. Finally, we present case studies that demonstrate the practical
implications of this result. Our findings advance the theoretical understanding
of Transformer models and help bridge the gap between theory and practice.

</details>


### [153] [Step-wise Policy for Rare-tool Knowledge (SPaRK): Offline RL that Drives Diverse Tool Use in LLMs](https://arxiv.org/abs/2507.11371)
*Gabriel Bo,Koa Chang,Justin Gu*

Main category: cs.LG

TL;DR: SPaRK是一个强化学习框架，通过双目标奖励系统优化答案质量和工具多样性，训练语言模型探索多样化工具使用模式。


<details>
  <summary>Details</summary>
Motivation: 传统的高温采样方法限制了工具使用的多样性，SPaRK旨在通过强化学习鼓励模型探索更多工具使用模式。

Method: 采用离线PPO训练Llama-3.1 8B模型，引入双目标奖励系统，结合GPT-4o评分和稀有工具优先策略。

Result: 在14个MMLU-Pro类别中表现优异，工具选择熵显著高于基线方法，表明工具多样性可提升推理能力。

Conclusion: SPaRK通过显式工具多样性增强推理能力，同时保持准确性。

Abstract: We present Step-wise Policy for Rare-tool Knowledge (SPaRK), a novel
reinforcement learning framework that teaches large language models to explore
diverse tool usage patterns beyond conventional high-temperature sampling.
Building on recent advances in step-wise reinforcement learning, we introduce a
dual-objective reward system that simultaneously optimizes for answer quality
and tool diversity, training a Llama-3.1 8B model through offline PPO on
synthetically generated trajectories from the MMLU-Pro dataset. Our approach
uniquely employs a rarity-first exploitation strategy where a GPT-4o judge
scores candidate actions across eight distinct tools plus chain-of-thought
reasoning, with the policy favoring less-frequently used but still viable tools
to encourage systematic exploration. Empirical results demonstrate that SPaRK
achieves competitive performance across 14 MMLU-Pro categories while exhibiting
significantly higher entropy in tool selection compared to both baseline and
supervised fine-tuning approaches, suggesting that algorithmic exploration
through explicit tool diversity can enhance reasoning capabilities without
sacrificing accuracy.

</details>


### [154] [MH-FSF: A Unified Framework for Overcoming Benchmarking and Reproducibility Limitations in Feature Selection Evaluation](https://arxiv.org/abs/2507.10591)
*Vanderson Rocha,Diego Kreutz,Gabriel Canto,Hendrio Bragança,Eduardo Feitosa*

Main category: cs.LG

TL;DR: 论文提出了MH-FSF框架，用于解决特征选择研究中缺乏基准测试和依赖专有数据集的问题，通过模块化平台实现17种方法的复现和评估。


<details>
  <summary>Details</summary>
Motivation: 当前特征选择研究存在基准测试不足和依赖专有数据集的问题，限制了可复现性和性能。

Method: 开发了MH-FSF框架，包含17种特征选择方法（11种经典，6种领域特定），并在10个公开Android恶意软件数据集上评估。

Result: 结果显示性能在不同数据集上存在差异，强调数据预处理和选择标准的重要性。

Conclusion: MH-FSF框架为特征选择方法比较提供了统一平台，推动了方法学一致性和严谨性，并为Android恶意软件检测提供了新研究方向。

Abstract: Feature selection is vital for building effective predictive models, as it
reduces dimensionality and emphasizes key features. However, current research
often suffers from limited benchmarking and reliance on proprietary datasets.
This severely hinders reproducibility and can negatively impact overall
performance. To address these limitations, we introduce the MH-FSF framework, a
comprehensive, modular, and extensible platform designed to facilitate the
reproduction and implementation of feature selection methods. Developed through
collaborative research, MH-FSF provides implementations of 17 methods (11
classical, 6 domain-specific) and enables systematic evaluation on 10 publicly
available Android malware datasets. Our results reveal performance variations
across both balanced and imbalanced datasets, highlighting the critical need
for data preprocessing and selection criteria that account for these
asymmetries. We demonstrate the importance of a unified platform for comparing
diverse feature selection techniques, fostering methodological consistency and
rigor. By providing this framework, we aim to significantly broaden the
existing literature and pave the way for new research directions in feature
selection, particularly within the context of Android malware detection.

</details>


### [155] [Extension OL-MDISF: Online Learning from Mix-Typed, Drifted, and Incomplete Streaming Features](https://arxiv.org/abs/2507.10594)
*Shengda Zhuo,Di Wu,Yi He,Shuqiang Huang,Xindong Wu*

Main category: cs.LG

TL;DR: 论文提出OL-MDISF方法，解决在线学习中混合类型特征、数据漂移和标签缺失问题，并通过实验验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 在线学习中，混合类型特征、数据漂移和标签缺失是三大挑战，传统方法难以应对。

Method: OL-MDISF通过潜在copula表示处理混合特征，利用集成熵和潜在失配检测漂移，并进行结构感知伪标记。

Result: 在14个真实数据集上的实验表明，OL-MDISF在漂移场景下表现优异，包括CER趋势、消融研究和敏感性分析。

Conclusion: OL-MDISF为复杂、弱监督的流数据在线学习提供了可复现的基准。

Abstract: Online learning, where feature spaces can change over time, offers a flexible
learning paradigm that has attracted considerable attention. However, it still
faces three significant challenges. First, the heterogeneity of real-world data
streams with mixed feature types presents challenges for traditional parametric
modeling. Second, data stream distributions can shift over time, causing an
abrupt and substantial decline in model performance. Third, it is often
infeasible to label every data instance due to time and cost constraints. To
address these issues, we proposed OL-MDISF (Online Learning from Mix-typed,
Drifted, and Incomplete Streaming Features), which constructs a latent
copula-based representation for heterogeneous features, detects drifts via
ensemble entropy and latent mismatch, and performs structure-aware
pseudo-labeling.
  This companion paper serves as a standalone technical reference to OL-MDISF.
It provides a contextual discussion of related work in mixed-type modeling,
drift adaptation, and weak supervision, as well as a comprehensive set of
experiments across 14 real-world datasets under two types of drift scenarios.
These include CER trends, ablation studies, sensitivity analyses, and temporal
ensemble dynamics. We hope this document offers a reproducible benchmark for
online learning on complex, weakly supervised streaming data.

</details>


### [156] [Divide-Then-Rule: A Cluster-Driven Hierarchical Interpolator for Attribute-Missing Graphs](https://arxiv.org/abs/2507.10595)
*Yaowen Hu,Wenxuan Tu,Yue Liu,Miaomiao Li,Wenpeng Lu,Zhigang Luo,Xinwang Liu,Ping Chen*

Main category: cs.LG

TL;DR: 提出了一种名为DTRGC的新方法，用于属性缺失图的深度图聚类，通过分层填补和聚类信息修正，显著提升了聚类性能。


<details>
  <summary>Details</summary>
Motivation: 属性缺失图的深度图聚类在实际应用中至关重要，但现有填补方法未能充分利用节点邻域信息，导致结果不可靠。

Method: DTRGC方法包括动态聚类感知特征传播（DCFP）、分层邻域感知填补（HNAI）和跳级表示增强（HRE），通过分层填补和聚类修正提升性能。

Result: 在六个广泛使用的图数据集上，DTRGC显著提升了属性缺失图下多种深度图聚类方法的性能。

Conclusion: DTRGC通过分层填补和聚类信息修正，有效解决了属性缺失图聚类的挑战，为实际应用提供了可靠解决方案。

Abstract: Deep graph clustering (DGC) for attribute-missing graphs is an unsupervised
task aimed at partitioning nodes with incomplete attributes into distinct
clusters. Addressing this challenging issue is vital for practical
applications. However, research in this area remains underexplored. Existing
imputation methods for attribute-missing graphs often fail to account for the
varying amounts of information available across node neighborhoods, leading to
unreliable results, especially for nodes with insufficient known neighborhood.
To address this issue, we propose a novel method named Divide-Then-Rule Graph
Completion (DTRGC). This method first addresses nodes with sufficient known
neighborhood information and treats the imputed results as new knowledge to
iteratively impute more challenging nodes, while leveraging clustering
information to correct imputation errors. Specifically, Dynamic Cluster-Aware
Feature Propagation (DCFP) initializes missing node attributes by adjusting
propagation weights based on the clustering structure. Subsequently,
Hierarchical Neighborhood-aware Imputation (HNAI) categorizes attribute-missing
nodes into three groups based on the completeness of their neighborhood
attributes. The imputation is performed hierarchically, prioritizing the groups
with nodes that have the most available neighborhood information. The cluster
structure is then used to refine the imputation and correct potential errors.
Finally, Hop-wise Representation Enhancement (HRE) integrates information
across multiple hops, thereby enriching the expressiveness of node
representations. Experimental results on six widely used graph datasets show
that DTRGC significantly improves the clustering performance of various DGC
methods under attribute-missing graphs.

</details>


### [157] [RedOne: Revealing Domain-specific LLM Post-Training in Social Networking Services](https://arxiv.org/abs/2507.10605)
*Fei Zhao,Chonggang Lu,Yue Wang,Zheyong Xie,Ziyan Liu,Haofu Qian,JianZhao Huang,Fangcheng Shi,Zijie Meng,Hongcheng Guo,Mingqian He,Xinze Lyu,Yiming Lu,Ziyang Xiang,Zheyu Ye,Chengqiang Lu,Zhe Xu,Yi Wu,Yao Hu,Yan Gao,Jun Fan,Xiaolong Jiang,Weiting Liu,Boyang Wang,Shaosheng Cao*

Main category: cs.LG

TL;DR: RedOne是一种针对社交网络服务（SNS）的领域特定大语言模型（LLM），通过三阶段训练策略显著提升多任务性能，并在实际应用中表现出色。


<details>
  <summary>Details</summary>
Motivation: 现有研究集中于孤立任务，难以适应多样化的真实场景，且数据扩展效益递减。RedOne旨在打破单任务基线的性能瓶颈，为SNS提供全面解决方案。

Method: 采用三阶段训练策略：持续预训练、监督微调和偏好优化，使用大规模真实数据集。

Result: 在8项主要SNS任务中平均提升14.02%，双语评测提升7.56%，有害内容检测曝光率降低11.23%，帖子搜索点击率提高14.95%。

Conclusion: RedOne作为SNS领域特定LLM，表现出强大的泛化能力和实际应用潜力。

Abstract: As a primary medium for modern information dissemination, social networking
services (SNS) have experienced rapid growth, which has proposed significant
challenges for platform content management and interaction quality improvement.
Recently, the development of large language models (LLMs) has offered potential
solutions but existing studies focus on isolated tasks, which not only
encounter diminishing benefit from the data scaling within individual scenarios
but also fail to flexibly adapt to diverse real-world context. To address these
challenges, we introduce RedOne, a domain-specific LLM designed to break the
performance bottleneck of single-task baselines and establish a comprehensive
foundation for the SNS. RedOne was developed through a three-stage training
strategy consisting of continue pretraining, supervised fine-tuning, and
preference optimization, using a large-scale real-world dataset. Through
extensive experiments, RedOne maintains strong general capabilities, and
achieves an average improvement up to 14.02% across 8 major SNS tasks and 7.56%
in SNS bilingual evaluation benchmark, compared with base models. Furthermore,
through online testing, RedOne reduced the exposure rate in harmful content
detection by 11.23% and improved the click page rate in post-view search by
14.95% compared with single-tasks finetuned baseline models. These results
establish RedOne as a robust domain-specific LLM for SNS, demonstrating
excellent generalization across various tasks and promising applicability in
real-world scenarios.

</details>


### [158] [DALI-PD: Diffusion-based Synthetic Layout Heatmap Generation for ML in Physical Design](https://arxiv.org/abs/2507.10606)
*Bing-Yue Wu,Vidya A. Chhabria*

Main category: cs.LG

TL;DR: DALI-PD是一个可扩展的框架，通过扩散模型快速生成合成布局热图，解决了机器学习在物理设计任务中数据不足的问题。


<details>
  <summary>Details</summary>
Motivation: 机器学习在物理设计任务中表现优异，但高质量、大规模训练数据的缺乏限制了模型的泛化能力。现有数据集生成成本高且更新缓慢。

Method: DALI-PD利用扩散模型快速生成多样化的布局热图，包括功率、IR压降、拥塞、宏布局和单元密度图。

Result: 生成了包含20,000多种布局配置的数据集，这些热图与真实布局相似，提升了IR压降和拥塞预测等下游任务的准确性。

Conclusion: DALI-PD为物理设计研究提供了一种高效的数据生成方法，显著提升了机器学习的应用潜力。

Abstract: Machine learning (ML) has demonstrated significant promise in various
physical design (PD) tasks. However, model generalizability remains limited by
the availability of high-quality, large-scale training datasets. Creating such
datasets is often computationally expensive and constrained by IP. While very
few public datasets are available, they are typically static, slow to generate,
and require frequent updates. To address these limitations, we present DALI-PD,
a scalable framework for generating synthetic layout heatmaps to accelerate ML
in PD research. DALI-PD uses a diffusion model to generate diverse layout
heatmaps via fast inference in seconds. The heatmaps include power, IR drop,
congestion, macro placement, and cell density maps. Using DALI-PD, we created a
dataset comprising over 20,000 layout configurations with varying macro counts
and placements. These heatmaps closely resemble real layouts and improve ML
accuracy on downstream ML tasks such as IR drop or congestion prediction.

</details>


### [159] [A Feed-Forward Artificial Intelligence Pipeline for Sustainable Desalination under Climate Uncertainties: UAE Insights](https://arxiv.org/abs/2507.10609)
*Obumneme Nwafor,Chioma Nwafor,Amro Zakaria,Nkechi Nwankwo*

Main category: cs.LG

TL;DR: 阿联酋依赖海水淡化满足90%以上饮用水需求，但该过程能耗高且面临气候不确定性挑战。研究提出两阶段预测模型，准确率达98%，并开发交互式仪表盘支持决策。


<details>
  <summary>Details</summary>
Motivation: 海水淡化是阿联酋主要水源，但能耗高且受气候因素（如气溶胶光学厚度）影响效率，亟需解决方案。

Method: 提出两阶段预测模型：第一阶段预测气溶胶光学厚度，第二阶段预测淡化效率损失；开发基于预测的规则控制逻辑。

Result: 模型准确率达98%，揭示了系统退化的关键因素，并开发了交互式决策支持仪表盘。

Conclusion: 研究为气候适应性规划提供了有效工具，提升了海水淡化系统的可持续性。

Abstract: The United Arab Emirates (UAE) relies heavily on seawater desalination to
meet over 90% of its drinking water needs. Desalination processes are highly
energy intensive and account for approximately 15% of the UAE's electricity
consumption, contributing to over 22% of the country's energy-related CO2
emissions. Moreover, these processes face significant sustainability challenges
in the face of climate uncertainties such as rising seawater temperatures,
salinity, and aerosol optical depth (AOD). AOD greatly affects the operational
and economic performance of solar-powered desalination systems through
photovoltaic soiling, membrane fouling, and water turbidity cycles.
  This study proposes a novel pipelined two-stage predictive modelling
architecture: the first stage forecasts AOD using satellite-derived time series
and meteorological data; the second stage uses the predicted AOD and other
meteorological factors to predict desalination performance efficiency losses.
The framework achieved 98% accuracy, and SHAP (SHapley Additive exPlanations)
was used to reveal key drivers of system degradation. Furthermore, this study
proposes a dust-aware rule-based control logic for desalination systems based
on predicted values of AOD and solar efficiency. This control logic is used to
adjust the desalination plant feed water pressure, adapt maintenance
scheduling, and regulate energy source switching.
  To enhance the practical utility of the research findings, the predictive
models and rule-based controls were packaged into an interactive dashboard for
scenario and predictive analytics. This provides a management decision-support
system for climate-adaptive planning.

</details>


### [160] [FedGSCA: Medical Federated Learning with Global Sample Selector and Client Adaptive Adjuster under Label Noise](https://arxiv.org/abs/2507.10611)
*Mengwen Ye,Yingzi Huangfu,Shujian Gao,Wei Ren,Weifan Liu,Zekuan Yu*

Main category: cs.LG

TL;DR: FedGSCA是一种针对医疗联邦学习中标签噪声问题的新框架，通过全局样本选择器和客户端自适应调整机制，显著提升了模型在噪声环境下的鲁棒性和稳定性。


<details>
  <summary>Details</summary>
Motivation: 医疗联邦学习中的数据标签噪声和异质性会导致训练不稳定和性能下降，现有方法难以应对这些问题。

Method: FedGSCA结合全局样本选择器和客户端自适应调整机制，动态处理噪声标签和类别不平衡。

Result: 在真实和合成医疗数据集上，FedGSCA在极端和异质噪声场景下表现优于现有方法。

Conclusion: FedGSCA在医疗联邦学习中展现出强大的噪声处理能力和模型稳定性，适用于实际应用。

Abstract: Federated Learning (FL) emerged as a solution for collaborative medical image
classification while preserving data privacy. However, label noise, which
arises from inter-institutional data variability, can cause training
instability and degrade model performance. Existing FL methods struggle with
noise heterogeneity and the imbalance in medical data. Motivated by these
challenges, we propose FedGSCA, a novel framework for enhancing robustness in
noisy medical FL. FedGSCA introduces a Global Sample Selector that aggregates
noise knowledge from all clients, effectively addressing noise heterogeneity
and improving global model stability. Furthermore, we develop a Client Adaptive
Adjustment (CAA) mechanism that combines adaptive threshold pseudo-label
generation and Robust Credal Labeling Loss. CAA dynamically adjusts to class
distributions, ensuring the inclusion of minority samples and carefully
managing noisy labels by considering multiple plausible labels. This dual
approach mitigates the impact of noisy data and prevents overfitting during
local training, which improves the generalizability of the model. We evaluate
FedGSCA on one real-world colon slides dataset and two synthetic medical
datasets under various noise conditions, including symmetric, asymmetric,
extreme, and heterogeneous types. The results show that FedGSCA outperforms the
state-of-the-art methods, excelling in extreme and heterogeneous noise
scenarios. Moreover, FedGSCA demonstrates significant advantages in improving
model stability and handling complex noise, making it well-suited for
real-world medical federated learning scenarios.

</details>


### [161] [Sub-Scaling Laws: On the Role of Data Density and Training Strategies in LLMs](https://arxiv.org/abs/2507.10613)
*Zhengyu Chen,Siqi Wang,Teng Xiao,Yudong Wang,Shiqi Chen,Xunliang Cai,Junxian He,Jingang Wang*

Main category: cs.LG

TL;DR: 论文重新审视了自然语言处理中的传统扩展定律，发现数据质量和训练策略对模型性能的影响，提出了适用于次优扩展的新定律。


<details>
  <summary>Details</summary>
Motivation: 传统扩展定律在大语言模型中表现不佳，出现性能提升减缓的现象（次优扩展），研究旨在探索其背后的原因。

Method: 通过对400多个模型进行实证分析，研究数据密度和资源分配对性能的影响。

Result: 发现高数据密度导致收益递减，资源分配不当是次优扩展的关键因素。

Conclusion: 提出新的次优扩展定律，强调数据质量和多样性的重要性。

Abstract: Traditional scaling laws in natural language processing suggest that
increasing model size and training data enhances performance. However, recent
studies reveal deviations, particularly in large language models, where
performance improvements decelerate, which is a phenomenon known as
sub-scaling. This paper revisits these scaling laws by examining the impact of
data quality and training strategies on model performance. Through extensive
empirical analysis of over 400 models, we identify high data density and
non-optimal resource allocation as key factors contributing to sub-scaling.
High data density leads to diminishing returns due to redundant information,
while optimal resource allocation is crucial for sustained performance
improvements. We propose a sub-optimal scaling law that better predicts
performance in sub-scaling regimes, highlighting the importance of data quality
and diversity.

</details>


### [162] [Fine-tuning Large Language Model for Automated Algorithm Design](https://arxiv.org/abs/2507.10614)
*Fei Liu,Rui Zhang,Xi Lin,Zhichao Lu,Qingfu Zhang*

Main category: cs.LG

TL;DR: 论文探讨了为算法设计定制大语言模型（LLMs）的必要性，并提出了多样性感知排名采样策略（DAR）和直接偏好优化方法，实验表明微调后的LLMs在算法设计任务中表现更优且具有泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖通用编码任务的LLMs，但未验证是否需要专门为算法设计定制的LLMs及其泛化能力。

Method: 引入DAR采样策略平衡数据多样性和质量，结合直接偏好优化微调LLMs。

Result: 微调后的LLMs在多个算法设计任务中表现优于通用LLMs，并展现出泛化能力。

Conclusion: 任务特定微调对LLMs在算法设计中具有重要价值，为未来研究开辟了新方向。

Abstract: The integration of large language models (LLMs) into automated algorithm
design has shown promising potential. A prevalent approach embeds LLMs within
search routines to iteratively generate and refine candidate algorithms.
However, most existing methods rely on off-the-shelf LLMs trained for general
coding tasks,leaving a key question open: Do we need LLMs specifically tailored
for algorithm design? If so, how can such LLMs be effectively obtained and how
well can they generalize across different algorithm design tasks? In this
paper, we take a first step toward answering these questions by exploring
fine-tuning of LLMs for algorithm design. We introduce a Diversity-Aware Rank
based (DAR) sampling strategy to balance training data diversity and quality,
then we leverage direct preference optimization to efficiently align LLM
outputs with task objectives. Our experiments, conducted on
Llama-3.2-1B-Instruct and Llama- 3.1-8B-Instruct, span three distinct algorithm
design tasks. Results suggest that finetuned LLMs can significantly outperform
their off-the-shelf counterparts with the smaller Llama-3.2-1B-Instruct and
match the larger Llama-3.1-8B-Instruct on the admissible set problem. Moreover,
we observe promising generalization: LLMs finetuned on specific algorithm
design tasks also improve performance on related tasks with varying settings.
These findings highlight the value of task-specific adaptation for LLMs in
algorithm design and open new avenues for future research.

</details>


### [163] [Scalpel vs. Hammer: GRPO Amplifies Existing Capabilities, SFT Replaces Them](https://arxiv.org/abs/2507.10616)
*Neel Rajani,Aryo Pradipta Gema,Seraphina Goldfarb-Tarrant,Ivan Titov*

Main category: cs.LG

TL;DR: 比较了强化学习（RL）和监督微调（SFT）在数学问题上的表现，发现RL在数学领域有微小提升，而SFT在知识密集型任务上表现更差。


<details>
  <summary>Details</summary>
Motivation: 理解RL和SFT在大型语言模型（LLM）后训练中的动态差异。

Method: 在同一模型和相似超参数下，比较RL和SFT在数学问题上的表现，并分析模型参数变化。

Result: RL在数学领域有轻微提升，SFT在知识密集型任务上表现更差；SFT对模型参数的修改更显著。

Conclusion: RL可能增强现有能力，而SFT可能替换旧技能；冻结部分模型的效果不明确。

Abstract: Training large language models (LLMs) for reasoning via maths and code
datasets has become a major new focus in LLM post-training. Two particularly
popular approaches are reinforcement learning (RL) and supervised fine-tuning
(SFT), but their training dynamics are poorly understood. We present a
comparative analysis of RL and SFT on the same maths problems with the same
model and similar hyperparameters. We find that RL yields minor in-domain gains
on maths and slight degradation on knowledge-intensive benchmarks like MMLU,
while both trends are more pronounced in SFT. We also analyse model parameters
across checkpoints, observing that both algorithms modify query and key weights
the most. Meanwhile, SFT exhibits greater updates and also affects mid-layer
MLPs more, leading us to hypothesise that this may have caused the
out-of-domain degradation. We therefore investigate whether freezing parts of
the model during training can mitigate the reduced performance on
knowledge-intensive benchmarks. However, our results are inconclusive, with
benefits on GPQA:Diamond and degradation on other benchmarks. Taken together,
our observations provide a preliminary indication for why RL amplifies existing
capabilities, while SFT replaces old skills with new ones.

</details>


### [164] [Compute Requirements for Algorithmic Innovation in Frontier AI Models](https://arxiv.org/abs/2507.10618)
*Peter Barnett*

Main category: cs.LG

TL;DR: 本文研究了大型语言模型预训练中的算法创新对计算资源的需求，发现即使严格的计算限制也难以显著减缓AI算法的进步。


<details>
  <summary>Details</summary>
Motivation: 探讨算法创新在预训练大型语言模型中对计算资源的需求及其对AI进步的影响。

Method: 通过分析36种预训练算法创新（如Llama 3和DeepSeek-V3中的创新），估计其开发所需的总FLOP和硬件FLOP/s。

Result: 研究发现，资源密集型的创新需求每年翻倍，但即使严格的计算限制（如GPT-2的训练计算量或8个H100 GPU的限制）仍能支持半数创新。

Conclusion: 计算限制本身不太可能显著减缓AI算法的进步。

Abstract: Algorithmic innovation in the pretraining of large language models has driven
a massive reduction in the total compute required to reach a given level of
capability. In this paper we empirically investigate the compute requirements
for developing algorithmic innovations. We catalog 36 pre-training algorithmic
innovations used in Llama 3 and DeepSeek-V3. For each innovation we estimate
both the total FLOP used in development and the FLOP/s of the hardware
utilized. Innovations using significant resources double in their requirements
each year. We then use this dataset to investigate the effect of compute caps
on innovation. Our analysis suggests that compute caps alone are unlikely to
dramatically slow AI algorithmic progress. Even stringent compute caps -- such
as capping total operations to the compute used to train GPT-2 or capping
hardware capacity to 8 H100 GPUs -- could still have allowed for half of the
cataloged innovations.

</details>


### [165] [Meta-Reinforcement Learning for Fast and Data-Efficient Spectrum Allocation in Dynamic Wireless Networks](https://arxiv.org/abs/2507.10619)
*Oluwaseyi Giwa,Tobi Awodunmila,Muhammad Ahmed Mohsin,Ahsan Bilal,Muhammad Ali Jamshed*

Main category: cs.LG

TL;DR: 论文提出了一种基于元学习的框架，用于5G/6G网络中动态频谱分配，解决了传统深度强化学习样本复杂度高和安全风险的问题。


<details>
  <summary>Details</summary>
Motivation: 传统深度强化学习在动态频谱分配中存在样本复杂度高和探索安全风险的问题，需要一种更高效和安全的方法。

Method: 提出了三种元学习架构（MAML、RNN和注意力增强RNN），并在模拟动态IAB环境中与非元学习的PPO基线进行比较。

Result: 注意力元学习代理峰值吞吐量达48 Mbps，优于PPO基线的10 Mbps，同时减少了50%以上的SINR和延迟违规，公平指数为0.7。

Conclusion: 元学习是复杂无线系统中智能控制的高效且安全的选择。

Abstract: The dynamic allocation of spectrum in 5G / 6G networks is critical to
efficient resource utilization. However, applying traditional deep
reinforcement learning (DRL) is often infeasible due to its immense sample
complexity and the safety risks associated with unguided exploration, which can
cause severe network interference. To address these challenges, we propose a
meta-learning framework that enables agents to learn a robust initial policy
and rapidly adapt to new wireless scenarios with minimal data. We implement
three meta-learning architectures, model-agnostic meta-learning (MAML),
recurrent neural network (RNN), and an attention-enhanced RNN, and evaluate
them against a non-meta-learning DRL algorithm, proximal policy optimization
(PPO) baseline, in a simulated dynamic integrated access/backhaul (IAB)
environment. Our results show a clear performance gap. The attention-based
meta-learning agent reaches a peak mean network throughput of 48 Mbps, while
the PPO baseline decreased drastically to 10 Mbps. Furthermore, our method
reduces SINR and latency violations by more than 50% compared to PPO. It also
shows quick adaptation, with a fairness index 0.7, showing better resource
allocation. This work proves that meta-learning is a very effective and safer
option for intelligent control in complex wireless systems.

</details>


### [166] [LLMs Meet Cross-Modal Time Series Analytics: Overview and Directions](https://arxiv.org/abs/2507.10620)
*Chenxi Liu,Hao Miao,Cheng Long,Yan Zhao,Ziyue Li,Panos Kalnis*

Main category: cs.LG

TL;DR: 本文概述了基于大语言模型（LLMs）的跨模态时间序列分析，分类了现有方法，并讨论了应用与挑战。


<details>
  <summary>Details</summary>
Motivation: 解决时间序列与文本数据之间的跨模态差距，扩展LLMs在时间序列分析中的实际应用。

Method: 提出分类法，将现有方法分为转换、对齐和融合三类，并讨论其在下游任务中的应用。

Result: 总结了当前进展、方法论及未来研究方向。

Conclusion: 旨在平衡效果与效率，推动LLMs在跨模态时间序列分析中的实际应用。

Abstract: Large Language Models (LLMs) have emerged as a promising paradigm for time
series analytics, leveraging their massive parameters and the shared sequential
nature of textual and time series data. However, a cross-modality gap exists
between time series and textual data, as LLMs are pre-trained on textual
corpora and are not inherently optimized for time series. In this tutorial, we
provide an up-to-date overview of LLM-based cross-modal time series analytics.
We introduce a taxonomy that classifies existing approaches into three groups
based on cross-modal modeling strategies, e.g., conversion, alignment, and
fusion, and then discuss their applications across a range of downstream tasks.
In addition, we summarize several open challenges. This tutorial aims to expand
the practical application of LLMs in solving real-world problems in cross-modal
time series analytics while balancing effectiveness and efficiency.
Participants will gain a thorough understanding of current advancements,
methodologies, and future research directions in cross-modal time series
analytics.

</details>


### [167] [Flows and Diffusions on the Neural Manifold](https://arxiv.org/abs/2507.10623)
*Daniel Saragih,Deyu Cao,Tejas Balaji*

Main category: cs.LG

TL;DR: 该论文将扩散和基于流的生成模型扩展到权重空间学习，通过梯度流匹配统一轨迹推断技术，优化权重生成并提升下游任务性能。


<details>
  <summary>Details</summary>
Motivation: 扩散和流生成模型在图像、视频和自然语言领域已取得成功，但尚未广泛应用于权重空间学习。本文旨在填补这一空白，利用优化动态的结构先验提升权重生成效果。

Method: 通过梯度流匹配框架统一轨迹推断技术，结合奖励微调、自编码器表示、任务上下文条件和Kaiming均匀分布等方法优化权重生成。

Result: 实验表明，该方法在生成分布内权重、改进下游训练初始化和微调性能方面优于基线，并在安全关键系统中表现出色。

Conclusion: 该研究为权重空间学习提供了新框架，展示了梯度流匹配的潜力，并在实际应用中验证了其有效性。

Abstract: Diffusion and flow-based generative models have achieved remarkable success
in domains such as image synthesis, video generation, and natural language
modeling. In this work, we extend these advances to weight space learning by
leveraging recent techniques to incorporate structural priors derived from
optimization dynamics. Central to our approach is modeling the trajectory
induced by gradient descent as a trajectory inference problem. We unify several
trajectory inference techniques under the framework of gradient flow matching,
providing a theoretical framework for treating optimization paths as inductive
bias. We further explore architectural and algorithmic choices, including
reward fine-tuning by adjoint matching, the use of autoencoders for latent
weight representation, conditioning on task-specific context data, and adopting
informative source distributions such as Kaiming uniform. Experiments
demonstrate that our method matches or surpasses baselines in generating
in-distribution weights, improves initialization for downstream training, and
supports fine-tuning to enhance performance. Finally, we illustrate a practical
application in safety-critical systems: detecting harmful covariate shifts,
where our method outperforms the closest comparable baseline.

</details>


### [168] [Player-Team Heterogeneous Interaction Graph Transformer for Soccer Outcome Prediction](https://arxiv.org/abs/2507.10626)
*Lintao Wang,Shiwen Xu,Michael Horton,Joachim Gudmundsson,Zhiyong Wang*

Main category: cs.LG

TL;DR: HIGFormer是一种基于图增强Transformer的深度学习模型，用于预测足球比赛结果，通过多级交互框架捕捉球员和团队动态，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 传统方法依赖特征工程且忽略球员和团队间的异质交互，而深度学习模型能直接学习有效表示，但现有方法未充分建模交互动态。

Method: HIGFormer包含三个模块：Player Interaction Network（编码球员交互）、Team Interaction Network（建模团队历史关系）、Match Comparison Transformer（联合分析预测结果）。

Result: 在WyScout数据集上，HIGFormer显著提升了预测准确性，并为球员表现评估和团队策略分析提供了新视角。

Conclusion: HIGFormer通过多级交互建模，为足球比赛预测和球员评估提供了高效解决方案。

Abstract: Predicting soccer match outcomes is a challenging task due to the inherently
unpredictable nature of the game and the numerous dynamic factors influencing
results. While it conventionally relies on meticulous feature engineering, deep
learning techniques have recently shown a great promise in learning effective
player and team representations directly for soccer outcome prediction.
However, existing methods often overlook the heterogeneous nature of
interactions among players and teams, which is crucial for accurately modeling
match dynamics. To address this gap, we propose HIGFormer (Heterogeneous
Interaction Graph Transformer), a novel graph-augmented transformer-based deep
learning model for soccer outcome prediction. HIGFormer introduces a
multi-level interaction framework that captures both fine-grained player
dynamics and high-level team interactions. Specifically, it comprises (1) a
Player Interaction Network, which encodes player performance through
heterogeneous interaction graphs, combining local graph convolutions with a
global graph-augmented transformer; (2) a Team Interaction Network, which
constructs interaction graphs from a team-to-team perspective to model
historical match relationships; and (3) a Match Comparison Transformer, which
jointly analyzes both team and player-level information to predict match
outcomes. Extensive experiments on the WyScout Open Access Dataset, a
large-scale real-world soccer dataset, demonstrate that HIGFormer significantly
outperforms existing methods in prediction accuracy. Furthermore, we provide
valuable insights into leveraging our model for player performance evaluation,
offering a new perspective on talent scouting and team strategy analysis.

</details>


### [169] [GHPO: Adaptive Guidance for Stable and Efficient LLM Reinforcement Learning](https://arxiv.org/abs/2507.10628)
*Ziru Liu,Cheng Gong,Xinyu Fu,Yaofang Liu,Ran Chen,Shoubo Hu,Suiyun Zhang,Rui Liu,Qingfu Zhang,Dandan Tu*

Main category: cs.LG

TL;DR: 论文提出了一种名为GHPO的新强化学习框架，通过动态调整任务难度和结合模仿学习与探索学习，解决了现有RL方法在训练不稳定和效率低下的问题。


<details>
  <summary>Details</summary>
Motivation: 现有RL方法在复杂推理任务中因任务难度与模型能力不匹配导致训练不稳定和效率低下，尤其是对小规模LLMs。

Method: 提出GHPO框架，通过自适应提示细化动态调整任务难度，结合模仿学习和探索学习。

Result: 在六个数学基准测试中平均性能提升约5%，显著优于现有RL和课程学习方法。

Conclusion: GHPO显著提升了训练稳定性和推理性能，为开发高效推理模型提供了可扩展的解决方案。

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) has recently emerged as
a powerful paradigm for facilitating the self-improvement of large language
models (LLMs), particularly in the domain of complex reasoning tasks. However,
prevailing on-policy RL methods often contend with significant training
instability and inefficiency. This is primarily due to a capacity-difficulty
mismatch, where the complexity of training data frequently outpaces the model's
current capabilities, leading to critically sparse reward signals and stalled
learning progress. This challenge is particularly acute for smaller, more
resource-efficient LLMs. To overcome this, we introduce the Guided Hybrid
Policy Optimization (GHPO), a novel difficulty-aware reinforcement learning
framework. GHPO dynamically calibrates task difficulty by employing adaptive
prompt refinement to provide targeted guidance. This unique approach adaptively
balances direct imitation learning for problems currently beyond the model's
reach with exploration-based reinforcement learning for more manageable tasks,
effectively creating a smooth and optimized learning curriculum. Extensive
experiments demonstrate that GHPO achieves an average performance gain of
approximately 5% across six challenging mathematics benchmarks, consistently
outperforming strong on-policy reinforcement learning and curriculum learning
baselines. Further analysis confirms that our framework significantly enhances
both training stability and final reasoning performance, thus offering a
scalable and efficient solution for developing powerful and robust reasoning
models.

</details>


### [170] [Scalable Unsupervised Segmentation via Random Fourier Feature-based Gaussian Process](https://arxiv.org/abs/2507.10632)
*Issei Saito,Masatoshi Nagano,Tomoaki Nakamura,Daichi Mochihashi,Koki Mimura*

Main category: cs.LG

TL;DR: 提出RFF-GP-HSMM，一种快速无监督时间序列分割方法，通过随机傅里叶特征（RFF）降低计算成本。


<details>
  <summary>Details</summary>
Motivation: 传统GP-HSMM因需计算大规模核矩阵逆矩阵，计算成本高，难以扩展。

Method: 用RFF近似高斯过程，转化为线性回归问题，避免核矩阵逆运算。

Result: 在CMU动作捕捉数据集上，性能接近传统方法，速度提升约278倍。

Conclusion: RFF-GP-HSMM在保持性能的同时显著提升计算效率。

Abstract: In this paper, we propose RFF-GP-HSMM, a fast unsupervised time-series
segmentation method that incorporates random Fourier features (RFF) to address
the high computational cost of the Gaussian process hidden semi-Markov model
(GP-HSMM). GP-HSMM models time-series data using Gaussian processes, requiring
inversion of an N times N kernel matrix during training, where N is the number
of data points. As the scale of the data increases, matrix inversion incurs a
significant computational cost. To address this, the proposed method
approximates the Gaussian process with linear regression using RFF, preserving
expressive power while eliminating the need for inversion of the kernel matrix.
Experiments on the Carnegie Mellon University (CMU) motion-capture dataset
demonstrate that the proposed method achieves segmentation performance
comparable to that of conventional methods, with approximately 278 times faster
segmentation on time-series data comprising 39,200 frames.

</details>


### [171] [GeoHopNet: Hopfield-Augmented Sparse Spatial Attention for Dynamic UAV Site Location Problem](https://arxiv.org/abs/2507.10636)
*Jianing Zhi,Xinghua Li,Zidong Chen*

Main category: cs.LG

TL;DR: GeoHopNet是一种用于动态无人机站点选择的Hopfield增强稀疏空间注意力网络，通过距离偏置多头注意力、K近邻稀疏注意力、Hopfield外部记忆模块和记忆正则化策略，显著提高了计算效率和解决方案质量。


<details>
  <summary>Details</summary>
Motivation: 解决传统深度强化学习方法在处理大规模城市级无人机站点选择问题时计算复杂度高的问题。

Method: 提出GeoHopNet，包含距离偏置多头注意力、K近邻稀疏注意力、Hopfield外部记忆模块和记忆正则化策略。

Result: 在1000节点实例中，GeoHopNet在0.1秒内找到高质量解决方案（0.22%最优性差距），优于传统方法。

Conclusion: GeoHopNet扩展了可解决问题规模的边界，显著提升了计算效率和解决方案质量。

Abstract: The rapid development of urban low-altitude unmanned aerial vehicle (UAV)
economy poses new challenges for dynamic site selection of UAV landing points
and supply stations. Traditional deep reinforcement learning methods face
computational complexity bottlenecks, particularly with standard attention
mechanisms, when handling large-scale urban-level location problems. This paper
proposes GeoHopNet, a Hopfield-augmented sparse spatial attention network
specifically designed for dynamic UAV site location problems. Our approach
introduces four core innovations: (1) distance-biased multi-head attention
mechanism that explicitly encodes spatial geometric information; (2) K-nearest
neighbor sparse attention that reduces computational complexity from $O(N^2)$
to $O(NK)$; (3) a modern Hopfield external memory module; and (4) a memory
regularization strategy. Experimental results demonstrate that GeoHopNet
extends the boundary of solvable problem sizes. For large-scale instances with
1,000 nodes, where standard attention models become prohibitively slow (over 3
seconds per instance) and traditional solvers fail, GeoHopNet finds
high-quality solutions (0.22\% optimality gap) in under 0.1 seconds. Compared
to the state-of-the-art ADNet baseline on 100-node instances, our method
improves solution quality by 22.2\% and is 1.8$\times$ faster.

</details>


### [172] [A Simple Baseline for Stable and Plastic Neural Networks](https://arxiv.org/abs/2507.10637)
*É. Künzel,A. Jaziri,V. Ramesh*

Main category: cs.LG

TL;DR: RDBP是一种简单、低开销的持续学习方法，结合了ReLUDown和Decreasing Backpropagation机制，在Continual ImageNet基准测试中表现优异。


<details>
  <summary>Details</summary>
Motivation: 解决持续学习中模型在适应新任务时遗忘旧知识的问题，平衡可塑性和稳定性。

Method: 结合ReLUDown（轻量级激活修改）和Decreasing Backpropagation（梯度调度方案）。

Result: 在Continual ImageNet基准测试中表现优于或匹配现有方法，同时降低计算成本。

Conclusion: RDBP为持续学习提供了实用解决方案，并可作为未来方法的基准。

Abstract: Continual learning in computer vision requires that models adapt to a
continuous stream of tasks without forgetting prior knowledge, yet existing
approaches often tip the balance heavily toward either plasticity or stability.
We introduce RDBP, a simple, low-overhead baseline that unites two
complementary mechanisms: ReLUDown, a lightweight activation modification that
preserves feature sensitivity while preventing neuron dormancy, and Decreasing
Backpropagation, a biologically inspired gradient-scheduling scheme that
progressively shields early layers from catastrophic updates. Evaluated on the
Continual ImageNet benchmark, RDBP matches or exceeds the plasticity and
stability of state-of-the-art methods while reducing computational cost. RDBP
thus provides both a practical solution for real-world continual learning and a
clear benchmark against which future continual learning strategies can be
measured.

</details>


### [173] [ZClassifier: Temperature Tuning and Manifold Approximation via KL Divergence on Logit Space](https://arxiv.org/abs/2507.10638)
*Shim Soon Yong*

Main category: cs.LG

TL;DR: ZClassifier使用高斯分布替代传统确定性logits，通过最小化KL散度统一不确定性校准和潜在控制，提升了分类器的鲁棒性、校准性和潜在分离能力。


<details>
  <summary>Details</summary>
Motivation: 传统softmax分类器在不确定性校准和潜在控制方面存在不足，ZClassifier旨在通过概率方法解决这些问题。

Method: ZClassifier用对角高斯分布logits替代确定性logits，最小化预测高斯分布与单位各向同性高斯之间的KL散度。

Result: 在CIFAR-10和CIFAR-100上，ZClassifier在鲁棒性、校准性和潜在分离方面优于softmax分类器，并适用于分类器引导生成。

Conclusion: ZClassifier通过概率框架统一了不确定性校准和潜在控制，为分类和生成任务提供了更自然的解释和性能提升。

Abstract: We introduce a novel classification framework, ZClassifier, that replaces
conventional deterministic logits with diagonal Gaussian-distributed logits.
Our method simultaneously addresses temperature scaling and manifold
approximation by minimizing the Kullback-Leibler (KL) divergence between the
predicted Gaussian distributions and a unit isotropic Gaussian. This unifies
uncertainty calibration and latent control in a principled probabilistic
manner, enabling a natural interpretation of class confidence and geometric
consistency. Experiments on CIFAR-10 and CIFAR-100 show that ZClassifier
improves over softmax classifiers in robustness, calibration, and latent
separation. We also demonstrate its effectiveness for classifier-guided
generation by interpreting logits as Gaussian semantic potentials.

</details>


### [174] [First-of-its-kind AI model for bioacoustic detection using a lightweight associative memory Hopfield neural network](https://arxiv.org/abs/2507.10642)
*Andrew Gascoyne,Wendy Lomas*

Main category: cs.LG

TL;DR: 提出了一种基于Hopfield神经网络的轻量级AI模型，用于快速、可持续且透明的生物声学分析，解决了现有模型的数据限制、环境影响和硬件需求问题。


<details>
  <summary>Details</summary>
Motivation: 解决生物声学分析中大量被动声学监测数据的处理问题，同时应对现有AI模型在训练数据、环境影响和硬件需求上的不足。

Method: 使用透明且可解释的Hopfield神经网络，通过关联记忆存储信号并检测相似信号，仅需一个代表性信号进行快速训练。

Result: 模型训练仅需3毫秒，分类10384个蝙蝠录音仅需5.4秒，内存占用144.09 MB，准确率达86%，与专家手动识别一致。

Conclusion: 该模型为快速、轻量、可持续、透明且准确的生物声学分析提供了创新解决方案，具有广泛的应用潜力。

Abstract: A growing issue within conservation bioacoustics is the task of analysing the
vast amount of data generated from the use of passive acoustic monitoring
devices. In this paper, we present an alternative AI model which has the
potential to help alleviate this problem. Our model formulation addresses the
key issues encountered when using current AI models for bioacoustic analysis,
namely the: limited training data available; environmental impact, particularly
in energy consumption and carbon footprint of training and implementing these
models; and associated hardware requirements. The model developed in this work
uses associative memory via a transparent, explainable Hopfield neural network
to store signals and detect similar signals which can then be used to classify
species. Training is rapid ($3$\,ms), as only one representative signal is
required for each target sound within a dataset. The model is fast, taking only
$5.4$\,s to pre-process and classify all $10384$ publicly available bat
recordings, on a standard Apple MacBook Air. The model is also lightweight with
a small memory footprint of $144.09$\,MB of RAM usage. Hence, the low
computational demands make the model ideal for use on a variety of standard
personal devices with potential for deployment in the field via edge-processing
devices. It is also competitively accurate, with up to $86\%$ precision on the
dataset used to evaluate the model. In fact, we could not find a single case of
disagreement between model and manual identification via expert field guides.
Although a dataset of bat echolocation calls was chosen to demo this
first-of-its-kind AI model, trained on only two representative calls, the model
is not species specific. In conclusion, we propose an equitable AI model that
has the potential to be a game changer for fast, lightweight, sustainable,
transparent, explainable and accurate bioacoustic analysis.

</details>


### [175] [A Group Theoretic Analysis of the Symmetries Underlying Base Addition and Their Learnability by Neural Networks](https://arxiv.org/abs/2507.10678)
*Cutter Dawes,Simon Segert,Kamesh Krishnamurthy,Jonathan D. Cohen*

Main category: cs.LG

TL;DR: 论文研究了神经网络如何通过对称性实现激进泛化，以基数加法为例，分析了不同的进位函数对学习效率的影响。


<details>
  <summary>Details</summary>
Motivation: 设计能高效学习支持激进泛化的系统是神经网络的重大挑战，尤其是发现和实现对称性函数的能力。

Method: 通过群论分析基数加法，提出多种进位函数，并训练神经网络比较其学习效率和结构。

Result: 发现简单神经网络在合适的输入格式和进位函数下可实现激进泛化，学习速度与进位函数结构密切相关。

Conclusion: 研究对认知科学和机器学习有重要意义，揭示了对称性学习中的归纳偏差。

Abstract: A major challenge in the use of neural networks both for modeling human
cognitive function and for artificial intelligence is the design of systems
with the capacity to efficiently learn functions that support radical
generalization. At the roots of this is the capacity to discover and implement
symmetry functions. In this paper, we investigate a paradigmatic example of
radical generalization through the use of symmetry: base addition. We present a
group theoretic analysis of base addition, a fundamental and defining
characteristic of which is the carry function -- the transfer of the remainder,
when a sum exceeds the base modulus, to the next significant place. Our
analysis exposes a range of alternative carry functions for a given base, and
we introduce quantitative measures to characterize these. We then exploit
differences in carry functions to probe the inductive biases of neural networks
in symmetry learning, by training neural networks to carry out base addition
using different carries, and comparing efficacy and rate of learning as a
function of their structure. We find that even simple neural networks can
achieve radical generalization with the right input format and carry function,
and that learning speed is closely correlated with carry function structure. We
then discuss the relevance this has for cognitive science and machine learning.

</details>


### [176] [A Simple Approximate Bayesian Inference Neural Surrogate for Stochastic Petri Net Models](https://arxiv.org/abs/2507.10714)
*Bright Kwaku Manu,Trevor Reckell,Beckett Sterner,Petar Jevtic*

Main category: cs.LG

TL;DR: 该论文提出了一种基于神经网络的框架，用于解决随机Petri网（SPN）中参数估计的挑战，特别是在转移率依赖于外部协变量且无法获得显式似然的情况下。


<details>
  <summary>Details</summary>
Motivation: 随机Petri网在建模离散事件动态时面临参数估计困难，尤其是在协变量依赖和部分观测条件下。

Method: 采用轻量级1D卷积残差网络，通过端到端训练学习从噪声、部分观测的轨迹中预测协变量依赖的速率函数系数。

Result: 在20%事件缺失的合成SPN中，该方法以RMSE=0.108恢复速率函数系数，且速度显著快于传统贝叶斯方法。

Conclusion: 数据驱动的无似然代理模型能够在复杂、部分观测的离散事件系统中实现准确、鲁棒且实时的参数恢复。

Abstract: Stochastic Petri Nets (SPNs) are an increasingly popular tool of choice for
modeling discrete-event dynamics in areas such as epidemiology and systems
biology, yet their parameter estimation remains challenging in general and in
particular when transition rates depend on external covariates and explicit
likelihoods are unavailable. We introduce a neural-surrogate
(neural-network--based approximation of the posterior distribution) framework
that predicts the coefficients of known covariate-dependent rate functions
directly from noisy, partially observed token trajectories. Our model employs a
lightweight 1D Convolutional Residual Network trained end-to-end on
Gillespie-simulated SPN realizations, learning to invert system dynamics under
realistic conditions of event dropout. During inference, Monte Carlo dropout
provides calibrated uncertainty bounds together with point estimates. On
synthetic SPNs with 20% missing events, our surrogate recovers rate-function
coefficients with an RMSE = 0.108 and substantially runs faster than
traditional Bayesian approaches. These results demonstrate that data-driven,
likelihood-free surrogates can enable accurate, robust, and real-time parameter
recovery in complex, partially observed discrete-event systems.

</details>


### [177] [Distributionally Robust Optimization with Adversarial Data Contamination](https://arxiv.org/abs/2507.10718)
*Shuyao Li,Ilias Diakonikolas,Jelena Diakonikolas*

Main category: cs.LG

TL;DR: 本文提出了一种结合数据污染和分布偏移双重挑战的鲁棒优化方法，通过Wasserstein-1 DRO框架和高效算法，实现了对真实DRO目标的估计误差为O(√ε)。


<details>
  <summary>Details</summary>
Motivation: 解决训练数据中的异常值和分布不确定性对DRO框架的影响，提出一种同时应对这两种挑战的方法。

Method: 采用Wasserstein-1 DRO目标优化广义线性模型，结合鲁棒统计方法，设计高效算法处理数据污染问题。

Result: 在数据污染和分布偏移的双重挑战下，方法实现了O(√ε)的估计误差，并提供了高效计算支持。

Conclusion: 本文首次为数据污染和分布偏移双重挑战下的学习问题提供了严格的理论保证和高效算法。

Abstract: Distributionally Robust Optimization (DRO) provides a framework for
decision-making under distributional uncertainty, yet its effectiveness can be
compromised by outliers in the training data. This paper introduces a
principled approach to simultaneously address both challenges. We focus on
optimizing Wasserstein-1 DRO objectives for generalized linear models with
convex Lipschitz loss functions, where an $\epsilon$-fraction of the training
data is adversarially corrupted. Our primary contribution lies in a novel
modeling framework that integrates robustness against training data
contamination with robustness against distributional shifts, alongside an
efficient algorithm inspired by robust statistics to solve the resulting
optimization problem. We prove that our method achieves an estimation error of
$O(\sqrt{\epsilon})$ for the true DRO objective value using only the
contaminated data under the bounded covariance assumption. This work
establishes the first rigorous guarantees, supported by efficient computation,
for learning under the dual challenges of data contamination and distributional
shifts.

</details>


### [178] [Ground-Compose-Reinforce: Tasking Reinforcement Learning Agents through Formal Language](https://arxiv.org/abs/2507.10741)
*Andrew C. Li,Toryn Q. Klassen,Andrew Wang,Parand A. Alamdari,Sheila A. McIlraith*

Main category: cs.LG

TL;DR: 提出了一种名为Ground-Compose-Reinforce的神经符号框架，用于从数据中学习语言基础，并通过语言直接指导RL代理的行为，避免了手动设计领域特定元素。


<details>
  <summary>Details</summary>
Motivation: 解决语言在复杂感知和动作中的基础问题，避免传统方法中手动设计或大规模数据标注的需求。

Method: 使用神经符号框架，结合数据驱动学习和组合形式语言语义，实现高效的语言基础和行为生成。

Result: 在图像网格世界和MuJoCo机器人领域的实验中，该方法在有限数据下成功将语言指令映射为行为，而端到端数据驱动方法失败。

Conclusion: 该框架通过数据驱动和组合语言语义的结合，实现了高效的语言基础和行为生成，具有广泛的应用潜力。

Abstract: Grounding language in complex perception (e.g. pixels) and action is a key
challenge when building situated agents that can interact with humans via
language. In past works, this is often solved via manual design of the language
grounding or by curating massive datasets relating language to elements of the
environment. We propose Ground-Compose-Reinforce, a neurosymbolic framework for
grounding formal language from data, and eliciting behaviours by directly
tasking RL agents through this language. By virtue of data-driven learning, our
framework avoids the manual design of domain-specific elements like reward
functions or symbol detectors. By virtue of compositional formal language
semantics, our framework achieves data-efficient grounding and generalization
to arbitrary language compositions. Experiments on an image-based gridworld and
a MuJoCo robotics domain show that our approach reliably maps formal language
instructions to behaviours with limited data while end-to-end, data-driven
approaches fail.

</details>


### [179] [A Benchmarking Framework for AI models in Automotive Aerodynamics](https://arxiv.org/abs/2507.10747)
*Kaustubh Tangsali,Rishikesh Ranade,Mohammad Amin Nabian,Alexey Kamenev,Peter Sharpe,Neil Ashton,Ram Cherukuri,Sanjay Choudhry*

Main category: cs.LG

TL;DR: 介绍了一个用于评估AI模型在汽车空气动力学预测中的准确性、性能、可扩展性和泛化能力的开源基准框架。


<details>
  <summary>Details</summary>
Motivation: 提升AI模型在汽车空气动力学领域的透明度和一致性评估，加速研究和创新。

Method: 在NVIDIA PhysicsNeMo-CFD框架中开发标准化方法，评估三种AI模型（DoMINO、X-MeshGraphNet、FIGConvNet）的表面和体积流场预测。

Result: 框架提供了可扩展的评估方法，支持更多模型和数据集的集成，促进物理一致性指标的开发。

Conclusion: 该框架有助于研究人员和行业专业人士选择和优化AI驱动的空气动力学建模方法，推动更高效、准确和可解释的解决方案发展。

Abstract: In this paper, we introduce a benchmarking framework within the open-source
NVIDIA PhysicsNeMo-CFD framework designed to systematically assess the
accuracy, performance, scalability, and generalization capabilities of AI
models for automotive aerodynamics predictions. The open extensible framework
enables incorporation of a diverse set of metrics relevant to the
Computer-Aided Engineering (CAE) community. By providing a standardized
methodology for comparing AI models, the framework enhances transparency and
consistency in performance assessment, with the overarching goal of improving
the understanding and development of these models to accelerate research and
innovation in the field. To demonstrate its utility, the framework includes
evaluation of both surface and volumetric flow field predictions on three AI
models: DoMINO, X-MeshGraphNet, and FIGConvNet using the DrivAerML dataset. It
also includes guidelines for integrating additional models and datasets, making
it extensible for physically consistent metrics. This benchmarking study aims
to enable researchers and industry professionals in selecting, refining, and
advancing AI-driven aerodynamic modeling approaches, ultimately fostering the
development of more efficient, accurate, and interpretable solutions in
automotive aerodynamics

</details>


### [180] [Spatial Reasoners for Continuous Variables in Any Domain](https://arxiv.org/abs/2507.10768)
*Bart Pogodzinski,Christopher Wewer,Bernt Schiele,Jan Eric Lenssen*

Main category: cs.LG

TL;DR: Spatial Reasoners是一个用于通过生成去噪模型进行空间推理的软件框架，旨在简化研究并支持多种模型和推理策略。


<details>
  <summary>Details</summary>
Motivation: 去噪生成模型在图像生成中表现优异，但在多连续变量推理中的应用研究缺乏基础设施支持。

Method: 提供易于使用的接口，支持变量映射、生成模型范式和推理策略的灵活控制。

Result: 框架开源，支持多种去噪模型和推理方法，促进相关研究。

Conclusion: Spatial Reasoners为生成推理研究提供了高效工具，推动领域发展。

Abstract: We present Spatial Reasoners, a software framework to perform spatial
reasoning over continuous variables with generative denoising models. Denoising
generative models have become the de-facto standard for image generation, due
to their effectiveness in sampling from complex, high-dimensional
distributions. Recently, they have started being explored in the context of
reasoning over multiple continuous variables. Providing infrastructure for
generative reasoning with such models requires a high effort, due to a wide
range of different denoising formulations, samplers, and inference strategies.
Our presented framework aims to facilitate research in this area, providing
easy-to-use interfaces to control variable mapping from arbitrary data domains,
generative model paradigms, and inference strategies. Spatial Reasoners are
openly available at https://spatialreasoners.github.io/

</details>


### [181] [A Generalizable Physics-Enhanced State Space Model for Long-Term Dynamics Forecasting in Complex Environments](https://arxiv.org/abs/2507.10792)
*Yuchen Wang,Hongjue Zhao,Haohong Lin,Enze Xu,Lifang He,Huajie Shao*

Main category: cs.LG

TL;DR: 提出了一种名为Phy-SSM的方法，将部分物理知识融入状态空间模型，用于复杂环境中的长期动态预测。


<details>
  <summary>Details</summary>
Motivation: 现有方法在复杂环境中长期预测表现不佳，而状态空间模型能有效捕捉序列数据的长期依赖关系，结合物理知识可提升泛化能力。

Method: 将部分已知的系统动力学分解为已知和未知状态矩阵，融入Phy-SSM单元，并引入物理状态正则化项以优化长期预测。

Result: 在车辆运动预测、无人机状态预测和COVID-19流行病学预测等实验中，Phy-SSM表现优于基线方法。

Conclusion: Phy-SSM通过结合物理知识显著提升了长期预测性能，适用于复杂环境中的动态预测任务。

Abstract: This work aims to address the problem of long-term dynamic forecasting in
complex environments where data are noisy and irregularly sampled. While recent
studies have introduced some methods to improve prediction performance, these
approaches still face a significant challenge in handling long-term
extrapolation tasks under such complex scenarios. To overcome this challenge,
we propose Phy-SSM, a generalizable method that integrates partial physics
knowledge into state space models (SSMs) for long-term dynamics forecasting in
complex environments. Our motivation is that SSMs can effectively capture
long-range dependencies in sequential data and model continuous dynamical
systems, while the incorporation of physics knowledge improves generalization
ability. The key challenge lies in how to seamlessly incorporate partially
known physics into SSMs. To achieve this, we decompose partially known system
dynamics into known and unknown state matrices, which are integrated into a
Phy-SSM unit. To further enhance long-term prediction performance, we introduce
a physics state regularization term to make the estimated latent states align
with system dynamics. Besides, we theoretically analyze the uniqueness of the
solutions for our method. Extensive experiments on three real-world
applications, including vehicle motion prediction, drone state prediction, and
COVID-19 epidemiology forecasting, demonstrate the superior performance of
Phy-SSM over the baselines in both long-term interpolation and extrapolation
tasks. The code is available at https://github.com/511205787/Phy_SSM-ICML2025.

</details>


### [182] [Multi-Armed Sampling Problem and the End of Exploration](https://arxiv.org/abs/2507.10797)
*Mohammad Pedramfar,Siamak Ravanbakhsh*

Main category: cs.LG

TL;DR: 本文提出了多臂采样框架，作为多臂老虎机优化问题的采样对应。通过定义合理的遗憾概念并建立下界，提出了一种简单算法达到最优遗憾界。理论结果表明采样不需要探索，并通过温度参数统一了多臂采样和多臂老虎机问题。


<details>
  <summary>Details</summary>
Motivation: 研究采样中的探索-利用权衡，为神经采样器等提供理论基础。

Method: 定义采样框架的遗憾概念，提出简单算法并证明其最优性，通过温度参数统一采样与优化问题。

Result: 理论证明采样无需探索，算法达到最优遗憾界，框架为熵正则化强化学习等提供新视角。

Conclusion: 多臂采样框架为采样研究奠定基础，揭示了探索需求与算法收敛性，对RLHF等领域有启示。

Abstract: This paper introduces the framework of multi-armed sampling, as the sampling
counterpart to the optimization problem of multi-arm bandits. Our primary
motivation is to rigorously examine the exploration-exploitation trade-off in
the context of sampling. We systematically define plausible notions of regret
for this framework and establish corresponding lower bounds. We then propose a
simple algorithm that achieves these optimal regret bounds. Our theoretical
results demonstrate that in contrast to optimization, sampling does not require
exploration. To further connect our findings with those of multi-armed bandits,
we define a continuous family of problems and associated regret measures that
smoothly interpolates and unifies multi-armed sampling and multi-armed bandit
problems using a temperature parameter. We believe the multi-armed sampling
framework, and our findings in this setting can have a foundational role in the
study of sampling including recent neural samplers, akin to the role of
multi-armed bandits in reinforcement learning. In particular, our work sheds
light on the need for exploration and the convergence properties of algorithm
for entropy-regularized reinforcement learning, fine-tuning of pretrained
models and reinforcement learning with human feedback (RLHF).

</details>


### [183] [Uncovering Causal Relation Shifts in Event Sequences under Out-of-Domain Interventions](https://arxiv.org/abs/2507.10809)
*Kazi Tasnim Zinat,Yun Zhou,Xiang Lyu,Yawei Wang,Zhicheng Liu,Panpan Xu*

Main category: cs.LG

TL;DR: 提出了一种新的因果框架，用于捕捉时间序列中事件间的因果关系，特别是在外域干预下的因果动态变化。


<details>
  <summary>Details</summary>
Motivation: 现有因果推理方法主要关注域内事件类型，忽略了外域干预的影响，而现实中这些干预可能显著改变因果动态。

Method: 设计了无偏的平均处理效应（ATE）估计器，并开发了基于Transformer的神经网络模型，以处理长时间依赖和局部模式，同时整合外域干预信息。

Result: 在模拟和真实数据集上的实验表明，该方法在外域干预增强的点过程中优于基线模型。

Conclusion: 该方法能有效捕捉外域干预下的因果动态变化，为因果推理提供了新思路。

Abstract: Inferring causal relationships between event pairs in a temporal sequence is
applicable in many domains such as healthcare, manufacturing, and
transportation. Most existing work on causal inference primarily focuses on
event types within the designated domain, without considering the impact of
exogenous out-of-domain interventions. In real-world settings, these
out-of-domain interventions can significantly alter causal dynamics. To address
this gap, we propose a new causal framework to define average treatment effect
(ATE), beyond independent and identically distributed (i.i.d.) data in classic
Rubin's causal framework, to capture the causal relation shift between events
of temporal process under out-of-domain intervention. We design an unbiased ATE
estimator, and devise a Transformer-based neural network model to handle both
long-range temporal dependencies and local patterns while integrating
out-of-domain intervention information into process modeling. Extensive
experiments on both simulated and real-world datasets demonstrate that our
method outperforms baselines in ATE estimation and goodness-of-fit under
out-of-domain-augmented point processes.

</details>


### [184] [Semantic Context for Tool Orchestration](https://arxiv.org/abs/2507.10820)
*Robert Müller*

Main category: cs.LG

TL;DR: 论文提出语义上下文（SC）是工具编排的基础，通过理论、实验和实际应用验证了其重要性。


<details>
  <summary>Details</summary>
Motivation: 研究如何利用语义上下文提升工具编排的鲁棒性和适应性。

Method: 结合上下文多臂老虎机理论（SC-LinUCB），并通过大语言模型进行实证验证，提出FiReAct流程。

Result: SC-LinUCB降低遗憾值，SC提升大语言模型的学习效率与适应性，FiReAct在10,000+工具基准测试中表现优异。

Conclusion: SC是构建高效、自适应和可扩展编排代理的关键。

Abstract: This paper demonstrates that Semantic Context (SC), leveraging descriptive
tool information, is a foundational component for robust tool orchestration.
Our contributions are threefold. First, we provide a theoretical foundation
using contextual bandits, introducing SC-LinUCB and proving it achieves lower
regret and adapts favourably in dynamic action spaces. Second, we provide
parallel empirical validation with Large Language Models, showing that SC is
critical for successful in-context learning in both static (efficient learning)
and non-stationary (robust adaptation) settings. Third, we propose the FiReAct
pipeline, and demonstrate on a benchmark with over 10,000 tools that SC-based
retrieval enables an LLM to effectively orchestrate over a large action space.
These findings provide a comprehensive guide to building more sample-efficient,
adaptive, and scalable orchestration agents.

</details>


### [185] [Offline Reinforcement Learning with Wasserstein Regularization via Optimal Transport Maps](https://arxiv.org/abs/2507.10843)
*Motoki Omura,Yusuke Mukuta,Kazuki Ota,Takayuki Osa,Tatsuya Harada*

Main category: cs.LG

TL;DR: 论文提出了一种基于Wasserstein距离的离线强化学习方法，避免了对抗训练，提高了稳定性。


<details>
  <summary>Details</summary>
Motivation: 离线强化学习中分布偏移是一个主要挑战，现有方法多基于密度比，而本文提出更稳健的Wasserstein距离方法。

Method: 使用输入凸神经网络（ICNNs）建模最优传输映射，计算Wasserstein距离，无需判别器。

Result: 在D4RL基准数据集上表现优于或与现有方法相当。

Conclusion: 提出的方法有效解决了分布偏移问题，且避免了对抗训练的不稳定性。

Abstract: Offline reinforcement learning (RL) aims to learn an optimal policy from a
static dataset, making it particularly valuable in scenarios where data
collection is costly, such as robotics. A major challenge in offline RL is
distributional shift, where the learned policy deviates from the dataset
distribution, potentially leading to unreliable out-of-distribution actions. To
mitigate this issue, regularization techniques have been employed. While many
existing methods utilize density ratio-based measures, such as the
$f$-divergence, for regularization, we propose an approach that utilizes the
Wasserstein distance, which is robust to out-of-distribution data and captures
the similarity between actions. Our method employs input-convex neural networks
(ICNNs) to model optimal transport maps, enabling the computation of the
Wasserstein distance in a discriminator-free manner, thereby avoiding
adversarial training and ensuring stable learning. Our approach demonstrates
comparable or superior performance to widely used existing methods on the D4RL
benchmark dataset. The code is available at
https://github.com/motokiomura/Q-DOT .

</details>


### [186] [From Small to Large: A Graph Convolutional Network Approach for Solving Assortment Optimization Problems](https://arxiv.org/abs/2507.10834)
*Guokai Li,Pin Gao,Stefanus Jasin,Zizhuo Wang*

Main category: cs.LG

TL;DR: 该论文提出了一种基于图卷积网络（GCN）的方法，用于解决受限品种优化问题，并在混合多项Logit选择模型下表现出高效性和优越性能。


<details>
  <summary>Details</summary>
Motivation: 品种优化是一个经典的NP难问题，传统方法难以高效处理大规模实例。论文旨在利用GCN的泛化能力，解决这一问题。

Method: 首先将品种问题表示为图结构，然后训练GCN学习最优品种模式，并提出两种基于GCN输出的推理策略。

Result: 实验表明，使用小规模实例训练的GCN可在大规模实例（如2000种产品）上实现90%以上的最优性，且速度优于现有启发式方法。

Conclusion: GCN方法在品种优化中表现出高效性和可扩展性，且适用于模型未知但数据可用的场景。

Abstract: Assortment optimization involves selecting a subset of substitutable products
(subject to certain constraints) to maximize the expected revenue. It is a
classic problem in revenue management and finds applications across various
industries. However, the problem is usually NP-hard due to its combinatorial
and non-linear nature. In this work, we explore how graph concolutional
networks (GCNs) can be leveraged to efficiently solve constrained assortment
optimization under the mixed multinomial logit choice model. We first develop a
graph representation of the assortment problem, then train a GCN to learn the
patterns of optimal assortments, and lastly propose two inference policies
based on the GCN's output. Due to the GCN's inherent ability to generalize
across inputs of varying sizes, we can use a GCN trained on small-scale
instances to facilitate large-scale instances. Extensive numerical experiments
demonstrate that given a GCN trained on small-scale instances (e.g., with 20
products), the proposed policies can achieve superior performance (90%+
optimality) on large-scale instances (with up to 2,000 products) within
seconds, which outperform existing heuristic policies in both performance and
efficiency. Furthermore, we extend our framework to a model-free setting where
the underlying choice model is unknown but transaction data is available. We
also conduct numerical experiments to demonstrate the effectiveness and
efficiency of our proposed policies in this setting.

</details>


### [187] [Collaboration Promotes Group Resilience in Multi-Agent RL](https://arxiv.org/abs/2111.06614)
*Ilai Shraga,Guy Azran,Matthias Gerstgrasser,Ofir Abu,Jeffrey S. Rosenschein,Sarah Keren*

Main category: cs.LG

TL;DR: 论文提出多智能体强化学习中的群体韧性概念，并通过实验验证协作协议对提升群体韧性的有效性。


<details>
  <summary>Details</summary>
Motivation: 研究多智能体环境下智能体如何通过协作应对环境变化，填补了此前单智能体韧性研究的空白。

Method: 提出群体韧性概念，并通过不同协作协议的实验验证其效果。

Result: 所有协作方法均比非协作方法表现出更高的群体韧性。

Conclusion: 协作是多智能体强化学习中实现群体韧性的关键。

Abstract: To effectively operate in various dynamic scenarios, RL agents must be
resilient to unexpected changes in their environment. Previous work on this
form of resilience has focused on single-agent settings. In this work, we
introduce and formalize a multi-agent variant of resilience, which we term
group resilience. We further hypothesize that collaboration with other agents
is key to achieving group resilience; collaborating agents adapt better to
environmental perturbations in multi-agent reinforcement learning (MARL)
settings. We test our hypothesis empirically by evaluating different
collaboration protocols and examining their effect on group resilience. Our
experiments show that all the examined collaborative approaches achieve higher
group resilience than their non-collaborative counterparts.

</details>


### [188] [Visually grounded emotion regulation via diffusion models and user-driven reappraisal](https://arxiv.org/abs/2507.10861)
*Edoardo Pinzuti,Oliver Tüscher,André Ferreira Castro*

Main category: cs.LG

TL;DR: 论文提出了一种基于视觉的认知重评增强方法，利用文本到图像的扩散模型生成支持性视觉反馈，显著降低了负面情绪。


<details>
  <summary>Details</summary>
Motivation: 传统的认知重评方法依赖高阶认知和语言过程，对创伤或抑郁患者效果有限，因此需要一种更直观的干预方式。

Method: 通过稳定扩散模型和微调的IP适配器，将用户的口头重评转化为情感一致的视觉反馈，并在实验中对比了有无AI辅助的效果。

Result: AI辅助的重评显著减少了负面情绪，且情感一致性越强，情绪缓解效果越好。

Conclusion: 生成式视觉输入能有效支持认知重评，为生成式AI、情感计算和治疗技术的结合开辟了新方向。

Abstract: Cognitive reappraisal is a key strategy in emotion regulation, involving
reinterpretation of emotionally charged stimuli to alter affective responses.
Despite its central role in clinical and cognitive science, real-world
reappraisal interventions remain cognitively demanding, abstract, and primarily
verbal. This reliance on higher-order cognitive and linguistic processes is
often impaired in individuals with trauma or depression, limiting the
effectiveness of standard approaches. Here, we propose a novel, visually based
augmentation of cognitive reappraisal by integrating large-scale text-to-image
diffusion models into the emotional regulation process. Specifically, we
introduce a system in which users reinterpret emotionally negative images via
spoken reappraisals, which are transformed into supportive, emotionally
congruent visualizations using stable diffusion models with a fine-tuned
IP-adapter. This generative transformation visually instantiates users'
reappraisals while maintaining structural similarity to the original stimuli,
externalizing and reinforcing regulatory intent. To test this approach, we
conducted a within-subject experiment (N = 20) using a modified cognitive
emotion regulation (CER) task. Participants reappraised or described aversive
images from the International Affective Picture System (IAPS), with or without
AI-generated visual feedback. Results show that AI-assisted reappraisal
significantly reduced negative affect compared to both non-AI and control
conditions. Further analyses reveal that sentiment alignment between
participant reappraisals and generated images correlates with affective relief,
suggesting that multimodal coherence enhances regulatory efficacy. These
findings demonstrate that generative visual input can support cogitive
reappraisal and open new directions at the intersection of generative AI,
affective computing, and therapeutic technology.

</details>


### [189] [GALDS: A Graph-Autoencoder-based Latent Dynamics Surrogate model to predict neurite material transport](https://arxiv.org/abs/2507.10871)
*Tsung Yeh Hsieh,Yongjie Jessica Zhang*

Main category: cs.LG

TL;DR: 提出了一种基于图自动编码器的潜在动态替代模型（GALDS），用于高效模拟神经树中的物质运输，显著提升了计算速度和准确性。


<details>
  <summary>Details</summary>
Motivation: 神经元的复杂几何结构对物质运输模拟提出了高计算需求，传统方法效率低下，需要优化。

Method: 使用图自动编码器编码网络几何、速度场和浓度分布，结合神经ODE预测潜在空间动态。

Result: 在8种未见几何和4种异常运输案例中，平均相对误差3%，最大误差<8%，速度提升10倍。

Conclusion: GALDS模型高效且准确，为神经树物质运输模拟提供了新解决方案。

Abstract: Neurons exhibit intricate geometries within their neurite networks, which
play a crucial role in processes such as signaling and nutrient transport.
Accurate simulation of material transport in the networks is essential for
understanding these biological phenomena but poses significant computational
challenges because of the complex tree-like structures involved. Traditional
approaches are time-intensive and resource-demanding, yet the inherent
properties of neuron trees, which consists primarily of pipes with steady-state
parabolic velocity profiles and bifurcations, provide opportunities for
computational optimization. To address these challenges, we propose a
Graph-Autoencoder-based Latent Dynamics Surrogate (GALDS) model, which is
specifically designed to streamline the simulation of material transport in
neural trees. GALDS employs a graph autoencoder to encode latent
representations of the network's geometry, velocity fields, and concentration
profiles. These latent space representations are then assembled into a global
graph, which is subsequently used to predict system dynamics in the latent
space via a trained graph latent space system dynamic model, inspired by the
Neural Ordinary Differential Equations (Neural ODEs) concept. The integration
of an autoencoder allows for the use of smaller graph neural network models
with reduced training data requirements. Furthermore, the Neural ODE component
effectively mitigates the issue of error accumulation commonly encountered in
recurrent neural networks. The effectiveness of the GALDS model is demonstrated
through results on eight unseen geometries and four abnormal transport
examples, where our approach achieves mean relative error of 3% with maximum
relative error <8% and demonstrates a 10-fold speed improvement compared to
previous surrogate model approaches.

</details>


### [190] [Domain-Adaptive Small Language Models for Structured Tax Code Prediction](https://arxiv.org/abs/2507.10880)
*Souvik Nath,Sumit Wadhwa,Luiz Perez*

Main category: cs.LG

TL;DR: 提出一种基于编码器-解码器架构的小型语言模型（SLM），用于改进产品和服务的税务代码预测，优于传统分类器和其他架构。


<details>
  <summary>Details</summary>
Motivation: 跨国企业需处理大量税务合规事务，准确预测税务代码（如HSN或SAC）至关重要，以避免罚款。

Method: 采用编码器-解码器架构的SLM，利用非结构化数据预测层次化税务代码序列。

Result: 实验表明，该模型在HSN等税务代码预测任务中表现优于传统分类器及其他架构。

Conclusion: 该方法可扩展至其他政府规定的税务代码，具有广泛应用潜力。

Abstract: Every day, multinational firms process thousands of transactions, each of
which must adhere to tax regulations that vary by jurisdiction and are often
nuanced. The determination of product and service tax codes, such as HSN or SAC
is a major use case in Tax compliance. An accurate determination of such codes
is imperative to avoid any tax penalties. This paper proposes a domain-adaptive
small language model (SLM) with an encoder-decoder architecture for the
enhanced prediction of product and service tax codes. In this approach, we
address the problem of predicting hierarchical tax code sequences using
unstructured product and services data. We employ an SLM based upon
encoder-decoder architecture as this enables sequential generation of tax codes
to capture the hierarchical dependencies present within the tax codes. Our
experiments demonstrate that encoder-decoder SLMs can be successfully applied
to the sequential prediction of structured tax codes, a domain that remains
comparatively unexplored in current NLP research. In this paper, we demonstrate
the superior performance of the domain-adaptive encoder-decoder SLMs over flat
classifiers when applied to the Harmonized System of Nomenclature (HSN), and
achieve superior results compared to decoder-only and encoder-only
architectures for structured sequence generation tasks. This approach can also
be scaled to other government-mandated tax commodity codes, such as United
Nations Standard Products and Services Codes (UNSPSC), or Brazil's Nomenclatura
Comum do Mercosul (NCM).

</details>


### [191] [Learning from Imperfect Data: Robust Inference of Dynamic Systems using Simulation-based Generative Model](https://arxiv.org/abs/2507.10884)
*Hyunwoo Cho,Hyeontae Jo,Hyung Ju Hwang*

Main category: cs.LG

TL;DR: 提出了一种基于模拟的生成模型SiGMoID，用于从噪声、稀疏或部分可观测数据中精确推断非线性动态系统的ODE参数和未观测组件。


<details>
  <summary>Details</summary>
Motivation: 非线性动态模型的系统推断在数据噪声大、稀疏或部分可观测时具有挑战性，需要一种鲁棒且精确的方法。

Method: 结合物理信息神经网络与超网络构建ODE求解器，以及Wasserstein生成对抗网络估计ODE参数。

Result: SiGMoID能量化数据噪声、估计系统参数并推断未观测组件，实验验证了其广泛适用性。

Conclusion: SiGMoID为非线性动态系统的推断提供了一种有效工具，适用于科学研究和工程系统。

Abstract: System inference for nonlinear dynamic models, represented by ordinary
differential equations (ODEs), remains a significant challenge in many fields,
particularly when the data are noisy, sparse, or partially observable. In this
paper, we propose a Simulation-based Generative Model for Imperfect Data
(SiGMoID) that enables precise and robust inference for dynamic systems. The
proposed approach integrates two key methods: (1) physics-informed neural
networks with hyper-networks that constructs an ODE solver, and (2) Wasserstein
generative adversarial networks that estimates ODE parameters by effectively
capturing noisy data distributions. We demonstrate that SiGMoID quantifies data
noise, estimates system parameters, and infers unobserved system components.
Its effectiveness is validated validated through realistic experimental
examples, showcasing its broad applicability in various domains, from
scientific research to engineered systems, and enabling the discovery of full
system dynamics.

</details>


### [192] [How to Protect Models against Adversarial Unlearning?](https://arxiv.org/abs/2507.10886)
*Patryk Jasiorski,Marek Klonowski,Michał Woźniak*

Main category: cs.LG

TL;DR: 论文研究了对抗性遗忘问题，提出了一种保护模型性能的新方法。


<details>
  <summary>Details</summary>
Motivation: AI模型需要遗忘以满足法律要求或应对数据问题，但遗忘可能导致性能下降。本文探讨了对抗性遗忘问题。

Method: 分析了对抗性遗忘的影响因素，提出了一种保护模型性能的方法。

Result: 结果表明，对抗性遗忘的影响取决于模型和遗忘策略，新方法能有效保护性能。

Conclusion: 新方法能有效应对自发或对抗性遗忘带来的性能下降。

Abstract: AI models need to be unlearned to fulfill the requirements of legal acts such
as the AI Act or GDPR, and also because of the need to remove toxic content,
debiasing, the impact of malicious instances, or changes in the data
distribution structure in which a model works. Unfortunately, removing
knowledge may cause undesirable side effects, such as a deterioration in model
performance. In this paper, we investigate the problem of adversarial
unlearning, where a malicious party intentionally sends unlearn requests to
deteriorate the model's performance maximally. We show that this phenomenon and
the adversary's capabilities depend on many factors, primarily on the backbone
model itself and strategy/limitations in selecting data to be unlearned. The
main result of this work is a new method of protecting model performance from
these side effects, both in the case of unlearned behavior resulting from
spontaneous processes and adversary actions.

</details>


### [193] [Outbound Modeling for Inventory Management](https://arxiv.org/abs/2507.10890)
*Riccardo Savorgnan,Udaya Ghai,Carson Eisenach,Dean Foster*

Main category: cs.LG

TL;DR: 论文研究了预测库存仓库满足客户需求的单位数量（“消耗”）及关联的运输成本问题，提出了一种概率预测模型，用于强化学习环境中的模拟。


<details>
  <summary>Details</summary>
Motivation: 准确建模库存消耗和运输成本对区域库存规划至关重要，尤其是在使用强化学习开发控制策略时。现有方法（如调用内部软件系统）不可微分且效率低，因此需要一种高效且可微分的方法。

Method: 将问题建模为概率预测问题，预测所有仓库在每个时间段的消耗和运输成本的联合分布，考虑库存位置和外部客户需求。模型需处理强化学习中的分布外场景，并提出了基于生产系统的验证方案。

Result: 初步结果表明模型在分布内设置下具有较高的准确性。

Conclusion: 提出的概率预测模型为强化学习环境中的库存规划提供了一种高效且可微分的解决方案，并通过验证方案确保了模型的鲁棒性。

Abstract: We study the problem of forecasting the number of units fulfilled (or
``drained'') from each inventory warehouse to meet customer demand, along with
the associated outbound shipping costs. The actual drain and shipping costs are
determined by complex production systems that manage the planning and execution
of customers' orders fulfillment, i.e. from where and how to ship a unit to be
delivered to a customer. Accurately modeling these processes is critical for
regional inventory planning, especially when using Reinforcement Learning (RL)
to develop control policies. For the RL usecase, a drain model is incorporated
into a simulator to produce long rollouts, which we desire to be
differentiable. While simulating the calls to the internal software systems can
be used to recover this transition, they are non-differentiable and too slow
and costly to run within an RL training environment. Accordingly, we frame this
as a probabilistic forecasting problem, modeling the joint distribution of
outbound drain and shipping costs across all warehouses at each time period,
conditioned on inventory positions and exogenous customer demand. To ensure
robustness in an RL environment, the model must handle out-of-distribution
scenarios that arise from off-policy trajectories. We propose a validation
scheme that leverages production systems to evaluate the drain model on
counterfactual inventory states induced by RL policies. Preliminary results
demonstrate the model's accuracy within the in-distribution setting.

</details>


### [194] [Class-Proportional Coreset Selection for Difficulty-Separable Data](https://arxiv.org/abs/2507.10904)
*Elisa Tsai,Haizhong Zheng,Atul Prakash*

Main category: cs.LG

TL;DR: 论文提出了一种考虑类别间数据难度差异的核集选择方法，通过引入类别难度可分性系数（CDSC）和改进的采样策略，显著提升了数据修剪的效果。


<details>
  <summary>Details</summary>
Motivation: 现有核集选择方法假设数据难度在类别间均匀分布，忽略了实际中类别间难度差异的问题，导致性能下降。

Method: 提出类别难度可分性系数（CDSC）衡量类别间难度差异，并设计类别比例采样策略（如CCS-CP）。

Result: 在多个数据集上验证，新方法在极端修剪率下性能下降更小，优于现有方法。

Conclusion: 显式建模类别难度可分性可提升数据修剪的效果，尤其在高风险场景中。

Abstract: High-quality training data is essential for building reliable and efficient
machine learning systems. One-shot coreset selection addresses this by pruning
the dataset while maintaining or even improving model performance, often
relying on training-dynamics-based data difficulty scores. However, most
existing methods implicitly assume class-wise homogeneity in data difficulty,
overlooking variation in data difficulty across different classes.
  In this work, we challenge this assumption by showing that, in domains such
as network intrusion detection and medical imaging, data difficulty often
clusters by class. We formalize this as class-difficulty separability and
introduce the Class Difficulty Separability Coefficient (CDSC) as a
quantitative measure. We demonstrate that high CDSC values correlate with
performance degradation in class-agnostic coreset methods, which tend to
overrepresent easy majority classes while neglecting rare but informative ones.
  To address this, we introduce class-proportional variants of multiple
sampling strategies. Evaluated on five diverse datasets spanning security and
medical domains, our methods consistently achieve state-of-the-art data
efficiency. For instance, on CTU-13, at an extreme 99% pruning rate, a
class-proportional variant of Coverage-centric Coreset Selection (CCS-CP) shows
remarkable stability, with accuracy dropping only 2.58%, precision 0.49%, and
recall 0.19%. In contrast, the class-agnostic CCS baseline, the next best
method, suffers sharper declines of 7.59% in accuracy, 4.57% in precision, and
4.11% in recall.
  We further show that aggressive pruning enhances generalization in noisy,
imbalanced, and large-scale datasets. Our results underscore that explicitly
modeling class-difficulty separability leads to more effective, robust, and
generalizable data pruning, particularly in high-stakes scenarios.

</details>


### [195] [Diffusion Decoding for Peptide De Novo Sequencing](https://arxiv.org/abs/2507.10955)
*Chi-en Amy Tai,Alexander Wong*

Main category: cs.LG

TL;DR: 论文探讨了在肽段从头测序中使用扩散解码器替代传统自回归解码器的方法，以解决错误传递问题并提升预测准确性。


<details>
  <summary>Details</summary>
Motivation: 传统自回归解码器（如Casanovo）在肽段测序中存在错误传递问题，且无法有效利用高置信区域，扩散解码器提供了一种新思路。

Method: 研究采用三种扩散解码器设计，结合背包束搜索和多种损失函数（如DINOISER），并与自回归解码器进行对比。

Result: 尽管肽段精度和召回率仍为0，但最佳扩散解码器设计在氨基酸召回率上显著提升了0.373。

Conclusion: 扩散解码器在提升模型敏感性和肽段测序技术方面具有潜力。

Abstract: Peptide de novo sequencing is a method used to reconstruct amino acid
sequences from tandem mass spectrometry data without relying on existing
protein sequence databases. Traditional deep learning approaches, such as
Casanovo, mainly utilize autoregressive decoders and predict amino acids
sequentially. Subsequently, they encounter cascading errors and fail to
leverage high-confidence regions effectively. To address these issues, this
paper investigates using diffusion decoders adapted for the discrete data
domain. These decoders provide a different approach, allowing sequence
generation to start from any peptide segment, thereby enhancing prediction
accuracy. We experiment with three different diffusion decoder designs,
knapsack beam search, and various loss functions. We find knapsack beam search
did not improve performance metrics and simply replacing the transformer
decoder with a diffusion decoder lowered performance. Although peptide
precision and recall were still 0, the best diffusion decoder design with the
DINOISER loss function obtained a statistically significant improvement in
amino acid recall by 0.373 compared to the baseline autoregressive
decoder-based Casanovo model. These findings highlight the potential of
diffusion decoders to not only enhance model sensitivity but also drive
significant advancements in peptide de novo sequencing.

</details>


### [196] [Physics-Informed Neural Networks For Semiconductor Film Deposition: A Review](https://arxiv.org/abs/2507.10983)
*Tao Han,Zahra Taheri,Hyunwoong Ko*

Main category: cs.LG

TL;DR: 本文综述了机器学习（ML）在半导体薄膜沉积工艺中的应用，重点介绍了物理信息神经网络（PINNs）的潜力，并提出了未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 半导体薄膜沉积工艺需要精确控制，而传统方法存在局限性。ML和PINNs为解决这些问题提供了新思路。

Method: 通过主题分析，总结了ML在薄膜沉积中的应用趋势、局限性和研究空白，并探讨了PINNs的物理知识嵌入策略。

Result: 研究发现ML和PINNs能提升工艺的准确性、可解释性和鲁棒性，但仍需解决现有方法的不足。

Conclusion: 本文为未来研究提供了方向，强调了PINNs在半导体制造中的潜力，以提升工艺效率和精度。

Abstract: Semiconductor manufacturing relies heavily on film deposition processes, such
as Chemical Vapor Deposition and Physical Vapor Deposition. These complex
processes require precise control to achieve film uniformity, proper adhesion,
and desired functionality. Recent advancements in Physics-Informed Neural
Networks (PINNs), an innovative machine learning (ML) approach, have shown
significant promise in addressing challenges related to process control,
quality assurance, and predictive modeling within semiconductor film deposition
and other manufacturing domains. This paper provides a comprehensive review of
ML applications targeted at semiconductor film deposition processes. Through a
thematic analysis, we identify key trends, existing limitations, and research
gaps, offering insights into both the advantages and constraints of current
methodologies. Our structured analysis aims to highlight the potential
integration of these ML techniques to enhance interpretability, accuracy, and
robustness in film deposition processes. Additionally, we examine
state-of-the-art PINN methods, discussing strategies for embedding physical
knowledge, governing laws, and partial differential equations into advanced
neural network architectures tailored for semiconductor manufacturing. Based on
this detailed review, we propose novel research directions that integrate the
strengths of PINNs to significantly advance film deposition processes. The
contributions of this study include establishing a clear pathway for future
research in integrating physics-informed ML frameworks, addressing existing
methodological gaps, and ultimately improving precision, scalability, and
operational efficiency within semiconductor manufacturing.

</details>


### [197] [StellarF: A Lora-Adapter Integrated Large Model Framework for Stellar Flare Forecasting with Historical & Statistical Data](https://arxiv.org/abs/2507.10986)
*Tianyu Su,Zhiqiang Zou,Ali Luo,Xiao Kong,Qingyu Lu,Min Li*

Main category: cs.LG

TL;DR: StellarF是一种利用LoRA和Adapter技术的高效参数学习方法，用于恒星耀斑预测，通过整合耀斑统计信息和历史记录模块，实现了多尺度模式识别，并在自建数据集上取得了最优性能。


<details>
  <summary>Details</summary>
Motivation: 恒星耀斑预测是天文学的重要研究方向，但受限于耀斑事件记录的稀疏性和缺乏领域特定的大规模预测模型。

Method: 提出StellarF模型，结合LoRA和Adapter技术，整合耀斑统计信息和历史记录模块，实现多尺度模式识别。

Result: 在Kepler和TESS光变曲线数据集上，StellarF表现优于现有方法。

Conclusion: StellarF为天体物理研究和跨学科应用提供了新的方法论框架。

Abstract: Stellar flare forecasting, a critical research frontier in astronomy, offers
profound insights into stellar activity. However, the field is constrained by
both the sparsity of recorded flare events and the absence of domain-specific
large-scale predictive models. To address these challenges, this study
introduces StellarF (Stellar Flare Forecasting), a novel large model that
leverages Low-Rank (LoRA) and Adapter techniques to parameter-efficient
learning for stellar flare forecasting. At its core, StellarF integrates an
flare statistical information module with a historical flare record module,
enabling multi-scale pattern recognition from observational data. Extensive
experiments on our self-constructed datasets (derived from Kepler and TESS
light curves) demonstrate that StellarF achieves state-of-the-art performance
compared to existing methods. The proposed prediction paradigm establishes a
novel methodological framework for advancing astrophysical research and
cross-disciplinary applications.

</details>


### [198] [High-Throughput Distributed Reinforcement Learning via Adaptive Policy Synchronization](https://arxiv.org/abs/2507.10990)
*Rodney Lafuente-Mercado*

Main category: cs.LG

TL;DR: ClusterEnv是一个轻量级、与学习器无关的分布式环境执行接口，采用DETACH模式解耦模拟与训练，并提出AAPS机制减少同步开销。


<details>
  <summary>Details</summary>
Motivation: 现有框架将模拟、学习逻辑和编排耦合为单一系统，限制了模块化和可重用性。

Method: ClusterEnv通过DETACH模式将reset()和step()操作卸载到远程工作器，并引入AAPS机制以减少策略过时问题。

Result: 实验表明，AAPS在离散控制任务中实现了高样本效率，且权重更新次数显著减少。

Conclusion: ClusterEnv能无缝集成到现有RL流程中，支持多种策略方法，且代码改动极少。

Abstract: Scaling reinforcement learning (RL) workloads often requires distributing
environment simulation across compute clusters. Existing frameworks entangle
simulation, learning logic, and orchestration into monolithic systems, limiting
modularity and reusability. We present ClusterEnv, a lightweight,
learner-agnostic interface for distributed environment execution that mirrors
the Gymnasium API. ClusterEnv introduces the DETACH pattern, which decouples
simulation from training by offloading reset() and step() operations to remote
workers while keeping learning centralized. To address policy staleness in
distributed execution, we propose Adaptive Actor Policy Synchronization (AAPS),
a divergence-triggered update mechanism that reduces synchronization overhead
without sacrificing performance. ClusterEnv integrates cleanly into existing RL
pipelines, supports both on-policy and off-policy methods, and requires minimal
code changes. Experiments on discrete control tasks demonstrate that AAPS
achieves high sample efficiency with significantly fewer weight updates. Source
code is available at https://github.com/rodlaf/ClusterEnv.

</details>


### [199] [Misalignment from Treating Means as Ends](https://arxiv.org/abs/2507.10995)
*Henrik Marklund,Alex Infanger,Benjamin Van Roy*

Main category: cs.LG

TL;DR: 论文探讨了奖励函数中终端目标和工具目标的混淆问题，指出这种混淆会导致强化学习中的严重对齐错误。


<details>
  <summary>Details</summary>
Motivation: 奖励函数通常不完美，可能混淆终端目标和工具目标，导致优化后的性能与真实目标不符。

Method: 通过一个简单例子展示终端目标和工具目标的混淆如何导致严重对齐问题，并分析其环境特性。

Result: 研究表明，即使轻微的混淆也会导致优化后的奖励函数表现不佳。

Conclusion: 论文强调了区分终端目标和工具目标的重要性，并讨论了奖励学习中可能出现的问题及其实际影响。

Abstract: Reward functions, learned or manually specified, are rarely perfect. Instead
of accurately expressing human goals, these reward functions are often
distorted by human beliefs about how best to achieve those goals. Specifically,
these reward functions often express a combination of the human's terminal
goals -- those which are ends in themselves -- and the human's instrumental
goals -- those which are means to an end. We formulate a simple example in
which even slight conflation of instrumental and terminal goals results in
severe misalignment: optimizing the misspecified reward function results in
poor performance when measured by the true reward function. This example
distills the essential properties of environments that make reinforcement
learning highly sensitive to conflation of instrumental and terminal goals. We
discuss how this issue can arise with a common approach to reward learning and
how it can manifest in real environments.

</details>


### [200] [Crafting Imperceptible On-Manifold Adversarial Attacks for Tabular Data](https://arxiv.org/abs/2507.10998)
*Zhipeng He,Alexander Stevens,Chun Ouyang,Johannes De Smedt,Alistair Barros,Catarina Moreira*

Main category: cs.LG

TL;DR: 提出了一种基于混合输入变分自编码器（VAE）的潜在空间扰动框架，用于生成不易察觉的对抗样本，适用于表格数据。


<details>
  <summary>Details</summary>
Motivation: 表格数据的异构性（混合类别和数值特征）使得对抗攻击难以定义不易察觉的修改，传统梯度方法生成的对抗样本易偏离原始数据分布。

Method: 使用混合输入VAE将类别嵌入和数值特征整合到统一潜在空间，生成统计一致的扰动。提出In-Distribution Success Rate（IDSR）衡量对抗样本的统计不可区分性。

Result: 在六个公开数据集和三种模型架构上验证，该方法显著降低了异常率，性能更一致，优于传统输入空间攻击和其他VAE方法。

Conclusion: 研究表明，基于VAE的攻击依赖于重建质量，但在足够训练数据下具有优越实用性，强调了在流形上扰动对表格数据对抗攻击的重要性。

Abstract: Adversarial attacks on tabular data present fundamental challenges distinct
from image or text domains due to the heterogeneous nature of mixed categorical
and numerical features. Unlike images where pixel perturbations maintain visual
similarity, tabular data lacks intuitive similarity metrics, making it
difficult to define imperceptible modifications. Additionally, traditional
gradient-based methods prioritise $\ell_p$-norm constraints, often producing
adversarial examples that deviate from the original data distributions, making
them detectable. We propose a latent space perturbation framework using a
mixed-input Variational Autoencoder (VAE) to generate imperceptible adversarial
examples. The proposed VAE integrates categorical embeddings and numerical
features into a unified latent manifold, enabling perturbations that preserve
statistical consistency. We specify In-Distribution Success Rate (IDSR) to
measure the proportion of adversarial examples that remain statistically
indistinguishable from the input distribution. Evaluation across six publicly
available datasets and three model architectures demonstrates that our method
achieves substantially lower outlier rates and more consistent performance
compared to traditional input-space attacks and other VAE-based methods adapted
from image domain approaches. Our comprehensive analysis includes
hyperparameter sensitivity, sparsity control mechanisms, and generative
architectural comparisons, revealing that VAE-based attacks depend critically
on reconstruction quality but offer superior practical utility when sufficient
training data is available. This work highlights the importance of on-manifold
perturbations for realistic adversarial attacks on tabular data, offering a
robust approach for practical deployment. The source code can be accessed
through https://github.com/ZhipengHe/VAE-TabAttack.

</details>


### [201] [AdaMuon: Adaptive Muon Optimizer](https://arxiv.org/abs/2507.11005)
*Chongjie Si,Debing Zhang,Wei Shen*

Main category: cs.LG

TL;DR: AdaMuon是一个基于Muon优化器的自适应学习率框架，通过两个模块提升训练效率，优于Muon且无需额外调参。


<details>
  <summary>Details</summary>
Motivation: 提升Muon优化器在大规模模型训练中的效率，同时保持训练稳定性。

Method: 引入两个模块：1) 参数级第二矩调制，捕捉正交梯度更新；2) RMS对齐的重新缩放，调整更新幅度。

Result: 在多模型规模和学习率下，AdaMuon表现优于Muon，加速收敛且稳定。

Conclusion: AdaMuon可无缝集成到现有Muon训练流程中，无需额外调参。

Abstract: We propose AdaMuon, an adaptive learning-rate framework built upon the
recently validated Muon optimizer, which has demonstrated substantial
efficiency gains over AdamW in large-scale model training. AdaMuon augments
Muon with two mutually dependent modules: (1) a per-parameter second-moment
modulation that captures orthogonal gradient updates to ensure update-level
adaptivity, and (2) a RMS-aligned rescaling that regulates the overall update
magnitude by aligning it with the intrinsic structure of the parameter space.
Empirical results on multiple model scales and learning-rate regimes confirm
that AdaMuon consistently outperforms the original Muon, delivering higher
acceleration in convergence while maintaining training stability. Our method
introduces no additional tuning burden and can be seamlessly integrated into
existing Muon training pipelines.

</details>


### [202] [Leveraging Advanced Machine Learning to Predict Turbulence Dynamics from Temperature Observations at an Experimental Prescribed Fire](https://arxiv.org/abs/2507.11012)
*Dipak Dulal,Joseph J. Charney,Michael R. Gallagher,Pitambar Acharya,Carmeliza Navasca,Nicholas S. Skowronski*

Main category: cs.LG

TL;DR: 利用机器学习模型从温度数据预测湍流动能（TKE），揭示了火灾环境中温度与气流的新关系。


<details>
  <summary>Details</summary>
Motivation: 探索通过更易获取的温度数据预测TKE的可能性，以改进火灾环境研究和模型预测。

Method: 使用深度神经网络、随机森林回归、梯度提升和高斯过程回归等机器学习模型分析温度扰动与TKE的关系。

Result: 尽管预测变量与目标变量相关性较弱，机器学习模型仍能较准确地预测TKE，回归模型表现尤为突出。

Conclusion: 研究展示了机器学习在火灾环境数据分析中的潜力，为改进火灾研究和操作策略提供了新方法。

Abstract: This study explores the potential for predicting turbulent kinetic energy
(TKE) from more readily acquired temperature data using temperature profiles
and turbulence data collected concurrently at 10 Hz during a small experimental
prescribed burn in the New Jersey Pine Barrens. Machine learning models,
including Deep Neural Networks, Random Forest Regressor, Gradient Boosting, and
Gaussian Process Regressor, were employed to assess the potential to predict
TKE from temperature perturbations and explore temporal and spatial dynamics of
correlations. Data visualization and correlation analyses revealed patterns and
relationships between thermocouple temperatures and TKE, providing insight into
the underlying dynamics. More accurate predictions of TKE were achieved by
employing various machine learning models despite a weak correlation between
the predictors and the target variable. The results demonstrate significant
success, particularly from regression models, in accurately predicting the TKE.
The findings of this study demonstrate a novel numerical approach to
identifying new relationships between temperature and airflow processes in and
around the fire environment. These relationships can help refine our
understanding of combustion environment processes and the coupling and
decoupling of fire environment processes necessary for improving fire
operations strategy and fire and smoke model predictions. The findings of this
study additionally highlight the valuable role of machine learning techniques
in analyzing the complex large datasets of the fire environments, showcasing
their potential to advance fire research and management practices.

</details>


### [203] [First-Order Error Matters: Accurate Compensation for Quantized Large Language Models](https://arxiv.org/abs/2507.11017)
*Xingyu Zheng,Haotong Qin,Yuye Li,Jiakai Wang,Jinyang Guo,Michele Magno,Xianglong Liu*

Main category: cs.LG

TL;DR: FOEM是一种新的PTQ方法，通过显式引入一阶梯度项改进量化误差补偿，显著提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于补偿的权重校准方法假设一阶项可忽略，但实际量化过程中一阶偏差会累积，导致假设不成立。

Method: FOEM通过直接计算潜在权重与全精度权重的差异近似梯度，避免高成本的反向传播，并利用预计算的Cholesky因子高效恢复Hessian子矩阵的逆。

Result: 在3位权重量化中，FOEM显著降低困惑度并提高准确率，接近全精度性能，且能与先进技术无缝集成。

Conclusion: FOEM在多种模型和基准测试中表现优异，显著缩小与全精度基线的差距，代码已开源。

Abstract: Post-training quantization (PTQ) offers an efficient approach to compressing
large language models (LLMs), significantly reducing memory access and
computational costs. Existing compensation-based weight calibration methods
often rely on a second-order Taylor expansion to model quantization error,
under the assumption that the first-order term is negligible in well-trained
full-precision models. However, we reveal that the progressive compensation
process introduces accumulated first-order deviations between latent weights
and their full-precision counterparts, making this assumption fundamentally
flawed. To address this, we propose FOEM, a novel PTQ method that explicitly
incorporates first-order gradient terms to improve quantization error
compensation. FOEM approximates gradients by directly computing the difference
between latent and full-precision weights, avoiding the high cost and limited
generalization of backpropagation-based gradient computation. This approach
introduces minimal additional computational overhead. Moreover, FOEM leverages
precomputed Cholesky factors to efficiently recover the inverse of Hessian
submatrices in real time. Extensive experiments across a wide range of models
and benchmarks demonstrate that FOEM consistently outperforms the classical
GPTQ method. In 3-bit weight-only quantization, FOEM reduces the perplexity of
Llama3-8B by 89.6%, and improves the 5-shot MMLU accuracy of Llama3-70B from
51.7% to 74.9%, approaching the full-precision performance of 78.6%.
Furthermore, FOEM can be seamlessly integrated with advanced techniques such as
GPTAQ and SpinQuant, yielding additional improvements under the challenging
W4A4KV4 setting, and further narrowing the accuracy gap with full-precision
baselines beyond what current state-of-the-art methods achieve. The code is
available at https://github.com/Xingyu-Zheng/FOEM.

</details>


### [204] [Relative Entropy Pathwise Policy Optimization](https://arxiv.org/abs/2507.11019)
*Claas Voelcker,Axel Brunnbauer,Marcel Hussing,Michal Nauman,Pieter Abbeel,Eric Eaton,Radu Grosu,Amir-massoud Farahmand,Igor Gilitschenski*

Main category: cs.LG

TL;DR: 论文提出了一种基于价值梯度的策略优化方法REPPO，结合路径策略梯度的样本效率和标准策略学习的简单性，降低了训练方差和资源需求。


<details>
  <summary>Details</summary>
Motivation: 解决策略梯度方法中高方差和路径策略梯度依赖准确动作条件值函数的问题，实现在策略学习中使用路径策略更新。

Method: 构建基于价值梯度的策略优化算法，平衡随机策略探索和稳定训练，提出REPPO方法。

Result: REPPO在实验中表现出高效样本利用、低资源需求和强健的超参数鲁棒性。

Conclusion: REPPO结合了路径策略梯度和标准策略学习的优势，为策略优化提供了高效稳定的解决方案。

Abstract: Score-function policy gradients have delivered strong results in
game-playing, robotics and language-model fine-tuning. Yet its high-variance
often undermines training stability. On the other hand, pathwise policy
gradients alleviate the training variance, but are reliable only when driven by
an accurate action-conditioned value function which is notoriously hard to
train without relying on past off-policy data. In this paper, we discuss how to
construct a value-gradient driven, on-policy algorithm that allow training
Q-value models purely from on-policy data, unlocking the possibility of using
pathwise policy updates in the context of on-policy learning. We show how to
balance stochastic policies for exploration with constrained policy updates for
stable training, and evaluate important architectural components that
facilitate accurate value function learning. Building on these insights, we
propose Relative Entropy Pathwise Policy Optimization (REPPO), an efficient
on-policy algorithm that combines the sample-efficiency of pathwise policy
gradients with the simplicity and minimal memory footprint of standard
on-policy learning. We demonstrate that REPPO provides strong empirical
performance at decreased sample requirements, wall-clock time, memory footprint
as well as high hyperparameter robustness in a set of experiments on two
standard GPU-parallelized benchmarks.

</details>


### [205] [GATE: Graph Attention Neural Networks with Real-Time Edge Construction for Robust Indoor Localization using Mobile Embedded Devices](https://arxiv.org/abs/2507.11053)
*Danish Gufran,Sudeep Pasricha*

Main category: cs.LG

TL;DR: GATE框架通过自适应图表示和新型向量方法，显著提升了室内定位精度，解决了设备异构性和噪声分布问题。


<details>
  <summary>Details</summary>
Motivation: 室内定位对智能环境和导航系统至关重要，但现有深度学习方法无法有效处理非欧几里得噪声和设备异构性。

Method: GATE框架提出AHV增强消息传递、MDHV解决GNN盲点问题，以及RTEC动态图构建方法。

Result: 实验表明，GATE在多种环境下定位误差显著低于现有方法。

Conclusion: GATE通过创新方法有效提升了室内定位的准确性和鲁棒性。

Abstract: Accurate indoor localization is crucial for enabling spatial context in smart
environments and navigation systems. Wi-Fi Received Signal Strength (RSS)
fingerprinting is a widely used indoor localization approach due to its
compatibility with mobile embedded devices. Deep Learning (DL) models improve
accuracy in localization tasks by learning RSS variations across locations, but
they assume fingerprint vectors exist in a Euclidean space, failing to
incorporate spatial relationships and the non-uniform distribution of
real-world RSS noise. This results in poor generalization across heterogeneous
mobile devices, where variations in hardware and signal processing distort RSS
readings. Graph Neural Networks (GNNs) can improve upon conventional DL models
by encoding indoor locations as nodes and modeling their spatial and signal
relationships as edges. However, GNNs struggle with non-Euclidean noise
distributions and suffer from the GNN blind spot problem, leading to degraded
accuracy in environments with dense access points (APs). To address these
challenges, we propose GATE, a novel framework that constructs an adaptive
graph representation of fingerprint vectors while preserving an indoor
state-space topology, modeling the non-Euclidean structure of RSS noise to
mitigate environmental noise and address device heterogeneity. GATE introduces
1) a novel Attention Hyperspace Vector (AHV) for enhanced message passing, 2) a
novel Multi-Dimensional Hyperspace Vector (MDHV) to mitigate the GNN blind
spot, and 3) an new Real-Time Edge Construction (RTEC) approach for dynamic
graph adaptation. Extensive real-world evaluations across multiple indoor
spaces with varying path lengths, AP densities, and heterogeneous devices
demonstrate that GATE achieves 1.6x to 4.72x lower mean localization errors and
1.85x to 4.57x lower worst-case errors compared to state-of-the-art indoor
localization frameworks.

</details>


### [206] [A Distance Metric for Mixed Integer Programming Instances](https://arxiv.org/abs/2507.11063)
*Gwen Maudet,Grégoire Danoy*

Main category: cs.LG

TL;DR: 本文提出了一种基于数学公式的MILP实例距离度量方法，用于解决现有相似性度量精度不足或依赖标签数据的问题。该方法通过离散化处理实现高效比较，并在实验中表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有MILP相似性度量方法精度不足或依赖标签数据，限制了其应用和泛化能力。

Method: 提出一种数学距离度量方法，通过离散化右端项、权重和变量，并借鉴Earth mover's distance进行约束比较。

Result: 贪婪版本在保持高精度的同时速度提升近200倍，优于现有非学习方法，与监督分类器性能相当。

Conclusion: 该方法为MILP实例比较提供了高效且无监督的解决方案，具有广泛的应用潜力。

Abstract: Mixed-integer linear programming (MILP) is a powerful tool for addressing a
wide range of real-world problems, but it lacks a clear structure for comparing
instances. A reliable similarity metric could establish meaningful
relationships between instances, enabling more effective evaluation of instance
set heterogeneity and providing better guidance to solvers, particularly when
machine learning is involved. Existing similarity metrics often lack precision
in identifying instance classes or rely heavily on labeled data, which limits
their applicability and generalization. To bridge this gap, this paper
introduces the first mathematical distance metric for MILP instances, derived
directly from their mathematical formulations. By discretizing right-hand
sides, weights, and variables into classes, the proposed metric draws
inspiration from the Earth mover's distance to quantify mismatches in
weight-variable distributions for constraint comparisons. This approach
naturally extends to enable instance-level comparisons. We evaluate both an
exact and a greedy variant of our metric under various parameter settings,
using the StrIPLIB dataset. Results show that all components of the metric
contribute to class identification, and that the greedy version achieves
accuracy nearly identical to the exact formulation while being nearly 200 times
faster. Compared to state-of-the-art baselines, including feature-based,
image-based, and neural network models, our unsupervised method consistently
outperforms all non-learned approaches and rivals the performance of a
supervised classifier on class and subclass grouping tasks.

</details>


### [207] [LogTinyLLM: Tiny Large Language Models Based Contextual Log Anomaly Detection](https://arxiv.org/abs/2507.11071)
*Isaiah Thompson Ocansey,Ritwik Bhattacharya,Tanmay Sen*

Main category: cs.LG

TL;DR: 论文提出了一种基于LoRA和适配器的高效微调方法，用于检测大规模日志数据中的异常序列，相比传统方法显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 传统基于规则或深度学习的日志异常检测方法因日志数据量大且复杂而效果有限，需要更高效的方法。

Method: 采用参数高效的微调方法（如LoRA和适配器）在Thunderbird数据集上测试不同小型LLM。

Result: LoRA微调比LogBert全微调性能提升18-19%，准确率达到97.76%-98.83%，而后者仅为79.37%。

Conclusion: LoRA微调是一种高效且性能优越的日志异常检测方法。

Abstract: Log anomaly detection using traditional rule based or deep learning based
methods is often challenging due to the large volume and highly complex nature
of log sequence. So effective way of detection of anomalous sequence of logs is
crucial for system maintenance and development. This paper proposes parameter
efficient finetuning specifically low rank adaptation (LoRA) and adapter based
approaches for finding contextual anomalies in sequence of logs in large log
data set. It compares different tiny large language models (LLMs) on the
Thunderbird dataset. The results show that LoRA based finetuning provides
substantial performance improvements of 18 to 19 percentage over LogBert based
full finetuning approach, achieving accuracy scores between 97.76% and 98.83%
compared to 79.37%.

</details>


### [208] [Real-Time Bayesian Detection of Drift-Evasive GNSS Spoofing in Reinforcement Learning Based UAV Deconfliction](https://arxiv.org/abs/2507.11173)
*Deepak Kumar Panda,Weisi Guo*

Main category: cs.LG

TL;DR: 论文提出了一种基于贝叶斯在线变化点检测（BOCPD）的方法，用于检测无人机导航中的漂移规避欺骗攻击，通过监测强化学习评论网络的时序变化，提高了检测准确性和响应速度。


<details>
  <summary>Details</summary>
Motivation: 无人机依赖全球导航卫星系统（GNSS）进行定位，但易受欺骗攻击，尤其是漂移规避攻击，传统检测方法因延迟问题难以应对。

Method: 采用贝叶斯在线变化点检测（BOCPD）方法，结合强化学习评论网络的时序价值估计，检测导航行为的细微偏差。

Result: 实验表明，该方法在检测漂移规避欺骗攻击时，准确率更高，误报率和漏报率更低，优于传统检测方法。

Conclusion: 该时序价值框架为无人机导航提供了更强的抗欺骗能力，适用于实时检测和应急响应。

Abstract: Autonomous unmanned aerial vehicles (UAVs) rely on global navigation
satellite system (GNSS) pseudorange measurements for accurate real-time
localization and navigation. However, this dependence exposes them to
sophisticated spoofing threats, where adversaries manipulate pseudoranges to
deceive UAV receivers. Among these, drift-evasive spoofing attacks subtly
perturb measurements, gradually diverting the UAVs trajectory without
triggering conventional signal-level anti-spoofing mechanisms. Traditional
distributional shift detection techniques often require accumulating a
threshold number of samples, causing delays that impede rapid detection and
timely response. Consequently, robust temporal-scale detection methods are
essential to identify attack onset and enable contingency planning with
alternative sensing modalities, improving resilience against stealthy
adversarial manipulations. This study explores a Bayesian online change point
detection (BOCPD) approach that monitors temporal shifts in value estimates
from a reinforcement learning (RL) critic network to detect subtle behavioural
deviations in UAV navigation. Experimental results show that this temporal
value-based framework outperforms conventional GNSS spoofing detectors,
temporal semi-supervised learning frameworks, and the Page-Hinkley test,
achieving higher detection accuracy and lower false-positive and false-negative
rates for drift-evasive spoofing attacks.

</details>


### [209] [Gradient Regularization-based Neural Granger Causality](https://arxiv.org/abs/2507.11178)
*Meiliang Liu,Huiwen Dong,Xiaoxiao Yang,Yunfang Xu,Zijin Li,Zhengye Si,Xinyue Yang,Zhiwen Zhao*

Main category: cs.LG

TL;DR: 提出了一种基于梯度正则化的神经格兰杰因果模型（GRNGC），解决了现有方法计算成本高和捕捉复杂交互能力弱的问题。


<details>
  <summary>Details</summary>
Motivation: 现有神经格兰杰因果模型需要为每个时间序列单独建模，计算成本高，且稀疏性惩罚削弱了模型捕捉复杂交互的能力。

Method: GRNGC仅需一个时间序列预测模型，并在输入与输出的梯度上应用L1正则化来推断因果关系，支持多种架构（如KAN、MLP、LSTM）。

Result: 在模拟和真实数据集（如DREAM、fMRI BOLD、基因数据）上，GRNGC优于基线方法，显著降低计算开销。

Conclusion: GRNGC是一种灵活高效的神经格兰杰因果模型，适用于多种时间序列预测任务。

Abstract: With the advancement of deep learning technologies, various neural
network-based Granger causality models have been proposed. Although these
models have demonstrated notable improvements, several limitations remain. Most
existing approaches adopt the component-wise architecture, necessitating the
construction of a separate model for each time series, which results in
substantial computational costs. In addition, imposing the sparsity-inducing
penalty on the first-layer weights of the neural network to extract causal
relationships weakens the model's ability to capture complex interactions. To
address these limitations, we propose Gradient Regularization-based Neural
Granger Causality (GRNGC), which requires only one time series prediction model
and applies $L_{1}$ regularization to the gradient between model's input and
output to infer Granger causality. Moreover, GRNGC is not tied to a specific
time series forecasting model and can be implemented with diverse architectures
such as KAN, MLP, and LSTM, offering enhanced flexibility. Numerical
simulations on DREAM, Lorenz-96, fMRI BOLD, and CausalTime show that GRNGC
outperforms existing baselines and significantly reduces computational
overhead. Meanwhile, experiments on real-world DNA, Yeast, HeLa, and bladder
urothelial carcinoma datasets further validate the model's effectiveness in
reconstructing gene regulatory networks.

</details>


### [210] [Mixture of Experts in Large Language Models](https://arxiv.org/abs/2507.11181)
*Danyang Zhang,Junhao Song,Ziqian Bi,Yingfang Yuan,Tianyang Wang,Joe Yeong,Junfeng Hao*

Main category: cs.LG

TL;DR: 本文综述了混合专家（MoE）架构在大语言模型中的应用，强调其在提升性能的同时保持低计算开销的能力，并探讨了其优势、挑战及未来方向。


<details>
  <summary>Details</summary>
Motivation: 研究MoE架构在大语言模型中的潜力，以提升模型性能并减少计算成本。

Method: 通过系统分析理论、架构设计及应用案例，探讨专家门控与路由机制、稀疏配置、元学习等。

Result: MoE架构在模型容量、任务性能及扩展性方面表现优越，但需关注专家多样性、校准和推理聚合。

Conclusion: MoE架构具有显著优势，但仍需解决当前局限性和挑战，为未来研究提供方向。

Abstract: This paper presents a comprehensive review of the Mixture-of-Experts (MoE)
architecture in large language models, highlighting its ability to
significantly enhance model performance while maintaining minimal computational
overhead. Through a systematic analysis spanning theoretical foundations, core
architectural designs, and large language model (LLM) applications, we examine
expert gating and routing mechanisms, hierarchical and sparse MoE
configurations, meta-learning approaches, multimodal and multitask learning
scenarios, real-world deployment cases, and recent advances and challenges in
deep learning. Our analysis identifies key advantages of MoE, including
superior model capacity compared to equivalent Bayesian approaches, improved
task-specific performance, and the ability to scale model capacity efficiently.
We also underscore the importance of ensuring expert diversity, accurate
calibration, and reliable inference aggregation, as these are essential for
maximizing the effectiveness of MoE architectures. Finally, this review
outlines current research limitations, open challenges, and promising future
directions, providing a foundation for continued innovation in MoE architecture
and its applications.

</details>


### [211] [Quantized Rank Reduction: A Communications-Efficient Federated Learning Scheme for Network-Critical Applications](https://arxiv.org/abs/2507.11183)
*Dimitrios Kritsiolis,Constantine Kotropoulos*

Main category: cs.LG

TL;DR: 提出了一种基于低秩近似和量化的高效通信联邦学习方案，以减少网络负载，同时保持模型准确性。


<details>
  <summary>Details</summary>
Motivation: 联邦学习中频繁的模型更新交换导致通信开销大，影响效率。

Method: 采用低秩近似神经网络梯度和量化技术，减少通信数据量。

Result: 显著降低了网络负载，同时对模型准确性影响最小。

Conclusion: 该方法在保持隐私和安全的同时，有效提升了联邦学习的通信效率。

Abstract: Federated learning is a machine learning approach that enables multiple
devices (i.e., agents) to train a shared model cooperatively without exchanging
raw data. This technique keeps data localized on user devices, ensuring privacy
and security, while each agent trains the model on their own data and only
shares model updates. The communication overhead is a significant challenge due
to the frequent exchange of model updates between the agents and the central
server. In this paper, we propose a communication-efficient federated learning
scheme that utilizes low-rank approximation of neural network gradients and
quantization to significantly reduce the network load of the decentralized
learning process with minimal impact on the model's accuracy.

</details>


### [212] [An Explainable AI-Enhanced Machine Learning Approach for Cardiovascular Disease Detection and Risk Assessment](https://arxiv.org/abs/2507.11185)
*Md. Emon Akter Sourov,Md. Sabbir Hossen,Pabon Shaha,Mohammad Minoar Hossain,Md Sadiq Iqbal*

Main category: cs.LG

TL;DR: 该研究提出了一种结合分类和回归模型的机器学习框架，用于心脏病检测和风险预测，使用SMOTE处理数据不平衡，随机森林和线性回归表现最佳。


<details>
  <summary>Details</summary>
Motivation: 传统心脏病诊断方法准确性不足，机器学习可提升诊断效率和准确性。

Method: 使用Heart Disease数据集，应用SMOTE生成合成数据，评估多种性能指标。

Result: 随机森林分类准确率97.2%，线性回归R2值0.992，模型表现优异。

Conclusion: 机器学习可显著改善心脏病诊断和风险预测，支持早期干预和临床决策。

Abstract: Heart disease remains a major global health concern, particularly in regions
with limited access to medical resources and diagnostic facilities. Traditional
diagnostic methods often fail to accurately identify and manage heart disease
risks, leading to adverse outcomes. Machine learning has the potential to
significantly enhance the accuracy, efficiency, and speed of heart disease
diagnosis. In this study, we proposed a comprehensive framework that combines
classification models for heart disease detection and regression models for
risk prediction. We employed the Heart Disease dataset, which comprises 1,035
cases. To address the issue of class imbalance, the Synthetic Minority
Oversampling Technique (SMOTE) was applied, resulting in the generation of an
additional 100,000 synthetic data points. Performance metrics, including
accuracy, precision, recall, F1-score, R2, MSE, RMSE, and MAE, were used to
evaluate the model's effectiveness. Among the classification models, Random
Forest emerged as the standout performer, achieving an accuracy of 97.2% on
real data and 97.6% on synthetic data. For regression tasks, Linear Regression
demonstrated the highest R2 values of 0.992 and 0.984 on real and synthetic
datasets, respectively, with the lowest error metrics. Additionally,
Explainable AI techniques were employed to enhance the interpretability of the
models. This study highlights the potential of machine learning to
revolutionize heart disease diagnosis and risk prediction, thereby facilitating
early intervention and enhancing clinical decision-making.

</details>


### [213] [AirLLM: Diffusion Policy-based Adaptive LoRA for Remote Fine-Tuning of LLM over the Air](https://arxiv.org/abs/2507.11515)
*Shiyi Yang,Xiaoxue Yu,Rongpeng Li,Jianhang Zhu,Zhifeng Zhao,Honggang Zhang*

Main category: cs.LG

TL;DR: AirLLM提出了一种通信感知的LoRA自适应框架，通过强化学习和扩散模型优化LoRA的秩配置，显著降低传输成本并提升性能。


<details>
  <summary>Details</summary>
Motivation: 边缘设备上运行大型语言模型（LLM）面临通信带宽和计算资源的限制，现有LoRA方法的固定秩配置和参数传输效率低下。

Method: AirLLM结合PPO和DDIM，通过观察无线状态和语言复杂性生成秩向量，优化传输效率。

Result: 实验表明，AirLLM在不同信噪比下均能提升性能并显著减少传输成本。

Conclusion: AirLLM为远程高效微调提供了一种可扩展的解决方案。

Abstract: Operating Large Language Models (LLMs) on edge devices is increasingly
challenged by limited communication bandwidth and strained computational and
memory costs. Thus, cloud-assisted remote fine-tuning becomes indispensable.
Nevertheless, existing Low-Rank Adaptation (LoRA) approaches typically employ
fixed or heuristic rank configurations, and the subsequent over-the-air
transmission of all LoRA parameters could be rather inefficient. To address
this limitation, we develop AirLLM, a hierarchical diffusion policy framework
for communication-aware LoRA adaptation. Specifically, AirLLM models the rank
configuration as a structured action vector that spans all LoRA-inserted
projections. To solve the underlying high-dimensional sequential
decision-making problem, a Proximal Policy Optimization (PPO) agent generates
coarse-grained decisions by jointly observing wireless states and linguistic
complexity, which are then refined via Denoising Diffusion Implicit Models
(DDIM) to produce high-resolution, task- and channel-adaptive rank vectors. The
two modules are optimized alternatively, with the DDIM trained under the
Classifier-Free Guidance (CFG) paradigm to maintain alignment with PPO rewards.
Experiments under varying signal-to-noise ratios demonstrate that AirLLM
consistently enhances fine-tuning performance while significantly reducing
transmission costs, highlighting the effectiveness of reinforcement-driven,
diffusion-refined rank adaptation for scalable and efficient remote fine-tuning
over the air.

</details>


### [214] [Striking the Perfect Balance: Preserving Privacy While Boosting Utility in Collaborative Medical Prediction Platforms](https://arxiv.org/abs/2507.11187)
*Shao-Bo Lin,Xiaotong Liu,Yao Wang*

Main category: cs.LG

TL;DR: 论文提出了一种隐私保护机制，并将其集成到一次性分布式学习框架中，以同时满足隐私需求和预测性能目标。


<details>
  <summary>Details</summary>
Motivation: 在线协作医疗预测平台虽然便捷，但隐私问题和预测质量低可能阻碍患者和医生的参与。

Method: 提出隐私保护机制，并结合一次性分布式学习框架，通过统计学习理论验证其性能。

Result: 理论证明该框架在特定隐私要求下可实现最优预测性能，并通过实验验证。

Conclusion: 该隐私保护协作医疗预测平台有效平衡了隐私和性能需求。

Abstract: Online collaborative medical prediction platforms offer convenience and
real-time feedback by leveraging massive electronic health records. However,
growing concerns about privacy and low prediction quality can deter patient
participation and doctor cooperation. In this paper, we first clarify the
privacy attacks, namely attribute attacks targeting patients and model
extraction attacks targeting doctors, and specify the corresponding privacy
principles. We then propose a privacy-preserving mechanism and integrate it
into a novel one-shot distributed learning framework, aiming to simultaneously
meet both privacy requirements and prediction performance objectives. Within
the framework of statistical learning theory, we theoretically demonstrate that
the proposed distributed learning framework can achieve the optimal prediction
performance under specific privacy requirements. We further validate the
developed privacy-preserving collaborative medical prediction platform through
both toy simulations and real-world data experiments.

</details>


### [215] [Gradient Descent on Logistic Regression: Do Large Step-Sizes Work with Data on the Sphere?](https://arxiv.org/abs/2507.11228)
*Si Yi Meng,Baptiste Goujaud,Antonio Orvieto,Christopher De Sa*

Main category: cs.LG

TL;DR: 论文探讨了梯度下降在逻辑回归中的行为，重点研究了数据等幅条件下是否保证全局收敛。


<details>
  <summary>Details</summary>
Motivation: 研究梯度下降在逻辑回归中的收敛行为，特别是在非可分数据集上的循环现象。

Method: 通过理论分析，验证数据等幅条件在一维和高维空间中的影响。

Result: 一维空间中数据等幅可保证全局收敛，但高维空间中仍可能出现循环行为。

Conclusion: 希望进一步研究循环行为的普遍性，并寻找保证大步长下全局收敛的充分条件。

Abstract: Gradient descent (GD) on logistic regression has many fascinating properties.
When the dataset is linearly separable, it is known that the iterates converge
in direction to the maximum-margin separator regardless of how large the step
size is. In the non-separable case, however, it has been shown that GD can
exhibit a cycling behaviour even when the step sizes is still below the
stability threshold $2/\lambda$, where $\lambda$ is the largest eigenvalue of
the Hessian at the solution. This short paper explores whether restricting the
data to have equal magnitude is a sufficient condition for global convergence,
under any step size below the stability threshold. We prove that this is true
in a one dimensional space, but in higher dimensions cycling behaviour can
still occur. We hope to inspire further studies on quantifying how common these
cycles are in realistic datasets, as well as finding sufficient conditions to
guarantee global convergence with large step sizes.

</details>


### [216] [Generative Click-through Rate Prediction with Applications to Search Advertising](https://arxiv.org/abs/2507.11246)
*Lingwei Kong,Lu Wang,Changping Peng,Zhangang Lin,Ching Law,Jingping Shao*

Main category: cs.LG

TL;DR: 论文提出了一种结合生成模型和判别模型的两阶段训练方法，以提高点击率（CTR）预测的精度。


<details>
  <summary>Details</summary>
Motivation: 生成模型（如GPT）在表达能力上具有优势，可以弥补判别模型的不足，从而提升CTR预测的准确性。

Method: 设计了两阶段训练：1）生成模型预训练用于用户行为序列中的下一项预测；2）在判别式CTR预测框架中对生成模型进行微调。

Result: 在新数据集上的实验和在线A/B测试验证了方法的有效性，并已部署于全球最大电商平台之一。

Conclusion: 生成模型与判别模型的结合显著提升了CTR预测性能，未来将公开代码和数据集。

Abstract: Click-Through Rate (CTR) prediction models are integral to a myriad of
industrial settings, such as personalized search advertising. Current methods
typically involve feature extraction from users' historical behavior sequences
combined with product information, feeding into a discriminative model that is
trained on user feedback to estimate CTR. With the success of models such as
GPT, the potential for generative models to enrich expressive power beyond
discriminative models has become apparent. In light of this, we introduce a
novel model that leverages generative models to enhance the precision of CTR
predictions in discriminative models. To reconcile the disparate data
aggregation needs of both model types, we design a two-stage training process:
1) Generative pre-training for next-item prediction with the given item
category in user behavior sequences; 2) Fine-tuning the well-trained generative
model within a discriminative CTR prediction framework. Our method's efficacy
is substantiated through extensive experiments on a new dataset, and its
significant utility is further corroborated by online A/B testing results.
Currently, the model is deployed on one of the world's largest e-commerce
platforms, and we intend to release the associated code and dataset in the
future.

</details>


### [217] [LyAm: Robust Non-Convex Optimization for Stable Learning in Noisy Environments](https://arxiv.org/abs/2507.11262)
*Elmira Mirzabeigi,Sepehr Rezaee,Kourosh Parand*

Main category: cs.LG

TL;DR: LyAm是一种新型优化器，结合了Adam的自适应矩估计和李雅普诺夫稳定性机制，用于解决深度神经网络训练中的噪声梯度和不稳定收敛问题。


<details>
  <summary>Details</summary>
Motivation: 深度神经网络在计算机视觉任务中常面临噪声梯度和不稳定收敛问题，影响性能和泛化能力。

Method: LyAm通过李雅普诺夫稳定性理论动态调整学习率，增强收敛鲁棒性并减少训练噪声。

Result: 在CIFAR-10和CIFAR-100等数据集上的实验表明，LyAm在准确性、收敛速度和稳定性上均优于现有优化器。

Conclusion: LyAm是一种强大的深度学习优化器，适用于复杂非凸场景，具有理论保证和实际优势。

Abstract: Training deep neural networks, particularly in computer vision tasks, often
suffers from noisy gradients and unstable convergence, which hinder performance
and generalization. In this paper, we propose LyAm, a novel optimizer that
integrates Adam's adaptive moment estimation with Lyapunov-based stability
mechanisms. LyAm dynamically adjusts the learning rate using Lyapunov stability
theory to enhance convergence robustness and mitigate training noise. We
provide a rigorous theoretical framework proving the convergence guarantees of
LyAm in complex, non-convex settings. Extensive experiments on like as CIFAR-10
and CIFAR-100 show that LyAm consistently outperforms state-of-the-art
optimizers in terms of accuracy, convergence speed, and stability, establishing
it as a strong candidate for robust deep learning optimization.

</details>


### [218] [Turning Sand to Gold: Recycling Data to Bridge On-Policy and Off-Policy Learning via Causal Bound](https://arxiv.org/abs/2507.11269)
*Tal Fiskus,Uri Shaham*

Main category: cs.LG

TL;DR: 论文提出了一种基于Neyman-Rubin潜在结果框架的深度强化学习方法，通过存储历史值网络输出来减少经验回放缓冲区大小，显著提高了样本效率。


<details>
  <summary>Details</summary>
Motivation: 深度强化学习（DRL）在复杂决策任务中表现出色，但需要大量训练步骤和经验回放缓冲区，导致计算和资源需求高。本文旨在解决这些问题。

Method: 利用Neyman-Rubin潜在结果框架，建立对事实损失的因果界限，通过存储历史值网络输出来优化数据利用。

Result: 在Atari 2600和MuJoCo领域的实验中，奖励比率提高了2427%，经验回放缓冲区大小减少了96%。

Conclusion: 该方法显著提高了样本效率，且计算成本几乎可以忽略不计。

Abstract: Deep reinforcement learning (DRL) agents excel in solving complex
decision-making tasks across various domains. However, they often require a
substantial number of training steps and a vast experience replay buffer,
leading to significant computational and resource demands. To address these
challenges, we introduce a novel theoretical result that leverages the
Neyman-Rubin potential outcomes framework into DRL. Unlike most methods that
focus on bounding the counterfactual loss, we establish a causal bound on the
factual loss, which is analogous to the on-policy loss in DRL. This bound is
computed by storing past value network outputs in the experience replay buffer,
effectively utilizing data that is usually discarded. Extensive experiments
across the Atari 2600 and MuJoCo domains on various agents, such as DQN and
SAC, achieve up to 2,427% higher reward ratio, outperforming the same agents
without our proposed term, and reducing the experience replay buffer size by up
to 96%, significantly improving sample efficiency at negligible cost.

</details>


### [219] [Fast Last-Iterate Convergence of SGD in the Smooth Interpolation Regime](https://arxiv.org/abs/2507.11274)
*Amit Attia,Matan Schliserman,Uri Sherman,Tomer Koren*

Main category: cs.LG

TL;DR: 研究了SGD在平滑凸目标函数下的收敛性，特别是在插值区域（噪声为零或接近零）的表现。


<details>
  <summary>Details</summary>
Motivation: 探索SGD在大步长下的最后迭代行为，对过参数化模型训练、持续学习的遗忘问题以及随机Kaczmarz方法收敛性的理解具有重要意义。

Method: 分析了SGD在β-平滑凸损失函数上的表现，步长η≤1/β，推导了最后迭代的期望超额风险。

Result: 在最优步长下，最后迭代的收敛率为O~(1/T + σ*/√T)；当噪声为零时，收敛率提升至O(1/√T)。

Conclusion: 扩展了Varre等人的结果，并改进了Evron等人在线性回归中的收敛率。

Abstract: We study population convergence guarantees of stochastic gradient descent
(SGD) for smooth convex objectives in the interpolation regime, where the noise
at optimum is zero or near zero. The behavior of the last iterate of SGD in
this setting -- particularly with large (constant) stepsizes -- has received
growing attention in recent years due to implications for the training of
over-parameterized models, as well as to analyzing forgetting in continual
learning and to understanding the convergence of the randomized Kaczmarz method
for solving linear systems. We establish that after $T$ steps of SGD on
$\beta$-smooth convex loss functions with stepsize $\eta \leq 1/\beta$, the
last iterate exhibits expected excess risk $\widetilde{O}(1/(\eta
T^{1-\beta\eta/2}) + \eta T^{\beta\eta/2} \sigma_\star^2)$, where
$\sigma_\star^2$ denotes the variance of the stochastic gradients at the
optimum. In particular, for a well-tuned stepsize we obtain a near optimal
$\widetilde{O}(1/T + \sigma_\star/\sqrt{T})$ rate for the last iterate,
extending the results of Varre et al. (2021) beyond least squares regression;
and when $\sigma_\star=0$ we obtain a rate of $O(1/\sqrt{T})$ with
$\eta=1/\beta$, improving upon the best-known $O(T^{-1/4})$ rate recently
established by Evron et al. (2025) in the special case of realizable linear
regression.

</details>


### [220] [Guiding LLM Decision-Making with Fairness Reward Models](https://arxiv.org/abs/2507.11344)
*Zara Hall,Melanie Subbiah,Thomas P Zollo,Kathleen McKeown,Richard Zemel*

Main category: cs.LG

TL;DR: 提出了一种公平奖励模型（FRM）框架，用于减少大语言模型在高风险决策中的偏见，同时保持或提高准确性。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在高风险决策（如保释或贷款审批）中的应用可能放大不公平偏见，需要一种方法来提升公平性。

Method: 训练一个通用的公平奖励模型（FRM），为LLM推理分配公平分数，减少偏见轨迹的权重。

Result: FRM无需额外微调即可跨任务、领域和模型家族迁移，在真实任务中提升公平性且不降低准确性。

Conclusion: FRM框架为高风险决策中可信赖的推理模型提供了可行解决方案。

Abstract: Large language models are increasingly used to support high-stakes decisions,
potentially influencing who is granted bail or receives a loan. Naive
chain-of-thought sampling can improve average decision accuracy, but has also
been shown to amplify unfair bias. To address this challenge and enable the
trustworthy use of reasoning models in high-stakes decision-making, we propose
a framework for training a generalizable Fairness Reward Model (FRM). Our model
assigns a fairness score to LLM reasoning, enabling the system to down-weight
biased trajectories and favor equitable ones when aggregating decisions across
reasoning chains. We show that a single Fairness Reward Model, trained on
weakly supervised, LLM-annotated examples of biased versus unbiased reasoning,
transfers across tasks, domains, and model families without additional
fine-tuning. Applied to real-world decision-making tasks including recidivism
prediction and social media moderation, we show that our approach consistently
improves fairness while matching, or even surpassing, baseline accuracy.

</details>


### [221] [Neurosymbolic Reasoning Shortcuts under the Independence Assumption](https://arxiv.org/abs/2507.11357)
*Emile van Krieken,Pasquale Minervini,Edoardo Ponti,Antonio Vergari*

Main category: cs.LG

TL;DR: 论文探讨了神经符号（NeSy）预测器中符号概念独立性假设的局限性，指出其可能导致模型无法正确表示某些概念组合的不确定性。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于解决NeSy预测器中独立性假设对学习和不确定性建模的潜在负面影响，尤其是推理捷径问题。

Method: 通过形式化分析，证明了符号概念独立性假设会导致模型无法表示特定概念组合的不确定性。

Result: 结果表明，独立性假设限制了NeSy系统对推理捷径的识别能力，导致模型可能基于错误原因做出正确预测。

Conclusion: 结论指出，独立性假设在NeSy预测器中存在根本性缺陷，需重新审视其适用性。

Abstract: The ubiquitous independence assumption among symbolic concepts in
neurosymbolic (NeSy) predictors is a convenient simplification: NeSy predictors
use it to speed up probabilistic reasoning. Recent works like van Krieken et
al. (2024) and Marconato et al. (2024) argued that the independence assumption
can hinder learning of NeSy predictors and, more crucially, prevent them from
correctly modelling uncertainty. There is, however, scepticism in the NeSy
community around the scenarios in which the independence assumption actually
limits NeSy systems (Faronius and Dos Martires, 2025). In this work, we settle
this question by formally showing that assuming independence among symbolic
concepts entails that a model can never represent uncertainty over certain
concept combinations. Thus, the model fails to be aware of reasoning shortcuts,
i.e., the pathological behaviour of NeSy predictors that predict correct
downstream tasks but for the wrong reasons.

</details>


### [222] [Local Pairwise Distance Matching for Backpropagation-Free Reinforcement Learning](https://arxiv.org/abs/2507.11367)
*Daniel Tanneberg*

Main category: cs.LG

TL;DR: 提出了一种无需反向传播的神经网络训练方法，通过局部信号和层间损失函数在强化学习环境中训练每一层，避免了梯度消失或爆炸问题。


<details>
  <summary>Details</summary>
Motivation: 传统反向传播方法需要存储激活值且易出现梯度问题，影响学习性能和稳定性。

Method: 利用多维尺度分析的局部损失函数，结合奖励驱动信号，在正向传播中训练每一层。

Result: 实验表明，该方法在性能上与传统反向传播方法相当，且提高了稳定性和一致性。

Conclusion: 该方法在强化学习中具有潜力，尤其在复杂环境中表现更优。

Abstract: Training neural networks with reinforcement learning (RL) typically relies on
backpropagation (BP), necessitating storage of activations from the forward
pass for subsequent backward updates. Furthermore, backpropagating error
signals through multiple layers often leads to vanishing or exploding
gradients, which can degrade learning performance and stability. We propose a
novel approach that trains each layer of the neural network using local signals
during the forward pass in RL settings. Our approach introduces local,
layer-wise losses leveraging the principle of matching pairwise distances from
multi-dimensional scaling, enhanced with optional reward-driven guidance. This
method allows each hidden layer to be trained using local signals computed
during forward propagation, thus eliminating the need for backward passes and
storing intermediate activations. Our experiments, conducted with policy
gradient methods across common RL benchmarks, demonstrate that this
backpropagation-free method achieves competitive performance compared to their
classical BP-based counterpart. Additionally, the proposed method enhances
stability and consistency within and across runs, and improves performance
especially in challenging environments.

</details>


### [223] [A Neural Network Model of Complementary Learning Systems: Pattern Separation and Completion for Continual Learning](https://arxiv.org/abs/2507.11393)
*James P Jun,Vijay Marupudi,Raj Sanjay Shah,Sashank Varma*

Main category: cs.LG

TL;DR: 论文提出了一种结合变分自编码器（VAE）和现代Hopfield网络（MHN）的持续学习模型，以减少神经网络中的灾难性遗忘问题。


<details>
  <summary>Details</summary>
Motivation: 人类能够在不遗忘旧知识的情况下学习新信息，而神经网络存在灾难性遗忘问题。研究旨在模拟人脑的互补学习系统（CLS）理论，实现持续学习。

Method: 利用VAE的模式完成能力和MHN的模式分离能力，构建了一个神经合理的持续学习模型。

Result: 在Split-MNIST任务上实现了接近最先进的准确率（约90%），显著减少了遗忘。

Conclusion: 该模型为生物和人工系统中的记忆巩固、泛化和持续学习提供了功能模板。

Abstract: Learning new information without forgetting prior knowledge is central to
human intelligence. In contrast, neural network models suffer from catastrophic
forgetting: a significant degradation in performance on previously learned
tasks when acquiring new information. The Complementary Learning Systems (CLS)
theory offers an explanation for this human ability, proposing that the brain
has distinct systems for pattern separation (encoding distinct memories) and
pattern completion (retrieving complete memories from partial cues). To capture
these complementary functions, we leverage the representational generalization
capabilities of variational autoencoders (VAEs) and the robust memory storage
properties of Modern Hopfield networks (MHNs), combining them into a neurally
plausible continual learning model. We evaluate this model on the Split-MNIST
task, a popular continual learning benchmark, and achieve close to
state-of-the-art accuracy (~90%), substantially reducing forgetting.
Representational analyses empirically confirm the functional dissociation: the
VAE underwrites pattern completion, while the MHN drives pattern separation. By
capturing pattern separation and completion in scalable architectures, our work
provides a functional template for modeling memory consolidation,
generalization, and continual learning in both biological and artificial
systems.

</details>


### [224] [Robust-Multi-Task Gradient Boosting](https://arxiv.org/abs/2507.11411)
*Seyedsaman Emami,Gonzalo Martínez-Muñoz,Daniel Hernández-Lobato*

Main category: cs.LG

TL;DR: 提出了一种名为R-MTGB的新型多任务梯度提升框架，通过分块学习共享模式、识别异常任务和微调任务特定预测器，有效处理任务异质性，提升多任务学习性能。


<details>
  <summary>Details</summary>
Motivation: 现实中的多任务学习常包含异常任务，这些任务与其他任务无益相似性，反而可能降低模型整体性能。为解决这一问题，需要一种能自动检测并适应任务异质性的方法。

Method: R-MTGB框架分为三部分：1) 学习共享模式；2) 通过正则化参数将任务划分为异常和非异常；3) 微调任务特定预测器。

Result: 实验表明，R-MTGB能有效隔离异常任务，促进知识迁移，减少预测误差，并在所有任务上实现性能提升。

Conclusion: R-MTGB在多任务学习中表现出鲁棒性、适应性和可靠收敛性，适用于复杂任务环境。

Abstract: Multi-task learning (MTL) has shown effectiveness in exploiting shared
information across tasks to improve generalization. MTL assumes tasks share
similarities that can improve performance. In addition, boosting algorithms
have demonstrated exceptional performance across diverse learning problems,
primarily due to their ability to focus on hard-to-learn instances and
iteratively reduce residual errors. This makes them a promising approach for
learning multi-task problems. However, real-world MTL scenarios often involve
tasks that are not well-aligned (known as outlier or adversarial tasks), which
do not share beneficial similarities with others and can, in fact, deteriorate
the performance of the overall model. To overcome this challenge, we propose
Robust-Multi-Task Gradient Boosting (R-MTGB), a novel boosting framework that
explicitly models and adapts to task heterogeneity during training. R-MTGB
structures the learning process into three sequential blocks: (1) learning
shared patterns, (2) partitioning tasks into outliers and non-outliers with
regularized parameters, and (3) fine-tuning task-specific predictors. This
architecture enables R-MTGB to automatically detect and penalize outlier tasks
while promoting effective knowledge transfer among related tasks. Our method
integrates these mechanisms seamlessly within gradient boosting, allowing
robust handling of noisy or adversarial tasks without sacrificing accuracy.
Extensive experiments on both synthetic benchmarks and real-world datasets
demonstrate that our approach successfully isolates outliers, transfers
knowledge, and consistently reduces prediction errors for each task
individually, and achieves overall performance gains across all tasks. These
results highlight robustness, adaptability, and reliable convergence of R-MTGB
in challenging MTL environments.

</details>


### [225] [Toward Improving fNIRS Classification: A Study on Activation Functions in Deep Neural Architectures](https://arxiv.org/abs/2507.11436)
*Behtom Adeli,John McLinden,Pankaj Pandey,Ming Shao,Yalda Shahriari*

Main category: cs.LG

TL;DR: 该研究探讨了激活函数在fNIRS深度学习任务中的影响，发现对称激活函数（如Tanh和Abs(x））在某些架构中优于ReLU。


<details>
  <summary>Details</summary>
Motivation: fNIRS领域中的非线性、低信噪比和信号变异性对模型准确性构成挑战，但激活函数的影响尚未系统研究。

Method: 使用多种深度学习架构（如fNIRSNet、AbsoluteNet等）评估传统和领域特定的激活函数，并进行标准化预处理和训练。

Result: 对称激活函数（如Tanh和Abs(x））在某些架构中表现优于ReLU，且对称性分析（通过MAF）进一步支持其有效性。

Conclusion: 选择与fNIRS信号特性匹配的激活函数对性能提升至关重要。

Abstract: Activation functions are critical to the performance of deep neural networks,
particularly in domains such as functional near-infrared spectroscopy (fNIRS),
where nonlinearity, low signal-to-noise ratio (SNR), and signal variability
poses significant challenges to model accuracy. However, the impact of
activation functions on deep learning (DL) performance in the fNIRS domain
remains underexplored and lacks systematic investigation in the current
literature. This study evaluates a range of conventional and field-specific
activation functions for fNIRS classification tasks using multiple deep
learning architectures, including the domain-specific fNIRSNet, AbsoluteNet,
MDNN, and shallowConvNet (as the baseline), all tested on a single dataset
recorded during an auditory task. To ensure fair a comparison, all networks
were trained and tested using standardized preprocessing and consistent
training parameters. The results show that symmetrical activation functions
such as Tanh and the Absolute value function Abs(x) can outperform commonly
used functions like the Rectified Linear Unit (ReLU), depending on the
architecture. Additionally, a focused analysis of the role of symmetry was
conducted using a Modified Absolute Function (MAF), with results further
supporting the effectiveness of symmetrical activation functions on performance
gains. These findings underscore the importance of selecting proper activation
functions that align with the signal characteristics of fNIRS data.

</details>


### [226] [Data Augmentation in Time Series Forecasting through Inverted Framework](https://arxiv.org/abs/2507.11439)
*Hongming Tan,Ting Chen,Ruochong Jin,Wai Kin Chan*

Main category: cs.LG

TL;DR: 论文提出了一种名为DAIF的新型数据增强方法，针对iTransformer在多元时间序列预测中的局限性，通过频率过滤和跨变量修补策略提升性能。


<details>
  <summary>Details</summary>
Motivation: iTransformer虽能有效捕捉多元相关性，但其倒置框架会削弱时间依赖性信息，并在变量相关性不显著时引入噪声。

Method: 提出了两种DAIF策略：频率过滤和跨变量修补，专门为倒置框架设计。

Result: 在多个数据集和倒置模型上的实验验证了DAIF的有效性。

Conclusion: DAIF成功解决了iTransformer的局限性，提升了多元时间序列预测的性能。

Abstract: Currently, iTransformer is one of the most popular and effective models for
multivariate time series (MTS) forecasting. Thanks to its inverted framework,
iTransformer effectively captures multivariate correlation. However, the
inverted framework still has some limitations. It diminishes temporal
interdependency information, and introduces noise in cases of nonsignificant
variable correlation. To address these limitations, we introduce a novel data
augmentation method on inverted framework, called DAIF. Unlike previous data
augmentation methods, DAIF stands out as the first real-time augmentation
specifically designed for the inverted framework in MTS forecasting. We first
define the structure of the inverted sequence-to-sequence framework, then
propose two different DAIF strategies, Frequency Filtering and Cross-variation
Patching to address the existing challenges of the inverted framework.
Experiments across multiple datasets and inverted models have demonstrated the
effectiveness of our DAIF.

</details>


### [227] [LRMR: LLM-Driven Relational Multi-node Ranking for Lymph Node Metastasis Assessment in Rectal Cancer](https://arxiv.org/abs/2507.11457)
*Yaoxian Dong,Yifan Gao,Haoyue Li,Yanfen Cui,Xin Gao*

Main category: cs.LG

TL;DR: LRMR框架通过两阶段LLM方法提升直肠癌淋巴结转移评估的准确性和可解释性，优于传统深度学习方法。


<details>
  <summary>Details</summary>
Motivation: 传统MRI评估和现有AI模型在淋巴结转移诊断中表现有限且缺乏可解释性，LRMR旨在解决这些问题。

Method: LRMR采用两阶段方法：1) 多模态LLM生成淋巴结的放射学特征报告；2) 文本LLM进行患者间特征比较，生成风险排名。

Result: 在117名患者中，LRMR的AUC为0.7917，F1-score为0.7200，优于ResNet50等基线模型。

Conclusion: LRMR通过分离视觉感知与认知推理，为淋巴结转移评估提供了高效、可解释的新范式。

Abstract: Accurate preoperative assessment of lymph node (LN) metastasis in rectal
cancer guides treatment decisions, yet conventional MRI evaluation based on
morphological criteria shows limited diagnostic performance. While some
artificial intelligence models have been developed, they often operate as black
boxes, lacking the interpretability needed for clinical trust. Moreover, these
models typically evaluate nodes in isolation, overlooking the patient-level
context. To address these limitations, we introduce LRMR, an LLM-Driven
Relational Multi-node Ranking framework. This approach reframes the diagnostic
task from a direct classification problem into a structured reasoning and
ranking process. The LRMR framework operates in two stages. First, a multimodal
large language model (LLM) analyzes a composite montage image of all LNs from a
patient, generating a structured report that details ten distinct radiological
features. Second, a text-based LLM performs pairwise comparisons of these
reports between different patients, establishing a relative risk ranking based
on the severity and number of adverse features. We evaluated our method on a
retrospective cohort of 117 rectal cancer patients. LRMR achieved an area under
the curve (AUC) of 0.7917 and an F1-score of 0.7200, outperforming a range of
deep learning baselines, including ResNet50 (AUC 0.7708). Ablation studies
confirmed the value of our two main contributions: removing the relational
ranking stage or the structured prompting stage led to a significant
performance drop, with AUCs falling to 0.6875 and 0.6458, respectively. Our
work demonstrates that decoupling visual perception from cognitive reasoning
through a two-stage LLM framework offers a powerful, interpretable, and
effective new paradigm for assessing lymph node metastasis in rectal cancer.

</details>


### [228] [D3FL: Data Distribution and Detrending for Robust Federated Learning in Non-linear Time-series Data](https://arxiv.org/abs/2507.11471)
*Harsha Varun Marisetty,Manik Gupta,Yogesh Simmhan*

Main category: cs.LG

TL;DR: 论文探讨了联邦学习在处理非线性、非平稳时间序列数据时的性能，发现其表现不如集中式方法，但通过适当的去趋势技术可以改善效果。


<details>
  <summary>Details</summary>
Motivation: 随着物联网设备的普及，其收集的时间序列数据具有非线性和非平稳特性，传统集中式分析方法存在延迟和通信成本问题。联邦学习作为一种分布式替代方案，但其在这些数据上的表现尚不明确。

Method: 研究使用非线性数据分布生成合成时间序列数据集，训练基于LSTM的预测模型，并比较集中式和联邦学习方法。同时评估去趋势技术对真实数据集的影响。

Result: 实验表明，联邦学习在非线性数据分布下表现较差，但合适的去趋势技术能显著提升其性能。

Conclusion: 联邦学习在处理复杂时间序列数据时需结合去趋势技术以优化性能。

Abstract: With advancements in computing and communication technologies, the Internet
of Things (IoT) has seen significant growth. IoT devices typically collect data
from various sensors, such as temperature, humidity, and energy meters. Much of
this data is temporal in nature. Traditionally, data from IoT devices is
centralized for analysis, but this approach introduces delays and increased
communication costs. Federated learning (FL) has emerged as an effective
alternative, allowing for model training across distributed devices without the
need to centralize data. In many applications, such as smart home energy and
environmental monitoring, the data collected by IoT devices across different
locations can exhibit significant variation in trends and seasonal patterns.
Accurately forecasting such non-stationary, non-linear time-series data is
crucial for applications like energy consumption estimation and weather
forecasting. However, these data variations can severely impact prediction
accuracy. The key contributions of this paper are: (1) Investigating how
non-linear, non-stationary time-series data distributions, like generalized
extreme value (gen-extreme) and log norm distributions, affect FL performance.
(2) Analyzing how different detrending techniques for non-linear time-series
data influence the forecasting model's performance in a FL setup. We generated
several synthetic time-series datasets using non-linear data distributions and
trained an LSTM-based forecasting model using both centralized and FL
approaches. Additionally, we evaluated the impact of detrending on real-world
datasets with non-linear time-series data distributions. Our experimental
results show that: (1) FL performs worse than centralized approaches when
dealing with non-linear data distributions. (2) The use of appropriate
detrending techniques improves FL performance, reducing loss across different
data distributions.

</details>


### [229] [Exploring the robustness of TractOracle methods in RL-based tractography](https://arxiv.org/abs/2507.11486)
*Jeremi Levesque,Antoine Théberge,Maxime Descoteaux,Pierre-Marc Jodoin*

Main category: cs.LG

TL;DR: 论文探讨了基于强化学习的TractOracle-RL框架的四种扩展方法，结合RL最新进展，提出了一种新的训练方案IRT，显著提升了纤维追踪的准确性和解剖学有效性。


<details>
  <summary>Details</summary>
Motivation: 传统纤维追踪方法存在局限性，强化学习（RL）因其潜力成为改进方向。TractOracle-RL通过引入解剖学先验减少假阳性，但仍有优化空间。

Method: 扩展了TractOracle-RL框架的四种方法，并提出迭代奖励训练（IRT）方案，利用束过滤方法迭代优化训练过程。

Result: 实验表明，结合RL框架的方法在不同数据集上均表现稳健，IRT显著提升了追踪的准确性和解剖学有效性。

Conclusion: 强化学习结合解剖学先验和迭代优化是纤维追踪的有效方向，IRT为未来研究提供了新思路。

Abstract: Tractography algorithms leverage diffusion MRI to reconstruct the fibrous
architecture of the brain's white matter. Among machine learning approaches,
reinforcement learning (RL) has emerged as a promising framework for
tractography, outperforming traditional methods in several key aspects.
TractOracle-RL, a recent RL-based approach, reduces false positives by
incorporating anatomical priors into the training process via a reward-based
mechanism. In this paper, we investigate four extensions of the original
TractOracle-RL framework by integrating recent advances in RL, and we evaluate
their performance across five diverse diffusion MRI datasets. Results
demonstrate that combining an oracle with the RL framework consistently leads
to robust and reliable tractography, regardless of the specific method or
dataset used. We also introduce a novel RL training scheme called Iterative
Reward Training (IRT), inspired by the Reinforcement Learning from Human
Feedback (RLHF) paradigm. Instead of relying on human input, IRT leverages
bundle filtering methods to iteratively refine the oracle's guidance throughout
training. Experimental results show that RL methods trained with oracle
feedback significantly outperform widely used tractography techniques in terms
of accuracy and anatomical validity.

</details>


### [230] [A parametric activation function based on Wendland RBF](https://arxiv.org/abs/2507.11493)
*Majid Darehmiraki*

Main category: cs.LG

TL;DR: 论文提出了一种基于Wendland径向基函数（RBFs）的新型参数化激活函数，用于解决传统激活函数的局限性，并通过理论和实验验证其优越性。


<details>
  <summary>Details</summary>
Motivation: 传统激活函数（如ReLU、sigmoid和tanh）存在局限性，Wendland RBFs因其紧支撑性、光滑性和正定性，被引入以改进梯度传播和训练稳定性。

Method: 结合标准Wendland分量与线性和指数项，提出增强型Wendland激活函数，具备可调局部性和改进的梯度传播特性。

Result: 在合成任务（如正弦波逼近）和基准数据集（MNIST、Fashion-MNIST）上表现出竞争力，尤其在回归任务中准确率更高。

Conclusion: Wendland激活函数将经典RBF理论与现代深度学习结合，能缓解过拟合并提升泛化能力，未来可探索混合架构和领域特定适配。

Abstract: This paper introduces a novel parametric activation function based on
Wendland radial basis functions (RBFs) for deep neural networks. Wendland RBFs,
known for their compact support, smoothness, and positive definiteness in
approximation theory, are adapted to address limitations of traditional
activation functions like ReLU, sigmoid, and tanh. The proposed enhanced
Wendland activation combines a standard Wendland component with linear and
exponential terms, offering tunable locality, improved gradient propagation,
and enhanced stability during training. Theoretical analysis highlights its
mathematical properties, including smoothness and adaptability, while empirical
experiments on synthetic tasks (e.g., sine wave approximation) and benchmark
datasets (MNIST, Fashion-MNIST) demonstrate competitive performance. Results
show that the Wendland-based activation achieves superior accuracy in certain
scenarios, particularly in regression tasks, while maintaining computational
efficiency. The study bridges classical RBF theory with modern deep learning,
suggesting that Wendland activations can mitigate overfitting and improve
generalization through localized, smooth transformations. Future directions
include hybrid architectures and domain-specific adaptations.

</details>


### [231] [Langevin Flows for Modeling Neural Latent Dynamics](https://arxiv.org/abs/2507.11531)
*Yue Song,T. Anderson Keller,Yisong Yue,Pietro Perona,Max Welling*

Main category: cs.LG

TL;DR: LangevinFlow是一种基于物理先验的变分自编码器，用于建模神经群体的动态结构和外部未观测影响。


<details>
  <summary>Details</summary>
Motivation: 神经群体表现出潜在的动态结构，需要模型捕捉其内在网络动态和外部未观测影响。

Method: 结合Langevin方程的时间演化，使用局部耦合振荡器网络参数化势函数，并采用循环编码器和Transformer解码器。

Result: 在合成数据和NLB基准测试中表现优异，匹配或超越现有方法。

Conclusion: LangevinFlow提供了一个灵活、高性能的框架，用于建模复杂神经动态及其未观测影响。

Abstract: Neural populations exhibit latent dynamical structures that drive
time-evolving spiking activities, motivating the search for models that capture
both intrinsic network dynamics and external unobserved influences. In this
work, we introduce LangevinFlow, a sequential Variational Auto-Encoder where
the time evolution of latent variables is governed by the underdamped Langevin
equation. Our approach incorporates physical priors -- such as inertia,
damping, a learned potential function, and stochastic forces -- to represent
both autonomous and non-autonomous processes in neural systems. Crucially, the
potential function is parameterized as a network of locally coupled
oscillators, biasing the model toward oscillatory and flow-like behaviors
observed in biological neural populations. Our model features a recurrent
encoder, a one-layer Transformer decoder, and Langevin dynamics in the latent
space. Empirically, our method outperforms state-of-the-art baselines on
synthetic neural populations generated by a Lorenz attractor, closely matching
ground-truth firing rates. On the Neural Latents Benchmark (NLB), the model
achieves superior held-out neuron likelihoods (bits per spike) and forward
prediction accuracy across four challenging datasets. It also matches or
surpasses alternative methods in decoding behavioral metrics such as hand
velocity. Overall, this work introduces a flexible, physics-inspired,
high-performing framework for modeling complex neural population dynamics and
their unobserved influences.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [232] [A Learning Framework For Cooperative Collision Avoidance of UAV Swarms Leveraging Domain Knowledge](https://arxiv.org/abs/2507.10913)
*Shuangyao Huang,Haibo Zhang,Zhiyi Huang*

Main category: cs.MA

TL;DR: 提出了一种基于领域知识驱动奖励的多智能体强化学习框架，用于无人机群的协同避碰。


<details>
  <summary>Details</summary>
Motivation: 解决无人机群在复杂环境中的协同避碰问题，减少智能体间的交互和复杂信用分配需求。

Method: 利用图像处理领域的知识设计奖励函数，将障碍物建模为二维场中的最大值，避免碰撞。

Result: 框架支持大规模无人机群训练，适应复杂环境，性能优于现有MARL算法。

Conclusion: 该框架通过领域知识驱动的奖励，实现了高效、节能的无人机群协同避碰。

Abstract: This paper presents a multi-agent reinforcement learning (MARL) framework for
cooperative collision avoidance of UAV swarms leveraging domain
knowledge-driven reward. The reward is derived from knowledge in the domain of
image processing, approximating contours on a two-dimensional field. By
modeling obstacles as maxima on the field, collisions are inherently avoided as
contours never go through peaks or intersect. Additionally, counters are smooth
and energy-efficient. Our framework enables training with large swarm sizes as
the agent interaction is minimized and the need for complex credit assignment
schemes or observation sharing mechanisms in state-of-the-art MARL approaches
are eliminated. Moreover, UAVs obtain the ability to adapt to complex
environments where contours may be non-viable or non-existent through intensive
training. Extensive experiments are conducted to evaluate the performances of
our framework against state-of-the-art MARL algorithms.

</details>


<div id='cs.MM'></div>

# cs.MM [[Back]](#toc)

### [233] [MultiVox: Benchmarking Voice Assistants for Multimodal Interactions](https://arxiv.org/abs/2507.10859)
*Ramaneswaran Selvakumar,Ashish Seth,Nishit Anand,Utkarsh Tyagi,Sonal Kumar,Sreyan Ghosh,Dinesh Manocha*

Main category: cs.MM

TL;DR: MultiVox是一个新的基准测试，旨在评估语音助手整合语音和视觉线索的能力，填补现有基准在评估多模态理解和上下文感知响应方面的不足。


<details>
  <summary>Details</summary>
Motivation: 当前基准未能全面评估语音助手在理解细粒度语音特征（如音高、情感、音色）和环境声学背景方面的能力，以及如何结合视觉信号生成上下文感知响应。

Method: 提出MultiVox基准，包含1000个人工标注的语音对话，涵盖多样化的副语言特征和视觉线索（如图像和视频），并对9种先进模型进行评估。

Result: 评估显示，尽管人类在这些任务中表现优异，现有模型在生成上下文接地响应方面仍存在困难。

Conclusion: MultiVox填补了多模态语音助手评估的空白，揭示了当前模型在整合语音和视觉线索方面的不足。

Abstract: The rapid progress of Large Language Models (LLMs) has empowered omni models
to act as voice assistants capable of understanding spoken dialogues. These
models can process multimodal inputs beyond text, such as speech and visual
data, enabling more context-aware interactions. However, current benchmarks
fall short in comprehensively evaluating how well these models generate
context-aware responses, particularly when it comes to implicitly understanding
fine-grained speech characteristics, such as pitch, emotion, timbre, and volume
or the environmental acoustic context such as background sounds. Additionally,
they inadequately assess the ability of models to align paralinguistic cues
with complementary visual signals to inform their responses. To address these
gaps, we introduce MultiVox, the first omni voice assistant benchmark designed
to evaluate the ability of voice assistants to integrate spoken and visual cues
including paralinguistic speech features for truly multimodal understanding.
Specifically, MultiVox includes 1000 human-annotated and recorded speech
dialogues that encompass diverse paralinguistic features and a range of visual
cues such as images and videos. Our evaluation on 9 state-of-the-art models
reveals that, although humans excel at these tasks, current models consistently
struggle to produce contextually grounded responses.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [234] [Learning to Move in Rhythm: Task-Conditioned Motion Policies with Orbital Stability Guarantees](https://arxiv.org/abs/2507.10602)
*Maximilian Stölzle,T. Konstantin Rusch,Zach J. Patterson,Rodrigo Pérez-Dattari,Francesco Stella,Josie Hughes,Cosimo Della Santina,Daniela Rus*

Main category: cs.RO

TL;DR: 论文提出了一种名为OSMPs的新框架，通过结合学习的微分同胚编码器和超临界Hopf分岔，解决了动态运动基元在捕捉复杂周期性行为和任务间插值方面的局限性。


<details>
  <summary>Details</summary>
Motivation: 动态运动基元（DMPs）在稳定性和抗干扰性方面表现良好，但难以捕捉复杂周期性行为且任务间插值能力有限，限制了其实际应用范围。

Method: 引入轨道稳定运动基元（OSMPs），结合学习的微分同胚编码器和超临界Hopf分岔，确保周期性运动的准确学习和轨道稳定性。

Result: OSMPs在仿真和真实机器人实验中表现优异，能够零样本泛化到未见过的任务，并优于现有基线方法。

Conclusion: OSMPs为机器人学习复杂周期性行为提供了高效且稳定的解决方案，扩展了动态运动基元的应用范围。

Abstract: Learning from demonstration provides a sample-efficient approach to acquiring
complex behaviors, enabling robots to move robustly, compliantly, and with
fluidity. In this context, Dynamic Motion Primitives offer built - in stability
and robustness to disturbances but often struggle to capture complex periodic
behaviors. Moreover, they are limited in their ability to interpolate between
different tasks. These shortcomings substantially narrow their applicability,
excluding a wide class of practically meaningful tasks such as locomotion and
rhythmic tool use. In this work, we introduce Orbitally Stable Motion
Primitives (OSMPs) - a framework that combines a learned diffeomorphic encoder
with a supercritical Hopf bifurcation in latent space, enabling the accurate
acquisition of periodic motions from demonstrations while ensuring formal
guarantees of orbital stability and transverse contraction. Furthermore, by
conditioning the bijective encoder on the task, we enable a single learned
policy to represent multiple motion objectives, yielding consistent zero-shot
generalization to unseen motion objectives within the training distribution. We
validate the proposed approach through extensive simulation and real-world
experiments across a diverse range of robotic platforms - from collaborative
arms and soft manipulators to a bio-inspired rigid-soft turtle robot -
demonstrating its versatility and effectiveness in consistently outperforming
state-of-the-art baselines such as diffusion policies, among others.

</details>


### [235] [Vision Language Action Models in Robotic Manipulation: A Systematic Review](https://arxiv.org/abs/2507.10672)
*Muhayy Ud Din,Waseem Akram,Lyes Saad Saoud,Jan Rosell,Irfan Hussain*

Main category: cs.RO

TL;DR: 本文综述了视觉语言动作（VLA）模型在机器人领域的应用，分析了102个模型、26个数据集和12个仿真平台，提出了数据集评估新标准和二维分类框架，并指出了未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 旨在统一视觉感知、自然语言理解和机器人控制，推动通用机器人代理的发展。

Method: 通过分析102个VLA模型、26个数据集和12个仿真平台，提出新的数据集评估标准和二维分类框架。

Result: 揭示了当前数据集的不足，提出了未来研究方向，如可扩展预训练协议和模块化架构设计。

Conclusion: 本文为机器人控制提供了技术参考和概念路线图，从数据生成到实际部署提供了全面指导。

Abstract: Vision Language Action (VLA) models represent a transformative shift in
robotics, with the aim of unifying visual perception, natural language
understanding, and embodied control within a single learning framework. This
review presents a comprehensive and forward-looking synthesis of the VLA
paradigm, with a particular emphasis on robotic manipulation and
instruction-driven autonomy. We comprehensively analyze 102 VLA models, 26
foundational datasets, and 12 simulation platforms that collectively shape the
development and evaluation of VLAs models. These models are categorized into
key architectural paradigms, each reflecting distinct strategies for
integrating vision, language, and control in robotic systems. Foundational
datasets are evaluated using a novel criterion based on task complexity,
variety of modalities, and dataset scale, allowing a comparative analysis of
their suitability for generalist policy learning. We introduce a
two-dimensional characterization framework that organizes these datasets based
on semantic richness and multimodal alignment, showing underexplored regions in
the current data landscape. Simulation environments are evaluated for their
effectiveness in generating large-scale data, as well as their ability to
facilitate transfer from simulation to real-world settings and the variety of
supported tasks. Using both academic and industrial contributions, we recognize
ongoing challenges and outline strategic directions such as scalable
pretraining protocols, modular architectural design, and robust multimodal
alignment strategies. This review serves as both a technical reference and a
conceptual roadmap for advancing embodiment and robotic control, providing
insights that span from dataset generation to real world deployment of
generalist robotic agents.

</details>


### [236] [Exteroception through Proprioception Sensing through Improved Contact Modeling for Soft Growing Robots](https://arxiv.org/abs/2507.10694)
*Francesco Fuentes,Serigne Diagne,Zachary Kingston,Laura H. Blumenschein*

Main category: cs.RO

TL;DR: 本文提出利用软体生长机器人作为环境探索和地图构建工具，通过建模碰撞行为和开发几何模拟器，验证了其在非结构化环境中的潜力。


<details>
  <summary>Details</summary>
Motivation: 软体机器人的被动变形特性使其在非结构化环境中具有优势，若能更好地理解其碰撞和变形行为，可通过触觉测量探索环境结构。

Method: 首先对离散转向中的碰撞行为进行建模，随后开发几何模拟器模拟2D环境中的机器人轨迹，最后通过蒙特卡洛采样验证模型和模拟器的有效性。

Result: 在均匀和非均匀环境中，该方法能快速逼近理想动作，展示了软体生长机器人在环境探索和地图构建中的潜力。

Conclusion: 软体生长机器人有望成为非结构化环境探索和地图构建的有效工具。

Abstract: Passive deformation due to compliance is a commonly used benefit of soft
robots, providing opportunities to achieve robust actuation with few active
degrees of freedom. Soft growing robots in particular have shown promise in
navigation of unstructured environments due to their passive deformation. If
their collisions and subsequent deformations can be better understood, soft
robots could be used to understand the structure of the environment from direct
tactile measurements. In this work, we propose the use of soft growing robots
as mapping and exploration tools. We do this by first characterizing collision
behavior during discrete turns, then leveraging this model to develop a
geometry-based simulator that models robot trajectories in 2D environments.
Finally, we demonstrate the model and simulator validity by mapping unknown
environments using Monte Carlo sampling to estimate the optimal next deployment
given current knowledge. Over both uniform and non-uniform environments, this
selection method rapidly approaches ideal actions, showing the potential for
soft growing robots in unstructured environment exploration and mapping.

</details>


### [237] [RCG: Safety-Critical Scenario Generation for Robust Autonomous Driving via Real-World Crash Grounding](https://arxiv.org/abs/2507.10749)
*Benjamin Stoler,Juliet Yang,Jonathan Francis,Jean Oh*

Main category: cs.RO

TL;DR: 论文提出了一种名为RCG的场景生成框架，通过将碰撞语义融入对抗扰动流程，生成更真实的高风险驾驶场景，提升自动驾驶系统的训练效果。


<details>
  <summary>Details</summary>
Motivation: 现实驾驶数据中安全关键场景稀缺，限制了自动驾驶系统的训练和评估效果。

Method: 结合对比预训练和基于视频的轨迹标注微调，构建安全感知行为表示，并嵌入到现有场景生成流程中。

Result: 实验显示，使用生成场景训练的自动驾驶系统下游成功率平均提升9.2%，且生成的对抗行为更真实。

Conclusion: RCG框架能有效生成高真实性和高风险场景，提升自动驾驶系统的压力测试效果。

Abstract: Safety-critical scenarios are essential for training and evaluating
autonomous driving (AD) systems, yet remain extremely rare in real-world
driving datasets. To address this, we propose Real-world Crash Grounding (RCG),
a scenario generation framework that integrates crash-informed semantics into
adversarial perturbation pipelines. We construct a safety-aware behavior
representation through contrastive pre-training on large-scale driving logs,
followed by fine-tuning on a small, crash-rich dataset with approximate
trajectory annotations extracted from video. This embedding captures semantic
structure aligned with real-world accident behaviors and supports selection of
adversary trajectories that are both high-risk and behaviorally realistic. We
incorporate the resulting selection mechanism into two prior scenario
generation pipelines, replacing their handcrafted scoring objectives with an
embedding-based criterion. Experimental results show that ego agents trained
against these generated scenarios achieve consistently higher downstream
success rates, with an average improvement of 9.2% across seven evaluation
settings. Qualitative and quantitative analyses further demonstrate that our
approach produces more plausible and nuanced adversary behaviors, enabling more
effective and realistic stress testing of AD systems. Code and tools will be
released publicly.

</details>


### [238] [rt-RISeg: Real-Time Model-Free Robot Interactive Segmentation for Active Instance-Level Object Understanding](https://arxiv.org/abs/2507.10776)
*Howard H. Qian,Yiting Chen,Gaotian Wang,Podshara Chanrungmaneekul,Kaiyu Hang*

Main category: cs.RO

TL;DR: 提出了一种实时交互感知框架rt-RISeg，通过机器人交互和设计的体帧不变特征（BFIF）实现未见物体的分割，无需学习分割模型，性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有未见物体实例分割（UOIS）方法依赖大规模数据集训练，容易过拟合静态视觉特征，泛化性能差。

Method: 基于视觉交互性原则，提出rt-RISeg框架，利用机器人交互和体帧不变特征（BFIF）实时分割物体。

Result: 平均分割准确率比现有UOIS方法高27.5%，并可作为视觉基础模型的提示进一步提升性能。

Conclusion: rt-RISeg通过交互感知实现了高效未见物体分割，无需依赖学习模型，具有广泛适用性。

Abstract: Successful execution of dexterous robotic manipulation tasks in new
environments, such as grasping, depends on the ability to proficiently segment
unseen objects from the background and other objects. Previous works in unseen
object instance segmentation (UOIS) train models on large-scale datasets, which
often leads to overfitting on static visual features. This dependency results
in poor generalization performance when confronted with out-of-distribution
scenarios. To address this limitation, we rethink the task of UOIS based on the
principle that vision is inherently interactive and occurs over time. We
propose a novel real-time interactive perception framework, rt-RISeg, that
continuously segments unseen objects by robot interactions and analysis of a
designed body frame-invariant feature (BFIF). We demonstrate that the relative
rotational and linear velocities of randomly sampled body frames, resulting
from selected robot interactions, can be used to identify objects without any
learned segmentation model. This fully self-contained segmentation pipeline
generates and updates object segmentation masks throughout each robot
interaction without the need to wait for an action to finish. We showcase the
effectiveness of our proposed interactive perception method by achieving an
average object segmentation accuracy rate 27.5% greater than state-of-the-art
UOIS methods. Furthermore, although rt-RISeg is a standalone framework, we show
that the autonomously generated segmentation masks can be used as prompts to
vision foundation models for significantly improved performance.

</details>


### [239] [Versatile and Generalizable Manipulation via Goal-Conditioned Reinforcement Learning with Grounded Object Detection](https://arxiv.org/abs/2507.10814)
*Huiyi Wang,Fahim Shahriar,Alireza Azimi,Gautham Vasan,Rupam Mahmood,Colin Bellinger*

Main category: cs.RO

TL;DR: 论文提出了一种将预训练模型（如大型语言模型和物体检测器）融入目标条件强化学习的方法，以提升机器人的通用抓取能力。


<details>
  <summary>Details</summary>
Motivation: 通用机器人操作（如抓取）在家庭和工作场景中需求广泛，但传统方法资源消耗大。预训练模型能高效处理文本提示和物体识别，为强化学习提供支持。

Method: 使用预训练物体检测模型，通过文本提示生成物体掩码，用于目标条件强化学习。掩码提供物体无关的线索，提升特征共享和泛化能力。

Result: 在模拟抓取任务中，掩码目标条件方法保持约90%的成功率，且收敛更快，适用于分布内外物体。

Conclusion: 该方法有效提升了机器人抓取的通用性和效率，展示了预训练模型在强化学习中的潜力。

Abstract: General-purpose robotic manipulation, including reach and grasp, is essential
for deployment into households and workspaces involving diverse and evolving
tasks. Recent advances propose using large pre-trained models, such as Large
Language Models and object detectors, to boost robotic perception in
reinforcement learning. These models, trained on large datasets via
self-supervised learning, can process text prompts and identify diverse objects
in scenes, an invaluable skill in RL where learning object interaction is
resource-intensive. This study demonstrates how to integrate such models into
Goal-Conditioned Reinforcement Learning to enable general and versatile robotic
reach and grasp capabilities. We use a pre-trained object detection model to
enable the agent to identify the object from a text prompt and generate a mask
for goal conditioning. Mask-based goal conditioning provides object-agnostic
cues, improving feature sharing and generalization. The effectiveness of the
proposed framework is demonstrated in a simulated reach-and-grasp task, where
the mask-based goal conditioning consistently maintains a $\sim$90\% success
rate in grasping both in and out-of-distribution objects, while also ensuring
faster convergence to higher returns.

</details>


### [240] [LF: Online Multi-Robot Path Planning Meets Optimal Trajectory Control](https://arxiv.org/abs/2507.11464)
*Ajay Shankar,Keisuke Okumura,Amanda Prorok*

Main category: cs.RO

TL;DR: 提出了一种多机器人控制框架，结合集中式路径规划和分布式轨迹控制，实现高效、可扩展的点对点导航。


<details>
  <summary>Details</summary>
Motivation: 解决多机器人在完全环境信息下的点对点导航问题，确保无碰撞和死锁，同时适应动态环境。

Method: 采用分层框架：集中式离散路径规划（利用MAPF技术）和分布式连续轨迹控制。具体实现为LF系统，结合LaCAM和Freyja。

Result: LF系统展示了高效、可扩展的多机器人导航能力，适应异步目标更新和动态环境，成功部署15架真实多旋翼无人机。

Conclusion: 该框架为多机器人导航提供了鲁棒且灵活的解决方案，适用于动态和复杂环境。

Abstract: We propose a multi-robot control paradigm to solve point-to-point navigation
tasks for a team of holonomic robots with access to the full environment
information. The framework invokes two processes asynchronously at high
frequency: (i) a centralized, discrete, and full-horizon planner for computing
collision- and deadlock-free paths rapidly, leveraging recent advances in
multi-agent pathfinding (MAPF), and (ii) dynamics-aware, robot-wise optimal
trajectory controllers that ensure all robots independently follow their
assigned paths reliably. This hierarchical shift in planning representation
from (i) discrete and coupled to (ii) continuous and decoupled domains enables
the framework to maintain long-term scalable motion synthesis. As an
instantiation of this idea, we present LF, which combines a fast
state-of-the-art MAPF solver (LaCAM), and a robust feedback control stack
(Freyja) for executing agile robot maneuvers. LF provides a robust and
versatile mechanism for lifelong multi-robot navigation even under asynchronous
and partial goal updates, and adapts to dynamic workspaces simply by quick
replanning. We present various multirotor and ground robot demonstrations,
including the deployment of 15 real multirotors with random, consecutive target
updates while a person walks through the operational workspace.

</details>


### [241] [Mixed Discrete and Continuous Planning using Shortest Walks in Graphs of Convex Sets](https://arxiv.org/abs/2507.10878)
*Savva Morozov,Tobia Marcucci,Bernhard Paus Graesdal,Alexandre Amice,Pablo A. Parrilo,Russ Tedrake*

Main category: cs.RO

TL;DR: 研究了在凸集图（GCS）中的最短路径问题（SWP），提出了一种基于半定规划和增量搜索的近似解法，适用于机器人学中的混合离散-连续规划问题。


<details>
  <summary>Details</summary>
Motivation: 凸集图（GCS）为混合离散-连续规划问题提供了一种统一的建模语言，但缺乏高效的求解方法。

Method: 通过半定规划合成成本函数的二次下界，并利用增量搜索算法近似求解最短路径。

Result: 实验验证了该方法在碰撞避免运动规划、技能链和混合系统最优控制中的高效性和性能。

Conclusion: SWP在GCS中为多种机器人学问题提供了统一且高效的解决方案。

Abstract: We study the Shortest-Walk Problem (SWP) in a Graph of Convex Sets (GCS). A
GCS is a graph where each vertex is paired with a convex program, and each edge
couples adjacent programs via additional costs and constraints. A walk in a GCS
is a sequence of vertices connected by edges, where vertices may be repeated.
The length of a walk is given by the cumulative optimal value of the
corresponding convex programs. To solve the SWP in GCS, we first synthesize a
piecewise-quadratic lower bound on the problem's cost-to-go function using
semidefinite programming. Then we use this lower bound to guide an
incremental-search algorithm that yields an approximate shortest walk. We show
that the SWP in GCS is a natural language for many mixed discrete-continuous
planning problems in robotics, unifying problems that typically require
specialized solutions while delivering high performance and computational
efficiency. We demonstrate this through experiments in collision-free motion
planning, skill chaining, and optimal control of hybrid systems.

</details>


### [242] [Object-Centric Mobile Manipulation through SAM2-Guided Perception and Imitation Learning](https://arxiv.org/abs/2507.10899)
*Wang Zhicheng,Satoshi Yagi,Satoshi Yamamori,Jun Morimoto*

Main category: cs.RO

TL;DR: 提出了一种基于SAM2的对象中心方法，用于提升移动机械臂在不同方向下的任务泛化能力。


<details>
  <summary>Details</summary>
Motivation: 当前移动机械臂框架通常将导航和操作解耦，导致导航不精确时性能下降，尤其是在角度不对齐的情况下。

Method: 利用SAM2基础模型，将操作方向信息融入模型，实现从不同方向对任务的一致理解。

Result: 在自定义移动机械臂上部署模型，并在不同角度下进行拾取放置任务测试，相比Action Chunking Transformer表现出更好的泛化能力。

Conclusion: 该方法显著提升了基于模仿学习的移动机械臂系统的泛化性和鲁棒性。

Abstract: Imitation learning for mobile manipulation is a key challenge in the field of
robotic manipulation. However, current mobile manipulation frameworks typically
decouple navigation and manipulation, executing manipulation only after
reaching a certain location. This can lead to performance degradation when
navigation is imprecise, especially due to misalignment in approach angles. To
enable a mobile manipulator to perform the same task from diverse orientations,
an essential capability for building general-purpose robotic models, we propose
an object-centric method based on SAM2, a foundation model towards solving
promptable visual segmentation in images, which incorporates manipulation
orientation information into our model. Our approach enables consistent
understanding of the same task from different orientations. We deploy the model
on a custom-built mobile manipulator and evaluate it on a pick-and-place task
under varied orientation angles. Compared to Action Chunking Transformer, our
model maintains superior generalization when trained with demonstrations from
varied approach angles. This work significantly enhances the generalization and
robustness of imitation learning-based mobile manipulation systems.

</details>


### [243] [Fast Non-Episodic Adaptive Tuning of Robot Controllers with Online Policy Optimization](https://arxiv.org/abs/2507.10914)
*James A. Preiss,Fengze Xie,Yiheng Lin,Adam Wierman,Yisong Yue*

Main category: cs.RO

TL;DR: 论文提出了一种在线参数调整算法M-GAPS，用于动态变化环境中的机器人控制器优化，并在硬件实验中验证了其高效性和适应性。


<details>
  <summary>Details</summary>
Motivation: 研究在动态变化的环境下，如何在线调整机器人控制器的参数，以应对未知的时间变化信息。

Method: 提出M-GAPS算法，结合状态空间和策略类的重新参数化，优化非线性几何四旋翼控制器的性能。

Result: M-GAPS在硬件实验中表现优于基于模型和无模型的基线方法，能快速适应未建模的风和负载扰动。

Conclusion: M-GAPS展示了在线策略优化的硬件实用性，比经典自适应控制更灵活，比无模型强化学习更稳定高效。

Abstract: We study online algorithms to tune the parameters of a robot controller in a
setting where the dynamics, policy class, and optimality objective are all
time-varying. The system follows a single trajectory without episodes or state
resets, and the time-varying information is not known in advance. Focusing on
nonlinear geometric quadrotor controllers as a test case, we propose a
practical implementation of a single-trajectory model-based online policy
optimization algorithm, M-GAPS,along with reparameterizations of the quadrotor
state space and policy class to improve the optimization landscape. In hardware
experiments,we compare to model-based and model-free baselines that impose
artificial episodes. We show that M-GAPS finds near-optimal parameters more
quickly, especially when the episode length is not favorable. We also show that
M-GAPS rapidly adapts to heavy unmodeled wind and payload disturbances, and
achieves similar strong improvement on a 1:6-scale Ackermann-steered car. Our
results demonstrate the hardware practicality of this emerging class of online
policy optimization that offers significantly more flexibility than classic
adaptive control, while being more stable and data-efficient than model-free
reinforcement learning.

</details>


### [244] [Unified Modeling and Structural Optimization of Multi-magnet Embedded Soft Continuum Robots for Enhanced Kinematic Performances](https://arxiv.org/abs/2507.10950)
*Zhiwei Wu,Jiahao Luo,Siyi Wei,Jinhui Zhang*

Main category: cs.RO

TL;DR: 本文提出了一种统一的建模和优化框架，用于提升多磁体嵌入式软连续体机器人（MeSCRs）的运动性能。通过建立基于扩展伪刚体模型的可微分系统公式，分析了磁驱动下的平衡适定性和诱导构型的几何特性。


<details>
  <summary>Details</summary>
Motivation: 提升多磁体嵌入式软连续体机器人的运动性能，解决其运动控制中的自由度优化问题。

Method: 基于扩展伪刚体模型建立可微分系统公式，结合微分几何开发结构优化框架，将经典运动学指标与磁体配置关联。

Result: 证明了MeSCR的最大可控自由度是嵌入磁体数量的两倍，并提出了优化条件和闭式解。仿真验证了框架的有效性。

Conclusion: 该框架通过优化磁体配置显著提升了MeSCR的运动性能，为软体机器人设计提供了新思路。

Abstract: This paper presents a unified modeling and optimization framework to enhance
the kinematic performance of multi-magnet embedded soft continuum robots
(MeSCRs). To this end, we establish a differentiable system formulation based
on an extended pseudo-rigid-body model. This formulation enables analysis of
the equilibrium well-posedness and the geometry of the induced configuration
under magnetic actuation. In particular, we show that the maximum controllable
degrees of freedom of a MeSCR equal twice the number of embedded magnets. We
subsequently develop a structural optimization framework based on differential
geometry that links classical kinematic measures (e.g., manipulability and
dexterity) to the configuration of embedded magnets. The resulting optimization
condition reveals that improving local performance requires structurally
modulating the spectrum of the configuration space metric to counteract its
distortion. Closed-form solutions for optimal magnet configurations are derived
under representative conditions, and a gradient-based numerical method is
proposed for general design scenarios. Simulation studies validate the
effectiveness of the proposed framework.

</details>


### [245] [Whom to Respond To? A Transformer-Based Model for Multi-Party Social Robot Interaction](https://arxiv.org/abs/2507.10960)
*He Zhu,Ryo Miyoshi,Yuki Okafuji*

Main category: cs.RO

TL;DR: 提出了一种基于Transformer的多任务学习框架，用于提升社交机器人在多用户环境中的决策能力，通过两种新的损失函数和新的数据集实现了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 多用户环境中，社交机器人需要理解上下文并决定何时及向谁回应，而现有研究主要关注单用户交互。

Method: 采用Transformer框架，引入两种新的损失函数：一种用于改进场景建模，另一种用于引导机器人选择回应的对象。构建了一个新的多用户HRI数据集。

Result: 模型在回应决策上表现优异，超越了现有的启发式和单任务方法。

Conclusion: 该研究为开发具有社交智能的机器人提供了支持，使其能够进行自然且上下文感知的多方交互。

Abstract: Prior human-robot interaction (HRI) research has primarily focused on
single-user interactions, where robots do not need to consider the timing or
recipient of their responses. However, in multi-party interactions, such as at
malls and hospitals, social robots must understand the context and decide both
when and to whom they should respond. In this paper, we propose a
Transformer-based multi-task learning framework to improve the decision-making
process of social robots, particularly in multi-user environments. Considering
the characteristics of HRI, we propose two novel loss functions: one that
enforces constraints on active speakers to improve scene modeling, and another
that guides response selection towards utterances specifically directed at the
robot. Additionally, we construct a novel multi-party HRI dataset that captures
real-world complexities, such as gaze misalignment. Experimental results
demonstrate that our model achieves state-of-the-art performance in respond
decisions, outperforming existing heuristic-based and single-task approaches.
Our findings contribute to the development of socially intelligent social
robots capable of engaging in natural and context-aware multi-party
interactions.

</details>


### [246] [EquiContact: A Hierarchical SE(3) Vision-to-Force Equivariant Policy for Spatially Generalizable Contact-rich Tasks](https://arxiv.org/abs/2507.10961)
*Joohwan Seo,Arvind Kruthiventy,Soomi Lee,Megan Teng,Xiang Zhang,Seoyeon Choi,Jongeun Choi,Roberto Horowitz*

Main category: cs.RO

TL;DR: 提出了一种名为EquiContact的分层策略框架，用于学习视觉驱动的机器人策略，在接触密集型任务（如peg-in-hole）中实现空间泛化。


<details>
  <summary>Details</summary>
Motivation: 解决接触密集型任务（如peg-in-hole）中策略的空间泛化问题，尤其是从少量演示中学习。

Method: 采用分层策略：高层视觉规划器（Diff-EDF）和低层顺应性视觉运动策略（G-CompACT），利用局部观测和SE(3)-等变性设计。

Result: 在真实世界的peg-in-hole任务中表现出近乎完美的成功率和未见空间配置的鲁棒泛化能力。

Conclusion: EquiContact框架及其设计原则（顺应性、局部策略和等变性）有效实现了接触密集型任务的空间泛化。

Abstract: This paper presents a framework for learning vision-based robotic policies
for contact-rich manipulation tasks that generalize spatially across task
configurations. We focus on achieving robust spatial generalization of the
policy for the peg-in-hole (PiH) task trained from a small number of
demonstrations. We propose EquiContact, a hierarchical policy composed of a
high-level vision planner (Diffusion Equivariant Descriptor Field, Diff-EDF)
and a novel low-level compliant visuomotor policy (Geometric Compliant ACT,
G-CompACT). G-CompACT operates using only localized observations (geometrically
consistent error vectors (GCEV), force-torque readings, and wrist-mounted RGB
images) and produces actions defined in the end-effector frame. Through these
design choices, we show that the entire EquiContact pipeline is
SE(3)-equivariant, from perception to force control. We also outline three key
components for spatially generalizable contact-rich policies: compliance,
localized policies, and induced equivariance. Real-world experiments on PiH
tasks demonstrate a near-perfect success rate and robust generalization to
unseen spatial configurations, validating the proposed framework and
principles. The experimental videos can be found on the project website:
https://sites.google.com/berkeley.edu/equicontact

</details>


### [247] [SMART-Merge Planner: A Safe Merging and Real-Time Motion Planner for Autonomous Highway On-Ramp Merging](https://arxiv.org/abs/2507.10968)
*Toktam Mohammadnejad,Jovin D'sa,Behdad Chalaki,Hossein Nourkhiz Mahjoub,Ehsan Moradi-Pari*

Main category: cs.RO

TL;DR: 论文提出了一种名为SMART-Merge的基于格点的运动规划器，用于实现安全舒适的强制并道，通过优化成本项和引入期望速度启发式，成功实现高效并道。


<details>
  <summary>Details</summary>
Motivation: 高速公路并道是一项复杂的驾驶任务，需要识别安全间隙、调整速度并完成并道操作，同时确保安全和舒适性。

Method: 采用基于格点的运动规划器，优化成本项以应对强制并道的挑战，并引入期望速度启发式。

Result: 通过高保真CarMaker模拟验证，SMART-Merge规划器在数百种高速公路并道场景中实现了100%的成功率，并道时间最短。

Conclusion: SMART-Merge规划器能够高效处理复杂的强制并道任务，为自动驾驶提供可靠且鲁棒的解决方案。

Abstract: Merging onto a highway is a complex driving task that requires identifying a
safe gap, adjusting speed, often interactions to create a merging gap, and
completing the merge maneuver within a limited time window while maintaining
safety and driving comfort. In this paper, we introduce a Safe Merging and
Real-Time Merge (SMART-Merge) planner, a lattice-based motion planner designed
to facilitate safe and comfortable forced merging. By deliberately adapting
cost terms to the unique challenges of forced merging and introducing a desired
speed heuristic, SMART-Merge planner enables the ego vehicle to merge
successfully while minimizing the merge time. We verify the efficiency and
effectiveness of the proposed merge planner through high-fidelity CarMaker
simulations on hundreds of highway merge scenarios. Our proposed planner
achieves the success rate of 100% as well as completes the merge maneuver in
the shortest amount of time compared with the baselines, demonstrating our
planner's capability to handle complex forced merge tasks and provide a
reliable and robust solution for autonomous highway merge. The simulation
result videos are available at
https://sites.google.com/view/smart-merge-planner/home.

</details>


### [248] [Uncertainty Aware Mapping for Vision-Based Underwater Robots](https://arxiv.org/abs/2507.10991)
*Abhimanyu Bhowmik,Mohit Singh,Madhushree Sannigrahi,Martin Ludvigsen,Kostas Alexis*

Main category: cs.RO

TL;DR: 本文探讨了如何在基于视觉的水下机器人中表示地图不一致性，并将深度估计置信度融入体素地图框架Voxblox中，改进了权重计算和更新机制。实验验证了方法的有效性。


<details>
  <summary>Details</summary>
Motivation: 传统传感器和预规划路径在受限空间中的水下机器人应用中存在局限性，视觉感知的环境表示易受噪声和变化影响，因此需要解决地图不一致性问题。

Method: 使用RAFT-Stereo模型估计场景深度和置信度，并将其集成到Voxblox体素地图框架中，改进了权重计算和更新机制。

Result: 在受限水池和Trondheim峡湾码头进行的实验中，水下机器人展示了不确定性可视化的变化。

Conclusion: 提出的方法有效提升了视觉感知在受限空间中的地图表示精度和可靠性。

Abstract: Vision-based underwater robots can be useful in inspecting and exploring
confined spaces where traditional sensors and preplanned paths cannot be
followed. Sensor noise and situational change can cause significant uncertainty
in environmental representation. Thus, this paper explores how to represent
mapping inconsistency in vision-based sensing and incorporate depth estimation
confidence into the mapping framework. The scene depth and the confidence are
estimated using the RAFT-Stereo model and are integrated into a voxel-based
mapping framework, Voxblox. Improvements in the existing Voxblox weight
calculation and update mechanism are also proposed. Finally, a qualitative
analysis of the proposed method is performed in a confined pool and in a pier
in the Trondheim fjord. Experiments using an underwater robot demonstrated the
change in uncertainty in the visualization.

</details>


### [249] [ILCL: Inverse Logic-Constraint Learning from Temporally Constrained Demonstrations](https://arxiv.org/abs/2507.11000)
*Minwoo Cho,Jaehwi Jang,Daehyung Park*

Main category: cs.RO

TL;DR: 提出了一种新的时间约束学习方法（ILCL），通过遗传算法和逻辑约束强化学习的博弈，高效学习并转移时间逻辑约束。


<details>
  <summary>Details</summary>
Motivation: 解决从演示中学习时间约束的挑战，包括组合空间大和非马尔可夫约束的模糊性。

Method: ILCL结合遗传算法的时间逻辑挖掘（GA-TL-Mining）和逻辑约束强化学习（Logic-CRL），前者构建TLTL语法树，后者优化策略。

Result: 在四个时间约束任务中优于现有方法，并成功应用于现实任务。

Conclusion: ILCL是一种高效的时间约束学习方法，适用于复杂任务和现实场景。

Abstract: We aim to solve the problem of temporal-constraint learning from
demonstrations to reproduce demonstration-like logic-constrained behaviors.
Learning logic constraints is challenging due to the combinatorially large
space of possible specifications and the ill-posed nature of non-Markovian
constraints. To figure it out, we introduce a novel temporal-constraint
learning method, which we call inverse logic-constraint learning (ILCL). Our
method frames ICL as a two-player zero-sum game between 1) a genetic
algorithm-based temporal-logic mining (GA-TL-Mining) and 2) logic-constrained
reinforcement learning (Logic-CRL). GA-TL-Mining efficiently constructs syntax
trees for parameterized truncated linear temporal logic (TLTL) without
predefined templates. Subsequently, Logic-CRL finds a policy that maximizes
task rewards under the constructed TLTL constraints via a novel constraint
redistribution scheme. Our evaluations show ILCL outperforms state-of-the-art
baselines in learning and transferring TL constraints on four temporally
constrained tasks. We also demonstrate successful transfer to real-world
peg-in-shallow-hole tasks.

</details>


### [250] [Learning to Tune Like an Expert: Interpretable and Scene-Aware Navigation via MLLM Reasoning and CVAE-Based Adaptation](https://arxiv.org/abs/2507.11001)
*Yanbo Wang,Zipeng Fang,Lei Zhao,Weidong Chen*

Main category: cs.RO

TL;DR: LE-Nav是一种基于多模态大语言模型和条件变分自编码器的导航框架，通过自适应调整规划器超参数，实现零样本场景理解和专家级调优，显著提升了导航性能和社会接受度。


<details>
  <summary>Details</summary>
Motivation: 传统导航系统在动态和非结构化环境中表现不佳，强化学习方法因泛化能力差和仿真多样性不足而难以实际应用。

Method: 利用多模态大语言模型推理和条件变分自编码器，结合一次性示例和思维链提示策略，实现场景感知和超参数自适应调整。

Result: 实验表明LE-Nav在多种规划器和场景中实现人类级调优，实际导航测试和用户研究显示其在成功率、效率、安全性和舒适度上优于现有方法。

Conclusion: LE-Nav通过智能超参数调优和场景理解，显著提升了导航系统的性能和社会接受度，适用于动态和非结构化环境。

Abstract: Service robots are increasingly deployed in diverse and dynamic environments,
where both physical layouts and social contexts change over time and across
locations. In these unstructured settings, conventional navigation systems that
rely on fixed parameters often fail to generalize across scenarios, resulting
in degraded performance and reduced social acceptance. Although recent
approaches have leveraged reinforcement learning to enhance traditional
planners, these methods often fail in real-world deployments due to poor
generalization and limited simulation diversity, which hampers effective
sim-to-real transfer. To tackle these issues, we present LE-Nav, an
interpretable and scene-aware navigation framework that leverages multi-modal
large language model reasoning and conditional variational autoencoders to
adaptively tune planner hyperparameters. To achieve zero-shot scene
understanding, we utilize one-shot exemplars and chain-of-thought prompting
strategies. Additionally, a conditional variational autoencoder captures the
mapping between natural language instructions and navigation hyperparameters,
enabling expert-level tuning. Experiments show that LE-Nav can generate
hyperparameters achieving human-level tuning across diverse planners and
scenarios. Real-world navigation trials and a user study on a smart wheelchair
platform demonstrate that it outperforms state-of-the-art methods on
quantitative metrics such as success rate, efficiency, safety, and comfort,
while receiving higher subjective scores for perceived safety and social
acceptance. Code is available at https://github.com/Cavendish518/LE-Nav.

</details>


### [251] [Enhancing Autonomous Manipulator Control with Human-in-loop for Uncertain Assembly Environments](https://arxiv.org/abs/2507.11006)
*Ashutosh Mishra,Shreya Santra,Hazal Gozbasi,Kentaro Uno,Kazuya Yoshida*

Main category: cs.RO

TL;DR: 研究提出了一种结合自主机器人与人类在环控制的先进方法，用于增强月球任务中机器人在不确定和挑战性环境下的操作能力。


<details>
  <summary>Details</summary>
Motivation: 提高太空任务中机器人操作的可靠性和效率，特别是在复杂环境下部署柔性太阳能板的任务。

Method: 集成人类决策与自主机器人功能，利用实时反馈和数字孪生技术优化任务执行。

Result: 系统在模拟月球条件下验证了其性能，能够应对极端光照、多变地形和传感器限制。

Conclusion: 该方法显著提升了任务可靠性，为未来太空任务提供了可行的解决方案。

Abstract: This study presents an advanced approach to enhance robotic manipulation in
uncertain and challenging environments, with a focus on autonomous operations
augmented by human-in-the-loop (HITL) control for lunar missions. By
integrating human decision-making with autonomous robotic functions, the
research improves task reliability and efficiency for space applications. The
key task addressed is the autonomous deployment of flexible solar panels using
an extendable ladder-like structure and a robotic manipulator with real-time
feedback for precision. The manipulator relays position and force-torque data,
enabling dynamic error detection and adaptive control during deployment. To
mitigate the effects of sinkage, variable payload, and low-lighting conditions,
efficient motion planning strategies are employed, supplemented by human
control that allows operators to intervene in ambiguous scenarios. Digital twin
simulation enhances system robustness by enabling continuous feedback,
iterative task refinement, and seamless integration with the deployment
pipeline. The system has been tested to validate its performance in simulated
lunar conditions and ensure reliability in extreme lighting, variable terrain,
changing payloads, and sensor limitations.

</details>


### [252] [TRAN-D: 2D Gaussian Splatting-based Sparse-view Transparent Object Depth Reconstruction via Physics Simulation for Scene Update](https://arxiv.org/abs/2507.11069)
*Jeongyun Kim,Seunghoon Jeong,Giseop Kim,Myung-Hwan Jeon,Eunji Jun,Ayoung Kim*

Main category: cs.RO

TL;DR: TRAN-D是一种基于2D高斯泼溅的透明物体深度重建方法，通过分离透明物体与背景并优化高斯分布，显著提升了稀疏视图和动态环境下的3D几何重建效果。


<details>
  <summary>Details</summary>
Motivation: 透明物体的3D几何重建因反射和折射等物理特性而具有挑战性，尤其是在稀疏视图和动态环境中。

Method: TRAN-D通过分离透明物体与背景，优化对应的高斯分布，并引入物体感知损失和物理模拟，减少伪影并提升重建效率。

Result: 在合成和真实世界序列中，TRAN-D表现优于现有方法，合成数据的平均绝对误差降低39%，单图像更新的精度显著提升。

Conclusion: TRAN-D在透明物体重建中表现出高效性和鲁棒性，为稀疏视图和动态环境提供了可行的解决方案。

Abstract: Understanding the 3D geometry of transparent objects from RGB images is
challenging due to their inherent physical properties, such as reflection and
refraction. To address these difficulties, especially in scenarios with sparse
views and dynamic environments, we introduce TRAN-D, a novel 2D Gaussian
Splatting-based depth reconstruction method for transparent objects. Our key
insight lies in separating transparent objects from the background, enabling
focused optimization of Gaussians corresponding to the object. We mitigate
artifacts with an object-aware loss that places Gaussians in obscured regions,
ensuring coverage of invisible surfaces while reducing overfitting.
Furthermore, we incorporate a physics-based simulation that refines the
reconstruction in just a few seconds, effectively handling object removal and
chain-reaction movement of remaining objects without the need for rescanning.
TRAN-D is evaluated on both synthetic and real-world sequences, and it
consistently demonstrated robust improvements over existing GS-based
state-of-the-art methods. In comparison with baselines, TRAN-D reduces the mean
absolute error by over 39% for the synthetic TRansPose sequences. Furthermore,
despite being updated using only one image, TRAN-D reaches a {\delta} < 2.5 cm
accuracy of 48.46%, over 1.5 times that of baselines, which uses six images.
Code and more results are available at https://jeongyun0609.github.io/TRAN-D/.

</details>


### [253] [Closed Form Time Derivatives of the Equations of Motion of Rigid Body Systems](https://arxiv.org/abs/2507.11076)
*Andreas Mueller,Shivesh Kumar*

Main category: cs.RO

TL;DR: 本文提出了刚性体系统运动方程（EOM）的二阶时间导数闭式解，替代现有递归算法，为机器人控制提供直接结构洞察。


<details>
  <summary>Details</summary>
Motivation: 机器人控制需要平滑轨迹及控制力/力矩的时间导数，尤其是含弹性组件的多体系统。现有递归算法缺乏直观性。

Method: 采用李群理论，推导出紧凑且易参数化的EOM二阶时间导数闭式解。

Result: 提供了EOM二阶时间导数的闭式表达式，结构清晰且计算高效。

Conclusion: 该方法为机器人系统设计与控制提供了更直观且高效的数学工具。

Abstract: Derivatives of equations of motion(EOM) describing the dynamics of rigid body
systems are becoming increasingly relevant for the robotics community and find
many applications in design and control of robotic systems. Controlling robots,
and multibody systems comprising elastic components in particular, not only
requires smooth trajectories but also the time derivatives of the control
forces/torques, hence of the EOM. This paper presents the time derivatives of
the EOM in closed form up to second-order as an alternative formulation to the
existing recursive algorithms for this purpose, which provides a direct insight
into the structure of the derivatives. The Lie group formulation for rigid body
systems is used giving rise to very compact and easily parameterized equations.

</details>


### [254] [Force-Based Viscosity and Elasticity Measurements for Material Biomechanical Characterisation with a Collaborative Robotic Arm](https://arxiv.org/abs/2507.11133)
*Luca Beber,Edoardo Lamon,Giacomo Moretti,Matteo Saveriano,Luca Fambri,Luigi Palopoli,Daniele Fontanelli*

Main category: cs.RO

TL;DR: 论文评估了机器人系统在估计材料粘弹性参数方面的准确性，初步验证了其在生物样本中的应用潜力。


<details>
  <summary>Details</summary>
Motivation: 诊断活动（如超声扫描和触诊）易出错且依赖高技能医疗人员，机器人解决方案可减少结果的主观性和等待时间。

Method: 通过机器人系统测量不同材料（包括离体组织）的粘弹性参数，并与高精度仪器测量的硅胶标本数据对比。

Result: 机器人系统的准确性接近地面真实值，支持其在临床应用中的潜力。

Conclusion: 机器人系统在估计组织生物力学特性方面具有高准确性，有望用于临床诊断。

Abstract: Diagnostic activities, such as ultrasound scans and palpation, are relatively
low-cost. They play a crucial role in the early detection of health problems
and in assessing their progression. However, they are also error-prone
activities, which require highly skilled medical staff. The use of robotic
solutions can be key to decreasing the inherent subjectivity of the results and
reducing the waiting list. For a robot to perform palpation or ultrasound
scans, it must effectively manage physical interactions with the human body,
which greatly benefits from precise estimation of the patient's tissue
biomechanical properties. This paper assesses the accuracy and precision of a
robotic system in estimating the viscoelastic parameters of various materials,
including some tests on ex vivo tissues as a preliminary proof-of-concept
demonstration of the method's applicability to biological samples. The
measurements are compared against a ground truth derived from silicone
specimens with different viscoelastic properties, characterised using a
high-precision instrument. Experimental results show that the robotic system's
accuracy closely matches the ground truth, increasing confidence in the
potential use of robots for such clinical applications.

</details>


### [255] [A Robust Controller based on Gaussian Processes for Robotic Manipulators with Unknown Uncertainty](https://arxiv.org/abs/2507.11170)
*Giulio Giacomuzzo,Mohamed Abdelwahab,Marco Calì,Alberto Dalla Libera,Ruggero Carli*

Main category: cs.RO

TL;DR: 提出了一种基于学习的鲁棒反馈线性化策略，用于拉格朗日系统的精确轨迹跟踪。


<details>
  <summary>Details</summary>
Motivation: 解决拉格朗日系统在模型不匹配情况下的精确轨迹跟踪问题，无需先验模型误差界限。

Method: 采用高斯过程回归（GPR）估计模型不匹配，结合经典反馈线性化，并通过鲁棒化控制器补偿剩余不确定性。

Result: 理论证明高概率下能保证渐近跟踪，并在2自由度平面机器人上进行了数值验证。

Conclusion: 该方法有效解决了模型不匹配问题，实现了精确轨迹跟踪。

Abstract: In this paper, we propose a novel learning-based robust feedback
linearization strategy to ensure precise trajectory tracking for an important
family of Lagrangian systems. We assume a nominal knowledge of the dynamics is
given but no a-priori bounds on the model mismatch are available. In our
approach, the key ingredient is the adoption of a regression framework based on
Gaussian Processes (GPR) to estimate the model mismatch. This estimate is added
to the outer loop of a classical feedback linearization scheme based on the
nominal knowledge available. Then, to compensate for the residual uncertainty,
we robustify the controller including an additional term whose size is designed
based on the variance provided by the GPR framework. We proved that, with high
probability, the proposed scheme is able to guarantee asymptotic tracking of a
desired trajectory. We tested numerically our strategy on a 2 degrees of
freedom planar robot.

</details>


### [256] [MPC-based Coarse-to-Fine Motion Planning for Robotic Object Transportation in Cluttered Environments](https://arxiv.org/abs/2507.11211)
*Chen Cai,Ernesto Dickel Saraiva,Ya-jun Pan,Steven Liu*

Main category: cs.RO

TL;DR: 提出了一种新颖的从粗到细的运动规划框架，用于机器人在杂乱、未建模环境中的操作。


<details>
  <summary>Details</summary>
Motivation: 解决机器人在杂乱、未建模环境中运动规划的挑战，特别是在部分和不确定观测条件下。

Method: 结合双摄像头感知系统和基于B样条的模型预测控制（MPC）方案，逐步优化环境模型和运动规划。

Result: 实验验证了该框架在不确定性和杂乱环境中的鲁棒性和适应性。

Conclusion: 该框架能够有效支持动态重新规划和闭环运动学，适用于复杂环境中的机器人操作。

Abstract: This letter presents a novel coarse-to-fine motion planning framework for
robotic manipulation in cluttered, unmodeled environments. The system
integrates a dual-camera perception setup with a B-spline-based model
predictive control (MPC) scheme. Initially, the planner generates feasible
global trajectories from partial and uncertain observations. As new visual data
are incrementally fused, both the environment model and motion planning are
progressively refined. A vision-based cost function promotes target-driven
exploration, while a refined kernel-perceptron collision detector enables
efficient constraint updates for real-time planning. The framework accommodates
closed-chain kinematics and supports dynamic replanning. Experiments on a
multi-arm platform validate its robustness and adaptability under uncertainties
and clutter.

</details>


### [257] [Comparison of Localization Algorithms between Reduced-Scale and Real-Sized Vehicles Using Visual and Inertial Sensors](https://arxiv.org/abs/2507.11241)
*Tobias Kern,Leon Tolksdorf,Christian Birkner*

Main category: cs.RO

TL;DR: 研究探讨了物理缩比车辆对视觉和视觉-惯性自定位算法精度的影响，发现OpenVINS在缩比和真实尺寸车辆中表现最佳。


<details>
  <summary>Details</summary>
Motivation: 加速先进自动驾驶功能的开发，需验证缩比车辆作为测试平台的可行性。

Method: 选择ROS2兼容的视觉和视觉-惯性算法（OpenVINS、VINS-Fusion、RTAB-Map），对比其在缩比与真实尺寸车辆数据集上的定位精度。

Result: OpenVINS平均定位误差最低，缩比与真实尺寸车辆在旋转运动估计上无显著差异。

Conclusion: 缩比车辆可作为自定位算法的有效测试平台。

Abstract: Physically reduced-scale vehicles are emerging to accelerate the development
of advanced automated driving functions. In this paper, we investigate the
effects of scaling on self-localization accuracy with visual and
visual-inertial algorithms using cameras and an inertial measurement unit
(IMU). For this purpose, ROS2-compatible visual and visual-inertial algorithms
are selected, and datasets are chosen as a baseline for real-sized vehicles. A
test drive is conducted to record data of reduced-scale vehicles. We compare
the selected localization algorithms, OpenVINS, VINS-Fusion, and RTAB-Map, in
terms of their pose accuracy against the ground-truth and against data from
real-sized vehicles. When comparing the implementation of the selected
localization algorithms to real-sized vehicles, OpenVINS has the lowest average
localization error. Although all selected localization algorithms have
overlapping error ranges, OpenVINS also performs best when applied to a
reduced-scale vehicle. When reduced-scale vehicles were compared to real-sized
vehicles, minor differences were found in translational vehicle motion
estimation accuracy. However, no significant differences were found when
comparing the estimation accuracy of rotational vehicle motion, allowing RSVRs
to be used as testing platforms for self-localization algorithms.

</details>


### [258] [Development of an Autonomous Mobile Robotic System for Efficient and Precise Disinfection](https://arxiv.org/abs/2507.11270)
*Ting-Wei Ou,Jia-Hao Jiang,Guan-Lin Huang,Kuu-Young Young*

Main category: cs.RO

TL;DR: 提出了一种针对病毒热点区域的移动机器人紫外线消毒系统，显著减少消毒时间并提高效率。


<details>
  <summary>Details</summary>
Motivation: COVID-19疫情凸显了医院自动化消毒的紧迫性，尤其是紫外线消毒的高效性，但现有研究忽视了人类活动对病毒分布的影响。

Method: 开发了一种移动机器人系统，优先消毒高风险区域，并优化紫外线剂量，确保所有表面得到足够照射。

Result: 在两个医院场景中，消毒效果相同，但消毒时间分别减少了30.7%和31.9%。

Conclusion: 该系统显著提升了消毒效率，同时减少了低风险区域的不必要照射。

Abstract: The COVID-19 pandemic has severely affected public health, healthcare
systems, and daily life, especially amid resource shortages and limited
workers. This crisis has underscored the urgent need for automation in hospital
environments, particularly disinfection, which is crucial to controlling virus
transmission and improving the safety of healthcare personnel and patients.
Ultraviolet (UV) light disinfection, known for its high efficiency, has been
widely adopted in hospital settings. However, most existing research focuses on
maximizing UV coverage while paying little attention to the impact of human
activity on virus distribution. To address this issue, we propose a mobile
robotic system for UV disinfection focusing on the virus hotspot. The system
prioritizes disinfection in high-risk areas and employs an approach for
optimized UV dosage to ensure that all surfaces receive an adequate level of UV
exposure while significantly reducing disinfection time. It not only improves
disinfection efficiency but also minimizes unnecessary exposure in low-risk
areas. In two representative hospital scenarios, our method achieves the same
disinfection effectiveness while reducing disinfection time by 30.7% and 31.9%,
respectively. The video of the experiment is available at:
https://youtu.be/wHcWzOcoMPM.

</details>


### [259] [Ocean Diviner: A Diffusion-Augmented Reinforcement Learning for AUV Robust Control in the Underwater Tasks](https://arxiv.org/abs/2507.11283)
*Weiyi Liu,Jingzehua Xu,Guanwen Xie,Yi Li*

Main category: cs.RO

TL;DR: 本文提出了一种扩散增强的强化学习方法，用于自主水下车辆（AUV）的鲁棒控制，解决了水下轨迹规划和动态环境适应的关键挑战。


<details>
  <summary>Details</summary>
Motivation: 水下环境复杂多变，传统控制方法难以应对动态变化和长时程规划的需求，因此需要一种更鲁棒和灵活的方法。

Method: 结合扩散模型和强化学习，通过扩散U-Net架构生成多步轨迹，并通过混合学习架构优化策略，提高探索效率和策略稳定性。

Result: 仿真实验表明，该方法在复杂海洋环境中优于传统控制方法，具有更高的适应性和可靠性。

Conclusion: 该方法为AUV在水下任务中提供了更鲁棒和灵活的解决方案。

Abstract: This paper presents a diffusion-augmented reinforcement learning (RL)
approach for robust autonomous underwater vehicle (AUV) control, addressing key
challenges in underwater trajectory planning and dynamic environment
adaptation. The proposed method integrates three core innovations: (1) A
diffusion-based trajectory generation framework that produces physically
feasible multi-step trajectories, enhanced by a high-dimensional state encoding
mechanism combining current observations with historical states and actions
through a novel diffusion U-Net architecture, significantly improving
long-horizon planning. (2) A sample-efficient hybrid learning architecture that
synergizes diffusion-guided exploration with RL policy optimization, where the
diffusion model generates diverse candidate actions and the RL critic selects
optimal actions, achieving higher exploration efficiency and policy stability
in dynamic underwater environments. Extensive simulation experiments validating
the method's superior robustness and flexibility, outperforms conventional
control methods in challenging marine conditions, offering enhanced
adaptability and reliability for AUV operations in the underwater tasks.

</details>


### [260] [Diffusion-Based Imaginative Coordination for Bimanual Manipulation](https://arxiv.org/abs/2507.11296)
*Huilin Xu,Jian Ding,Jiakun Xu,Ruixiang Wang,Jun Chen,Jinjie Mai,Yanwei Fu,Bernard Ghanem,Feng Xu,Mohamed Elhoseiny*

Main category: cs.RO

TL;DR: 提出了一种基于扩散的统一框架，联合优化视频和动作预测，显著提升双手机器人操作的成功率。


<details>
  <summary>Details</summary>
Motivation: 双手机器人操作在工业自动化和家庭服务中至关重要，但高维动作空间和复杂协调需求带来挑战。视频预测的潜力在提升双手机器人协调方面尚未充分探索。

Method: 采用多帧潜在预测策略压缩未来状态，并引入单向注意力机制，视频预测依赖于动作，而动作预测独立于视频预测，提高推理效率。

Result: 在模拟基准和真实实验中，成功率显著提升，ALOHA提高24.9%，RoboTwin提高11.1%，真实实验提高32.5%。

Conclusion: 提出的方法有效提升了双手机器人操作的协调性和成功率，代码和模型已开源。

Abstract: Bimanual manipulation is crucial in robotics, enabling complex tasks in
industrial automation and household services. However, it poses significant
challenges due to the high-dimensional action space and intricate coordination
requirements. While video prediction has been recently studied for
representation learning and control, leveraging its ability to capture rich
dynamic and behavioral information, its potential for enhancing bimanual
coordination remains underexplored. To bridge this gap, we propose a unified
diffusion-based framework for the joint optimization of video and action
prediction. Specifically, we propose a multi-frame latent prediction strategy
that encodes future states in a compressed latent space, preserving
task-relevant features. Furthermore, we introduce a unidirectional attention
mechanism where video prediction is conditioned on the action, while action
prediction remains independent of video prediction. This design allows us to
omit video prediction during inference, significantly enhancing efficiency.
Experiments on two simulated benchmarks and a real-world setting demonstrate a
significant improvement in the success rate over the strong baseline ACT using
our method, achieving a \textbf{24.9\%} increase on ALOHA, an \textbf{11.1\%}
increase on RoboTwin, and a \textbf{32.5\%} increase in real-world experiments.
Our models and code are publicly available at
https://github.com/return-sleep/Diffusion_based_imaginative_Coordination.

</details>


### [261] [All Eyes, no IMU: Learning Flight Attitude from Vision Alone](https://arxiv.org/abs/2507.11302)
*Jesse J. Hagenaars,Stein Stroobants,Sander M. Bohte,Guido C. H. E. De Croon*

Main category: cs.RO

TL;DR: 首次提出仅依赖视觉的飞行控制方法，使用事件相机和神经网络替代传统惯性传感器，实现无人机稳定飞行。


<details>
  <summary>Details</summary>
Motivation: 飞行机器人通常依赖惯性传感器，而许多飞行生物仅依赖视觉。研究旨在探索仅视觉的飞行控制在通用环境中的可行性。

Method: 使用下视事件相机和循环卷积神经网络，通过监督学习训练，估计无人机姿态和旋转速率。

Result: 实验证明该方法可替代传统惯性测量单元，且不同视野和记忆配置影响性能与泛化能力。

Conclusion: 视觉飞行控制是昆虫级飞行机器人自主化的有潜力方案。

Abstract: Vision is an essential part of attitude control for many flying animals, some
of which have no dedicated sense of gravity. Flying robots, on the other hand,
typically depend heavily on accelerometers and gyroscopes for attitude
stabilization. In this work, we present the first vision-only approach to
flight control for use in generic environments. We show that a quadrotor drone
equipped with a downward-facing event camera can estimate its attitude and
rotation rate from just the event stream, enabling flight control without
inertial sensors. Our approach uses a small recurrent convolutional neural
network trained through supervised learning. Real-world flight tests
demonstrate that our combination of event camera and low-latency neural network
is capable of replacing the inertial measurement unit in a traditional flight
control loop. Furthermore, we investigate the network's generalization across
different environments, and the impact of memory and different fields of view.
While networks with memory and access to horizon-like visual cues achieve best
performance, variants with a narrower field of view achieve better relative
generalization. Our work showcases vision-only flight control as a promising
candidate for enabling autonomous, insect-scale flying robots.

</details>


### [262] [Acting and Planning with Hierarchical Operational Models on a Mobile Robot: A Study with RAE+UPOM](https://arxiv.org/abs/2507.11345)
*Oscar Lima,Marc Vinci,Sunandita Patra,Sebastian Stock,Joachim Hertzberg,Martin Atzmueller,Malik Ghallab,Dana Nau,Paolo Traverso*

Main category: cs.RO

TL;DR: 论文提出了一种结合层次化操作模型的执行与规划系统（RAE+UPOM），并在真实机器人上部署，展示了其在任务执行中的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 解决符号规划模型与实际机器人控制结构之间的不一致性问题。

Method: 结合Reactive Acting Engine（RAE）与UCT-like Monte Carlo规划器（UPOM），共享层次化操作模型。

Result: 实验表明系统在动作失败和传感器噪声下仍能鲁棒执行任务。

Conclusion: RAE+UPOM系统为执行与规划的交互决策提供了实证支持。

Abstract: Robotic task execution faces challenges due to the inconsistency between
symbolic planner models and the rich control structures actually running on the
robot. In this paper, we present the first physical deployment of an integrated
actor-planner system that shares hierarchical operational models for both
acting and planning, interleaving the Reactive Acting Engine (RAE) with an
anytime UCT-like Monte Carlo planner (UPOM). We implement RAE+UPOM on a mobile
manipulator in a real-world deployment for an object collection task. Our
experiments demonstrate robust task execution under action failures and sensor
noise, and provide empirical insights into the interleaved acting-and-planning
decision making process.

</details>


### [263] [From Production Logistics to Smart Manufacturing: The Vision for a New RoboCup Industrial League](https://arxiv.org/abs/2507.11402)
*Supun Dissanayaka,Alexander Ferrein,Till Hofmann,Kosuke Nakajima,Mario Sanz-Lopez,Jesus Savage,Daniel Swoboda,Matteo Tschesche,Wataru Uemura,Tarik Viehmann,Shohei Yasuda*

Main category: cs.RO

TL;DR: 本文提出RoboCup智能制造联盟的新愿景，旨在扩展原有物流联盟的范围，涵盖现代工厂的更多方面，以提升竞赛的吸引力和相关性。


<details>
  <summary>Details</summary>
Motivation: 原有RoboCup物流联盟专注于生产物流，但未能反映智能制造的近期发展，导致其相关性下降。

Method: 设计新的智能制造联盟，包含多个独立但逐步整合的赛道，涵盖工业机器人挑战如装配、人机协作和人形机器人，同时保留生产物流重点。

Result: 预期新竞赛将更吸引新老团队，并将焦点转向工业机器人的当前和未来挑战。

Conclusion: 通过扩展竞赛范围和内容，新联盟有望提升RoboCup在智能制造领域的相关性和吸引力。

Abstract: The RoboCup Logistics League is a RoboCup competition in a smart factory
scenario that has focused on task planning, job scheduling, and multi-agent
coordination. The focus on production logistics allowed teams to develop highly
competitive strategies, but also meant that some recent developments in the
context of smart manufacturing are not reflected in the competition, weakening
its relevance over the years. In this paper, we describe the vision for the
RoboCup Smart Manufacturing League, a new competition designed as a larger
smart manufacturing scenario, reflecting all the major aspects of a modern
factory. It will consist of several tracks that are initially independent but
gradually combined into one smart manufacturing scenario. The new tracks will
cover industrial robotics challenges such as assembly, human-robot
collaboration, and humanoid robotics, but also retain a focus on production
logistics. We expect the reenvisioned competition to be more attractive to
newcomers and well-tried teams, while also shifting the focus to current and
future challenges of industrial robotics.

</details>


### [264] [Multi-IMU Sensor Fusion for Legged Robots](https://arxiv.org/abs/2507.11447)
*Shuo Yang,John Z. Zhang,Ibrahima Sory Sow,Zachary Manchester*

Main category: cs.RO

TL;DR: 提出了一种用于腿式机器人的状态估计方法，通过多传感器融合实现低漂移的位姿和速度估计。


<details>
  <summary>Details</summary>
Motivation: 解决腿式机器人在复杂运动条件下标准本体感知里程计的主要误差源问题。

Method: 利用多个IMU和关节编码器数据，结合扩展卡尔曼滤波和因子图滑动窗口估计器，形成视觉-惯性-腿里程计方法。

Result: 在多种挑战性运动任务中，算法表现出最小的位置偏差。

Conclusion: 该方法在复杂运动条件下表现优异，提供了开源实现和数据集。

Abstract: This paper presents a state-estimation solution for legged robots that uses a
set of low-cost, compact, and lightweight sensors to achieve low-drift pose and
velocity estimation under challenging locomotion conditions. The key idea is to
leverage multiple inertial measurement units on different links of the robot to
correct a major error source in standard proprioceptive odometry. We fuse the
inertial sensor information and joint encoder measurements in an extended
Kalman filter, then combine the velocity estimate from this filter with camera
data in a factor-graph-based sliding-window estimator to form a
visual-inertial-leg odometry method. We validate our state estimator through
comprehensive theoretical analysis and hardware experiments performed using
real-world robot data collected during a variety of challenging locomotion
tasks. Our algorithm consistently achieves minimal position deviation, even in
scenarios involving substantial ground impact, foot slippage, and sudden body
rotations. A C++ implementation, along with a large-scale dataset, is available
at https://github.com/ShuoYangRobotics/Cerberus2.0.

</details>


### [265] [Human-Robot collaboration in surgery: Advances and challenges towards autonomous surgical assistants](https://arxiv.org/abs/2507.11460)
*Jacinto Colan,Ana Davila,Yutaro Yamada,Yasuhisa Hasegawa*

Main category: cs.RO

TL;DR: 本文系统综述了自主手术机器人助手（ASARs）的研究进展与挑战，重点分析了其在手术中为人类外科医生提供主动支持的应用场景。


<details>
  <summary>Details</summary>
Motivation: 推动自主机器人系统在复杂手术中辅助外科医生的能力，提升手术效率和安全性。

Method: 遵循PRISMA指南，对IEEE Xplore、Scopus和Web of Science数据库中的32项研究进行了详细分析，识别了两种主要协作模式：远程操作辅助和直接交互。

Result: 研究发现ASARs研究日益增多，主要应用于内窥镜引导，同时在自主工具操作方面取得进展。关键挑战包括机器人行为与外科医生偏好的匹配、自主系统的程序意识、人机信息交换的顺畅性以及共享工作空间中的技能获取。

Conclusion: 综述总结了当前趋势、关键限制，并提出了未来研究方向，以提高手术中人机协作的可靠性、安全性和有效性。

Abstract: Human-robot collaboration in surgery represents a significant area of
research, driven by the increasing capability of autonomous robotic systems to
assist surgeons in complex procedures. This systematic review examines the
advancements and persistent challenges in the development of autonomous
surgical robotic assistants (ASARs), focusing specifically on scenarios where
robots provide meaningful and active support to human surgeons. Adhering to the
PRISMA guidelines, a comprehensive literature search was conducted across the
IEEE Xplore, Scopus, and Web of Science databases, resulting in the selection
of 32 studies for detailed analysis. Two primary collaborative setups were
identified: teleoperation-based assistance and direct hands-on interaction. The
findings reveal a growing research emphasis on ASARs, with predominant
applications currently in endoscope guidance, alongside emerging progress in
autonomous tool manipulation. Several key challenges hinder wider adoption,
including the alignment of robotic actions with human surgeon preferences, the
necessity for procedural awareness within autonomous systems, the establishment
of seamless human-robot information exchange, and the complexities of skill
acquisition in shared workspaces. This review synthesizes current trends,
identifies critical limitations, and outlines future research directions
essential to improve the reliability, safety, and effectiveness of human-robot
collaboration in surgical environments.

</details>


### [266] [Robot Drummer: Learning Rhythmic Skills for Humanoid Drumming](https://arxiv.org/abs/2507.11498)
*Asad Ali Shahid,Francesco Braghin,Loris Roveda*

Main category: cs.RO

TL;DR: 本文介绍了Robot Drummer，一种能够进行高精度、多样化鼓乐表演的人形机器人系统，通过强化学习实现长时程音乐表演。


<details>
  <summary>Details</summary>
Motivation: 探索人形机器人在音乐表演等表达性领域的潜力，解决鼓乐表演中的快速时序、多肢协调等挑战。

Method: 将鼓乐表演建模为时序接触链，将乐曲分解为固定长度片段，并通过强化学习并行训练策略。

Result: 在三十多首流行摇滚、金属和爵士乐曲中，Robot Drummer表现出高F1分数，并展现出类似人类的鼓乐策略。

Conclusion: 强化学习有望将人形机器人引入创造性音乐表演领域。

Abstract: Humanoid robots have seen remarkable advances in dexterity, balance, and
locomotion, yet their role in expressive domains, such as music performance,
remains largely unexplored. Musical tasks, like drumming, present unique
challenges, including split-second timing, rapid contacts, and multi-limb
coordination over pieces lasting minutes. In this paper, we introduce Robot
Drummer, a humanoid system capable of expressive, high-precision drumming
across a diverse repertoire of songs. We formulate humanoid drumming as
sequential fulfillment of timed-contacts and transform drum scores in to a
Rhythmic Contact Chain. To handle the long-horizon nature of musical
performance, we decompose each piece into fixed-length segments and train a
single policy across all segments in parallel using reinforcement learning.
Through extensive experiments on over thirty popular rock, metal, and jazz
tracks, our results demonstrate that Robot Drummer consistently achieves high
F1 scores. The learned behaviors exhibit emergent human-like drumming
strategies, such as cross-arm strikes, and adaptive sticks assignments,
demonstrating the potential of reinforcement learning to bring humanoid robots
into the domain of creative musical performance. Project page:
\href{https://robot-drummer.github.io}{robot-drummer.github.io}

</details>


### [267] [LLM-based ambiguity detection in natural language instructions for collaborative surgical robots](https://arxiv.org/abs/2507.11525)
*Ana Davila,Jacinto Colan,Yasuhisa Hasegawa*

Main category: cs.RO

TL;DR: 提出了一种基于大语言模型（LLMs）的框架，用于检测手术场景中的自然语言指令歧义，以提高人机协作的安全性。


<details>
  <summary>Details</summary>
Motivation: 自然语言指令的歧义在安全关键的人机交互（如手术）中带来风险，需解决以提高可靠性。

Method: 采用多种LLM评估器（包括链式思维评估器）和符合预测方法，综合检测语言、上下文、程序和关键歧义。

Result: 在Llama 3.2 11B和Gemma 3 12B上，分类准确率超过60%，能区分手术指令的歧义性。

Conclusion: 该框架通过提前识别歧义指令，提升了手术中人机协作的安全性和可靠性。

Abstract: Ambiguity in natural language instructions poses significant risks in
safety-critical human-robot interaction, particularly in domains such as
surgery. To address this, we propose a framework that uses Large Language
Models (LLMs) for ambiguity detection specifically designed for collaborative
surgical scenarios. Our method employs an ensemble of LLM evaluators, each
configured with distinct prompting techniques to identify linguistic,
contextual, procedural, and critical ambiguities. A chain-of-thought evaluator
is included to systematically analyze instruction structure for potential
issues. Individual evaluator assessments are synthesized through conformal
prediction, which yields non-conformity scores based on comparison to a labeled
calibration dataset. Evaluating Llama 3.2 11B and Gemma 3 12B, we observed
classification accuracy exceeding 60% in differentiating ambiguous from
unambiguous surgical instructions. Our approach improves the safety and
reliability of human-robot collaboration in surgery by offering a mechanism to
identify potentially ambiguous instructions before robot action.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [268] [Supporting SENĆOTEN Language Documentation Efforts with Automatic Speech Recognition](https://arxiv.org/abs/2507.10827)
*Mengzhe Geng,Patrick Littell,Aidan Pine,PENÁĆ,Marc Tessier,Roland Kuhn*

Main category: cs.SD

TL;DR: 论文提出了一种结合TTS生成数据和跨语言迁移学习的ASR驱动管道，用于支持SENĆOTEN语言的复兴，实验显示WER和CER显著降低。


<details>
  <summary>Details</summary>
Motivation: 支持SENĆOTEN语言的复兴，解决因殖民政策导致的语言流失问题，利用数字技术加速语言文档化和教育资源创建。

Method: 提出ASR驱动管道，结合TTS生成数据和跨语言迁移学习，使用n-gram语言模型优化数据利用。

Result: 测试集上WER为19.34%，CER为5.09%；过滤小错误后WER降至14.32%，CER降至3.45%。

Conclusion: ASR驱动管道在支持SENĆOTEN语言文档化方面具有潜力，为语言复兴提供了有效工具。

Abstract: The SEN\'{C}OTEN language, spoken on the Saanich peninsula of southern
Vancouver Island, is in the midst of vigorous language revitalization efforts
to turn the tide of language loss as a result of colonial language policies. To
support these on-the-ground efforts, the community is turning to digital
technology. Automatic Speech Recognition (ASR) technology holds great promise
for accelerating language documentation and the creation of educational
resources. However, developing ASR systems for SEN\'{C}OTEN is challenging due
to limited data and significant vocabulary variation from its polysynthetic
structure and stress-driven metathesis. To address these challenges, we propose
an ASR-driven documentation pipeline that leverages augmented speech data from
a text-to-speech (TTS) system and cross-lingual transfer learning with Speech
Foundation Models (SFMs). An n-gram language model is also incorporated via
shallow fusion or n-best restoring to maximize the use of available data.
Experiments on the SEN\'{C}OTEN dataset show a word error rate (WER) of 19.34%
and a character error rate (CER) of 5.09% on the test set with a 57.02%
out-of-vocabulary (OOV) rate. After filtering minor cedilla-related errors, WER
improves to 14.32% (26.48% on unseen words) and CER to 3.45%, demonstrating the
potential of our ASR-driven pipeline to support SEN\'{C}OTEN language
documentation.

</details>


### [269] [Pronunciation Deviation Analysis Through Voice Cloning and Acoustic Comparison](https://arxiv.org/abs/2507.10985)
*Andrew Valdivia,Yueming Zhang,Hailu Xu,Amir Ghasemkhani,Xin Qin*

Main category: cs.SD

TL;DR: 提出了一种通过分析用户原始语音与发音纠正后的克隆语音之间的偏差来检测发音错误的新方法。


<details>
  <summary>Details</summary>
Motivation: 传统发音错误检测方法依赖预定义的语音规则或大量训练数据，限制了其适用性。本文旨在开发一种无需这些限制的通用方法。

Method: 利用语音克隆技术生成发音正确的用户语音克隆，通过逐帧比较原始与克隆语音的声学偏差定位发音错误。

Result: 实验证明该方法能有效识别特定发音错误，无需依赖预定义规则或大量语言特定数据。

Conclusion: 该方法为发音错误检测提供了一种通用且高效的解决方案。

Abstract: This paper presents a novel approach for detecting mispronunciations by
analyzing deviations between a user's original speech and their voice-cloned
counterpart with corrected pronunciation. We hypothesize that regions with
maximal acoustic deviation between the original and cloned utterances indicate
potential mispronunciations. Our method leverages recent advances in voice
cloning to generate a synthetic version of the user's voice with proper
pronunciation, then performs frame-by-frame comparisons to identify problematic
segments. Experimental results demonstrate the effectiveness of this approach
in pinpointing specific pronunciation errors without requiring predefined
phonetic rules or extensive training data for each target language.

</details>


### [270] [EditGen: Harnessing Cross-Attention Control for Instruction-Based Auto-Regressive Audio Editing](https://arxiv.org/abs/2507.11096)
*Vassilis Sioros,Alexandros Potamianos,Giorgos Paraskevopoulos*

Main category: cs.SD

TL;DR: 该论文提出了一种利用交叉注意力控制的自回归模型音频编辑方法，结合扩散策略和预训练模型MUSICGEN，通过三种编辑机制提升音频编辑效果。


<details>
  <summary>Details</summary>
Motivation: 受图像编辑方法的启发，研究旨在通过交叉和自注意力机制实现高效的音频编辑，并建立提示引导的音频编辑基准。

Method: 开发了类似Prompt-to-Prompt的方法，结合扩散策略和MUSICGEN模型，提出替换、重加权和细化三种注意力分数编辑机制。

Result: 自动和人工评估表明，该方法在旋律、动态和节奏方面显著优于基于扩散的基准方法。

Conclusion: 结合提示引导和自回归生成模型的方法在音频编辑中表现出色，为未来研究提供了新方向。

Abstract: In this study, we investigate leveraging cross-attention control for
efficient audio editing within auto-regressive models. Inspired by image
editing methodologies, we develop a Prompt-to-Prompt-like approach that guides
edits through cross and self-attention mechanisms. Integrating a
diffusion-based strategy, influenced by Auffusion, we extend the model's
functionality to support refinement edits, establishing a baseline for
prompt-guided audio editing. Additionally, we introduce an alternative approach
by incorporating MUSICGEN, a pre-trained frozen auto-regressive model, and
propose three editing mechanisms, based on Replacement, Reweighting, and
Refinement of the attention scores. We employ commonly-used music-specific
evaluation metrics and a human study, to gauge time-varying controllability,
adherence to global text cues, and overall audio realism. The automatic and
human evaluations indicate that the proposed combination of prompt-to-prompt
guidance with autoregressive generation models significantly outperforms the
diffusion-based baseline in terms of melody, dynamics, and tempo of the
generated audio. Our code is available at https://github.com/billsioros/EditGen

</details>


### [271] [Improving Neural Pitch Estimation with SWIPE Kernels](https://arxiv.org/abs/2507.11233)
*David Marttila,Joshua D. Reiss*

Main category: cs.SD

TL;DR: 论文研究了Sawtooth-Inspired Pitch Estimation (SWIPE)核作为音频前端对神经网络音高估计的改进效果，发现其能提升准确性、抗噪性和参数效率。


<details>
  <summary>Details</summary>
Motivation: 尽管神经网络在音高和周期性估计中占主导地位，但大多数方法直接处理原始音频或通用时频表示，缺乏任务特定性。

Method: 使用SWIPE核作为音频前端，评估监督和自监督的先进架构，并比较网络大小和性能。

Result: SWIPE前端可将网络规模缩小一个数量级而不损失性能，且SWIPE算法本身比自监督神经音高估计器更准确。

Conclusion: 任务特定的SWIPE前端能显著提升神经音高估计器的效率和性能。

Abstract: Neural networks have become the dominant technique for accurate pitch and
periodicity estimation. Although a lot of research has gone into improving
network architectures and training paradigms, most approaches operate directly
on the raw audio waveform or on general-purpose time-frequency representations.
We investigate the use of Sawtooth-Inspired Pitch Estimation (SWIPE) kernels as
an audio frontend and find that these hand-crafted, task-specific features can
make neural pitch estimators more accurate, robust to noise, and more
parameter-efficient. We evaluate supervised and self-supervised
state-of-the-art architectures on common datasets and show that the SWIPE audio
frontend allows for reducing the network size by an order of magnitude without
performance degradation. Additionally, we show that the SWIPE algorithm on its
own is much more accurate than commonly reported, outperforming
state-of-the-art self-supervised neural pitch estimators.

</details>


### [272] [FasTUSS: Faster Task-Aware Unified Source Separation](https://arxiv.org/abs/2507.11435)
*Francesco Paissan,Gordon Wichern,Yoshiki Masuyama,Ryo Aihara,François G. Germain,Kohei Saijo,Jonathan Le Roux*

Main category: cs.SD

TL;DR: 论文分析了TF双路径模型TUSS的设计选择，提出了两种更高效的模型FasTUSS-8.3G和FasTUSS-11.7G，显著减少了计算量，同时性能下降较小。


<details>
  <summary>Details</summary>
Motivation: 解决TF双路径模型计算复杂度高的问题，优化性能与复杂度的权衡。

Method: 通过分析TUSS的设计选择，提出两种更高效的模型FasTUSS-8.3G和FasTUSS-11.7G，并研究提示条件对因果TUSS模型的影响。

Result: FasTUSS-8.3G和FasTUSS-11.7G分别减少了81%和73%的计算量，性能仅下降1.2dB和0.4dB。

Conclusion: 提出的高效模型在保持性能的同时显著降低了计算复杂度，为音频源分离任务提供了更实用的解决方案。

Abstract: Time-Frequency (TF) dual-path models are currently among the best performing
audio source separation network architectures, achieving state-of-the-art
performance in speech enhancement, music source separation, and cinematic audio
source separation. While they are characterized by a relatively low parameter
count, they still require a considerable number of operations, implying a
higher execution time. This problem is exacerbated by the trend towards bigger
models trained on large amounts of data to solve more general tasks, such as
the recently introduced task-aware unified source separation (TUSS) model.
TUSS, which aims to solve audio source separation tasks using a single,
conditional model, is built upon TF-Locoformer, a TF dual-path model combining
convolution and attention layers. The task definition comes in the form of a
sequence of prompts that specify the number and type of sources to be
extracted. In this paper, we analyze the design choices of TUSS with the goal
of optimizing its performance-complexity trade-off. We derive two more
efficient models, FasTUSS-8.3G and FasTUSS-11.7G that reduce the original
model's operations by 81\% and 73\% with minor performance drops of 1.2~dB and
0.4~dB averaged over all benchmarks, respectively. Additionally, we investigate
the impact of prompt conditioning to derive a causal TUSS model.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [273] [Overview of the TREC 2022 deep learning track](https://arxiv.org/abs/2507.10865)
*Nick Craswell,Bhaskar Mitra,Emine Yilmaz,Daniel Campos,Jimmy Lin,Ellen M. Voorhees,Ian Soboroff*

Main category: cs.IR

TL;DR: TREC Deep Learning track第四年，利用MS MARCO数据集扩展了训练标签和集合规模，重点构建更完整的测试集合。深度神经排序模型仍优于传统方法，但今年密集检索表现不如预期。


<details>
  <summary>Details</summary>
Motivation: 构建更完整的测试集合以提升数据集质量，并评估深度神经排序模型与传统方法的性能差异。

Method: 利用扩展的MS MARCO数据集，重点优化通道检索任务，文档排序任务通过通道标签推断完成。

Result: 深度神经排序模型仍优于传统方法，但密集检索表现不如去年。

Conclusion: 数据集质量提升，但密集检索方法的竞争力下降，未来需进一步研究。

Abstract: This is the fourth year of the TREC Deep Learning track. As in previous
years, we leverage the MS MARCO datasets that made hundreds of thousands of
human annotated training labels available for both passage and document ranking
tasks. In addition, this year we also leverage both the refreshed passage and
document collections that were released last year leading to a nearly $16$
times increase in the size of the passage collection and nearly four times
increase in the document collection size. Unlike previous years, in 2022 we
mainly focused on constructing a more complete test collection for the passage
retrieval task, which has been the primary focus of the track. The document
ranking task was kept as a secondary task, where document-level labels were
inferred from the passage-level labels. Our analysis shows that similar to
previous years, deep neural ranking models that employ large scale pretraining
continued to outperform traditional retrieval methods. Due to the focusing our
judging resources on passage judging, we are more confident in the quality of
this year's queries and judgments, with respect to our ability to distinguish
between runs and reuse the dataset in future. We also see some surprises in
overall outcomes. Some top-performing runs did not do dense retrieval. Runs
that did single-stage dense retrieval were not as competitive this year as they
were last year.

</details>


### [274] [Extracting Document Relations from Search Corpus by Marginalizing over User Queries](https://arxiv.org/abs/2507.10726)
*Yuki Iwamoto,Kaoru Tsunoda,Ken Kaneiwa*

Main category: cs.IR

TL;DR: EDR-MQ通过查询边缘化发现文档关系，无需标注数据或预定义分类法，利用MC-RAG技术实现多条件检索，实验证明其能识别传统方法难以发现的关系。


<details>
  <summary>Details</summary>
Motivation: 大规模语料库中文档关系的理解对知识发现和信息组织至关重要，但现有方法依赖人工标注或预定义分类法。

Method: 提出EDR-MQ框架，通过查询边缘化估计文档对的联合概率，并开发MC-RAG技术实现多条件检索。

Result: 实验显示EDR-MQ能识别主题集群、证据链和跨领域连接，优于传统相似性方法。

Conclusion: EDR-MQ提供了一种适应不同用户视角的文档组织方法，无需标注或预定义分类法。

Abstract: Understanding relationships between documents in large-scale corpora is
essential for knowledge discovery and information organization. However,
existing approaches rely heavily on manual annotation or predefined
relationship taxonomies. We propose EDR-MQ (Extracting Document Relations by
Marginalizing over User Queries), a novel framework that discovers document
relationships through query marginalization. EDR-MQ is based on the insight
that strongly related documents often co-occur in results across diverse user
queries, enabling us to estimate joint probabilities between document pairs by
marginalizing over a collection of queries. To enable this query
marginalization approach, we develop Multiply Conditioned Retrieval-Augmented
Generation (MC-RAG), which employs conditional retrieval where subsequent
document retrievals depend on previously retrieved content. By observing
co-occurrence patterns across diverse queries, EDR-MQ estimates joint
probabilities between document pairs without requiring labeled training data or
predefined taxonomies. Experimental results show that our query marginalization
approach successfully identifies meaningful document relationships, revealing
topical clusters, evidence chains, and cross-domain connections that are not
apparent through traditional similarity-based methods. Our query-driven
framework offers a practical approach to document organization that adapts to
different user perspectives and information needs.

</details>


<div id='eess.SY'></div>

# eess.SY [[Back]](#toc)

### [275] [Learning to Quantize and Precode in Massive MIMO Systems for Energy Reduction: a Graph Neural Network Approach](https://arxiv.org/abs/2507.10634)
*Thomas Feys,Liesbet Van der Perre,François Rottenberg*

Main category: eess.SY

TL;DR: 论文研究了在粗量化下行大规模MIMO系统中的非线性预编码问题，提出了一种基于图神经网络（GNN）的自监督学习方法，显著提高了可实现的总速率，同时降低了DAC的功耗。


<details>
  <summary>Details</summary>
Motivation: 随着大规模MIMO系统的发展，DAC在硬件复杂性和功耗方面成为瓶颈，需要解决粗量化下的预编码问题。

Method: 提出了一种基于GNN的非线性预编码方法，通过自监督学习直接最大化可实现速率，并使用Gumbel-softmax梯度估计解决不可微问题。

Result: 在单用户情况下，使用1位DAC即可达到与3位DAC的MRT相同的总速率，DAC功耗显著降低。

Conclusion: 该方法在降低DAC功耗方面效果显著，但增加了数字信号处理的功耗，整体功耗降低适用于特定带宽范围。

Abstract: Massive MIMO systems are moving toward increased numbers of radio frequency
chains, higher carrier frequencies and larger bandwidths. As such,
digital-to-analog converters (DACs) are becoming a bottleneck in terms of
hardware complexity and power consumption. In this work, non-linear precoding
for coarsely quantized downlink massive MIMO is studied. Given the NP-hard
nature of this problem, a graph neural network (GNN) is proposed that directly
outputs the precoded quantized vector based on the channel matrix and the
intended transmit symbols. The model is trained in a self-supervised manner, by
directly maximizing the achievable rate. To overcome the non-differentiability
of the objective function, introduced due to the non-differentiable DAC
functions, a straight-through Gumbel-softmax estimation of the gradient is
proposed. The proposed method achieves a significant increase in achievable sum
rate under coarse quantization. For instance, in the single-user case, the
proposed method can achieve the same sum rate as maximum ratio transmission
(MRT) by using one-bit DAC's as compared to 3 bits for MRT. This reduces the
DAC's power consumption by a factor 4-7 and 3 for baseband and RF DACs
respectively. This, however, comes at the cost of increased digital signal
processing power consumption. When accounting for this, the reduction in
overall power consumption holds for a system bandwidth up to 3.5 MHz for
baseband DACs, while the RF DACs can maintain a power reduction of 2.9 for
higher bandwidths. Notably, indirect effects, which further reduce the power
consumption, such as a reduced fronthaul consumption and reduction in other
components, are not considered in this analysis.

</details>


### [276] [Standards-Compliant DM-RS Allocation via Temporal Channel Prediction for Massive MIMO Systems](https://arxiv.org/abs/2507.11064)
*Sehyun Ryu,Hyun Jong Yang*

Main category: eess.SY

TL;DR: 论文提出了一种基于信道预测的参考信号分配方法（CPRS），通过联合优化信道预测和DM-RS分配，无需CSI反馈即可提高数据吞吐量。


<details>
  <summary>Details</summary>
Motivation: 减少5G网络中反馈开销是关键挑战，现有研究多集中于CSI压缩和预测，但参考信号分配在CSI受限条件下仍未被充分探索。

Method: 提出CPRS方法，结合ViViT/CNN架构，将CSI矩阵视为序列图像数据，优化信道预测和DM-RS分配。

Result: 仿真结果显示，相比基准策略，吞吐量提升高达36.60%。

Conclusion: CPRS方法在动态环境中实现了高效和自适应的传输，为5G网络优化提供了新思路。

Abstract: Reducing feedback overhead in beyond 5G networks is a critical challenge, as
the growing number of antennas in modern massive MIMO systems substantially
increases the channel state information (CSI) feedback demand in frequency
division duplex (FDD) systems. To address this, extensive research has focused
on CSI compression and prediction, with neural network-based approaches gaining
momentum and being considered for integration into the 3GPP 5G-Advanced
standards. While deep learning has been effectively applied to CSI-limited
beamforming and handover optimization, reference signal allocation under such
constraints remains surprisingly underexplored. To fill this gap, we introduce
the concept of channel prediction-based reference signal allocation (CPRS),
which jointly optimizes channel prediction and DM-RS allocation to improve data
throughput without requiring CSI feedback. We further propose a
standards-compliant ViViT/CNN-based architecture that implements CPRS by
treating evolving CSI matrices as sequential image-like data, enabling
efficient and adaptive transmission in dynamic environments. Simulation results
using ray-tracing channel data generated in NVIDIA Sionna validate the proposed
method, showing up to 36.60% throughput improvement over benchmark strategies.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [277] [SQLord: A Robust Enterprise Text-to-SQL Solution via Reverse Data Generation and Workflow Decomposition](https://arxiv.org/abs/2507.10629)
*Song Cheng,Qiannan Cheng,Linbo Jin,Lei Yi,Guannan Zhang*

Main category: cs.DB

TL;DR: SQLord是一个企业级NL2SQL框架，通过数据逆向生成和查询分解方法解决现有框架在复杂业务逻辑和领域数据不足的问题，并引入多维度评估框架。


<details>
  <summary>Details</summary>
Motivation: 现有NL2SQL框架在复杂业务逻辑和领域数据不足时表现不佳，且评估方法依赖标注数据和数据库环境，难以适应实际场景。

Method: SQLord采用数据逆向生成方法生成标注数据，并设计查询分解工作流。同时提出GPT-Judge评估框架，包括EXE、QSE和SSE。

Result: 离线测试显著优于现有基线，在线准确率超过90%，并在全球最大B2B电商平台成功应用。

Conclusion: SQLord在复杂实际场景中表现出色，解决了现有框架的局限性。

Abstract: Transforming natural language into SQL queries (NL2SQL) is crucial for
data-driven business applications. Existing frameworks, trained on open-source
datasets, struggle with complex business logic and lack domain-specific data
for fine-tuning. Additionally, evaluation methods often require annotated data
and executable database environments, which are scarce in real-world scenarios.
To address these challenges, we propose SQLord, an enterprise-level NL2SQL
framework. First, SQLord introduces a data reverse generation approach to
convert raw SQL statements into annotated data for supervised fine-tuning
(SFT). Second, it proposes a decomposition method for complex queries using an
automated workflow generator. Additionally, SQLord features a comprehensive
GPT-Judge evaluation framework, including Execution Evaluation (EXE), Query-SQL
Evaluation (QSE), and SQL-SQL Evaluation (SSE), tailored to diverse scenarios.
Offline tests significantly outperform state of the art baselines, and online
accuracy consistently exceeds 90, highlighting SQLord's advantages and
effectiveness in complex real world scenarios. SQLord has been successfully
applied across multiple scenarios on the world's largest B2B e-commerce
platform.

</details>


### [278] [Towards Practical Benchmarking of Data Cleaning Techniques: On Generating Authentic Errors via Large Language Models](https://arxiv.org/abs/2507.10934)
*Xinyuan Liu,Jiahui Chen,Bocheng Hu,Yu Sun,Xinyang Chen,Shaoxu Song*

Main category: cs.DB

TL;DR: TableEG是一个利用大型语言模型（LLMs）生成真实错误的框架，通过表格微调和三元组表示（I, T, O）来模拟错误生成、检测和纠正任务，填补了合成与真实错误之间的差距。


<details>
  <summary>Details</summary>
Motivation: 数据质量问题严重影响下游分析和机器学习性能，但缺乏多样化的真实错误数据集限制了全面评估。手动标注错误耗时且不一致，因此探索合成错误生成作为替代方案。

Method: TableEG采用表格微调策略和三元组表示（I, T, O），在12个真实数据集上训练，确保生成的错误分布与真实错误一致。

Result: TableEG生成的错误在模式和分布相似性上优于基于规则的方法和未经微调的LLM生成错误，且性能指标与真实错误高度一致。

Conclusion: TableEG不仅填补了合成与真实错误之间的差距，还为后续错误检测和纠正任务提供了稳健的基准。

Abstract: Data quality remains an important challenge in data-driven systems, as errors
in tabular data can severely compromise downstream analytics and machine
learning performance. Although numerous error detection algorithms have been
proposed, the lack of diverse, real-world error datasets limits comprehensive
evaluation. Manual error annotation is both time-consuming and inconsistent,
motivating the exploration of synthetic error generation as an alternative. In
this work, we introduce TableEG, a framework that leverages large language
models (LLMs) to generate authentic errors. By employing a table fine-tuning
strategy and a triplet representation $(I, T, O)$ to model error generation,
detection, and correction tasks, TableEG captures the complex dependencies
inherent in two-dimensional tables. Trained on 12 real-world datasets spanning
10 diverse domains, TableEG ensures that the synthesized errors faithfully
reflect authentic error distributions. Experimental results indicate that
errors generated by TableEG exhibit superior pattern and distribution
similarity compared to both rule-based methods and LLM-generated errors without
fine-tuning. Furthermore, performance metrics on TableEG-generated errors
closely align with those on real-world errors across nearly all datasets and
detection algorithms, particularly for machine learning based detection
techniques. Overall, TableEG not only bridges the gap between synthetic and
real-world errors but also establishes a robust benchmark for subsequent error
detection and correction tasks.

</details>


<div id='cs.NE'></div>

# cs.NE [[Back]](#toc)

### [279] [Grammatical Structure and Grammatical Variations in Non-Metric Iranian Classical Music](https://arxiv.org/abs/2507.10708)
*Maziar Kanani,Sean O Leary,James McDermott*

Main category: cs.NE

TL;DR: 研究介绍了非节拍伊朗古典音乐的符号数据集及结构解析与变奏生成算法，验证了方法的有效性。


<details>
  <summary>Details</summary>
Motivation: 研究旨在解析和生成非节拍伊朗古典音乐的结构，填补西方音乐分析方法在此类音乐中的不足。

Method: 使用MIDI文件和乐谱数据，应用解析算法识别动机和乐句，并通过语法变异生成新变奏。

Result: 系统成功生成可接受的变奏，统计分析了不同表示设置对变异的影响。

Conclusion: 方法适用于伊朗古典音乐，并可扩展至阿拉伯或土耳其古典音乐。

Abstract: In this study we introduce a symbolic dataset composed of non-metric Iranian
classical music, and algorithms for structural parsing of this music, and
generation of variations. The corpus comprises MIDI files and data sheets of
Dastgah Shour from Radif Mirza Abdollah, the foundational repertoire of Iranian
classical music. Furthermore, we apply our previously-introduced algorithm for
parsing melodic structure (Kanani et al., 2023b)to the dataset. Unlike much
Western music, this type of non-metric music does not follow bar-centric
organisation. The non-metric organisation can be captured well by our parsing
algorithm. We parse each tune (Gusheh) into a grammar to identify motifs and
phrases. These grammar representations can be useful for educational and
ethnomusicological purposes. We also further develop a previously-introduced
method of creating melodic variations (Kanani et al., 2023b). After parsing an
existing tune to produce a grammar, by applying mutations to this grammar, we
generate a new grammar. Expanding this new version yields a variation of the
original tune. Variations are assessed by a domain-expert listener.
Additionally, we conduct a statistical analysis of mutation with different
representation setups for our parsing and generation algorithms. The
overarching conclusion is that the system successfully produces acceptable
variations post-mutation. While our case study focuses on Iranian classical
music, the methodology can be adapted for Arabic or Turkish classical music.

</details>


### [280] [A Biomimetic Way for Coral-Reef-Inspired Swarm Intelligence for Carbon-Neutral Wastewater Treatment](https://arxiv.org/abs/2507.10563)
*Antonis Messinis*

Main category: cs.NE

TL;DR: 提出了一种受珊瑚礁启发的群体交互网络，用于碳中性废水处理，结合形态生成抽象和多任务碳意识，实现了高效率和低能耗。


<details>
  <summary>Details</summary>
Motivation: 随着废水处理需求的增加，实现能源中性的净化具有挑战性。

Method: 采用珊瑚礁启发的群体交互网络，结合形态生成抽象和多任务碳意识，通过线性令牌复杂度实现可扩展性。

Result: 与七个基线相比，实现了96.7%的去除效率，0.31 kWh/m³的能耗和14.2 g/m³的CO₂排放。

Conclusion: 该方法在多种场景下表现出潜力，但数据科学人员配备和治理限制是未来需要解决的问题。

Abstract: With increasing wastewater rates, achieving energy-neutral purification is
challenging. We introduce a coral-reef-inspired Swarm Interaction Network for
carbon-neutral wastewater treatment, combining morphogenetic abstraction with
multi-task carbon awareness. Scalability stems from linear token complexity,
mitigating the energy-removal problem. Compared with seven baselines, our
approach achieves 96.7\% removal efficiency, 0.31~kWh~m$^{-3}$ energy
consumption, and 14.2~g~m$^{-3}$ CO$_2$ emissions. Variance analysis
demonstrates robustness under sensor drift. Field scenarios--insular lagoons,
brewery spikes, and desert greenhouses--show potential diesel savings of up to
22\%. However, data-science staffing remains an impediment. Future work will
integrate AutoML wrappers within the project scope, although governance
restrictions pose interpretability challenges that require further visual
analytics.

</details>


### [281] [SFATTI: Spiking FPGA Accelerator for Temporal Task-driven Inference -- A Case Study on MNIST](https://arxiv.org/abs/2507.10561)
*Alessio Caviglia,Filippo Marostica,Alessio Carpegna,Alessandro Savino,Stefano Di Carlo*

Main category: cs.NE

TL;DR: 本文探讨了使用Spiker+框架为MNIST数据集上的手写数字识别生成优化的SNN加速器，分析了边缘计算限制下的权衡。


<details>
  <summary>Details</summary>
Motivation: 硬件加速器对于边缘应用（如图像识别）的低延迟、高能效推理至关重要，而SNN因其事件驱动和稀疏特性特别适合低功耗FPGA部署。

Method: 利用开源的Spiker+框架，支持高级网络拓扑、神经元模型和量化规范，自动生成可部署的HDL。

Result: 评估了多种配置，分析了与边缘计算约束相关的权衡。

Conclusion: Spiker+框架为SNN在边缘计算中的应用提供了高效且灵活的解决方案。

Abstract: Hardware accelerators are essential for achieving low-latency,
energy-efficient inference in edge applications like image recognition. Spiking
Neural Networks (SNNs) are particularly promising due to their event-driven and
temporally sparse nature, making them well-suited for low-power Field
Programmable Gate Array (FPGA)-based deployment. This paper explores using the
open-source Spiker+ framework to generate optimized SNNs accelerators for
handwritten digit recognition on the MNIST dataset. Spiker+ enables high-level
specification of network topologies, neuron models, and quantization,
automatically generating deployable HDL. We evaluate multiple configurations
and analyze trade-offs relevant to edge computing constraints.

</details>


### [282] [Tangma: A Tanh-Guided Activation Function with Learnable Parameters](https://arxiv.org/abs/2507.10560)
*Shreel Golwala*

Main category: cs.NE

TL;DR: Tangma是一种新的激活函数，结合了双曲正切的平滑形状和两个可学习参数，提升了训练稳定性和效率，在MNIST和CIFAR-10上表现优于ReLU、Swish和GELU。


<details>
  <summary>Details</summary>
Motivation: 激活函数对深度神经网络的反向传播和表达能力至关重要，Tangma旨在通过可学习参数优化激活行为，提升训练效果。

Method: Tangma结合双曲正切的平滑性和两个可学习参数（α调整拐点，γ增加线性以保留弱梯度），在MNIST和CIFAR-10上测试并与ReLU、Swish和GELU对比。

Result: Tangma在MNIST上验证准确率99.09%，CIFAR-10上78.15%，收敛更快且训练更稳定，运行效率也优于Swish和GELU。

Conclusion: Tangma在标准视觉任务中表现优异，其可学习设计为更大模型提供了优化激活行为的潜力。

Abstract: Activation functions are key to effective backpropagation and expressiveness
in deep neural networks. This work introduces Tangma, a new activation function
that combines the smooth shape of the hyperbolic tangent with two learnable
parameters: $\alpha$, which shifts the curve's inflection point to adjust
neuron activation, and $\gamma$, which adds linearity to preserve weak
gradients and improve training stability. Tangma was evaluated on MNIST and
CIFAR-10 using custom networks composed of convolutional and linear layers, and
compared against ReLU, Swish, and GELU. On MNIST, Tangma achieved the highest
validation accuracy of 99.09% and the lowest validation loss, demonstrating
faster and more stable convergence than the baselines. On CIFAR-10, Tangma
reached a top validation accuracy of 78.15%, outperforming all other activation
functions while maintaining a competitive training loss. Tangma also showed
improved training efficiency, with lower average epoch runtimes compared to
Swish and GELU. These results suggest that Tangma performs well on standard
vision tasks and enables reliable, efficient training. Its learnable design
gives more control over activation behavior, which may benefit larger models in
tasks such as image recognition or language modeling.

</details>


### [283] [Biological Processing Units: Leveraging an Insect Connectome to Pioneer Biofidelic Neural Architectures](https://arxiv.org/abs/2507.10951)
*Siyu Yu,Zihan Qin,Tingshan Liu,Beiya Xu,R. Jacob Vogelstein,Jason Brown,Joshua T. Vogelstein*

Main category: cs.NE

TL;DR: 果蝇幼虫大脑的全连接组被转化为生物处理单元（BPU），在MNIST和CIFAR-10等任务上表现优异，甚至超越传统MLP和Transformer模型。


<details>
  <summary>Details</summary>
Motivation: 探索生物演化电路是否支持人工智能，并验证其性能。

Method: 将果蝇幼虫大脑的连接组转化为固定循环网络（BPU），并进行扩展和模态特异性消融实验。

Result: BPU在MNIST上达到98%准确率，CIFAR-10上58%；轻量级GNN-BPU在ChessBench上表现优异；CNN-BPU超越参数匹配的Transformer。

Conclusion: 生物仿真神经架构在复杂认知任务中具有潜力，未来可扩展至更大规模的连接组。

Abstract: The complete connectome of the Drosophila larva brain offers a unique
opportunity to investigate whether biologically evolved circuits can support
artificial intelligence. We convert this wiring diagram into a Biological
Processing Unit (BPU), a fixed recurrent network derived directly from synaptic
connectivity. Despite its modest size 3,000 neurons and 65,000 weights between
them), the unmodified BPU achieves 98% accuracy on MNIST and 58% on CIFAR-10,
surpassing size-matched MLPs. Scaling the BPU via structured connectome
expansions further improves CIFAR-10 performance, while modality-specific
ablations reveal the uneven contributions of different sensory subsystems. On
the ChessBench dataset, a lightweight GNN-BPU model trained on only 10,000
games achieves 60% move accuracy, nearly 10x better than any size transformer.
Moreover, CNN-BPU models with ~2M parameters outperform parameter-matched
Transformers, and with a depth-6 minimax search at inference, reach 91.7%
accuracy, exceeding even a 9M-parameter Transformer baseline. These results
demonstrate the potential of biofidelic neural architectures to support complex
cognitive tasks and motivate scaling to larger and more intelligent connectomes
in future work.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [284] [$\texttt{Droid}$: A Resource Suite for AI-Generated Code Detection](https://arxiv.org/abs/2507.10583)
*Daniil Orel,Indraneil Paul,Iryna Gurevych,Preslav Nakov*

Main category: cs.SE

TL;DR: 论文介绍了DroidCollection，一个包含百万代码样本、七种编程语言和43种编码模型输出的数据集，并开发了DroidDetect检测器。实验表明现有检测器泛化能力不足，但通过对抗训练可提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有机器生成代码检测器在多样化编程语言和领域泛化能力不足，且易受对抗样本攻击。

Method: 构建DroidCollection数据集，开发DroidDetect检测器，采用多任务目标和对抗训练。

Result: 现有检测器泛化能力差，但对抗训练可显著提升性能；度量学习和不确定性重采样有效。

Conclusion: DroidDetect通过对抗训练和多样化数据集提升了检测器的鲁棒性和泛化能力。

Abstract: In this work, we compile $\textbf{$\texttt{DroidCollection}$}$, the most
extensive open data suite for training and evaluating machine-generated code
detectors, comprising over a million code samples, seven programming languages,
outputs from 43 coding models, and over three real-world coding domains.
Alongside fully AI-generated samples, our collection includes human-AI
co-authored code, as well as adversarial samples explicitly crafted to evade
detection. Subsequently, we develop $\textbf{$\texttt{DroidDetect}$}$, a suite
of encoder-only detectors trained using a multi-task objective over
$\texttt{DroidCollection}$. Our experiments show that existing detectors'
performance fails to generalise to diverse coding domains and programming
languages outside of their narrow training data. Additionally, we demonstrate
that while most detectors are easily compromised by humanising the output
distributions using superficial prompting and alignment approaches, this
problem can be easily amended by training on a small amount of adversarial
data. Finally, we demonstrate the effectiveness of metric learning and
uncertainty-based resampling as means to enhance detector training on possibly
noisy distributions.

</details>


### [285] [ARPaCCino: An Agentic-RAG for Policy as Code Compliance](https://arxiv.org/abs/2507.10584)
*Francesco Romeo,Luigi Arena,Francesco Blefari,Francesco Aurelio Pironti,Matteo Lupinacci,Angelo Furfaro*

Main category: cs.SE

TL;DR: ARPaCCino是一个结合LLM、RAG和工具验证的系统，用于自动化生成和验证策略即代码（PaC）规则，提升IaC环境中的策略执行效率和准确性。


<details>
  <summary>Details</summary>
Motivation: 策略即代码（PaC）的复杂性阻碍了其采用，ARPaCCino旨在通过自动化解决这一问题。

Method: 结合LLM、RAG和工具验证，从自然语言生成Rego规则，并验证和优化IaC配置。

Result: 实验证明ARPaCCino能生成语法和语义正确的策略，识别不合规基础设施并进行修正。

Conclusion: ARPaCCino展示了基于RAG的智能架构在提升PaC自动化、可靠性和可访问性方面的潜力。

Abstract: Policy as Code (PaC) is a paradigm that encodes security and compliance
policies into machine-readable formats, enabling automated enforcement in
Infrastructure as Code (IaC) environments. However, its adoption is hindered by
the complexity of policy languages and the risk of misconfigurations. In this
work, we present ARPaCCino, an agentic system that combines Large Language
Models (LLMs), Retrieval-Augmented-Generation (RAG), and tool-based validation
to automate the generation and verification of PaC rules. Given natural
language descriptions of the desired policies, ARPaCCino generates formal Rego
rules, assesses IaC compliance, and iteratively refines the IaC configurations
to ensure conformance. Thanks to its modular agentic architecture and
integration with external tools and knowledge bases, ARPaCCino supports policy
validation across a wide range of technologies, including niche or emerging IaC
frameworks. Experimental evaluation involving a Terraform-based case study
demonstrates ARPaCCino's effectiveness in generating syntactically and
semantically correct policies, identifying non-compliant infrastructures, and
applying corrective modifications, even when using smaller, open-weight LLMs.
Our results highlight the potential of agentic RAG architectures to enhance the
automation, reliability, and accessibility of PaC workflows.

</details>


### [286] [Repairing Language Model Pipelines by Meta Self-Refining Competing Constraints at Runtime](https://arxiv.org/abs/2507.10590)
*Mojtaba Eshghie*

Main category: cs.SE

TL;DR: Meta Self-Refining框架通过元修正层修复语言模型管道中的竞争约束问题，提高效率。


<details>
  <summary>Details</summary>
Motivation: 语言模型管道在动态优化输出时，面对竞争性软约束会导致效率低下的回溯循环。

Method: 引入Meta Self-Refining框架，通过监控执行历史检测问题，并调用元修复器LM生成平衡指令。

Result: 实验表明该框架能有效修复回溯循环，提升语言模型程序的效率。

Conclusion: Meta Self-Refining为解决竞争约束问题提供了有效方法，优化了语言模型的输出效率。

Abstract: Language Model (LM) pipelines can dynamically refine their outputs against
programmatic constraints. However, their effectiveness collapses when faced
with competing soft constraints, leading to inefficient backtracking loops
where satisfying one constraint violates another. We introduce Meta
Self-Refining, a framework that equips LM pipelines with a meta-corrective
layer to repair these competitions at runtime/inference-time. Our approach
monitors the pipeline's execution history to detect oscillatory failures. Upon
detection, it invokes a meta-repairer LM that analyzes the holistic state of
the backtracking attempts and synthesizes a strategic instruction to balance
the competing requirements. This self-repair instruction guides the original LM
out of a failing refining loop towards a successful output. Our results show
Meta Self-Refining can successfully repair these loops, leading to more
efficient LM programs.

</details>


### [287] [ToolRegistry: A Protocol-Agnostic Tool Management Library for Function-Calling LLMs](https://arxiv.org/abs/2507.10593)
*Peng Ding*

Main category: cs.SE

TL;DR: Toolregistry是一个协议无关的工具管理库，通过统一接口简化工具注册、表示、执行和生命周期管理，显著减少代码量并提升性能。


<details>
  <summary>Details</summary>
Motivation: 当前LLM应用中工具集成方法存在碎片化、协议限制和实现复杂性问题，增加了开发负担。

Method: 提出Toolregistry，提供统一的工具管理接口，支持工具注册、表示、执行和生命周期管理。

Result: Toolregistry减少60-80%的集成代码，性能提升3.1倍，完全兼容OpenAI函数调用标准。

Conclusion: Toolregistry显著提升开发效率和代码可维护性，已在开源社区发布。

Abstract: Large Language Model (LLM) applications are increasingly relying on external
tools to extend their capabilities beyond text generation. However, current
tool integration approaches suffer from fragmentation, protocol limitations,
and implementation complexity, leading to substantial development overhead.
This paper presents Toolregistry, a protocol-agnostic tool management library
that simplifies tool registration, representation, execution, and lifecycle
management via a unified interface. Our evaluation demonstrates that
\toolregistry achieves 60-80% reduction in tool integration code, up to 3.1x
performance improvements through concurrent execution, and 100% compatibility
with OpenAI function calling standards. Real-world case studies show
significant improvements in development efficiency and code maintainability
across diverse integration scenarios. \toolregistry is open-source and
available at https://github.com/Oaklight/ToolRegistry, with comprehensive
documentation at https://toolregistry.readthedocs.io/.

</details>


### [288] [SWE-MERA: A Dynamic Benchmark for Agenticly Evaluating Large Language Models on Software Engineering Tasks](https://arxiv.org/abs/2507.11059)
*Pavel Adamenko,Mikhail Ivanov,Aidar Valeev,Rodion Levichev,Pavel Zadorozhny,Ivan Lopatin,Dmitry Babayev,Alena Fenogenova,Valentin Malykh*

Main category: cs.SE

TL;DR: SWE-MERA是一个动态更新的基准测试，旨在解决SWE-bench数据集中的数据污染问题，通过自动化收集GitHub问题并严格验证质量。


<details>
  <summary>Details</summary>
Motivation: 现有SWE-bench数据集存在严重的数据污染问题（如解决方案泄漏和测试用例不足），限制了LLM在软件工程中的评估效果。

Method: 采用自动化管道从GitHub收集真实问题，并进行严格质量验证，生成约10,000个潜在任务（目前300个样本）。

Result: 使用Aider编码代理评估，显示SWE-MERA对最新LLM具有强区分能力，报告了2024年9月至2025年6月间12种LLM的性能。

Conclusion: SWE-MERA通过动态更新和严格验证，有效解决了数据污染问题，为LLM评估提供了更可靠的基准。

Abstract: The rapid advancement of Large Language Models (LLMs) in software engineering
has revealed critical limitations in existing benchmarks, particularly the
widely used SWE-bench dataset. Recent studies have uncovered severe data
contamination issues, e.g. SWE-bench reports 32.67% of successful patches
involve direct solution leakage and 31.08\% pass due to inadequate test cases.
We introduce SWE-MERA, a dynamic, continuously updated benchmark designed to
address these fundamental challenges through an automated collection of
real-world GitHub issues and rigorous quality validation. Our approach
implements a reliable pipeline that ensures quality while minimizing
contamination risks, resulting in approximately 10,000 potential tasks with 300
samples currently available. Evaluation using the Aider coding agent
demonstrates strong discriminative power in state-of-the-art models. We report
performance across a dozen recent LLMs evaluated on tasks collected between
September 2024 and June 2025.

</details>


### [289] [A Code Comprehension Benchmark for Large Language Models for Code](https://arxiv.org/abs/2507.10641)
*Jayant Havare,Saurav Chaudhary,Ganesh Ramakrishnan,Kaushik Maharajan,Srikanth Tamilselvam*

Main category: cs.SE

TL;DR: 论文探讨了大语言模型在代码任务中的表现，指出其表面语法学习能力强但语义理解不足，提出通过微调提升代码理解能力，并在实验中验证了效果。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在代码生成和补全任务中表现优异，但缺乏深层语义理解能力，导致在调试和优化等任务中表现不佳。

Method: 通过在大规模数据集上微调模型，专门针对代码理解任务进行优化，评估不同规模模型在语义理解任务上的表现。

Result: 微调后模型性能显著提升，尤其是QWQ-32B模型准确率从70%提高到83.47%，DPO微调的Codestral-22B在主观评分任务中达到87.66%的最高准确率。

Conclusion: 微调能有效提升大语言模型的代码语义理解能力，尤其在复杂任务中表现更优。

Abstract: Large Language Models have shown impressive capabilities in coding tasks like
code generation and code completion, as they have been trained on a large
amount of code data. Also, since one of the core pretraining objectives is Next
Token Prediction, these models tends to learn surface-level syntactic patterns
in code. However, this does not guarantee code comprehension ability i.e. the
ability to capture the semantics of the code. In our opinion, this is the
reason why these models often underperform on tasks that require deeper
semantic understanding, such as code debugging and code optimization. To
address this, we propose fine-tuning these models specifically for code
comprehension tasks using large-scale datasets, enabling them to develop a more
robust understanding of code semantics. We evaluate three code models of
varying sizes on a suite of code comprehension tasks designed to assess
semantic understanding beyond surface-level syntactic pattern matching. In
particular, we analyze performance on the Subjectivity Grading Task and observe
that model performance improves after fine-tuning on relevant downstream tasks.
The most significant improvement is seen in the QWQ-32B model, where accuracy
increases from 70% to 83.47%. A similar or explainable trend is observed across
other models, clearly indicating an enhancement in code comprehension ability.
Among the models studied, the DPO-fine-tuned Codestral-22B achieves the highest
micro-accuracy of 87.66% on the Subjectivity Grading Task.

</details>


### [290] [CodeAssistBench (CAB): Dataset & Benchmarking for Multi-turn Chat-Based Code Assistance](https://arxiv.org/abs/2507.10646)
*Myeongsoo Kim,Shweta Garg,Baishakhi Ray,Varun Kumar,Anoop Deoras*

Main category: cs.SE

TL;DR: CodeAssistBench (CAB) 是一个新的基准框架，用于评估多轮编程辅助在真实环境中的表现，填补了现有基准的不足。


<details>
  <summary>Details</summary>
Motivation: 现有基准主要关注代码生成任务，且多为单轮交互，无法反映真实项目环境的需求。

Method: CAB 通过从 GitHub 问题自动生成可扩展数据集，并利用容器化代码库和模拟用户进行评估。

Result: 测试集包含 3,286 个真实编程问题，评估显示主流 LLM 在 Stack Overflow 上表现良好（70-83%），但在 CAB 中仅解决 16.49% 的问题。

Conclusion: CAB 揭示了在复杂项目环境中提供编程辅助的挑战，强调了现有模型与真实需求的差距。

Abstract: Programming assistants powered by large language models have transformed
software development, yet most benchmarks focus narrowly on code generation
tasks. Recent efforts like InfiBench and StackEval attempt to address this gap
using Stack Overflow data but remain limited to single-turn interactions in
isolated contexts, require significant manual curation, and fail to represent
complete project environments. We introduce CodeAssistBench (CAB), the first
benchmark framework for evaluating multi-turn programming assistance in
realistic settings that address real-world questions about actual codebases.
Unlike existing programming Q&A benchmarks, CAB automatically generates
scalable datasets from question-related GitHub issues using configurable
parameters (e.g., repository creation date, star count, programming languages),
and includes automatic containerization of codebases for evaluation. It then
evaluates models through simulated users in these containerized environments
with full codebase access. Using this framework, we constructed a test set of
3,286 real-world programming questions across 231 repositories, spanning seven
programming languages and diverse problem domains. Our evaluation of leading
LLMs reveals a substantial capability gap: while models perform well on Stack
Overflow questions with success rates of 70-83%, they resolve only up to 16.49%
of CAB's recent issues. This discrepancy highlights the challenges of providing
assistance in complex, project-specific contexts versus answering standalone
questions.

</details>


### [291] [Past, Present and Future: Exploring Adaptive AI in Software Development Bots](https://arxiv.org/abs/2507.10822)
*Omar Elsisi,Glaucia Melo*

Main category: cs.SE

TL;DR: 本文探讨了自适应AI驱动的对话代理在软件开发中的作用，强调其动态、上下文感知的辅助能力，并分析了其从简单查询系统到高级AI解决方案的演变。


<details>
  <summary>Details</summary>
Motivation: 研究自适应AI对话代理如何通过机器学习和自然语言处理提供个性化支持，提升软件开发效率和协作。

Method: 通过分析现有工具（如GitHub Copilot和Microsoft Teams机器人）的演变，探讨自适应AI在软件开发中的应用和挑战。

Result: 自适应AI对话代理能显著提高开发效率，但需解决数据隐私和伦理问题。

Conclusion: 自适应AI对话代理有望通过实时定制支持革新软件开发，但需进一步优化和规范。

Abstract: Conversational agents, such as chatbots and virtual assistants, have become
essential in software development, boosting productivity, collaboration, and
automating various tasks. This paper examines the role of adaptive AI-powered
conversational agents in software development, highlighting their ability to
offer dynamic, context-aware assistance to developers. Unlike traditional
rule-based systems, adaptive AI agents use machine learning and natural
language processing to learn from interactions and improve over time, providing
more personalized and responsive help. We look at how these tools have evolved
from simple query-based systems to advanced AI-driven solutions like GitHub
Copilot and Microsoft Teams bots. We also explore the challenges of integrating
adaptive AI into software development processes. The study aims to assess the
benefits and limitations of these systems, address concerns like data privacy
and ethical issues, and offer insights into their future use in the field.
Ultimately, adaptive AI chatbots have great potential to revolutionize software
development by delivering real-time, customized support and enhancing the
efficiency of development cycles.

</details>


### [292] [SENSOR: An ML-Enhanced Online Annotation Tool to Uncover Privacy Concerns from User Reviews in Social-Media Applications](https://arxiv.org/abs/2507.10640)
*Labiba Farah,Mohammad Ridwan Kabir,Shohel Ahmed,MD Mohaymen Ul Anam,Md. Sakibul Islam*

Main category: cs.SE

TL;DR: 本文提出了一种名为SENSOR的自动化工具，用于分类用户评论中的隐私相关需求（如功能请求或错误报告），并介绍了GRACE模型（基于GRU和注意力机制）以实现高效标注。


<details>
  <summary>Details</summary>
Motivation: 社交媒体的广泛使用引发了隐私问题，用户评论中常提到这些问题，但手动分类和优先处理这些评论对开发者来说具有挑战性。

Method: 开发了SENSOR工具和GRACE模型（结合GRU、CBOW和注意力机制），分析了16000条用户评论，并通过人工标注训练模型。

Result: GRACE模型表现最佳（宏F1分数：0.9434，宏ROC-AUC：0.9934，准确率：95.10%），SENSOR工具能有效帮助开发者处理隐私相关评论。

Conclusion: SENSOR工具和GRACE模型能显著提升开发者处理隐私相关评论的效率，增强用户隐私和信任。

Abstract: The widespread use of social media applications has raised significant
privacy concerns, often highlighted in user reviews. These reviews also provide
developers with valuable insights into improving apps by addressing issues and
introducing better features. However, the sheer volume and nuanced nature of
reviews make manual identification and prioritization of privacy-related
concerns challenging for developers. Previous studies have developed software
utilities to automatically classify user reviews as privacy-relevant,
privacy-irrelevant, bug reports, feature requests, etc., using machine
learning. Notably, there is a lack of focus on classifying reviews specifically
as privacy-related feature requests, privacy-related bug reports, or
privacy-irrelevant. This paper introduces SENtinel SORt (SENSOR), an automated
online annotation tool designed to help developers annotate and classify user
reviews into these categories. For automating the annotation of such reviews,
this paper introduces the annotation model, GRACE (GRU-based Attention with
CBOW Embedding), using Gated Recurrent Units (GRU) with Continuous Bag of Words
(CBOW) and Attention mechanism. Approximately 16000 user reviews from seven
popular social media apps on Google Play Store, including Instagram, Facebook,
WhatsApp, Snapchat, X (formerly Twitter), Facebook Lite, and Line were
analyzed. Two annotators manually labelled the reviews, achieving a Cohen's
Kappa value of 0.87, ensuring a labeled dataset with high inter-rater agreement
for training machine learning models. Among the models tested, GRACE
demonstrated the best performance (macro F1-score: 0.9434, macro ROC-AUC:
0.9934, and accuracy: 95.10%) despite class imbalance. SENSOR demonstrates
significant potential to assist developers with extracting and addressing
privacy-related feature requests or bug reports from user reviews, enhancing
user privacy and trust.

</details>


<div id='cs.SI'></div>

# cs.SI [[Back]](#toc)

### [293] [The Shape of Deceit: Behavioral Consistency and Fragility in Money Laundering Patterns](https://arxiv.org/abs/2507.10608)
*Danny Butvinik,Ofir Yakobi,Michal Einhorn Cohen,Elina Maliarsky*

Main category: cs.SI

TL;DR: 论文挑战了传统的反洗钱（AML）系统，提出了一种基于网络理论的方法，强调通过行为一致性检测洗钱模式，而非统计异常。


<details>
  <summary>Details</summary>
Motivation: 传统AML系统误判了洗钱的本质，洗钱行为通常是故意且重复的，而非统计异常。

Method: 提出网络理论视角，通过子图结构捕捉行为一致性，并引入模式脆弱性概念。

Result: 洗钱检测应关注行为本质的保留，而非统计异常。

Conclusion: 论文为AML系统提供了新的建模和解释网络的方法，对抗金融犯罪。

Abstract: Conventional anti-money laundering (AML) systems predominantly focus on
identifying anomalous entities or transactions, flagging them for manual
investigation based on statistical deviation or suspicious behavior. This
paradigm, however, misconstrues the true nature of money laundering, which is
rarely anomalous but often deliberate, repeated, and concealed within
consistent behavioral routines. In this paper, we challenge the entity-centric
approach and propose a network-theoretic perspective that emphasizes detecting
predefined laundering patterns across directed transaction networks. We
introduce the notion of behavioral consistency as the core trait of laundering
activity, and argue that such patterns are better captured through subgraph
structures expressing semantic and functional roles - not solely geometry.
Crucially, we explore the concept of pattern fragility: the sensitivity of
laundering patterns to small attribute changes and, conversely, their semantic
robustness even under drastic topological transformations. We claim that
laundering detection should not hinge on statistical outliers, but on
preservation of behavioral essence, and propose a reconceptualization of
pattern similarity grounded in this insight. This philosophical and practical
shift has implications for how AML systems model, scan, and interpret networks
in the fight against financial crime.

</details>


### [294] [Multilayer Artificial Benchmark for Community Detection (mABCD)](https://arxiv.org/abs/2507.10795)
*Łukasz Kraiński,Michał Czuba,Piotr Bródka,Paweł Prałat,Bogumił Kamiński,François Théberge*

Main category: cs.SI

TL;DR: ABCD模型是一种具有社区结构和幂律分布的随机图模型，其变体mABCD扩展到了多层网络。


<details>
  <summary>Details</summary>
Motivation: 为了提供一个比LFR模型更快、更易解释且可分析的随机图模型，并进一步扩展到多层网络。

Method: 基于ABCD模型的底层原理，开发了适用于多层网络的变体mABCD。

Result: mABCD模型能够生成类似LFR模型的多层网络图，同时保持了ABCD模型的优势。

Conclusion: mABCD模型为多层网络分析提供了一个高效且可解释的工具。

Abstract: The Artificial Benchmark for Community Detection (ABCD) model is a random
graph model with community structure and power-law distribution for both
degrees and community sizes. The model generates graphs similar to the
well-known LFR model but it is faster, more interpretable, and can be
investigated analytically. In this paper, we use the underlying ingredients of
the ABCD model and introduce its variant for multilayer networks, mABCD.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [295] [Latent Space Consistency for Sparse-View CT Reconstruction](https://arxiv.org/abs/2507.11152)
*Duoyou Chen,Yunqing Chen,Can Zhang,Zhou Wang,Cheng Chen,Ruoxiu Xiao*

Main category: eess.IV

TL;DR: 提出了一种名为CLS-DM的模型，通过跨模态特征对比学习，解决2D X射线图像与3D CT图像潜在空间对齐问题，提升稀疏X射线重建CT的效果。


<details>
  <summary>Details</summary>
Motivation: 传统CT重建方法存在时间消耗大和辐射风险高的问题，稀疏X射线图像重建方法成为研究热点，但现有扩散模型在跨模态潜在空间对齐上表现不足。

Method: 提出CLS-DM模型，结合跨模态特征对比学习，从2D X射线图像中提取3D潜在信息，实现模态间潜在空间对齐。

Result: 在LIDC-IDRI和CTSpine1K数据集上，CLS-DM在PSNR和SSIM指标上优于经典和先进生成模型。

Conclusion: CLS-DM不仅提升了稀疏X射线重建CT的效果，还可推广至其他跨模态转换任务，如文本到图像合成。

Abstract: Computed Tomography (CT) is a widely utilized imaging modality in clinical
settings. Using densely acquired rotational X-ray arrays, CT can capture 3D
spatial features. However, it is confronted with challenged such as significant
time consumption and high radiation exposure. CT reconstruction methods based
on sparse-view X-ray images have garnered substantial attention from
researchers as they present a means to mitigate costs and risks. In recent
years, diffusion models, particularly the Latent Diffusion Model (LDM), have
demonstrated promising potential in the domain of 3D CT reconstruction.
Nonetheless, due to the substantial differences between the 2D latent
representation of X-ray modalities and the 3D latent representation of CT
modalities, the vanilla LDM is incapable of achieving effective alignment
within the latent space. To address this issue, we propose the Consistent
Latent Space Diffusion Model (CLS-DM), which incorporates cross-modal feature
contrastive learning to efficiently extract latent 3D information from 2D X-ray
images and achieve latent space alignment between modalities. Experimental
results indicate that CLS-DM outperforms classical and state-of-the-art
generative models in terms of standard voxel-level metrics (PSNR, SSIM) on the
LIDC-IDRI and CTSpine1K datasets. This methodology not only aids in enhancing
the effectiveness and economic viability of sparse X-ray reconstructed CT but
can also be generalized to other cross-modal transformation tasks, such as
text-to-image synthesis. We have made our code publicly available at
https://anonymous.4open.science/r/CLS-DM-50D6/ to facilitate further research
and applications in other domains.

</details>


### [296] [Comparative Analysis of Vision Transformers and Traditional Deep Learning Approaches for Automated Pneumonia Detection in Chest X-Rays](https://arxiv.org/abs/2507.10589)
*Gaurav Singh*

Main category: eess.IV

TL;DR: 该研究比较了传统机器学习和深度学习在肺炎检测中的表现，发现Vision Transformers（尤其是Cross-ViT）在准确率和召回率上优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 肺炎（如COVID-19引起的）是全球健康挑战，需要快速准确的诊断方法。

Method: 比较了传统机器学习（PCA聚类、逻辑回归、支持向量分类）和深度学习（CNN、ViT）在5,856张儿童胸透图像上的表现。

Result: Cross-ViT表现最佳，准确率88.25%，召回率99.42%，且模型规模影响较小。

Conclusion: Vision Transformers在肺炎检测中具有潜力，可提升诊断速度和准确性。

Abstract: Pneumonia, particularly when induced by diseases like COVID-19, remains a
critical global health challenge requiring rapid and accurate diagnosis. This
study presents a comprehensive comparison of traditional machine learning and
state-of-the-art deep learning approaches for automated pneumonia detection
using chest X-rays (CXRs). We evaluate multiple methodologies, ranging from
conventional machine learning techniques (PCA-based clustering, Logistic
Regression, and Support Vector Classification) to advanced deep learning
architectures including Convolutional Neural Networks (Modified LeNet,
DenseNet-121) and various Vision Transformer (ViT) implementations (Deep-ViT,
Compact Convolutional Transformer, and Cross-ViT). Using a dataset of 5,856
pediatric CXR images, we demonstrate that Vision Transformers, particularly the
Cross-ViT architecture, achieve superior performance with 88.25% accuracy and
99.42% recall, surpassing traditional CNN approaches. Our analysis reveals that
architectural choices impact performance more significantly than model size,
with Cross-ViT's 75M parameters outperforming larger models. The study also
addresses practical considerations including computational efficiency, training
requirements, and the critical balance between precision and recall in medical
diagnostics. Our findings suggest that Vision Transformers offer a promising
direction for automated pneumonia detection, potentially enabling more rapid
and accurate diagnosis during health crises.

</details>


### [297] [3D Magnetic Inverse Routine for Single-Segment Magnetic Field Images](https://arxiv.org/abs/2507.11293)
*J. Senthilnath,Chen Hao,F. C. Wellstood*

Main category: eess.IV

TL;DR: 论文提出了一种名为3D MIR的新方法，结合深度学习和物理约束，通过磁力图像恢复半导体封装中的3D电流信息。


<details>
  <summary>Details</summary>
Motivation: 在半导体封装中，准确恢复3D信息对无损检测和电路缺陷定位至关重要。

Method: 3D MIR分三阶段：1) CNN预测参数和分类；2) 物理约束提供初始估计；3) 优化器调整参数以最小化误差。

Result: 3D MIR方法高精度恢复3D信息，为磁力图像重建设定了新标准。

Conclusion: 结合深度学习和物理驱动优化在实际应用中具有潜力。

Abstract: In semiconductor packaging, accurately recovering 3D information is crucial
for non-destructive testing (NDT) to localize circuit defects. This paper
presents a novel approach called the 3D Magnetic Inverse Routine (3D MIR),
which leverages Magnetic Field Images (MFI) to retrieve the parameters for the
3D current flow of a single-segment. The 3D MIR integrates a deep learning
(DL)-based Convolutional Neural Network (CNN), spatial-physics-based
constraints, and optimization techniques. The method operates in three stages:
i) The CNN model processes the MFI data to predict ($\ell/z_o$), where $\ell$
is the wire length and $z_o$ is the wire's vertical depth beneath the magnetic
sensors and classify segment type ($c$). ii) By leveraging
spatial-physics-based constraints, the routine provides initial estimates for
the position ($x_o$, $y_o$, $z_o$), length ($\ell$), current ($I$), and current
flow direction (positive or negative) of the current segment. iii) An optimizer
then adjusts these five parameters ($x_o$, $y_o$, $z_o$, $\ell$, $I$) to
minimize the difference between the reconstructed MFI and the actual MFI. The
results demonstrate that the 3D MIR method accurately recovers 3D information
with high precision, setting a new benchmark for magnetic image reconstruction
in semiconductor packaging. This method highlights the potential of combining
DL and physics-driven optimization in practical applications.

</details>


### [298] [HANS-Net: Hyperbolic Convolution and Adaptive Temporal Attention for Accurate and Generalizable Liver and Tumor Segmentation in CT Imaging](https://arxiv.org/abs/2507.11325)
*Arefin Ittesafun Abian,Ripon Kumar Debnath,Md. Abdur Rahman,Mohaimenul Azam Khan Raiaan,Md Rafiqul Islam,Asif Karim,Reem E. Mohamed,Sami Azam*

Main category: eess.IV

TL;DR: HANS-Net是一种新型肝脏和肿瘤分割框架，结合双曲卷积、多尺度纹理学习和自适应特征增强，在CT图像上实现高精度分割。


<details>
  <summary>Details</summary>
Motivation: 解决腹部CT图像中肝脏和肿瘤分割的挑战，如复杂解剖结构、肿瘤外观多变和标注数据有限。

Method: 结合双曲卷积、小波分解模块、突触可塑性机制和隐式神经表示，并引入不确定性感知和轻量级时间注意力。

Result: 在LiTS数据集上Dice得分为93.26%，IoU为88.09%；在3D-IRCADb-01数据集上Dice为87.45%，IoU为80.30%。

Conclusion: HANS-Net在肝脏和肿瘤分割中表现出高效性、鲁棒性和泛化能力。

Abstract: Accurate liver and tumor segmentation on abdominal CT images is critical for
reliable diagnosis and treatment planning, but remains challenging due to
complex anatomical structures, variability in tumor appearance, and limited
annotated data. To address these issues, we introduce Hyperbolic-convolutions
Adaptive-temporal-attention with Neural-representation and Synaptic-plasticity
Network (HANS-Net), a novel segmentation framework that synergistically
combines hyperbolic convolutions for hierarchical geometric representation, a
wavelet-inspired decomposition module for multi-scale texture learning, a
biologically motivated synaptic plasticity mechanism for adaptive feature
enhancement, and an implicit neural representation branch to model fine-grained
and continuous anatomical boundaries. Additionally, we incorporate
uncertainty-aware Monte Carlo dropout to quantify prediction confidence and
lightweight temporal attention to improve inter-slice consistency without
sacrificing efficiency. Extensive evaluations of the LiTS dataset demonstrate
that HANS-Net achieves a mean Dice score of 93.26%, an IoU of 88.09%, an
average symmetric surface distance (ASSD) of 0.72 mm, and a volume overlap
error (VOE) of 11.91%. Furthermore, cross-dataset validation on the
3D-IRCADb-01 dataset obtains an average Dice of 87.45%, IoU of 80.30%, ASSD of
1.525 mm, and VOE of 19.71%, indicating strong generalization across different
datasets. These results confirm the effectiveness and robustness of HANS-Net in
providing anatomically consistent, accurate, and confident liver and tumor
segmentation.

</details>


### [299] [Focus on Texture: Rethinking Pre-training in Masked Autoencoders for Medical Image Classification](https://arxiv.org/abs/2507.10869)
*Chetan Madan,Aarjav Satia,Soumen Basu,Pankaj Gupta,Usha Dutta,Chetan Arora*

Main category: eess.IV

TL;DR: GLCM-MAE是一种基于GLCM重建损失的自监督预训练框架，用于医学图像分析，显著提升了多种下游任务的性能。


<details>
  <summary>Details</summary>
Motivation: 传统MAE在医学图像中因纹理信息重要而表现不佳，GLCM-MAE通过捕捉图像强度和空间关系来解决这一问题。

Method: 提出GLCM-MAE框架，使用GLCM矩阵匹配作为重建损失，并设计了一种可微分的损失函数。

Result: 在胆囊癌、乳腺癌、肺炎和COVID检测任务中，GLCM-MAE分别提升了2.1%、3.1%、0.5%和0.6%的性能。

Conclusion: GLCM-MAE通过保留形态特征，显著提升了医学图像的自监督表示学习效果。

Abstract: Masked Autoencoders (MAEs) have emerged as a dominant strategy for
self-supervised representation learning in natural images, where models are
pre-trained to reconstruct masked patches with a pixel-wise mean squared error
(MSE) between original and reconstructed RGB values as the loss. We observe
that MSE encourages blurred image re-construction, but still works for natural
images as it preserves dominant edges. However, in medical imaging, when the
texture cues are more important for classification of a visual abnormality, the
strategy fails. Taking inspiration from Gray Level Co-occurrence Matrix (GLCM)
feature in Radiomics studies, we propose a novel MAE based pre-training
framework, GLCM-MAE, using reconstruction loss based on matching GLCM. GLCM
captures intensity and spatial relationships in an image, hence proposed loss
helps preserve morphological features. Further, we propose a novel formulation
to convert matching GLCM matrices into a differentiable loss function. We
demonstrate that unsupervised pre-training on medical images with the proposed
GLCM loss improves representations for downstream tasks. GLCM-MAE outperforms
the current state-of-the-art across four tasks - gallbladder cancer detection
from ultrasound images by 2.1%, breast cancer detection from ultrasound by
3.1%, pneumonia detection from x-rays by 0.5%, and COVID detection from CT by
0.6%. Source code and pre-trained models are available at:
https://github.com/ChetanMadan/GLCM-MAE.

</details>


### [300] [U-RWKV: Lightweight medical image segmentation with direction-adaptive RWKV](https://arxiv.org/abs/2507.11415)
*Hongbo Ye,Fenghe Tang,Peiang Zhao,Zhen Huang,Dexin Zhao,Minghao Bian,S. Kevin Zhou*

Main category: eess.IV

TL;DR: U-RWKV是一种轻量级高性能医学图像分割框架，通过RWKV架构实现高效长程建模，解决了现有方法全局感受野有限的问题。


<details>
  <summary>Details</summary>
Motivation: 在资源有限的环境中实现医疗图像分割的公平性，需要轻量级高性能的解决方案。现有方法如U-Net及其变体因全局感受野有限，难以捕捉长程依赖关系。

Method: 提出U-RWKV框架，采用RWKV架构（O(N)计算成本），并引入方向自适应RWKV模块（DARM）和阶段自适应Squeeze-and-Excitation模块（SASE），以高效捕捉全局上下文和细节。

Result: 实验表明，U-RWKV在计算高效的同时，实现了最先进的分割性能。

Conclusion: U-RWKV为资源受限环境中的高级医学影像技术提供了实用解决方案。

Abstract: Achieving equity in healthcare accessibility requires lightweight yet
high-performance solutions for medical image segmentation, particularly in
resource-limited settings. Existing methods like U-Net and its variants often
suffer from limited global Effective Receptive Fields (ERFs), hindering their
ability to capture long-range dependencies. To address this, we propose U-RWKV,
a novel framework leveraging the Recurrent Weighted Key-Value(RWKV)
architecture, which achieves efficient long-range modeling at O(N)
computational cost. The framework introduces two key innovations: the
Direction-Adaptive RWKV Module(DARM) and the Stage-Adaptive
Squeeze-and-Excitation Module(SASE). DARM employs Dual-RWKV and QuadScan
mechanisms to aggregate contextual cues across images, mitigating directional
bias while preserving global context and maintaining high computational
efficiency. SASE dynamically adapts its architecture to different feature
extraction stages, balancing high-resolution detail preservation and semantic
relationship capture. Experiments demonstrate that U-RWKV achieves
state-of-the-art segmentation performance with high computational efficiency,
offering a practical solution for democratizing advanced medical imaging
technologies in resource-constrained environments. The code is available at
https://github.com/hbyecoding/U-RWKV.

</details>


<div id='q-fin.TR'></div>

# q-fin.TR [[Back]](#toc)

### [301] [Kernel Learning for Mean-Variance Trading Strategies](https://arxiv.org/abs/2507.10701)
*Owen Futter,Nicola Muca Cirone,Blanka Horvath*

Main category: q-fin.TR

TL;DR: 提出了一种基于核的动态路径依赖交易策略框架，优于传统马尔可夫方法。


<details>
  <summary>Details</summary>
Motivation: 解决资产动态或预测信号存在时间依赖时的最优投资组合问题。

Method: 利用再生核希尔伯特空间（RKHS）参数化交易策略，提供非马尔可夫方法。

Result: 在合成和市场数据中显著优于传统方法，建模灵活。

Conclusion: 核方法提供闭式解，是梯度优化的替代方案。

Abstract: In this article, we develop a kernel-based framework for constructing
dynamic, pathdependent trading strategies under a mean-variance optimisation
criterion. Building on the theoretical results of (Muca Cirone and Salvi,
2025), we parameterise trading strategies as functions in a reproducing kernel
Hilbert space (RKHS), enabling a flexible and non-Markovian approach to optimal
portfolio problems. We compare this with the signature-based framework of
(Futter, Horvath, Wiese, 2023) and demonstrate that both significantly
outperform classical Markovian methods when the asset dynamics or predictive
signals exhibit temporal dependencies for both synthetic and market-data
examples. Using kernels in this context provides significant modelling
flexibility, as the choice of feature embedding can range from randomised
signatures to the final layers of neural network architectures. Crucially, our
framework retains closed-form solutions and provides an alternative to
gradient-based optimisation.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [302] [FLsim: A Modular and Library-Agnostic Simulation Framework for Federated Learning](https://arxiv.org/abs/2507.11430)
*Arnab Mukherjee,Raju Halder,Joydeep Chandra*

Main category: cs.DC

TL;DR: FLsim是一个模块化、可扩展的联邦学习模拟框架，支持定制化数据分布、本地学习算法选择、网络拓扑定义等，为研究者和实践者提供灵活的实验工具。


<details>
  <summary>Details</summary>
Motivation: 联邦学习（FL）的研究和基准测试面临挑战，需要一个统一的框架来简化实验流程并支持多样化需求。

Method: 开发FLsim框架，提供模块化设计、资源高效性和实验可重复性，支持定制化配置和区块链集成。

Result: 实验证明FLsim能有效模拟多种先进联邦学习场景，具有高度的灵活性和功能性。

Conclusion: FLsim是联邦学习模拟框架的重要进步，为研究者和实践者提供了前所未有的灵活性和功能。

Abstract: Federated Learning (FL) has undergone significant development since its
inception in 2016, advancing from basic algorithms to complex methodologies
tailored to address diverse challenges and use cases. However, research and
benchmarking of novel FL techniques against a plethora of established
state-of-the-art solutions remain challenging. To streamline this process, we
introduce FLsim, a comprehensive FL simulation framework designed to meet the
diverse requirements of FL workflows in the literature. FLsim is characterized
by its modularity, scalability, resource efficiency, and controlled
reproducibility of experimental outcomes. Its easy to use interface allows
users to specify customized FL requirements through job configuration, which
supports: (a) customized data distributions, ranging from non-independent and
identically distributed (non-iid) data to independent and identically
distributed (iid) data, (b) selection of local learning algorithms according to
user preferences, with complete agnosticism to ML libraries, (c) choice of
network topology illustrating communication patterns among nodes, (d)
definition of model aggregation and consensus algorithms, and (e) pluggable
blockchain support for enhanced robustness. Through a series of experimental
evaluations, we demonstrate the effectiveness and versatility of FLsim in
simulating a diverse range of state-of-the-art FL experiments. We envisage that
FLsim would mark a significant advancement in FL simulation frameworks,
offering unprecedented flexibility and functionality for researchers and
practitioners alike.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [303] [When and Where do Data Poisons Attack Textual Inversion?](https://arxiv.org/abs/2507.10578)
*Jeremy Styborski,Mingzhi Lyu,Jiayou Lu,Nupur Kapur,Adams Kong*

Main category: cs.CR

TL;DR: 本文分析了扩散模型（DMs）中的中毒攻击对文本反转（TI）的影响，并提出了一种新的防御机制Safe-Zone Training（SZT），通过JPEG压缩、限制高时间步训练和损失掩码来增强TI的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 中毒攻击对扩散模型的鲁棒性构成重大威胁，尤其是在文本反转（TI）这种广泛使用的个性化技术中。本文旨在系统分析中毒攻击的机制并提出有效的防御方法。

Method: 1. 引入语义敏感度图可视化中毒对文本嵌入的影响；2. 发现并验证扩散模型在时间步上的非均匀学习行为；3. 提出Safe-Zone Training（SZT），包含JPEG压缩、高时间步训练限制和损失掩码三个关键组件。

Result: 实验表明，SZT显著提升了TI对所有中毒攻击的鲁棒性，生成质量优于现有防御方法。

Conclusion: SZT是一种有效的防御机制，能够显著增强扩散模型在文本反转任务中对中毒攻击的抵抗力。

Abstract: Poisoning attacks pose significant challenges to the robustness of diffusion
models (DMs). In this paper, we systematically analyze when and where poisoning
attacks textual inversion (TI), a widely used personalization technique for
DMs. We first introduce Semantic Sensitivity Maps, a novel method for
visualizing the influence of poisoning on text embeddings. Second, we identify
and experimentally verify that DMs exhibit non-uniform learning behavior across
timesteps, focusing on lower-noise samples. Poisoning attacks inherit this bias
and inject adversarial signals predominantly at lower timesteps. Lastly, we
observe that adversarial signals distract learning away from relevant concept
regions within training data, corrupting the TI process. Based on these
insights, we propose Safe-Zone Training (SZT), a novel defense mechanism
comprised of 3 key components: (1) JPEG compression to weaken high-frequency
poison signals, (2) restriction to high timesteps during TI training to avoid
adversarial signals at lower timesteps, and (3) loss masking to constrain
learning to relevant regions. Extensive experiments across multiple poisoning
methods demonstrate that SZT greatly enhances the robustness of TI against all
poisoning attacks, improving generative quality beyond prior published
defenses. Code: www.github.com/JStyborski/Diff_Lab Data:
www.github.com/JStyborski/NC10

</details>


### [304] [LaSM: Layer-wise Scaling Mechanism for Defending Pop-up Attack on GUI Agents](https://arxiv.org/abs/2507.10610)
*Zihe Yan,Zhuosheng Zhang*

Main category: cs.CR

TL;DR: 该论文提出了一种名为LaSM的层间缩放机制，用于增强多模态大语言模型（MLLM）GUI代理对弹出式环境注入攻击的防御能力，无需额外训练即可显著提升模型的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有GUI代理在面对恶意视觉元素的弹出式攻击时表现脆弱，现有防御方法要么成本高昂，要么在归纳干扰下效果不佳。

Method: 通过系统研究攻击如何改变GUI代理的注意力行为，发现正确与错误输出间的层间注意力差异模式，并基于此提出LaSM机制，选择性放大关键层的注意力和MLP模块。

Result: 在12种弹出式扰动和4种模型骨干上的实验表明，LaSM显著提升了防御成功率，结合提示级警报后，即使在强归纳攻击下也能达到98%以上的鲁棒性。

Conclusion: 注意力错位是MLLM代理的核心漏洞，通过选择性层间调制可以有效解决。

Abstract: Graphical user interface (GUI) agents built on multimodal large language
models (MLLMs) have recently demonstrated strong decision-making abilities in
screen-based interaction tasks. However, they remain highly vulnerable to
pop-up-based environmental injection attacks, where malicious visual elements
divert model attention and lead to unsafe or incorrect actions. Existing
defense methods either require costly retraining or perform poorly under
inductive interference. In this work, we systematically study how such attacks
alter the attention behavior of GUI agents and uncover a layer-wise attention
divergence pattern between correct and incorrect outputs. Based on this
insight, we propose \textbf{LaSM}, a \textit{Layer-wise Scaling Mechanism} that
selectively amplifies attention and MLP modules in critical layers. LaSM
improves the alignment between model saliency and task-relevant regions without
additional training. Extensive experiments across 12 types of pop-up
perturbations and 4 different model backbones show that LaSM consistently
enhances the defense success rate. When combined with prompt-level alerts, LaSM
achieves over 98\% robustness even under strong inductive attacks. Our findings
reveal that attention misalignment is a core vulnerability in MLLM agents and
can be effectively addressed through selective layer-wise modulation.

</details>


### [305] [Game Theory Meets LLM and Agentic AI: Reimagining Cybersecurity for the Age of Intelligent Threats](https://arxiv.org/abs/2507.10621)
*Quanyan Zhu*

Main category: cs.CR

TL;DR: 论文探讨了如何通过博弈论和基于LLM的智能代理提升网络安全的主动性和智能性，弥合理论与实践的差距。


<details>
  <summary>Details</summary>
Motivation: 传统网络安全方法依赖手动响应和脆弱启发式，需要更智能的防御系统。博弈论和LLM代理的结合为建模对抗行为、设计战略防御提供了新路径。

Method: 结合博弈论框架（静态、动态、贝叶斯和信号博弈）与LLM代理，设计智能防御系统，并探索多代理工作流和协调博弈。

Result: LLM代理能将抽象策略转化为实际决策，博弈论则指导代理在复杂工作流中的推理与协调，推动理论与实践的融合。

Conclusion: 博弈论与LLM代理的协同发展有望构建更安全、智能和自适应的网络系统。

Abstract: Protecting cyberspace requires not only advanced tools but also a shift in
how we reason about threats, trust, and autonomy. Traditional cybersecurity
methods rely on manual responses and brittle heuristics. To build proactive and
intelligent defense systems, we need integrated theoretical frameworks and
software tools. Game theory provides a rigorous foundation for modeling
adversarial behavior, designing strategic defenses, and enabling trust in
autonomous systems. Meanwhile, software tools process cyber data, visualize
attack surfaces, verify compliance, and suggest mitigations. Yet a disconnect
remains between theory and practical implementation.
  The rise of Large Language Models (LLMs) and agentic AI offers a new path to
bridge this gap. LLM-powered agents can operationalize abstract strategies into
real-world decisions. Conversely, game theory can inform the reasoning and
coordination of these agents across complex workflows. LLMs also challenge
classical game-theoretic assumptions, such as perfect rationality or static
payoffs, prompting new models aligned with cognitive and computational
realities. This co-evolution promises richer theoretical foundations and novel
solution concepts. Agentic AI also reshapes software design: systems must now
be modular, adaptive, and trust-aware from the outset.
  This chapter explores the intersection of game theory, agentic AI, and
cybersecurity. We review key game-theoretic frameworks (e.g., static, dynamic,
Bayesian, and signaling games) and solution concepts. We then examine how LLM
agents can enhance cyber defense and introduce LLM-driven games that embed
reasoning into AI agents. Finally, we explore multi-agent workflows and
coordination games, outlining how this convergence fosters secure, intelligent,
and adaptive cyber systems.

</details>


### [306] [Spectral Feature Extraction for Robust Network Intrusion Detection Using MFCCs](https://arxiv.org/abs/2507.10622)
*HyeYoung Lee,Muhammad Nadeem,Pavel Tsoi*

Main category: cs.CR

TL;DR: 提出了一种基于MFCC和ResNet-18的异常检测方法，用于IoT网络流量，通过自适应特征提取提升分类效果。


<details>
  <summary>Details</summary>
Motivation: IoT网络扩展导致安全漏洞增加，需要更强大的异常检测技术。

Method: 结合可学习的MFCC和ResNet-18，将原始信号转换为高维空间以增强分类能力。

Result: 在CICIoT2023、NSL-KDD和IoTID20数据集上验证了方法的有效性。

Conclusion: 自适应信号处理与深度学习结合，为异构IoT网络提供鲁棒且可扩展的异常检测方案。

Abstract: The rapid expansion of Internet of Things (IoT) networks has led to a surge
in security vulnerabilities, emphasizing the critical need for robust anomaly
detection and classification techniques. In this work, we propose a novel
approach for identifying anomalies in IoT network traffic by leveraging the
Mel-frequency cepstral coefficients (MFCC) and ResNet-18, a deep learning model
known for its effectiveness in feature extraction and image-based tasks.
Learnable MFCCs enable adaptive spectral feature representation, capturing the
temporal patterns inherent in network traffic more effectively than traditional
fixed MFCCs. We demonstrate that transforming raw signals into MFCCs maps the
data into a higher-dimensional space, enhancing class separability and enabling
more effective multiclass classification. Our approach combines the strengths
of MFCCs with the robust feature extraction capabilities of ResNet-18, offering
a powerful framework for anomaly detection. The proposed model is evaluated on
three widely used IoT intrusion detection datasets: CICIoT2023, NSL-KDD, and
IoTID20. The experimental results highlight the potential of integrating
adaptive signal processing techniques with deep learning architectures to
achieve robust and scalable anomaly detection in heterogeneous IoT network
landscapes.

</details>


### [307] [PhreshPhish: A Real-World, High-Quality, Large-Scale Phishing Website Dataset and Benchmark](https://arxiv.org/abs/2507.10854)
*Thomas Dalton,Hemanth Gowda,Girish Rao,Sachin Pargi,Alireza Hadj Khodabakhshi,Joseph Rombs,Stephan Jou,Manish Marwah*

Main category: cs.CR

TL;DR: 论文介绍了PhreshPhish数据集，解决了现有钓鱼网站数据集质量低、泄漏和基准率不真实的问题，并提供了基准测试套件。


<details>
  <summary>Details</summary>
Motivation: 钓鱼攻击威胁日益严重，但现有数据集质量差且存在泄漏问题，阻碍了机器学习在实时检测中的进展。

Method: 提出PhreshPhish数据集，规模更大、质量更高，并设计了基准测试套件，以减少泄漏、增加任务难度和多样性。

Result: PhreshPhish数据集显著优于现有公共数据集，提供了更真实的模型评估基准。

Conclusion: 该数据集和基准测试将促进钓鱼检测领域的标准化比较和进一步研究。

Abstract: Phishing remains a pervasive and growing threat, inflicting heavy economic
and reputational damage. While machine learning has been effective in real-time
detection of phishing attacks, progress is hindered by lack of large,
high-quality datasets and benchmarks. In addition to poor-quality due to
challenges in data collection, existing datasets suffer from leakage and
unrealistic base rates, leading to overly optimistic performance results. In
this paper, we introduce PhreshPhish, a large-scale, high-quality dataset of
phishing websites that addresses these limitations. Compared to existing public
datasets, PhreshPhish is substantially larger and provides significantly higher
quality, as measured by the estimated rate of invalid or mislabeled data
points. Additionally, we propose a comprehensive suite of benchmark datasets
specifically designed for realistic model evaluation by minimizing leakage,
increasing task difficulty, enhancing dataset diversity, and adjustment of base
rates more likely to be seen in the real world. We train and evaluate multiple
solution approaches to provide baseline performance on the benchmark sets. We
believe the availability of this dataset and benchmarks will enable realistic,
standardized model comparison and foster further advances in phishing
detection. The datasets and benchmarks are available on Hugging Face
(https://huggingface.co/datasets/phreshphish/phreshphish).

</details>


### [308] [MalCodeAI: Autonomous Vulnerability Detection and Remediation via Language Agnostic Code Reasoning](https://arxiv.org/abs/2507.10898)
*Jugal Gajjar,Kamalasankari Subramaniakuppusamy,Noha El Kachach*

Main category: cs.CR

TL;DR: MalCodeAI是一种语言无关的多阶段AI管道，用于自主代码安全分析和修复，结合代码分解和语义推理，支持14种编程语言，并在漏洞检测和修复方面表现优异。


<details>
  <summary>Details</summary>
Motivation: 传统漏洞检测工具的局限性及网络威胁日益复杂，需要新的软件系统安全方法。

Method: 使用Qwen2.5-Coder-3B-Instruct模型进行代码分解和语义推理，通过LoRA优化，分两阶段实现功能分解和漏洞检测修复。

Result: 验证损失低至0.397（功能分解）和0.199（漏洞检测），支持零样本泛化和CVSS风险评分，开发者评价高。

Conclusion: MalCodeAI为智能、可解释且以开发者为中心的软件安全解决方案提供了重要进展。

Abstract: The growing complexity of cyber threats and the limitations of traditional
vulnerability detection tools necessitate novel approaches for securing
software systems. We introduce MalCodeAI, a language-agnostic, multi-stage AI
pipeline for autonomous code security analysis and remediation. MalCodeAI
combines code decomposition and semantic reasoning using fine-tuned
Qwen2.5-Coder-3B-Instruct models, optimized through Low-Rank Adaptation (LoRA)
within the MLX framework, and delivers scalable, accurate results across 14
programming languages. In Phase 1, the model achieved a validation loss as low
as 0.397 for functional decomposition and summarization of code segments after
200 iterations, 6 trainable layers, and a learning rate of 2 x 10^(-5). In
Phase 2, for vulnerability detection and remediation, it achieved a best
validation loss of 0.199 using the same number of iterations and trainable
layers but with an increased learning rate of 4 x 10^(-5), effectively
identifying security flaws and suggesting actionable fixes. MalCodeAI supports
red-hat-style exploit tracing, CVSS-based risk scoring, and zero-shot
generalization to detect complex, zero-day vulnerabilities. In a qualitative
evaluation involving 15 developers, the system received high scores in
usefulness (mean 8.06/10), interpretability (mean 7.40/10), and readability of
outputs (mean 7.53/10), confirming its practical value in real-world
development workflows. This work marks a significant advancement toward
intelligent, explainable, and developer-centric software security solutions.

</details>


### [309] [Hashed Watermark as a Filter: Defeating Forging and Overwriting Attacks in Weight-based Neural Network Watermarking](https://arxiv.org/abs/2507.11137)
*Yuan Yao,Jin Song,Jian Jin*

Main category: cs.CR

TL;DR: NeuralMark是一种基于哈希水印滤波器的神经网络水印方法，能有效防御伪造和覆盖攻击，并适用于多种网络架构。


<details>
  <summary>Details</summary>
Motivation: 深度神经网络作为有价值的数字资产需要所有权保护，现有权重水印方法易受攻击。

Method: 利用哈希函数生成不可逆二进制水印，并通过滤波器选择嵌入参数，结合平均池化抵抗微调和剪枝攻击。

Result: 在13种卷积和Transformer架构中验证了方法的有效性和鲁棒性，覆盖图像分类和文本生成任务。

Conclusion: NeuralMark提供了一种安全、通用且鲁棒的神经网络水印解决方案。

Abstract: As valuable digital assets, deep neural networks necessitate robust ownership
protection, positioning neural network watermarking (NNW) as a promising
solution. Among various NNW approaches, weight-based methods are favored for
their simplicity and practicality; however, they remain vulnerable to forging
and overwriting attacks. To address those challenges, we propose NeuralMark, a
robust method built around a hashed watermark filter. Specifically, we utilize
a hash function to generate an irreversible binary watermark from a secret key,
which is then used as a filter to select the model parameters for embedding.
This design cleverly intertwines the embedding parameters with the hashed
watermark, providing a robust defense against both forging and overwriting
attacks. An average pooling is also incorporated to resist fine-tuning and
pruning attacks. Furthermore, it can be seamlessly integrated into various
neural network architectures, ensuring broad applicability. Theoretically, we
analyze its security boundary. Empirically, we verify its effectiveness and
robustness across 13 distinct Convolutional and Transformer architectures,
covering five image classification tasks and one text generation task. The
source codes are available at https://github.com/AIResearch-Group/NeuralMark.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [310] [Theory of Mind and Self-Disclosure to CUIs](https://arxiv.org/abs/2507.10773)
*Samuel Rhys Cox*

Main category: cs.HC

TL;DR: 探讨了自我表露在对话用户界面（CUI）中的重要性，以及如何通过表达不确定性和透明化CUI的推理来鼓励自我表露。


<details>
  <summary>Details</summary>
Motivation: 自我表露对心理健康很重要，但往往因担心他人反应而变得困难。研究旨在探索CUI如何通过社交线索促进自我表露。

Method: 通过讨论CUI中的社交线索，分析表达不确定性和透明化推理对自我表露的影响。

Result: 表达不确定性和透明化CUI的推理可以增强用户的信任，从而鼓励自我表露。

Conclusion: 通过改进CUI的设计，使其更透明和人性化，可以有效促进用户的自我表露行为。

Abstract: Self-disclosure is important to help us feel better, yet is often difficult.
This difficulty can arise from how we think people are going to react to our
self-disclosure. In this workshop paper, we briefly discuss self-disclosure to
conversational user interfaces (CUIs) in relation to various social cues. We
then, discuss how expressions of uncertainty or representation of a CUI's
reasoning could help encourage self-disclosure, by making a CUI's intended
"theory of mind" more transparent to users.

</details>


### [311] [React to This (RTT): A Nonverbal Turing Test for Embodied AI](https://arxiv.org/abs/2507.10812)
*Chuxuan Zhang,Yasaman Etesam,Angelica Lim*

Main category: cs.HC

TL;DR: 提出了一种测试具身AI代理交互意识和可信度的方法，特别是在人类将其推向极限的场景中。


<details>
  <summary>Details</summary>
Motivation: 探索机器是否能像人类一样反应，扩展了图灵测试的范围，引入非语言行为的测试。

Method: 提出了“React to This”（RTT）测试，用于评估非语言行为，并进行了初步实验。

Result: 初步实验结果展示了RTT测试的可行性和潜在价值。

Conclusion: RTT测试为评估AI代理的非语言反应能力提供了新方向。

Abstract: We propose an approach to test embodied AI agents for interaction awareness
and believability, particularly in scenarios where humans push them to their
limits. Turing introduced the Imitation Game as a way to explore the question:
"Can machines think?" The Total Turing Test later expanded this concept beyond
purely verbal communication, incorporating perceptual and physical interaction.
Building on this, we propose a new guiding question: "Can machines react?" and
introduce the React to This (RTT) test for nonverbal behaviors, presenting
results from an initial experiment.

</details>


### [312] [Role-Playing LLM-Based Multi-Agent Support Framework for Detecting and Addressing Family Communication Bias](https://arxiv.org/abs/2507.11210)
*Rushia Harada,Yuken Kimura,Keito Inoshita*

Main category: cs.HC

TL;DR: 研究探讨了LLM在家庭心理安全沟通中的应用，通过多智能体框架检测和改善亲子对话中的情感压抑与理想父母偏见。


<details>
  <summary>Details</summary>
Motivation: 传统指标常忽视家庭中的微妙心理动态，如理想父母偏见导致的情感压抑，需通过技术手段支持安全沟通。

Method: 构建日本亲子对话语料库，开发多智能体LLM框架，检测情感压抑与偏见，生成结构化反馈。

Result: 系统能适度准确检测情感压抑，生成高共情与实用性反馈，模拟对话显示情感表达改善。

Conclusion: 该框架有望促进家庭互动中的积极转变，支持情感表达与相互理解。

Abstract: Well-being in family settings involves subtle psychological dynamics that
conventional metrics often overlook. In particular, unconscious parental
expectations, termed ideal parent bias, can suppress children's emotional
expression and autonomy. This suppression, referred to as suppressed emotion,
often stems from well-meaning but value-driven communication, which is
difficult to detect or address from outside the family. Focusing on these
latent dynamics, this study explores Large Language Model (LLM)-based support
for psychologically safe family communication. We constructed a Japanese
parent-child dialogue corpus of 30 scenarios, each annotated with metadata on
ideal parent bias and suppressed emotion. Based on this corpus, we developed a
Role-Playing LLM-based multi-agent dialogue support framework that analyzes
dialogue and generates feedback. Specialized agents detect suppressed emotion,
describe implicit ideal parent bias in parental speech, and infer contextual
attributes such as the child's age and background. A meta-agent compiles these
outputs into a structured report, which is then passed to five selected expert
agents. These agents collaboratively generate empathetic and actionable
feedback through a structured four-step discussion process. Experiments show
that the system can detect categories of suppressed emotion with moderate
accuracy and produce feedback rated highly in empathy and practicality.
Moreover, simulated follow-up dialogues incorporating this feedback exhibited
signs of improved emotional expression and mutual understanding, suggesting the
framework's potential in supporting positive transformation in family
interactions.

</details>


<div id='cs.CE'></div>

# cs.CE [[Back]](#toc)

### [313] [Data-Driven Differential Evolution in Tire Industry Extrusion: Leveraging Surrogate Models](https://arxiv.org/abs/2507.11191)
*Eider Garate-Perez,Kerman López de Calle-Etxabe,Susana Ferreiro*

Main category: cs.CE

TL;DR: 提出了一种基于代理模型和数据驱动的方法，用于优化复杂工业过程，显著减少初始化和设置时间以及材料浪费。


<details>
  <summary>Details</summary>
Motivation: 工业过程优化在缺乏目标函数或约束的数学表述时具有挑战性，本研究旨在解决这一问题。

Method: 利用机器学习模型构建代理模型，并结合改进的差分进化算法（Data-Driven Differential Evolution with Multi-Level Penalty Functions and Surrogate Models）进行优化。

Result: 在轮胎制造挤出过程中，该方法比历史最佳配置减少了65%的初始化和设置时间，并显著减少了材料浪费。

Conclusion: 结合数据驱动建模和元启发式优化方法，为缺乏显式数学表述的工业过程提供了有效解决方案。

Abstract: The optimization of industrial processes remains a critical challenge,
particularly when no mathematical formulation of objective functions or
constraints is available. This study addresses this issue by proposing a
surrogate-based, data-driven methodology for optimizing complex real-world
manufacturing systems using only historical process data. Machine learning
models are employed to approximate system behavior and construct surrogate
models, which are integrated into a tailored metaheuristic approach:
Data-Driven Differential Evolution with Multi-Level Penalty Functions and
Surrogate Models, an adapted version of Differential Evolution suited to the
characteristics of the studied process. The methodology is applied to an
extrusion process in the tire manufacturing industry, with the goal of
optimizing initialization parameters to reduce waste and production time.
Results show that the surrogate-based optimization approach outperforms
historical best configurations, achieving a 65\% reduction in initialization
and setup time, while also significantly minimizing material waste. These
findings highlight the potential of combining data-driven modeling and
metaheuristic optimization for industrial processes where explicit formulations
are unavailable.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [314] [SPICEAssistant: LLM using SPICE Simulation Tools for Schematic Design of Switched-Mode Power Supplies](https://arxiv.org/abs/2507.10639)
*Simon Nau,Jan Krummenauer,André Zimmermann*

Main category: cs.AR

TL;DR: 论文探讨了大型语言模型（LLM）在电子设计自动化（EDA）中的应用，特别是开关模式电源（SMPS）设计，提出了SPICEAssistant框架以提升LLM的电路设计能力。


<details>
  <summary>Details</summary>
Motivation: 研究LLM在EDA领域的潜力，尤其是SMPS设计中面临的挑战，如无法直接解析SPICE仿真结果和多步骤设计过程。

Method: 提出SPICEAssistant框架，为LLM提供工具接口，使其能灵活与SPICE仿真器交互，优化电路设计。

Result: 通过256个问题的基准测试，发现仿真反馈显著提升LLM的SMPS设计能力，SPICEAssistant比GPT-4o性能提升约38%。

Conclusion: SPICEAssistant框架有效解决了LLM在EDA中的局限性，仿真反馈和多步迭代显著提升了设计能力。

Abstract: State-of-the-art large language models (LLMs) show high performance across a
wide range of tasks in many domains of science. In the field of electronic
design automation (EDA), it is yet to be determined to what extent they are
capable to understand, adapt, and dimension electronic circuits. This paper
focuses on the application of LLMs to switched-mode power supply (SMPS) design
on printed circuit boards (PCBs). Particular challenges for LLMs in this
context include their limited ability to interpret results from key simulation
tools like SPICE and the multi-step design process. To address these
challenges, we suggest SPICEAssistant, a framework that provides a broad
selection of tools to an LLM. The tools serve as an interface to SPICE,
allowing the LLM to interact flexibly with the simulator to estimate the impact
of its modifications to the circuit. To evaluate the performance of
SPICEAssistant, we defined a benchmark consisting of 256 questions testing the
ability to adapt circuit netlists to fulfil different SMPS design tasks. The
benchmarking results show that simulation feedback effectively improves SMPS
design capabilities of LLMs. An increasing number of simulation iterations
leads to enhanced performance. The SPICEAssistant framework significantly
outperforms the standalone LLM GPT-4o on the benchmark by approximately 38%.

</details>


### [315] [Elk: Exploring the Efficiency of Inter-core Connected AI Chips with Deep Learning Compiler Techniques](https://arxiv.org/abs/2507.11506)
*Yiqi Liu,Yuqi Xue,Noelle Crawford,Jilong Xue,Jian Huang*

Main category: cs.AR

TL;DR: Elk是一个深度学习编译器框架，通过联合优化计算、通信和I/O性能，最大化ICCA芯片的效率。


<details>
  <summary>Details</summary>
Motivation: 由于ICCA芯片在计算、通信和I/O之间存在性能权衡，探索其效率具有挑战性。

Method: Elk通过可配置参数构建全局权衡空间，采用新的归纳操作调度策略和成本感知的片上内存分配算法。

Result: Elk在ICCA芯片上实现了平均94%的理想性能，支持大型DL模型。

Conclusion: Elk不仅优化了ICCA芯片的效率，还能支持新ICCA芯片的架构设计空间探索。

Abstract: To meet the increasing demand of deep learning (DL) models, AI chips are
employing both off-chip memory (e.g., HBM) and high-bandwidth low-latency
interconnect for direct inter-core data exchange. However, it is not easy to
explore the efficiency of these inter-core connected AI (ICCA) chips, due to a
fundamental tussle among compute (per-core execution), communication
(inter-core data exchange), and I/O (off-chip data access).
  In this paper, we develop Elk, a DL compiler framework to maximize the
efficiency of ICCA chips by jointly trading off all the three performance
factors discussed above. Elk structures these performance factors into
configurable parameters and forms a global trade-off space in the DL compiler.
To systematically explore this space and maximize overall efficiency, Elk
employs a new inductive operator scheduling policy and a cost-aware on-chip
memory allocation algorithm. It generates globally optimized execution plans
that best overlap off-chip data loading and on-chip execution. To examine the
efficiency of Elk, we build a full-fledged emulator based on a real ICCA chip
IPU-POD4, and an ICCA chip simulator for sensitivity analysis with different
interconnect network topologies. Elk achieves 94% of the ideal roofline
performance of ICCA chips on average, showing the benefits of supporting large
DL models on ICCA chips. We also show Elk's capability of enabling architecture
design space exploration for new ICCA chip development.

</details>


### [316] [SystolicAttention: Fusing FlashAttention within a Single Systolic Array](https://arxiv.org/abs/2507.11331)
*Jiawei Lin,Guokai Chen,Yuanlong Li,Thomas Bourgeat*

Main category: cs.AR

TL;DR: 论文提出了一种名为FSA的新型脉动阵列架构，通过SystolicAttention调度算法，将FlashAttention完全运行在单一脉动阵列中，显著提升了计算效率。


<details>
  <summary>Details</summary>
Motivation: 当前基于脉动阵列的加速器在执行FlashAttention时面临效率低下的问题，主要由于频繁的数据交换和非矩阵操作的不适应性。

Method: 提出FSA架构和SystolicAttention调度算法，实现FlashAttention在单一脉动阵列中的高效运行。

Result: FSA在注意力计算效率上比AWS NeuronCore-v2和Google TPUv5e分别高出1.77倍和4.83倍，面积开销仅约10%。

Conclusion: FSA通过优化脉动阵列的调度和架构设计，显著提升了FlashAttention的执行效率，为Transformer模型的加速提供了新思路。

Abstract: Transformer models rely heavily on scaled dot-product attention (SDPA),
typically implemented using the FlashAttention algorithm. However, current
systolic-array-based accelerators face significant challenges when executing
FlashAttention. Systolic arrays can only achieve high utilization for
consecutive and large matrix multiplications. In contrast, FlashAttention
requires frequently interleaved matrix multiplications and softmax operations.
  The frequent data swaps between the systolic array and external vector units
result in low systolic array utilization. This is further exacerbated by the
fact that softmax involves numerous non-matrix operations, which are not
well-suited for systolic arrays. Moreover, the concurrent execution of matrix
multiplication on systolic arrays and softmax on vector units leads to register
file and SRAM port contention, further degrading performance.
  To overcome these limitations, we propose FSA, an enhanced systolic array
architecture that enables the entire FlashAttention algorithm to run entirely
within a single systolic array, eliminating the need for external vector units.
At the core of FSA is SystolicAttention, a novel scheduling algorithm that maps
FlashAttention operations onto systolic arrays with fine-grained, element-wise
overlap. This significantly improves array utilization while preserving the
original floating-point operation order to maintain numerical stability.
  We implement FSA in synthesizable RTL and evaluate its performance against
state-of-the-art commercial accelerators. Our results show that FSA achieves
1.77x and 4.83x higher attention FLOPs/s utilization compared to AWS
NeuronCore-v2 and Google TPUv5e, respectively, with only about 10% area
overhead.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [317] [Stochastic Entanglement Configuration for Constructive Entanglement Topologies in Quantum Machine Learning with Application to Cardiac MRI](https://arxiv.org/abs/2507.11401)
*Mehri Mehrnia,Mohammed S. M. Elbaz*

Main category: quant-ph

TL;DR: 提出了一种随机纠缠配置方法，用于生成多样化的纠缠拓扑，显著提升量子机器学习模型的性能。


<details>
  <summary>Details</summary>
Motivation: 当前固定纠缠拓扑方法无法适应任务需求，限制了量子模型超越经典模型的潜力。

Method: 通过随机二进制矩阵编码纠缠拓扑，利用纠缠密度和单量子比特约束进行可扩展探索。

Result: 在心脏MRI疾病分类任务中，16%的配置优于经典基线，最高准确率提升20%。

Conclusion: 随机纠缠配置方法具有鲁棒性和通用性，显著优于传统固定拓扑。

Abstract: Efficient entanglement strategies are essential for advancing variational
quantum circuits (VQCs) for quantum machine learning (QML). However, most
current approaches use fixed entanglement topologies that are not adaptive to
task requirements, limiting potential gains over classical models. We introduce
a novel stochastic entanglement configuration method that systematically
generates diverse entanglement topologies to identify a subspace of
constructive entanglement configurations, defined as entanglement topologies
that boost hybrid model performance (e.g., classification accuracy) beyond
classical baselines. Each configuration is encoded as a stochastic binary
matrix, denoting directed entanglement between qubits. This enables scalable
exploration of the hyperspace of candidate entanglement topologies using
entanglement density and per-qubit constraints as key metrics. We define
unconstrained and constrained sampling modes, controlling entanglement per
qubit. Using our method, 400 stochastic configurations were generated and
evaluated in a hybrid QML for cardiac MRI disease classification. We identified
64 (16%) novel constructive entanglement configurations that consistently
outperformed the classical baseline. Ensemble aggregation of top-performing
configurations achieved ~0.92 classification accuracy, exceeding the classical
model (~0.87) by over 5%. Compared to four conventional topologies (ring,
nearest neighbor, no entanglement, fully entangled), none surpassed the
classical baseline (maximum accuracy ~0.82), while our configurations delivered
up to ~20% higher accuracy. Thus, highlighting the robustness and
generalizability of the identified constructive entanglements.

</details>


### [318] [Formal Verification of Variational Quantum Circuits](https://arxiv.org/abs/2507.10635)
*Nicola Assolini,Luca Marzari,Isabella Mastroeni,Alessandra di Pierro*

Main category: quant-ph

TL;DR: 该论文首次深入研究了变分量子电路（VQC）的形式验证问题，提出了基于抽象解释的新语义框架，并分析了其复杂性和局限性。


<details>
  <summary>Details</summary>
Motivation: VQC在量子机器学习中广泛应用，但其对抗性输入的脆弱性尚未有形式化验证框架，本文旨在填补这一空白。

Method: 受深度学习抽象解释方法启发，提出基于区间的可达性技术，并引入新语义框架分析VQC的验证问题。

Result: 研究表明量子特性（如状态归一化）引入了变量依赖关系，挑战现有方法，新框架能有效定义和分析验证问题。

Conclusion: 论文为VQC的形式验证提供了理论基础和实践方法，并在标准验证基准上验证了其有效性。

Abstract: Variational quantum circuits (VQCs) are a central component of many quantum
machine learning algorithms, offering a hybrid quantum-classical framework
that, under certain aspects, can be considered similar to classical deep neural
networks. A shared aspect is, for instance, their vulnerability to adversarial
inputs, small perturbations that can lead to incorrect predictions. While
formal verification techniques have been extensively developed for classical
models, no comparable framework exists for certifying the robustness of VQCs.
Here, we present the first in-depth theoretical and practical study of the
formal verification problem for VQCs. Inspired by abstract interpretation
methods used in deep learning, we analyze the applicability and limitations of
interval-based reachability techniques in the quantum setting. We show that
quantum-specific aspects, such as state normalization, introduce inter-variable
dependencies that challenge existing approaches. We investigate these issues by
introducing a novel semantic framework based on abstract interpretation, where
the verification problem for VQCs can be formally defined, and its complexity
analyzed. Finally, we demonstrate our approach on standard verification
benchmarks.

</details>


<div id='cs.DS'></div>

# cs.DS [[Back]](#toc)

### [319] [Improved sampling algorithms and Poincaré inequalities for non-log-concave distributions](https://arxiv.org/abs/2507.11236)
*Yuchen He,Zhehan Lei,Jianan Shao,Chihao Zhang*

Main category: cs.DS

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: We study the problem of sampling from a distribution $\mu$ with density
$\propto e^{-V}$ for some potential function $V:\mathbb R^d\to \mathbb R$ with
query access to $V$ and $\nabla V$. We start with the following standard
assumptions:
  (1) The potential function $V$ is $L$-smooth.
  (2) The second moment $\mathbf{E}_{X\sim \mu}[\|X\|^2]\leq M$.
  Recently, He and Zhang (COLT'25) showed that the query complexity of sampling
from such distributions is at least
$\left(\frac{LM}{d\epsilon}\right)^{\Omega(d)}$ where $\epsilon$ is the desired
accuracy in total variation distance, and the Poincar\'e constant can be
arbitrarily large.
  Meanwhile, another common assumption in the study of diffusion based samplers
(see e.g., the work of Chen, Chewi, Li, Li, Salim and Zhang (ICLR'23))
strengthens the smoothness condition (1) to the following:
  (1*) The potential function of *every* distribution along the
Ornstein-Uhlenbeck process starting from $\mu$ is $L$-smooth.
  We show that under the assumptions (1*) and (2), the query complexity of
sampling from $\mu$ can be $\mathrm{poly}(L,d)\cdot
\left(\frac{Ld+M}{\epsilon^2}\right)^{\mathcal{O}(L+1)}$, which is polynomial
in $d$ and $\frac{1}{\epsilon}$ when $L=\mathcal{O}(1)$ and
$M=\mathrm{poly}(d)$. This improves the algorithm with quasi-polynomial query
complexity developed by Huang et al. (COLT'24). Our results imply that the
seemly moderate strengthening of the smoothness condition (1) to (1*) can lead
to an exponential gap in the query complexity of sampling algorithms.
  Moreover, we show that together with the assumption (1*) and the stronger
moment assumption that $\|X\|$ is $\lambda$-sub-Gaussian for $X\sim\mu$, the
Poincar\'e constant of $\mu$ is at most $\mathcal{O}(\lambda)^{2(L+1)}$. As an
application of our technique, we obtain improved estimate of the Poincar\'e
constant for mixture of Gaussians with the same covariance.

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [320] [Elevating 3D Models: High-Quality Texture and Geometry Refinement from a Low-Quality Model](https://arxiv.org/abs/2507.11465)
*Nuri Ryu,Jiyun Won,Jooeun Son,Minsu Gong,Joo-Haeng Lee,Sunghyun Cho*

Main category: cs.GR

TL;DR: Elevate3D框架通过HFS-SDEdit方法提升低质量3D资产的纹理和几何细节，解决了高质量3D资产稀缺的问题。


<details>
  <summary>Details</summary>
Motivation: 高质量3D资产稀缺且获取成本高，限制了计算机图形学和3D视觉的应用。

Method: 采用HFS-SDEdit方法逐视图优化纹理和几何，结合单目几何预测器提升几何细节。

Result: Elevate3D在3D模型细化中达到最先进质量，显著优于现有方法。

Conclusion: Elevate3D有效解决了高质量开源3D资产的稀缺问题，为相关领域提供了实用工具。

Abstract: High-quality 3D assets are essential for various applications in computer
graphics and 3D vision but remain scarce due to significant acquisition costs.
To address this shortage, we introduce Elevate3D, a novel framework that
transforms readily accessible low-quality 3D assets into higher quality. At the
core of Elevate3D is HFS-SDEdit, a specialized texture enhancement method that
significantly improves texture quality while preserving the appearance and
geometry while fixing its degradations. Furthermore, Elevate3D operates in a
view-by-view manner, alternating between texture and geometry refinement.
Unlike previous methods that have largely overlooked geometry refinement, our
framework leverages geometric cues from images refined with HFS-SDEdit by
employing state-of-the-art monocular geometry predictors. This approach ensures
detailed and accurate geometry that aligns seamlessly with the enhanced
texture. Elevate3D outperforms recent competitors by achieving state-of-the-art
quality in 3D model refinement, effectively addressing the scarcity of
high-quality open-source 3D assets.

</details>


<div id='cs.GT'></div>

# cs.GT [[Back]](#toc)

### [321] [Protocols for Verifying Smooth Strategies in Bandits and Games](https://arxiv.org/abs/2507.10567)
*Miranda Christ,Daniel Reichman,Jonathan Shafer*

Main category: cs.GT

TL;DR: 研究在多臂老虎机和正规形式游戏中验证策略近似最优性的协议，要求查询次数少于动作数量，适用于平滑策略。


<details>
  <summary>Details</summary>
Motivation: 由于玩家可选动作数量庞大，需要减少对效用查询的次数，以高效验证策略的最优性。

Method: 提出验证平滑策略ε-最优性的协议，证明其查询次数少于学习过程，并建立查询复杂度的下限。

Result: 验证协议在查询次数上优于学习，且适用于正规形式游戏的近似强平滑纳什均衡验证。

Conclusion: 验证协议在多臂老虎机和正规形式游戏中高效，查询复杂度低于动作数量。

Abstract: We study protocols for verifying approximate optimality of strategies in
multi-armed bandits and normal-form games. As the number of actions available
to each player is often large, we seek protocols where the number of queries to
the utility oracle is sublinear in the number of actions. We prove that such
verification is possible for sufficiently smooth strategies that do not put too
much probability mass on any specific action. We provide protocols for
verifying that a smooth policy for a multi-armed bandit is
$\varepsilon$-optimal. Our verification protocols require provably fewer arm
queries than learning. Furthermore, we establish a nearly-tight lower bound on
the query complexity of verification in our settings. As an application, we
show how to use verification for bandits to achieve verification in normal-form
games. This gives a protocol for verifying whether a given strategy profile is
an approximate strong smooth Nash equilibrium, with a query complexity that is
sublinear in the number of actions.

</details>


### [322] [A Parallelizable Approach for Characterizing NE in Zero-Sum Games After a Linear Number of Iterations of Gradient Descent](https://arxiv.org/abs/2507.11366)
*Taemin Kim,James P. Bailey*

Main category: cs.GT

TL;DR: 提出一种基于哈密顿动力学的新方法，用于零和博弈的在线优化，首次在无界设置中以线性迭代次数逼近纳什均衡，并支持并行化和任意学习率。


<details>
  <summary>Details</summary>
Motivation: 研究零和博弈的在线优化方法，传统方法存在局限性，需要更高效且通用的解决方案。

Method: 基于哈密顿动力学设计新方法，通过交替梯度下降在无界设置中逼近纳什均衡。

Result: 新方法在有限线性迭代次数内逼近纳什均衡，支持并行化和任意学习率，实验表现显著优于传统方法。

Conclusion: 新方法在理论和实验上均优于传统方法，为零和博弈的在线优化提供了更高效的解决方案。

Abstract: We study online optimization methods for zero-sum games, a fundamental
problem in adversarial learning in machine learning, economics, and many other
domains. Traditional methods approximate Nash equilibria (NE) using either
regret-based methods (time-average convergence) or contraction-map-based
methods (last-iterate convergence). We propose a new method based on
Hamiltonian dynamics in physics and prove that it can characterize the set of
NE in a finite (linear) number of iterations of alternating gradient descent in
the unbounded setting, modulo degeneracy, a first in online optimization.
Unlike standard methods for computing NE, our proposed approach can be
parallelized and works with arbitrary learning rates, both firsts in
algorithmic game theory. Experimentally, we support our results by showing our
approach drastically outperforms standard methods.

</details>


### [323] [Better Regret Rates in Bilateral Trade via Sublinear Budget Violation](https://arxiv.org/abs/2507.11419)
*Anna Lunghi,Matteo Castiglioni,Alberto Marchesi*

Main category: cs.GT

TL;DR: 本文研究了双边贸易中预算平衡约束与遗憾率之间的权衡关系，通过设计算法在不同约束条件下实现最优遗憾率。


<details>
  <summary>Details</summary>
Motivation: 探索在双边贸易中如何通过放松预算平衡约束来优化遗憾率，填补了严格约束和无约束之间的研究空白。

Method: 设计了一种算法，允许预算平衡约束在一定范围内违反（$T^{\beta}$），并分析其对遗憾率的影响。

Result: 算法在预算违反$T^{\beta}$时，遗憾率为$\tilde O(T^{1 - \beta/3})$，并提供了匹配的下界。

Conclusion: 研究证明了Bernasconi等人的上下界结果在特定约束条件下是紧的，并全面刻画了遗憾率与预算违反之间的权衡关系。

Abstract: Bilateral trade is a central problem in algorithmic economics, and recent
work has explored how to design trading mechanisms using no-regret learning
algorithms. However, no-regret learning is impossible when budget balance has
to be enforced at each time step. Bernasconi et al. [Ber+24] show how this
impossibility can be circumvented by relaxing the budget balance constraint to
hold only globally over all time steps. In particular, they design an algorithm
achieving regret of the order of $\tilde O(T^{3/4})$ and provide a lower bound
of $\Omega(T^{5/7})$.
  In this work, we interpolate between these two extremes by studying how the
optimal regret rate varies with the allowed violation of the global budget
balance constraint. Specifically, we design an algorithm that, by violating the
constraint by at most $T^{\beta}$ for any given $\beta \in [\frac{3}{4},
\frac{6}{7}]$, attains regret $\tilde O(T^{1 - \beta/3})$. We complement this
result with a matching lower bound, thus fully characterizing the trade-off
between regret and budget violation. Our results show that both the $\tilde
O(T^{3/4})$ upper bound in the global budget balance case and the
$\Omega(T^{5/7})$ lower bound under unconstrained budget balance violation
obtained by Bernasconi et al. [Ber+24] are tight.

</details>


<div id='physics.app-ph'></div>

# physics.app-ph [[Back]](#toc)

### [324] [Real-time, Adaptive Radiological Anomaly Detection and Isotope Identification Using Non-negative Matrix Factorization](https://arxiv.org/abs/2507.10715)
*Chandler Jones,Mark Bandstra,Stefan Faaland,Yue Shi Lai,Nico Abgrall,Scott Suchyta,Reynold Cooper*

Main category: physics.app-ph

TL;DR: 提出了一种自适应非负矩阵分解（NMF）算法，用于动态更新背景模型以适应环境变化，提升核不扩散应用中的光谱异常检测和同位素识别性能。


<details>
  <summary>Details</summary>
Motivation: 移动探测器系统因背景变化导致传统NMF算法性能下降，需改进以适应动态环境。

Method: 开发自适应NMF算法，定期更新背景模型，减少环境假设，提高泛化能力。

Result: 在模拟和真实数据上保持或超越现有NMF方法的检测性能。

Conclusion: 自适应NMF算法在动态环境中表现更优，适用于核不扩散应用。

Abstract: Spectroscopic anomaly detection and isotope identification algorithms are
integral components in nuclear nonproliferation applications such as search
operations. The task is especially challenging in the case of mobile detector
systems due to the fact that the observed gamma-ray background changes more
than for a static detector system, and a pretrained background model can easily
find itself out of domain. The result is that algorithms may exceed their
intended false alarm rate, or sacrifice detection sensitivity in order to
maintain the desired false alarm rate. Non-negative matrix factorization (NMF)
has been shown to be a powerful tool for spectral anomaly detection and
identification, but, like many similar algorithms that rely on data-driven
background models, in its conventional implementation it is unable to update in
real time to account for environmental changes that affect the background
spectroscopic signature. We have developed a novel NMF-based algorithm that
periodically updates its background model to accommodate changing environmental
conditions. The Adaptive NMF algorithm involves fewer assumptions about its
environment, making it more generalizable than existing NMF-based methods while
maintaining or exceeding detection performance on simulated and real-world
datasets.

</details>


<div id='math.PR'></div>

# math.PR [[Back]](#toc)

### [325] [Neural Expectation Operators](https://arxiv.org/abs/2507.10607)
*Qian Qi*

Main category: math.PR

TL;DR: 论文提出了一种通过非线性期望建模模糊性的方法——Measure Learning，利用神经网络参数化的BSDEs定义Neural Expectation Operators，并证明了局部Lipschitz条件下的BSDEs解的存在性和唯一性。


<details>
  <summary>Details</summary>
Motivation: 传统BSDEs理论中的全局Lipschitz假设限制了其在机器学习中的应用，论文旨在通过局部Lipschitz条件和神经网络设计，构建理论与实践的桥梁。

Method: 定义Neural Expectation Operators为BSDEs的解，其驱动项由神经网络参数化；通过局部Lipschitz条件和二次增长假设，证明解的适定性。

Result: 证明了在局部Lipschitz条件下BSDEs的适定性，适用于ReLU等常见神经网络架构，并扩展至完全耦合FBSDEs和大规模交互粒子系统的渐近分析。

Conclusion: 论文为模糊性下的数据驱动建模提供了数学框架，并通过神经网络设计实现了理论假设的具体化。

Abstract: This paper introduces \textbf{Measure Learning}, a paradigm for modeling
ambiguity via non-linear expectations. We define Neural Expectation Operators
as solutions to Backward Stochastic Differential Equations (BSDEs) whose
drivers are parameterized by neural networks. The main mathematical
contribution is a rigorous well-posedness theorem for BSDEs whose drivers
satisfy a local Lipschitz condition in the state variable $y$ and quadratic
growth in its martingale component $z$. This result circumvents the classical
global Lipschitz assumption, is applicable to common neural network
architectures (e.g., with ReLU activations), and holds for exponentially
integrable terminal data, which is the sharp condition for this setting. Our
primary innovation is to build a constructive bridge between the abstract, and
often restrictive, assumptions of the deep theory of quadratic BSDEs and the
world of machine learning, demonstrating that these conditions can be met by
concrete, verifiable neural network designs. We provide constructive methods
for enforcing key axiomatic properties, such as convexity, by architectural
design. The theory is extended to the analysis of fully coupled
Forward-Backward SDE systems and to the asymptotic analysis of large
interacting particle systems, for which we establish both a Law of Large
Numbers (propagation of chaos) and a Central Limit Theorem. This work provides
the foundational mathematical framework for data-driven modeling under
ambiguity.

</details>


<div id='cond-mat.str-el'></div>

# cond-mat.str-el [[Back]](#toc)

### [326] [Functional Neural Wavefunction Optimization](https://arxiv.org/abs/2507.10835)
*Victor Armegioiu,Juan Carrasquilla,Siddhartha Mishra,Johannes Müller,Jannes Nys,Marius Zeinhofer,Hang Zhang*

Main category: cond-mat.str-el

TL;DR: 提出了一种基于几何视角的变分量子蒙特卡洛优化算法设计与分析框架，将无限维优化动态转化为参数空间算法，统一了现有方法并推导出新算法。


<details>
  <summary>Details</summary>
Motivation: 通过几何视角理解变分量子蒙特卡洛中的优化问题，统一现有方法并开发更高效的算法。

Method: 利用Galerkin投影将无限维优化动态映射到变分ansatz的切空间，转化为参数空间算法。

Result: 数值实验验证了框架的实用性，成功估计了凝聚态物理中多个原型模型的基态能量。

Conclusion: 该框架为变分量子蒙特卡洛优化提供了统一的几何视角，并展示了其在实际问题中的应用潜力。

Abstract: We propose a framework for the design and analysis of optimization algorithms
in variational quantum Monte Carlo, drawing on geometric insights into the
corresponding function space. The framework translates infinite-dimensional
optimization dynamics into tractable parameter-space algorithms through a
Galerkin projection onto the tangent space of the variational ansatz. This
perspective unifies existing methods such as stochastic reconfiguration and
Rayleigh-Gauss-Newton, provides connections to classic function-space
algorithms, and motivates the derivation of novel algorithms with geometrically
principled hyperparameter choices. We validate our framework with numerical
experiments demonstrating its practical relevance through the accurate
estimation of ground-state energies for several prototypical models in
condensed matter physics modeled with neural network wavefunctions.

</details>


<div id='physics.geo-ph'></div>

# physics.geo-ph [[Back]](#toc)

### [327] [HEIMDALL: a grapH-based sEIsMic Detector And Locator for microseismicity](https://arxiv.org/abs/2507.10850)
*Matteo Bagagli,Francesco Grigoli,Davide Bacciu*

Main category: physics.geo-ph

TL;DR: 提出了一种基于图神经网络的新型深度学习模型，用于微地震监测，通过连续时空关系实现端到端的地震目录生成。


<details>
  <summary>Details</summary>
Motivation: 随着绿色能源转型中对增强地热系统的兴趣增加，需要更高效的微地震监测工具以减少碳排放。

Method: 利用图理论和图神经网络架构，在滚动窗口内同时进行相位拾取、关联和事件定位。

Result: 在冰岛Hengill地区测试中，模型显著提高了事件检测率，减少了误报事件和人工干预需求。

Conclusion: 该模型为地热地震区域提供了一种稳健的监测工具，补充了现有系统并降低了地热能源开发中的操作风险。

Abstract: In this work, we present a new deep-learning model for microseismicity
monitoring that utilizes continuous spatiotemporal relationships between
seismic station recordings, forming an end-to-end pipeline for seismic catalog
creation. It employs graph theory and state-of-the-art graph neural network
architectures to perform phase picking, association, and event location
simultaneously over rolling windows, making it suitable for both playback and
near-real-time monitoring. As part of the global strategy to reduce carbon
emissions within the broader context of a green-energy transition, there has
been growing interest in exploiting enhanced geothermal systems. Tested in the
complex geothermal area of Iceland's Hengill region using open-access data from
a temporary experiment, our model was trained and validated using both manually
revised and automatic seismic catalogs. Results showed a significant increase
in event detection compared to previously published automatic systems and
reference catalogs, including a $4 M_w$ seismic sequence in December 2018 and a
single-day sequence in February 2019. Our method reduces false events,
minimizes manual oversight, and decreases the need for extensive tuning of
pipelines or transfer learning of deep-learning models. Overall, it validates a
robust monitoring tool for geothermal seismic regions, complementing existing
systems and enhancing operational risk mitigation during geothermal energy
exploitation.

</details>


<div id='econ.GN'></div>

# econ.GN [[Back]](#toc)

### [328] [Artificial Finance: How AI Thinks About Money](https://arxiv.org/abs/2507.10933)
*Orhan Erdem,Ragavi Pobbathi Ashok*

Main category: econ.GN

TL;DR: 论文比较了大型语言模型（LLMs）与全球人类在金融决策上的表现，发现LLMs倾向于风险中性决策，偶尔与规范性推理不一致，且其响应与坦桑尼亚参与者最相似。


<details>
  <summary>Details</summary>
Motivation: 探讨LLMs在金融决策中如何模拟人类行为，并揭示其潜在的文化和训练影响。

Method: 向七个领先的LLMs和来自53个国家的人类参与者提出金融决策问题，比较其响应。

Result: LLMs表现出风险中性决策模式，偶尔与规范性推理不一致，且其响应与坦桑尼亚参与者最相似。

Conclusion: 研究揭示了LLMs模拟人类决策行为的方式及其潜在的文化和训练影响。

Abstract: In this paper, we explore how large language models (LLMs) approach financial
decision-making by systematically comparing their responses to those of human
participants across the globe. We posed a set of commonly used financial
decision-making questions to seven leading LLMs, including five models from the
GPT series(GPT-4o, GPT-4.5, o1, o3-mini), Gemini 2.0 Flash, and DeepSeek R1. We
then compared their outputs to human responses drawn from a dataset covering 53
nations. Our analysis reveals three main results. First, LLMs generally exhibit
a risk-neutral decision-making pattern, favoring choices aligned with expected
value calculations when faced with lottery-type questions. Second, when
evaluating trade-offs between present and future, LLMs occasionally produce
responses that appear inconsistent with normative reasoning. Third, when we
examine cross-national similarities, we find that the LLMs' aggregate responses
most closely resemble those of participants from Tanzania. These findings
contribute to the understanding of how LLMs emulate human-like decision
behaviors and highlight potential cultural and training influences embedded
within their outputs.

</details>


<div id='physics.chem-ph'></div>

# physics.chem-ph [[Back]](#toc)

### [329] [BioScore: A Foundational Scoring Function For Diverse Biomolecular Complexes](https://arxiv.org/abs/2507.10877)
*Yuchen Zhu,Jihong Chen,Yitong Li,Xiaomin Fang,Xianbin Ye,Jingzhou He,Xujun Zhang,Jingxuan Ge,Chao Shen,Xiaonan Zhang,Tingjun Hou,Chang-Yu Hsieh*

Main category: physics.chem-ph

TL;DR: BioScore是一种基于双尺度几何图学习的评分函数，解决了数据稀疏性、跨系统表示和任务兼容性等挑战，在多种生物分子系统中表现优异。


<details>
  <summary>Details</summary>
Motivation: 当前基于结构的评分函数在多样化的生物分子系统中缺乏通用性，限制了其功能理解和药物发现的应用。

Method: BioScore采用双尺度几何图学习框架，包含结构评估和亲和力预测的定制模块。

Result: 在16个基准测试中，BioScore优于或匹配70种传统和深度学习方法，并在蛋白质-蛋白质亲和力预测等方面表现突出。

Conclusion: BioScore为复杂生物分子景观的结构评估提供了一个稳健且通用的框架。

Abstract: Structural assessment of biomolecular complexes is vital for translating
molecular models into functional insights, shaping our understanding of biology
and aiding drug discovery. However, current structure-based scoring functions
often lack generalizability across diverse biomolecular systems. We present
BioScore, a foundational scoring function that addresses key challenges -- data
sparsity, cross-system representation, and task compatibility -- through a
dual-scale geometric graph learning framework with tailored modules for
structure assessment and affinity prediction. BioScore supports a wide range of
tasks, including affinity prediction, conformation ranking, and structure-based
virtual screening. Evaluated on 16 benchmarks spanning proteins, nucleic acids,
small molecules, and carbohydrates, BioScore consistently outperforms or
matches 70 traditional and deep learning methods. Our newly proposed PPI
Benchmark further enables comprehensive evaluation of protein-protein complex
scoring. BioScore demonstrates broad applicability: (1) pretraining on
mixed-structure data boosts protein-protein affinity prediction by up to 40%
and antigen-antibody binding correlation by over 90%; (2) cross-system
generalizability enables zero- and few-shot prediction with up to 71%
correlation gain; and (3) its unified representation captures chemically
challenging systems such as cyclic peptides, improving affinity prediction by
over 60%. BioScore establishes a robust and generalizable framework for
structural assessment across complex biomolecular landscapes.

</details>


<div id='math-ph'></div>

# math-ph [[Back]](#toc)

### [330] [From Kinetic Theory to AI: a Rediscovery of High-Dimensional Divergences and Their Properties](https://arxiv.org/abs/2507.11387)
*Gennaro Auricchio,Giovanni Brigati,Paolo Giudici,Giuseppe Toscani*

Main category: math-ph

TL;DR: 本文比较了基于动力学理论的散度度量，探讨其在机器学习和人工智能中的潜在应用。


<details>
  <summary>Details</summary>
Motivation: 选择适当的散度度量对机器学习模型性能至关重要，而动力学理论中的散度度量（如KL散度）在此领域有重要应用。

Method: 通过比较动力学理论中的散度度量，分析其理论基础。

Result: 总结了动力学理论中散度度量的特点及其在机器学习和人工智能中的适用性。

Conclusion: 动力学理论中的散度度量对机器学习和人工智能具有潜在价值，值得进一步研究。

Abstract: Selecting an appropriate divergence measure is a critical aspect of machine
learning, as it directly impacts model performance. Among the most widely used,
we find the Kullback-Leibler (KL) divergence, originally introduced in kinetic
theory as a measure of relative entropy between probability distributions. Just
as in machine learning, the ability to quantify the proximity of probability
distributions plays a central role in kinetic theory. In this paper, we present
a comparative review of divergence measures rooted in kinetic theory,
highlighting their theoretical foundations and exploring their potential
applications in machine learning and artificial intelligence.

</details>


<div id='q-bio.QM'></div>

# q-bio.QM [[Back]](#toc)

### [331] [AGFS-Tractometry: A Novel Atlas-Guided Fine-Scale Tractometry Approach for Enhanced Along-Tract Group Statistical Comparison Using Diffusion MRI Tractography](https://arxiv.org/abs/2507.10601)
*Ruixi Zheng,Wei Zhang,Yijie Li,Xi Zhu,Zhou Lan,Jarrett Rushmore,Yogesh Rathi,Nikos Makris,Lauren J. O'Donnell,Fan Zhang*

Main category: q-bio.QM

TL;DR: 提出了一种新型的基于图谱引导的细尺度纤维束分析方法AGFS-Tractometry，通过结合空间信息和置换检验，提高了纤维束沿线的统计分析能力。


<details>
  <summary>Details</summary>
Motivation: 当前纤维束分析方法在检测局部白质差异时存在灵敏度与特异性不足的问题，需要更精细的统计方法。

Method: 1. 创建图谱引导的纤维束分析模板，实现一致的细尺度分割；2. 提出非参数置换检验方法，支持多比较校正。

Result: 在合成和真实数据实验中，AGFS-Tractometry表现出更高的灵敏度和特异性，能检测更多解剖学一致的显著差异区域。

Conclusion: AGFS-Tractometry能有效检测细微或局部白质差异，其模板和代码已开源。

Abstract: Diffusion MRI (dMRI) tractography is currently the only method for in vivo
mapping of the brain's white matter (WM) connections. Tractometry is an
advanced tractography analysis technique for along-tract profiling to
investigate the morphology and microstructural properties along the fiber
tracts. Tractometry has become an essential tool for studying local along-tract
differences between different populations (e.g., health vs disease). In this
study, we propose a novel atlas-guided fine-scale tractometry method, namely
AGFS-Tractometry, that leverages tract spatial information and permutation
testing to enhance the along-tract statistical analysis between populations.
There are two major contributions in AGFS-Tractometry. First, we create a novel
atlas-guided tract profiling template that enables consistent, fine-scale,
along-tract parcellation of subject-specific fiber tracts. Second, we propose a
novel nonparametric permutation testing group comparison method to enable
simultaneous analysis across all along-tract parcels while correcting for
multiple comparisons. We perform experimental evaluations on synthetic datasets
with known group differences and in vivo real data. We compare AGFS-Tractometry
with two state-of-the-art tractometry methods, including Automated Fiber-tract
Quantification (AFQ) and BUndle ANalytics (BUAN). Our results show that the
proposed AGFS-Tractometry obtains enhanced sensitivity and specificity in
detecting local WM differences. In the real data analysis experiments,
AGFS-Tractometry can identify more regions with significant differences, which
are anatomically consistent with the existing literature. Overall, these
demonstrate the ability of AGFS-Tractometry to detect subtle or spatially
localized WM group-level differences. The created tract profiling template and
related code are available at:
https://github.com/ZhengRuixi/AGFS-Tractometry.git.

</details>


<div id='physics.med-ph'></div>

# physics.med-ph [[Back]](#toc)

### [332] [Quantitative multi-metabolite imaging of Parkinson's disease using AI boosted molecular MRI](https://arxiv.org/abs/2507.11329)
*Hagar Shmuely,Michal Rivlin,Or Perlman*

Main category: physics.med-ph

TL;DR: 该论文提出了一种结合快速分子MRI采集与深度学习重建的方法，用于多代谢物定量分析帕金森病（PD）模型中的代谢物，并验证了其作为PD生物标志物的潜力。


<details>
  <summary>Details</summary>
Motivation: 传统PD分子成像方法存在放射性、耗时或分辨率低的问题，而现有的MRI技术虽提供生化信息，但对比度半定量且非特异性。

Method: 结合快速分子MRI采集与深度学习重建，定量分析MPTP小鼠模型中的谷氨酸、移动蛋白、半固体及移动大分子。

Result: 定量参数图与组织学和MR光谱学结果一致，表明半固体MT、酰胺及脂肪族rNOE质子体积分数可作为PD生物标志物。

Conclusion: 该方法为PD分子成像提供了快速、定量且高分辨率的解决方案，并验证了潜在生物标志物。

Abstract: Traditional approaches for molecular imaging of Parkinson's disease (PD) in
vivo require radioactive isotopes, lengthy scan times, or deliver only low
spatial resolution. Recent advances in saturation transfer-based PD magnetic
resonance imaging (MRI) have provided biochemical insights, although the image
contrast is semi-quantitative and nonspecific. Here, we combined a rapid
molecular MRI acquisition paradigm with deep learning based reconstruction for
multi-metabolite quantification of glutamate, mobile proteins, semisolid, and
mobile macromolecules in an acute MPTP
(1-methyl-4-phenyl-1,2,3,6-tetrahydropyridine) mouse model. The quantitative
parameter maps are in general agreement with the histology and MR spectroscopy,
and demonstrate that semisolid magnetization transfer (MT), amide, and
aliphatic relayed nuclear Overhauser effect (rNOE) proton volume fractions may
serve as PD biomarkers.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [333] [LiLM-RDB-SFC: Lightweight Language Model with Relational Database-Guided DRL for Optimized SFC Provisioning](https://arxiv.org/abs/2507.10903)
*Parisa Fard Moshiri,Xinyu Zhu,Poonam Lohan,Burak Kantarci,Emil Janulewicz*

Main category: cs.NI

TL;DR: 论文提出LiLM-RDB-SFC方法，结合轻量级语言模型（LiLM）和关系数据库（RDB），通过查询网络状态指导DRL模型优化SFC配置。FLAN-T5表现优于BART和SQLCoder。


<details>
  <summary>Details</summary>
Motivation: 解决DRL在动态网络决策中因依赖结构化数据和固定规则而受限的问题，提升SFC和VNF管理的适应性和响应能力。

Method: 使用两种LiLM（BART和FLAN-T5）解析网络数据，支持多样化的SFC需求、数据中心资源和VNF可用性查询。

Result: FLAN-T5在测试损失（0.00161 vs 0.00734）、准确率（94.79% vs 80.2%）和处理时间（2h 2min vs 2h 38min）上优于BART，且处理时间比SQLCoder减少96%。

Conclusion: LiLM-RDB-SFC方法通过FLAN-T5显著提升了SFC配置的效率和准确性，适用于动态网络环境。

Abstract: Effective management of Service Function Chains (SFCs) and optimal Virtual
Network Function (VNF) placement are critical challenges in modern
Software-Defined Networking (SDN) and Network Function Virtualization (NFV)
environments. Although Deep Reinforcement Learning (DRL) is widely adopted for
dynamic network decision-making, its inherent dependency on structured data and
fixed action rules often limits adaptability and responsiveness, particularly
under unpredictable network conditions. This paper introduces LiLM-RDB-SFC, a
novel approach combining Lightweight Language Model (LiLM) with Relational
Database (RDB) to answer network state queries to guide DRL model for efficient
SFC provisioning. Our proposed approach leverages two LiLMs, Bidirectional and
Auto-Regressive Transformers (BART) and the Fine-tuned Language Net T5
(FLAN-T5), to interpret network data and support diverse query types related to
SFC demands, data center resources, and VNF availability. Results demonstrate
that FLAN-T5 outperforms BART with a lower test loss (0.00161 compared to
0.00734), higher accuracy (94.79% compared to 80.2%), and less processing time
(2h 2min compared to 2h 38min). Moreover, when compared to the large language
model SQLCoder, FLAN-T5 matches the accuracy of SQLCoder while cutting
processing time by 96% (SQLCoder: 54 h 43 min; FLAN-T5: 2 h 2 min).

</details>


### [334] [Improving Wi-Fi Network Performance Prediction with Deep Learning Models](https://arxiv.org/abs/2507.11168)
*Gabriele Formis,Amanda Ericson,Stefan Forsstrom,Kyi Thar,Gianluca Cena,Stefano Scanzio*

Main category: cs.NI

TL;DR: 论文利用机器学习预测Wi-Fi网络的帧传输率，优化工业应用中的网络操作。


<details>
  <summary>Details</summary>
Motivation: 工业与关键任务应用对无线网络的鲁棒性、可靠性和确定性需求增长。

Method: 使用卷积神经网络和长短期记忆网络分析真实Wi-Fi数据集，比较预测准确性和计算复杂度。

Result: 帧传输率可可靠预测，卷积神经网络在CPU和内存使用上更高效。

Conclusion: 卷积神经网络更适合嵌入式与工业系统应用。

Abstract: The increasing need for robustness, reliability, and determinism in wireless
networks for industrial and mission-critical applications is the driver for the
growth of new innovative methods. The study presented in this work makes use of
machine learning techniques to predict channel quality in a Wi-Fi network in
terms of the frame delivery ratio. Predictions can be used proactively to
adjust communication parameters at runtime and optimize network operations for
industrial applications. Methods including convolutional neural networks and
long short-term memory were analyzed on datasets acquired from a real Wi-Fi
setup across multiple channels. The models were compared in terms of prediction
accuracy and computational complexity. Results show that the frame delivery
ratio can be reliably predicted, and convolutional neural networks, although
slightly less effective than other models, are more efficient in terms of CPU
usage and memory consumption. This enhances the model's usability on embedded
and industrial systems.

</details>


<div id='math.OC'></div>

# math.OC [[Back]](#toc)

### [335] [Deep Equilibrium models for Poisson Imaging Inverse problems via Mirror Descent](https://arxiv.org/abs/2507.11461)
*Christian Daniele,Silvia Villa,Samuel Vaiter,Luca Calatroni*

Main category: math.OC

TL;DR: 论文提出了一种基于镜像下降的深度均衡模型（DEQ），用于解决泊松逆问题，通过非欧几里得几何结构适应数据项，性能优于传统方法，且避免了Bregman Plug-and-Play方法的缺点。


<details>
  <summary>Details</summary>
Motivation: 泊松逆问题中的数据保真项更适合用Kullback-Leibler散度建模，但现有方法（如DEQ）主要针对高斯保真项，因此需要扩展DEQ以处理泊松问题。

Method: 提出了一种基于镜像下降的DEQ新形式，采用非欧几里得几何结构，确保收敛性，并设计了高效训练和无参数推理策略。

Result: 数值实验表明，该方法优于传统模型方法，性能与Bregman Plug-and-Play方法相当，但避免了其对初始化和超参数的敏感性。

Conclusion: 该方法为泊松逆问题提供了一种高效、鲁棒的解决方案，代码已开源。

Abstract: Deep Equilibrium Models (DEQs) are implicit neural networks with fixed
points, which have recently gained attention for learning image regularization
functionals, particularly in settings involving Gaussian fidelities, where
assumptions on the forward operator ensure contractiveness of standard
(proximal) Gradient Descent operators. In this work, we extend the application
of DEQs to Poisson inverse problems, where the data fidelity term is more
appropriately modeled by the Kullback-Leibler divergence. To this end, we
introduce a novel DEQ formulation based on Mirror Descent defined in terms of a
tailored non-Euclidean geometry that naturally adapts with the structure of the
data term. This enables the learning of neural regularizers within a principled
training framework. We derive sufficient conditions to guarantee the
convergence of the learned reconstruction scheme and propose computational
strategies that enable both efficient training and fully parameter-free
inference. Numerical experiments show that our method outperforms traditional
model-based approaches and it is comparable to the performance of Bregman
Plug-and-Play methods, while mitigating their typical drawbacks - namely,
sensitivity to initialization and careful tuning of hyperparameters. The code
is publicly available at https://github.com/christiandaniele/DEQ-MD.

</details>


### [336] [A Mathematical Optimization Approach to Multisphere Support Vector Data Description](https://arxiv.org/abs/2507.11106)
*Víctor Blanco,Inmaculada Espejo,Raúl Páez,Antonio M. Rodríguez-Chía*

Main category: math.OC

TL;DR: 提出了一种新的数学优化框架，用于多模态数据集中的异常值检测，扩展了支持向量数据描述方法。


<details>
  <summary>Details</summary>
Motivation: 现有方法在多模态和非线性数据结构中检测异常值的准确性和鲁棒性不足，需要更有效的解决方案。

Method: 提出了一个混合整数二阶锥模型的原问题公式，并开发了支持核技巧的对偶模型。

Result: 计算研究表明，该方法在准确性和鲁棒性上优于现有启发式技术。

Conclusion: 该框架为复杂数据结构中的异常值检测提供了有效的解决方案。

Abstract: We present a novel mathematical optimization framework for outlier detection
in multimodal datasets, extending Support Vector Data Description approaches.
We provide a primal formulation, in the shape of a Mixed Integer Second Order
Cone model, that constructs Euclidean hyperspheres to identify anomalous
observations. Building on this, we develop a dual model that enables the
application of the kernel trick, thus allowing for the detection of outliers
within complex, non-linear data structures. An extensive computational study
demonstrates the effectiveness of our exact method, showing clear advantages
over existing heuristic techniques in terms of accuracy and robustness.

</details>


### [337] [Recursive Bound-Constrained AdaGrad with Applications to Multilevel and Domain Decomposition Minimization](https://arxiv.org/abs/2507.11513)
*Serge Gratton,Alena Kopaničáková,Philippe Toint*

Main category: math.OC

TL;DR: 提出了两种噪声容忍的OFFO算法，处理边界约束和不精确梯度，并在可用时利用二阶信息。一种是多级方法，另一种是域分解方法，均为AdaGrad算法的推广。理论框架统一，收敛性分析表明两种方法均需$O(\epsilon^{-2})$次迭代达到近似临界点。数值实验验证了其高效性。


<details>
  <summary>Details</summary>
Motivation: 解决边界约束、不精确梯度和噪声环境下的优化问题，同时利用二阶信息提升效率。

Method: 提出多级方法和域分解方法，推广AdaGrad算法，统一理论框架分析收敛性。

Result: 两种方法均需$O(\epsilon^{-2})$次迭代达到近似临界点，数值实验验证了高效性。

Conclusion: 所提算法在噪声环境下高效，适用于PDE和深度学习等多种应用。

Abstract: Two OFFO (Objective-Function Free Optimization) noise tolerant algorithms are
presented that handle bound constraints, inexact gradients and use second-order
information when available.The first is a multi-level method exploiting a
hierarchical description of the problem and the second is a
domain-decomposition method covering the standard addditive Schwarz
decompositions. Both are generalizations of the first-order AdaGrad algorithm
for unconstrained optimization. Because these algorithms share a common
theoretical framework, a single convergence/complexity theory is provided which
covers them both. Its main result is that, with high probability, both methods
need at most $O(\epsilon^{-2})$ iterations and noisy gradient evaluations to
compute an $\epsilon$-approximate first-order critical point of the
bound-constrained problem. Extensive numerical experiments are discussed on
applications ranging from PDE-based problems to deep neural network training,
illustrating their remarkable computational efficiency.

</details>


<div id='eess.AS'></div>

# eess.AS [[Back]](#toc)

### [338] [Physics-Informed Transfer Learning for Data-Driven Sound Source Reconstruction in Near-Field Acoustic Holography](https://arxiv.org/abs/2507.11070)
*Xinmeng Luan,Mirco Pezzoli,Fabio Antonacci,Augusto Sarti*

Main category: eess.AS

TL;DR: 提出了一种基于物理信息的迁移学习框架，用于近场声全息中的声源重建，通过预训练和微调两阶段实现跨数据集泛化。


<details>
  <summary>Details</summary>
Motivation: 解决传统方法在不同声源类型间泛化能力不足的问题，利用迁移学习和物理信息提升重建精度。

Method: 两阶段框架：1) 在大数据集上预训练复数卷积神经网络；2) 基于基尔霍夫-亥姆霍兹积分的单样本物理信息微调。

Result: 从矩形板数据集迁移到小提琴面板数据集时，重建精度优于预训练模型，接近C-ESM，部分模式下表现更优。

Conclusion: 该框架通过物理信息微调显著提升跨数据集声源重建性能，展示了迁移学习在声学问题中的潜力。

Abstract: We propose a transfer learning framework for sound source reconstruction in
Near-field Acoustic Holography (NAH), which adapts a well-trained data-driven
model from one type of sound source to another using a physics-informed
procedure. The framework comprises two stages: (1) supervised pre-training of a
complex-valued convolutional neural network (CV-CNN) on a large dataset, and
(2) purely physics-informed fine-tuning on a single data sample based on the
Kirchhoff-Helmholtz integral. This method follows the principles of transfer
learning by enabling generalization across different datasets through
physics-informed adaptation. The effectiveness of the approach is validated by
transferring a pre-trained model from a rectangular plate dataset to a violin
top plate dataset, where it shows improved reconstruction accuracy compared to
the pre-trained model and delivers performance comparable to that of
Compressive-Equivalent Source Method (C-ESM). Furthermore, for successful
modes, the fine-tuned model outperforms both the pre-trained model and C-ESM in
accuracy.

</details>


<div id='gr-qc'></div>

# gr-qc [[Back]](#toc)

### [339] [Recent Advances in Simulation-based Inference for Gravitational Wave Data Analysis](https://arxiv.org/abs/2507.11192)
*Bo Liang,He Wang*

Main category: gr-qc

TL;DR: 综述探讨了基于模拟的推理方法在引力波天文学中的应用，重点介绍了机器学习技术如归一化流和神经后验估计，并讨论了其优势与局限性。


<details>
  <summary>Details</summary>
Motivation: 传统贝叶斯推断方法在处理高维参数空间和复杂噪声时面临计算挑战，需要更快速、高效的方法。

Method: 介绍了多种基于模拟的推理方法，包括神经后验估计、神经比率估计、神经似然估计、流匹配和一致性模型。

Result: 这些方法在速度上优于传统方法，但模型依赖性和先验假设敏感性限制了其广泛应用。

Conclusion: 尽管这些方法在速度上有优势，但其准确性仍需在更广泛的参数空间和噪声条件下进一步验证。

Abstract: The detection of gravitational waves by the LIGO-Virgo-KAGRA collaboration
has ushered in a new era of observational astronomy, emphasizing the need for
rapid and detailed parameter estimation and population-level analyses.
Traditional Bayesian inference methods, particularly Markov chain Monte Carlo,
face significant computational challenges when dealing with the
high-dimensional parameter spaces and complex noise characteristics inherent in
gravitational wave data. This review examines the emerging role of
simulation-based inference methods in gravitational wave astronomy, with a
focus on approaches that leverage machine-learning techniques such as
normalizing flows and neural posterior estimation. We provide a comprehensive
overview of the theoretical foundations underlying various simulation-based
inference methods, including neural posterior estimation, neural ratio
estimation, neural likelihood estimation, flow matching, and consistency
models. We explore the applications of these methods across diverse
gravitational wave data processing scenarios, from single-source parameter
estimation and overlapping signal analysis to testing general relativity and
conducting population studies. Although these techniques demonstrate speed
improvements over traditional methods in controlled studies, their
model-dependent nature and sensitivity to prior assumptions are barriers to
their widespread adoption. Their accuracy, which is similar to that of
conventional methods, requires further validation across broader parameter
spaces and noise conditions.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [340] [TaylorPODA: A Taylor Expansion-Based Method to Improve Post-Hoc Attributions for Opaque Models](https://arxiv.org/abs/2507.10643)
*Yuchi Tang,Iñaki Esnaola,Suzanne Mason,George Panoutsos*

Main category: stat.ML

TL;DR: 论文提出了一种基于泰勒展开框架的模型无关解释方法TaylorPODA，通过引入三个严格公理和适应性属性，生成更具理论依据和可视化友好的解释。


<details>
  <summary>Details</summary>
Motivation: 现有的事后模型无关解释方法缺乏系统性框架量化特征贡献，难以满足可信部署的需求。

Method: 基于泰勒展开框架，提出“精确性”、“联邦性”和“零差异”公理，并引入适应性属性，开发TaylorPODA方法。

Result: 实验表明，TaylorPODA在基线方法中表现优异，提供理论更强且可视化友好的解释。

Conclusion: TaylorPODA为不透明模型的可信部署提供了理论支持，是解释方法的重要进展。

Abstract: Existing post-hoc model-agnostic methods generate external explanations for
opaque models, primarily by locally attributing the model output to its input
features. However, they often lack an explicit and systematic framework for
quantifying the contribution of individual features. Building on the Taylor
expansion framework introduced by Deng et al. (2024) to unify existing local
attribution methods, we propose a rigorous set of postulates -- "precision",
"federation", and "zero-discrepancy" -- to govern Taylor term-specific
attribution. Guided by these postulates, we introduce TaylorPODA (Taylor
expansion-derived imPortance-Order aDapted Attribution), which incorporates an
additional "adaptation" property. This property enables alignment with
task-specific goals, especially in post-hoc settings lacking ground-truth
explanations. Empirical evaluations demonstrate that TaylorPODA achieves
competitive results against baseline methods, providing principled and
visualization-friendly explanations. This work represents a step toward the
trustworthy deployment of opaque models by offering explanations with stronger
theoretical grounding.

</details>


### [341] [Robust Multi-Manifold Clustering via Simplex Paths](https://arxiv.org/abs/2507.10710)
*Haoyu Chen,Anna Little,Akin Narayan*

Main category: stat.ML

TL;DR: 提出了一种基于几何的多流形聚类方法，通过计算d-单纯形的局部图及其最大角度路径距离（LAPD），有效分离相交流形。


<details>
  <summary>Details</summary>
Motivation: 解决多流形聚类中流形相交、噪声干扰等问题。

Method: 构建d-单纯形局部图，利用二面角作为权重，计算无穷路径距离（LAPD），并进行去噪处理。

Result: LAPD在高概率下能分离流形，实验证明其对噪声、曲率和小交角具有鲁棒性，性能优于其他方法。

Conclusion: 该方法高效且可扩展，适用于合成和真实数据集。

Abstract: This article introduces a novel, geometric approach for multi-manifold
clustering (MMC), i.e. for clustering a collection of potentially intersecting,
d-dimensional manifolds into the individual manifold components. We first
compute a locality graph on d-simplices, using the dihedral angle in between
adjacent simplices as the graph weights, and then compute infinity path
distances in this simplex graph. This procedure gives a metric on simplices
which we refer to as the largest angle path distance (LAPD). We analyze the
properties of LAPD under random sampling, and prove that with an appropriate
denoising procedure, this metric separates the manifold components with high
probability. We validate the proposed methodology with extensive numerical
experiments on both synthetic and real-world data sets. These experiments
demonstrate that the method is robust to noise, curvature, and small
intersection angle, and generally out-performs other MMC algorithms. In
addition, we provide a highly scalable implementation of the proposed
algorithm, which leverages approximation schemes for infinity path distance to
achieve quasi-linear computational complexity.

</details>


### [342] [GOLFS: Feature Selection via Combining Both Global and Local Information for High Dimensional Clustering](https://arxiv.org/abs/2507.10956)
*Zhaoyu Xing,Yang Wan,Juan Wen,Wei Zhong*

Main category: stat.ML

TL;DR: 提出了一种名为GOLFS的无监督特征选择方法，结合全局和局部信息，用于高维聚类问题。


<details>
  <summary>Details</summary>
Motivation: 由于缺乏聚类标签，监督特征选择的正则化方法无法直接应用，需要一种同时学习伪标签和选择判别特征的方法。

Method: GOLFS结合了局部几何结构（流形学习）和全局相关结构（正则化自表示），提出了一种迭代算法解决优化问题。

Result: 模拟和实际数据应用表明，GOLFS在特征选择和聚类方面表现出色。

Conclusion: GOLFS通过利用更全面的信息，提高了特征选择和聚类的准确性。

Abstract: It is important to identify the discriminative features for high dimensional
clustering. However, due to the lack of cluster labels, the regularization
methods developed for supervised feature selection can not be directly applied.
To learn the pseudo labels and select the discriminative features
simultaneously, we propose a new unsupervised feature selection method, named
GlObal and Local information combined Feature Selection (GOLFS), for high
dimensional clustering problems. The GOLFS algorithm combines both local
geometric structure via manifold learning and global correlation structure of
samples via regularized self-representation to select the discriminative
features. The combination improves the accuracy of both feature selection and
clustering by exploiting more comprehensive information. In addition, an
iterative algorithm is proposed to solve the optimization problem and the
convergency is proved. Simulations and two real data applications demonstrate
the excellent finite-sample performance of GOLFS on both feature selection and
clustering.

</details>


### [343] [Interpretable Bayesian Tensor Network Kernel Machines with Automatic Rank and Feature Selection](https://arxiv.org/abs/2507.11136)
*Afra Kilic,Kim Batselier*

Main category: stat.ML

TL;DR: 论文提出了一种贝叶斯张量网络核机器，通过层次先验自动推断模型复杂度，提升预测准确性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有张量网络核方法多为确定性，忽略参数不确定性，且需手动调参。

Method: 采用稀疏诱导层次先验，结合变分推断自动推断张量秩和特征维度。

Result: 实验显示模型在预测准确性、不确定性量化、可解释性和扩展性上表现优越。

Conclusion: 贝叶斯框架为张量网络核机器提供了自动化和不确定性量化的优势。

Abstract: Tensor Network (TN) Kernel Machines speed up model learning by representing
parameters as low-rank TNs, reducing computation and memory use. However, most
TN-based Kernel methods are deterministic and ignore parameter uncertainty.
Further, they require manual tuning of model complexity hyperparameters like
tensor rank and feature dimensions, often through trial-and-error or
computationally costly methods like cross-validation. We propose Bayesian
Tensor Network Kernel Machines, a fully probabilistic framework that uses
sparsity-inducing hierarchical priors on TN factors to automatically infer
model complexity. This enables automatic inference of tensor rank and feature
dimensions, while also identifying the most relevant features for prediction,
thereby enhancing model interpretability. All the model parameters and
hyperparameters are treated as latent variables with corresponding priors.
Given the Bayesian approach and latent variable dependencies, we apply a
mean-field variational inference to approximate their posteriors. We show that
applying a mean-field approximation to TN factors yields a Bayesian ALS
algorithm with the same computational complexity as its deterministic
counterpart, enabling uncertainty quantification at no extra computational
cost. Experiments on synthetic and real-world datasets demonstrate the superior
performance of our model in prediction accuracy, uncertainty quantification,
interpretability, and scalability.

</details>


### [344] [How does Labeling Error Impact Contrastive Learning? A Perspective from Data Dimensionality Reduction](https://arxiv.org/abs/2507.11161)
*Jun Chen,Hong Chen,Yonghua Yu,Yiming Ying*

Main category: stat.ML

TL;DR: 本文研究了对比学习中标签错误对下游分类性能的理论影响，并提出通过数据降维（如SVD）减少错误样本，同时分析了降维对增强图连通性的双刃剑效应。


<details>
  <summary>Details</summary>
Motivation: 由于常见增强策略（如随机裁剪）可能导致标签错误，本文旨在探讨其对对比学习下游分类性能的影响，并提出改进方法。

Method: 通过数据降维（如SVD）减少错误样本，并理论分析和实证评估其对分类风险的影响。

Result: 发现SVD能减少标签错误但可能损害增强图连通性，建议使用适中的嵌入维度、数据膨胀和弱增强以平衡性能。

Conclusion: 建议采用适中的嵌入维度、数据膨胀和弱增强结合SVD，以优化模型性能。

Abstract: In recent years, contrastive learning has achieved state-of-the-art
performance in the territory of self-supervised representation learning. Many
previous works have attempted to provide the theoretical understanding
underlying the success of contrastive learning. Almost all of them rely on a
default assumption, i.e., the label consistency assumption, which may not hold
in practice (the probability of failure is called labeling error) due to the
strength and randomness of common augmentation strategies, such as random
resized crop (RRC). This paper investigates the theoretical impact of labeling
error on the downstream classification performance of contrastive learning. We
first reveal several significant negative impacts of labeling error on
downstream classification risk. To mitigate these impacts, data dimensionality
reduction method (e.g., singular value decomposition, SVD) is applied on
original data to reduce false positive samples, and establish both theoretical
and empirical evaluations. Moreover, it is also found that SVD acts as a
double-edged sword, which may lead to the deterioration of downstream
classification accuracy due to the reduced connectivity of the augmentation
graph. Based on the above observations, we give the augmentation suggestion
that we should use some moderate embedding dimension (such as $512, 1024$ in
our experiments), data inflation, weak augmentation, and SVD to ensure large
graph connectivity and small labeling error to improve model performance.

</details>


### [345] [From Observational Data to Clinical Recommendations: A Causal Framework for Estimating Patient-level Treatment Effects and Learning Policies](https://arxiv.org/abs/2507.11381)
*Rom Gutman,Shimon Sheiba,Omer Noy Klien,Naama Dekel Bird,Amit Gruber,Doron Aronson,Oren Caspi,Uri Shalit*

Main category: stat.ML

TL;DR: 提出一个框架，用于构建患者特异性治疗推荐模型，结合因果模型学习和目标试验范式，强调安全性和有效性。


<details>
  <summary>Details</summary>
Motivation: 解决使用观察数据时的因果识别问题，并整合现有方法为实用流程。

Method: 不提供具体模型，而是整合现有方法和知识为实用流程。

Result: 在心力衰竭患者并发急性肾损伤的实际案例中，流程优于当前治疗方案。

Conclusion: 该框架能改善患者治疗效果，具有实际应用价值。

Abstract: We propose a framework for building patient-specific treatment recommendation
models, building on the large recent literature on learning patient-level
causal models and inspired by the target trial paradigm of Hernan and Robins.
We focus on safety and validity, including the crucial issue of causal
identification when using observational data. We do not provide a specific
model, but rather a way to integrate existing methods and know-how into a
practical pipeline. We further provide a real world use-case of treatment
optimization for patients with heart failure who develop acute kidney injury
during hospitalization. The results suggest our pipeline can improve patient
outcomes over the current treatment regime.

</details>


### [346] [Joint space-time wind field data extrapolation and uncertainty quantification using nonparametric Bayesian dictionary learning](https://arxiv.org/abs/2507.11385)
*George D. Pasparakis,Ioannis A. Kougioumtzoglou,Michael D. Shields*

Main category: stat.ML

TL;DR: 提出了一种基于非参数贝叶斯字典学习的时空风场数据外推方法，通过有限测量估计相关统计量。


<details>
  <summary>Details</summary>
Motivation: 解决在有限传感器条件下高维风场数据外推和统计量估计的问题。

Method: 利用稀疏/不完整测量数据，构建时间依赖优化问题，确定随机风场低维表示的展开系数。

Result: 相比传统压缩感知方法，新方法能量化估计不确定性，自适应选择展开基，外推精度更高。

Conclusion: 该方法适用于多种风工程应用，通过两个案例验证了其有效性。

Abstract: A methodology is developed, based on nonparametric Bayesian dictionary
learning, for joint space-time wind field data extrapolation and estimation of
related statistics by relying on limited/incomplete measurements. Specifically,
utilizing sparse/incomplete measured data, a time-dependent optimization
problem is formulated for determining the expansion coefficients of an
associated low-dimensional representation of the stochastic wind field.
Compared to an alternative, standard, compressive sampling treatment of the
problem, the developed methodology exhibits the following advantages. First,
the Bayesian formulation enables also the quantification of the uncertainty in
the estimates. Second, the requirement in standard CS-based applications for an
a priori selection of the expansion basis is circumvented. Instead, this is
done herein in an adaptive manner based on the acquired data. Overall, the
methodology exhibits enhanced extrapolation accuracy, even in cases of
high-dimensional data of arbitrary form, and of relatively large extrapolation
distances. Thus, it can be used, potentially, in a wide range of wind
engineering applications where various constraints dictate the use of a limited
number of sensors. The efficacy of the methodology is demonstrated by
considering two case studies. The first relates to the extrapolation of
simulated wind velocity records consistent with a prescribed joint
wavenumber-frequency power spectral density in a three-dimensional domain (2D
and time). The second pertains to the extrapolation of four-dimensional (3D and
time) boundary layer wind tunnel experimental data that exhibit significant
spatial variability and non-Gaussian characteristics.

</details>


### [347] [Canonical Bayesian Linear System Identification](https://arxiv.org/abs/2507.11535)
*Andrey Bryutkin,Matthew E. Levine,Iñigo Urteaga,Youssef Marzouk*

Main category: stat.ML

TL;DR: 论文提出了一种基于规范形式的贝叶斯方法，解决了LTI系统识别中的参数不可识别性问题，提高了推断效率和实用性。


<details>
  <summary>Details</summary>
Motivation: 标准贝叶斯方法在LTI系统识别中因参数不可识别性导致复杂多峰后验，推断效率低下。

Method: 通过将LTI系统的规范形式嵌入贝叶斯框架，解决了参数不可识别性问题，并支持结构感知先验。

Result: 规范形式在计算效率、后验解释性和不确定性估计方面优于标准参数化方法。

Conclusion: 该方法为LTI系统识别提供了高效、稳健的贝叶斯推断框架。

Abstract: Standard Bayesian approaches for linear time-invariant (LTI) system
identification are hindered by parameter non-identifiability; the resulting
complex, multi-modal posteriors make inference inefficient and impractical. We
solve this problem by embedding canonical forms of LTI systems within the
Bayesian framework. We rigorously establish that inference in these minimal
parameterizations fully captures all invariant system dynamics (e.g., transfer
functions, eigenvalues, predictive distributions of system outputs) while
resolving identifiability. This approach unlocks the use of meaningful,
structure-aware priors (e.g., enforcing stability via eigenvalues) and ensures
conditions for a Bernstein--von Mises theorem -- a link between Bayesian and
frequentist large-sample asymptotics that is broken in standard forms.
Extensive simulations with modern MCMC methods highlight advantages over
standard parameterizations: canonical forms achieve higher computational
efficiency, generate interpretable and well-behaved posteriors, and provide
robust uncertainty estimates, particularly from limited data.

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [348] [NLP Meets the World: Toward Improving Conversations With the Public About Natural Language Processing Research](https://arxiv.org/abs/2507.10559)
*Shomir Wilson*

Main category: cs.CY

TL;DR: 本文为与公众沟通大语言模型（LLMs）的能力和限制提供了建议，涵盖术语模糊、期望过高和伦理问题三个主题。


<details>
  <summary>Details</summary>
Motivation: 利用当前公众对自然语言处理（NLP）的兴趣，促进研究领域和个人的发展，同时增强公众对NLP的理解和支持。

Method: 通过分析已发表的NLP研究和新闻报道，提出关于术语、期望和伦理的沟通建议。

Result: 提出了有效、透明的沟通策略，以提升公众理解并支持NLP研究。

Conclusion: 透明沟通有助于公众理解NLP，促进研究的可持续发展。

Abstract: Recent developments in large language models (LLMs) have been accompanied by
rapidly growing public interest in natural language processing (NLP). This
attention is reflected by major news venues, which sometimes invite NLP
researchers to share their knowledge and views with a wide audience.
Recognizing the opportunities of the present, for both the research field and
for individual researchers, this paper shares recommendations for communicating
with a general audience about LLMs' capabilities and limitations. These
recommendations cover three themes: vague terminology as an obstacle to public
understanding, unreasonable expectations as obstacles to sustainable growth,
and ethical failures as obstacles to continued support. Published NLP research
and popular news coverage are cited to illustrate these themes with examples.
The recommendations promote effective, transparent communication with the
general public about NLP, in order to strengthen public understanding and
encourage support for research.

</details>


### [349] [Can Large Language Models Understand As Well As Apply Patent Regulations to Pass a Hands-On Patent Attorney Test?](https://arxiv.org/abs/2507.10576)
*Bhakti Khera,Rezvan Alamian,Pascal A. Scherz,Stephan M. Goetz*

Main category: cs.CY

TL;DR: 论文评估了多种开源和专有大型语言模型（LLMs）在欧洲专利代理人资格考试（EQE）中的表现，发现虽然某些模型（如GPT-4o）表现较好，但所有模型均未达到专业标准（0.90准确率）。专家评估揭示了模型在逻辑一致性和多模态处理上的不足，并指出自动指标与专家判断之间的偏差。


<details>
  <summary>Details</summary>
Motivation: 探讨LLMs在法律领域（尤其是专利代理人考试）中的实际表现及其局限性，揭示公众对模型性能的高估。

Method: 评估了包括GPT系列、Anthropic、Deepseek和Llama-3等模型在EQE考试中的表现，并通过人类专家对文本解释进行评估。

Result: OpenAI o1表现最佳（准确率0.82），但所有模型均未达到专业标准。模型输出对温度和提示词敏感，专家更重视清晰的法律逻辑而非答案正确性。

Conclusion: 尽管LLMs表现突出，但仍需改进逻辑一致性、多模态处理和提示适应性，才能接近人类专业水平。

Abstract: The legal field already uses various large language models (LLMs) in actual
applications, but their quantitative performance and reasons for it are
underexplored. We evaluated several open-source and proprietary LLMs --
including GPT-series, Anthropic, Deepseek and Llama-3, variants -- on parts of
the European Qualifying Examination (EQE) for future European Patent Attorneys.
OpenAI o1 led with 0.82 accuracy and 0.81 F1 score, whereas (Amazon Web
Services) AWS Llama 3.1 8B lagged at 0.50 accuracy, and a Python-deployed Llama
3.1 8B scored 0.55. The latter two are within the range of mere guessing for
the two-answer forced-choice design. None of the evaluated models could have
passed the examination fully, as accuracy never exceeded the average threshold
of 0.90 required for professional-level standards -- also not models that are
regularly promoted for their assumed beyond-PhD- and bar-admitted-lawyer-level
performance. GPT-4o excelled at integrating text and graphics, while Claude 3
Opus often lost formatting coherence. Human patent experts evaluated the
textual justifications and uncovered various critical shortcomings of each
model. They valued clarity and legal rationale over the raw correctness of the
answers, which revealed misalignment between automatic metrics and expert
judgment. Model outputs were sensitive to modest temperature changes and prompt
wording, which underscores the remaining necessity of expert oversight. Future
work should target logical consistency, robust multimodality, and adaptive
prompting to approach human-level patent proficiency. In summary, despite the
outstanding performance of recent large models, the general public might
overestimate their performance. The field has a long way to go to develop a
virtual patent attorney. This paper wants to point out several specific
limitations that need solutions.

</details>


### [350] [Findings of the BEA 2025 Shared Task on Pedagogical Ability Assessment of AI-powered Tutors](https://arxiv.org/abs/2507.10579)
*Ekaterina Kochmar,Kaushal Kumar Maurya,Kseniia Petukhova,KV Aditya Srivatsa,Anaïs Tack,Justin Vasselli*

Main category: cs.CY

TL;DR: 该论文评估了基于大语言模型（LLMs）的AI导师在教学对话中的错误纠正能力，通过五个维度测试其表现，结果显示仍有改进空间。


<details>
  <summary>Details</summary>
Motivation: 评估AI导师在教学对话中的错误纠正能力，推动基于学习科学原则的AI教学工具发展。

Method: 设计五个任务维度（如错误识别、指导提供等），邀请国际团队提交模型，并与人工标注的金标准对比评估。

Result: 最佳模型在四个教学能力评估任务中的F1分数为58.34至71.81，导师身份识别任务达到96.98。

Conclusion: AI导师表现有潜力但需改进，相关资源已公开以支持未来研究。

Abstract: This shared task has aimed to assess pedagogical abilities of AI tutors
powered by large language models (LLMs), focusing on evaluating the quality of
tutor responses aimed at student's mistake remediation within educational
dialogues. The task consisted of five tracks designed to automatically evaluate
the AI tutor's performance across key dimensions of mistake identification,
precise location of the mistake, providing guidance, and feedback
actionability, grounded in learning science principles that define good and
effective tutor responses, as well as the track focusing on detection of the
tutor identity. The task attracted over 50 international teams across all
tracks. The submitted models were evaluated against gold-standard human
annotations, and the results, while promising, show that there is still
significant room for improvement in this domain: the best results for the four
pedagogical ability assessment tracks range between macro F1 scores of 58.34
(for providing guidance) and 71.81 (for mistake identification) on three-class
problems, with the best F1 score in the tutor identification track reaching
96.98 on a 9-class task. In this paper, we overview the main findings of the
shared task, discuss the approaches taken by the teams, and analyze their
performance. All resources associated with this task are made publicly
available to support future research in this critical domain.

</details>


### [351] [Exploring User Security and Privacy Attitudes and Concerns Toward the Use of General-Purpose LLM Chatbots for Mental Health](https://arxiv.org/abs/2507.10695)
*Jabari Kwesi,Jiaxun Cao,Riya Manchanda,Pardis Emami-Naeini*

Main category: cs.CY

TL;DR: 研究探讨了用户在使用通用LLM聊天机器人进行心理健康支持时的隐私和安全问题，发现用户存在误解和风险意识不足。


<details>
  <summary>Details</summary>
Motivation: 了解用户在使用通用LLM聊天机器人时的隐私和安全态度，填补现有研究的空白。

Method: 通过21次半结构化访谈，分析美国参与者的观点。

Result: 用户误将LLM的共情能力等同于人类的责任感，并错误认为其互动受法规保护。

Conclusion: 提出“无形脆弱性”概念，并建议采取措施更有效地保护用户心理健康信息。

Abstract: Individuals are increasingly relying on large language model (LLM)-enabled
conversational agents for emotional support. While prior research has examined
privacy and security issues in chatbots specifically designed for mental health
purposes, these chatbots are overwhelmingly "rule-based" offerings that do not
leverage generative AI. Little empirical research currently measures users'
privacy and security concerns, attitudes, and expectations when using
general-purpose LLM-enabled chatbots to manage and improve mental health.
Through 21 semi-structured interviews with U.S. participants, we identified
critical misconceptions and a general lack of risk awareness. Participants
conflated the human-like empathy exhibited by LLMs with human-like
accountability and mistakenly believed that their interactions with these
chatbots were safeguarded by the same regulations (e.g., HIPAA) as disclosures
with a licensed therapist. We introduce the concept of "intangible
vulnerability," where emotional or psychological disclosures are undervalued
compared to more tangible forms of information (e.g., financial or
location-based data). To address this, we propose recommendations to safeguard
user mental health disclosures with general-purpose LLM-enabled chatbots more
effectively.

</details>


### [352] ["Is it always watching? Is it always listening?" Exploring Contextual Privacy and Security Concerns Toward Domestic Social Robots](https://arxiv.org/abs/2507.10786)
*Henry Bell,Jabari Kwesi,Hiba Laabadli,Pardis Emami-Naeini*

Main category: cs.CY

TL;DR: 社交机器人因其AI和高级传感能力在美国消费者中受欢迎，但也带来安全和隐私威胁。研究发现用户需求透明度和隐私控制。


<details>
  <summary>Details</summary>
Motivation: 社交机器人作为智能家居的进化，其数据收集和交互能力带来了安全和隐私风险，需研究用户需求以指导设计。

Method: 通过19次半结构化访谈，探讨用户的安全和隐私需求。

Result: 用户关注透明度、可用性和隐私控制，教育应用中担心误导信息，医疗应用中担心可靠性。

Conclusion: 社交机器人设计需注重隐私控制、数据收集提示和功能适应性。

Abstract: Equipped with artificial intelligence (AI) and advanced sensing capabilities,
social robots are gaining interest among consumers in the United States. These
robots seem like a natural evolution of traditional smart home devices.
However, their extensive data collection capabilities, anthropomorphic
features, and capacity to interact with their environment make social robots a
more significant security and privacy threat. Increased risks include data
linkage, unauthorized data sharing, and the physical safety of users and their
homes. It is critical to investigate U.S. users' security and privacy needs and
concerns to guide the design of social robots while these devices are still in
the early stages of commercialization in the U.S. market. Through 19
semi-structured interviews, we identified significant security and privacy
concerns, highlighting the need for transparency, usability, and robust privacy
controls to support adoption. For educational applications, participants
worried most about misinformation, and in medical use cases, they worried about
the reliability of these devices. Participants were also concerned with the
data inference that social robots could enable. We found that participants
expect tangible privacy controls, indicators of data collection, and
context-appropriate functionality.

</details>
