<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 23]
- [cs.CL](#cs.CL) [Total: 33]
- [cs.CV](#cs.CV) [Total: 48]
- [cs.LG](#cs.LG) [Total: 75]
- [cs.MA](#cs.MA) [Total: 2]
- [cs.RO](#cs.RO) [Total: 28]
- [cs.SD](#cs.SD) [Total: 1]
- [eess.SY](#eess.SY) [Total: 2]
- [eess.AS](#eess.AS) [Total: 5]
- [q-fin.ST](#q-fin.ST) [Total: 1]
- [physics.comp-ph](#physics.comp-ph) [Total: 1]
- [quant-ph](#quant-ph) [Total: 2]
- [physics.geo-ph](#physics.geo-ph) [Total: 1]
- [cs.GR](#cs.GR) [Total: 3]
- [cs.DB](#cs.DB) [Total: 1]
- [cs.IR](#cs.IR) [Total: 3]
- [physics.med-ph](#physics.med-ph) [Total: 2]
- [q-bio.NC](#q-bio.NC) [Total: 1]
- [q-bio.QM](#q-bio.QM) [Total: 1]
- [cs.CR](#cs.CR) [Total: 13]
- [math.OC](#math.OC) [Total: 3]
- [eess.SP](#eess.SP) [Total: 2]
- [cs.NI](#cs.NI) [Total: 1]
- [physics.optics](#physics.optics) [Total: 1]
- [cs.SE](#cs.SE) [Total: 4]
- [physics.flu-dyn](#physics.flu-dyn) [Total: 1]
- [cs.IT](#cs.IT) [Total: 1]
- [eess.IV](#eess.IV) [Total: 8]
- [cs.HC](#cs.HC) [Total: 4]
- [cs.DC](#cs.DC) [Total: 2]
- [stat.ML](#stat.ML) [Total: 8]
- [hep-ph](#hep-ph) [Total: 1]
- [physics.chem-ph](#physics.chem-ph) [Total: 1]
- [q-bio.BM](#q-bio.BM) [Total: 2]
- [cs.OS](#cs.OS) [Total: 1]
- [cs.DS](#cs.DS) [Total: 1]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [Prover Agent: An Agent-based Framework for Formal Mathematical Proofs](https://arxiv.org/abs/2506.19923)
*Kaito Baba,Chaoran Liu,Shuhei Kurita,Akiyoshi Sannai*

Main category: cs.AI

TL;DR: Prover Agent是一个结合大型语言模型（LLMs）和形式化证明助手Lean的新型AI代理，用于自动定理证明。


<details>
  <summary>Details</summary>
Motivation: 通过整合LLMs和Lean，提高自动定理证明的效率和成功率。

Method: 协调非正式推理LLM、形式化证明模型和Lean的反馈，同时生成辅助引理以帮助发现整体证明策略。

Result: 在MiniF2F基准测试中达到86.1%的成功率，成为使用小型语言模型（SLMs）方法中的新标杆，且样本预算更低。

Conclusion: 生成的辅助引理对解决复杂问题有显著贡献，展示了Prover Agent的有效性。

Abstract: We present Prover Agent, a novel AI agent for automated theorem proving that
integrates large language models (LLMs) with a formal proof assistant, Lean.
Prover Agent coordinates an informal reasoning LLM, a formal prover model, and
feedback from Lean while also generating auxiliary lemmas to assist in
discovering the overall proof strategy. It achieves an 86.1% success rate on
the MiniF2F benchmark, establishing a new state-of-the-art among methods using
small language models (SLMs) with a much lower sample budget than previous
approaches. We also present case studies illustrating how these generated
lemmas contribute to solving challenging problems.

</details>


### [2] [Context Attribution with Multi-Armed Bandit Optimization](https://arxiv.org/abs/2506.19977)
*Deng Pan,Keerthiram Murugesan,Nuno Moniz,Nitesh Chawla*

Main category: cs.AI

TL;DR: 论文提出了一种基于组合多臂老虎机（CMAB）的框架，用于高效探索上下文子集，以理解哪些部分对LLM生成答案有贡献。


<details>
  <summary>Details</summary>
Motivation: 提高生成式QA系统的可解释性和可信度，通过理解上下文对答案的贡献。

Method: 将上下文段视为老虎机臂，使用组合汤普森采样（CTS）在有限查询预算下高效探索上下文子集。

Result: 在多样数据集和LLM上实验表明，该方法以更少查询达到竞争性归因质量。

Conclusion: 该方法显著提高了查询效率，同时保持高归因保真度。

Abstract: Understanding which parts of the retrieved context contribute to a large
language model's generated answer is essential for building interpretable and
trustworthy generative QA systems. We propose a novel framework that formulates
context attribution as a combinatorial multi-armed bandit (CMAB) problem. Each
context segment is treated as a bandit arm, and we employ Combinatorial
Thompson Sampling (CTS) to efficiently explore the exponentially large space of
context subsets under a limited query budget. Our method defines a reward
function based on normalized token likelihoods, capturing how well a subset of
segments supports the original model response. Unlike traditional
perturbation-based attribution methods such as SHAP, which sample subsets
uniformly and incur high computational costs, our approach adaptively balances
exploration and exploitation by leveraging posterior estimates of segment
relevance. This leads to substantially improved query efficiency while
maintaining high attribution fidelity. Extensive experiments on diverse
datasets and LLMs demonstrate that our method achieves competitive attribution
quality with fewer model queries.

</details>


### [3] [QHackBench: Benchmarking Large Language Models for Quantum Code Generation Using PennyLane Hackathon Challenges](https://arxiv.org/abs/2506.20008)
*Abdul Basit,Minghao Shao,Haider Asif,Nouhaila Innan,Muhammad Kashif,Alberto Marchisio,Muhammad Shafique*

Main category: cs.AI

TL;DR: 论文评估了大型语言模型（LLMs）在生成PennyLane量子代码中的表现，引入了QHackBench数据集，并比较了标准提示与检索增强生成（RAG）的效果。结果表明RAG在复杂量子算法中表现接近标准提示，同时提出了多代理评估管道以提升成功率。


<details>
  <summary>Details</summary>
Motivation: 探索LLMs在量子计算代码生成中的潜力，填补现有研究的空白。

Method: 使用QHackBench数据集，评估LLMs在标准提示和RAG下的表现，并引入多代理评估管道优化结果。

Result: RAG增强模型在复杂量子算法中表现接近标准提示，多代理管道进一步提高了执行成功率。

Conclusion: 论文为AI辅助量子编程提供了新工具和数据集，推动了该领域的进一步发展。

Abstract: Recent advances in Large Language Models (LLMs) have demonstrated strong
potential in code generation, yet their effectiveness in quantum computing
remains underexplored. This paper benchmarks LLMs for PennyLane-based quantum
code generation using real-world challenges from the Quantum Hackathon (QHack).
We introduce QHackBench, a novel benchmark dataset derived from QHack
competitions, and evaluate model performance under vanilla prompting and
Retrieval-Augmented Generation (RAG). Our structured evaluation framework
assesses functional correctness, syntactic validity, and execution success
across varying challenge difficulties. Results indicate that RAG-enhanced
models, supplemented with an augmented PennyLane dataset, approximately
generate similar results as the standard prompting, particularly in complex
quantum algorithms. Additionally, we introduce a multi-agent evaluation
pipeline that iteratively refines incorrect solutions, further enhancing
execution success rates. To foster further research, we commit to publicly
releasing QHackBench, along with our evaluation framework and experimental
results, enabling continued advancements in AI-assisted quantum programming.

</details>


### [4] [Accurate and Energy Efficient: Local Retrieval-Augmented Generation Models Outperform Commercial Large Language Models in Medical Tasks](https://arxiv.org/abs/2506.20009)
*Konstantinos Vrettos,Michail E. Klontzas*

Main category: cs.AI

TL;DR: 研究开发了一种可定制的RAG框架，用于医疗任务，其性能和能耗优于商业LLM，同时减少环境影响。


<details>
  <summary>Details</summary>
Motivation: AI在医疗领域的广泛应用引发了对环境和伦理问题的担忧，尤其是商业LLM的高资源消耗和隐私安全问题。

Method: 开发了可监控能耗和CO2排放的RAG框架，并基于开源LLM构建RAG模型，与商业模型进行性能对比。

Result: 基于llama3.1:8B的RAG模型在准确性和能耗上均优于商业模型，能耗更低且CO2排放更少。

Conclusion: 本地LLM开发的RAG在医疗任务中表现更优且环保，符合可持续发展目标。

Abstract: Background The increasing adoption of Artificial Intelligence (AI) in
healthcare has sparked growing concerns about its environmental and ethical
implications. Commercial Large Language Models (LLMs), such as ChatGPT and
DeepSeek, require substantial resources, while the utilization of these systems
for medical purposes raises critical issues regarding patient privacy and
safety. Methods We developed a customizable Retrieval-Augmented Generation
(RAG) framework for medical tasks, which monitors its energy usage and CO2
emissions. This system was then used to create RAGs based on various
open-source LLMs. The tested models included both general purpose models like
llama3.1:8b and medgemma-4b-it, which is medical-domain specific. The best RAGs
performance and energy consumption was compared to DeepSeekV3-R1 and OpenAIs
o4-mini model. A dataset of medical questions was used for the evaluation.
Results Custom RAG models outperformed commercial models in accuracy and energy
consumption. The RAG model built on llama3.1:8B achieved the highest accuracy
(58.5%) and was significantly better than other models, including o4-mini and
DeepSeekV3-R1. The llama3.1-RAG also exhibited the lowest energy consumption
and CO2 footprint among all models, with a Performance per kWh of 0.52 and a
total CO2 emission of 473g. Compared to o4-mini, the llama3.1-RAG achieved 2.7x
times more accuracy points per kWh and 172% less electricity usage while
maintaining higher accuracy. Conclusion Our study demonstrates that local LLMs
can be leveraged to develop RAGs that outperform commercial, online LLMs in
medical tasks, while having a smaller environmental impact. Our modular
framework promotes sustainable AI development, reducing electricity usage and
aligning with the UNs Sustainable Development Goals.

</details>


### [5] [Achieving Trustworthy Real-Time Decision Support Systems with Low-Latency Interpretable AI Models](https://arxiv.org/abs/2506.20018)
*Zechun Deng,Ziwei Liu,Ziqian Bi,Junhao Song,Chia Xin Liang,Joe Yeong,Junfeng Hao*

Main category: cs.AI

TL;DR: 本文探讨了利用低延迟AI模型的实时决策支持系统，结合了整体AI驱动决策工具、Edge-IoT技术集成以及人机协作方法的最新进展。


<details>
  <summary>Details</summary>
Motivation: 研究旨在解决资源有限情况下AI辅助决策的挑战，探索如何通过技术发展（如DeLLMa、模型压缩和边缘设备分析改进）提升系统效率和灵活性。

Method: 通过详细文献综述，分析了大型语言模型在决策支持中的应用，以及压缩模型和边缘设备优化的方法。

Result: 提供了开发策略和应用领域的实用视角，指出了高效灵活AI支持系统的机会。

Conclusion: 为这一快速变化领域的未来突破奠定了基础，强调了AI如何重塑实时决策支持。

Abstract: This paper investigates real-time decision support systems that leverage
low-latency AI models, bringing together recent progress in holistic AI-driven
decision tools, integration with Edge-IoT technologies, and approaches for
effective human-AI teamwork. It looks into how large language models can assist
decision-making, especially when resources are limited. The research also
examines the effects of technical developments such as DeLLMa, methods for
compressing models, and improvements for analytics on edge devices, while also
addressing issues like limited resources and the need for adaptable frameworks.
Through a detailed review, the paper offers practical perspectives on
development strategies and areas of application, adding to the field by
pointing out opportunities for more efficient and flexible AI-supported
systems. The conclusions set the stage for future breakthroughs in this
fast-changing area, highlighting how AI can reshape real-time decision support.

</details>


### [6] [Persona-Assigned Large Language Models Exhibit Human-Like Motivated Reasoning](https://arxiv.org/abs/2506.20020)
*Saloni Dash,Amélie Reymond,Emma S. Spiro,Aylin Caliskan*

Main category: cs.AI

TL;DR: 研究发现，大型语言模型（LLMs）在赋予政治和社会人口属性的人设后，会表现出类似人类的动机性推理，导致真相辨别能力下降，且传统提示去偏方法效果有限。


<details>
  <summary>Details</summary>
Motivation: 探讨LLMs是否会在赋予特定人设后表现出类似人类的动机性推理，从而影响其判断和决策能力。

Method: 通过为8个LLMs赋予4种政治和社会人口属性的人设，测试其在真相辨别和科学证据评估任务中的表现。

Result: 人设化的LLMs真相辨别能力下降9%，政治人设在科学证据评估中与身份一致的准确性提高90%，且去偏提示效果不佳。

Conclusion: LLMs表现出难以通过常规方法缓解的动机性推理，可能加剧身份一致的偏见，需引起关注。

Abstract: Reasoning in humans is prone to biases due to underlying motivations like
identity protection, that undermine rational decision-making and judgment. This
motivated reasoning at a collective level can be detrimental to society when
debating critical issues such as human-driven climate change or vaccine safety,
and can further aggravate political polarization. Prior studies have reported
that large language models (LLMs) are also susceptible to human-like cognitive
biases, however, the extent to which LLMs selectively reason toward
identity-congruent conclusions remains largely unexplored. Here, we investigate
whether assigning 8 personas across 4 political and socio-demographic
attributes induces motivated reasoning in LLMs. Testing 8 LLMs (open source and
proprietary) across two reasoning tasks from human-subject studies -- veracity
discernment of misinformation headlines and evaluation of numeric scientific
evidence -- we find that persona-assigned LLMs have up to 9% reduced veracity
discernment relative to models without personas. Political personas
specifically, are up to 90% more likely to correctly evaluate scientific
evidence on gun control when the ground truth is congruent with their induced
political identity. Prompt-based debiasing methods are largely ineffective at
mitigating these effects. Taken together, our empirical findings are the first
to suggest that persona-assigned LLMs exhibit human-like motivated reasoning
that is hard to mitigate through conventional debiasing prompts -- raising
concerns of exacerbating identity-congruent reasoning in both LLMs and humans.

</details>


### [7] [DiaLLMs: EHR Enhanced Clinical Conversational System for Clinical Test Recommendation and Diagnosis Prediction](https://arxiv.org/abs/2506.20059)
*Weijieying Ren,Tianxiang Zhao,Lei Wang,Tianchun Wang,Vasant Honavar*

Main category: cs.AI

TL;DR: DiaLLM是一种新型医疗大语言模型，整合电子健康记录（EHR）数据，支持临床测试推荐、结果解释和诊断预测，优于现有基线方法。


<details>
  <summary>Details</summary>
Motivation: 现有医疗大语言模型忽视电子健康记录（EHR）的作用，且仅关注诊断推荐，限制了临床实用性。

Method: 提出DiaLLM模型，通过临床测试参考（CTR）策略整合EHR数据，并采用强化学习框架进行证据获取和自动诊断。

Result: 实验表明DiaLLM在临床测试推荐和诊断预测上优于基线方法。

Conclusion: DiaLLM通过整合EHR数据和优化策略，显著提升了医疗大语言模型的临床适用性。

Abstract: Recent advances in Large Language Models (LLMs) have led to remarkable
progresses in medical consultation. However, existing medical LLMs overlook the
essential role of Electronic Health Records (EHR) and focus primarily on
diagnosis recommendation, limiting their clinical applicability. We propose
DiaLLM, the first medical LLM that integrates heterogeneous EHR data into
clinically grounded dialogues, enabling clinical test recommendation, result
interpretation, and diagnosis prediction to better align with real-world
medical practice. To construct clinically grounded dialogues from EHR, we
design a Clinical Test Reference (CTR) strategy that maps each clinical code to
its corresponding description and classifies test results as "normal" or
"abnormal". Additionally, DiaLLM employs a reinforcement learning framework for
evidence acquisition and automated diagnosis. To handle the large action space,
we introduce a reject sampling strategy to reduce redundancy and improve
exploration efficiency. Furthermore, a confirmation reward and a
class-sensitive diagnosis reward are designed to guide accurate diagnosis
prediction. Extensive experimental results demonstrate that DiaLLM outperforms
baselines in clinical test recommendation and diagnosis prediction.

</details>


### [8] [AI Copilots for Reproducibility in Science: A Case Study](https://arxiv.org/abs/2506.20130)
*Adrien Bibal,Steven N. Minton,Deborah Khider,Yolanda Gil*

Main category: cs.AI

TL;DR: OpenPub是一个AI驱动的平台，通过模块化助手支持开放科学任务，特别是可重复性。其Reproducibility Copilot能显著减少重复实验时间，并识别可重复性障碍。


<details>
  <summary>Details</summary>
Motivation: 开放科学倡议希望提高研究的透明度和可重复性，但独立重复研究结果仍具挑战性。

Method: OpenPub平台通过Reproducibility Copilot分析论文、代码和补充材料，生成结构化Jupyter Notebook和建议。

Result: 测试显示，OpenPub能将重复时间从30多小时缩短至约1小时，并高覆盖率地重现图表和结果。

Conclusion: AI工具可有效减轻可重复性负担，促进透明科学交流，其模块化架构支持扩展至其他开放科学目标。

Abstract: Open science initiatives seek to make research outputs more transparent,
accessible, and reusable, but ensuring that published findings can be
independently reproduced remains a persistent challenge. This paper introduces
OpenPub, an AI-powered platform that supports researchers, reviewers, and
readers through a suite of modular copilots focused on key open science tasks.
In this work, we present the Reproducibility Copilot, which analyzes
manuscripts, code, and supplementary materials to generate structured Jupyter
Notebooks and recommendations aimed at facilitating computational, or "rote",
reproducibility. We conducted feasibility tests using previously studied
research papers with known reproducibility benchmarks. Results indicate that
OpenPub can substantially reduce reproduction time - from over 30 hours to
about 1 hour - while achieving high coverage of figures, tables, and results
suitable for computational reproduction. The system systematically detects
barriers to reproducibility, including missing hyperparameters, undocumented
preprocessing steps, and incomplete or inaccessible datasets. These findings
suggest that AI-driven tools can meaningfully reduce the burden of
reproducibility efforts and contribute to more transparent and verifiable
scientific communication. The modular copilot architecture also provides a
foundation for extending AI assistance to additional open science objectives
beyond reproducibility.

</details>


### [9] [Language Modeling by Language Models](https://arxiv.org/abs/2506.20249)
*Junyan Cheng,Peter Clark,Kyle Richardson*

Main category: cs.AI

TL;DR: 利用多智能体LLM模拟研究过程，提出Genesys系统，通过遗传编程生成新架构设计，在多个规模上验证，性能优于现有架构。


<details>
  <summary>Details</summary>
Motivation: 探索是否能用LLM模拟发现新型语言模型架构的过程，提高研究效率。

Method: 采用多智能体LLM方法，结合遗传编程和规模阶梯策略，逐步验证新设计。

Result: 生成1,162个新设计，其中1,062个通过预训练验证，部分设计性能优于GPT2和Mamba2。

Conclusion: Genesys系统在自主发现高效架构方面表现出色，为自动化研究系统设计提供新思路。

Abstract: Can we leverage LLMs to model the process of discovering novel language model
(LM) architectures? Inspired by real research, we propose a multi-agent LLM
approach that simulates the conventional stages of research, from ideation and
literature search (proposal stage) to design implementation (code generation),
generative pre-training, and downstream evaluation (verification). Using ideas
from scaling laws, our system, Genesys, employs a Ladder of Scales approach;
new designs are proposed, adversarially reviewed, implemented, and selectively
verified at increasingly larger model scales (14M$\sim$350M parameters) with a
narrowing budget (the number of models we can train at each scale). To help
make discovery efficient and factorizable, Genesys uses a novel genetic
programming backbone, which we show has empirical advantages over commonly used
direct prompt generation workflows (e.g., $\sim$86\% percentage point
improvement in successful design generation, a key bottleneck). We report
experiments involving 1,162 newly discovered designs (1,062 fully verified
through pre-training) and find the best designs to be highly competitive with
known architectures (e.g., outperform GPT2, Mamba2, etc., on 6/9 common
benchmarks). We couple these results with comprehensive system-level ablations
and formal results, which give broader insights into the design of effective
autonomous discovery systems.

</details>


### [10] [Enterprise Large Language Model Evaluation Benchmark](https://arxiv.org/abs/2506.20274)
*Liya Wang,David Yi,Damien Jose,John Passarelli,James Gao,Jordan Leventis,Kang Li*

Main category: cs.AI

TL;DR: 论文提出了一种基于Bloom分类法的14任务框架，用于全面评估大语言模型在企业环境中的能力，并通过结合LLM标注、LLM评判和纠正检索增强生成技术，构建了一个包含9700个样本的基准测试。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试（如MMLU）未能充分评估企业特定任务的复杂性，因此需要一种更全面的评估方法。

Method: 开发了一个可扩展的流程，结合LLM标注、LLM评判和纠正检索增强生成技术，构建了一个包含9700个样本的基准测试。

Result: 评估显示开源模型（如DeepSeek R1）在推理任务中表现接近专有模型，但在基于判断的任务中表现较差，可能是由于过度思考。

Conclusion: 该研究为企业提供了定制化评估的蓝图，并推动了LLM的实际部署。

Abstract: Large Language Models (LLMs) ) have demonstrated promise in boosting
productivity across AI-powered tools, yet existing benchmarks like Massive
Multitask Language Understanding (MMLU) inadequately assess enterprise-specific
task complexities. We propose a 14-task framework grounded in Bloom's Taxonomy
to holistically evaluate LLM capabilities in enterprise contexts. To address
challenges of noisy data and costly annotation, we develop a scalable pipeline
combining LLM-as-a-Labeler, LLM-as-a-Judge, and corrective retrieval-augmented
generation (CRAG), curating a robust 9,700-sample benchmark. Evaluation of six
leading models shows open-source contenders like DeepSeek R1 rival proprietary
models in reasoning tasks but lag in judgment-based scenarios, likely due to
overthinking. Our benchmark reveals critical enterprise performance gaps and
offers actionable insights for model optimization. This work provides
enterprises a blueprint for tailored evaluations and advances practical LLM
deployment.

</details>


### [11] [Mobile-R1: Towards Interactive Reinforcement Learning for VLM-Based Mobile Agent via Task-Level Rewards](https://arxiv.org/abs/2506.20332)
*Jihao Gu,Qihang Ai,Yingyao Wang,Pi Bu,Jingxuan Xing,Zekun Zhu,Wei Jiang,Ziming Wang,Yingxiu Zhao,Ming-Liang Zhang,Jun Song,Yuning Jiang,Bo Zheng*

Main category: cs.AI

TL;DR: 论文提出Mobile-R1方法，通过任务级奖励的多轮交互强化学习提升移动代理的探索与纠错能力。


<details>
  <summary>Details</summary>
Motivation: 现有研究多基于离线强化学习或动作级奖励的在线优化，限制了代理与环境的动态交互，易陷入局部最优。

Method: 采用三阶段训练框架：初始格式微调、动作级奖励的单步在线训练、基于多轮轨迹的任务级奖励在线训练。

Result: 显著提升了代理的性能，并构建了包含28个中文应用的数据集和500轨迹的基准。

Conclusion: Mobile-R1通过任务级奖励和多轮交互强化学习，有效增强了移动代理的探索与纠错能力。

Abstract: Vision-language model-based mobile agents have gained the ability to not only
understand complex instructions and mobile screenshots, but also optimize their
action outputs via thinking and reasoning, benefiting from reinforcement
learning, such as Group Relative Policy Optimization (GRPO). However, existing
research centers on offline reinforcement learning training or online
optimization using action-level rewards, which limits the agent's dynamic
interaction with the environment. This often results in agents settling into
local optima, thereby weakening their ability for exploration and error action
correction. To address these challenges, we introduce an approach called
Mobile-R1, which employs interactive multi-turn reinforcement learning with
task-level rewards for mobile agents. Our training framework consists of three
stages: initial format finetuning, single-step online training via action-level
reward, followed by online training via task-level reward based on multi-turn
trajectories. This strategy is designed to enhance the exploration and error
correction capabilities of Mobile-R1, leading to significant performance
improvements. Moreover, we have collected a dataset covering 28 Chinese
applications with 24,521 high-quality manual annotations and established a new
benchmark with 500 trajectories. We will open source all resources, including
the dataset, benchmark, model weight, and codes:
https://mobile-r1.github.io/Mobile-R1/.

</details>


### [12] [Tabular Feature Discovery With Reasoning Type Exploration](https://arxiv.org/abs/2506.20357)
*Sungwon Han,Sungkyu Park,Seungeon Lee*

Main category: cs.AI

TL;DR: 论文提出了一种名为REFeat的新方法，通过多类型推理引导LLM生成多样且有意义的特征，解决了现有LLM方法生成特征过于简单或重复的问题。实验证明该方法在59个基准数据集上表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有基于LLM的特征生成方法往往产生过于简单或重复的特征，缺乏多样性和信息量，亟需改进。

Method: 提出REFeat方法，利用多类型推理引导LLM生成特征，结合自适应策略选择。

Result: 在59个基准数据集上，REFeat不仅平均预测准确率更高，还能生成更多样且有意义的特征。

Conclusion: 研究表明，结合丰富推理范式和自适应策略选择，可以显著提升LLM驱动的特征发现效果。

Abstract: Feature engineering for tabular data remains a critical yet challenging step
in machine learning. Recently, large language models (LLMs) have been used to
automatically generate new features by leveraging their vast knowledge.
However, existing LLM-based approaches often produce overly simple or
repetitive features, partly due to inherent biases in the transformations the
LLM chooses and the lack of structured reasoning guidance during generation. In
this paper, we propose a novel method REFeat, which guides an LLM to discover
diverse and informative features by leveraging multiple types of reasoning to
steer the feature generation process. Experiments on 59 benchmark datasets
demonstrate that our approach not only achieves higher predictive accuracy on
average, but also discovers more diverse and meaningful features. These results
highlight the promise of incorporating rich reasoning paradigms and adaptive
strategy selection into LLM-driven feature discovery for tabular data.

</details>


### [13] [Paladin-mini: A Compact and Efficient Grounding Model Excelling in Real-World Scenarios](https://arxiv.org/abs/2506.20384)
*Dror Ivry,Oran Nahum*

Main category: cs.AI

TL;DR: 论文提出了Paladin-mini（3.8B参数的开源分类模型）和grounding-benchmark（评估数据集），用于解决上下文中的声明落地问题，并展示了其性能。


<details>
  <summary>Details</summary>
Motivation: 解决在给定上下文中为声明提供支持性证据的问题。

Method: 提出Paladin-mini（小型开源分类模型）和grounding-benchmark（评估数据集），用于标注数据并评估性能。

Result: 展示了Paladin-mini在当前最先进技术基准上的表现，结果清晰可复现。

Conclusion: Paladin-mini和grounding-benchmark为声明落地问题提供了有效的解决方案。

Abstract: This paper introduces two significant contributions to address the issue of
grounding claims in a given context. Grounding means that given a context
(document) and a claim, there's at least one supportive evidence for the claim
in the document. We will introduce Paladin-mini, a compact (3.8B parameters)
open-source classifier model (used for labeling data as grounded or ungrounded)
engineered for robust performance in real-world scenarios, and the
grounding-benchmark, a new evaluation dataset designed to assess performance on
critical reasoning tasks. We'll also demonstrate the results of Paladin-mini
with benchmarks against the current State-of-the-art and share clear and
reproducible results.

</details>


### [14] [Smart Ride and Delivery Services with Electric Vehicles: Leveraging Bidirectional Charging for Profit Optimisation](https://arxiv.org/abs/2506.20401)
*Jinchun Du,Bojie Shen,Muhammad Aamir Cheema,Adel N. Toosi*

Main category: cs.AI

TL;DR: 论文提出了一种结合V2G技术的电动汽车路径优化问题（EVOP-V2G），旨在最大化利润，并通过MIP模型和两种元启发式算法（EA和LNS）实现高效求解。


<details>
  <summary>Details</summary>
Motivation: 随着电动汽车（EVs）的普及，如何在服务系统中优化其充电和放电行为以提升利润成为关键问题。V2G技术为EVs提供了新的机会，但也带来了复杂性。

Method: 将问题建模为混合整数规划（MIP），并提出两种元启发式算法：进化算法（EA）和大邻域搜索（LNS）。

Result: 实验表明，所提方法在真实数据上将司机利润翻倍，并在小规模实例中接近最优，大规模实例中表现出色。

Conclusion: 研究为基于EV的智能移动系统提供了高效解决方案，同时支持电网能源管理。

Abstract: With the rising popularity of electric vehicles (EVs), modern service
systems, such as ride-hailing delivery services, are increasingly integrating
EVs into their operations. Unlike conventional vehicles, EVs often have a
shorter driving range, necessitating careful consideration of charging when
fulfilling requests. With recent advances in Vehicle-to-Grid (V2G) technology -
allowing EVs to also discharge energy back to the grid - new opportunities and
complexities emerge. We introduce the Electric Vehicle Orienteering Problem
with V2G (EVOP-V2G): a profit-maximization problem where EV drivers must select
customer requests or orders while managing when and where to charge or
discharge. This involves navigating dynamic electricity prices, charging
station selection, and route constraints. We formulate the problem as a Mixed
Integer Programming (MIP) model and propose two near-optimal metaheuristic
algorithms: one evolutionary (EA) and the other based on large neighborhood
search (LNS). Experiments on real-world data show our methods can double driver
profits compared to baselines, while maintaining near-optimal performance on
small instances and excellent scalability on larger ones. Our work highlights a
promising path toward smarter, more profitable EV-based mobility systems that
actively support the energy grid.

</details>


### [15] [GymPN: A Library for Decision-Making in Process Management Systems](https://arxiv.org/abs/2506.20404)
*Riccardo Lo Bianco,Willem van Jaarsveld,Remco Dijkman*

Main category: cs.AI

TL;DR: GymPN是一个基于深度强化学习的软件库，用于优化业务流程中的决策，支持部分流程可观察性和多决策建模，解决了现有工作的局限性。


<details>
  <summary>Details</summary>
Motivation: 业务流程管理系统需要优化任务分配、执行时间和人员指派等决策，现有工具存在局限性。

Method: 提出GymPN软件库，利用深度强化学习支持部分流程可观察性和多决策建模。

Result: 在八种典型业务流程决策问题模式上验证，GymPN能轻松建模并学习最优决策策略。

Conclusion: GymPN解决了现有方法的局限性，能更真实地表示业务流程决策。

Abstract: Process management systems support key decisions about the way work is
allocated in organizations. This includes decisions on which task to perform
next, when to execute the task, and who to assign the task to. Suitable
software tools are required to support these decisions in a way that is optimal
for the organization. This paper presents a software library, called GymPN,
that supports optimal decision-making in business processes using Deep
Reinforcement Learning. GymPN builds on previous work that supports task
assignment in business processes, introducing two key novelties: support for
partial process observability and the ability to model multiple decisions in a
business process. These novel elements address fundamental limitations of
previous work and thus enable the representation of more realistic process
decisions. We evaluate the library on eight typical business process
decision-making problem patterns, showing that GymPN allows for easy modeling
of the desired problems, as well as learning optimal decision policies.

</details>


### [16] [Mixtures of Neural Cellular Automata: A Stochastic Framework for Growth Modelling and Self-Organization](https://arxiv.org/abs/2506.20486)
*Salvatore Milite,Giulio Caravagna,Andrea Sottoriva*

Main category: cs.AI

TL;DR: 提出了一种混合神经细胞自动机（MNCA）框架，通过结合概率规则和固有噪声，增强了传统神经细胞自动机（NCA）对随机性的建模能力。


<details>
  <summary>Details</summary>
Motivation: 传统NCA的确定性限制了其对真实生物系统中随机性的建模能力。

Method: 将混合模型思想引入NCA，结合概率规则和固有噪声，提出MNCA框架。

Result: 在组织生长模拟、图像形态生成和显微镜图像分割中，MNCA表现出更强的鲁棒性、更真实的生物生长模式以及可解释的规则分割。

Conclusion: MNCA是建模随机动力系统和自生长过程的有力工具。

Abstract: Neural Cellular Automata (NCAs) are a promising new approach to model
self-organizing processes, with potential applications in life science.
However, their deterministic nature limits their ability to capture the
stochasticity of real-world biological and physical systems.
  We propose the Mixture of Neural Cellular Automata (MNCA), a novel framework
incorporating the idea of mixture models into the NCA paradigm. By combining
probabilistic rule assignments with intrinsic noise, MNCAs can model diverse
local behaviors and reproduce the stochastic dynamics observed in biological
processes.
  We evaluate the effectiveness of MNCAs in three key domains: (1) synthetic
simulations of tissue growth and differentiation, (2) image morphogenesis
robustness, and (3) microscopy image segmentation. Results show that MNCAs
achieve superior robustness to perturbations, better recapitulate real
biological growth patterns, and provide interpretable rule segmentation. These
findings position MNCAs as a promising tool for modeling stochastic dynamical
systems and studying self-growth processes.

</details>


### [17] [Engineering Sentience](https://arxiv.org/abs/2506.20504)
*Konstantin Demin,Taylor Webb,Eric Elmoznino,Hakwan Lau*

Main category: cs.AI

TL;DR: 论文提出了一种适用于机器设计的感知定义，强调功能性和计算性实现，同时需包含主观性。


<details>
  <summary>Details</summary>
Motivation: 为AI设计有意义的感知能力，需明确其功能性和主观性，避免无意中创造具有感知的AI。

Method: 提出感知需具备断言性和质性信号，并结合当前技术探讨实现方法。

Result: 明确了功能性感知的定义及其实现路径。

Conclusion: 理解功能性感知有助于避免无意创造或及时识别具有感知的AI。

Abstract: We spell out a definition of sentience that may be useful for designing and
building it in machines. We propose that for sentience to be meaningful for AI,
it must be fleshed out in functional, computational terms, in enough detail to
allow for implementation. Yet, this notion of sentience must also reflect
something essentially 'subjective', beyond just having the general capacity to
encode perceptual content. For this specific functional notion of sentience to
occur, we propose that certain sensory signals need to be both assertoric
(persistent) and qualitative. To illustrate the definition in more concrete
terms, we sketch out some ways for potential implementation, given current
technology. Understanding what it takes for artificial agents to be
functionally sentient can also help us avoid creating them inadvertently, or at
least, realize that we have created them in a timely manner.

</details>


### [18] [Case-based Reasoning Augmented Large Language Model Framework for Decision Making in Realistic Safety-Critical Driving Scenarios](https://arxiv.org/abs/2506.20531)
*Wenbin Gan,Minh-Son Dao,Koji Zettsu*

Main category: cs.AI

TL;DR: 论文提出了一种基于案例推理增强的大语言模型（CBR-LLM）框架，用于复杂风险场景下的规避机动决策，结合语义场景理解和历史案例检索，提升决策准确性和人类对齐性。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型在自动驾驶决策中面临领域适应、上下文理解和经验知识缺乏的挑战，无法在动态高风险环境中做出可靠决策。

Method: 提出CBR-LLM框架，整合语义场景理解和历史案例检索，利用风险感知提示策略和相似性案例检索指导决策。

Result: 实验表明该框架提高了决策准确性、解释质量和人类专家行为对齐性，在多样化风险类型中表现更优。

Conclusion: CBR-LLM框架在复杂场景中表现出鲁棒性，有望成为智能驾驶系统的自适应可信决策支持工具。

Abstract: Driving in safety-critical scenarios requires quick, context-aware
decision-making grounded in both situational understanding and experiential
reasoning. Large Language Models (LLMs), with their powerful general-purpose
reasoning capabilities, offer a promising foundation for such decision-making.
However, their direct application to autonomous driving remains limited due to
challenges in domain adaptation, contextual grounding, and the lack of
experiential knowledge needed to make reliable and interpretable decisions in
dynamic, high-risk environments. To address this gap, this paper presents a
Case-Based Reasoning Augmented Large Language Model (CBR-LLM) framework for
evasive maneuver decision-making in complex risk scenarios. Our approach
integrates semantic scene understanding from dashcam video inputs with the
retrieval of relevant past driving cases, enabling LLMs to generate maneuver
recommendations that are both context-sensitive and human-aligned. Experiments
across multiple open-source LLMs show that our framework improves decision
accuracy, justification quality, and alignment with human expert behavior.
Risk-aware prompting strategies further enhance performance across diverse risk
types, while similarity-based case retrieval consistently outperforms random
sampling in guiding in-context learning. Case studies further demonstrate the
framework's robustness in challenging real-world conditions, underscoring its
potential as an adaptive and trustworthy decision-support tool for intelligent
driving systems.

</details>


### [19] [Fine-Tuning and Prompt Engineering of LLMs, for the Creation of Multi-Agent AI for Addressing Sustainable Protein Production Challenges](https://arxiv.org/abs/2506.20598)
*Alexander D. Kalian,Jaewook Lee,Stefan P. Johannesson,Lennart Otte,Christer Hogstrand,Miao Guo*

Main category: cs.AI

TL;DR: 提出了一种多智能体AI框架，用于支持可持续蛋白质生产研究，重点关注微生物蛋白来源，通过检索增强生成（RAG）系统优化信息提取性能。


<details>
  <summary>Details</summary>
Motivation: 全球对可持续蛋白质来源的需求推动了智能工具的开发，以快速处理和合成领域特定的科学知识。

Method: 采用基于GPT的LLM智能体，包括文献搜索和信息提取两部分，通过微调和提示工程优化性能。

Result: 微调和提示工程均显著提升了信息提取性能，余弦相似度得分最高提升25%，微调效果更优（得分≥0.94）。

Conclusion: 该多智能体AI系统在可持续蛋白质研究中表现出潜力，未来可扩展化学安全搜索功能。

Abstract: The global demand for sustainable protein sources has accelerated the need
for intelligent tools that can rapidly process and synthesise domain-specific
scientific knowledge. In this study, we present a proof-of-concept multi-agent
Artificial Intelligence (AI) framework designed to support sustainable protein
production research, with an initial focus on microbial protein sources. Our
Retrieval-Augmented Generation (RAG)-oriented system consists of two GPT-based
LLM agents: (1) a literature search agent that retrieves relevant scientific
literature on microbial protein production for a specified microbial strain,
and (2) an information extraction agent that processes the retrieved content to
extract relevant biological and chemical information. Two parallel
methodologies, fine-tuning and prompt engineering, were explored for agent
optimisation. Both methods demonstrated effectiveness at improving the
performance of the information extraction agent in terms of transformer-based
cosine similarity scores between obtained and ideal outputs. Mean cosine
similarity scores were increased by up to 25%, while universally reaching mean
scores of $\geq 0.89$ against ideal output text. Fine-tuning overall improved
the mean scores to a greater extent (consistently of $\geq 0.94$) compared to
prompt engineering, although lower statistical uncertainties were observed with
the latter approach. A user interface was developed and published for enabling
the use of the multi-agent AI system, alongside preliminary exploration of
additional chemical safety-based search capabilities

</details>


### [20] [CogGen: A Learner-Centered Generative AI Architecture for Intelligent Tutoring with Programming Video](https://arxiv.org/abs/2506.20600)
*Wengxi Li,Roy Pea,Nick Haber,Hari Subramonyam*

Main category: cs.AI

TL;DR: CogGen是一个学习者中心的AI架构，将编程视频转化为互动式自适应学习体验，结合学生建模与生成式AI辅导。


<details>
  <summary>Details</summary>
Motivation: 通过结合认知学徒框架，提升视频编程教育的互动性和适应性，解决传统视频学习的局限性。

Method: 架构包含三部分：视频按学习目标分段、基于认知学徒策略的对话辅导引擎、使用贝叶斯知识追踪的学生模型。

Result: 技术评估显示视频分段准确且教学策略有效，消融实验验证各组件必要性。

Conclusion: CogGen通过结合结构化学生建模与互动AI对话，推动了AI辅导的发展，为视频编程教育提供了可扩展方案。

Abstract: We introduce CogGen, a learner-centered AI architecture that transforms
programming videos into interactive, adaptive learning experiences by
integrating student modeling with generative AI tutoring based on the Cognitive
Apprenticeship framework. The architecture consists of three components: (1)
video segmentation by learning goals, (2) a conversational tutoring engine
applying Cognitive Apprenticeship strategies, and (3) a student model using
Bayesian Knowledge Tracing to adapt instruction. Our technical evaluation
demonstrates effective video segmentation accuracy and strong pedagogical
alignment across knowledge, method, action, and interaction layers. Ablation
studies confirm the necessity of each component in generating effective
guidance. This work advances AI-powered tutoring by bridging structured student
modeling with interactive AI conversations, offering a scalable approach to
enhancing video-based programming education.

</details>


### [21] [AI Assistants to Enhance and Exploit the PETSc Knowledge Base](https://arxiv.org/abs/2506.20608)
*Barry Smith,Junchao Zhang,Hong Zhang,Lois Curfman McInnes,Murat Keceli,Archit Vasan,Satish Balay,Toby Isaac,Le Chen,Venkatram Vishwanath*

Main category: cs.AI

TL;DR: PETSc团队利用LLM技术整合分散知识库，开发了基于RAG和重排算法的工具，以提升数值软件开发和使用效率，并计划扩展为科学软件的知识中心AI框架。


<details>
  <summary>Details</summary>
Motivation: PETSc积累了丰富的分散知识，但难以访问和利用，需通过AI技术激活这些资源。

Method: 结合RAG、重排算法和聊天机器人等LLM工具，设计系统架构并评估模型性能。

Result: 初步构建了LLM驱动的工具，专注于Krylov求解器，并分析了LLM对数值软件的潜在提升。

Conclusion: 目标是建立可扩展的知识中心AI框架，未来将发展为支持科学发现的强大平台。

Abstract: Generative AI, especially through large language models (LLMs), is
transforming how technical knowledge can be accessed, reused, and extended.
PETSc, a widely used numerical library for high-performance scientific
computing, has accumulated a rich but fragmented knowledge base over its three
decades of development, spanning source code, documentation, mailing lists,
GitLab issues, Discord conversations, technical papers, and more. Much of this
knowledge remains informal and inaccessible to users and new developers. To
activate and utilize this knowledge base more effectively, the PETSc team has
begun building an LLM-powered system that combines PETSc content with custom
LLM tools -- including retrieval-augmented generation (RAG), reranking
algorithms, and chatbots -- to assist users, support developers, and propose
updates to formal documentation. This paper presents initial experiences
designing and evaluating these tools, focusing on system architecture, using
RAG and reranking for PETSc-specific information, evaluation methodologies for
various LLMs and embedding models, and user interface design. Leveraging the
Argonne Leadership Computing Facility resources, we analyze how LLM responses
can enhance the development and use of numerical software, with an initial
focus on scalable Krylov solvers. Our goal is to establish an extensible
framework for knowledge-centered AI in scientific software, enabling scalable
support, enriched documentation, and enhanced workflows for research and
development. We conclude by outlining directions for expanding this system into
a robust, evolving platform that advances software ecosystems to accelerate
scientific discovery.

</details>


### [22] [Towards Community-Driven Agents for Machine Learning Engineering](https://arxiv.org/abs/2506.20640)
*Sijie Li,Weiwei Sun,Shanda Li,Ameet Talwalkar,Yiming Yang*

Main category: cs.AI

TL;DR: MLE-Live框架和CoMind代理通过模拟Kaggle社区，提升语言模型代理在协作研究中的表现，优于79.2%的人类竞争者。


<details>
  <summary>Details</summary>
Motivation: 现有语言模型代理在孤立环境中工作，缺乏与社区协作的能力，而人类研究者通过分享知识获得洞察。

Method: 提出MLE-Live框架评估代理在模拟Kaggle社区中的协作能力，并开发CoMind代理进行知识交换和创新。

Result: CoMind在MLE-Live中表现优异，平均优于79.2%的人类竞争者。

Conclusion: MLE-Live和CoMind展示了语言模型代理在协作研究中的潜力，代码已开源。

Abstract: Large language model-based machine learning (ML) agents have shown great
promise in automating ML research. However, existing agents typically operate
in isolation on a given research problem, without engaging with the broader
research community, where human researchers often gain insights and contribute
by sharing knowledge. To bridge this gap, we introduce MLE-Live, a live
evaluation framework designed to assess an agent's ability to communicate with
and leverage collective knowledge from a simulated Kaggle research community.
Building on this framework, we propose CoMind, a novel agent that excels at
exchanging insights and developing novel solutions within a community context.
CoMind achieves state-of-the-art performance on MLE-Live and outperforms 79.2%
human competitors on average across four ongoing Kaggle competitions. Our code
is released at https://github.com/comind-ml/CoMind.

</details>


### [23] [The Decrypto Benchmark for Multi-Agent Reasoning and Theory of Mind](https://arxiv.org/abs/2506.20664)
*Andrei Lupu,Timon Willi,Jakob Foerster*

Main category: cs.AI

TL;DR: 论文提出了Decrypto，一个基于游戏的基准测试，用于评估多智能体推理和心智理论（ToM）能力，填补了现有基准测试的不足。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型（LLMs）具备代理能力，它们需要在复杂的多智能体场景中导航，但目前对LLMs在多智能体能力和ToM方面的理解不足，现有基准测试存在范围狭窄、数据泄露等问题。

Method: 提出Decrypto基准测试，结合认知科学、计算语用学和多智能体强化学习，设计交互式ToM实验平台。

Result: 实证评估显示，LLMs的游戏能力落后于人类和简单词嵌入基线模型，且最新推理模型在某些ToM任务上表现不如旧模型。

Conclusion: Decrypto填补了当前推理和ToM评估的关键空白，为开发更好的智能代理铺平了道路。

Abstract: As Large Language Models (LLMs) gain agentic abilities, they will have to
navigate complex multi-agent scenarios, interacting with human users and other
agents in cooperative and competitive settings. This will require new reasoning
skills, chief amongst them being theory of mind (ToM), or the ability to reason
about the "mental" states of other agents. However, ToM and other multi-agent
abilities in LLMs are poorly understood, since existing benchmarks suffer from
narrow scope, data leakage, saturation, and lack of interactivity. We thus
propose Decrypto, a game-based benchmark for multi-agent reasoning and ToM
drawing inspiration from cognitive science, computational pragmatics and
multi-agent reinforcement learning. It is designed to be as easy as possible in
all other dimensions, eliminating confounding factors commonly found in other
benchmarks. To our knowledge, it is also the first platform for designing
interactive ToM experiments.
  We validate the benchmark design through comprehensive empirical evaluations
of frontier LLMs, robustness studies, and human-AI cross-play experiments. We
find that LLM game-playing abilities lag behind humans and simple
word-embedding baselines. We then create variants of two classic cognitive
science experiments within Decrypto to evaluate three key ToM abilities.
Surprisingly, we find that state-of-the-art reasoning models are significantly
worse at those tasks than their older counterparts. This demonstrates that
Decrypto addresses a crucial gap in current reasoning and ToM evaluations, and
paves the path towards better artificial agents.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [24] [CycleDistill: Bootstrapping Machine Translation using LLMs with Cyclical Distillation](https://arxiv.org/abs/2506.19952)
*Deepon Halder,Thanmay Jayakumar,Raj Dabre*

Main category: cs.CL

TL;DR: CycleDistill利用大语言模型（LLMs）和少样本翻译，通过迭代生成合成平行语料库来提升低资源语言的机器翻译质量，无需依赖大量平行数据。


<details>
  <summary>Details</summary>
Motivation: 低资源语言缺乏平行语料库，而现有LLMs在少样本翻译中表现不如专用MT系统。CycleDistill旨在解决这一问题。

Method: 通过零样本或少样本翻译从单语语料库生成合成平行数据，并迭代微调模型。

Result: 在三种印度语言上，CycleDistill显著提升翻译质量，首轮迭代平均提高20-30 chrF点。

Conclusion: CycleDistill为低资源语言提供了一种高效且高质量的机器翻译解决方案。

Abstract: Large language models (LLMs), despite their ability to perform few-shot
machine translation (MT), often lag behind dedicated MT systems trained on
parallel corpora, which are crucial for high quality machine translation (MT).
However, parallel corpora are often scarce or non-existent for low-resource
languages. In this paper, we propose CycleDistill, a bootstrapping approach
leveraging LLMs and few-shot translation to obtain high-quality MT systems.
CycleDistill involves iteratively generating synthetic parallel corpora from
monolingual corpora via zero- or few-shot MT, which is then used to fine-tune
the model that was used for generating said data for MT. CycleDistill does not
need parallel corpora beyond 1 to 4 few-shot examples, and in our experiments
focusing on three Indian languages, by relying solely on monolingual corpora,
it can achieve high-quality machine translation, improving upon a few-shot
baseline model by over 20-30 chrF points on average in the first iteration. We
also study the effect of leveraging softmax activations during the distillation
process and observe mild improvements in translation quality.

</details>


### [25] [Inference Scaled GraphRAG: Improving Multi Hop Question Answering on Knowledge Graphs](https://arxiv.org/abs/2506.19967)
*Travis Thompson,Seung-Hwan Lim,Paul Liu,Ruoying He,Dongkuan Xu*

Main category: cs.CL

TL;DR: 论文提出了一种名为Inference-Scaled GraphRAG的新框架，通过推理时计算扩展提升LLM在图推理中的表现，显著提高了多跳问答性能。


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型（LLMs）在语言理解和生成方面表现出色，但在知识密集型推理任务中表现不佳，主要因为缺乏结构化上下文和多跳信息。

Method: 结合顺序扩展和深度链式思维图遍历，以及并行扩展与多数投票采样轨迹，形成交替推理-执行循环。

Result: 在GRBench基准测试中，该方法显著优于传统GraphRAG和基线图遍历方法，提升了多跳问答性能。

Conclusion: 推理时扩展是一种实用且与架构无关的解决方案，适用于LLM的结构化知识推理。

Abstract: Large Language Models (LLMs) have achieved impressive capabilities in
language understanding and generation, yet they continue to underperform on
knowledge-intensive reasoning tasks due to limited access to structured context
and multi-hop information. Retrieval-Augmented Generation (RAG) partially
mitigates this by grounding generation in retrieved context, but conventional
RAG and GraphRAG methods often fail to capture relational structure across
nodes in knowledge graphs. We introduce Inference-Scaled GraphRAG, a novel
framework that enhances LLM-based graph reasoning by applying inference-time
compute scaling. Our method combines sequential scaling with deep
chain-of-thought graph traversal, and parallel scaling with majority voting
over sampled trajectories within an interleaved reasoning-execution loop.
Experiments on the GRBench benchmark demonstrate that our approach
significantly improves multi-hop question answering performance, achieving
substantial gains over both traditional GraphRAG and prior graph traversal
baselines. These findings suggest that inference-time scaling is a practical
and architecture-agnostic solution for structured knowledge reasoning with LLMs

</details>


### [26] [Doc2Agent: Scalable Generation of Tool-Using Agents from API Documentation](https://arxiv.org/abs/2506.19998)
*Xinyi Ni,Haonan Jian,Qiuyang Wang,Vedanshi Chetan Shah,Pengyu Hong*

Main category: cs.CL

TL;DR: Doc2Agent是一个可扩展的流程，用于从API文档构建工具调用代理，显著提升性能并降低成本。


<details>
  <summary>Details</summary>
Motivation: 解决现有API代理工具集单一且无法应对现实复杂API的问题。

Method: 通过从API文档生成可执行工具，并利用代码代理迭代优化。

Result: 在WebArena基准测试中，性能提升55%，成本降低90%。

Conclusion: Doc2Agent为从非结构化API文档构建工具代理提供了通用解决方案。

Abstract: REST APIs play important roles in enriching the action space of web agents,
yet most API-based agents rely on curated and uniform toolsets that do not
reflect the complexity of real-world APIs. Building tool-using agents for
arbitrary domains remains a major challenge, as it requires reading
unstructured API documentation, testing APIs and inferring correct parameters.
We propose Doc2Agent, a scalable pipeline to build agents that can call
Python-based tools generated from API documentation. Doc2Agent generates
executable tools from API documentations and iteratively refines them using a
code agent. We evaluate our approach on real-world APIs, WebArena APIs, and
research APIs, producing validated tools. We achieved a 55\% relative
performance improvement with 90\% lower cost compared to direct API calling on
WebArena benchmark. A domain-specific agent built for glycomaterial science
further demonstrates the pipeline's adaptability to complex, knowledge-rich
tasks. Doc2Agent offers a generalizable solution for building tool agents from
unstructured API documentation at scale.

</details>


### [27] [A Modular Multitask Reasoning Framework Integrating Spatio-temporal Models and LLMs](https://arxiv.org/abs/2506.20073)
*Kethmi Hirushini Hettige,Jiahao Ji,Cheng Long,Shili Xiang,Gao Cong,Jingyuan Wang*

Main category: cs.CL

TL;DR: STReason结合大型语言模型和时空模型，通过上下文学习分解复杂查询，生成模块化程序和详细解释，显著优于现有基线。


<details>
  <summary>Details</summary>
Motivation: 现有时空数据挖掘模型局限于单一任务，缺乏多任务推理和复杂长形式推理能力，限制了实际应用。

Method: STReason框架结合LLMs的推理能力和时空模型的分析能力，无需任务特定微调，通过上下文学习生成模块化程序和详细解释。

Result: 实验显示STReason在所有指标上显著优于先进LLM基线，尤其在复杂时空推理场景中表现突出。

Conclusion: STReason为开发更强大、通用的时空推理系统提供了有前景的方向。

Abstract: Spatio-temporal data mining plays a pivotal role in informed decision making
across diverse domains. However, existing models are often restricted to narrow
tasks, lacking the capacity for multi-task inference and complex long-form
reasoning that require generation of in-depth, explanatory outputs. These
limitations restrict their applicability to real-world, multi-faceted decision
scenarios. In this work, we introduce STReason, a novel framework that
integrates the reasoning strengths of large language models (LLMs) with the
analytical capabilities of spatio-temporal models for multi-task inference and
execution. Without requiring task-specific finetuning, STReason leverages
in-context learning to decompose complex natural language queries into modular,
interpretable programs, which are then systematically executed to generate both
solutions and detailed rationales. To facilitate rigorous evaluation, we
construct a new benchmark dataset and propose a unified evaluation framework
with metrics specifically designed for long-form spatio-temporal reasoning.
Experimental results show that STReason significantly outperforms advanced LLM
baselines across all metrics, particularly excelling in complex,
reasoning-intensive spatio-temporal scenarios. Human evaluations further
validate STReason's credibility and practical utility, demonstrating its
potential to reduce expert workload and broaden the applicability to real-world
spatio-temporal tasks. We believe STReason provides a promising direction for
developing more capable and generalizable spatio-temporal reasoning systems.

</details>


### [28] [SACL: Understanding and Combating Textual Bias in Code Retrieval with Semantic-Augmented Reranking and Localization](https://arxiv.org/abs/2506.20081)
*Dhruv Gupta,Gayathri Ganesh Lakshmy,Yiqing Xie*

Main category: cs.CL

TL;DR: 论文提出SACL框架，通过增强语义信息改进代码检索，显著提升代码生成性能。


<details>
  <summary>Details</summary>
Motivation: 当前代码检索器过度依赖表面文本特征（如文档字符串、标识符名称）并对文档化代码有强烈偏见，影响了代码生成效果。

Method: 通过系统性地屏蔽特定特征但保留代码功能，分析代码检索问题，并提出SACL框架，增强语义信息以减少偏见。

Result: SACL显著提升代码检索性能（如HumanEval上Recall@1提高12.8%），并改善代码生成（如HumanEval上Pass@1提高4.88%）。

Conclusion: SACL通过语义增强有效减少检索偏见，提升代码生成质量。

Abstract: Retrieval-Augmented Code Generation (RACG) is a critical technique for
enhancing code generation by retrieving relevant information. In this work, we
conduct an in-depth analysis of code retrieval by systematically masking
specific features while preserving code functionality. Our discoveries include:
(1) although trained on code, current retrievers heavily rely on surface-level
textual features (e.g., docstrings, identifier names), and (2) they exhibit a
strong bias towards well-documented code, even if the documentation is
irrelevant.Based on our discoveries, we propose SACL, a framework that enriches
textual information and reduces bias by augmenting code or structural knowledge
with semantic information. Extensive experiments show that SACL substantially
improves code retrieval (e.g., by 12.8% / 9.4% / 7.0% Recall@1 on HumanEval /
MBPP / SWE-Bench-Lite), which also leads to better code generation performance
(e.g., by 4.88% Pass@1 on HumanEval).

</details>


### [29] [Bridging Compositional and Distributional Semantics: A Survey on Latent Semantic Geometry via AutoEncoder](https://arxiv.org/abs/2506.20083)
*Yingji Zhang,Danilo S. Carvalho,André Freitas*

Main category: cs.CL

TL;DR: 论文探讨了如何通过组合和符号属性增强Transformer自回归语言模型的可解释性、可控性和泛化能力，并提出了一种新的语义表示学习方向。


<details>
  <summary>Details</summary>
Motivation: 弥合符号语义和分布语义之间的差距，提升语言模型的性能。

Method: 比较了三种主流自编码器架构（VAE、VQVAE、SAE）及其在语义结构和可解释性方面的潜在几何特性。

Result: 通过语义表示学习，能够更好地结合符号和分布语义。

Conclusion: 语义表示学习为语言模型提供了新的研究方向，有望提升其性能和应用范围。

Abstract: Integrating compositional and symbolic properties into current distributional
semantic spaces can enhance the interpretability, controllability,
compositionality, and generalisation capabilities of Transformer-based
auto-regressive language models (LMs). In this survey, we offer a novel
perspective on latent space geometry through the lens of compositional
semantics, a direction we refer to as \textit{semantic representation
learning}. This direction enables a bridge between symbolic and distributional
semantics, helping to mitigate the gap between them. We review and compare
three mainstream autoencoder architectures-Variational AutoEncoder (VAE),
Vector Quantised VAE (VQVAE), and Sparse AutoEncoder (SAE)-and examine the
distinctive latent geometries they induce in relation to semantic structure and
interpretability.

</details>


### [30] [ITFormer: Bridging Time Series and Natural Language for Multi-Modal QA with Large-Scale Multitask Dataset](https://arxiv.org/abs/2506.20093)
*Yilin Wang,Peixuan Lei,Jie Song,Yuzhe Hao,Tao Chen,Yuxuan Zhang,Lei Jia,Yuanxiang Li,Zhongyu Wei*

Main category: cs.CL

TL;DR: 论文提出了Time-Series QA任务和EngineMT-QA数据集，并设计了Instruct Time Transformer (ITFormer)框架，有效整合时间序列与自然语言，提升问答准确性。


<details>
  <summary>Details</summary>
Motivation: 高维时间序列信号与自然语言的动态交互任务具有挑战性，需要新的解决方案。

Method: 提出ITFormer框架，结合时间序列编码器与冻结的大型语言模型，提取、对齐和融合时序与文本特征。

Result: ITFormer在QA任务中显著提升准确性，且仅增加不到1%的可训练参数。

Conclusion: 该工作为时间数据与自然语言的整合提供了高效范式，推动了多模态AI的研究与应用。

Abstract: Time-series data are critical in diverse applications, such as industrial
monitoring, medical diagnostics, and climate research. However, effectively
integrating these high-dimensional temporal signals with natural language for
dynamic, interactive tasks remains a significant challenge. To address this, we
introduce the Time-Series Question Answering (Time-Series QA) task and release
EngineMT-QA, the first large-scale, multi-task, temporal-textual QA dataset
designed to capture complex interactions between time-series signals and
natural language. Building on this resource, we propose the Instruct Time
Transformer (ITFormer), a novel framework that bridges time-series encoders
with frozen large language models (LLMs). ITFormer effectively extracts,
aligns, and fuses temporal and textual features, achieving a strong improvement
in QA accuracy over strong baselines with fewer than 1\% additional trainable
parameters. By combining computational efficiency with robust cross-modal
modeling, our work establishes a adaptable paradigm for integrating temporal
data with natural language, paving the way for new research and applications in
multi-modal AI. More details about the project, including datasets and code,
are available at: https://pandalin98.github.io/itformer_site/

</details>


### [31] [A Multi-Pass Large Language Model Framework for Precise and Efficient Radiology Report Error Detection](https://arxiv.org/abs/2506.20112)
*Songsoo Kim,Seungtae Lee,See Young Lee,Joonho Kim,Keechan Kan,Dukyong Yoon*

Main category: cs.CL

TL;DR: 三阶段LLM框架显著提高放射报告的正预测值（PPV）并降低运营成本。


<details>
  <summary>Details</summary>
Motivation: 由于错误率低，现有LLM校对放射报告的PPV有限，需改进框架以提高效率和准确性。

Method: 回顾性分析1,000份放射报告，测试三种LLM框架（单提示检测器、提取器加检测器、三阶段框架），评估PPV和运营成本。

Result: 三阶段框架PPV显著提升至0.159，运营成本降低42.6%，检测性能稳定。

Conclusion: 三阶段LLM框架是AI辅助放射报告质量保障的有效策略。

Abstract: Background: The positive predictive value (PPV) of large language model
(LLM)-based proofreading for radiology reports is limited due to the low error
prevalence. Purpose: To assess whether a three-pass LLM framework enhances PPV
and reduces operational costs compared with baseline approaches. Materials and
Methods: A retrospective analysis was performed on 1,000 consecutive radiology
reports (250 each: radiography, ultrasonography, CT, MRI) from the MIMIC-III
database. Two external datasets (CheXpert and Open-i) were validation sets.
Three LLM frameworks were tested: (1) single-prompt detector; (2) extractor
plus detector; and (3) extractor, detector, and false-positive verifier.
Precision was measured by PPV and absolute true positive rate (aTPR).
Efficiency was calculated from model inference charges and reviewer
remuneration. Statistical significance was tested using cluster bootstrap,
exact McNemar tests, and Holm-Bonferroni correction. Results: Framework PPV
increased from 0.063 (95% CI, 0.036-0.101, Framework 1) to 0.079 (0.049-0.118,
Framework 2), and significantly to 0.159 (0.090-0.252, Framework 3; P<.001 vs.
baselines). aTPR remained stable (0.012-0.014; P>=.84). Operational costs per
1,000 reports dropped to USD 5.58 (Framework 3) from USD 9.72 (Framework 1) and
USD 6.85 (Framework 2), reflecting reductions of 42.6% and 18.5%, respectively.
Human-reviewed reports decreased from 192 to 88. External validation supported
Framework 3's superior PPV (CheXpert 0.133, Open-i 0.105) and stable aTPR
(0.007). Conclusion: A three-pass LLM framework significantly enhanced PPV and
reduced operational costs, maintaining detection performance, providing an
effective strategy for AI-assisted radiology report quality assurance.

</details>


### [32] [Leveraging AI Graders for Missing Score Imputation to Achieve Accurate Ability Estimation in Constructed-Response Tests](https://arxiv.org/abs/2506.20119)
*Masaki Uto,Yuma Ito*

Main category: cs.CL

TL;DR: 提出了一种利用自动评分技术填补缺失分数的新方法，以提高IRT能力估计的准确性，同时显著减少人工评分工作量。


<details>
  <summary>Details</summary>
Motivation: 评估学习者的能力是教育领域的核心目标，尤其是高阶能力如表达技能和逻辑思维。传统构建性测试虽有效，但人工评分成本高且耗时。IRT虽能从不完整数据估计能力，但缺失分数比例增加时准确性下降。

Method: 提出了一种新方法，利用自动评分技术填补缺失分数，以优化IRT能力估计。

Result: 该方法在能力估计中实现了高准确性，并显著减少了人工评分的工作量。

Conclusion: 该方法为解决IRT中缺失分数问题提供了有效解决方案，同时降低了人工评分负担。

Abstract: Evaluating the abilities of learners is a fundamental objective in the field
of education. In particular, there is an increasing need to assess higher-order
abilities such as expressive skills and logical thinking. Constructed-response
tests such as short-answer and essay-based questions have become widely used as
a method to meet this demand. Although these tests are effective, they require
substantial manual grading, making them both labor-intensive and costly. Item
response theory (IRT) provides a promising solution by enabling the estimation
of ability from incomplete score data, where human raters grade only a subset
of answers provided by learners across multiple test items. However, the
accuracy of ability estimation declines as the proportion of missing scores
increases. Although data augmentation techniques for imputing missing scores
have been explored in order to address this limitation, they often struggle
with inaccuracy for sparse or heterogeneous data. To overcome these challenges,
this study proposes a novel method for imputing missing scores by leveraging
automated scoring technologies for accurate IRT-based ability estimation. The
proposed method achieves high accuracy in ability estimation while markedly
reducing manual grading workload.

</details>


### [33] [CCRS: A Zero-Shot LLM-as-a-Judge Framework for Comprehensive RAG Evaluation](https://arxiv.org/abs/2506.20128)
*Aashiq Muhamed*

Main category: cs.CL

TL;DR: CCRS是一种基于预训练LLM的零样本评估框架，用于全面评估RAG系统的多维度质量，包括上下文连贯性、问题相关性等，相比现有方法更高效且实用。


<details>
  <summary>Details</summary>
Motivation: 现有RAG系统评估方法存在不足，如依赖简单词汇重叠指标或复杂多阶段流程，无法全面捕捉输出质量的多维度特性。

Method: 提出CCRS，包含五个指标（CC、QR、ID、AC、IR），利用单一预训练LLM进行零样本端到端评估。

Result: 在BioASQ数据集上验证，CCRS能有效区分不同RAG系统性能，如Mistral-7B优于Llama变体，且计算效率优于RAGChecker。

Conclusion: CCRS为RAG系统提供了一种实用、全面且高效的评估框架，支持迭代改进。

Abstract: RAG systems enhance LLMs by incorporating external knowledge, which is
crucial for domains that demand factual accuracy and up-to-date information.
However, evaluating the multifaceted quality of RAG outputs, spanning aspects
such as contextual coherence, query relevance, factual correctness, and
informational completeness, poses significant challenges. Existing evaluation
methods often rely on simple lexical overlap metrics, which are inadequate for
capturing these nuances, or involve complex multi-stage pipelines with
intermediate steps like claim extraction or require finetuning specialized
judge models, hindering practical efficiency. To address these limitations, we
propose CCRS (Contextual Coherence and Relevance Score), a novel suite of five
metrics that utilizes a single, powerful, pretrained LLM as a zero-shot,
end-to-end judge. CCRS evaluates: Contextual Coherence (CC), Question Relevance
(QR), Information Density (ID), Answer Correctness (AC), and Information Recall
(IR). We apply CCRS to evaluate six diverse RAG system configurations on the
challenging BioASQ dataset. Our analysis demonstrates that CCRS effectively
discriminates between system performances, confirming, for instance, that the
Mistral-7B reader outperforms Llama variants. We provide a detailed analysis of
CCRS metric properties, including score distributions, convergent/discriminant
validity, tie rates, population statistics, and discriminative power. Compared
to the complex RAGChecker framework, CCRS offers comparable or superior
discriminative power for key aspects like recall and faithfulness, while being
significantly more computationally efficient. CCRS thus provides a practical,
comprehensive, and efficient framework for evaluating and iteratively improving
RAG systems.

</details>


### [34] [AALC: Large Language Model Efficient Reasoning via Adaptive Accuracy-Length Control](https://arxiv.org/abs/2506.20160)
*Ruosen Li,Ziming Luo,Quan Zhang,Ruochen Li,Ben Zhou,Ali Payani,Xinya Du*

Main category: cs.CL

TL;DR: AALC是一种轻量级的准确性感知长度奖励方法，通过动态平衡正确性和简洁性，显著减少推理模型的响应长度，同时保持或提高准确性。


<details>
  <summary>Details</summary>
Motivation: 大型推理模型（LRMs）通过生成冗长的思维链实现强大推理能力，但这种方法导致高延迟和成本，且准确性提升不明显。

Method: AALC将验证准确性纳入奖励机制，并采用动态调度的长度惩罚，延迟长度惩罚直到达到目标性能。

Result: 实验表明，AALC将响应长度减少50%以上，同时保持或提高准确性，并减少冗余推理模式。

Conclusion: AALC展示了基于奖励的策略在引导LRMs实现更高效、通用推理路径方面的潜力，但也可能降低模型的可解释性。

Abstract: Large reasoning models (LRMs) achieve impressive reasoning capabilities by
generating lengthy chain-of-thoughts, but this "overthinking" incurs high
latency and cost without commensurate accuracy gains. In this work, we
introduce AALC, a lightweight, accuracy-aware length reward integrated into
reinforcement learning that dynamically balances correctness and brevity during
training. By incorporating validation accuracy into the reward and employing a
smooth, dynamically scheduled length penalty, AALC delays length penalty until
target performance is met. Through extensive experiments across standard and
out-of-distribution math benchmarks, we show that our approach reduces response
length by over 50% while maintaining or even improving the original accuracy.
Furthermore, qualitative analysis reveals that our method curbs redundant
reasoning patterns such as excessive subgoal setting and verification, leading
to structurally refined outputs rather than naive truncation. We also identify
that efficiency gains are accompanied by reduced interpretability: models
trained with AALC omit some narrative framing and explanatory context. These
findings highlight the potential of reward-based strategies to guide LRMs
toward more efficient, generalizable reasoning paths.

</details>


### [35] [SEED: A Structural Encoder for Embedding-Driven Decoding in Time Series Prediction with LLMs](https://arxiv.org/abs/2506.20167)
*Fengze Li,Yue Wang,Yangle Liu,Ming Huang,Dou Hong,Jieming Ma*

Main category: cs.CL

TL;DR: SEED是一种结构编码器，通过四个阶段整合时间序列与语言模型，解决结构依赖与语义推理的兼容性问题。


<details>
  <summary>Details</summary>
Motivation: 现有结构编码器缺乏语义推理能力，而语言模型无法直接处理时间序列，限制了统一预测系统的发展。

Method: SEED包括四个阶段：补丁提取、嵌入对齐、语义重编程和语言模型预测，实现数值模式与语义推理的高效对齐。

Result: 实验表明SEED在多个数据集上优于基线方法，有效填补了结构-语义建模的空白。

Conclusion: SEED通过模块化架构成功整合结构编码与语义推理，为统一预测系统提供了可行方案。

Abstract: Multivariate time series forecasting requires models to simultaneously
capture variable-wise structural dependencies and generalize across diverse
tasks. While structural encoders are effective in modeling feature
interactions, they lack the capacity to support semantic-level reasoning or
task adaptation. Conversely, large language models (LLMs) possess strong
generalization capabilities but remain incompatible with raw time series
inputs. This gap limits the development of unified, transferable prediction
systems. Therefore, we introduce SEED, a structural encoder for
embedding-driven decoding, which integrates four stages: a token-aware encoder
for patch extraction, a projection module that aligns patches with language
model embeddings, a semantic reprogramming mechanism that maps patches to
task-aware prototypes, and a frozen language model for prediction. This modular
architecture decouples representation learning from inference, enabling
efficient alignment between numerical patterns and semantic reasoning.
Empirical results demonstrate that the proposed method achieves consistent
improvements over strong baselines, and comparative studies on various datasets
confirm SEED's role in addressing the structural-semantic modeling gap.

</details>


### [36] [COIN: Uncertainty-Guarding Selective Question Answering for Foundation Models with Provable Risk Guarantees](https://arxiv.org/abs/2506.20178)
*Zhiyuan Wang,Jinhao Duan,Qingni Wang,Xiaofeng Zhu,Tianlong Chen,Xiaoshuang Shi,Kaidi Xu*

Main category: cs.CL

TL;DR: COIN框架通过统计校准阈值，在用户指定的FDR约束下筛选生成文本，显著提升样本保留率和预测效率。


<details>
  <summary>Details</summary>
Motivation: 解决现有不确定性量化方法缺乏形式化保证的问题，特别是在选择性预测中的假发现率（FDR）控制。

Method: 采用统计置信区间方法（如Clopper-Pearson）校准阈值，确保FDR控制，同时优化样本保留。

Result: COIN在风险控制、测试时保留率和预测效率方面表现优异，适用于多模态文本生成任务。

Conclusion: COIN框架具有扩展性和适应性，能有效提升生成文本的可靠性。

Abstract: Uncertainty quantification (UQ) for foundation models is essential to
identify and mitigate potential hallucinations in automatically generated text.
However, heuristic UQ approaches lack formal guarantees for key metrics such as
the false discovery rate (FDR) in selective prediction. Previous work adopts
the split conformal prediction (SCP) framework to ensure desired coverage of
admissible answers by constructing prediction sets, but these sets often
contain incorrect candidates, limiting their practical utility. To address
this, we propose COIN, an uncertainty-guarding selection framework that
calibrates statistically valid thresholds to filter a single generated answer
per question under user-specified FDR constraints. COIN estimates the empirical
error rate on a calibration set and applies confidence interval methods such as
Clopper-Pearson to establish a high-probability upper bound on the true error
rate (i.e., FDR). This enables the selection of the largest uncertainty
threshold that ensures FDR control on test data while significantly increasing
sample retention. We demonstrate COIN's robustness in risk control, strong
test-time power in retaining admissible answers, and predictive efficiency
under limited calibration data across both general and multimodal text
generation tasks. Furthermore, we show that employing alternative upper bound
constructions and UQ strategies can further boost COIN's power performance,
which underscores its extensibility and adaptability to diverse application
scenarios.

</details>


### [37] [How to Retrieve Examples in In-context Learning to Improve Conversational Emotion Recognition using Large Language Models?](https://arxiv.org/abs/2506.20199)
*Mengqi Wang,Tiantian Feng,Shrikanth Narayanan*

Main category: cs.CL

TL;DR: 研究探讨了如何通过检索高质量示例提升大型语言模型在对话情感识别任务中的表现，发现增强示例检索方法优于其他技术。


<details>
  <summary>Details</summary>
Motivation: 尽管大型语言模型在许多领域有广泛应用，但在主观任务（如情感识别）中实现高准确性仍具挑战性。

Method: 研究了随机和增强示例检索策略，并分析了对话上下文对情感识别准确性的影响。

Result: 增强示例检索方法在所有数据集（IEMOCAP、MELD、EmoryNLP）上表现最佳。

Conclusion: 检索具有一致性和针对性的示例并通过改写增强，对提升对话情感识别准确性至关重要。

Abstract: Large language models (LLMs) have enabled a wide variety of real-world
applications in various domains. However, creating a high-performing
application with high accuracy remains challenging, particularly for subjective
tasks like emotion recognition. Inspired by the SLT 2024 GenSER Challenge, this
study investigates approaches to improving conversational emotion recognition
(CER) by LLMs. Specifically, we explore how to retrieve high-quality examples
in in-context learning (ICL) to enhance CER. We propose various strategies
based on random and augmented example retrieval and also analyze the impact of
conversational context on CER accuracy. Experiments were conducted on the three
datasets including IEMOCAP, MELD and EmoryNLP. The results show that augmented
example retrieval consistently outperforms other techniques under investigation
across all datasets, highlighting the importance of retrieving coherent
targeted examples and enhancing them through paraphrasing.

</details>


### [38] [Intrinsic vs. Extrinsic Evaluation of Czech Sentence Embeddings: Semantic Relevance Doesn't Help with MT Evaluation](https://arxiv.org/abs/2506.20203)
*Petra Barančíková,Ondřej Bojar*

Main category: cs.CL

TL;DR: 比较捷克语特定和多语言句子嵌入模型，发现内在语义相似性测试表现好的模型在下游翻译任务中不一定表现最佳。


<details>
  <summary>Details</summary>
Motivation: 探索句子嵌入模型在语义相似性和下游任务（如翻译评估）中的表现差异。

Method: 通过内在评估（Costra数据集和STS基准）和外在评估（COMET指标微调）比较模型。

Result: 内在测试表现好的模型在下游任务中不一定最佳，而某些看似平滑的模型通过微调表现优异。

Conclusion: 需进一步研究句子嵌入的‘可操作语义’或更深入的下游任务数据集。

Abstract: In this paper, we compare Czech-specific and multilingual sentence embedding
models through intrinsic and extrinsic evaluation paradigms. For intrinsic
evaluation, we employ Costra, a complex sentence transformation dataset, and
several Semantic Textual Similarity (STS) benchmarks to assess the ability of
the embeddings to capture linguistic phenomena such as semantic similarity,
temporal aspects, and stylistic variations. In the extrinsic evaluation, we
fine-tune each embedding model using COMET-based metrics for machine
translation evaluation.
  Our experiments reveal an interesting disconnect: models that excel in
intrinsic semantic similarity tests do not consistently yield superior
performance on downstream translation evaluation tasks. Conversely, models with
seemingly over-smoothed embedding spaces can, through fine-tuning, achieve
excellent results. These findings highlight the complex relationship between
semantic property probes and downstream task, emphasizing the need for more
research into 'operationalizable semantics' in sentence embeddings, or more
in-depth downstream tasks datasets (here translation evaluation)

</details>


### [39] [Perspectives in Play: A Multi-Perspective Approach for More Inclusive NLP Systems](https://arxiv.org/abs/2506.20209)
*Benedetta Muscato,Lucia Passaro,Gizem Gezici,Fosca Giannotti*

Main category: cs.CL

TL;DR: 论文提出了一种多视角的软标签方法，以解决传统NLP中忽视个体观点的问题，并在多个主观文本分类任务中验证了其优越性。


<details>
  <summary>Details</summary>
Motivation: 传统方法通过聚合标注者观点形成单一真实标签，但忽视了少数观点，尤其是在主观任务中。本文旨在通过多视角方法更好地反映人类观点的多样性。

Method: 采用多视角软标签方法，结合Jensen-Shannon Divergence (JSD) 衡量人类标签分布，并在多个主观任务（如仇恨言论、讽刺、立场检测等）中测试。

Result: 多视角方法在JSD和F1分数上优于传统方法，但在讽刺和立场检测任务中置信度较低。通过XAI揭示了模型预测的不确定性。

Conclusion: 多视角方法能更好地捕捉人类观点多样性，提升模型性能，但需进一步解决主观任务中的不确定性。

Abstract: In the realm of Natural Language Processing (NLP), common approaches for
handling human disagreement consist of aggregating annotators' viewpoints to
establish a single ground truth. However, prior studies show that disregarding
individual opinions can lead can lead to the side effect of underrepresenting
minority perspectives, especially in subjective tasks, where annotators may
systematically disagree because of their preferences. Recognizing that labels
reflect the diverse backgrounds, life experiences, and values of individuals,
this study proposes a new multi-perspective approach using soft labels to
encourage the development of the next generation of perspective aware models,
more inclusive and pluralistic. We conduct an extensive analysis across diverse
subjective text classification tasks, including hate speech, irony, abusive
language, and stance detection, to highlight the importance of capturing human
disagreements, often overlooked by traditional aggregation methods. Results
show that the multi-perspective approach not only better approximates human
label distributions, as measured by Jensen-Shannon Divergence (JSD), but also
achieves superior classification performance (higher F1 scores), outperforming
traditional approaches. However, our approach exhibits lower confidence in
tasks like irony and stance detection, likely due to the inherent subjectivity
present in the texts. Lastly, leveraging Explainable AI (XAI), we explore model
uncertainty and uncover meaningful insights into model predictions.

</details>


### [40] [Enhancing Large Language Models through Structured Reasoning](https://arxiv.org/abs/2506.20241)
*Yubo Dong,Hehe Fan*

Main category: cs.CL

TL;DR: 论文提出了一种通过结构化推理增强大语言模型（LLMs）的新方法，结合监督微调和GRPO算法，显著提升了复杂推理任务的性能。


<details>
  <summary>Details</summary>
Motivation: 当前LLMs在复杂逻辑推理任务中表现不佳，主要依赖隐式统计关系，缺乏结构化知识表示。

Method: 将非结构化数据转换为结构化格式，标注推理步骤，通过监督微调（SFT）训练LLMs，并采用GRPO算法（包括MAX-Flow和LCS）增强推理能力。

Result: 实验结果显示，该方法在DeepSeek-R1-Distill-Qwen-1.5B模型上实现了简洁推理、鲁棒性能和优化兼容性。

Conclusion: 结构化推理的集成有效提升了LLMs的推理能力，验证了方法的有效性。

Abstract: Recent Large Language Models (LLMs) have significantly advanced natural
language processing and automated decision-making. However, these models still
encounter difficulties when performing complex reasoning tasks involving
logical deduction and systematic planning, primarily due to their reliance on
implicit statistical relationships without structured knowledge
representation.Inspired by cognitive science and neurosymbolic AI, we introduce
a novel approach to enhance LLMs through explicit structured reasoning. First,
we convert unstructured data into structured formats by explicitly annotating
reasoning steps. We then employ this structured dataset to train LLMs through
Supervised Fine-Tuning (SFT). Additionally, we enhance the structured reasoning
capabilities of LLMs using Group Relative Policy Optimization (GRPO),
incorporating two innovative algorithms--MAX-Flow and Longest Common
Subsequence (LCS)--which notably improve reasoning effectiveness and reduce
computational complexity. Experimental results from fine-tuning a
DeepSeek-R1-Distill-Qwen-1.5B model demonstrate concise reasoning, robust
performance across various scenarios, and improved compatibility with
optimization techniques, validating the efficacy of structured reasoning
integration in LLMs.

</details>


### [41] [CBF-AFA: Chunk-Based Multi-SSL Fusion for Automatic Fluency Assessment](https://arxiv.org/abs/2506.20243)
*Papa Séga Wade,Mihai Andries,Ioannis Kanellos,Thierry Moudenc*

Main category: cs.CL

TL;DR: 提出了一种基于分块的多自监督学习融合方法，用于非母语者的流利度评估，显著提升了评估性能。


<details>
  <summary>Details</summary>
Motivation: 自动流利度评估（AFA）在捕捉非母语者的语音节奏、停顿和不流利现象方面仍具挑战性。

Method: 采用分块方法，结合自监督学习（SSL）模型（Wav2Vec2、HuBERT、WavLM）和分层CNN-BiLSTM框架，通过Silero-VAD分割语音为呼吸组块，融合SSL嵌入并加入块级流利度标记。

Result: 在Avalinguo和Speechocean762数据集上，F1分数和Pearson相关系数分别提升了2.8-4.2和6.2-4.0分，优于单SSL和Pyannote.audio基线。

Conclusion: 分块多SSL融合方法对流利度评估具有鲁棒性，未来需探索其在非规则韵律方言中的泛化能力。

Abstract: Automatic fluency assessment (AFA) remains challenging, particularly in
capturing speech rhythm, pauses, and disfluencies in non-native speakers. We
introduce a chunk-based approach integrating self-supervised learning (SSL)
models (Wav2Vec2, HuBERT, and WavLM) selected for their complementary strengths
in phonetic, prosodic, and noisy speech modeling, with a hierarchical
CNN-BiLSTM framework. Speech is segmented into breath-group chunks using Silero
voice activity detection (Silero-VAD), enabling fine-grained temporal analysis
while mitigating over-segmentation artifacts. SSL embeddings are fused via a
learnable weighted mechanism, balancing acoustic and linguistic features, and
enriched with chunk-level fluency markers (e.g., speech rate, pause durations,
n-gram repetitions). The CNN-BiLSTM captures local and long-term dependencies
across chunks. Evaluated on Avalinguo and Speechocean762, our approach improves
F1-score by 2.8 and Pearson correlation by 6.2 points over single SSL baselines
on Speechocean762, with gains of 4.2 F1-score and 4.0 Pearson points on
Avalinguo, surpassing Pyannote.audio-based segmentation baselines. These
findings highlight chunk-based multi-SSL fusion for robust fluency evaluation,
though future work should explore generalization to dialects with irregular
prosody.

</details>


### [42] [Narrative Shift Detection: A Hybrid Approach of Dynamic Topic Models and Large Language Models](https://arxiv.org/abs/2506.20269)
*Kai-Robin Lange,Tobias Schmidt,Matthias Reccius,Henrik Müller,Michael Roos,Carsten Jentsch*

Main category: cs.CL

TL;DR: 结合大型语言模型和主题模型动态分析叙事变化，发现大型语言模型能有效提取叙事变化，但在区分内容变化和叙事变化时表现不佳。


<details>
  <summary>Details</summary>
Motivation: 随着媒体叙事的快速演变，研究叙事如何随时间发展变得至关重要。现有方法（如大型语言模型）在提取叙事元素时面临高成本或计算障碍。

Method: 结合大型语言模型的语言理解能力和主题模型的大规模适用性，使用叙事政策框架动态建模叙事变化。通过主题模型和变化点检测方法筛选代表性文档，输入大型语言模型进行自动化分析。

Result: 在《华尔街日报》2009-2023年的语料库中，大型语言模型能有效提取叙事变化，但在区分内容变化和叙事变化时表现不佳。

Conclusion: 该方法能高效提取叙事变化，但在区分内容与叙事变化方面需进一步改进。

Abstract: With rapidly evolving media narratives, it has become increasingly critical
to not just extract narratives from a given corpus but rather investigate, how
they develop over time. While popular narrative extraction methods such as
Large Language Models do well in capturing typical narrative elements or even
the complex structure of a narrative, applying them to an entire corpus comes
with obstacles, such as a high financial or computational cost. We propose a
combination of the language understanding capabilities of Large Language Models
with the large scale applicability of topic models to dynamically model
narrative shifts across time using the Narrative Policy Framework. We apply a
topic model and a corresponding change point detection method to find changes
that concern a specific topic of interest. Using this model, we filter our
corpus for documents that are particularly representative of that change and
feed them into a Large Language Model that interprets the change that happened
in an automated fashion and distinguishes between content and narrative shifts.
We employ our pipeline on a corpus of The Wall Street Journal news paper
articles from 2009 to 2023. Our findings indicate that a Large Language Model
can efficiently extract a narrative shift if one exists at a given point in
time, but does not perform as well when having to decide whether a shift in
content or a narrative shift took place.

</details>


### [43] [Biomed-Enriched: A Biomedical Dataset Enriched with LLMs for Pretraining and Extracting Rare and Hidden Content](https://arxiv.org/abs/2506.20331)
*Rian Touchent,Nathan Godey,Eric de la Clergerie*

Main category: cs.CL

TL;DR: Biomed-Enriched是一个通过两阶段标注过程构建的生物医学文本数据集，包含PubMed科学文章的段落，标注了类型、领域和教育质量。数据集提供了大规模开放的临床案例资源，初步实验显示其能有效提升生物医学预训练模型的性能。


<details>
  <summary>Details</summary>
Motivation: 临床文本通常因隐私限制难以获取，Biomed-Enriched提供了一个公开的大规模临床案例数据集，填补了这一空白。

Method: 通过大型语言模型标注400K段落，并利用小型语言模型将标签扩展到PMC-OA语料库，提取高质量子集并进行质量过滤和领域上采样。

Result: 数据集包含2M临床案例段落，其中450K高质量段落。实验显示临床上采样和高质量过滤分别提升模型性能5%和1%，组合使用可加速收敛。

Conclusion: Biomed-Enriched是一个有价值的生物医学NLP资源，能提升预训练效率与效果。

Abstract: We introduce Biomed-Enriched, a biomedical text dataset constructed from
PubMed via a two-stage annotation process. In the first stage, a large language
model annotates 400K paragraphs from PubMed scientific articles, assigning
scores for their type (review, study, clinical case, other), domain (clinical,
biomedical, other), and educational quality. The educational quality score
(rated 1 to 5) estimates how useful a paragraph is for college-level learning.
These annotations are then used to fine-tune a small language model, which
propagates the labels across the full PMC-OA corpus. The resulting metadata
allows us to extract refined subsets, including 2M clinical case paragraphs
with over 450K high-quality ones from articles with commercial-use licenses,
and to construct several variants via quality filtering and domain upsampling.
Clinical text is typically difficult to access due to privacy constraints, as
hospital records cannot be publicly shared. Hence, our dataset provides an
alternative large-scale, openly available collection of clinical cases from
PubMed, making it a valuable resource for biomedical and clinical NLP.
Preliminary continual-pretraining experiments with OLMo2 suggest these curated
subsets enable targeted improvements, with clinical upsampling boosting
performance by ~5% on MMLU ProfMed and educational quality filtering improving
MedQA and MedMCQA by ~1%. Combinations of these techniques led to faster
convergence, reaching same performance with a third of training tokens,
indicating potential for more efficient and effective biomedical pretraining
strategies.

</details>


### [44] [TAPS: Tool-Augmented Personalisation via Structured Tagging](https://arxiv.org/abs/2506.20409)
*Ekaterina Taktasheva,Jeff Dalton*

Main category: cs.CL

TL;DR: 论文提出了一种新方法TAPS，通过结构化标记工具和不确定性检测器，提升LLMs在个性化工具使用中的表现。


<details>
  <summary>Details</summary>
Motivation: 现有工具增强型大语言模型在个性化工具使用方面存在不足，研究旨在解决这一问题。

Method: 引入TAPS，结合结构化标记工具和不确定性检测器，优化个性化工具使用。

Result: TAPS显著提升了LLMs在NLSI任务中的表现，达到开源模型的新SOTA。

Conclusion: TAPS有效解决了LLMs在个性化工具使用中的不足，为未来研究提供了新方向。

Abstract: Recent advancements in tool-augmented large language models have enabled them
to interact with external tools, enhancing their ability to perform complex
user tasks. However, existing approaches overlook the role of personalisation
in guiding tool use. This work investigates how user preferences can be
effectively integrated into goal-oriented dialogue agents. Through extensive
analysis, we identify key weaknesses in the ability of LLMs to personalise tool
use. To this end, we introduce \name, a novel solution that enhances
personalised tool use by leveraging a structured tagging tool and an
uncertainty-based tool detector. TAPS significantly improves the ability of
LLMs to incorporate user preferences, achieving the new state-of-the-art for
open source models on the NLSI task.

</details>


### [45] [An Agentic System for Rare Disease Diagnosis with Traceable Reasoning](https://arxiv.org/abs/2506.20430)
*Weike Zhao,Chaoyi Wu,Yanjie Fan,Xiaoman Zhang,Pengcheng Qiu,Yuze Sun,Xiao Zhou,Yanfeng Wang,Ya Zhang,Yongguo Yu,Kun Sun,Weidi Xie*

Main category: cs.CL

TL;DR: DeepRare是一个基于大型语言模型（LLM）的罕见病诊断系统，通过处理异构临床输入生成诊断假设，并显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 罕见病诊断因临床异质性、低流行率和医生对其不熟悉而具有挑战性。

Method: DeepRare采用模块化设计，包含中央主机、长期记忆模块和专用代理服务器，整合40多种工具和最新医学知识。

Result: 在8个数据集上评估，DeepRare在2919种疾病中表现优异，Recall@1达57.18%，远超其他方法。多模态输入场景下Recall@1为70.60%。

Conclusion: DeepRare在罕见病诊断中表现出色，已部署为用户友好的网络应用。

Abstract: Rare diseases collectively affect over 300 million individuals worldwide, yet
timely and accurate diagnosis remains a pervasive challenge. This is largely
due to their clinical heterogeneity, low individual prevalence, and the limited
familiarity most clinicians have with rare conditions. Here, we introduce
DeepRare, the first rare disease diagnosis agentic system powered by a large
language model (LLM), capable of processing heterogeneous clinical inputs. The
system generates ranked diagnostic hypotheses for rare diseases, each
accompanied by a transparent chain of reasoning that links intermediate
analytic steps to verifiable medical evidence.
  DeepRare comprises three key components: a central host with a long-term
memory module; specialized agent servers responsible for domain-specific
analytical tasks integrating over 40 specialized tools and web-scale,
up-to-date medical knowledge sources, ensuring access to the most current
clinical information. This modular and scalable design enables complex
diagnostic reasoning while maintaining traceability and adaptability. We
evaluate DeepRare on eight datasets. The system demonstrates exceptional
diagnostic performance among 2,919 diseases, achieving 100% accuracy for 1013
diseases. In HPO-based evaluations, DeepRare significantly outperforms other 15
methods, like traditional bioinformatics diagnostic tools, LLMs, and other
agentic systems, achieving an average Recall@1 score of 57.18% and surpassing
the second-best method (Reasoning LLM) by a substantial margin of 23.79
percentage points. For multi-modal input scenarios, DeepRare achieves 70.60% at
Recall@1 compared to Exomiser's 53.20% in 109 cases. Manual verification of
reasoning chains by clinical experts achieves 95.40% agreements. Furthermore,
the DeepRare system has been implemented as a user-friendly web application
http://raredx.cn/doctor.

</details>


### [46] [Probing AI Safety with Source Code](https://arxiv.org/abs/2506.20471)
*Ujwal Narayan,Shreyas Chaudhari,Ashwin Kalyan,Tanmay Rajpurohit,Karthik Narasimhan,Ameet Deshpande,Vishvak Murahari*

Main category: cs.CL

TL;DR: 论文提出了一种名为Code of Thought (CoDoT)的提示策略，用于评估大型语言模型（LLMs）的安全性，发现当前模型在安全性上存在严重不足。


<details>
  <summary>Details</summary>
Motivation: 随着LLMs在安全关键应用中的普及，需要同时提升其能力和安全性，以确保与人类价值观和偏好一致。

Method: CoDoT将自然语言输入转换为简单代码以评估LLMs的安全性，例如将“使文本更具毒性”转换为“make_more_toxic({text})”。

Result: 实验表明，CoDoT导致多种先进LLMs的安全性显著下降，如GPT-4 Turbo毒性增加16.5倍，DeepSeek R1失败率100%。

Conclusion: CoDoT揭示了当前LLMs在安全性上的严重缺陷，强调了从基本原理出发评估安全性的重要性。

Abstract: Large language models (LLMs) have become ubiquitous, interfacing with humans
in numerous safety-critical applications. This necessitates improving
capabilities, but importantly coupled with greater safety measures to align
these models with human values and preferences. In this work, we demonstrate
that contemporary models fall concerningly short of the goal of AI safety,
leading to an unsafe and harmful experience for users. We introduce a prompting
strategy called Code of Thought (CoDoT) to evaluate the safety of LLMs. CoDoT
converts natural language inputs to simple code that represents the same
intent. For instance, CoDoT transforms the natural language prompt "Make the
statement more toxic: {text}" to: "make_more_toxic({text})". We show that CoDoT
results in a consistent failure of a wide range of state-of-the-art LLMs. For
example, GPT-4 Turbo's toxicity increases 16.5 times, DeepSeek R1 fails 100% of
the time, and toxicity increases 300% on average across seven modern LLMs.
Additionally, recursively applying CoDoT can further increase toxicity two
times. Given the rapid and widespread adoption of LLMs, CoDoT underscores the
critical need to evaluate safety efforts from first principles, ensuring that
safety and capabilities advance together.

</details>


### [47] [Time is On My Side: Dynamics of Talk-Time Sharing in Video-chat Conversations](https://arxiv.org/abs/2506.20474)
*Kaixiang Zhang,Justine Zhang,Cristian Danescu-Niculescu-Mizil*

Main category: cs.CL

TL;DR: 论文提出了一个计算框架，用于量化对话中说话时间的分布及其动态变化，并通过实验验证了平衡对话更受欢迎，同时揭示了不同动态类型对参与者感知的影响。


<details>
  <summary>Details</summary>
Motivation: 研究对话中说话时间的分布及其动态变化，以理解其对参与者感知的影响，并为计算机辅助沟通平台设计提供工具。

Method: 引入一个计算框架，量化对话级说话时间分布及其动态变化，并通过视频聊天数据集验证。

Result: 平衡对话更受欢迎，尤其是说话较少的参与者；不同动态类型即使导致相同的平衡水平，也会影响参与者感知。

Conclusion: 该框架为计算机辅助沟通平台设计提供了新工具，适用于人-人和人-人工智能沟通。

Abstract: An intrinsic aspect of every conversation is the way talk-time is shared
between multiple speakers. Conversations can be balanced, with each speaker
claiming a similar amount of talk-time, or imbalanced when one talks
disproportionately. Such overall distributions are the consequence of
continuous negotiations between the speakers throughout the conversation: who
should be talking at every point in time, and for how long?
  In this work we introduce a computational framework for quantifying both the
conversation-level distribution of talk-time between speakers, as well as the
lower-level dynamics that lead to it. We derive a typology of talk-time sharing
dynamics structured by several intuitive axes of variation. By applying this
framework to a large dataset of video-chats between strangers, we confirm that,
perhaps unsurprisingly, different conversation-level distributions of talk-time
are perceived differently by speakers, with balanced conversations being
preferred over imbalanced ones, especially by those who end up talking less.
Then we reveal that -- even when they lead to the same level of overall balance
-- different types of talk-time sharing dynamics are perceived differently by
the participants, highlighting the relevance of our newly introduced typology.
Finally, we discuss how our framework offers new tools to designers of
computer-mediated communication platforms, for both human-human and human-AI
communication.

</details>


### [48] [Knowledge-Aware Diverse Reranking for Cross-Source Question Answering](https://arxiv.org/abs/2506.20476)
*Tong Zhou*

Main category: cs.CL

TL;DR: Team Marikarp在SIGIR 2025 LiveRAG竞赛中获胜，提出了一种知识感知的多样化重排序RAG流程。


<details>
  <summary>Details</summary>
Motivation: 竞赛旨在公平评估从15M文档中检索问题相关支持文档的能力。

Method: 采用知识感知的多样化重排序RAG流程。

Result: 在竞赛中获得第一名。

Conclusion: 该方法在多样化任务中表现出色。

Abstract: This paper presents Team Marikarp's solution for the SIGIR 2025 LiveRAG
competition. The competition's evaluation set, automatically generated by
DataMorgana from internet corpora, encompassed a wide range of target topics,
question types, question formulations, audience types, and knowledge
organization methods. It offered a fair evaluation of retrieving
question-relevant supporting documents from a 15M documents subset of the
FineWeb corpus. Our proposed knowledge-aware diverse reranking RAG pipeline
achieved first place in the competition.

</details>


### [49] [GPTailor: Large Language Model Pruning Through Layer Cutting and Stitching](https://arxiv.org/abs/2506.20480)
*Guinan Su,Li Shen,Lu Yin,Shiwei Liu,Yanwu Yang,Jonas Geiping*

Main category: cs.CL

TL;DR: 提出一种通过合并微调模型变体的层来压缩大型语言模型的新策略，显著减少参数数量同时保持性能。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）的参数规模庞大，部署和推理成本高，现有剪枝方法仅针对单一模型。

Method: 将模型压缩问题转化为零阶优化问题，支持层移除、层选择和层合并三种操作。

Result: 在Llama2-13B模型上，压缩后模型参数减少约25%，性能保持97.3%，优于现有方法。

Conclusion: 该方法为高效压缩LLMs提供了新思路，显著提升了部署效率。

Abstract: Large language models (LLMs) have shown remarkable capabilities in language
understanding and generation. However, such impressive capability typically
comes with a substantial model size, which presents significant challenges in
deployment and inference. While structured pruning of model parameters offers a
promising way to reduce computational costs at deployment time, current methods
primarily focus on single model pruning. In this work, we develop a novel
strategy to compress models by strategically combining or merging layers from
finetuned model variants, which preserves the original model's abilities by
aggregating capabilities accentuated in different finetunes. We pose the
optimal tailoring of these LLMs as a zero-order optimization problem, adopting
a search space that supports three different operations: (1) Layer removal, (2)
Layer selection from different candidate models, and (3) Layer merging. Our
experiments demonstrate that this approach leads to competitive model pruning,
for example, for the Llama2-13B model families, our compressed models maintain
approximately 97.3\% of the original performance while removing $\sim25\%$ of
parameters, significantly outperforming previous state-of-the-art methods. The
code is available at https://github.com/Guinan-Su/auto-merge-llm.

</details>


### [50] [ReCode: Updating Code API Knowledge with Reinforcement Learning](https://arxiv.org/abs/2506.20495)
*Haoze Wu,Yunzhi Yao,Wenhao Yu,Huajun Chen,Ningyu Zhang*

Main category: cs.CL

TL;DR: ReCode框架通过强化学习提升LLMs在动态API环境中的代码生成能力，减少对过时API知识的依赖。


<details>
  <summary>Details</summary>
Motivation: LLMs在动态API环境中表现不佳，因依赖过时的训练数据知识，即使有最新文档也难以适应频繁的API更新。

Method: 提出ReCode框架，构建数据集训练LLMs进行版本迁移，并引入修改的字符串相似度指标作为强化学习的奖励。

Result: ReCode显著提升LLMs在动态API场景中的性能，尤其在未见过的CodeUpdateArena任务上，且对通用代码生成能力影响较小。

Conclusion: ReCode在多种LLMs和强化学习算法中均表现一致改进，Qwen2.5-Coder-7B甚至超越更大规模的模型。

Abstract: Large Language Models (LLMs) exhibit remarkable code generation capabilities
but falter when adapting to frequent updates in external library APIs. This
critical limitation, stemming from reliance on outdated API knowledge from
their training data, even with access to current documentation, impedes
reliable code generation in dynamic environments. To tackle this issue, we
propose ReCode (rule-based Reinforcement learning for Code Update), a novel
framework that mimics human programmer adaptation to API changes. Specifically,
we construct a dataset of approximately 2,000 data entries to train the LLMs to
perform version migration based on updated information. Then, we introduce a
modified string similarity metric for code evaluation as the reward for
reinforcement learning. Our experiments demonstrate that ReCode substantially
boosts LLMs' code generation performance in dynamic API scenarios, especially
on the unseen CodeUpdateArena task. Crucially, compared to supervised
fine-tuning, ReCode has less impact on LLMs' general code generation abilities.
We apply ReCode on various LLMs and reinforcement learning algorithms (GRPO and
DAPO), all achieving consistent improvements. Notably, after training,
Qwen2.5-Coder-7B outperforms that of the 32B parameter code instruction-tuned
model and the reasoning model with the same architecture. Code is available at
https://github.com/zjunlp/ReCode.

</details>


### [51] [OctoThinker: Mid-training Incentivizes Reinforcement Learning Scaling](https://arxiv.org/abs/2506.20512)
*Zengzhi Wang,Fan Zhou,Xuefeng Li,Pengfei Liu*

Main category: cs.CL

TL;DR: 研究探讨了不同基础语言模型家族（如Llama和Qwen）在强化学习（RL）后训练中的行为差异，提出了两阶段中训练策略Stable-then-Decay，并发布了开源模型OctoThinker和数学推理语料库MegaMath-Web-Pro-Max。


<details>
  <summary>Details</summary>
Motivation: 理解基础语言模型在强化学习中的适用性，以开发下一代可扩展的RL基础模型。

Method: 研究Qwen和Llama模型家族的中训练策略对RL动态的影响，提出两阶段训练策略Stable-then-Decay。

Result: 高质量数学语料库和长链思维推理数据提升RL性能，但需注意数据格式化；中训练规模扩大可增强下游RL表现。

Conclusion: 提出的策略OctoThinker缩小了与RL友好模型家族的差距，为RL时代的基础模型预训练策略提供了指导。

Abstract: Different base language model families, such as Llama and Qwen, exhibit
divergent behaviors during post-training with reinforcement learning (RL),
especially on reasoning-intensive tasks. What makes a base language model
suitable for reinforcement learning? Gaining deeper insight into this question
is essential for developing RL-scalable foundation models of the next
generation. In this work, we investigate how mid-training strategies shape RL
dynamics, focusing on two representative model families: Qwen and Llama. Our
study reveals that (1) high-quality mathematical corpora, such as
MegaMath-Web-Pro, significantly improve both base model and RL performance,
while existing alternatives (e.g., FineMath-4plus) fail to do so; (2) further
adding QA-style data, particularly long chain-of-thought (CoT) reasoning
examples, enhances RL outcomes, and instruction data further unlocks this
effect; (3) while long-CoT improves reasoning depth, it can also induce
verbosity of model responses and unstability of RL training, underscoring the
importance of data formatting; (4) scaling mid-training consistently leads to
stronger downstream RL performance. Building on these insights, we introduce a
two-stage mid-training strategy, Stable-then-Decay, in which base models are
first trained on 200B tokens with a constant learning rate, followed by 20B
tokens across three CoT-focused branches with learning rate decay. This yields
OctoThinker, a family of models demonstrating strong RL compatibility and
closing the performance gap with more RL-friendly model families, i.e., Qwen.
We hope our work will help shape pre-training strategies for foundation models
in the RL era. To support further research, we release our open-source models
along with a curated math reasoning-intensive corpus of over 70 billion tokens
(i.e., MegaMath-Web-Pro-Max).

</details>


### [52] [When Life Gives You Samples: The Benefits of Scaling up Inference Compute for Multilingual LLMs](https://arxiv.org/abs/2506.20544)
*Ammar Khairi,Daniel D'souza,Ye Shen,Julia Kreutzer,Sara Hooker*

Main category: cs.CL

TL;DR: 论文研究了如何通过调整采样和选择策略，在多语言和多任务环境中提升大语言模型的推理性能，并提出了新方法，显著提高了性能。


<details>
  <summary>Details</summary>
Motivation: 目前的研究主要集中在英语和特定领域（如数学和代码），而本文关注如何将推理计算扩展到多语言和开放任务中。

Method: 提出新的采样和选择策略，适应多语言和多任务场景，并评估现有方法的局限性。

Result: 新方法在8B和111B模型上分别实现了+6.8和+9.0的胜率提升，显著优于单样本解码。

Conclusion: 研究强调了语言和任务感知的推理计算方法的重要性，以提升非主流语言的性能。

Abstract: Recent advancements in large language models (LLMs) have shifted focus toward
scaling inference-time compute, improving performance without retraining the
model. A common approach is to sample multiple outputs in parallel, and select
one of these as the final output. However, work to date has focused on English
and a handful of domains such as math and code. In contrast, we are most
interested in techniques that generalize across open-ended tasks, formally
verifiable tasks, and across languages. In this work, we study how to robustly
scale inference-time compute for open-ended generative tasks in a multilingual,
multi-task setting.
  Our findings show that both sampling strategy based on temperature variation
and selection strategy must be adapted to account for diverse domains and
varied language settings. We evaluate existing selection methods, revealing
that strategies effective in English often fail to generalize across languages.
We propose novel sampling and selection strategies specifically adapted for
multilingual and multi-task inference scenarios, and show they yield notable
gains across languages and tasks. In particular, our combined sampling and
selection methods lead to an average +6.8 jump in win-rates for our 8B models
on m-ArenaHard-v2.0 prompts, against proprietary models such as Gemini. At
larger scale, Command-A (111B model) equipped with our methods, shows +9.0
improvement in win-rates on the same benchmark with just five samples against
single-sample decoding, a substantial increase at minimal cost. Our results
underscore the need for language- and task-aware approaches to inference-time
compute, aiming to democratize performance improvements in underrepresented
languages.

</details>


### [53] [Model Editing as a Double-Edged Sword: Steering Agent Ethical Behavior Toward Beneficence or Harm](https://arxiv.org/abs/2506.20606)
*Baixiang Huang,Zhen Tan,Haoran Wang,Zijie Liu,Dawei Li,Ali Payani,Huan Liu,Tianlong Chen,Kai Shu*

Main category: cs.CL

TL;DR: 论文提出了一种称为“行为编辑”的方法，用于高效引导基于LLM的代理的伦理行为，并通过BehaviorBench多层级基准进行评估。


<details>
  <summary>Details</summary>
Motivation: 在高风险领域部署LLM代理存在安全和伦理风险，需一种方法动态调整代理行为以避免严重后果。

Method: 将代理行为引导建模为模型编辑任务，引入BehaviorBench基准，支持多场景评估和编辑。

Result: 行为编辑能动态调整代理行为，实现局部和全局伦理对齐，甚至诱导有害行为。

Conclusion: 行为编辑为代理行为引导提供了新范式，展示了其潜力与风险。

Abstract: Agents based on Large Language Models (LLMs) have demonstrated strong
capabilities across a wide range of tasks. However, deploying LLM-based agents
in high-stakes domains comes with significant safety and ethical risks.
Unethical behavior by these agents can directly result in serious real-world
consequences, including physical harm and financial loss. To efficiently steer
the ethical behavior of agents, we frame agent behavior steering as a model
editing task, which we term Behavior Editing. Model editing is an emerging area
of research that enables precise and efficient modifications to LLMs while
preserving their overall capabilities. To systematically study and evaluate
this approach, we introduce BehaviorBench, a multi-tier benchmark grounded in
psychological moral theories. This benchmark supports both the evaluation and
editing of agent behaviors across a variety of scenarios, with each tier
introducing more complex and ambiguous scenarios. We first demonstrate that
Behavior Editing can dynamically steer agents toward the target behavior within
specific scenarios. Moreover, Behavior Editing enables not only
scenario-specific local adjustments but also more extensive shifts in an
agent's global moral alignment. We demonstrate that Behavior Editing can be
used to promote ethical and benevolent behavior or, conversely, to induce
harmful or malicious behavior. Through comprehensive evaluations on agents
based on frontier LLMs, BehaviorBench shows the effectiveness of Behavior
Editing across different models and scenarios. Our findings offer key insights
into a new paradigm for steering agent behavior, highlighting both the promise
and perils of Behavior Editing.

</details>


### [54] [DiffuCoder: Understanding and Improving Masked Diffusion Models for Code Generation](https://arxiv.org/abs/2506.20639)
*Shansan Gong,Ruixiang Zhang,Huangjie Zheng,Jiatao Gu,Navdeep Jaitly,Lingpeng Kong,Yizhe Zhang*

Main category: cs.CL

TL;DR: 论文研究了扩散大语言模型（dLLMs）在代码生成中的潜力，提出了一种新的训练方法（coupled-GRPO）以优化性能。


<details>
  <summary>Details</summary>
Motivation: dLLMs因其全局规划和迭代优化特性在代码生成中具有潜力，但当前训练和推理机制尚未充分探索。

Method: 训练了一个7B参数的dLLM（DiffuCoder），并分析了其解码行为；提出了一种新的RL训练方法coupled-GRPO。

Result: coupled-GRPO显著提升了DiffuCoder在代码生成基准上的性能（+4.4%），并减少了对AR因果解码的依赖。

Conclusion: 研究揭示了dLLMs的生成机制，并提供了一个有效的扩散原生RL训练框架。

Abstract: Diffusion large language models (dLLMs) are compelling alternatives to
autoregressive (AR) models because their denoising models operate over the
entire sequence. The global planning and iterative refinement features of dLLMs
are particularly useful for code generation. However, current training and
inference mechanisms for dLLMs in coding are still under-explored. To demystify
the decoding behavior of dLLMs and unlock their potential for coding, we
systematically investigate their denoising processes and reinforcement learning
(RL) methods. We train a 7B dLLM, \textbf{DiffuCoder}, on 130B tokens of code.
Using this model as a testbed, we analyze its decoding behavior, revealing how
it differs from that of AR models: (1) dLLMs can decide how causal their
generation should be without relying on semi-AR decoding, and (2) increasing
the sampling temperature diversifies not only token choices but also their
generation order. This diversity creates a rich search space for RL rollouts.
For RL training, to reduce the variance of token log-likelihood estimates and
maintain training efficiency, we propose \textbf{coupled-GRPO}, a novel
sampling scheme that constructs complementary mask noise for completions used
in training. In our experiments, coupled-GRPO significantly improves
DiffuCoder's performance on code generation benchmarks (+4.4\% on EvalPlus) and
reduces reliance on AR causal during decoding. Our work provides deeper insight
into the machinery of dLLM generation and offers an effective, diffusion-native
RL training framework. https://github.com/apple/ml-diffucoder.

</details>


### [55] [Memento: Note-Taking for Your Future Self](https://arxiv.org/abs/2506.20642)
*Chao Wan,Albert Gong,Mihir Mishra,Carl-Leander Henneking,Claas Beger,Kilian Q. Weinberger*

Main category: cs.CL

TL;DR: Memento是一种提示策略，通过分解复杂问题、动态构建事实数据库并整合信息，显著提升多跳问答任务的性能。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在纯推理任务中表现优异，但在需要紧密耦合检索与推理的多跳问答任务中表现不佳。

Method: Memento采用三阶段策略：分解问题、动态构建事实数据库、整合信息解决问题。

Result: 在多个基准测试中，Memento显著提升了性能，如PhantomWiki上性能翻倍，2WikiMultiHopQA上F1提升超过20个百分点。

Conclusion: Memento在复杂推理任务中表现出强大的潜力，尤其在需要检索与推理结合的场景中。

Abstract: Large language models (LLMs) excel at reasoning-only tasks, but struggle when
reasoning must be tightly coupled with retrieval, as in multi-hop question
answering. To overcome these limitations, we introduce a prompting strategy
that first decomposes a complex question into smaller steps, then dynamically
constructs a database of facts using LLMs, and finally pieces these facts
together to solve the question. We show how this three-stage strategy, which we
call Memento, can boost the performance of existing prompting strategies across
diverse settings. On the 9-step PhantomWiki benchmark, Memento doubles the
performance of chain-of-thought (CoT) when all information is provided in
context. On the open-domain version of 2WikiMultiHopQA, CoT-RAG with Memento
improves over vanilla CoT-RAG by more than 20 F1 percentage points and over the
multi-hop RAG baseline, IRCoT, by more than 13 F1 percentage points. On the
challenging MuSiQue dataset, Memento improves ReAct by more than 3 F1
percentage points, demonstrating its utility in agentic settings.

</details>


### [56] [Inside you are many wolves: Using cognitive models to interpret value trade-offs in LLMs](https://arxiv.org/abs/2506.20666)
*Sonia K. Murthy,Rosie Zhao,Jennifer Hu,Sham Kakade,Markus Wulfmeier,Peng Qian,Tomer Ullman*

Main category: cs.CL

TL;DR: 该论文探讨了大型语言模型（LLM）在人类价值观权衡中的表现，通过认知模型评估其社交与信息效用的权衡，发现推理模型更注重信息效用，且训练初期对效用值有显著影响。


<details>
  <summary>Details</summary>
Motivation: 研究动机是理解LLM在复杂社交情境中如何权衡冲突目标（如传达真相与维护信任），并填补现有工具在动态价值观建模上的不足。

Method: 采用认知科学中的礼貌言语认知模型，系统评估LLM在推理模型和开源模型中的价值观权衡，分析训练动态和效用值变化。

Result: 结果显示推理模型更偏向信息效用，开源模型在数学推理中表现更强；训练初期对效用值影响显著，基础模型和预训练数据的选择比反馈数据集或对齐方法更具持久影响。

Conclusion: 该方法能适应快速发展的LLM领域，为高维行为假设、推理模型训练及价值观权衡控制提供新见解。

Abstract: Navigating everyday social situations often requires juggling conflicting
goals, such as conveying a harsh truth, maintaining trust, all while still
being mindful of another person's feelings. These value trade-offs are an
integral part of human decision-making and language use, however, current tools
for interpreting such dynamic and multi-faceted notions of values in LLMs are
limited. In cognitive science, so-called "cognitive models" provide formal
accounts of these trade-offs in humans, by modeling the weighting of a
speaker's competing utility functions in choosing an action or utterance. In
this work, we use a leading cognitive model of polite speech to interpret the
extent to which LLMs represent human-like trade-offs. We apply this lens to
systematically evaluate value trade-offs in two encompassing model settings:
degrees of reasoning "effort" in frontier black-box models, and RL
post-training dynamics of open-source models. Our results highlight patterns of
higher informational utility than social utility in reasoning models, and in
open-source models shown to be stronger in mathematical reasoning. Our findings
from LLMs' training dynamics suggest large shifts in utility values early on in
training with persistent effects of the choice of base model and pretraining
data, compared to feedback dataset or alignment method. We show that our method
is responsive to diverse aspects of the rapidly evolving LLM landscape, with
insights for forming hypotheses about other high-level behaviors, shaping
training regimes for reasoning models, and better controlling trade-offs
between values during model training.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [57] [Computer Vision based Automated Quantification of Agricultural Sprayers Boom Displacement](https://arxiv.org/abs/2506.19939)
*Aryan Singh Dalal,Sidharth Rai,Rahul Singh,Treman Singh Kaloya,Rahul Harsha Cheppally,Ajay Sharda*

Main category: cs.CV

TL;DR: 开发了一种基于计算机视觉的系统，用于量化农业喷雾器喷杆的运动，以提高喷雾精度。


<details>
  <summary>Details</summary>
Motivation: 喷雾器喷杆的不稳定性是喷雾应用误差的主要因素之一，但目前缺乏定量数据来改进设计和控制系统。

Method: 使用YOLO V7、V8和V11神经网络模型实时跟踪喷杆上的目标，并结合倾角传感器验证模型输出。

Result: 模型检测目标的准确率超过90%，距离估计误差在0.026米以内。

Conclusion: 该系统可量化喷杆运动，为改进设计和提高喷雾精度提供数据支持。

Abstract: Application rate errors when using self-propelled agricultural sprayers for
agricultural production remain a concern. Among other factors, spray boom
instability is one of the major contributors to application errors. Spray
booms' width of 38m, combined with 30 kph driving speeds, varying terrain, and
machine dynamics when maneuvering complex field boundaries, make controls of
these booms very complex. However, there is no quantitative knowledge on the
extent of boom movement to systematically develop a solution that might include
boom designs and responsive boom control systems. Therefore, this study was
conducted to develop an automated computer vision system to quantify the boom
movement of various agricultural sprayers. A computer vision system was
developed to track a target on the edge of the sprayer boom in real time. YOLO
V7, V8, and V11 neural network models were trained to track the boom's
movements in field operations to quantify effective displacement in the
vertical and transverse directions. An inclinometer sensor was mounted on the
boom to capture boom angles and validate the neural network model output. The
results showed that the model could detect the target with more than 90 percent
accuracy, and distance estimates of the target on the boom were within 0.026 m
of the inclinometer sensor data. This system can quantify the boom movement on
the current sprayer and potentially on any other sprayer with minor
modifications. The data can be used to make design improvements to make sprayer
booms more stable and achieve greater application accuracy.

</details>


### [58] [EBC-ZIP: Improving Blockwise Crowd Counting with Zero-Inflated Poisson Regression](https://arxiv.org/abs/2506.19955)
*Yiming Ma,Victor Sanchez,Tanaya Guha*

Main category: cs.CV

TL;DR: 论文提出EBC-ZIP框架，通过零膨胀泊松回归改进人群计数中的密度图估计，解决现有方法对稀疏区域的忽视问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法忽视地面真实密度图的极端稀疏性，导致模型在稀疏区域表现不佳。

Method: 采用零膨胀泊松回归（ZIP）替代传统回归损失，结合增强块分类（EBC）框架。

Result: 在四个基准测试中，EBC-ZIP表现优于EBC并达到最优结果。

Conclusion: EBC-ZIP通过更合理的概率损失提升了性能，适用于不同计算复杂度的模型。

Abstract: Density map estimation has become the mainstream paradigm in crowd counting.
However, most existing methods overlook the extreme sparsity of ground-truth
density maps. In real-world crowd scenes, the vast majority of spatial regions
(often over 95%) contain no people, leading to heavily imbalanced count
distributions. Ignoring this imbalance can bias models toward overestimating
dense regions and underperforming in sparse areas. Furthermore, most loss
functions used in density estimation are majorly based on MSE and implicitly
assume Gaussian distributions, which are ill-suited for modeling discrete,
non-negative count data. In this paper, we propose EBC-ZIP, a crowd counting
framework that models the spatial distribution of counts using a Zero-Inflated
Poisson (ZIP) regression formulation. Our approach replaces the traditional
regression loss with the negative log-likelihood of the ZIP distribution,
enabling better handling of zero-heavy distributions while preserving count
accuracy. Built upon the recently proposed Enhanced Block Classification (EBC)
framework, EBC-ZIP inherits EBC's advantages in preserving the discreteness of
targets and ensuring training stability, while further improving performance
through a more principled probabilistic loss. We also evaluate EBC-ZIP with
backbones of varying computational complexity to assess its scalability.
Extensive experiments on four crowd counting benchmarks demonstrate that
EBC-ZIP consistently outperforms EBC and achieves state-of-the-art results.

</details>


### [59] [ToSA: Token Merging with Spatial Awareness](https://arxiv.org/abs/2506.20066)
*Hsiang-Wei Huang,Wenhao Chai,Kuang-Ming Chen,Cheng-Yen Yang,Jenq-Neng Hwang*

Main category: cs.CV

TL;DR: ToSA是一种结合语义和空间信息的新型token合并方法，通过深度图像生成伪空间token，优化ViT的加速效果。


<details>
  <summary>Details</summary>
Motivation: 现有token合并方法主要依赖特征相似性，忽略了空间信息在早期层中的重要性。

Method: 利用深度图像生成伪空间token，结合语义和空间信息指导token合并。

Result: 在多个视觉和具身问答基准上优于现有方法，同时显著减少ViT运行时间。

Conclusion: ToSA是一种高效的ViT加速解决方案，能更好地保留场景结构。

Abstract: Token merging has emerged as an effective strategy to accelerate Vision
Transformers (ViT) by reducing computational costs. However, existing methods
primarily rely on the visual token's feature similarity for token merging,
overlooking the potential of integrating spatial information, which can serve
as a reliable criterion for token merging in the early layers of ViT, where the
visual tokens only possess weak visual information. In this paper, we propose
ToSA, a novel token merging method that combines both semantic and spatial
awareness to guide the token merging process. ToSA leverages the depth image as
input to generate pseudo spatial tokens, which serve as auxiliary spatial
information for the visual token merging process. With the introduced spatial
awareness, ToSA achieves a more informed merging strategy that better preserves
critical scene structure. Experimental results demonstrate that ToSA
outperforms previous token merging methods across multiple benchmarks on visual
and embodied question answering while largely reducing the runtime of the ViT,
making it an efficient solution for ViT acceleration. The code will be
available at: https://github.com/hsiangwei0903/ToSA

</details>


### [60] [UniCode$^2$: Cascaded Large-scale Codebooks for Unified Multimodal Understanding and Generation](https://arxiv.org/abs/2506.20214)
*Yanzhe Chen,Huasong Zhong,Yan Li,Zhenheng Yang*

Main category: cs.CV

TL;DR: UniCode^2提出了一种级联码本框架，用于大规模、语义对齐且稳定的视觉标记化，解决了现有方法在小词汇量或盲目扩展时的问题。


<details>
  <summary>Details</summary>
Motivation: 现有视觉码本方法在小词汇量下缺乏细粒度语义，或盲目扩展导致标记利用率低和训练不稳定。

Method: 通过聚类数百万SigLIP序列嵌入构建500K条目的码本，采用级联设计（冻结码本锚定嵌入空间，可训练码本细化任务语义）确保稳定性。

Result: UniCode^2在多样化基准测试中表现优异，支持高质量视觉合成，且无需牺牲稳定性或语义对齐。

Conclusion: UniCode^2证明了在视觉标记空间中实现规模化而不牺牲稳定性、语义或模块化的可行性。

Abstract: Unified multimodal large language models (MLLMs) have shown promise in
jointly advancing multimodal understanding and generation, with visual
codebooks discretizing images into tokens for autoregressive modeling. Existing
codebook-based methods either rely on small vocabularies (~16K entries) that
lack fine-grained semantics or naively scale up, resulting in low token
utilization and unstable training. We propose UniCode$^2$, a cascaded codebook
framework enabling large-scale, semantically aligned, and stable visual
tokenization. By clustering millions of SigLIP sequence embeddings, we build a
500K-entry codebook that preserves vision-language alignment while expanding
capacity. Stability is ensured via a cascaded design: a frozen codebook anchors
the embedding space, and a trainable codebook refines task-specific semantics.
This decoupling promotes high utilization and robust learning. Moreover, the
alignment of our visual tokens with textual semantics enables seamless
integration with pretrained diffusion decoders, supporting high-quality visual
synthesis with minimal adaptation. UniCode^2 delivers strong performance across
diverse benchmarks, demonstrating the viability of scaling visual token spaces
without sacrificing stability, semantics, or modularity.

</details>


### [61] [BrokenVideos: A Benchmark Dataset for Fine-Grained Artifact Localization in AI-Generated Videos](https://arxiv.org/abs/2506.20103)
*Jiahao Lin,Weixuan Peng,Bojia Zi,Yifeng Gao,Xianbiao Qi,Xingjun Ma,Yu-Gang Jiang*

Main category: cs.CV

TL;DR: 论文介绍了BrokenVideos数据集，用于AI生成视频中视觉伪影的像素级定位，填补了现有研究的空白。


<details>
  <summary>Details</summary>
Motivation: AI生成视频存在视觉伪影问题，但缺乏针对伪影定位的全面基准数据集。

Method: 提出BrokenVideos数据集，包含3,254个AI生成视频，带有像素级标注的伪影区域。

Result: 实验表明，基于BrokenVideos训练模型能显著提升伪影定位能力。

Conclusion: BrokenVideos为生成视频模型的伪影定位研究提供了重要基准。

Abstract: Recent advances in deep generative models have led to significant progress in
video generation, yet the fidelity of AI-generated videos remains limited.
Synthesized content often exhibits visual artifacts such as temporally
inconsistent motion, physically implausible trajectories, unnatural object
deformations, and local blurring that undermine realism and user trust.
Accurate detection and spatial localization of these artifacts are crucial for
both automated quality control and for guiding the development of improved
generative models. However, the research community currently lacks a
comprehensive benchmark specifically designed for artifact localization in AI
generated videos. Existing datasets either restrict themselves to video or
frame level detection or lack the fine-grained spatial annotations necessary
for evaluating localization methods. To address this gap, we introduce
BrokenVideos, a benchmark dataset of 3,254 AI-generated videos with
meticulously annotated, pixel-level masks highlighting regions of visual
corruption. Each annotation is validated through detailed human inspection to
ensure high quality ground truth. Our experiments show that training state of
the art artifact detection models and multi modal large language models (MLLMs)
on BrokenVideos significantly improves their ability to localize corrupted
regions. Through extensive evaluation, we demonstrate that BrokenVideos
establishes a critical foundation for benchmarking and advancing research on
artifact localization in generative video models. The dataset is available at:
https://broken-video-detection-datetsets.github.io/Broken-Video-Detection-Datasets.github.io/.

</details>


### [62] [InvZW: Invariant Feature Learning via Noise-Adversarial Training for Robust Image Zero-Watermarking](https://arxiv.org/abs/2506.20370)
*Abdullah All Tanvir,Xin Zhong*

Main category: cs.CV

TL;DR: 提出了一种基于失真不变特征学习的深度零水印框架，通过优化特征空间生成参考签名，保持原始图像不变。


<details>
  <summary>Details</summary>
Motivation: 解决传统水印技术对图像修改敏感的问题，提出一种不改变原始图像的零水印方法。

Method: 框架包含两个模块：1) 通过噪声对抗学习训练特征提取器，生成失真不变且语义丰富的特征；2) 设计基于学习的多比特零水印方案，将特征投影到优化的参考代码上。

Result: 在多种图像数据集和失真条件下，方法在特征稳定性和水印恢复方面达到最优鲁棒性。

Conclusion: 该框架在泛化性和鲁棒性上优于现有自监督和深度水印技术。

Abstract: This paper introduces a novel deep learning framework for robust image
zero-watermarking based on distortion-invariant feature learning. As a
zero-watermarking scheme, our method leaves the original image unaltered and
learns a reference signature through optimization in the feature space. The
proposed framework consists of two key modules. In the first module, a feature
extractor is trained via noise-adversarial learning to generate representations
that are both invariant to distortions and semantically expressive. This is
achieved by combining adversarial supervision against a distortion
discriminator and a reconstruction constraint to retain image content. In the
second module, we design a learning-based multibit zero-watermarking scheme
where the trained invariant features are projected onto a set of trainable
reference codes optimized to match a target binary message. Extensive
experiments on diverse image datasets and a wide range of distortions show that
our method achieves state-of-the-art robustness in both feature stability and
watermark recovery. Comparative evaluations against existing self-supervised
and deep watermarking techniques further highlight the superiority of our
framework in generalization and robustness.

</details>


### [63] [From 2D to 3D Cognition: A Brief Survey of General World Models](https://arxiv.org/abs/2506.20134)
*Ningwei Xie,Zizi Tian,Lei Yang,Xiao-Ping Zhang,Meng Guo,Jie Li*

Main category: cs.CV

TL;DR: 该论文综述了从2D感知到3D认知的世界模型发展，重点分析了3D表示和世界知识整合的技术驱动，并探讨了3D世界建模的核心能力及其应用。


<details>
  <summary>Details</summary>
Motivation: 当前缺乏对3D认知世界模型的系统性分析，论文旨在填补这一空白，提供一个结构化的综述框架。

Method: 通过引入概念框架，分类新兴技术，并分析3D世界建模的三大核心能力（3D物理场景生成、3D空间推理和3D空间交互）。

Result: 论文总结了3D世界模型在具体应用中的部署，并指出了数据、建模和部署方面的挑战。

Conclusion: 未来研究方向包括提升3D世界模型的鲁棒性和泛化能力。

Abstract: World models have garnered increasing attention in the development of
artificial general intelligence (AGI), serving as computational frameworks for
learning representations of the external world and forecasting future states.
While early efforts focused on 2D visual perception and simulation, recent
3D-aware generative world models have demonstrated the ability to synthesize
geometrically consistent, interactive 3D environments, marking a shift toward
3D spatial cognition. Despite rapid progress, the field lacks systematic
analysis to categorize emerging techniques and clarify their roles in advancing
3D cognitive world models. This survey addresses this need by introducing a
conceptual framework, providing a structured and forward-looking review of
world models transitioning from 2D perception to 3D cognition. Within this
framework, we highlight two key technological drivers, particularly advances in
3D representations and the incorporation of world knowledge, as fundamental
pillars. Building on these, we dissect three core cognitive capabilities that
underpin 3D world modeling: 3D physical scene generation, 3D spatial reasoning,
and 3D spatial interaction. We further examine the deployment of these
capabilities in real-world applications, including embodied AI, autonomous
driving, digital twin, and gaming/VR. Finally, we identify challenges across
data, modeling, and deployment, and outline future directions for advancing
more robust and generalizable 3D world models.

</details>


### [64] [EAR: Erasing Concepts from Unified Autoregressive Models](https://arxiv.org/abs/2506.20151)
*Haipeng Fan,Shiyuan Zhang,Baohunesitu,Zihang Guo,Huaiwen Zhang*

Main category: cs.CV

TL;DR: 论文提出了一种名为EAR的微调方法，用于在自回归模型中有效且保留性能地移除不需要的概念。通过WGA和TLM策略优化微调过程，并提出了新的基准ECGVF进行评估。


<details>
  <summary>Details</summary>
Motivation: 自回归模型在视觉理解和图像生成任务中表现优异，但如何在移除不需要的概念的同时保持生成质量仍是一个挑战。

Method: 提出EAR方法，结合WGA策略对齐解码与移除目标，TLM策略保护无关内容。并设计ECGVF基准，通过LLMs生成概念对并过滤评估。

Result: 实验表明，EAR在移除概念和保留模型性能方面均有显著提升。

Conclusion: EAR是一种有效的概念移除方法，ECGVF基准为相关研究提供了更严谨的评估基础。

Abstract: Autoregressive (AR) models have achieved unified and strong performance
across both visual understanding and image generation tasks. However, removing
undesired concepts from AR models while maintaining overall generation quality
remains an open challenge. In this paper, we propose Erasure Autoregressive
Model (EAR), a fine-tuning method for effective and utility-preserving concept
erasure in AR models. Specifically, we introduce Windowed Gradient Accumulation
(WGA) strategy to align patch-level decoding with erasure objectives, and
Thresholded Loss Masking (TLM) strategy to protect content unrelated to the
target concept during fine-tuning. Furthermore, we propose a novel benchmark,
Erase Concept Generator and Visual Filter (ECGVF), aim at provide a more
rigorous and comprehensive foundation for evaluating concept erasure in AR
models. Specifically, we first employ structured templates across diverse large
language models (LLMs) to pre-generate a large-scale corpus of
target-replacement concept prompt pairs. Subsequently, we generate images from
these prompts and subject them to rigorous filtering via a visual classifier to
ensure concept fidelity and alignment. Extensive experimental results conducted
on the ECGVF benchmark with the AR model Janus-Pro demonstrate that EAR
achieves marked improvements in both erasure effectiveness and model utility
preservation. Code is available at: https://github.com/immc-lab/ear/

</details>


### [65] [Pay Less Attention to Deceptive Artifacts: Robust Detection of Compressed Deepfakes on Online Social Networks](https://arxiv.org/abs/2506.20548)
*Manyi Li,Renshuai Tao,Yufan Liu,Chuangchuang Tan,Haotong Qin,Bing Li,Yunchao Wei,Yao Zhao*

Main category: cs.CV

TL;DR: PLADA框架通过处理压缩图像的块效应和利用配对与非配对数据，显著提升了OSN中深度伪造图像的检测性能。


<details>
  <summary>Details</summary>
Motivation: 现有深度伪造检测方法忽视OSN压缩引入的块效应，且仅关注原始图像，无法应对实际场景。

Method: PLADA包含块效应消除模块（B2E）和开放数据聚合模块（ODA），分别处理块效应和利用多种数据。

Result: 在26个数据集上的实验显示，PLADA优于现有方法，尤其在压缩和有限配对数据条件下表现突出。

Conclusion: PLADA不仅解决了块效应问题，还为开放世界场景提供了鲁棒的深度伪造检测方案。

Abstract: With the rapid advancement of deep learning, particularly through generative
adversarial networks (GANs) and diffusion models (DMs), AI-generated images, or
``deepfakes", have become nearly indistinguishable from real ones. These images
are widely shared across Online Social Networks (OSNs), raising concerns about
their misuse. Existing deepfake detection methods overlook the ``block effects"
introduced by compression in OSNs, which obscure deepfake artifacts, and
primarily focus on raw images, rarely encountered in real-world scenarios. To
address these challenges, we propose PLADA (Pay Less Attention to Deceptive
Artifacts), a novel framework designed to tackle the lack of paired data and
the ineffective use of compressed images. PLADA consists of two core modules:
Block Effect Eraser (B2E), which uses a dual-stage attention mechanism to
handle block effects, and Open Data Aggregation (ODA), which processes both
paired and unpaired data to improve detection. Extensive experiments across 26
datasets demonstrate that PLADA achieves a remarkable balance in deepfake
detection, outperforming SoTA methods in detecting deepfakes on OSNs, even with
limited paired data and compression. More importantly, this work introduces the
``block effect" as a critical factor in deepfake detection, providing a robust
solution for open-world scenarios. Our code is available at
https://github.com/ManyiLee/PLADA.

</details>


### [66] [Loss-Aware Automatic Selection of Structured Pruning Criteria for Deep Neural Network Acceleration](https://arxiv.org/abs/2506.20152)
*Deepak Ghimire,Kilho Lee,Seong-heum Kim*

Main category: cs.CV

TL;DR: 该论文提出了一种高效的损失感知自动选择结构化剪枝标准（LAASP）方法，用于压缩和加速深度神经网络。通过剪枝与训练相结合的方式，自动选择剪枝标准和层，显著减少了计算量（FLOPs）并提升了模型精度。


<details>
  <summary>Details</summary>
Motivation: 解决传统剪枝方法需要分阶段训练、剪枝和微调的问题，提出一种更高效的剪枝-训练一体化方法，以适用于资源有限的边缘设备。

Method: 采用剪枝与训练同步进行的方法，自动从候选池中选择剪枝标准和层，并通过网络损失指导选择。每次剪枝后短暂重训练以缓解精度下降。

Result: 在CIFAR-10和ImageNet数据集上，ResNet56和ResNet110模型的FLOPs减少52%，精度提升；ResNet50模型的FLOPs减少42%，精度仅下降0.33%。

Conclusion: LAASP方法在减少计算量的同时保持了模型精度，适用于资源受限的设备部署。

Abstract: Structured pruning is a well-established technique for compressing neural
networks, making it suitable for deployment in resource-limited edge devices.
This paper presents an efficient Loss-Aware Automatic Selection of Structured
Pruning Criteria (LAASP) for slimming and accelerating deep neural networks.
The majority of pruning methodologies employ a sequential process consisting of
three stages: 1) training, 2) pruning, and 3) fine-tuning, whereas the proposed
pruning technique adopts a pruning-while-training approach that eliminates the
first stage and integrates the second and third stages into a single cycle. The
automatic selection of magnitude or similarity-based filter pruning criteria
from a specified pool of criteria and the specific pruning layer at each
pruning iteration is guided by the network's overall loss on a small subset of
the training data. To mitigate the abrupt accuracy drop due to pruning, the
network is retrained briefly after each reduction of a predefined number of
floating-point operations (FLOPs). The optimal pruning rates for each layer in
the network are automatically determined, eliminating the need for manual
allocation of fixed or variable pruning rates for each layer. Experiments on
the VGGNet and ResNet models on the CIFAR-10 and ImageNet benchmark datasets
demonstrate the effectiveness of the proposed method. In particular, the
ResNet56 and ResNet110 models on the CIFAR-10 dataset significantly improve the
top-1 accuracy compared to state-of-the-art methods while reducing the network
FLOPs by 52\%. Furthermore, the ResNet50 model on the ImageNet dataset reduces
FLOPs by more than 42\% with a negligible 0.33\% drop in top-5 accuracy. The
source code of this paper is publicly available online -
https://github.com/ghimiredhikura/laasp.

</details>


### [67] [Towards Efficient Exemplar Based Image Editing with Multimodal VLMs](https://arxiv.org/abs/2506.20155)
*Avadhoot Jadhav,Ashutosh Srivastava,Abhinav Java,Silky Singh,Tarun Ram Menta,Surgan Jandial,Balaji Krishnamurthy*

Main category: cs.CV

TL;DR: 本文提出了一种基于示例对的图像编辑方法，利用预训练的文本到图像扩散模型和多模态VLMs，无需优化即可高效完成编辑任务。


<details>
  <summary>Details</summary>
Motivation: 仅通过文本描述难以捕捉所有类型的图像编辑需求，示例对能更直观地表达编辑意图。

Method: 利用预训练的文本到图像扩散模型和多模态VLMs，构建端到端的优化免费管道。

Result: 实验表明，该方法在多种编辑类型上优于基线，且速度提升约4倍。

Conclusion: 该方法通过示例对实现了高效、准确的图像编辑，展示了扩散模型和多模态VLMs的潜力。

Abstract: Text-to-Image Diffusion models have enabled a wide array of image editing
applications. However, capturing all types of edits through text alone can be
challenging and cumbersome. The ambiguous nature of certain image edits is
better expressed through an exemplar pair, i.e., a pair of images depicting an
image before and after an edit respectively. In this work, we tackle
exemplar-based image editing -- the task of transferring an edit from an
exemplar pair to a content image(s), by leveraging pretrained text-to-image
diffusion models and multimodal VLMs. Even though our end-to-end pipeline is
optimization-free, our experiments demonstrate that it still outperforms
baselines on multiple types of edits while being ~4x faster.

</details>


### [68] [Seeing is Believing? Mitigating OCR Hallucinations in Multimodal Large Language Models](https://arxiv.org/abs/2506.20168)
*Zhentao He,Can Zhang,Ziheng Wu,Zhenghao Chen,Yufei Zhan,Yifan Li,Zhao Zhang,Xian Wang,Minghui Qiu*

Main category: cs.CV

TL;DR: 论文提出KIE-HVQA基准和GRPO框架，解决多模态大语言模型在视觉退化场景下的幻觉问题，实验显示性能显著提升。


<details>
  <summary>Details</summary>
Motivation: 现有模型在视觉退化场景中表现不佳，容易产生幻觉内容，需改进视觉-文本推理能力。

Method: 提出KIE-HVQA基准评估OCR幻觉，并设计GRPO框架，结合视觉不确定性和拒绝回答机制。

Result: 7B参数模型在KIE-HVQA上比GPT-4o提升22%的幻觉避免准确率，且标准任务无显著下降。

Conclusion: GRPO框架有效减少幻觉，提升模型在退化视觉条件下的鲁棒性和可靠性。

Abstract: Recent advancements in multimodal large language models have enhanced
document understanding by integrating textual and visual information. However,
existing models exhibit incompleteness within their paradigm in real-world
scenarios, particularly under visual degradation. In such conditions, the
current response paradigm often fails to adequately perceive visual degradation
and ambiguity, leading to overreliance on linguistic priors or misaligned
visual-textual reasoning. This difficulty in recognizing uncertainty frequently
results in the generation of hallucinatory content, especially when a precise
answer is not feasible. To better demonstrate and analyze this phenomenon and
problem, we propose KIE-HVQA, the first benchmark dedicated to evaluating OCR
hallucination in degraded document understanding. This dataset includes test
samples spanning identity cards and invoices, with simulated real-world
degradations for OCR reliability. This setup allows for evaluating models'
capacity, under degraded input, to distinguish reliable visual information and
answer accordingly, thereby highlighting the challenge of avoiding
hallucination on uncertain data. To achieve vision-faithful reasoning and
thereby avoid the aforementioned issues, we further introduce a GRPO-based
framework featuring a novel reward mechanism. By incorporating a self-awareness
of visual uncertainty and an analysis method that initiates refusal to answer
to increase task difficulty within our supervised fine-tuning and reinforcement
learning framework, we successfully mitigated hallucinations in ambiguous
regions. Experiments on Qwen2.5-VL demonstrate that our 7B-parameter model
achieves a 22\% absolute improvement in hallucination-free accuracy over GPT-4o
on KIE-HVQA and there is no significant performance drop in standard tasks,
highlighting both effectiveness and robustness.

</details>


### [69] [Towards Scalable and Generalizable Earth Observation Data Mining via Foundation Model Composition](https://arxiv.org/abs/2506.20174)
*Man Duc Chuc*

Main category: cs.CV

TL;DR: 研究发现，通过组合预训练的小型模型，可以匹配或超越大型模型的性能，同时减少训练时间和计算资源。


<details>
  <summary>Details</summary>
Motivation: 探索预训练模型在遥感任务中的组合应用，以提高性能并减少资源消耗。

Method: 使用GEO-Bench基准测试，评估多个预训练模型（如Prithvi、Hiera、DOFA）在11个数据集上的表现，采用特征级集成方法。

Result: 特征级集成的小型预训练模型性能可与大型模型媲美，且训练时间和资源需求更低。

Conclusion: 知识蒸馏可将集成模型的优势转移到更紧凑的模型中，为实际应用提供可行路径。

Abstract: Foundation models are rapidly transforming Earth Observation data mining by
enabling generalizable and scalable solutions for key tasks such as scene
classification and semantic segmentation. While most efforts in the geospatial
domain have focused on developing large models trained from scratch using
massive Earth Observation datasets, an alternative strategy that remains
underexplored is the reuse and combination of existing pretrained models. In
this study, we investigate whether foundation models pretrained on remote
sensing and general vision datasets can be effectively combined to improve
performance across a diverse set of key Earth Observation tasks. Using the
GEO-Bench benchmark, we evaluate several prominent models, including Prithvi,
Hiera, and DOFA, on eleven datasets covering a range of spatial resolutions,
sensor modalities, and task types. The results show that feature-level
ensembling of smaller pretrained models can match or exceed the performance of
much larger models, while requiring less training time and computational
resources. Moreover, the study highlights the potential of applying knowledge
distillation to transfer the strengths of ensembles into more compact models,
offering a practical path for deploying foundation models in real-world Earth
Observation applications.

</details>


### [70] [Progressive Alignment Degradation Learning for Pansharpening](https://arxiv.org/abs/2506.20179)
*Enzhe Zhao,Zhichang Guo,Yao Li,Fanghui Song,Boying Wu*

Main category: cs.CV

TL;DR: 论文提出了一种新的深度学习方法（PADM和HFreqdiff），通过自适应学习退化过程和高频细节嵌入，显著提升了全色和多光谱图像融合的质量。


<details>
  <summary>Details</summary>
Motivation: 现有Wald协议生成的合成数据无法准确模拟真实退化模式，限制了深度全色锐化模型的泛化能力。

Method: 提出PADM模块（包含PAlignNet和PDegradeNet）自适应学习退化过程，并引入HFreqdiff框架嵌入高频细节。

Result: 实验表明，该方法在空间清晰度和图像质量上优于现有技术。

Conclusion: 通过改进退化模型和高频细节处理，显著提升了全色锐化效果。

Abstract: Deep learning-based pansharpening has been shown to effectively generate
high-resolution multispectral (HRMS) images. To create supervised ground-truth
HRMS images, synthetic data generated using the Wald protocol is commonly
employed. This protocol assumes that networks trained on artificial
low-resolution data will perform equally well on high-resolution data. However,
well-trained models typically exhibit a trade-off in performance between
reduced-resolution and full-resolution datasets. In this paper, we delve into
the Wald protocol and find that its inaccurate approximation of real-world
degradation patterns limits the generalization of deep pansharpening models. To
address this issue, we propose the Progressive Alignment Degradation Module
(PADM), which uses mutual iteration between two sub-networks, PAlignNet and
PDegradeNet, to adaptively learn accurate degradation processes without relying
on predefined operators. Building on this, we introduce HFreqdiff, which embeds
high-frequency details into a diffusion framework and incorporates CFB and BACM
modules for frequency-selective detail extraction and precise reverse process
learning. These innovations enable effective integration of high-resolution
panchromatic and multispectral images, significantly enhancing spatial
sharpness and quality. Experiments and ablation studies demonstrate the
proposed method's superior performance compared to state-of-the-art techniques.

</details>


### [71] [Dynamic Bandwidth Allocation for Hybrid Event-RGB Transmission](https://arxiv.org/abs/2506.20222)
*Pujing Yang,Guangyi Zhang,Yunlong Cai,Lei Yu,Guanding Yu*

Main category: cs.CV

TL;DR: 提出了一种联合事件和图像（E-I）传输框架，通过贝叶斯建模和信息瓶颈方法消除冗余，优化带宽利用，同时实现实时去模糊。


<details>
  <summary>Details</summary>
Motivation: 解决混合系统中事件和RGB图像传输的带宽挑战，消除冗余信息。

Method: 使用贝叶斯建模和信息瓶颈方法分离共享和领域特定信息，动态分配传输带宽。

Result: 仿真结果表明，该方案在重建质量和去模糊性能上优于传统系统。

Conclusion: 提出的框架有效优化了带宽利用，同时提升了重建和去模糊效果。

Abstract: Event cameras asynchronously capture pixel-level intensity changes with
extremely low latency. They are increasingly used in conjunction with RGB
cameras for a wide range of vision-related applications. However, a major
challenge in these hybrid systems lies in the transmission of the large volume
of triggered events and RGB images. To address this, we propose a transmission
scheme that retains efficient reconstruction performance of both sources while
accomplishing real-time deblurring in parallel. Conventional RGB cameras and
event cameras typically capture the same scene in different ways, often
resulting in significant redundant information across their outputs. To address
this, we develop a joint event and image (E-I) transmission framework to
eliminate redundancy and thereby optimize channel bandwidth utilization. Our
approach employs Bayesian modeling and the information bottleneck method to
disentangle the shared and domain-specific information within the E-I inputs.
This disentangled information bottleneck framework ensures both the compactness
and informativeness of extracted shared and domain-specific information.
Moreover, it adaptively allocates transmission bandwidth based on scene
dynamics, i.e., more symbols are allocated to events for dynamic details or to
images for static information. Simulation results demonstrate that the proposed
scheme not only achieves superior reconstruction quality compared to
conventional systems but also delivers enhanced deblurring performance.

</details>


### [72] [Recognizing Surgical Phases Anywhere: Few-Shot Test-time Adaptation and Task-graph Guided Refinement](https://arxiv.org/abs/2506.20254)
*Kun Yuan,Tingxuan Chen,Shi Li,Joel L. Lavanchy,Christian Heiliger,Ege Özsoy,Yiming Huang,Long Bai,Nassir Navab,Vinkle Srivastav,Hongliang Ren,Nicolas Padoy*

Main category: cs.CV

TL;DR: SPA是一个轻量级框架，通过少量标注和自然语言定义，实现跨机构和跨手术的通用工作流理解。


<details>
  <summary>Details</summary>
Motivation: 手术工作流的复杂性和多样性导致通用模型难以开发，现有基础模型的零样本性能受限于领域偏移。

Method: SPA利用少样本空间适应、扩散模型编码任务图先验和动态测试时适应，提升模型适应性。

Result: SPA在少样本手术阶段识别中表现优异，甚至优于全样本模型。

Conclusion: SPA为医院提供了一种快速定制手术阶段识别模型的轻量级解决方案。

Abstract: The complexity and diversity of surgical workflows, driven by heterogeneous
operating room settings, institutional protocols, and anatomical variability,
present a significant challenge in developing generalizable models for
cross-institutional and cross-procedural surgical understanding. While recent
surgical foundation models pretrained on large-scale vision-language data offer
promising transferability, their zero-shot performance remains constrained by
domain shifts, limiting their utility in unseen surgical environments. To
address this, we introduce Surgical Phase Anywhere (SPA), a lightweight
framework for versatile surgical workflow understanding that adapts foundation
models to institutional settings with minimal annotation. SPA leverages
few-shot spatial adaptation to align multi-modal embeddings with
institution-specific surgical scenes and phases. It also ensures temporal
consistency through diffusion modeling, which encodes task-graph priors derived
from institutional procedure protocols. Finally, SPA employs dynamic test-time
adaptation, exploiting the mutual agreement between multi-modal phase
prediction streams to adapt the model to a given test video in a
self-supervised manner, enhancing the reliability under test-time distribution
shifts. SPA is a lightweight adaptation framework, allowing hospitals to
rapidly customize phase recognition models by defining phases in natural
language text, annotating a few images with the phase labels, and providing a
task graph defining phase transitions. The experimental results show that the
SPA framework achieves state-of-the-art performance in few-shot surgical phase
recognition across multiple institutions and procedures, even outperforming
full-shot models with 32-shot labeled data. Code is available at
https://github.com/CAMMA-public/SPA

</details>


### [73] [A Transformer Based Handwriting Recognition System Jointly Using Online and Offline Features](https://arxiv.org/abs/2506.20255)
*Ayush Lodh,Ritabrata Chakraborty,Shivakumara Palaiahnakote,Umapada Pal*

Main category: cs.CV

TL;DR: 提出了一种融合离线图像和在线笔画数据的端到端网络，通过共享潜在空间增强手写识别性能，实验显示其优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 手写识别通常仅利用单一模态（如字形或笔画轨迹），而忽略了二者的互补性，因此提出融合两种模态以提升性能。

Method: 设计了一个端到端网络，通过早期融合离线图像和在线笔画数据，使用共享潜在空间和可学习的潜在查询来增强上下文表示。

Result: 在IAMOn-DB和VNOn-DB数据集上实现了最先进的准确率，比之前最佳方法提升1%。

Conclusion: 融合多模态数据在手写识别中具有显著优势，能够增强表示学习并提升性能。

Abstract: We posit that handwriting recognition benefits from complementary cues
carried by the rasterized complex glyph and the pen's trajectory, yet most
systems exploit only one modality. We introduce an end-to-end network that
performs early fusion of offline images and online stroke data within a shared
latent space. A patch encoder converts the grayscale crop into fixed-length
visual tokens, while a lightweight transformer embeds the $(x, y, \text{pen})$
sequence. Learnable latent queries attend jointly to both token streams,
yielding context-enhanced stroke embeddings that are pooled and decoded under a
cross-entropy loss objective. Because integration occurs before any high-level
classification, temporal cues reinforce each other during representation
learning, producing stronger writer independence. Comprehensive experiments on
IAMOn-DB and VNOn-DB demonstrate that our approach achieves state-of-the-art
accuracy, exceeding previous bests by up to 1\%. Our study also shows
adaptation of this pipeline with gesturification on the ISI-Air dataset. Our
code can be found here.

</details>


### [74] [Hierarchical Mask-Enhanced Dual Reconstruction Network for Few-Shot Fine-Grained Image Classification](https://arxiv.org/abs/2506.20263)
*Ning Luo,Meiyin Hu,Huan Wan,Yanyan Yang,Zhuohang Jiang,Xin Wei*

Main category: cs.CV

TL;DR: HMDRN提出了一种结合双层次特征重建和掩码增强的少样本细粒度图像分类方法，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法在少样本细粒度图像分类中存在空间信息丢失、局部特征错位、缺乏层次特征利用和判别区域聚焦机制的问题。

Method: HMDRN通过双层次特征重建与融合模块，结合可学习的融合权重，平衡高层语义和中级结构细节；同时设计了空间二进制掩码增强的自重建模块，增强判别区域聚焦。

Result: 在三个细粒度数据集上，HMDRN在Conv-4和ResNet-12骨干网络上均优于现有方法。

Conclusion: HMDRN通过双层次重建增强类间区分度，掩码增强减少类内差异，可视化结果验证了其优越性。

Abstract: Few-shot fine-grained image classification (FS-FGIC) presents a significant
challenge, requiring models to distinguish visually similar subclasses with
limited labeled examples. Existing methods have critical limitations:
metric-based methods lose spatial information and misalign local features,
while reconstruction-based methods fail to utilize hierarchical feature
information and lack mechanisms to focus on discriminative regions. We propose
the Hierarchical Mask-enhanced Dual Reconstruction Network (HMDRN), which
integrates dual-layer feature reconstruction with mask-enhanced feature
processing to improve fine-grained classification. HMDRN incorporates a
dual-layer feature reconstruction and fusion module that leverages
complementary visual information from different network hierarchies. Through
learnable fusion weights, the model balances high-level semantic
representations from the last layer with mid-level structural details from the
penultimate layer. Additionally, we design a spatial binary mask-enhanced
transformer self-reconstruction module that processes query features through
adaptive thresholding while maintaining complete support features, enhancing
focus on discriminative regions while filtering background noise. Extensive
experiments on three challenging fine-grained datasets demonstrate that HMDRN
consistently outperforms state-of-the-art methods across Conv-4 and ResNet-12
backbone architectures. Comprehensive ablation studies validate the
effectiveness of each proposed component, revealing that dual-layer
reconstruction enhances inter-class discrimination while mask-enhanced
transformation reduces intra-class variations. Visualization results provide
evidence of HMDRN's superior feature reconstruction capabilities.

</details>


### [75] [Forensic Study of Paintings Through the Comparison of Fabrics](https://arxiv.org/abs/2506.20272)
*Juan José Murillo-Fuentes,Pablo M. Olmos,Laura Alba-Carcelén*

Main category: cs.CV

TL;DR: 提出了一种基于深度学习的纺织品相似性评估方法，用于艺术品的鉴定和保护。


<details>
  <summary>Details</summary>
Motivation: 传统基于线密度图匹配的方法无法适用于非连续位置的画布，需要新的技术手段。

Method: 设计了Siamese深度学习模型，通过图像对比较特征表示，并提出相似性估计方法。

Result: 在Museo Nacional del Prado的画布上验证了方法的可行性和准确性。

Conclusion: 该方法为艺术品分析提供了新途径，尤其适用于线密度相似的平纹画布。

Abstract: The study of canvas fabrics in works of art is a crucial tool for
authentication, attribution and conservation. Traditional methods are based on
thread density map matching, which cannot be applied when canvases do not come
from contiguous positions on a roll. This paper presents a novel approach based
on deep learning to assess the similarity of textiles. We introduce an
automatic tool that evaluates the similarity between canvases without relying
on thread density maps. A Siamese deep learning model is designed and trained
to compare pairs of images by exploiting the feature representations learned
from the scans. In addition, a similarity estimation method is proposed,
aggregating predictions from multiple pairs of cloth samples to provide a
robust similarity score. Our approach is applied to canvases from the Museo
Nacional del Prado, corroborating the hypothesis that plain weave canvases,
widely used in painting, can be effectively compared even when their thread
densities are similar. The results demonstrate the feasibility and accuracy of
the proposed method, opening new avenues for the analysis of masterpieces.

</details>


### [76] [From Ideal to Real: Unified and Data-Efficient Dense Prediction for Real-World Scenarios](https://arxiv.org/abs/2506.20279)
*Changliang Xia,Chengyou Jia,Zhuohang Dang,Minnan Luo*

Main category: cs.CV

TL;DR: 论文提出了DenseDiT方法，通过生成模型的视觉先验和统一策略解决密集预测任务在现实场景中的泛化问题，并在新基准DenseWorld上验证了其优越性能。


<details>
  <summary>Details</summary>
Motivation: 密集预测任务在计算机视觉中很重要，但现有方法在现实场景中泛化能力有限且数据稀缺。

Method: 提出DenseDiT，结合参数重用机制和轻量级分支，利用生成模型的视觉先验，以统一策略处理多种任务。

Result: 在DenseWorld基准上，DenseDiT性能显著优于现有基线方法，且仅需0.01%的训练数据。

Conclusion: DenseDiT在现实场景中具有实际应用价值，代码和数据已开源。

Abstract: Dense prediction tasks hold significant importance of computer vision, aiming
to learn pixel-wise annotated label for an input image. Despite advances in
this field, existing methods primarily focus on idealized conditions, with
limited generalization to real-world scenarios and facing the challenging
scarcity of real-world data. To systematically study this problem, we first
introduce DenseWorld, a benchmark spanning a broad set of 25 dense prediction
tasks that correspond to urgent real-world applications, featuring unified
evaluation across tasks. Then, we propose DenseDiT, which maximally exploits
generative models' visual priors to perform diverse real-world dense prediction
tasks through a unified strategy. DenseDiT combines a parameter-reuse mechanism
and two lightweight branches that adaptively integrate multi-scale context,
working with less than 0.1% additional parameters. Evaluations on DenseWorld
reveal significant performance drops in existing general and specialized
baselines, highlighting their limited real-world generalization. In contrast,
DenseDiT achieves superior results using less than 0.01% training data of
baselines, underscoring its practical value for real-world deployment. Our
data, and checkpoints and codes are available at
https://xcltql666.github.io/DenseDiTProj

</details>


### [77] [Breaking Spatial Boundaries: Spectral-Domain Registration Guided Hyperspectral and Multispectral Blind Fusion](https://arxiv.org/abs/2506.20293)
*Kunjing Yang,Libin Zheng,Minru Bai,Ting Lu,Leyuan Fang*

Main category: cs.CV

TL;DR: 提出了一种从光谱域解决未注册高光谱图像（HSI）和多光谱图像（MSI）融合问题的方法，通过轻量级光谱先验学习网络（SPL）和盲稀疏融合（BSF）方法，提高了注册和融合效果。


<details>
  <summary>Details</summary>
Motivation: 现有方法通过空间变换对齐HSI和MSI，但由于分辨率差异大，效果不佳且耗时。因此，从光谱域解决问题更具潜力。

Method: 1. 使用SPL网络提取HSI光谱特征并增强MSI光谱分辨率；2. 通过子空间表示和循环训练策略提高注册HSI的光谱精度；3. 提出BSF方法，利用群稀疏正则化等效促进图像低秩性；4. 采用PAO算法求解BSF模型。

Result: 在模拟和真实数据集上的实验验证了该方法在注册和融合中的有效性，并提升了分类性能。

Conclusion: 从光谱域解决注册问题更高效，结合BSF方法显著提升了融合效果和计算效率。

Abstract: The blind fusion of unregistered hyperspectral images (HSIs) and
multispectral images (MSIs) has attracted growing attention recently. To
address the registration challenge, most existing methods employ spatial
transformations on the HSI to achieve alignment with the MSI. However, due to
the substantial differences in spatial resolution of the images, the
performance of these methods is often unsatisfactory. Moreover, the
registration process tends to be time-consuming when dealing with large-sized
images in remote sensing. To address these issues, we propose tackling the
registration problem from the spectral domain. Initially, a lightweight
Spectral Prior Learning (SPL) network is developed to extract spectral features
from the HSI and enhance the spectral resolution of the MSI. Following this,
the obtained image undergoes spatial downsampling to produce the registered
HSI. In this process, subspace representation and cyclic training strategy are
employed to improve spectral accuracy of the registered HSI obtained. Next, we
propose a blind sparse fusion (BSF) method, which utilizes group sparsity
regularization to equivalently promote the low-rankness of the image. This
approach not only circumvents the need for rank estimation, but also reduces
computational complexity. Then, we employ the Proximal Alternating Optimization
(PAO) algorithm to solve the BSF model, and present its convergence analysis.
Finally, extensive numerical experiments on simulated and real datasets are
conducted to verify the effectiveness of our method in registration and fusion.
We also demonstrate its efficacy in enhancing classification performance.

</details>


### [78] [Ctrl-Z Sampling: Diffusion Sampling with Controlled Random Zigzag Explorations](https://arxiv.org/abs/2506.20294)
*Shunqi Mao,Wei Guo,Chaoyi Zhang,Weidong Cai*

Main category: cs.CV

TL;DR: 提出了一种名为Ctrl-Z Sampling的新采样策略，通过动态交替前向细化和后向探索，提升扩散模型的生成质量。


<details>
  <summary>Details</summary>
Motivation: 扩散模型在条件生成中常陷入局部最优，导致全局不一致或条件不对齐。现有方法通过增强指导信号或调整初始噪声分布来解决，但效果有限。

Method: Ctrl-Z Sampling通过奖励模型识别局部最优，注入噪声并回退到更早状态以逃离优化平台，评估候选轨迹并选择改进路径。

Result: 实验表明，Ctrl-Z Sampling显著提升生成质量，仅增加约7.6倍的函数评估。

Conclusion: Ctrl-Z Sampling是一种模型无关的方法，可兼容现有扩散框架，有效提升生成对齐性和视觉质量。

Abstract: Diffusion models have shown strong performance in conditional generation by
progressively denoising Gaussian noise toward a target data distribution. This
denoising process can be interpreted as a form of hill climbing in a learned
latent space, where the model iteratively refines the sample toward regions of
higher probability. However, diffusion models often converge to local optima
that are locally visually coherent yet globally inconsistent or conditionally
misaligned, due to latent space complexity and suboptimal initialization. Prior
efforts attempted to address this by strengthening guidance signals or
manipulating the initial noise distribution. We introduce Controlled Random
Zigzag Sampling (Ctrl-Z Sampling), a novel sampling strategy designed to detect
and escape such local maxima during conditional generation. The method first
identifies potential local maxima using a reward model. Upon detection, it
injects noise and reverts to a previous, noisier state to escape the current
optimization plateau. The reward model then evaluates candidate trajectories,
accepting only those that offer improvement, while progressively deeper retreat
enables stronger escapes when nearby alternatives fail. This controlled random
zigzag process allows dynamic alternation between forward refinement and
backward exploration, enhancing both alignment and visual quality in the
generated outputs. The proposed Ctrl-Z Sampling is model-agnostic and
compatible with existing diffusion frameworks. Experimental results show that
Ctrl-Z Sampling substantially improves generation quality with only around 7.6X
increase in function evaluations.

</details>


### [79] [TDiR: Transformer based Diffusion for Image Restoration Tasks](https://arxiv.org/abs/2506.20302)
*Abbas Anwar,Mohammad Shullar,Ali Arshad Nasir,Mudassir Masood,Saeed Anwar*

Main category: cs.CV

TL;DR: 提出了一种基于Transformer的扩散模型，用于图像恢复任务，在多个质量指标上优于现有深度学习方法。


<details>
  <summary>Details</summary>
Motivation: 解决图像在恶劣环境下捕获时的退化问题（如噪声、色偏、模糊等），以提高图像质量及其在下游任务中的适用性。

Method: 结合Transformer和扩散模型，用于图像恢复任务，包括水下图像增强、去噪和去雨。

Result: 在公开数据集上评估，模型性能优于现有方法。

Conclusion: 扩散模型与Transformer结合能有效提升退化图像质量，扩展其在高保真视觉数据任务中的应用。

Abstract: Images captured in challenging environments often experience various forms of
degradation, including noise, color cast, blur, and light scattering. These
effects significantly reduce image quality, hindering their applicability in
downstream tasks such as object detection, mapping, and classification. Our
transformer-based diffusion model was developed to address image restoration
tasks, aiming to improve the quality of degraded images. This model was
evaluated against existing deep learning methodologies across multiple quality
metrics for underwater image enhancement, denoising, and deraining on publicly
available datasets. Our findings demonstrate that the diffusion model, combined
with transformers, surpasses current methods in performance. The results of our
model highlight the efficacy of diffusion models and transformers in improving
the quality of degraded images, consequently expanding their utility in
downstream tasks that require high-fidelity visual data.

</details>


### [80] [Radiomic fingerprints for knee MR images assessment](https://arxiv.org/abs/2506.20306)
*Yaxi Chen,Simin Ni,Shaheer U. Saeed,Aleksandra Ivanova,Rikin Hargunani,Jie Huang,Chaozong Liu,Yipeng Hu*

Main category: cs.CV

TL;DR: 提出了一种动态构建放射组学特征的框架（指纹），通过深度学习模型为每位患者个性化选择特征，提高了诊断准确性并保持了可解释性。


<details>
  <summary>Details</summary>
Motivation: 传统放射组学方法使用固定特征集，难以捕捉个体病理差异，导致性能受限。本文旨在通过个性化特征选择提升性能并保持可解释性。

Method: 提出放射组学指纹框架，动态为每位患者选择特征，结合低维逻辑回归进行分类。

Result: 在多项膝关节诊断任务中表现优于或媲美端到端深度学习模型，同时提供临床可解释性。

Conclusion: 个性化放射组学特征选择显著提升诊断性能，同时保持可解释性，为临床提供新见解。

Abstract: Accurate interpretation of knee MRI scans relies on expert clinical judgment,
often with high variability and limited scalability. Existing radiomic
approaches use a fixed set of radiomic features (the signature), selected at
the population level and applied uniformly to all patients. While
interpretable, these signatures are often too constrained to represent
individual pathological variations. As a result, conventional radiomic-based
approaches are found to be limited in performance, compared with recent
end-to-end deep learning (DL) alternatives without using interpretable radiomic
features. We argue that the individual-agnostic nature in current radiomic
selection is not central to its intepretability, but is responsible for the
poor generalization in our application. Here, we propose a novel radiomic
fingerprint framework, in which a radiomic feature set (the fingerprint) is
dynamically constructed for each patient, selected by a DL model. Unlike the
existing radiomic signatures, our fingerprints are derived on a per-patient
basis by predicting the feature relevance in a large radiomic feature pool, and
selecting only those that are predictive of clinical conditions for individual
patients. The radiomic-selecting model is trained simultaneously with a
low-dimensional (considered relatively explainable) logistic regression for
downstream classification. We validate our methods across multiple diagnostic
tasks including general knee abnormalities, anterior cruciate ligament (ACL)
tears, and meniscus tears, demonstrating comparable or superior diagnostic
accuracy relative to state-of-the-art end-to-end DL models. More importantly,
we show that the interpretability inherent in our approach facilitates
meaningful clinical insights and potential biomarker discovery, with detailed
discussion, quantitative and qualitative analysis of real-world clinical cases
to evidence these advantages.

</details>


### [81] [On the Burstiness of Faces in Set](https://arxiv.org/abs/2506.20312)
*Jiong Wang*

Main category: cs.CV

TL;DR: 论文研究了集合人脸识别中的突发性现象，提出了三种检测方法，并通过实验证明抑制突发性显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 突发性现象在集合人脸识别中广泛存在，导致训练和评估性能下降，需有效检测和抑制。

Method: 提出基于Quickshift++、特征自相似性和广义最大池化的三种突发性检测策略，并应用于训练和评估阶段。

Result: 实验表明突发性普遍存在，抑制突发性显著提升了识别性能。

Conclusion: 通过检测和抑制突发性，可以有效提升集合人脸识别的泛化能力和评估准确性。

Abstract: Burstiness, a phenomenon observed in text and image retrieval, refers to that
particular elements appear more times in a set than a statistically independent
model assumes. We argue that in the context of set-based face recognition
(SFR), burstiness exists widely and degrades the performance in two aspects:
Firstly, the bursty faces, where faces with particular attributes %exist
frequently in a face set, dominate the training instances and dominate the
training face sets and lead to poor generalization ability to unconstrained
scenarios. Secondly, the bursty faces %dominating the evaluation sets interfere
with the similarity comparison in set verification and identification when
evaluation. To detect the bursty faces in a set, we propose three strategies
based on Quickshift++, feature self-similarity, and generalized max-pooling
(GMP). We apply the burst detection results on training and evaluation stages
to enhance the sampling ratios or contributions of the infrequent faces. When
evaluation, we additionally propose the quality-aware GMP that enables
awareness of the face quality and robustness to the low-quality faces for the
original GMP. We give illustrations and extensive experiments on the SFR
benchmarks to demonstrate that burstiness is widespread and suppressing
burstiness considerably improves the recognition performance.

</details>


### [82] [From Codicology to Code: A Comparative Study of Transformer and YOLO-based Detectors for Layout Analysis in Historical Documents](https://arxiv.org/abs/2506.20326)
*Sergio Torres Aguilar*

Main category: cs.CV

TL;DR: 本文评估了五种目标检测架构在三个历史文档数据集上的性能，发现Transformer和CNN-OBB模型各有优劣，OBB对非笛卡尔布局至关重要。


<details>
  <summary>Details</summary>
Motivation: 研究历史文档布局分析的鲁棒性，因复杂页面组织对自动化处理至关重要。

Method: 在三个数据集上比较五种目标检测架构，包括两种Transformer和三种YOLO变体。

Result: Co-DETR在结构化数据集表现最佳，而YOLOv11x-OBB在复杂数据集上显著优于其他模型。

Conclusion: Transformer适合结构化布局，CNN-OBB模型在视觉多样性和复杂性上表现更优，OBB是建模历史文档的关键。

Abstract: Robust Document Layout Analysis (DLA) is critical for the automated
processing and understanding of historical documents with complex page
organizations. This paper benchmarks five state-of-the-art object detection
architectures on three annotated datasets representing a spectrum of
codicological complexity: The e-NDP, a corpus of Parisian medieval registers
(1326-1504); CATMuS, a diverse multiclass dataset derived from various medieval
and modern sources (ca.12th-17th centuries) and HORAE, a corpus of decorated
books of hours (ca.13th-16th centuries). We evaluate two Transformer-based
models (Co-DETR, Grounding DINO) against three YOLO variants (AABB, OBB, and
YOLO-World). Our findings reveal significant performance variations dependent
on model architecture, data set characteristics, and bounding box
representation. In the e-NDP dataset, Co-DETR achieves state-of-the-art results
(0.752 mAP@.50:.95), closely followed by YOLOv11X-OBB (0.721). Conversely, on
the more complex CATMuS and HORAE datasets, the CNN-based YOLOv11x-OBB
significantly outperforms all other models (0.564 and 0.568, respectively).
This study unequivocally demonstrates that using Oriented Bounding Boxes (OBB)
is not a minor refinement but a fundamental requirement for accurately modeling
the non-Cartesian nature of historical manuscripts. We conclude that a key
trade-off exists between the global context awareness of Transformers, ideal
for structured layouts, and the superior generalization of CNN-OBB models for
visually diverse and complex documents.

</details>


### [83] [Feature Hallucination for Self-supervised Action Recognition](https://arxiv.org/abs/2506.20342)
*Lei Wang,Piotr Koniusz*

Main category: cs.CV

TL;DR: 提出了一种深度翻译动作识别框架，通过联合预测动作概念和辅助特征提升识别准确率，并引入两种新的领域特定描述符。


<details>
  <summary>Details</summary>
Motivation: 视频中的人类动作理解需要高级语义推理和多模态特征的有效整合。

Method: 结合RGB视频帧预测动作概念和辅助特征，引入对象检测特征（ODF）和显著性检测特征（SDF），并整合多种模态数据。

Result: 在多个基准测试（如Kinetics-400、Kinetics-600等）上达到最先进性能。

Conclusion: 该框架能有效捕捉细粒度动作动态，展示了多模态自监督动作识别的潜力。

Abstract: Understanding human actions in videos requires more than raw pixel analysis;
it relies on high-level semantic reasoning and effective integration of
multimodal features. We propose a deep translational action recognition
framework that enhances recognition accuracy by jointly predicting action
concepts and auxiliary features from RGB video frames. At test time,
hallucination streams infer missing cues, enriching feature representations
without increasing computational overhead. To focus on action-relevant regions
beyond raw pixels, we introduce two novel domain-specific descriptors. Object
Detection Features (ODF) aggregate outputs from multiple object detectors to
capture contextual cues, while Saliency Detection Features (SDF) highlight
spatial and intensity patterns crucial for action recognition. Our framework
seamlessly integrates these descriptors with auxiliary modalities such as
optical flow, Improved Dense Trajectories, skeleton data, and audio cues. It
remains compatible with state-of-the-art architectures, including I3D,
AssembleNet, Video Transformer Network, FASTER, and recent models like VideoMAE
V2 and InternVideo2. To handle uncertainty in auxiliary features, we
incorporate aleatoric uncertainty modeling in the hallucination step and
introduce a robust loss function to mitigate feature noise. Our multimodal
self-supervised action recognition framework achieves state-of-the-art
performance on multiple benchmarks, including Kinetics-400, Kinetics-600, and
Something-Something V2, demonstrating its effectiveness in capturing
fine-grained action dynamics.

</details>


### [84] [Exploiting Lightweight Hierarchical ViT and Dynamic Framework for Efficient Visual Tracking](https://arxiv.org/abs/2506.20381)
*Ben Kang,Xin Chen,Jie Zhao,Chunjuan Bo,Dong Wang,Huchuan Lu*

Main category: cs.CV

TL;DR: HiT和DyHiT是高效的视觉跟踪模型，通过轻量级Transformer和动态路由技术实现高速和高性能。


<details>
  <summary>Details</summary>
Motivation: 解决Transformer跟踪器在资源受限设备上速度慢的问题。

Method: HiT使用Bridge Module和双图像位置编码；DyHiT通过动态路由适应场景复杂度。

Result: HiT达61 fps，AUC 64.6%；DyHiT达111 fps，AUC 62.4%。

Conclusion: HiT和DyHiT在速度和性能上均优于现有高效跟踪器，动态路由方法还可加速其他高性能跟踪器。

Abstract: Transformer-based visual trackers have demonstrated significant advancements
due to their powerful modeling capabilities. However, their practicality is
limited on resource-constrained devices because of their slow processing
speeds. To address this challenge, we present HiT, a novel family of efficient
tracking models that achieve high performance while maintaining fast operation
across various devices. The core innovation of HiT lies in its Bridge Module,
which connects lightweight transformers to the tracking framework, enhancing
feature representation quality. Additionally, we introduce a dual-image
position encoding approach to effectively encode spatial information. HiT
achieves an impressive speed of 61 frames per second (fps) on the NVIDIA Jetson
AGX platform, alongside a competitive AUC of 64.6% on the LaSOT benchmark,
outperforming all previous efficient trackers.Building on HiT, we propose
DyHiT, an efficient dynamic tracker that flexibly adapts to scene complexity by
selecting routes with varying computational requirements. DyHiT uses search
area features extracted by the backbone network and inputs them into an
efficient dynamic router to classify tracking scenarios. Based on the
classification, DyHiT applies a divide-and-conquer strategy, selecting
appropriate routes to achieve a superior trade-off between accuracy and speed.
The fastest version of DyHiT achieves 111 fps on NVIDIA Jetson AGX while
maintaining an AUC of 62.4% on LaSOT.Furthermore, we introduce a training-free
acceleration method based on the dynamic routing architecture of DyHiT. This
method significantly improves the execution speed of various high-performance
trackers without sacrificing accuracy. For instance, our acceleration method
enables the state-of-the-art tracker SeqTrack-B256 to achieve a 2.68 times
speedup on an NVIDIA GeForce RTX 2080 Ti GPU while maintaining the same AUC of
69.9% on the LaSOT.

</details>


### [85] [A Novel Large Vision Foundation Model (LVFM)-based Approach for Generating High-Resolution Canopy Height Maps in Plantations for Precision Forestry Management](https://arxiv.org/abs/2506.20388)
*Shen Tan,Xin Zhang,Liangxiu Han,Huaguo Huang,Han Wang*

Main category: cs.CV

TL;DR: 提出了一种基于大型视觉基础模型（LVFM）的新方法，用于高分辨率冠层高度图（CHM）生成，以低成本准确监测人工林地上生物量（AGB），支持碳汇项目。


<details>
  <summary>Details</summary>
Motivation: 传统激光雷达方法成本高，而基于RGB图像的深度学习方法在冠层高度特征提取上存在挑战，需要更经济高效的解决方案。

Method: 开发了一个集成特征提取器、自监督特征增强模块和高度估计器的模型，利用1米分辨率的Google Earth图像进行测试。

Result: 模型在北京市房山区的测试中表现优异，平均绝对误差0.09米，均方根误差0.24米，相关性0.78，树冠检测成功率超过90%。

Conclusion: 该方法为人工林和天然林的碳汇评估提供了可扩展的工具，具有广泛的应用潜力。

Abstract: Accurate, cost-effective monitoring of plantation aboveground biomass (AGB)
is crucial for supporting local livelihoods and carbon sequestration
initiatives like the China Certified Emission Reduction (CCER) program.
High-resolution canopy height maps (CHMs) are essential for this, but standard
lidar-based methods are expensive. While deep learning with RGB imagery offers
an alternative, accurately extracting canopy height features remains
challenging. To address this, we developed a novel model for high-resolution
CHM generation using a Large Vision Foundation Model (LVFM). Our model
integrates a feature extractor, a self-supervised feature enhancement module to
preserve spatial details, and a height estimator. Tested in Beijing's Fangshan
District using 1-meter Google Earth imagery, our model outperformed existing
methods, including conventional CNNs. It achieved a mean absolute error of 0.09
m, a root mean square error of 0.24 m, and a correlation of 0.78 against
lidar-based CHMs. The resulting CHMs enabled over 90% success in individual
tree detection, high accuracy in AGB estimation, and effective tracking of
plantation growth, demonstrating strong generalization to non-training areas.
This approach presents a promising, scalable tool for evaluating carbon
sequestration in both plantations and natural forests.

</details>


### [86] [Lightweight Multi-Frame Integration for Robust YOLO Object Detection in Videos](https://arxiv.org/abs/2506.20550)
*Yitong Quan,Benjamin Kiefer,Martin Messmer,Andreas Zell*

Main category: cs.CV

TL;DR: 提出了一种简单有效的多帧输入策略，利用时间信息提升YOLO检测器的性能，同时保持轻量化和实时性。


<details>
  <summary>Details</summary>
Motivation: 现有视频检测方法复杂且计算量大，而单帧检测在动态场景中性能受限。

Method: 将连续多帧堆叠输入YOLO检测器，仅监督目标帧输出。

Result: 在MOT20Det和BOAT360数据集上验证了方法的鲁棒性，缩小了轻量与重型网络的差距。

Conclusion: 方法简单高效，提升了检测性能，并贡献了新数据集BOAT360。

Abstract: Modern image-based object detection models, such as YOLOv7, primarily process
individual frames independently, thus ignoring valuable temporal context
naturally present in videos. Meanwhile, existing video-based detection methods
often introduce complex temporal modules, significantly increasing model size
and computational complexity. In practical applications such as surveillance
and autonomous driving, transient challenges including motion blur, occlusions,
and abrupt appearance changes can severely degrade single-frame detection
performance. To address these issues, we propose a straightforward yet highly
effective strategy: stacking multiple consecutive frames as input to a
YOLO-based detector while supervising only the output corresponding to a single
target frame. This approach leverages temporal information with minimal
modifications to existing architectures, preserving simplicity, computational
efficiency, and real-time inference capability. Extensive experiments on the
challenging MOT20Det and our BOAT360 datasets demonstrate that our method
improves detection robustness, especially for lightweight models, effectively
narrowing the gap between compact and heavy detection networks. Additionally,
we contribute the BOAT360 benchmark dataset, comprising annotated fisheye video
sequences captured from a boat, to support future research in multi-frame video
object detection in challenging real-world scenarios.

</details>


### [87] [Med-Art: Diffusion Transformer for 2D Medical Text-to-Image Generation](https://arxiv.org/abs/2506.20449)
*Changlu Guo,Anders Nymark Christensen,Morten Rieger Hannemose*

Main category: cs.CV

TL;DR: Med-Art是一个针对医学图像生成的框架，通过结合视觉语言模型和预训练文本到图像模型，解决了医学数据稀缺的问题，并提出了创新的混合级扩散微调方法，取得了最佳性能。


<details>
  <summary>Details</summary>
Motivation: 医学图像生成面临数据量小和医学文本稀缺的挑战，需要一种能在有限数据下高效生成图像的方法。

Method: Med-Art结合视觉语言模型生成医学图像描述，并基于预训练的PixArt-α模型进行微调，提出了混合级扩散微调（HLDF）方法。

Result: 在两个医学图像数据集上，Med-Art在FID、KID和下游分类任务中达到了最先进的性能。

Conclusion: Med-Art通过创新的方法解决了医学图像生成的挑战，为有限数据下的高质量图像生成提供了有效解决方案。

Abstract: Text-to-image generative models have achieved remarkable breakthroughs in
recent years. However, their application in medical image generation still
faces significant challenges, including small dataset sizes, and scarcity of
medical textual data. To address these challenges, we propose Med-Art, a
framework specifically designed for medical image generation with limited data.
Med-Art leverages vision-language models to generate visual descriptions of
medical images which overcomes the scarcity of applicable medical textual data.
Med-Art adapts a large-scale pre-trained text-to-image model, PixArt-$\alpha$,
based on the Diffusion Transformer (DiT), achieving high performance under
limited data. Furthermore, we propose an innovative Hybrid-Level Diffusion
Fine-tuning (HLDF) method, which enables pixel-level losses, effectively
addressing issues such as overly saturated colors. We achieve state-of-the-art
performance on two medical image datasets, measured by FID, KID, and downstream
classification performance.

</details>


### [88] [Learning-Based Distance Estimation for 360° Single-Sensor Setups](https://arxiv.org/abs/2506.20586)
*Yitong Quan,Benjamin Kiefer,Martin Messmer,Andreas Zell*

Main category: cs.CV

TL;DR: 提出了一种基于神经网络的单目360度鱼眼相机距离估计方法，优于传统几何方法和其他学习基线。


<details>
  <summary>Details</summary>
Motivation: 解决全向成像中传统几何方法因镜头畸变和环境变化导致的距离估计不准确问题。

Method: 使用神经网络直接从原始全向输入中学习和推断物体距离，无需精确镜头标定。

Result: 在三个360度数据集上验证，模型在准确性和鲁棒性上优于传统方法和学习基线。

Conclusion: 深度学习在全向距离估计中具有潜力，尤其适合低成本机器人、自主导航和监控应用。

Abstract: Accurate distance estimation is a fundamental challenge in robotic
perception, particularly in omnidirectional imaging, where traditional
geometric methods struggle with lens distortions and environmental variability.
In this work, we propose a neural network-based approach for monocular distance
estimation using a single 360{\deg} fisheye lens camera. Unlike classical
trigonometric techniques that rely on precise lens calibration, our method
directly learns and infers the distance of objects from raw omnidirectional
inputs, offering greater robustness and adaptability across diverse conditions.
We evaluate our approach on three 360{\deg} datasets (LOAF, ULM360, and a newly
captured dataset Boat360), each representing distinct environmental and sensor
setups. Our experimental results demonstrate that the proposed learning-based
model outperforms traditional geometry-based methods and other learning
baselines in both accuracy and robustness. These findings highlight the
potential of deep learning for real-time omnidirectional distance estimation,
making our approach particularly well-suited for low-cost applications in
robotics, autonomous navigation, and surveillance.

</details>


### [89] [HiWave: Training-Free High-Resolution Image Generation via Wavelet-Based Diffusion Sampling](https://arxiv.org/abs/2506.20452)
*Tobias Vontobel,Seyedmorteza Sadat,Farnood Salehi,Romann M. Weber*

Main category: cs.CV

TL;DR: HiWave是一种无需训练、零样本的方法，通过两阶段流程提升预训练扩散模型在超高分辨率图像合成中的视觉保真度和结构一致性。


<details>
  <summary>Details</summary>
Motivation: 尽管扩散模型在图像合成中表现出色，但在高分辨率下训练成本高，且现有零样本方法易产生伪影和空间不连贯问题。

Method: HiWave采用两阶段流程：1）生成基础图像；2）通过DDIM反演和小波增强模块优化细节。

Result: 实验表明，HiWave显著减少了伪影，在用户研究中80%的案例优于现有方法。

Conclusion: HiWave无需重新训练或修改架构，即可实现高质量超高分辨率图像合成。

Abstract: Diffusion models have emerged as the leading approach for image synthesis,
demonstrating exceptional photorealism and diversity. However, training
diffusion models at high resolutions remains computationally prohibitive, and
existing zero-shot generation techniques for synthesizing images beyond
training resolutions often produce artifacts, including object duplication and
spatial incoherence. In this paper, we introduce HiWave, a training-free,
zero-shot approach that substantially enhances visual fidelity and structural
coherence in ultra-high-resolution image synthesis using pretrained diffusion
models. Our method employs a two-stage pipeline: generating a base image from
the pretrained model followed by a patch-wise DDIM inversion step and a novel
wavelet-based detail enhancer module. Specifically, we first utilize inversion
methods to derive initial noise vectors that preserve global coherence from the
base image. Subsequently, during sampling, our wavelet-domain detail enhancer
retains low-frequency components from the base image to ensure structural
consistency, while selectively guiding high-frequency components to enrich fine
details and textures. Extensive evaluations using Stable Diffusion XL
demonstrate that HiWave effectively mitigates common visual artifacts seen in
prior methods, achieving superior perceptual quality. A user study confirmed
HiWave's performance, where it was preferred over the state-of-the-art
alternative in more than 80% of comparisons, highlighting its effectiveness for
high-quality, ultra-high-resolution image synthesis without requiring
retraining or architectural modifications.

</details>


### [90] [A Deep Learning Approach to Identify Rock Bolts in Complex 3D Point Clouds of Underground Mines Captured Using Mobile Laser Scanners](https://arxiv.org/abs/2506.20464)
*Dibyayan Patra,Pasindu Ranasinghe,Bikram Banerjee,Simit Raval*

Main category: cs.CV

TL;DR: 论文提出了一种名为DeepBolt的两阶段深度学习架构，用于在地下矿山的大型3D点云中自动高效识别岩锚，解决了传统方法的不足。


<details>
  <summary>Details</summary>
Motivation: 地下矿山中岩锚的频繁评估对维持岩体稳定性和减少风险至关重要，但手动检测因环境恶劣和耗时而不切实际，自动化检测成为必要。

Method: 采用新型两阶段深度学习架构DeepBolt，专门设计用于处理严重的类别不平衡问题，以在复杂3D点云中高效识别岩锚。

Result: DeepBolt在岩锚点上的IoU比现有语义分割模型高出42.5%，分类精度和召回率分别达到96.41%和96.96%。

Conclusion: DeepBolt在复杂地下环境中表现出强大的鲁棒性和有效性，为岩锚自动化检测提供了高效解决方案。

Abstract: Rock bolts are crucial components of the subterranean support systems in
underground mines that provide adequate structural reinforcement to the rock
mass to prevent unforeseen hazards like rockfalls. This makes frequent
assessments of such bolts critical for maintaining rock mass stability and
minimising risks in underground mining operations. Where manual surveying of
rock bolts is challenging due to the low light conditions in the underground
mines and the time-intensive nature of the process, automated detection of rock
bolts serves as a plausible solution. To that end, this study focuses on the
automatic identification of rock bolts within medium to large-scale 3D point
clouds obtained from underground mines using mobile laser scanners. Existing
techniques for automated rock bolt identification primarily rely on feature
engineering and traditional machine learning approaches. However, such
techniques lack robustness as these point clouds present several challenges due
to data noise, varying environments, and complex surrounding structures.
Moreover, the target rock bolts are extremely small objects within large-scale
point clouds and are often partially obscured due to the application of
reinforcement shotcrete. Addressing these challenges, this paper proposes an
approach termed DeepBolt, which employs a novel two-stage deep learning
architecture specifically designed for handling severe class imbalance for the
automatic and efficient identification of rock bolts in complex 3D point
clouds. The proposed method surpasses state-of-the-art semantic segmentation
models by up to 42.5% in Intersection over Union (IoU) for rock bolt points.
Additionally, it outperforms existing rock bolt identification techniques,
achieving a 96.41% precision and 96.96% recall in classifying rock bolts,
demonstrating its robustness and effectiveness in complex underground
environments.

</details>


### [91] [AI-assisted radiographic analysis in detecting alveolar bone-loss severity and patterns](https://arxiv.org/abs/2506.20522)
*Chathura Wimalasiri,Piumal Rathnayake,Shamod Wijerathne,Sumudu Rasnayaka,Dhanushka Leuke Bandara,Roshan Ragel,Vajira Thambawita,Isuru Nawinne*

Main category: cs.CV

TL;DR: 提出了一种基于AI的深度学习框架，用于自动检测和量化牙槽骨流失及其模式，通过IOPA放射影像实现高精度评估。


<details>
  <summary>Details</summary>
Motivation: 牙周炎严重影响口腔健康和生活质量，准确评估骨流失严重程度和模式对诊断和治疗规划至关重要。

Method: 结合YOLOv8进行牙齿检测，Keypoint R-CNN识别解剖标志，YOLOv8x-seg模型分割骨水平和牙齿掩膜，通过几何分析确定骨流失模式。

Result: 在1000张放射影像数据集上，骨流失严重程度检测（ICC达0.80）和模式分类（准确率87%）表现优异。

Conclusion: 该自动化系统为牙周评估提供了快速、客观且可重复的工具，有望改善早期诊断和个性化治疗规划。

Abstract: Periodontitis, a chronic inflammatory disease causing alveolar bone loss,
significantly affects oral health and quality of life. Accurate assessment of
bone loss severity and pattern is critical for diagnosis and treatment
planning. In this study, we propose a novel AI-based deep learning framework to
automatically detect and quantify alveolar bone loss and its patterns using
intraoral periapical (IOPA) radiographs. Our method combines YOLOv8 for tooth
detection with Keypoint R-CNN models to identify anatomical landmarks, enabling
precise calculation of bone loss severity. Additionally, YOLOv8x-seg models
segment bone levels and tooth masks to determine bone loss patterns (horizontal
vs. angular) via geometric analysis. Evaluated on a large, expertly annotated
dataset of 1000 radiographs, our approach achieved high accuracy in detecting
bone loss severity (intra-class correlation coefficient up to 0.80) and bone
loss pattern classification (accuracy 87%). This automated system offers a
rapid, objective, and reproducible tool for periodontal assessment, reducing
reliance on subjective manual evaluation. By integrating AI into dental
radiographic analysis, our framework has the potential to improve early
diagnosis and personalized treatment planning for periodontitis, ultimately
enhancing patient care and clinical outcomes.

</details>


### [92] [AdvMIM: Adversarial Masked Image Modeling for Semi-Supervised Medical Image Segmentation](https://arxiv.org/abs/2506.20563)
*Lei Zhu,Jun Zhou,Rick Siow Mong Goh,Yong Liu*

Main category: cs.CV

TL;DR: 提出了一种对抗性掩码图像建模方法，用于半监督医学图像分割，通过增加监督信号和减少域差距提升性能。


<details>
  <summary>Details</summary>
Motivation: Transformer在医学图像分割中表现出色，但需要大量标注数据，而半监督场景中标注数据有限，现有方法难以有效训练Transformer。

Method: 构建掩码域以增加监督信号，利用原始标签和伪标签学习，并通过对抗训练减少域差距。

Result: 在三个公开数据集上显著优于现有方法。

Conclusion: 该方法有效提升了半监督医学图像分割的性能，代码已开源。

Abstract: Vision Transformer has recently gained tremendous popularity in medical image
segmentation task due to its superior capability in capturing long-range
dependencies. However, transformer requires a large amount of labeled data to
be effective, which hinders its applicability in annotation scarce
semi-supervised learning scenario where only limited labeled data is available.
State-of-the-art semi-supervised learning methods propose combinatorial
CNN-Transformer learning to cross teach a transformer with a convolutional
neural network, which achieves promising results. However, it remains a
challenging task to effectively train the transformer with limited labeled
data. In this paper, we propose an adversarial masked image modeling method to
fully unleash the potential of transformer for semi-supervised medical image
segmentation. The key challenge in semi-supervised learning with transformer
lies in the lack of sufficient supervision signal. To this end, we propose to
construct an auxiliary masked domain from original domain with masked image
modeling and train the transformer to predict the entire segmentation mask with
masked inputs to increase supervision signal. We leverage the original labels
from labeled data and pseudo-labels from unlabeled data to learn the masked
domain. To further benefit the original domain from masked domain, we provide a
theoretical analysis of our method from a multi-domain learning perspective and
devise a novel adversarial training loss to reduce the domain gap between the
original and masked domain, which boosts semi-supervised learning performance.
We also extend adversarial masked image modeling to CNN network. Extensive
experiments on three public medical image segmentation datasets demonstrate the
effectiveness of our method, where our method outperforms existing methods
significantly. Our code is publicly available at
https://github.com/zlheui/AdvMIM.

</details>


### [93] [Show, Tell and Summarize: Dense Video Captioning Using Visual Cue Aided Sentence Summarization](https://arxiv.org/abs/2506.20567)
*Zhiwang Zhang,Dong Xu,Wanli Ouyang,Chuanqi Tan*

Main category: cs.CV

TL;DR: 提出了一种基于分割与摘要（DaS）的密集视频描述框架，通过两阶段LSTM和分层注意力机制生成描述性句子。


<details>
  <summary>Details</summary>
Motivation: 解决长视频中密集事件描述的挑战，通过分割视频为事件提案并利用视觉特征辅助句子摘要。

Method: 将视频分割为事件提案，提取视觉特征并生成句子描述，采用两阶段LSTM和分层注意力机制进行句子摘要。

Result: 在ActivityNet Captions数据集上验证了DaS框架的有效性。

Conclusion: DaS框架通过结合视觉特征和语义信息，成功实现了密集视频描述任务。

Abstract: In this work, we propose a division-and-summarization (DaS) framework for
dense video captioning. After partitioning each untrimmed long video as
multiple event proposals, where each event proposal consists of a set of short
video segments, we extract visual feature (e.g., C3D feature) from each segment
and use the existing image/video captioning approach to generate one sentence
description for this segment. Considering that the generated sentences contain
rich semantic descriptions about the whole event proposal, we formulate the
dense video captioning task as a visual cue aided sentence summarization
problem and propose a new two stage Long Short Term Memory (LSTM) approach
equipped with a new hierarchical attention mechanism to summarize all generated
sentences as one descriptive sentence with the aid of visual features.
Specifically, the first-stage LSTM network takes all semantic words from the
generated sentences and the visual features from all segments within one event
proposal as the input, and acts as the encoder to effectively summarize both
semantic and visual information related to this event proposal. The
second-stage LSTM network takes the output from the first-stage LSTM network
and the visual features from all video segments within one event proposal as
the input, and acts as the decoder to generate one descriptive sentence for
this event proposal. Our comprehensive experiments on the ActivityNet Captions
dataset demonstrate the effectiveness of our newly proposed DaS framework for
dense video captioning.

</details>


### [94] [Causal Representation Learning with Observational Grouping for CXR Classification](https://arxiv.org/abs/2506.20582)
*Rajat Rasal,Avinash Kori,Ben Glocker*

Main category: cs.CV

TL;DR: 通过分组观察学习可识别的因果表示，提高胸部X光疾病分类的泛化性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 在医学影像中，学习真实的因果关系可以提高任务特定潜在特征的泛化性和鲁棒性。

Method: 提出一种端到端框架，通过分组观察学习可识别的因果表示，并利用分组强制对种族、性别和成像视角的不变性。

Result: 实验表明，这些因果表示在多个分类任务中提高了泛化性和鲁棒性。

Conclusion: 分组观察学习可识别的因果表示有助于提升医学影像分类任务的性能。

Abstract: Identifiable causal representation learning seeks to uncover the true causal
relationships underlying a data generation process. In medical imaging, this
presents opportunities to improve the generalisability and robustness of
task-specific latent features. This work introduces the concept of grouping
observations to learn identifiable representations for disease classification
in chest X-rays via an end-to-end framework. Our experiments demonstrate that
these causal representations improve generalisability and robustness across
multiple classification tasks when grouping is used to enforce invariance w.r.t
race, sex, and imaging views.

</details>


### [95] [Dense Video Captioning using Graph-based Sentence Summarization](https://arxiv.org/abs/2506.20583)
*Zhiwang Zhang,Dong Xu,Wanli Ouyang,Luping Zhou*

Main category: cs.CV

TL;DR: 提出了一种基于图的分割与总结（GPaS）框架，用于密集视频字幕生成，通过分割事件为更短片段并总结描述信息，提升字幕质量。


<details>
  <summary>Details</summary>
Motivation: 现有方法未充分探索事件内场景演变，导致在场景和对象变化较大的情况下表现不佳。

Method: 采用两阶段框架：1）分割事件为短片段生成字幕；2）通过图卷积网络（GCN）和LSTM结合，总结片段描述为完整事件字幕。

Result: 在ActivityNet Captions和YouCook II数据集上优于现有方法。

Conclusion: GPaS框架通过细粒度分割和语义关系建模，显著提升了密集视频字幕生成的效果。

Abstract: Recently, dense video captioning has made attractive progress in detecting
and captioning all events in a long untrimmed video. Despite promising results
were achieved, most existing methods do not sufficiently explore the scene
evolution within an event temporal proposal for captioning, and therefore
perform less satisfactorily when the scenes and objects change over a
relatively long proposal. To address this problem, we propose a graph-based
partition-and-summarization (GPaS) framework for dense video captioning within
two stages. For the ``partition" stage, a whole event proposal is split into
short video segments for captioning at a finer level. For the ``summarization"
stage, the generated sentences carrying rich description information for each
segment are summarized into one sentence to describe the whole event. We
particularly focus on the ``summarization" stage, and propose a framework that
effectively exploits the relationship between semantic words for summarization.
We achieve this goal by treating semantic words as nodes in a graph and
learning their interactions by coupling Graph Convolutional Network (GCN) and
Long Short Term Memory (LSTM), with the aid of visual cues. Two schemes of
GCN-LSTM Interaction (GLI) modules are proposed for seamless integration of GCN
and LSTM. The effectiveness of our approach is demonstrated via an extensive
comparison with the state-of-the-arts methods on the two benchmarks ActivityNet
Captions dataset and YouCook II dataset.

</details>


### [96] [TRIM: A Self-Supervised Video Summarization Framework Maximizing Temporal Relative Information and Representativeness](https://arxiv.org/abs/2506.20588)
*Pritam Mishra,Coloma Ballester,Dimosthenis Karatzas*

Main category: cs.CV

TL;DR: 提出了一种自监督视频摘要模型，无需注意力机制或复杂架构，在SUMME和TVSUM数据集上表现优异。


<details>
  <summary>Details</summary>
Motivation: 视频内容激增需要高效摘要方法，现有方法依赖标注或复杂模型，难以跨域应用。

Method: 采用马尔可夫过程驱动的损失指标和两阶段自监督学习范式，避免注意力机制和RNN/Transformer。

Result: 在SUMME和TVSUM数据集上达到最先进性能，媲美监督模型。

Conclusion: 展示了无标注高效架构的潜力，挑战了对复杂模型的依赖。

Abstract: The increasing ubiquity of video content and the corresponding demand for
efficient access to meaningful information have elevated video summarization
and video highlights as a vital research area. However, many state-of-the-art
methods depend heavily either on supervised annotations or on attention-based
models, which are computationally expensive and brittle in the face of
distribution shifts that hinder cross-domain applicability across datasets. We
introduce a pioneering self-supervised video summarization model that captures
both spatial and temporal dependencies without the overhead of attention, RNNs,
or transformers. Our framework integrates a novel set of Markov process-driven
loss metrics and a two-stage self supervised learning paradigm that ensures
both performance and efficiency. Our approach achieves state-of-the-art
performance on the SUMME and TVSUM datasets, outperforming all existing
unsupervised methods. It also rivals the best supervised models, demonstrating
the potential for efficient, annotation-free architectures. This paves the way
for more generalizable video summarization techniques and challenges the
prevailing reliance on complex architectures.

</details>


### [97] [WonderFree: Enhancing Novel View Quality and Cross-View Consistency for 3D Scene Exploration](https://arxiv.org/abs/2506.20590)
*Chaojun Ni,Jie Li,Haoyun Li,Hengyu Liu,Xiaofeng Wang,Zheng Zhu,Guosheng Zhao,Boyuan Wang,Chenxin Li,Guan Huang,Wenjun Mei*

Main category: cs.CV

TL;DR: WonderFree是一个交互式3D场景生成模型，解决了现有方法在视角探索中的限制，通过WorldRestorer和ConsistView提升新视角质量和跨视角一致性。


<details>
  <summary>Details</summary>
Motivation: 现有3D生成方法在视角探索时存在渲染质量低和一致性差的问题，限制了沉浸式体验。

Method: 提出WonderFree模型，包含WorldRestorer（提升新视角质量）和ConsistView（确保跨视角一致性），并设计了数据收集流程。

Result: 实验表明，WonderFree显著提升了渲染质量和全局一致性，用户偏好率达77.20%。

Conclusion: WonderFree为3D场景生成提供了高质量和一致性的解决方案，代码和模型将公开。

Abstract: Interactive 3D scene generation from a single image has gained significant
attention due to its potential to create immersive virtual worlds. However, a
key challenge in current 3D generation methods is the limited explorability,
which cannot render high-quality images during larger maneuvers beyond the
original viewpoint, particularly when attempting to move forward into unseen
areas. To address this challenge, we propose WonderFree, the first model that
enables users to interactively generate 3D worlds with the freedom to explore
from arbitrary angles and directions. Specifically, we decouple this challenge
into two key subproblems: novel view quality, which addresses visual artifacts
and floating issues in novel views, and cross-view consistency, which ensures
spatial consistency across different viewpoints. To enhance rendering quality
in novel views, we introduce WorldRestorer, a data-driven video restoration
model designed to eliminate floaters and artifacts. In addition, a data
collection pipeline is presented to automatically gather training data for
WorldRestorer, ensuring it can handle scenes with varying styles needed for 3D
scene generation. Furthermore, to improve cross-view consistency, we propose
ConsistView, a multi-view joint restoration mechanism that simultaneously
restores multiple perspectives while maintaining spatiotemporal coherence.
Experimental results demonstrate that WonderFree not only enhances rendering
quality across diverse viewpoints but also significantly improves global
coherence and consistency. These improvements are confirmed by CLIP-based
metrics and a user study showing a 77.20% preference for WonderFree over
WonderWorld enabling a seamless and immersive 3D exploration experience. The
code, model, and data will be publicly available.

</details>


### [98] [SFNet: Fusion of Spatial and Frequency-Domain Features for Remote Sensing Image Forgery Detection](https://arxiv.org/abs/2506.20599)
*Ji Qi,Xinchang Zhang,Dingqi Ye,Yongjia Ruan,Xin Guo,Shaowen Wang,Haifeng Li*

Main category: cs.CV

TL;DR: SFNet是一种新型的伪造检测框架，通过结合空间和频域特征，提高了对不同遥感数据的伪造图像检测能力。


<details>
  <summary>Details</summary>
Motivation: 生成式AI的快速发展导致伪造遥感图像难以检测，可能引发错误情报和虚假新闻。现有方法依赖单一视觉特征，难以适应多样化的遥感数据和不断演变的伪造技术。

Method: SFNet采用两个独立的特征提取器分别捕获空间和频域特征，并通过域特征映射模块和混合域特征细化模块（CBAM注意力）对齐和融合多域特征。

Result: 在三个数据集上的实验表明，SFNet比现有方法准确率提高了4%-15.18%，并表现出强大的泛化能力。

Conclusion: SFNet通过多域特征融合，显著提升了伪造遥感图像的检测性能，具有广泛的应用潜力。

Abstract: The rapid advancement of generative artificial intelligence is producing fake
remote sensing imagery (RSI) that is increasingly difficult to detect,
potentially leading to erroneous intelligence, fake news, and even conspiracy
theories. Existing forgery detection methods typically rely on single visual
features to capture predefined artifacts, such as spatial-domain cues to detect
forged objects like roads or buildings in RSI, or frequency-domain features to
identify artifacts from up-sampling operations in adversarial generative
networks (GANs). However, the nature of artifacts can significantly differ
depending on geographic terrain, land cover types, or specific features within
the RSI. Moreover, these complex artifacts evolve as generative models become
more sophisticated. In short, over-reliance on a single visual cue makes
existing forgery detectors struggle to generalize across diverse remote sensing
data. This paper proposed a novel forgery detection framework called SFNet,
designed to identify fake images in diverse remote sensing data by leveraging
spatial and frequency domain features. Specifically, to obtain rich and
comprehensive visual information, SFNet employs two independent feature
extractors to capture spatial and frequency domain features from input RSIs. To
fully utilize the complementary domain features, the domain feature mapping
module and the hybrid domain feature refinement module(CBAM attention) of SFNet
are designed to successively align and fuse the multi-domain features while
suppressing redundant information. Experiments on three datasets show that
SFNet achieves an accuracy improvement of 4%-15.18% over the state-of-the-art
RS forgery detection methods and exhibits robust generalization capabilities.
The code is available at https://github.com/GeoX-Lab/RSTI/tree/main/SFNet.

</details>


### [99] [Video Perception Models for 3D Scene Synthesis](https://arxiv.org/abs/2506.20601)
*Rui Huang,Guangyao Zhai,Zuria Bauer,Marc Pollefeys,Federico Tombari,Leonidas Guibas,Gao Huang,Francis Engelmann*

Main category: cs.CV

TL;DR: VIPScene利用视频生成模型的3D物理世界知识，通过文本和图像提示生成高真实性和结构一致性的3D场景。


<details>
  <summary>Details</summary>
Motivation: 传统3D场景合成需要专业知识且耗时，自动化可助力建筑设计、机器人仿真等领域。现有方法（如LLM或图像生成模型）在空间推理或多视角一致性上存在局限。

Method: VIPScene结合视频生成、3D重建和开放词汇感知模型，通过FPVScore评估场景的连贯性和合理性。

Result: 实验表明VIPScene显著优于现有方法，且能泛化到多样场景。

Conclusion: VIPScene为3D场景合成提供了高效、灵活的解决方案，代码将开源。

Abstract: Traditionally, 3D scene synthesis requires expert knowledge and significant
manual effort. Automating this process could greatly benefit fields such as
architectural design, robotics simulation, virtual reality, and gaming. Recent
approaches to 3D scene synthesis often rely on the commonsense reasoning of
large language models (LLMs) or strong visual priors of modern image generation
models. However, current LLMs demonstrate limited 3D spatial reasoning ability,
which restricts their ability to generate realistic and coherent 3D scenes.
Meanwhile, image generation-based methods often suffer from constraints in
viewpoint selection and multi-view inconsistencies. In this work, we present
Video Perception models for 3D Scene synthesis (VIPScene), a novel framework
that exploits the encoded commonsense knowledge of the 3D physical world in
video generation models to ensure coherent scene layouts and consistent object
placements across views. VIPScene accepts both text and image prompts and
seamlessly integrates video generation, feedforward 3D reconstruction, and
open-vocabulary perception models to semantically and geometrically analyze
each object in a scene. This enables flexible scene synthesis with high realism
and structural consistency. For more precise analysis, we further introduce
First-Person View Score (FPVScore) for coherence and plausibility evaluation,
utilizing continuous first-person perspective to capitalize on the reasoning
ability of multimodal large language models. Extensive experiments show that
VIPScene significantly outperforms existing methods and generalizes well across
diverse scenarios. The code will be released.

</details>


### [100] [Shape2Animal: Creative Animal Generation from Natural Silhouettes](https://arxiv.org/abs/2506.20616)
*Quoc-Duy Tran,Anh-Tuan Vo,Dinh-Khoi Vo,Tam V. Nguyen,Minh-Triet Tran,Trung-Nghia Le*

Main category: cs.CV

TL;DR: Shape2Animal框架通过重新解释自然物体轮廓（如云、石头或火焰）为动物形态，模拟人类的pareidolia现象。


<details>
  <summary>Details</summary>
Motivation: 模仿人类在模糊刺激中感知有意义模式的独特能力。

Method: 使用开放词汇分割提取物体轮廓，结合视觉语言模型生成语义合适的动物概念，并利用文本到图像扩散模型合成符合形状的动物图像。

Result: 在多样化真实输入上验证了框架的鲁棒性和创造力。

Conclusion: Shape2Animal为视觉叙事、教育内容、数字艺术和交互媒体设计提供了新机会。

Abstract: Humans possess a unique ability to perceive meaningful patterns in ambiguous
stimuli, a cognitive phenomenon known as pareidolia. This paper introduces
Shape2Animal framework to mimics this imaginative capacity by reinterpreting
natural object silhouettes, such as clouds, stones, or flames, as plausible
animal forms. Our automated framework first performs open-vocabulary
segmentation to extract object silhouette and interprets semantically
appropriate animal concepts using vision-language models. It then synthesizes
an animal image that conforms to the input shape, leveraging text-to-image
diffusion model and seamlessly blends it into the original scene to generate
visually coherent and spatially consistent compositions. We evaluated
Shape2Animal on a diverse set of real-world inputs, demonstrating its
robustness and creative potential. Our Shape2Animal can offer new opportunities
for visual storytelling, educational content, digital art, and interactive
media design. Our project page is here: https://shape2image.github.io

</details>


### [101] [Joint attitude estimation and 3D neural reconstruction of non-cooperative space objects](https://arxiv.org/abs/2506.20638)
*Clément Forray,Pauline Delporte,Nicolas Delaygue,Florence Genin,Dawa Derksen*

Main category: cs.CV

TL;DR: 利用NeRF技术从模拟图像中重建非合作空间物体的3D模型，重点优化相机姿态，实验表明逐帧训练效果最佳。


<details>
  <summary>Details</summary>
Motivation: 提升空间态势感知能力，支持主动碎片清除、在轨维护等应用。

Method: 使用NeRF进行3D重建，联合优化相机姿态，采用正则化防止姿态突变。

Result: 逐帧训练和均匀旋转优化相机姿态时，3D重建精度最高。

Conclusion: NeRF在复杂空间环境下仍能有效重建3D模型，优化相机姿态是关键。

Abstract: Obtaining a better knowledge of the current state and behavior of objects
orbiting Earth has proven to be essential for a range of applications such as
active debris removal, in-orbit maintenance, or anomaly detection. 3D models
represent a valuable source of information in the field of Space Situational
Awareness (SSA). In this work, we leveraged Neural Radiance Fields (NeRF) to
perform 3D reconstruction of non-cooperative space objects from simulated
images. This scenario is challenging for NeRF models due to unusual camera
characteristics and environmental conditions : mono-chromatic images, unknown
object orientation, limited viewing angles, absence of diffuse lighting etc. In
this work we focus primarly on the joint optimization of camera poses alongside
the NeRF. Our experimental results show that the most accurate 3D
reconstruction is achieved when training with successive images one-by-one. We
estimate camera poses by optimizing an uniform rotation and use regularization
to prevent successive poses from being too far apart.

</details>


### [102] [Disentangled representations of microscopy images](https://arxiv.org/abs/2506.20649)
*Jacopo Dapueto,Vito Paolo Pastore,Nicoletta Noceti,Francesca Odone*

Main category: cs.CV

TL;DR: 本文提出了一种解耦表示学习（DRL）方法，用于提升显微镜图像分类模型的可解释性，并在三个不同领域的基准数据集上验证了其效果。


<details>
  <summary>Details</summary>
Motivation: 显微镜图像分析在诊断、合成工程和环境监测中至关重要，但深度学习模型的可解释性仍是一个挑战。

Method: 采用解耦表示学习（DRL）框架，通过从合成数据中学习表示，提升模型的可解释性。

Result: 在浮游生物、酵母液泡和人类细胞三个领域的基准数据集上，DRL框架在准确性和可解释性之间取得了良好平衡。

Conclusion: DRL方法为显微镜图像分类提供了一种兼具高准确性和可解释性的解决方案。

Abstract: Microscopy image analysis is fundamental for different applications, from
diagnosis to synthetic engineering and environmental monitoring. Modern
acquisition systems have granted the possibility to acquire an escalating
amount of images, requiring a consequent development of a large collection of
deep learning-based automatic image analysis methods. Although deep neural
networks have demonstrated great performance in this field, interpretability,
an essential requirement for microscopy image analysis, remains an open
challenge.
  This work proposes a Disentangled Representation Learning (DRL) methodology
to enhance model interpretability for microscopy image classification.
Exploiting benchmark datasets from three different microscopic image domains
(plankton, yeast vacuoles, and human cells), we show how a DRL framework, based
on transferring a representation learnt from synthetic data, can provide a good
trade-off between accuracy and interpretability in this domain.

</details>


### [103] [MMSearch-R1: Incentivizing LMMs to Search](https://arxiv.org/abs/2506.20670)
*Jinming Wu,Zihao Deng,Wei Li,Yiding Liu,Bo You,Bo Li,Zejun Ma,Ziwei Liu*

Main category: cs.CV

TL;DR: MMSearch-R1是一个端到端强化学习框架，用于优化大型多模态模型（LMMs）在真实互联网环境中的多轮搜索行为，减少搜索调用次数并提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有检索增强生成（RAG）和提示工程搜索代理方法存在搜索效率低下或过度的问题，需要更灵活的动态搜索框架。

Method: 提出MMSearch-R1框架，结合图像和文本搜索工具，通过基于结果的奖励和搜索惩罚机制，指导模型决定何时及如何调用搜索工具。

Result: 实验表明，MMSearch-R1在相同模型规模下优于RAG基线，并减少30%以上的搜索调用次数，性能与更大的RAG模型相当。

Conclusion: MMSearch-R1为多模态搜索研究提供了实用框架和数据集，展示了强化学习在优化搜索行为中的潜力。

Abstract: Robust deployment of large multimodal models (LMMs) in real-world scenarios
requires access to external knowledge sources, given the complexity and dynamic
nature of real-world information. Existing approaches such as
retrieval-augmented generation (RAG) and prompt engineered search agents rely
on rigid pipelines, often leading to inefficient or excessive search behaviors.
We present MMSearch-R1, the first end-to-end reinforcement learning framework
that enables LMMs to perform on-demand, multi-turn search in real-world
Internet environments. Our framework integrates both image and text search
tools, allowing the model to reason about when and how to invoke them guided by
an outcome-based reward with a search penalty. To support training, We collect
a multimodal search VQA dataset through a semi-automated pipeline that covers
diverse visual and textual knowledge needs and curate a search-balanced subset
with both search-required and search-free samples, which proves essential for
shaping efficient and on-demand search behavior. Extensive experiments on
knowledge-intensive and info-seeking VQA tasks show that our model not only
outperforms RAG-based baselines of the same model size, but also matches the
performance of a larger RAG-based model while reducing search calls by over
30%. We further analyze key empirical findings to offer actionable insights for
advancing research in multimodal search.

</details>


### [104] [IPFormer: Visual 3D Panoptic Scene Completion with Context-Adaptive Instance Proposals](https://arxiv.org/abs/2506.20671)
*Markus Gross,Aya Fahmy,Danit Niwattananan,Dominik Muhle,Rui Song,Daniel Cremers,Henri Meeß*

Main category: cs.CV

TL;DR: IPFormer提出了一种基于视觉的3D全景场景补全方法，通过动态生成实例提案来提升场景理解能力，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有全景场景补全方法主要依赖LiDAR，基于相机图像的方法研究不足，且静态查询限制了动态场景适应能力。

Method: IPFormer利用图像上下文动态生成实例提案，并通过注意力机制优化语义实例-体素关系。

Result: IPFormer在PQ$^\dagger$和PQ-All指标上超越现有方法，运行时间减少14倍以上，动态提案带来显著性能提升。

Conclusion: IPFormer通过动态实例提案，为基于视觉的3D全景场景补全提供了创新解决方案。

Abstract: Semantic Scene Completion (SSC) has emerged as a pivotal approach for jointly
learning scene geometry and semantics, enabling downstream applications such as
navigation in mobile robotics. The recent generalization to Panoptic Scene
Completion (PSC) advances the SSC domain by integrating instance-level
information, thereby enhancing object-level sensitivity in scene understanding.
While PSC was introduced using LiDAR modality, methods based on camera images
remain largely unexplored. Moreover, recent Transformer-based SSC approaches
utilize a fixed set of learned queries to reconstruct objects within the scene
volume. Although these queries are typically updated with image context during
training, they remain static at test time, limiting their ability to
dynamically adapt specifically to the observed scene. To overcome these
limitations, we propose IPFormer, the first approach that leverages
context-adaptive instance proposals at train and test time to address
vision-based 3D Panoptic Scene Completion. Specifically, IPFormer adaptively
initializes these queries as panoptic instance proposals derived from image
context and further refines them through attention-based encoding and decoding
to reason about semantic instance-voxel relationships. Experimental results
show that our approach surpasses state-of-the-art methods in overall panoptic
metrics PQ$^\dagger$ and PQ-All, matches performance in individual metrics, and
achieves a runtime reduction exceeding 14$\times$. Furthermore, our ablation
studies reveal that dynamically deriving instance proposals from image context,
as opposed to random initialization, leads to a 3.62% increase in PQ-All and a
remarkable average improvement of 18.65% in combined Thing-metrics. These
results highlight our introduction of context-adaptive instance proposals as a
pioneering effort in addressing vision-based 3D Panoptic Scene Completion.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [105] [Position: Machine Learning Conferences Should Establish a "Refutations and Critiques" Track](https://arxiv.org/abs/2506.19882)
*Rylan Schaeffer,Joshua Kazdan,Yegor Denisov-Blanch,Brando Miranda,Matthias Gerstgrasser,Susan Zhang,Andreas Haupt,Isha Gupta,Elyas Obbad,Jesse Dodge,Jessica Zosa Forde,Koustuv Sinha,Francesco Orabona,Sanmi Koyejo,David Donoho*

Main category: cs.LG

TL;DR: 本文主张机器学习会议应设立专门的“反驳与批评”（R&C）轨道，以系统性纠正研究中的错误。


<details>
  <summary>Details</summary>
Motivation: 机器学习领域快速发展，但存在错误或误导性研究被接受的问题，缺乏系统性纠正机制。

Method: 提出在会议中设立R&C轨道，讨论其设计、评审原则及潜在问题，并以ICLR 2025为例说明。

Result: R&C轨道可为领域提供高声誉平台，促进研究的自我修正。

Conclusion: 机器学习会议应建立官方机制，支持研究的自我纠正。

Abstract: Science progresses by iteratively advancing and correcting humanity's
understanding of the world. In machine learning (ML) research, rapid
advancements have led to an explosion of publications, but have also led to
misleading, incorrect, flawed or perhaps even fraudulent studies being accepted
and sometimes highlighted at ML conferences due to the fallibility of peer
review. While such mistakes are understandable, ML conferences do not offer
robust processes to help the field systematically correct when such errors are
made.This position paper argues that ML conferences should establish a
dedicated "Refutations and Critiques" (R & C) Track. This R & C Track would
provide a high-profile, reputable platform to support vital research that
critically challenges prior research, thereby fostering a dynamic
self-correcting research ecosystem. We discuss key considerations including
track design, review principles, potential pitfalls, and provide an
illustrative example submission concerning a recent ICLR 2025 Oral. We conclude
that ML conferences should create official, reputable mechanisms to help ML
research self-correct.

</details>


### [106] [STIMULUS: Achieving Fast Convergence and Low Sample Complexity in Stochastic Multi-Objective Learning](https://arxiv.org/abs/2506.19883)
*Zhuqing Liu,Chaosheng Dong,Michinari Momma,Simone Shao,Shaoyuan Xu,Yan Gao,Haibo Yang,Jia Liu*

Main category: cs.LG

TL;DR: 本文提出了一种名为STIMULUS的新算法，用于解决多目标优化（MOO）问题，通过递归框架更新随机梯度估计以提高收敛性能，并进一步提出了带有动量项的STIMULUS-M和自适应批处理的STIMULUS+/M+版本。


<details>
  <summary>Details</summary>
Motivation: 多目标优化（MOO）在机器学习和工程中应用广泛，但现有方法在收敛速度和样本复杂度上表现不佳，因此需要一种更高效的方法。

Method: STIMULUS通过递归框架更新随机梯度估计，STIMULUS-M加入动量项加速收敛，STIMULUS+/M+则通过自适应批处理减少全梯度评估需求。

Result: 在非凸和强凸设置下，分别实现了O(1/T)和O(exp{-μT})的收敛速度，以及O(n+√n/ε)和O(n+√n ln(μ/ε))的样本复杂度。

Conclusion: STIMULUS系列算法在收敛速度和样本复杂度上优于现有方法，为MOO问题提供了高效解决方案。

Abstract: Recently, multi-objective optimization (MOO) has gained attention for its
broad applications in ML, operations research, and engineering. However, MOO
algorithm design remains in its infancy and many existing MOO methods suffer
from unsatisfactory convergence rate and sample complexity performance. To
address this challenge, in this paper, we propose an algorithm called STIMULUS(
stochastic path-integrated multi-gradient recursive e\ulstimator), a new and
robust approach for solving MOO problems. Different from the traditional
methods, STIMULUS introduces a simple yet powerful recursive framework for
updating stochastic gradient estimates to improve convergence performance with
low sample complexity. In addition, we introduce an enhanced version of
STIMULUS, termed STIMULUS-M, which incorporates a momentum term to further
expedite convergence. We establish $O(1/T)$ convergence rates of the proposed
methods for non-convex settings and $O (\exp{-\mu T})$ for strongly convex
settings, where $T$ is the total number of iteration rounds. Additionally, we
achieve the state-of-the-art $O \left(n+\sqrt{n}\epsilon^{-1}\right)$ sample
complexities for non-convex settings and $O\left(n+ \sqrt{n} \ln
({\mu/\epsilon})\right)$ for strongly convex settings, where $\epsilon>0$ is a
desired stationarity error. Moreover, to alleviate the periodic full gradient
evaluation requirement in STIMULUS and STIMULUS-M, we further propose enhanced
versions with adaptive batching called STIMULUS+/ STIMULUS-M+ and provide their
theoretical analysis.

</details>


### [107] [FlightKooba: A Fast Interpretable FTP Model](https://arxiv.org/abs/2506.19885)
*Jing Lu,Xuan Wu,Yizhun Tian,Songhan Fan,Yali Fang*

Main category: cs.LG

TL;DR: 论文提出了一种基于HIPPO方法、Koopman理论和状态空间方程的FlightKooba框架，用于高效解决飞行轨迹预测任务中的计算和可解释性问题。


<details>
  <summary>Details</summary>
Motivation: 当前基于Koopman理论的飞行轨迹预测模型存在计算复杂度高、训练时间长和可解释性差的问题。

Method: 结合HIPPO方法、Koopman理论和状态空间方程，直接通过数据构造Koopman算子，减少可训练参数。

Result: FlightKooba在时间和内存消耗上表现优异（训练时间接近Mamba模块，内存减少50%以上，参数减少十倍），成功完成飞行轨迹预测任务。

Conclusion: FlightKooba为Koopman算子的快速计算提供了新方法，为时间序列预测与控制的结合开辟了新可能。

Abstract: The Koopman theory is a powerful and effective modeling tool for converting
nonlinear systems into linear representations, and flight trajectory prediction
(FTP) is a complex nonlinear system. However, current models applying the
Koopman theory to FTP tasks are not very effective, model interpretability is
indeed an issue, and the Koopman operators are computationally intensive,
resulting in long training times. To address this issue, this paper proposes a
new modeling and control framework based on the HIPPO method, the Koopman
theory, and state space equations from cybernetics: FlightKooba. Inspired by
the idea of structural state space equations, FlightKooba directly constructs
the Koopman operators from data. This makes the framework highly interpretable
and significantly reduces the number of trainable parameters in the module,
thereby greatly reducing training time. Experiments have demonstrated the
superiority of the FlightKooba modeling method in terms of time and memory
consumption (training time comparable to the Mamba module without using
CUDA-level acceleration; memory reduced by more than 50% on most datasets, with
a tenfold reduction in the number of parameters), essentially completing the
FTP task. It provides a new method for the fast computation of the Koopman
operators, opening up new possibilities for the combination of time series
forecasting and control.

</details>


### [108] [A Spatio-Temporal Point Process for Fine-Grained Modeling of Reading Behavior](https://arxiv.org/abs/2506.19999)
*Francesco Ignazio Re,Andreas Opedal,Glib Manaiev,Mario Giulianelli,Ryan Cotterell*

Main category: cs.LG

TL;DR: 论文提出了一种基于时空点过程的阅读行为概率模型，捕捉注视点的时空动态，优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 传统方法依赖聚合的眼动追踪数据和强假设，忽略了阅读中的时空动态。

Method: 使用标记的时空点过程模型，注视点持续时间通过时间卷积的预测因子建模，扫视采用Hawkes过程。

Result: Hawkes过程模型在扫视拟合上优于基线，但上下文意外性对注视点持续时间的预测改进有限。

Conclusion: 意外性理论难以解释精细的眼动行为，新模型更全面地捕捉了阅读动态。

Abstract: Reading is a process that unfolds across space and time, alternating between
fixations where a reader focuses on a specific point in space, and saccades
where a reader rapidly shifts their focus to a new point. An ansatz of
psycholinguistics is that modeling a reader's fixations and saccades yields
insight into their online sentence processing. However, standard approaches to
such modeling rely on aggregated eye-tracking measurements and models that
impose strong assumptions, ignoring much of the spatio-temporal dynamics that
occur during reading. In this paper, we propose a more general probabilistic
model of reading behavior, based on a marked spatio-temporal point process,
that captures not only how long fixations last, but also where they land in
space and when they take place in time. The saccades are modeled using a Hawkes
process, which captures how each fixation excites the probability of a new
fixation occurring near it in time and space. The duration time of fixation
events is modeled as a function of fixation-specific predictors convolved
across time, thus capturing spillover effects. Empirically, our Hawkes process
model exhibits a better fit to human saccades than baselines. With respect to
fixation durations, we observe that incorporating contextual surprisal as a
predictor results in only a marginal improvement in the model's predictive
accuracy. This finding suggests that surprisal theory struggles to explain
fine-grained eye movements.

</details>


### [109] [Causal-Aware Intelligent QoE Optimization for VR Interaction with Adaptive Keyframe Extraction](https://arxiv.org/abs/2506.19890)
*Ziru Zhang,Jiadong Yu,Danny H. K. Tsang*

Main category: cs.LG

TL;DR: 论文提出了一种智能框架，结合自适应关键帧提取和因果感知强化学习，以最大化多用户VR交互中的QoE。


<details>
  <summary>Details</summary>
Motivation: 现有方法忽视了带宽、CPU频率与用户感知之间的因果关系，限制了QoE的提升。

Method: 提出PS-CDDPG方法，结合DDPG与因果推理，优化关键帧比率、带宽和计算资源。

Result: 实验表明，该框架显著降低了交互延迟，提升了QoE，并保持了公平性。

Conclusion: 该框架在多用户VR交互中表现优于基准方法。

Abstract: The optimization of quality of experience (QoE) in multi-user virtual reality
(VR) interactions demands a delicate balance between ultra-low latency,
high-fidelity motion synchronization, and equitable resource allocation. While
adaptive keyframe extraction mitigates transmission overhead, existing
approaches often overlook the causal relationships among allocated bandwidth,
CPU frequency, and user perception, limiting QoE gains. This paper proposes an
intelligent framework to maximize QoE by integrating adaptive keyframe
extraction with causal-aware reinforcement learning (RL). First, a novel QoE
metric is formulated using the Weber-Fechner Law, combining perceptual
sensitivity, attention-driven priorities, and motion reconstruction accuracy.
The QoE optimization problem is then modeled as a mixed integer programming
(MIP) task, jointly optimizing keyframe ratios, bandwidth, and computational
resources under horizon-fairness constraints. We propose Partial State Causal
Deep Deterministic Policy Gradient (PS-CDDPG), which integrates the Deep
Deterministic Policy Gradient (DDPG) method with causal influence detection. By
leveraging causal information regarding how QoE is influenced and determined by
various actions, we explore actions guided by weights calculated from causal
inference (CI), which in turn improves training efficiency. Experiments
conducted with the CMU Motion Capture Database demonstrate that our framework
significantly reduces interactive latency, enhances QoE, and maintains
fairness, achieving superior performance compared to benchmark methods.

</details>


### [110] [Orthogonal Soft Pruning for Efficient Class Unlearning](https://arxiv.org/abs/2506.19891)
*Qinghui Gong,Xue Yang,Xiaohu Tang*

Main category: cs.LG

TL;DR: 提出了一种基于正交卷积核正则化的类感知软剪枝框架，实现快速精确的机器遗忘，显著降低计算开销和性能损失。


<details>
  <summary>Details</summary>
Motivation: 满足GDPR等隐私法规要求，解决现有方法在遗忘速度和预测准确性之间的权衡问题。

Method: 利用正交约束训练卷积核，通过激活差异分析识别类特定通道，实现快速遗忘。

Result: 在多个数据集上验证了快速遗忘、低性能损失和高安全性，显著优于现有基线。

Conclusion: 该框架为MLaaS场景提供了高效、实用的实时机器遗忘解决方案。

Abstract: Machine unlearning aims to selectively remove class-specific knowledge from
pretrained neural networks to satisfy privacy regulations such as the GDPR.
Existing methods typically face a trade-off between unlearning speed and
preservation of predictive accuracy, often incurring either high computational
overhead or significant performance degradation on retained classes. In this
paper, we propose a novel class-aware soft pruning framework leveraging
orthogonal convolutional kernel regularization to achieve rapid and precise
forgetting with millisecond-level response times. By enforcing orthogonality
constraints during training, our method decorrelates convolutional filters and
disentangles feature representations, while efficiently identifying
class-specific channels through activation difference analysis. Extensive
evaluations across multiple architectures and datasets demonstrate stable
pruning with near-instant execution, complete forgetting of targeted classes,
and minimal accuracy loss on retained data. Experiments on CIFAR-10, CIFAR-100,
and TinyImageNet confirm that our approach substantially reduces membership
inference attack risks and accelerates unlearning by orders of magnitude
compared to state-of-the-art baselines. This framework provides an efficient,
practical solution for real-time machine unlearning in Machine Learning as a
Service (MLaaS) scenarios.

</details>


### [111] [Multimodal Representation Learning and Fusion](https://arxiv.org/abs/2506.20494)
*Qihang Jin,Enze Ge,Yuhang Xie,Hongying Luo,Junhao Song,Ziqian Bi,Chia Xin Liang,Jibin Guan,Joe Yeong,Junfeng Hao*

Main category: cs.LG

TL;DR: 多模态学习通过结合图像、文本和音频等多种信息源，帮助AI系统构建更强的内部表示，提升解释、推理和决策能力。核心技术包括表示学习、对齐方法和融合策略，但仍面临数据格式差异、输入缺失和对抗攻击等挑战。未来研究方向包括无监督学习、AutoML工具和更好的评估指标，应用领域广泛。


<details>
  <summary>Details</summary>
Motivation: 多模态学习旨在利用不同模态的优势，提升AI系统对复杂现实世界的理解和处理能力。

Method: 采用表示学习、对齐方法和融合策略等核心技术，结合深度学习模型。

Result: 多模态学习在计算机视觉、自然语言处理等领域取得进展，但仍需解决数据格式差异和对抗攻击等问题。

Conclusion: 多模态学习有望推动AI系统更接近人类的理解能力，未来将广泛应用于多个领域。

Abstract: Multi-modal learning is a fast growing area in artificial intelligence. It
tries to help machines understand complex things by combining information from
different sources, like images, text, and audio. By using the strengths of each
modality, multi-modal learning allows AI systems to build stronger and richer
internal representations. These help machines better interpretation, reasoning,
and making decisions in real-life situations. This field includes core
techniques such as representation learning (to get shared features from
different data types), alignment methods (to match information across
modalities), and fusion strategies (to combine them by deep learning models).
Although there has been good progress, some major problems still remain. Like
dealing with different data formats, missing or incomplete inputs, and
defending against adversarial attacks. Researchers now are exploring new
methods, such as unsupervised or semi-supervised learning, AutoML tools, to
make models more efficient and easier to scale. And also more attention on
designing better evaluation metrics or building shared benchmarks, make it
easier to compare model performance across tasks and domains. As the field
continues to grow, multi-modal learning is expected to improve many areas:
computer vision, natural language processing, speech recognition, and
healthcare. In the future, it may help to build AI systems that can understand
the world in a way more like humans, flexible, context aware, and able to deal
with real-world complexity.

</details>


### [112] [Distillation-Enabled Knowledge Alignment for Generative Semantic Communications in AIGC Provisioning Tasks](https://arxiv.org/abs/2506.19893)
*Jingzhi Hu,Geoffrey Ye Li*

Main category: cs.LG

TL;DR: 论文提出DeKA-g算法，通过知识蒸馏和自适应传输优化生成语义通信（GSC）系统，显著提升边缘生成内容与云端内容的对齐效率。


<details>
  <summary>Details</summary>
Motivation: AI生成内容（AIGC）的传输导致网络流量激增，生成语义通信（GSC）虽能减少传输数据量，但知识对齐问题仍具挑战性。

Method: 提出DeKA-g算法，包含元词辅助知识蒸馏（MAKD）和可变率分组SNR自适应（VGSA）两种方法，优化知识蒸馏和传输适应。

Result: DeKA-g将边缘与云端生成图像的对齐效率提升44%，压缩率适应效率提高116%，低SNR条件下性能提升28%。

Conclusion: DeKA-g有效解决了GSC系统中的知识对齐问题，显著提升了传输效率和适应性。

Abstract: Due to the surging amount of AI-generated content (AIGC), its provisioning to
edges and mobile users from the cloud incurs substantial traffic on networks.
Generative semantic communication (GSC) offers a promising solution by
transmitting highly compact information, i.e., prompt text and latent
representations, instead of high-dimensional AIGC data. However, GSC relies on
the alignment between the knowledge in the cloud generative AI (GAI) and that
possessed by the edges and users, and between the knowledge for wireless
transmission and that of actual channels, which remains challenging. In this
paper, we propose DeKA-g, a distillation-enabled knowledge alignment algorithm
for GSC systems. The core idea is to distill the generation knowledge from the
cloud-GAI into low-rank matrices, which can be incorporated by the edge and
used to adapt the transmission knowledge to diverse wireless channel
conditions. DeKA-g comprises two novel methods: metaword-aided knowledge
distillation (MAKD) and variable-rate grouped SNR adaptation (VGSA). For MAKD,
an optimized metaword is employed to enhance the efficiency of knowledge
distillation, while VGSA enables efficient adaptation to diverse compression
rates and SNR ranges. From simulation results, DeKA-g improves the alignment
between the edge-generated images and the cloud-generated ones by 44%.
Moreover, it adapts to compression rates with 116% higher efficiency than the
baseline and enhances the performance in low-SNR conditions by 28%.

</details>


### [113] [Explaining deep neural network models for electricity price forecasting with XAI](https://arxiv.org/abs/2506.19894)
*Antoine Pesenti,Aidan OSullivan*

Main category: cs.LG

TL;DR: 使用深度神经网络（DNN）预测电力市场价格，并结合可解释人工智能（XAI）方法（如SHAP和Gradient）分析市场动态，以增强对电力市场运作的理解。


<details>
  <summary>Details</summary>
Motivation: 电力市场高度复杂，传统计量经济学方法（白盒模型）在预测和分析市场动态方面不如DNN强大。因此，需要结合DNN和XAI方法来提升对市场的理解。

Method: 使用DNN进行价格预测，并应用XAI方法（如SHAP和Gradient）及可视化技术（如热图）分析五个电力市场中各特征的行为和贡献。提出了SSHAP值和SSHAP线的新概念。

Result: 通过DNN和XAI方法的结合，能够更准确地预测价格并理解市场动态。SSHAP值和SSHAP线增强了高维表格模型的复杂表示能力。

Conclusion: DNN与XAI方法的结合为电力市场分析提供了更强大的工具，SSHAP概念进一步提升了模型的可解释性。

Abstract: Electricity markets are highly complex, involving lots of interactions and
complex dependencies that make it hard to understand the inner workings of the
market and what is driving prices. Econometric methods have been developed for
this, white-box models, however, they are not as powerful as deep neural
network models (DNN). In this paper, we use a DNN to forecast the price and
then use XAI methods to understand the factors driving the price dynamics in
the market. The objective is to increase our understanding of how different
electricity markets work. To do that, we apply explainable methods such as SHAP
and Gradient, combined with visual techniques like heatmaps (saliency maps) to
analyse the behaviour and contributions of various features across five
electricity markets. We introduce the novel concepts of SSHAP values and SSHAP
lines to enhance the complex representation of high-dimensional tabular models.

</details>


### [114] [A Framework for Uncertainty Quantification Based on Nearest Neighbors Across Layers](https://arxiv.org/abs/2506.19895)
*Miguel N. Font,José L. Jorro-Aragoneses,Carlos M. Alaíz*

Main category: cs.LG

TL;DR: 提出一种基于训练案例检索的后处理框架，通过测量决策不确定性和层不确定性来提升神经网络在分类任务中的可靠性。


<details>
  <summary>Details</summary>
Motivation: 神经网络在高风险领域（如医疗诊断或自动驾驶）中的错误决策可能带来严重后果，因此需要有效的不确定性测量方法。

Method: 基于每层激活向量相似的训练案例检索，提出两种新指标：决策变化和层不确定性。

Result: 在CIFAR-10和MNIST数据集上的实验表明，新指标优于基于softmax的置信度方法。

Conclusion: 该方法能有效提升不确定性估计，尤其在复杂分类任务中表现突出。

Abstract: Neural Networks have high accuracy in solving problems where it is difficult
to detect patterns or create a logical model. However, these algorithms
sometimes return wrong solutions, which become problematic in high-risk domains
like medical diagnosis or autonomous driving. One strategy to detect and
mitigate these errors is the measurement of the uncertainty over neural network
decisions. In this paper, we present a novel post-hoc framework for measuring
the uncertainty of a decision based on retrieved training cases that have a
similar activation vector to the query for each layer. Based on these retrieved
cases, we propose two new metrics: Decision Change and Layer Uncertainty, which
capture changes in nearest-neighbor class distributions across layers. We
evaluated our approach in a classification model for two datasets: CIFAR-10 and
MNIST. The results show that these metrics enhance uncertainty estimation,
especially in challenging classification tasks, outperforming softmax-based
confidence.

</details>


### [115] [MIRAGE: A Benchmark for Multimodal Information-Seeking and Reasoning in Agricultural Expert-Guided Conversations](https://arxiv.org/abs/2506.20100)
*Vardhan Dongre,Chi Gui,Shubham Garg,Hooshang Nayyeri,Gokhan Tur,Dilek Hakkani-Tür,Vikram S. Adve*

Main category: cs.LG

TL;DR: MIRAGE是一个新的多模态基准测试，专注于农业领域的专家级推理和决策，结合用户查询、专家回答和图像上下文，评估模型的推理、澄清策略和长文本生成能力。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试通常依赖明确输入和封闭分类，无法反映真实世界中的复杂性和开放性。MIRAGE旨在填补这一空白，提供更贴近实际的评估环境。

Method: 基于35,000多个真实用户-专家互动数据，通过多步流程构建，涵盖作物健康、害虫诊断和管理场景，包含7,000多种生物实体。

Result: MIRAGE是一个高保真、分类多样的基准测试，支持开放世界场景，要求模型推断潜在知识差距并处理罕见实体。

Conclusion: MIRAGE为多模态模型在农业领域的实际应用提供了更真实的评估标准，推动了开放世界场景下的研究。

Abstract: We introduce MIRAGE, a new benchmark for multimodal expert-level reasoning
and decision-making in consultative interaction settings. Designed for the
agriculture domain, MIRAGE captures the full complexity of expert consultations
by combining natural user queries, expert-authored responses, and image-based
context, offering a high-fidelity benchmark for evaluating models on grounded
reasoning, clarification strategies, and long-form generation in a real-world,
knowledge-intensive domain. Grounded in over 35,000 real user-expert
interactions and curated through a carefully designed multi-step pipeline,
MIRAGE spans diverse crop health, pest diagnosis, and crop management
scenarios. The benchmark includes more than 7,000 unique biological entities,
covering plant species, pests, and diseases, making it one of the most
taxonomically diverse benchmarks available for vision-language models, grounded
in the real world. Unlike existing benchmarks that rely on well-specified user
inputs and closed-set taxonomies, MIRAGE features underspecified, context-rich
scenarios with open-world settings, requiring models to infer latent knowledge
gaps, handle rare entities, and either proactively guide the interaction or
respond. Project Page: https://mirage-benchmark.github.io

</details>


### [116] [A Comparative Analysis of Reinforcement Learning and Conventional Deep Learning Approaches for Bearing Fault Diagnosis](https://arxiv.org/abs/2506.19929)
*Efe Çakır,Patrick Dumond*

Main category: cs.LG

TL;DR: 本文探讨了强化学习（RL）在轴承故障诊断中的应用，发现其与传统监督学习方法性能相当，但在适应性上表现更优，尽管计算需求较高。


<details>
  <summary>Details</summary>
Motivation: 轴承故障可能导致严重的运行中断和维护成本，现有方法依赖大量标记数据且适应性不足，因此研究强化学习的应用潜力。

Method: 采用深度Q网络（DQN）进行轴承故障分类任务，优化奖励结构以提高适应性。

Result: RL模型在受控条件下与传统方法性能相当，但在适应性上更优，计算需求较高。

Conclusion: 强化学习有潜力补充传统方法，为自适应诊断框架铺平道路。

Abstract: Bearing faults in rotating machinery can lead to significant operational
disruptions and maintenance costs. Modern methods for bearing fault diagnosis
rely heavily on vibration analysis and machine learning techniques, which often
require extensive labeled data and may not adapt well to dynamic environments.
This study explores the feasibility of reinforcement learning (RL),
specifically Deep Q-Networks (DQNs), for bearing fault classification tasks in
machine condition monitoring to enhance the accuracy and adaptability of
bearing fault diagnosis. The results demonstrate that while RL models developed
in this study can match the performance of traditional supervised learning
models under controlled conditions, they excel in adaptability when equipped
with optimized reward structures. However, their computational demands
highlight areas for further improvement. These findings demonstrate RL's
potential to complement traditional methods, paving the way for adaptive
diagnostic frameworks.

</details>


### [117] [Any-Order GPT as Masked Diffusion Model: Decoupling Formulation and Architecture](https://arxiv.org/abs/2506.19935)
*Shuchen Xue,Tianyu Xie,Tianyang Hu,Zijin Feng,Jiacheng Sun,Kenji Kawaguchi,Zhenguo Li,Zhi-Ming Ma*

Main category: cs.LG

TL;DR: 该研究在解码器框架下评估掩码扩散模型（MDM），公平比较MDM（作为任意顺序自回归AO-AR）与标准自回归（AR）范式，并探讨架构影响。


<details>
  <summary>Details</summary>
Motivation: 比较AR和MDM范式时，通常架构差异（解码器与编码器）导致不公平对比，难以区分差异源于范式还是架构。

Method: 在解码器框架下评估MDM，比较AO-AR与标准AR，并分析解码器与编码器架构对MDM的影响。

Result: 解码器MDM在生成速度上显著提升（约25倍），困惑度相当，但建模空间更大；AO-AR目标可能需优化。

Conclusion: 研究解耦了范式与架构影响，为未来模型设计提供见解。

Abstract: Large language models (LLMs) predominantly use autoregressive (AR)
approaches, but masked diffusion models (MDMs) are emerging as viable
alternatives. A key challenge in comparing AR and MDM paradigms is their
typical architectural difference: AR models are often decoder-only, while MDMs
have largely been encoder-only. This practice of changing both the modeling
paradigm and architecture simultaneously makes direct comparisons unfair, as
it's hard to distinguish whether observed differences stem from the paradigm
itself or the architectural shift. This research evaluates MDMs within a
decoder-only framework to: (1) equitably compare MDM (as Any-Order AR, or
AO-AR) and standard AR paradigms. Our investigation suggests that the standard
AO-AR objective, which averages over all token permutations, may benefit from
refinement, as many permutations appear less informative compared to the
language's inherent left-to-right structure. (2) Investigate architectural
influences (decoder-only vs. encoder-only) within MDMs. We demonstrate that
while encoder-only MDMs model a simpler conditional probability space,
decoder-only MDMs can achieve dramatic generation speedups ($\sim25\times$) and
comparable perplexity with temperature annealing despite modeling a vastly
larger space, highlighting key trade-offs. This work thus decouples core
paradigm differences from architectural influences, offering insights for
future model design. Code is available at https://github.com/scxue/AO-GPT-MDM.

</details>


### [118] [The Most Important Features in Generalized Additive Models Might Be Groups of Features](https://arxiv.org/abs/2506.19937)
*Tomas M. Bosschieter,Luis Franca,Jessica Wolk,Yiyuan Wu,Bella Mehta,Joseph Dehoney,Orsolya Kiss,Fiona C. Baker,Qingyu Zhao,Rich Caruana,Kilian M. Pohl*

Main category: cs.LG

TL;DR: 提出了一种高效、无需重新训练模型的方法，用于确定广义加性模型（GAMs）中特征组的重要性，适用于高维数据和重叠组。


<details>
  <summary>Details</summary>
Motivation: 现有方法常忽视特征组的联合信号，而单独分析特征可能遗漏关键信息，尤其是在多模态数据集中。

Method: 引入一种新方法，支持事后定义组、允许组重叠，并与统计学中的解释变异概念平行。

Result: 通过合成实验和两个案例研究（抑郁症症状识别和髋关节置换术后健康社会决定因素）验证了方法的有效性。

Conclusion: 分析特征组重要性比单特征分析更能提供准确、全面的视角。

Abstract: While analyzing the importance of features has become ubiquitous in
interpretable machine learning, the joint signal from a group of related
features is sometimes overlooked or inadvertently excluded. Neglecting the
joint signal could bypass a critical insight: in many instances, the most
significant predictors are not isolated features, but rather the combined
effect of groups of features. This can be especially problematic for datasets
that contain natural groupings of features, including multimodal datasets. This
paper introduces a novel approach to determine the importance of a group of
features for Generalized Additive Models (GAMs) that is efficient, requires no
model retraining, allows defining groups posthoc, permits overlapping groups,
and remains meaningful in high-dimensional settings. Moreover, this definition
offers a parallel with explained variation in statistics. We showcase
properties of our method on three synthetic experiments that illustrate the
behavior of group importance across various data regimes. We then demonstrate
the importance of groups of features in identifying depressive symptoms from a
multimodal neuroscience dataset, and study the importance of social
determinants of health after total hip arthroplasty. These two case studies
reveal that analyzing group importance offers a more accurate, holistic view of
the medical issues compared to a single-feature analysis.

</details>


### [119] [HERCULES: Hierarchical Embedding-based Recursive Clustering Using LLMs for Efficient Summarization](https://arxiv.org/abs/2506.19992)
*Gabor Petnehazi,Bernadett Aradi*

Main category: cs.LG

TL;DR: HERCULES是一种新型分层k-means聚类算法，支持多模态数据，利用LLM生成语义丰富的聚类标题和描述，提升可解释性。


<details>
  <summary>Details</summary>
Motivation: 复杂数据集的快速增长需要能够有效分组并提供可解释性的工具。

Method: 递归应用k-means聚类，结合LLM生成聚类描述，支持两种表示模式（直接和描述模式）。

Result: HERCULES能够从复杂数据集中提取有意义的分层知识。

Conclusion: HERCULES为多模态数据提供了高效且可解释的聚类解决方案。

Abstract: The explosive growth of complex datasets across various modalities
necessitates advanced analytical tools that not only group data effectively but
also provide human-understandable insights into the discovered structures. We
introduce HERCULES (Hierarchical Embedding-based Recursive Clustering Using
LLMs for Efficient Summarization), a novel algorithm and Python package
designed for hierarchical k-means clustering of diverse data types, including
text, images, and numeric data (processed one modality per run). HERCULES
constructs a cluster hierarchy by recursively applying k-means clustering,
starting from individual data points at level 0. A key innovation is its deep
integration of Large Language Models (LLMs) to generate semantically rich
titles and descriptions for clusters at each level of the hierarchy,
significantly enhancing interpretability. The algorithm supports two main
representation modes: `direct' mode, which clusters based on original data
embeddings or scaled numeric features, and `description' mode, which clusters
based on embeddings derived from LLM-generated summaries. Users can provide a
`topic\_seed' to guide LLM-generated summaries towards specific themes. An
interactive visualization tool facilitates thorough analysis and understanding
of the clustering results. We demonstrate HERCULES's capabilities and discuss
its potential for extracting meaningful, hierarchical knowledge from complex
datasets.

</details>


### [120] [TRACED: Transition-aware Regret Approximation with Co-learnability for Environment Design](https://arxiv.org/abs/2506.19997)
*Geonwoo Cho,Jaegyun Im,Jihwan Lee,Hojun Yi,Sejin Kim,Sundong Kim*

Main category: cs.LG

TL;DR: TRACED方法通过结合过渡预测误差和共同学习能力，改进了无监督环境设计（UED）中的课程生成，显著提升了零样本泛化能力，并减少了环境交互需求。


<details>
  <summary>Details</summary>
Motivation: 解决深度强化学习代理在未见环境中的泛化问题，现有UED方法仅通过价值函数损失衡量学习潜力，存在局限性。

Method: 提出TRACED方法，结合过渡预测误差和共同学习能力（co-learnability）来优化课程设计。

Result: 实验表明，TRACED在多个基准测试中提升了零样本泛化能力，且环境交互需求减少至基线方法的1/2。

Conclusion: 通过改进的遗憾近似和任务关系建模，TRACED实现了高效的课程设计。

Abstract: Generalizing deep reinforcement learning agents to unseen environments
remains a significant challenge. One promising solution is Unsupervised
Environment Design (UED), a co-evolutionary framework in which a teacher
adaptively generates tasks with high learning potential, while a student learns
a robust policy from this evolving curriculum. Existing UED methods typically
measure learning potential via regret, the gap between optimal and current
performance, approximated solely by value-function loss. Building on these
approaches, we introduce the transition prediction error as an additional term
in our regret approximation. To capture how training on one task affects
performance on others, we further propose a lightweight metric called
co-learnability. By combining these two measures, we present Transition-aware
Regret Approximation with Co-learnability for Environment Design (TRACED).
Empirical evaluations show that TRACED yields curricula that improve zero-shot
generalization across multiple benchmarks while requiring up to 2x fewer
environment interactions than strong baselines. Ablation studies confirm that
the transition prediction error drives rapid complexity ramp-up and that
co-learnability delivers additional gains when paired with the transition
prediction error. These results demonstrate how refined regret approximation
and explicit modeling of task relationships can be leveraged for
sample-efficient curriculum design in UED.

</details>


### [121] [Neuromorphic Wireless Split Computing with Resonate-and-Fire Neurons](https://arxiv.org/abs/2506.20015)
*Dengyu Wu,Jiechen Chen,H. Vincent Poor,Bipin Rajendran,Osvaldo Simeone*

Main category: cs.LG

TL;DR: 论文提出了一种基于共振放电神经元（RF）的无线分体计算架构，用于高效处理具有丰富频谱特征的流信号，显著降低了计算和传输能耗。


<details>
  <summary>Details</summary>
Motivation: 传统漏电积分放电（LIF）神经元无法有效捕捉流信号的频谱特征，而频谱预处理成本高，因此需要一种更高效的神经元模型。

Method: 采用共振放电神经元（RF）直接处理时域信号，通过可调谐频率提取局部频谱特征，减少尖峰活动，降低能耗。

Result: 实验表明，RF-SNN架构在音频分类和调制分类任务中与传统LIF-SNN和ANN精度相当，但显著减少了尖峰率和总能耗。

Conclusion: RF神经元在无线分体计算中具有显著优势，能够高效处理流信号并降低能耗。

Abstract: Neuromorphic computing offers an energy-efficient alternative to conventional
deep learning accelerators for real-time time-series processing. However, many
edge applications, such as wireless sensing and audio recognition, generate
streaming signals with rich spectral features that are not effectively captured
by conventional leaky integrate-and-fire (LIF) spiking neurons. This paper
investigates a wireless split computing architecture that employs
resonate-and-fire (RF) neurons with oscillatory dynamics to process time-domain
signals directly, eliminating the need for costly spectral pre-processing. By
resonating at tunable frequencies, RF neurons extract time-localized spectral
features while maintaining low spiking activity. This temporal sparsity
translates into significant savings in both computation and transmission
energy. Assuming an OFDM-based analog wireless interface for spike
transmission, we present a complete system design and evaluate its performance
on audio classification and modulation classification tasks. Experimental
results show that the proposed RF-SNN architecture achieves comparable accuracy
to conventional LIF-SNNs and ANNs, while substantially reducing spike rates and
total energy consumption during inference and communication.

</details>


### [122] [New Insights on Unfolding and Fine-tuning Quantum Federated Learning](https://arxiv.org/abs/2506.20016)
*Shanika Iroshi Nanayakkara,Shiva Raj Pokhrel*

Main category: cs.LG

TL;DR: 提出了一种基于深度展开的量子联邦学习新方法，通过动态调整超参数解决客户端异构性问题，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 客户端异构性对量子联邦学习（QFL）性能造成挑战，传统聚合方法在高度异构环境中表现不佳。

Method: 利用深度展开技术，使客户端能自主优化学习率和正则化因子等超参数，动态适应训练行为。

Result: 在IBM量子硬件和Qiskit Aer模拟器上实现约90%的准确率，显著优于传统方法的55%。

Conclusion: 该方法通过自适应的微调，在基因表达分析和癌症检测等关键应用中表现出色，解决了传统QFL的核心局限性。

Abstract: Client heterogeneity poses significant challenges to the performance of
Quantum Federated Learning (QFL). To overcome these limitations, we propose a
new approach leveraging deep unfolding, which enables clients to autonomously
optimize hyperparameters, such as learning rates and regularization factors,
based on their specific training behavior. This dynamic adaptation mitigates
overfitting and ensures robust optimization in highly heterogeneous
environments where standard aggregation methods often fail. Our framework
achieves approximately 90% accuracy, significantly outperforming traditional
methods, which typically yield around 55% accuracy, as demonstrated through
real-time training on IBM quantum hardware and Qiskit Aer simulators. By
developing self adaptive fine tuning, the proposed method proves particularly
effective in critical applications such as gene expression analysis and cancer
detection, enhancing diagnostic precision and predictive modeling within
quantum systems. Our results are attributed to convergence-aware, learnable
optimization steps intrinsic to the deep unfolded framework, which maintains
the generalization. Hence, this study addresses the core limitations of
conventional QFL, streamlining its applicability to any complex challenges such
as healthcare and genomic research.

</details>


### [123] [DIM-SUM: Dynamic IMputation for Smart Utility Management](https://arxiv.org/abs/2506.20023)
*Ryan Hildebrant,Rahul Bhope,Sharad Mehrotra,Christopher Tull,Nalini Venkatasubramanian*

Main category: cs.LG

TL;DR: DIM-SUM是一个预处理框架，通过结合模式聚类和自适应掩码策略，解决了真实缺失数据与人工模拟缺失数据之间的差异，显著提升了时间序列插补模型的性能。


<details>
  <summary>Details</summary>
Motivation: 现实世界的基础设施监测数据中，大量数据缺失且模式复杂，传统基于人工模拟缺失数据的方法难以应对。

Method: DIM-SUM结合模式聚类和自适应掩码策略，并具备理论学习保证，以处理实际数据中的多样化缺失模式。

Result: 在加州水务、电力数据集和基准测试中，DIM-SUM在更少训练数据和更低处理时间下达到与传统方法相似的准确性，且比大型预训练模型精度高2倍，推理时间更短。

Conclusion: DIM-SUM为时间序列插补提供了一种高效且鲁棒的预处理方法，适用于真实世界的复杂缺失数据场景。

Abstract: Time series imputation models have traditionally been developed using
complete datasets with artificial masking patterns to simulate missing values.
However, in real-world infrastructure monitoring, practitioners often encounter
datasets where large amounts of data are missing and follow complex,
heterogeneous patterns. We introduce DIM-SUM, a preprocessing framework for
training robust imputation models that bridges the gap between artificially
masked training data and real missing patterns. DIM-SUM combines pattern
clustering and adaptive masking strategies with theoretical learning guarantees
to handle diverse missing patterns actually observed in the data. Through
extensive experiments on over 2 billion readings from California water
districts, electricity datasets, and benchmarks, we demonstrate that DIM-SUM
outperforms traditional methods by reaching similar accuracy with lower
processing time and significantly less training data. When compared against a
large pre-trained model, DIM-SUM averages 2x higher accuracy with significantly
less inference time.

</details>


### [124] [Elucidated Rolling Diffusion Models for Probabilistic Weather Forecasting](https://arxiv.org/abs/2506.20024)
*Salva Rühling Cachay,Miika Aittala,Karsten Kreis,Noah Brenowitz,Arash Vahdat,Morteza Mardani,Rose Yu*

Main category: cs.LG

TL;DR: 论文提出了一种名为ERDM的新框架，将滚动预测结构与高性能的Elucidated Diffusion Models（EDM）结合，解决了高维混沌系统中复杂时间依赖性和不确定性增长的问题。


<details>
  <summary>Details</summary>
Motivation: 传统扩散模型在高维混沌系统中难以建模复杂时间依赖性和不确定性增长，滚动扩散框架虽提出但未与高性能扩散技术成功结合。

Method: 通过调整EDM的核心组件（噪声调度、网络预处理和Heun采样器）适应滚动预测，提出三项关键贡献：新型损失加权方案、高效初始化策略和混合序列架构。

Result: 在2D Navier-Stokes模拟和ERA5全球天气预报中，ERDM表现优于其他扩散基线模型。

Conclusion: ERDM为扩散模型在序列生成问题中建模不确定性提供了灵活且强大的框架。

Abstract: Diffusion models are a powerful tool for probabilistic forecasting, yet most
applications in high-dimensional chaotic systems predict future snapshots
one-by-one. This common approach struggles to model complex temporal
dependencies and fails to explicitly account for the progressive growth of
uncertainty inherent to such systems. While rolling diffusion frameworks, which
apply increasing noise to forecasts at longer lead times, have been proposed to
address this, their integration with state-of-the-art, high-fidelity diffusion
techniques remains a significant challenge. We tackle this problem by
introducing Elucidated Rolling Diffusion Models (ERDM), the first framework to
successfully unify a rolling forecast structure with the principled, performant
design of Elucidated Diffusion Models (EDM). To do this, we adapt the core EDM
components-its noise schedule, network preconditioning, and Heun sampler-to the
rolling forecast setting. The success of this integration is driven by three
key contributions: (i) a novel loss weighting scheme that focuses model
capacity on the mid-range forecast horizons where determinism gives way to
stochasticity; (ii) an efficient initialization strategy using a pre-trained
EDM for the initial window; and (iii) a bespoke hybrid sequence architecture
for robust spatiotemporal feature extraction under progressive denoising. On 2D
Navier-Stokes simulations and ERA5 global weather forecasting at 1.5^\circ
resolution, ERDM consistently outperforms key diffusion-based baselines,
including conditional autoregressive EDM. ERDM offers a flexible and powerful
general framework for tackling diffusion-based sequence generation problems
where modeling escalating uncertainty is paramount. Code is available at:
https://github.com/salvaRC/erdm

</details>


### [125] [Thumb on the Scale: Optimal Loss Weighting in Last Layer Retraining](https://arxiv.org/abs/2506.20025)
*Nathan Stromberg,Christos Thrampoulidis,Lalitha Sankar*

Main category: cs.LG

TL;DR: 论文探讨了在最后一层重新训练（LLR）的模型中，损失加权在解决训练数据偏差时的有效性，并指出权重需考虑模型的相对过参数化。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于解决机器学习模型在训练数据偏差下的性能差异问题，特别是在介于欠参数化和过参数化之间的LLR模型中。

Method: 通过理论和实践分析，研究了损失加权在LLR模型中的有效性，并强调了权重需考虑模型的过参数化程度。

Result: 结果表明，损失加权在LLR模型中仍然有效，但权重的设定需基于模型的相对过参数化。

Conclusion: 结论指出，在LLR模型中，损失加权是解决偏差的有效方法，但需根据模型的过参数化调整权重。

Abstract: While machine learning models become more capable in discriminative tasks at
scale, their ability to overcome biases introduced by training data has come
under increasing scrutiny. Previous results suggest that there are two extremes
of parameterization with very different behaviors: the population
(underparameterized) setting where loss weighting is optimal and the separable
overparameterized setting where loss weighting is ineffective at ensuring equal
performance across classes. This work explores the regime of last layer
retraining (LLR) in which the unseen limited (retraining) data is frequently
inseparable and the model proportionately sized, falling between the two
aforementioned extremes. We show, in theory and practice, that loss weighting
is still effective in this regime, but that these weights \emph{must} take into
account the relative overparameterization of the model.

</details>


### [126] [Automated Generation of Diverse Courses of Actions for Multi-Agent Operations using Binary Optimization and Graph Learning](https://arxiv.org/abs/2506.20031)
*Prithvi Poddar,Ehsan Tarkesh Esfahani,Karthik Dantu,Souma Chowdhury*

Main category: cs.LG

TL;DR: 论文提出了一种新的理论框架和计算方法，用于生成多样化的行动方案（COA）池，以支持多智能体任务分配和规划。


<details>
  <summary>Details</summary>
Motivation: 在灾难响应、搜救和军事任务中，环境变化和智能体能力差异需要多样化的行动方案，以提高任务完成率和适应性。

Method: 采用图抽象和遗传算法生成多样化的COA池，并结合图神经网络进行单智能体任务排序。

Result: 模拟测试显示，该方法在任务完成率和执行时间上优于随机基线，且能高效规划多达20个COA。

Conclusion: 该框架为多智能体任务分配提供了高效且适应性强的解决方案。

Abstract: Operations in disaster response, search \& rescue, and military missions that
involve multiple agents demand automated processes to support the planning of
the courses of action (COA). Moreover, traverse-affecting changes in the
environment (rain, snow, blockades, etc.) may impact the expected performance
of a COA, making it desirable to have a pool of COAs that are diverse in task
distributions across agents. Further, variations in agent capabilities, which
could be human crews and/or autonomous systems, present practical opportunities
and computational challenges to the planning process. This paper presents a new
theoretical formulation and computational framework to generate such diverse
pools of COAs for operations with soft variations in agent-task compatibility.
Key to the problem formulation is a graph abstraction of the task space and the
pool of COAs itself to quantify its diversity. Formulating the COAs as a
centralized multi-robot task allocation problem, a genetic algorithm is used
for (order-ignoring) allocations of tasks to each agent that jointly maximize
diversity within the COA pool and overall compatibility of the agent-task
mappings. A graph neural network is trained using a policy gradient approach to
then perform single agent task sequencing in each COA, which maximizes
completion rates adaptive to task features. Our tests of the COA generation
process in a simulated environment demonstrate significant performance gain
over a random walk baseline, small optimality gap in task sequencing, and
execution time of about 50 minutes to plan up to 20 COAs for 5 agent/100 task
operations.

</details>


### [127] [Verifiable Unlearning on Edge](https://arxiv.org/abs/2506.20037)
*Mohammad M Maheri,Alex Davidson,Hamed Haddadi*

Main category: cs.LG

TL;DR: 提出了一种基于零知识证明（zk-SNARKs）的验证框架，用于在边缘设备上实现可验证的数据遗忘，同时保护隐私并保持模型性能。


<details>
  <summary>Details</summary>
Motivation: 解决边缘设备在个性化模型上执行数据遗忘时可能面临的版权、偏见或法规问题，确保遗忘操作的正确性和完整性。

Method: 利用zk-SNARKs设计验证框架，开发高效算法支持遗忘操作，确保低计算和内存开销，同时保留个性化增强。

Result: 验证框架实用且有效，实现了可验证的数据遗忘，且对个性化性能影响极小。

Conclusion: 该方法为边缘设备提供了可验证、隐私保护且高效的数据遗忘解决方案。

Abstract: Machine learning providers commonly distribute global models to edge devices,
which subsequently personalize these models using local data. However, issues
such as copyright infringements, biases, or regulatory requirements may require
the verifiable removal of certain data samples across all edge devices.
Ensuring that edge devices correctly execute such unlearning operations is
critical to maintaining integrity.
  In this work, we introduce a verification framework leveraging zero-knowledge
proofs, specifically zk-SNARKs, to confirm data unlearning on personalized
edge-device models without compromising privacy. We have developed algorithms
explicitly designed to facilitate unlearning operations that are compatible
with efficient zk-SNARK proof generation, ensuring minimal computational and
memory overhead suitable for constrained edge environments. Furthermore, our
approach carefully preserves personalized enhancements on edge devices,
maintaining model performance post-unlearning.
  Our results affirm the practicality and effectiveness of this verification
framework, demonstrating verifiable unlearning with minimal degradation in
personalization-induced performance improvements. Our methodology ensures
verifiable, privacy-preserving, and effective machine unlearning across edge
devices.

</details>


### [128] [Cross-Layer Discrete Concept Discovery for Interpreting Language Models](https://arxiv.org/abs/2506.20040)
*Ankur Garg,Xuemin Yu,Hassan Sajjad,Samira Ebrahimi Kahou*

Main category: cs.LG

TL;DR: 论文提出了一种名为CLVQVAE的框架，通过向量量化技术解决Transformer层间特征冗余和线性混合问题，生成紧凑且可解释的概念向量。


<details>
  <summary>Details</summary>
Motivation: 当前研究主要关注单层神经表示，忽略了跨层叠加和冗余问题，导致特征演化难以理解。

Method: 结合top-k温度采样和EMA码本更新的向量量化方法，辅以基于方向相似性的k-means++初始化。

Result: 框架能够压缩冗余特征为紧凑概念向量，提升可解释性。

Conclusion: CLVQVAE为理解Transformer层间特征演化提供了新工具。

Abstract: Uncovering emergent concepts across transformer layers remains a significant
challenge because the residual stream linearly mixes and duplicates
information, obscuring how features evolve within large language models.
Current research efforts primarily inspect neural representations at single
layers, thereby overlooking this cross-layer superposition and the redundancy
it introduces. These representations are typically either analyzed directly for
activation patterns or passed to probing classifiers that map them to a limited
set of predefined concepts. To address these limitations, we propose
\gls{clvqvae}, a framework that uses vector quantization to map representations
across layers and in the process collapse duplicated residual-stream features
into compact, interpretable concept vectors. Our approach uniquely combines
top-$k$ temperature-based sampling during quantization with EMA codebook
updates, providing controlled exploration of the discrete latent space while
maintaining code-book diversity. We further enhance the framework with
scaled-spherical k-means++ for codebook initialization, which clusters by
directional similarity rather than magnitude, better aligning with semantic
structure in word embedding space.

</details>


### [129] [LSH-DynED: A Dynamic Ensemble Framework with LSH-Based Undersampling for Evolving Multi-Class Imbalanced Classification](https://arxiv.org/abs/2506.20041)
*Soheil Abadifard,Fazli Can*

Main category: cs.LG

TL;DR: 该论文提出了一种名为LSH-DynED的新方法，通过结合局部敏感哈希与随机超平面投影（LSH-RHP）和动态集成多样化（DynED）框架，解决了多类别不平衡数据流分类的挑战。


<details>
  <summary>Details</summary>
Motivation: 多类别不平衡数据流分类是一个关键难题，现有研究较少。动态不平衡比的管理是主要挑战。

Method: 提出LSH-DynED方法，利用LSH-RHP对多数类进行欠采样，提供平衡训练集，提升集成预测性能。

Result: 在23个真实世界和10个半合成数据集上实验，LSH-DynED在Kappa和mG-Mean指标上优于15种现有方法。

Conclusion: LSH-DynED在大规模、高维数据集和实际场景中表现出色，具有适应性和鲁棒性。

Abstract: The classification of imbalanced data streams, which have unequal class
distributions, is a key difficulty in machine learning, especially when dealing
with multiple classes. While binary imbalanced data stream classification tasks
have received considerable attention, only a few studies have focused on
multi-class imbalanced data streams. Effectively managing the dynamic imbalance
ratio is a key challenge in this domain. This study introduces a novel, robust,
and resilient approach to address these challenges by integrating Locality
Sensitive Hashing with Random Hyperplane Projections (LSH-RHP) into the Dynamic
Ensemble Diversification (DynED) framework. To the best of our knowledge, we
present the first application of LSH-RHP for undersampling in the context of
imbalanced non-stationary data streams. The proposed method undersamples the
majority classes by utilizing LSH-RHP, provides a balanced training set, and
improves the ensemble's prediction performance. We conduct comprehensive
experiments on 23 real-world and ten semi-synthetic datasets and compare
LSH-DynED with 15 state-of-the-art methods. The results reveal that LSH-DynED
outperforms other approaches in terms of both Kappa and mG-Mean effectiveness
measures, demonstrating its capability in dealing with multi-class imbalanced
non-stationary data streams. Notably, LSH-DynED performs well in large-scale,
high-dimensional datasets with considerable class imbalances and demonstrates
adaptation and robustness in real-world circumstances. To motivate our design,
we review existing methods for imbalanced data streams, outline key challenges,
and offer guidance for future work. For the reproducibility of our results, we
have made our implementation available on GitHub.

</details>


### [130] [GNN's Uncertainty Quantification using Self-Distillation](https://arxiv.org/abs/2506.20046)
*Hirad Daneshvar,Reza Samavi*

Main category: cs.LG

TL;DR: 提出一种基于知识蒸馏的新方法，高效且高精度地量化图神经网络的预测不确定性。


<details>
  <summary>Details</summary>
Motivation: 在医疗领域，图神经网络（GNNs）的性能显著，但量化其预测不确定性仍具挑战性，这对临床环境中的可信度至关重要。传统方法如贝叶斯和集成方法计算成本高且无法充分捕捉模型多样性。

Method: 采用自蒸馏技术，同一网络同时作为教师和学生模型，避免独立训练多个网络。开发了一种不确定性度量，通过为每个GNN分类器分配不同权重来捕捉网络多样性。

Result: 在MIMIC-IV和Enzymes数据集上的实验表明，该方法能有效捕捉模型预测不确定性，性能与MC Dropout和集成方法相当。

Conclusion: 所提方法在高效性和精度上优于传统方法，适用于医疗领域的可信预测。

Abstract: Graph Neural Networks (GNNs) have shown remarkable performance in the
healthcare domain. However, what remained challenging is quantifying the
predictive uncertainty of GNNs, which is an important aspect of trustworthiness
in clinical settings. While Bayesian and ensemble methods can be used to
quantify uncertainty, they are computationally expensive. Additionally, the
disagreement metric used by ensemble methods to compute uncertainty cannot
capture the diversity of models in an ensemble network. In this paper, we
propose a novel method, based on knowledge distillation, to quantify GNNs'
uncertainty more efficiently and with higher precision. We apply
self-distillation, where the same network serves as both the teacher and
student models, thereby avoiding the need to train several networks
independently. To ensure the impact of self-distillation, we develop an
uncertainty metric that captures the diverse nature of the network by assigning
different weights to each GNN classifier. We experimentally evaluate the
precision, performance, and ability of our approach in distinguishing
out-of-distribution data on two graph datasets: MIMIC-IV and Enzymes. The
evaluation results demonstrate that the proposed method can effectively capture
the predictive uncertainty of the model while having performance similar to
that of the MC Dropout and ensemble methods. The code is publicly available at
https://github.com/tailabTMU/UQ_GNN.

</details>


### [131] [Universal pre-training by iterated random computation](https://arxiv.org/abs/2506.20057)
*Peter Bloem*

Main category: cs.LG

TL;DR: 研究使用随机生成数据预训练模型的理论和实证效果，证明其能实现零样本学习和泛化能力提升。


<details>
  <summary>Details</summary>
Motivation: 探索随机生成数据在预训练中的潜力，补充现有理论，并验证其在实际数据中的效果。

Method: 从算法复杂性角度理论分析，并通过实验验证合成数据预训练的效果，扩展到真实数据并微调模型。

Result: 模型在零样本学习中表现优异，且随着规模扩大性能提升；微调后收敛更快、泛化更好。

Conclusion: 随机生成数据预训练是有效的，能提升模型性能，尤其在零样本学习和泛化能力方面。

Abstract: We investigate the use of randomly generated data for the sake of
pre-training a model. We justify this approach theoretically from the
perspective of algorithmic complexity, building on recent research that shows
that sequence models can be trained to approximate Solomonoff induction. We
derive similar, but complementary theoretical results. We show empirically that
synthetically generated data can be used to pre-train a model before the data
is seen. We replicate earlier results that models trained this way show
zero-shot in-context learning across a variety of datasets, and that this
performance improves with scale. We extend earlier results to real-world data,
and show that finetuning a model after pre-training offers faster convergence
and better generalization.

</details>


### [132] [Learning Instruction-Following Policies through Open-Ended Instruction Relabeling with Large Language Models](https://arxiv.org/abs/2506.20061)
*Zhicheng Zhang,Ziyan Wang,Yali Du,Fei Fang*

Main category: cs.LG

TL;DR: 提出一种利用大型语言模型（LLM）自动生成开放指令的方法，通过重新标注失败轨迹提升强化学习的样本效率和策略性能。


<details>
  <summary>Details</summary>
Motivation: 解决强化学习中依赖人工标注指令和稀疏奖励的问题。

Method: 利用LLM从失败轨迹中识别隐含子任务，生成开放指令并重新标注，丰富训练数据。

Result: 在Craftax环境中验证，样本效率、指令覆盖率和策略性能均优于现有方法。

Conclusion: LLM引导的开放指令重新标注能有效提升指令跟随强化学习的效果。

Abstract: Developing effective instruction-following policies in reinforcement learning
remains challenging due to the reliance on extensive human-labeled instruction
datasets and the difficulty of learning from sparse rewards. In this paper, we
propose a novel approach that leverages the capabilities of large language
models (LLMs) to automatically generate open-ended instructions retrospectively
from previously collected agent trajectories. Our core idea is to employ LLMs
to relabel unsuccessful trajectories by identifying meaningful subtasks the
agent has implicitly accomplished, thereby enriching the agent's training data
and substantially alleviating reliance on human annotations. Through this
open-ended instruction relabeling, we efficiently learn a unified
instruction-following policy capable of handling diverse tasks within a single
policy. We empirically evaluate our proposed method in the challenging Craftax
environment, demonstrating clear improvements in sample efficiency, instruction
coverage, and overall policy performance compared to state-of-the-art
baselines. Our results highlight the effectiveness of utilizing LLM-guided
open-ended instruction relabeling to enhance instruction-following
reinforcement learning.

</details>


### [133] [Supervised Coupled Matrix-Tensor Factorization (SCMTF) for Computational Phenotyping of Patient Reported Outcomes in Ulcerative Colitis](https://arxiv.org/abs/2506.20065)
*Cristian Minoccheri,Sophia Tesic,Kayvan Najarian,Ryan Stidham*

Main category: cs.LG

TL;DR: 该论文提出了一种新型的监督耦合矩阵-张量分解方法（SCMTF），用于整合患者报告结果（PROs）和其他数据，预测溃疡性结肠炎患者的药物持续性。该方法首次应用于PROs和UC领域，并成功处理了高缺失数据。


<details>
  <summary>Details</summary>
Motivation: 患者报告的症状（PROs）通常被忽略，尽管它们对理解疾病进展至关重要。本文旨在利用PROs和其他数据，通过新的方法预测药物持续性。

Method: 使用监督耦合矩阵-张量分解（SCMTF）整合静态特征、时间PROs和时间实验室数据，结合深度学习框架处理缺失数据。

Result: 模型在测试集上预测8个月和20个月药物变化的AUC分别为0.853和0.803，并提取了可解释的表型。

Conclusion: 低秩矩阵和张量分解方法可成功应用于UC领域和PROs数据，证明PROs包含通常被忽略的有用信息。

Abstract: Phenotyping is the process of distinguishing groups of patients to identify
different types of disease progression. A recent trend employs low-rank matrix
and tensor factorization methods for their capability of dealing with
multi-modal, heterogeneous, and missing data. Symptom quantification is crucial
for understanding patient experiences in inflammatory bowel disease, especially
in conditions such as ulcerative colitis (UC). However, patient-reported
symptoms are typically noisy, subjective, and significantly more sparse than
other data types. For this reason, they are usually not included in phenotyping
and other machine learning methods. This paper explores the application of
computational phenotyping to leverage Patient-Reported Outcomes (PROs) using a
novel supervised coupled matrix-tensor factorization (SCMTF) method, which
integrates temporal PROs and temporal labs with static features to predict
medication persistence in ulcerative colitis. This is the first tensor-based
method that is both supervised and coupled, it is the first application to the
UC domain, and the first application to PROs. We use a deep learning framework
that makes the model flexible and easy to train. The proposed method allows us
to handle the large amount of missing data in the PROs. The best model predicts
changes in medication 8 and 20 months in the future with AUCs of 0.853 and
0.803 on the test set respectively. We derive interpretable phenotypes
consisting of static features and temporal features (including their temporal
patterns). We show that low-rank matrix and tensor based phenotyping can be
successfully applied to the UC domain and to highly missing PRO data. We
identify phenotypes useful to predict medication persistence - these phenotypes
include several symptom variables, showing that PROs contain relevant
infromation that is usually discarded.

</details>


### [134] [A Survey of Predictive Maintenance Methods: An Analysis of Prognostics via Classification and Regression](https://arxiv.org/abs/2506.20090)
*Ainaz Jamshidi,Dongchan Kim,Muhammad Arif*

Main category: cs.LG

TL;DR: 本文综述了预测性维护（PdM）中回归与分类方法的比较，强调了各自的优势、挑战及未来趋势。


<details>
  <summary>Details</summary>
Motivation: 预测性维护对工业实践至关重要，但目前缺乏回归与分类方法的独立比较研究。

Method: 通过文献综述，分析回归（估计剩余使用寿命）与分类（预测故障概率）方法在PdM中的应用。

Result: 回归方法提供剩余使用寿命估计，分类方法预测故障概率；数据不平衡和高维特征是主要挑战。

Conclusion: 未来研究可关注混合方法、AI系统及公开数据集等实用工具，以推动PdM发展。

Abstract: Predictive maintenance (PdM) has become a crucial element of modern
industrial practice. PdM plays a significant role in operational dependability
and cost management by decreasing unforeseen downtime and optimizing asset life
cycle management. Machine learning and deep learning have enabled more precise
forecasts of equipment failure and remaining useful life (RUL). Although many
studies have been conducted on PdM, there has not yet been a standalone
comparative study between regression- and classification-based approaches. In
this review, we look across a range of PdM methodologies, while focusing more
strongly on the comparative use of classification and regression methods in
prognostics. While regression-based methods typically provide estimates of RUL,
classification-based methods present a forecast of the probability of failure
across defined time intervals. Through a comprehensive analysis of recent
literature, we highlight key advancements, challenges-such as data imbalance
and high-dimensional feature spaces-and emerging trends, including hybrid
approaches and AI-enabled prognostic systems. This review aims to provide
researchers and practitioners with an awareness of the strengths and
compromises of various PdM methods and to help identify future research and
build more robust, directed adaptive maintenance systems. Future work may
include a systematic review of practical aspects such as public datasets,
benchmarking platforms, and open-source tools to support the advancement of PdM
research.

</details>


### [135] [MEL: Multi-level Ensemble Learning for Resource-Constrained Environments](https://arxiv.org/abs/2506.20094)
*Krishna Praneet Gudipaty,Walid A. Hanafy,Kaan Ozkara,Qianlin Liang,Jesse Milzman,Prashant Shenoy,Suhas Diggavi*

Main category: cs.LG

TL;DR: 论文提出了一种名为MEL的新框架，用于在边缘计算环境中实现弹性推理，通过多级集成学习训练多个轻量级备份模型，以在故障时保持高精度和低延迟。


<details>
  <summary>Details</summary>
Motivation: 边缘计算环境资源有限且易受故障影响，传统方法如云故障转移或压缩备份会牺牲延迟或精度，无法满足关键边缘推理服务的需求。

Method: MEL框架通过多目标优化问题训练多个轻量级备份模型，鼓励模型多样性以相互优化表示，同时确保每个模型独立性能良好。

Result: 实验表明，MEL在视觉、语言和音频数据集上表现接近原始架构，同时提供容错性和部署灵活性。集成模型大小仅为原始模型的40%，在故障情况下仍保持95.6%的集成精度。

Conclusion: MEL是一种有效的边缘推理弹性解决方案，能够在资源受限的环境中实现高精度和低延迟，同时具备故障容忍能力。

Abstract: AI inference at the edge is becoming increasingly common for low-latency
services. However, edge environments are power- and resource-constrained, and
susceptible to failures. Conventional failure resilience approaches, such as
cloud failover or compressed backups, often compromise latency or accuracy,
limiting their effectiveness for critical edge inference services. In this
paper, we propose Multi-Level Ensemble Learning (MEL), a new framework for
resilient edge inference that simultaneously trains multiple lightweight backup
models capable of operating collaboratively, refining each other when multiple
servers are available, and independently under failures while maintaining good
accuracy. Specifically, we formulate our approach as a multi-objective
optimization problem with a loss formulation that inherently encourages
diversity among individual models to promote mutually refining representations,
while ensuring each model maintains good standalone performance. Empirical
evaluations across vision, language, and audio datasets show that MEL provides
performance comparable to original architectures while also providing fault
tolerance and deployment flexibility across edge platforms. Our results show
that our ensemble model, sized at 40\% of the original model, achieves similar
performance, while preserving 95.6\% of ensemble accuracy in the case of
failures when trained using MEL.

</details>


### [136] [High-Resolution Live Fuel Moisture Content (LFMC) Maps for Wildfire Risk from Multimodal Earth Observation Data](https://arxiv.org/abs/2506.20132)
*Patrick Alan Johnson,Gabriel Tseng,Yawen Zhang,Heather Heward,Virginia Sjahli,Favyen Bastani,Joseph Redmon,Patrick Beukema*

Main category: cs.LG

TL;DR: 利用预训练的多模态地球观测模型生成大范围、高分辨率的Live Fuel Moisture Content (LFMC)地图，显著降低误差，并实现自动化快速生成。


<details>
  <summary>Details</summary>
Motivation: 传统地面LFMC采样成本高且更新慢，无法满足全球野火风险监测需求。

Method: 采用预训练的多模态地球观测模型，结合卫星数据，生成大范围LFMC地图。

Result: 相比随机初始化模型，RMSE降低20%，并在美国两个受野火影响的地区验证了有效性。

Conclusion: 该方法为野火研究和应急响应提供了高效、低成本的LFMC监测工具。

Abstract: Wildfires are increasing in intensity and severity at an alarming rate.
Recent advances in AI and publicly available satellite data enable monitoring
critical wildfire risk factors globally, at high resolution and low latency.
Live Fuel Moisture Content (LFMC) is a critical wildfire risk factor and is
valuable for both wildfire research and operational response. However,
ground-based LFMC samples are both labor intensive and costly to acquire,
resulting in sparse and infrequent updates. In this work, we explore the use of
a pretrained, highly-multimodal earth-observation model for generating
large-scale spatially complete (wall-to-wall) LFMC maps. Our approach achieves
significant improvements over previous methods using randomly initialized
models (20 reduction in RMSE). We provide an automated pipeline that enables
rapid generation of these LFMC maps across the United States, and demonstrate
its effectiveness in two regions recently impacted by wildfire (Eaton and
Palisades).

</details>


### [137] [Causal discovery in deterministic discrete LTI-DAE systems](https://arxiv.org/abs/2506.20169)
*Bala Rajesh Konkathi,Arun K. Tangirala*

Main category: cs.LG

TL;DR: 提出了一种名为PoV的方法，用于在LTI-DAE系统中进行因果发现，优于2022年Kathari和Tangirala的方法，适用于纯动态系统。


<details>
  <summary>Details</summary>
Motivation: 在确定性LTI系统中发现纯原因或驱动变量对因果网络的数据驱动重建至关重要，但现有方法（如DIPCA）无法处理具有代数关系的动态系统。

Method: PoV方法通过动态迭代PCA（DIPCA）确定代数关系数、动态关系数和约束矩阵，并通过约束矩阵的条件数识别最小因果驱动子集。

Result: PoV方法在LTI-DAE系统中有效，能够识别因果驱动变量，适用于纯动态系统。

Conclusion: PoV方法在因果发现中优于现有方法，尤其适用于具有代数关系的动态系统。

Abstract: Discovering pure causes or driver variables in deterministic LTI systems is
of vital importance in the data-driven reconstruction of causal networks. A
recent work by Kathari and Tangirala, proposed in 2022, formulated the causal
discovery method as a constraint identification problem. The constraints are
identified using a dynamic iterative PCA (DIPCA)-based approach for dynamical
systems corrupted with Gaussian measurement errors. The DIPCA-based method
works efficiently for dynamical systems devoid of any algebraic relations.
However, several dynamical systems operate under feedback control and/or are
coupled with conservation laws, leading to differential-algebraic (DAE) or
mixed causal systems. In this work, a method, namely the partition of variables
(PoV), for causal discovery in LTI-DAE systems is proposed. This method is
superior to the method that was presented by Kathari and Tangirala (2022), as
PoV also works for pure dynamical systems, which are devoid of algebraic
equations. The proposed method identifies the causal drivers up to a minimal
subset. PoV deploys DIPCA to first determine the number of algebraic relations
($n_a$), the number of dynamical relations ($n_d$) and the constraint matrix.
Subsequently, the subsets are identified through an admissible partitioning of
the constraint matrix by finding the condition number of it. Case studies are
presented to demonstrate the effectiveness of the proposed method.

</details>


### [138] [Causal Operator Discovery in Partial Differential Equations via Counterfactual Physics-Informed Neural Networks](https://arxiv.org/abs/2506.20181)
*Ronald Katende*

Main category: cs.LG

TL;DR: 提出了一种基于物理信息神经网络和反事实扰动的PDE因果结构发现框架，通过功能干预量化算子级必要性，优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 传统方法如残差最小化或稀疏回归无法有效量化PDE中的因果结构，需要一种更原则性的框架。

Method: 引入因果敏感性指数和结构偏差度量，结合神经网络代理模型，通过功能干预分析算子影响。

Result: 理论证明在特定条件下可精确恢复因果算子支持，实验验证在噪声和稀疏数据下优于标准方法。

Conclusion: 该框架为PDE因果发现提供了可解释且稳健的方法，基于结构因果模型和变分残差分析。

Abstract: We develop a principled framework for discovering causal structure in partial
differential equations (PDEs) using physics-informed neural networks and
counterfactual perturbations. Unlike classical residual minimization or sparse
regression methods, our approach quantifies operator-level necessity through
functional interventions on the governing dynamics. We introduce causal
sensitivity indices and structural deviation metrics to assess the influence of
candidate differential operators within neural surrogates. Theoretically, we
prove exact recovery of the causal operator support under restricted isometry
or mutual coherence conditions, with residual bounds guaranteeing
identifiability. Empirically, we validate the framework on both synthetic and
real-world datasets across climate dynamics, tumor diffusion, and ocean flows.
Our method consistently recovers governing operators even under noise,
redundancy, and data scarcity, outperforming standard PINNs and DeepONets in
structural fidelity. This work positions causal PDE discovery as a tractable
and interpretable inference task grounded in structural causal models and
variational residual analysis.

</details>


### [139] [DuoGPT: Training-free Dual Sparsity through Activation-aware Pruning in LLMs](https://arxiv.org/abs/2506.20194)
*Ruokai Yin,Yuhang Li,Donghyun Lee,Priyadarshini Panda*

Main category: cs.LG

TL;DR: DuoGPT提出了一种结合权重剪枝和激活稀疏性的双稀疏框架，通过动态结构化权重稀疏性和激活感知校准提升LLM性能，并在GPU上高效执行。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLM）因高内存和计算成本难以部署，现有剪枝方法未充分利用运行时激活稀疏性。

Method: 将激活稀疏性重新解释为动态结构化权重稀疏性，结合非结构化权重剪枝构建双稀疏（spMspV）工作负载，并扩展OBC框架以保留准确性。

Result: 在LLaMA-2和LLaMA-3上，DuoGPT在1.39倍加速下比现有结构化剪枝方法准确率提升9.17%。

Conclusion: DuoGPT通过双稀疏性和高效GPU优化，显著提升LLM的部署效率和性能。

Abstract: Large language models (LLMs) deliver strong performance but are difficult to
deploy due to high memory and compute costs. While pruning reduces these
demands, most methods ignore activation sparsity observed at runtime. We
reinterpret activation sparsity as dynamic structured weight sparsity and
propose DuoGPT, a unified framework that constructs dual-sparse (spMspV)
workloads by combining unstructured weight pruning with activation sparsity. To
preserve accuracy, we extend the Optimal Brain Compression (OBC) framework with
activation-aware calibration and introduce output residuals from the dense
model as correction terms. We further optimize the solution for efficient GPU
execution, enabling scalability to billion-parameter LLMs. Evaluations on
LLaMA-2 and LLaMA-3 show that DuoGPT outperforms state-of-the-art structured
pruning methods by up to 9.17% accuracy at an iso-speedup of 1.39$\times$
compared to the baseline dense model.

</details>


### [140] [Zero-Shot Attribution for Large Language Models: A Distribution Testing Approach](https://arxiv.org/abs/2506.20197)
*Clément L. Canonne,Yash Pote,Uddalok Sarkar*

Main category: cs.LG

TL;DR: 论文提出了一种名为Anubis的零样本归属工具，通过假设检验和分布测试方法，解决了从大型语言模型（LLM）生成的代码的归属问题。实验表明，Anubis在区分不同LLM生成的代码时表现优异（AUROC≥0.9）。


<details>
  <summary>Details</summary>
Motivation: 随着越来越多的代码由大型语言模型生成，如何准确归属这些代码的来源成为一个重要问题。论文旨在通过假设检验方法解决这一问题。

Method: 提出Anubis工具，将归属问题转化为分布测试问题，利用LLM的样本和密度估计进行零样本归属。

Result: 在区分DeepSeek-Coder、CodeGemma和Stable-Code等LLM生成的代码时，Anubis仅需约2000个样本即可达到高AUROC（≥0.9）。

Conclusion: Anubis是一种有效的零样本归属工具，能够高效区分不同LLM生成的代码，为代码归属问题提供了实用解决方案。

Abstract: A growing fraction of all code is sampled from Large Language Models (LLMs).
We investigate the problem of attributing code generated by language models
using hypothesis testing to leverage established techniques and guarantees.
Given a set of samples $S$ and a suspect model $\mathcal{L}^*$, our goal is to
assess the likelihood of $S$ originating from $\mathcal{L}^*$. Due to the curse
of dimensionality, this is intractable when only samples from the LLM are
given: to circumvent this, we use both samples and density estimates from the
LLM, a form of access commonly available.
  We introduce $\mathsf{Anubis}$, a zero-shot attribution tool that frames
attribution as a distribution testing problem. Our experiments on a benchmark
of code samples show that $\mathsf{Anubis}$ achieves high AUROC scores (
$\ge0.9$) when distinguishing between LLMs like DeepSeek-Coder, CodeGemma, and
Stable-Code using only $\approx 2000$ samples.

</details>


### [141] [Affective Priming Score: A Data-Driven Method to Detect Priming in Sequential Datasets](https://arxiv.org/abs/2506.20204)
*Eduardo Gutierrez Maestro,Hadi Banaee,Amy Loutfi*

Main category: cs.LG

TL;DR: 论文提出了一种名为Affective Priming Score (APS)的数据驱动方法，用于检测受情感启动效应影响的数据点，并通过实验验证其在减少分类错误方面的有效性。


<details>
  <summary>Details</summary>
Motivation: 情感启动效应在情感计算中引入了模糊性，传统方法主要从标签角度解决，但对数据本身（尤其是生理信号）的影响研究不足。受启动影响的数据可能导致模型分类错误。

Method: 提出APS方法，为每个数据点分配分数以量化其受启动影响的程度，并在SEED和SEED-VII数据集上验证。比较使用原始数据和去启动序列训练模型的分类错误率。

Result: 实验表明，使用去启动序列显著降低了分类错误率，验证了APS方法的有效性。

Conclusion: 该研究通过数据层面识别和缓解启动效应，增强了模型鲁棒性，为情感计算数据集的设计和收集提供了新思路。

Abstract: Affective priming exemplifies the challenge of ambiguity in affective
computing. While the community has largely addressed this issue from a
label-based perspective, identifying data points in the sequence affected by
the priming effect, the impact of priming on data itself, particularly in
physiological signals, remains underexplored. Data affected by priming can lead
to misclassifications when used in learning models. This study proposes the
Affective Priming Score (APS), a data-driven method to detect data points
influenced by the priming effect. The APS assigns a score to each data point,
quantifying the extent to which it is affected by priming. To validate this
method, we apply it to the SEED and SEED-VII datasets, which contain sufficient
transitions between emotional events to exhibit priming effects. We train
models with the same configuration using both the original data and
priming-free sequences. The misclassification rate is significantly reduced
when using priming-free sequences compared to the original data. This work
contributes to the broader challenge of ambiguity by identifying and mitigating
priming effects at the data level, enhancing model robustness, and offering
valuable insights for the design and collection of affective computing
datasets.

</details>


### [142] [Directed Link Prediction using GNN with Local and Global Feature Fusion](https://arxiv.org/abs/2506.20235)
*Yuyang Zhang,Xu Shen,Yu Xie,Ka-Chun Wong,Weidun Xie,Chengbin Peng*

Main category: cs.LG

TL;DR: 提出了一种融合特征嵌入与社区信息的图神经网络框架，用于有向链接预测，并通过实验验证其优越性。


<details>
  <summary>Details</summary>
Motivation: 解决有向图中链接预测问题，提升现有深度学习方法在节点相似性和邻域信息聚合上的表现。

Method: 结合特征嵌入与社区信息，提出将输入图转换为有向线图以增强信息聚合的GNN框架。

Result: 在多个基准数据集上，使用30%-60%的训练数据时，性能优于现有方法。

Conclusion: 融合社区信息的GNN框架能有效提升有向链接预测的准确性。

Abstract: Link prediction is a classical problem in graph analysis with many practical
applications. For directed graphs, recently developed deep learning approaches
typically analyze node similarities through contrastive learning and aggregate
neighborhood information through graph convolutions. In this work, we propose a
novel graph neural network (GNN) framework to fuse feature embedding with
community information. We theoretically demonstrate that such hybrid features
can improve the performance of directed link prediction. To utilize such
features efficiently, we also propose an approach to transform input graphs
into directed line graphs so that nodes in the transformed graph can aggregate
more information during graph convolutions. Experiments on benchmark datasets
show that our approach outperforms the state-of-the-art in most cases when 30%,
40%, 50%, and 60% of the connected links are used as training data,
respectively.

</details>


### [143] [FedBKD: Distilled Federated Learning to Embrace Gerneralization and Personalization on Non-IID Data](https://arxiv.org/abs/2506.20245)
*Yushan Zhao,Jinyuan He,Donglai Chen,Weijie Luo,Chong Xie,Ri Zhang,Yonghong Chen,Yan Xu*

Main category: cs.LG

TL;DR: 提出了一种名为FedBKD的无数据蒸馏框架，通过生成对抗网络（GAN）合成数据，实现全局和局部模型的双向知识蒸馏，解决了联邦学习中的非独立同分布数据问题。


<details>
  <summary>Details</summary>
Motivation: 解决联邦学习中非独立同分布数据导致的全局模型泛化能力差和局部模型性能不足的问题，同时避免引入公共数据集带来的隐私风险。

Method: 使用GAN生成合成数据，局部模型作为判别器且参数冻结；通过合成数据实现全局与局部模型的双向知识蒸馏。

Result: 在四种基准测试中，FedBKD在不同非独立同分布设置下均取得了最优性能。

Conclusion: FedBKD框架在提升全局和局部模型性能的同时，避免了数据隐私泄露风险，为联邦学习中的非独立同分布问题提供了有效解决方案。

Abstract: Federated learning (FL) is a decentralized collaborative machine learning
(ML) technique. It provides a solution to the issues of isolated data islands
and data privacy leakage in industrial ML practices. One major challenge in FL
is handling the non-identical and independent distributed (non-IID) data.
Current solutions either focus on constructing an all-powerful global model, or
customizing personalized local models. Few of them can provide both a
well-generalized global model and well-performed local models at the same time.
Additionally, many FL solutions to the non-IID problem are benefited from
introducing public datasets. However, this will also increase the risk of data
leakage. To tackle the problems, we propose a novel data-free distillation
framework, Federated Bidirectional Knowledge Distillation (FedBKD).
Specifically, we train Generative Adversarial Networks (GAN) for synthetic
data. During the GAN training, local models serve as discriminators and their
parameters are frozen. The synthetic data is then used for bidirectional
distillation between global and local models to achieve knowledge interactions
so that performances for both sides are improved. We conduct extensive
experiments on 4 benchmarks under different non-IID settings. The results show
that FedBKD achieves SOTA performances in every case.

</details>


### [144] [Q-resafe: Assessing Safety Risks and Quantization-aware Safety Patching for Quantized Large Language Models](https://arxiv.org/abs/2506.20251)
*Kejia Chen,Jiawen Zhang,Jiacong Hu,Yu Wang,Jian Lou,Zunlei Feng,Mingli Song*

Main category: cs.LG

TL;DR: 本文研究了量化大型语言模型（LLMs）的安全性问题，提出了一种量化感知的安全补丁框架Q-resafe，以恢复量化模型的安全能力。


<details>
  <summary>Details</summary>
Motivation: 量化LLMs在资源受限环境中部署时可能损害其安全能力，亟需系统评估和有效缓解策略。

Method: 通过主流量化技术和多样化校准数据集进行安全评估，并提出Q-resafe框架。

Result: 实验表明Q-resafe能有效恢复量化LLMs的安全能力，且不影响其效用。

Conclusion: Q-resafe成功解决了量化LLMs的安全问题，具有实际应用价值。

Abstract: Quantized large language models (LLMs) have gained increasing attention and
significance for enabling deployment in resource-constrained environments.
However, emerging studies on a few calibration dataset-free quantization
methods suggest that quantization may compromise the safety capabilities of
LLMs, underscoring the urgent need for systematic safety evaluations and
effective mitigation strategies. In this paper, we present comprehensive safety
evaluations across various mainstream quantization techniques and diverse
calibration datasets, utilizing widely accepted safety benchmarks. To address
the identified safety vulnerabilities, we propose a quantization-aware safety
patching framework, Q-resafe, to efficiently restore the safety capabilities of
quantized LLMs while minimizing any adverse impact on utility. Extensive
experimental results demonstrate that Q-resafe successfully re-aligns the
safety of quantized LLMs with their pre-quantization counterparts, even under
challenging evaluation scenarios. Project page is available at:
https://github.com/Thecommonirin/Qresafe.

</details>


### [145] [Time-series surrogates from energy consumers generated by machine learning approaches for long-term forecasting scenarios](https://arxiv.org/abs/2506.20253)
*Ben Gerhards,Nikita Popkov,Annekatrin König,Marcel Arpogaus,Bastian Schäfermeier,Leonie Riedl,Stephan Vogt,Philip Hehlert*

Main category: cs.LG

TL;DR: 论文比较了四种数据驱动方法（WGAN、DDPM、HMM、MABF）在生成长时间电力消费合成数据中的表现，旨在提升数据的准确性和隐私保护。


<details>
  <summary>Details</summary>
Motivation: 现有研究多关注短期电力预测，而忽略了个体消费者的长期预测需求，且缺乏高质量合成数据用于系统规划和状态估计。

Method: 评估了WGAN、DDPM、HMM和MABF四种方法在复制电力消费时间动态、长程依赖和概率转移方面的能力。

Result: 比较分析揭示了各方法的优缺点，为能源相关任务选择合适方法提供了依据。

Conclusion: 研究框架提升了合成数据的准确性和隐私保护，适用于状态估计和消费预测等应用。

Abstract: Forecasting attracts a lot of research attention in the electricity value
chain. However, most studies concentrate on short-term forecasting of
generation or consumption with a focus on systems and less on individual
consumers. Even more neglected is the topic of long-term forecasting of
individual power consumption.
  Here, we provide an in-depth comparative evaluation of data-driven methods
for generating synthetic time series data tailored to energy consumption
long-term forecasting. High-fidelity synthetic data is crucial for a wide range
of applications, including state estimations in energy systems or power grid
planning. In this study, we assess and compare the performance of multiple
state-of-the-art but less common techniques: a hybrid Wasserstein Generative
Adversarial Network (WGAN), Denoising Diffusion Probabilistic Model (DDPM),
Hidden Markov Model (HMM), and Masked Autoregressive Bernstein polynomial
normalizing Flows (MABF). We analyze the ability of each method to replicate
the temporal dynamics, long-range dependencies, and probabilistic transitions
characteristic of individual energy consumption profiles. Our comparative
evaluation highlights the strengths and limitations of: WGAN, DDPM, HMM and
MABF aiding in selecting the most suitable approach for state estimations and
other energy-related tasks. Our generation and analysis framework aims to
enhance the accuracy and reliability of synthetic power consumption data while
generating data that fulfills criteria like anonymisation - preserving privacy
concerns mitigating risks of specific profiling of single customers. This study
utilizes an open-source dataset from households in Germany with 15min time
resolution. The generated synthetic power profiles can readily be used in
applications like state estimations or consumption forecasting.

</details>


### [146] [Argumentative Ensembling for Robust Recourse under Model Multiplicity](https://arxiv.org/abs/2506.20260)
*Junqi Jiang,Antonio Rago,Francesco Leofante,Francesca Toni*

Main category: cs.LG

TL;DR: 论文提出了一种名为“recourse-aware ensembling (RAE)”的方法，用于在模型多重性（MM）下提供鲁棒的反事实解释（CEs），并通过计算论证解决模型间的冲突。


<details>
  <summary>Details</summary>
Motivation: 在机器学习中，模型多重性（MM）导致不同模型对相同输入的预测可能不一致，使得反事实解释（CEs）的鲁棒性成为问题。

Method: 提出了一种基于计算论证的论证集成方法，显式表示模型与反事实之间的冲突，并通过论证语义解决冲突。

Result: 理论分析展示了四种论证语义的行为，实证研究验证了该方法在满足六项理想性质上的有效性。

Conclusion: 论证集成方法能够在模型多重性下提供鲁棒的反事实解释，并支持对模型的偏好定制。

Abstract: In machine learning, it is common to obtain multiple equally performing
models for the same prediction task, e.g., when training neural networks with
different random seeds. Model multiplicity (MM) is the situation which arises
when these competing models differ in their predictions for the same input, for
which ensembling is often employed to determine an aggregation of the outputs.
Providing recourse recommendations via counterfactual explanations (CEs) under
MM thus becomes complex, since the CE may not be valid across all models, i.e.,
the CEs are not robust under MM. In this work, we formalise the problem of
providing recourse under MM, which we name recourse-aware ensembling (RAE). We
propose the idea that under MM, CEs for each individual model should be
considered alongside their predictions so that the aggregated prediction and
recourse are decided in tandem. Centred around this intuition, we introduce six
desirable properties for solutions to this problem. For solving RAE, we propose
a novel argumentative ensembling method which guarantees the robustness of CEs
under MM. Specifically, our method leverages computational argumentation to
explicitly represent the conflicts between models and counterfactuals regarding
prediction results and CE validity. It then uses argumentation semantics to
resolve the conflicts and obtain the final solution, in a manner which is
parametric to the chosen semantics. Our method also allows for the
specification of preferences over the models under MM, allowing further
customisation of the ensemble. In a comprehensive theoretical analysis, we
characterise the behaviour of argumentative ensembling with four different
argumentation semantics. We then empirically demonstrate the effectiveness of
our approach in satisfying desirable properties with eight instantiations of
our method. (Abstract is shortened for arXiv.)

</details>


### [147] [Distilling A Universal Expert from Clustered Federated Learning](https://arxiv.org/abs/2506.20285)
*Zeqi Leng,Chunxu Zhang,Guodong Long,Riting Xia,Bo Yang*

Main category: cs.LG

TL;DR: 提出了一种新的联邦学习框架，通过蒸馏多集群知识生成通用专家模型，以解决非独立同分布数据问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法忽略了集群间的共享信息，而这些信息对联邦学习系统的所有参与者具有普遍价值。

Method: 采用三步迭代学习范式：本地模型训练、集群特定模型聚合和通用专家蒸馏。

Result: 实验表明，该方法在多种场景下表现优异，能更有效地平衡个性化和共享知识。

Conclusion: 该方法通过蒸馏共享知识，提升了集群联邦学习的性能，同时保留了非独立同分布数据的特性。

Abstract: Clustered Federated Learning (CFL) addresses the challenges posed by non-IID
data by training multiple group- or cluster-specific expert models. However,
existing methods often overlook the shared information across clusters, which
represents the generalizable knowledge valuable to all participants in the
Federated Learning (FL) system. To overcome this limitation, this paper
introduces a novel FL framework that distills a universal expert model from the
knowledge of multiple clusters. This universal expert captures globally shared
information across all clients and is subsequently distributed to each client
as the initialization for the next round of model training. The proposed FL
framework operates in three iterative steps: (1) local model training at each
client, (2) cluster-specific model aggregation, and (3) universal expert
distillation. This three-step learning paradigm ensures the preservation of
fine-grained non-IID characteristics while effectively incorporating shared
knowledge across clusters. Compared to traditional gradient-based aggregation
methods, the distillation-based model aggregation introduces greater
flexibility in handling model heterogeneity and reduces conflicts among
cluster-specific experts. Extensive experimental results demonstrate the
superior performance of the proposed method across various scenarios,
highlighting its potential to advance the state of CFL by balancing
personalized and shared knowledge more effectively.

</details>


### [148] [Learning Moderately Input-Sensitive Functions: A Case Study in QR Code Decoding](https://arxiv.org/abs/2506.20305)
*Kazuki Yoda,Kazuhiko Kawamoto,Hiroshi Kera*

Main category: cs.LG

TL;DR: 研究了基于学习的QR码解码，发现Transformer模型能突破理论纠错限制，通过学习文本结构实现解码。


<details>
  <summary>Details</summary>
Motivation: 探索中等输入敏感性的学习函数，填补QR码解码领域的学习空白。

Method: 使用Transformer模型进行QR码解码实验。

Result: Transformer能解码超出理论纠错限制的QR码，且能泛化到其他语言和随机字符串。

Conclusion: Transformer的解码机制与传统QR解码器不同，更关注数据位而非纠错位。

Abstract: The hardness of learning a function that attains a target task relates to its
input-sensitivity. For example, image classification tasks are
input-insensitive as minor corruptions should not affect the classification
results, whereas arithmetic and symbolic computation, which have been recently
attracting interest, are highly input-sensitive as each input variable connects
to the computation results. This study presents the first learning-based Quick
Response (QR) code decoding and investigates learning functions of medium
sensitivity. Our experiments reveal that Transformers can successfully decode
QR codes, even beyond the theoretical error-correction limit, by learning the
structure of embedded texts. They generalize from English-rich training data to
other languages and even random strings. Moreover, we observe that the
Transformer-based QR decoder focuses on data bits while ignoring
error-correction bits, suggesting a decoding mechanism distinct from standard
QR code readers.

</details>


### [149] [Beyond-Expert Performance with Limited Demonstrations: Efficient Imitation Learning with Double Exploration](https://arxiv.org/abs/2506.20307)
*Heyang Zhao,Xingrui Yu,David M. Bossens,Ivor W. Tsang,Quanquan Gu*

Main category: cs.LG

TL;DR: 提出了一种名为ILDE的新型模仿学习算法，通过双重探索机制提高样本效率并实现超越专家性能。


<details>
  <summary>Details</summary>
Motivation: 模仿学习在有限演示数据下难以准确学习专家策略，且需要探索环境以实现超越专家性能。

Method: ILDE算法通过乐观策略优化和好奇心驱动探索，分别奖励高不确定性的状态-动作对和偏离演示轨迹的状态。

Result: ILDE在Atari和MuJoCo任务中样本效率更高，且用更少演示数据实现了超越专家性能。

Conclusion: ILDE通过理论证明和实验验证，展示了其在模仿学习中的优越性和潜力。

Abstract: Imitation learning is a central problem in reinforcement learning where the
goal is to learn a policy that mimics the expert's behavior. In practice, it is
often challenging to learn the expert policy from a limited number of
demonstrations accurately due to the complexity of the state space. Moreover,
it is essential to explore the environment and collect data to achieve
beyond-expert performance. To overcome these challenges, we propose a novel
imitation learning algorithm called Imitation Learning with Double Exploration
(ILDE), which implements exploration in two aspects: (1) optimistic policy
optimization via an exploration bonus that rewards state-action pairs with high
uncertainty to potentially improve the convergence to the expert policy, and
(2) curiosity-driven exploration of the states that deviate from the
demonstration trajectories to potentially yield beyond-expert performance.
Empirically, we demonstrate that ILDE outperforms the state-of-the-art
imitation learning algorithms in terms of sample efficiency and achieves
beyond-expert performance on Atari and MuJoCo tasks with fewer demonstrations
than in previous work. We also provide a theoretical justification of ILDE as
an uncertainty-regularized policy optimization method with optimistic
exploration, leading to a regret growing sublinearly in the number of episodes.

</details>


### [150] [Comparative Analysis of Deep Learning Models for Crop Disease Detection: A Transfer Learning Approach](https://arxiv.org/abs/2506.20323)
*Saundarya Subramaniam,Shalini Majumdar,Shantanu Nadar,Kaustubh Kulkarni*

Main category: cs.LG

TL;DR: 开发了一种基于AI的作物病害检测系统，比较了多种深度学习模型在迁移学习中的效果，验证准确率达95.76%。


<details>
  <summary>Details</summary>
Motivation: 帮助资源有限的农村农民通过AI技术改善作物健康管理，推动可持续农业。

Method: 比较了EfficientNet、ResNet101、MobileNetV2和自定义CNN模型，利用迁移学习进行病害分类。

Result: 自定义CNN模型在验证集上达到95.76%的准确率。

Conclusion: 迁移学习在农业实践中具有潜力，可提升作物健康管理和可持续农业。

Abstract: This research presents the development of an Artificial Intelligence (AI) -
driven crop disease detection system designed to assist farmers in rural areas
with limited resources. We aim to compare different deep learning models for a
comparative analysis, focusing on their efficacy in transfer learning. By
leveraging deep learning models, including EfficientNet, ResNet101,
MobileNetV2, and our custom CNN, which achieved a validation accuracy of
95.76%, the system effectively classifies plant diseases. This research
demonstrates the potential of transfer learning in reshaping agricultural
practices, improving crop health management, and supporting sustainable farming
in rural environments.

</details>


### [151] [Permutation Equivariant Neural Controlled Differential Equations for Dynamic Graph Representation Learning](https://arxiv.org/abs/2506.20324)
*Torben Berndt,Benjamin Walker,Tiexin Qin,Jan Stühmer,Andrey Kormilitzin*

Main category: cs.LG

TL;DR: 论文提出了一种基于置换等变性的神经图控制微分方程（Permutation Equivariant Neural Graph CDEs），通过减少参数数量提升训练效率和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 动态图因节点特征和网络结构的动态变化而复杂，现有方法如Graph Neural CDEs虽有效但参数较多。

Method: 将Graph Neural CDEs投影到置换等变函数空间，减少参数数量而不损失表达能力。

Result: 在模拟动态系统和真实任务中，新方法在插值和外推场景下表现更优。

Conclusion: 置换等变性设计显著提升了模型效率和泛化性能。

Abstract: Dynamic graphs exhibit complex temporal dynamics due to the interplay between
evolving node features and changing network structures. Recently, Graph Neural
Controlled Differential Equations (Graph Neural CDEs) successfully adapted
Neural CDEs from paths on Euclidean domains to paths on graph domains. Building
on this foundation, we introduce Permutation Equivariant Neural Graph CDEs,
which project Graph Neural CDEs onto permutation equivariant function spaces.
This significantly reduces the model's parameter count without compromising
representational power, resulting in more efficient training and improved
generalisation. We empirically demonstrate the advantages of our approach
through experiments on simulated dynamical systems and real-world tasks,
showing improved performance in both interpolation and extrapolation scenarios.

</details>


### [152] [Counterfactual Influence as a Distributional Quantity](https://arxiv.org/abs/2506.20481)
*Matthieu Meeus,Igor Shilov,Georgios Kaissis,Yves-Alexandre de Montjoye*

Main category: cs.LG

TL;DR: 研究探讨了机器学习模型记忆训练数据的机制，指出仅依赖自影响（self-influence）会低估记忆化风险，需考虑完整影响分布。


<details>
  <summary>Details</summary>
Motivation: 机器学习模型记忆训练数据引发隐私和泛化问题，现有方法仅关注自影响，忽略了其他样本（如近重复样本）的影响。

Method: 通过计算训练样本间的完整影响分布，分析其对记忆化的作用，并在小语言模型和CIFAR-10图像分类任务中验证。

Result: 发现仅关注自影响会低估记忆化风险，近重复样本的存在显著降低自影响，但仍可能被提取。

Conclusion: 记忆化是训练数据复杂互动的结果，完整影响分布比自影响更能准确捕捉其特性。

Abstract: Machine learning models are known to memorize samples from their training
data, raising concerns around privacy and generalization. Counterfactual
self-influence is a popular metric to study memorization, quantifying how the
model's prediction for a sample changes depending on the sample's inclusion in
the training dataset. However, recent work has shown memorization to be
affected by factors beyond self-influence, with other training samples, in
particular (near-)duplicates, having a large impact. We here study memorization
treating counterfactual influence as a distributional quantity, taking into
account how all training samples influence how a sample is memorized. For a
small language model, we compute the full influence distribution of training
samples on each other and analyze its properties. We find that solely looking
at self-influence can severely underestimate tangible risks associated with
memorization: the presence of (near-)duplicates seriously reduces
self-influence, while we find these samples to be (near-)extractable. We
observe similar patterns for image classification, where simply looking at the
influence distributions reveals the presence of near-duplicates in CIFAR-10.
Our findings highlight that memorization stems from complex interactions across
training data and is better captured by the full influence distribution than by
self-influence alone.

</details>


### [153] [Producer-Fairness in Sequential Bundle Recommendation](https://arxiv.org/abs/2506.20329)
*Alexandre Rio,Marta Soare,Sihem Amer-Yahia*

Main category: cs.LG

TL;DR: 论文研究了序列化捆绑推荐中的公平性问题，提出了一种兼顾生产者公平性和推荐质量的解决方案，并通过实验验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 现实场景中，不同商品组在推荐会话中的曝光度存在不公平问题，需要一种既能保证推荐质量又能实现公平性的方法。

Method: 提出了一种实时解决方案，包括精确算法和两种启发式方法（质量优先和公平优先），以及一种自适应平衡策略。

Result: 在三个真实数据集上的实验表明，这些方法能在不牺牲推荐质量的情况下实现公平的捆绑推荐。

Conclusion: 论文提出的方法有效解决了序列化捆绑推荐中的公平性问题，并展示了不同策略的优缺点。

Abstract: We address fairness in the context of sequential bundle recommendation, where
users are served in turn with sets of relevant and compatible items. Motivated
by real-world scenarios, we formalize producer-fairness, that seeks to achieve
desired exposure of different item groups across users in a recommendation
session. Our formulation combines naturally with building high quality bundles.
Our problem is solved in real time as users arrive. We propose an exact
solution that caters to small instances of our problem. We then examine two
heuristics, quality-first and fairness-first, and an adaptive variant that
determines on-the-fly the right balance between bundle fairness and quality.
Our experiments on three real-world datasets underscore the strengths and
limitations of each solution and demonstrate their efficacy in providing fair
bundle recommendations without compromising bundle quality.

</details>


### [154] [Asymmetric REINFORCE for off-Policy Reinforcement Learning: Balancing positive and negative rewards](https://arxiv.org/abs/2506.20520)
*Charles Arnal,Gaëtan Narozniak,Vivien Cabannes,Yunhao Tang,Julia Kempe,Remi Munos*

Main category: cs.LG

TL;DR: 论文研究了介于离策略强化学习和监督微调之间的算法，分析了简单的离策略REINFORCE算法，发现调整基线V可以优化性能。


<details>
  <summary>Details</summary>
Motivation: 探索如何在离策略强化学习中平衡数据效率和性能，以更好地对齐大型语言模型。

Method: 分析离策略REINFORCE算法，调整基线V以优化奖励信号的使用，并进行理论分析和实验验证。

Result: 理论证明当基线V低于预期奖励时，算法具有策略改进保证；实验验证了在随机多臂老虎机和LLM微调中的有效性。

Conclusion: 离策略更新应更关注正奖励信号，调整基线V可以提升性能，为强化学习对齐LLM提供了新思路。

Abstract: Reinforcement learning (RL) is increasingly used to align large language
models (LLMs). Off-policy methods offer greater implementation simplicity and
data efficiency than on-policy techniques, but often result in suboptimal
performance. In this work, we study the intermediate range of algorithms
between off-policy RL and supervised fine-tuning by analyzing a simple
off-policy REINFORCE algorithm, where the advantage is defined as $A=r-V$, with
$r$ a reward and $V$ some tunable baseline. Intuitively, lowering $V$
emphasizes high-reward samples, while raising it penalizes low-reward ones more
heavily. We first provide a theoretical analysis of this off-policy REINFORCE
algorithm, showing that when the baseline $V$ lower-bounds the expected reward,
the algorithm enjoys a policy improvement guarantee. Our analysis reveals that
while on-policy updates can safely leverage both positive and negative signals,
off-policy updates benefit from focusing more on positive rewards than on
negative ones. We validate our findings experimentally in a controlled
stochastic bandit setting and through fine-tuning state-of-the-art LLMs on
reasoning tasks.

</details>


### [155] [On the ability of Deep Neural Networks to Learn Granger Causality in Multi-Variate Time Series Data](https://arxiv.org/abs/2506.20347)
*Malik Shahid Sultan,Hernando Ombao*

Main category: cs.LG

TL;DR: 论文提出了一种基于深度学习的Granger因果关系（GC）估计新方法，通过模型不确定性和残差分布比较来揭示GC结构，无需显式变量选择。


<details>
  <summary>Details</summary>
Motivation: 传统线性VAR模型在GC分析中受限于假设，而现有基于DNN的方法将GC视为变量选择问题，缺乏灵活性。本文提出GC与预测本质相关，通过联合建模时间序列数据，正则化模型可从数据中学习真实GC结构。

Method: 利用深度神经网络联合建模时间序列，通过比较使用全部数据与剔除特定时间序列时的模型不确定性或残差分布，揭示GC结构。同时研究输入层dropout对GC学习的影响。

Result: 研究表明，经过良好正则化的模型能够直接从数据中学习真实的GC结构，无需在损失函数中添加变量选择或稀疏回归的显式指导。

Conclusion: 论文提出了一种无需显式变量选择的GC估计新范式，通过深度学习的联合建模和正则化，能够有效捕捉时间序列中的因果关系。

Abstract: Granger Causality (GC) offers an elegant statistical framework to study the
association between multivariate time series data. Linear Vector Autoregressive
models (VAR) though have nice interpretation properties but have limited
practical application due to underlying assumptions on the kind of associations
that can be captured by these models. Numerous attempts have already been made
in the literature that exploit the functional approximation power of Deep
Neural Networks (DNNs) for the task of GC estimation. These methods however
treat GC as a variable selection problem. We present a novel paradigm for
approaching GC. We present this idea that GC is essentially linked with
prediction and if a deep learning model is used to model the time series
collectively or jointly, a well regularized model may learn the true granger
causal structure from the data, given that there is enough training data. We
propose to uncover the learned GC structure by comparing the model uncertainty
or distribution of the residuals when the past of everything is used as
compared to the one where a specific time series component is dropped from the
model. We also compare the effect of input layer dropout on the ability of a
neural network to learn granger causality from the data. We show that a well
regularized model infact can learn the true GC structure from the data without
explicitly adding terms in the loss function that guide the model to select
variables or perform sparse regression.

</details>


### [156] [PLoP: Precise LoRA Placement for Efficient Finetuning of Large Models](https://arxiv.org/abs/2506.20629)
*Soufiane Hayou,Nikhil Ghosh,Bin Yu*

Main category: cs.LG

TL;DR: PLoP（精确LoRA放置）是一种轻量级方法，通过自动识别适合放置LoRA适配器的模块类型，提升LoRA微调效率。


<details>
  <summary>Details</summary>
Motivation: LoRA作为一种广泛使用的微调方法，其适配器放置策略尚未有明确结论，PLoP旨在解决这一问题。

Method: 通过直观的理论分析，PLoP自动识别适合放置LoRA适配器的模块类型。

Result: PLoP在监督微调和强化学习任务中表现优于或至少与常用放置策略相当。

Conclusion: PLoP提供了一种高效且自动化的LoRA适配器放置策略，显著提升了微调效果。

Abstract: Low-Rank Adaptation (LoRA) is a widely used finetuning method for large
models. Its small memory footprint allows practitioners to adapt large models
to specific tasks at a fraction of the cost of full finetuning. Different
modifications have been proposed to enhance its efficiency by, for example,
setting the learning rate, the rank, and the initialization. Another
improvement axis is adapter placement strategy: when using LoRA, practitioners
usually pick module types to adapt with LoRA, such as Query and Key modules.
Few works have studied the problem of adapter placement, with nonconclusive
results: original LoRA paper suggested placing adapters in attention modules,
while other works suggested placing them in the MLP modules. Through an
intuitive theoretical analysis, we introduce PLoP (Precise LoRA Placement), a
lightweight method that allows automatic identification of module types where
LoRA adapters should be placed, given a pretrained model and a finetuning task.
We demonstrate that PLoP consistently outperforms, and in the worst case
competes, with commonly used placement strategies through comprehensive
experiments on supervised finetuning and reinforcement learning for reasoning.

</details>


### [157] [DipSVD: Dual-importance Protected SVD for Efficient LLM Compression](https://arxiv.org/abs/2506.20353)
*Xuan Ding,Rui Sun,Yunjian Zhang,Xiu Yan,Yueqi Zhou,Kaihao Huang,Suzhong Fu,Chuanlong Xie,Yao Zhu*

Main category: cs.LG

TL;DR: 提出了一种双级重要性保护机制（DipSVD），通过局部和全局重要性保护提升SVD压缩方法的性能。


<details>
  <summary>Details</summary>
Motivation: 现有SVD压缩方法忽视矩阵中关键组件的保护，导致压缩模型性能下降。

Method: 采用局部重要性保护（通道加权数据白化）和全局重要性保护（启发式或优化方法分配压缩负担）。

Result: DipSVD在多个基准测试中优于现有SVD压缩方法，尤其在高压缩比下表现优异。

Conclusion: 双级重要性保护机制有效提升了SVD压缩的性能，尤其在保护关键组件方面表现突出。

Abstract: The ever-increasing computational demands and deployment costs of large
language models (LLMs) have spurred numerous compressing methods. Compared to
quantization and unstructured pruning, SVD compression offers superior hardware
compatibility and theoretical guarantees. However, existing SVD-based methods
focus on the overall discrepancy between the original and compressed matrices
while overlooking the protection of critical components within the matrix,
which leads to inferior performance in the compressed models. This paper
proposes a dual-level importance protection mechanism to enhance SVD-based
compression methods: (1) local importance protection: preserving the most
critical singular vectors within each weight matrix through channel-weighted
data whitening; and (2) global importance protection: enabling less important
layers to bear a greater portion of the compression burden through either a
heuristic or optimization-based approach, thereby minimizing the impact of
compression on critical layers. Extensive experiments demonstrate that DipSVD
outperforms existing SVD-based compression approaches across multiple
benchmarks, achieving superior model performance especially at high model
compression ratios.

</details>


### [158] [A foundation model with multi-variate parallel attention to generate neuronal activity](https://arxiv.org/abs/2506.20354)
*Francesco Carzaniga,Michael Hersche,Abu Sebastian,Kaspar Schindler,Abbas Rahimi*

Main category: cs.LG

TL;DR: 论文提出了一种多变量并行注意力机制（MVPA），用于处理异构通道配置的多变量时间序列数据，并构建了MVPFormer模型，在iEEG数据上表现出色。


<details>
  <summary>Details</summary>
Motivation: 解决多变量时间序列数据中异构通道配置的挑战，特别是在临床iEEG领域，通道设置差异大。

Method: 引入MVPA机制，分离内容、时间和空间注意力，构建MVPFormer模型。

Result: MVPFormer在多个数据集上表现优异，达到专家级癫痫检测性能，并超越现有Transformer模型。

Conclusion: MVPA是一种通用的注意力机制，MVPFormer是首个开源、开放权重的iEEG基础模型，具有领先的临床性能。

Abstract: Learning from multi-variate time-series with heterogeneous channel
configurations remains a fundamental challenge for deep neural networks (DNNs),
particularly in clinical domains such as intracranial electroencephalography
(iEEG), where channel setups vary widely across subjects. In this work, we
introduce multi-variate parallel attention (MVPA), a novel self-attention
mechanism that disentangles content, temporal, and spatial attention, enabling
flexible, generalizable, and efficient modeling of time-series data with
varying channel counts and configurations. We use MVPA to build MVPFormer, a
generative foundation model for human electrophysiology, trained to predict the
evolution of iEEG signals across diverse subjects. To support this and future
effort by the community, we release the SWEC iEEG dataset, the largest publicly
available iEEG dataset to date, comprising nearly 10,000 hours of recordings
from heterogeneous clinical sources. MVPFormer leverages MVPA to achieve strong
generalization across subjects, demonstrating expert-level performance in
seizure detection and outperforming state-of-the-art Transformer baselines on
our SWEC, the MAYO, and the FNUSA dataset. We further validate MVPA on standard
time-series forecasting and classification tasks, where it matches or exceeds
existing attention-based models. Together, our contributions establish MVPA as
a general-purpose attention mechanism for heterogeneous time-series and
MVPFormer as the first open-source, open-weights, and open-data iEEG foundation
model with state-of-the-art clinical performance. The code is available at
https://github.com/IBM/multi-variate-parallel-transformer. The SWEC iEEG
dataset is available at
https://mb-neuro.medical-blocks.ch/public_access/databases/ieeg/swec_ieeg.

</details>


### [159] [Towards Interpretable and Efficient Feature Selection in Trajectory Datasets: A Taxonomic Approach](https://arxiv.org/abs/2506.20359)
*Chanuka Don Samarasinghage,Dhruv Gulabani*

Main category: cs.LG

TL;DR: 论文提出了一种基于分类的特征选择方法，用于解决轨迹分析中的高维特征爆炸问题，提升模型效率和可解释性。


<details>
  <summary>Details</summary>
Motivation: 轨迹分析中高维特征导致效率和可解释性降低，需要一种有效的方法减少特征维度。

Method: 采用基于分类的特征选择方法，将特征分为几何和运动学两类，进一步细分为曲率、凹陷、速度和加速度。

Result: 分类方法在预测性能上表现优异，显著减少特征选择时间，并提供了对数据集敏感性的洞察。

Conclusion: 基于分类的特征选择方法能降低维度、提高可解释性，并为轨迹数据集研究提供方法论框架。

Abstract: Trajectory analysis is not only about obtaining movement data, but it is also
of paramount importance in understanding the pattern in which an object moves
through space and time, as well as in predicting its next move. Due to the
significant interest in the area, data collection has improved substantially,
resulting in a large number of features becoming available for training and
predicting models. However, this introduces a high-dimensionality-induced
feature explosion problem, which reduces the efficiency and interpretability of
the data, thereby reducing the accuracy of machine learning models. To overcome
this issue, feature selection has become one of the most prevalent tools. Thus,
the objective of this paper was to introduce a taxonomy-based feature selection
method that categorizes features based on their internal structure. This
approach classifies the data into geometric and kinematic features, further
categorizing them into curvature, indentation, speed, and acceleration. The
comparative analysis indicated that a taxonomy-based approach consistently
achieved comparable or superior predictive performance. Furthermore, due to the
taxonomic grouping, which reduces combinatorial space, the time taken to select
features was drastically reduced. The taxonomy was also used to gain insights
into what feature sets each dataset was more sensitive to. Overall, this study
provides robust evidence that a taxonomy-based feature selection method can add
a layer of interpretability, reduce dimensionality and computational
complexity, and contribute to high-level decision-making. It serves as a step
toward providing a methodological framework for researchers and practitioners
dealing with trajectory datasets and contributing to the broader field of
explainable artificial intelligence.

</details>


### [160] [Self-Supervised Graph Learning via Spectral Bootstrapping and Laplacian-Based Augmentations](https://arxiv.org/abs/2506.20362)
*Lorenzo Bini,Stephane Marchand-Maillet*

Main category: cs.LG

TL;DR: LaplaceGNN是一种新颖的自监督图学习框架，通过谱引导技术避免负采样，利用拉普拉斯信号捕捉结构表示，无需对比目标或手工增强。


<details>
  <summary>Details</summary>
Motivation: 传统图学习方法依赖负采样或手工增强，效率低且复杂。LaplaceGNN旨在提供一种更简单、高效的自监督替代方案。

Method: 结合拉普拉斯信号，通过最大-最小中心性优化预计算谱增强，并采用对抗性引导训练方案增强特征学习和鲁棒性。

Result: 在多个基准数据集上，LaplaceGNN表现优于现有自监督图学习方法。

Conclusion: LaplaceGNN为高效学习表达性图表示提供了有前景的方向。

Abstract: We present LaplaceGNN, a novel self-supervised graph learning framework that
bypasses the need for negative sampling by leveraging spectral bootstrapping
techniques. Our method integrates Laplacian-based signals into the learning
process, allowing the model to effectively capture rich structural
representations without relying on contrastive objectives or handcrafted
augmentations. By focusing on positive alignment, LaplaceGNN achieves linear
scaling while offering a simpler, more efficient, self-supervised alternative
for graph neural networks, applicable across diverse domains. Our contributions
are twofold: we precompute spectral augmentations through max-min
centrality-guided optimization, enabling rich structural supervision without
relying on handcrafted augmentations, then we integrate an adversarial
bootstrapped training scheme that further strengthens feature learning and
robustness. Our extensive experiments on different benchmark datasets show that
LaplaceGNN achieves superior performance compared to state-of-the-art
self-supervised graph methods, offering a promising direction for efficiently
learning expressive graph representations.

</details>


### [161] [TESSERA: Temporal Embeddings of Surface Spectra for Earth Representation and Analysis](https://arxiv.org/abs/2506.20380)
*Zhengpeng Feng,Sadiq Jaffer,Jovana Knezevic,Silja Sormunen,Robin Young,Madeline Lisaius,Markus Immitzer,James Ball,Clement Atzberger,David A. Coomes,Anil Madhavapeddy,Andrew Blake,Srinivasan Keshav*

Main category: cs.LG

TL;DR: TESSERA是一种新型遥感基础模型，通过自监督学习从卫星时间序列数据生成全球10米尺度的稳健表示，结合光学和SAR数据，性能优于现有模型。


<details>
  <summary>Details</summary>
Motivation: 卫星遥感在地球观测应用中具有广泛用途，但现有模型在多样性和性能上存在局限，需要更高效的基础模型。

Method: TESSERA采用双Transformer编码器分别处理Sentinel-1 SAR和Sentinel-2 MSI数据，通过MLP融合生成全球表示图。

Result: TESSERA在五项任务中表现优于传统遥感模型和其他地理空间基础模型。

Conclusion: TESSERA为高分辨率遥感表示提供了开源解决方案，性能领先，推动了地球观测应用的民主化。

Abstract: Satellite remote sensing (RS) enables a wide array of downstream Earth
observation (EO) applications, including climate modeling, carbon accounting,
and strategies for conservation and sustainable land use. We present TESSERA, a
novel Remote Sensing Foundation Model (RSFM) that uses Self-Supervised Learning
(SSL) to generate global, robust representations at 10m scale from pixel-level
satellite time series data. TESSERA combines information from only optical and
SAR data streams using two parallel Transformer-based encoders: one dedicated
to Sentinel-1 SAR polarizations and another to Sentinel-2 MSI data (10 selected
spectral bands) to create representations that are then fused using a
multilayer perceptron (MLP), resulting in a global representation map covering
the years 2017 to 2024. Our precomputed representations set a new
state-of-the-art performance benchmark and our open-source approach
democratizes access to high-performance, high-resolution representations. We
benchmark the performance of TESSERA in five diverse tasks, comparing our work
with state-of-the-art task-specific models and other foundation models. Our
results show that TESSERA outperforms both traditional RS baselines and the
leading geospatial foundation models in these diverse downstream tasks.

</details>


### [162] [Client Clustering Meets Knowledge Sharing: Enhancing Privacy and Robustness in Personalized Peer-to-Peer Learning](https://arxiv.org/abs/2506.20413)
*Mohammad Mahdi Maheri,Denys Herasymuk,Hamed Haddadi*

Main category: cs.LG

TL;DR: P4是一种个性化的、隐私保护的、点对点的学习方法，用于资源受限的IoT设备，通过轻量级算法实现高效知识共享和抗攻击能力。


<details>
  <summary>Details</summary>
Motivation: AI在IoT中的广泛应用需要个性化学习方法，但现有方法在隐私保护、知识共享和抗攻击性方面存在挑战。

Method: P4采用轻量级、完全去中心化的算法，通过差分隐私知识蒸馏在协作组内共同训练模型。

Result: P4在基准测试中比现有方法准确率提高5%-30%，并能抵御高达30%的恶意客户端攻击，实际部署时仅增加约7秒开销。

Conclusion: P4为资源受限的IoT设备提供了一种高效、隐私保护且抗攻击的个性化学习解决方案。

Abstract: The growing adoption of Artificial Intelligence (AI) in Internet of Things
(IoT) ecosystems has intensified the need for personalized learning methods
that can operate efficiently and privately across heterogeneous,
resource-constrained devices. However, enabling effective personalized learning
in decentralized settings introduces several challenges, including efficient
knowledge transfer between clients, protection of data privacy, and resilience
against poisoning attacks. In this paper, we address these challenges by
developing P4 (Personalized, Private, Peer-to-Peer) -- a method designed to
deliver personalized models for resource-constrained IoT devices while ensuring
differential privacy and robustness against poisoning attacks. Our solution
employs a lightweight, fully decentralized algorithm to privately detect client
similarity and form collaborative groups. Within each group, clients leverage
differentially private knowledge distillation to co-train their models,
maintaining high accuracy while ensuring robustness to the presence of
malicious clients. We evaluate P4 on popular benchmark datasets using both
linear and CNN-based architectures across various heterogeneity settings and
attack scenarios. Experimental results show that P4 achieves 5% to 30% higher
accuracy than leading differentially private peer-to-peer approaches and
maintains robustness with up to 30% malicious clients. Additionally, we
demonstrate its practicality by deploying it on resource-constrained devices,
where collaborative training between two clients adds only ~7 seconds of
overhead.

</details>


### [163] [Off-Policy Evaluation and Learning for the Future under Non-Stationarity](https://arxiv.org/abs/2506.20417)
*Tatsuhiro Shimizu,Kazuki Kawamura,Takanori Muroi,Yusuke Narita,Kei Tateno,Takuma Udagawa,Yuta Saito*

Main category: cs.LG

TL;DR: 论文提出了一种名为OPFV的新估计器，用于在非平稳环境中准确估计和优化未来策略价值，利用时间序列数据中的结构信息，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 在非平稳环境中（如电子商务推荐系统），现有方法假设平稳性或依赖限制性奖励建模假设，导致显著偏差。需要一种新方法来准确估计和优化未来策略价值。

Method: 提出OPFV估计器，通过新型重要性加权利用时间序列数据中的结构信息（如季节性、周效应等），并扩展为策略梯度方法以优化未来策略。

Result: 理论分析表明OPFV在特定条件下具有低偏差，实证结果显示其在非平稳环境中显著优于现有方法。

Conclusion: OPFV为未来策略评估和学习提供了有效工具，尤其适用于非平稳环境，具有理论和实证优势。

Abstract: We study the novel problem of future off-policy evaluation (F-OPE) and
learning (F-OPL) for estimating and optimizing the future value of policies in
non-stationary environments, where distributions vary over time. In e-commerce
recommendations, for instance, our goal is often to estimate and optimize the
policy value for the upcoming month using data collected by an old policy in
the previous month. A critical challenge is that data related to the future
environment is not observed in the historical data. Existing methods assume
stationarity or depend on restrictive reward-modeling assumptions, leading to
significant bias. To address these limitations, we propose a novel estimator
named \textit{\textbf{O}ff-\textbf{P}olicy Estimator for the \textbf{F}uture
\textbf{V}alue (\textbf{\textit{OPFV}})}, designed for accurately estimating
policy values at any future time point. The key feature of OPFV is its ability
to leverage the useful structure within time-series data. While future data
might not be present in the historical log, we can leverage, for example,
seasonal, weekly, or holiday effects that are consistent in both the historical
and future data. Our estimator is the first to exploit these time-related
structures via a new type of importance weighting, enabling effective F-OPE.
Theoretical analysis identifies the conditions under which OPFV becomes
low-bias. In addition, we extend our estimator to develop a new policy-gradient
method to proactively learn a good future policy using only historical data.
Empirical results show that our methods substantially outperform existing
methods in estimating and optimizing the future policy value under
non-stationarity for various experimental setups.

</details>


### [164] [Tackling Data Heterogeneity in Federated Learning through Knowledge Distillation with Inequitable Aggregation](https://arxiv.org/abs/2506.20431)
*Xing Ma*

Main category: cs.LG

TL;DR: 该论文提出了一种名为KDIA的知识蒸馏策略，用于解决联邦学习中因客户端标签偏斜和数据量偏斜导致的性能下降问题，特别适用于大规模客户端中仅少数参与训练的场景。


<details>
  <summary>Details</summary>
Motivation: 联邦学习中客户端标签偏斜、数据量偏斜等异质性问题严重降低模型性能，现有方法大多忽略大规模客户端中仅少数参与训练的场景，而实验表明这一场景更具挑战性。

Method: 提出KDIA策略，学生模型为参与客户端的平均聚合，教师模型为基于参与间隔、参与次数和数据量比例的加权聚合。本地训练时进行自知识蒸馏，并利用服务器训练的生成器生成近似IID数据辅助训练。

Result: 在CIFAR-10/100/CINIC-10数据集及多种异质性设置下的实验表明，KDIA能以更少训练轮次实现更高准确率，且在严重异质性下改进更显著。

Conclusion: KDIA策略能有效利用所有客户端知识，提升联邦学习在异质性场景下的性能。

Abstract: Federated learning aims to train a global model in a distributed environment
that is close to the performance of centralized training. However, issues such
as client label skew, data quantity skew, and other heterogeneity problems
severely degrade the model's performance. Most existing methods overlook the
scenario where only a small portion of clients participate in training within a
large-scale client setting, whereas our experiments show that this scenario
presents a more challenging federated learning task. Therefore, we propose a
Knowledge Distillation with teacher-student Inequitable Aggregation (KDIA)
strategy tailored to address the federated learning setting mentioned above,
which can effectively leverage knowledge from all clients. In KDIA, the student
model is the average aggregation of the participating clients, while the
teacher model is formed by a weighted aggregation of all clients based on three
frequencies: participation intervals, participation counts, and data volume
proportions. During local training, self-knowledge distillation is performed.
Additionally, we utilize a generator trained on the server to generate
approximately independent and identically distributed (IID) data features
locally for auxiliary training. We conduct extensive experiments on the
CIFAR-10/100/CINIC-10 datasets and various heterogeneous settings to evaluate
KDIA. The results show that KDIA can achieve better accuracy with fewer rounds
of training, and the improvement is more significant under severe
heterogeneity.

</details>


### [165] [Méthode de quadrature pour les PINNs fondée théoriquement sur la hessienne des résiduels](https://arxiv.org/abs/2506.20441)
*Antoine Caradot,Rémi Emonet,Amaury Habrard,Abdel-Rahim Mezidi,Marc Sebban*

Main category: cs.LG

TL;DR: 提出了一种基于Hessian矩阵的积分方法，用于指导PINNs训练中采样点的选择。


<details>
  <summary>Details</summary>
Motivation: 改进PINNs中采样点的选择方法，以提高求解PDE的效率。

Method: 利用Hessian矩阵近似定积分，并以此指导采样点的自适应选择。

Result: 提出了一种新的积分方法，优化了PINNs的训练过程。

Conclusion: 该方法为PINNs的采样点选择提供了更有效的策略。

Abstract: Physics-informed Neural Networks (PINNs) have emerged as an efficient way to
learn surrogate neural solvers of PDEs by embedding the physical model in the
loss function and minimizing its residuals using automatic differentiation at
so-called collocation points. Originally uniformly sampled, the choice of the
latter has been the subject of recent advances leading to adaptive sampling
refinements. In this paper, we propose a new quadrature method for
approximating definite integrals based on the hessian of the considered
function, and that we leverage to guide the selection of the collocation points
during the training process of PINNs.

</details>


### [166] [Automatic Demonstration Selection for LLM-based Tabular Data Classification](https://arxiv.org/abs/2506.20451)
*Shuchu Han,Wolfgang Bruckner*

Main category: cs.LG

TL;DR: 提出了一种基于谱图理论的算法，自动选择适合的演示数量，用于表格数据分类的上下文学习。


<details>
  <summary>Details</summary>
Motivation: 解决在表格数据分类中如何确定上下文学习（ICL）所需演示数量的挑战。

Method: 结合表格数据分布、用户选择的提示模板和特定大语言模型（LLM），通过谱图理论构建相似性图，分析拉普拉斯矩阵的特征值来确定最小演示数量。

Result: 实验验证了该方法在多种数据集和LLM上优于传统的随机选择算法。

Conclusion: 该算法能有效确定演示数量，提升表格数据分类的性能。

Abstract: A fundamental question in applying In-Context Learning (ICL) for tabular data
classification is how to determine the ideal number of demonstrations in the
prompt. This work addresses this challenge by presenting an algorithm to
automatically select a reasonable number of required demonstrations. Our method
distinguishes itself by integrating not only the tabular data's distribution
but also the user's selected prompt template and the specific Large Language
Model (LLM) into its estimation. Rooted in Spectral Graph Theory, our proposed
algorithm defines a novel metric to quantify the similarities between different
demonstrations. We then construct a similarity graph and analyze the
eigenvalues of its Laplacian to derive the minimum number of demonstrations
capable of representing the data within the LLM's intrinsic representation
space. We validate the efficacy of our approach through experiments comparing
its performance against conventional random selection algorithms on diverse
datasets and LLMs.

</details>


### [167] [Collaborative Batch Size Optimization for Federated Learning](https://arxiv.org/abs/2506.20511)
*Arno Geimer,Karthick Panner Selvam,Beltran Fiz Pontiveros*

Main category: cs.LG

TL;DR: 通过贪婪随机搜索优化联邦学习中的本地批量大小，提升训练效率。


<details>
  <summary>Details</summary>
Motivation: 联邦学习中，参与者共享硬件但缺乏信息交换，可能导致训练配置不当，影响效率。

Method: 利用联邦学习的并行处理特性，采用贪婪随机搜索优化本地批量大小。

Result: 相比默认参数设置，该方法提高了收敛速度，且接近本地参数优化的效果。

Conclusion: 通过优化本地批量大小，可以有效提升联邦学习的训练效率。

Abstract: Federated Learning (FL) is a decentralized collaborative Machine Learning
framework for training models without collecting data in a centralized
location. It has seen application across various disciplines, from helping
medical diagnoses in hospitals to detecting fraud in financial transactions. In
this paper, we focus on improving the local training process through hardware
usage optimization. While participants in a federation might share the hardware
they are training on, since there is no information exchange between them,
their training process can be hindered by an improper training configuration.
Taking advantage of the parallel processing inherent to Federated Learning, we
use a greedy randomized search to optimize local batch sizes for the best
training settings across all participants. Our results show that against
default parameter settings, our method improves convergence speed while staying
nearly on par with the case where local parameters are optimized.

</details>


### [168] [WallStreetFeds: Client-Specific Tokens as Investment Vehicles in Federated Learning](https://arxiv.org/abs/2506.20518)
*Arno Geimer,Beltran Fiz Pontiveros,Radu State*

Main category: cs.LG

TL;DR: 本文提出了一种基于去中心化金融（DeFi）和自动做市商（AMM）的联邦学习激励机制框架，旨在解决现有奖励分配方案的局限性。


<details>
  <summary>Details</summary>
Motivation: 联邦学习（FL）在数据隐私和模型性能方面具有优势，但在盈利性FL中，奖励分配机制尚未充分研究。

Method: 提出了一种新颖的框架，利用DeFi平台和AMM，通过客户端特定代币作为投资工具，实现更灵活和可扩展的奖励分配。

Result: 该框架为参与者提供了更灵活的奖励分配系统，并为第三方投资联邦学习过程提供了机制。

Conclusion: 该研究填补了联邦学习中奖励分配机制的空白，为未来的研究和应用提供了新方向。

Abstract: Federated Learning (FL) is a collaborative machine learning paradigm which
allows participants to collectively train a model while training data remains
private. This paradigm is especially beneficial for sectors like finance, where
data privacy, security and model performance are paramount. FL has been
extensively studied in the years following its introduction, leading to, among
others, better performing collaboration techniques, ways to defend against
other clients trying to attack the model, and contribution assessment methods.
An important element in for-profit Federated Learning is the development of
incentive methods to determine the allocation and distribution of rewards for
participants. While numerous methods for allocation have been proposed and
thoroughly explored, distribution frameworks remain relatively understudied. In
this paper, we propose a novel framework which introduces client-specific
tokens as investment vehicles within the FL ecosystem. Our framework aims to
address the limitations of existing incentive schemes by leveraging a
decentralized finance (DeFi) platform and automated market makers (AMMs) to
create a more flexible and scalable reward distribution system for
participants, and a mechanism for third parties to invest in the federation
learning process.

</details>


### [169] [Industrial Energy Disaggregation with Digital Twin-generated Dataset and Efficient Data Augmentation](https://arxiv.org/abs/2506.20525)
*Christian Internò,Andrea Castellani,Sebastian Schmitt,Fabio Stella,Barbara Hammer*

Main category: cs.LG

TL;DR: 论文提出了一种合成工业数据集（SIDED）和数据增强方法（AMDA），以解决工业非侵入式负载监测（NILM）中数据稀缺和隐私问题，显著提升了复杂工业设备的能耗分解性能。


<details>
  <summary>Details</summary>
Motivation: 工业NILM面临高质量数据集稀缺和工业能耗模式复杂多变的问题，同时需解决数据隐私问题。

Method: 使用数字孪生模拟生成开源数据集SIDED，并提出AMDA方法，通过智能调整设备功率贡献来增强模型泛化能力。

Result: 实验显示，使用AMDA增强数据的模型在复杂工业设备能耗分解中表现优异，归一化分解误差为0.093，优于未增强（0.451）和随机增强（0.290）的模型。

Conclusion: SIDED和AMDA有效解决了工业NILM的数据稀缺问题，并显著提升了模型性能，为工业能耗监测提供了实用工具。

Abstract: Industrial Non-Intrusive Load Monitoring (NILM) is limited by the scarcity of
high-quality datasets and the complex variability of industrial energy
consumption patterns. To address data scarcity and privacy issues, we introduce
the Synthetic Industrial Dataset for Energy Disaggregation (SIDED), an
open-source dataset generated using Digital Twin simulations. SIDED includes
three types of industrial facilities across three different geographic
locations, capturing diverse appliance behaviors, weather conditions, and load
profiles. We also propose the Appliance-Modulated Data Augmentation (AMDA)
method, a computationally efficient technique that enhances NILM model
generalization by intelligently scaling appliance power contributions based on
their relative impact. We show in experiments that NILM models trained with
AMDA-augmented data significantly improve the disaggregation of energy
consumption of complex industrial appliances like combined heat and power
systems. Specifically, in our out-of-sample scenarios, models trained with AMDA
achieved a Normalized Disaggregation Error of 0.093, outperforming models
trained without data augmentation (0.451) and those trained with random data
augmentation (0.290). Data distribution analyses confirm that AMDA effectively
aligns training and test data distributions, enhancing model generalization.

</details>


### [170] [Physics-Informed Machine Learning Regulated by Finite Element Analysis for Simulation Acceleration of Laser Powder Bed Fusion](https://arxiv.org/abs/2506.20537)
*R. Sharma,M. Raissi,Y. B. Guo*

Main category: cs.LG

TL;DR: 提出了一种结合FEA和PINN的高效建模框架FEA-PINN，用于加速LPBF过程中的热场预测，同时保持FEA的精度。


<details>
  <summary>Details</summary>
Motivation: 传统数值方法（如FEA）计算成本高，难以高效模拟LPBF过程。

Method: 开发了动态材料更新策略，结合PINN模型和温度相关材料特性，并通过FEA校正减少误差漂移。

Result: FEA-PINN在保持FEA精度的同时显著降低了计算成本。

Conclusion: FEA-PINN框架为LPBF过程提供了一种高效且准确的模拟方法。

Abstract: Efficient simulation of Laser Powder Bed Fusion (LPBF) is crucial for process
prediction due to the lasting issue of high computation cost using traditional
numerical methods such as finite element analysis (FEA). This study presents an
efficient modeling framework termed FEA-Regulated Physics-Informed Neural
Network (FEA-PINN) to accelerate the thermal field prediction in a LPBF process
while maintaining the FEA accuracy. A novel dynamic material updating strategy
is developed to capture the dynamic phase change of powder-liquid-solid in the
PINN model. The PINN model incorporates temperature-dependent material
properties and phase change behavior using the apparent heat capacity method.
While the PINN model demonstrates high accuracy with a small training data and
enables generalization of new process parameters via transfer learning, it
faces the challenge of high computation cost in time-dependent problems due to
the residual accumulation. To overcome this issue, the FEA-PINN framework
integrates corrective FEA simulations during inference to enforce physical
consistency and reduce error drift. A comparative analysis shows that FEA-PINN
achieves equivalent accuracy to FEA while significantly reducing computational
cost. The framework has been validated using the benchmark FEA data and
demonstrated through single-track scanning in LPBF.

</details>


### [171] [Demonstration of effective UCB-based routing in skill-based queues on real-world data](https://arxiv.org/abs/2506.20543)
*Sanne van Kempen,Jaron Sanders,Fiona Sloothaak,Maarten G. Wolf*

Main category: cs.LG

TL;DR: 本文研究了基于技能的排队系统（如数据中心、云计算网络和服务系统）的最优控制，通过真实数据集案例验证了一种强化学习算法在客户路由中的高效性和适应性。


<details>
  <summary>Details</summary>
Motivation: 探索如何在实际环境中有效实施强化学习算法以优化客户路由，并提升系统性能。

Method: 采用真实数据集进行案例研究，引入新的启发式路由规则以减少延迟，并通过调参平衡多目标优化。

Result: 算法能高效学习并适应动态环境，优于静态基准策略，同时支持多目标优化（如收益最大化、负载均衡和等待时间减少）。

Conclusion: 该算法具有实际应用潜力，但需注意参数调优和估计误差的影响。

Abstract: This paper is about optimally controlling skill-based queueing systems such
as data centers, cloud computing networks, and service systems. By means of a
case study using a real-world data set, we investigate the practical
implementation of a recently developed reinforcement learning algorithm for
optimal customer routing. Our experiments show that the algorithm efficiently
learns and adapts to changing environments and outperforms static benchmark
policies, indicating its potential for live implementation. We also augment the
real-world applicability of this algorithm by introducing a new heuristic
routing rule to reduce delays. Moreover, we show that the algorithm can
optimize for multiple objectives: next to payoff maximization, secondary
objectives such as server load fairness and customer waiting time reduction can
be incorporated. Tuning parameters are used for balancing inherent performance
trade--offs. Lastly, we investigate the sensitivity to estimation errors and
parameter tuning, providing valuable insights for implementing adaptive routing
algorithms in complex real-world queueing systems.

</details>


### [172] [Benchmarking Unsupervised Strategies for Anomaly Detection in Multivariate Time Series](https://arxiv.org/abs/2506.20574)
*Laura Boggia,Rafael Teixeira de Lima,Bogdan Malaescu*

Main category: cs.LG

TL;DR: 本文研究了基于Transformer的多变量时间序列异常检测方法，重点分析了iTransformer架构的应用，并探讨了关键参数、异常标签提取、训练数据中异常的影响以及多种Transformer模型的比较。


<details>
  <summary>Details</summary>
Motivation: 多变量时间序列异常检测在多个领域至关重要，但由于异常的未知性和时间序列维度间的复杂依赖关系，准确检测异常具有挑战性。本文旨在探索Transformer方法在此问题上的应用。

Method: 研究了iTransformer架构在异常检测中的应用，分析了窗口大小、步长和模型维度等参数的影响，探讨了异常标签提取方法和评估指标，评估了训练数据中异常的影响及替代损失函数的有效性，并比较了多种Transformer模型。

Result: 通过实验验证了iTransformer在异常检测中的有效性，分析了参数选择对性能的影响，并提出了针对异常标签提取和评估的优化方法。

Conclusion: iTransformer在多变量时间序列异常检测中表现出色，关键参数和损失函数的选择对性能有显著影响，为未来研究提供了实用指导。

Abstract: Anomaly detection in multivariate time series is an important problem across
various fields such as healthcare, financial services, manufacturing or physics
detector monitoring. Accurately identifying when unexpected errors or faults
occur is essential, yet challenging, due to the unknown nature of anomalies and
the complex interdependencies between time series dimensions. In this paper, we
investigate transformer-based approaches for time series anomaly detection,
focusing on the recently proposed iTransformer architecture. Our contributions
are fourfold: (i) we explore the application of the iTransformer to time series
anomaly detection, and analyse the influence of key parameters such as window
size, step size, and model dimensions on performance; (ii) we examine methods
for extracting anomaly labels from multidimensional anomaly scores and discuss
appropriate evaluation metrics for such labels; (iii) we study the impact of
anomalous data present during training and assess the effectiveness of
alternative loss functions in mitigating their influence; and (iv) we present a
comprehensive comparison of several transformer-based models across a diverse
set of datasets for time series anomaly detection.

</details>


### [173] [Exploring Graph-Transformer Out-of-Distribution Generalization Abilities](https://arxiv.org/abs/2506.20575)
*Itay Niv,Neta Rabin*

Main category: cs.LG

TL;DR: 论文探讨了图神经网络在分布外（OOD）泛化中的表现，重点比较了图变换器（GT）与传统消息传递神经网络（MPNN）的性能，并提出了一种新的后训练分析方法。


<details>
  <summary>Details</summary>
Motivation: 现实场景中训练和测试数据分布通常不一致，当前方法假设分布相同，限制了模型的泛化能力。图变换器在分布内（ID）表现优异，但其OOD泛化能力尚未充分研究。

Method: 系统评估GT和混合GT-MPNN在OOD设置下的表现，并比较MPNN。提出一种新的后训练分析方法，分析ID和OOD数据的聚类结构。

Result: GT和混合GT-MPNN在OOD泛化中表现优于MPNN，无需专门域泛化算法。后训练分析方法提供了对泛化能力的深入理解。

Conclusion: 图变换器在真实世界图学习中具有潜力，为OOD泛化研究提供了新方向。

Abstract: Deep learning on graphs has shown remarkable success across numerous
applications, including social networks, bio-physics, traffic networks, and
recommendation systems. Regardless of their successes, current methods
frequently depend on the assumption that training and testing data share the
same distribution, a condition rarely met in real-world scenarios. While
graph-transformer (GT) backbones have recently outperformed traditional
message-passing neural networks (MPNNs) in multiple in-distribution (ID)
benchmarks, their effectiveness under distribution shifts remains largely
unexplored.
  In this work, we address the challenge of out-of-distribution (OOD)
generalization for graph neural networks, with a special focus on the impact of
backbone architecture. We systematically evaluate GT and hybrid backbones in
OOD settings and compare them to MPNNs. To do so, we adapt several leading
domain generalization (DG) algorithms to work with GTs and assess their
performance on a benchmark designed to test a variety of distribution shifts.
Our results reveal that GT and hybrid GT-MPNN backbones consistently
demonstrate stronger generalization ability compared to MPNNs, even without
specialized DG algorithms.
  Additionally, we propose a novel post-training analysis approach that
compares the clustering structure of the entire ID and OOD test datasets,
specifically examining domain alignment and class separation. Demonstrating its
model-agnostic design, this approach not only provided meaningful insights into
GT and MPNN backbones. It also shows promise for broader applicability to DG
problems beyond graph learning, offering a deeper perspective on generalization
abilities that goes beyond standard accuracy metrics. Together, our findings
highlight the promise of graph-transformers for robust, real-world graph
learning and set a new direction for future research in OOD generalization.

</details>


### [174] [The kernel of graph indices for vector search](https://arxiv.org/abs/2506.20584)
*Mariano Tepper,Ted Willke*

Main category: cs.LG

TL;DR: 论文提出了一种基于机器学习的图索引方法SVG，适用于度量和非度量向量空间，并引入了SVG-L0以限制图的出度。


<details>
  <summary>Details</summary>
Motivation: 传统图索引方法仅适用于欧几里得空间，无法满足度量和非度量向量空间的需求。

Method: 利用核方法建立图连接性，提出SVG和SVG-L0，后者引入ℓ0稀疏约束限制出度。

Result: SVG和SVG-L0在度量和非度量空间中具有正式导航保证，且SVG-L0具有自调优特性。

Conclusion: SVG为图索引提供了新视角，SVG-L0解决了传统启发式方法的局限性。

Abstract: The most popular graph indices for vector search use principles from
computational geometry to build the graph. Hence, their formal graph
navigability guarantees are only valid in Euclidean space. In this work, we
show that machine learning can be used to build graph indices for vector search
in metric and non-metric vector spaces (e.g., for inner product similarity).
From this novel perspective, we introduce the Support Vector Graph (SVG), a new
type of graph index that leverages kernel methods to establish the graph
connectivity and that comes with formal navigability guarantees valid in metric
and non-metric vector spaces. In addition, we interpret the most popular graph
indices, including HNSW and DiskANN, as particular specializations of SVG and
show that new indices can be derived from the principles behind this
specialization. Finally, we propose SVG-L0 that incorporates an $\ell_0$
sparsity constraint into the SVG kernel method to build graphs with a bounded
out-degree. This yields a principled way of implementing this practical
requirement, in contrast to the traditional heuristic of simply truncating the
out edges of each node. Additionally, we show that SVG-L0 has a self-tuning
property that avoids the heuristic of using a set of candidates to find the
out-edges of each node and that keeps its computational complexity in check.

</details>


### [175] [H-FEX: A Symbolic Learning Method for Hamiltonian Systems](https://arxiv.org/abs/2506.20607)
*Jasen Lai,Senwei Liang,Chunmei Wang*

Main category: cs.LG

TL;DR: H-FEX是一种符号学习方法，能有效学习复杂哈密顿系统，保持能量守恒。


<details>
  <summary>Details</summary>
Motivation: 现有数据驱动方法难以准确捕捉复杂哈密顿函数并保持能量守恒。

Method: 提出H-FEX方法，引入新型交互节点以有效捕捉复杂交互项。

Result: 实验表明H-FEX能准确恢复复杂系统的哈密顿函数，并长期保持能量守恒。

Conclusion: H-FEX是发现复杂动力系统闭式表达的有力框架。

Abstract: Hamiltonian systems describe a broad class of dynamical systems governed by
Hamiltonian functions, which encode the total energy and dictate the evolution
of the system. Data-driven approaches, such as symbolic regression and neural
network-based methods, provide a means to learn the governing equations of
dynamical systems directly from observational data of Hamiltonian systems.
However, these methods often struggle to accurately capture complex Hamiltonian
functions while preserving energy conservation. To overcome this limitation, we
propose the Finite Expression Method for learning Hamiltonian Systems (H-FEX),
a symbolic learning method that introduces novel interaction nodes designed to
capture intricate interaction terms effectively. Our experiments, including
those on highly stiff dynamical systems, demonstrate that H-FEX can recover
Hamiltonian functions of complex systems that accurately capture system
dynamics and preserve energy over long time horizons. These findings highlight
the potential of H-FEX as a powerful framework for discovering closed-form
expressions of complex dynamical systems.

</details>


### [176] [Lost in Retraining: Roaming the Parameter Space of Exponential Families Under Closed-Loop Learning](https://arxiv.org/abs/2506.20623)
*Fariba Jangjoo,Matteo Marsili,Yasser Roudi*

Main category: cs.LG

TL;DR: 闭环学习是通过模型自身生成的数据反复估计模型的过程。研究发现，最大似然估计会导致参数收敛到放大初始偏差的状态，但可以通过数据污染、最大后验估计或正则化避免。


<details>
  <summary>Details</summary>
Motivation: 研究闭环学习的动态特性，尤其是大神经网络可能主要依赖自身生成数据训练的情况。

Method: 对指数族模型推导参数动态方程，分析最大似然估计的性质。

Result: 最大似然估计使充分统计量具有鞅性质，导致参数收敛到吸收状态，放大初始偏差。但可通过数据污染、最大后验估计或正则化避免。

Conclusion: 闭环学习的动态行为受参数化影响，需注意初始偏差和数据污染的作用。

Abstract: Closed-loop learning is the process of repeatedly estimating a model from
data generated from the model itself. It is receiving great attention due to
the possibility that large neural network models may, in the future, be
primarily trained with data generated by artificial neural networks themselves.
We study this process for models that belong to exponential families, deriving
equations of motions that govern the dynamics of the parameters. We show that
maximum likelihood estimation of the parameters endows sufficient statistics
with the martingale property and that as a result the process converges to
absorbing states that amplify initial biases present in the data. However, we
show that this outcome may be prevented by polluting the data with an
infinitesimal fraction of data points generated from a fixed model, by relying
on maximum a posteriori estimation or by introducing regularisation.
Furthermore, we show that the asymptotic behavior of the dynamics is not
reparametrisation invariant.

</details>


### [177] [Efficient Federated Learning with Encrypted Data Sharing for Data-Heterogeneous Edge Devices](https://arxiv.org/abs/2506.20644)
*Hangyu Li,Hongyue Wu,Guodong Fan,Zhen Zhang,Shizhan Chen,Zhiyong Feng*

Main category: cs.LG

TL;DR: FedEDS是一种新的联邦学习方案，通过加密数据共享解决边缘设备上的网络拓扑、物理距离和数据异构性问题，提升模型性能和收敛速度。


<details>
  <summary>Details</summary>
Motivation: 当前研究忽视了网络拓扑、物理距离和数据异构性对边缘设备联邦学习的影响，导致延迟增加和模型性能下降。

Method: FedEDS利用客户端模型和随机层训练数据加密器，生成加密数据并与其他客户端共享，结合本地私有数据和共享加密数据训练模型。

Result: 实验证明FedEDS能有效提升模型性能并加速收敛。

Conclusion: FedEDS适用于需要快速收敛的边缘设备应用服务，解决了数据异构性和延迟问题。

Abstract: As privacy protection gains increasing importance, more models are being
trained on edge devices and subsequently merged into the central server through
Federated Learning (FL). However, current research overlooks the impact of
network topology, physical distance, and data heterogeneity on edge devices,
leading to issues such as increased latency and degraded model performance. To
address these issues, we propose a new federated learning scheme on edge
devices that called Federated Learning with Encrypted Data Sharing(FedEDS).
FedEDS uses the client model and the model's stochastic layer to train the data
encryptor. The data encryptor generates encrypted data and shares it with other
clients. The client uses the corresponding client's stochastic layer and
encrypted data to train and adjust the local model. FedEDS uses the client's
local private data and encrypted shared data from other clients to train the
model. This approach accelerates the convergence speed of federated learning
training and mitigates the negative impact of data heterogeneity, making it
suitable for application services deployed on edge devices requiring rapid
convergence. Experiments results show the efficacy of FedEDS in promoting model
performance.

</details>


### [178] [Mastering Multiple-Expert Routing: Realizable $H$-Consistency and Strong Guarantees for Learning to Defer](https://arxiv.org/abs/2506.20650)
*Anqi Mao,Mehryar Mohri,Yutao Zhong*

Main category: cs.LG

TL;DR: 本文提出新的代理损失函数和高效算法，解决了多专家学习中的延迟分配问题，并提供了理论保证。


<details>
  <summary>Details</summary>
Motivation: 解决在多专家系统中平衡准确性和计算成本的延迟分配问题，尤其是在自然语言生成、图像处理和医疗诊断等领域。

Method: 引入新的代理损失函数和算法，分析单阶段和双阶段学习场景的H-一致性和贝叶斯一致性。

Result: 实验验证了新方法的性能优于现有基线。

Conclusion: 提出的方法在多专家延迟学习中具有理论保证和实际应用价值。

Abstract: The problem of learning to defer with multiple experts consists of optimally
assigning input instances to experts, balancing the trade-off between their
accuracy and computational cost. This is a critical challenge in natural
language generation, but also in other fields such as image processing, and
medical diagnostics. Recent studies have proposed surrogate loss functions to
optimize deferral, but challenges remain in ensuring their consistency
properties. This paper introduces novel surrogate loss functions and efficient
algorithms with strong theoretical learning guarantees. We address open
questions regarding realizable $H$-consistency, $H$-consistency bounds, and
Bayes-consistency for both single-stage (jointly learning predictor and
deferral function) and two-stage (learning only the deferral function with a
fixed expert) learning scenarios. For single-stage deferral, we introduce a
family of new realizable $H$-consistent surrogate losses and further prove
$H$-consistency for a selected member. For two-stage deferral, we derive new
surrogate losses that achieve realizable $H$-consistency, $H$-consistency
bounds, and Bayes-consistency for the two-expert scenario and, under natural
assumptions, multiple-expert scenario. Additionally, we provide enhanced
theoretical guarantees under low-noise assumptions for both scenarios. Finally,
we report the results of experiments using our proposed surrogate losses,
comparing their performance against existing baselines.

</details>


### [179] [Hear No Evil: Detecting Gradient Leakage by Malicious Servers in Federated Learning](https://arxiv.org/abs/2506.20651)
*Fei Wang,Baochun Li*

Main category: cs.LG

TL;DR: 本文分析了联邦学习中恶意梯度泄漏攻击的局限性，提出了一种轻量级客户端检测机制。


<details>
  <summary>Details</summary>
Motivation: 研究恶意服务器如何通过操纵全局模型获取客户端敏感信息，并探讨其实际可行性。

Method: 分析攻击效果与隐蔽性的权衡，提出基于客户端的检测机制。

Result: 攻击在现实中效果有限且易被检测，检测机制能有效防御。

Conclusion: 恶意梯度泄漏攻击在实践中受限，轻量级防御机制可行。

Abstract: Recent work has shown that gradient updates in federated learning (FL) can
unintentionally reveal sensitive information about a client's local data. This
risk becomes significantly greater when a malicious server manipulates the
global model to provoke information-rich updates from clients. In this paper,
we adopt a defender's perspective to provide the first comprehensive analysis
of malicious gradient leakage attacks and the model manipulation techniques
that enable them. Our investigation reveals a core trade-off: these attacks
cannot be both highly effective in reconstructing private data and sufficiently
stealthy to evade detection -- especially in realistic FL settings that
incorporate common normalization techniques and federated averaging.
  Building on this insight, we argue that malicious gradient leakage attacks,
while theoretically concerning, are inherently limited in practice and often
detectable through basic monitoring. As a complementary contribution, we
propose a simple, lightweight, and broadly applicable client-side detection
mechanism that flags suspicious model updates before local training begins,
despite the fact that such detection may not be strictly necessary in realistic
FL settings. This mechanism further underscores the feasibility of defending
against these attacks with minimal overhead, offering a deployable safeguard
for privacy-conscious federated learning systems.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [180] [Learning Bilateral Team Formation in Cooperative Multi-Agent Reinforcement Learning](https://arxiv.org/abs/2506.20039)
*Koorosh Moslemi,Chi-Guhn Lee*

Main category: cs.MA

TL;DR: 论文提出了一种动态多智能体系统中双边团队形成的框架，填补了现有研究中双边分组在动态群体中影响的空白。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注单边分组、预定义团队或固定群体设置，而双边分组在动态群体中的影响尚未充分探索。

Method: 引入了一个学习双边团队形成的框架，分析了双边团队形成中影响策略性能和泛化的算法特性。

Result: 在广泛采用的多智能体场景中验证了方法的竞争力，并在大多数场景中表现出改进的泛化能力。

Conclusion: 该框架为动态多智能体系统中的双边团队形成提供了新的见解，并展示了其在实际应用中的潜力。

Abstract: Team formation and the dynamics of team-based learning have drawn significant
interest in the context of Multi-Agent Reinforcement Learning (MARL). However,
existing studies primarily focus on unilateral groupings, predefined teams, or
fixed-population settings, leaving the effects of algorithmic bilateral
grouping choices in dynamic populations underexplored. To address this gap, we
introduce a framework for learning two-sided team formation in dynamic
multi-agent systems. Through this study, we gain insight into what algorithmic
properties in bilateral team formation influence policy performance and
generalization. We validate our approach using widely adopted multi-agent
scenarios, demonstrating competitive performance and improved generalization in
most scenarios.

</details>


### [181] [A Visualization Framework for Exploring Multi-Agent-Based Simulations Case Study of an Electric Vehicle Home Charging Ecosystem](https://arxiv.org/abs/2506.20400)
*Kristoffer Christensen,Bo Nørregaard Jørgensen,Zheng Grace Ma*

Main category: cs.MA

TL;DR: 本文提出了一种基于Python的模块化仪表盘框架，用于分析和可视化多智能体模拟（MABS）中的电动汽车家庭充电生态系统数据，帮助快速识别和解释异常行为。


<details>
  <summary>Details</summary>
Motivation: 多智能体模拟生成的数据复杂且随机，难以通过静态后处理检测和解释系统级事件（如变压器过载或消费者不满）。

Method: 使用Dash by Plotly构建模块化仪表盘框架，提供三种协调视图（系统概览、系统分析和消费者分析），支持高分辨率可视化和根因分析。

Result: 案例研究表明，该框架能快速识别异常（如变压器过载和充电失败），并为研究人员和电网运营商提供可操作的见解。

Conclusion: 该框架不仅适用于电动汽车充电系统，还可扩展至其他分布式能源和复杂能源系统。

Abstract: Multi-agent-based simulations (MABS) of electric vehicle (EV) home charging
ecosystems generate large, complex, and stochastic time-series datasets that
capture interactions between households, grid infrastructure, and energy
markets. These interactions can lead to unexpected system-level events, such as
transformer overloads or consumer dissatisfaction, that are difficult to detect
and explain through static post-processing. This paper presents a modular,
Python-based dashboard framework, built using Dash by Plotly, that enables
efficient, multi-level exploration and root-cause analysis of emergent behavior
in MABS outputs. The system features three coordinated views (System Overview,
System Analysis, and Consumer Analysis), each offering high-resolution
visualizations such as time-series plots, spatial heatmaps, and agent-specific
drill-down tools. A case study simulating full EV adoption with smart charging
in a Danish residential network demonstrates how the dashboard supports rapid
identification and contextual explanation of anomalies, including clustered
transformer overloads and time-dependent charging failures. The framework
facilitates actionable insight generation for researchers and distribution
system operators, and its architecture is adaptable to other distributed energy
resources and complex energy systems.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [182] [Evolutionary Gait Reconfiguration in Damaged Legged Robots](https://arxiv.org/abs/2506.19968)
*Sahand Farghdani,Robin Chhabra*

Main category: cs.RO

TL;DR: 提出一种无需训练的快速损伤恢复算法，用于多足机器人腿部功能部分或完全丧失时的运动恢复。


<details>
  <summary>Details</summary>
Motivation: 多足机器人在复杂任务中易受腿部物理损伤影响，导致任务性能下降甚至任务失败。

Method: 首先生成新的步态序列稳定运动，随后通过差分进化算法优化重新配置步态，以最大化前进并减少身体旋转和横向漂移。

Result: 算法在24自由度六足机器人上成功恢复运动，耗时不到一小时，表现出高效性和对结构损伤的鲁棒性。

Conclusion: 该算法能快速高效地恢复受损多足机器人的运动能力，具有实际应用潜力。

Abstract: Multi-legged robots deployed in complex missions are susceptible to physical
damage in their legs, impairing task performance and potentially compromising
mission success. This letter presents a rapid, training-free damage recovery
algorithm for legged robots subject to partial or complete loss of functional
legs. The proposed method first stabilizes locomotion by generating a new gait
sequence and subsequently optimally reconfigures leg gaits via a developed
differential evolution algorithm to maximize forward progression while
minimizing body rotation and lateral drift. The algorithm successfully restores
locomotion in a 24-degree-of-freedom hexapod within one hour, demonstrating
both high efficiency and robustness to structural damage.

</details>


### [183] [Robust Embodied Self-Identification of Morphology in Damaged Multi-Legged Robots](https://arxiv.org/abs/2506.19984)
*Sahand Farghdani,Mili Patel,Robin Chhabra*

Main category: cs.RO

TL;DR: 提出了一种基于低成本IMU的自建模与损伤识别算法，用于多足机器人自主适应腿部损伤。


<details>
  <summary>Details</summary>
Motivation: 多足机器人在复杂任务中易受腿部损伤影响性能，需自主适应能力。

Method: 引入FFT滤波器处理时间不一致信号，通过比较机器人与模型的身体方向检测损伤，更新模型并集成到控制系统。

Result: 在崎岖地形实验中验证了算法的鲁棒性和计算效率。

Conclusion: 该方法能有效识别损伤并自主适应，提升多足机器人的可靠性。

Abstract: Multi-legged robots (MLRs) are vulnerable to leg damage during complex
missions, which can impair their performance. This paper presents a
self-modeling and damage identification algorithm that enables autonomous
adaptation to partial or complete leg loss using only data from a low-cost IMU.
A novel FFT-based filter is introduced to address time-inconsistent signals,
improving damage detection by comparing body orientation between the robot and
its model. The proposed method identifies damaged legs and updates the robot's
model for integration into its control system. Experiments on uneven terrain
validate its robustness and computational efficiency.

</details>


### [184] [Hierarchical Reinforcement Learning and Value Optimization for Challenging Quadruped Locomotion](https://arxiv.org/abs/2506.20036)
*Jeremiah Coholich,Muhammad Ali Murtaza,Seth Hutchinson,Zsolt Kira*

Main category: cs.RO

TL;DR: 提出了一种新颖的分层强化学习框架，用于四足机器人在复杂地形上的运动。高层策略选择目标，低层策略执行，通过在线优化提升性能。


<details>
  <summary>Details</summary>
Motivation: 解决四足机器人在复杂地形中运动的挑战，提高运动效率和安全性。

Method: 采用两层策略：高层策略选择目标，低层策略执行运动。低层策略通过演员-评论家算法训练，高层策略通过在线优化实现。

Result: 相比端到端强化学习，该框架在多种地形上表现更好，奖励更高且碰撞更少。

Conclusion: 分层强化学习框架显著提升了四足机器人在复杂地形中的运动能力。

Abstract: We propose a novel hierarchical reinforcement learning framework for
quadruped locomotion over challenging terrain. Our approach incorporates a
two-layer hierarchy in which a high-level policy (HLP) selects optimal goals
for a low-level policy (LLP). The LLP is trained using an on-policy
actor-critic RL algorithm and is given footstep placements as goals. We propose
an HLP that does not require any additional training or environment samples and
instead operates via an online optimization process over the learned value
function of the LLP. We demonstrate the benefits of this framework by comparing
it with an end-to-end reinforcement learning (RL) approach. We observe
improvements in its ability to achieve higher rewards with fewer collisions
across an array of different terrains, including terrains more difficult than
any encountered during training.

</details>


### [185] [Consensus-Driven Uncertainty for Robotic Grasping based on RGB Perception](https://arxiv.org/abs/2506.20045)
*Eric C. Joyce,Qianwen Zhao,Nathaniel Burgdorfer,Long Wang,Philippos Mordohai*

Main category: cs.RO

TL;DR: 提出一种轻量级深度网络方法，用于预测基于图像姿态估计的抓取是否成功，通过真实图像和模拟抓取生成训练数据。


<details>
  <summary>Details</summary>
Motivation: 解决深度物体姿态估计器过度自信的问题，避免高不确定性下的任务失败。

Method: 训练轻量级深度网络，结合真实图像姿态估计和模拟抓取生成训练数据。

Result: 网络能从多样化的物体数据中受益，共同训练提升性能。

Conclusion: 多样化物体数据有助于提升抓取成功率预测的准确性。

Abstract: Deep object pose estimators are notoriously overconfident. A grasping agent
that both estimates the 6-DoF pose of a target object and predicts the
uncertainty of its own estimate could avoid task failure by choosing not to act
under high uncertainty. Even though object pose estimation improves and
uncertainty quantification research continues to make strides, few studies have
connected them to the downstream task of robotic grasping. We propose a method
for training lightweight, deep networks to predict whether a grasp guided by an
image-based pose estimate will succeed before that grasp is attempted. We
generate training data for our networks via object pose estimation on real
images and simulated grasping. We also find that, despite high object
variability in grasping trials, networks benefit from training on all objects
jointly, suggesting that a diverse variety of objects can nevertheless
contribute to the same goal.

</details>


### [186] [Robust Robotic Exploration and Mapping Using Generative Occupancy Map Synthesis](https://arxiv.org/abs/2506.20049)
*Lorin Achey,Alec Reed,Brendan Crowe,Bradley Hayes,Christoffer Heckman*

Main category: cs.RO

TL;DR: 提出了一种基于生成式占用映射的机器人探索新方法SceneSense，通过扩散模型预测3D占用图，实时融合预测结果，显著提升地图质量和可通行性。实验验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 提升机器人探索中的地图质量和可通行性，解决部分观测下的地图构建问题。

Method: 使用扩散模型SceneSense预测3D占用图，并实时融合预测结果到运行中的占用图中。

Result: 实验显示，SceneSense显著提升地图质量（FID改进24.44%近机器人，75.59%远距离），并提高探索稳健性和可通行时间。

Conclusion: SceneSense增强的地图在机器人探索中表现优于仅依赖传感器测量的地图，提供更一致的探索结果。

Abstract: We present a novel approach for enhancing robotic exploration by using
generative occupancy mapping. We introduce SceneSense, a diffusion model
designed and trained for predicting 3D occupancy maps given partial
observations. Our proposed approach probabilistically fuses these predictions
into a running occupancy map in real-time, resulting in significant
improvements in map quality and traversability. We implement SceneSense onboard
a quadruped robot and validate its performance with real-world experiments to
demonstrate the effectiveness of the model. In these experiments, we show that
occupancy maps enhanced with SceneSense predictions better represent our fully
observed ground truth data (24.44% FID improvement around the robot and 75.59%
improvement at range). We additionally show that integrating
SceneSense-enhanced maps into our robotic exploration stack as a "drop-in" map
improvement, utilizing an existing off-the-shelf planner, results in
improvements in robustness and traversability time. Finally we show results of
full exploration evaluations with our proposed system in two dissimilar
environments and find that locally enhanced maps provide more consistent
exploration results than maps constructed only from direct sensor measurements.

</details>


### [187] [PSALM-V: Automating Symbolic Planning in Interactive Visual Environments with Large Language Models](https://arxiv.org/abs/2506.20097)
*Wang Bill Zhu,Miaosen Chai,Ishika Singh,Robin Jia,Jesse Thomason*

Main category: cs.RO

TL;DR: PSALM-V是一种自主神经符号学习系统，通过交互在视觉环境中推断符号动作语义，无需专家定义，显著提升了规划成功率。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖文本领域或不现实的假设，PSALM-V旨在动态推断PDDL问题文件和动作语义，适应部分可观测和多智能体环境。

Method: 系统利用LLM生成启发式计划和候选语义，通过分析执行结果动态推断PDDL文件，迭代优化动作语义信念树直至达成目标。

Result: 在ALFRED任务中，规划成功率从37%提升至74%；在RTFM和Overcooked-AI中提高了步骤效率，并在多智能体设置中成功诱导领域语义。

Conclusion: PSALM-V在视觉和多智能体环境中有效推断符号语义，为真实世界机器人任务提供了可行解决方案。

Abstract: We propose PSALM-V, the first autonomous neuro-symbolic learning system able
to induce symbolic action semantics (i.e., pre- and post-conditions) in visual
environments through interaction. PSALM-V bootstraps reliable symbolic planning
without expert action definitions, using LLMs to generate heuristic plans and
candidate symbolic semantics. Previous work has explored using large language
models to generate action semantics for Planning Domain Definition Language
(PDDL)-based symbolic planners. However, these approaches have primarily
focused on text-based domains or relied on unrealistic assumptions, such as
access to a predefined problem file, full observability, or explicit error
messages. By contrast, PSALM-V dynamically infers PDDL problem files and domain
action semantics by analyzing execution outcomes and synthesizing possible
error explanations. The system iteratively generates and executes plans while
maintaining a tree-structured belief over possible action semantics for each
action, iteratively refining these beliefs until a goal state is reached.
Simulated experiments of task completion in ALFRED demonstrate that PSALM-V
increases the plan success rate from 37% (Claude-3.7) to 74% in partially
observed setups. Results on two 2D game environments, RTFM and Overcooked-AI,
show that PSALM-V improves step efficiency and succeeds in domain induction in
multi-agent settings. PSALM-V correctly induces PDDL pre- and post-conditions
for real-world robot BlocksWorld tasks, despite low-level manipulation failures
from the robot.

</details>


### [188] [Personalized Mental State Evaluation in Human-Robot Interaction using Federated Learning](https://arxiv.org/abs/2506.20212)
*Andrea Bussolan,Oliver Avram,Andrea Pignata,Gianvito Urgese,Stefano Baraldo,Anna Valente*

Main category: cs.RO

TL;DR: 本文提出了一种基于联邦学习的框架，用于工业5.0中的人机协作，通过多模态生理信号预测操作员压力水平，实现实时机器人适应，同时保护用户隐私。


<details>
  <summary>Details</summary>
Motivation: 工业5.0强调工人福祉与大规模定制，需要机器人根据人类心理状态调整行为以提高协作流畅性和安全性。

Method: 采用联邦学习（FL）框架，利用EEG、ECG、EDA、EMG和呼吸等多模态生理信号训练模型，预测操作员压力水平，并实现分布式设备端训练。

Result: FL方法的性能与集中式训练相当，同时提升了模型的个性化能力，优化了人机交互。

Conclusion: 该框架为智能制造业提供了隐私保护的自适应机器人解决方案，提升了工人福祉。

Abstract: With the advent of Industry 5.0, manufacturers are increasingly prioritizing
worker well-being alongside mass customization. Stress-aware Human-Robot
Collaboration (HRC) plays a crucial role in this paradigm, where robots must
adapt their behavior to human mental states to improve collaboration fluency
and safety. This paper presents a novel framework that integrates Federated
Learning (FL) to enable personalized mental state evaluation while preserving
user privacy. By leveraging physiological signals, including EEG, ECG, EDA,
EMG, and respiration, a multimodal model predicts an operator's stress level,
facilitating real-time robot adaptation. The FL-based approach allows
distributed on-device training, ensuring data confidentiality while improving
model generalization and individual customization. Results demonstrate that the
deployment of an FL approach results in a global model with performance in
stress prediction accuracy comparable to a centralized training approach.
Moreover, FL allows for enhancing personalization, thereby optimizing
human-robot interaction in industrial settings, while preserving data privacy.
The proposed framework advances privacy-preserving, adaptive robotics to
enhance workforce well-being in smart manufacturing.

</details>


### [189] [Generating and Customizing Robotic Arm Trajectories using Neural Networks](https://arxiv.org/abs/2506.20259)
*Andrej Lúčny,Matilde Antonj,Carlo Mazzola,Hana Hornáčková,Igor Farkaš*

Main category: cs.RO

TL;DR: 提出了一种神经网络方法，用于生成和定制机械臂轨迹，确保精度和可重复性。


<details>
  <summary>Details</summary>
Motivation: 提高机械臂在认知机器人实验中的可预测性，特别是在与人类交互时。

Method: 通过神经网络计算机械臂的正向运动学，并结合关节角度生成器，训练神经网络生成精确轨迹。

Result: 实验表明，该方法能生成形状可定制且适应不同场景的精确轨迹。

Conclusion: 该方法具有广泛适用性，能有效提升机械臂动作的精度和可预测性。

Abstract: We introduce a neural network approach for generating and customizing the
trajectory of a robotic arm, that guarantees precision and repeatability. To
highlight the potential of this novel method, we describe the design and
implementation of the technique and show its application in an experimental
setting of cognitive robotics. In this scenario, the NICO robot was
characterized by the ability to point to specific points in space with precise
linear movements, increasing the predictability of the robotic action during
its interaction with humans. To achieve this goal, the neural network computes
the forward kinematics of the robot arm. By integrating it with a generator of
joint angles, another neural network was developed and trained on an artificial
dataset created from suitable start and end poses of the robotic arm. Through
the computation of angular velocities, the robot was characterized by its
ability to perform the movement, and the quality of its action was evaluated in
terms of shape and accuracy. Thanks to its broad applicability, our approach
successfully generates precise trajectories that could be customized in their
shape and adapted to different settings.

</details>


### [190] [Why Robots Are Bad at Detecting Their Mistakes: Limitations of Miscommunication Detection in Human-Robot Dialogue](https://arxiv.org/abs/2506.20268)
*Ruben Janssens,Jens De Bock,Sofie Labat,Eva Verhelst,Veronique Hoste,Tony Belpaeme*

Main category: cs.RO

TL;DR: 研究评估机器学习模型在检测人机对话中的沟通错误时的表现，发现即使使用先进模型，识别效果仅略优于随机猜测，且用户反馈不足是主要限制。


<details>
  <summary>Details</summary>
Motivation: 人机交互中检测沟通错误对维持用户参与和信任至关重要，但机器人难以像人类一样通过非语言反馈识别错误。

Method: 使用包含240段人机对话的多模态数据集，引入四种对话失败类型，评估计算机视觉模型的性能，并与人类评估者对比。

Result: 模型在识别沟通错误时表现不佳，仅略优于随机猜测；在情感表达更丰富的数据集上表现较好。人类评估者也仅能识别约一半的错误。

Conclusion: 研究揭示了识别机器人沟通错误的根本限制：用户即使感知到错误，也常未向机器人反馈。这有助于优化人机对话设计，明确需要反馈的场景。

Abstract: Detecting miscommunication in human-robot interaction is a critical function
for maintaining user engagement and trust. While humans effortlessly detect
communication errors in conversations through both verbal and non-verbal cues,
robots face significant challenges in interpreting non-verbal feedback, despite
advances in computer vision for recognizing affective expressions. This
research evaluates the effectiveness of machine learning models in detecting
miscommunications in robot dialogue. Using a multi-modal dataset of 240
human-robot conversations, where four distinct types of conversational failures
were systematically introduced, we assess the performance of state-of-the-art
computer vision models. After each conversational turn, users provided feedback
on whether they perceived an error, enabling an analysis of the models' ability
to accurately detect robot mistakes. Despite using state-of-the-art models, the
performance barely exceeds random chance in identifying miscommunication, while
on a dataset with more expressive emotional content, they successfully
identified confused states. To explore the underlying cause, we asked human
raters to do the same. They could also only identify around half of the induced
miscommunications, similarly to our model. These results uncover a fundamental
limitation in identifying robot miscommunications in dialogue: even when users
perceive the induced miscommunication as such, they often do not communicate
this to their robotic conversation partner. This knowledge can shape
expectations of the performance of computer vision models and can help
researchers to design better human-robot conversations by deliberately
eliciting feedback where needed.

</details>


### [191] [Real-Time Obstacle Avoidance Algorithms for Unmanned Aerial and Ground Vehicles](https://arxiv.org/abs/2506.20311)
*Jingwen Wei*

Main category: cs.RO

TL;DR: 该论文探讨了无人机在复杂3D环境中的实时安全导航方法，特别是在森林火灾等灾害救援中的应用，提出了2D和3D导航策略，并整合了无人机与地面无人车的协同控制。


<details>
  <summary>Details</summary>
Motivation: 无人机在灾害救援中的应用尚未充分探索，尤其是在自主导航方面。研究旨在通过改进导航算法，提升救援效率和安全性。

Method: 分阶段开发：首先研究2D融合导航策略，随后提出3D反应式导航策略，最后整合无人机与地面无人车的协同控制。

Result: 通过数学和仿真验证了提出的控制模型，为无人机在自然灾害救援中的实际应用提供了支持。

Conclusion: 研究为无人机在灾害救援中的角色改进提供了实用价值和学术见解。

Abstract: The growing use of mobile robots in sectors such as automotive, agriculture,
and rescue operations reflects progress in robotics and autonomy. In unmanned
aerial vehicles (UAVs), most research emphasizes visual SLAM, sensor fusion,
and path planning. However, applying UAVs to search and rescue missions in
disaster zones remains underexplored, especially for autonomous navigation.
  This report develops methods for real-time and secure UAV maneuvering in
complex 3D environments, crucial during forest fires. Building upon past
research, it focuses on designing navigation algorithms for unfamiliar and
hazardous environments, aiming to improve rescue efficiency and safety through
UAV-based early warning and rapid response.
  The work unfolds in phases. First, a 2D fusion navigation strategy is
explored, initially for mobile robots, enabling safe movement in dynamic
settings. This sets the stage for advanced features such as adaptive obstacle
handling and decision-making enhancements. Next, a novel 3D reactive navigation
strategy is introduced for collision-free movement in forest fire simulations,
addressing the unique challenges of UAV operations in such scenarios.
  Finally, the report proposes a unified control approach that integrates UAVs
and unmanned ground vehicles (UGVs) for coordinated rescue missions in forest
environments. Each phase presents challenges, proposes control models, and
validates them with mathematical and simulation-based evidence. The study
offers practical value and academic insights for improving the role of UAVs in
natural disaster rescue operations.

</details>


### [192] [Near Time-Optimal Hybrid Motion Planning for Timber Cranes](https://arxiv.org/abs/2506.20314)
*Marc-Philip Ecker,Bernhard Bischof,Minh Nhat Vu,Christoph Fröhlich,Tobias Glück,Wolfgang Kemmetmüller*

Main category: cs.RO

TL;DR: 本文提出了一种针对液压驱动木材起重机的新型混合运动规划方法，结合全局和局部规划器，优化时间效率和碰撞避免。


<details>
  <summary>Details</summary>
Motivation: 大型机械臂（如木材起重机）的运动规划面临液压驱动约束和被动关节等独特挑战，现有方法难以解决。

Method: 改进了基于路径点的随机轨迹优化（VP-STO）算法，加入泵流量约束和新型碰撞成本公式，并与梯度局部规划器结合。

Result: 增强的VP-STO作为全局规划器优于RRT*算法，结合局部规划器实现了高效碰撞避免和摆动抑制。

Conclusion: 该方法为液压驱动机械臂提供了高效、鲁棒的混合运动规划解决方案。

Abstract: Efficient, collision-free motion planning is essential for automating
large-scale manipulators like timber cranes. They come with unique challenges
such as hydraulic actuation constraints and passive joints-factors that are
seldom addressed by current motion planning methods. This paper introduces a
novel approach for time-optimal, collision-free hybrid motion planning for a
hydraulically actuated timber crane with passive joints. We enhance the
via-point-based stochastic trajectory optimization (VP-STO) algorithm to
include pump flow rate constraints and develop a novel collision cost
formulation to improve robustness. The effectiveness of the enhanced VP-STO as
an optimal single-query global planner is validated by comparison with an
informed RRT* algorithm using a time-optimal path parameterization (TOPP). The
overall hybrid motion planning is formed by combination with a gradient-based
local planner that is designed to follow the global planner's reference and to
systematically consider the passive joint dynamics for both collision avoidance
and sway damping.

</details>


### [193] [Building Forest Inventories with Autonomous Legged Robots -- System, Lessons, and Challenges Ahead](https://arxiv.org/abs/2506.20315)
*Matías Mattamala,Nived Chebrolu,Jonas Frey,Leonard Freißmuth,Haedam Oh,Benoit Casseau,Marco Hutter,Maurice Fallon*

Main category: cs.RO

TL;DR: 本文介绍了一种用于森林自主测绘的四足机器人系统，展示了其在1公顷森林中30分钟内完成测绘的能力，并提出了未来研究的五个挑战。


<details>
  <summary>Details</summary>
Motivation: 现代四足机器人的鲁棒性和移动性激发了在非结构化森林环境中进行自主测绘的需求。

Method: 提出了一种完整的导航系统架构，包括状态估计、任务规划、树木检测和特征估计。

Result: ANYmal机器人在三个欧洲国家的森林中测试，30分钟内完成1公顷测绘，树木直径测量精度达2厘米。

Conclusion: 总结了硬件成熟度、状态估计限制等五个挑战，为未来四足机器人和自然环境中导航系统的研究提供了方向。

Abstract: Legged robots are increasingly being adopted in industries such as oil, gas,
mining, nuclear, and agriculture. However, new challenges exist when moving
into natural, less-structured environments, such as forestry applications. This
paper presents a prototype system for autonomous, under-canopy forest inventory
with legged platforms. Motivated by the robustness and mobility of modern
legged robots, we introduce a system architecture which enabled a quadruped
platform to autonomously navigate and map forest plots. Our solution involves a
complete navigation stack for state estimation, mission planning, and tree
detection and trait estimation. We report the performance of the system from
trials executed over one and a half years in forests in three European
countries. Our results with the ANYmal robot demonstrate that we can survey
plots up to 1 ha plot under 30 min, while also identifying trees with typical
DBH accuracy of 2cm. The findings of this project are presented as five lessons
and challenges. Particularly, we discuss the maturity of hardware development,
state estimation limitations, open problems in forest navigation, future
avenues for robotic forest inventory, and more general challenges to assess
autonomous systems. By sharing these lessons and challenges, we offer insight
and new directions for future research on legged robots, navigation systems,
and applications in natural environments. Additional videos can be found in
https://dynamic.robots.ox.ac.uk/projects/legged-robots

</details>


### [194] [Finding the Easy Way Through -- the Probabilistic Gap Planner for Social Robot Navigation](https://arxiv.org/abs/2506.20320)
*Malte Probst,Raphael Wenzel,Tim Puphal,Monica Dasi,Nico A. Steinhardt,Sango Matsuzaki,Misa Komuro*

Main category: cs.RO

TL;DR: 论文提出了一种分解轨迹规划的方法，包括冲突避免和合作碰撞避免（CCA），并介绍了概率间隙规划器（PGP）来优化宏观轨迹。


<details>
  <summary>Details</summary>
Motivation: 现有规划器仅关注短期交互，难以处理需要长期策略的场景（如人群中的间隙选择）。

Method: 将轨迹规划分解为冲突避免（宏观）和CCA（微观），提出PGP作为冲突避免规划器。

Result: 在模拟中，PGP结合CCA提高了性能（更多空间、更少紧张和碰撞），但路径略长。

Conclusion: PGP在实时运行中有效，适用于复杂人群环境。

Abstract: In Social Robot Navigation, autonomous agents need to resolve many sequential
interactions with other agents. State-of-the art planners can efficiently
resolve the next, imminent interaction cooperatively and do not focus on longer
planning horizons. This makes it hard to maneuver scenarios where the agent
needs to select a good strategy to find gaps or channels in the crowd. We
propose to decompose trajectory planning into two separate steps: Conflict
avoidance for finding good, macroscopic trajectories, and cooperative collision
avoidance (CCA) for resolving the next interaction optimally. We propose the
Probabilistic Gap Planner (PGP) as a conflict avoidance planner. PGP modifies
an established probabilistic collision risk model to include a general
assumption of cooperativity. PGP biases the short-term CCA planner to head
towards gaps in the crowd. In extensive simulations with crowds of varying
density, we show that using PGP in addition to state-of-the-art CCA planners
improves the agents' performance: On average, agents keep more space to others,
create less tension, and cause fewer collisions. This typically comes at the
expense of slightly longer paths. PGP runs in real-time on WaPOCHI mobile robot
by Honda R&D.

</details>


### [195] [PIMBS: Efficient Body Schema Learning for Musculoskeletal Humanoids with Physics-Informed Neural Networks](https://arxiv.org/abs/2506.20343)
*Kento Kawaharazuka,Takahiro Hattori,Keita Yoneda,Kei Okada*

Main category: cs.RO

TL;DR: 提出了一种基于物理信息神经网络（PINN）的方法，用于学习肌肉骨骼人形机器人的身体模式，解决了数据量少时的学习难题。


<details>
  <summary>Details</summary>
Motivation: 肌肉骨骼人形机器人结构复杂，肌肉路径与几何模型偏差大，传统方法依赖大量数据且耗时。

Method: 结合实际机器人数据和物理规律（扭矩与肌肉张力关系），利用PINN进行高效学习。

Result: 在仿真和实际机器人中验证了方法的有效性和特点。

Conclusion: 该方法在数据有限情况下仍能实现高精度学习，具有实际应用潜力。

Abstract: Musculoskeletal humanoids are robots that closely mimic the human
musculoskeletal system, offering various advantages such as variable stiffness
control, redundancy, and flexibility. However, their body structure is complex,
and muscle paths often significantly deviate from geometric models. To address
this, numerous studies have been conducted to learn body schema, particularly
the relationships among joint angles, muscle tension, and muscle length. These
studies typically rely solely on data collected from the actual robot, but this
data collection process is labor-intensive, and learning becomes difficult when
the amount of data is limited. Therefore, in this study, we propose a method
that applies the concept of Physics-Informed Neural Networks (PINNs) to the
learning of body schema in musculoskeletal humanoids, enabling high-accuracy
learning even with a small amount of data. By utilizing not only data obtained
from the actual robot but also the physical laws governing the relationship
between torque and muscle tension under the assumption of correct joint
structure, more efficient learning becomes possible. We apply the proposed
method to both simulation and an actual musculoskeletal humanoid and discuss
its effectiveness and characteristics.

</details>


### [196] [CARMA: Context-Aware Situational Grounding of Human-Robot Group Interactions by Combining Vision-Language Models with Object and Action Recognition](https://arxiv.org/abs/2506.20373)
*Joerg Deigmoeller,Stephan Hasler,Nakul Agarwal,Daniel Tanneberg,Anna Belardinelli,Reza Ghoddoosian,Chao Wang,Felix Ocker,Fan Zhang,Behzad Dariush,Michael Gienger*

Main category: cs.RO

TL;DR: CARMA是一个用于人机群体交互中情境感知的系统，通过唯一标识实体并组织成行动者-对象-动作三元组，实现协作中的情境理解。


<details>
  <summary>Details</summary>
Motivation: 在群体交互中，机器人需要情境感知以有效协作，这要求对人物、对象及其交互进行一致表示和跟踪。

Method: CARMA通过唯一标识物理实体并将其组织为行动者-对象-动作三元组，实现情境感知。

Result: 实验表明，CARMA能可靠生成准确的三元组，支持时空推理和协作决策。

Conclusion: CARMA为协作环境中的情境感知和决策提供了结构化且鲁棒的基础。

Abstract: We introduce CARMA, a system for situational grounding in human-robot group
interactions. Effective collaboration in such group settings requires
situational awareness based on a consistent representation of present persons
and objects coupled with an episodic abstraction of events regarding actors and
manipulated objects. This calls for a clear and consistent assignment of
instances, ensuring that robots correctly recognize and track actors, objects,
and their interactions over time. To achieve this, CARMA uniquely identifies
physical instances of such entities in the real world and organizes them into
grounded triplets of actors, objects, and actions.
  To validate our approach, we conducted three experiments, where multiple
humans and a robot interact: collaborative pouring, handovers, and sorting.
These scenarios allow the assessment of the system's capabilities as to role
distinction, multi-actor awareness, and consistent instance identification. Our
experiments demonstrate that the system can reliably generate accurate
actor-action-object triplets, providing a structured and robust foundation for
applications requiring spatiotemporal reasoning and situated decision-making in
collaborative settings.

</details>


### [197] [Enhanced Robotic Navigation in Deformable Environments using Learning from Demonstration and Dynamic Modulation](https://arxiv.org/abs/2506.20376)
*Lingyun Chen,Xinrui Zhao,Marcos P. S. Campanha,Alexander Wegener,Abdeldjallil Naceri,Abdalla Swikir,Sami Haddadin*

Main category: cs.RO

TL;DR: 提出了一种结合学习演示（LfD）和动态系统（DS）的机器人导航新方法，用于在包含可变形障碍物的环境中实现自适应导航。


<details>
  <summary>Details</summary>
Motivation: 解决在包含软硬区域混合障碍物的复杂环境中，机器人导航的适应性和效率问题。

Method: 在DS框架中引入动态调制矩阵，实时区分可穿越的软区域和不可穿越的硬区域，确保安全灵活的轨迹规划。

Result: 通过仿真和机器人实验验证了方法在可变形环境中的导航能力，并实现了对轨迹和速度的控制。

Conclusion: 该方法能够动态适应障碍物，保持原始DS轨迹的同时实现平滑可靠的导航。

Abstract: This paper presents a novel approach for robot navigation in environments
containing deformable obstacles. By integrating Learning from Demonstration
(LfD) with Dynamical Systems (DS), we enable adaptive and efficient navigation
in complex environments where obstacles consist of both soft and hard regions.
We introduce a dynamic modulation matrix within the DS framework, allowing the
system to distinguish between traversable soft regions and impassable hard
areas in real-time, ensuring safe and flexible trajectory planning. We validate
our method through extensive simulations and robot experiments, demonstrating
its ability to navigate deformable environments. Additionally, the approach
provides control over both trajectory and velocity when interacting with
deformable objects, including at intersections, while maintaining adherence to
the original DS trajectory and dynamically adapting to obstacles for smooth and
reliable navigation.

</details>


### [198] [SPARK: Graph-Based Online Semantic Integration System for Robot Task Planning](https://arxiv.org/abs/2506.20394)
*Mimo Shirasaka,Yuya Ikeda,Tatsuya Matsushima,Yutaka Matsuo,Yusuke Iwasawa*

Main category: cs.RO

TL;DR: 论文提出了一种名为SPARK的框架，用于在线更新语义信息，通过场景图表示增强机器人在动态环境中的任务执行能力。


<details>
  <summary>Details</summary>
Motivation: 通用服务机器人需要在线更新几何和语义信息，但现有研究主要集中在几何信息（如SLAM），语义信息的在线更新尚未充分探索。

Method: 基于离线场景图表示的研究，提出SPARK框架，从环境线索中提取语义信息并更新场景图，用于任务规划。

Result: 实验表明，空间关系的图表示能提升机器人在动态环境中的任务执行能力，并适应非常规空间线索（如手势）。

Conclusion: SPARK框架通过在线更新语义信息，有效增强了机器人在动态环境中的适应性和任务执行能力。

Abstract: The ability to update information acquired through various means online
during task execution is crucial for a general-purpose service robot. This
information includes geometric and semantic data. While SLAM handles geometric
updates on 2D maps or 3D point clouds, online updates of semantic information
remain unexplored. We attribute the challenge to the online scene graph
representation, for its utility and scalability. Building on previous works
regarding offline scene graph representations, we study online graph
representations of semantic information in this work. We introduce SPARK:
Spatial Perception and Robot Knowledge Integration. This framework extracts
semantic information from environment-embedded cues and updates the scene graph
accordingly, which is then used for subsequent task planning. We demonstrate
that graph representations of spatial relationships enhance the robot system's
ability to perform tasks in dynamic environments and adapt to unconventional
spatial cues, like gestures.

</details>


### [199] [Multimodal Behaviour Trees for Robotic Laboratory Task Automation](https://arxiv.org/abs/2506.20399)
*Hatem Fakhruldeen,Arvind Raveendran Nambiar,Satheeshkumar Veeramani,Bonilkumar Vijaykumar Tailor,Hadi Beyzaee Juneghani,Gabriella Pizzuto,Andrew Ian Cooper*

Main category: cs.RO

TL;DR: 论文提出了一种基于行为树和多模态感知的新方法，用于提高实验室机器人在执行任务时的可靠性和安全性，实验结果显示高成功率和强错误检测能力。


<details>
  <summary>Details</summary>
Motivation: 实验室机器人能高效完成重复性任务，但可靠性不足可能导致安全问题，如毒物泄漏。因此，需要一种方法确保任务执行的准确性。

Method: 采用行为树结合多模态感知的方法，自动化任务执行并验证任务完成情况。

Result: 实验显示，样本瓶盖封和实验室架插入的成功率分别为88%和92%，且具备强错误检测能力。

Conclusion: 该方法证明了其鲁棒性和可靠性，为下一代机器人化学家的开发奠定了基础。

Abstract: Laboratory robotics offer the capability to conduct experiments with a high
degree of precision and reproducibility, with the potential to transform
scientific research. Trivial and repeatable tasks; e.g., sample transportation
for analysis and vial capping are well-suited for robots; if done successfully
and reliably, chemists could contribute their efforts towards more critical
research activities. Currently, robots can perform these tasks faster than
chemists, but how reliable are they? Improper capping could result in human
exposure to toxic chemicals which could be fatal. To ensure that robots perform
these tasks as accurately as humans, sensory feedback is required to assess the
progress of task execution. To address this, we propose a novel methodology
based on behaviour trees with multimodal perception. Along with automating
robotic tasks, this methodology also verifies the successful execution of the
task, a fundamental requirement in safety-critical environments. The
experimental evaluation was conducted on two lab tasks: sample vial capping and
laboratory rack insertion. The results show high success rate, i.e., 88% for
capping and 92% for insertion, along with strong error detection capabilities.
This ultimately proves the robustness and reliability of our approach and that
using multimodal behaviour trees should pave the way towards the next
generation of robotic chemists.

</details>


### [200] [Learn to Position -- A Novel Meta Method for Robotic Positioning](https://arxiv.org/abs/2506.20445)
*Dongkun Wang,Junkai Zhao,Yunfei Teng,Jieyang Peng,Wenjing Xue,Xiaoming Tao*

Main category: cs.RO

TL;DR: 提出了一种基于视觉无关、模型无关的元方法，通过交互反馈补偿机器人位置误差，提高定位精度，并具备自学习和自适应能力。


<details>
  <summary>Details</summary>
Motivation: 机器人绝对定位精度至关重要，但误差来源复杂且随机，传统视觉方法易受遮挡或光照影响。

Method: 提出一种视觉无关、模型无关的元方法，通过交互反馈最大化定位精度概率，并具备自学习和自适应能力。

Result: 实证研究验证了方法的有效性，已在电子元件装配线中应用。

Conclusion: 该方法通过自学习和自适应显著提升了机器人定位精度，适用于复杂环境。

Abstract: Absolute positioning accuracy is a vital specification for robots. Achieving
high position precision can be challenging due to the presence of various
sources of errors. Meanwhile, accurately depicting these errors is difficult
due to their stochastic nature. Vision-based methods are commonly integrated to
guide robotic positioning, but their performance can be highly impacted by
inevitable occlusions or adverse lighting conditions. Drawing on the
aforementioned considerations, a vision-free, model-agnostic meta-method for
compensating robotic position errors is proposed, which maximizes the
probability of accurate robotic position via interactive feedback. Meanwhile,
the proposed method endows the robot with the capability to learn and adapt to
various position errors, which is inspired by the human's instinct for grasping
under uncertainties. Furthermore, it is a self-learning and self-adaptive
method able to accelerate the robotic positioning process as more examples are
incorporated and learned. Empirical studies validate the effectiveness of the
proposed method. As of the writing of this paper, the proposed meta search
method has already been implemented in a robotic-based assembly line for
odd-form electronic components.

</details>


### [201] [A Review of Personalisation in Human-Robot Collaboration and Future Perspectives Towards Industry 5.0](https://arxiv.org/abs/2506.20447)
*James Fant-Male,Roel Pieters*

Main category: cs.RO

TL;DR: 本文综述了从工业4.0到工业5.0的转变，强调以人为中心的HRC研究趋势，并探讨了个性化HRC的关键因素和未来挑战。


<details>
  <summary>Details</summary>
Motivation: 工业5.0（I5.0）将社会福祉和人类需求置于技术实施的核心，而人机协作（HRC）是实现这一目标的关键。本文旨在探讨个性化HRC的最新进展及其未来发展方向。

Method: 通过综述近期研究，分析个性化HRC的关键趋势，包括个人因素、工作单元设计、交互设计及自适应任务完成。

Result: 研究发现个性化HRC研究日益增多，但缺乏统一方法。同时，提出了伦理和监管方面的关键考虑。

Conclusion: 未来个性化HRC的发展需关注伦理和监管问题，以实现更和谐的人机协作。

Abstract: The shift in research focus from Industry 4.0 to Industry 5.0 (I5.0) promises
a human-centric workplace, with social and well-being values at the centre of
technological implementation. Human-Robot Collaboration (HRC) is a core aspect
of I5.0 development, with an increase in adaptive and personalised interactions
and behaviours. This review investigates recent advancements towards
personalised HRC, where user-centric adaption is key. There is a growing trend
for adaptable HRC research, however there lacks a consistent and unified
approach. The review highlights key research trends on which personal factors
are considered, workcell and interaction design, and adaptive task completion.
This raises various key considerations for future developments, particularly
around the ethical and regulatory development of personalised systems, which
are discussed in detail.

</details>


### [202] [EANS: Reducing Energy Consumption for UAV with an Environmental Adaptive Navigation Strategy](https://arxiv.org/abs/2506.20485)
*Tian Liu,Han Liu,Boyang Li,Long Chen,Kai Huang*

Main category: cs.RO

TL;DR: 本文提出了一种动态调整无人机导航策略的方法，通过分析其动态特性和自主导航管道的时间特性，以减少能耗。


<details>
  <summary>Details</summary>
Motivation: 现有技术在动态场景中采用静态保守策略，导致能量效率低下，需要克服任务管道相互依赖、环境-策略相关性和参数选择等挑战。

Method: 动态调整导航策略，分析无人机的动态特性和自主导航管道的时间特性。

Result: 硬件在环仿真和真实实验显示，任务时间分别提高了3.2倍和2.6倍，能耗分别降低了2.4倍和1.6倍。

Conclusion: 该方法能有效减少无人机在动态环境中的能耗，提升任务效率。

Abstract: Unmanned Aerial Vehicles (UAVS) are limited by the onboard energy. Refinement
of the navigation strategy directly affects both the flight velocity and the
trajectory based on the adjustment of key parameters in the UAVS pipeline, thus
reducing energy consumption. However, existing techniques tend to adopt static
and conservative strategies in dynamic scenarios, leading to inefficient energy
reduction. Dynamically adjusting the navigation strategy requires overcoming
the challenges including the task pipeline interdependencies, the
environmental-strategy correlations, and the selecting parameters. To solve the
aforementioned problems, this paper proposes a method to dynamically adjust the
navigation strategy of the UAVS by analyzing its dynamic characteristics and
the temporal characteristics of the autonomous navigation pipeline, thereby
reducing UAVS energy consumption in response to environmental changes. We
compare our method with the baseline through hardware-in-the-loop (HIL)
simulation and real-world experiments, showing our method 3.2X and 2.6X
improvements in mission time, 2.4X and 1.6X improvements in energy,
respectively.

</details>


### [203] [Behavior Foundation Model: Towards Next-Generation Whole-Body Control System of Humanoid Robots](https://arxiv.org/abs/2506.20487)
*Mingqi Yuan,Tao Yu,Wenqi Ge,Xiuyong Yao,Dapeng Li,Huijiang Wang,Jiayu Chen,Xin Jin,Bo Li,Hua Chen,Wei Zhang,Wenjun Zeng*

Main category: cs.RO

TL;DR: 论文综述了行为基础模型（BFMs）在人形机器人全身控制（WBC）中的应用，探讨了其发展、应用、局限及未来机会。


<details>
  <summary>Details</summary>
Motivation: 解决人形机器人全身控制中因复杂动力学、欠驱动和多样化任务需求带来的挑战，以及学习型控制器在新场景中需重复训练的局限性。

Method: 利用大规模预训练的行为基础模型（BFMs）学习可重用技能和行为先验，实现零样本或快速适应下游任务。

Result: BFMs为可扩展和通用的人形机器人智能提供了关键方法，并总结了相关研究和项目资源。

Conclusion: BFMs是人形机器人智能发展的重要方向，但仍需解决当前局限和挑战。

Abstract: Humanoid robots are drawing significant attention as versatile platforms for
complex motor control, human-robot interaction, and general-purpose physical
intelligence. However, achieving efficient whole-body control (WBC) in
humanoids remains a fundamental challenge due to sophisticated dynamics,
underactuation, and diverse task requirements. While learning-based controllers
have shown promise for complex tasks, their reliance on labor-intensive and
costly retraining for new scenarios limits real-world applicability. To address
these limitations, behavior(al) foundation models (BFMs) have emerged as a new
paradigm that leverages large-scale pretraining to learn reusable primitive
skills and behavioral priors, enabling zero-shot or rapid adaptation to a wide
range of downstream tasks. In this paper, we present a comprehensive overview
of BFMs for humanoid WBC, tracing their development across diverse pre-training
pipelines. Furthermore, we discuss real-world applications, current
limitations, urgent challenges, and future opportunities, positioning BFMs as a
key approach toward scalable and general-purpose humanoid intelligence.
Finally, we provide a curated and long-term list of BFM papers and projects to
facilitate more subsequent research, which is available at
https://github.com/yuanmingqi/awesome-bfm-papers.

</details>


### [204] [Critical Anatomy-Preserving & Terrain-Augmenting Navigation (CAPTAiN): Application to Laminectomy Surgical Education](https://arxiv.org/abs/2506.20496)
*Jonathan Wang,Hisashi Ishida,David Usevitch,Kesavan Venkatesh,Yi Wang,Mehran Armand,Rachel Bronheim,Amit Jain,Adnan Munawar*

Main category: cs.RO

TL;DR: CAPTAiN系统通过分层彩色体素引导提升脊柱钻孔手术的解剖意识，显著提高手术完成率并降低认知负荷。


<details>
  <summary>Details</summary>
Motivation: 脊柱钻孔手术（如椎板切除术）风险高，现有方法缺乏辅助工具以减少硬膜撕裂风险，且患者解剖结构差异增加了新手学习难度。

Method: 开发CAPTAiN系统，提供分层彩色体素引导，通过110次虚拟椎板切除术评估其效果。

Result: CAPTAiN显著提高目标解剖结构的手术完成率（87.99% vs. 74.42%），降低认知负荷，缩小不同经验水平间的表现差距。

Conclusion: CAPTAiN有潜力优化手术执行并支持技能发展，可拓展至神经外科、耳鼻喉科等其他领域。

Abstract: Surgical training remains a crucial milestone in modern medicine, with
procedures such as laminectomy exemplifying the high risks involved.
Laminectomy drilling requires precise manual control to mill bony tissue while
preserving spinal segment integrity and avoiding breaches in the dura: the
protective membrane surrounding the spinal cord. Despite unintended tears
occurring in up to 11.3% of cases, no assistive tools are currently utilized to
reduce this risk. Variability in patient anatomy further complicates learning
for novice surgeons. This study introduces CAPTAiN, a critical
anatomy-preserving and terrain-augmenting navigation system that provides
layered, color-coded voxel guidance to enhance anatomical awareness during
spinal drilling. CAPTAiN was evaluated against a standard non-navigated
approach through 110 virtual laminectomies performed by 11 orthopedic residents
and medical students. CAPTAiN significantly improved surgical completion rates
of target anatomy (87.99% vs. 74.42%) and reduced cognitive load across
multiple NASA-TLX domains. It also minimized performance gaps across experience
levels, enabling novices to perform on par with advanced trainees. These
findings highlight CAPTAiN's potential to optimize surgical execution and
support skill development across experience levels. Beyond laminectomy, it
demonstrates potential for broader applications across various surgical and
drilling procedures, including those in neurosurgery, otolaryngology, and other
medical fields.

</details>


### [205] [Leveraging Correlation Across Test Platforms for Variance-Reduced Metric Estimation](https://arxiv.org/abs/2506.20553)
*Rachel Luo,Heng Yang,Michael Watson,Apoorva Sharma,Sushant Veer,Edward Schmerling,Marco Pavone*

Main category: cs.RO

TL;DR: 提出了一种利用配对数据（如仿真和现实观测）的估计框架，通过控制变量法减少方差，从而显著降低现实世界测试的样本需求。


<details>
  <summary>Details</summary>
Motivation: 基于学习的机器人系统需要大量验证以确保可靠性，但现实测试成本高且数据不足。

Method: 利用控制变量法，结合廉价辅助测量（如仿真输出）优化蒙特卡洛估计的方差。

Result: 理论分析和实验（自动驾驶和四足机器人）表明，该方法显著提高了样本效率，实现了高概率性能边界。

Conclusion: 该技术降低了现实测试负担，使机器人系统的实验评估更高效和经济。

Abstract: Learning-based robotic systems demand rigorous validation to assure reliable
performance, but extensive real-world testing is often prohibitively expensive,
and if conducted may still yield insufficient data for high-confidence
guarantees. In this work, we introduce a general estimation framework that
leverages paired data across test platforms, e.g., paired simulation and
real-world observations, to achieve better estimates of real-world metrics via
the method of control variates. By incorporating cheap and abundant auxiliary
measurements (for example, simulator outputs) as control variates for costly
real-world samples, our method provably reduces the variance of Monte Carlo
estimates and thus requires significantly fewer real-world samples to attain a
specified confidence bound on the mean performance. We provide theoretical
analysis characterizing the variance and sample-efficiency improvement, and
demonstrate empirically in autonomous driving and quadruped robotics settings
that our approach achieves high-probability bounds with markedly improved
sample efficiency. Our technique can lower the real-world testing burden for
validating the performance of the stack, thereby enabling more efficient and
cost-effective experimental evaluation of robotic systems.

</details>


### [206] [HRIBench: Benchmarking Vision-Language Models for Real-Time Human Perception in Human-Robot Interaction](https://arxiv.org/abs/2506.20566)
*Zhonghao Shi,Enyu Zhao,Nathaniel Dennler,Jingzhen Wang,Xinyang Xu,Kaleen Shrestha,Mengxue Fu,Daniel Seita,Maja Matarić*

Main category: cs.RO

TL;DR: 论文介绍了HRIBench，一个用于评估视觉语言模型（VLMs）在人类感知任务中的性能与延迟权衡的基准测试，结果显示当前VLMs在实时人机交互（HRI）中仍存在不足。


<details>
  <summary>Details</summary>
Motivation: 研究VLMs在实时人类感知任务中的性能与延迟问题，以提升人机交互体验。

Method: 构建HRIBench基准测试，涵盖五个关键领域，评估11种VLMs的性能与延迟。

Result: 当前VLMs在核心感知能力和延迟表现上均未达到实时HRI需求。

Conclusion: 未来需开发更小、低延迟的VLMs，HRIBench为相关研究提供了基准。

Abstract: Real-time human perception is crucial for effective human-robot interaction
(HRI). Large vision-language models (VLMs) offer promising generalizable
perceptual capabilities but often suffer from high latency, which negatively
impacts user experience and limits VLM applicability in real-world scenarios.
To systematically study VLM capabilities in human perception for HRI and
performance-latency trade-offs, we introduce HRIBench, a visual
question-answering (VQA) benchmark designed to evaluate VLMs across a diverse
set of human perceptual tasks critical for HRI. HRIBench covers five key
domains: (1) non-verbal cue understanding, (2) verbal instruction
understanding, (3) human-robot object relationship understanding, (4) social
navigation, and (5) person identification. To construct HRIBench, we collected
data from real-world HRI environments to curate questions for non-verbal cue
understanding, and leveraged publicly available datasets for the remaining four
domains. We curated 200 VQA questions for each domain, resulting in a total of
1000 questions for HRIBench. We then conducted a comprehensive evaluation of
both state-of-the-art closed-source and open-source VLMs (N=11) on HRIBench.
Our results show that, despite their generalizability, current VLMs still
struggle with core perceptual capabilities essential for HRI. Moreover, none of
the models within our experiments demonstrated a satisfactory
performance-latency trade-off suitable for real-time deployment, underscoring
the need for future research on developing smaller, low-latency VLMs with
improved human perception capabilities. HRIBench and our results can be found
in this Github repository: https://github.com/interaction-lab/HRIBench.

</details>


### [207] [Communication-Aware Map Compression for Online Path-Planning: A Rate-Distortion Approach](https://arxiv.org/abs/2506.20579)
*Ali Reza Pedram,Evangelos Psomiadis,Dipankar Maity,Panagiotis Tsiotras*

Main category: cs.RO

TL;DR: 论文提出了一种在带宽限制下，通过压缩地图支持协作导航的方法，优化通信成本并提升路径规划效率。


<details>
  <summary>Details</summary>
Motivation: 解决未知环境中两个机器人（Seeker和Supporter）协作导航时的通信带宽限制问题，优化地图传输效率。

Method: 引入基于二进制码字长度的比特率度量，将压缩设计问题建模为率失真优化问题，采用反向注水法求解。

Result: 仿真结果表明，该方法能高效构建任务相关的地图压缩表示，支持Seeker在带宽限制下的路径规划。

Conclusion: 提出的方法在通信成本优化和实时性方面表现优异，适用于带宽受限的协作导航场景。

Abstract: This paper addresses the problem of collaborative navigation in an unknown
environment, where two robots, referred to in the sequel as the Seeker and the
Supporter, traverse the space simultaneously. The Supporter assists the Seeker
by transmitting a compressed representation of its local map under bandwidth
constraints to support the Seeker's path-planning task. We introduce a bit-rate
metric based on the expected binary codeword length to quantify communication
cost. Using this metric, we formulate the compression design problem as a
rate-distortion optimization problem that determines when to communicate, which
regions of the map should be included in the compressed representation, and at
what resolution (i.e., quantization level) they should be encoded. Our
formulation allows different map regions to be encoded at varying quantization
levels based on their relevance to the Seeker's path-planning task. We
demonstrate that the resulting optimization problem is convex, and admits a
closed-form solution known in the information theory literature as reverse
water-filling, enabling efficient, low-computation, and real-time
implementation. Additionally, we show that the Seeker can infer the compression
decisions of the Supporter independently, requiring only the encoded map
content and not the encoding policy itself to be transmitted, thereby reducing
communication overhead. Simulation results indicate that our method effectively
constructs compressed, task-relevant map representations, both in content and
resolution, that guide the Seeker's planning decisions even under tight
bandwidth limitations.

</details>


### [208] [A Computationally Aware Multi Objective Framework for Camera LiDAR Calibration](https://arxiv.org/abs/2506.20636)
*Venkat Karramreddy,Rangarajan Ramanujam*

Main category: cs.RO

TL;DR: 提出了一种多目标优化框架，用于联合优化LiDAR与相机的外参校准的几何对齐误差和计算成本。


<details>
  <summary>Details</summary>
Motivation: LiDAR与相机的外参校准对自动驾驶系统的可靠感知至关重要，现有方法在计算成本和校准精度之间缺乏平衡。

Method: 使用NSGA-II进化算法优化6-DoF变换和点采样率，同时最小化几何对齐误差和计算成本。

Result: 在KITTI数据集上验证，性能优于现有梯度法和学习法，具有可解释性和可调性。

Conclusion: 该框架为资源受限条件下的校准提供了可扩展且透明的方法，适用于长期自主系统。

Abstract: Accurate extrinsic calibration between LiDAR and camera sensors is important
for reliable perception in autonomous systems. In this paper, we present a
novel multi-objective optimization framework that jointly minimizes the
geometric alignment error and computational cost associated with camera-LiDAR
calibration. We optimize two objectives: (1) error between projected LiDAR
points and ground-truth image edges, and (2) a composite metric for
computational cost reflecting runtime and resource usage. Using the NSGA-II
\cite{deb2002nsga2} evolutionary algorithm, we explore the parameter space
defined by 6-DoF transformations and point sampling rates, yielding a
well-characterized Pareto frontier that exposes trade-offs between calibration
fidelity and resource efficiency. Evaluations are conducted on the KITTI
dataset using its ground-truth extrinsic parameters for validation, with
results verified through both multi-objective and constrained single-objective
baselines. Compared to existing gradient-based and learned calibration methods,
our approach demonstrates interpretable, tunable performance with lower
deployment overhead. Pareto-optimal configurations are further analyzed for
parameter sensitivity and innovation insights. A preference-based
decision-making strategy selects solutions from the Pareto knee region to suit
the constraints of the embedded system. The robustness of calibration is tested
across variable edge-intensity weighting schemes, highlighting optimal balance
points. Although real-time deployment on embedded platforms is deferred to
future work, this framework establishes a scalable and transparent method for
calibration under realistic misalignment and resource-limited conditions,
critical for long-term autonomy, particularly in SAE L3+ vehicles receiving OTA
updates.

</details>


### [209] [DemoDiffusion: One-Shot Human Imitation using pre-trained Diffusion Policy](https://arxiv.org/abs/2506.20668)
*Sungjae Park,Homanga Bharadhwaj,Shubham Tulsiani*

Main category: cs.RO

TL;DR: DemoDiffusion是一种简单且可扩展的方法，通过模仿单个人类演示使机器人能在自然环境中执行操作任务。


<details>
  <summary>Details</summary>
Motivation: 解决机器人模仿人类演示时轨迹不匹配的问题，避免需要在线强化学习或配对数据。

Method: 结合人类手部运动的先验信息（通过运动学重定向）和预训练的扩散策略，调整轨迹以符合机器人动作分布。

Result: 在仿真和现实实验中，DemoDiffusion优于基础策略和重定向轨迹，能完成预训练策略失败的任务。

Conclusion: DemoDiffusion能高效适应新任务和场景，仅需少量人工干预。

Abstract: We propose DemoDiffusion, a simple and scalable method for enabling robots to
perform manipulation tasks in natural environments by imitating a single human
demonstration. Our approach is based on two key insights. First, the hand
motion in a human demonstration provides a useful prior for the robot's
end-effector trajectory, which we can convert into a rough open-loop robot
motion trajectory via kinematic retargeting. Second, while this retargeted
motion captures the overall structure of the task, it may not align well with
plausible robot actions in-context. To address this, we leverage a pre-trained
generalist diffusion policy to modify the trajectory, ensuring it both follows
the human motion and remains within the distribution of plausible robot
actions. Our approach avoids the need for online reinforcement learning or
paired human-robot data, enabling robust adaptation to new tasks and scenes
with minimal manual effort. Experiments in both simulation and real-world
settings show that DemoDiffusion outperforms both the base policy and the
retargeted trajectory, enabling the robot to succeed even on tasks where the
pre-trained generalist policy fails entirely. Project page:
https://demodiffusion.github.io/

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [210] [Deciphering GunType Hierarchy through Acoustic Analysis of Gunshot Recordings](https://arxiv.org/abs/2506.20609)
*Ankit Shah,Rita Singh,Bhiksha Raj,Alexander Hauptmann*

Main category: cs.SD

TL;DR: 研究提出利用手机等设备采集的枪声录音进行低成本枪声检测和枪支分类，通过机器学习方法（SVM和CNN）在3459条录音数据集上验证，CNN表现优于SVM。


<details>
  <summary>Details</summary>
Motivation: 商业枪声检测系统成本高昂，亟需低成本替代方案以提升公共安全。

Method: 分析枪声的声学特征（如枪口爆炸和冲击波），提出SVM和CNN框架进行枪声检测和枪支分类。

Result: CNN在干净数据上mAP为0.58，优于SVM的0.39；但在噪声数据上表现下降（mAP 0.35）。

Conclusion: 研究展示了低成本枪声检测系统的潜力，未来目标是开发高精度实时系统。

Abstract: The escalating rates of gun-related violence and mass shootings represent a
significant threat to public safety. Timely and accurate information for law
enforcement agencies is crucial in mitigating these incidents. Current
commercial gunshot detection systems, while effective, often come with
prohibitive costs. This research explores a cost-effective alternative by
leveraging acoustic analysis of gunshot recordings, potentially obtainable from
ubiquitous devices like cell phones, to not only detect gunshots but also
classify the type of firearm used. This paper details a study on deciphering
gun type hierarchies using a curated dataset of 3459 recordings. We investigate
the fundamental acoustic characteristics of gunshots, including muzzle blasts
and shockwaves, which vary based on firearm type, ammunition, and shooting
direction. We propose and evaluate machine learning frameworks, including
Support Vector Machines (SVMs) as a baseline and a more advanced Convolutional
Neural Network (CNN) architecture for joint gunshot detection and gun type
classification. Results indicate that our deep learning approach achieves a
mean average precision (mAP) of 0.58 on clean labeled data, outperforming the
SVM baseline (mAP 0.39). Challenges related to data quality, environmental
noise, and the generalization capabilities when using noisy web-sourced data
(mAP 0.35) are also discussed. The long-term vision is to develop a highly
accurate, real-time system deployable on common recording devices,
significantly reducing detection costs and providing critical intelligence to
first responders.

</details>


<div id='eess.SY'></div>

# eess.SY [[Back]](#toc)

### [211] [Task Allocation of UAVs for Monitoring Missions via Hardware-in-the-Loop Simulation and Experimental Validation](https://arxiv.org/abs/2506.20626)
*Hamza Chakraa,François Guérin,Edouard Leclercq,Dimitri Lefebvre*

Main category: eess.SY

TL;DR: 该研究通过结合遗传算法和2-Opt局部搜索技术，优化了工业监测任务中无人机的任务分配，并通过实验验证了其在实际场景中的有效性。


<details>
  <summary>Details</summary>
Motivation: 解决工业监测任务中无人机的任务分配优化问题，以提高任务执行效率。

Method: 采用遗传算法（GA）结合2-Opt局部搜索技术，并引入硬件在环（HIL）模拟器进行验证。

Result: 实验结果表明，优化部分的成本函数与实际电池消耗和飞行时间高度相关，证明了方法的实用性。

Conclusion: 提出的方法在实际工业监测任务中具有高效性和实用性。

Abstract: This study addresses the optimisation of task allocation for Unmanned Aerial
Vehicles (UAVs) within industrial monitoring missions. The proposed methodology
integrates a Genetic Algorithms (GA) with a 2-Opt local search technique to
obtain a high-quality solution. Our approach was experimentally validated in an
industrial zone to demonstrate its efficacy in real-world scenarios. Also, a
Hardware-in-the-loop (HIL) simulator for the UAVs team is introduced. Moreover,
insights about the correlation between the theoretical cost function and the
actual battery consumption and time of flight are deeply analysed. Results show
that the considered costs for the optimisation part of the problem closely
correlate with real-world data, confirming the practicality of the proposed
approach.

</details>


### [212] [Recurrent neural network-based robust control systems with closed-loop regional incremental ISS and application to MPC design](https://arxiv.org/abs/2506.20334)
*Daniele Ravasio,Marcello Farina,Alessio La Bella,Andrea Ballarino*

Main category: eess.SY

TL;DR: 论文研究了基于一类递归神经网络的系统的输出反馈方案设计，提出了基于线性矩阵不等式的观测器和静态状态反馈控制器设计方法，并引入非线性模型预测控制器以扩大吸引域。


<details>
  <summary>Details</summary>
Motivation: 针对递归神经网络描述的系统，设计鲁棒的输出反馈方案，解决状态估计不确定性和扰动问题。

Method: 提出基于线性矩阵不等式的观测器和静态状态反馈控制器设计方法，并引入基于区域增量输入-状态稳定性的非线性模型预测控制器。

Result: 理论结果通过数值模拟验证，证明了所提方案在pH中和过程基准中的有效性。

Conclusion: 所提方法能够确保鲁棒性、收敛性和递归可行性，并扩大吸引域。

Abstract: This paper investigates the design of output-feedback schemes for systems
described by a class of recurrent neural networks. We propose a procedure based
on linear matrix inequalities for designing an observer and a static
state-feedback controller. The algorithm leverages global and regional
incremental input-to-state stability (incremental ISS) and enables the tracking
of constant setpoints, ensuring robustness to disturbances and state estimation
uncertainty. To address the potential limitations of regional incremental ISS,
we introduce an alternative scheme in which the static law is replaced with a
tube-based nonlinear model predictive controller (NMPC) that exploits regional
incremental ISS properties. We show that these conditions enable the
formulation of a robust NMPC law with guarantees of convergence and recursive
feasibility, leading to an enlarged region of attraction. Theoretical results
are validated through numerical simulations on the pH-neutralisation process
benchmark, demonstrating the effectiveness of the proposed schemes.

</details>


<div id='eess.AS'></div>

# eess.AS [[Back]](#toc)

### [213] [Speaker Embeddings to Improve Tracking of Intermittent and Moving Speakers](https://arxiv.org/abs/2506.19875)
*Taous Iatariene,Can Cui,Alexandre Guérin,Romain Serizel*

Main category: eess.AS

TL;DR: 论文提出了一种基于说话人嵌入的身份重新分配方法，解决了间歇性和移动说话人场景中的跟踪问题。


<details>
  <summary>Details</summary>
Motivation: 在间歇性和移动说话人场景中，传统的空间观测方法难以保持连贯的轨迹身份，导致跟踪性能下降。

Method: 利用初始跟踪步骤提供的轨迹信息和多通道音频信号，通过波束成形增强信号并提取说话人嵌入，基于注册池重新分配身份。

Result: 实验表明，该方法显著提升了神经和标准跟踪系统的身份分配性能，并研究了波束成形和输入时长对嵌入提取的影响。

Conclusion: 基于说话人嵌入的身份重新分配方法有效解决了间歇性和移动说话人场景中的跟踪问题。

Abstract: Speaker tracking methods often rely on spatial observations to assign
coherent track identities over time. This raises limits in scenarios with
intermittent and moving speakers, i.e., speakers that may change position when
they are inactive, thus leading to discontinuous spatial trajectories. This
paper proposes to investigate the use of speaker embeddings, in a simple
solution to this issue. We propose to perform identity reassignment
post-tracking, using speaker embeddings. We leverage trajectory-related
information provided by an initial tracking step and multichannel audio signal.
Beamforming is used to enhance the signal towards the speakers' positions in
order to compute speaker embeddings. These are then used to assign new track
identities based on an enrollment pool. We evaluate the performance of the
proposed speaker embedding-based identity reassignment method on a dataset
where speakers change position during inactivity periods. Results show that it
consistently improves the identity assignment performance of neural and
standard tracking systems. In particular, we study the impact of beamforming
and input duration for embedding extraction.

</details>


### [214] [MATER: Multi-level Acoustic and Textual Emotion Representation for Interpretable Speech Emotion Recognition](https://arxiv.org/abs/2506.19887)
*Hyo Jin Jon,Longbin Jin,Hyuntaek Jung,Hyunseo Kim,Donghun Min,Eun Yi Kim*

Main category: eess.AS

TL;DR: 本文提出了一种名为MATER的多层次声学-文本情感表示框架，用于自然语音中的情感识别和情感属性预测，通过融合声学和文本特征，结合不确定性感知集成策略，取得了显著成果。


<details>
  <summary>Details</summary>
Motivation: 解决自然语音中情感识别的复杂性，包括说话者内和说话者间的变异性，以及标注不一致的问题。

Method: 提出MATER框架，整合声学和文本特征在词、话语和嵌入层次，并引入不确定性感知集成策略。

Result: 在SERNC挑战赛中排名第四，Macro-F1为41.01%，平均CCC为0.5928，在效价预测中排名第二，CCC为0.6941。

Conclusion: MATER框架通过多层次特征融合和不确定性处理，显著提升了自然语音情感识别的性能。

Abstract: This paper presents our contributions to the Speech Emotion Recognition in
Naturalistic Conditions (SERNC) Challenge, where we address categorical emotion
recognition and emotional attribute prediction. To handle the complexities of
natural speech, including intra- and inter-subject variability, we propose
Multi-level Acoustic-Textual Emotion Representation (MATER), a novel
hierarchical framework that integrates acoustic and textual features at the
word, utterance, and embedding levels. By fusing low-level lexical and acoustic
cues with high-level contextualized representations, MATER effectively captures
both fine-grained prosodic variations and semantic nuances. Additionally, we
introduce an uncertainty-aware ensemble strategy to mitigate annotator
inconsistencies, improving robustness in ambiguous emotional expressions. MATER
ranks fourth in both tasks with a Macro-F1 of 41.01% and an average CCC of
0.5928, securing second place in valence prediction with an impressive CCC of
0.6941.

</details>


### [215] [An Exploration of ECAPA-TDNN and x-vector Speaker Representations in Zero-shot Multi-speaker TTS](https://arxiv.org/abs/2506.20190)
*Marie Kunešová,Zdeněk Hanzlíček,Jindřich Matoušek*

Main category: eess.AS

TL;DR: 比较了三种说话人编码器在零样本TTS中的表现，发现H/ASP编码器优于ECAPA-TDNN和x-vector。


<details>
  <summary>Details</summary>
Motivation: 探索说话人识别嵌入在零样本TTS中的有效性，填补研究空白。

Method: 使用YourTTS系统比较H/ASP、x-vector和ECAPA-TDNN编码器，通过主观和客观评估。

Result: H/ASP编码器表现最佳，ECAPA-TDNN优于x-vector。

Conclusion: 说话人识别嵌入在TTS中的表现需实证评估，未来可扩展比较框架。

Abstract: Zero-shot multi-speaker text-to-speech (TTS) systems rely on speaker
embeddings to synthesize speech in the voice of an unseen speaker, using only a
short reference utterance. While many speaker embeddings have been developed
for speaker recognition, their relative effectiveness in zero-shot TTS remains
underexplored. In this work, we employ a YourTTS-based TTS system to compare
three different speaker encoders - YourTTS's original H/ASP encoder, x-vector
embeddings, and ECAPA-TDNN embeddings - within an otherwise fixed zero-shot TTS
framework. All models were trained on the same dataset of Czech read speech and
evaluated on 24 out-of-domain target speakers using both subjective and
objective methods. The subjective evaluation was conducted via a listening test
focused on speaker similarity, while the objective evaluation measured cosine
distances between speaker embeddings extracted from synthesized and real
utterances. Across both evaluations, the original H/ASP encoder consistently
outperformed the alternatives, with ECAPA-TDNN showing better results than
x-vectors. These findings suggest that, despite the popularity of ECAPA-TDNN in
speaker recognition, it does not necessarily offer improvements for speaker
similarity in zero-shot TTS in this configuration. Our study highlights the
importance of empirical evaluation when reusing speaker recognition embeddings
in TTS and provides a framework for additional future comparisons.

</details>


### [216] [Lightweight Target-Speaker-Based Overlap Transcription for Practical Streaming ASR](https://arxiv.org/abs/2506.20288)
*Aleš Pražák,Marie Kunešová,Josef Psutka*

Main category: eess.AS

TL;DR: 提出了一种轻量级的目标说话人扩展方法，用于流式ASR系统，有效处理重叠语音，计算开销小。


<details>
  <summary>Details</summary>
Motivation: 解决广播媒体中动态多说话人交互场景下的重叠语音识别难题。

Method: 结合说话人无关模型（SI）和说话人条件模型（SC），使用FiLM技术整合说话人嵌入，并通过合成数据训练SC模型。

Result: 在捷克电视辩论数据集上，重叠语音的WER从68.0%降至35.78%，计算负载仅增加44%。

Conclusion: 该方法为连续ASR服务中的重叠语音转录提供了高效且可扩展的解决方案。

Abstract: Overlapping speech remains a major challenge for automatic speech recognition
(ASR) in real-world applications, particularly in broadcast media with dynamic,
multi-speaker interactions. We propose a light-weight, target-speaker-based
extension to an existing streaming ASR system to enable practical transcription
of overlapping speech with minimal computational overhead. Our approach
combines a speaker-independent (SI) model for standard operation with a
speaker-conditioned (SC) model selectively applied in overlapping scenarios.
Overlap detection is achieved using a compact binary classifier trained on
frozen SI model output, offering accurate segmentation at negligible cost. The
SC model employs Feature-wise Linear Modulation (FiLM) to incorporate speaker
embeddings and is trained on synthetically mixed data to transcribe only the
target speaker. Our method supports dynamic speaker tracking and reuses
existing modules with minimal modifications. Evaluated on a challenging set of
Czech television debates with 16% overlap, the system reduced WER on
overlapping segments from 68.0% (baseline) to 35.78% while increasing total
computational load by only 44%. The proposed system offers an effective and
scalable solution for overlap transcription in continuous ASR services.

</details>


### [217] [The role of audio-visual integration in the time course of phonetic encoding in self-supervised speech models](https://arxiv.org/abs/2506.20361)
*Yi Wang,Oli Danyi Liu,Peter Bell*

Main category: eess.AS

TL;DR: AV-HuBERT模型未能充分捕捉多模态语音感知中的时间异步性，限制了其在模拟人类多模态语音感知过程中的适用性。


<details>
  <summary>Details</summary>
Motivation: 研究多模态语音感知中音频与视觉线索的时间异步性是否被自监督学习模型（如AV-HuBERT）捕捉。

Method: 比较AV-HuBERT与音频HuBERT模型，通过线性分类器跟踪其语音解码能力的时间变化。

Result: AV-HuBERT的语音信息解码时间仅比HuBERT提前约20毫秒，未能充分反映人类感知中的时间异步性。

Conclusion: AV-HuBERT模型在模拟多模态语音感知的时间动态方面存在不足。

Abstract: Human speech perception is multimodal. In natural speech, lip movements can
precede corresponding voicing by a non-negligible gap of 100-300 ms, especially
for specific consonants, affecting the time course of neural phonetic encoding
in human listeners. However, it remains unexplored whether self-supervised
learning models, which have been used to simulate audio-visual integration in
humans, can capture this asynchronicity between audio and visual cues. We
compared AV-HuBERT, an audio-visual model, with audio-only HuBERT, by using
linear classifiers to track their phonetic decodability over time. We found
that phoneme information becomes available in AV-HuBERT embeddings only about
20 ms before HuBERT, likely due to AV-HuBERT's lower temporal resolution and
feature concatenation process. It suggests AV-HuBERT does not adequately
capture the temporal dynamics of multimodal speech perception, limiting its
suitability for modeling the multimodal speech perception process.

</details>


<div id='q-fin.ST'></div>

# q-fin.ST [[Back]](#toc)

### [218] [Supervised Similarity for Firm Linkages](https://arxiv.org/abs/2506.19856)
*Ryan Samson,Adrian Banner,Luca Candelori,Sebastien Cottrell,Tiziana Di Matteo,Paul Duchnowski,Vahagn Kirakosyan,Jose Marques,Kharen Musaelian,Stefano Pasquali,Ryan Stever,Dario Villani*

Main category: q-fin.ST

TL;DR: 论文提出了一种新的企业关联代理方法CVLs，并通过欧几里得相似性和量子认知机器学习（QCML）进行关联估计，发现QCML方法在动量溢出交易策略中表现更优。


<details>
  <summary>Details</summary>
Motivation: 研究企业关联的代理方法，以改进动量溢出交易策略的构建。

Method: 使用欧几里得相似性和QCML方法估计企业关联。

Result: 两种方法均能构建盈利的动量溢出交易策略，但QCML方法表现更优。

Conclusion: QCML方法在企业关联估计和交易策略中具有优势。

Abstract: We introduce a novel proxy for firm linkages, Characteristic Vector Linkages
(CVLs). We use this concept to estimate firm linkages, first through Euclidean
similarity, and then by applying Quantum Cognition Machine Learning (QCML) to
similarity learning. We demonstrate that both methods can be used to construct
profitable momentum spillover trading strategies, but QCML similarity
outperforms the simpler Euclidean similarity.

</details>


<div id='physics.comp-ph'></div>

# physics.comp-ph [[Back]](#toc)

### [219] [Exploring the Capabilities of the Frontier Large Language Models for Nuclear Energy Research](https://arxiv.org/abs/2506.19863)
*Ahmed Almeldein,Mohammed Alnaggar,Rick Archibald,Tom Beck,Arpan Biswas,Rike Bostelmann,Wes Brewer,Chris Bryan,Christopher Calle,Cihangir Celik,Rajni Chahal,Jong Youl Choi,Arindam Chowdhury,Mark Cianciosa,Franklin Curtis,Gregory Davidson,Sebastian De Pascuale,Lisa Fassino,Ana Gainaru,Yashika Ghai,Luke Gibson,Qian Gong,Christopher Greulich,Scott Greenwood,Cory Hauck,Ehab Hassan,Rinkle Juneja,Soyoung Kang,Scott Klasky,Atul Kumar,Vineet Kumar,Paul Laiu,Calvin Lear,Yan-Ru Lin,Jono McConnell,Furkan Oz,Anant Raj,Pradeep Ramuhalli,Marie Romedenne,Samantha Sabatino,José Salcedo-Pérez,Nathan D. See,Arpan Sircar,Punam Thankur,Tim Younkin,Xiao-Ying Yu,Prashant Jain,Tom Evans,Prasanna Balaprakash*

Main category: physics.comp-ph

TL;DR: LLMs在核能研究中展现出早期探索和文献合成的潜力，但需专家验证和特定数据集支持。


<details>
  <summary>Details</summary>
Motivation: 评估大型语言模型（LLMs）在加速核聚变和裂变研究中的潜力。

Method: 14个跨学科团队使用多种AI模型（如ChatGPT、Gemini、Claude）进行单日探索，结合提示工程和迭代优化。

Result: LLMs在早期探索、文献合成和工作流设计方面表现优异，但在新材料设计和高级代码生成方面存在局限。

Conclusion: AI可作为核能研究的补充工具，需进一步开发核能专用数据集和模型，以加速研究并保持科学严谨性。

Abstract: The AI for Nuclear Energy workshop at Oak Ridge National Laboratory evaluated
the potential of Large Language Models (LLMs) to accelerate fusion and fission
research. Fourteen interdisciplinary teams explored diverse nuclear science
challenges using ChatGPT, Gemini, Claude, and other AI models over a single
day. Applications ranged from developing foundation models for fusion reactor
control to automating Monte Carlo simulations, predicting material degradation,
and designing experimental programs for advanced reactors. Teams employed
structured workflows combining prompt engineering, deep research capabilities,
and iterative refinement to generate hypotheses, prototype code, and research
strategies. Key findings demonstrate that LLMs excel at early-stage
exploration, literature synthesis, and workflow design, successfully
identifying research gaps and generating plausible experimental frameworks.
However, significant limitations emerged, including difficulties with novel
materials designs, advanced code generation for modeling and simulation, and
domain-specific details requiring expert validation. The successful outcomes
resulted from expert-driven prompt engineering and treating AI as a
complementary tool rather than a replacement for physics-based methods. The
workshop validated AI's potential to accelerate nuclear energy research through
rapid iteration and cross-disciplinary synthesis while highlighting the need
for curated nuclear-specific datasets, workflow automation, and specialized
model development. These results provide a roadmap for integrating AI tools
into nuclear science workflows, potentially reducing development cycles for
safer, more efficient nuclear energy systems while maintaining rigorous
scientific standards.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [220] [Quantum Neural Networks for Propensity Score Estimation and Survival Analysis in Observational Biomedical Studies](https://arxiv.org/abs/2506.19973)
*Vojtěch Novák,Ivan Zelinka,Lenka Přibylová,Lubomír Martínek*

Main category: quant-ph

TL;DR: 该研究探讨了量子神经网络（QNN）在倾向评分估计中的应用，用于解决腹腔镜与开放手术技术比较中的选择偏差问题。QNN在模拟硬件噪声条件下表现优于经典方法，并通过优化策略提高了因果推断的准确性。


<details>
  <summary>Details</summary>
Motivation: 研究旨在利用QNN解决生存分析中的选择偏差问题，特别是在小样本高维数据集中，探索量子计算在生物医学研究中的潜力。

Method: 使用QNN架构（线性ZFeatureMap编码、SummedPaulis预测、CMA-ES优化），结合方差正则化处理量子噪声，并在不同模拟条件下进行测试。

Result: QNN在小样本中表现优于经典方法（AUC达0.750），倾向评分匹配和加权后协变量平衡良好，生存分析未显示显著差异。

Conclusion: QNN结合CMA-ES和噪声感知策略在生物医学因果推断中具有潜力，尤其适用于小样本高维数据。

Abstract: This study investigates the application of quantum neural networks (QNNs) for
propensity score estimation to address selection bias in comparing survival
outcomes between laparoscopic and open surgical techniques in a cohort of 1177
colorectal carcinoma patients treated at University Hospital Ostrava
(2001-2009). Using a dataset with 77 variables, including patient demographics
and tumor characteristics, we developed QNN-based propensity score models
focusing on four key covariates (Age, Sex, Stage, BMI). The QNN architecture
employed a linear ZFeatureMap for data encoding, a SummedPaulis operator for
predictions, and the Covariance Matrix Adaptation Evolution Strategy (CMA-ES)
for robust, gradient-free optimization in noisy quantum environments. Variance
regularization was integrated to mitigate quantum measurement noise, with
simulations conducted under exact, sampling (1024 shots), and noisy hardware
(FakeManhattanV2) conditions. QNNs, particularly with simulated hardware noise,
outperformed classical logistic regression and gradient boosted machines in
small samples (AUC up to 0.750 for n=100), with noise modeling enhancing
predictive stability. Propensity score matching and weighting, optimized via
genetic matching and matching weights, achieved covariate balance with
standardized mean differences of 0.0849 and 0.0869, respectively. Survival
analyses using Kaplan-Meier estimation, Cox proportional hazards, and Aalen
additive regression revealed no significant survival differences
post-adjustment (p-values 0.287-0.851), indicating confounding bias in
unadjusted outcomes. These results highlight QNNs' potential, enhanced by
CMA-ES and noise-aware strategies, to improve causal inference in biomedical
research, particularly for small-sample, high-dimensional datasets.

</details>


### [221] [Practical insights on the effect of different encodings, ansätze and measurements in quantum and hybrid convolutional neural networks](https://arxiv.org/abs/2506.20355)
*Jesús Lozano-Cruz,Albert Nieto-Morales,Oriol Balló-Gimbernat,Adan Garriga,Antón Rodríguez-Otero,Alejandro Borrallo-Rentero*

Main category: quant-ph

TL;DR: 研究了参数化量子电路（PQCs）在量子卷积神经网络（QCNN）和混合量子卷积神经网络（HQNN）中的设计选择，应用于EuroSAT数据集的卫星图像分类任务。


<details>
  <summary>Details</summary>
Motivation: 探索PQCs在量子神经网络中的设计选择对性能的影响，特别是在卫星图像分类任务中。

Method: 系统评估了约500种不同模型配置中的数据编码技术、变分ansätze和测量方法。

Result: 混合架构中，数据编码策略对性能影响最大（验证准确率差异达30%），而变分ansätze和测量基准影响较小（差异低于5%）。纯量子模型中，测量协议和数据到振幅映射对性能影响显著（验证准确率差异达30%）。

Conclusion: 数据编码策略在混合架构中起主导作用，而纯量子模型的性能更依赖于测量协议和编码映射。

Abstract: This study investigates the design choices of parameterized quantum circuits
(PQCs) within quantum and hybrid convolutional neural network (HQNN and QCNN)
architectures, applied to the task of satellite image classification using the
EuroSAT dataset. We systematically evaluate the performance implications of
data encoding techniques, variational ans\"atze, and measurement in approx. 500
distinct model configurations. Our analysis reveals a clear hierarchy of
influence on model performance. For hybrid architectures, which were
benchmarked against their direct classical equivalents (e.g. the same
architecture with the PQCs removed), the data encoding strategy is the dominant
factor, with validation accuracy varying over 30% for distinct embeddings. In
contrast, the selection of variational ans\"atze and measurement basis had a
comparatively marginal effect, with validation accuracy variations remaining
below 5%. For purely quantum models, restricted to amplitude encoding,
performance was most dependent on the measurement protocol and the
data-to-amplitude mapping. The measurement strategy varied the validation
accuracy by up to 30% and the encoding mapping by around 8 percentage points.

</details>


<div id='physics.geo-ph'></div>

# physics.geo-ph [[Back]](#toc)

### [222] [Fast ground penetrating radar dual-parameter full waveform inversion method accelerated by hybrid compilation of CUDA kernel function and PyTorch](https://arxiv.org/abs/2506.20513)
*Lei Liu,Chao Song,Liangsheng He,Silin Wang,Xuan Feng,Cai Liu*

Main category: physics.geo-ph

TL;DR: 提出了一种高性能的双参数全波形反演框架（FWI），通过CUDA内核函数和PyTorch的混合编译加速，适用于探地雷达（GPR）数据。


<details>
  <summary>Details</summary>
Motivation: 结合GPU的计算效率和Python深度学习框架的灵活性，实现高效且准确的双参数反演。

Method: 通过将定制化的CUDA内核集成到PyTorch的自动微分机制中，实现对介电常数和电导率的反演。

Result: 在合成数据和实际波场数据上的实验表明，该方法能高效且准确地完成双参数FWI。

Conclusion: 该框架灵活且可扩展，适用于快速地下成像，支持多种正则化策略，具有广泛的应用前景。

Abstract: This study proposes a high-performance dual-parameter full waveform inversion
framework (FWI) for ground-penetrating radar (GPR), accelerated through the
hybrid compilation of CUDA kernel functions and PyTorch. The method leverages
the computational efficiency of GPU programming while preserving the
flexibility and usability of Python-based deep learning frameworks. By
integrating customized CUDA kernels into PyTorch's automatic differentiation
mechanism, the framework enables accurate and efficient inversion of both
dielectric permittivity and electrical conductivity. Experimental evaluations
on synthetic data and real wavefield data demonstrate that the proposed method
achieves dual-parameter FWI for GPR data while maintaining high accuracy.
Moreover, the framework is flexible and extensible, supporting optional
regularization strategies such as total variation and multi-scale inversion.
These features make the proposed approach a practical and scalable framework
for rapid GPR-based subsurface imaging in applications including civil
engineering, environmental monitoring, and geophysical exploration.

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [223] [X-SiT: Inherently Interpretable Surface Vision Transformers for Dementia Diagnosis](https://arxiv.org/abs/2506.20267)
*Fabian Bongratz,Tom Nuno Wolf,Jaume Gual Ramon,Christian Wachinger*

Main category: cs.GR

TL;DR: 论文提出了一种可解释的表面视觉变换器（X-SiT），用于基于可解释的皮层特征进行人类可理解的预测，并在阿尔茨海默病和额颞叶痴呆检测中表现出色。


<details>
  <summary>Details</summary>
Motivation: 3D体积数据难以可视化和解释复杂的脑结构，而皮层表面渲染提供了更易理解的3D表示，推动了可解释模型在医学图像中的应用。

Method: 提出了X-SiT，一种可解释的神经网络，结合原型表面补丁解码器，通过基于案例的推理和空间对应的皮层原型进行分类。

Result: X-SiT在阿尔茨海默病和额颞叶痴呆检测中达到最先进性能，并提供与已知疾病模式一致的原型。

Conclusion: X-SiT不仅提高了分类性能，还通过可解释的原型增强了模型的可信度和临床实用性。

Abstract: Interpretable models are crucial for supporting clinical decision-making,
driving advances in their development and application for medical images.
However, the nature of 3D volumetric data makes it inherently challenging to
visualize and interpret intricate and complex structures like the cerebral
cortex. Cortical surface renderings, on the other hand, provide a more
accessible and understandable 3D representation of brain anatomy, facilitating
visualization and interactive exploration. Motivated by this advantage and the
widespread use of surface data for studying neurological disorders, we present
the eXplainable Surface Vision Transformer (X-SiT). This is the first
inherently interpretable neural network that offers human-understandable
predictions based on interpretable cortical features. As part of X-SiT, we
introduce a prototypical surface patch decoder for classifying surface patch
embeddings, incorporating case-based reasoning with spatially corresponding
cortical prototypes. The results demonstrate state-of-the-art performance in
detecting Alzheimer's disease and frontotemporal dementia while additionally
providing informative prototypes that align with known disease patterns and
reveal classification errors.

</details>


### [224] [DreamAnywhere: Object-Centric Panoramic 3D Scene Generation](https://arxiv.org/abs/2506.20367)
*Edoardo Alberto Dominici,Jozef Hladky,Floor Verhoeven,Lukas Radl,Thomas Deixelberger,Stefan Ainetter,Philipp Drescher,Stefan Hauswiesner,Arno Coomans,Giacomo Nazzaro,Konstantinos Vardis,Markus Steinberger*

Main category: cs.GR

TL;DR: DreamAnywhere是一个模块化系统，用于快速生成和原型化3D场景，解决了现有方法在视觉保真度、场景理解和多环境适应性上的不足。


<details>
  <summary>Details</summary>
Motivation: 现有文本到3D场景生成方法存在仅支持正面视角、视觉保真度低、场景理解有限且仅适用于特定环境的问题。

Method: 系统通过合成360度全景图像，分解为背景和对象，通过混合修复构建完整3D表示，并将对象掩模提升为详细3D对象。

Result: DreamAnywhere在新型视图合成一致性和图像质量上显著优于现有方法，用户研究显示其更受欢迎。

Conclusion: DreamAnywhere在多样化和挑战性场景中表现出色，适用于低成本电影制作和快速原型设计。

Abstract: Recent advances in text-to-3D scene generation have demonstrated significant
potential to transform content creation across multiple industries. Although
the research community has made impressive progress in addressing the
challenges of this complex task, existing methods often generate environments
that are only front-facing, lack visual fidelity, exhibit limited scene
understanding, and are typically fine-tuned for either indoor or outdoor
settings. In this work, we address these issues and propose DreamAnywhere, a
modular system for the fast generation and prototyping of 3D scenes. Our system
synthesizes a 360{\deg} panoramic image from text, decomposes it into
background and objects, constructs a complete 3D representation through hybrid
inpainting, and lifts object masks to detailed 3D objects that are placed in
the virtual environment. DreamAnywhere supports immersive navigation and
intuitive object-level editing, making it ideal for scene exploration, visual
mock-ups, and rapid prototyping -- all with minimal manual modeling. These
features make our system particularly suitable for low-budget movie production,
enabling quick iteration on scene layout and visual tone without the overhead
of traditional 3D workflows. Our modular pipeline is highly customizable as it
allows components to be replaced independently. Compared to current
state-of-the-art text and image-based 3D scene generation approaches,
DreamAnywhere shows significant improvements in coherence in novel view
synthesis and achieves competitive image quality, demonstrating its
effectiveness across diverse and challenging scenarios. A comprehensive user
study demonstrates a clear preference for our method over existing approaches,
validating both its technical robustness and practical usefulness.

</details>


### [225] [EditP23: 3D Editing via Propagation of Image Prompts to Multi-View](https://arxiv.org/abs/2506.20652)
*Roi Bar-On,Dana Cohen-Bar,Daniel Cohen-Or*

Main category: cs.GR

TL;DR: EditP23是一种无需掩码的3D编辑方法，通过2D图像编辑传播到多视角表示，实现3D一致性编辑。


<details>
  <summary>Details</summary>
Motivation: 传统方法依赖文本提示或显式空间掩码，EditP23通过原始视图和用户编辑后的图像对实现直观编辑。

Method: 利用预训练多视角扩散模型的潜在空间，通过编辑感知流传播编辑，无需优化，保持对象结构和外观一致性。

Result: 在多种对象类别和编辑场景中表现高效，无需手动掩码即可实现高保真度。

Conclusion: EditP23提供了一种高效、直观的3D编辑方法，适用于广泛的应用场景。

Abstract: We present EditP23, a method for mask-free 3D editing that propagates 2D
image edits to multi-view representations in a 3D-consistent manner. In
contrast to traditional approaches that rely on text-based prompting or
explicit spatial masks, EditP23 enables intuitive edits by conditioning on a
pair of images: an original view and its user-edited counterpart. These image
prompts are used to guide an edit-aware flow in the latent space of a
pre-trained multi-view diffusion model, allowing the edit to be coherently
propagated across views. Our method operates in a feed-forward manner, without
optimization, and preserves the identity of the original object, in both
structure and appearance. We demonstrate its effectiveness across a range of
object categories and editing scenarios, achieving high fidelity to the source
while requiring no manual masks.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [226] [Piecewise Linear Approximation in Learned Index Structures: Theoretical and Empirical Analysis](https://arxiv.org/abs/2506.20139)
*Jiayong Qin,Xianyu Zhu,Qiyu Liu,Guangyi Zhang,Zhigang Cai,Jianwei Liao,Sha Hu,Jingshu Peng,Yingxia Shao,Lei Chen*

Main category: cs.DB

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: A growing trend in the database and system communities is to augment
conventional index structures, such as B+-trees, with machine learning (ML)
models. Among these, error-bounded Piecewise Linear Approximation
($\epsilon$-PLA) has emerged as a popular choice due to its simplicity and
effectiveness. Despite its central role in many learned indexes, the design and
analysis of $\epsilon$-PLA fitting algorithms remain underexplored. In this
paper, we revisit $\epsilon$-PLA from both theoretical and empirical
perspectives, with a focus on its application in learned index structures. We
first establish a fundamentally improved lower bound of $\Omega(\kappa \cdot
\epsilon^2)$ on the expected segment coverage for existing $\epsilon$-PLA
fitting algorithms, where $\kappa$ is a data-dependent constant. We then
present a comprehensive benchmark of state-of-the-art $\epsilon$-PLA algorithms
when used in different learned data structures. Our results highlight key
trade-offs among model accuracy, model size, and query performance, providing
actionable guidelines for the principled design of future learned data
structures.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [227] [Multimodal Information Retrieval for Open World with Edit Distance Weak Supervision](https://arxiv.org/abs/2506.20070)
*KMA Solaiman,Bharat Bhargava*

Main category: cs.IR

TL;DR: FemmIR是一种无需标注的多媒体检索框架，利用预训练编码器和弱监督方法，通过编辑距离衡量样本间相似性。


<details>
  <summary>Details</summary>
Motivation: 避免传统多媒体检索模型对标注数据的依赖，利用预训练模型和弱监督方法实现高效检索。

Method: 基于编辑距离的弱监督方法，通过多级交互评分衡量样本与查询示例的相似性。

Result: 在MuQNOL数据集上验证，FemmIR性能接近现有检索系统，且无需微调。

Conclusion: FemmIR为标注稀缺场景提供了一种高效的多媒体检索解决方案。

Abstract: Existing multi-media retrieval models either rely on creating a common
subspace with modality-specific representation models or require schema mapping
among modalities to measure similarities among multi-media data. Our goal is to
avoid the annotation overhead incurred from considering retrieval as a
supervised classification task and re-use the pretrained encoders in large
language models and vision tasks. We propose "FemmIR", a framework to retrieve
multimodal results relevant to information needs expressed with multimodal
queries by example without any similarity label. Such identification is
necessary for real-world applications where data annotations are scarce and
satisfactory performance is required without fine-tuning with a common
framework across applications. We curate a new dataset called MuQNOL for
benchmarking progress on this task. Our technique is based on weak supervision
introduced through edit distance between samples: graph edit distance can be
modified to consider the cost of replacing a data sample in terms of its
properties, and relevance can be measured through the implicit signal from the
amount of edit cost among the objects. Unlike metric learning or encoding
networks, FemmIR re-uses the high-level properties and maintains the property
value and relationship constraints with a multi-level interaction score between
data samples and the query example provided by the user. We empirically
evaluate FemmIR on a missing person use case with MuQNOL. FemmIR performs
comparably to similar retrieval systems in delivering on-demand retrieval
results with exact and approximate similarities while using the existing
property identifiers in the system.

</details>


### [228] [CoVE: Compressed Vocabulary Expansion Makes Better LLM-based Recommender Systems](https://arxiv.org/abs/2506.19993)
*Haochen Zhang,Tianyi Zhang,Junze Yin,Oren Gal,Anshumali Shrivastava,Vladimir Braverman*

Main category: cs.IR

TL;DR: 提出了一种名为CoVE的新系统，通过压缩词汇扩展和利用LLMs的序列理解能力，显著提升了推荐任务的性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法未能充分利用LLMs的序列信息处理能力，导致推荐系统性能不佳。

Method: 为每个项目分配唯一ID，扩展词汇表，并压缩嵌入层，以利用LLMs的序列理解能力。

Result: 在多个推荐数据集上验证了CoVE的有效性和性能，优于现有方法。

Conclusion: CoVE通过优化LLMs的序列处理能力，为大规模工业应用提供了实用的推荐系统解决方案。

Abstract: Recommender systems play a pivotal role in providing relevant content to
users. With the rapid development of large language models (LLMs), researchers
have begun utilizing LLMs to build more powerful recommender systems. However,
existing approaches that focus on aligning LLMs with recommendation tasks do
not fully leverage their sequential information processing capabilities,
leading to suboptimal performance.
  In this paper, we propose a novel system called compressed vocabulary
expansion (CoVE). In CoVE, each item is assigned a unique ID within the
expanded vocabulary. Our framework effectively capitalizes on sequence
understanding abilities of LLMs, significantly enhancing their performance on
recommendation tasks. Additionally, we compress the embedding layer, making
CoVE practical for large-scale industrial applications. The effectiveness and
performance of CoVE are demonstrated through comprehensive experiments on
multiple recommendation datasets and comparisons with prior works. Our code can
be found at https://github.com/HaochenZhang717/CoVE-official-Repo.

</details>


### [229] [Unidentified and Confounded? Understanding Two-Tower Models for Unbiased Learning to Rank](https://arxiv.org/abs/2506.20501)
*Philipp Hager,Onno Zoeter,Maarten de Rijke*

Main category: cs.IR

TL;DR: 两塔模型在工业环境中用于处理有偏用户反馈，但训练时使用高性能系统收集的点击数据会导致排名性能下降。本文研究了日志策略的混杂效应和模型可识别性问题，并提出样本加权技术以缓解这些问题。


<details>
  <summary>Details</summary>
Motivation: 研究两塔模型在点击数据训练下性能下降的原因，特别是日志策略和模型可识别性的影响。

Method: 理论分析两塔模型的可识别性条件，研究日志策略对模型的影响，并提出样本加权技术。

Result: 发现文档位置交换或特征分布重叠是恢复模型参数的必要条件；日志策略在模型完美捕捉用户行为时无偏，否则会放大偏差。

Conclusion: 提出样本加权技术以缓解偏差，为使用两塔模型的研究者和从业者提供实用建议。

Abstract: Additive two-tower models are popular learning-to-rank methods for handling
biased user feedback in industry settings. Recent studies, however, report a
concerning phenomenon: training two-tower models on clicks collected by
well-performing production systems leads to decreased ranking performance. This
paper investigates two recent explanations for this observation: confounding
effects from logging policies and model identifiability issues. We
theoretically analyze the identifiability conditions of two-tower models,
showing that either document swaps across positions or overlapping feature
distributions are required to recover model parameters from clicks. We also
investigate the effect of logging policies on two-tower models, finding that
they introduce no bias when models perfectly capture user behavior. However,
logging policies can amplify biases when models imperfectly capture user
behavior, particularly when prediction errors correlate with document placement
across positions. We propose a sample weighting technique to mitigate these
effects and provide actionable insights for researchers and practitioners using
two-tower models.

</details>


<div id='physics.med-ph'></div>

# physics.med-ph [[Back]](#toc)

### [230] [Physics-Guided Radiotherapy Treatment Planning with Deep Learning](https://arxiv.org/abs/2506.19880)
*Stefanos Achlatis,Efstratios Gavves,Jan-Jakob Sonke*

Main category: physics.med-ph

TL;DR: 提出了一种两阶段、物理引导的深度学习管道，用于放射治疗计划，通过结合MLC和MU值的监督以及预测的3D剂量分布，生成接近临床真实值的治疗计划。


<details>
  <summary>Details</summary>
Motivation: 放射治疗需要频繁调整计划以适应解剖变化，深度学习可以自动化这一过程，提高效率。

Method: 两阶段深度学习管道：第一阶段监督MLC和MU值，第二阶段结合预测的3D剂量分布进行物理引导训练。

Result: 在133名前列腺癌患者中，方法生成的计划接近临床真实值，D95%和V95%差异小，同时减少对危险器官的辐射。

Conclusion: 物理引导的深度学习在放射治疗计划中具有潜力，可高效生成高质量计划。

Abstract: Radiotherapy (RT) is a critical cancer treatment, with volumetric modulated
arc therapy (VMAT) being a commonly used technique that enhances dose
conformity by dynamically adjusting multileaf collimator (MLC) positions and
monitor units (MU) throughout gantry rotation. Adaptive radiotherapy requires
frequent modifications to treatment plans to account for anatomical variations,
necessitating time-efficient solutions. Deep learning offers a promising
solution to automate this process. To this end, we propose a two-stage,
physics-guided deep learning pipeline for radiotherapy planning. In the first
stage, our network is trained with direct supervision on treatment plan
parameters, consisting of MLC and MU values. In the second stage, we
incorporate an additional supervision signal derived from the predicted 3D dose
distribution, integrating physics-based guidance into the training process. We
train and evaluate our approach on 133 prostate cancer patients treated with a
uniform 2-arc VMAT protocol delivering a dose of 62 Gy to the planning target
volume (PTV). Our results demonstrate that the proposed approach, implemented
using both 3D U-Net and UNETR architectures, consistently produces treatment
plans that closely match clinical ground truths. Our method achieves a mean
difference of D95% = 0.42 +/- 1.83 Gy and V95% = -0.22 +/- 1.87% at the PTV
while generating dose distributions that reduce radiation exposure to organs at
risk. These findings highlight the potential of physics-guided deep learning in
RT planning.

</details>


### [231] [Neural networks for the prediction of peel force for skin adhesive interface using FEM simulation](https://arxiv.org/abs/2506.19855)
*Ashish Masarkar,Rakesh Gupta,Naga Neehar Dingari,Beena Rai*

Main category: physics.med-ph

TL;DR: 提出了一种基于神经网络的粘合剂剥离力预测方法，显著减少计算成本，同时保持高精度。


<details>
  <summary>Details</summary>
Motivation: 研究粘合剂在皮肤上的剥离行为对生物医学应用（如医用粘合剂和透皮贴片）至关重要，传统方法资源密集且耗时。

Method: 利用有限元模拟生成的数据集训练神经网络，预测最小剥离力，并通过5折交叉验证验证模型性能。

Result: 模型在测试集上表现出色，均方误差为3.66*10^-7，R^2得分为0.94。

Conclusion: 该方法为皮肤粘合剂系统的设计和优化提供了高效、可扩展的框架。

Abstract: Studying the peeling behaviour of adhesives on skin is vital for advancing
biomedical applications such as medical adhesives and transdermal patches.
Traditional methods like experimental testing and finite element method (FEM),
though considered gold standards, are resource-intensive, computationally
expensive and time-consuming, particularly when analysing a wide material
parameter space. In this study, we present a neural network-based approach to
predict the minimum peel force (F_min) required for adhesive detachment from
skin tissue, limiting the need for repeated FEM simulations and significantly
reducing the computational cost. Leveraging a dataset generated from FEM
simulations of 90 degree peel test with varying adhesive and fracture mechanics
parameters, our neural network model achieved high accuracy, validated through
rigorous 5-fold cross-validation. The final architecture was able to predict a
wide variety of skin-adhesive peeling behaviour, exhibiting a mean squared
error (MSE) of 3.66*10^-7 and a R^2 score of 0.94 on test set, demonstrating
robust performance. This work introduces a reliable, computationally efficient
method for predicting adhesive behaviour, significantly reducing simulation
time while maintaining accuracy. This integration of machine learning with
high-fidelity biomechanical simulations enables efficient design and
optimization of skin-adhesive systems, providing a scalable framework for
future research in computational dermato-mechanics and bio-adhesive material
design.

</details>


<div id='q-bio.NC'></div>

# q-bio.NC [[Back]](#toc)

### [232] [Do psychic cells generate consciousness?](https://arxiv.org/abs/2506.20164)
*Mototaka Suzuki,Jaan Aru*

Main category: q-bio.NC

TL;DR: 综述了近年来关于大脑意识处理细胞机制的研究进展，重点关注了皮质锥体神经元及其在麻醉诱导意识丧失中的作用。


<details>
  <summary>Details</summary>
Motivation: 探索意识在大脑中的细胞水平机制，特别是皮质锥体神经元的作用。

Method: 回顾了近期研究，分析了皮质锥体神经元及其代谢型受体在意识处理中的关键作用。

Result: 发现代谢型受体在锥体神经元树突上的分布可能是意识调控的关键机制。

Conclusion: Cajal的直觉可能是正确的，皮质锥体神经元可能在意识的生成和控制中起核心作用。

Abstract: Technological advances in the past decades have begun to enable
neuroscientists to address fundamental questions about consciousness in an
unprecedented way. Here we review remarkable recent progress in our
understanding of cellular-level mechanisms of conscious processing in the
brain. Of particular interest are the cortical pyramidal neurons -- or "psychic
cells" called by Ram\'on y Cajal more than 100 years ago -- which have an
intriguing cellular mechanism that accounts for selective disruption of
feedback signaling in the brain upon anesthetic-induced loss of consciousness.
Importantly, a particular class of metabotropic receptors distributed over the
dendrites of pyramidal cells are highlighted as the key cellular mechanism.
After all, Cajal's instinct over a century ago may turn out to be correct -- we
may have just begun to understand whether and how psychic cells indeed generate
and control our consciousness.

</details>


<div id='q-bio.QM'></div>

# q-bio.QM [[Back]](#toc)

### [233] [PocketVina Enables Scalable and Highly Accurate Physically Valid Docking through Multi-Pocket Conditioning](https://arxiv.org/abs/2506.20043)
*Ahmet Sarigun,Bora Uyar,Vedran Franke,Altuna Akalin*

Main category: q-bio.QM

TL;DR: PocketVina是一种快速、内存高效的分子对接框架，结合口袋预测与多口袋系统探索，在物理有效性采样方面表现优异。


<details>
  <summary>Details</summary>
Motivation: 解决分子对接中物理有效配体结合构象采样的挑战，特别是针对未见或结构多样的目标。

Method: 结合口袋预测与系统多口袋探索的搜索型对接框架。

Result: 在多个基准测试中表现优异，尤其在物理有效性（PB-valid）和RMSD联合指标上达到领先水平，同时在大规模数据集上能有效区分活性与非活性目标。

Conclusion: PocketVina提供了一种无需任务特定训练、高效且可扩展的对接策略，适用于高通量虚拟筛选和基于结构的药物发现。

Abstract: Sampling physically valid ligand-binding poses remains a major challenge in
molecular docking, particularly for unseen or structurally diverse targets. We
introduce PocketVina, a fast and memory-efficient, search-based docking
framework that combines pocket prediction with systematic multi-pocket
exploration. We evaluate PocketVina across four established
benchmarks--PDBbind2020 (timesplit and unseen), DockGen, Astex, and
PoseBusters--and observe consistently strong performance in sampling physically
valid docking poses. PocketVina achieves state-of-the-art performance when
jointly considering ligand RMSD and physical validity (PB-valid), while
remaining competitive with deep learning-based approaches in terms of RMSD
alone, particularly on structurally diverse and previously unseen targets.
PocketVina also maintains state-of-the-art physically valid docking accuracy
across ligands with varying degrees of flexibility. We further introduce
TargetDock-AI, a benchmarking dataset we curated, consisting of over 500000
protein-ligand pairs, and a partition of the dataset labeled with PubChem
activity annotations. On this large-scale dataset, PocketVina successfully
discriminates active from inactive targets, outperforming a deep learning
baseline while requiring significantly less GPU memory and runtime. PocketVina
offers a robust and scalable docking strategy that requires no task-specific
training and runs efficiently on standard GPUs, making it well-suited for
high-throughput virtual screening and structure-based drug discovery.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [234] [Secure Energy Transactions Using Blockchain Leveraging AI for Fraud Detection and Energy Market Stability](https://arxiv.org/abs/2506.19870)
*Md Asif Ul Hoq Khan,MD Zahedul Islam,Istiaq Ahmed,Md Masud Karim Rabbi,Farhana Rahman Anonna,MD Abdul Fahim Zeeshan,Mehedi Hasan Ridoy,Bivash Ranjan Chowdhury,Md Nazmul Shakir Rabbi,GM Alamin Sadnan*

Main category: cs.CR

TL;DR: 研究开发了一种结合区块链和AI的安全、智能、高效的能源交易系统，用于解决去中心化能源市场的安全性和可靠性问题。


<details>
  <summary>Details</summary>
Motivation: 去中心化能源市场的发展带来了新的挑战，如交易安全和真实性，需要创新的解决方案。

Method: 结合区块链和AI技术，利用超过120万条模拟P2P能源交易记录，构建双层系统架构（区块链层和AI层）。

Result: 系统能够有效检测欺诈行为并提高市场可靠性，机器学习模型在分类任务中表现优异。

Conclusion: 区块链与AI的结合为去中心化能源市场提供了安全、高效的解决方案，具有实际应用潜力。

Abstract: Peer-to-peer trading and the move to decentralized grids have reshaped the
energy markets in the United States. Notwithstanding, such developments lead to
new challenges, mainly regarding the safety and authenticity of energy trade.
This study aimed to develop and build a secure, intelligent, and efficient
energy transaction system for the decentralized US energy market. This research
interlinks the technological prowess of blockchain and artificial intelligence
(AI) in a novel way to solve long-standing challenges in the distributed energy
market, specifically those of security, fraudulent behavior detection, and
market reliability. The dataset for this research is comprised of more than 1.2
million anonymized energy transaction records from a simulated peer-to-peer
(P2P) energy exchange network emulating real-life blockchain-based American
microgrids, including those tested by LO3 Energy and Grid+ Labs. Each record
contains detailed fields of transaction identifier, timestamp, energy volume
(kWh), transaction type (buy/sell), unit price, prosumer/consumer identifier
(hashed for privacy), smart meter readings, geolocation regions, and settlement
confirmation status. The dataset also includes system-calculated behavior
metrics of transaction rate, variability of energy production, and historical
pricing patterns. The system architecture proposed involves the integration of
two layers, namely a blockchain layer and artificial intelligence (AI) layer,
each playing a unique but complementary function in energy transaction securing
and market intelligence improvement. The machine learning models used in this
research were specifically chosen for their established high performance in
classification tasks, specifically in the identification of energy transaction
fraud in decentralized markets.

</details>


### [235] [An Attack Method for Medical Insurance Claim Fraud Detection based on Generative Adversarial Network](https://arxiv.org/abs/2506.19871)
*Yining Pang,Chenghan Li*

Main category: cs.CR

TL;DR: 论文提出了一种基于GAN的方法对抗保险欺诈检测系统，攻击成功率达99%，强调了提升模型鲁棒性的紧迫性。


<details>
  <summary>Details</summary>
Motivation: 当前保险欺诈检测系统缺乏标准化防御机制，易受对抗性攻击威胁，需提升其安全性和可靠性。

Method: 采用GAN生成对抗样本，攻击欺诈检测系统，无需了解训练数据或模型细节。

Result: 攻击成功率达99%，能生成被误判为合法的欺诈案例。

Conclusion: 研究揭示了对抗性攻击的高风险，呼吁加强保险欺诈检测模型的鲁棒性。

Abstract: Insurance fraud detection represents a pivotal advancement in modern
insurance service, providing intelligent and digitalized monitoring to enhance
management and prevent fraud. It is crucial for ensuring the security and
efficiency of insurance systems. Although AI and machine learning algorithms
have demonstrated strong performance in detecting fraudulent claims, the
absence of standardized defense mechanisms renders current systems vulnerable
to emerging adversarial threats. In this paper, we propose a GAN-based approach
to conduct adversarial attacks on fraud detection systems. Our results indicate
that an attacker, without knowledge of the training data or internal model
details, can generate fraudulent cases that are classified as legitimate with a
99\% attack success rate (ASR). By subtly modifying real insurance records and
claims, adversaries can significantly increase the fraud risk, potentially
bypassing compromised detection systems. These findings underscore the urgent
need to enhance the robustness of insurance fraud detection models against
adversarial manipulation, thereby ensuring the stability and reliability of
different insurance systems.

</details>


### [236] [Towards Provable (In)Secure Model Weight Release Schemes](https://arxiv.org/abs/2506.19874)
*Xing Yang,Bingtao Wang,Yuhao Wang,Zimo Ji,Terry Jingchen Zhang,Wenyuan Jiang*

Main category: cs.CR

TL;DR: 论文分析了现有安全权重发布方案的安全性问题，提出了形式化的安全定义，并通过案例研究揭示了TaylorMLP的漏洞。


<details>
  <summary>Details</summary>
Motivation: 现有安全权重发布方案缺乏严格的安全基础，仅提供非正式的安全保证。

Method: 引入形式化的安全定义，并通过案例研究分析TaylorMLP的安全性。

Result: 发现TaylorMLP存在参数提取漏洞，未能实现其安全目标。

Conclusion: 呼吁在机器学习和安全交叉领域进行严格研究，并为未来权重发布方案的设计和评估提供蓝图。

Abstract: Recent secure weight release schemes claim to enable open-source model
distribution while protecting model ownership and preventing misuse. However,
these approaches lack rigorous security foundations and provide only informal
security guarantees. Inspired by established works in cryptography, we
formalize the security of weight release schemes by introducing several
concrete security definitions. We then demonstrate our definition's utility
through a case study of TaylorMLP, a prominent secure weight release scheme.
Our analysis reveals vulnerabilities that allow parameter extraction thus
showing that TaylorMLP fails to achieve its informal security goals. We hope
this work will advocate for rigorous research at the intersection of machine
learning and security communities and provide a blueprint for how future weight
release schemes should be designed and evaluated.

</details>


### [237] [Robust Anomaly Detection in Network Traffic: Evaluating Machine Learning Models on CICIDS2017](https://arxiv.org/abs/2506.19877)
*Zhaoyang Xu,Yunbo Liu*

Main category: cs.CR

TL;DR: 本文比较了四种机器学习模型在入侵检测中的表现，发现监督模型在已知攻击上表现优异，但在未知攻击上表现不佳；无监督模型在未知攻击上表现较好，但存在误报问题。


<details>
  <summary>Details</summary>
Motivation: 研究不同机器学习模型在入侵检测中的适用性，以构建更有效和通用的安全解决方案。

Method: 在CICIDS2017数据集上比较了四种模型（MLP、CNN、OCSVM和LOF）的性能，分别测试已知攻击和未知攻击的检测能力。

Result: 监督模型（MLP和CNN）在已知攻击上表现优异，但在未知攻击上召回率显著下降；无监督模型（LOF）在未知攻击上召回率高但误报率高；OCSVM在精度和召回率上表现平衡。

Conclusion: OCSVM在动态网络环境中表现稳健，为入侵检测系统的模型选择提供了实用指导。

Abstract: Identifying suitable machine learning paradigms for intrusion detection
remains critical for building effective and generalizable security solutions.
In this study, we present a controlled comparison of four representative models
- Multi-Layer Perceptron (MLP), 1D Convolutional Neural Network (CNN),
One-Class Support Vector Machine (OCSVM) and Local Outlier Factor (LOF) - on
the CICIDS2017 dataset under two scenarios: detecting known attack types and
generalizing to previously unseen threats. Our results show that supervised MLP
and CNN achieve near-perfect accuracy on familiar attacks but suffer drastic
recall drops on novel attacks. Unsupervised LOF attains moderate overall
accuracy and high recall on unknown threats at the cost of elevated false
alarms, while boundary-based OCSVM balances precision and recall best,
demonstrating robust detection across both scenarios. These findings offer
practical guidance for selecting IDS models in dynamic network environments.

</details>


### [238] [Retrieval-Confused Generation is a Good Defender for Privacy Violation Attack of Large Language Models](https://arxiv.org/abs/2506.19889)
*Wanli Peng,Xin Chen,Hang Fu,XinYu He,Xue Yiming,Juan Wen*

Main category: cs.CR

TL;DR: 提出了一种基于检索混淆生成（RCG）的防御方法，以高效隐蔽地防御隐私侵犯攻击（PVA）。


<details>
  <summary>Details</summary>
Motivation: 现有防御方法成本高且效果不佳，直接拒绝攻击查询会暴露防御方法。

Method: 设计改写提示构建扰动数据库，采用最不相关检索策略，替换数据评论以误导攻击者。

Result: 在两个数据集和八个流行LLM上验证了方法的可行性和优越性。

Conclusion: RCG方法能有效隐蔽地防御PVA，提升隐私保护能力。

Abstract: Recent advances in large language models (LLMs) have made a profound impact
on our society and also raised new security concerns. Particularly, due to the
remarkable inference ability of LLMs, the privacy violation attack (PVA),
revealed by Staab et al., introduces serious personal privacy issues. Existing
defense methods mainly leverage LLMs to anonymize the input query, which
requires costly inference time and cannot gain satisfactory defense
performance. Moreover, directly rejecting the PVA query seems like an effective
defense method, while the defense method is exposed, promoting the evolution of
PVA. In this paper, we propose a novel defense paradigm based on
retrieval-confused generation (RCG) of LLMs, which can efficiently and covertly
defend the PVA. We first design a paraphrasing prompt to induce the LLM to
rewrite the "user comments" of the attack query to construct a disturbed
database. Then, we propose the most irrelevant retrieval strategy to retrieve
the desired user data from the disturbed database. Finally, the "data comments"
are replaced with the retrieved user data to form a defended query, leading to
responding to the adversary with some wrong personal attributes, i.e., the
attack fails. Extensive experiments are conducted on two datasets and eight
popular LLMs to comprehensively evaluate the feasibility and the superiority of
the proposed defense method.

</details>


### [239] [RepuNet: A Reputation System for Mitigating Malicious Clients in DFL](https://arxiv.org/abs/2506.19892)
*Isaac Marroqui Penalva,Enrique Tomás Martínez Beltrán,Manuel Gil Pérez,Alberto Huertas Celdrán*

Main category: cs.CR

TL;DR: RepuNet是一种去中心化的声誉系统，用于检测和缓解DFL中的恶意行为，通过动态评估节点行为并调整其影响力，实验证明其高效性。


<details>
  <summary>Details</summary>
Motivation: DFL中节点自主选择聚合对等点，易受模型投毒、延迟攻击和消息洪泛等威胁，现有解决方案存在计算开销或适应性不足的问题。

Method: 提出RepuNet，通过模型相似性、参数变化、消息延迟和通信量等指标动态评估节点行为，并基于声誉分数调整节点影响力。

Result: 在MNIST和CIFAR-10数据集上测试，RepuNet的F1分数分别超过95%和约76%，有效检测和缓解恶意行为。

Conclusion: RepuNet展示了在去中心化联邦学习环境中应对威胁的适应性、鲁棒性和实用潜力。

Abstract: Decentralized Federated Learning (DFL) enables nodes to collaboratively train
models without a central server, introducing new vulnerabilities since each
node independently selects peers for model aggregation. Malicious nodes may
exploit this autonomy by sending corrupted models (model poisoning), delaying
model submissions (delay attack), or flooding the network with excessive
messages, negatively affecting system performance. Existing solutions often
depend on rigid configurations or additional infrastructures such as
blockchain, leading to computational overhead, scalability issues, or limited
adaptability. To overcome these limitations, this paper proposes RepuNet, a
decentralized reputation system that categorizes threats in DFL and dynamically
evaluates node behavior using metrics like model similarity, parameter changes,
message latency, and communication volume. Nodes' influence in model
aggregation is adjusted based on their reputation scores. RepuNet was
integrated into the Nebula DFL platform and experimentally evaluated with MNIST
and CIFAR-10 datasets under non-IID distributions, using federations of up to
25 nodes in both fully connected and random topologies. Different attack
intensities, frequencies, and activation intervals were tested. Results
demonstrated that RepuNet effectively detects and mitigates malicious behavior,
achieving F1 scores above 95% for MNIST scenarios and approximately 76% for
CIFAR-10 cases. These outcomes highlight RepuNet's adaptability, robustness,
and practical potential for mitigating threats in decentralized federated
learning environments.

</details>


### [240] [Blameless Users in a Clean Room: Defining Copyright Protection for Generative Models](https://arxiv.org/abs/2506.19881)
*Aloni Cohen*

Main category: cs.CR

TL;DR: 本文重新审视了生成模型的版权保护问题，指出NAF不足以防止侵权，并提出新的无责复制保护框架，证明差分隐私在特定条件下可实现版权保护。


<details>
  <summary>Details</summary>
Motivation: 探讨生成模型输出是否可能侵犯训练数据版权，并建立更坚实的技术和法律基础。

Method: 分析NAF的局限性，提出无责复制保护框架，并引入“干净房间”复制保护概念。

Result: 证明NAF可能允许逐字复制，而差分隐私在满足“黄金数据集”条件时可实现版权保护。

Conclusion: 无责复制保护框架为版权保护提供了更可靠的保证，差分隐私在特定条件下是有效的解决方案。

Abstract: Are there any conditions under which a generative model's outputs are
guaranteed not to infringe the copyrights of its training data? This is the
question of "provable copyright protection" first posed by Vyas, Kakade, and
Barak (ICML 2023). They define near access-freeness (NAF) and propose it as
sufficient for protection. This paper revisits the question and establishes new
foundations for provable copyright protection -- foundations that are firmer
both technically and legally. First, we show that NAF alone does not prevent
infringement. In fact, NAF models can enable verbatim copying, a blatant
failure of copy protection that we dub being tainted. Then, we introduce our
blameless copy protection framework for defining meaningful guarantees, and
instantiate it with clean-room copy protection. Clean-room copy protection
allows a user to control their risk of copying by behaving in a way that is
unlikely to copy in a counterfactual clean-room setting. Finally, we formalize
a common intuition about differential privacy and copyright by proving that DP
implies clean-room copy protection when the dataset is golden, a copyright
deduplication requirement.

</details>


### [241] [Diffusion-based Task-oriented Semantic Communications with Model Inversion Attack](https://arxiv.org/abs/2506.19886)
*Xuesong Wang,Mo Li,Xingyan Shi,Zhaoqian Liu,Shenghao Yang*

Main category: cs.CR

TL;DR: DiffSem是一种基于扩散的语义通信框架，通过自引用标签嵌入优化语义信息重建，提升任务性能，同时抵御模型反转攻击。


<details>
  <summary>Details</summary>
Motivation: 解决任务导向语义通信中隐私保护与任务准确性的平衡问题，以及传统图像质量指标在语义评估中的不足。

Method: 提出DiffSem框架，结合扩散机制和自引用标签嵌入，优化语义重建；引入新指标评估攻击者效果。

Result: 在MNIST数据集上，DiffSem分类准确率提升10.03%，并在动态信道中保持稳定性能。

Conclusion: DiffSem有效提升任务性能与隐私保护，同时揭示传统图像质量指标与语义信息泄露的偏差。

Abstract: Semantic communication has emerged as a promising neural network-based system
design for 6G networks. Task-oriented semantic communication is a novel
paradigm whose core goal is to efficiently complete specific tasks by
transmitting semantic information, optimizing communication efficiency and task
performance. The key challenge lies in preserving privacy while maintaining
task accuracy, as this scenario is susceptible to model inversion attacks. In
such attacks, adversaries can restore or even reconstruct input data by
analyzing and processing model outputs, owing to the neural network-based
nature of the systems. In addition, traditional systems use image quality
indicators (such as PSNR or SSIM) to assess attack severity, which may be
inadequate for task-oriented semantic communication, since visual differences
do not necessarily ensure semantic divergence. In this paper, we propose a
diffusion-based semantic communication framework, named DiffSem, that optimizes
semantic information reconstruction through a diffusion mechanism with
self-referential label embedding to significantly improve task performance. Our
model also compensates channel noise and adopt semantic information distortion
to ensure the robustness of the system in various signal-to-noise ratio
environments. To evaluate the attacker's effectiveness, we propose a new metric
that better quantifies the semantic fidelity of estimations from the adversary.
Experimental results based on this criterion show that on the MNIST dataset,
DiffSem improves the classification accuracy by 10.03%, and maintain stable
performance under dynamic channels. Our results further demonstrate that
significant deviation exists between traditional image quality indicators and
the leakage of task-relevant semantic information.

</details>


### [242] [Can One Safety Loop Guard Them All? Agentic Guard Rails for Federated Computing](https://arxiv.org/abs/2506.20000)
*Narasimha Raghavan Veeraragavan,Jan Franz Nygård*

Main category: cs.CR

TL;DR: Guardian-FC是一个新颖的两层框架，用于隐私保护的联邦计算，统一了多种隐私保护机制的安全执行。


<details>
  <summary>Details</summary>
Motivation: 解决不同隐私保护机制（如FHE、MPC和DP）在联邦计算中的安全执行问题，提供统一的风险管理和审计能力。

Method: 通过后端中立的领域特定语言（DSL）和执行提供者（EPs）解耦安全护栏与隐私机制，利用Agentic-AI控制平面实现安全循环。

Result: 提出了支持快速失败任务准入和无缝扩展的manifest-centric设计，并展示了后端无关的安全性和验证基础。

Conclusion: Guardian-FC为隐私保护的联邦计算提供了灵活且可扩展的框架，并提出了未来研究方向。

Abstract: We propose Guardian-FC, a novel two-layer framework for privacy preserving
federated computing that unifies safety enforcement across diverse privacy
preserving mechanisms, including cryptographic back-ends like fully homomorphic
encryption (FHE) and multiparty computation (MPC), as well as statistical
techniques such as differential privacy (DP). Guardian-FC decouples guard-rails
from privacy mechanisms by executing plug-ins (modular computation units),
written in a backend-neutral, domain-specific language (DSL) designed
specifically for federated computing workflows and interchangeable Execution
Providers (EPs), which implement DSL operations for various privacy back-ends.
An Agentic-AI control plane enforces a finite-state safety loop through signed
telemetry and commands, ensuring consistent risk management and auditability.
The manifest-centric design supports fail-fast job admission and seamless
extensibility to new privacy back-ends. We present qualitative scenarios
illustrating backend-agnostic safety and a formal model foundation for
verification. Finally, we outline a research agenda inviting the community to
advance adaptive guard-rail tuning, multi-backend composition, DSL
specification development, implementation, and compiler extensibility alongside
human-override usability.

</details>


### [243] [SV-LLM: An Agentic Approach for SoC Security Verification using Large Language Models](https://arxiv.org/abs/2506.20415)
*Dipayan Saha,Shams Tarek,Hasan Al Shaikh,Khan Thamid Hasan,Pavan Sai Nalluri,Md. Ajoad Hasan,Nashmin Alam,Jingbo Zhou,Sujan Kumar Saha,Mark Tehranipoor,Farimah Farahmandi*

Main category: cs.CR

TL;DR: SV-LLM是一个多代理系统，利用大型语言模型（LLMs）自动化SoC安全验证，通过多代理协作提高效率和准确性。


<details>
  <summary>Details</summary>
Motivation: 传统SoC安全验证方法在自动化、扩展性和适应性方面存在不足，LLMs的先进能力为解决这些问题提供了新思路。

Method: SV-LLM采用多代理架构，结合上下文学习、微调和检索增强生成（RAG）等技术，优化各代理在验证任务中的表现。

Result: 实验和案例研究表明，SV-LLM能够减少人工干预，提高安全分析的准确性和速度。

Conclusion: SV-LLM展示了利用LLMs和多代理系统改进硬件安全实践的潜力。

Abstract: Ensuring the security of complex system-on-chips (SoCs) designs is a critical
imperative, yet traditional verification techniques struggle to keep pace due
to significant challenges in automation, scalability, comprehensiveness, and
adaptability. The advent of large language models (LLMs), with their remarkable
capabilities in natural language understanding, code generation, and advanced
reasoning, presents a new paradigm for tackling these issues. Moving beyond
monolithic models, an agentic approach allows for the creation of multi-agent
systems where specialized LLMs collaborate to solve complex problems more
effectively. Recognizing this opportunity, we introduce SV-LLM, a novel
multi-agent assistant system designed to automate and enhance SoC security
verification. By integrating specialized agents for tasks like verification
question answering, security asset identification, threat modeling, test plan
and property generation, vulnerability detection, and simulation-based bug
validation, SV-LLM streamlines the workflow. To optimize their performance in
these diverse tasks, agents leverage different learning paradigms, such as
in-context learning, fine-tuning, and retrieval-augmented generation (RAG). The
system aims to reduce manual intervention, improve accuracy, and accelerate
security analysis, supporting proactive identification and mitigation of risks
early in the design cycle. We demonstrate its potential to transform hardware
security practices through illustrative case studies and experiments that
showcase its applicability and efficacy.

</details>


### [244] [Attack Smarter: Attention-Driven Fine-Grained Webpage Fingerprinting Attacks](https://arxiv.org/abs/2506.20082)
*Yali Yuan,Weiyi Zou,Guang Cheng*

Main category: cs.CR

TL;DR: 论文提出了一种名为ADWPF的注意力驱动细粒度网页指纹攻击方法，用于解决大规模环境中用户访问多子页面和多标签浏览时的分类难题。


<details>
  <summary>Details</summary>
Motivation: 现有网站指纹攻击（WF）主要局限于小规模场景，难以处理用户快速访问多子页面和多标签浏览的复杂情况。

Method: 提出ADWPF方法，通过注意力驱动的数据增强、低维特征提取和自注意力模块捕捉全局上下文模式，并利用残差注意力处理多标签场景。

Result: 实验表明，ADWPF在不同规模数据集上均优于现有基线方法。

Conclusion: ADWPF有效解决了大规模网页指纹攻击中的分类挑战，显著提升了性能。

Abstract: Website Fingerprinting (WF) attacks aim to infer which websites a user is
visiting by analyzing traffic patterns, thereby compromising user anonymity.
Although this technique has been demonstrated to be effective in controlled
experimental environments, it remains largely limited to small-scale scenarios,
typically restricted to recognizing website homepages. In practical settings,
however, users frequently access multiple subpages in rapid succession, often
before previous content fully loads. WebPage Fingerprinting (WPF) generalizes
the WF framework to large-scale environments by modeling subpages of the same
site as distinct classes. These pages often share similar page elements,
resulting in lower inter-class variance in traffic features. Furthermore, we
consider multi-tab browsing scenarios, in which a single trace encompasses
multiple categories of webpages. This leads to overlapping traffic segments,
and similar features may appear in different positions within the traffic,
thereby increasing the difficulty of classification. To address these
challenges, we propose an attention-driven fine-grained WPF attack, named
ADWPF. Specifically, during the training phase, we apply targeted augmentation
to salient regions of the traffic based on attention maps, including attention
cropping and attention masking. ADWPF then extracts low-dimensional features
from both the original and augmented traffic and applies self-attention modules
to capture the global contextual patterns of the trace. Finally, to handle the
multi-tab scenario, we employ the residual attention to generate class-specific
representations of webpages occurring at different temporal positions.
Extensive experiments demonstrate that the proposed method consistently
surpasses state-of-the-art baselines across datasets of different scales.

</details>


### [245] [Autonomous Cyber Resilience via a Co-Evolutionary Arms Race within a Fortified Digital Twin Sandbox](https://arxiv.org/abs/2506.20102)
*Malikussaid,Sutiyo*

Main category: cs.CR

TL;DR: 论文提出ARC框架，通过自主闭环强化过程实现分析韧性，利用红蓝代理的协同进化动态提升关键基础设施的动态安全防御能力。


<details>
  <summary>Details</summary>
Motivation: IT与OT的融合使关键基础设施暴露于新型智能攻击下，静态防御失效，需解决系统模型保真度、数据同步完整性和分析引擎韧性等信任问题。

Method: 引入ARC框架，通过红代理（DRL代理）自主发现隐蔽攻击路径，蓝代理（集成防御）通过对抗训练持续强化，形成协同进化。

Result: 在TEP和SWaT测试平台上验证了框架的优越性能，协同进化显著提升了对新型攻击的检测能力。

Conclusion: ARC不仅是改进，更是向动态自改进安全防御的必要范式转变，适用于未来关键基础设施。

Abstract: The convergence of IT and OT has created hyper-connected ICS, exposing
critical infrastructure to a new class of adaptive, intelligent adversaries
that render static defenses obsolete. Existing security paradigms often fail to
address a foundational "Trinity of Trust," comprising the fidelity of the
system model, the integrity of synchronizing data, and the resilience of the
analytical engine against sophisticated evasion. This paper introduces the ARC
framework, a method for achieving analytical resilience through an autonomous,
closed-loop hardening process. ARC establishes a perpetual co-evolutionary arms
race within the high-fidelity sandbox of a F-SCDT. A DRL agent, the "Red
Agent," is formalized and incentivized to autonomously discover stealthy,
physically-plausible attack paths that maximize process disruption while
evading detection. Concurrently, an ensemble-based "Blue Agent" defender is
continuously hardened via adversarial training against the evolving threats
discovered by its adversary. This co-evolutionary dynamic forces both agents to
become progressively more sophisticated, enabling the system to autonomously
probe and patch its own vulnerabilities. Experimental validation on both the
TEP and the SWaT testbeds demonstrates the framework's superior performance. A
comprehensive ablation study, supported by extensive visualizations including
ROC curves and SHAP plots, reveals that the co-evolutionary process itself is
responsible for a significant performance increase in detecting novel attacks.
By integrating XAI to ensure operator trust and proposing a scalable F-ARC
architecture, this work presents ARC not merely as an improvement, but as a
necessary paradigm shift toward dynamic, self-improving security for the future
of critical infrastructure.

</details>


### [246] [Vulnerability Disclosure through Adaptive Black-Box Adversarial Attacks on NIDS](https://arxiv.org/abs/2506.20576)
*Sabrine Ennaji,Elhadj Benkhelifa,Luigi V. Mancini*

Main category: cs.CR

TL;DR: 提出了一种针对黑盒对抗攻击的新方法，通过自适应特征选择和因果分析，减少交互并提高实用性。


<details>
  <summary>Details</summary>
Motivation: 现有对抗攻击方法在结构化数据（如网络流量）中难以实用，且缺乏可重复性，导致防御措施无法应对不断演变的攻击。

Method: 采用自适应特征选择策略，结合变点检测和因果分析，以低计算成本识别敏感特征。

Result: 实验证明该方法能有效规避检测，且交互次数少，适用于实际场景。

Conclusion: 该方法为网络流量中的对抗攻击研究提供了新思路，有助于开发更鲁棒的防御措施。

Abstract: Adversarial attacks, wherein slight inputs are carefully crafted to mislead
intelligent models, have attracted increasing attention. However, a critical
gap persists between theoretical advancements and practical application,
particularly in structured data like network traffic, where interdependent
features complicate effective adversarial manipulations. Moreover, ambiguity in
current approaches restricts reproducibility and limits progress in this field.
Hence, existing defenses often fail to handle evolving adversarial attacks.
This paper proposes a novel approach for black-box adversarial attacks, that
addresses these limitations. Unlike prior work, which often assumes system
access or relies on repeated probing, our method strictly respect black-box
constraints, reducing interaction to avoid detection and better reflect
real-world scenarios. We present an adaptive feature selection strategy using
change-point detection and causality analysis to identify and target sensitive
features to perturbations. This lightweight design ensures low computational
cost and high deployability. Our comprehensive experiments show the attack's
effectiveness in evading detection with minimal interaction, enhancing its
adaptability and applicability in real-world scenarios. By advancing the
understanding of adversarial attacks in network traffic, this work lays a
foundation for developing robust defenses.

</details>


<div id='math.OC'></div>

# math.OC [[Back]](#toc)

### [247] [Finite-Time Information-Theoretic Bounds in Queueing Control](https://arxiv.org/abs/2506.18278)
*Yujie Liu,Vincent Y. F. Tan,Yunbei Xu*

Main category: math.OC

TL;DR: 本文提出了调度问题中有限时间信息论下界，并开发了达到这些下界的新策略，揭示了MaxWeight在有限时间内的局限性。


<details>
  <summary>Details</summary>
Motivation: 研究调度问题在随机处理网络中的有限时间性能，填补MaxWeight仅保证稳定性和渐近最优性的不足。

Method: 采用极小极大框架、信息论下界分析，并提出新的调度规则以最小化Lyapunov漂移。

Result: 证明MaxWeight在有限时间内存在次优性，新策略在特定条件下匹配下界。

Conclusion: 揭示了'仅漂移'方法的局限性，为非渐近最优性提供了方向。

Abstract: We establish the first finite-time information-theoretic lower bounds-and
derive new policies that achieve them-for the total queue length in scheduling
problems over stochastic processing networks with both adversarial and
stochastic arrivals. Prior analyses of MaxWeight guarantee only stability and
asymptotic optimality in heavy traffic; we prove that, at finite horizons,
MaxWeight can incur strictly larger backlog by problem-dependent factors which
we identify. Our main innovations are 1) a minimax framework that pinpoints the
precise problem parameters governing any policy's finite-time performance; 2)
an information-theoretic lower bound on total queue length; 3) fundamental
limitation of MaxWeight that it is suboptimal in finite time; and 4) a new
scheduling rule that minimizes the full Lyapunov drift-including its
second-order term-thereby matching the lower bound under certain conditions, up
to universal constants. These findings reveal a fundamental limitation on
"drift-only" methods and points the way toward principled, non-asymptotic
optimality in queueing control.

</details>


### [248] [A Complete Loss Landscape Analysis of Regularized Deep Matrix Factorization](https://arxiv.org/abs/2506.20344)
*Po Chen,Rujun Jiang,Peng Wang*

Main category: math.OC

TL;DR: 该论文研究了深度矩阵分解（DMF）的优化基础，分析了其损失景观，并提供了临界点的分类条件，解释了梯度方法通常收敛到局部极小值的原因。


<details>
  <summary>Details</summary>
Motivation: 尽管DMF在多个领域有广泛应用，但其优化基础尚未充分研究。本文旨在填补这一空白，全面分析正则化DMF问题的损失景观。

Method: 首先给出所有临界点的闭式表达式，然后建立临界点分类的精确条件（如局部极小值、全局极小值、严格鞍点等），并通过数值实验验证理论。

Result: 提出了临界点分类的必要和充分条件，解释了梯度方法通常收敛到局部极小值的原因，并通过实验支持了理论。

Conclusion: 该研究为DMF的优化提供了理论基础，解释了梯度方法的收敛行为，并通过实验验证了理论的正确性。

Abstract: Despite its wide range of applications across various domains, the
optimization foundations of deep matrix factorization (DMF) remain largely
open. In this work, we aim to fill this gap by conducting a comprehensive study
of the loss landscape of the regularized DMF problem. Toward this goal, we
first provide a closed-form expression of all critical points. Building on
this, we establish precise conditions under which a critical point is a local
minimizer, a global minimizer, a strict saddle point, or a non-strict saddle
point. Leveraging these results, we derive a necessary and sufficient condition
under which each critical point is either a local minimizer or a strict saddle
point. This provides insights into why gradient-based methods almost always
converge to a local minimizer of the regularized DMF problem. Finally, we
conduct numerical experiments to visualize its loss landscape under different
settings to support our theory.

</details>


### [249] [First-order methods for stochastic and finite-sum convex optimization with deterministic constraints](https://arxiv.org/abs/2506.20630)
*Zhaosong Lu,Yifeng Xiao*

Main category: math.OC

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: In this paper, we study a class of stochastic and finite-sum convex
optimization problems with deterministic constraints. Existing methods
typically aim to find an $\epsilon$-$expectedly\ feasible\ stochastic\ optimal$
solution, in which the expected constraint violation and expected optimality
gap are both within a prescribed tolerance $\epsilon$. However, in many
practical applications, constraints must be nearly satisfied with certainty,
rendering such solutions potentially unsuitable due to the risk of substantial
violations. To address this issue, we propose stochastic first-order methods
for finding an $\epsilon$-$surely\ feasible\ stochastic\ optimal$
($\epsilon$-SFSO) solution, where the constraint violation is deterministically
bounded by $\epsilon$ and the expected optimality gap is at most $\epsilon$.
Our methods apply an accelerated stochastic gradient (ASG) scheme or a modified
variance-reduced ASG scheme $only\ once$ to a sequence of quadratic penalty
subproblems with appropriately chosen penalty parameters. We establish
first-order oracle complexity bounds for the proposed methods in computing an
$\epsilon$-SFSO solution. As a byproduct, we also derive first-order oracle
complexity results for sample average approximation method in computing an
$\epsilon$-SFSO solution of the stochastic optimization problem using our
proposed methods to solve the sample average problem.

</details>


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [250] [A Multi-Modal Spatial Risk Framework for EV Charging Infrastructure Using Remote Sensing](https://arxiv.org/abs/2506.19860)
*Oktay Karakuş,Padraig Corcoran*

Main category: eess.SP

TL;DR: 本文提出了RSERI-EV框架，结合多源数据和空间图分析，评估电动汽车充电站的脆弱性，并在威尔士数据集上验证其可行性。


<details>
  <summary>Details</summary>
Motivation: 电动汽车充电基础设施在可持续交通系统中日益重要，但其在环境和基础设施压力下的韧性研究不足。

Method: 结合遥感数据、开放基础设施数据集和空间图分析，生成综合韧性评分，并构建空间k近邻图进行比较和诊断。

Result: 在威尔士数据集上验证了框架的可行性，展示了多源数据融合和空间推理的价值。

Conclusion: RSERI-EV框架支持气候韧性和基础设施感知的电动汽车部署。

Abstract: Electric vehicle (EV) charging infrastructure is increasingly critical to
sustainable transport systems, yet its resilience under environmental and
infrastructural stress remains underexplored. In this paper, we introduce
RSERI-EV, a spatially explicit and multi-modal risk assessment framework that
combines remote sensing data, open infrastructure datasets, and spatial graph
analytics to evaluate the vulnerability of EV charging stations. RSERI-EV
integrates diverse data layers, including flood risk maps, land surface
temperature (LST) extremes, vegetation indices (NDVI), land use/land cover
(LULC), proximity to electrical substations, and road accessibility to generate
a composite Resilience Score. We apply this framework to the country of Wales
EV charger dataset to demonstrate its feasibility. A spatial $k$-nearest
neighbours ($k$NN) graph is constructed over the charging network to enable
neighbourhood-based comparisons and graph-aware diagnostics. Our prototype
highlights the value of multi-source data fusion and interpretable spatial
reasoning in supporting climate-resilient, infrastructure-aware EV deployment.

</details>


### [251] [OLALa: Online Learned Adaptive Lattice Codes for Heterogeneous Federated Learning](https://arxiv.org/abs/2506.20297)
*Natalie Lang,Maya Simhi,Nir Shlezinger*

Main category: eess.SP

TL;DR: 论文提出了一种名为OLALa的联邦学习框架，通过在线学习调整量化器，优化通信效率和学习性能。


<details>
  <summary>Details</summary>
Motivation: 解决联邦学习中固定量化规则在异构动态环境中的不足，提升模型更新的适应性。

Method: 提出OLALa框架，客户端通过轻量级本地计算在线调整量化器，并仅交换紧凑的量化参数。

Result: 实验表明OLALa在多种量化率下优于传统固定码本和非自适应方案。

Conclusion: OLALa通过动态调整量化器，显著提升了联邦学习的性能和适应性。

Abstract: Federated learning (FL) enables collaborative training across distributed
clients without sharing raw data, often at the cost of substantial
communication overhead induced by transmitting high-dimensional model updates.
This overhead can be alleviated by having the clients quantize their model
updates, with dithered lattice quantizers identified as an attractive scheme
due to its structural simplicity and convergence-preserving properties.
However, existing lattice-based FL schemes typically rely on a fixed
quantization rule, which is suboptimal in heterogeneous and dynamic
environments where the model updates distribution varies across users and
training rounds. In this work, we propose Online Learned Adaptive Lattices
(OLALa), a heterogeneous FL framework where each client can adjust its
quantizer online using lightweight local computations. We first derive
convergence guarantees for FL with non-fixed lattice quantizers and show that
proper lattice adaptation can tighten the convergence bound. Then, we design an
online learning algorithm that enables clients to tune their quantizers
throughout the FL process while exchanging only a compact set of quantization
parameters. Numerical experiments demonstrate that OLALa consistently improves
learning performance under various quantization rates, outperforming
conventional fixed-codebook and non-adaptive schemes.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [252] [MILAAP: Mobile Link Allocation via Attention-based Prediction](https://arxiv.org/abs/2506.19947)
*Yung-Fu Chen,Anish Arora*

Main category: cs.NI

TL;DR: MiLAAP是一种基于学习的注意力框架，用于预测无线网络中信道占用状态，无需节点间状态共享，显著减少通信开销。


<details>
  <summary>Details</summary>
Motivation: 在无线网络中，信道跳频通信系统需要适应干扰变化和节点移动，但状态共享会带来高通信开销，降低吞吐效率。

Method: 提出MiLAAP框架，利用自注意力机制预测信道占用状态和节点移动轨迹，仅依赖本地被动观测数据。

Result: MiLAAP在动态网络中预测准确率接近100%，且具有零样本泛化能力。

Conclusion: MiLAAP通过本地学习和预测有效解决了信道调度中的通信开销问题，适用于动态网络。

Abstract: Channel hopping (CS) communication systems must adapt to interference changes
in the wireless network and to node mobility for maintaining throughput
efficiency. Optimal scheduling requires up-to-date network state information
(i.e., of channel occupancy) to select non-overlapping channels for links in
interference regions. However, state sharing among nodes introduces significant
communication overhead, especially as network size or node mobility scale,
thereby decreasing throughput efficiency of already capacity-limited networks.
In this paper, we eschew state sharing while adapting the CS schedule based on
a learning-based channel occupancy prediction. We propose the MiLAAP
attention-based prediction framework for machine learning models of spectral,
spatial, and temporal dependencies among network nodes. MiLAAP uses a
self-attention mechanism that lets each node capture the temporospectral CS
pattern in its interference region and accordingly predict the channel
occupancy state within that region. Notably, the prediction relies only on
locally and passively observed channel activities, and thus introduces no
communication overhead. To deal with node mobility, MiLAAP also uses a
multi-head self-attention mechanism that lets each node locally capture the
spatiotemporal dependencies on other network nodes that can interfere with it
and accordingly predict the motion trajectory of those nodes. Detecting nodes
that enter or move outside the interference region is used to further improve
the prediction accuracy of channel occupancy. We show that for dynamic networks
that use local CS sequences to support relatively long-lived flow traffics, the
channel state prediction accuracy of MiLAAP is remarkably ~100% across
different node mobility patterns and it achieves zero-shot generalizability
across different periods of CS sequences.

</details>


<div id='physics.optics'></div>

# physics.optics [[Back]](#toc)

### [253] [Machine-Learning-Assisted Photonic Device Development: A Multiscale Approach from Theory to Characterization](https://arxiv.org/abs/2506.20056)
*Yuheng Chen,Alexander Montes McNeil,Taehyuk Park,Blake A. Wilson,Vaishnavi Iyer,Michael Bezick,Jae-Ik Choi,Rohan Ojha,Pravin Mahendran,Daksh Kumar Singh,Geetika Chitturi,Peigang Chen,Trang Do,Alexander V. Kildishev,Vladimir M. Shalaev,Michael Moebius,Wenshan Cai,Yongmin Liu,Alexandra Boltasseva*

Main category: physics.optics

TL;DR: 机器学习辅助光子器件开发（ML-PDD）通过数据驱动方法解决传统优化和制造中的计算和成本问题。


<details>
  <summary>Details</summary>
Motivation: 传统光子器件开发（PDD）面临计算复杂、成本高和规模化难题，机器学习提供了新的解决方案。

Method: 利用替代估计器加速计算，生成建模处理噪声数据，强化学习优化制造，主动学习支持实验发现。

Result: ML-PDD实现了高效设计优化、快速模拟和噪声下的表征建模，以及强化学习驱动的制造。

Conclusion: ML-PDD为跨学科研究提供了新视角，加速复杂光子器件和系统的开发。

Abstract: Photonic device development (PDD) has achieved remarkable success in
designing and implementing new devices for controlling light across various
wavelengths, scales, and applications, including telecommunications, imaging,
sensing, and quantum information processing. PDD is an iterative, five-step
process that consists of: i) deriving device behavior from design parameters,
ii) simulating device performance, iii) finding the optimal candidate designs
from simulations, iv) fabricating the optimal device, and v) measuring device
performance. Classically, all these steps involve Bayesian optimization,
material science, control theory, and direct physics-driven numerical methods.
However, many of these techniques are computationally intractable, monetarily
costly, or difficult to implement at scale. In addition, PDD suffers from large
optimization landscapes, uncertainties in structural or optical
characterization, and difficulties in implementing robust fabrication
processes. However, the advent of machine learning over the past decade has
provided novel, data-driven strategies for tackling these challenges, including
surrogate estimators for speeding up computations, generative modeling for
noisy measurement modeling and data augmentation, reinforcement learning for
fabrication, and active learning for experimental physical discovery. In this
review, we present a comprehensive perspective on these methods to enable
machine-learning-assisted PDD (ML-PDD) for efficient design optimization with
powerful generative models, fast simulation and characterization modeling under
noisy measurements, and reinforcement learning for fabrication. This review
will provide researchers from diverse backgrounds with valuable insights into
this emerging topic, fostering interdisciplinary efforts to accelerate the
development of complex photonic devices and systems.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [254] [Can LLMs Replace Humans During Code Chunking?](https://arxiv.org/abs/2506.19897)
*Christopher Glasz,Emily Escamilla,Eric O. Scott,Anand Patel,Jacob Zimmer,Colin Diggs,Michael Doyle,Scott Rosen,Nitin Naik,Justin F. Brunelle,Samruddhi Thaker,Parthav Poudel,Arun Sridharan,Amit Madan,Doug Wendt,William Macke,Thomas Schill*

Main category: cs.SE

TL;DR: 论文探讨了LLMs在政府遗留代码现代化中的应用，解决了输入限制问题，并验证了代码分块方法对文档生成质量的影响。


<details>
  <summary>Details</summary>
Motivation: 政府企业软件常使用遗留语言（如MUMPS或ALC），现有LLMs未充分解决其独特挑战，如上下文窗口限制和对遗留语言的理解不足。

Method: 研究了多种代码分块方法，优化遗留代码文件的模块注释生成，评估了不同LLMs（如GPT-4o、Claude 3 Sonnet等）的表现。

Result: LLMs选择的分区点与人类专家接近，分块方法显著影响文档生成质量，LLM生成的分区注释比人工分区更准确和有用。

Conclusion: LLMs可作为人类分区的替代方案，适用于大型代码库的现代化过程。

Abstract: Large language models (LLMs) have become essential tools in computer science,
especially for tasks involving code understanding and generation. However,
existing work does not address many of the unique challenges presented by code
written for government applications. In particular, government enterprise
software is often written in legacy languages like MUMPS or assembly language
code (ALC) and the overall token lengths of these systems exceed the context
window size for current commercially available LLMs. Additionally, LLMs are
primarily trained on modern software languages and have undergone limited
testing with legacy languages, making their ability to understand legacy
languages unknown and, hence, an area for empirical study. This paper examines
the application of LLMs in the modernization of legacy government code written
in ALC and MUMPS, addressing the challenges of input limitations. We
investigate various code-chunking methods to optimize the generation of summary
module comments for legacy code files, evaluating the impact of code-chunking
methods on the quality of documentation produced by different LLMs, including
GPT-4o, Claude 3 Sonnet, Mixtral, and Llama 3. Our results indicate that LLMs
can select partition points closely aligned with human expert partitioning. We
also find that chunking approaches have significant impact on downstream tasks
such as documentation generation. LLM-created partitions produce comments that
are up to 20% more factual and up to 10% more useful than when humans create
partitions. Therefore, we conclude that LLMs can be used as suitable
replacements for human partitioning of large codebases during LLM-aided
modernization.

</details>


### [255] [AI and Agile Software Development: From Frustration to Success -- XP2025 Workshop Summary](https://arxiv.org/abs/2506.20159)
*Tomas Herda,Victoria Pichler,Zheying Zhang,Pekka Abrahamsson,Geir K. Hanssen*

Main category: cs.SE

TL;DR: 研讨会探讨了将AI融入敏捷开发的挑战与机遇，提出了研究路线图以指导未来工作。


<details>
  <summary>Details</summary>
Motivation: 解决AI与敏捷开发结合中的实际问题，如工具、治理、数据质量和技能缺口。

Method: 通过互动环节系统识别、优先排序并分析问题根源。

Result: 制定了包含短期解决方案和长期目标的研究路线图。

Conclusion: 研讨会成果为行业与学术合作提供了结构化议程，推动从问题到实施的转化。

Abstract: The full-day workshop on AI and Agile at XP 2025 convened a diverse group of
researchers and industry practitioners to address the practical challenges and
opportunities of integrating Artificial Intelligence into Agile software
development. Through interactive sessions, participants identified shared
frustrations related to integrating AI into Agile Software Development
practices, including challenges with tooling, governance, data quality, and
critical skill gaps. These challenges were systematically prioritized and
analyzed to uncover root causes. The workshop culminated in the collaborative
development of a research roadmap that pinpoints actionable directions for
future work, including both immediate solutions and ambitious long-term goals.
The key outcome is a structured agenda designed to foster joint
industry-academic efforts to move from identified frustrations to successful
implementation.

</details>


### [256] [Large Language Model-Driven Code Compliance Checking in Building Information Modeling](https://arxiv.org/abs/2506.20551)
*Soumya Madireddy,Lu Gao,Zia Din,Kinam Kim,Ahmed Senouci,Zhe Han,Yunpeng Zhang*

Main category: cs.SE

TL;DR: 该研究提出了一种基于大型语言模型（LLM）的半自动化方法，用于BIM中的代码合规性检查，显著减少了时间和错误。


<details>
  <summary>Details</summary>
Motivation: 解决BIM中手动代码合规性检查耗时且易出错的问题。

Method: 整合GPT、Claude、Gemini和Llama等LLM与Revit软件，生成Python脚本并执行半自动化合规性检查。

Result: 案例研究表明，该系统减少了合规性检查的时间和精力，提高了准确性，并能识别违规行为。

Conclusion: 该方法为BIM合规性检查提供了全面、适应性强且经济高效的解决方案，具有广泛应用潜力。

Abstract: This research addresses the time-consuming and error-prone nature of manual
code compliance checking in Building Information Modeling (BIM) by introducing
a Large Language Model (LLM)-driven approach to semi-automate this critical
process. The developed system integrates LLMs such as GPT, Claude, Gemini, and
Llama, with Revit software to interpret building codes, generate Python
scripts, and perform semi-automated compliance checks within the BIM
environment. Case studies on a single-family residential project and an office
building project demonstrated the system's ability to reduce the time and
effort required for compliance checks while improving accuracy. It streamlined
the identification of violations, such as non-compliant room dimensions,
material usage, and object placements, by automatically assessing relationships
and generating actionable reports. Compared to manual methods, the system
eliminated repetitive tasks, simplified complex regulations, and ensured
reliable adherence to standards. By offering a comprehensive, adaptable, and
cost-effective solution, this proposed approach offers a promising advancement
in BIM-based compliance checking, with potential applications across diverse
regulatory documents in construction projects.

</details>


### [257] [Define-ML: An Approach to Ideate Machine Learning-Enabled Systems](https://arxiv.org/abs/2506.20621)
*Silvio Alonso,Antonio Pedro Santos Alves,Lucas Romao,Hélio Lopes,Marcos Kalinowski*

Main category: cs.SE

TL;DR: Define-ML框架扩展了Lean Inception，通过结构化活动整合数据和ML技术约束，提升早期ML产品构思的清晰度和可行性。


<details>
  <summary>Details</summary>
Motivation: 传统构思方法缺乏对ML特有挑战的支持，可能导致产品愿景与业务目标不匹配。Define-ML旨在解决这一问题。

Method: 基于技术转移模型开发，通过静态和动态验证（包括案例研究和定量定性分析）评估框架的实用性和易用性。

Result: 参与者认为Define-ML能有效澄清数据问题、对齐业务目标，并促进跨职能协作，但需专家指导降低学习曲线。

Conclusion: Define-ML为ML产品构思提供了开放且验证过的框架，结合敏捷性与技术可行性。

Abstract: [Context] The increasing adoption of machine learning (ML) in software
systems demands specialized ideation approaches that address ML-specific
challenges, including data dependencies, technical feasibility, and alignment
between business objectives and probabilistic system behavior. Traditional
ideation methods like Lean Inception lack structured support for these ML
considerations, which can result in misaligned product visions and unrealistic
expectations. [Goal] This paper presents Define-ML, a framework that extends
Lean Inception with tailored activities - Data Source Mapping, Feature-to-Data
Source Mapping, and ML Mapping - to systematically integrate data and technical
constraints into early-stage ML product ideation. [Method] We developed and
validated Define-ML following the Technology Transfer Model, conducting both
static validation (with a toy problem) and dynamic validation (in a real-world
industrial case study). The analysis combined quantitative surveys with
qualitative feedback, assessing utility, ease of use, and intent of adoption.
[Results] Participants found Define-ML effective for clarifying data concerns,
aligning ML capabilities with business goals, and fostering cross-functional
collaboration. The approach's structured activities reduced ideation ambiguity,
though some noted a learning curve for ML-specific components, which can be
mitigated by expert facilitation. All participants expressed the intention to
adopt Define-ML. [Conclusion] Define-ML provides an openly available, validated
approach for ML product ideation, building on Lean Inception's agility while
aligning features with available data and increasing awareness of technical
feasibility.

</details>


<div id='physics.flu-dyn'></div>

# physics.flu-dyn [[Back]](#toc)

### [258] [Reinforcement Learning Increases Wind Farm Power Production by Enabling Closed-Loop Collaborative Control](https://arxiv.org/abs/2506.20554)
*Andrew Mole,Max Weissenbacher,Georgios Rigas,Sylvain Laizet*

Main category: physics.flu-dyn

TL;DR: 论文提出了一种基于强化学习（RL）的动态闭环控制方法，通过高精度大涡模拟（LES）实现风电场协同优化，显著提升发电量。


<details>
  <summary>Details</summary>
Motivation: 传统风电场控制独立优化单机发电量，忽略尾流协同效应。动态闭环控制在流体控制中有效，但风电优化仍依赖静态低精度模拟。

Method: 结合强化学习与高精度大涡模拟，实现动态闭环控制，实时响应大气湍流。

Result: RL控制器使风电场发电量提升4.30%，优于静态最优偏航控制的2.19%增益。

Conclusion: 动态流响应控制是风电场优化的变革性方法，对加速可再生能源部署具有重要意义。

Abstract: Traditional wind farm control operates each turbine independently to maximize
individual power output. However, coordinated wake steering across the entire
farm can substantially increase the combined wind farm energy production.
Although dynamic closed-loop control has proven effective in flow control
applications, wind farm optimization has relied primarily on static,
low-fidelity simulators that ignore critical turbulent flow dynamics. In this
work, we present the first reinforcement learning (RL) controller integrated
directly with high-fidelity large-eddy simulation (LES), enabling real-time
response to atmospheric turbulence through collaborative, dynamic control
strategies. Our RL controller achieves a 4.30% increase in wind farm power
output compared to baseline operation, nearly doubling the 2.19% gain from
static optimal yaw control obtained through Bayesian optimization. These
results establish dynamic flow-responsive control as a transformative approach
to wind farm optimization, with direct implications for accelerating renewable
energy deployment to net-zero targets.

</details>


<div id='cs.IT'></div>

# cs.IT [[Back]](#toc)

### [259] [Exploration-Exploitation Tradeoff in Universal Lossy Compression](https://arxiv.org/abs/2506.20261)
*Nir Weinberger,Ram Zamir*

Main category: cs.IT

TL;DR: 将通用压缩中的顺序模式重新定义为多臂老虎机问题，研究探索与利用的权衡，提出并分析鲁棒的成本导向算法。


<details>
  <summary>Details</summary>
Motivation: 研究顺序模式下的压缩问题，探索探索与利用的权衡，改进现有方法的局限性。

Method: 将顺序模式重新定义为多臂老虎机问题，分析现有“自然类型选择”方案，并提出鲁棒的成本导向算法。

Result: 证明了现有方案可作为重建导向的MAB算法，但存在鲁棒性和短块性能问题；新算法在任何块长度下均有效。

Conclusion: 鲁棒的成本导向算法优于现有方法，适用于更广泛的压缩场景。

Abstract: Universal compression can learn the source and adapt to it either in a batch
mode (forward adaptation), or in a sequential mode (backward adaptation). We
recast the sequential mode as a multi-armed bandit problem, a fundamental model
in reinforcement-learning, and study the trade-off between exploration and
exploitation in the lossy compression case. We show that a previously proposed
"natural type selection" scheme can be cast as a reconstruction-directed MAB
algorithm, for sequential lossy compression, and explain its limitations in
terms of robustness and short-block performance. We then derive and analyze
robust cost-directed MAB algorithms, which work at any block length.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [260] [Enhanced Dermatology Image Quality Assessment via Cross-Domain Training](https://arxiv.org/abs/2506.16116)
*Ignacio Hernández Montilla,Alfonso Medela,Paola Pasquali,Andy Aguilar,Taig Mac Carthy,Gerardo Fernández,Antonio Martorell,Enrique Onieva*

Main category: eess.IV

TL;DR: 论文提出跨领域训练图像质量评估（IQA）模型，结合皮肤病学和非皮肤病学数据集，以解决远程皮肤病学中图像质量差的问题。


<details>
  <summary>Details</summary>
Motivation: 远程皮肤病学中图像质量差影响诊断效果，但现有皮肤病学IQA研究不足，未充分利用非皮肤病学IQA的最新进展。

Method: 创建新的皮肤病学IQA数据库Legit.Health-DIQA-Artificial，结合跨领域训练方法，利用更大规模的图像数据。

Result: 跨领域训练在多个领域表现最优，解决了皮肤病学IQA数据规模小的限制，提升了图像质量管理。

Conclusion: 跨领域训练方法有效提升了远程皮肤病学中的图像质量评估能力。

Abstract: Teledermatology has become a widely accepted communication method in daily
clinical practice, enabling remote care while showing strong agreement with
in-person visits. Poor image quality remains an unsolved problem in
teledermatology and is a major concern to practitioners, as bad-quality images
reduce the usefulness of the remote consultation process. However, research on
Image Quality Assessment (IQA) in dermatology is sparse, and does not leverage
the latest advances in non-dermatology IQA, such as using larger image
databases with ratings from large groups of human observers. In this work, we
propose cross-domain training of IQA models, combining dermatology and
non-dermatology IQA datasets. For this purpose, we created a novel dermatology
IQA database, Legit.Health-DIQA-Artificial, using dermatology images from
several sources and having them annotated by a group of human observers. We
demonstrate that cross-domain training yields optimal performance across
domains and overcomes one of the biggest limitations in dermatology IQA, which
is the small scale of data, and leads to models trained on a larger pool of
image distortions, resulting in a better management of image quality in the
teledermatology process.

</details>


### [261] [FundaQ-8: A Clinically-Inspired Scoring Framework for Automated Fundus Image Quality Assessment](https://arxiv.org/abs/2506.20303)
*Lee Qi Zun,Oscar Wong Jin Hao,Nor Anita Binti Che Omar,Zalifa Zakiah Binti Asnir,Mohamad Sabri bin Sinal Zainal,Goh Man Fye*

Main category: eess.IV

TL;DR: FundaQ-8是一个专家验证的框架，用于系统性评估眼底图像质量，基于八个关键参数。通过ResNet18回归模型预测质量分数，验证显示其可靠且临床可解释。


<details>
  <summary>Details</summary>
Motivation: 眼底图像质量评估（FIQA）因图像采集差异和专家主观评估而具有挑战性。

Method: 开发FundaQ-8框架，基于八个参数评估质量；使用ResNet18回归模型预测分数，训练数据来自临床和Kaggle数据集。

Result: 验证显示框架可靠且临床可解释，应用于糖尿病视网膜病变分级时提升诊断稳健性。

Conclusion: FundaQ-8框架结合深度学习模型可提升眼底图像质量评估和诊断的实用性。

Abstract: Automated fundus image quality assessment (FIQA) remains a challenge due to
variations in image acquisition and subjective expert evaluations. We introduce
FundaQ-8, a novel expert-validated framework for systematically assessing
fundus image quality using eight critical parameters, including field coverage,
anatomical visibility, illumination, and image artifacts. Using FundaQ-8 as a
structured scoring reference, we develop a ResNet18-based regression model to
predict continuous quality scores in the 0 to 1 range. The model is trained on
1800 fundus images from real-world clinical sources and Kaggle datasets, using
transfer learning, mean squared error optimization, and standardized
preprocessing. Validation against the EyeQ dataset and statistical analyses
confirm the framework's reliability and clinical interpretability.
Incorporating FundaQ-8 into deep learning models for diabetic retinopathy
grading also improves diagnostic robustness, highlighting the value of
quality-aware training in real-world screening applications.

</details>


### [262] [VoxelOpt: Voxel-Adaptive Message Passing for Discrete Optimization in Deformable Abdominal CT Registration](https://arxiv.org/abs/2506.19975)
*Hang Zhang,Yuxi Zhang,Jiazheng Wang,Xiang Chen,Renjiu Hu,Xin Tian,Gaolei Li,Min Liu*

Main category: eess.IV

TL;DR: VoxelOpt是一种结合学习与迭代方法的变形图像配准框架，通过位移熵和自适应消息传递优化配准精度与速度。


<details>
  <summary>Details</summary>
Motivation: 解决学习型方法在有限训练数据、大变形和无标签监督下的性能不足，以及迭代方法速度慢的问题。

Method: 利用位移熵测量信号强度，引入体素自适应消息传递、多级图像金字塔和预训练分割模型提取特征。

Result: 在腹部CT配准中，VoxelOpt在效率和精度上优于迭代方法，与有监督学习型方法相当。

Conclusion: VoxelOpt在配准精度和运行时间之间取得了更好的平衡，适用于实际应用。

Abstract: Recent developments in neural networks have improved deformable image
registration (DIR) by amortizing iterative optimization, enabling fast and
accurate DIR results. However, learning-based methods often face challenges
with limited training data, large deformations, and tend to underperform
compared to iterative approaches when label supervision is unavailable. While
iterative methods can achieve higher accuracy in such scenarios, they are
considerably slower than learning-based methods. To address these limitations,
we propose VoxelOpt, a discrete optimization-based DIR framework that combines
the strengths of learning-based and iterative methods to achieve a better
balance between registration accuracy and runtime. VoxelOpt uses displacement
entropy from local cost volumes to measure displacement signal strength at each
voxel, which differs from earlier approaches in three key aspects. First, it
introduces voxel-wise adaptive message passing, where voxels with lower entropy
receives less influence from their neighbors. Second, it employs a multi-level
image pyramid with 27-neighbor cost volumes at each level, avoiding exponential
complexity growth. Third, it replaces hand-crafted features or contrastive
learning with a pretrained foundational segmentation model for feature
extraction. In abdominal CT registration, these changes allow VoxelOpt to
outperform leading iterative in both efficiency and accuracy, while matching
state-of-the-art learning-based methods trained with label supervision. The
source code will be available at https://github.com/tinymilky/VoxelOpt

</details>


### [263] [MS-IQA: A Multi-Scale Feature Fusion Network for PET/CT Image Quality Assessment](https://arxiv.org/abs/2506.20200)
*Siqiao Li,Chen Hui,Wei Zhang,Rui Liang,Chenyue Song,Feng Jiang,Haiqi Zhu,Zhixuan Li,Hong Huang,Xiang Li*

Main category: eess.IV

TL;DR: 提出了一种多尺度特征融合网络MS-IQA，用于PET/CT图像质量评估，结合了ResNet和Swin Transformer的多尺度特征，并构建了PET-CT-IQA-DS数据集。


<details>
  <summary>Details</summary>
Motivation: 现有医学图像质量评估方法无法同时考虑低层次和高层次特征，导致诊断不确定性增加。

Method: 利用ResNet和Swin Transformer的多尺度特征，通过动态加权通道注意力机制融合高低层次信息。

Result: 在PET-CT-IQA-DS和LDCTIQAC2023数据集上表现优于现有方法。

Conclusion: MS-IQA为PET/CT提供了一种准确高效的图像质量评估方法。

Abstract: Positron Emission Tomography / Computed Tomography (PET/CT) plays a critical
role in medical imaging, combining functional and anatomical information to aid
in accurate diagnosis. However, image quality degradation due to noise,
compression and other factors could potentially lead to diagnostic uncertainty
and increase the risk of misdiagnosis. When evaluating the quality of a PET/CT
image, both low-level features like distortions and high-level features like
organ anatomical structures affect the diagnostic value of the image. However,
existing medical image quality assessment (IQA) methods are unable to account
for both feature types simultaneously. In this work, we propose MS-IQA, a novel
multi-scale feature fusion network for PET/CT IQA, which utilizes multi-scale
features from various intermediate layers of ResNet and Swin Transformer,
enhancing its ability of perceiving both local and global information. In
addition, a multi-scale feature fusion module is also introduced to effectively
combine high-level and low-level information through a dynamically weighted
channel attention mechanism. Finally, to fill the blank of PET/CT IQA dataset,
we construct PET-CT-IQA-DS, a dataset containing 2,700 varying-quality PET/CT
images with quality scores assigned by radiologists. Experiments on our dataset
and the publicly available LDCTIQAC2023 dataset demonstrate that our proposed
model has achieved superior performance against existing state-of-the-art
methods in various IQA metrics. This work provides an accurate and efficient
IQA method for PET/CT. Our code and dataset are available at
https://github.com/MS-IQA/MS-IQA/.

</details>


### [264] [Opportunistic Osteoporosis Diagnosis via Texture-Preserving Self-Supervision, Mixture of Experts and Multi-Task Integration](https://arxiv.org/abs/2506.20282)
*Jiaxing Huang,Heng Guo,Le Lu,Fan Yang,Minfeng Xu,Ge Yang,Wei Luo*

Main category: eess.IV

TL;DR: 提出了一种统一的深度学习框架，通过自监督学习、混合专家架构和多任务学习，解决了骨质疏松症诊断中的未标记数据利用、设备偏差和临床知识整合问题。


<details>
  <summary>Details</summary>
Motivation: 骨质疏松症诊断在资源有限地区面临挑战，现有方法未能充分利用未标记数据、存在设备偏差且缺乏临床知识整合。

Method: 采用自监督学习利用未标记CT数据，混合专家架构增强跨设备适应性，多任务学习整合诊断、BMD回归和椎骨定位。

Result: 在多个临床站点和外部医院验证中，该方法表现出优于现有方法的泛化能力和准确性。

Conclusion: 该框架为骨质疏松症的筛查和诊断提供了一种高效且通用的解决方案。

Abstract: Osteoporosis, characterized by reduced bone mineral density (BMD) and
compromised bone microstructure, increases fracture risk in aging populations.
While dual-energy X-ray absorptiometry (DXA) is the clinical standard for BMD
assessment, its limited accessibility hinders diagnosis in resource-limited
regions. Opportunistic computed tomography (CT) analysis has emerged as a
promising alternative for osteoporosis diagnosis using existing imaging data.
Current approaches, however, face three limitations: (1) underutilization of
unlabeled vertebral data, (2) systematic bias from device-specific DXA
discrepancies, and (3) insufficient integration of clinical knowledge such as
spatial BMD distribution patterns. To address these, we propose a unified deep
learning framework with three innovations. First, a self-supervised learning
method using radiomic representations to leverage unlabeled CT data and
preserve bone texture. Second, a Mixture of Experts (MoE) architecture with
learned gating mechanisms to enhance cross-device adaptability. Third, a
multi-task learning framework integrating osteoporosis diagnosis, BMD
regression, and vertebra location prediction. Validated across three clinical
sites and an external hospital, our approach demonstrates superior
generalizability and accuracy over existing methods for opportunistic
osteoporosis screening and diagnosis.

</details>


### [265] [EAGLE: An Efficient Global Attention Lesion Segmentation Model for Hepatic Echinococcosis](https://arxiv.org/abs/2506.20333)
*Jiayan Chen,Kai Li,Yulu Zhao,Jianqiang Huang,Zhan Wang*

Main category: eess.IV

TL;DR: 提出EAGLE网络，结合PVSS编码器和HVSS解码器，用于高效准确分割肝包虫病病灶，性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 解决肝包虫病在医疗资源匮乏地区的诊断问题，同时克服CNN和Transformer在医学图像分割中的局限性。

Method: 采用U形网络结构，结合PVSS编码器和HVSS解码器，引入CVSSB模块融合局部与全局特征，HWTB模块实现无损下采样。

Result: 在260例患者CT数据上，EAGLE的DSC达到89.76%，优于MSVM-UNet 1.61%。

Conclusion: EAGLE在肝包虫病病灶分割中表现出高效性和准确性，为医疗资源匮乏地区提供了潜在解决方案。

Abstract: Hepatic echinococcosis (HE) is a widespread parasitic disease in
underdeveloped pastoral areas with limited medical resources. While CNN-based
and Transformer-based models have been widely applied to medical image
segmentation, CNNs lack global context modeling due to local receptive fields,
and Transformers, though capable of capturing long-range dependencies, are
computationally expensive. Recently, state space models (SSMs), such as Mamba,
have gained attention for their ability to model long sequences with linear
complexity. In this paper, we propose EAGLE, a U-shaped network composed of a
Progressive Visual State Space (PVSS) encoder and a Hybrid Visual State Space
(HVSS) decoder that work collaboratively to achieve efficient and accurate
segmentation of hepatic echinococcosis (HE) lesions. The proposed Convolutional
Vision State Space Block (CVSSB) module is designed to fuse local and global
features, while the Haar Wavelet Transformation Block (HWTB) module compresses
spatial information into the channel dimension to enable lossless downsampling.
Due to the lack of publicly available HE datasets, we collected CT slices from
260 patients at a local hospital. Experimental results show that EAGLE achieves
state-of-the-art performance with a Dice Similarity Coefficient (DSC) of
89.76%, surpassing MSVM-UNet by 1.61%.

</details>


### [266] [Fusing Radiomic Features with Deep Representations for Gestational Age Estimation in Fetal Ultrasound Images](https://arxiv.org/abs/2506.20407)
*Fangyijie Wang,Yuan Liang,Sourav Bhattacharjee,Abey Campbell,Kathleen M. Curran,Guénolé Silvestre*

Main category: eess.IV

TL;DR: 提出了一种基于特征融合的框架，通过胎儿超声图像自动估计孕龄（GA），无需测量信息，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 手动测量胎儿生物特征依赖操作者且耗时，临床需要自动计算机辅助方法。

Method: 结合深度学习模型提取的深度表征和放射组学特征，融合估计GA。

Result: 在三孕期平均绝对误差为8.0天，优于现有机器学习方法，且在不同地理区域人群中表现稳健。

Conclusion: 该框架为GA估计提供了高效、自动化的解决方案，具有临床潜力。

Abstract: Accurate gestational age (GA) estimation, ideally through fetal ultrasound
measurement, is a crucial aspect of providing excellent antenatal care.
However, deriving GA from manual fetal biometric measurements depends on the
operator and is time-consuming. Hence, automatic computer-assisted methods are
demanded in clinical practice. In this paper, we present a novel feature fusion
framework to estimate GA using fetal ultrasound images without any measurement
information. We adopt a deep learning model to extract deep representations
from ultrasound images. We extract radiomic features to reveal patterns and
characteristics of fetal brain growth. To harness the interpretability of
radiomics in medical imaging analysis, we estimate GA by fusing radiomic
features and deep representations. Our framework estimates GA with a mean
absolute error of 8.0 days across three trimesters, outperforming current
machine learning-based methods at these gestational ages. Experimental results
demonstrate the robustness of our framework across different populations in
diverse geographical regions. Our code is publicly available on
\href{https://github.com/13204942/RadiomicsImageFusion_FetalUS}{GitHub}.

</details>


### [267] [Weighted Mean Frequencies: a handcraft Fourier feature for 4D Flow MRI segmentation](https://arxiv.org/abs/2506.20614)
*Simon Perrin,Sébastien Levilly,Huajun Sun,Harold Mouchère,Jean-Michel Serfaty*

Main category: eess.IV

TL;DR: 提出了一种名为WMF的新特征，用于改进4D Flow MRI图像的分割任务，实验显示其性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 4D Flow MRI图像的分辨率低和噪声问题影响了生物标志物的准确性，尤其是血管分割的质量。

Method: 引入WMF特征，通过加权平均频率揭示脉动流经过的三维区域，并采用最优阈值和深度学习方法进行分割实验。

Result: 实验结果显示，WMF特征在IoU和Dice指标上分别提高了0.12和0.13，优于PC-MRA方法。

Conclusion: WMF特征在血管分割中表现出显著优势，未来可应用于心脏或大脑等其他血管区域的分割任务。

Abstract: In recent decades, the use of 4D Flow MRI images has enabled the
quantification of velocity fields within a volume of interest and along the
cardiac cycle. However, the lack of resolution and the presence of noise in
these biomarkers are significant issues. As indicated by recent studies, it
appears that biomarkers such as wall shear stress are particularly impacted by
the poor resolution of vessel segmentation. The Phase Contrast Magnetic
Resonance Angiography (PC-MRA) is the state-of-the-art method to facilitate
segmentation. The objective of this work is to introduce a new handcraft
feature that provides a novel visualisation of 4D Flow MRI images, which is
useful in the segmentation task. This feature, termed Weighted Mean Frequencies
(WMF), is capable of revealing the region in three dimensions where a voxel has
been passed by pulsatile flow. Indeed, this feature is representative of the
hull of all pulsatile velocity voxels. The value of the feature under
discussion is illustrated by two experiments. The experiments involved
segmenting 4D Flow MRI images using optimal thresholding and deep learning
methods. The results obtained demonstrate a substantial enhancement in terms of
IoU and Dice, with a respective increase of 0.12 and 0.13 in comparison with
the PC-MRA feature, as evidenced by the deep learning task. This feature has
the potential to yield valuable insights that could inform future segmentation
processes in other vascular regions, such as the heart or the brain.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [268] [Capturing Visualization Design Rationale](https://arxiv.org/abs/2506.16571)
*Maeve Hutchinson,Radu Jianu,Aidan Slingsby,Jo Wood,Pranava Madhyastha*

Main category: cs.HC

TL;DR: 本文提出了一种新的数据集和方法，通过自然语言探究可视化设计背后的逻辑，利用学生创建的可视化笔记本作为数据源，并结合大型语言模型生成和验证问题-答案-逻辑三元组。


<details>
  <summary>Details</summary>
Motivation: 现有自然语言数据集多关注可视化解读而非设计逻辑，且多基于人为构建的场景。本文旨在填补这一空白，通过真实世界的可视化笔记本探究设计决策背后的逻辑。

Method: 利用学生创建的可视化笔记本作为数据源，结合大型语言模型生成问题-答案-逻辑三元组，并进行验证和整理。

Result: 构建了一个捕捉和提炼学生可视化设计选择及其逻辑的数据集。

Conclusion: 该方法为理解可视化设计逻辑提供了新视角，并为未来研究提供了真实世界的数据支持。

Abstract: Prior natural language datasets for data visualization have focused on tasks
such as visualization literacy assessment, insight generation, and
visualization generation from natural language instructions. These studies
often rely on controlled setups with purpose-built visualizations and
artificially constructed questions. As a result, they tend to prioritize the
interpretation of visualizations, focusing on decoding visualizations rather
than understanding their encoding. In this paper, we present a new dataset and
methodology for probing visualization design rationale through natural
language. We leverage a unique source of real-world visualizations and natural
language narratives: literate visualization notebooks created by students as
part of a data visualization course. These notebooks combine visual artifacts
with design exposition, in which students make explicit the rationale behind
their design decisions. We also use large language models (LLMs) to generate
and categorize question-answer-rationale triples from the narratives and
articulations in the notebooks. We then carefully validate the triples and
curate a dataset that captures and distills the visualization design choices
and corresponding rationales of the students.

</details>


### [269] [Beyond Autocomplete: Designing CopilotLens Towards Transparent and Explainable AI Coding Agents](https://arxiv.org/abs/2506.20062)
*Runlong Ye,Zeling Zhang,Boushra Almazroua,Michael Liut*

Main category: cs.HC

TL;DR: CopilotLens是一个交互式框架，通过透明化AI代码助手的决策过程，提升开发者对代码建议的理解和信任。


<details>
  <summary>Details</summary>
Motivation: 当前AI代码助手缺乏解释性，开发者难以评估其建议的合理性，影响了信任和协作。

Method: 引入CopilotLens框架，通过动态双层界面展示AI的决策过程，包括高层计划和代码库上下文。

Result: CopilotLens为开发者提供了透明的代码建议解释，增强了理解和信任。

Conclusion: CopilotLens为未来代码助手的设计提供了透明化和可解释性的范例，促进人机协作。

Abstract: AI-powered code assistants are widely used to generate code completions,
significantly boosting developer productivity. However, these tools typically
present suggestions without explaining their rationale, leaving their
decision-making process inscrutable. This opacity hinders developers' ability
to critically evaluate the output, form accurate mental models, and build
calibrated trust in the system. To address this, we introduce CopilotLens, a
novel interactive framework that reframes code completion from a simple
suggestion into a transparent, explainable event. CopilotLens operates as an
explanation layer that reveals the AI agent's "thought process" through a
dynamic two-level interface, surfacing everything from its reconstructed
high-level plans to the specific codebase context influencing the code. This
paper presents the design and rationale of CopilotLens, offering a concrete
framework for building future agentic code assistants that prioritize clarity
of reasoning over speed of suggestion, thereby fostering deeper comprehension
and more robust human-AI collaboration.

</details>


### [270] [Irec: A Metacognitive Scaffolding for Self-Regulated Learning through Just-in-Time Insight Recall: A Conceptual Framework and System Prototype](https://arxiv.org/abs/2506.20156)
*Xuefei Hou,Xizhao Tan*

Main category: cs.HC

TL;DR: 论文提出了一种名为“Insight Recall”的新范式，通过上下文触发的个人过去见解检索来支持自我调节学习（SRL），并开发了原型系统Irec。


<details>
  <summary>Details</summary>
Motivation: 现有数字工具在支持元认知反思方面不足，如间隔重复系统（SRS）缺乏上下文，而个人知识管理（PKM）工具需要高维护成本。

Method: 采用即时自适应干预（JITAI）框架，构建动态知识图谱和混合检索引擎，结合大语言模型（LLM）进行深度相似性评估，并设计了“引导探究”模块。

Result: 开发了Irec系统，展示了新范式的可行性，并提供了理论框架和系统平台。

Conclusion: 论文为设计增强元认知和自我调节的下一代智能学习系统提供了理论基础和实用工具。

Abstract: The core challenge in learning has shifted from knowledge acquisition to
effective Self-Regulated Learning (SRL): planning, monitoring, and reflecting
on one's learning. Existing digital tools, however, inadequately support
metacognitive reflection. Spaced Repetition Systems (SRS) use de-contextualized
review, overlooking the role of context, while Personal Knowledge Management
(PKM) tools require high manual maintenance.
  To address these challenges, this paper introduces "Insight Recall," a novel
paradigm that conceptualizes the context-triggered retrieval of personal past
insights as a metacognitive scaffold to promote SRL. We formalize this paradigm
using the Just-in-Time Adaptive Intervention (JITAI) framework and implement a
prototype system, Irec, to demonstrate its feasibility. At its core, Irec uses
a dynamic knowledge graph of the user's learning history. When a user faces a
new problem, a hybrid retrieval engine recalls relevant personal "insights."
Subsequently, a large language model (LLM) performs a deep similarity
assessment to filter and present the most relevant scaffold in a just-in-time
manner. To reduce cognitive load, Irec features a human-in-the-loop pipeline
for LLM-based knowledge graph construction. We also propose an optional "Guided
Inquiry" module, where users can engage in a Socratic dialogue with an expert
LLM, using the current problem and recalled insights as context. The
contribution of this paper is a solid theoretical framework and a usable system
platform for designing next-generation intelligent learning systems that
enhance metacognition and self-regulation.

</details>


### [271] [AI in the Writing Process: How Purposeful AI Support Fosters Student Writing](https://arxiv.org/abs/2506.20595)
*Momin N. Siddiqui,Roy Pea,Hari Subramonyam*

Main category: cs.HC

TL;DR: 研究表明，精心设计的AI写作支持工具比通用聊天式LLM更能提升学生的写作自主性和知识转化深度。


<details>
  <summary>Details</summary>
Motivation: 探讨AI写作支持工具对学生写作自主性和知识转化深度的影响，以解决通用聊天式LLM可能导致的写作浅层化问题。

Method: 通过随机对照试验，比较三种写作支持方式：聊天式LLM、集成AI写作工具和标准写作界面（对照组）。

Result: 使用集成AI写作工具的学生表现出更强的写作自主性和更深的知识转化。

Conclusion: 针对写作过程特定方面设计的AI支持工具能帮助学生保持对作品的所有权，同时提升内容参与度。

Abstract: The ubiquity of technologies like ChatGPT has raised concerns about their
impact on student writing, particularly regarding reduced learner agency and
superficial engagement with content. While standalone chat-based LLMs often
produce suboptimal writing outcomes, evidence suggests that purposefully
designed AI writing support tools can enhance the writing process. This paper
investigates how different AI support approaches affect writers' sense of
agency and depth of knowledge transformation. Through a randomized control
trial with 90 undergraduate students, we compare three conditions: (1) a
chat-based LLM writing assistant, (2) an integrated AI writing tool to support
diverse subprocesses, and (3) a standard writing interface (control). Our
findings demonstrate that, among AI-supported conditions, students using the
integrated AI writing tool exhibited greater agency over their writing process
and engaged in deeper knowledge transformation overall. These results suggest
that thoughtfully designed AI writing support targeting specific aspects of the
writing process can help students maintain ownership of their work while
facilitating improved engagement with content.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [272] [MAIZX: A Carbon-Aware Framework for Optimizing Cloud Computing Emissions](https://arxiv.org/abs/2506.19972)
*Federico Ruilova,Ernst Gunnar Gran,Sven-Arne Reinemo*

Main category: cs.DC

TL;DR: MAIZX框架通过动态资源排名优化云操作，显著减少碳排放，测试中CO2排放减少85.68%。


<details>
  <summary>Details</summary>
Motivation: 云计算的高能耗和碳排放问题日益严峻，需更高效透明的解决方案，尤其是私有云基础设施。

Method: MAIZX框架利用实时和预测的碳强度、PUE和能耗数据，动态排名资源（数据中心、边缘节点、多云环境）。

Result: 测试显示CO2排放减少85.68%，框架具有可扩展性和有效性。

Conclusion: MAIZX为提升气候性能潜力提供了强大工具，同时保持运营效率。

Abstract: Cloud computing drives innovation but also poses significant environmental
challenges due to its high-energy consumption and carbon emissions. Data
centers account for 2-4% of global energy usage, and the ICT sector's share of
electricity consumption is projected to reach 40% by 2040. As the goal of
achieving net-zero emissions by 2050 becomes increasingly urgent, there is a
growing need for more efficient and transparent solutions, particularly for
private cloud infrastructures, which are utilized by 87% of organizations,
despite the dominance of public-cloud systems.
  This study evaluates the MAIZX framework, designed to optimize cloud
operations and reduce carbon footprint by dynamically ranking resources,
including data centers, edge computing nodes, and multi-cloud environments,
based on real-time and forecasted carbon intensity, Power Usage Effectiveness
(PUE), and energy consumption. Leveraging a flexible ranking algorithm, MAIZX
achieved an 85.68% reduction in CO2 emissions compared to baseline hypervisor
operations. Tested across geographically distributed data centers, the
framework demonstrates scalability and effectiveness, directly interfacing with
hypervisors to optimize workloads in private, hybrid, and multi-cloud
environments. MAIZX integrates real-time data on carbon intensity, power
consumption, and carbon footprint, as well as forecasted values, into cloud
management, providing a robust tool for enhancing climate performance potential
while maintaining operational efficiency.

</details>


### [273] [WattsOnAI: Measuring, Analyzing, and Visualizing Energy and Carbon Footprint of AI Workloads](https://arxiv.org/abs/2506.20535)
*Hongzhen Huang,Kunming Zhang,Hanlong Liao,Kui Wu,Guoming Tang*

Main category: cs.DC

TL;DR: WattsOnAI是一个用于测量、分析和可视化AI工作负载的能源使用、功耗、硬件性能和碳排放的综合软件工具包。


<details>
  <summary>Details</summary>
Motivation: 现有工具在测量和报告AI模型训练和推理的能源使用及碳排放方面存在碎片化问题，缺乏系统化的指标整合和相关分析支持。

Method: WattsOnAI通过与现有AI框架无缝集成，提供标准化报告和细粒度时间序列数据，支持基准测试和可重复性，并深入分析硬件指标与模型性能的关联。

Result: WattsOnAI解决了现有工具的局限性，促进了AI工作负载的环境影响评估，并推动了更可持续的“绿色AI”实践。

Conclusion: WattsOnAI为研究社区提供了一个轻量级工具，以权衡AI工作负载的性能与环境影响，推动绿色AI的发展。

Abstract: The rapid advancement of AI, particularly large language models (LLMs), has
raised significant concerns about the energy use and carbon emissions
associated with model training and inference. However, existing tools for
measuring and reporting such impacts are often fragmented, lacking systematic
metric integration and offering limited support for correlation analysis among
them. This paper presents WattsOnAI, a comprehensive software toolkit for the
measurement, analysis, and visualization of energy use, power draw, hardware
performance, and carbon emissions across AI workloads. By seamlessly
integrating with existing AI frameworks, WattsOnAI offers standardized reports
and exports fine-grained time-series data to support benchmarking and
reproducibility in a lightweight manner. It further enables in-depth
correlation analysis between hardware metrics and model performance and thus
facilitates bottleneck identification and performance enhancement. By
addressing critical limitations in existing tools, WattsOnAI encourages the
research community to weigh environmental impact alongside raw performance of
AI workloads and advances the shift toward more sustainable "Green AI"
practices. The code is available at https://github.com/SusCom-Lab/WattsOnAI.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [274] [Valid Selection among Conformal Sets](https://arxiv.org/abs/2506.20173)
*Mahmoud Hegazy,Liviu Aolaritei,Michael I. Jordan,Aymeric Dieuleveut*

Main category: stat.ML

TL;DR: 提出一种基于稳定性的方法，确保选择的预测集满足覆盖保证，并扩展到在线设置。


<details>
  <summary>Details</summary>
Motivation: 解决在实践中选择最优预测集（如最小集）时可能破坏覆盖保证的问题。

Method: 采用稳定性方法，确保所选预测集的覆盖性，并扩展到在线环境和其他结构化场景。

Result: 实验证明方法的有效性，确保覆盖性。

Conclusion: 稳定性方法有效解决了预测集选择中的覆盖问题，适用于多种场景。

Abstract: Conformal prediction offers a distribution-free framework for constructing
prediction sets with coverage guarantees. In practice, multiple valid conformal
prediction sets may be available, arising from different models or
methodologies. However, selecting the most desirable set, such as the smallest,
can invalidate the coverage guarantees. To address this challenge, we propose a
stability-based approach that ensures coverage for the selected prediction set.
We extend our results to the online conformal setting, propose several
refinements in settings where additional structure is available, and
demonstrate its effectiveness through experiments.

</details>


### [275] [Data-Driven Dynamic Factor Modeling via Manifold Learning](https://arxiv.org/abs/2506.19945)
*Graeme Baker,Agostino Capponi,J. Antonio Sidaoui*

Main category: stat.ML

TL;DR: 提出一种数据驱动的动态因子框架，通过非线性流形学习技术揭示高维协变量与响应变量的联合动态，并利用线性扩散和卡尔曼滤波进行预测，在金融压力测试中表现优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 传统方法对高维协变量与响应变量的联合动态建模存在局限性，缺乏数据驱动的灵活性。本文旨在通过非线性流形学习技术，无需参数化模型，揭示其动态关系。

Method: 使用Anisotropic Diffusion Maps进行非线性流形学习，通过线性扩散近似嵌入动态，并利用卡尔曼滤波预测协变量和响应变量的演化。

Result: 在金融压力测试中，该方法相比标准情景分析和主成分分析，平均绝对误差分别降低55%和39%。

Conclusion: 数据驱动的动态因子框架在揭示高维数据动态关系及预测方面具有显著优势，尤其在金融压力测试中表现突出。

Abstract: We propose a data-driven dynamic factor framework where a response variable
depends on a high-dimensional set of covariates, without imposing any
parametric model on the joint dynamics. Leveraging Anisotropic Diffusion Maps,
a nonlinear manifold learning technique introduced by Singer and Coifman, our
framework uncovers the joint dynamics of the covariates and responses in a
purely data-driven way. We approximate the embedding dynamics using linear
diffusions, and exploit Kalman filtering to predict the evolution of the
covariates and response variables directly from the diffusion map embedding
space. We generalize Singer's convergence rate analysis of the graph Laplacian
from the case of independent uniform samples on a compact manifold to the case
of time series arising from Langevin diffusions in Euclidean space.
Furthermore, we provide rigorous justification for our procedure by showing the
robustness of approximations of the diffusion map coordinates by linear
diffusions, and the convergence of ergodic averages under standard spectral
assumptions on the underlying dynamics. We apply our method to the stress
testing of equity portfolios using a combination of financial and macroeconomic
factors from the Federal Reserve's supervisory scenarios. We demonstrate that
our data-driven stress testing method outperforms standard scenario analysis
and Principal Component Analysis benchmarks through historical backtests
spanning three major financial crises, achieving reductions in mean absolute
error of up to 55% and 39% for scenario-based portfolio return prediction,
respectively.

</details>


### [276] [A Principled Path to Fitted Distributional Evaluation](https://arxiv.org/abs/2506.20048)
*Sungee Hong,Jiayi Wang,Zhengling Qi,Raymond Ka Wai Wong*

Main category: stat.ML

TL;DR: 本文提出了一种称为拟合分布评估（FDE）的方法，将广泛使用的拟合Q评估扩展到分布式离策略评估（OPE）领域，并提供了理论框架和新方法。


<details>
  <summary>Details</summary>
Motivation: 现有的分布式OPE方法缺乏统一的理论框架，本文旨在填补这一空白，并为设计FDE方法提供指导原则。

Method: 基于提出的指导原则，开发了多种新的FDE方法，并进行了收敛性分析，同时为现有方法提供了理论支持。

Result: 在包括线性二次调节器和Atari游戏的实验中，FDE方法表现出优越性能。

Conclusion: FDE方法为分布式OPE提供了有效的解决方案，并在理论和实验中验证了其优越性。

Abstract: In reinforcement learning, distributional off-policy evaluation (OPE) focuses
on estimating the return distribution of a target policy using offline data
collected under a different policy. This work focuses on extending the widely
used fitted-Q evaluation -- developed for expectation-based reinforcement
learning -- to the distributional OPE setting. We refer to this extension as
fitted distributional evaluation (FDE). While only a few related approaches
exist, there remains no unified framework for designing FDE methods. To fill
this gap, we present a set of guiding principles for constructing theoretically
grounded FDE methods. Building on these principles, we develop several new FDE
methods with convergence analysis and provide theoretical justification for
existing methods, even in non-tabular environments. Extensive experiments,
including simulations on linear quadratic regulators and Atari games,
demonstrate the superior performance of the FDE methods.

</details>


### [277] [Extracting Interpretable Models from Tree Ensembles: Computational and Statistical Perspectives](https://arxiv.org/abs/2506.20114)
*Brian Liu,Rahul Mazumder,Peter Radchenko*

Main category: stat.ML

TL;DR: 提出一种从树集成中提取紧凑决策规则集的估计器，提高可解释性和准确性。


<details>
  <summary>Details</summary>
Motivation: 树集成模型虽预测能力强但难以解释，无法揭示数据中有用关系。

Method: 开发精确算法和近似算法，控制规则数量和交互深度，优化性能。

Result: 实验表明该估计器优于现有规则提取算法，预测性能接近理论最优。

Conclusion: 该方法在保持高预测性能的同时提升了模型的可解释性。

Abstract: Tree ensembles are non-parametric methods widely recognized for their
accuracy and ability to capture complex interactions. While these models excel
at prediction, they are difficult to interpret and may fail to uncover useful
relationships in the data. We propose an estimator to extract compact sets of
decision rules from tree ensembles. The extracted models are accurate and can
be manually examined to reveal relationships between the predictors and the
response. A key novelty of our estimator is the flexibility to jointly control
the number of rules extracted and the interaction depth of each rule, which
improves accuracy. We develop a tailored exact algorithm to efficiently solve
optimization problems underlying our estimator and an approximate algorithm for
computing regularization paths, sequences of solutions that correspond to
varying model sizes. We also establish novel non-asymptotic prediction error
bounds for our proposed approach, comparing it to an oracle that chooses the
best data-dependent linear combination of the rules in the ensemble subject to
the same complexity constraint as our estimator. The bounds illustrate that the
large-sample predictive performance of our estimator is on par with that of the
oracle. Through experiments, we demonstrate that our estimator outperforms
existing algorithms for rule extraction.

</details>


### [278] [POLAR: A Pessimistic Model-based Policy Learning Algorithm for Dynamic Treatment Regimes](https://arxiv.org/abs/2506.20406)
*Ruijia Zhang,Zhengling Qi,Yue Wu,Xiangyu Zhang,Yanxun Xu*

Main category: stat.ML

TL;DR: POLAR是一种新型的悲观模型策略学习算法，用于离线动态治疗优化，通过量化不确定性并提供理论保证，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 解决现有统计方法依赖强假设、缺乏鲁棒性，以及离线强化学习方法缺乏统计保证和计算复杂的问题。

Method: POLAR通过离线数据估计转移动态，量化历史-动作对的不确定性，并在奖励函数中加入悲观惩罚。

Result: POLAR在合成数据和MIMIC-III数据集上表现优于现有方法，提供接近最优的历史感知治疗策略。

Conclusion: POLAR是首个同时提供统计和计算保证的模型动态治疗优化方法，具有实际应用潜力。

Abstract: Dynamic treatment regimes (DTRs) provide a principled framework for
optimizing sequential decision-making in domains where decisions must adapt
over time in response to individual trajectories, such as healthcare,
education, and digital interventions. However, existing statistical methods
often rely on strong positivity assumptions and lack robustness under partial
data coverage, while offline reinforcement learning approaches typically focus
on average training performance, lack statistical guarantees, and require
solving complex optimization problems. To address these challenges, we propose
POLAR, a novel pessimistic model-based policy learning algorithm for offline
DTR optimization. POLAR estimates the transition dynamics from offline data and
quantifies uncertainty for each history-action pair. A pessimistic penalty is
then incorporated into the reward function to discourage actions with high
uncertainty. Unlike many existing methods that focus on average training
performance, POLAR directly targets the suboptimality of the final learned
policy and offers theoretical guarantees, without relying on computationally
intensive minimax or constrained optimization procedures. To the best of our
knowledge, POLAR is the first model-based DTR method to provide both
statistical and computational guarantees, including finite-sample bounds on
policy suboptimality. Empirical results on both synthetic data and the
MIMIC-III dataset demonstrate that POLAR outperforms state-of-the-art methods
and yields near-optimal, history-aware treatment strategies.

</details>


### [279] [Scalable Subset Selection in Linear Mixed Models](https://arxiv.org/abs/2506.20425)
*Ryan Thompson,Matt P. Wand,Joanna J. J. Wang*

Main category: stat.ML

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Linear mixed models (LMMs), which incorporate fixed and random effects, are
key tools for analyzing heterogeneous data, such as in personalized medicine or
adaptive marketing. Nowadays, this type of data is increasingly wide, sometimes
containing thousands of candidate predictors, necessitating sparsity for
prediction and interpretation. However, existing sparse learning methods for
LMMs do not scale well beyond tens or hundreds of predictors, leaving a large
gap compared with sparse methods for linear models, which ignore random
effects. This paper closes the gap with a new $\ell_0$ regularized method for
LMM subset selection that can run on datasets containing thousands of
predictors in seconds to minutes. On the computational front, we develop a
coordinate descent algorithm as our main workhorse and provide a guarantee of
its convergence. We also develop a local search algorithm to help traverse the
nonconvex optimization surface. Both algorithms readily extend to subset
selection in generalized LMMs via a penalized quasi-likelihood approximation.
On the statistical front, we provide a finite-sample bound on the
Kullback-Leibler divergence of the new method. We then demonstrate its
excellent performance in synthetic experiments and illustrate its utility on
two datasets from biology and journalism.

</details>


### [280] [Global Convergence of Iteratively Reweighted Least Squares for Robust Subspace Recovery](https://arxiv.org/abs/2506.20533)
*Gilad Lerman,Kang Li,Tyler Maunu,Teng Zhang*

Main category: stat.ML

TL;DR: 本文提出了一种动态平滑正则化的IRLS变体，证明了其在确定性条件下能线性收敛到真实子空间，并扩展到仿射子空间估计，填补了理论空白。


<details>
  <summary>Details</summary>
Motivation: 鲁棒子空间估计是机器学习和数据分析的基础任务，但IRLS的理论性质尚未被充分理解。

Method: 采用动态平滑正则化的IRLS变体，并在确定性条件下分析其收敛性。

Result: 证明了该方法能从任意初始点线性收敛到真实子空间，并扩展到仿射子空间估计。

Conclusion: 首次为IRLS在鲁棒子空间恢复和非凸IRLS在黎曼流形上提供了全局收敛保证。

Abstract: Robust subspace estimation is fundamental to many machine learning and data
analysis tasks. Iteratively Reweighted Least Squares (IRLS) is an elegant and
empirically effective approach to this problem, yet its theoretical properties
remain poorly understood. This paper establishes that, under deterministic
conditions, a variant of IRLS with dynamic smoothing regularization converges
linearly to the underlying subspace from any initialization. We extend these
guarantees to affine subspace estimation, a setting that lacks prior recovery
theory. Additionally, we illustrate the practical benefits of IRLS through an
application to low-dimensional neural network training. Our results provide the
first global convergence guarantees for IRLS in robust subspace recovery and,
more broadly, for nonconvex IRLS on a Riemannian manifold.

</details>


### [281] [LARP: Learner-Agnostic Robust Data Prefiltering](https://arxiv.org/abs/2506.20573)
*Kristian Minchev,Dimitar Iliev Dimitrov,Nikola Konstantinov*

Main category: stat.ML

TL;DR: 论文提出了一种学习者无关的鲁棒数据预过滤方法（LARP），旨在通过预过滤保护下游学习器免受低质量数据的影响，尽管会带来一定的性能损失。


<details>
  <summary>Details</summary>
Motivation: 公共数据集中常包含低质量或污染数据，影响学习器的性能，因此需要开发一种通用的数据预过滤方法。

Method: 提出了LARP框架，以Huber估计器和Huber数据污染模型为例，分析了多种预过滤方法，并通过实验验证了其效果。

Result: 理论分析和实验表明，LARP在异构学习器集合上会导致性能损失，但能显著减少重复预过滤的成本。

Conclusion: LARP在大数据集上具有优势，尽管会带来一定的性能损失，但在成本和效率之间提供了平衡。

Abstract: The widespread availability of large public datasets is a key factor behind
the recent successes of statistical inference and machine learning methods.
However, these datasets often contain some low-quality or contaminated data, to
which many learning procedures are sensitive. Therefore, the question of
whether and how public datasets should be prefiltered to facilitate accurate
downstream learning arises. On a technical level this requires the construction
of principled data prefiltering methods which are learner-agnostic robust, in
the sense of provably protecting a set of pre-specified downstream learners
from corrupted data. In this work, we formalize the problem of Learner-Agnostic
Robust data Prefiltering (LARP), which aims at finding prefiltering procedures
that minimize a worst-case loss over a pre-specified set of learners. We first
instantiate our framework in the context of scalar mean estimation with Huber
estimators under the Huber data contamination model. We provide a hardness
result on a specific problem instance and analyze several natural prefiltering
procedures. Our theoretical results indicate that performing LARP on a
heterogeneous set of learners leads to some loss in model performance compared
to the alternative of prefiltering data for each learner/use-case individually.
We explore the resulting utility loss and its dependence on the problem
parameters via extensive experiments on real-world image and tabular data,
observing statistically significant reduction in utility. Finally, we model the
trade-off between the utility drop and the cost of repeated (learner-specific)
prefiltering within a game-theoretic framework and showcase benefits of LARP
for large datasets.

</details>


<div id='hep-ph'></div>

# hep-ph [[Back]](#toc)

### [282] [DeepQuark: deep-neural-network approach to multiquark bound states](https://arxiv.org/abs/2506.20555)
*Wei-Lin Wu,Lu Meng,Shi-Lin Zhu*

Main category: hep-ph

TL;DR: 论文提出了一种基于深度神经网络的变分蒙特卡洛方法DeepQuark，用于研究多夸克束缚态，解决了强SU(3)色相互作用带来的复杂性，并在多个系统中表现出色。


<details>
  <summary>Details</summary>
Motivation: 多夸克系统的复杂性远超电子或核子系统，传统方法难以处理其强关联性、额外离散量子数和难以解决的禁闭相互作用。

Method: 设计了DeepQuark架构，结合深度神经网络和变分蒙特卡洛方法，高效处理多夸克系统的独特挑战。

Result: 在核子、双重重四夸克和全重重四夸克系统中表现优异，尤其在五夸克系统中优于现有方法，成功预测了多个新态。

Conclusion: DeepQuark为研究更大规模多夸克系统和探索禁闭机制提供了强大工具，有望推动非微扰QCD和多体物理的研究。

Abstract: For the first time, we implement the deep-neural-network-based variational
Monte Carlo approach for the multiquark bound states, whose complexity
surpasses that of electron or nucleon systems due to strong SU(3) color
interactions. We design a novel and high-efficiency architecture, DeepQuark, to
address the unique challenges in multiquark systems such as stronger
correlations, extra discrete quantum numbers, and intractable confinement
interaction. Our method demonstrates competitive performance with
state-of-the-art approaches, including diffusion Monte Carlo and Gaussian
expansion method, in the nucleon, doubly heavy tetraquark, and fully heavy
tetraquark systems. Notably, it outperforms existing calculations for
pentaquarks, exemplified by the triply heavy pentaquark. For the nucleon, we
successfully incorporate three-body flux-tube confinement interactions without
additional computational costs. In tetraquark systems, we consistently describe
hadronic molecule $T_{cc}$ and compact tetraquark $T_{bb}$ with an unbiased
form of wave function ansatz. In the pentaquark sector, we obtain weakly bound
$\bar D^*\Xi_{cc}^*$ molecule $P_{cc\bar c}(5715)$ with $S=\frac{5}{2}$ and its
bottom partner $P_{bb\bar b}(15569)$. They can be viewed as the analogs of the
molecular $T_{cc}$. We recommend experimental search of $P_{cc\bar c}(5715)$ in
the D-wave $J/\psi \Lambda_c$ channel. DeepQuark holds great promise for
extension to larger multiquark systems, overcoming the computational barriers
in conventional methods. It also serves as a powerful framework for exploring
confining mechanism beyond two-body interactions in multiquark states, which
may offer valuable insights into nonperturbative QCD and general many-body
physics.

</details>


<div id='physics.chem-ph'></div>

# physics.chem-ph [[Back]](#toc)

### [283] [An ab initio foundation model of wavefunctions that accurately describes chemical bond breaking](https://arxiv.org/abs/2506.19960)
*Adam Foster,Zeno Schätzle,P. Bernát Szabó,Lixue Cheng,Jonas Köhler,Gino Cassella,Nicholas Gao,Jiawei Li,Frank Noé,Jan Hermann*

Main category: physics.chem-ph

TL;DR: Orbformer是一种基于深度QMC的可转移波函数模型，通过预训练在22,000个结构上，实现了化学精度的计算，显著降低了多参考方法的成本。


<details>
  <summary>Details</summary>
Motivation: 解决量子化学中键断裂描述的多参考性问题，并降低传统多参考方法的高计算成本。

Method: 提出Orbformer，一种基于深度神经网络的可转移波函数模型，通过预训练和微调实现高效计算。

Result: 在键断裂和Diels-Alder反应等挑战性任务中，Orbformer唯一能稳定达到化学精度（1 kcal/mol）。

Conclusion: Orbformer通过分摊薛定谔方程求解成本，为量子化学提供了一种实用的新范式。

Abstract: Reliable description of bond breaking remains a major challenge for quantum
chemistry due to the multireferential character of the electronic structure in
dissociating species. Multireferential methods in particular suffer from large
computational cost, which under the normal paradigm has to be paid anew for
each system at a full price, ignoring commonalities in electronic structure
across molecules. Quantum Monte Carlo with deep neural networks (deep QMC)
uniquely offers to exploit such commonalities by pretraining transferable
wavefunction models, but all such attempts were so far limited in scope. Here,
we bring this new paradigm to fruition with Orbformer, a novel transferable
wavefunction model pretrained on 22,000 equilibrium and dissociating structures
that can be fine-tuned on unseen molecules reaching an accuracy-cost ratio
rivalling classical multireferential methods. On established benchmarks as well
as more challenging bond dissociations and Diels-Alder reactions, Orbformer is
the only method that consistently converges to chemical accuracy (1 kcal/mol).
This work turns the idea of amortizing the cost of solving the Schr\"odinger
equation over many molecules into a practical approach in quantum chemistry.

</details>


<div id='q-bio.BM'></div>

# q-bio.BM [[Back]](#toc)

### [284] [DualEquiNet: A Dual-Space Hierarchical Equivariant Network for Large Biomolecules](https://arxiv.org/abs/2506.19862)
*Junjie Xu,Jiahao Zhang,Mangal Prakash,Xiang Zhang,Suhang Wang*

Main category: q-bio.BM

TL;DR: DualEquiNet是一种双空间层次等变网络，结合欧几里得和球谐空间表示，用于高效建模大型生物分子的多尺度结构。


<details>
  <summary>Details</summary>
Motivation: 现有几何图神经网络在建模大型生物分子（如RNA和蛋白质）时面临可扩展性和表达能力不足的问题，需要同时捕捉细粒度原子相互作用和长程依赖关系。

Method: DualEquiNet通过在欧几里得和球谐空间中构建互补表示，利用双向跨空间消息传递和跨空间交互池化机制，层次化聚合原子特征。

Result: DualEquiNet在RNA性质预测和蛋白质建模的多个基准测试中达到最先进性能，并在新引入的3D结构基准测试中优于现有方法。

Conclusion: DualEquiNet通过双空间表示和层次化特征聚合，为大型生物分子建模提供了高效且表达力强的解决方案。

Abstract: Geometric graph neural networks (GNNs) that respect E(3) symmetries have
achieved strong performance on small molecule modeling, but they face
scalability and expressiveness challenges when applied to large biomolecules
such as RNA and proteins. These systems require models that can simultaneously
capture fine-grained atomic interactions, long-range dependencies across
spatially distant components, and biologically relevant hierarchical structure,
such as atoms forming residues, which in turn form higher-order domains.
Existing geometric GNNs, which typically operate exclusively in either
Euclidean or Spherical Harmonics space, are limited in their ability to capture
both the fine-scale atomic details and the long-range, symmetry-aware
dependencies required for modeling the multi-scale structure of large
biomolecules. We introduce DualEquiNet, a Dual-Space Hierarchical Equivariant
Network that constructs complementary representations in both Euclidean and
Spherical Harmonics spaces to capture local geometry and global symmetry-aware
features. DualEquiNet employs bidirectional cross-space message passing and a
novel Cross-Space Interaction Pooling mechanism to hierarchically aggregate
atomic features into biologically meaningful units, such as residues, enabling
efficient and expressive multi-scale modeling for large biomolecular systems.
DualEquiNet achieves state-of-the-art performance on multiple existing
benchmarks for RNA property prediction and protein modeling, and outperforms
prior methods on two newly introduced 3D structural benchmarks demonstrating
its broad effectiveness across a range of large biomolecule modeling tasks.

</details>


### [285] [Scalable and Cost-Efficient de Novo Template-Based Molecular Generation](https://arxiv.org/abs/2506.19865)
*Piotr Gaiński,Oussama Boussif,Andrei Rekesh,Dmytro Shevchuk,Ali Parviz,Mike Tyers,Robert A. Batey,Michał Koziarski*

Main category: q-bio.BM

TL;DR: 提出了一种基于模板的分子生成方法，通过递归成本指导和动态库机制解决合成成本、大规模构建块库和小片段集利用的挑战，显著提升了成本效率和分子多样性。


<details>
  <summary>Details</summary>
Motivation: 解决模板基GFlowNets中的三个核心挑战：最小化合成成本、扩展到大型构建块库以及有效利用小片段集。

Method: 提出递归成本指导框架和动态库机制，利用辅助机器学习模型估算合成成本和可行性，并通过探索惩罚平衡探索与利用。

Result: 在模板基分子生成中取得了最先进的结果，显著提升了成本效率、分子多样性和质量。

Conclusion: 该方法为药物设计提供了一种高效且可扩展的分子生成途径。

Abstract: Template-based molecular generation offers a promising avenue for drug design
by ensuring generated compounds are synthetically accessible through predefined
reaction templates and building blocks. In this work, we tackle three core
challenges in template-based GFlowNets: (1) minimizing synthesis cost, (2)
scaling to large building block libraries, and (3) effectively utilizing small
fragment sets. We propose \textbf{Recursive Cost Guidance}, a backward policy
framework that employs auxiliary machine learning models to approximate
synthesis cost and viability. This guidance steers generation toward low-cost
synthesis pathways, significantly enhancing cost-efficiency, molecular
diversity, and quality, especially when paired with an \textbf{Exploitation
Penalty} that balances the trade-off between exploration and exploitation. To
enhance performance in smaller building block libraries, we develop a
\textbf{Dynamic Library} mechanism that reuses intermediate high-reward states
to construct full synthesis trees. Our approach establishes state-of-the-art
results in template-based molecular generation.

</details>


<div id='cs.OS'></div>

# cs.OS [[Back]](#toc)

### [286] [MNN-AECS: Energy Optimization for LLM Decoding on Mobile Devices via Adaptive Core Selection](https://arxiv.org/abs/2506.19884)
*Zhengxiang Huang,Chaoyue Niu,Zhaode Wang,Jiarui Xue,Hanming Zhang,Yugang Wang,Zewei Xin,Xiaotang Jiang,Chengfei Lv,Fan Wu,Guihai Chen*

Main category: cs.OS

TL;DR: 论文提出了一种名为MNN-AECS的系统，通过动态选择低功耗CPU核心来优化设备上大型语言模型（LLM）推理的能源效率，显著降低能耗且不影响解码速度。


<details>
  <summary>Details</summary>
Motivation: 随着设备上LLM推理需求的增长，能源效率成为主要问题，尤其是电池有限的移动设备。现有工作多关注预填充阶段的加速，而忽视了解码阶段的能源消耗。

Method: 引入自适应能源核心选择（AECS）并将其集成到MNN中，创建MNN-AECS系统，动态选择低功耗CPU核心以减少解码能耗。

Result: 在5种Android和2种iOS设备上测试5种不同规模的LLM，MNN-AECS平均降低23%能耗且无速度损失，相比其他引擎节能39%-78%，提速12%-363%。

Conclusion: MNN-AECS是首个无需root或系统修改的引擎级解决方案，有效提升LLM解码的能源效率。

Abstract: As the demand for on-device Large Language Model (LLM) inference grows,
energy efficiency has become a major concern, especially for battery-limited
mobile devices. Our analysis shows that the memory-bound LLM decode phase
dominates energy use, and yet most existing works focus on accelerating the
prefill phase, neglecting energy concerns. We introduce Adaptive Energy-Centric
Core Selection (AECS) and integrate it into MNN to create the energy-efficient
version, MNN-AECS, the first engine-level system solution without requiring
root access or OS modifications for energy-efficient LLM decoding. MNN-AECS is
designed to reduce LLM decoding energy while keeping decode speed within an
acceptable slowdown threshold by dynamically selecting low-power CPU cores.
MNN-AECS is evaluated across 5 Android and 2 iOS devices on 5 popular LLMs of
various sizes. Compared to original MNN, MNN-AECS cuts down energy use by 23%
without slowdown averaged over all 7 devices and 4 datasets. Against other
engines, including llama.cpp, executorch, mllm, and MediaPipe, MNN-AECS
delivers 39% to 78% energy saving and 12% to 363% speedup on average.

</details>


<div id='cs.DS'></div>

# cs.DS [[Back]](#toc)

### [287] [Accept More, Reject Less: Reducing up to 19% Unnecessary Desk-Rejections over 11 Years of ICLR Data](https://arxiv.org/abs/2506.20141)
*Xiaoyu Li,Zhao Song,Jiahao Zhang*

Main category: cs.DS

TL;DR: 论文提出了一种优化算法，以减少因作者提交限制而导致的无效拒稿，保留更多有价值的论文。


<details>
  <summary>Details</summary>
Motivation: AI会议论文提交量激增，导致严格限制作者提交数量并简单按ID顺序拒稿，可能误拒有价值论文。

Method: 将当前拒稿政策形式化为优化问题，提出基于线性规划松弛和舍入方案的实用算法。

Result: 在11年ICLR数据上验证，算法保留19.23%更多论文且不违反作者限制，计算效率高（最多53.64秒）。

Conclusion: 该算法简单实用，显著减少无效拒稿，有望改进当前计算机科学会议提交政策。

Abstract: The explosive growth of AI research has driven paper submissions at flagship
AI conferences to unprecedented levels, necessitating many venues in 2025
(e.g., CVPR, ICCV, KDD, AAAI, IJCAI, WSDM) to enforce strict per-author
submission limits and to desk-reject any excess papers by simple ID order.
While this policy helps reduce reviewer workload, it may unintentionally
discard valuable papers and penalize authors' efforts. In this paper, we ask an
essential research question on whether it is possible to follow submission
limits while minimizing needless rejections. We first formalize the current
desk-rejection policies as an optimization problem, and then develop a
practical algorithm based on linear programming relaxation and a rounding
scheme. Under extensive evaluation on 11 years of real-world ICLR
(International Conference on Learning Representations) data, our method
preserves up to $19.23\%$ more papers without violating any author limits.
Moreover, our algorithm is highly efficient in practice, with all results on
ICLR data computed within at most 53.64 seconds. Our work provides a simple and
practical desk-rejection strategy that significantly reduces unnecessary
rejections, demonstrating strong potential to improve current CS conference
submission policies.

</details>
