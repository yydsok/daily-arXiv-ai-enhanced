<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 34]
- [cs.CL](#cs.CL) [Total: 81]
- [cs.CV](#cs.CV) [Total: 98]
- [cs.HC](#cs.HC) [Total: 13]
- [cs.LG](#cs.LG) [Total: 123]
- [cs.MA](#cs.MA) [Total: 1]
- [cs.RO](#cs.RO) [Total: 24]
- [cs.SD](#cs.SD) [Total: 10]
- [cs.IR](#cs.IR) [Total: 4]
- [q-bio.BM](#q-bio.BM) [Total: 1]
- [cs.IT](#cs.IT) [Total: 1]
- [eess.IV](#eess.IV) [Total: 6]
- [cs.AR](#cs.AR) [Total: 1]
- [eess.SY](#eess.SY) [Total: 5]
- [q-bio.QM](#q-bio.QM) [Total: 2]
- [cs.SE](#cs.SE) [Total: 6]
- [cs.DC](#cs.DC) [Total: 1]
- [q-fin.ST](#q-fin.ST) [Total: 1]
- [cond-mat.mtrl-sci](#cond-mat.mtrl-sci) [Total: 2]
- [cs.CR](#cs.CR) [Total: 5]
- [cs.GR](#cs.GR) [Total: 5]
- [cs.CY](#cs.CY) [Total: 5]
- [quant-ph](#quant-ph) [Total: 5]
- [astro-ph.CO](#astro-ph.CO) [Total: 1]
- [physics.med-ph](#physics.med-ph) [Total: 1]
- [stat.ME](#stat.ME) [Total: 1]
- [q-bio.NC](#q-bio.NC) [Total: 1]
- [physics.soc-ph](#physics.soc-ph) [Total: 1]
- [cs.CE](#cs.CE) [Total: 1]
- [cs.DB](#cs.DB) [Total: 3]
- [nlin.CG](#nlin.CG) [Total: 1]
- [astro-ph.IM](#astro-ph.IM) [Total: 1]
- [eess.AS](#eess.AS) [Total: 1]
- [physics.geo-ph](#physics.geo-ph) [Total: 1]
- [eess.SP](#eess.SP) [Total: 2]
- [physics.chem-ph](#physics.chem-ph) [Total: 1]
- [stat.ML](#stat.ML) [Total: 6]
- [math.NA](#math.NA) [Total: 3]
- [math.OC](#math.OC) [Total: 5]
- [math.ST](#math.ST) [Total: 1]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [TIP-Search: Time-Predictable Inference Scheduling for Market Prediction under Uncertain Load](https://arxiv.org/abs/2506.08026)
*Xibai Wang*

Main category: cs.AI

TL;DR: TIP-Search是一种时间可预测的推理调度框架，用于在不确定工作负载下进行实时市场预测，旨在满足高延迟要求的同时最大化预测准确性。


<details>
  <summary>Details</summary>
Motivation: 高频率金融系统对延迟有严格要求，需要一种动态选择深度学习模型的方法以满足任务截止时间。

Method: TIP-Search通过离线分析延迟和泛化性能，在线进行任务感知选择，无需显式输入域标签。

Result: 在三个实际订单簿数据集上测试，TIP-Search比静态基线准确率提高8.5%，且100%满足截止时间。

Conclusion: TIP-Search在不确定环境下实现了高效的低延迟金融推理。

Abstract: This paper proposes TIP-Search, a time-predictable inference scheduling
framework for real-time market prediction under uncertain workloads. Motivated
by the strict latency demands in high-frequency financial systems, TIP-Search
dynamically selects a deep learning model from a heterogeneous pool, aiming to
maximize predictive accuracy while satisfying per-task deadline constraints.
Our approach profiles latency and generalization performance offline, then
performs online task-aware selection without relying on explicit input domain
labels. We evaluate TIP-Search on three real-world limit order book datasets
(FI-2010, Binance BTC/USDT, LOBSTER AAPL) and demonstrate that it outperforms
static baselines with up to 8.5% improvement in accuracy and 100% deadline
satisfaction. Our results highlight the effectiveness of TIP-Search in robust
low-latency financial inference under uncertainty.

</details>


### [2] [Cognitive Weave: Synthesizing Abstracted Knowledge with a Spatio-Temporal Resonance Graph](https://arxiv.org/abs/2506.08098)
*Akash Vishwakarma,Hojin Lee,Mohith Suresh,Priyam Shankar Sharma,Rahul Vishwakarma,Sparsh Gupta,Yuvraj Anupam Chauhan*

Main category: cs.AI

TL;DR: 论文提出了一种名为Cognitive Weave的新型记忆框架，通过多层时空共振图（STRG）管理信息，显著提升了任务完成率和查询效率。


<details>
  <summary>Details</summary>
Motivation: 现有记忆系统在结构灵活性、时间感知和从原始数据中提取高级见解方面存在局限，需要更先进的记忆架构来支持LLM代理的持续学习和动态适应。

Method: 采用多层的时空共振图（STRG）管理语义丰富的见解粒子（IPs），并通过语义预言接口（SOI）动态丰富IPs，形成知识图谱。还包括认知提炼过程，生成高级知识结构（IAs）。

Result: Cognitive Weave在长期规划任务、动态问答场景和多会话对话连贯性方面表现优异，任务完成率平均提升34%，查询延迟降低42%。

Conclusion: 论文展示了Cognitive Weave的优越性能，并探讨了高级记忆系统的伦理问题和未来研究方向。

Abstract: The emergence of capable large language model (LLM) based agents necessitates
memory architectures that transcend mere data storage, enabling continuous
learning, nuanced reasoning, and dynamic adaptation. Current memory systems
often grapple with fundamental limitations in structural flexibility, temporal
awareness, and the ability to synthesize higher-level insights from raw
interaction data. This paper introduces Cognitive Weave, a novel memory
framework centered around a multi-layered spatio-temporal resonance graph
(STRG). This graph manages information as semantically rich insight particles
(IPs), which are dynamically enriched with resonance keys, signifiers, and
situational imprints via a dedicated semantic oracle interface (SOI). These IPs
are interconnected through typed relational strands, forming an evolving
knowledge tapestry. A key component of Cognitive Weave is the cognitive
refinement process, an autonomous mechanism that includes the synthesis of
insight aggregates (IAs) condensed, higher-level knowledge structures derived
from identified clusters of related IPs. We present comprehensive experimental
results demonstrating Cognitive Weave's marked enhancement over existing
approaches in long-horizon planning tasks, evolving question-answering
scenarios, and multi-session dialogue coherence. The system achieves a notable
34% average improvement in task completion rates and a 42% reduction in mean
query latency when compared to state-of-the-art baselines. Furthermore, this
paper explores the ethical considerations inherent in such advanced memory
systems, discusses the implications for long-term memory in LLMs, and outlines
promising future research trajectories.

</details>


### [3] [SOP-Bench: Complex Industrial SOPs for Evaluating LLM Agents](https://arxiv.org/abs/2506.08119)
*Subhrangshu Nandi,Arghya Datta,Nikhil Vichare,Indranil Bhattacharya,Huzefa Raja,Jing Xu,Shayan Ray,Giuseppe Carenini,Abhi Srivastava,Aaron Chan,Man Ho Woo,Amar Kandola,Brandon Theresa,Francesco Carbone*

Main category: cs.AI

TL;DR: 论文提出SOP-Bench基准，评估LLM在复杂工业SOP任务中的表现，发现当前LLM代理能力与实际需求存在显著差距。


<details>
  <summary>Details</summary>
Motivation: 现有LLM在复杂工业SOP任务中表现不足，缺乏公开基准测试其能力。

Method: 开发合成数据生成框架和SOP-Bench基准，评估两种代理架构（Function-Calling和ReAct）。

Result: 两种代理平均成功率仅为27%和48%，工具调用错误率高达100%。

Conclusion: LLM代理能力与工业SOP需求差距大，需领域特定优化。SOP-Bench开源以推动社区扩展。

Abstract: Large Language Models (LLMs) demonstrate impressive general-purpose reasoning
and problem-solving abilities. However, they struggle with executing complex,
long-horizon workflows that demand strict adherence to Standard Operating
Procedures (SOPs), a critical requirement for real-world industrial automation.
Despite this need, there is a lack of public benchmarks that reflect the
complexity, structure, and domain-specific nuances of SOPs. To address this, we
present three main contributions. First, we introduce a synthetic data
generation framework to create realistic, industry-grade SOPs that rigorously
test the planning, reasoning, and tool-use capabilities of LLM-based agents.
Second, using this framework, we develop SOP-Bench, a benchmark of over 1,800
tasks across 10 industrial domains, each with APIs, tool interfaces, and
human-validated test cases. Third, we evaluate two prominent agent
architectures: Function-Calling and ReAct Agents, on SOP-Bench, observing
average success rates of only 27% and 48%, respectively. Remarkably, when the
tool registry is much larger than necessary, agents invoke incorrect tools
nearly 100% of the time. These findings underscore a substantial gap between
current agentic capabilities of LLMs and the demands of automating real-world
SOPs. Performance varies significantly by task and domain, highlighting the
need for domain-specific benchmarking and architectural choices before
deployment. SOP-Bench is publicly available at
http://sop-bench.s3-website-us-west-2.amazonaws.com/. We also release the
prompts underpinning the data generation framework to support new
domain-specific SOP benchmarks. We invite the community to extend SOP-Bench
with SOPs from their industrial domains.

</details>


### [4] [The AI Imperative: Scaling High-Quality Peer Review in Machine Learning](https://arxiv.org/abs/2506.08134)
*Qiyao Wei,Samuel Holt,Jing Yang,Markus Wulfmeier,Mihaela van der Schaar*

Main category: cs.AI

TL;DR: 论文主张利用AI辅助同行评审，以应对机器学习领域评审规模危机，提出AI作为协作工具的具体应用和研究议程。


<details>
  <summary>Details</summary>
Motivation: 机器学习领域论文提交量激增，评审资源有限，导致评审质量、一致性和评审员疲劳问题，亟需AI辅助解决方案。

Method: 提出利用大型语言模型（LLMs）作为协作工具，辅助作者、评审员和领域主席，具体包括事实核查、评审指导、质量提升和决策支持。

Result: 论文未提供具体实验结果，但提出了研究议程和技术与伦理挑战。

Conclusion: 呼吁机器学习社区积极构建AI辅助评审系统，以保障科学验证的完整性和可扩展性，同时维持高质量同行评审标准。

Abstract: Peer review, the bedrock of scientific advancement in machine learning (ML),
is strained by a crisis of scale. Exponential growth in manuscript submissions
to premier ML venues such as NeurIPS, ICML, and ICLR is outpacing the finite
capacity of qualified reviewers, leading to concerns about review quality,
consistency, and reviewer fatigue. This position paper argues that AI-assisted
peer review must become an urgent research and infrastructure priority. We
advocate for a comprehensive AI-augmented ecosystem, leveraging Large Language
Models (LLMs) not as replacements for human judgment, but as sophisticated
collaborators for authors, reviewers, and Area Chairs (ACs). We propose
specific roles for AI in enhancing factual verification, guiding reviewer
performance, assisting authors in quality improvement, and supporting ACs in
decision-making. Crucially, we contend that the development of such systems
hinges on access to more granular, structured, and ethically-sourced peer
review process data. We outline a research agenda, including illustrative
experiments, to develop and validate these AI assistants, and discuss
significant technical and ethical challenges. We call upon the ML community to
proactively build this AI-assisted future, ensuring the continued integrity and
scalability of scientific validation, while maintaining high standards of peer
review.

</details>


### [5] [Compiling Metric Temporal Answer Set Programming](https://arxiv.org/abs/2506.08150)
*Arvid Becker,Pedro Cabalar,Martin Diéguez,Javier Romero,Susana Hahn,Torsten Schaub*

Main category: cs.AI

TL;DR: 提出了一种计算方法来处理度量答案集编程（ASP）中的定量时间约束，如持续时间和截止时间。


<details>
  <summary>Details</summary>
Motivation: 解决在细粒度时间约束下ASP的可扩展性问题，避免因时间精度导致的性能瓶颈。

Method: 利用ASP的扩展（如差异约束）将时间相关部分外部化处理，从而解耦度量ASP与时间粒度的关系。

Result: 实现了不受时间精度影响的解决方案，显著提升了处理定量时间约束的效率。

Conclusion: 该方法有效解决了度量ASP中的时间约束问题，同时保持了系统的可扩展性。

Abstract: We develop a computational approach to Metric Answer Set Programming (ASP) to
allow for expressing quantitative temporal constrains, like durations and
deadlines. A central challenge is to maintain scalability when dealing with
fine-grained timing constraints, which can significantly exacerbate ASP's
grounding bottleneck. To address this issue, we leverage extensions of ASP with
difference constraints, a simplified form of linear constraints, to handle
time-related aspects externally. Our approach effectively decouples metric ASP
from the granularity of time, resulting in a solution that is unaffected by
time precision.

</details>


### [6] [AstroCompress: A benchmark dataset for multi-purpose compression of astronomical data](https://arxiv.org/abs/2506.08306)
*Tuan Truong,Rithwik Sudharsan,Yibo Yang,Peter Xiangyuan Ma,Ruihan Yang,Stephan Mandt,Joshua S. Bloom*

Main category: cs.AI

TL;DR: 论文提出AstroCompress，一种用于天体物理数据的神经压缩方法，通过端到端学习提升数据压缩效率，优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 天文台因环境限制导致数据传输能力受限，改进无损压缩技术可显著提升科学数据收集效率。

Method: 引入AstroCompress挑战，提供五个数据集，比较七种无损压缩方法（三种神经方法和四种传统方法）。

Result: 神经压缩技术可提升天文台数据收集效率，并为科学应用中的神经压缩采用提供指导。

Conclusion: 神经压缩在天文数据中具有潜力，未来可探索有损压缩方法。

Abstract: The site conditions that make astronomical observatories in space and on the
ground so desirable -- cold and dark -- demand a physical remoteness that leads
to limited data transmission capabilities. Such transmission limitations
directly bottleneck the amount of data acquired and in an era of costly modern
observatories, any improvements in lossless data compression has the potential
scale to billions of dollars worth of additional science that can be
accomplished on the same instrument. Traditional lossless methods for
compressing astrophysical data are manually designed. Neural data compression,
on the other hand, holds the promise of learning compression algorithms
end-to-end from data and outperforming classical techniques by leveraging the
unique spatial, temporal, and wavelength structures of astronomical images.
This paper introduces AstroCompress: a neural compression challenge for
astrophysics data, featuring four new datasets (and one legacy dataset) with
16-bit unsigned integer imaging data in various modes: space-based,
ground-based, multi-wavelength, and time-series imaging. We provide code to
easily access the data and benchmark seven lossless compression methods (three
neural and four non-neural, including all practical state-of-the-art
algorithms). Our results on lossless compression indicate that lossless neural
compression techniques can enhance data collection at observatories, and
provide guidance on the adoption of neural compression in scientific
applications. Though the scope of this paper is restricted to lossless
compression, we also comment on the potential exploration of lossy compression
methods in future studies.

</details>


### [7] [LeanTutor: A Formally-Verified AI Tutor for Mathematical Proofs](https://arxiv.org/abs/2506.08321)
*Manooshree Patel,Rayna Bhattacharyya,Thomas Lu,Arnav Mehta,Niels Voss,Narges Norouzi,Gireeja Ranade*

Main category: cs.AI

TL;DR: LeanTutor是一个基于大型语言模型的数学证明辅导系统，通过自然语言交互、形式化验证和生成下一步指导来辅助学生。


<details>
  <summary>Details</summary>
Motivation: 开发一个能够以自然语言交互、形式化验证学生证明并提供教学指导的智能辅导系统。

Method: 系统由三个模块组成：自动形式化/证明检查器、下一步生成器和自然语言反馈生成器。

Result: 自动形式化器在正确证明中准确形式化57%的策略，并在错误证明中准确识别30%的错误步骤；反馈生成器在准确性和相关性上优于基线。

Conclusion: LeanTutor展示了基于LLM的数学辅导系统的潜力，尤其在形式化验证和教学反馈方面表现良好。

Abstract: We present LeanTutor, a Large Language Model (LLM)-based tutoring system for
math proofs. LeanTutor interacts with the student in natural language, formally
verifies student-written math proofs in Lean, generates correct next steps, and
provides the appropriate instructional guidance. LeanTutor is composed of three
modules: (i) an autoformalizer/proof-checker, (ii) a next-step generator, and
(iii) a natural language feedback generator. The first module faithfully
autoformalizes student proofs into Lean and verifies proof accuracy via
successful code compilation. If the proof has an error, the incorrect step is
identified. The next-step generator module outputs a valid next Lean tactic for
incorrect proofs via LLM-based candidate generation and proof search. The
feedback generator module leverages Lean data to produce a
pedagogically-motivated natural language hint for the student user. To evaluate
our system, we introduce PeanoBench, a human-written dataset derived from the
Natural Numbers Game, consisting of 371 Peano Arithmetic proofs, where each
natural language proof step is paired with the corresponding logically
equivalent tactic in Lean. The Autoformalizer correctly formalizes 57% of
tactics in correct proofs and accurately identifies the incorrect step in 30%
of incorrect proofs. In generating natural language hints for erroneous proofs,
LeanTutor outperforms a simple baseline on accuracy and relevance metrics.

</details>


### [8] [ORFS-agent: Tool-Using Agents for Chip Design Optimization](https://arxiv.org/abs/2506.08332)
*Amur Ghose,Andrew B. Kahng,Sayak Kundu,Zhiang Wang*

Main category: cs.AI

TL;DR: ORFS-agent是一种基于大型语言模型（LLM）的迭代优化代理，用于自动化硬件设计流程中的参数调优，显著优于传统贝叶斯优化方法。


<details>
  <summary>Details</summary>
Motivation: 集成电路设计流程涉及大量参数配置，微小变化可能对性能、功耗和面积产生重大影响。LLM的进展为高维优化任务提供了新机会。

Method: ORFS-agent通过自适应探索参数配置，结合自然语言目标进行多目标优化，无需额外微调即可与前沿LLM兼容。

Result: 在两个技术节点和多种电路基准测试中，ORFS-agent将布线长度和有效时钟周期提升超过13%，同时减少40%的优化迭代次数。

Conclusion: ORFS-agent为硬件设计提供了一种高效、灵活且可解释的多目标优化框架。

Abstract: Machine learning has been widely used to optimize complex engineering
workflows across numerous domains. In the context of integrated circuit design,
modern flows (e.g., going from a register-transfer level netlist to physical
layouts) involve extensive configuration via thousands of parameters, and small
changes to these parameters can have large downstream impacts on desired
outcomes - namely design performance, power, and area. Recent advances in Large
Language Models (LLMs) offer new opportunities for learning and reasoning
within such high-dimensional optimization tasks. In this work, we introduce
ORFS-agent, an LLM-based iterative optimization agent that automates parameter
tuning in an open-source hardware design flow. ORFS-agent adaptively explores
parameter configurations, demonstrating clear improvements over standard
Bayesian optimization approaches in terms of resource efficiency and final
design metrics. Our empirical evaluations on two different technology nodes and
a range of circuit benchmarks indicate that ORFS-agent can improve both routed
wirelength and effective clock period by over 13%, all while using 40% fewer
optimization iterations. Moreover, by following natural language objectives to
trade off certain metrics for others, ORFS-agent demonstrates a flexible and
interpretable framework for multi-objective optimization. Crucially, RFS-agent
is modular and model-agnostic, and can be plugged in to any frontier LLM
without any further fine-tuning.

</details>


### [9] [FloorplanMAE:A self-supervised framework for complete floorplan generation from partial inputs](https://arxiv.org/abs/2506.08363)
*Jun Yin,Jing Zhong,Pengyu Zeng,Peilin Li,Miao Zhang,Ran Luo,Shuai Lu*

Main category: cs.AI

TL;DR: FloorplanMAE是一个自监督学习框架，用于从不完整的平面图预测完整平面图，提高设计效率。


<details>
  <summary>Details</summary>
Motivation: 建筑设计过程中，平面图设计是动态迭代的，预测完整平面图能帮助建筑师快速生成初步设计，减少重复修改的工作量。

Method: 提出基于Masked Autoencoders (MAE)的方法，通过掩码部分平面图并训练轻量级Vision Transformer (ViT)来重建缺失部分。

Result: FloorplanMAE能从不完整平面图生成高质量完整平面图，实验验证其优于现有基准。

Conclusion: 该框架为平面图生成提供了可扩展的解决方案，具有广泛应用前景。

Abstract: In the architectural design process, floorplan design is often a dynamic and
iterative process. Architects progressively draw various parts of the floorplan
according to their ideas and requirements, continuously adjusting and refining
throughout the design process. Therefore, the ability to predict a complete
floorplan from a partial one holds significant value in the design process.
Such prediction can help architects quickly generate preliminary designs,
improve design efficiency, and reduce the workload associated with repeated
modifications. To address this need, we propose FloorplanMAE, a self-supervised
learning framework for restoring incomplete floor plans into complete ones.
First, we developed a floor plan reconstruction dataset, FloorplanNet,
specifically trained on architectural floor plans. Secondly, we propose a floor
plan reconstruction method based on Masked Autoencoders (MAE), which
reconstructs missing parts by masking sections of the floor plan and training a
lightweight Vision Transformer (ViT). We evaluated the reconstruction accuracy
of FloorplanMAE and compared it with state-of-the-art benchmarks. Additionally,
we validated the model using real sketches from the early stages of
architectural design. Experimental results show that the FloorplanMAE model can
generate high-quality complete floor plans from incomplete partial plans. This
framework provides a scalable solution for floor plan generation, with broad
application prospects.

</details>


### [10] [On Reasoning Strength Planning in Large Reasoning Models](https://arxiv.org/abs/2506.08390)
*Leheng Sheng,An Zhang,Zijian Wu,Weixiang Zhao,Changshuo Shen,Yi Zhang,Xiang Wang,Tat-Seng Chua*

Main category: cs.AI

TL;DR: 大型推理模型（LRMs）能自动为更困难的问题分配更多推理强度（即推理标记数量），本文从模型激活的角度解释了这一现象，并揭示了其机制。


<details>
  <summary>Details</summary>
Motivation: 探索大型推理模型自动分配推理强度的内在机制，以更好地理解其行为。

Method: 通过分析模型激活，使用线性探针预测推理标记数量，并识别控制推理强度的预分配方向向量。

Result: 发现方向向量的大小调节推理强度，修改该向量可改变推理标记数量和性能。

Conclusion: 研究揭示了LRMs推理的内部机制，并提供了控制其行为的实用工具。

Abstract: Recent studies empirically reveal that large reasoning models (LRMs) can
automatically allocate more reasoning strengths (i.e., the number of reasoning
tokens) for harder problems, exhibiting difficulty-awareness for better task
performance. While this automatic reasoning strength allocation phenomenon has
been widely observed, its underlying mechanism remains largely unexplored. To
this end, we provide explanations for this phenomenon from the perspective of
model activations. We find evidence that LRMs pre-plan the reasoning strengths
in their activations even before generation, with this reasoning strength
causally controlled by the magnitude of a pre-allocated directional vector.
Specifically, we show that the number of reasoning tokens is predictable solely
based on the question activations using linear probes, indicating that LRMs
estimate the required reasoning strength in advance. We then uncover that LRMs
encode this reasoning strength through a pre-allocated directional vector
embedded in the activations of the model, where the vector's magnitude
modulates the reasoning strength. Subtracting this vector can lead to reduced
reasoning token number and performance, while adding this vector can lead to
increased reasoning token number and even improved performance. We further
reveal that this direction vector consistently yields positive reasoning length
prediction, and it modifies the logits of end-of-reasoning token </think> to
affect the reasoning length. Finally, we demonstrate two potential applications
of our findings: overthinking behavior detection and enabling efficient
reasoning on simple problems. Our work provides new insights into the internal
mechanisms of reasoning in LRMs and offers practical tools for controlling
their reasoning behaviors. Our code is available at
https://github.com/AlphaLab-USTC/LRM-plans-CoT.

</details>


### [11] [SafeCoT: Improving VLM Safety with Minimal Reasoning](https://arxiv.org/abs/2506.08399)
*Jiachen Ma,Zhanhui Zhou,Chao Yang,Chaochao Lu*

Main category: cs.AI

TL;DR: SafeCoT是一个轻量级、可解释的框架，通过规则引导的思维链监督改进视觉语言模型的安全拒绝行为。


<details>
  <summary>Details</summary>
Motivation: 解决视觉语言模型在高风险或模糊场景中安全响应的问题。

Method: 利用规则引导的思维链监督，最小化监督需求，帮助模型推理安全风险并做出上下文感知的拒绝。

Result: 实验表明，SafeCoT显著减少了过度拒绝并提升了泛化能力，即使在有限训练数据下。

Conclusion: SafeCoT为对齐视觉语言模型与安全关键目标提供了可扩展的解决方案。

Abstract: Ensuring safe and appropriate responses from vision-language models (VLMs)
remains a critical challenge, particularly in high-risk or ambiguous scenarios.
We introduce SafeCoT, a lightweight, interpretable framework that leverages
rule-based chain-of-thought (CoT) supervision to improve refusal behavior in
VLMs. Unlike prior methods that rely on large-scale safety annotations or
complex modeling, SafeCoT uses minimal supervision to help models reason about
safety risks and make context-aware refusals. Experiments across multiple
benchmarks show that SafeCoT significantly reduces overrefusal and enhances
generalization, even with limited training data. Our approach offers a scalable
solution for aligning VLMs with safety-critical objectives.

</details>


### [12] [Single-Node Trigger Backdoor Attacks in Graph-Based Recommendation Systems](https://arxiv.org/abs/2506.08401)
*Runze Li,Di Jin,Xiaobao Wang,Dongxiao He,Bingdao Feng,Zhen Wang*

Main category: cs.AI

TL;DR: 本文提出了一种新的图后门攻击方法，通过插入单个虚假用户节点，隐蔽地提升目标物品对目标用户的曝光率，同时减少对推荐系统性能的影响。


<details>
  <summary>Details</summary>
Motivation: 现有的图推荐系统在面对攻击时存在低隐蔽性和高破坏性的问题，需要一种更隐蔽且破坏性更低的攻击方法。

Method: 设计了一种单节点触发器生成器，通过插入一个虚假用户节点来暴露多个目标物品，并引入约束条件以减少对无关节点的影响。

Result: 实验结果显示，目标物品在99%的目标用户中的曝光率不低于50%，对推荐系统性能的影响控制在约5%以内。

Conclusion: 该方法有效提升了攻击的隐蔽性，同时显著降低了对推荐系统性能的破坏。

Abstract: Graph recommendation systems have been widely studied due to their ability to
effectively capture the complex interactions between users and items. However,
these systems also exhibit certain vulnerabilities when faced with attacks. The
prevailing shilling attack methods typically manipulate recommendation results
by injecting a large number of fake nodes and edges. However, such attack
strategies face two primary challenges: low stealth and high destructiveness.
To address these challenges, this paper proposes a novel graph backdoor attack
method that aims to enhance the exposure of target items to the target user in
a covert manner, without affecting other unrelated nodes. Specifically, we
design a single-node trigger generator, which can effectively expose multiple
target items to the target user by inserting only one fake user node.
Additionally, we introduce constraint conditions between the target nodes and
irrelevant nodes to mitigate the impact of fake nodes on the recommendation
system's performance. Experimental results show that the exposure of the target
items reaches no less than 50% in 99% of the target users, while the impact on
the recommendation system's performance is controlled within approximately 5%.

</details>


### [13] [Transforming Expert Knowledge into Scalable Ontology via Large Language Models](https://arxiv.org/abs/2506.08422)
*Ikkei Itoku,David Theil,Evelyn Eichelsdoerfer Uehara,Sreyoshi Bhaduri,Junnosuke Kuroda,Toshi Yumoto,Alex Gil,Natalie Perez,Rajesh Cherukuri,Naumaan Nayyar*

Main category: cs.AI

TL;DR: 提出了一种结合大型语言模型（LLMs）与专家校准的新框架，用于自动化分类法对齐，显著提升了映射质量与效率。


<details>
  <summary>Details</summary>
Motivation: 传统手动分类法对齐方法成本高且主观性强，现有自动化方法在处理语义关系和跨领域一致性方面存在不足。

Method: 结合LLMs、专家校准和迭代提示优化，通过多阶段提示工程和人工验证生成分类法链接及支持理由。

Result: 在概念必要性映射任务中，F1分数达到0.97，远超人工基准0.68。

Conclusion: 该方法能高效扩展分类法对齐，同时保持高质量映射，并在模糊案例中保留专家监督。

Abstract: Having a unified, coherent taxonomy is essential for effective knowledge
representation in domain-specific applications as diverse terminologies need to
be mapped to underlying concepts. Traditional manual approaches to taxonomy
alignment rely on expert review of concept pairs, but this becomes
prohibitively expensive and time-consuming at scale, while subjective
interpretations often lead to expert disagreements. Existing automated methods
for taxonomy alignment have shown promise but face limitations in handling
nuanced semantic relationships and maintaining consistency across different
domains. These approaches often struggle with context-dependent concept
mappings and lack transparent reasoning processes. We propose a novel framework
that combines large language models (LLMs) with expert calibration and
iterative prompt optimization to automate taxonomy alignment. Our method
integrates expert-labeled examples, multi-stage prompt engineering, and human
validation to guide LLMs in generating both taxonomy linkages and supporting
rationales. In evaluating our framework on a domain-specific mapping task of
concept essentiality, we achieved an F1-score of 0.97, substantially exceeding
the human benchmark of 0.68. These results demonstrate the effectiveness of our
approach in scaling taxonomy alignment while maintaining high-quality mappings
and preserving expert oversight for ambiguous cases.

</details>


### [14] [SHIELD: Multi-task Multi-distribution Vehicle Routing Solver with Sparsity and Hierarchy](https://arxiv.org/abs/2506.08424)
*Yong Liang Goh,Zhiguang Cao,Yining Ma,Jianan Zhou,Mohammad Haroon Dupty,Wee Sun Lee*

Main category: cs.AI

TL;DR: 论文提出SHIELD模型，通过稀疏性和层次性原则改进多任务多分布VRP问题，显著提升泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有基础模型忽略了现实世界中复杂的客户分布，需要更适应多任务多分布VRP的解决方案。

Method: 结合Mixture-of-Depths技术和上下文聚类层，动态选择节点并利用层次结构优化表示。

Result: 在9个真实地图和16种VRP变体上，SHIELD表现优于现有方法。

Conclusion: SHIELD通过稀疏性和层次性设计，显著提升了多任务多分布VRP的泛化能力。

Abstract: Recent advances toward foundation models for routing problems have shown
great potential of a unified deep model for various VRP variants. However, they
overlook the complex real-world customer distributions. In this work, we
advance the Multi-Task VRP (MTVRP) setting to the more realistic yet
challenging Multi-Task Multi-Distribution VRP (MTMDVRP) setting, and introduce
SHIELD, a novel model that leverages both sparsity and hierarchy principles.
Building on a deeper decoder architecture, we first incorporate the
Mixture-of-Depths (MoD) technique to enforce sparsity. This improves both
efficiency and generalization by allowing the model to dynamically select nodes
to use or skip each decoder layer, providing the needed capacity to adaptively
allocate computation for learning the task/distribution specific and shared
representations. We also develop a context-based clustering layer that exploits
the presence of hierarchical structures in the problems to produce better local
representations. These two designs inductively bias the network to identify key
features that are common across tasks and distributions, leading to
significantly improved generalization on unseen ones. Our empirical results
demonstrate the superiority of our approach over existing methods on 9
real-world maps with 16 VRP variants each.

</details>


### [15] [Hybrid Reasoning for Perception, Explanation, and Autonomous Action in Manufacturing](https://arxiv.org/abs/2506.08462)
*Christos Margadji,Sebastian W. Pattinson*

Main category: cs.AI

TL;DR: CIPHER是一种结合视觉-语言-动作（VLA）的模型框架，旨在通过混合专家和推理实现工业控制的类人推理，适用于数据稀缺的工业环境。


<details>
  <summary>Details</summary>
Motivation: 工业过程需要鲁棒性和适应性，但传统AI控制依赖大量标注数据且泛化能力有限。基础模型虽能整合知识，但缺乏工程所需的定量精度。

Method: CIPHER结合过程专家和回归模型，利用检索增强生成技术整合外部知识，支持物理驱动的链式推理，实现无标注输入的自主控制。

Result: CIPHER在分布外任务中表现出强泛化能力，能解释决策并生成精确指令，无需显式标注。

Conclusion: CIPHER为工业环境中的自主系统提供了精准、透明和可信任的部署基础。

Abstract: Industrial processes must be robust and adaptable, as environments and tasks
are often unpredictable, while operational errors remain costly and difficult
to detect. AI-based control systems offer a path forward, yet typically depend
on supervised learning with extensive labelled datasets, which limits their
ability to generalize across variable and data-scarce industrial settings.
Foundation models could enable broader reasoning and knowledge integration, but
rarely deliver the quantitative precision demanded by engineering applications.
Here, we introduceControl and Interpretation of Production via Hybrid Expertise
and Reasoning (CIPHER): a vision-language-action (VLA) model framework aiming
to replicate human-like reasoning for industrial control, instantiated in a
commercial-grade 3D printer. It integrates a process expert, a regression model
enabling quantitative characterization of system states required for
engineering tasks. CIPHER also incorporates retrieval-augmented generation to
access external expert knowledge and support physics-informed, chain-of-thought
reasoning. This hybrid architecture exhibits strong generalization to
out-of-distribution tasks. It interprets visual or textual inputs from process
monitoring, explains its decisions, and autonomously generates precise machine
instructions, without requiring explicit annotations. CIPHER thus lays the
foundations for autonomous systems that act with precision, reason with
context, and communicate decisions transparently, supporting safe and trusted
deployment in industrial settings.

</details>


### [16] [A Survey on Large Language Models for Mathematical Reasoning](https://arxiv.org/abs/2506.08446)
*Peng-Yuan Wang,Tian-Shuo Liu,Chenyang Wang,Yi-Di Wang,Shu Yan,Cheng-Xing Jia,Xu-Hui Liu,Xin-Wei Chen,Jia-Cheng Xu,Ziniu Li,Yang Yu*

Main category: cs.AI

TL;DR: 本文综述了大型语言模型（LLMs）在数学推理能力上的发展，分为理解和答案生成两个阶段，并探讨了提升方法及未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 数学推理是人工智能研究的重要挑战，LLMs在此领域取得显著进展，但仍有容量、效率和泛化等基础问题。

Method: 通过预训练策略提升理解能力，从直接预测到逐步推理（CoT），并采用训练无关提示和微调方法。

Result: LLMs在数学推理上取得进展，但仍面临挑战。

Conclusion: 未来研究方向包括高级预训练、知识增强、形式化推理框架和元泛化。

Abstract: Mathematical reasoning has long represented one of the most fundamental and
challenging frontiers in artificial intelligence research. In recent years,
large language models (LLMs) have achieved significant advances in this area.
This survey examines the development of mathematical reasoning abilities in
LLMs through two high-level cognitive phases: comprehension, where models gain
mathematical understanding via diverse pretraining strategies, and answer
generation, which has progressed from direct prediction to step-by-step
Chain-of-Thought (CoT) reasoning. We review methods for enhancing mathematical
reasoning, ranging from training-free prompting to fine-tuning approaches such
as supervised fine-tuning and reinforcement learning, and discuss recent work
on extended CoT and "test-time scaling". Despite notable progress, fundamental
challenges remain in terms of capacity, efficiency, and generalization. To
address these issues, we highlight promising research directions, including
advanced pretraining and knowledge augmentation techniques, formal reasoning
frameworks, and meta-generalization through principled learning paradigms. This
survey tries to provide some insights for researchers interested in enhancing
reasoning capabilities of LLMs and for those seeking to apply these techniques
to other domains.

</details>


### [17] [RHealthTwin: Towards Responsible and Multimodal Digital Twins for Personalized Well-being](https://arxiv.org/abs/2506.08486)
*Rahatara Ferdousi,M Anwar Hossain*

Main category: cs.AI

TL;DR: 论文提出了一种名为RHealthTwin的负责任框架，用于构建和管理AI驱动的数字孪生，以解决大型语言模型在医疗健康领域中的幻觉、偏见和伦理问题。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在医疗健康领域的应用存在幻觉、偏见、透明度和伦理问题，需要一种负责任的方法来解决这些问题。

Method: 提出了RHealthTwin框架，其核心是负责任提示引擎（RPE），通过动态提取预定义槽位来结构化输入，生成安全、相关且可解释的响应。

Result: RHealthTwin在四个健康领域（心理支持、症状分诊、营养规划和活动指导）中表现优异，BLEU=0.41，ROUGE-L=0.63，BERTScore=0.89，伦理合规和指令遵循指标超过90%。

Conclusion: RHealthTwin为负责任的大型语言模型应用提供了前瞻性基础，适用于健康和福祉领域。

Abstract: The rise of large language models (LLMs) has created new possibilities for
digital twins in healthcare. However, the deployment of such systems in
consumer health contexts raises significant concerns related to hallucination,
bias, lack of transparency, and ethical misuse. In response to recommendations
from health authorities such as the World Health Organization (WHO), we propose
Responsible Health Twin (RHealthTwin), a principled framework for building and
governing AI-powered digital twins for well-being assistance. RHealthTwin
processes multimodal inputs that guide a health-focused LLM to produce safe,
relevant, and explainable responses. At the core of RHealthTwin is the
Responsible Prompt Engine (RPE), which addresses the limitations of traditional
LLM configuration. Conventionally, users input unstructured prompt and the
system instruction to configure the LLM, which increases the risk of
hallucination. In contrast, RPE extracts predefined slots dynamically to
structure both inputs. This guides the language model to generate responses
that are context aware, personalized, fair, reliable, and explainable for
well-being assistance. The framework further adapts over time through a
feedback loop that updates the prompt structure based on user satisfaction. We
evaluate RHealthTwin across four consumer health domains including mental
support, symptom triage, nutrition planning, and activity coaching. RPE
achieves state-of-the-art results with BLEU = 0.41, ROUGE-L = 0.63, and
BERTScore = 0.89 on benchmark datasets. Also, we achieve over 90% in ethical
compliance and instruction-following metrics using LLM-as-judge evaluation,
outperforming baseline strategies. We envision RHealthTwin as a forward-looking
foundation for responsible LLM-based applications in health and well-being.

</details>


### [18] [FEDTAIL: Federated Long-Tailed Domain Generalization with Sharpness-Guided Gradient Matching](https://arxiv.org/abs/2506.08518)
*Sunny Gupta,Nikita Jangid,Shounak Das,Amit Sethi*

Main category: cs.AI

TL;DR: FedTAIL是一个联邦领域泛化框架，通过锐度引导的梯度对齐优化解决长尾分布和优化冲突问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法在长尾类分布和冲突优化目标下表现不佳，FedTAIL旨在解决这些问题。

Method: 采用梯度一致性正则化、类感知锐度最小化和曲率感知动态加权方案，并结合锐度感知扰动增强条件分布对齐。

Result: 在标准领域泛化基准测试中，FedTAIL实现了最先进的性能，尤其在领域偏移和标签不平衡情况下表现优异。

Conclusion: FedTAIL通过优化协调、类感知正则化和条件对齐的统一框架，在集中式和联邦式设置中均表现出有效性。

Abstract: Domain Generalization (DG) seeks to train models that perform reliably on
unseen target domains without access to target data during training. While
recent progress in smoothing the loss landscape has improved generalization,
existing methods often falter under long-tailed class distributions and
conflicting optimization objectives. We introduce FedTAIL, a federated domain
generalization framework that explicitly addresses these challenges through
sharpness-guided, gradient-aligned optimization. Our method incorporates a
gradient coherence regularizer to mitigate conflicts between classification and
adversarial objectives, leading to more stable convergence. To combat class
imbalance, we perform class-wise sharpness minimization and propose a
curvature-aware dynamic weighting scheme that adaptively emphasizes
underrepresented tail classes. Furthermore, we enhance conditional distribution
alignment by integrating sharpness-aware perturbations into entropy
regularization, improving robustness under domain shift. FedTAIL unifies
optimization harmonization, class-aware regularization, and conditional
alignment into a scalable, federated-compatible framework. Extensive
evaluations across standard domain generalization benchmarks demonstrate that
FedTAIL achieves state-of-the-art performance, particularly in the presence of
domain shifts and label imbalance, validating its effectiveness in both
centralized and federated settings. Code: https://github.com/sunnyinAI/FedTail

</details>


### [19] [Safe and Economical UAV Trajectory Planning in Low-Altitude Airspace: A Hybrid DRL-LLM Approach with Compliance Awareness](https://arxiv.org/abs/2506.08532)
*Yanwei Gong,Xiaolin Chang*

Main category: cs.AI

TL;DR: 论文提出了一种结合深度强化学习（DRL）和大语言模型（LLM）的新型无人机轨迹规划框架，以解决复杂城市环境中的路径规划问题。


<details>
  <summary>Details</summary>
Motivation: 低空经济的快速发展推动了无人机的广泛应用，但现有研究常忽略城市空域约束和经济效率等关键因素。

Method: 结合DRL与LLM推理，提出新型轨迹规划框架。

Result: 实验结果表明，该方法在数据收集率、避碰、成功着陆、法规合规性和能源效率等方面显著优于现有基线。

Conclusion: 该方法有效解决了低空经济网络约束下的无人机轨迹规划关键挑战。

Abstract: The rapid growth of the low-altitude economy has driven the widespread
adoption of unmanned aerial vehicles (UAVs). This growing deployment presents
new challenges for UAV trajectory planning in complex urban environments.
However, existing studies often overlook key factors, such as urban airspace
constraints and economic efficiency, which are essential in low-altitude
economy contexts. Deep reinforcement learning (DRL) is regarded as a promising
solution to these issues, while its practical adoption remains limited by low
learning efficiency. To overcome this limitation, we propose a novel UAV
trajectory planning framework that combines DRL with large language model (LLM)
reasoning to enable safe, compliant, and economically viable path planning.
Experimental results demonstrate that our method significantly outperforms
existing baselines across multiple metrics, including data collection rate,
collision avoidance, successful landing, regulatory compliance, and energy
efficiency. These results validate the effectiveness of our approach in
addressing UAV trajectory planning key challenges under constraints of the
low-altitude economy networking.

</details>


### [20] [HGFormer: A Hierarchical Graph Transformer Framework for Two-Stage Colonel Blotto Games via Reinforcement Learning](https://arxiv.org/abs/2506.08580)
*Yang Lv,Jinlong Lei,Peng Yi*

Main category: cs.AI

TL;DR: HGformer框架通过结合图Transformer和分层决策模型，解决了两阶段Colonel Blotto游戏中的资源分配问题，显著提升了效率和对抗收益。


<details>
  <summary>Details</summary>
Motivation: 传统方法难以在复杂网络拓扑和阶段依赖的两阶段Colonel Blotto游戏中实现全局最优策略。

Method: 提出HGformer框架，结合增强的图Transformer编码器和分层决策模型，并设计分层反馈强化学习算法。

Result: 实验表明HGformer在资源分配效率和对抗收益上优于现有方法。

Conclusion: HGformer在复杂动态游戏场景中表现优异，解决了传统方法的局限性。

Abstract: Two-stage Colonel Blotto game represents a typical adversarial resource
allocation problem, in which two opposing agents sequentially allocate
resources in a network topology across two phases: an initial resource
deployment followed by multiple rounds of dynamic reallocation adjustments. The
sequential dependency between game stages and the complex constraints imposed
by the graph topology make it difficult for traditional approaches to attain a
globally optimal strategy. To address these challenges, we propose a
hierarchical graph Transformer framework called HGformer. By incorporating an
enhanced graph Transformer encoder with structural biases and a two-agent
hierarchical decision model, our approach enables efficient policy generation
in large-scale adversarial environments. Moreover, we design a layer-by-layer
feedback reinforcement learning algorithm that feeds the long-term returns from
lower-level decisions back into the optimization of the higher-level strategy,
thus bridging the coordination gap between the two decision-making stages.
Experimental results demonstrate that, compared to existing hierarchical
decision-making or graph neural network methods, HGformer significantly
improves resource allocation efficiency and adversarial payoff, achieving
superior overall performance in complex dynamic game scenarios.

</details>


### [21] [FoldA: Computing Partial-Order Alignments Using Directed Net Unfoldings](https://arxiv.org/abs/2506.08627)
*Douwe Geurtjens,Xixi Lu*

Main category: cs.AI

TL;DR: 提出了一种基于Petri网展开的实时部分序对齐方法FoldA，解决了传统对齐方法的状态爆炸问题，并更准确地表示并发行为。


<details>
  <summary>Details</summary>
Motivation: 传统对齐方法在处理高并发和选择性的模型时会导致状态空间爆炸，且无法充分表示并发行为。

Method: 使用有向Petri网展开实时计算部分序对齐。

Result: 在合成和真实数据上验证，FoldA减少了排队状态数量并更准确地表示并发，但计算时间更长。

Conclusion: FoldA在并发表示和状态管理上优于传统方法，尽管计算时间增加。

Abstract: Conformance checking is a fundamental task of process mining, which
quantifies the extent to which the observed process executions match a
normative process model. The state-of-the-art approaches compute alignments by
exploring the state space formed by the synchronous product of the process
model and the trace. This often leads to state space explosion, particularly
when the model exhibits a high degree of choice and concurrency. Moreover, as
alignments inherently impose a sequential structure, they fail to fully
represent the concurrent behavior present in many real-world processes. To
address these limitations, this paper proposes a new technique for computing
partial-order alignments {on the fly using directed Petri net unfoldings, named
FoldA. We evaluate our technique on 485 synthetic model-log pairs and compare
it against Astar- and Dijkstra-alignments on 13 real-life model-log pairs and 6
benchmark pairs. The results show that our unfolding alignment, although it
requires more computation time, generally reduces the number of queued states
and provides a more accurate representation of concurrency.

</details>


### [22] [Modular Recurrence in Contextual MDPs for Universal Morphology Control](https://arxiv.org/abs/2506.08630)
*Laurens Engwegen,Daan Brinks,Wendelin Böhmer*

Main category: cs.AI

TL;DR: 提出了一种模块化循环架构，通过推断部分可观测的上下文信息，提升对未见机器人形态的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 开发通用控制器以提高计算和数据效率，解决多机器人控制中未见机器人泛化问题。

Method: 采用模块化循环架构，通过交互推断上下文信息，并在MuJoCo机器人集上评估。

Result: 在未见动态、运动学和拓扑的机器人上表现出显著性能提升。

Conclusion: 模块化循环架构能有效推断上下文信息，提升对未见机器人形态的泛化能力。

Abstract: A universal controller for any robot morphology would greatly improve
computational and data efficiency. By utilizing contextual information about
the properties of individual robots and exploiting their modular structure in
the architecture of deep reinforcement learning agents, steps have been made
towards multi-robot control. Generalization to new, unseen robots, however,
remains a challenge. In this paper we hypothesize that the relevant contextual
information is partially observable, but that it can be inferred through
interactions for better generalization to contexts that are not seen during
training. To this extent, we implement a modular recurrent architecture and
evaluate its generalization performance on a large set of MuJoCo robots. The
results show a substantial improved performance on robots with unseen dynamics,
kinematics, and topologies, in four different environments.

</details>


### [23] [Consistent Paths Lead to Truth: Self-Rewarding Reinforcement Learning for LLM Reasoning](https://arxiv.org/abs/2506.08745)
*Kongcheng Zhang,Qi Yao,Shunyu Liu,Yingjie Wang,Baisheng Lai,Jieping Ye,Mingli Song,Dacheng Tao*

Main category: cs.AI

TL;DR: 提出了一种自奖励强化学习框架CoVo，通过一致性奖励机制提升大语言模型的推理能力，无需外部监督。


<details>
  <summary>Details</summary>
Motivation: 强化学习在复杂推理任务中潜力巨大，但依赖外部监督限制了其广泛应用。

Method: 利用中间推理状态的一致性设计CoVo奖励机制，结合一致性、波动性和好奇心奖励。

Result: 在多样推理基准测试中，CoVo性能媲美或超越监督强化学习。

Conclusion: CoVo为无监督推理学习提供了可扩展的解决方案。

Abstract: Recent advances of Reinforcement Learning (RL) have highlighted its potential
in complex reasoning tasks, yet effective training often relies on external
supervision, which limits the broader applicability. In this work, we propose a
novel self-rewarding reinforcement learning framework to enhance Large Language
Model (LLM) reasoning by leveraging the consistency of intermediate reasoning
states across different reasoning trajectories. Our key insight is that correct
responses often exhibit consistent trajectory patterns in terms of model
likelihood: their intermediate reasoning states tend to converge toward their
own final answers (high consistency) with minimal deviation toward other
candidates (low volatility). Inspired by this observation, we introduce CoVo,
an intrinsic reward mechanism that integrates Consistency and Volatility via a
robust vector-space aggregation strategy, complemented by a curiosity bonus to
promote diverse exploration. CoVo enables LLMs to perform RL in a
self-rewarding manner, offering a scalable pathway for learning to reason
without external supervision. Extensive experiments on diverse reasoning
benchmarks show that CoVo achieves performance comparable to or even surpassing
supervised RL. Our code is available at https://github.com/sastpg/CoVo.

</details>


### [24] [A Sample Efficient Conditional Independence Test in the Presence of Discretization](https://arxiv.org/abs/2506.08747)
*Boyang Sun,Yu Yao,Xinshuai Dong,Zongfang Liu,Tongliang Liu,Yumou Qiu,Kun Zhang*

Main category: cs.AI

TL;DR: 本文提出了一种不依赖二值化的样本高效条件独立性检验方法，通过广义矩方法（GMM）解决过度识别限制问题，并利用节点回归建立统计量。


<details>
  <summary>Details</summary>
Motivation: 在离散化数据上直接应用条件独立性检验可能导致错误结论，而现有方法通过二值化会损失信息，因此需要一种更高效的方法。

Method: 通过广义矩方法（GMM）解决过度识别限制问题，利用节点回归建立统计量并推导其渐近分布。

Result: 理论和实证结果表明，该方法在多种数据集上表现出优越性和有效性。

Conclusion: 提出的方法避免了信息损失，显著提升了条件独立性检验的性能。

Abstract: In many real-world scenarios, interested variables are often represented as
discretized values due to measurement limitations. Applying Conditional
Independence (CI) tests directly to such discretized data, however, can lead to
incorrect conclusions. To address this, recent advancements have sought to
infer the correct CI relationship between the latent variables through
binarizing observed data. However, this process inevitably results in a loss of
information, which degrades the test's performance. Motivated by this, this
paper introduces a sample-efficient CI test that does not rely on the
binarization process. We find that the independence relationships of latent
continuous variables can be established by addressing an over-identifying
restriction problem with Generalized Method of Moments (GMM). Based on this
insight, we derive an appropriate test statistic and establish its asymptotic
distribution correctly reflecting CI by leveraging nodewise regression.
Theoretical findings and Empirical results across various datasets demonstrate
that the superiority and effectiveness of our proposed test. Our code
implementation is provided in https://github.com/boyangaaaaa/DCT

</details>


### [25] [Paths to Causality: Finding Informative Subgraphs Within Knowledge Graphs for Knowledge-Based Causal Discovery](https://arxiv.org/abs/2506.08771)
*Yuni Susanti,Michael Färber*

Main category: cs.AI

TL;DR: 论文提出了一种结合知识图谱（KGs）与大语言模型（LLMs）的新方法，用于提升基于知识的因果发现，解决了现有LLM方法的不稳定性问题。


<details>
  <summary>Details</summary>
Motivation: 传统基于观测数据的因果发现方法存在局限性，而基于知识的因果发现（如利用变量元数据）虽具潜力，但现有LLM方法结果不稳定且不可靠。

Method: 通过从KGs中提取信息丰富的元路径子图，并利用学习排序模型优化子图选择，将排名靠前的子图融入零样本提示，提升LLMs的因果推理能力。

Result: 在生物医学和开放领域数据集上的实验表明，该方法在F1分数上比基线方法最高提升44.4分。

Conclusion: 结合KGs与LLMs的方法显著提升了基于知识的因果发现的可靠性和效果，为复杂系统中的因果推理提供了新思路。

Abstract: Inferring causal relationships between variable pairs is crucial for
understanding multivariate interactions in complex systems. Knowledge-based
causal discovery -- which involves inferring causal relationships by reasoning
over the metadata of variables (e.g., names or textual context) -- offers a
compelling alternative to traditional methods that rely on observational data.
However, existing methods using Large Language Models (LLMs) often produce
unstable and inconsistent results, compromising their reliability for causal
inference. To address this, we introduce a novel approach that integrates
Knowledge Graphs (KGs) with LLMs to enhance knowledge-based causal discovery.
Our approach identifies informative metapath-based subgraphs within KGs and
further refines the selection of these subgraphs using Learning-to-Rank-based
models. The top-ranked subgraphs are then incorporated into zero-shot prompts,
improving the effectiveness of LLMs in inferring the causal relationship.
Extensive experiments on biomedical and open-domain datasets demonstrate that
our method outperforms most baselines by up to 44.4 points in F1 scores,
evaluated across diverse LLMs and KGs. Our code and datasets are available on
GitHub: https://github.com/susantiyuni/path-to-causality

</details>


### [26] [Measuring Data Science Automation: A Survey of Evaluation Tools for AI Assistants and Agents](https://arxiv.org/abs/2506.08800)
*Irene Testini,José Hernández-Orallo,Lorenzo Pacchiardi*

Main category: cs.AI

TL;DR: 本文综述了大型语言模型（LLM）助手和代理在数据科学中的评估，发现当前研究集中在少数目标导向活动，忽视了数据管理和探索性活动，且缺乏对中间层次人机协作的考虑。


<details>
  <summary>Details</summary>
Motivation: 数据科学需要从数据中提取洞察以支持决策，而LLM作为助手或代理的潜力尚未被充分评估。

Method: 通过文献综述，分析了LLM助手和代理在数据科学中的应用及其局限性。

Result: 研究发现当前评估集中在目标导向活动，忽视了数据管理和探索性任务，且缺乏对中间层次人机协作的关注。

Conclusion: 未来研究应更全面地评估LLM在数据科学中的作用，包括任务转换和更灵活的人机协作模式。

Abstract: Data science aims to extract insights from data to support decision-making
processes. Recently, Large Language Models (LLMs) are increasingly used as
assistants for data science, by suggesting ideas, techniques and small code
snippets, or for the interpretation of results and reporting. Proper automation
of some data-science activities is now promised by the rise of LLM agents,
i.e., AI systems powered by an LLM equipped with additional affordances--such
as code execution and knowledge bases--that can perform self-directed actions
and interact with digital environments. In this paper, we survey the evaluation
of LLM assistants and agents for data science. We find (1) a dominant focus on
a small subset of goal-oriented activities, largely ignoring data management
and exploratory activities; (2) a concentration on pure assistance or fully
autonomous agents, without considering intermediate levels of human-AI
collaboration; and (3) an emphasis on human substitution, therefore neglecting
the possibility of higher levels of automation thanks to task transformation.

</details>


### [27] [Your Brain on ChatGPT: Accumulation of Cognitive Debt when Using an AI Assistant for Essay Writing Task](https://arxiv.org/abs/2506.08872)
*Nataliya Kosmyna,Eugene Hauptmann,Ye Tong Yuan,Jessica Situ,Xian-Hao Liao,Ashly Vivian Beresnitzky,Iris Braunstein,Pattie Maes*

Main category: cs.AI

TL;DR: 研究探讨了LLM辅助写作对神经和行为的影响，发现LLM使用者在认知、语言和行为层面表现较差，长期依赖可能对教育产生负面影响。


<details>
  <summary>Details</summary>
Motivation: 探索LLM辅助写作对认知和行为的影响，评估其长期教育意义。

Method: 将参与者分为LLM、搜索引擎和无工具组，通过EEG和NLP分析认知负荷和写作质量。

Result: LLM使用者脑连接最弱，认知活动减少，写作所有权感最低，长期表现较差。

Conclusion: LLM依赖可能带来认知成本，需进一步研究AI在教育中的角色。

Abstract: This study explores the neural and behavioral consequences of LLM-assisted
essay writing. Participants were divided into three groups: LLM, Search Engine,
and Brain-only (no tools). Each completed three sessions under the same
condition. In a fourth session, LLM users were reassigned to Brain-only group
(LLM-to-Brain), and Brain-only users were reassigned to LLM condition
(Brain-to-LLM). A total of 54 participants took part in Sessions 1-3, with 18
completing session 4. We used electroencephalography (EEG) to assess cognitive
load during essay writing, and analyzed essays using NLP, as well as scoring
essays with the help from human teachers and an AI judge. Across groups, NERs,
n-gram patterns, and topic ontology showed within-group homogeneity. EEG
revealed significant differences in brain connectivity: Brain-only participants
exhibited the strongest, most distributed networks; Search Engine users showed
moderate engagement; and LLM users displayed the weakest connectivity.
Cognitive activity scaled down in relation to external tool use. In session 4,
LLM-to-Brain participants showed reduced alpha and beta connectivity,
indicating under-engagement. Brain-to-LLM users exhibited higher memory recall
and activation of occipito-parietal and prefrontal areas, similar to Search
Engine users. Self-reported ownership of essays was the lowest in the LLM group
and the highest in the Brain-only group. LLM users also struggled to accurately
quote their own work. While LLMs offer immediate convenience, our findings
highlight potential cognitive costs. Over four months, LLM users consistently
underperformed at neural, linguistic, and behavioral levels. These results
raise concerns about the long-term educational implications of LLM reliance and
underscore the need for deeper inquiry into AI's role in learning.

</details>


### [28] [Preference-Driven Multi-Objective Combinatorial Optimization with Conditional Computation](https://arxiv.org/abs/2506.08898)
*Mingfeng Fan,Jianan Zhou,Yifeng Zhang,Yaoxin Wu,Jinbiao Chen,Guillaume Adrien Sartoretti*

Main category: cs.AI

TL;DR: POCCO是一个新颖的即插即用框架，通过自适应选择子问题的模型结构并基于偏好信号优化，解决了传统方法在MOCOPs中性能不佳的问题。


<details>
  <summary>Details</summary>
Motivation: 传统深度强化学习方法在处理多目标组合优化问题时，对所有子问题采用单一模型，导致解空间探索不足和性能不佳。

Method: 设计了条件计算块，将子问题路由到专用神经架构，并提出基于偏好的优化算法，学习获胜与失败解之间的成对偏好。

Result: 在四个经典MOCOP基准测试中，POCCO表现出显著优势和强泛化能力。

Conclusion: POCCO通过自适应模型选择和偏好驱动优化，显著提升了MOCOPs的性能。

Abstract: Recent deep reinforcement learning methods have achieved remarkable success
in solving multi-objective combinatorial optimization problems (MOCOPs) by
decomposing them into multiple subproblems, each associated with a specific
weight vector. However, these methods typically treat all subproblems equally
and solve them using a single model, hindering the effective exploration of the
solution space and thus leading to suboptimal performance. To overcome the
limitation, we propose POCCO, a novel plug-and-play framework that enables
adaptive selection of model structures for subproblems, which are subsequently
optimized based on preference signals rather than explicit reward values.
Specifically, we design a conditional computation block that routes subproblems
to specialized neural architectures. Moreover, we propose a preference-driven
optimization algorithm that learns pairwise preferences between winning and
losing solutions. We evaluate the efficacy and versatility of POCCO by applying
it to two state-of-the-art neural methods for MOCOPs. Experimental results
across four classic MOCOP benchmarks demonstrate its significant superiority
and strong generalization.

</details>


### [29] [IntTrajSim: Trajectory Prediction for Simulating Multi-Vehicle driving at Signalized Intersections](https://arxiv.org/abs/2506.08957)
*Yash Ranjan,Rahul Sengupta,Anand Rangarajan,Sanjay Ranka*

Main category: cs.AI

TL;DR: 论文提出了一种基于数据驱动的交通模拟器，通过深度生成模型预测车辆轨迹，并引入交通工程相关指标进行评估。


<details>
  <summary>Details</summary>
Motivation: 传统基于规则的交通模拟器难以准确模拟真实驾驶行为，而交通交叉口在安全和效率方面至关重要，因此需要开发更真实的模拟方法。

Method: 提出了一种多头部自注意力轨迹预测模型，结合信号信息，并通过模拟闭环流程评估模型。

Result: 新模型在交通工程相关指标上优于先前模型。

Conclusion: 研究为交通交叉口的模拟提供了更真实的数据驱动方法，并验证了其有效性。

Abstract: Traffic simulators are widely used to study the operational efficiency of
road infrastructure, but their rule-based approach limits their ability to
mimic real-world driving behavior. Traffic intersections are critical
components of the road infrastructure, both in terms of safety risk (nearly 28%
of fatal crashes and 58% of nonfatal crashes happen at intersections) as well
as the operational efficiency of a road corridor. This raises an important
question: can we create a data-driven simulator that can mimic the macro- and
micro-statistics of the driving behavior at a traffic intersection? Deep
Generative Modeling-based trajectory prediction models provide a good starting
point to model the complex dynamics of vehicles at an intersection. But they
are not tested in a "live" micro-simulation scenario and are not evaluated on
traffic engineering-related metrics. In this study, we propose traffic
engineering-related metrics to evaluate generative trajectory prediction models
and provide a simulation-in-the-loop pipeline to do so. We also provide a
multi-headed self-attention-based trajectory prediction model that incorporates
the signal information, which outperforms our previous models on the evaluation
metrics.

</details>


### [30] [Evaluating Generative Vehicle Trajectory Models for Traffic Intersection Dynamics](https://arxiv.org/abs/2506.08963)
*Yash Ranjan,Rahul Sengupta,Anand Rangarajan,Sanjay Ranka*

Main category: cs.AI

TL;DR: 论文提出了一种新的分析工具，用于训练和评估交通预测模型，重点关注交通工程视角的指标，揭示了现有模型在规则遵守上的不足。


<details>
  <summary>Details</summary>
Motivation: 交通路口是事故多发区，现有模型仅关注轨迹重建误差，缺乏对交通规则遵守的评估。

Method: 训练多车辆轨迹预测模型，并在微模拟器中在线评估其性能，引入新指标衡量违规行为。

Result: 即使输入理想轨迹且重建误差低，模型生成的轨迹仍违反交通规则。

Conclusion: 新工具和指标能更全面地评估模型性能，为交通工程提供更实用的洞察。

Abstract: Traffic Intersections are vital to urban road networks as they regulate the
movement of people and goods. However, they are regions of conflicting
trajectories and are prone to accidents. Deep Generative models of traffic
dynamics at signalized intersections can greatly help traffic authorities
better understand the efficiency and safety aspects. At present, models are
evaluated on computational metrics that primarily look at trajectory
reconstruction errors. They are not evaluated online in a `live'
microsimulation scenario. Further, these metrics do not adequately consider
traffic engineering-specific concerns such as red-light violations, unallowed
stoppage, etc. In this work, we provide a comprehensive analytics tool to
train, run, and evaluate models with metrics that give better insights into
model performance from a traffic engineering point of view. We train a
state-of-the-art multi-vehicle trajectory forecasting model on a large dataset
collected by running a calibrated scenario of a real-world urban intersection.
We then evaluate the performance of the prediction models, online in a
microsimulator, under unseen traffic conditions. We show that despite using
ideally-behaved trajectories as input, and achieving low trajectory
reconstruction errors, the generated trajectories show behaviors that break
traffic rules. We introduce new metrics to evaluate such undesired behaviors
and present our results.

</details>


### [31] [A Survey of Link Prediction in N-ary Knowledge Graphs](https://arxiv.org/abs/2506.08970)
*Jiyao Wei,Saiping Guan,Da Li,Xiaolong Jin,Jiafeng Guo,Xueqi Cheng*

Main category: cs.AI

TL;DR: 本文是关于N-ary知识图谱（NKGs）中链接预测的首个全面综述，涵盖领域概述、方法分类、性能分析和未来研究方向。


<details>
  <summary>Details</summary>
Motivation: NKGs能高效表示复杂事实，但链接预测任务对补全图谱和提升下游应用性能至关重要，需要系统总结。

Method: 通过综述现有方法，系统分类并分析其性能和应用场景。

Result: 提供了对NKGs链接预测领域的全面概述，并识别了未来研究的潜在方向。

Conclusion: 本文填补了NKGs链接预测领域的综述空白，为未来研究提供了指导。

Abstract: N-ary Knowledge Graphs (NKGs) are a specialized type of knowledge graph
designed to efficiently represent complex real-world facts. Unlike traditional
knowledge graphs, where a fact typically involves two entities, NKGs can
capture n-ary facts containing more than two entities. Link prediction in NKGs
aims to predict missing elements within these n-ary facts, which is essential
for completing NKGs and improving the performance of downstream applications.
This task has recently gained significant attention. In this paper, we present
the first comprehensive survey of link prediction in NKGs, providing an
overview of the field, systematically categorizing existing methods, and
analyzing their performance and application scenarios. We also outline
promising directions for future research.

</details>


### [32] [AbstentionBench: Reasoning LLMs Fail on Unanswerable Questions](https://arxiv.org/abs/2506.09038)
*Polina Kirichenko,Mark Ibrahim,Kamalika Chaudhuri,Samuel J. Bell*

Main category: cs.AI

TL;DR: 论文提出了AbstentionBench，一个评估大型语言模型（LLMs）在不确定情况下拒绝回答能力的基准测试，发现现有模型在此方面表现不佳，且推理微调反而降低了拒绝回答的能力。


<details>
  <summary>Details</summary>
Motivation: 研究LLMs在现实场景中如何合理拒绝回答不确定或无法回答的问题，以提高其可靠性。

Method: 引入AbstentionBench，一个包含20个多样化数据集的基准测试，评估LLMs在未知答案、模糊问题、错误前提等情况下的拒绝回答能力。

Result: 研究发现LLMs在拒绝回答方面表现不佳，推理微调反而降低了能力（平均下降24%），且模型规模扩展对此帮助有限。

Conclusion: LLMs在拒绝回答不确定问题方面仍有待改进，AbstentionBench的发布旨在推动相关研究。

Abstract: For Large Language Models (LLMs) to be reliably deployed in both everyday and
high-stakes domains, knowing when not to answer is equally critical as
answering correctly. Real-world user queries, which can be underspecified,
ill-posed, or fundamentally unanswerable, require LLMs to reason about
uncertainty and selectively abstain -- i.e., refuse to answer definitively.
However, abstention remains understudied, without a systematic evaluation
framework for modern LLMs. In this work, we introduce AbstentionBench, a
large-scale benchmark for holistically evaluating abstention across 20 diverse
datasets, including questions with unknown answers, underspecification, false
premises, subjective interpretations, and outdated information. Evaluating 20
frontier LLMs reveals abstention is an unsolved problem, and one where scaling
models is of little use. While recent reasoning LLMs have shown impressive
results in complex problem solving, surprisingly, we find that reasoning
fine-tuning degrades abstention (by $24\%$ on average), even for math and
science domains on which reasoning models are explicitly trained. We find that
while a carefully crafted system prompt can boost abstention in practice, it
does not resolve models' fundamental inability to reason about uncertainty. We
release AbstentionBench to foster research into advancing LLM reliability.

</details>


### [33] [VIKI-R: Coordinating Embodied Multi-Agent Cooperation via Reinforcement Learning](https://arxiv.org/abs/2506.09049)
*Li Kang,Xiufeng Song,Heng Zhou,Yiran Qin,Jie Yang,Xiaohong Liu,Philip Torr,Lei Bai,Zhenfei Yin*

Main category: cs.AI

TL;DR: VIKI-Bench是一个针对多智能体协作的层次化基准测试，包含三个层次：智能体激活、任务规划和轨迹感知。VIKI-R是一个两阶段框架，结合了视觉语言模型和强化学习，显著优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 协调动态环境中的多智能体是AI的核心挑战，现有视觉语言模型在多智能体协作中支持有限。

Method: 提出VIKI-Bench基准和VIKI-R框架，后者通过微调预训练视觉语言模型并结合强化学习。

Result: VIKI-R在所有任务层次上显著优于基线方法，并展现出异构智能体的组合协作模式。

Conclusion: VIKI-Bench和VIKI-R为多智能体视觉驱动协作提供了统一的测试平台和方法。

Abstract: Coordinating multiple embodied agents in dynamic environments remains a core
challenge in artificial intelligence, requiring both perception-driven
reasoning and scalable cooperation strategies. While recent works have
leveraged large language models (LLMs) for multi-agent planning, a few have
begun to explore vision-language models (VLMs) for visual reasoning. However,
these VLM-based approaches remain limited in their support for diverse
embodiment types. In this work, we introduce VIKI-Bench, the first hierarchical
benchmark tailored for embodied multi-agent cooperation, featuring three
structured levels: agent activation, task planning, and trajectory perception.
VIKI-Bench includes diverse robot embodiments, multi-view visual observations,
and structured supervision signals to evaluate reasoning grounded in visual
inputs. To demonstrate the utility of VIKI-Bench, we propose VIKI-R, a
two-stage framework that fine-tunes a pretrained vision-language model (VLM)
using Chain-of-Thought annotated demonstrations, followed by reinforcement
learning under multi-level reward signals. Our extensive experiments show that
VIKI-R significantly outperforms baselines method across all task levels.
Furthermore, we show that reinforcement learning enables the emergence of
compositional cooperation patterns among heterogeneous agents. Together,
VIKI-Bench and VIKI-R offer a unified testbed and method for advancing
multi-agent, visual-driven cooperation in embodied AI systems.

</details>


### [34] [ALE-Bench: A Benchmark for Long-Horizon Objective-Driven Algorithm Engineering](https://arxiv.org/abs/2506.09050)
*Yuki Imajuku,Kohki Horie,Yoichi Iwata,Kensho Aoki,Naohiro Takahashi,Takuya Akiba*

Main category: cs.AI

TL;DR: ALE-Bench是一个新的基准测试，用于评估AI系统在基于分数的算法编程竞赛中的表现，重点关注硬优化问题。


<details>
  <summary>Details</summary>
Motivation: 评估AI系统在算法工程中的表现，尤其是在复杂优化问题（如物流、排班等）上的能力。

Method: 基于AtCoder Heuristic Contests的真实任务，设计了一个支持长时间迭代优化的软件框架，并测试了前沿LLMs的表现。

Result: 前沿LLMs在特定问题上表现优异，但在问题一致性和长期规划能力上与人类仍有差距。

Conclusion: ALE-Bench有助于推动AI在复杂优化问题上的进一步发展。

Abstract: How well do AI systems perform in algorithm engineering for hard optimization
problems in domains such as package-delivery routing, crew scheduling, factory
production planning, and power-grid balancing? We introduce ALE-Bench, a new
benchmark for evaluating AI systems on score-based algorithmic programming
contests. Drawing on real tasks from the AtCoder Heuristic Contests, ALE-Bench
presents optimization problems that are computationally hard and admit no known
exact solution. Unlike short-duration, pass/fail coding benchmarks, ALE-Bench
encourages iterative solution refinement over long time horizons. Our software
framework supports interactive agent architectures that leverage test-run
feedback and visualizations. Our evaluation of frontier LLMs revealed that
while they demonstrate high performance on specific problems, a notable gap
remains compared to humans in terms of consistency across problems and
long-horizon problem-solving capabilities. This highlights the need for this
benchmark to foster future AI advancements.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [35] [CAF-I: A Collaborative Multi-Agent Framework for Enhanced Irony Detection with Large Language Models](https://arxiv.org/abs/2506.08430)
*Ziqi. Liu,Ziyang. Zhou,Mingxuan. Hu*

Main category: cs.CL

TL;DR: 本文提出了一种名为CAF-I的多智能体框架，用于解决现有LLM在讽刺检测中的单视角限制、理解不足和缺乏可解释性问题，通过多维度分析和协作优化实现了SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 现有LLM方法在讽刺检测中存在单视角限制、理解不足和缺乏可解释性等问题，亟需一种更全面且可解释的解决方案。

Method: CAF-I采用多智能体系统，包括Context、Semantics和Rhetoric三个专业代理进行多维分析，并通过交互协作优化，最终由Decision Agent整合结果，Refinement Evaluator Agent提供反馈。

Result: 在基准数据集上，CAF-I实现了零样本SOTA性能，平均Macro-F1为76.31，比之前最强基线提升了4.98。

Conclusion: CAF-I通过模拟人类多视角分析，显著提升了讽刺检测的准确性和可解释性。

Abstract: Large language model (LLM) have become mainstream methods in the field of
sarcasm detection. However, existing LLM methods face challenges in irony
detection, including: 1. single-perspective limitations, 2. insufficient
comprehensive understanding, and 3. lack of interpretability. This paper
introduces the Collaborative Agent Framework for Irony (CAF-I), an LLM-driven
multi-agent system designed to overcome these issues. CAF-I employs specialized
agents for Context, Semantics, and Rhetoric, which perform multidimensional
analysis and engage in interactive collaborative optimization. A Decision Agent
then consolidates these perspectives, with a Refinement Evaluator Agent
providing conditional feedback for optimization. Experiments on benchmark
datasets establish CAF-I's state-of-the-art zero-shot performance. Achieving
SOTA on the vast majority of metrics, CAF-I reaches an average Macro-F1 of
76.31, a 4.98 absolute improvement over the strongest prior baseline. This
success is attained by its effective simulation of human-like multi-perspective
analysis, enhancing detection accuracy and interpretability.

</details>


### [36] [Conservative Bias in Large Language Models: Measuring Relation Predictions](https://arxiv.org/abs/2506.08120)
*Toyin Aguda,Erik Wilson,Allan Anzagira,Simerjot Kaur,Charese Smiley*

Main category: cs.CL

TL;DR: LLMs在关系抽取任务中表现出保守偏差，倾向于选择无关系标签，导致信息丢失。研究发现保守偏差比幻觉更常见，并通过SBERT和LLM提示量化其影响。


<details>
  <summary>Details</summary>
Motivation: 研究LLMs在关系抽取任务中的保守偏差行为及其对信息完整性的影响。

Method: 通过多提示、数据集和关系类型系统评估保守偏差，引入Hobson选择概念，并使用SBERT和LLM提示量化语义相似性。

Result: 保守偏差比幻觉更常见，频率是后者的两倍。

Conclusion: 保守偏差虽避免错误关系分配，但导致信息丢失，需在模型设计中权衡。

Abstract: Large language models (LLMs) exhibit pronounced conservative bias in relation
extraction tasks, frequently defaulting to No_Relation label when an
appropriate option is unavailable. While this behavior helps prevent incorrect
relation assignments, our analysis reveals that it also leads to significant
information loss when reasoning is not explicitly included in the output. We
systematically evaluate this trade-off across multiple prompts, datasets, and
relation types, introducing the concept of Hobson's choice to capture scenarios
where models opt for safe but uninformative labels over hallucinated ones. Our
findings suggest that conservative bias occurs twice as often as hallucination.
To quantify this effect, we use SBERT and LLM prompts to capture the semantic
similarity between conservative bias behaviors in constrained prompts and
labels generated from semi-constrained and open-ended prompts.

</details>


### [37] [QA-LIGN: Aligning LLMs through Constitutionally Decomposed QA](https://arxiv.org/abs/2506.08123)
*Jacob Dineen,Aswin RRV,Qin Liu,Zhikun Xu,Xiao Ye,Ming Shen,Zhaonan Li,Shijie Lu,Chitta Baral,Muhao Chen,Ben Zhou*

Main category: cs.CL

TL;DR: QA-LIGN是一种自动符号奖励分解方法，通过为每个原则设计独立奖励组件，提升语言模型对齐的透明度和可控性。


<details>
  <summary>Details</summary>
Motivation: 传统奖励对齐方法将多样反馈压缩为单一标量奖励，导致目标不透明且难以解释。

Method: QA-LIGN为每个原则设计评估问题并生成独立奖励组件，替代传统黑盒奖励模型。

Result: 实验显示QA-LIGN在透明度和适应性上优于传统方法，性能与DPO基线相当或更好。

Conclusion: QA-LIGN在不牺牲性能的前提下，推动了语言模型对齐的可解释性和可控性。

Abstract: Alignment of large language models with explicit principles (such as
helpfulness, honesty, and harmlessness) is crucial for ensuring safe and
reliable AI systems. However, standard reward-based alignment methods typically
collapse diverse feedback into a single scalar reward, entangling multiple
objectives into one opaque training signal, which hinders interpretability. In
this work, we introduce QA-LIGN, an automatic symbolic reward decomposition
approach that preserves the structure of each constitutional principle within
the reward mechanism. Instead of training a black-box reward model that outputs
a monolithic score, QA-LIGN formulates principle-specific evaluation questions
and derives separate reward components for each principle, making it a drop-in
reward model replacement. Experiments aligning an uncensored large language
model with a set of constitutional principles demonstrate that QA-LIGN offers
greater transparency and adaptability in the alignment process. At the same
time, our approach achieves performance on par with or better than a DPO
baseline. Overall, these results represent a step toward more interpretable and
controllable alignment of language models, achieved without sacrificing
end-task performance.

</details>


### [38] [EconWebArena: Benchmarking Autonomous Agents on Economic Tasks in Realistic Web Environments](https://arxiv.org/abs/2506.08136)
*Zefang Liu,Yinzhu Quan*

Main category: cs.CL

TL;DR: EconWebArena是一个用于评估自主代理在复杂、多模态经济任务中的表现的基准测试，包含360个任务，覆盖多个经济领域。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试缺乏对权威数据源和多模态经济推理的关注，EconWebArena旨在填补这一空白。

Method: 通过大型语言模型生成候选任务，并经过人工筛选以确保清晰性、可行性和数据可靠性。

Result: 评估显示多模态LLM在接地性、导航和多模态理解方面存在显著性能差距。

Conclusion: EconWebArena为经济网络智能提供了一个严格的测试平台，揭示了当前技术的局限性。

Abstract: We introduce EconWebArena, a benchmark for evaluating autonomous agents on
complex, multimodal economic tasks in realistic web environments. The benchmark
comprises 360 curated tasks from 82 authoritative websites spanning domains
such as macroeconomics, labor, finance, trade, and public policy. Each task
challenges agents to navigate live websites, interpret structured and visual
content, interact with real interfaces, and extract precise, time-sensitive
data through multi-step workflows. We construct the benchmark by prompting
multiple large language models (LLMs) to generate candidate tasks, followed by
rigorous human curation to ensure clarity, feasibility, and source reliability.
Unlike prior work, EconWebArena emphasizes fidelity to authoritative data
sources and the need for grounded web-based economic reasoning. We evaluate a
diverse set of state-of-the-art multimodal LLMs as web agents, analyze failure
cases, and conduct ablation studies to assess the impact of visual grounding,
plan-based reasoning, and interaction design. Our results reveal substantial
performance gaps and highlight persistent challenges in grounding, navigation,
and multimodal understanding, positioning EconWebArena as a rigorous testbed
for economic web intelligence.

</details>


### [39] [Multilingual Hate Speech Detection in Social Media Using Translation-Based Approaches with Large Language Models](https://arxiv.org/abs/2506.08147)
*Muhammad Usman,Muhammad Ahmad,M. Shahiki Tash,Irina Gelbukh,Rolando Quintero Tellez,Grigori Sidorov*

Main category: cs.CL

TL;DR: 论文提出了一种基于注意力层和多语言数据集（英语、乌尔都语、西班牙语）的仇恨言论检测方法，结合GPT-3.5 Turbo和Qwen 2.5 72B等模型，显著提升了检测性能。


<details>
  <summary>Details</summary>
Motivation: 社交媒体上的仇恨言论威胁在线安全和包容性，而乌尔都语等语言的仇恨言论检测研究不足，尤其是基于翻译的方法。

Method: 构建了一个三语数据集，采用注意力层和TF-IDF特征提取，结合GPT-3.5 Turbo和Qwen 2.5 72B等模型进行多语言仇恨言论检测。

Result: 在英语、西班牙语、乌尔都语及多语言联合模型中，F1分数显著提升（最高达8.97%），表现优于传统模型如SVM。

Conclusion: 该方法为多语言仇恨言论检测提供了高效解决方案，有助于构建更安全的数字社区。

Abstract: Social media platforms are critical spaces for public discourse, shaping
opinions and community dynamics, yet their widespread use has amplified harmful
content, particularly hate speech, threatening online safety and inclusivity.
While hate speech detection has been extensively studied in languages like
English and Spanish, Urdu remains underexplored, especially using
translation-based approaches. To address this gap, we introduce a trilingual
dataset of 10,193 tweets in English (3,834 samples), Urdu (3,197 samples), and
Spanish (3,162 samples), collected via keyword filtering, with a balanced
distribution of 4,849 Hateful and 5,344 Not-Hateful labels. Our methodology
leverages attention layers as a precursor to transformer-based models and large
language models (LLMs), enhancing feature extraction for multilingual hate
speech detection. For non-transformer models, we use TF-IDF for feature
extraction. The dataset is benchmarked using state-of-the-art models, including
GPT-3.5 Turbo and Qwen 2.5 72B, alongside traditional machine learning models
like SVM and other transformers (e.g., BERT, RoBERTa). Three annotators,
following rigorous guidelines, ensured high dataset quality, achieving a
Fleiss' Kappa of 0.821. Our approach, integrating attention layers with GPT-3.5
Turbo and Qwen 2.5 72B, achieves strong performance, with macro F1 scores of
0.87 for English (GPT-3.5 Turbo), 0.85 for Spanish (GPT-3.5 Turbo), 0.81 for
Urdu (Qwen 2.5 72B), and 0.88 for the joint multilingual model (Qwen 2.5 72B).
These results reflect improvements of 8.75% in English (over SVM baseline
0.80), 8.97% in Spanish (over SVM baseline 0.78), 5.19% in Urdu (over SVM
baseline 0.77), and 7.32% in the joint multilingual model (over SVM baseline
0.82). Our framework offers a robust solution for multilingual hate speech
detection, fostering safer digital communities worldwide.

</details>


### [40] [ETT-CKGE: Efficient Task-driven Tokens for Continual Knowledge Graph Embedding](https://arxiv.org/abs/2506.08158)
*Lijing Zhu,Qizhen Lan,Qing Tian,Wenbo Sun,Li Yang,Lu Xia,Yixin Xie,Xi Xiao,Tiehang Duan,Cui Tao,Shuteng Niu*

Main category: cs.CL

TL;DR: ETT-CKGE提出了一种高效、任务驱动的持续知识图谱嵌入方法，通过可学习的任务相关令牌实现知识迁移，显著提升了训练效率和可扩展性。


<details>
  <summary>Details</summary>
Motivation: 现有方法在知识保留和计算效率上存在不足，ETT-CKGE旨在解决这些问题。

Method: 引入可学习的任务驱动令牌，避免显式节点评分或遍历，通过矩阵操作实现知识迁移。

Result: 在六个基准数据集上表现优异，显著提升了训练效率和可扩展性。

Conclusion: ETT-CKGE在性能和效率上均优于现有方法，代码已开源。

Abstract: Continual Knowledge Graph Embedding (CKGE) seeks to integrate new knowledge
while preserving past information. However, existing methods struggle with
efficiency and scalability due to two key limitations: (1) suboptimal knowledge
preservation between snapshots caused by manually designed node/relation
importance scores that ignore graph dependencies relevant to the downstream
task, and (2) computationally expensive graph traversal for node/relation
importance calculation, leading to slow training and high memory overhead. To
address these limitations, we introduce ETT-CKGE (Efficient, Task-driven,
Tokens for Continual Knowledge Graph Embedding), a novel task-guided CKGE
method that leverages efficient task-driven tokens for efficient and effective
knowledge transfer between snapshots. Our method introduces a set of learnable
tokens that directly capture task-relevant signals, eliminating the need for
explicit node scoring or traversal. These tokens serve as consistent and
reusable guidance across snapshots, enabling efficient token-masked embedding
alignment between snapshots. Importantly, knowledge transfer is achieved
through simple matrix operations, significantly reducing training time and
memory usage. Extensive experiments across six benchmark datasets demonstrate
that ETT-CKGE consistently achieves superior or competitive predictive
performance, while substantially improving training efficiency and scalability
compared to state-of-the-art CKGE methods. The code is available at:
https://github.com/lijingzhu1/ETT-CKGE/tree/main

</details>


### [41] [Can Artificial Intelligence Write Like Borges? An Evaluation Protocol for Spanish Microfiction](https://arxiv.org/abs/2506.08172)
*Gerardo Aleman Manzanarez,Nora de la Cruz Arana,Jorge Garcia Flores,Yobany Garcia Medina,Raul Monroy,Nathalie Pernelle*

Main category: cs.CL

TL;DR: 论文提出GrAImes协议，基于文学理论评估AI生成的微小说，关注主题一致性、文本清晰度、解释深度和美学质量。


<details>
  <summary>Details</summary>
Motivation: 现有研究对AI生成故事的文学价值评估不足，尤其是美学质量。

Method: 提出GrAImes评估协议，结合文学理论，由文学专家和爱好者验证。

Result: 验证了评估协议的有效性，为AI生成微小说的文学价值评估提供基础。

Conclusion: GrAImes协议为AI生成微小说的文学评估提供了客观框架。

Abstract: Automated story writing has been a subject of study for over 60 years. Large
language models can generate narratively consistent and linguistically coherent
short fiction texts. Despite these advancements, rigorous assessment of such
outputs for literary merit - especially concerning aesthetic qualities - has
received scant attention. In this paper, we address the challenge of evaluating
AI-generated microfictions and argue that this task requires consideration of
literary criteria across various aspects of the text, such as thematic
coherence, textual clarity, interpretive depth, and aesthetic quality. To
facilitate this, we present GrAImes: an evaluation protocol grounded in
literary theory, specifically drawing from a literary perspective, to offer an
objective framework for assessing AI-generated microfiction. Furthermore, we
report the results of our validation of the evaluation protocol, as answered by
both literature experts and literary enthusiasts. This protocol will serve as a
foundation for evaluating automatically generated microfictions and assessing
their literary value.

</details>


### [42] [LLM-BT: Back-Translation as a Framework for Terminology Standardization and Dynamic Semantic Embedding](https://arxiv.org/abs/2506.08174)
*Li Weigang,Pedro Carvalho Brom*

Main category: cs.CL

TL;DR: LLM-BT是一种基于大语言模型的回译框架，用于自动化术语验证和标准化，通过跨语言语义对齐实现高一致性。


<details>
  <summary>Details</summary>
Motivation: 快速增长的英语技术术语挑战了传统专家驱动的标准化方法，尤其在AI和量子计算等快速发展的领域。手动方法难以保证多语言一致性。

Method: LLM-BT采用回译框架，结合术语级一致性验证、多路径验证工作流和动态语义嵌入，实现跨语言术语标准化。

Result: 实验显示，LLM-BT在术语一致性（90%以上匹配）和跨语言鲁棒性（BLEU > 0.45，葡萄牙语准确率100%）方面表现优异。

Conclusion: LLM-BT将回译转化为多语言术语标准化的主动引擎，支持人机协作，为全球科技领域的术语治理提供基础设施。

Abstract: The rapid growth of English technical terms challenges traditional
expert-driven standardization, especially in fast-evolving fields like AI and
quantum computing. Manual methods struggle to ensure multilingual consistency.
We propose \textbf{LLM-BT}, a back-translation framework powered by large
language models (LLMs) to automate terminology verification and standardization
via cross-lingual semantic alignment. Our contributions are: \textbf{(1)
Term-Level Consistency Validation:} Using English $\rightarrow$ intermediate
language $\rightarrow$ English back-translation, LLM-BT achieves high term
consistency across models (e.g., GPT-4, DeepSeek, Grok), with case studies
showing over 90\% exact or semantic matches. \textbf{(2) Multi-Path
Verification Workflow:} A novel ``Retrieve--Generate--Verify--Optimize''
pipeline integrates serial (e.g., EN $\rightarrow$ ZHcn $\rightarrow$ ZHtw
$\rightarrow$ EN) and parallel (e.g., EN $\rightarrow$ Chinese/Portuguese
$\rightarrow$ EN) BT routes. BLEU and term accuracy indicate strong
cross-lingual robustness (BLEU $>$ 0.45; Portuguese accuracy 100\%).
\textbf{(3) Back-Translation as Semantic Embedding:} BT is conceptualized as
dynamic semantic embedding, revealing latent meaning trajectories. Unlike
static embeddings, LLM-BT provides transparent path-based embeddings shaped by
model evolution. LLM-BT transforms back-translation into an active engine for
multilingual terminology standardization, enabling human--AI collaboration:
machines ensure semantic fidelity, humans guide cultural interpretation. This
infrastructure supports terminology governance across scientific and
technological fields worldwide.

</details>


### [43] [Unable to forget: Proactive lnterference Reveals Working Memory Limits in LLMs Beyond Context Length](https://arxiv.org/abs/2506.08184)
*Chupei Wang,Jiaqiu Vince Sun*

Main category: cs.CL

TL;DR: 研究探讨了大型语言模型（LLM）中信息检索与生成能力的关联，发现上下文干扰（intra-context interference）显著影响检索准确性，提示存在工作记忆瓶颈。


<details>
  <summary>Details</summary>
Motivation: 尽管长上下文被认为有助于检索，但上下文干扰的影响尚未充分研究。研究旨在揭示LLM在干扰下的检索能力限制。

Method: 采用认知科学中的主动干扰（PI）范式，设计PI-LLM评估任务，通过流式更新语义相关的键值对并查询最终值，测试LLM的检索准确性。

Result: 随着干扰累积，LLM检索准确性呈对数线性下降至零，错误源于检索被覆盖的旧值。提示工程缓解干扰效果有限。

Conclusion: LLM在干扰下的检索能力受限，表明存在工作记忆瓶颈，需开发新方法增强模型抑制无关内容的能力。

Abstract: Information retrieval in Large Language Models (LLMs) is increasingly
recognized as intertwined with generation capabilities rather than mere lookup.
While longer contexts are often assumed to improve retrieval, the effects of
intra-context interference remain understudied. To address this, we adapt the
proactive interference (PI) paradigm from cognitive science, where earlier
information disrupts recall of newer updates. In humans, susceptibility to such
interference is inversely linked to working memory capacity. We introduce
PI-LLM, an evaluation that sequentially streams semantically related key-value
updates and queries only the final values. Although these final values are
clearly positioned just before the query, LLM retrieval accuracy declines
log-linearly toward zero as interference accumulates; errors arise from
retrieving previously overwritten values. Attempts to mitigate interference via
prompt engineering (e.g., instructing models to ignore earlier input) yield
limited success. These findings reveal a fundamental constraint on LLMs'
ability to disentangle interference and flexibly manipulate information,
suggesting a working memory bottleneck beyond mere context access. This calls
for approaches that strengthen models' ability to suppress irrelevant content
during retrieval.

</details>


### [44] ["I Wrote, I Paused, I Rewrote" Teaching LLMs to Read Between the Lines of Student Writing](https://arxiv.org/abs/2506.08221)
*Samra Zafar,Shaheer Minhas,Syed Ali Hassan Zaidi,Arfa Naeem,Zahra Ali*

Main category: cs.CL

TL;DR: 研究探讨了利用写作过程数据（如键盘记录和定期快照）改进大型语言模型（LLM）对学生写作反馈的效果。结果显示，学生更喜欢基于过程的反馈，且某些编辑行为与更高的写作得分相关。


<details>
  <summary>Details</summary>
Motivation: 当前LLM的反馈仅基于最终文本，缺乏对写作过程的了解，可能无法准确反映学生的思考和修改过程。

Method: 开发了一个数字写作工具，记录学生的打字行为和文章演变过程。20名学生使用该工具完成限时写作，LLM基于最终文本和完整写作轨迹生成反馈，学生随后评估反馈的实用性和相关性。

Result: 学生更倾向于过程感知的LLM反馈，认为其更符合自身思维。某些编辑行为（如添加内容或重组段落）与更高的连贯性和详述得分相关。

Conclusion: 通过让LLM更了解写作过程，可以提供更有意义、个性化和支持性的反馈。

Abstract: Large language models(LLMs) like Gemini are becoming common tools for
supporting student writing. But most of their feedback is based only on the
final essay missing important context about how that text was written. In this
paper, we explore whether using writing process data, collected through
keystroke logging and periodic snapshots, can help LLMs give feedback that
better reflects how learners think and revise while writing. We built a digital
writing tool that captures both what students type and how their essays evolve
over time. Twenty students used this tool to write timed essays, which were
then evaluated in two ways: (i) LLM generated feedback using both the final
essay and the full writing trace, and (ii) After the task, students completed
surveys about how useful and relatable they found the feedback. Early results
show that learners preferred the process-aware LLM feedback, finding it more in
tune with their own thinking. We also found that certain types of edits, like
adding new content or reorganizing paragraphs, aligned closely with higher
scores in areas like coherence and elaboration. Our findings suggest that
making LLMs more aware of the writing process can lead to feedback that feels
more meaningful, personal, and supportive.

</details>


### [45] [Compound AI Systems Optimization: A Survey of Methods, Challenges, and Future Directions](https://arxiv.org/abs/2506.08234)
*Yu-Ang Lee,Guan-Ting Yi,Mei-Yi Liu,Jui-Chao Lu,Guan-Bo Yang,Yun-Nung Chen*

Main category: cs.CL

TL;DR: 本文综述了复合AI系统优化的最新进展，包括数值和语言技术，并提出了未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 随着复合AI系统复杂度的增加，优化组件及其交互的新挑战出现，需要系统性研究。

Method: 通过系统综述，形式化复合AI系统优化概念，并对现有方法进行分类。

Result: 总结了数值和语言技术的最新进展，并提出了开放研究挑战。

Conclusion: 复合AI系统优化是一个快速发展的领域，未来需进一步探索语言反馈等新方法。

Abstract: Recent advancements in large language models (LLMs) and AI systems have led
to a paradigm shift in the design and optimization of complex AI workflows. By
integrating multiple components, compound AI systems have become increasingly
adept at performing sophisticated tasks. However, as these systems grow in
complexity, new challenges arise in optimizing not only individual components
but also their interactions. While traditional optimization methods such as
supervised fine-tuning (SFT) and reinforcement learning (RL) remain
foundational, the rise of natural language feedback introduces promising new
approaches, especially for optimizing non-differentiable systems. This paper
provides a systematic review of recent progress in optimizing compound AI
systems, encompassing both numerical and language-based techniques. We
formalize the notion of compound AI system optimization, classify existing
methods along several key dimensions, and highlight open research challenges
and future directions in this rapidly evolving field. A list of surveyed papers
is publicly available at https://github.com/MiuLab/AISysOpt-Survey.

</details>


### [46] [mSTEB: Massively Multilingual Evaluation of LLMs on Speech and Text Tasks](https://arxiv.org/abs/2506.08400)
*Luel Hagos Beyene,Vivek Verma,Min Ma,Jesujoba O. Alabi,Fabian David Schmidt,Joyce Nakatumba-Nabende,David Ifeoluwa Adelani*

Main category: cs.CL

TL;DR: 论文介绍了mSTEB基准，用于评估LLMs在低资源语言上的表现，发现高资源与低资源语言间存在显著性能差距。


<details>
  <summary>Details</summary>
Motivation: 现有LLMs评估多集中于英语和高资源语言，缺乏对低资源语言的标准化评估。

Method: 提出mSTEB基准，涵盖语言识别、文本分类、问答和翻译任务，评估了Gemini 2.0 Flash、GPT-4o (Audio)等模型。

Result: 高资源与低资源语言（尤其是非洲和美洲/大洋洲语言）间存在显著性能差距。

Conclusion: 需更多投资以解决低资源语言在LLMs中的代表性不足问题。

Abstract: Large Language models (LLMs) have demonstrated impressive performance on a
wide range of tasks, including in multimodal settings such as speech. However,
their evaluation is often limited to English and a few high-resource languages.
For low-resource languages, there is no standardized evaluation benchmark. In
this paper, we address this gap by introducing mSTEB, a new benchmark to
evaluate the performance of LLMs on a wide range of tasks covering language
identification, text classification, question answering, and translation tasks
on both speech and text modalities. We evaluated the performance of leading
LLMs such as Gemini 2.0 Flash and GPT-4o (Audio) and state-of-the-art open
models such as Qwen 2 Audio and Gemma 3 27B. Our evaluation shows a wide gap in
performance between high-resource and low-resource languages, especially for
languages spoken in Africa and Americas/Oceania. Our findings show that more
investment is needed to address their under-representation in LLMs coverage.

</details>


### [47] [Can AI Validate Science? Benchmarking LLMs for Accurate Scientific Claim $\rightarrow$ Evidence Reasoning](https://arxiv.org/abs/2506.08235)
*Shashidhar Reddy Javaji,Yupeng Cao,Haohang Li,Yangyang Yu,Nikhil Muralidhar,Zining Zhu*

Main category: cs.CL

TL;DR: CLAIM-BENCH是一个评估大语言模型（LLMs）在科学论文中提取和验证声明与证据能力的基准测试，揭示了LLMs在处理复杂科学内容时的局限性。


<details>
  <summary>Details</summary>
Motivation: 研究动机是探索LLMs是否能够真正理解和处理科学论文中声明与证据之间的复杂逻辑关系。

Method: 通过系统比较三种启发自分治策略的方法，在六种LLMs上进行评估，涉及300多个声明-证据对。

Result: 结果显示，闭源模型（如GPT-4和Claude）在精确率和召回率上优于开源模型，且三阶段和逐一提示策略显著提升了性能，但计算成本增加。

Conclusion: CLAIM-BENCH为评估LLMs的科学理解能力设定了新标准，并为构建更可靠的推理系统提供了路径。

Abstract: Large language models (LLMs) are increasingly being used for complex research
tasks such as literature review, idea generation, and scientific paper
analysis, yet their ability to truly understand and process the intricate
relationships within complex research papers, such as the logical links between
claims and supporting evidence remains largely unexplored. In this study, we
present CLAIM-BENCH, a comprehensive benchmark for evaluating LLMs'
capabilities in scientific claim-evidence extraction and validation, a task
that reflects deeper comprehension of scientific argumentation. We
systematically compare three approaches which are inspired by divide and
conquer approaches, across six diverse LLMs, highlighting model-specific
strengths and weaknesses in scientific comprehension. Through evaluation
involving over 300 claim-evidence pairs across multiple research domains, we
reveal significant limitations in LLMs' ability to process complex scientific
content. Our results demonstrate that closed-source models like GPT-4 and
Claude consistently outperform open-source counterparts in precision and recall
across claim-evidence identification tasks. Furthermore, strategically designed
three-pass and one-by-one prompting approaches significantly improve LLMs'
abilities to accurately link dispersed evidence with claims, although this
comes at increased computational cost. CLAIM-BENCH sets a new standard for
evaluating scientific comprehension in LLMs, offering both a diagnostic tool
and a path forward for building systems capable of deeper, more reliable
reasoning across full-length papers.

</details>


### [48] [Neighbors and relatives: How do speech embeddings reflect linguistic connections across the world?](https://arxiv.org/abs/2506.08564)
*Tuukka Törö,Antti Suni,Juraj Šimko*

Main category: cs.CL

TL;DR: 该研究利用机器学习模型XLS-R的嵌入分析106种世界语言的关系，发现嵌入距离与传统语言分类方法一致，为大规模语言分析提供了新途径。


<details>
  <summary>Details</summary>
Motivation: 研究动机是通过机器学习方法补充传统语言分类方法，探索语音嵌入在大规模语言关系分析中的潜力。

Method: 使用XLS-R模型的语音嵌入，结合线性判别分析（LDA），将语言聚类并与谱系、词汇和地理距离进行比较。

Result: 嵌入距离与传统语言分类方法高度一致，能够捕捉全局和局部语言模式。

Conclusion: 该方法为低资源语言研究提供了新视角，未来将扩展至更多语言并整合社会语言学因素。

Abstract: Investigating linguistic relationships on a global scale requires analyzing
diverse features such as syntax, phonology and prosody, which evolve at varying
rates influenced by internal diversification, language contact, and
sociolinguistic factors. Recent advances in machine learning (ML) offer
complementary alternatives to traditional historical and typological
approaches. Instead of relying on expert labor in analyzing specific linguistic
features, these new methods enable the exploration of linguistic variation
through embeddings derived directly from speech, opening new avenues for
large-scale, data-driven analyses.
  This study employs embeddings from the fine-tuned XLS-R self-supervised
language identification model voxlingua107-xls-r-300m-wav2vec, to analyze
relationships between 106 world languages based on speech recordings. Using
linear discriminant analysis (LDA), language embeddings are clustered and
compared with genealogical, lexical, and geographical distances. The results
demonstrate that embedding-based distances align closely with traditional
measures, effectively capturing both global and local typological patterns.
Challenges in visualizing relationships, particularly with hierarchical
clustering and network-based methods, highlight the dynamic nature of language
change.
  The findings show potential for scalable analyses of language variation based
on speech embeddings, providing new perspectives on relationships among
languages. By addressing methodological considerations such as corpus size and
latent space dimensionality, this approach opens avenues for studying
low-resource languages and bridging macro- and micro-level linguistic
variation. Future work aims to extend these methods to underrepresented
languages and integrate sociolinguistic variation for a more comprehensive
understanding of linguistic diversity.

</details>


### [49] [Automatic Generation of Inference Making Questions for Reading Comprehension Assessments](https://arxiv.org/abs/2506.08260)
*Wanjing Anya Ma,Michael Flor,Zuowei Wang*

Main category: cs.CL

TL;DR: 论文提出了一种阅读理解推理类型的分类法，并利用GPT-4o生成桥接推理题目，实验表明生成题目质量高但类型匹配度较低。


<details>
  <summary>Details</summary>
Motivation: 诊断性阅读理解问题能帮助教育者提供更有针对性的阅读指导，但人工生成题目成本高，因此探索自动生成题目的方法。

Method: 提出推理类型分类法，分析题目库分布；使用GPT-4o通过少量示例生成桥接推理题目，比较有无链式思考提示的效果。

Result: GPT-4o生成93.8%高质量题目，但仅42.6%准确匹配目标推理类型；人工与自动结合生成题目效果良好。

Conclusion: 自动生成题目结合人工判断是扩展高质量诊断性阅读理解评估的有效途径。

Abstract: Inference making is an essential but complex skill in reading comprehension
(RC). Some inferences require resolving references across sentences, and some
rely on using prior knowledge to fill in the detail that is not explicitly
written in the text. Diagnostic RC questions can help educators provide more
effective and targeted reading instruction and interventions for school-age
students. We introduce a taxonomy of inference types for RC and use it to
analyze the distribution of items within a diagnostic RC item bank. Next, we
present experiments using GPT-4o to generate bridging-inference RC items for
given reading passages via few-shot prompting, comparing conditions with and
without chain-of-thought prompts. Generated items were evaluated on three
aspects: overall item quality, appropriate inference type, and LLM reasoning,
achieving high inter-rater agreements above 0.90. Our results show that GPT-4o
produced 93.8% good-quality questions suitable for operational use in grade
3-12 contexts; however, only 42.6% of the generated questions accurately
matched the targeted inference type. We conclude that combining automatic item
generation with human judgment offers a promising path toward scalable,
high-quality diagnostic RC assessments.

</details>


### [50] [Multi-Teacher Language-Aware Knowledge Distillation for Multilingual Speech Emotion Recognition](https://arxiv.org/abs/2506.08717)
*Mehedi Hasan Bijoy,Dejan Porjazovski,Tamás Grósz,Mikko Kurimo*

Main category: cs.CL

TL;DR: 本文提出了一种语言感知的多教师知识蒸馏方法，用于构建多语言语音情感识别（SER）模型，性能优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 尽管单语言SER取得了进展，但扩展到多语言系统仍具挑战性。目标是训练一个能处理多语言SER的单一模型。

Method: 利用Wav2Vec2.0构建单语言教师模型，并通过语言感知的多教师知识蒸馏方法将知识传递给学生模型。

Result: 学生模型在英语和芬兰语数据集上表现优异，加权召回率为72.9（英语）和63.4（芬兰语），优于基线方法。

Conclusion: 该方法在识别悲伤和中性情绪上表现突出，但在愤怒和快乐情绪识别上仍有改进空间。

Abstract: Speech Emotion Recognition (SER) is crucial for improving human-computer
interaction. Despite strides in monolingual SER, extending them to build a
multilingual system remains challenging. Our goal is to train a single model
capable of multilingual SER by distilling knowledge from multiple teacher
models. To address this, we introduce a novel language-aware multi-teacher
knowledge distillation method to advance SER in English, Finnish, and French.
It leverages Wav2Vec2.0 as the foundation of monolingual teacher models and
then distills their knowledge into a single multilingual student model. The
student model demonstrates state-of-the-art performance, with a weighted recall
of 72.9 on the English dataset and an unweighted recall of 63.4 on the Finnish
dataset, surpassing fine-tuning and knowledge distillation baselines. Our
method excels in improving recall for sad and neutral emotions, although it
still faces challenges in recognizing anger and happiness.

</details>


### [51] [Institutional Books 1.0: A 242B token dataset from Harvard Library's collections, refined for accuracy and usability](https://arxiv.org/abs/2506.08300)
*Matteo Cargnelutti,Catherine Brobston,John Hess,Jack Cushman,Kristi Mukk,Aristana Scourtas,Kyle Courtney,Greg Leppert,Amanda Watson,Martha Whitehead,Jonathan Zittrain*

Main category: cs.CL

TL;DR: 本文介绍了Institutional Books 1.0数据集，一个基于哈佛图书馆公共领域书籍的大规模数据集，旨在支持高质量语言模型的训练。


<details>
  <summary>Details</summary>
Motivation: 由于高质量公开训练数据的稀缺性，本文旨在通过整理和发布哈佛图书馆的公共领域书籍数据集，为语言模型提供更可靠的数据支持。

Method: 从哈佛图书馆的Google Books项目中提取、分析和处理公共领域书籍，生成包含OCR文本和元数据的文档化数据集。

Result: 发布了983,004卷书籍的OCR文本和元数据，总计242B tokens，覆盖多种语言和历史文本。

Conclusion: 该数据集为语言模型提供了高质量的历史文本资源，支持更可持续的数据管理和使用。

Abstract: Large language models (LLMs) use data to learn about the world in order to
produce meaningful correlations and predictions. As such, the nature, scale,
quality, and diversity of the datasets used to train these models, or to
support their work at inference time, have a direct impact on their quality.
The rapid development and adoption of LLMs of varying quality has brought into
focus the scarcity of publicly available, high-quality training data and
revealed an urgent need to ground the stewardship of these datasets in
sustainable practices with clear provenance chains. To that end, this technical
report introduces Institutional Books 1.0, a large collection of public domain
books originally digitized through Harvard Library's participation in the
Google Books project, beginning in 2006. Working with Harvard Library, we
extracted, analyzed, and processed these volumes into an extensively-documented
dataset of historic texts. This analysis covers the entirety of Harvard
Library's collection scanned as part of that project, originally spanning
1,075,899 volumes written in over 250 different languages for a total of
approximately 250 billion tokens. As part of this initial release, the
OCR-extracted text (original and post-processed) as well as the metadata
(bibliographic, source, and generated) of the 983,004 volumes, or 242B tokens,
identified as being in the public domain have been made available. This report
describes this project's goals and methods as well as the results of the
analyses we performed, all in service of making this historical collection more
accessible and easier for humans and machines alike to filter, read and use.

</details>


### [52] [Wait, We Don't Need to "Wait"! Removing Thinking Tokens Improves Reasoning Efficiency](https://arxiv.org/abs/2506.08343)
*Chenlong Wang,Yuanning Feng,Dongping Chen,Zhaoyang Chu,Ranjay Krishna,Tianyi Zhou*

Main category: cs.CL

TL;DR: NoWait通过禁用显式自我反思标记（如'Wait'和'Hmm'），显著减少了推理过程中的冗余输出，同时保持模型性能。


<details>
  <summary>Details</summary>
Motivation: 大型推理模型在复杂推理中常产生冗余输出，影响效率。研究探讨显式自我反思是否必要。

Method: 提出NoWait方法，在推理过程中抑制显式自我反思标记。

Result: 在10个基准测试中，NoWait将推理轨迹长度减少27%-51%，且不影响模型效用。

Conclusion: NoWait为高效且保持性能的多模态推理提供了即插即用的解决方案。

Abstract: Recent advances in large reasoning models have enabled complex, step-by-step
reasoning but often introduce significant overthinking, resulting in verbose
and redundant outputs that hinder efficiency. In this study, we examine whether
explicit self-reflection, signaled by tokens such as "Wait" and "Hmm", is
necessary for advanced reasoning. We propose NoWait, a simple yet effective
approach that disables explicit self-reflection by suppressing these tokens
during inference. Extensive experiments on ten benchmarks across textual,
visual, and video reasoning tasks show that NoWait reduces chain-of-thought
trajectory length by up to 27%-51% in five R1-style model series, without
compromising model utility. NoWait thus offers a plug-and-play solution for
efficient and utility-preserving multimodal reasoning.

</details>


### [53] [Evaluating LLMs Across Multi-Cognitive Levels: From Medical Knowledge Mastery to Scenario-Based Problem Solving](https://arxiv.org/abs/2506.08349)
*Yuxuan Zhou,Xien Liu,Chenwei Yan,Chen Ning,Xiao Zhang,Boxun Li,Xiangling Fu,Shijin Wang,Guoping Hu,Yu Wang,Ji Wu*

Main category: cs.CL

TL;DR: 该研究提出了一个基于布鲁姆分类法的多认知层次评估框架，用于评估大语言模型在医学领域的表现，发现模型性能随认知复杂度增加而下降，模型规模对高层次认知任务更为关键。


<details>
  <summary>Details</summary>
Motivation: 探索大语言模型在不同认知层次上的医学能力表现，填补现有研究的空白。

Method: 提出多认知层次评估框架，整合现有医学数据集，设计针对三个认知层次的任务，并评估六类主流大语言模型。

Result: 模型性能随认知复杂度增加显著下降，模型规模对高层次认知任务的影响更大。

Conclusion: 研究强调需提升大语言模型在高认知层次的医学能力，为开发适合实际医学应用的模型提供参考。

Abstract: Large language models (LLMs) have demonstrated remarkable performance on
various medical benchmarks, but their capabilities across different cognitive
levels remain underexplored. Inspired by Bloom's Taxonomy, we propose a
multi-cognitive-level evaluation framework for assessing LLMs in the medical
domain in this study. The framework integrates existing medical datasets and
introduces tasks targeting three cognitive levels: preliminary knowledge grasp,
comprehensive knowledge application, and scenario-based problem solving. Using
this framework, we systematically evaluate state-of-the-art general and medical
LLMs from six prominent families: Llama, Qwen, Gemma, Phi, GPT, and DeepSeek.
Our findings reveal a significant performance decline as cognitive complexity
increases across evaluated models, with model size playing a more critical role
in performance at higher cognitive levels. Our study highlights the need to
enhance LLMs' medical capabilities at higher cognitive levels and provides
insights for developing LLMs suited to real-world medical applications.

</details>


### [54] [Text Embeddings Should Capture Implicit Semantics, Not Just Surface Meaning](https://arxiv.org/abs/2506.08354)
*Yiqun Sun,Qiang Huang,Anthony K. H. Tung,Jun Yu*

Main category: cs.CL

TL;DR: 本文主张文本嵌入研究应超越表层语义，以隐含语义为核心建模目标。


<details>
  <summary>Details</summary>
Motivation: 当前文本嵌入模型主要关注表层语义，而忽略了隐含语义（如语用、说话者意图和社会文化背景），导致其在需要深层语义理解的任务中表现不佳。

Method: 提出通过更丰富且语言学基础的数据训练、设计评估深层语义理解的基准测试，并将隐含语义明确作为建模目标。

Result: 初步研究表明，即使是先进模型在隐含语义任务上的表现也仅略优于简单基线。

Conclusion: 呼吁研究范式转变，以更好地应对现实语言的复杂性。

Abstract: This position paper argues that the text embedding research community should
move beyond surface meaning and embrace implicit semantics as a central
modeling goal. Text embedding models have become foundational in modern NLP,
powering a wide range of applications and drawing increasing research
attention. Yet, much of this progress remains narrowly focused on surface-level
semantics. In contrast, linguistic theory emphasizes that meaning is often
implicit, shaped by pragmatics, speaker intent, and sociocultural context.
Current embedding models are typically trained on data that lacks such depth
and evaluated on benchmarks that reward the capture of surface meaning. As a
result, they struggle with tasks requiring interpretive reasoning, speaker
stance, or social meaning. Our pilot study highlights this gap, showing that
even state-of-the-art models perform only marginally better than simplistic
baselines on implicit semantics tasks. To address this, we call for a paradigm
shift: embedding research should prioritize more diverse and linguistically
grounded training data, design benchmarks that evaluate deeper semantic
understanding, and explicitly frame implicit meaning as a core modeling
objective, better aligning embeddings with real-world language complexity.

</details>


### [55] [Advancing STT for Low-Resource Real-World Speech](https://arxiv.org/abs/2506.08836)
*Flavio D'Intino,Hans-Peter Hutter*

Main category: cs.CL

TL;DR: 论文介绍了SRB-300数据集，用于提升瑞士德语（低资源语言）的语音识别性能，通过微调Whisper模型显著降低了词错误率并提高了翻译质量。


<details>
  <summary>Details</summary>
Motivation: 瑞士德语缺乏标准化书写形式，现有数据集在自然对话场景下表现不佳，需要更真实的数据集和模型改进。

Method: 使用SRB-300数据集（300小时真实广播录音）微调多个Whisper模型。

Result: 微调后模型显著提升性能，最佳模型WER为17.1%，BLEU为74.8%。

Conclusion: SRB-300数据集和微调方法为低资源语言的语音识别提供了有效解决方案。

Abstract: Swiss German is a low-resource language represented by diverse dialects that
differ significantly from Standard German and from each other, lacking a
standardized written form. As a result, transcribing Swiss German involves
translating into Standard German. Existing datasets have been collected in
controlled environments, yielding effective speech-to-text (STT) models, but
these models struggle with spontaneous conversational speech.
  This paper, therefore, introduces the new SRB-300 dataset, a 300-hour
annotated speech corpus featuring real-world long-audio recordings from 39
Swiss German radio and TV stations. It captures spontaneous speech across all
major Swiss dialects recorded in various realistic environments and overcomes
the limitation of prior sentence-level corpora.
  We fine-tuned multiple OpenAI Whisper models on the SRB-300 dataset,
achieving notable enhancements over previous zero-shot performance metrics.
Improvements in word error rate (WER) ranged from 19% to 33%, while BLEU scores
increased between 8% and 40%. The best fine-tuned model, large-v3, achieved a
WER of 17.1% and a BLEU score of 74.8. This advancement is crucial for
developing effective and robust STT systems for Swiss German and other
low-resource languages in real-world contexts.

</details>


### [56] [DEAL: Disentangling Transformer Head Activations for LLM Steering](https://arxiv.org/abs/2506.08359)
*Li-Ming Zhan,Bo Liu,Zexin Lu,Chengqiang Xie,Jiannong Cao,Xiao-Ming Wu*

Main category: cs.CL

TL;DR: 提出了一种基于因果归因的框架，用于识别Transformer中与行为相关的注意力头，通过VQ-AE量化注意力激活，评估行为相关性，并在实验中验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 当前方法依赖表面线索或启发式规则，导致模块选择不理想或产生意外结果。

Method: 训练VQ-AE量化注意力头的激活，将潜在空间划分为行为相关和不相关子空间，通过二分类指标评估行为相关性。

Result: 在七个LLM和五个行为数据集上验证，方法能更准确干预推理，尤其在真实性任务中表现优异。

Conclusion: 提出的方法能有效识别行为相关注意力头，并具有跨领域零样本泛化能力。

Abstract: Inference-time steering aims to alter the response characteristics of large
language models (LLMs) without modifying their underlying parameters. A
critical step in this process is the identification of internal modules within
LLMs that are associated with the target behavior. However, current approaches
to module selection often depend on superficial cues or ad-hoc heuristics,
which can result in suboptimal or unintended outcomes. In this work, we propose
a principled causal-attribution framework for identifying behavior-relevant
attention heads in transformers. For each head, we train a vector-quantized
autoencoder (VQ-AE) on its attention activations, partitioning the latent space
into behavior-relevant and behavior-irrelevant subspaces, each quantized with a
shared learnable codebook. We assess the behavioral relevance of each head by
quantifying the separability of VQ-AE encodings for behavior-aligned versus
behavior-violating responses using a binary classification metric. This yields
a behavioral relevance score that reflects each head discriminative capacity
with respect to the target behavior, guiding both selection and importance
weighting. Experiments on seven LLMs from two model families and five
behavioral steering datasets demonstrate that our method enables more accurate
inference-time interventions, achieving superior performance on the
truthfulness-steering task. Furthermore, the heads selected by our approach
exhibit strong zero-shot generalization in cross-domain truthfulness-steering
scenarios.

</details>


### [57] [CC-RAG: Structured Multi-Hop Reasoning via Theme-Based Causal Graphs](https://arxiv.org/abs/2506.08364)
*Jash Rajesh Parekh,Pengcheng Jiang,Jiawei Han*

Main category: cs.CL

TL;DR: 论文提出了一种名为CC-RAG的新方法，通过结合零样本三元组提取和主题感知图链技术，改进了RAG管道，以更好地建模因果关系。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）在理解因果关系方面存在挑战，尤其是在需要深层推理的专业领域。标准RAG管道缺乏结构化建模能力。

Method: CC-RAG通过构建有向无环图（DAG）进行多跳推理，结合零样本三元组提取和主题感知图链。

Result: 在比特币价格波动和高雪氏病两个领域的实验中，CC-RAG在链相似性、信息密度和词汇多样性上优于标准RAG和零样本LLMs。

Conclusion: 显式建模因果结构使LLMs能够生成更准确和可解释的响应，尤其是在标准检索方法失效的专业领域。

Abstract: Understanding cause and effect relationships remains a formidable challenge
for Large Language Models (LLMs), particularly in specialized domains where
reasoning requires more than surface-level correlations. Retrieval-Augmented
Generation (RAG) improves factual accuracy, but standard RAG pipelines treat
evidence as flat context, lacking the structure required to model true causal
dependencies. We introduce Causal-Chain RAG (CC-RAG), a novel approach that
integrates zero-shot triple extraction and theme-aware graph chaining into the
RAG pipeline, enabling structured multi-hop inference. Given a domain specific
corpus, CC-RAG constructs a Directed Acyclic Graph (DAG) of <cause, relation,
effect> triples and uses forward/backward chaining to guide structured answer
generation. Experiments on two real-world domains: Bitcoin price fluctuations
and Gaucher disease, show that CC-RAG outperforms standard RAG and zero-shot
LLMs in chain similarity, information density, and lexical diversity. Both
LLM-as-a-Judge and human evaluations consistently favor CC-RAG. Our results
demonstrate that explicitly modeling causal structure enables LLMs to generate
more accurate and interpretable responses, especially in specialized domains
where flat retrieval fails.

</details>


### [58] [Mitigating Posterior Salience Attenuation in Long-Context LLMs with Positional Contrastive Decoding](https://arxiv.org/abs/2506.08371)
*Zikai Xiao,Ziyang Wang,Wen Ma,Yan Zhang,Wei Shen,Yan Wang,Luqi Gong,Zuozhu Liu*

Main category: cs.CL

TL;DR: 论文提出了一种无需训练的位置对比解码方法（PCD），通过对比长文本和局部注意力的logits，有效缓解了长上下文性能退化问题。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）在长上下文窗口中存在性能退化问题，现有解决方案训练成本高，且统计行为和成本效益方法未充分探索。

Method: 从解码角度识别后验显著性衰减（PSA）现象，提出PCD方法，对比长文本和局部注意力的logits，利用大规模短到长训练的优势。

Result: 实验证明PCD能有效缓解注意力分数退化，并在长上下文基准测试中达到最先进性能。

Conclusion: PCD是一种无需训练的高效方法，显著提升了LLMs在长上下文中的表现。

Abstract: While Large Language Models (LLMs) support long contexts, they struggle with
performance degradation within the context window. Current solutions incur
prohibitive training costs, leaving statistical behaviors and cost-effective
approaches underexplored. From the decoding perspective, we identify the
Posterior Salience Attenuation (PSA) phenomenon, where the salience ratio
correlates with long-text performance degradation. Notably, despite the
attenuation, gold tokens still occupy high-ranking positions in the decoding
space. Motivated by it, we propose the training-free Positional Contrastive
Decoding (PCD) that contrasts the logits derived from long-aware attention with
those from designed local-aware attention, enabling the model to focus on the
gains introduced by large-scale short-to-long training. Through the analysis of
long-term decay simulation, we demonstrate that PCD effectively alleviates
attention score degradation. Experimental results show that PCD achieves
state-of-the-art performance on long-context benchmarks.

</details>


### [59] [Draft-based Approximate Inference for LLMs](https://arxiv.org/abs/2506.08373)
*Kevin Galim,Ethan Ewer,Wonjun Kang,Minjae Lee,Hyung Il Koo,Kangwook Lee*

Main category: cs.CL

TL;DR: 提出了一种利用小型草稿模型优化长上下文LLM推理的新框架，包括SpecKV和SpecPC两种方法，显著提升了准确性和效率。


<details>
  <summary>Details</summary>
Motivation: 由于Transformer的二次计算和线性内存复杂度，优化长上下文LLM推理变得重要，现有方法依赖粗略预测，准确性不足。

Method: 提出SpecKV（利用草稿输出评估KV对重要性）和SpecPC（利用草稿模型注意力激活识别不重要提示词）两种方法。

Result: 实验表明，新方法在长上下文基准测试中准确性优于现有基线，同时保持内存、延迟和吞吐量的改进。

Conclusion: 草稿模型首次用于近似LLM推理加速，扩展了其应用范围，实验验证了其有效性。

Abstract: Optimizing inference for long-context Large Language Models (LLMs) is
increasingly important due to the quadratic compute and linear memory
complexity of Transformers. Existing approximation methods, such as key-value
(KV) cache dropping, sparse attention, and prompt compression, typically rely
on rough predictions of token or KV pair importance. We propose a novel
framework for approximate LLM inference that leverages small draft models to
more accurately predict the importance of tokens and KV pairs. Specifically, we
introduce two instantiations of our proposed framework: (i) SpecKV, which
leverages a draft output to accurately assess the importance of each KV pair
for more effective KV cache dropping, and (ii) SpecPC, which uses the draft
model's attention activations to identify and discard unimportant prompt
tokens. To the best of our knowledge, this is the first work to use draft
models for approximate LLM inference acceleration, extending their utility
beyond traditional lossless speculative decoding. We motivate our methods with
theoretical and empirical analyses, and show a strong correlation between the
attention patterns of draft and target models. Extensive experiments on
long-context benchmarks show that our methods consistently achieve higher
accuracy than existing baselines, while preserving the same improvements in
memory usage, latency, and throughput. Our code is available at
https://github.com/furiosa-ai/draft-based-approx-llm.

</details>


### [60] [EIFBENCH: Extremely Complex Instruction Following Benchmark for Large Language Models](https://arxiv.org/abs/2506.08375)
*Tao Zou,Xinghua Zhang,Haiyang Yu,Minzheng Wang,Fei Huang,Yongbin Li*

Main category: cs.CL

TL;DR: EIFBENCH是一个针对大语言模型（LLMs）的复杂指令跟随基准测试，旨在通过多任务场景和多样化约束更真实地评估LLMs性能。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试局限于单任务环境，无法反映现实复杂性，因此需要更全面的评估工具。

Method: 提出EIFBENCH基准测试，包含多任务场景和多样化约束，并开发SegPO算法提升LLMs的多任务处理能力。

Result: 评估显示现有LLMs在复杂指令下表现差异显著，需进一步优化。

Conclusion: EIFBENCH和SegPO为LLMs在复杂场景中的应用提供了重要工具和优化方向。

Abstract: With the development and widespread application of large language models
(LLMs), the new paradigm of "Model as Product" is rapidly evolving, and demands
higher capabilities to address complex user needs, often requiring precise
workflow execution which involves the accurate understanding of multiple tasks.
However, existing benchmarks focusing on single-task environments with limited
constraints lack the complexity required to fully reflect real-world scenarios.
To bridge this gap, we present the Extremely Complex Instruction Following
Benchmark (EIFBENCH), meticulously crafted to facilitate a more realistic and
robust evaluation of LLMs. EIFBENCH not only includes multi-task scenarios that
enable comprehensive assessment across diverse task types concurrently, but
also integrates a variety of constraints, replicating complex operational
environments. Furthermore, we propose the Segment Policy Optimization (SegPO)
algorithm to enhance the LLM's ability to accurately fulfill multi-task
workflow. Evaluations on EIFBENCH have unveiled considerable performance
discrepancies in existing LLMs when challenged with these extremely complex
instructions. This finding underscores the necessity for ongoing optimization
to navigate the intricate challenges posed by LLM applications.

</details>


### [61] [TACTIC: Translation Agents with Cognitive-Theoretic Interactive Collaboration](https://arxiv.org/abs/2506.08403)
*Weiya Li,Junjie Chen,Bei Li,Boyang Liu,Zichen Wen,Nuanqiao Shan,Xiaoqian Liu,Anping Liu,Huajie Liu,Youyan Wang,Wujiuge Yin,Hu Song,Bing Huang,Zhiyuan Xia,Jialiang Chen,Linfeng Zhang*

Main category: cs.CL

TL;DR: 论文提出了一种名为TACTIC的认知理论驱动的多智能体翻译框架，通过模拟人类翻译的认知过程，显著提升了大型语言模型（LLMs）的翻译质量。


<details>
  <summary>Details</summary>
Motivation: 尽管多智能体系统在翻译任务中展现出潜力，但现有框架忽视了认知翻译研究的关键见解，如人类翻译的认知策略。

Method: TACTIC框架包含六个功能各异的智能体，分别负责起草、优化、评估、评分、上下文推理和外部知识收集，模拟人类翻译的认知流程。

Result: 在FLORES-200和WMT24基准测试中，TACTIC表现优于GPT-4.1和DeepSeek-R1，平均提升0.6 XCOMET和1.18 COMETKIWI-23。

Conclusion: TACTIC通过理论驱动的多智能体协作，有效释放了LLMs的翻译潜力，实现了最先进的性能。

Abstract: Machine translation has long been a central task in natural language
processing. With the rapid advancement of large language models (LLMs), there
has been remarkable progress in translation quality. However, fully realizing
the translation potential of LLMs remains an open challenge. Recent studies
have explored multi-agent systems to decompose complex translation tasks into
collaborative subtasks, showing initial promise in enhancing translation
quality through agent cooperation and specialization. Nevertheless, existing
multi-agent translation frameworks largely neglect foundational insights from
cognitive translation studies. These insights emphasize how human translators
employ different cognitive strategies, such as balancing literal and free
translation, refining expressions based on context, and iteratively evaluating
outputs. To address this limitation, we propose a cognitively informed
multi-agent framework called TACTIC, which stands for T ranslation A gents with
Cognitive- T heoretic Interactive Collaboration. The framework comprises six
functionally distinct agents that mirror key cognitive processes observed in
human translation behavior. These include agents for drafting, refinement,
evaluation, scoring, context reasoning, and external knowledge gathering. By
simulating an interactive and theory-grounded translation workflow, TACTIC
effectively leverages the full capacity of LLMs for high-quality translation.
Experimental results on diverse language pairs from the FLORES-200 and WMT24
benchmarks show that our method consistently achieves state-of-the-art
performance. Using DeepSeek-V3 as the base model, TACTIC surpasses GPT-4.1 by
an average of +0.6 XCOMET and +1.18 COMETKIWI-23. Compared to DeepSeek-R1, it
further improves by +0.84 XCOMET and +2.99 COMETKIWI-23. Code is available at
https://github.com/weiyali126/TACTIC.

</details>


### [62] [Large Language Models Have Intrinsic Meta-Cognition, but Need a Good Lens](https://arxiv.org/abs/2506.08410)
*Ziyang Ma,Qingyue Yuan,Zhenglin Wang,Deyu Zhou*

Main category: cs.CL

TL;DR: 本文提出AutoMeco框架和MIRA策略，用于评估和改进LLM的元认知能力。


<details>
  <summary>Details</summary>
Motivation: 现有研究缺乏对LLM元认知能力的深入分析，尤其是步骤级错误的自省能力。

Method: 提出AutoMeco框架评估现有元认知指标，并设计MIRA策略优化这些指标。

Result: 实验表明AutoMeco合理，MIRA能更准确评估LLM的元认知能力。

Conclusion: AutoMeco和MIRA为LLM元认知能力的评估和改进提供了有效工具。

Abstract: Previous research has primarily focused on the cognitive error detection
capabilities of Large Language Models (LLMs), often prompting them to analyze
mistakes in reasoning chains. However, few studies have examined the
meta-cognitive abilities of LLMs (e.g., their self-awareness of step errors),
which are crucial for their reliability. While studies on LLM self-evaluation
present some measures, such as perplexity, which can reflect the answer
correctness and be viewed as the lens of meta-cognition, they lack step-level
analysis and adaptation. This paper studies the evaluation of LLM
meta-cognition using the current lenses and how to improve these lenses.
Specifically, we propose AutoMeco, an Automated Meta-cognition Evaluation
framework for benchmarking the existing lenses. Furthermore, a training-free
Markovian Intrinsic Reward Adjustment strategy, MIRA, is proposed to boost
current meta-cognition lenses. Experimental results on three mathematical
reasoning datasets and three LLMs show the reasonableness of AutoMeco by
comparing it with Best-of-N verification. Moreover, the meta-cognition ability
of LLMs can be better evaluated using MIRA.

</details>


### [63] [Know-MRI: A Knowledge Mechanisms Revealer&Interpreter for Large Language Models](https://arxiv.org/abs/2506.08427)
*Jiaxiang Liu,Boxuan Xing,Chenhao Yuan,Chenxiang Zhang,Di Wu,Xiusheng Huang,Haida Yu,Chuhan Lang,Pengfei Cao,Jun Zhao,Kang Liu*

Main category: cs.CL

TL;DR: Know-MRI是一个开源工具，旨在系统分析大语言模型（LLMs）的知识机制，通过可扩展的核心模块自动匹配输入数据与解释方法，并整合输出结果。


<details>
  <summary>Details</summary>
Motivation: 当前解释方法在输入数据格式和输出结果上存在差异，且工具仅支持特定输入任务，限制了实际应用。

Method: 开发了一个可扩展的核心模块，自动匹配输入数据与解释方法，并整合解释输出。

Result: Know-MRI支持用户根据输入自由选择解释方法，便于从多角度全面诊断模型内部知识机制。

Conclusion: Know-MRI为LLMs知识机制的解释提供了灵活且系统的解决方案。

Abstract: As large language models (LLMs) continue to advance, there is a growing
urgency to enhance the interpretability of their internal knowledge mechanisms.
Consequently, many interpretation methods have emerged, aiming to unravel the
knowledge mechanisms of LLMs from various perspectives. However, current
interpretation methods differ in input data formats and interpreting outputs.
The tools integrating these methods are only capable of supporting tasks with
specific inputs, significantly constraining their practical applications. To
address these challenges, we present an open-source Knowledge Mechanisms
Revealer&Interpreter (Know-MRI) designed to analyze the knowledge mechanisms
within LLMs systematically. Specifically, we have developed an extensible core
module that can automatically match different input data with interpretation
methods and consolidate the interpreting outputs. It enables users to freely
choose appropriate interpretation methods based on the inputs, making it easier
to comprehensively diagnose the model's internal knowledge mechanisms from
multiple perspectives. Our code is available at
https://github.com/nlpkeg/Know-MRI. We also provide a demonstration video on
https://youtu.be/NVWZABJ43Bs.

</details>


### [64] [Low-resource domain adaptation while minimizing energy and hardware resource consumption](https://arxiv.org/abs/2506.08433)
*Hernán Maina,Nicolás Wolovick,Luciana Benotti*

Main category: cs.CL

TL;DR: 研究探讨了不同数值精度和数据并行化策略对LLM训练速度和模型准确性的影响，以促进低资源环境下的领域适应。


<details>
  <summary>Details</summary>
Motivation: LLM训练成本高且存在文化偏见，领域适应虽有效但计算成本高，尤其在资源有限的环境中。

Method: 评估不同数值精度和数据并行化策略对训练速度和模型准确性的影响。

Result: 研究结果为关注能效、可访问性或硬件限制的环境提供了实用参考。

Conclusion: 通过优化训练策略，可以在低资源环境中更高效地实现领域适应。

Abstract: Training Large Language Models (LLMs) is costly in terms of energy, hardware,
and annotated data, often resulting in a positionality rooted in predominant
cultures and values (Santy et al., 2023). Domain adaptation has emerged as a
promising strategy to better align models with diverse cultural and value
contexts (Hershcovich et al., 2022), but its computational cost remains a
significant barrier, particularly for research groups lacking access to
large-scale infrastructure. In this paper, we evaluate how the use of different
numerical precisions and data parallelization strategies impacts both training
speed (as a proxy to energy and hardware consumption) and model accuracy, with
the goal of facilitating domain adaptation in low-resource environments. Our
findings are relevant to any setting where energy efficiency, accessibility, or
limited hardware availability are key concerns.

</details>


### [65] [Olica: Efficient Structured Pruning of Large Language Models without Retraining](https://arxiv.org/abs/2506.08436)
*Jiujun He,Huazhen Lin*

Main category: cs.CL

TL;DR: Olica是一种无需重新训练的大语言模型剪枝框架，通过正交分解和线性校准压缩模型，保持性能。


<details>
  <summary>Details</summary>
Motivation: 现有剪枝方法需大量计算和数据资源重新训练，成本高昂。

Method: 利用PCA处理多头注意力层的矩阵乘积，设计快速分解方法；通过SVD解决最小二乘问题，线性校准FFN层残差。

Result: Olica在数据使用、GPU内存和运行时间上高效，性能优于基准测试。

Conclusion: Olica无需重新训练即可高效剪枝LLMs，保持准确性。

Abstract: Most existing structured pruning methods for Large Language Models (LLMs)
require substantial computational and data resources for retraining to
reestablish the corrupted correlations, making them prohibitively expensive. To
address this, we propose a pruning framework for LLMs called Orthogonal
decomposition and Linear Calibration (Olica), which eliminates the need for
retraining. A key observation is that the multi-head attention (MHA) layer
depends on two types of matrix products. By treating these matrix products as
unified entities and applying principal component analysis (PCA), we extract
the most important information to compress LLMs without sacrificing accuracy or
disrupting their original structure. Consequently, retraining becomes
unnecessary. A fast decomposition method is devised, reducing the complexity of
PCA by a factor of the square of the number of attention heads. Additionally,
to mitigate error accumulation problem caused by pruning the feed-forward
network (FFN) layer, we introduce a linear calibration method to reconstruct
the residual errors of pruned layers using low-rank matrices. By leveraging
singular value decomposition (SVD) on the solution of the least-squares
problem, these matrices are obtained without requiring retraining. Extensive
experiments show that the proposed Olica is efficient in terms of data usage,
GPU memory, and running time, while delivering superior performance across
multiple benchmarks.

</details>


### [66] [Detecting Harmful Memes with Decoupled Understanding and Guided CoT Reasoning](https://arxiv.org/abs/2506.08477)
*Fengjun Pan,Anh Tuan Luu,Xiaobao Wu*

Main category: cs.CL

TL;DR: U-CoT+是一个用于有害模因检测的新框架，通过将视觉模因转换为文本描述，并结合人类指导的零样本CoT提示，实现高效、灵活和可解释的检测。


<details>
  <summary>Details</summary>
Motivation: 当前有害模因检测方法在资源效率、灵活性和可解释性方面存在不足，限制了其在实际内容审核系统中的部署。

Method: 开发了一个高保真的模因到文本转换管道，将视觉模因转换为文本描述，并结合人类指导的零样本CoT提示进行分类。

Result: 在七个基准数据集上的实验验证了该框架的有效性，展示了其在小规模LLMs上实现可解释和低资源有害模因检测的潜力。

Conclusion: U-CoT+框架为有害模因检测提供了一种高效、灵活且可解释的解决方案，适用于跨平台、跨区域和随时间变化的检测需求。

Abstract: Detecting harmful memes is essential for maintaining the integrity of online
environments. However, current approaches often struggle with resource
efficiency, flexibility, or explainability, limiting their practical deployment
in content moderation systems. To address these challenges, we introduce
U-CoT+, a novel framework for harmful meme detection. Instead of relying solely
on prompting or fine-tuning multimodal models, we first develop a high-fidelity
meme-to-text pipeline that converts visual memes into detail-preserving textual
descriptions. This design decouples meme interpretation from meme
classification, thus avoiding immediate reasoning over complex raw visual
content and enabling resource-efficient harmful meme detection with general
large language models (LLMs). Building on these textual descriptions, we
further incorporate targeted, interpretable human-crafted guidelines to guide
models' reasoning under zero-shot CoT prompting. As such, this framework allows
for easy adaptation to different harmfulness detection criteria across
platforms, regions, and over time, offering high flexibility and
explainability. Extensive experiments on seven benchmark datasets validate the
effectiveness of our framework, highlighting its potential for explainable and
low-resource harmful meme detection using small-scale LLMs. Codes and data are
available at: https://anonymous.4open.science/r/HMC-AF2B/README.md.

</details>


### [67] [Efficient Context Selection for Long-Context QA: No Tuning, No Iteration, Just Adaptive-$k$](https://arxiv.org/abs/2506.08479)
*Chihiro Taguchi,Seiji Maekawa,Nikita Bhutani*

Main category: cs.CL

TL;DR: 论文提出了一种自适应检索方法Adaptive-$k$，通过动态调整检索段落数量，解决了固定检索大小在开放域问答中的效率问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法如Self-RAG和Self-Route在事实性问答中表现良好，但在聚合问答中效果不佳，因为最优上下文大小未知且可变。

Method: Adaptive-$k$基于查询与候选段落相似度分数的分布，单次选择段落数量，无需模型微调或额外推理。

Result: 在事实性和聚合问答基准测试中，Adaptive-$k$性能优于固定检索方法，同时减少90%的token使用，仍保留70%的相关段落。

Conclusion: 动态调整上下文大小能显著提升问答系统的效率和准确性，适用于多种语言模型和嵌入模型。

Abstract: Retrieval-augmented generation (RAG) and long-context language models (LCLMs)
both address context limitations of LLMs in open-domain question answering
(QA). However, optimal external context to retrieve remains an open problem:
fixing the retrieval size risks either wasting tokens or omitting key evidence.
Existing adaptive methods like Self-RAG and Self-Route rely on iterative LLM
prompting and perform well on factoid QA, but struggle with aggregation QA,
where the optimal context size is both unknown and variable. We present
Adaptive-$k$ retrieval, a simple and effective single-pass method that
adaptively selects the number of passages based on the distribution of the
similarity scores between the query and the candidate passages. It does not
require model fine-tuning, extra LLM inferences or changes to existing
retriever-reader pipelines. On both factoid and aggregation QA benchmarks,
Adaptive-$k$ matches or outperforms fixed-$k$ baselines while using up to 10x
fewer tokens than full-context input, yet still retrieves 70% of relevant
passages. It improves accuracy across five LCLMs and two embedding models,
highlighting that dynamically adjusting context size leads to more efficient
and accurate QA.

</details>


### [68] [Re-Thinking the Automatic Evaluation of Image-Text Alignment in Text-to-Image Models](https://arxiv.org/abs/2506.08480)
*Huixuan Zhang,Xiaojun Wan*

Main category: cs.CL

TL;DR: 本文指出现有文本到图像生成评估框架的不足，提出改进建议。


<details>
  <summary>Details</summary>
Motivation: 现有评估主要依赖人类判断，忽略了评估框架的其他关键特性。

Method: 通过实证分析主流评估框架的局限性。

Result: 发现现有框架未能满足可靠性评估的关键要求。

Conclusion: 提出了改进图像-文本对齐评估的建议。

Abstract: Text-to-image models often struggle to generate images that precisely match
textual prompts. Prior research has extensively studied the evaluation of
image-text alignment in text-to-image generation. However, existing evaluations
primarily focus on agreement with human assessments, neglecting other critical
properties of a trustworthy evaluation framework. In this work, we first
identify two key aspects that a reliable evaluation should address. We then
empirically demonstrate that current mainstream evaluation frameworks fail to
fully satisfy these properties across a diverse range of metrics and models.
Finally, we propose recommendations for improving image-text alignment
evaluation.

</details>


### [69] [Fairness is Not Silence: Unmasking Vacuous Neutrality in Small Language Models](https://arxiv.org/abs/2506.08487)
*Sumanth Manduru,Carlotta Domeniconi*

Main category: cs.CL

TL;DR: 该论文首次对中等规模的小型语言模型（SLMs）进行了大规模伦理风险审计，揭示了模型在公平性和能力上的权衡，并提供了实际部署建议。


<details>
  <summary>Details</summary>
Motivation: 随着小型语言模型（SLMs）在资源受限环境中的快速应用，其伦理风险尚未被充分理解。本文旨在填补这一空白，评估中等规模SLMs的公平性和实用性。

Method: 研究对9个开源SLMs（包括Qwen 2.5、LLaMA 3.2、Gemma 3和Phi系列）进行了零样本提示下的BBQ基准测试，分析其在模糊和明确上下文中的表现。

Result: 研究发现：1）Phi模型在高效和公平性上表现优异；2）Qwen 2.5模型的公平性可能源于随机猜测而非伦理对齐；3）量化技术对性能与偏见的权衡有复杂影响。

Conclusion: 研究为资源受限环境中SLMs的负责任部署提供了实用指导，强调了公平性和效率的平衡。

Abstract: The rapid adoption of Small Language Models (SLMs) for on-device and
resource-constrained deployments has outpaced our understanding of their
ethical risks. To the best of our knowledge, we present the first large-scale
audit of instruction-tuned SLMs spanning 0.5 to 5 billion parameters-an
overlooked "middle tier" between BERT-class encoders and flagship LLMs. Our
evaluation includes nine open-source models from the Qwen 2.5, LLaMA 3.2, Gemma
3, and Phi families. Using the BBQ benchmark under zero-shot prompting, we
analyze both utility and fairness across ambiguous and disambiguated contexts.
This evaluation reveals three key insights. First, competence and fairness need
not be antagonistic: Phi models achieve F1 scores exceeding 90 percent while
exhibiting minimal bias, showing that efficient and ethical NLP is attainable.
Second, social bias varies significantly by architecture: Qwen 2.5 models may
appear fair, but this often reflects vacuous neutrality, random guessing, or
evasive behavior rather than genuine ethical alignment. In contrast, LLaMA 3.2
models exhibit stronger stereotypical bias, suggesting overconfidence rather
than neutrality. Third, compression introduces nuanced trade-offs: 4-bit AWQ
quantization improves F1 scores in ambiguous settings for LLaMA 3.2-3B but
increases disability-related bias in Phi-4-Mini by over 7 percentage points.
These insights provide practical guidance for the responsible deployment of
SLMs in applications demanding fairness and efficiency, particularly benefiting
small enterprises and resource-constrained environments.

</details>


### [70] [EtiCor++: Towards Understanding Etiquettical Bias in LLMs](https://arxiv.org/abs/2506.08488)
*Ashutosh Dwivedi,Siddhant Shivdutt Singh,Ashutosh Modi*

Main category: cs.CL

TL;DR: 论文介绍了EtiCor++语料库，用于评估LLMs对全球礼仪的理解和偏见，并提出了相关任务和指标。


<details>
  <summary>Details</summary>
Motivation: 研究LLMs的文化敏感性，尤其是对区域礼仪的理解和偏见，目前缺乏相关资源。

Method: 引入EtiCor++语料库，设计评估任务和偏见度量指标，对LLMs进行广泛实验。

Result: 实验显示LLMs对某些地区存在固有偏见。

Conclusion: EtiCor++为评估LLMs的礼仪知识和偏见提供了重要资源。

Abstract: In recent years, researchers have started analyzing the cultural sensitivity
of LLMs. In this respect, Etiquettes have been an active area of research.
Etiquettes are region-specific and are an essential part of the culture of a
region; hence, it is imperative to make LLMs sensitive to etiquettes. However,
there needs to be more resources in evaluating LLMs for their understanding and
bias with regard to etiquettes. In this resource paper, we introduce EtiCor++,
a corpus of etiquettes worldwide. We introduce different tasks for evaluating
LLMs for knowledge about etiquettes across various regions. Further, we
introduce various metrics for measuring bias in LLMs. Extensive experimentation
with LLMs shows inherent bias towards certain regions.

</details>


### [71] [Integration of Old and New Knowledge for Generalized Intent Discovery: A Consistency-driven Prototype-Prompting Framework](https://arxiv.org/abs/2506.08490)
*Xiao Wei,Xiaobao Wang,Ning Zhuang,Chenyang Wang,Longbiao Wang,Jianwu dang*

Main category: cs.CL

TL;DR: 论文提出了一种一致性驱动的原型提示框架（GID），用于从新旧知识整合的角度解决广义意图发现问题，显著优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 监督方法依赖标注数据且难以处理域外意图，现有GID方法忽视域适应问题。

Method: 提出原型提示框架和分层一致性约束，整合外部知识并学习目标域新知识。

Result: 实验表明该方法显著优于基线，达到最优性能。

Conclusion: 方法有效且泛化性强，代码已开源。

Abstract: Intent detection aims to identify user intents from natural language inputs,
where supervised methods rely heavily on labeled in-domain (IND) data and
struggle with out-of-domain (OOD) intents, limiting their practical
applicability. Generalized Intent Discovery (GID) addresses this by leveraging
unlabeled OOD data to discover new intents without additional annotation.
However, existing methods focus solely on clustering unsupervised data while
neglecting domain adaptation. Therefore, we propose a consistency-driven
prototype-prompting framework for GID from the perspective of integrating old
and new knowledge, which includes a prototype-prompting framework for
transferring old knowledge from external sources, and a hierarchical
consistency constraint for learning new knowledge from target domains. We
conducted extensive experiments and the results show that our method
significantly outperforms all baseline methods, achieving state-of-the-art
results, which strongly demonstrates the effectiveness and generalization of
our methods. Our source code is publicly available at
https://github.com/smileix/cpp.

</details>


### [72] [Werewolf: A Straightforward Game Framework with TTS for Improved User Engagement](https://arxiv.org/abs/2506.00160)
*Qihui Fan,Enfu Nan,Wenbo Li,Lei Lu,Pu Zhao,Yanzhi Wang*

Main category: cs.CL

TL;DR: 本文提出了一种基于LLM的狼人杀游戏系统，通过优化的TTS模型提升兼容性和用户体验，认为随着LLM推理能力的增强，额外组件将变得多余。


<details>
  <summary>Details</summary>
Motivation: 随着LLM推理和说服能力的提升，结合社交推理游戏的商业和AI研究需求，设计更吸引人的LLM代理狼人杀游戏系统。

Method: 提出一种新颖且直接的基于LLM的狼人杀系统，结合优化的TTS模型，无需额外组件。

Result: 系统提升了与多种LLM模型的兼容性，并改善了用户参与度。

Conclusion: 随着LLM推理能力的持续增强，未来在狼人杀等游戏中，额外组件将不再必要。

Abstract: The growing popularity of social deduction game systems for both business
applications and AI research has greatly benefited from the rapid advancements
in Large Language Models (LLMs), which now demonstrate stronger reasoning and
persuasion capabilities. Especially with the raise of DeepSeek R1 and V3
models, LLMs should enable a more engaging experience for human players in
LLM-agent-based social deduction games like Werewolf. Previous works either
fine-tuning, advanced prompting engineering, or additional experience pool to
achieve engaging text-format Werewolf game experience. We propose a novel yet
straightforward LLM-based Werewolf game system with tuned Text-to-Speech(TTS)
models designed for enhanced compatibility with various LLM models, and
improved user engagement. We argue with ever enhancing LLM reasoning, extra
components will be unnecessary in the case of Werewolf.

</details>


### [73] [DRAGged into Conflicts: Detecting and Addressing Conflicting Sources in Search-Augmented LLMs](https://arxiv.org/abs/2506.08500)
*Arie Cattan,Alon Jacovi,Ori Ram,Jonathan Herzig,Roee Aharoni,Sasha Goldshtein,Eran Ofek,Idan Szpektor,Avi Caciularu*

Main category: cs.CL

TL;DR: 论文提出了一种新的知识冲突分类法，并创建了CONFLICTS基准来评估大语言模型在检索增强生成（RAG）中处理冲突的能力。实验表明，模型在冲突解决上仍有不足，但显式推理提示能显著改善结果。


<details>
  <summary>Details</summary>
Motivation: 解决RAG中检索信息冲突的问题，明确模型应如何处理不同类型的知识冲突。

Method: 提出知识冲突分类法，创建CONFLICTS基准，并通过实验评估模型表现。

Result: 大语言模型在冲突解决上表现不佳，但显式推理提示能显著提升响应质量。

Conclusion: 未来研究需进一步改进模型处理知识冲突的能力。

Abstract: Retrieval Augmented Generation (RAG) is a commonly used approach for
enhancing large language models (LLMs) with relevant and up-to-date
information. However, the retrieved sources can often contain conflicting
information and it remains unclear how models should address such
discrepancies. In this work, we first propose a novel taxonomy of knowledge
conflict types in RAG, along with the desired model behavior for each type. We
then introduce CONFLICTS, a high-quality benchmark with expert annotations of
conflict types in a realistic RAG setting. CONFLICTS is the first benchmark
that enables tracking progress on how models address a wide range of knowledge
conflicts. We conduct extensive experiments on this benchmark, showing that
LLMs often struggle to appropriately resolve conflicts between sources. While
prompting LLMs to explicitly reason about the potential conflict in the
retrieved documents significantly improves the quality and appropriateness of
their responses, substantial room for improvement in future research remains.

</details>


### [74] [CoMuMDR: Code-mixed Multi-modal Multi-domain corpus for Discourse paRsing in conversations](https://arxiv.org/abs/2506.08504)
*Divyaksh Shukla,Ritesh Baviskar,Dwijesh Gohil,Aniket Tiwari,Atul Shree,Ashutosh Modi*

Main category: cs.CL

TL;DR: 论文介绍了CoMuMDR语料库，用于多领域、多模态（音频和文本）的印地语-英语混合语言对话话语解析，并测试了现有模型的表现。


<details>
  <summary>Details</summary>
Motivation: 现有的话语解析数据集局限于单一领域的英文书面对话，而实际应用需要多领域、多模态的混合语言数据。

Method: 构建了印地语-英语混合的多模态多领域语料库（CoMuMDR），并标注了九种话语关系，测试了多种现有模型。

Result: 现有模型在CoMuMDR上表现不佳，凸显了多领域混合语言数据的挑战。

Conclusion: 需要开发更好的模型以应对多领域混合语言的实际场景。

Abstract: Discourse parsing is an important task useful for NLU applications such as
summarization, machine comprehension, and emotion recognition. The current
discourse parsing datasets based on conversations consists of written English
dialogues restricted to a single domain. In this resource paper, we introduce
CoMuMDR: Code-mixed Multi-modal Multi-domain corpus for Discourse paRsing in
conversations. The corpus (code-mixed in Hindi and English) has both audio and
transcribed text and is annotated with nine discourse relations. We experiment
with various SoTA baseline models; the poor performance of SoTA models
highlights the challenges of multi-domain code-mixed corpus, pointing towards
the need for developing better models for such realistic settings.

</details>


### [75] [Being Strong Progressively! Enhancing Knowledge Distillation of Large Language Models through a Curriculum Learning Framework](https://arxiv.org/abs/2506.05695)
*Lingyuan Liu,Mengxiang Zhang*

Main category: cs.CL

TL;DR: 论文提出了一种基于渐进式过载原则的课程学习框架（POCL），用于改进大语言模型的知识蒸馏（KD），通过分阶段引入训练样本提升学习稳定性和效率。


<details>
  <summary>Details</summary>
Motivation: 现有KD方法在大语言模型训练中常导致学生模型分布偏移，引发灾难性遗忘等问题，需一种更稳定的方法。

Method: POCL框架包含难度测量器和训练调度器，分阶段引入从易到难的样本，并逐步提高损失函数温度。

Result: 实验表明POCL能显著提升蒸馏学生模型的性能，适用于多种KD方法和模型家族。

Conclusion: POCL通过结构化训练数据，有效提升KD过程的稳定性和性能。

Abstract: Knowledge Distillation (KD) compresses large language models (LLMs) by
transferring the teacher model's capabilities to a smaller student model,
reducing inference cost and memory usage while maintaining performance.
However, existing KD methods for LLMs often fail to prevent significant shifts
in the student model's distribution during training, leading to issues such as
catastrophic forgetting, mode collapse, and training-inference mismatch. To
address these challenges, we propose a novel, plug-in curriculum learning
framework inspired by the strength training principle of "progressive overload"
(POCL), which can be seamlessly integrated into existing white-box KD
approaches with minimal computational overhead. The framework comprises two
core components: (1) a difficulty measurer that ranks and partitions training
samples from easy to hard, and (2) a training scheduler that incrementally
introduces these subsets into the distillation process at fixed intervals while
applying loss functions with progressively rising temperatures. By starting
with the easiest samples and progressively increasing the difficulty, the
approach enhances both the stability and efficiency of learning. Extensive
experiments in instruction-following settings demonstrate that POCL
consistently improves the performance of distilled student models across
various white-box KD methods and model families. Our findings highlight the
effectiveness of sorted training samples in KD for LLMs. More generally, our
work demonstrates how to structure training data within the KD process to
enhance the stability and performance of distilled LLMs.

</details>


### [76] [Efficient Post-Training Refinement of Latent Reasoning in Large Language Models](https://arxiv.org/abs/2506.08552)
*Xinyuan Wang,Dongjie Wang,Wangyang Ying,Haoyue Bai,Nanxu Gong,Sixun Dong,Kunpeng Liu,Yanjie Fu*

Main category: cs.CL

TL;DR: 论文提出了一种轻量级后训练框架，通过对比推理反馈和残差嵌入细化策略，优化潜在推理轨迹，提升大语言模型的推理准确性。


<details>
  <summary>Details</summary>
Motivation: 解决现有推理方法（如Chain-of-Thought）的固定推理轨迹和额外标记开销问题，以及潜在推理中如何有效更新推理嵌入的挑战。

Method: 提出两种策略：1）对比推理反馈，通过比较强/弱基线优化嵌入方向；2）残差嵌入细化，结合历史梯度稳定更新。

Result: 在五个推理基准测试中验证有效性，MathQA上实现5%的准确率提升。

Conclusion: 该框架显著提升了模型的推理能力，无需额外训练即可实现性能提升。

Abstract: Reasoning is a key component of language understanding in Large Language
Models. While Chain-of-Thought prompting enhances performance via explicit
intermediate steps, it suffers from sufficient token overhead and a fixed
reasoning trajectory, preventing step-wise refinement. Recent advances in
latent reasoning address these limitations by refining internal reasoning
processes directly in the model's latent space, without producing explicit
outputs. However, a key challenge remains: how to effectively update reasoning
embeddings during post-training to guide the model toward more accurate
solutions. To overcome this challenge, we propose a lightweight post-training
framework that refines latent reasoning trajectories using two novel
strategies: 1) Contrastive reasoning feedback, which compares reasoning
embeddings against strong and weak baselines to infer effective update
directions via embedding enhancement; 2) Residual embedding refinement, which
stabilizes updates by progressively integrating current and historical
gradients, enabling fast yet controlled convergence. Extensive experiments and
case studies are conducted on five reasoning benchmarks to demonstrate the
effectiveness of the proposed framework. Notably, a 5\% accuracy gain on MathQA
without additional training.

</details>


### [77] [CounselBench: A Large-Scale Expert Evaluation and Adversarial Benchmark of Large Language Models in Mental Health Counseling](https://arxiv.org/abs/2506.08584)
*Yahan Li,Jifan Yao,John Bosco S. Bunyi,Adam C. Frank,Angel Hwang,Ruishan Liu*

Main category: cs.CL

TL;DR: CounselBench是一个用于评估大型语言模型（LLMs）在心理健康咨询中表现的大规模基准，包含专家评价和对抗性测试，发现LLMs在质量上优于人类治疗师，但存在安全隐患。


<details>
  <summary>Details</summary>
Motivation: 测试LLMs在真实心理咨询场景中的行为，填补其在这一高风险领域的研究空白。

Method: 开发CounselBench基准，包含专家评价（CounselBench-EVAL）和对抗性测试（CounselBench-Adv），评估LLMs在六个临床维度上的表现。

Result: LLMs在感知质量上优于人类治疗师，但常被专家标记为存在安全隐患；LLM评委高估模型表现且忽视安全问题。

Conclusion: CounselBench为LLMs在心理健康领域的评估和改进提供了临床基础框架。

Abstract: Large language models (LLMs) are increasingly proposed for use in mental
health support, yet their behavior in realistic counseling scenarios remains
largely untested. We introduce CounselBench, a large-scale benchmark developed
with 100 mental health professionals to evaluate and stress-test LLMs in
single-turn counseling. The first component, CounselBench-EVAL, contains 2,000
expert evaluations of responses from GPT-4, LLaMA 3, Gemini, and online human
therapists to real patient questions. Each response is rated along six
clinically grounded dimensions, with written rationales and span-level
annotations. We find that LLMs often outperform online human therapists in
perceived quality, but experts frequently flag their outputs for safety
concerns such as unauthorized medical advice. Follow-up experiments show that
LLM judges consistently overrate model responses and overlook safety issues
identified by human experts. To probe failure modes more directly, we construct
CounselBench-Adv, an adversarial dataset of 120 expert-authored counseling
questions designed to trigger specific model issues. Evaluation across 2,880
responses from eight LLMs reveals consistent, model-specific failure patterns.
Together, CounselBench establishes a clinically grounded framework for
benchmarking and improving LLM behavior in high-stakes mental health settings.

</details>


### [78] [Dense Retrievers Can Fail on Simple Queries: Revealing The Granularity Dilemma of Embeddings](https://arxiv.org/abs/2506.08592)
*Liyan Xu,Zhenlin Su,Mo Yu,Jiangnan Li,Fandong Meng,Jie Zhou*

Main category: cs.CL

TL;DR: 论文研究了文本编码器在识别细粒度实体或事件时的局限性，并提出了新的中文评估数据集CapRetrieval，通过微调编码器提升了性能。


<details>
  <summary>Details</summary>
Motivation: 现有文本编码器在细粒度实体或事件识别上表现不佳，导致密集检索失败，需要改进。

Method: 引入CapRetrieval数据集，提出数据生成策略微调编码器，解决细粒度匹配问题。

Result: 微调后的编码器在CapRetrieval上表现最佳，但发现粒度困境问题。

Conclusion: 论文公开了数据集、代码和模型，为细粒度语义匹配提供了新思路。

Abstract: This work focuses on an observed limitation of text encoders: embeddings may
not be able to recognize fine-grained entities or events within the semantics,
resulting in failed dense retrieval on even simple cases. To examine such
behaviors, we first introduce a new evaluation dataset in Chinese, named
CapRetrieval, whose passages are image captions, and queries are phrases
inquiring entities or events in various forms. Zero-shot evaluation suggests
that encoders may fail on these fine-grained matching, regardless of training
sources or model sizes. Aiming for enhancement, we proceed to finetune encoders
with our proposed data generation strategies, which obtains the best
performance on CapRetrieval. Within this process, we further identify an issue
of granularity dilemma, a challenge for embeddings to express fine-grained
salience while aligning with overall semantics. Our dataset, code and models in
this work are publicly released at https://github.com/lxucs/CapRetrieval.

</details>


### [79] [Hateful Person or Hateful Model? Investigating the Role of Personas in Hate Speech Detection by Large Language Models](https://arxiv.org/abs/2506.08593)
*Shuzhou Yuan,Ercong Nie,Mario Tawfelis,Helmut Schmid,Hinrich Schütze,Michael Färber*

Main category: cs.CL

TL;DR: 本文首次全面研究了MBTI人格特质对大型语言模型（LLMs）在仇恨言论分类中的影响，发现人格提示显著影响标注行为，并揭示了模型输出的不一致性和偏见。


<details>
  <summary>Details</summary>
Motivation: 探讨人格特质（如MBTI）对LLMs在仇恨言论检测中的影响，填补了相关研究的空白。

Method: 通过人类标注调查验证MBTI特质对标注行为的影响，并在四个开源LLMs中应用MBTI人格提示，评估其在三个仇恨言论数据集上的表现。

Result: 研究发现人格提示导致模型输出显著变化，包括与真实标签的不一致、人格间分歧以及逻辑层面的偏见。

Conclusion: 研究强调了在LLM标注流程中谨慎定义人格提示的重要性，以确保公平性和与人类价值观的一致性。

Abstract: Hate speech detection is a socially sensitive and inherently subjective task,
with judgments often varying based on personal traits. While prior work has
examined how socio-demographic factors influence annotation, the impact of
personality traits on Large Language Models (LLMs) remains largely unexplored.
In this paper, we present the first comprehensive study on the role of persona
prompts in hate speech classification, focusing on MBTI-based traits. A human
annotation survey confirms that MBTI dimensions significantly affect labeling
behavior. Extending this to LLMs, we prompt four open-source models with MBTI
personas and evaluate their outputs across three hate speech datasets. Our
analysis uncovers substantial persona-driven variation, including
inconsistencies with ground truth, inter-persona disagreement, and logit-level
biases. These findings highlight the need to carefully define persona prompts
in LLM-based annotation workflows, with implications for fairness and alignment
with human values.

</details>


### [80] [RAISE: Enhancing Scientific Reasoning in LLMs via Step-by-Step Retrieval](https://arxiv.org/abs/2506.08625)
*Minhae Oh,Jeonghye Kim,Nakyung Lee,Donggeon Seo,Taeuk Kim,Jungwoo Lee*

Main category: cs.CL

TL;DR: RAISE是一个分步检索增强框架，用于解决科学推理中的挑战，包括问题分解、逻辑查询生成和逻辑检索，性能优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 科学推理需要长链推理、领域术语知识和适应新发现，RAISE旨在应对这些挑战。

Method: RAISE分为三步：问题分解、逻辑查询生成和逻辑检索，从开放语料库中检索逻辑相关文档。

Result: RAISE在科学推理基准测试中表现优于其他基线方法，检索的文档不仅领域相似，逻辑相关性也更强。

Conclusion: RAISE通过逻辑检索和问题分解，显著提升了科学推理的效果。

Abstract: Scientific reasoning requires not only long-chain reasoning processes, but
also knowledge of domain-specific terminologies and adaptation to updated
findings. To deal with these challenges for scientific reasoning, we introduce
RAISE, a step-by-step retrieval-augmented framework which retrieves logically
relevant documents from in-the-wild corpus. RAISE is divided into three steps:
problem decomposition, logical query generation, and logical retrieval. We
observe that RAISE consistently outperforms other baselines on scientific
reasoning benchmarks. We analyze that unlike other baselines, RAISE retrieves
documents that are not only similar in terms of the domain knowledge, but also
documents logically more relevant.

</details>


### [81] [MEMETRON: Metaheuristic Mechanisms for Test-time Response Optimization of Large Language Models](https://arxiv.org/abs/2506.08643)
*Son The Nguyen,Theja Tulabandhula*

Main category: cs.CL

TL;DR: MEMETRON是一个任务无关的框架，将LLM解码视为离散黑盒优化问题，通过混合元启发式算法高效发现高奖励响应。


<details>
  <summary>Details</summary>
Motivation: 现有LLM的解码策略（如贪婪搜索、采样或重排序）缺乏对任务目标的明确优化，限制了控制能力。

Method: MEMETRON利用混合元启发式算法（GENETRON和ANNETRON）搜索响应空间，由奖励模型和LLM的上下文操作引导。

Result: 在人类偏好对齐任务中，MEMETRON显著优于标准解码和重排序方法。

Conclusion: MEMETRON无需模型重新训练即可提升对齐效果，具有广泛适用性。

Abstract: Large language models (LLMs) are increasingly used for both open-ended and
structured tasks, yet their inference-time behavior is still largely dictated
by heuristic decoding strategies such as greedy search, sampling, or reranking.
These methods provide limited control and do not explicitly optimize for
task-specific objectives. We introduce MEMETRON, a task-agnostic framework that
formulates LLM decoding as a discrete black-box optimization problem. MEMETRON
leverages hybrid metaheuristic algorithms, GENETRON and ANNETRON, to search the
response space, guided by reward models and contextual operations performed by
the LLM itself. This approach enables efficient discovery of high-reward
responses without requiring model retraining or gradient access. The framework
is modular and generalizes across diverse tasks, requiring only a reward
function and lightweight prompt templates. We evaluate our framework on the
critical human preference alignment task and demonstrate that it significantly
outperforms standard decoding and reranking methods, highlighting its potential
to improve alignment without model retraining.

</details>


### [82] [TableDreamer: Progressive and Weakness-guided Data Synthesis from Scratch for Table Instruction Tuning](https://arxiv.org/abs/2506.08646)
*Mingyu Zheng,Zhifan Feng,Jia Wang,Lanrui Wang,Zheng Lin,Yang Hao,Weiping Wang*

Main category: cs.CL

TL;DR: TableDreamer是一个渐进式、弱点引导的数据合成框架，用于解决LLM在表格指令调优中数据多样性和效率不足的问题。


<details>
  <summary>Details</summary>
Motivation: 现有LLM数据合成方法在表格理解任务中存在输入空间探索不足和数据效率低下的问题。

Method: 通过合成多样化表格和指令作为种子数据，并在弱点数据引导下迭代探索输入空间，最终生成训练数据。

Result: 在10个表格基准测试中，TableDreamer将Llama3.1-8B-instruct的平均准确率提升11.62%，优于现有方法。

Conclusion: TableDreamer显著提升了表格指令调优的数据多样性和效率，为LLM优化提供了新思路。

Abstract: Despite the commendable progress of recent LLM-based data synthesis methods,
they face two limitations in generating table instruction tuning data. First,
they can not thoroughly explore the vast input space of table understanding
tasks, leading to limited data diversity. Second, they ignore the weaknesses in
table understanding ability of the target LLM and blindly pursue the increase
of data quantity, resulting in suboptimal data efficiency. In this paper, we
introduce a progressive and weakness-guided data synthesis framework tailored
for table instruction tuning, named TableDreamer, to mitigate the above issues.
Specifically, we first synthesize diverse tables and related instructions as
seed data, and then perform an iterative exploration of the input space under
the guidance of the newly identified weakness data, which eventually serve as
the final training data for fine-tuning the target LLM. Extensive experiments
on 10 tabular benchmarks demonstrate the effectiveness of the proposed
framework, which boosts the average accuracy of Llama3.1-8B-instruct by 11.62%
(49.07% to 60.69%) with 27K GPT-4o synthetic data and outperforms
state-of-the-art data synthesis baselines which use more training data. The
code and data is available at https://github.com/SpursGoZmy/TableDreamer

</details>


### [83] [Summarization for Generative Relation Extraction in the Microbiome Domain](https://arxiv.org/abs/2506.08647)
*Oumaima El Khettari,Solen Quiniou,Samuel Chaffron*

Main category: cs.CL

TL;DR: 生成式关系提取方法在肠道微生物组研究中表现潜力，但BERT方法仍更优。


<details>
  <summary>Details</summary>
Motivation: 研究肠道微生物组这一低资源生物医学领域中的关系提取问题。

Method: 利用大型语言模型（LLMs）进行上下文总结，再通过指令调优生成关系。

Result: 总结能减少噪声并提升生成式关系提取性能，但BERT方法仍优于生成模型。

Conclusion: 生成式方法在低资源专业领域研究中具有潜力，但需进一步改进。

Abstract: We explore a generative relation extraction (RE) pipeline tailored to the
study of interactions in the intestinal microbiome, a complex and low-resource
biomedical domain. Our method leverages summarization with large language
models (LLMs) to refine context before extracting relations via
instruction-tuned generation. Preliminary results on a dedicated corpus show
that summarization improves generative RE performance by reducing noise and
guiding the model. However, BERT-based RE approaches still outperform
generative models. This ongoing work demonstrates the potential of generative
methods to support the study of specialized domains in low-resources setting.

</details>


### [84] [RuleReasoner: Reinforced Rule-based Reasoning via Domain-aware Dynamic Sampling](https://arxiv.org/abs/2506.08672)
*Yang Liu,Jiaqi Li,Zilong Zheng*

Main category: cs.CL

TL;DR: 论文提出了一种名为RuleReasoner的强化学习方法，用于提升小型推理模型在规则推理任务中的表现，并通过动态采样实现跨领域泛化。


<details>
  <summary>Details</summary>
Motivation: 解决小型推理模型在多样化任务和领域中规则推理能力不足的问题。

Method: 引入RuleReasoner方法，结合动态采样和强化学习，优化训练过程。

Result: 在ID和OOD任务中显著优于前沿大型推理模型，且计算效率更高。

Conclusion: RuleReasoner是一种高效且泛化能力强的规则推理方法。

Abstract: Rule-based reasoning has been acknowledged as one of the fundamental problems
in reasoning, while deviations in rule formats, types, and complexity in
real-world applications pose severe challenges. Recent studies have shown that
large reasoning models (LRMs) have remarkable reasoning capabilities, and their
performance is substantially enhanced by reinforcement learning (RL). However,
it remains an open question whether small reasoning models (SRMs) can learn
rule-based reasoning effectively with robust generalization across diverse
tasks and domains. To address this, we introduce Reinforced Rule-based
Reasoning, a.k.a. RuleReasoner, a simple yet effective method to conduct
rule-based reasoning via a wide collection of curated tasks and a novel
domain-aware dynamic sampling approach. Specifically, RuleReasoner resamples
each training batch by updating the sampling weights of different domains based
on historical rewards. This facilitates domain augmentation and flexible online
learning schedules for RL, obviating the need for pre-hoc human-engineered
mix-training recipes used in existing methods. Empirical evaluations on
in-distribution (ID) and out-of-distribution (OOD) benchmarks reveal that
RuleReasoner outperforms frontier LRMs by a significant margin ($\Delta$4.1%
average points on eight ID tasks and $\Delta$10.4% average points on three OOD
tasks over OpenAI-o1). Notably, our approach also exhibits higher computational
efficiency compared to prior dynamic sampling methods for RL.

</details>


### [85] [Brevity is the soul of sustainability: Characterizing LLM response lengths](https://arxiv.org/abs/2506.08686)
*Soham Poddar,Paramita Koley,Janardan Misra,Sanjay Podder,Navveen Balani,Niloy Ganguly,Saptarshi Ghosh*

Main category: cs.CL

TL;DR: 论文研究了大型语言模型（LLM）推理过程中的能源消耗问题，提出通过输出压缩和提示工程策略减少响应长度，实现25-60%的能源优化。


<details>
  <summary>Details</summary>
Motivation: LLM推理过程消耗大量能源，而输出压缩领域研究较少，因此探索减少响应长度的方法至关重要。

Method: 对12个仅解码器LLM在5个数据集上进行基准测试，定义LLM响应中的六类信息，并通过提示工程策略优化响应长度。

Result: 适当的提示策略可减少响应长度25-60%，同时保持响应质量。

Conclusion: 通过提示工程优化LLM响应长度是减少能源消耗的有效方法。

Abstract: A significant portion of the energy consumed by Large Language Models (LLMs)
arises from their inference processes; hence developing energy-efficient
methods for inference is crucial. While several techniques exist for inference
optimization, output compression remains relatively unexplored, with only a few
preliminary efforts addressing this aspect. In this work, we first benchmark 12
decoder-only LLMs across 5 datasets, revealing that these models often produce
responses that are substantially longer than necessary. We then conduct a
comprehensive quality assessment of LLM responses, formally defining six
information categories present in LLM responses. We show that LLMs often tend
to include redundant or additional information besides the minimal answer. To
address this issue of long responses by LLMs, we explore several simple and
intuitive prompt-engineering strategies. Empirical evaluation shows that
appropriate prompts targeting length reduction and controlling information
content can achieve significant energy optimization between 25-60\% by reducing
the response length while preserving the quality of LLM responses.

</details>


### [86] [ClimateViz: A Benchmark for Statistical Reasoning and Fact Verification on Scientific Charts](https://arxiv.org/abs/2506.08700)
*Ruiran Su,Jiasheng Si,Zhijiang Guo,Janet B. Pierrehumbert*

Main category: cs.CL

TL;DR: ClimateViz是首个基于科学图表的大规模事实核查基准，包含49,862个与2,896个可视化图表关联的声明，评估了多模态语言模型在图表推理上的表现。


<details>
  <summary>Details</summary>
Motivation: 科学事实核查主要关注文本和表格，忽略了科学图表在呈现定量证据和统计推理中的重要性。

Method: 引入ClimateViz数据集，评估了包括专有和开源系统在内的多模态语言模型在零样本和少样本设置下的表现。

Result: 当前模型在图表推理上表现不佳，最佳模型准确率仅为76.2%至77.8%，远低于人类水平（89.3%至92.7%）。

Conclusion: 解释增强的输出可提升部分模型性能，数据集和代码已公开。

Abstract: Scientific fact-checking has mostly focused on text and tables, overlooking
scientific charts, which are key for presenting quantitative evidence and
statistical reasoning. We introduce ClimateViz, the first large-scale benchmark
for scientific fact-checking using expert-curated scientific charts. ClimateViz
contains 49,862 claims linked to 2,896 visualizations, each labeled as support,
refute, or not enough information. To improve interpretability, each example
includes structured knowledge graph explanations covering trends, comparisons,
and causal relations. We evaluate state-of-the-art multimodal language models,
including both proprietary and open-source systems, in zero-shot and few-shot
settings. Results show that current models struggle with chart-based reasoning:
even the best systems, such as Gemini 2.5 and InternVL 2.5, reach only 76.2 to
77.8 percent accuracy in label-only settings, far below human performance (89.3
and 92.7 percent). Explanation-augmented outputs improve performance in some
models. We released our dataset and code alongside the paper.

</details>


### [87] [ConfPO: Exploiting Policy Model Confidence for Critical Token Selection in Large Language Model Preference Optimization](https://arxiv.org/abs/2506.08712)
*Hee Suk Yoon,Eunseop Yoon,Mark A. Hasegawa-Johnson,Sungwoong Kim,Chang D. Yoo*

Main category: cs.CL

TL;DR: ConfPO是一种基于训练策略置信度的偏好学习方法，专注于优化对偏好影响最大的关键token，无需额外模型或计算资源。


<details>
  <summary>Details</summary>
Motivation: 现有直接对齐算法（如DPO）对所有token进行均匀调整，而ConfPO旨在通过更高效地利用KL散度预算，提高对齐质量并避免过优化。

Method: ConfPO仅依赖训练策略的置信度来识别和优化偏好关键token，无需辅助模型或额外计算。

Result: 在AlpacaEval 2和Arena-Hard等基准测试中，ConfPO表现优于均匀调整的直接对齐算法，且无额外计算开销。

Conclusion: ConfPO是一种简单、轻量且无需模型的方法，能有效提升LLM的对齐质量。

Abstract: We introduce ConfPO, a method for preference learning in Large Language
Models (LLMs) that identifies and optimizes preference-critical tokens based
solely on the training policy's confidence, without requiring any auxiliary
models or compute. Unlike prior Direct Alignment Algorithms (DAAs) such as
Direct Preference Optimization (DPO), which uniformly adjust all token
probabilities regardless of their relevance to preference, ConfPO focuses
optimization on the most impactful tokens. This targeted approach improves
alignment quality while mitigating overoptimization (i.e., reward hacking) by
using the KL divergence budget more efficiently. In contrast to recent
token-level methods that rely on credit-assignment models or AI annotators,
raising concerns about scalability and reliability, ConfPO is simple,
lightweight, and model-free. Experimental results on challenging alignment
benchmarks, including AlpacaEval 2 and Arena-Hard, demonstrate that ConfPO
consistently outperforms uniform DAAs across various LLMs, delivering better
alignment with zero additional computational overhead.

</details>


### [88] [Explainable Compliance Detection with Multi-Hop Natural Language Inference on Assurance Case Structure](https://arxiv.org/abs/2506.08713)
*Fariz Ikhwantri,Dusica Marijan*

Main category: cs.CL

TL;DR: 论文提出了一种基于自然语言推理（NLI）的合规检测方法EXCLAIM，通过多跳推理实现可解释和可追溯的合规检测，并利用大型语言模型生成保证案例。


<details>
  <summary>Details</summary>
Motivation: 解决复杂系统中合规性检测的挑战，如法律和技术文本的复杂性、模型解释需求以及保证案例数据的有限性。

Method: 将保证案例的声明-论据-证据结构建模为多跳推理，利用大型语言模型生成保证案例，并引入覆盖率和结构一致性指标。

Result: 通过GDPR要求的案例研究验证了生成保证案例在多跳推理任务中的有效性。

Conclusion: NLI方法在自动化监管合规过程中具有潜力。

Abstract: Ensuring complex systems meet regulations typically requires checking the
validity of assurance cases through a claim-argument-evidence framework. Some
challenges in this process include the complicated nature of legal and
technical texts, the need for model explanations, and limited access to
assurance case data. We propose a compliance detection approach based on
Natural Language Inference (NLI): EXplainable CompLiance detection with
Argumentative Inference of Multi-hop reasoning (EXCLAIM). We formulate the
claim-argument-evidence structure of an assurance case as a multi-hop inference
for explainable and traceable compliance detection. We address the limited
number of assurance cases by generating them using large language models
(LLMs). We introduce metrics that measure the coverage and structural
consistency. We demonstrate the effectiveness of the generated assurance case
from GDPR requirements in a multi-hop inference task as a case study. Our
results highlight the potential of NLI-based approaches in automating the
regulatory compliance process.

</details>


### [89] [Improved LLM Agents for Financial Document Question Answering](https://arxiv.org/abs/2506.08726)
*Nelvin Tan,Zian Seng,Liang Zhang,Yu-Ching Shih,Dong Yang,Amol Salunkhe*

Main category: cs.CL

TL;DR: 论文研究了大型语言模型（LLM）在金融文档数值问答任务中的局限性，提出了一种改进的批评代理和计算器代理，提升了性能并增强了安全性。


<details>
  <summary>Details</summary>
Motivation: LLM在金融文档的数值问答任务中表现不佳，尤其是缺乏标注数据时，传统批评代理性能下降。

Method: 提出改进的批评代理和计算器代理，并研究它们的交互对性能的影响。

Result: 新方法优于现有技术（程序思维），且更安全。

Conclusion: 改进的代理方法在无标注数据时表现更优，交互研究为其性能提升提供了新视角。

Abstract: Large language models (LLMs) have shown impressive capabilities on numerous
natural language processing tasks. However, LLMs still struggle with numerical
question answering for financial documents that include tabular and textual
data. Recent works have showed the effectiveness of critic agents (i.e.,
self-correction) for this task given oracle labels. Building upon this
framework, this paper examines the effectiveness of the traditional critic
agent when oracle labels are not available, and show, through experiments, that
this critic agent's performance deteriorates in this scenario. With this in
mind, we present an improved critic agent, along with the calculator agent
which outperforms the previous state-of-the-art approach (program-of-thought)
and is safer. Furthermore, we investigate how our agents interact with each
other, and how this interaction affects their performance.

</details>


### [90] [Societal AI Research Has Become Less Interdisciplinary](https://arxiv.org/abs/2506.08738)
*Dror Kris Markus,Fabrizio Gilardi,Daria Stetsenko*

Main category: cs.CL

TL;DR: 研究分析了2014-2024年ArXiv上10万篇AI相关论文，发现跨学科团队虽更倾向于关注社会伦理问题，但纯计算机科学团队在此领域的贡献也在显著增加。


<details>
  <summary>Details</summary>
Motivation: 探讨AI技术研究中伦理和社会价值的整合情况，以及跨学科合作的实际效果。

Method: 开发分类器识别论文中的社会内容，并量化其表达程度。

Result: 跨学科团队仍主导社会导向研究，但纯技术团队在此类研究中的占比显著上升，涉及领域广泛。

Conclusion: 研究挑战了跨学科合作是推动社会导向AI研究的唯一途径的假设，并引发了对技术团队主导社会研究的潜在影响及人文社科独特贡献的思考。

Abstract: As artificial intelligence (AI) systems become deeply embedded in everyday
life, calls to align AI development with ethical and societal values have
intensified. Interdisciplinary collaboration is often championed as a key
pathway for fostering such engagement. Yet it remains unclear whether
interdisciplinary research teams are actually leading this shift in practice.
This study analyzes over 100,000 AI-related papers published on ArXiv between
2014 and 2024 to examine how ethical values and societal concerns are
integrated into technical AI research. We develop a classifier to identify
societal content and measure the extent to which research papers express these
considerations. We find a striking shift: while interdisciplinary teams remain
more likely to produce societally-oriented research, computer science-only
teams now account for a growing share of the field's overall societal output.
These teams are increasingly integrating societal concerns into their papers
and tackling a wide range of domains - from fairness and safety to healthcare
and misinformation. These findings challenge common assumptions about the
drivers of societal AI and raise important questions. First, what are the
implications for emerging understandings of AI safety and governance if most
societally-oriented research is being undertaken by exclusively technical
teams? Second, for scholars in the social sciences and humanities: in a
technical field increasingly responsive to societal demands, what distinctive
perspectives can we still offer to help shape the future of AI?

</details>


### [91] [Towards Secure and Private Language Models for Nuclear Power Plants](https://arxiv.org/abs/2506.08746)
*Muhammad Anwar,Mishca de Costa,Issam Hammad,Daniel Lau*

Main category: cs.CL

TL;DR: 本文介绍了一种用于核能应用的领域特定大语言模型，基于公开的Essential CANDU教材构建，采用紧凑的Transformer架构，在单GPU上训练以保护敏感数据。尽管数据集较小，模型能捕捉专业核词汇，但生成文本有时缺乏句法连贯性。


<details>
  <summary>Details</summary>
Motivation: 开发一种符合严格网络安全和数据保密标准的内部LLM解决方案，专注于核能领域内容。

Method: 基于Transformer架构，使用单GPU训练，数据集来自Essential CANDU教材。

Result: 模型能捕捉专业核词汇，但生成文本有时不连贯。初步成功展示了其在专业任务中的实用性。

Conclusion: 未来需扩展数据集、优化预处理和指令微调，以提升领域准确性，并评估模型在核能领域的实际应用准备度。

Abstract: This paper introduces a domain-specific Large Language Model for nuclear
applications, built from the publicly accessible Essential CANDU textbook.
Drawing on a compact Transformer-based architecture, the model is trained on a
single GPU to protect the sensitive data inherent in nuclear operations.
Despite relying on a relatively small dataset, it shows encouraging signs of
capturing specialized nuclear vocabulary, though the generated text sometimes
lacks syntactic coherence. By focusing exclusively on nuclear content, this
approach demonstrates the feasibility of in-house LLM solutions that align with
rigorous cybersecurity and data confidentiality standards. Early successes in
text generation underscore the model's utility for specialized tasks, while
also revealing the need for richer corpora, more sophisticated preprocessing,
and instruction fine-tuning to enhance domain accuracy. Future directions
include extending the dataset to cover diverse nuclear subtopics, refining
tokenization to reduce noise, and systematically evaluating the model's
readiness for real-world applications in nuclear domain.

</details>


### [92] [Unlocking the Potential of Large Language Models in the Nuclear Industry with Synthetic Data](https://arxiv.org/abs/2506.08750)
*Muhammad Anwar,Daniel Lau,Mishca de Costa,Issam Hammad*

Main category: cs.CL

TL;DR: 论文探讨了如何通过合成数据生成解决核工业中非结构化文本数据的可用性问题，以支持大型语言模型的应用。


<details>
  <summary>Details</summary>
Motivation: 核工业中存在大量非结构化文本数据，但这些数据难以直接用于需要结构化问答对的LLM任务。

Method: 利用LLM分析文本、提取关键信息、生成相关问题，并评估合成数据集的质量。

Result: 合成数据能够填补数据稀缺和隐私问题的缺口，支持核工业中的LLM应用。

Conclusion: 合成数据为核工业的信息检索、知识共享和决策提供了新途径。

Abstract: The nuclear industry possesses a wealth of valuable information locked away
in unstructured text data. This data, however, is not readily usable for
advanced Large Language Model (LLM) applications that require clean, structured
question-answer pairs for tasks like model training, fine-tuning, and
evaluation. This paper explores how synthetic data generation can bridge this
gap, enabling the development of robust LLMs for the nuclear domain. We discuss
the challenges of data scarcity and privacy concerns inherent in the nuclear
industry and how synthetic data provides a solution by transforming existing
text data into usable Q&A pairs. This approach leverages LLMs to analyze text,
extract key information, generate relevant questions, and evaluate the quality
of the resulting synthetic dataset. By unlocking the potential of LLMs in the
nuclear industry, synthetic data can pave the way for improved information
retrieval, enhanced knowledge sharing, and more informed decision-making in
this critical sector.

</details>


### [93] [Factors affecting the in-context learning abilities of LLMs for dialogue state tracking](https://arxiv.org/abs/2506.08753)
*Pradyoth Hegde,Santosh Kesiraju,Jan Švec,Šimon Sedláček,Bolaji Yusuf,Oldřich Plchot,Deepak K T,Jan Černocký*

Main category: cs.CL

TL;DR: 该研究探讨了上下文学习（ICL）在对话状态跟踪（DST）中的应用，并分析了影响其效果的因素。


<details>
  <summary>Details</summary>
Motivation: 研究旨在探索ICL在DST中的潜力，并识别影响其性能的关键因素。

Method: 采用基于句子嵌入的k近邻方法检索适合ICL的示例，并将示例与测试样本通过模板输入LLM。

Result: 在MultiWoZ2.4数据集上，对OLMo-7B-instruct等模型进行了系统分析，提供了关于LLM在DST中上下文学习能力的见解。

Conclusion: 研究为LLM在DST任务中的上下文学习能力提供了有价值的发现。

Abstract: This study explores the application of in-context learning (ICL) to the
dialogue state tracking (DST) problem and investigates the factors that
influence its effectiveness. We use a sentence embedding based k-nearest
neighbour method to retrieve the suitable demonstrations for ICL. The selected
demonstrations, along with the test samples, are structured within a template
as input to the LLM. We then conduct a systematic study to analyse the impact
of factors related to demonstration selection and prompt context on DST
performance. This work is conducted using the MultiWoZ2.4 dataset and focuses
primarily on the OLMo-7B-instruct, Mistral-7B-Instruct-v0.3, and
Llama3.2-3B-Instruct models. Our findings provide several useful insights on
in-context learning abilities of LLMs for dialogue state tracking.

</details>


### [94] [Enhancing Accuracy and Maintainability in Nuclear Plant Data Retrieval: A Function-Calling LLM Approach Over NL-to-SQL](https://arxiv.org/abs/2506.08757)
*Mishca de Costa,Muhammad Anwar,Dave Mercier,Mark Randall,Issam Hammad*

Main category: cs.CL

TL;DR: 论文提出了一种基于函数调用的大型语言模型（LLM）方法，替代直接生成SQL查询，以提高核电站数据检索的准确性和安全性。


<details>
  <summary>Details</summary>
Motivation: 传统自然语言转SQL（NL-to-SQL）方法在核电站等关键系统中存在验证困难和准确性风险，亟需更可靠的解决方案。

Method: 通过预定义一组经过专家验证的专用函数，封装SQL逻辑，利用LLM调用这些函数而非直接生成SQL。

Result: 实验表明，该方法在准确性和可维护性上优于直接NL-to-SQL生成。

Conclusion: 该框架平衡了用户易用性与操作安全性，为关键系统提供了可靠的数据检索方案。

Abstract: Retrieving operational data from nuclear power plants requires exceptional
accuracy and transparency due to the criticality of the decisions it supports.
Traditionally, natural language to SQL (NL-to-SQL) approaches have been
explored for querying such data. While NL-to-SQL promises ease of use, it poses
significant risks: end-users cannot easily validate generated SQL queries, and
legacy nuclear plant databases -- often complex and poorly structured --
complicate query generation due to decades of incremental modifications. These
challenges increase the likelihood of inaccuracies and reduce trust in the
approach. In this work, we propose an alternative paradigm: leveraging
function-calling large language models (LLMs) to address these challenges.
Instead of directly generating SQL queries, we define a set of pre-approved,
purpose-specific functions representing common use cases. Queries are processed
by invoking these functions, which encapsulate validated SQL logic. This hybrid
approach mitigates the risks associated with direct NL-to-SQL translations by
ensuring that SQL queries are reviewed and optimized by experts before
deployment. While this strategy introduces the upfront cost of developing and
maintaining the function library, we demonstrate how NL-to-SQL tools can assist
in the initial generation of function code, allowing experts to focus on
validation rather than creation. Our study includes a performance comparison
between direct NL-to-SQL generation and the proposed function-based approach,
highlighting improvements in accuracy and maintainability. This work
underscores the importance of balancing user accessibility with operational
safety and provides a novel, actionable framework for robust data retrieval in
critical systems.

</details>


### [95] [AraReasoner: Evaluating Reasoning-Based LLMs for Arabic NLP](https://arxiv.org/abs/2506.08768)
*Ahmed Hasanaath,Aisha Alansari,Ahmed Ashraf,Chafik Salmane,Hamzah Luqman,Saad Ezzini*

Main category: cs.CL

TL;DR: 论文研究了大型语言模型（LLMs）在阿拉伯语数据上的表现，特别关注DeepSeek模型，通过多种策略（零样本、少样本和微调）在15个阿拉伯语NLP任务上进行了全面评估。


<details>
  <summary>Details</summary>
Motivation: 阿拉伯语因其丰富的形态、多样的方言和复杂的书写系统，LLMs在该语言上的表现尚未充分研究。

Method: 使用零样本、少样本和微调策略，在15个阿拉伯语NLP任务上评估多个推理型LLMs，重点关注DeepSeek模型。

Result: 关键发现包括：少样本策略显著提升分类任务性能；DeepSeek在零样本设置下优于GPT o4-mini；LoRA微调比模型规模扩展更有效。

Conclusion: 研究表明，适当的策略选择和模型架构能显著提升LLMs在阿拉伯语任务上的表现，为未来研究提供了方向。

Abstract: Large language models (LLMs) have shown remarkable progress in reasoning
abilities and general natural language processing (NLP) tasks, yet their
performance on Arabic data, characterized by rich morphology, diverse dialects,
and complex script, remains underexplored. This paper presents a comprehensive
benchmarking study of multiple reasoning-focused LLMs, with a special emphasis
on the newly introduced DeepSeek models, across a suite of fifteen Arabic NLP
tasks. We experiment with various strategies, including zero-shot, few-shot,
and fine-tuning. This allows us to systematically evaluate performance on
datasets covering a range of applications to examine their capacity for
linguistic reasoning under different levels of complexity. Our experiments
reveal several key findings. First, carefully selecting just three in-context
examples delivers an average uplift of over 13 F1 points on classification
tasks-boosting sentiment analysis from 35.3% to 87.5% and paraphrase detection
from 56.1% to 87.0%. Second, reasoning-focused DeepSeek architectures
outperform a strong GPT o4-mini baseline by an average of 12 F1 points on
complex inference tasks in the zero-shot setting. Third, LoRA-based fine-tuning
yields up to an additional 8 points in F1 and BLEU compared to equivalent
increases in model scale. The code is available at
https://anonymous.4open.science/r/AraReasoner41299

</details>


### [96] [The impact of fine tuning in LLaMA on hallucinations for named entity extraction in legal documentation](https://arxiv.org/abs/2506.08827)
*Francisco Vargas,Alejandro González Coene,Gaston Escalante,Exequiel Lobón,Manuel Pulido*

Main category: cs.CL

TL;DR: 论文提出了一种两阶段方法，从法律文件中提取交通事故信息，包括伤残百分比和赔偿金额。通过文本分割和实体提取，比较了传统正则表达式与基于语义搜索的方法，并评估了多种LLM的性能。


<details>
  <summary>Details</summary>
Motivation: 从法律文件中提取交通事故信息对保险公司成本量化至关重要，但由于法院判决中的复杂论证，这一过程极具挑战性。

Method: 提出两阶段方法：1）文本分割（正则表达式或语义搜索）；2）使用LLM（如LLaMA-2、LLaMA-3、GPT-4 Turbo）进行实体提取，并对部分模型进行微调。

Result: 基于语义搜索和LLM的方法显著优于传统方法（39.5%）。微调后的LLaMA-2 70B达到79.4%，LLaMA-3 8B基础模型表现接近（76.6%），GPT-4 Turbo表现最佳（86.1%）。

Conclusion: 语义搜索结合LLM的方法优于传统方法，微调和模型进步显著提升性能，GPT-4 Turbo表现最优。

Abstract: The extraction of information about traffic accidents from legal documents is
crucial for quantifying insurance company costs. Extracting entities such as
percentages of physical and/or psychological disability and the involved
compensation amounts is a challenging process, even for experts, due to the
subtle arguments and reasoning in the court decision. A two-step procedure is
proposed: first, segmenting the document identifying the most relevant
segments, and then extracting the entities. For text segmentation, two
methodologies are compared: a classic method based on regular expressions and a
second approach that divides the document into blocks of n-tokens, which are
then vectorized using multilingual models for semantic searches
(text-embedding-ada-002/MiniLM-L12-v2 ). Subsequently, large language models
(LLaMA-2 7b, 70b, LLaMA-3 8b, and GPT-4 Turbo) are applied with prompting to
the selected segments for entity extraction. For the LLaMA models, fine-tuning
is performed using LoRA. LLaMA-2 7b, even with zero temperature, shows a
significant number of hallucinations in extractions which are an important
contention point for named entity extraction. This work shows that these
hallucinations are substantially reduced after finetuning the model. The
performance of the methodology based on segment vectorization and subsequent
use of LLMs significantly surpasses the classic method which achieves an
accuracy of 39.5%. Among open-source models, LLaMA-2 70B with finetuning
achieves the highest accuracy 79.4%, surpassing its base version 61.7%.
Notably, the base LLaMA-3 8B model already performs comparably to the finetuned
LLaMA-2 70B model, achieving 76.6%, highlighting the rapid progress in model
development. Meanwhile, GPT-4 Turbo achieves the highest accuracy at 86.1%.

</details>


### [97] [AdversariaL attacK sAfety aLIgnment(ALKALI): Safeguarding LLMs through GRACE: Geometric Representation-Aware Contrastive Enhancement- Introducing Adversarial Vulnerability Quality Index (AVQI)](https://arxiv.org/abs/2506.08885)
*Danush Khanna,Krishna Kumar,Basab Ghosh,Vinija Jain,Vasu Sharma,Aman Chadha,Amitava Das*

Main category: cs.CL

TL;DR: 论文揭示了LLMs中潜在伪装的安全盲点，提出了ALKALI基准和GRACE框架，显著降低了攻击成功率。


<details>
  <summary>Details</summary>
Motivation: 当前防御机制无法快速应对LLMs的对抗性威胁，特别是潜在伪装问题。

Method: 引入ALKALI基准和GRACE框架，通过几何表示对比增强和潜在空间正则化来防御攻击。

Result: GRACE框架将攻击成功率降低39%，并提出了量化潜在对齐失败的AVQI指标。

Conclusion: 论文为LLMs的安全对齐提供了新方法和工具，揭示了潜在伪装的结构性漏洞。

Abstract: Adversarial threats against LLMs are escalating faster than current defenses
can adapt. We expose a critical geometric blind spot in alignment: adversarial
prompts exploit latent camouflage, embedding perilously close to the safe
representation manifold while encoding unsafe intent thereby evading surface
level defenses like Direct Preference Optimization (DPO), which remain blind to
the latent geometry. We introduce ALKALI, the first rigorously curated
adversarial benchmark and the most comprehensive to date spanning 9,000 prompts
across three macro categories, six subtypes, and fifteen attack families.
Evaluation of 21 leading LLMs reveals alarmingly high Attack Success Rates
(ASRs) across both open and closed source models, exposing an underlying
vulnerability we term latent camouflage, a structural blind spot where
adversarial completions mimic the latent geometry of safe ones. To mitigate
this vulnerability, we introduce GRACE - Geometric Representation Aware
Contrastive Enhancement, an alignment framework coupling preference learning
with latent space regularization. GRACE enforces two constraints: latent
separation between safe and adversarial completions, and adversarial cohesion
among unsafe and jailbreak behaviors. These operate over layerwise pooled
embeddings guided by a learned attention profile, reshaping internal geometry
without modifying the base model, and achieve up to 39% ASR reduction.
Moreover, we introduce AVQI, a geometry aware metric that quantifies latent
alignment failure via cluster separation and compactness. AVQI reveals when
unsafe completions mimic the geometry of safe ones, offering a principled lens
into how models internally encode safety. We make the code publicly available
at https://anonymous.4open.science/r/alkali-B416/README.md.

</details>


### [98] [PlantBert: An Open Source Language Model for Plant Science](https://arxiv.org/abs/2506.08897)
*Hiba Khey,Amine Lakhder,Salma Rouichi,Imane El Ghabi,Kamal Hejjaoui,Younes En-nahli,Fahd Kalloubi,Moez Amri*

Main category: cs.CL

TL;DR: PlantBert是一个基于DeBERTa架构的高性能开源语言模型，专为植物胁迫响应文献的结构化知识提取设计，填补了植物科学领域NLP工具的空白。


<details>
  <summary>Details</summary>
Motivation: 植物科学领域缺乏针对性的语言模型工具，PlantBert旨在解决这一问题，为植物胁迫响应研究提供高效支持。

Method: 基于DeBERTa架构，结合规则增强的语言后处理和本体实体归一化，使用专家标注的植物胁迫响应文献进行微调。

Result: PlantBert在实体识别和语义关系提取上表现出色，展示了在低资源科学领域的强大适应性。

Conclusion: PlantBert为农业NLP提供了可扩展的解决方案，推动了植物科学领域的数据驱动研究。

Abstract: The rapid advancement of transformer-based language models has catalyzed
breakthroughs in biomedical and clinical natural language processing; however,
plant science remains markedly underserved by such domain-adapted tools. In
this work, we present PlantBert, a high-performance, open-source language model
specifically tailored for extracting structured knowledge from plant
stress-response literature. Built upon the DeBERTa architecture-known for its
disentangled attention and robust contextual encoding-PlantBert is fine-tuned
on a meticulously curated corpus of expert-annotated abstracts, with a primary
focus on lentil (Lens culinaris) responses to diverse abiotic and biotic
stressors. Our methodology combines transformer-based modeling with
rule-enhanced linguistic post-processing and ontology-grounded entity
normalization, enabling PlantBert to capture biologically meaningful
relationships with precision and semantic fidelity. The underlying corpus is
annotated using a hierarchical schema aligned with the Crop Ontology,
encompassing molecular, physiological, biochemical, and agronomic dimensions of
plant adaptation. PlantBert exhibits strong generalization capabilities across
entity types and demonstrates the feasibility of robust domain adaptation in
low-resource scientific fields. By providing a scalable and reproducible
framework for high-resolution entity recognition, PlantBert bridges a critical
gap in agricultural NLP and paves the way for intelligent, data-driven systems
in plant genomics, phenomics, and agronomic knowledge discovery. Our model is
publicly released to promote transparency and accelerate cross-disciplinary
innovation in computational plant science.

</details>


### [99] [From Legal Texts to Defeasible Deontic Logic via LLMs: A Study in Automated Semantic Analysis](https://arxiv.org/abs/2506.08899)
*Elias Horner,Cristinel Mateis,Guido Governatori,Agata Ciabattoni*

Main category: cs.CL

TL;DR: 论文提出了一种利用大语言模型（LLMs）自动分析法律文本语义的新方法，并将其转化为可废止道义逻辑（DDL）的形式表示。


<details>
  <summary>Details</summary>
Motivation: 目标是解决法律文本的复杂性和规范性，通过自动化方法提高法律信息学的可扩展性。

Method: 采用结构化流程，将复杂法律语言分解为原子片段，提取道义规则，并评估其语法和语义一致性。方法包括多种LLM配置，如提示工程、微调模型和多阶段流程。

Result: 实验结果表明，机器生成的形式化表示与专家手工构建的结果具有较高一致性，特别是通过有效提示的LLMs表现突出。

Conclusion: LLMs在有效提示下能够显著提升法律信息学的可扩展性。

Abstract: We present a novel approach to the automated semantic analysis of legal texts
using large language models (LLMs), targeting their transformation into formal
representations in Defeasible Deontic Logic (DDL). We propose a structured
pipeline that segments complex normative language into atomic snippets,
extracts deontic rules, and evaluates them for syntactic and semantic
coherence. Our methodology is evaluated across various LLM configurations,
including prompt engineering strategies, fine-tuned models, and multi-stage
pipelines, focusing on legal norms from the Australian Telecommunications
Consumer Protections Code. Empirical results demonstrate promising alignment
between machine-generated and expert-crafted formalizations, showing that LLMs
- particularly when prompted effectively - can significantly contribute to
scalable legal informatics.

</details>


### [100] [Dialect Normalization using Large Language Models and Morphological Rules](https://arxiv.org/abs/2506.08907)
*Antonios Dimakis,John Pavlopoulos,Antonios Anastasopoulos*

Main category: cs.CL

TL;DR: 论文提出了一种结合规则和大型语言模型的方法，用于希腊方言到标准语言的转换，无需平行数据，并通过实验验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 解决低资源语言（包括高资源语言的方言）在自然语言理解系统中的困难，通过方言到标准语言的转换提升下游任务表现。

Method: 结合基于规则的语言学转换和大型语言模型（LLMs）的少样本提示，无需平行数据。

Result: 在希腊方言数据集上验证了方法的有效性，并发现以往研究仅依赖表面语言信息，而新方法能保留更多语义。

Conclusion: 该方法为方言标准化提供了有效途径，同时揭示了以往研究的局限性。

Abstract: Natural language understanding systems struggle with low-resource languages,
including many dialects of high-resource ones. Dialect-to-standard
normalization attempts to tackle this issue by transforming dialectal text so
that it can be used by standard-language tools downstream. In this study, we
tackle this task by introducing a new normalization method that combines
rule-based linguistically informed transformations and large language models
(LLMs) with targeted few-shot prompting, without requiring any parallel data.
We implement our method for Greek dialects and apply it on a dataset of
regional proverbs, evaluating the outputs using human annotators. We then use
this dataset to conduct downstream experiments, finding that previous results
regarding these proverbs relied solely on superficial linguistic information,
including orthographic artifacts, while new observations can still be made
through the remaining semantics.

</details>


### [101] [PropMEND: Hypernetworks for Knowledge Propagation in LLMs](https://arxiv.org/abs/2506.08920)
*Zeyu Leo Liu,Greg Durrett,Eunsol Choi*

Main category: cs.CL

TL;DR: PropMEND是一种基于超网络的知识传播方法，通过元学习修改梯度以促进注入知识的传播，显著提升了多跳问题的回答能力。


<details>
  <summary>Details</summary>
Motivation: 现有知识编辑技术无法支持基于注入知识的推理，限制了模型的应用范围。

Method: 采用超网络元学习方法，扩展MEND的元目标，修改梯度更新以支持多跳问题回答。

Result: 在RippleEdit数据集上表现优异，多跳问题准确率提升近2倍；在未见实体关系对上也优于现有方法。

Conclusion: PropMEND有效提升了知识传播能力，但在未见关系上的表现仍有改进空间，未来需进一步优化。

Abstract: Knowledge editing techniques for large language models (LLMs) can inject
knowledge that is later reproducible verbatim, but they fall short on
propagating that knowledge: models cannot answer questions that require
reasoning with the injected knowledge. We present a hypernetwork-based approach
for knowledge propagation, named PropMEND, where we meta-learn how to modify
gradients of a language modeling loss to encourage injected information to
propagate. Our approach extends the meta-objective of MEND [29] so that
gradient updates on knowledge are transformed to enable answering multi-hop
questions involving that knowledge. We show improved performance on the
RippleEdit dataset, showing almost 2x accuracy on challenging multi-hop
questions whose answers are not explicitly stated in the injected fact. We
further introduce a new dataset, Controlled RippleEdit, to evaluate the
generalization of our hypernetwork, testing knowledge propagation along
relations and entities unseen during hypernetwork training. PropMEND still
outperforms existing approaches in unseen entity-relation pairs, yet the
performance gap decreases substantially, suggesting future work in propagating
knowledge to a wide range of relations.

</details>


### [102] [Can A Gamer Train A Mathematical Reasoning Model?](https://arxiv.org/abs/2506.08935)
*Andrew Shin*

Main category: cs.CL

TL;DR: 论文提出了一种在单张普通游戏GPU（RTX 3080 Ti）上训练高性能数学推理模型的方法，通过结合强化学习和内存优化技术，实现了与更大模型相当的性能。


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型（LLMs）在数学推理等任务上表现优异，但其训练通常需要高昂的计算资源。本文旨在降低高性能AI研究的门槛，证明在资源受限环境下也能训练出高性能模型。

Method: 结合强化学习和内存优化技术，在单张16GB显存的RTX 3080 Ti上训练了一个1.5B参数的数学推理模型。

Result: 该模型在数学推理基准测试中表现优于或与更大模型相当，挑战了高性能数学推理需要大规模基础设施的范式。

Conclusion: 研究表明，高性能数学推理模型可以在资源受限的环境中训练，为更广泛的研究者提供了可能性。

Abstract: While large language models (LLMs) have achieved remarkable performance in
various tasks including mathematical reasoning, their development typically
demands prohibitive computational resources. Recent advancements have reduced
costs for training capable models, yet even these approaches rely on high-end
hardware clusters. In this paper, we demonstrate that a single average gaming
GPU can train a solid mathematical reasoning model, by integrating
reinforcement learning and memory optimization techniques. Specifically, we
train a 1.5B parameter mathematical reasoning model on RTX 3080 Ti of 16GB
memory that achieves comparable or better performance on mathematical reasoning
benchmarks than models several times larger, in resource-constrained
environments. Our results challenge the paradigm that state-of-the-art
mathematical reasoning necessitates massive infrastructure, democratizing
access to high-performance AI research.
https://github.com/shinandrew/YouronMath.

</details>


### [103] [FaithfulRAG: Fact-Level Conflict Modeling for Context-Faithful Retrieval-Augmented Generation](https://arxiv.org/abs/2506.08938)
*Qinggang Zhang,Zhishang Xiang,Yilin Xiao,Le Wang,Junhui Li,Xinrun Wang,Jinsong Su*

Main category: cs.CL

TL;DR: FaithfulRAG是一个新框架，通过显式建模LLM参数知识与检索上下文之间的差异，解决知识冲突问题，提升生成结果的忠实性。


<details>
  <summary>Details</summary>
Motivation: 现有基于检索的LLM在知识冲突时存在忠实性问题，现有方法通过压制模型参数知识实现忠实性，但损害了模型内部知识结构。

Method: FaithfulRAG在事实层面识别知识冲突，设计自我思考过程，让LLM在生成前推理并整合冲突事实。

Result: 实验表明，FaithfulRAG优于现有方法。

Conclusion: FaithfulRAG通过显式建模知识冲突，提升了生成结果的忠实性，同时保留了模型参数知识。

Abstract: Large language models (LLMs) augmented with retrieval systems have
demonstrated significant potential in handling knowledge-intensive tasks.
However, these models often struggle with unfaithfulness issues, generating
outputs that either ignore the retrieved context or inconsistently blend it
with the LLM`s parametric knowledge. This issue is particularly severe in cases
of knowledge conflict, where the retrieved context conflicts with the model`s
parametric knowledge. While existing faithful RAG approaches enforce strict
context adherence through well-designed prompts or modified decoding
strategies, our analysis reveals a critical limitation: they achieve
faithfulness by forcibly suppressing the model`s parametric knowledge, which
undermines the model`s internal knowledge structure and increases the risk of
misinterpreting the context. To this end, this paper proposes FaithfulRAG, a
novel framework that resolves knowledge conflicts by explicitly modeling
discrepancies between the model`s parametric knowledge and retrieved context.
Specifically, FaithfulRAG identifies conflicting knowledge at the fact level
and designs a self-thinking process, allowing LLMs to reason about and
integrate conflicting facts before generating responses. Extensive experiments
demonstrate that our method outperforms state-of-the-art methods. The code is
available at https:// github.com/DeepLearnXMU/Faithful-RAG

</details>


### [104] [Can LLMs Ground when they (Don't) Know: A Study on Direct and Loaded Political Questions](https://arxiv.org/abs/2506.08952)
*Clara Lachenmaier,Judith Sieker,Sina Zarrieß*

Main category: cs.CL

TL;DR: 论文研究了大型语言模型（LLMs）在政治领域中如何管理共同基础，尤其是在处理错误信息时的表现。


<details>
  <summary>Details</summary>
Motivation: 探讨LLMs在政治领域中对错误信息的处理能力，以评估其在缓解政治话语中错误信息方面的作用。

Method: 通过直接知识问题和预设错误信息的引导性问题，评估LLMs的回应能力及其政治偏见。

Result: 发现LLMs在纠正用户错误信念和建立共同基础方面存在显著挑战。

Conclusion: LLMs在政治话语中缓解错误信息的能力有限，需进一步改进。

Abstract: Communication among humans relies on conversational grounding, allowing
interlocutors to reach mutual understanding even when they do not have perfect
knowledge and must resolve discrepancies in each other's beliefs. This paper
investigates how large language models (LLMs) manage common ground in cases
where they (don't) possess knowledge, focusing on facts in the political domain
where the risk of misinformation and grounding failure is high. We examine the
ability of LLMs to answer direct knowledge questions and loaded questions that
presuppose misinformation. We evaluate whether loaded questions lead LLMs to
engage in active grounding and correct false user beliefs, in connection to
their level of knowledge and their political bias. Our findings highlight
significant challenges in LLMs' ability to engage in grounding and reject false
user beliefs, raising concerns about their role in mitigating misinformation in
political discourse.

</details>


### [105] [Pre-trained Language Models Learn Remarkably Accurate Representations of Numbers](https://arxiv.org/abs/2506.08966)
*Marek Kadlčík,Michal Štefánik,Timothee Mickus,Michal Spiegel,Josef Kuchař*

Main category: cs.CL

TL;DR: 本文提出了一种新的探测技术，能够从预训练语言模型的嵌入中高精度解码数值，证明了模型在预训练后对数字的精确表示能力，并发现这种精确性与模型在基础算术中的错误相关。


<details>
  <summary>Details</summary>
Motivation: 预训练语言模型在算术任务中表现不佳，现有方法未能有效探测模型嵌入中的数值信息，作者认为这是由于现有方法未能捕捉到嵌入中的正弦模式结构。

Method: 提出了一种新的探测技术，能够从输入嵌入中解码数值，并在多个开源语言模型上实现了近乎完美的准确性。

Result: 实验证明，预训练后的语言模型能够精确表示数字，且这种精确性与模型在基础算术中的错误密切相关。通过调整嵌入以匹配探测到的模式，可以显著减少这些错误。

Conclusion: 本文的新探测技术揭示了预训练语言模型对数字的精确表示能力，并提供了减少算术错误的方法。

Abstract: Pretrained language models (LMs) are prone to arithmetic errors. Existing
work showed limited success in probing numeric values from models'
representations, indicating that these errors can be attributed to the inherent
unreliability of distributionally learned embeddings in representing exact
quantities. However, we observe that previous probing methods are inadequate
for the emergent structure of learned number embeddings with sinusoidal
patterns.
  In response, we propose a novel probing technique that decodes numeric values
from input embeddings with near-perfect accuracy across a range of open-source
LMs. This proves that after the sole pre-training, LMs represent numbers with
remarkable precision. Finally, we find that the embeddings' preciseness judged
by our probe's accuracy explains a large portion of LM's errors in elementary
arithmetic, and show that aligning the embeddings with the pattern discovered
by our probe can mitigate these errors.

</details>


### [106] [Atomic-to-Compositional Generalization for Mobile Agents with A New Benchmark and Scheduling System](https://arxiv.org/abs/2506.08972)
*Yuan Guo,Tingjia Miao,Zheng Wu,Pengzhou Cheng,Ming Zhou,Zhuosheng Zhang*

Main category: cs.CL

TL;DR: 论文介绍了UI-NEXUS基准测试，用于评估移动代理在组合任务上的表现，并提出AGENT-NEXUS调度系统以提升任务成功率。


<details>
  <summary>Details</summary>
Motivation: 现有移动代理主要关注原子任务，而忽略了组合任务的重要性，UI-NEXUS旨在填补这一空白。

Method: 提出UI-NEXUS基准测试，包含三类组合操作任务，并设计AGENT-NEXUS调度系统动态分解任务。

Result: 现有代理在组合任务上表现不佳，AGENT-NEXUS显著提升了任务成功率（24%至40%）。

Conclusion: AGENT-NEXUS有效解决了组合任务的挑战，为移动代理的进一步发展提供了方向。

Abstract: Autonomous agents powered by multimodal large language models have been
developed to facilitate task execution on mobile devices. However, prior work
has predominantly focused on atomic tasks -- such as shot-chain execution tasks
and single-screen grounding tasks -- while overlooking the generalization to
compositional tasks, which are indispensable for real-world applications. This
work introduces UI-NEXUS, a comprehensive benchmark designed to evaluate mobile
agents on three categories of compositional operations: Simple Concatenation,
Context Transition, and Deep Dive. UI-NEXUS supports interactive evaluation in
20 fully controllable local utility app environments, as well as 30 online
Chinese and English service apps. It comprises 100 interactive task templates
with an average optimal step count of 14.05. Experimental results across a
range of mobile agents with agentic workflow or agent-as-a-model show that
UI-NEXUS presents significant challenges. Specifically, existing agents
generally struggle to balance performance and efficiency, exhibiting
representative failure modes such as under-execution, over-execution, and
attention drift, causing visible atomic-to-compositional generalization gap.
Inspired by these findings, we propose AGENT-NEXUS, a lightweight and efficient
scheduling system to tackle compositional mobile tasks. AGENT-NEXUS
extrapolates the abilities of existing mobile agents by dynamically decomposing
long-horizon tasks to a series of self-contained atomic subtasks. AGENT-NEXUS
achieves 24% to 40% task success rate improvement for existing mobile agents on
compositional operation tasks within the UI-NEXUS benchmark without
significantly sacrificing inference overhead. The demo video, dataset, and code
are available on the project page at https://ui-nexus.github.io.

</details>


### [107] [FROST-EMA: Finnish and Russian Oral Speech Dataset of Electromagnetic Articulography Measurements with L1, L2 and Imitated L2 Accents](https://arxiv.org/abs/2506.08981)
*Satu Hopponen,Tomi Kinnunen,Alexandre Nikolaev,Rosa González Hautamäki,Lauri Tavi,Einar Meister*

Main category: cs.CL

TL;DR: 介绍了FROST-EMA语料库，包含18名双语者的语音数据，用于研究语言变异性。


<details>
  <summary>Details</summary>
Motivation: 研究语言变异性，从语音和技术角度分析双语者的语音特征。

Method: 构建FROST-EMA语料库，包含L1、L2和模仿L2的语音数据，并进行两项初步案例研究。

Result: 案例研究展示了L2和模仿L2对说话人验证系统性能的影响，以及发音模式的差异。

Conclusion: FROST-EMA语料库为语言变异性的研究提供了新资源，并展示了其应用潜力。

Abstract: We introduce a new FROST-EMA (Finnish and Russian Oral Speech Dataset of
Electromagnetic Articulography) corpus. It consists of 18 bilingual speakers,
who produced speech in their native language (L1), second language (L2), and
imitated L2 (fake foreign accent). The new corpus enables research into
language variability from phonetic and technological points of view.
Accordingly, we include two preliminary case studies to demonstrate both
perspectives. The first case study explores the impact of L2 and imitated L2 on
the performance of an automatic speaker verification system, while the second
illustrates the articulatory patterns of one speaker in L1, L2, and a fake
accent.

</details>


### [108] [Naturalistic Language-related Movie-Watching fMRI Task for Detecting Neurocognitive Decline and Disorder](https://arxiv.org/abs/2506.08986)
*Yuejiao Wang,Xianmin Gong,Xixin Wu,Patrick Wong,Hoi-lam Helene Fung,Man Wai Mak,Helen Meng*

Main category: cs.CL

TL;DR: 该论文提出了一种基于自然语言任务的fMRI方法，用于早期检测老年认知衰退和神经认知障碍（NCD），并在97名非痴呆中国老年人中验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 早期检测对预防和延缓老年神经认知障碍（NCD）至关重要，语言相关的fMRI可能是一种有前景的检测方法。

Method: 研究设计了一种自然语言任务的fMRI方法，结合机器学习和人口统计学数据，对参与者认知状态进行分类。

Result: 分类模型的AUC达到0.86，特征定位显示与语言处理相关的脑区（如颞上回、颞中回和右小脑）是关键。

Conclusion: 自然语言任务的fMRI方法在早期检测老年认知衰退和NCD方面具有潜力。

Abstract: Early detection is crucial for timely intervention aimed at preventing and
slowing the progression of neurocognitive disorder (NCD), a common and
significant health problem among the aging population. Recent evidence has
suggested that language-related functional magnetic resonance imaging (fMRI)
may be a promising approach for detecting cognitive decline and early NCD. In
this paper, we proposed a novel, naturalistic language-related fMRI task for
this purpose. We examined the effectiveness of this task among 97 non-demented
Chinese older adults from Hong Kong. The results showed that machine-learning
classification models based on fMRI features extracted from the task and
demographics (age, gender, and education year) achieved an average area under
the curve of 0.86 when classifying participants' cognitive status (labeled as
NORMAL vs DECLINE based on their scores on a standard neurcognitive test).
Feature localization revealed that the fMRI features most frequently selected
by the data-driven approach came primarily from brain regions associated with
language processing, such as the superior temporal gyrus, middle temporal
gyrus, and right cerebellum. The study demonstrated the potential of the
naturalistic language-related fMRI task for early detection of aging-related
cognitive decline and NCD.

</details>


### [109] [Employing self-supervised learning models for cross-linguistic child speech maturity classification](https://arxiv.org/abs/2506.08999)
*Theo Zhang,Madurya Suresh,Anne S. Warlaumont,Kasia Hitczenko,Alejandrina Cristia,Margaret Cychosz*

Main category: cs.CL

TL;DR: 论文提出了一种名为SpeechMaturity的新数据集，用于改进儿童语音分类任务，并在多语言和多地区样本中验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 儿童语音技术系统因训练数据少和儿童语音复杂性而表现不佳，需要更有效的数据集和方法。

Method: 使用SpeechMaturity数据集训练Transformer模型，分类儿童发声（如哭声、笑声、成熟和不成熟语音）。

Result: 模型在新数据集上表现优于现有方法，分类准确率接近人类水平，且在不同环境中表现稳健。

Conclusion: SpeechMaturity数据集显著提升了儿童语音分类任务的性能，为相关研究提供了新工具。

Abstract: Speech technology systems struggle with many downstream tasks for child
speech due to small training corpora and the difficulties that child speech
pose. We apply a novel dataset, SpeechMaturity, to state-of-the-art transformer
models to address a fundamental classification task: identifying child
vocalizations. Unlike previous corpora, our dataset captures maximally
ecologically-valid child vocalizations across an unprecedented sample,
comprising children acquiring 25+ languages in the U.S., Bolivia, Vanuatu,
Papua New Guinea, Solomon Islands, and France. The dataset contains 242,004
labeled vocalizations, magnitudes larger than previous work. Models were
trained to distinguish between cry, laughter, mature (consonant+vowel), and
immature speech (just consonant or vowel). Models trained on the dataset
outperform state-of-the-art models trained on previous datasets, achieved
classification accuracy comparable to humans, and were robust across rural and
urban settings.

</details>


### [110] [SWE-Flow: Synthesizing Software Engineering Data in a Test-Driven Manner](https://arxiv.org/abs/2506.09003)
*Lei Zhang,Jiaxi Yang,Min Yang,Jian Yang,Mouxiang Chen,Jiajun Zhang,Zeyu Cui,Binyuan Hui,Junyang Lin*

Main category: cs.CL

TL;DR: SWE-Flow是一个基于测试驱动开发（TDD）的数据合成框架，通过单元测试自动推断增量开发步骤，生成结构化开发计划。


<details>
  <summary>Details</summary>
Motivation: 现有软件工程数据依赖人工提交的问题，SWE-Flow旨在通过单元测试自动生成开发步骤，提高数据生成效率。

Method: 构建运行时依赖图（RDG）以精确捕捉函数交互，生成逐步的开发计划，包括部分代码库、单元测试和代码修改。

Result: 从真实GitHub项目中生成16,061个训练实例和2,020个测试实例，实验显示基于此数据集微调的模型在TDD编码中表现显著提升。

Conclusion: SWE-Flow为TDD任务提供了高效的数据生成方法，并开源了所有资源以促进进一步研究。

Abstract: We introduce **SWE-Flow**, a novel data synthesis framework grounded in
Test-Driven Development (TDD). Unlike existing software engineering data that
rely on human-submitted issues, **SWE-Flow** automatically infers incremental
development steps directly from unit tests, which inherently encapsulate
high-level requirements. The core of **SWE-Flow** is the construction of a
Runtime Dependency Graph (RDG), which precisely captures function interactions,
enabling the generation of a structured, step-by-step *development schedule*.
At each step, **SWE-Flow** produces a partial codebase, the corresponding unit
tests, and the necessary code modifications, resulting in fully verifiable TDD
tasks. With this approach, we generated 16,061 training instances and 2,020
test instances from real-world GitHub projects, creating the **SWE-Flow-Eval**
benchmark. Our experiments show that fine-tuning open model on this dataset
significantly improves performance in TDD-based coding. To facilitate further
research, we release all code, datasets, models, and Docker images at
[Github](https://github.com/Hambaobao/SWE-Flow).

</details>


### [111] [UD-KSL Treebank v1.3: A semi-automated framework for aligning XPOS-extracted units with UPOS tags](https://arxiv.org/abs/2506.09009)
*Hakyung Sung,Gyu-Ho Shin,Chanyoung Lee,You Kyung Sung,Boo Kyung Jung*

Main category: cs.CL

TL;DR: 研究扩展了第二语言韩语的通用依存标注工作，提出半自动化框架，通过XPOS序列识别形态句法结构并与UPOS类别对齐。扩充了L2韩语语料库，评估对齐对模型性能的影响。


<details>
  <summary>Details</summary>
Motivation: 扩展L2韩语的标注工作，提升形态句法分析的准确性和一致性。

Method: 引入半自动化框架对齐XPOS与UPOS，扩充语料库，并通过两种NLP工具包评估对齐数据集的影响。

Result: 对齐数据集提高了标注层一致性，并提升了形态句法标注和依存分析的准确性，尤其在标注数据有限的情况下。

Conclusion: XPOS-UPOS对齐能显著提升L2韩语形态句法分析的性能，尤其在数据有限时效果更明显。

Abstract: The present study extends recent work on Universal Dependencies annotations
for second-language (L2) Korean by introducing a semi-automated framework that
identifies morphosyntactic constructions from XPOS sequences and aligns those
constructions with corresponding UPOS categories. We also broaden the existing
L2-Korean corpus by annotating 2,998 new sentences from argumentative essays.
To evaluate the impact of XPOS-UPOS alignments, we fine-tune L2-Korean
morphosyntactic analysis models on datasets both with and without these
alignments, using two NLP toolkits. Our results indicate that the aligned
dataset not only improves consistency across annotation layers but also
enhances morphosyntactic tagging and dependency-parsing accuracy, particularly
in cases of limited annotated data.

</details>


### [112] [Learning to Reason Across Parallel Samples for LLM Reasoning](https://arxiv.org/abs/2506.09014)
*Jianing Qi,Xi Ye,Hao Tang,Zhigang Zhu,Eunsol Choi*

Main category: cs.CL

TL;DR: 提出一种新方法SSA，通过训练小型LLM聚合多个样本答案，提升性能。


<details>
  <summary>Details</summary>
Motivation: 利用多样本集合提升LLM在数学领域的性能，但现有方法（如投票或验证器）效率有限。

Method: 训练小型LLM（SSA），将多个样本答案拼接后输入，通过强化学习优化答案准确性。

Result: SSA在多个推理数据集上优于其他测试时扩展方法，且泛化能力强。

Conclusion: SSA能高效利用黑盒模型输出，提升性能且通用性强。

Abstract: Scaling test-time compute brings substantial performance gains for large
language models (LLMs). By sampling multiple answers and heuristically
aggregate their answers (e.g., either through majority voting or using
verifiers to rank the answers), one can achieve consistent performance gains in
math domains. In this paper, we propose a new way to leverage such multiple
sample set. We train a compact LLM, called Sample Set Aggregator (SSA), that
takes a concatenated sequence of multiple samples and output the final answer,
optimizing it for the answer accuracy with reinforcement learning. Experiments
on multiple reasoning datasets show that SSA outperforms other test-time
scaling methods such as reward model-based re-ranking. Our approach also shows
a promising generalization ability, across sample set sizes, base model
families and scales, and tasks. By separating LLMs to generate answers and LLMs
to analyze and aggregate sampled answers, our approach can work with the
outputs from premier black box models easily and efficiently.

</details>


### [113] [Comparing human and LLM proofreading in L2 writing: Impact on lexical and syntactic features](https://arxiv.org/abs/2506.09021)
*Hakyung Sung,Karla Csuros,Min-Chang Sung*

Main category: cs.CL

TL;DR: 研究比较了人类和LLM（ChatGPT-4o、Llama3.1-8b、Deepseek-r1-8b）在第二语言写作校对中的词汇和句法干预效果，发现两者均能提升词汇连贯性，但LLM更倾向于生成性修改。


<details>
  <summary>Details</summary>
Motivation: 探讨人类与不同LLM在第二语言写作校对中的表现差异及其对文本可理解性的影响。

Method: 分析人类和三种LLM对相同第二语言写作的校对干预，评估词汇和句法特征的变化。

Result: 人类和LLM校对均提升了词汇连贯性，但LLM更倾向于使用多样化词汇和复杂句法结构。三种LLM的校对结果在主要特征上高度一致。

Conclusion: LLM校对在提升文本可理解性方面表现一致且生成性强，可能为第二语言写作提供高效支持。

Abstract: This study examines the lexical and syntactic interventions of human and LLM
proofreading aimed at improving overall intelligibility in identical second
language writings, and evaluates the consistency of outcomes across three LLMs
(ChatGPT-4o, Llama3.1-8b, Deepseek-r1-8b). Findings show that both human and
LLM proofreading enhance bigram lexical features, which may contribute to
better coherence and contextual connectedness between adjacent words. However,
LLM proofreading exhibits a more generative approach, extensively reworking
vocabulary and sentence structures, such as employing more diverse and
sophisticated vocabulary and incorporating a greater number of adjective
modifiers in noun phrases. The proofreading outcomes are highly consistent in
major lexical and syntactic features across the three models.

</details>


### [114] [Router-R1: Teaching LLMs Multi-Round Routing and Aggregation via Reinforcement Learning](https://arxiv.org/abs/2506.09033)
*Haozhen Zhang,Tao Feng,Jiaxuan You*

Main category: cs.CL

TL;DR: Router-R1是一个基于强化学习的框架，通过多轮决策动态分配和聚合多个LLM的能力，优化性能与成本。


<details>
  <summary>Details</summary>
Motivation: 现有LLM路由器仅支持单轮一对一映射，无法充分利用多个LLM的互补优势。

Method: Router-R1将路由和聚合建模为序列决策过程，利用LLM的推理能力动态调用模型，并通过轻量级规则奖励指导学习。

Result: 在七个基准测试中，Router-R1表现优于基线，实现了性能与成本的优化。

Conclusion: Router-R1通过强化学习实现了多LLM的动态路由，具有强泛化能力和成本管理优势。

Abstract: The rapid emergence of diverse large language models (LLMs) has spurred the
development of LLM routers that assign user queries to the most suitable model.
However, existing LLM routers typically perform a single-round, one-to-one
mapping (\textit{i.e.}, assigning each query to a single model in isolation),
which limits their capability to tackle complex tasks that demand the
complementary strengths of multiple LLMs. In this paper, we present
\textbf{Router-R1}, a reinforcement learning (RL)-based framework that
formulates multi-LLM routing and aggregation as a sequential decision process.
Router-R1 instantiates the router itself as a capable LLM, leveraging its
reasoning ability to interleave "think" actions (internal deliberation) with
"route" actions (dynamic model invocation), and integrates each response into
its evolving context. To guide learning, we employ a lightweight rule-based
reward comprising format rewards, final outcome rewards, and a novel cost
reward for performance and cost trade-off optimization, opening a pathway
toward optimizing performance-cost tradeoffs via RL. Router-R1 also conditions
only on simple model descriptors such as pricing, latency, and example
performance, enabling strong generalization to unseen model selection.
Experiments on seven general and multi-hop QA benchmarks show that Router-R1
outperforms over several strong baselines, achieving superior performance while
maintaining robust generalization and cost management.Code is available at
https://github.com/ulab-uiuc/Router-R1.

</details>


### [115] [Same Task, Different Circuits: Disentangling Modality-Specific Mechanisms in VLMs](https://arxiv.org/abs/2506.09047)
*Yaniv Nikankin,Dana Arad,Yossi Gandelsman,Yonatan Belinkov*

Main category: cs.CL

TL;DR: 研究发现视觉语言模型（VLM）在文本任务上表现优于视觉任务，通过分析模态间的计算子图（circuits）发现差异主要在数据处理层。通过将视觉数据的高层表征回传到低层，能缩小性能差距。


<details>
  <summary>Details</summary>
Motivation: 探索视觉语言模型在视觉和文本任务上的性能差异，并寻找缩小差距的方法。

Method: 比较不同模态的计算子图，分析数据处理差异，并提出将视觉数据的高层表征回传到低层的干预方法。

Result: 实验表明，该方法平均能缩小模态间性能差距的三分之一。

Conclusion: 揭示了多模态性能差距的原因，并提出了一种无需训练的改进方法。

Abstract: Vision-Language models (VLMs) show impressive abilities to answer questions
on visual inputs (e.g., counting objects in an image), yet demonstrate higher
accuracies when performing an analogous task on text (e.g., counting words in a
text). We investigate this accuracy gap by identifying and comparing the
\textit{circuits} - the task-specific computational sub-graphs - in different
modalities. We show that while circuits are largely disjoint between
modalities, they implement relatively similar functionalities: the differences
lie primarily in processing modality-specific data positions (an image or a
text sequence). Zooming in on the image data representations, we observe they
become aligned with the higher-performing analogous textual representations
only towards later layers, too late in processing to effectively influence
subsequent positions. To overcome this, we patch the representations of visual
data tokens from later layers back into earlier layers. In experiments with
multiple tasks and models, this simple intervention closes a third of the
performance gap between the modalities, on average. Our analysis sheds light on
the multi-modal performance gap in VLMs and suggests a training-free approach
for reducing it.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [116] [Towards Reliable AR-Guided Surgical Navigation: Interactive Deformation Modeling with Data-Driven Biomechanics and Prompts](https://arxiv.org/abs/2506.08048)
*Zheng Han,Jun Zhou,Jialun Pei,Jing Qin,Yingfang Fan,Qi Dou*

Main category: cs.CV

TL;DR: 提出了一种数据驱动的生物力学算法，结合人机交互机制，提升AR手术导航中变形建模的效率和准确性。


<details>
  <summary>Details</summary>
Motivation: 现有方法在术中变形建模中计算成本高且难以处理大范围解剖变化，导致AR导航不准确。

Method: 结合数据驱动的生物力学算法和人机交互机制，动态调整模型以适应复杂手术场景。

Result: 算法平均目标配准误差为3.42 mm，结合交互提示后降至2.78 mm，优于现有方法。

Conclusion: 该框架实现了高效准确的变形建模，并增强了医工协作，为计算机辅助手术提供了更安全可靠的解决方案。

Abstract: In augmented reality (AR)-guided surgical navigation, preoperative organ
models are superimposed onto the patient's intraoperative anatomy to visualize
critical structures such as vessels and tumors. Accurate deformation modeling
is essential to maintain the reliability of AR overlays by ensuring alignment
between preoperative models and the dynamically changing anatomy. Although the
finite element method (FEM) offers physically plausible modeling, its high
computational cost limits intraoperative applicability. Moreover, existing
algorithms often fail to handle large anatomical changes, such as those induced
by pneumoperitoneum or ligament dissection, leading to inaccurate anatomical
correspondences and compromised AR guidance. To address these challenges, we
propose a data-driven biomechanics algorithm that preserves FEM-level accuracy
while improving computational efficiency. In addition, we introduce a novel
human-in-the-loop mechanism into the deformation modeling process. This enables
surgeons to interactively provide prompts to correct anatomical misalignments,
thereby incorporating clinical expertise and allowing the model to adapt
dynamically to complex surgical scenarios. Experiments on a publicly available
dataset demonstrate that our algorithm achieves a mean target registration
error of 3.42 mm. Incorporating surgeon prompts through the interactive
framework further reduces the error to 2.78 mm, surpassing state-of-the-art
methods in volumetric accuracy. These results highlight the ability of our
framework to deliver efficient and accurate deformation modeling while
enhancing surgeon-algorithm collaboration, paving the way for safer and more
reliable computer-assisted surgeries.

</details>


### [117] [Context-aware TFL: A Universal Context-aware Contrastive Learning Framework for Temporal Forgery Localization](https://arxiv.org/abs/2506.08493)
*Qilin Yin,Wei Lu,Xiangyang Luo,Xiaochun Cao*

Main category: cs.CV

TL;DR: 论文提出了一种通用上下文感知对比学习框架（UniCaCLF），用于解决多媒体取证领域中部分视频片段篡改的时序伪造定位（TFL）问题。


<details>
  <summary>Details</summary>
Motivation: 现有研究多将深度伪造检测视为分类任务，忽略了部分视频片段被篡改的情况，而时序伪造定位更具现实应用价值。

Method: 采用监督对比学习，通过异常检测识别伪造片段，并提出上下文感知感知层和自适应上下文更新器，增强伪造片段特征的区分性。

Result: 在五个公开数据集上的实验表明，UniCaCLF显著优于现有算法。

Conclusion: UniCaCLF为时序伪造定位提供了一种高效且通用的解决方案。

Abstract: Most research efforts in the multimedia forensics domain have focused on
detecting forgery audio-visual content and reached sound achievements. However,
these works only consider deepfake detection as a classification task and
ignore the case where partial segments of the video are tampered with. Temporal
forgery localization (TFL) of small fake audio-visual clips embedded in real
videos is still challenging and more in line with realistic application
scenarios. To resolve this issue, we propose a universal context-aware
contrastive learning framework (UniCaCLF) for TFL. Our approach leverages
supervised contrastive learning to discover and identify forged instants by
means of anomaly detection, allowing for the precise localization of temporal
forged segments. To this end, we propose a novel context-aware perception layer
that utilizes a heterogeneous activation operation and an adaptive context
updater to construct a context-aware contrastive objective, which enhances the
discriminability of forged instant features by contrasting them with genuine
instant features in terms of their distances to the global context. An
efficient context-aware contrastive coding is introduced to further push the
limit of instant feature distinguishability between genuine and forged instants
in a supervised sample-by-sample manner, suppressing the cross-sample influence
to improve temporal forgery localization performance. Extensive experimental
results over five public datasets demonstrate that our proposed UniCaCLF
significantly outperforms the state-of-the-art competing algorithms.

</details>


### [118] [ReCogDrive: A Reinforced Cognitive Framework for End-to-End Autonomous Driving](https://arxiv.org/abs/2506.08052)
*Yongkang Li,Kaixin Xiong,Xiangyu Guo,Fang Li,Sixu Yan,Gangwei Xu,Lijun Zhou,Long Chen,Haiyang Sun,Bing Wang,Guang Chen,Hangjun Ye,Wenyu Liu,Xinggang Wang*

Main category: cs.CV

TL;DR: ReCogDrive提出了一种结合视觉语言模型（VLM）和扩散规划的自动驾驶系统，通过三阶段训练解决领域差距和动作空间不匹配问题，并在NAVSIM基准测试中取得最优性能。


<details>
  <summary>Details</summary>
Motivation: 解决端到端自动驾驶在罕见和长尾场景中性能下降的问题，同时克服现有方法中领域差距、动作空间不匹配和模仿学习的局限性。

Method: 采用三阶段训练：1）使用驾驶问答数据集训练VLM以减少领域差距；2）通过扩散规划器将语言空间映射到连续动作空间；3）利用强化学习在NAVSIM模拟器中微调规划器。

Result: 在NAVSIM基准测试中达到89.6 PDMS，超越之前视觉-only方法5.6 PDMS。

Conclusion: ReCogDrive通过结合VLM和扩散规划器，显著提升了自动驾驶在复杂场景中的性能，为未来研究提供了新方向。

Abstract: Although end-to-end autonomous driving has made remarkable progress, its
performance degrades significantly in rare and long-tail scenarios. Recent
approaches attempt to address this challenge by leveraging the rich world
knowledge of Vision-Language Models (VLMs), but these methods suffer from
several limitations: (1) a significant domain gap between the pre-training data
of VLMs and real-world driving data, (2) a dimensionality mismatch between the
discrete language space and the continuous action space, and (3) imitation
learning tends to capture the average behavior present in the dataset, which
may be suboptimal even dangerous. In this paper, we propose ReCogDrive, an
autonomous driving system that integrates VLMs with diffusion planner, which
adopts a three-stage paradigm for training. In the first stage, we use a
large-scale driving question-answering datasets to train the VLMs, mitigating
the domain discrepancy between generic content and real-world driving
scenarios. In the second stage, we employ a diffusion-based planner to perform
imitation learning, mapping representations from the latent language space to
continuous driving actions. Finally, we fine-tune the diffusion planner using
reinforcement learning with NAVSIM non-reactive simulator, enabling the model
to generate safer, more human-like driving trajectories. We evaluate our
approach on the planning-oriented NAVSIM benchmark, achieving a PDMS of 89.6
and setting a new state-of-the-art that surpasses the previous vision-only SOTA
by 5.6 PDMS.

</details>


### [119] [CuRe: Cultural Gaps in the Long Tail of Text-to-Image Systems](https://arxiv.org/abs/2506.08071)
*Aniket Rege,Zinnia Nie,Mahesh Ramesh,Unmesh Raskar,Zhuoran Yu,Aditya Kusupati,Yong Jae Lee,Ramya Korlakai Vinayak*

Main category: cs.CV

TL;DR: CuRe是一个用于评估文本到图像（T2I）系统文化代表性的新基准和评分套件，通过属性规范的边际效用作为人类判断的代理。


<details>
  <summary>Details</summary>
Motivation: 现有T2I系统训练数据偏向欧美文化，忽视了全球南方的文化多样性，CuRe旨在量化并分析这种偏见。

Method: 利用Wikimedia知识图谱构建层次化数据集，包含300个文化物品和32个子类别，通过分析T2I系统对文本条件信息增加的响应来评估文化代表性。

Result: CuRe评分器在多种图像编码器、视觉语言模型和T2I系统中，与人类对感知相似性、图文对齐和文化多样性的判断高度相关。

Conclusion: CuRe为T2I系统的文化多样性评估提供了可扩展且细粒度的工具，数据集和代码已开源。

Abstract: Popular text-to-image (T2I) systems are trained on web-scraped data, which is
heavily Amero and Euro-centric, underrepresenting the cultures of the Global
South. To analyze these biases, we introduce CuRe, a novel and scalable
benchmarking and scoring suite for cultural representativeness that leverages
the marginal utility of attribute specification to T2I systems as a proxy for
human judgments. Our CuRe benchmark dataset has a novel categorical hierarchy
built from the crowdsourced Wikimedia knowledge graph, with 300 cultural
artifacts across 32 cultural subcategories grouped into six broad cultural axes
(food, art, fashion, architecture, celebrations, and people). Our dataset's
categorical hierarchy enables CuRe scorers to evaluate T2I systems by analyzing
their response to increasing the informativeness of text conditioning, enabling
fine-grained cultural comparisons. We empirically observe much stronger
correlations of our class of scorers to human judgments of perceptual
similarity, image-text alignment, and cultural diversity across image encoders
(SigLIP 2, AIMV2 and DINOv2), vision-language models (OpenCLIP, SigLIP 2,
Gemini 2.0 Flash) and state-of-the-art text-to-image systems, including three
variants of Stable Diffusion (1.5, XL, 3.5 Large), FLUX.1 [dev], Ideogram 2.0,
and DALL-E 3. The code and dataset is open-sourced and available at
https://aniketrege.github.io/cure/.

</details>


### [120] [Diversity-Guided MLP Reduction for Efficient Large Vision Transformers](https://arxiv.org/abs/2506.08591)
*Chengchao Shen,Hourun Zhu,Gongfan Fang,Jianxin Wang,Xinchao Wang*

Main category: cs.CV

TL;DR: 论文提出了一种多样性引导的MLP压缩方法（DGMR），显著减少大型视觉Transformer的参数和计算量，同时保持性能。


<details>
  <summary>Details</summary>
Motivation: 大型Transformer模型参数过多导致计算和内存成本高昂，研究发现MLP模块占用了大部分参数。

Method: 采用Gram-Schmidt权重剪枝策略消除MLP隐藏层的冗余神经元，同时保留权重多样性以支持性能恢复。

Result: 在多个先进视觉Transformer上，DGMR实现了57%以上的参数和FLOPs减少，性能损失极小。EVA-CLIP-E模型参数和FLOPs减少71.5%，性能无下降。

Conclusion: DGMR是一种高效且近乎无损的模型压缩方法，适用于大规模视觉Transformer。

Abstract: Transformer models achieve excellent scaling property, where the performance
is improved with the increment of model capacity. However, large-scale model
parameters lead to an unaffordable cost of computing and memory. We analyze
popular transformer architectures and find that multilayer perceptron (MLP)
modules take up the majority of model parameters. To this end, we focus on the
recoverability of the compressed models and propose a Diversity-Guided MLP
Reduction (DGMR) method to significantly reduce the parameters of large vision
transformers with only negligible performance degradation. Specifically, we
conduct a Gram-Schmidt weight pruning strategy to eliminate redundant neurons
of MLP hidden layer, while preserving weight diversity for better performance
recover during distillation. Compared to the model trained from scratch, our
pruned model only requires 0.06\% data of LAION-2B (for the training of large
vision transformers) without labels (ImageNet-1K) to recover the original
performance. Experimental results on several state-of-the-art large vision
transformers demonstrate that our method achieves a more than 57.0\% parameter
and FLOPs reduction in a near lossless manner. Notably, for EVA-CLIP-E (4.4B),
our method accomplishes a 71.5\% parameter and FLOPs reduction without
performance degradation. The source code and trained weights are available at
https://github.com/visresearch/DGMR.

</details>


### [121] [IGraSS: Learning to Identify Infrastructure Networks from Satellite Imagery by Iterative Graph-constrained Semantic Segmentation](https://arxiv.org/abs/2506.08137)
*Oishee Bintey Hoque,Abhijin Adiga,Aniruddha Adiga,Siddharth Chaudhary,Madhav V. Marathe,S. S. Ravi,Kirti Rajagopalan,Amanda Wilson,Samarth Swarup*

Main category: cs.CV

TL;DR: IGraSS是一个结合语义分割和图优化的框架，用于改进不完整的地面真实数据，显著提升运河网络映射的准确性。


<details>
  <summary>Details</summary>
Motivation: 现有语义分割模型依赖高质量标注数据，但实际中地面真实数据常不完整，影响模型性能。利用基础设施网络的图级属性（如可达性）可以改进数据。

Method: 提出IGraSS框架，结合RGB和多模态数据（NDWI、DEM）的语义分割模块与基于图的真实数据优化模块。

Result: 实验显示，IGraSS将不可达运河段从18%降至3%，优化后的数据显著提升运河识别效果。

Conclusion: IGraSS是一个通用框架，适用于改进噪声数据和映射基础设施网络，如运河和道路。

Abstract: Accurate canal network mapping is essential for water management, including
irrigation planning and infrastructure maintenance. State-of-the-art semantic
segmentation models for infrastructure mapping, such as roads, rely on large,
well-annotated remote sensing datasets. However, incomplete or inadequate
ground truth can hinder these learning approaches. Many infrastructure networks
have graph-level properties such as reachability to a source (like canals) or
connectivity (roads) that can be leveraged to improve these existing ground
truth. This paper develops a novel iterative framework IGraSS, combining a
semantic segmentation module-incorporating RGB and additional modalities (NDWI,
DEM)-with a graph-based ground-truth refinement module. The segmentation module
processes satellite imagery patches, while the refinement module operates on
the entire data viewing the infrastructure network as a graph. Experiments show
that IGraSS reduces unreachable canal segments from around 18% to 3%, and
training with refined ground truth significantly improves canal identification.
IGraSS serves as a robust framework for both refining noisy ground truth and
mapping canal networks from remote sensing imagery. We also demonstrate the
effectiveness and generalizability of IGraSS using road networks as an example,
applying a different graph-theoretic constraint to complete road networks.

</details>


### [122] [Spectral Domain Neural Reconstruction for Passband FMCW Radars](https://arxiv.org/abs/2506.08163)
*Harshvardhan Takawale,Nirupam Roy*

Main category: cs.CV

TL;DR: SpINRv2是一个基于神经网络的框架，用于通过FMCW雷达实现高保真体积重建，改进了前作SpINR，解决了高频下的相位混叠和子区间模糊问题。


<details>
  <summary>Details</summary>
Motivation: 高频FMCW雷达在体积重建中面临相位混叠和子区间模糊的挑战，需要一种更精确的方法来解决这些问题。

Method: 提出了一种完全可微的频率域前向模型，结合隐式神经表示（INR）进行连续体积建模，并引入稀疏性和平滑性正则化。

Result: SpINRv2在高频场景下显著优于传统和基于学习的方法，成为神经雷达3D成像的新基准。

Conclusion: SpINRv2通过频率域建模和正则化技术，成功解决了高频雷达重建中的关键问题，提升了性能。

Abstract: We present SpINRv2, a neural framework for high-fidelity volumetric
reconstruction using Frequency-Modulated Continuous-Wave (FMCW) radar.
Extending our prior work (SpINR), this version introduces enhancements that
allow accurate learning under high start frequencies-where phase aliasing and
sub-bin ambiguity become prominent. Our core contribution is a fully
differentiable frequency-domain forward model that captures the complex radar
response using closed-form synthesis, paired with an implicit neural
representation (INR) for continuous volumetric scene modeling. Unlike
time-domain baselines, SpINRv2 directly supervises the complex frequency
spectrum, preserving spectral fidelity while drastically reducing computational
overhead. Additionally, we introduce sparsity and smoothness regularization to
disambiguate sub-bin ambiguities that arise at fine range resolutions.
Experimental results show that SpINRv2 significantly outperforms both classical
and learning-based baselines, especially under high-frequency regimes,
establishing a new benchmark for neural radar-based 3D imaging.

</details>


### [123] [Surgeon Style Fingerprinting and Privacy Risk Quantification via Discrete Diffusion Models in a Vision-Language-Action Framework](https://arxiv.org/abs/2506.08185)
*Huixin Zhan,Jason H. Moore*

Main category: cs.CV

TL;DR: 论文提出了一种基于离散扩散框架和视觉-语言-动作（VLA）管道的个性化外科医生指纹建模方法，用于机器人手术中的手势预测。


<details>
  <summary>Details</summary>
Motivation: 当前AI系统常忽略外科医生的个性化操作风格，而本文旨在捕捉这种个性化信号。

Method: 采用离散扩散框架，结合多模态输入（内窥镜视频、手术意图语言、隐私感知的医生身份嵌入），通过自然语言提示编码个性化指纹。

Result: 在JIGSAWS数据集上验证了方法能准确重建手势序列并学习独特的医生运动指纹，但个性化嵌入会增加身份泄露风险。

Conclusion: 个性化嵌入虽提升性能，但也增加隐私风险，需在手术建模中平衡两者。

Abstract: Surgeons exhibit distinct operating styles due to differences in training,
experience, and motor behavior - yet current AI systems often ignore this
personalization signal. We propose a novel approach to model fine-grained,
surgeon-specific fingerprinting in robotic surgery using a discrete diffusion
framework integrated with a vision-language-action (VLA) pipeline. Our method
formulates gesture prediction as a structured sequence denoising task,
conditioned on multimodal inputs including endoscopic video, surgical intent
language, and a privacy-aware embedding of surgeon identity and skill.
Personalized surgeon fingerprinting is encoded through natural language prompts
using third-party language models, allowing the model to retain individual
behavioral style without exposing explicit identity. We evaluate our method on
the JIGSAWS dataset and demonstrate that it accurately reconstructs gesture
sequences while learning meaningful motion fingerprints unique to each surgeon.
To quantify the privacy implications of personalization, we perform membership
inference attacks and find that more expressive embeddings improve task
performance but simultaneously increase susceptibility to identity leakage.
These findings demonstrate that while personalized embeddings improve
performance, they also increase vulnerability to identity leakage, revealing
the importance of balancing personalization with privacy risk in surgical
modeling. Code is available at:
https://github.com/huixin-zhan-ai/Surgeon_style_fingerprinting.

</details>


### [124] [Open World Scene Graph Generation using Vision Language Models](https://arxiv.org/abs/2506.08189)
*Amartya Dutta,Kazi Sajeed Mehrab,Medha Sawhney,Abhilash Neog,Mridul Khurana,Sepideh Fatemi,Aanish Pradhan,M. Maruf,Ismini Lourentzou,Arka Daw,Anuj Karpatne*

Main category: cs.CV

TL;DR: 论文提出了一种无需训练的开放世界场景图生成方法，利用预训练视觉语言模型实现零样本推理。


<details>
  <summary>Details</summary>
Motivation: 现有场景图生成方法依赖数据集特定监督，难以适应开放世界中的新对象和关系。

Method: 结合多模态提示、嵌入对齐和轻量级配对优化策略，实现零样本结构化推理。

Result: 在Visual Genome、Open Images V6和PSG数据集上验证了预训练模型的零样本关系理解能力。

Conclusion: 预训练视觉语言模型无需任务级训练即可实现开放世界场景图生成。

Abstract: Scene-Graph Generation (SGG) seeks to recognize objects in an image and
distill their salient pairwise relationships. Most methods depend on
dataset-specific supervision to learn the variety of interactions, restricting
their usefulness in open-world settings, involving novel objects and/or
relations. Even methods that leverage large Vision Language Models (VLMs)
typically require benchmark-specific fine-tuning. We introduce Open-World SGG,
a training-free, efficient, model-agnostic framework that taps directly into
the pretrained knowledge of VLMs to produce scene graphs with zero additional
learning. Casting SGG as a zero-shot structured-reasoning problem, our method
combines multimodal prompting, embedding alignment, and a lightweight
pair-refinement strategy, enabling inference over unseen object vocabularies
and relation sets. To assess this setting, we formalize an Open-World
evaluation protocol that measures performance when no SGG-specific data have
been observed either in terms of objects and relations. Experiments on Visual
Genome, Open Images V6, and the Panoptic Scene Graph (PSG) dataset demonstrate
the capacity of pretrained VLMs to perform relational understanding without
task-level training.

</details>


### [125] [Generative Learning of Differentiable Object Models for Compositional Interpretation of Complex Scenes](https://arxiv.org/abs/2506.08191)
*Antoni Nowinowski,Krzysztof Krawiec*

Main category: cs.CV

TL;DR: 扩展了DVP架构，使其能处理多对象场景，并通过潜在空间采样和多种训练模式提升训练效果。


<details>
  <summary>Details</summary>
Motivation: 解决DVP在处理多对象场景时的局限性，并优化训练过程。

Method: 扩展DVP架构，引入潜在空间采样和多训练模式，使用图像和潜在空间损失函数。

Result: 在重建质量和对象分解能力上优于基线模型（MONet和LIVE）。

Conclusion: 扩展后的DVP在多对象场景中表现优异，但可微分渲染在自编码器中仍有局限性。

Abstract: This study builds on the architecture of the Disentangler of Visual Priors
(DVP), a type of autoencoder that learns to interpret scenes by decomposing the
perceived objects into independent visual aspects of shape, size, orientation,
and color appearance. These aspects are expressed as latent parameters which
control a differentiable renderer that performs image reconstruction, so that
the model can be trained end-to-end with gradient using reconstruction loss. In
this study, we extend the original DVP so that it can handle multiple objects
in a scene. We also exploit the interpretability of its latent by using the
decoder to sample additional training examples and devising alternative
training modes that rely on loss functions defined not only in the image space,
but also in the latent space. This significantly facilitates training, which is
otherwise challenging due to the presence of extensive plateaus in the
image-space reconstruction loss. To examine the performance of this approach,
we propose a new benchmark featuring multiple 2D objects, which subsumes the
previously proposed Multi-dSprites dataset while being more parameterizable. We
compare the DVP extended in these ways with two baselines (MONet and LIVE) and
demonstrate its superiority in terms of reconstruction quality and capacity to
decompose overlapping objects. We also analyze the gradients induced by the
considered loss functions, explain how they impact the efficacy of training,
and discuss the limitations of differentiable rendering in autoencoders and the
ways in which they can be addressed.

</details>


### [126] [GIQ: Benchmarking 3D Geometric Reasoning of Vision Foundation Models with Simulated and Real Polyhedra](https://arxiv.org/abs/2506.08194)
*Mateusz Michalkiewicz,Anekha Sokhal,Tadeusz Michalkiewicz,Piotr Pawlikowski,Mahsa Baktashmotlagh,Varun Jampani,Guha Balakrishnan*

Main category: cs.CV

TL;DR: GIQ是一个评估视觉和视觉语言基础模型几何推理能力的综合性基准，揭示了当前模型在几何理解上的显著不足。


<details>
  <summary>Details</summary>
Motivation: 现有方法在标准基准上表现优异，但对几何属性的真实理解尚不明确，因此需要专门的评估工具。

Method: GIQ包含224种多面体的合成和真实图像，通过单目3D重建、对称性检测、心理旋转测试和零样本分类任务进行系统实验。

Result: 当前模型在基础几何形状重建、详细几何区分任务中表现不佳，视觉语言助手对复杂多面体的理解准确率极低。

Conclusion: GIQ为几何智能研究提供了结构化平台，有助于未来几何感知表示学习的进展。

Abstract: Monocular 3D reconstruction methods and vision-language models (VLMs)
demonstrate impressive results on standard benchmarks, yet their true
understanding of geometric properties remains unclear. We introduce GIQ , a
comprehensive benchmark specifically designed to evaluate the geometric
reasoning capabilities of vision and vision-language foundation models. GIQ
comprises synthetic and real-world images of 224 diverse polyhedra - including
Platonic, Archimedean, Johnson, and Catalan solids, as well as stellations and
compound shapes - covering varying levels of complexity and symmetry. Through
systematic experiments involving monocular 3D reconstruction, 3D symmetry
detection, mental rotation tests, and zero-shot shape classification tasks, we
reveal significant shortcomings in current models. State-of-the-art
reconstruction algorithms trained on extensive 3D datasets struggle to
reconstruct even basic geometric forms accurately. While foundation models
effectively detect specific 3D symmetry elements via linear probing, they
falter significantly in tasks requiring detailed geometric differentiation,
such as mental rotation. Moreover, advanced vision-language assistants exhibit
remarkably low accuracy on complex polyhedra, systematically misinterpreting
basic properties like face geometry, convexity, and compound structures. GIQ is
publicly available, providing a structured platform to highlight and address
critical gaps in geometric intelligence, facilitating future progress in
robust, geometry-aware representation learning.

</details>


### [127] [A Comprehensive Study of Decoder-Only LLMs for Text-to-Image Generation](https://arxiv.org/abs/2506.08210)
*Andrew Z. Wang,Songwei Ge,Tero Karras,Ming-Yu Liu,Yogesh Balaji*

Main category: cs.CV

TL;DR: 研究探讨了使用现代仅解码器LLMs作为文本编码器在文本到图像扩散模型中的效果，发现多层归一化平均嵌入优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 现有文本到图像模型仍使用过时的T5和CLIP作为文本编码器，研究旨在评估现代LLMs的潜力。

Method: 构建标准化训练和评估流程，训练27个模型，分析12种文本编码器，探索嵌入提取方法、LLMs变体和模型大小的影响。

Result: 多层归一化平均嵌入显著提升复杂提示的对齐效果，多数LLMs表现优于T5基线。

Conclusion: 现代LLMs作为文本编码器在文本到图像生成中具有优势，尤其是多层嵌入提取方法。

Abstract: Both text-to-image generation and large language models (LLMs) have made
significant advancements. However, many text-to-image models still employ the
somewhat outdated T5 and CLIP as their text encoders. In this work, we
investigate the effectiveness of using modern decoder-only LLMs as text
encoders for text-to-image diffusion models. We build a standardized training
and evaluation pipeline that allows us to isolate and evaluate the effect of
different text embeddings. We train a total of 27 text-to-image models with 12
different text encoders to analyze the critical aspects of LLMs that could
impact text-to-image generation, including the approaches to extract
embeddings, different LLMs variants, and model sizes. Our experiments reveal
that the de facto way of using last-layer embeddings as conditioning leads to
inferior performance. Instead, we explore embeddings from various layers and
find that using layer-normalized averaging across all layers significantly
improves alignment with complex prompts. Most LLMs with this conditioning
outperform the baseline T5 model, showing enhanced performance in advanced
visio-linguistic reasoning skills.

</details>


### [128] [Using Satellite Images And Self-supervised Machine Learning Networks To Detect Water Hidden Under Vegetation](https://arxiv.org/abs/2506.08214)
*Ioannis Iakovidis,Zahra Kalantari,Amir Hossein Payberah,Fernando Jaramillo,Francisco Pena Escobar*

Main category: cs.CV

TL;DR: 论文提出了一种结合深度聚类和负采样的自监督方法，用于雷达卫星图像的水陆分割，无需人工标注，并通过集成模型提升性能。


<details>
  <summary>Details</summary>
Motivation: 解决传统模型依赖大量人工标注数据的问题，降低标注成本和时间。

Method: 结合深度聚类和负采样进行自监督训练，并采用集成模型减少方差。

Result: 自监督集成模型在测试集上的IOU指标比全监督模型提升了0.02。

Conclusion: 自监督方法在减少标注依赖的同时，能有效提升模型性能。

Abstract: In recent years the wide availability of high-resolution radar satellite
images along with the advancement of computer vision models have enabled the
remote monitoring of the surface area of wetlands. However, these models
require large amounts of manually annotated satellite images, which are slow
and expensive to produce. To overcome this problem, self-supervised training
methods have been deployed to train models without using annotated data. In
this paper we use a combination of deep clustering and negative sampling to
train a model to segment radar satellite images into areas that separate water
from land without the use of any manual annotations. Furthermore, we implement
an ensemble version of the model to reduce variance and improve performance.
Compared to a single fully-supervised model using the same architecture, our
ensemble of self-supervised models achieves a 0.02 improvement in the
Intersection Over Union metric over our test dataset.

</details>


### [129] [Jamais Vu: Exposing the Generalization Gap in Supervised Semantic Correspondence](https://arxiv.org/abs/2506.08220)
*Octave Mariotti,Zhipeng Du,Yash Bhalgat,Oisin Mac Aodha,Hakan Bilen*

Main category: cs.CV

TL;DR: 论文提出了一种通过单目深度估计将2D关键点提升到3D空间的新方法，以学习密集语义对应关系，并在未见关键点上显著优于监督基线。


<details>
  <summary>Details</summary>
Motivation: 现有监督语义对应方法局限于稀疏标注的关键点，泛化能力不足，无法有效学习密集对应关系。

Method: 通过单目深度估计将2D关键点映射到3D空间，构建连续规范流形，无需显式3D监督或相机标注。

Result: 模型在未见关键点上显著优于监督基线，且无监督基线在跨数据集泛化中表现更优。

Conclusion: 提出的方法能有效学习鲁棒的密集语义对应关系，且在泛化能力上优于现有监督方法。

Abstract: Semantic correspondence (SC) aims to establish semantically meaningful
matches across different instances of an object category. We illustrate how
recent supervised SC methods remain limited in their ability to generalize
beyond sparsely annotated training keypoints, effectively acting as keypoint
detectors. To address this, we propose a novel approach for learning dense
correspondences by lifting 2D keypoints into a canonical 3D space using
monocular depth estimation. Our method constructs a continuous canonical
manifold that captures object geometry without requiring explicit 3D
supervision or camera annotations. Additionally, we introduce SPair-U, an
extension of SPair-71k with novel keypoint annotations, to better assess
generalization. Experiments not only demonstrate that our model significantly
outperforms supervised baselines on unseen keypoints, highlighting its
effectiveness in learning robust correspondences, but that unsupervised
baselines outperform supervised counterparts when generalized across different
datasets.

</details>


### [130] [A Good CREPE needs more than just Sugar: Investigating Biases in Compositional Vision-Language Benchmarks](https://arxiv.org/abs/2506.08227)
*Vishaal Udandarao,Mehdi Cherti,Shyamgopal Karthik,Jenia Jitsev,Samuel Albanie,Matthias Bethge*

Main category: cs.CV

TL;DR: 论文分析了17个用于评估视觉语言模型组合理解能力的基准测试，发现其设计存在偏差，导致简单启发式方法与CLIP模型表现相当。作者提出了改进建议以构建更鲁棒的基准。


<details>
  <summary>Details</summary>
Motivation: 研究动机是揭示现有视觉语言组合理解基准测试的设计缺陷，尤其是数据来源和构建过程中引入的偏差，导致其无法有效衡量模型的真实能力。

Method: 方法包括对17个基准测试的设计选择（如数据来源、负样本构建）进行详细分析，并评估简单启发式方法与CLIP模型的性能对比。

Result: 结果显示基准测试存在分布不对称性，导致简单启发式方法表现与CLIP相当，表明其未能有效衡量组合理解能力。

Conclusion: 结论是提出了改进基准测试设计的建议，以减少偏差并增强鲁棒性。

Abstract: We investigate 17 benchmarks (e.g. SugarCREPE, VALSE) commonly used for
measuring compositional understanding capabilities of vision-language models
(VLMs). We scrutinize design choices in their construction, including data
source (e.g. MS-COCO) and curation procedures (e.g. constructing negative
images/captions), uncovering several inherent biases across most benchmarks. We
find that blind heuristics (e.g. token-length, log-likelihood under a language
model) perform on par with CLIP models, indicating that these benchmarks do not
effectively measure compositional understanding. We demonstrate that the
underlying factor is a distribution asymmetry between positive and negative
images/captions, induced by the benchmark construction procedures. To mitigate
these issues, we provide a few key recommendations for constructing more robust
vision-language compositional understanding benchmarks, that would be less
prone to such simple attacks.

</details>


### [131] [Highly Compressed Tokenizer Can Generate Without Training](https://arxiv.org/abs/2506.08257)
*L. Lao Beyer,T. Li,X. Chen,S. Karaman,K. He*

Main category: cs.CV

TL;DR: 1D图像分词器通过高度压缩的一维序列表示图像，支持通过启发式操作实现图像编辑和生成能力。


<details>
  <summary>Details</summary>
Motivation: 探索1D图像分词器在高度压缩下仍能实现精细图像编辑和生成的潜力。

Method: 利用向量量化的1D分词器，通过梯度优化的测试时令牌操作，结合重建或CLIP相似性损失函数。

Result: 无需训练生成模型即可实现多样且真实的图像生成和编辑。

Conclusion: 1D图像分词器在图像编辑和生成中表现出高效性和灵活性。

Abstract: Commonly used image tokenizers produce a 2D grid of spatially arranged
tokens. In contrast, so-called 1D image tokenizers represent images as highly
compressed one-dimensional sequences of as few as 32 discrete tokens. We find
that the high degree of compression achieved by a 1D tokenizer with vector
quantization enables image editing and generative capabilities through
heuristic manipulation of tokens, demonstrating that even very crude
manipulations -- such as copying and replacing tokens between latent
representations of images -- enable fine-grained image editing by transferring
appearance and semantic attributes. Motivated by the expressivity of the 1D
tokenizer's latent space, we construct an image generation pipeline leveraging
gradient-based test-time optimization of tokens with plug-and-play loss
functions such as reconstruction or CLIP similarity. Our approach is
demonstrated for inpainting and text-guided image editing use cases, and can
generate diverse and realistic samples without requiring training of any
generative model.

</details>


### [132] [Seeing Voices: Generating A-Roll Video from Audio with Mirage](https://arxiv.org/abs/2506.08279)
*Aditi Sundararaman,Amogh Adishesha,Andrew Jaegle,Dan Bigioi,Hyoung-Kyu Song,Jon Kyl,Justin Mao,Kevin Lan,Mojtaba Komeili,ShahRukh Athar,Sheila Babayan,Stanislau Beliasau,William Buchwalter*

Main category: cs.CV

TL;DR: Mirage是一种音频到视频的基础模型，能够根据音频输入生成逼真、富有表现力的视频内容。


<details>
  <summary>Details</summary>
Motivation: 当前视频生成方法要么忽略声音专注于无声图像序列生成，要么局限于特定应用领域（如重新配音）。Mirage旨在解决这一问题，实现音频与视频的和谐集成。

Method: Mirage采用基于自注意力的统一训练方法，支持从零开始训练或基于现有权重训练，生成高质量视频。

Result: Mirage生成的视频在主观质量上优于其他方法，能够逼真地呈现音频中的表演内容。

Conclusion: Mirage为音频到视频生成提供了一种通用且高质量的方法，尤其在结合语音合成技术时表现突出。

Abstract: From professional filmmaking to user-generated content, creators and
consumers have long recognized that the power of video depends on the
harmonious integration of what we hear (the video's audio track) with what we
see (the video's image sequence). Current approaches to video generation either
ignore sound to focus on general-purpose but silent image sequence generation
or address both visual and audio elements but focus on restricted application
domains such as re-dubbing. We introduce Mirage, an audio-to-video foundation
model that excels at generating realistic, expressive output imagery from
scratch given an audio input. When integrated with existing methods for speech
synthesis (text-to-speech, or TTS), Mirage results in compelling multimodal
video. When trained on audio-video footage of people talking (A-roll) and
conditioned on audio containing speech, Mirage generates video of people
delivering a believable interpretation of the performance implicit in input
audio. Our central technical contribution is a unified method for training
self-attention-based audio-to-video generation models, either from scratch or
given existing weights. This methodology allows Mirage to retain generality as
an approach to audio-to-video generation while producing outputs of superior
subjective quality to methods that incorporate audio-specific architectures or
loss components specific to people, speech, or details of how images or audio
are captured. We encourage readers to watch and listen to the results of Mirage
for themselves (see paper and comments for links).

</details>


### [133] [Towards Cross-Subject EMG Pattern Recognition via Dual-Branch Adversarial Feature Disentanglement](https://arxiv.org/abs/2506.08555)
*Xinyue Niu,Akira Furui*

Main category: cs.CV

TL;DR: 论文提出了一种通过特征解耦消除校准需求的跨主题肌电图（EMG）模式识别方法，使用双分支对抗神经网络实现。


<details>
  <summary>Details</summary>
Motivation: 跨主题EMG模式识别因个体差异面临挑战，传统方法依赖用户校准数据，耗时且不实用。

Method: 提出端到端双分支对抗神经网络，将EMG特征解耦为模式特定和主题特定组件。

Result: 实验表明，模型在未见用户数据上表现优异，优于基线方法。

Conclusion: 研究为无需校准的跨主题EMG模式识别提供了新视角，并展示了模型在生物识别等领域的潜力。

Abstract: Cross-subject electromyography (EMG) pattern recognition faces significant
challenges due to inter-subject variability in muscle anatomy, electrode
placement, and signal characteristics. Traditional methods rely on
subject-specific calibration data to adapt models to new users, an approach
that is both time-consuming and impractical for large-scale, real-world
deployment. This paper presents an approach to eliminate calibration
requirements through feature disentanglement, enabling effective cross-subject
generalization. We propose an end-to-end dual-branch adversarial neural network
that simultaneously performs pattern recognition and individual identification
by disentangling EMG features into pattern-specific and subject-specific
components. The pattern-specific components facilitate robust pattern
recognition for new users without model calibration, while the subject-specific
components enable downstream applications such as task-invariant biometric
identification. Experimental results demonstrate that the proposed model
achieves robust performance on data from unseen users, outperforming various
baseline methods in cross-subject scenarios. Overall, this study offers a new
perspective for cross-subject EMG pattern recognition without model calibration
and highlights the proposed model's potential for broader applications, such as
task-independent biometric systems.

</details>


### [134] [SEMA: a Scalable and Efficient Mamba like Attention via Token Localization and Averaging](https://arxiv.org/abs/2506.08297)
*Nhat Thanh Tran,Fanghui Xue,Shuai Zhang,Jiancheng Lyu,Yunling Zheng,Yingyong Qi,Jack Xin*

Main category: cs.CV

TL;DR: 论文提出了一种名为SEMA的新型注意力机制，解决了传统注意力机制在计算复杂度和聚焦能力上的问题，并在图像分类任务中表现出色。


<details>
  <summary>Details</summary>
Motivation: 传统注意力机制（如vanilla full attention和linear attention）在计算机视觉任务中存在计算复杂度高或无法有效聚焦的问题。

Method: 通过数学定义广义注意力，并基于Mamba形式的注意力设计SEMA，利用token定位避免分散并保持聚焦，同时通过算术平均捕获全局注意力。

Result: 在Imagenet-1k上的分类结果表明，SEMA在更大规模的图像上优于线性注意力和近期视觉Mamba模型。

Conclusion: SEMA是一种可扩展且高效的注意力机制，为计算机视觉任务提供了新的解决方案。

Abstract: Attention is the critical component of a transformer. Yet the quadratic
computational complexity of vanilla full attention in the input size and the
inability of its linear attention variant to focus have been challenges for
computer vision tasks. We provide a mathematical definition of generalized
attention and formulate both vanilla softmax attention and linear attention
within the general framework. We prove that generalized attention disperses,
that is, as the number of keys tends to infinity, the query assigns equal
weights to all keys. Motivated by the dispersion property and recent
development of Mamba form of attention, we design Scalable and Efficient Mamba
like Attention (SEMA) which utilizes token localization to avoid dispersion and
maintain focusing, complemented by theoretically consistent arithmetic
averaging to capture global aspect of attention. We support our approach on
Imagenet-1k where classification results show that SEMA is a scalable and
effective alternative beyond linear attention, outperforming recent vision
Mamba models on increasingly larger scales of images at similar model parameter
sizes.

</details>


### [135] [OpenRR-1k: A Scalable Dataset for Real-World Reflection Removal](https://arxiv.org/abs/2506.08299)
*Kangning Yang,Ling Ouyang,Huiming Sun,Jie Cai,Lan Fu,Jiaming Ding,Chiu Man Ho,Zibo Meng*

Main category: cs.CV

TL;DR: 论文提出了一种新的反射数据集收集方法，并构建了OpenRR-1k数据集，包含1000对高质量图像对，提升了反射去除技术的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有反射去除技术因缺乏高质量真实场景数据集而受限，需要一种便捷、低成本且可扩展的数据收集方法。

Method: 提出了一种新的数据集收集范式，确保数据高质量、对齐且多样化，并构建了OpenRR-1k数据集。

Result: 通过实验验证，OpenRR-1k数据集显著提升了反射去除方法在真实环境中的鲁棒性。

Conclusion: OpenRR-1k数据集为反射去除技术提供了高质量的真实场景数据支持，推动了该领域的发展。

Abstract: Reflection removal technology plays a crucial role in photography and
computer vision applications. However, existing techniques are hindered by the
lack of high-quality in-the-wild datasets. In this paper, we propose a novel
paradigm for collecting reflection datasets from a fresh perspective. Our
approach is convenient, cost-effective, and scalable, while ensuring that the
collected data pairs are of high quality, perfectly aligned, and represent
natural and diverse scenarios. Following this paradigm, we collect a
Real-world, Diverse, and Pixel-aligned dataset (named OpenRR-1k dataset), which
contains 1,000 high-quality transmission-reflection image pairs collected in
the wild. Through the analysis of several reflection removal methods and
benchmark evaluation experiments on our dataset, we demonstrate its
effectiveness in improving robustness in challenging real-world environments.
Our dataset is available at https://github.com/caijie0620/OpenRR-1k.

</details>


### [136] [Hyperspectral Image Classification via Transformer-based Spectral-Spatial Attention Decoupling and Adaptive Gating](https://arxiv.org/abs/2506.08324)
*Guandong Li,Mengxia Ye*

Main category: cs.CV

TL;DR: STNet通过创新的空间-光谱Transformer模块，有效解决了高光谱图像分类中的过拟合和泛化能力问题。


<details>
  <summary>Details</summary>
Motivation: 高光谱图像分类面临高维数据、地物稀疏分布和光谱冗余等挑战，导致过拟合和泛化能力受限。

Method: 提出STNet网络架构，其核心是空间-光谱Transformer模块，通过解耦空间和光谱注意力，并结合两种门控机制（自适应注意力融合门控和GFFN）。

Result: 在IN、UP和KSC数据集上表现优于主流方法。

Conclusion: STNet在不增加网络深度或宽度的情况下提升了特征提取和融合能力，减少了小样本和高噪声场景的过拟合风险。

Abstract: Deep neural networks face several challenges in hyperspectral image
classification, including high-dimensional data, sparse distribution of ground
objects, and spectral redundancy, which often lead to classification
overfitting and limited generalization capability. To more effectively extract
and fuse spatial context with fine spectral information in hyperspectral image
(HSI) classification, this paper proposes a novel network architecture called
STNet. The core advantage of STNet stems from the dual innovative design of its
Spatial-Spectral Transformer module: first, the fundamental explicit decoupling
of spatial and spectral attention ensures targeted capture of key information
in HSI; second, two functionally distinct gating mechanisms perform intelligent
regulation at both the fusion level of attention flows (adaptive attention
fusion gating) and the internal level of feature transformation (GFFN). This
characteristic demonstrates superior feature extraction and fusion capabilities
compared to traditional convolutional neural networks, while reducing
overfitting risks in small-sample and high-noise scenarios. STNet enhances
model representation capability without increasing network depth or width. The
proposed method demonstrates superior performance on IN, UP, and KSC datasets,
outperforming mainstream hyperspectral image classification approaches.

</details>


### [137] [Locating Tennis Ball Impact on the Racket in Real Time Using an Event Camera](https://arxiv.org/abs/2506.08327)
*Yuto Kase,Kai Ishibe,Ryoma Yasuda,Yudai Washida,Sakiko Hashimoto*

Main category: cs.CV

TL;DR: 提出了一种使用事件相机实时定位网球拍击球位置的方法，解决了高速相机内存消耗大和手动数字化耗时的问题。


<details>
  <summary>Details</summary>
Motivation: 在网球等球拍运动中，准确测量击球位置对分析球员表现和个性化装备设计至关重要，但现有方法存在内存消耗大和人工误差的问题。

Method: 通过事件相机高效捕捉亮度变化，结合传统计算机视觉技术和原创的事件处理算法（PATS），分三步识别击球位置：挥拍时间范围、击球时机、球和球拍轮廓。

Result: 实验结果表明，该方法在测量网球运动员表现时处于允许误差范围内，且计算时间足够短，适合实时应用。

Conclusion: 该方法为实时监测球员表现提供了高效、低内存消耗的解决方案。

Abstract: In racket sports, such as tennis, locating the ball's position at impact is
important in clarifying player and equipment characteristics, thereby aiding in
personalized equipment design. High-speed cameras are used to measure the
impact location; however, their excessive memory consumption limits prolonged
scene capture, and manual digitization for position detection is time-consuming
and prone to human error. These limitations make it difficult to effectively
capture the entire playing scene, hindering the ability to analyze the player's
performance. We propose a method for locating the tennis ball impact on the
racket in real time using an event camera. Event cameras efficiently measure
brightness changes (called `events') with microsecond accuracy under high-speed
motion while using lower memory consumption. These cameras enable users to
continuously monitor their performance over extended periods. Our method
consists of three identification steps: time range of swing, timing at impact,
and contours of ball and racket. Conventional computer vision techniques are
utilized along with an original event-based processing to detect the timing at
impact (PATS: the amount of polarity asymmetry in time symmetry). The results
of the experiments were within the permissible range for measuring tennis
players' performance. Moreover, the computation time was sufficiently short for
real-time applications.

</details>


### [138] [How Much To Guide: Revisiting Adaptive Guidance in Classifier-Free Guidance Text-to-Vision Diffusion Models](https://arxiv.org/abs/2506.08351)
*Huixuan Zhang,Junzhe Zhang,Xiaojun Wan*

Main category: cs.CV

TL;DR: 论文提出了一种名为Step AG的自适应引导策略，通过限制分类器自由引导在前几个去噪步骤中应用，显著提升了生成效率，同时保持图像质量和文本对齐。


<details>
  <summary>Details</summary>
Motivation: 当前分类器自由引导方法在文本到视觉生成扩散模型中成本较高，需要两倍于无条件生成的步骤，而现有自适应引导方法缺乏分析和实证支持。

Method: 提出Step AG策略，限制分类器自由引导在前几个去噪步骤中应用，以减少计算成本。

Result: 实验表明，该方法在图像质量和文本对齐方面表现良好，平均提速20%至30%，且适用于不同设置和模型。

Conclusion: Step AG是一种简单通用的自适应引导策略，显著提升了生成效率，适用于多种扩散模型。

Abstract: With the rapid development of text-to-vision generation diffusion models,
classifier-free guidance has emerged as the most prevalent method for
conditioning. However, this approach inherently requires twice as many steps
for model forwarding compared to unconditional generation, resulting in
significantly higher costs. While previous study has introduced the concept of
adaptive guidance, it lacks solid analysis and empirical results, making
previous method unable to be applied to general diffusion models. In this work,
we present another perspective of applying adaptive guidance and propose Step
AG, which is a simple, universally applicable adaptive guidance strategy. Our
evaluations focus on both image quality and image-text alignment. whose results
indicate that restricting classifier-free guidance to the first several
denoising steps is sufficient for generating high-quality, well-conditioned
images, achieving an average speedup of 20% to 30%. Such improvement is
consistent across different settings such as inference steps, and various
models including video generation models, highlighting the superiority of our
method.

</details>


### [139] [MedMoE: Modality-Specialized Mixture of Experts for Medical Vision-Language Understanding](https://arxiv.org/abs/2506.08356)
*Shivang Chopra,Lingchao Mao,Gabriela Sanchez-Rodriguez,Andrew J Feola,Jing Li,Zsolt Kira*

Main category: cs.CV

TL;DR: MedMoE是一个动态适应不同医学成像模态的视觉-语言处理框架，通过Mixture-of-Experts模块和多尺度特征提取，提升模态对齐和检索性能。


<details>
  <summary>Details</summary>
Motivation: 现有医学视觉-语言框架采用统一策略提取局部特征，忽视了不同模态的特定需求。

Method: 基于Swin Transformer的多尺度特征金字塔，结合MoE模块动态路由模态专家分支，提取模态特定视觉语义。

Result: 在多个医学基准测试中，MedMoE显著提升了模态对齐和检索性能。

Conclusion: MedMoE证明了模态专用视觉表示在临床视觉-语言系统中的重要性。

Abstract: Different medical imaging modalities capture diagnostic information at
varying spatial resolutions, from coarse global patterns to fine-grained
localized structures. However, most existing vision-language frameworks in the
medical domain apply a uniform strategy for local feature extraction,
overlooking the modality-specific demands. In this work, we present MedMoE, a
modular and extensible vision-language processing framework that dynamically
adapts visual representation based on the diagnostic context. MedMoE
incorporates a Mixture-of-Experts (MoE) module conditioned on the report type,
which routes multi-scale image features through specialized expert branches
trained to capture modality-specific visual semantics. These experts operate
over feature pyramids derived from a Swin Transformer backbone, enabling
spatially adaptive attention to clinically relevant regions. This framework
produces localized visual representations aligned with textual descriptions,
without requiring modality-specific supervision at inference. Empirical results
on diverse medical benchmarks demonstrate that MedMoE improves alignment and
retrieval performance across imaging modalities, underscoring the value of
modality-specialized visual representations in clinical vision-language
systems.

</details>


### [140] [Image Demoiréing Using Dual Camera Fusion on Mobile Phones](https://arxiv.org/abs/2506.08361)
*Yanting Mei,Zhilu Zhang,Xiaohe Wu,Wangmeng Zuo*

Main category: cs.CV

TL;DR: 论文提出了一种利用双摄像头融合（DCID）去除图像摩尔纹的方法，通过超广角（UW）图像辅助广角（W）图像去除摩尔纹，效果优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现代智能手机通常配备双镜头，且超广角图像在广角图像出现摩尔纹时能提供正常颜色和纹理。

Method: 提出轻量级UW图像编码器集成到现有去摩尔纹网络中，并采用快速两阶段图像对齐方式。

Result: 在包含约9,000个样本的真实数据集上实验表明，该方法优于现有技术。

Conclusion: DCID方法通过双摄像头融合有效去除摩尔纹，且性能优越。

Abstract: When shooting electronic screens, moir\'e patterns usually appear in captured
images, which seriously affects the image quality. Existing image demoir\'eing
methods face great challenges in removing large and heavy moir\'e. To address
the issue, we propose to utilize Dual Camera fusion for Image Demoir\'eing
(DCID), \ie, using the ultra-wide-angle (UW) image to assist the moir\'e
removal of wide-angle (W) image. This is inspired by two motivations: (1) the
two lenses are commonly equipped with modern smartphones, (2) the UW image
generally can provide normal colors and textures when moir\'e exists in the W
image mainly due to their different focal lengths. In particular, we propose an
efficient DCID method, where a lightweight UW image encoder is integrated into
an existing demoir\'eing network and a fast two-stage image alignment manner is
present. Moreover, we construct a large-scale real-world dataset with diverse
mobile phones and monitors, containing about 9,000 samples. Experiments on the
dataset show our method performs better than state-of-the-art methods. Code and
dataset are available at https://github.com/Mrduckk/DCID.

</details>


### [141] [SECOND: Mitigating Perceptual Hallucination in Vision-Language Models via Selective and Contrastive Decoding](https://arxiv.org/abs/2506.08391)
*Woohyeon Park,Woojin Kim,Jaeik Kim,Jaeyoung Do*

Main category: cs.CV

TL;DR: SECOND是一种选择性对比解码方法，通过多尺度视觉信息减少视觉语言模型中的物体幻觉问题。


<details>
  <summary>Details</summary>
Motivation: 现有视觉语言模型因物体幻觉问题导致性能受限，需改进以实现更准确的视觉理解。

Method: 提出SECOND方法，通过选择性整合多尺度视觉信息并进行对比，减少幻觉。

Result: SECOND显著减少幻觉，并在多个基准测试中表现优异。

Conclusion: 多尺度视觉信息的应用潜力巨大，SECOND方法优于现有方法。

Abstract: Despite significant advancements in Vision-Language Models (VLMs), the
performance of existing VLMs remains hindered by object hallucination, a
critical challenge to achieving accurate visual understanding. To address this
issue, we propose SECOND: Selective and Contrastive Decoding, a novel approach
that enables VLMs to effectively leverage multi-scale visual information with
an object-centric manner, closely aligning with human visual perception. SECOND
progressively selects and integrates multi-scale visual information,
facilitating a more precise interpretation of images. By contrasting these
visual information iteratively, SECOND significantly reduces perceptual
hallucinations and outperforms a wide range of benchmarks. Our theoretical
analysis and experiments highlight the largely unexplored potential of
multi-scale application in VLMs, showing that prioritizing and contrasting
across scales outperforms existing methods.

</details>


### [142] [RadioDUN: A Physics-Inspired Deep Unfolding Network for Radio Map Estimation](https://arxiv.org/abs/2506.08418)
*Taiqin Chen,Zikun Zhou,Zheng Fang,Wenzhen Zou,Kanjun Liu,Ke Chen,Yongbing Zhang,Yaowei Wang*

Main category: cs.CV

TL;DR: 该论文提出了一种基于物理传播模型的稀疏信号恢复方法RadioDUN，用于从稀疏样本中估计密集无线电地图，并通过动态重加权模块和阴影损失优化性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法难以结合无线电地图的物理特性，导致密集无线电地图构建困难。

Method: 将无线电地图估计建模为稀疏信号恢复问题，结合物理传播模型分解为多因子优化子问题，提出RadioDUN网络和动态重加权模块（DRM），并设计阴影损失。

Result: 实验表明，RadioDUN优于现有方法。

Conclusion: RadioDUN通过结合物理特性和自适应优化，显著提升了无线电地图估计的精度。

Abstract: The radio map represents the spatial distribution of spectrum resources
within a region, supporting efficient resource allocation and interference
mitigation. However, it is difficult to construct a dense radio map as a
limited number of samples can be measured in practical scenarios. While
existing works have used deep learning to estimate dense radio maps from sparse
samples, they are hard to integrate with the physical characteristics of the
radio map. To address this challenge, we cast radio map estimation as the
sparse signal recovery problem. A physical propagation model is further
incorporated to decompose the problem into multiple factor optimization
sub-problems, thereby reducing recovery complexity. Inspired by the existing
compressive sensing methods, we propose the Radio Deep Unfolding Network
(RadioDUN) to unfold the optimization process, achieving adaptive parameter
adjusting and prior fitting in a learnable manner. To account for the radio
propagation characteristics, we develop a dynamic reweighting module (DRM) to
adaptively model the importance of each factor for the radio map. Inspired by
the shadowing factor in the physical propagation model, we integrate
obstacle-related factors to express the obstacle-induced signal stochastic
decay. The shadowing loss is further designed to constrain the factor
prediction and act as a supplementary supervised objective, which enhances the
performance of RadioDUN. Extensive experiments have been conducted to
demonstrate that the proposed method outperforms the state-of-the-art methods.
Our code will be made publicly available upon publication.

</details>


### [143] [Better Reasoning with Less Data: Enhancing VLMs Through Unified Modality Scoring](https://arxiv.org/abs/2506.08429)
*Mingjie Xu,Andrew Estornell,Hongzheng Yang,Yuzhi Zhao,Zhaowei Zhu,Qi Xuan,Jiaheng Wei*

Main category: cs.CV

TL;DR: SCALE提出了一种基于数据质量驱动的视觉语言模型指令调优数据集选择方法，解决了图像与文本对齐噪声和文本模糊问题。


<details>
  <summary>Details</summary>
Motivation: 视觉语言模型的效果依赖于高质量数据集，但现有数据存在图像与文本对齐噪声和文本模糊问题。

Method: SCALE通过跨模态评估框架，生成任务特定描述并评估数据条目质量。

Result: 发现现有单模态评估方法低估了任务关键样本，而生成描述可将多模态任务转化为文本模态。

Conclusion: SCALE为视觉语言模型提供了高效的数据选择方法，提升了模型性能。

Abstract: The application of visual instruction tuning and other post-training
techniques has significantly enhanced the capabilities of Large Language Models
(LLMs) in visual understanding, enriching Vision-Language Models (VLMs) with
more comprehensive visual language datasets. However, the effectiveness of VLMs
is highly dependent on large-scale, high-quality datasets that ensure precise
recognition and accurate reasoning. Two key challenges hinder progress: (1)
noisy alignments between images and the corresponding text, which leads to
misinterpretation, and (2) ambiguous or misleading text, which obscures visual
content. To address these challenges, we propose SCALE (Single modality data
quality and Cross modality Alignment Evaluation), a novel quality-driven data
selection pipeline for VLM instruction tuning datasets. Specifically, SCALE
integrates a cross-modality assessment framework that first assigns each data
entry to its appropriate vision-language task, generates general and
task-specific captions (covering scenes, objects, style, etc.), and evaluates
the alignment, clarity, task rarity, text coherence, and image clarity of each
entry based on the generated captions. We reveal that: (1) current unimodal
quality assessment methods evaluate one modality while overlooking the rest,
which can underestimate samples essential for specific tasks and discard the
lower-quality instances that help build model robustness; and (2) appropriately
generated image captions provide an efficient way to transfer the image-text
multimodal task into a unified text modality.

</details>


### [144] [Enhancing Motion Dynamics of Image-to-Video Models via Adaptive Low-Pass Guidance](https://arxiv.org/abs/2506.08456)
*June Suk Choi,Kyungmin Lee,Sihyun Yu,Yisol Choi,Jinwoo Shin,Kimin Lee*

Main category: cs.CV

TL;DR: 论文提出了一种自适应低通引导（ALG）方法，解决了图像到视频（I2V）生成中动态性不足的问题，显著提升了视频的动态性。


<details>
  <summary>Details</summary>
Motivation: 现有I2V方法在生成视频时动态性不足，原因是输入图像的高频细节过早影响采样过程。

Method: 提出ALG方法，在去噪早期阶段自适应地对条件图像进行低通滤波。

Result: ALG显著提升了视频的动态性（VBench-I2V测试中动态性平均提升36%），同时保持了图像质量和文本对齐。

Conclusion: ALG是一种简单有效的解决方案，能显著改善I2V生成的动态性。

Abstract: Recent text-to-video (T2V) models have demonstrated strong capabilities in
producing high-quality, dynamic videos. To improve the visual controllability,
recent works have considered fine-tuning pre-trained T2V models to support
image-to-video (I2V) generation. However, such adaptation frequently suppresses
motion dynamics of generated outputs, resulting in more static videos compared
to their T2V counterparts. In this work, we analyze this phenomenon and
identify that it stems from the premature exposure to high-frequency details in
the input image, which biases the sampling process toward a shortcut trajectory
that overfits to the static appearance of the reference image. To address this,
we propose adaptive low-pass guidance (ALG), a simple fix to the I2V model
sampling procedure to generate more dynamic videos without compromising
per-frame image quality. Specifically, ALG adaptively modulates the frequency
content of the conditioning image by applying low-pass filtering at the early
stage of denoising. Extensive experiments demonstrate that ALG significantly
improves the temporal dynamics of generated videos, while preserving image
fidelity and text alignment. Especially, under VBench-I2V test suite, ALG
achieves an average improvement of 36% in dynamic degree without a significant
drop in video quality or image fidelity.

</details>


### [145] [MARMOT: Masked Autoencoder for Modeling Transient Imaging](https://arxiv.org/abs/2506.08470)
*Siyuan Shen,Ziheng Wang,Xingyue Peng,Suan Xia,Ruiqian Li,Shiying Li,Jingyi Yu*

Main category: cs.CV

TL;DR: 该论文提出了一种名为MARMOT的掩码自编码器，用于建模瞬态成像，特别是在非视距（NLOS）场景中。通过预训练和Transformer架构，MARMOT能够从部分掩码的瞬态数据中学习特征，并预测完整测量结果。


<details>
  <summary>Details</summary>
Motivation: 瞬态成像是一种新兴的模态，尤其在NLOS场景中，现有方法多基于体积密度或表面优化，缺乏从数据集中学习先验知识的能力。MARMOT旨在通过自监督预训练填补这一空白。

Method: MARMOT是一种基于Transformer的自监督模型，通过扫描模式掩码（SPM）从部分掩码的瞬态数据中学习特征，并预测完整测量结果。预训练使用了包含500K 3D模型的合成数据集TransVerse。

Result: 实验表明，MARMOT在定量和定性评估中均优于现有方法，证明了其高效性。

Conclusion: MARMOT为NLOS瞬态成像提供了一种高效的预训练方法，能够通过特征迁移或解码器微调适应下游任务。

Abstract: Pretrained models have demonstrated impressive success in many modalities
such as language and vision. Recent works facilitate the pretraining paradigm
in imaging research. Transients are a novel modality, which are captured for an
object as photon counts versus arrival times using a precisely time-resolved
sensor. In particular for non-line-of-sight (NLOS) scenarios, transients of
hidden objects are measured beyond the sensor's direct line of sight. Using
NLOS transients, the majority of previous works optimize volume density or
surfaces to reconstruct the hidden objects and do not transfer priors learned
from datasets. In this work, we present a masked autoencoder for modeling
transient imaging, or MARMOT, to facilitate NLOS applications. Our MARMOT is a
self-supervised model pretrianed on massive and diverse NLOS transient
datasets. Using a Transformer-based encoder-decoder, MARMOT learns features
from partially masked transients via a scanning pattern mask (SPM), where the
unmasked subset is functionally equivalent to arbitrary sampling, and predicts
full measurements. Pretrained on TransVerse-a synthesized transient dataset of
500K 3D models-MARMOT adapts to downstream imaging tasks using direct feature
transfer or decoder finetuning. Comprehensive experiments are carried out in
comparisons with state-of-the-art methods. Quantitative and qualitative results
demonstrate the efficiency of our MARMOT.

</details>


### [146] [MLVTG: Mamba-Based Feature Alignment and LLM-Driven Purification for Multi-Modal Video Temporal Grounding](https://arxiv.org/abs/2506.08512)
*Zhiyi Zhu,Xiaoyu Wu,Zihao Liu,Linlin Yang*

Main category: cs.CV

TL;DR: MLVTG提出了一种新的视频时间定位框架，通过MambaAligner和LLMRefiner模块解决了现有Transformer方法的冗余注意力和多模态对齐问题，实现了更精确的定位。


<details>
  <summary>Details</summary>
Motivation: 现有的基于Transformer的视频时间定位方法存在冗余注意力和多模态对齐不足的问题，影响了定位的准确性。

Method: MLVTG结合了MambaAligner（使用Vision Mamba块建模时间依赖）和LLMRefiner（利用预训练LLM的特定层增强语义对齐），实现了双对齐策略。

Result: 在QVHighlights、Charades-STA和TVSum数据集上，MLVTG表现优于现有基线，达到了最先进的性能。

Conclusion: MLVTG通过创新的双对齐策略，显著提升了视频时间定位的精度和性能。

Abstract: Video Temporal Grounding (VTG), which aims to localize video clips
corresponding to natural language queries, is a fundamental yet challenging
task in video understanding. Existing Transformer-based methods often suffer
from redundant attention and suboptimal multi-modal alignment. To address these
limitations, we propose MLVTG, a novel framework that integrates two key
modules: MambaAligner and LLMRefiner. MambaAligner uses stacked Vision Mamba
blocks as a backbone instead of Transformers to model temporal dependencies and
extract robust video representations for multi-modal alignment. LLMRefiner
leverages the specific frozen layer of a pre-trained Large Language Model (LLM)
to implicitly transfer semantic priors, enhancing multi-modal alignment without
fine-tuning. This dual alignment strategy, temporal modeling via structured
state-space dynamics and semantic purification via textual priors, enables more
precise localization. Extensive experiments on QVHighlights, Charades-STA, and
TVSum demonstrate that MLVTG achieves state-of-the-art performance and
significantly outperforms existing baselines.

</details>


### [147] [Robust Visual Localization via Semantic-Guided Multi-Scale Transformer](https://arxiv.org/abs/2506.08526)
*Zhongtao Tian,Wenhao Huang,Zhidong Chen,Xiao Wei Sun*

Main category: cs.CV

TL;DR: 提出了一种结合多尺度特征学习和语义场景理解的框架，用于动态环境中的视觉定位，通过分层Transformer和语义监督提升性能。


<details>
  <summary>Details</summary>
Motivation: 动态环境中光照、天气和移动物体等因素干扰视觉定位，现有绝对姿态回归方法难以保持一致。

Method: 采用分层Transformer和跨尺度注意力融合几何细节与上下文线索，结合语义监督训练网络学习视角不变特征。

Result: 在TartanAir数据集上，该方法在动态物体、光照变化和遮挡等挑战性场景中优于现有姿态回归方法。

Conclusion: 多尺度处理与语义指导的结合为动态环境中的鲁棒视觉定位提供了有效策略。

Abstract: Visual localization remains challenging in dynamic environments where
fluctuating lighting, adverse weather, and moving objects disrupt appearance
cues. Despite advances in feature representation, current absolute pose
regression methods struggle to maintain consistency under varying conditions.
To address this challenge, we propose a framework that synergistically combines
multi-scale feature learning with semantic scene understanding. Our approach
employs a hierarchical Transformer with cross-scale attention to fuse geometric
details and contextual cues, preserving spatial precision while adapting to
environmental changes. We improve the performance of this architecture with
semantic supervision via neural scene representation during training, guiding
the network to learn view-invariant features that encode persistent structural
information while suppressing complex environmental interference. Experiments
on TartanAir demonstrate that our approach outperforms existing pose regression
methods in challenging scenarios with dynamic objects, illumination changes,
and occlusions. Our findings show that integrating multi-scale processing with
semantic guidance offers a promising strategy for robust visual localization in
real-world dynamic environments.

</details>


### [148] [LiftVSR: Lifting Image Diffusion to Video Super-Resolution via Hybrid Temporal Modeling with Only 4$\times$RTX 4090s](https://arxiv.org/abs/2506.08529)
*Xijun Wang,Xin Li,Bingchen Li,Zhibo Chen*

Main category: cs.CV

TL;DR: LiftVSR是一种高效的视频超分辨率框架，通过结合动态时间注意力和注意力记忆缓存，显著提升时间一致性并降低计算成本。


<details>
  <summary>Details</summary>
Motivation: 现有方法在时间一致性和计算成本方面存在不足，尤其是长视频处理时计算资源需求过高。

Method: 提出混合时间建模机制，包括动态时间注意力（DTA）和注意力记忆缓存（AMC），并结合非对称采样策略。

Result: 在多个VSR基准测试中表现出色，计算成本显著降低。

Conclusion: LiftVSR在性能和效率上取得平衡，为视频超分辨率提供了高效解决方案。

Abstract: Diffusion models have significantly advanced video super-resolution (VSR) by
enhancing perceptual quality, largely through elaborately designed temporal
modeling to ensure inter-frame consistency. However, existing methods usually
suffer from limited temporal coherence and prohibitively high computational
costs (e.g., typically requiring over 8 NVIDIA A100-80G GPUs), especially for
long videos. In this work, we propose LiftVSR, an efficient VSR framework that
leverages and elevates the image-wise diffusion prior from PixArt-$\alpha$,
achieving state-of-the-art results using only 4$\times$RTX 4090 GPUs. To
balance long-term consistency and efficiency, we introduce a hybrid temporal
modeling mechanism that decomposes temporal learning into two complementary
components: (i) Dynamic Temporal Attention (DTA) for fine-grained temporal
modeling within short frame segment ($\textit{i.e.}$, low complexity), and (ii)
Attention Memory Cache (AMC) for long-term temporal modeling across segments
($\textit{i.e.}$, consistency). Specifically, DTA identifies multiple token
flows across frames within multi-head query and key tokens to warp inter-frame
contexts in the value tokens. AMC adaptively aggregates historical segment
information via a cache unit, ensuring long-term coherence with minimal
overhead. To further stabilize the cache interaction during inference, we
introduce an asymmetric sampling strategy that mitigates feature mismatches
arising from different diffusion sampling steps. Extensive experiments on
several typical VSR benchmarks have demonstrated that LiftVSR achieves
impressive performance with significantly lower computational costs.

</details>


### [149] [TrajFlow: Multi-modal Motion Prediction via Flow Matching](https://arxiv.org/abs/2506.08541)
*Qi Yan,Brian Zhang,Yutong Zhang,Daniel Yang,Joshua White,Di Chen,Jiachao Liu,Langechuan Liu,Binnan Zhuang,Shaoshuai Shi,Renjie Liao*

Main category: cs.CV

TL;DR: TrajFlow是一种基于流匹配的运动预测框架，通过单次推理预测多模态轨迹，显著降低计算开销并保持预测一致性，同时引入排名损失和自条件训练技术提升性能。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶中高效准确的运动预测对安全和决策至关重要，现有生成方法在多模态预测和效率上存在挑战。

Method: 提出TrajFlow框架，采用流匹配技术单次预测多轨迹，引入Plackett-Luce分布排名损失和改进的自条件训练技术。

Result: 在Waymo Open Motion Dataset上表现优异，达到SOTA性能。

Conclusion: TrajFlow为安全关键型自动驾驶应用提供高效、准确的运动预测解决方案。

Abstract: Efficient and accurate motion prediction is crucial for ensuring safety and
informed decision-making in autonomous driving, particularly under dynamic
real-world conditions that necessitate multi-modal forecasts. We introduce
TrajFlow, a novel flow matching-based motion prediction framework that
addresses the scalability and efficiency challenges of existing generative
trajectory prediction methods. Unlike conventional generative approaches that
employ i.i.d. sampling and require multiple inference passes to capture diverse
outcomes, TrajFlow predicts multiple plausible future trajectories in a single
pass, significantly reducing computational overhead while maintaining coherence
across predictions. Moreover, we propose a ranking loss based on the
Plackett-Luce distribution to improve uncertainty estimation of predicted
trajectories. Additionally, we design a self-conditioning training technique
that reuses the model's own predictions to construct noisy inputs during a
second forward pass, thereby improving generalization and accelerating
inference. Extensive experiments on the large-scale Waymo Open Motion Dataset
(WOMD) demonstrate that TrajFlow achieves state-of-the-art performance across
various key metrics, underscoring its effectiveness for safety-critical
autonomous driving applications. The code and other details are available on
the project website https://traj-flow.github.io/.

</details>


### [150] [Convergence of Spectral Principal Paths: How Deep Networks Distill Linear Representations from Noisy Inputs](https://arxiv.org/abs/2506.08543)
*Bowei Tian,Xuntao Lyu,Meng Liu,Hongyi Wang,Ang Li*

Main category: cs.CV

TL;DR: 论文提出Input-Space Linearity Hypothesis（ISLH），认为概念对齐的方向源于输入空间，并通过Spectral Principal Path（SPP）框架展示深度网络如何逐步提取线性表示。


<details>
  <summary>Details</summary>
Motivation: 提升AI的透明度和控制性，从神经元或电路转向结构化语义方向，与人类可解释概念对齐。

Method: 提出ISLH假设，并引入SPP框架，验证深度网络中线性表示的逐步提取过程。

Result: 展示了这些表示在Vision-Language Models（VLMs）中的多模态鲁棒性。

Conclusion: 为深度网络表示形成的结构化理论奠定基础，有助于提升AI的鲁棒性、公平性和透明度。

Abstract: High-level representations have become a central focus in enhancing AI
transparency and control, shifting attention from individual neurons or
circuits to structured semantic directions that align with human-interpretable
concepts. Motivated by the Linear Representation Hypothesis (LRH), we propose
the Input-Space Linearity Hypothesis (ISLH), which posits that concept-aligned
directions originate in the input space and are selectively amplified with
increasing depth. We then introduce the Spectral Principal Path (SPP)
framework, which formalizes how deep networks progressively distill linear
representations along a small set of dominant spectral directions. Building on
this framework, we further demonstrate the multimodal robustness of these
representations in Vision-Language Models (VLMs). By bridging theoretical
insights with empirical validation, this work advances a structured theory of
representation formation in deep networks, paving the way for improving AI
robustness, fairness, and transparency.

</details>


### [151] [From Pixels to Graphs: using Scene and Knowledge Graphs for HD-EPIC VQA Challenge](https://arxiv.org/abs/2506.08553)
*Agnese Taluzzi,Davide Gesualdi,Riccardo Santambrogio,Chiara Plizzari,Francesca Palermo,Simone Mentasti,Matteo Matteucci*

Main category: cs.CV

TL;DR: SceneNet和KnowledgeNet是用于HD-EPIC VQA Challenge 2025的方法，分别利用场景图和外部常识知识提升视觉问答性能，组合后达到44.21%的准确率。


<details>
  <summary>Details</summary>
Motivation: 解决复杂自我中心视觉问答任务中细粒度对象交互和高级语义推理的需求。

Method: SceneNet使用多模态大语言模型生成场景图捕捉对象交互；KnowledgeNet引入ConceptNet的外部常识知识。

Result: 在HD-EPIC基准的七个类别中表现优异，组合后准确率为44.21%。

Conclusion: 该方法在复杂自我中心视觉问答任务中表现出高效性。

Abstract: This report presents SceneNet and KnowledgeNet, our approaches developed for
the HD-EPIC VQA Challenge 2025. SceneNet leverages scene graphs generated with
a multi-modal large language model (MLLM) to capture fine-grained object
interactions, spatial relationships, and temporally grounded events. In
parallel, KnowledgeNet incorporates ConceptNet's external commonsense knowledge
to introduce high-level semantic connections between entities, enabling
reasoning beyond directly observable visual evidence. Each method demonstrates
distinct strengths across the seven categories of the HD-EPIC benchmark, and
their combination within our framework results in an overall accuracy of 44.21%
on the challenge, highlighting its effectiveness for complex egocentric VQA
tasks.

</details>


### [152] [Hierarchical Neural Collapse Detection Transformer for Class Incremental Object Detection](https://arxiv.org/abs/2506.08562)
*Duc Thanh Pham,Hong Dang Nguyen,Nhat Minh Nguyen Quoc,Linh Ngo Van,Sang Dinh Viet,Duc Anh Nguyen*

Main category: cs.CV

TL;DR: 提出了一种名为Hier-DETR的新框架，用于增量目标检测（IOD），通过利用神经崩溃和类别标签的层次关系，实现了高效且具有竞争力的性能。


<details>
  <summary>Details</summary>
Motivation: 现实世界中新物体不断出现，需要检测模型持续学习而不遭受灾难性遗忘，但现有IOD模型因性能有限和推理时间长而不实用。

Method: Hier-DETR框架结合神经崩溃（Neural Collapse）处理不平衡数据集和类别标签的层次关系。

Result: 该框架在效率和性能上均表现优异。

Conclusion: Hier-DETR为解决增量目标检测的挑战提供了实用且高效的方案。

Abstract: Recently, object detection models have witnessed notable performance
improvements, particularly with transformer-based models. However, new objects
frequently appear in the real world, requiring detection models to continually
learn without suffering from catastrophic forgetting. Although Incremental
Object Detection (IOD) has emerged to address this challenge, these existing
models are still not practical due to their limited performance and prolonged
inference time. In this paper, we introduce a novel framework for IOD, called
Hier-DETR: Hierarchical Neural Collapse Detection Transformer, ensuring both
efficiency and competitive performance by leveraging Neural Collapse for
imbalance dataset and Hierarchical relation of classes' labels.

</details>


### [153] [Generating Vision-Language Navigation Instructions Incorporated Fine-Grained Alignment Annotations](https://arxiv.org/abs/2506.08566)
*Yibo Cui,Liang Xie,Yu Zhao,Jiawei Sun,Erwei Yin*

Main category: cs.CV

TL;DR: 论文提出FCA-NIG框架，自动生成具有细粒度跨模态标注的导航指令，解决了现有数据集中子指令和实体级对齐不足的问题，显著提升了VLN智能体的性能。


<details>
  <summary>Details</summary>
Motivation: 现有VLN数据集缺乏细粒度的跨模态对齐标注（如子指令和实体级对齐），限制了智能体的导航决策准确性。

Method: 提出FCA-NIG框架，通过子轨迹分割、地标检测、指令生成和实体选择，自动生成带有子指令-子轨迹和实体-地标对齐的FCA-R2R数据集。

Result: 实验表明，FCA-R2R显著提升了多个VLN智能体的性能，增强了状态感知和导航准确性。

Conclusion: FCA-NIG无需人工标注即可生成高质量训练数据，推动了复杂导航任务中的细粒度跨模态学习。

Abstract: Vision-Language Navigation (VLN) enables intelligent agents to navigate
environments by integrating visual perception and natural language
instructions, yet faces significant challenges due to the scarcity of
fine-grained cross-modal alignment annotations. Existing datasets primarily
focus on global instruction-trajectory matching, neglecting
sub-instruction-level and entity-level alignments critical for accurate
navigation action decision-making. To address this limitation, we propose
FCA-NIG, a generative framework that automatically constructs navigation
instructions with dual-level fine-grained cross-modal annotations. In this
framework, an augmented trajectory is first divided into sub-trajectories,
which are then processed through GLIP-based landmark detection, crafted
instruction construction, OFA-Speaker based R2R-like instruction generation,
and CLIP-powered entity selection, generating sub-instruction-trajectory pairs
with entity-landmark annotations. Finally, these sub-pairs are aggregated to
form a complete instruction-trajectory pair. The framework generates the
FCA-R2R dataset, the first large-scale augmentation dataset featuring precise
sub-instruction-sub-trajectory and entity-landmark alignments. Extensive
experiments demonstrate that training with FCA-R2R significantly improves the
performance of multiple state-of-the-art VLN agents, including SF, EnvDrop,
RecBERT, and HAMT. Incorporating sub-instruction-trajectory alignment enhances
agents' state awareness and decision accuracy, while entity-landmark alignment
further boosts navigation performance and generalization. These results
highlight the effectiveness of FCA-NIG in generating high-quality, scalable
training data without manual annotation, advancing fine-grained cross-modal
learning in complex navigation tasks.

</details>


### [154] [Transformers Meet Hyperspectral Imaging: A Comprehensive Study of Models, Challenges and Open Problems](https://arxiv.org/abs/2506.08596)
*Guyang Zhang,Waleed Abdulla*

Main category: cs.CV

TL;DR: 本文是第一篇专注于Transformer在HSI分类中的端到端综述，总结了300多篇论文，分析了Transformer在HSI中的应用及其设计选择。


<details>
  <summary>Details</summary>
Motivation: 尽管Transformer在长距离依赖学习中表现出色，但在HSI领域的应用仍处于起步阶段，需要系统性的综述来指导研究。

Method: 研究通过分类HSI处理流程的每个阶段（如预处理、特征提取、注意力机制等），对比不同设计选择，并结合HSI的独特特性进行分析。

Result: 总结了HSI领域的进展与挑战（如数据稀缺、计算开销大等），并提出了未来的研究方向（如轻量化模型、可解释性等）。

Conclusion: 本文旨在为研究者提供指导，帮助选择或扩展适合HSI应用的Transformer组件。

Abstract: Transformers have become the architecture of choice for learning long-range
dependencies, yet their adoption in hyperspectral imaging (HSI) is still
emerging. We reviewed more than 300 papers published up to 2025 and present the
first end-to-end survey dedicated to Transformer-based HSI classification. The
study categorizes every stage of a typical pipeline-pre-processing, patch or
pixel tokenization, positional encoding, spatial-spectral feature extraction,
multi-head self-attention variants, skip connections, and loss design-and
contrasts alternative design choices with the unique spatial-spectral
properties of HSI. We map the field's progress against persistent obstacles:
scarce labeled data, extreme spectral dimensionality, computational overhead,
and limited model explainability. Finally, we outline a research agenda
prioritizing valuable public data sets, lightweight on-edge models,
illumination and sensor shifts robustness, and intrinsically interpretable
attention mechanisms. Our goal is to guide researchers in selecting, combining,
or extending Transformer components that are truly fit for purpose for
next-generation HSI applications.

</details>


### [155] [Rethinking Range-View LiDAR Segmentation in Adverse Weather](https://arxiv.org/abs/2506.08979)
*Longyu Yang,Ping Hu,Lu Zhang,Jun Liu,Yap-Peng Tan,Heng Tao Shen,Xiaofeng Zhu*

Main category: cs.CV

TL;DR: 本文提出了一种轻量级模块化框架，通过分离处理几何属性和反射强度，提升了LiDAR分割在恶劣天气下的泛化性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于范围视图的LiDAR分割方法在恶劣天气下泛化性能不足，限制了其在实际环境中的可靠性。

Method: 提出了一种模块化框架，包含几何异常抑制（GAS）模块和反射失真校准（RDC）模块，分别处理几何噪声和反射失真。

Result: 实验表明，该方法显著提升了模型在恶劣天气下的泛化性能，且推理开销极小。

Conclusion: 该框架为实际LiDAR分割提供了一种实用且高效的解决方案。

Abstract: LiDAR segmentation has emerged as an important task to enrich multimedia
experiences and analysis. Range-view-based methods have gained popularity due
to their high computational efficiency and compatibility with real-time
deployment. However, their generalized performance under adverse weather
conditions remains underexplored, limiting their reliability in real-world
environments. In this work, we identify and analyze the unique challenges that
affect the generalization of range-view LiDAR segmentation in severe weather.
To address these challenges, we propose a modular and lightweight framework
that enhances robustness without altering the core architecture of existing
models. Our method reformulates the initial stem block of standard range-view
networks into two branches to process geometric attributes and reflectance
intensity separately. Specifically, a Geometric Abnormality Suppression (GAS)
module reduces the influence of weather-induced spatial noise, and a
Reflectance Distortion Calibration (RDC) module corrects reflectance
distortions through memory-guided adaptive instance normalization. The
processed features are then fused and passed to the original segmentation
pipeline. Extensive experiments on different benchmarks and baseline models
demonstrate that our approach significantly improves generalization to adverse
weather with minimal inference overhead, offering a practical and effective
solution for real-world LiDAR segmentation.

</details>


### [156] [Towards Class-wise Fair Adversarial Training via Anti-Bias Soft Label Distillation](https://arxiv.org/abs/2506.08611)
*Shiji Zhao,Chi Chen,Ranjie Duan,Xizhe Wang,Xingxing Wei*

Main category: cs.CV

TL;DR: 该论文提出了一种名为ABSLD的方法，通过调整软标签的平滑度来解决对抗训练中的公平性问题，显著提升了模型的鲁棒性和公平性。


<details>
  <summary>Details</summary>
Motivation: 对抗训练（AT）和对抗鲁棒性蒸馏（ARD）存在鲁棒公平性问题，即模型对某些类别的鲁棒性较强，而对其他类别的鲁棒性较弱。本文旨在探索这一问题的根源并提出解决方案。

Method: 提出了Anti-Bias Soft Label Distillation（ABSLD）方法，通过为不同类别分配不同的温度参数，调整软标签的平滑度，从而减少学生模型在不同类别间的误差风险差距。

Result: 实验表明，ABSLD在鲁棒性和公平性的综合表现上优于现有方法。

Conclusion: ABSLD是一种高效且适应性强的解决方案，能够显著提升对抗训练的公平性和鲁棒性。

Abstract: Adversarial Training (AT) is widely recognized as an effective approach to
enhance the adversarial robustness of Deep Neural Networks. As a variant of AT,
Adversarial Robustness Distillation (ARD) has shown outstanding performance in
enhancing the robustness of small models. However, both AT and ARD face robust
fairness issue: these models tend to display strong adversarial robustness
against some classes (easy classes) while demonstrating weak adversarial
robustness against others (hard classes). This paper explores the underlying
factors of this problem and points out the smoothness degree of soft labels for
different classes significantly impacts the robust fairness from both empirical
observation and theoretical analysis. Based on the above exploration, we
propose Anti-Bias Soft Label Distillation (ABSLD) within the Knowledge
Distillation framework to enhance the adversarial robust fairness.
Specifically, ABSLD adaptively reduces the student's error risk gap between
different classes, which is accomplished by adjusting the class-wise smoothness
degree of teacher's soft labels during the training process, and the adjustment
is managed by assigning varying temperatures to different classes.
Additionally, as a label-based approach, ABSLD is highly adaptable and can be
integrated with the sample-based methods. Extensive experiments demonstrate
ABSLD outperforms state-of-the-art methods on the comprehensive performance of
robustness and fairness.

</details>


### [157] [SDTagNet: Leveraging Text-Annotated Navigation Maps for Online HD Map Construction](https://arxiv.org/abs/2506.08997)
*Fabian Immel,Jan-Hendrik Pauls,Richard Fehler,Frank Bieder,Jonas Merkert,Christoph Stiller*

Main category: cs.CV

TL;DR: SDTagNet是一种在线高精地图构建方法，利用标准定义（SD）地图（如OpenStreetMap）提升远距离检测精度，结合语义信息和NLP特征，显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 高精（HD）地图维护成本高，在线构建方法受限于车载传感器感知范围短。利用SD地图作为先验信息，可以降低成本并提升性能。

Method: SDTagNet引入两个创新：1）结合SD地图的折线数据和文本注释的语义信息；2）使用点级SD地图编码器和正交元素标识符统一整合所有地图元素。

Result: 在Argoverse 2和nuScenes数据集上，性能提升显著：无先验方法提升5.9 mAP（45%），已有SD先验方法提升3.2 mAP（20%）。

Conclusion: SDTagNet通过充分利用SD地图信息，显著提升了在线高精地图构建的性能和实用性。

Abstract: Autonomous vehicles rely on detailed and accurate environmental information
to operate safely. High definition (HD) maps offer a promising solution, but
their high maintenance cost poses a significant barrier to scalable deployment.
This challenge is addressed by online HD map construction methods, which
generate local HD maps from live sensor data. However, these methods are
inherently limited by the short perception range of onboard sensors. To
overcome this limitation and improve general performance, recent approaches
have explored the use of standard definition (SD) maps as prior, which are
significantly easier to maintain. We propose SDTagNet, the first online HD map
construction method that fully utilizes the information of widely available SD
maps, like OpenStreetMap, to enhance far range detection accuracy. Our approach
introduces two key innovations. First, in contrast to previous work, we
incorporate not only polyline SD map data with manually selected classes, but
additional semantic information in the form of textual annotations. In this
way, we enrich SD vector map tokens with NLP-derived features, eliminating the
dependency on predefined specifications or exhaustive class taxonomies. Second,
we introduce a point-level SD map encoder together with orthogonal element
identifiers to uniformly integrate all types of map elements. Experiments on
Argoverse 2 and nuScenes show that this boosts map perception performance by up
to +5.9 mAP (+45%) w.r.t. map construction without priors and up to +3.2 mAP
(+20%) w.r.t. previous approaches that already use SD map priors. Code is
available at https://github.com/immel-f/SDTagNet

</details>


### [158] [Data-Efficient Challenges in Visual Inductive Priors: A Retrospective](https://arxiv.org/abs/2506.08612)
*Robert-Jan Bruintjes,Attila Lengyel,Osman Semih Kayhan,Davide Zambrano,Nergis Tömen,Hadi Jamali-Rad,Jan van Gemert*

Main category: cs.CV

TL;DR: 论文探讨了在数据不足情况下提升深度学习模型性能的方法，通过组织数据受限挑战赛，鼓励开发结合先验知识的新方法。


<details>
  <summary>Details</summary>
Motivation: 解决数据不足时深度学习模型性能下降的问题，推动数据高效深度学习的发展。

Method: 组织数据受限挑战赛，限制参与者使用少量样本从头训练模型，禁止迁移学习。

Result: 成功参赛方案采用混合Transformer和CNN的大型模型集成及数据增强，部分结合先验知识。

Conclusion: 结合先验知识和数据增强的方法在数据不足情况下能有效提升模型性能。

Abstract: Deep Learning requires large amounts of data to train models that work well.
In data-deficient settings, performance can be degraded. We investigate which
Deep Learning methods benefit training models in a data-deficient setting, by
organizing the "VIPriors: Visual Inductive Priors for Data-Efficient Deep
Learning" workshop series, featuring four editions of data-impaired challenges.
These challenges address the problem of training deep learning models for
computer vision tasks with limited data. Participants are limited to training
models from scratch using a low number of training samples and are not allowed
to use any form of transfer learning. We aim to stimulate the development of
novel approaches that incorporate prior knowledge to improve the data
efficiency of deep learning models. Successful challenge entries make use of
large model ensembles that mix Transformers and CNNs, as well as heavy data
augmentation. Novel prior knowledge-based methods contribute to success in some
entries.

</details>


### [159] [SAMSelect: A Spectral Index Search for Marine Debris Visualization using Segment Anything](https://arxiv.org/abs/2506.08613)
*Joost van Dalen,Yuki M. Asano,Marc Russwurm*

Main category: cs.CV

TL;DR: SAMSelect算法通过选择最佳分类准确率的波段组合，为多光谱图像提供显著的三通道可视化，帮助海洋科学家识别海洋垃圾。


<details>
  <summary>Details</summary>
Motivation: 海洋垃圾在中等分辨率图像中难以可视化，而专家通常依赖经验和启发式方法选择波段组合，SAMSelect旨在优化这一过程。

Method: 利用Segment Anything Model在小标注数据集上选择分类准确率最高的波段或指数组合，生成三通道可视化。

Result: 在加纳阿克拉和南非德班的Sentinel-2场景中测试，发现新波段组合（如B8和B2的归一化差异指数）优于文献中的方法。

Conclusion: SAMSelect为海洋领域科学家提供了一种有效的可视化工具，并开源了代码库。

Abstract: This work proposes SAMSelect, an algorithm to obtain a salient three-channel
visualization for multispectral images. We develop SAMSelect and show its use
for marine scientists visually interpreting floating marine debris in
Sentinel-2 imagery. These debris are notoriously difficult to visualize due to
their compositional heterogeneity in medium-resolution imagery. Out of these
difficulties, a visual interpretation of imagery showing marine debris remains
a common practice by domain experts, who select bands and spectral indices on a
case-by-case basis informed by common practices and heuristics. SAMSelect
selects the band or index combination that achieves the best classification
accuracy on a small annotated dataset through the Segment Anything Model. Its
central assumption is that the three-channel visualization achieves the most
accurate segmentation results also provide good visual information for
photo-interpretation.
  We evaluate SAMSelect in three Sentinel-2 scenes containing generic marine
debris in Accra, Ghana, and Durban, South Africa, and deployed plastic targets
from the Plastic Litter Project. This reveals the potential of new previously
unused band combinations (e.g., a normalized difference index of B8, B2), which
demonstrate improved performance compared to literature-based indices. We
describe the algorithm in this paper and provide an open-source code repository
that will be helpful for domain scientists doing visual photo interpretation,
especially in the marine field.

</details>


### [160] [A Probability-guided Sampler for Neural Implicit Surface Rendering](https://arxiv.org/abs/2506.08619)
*Gonçalo Dias Pais,Valter Piedade,Moitreya Chatterjee,Marcus Greiff,Pedro Miraldo*

Main category: cs.CV

TL;DR: 该论文提出了一种改进的NeRF方法，通过隐式表面表示和3D图像投影空间的概率密度函数实现更有针对性的采样，并结合新的表面重建损失，提升了3D重建和图像渲染的准确性。


<details>
  <summary>Details</summary>
Motivation: 现有NeRF方法由于可扩展性问题无法对所有可能的输入数据进行训练，导致采样效率低。本文旨在通过优化采样策略和引入新的损失函数，提高渲染和重建的精度。

Method: 利用隐式表面表示建模3D图像投影空间的概率密度函数，实现针对性采样；提出新的表面重建损失，结合近表面和空白空间信息。

Result: 通过集成新的采样策略和损失函数，显著提升了3D重建和图像渲染的准确性，尤其在感兴趣区域表现更优。

Conclusion: 本文提出的方法在NeRF框架中实现了更高效的采样和更精确的重建，为3D场景/对象的渲染和重建提供了新的优化方向。

Abstract: Several variants of Neural Radiance Fields (NeRFs) have significantly
improved the accuracy of synthesized images and surface reconstruction of 3D
scenes/objects. In all of these methods, a key characteristic is that none can
train the neural network with every possible input data, specifically, every
pixel and potential 3D point along the projection rays due to scalability
issues. While vanilla NeRFs uniformly sample both the image pixels and 3D
points along the projection rays, some variants focus only on guiding the
sampling of the 3D points along the projection rays. In this paper, we leverage
the implicit surface representation of the foreground scene and model a
probability density function in a 3D image projection space to achieve a more
targeted sampling of the rays toward regions of interest, resulting in improved
rendering. Additionally, a new surface reconstruction loss is proposed for
improved performance. This new loss fully explores the proposed 3D image
projection space model and incorporates near-to-surface and empty space
components. By integrating our novel sampling strategy and novel loss into
current state-of-the-art neural implicit surface renderers, we achieve more
accurate and detailed 3D reconstructions and improved image rendering,
especially for the regions of interest in any given scene.

</details>


### [161] [ECMNet:Lightweight Semantic Segmentation with Efficient CNN-Mamba Network](https://arxiv.org/abs/2506.08629)
*Feixiang Du,Shengkun Wu*

Main category: cs.CV

TL;DR: 论文提出了一种轻量级的CNN-Mamba网络（ECMNet）用于语义分割，结合CNN和Mamba的优势，设计了增强的双注意力块和多尺度注意力单元，显著提升了分割精度。


<details>
  <summary>Details</summary>
Motivation: 尽管CNN与Transformer模型在语义分割中表现优异，但全局上下文建模仍不足。Mamba在视觉任务中展现出长距离依赖建模的优势，因此结合两者以弥补各自的不足。

Method: ECMNet通过胶囊框架巧妙结合CNN与Mamba，设计了增强的双注意力块（EDAB）和多尺度注意力单元（MSAU），并利用Mamba增强的特征融合模块（FFM）整合多尺度特征。

Result: 在Cityscapes和CamVid数据集上分别达到70.6%和73.6%的mIoU，仅需0.87M参数和8.27G FLOPs。

Conclusion: ECMNet在精度和效率上取得了优异平衡，为语义分割任务提供了一种高效解决方案。

Abstract: In the past decade, Convolutional Neural Networks (CNNs) and Transformers
have achieved wide applicaiton in semantic segmentation tasks. Although CNNs
with Transformer models greatly improve performance, the global context
modeling remains inadequate. Recently, Mamba achieved great potential in vision
tasks, showing its advantages in modeling long-range dependency. In this paper,
we propose a lightweight Efficient CNN-Mamba Network for semantic segmentation,
dubbed as ECMNet. ECMNet combines CNN with Mamba skillfully in a capsule-based
framework to address their complementary weaknesses. Specifically, We design a
Enhanced Dual-Attention Block (EDAB) for lightweight bottleneck. In order to
improve the representations ability of feature, We devise a Multi-Scale
Attention Unit (MSAU) to integrate multi-scale feature aggregation, spatial
aggregation and channel aggregation. Moreover, a Mamba enhanced Feature Fusion
Module (FFM) merges diverse level feature, significantly enhancing segmented
accuracy. Extensive experiments on two representative datasets demonstrate that
the proposed model excels in accuracy and efficiency balance, achieving 70.6%
mIoU on Cityscapes and 73.6% mIoU on CamVid test datasets, with 0.87M
parameters and 8.27G FLOPs on a single RTX 3090 GPU platform.

</details>


### [162] [RoboSwap: A GAN-driven Video Diffusion Framework For Unsupervised Robot Arm Swapping](https://arxiv.org/abs/2506.08632)
*Yang Bai,Liudi Yang,George Eskandar,Fengyi Shen,Dong Chen,Mohammad Altillawi,Ziyuan Liu,Gitta Kutyniok*

Main category: cs.CV

TL;DR: RoboSwap提出了一种结合GAN和扩散模型的新框架，用于在未配对数据中替换视频中的机械臂，解决了跨平台机器人学习中的数据稀缺问题。


<details>
  <summary>Details</summary>
Motivation: 当前视频合成和编辑的生成模型虽先进，但高质量数据稀缺限制了视频条件机器人学习的跨平台泛化能力。

Method: RoboSwap通过分割机械臂背景，训练未配对GAN模型进行机械臂转换，再用扩散模型增强视频的连贯性和运动真实性。

Result: 实验表明，RoboSwap在三个基准测试中优于现有视频和图像编辑模型，结构一致性和运动一致性表现更优。

Conclusion: RoboSwap为机器人学习提供了可靠的跨平台数据生成解决方案。

Abstract: Recent advancements in generative models have revolutionized video synthesis
and editing. However, the scarcity of diverse, high-quality datasets continues
to hinder video-conditioned robotic learning, limiting cross-platform
generalization. In this work, we address the challenge of swapping a robotic
arm in one video with another: a key step for crossembodiment learning. Unlike
previous methods that depend on paired video demonstrations in the same
environmental settings, our proposed framework, RoboSwap, operates on unpaired
data from diverse environments, alleviating the data collection needs. RoboSwap
introduces a novel video editing pipeline integrating both GANs and diffusion
models, combining their isolated advantages. Specifically, we segment robotic
arms from their backgrounds and train an unpaired GAN model to translate one
robotic arm to another. The translated arm is blended with the original video
background and refined with a diffusion model to enhance coherence, motion
realism and object interaction. The GAN and diffusion stages are trained
independently. Our experiments demonstrate that RoboSwap outperforms
state-of-the-art video and image editing models on three benchmarks in terms of
both structural coherence and motion consistency, thereby offering a robust
solution for generating reliable, cross-embodiment data in robotic learning.

</details>


### [163] [SurfR: Surface Reconstruction with Multi-scale Attention](https://arxiv.org/abs/2506.08635)
*Siddhant Ranade,Gonçalo Dias Pais,Ross Tyler Whitaker,Jacinto C. Nascimento,Pedro Miraldo,Srikumar Ramalingam*

Main category: cs.CV

TL;DR: 提出了一种基于隐式表示的快速、准确的无组织点云表面重建算法，通过三个关键贡献实现了最佳精度-速度权衡。


<details>
  <summary>Details</summary>
Motivation: 现有学习方法要么需要针对单个对象训练的小模型（细节丰富但泛化性差），要么是泛化性好的大模型（细节不足且推理慢），需要一种兼顾速度与精度的新方法。

Method: 1. 延迟查询（lazy query）加速重建；2. 并行多尺度网格表示处理噪声和分辨率；3. 跨尺度注意力提升重建效果。

Result: 新方法在速度上优于所有基线模型，性能仅略低于最先进方法，实现了最佳精度-速度权衡。

Conclusion: 提出的隐式表示方法在速度和精度上均表现优异，适用于通用3D形状重建。

Abstract: We propose a fast and accurate surface reconstruction algorithm for
unorganized point clouds using an implicit representation. Recent learning
methods are either single-object representations with small neural models that
allow for high surface details but require per-object training or generalized
representations that require larger models and generalize to newer shapes but
lack details, and inference is slow. We propose a new implicit representation
for general 3D shapes that is faster than all the baselines at their optimum
resolution, with only a marginal loss in performance compared to the
state-of-the-art. We achieve the best accuracy-speed trade-off using three key
contributions. Many implicit methods extract features from the point cloud to
classify whether a query point is inside or outside the object. First, to speed
up the reconstruction, we show that this feature extraction does not need to
use the query point at an early stage (lazy query). Second, we use a parallel
multi-scale grid representation to develop robust features for different noise
levels and input resolutions. Finally, we show that attention across scales can
provide improved reconstruction results.

</details>


### [164] [Orientation Matters: Making 3D Generative Models Orientation-Aligned](https://arxiv.org/abs/2506.08640)
*Yichong Lu,Yuzhuo Tian,Zijin Jiang,Yikun Zhao,Yuanbo Yang,Hao Ouyang,Haoji Hu,Huimin Yu,Yujun Shen,Yiyi Liao*

Main category: cs.CV

TL;DR: 论文提出了一种解决3D生成模型方向不一致问题的方法，通过构建对齐数据集并微调模型，实现了跨类别的方向一致性生成。


<details>
  <summary>Details</summary>
Motivation: 现有3D生成模型因训练数据不一致导致生成结果方向错位，限制了其在下游任务中的应用。

Method: 构建Objaverse-OA数据集（14,832个对齐3D模型），并基于多视角扩散和3D变分自编码器框架微调模型。

Result: 实验表明，该方法优于后处理对齐方法，并能支持零样本方向估计和高效旋转操作等下游任务。

Conclusion: 通过方向对齐的3D生成，提升了模型的实用性和泛化能力。

Abstract: Humans intuitively perceive object shape and orientation from a single image,
guided by strong priors about canonical poses. However, existing 3D generative
models often produce misaligned results due to inconsistent training data,
limiting their usability in downstream tasks. To address this gap, we introduce
the task of orientation-aligned 3D object generation: producing 3D objects from
single images with consistent orientations across categories. To facilitate
this, we construct Objaverse-OA, a dataset of 14,832 orientation-aligned 3D
models spanning 1,008 categories. Leveraging Objaverse-OA, we fine-tune two
representative 3D generative models based on multi-view diffusion and 3D
variational autoencoder frameworks to produce aligned objects that generalize
well to unseen objects across various categories. Experimental results
demonstrate the superiority of our method over post-hoc alignment approaches.
Furthermore, we showcase downstream applications enabled by our aligned object
generation, including zero-shot object orientation estimation via
analysis-by-synthesis and efficient arrow-based object rotation manipulation.

</details>


### [165] [Enhancing Video Memorability Prediction with Text-Motion Cross-modal Contrastive Loss and Its Application in Video Summarization](https://arxiv.org/abs/2506.08649)
*Zhiyi Zhu,Xiaoyu Wu,Youwei Lu*

Main category: cs.CV

TL;DR: 提出了一种新的多模态视频记忆性预测模型TMCCL，通过文本-运动跨模态对比损失增强运动特征表示，并在视频摘要中应用记忆性加权校正（MWCVS）。


<details>
  <summary>Details</summary>
Motivation: 现有模型未能充分利用运动线索，且运动特征表示在微调阶段因缺乏标注数据而受损。

Method: 引入TMCCL，利用文本描述相似性构建正负运动样本集，提升运动特征表示；提出MWCVS，利用记忆性预测减少视频摘要标签的主观性。

Result: 在两个视频记忆性预测数据集上达到最优性能；MWCVS在两个视频摘要数据集上验证了其有效性。

Conclusion: TMCCL显著提升了运动特征表示，MWCVS展示了视频记忆性预测的潜在应用价值。

Abstract: Video memorability refers to the ability of videos to be recalled after
viewing, playing a crucial role in creating content that remains memorable.
Existing models typically focus on extracting multimodal features to predict
video memorability scores but often fail to fully utilize motion cues. The
representation of motion features is compromised during the fine-tuning phase
of the motion feature extractor due to a lack of labeled data. In this paper,
we introduce the Text-Motion Cross-modal Contrastive Loss (TMCCL), a multimodal
video memorability prediction model designed to enhance the representation of
motion features. We tackle the challenge of improving motion feature
representation by leveraging text description similarities across videos to
establish positive and negative motion sample sets for a given target. This
enhancement allows the model to learn similar feature representations for
semantically related motion content, resulting in more accurate memorability
predictions. Our model achieves state-of-the-art performance on two video
memorability prediction datasets. Moreover, the potential applications of video
memorability prediction have been underexplored. To address this gap, we
present Memorability Weighted Correction for Video Summarization (MWCVS), using
video memorability prediction to reduce subjectivity in video summarization
labels. Experimental results on two video summarization datasets demonstrate
the effectiveness of MWCVS, showcasing the promising applications of video
memorability prediction.

</details>


### [166] [Beyond Calibration: Physically Informed Learning for Raw-to-Raw Mapping](https://arxiv.org/abs/2506.08650)
*Peter Grönquist,Stepan Tulyakov,Dengxin Dai*

Main category: cs.CV

TL;DR: 论文提出了一种轻量级的神经物理模型（NPM），用于解决多相机间颜色一致性问题，适应性强且计算高效。


<details>
  <summary>Details</summary>
Motivation: 多相机系统中因传感器和光学差异导致颜色一致性难以实现，现有方法存在适应性差或计算成本高的问题。

Method: NPM通过模拟指定光照下的原始图像，估计设备间的转换关系，支持物理测量初始化及无配对数据训练。

Result: 在NUS和BeyondRGB数据集上，NPM优于现有方法，实现了跨传感器和光学系统的稳健颜色一致性。

Conclusion: NPM为多相机颜色一致性提供了一种高效、适应性强的解决方案。

Abstract: Achieving consistent color reproduction across multiple cameras is essential
for seamless image fusion and Image Processing Pipeline (ISP) compatibility in
modern devices, but it is a challenging task due to variations in sensors and
optics. Existing raw-to-raw conversion methods face limitations such as poor
adaptability to changing illumination, high computational costs, or impractical
requirements such as simultaneous camera operation and overlapping
fields-of-view. We introduce the Neural Physical Model (NPM), a lightweight,
physically-informed approach that simulates raw images under specified
illumination to estimate transformations between devices. The NPM effectively
adapts to varying illumination conditions, can be initialized with physical
measurements, and supports training with or without paired data. Experiments on
public datasets like NUS and BeyondRGB demonstrate that NPM outperforms recent
state-of-the-art methods, providing robust chromatic consistency across
different sensors and optical systems.

</details>


### [167] [LLaVA-c: Continual Improved Visual Instruction Tuning](https://arxiv.org/abs/2506.08666)
*Wenzhuo Liu,Fei Zhu,Haiyang Guo,Longhui Wei,Cheng-Lin Liu*

Main category: cs.CV

TL;DR: LLaVA-c通过谱感知巩固和无监督查询正则化改进LLaVA-1.5，解决了多任务学习中的任务平衡和基础模型退化问题，在持续学习中匹配或超越多任务联合学习效果。


<details>
  <summary>Details</summary>
Motivation: 多任务学习存在任务平衡和扩展成本问题，持续学习虽能增量获取知识但易导致基础模型退化。

Method: 在LLaVA-1.5基础上引入谱感知巩固和无监督查询正则化。

Result: LLaVA-c在持续预训练和微调中均提升性能并保留通用能力，首次证明持续学习可媲美多任务联合学习。

Conclusion: LLaVA-c为持续学习提供了一种简单有效的方法，解决了多任务学习的核心挑战。

Abstract: Multimodal models like LLaVA-1.5 achieve state-of-the-art visual
understanding through visual instruction tuning on multitask datasets, enabling
strong instruction-following and multimodal performance. However, multitask
learning faces challenges such as task balancing, requiring careful adjustment
of data proportions, and expansion costs, where new tasks risk catastrophic
forgetting and need costly retraining. Continual learning provides a promising
alternative to acquiring new knowledge incrementally while preserving existing
capabilities. However, current methods prioritize task-specific performance,
neglecting base model degradation from overfitting to specific instructions,
which undermines general capabilities. In this work, we propose a simple but
effective method with two modifications on LLaVA-1.5: spectral-aware
consolidation for improved task balance and unsupervised inquiry regularization
to prevent base model degradation. We evaluate both general and task-specific
performance across continual pretraining and fine-tuning. Experiments
demonstrate that LLaVA-c consistently enhances standard benchmark performance
and preserves general capabilities. For the first time, we show that
task-by-task continual learning can achieve results that match or surpass
multitask joint learning. The code will be publicly released.

</details>


### [168] [ATAS: Any-to-Any Self-Distillation for Enhanced Open-Vocabulary Dense Prediction](https://arxiv.org/abs/2506.08678)
*Juan Yeo,Soonwoo Cha,Jiwoo Song,Hyunbin Jin,Taesup Kim*

Main category: cs.CV

TL;DR: ATAS通过自蒸馏方法提升CLIP模型的语义一致性和细粒度对齐，无需额外模块或有监督微调，显著提升了开放词汇密集预测任务的性能。


<details>
  <summary>Details</summary>
Motivation: CLIP在细粒度和区域级理解上表现不足，影响了其在密集预测任务中的效果。

Method: 提出Any-to-Any Self-Distillation (ATAS)，利用模型自身知识跨所有表示层级增强语义一致性和细粒度对齐，仅需无标签图像和内部自蒸馏过程。

Result: 在开放词汇目标检测和语义分割任务中，ATAS显著优于基线CLIP模型。

Conclusion: ATAS验证了同时保持语义一致性和细粒度对齐对提升开放词汇密集预测任务的重要性。

Abstract: Vision-language models such as CLIP have recently propelled open-vocabulary
dense prediction tasks by enabling recognition of a broad range of visual
concepts. However, CLIP still struggles with fine-grained, region-level
understanding, hindering its effectiveness on these dense prediction tasks. We
identify two pivotal factors required to address this limitation: semantic
coherence and fine-grained vision-language alignment. Current adaptation
methods often improve fine-grained alignment at the expense of semantic
coherence, and often rely on extra modules or supervised fine-tuning. To
overcome these issues, we propose Any-to-Any Self-Distillation (ATAS), a novel
approach that simultaneously enhances semantic coherence and fine-grained
alignment by leveraging own knowledge of a model across all representation
levels. Unlike prior methods, ATAS uses only unlabeled images and an internal
self-distillation process to refine representations of CLIP vision encoders,
preserving local semantic consistency while sharpening local detail
recognition. On open-vocabulary object detection and semantic segmentation
benchmarks, ATAS achieves substantial performance gains, outperforming baseline
CLIP models. These results validate the effectiveness of our approach and
underscore the importance of jointly maintaining semantic coherence and
fine-grained alignment for advanced open-vocabulary dense prediction.

</details>


### [169] [CanadaFireSat: Toward high-resolution wildfire forecasting with multiple modalities](https://arxiv.org/abs/2506.08690)
*Hugo Porta,Emanuele Dalsasso,Jessica L. McCarty,Devis Tuia*

Main category: cs.CV

TL;DR: 加拿大2023年经历了近年来最严重的野火季节，凸显了气候变化对火灾季节长度和严重性的影响。本文提出高分辨率野火预测方法，利用多模态数据提升预测精度。


<details>
  <summary>Details</summary>
Motivation: 气候变化导致野火季节延长和加剧，亟需更好的减灾工具。高分辨率野火概率图对管理至关重要。

Method: 利用Sentinel-2、MODIS和ERA5等多模态数据，结合深度学习模型，开发100米分辨率的野火预测方法。

Result: 多模态输入显著优于单模态输入，2023年野火季节的F1分数达60.3%。

Conclusion: 多模态深度学习模型在高分辨率和大陆尺度野火预测中具有潜力。

Abstract: Canada experienced in 2023 one of the most severe wildfire seasons in recent
history, causing damage across ecosystems, destroying communities, and emitting
large quantities of CO2. This extreme wildfire season is symptomatic of a
climate-change-induced increase in the length and severity of the fire season
that affects the boreal ecosystem. Therefore, it is critical to empower
wildfire management in boreal communities with better mitigation solutions.
Wildfire probability maps represent an important tool for understanding the
likelihood of wildfire occurrence and the potential severity of future
wildfires. The massive increase in the availability of Earth observation data
has enabled the development of deep learning-based wildfire forecasting models,
aiming at providing precise wildfire probability maps at different spatial and
temporal scales. A main limitation of such methods is their reliance on
coarse-resolution environmental drivers and satellite products, leading to
wildfire occurrence prediction of reduced resolution, typically around $\sim
0.1${\deg}. This paper presents a benchmark dataset: CanadaFireSat, and
baseline methods for high-resolution: 100 m wildfire forecasting across Canada,
leveraging multi-modal data from high-resolution multi-spectral satellite
images (Sentinel-2 L1C), mid-resolution satellite products (MODIS), and
environmental factors (ERA5 reanalysis data). Our experiments consider two
major deep learning architectures. We observe that using multi-modal temporal
inputs outperforms single-modal temporal inputs across all metrics, achieving a
peak performance of 60.3% in F1 score for the 2023 wildfire season, a season
never seen during model training. This demonstrates the potential of
multi-modal deep learning models for wildfire forecasting at high-resolution
and continental scale.

</details>


### [170] [VReST: Enhancing Reasoning in Large Vision-Language Models through Tree Search and Self-Reward Mechanism](https://arxiv.org/abs/2506.08691)
*Congzhi Zhang,Jiawei Peng,Zhenglin Wang,Yilong Lai,Haowen Sun,Heng Chang,Fei Ma,Weijiang Yu*

Main category: cs.CV

TL;DR: VReST是一种无需训练的增强大型视觉语言模型（LVLM）推理能力的方法，通过蒙特卡洛树搜索和自奖励机制，显著提升了复杂视觉推理任务的性能。


<details>
  <summary>Details</summary>
Motivation: 当前LVLM在复杂视觉推理任务中的表现受限，尤其是使用思维链提示技术时，需要更高效的推理方法。

Method: 提出VReST，结合蒙特卡洛树搜索和自奖励机制，构建推理树并评估推理步骤质量。

Result: VReST在三个多模态数学推理基准测试中表现最优，验证了测试时扩展定律的有效性。

Conclusion: VReST为多模态任务提供了一种高效推理方法，并指明了未来研究方向。

Abstract: Large Vision-Language Models (LVLMs) have shown exceptional performance in
multimodal tasks, but their effectiveness in complex visual reasoning is still
constrained, especially when employing Chain-of-Thought prompting techniques.
In this paper, we propose VReST, a novel training-free approach that enhances
Reasoning in LVLMs through Monte Carlo Tree Search and Self-Reward mechanisms.
VReST meticulously traverses the reasoning landscape by establishing a search
tree, where each node encapsulates a reasoning step, and each path delineates a
comprehensive reasoning sequence. Our innovative multimodal Self-Reward
mechanism assesses the quality of reasoning steps by integrating the utility of
sub-questions, answer correctness, and the relevance of vision-language clues,
all without the need for additional models. VReST surpasses current prompting
methods and secures state-of-the-art performance across three multimodal
mathematical reasoning benchmarks. Furthermore, it substantiates the efficacy
of test-time scaling laws in multimodal tasks, offering a promising direction
for future research.

</details>


### [171] [MoSiC: Optimal-Transport Motion Trajectory for Dense Self-Supervised Learning](https://arxiv.org/abs/2506.08694)
*Mohammadreza Salehi,Shashanka Venkataramanan,Ioana Simion,Efstratios Gavves,Cees G. M. Snoek,Yuki M Asano*

Main category: cs.CV

TL;DR: 提出了一种基于运动引导的自监督学习框架，通过聚类密集点轨迹学习时空一致的表示，提升了动态场景和遮挡情况下的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖静态增强，难以处理物体变形、遮挡和相机运动，导致时间上特征学习不一致。

Method: 利用现有点跟踪器提取长程运动轨迹，通过动量编码器的最优传输机制优化特征聚类，并沿跟踪点传播聚类分配以确保时间一致性。

Result: 在六个图像和视频数据集及四个评估基准上，性能提升了1%至6%。

Conclusion: 通过运动作为隐式监督信号，该方法学习到跨帧泛化的表示，显著提升了动态场景下的性能。

Abstract: Dense self-supervised learning has shown great promise for learning pixel-
and patch-level representations, but extending it to videos remains challenging
due to the complexity of motion dynamics. Existing approaches struggle as they
rely on static augmentations that fail under object deformations, occlusions,
and camera movement, leading to inconsistent feature learning over time. We
propose a motion-guided self-supervised learning framework that clusters dense
point tracks to learn spatiotemporally consistent representations. By
leveraging an off-the-shelf point tracker, we extract long-range motion
trajectories and optimize feature clustering through a momentum-encoder-based
optimal transport mechanism. To ensure temporal coherence, we propagate cluster
assignments along tracked points, enforcing feature consistency across views
despite viewpoint changes. Integrating motion as an implicit supervisory
signal, our method learns representations that generalize across frames,
improving robustness in dynamic scenes and challenging occlusion scenarios. By
initializing from strong image-pretrained models and leveraging video data for
training, we improve state-of-the-art by 1% to 6% on six image and video
datasets and four evaluation benchmarks. The implementation is publicly
available at our GitHub repository: https://github.com/SMSD75/MoSiC/tree/main

</details>


### [172] [ArrowPose: Segmentation, Detection, and 5 DoF Pose Estimation Network for Colorless Point Clouds](https://arxiv.org/abs/2506.08699)
*Frederik Hagelskjaer*

Main category: cs.CV

TL;DR: 提出了一种快速检测和5自由度姿态估计网络，适用于无色点云，性能优于现有方法，推理速度快。


<details>
  <summary>Details</summary>
Motivation: 解决无色点云中物体姿态估计的效率和精度问题。

Method: 通过神经网络预测物体的中心和顶部点来计算姿态，使用合成数据训练。

Result: 在基准数据集上表现最优，推理时间仅250毫秒。

Conclusion: 该方法高效且实用，适用于多种场景。

Abstract: This paper presents a fast detection and 5 DoF (Degrees of Freedom) pose
estimation network for colorless point clouds. The pose estimation is
calculated from center and top points of the object, predicted by the neural
network. The network is trained on synthetic data, and tested on a benchmark
dataset, where it demonstrates state-of-the-art performance and outperforms all
colorless methods. The network is able to run inference in only 250
milliseconds making it usable in many scenarios. Project page with code at
arrowpose.github.io

</details>


### [173] [TraGraph-GS: Trajectory Graph-based Gaussian Splatting for Arbitrary Large-Scale Scene Rendering](https://arxiv.org/abs/2506.08704)
*Xiaohan Zhang,Sitong Wang,Yushen Yan,Yi Yang,Mingda Xu,Qi Liu*

Main category: cs.CV

TL;DR: 论文提出TraGraph-GS方法，通过轨迹图解决大规模场景中高质量新视角合成的挑战，显著提升了渲染精度和效率。


<details>
  <summary>Details</summary>
Motivation: 现有方法在大规模场景中因刚性空间分区和高斯重叠问题导致渲染效果不佳，无法适应任意相机轨迹。

Method: 采用基于图的动态空间分区方法，结合正则化约束和渐进式渲染策略，减少高斯重叠并提升纹理和远距离物体渲染质量。

Result: 在四个空中和四个地面数据集上，PSNR平均提升1.86 dB和1.62 dB，优于现有方法。

Conclusion: TraGraph-GS通过轨迹图和优化策略，有效解决了大规模场景的渲染问题，具有显著性能优势。

Abstract: High-quality novel view synthesis for large-scale scenes presents a
challenging dilemma in 3D computer vision. Existing methods typically partition
large scenes into multiple regions, reconstruct a 3D representation using
Gaussian splatting for each region, and eventually merge them for novel view
rendering. They can accurately render specific scenes, yet they do not
generalize effectively for two reasons: (1) rigid spatial partition techniques
struggle with arbitrary camera trajectories, and (2) the merging of regions
results in Gaussian overlap to distort texture details. To address these
challenges, we propose TraGraph-GS, leveraging a trajectory graph to enable
high-precision rendering for arbitrarily large-scale scenes. We present a
spatial partitioning method for large-scale scenes based on graphs, which
incorporates a regularization constraint to enhance the rendering of textures
and distant objects, as well as a progressive rendering strategy to mitigate
artifacts caused by Gaussian overlap. Experimental results demonstrate its
superior performance both on four aerial and four ground datasets and highlight
its remarkable efficiency: our method achieves an average improvement of 1.86
dB in PSNR on aerial datasets and 1.62 dB on ground datasets compared to
state-of-the-art approaches.

</details>


### [174] [SceneSplat++: A Large Dataset and Comprehensive Benchmark for Language Gaussian Splatting](https://arxiv.org/abs/2506.08710)
*Mengjiao Ma,Qi Ma,Yue Li,Jiahuan Cheng,Runyi Yang,Bin Ren,Nikola Popovic,Mingqiang Wei,Nicu Sebe,Luc Van Gool,Theo Gevers,Martin R. Oswald,Danda Pani Paudel*

Main category: cs.CV

TL;DR: 论文提出了首个大规模基准测试，系统评估3D高斯溅射（3DGS）在3D空间中的表现，并引入GaussianWorld-49K数据集以推动通用3DGS场景理解研究。


<details>
  <summary>Details</summary>
Motivation: 当前3DGS方法多局限于2D视图评估，缺乏对整体3D理解的深入分析，亟需系统性3D空间评估。

Method: 提出大规模基准测试，评估三类3DGS方法（基于优化、无优化、通用化），覆盖1060个场景和多个数据集。

Result: 通用化方法在场景适应性和分割性能上表现最优，且能快速推理新场景。

Conclusion: 通用化3DGS方法潜力显著，新数据集和基准将加速相关研究。

Abstract: 3D Gaussian Splatting (3DGS) serves as a highly performant and efficient
encoding of scene geometry, appearance, and semantics. Moreover, grounding
language in 3D scenes has proven to be an effective strategy for 3D scene
understanding. Current Language Gaussian Splatting line of work fall into three
main groups: (i) per-scene optimization-based, (ii) per-scene
optimization-free, and (iii) generalizable approach. However, most of them are
evaluated only on rendered 2D views of a handful of scenes and viewpoints close
to the training views, limiting ability and insight into holistic 3D
understanding. To address this gap, we propose the first large-scale benchmark
that systematically assesses these three groups of methods directly in 3D
space, evaluating on 1060 scenes across three indoor datasets and one outdoor
dataset. Benchmark results demonstrate a clear advantage of the generalizable
paradigm, particularly in relaxing the scene-specific limitation, enabling fast
feed-forward inference on novel scenes, and achieving superior segmentation
performance. We further introduce GaussianWorld-49K a carefully curated 3DGS
dataset comprising around 49K diverse indoor and outdoor scenes obtained from
multiple sources, with which we demonstrate the generalizable approach could
harness strong data priors. Our codes, benchmark, and datasets will be made
public to accelerate research in generalizable 3DGS scene understanding.

</details>


### [175] [Geometric deep learning for local growth prediction on abdominal aortic aneurysm surfaces](https://arxiv.org/abs/2506.08729)
*Dieuwertje Alblas,Patryk Rygiel,Julian Suk,Kaj O. Kappe,Marieke Hofman,Christoph Brune,Kak Khee Yeung,Jelmer M. Wolterink*

Main category: cs.CV

TL;DR: 该论文提出了一种基于SE(3)-对称变换器模型的方法，用于预测腹主动脉瘤（AAA）的生长，通过保留血管表面的解剖结构和几何保真度，实现个性化监测策略。


<details>
  <summary>Details</summary>
Motivation: 当前基于AAA直径的监测策略未考虑3D形状与生长的复杂关系，可能导致标准化监测间隔不适用。个性化生长预测可优化监测。

Method: 使用SE(3)-对称变换器模型，直接在血管模型表面结合局部多物理特征预测AAA生长。训练数据为24名患者的113次CTA扫描。

Result: 模型预测下一扫描时刻的AAA生长，中位直径误差为1.18毫米，并能以93%的准确率识别患者是否在两年内需手术修复。外部验证集表现良好。

Conclusion: 局部定向AAA生长预测可行，有助于个性化监测策略。

Abstract: Abdominal aortic aneurysms (AAAs) are progressive focal dilatations of the
abdominal aorta. AAAs may rupture, with a survival rate of only 20\%. Current
clinical guidelines recommend elective surgical repair when the maximum AAA
diameter exceeds 55 mm in men or 50 mm in women. Patients that do not meet
these criteria are periodically monitored, with surveillance intervals based on
the maximum AAA diameter. However, this diameter does not take into account the
complex relation between the 3D AAA shape and its growth, making standardized
intervals potentially unfit. Personalized AAA growth predictions could improve
monitoring strategies. We propose to use an SE(3)-symmetric transformer model
to predict AAA growth directly on the vascular model surface enriched with
local, multi-physical features. In contrast to other works which have
parameterized the AAA shape, this representation preserves the vascular
surface's anatomical structure and geometric fidelity. We train our model using
a longitudinal dataset of 113 computed tomography angiography (CTA) scans of 24
AAA patients at irregularly sampled intervals. After training, our model
predicts AAA growth to the next scan moment with a median diameter error of
1.18 mm. We further demonstrate our model's utility to identify whether a
patient will become eligible for elective repair within two years (acc = 0.93).
Finally, we evaluate our model's generalization on an external validation set
consisting of 25 CTAs from 7 AAA patients from a different hospital. Our
results show that local directional AAA growth prediction from the vascular
surface is feasible and may contribute to personalized surveillance strategies.

</details>


### [176] [InceptionMamba: An Efficient Hybrid Network with Large Band Convolution and Bottleneck Mamba](https://arxiv.org/abs/2506.08735)
*Yuhang Wang,Jun Li,Zhijian Wu,Jianhua Xu*

Main category: cs.CV

TL;DR: InceptionMamba提出了一种新的主干架构，通过正交带卷积和瓶颈Mamba模块解决了InceptionNeXt在空间依赖性和全局上下文建模上的不足，实现了高效的分类和下游任务性能。


<details>
  <summary>Details</summary>
Motivation: InceptionNeXt在图像分类和下游任务中表现优异，但其一维条带卷积限制了空间依赖性的捕捉和局部邻域的空间建模能力，且卷积操作的局部性约束不利于全局上下文建模。

Method: 提出InceptionMamba架构，用正交带卷积替代传统一维条带卷积以实现空间建模，并通过瓶颈Mamba模块实现全局上下文建模，增强跨通道信息融合和扩大感受野。

Result: 在分类和多种下游任务中，InceptionMamba表现出最先进的性能，同时具有优越的参数和计算效率。

Conclusion: InceptionMamba通过改进的空间和全局建模方法，显著提升了性能，为相关任务提供了高效解决方案。

Abstract: Within the family of convolutional neural networks, InceptionNeXt has shown
excellent competitiveness in image classification and a number of downstream
tasks. Built on parallel one-dimensional strip convolutions, however, it
suffers from limited ability of capturing spatial dependencies along different
dimensions and fails to fully explore spatial modeling in local neighborhood.
Besides, inherent locality constraints of convolution operations are
detrimental to effective global context modeling. To overcome these
limitations, we propose a novel backbone architecture termed InceptionMamba in
this study. More specifically, the traditional one-dimensional strip
convolutions are replaced by orthogonal band convolutions in our InceptionMamba
to achieve cohesive spatial modeling. Furthermore, global contextual modeling
can be achieved via a bottleneck Mamba module, facilitating enhanced
cross-channel information fusion and enlarged receptive field. Extensive
evaluations on classification and various downstream tasks demonstrate that the
proposed InceptionMamba achieves state-of-the-art performance with superior
parameter and computational efficiency. The source code will be available at
https://github.com/Wake1021/InceptionMamba.

</details>


### [177] [RS-MTDF: Multi-Teacher Distillation and Fusion for Remote Sensing Semi-Supervised Semantic Segmentation](https://arxiv.org/abs/2506.08772)
*Jiayi Song,Kaiyu Li,Xiangyong Cao,Deyu Meng*

Main category: cs.CV

TL;DR: 论文提出了一种名为RS-MTDF的半监督语义分割框架，利用预训练的视觉基础模型（VFMs）作为多教师，通过特征级蒸馏和知识融合提升遥感图像分割性能。


<details>
  <summary>Details</summary>
Motivation: 遥感图像的语义分割依赖大量标注数据，但标注成本高。半监督方法虽能缓解这一问题，但现有方法在分布不匹配时泛化能力不足。

Method: RS-MTDF框架利用多个冻结的VFMs（如DINOv2和CLIP）作为教师模型，通过特征蒸馏对齐学生模型特征，并将知识融合到解码器中。

Result: 在ISPRS Potsdam、LoveDA和DeepGlobe数据集上，RS-MTDF表现最优，尤其在LoveDA上优于现有方法，并在多数语义类别中取得最高IoU。

Conclusion: 多教师VFM指导显著提升了遥感分割的泛化能力和语义理解，消融实验验证了各模块的有效性。

Abstract: Semantic segmentation in remote sensing images is crucial for various
applications, yet its performance is heavily reliant on large-scale,
high-quality pixel-wise annotations, which are notoriously expensive and
time-consuming to acquire. Semi-supervised semantic segmentation (SSS) offers a
promising alternative to mitigate this data dependency. However, existing SSS
methods often struggle with the inherent distribution mismatch between limited
labeled data and abundant unlabeled data, leading to suboptimal generalization.
We propose that Vision Foundation Models (VFMs), pre-trained on vast and
diverse datasets, possess robust generalization capabilities that can
effectively bridge this distribution gap and provide strong semantic priors for
SSS. Inspired by this, we introduce RS-MTDF (Multi-Teacher Distillation and
Fusion), a novel framework that leverages the powerful semantic knowledge
embedded in VFMs to guide semi-supervised learning in remote sensing.
Specifically, RS-MTDF employs multiple frozen VFMs (\textit{e.g.}, DINOv2 and
CLIP) as expert teachers, utilizing feature-level distillation to align student
features with their robust representations. To further enhance discriminative
power, the distilled knowledge is seamlessly fused into the student decoder.
Extensive experiments on three challenging remote sensing datasets (ISPRS
Potsdam, LoveDA, and DeepGlobe) demonstrate that RS-MTDF consistently achieves
state-of-the-art performance. Notably, our method outperforms existing
approaches across various label ratios on LoveDA and secures the highest IoU in
the majority of semantic categories. These results underscore the efficacy of
multi-teacher VFM guidance in significantly enhancing both generalization and
semantic understanding for remote sensing segmentation. Ablation studies
further validate the contribution of each proposed module.

</details>


### [178] [Gaussian2Scene: 3D Scene Representation Learning via Self-supervised Learning with 3D Gaussian Splatting](https://arxiv.org/abs/2506.08777)
*Keyi Liu,Weidong Yang,Ben Fei,Ying He*

Main category: cs.CV

TL;DR: 论文提出Gaussian2Scene，一种基于3D高斯泼溅（3DGS）的自监督学习框架，用于点云预训练，解决了现有方法依赖隐式场景表示和高内存需求的问题。


<details>
  <summary>Details</summary>
Motivation: 现有自监督学习方法在场景级别依赖RGB-D图像和体积渲染，存在内存需求高且难以捕捉3D几何结构的问题。

Method: 采用3DGS进行预训练，分两阶段训练：第一阶段通过双分支掩码自编码器学习2D和3D场景表示；第二阶段利用重建点云和高斯基元的几何位置进行监督学习。

Result: 在多个3D目标检测任务中表现优于现有预训练方法。

Conclusion: Gaussian2Scene通过显式3D重建和高效计算，提升了模型的几何理解和跨模态学习能力。

Abstract: Self-supervised learning (SSL) for point cloud pre-training has become a
cornerstone for many 3D vision tasks, enabling effective learning from
large-scale unannotated data. At the scene level, existing SSL methods often
incorporate volume rendering into the pre-training framework, using RGB-D
images as reconstruction signals to facilitate cross-modal learning. This
strategy promotes alignment between 2D and 3D modalities and enables the model
to benefit from rich visual cues in the RGB-D inputs. However, these approaches
are limited by their reliance on implicit scene representations and high memory
demands. Furthermore, since their reconstruction objectives are applied only in
2D space, they often fail to capture underlying 3D geometric structures. To
address these challenges, we propose Gaussian2Scene, a novel scene-level SSL
framework that leverages the efficiency and explicit nature of 3D Gaussian
Splatting (3DGS) for pre-training. The use of 3DGS not only alleviates the
computational burden associated with volume rendering but also supports direct
3D scene reconstruction, thereby enhancing the geometric understanding of the
backbone network. Our approach follows a progressive two-stage training
strategy. In the first stage, a dual-branch masked autoencoder learns both 2D
and 3D scene representations. In the second stage, we initialize training with
reconstructed point clouds and further supervise learning using the geometric
locations of Gaussian primitives and rendered RGB images. This process
reinforces both geometric and cross-modal learning. We demonstrate the
effectiveness of Gaussian2Scene across several downstream 3D object detection
tasks, showing consistent improvements over existing pre-training methods.

</details>


### [179] [Landsat-Bench: Datasets and Benchmarks for Landsat Foundation Models](https://arxiv.org/abs/2506.08780)
*Isaac Corley,Lakshay Sharma,Ruth Crasto*

Main category: cs.CV

TL;DR: Landsat-Bench是一套基于Landsat影像的基准测试，包含EuroSAT-L、BigEarthNet-L和LC100-L，用于评估地理空间基础模型（GFM）的性能。


<details>
  <summary>Details</summary>
Motivation: Landsat数据缺乏标准化基准，限制了基于Landsat的地理空间基础模型的发展。

Method: 通过Landsat-Bench基准测试，评估了SSL4EO-L预训练模型与常见架构的性能。

Result: SSL4EO-L预训练的GFM在下游任务中表现优于ImageNet，EuroSAT-L和BigEarthNet-L的准确率分别提升4%和5.1%。

Conclusion: Landsat-Bench为Landsat数据提供了标准化评估工具，SSL4EO-L预训练模型在GFM中表现更优。

Abstract: The Landsat program offers over 50 years of globally consistent Earth
imagery. However, the lack of benchmarks for this data constrains progress
towards Landsat-based Geospatial Foundation Models (GFM). In this paper, we
introduce Landsat-Bench, a suite of three benchmarks with Landsat imagery that
adapt from existing remote sensing datasets -- EuroSAT-L, BigEarthNet-L, and
LC100-L. We establish baseline and standardized evaluation methods across both
common architectures and Landsat foundation models pretrained on the SSL4EO-L
dataset. Notably, we provide evidence that SSL4EO-L pretrained GFMs extract
better representations for downstream tasks in comparison to ImageNet,
including performance gains of +4% OA and +5.1% mAP on EuroSAT-L and
BigEarthNet-L.

</details>


### [180] [HomographyAD: Deep Anomaly Detection Using Self Homography Learning](https://arxiv.org/abs/2506.08784)
*Jongyub Seok,Chanjin Kang*

Main category: cs.CV

TL;DR: 提出了一种基于ImageNet预训练网络的新方法HomographyAD，用于解决工业环境中数据未对齐的异常检测问题。


<details>
  <summary>Details</summary>
Motivation: 现有异常检测方法仅适用于完全对齐的数据集，而实际工业环境中数据往往未对齐，因此需要一种更适应实际场景的方法。

Method: 通过深度单应性估计方法对齐输入前景，并通过自单应性学习微调模型以提取正常样本的形状信息，最后基于测试样本特征与正常特征分布的距离进行异常检测。

Result: 实验表明，该方法能显著提升现有异常检测方法的性能。

Conclusion: HomographyAD是一种适用于实际工业数据集的有效异常检测方法。

Abstract: Anomaly detection (AD) is a task that distinguishes normal and abnormal data,
which is important for applying automation technologies of the manufacturing
facilities. For MVTec dataset that is a representative AD dataset for
industrial environment, many recent works have shown remarkable performances.
However, the existing anomaly detection works have a limitation of showing good
performance for fully-aligned datasets only, unlike real-world industrial
environments. To solve this limitation, we propose HomographyAD, a novel deep
anomaly detection methodology based on the ImageNet-pretrained network, which
is specially designed for actual industrial dataset. Specifically, we first
suggest input foreground alignment using the deep homography estimation method.
In addition, we fine-tune the model by self homography learning to learn
additional shape information from normal samples. Finally, we conduct anomaly
detection based on the measure of how far the feature of test sample is from
the distribution of the extracted normal features. By applying our proposed
method to various existing AD approaches, we show performance enhancement
through extensive experiments.

</details>


### [181] [A PDE-Based Image Dehazing Method via Atmospheric Scattering Theory](https://arxiv.org/abs/2506.08793)
*Zhuoran Zheng*

Main category: cs.CV

TL;DR: 提出了一种基于偏微分方程（PDE）的单幅图像去雾新框架，结合大气散射模型、非局部正则化和暗通道先验，改进了PDE模型，并通过理论证明和实验验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 解决单幅图像去雾问题，结合现有模型的优势，提出更高效的PDE框架。

Method: 改进的PDE模型，包含边缘保持扩散系数、高斯卷积算子和自适应正则化参数，理论证明弱解的存在唯一性，并采用GPU加速的固定点迭代实现。

Result: 实验证明该方法是一种有效的去雾解决方案，并可推广至深度学习模型。

Conclusion: 提出的PDE框架在单幅图像去雾中表现优异，具有理论和实践的双重价值。

Abstract: This paper presents a novel partial differential equation (PDE) framework for
single-image dehazing. By integrating the atmospheric scattering model with
nonlocal regularization and dark channel prior, we propose the improved PDE: \[
-\text{div}\left(D(\nabla u)\nabla u\right) + \lambda(t) G(u) = \Phi(I,t,A) \]
where $D(\nabla u) = (|\nabla u| + \epsilon)^{-1}$ is the edge-preserving
diffusion coefficient, $G(u)$ is the Gaussian convolution operator, and
$\lambda(t)$ is the adaptive regularization parameter based on transmission map
$t$. We prove the existence and uniqueness of weak solutions in $H_0^1(\Omega)$
using Lax-Milgram theorem, and implement an efficient fixed-point iteration
scheme accelerated by PyTorch GPU computation. The experimental results
demonstrate that this method is a promising deghazing solution that can be
generalized to the deep model paradigm.

</details>


### [182] [Flow Diverse and Efficient: Learning Momentum Flow Matching via Stochastic Velocity Field Sampling](https://arxiv.org/abs/2506.08796)
*Zhiyuan Ma,Ruixun Liu,Sixian Liu,Jianjun Li,Bowen Zhou*

Main category: cs.CV

TL;DR: Discretized-RF是一种新的整流流模型，通过将直线路径离散化为多段可变速度子路径，提升多样性和多尺度噪声建模能力。


<details>
  <summary>Details</summary>
Motivation: 传统整流流模型的直线路径限制了采样空间多样性，且难以处理多尺度噪声建模问题。

Method: 提出Discretized-RF，将路径离散化为动量场子路径，并在速度上引入噪声以改变方向。

Result: 实验表明，该方法能生成多样且高效的轨迹，提升生成结果的质量和多样性。

Conclusion: Discretized-RF通过优化速度场噪声，显著改善了整流流模型的性能。

Abstract: Recently, the rectified flow (RF) has emerged as the new state-of-the-art
among flow-based diffusion models due to its high efficiency advantage in
straight path sampling, especially with the amazing images generated by a
series of RF models such as Flux 1.0 and SD 3.0. Although a straight-line
connection between the noisy and natural data distributions is intuitive, fast,
and easy to optimize, it still inevitably leads to: 1) Diversity concerns,
which arise since straight-line paths only cover a fairly restricted sampling
space. 2) Multi-scale noise modeling concerns, since the straight line flow
only needs to optimize the constant velocity field $\bm v$ between the two
distributions $\bm\pi_0$ and $\bm\pi_1$. In this work, we present
Discretized-RF, a new family of rectified flow (also called momentum flow
models since they refer to the previous velocity component and the random
velocity component in each diffusion step), which discretizes the straight path
into a series of variable velocity field sub-paths (namely ``momentum fields'')
to expand the search space, especially when close to the distribution
$p_\text{noise}$. Different from the previous case where noise is directly
superimposed on $\bm x$, we introduce noise on the velocity $\bm v$ of the
sub-path to change its direction in order to improve the diversity and
multi-scale noise modeling abilities. Experimental results on several
representative datasets demonstrate that learning momentum flow matching by
sampling random velocity fields will produce trajectories that are both diverse
and efficient, and can consistently generate high-quality and diverse results.
Code is available at https://github.com/liuruixun/momentum-fm.

</details>


### [183] [HunyuanVideo-HOMA: Generic Human-Object Interaction in Multimodal Driven Human Animation](https://arxiv.org/abs/2506.08797)
*Ziyao Huang,Zixiang Zhou,Juan Cao,Yifeng Ma,Yi Chen,Zejing Rao,Zhiyong Xu,Hongmei Wang,Qin Lin,Yuan Zhou,Qinglin Lu,Fan Tang*

Main category: cs.CV

TL;DR: HunyuanVideo-HOMA是一个弱条件多模态驱动框架，旨在解决HOI视频生成中的关键限制，如依赖精选运动数据和泛化能力不足。


<details>
  <summary>Details</summary>
Motivation: 解决HOI视频生成中对精选运动数据的依赖、对新对象/场景的泛化能力有限以及可访问性受限的问题。

Method: 采用稀疏解耦的运动引导，将外观和运动信号编码到多模态扩散变换器（MMDiT）的双输入空间，并通过共享上下文空间融合，生成时间一致且物理合理的交互。

Result: 在弱监督下实现了交互自然性和泛化能力的先进性能，并展示了文本条件生成和交互式对象操作的多样性。

Conclusion: HunyuanVideo-HOMA通过创新的框架设计，显著提升了HOI视频生成的灵活性和性能。

Abstract: To address key limitations in human-object interaction (HOI) video generation
-- specifically the reliance on curated motion data, limited generalization to
novel objects/scenarios, and restricted accessibility -- we introduce
HunyuanVideo-HOMA, a weakly conditioned multimodal-driven framework.
HunyuanVideo-HOMA enhances controllability and reduces dependency on precise
inputs through sparse, decoupled motion guidance. It encodes appearance and
motion signals into the dual input space of a multimodal diffusion transformer
(MMDiT), fusing them within a shared context space to synthesize temporally
consistent and physically plausible interactions. To optimize training, we
integrate a parameter-space HOI adapter initialized from pretrained MMDiT
weights, preserving prior knowledge while enabling efficient adaptation, and a
facial cross-attention adapter for anatomically accurate audio-driven lip
synchronization. Extensive experiments confirm state-of-the-art performance in
interaction naturalness and generalization under weak supervision. Finally,
HunyuanVideo-HOMA demonstrates versatility in text-conditioned generation and
interactive object manipulation, supported by a user-friendly demo interface.
The project page is at https://anonymous.4open.science/w/homa-page-0FBE/.

</details>


### [184] [HiSin: Efficient High-Resolution Sinogram Inpainting via Resolution-Guided Progressive Inference](https://arxiv.org/abs/2506.08809)
*Jiaze E,Srutarshi Banerjee,Tekin Bicer,Guannan Wang,Yanfu Zhang,Bin Ren*

Main category: cs.CV

TL;DR: HiSin是一种基于扩散模型的高效正弦图修复框架，通过分辨率引导的渐进推理降低内存和计算需求。


<details>
  <summary>Details</summary>
Motivation: 高分辨率正弦图修复对CT重建至关重要，但现有扩散模型因内存和计算需求过高而受限。

Method: HiSin采用分辨率引导的渐进推理，先在低分辨率提取全局结构，再在高分辨率处理小补丁，并结合频率感知补丁跳过和结构自适应步长分配。

Result: 实验显示，HiSin将峰值内存使用降低31.25%，推理时间减少18.15%，同时保持修复精度。

Conclusion: HiSin是一种高效且精确的正弦图修复方法，适用于不同数据集、分辨率和掩码条件。

Abstract: High-resolution sinogram inpainting is essential for computed tomography
reconstruction, as missing high-frequency projections can lead to visible
artifacts and diagnostic errors. Diffusion models are well-suited for this task
due to their robustness and detail-preserving capabilities, but their
application to high-resolution inputs is limited by excessive memory and
computational demands. To address this limitation, we propose HiSin, a novel
diffusion based framework for efficient sinogram inpainting via
resolution-guided progressive inference. It progressively extracts global
structure at low resolution and defers high-resolution inference to small
patches, enabling memory-efficient inpainting. It further incorporates
frequency-aware patch skipping and structure-adaptive step allocation to reduce
redundant computation. Experimental results show that HiSin reduces peak memory
usage by up to 31.25% and inference time by up to 18.15%, and maintains
inpainting accuracy across datasets, resolutions, and mask conditions.

</details>


### [185] [Video-CoT: A Comprehensive Dataset for Spatiotemporal Understanding of Videos Based on Chain-of-Thought](https://arxiv.org/abs/2506.08817)
*Shuyi Zhang,Xiaoshuai Hao,Yingbo Tang,Lingfeng Zhang,Pengwei Wang,Zhongyuan Wang,Hongxuan Ma,Shanghang Zhang*

Main category: cs.CV

TL;DR: Video-CoT是一个新的数据集和基准测试，旨在通过Chain-of-Thought方法提升视频内容理解的时空细节捕捉能力。


<details>
  <summary>Details</summary>
Motivation: 现有的大规模视觉语言模型在视频分析中难以捕捉时空细节，需要更精细的数据支持。

Method: 引入Video-CoT数据集，包含192,000个时空问答对和23,000个CoT标注样本，并提供基准测试。

Result: 实验显示当前模型在时空理解任务中表现不佳，凸显了任务的挑战性。

Conclusion: Video-CoT为多媒体理解和智能系统研究提供了新方向，资源公开以促进进一步探索。

Abstract: Video content comprehension is essential for various applications, ranging
from video analysis to interactive systems. Despite advancements in large-scale
vision-language models (VLMs), these models often struggle to capture the
nuanced, spatiotemporal details essential for thorough video analysis. To
address this gap, we introduce Video-CoT, a groundbreaking dataset designed to
enhance spatiotemporal understanding using Chain-of-Thought (CoT)
methodologies. Video-CoT contains 192,000 fine-grained spa-tiotemporal
question-answer pairs and 23,000 high-quality CoT-annotated samples, providing
a solid foundation for evaluating spatiotemporal understanding in video
comprehension. Additionally, we provide a comprehensive benchmark for assessing
these tasks, with each task featuring 750 images and tailored evaluation
metrics. Our extensive experiments reveal that current VLMs face significant
challenges in achieving satisfactory performance, high-lighting the
difficulties of effective spatiotemporal understanding. Overall, the Video-CoT
dataset and benchmark open new avenues for research in multimedia understanding
and support future innovations in intelligent systems requiring advanced video
analysis capabilities. By making these resources publicly available, we aim to
encourage further exploration in this critical area. Project
website:https://video-cot.github.io/ .

</details>


### [186] [CulturalFrames: Assessing Cultural Expectation Alignment in Text-to-Image Models and Evaluation Metrics](https://arxiv.org/abs/2506.08835)
*Shravan Nayak,Mehar Bhatia,Xiaofeng Zhang,Verena Rieser,Lisa Anne Hendricks,Sjoerd van Steenkiste,Yash Goyal,Karolina Stańczak,Aishwarya Agrawal*

Main category: cs.CV

TL;DR: 该研究首次系统量化了文本到图像（T2I）模型在文化和隐性文化期望上的表现，发现模型在显性和隐性文化期望上的失败率分别为68%和49%，且现有评估指标与人类判断相关性差。


<details>
  <summary>Details</summary>
Motivation: 随着T2I模型在视觉内容生成中的普及，其能否准确代表多元文化背景成为关注焦点。

Method: 研究引入CulturalFrames基准，涵盖10个国家、5个社会文化领域，包含983个提示、3637张图像和10k+人类标注，用于评估文化代表性。

Result: T2I模型在显性和隐性文化期望上的失败率分别为68%和49%，且现有评估指标与人类判断相关性差。

Conclusion: 研究揭示了T2I模型在文化代表性上的重大缺陷，为开发更具文化敏感性的模型和评估方法提供了方向。

Abstract: The increasing ubiquity of text-to-image (T2I) models as tools for visual
content generation raises concerns about their ability to accurately represent
diverse cultural contexts. In this work, we present the first study to
systematically quantify the alignment of T2I models and evaluation metrics with
respect to both explicit as well as implicit cultural expectations. To this
end, we introduce CulturalFrames, a novel benchmark designed for rigorous human
evaluation of cultural representation in visual generations. Spanning 10
countries and 5 socio-cultural domains, CulturalFrames comprises 983 prompts,
3637 corresponding images generated by 4 state-of-the-art T2I models, and over
10k detailed human annotations. We find that T2I models not only fail to meet
the more challenging implicit expectations but also the less challenging
explicit expectations. Across models and countries, cultural expectations are
missed an average of 44% of the time. Among these failures, explicit
expectations are missed at a surprisingly high average rate of 68%, while
implicit expectation failures are also significant, averaging 49%. Furthermore,
we demonstrate that existing T2I evaluation metrics correlate poorly with human
judgments of cultural alignment, irrespective of their internal reasoning.
Collectively, our findings expose critical gaps, providing actionable
directions for developing more culturally informed T2I models and evaluation
methodologies.

</details>


### [187] [Adapting Vision-Language Foundation Model for Next Generation Medical Ultrasound Image Analysis](https://arxiv.org/abs/2506.08849)
*Jingguo Qu,Xinyang Han,Tonghuan Xiao,Jia Ai,Juan Wu,Tong Zhao,Jing Qin,Ann Dorothy King,Winnie Chiu-Wing Chu,Jing Cai,Michael Tin-Cheung Yingınst*

Main category: cs.CV

TL;DR: 该研究提出了一种针对医学超声图像的视觉-语言基础模型领域适应方法，通过微调流程和任务驱动头提升性能，实验表明其优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 医学超声图像分析中手动标注耗时且不一致，视觉-语言基础模型在医学领域表现受限，需解决领域差异问题。

Method: 利用大语言模型作为文本细化器，结合特殊设计的适应策略和任务驱动头，对视觉-语言基础模型进行微调。

Result: 在六个超声数据集和两项任务（分割与分类）上验证，性能优于现有视觉-语言及纯基础模型。

Conclusion: 该方法有效提升了视觉-语言基础模型在超声图像分析中的性能，具有实际应用潜力。

Abstract: Medical ultrasonography is an essential imaging technique for examining
superficial organs and tissues, including lymph nodes, breast, and thyroid. It
employs high-frequency ultrasound waves to generate detailed images of the
internal structures of the human body. However, manually contouring regions of
interest in these images is a labor-intensive task that demands expertise and
often results in inconsistent interpretations among individuals.
Vision-language foundation models, which have excelled in various computer
vision applications, present new opportunities for enhancing ultrasound image
analysis. Yet, their performance is hindered by the significant differences
between natural and medical imaging domains. This research seeks to overcome
these challenges by developing domain adaptation methods for vision-language
foundation models. In this study, we explore the fine-tuning pipeline for
vision-language foundation models by utilizing large language model as text
refiner with special-designed adaptation strategies and task-driven heads. Our
approach has been extensively evaluated on six ultrasound datasets and two
tasks: segmentation and classification. The experimental results show that our
method can effectively improve the performance of vision-language foundation
models for ultrasound image analysis, and outperform the existing
state-of-the-art vision-language and pure foundation models. The source code of
this study is available at
\href{https://github.com/jinggqu/NextGen-UIA}{GitHub}.

</details>


### [188] [Spatial Transcriptomics Expression Prediction from Histopathology Based on Cross-Modal Mask Reconstruction and Contrastive Learning](https://arxiv.org/abs/2506.08854)
*Junzhuo Liu,Markus Eckstein,Zhixiang Wang,Friedrich Feuerhake,Dorit Merhof*

Main category: cs.CV

TL;DR: 本文提出了一种基于对比学习的深度学习方法，用于从全切片图像预测空间分辨基因表达，显著提高了基因表达预测的准确性。


<details>
  <summary>Details</summary>
Motivation: 空间转录组学数据获取成本高，大规模数据难以获得，因此需要开发高效的方法从小样本中预测基因表达。

Method: 采用对比学习框架的深度学习方法，从全切片图像预测基因表达，并在六种疾病数据集上评估。

Result: 方法在预测高表达基因、高变异基因和标记基因时，Pearson相关系数分别提高了6.27%、6.11%和11.26%，并保留了基因间相关性。

Conclusion: 该方法在小样本数据中表现优异，并展示了在癌症组织定位中的潜在应用价值。

Abstract: Spatial transcriptomics is a technology that captures gene expression levels
at different spatial locations, widely used in tumor microenvironment analysis
and molecular profiling of histopathology, providing valuable insights into
resolving gene expression and clinical diagnosis of cancer. Due to the high
cost of data acquisition, large-scale spatial transcriptomics data remain
challenging to obtain. In this study, we develop a contrastive learning-based
deep learning method to predict spatially resolved gene expression from
whole-slide images. Evaluation across six different disease datasets
demonstrates that, compared to existing studies, our method improves Pearson
Correlation Coefficient (PCC) in the prediction of highly expressed genes,
highly variable genes, and marker genes by 6.27%, 6.11%, and 11.26%
respectively. Further analysis indicates that our method preserves gene-gene
correlations and applies to datasets with limited samples. Additionally, our
method exhibits potential in cancer tissue localization based on biomarker
expression.

</details>


### [189] [StreamSplat: Towards Online Dynamic 3D Reconstruction from Uncalibrated Video Streams](https://arxiv.org/abs/2506.08862)
*Zike Wu,Qi Yan,Xuanyu Yi,Lele Wang,Renjie Liao*

Main category: cs.CV

TL;DR: StreamSplat是一个实时从无标定视频流重建动态3D场景的框架，解决了实时处理、动态建模和长期稳定性的挑战。


<details>
  <summary>Details</summary>
Motivation: 现有方法难以同时处理无标定输入、动态场景建模和长期稳定性，StreamSplat旨在解决这些问题。

Method: 提出静态编码器中的概率采样机制和动态解码器中的双向变形场，支持在线处理任意长度视频流。

Result: 在静态和动态基准测试中表现优于现有方法，支持在线重建。

Conclusion: StreamSplat在重建质量和动态建模方面表现优异，适用于实时应用。

Abstract: Real-time reconstruction of dynamic 3D scenes from uncalibrated video streams
is crucial for numerous real-world applications. However, existing methods
struggle to jointly address three key challenges: 1) processing uncalibrated
inputs in real time, 2) accurately modeling dynamic scene evolution, and 3)
maintaining long-term stability and computational efficiency. To this end, we
introduce StreamSplat, the first fully feed-forward framework that transforms
uncalibrated video streams of arbitrary length into dynamic 3D Gaussian
Splatting (3DGS) representations in an online manner, capable of recovering
scene dynamics from temporally local observations. We propose two key technical
innovations: a probabilistic sampling mechanism in the static encoder for 3DGS
position prediction, and a bidirectional deformation field in the dynamic
decoder that enables robust and efficient dynamic modeling. Extensive
experiments on static and dynamic benchmarks demonstrate that StreamSplat
consistently outperforms prior works in both reconstruction quality and dynamic
scene modeling, while uniquely supporting online reconstruction of arbitrarily
long video streams. Code and models are available at
https://github.com/nickwzk/StreamSplat.

</details>


### [190] [DiscoVLA: Discrepancy Reduction in Vision, Language, and Alignment for Parameter-Efficient Video-Text Retrieval](https://arxiv.org/abs/2506.08887)
*Leqi Shen,Guoqiang Gong,Tianxiang Hao,Tao He,Yifeng Zhang,Pengzhang Liu,Sicheng Zhao,Jungong Han,Guiguang Ding*

Main category: cs.CV

TL;DR: 论文提出DiscoVLA方法，通过同时减少视觉、语言和对齐三方面的差异，提升视频-文本检索性能。


<details>
  <summary>Details</summary>
Motivation: CLIP模型专注于图像级视觉-语言匹配，而视频-文本检索需要视频级的全面理解。现有方法主要关注视觉差异，忽略了语言和对齐问题。

Method: DiscoVLA通过图像-视频特征融合解决视觉和语言差异，生成伪图像标题学习细粒度对齐，并利用图像-视频对齐蒸馏增强视频级对齐。

Result: 在MSRVTT数据集上，DiscoVLA以CLIP（ViT-B/16）为基础，R@1达到50.5%，优于之前方法1.5%。

Conclusion: DiscoVLA通过多维度差异减少，显著提升了视频-文本检索性能。

Abstract: The parameter-efficient adaptation of the image-text pretraining model CLIP
for video-text retrieval is a prominent area of research. While CLIP is focused
on image-level vision-language matching, video-text retrieval demands
comprehensive understanding at the video level. Three key discrepancies emerge
in the transfer from image-level to video-level: vision, language, and
alignment. However, existing methods mainly focus on vision while neglecting
language and alignment. In this paper, we propose Discrepancy Reduction in
Vision, Language, and Alignment (DiscoVLA), which simultaneously mitigates all
three discrepancies. Specifically, we introduce Image-Video Features Fusion to
integrate image-level and video-level features, effectively tackling both
vision and language discrepancies. Additionally, we generate pseudo image
captions to learn fine-grained image-level alignment. To mitigate alignment
discrepancies, we propose Image-to-Video Alignment Distillation, which
leverages image-level alignment knowledge to enhance video-level alignment.
Extensive experiments demonstrate the superiority of our DiscoVLA. In
particular, on MSRVTT with CLIP (ViT-B/16), DiscoVLA outperforms previous
methods by 1.5% in R@1, reaching a final score of 50.5% R@1. The code is
available at https://github.com/LunarShen/DsicoVLA.

</details>


### [191] [Product of Experts for Visual Generation](https://arxiv.org/abs/2506.08894)
*Yunzhi Zhang,Carson Murtuza-Lanier,Zizhang Li,Yilun Du,Jiajun Wu*

Main category: cs.CV

TL;DR: 提出了一种基于专家乘积（PoE）的框架，通过异构模型在推理时进行知识组合，无需训练，利用退火重要性采样（AIS）实现图像和视频合成任务的更好可控性。


<details>
  <summary>Details</summary>
Motivation: 整合来自视觉生成模型、视觉语言模型以及人类知识源（如图形引擎和物理模拟器）的多样化知识，以提升视觉生成任务的表现。

Method: 采用专家乘积（PoE）框架，通过退火重要性采样（AIS）从异构模型的乘积分布中采样。

Result: 在图像和视频合成任务中表现出更好的可控性，并提供灵活的用户界面以指定视觉生成目标。

Conclusion: 该框架为异构模型的知识组合提供了一种有效的训练免费方法，具有实际应用价值。

Abstract: Modern neural models capture rich priors and have complementary knowledge
over shared data domains, e.g., images and videos. Integrating diverse
knowledge from multiple sources -- including visual generative models, visual
language models, and sources with human-crafted knowledge such as graphics
engines and physics simulators -- remains under-explored. We propose a Product
of Experts (PoE) framework that performs inference-time knowledge composition
from heterogeneous models. This training-free approach samples from the product
distribution across experts via Annealed Importance Sampling (AIS). Our
framework shows practical benefits in image and video synthesis tasks, yielding
better controllability than monolithic methods and additionally providing
flexible user interfaces for specifying visual generation goals.

</details>


### [192] [WetCat: Automating Skill Assessment in Wetlab Cataract Surgery Videos](https://arxiv.org/abs/2506.08896)
*Negin Ghamsarian,Raphael Sznitman,Klaus Schoeffmann,Jens Kowal*

Main category: cs.CV

TL;DR: WetCat是首个专门为自动化技能评估设计的湿实验室白内障手术视频数据集，旨在解决传统手动评估的低效和主观性问题。


<details>
  <summary>Details</summary>
Motivation: 传统湿实验室手术训练依赖手动评估，效率低且主观性强，计算机视觉技术为自动化评估提供了可能。

Method: 构建WetCat数据集，包含高分辨率视频、详细阶段标注和关键解剖结构分割，专注于囊膜撕开和超声乳化阶段。

Result: WetCat为开发可解释的AI评估工具奠定了基础，支持标准化临床指标的自动化分析。

Conclusion: WetCat推动了客观、可扩展的手术教育，为眼科培训中的自动工作流分析和技能评估设定了新标准。

Abstract: To meet the growing demand for systematic surgical training, wetlab
environments have become indispensable platforms for hands-on practice in
ophthalmology. Yet, traditional wetlab training depends heavily on manual
performance evaluations, which are labor-intensive, time-consuming, and often
subject to variability. Recent advances in computer vision offer promising
avenues for automated skill assessment, enhancing both the efficiency and
objectivity of surgical education. Despite notable progress in ophthalmic
surgical datasets, existing resources predominantly focus on real surgeries or
isolated tasks, falling short of supporting comprehensive skill evaluation in
controlled wetlab settings. To address these limitations, we introduce WetCat,
the first dataset of wetlab cataract surgery videos specifically curated for
automated skill assessment. WetCat comprises high-resolution recordings of
surgeries performed by trainees on artificial eyes, featuring comprehensive
phase annotations and semantic segmentations of key anatomical structures.
These annotations are meticulously designed to facilitate skill assessment
during the critical capsulorhexis and phacoemulsification phases, adhering to
standardized surgical skill assessment frameworks. By focusing on these
essential phases, WetCat enables the development of interpretable, AI-driven
evaluation tools aligned with established clinical metrics. This dataset lays a
strong foundation for advancing objective, scalable surgical education and sets
a new benchmark for automated workflow analysis and skill assessment in
ophthalmology training. The dataset and annotations are publicly available in
Synapse https://www.synapse.org/Synapse:syn66401174/files.

</details>


### [193] [MIRAGE: Multimodal foundation model and benchmark for comprehensive retinal OCT image analysis](https://arxiv.org/abs/2506.08900)
*José Morano,Botond Fazekas,Emese Sükei,Ronald Fecso,Taha Emre,Markus Gumpinger,Georg Faustmann,Marzieh Oghbaie,Ursula Schmidt-Erfurth,Hrvoje Bogunović*

Main category: cs.CV

TL;DR: MIRAGE是一种新型多模态基础模型，用于分析OCT和SLO图像，并在分类和分割任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有AI模型在眼科图像分析中依赖大量标注且泛化能力不足，基础模型（FMs）虽有望解决这些问题，但缺乏验证且仅针对单一模态。

Method: 提出MIRAGE多模态基础模型，并设计新的评估基准，涵盖OCT/SLO分类和分割任务。

Result: MIRAGE在分类和分割任务中优于通用和专业FMs及分割方法。

Conclusion: MIRAGE适合作为开发稳健眼科AI系统的基础，模型和评估基准已开源。

Abstract: Artificial intelligence (AI) has become a fundamental tool for assisting
clinicians in analyzing ophthalmic images, such as optical coherence tomography
(OCT). However, developing AI models often requires extensive annotation, and
existing models tend to underperform on independent, unseen data. Foundation
models (FMs), large AI models trained on vast unlabeled datasets, have shown
promise in overcoming these challenges. Nonetheless, available FMs for
ophthalmology lack extensive validation, especially for segmentation tasks, and
focus on a single imaging modality. In this context, we propose MIRAGE, a novel
multimodal FM for the analysis of OCT and scanning laser ophthalmoscopy (SLO)
images. Additionally, we propose a new evaluation benchmark with OCT/SLO
classification and segmentation tasks. The comparison with general and
specialized FMs and segmentation methods shows the superiority of MIRAGE in
both types of tasks, highlighting its suitability as a basis for the
development of robust AI systems for retinal OCT image analysis. Both MIRAGE
and the evaluation benchmark are publicly available:
https://github.com/j-morano/MIRAGE.

</details>


### [194] [Hyperbolic Dual Feature Augmentation for Open-Environment](https://arxiv.org/abs/2506.08906)
*Peilin Yu,Yuwei Wu,Zhi Gao,Xiaomeng Fan,Shuo Yang,Yunde Jia*

Main category: cs.CV

TL;DR: 提出了一种双曲双特征增强方法，用于开放环境，增强已见和未见类别的特征。


<details>
  <summary>Details</summary>
Motivation: 现有双曲特征增强方法局限于封闭环境，无法处理开放环境中的未见类别。

Method: 采用神经ODE模块估计特征分布，引入正则化保持层次结构，推导损失上界以支持无限增强。

Result: 在五种开放环境任务中显著提升双曲算法性能。

Conclusion: 该方法有效增强了双曲算法在开放环境中的泛化能力。

Abstract: Feature augmentation generates novel samples in the feature space, providing
an effective way to enhance the generalization ability of learning algorithms
with hyperbolic geometry. Most hyperbolic feature augmentation is confined to
closed-environment, assuming the number of classes is fixed (\emph{i.e.}, seen
classes) and generating features only for these classes. In this paper, we
propose a hyperbolic dual feature augmentation method for open-environment,
which augments features for both seen and unseen classes in the hyperbolic
space. To obtain a more precise approximation of the real data distribution for
efficient training, (1) we adopt a neural ordinary differential equation
module, enhanced by meta-learning, estimating the feature distributions of both
seen and unseen classes; (2) we then introduce a regularizer to preserve the
latent hierarchical structures of data in the hyperbolic space; (3) we also
derive an upper bound for the hyperbolic dual augmentation loss, allowing us to
train a hyperbolic model using infinite augmentations for seen and unseen
classes. Extensive experiments on five open-environment tasks:
class-incremental learning, few-shot open-set recognition, few-shot learning,
zero-shot learning, and general image classification, demonstrate that our
method effectively enhances the performance of hyperbolic algorithms in
open-environment.

</details>


### [195] [SkipVAR: Accelerating Visual Autoregressive Modeling via Adaptive Frequency-Aware Skipping](https://arxiv.org/abs/2506.08908)
*Jiajun Li,Yue Ma,Xinyu Zhang,Qingyan Wei,Songhua Liu,Linfeng Zhang*

Main category: cs.CV

TL;DR: 论文提出SkipVAR框架，通过动态跳过冗余步骤和替换无条件分支，显著提升VAR模型的推理效率，同时保持生成质量。


<details>
  <summary>Details</summary>
Motivation: 现有VAR模型在高频生成步骤中存在计算冗余，导致推理延迟高，但相关研究不足。

Method: 提出自动跳过冗余步骤策略和无条件分支替换技术，并基于样本频率信息动态选择加速策略。

Result: SkipVAR在GenEval基准上实现1.81倍加速和2.62倍速度提升，平均SSIM达0.88。

Conclusion: 频率感知的自适应加速策略有效提升了VAR模型的可扩展性，且无需额外训练。

Abstract: Recent studies on Visual Autoregressive (VAR) models have highlighted that
high-frequency components, or later steps, in the generation process contribute
disproportionately to inference latency. However, the underlying computational
redundancy involved in these steps has yet to be thoroughly investigated. In
this paper, we conduct an in-depth analysis of the VAR inference process and
identify two primary sources of inefficiency: step redundancy and unconditional
branch redundancy. To address step redundancy, we propose an automatic
step-skipping strategy that selectively omits unnecessary generation steps to
improve efficiency. For unconditional branch redundancy, we observe that the
information gap between the conditional and unconditional branches is minimal.
Leveraging this insight, we introduce unconditional branch replacement, a
technique that bypasses the unconditional branch to reduce computational cost.
Notably, we observe that the effectiveness of acceleration strategies varies
significantly across different samples. Motivated by this, we propose SkipVAR,
a sample-adaptive framework that leverages frequency information to dynamically
select the most suitable acceleration strategy for each instance. To evaluate
the role of high-frequency information, we introduce high-variation benchmark
datasets that test model sensitivity to fine details. Extensive experiments
show SkipVAR achieves over 0.88 average SSIM with up to 1.81x overall
acceleration and 2.62x speedup on the GenEval benchmark, maintaining model
quality. These results confirm the effectiveness of frequency-aware,
training-free adaptive acceleration for scalable autoregressive image
generation. Our code is available at https://github.com/fakerone-li/SkipVAR and
has been publicly released.

</details>


### [196] [Inherently Faithful Attention Maps for Vision Transformers](https://arxiv.org/abs/2506.08915)
*Ananthu Aniraj,Cassio F. Dantas,Dino Ienco,Diego Marcos*

Main category: cs.CV

TL;DR: 提出一种基于注意力机制的两阶段框架，通过二进制注意力掩码限制预测仅受关注区域影响，提升对虚假相关性和分布外背景的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 上下文可能强烈影响物体感知，导致偏见表示，尤其是物体出现在分布外背景时。同时，许多图像级任务需要识别相关区域，通常依赖上下文。

Method: 两阶段框架：第一阶段处理完整图像以发现物体部分和任务相关区域；第二阶段利用注意力掩码限制其感受野至这些区域，过滤潜在虚假信息。

Result: 在多样化基准测试中显著提升对虚假相关性和分布外背景的鲁棒性。

Conclusion: 该方法通过注意力掩码和两阶段设计，有效平衡上下文依赖与区域聚焦，提升模型鲁棒性。

Abstract: We introduce an attention-based method that uses learned binary attention
masks to ensure that only attended image regions influence the prediction.
Context can strongly affect object perception, sometimes leading to biased
representations, particularly when objects appear in out-of-distribution
backgrounds. At the same time, many image-level object-centric tasks require
identifying relevant regions, often requiring context. To address this
conundrum, we propose a two-stage framework: stage 1 processes the full image
to discover object parts and identify task-relevant regions, while stage 2
leverages input attention masking to restrict its receptive field to these
regions, enabling a focused analysis while filtering out potentially spurious
information. Both stages are trained jointly, allowing stage 2 to refine stage
1. Extensive experiments across diverse benchmarks demonstrate that our
approach significantly improves robustness against spurious correlations and
out-of-distribution backgrounds.

</details>


### [197] [Socratic-MCTS: Test-Time Visual Reasoning by Asking the Right Questions](https://arxiv.org/abs/2506.08927)
*David Acuna,Ximing Lu,Jaehun Jung,Hyunwoo Kim,Amlan Kar,Sanja Fidler,Yejin Choi*

Main category: cs.CV

TL;DR: 论文探讨了如何通过蒙特卡洛树搜索（MCTS）启发式算法，在不额外训练或监督的情况下，从非推理视觉语言模型中提取隐藏知识并生成长推理链。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于探索是否可以通过搜索机制从已部署的非推理模型中提取隐藏知识，而无需重新训练或监督。

Method: 采用蒙特卡洛树搜索（MCTS）启发式算法，通过注入子问题-子答案对来引导模型生成长推理链。

Result: 在三个基准测试中表现一致提升，尤其在MMMU-PRO上整体提升2%，其中文科领域显著提升9%。

Conclusion: 研究表明，将推理视为搜索过程可以有效帮助非推理模型连接碎片化知识并生成长推理链。

Abstract: Recent research in vision-language models (VLMs) has centered around the
possibility of equipping them with implicit long-form chain-of-thought
reasoning -- akin to the success observed in language models -- via
distillation and reinforcement learning. But what about the non-reasoning
models already trained and deployed across the internet? Should we simply
abandon them, or is there hope for a search mechanism that can elicit hidden
knowledge and induce long reasoning traces -- without any additional training
or supervision? In this paper, we explore this possibility using a Monte Carlo
Tree Search (MCTS)-inspired algorithm, which injects subquestion-subanswer
pairs into the model's output stream. We show that framing reasoning as a
search process -- where subquestions act as latent decisions within a broader
inference trajectory -- helps the model "connect the dots" between fragmented
knowledge and produce extended reasoning traces in non-reasoning models. We
evaluate our method across three benchmarks and observe consistent
improvements. Notably, our approach yields a 2% overall improvement on
MMMU-PRO, including a significant 9% gain in Liberal Arts.

</details>


### [198] [What Limits Virtual Agent Application? OmniBench: A Scalable Multi-Dimensional Benchmark for Essential Virtual Agent Capabilities](https://arxiv.org/abs/2506.08933)
*Wendong Bu,Yang Wu,Qifan Yu,Minghe Gao,Bingchen Miao,Zhenkui Zhang,Kaihang Pan,Yunfei Li,Mengze Li,Wei Ji,Juncheng Li,Siliang Tang,Yueting Zhuang*

Main category: cs.CV

TL;DR: OmniBench是一个自生成、跨平台的图基准，通过子任务组合控制任务复杂度，并引入多维评估框架OmniEval。


<details>
  <summary>Details</summary>
Motivation: 现有基准存在任务复杂度不可控、人工标注有限和缺乏多维评估的问题。

Method: 提出OmniBench和OmniEval，通过子任务组合生成任务，并进行多维评估。

Result: 数据集包含36k图结构任务，人类接受率91%，训练效率优于人工标注数据。

Conclusion: OmniBench为虚拟代理的多维评估提供了新方法，推动了未来研究。

Abstract: As multimodal large language models (MLLMs) advance, MLLM-based virtual
agents have demonstrated remarkable performance. However, existing benchmarks
face significant limitations, including uncontrollable task complexity,
extensive manual annotation with limited scenarios, and a lack of
multidimensional evaluation. In response to these challenges, we introduce
OmniBench, a self-generating, cross-platform, graph-based benchmark with an
automated pipeline for synthesizing tasks of controllable complexity through
subtask composition. To evaluate the diverse capabilities of virtual agents on
the graph, we further present OmniEval, a multidimensional evaluation framework
that includes subtask-level evaluation, graph-based metrics, and comprehensive
tests across 10 capabilities. Our synthesized dataset contains 36k
graph-structured tasks across 20 scenarios, achieving a 91\% human acceptance
rate. Training on our graph-structured data shows that it can more efficiently
guide agents compared to manually annotated data. We conduct multidimensional
evaluations for various open-source and closed-source models, revealing their
performance across various capabilities and paving the way for future
advancements. Our project is available at https://omni-bench.github.io/.

</details>


### [199] [SSS: Semi-Supervised SAM-2 with Efficient Prompting for Medical Imaging Segmentation](https://arxiv.org/abs/2506.08949)
*Hongjie Zhu,Xiwei Liu,Rundong Xue,Zeyu Zhang,Yong Xu,Daji Ergu,Ying Cai,Yang Zhao*

Main category: cs.CV

TL;DR: 论文提出SSS方法，利用SAM-2模型的特征提取能力增强半监督医学图像分割性能，通过多视图特征差异优化和提示生成器提升效果。


<details>
  <summary>Details</summary>
Motivation: 在医学图像分析中，如何高效利用无标注数据并减少对高质量标注的依赖是一个关键挑战。

Method: 基于SAM-2模型，引入判别性特征增强机制和多尺度数据增强策略，结合物理约束滑动窗口生成提示。

Result: 在ACDC和BHSD数据集上表现优异，BHSD上的Dice分数达到53.15，超越之前最佳方法3.65分。

Conclusion: SSS方法显著提升了半监督医学图像分割的性能，为相关领域提供了新思路。

Abstract: In the era of information explosion, efficiently leveraging large-scale
unlabeled data while minimizing the reliance on high-quality pixel-level
annotations remains a critical challenge in the field of medical imaging.
Semi-supervised learning (SSL) enhances the utilization of unlabeled data by
facilitating knowledge transfer, significantly improving the performance of
fully supervised models and emerging as a highly promising research direction
in medical image analysis. Inspired by the ability of Vision Foundation Models
(e.g., SAM-2) to provide rich prior knowledge, we propose SSS (Semi-Supervised
SAM-2), a novel approach that leverages SAM-2's robust feature extraction
capabilities to uncover latent knowledge in unlabeled medical images, thus
effectively enhancing feature support for fully supervised medical image
segmentation. Specifically, building upon the single-stream "weak-to-strong"
consistency regularization framework, this paper introduces a Discriminative
Feature Enhancement (DFE) mechanism to further explore the feature
discrepancies introduced by various data augmentation strategies across
multiple views. By leveraging feature similarity and dissimilarity across
multi-scale augmentation techniques, the method reconstructs and models the
features, thereby effectively optimizing the salient regions. Furthermore, a
prompt generator is developed that integrates Physical Constraints with a
Sliding Window (PCSW) mechanism to generate input prompts for unlabeled data,
fulfilling SAM-2's requirement for additional prompts. Extensive experiments
demonstrate the superiority of the proposed method for semi-supervised medical
image segmentation on two multi-label datasets, i.e., ACDC and BHSD. Notably,
SSS achieves an average Dice score of 53.15 on BHSD, surpassing the previous
state-of-the-art method by +3.65 Dice. Code will be available at
https://github.com/AIGeeksGroup/SSS.

</details>


### [200] [Cross-Spectral Body Recognition with Side Information Embedding: Benchmarks on LLCM and Analyzing Range-Induced Occlusions on IJB-MDF](https://arxiv.org/abs/2506.08953)
*Anirudh Nanduri,Siyuan Huang,Rama Chellappa*

Main category: cs.CV

TL;DR: 该论文研究了如何将预训练的ViT模型应用于跨光谱人体识别任务，通过引入Side Information Embedding（SIE）提升性能，并探讨了遮挡问题在可见-红外Re-ID中的影响。


<details>
  <summary>Details</summary>
Motivation: 解决跨光谱（可见光和红外）人体识别中的挑战，特别是如何利用ViT模型和额外嵌入信息（如相机信息）提升匹配性能。

Method: 采用预训练的ViT模型，结合Side Information Embedding（SIE）编码相机信息，并在IJB-MDF数据集上分析遮挡影响。

Result: 仅编码相机信息即可在LLCM数据集上实现最先进的性能，同时揭示了遮挡在跨光谱Re-ID中的重要性。

Conclusion: 相机信息对跨光谱识别至关重要，而遮挡问题在跨光谱Re-ID中需进一步研究。

Abstract: Vision Transformers (ViTs) have demonstrated impressive performance across a
wide range of biometric tasks, including face and body recognition. In this
work, we adapt a ViT model pretrained on visible (VIS) imagery to the
challenging problem of cross-spectral body recognition, which involves matching
images captured in the visible and infrared (IR) domains. Recent ViT
architectures have explored incorporating additional embeddings beyond
traditional positional embeddings. Building on this idea, we integrate Side
Information Embedding (SIE) and examine the impact of encoding domain and
camera information to enhance cross-spectral matching. Surprisingly, our
results show that encoding only camera information - without explicitly
incorporating domain information - achieves state-of-the-art performance on the
LLCM dataset. While occlusion handling has been extensively studied in
visible-spectrum person re-identification (Re-ID), occlusions in
visible-infrared (VI) Re-ID remain largely underexplored - primarily because
existing VI-ReID datasets, such as LLCM, SYSU-MM01, and RegDB, predominantly
feature full-body, unoccluded images. To address this gap, we analyze the
impact of range-induced occlusions using the IARPA Janus Benchmark Multi-Domain
Face (IJB-MDF) dataset, which provides a diverse set of visible and infrared
images captured at various distances, enabling cross-range, cross-spectral
evaluations.

</details>


### [201] [Segment Concealed Objects with Incomplete Supervision](https://arxiv.org/abs/2506.08955)
*Chunming He,Kai Li,Yachao Zhang,Ziyun Yang,Youwei Pang,Longxiang Tang,Chengyu Fang,Yulun Zhang,Linghe Kong,Xiu Li,Sina Farsiu*

Main category: cs.CV

TL;DR: 论文提出了一种统一方法SEE，用于不完全监督的隐蔽物体分割（ISCOS），通过利用SAM生成伪标签和混合粒度特征分组模块，解决了不完全监督和隐蔽物体难以区分的问题。


<details>
  <summary>Details</summary>
Motivation: 隐蔽物体分割任务面临不完全标注数据和物体与背景相似性高的挑战，需要一种统一的方法来解决这些问题。

Method: 提出SEE框架，结合SAM生成伪标签，并设计混合粒度特征分组模块以提高分割一致性。

Result: 实验表明，SEE在多个ISCOS任务中达到最先进性能，并能作为即插即用的解决方案提升现有模型。

Conclusion: SEE通过伪标签策略和特征分组模块，有效解决了ISCOS任务中的关键挑战，具有广泛适用性。

Abstract: Incompletely-Supervised Concealed Object Segmentation (ISCOS) involves
segmenting objects that seamlessly blend into their surrounding environments,
utilizing incompletely annotated data, such as weak and semi-annotations, for
model training. This task remains highly challenging due to (1) the limited
supervision provided by the incompletely annotated training data, and (2) the
difficulty of distinguishing concealed objects from the background, which
arises from the intrinsic similarities in concealed scenarios. In this paper,
we introduce the first unified method for ISCOS to address these challenges. To
tackle the issue of incomplete supervision, we propose a unified mean-teacher
framework, SEE, that leverages the vision foundation model, ``\emph{Segment
Anything Model (SAM)}'', to generate pseudo-labels using coarse masks produced
by the teacher model as prompts. To mitigate the effect of low-quality
segmentation masks, we introduce a series of strategies for pseudo-label
generation, storage, and supervision. These strategies aim to produce
informative pseudo-labels, store the best pseudo-labels generated, and select
the most reliable components to guide the student model, thereby ensuring
robust network training. Additionally, to tackle the issue of intrinsic
similarity, we design a hybrid-granularity feature grouping module that groups
features at different granularities and aggregates these results. By clustering
similar features, this module promotes segmentation coherence, facilitating
more complete segmentation for both single-object and multiple-object images.
We validate the effectiveness of our approach across multiple ISCOS tasks, and
experimental results demonstrate that our method achieves state-of-the-art
performance. Furthermore, SEE can serve as a plug-and-play solution, enhancing
the performance of existing models.

</details>


### [202] [Data Augmentation For Small Object using Fast AutoAugment](https://arxiv.org/abs/2506.08956)
*DaeEun Yoon,Semin Kim,SangWook Yoo,Jongha Lee*

Main category: cs.CV

TL;DR: 论文提出了一种基于Fast AutoAugment的数据增强方法，显著提升了小目标检测性能。


<details>
  <summary>Details</summary>
Motivation: 尽管目标检测性能近年来有很大提升，但小目标的检测性能仍远低于大目标，这是计算机视觉中的重要挑战。

Method: 采用Fast AutoAugment快速寻找最优数据增强策略，以克服小目标检测中的性能下降问题。

Result: 在DOTA数据集上实现了20%的性能提升。

Conclusion: 该方法有效解决了小目标检测的难题，显著提升了性能。

Abstract: In recent years, there has been tremendous progress in object detection
performance. However, despite these advances, the detection performance for
small objects is significantly inferior to that of large objects. Detecting
small objects is one of the most challenging and important problems in computer
vision. To improve the detection performance for small objects, we propose an
optimal data augmentation method using Fast AutoAugment. Through our proposed
method, we can quickly find optimal augmentation policies that can overcome
degradation when detecting small objects, and we achieve a 20% performance
improvement on the DOTA dataset.

</details>


### [203] [ORIDa: Object-centric Real-world Image Composition Dataset](https://arxiv.org/abs/2506.08964)
*Jinwoo Kim,Sangmin Han,Jinho Jeong,Jiwoo Choi,Dongyoung Kim,Seon Joo Kim*

Main category: cs.CV

TL;DR: ORIDa是一个大规模真实世界图像合成数据集，包含30,000多张图像和200个独特对象，支持对象在不同场景中的放置和协调研究。


<details>
  <summary>Details</summary>
Motivation: 现有数据集缺乏多样性和规模，无法全面探索真实世界场景中的对象合成任务。

Method: ORIDa提供两种数据类型：事实-反事实集（每组5张图像）和事实场景（单张图像），涵盖多样化的环境和对象位置。

Result: ORIDa是首个公开的具有如此规模和复杂性的真实世界图像合成数据集。

Conclusion: ORIDa为对象合成研究的进一步推进提供了宝贵资源。

Abstract: Object compositing, the task of placing and harmonizing objects in images of
diverse visual scenes, has become an important task in computer vision with the
rise of generative models. However, existing datasets lack the diversity and
scale required to comprehensively explore real-world scenarios. We introduce
ORIDa (Object-centric Real-world Image Composition Dataset), a large-scale,
real-captured dataset containing over 30,000 images featuring 200 unique
objects, each of which is presented across varied positions and scenes. ORIDa
has two types of data: factual-counterfactual sets and factual-only scenes. The
factual-counterfactual sets consist of four factual images showing an object in
different positions within a scene and a single counterfactual (or background)
image of the scene without the object, resulting in five images per scene. The
factual-only scenes include a single image containing an object in a specific
context, expanding the variety of environments. To our knowledge, ORIDa is the
first publicly available dataset with its scale and complexity for real-world
image composition. Extensive analysis and experiments highlight the value of
ORIDa as a resource for advancing further research in object compositing.

</details>


### [204] [ADAM: Autonomous Discovery and Annotation Model using LLMs for Context-Aware Annotations](https://arxiv.org/abs/2506.08968)
*Amirreza Rouhi,Solmaz Arezoomandan,Knut Peterson,Joseph T. Woods,David K. Han*

Main category: cs.CV

TL;DR: ADAM是一种无需训练的自发现与标注框架，利用LLM和CLIP实现开放世界中的未知物体标注。


<details>
  <summary>Details</summary>
Motivation: 传统目标检测模型依赖预定义类别，无法识别开放世界中的新物体。

Method: 结合LLM生成候选标签和CLIP视觉嵌入构建ELR，通过频率投票和跨模态重排序标注新物体，并引入自优化循环提升一致性。

Result: 在COCO和PASCAL数据集上，ADAM成功标注新类别，无需微调或重训练。

Conclusion: ADAM为开放世界物体标注提供了一种高效、无需训练的新方法。

Abstract: Object detection models typically rely on predefined categories, limiting
their ability to identify novel objects in open-world scenarios. To overcome
this constraint, we introduce ADAM: Autonomous Discovery and Annotation Model,
a training-free, self-refining framework for open-world object labeling. ADAM
leverages large language models (LLMs) to generate candidate labels for unknown
objects based on contextual information from known entities within a scene.
These labels are paired with visual embeddings from CLIP to construct an
Embedding-Label Repository (ELR) that enables inference without category
supervision. For a newly encountered unknown object, ADAM retrieves visually
similar instances from the ELR and applies frequency-based voting and
cross-modal re-ranking to assign a robust label. To further enhance
consistency, we introduce a self-refinement loop that re-evaluates repository
labels using visual cohesion analysis and k-nearest-neighbor-based majority
re-labeling. Experimental results on the COCO and PASCAL datasets demonstrate
that ADAM effectively annotates novel categories using only visual and
contextual signals, without requiring any fine-tuning or retraining.

</details>


### [205] [Efficient Medical Vision-Language Alignment Through Adapting Masked Vision Models](https://arxiv.org/abs/2506.08990)
*Chenyu Lian,Hong-Yu Zhou,Dongyun Liang,Jing Qin,Liansheng Wang*

Main category: cs.CV

TL;DR: ALTA提出了一种高效的医学视觉-语言对齐方法，通过适应预训练的视觉模型，显著提升了检索和零样本分类任务的性能。


<details>
  <summary>Details</summary>
Motivation: 传统跨模态对比学习方法在视觉表示能力上表现不佳，而多模态掩码建模方法虽在视觉表示上表现优异，却难以直接进行跨模态匹配。ALTA旨在解决这一矛盾。

Method: ALTA通过适应预训练的视觉模型（来自掩码记录建模），仅需约8%的可训练参数和不到1/5的计算消耗，实现了高效的视觉-语言对齐。同时，整合了时间多视角放射图像输入以增强一致性。

Result: ALTA在文本到图像准确率上超过最佳对比方法4%以上，图像到文本检索准确率提升约6%。

Conclusion: ALTA不仅高效，还提升了视觉和语言理解能力，代码已开源。

Abstract: Medical vision-language alignment through cross-modal contrastive learning
shows promising performance in image-text matching tasks, such as retrieval and
zero-shot classification. However, conventional cross-modal contrastive
learning (CLIP-based) methods suffer from suboptimal visual representation
capabilities, which also limits their effectiveness in vision-language
alignment. In contrast, although the models pretrained via multimodal masked
modeling struggle with direct cross-modal matching, they excel in visual
representation. To address this contradiction, we propose ALTA (ALign Through
Adapting), an efficient medical vision-language alignment method that utilizes
only about 8% of the trainable parameters and less than 1/5 of the
computational consumption required for masked record modeling. ALTA achieves
superior performance in vision-language matching tasks like retrieval and
zero-shot classification by adapting the pretrained vision model from masked
record modeling. Additionally, we integrate temporal-multiview radiograph
inputs to enhance the information consistency between radiographs and their
corresponding descriptions in reports, further improving the vision-language
alignment. Experimental evaluations show that ALTA outperforms the
best-performing counterpart by over 4% absolute points in text-to-image
accuracy and approximately 6% absolute points in image-to-text retrieval
accuracy. The adaptation of vision-language models during efficient alignment
also promotes better vision and language understanding. Code is publicly
available at https://github.com/DopamineLcy/ALTA.

</details>


### [206] [Do Concept Replacement Techniques Really Erase Unacceptable Concepts?](https://arxiv.org/abs/2506.08991)
*Anudeep Das,Gurjot Singh,Prach Chantasantitam,N. Asokan*

Main category: cs.CV

TL;DR: 本文探讨了生成模型中概念替换技术（CRTs）在文本到图像（T2I）和图像到图像（I2I）场景中的局限性，并提出了一种新方法AntiMirror以提升效果和保真度。


<details>
  <summary>Details</summary>
Motivation: 尽管扩散式文本到图像模型取得了巨大成功，但如何避免生成不可接受的内容（如冒犯性或受版权保护的内容）仍是一大挑战。现有的概念替换技术在I2I场景中效果不佳，需要新的解决方案。

Method: 作者首先通过I2I模型验证了现有CRTs的不足，随后提出了一种结合目标图像编辑技术的新方法AntiMirror，旨在同时实现概念替换的有效性和保真度。

Result: 实验证明，现有CRTs在I2I场景中无法有效移除不可接受的概念，而AntiMirror方法在替换概念的同时保持了输入的其他内容。

Conclusion: 本文揭示了CRTs在T2I和I2I场景中的差异，提出AntiMirror作为解决方案，强调了保真度在概念替换中的重要性。

Abstract: Generative models, particularly diffusion-based text-to-image (T2I) models,
have demonstrated astounding success. However, aligning them to avoid
generating content with unacceptable concepts (e.g., offensive or copyrighted
content, or celebrity likenesses) remains a significant challenge. Concept
replacement techniques (CRTs) aim to address this challenge, often by trying to
"erase" unacceptable concepts from models. Recently, model providers have
started offering image editing services which accept an image and a text prompt
as input, to produce an image altered as specified by the prompt. These are
known as image-to-image (I2I) models. In this paper, we first use an I2I model
to empirically demonstrate that today's state-of-the-art CRTs do not in fact
erase unacceptable concepts. Existing CRTs are thus likely to be ineffective in
emerging I2I scenarios, despite their proven ability to remove unwanted
concepts in T2I pipelines, highlighting the need to understand this discrepancy
between T2I and I2I settings. Next, we argue that a good CRT, while replacing
unacceptable concepts, should preserve other concepts specified in the inputs
to generative models. We call this fidelity. Prior work on CRTs have neglected
fidelity in the case of unacceptable concepts. Finally, we propose the use of
targeted image-editing techniques to achieve both effectiveness and fidelity.
We present such a technique, AntiMirror, and demonstrate its viability.

</details>


### [207] [Do MIL Models Transfer?](https://arxiv.org/abs/2506.09022)
*Daniel Shao,Richard J. Chen,Andrew H. Song,Joel Runevic,Ming Y. Lu,Tong Ding,Faisal Mahmood*

Main category: cs.CV

TL;DR: 该论文研究了多实例学习（MIL）模型在计算病理学中的迁移学习能力，发现预训练的MIL模型即使在不同器官上训练也能显著优于从头训练的模型，且泛化能力强。


<details>
  <summary>Details</summary>
Motivation: 计算病理学中MIL模型在数据稀缺和小规模弱监督数据集上的表现不佳，而迁移学习的潜力尚未被充分探索。

Method: 系统评估了11个MIL模型在21个预训练任务中的表现，用于形态学和分子亚型预测。

Result: 预训练的MIL模型在不同器官和目标任务上表现优于从头训练的模型，且泛化能力更强。

Conclusion: MIL模型具有强大的适应性，迁移学习能显著提升计算病理学中的性能，并提供了标准化实现和预训练模型资源。

Abstract: Multiple Instance Learning (MIL) is a cornerstone approach in computational
pathology (CPath) for generating clinically meaningful slide-level embeddings
from gigapixel tissue images. However, MIL often struggles with small, weakly
supervised clinical datasets. In contrast to fields such as NLP and
conventional computer vision, where transfer learning is widely used to address
data scarcity, the transferability of MIL models remains poorly understood. In
this study, we systematically evaluate the transfer learning capabilities of
pretrained MIL models by assessing 11 models across 21 pretraining tasks for
morphological and molecular subtype prediction. Our results show that
pretrained MIL models, even when trained on different organs than the target
task, consistently outperform models trained from scratch. Moreover,
pretraining on pancancer datasets enables strong generalization across organs
and tasks, outperforming slide foundation models while using substantially less
pretraining data. These findings highlight the robust adaptability of MIL
models and demonstrate the benefits of leveraging transfer learning to boost
performance in CPath. Lastly, we provide a resource which standardizes the
implementation of MIL models and collection of pretrained model weights on
popular CPath tasks, available at https://github.com/mahmoodlab/MIL-Lab

</details>


### [208] [DIsoN: Decentralized Isolation Networks for Out-of-Distribution Detection in Medical Imaging](https://arxiv.org/abs/2506.09024)
*Felix Wagner,Pramit Saha,Harry Anthony,J. Alison Noble,Konstantinos Kamnitsas*

Main category: cs.CV

TL;DR: 论文提出了一种名为Decentralized Isolation Networks (DIsoN)的框架，用于在无法共享数据的情况下检测分布外（OOD）样本，解决了现有方法依赖数据共享或丢弃训练数据的问题。


<details>
  <summary>Details</summary>
Motivation: 在安全关键领域（如医学影像）部署机器学习模型时，检测分布外输入至关重要，但现有方法因数据隐私或规模限制无法直接共享训练数据。

Method: 提出了Isolation Network框架，通过二元分类任务量化测试样本与训练数据的分离难度，并进一步扩展为DIsoN，支持在不共享数据的情况下通过交换模型参数进行比较。

Result: 在四个医学影像数据集上的12项OOD检测任务中，DIsoN表现优于现有方法，同时保护数据隐私。

Conclusion: DIsoN为机器学习开发者提供了一种新的服务模式，即在不共享数据的情况下远程安全利用训练数据进行OOD检测。

Abstract: Safe deployment of machine learning (ML) models in safety-critical domains
such as medical imaging requires detecting inputs with characteristics not seen
during training, known as out-of-distribution (OOD) detection, to prevent
unreliable predictions. Effective OOD detection after deployment could benefit
from access to the training data, enabling direct comparison between test
samples and the training data distribution to identify differences.
State-of-the-art OOD detection methods, however, either discard training data
after deployment or assume that test samples and training data are centrally
stored together, an assumption that rarely holds in real-world settings. This
is because shipping training data with the deployed model is usually impossible
due to the size of training databases, as well as proprietary or privacy
constraints. We introduce the Isolation Network, an OOD detection framework
that quantifies the difficulty of separating a target test sample from the
training data by solving a binary classification task. We then propose
Decentralized Isolation Networks (DIsoN), which enables the comparison of
training and test data when data-sharing is impossible, by exchanging only
model parameters between the remote computational nodes of training and
deployment. We further extend DIsoN with class-conditioning, comparing a target
sample solely with training data of its predicted class. We evaluate DIsoN on
four medical imaging datasets (dermatology, chest X-ray, breast ultrasound,
histopathology) across 12 OOD detection tasks. DIsoN performs favorably against
existing methods while respecting data-privacy. This decentralized OOD
detection framework opens the way for a new type of service that ML developers
could provide along with their models: providing remote, secure utilization of
their training data for OOD detection services. Code will be available upon
acceptance at: *****

</details>


### [209] [Diffuse and Disperse: Image Generation with Representation Regularization](https://arxiv.org/abs/2506.09027)
*Runqian Wang,Kaiming He*

Main category: cs.CV

TL;DR: 提出了一种名为Dispersive Loss的简单正则化方法，用于改进基于扩散的生成模型，无需额外数据或预训练。


<details>
  <summary>Details</summary>
Motivation: 扩散生成模型通常缺乏显式正则化，且与表示学习进展脱节。本文旨在通过一种简单的方法弥合这一差距。

Method: 提出Dispersive Loss，鼓励隐藏空间中的表示分散，类似于对比自监督学习，但无需正样本对。

Result: 在ImageNet数据集上评估，显示该方法在各种模型中均优于现有基线。

Conclusion: Dispersive Loss为生成模型与表示学习的结合提供了简单有效的解决方案。

Abstract: The development of diffusion-based generative models over the past decade has
largely proceeded independently of progress in representation learning. These
diffusion models typically rely on regression-based objectives and generally
lack explicit regularization. In this work, we propose \textit{Dispersive
Loss}, a simple plug-and-play regularizer that effectively improves
diffusion-based generative models. Our loss function encourages internal
representations to disperse in the hidden space, analogous to contrastive
self-supervised learning, with the key distinction that it requires no positive
sample pairs and therefore does not interfere with the sampling process used
for regression. Compared to the recent method of representation alignment
(REPA), our approach is self-contained and minimalist, requiring no
pre-training, no additional parameters, and no external data. We evaluate
Dispersive Loss on the ImageNet dataset across a range of models and report
consistent improvements over widely used and strong baselines. We hope our work
will help bridge the gap between generative modeling and representation
learning.

</details>


### [210] [Princeton365: A Diverse Dataset with Accurate Camera Pose](https://arxiv.org/abs/2506.09035)
*Karhan Kayan,Stamatis Alexandropoulos,Rishabh Jain,Yiming Zuo,Erich Liang,Jia Deng*

Main category: cs.CV

TL;DR: Princeton365是一个包含365个视频的大规模多样化数据集，提供精确的相机位姿，填补了当前SLAM基准在精度和数据多样性上的空白。


<details>
  <summary>Details</summary>
Motivation: 解决现有SLAM基准在数据多样性和精度上的不足，提供更全面的评估工具。

Method: 利用标定板和360相机收集室内、室外及物体扫描视频，同步输出单目、立体RGB视频和IMU数据，并提出新的场景尺度感知评价指标。

Result: 提出了新的评价指标和挑战性新视角合成基准，支持跨场景SLAM方法比较和非朗伯场景的评估。

Conclusion: Princeton365数据集和提出的指标为SLAM研究提供了更全面的评估工具，有助于分析方法的失败模式。

Abstract: We introduce Princeton365, a large-scale diverse dataset of 365 videos with
accurate camera pose. Our dataset bridges the gap between accuracy and data
diversity in current SLAM benchmarks by introducing a novel ground truth
collection framework that leverages calibration boards and a 360-camera. We
collect indoor, outdoor, and object scanning videos with synchronized monocular
and stereo RGB video outputs as well as IMU. We further propose a new scene
scale-aware evaluation metric for SLAM based on the the optical flow induced by
the camera pose estimation error. In contrast to the current metrics, our new
metric allows for comparison between the performance of SLAM methods across
scenes as opposed to existing metrics such as Average Trajectory Error (ATE),
allowing researchers to analyze the failure modes of their methods. We also
propose a challenging Novel View Synthesis benchmark that covers cases not
covered by current NVS benchmarks, such as fully non-Lambertian scenes with
360-degree camera trajectories. Please visit
https://princeton365.cs.princeton.edu for the dataset, code, videos, and
submission.

</details>


### [211] [Autoregressive Semantic Visual Reconstruction Helps VLMs Understand Better](https://arxiv.org/abs/2506.09040)
*Dianyi Wang,Wei Song,Yikun Wang,Siyuan Wang,Kaicheng Yu,Zhongyu Wei,Jiaqi Wang*

Main category: cs.CV

TL;DR: 论文提出ASVR方法，通过自回归框架联合学习视觉和文本模态，解决了传统LVLMs忽视视觉细节的问题。


<details>
  <summary>Details</summary>
Motivation: 传统LVLMs仅对文本序列进行自回归监督，忽视了视觉模态的完整学习，导致无法利用无标注图像、遗漏关键视觉细节等问题。

Method: 引入ASVR方法，在统一的自回归框架中联合学习视觉和文本模态，重点重构图像的语义表示而非原始外观。

Result: ASVR在多种多模态理解基准测试中表现稳定，显著提升了性能（如LLaVA-1.5平均提升5%）。

Conclusion: 自回归重构语义表示能有效提升多模态理解，且适用于不同数据规模和LLM骨干。

Abstract: Typical large vision-language models (LVLMs) apply autoregressive supervision
solely to textual sequences, without fully incorporating the visual modality
into the learning process. This results in three key limitations: (1) an
inability to utilize images without accompanying captions, (2) the risk that
captions omit critical visual details, and (3) the challenge that certain
vision-centric content cannot be adequately conveyed through text. As a result,
current LVLMs often prioritize vision-to-language alignment while potentially
overlooking fine-grained visual information. While some prior works have
explored autoregressive image generation, effectively leveraging autoregressive
visual supervision to enhance image understanding remains an open challenge. In
this paper, we introduce Autoregressive Semantic Visual Reconstruction (ASVR),
which enables joint learning of visual and textual modalities within a unified
autoregressive framework. We show that autoregressively reconstructing the raw
visual appearance of images does not enhance and may even impair multimodal
understanding. In contrast, autoregressively reconstructing the semantic
representation of images consistently improves comprehension. Notably, we find
that even when models are given continuous image features as input, they can
effectively reconstruct discrete semantic tokens, resulting in stable and
consistent improvements across a wide range of multimodal understanding
benchmarks. Our approach delivers significant performance gains across varying
data scales (556k-2M) and types of LLM bacbones. Specifically, ASVR improves
LLaVA-1.5 by 5% in average scores across 14 multimodal benchmarks. The code is
available at https://github.com/AlenjandroWang/ASVR.

</details>


### [212] [Cosmos-Drive-Dreams: Scalable Synthetic Driving Data Generation with World Foundation Models](https://arxiv.org/abs/2506.09042)
*Xuanchi Ren,Yifan Lu,Tianshi Cao,Ruiyuan Gao,Shengyu Huang,Amirmojtaba Sabour,Tianchang Shen,Tobias Pfaff,Jay Zhangjie Wu,Runjian Chen,Seung Wook Kim,Jun Gao,Laura Leal-Taixe,Mike Chen,Sanja Fidler,Huan Ling*

Main category: cs.CV

TL;DR: 论文提出了一种名为Cosmos-Drive-Dreams的合成数据生成（SDG）流水线，用于生成具有挑战性的驾驶场景，以解决自动驾驶系统中真实数据收集和标注的高成本问题。


<details>
  <summary>Details</summary>
Motivation: 由于收集和标注真实世界数据（尤其是罕见边缘案例）对自动驾驶系统至关重要但成本高昂，作者希望通过合成数据生成来解决这一问题。

Method: 基于NVIDIA Cosmos世界基础模型，开发了Cosmos-Drive模型套件，能够生成可控、高保真、多视角且时空一致的驾驶视频。

Result: 实验表明，生成的数据有助于缓解长尾分布问题，并在3D车道检测、3D目标检测和驾驶策略学习等下游任务中提升泛化能力。

Conclusion: 作者开源了流水线工具包、数据集和模型权重，展示了合成数据生成在自动驾驶领域的实用性和潜力。

Abstract: Collecting and annotating real-world data for safety-critical physical AI
systems, such as Autonomous Vehicle (AV), is time-consuming and costly. It is
especially challenging to capture rare edge cases, which play a critical role
in training and testing of an AV system. To address this challenge, we
introduce the Cosmos-Drive-Dreams - a synthetic data generation (SDG) pipeline
that aims to generate challenging scenarios to facilitate downstream tasks such
as perception and driving policy training. Powering this pipeline is
Cosmos-Drive, a suite of models specialized from NVIDIA Cosmos world foundation
model for the driving domain and are capable of controllable, high-fidelity,
multi-view, and spatiotemporally consistent driving video generation. We
showcase the utility of these models by applying Cosmos-Drive-Dreams to scale
the quantity and diversity of driving datasets with high-fidelity and
challenging scenarios. Experimentally, we demonstrate that our generated data
helps in mitigating long-tail distribution problems and enhances generalization
in downstream tasks such as 3D lane detection, 3D object detection and driving
policy learning. We open source our pipeline toolkit, dataset and model weights
through the NVIDIA's Cosmos platform.
  Project page: https://research.nvidia.com/labs/toronto-ai/cosmos_drive_dreams

</details>


### [213] [MagCache: Fast Video Generation with Magnitude-Aware Cache](https://arxiv.org/abs/2506.09045)
*Zehong Ma,Longhui Wei,Feng Wang,Shiliang Zhang,Qi Tian*

Main category: cs.CV

TL;DR: 论文提出了一种基于统一幅度规律的视频扩散模型加速技术MagCache，通过自适应跳过不重要时间步长，显著提升了速度并保持了视觉保真度。


<details>
  <summary>Details</summary>
Motivation: 现有视频扩散模型加速技术依赖统一启发式方法或时间嵌入变体，容易因提示特定过拟合导致输出不一致。

Method: 利用观察到的幅度规律，提出MagCache，通过误差建模机制和自适应缓存策略跳过不重要时间步长。

Result: 实验表明，MagCache在Open-Sora和Wan 2.1上分别实现了2.1倍和2.68倍的加速，且在LPIPS、SSIM和PSNR上优于现有方法。

Conclusion: MagCache是一种高效且鲁棒的加速技术，仅需单一样本校准，显著优于现有方法。

Abstract: Existing acceleration techniques for video diffusion models often rely on
uniform heuristics or time-embedding variants to skip timesteps and reuse
cached features. These approaches typically require extensive calibration with
curated prompts and risk inconsistent outputs due to prompt-specific
overfitting. In this paper, we introduce a novel and robust discovery: a
unified magnitude law observed across different models and prompts.
Specifically, the magnitude ratio of successive residual outputs decreases
monotonically and steadily in most timesteps while rapidly in the last several
steps. Leveraging this insight, we introduce a Magnitude-aware Cache (MagCache)
that adaptively skips unimportant timesteps using an error modeling mechanism
and adaptive caching strategy. Unlike existing methods requiring dozens of
curated samples for calibration, MagCache only requires a single sample for
calibration. Experimental results show that MagCache achieves 2.1x and 2.68x
speedups on Open-Sora and Wan 2.1, respectively, while preserving superior
visual fidelity. It significantly outperforms existing methods in LPIPS, SSIM,
and PSNR, under comparable computational budgets.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [214] [AffectMachine-Pop: A controllable expert system for real-time pop music generation](https://arxiv.org/abs/2506.08200)
*Kat R. Agres,Adyasha Dash,Phoebe Chua,Stefan K. Ehrlich*

Main category: cs.HC

TL;DR: AffectMachine-Pop是一个基于专家系统的AI工具，能够根据预设或实时情绪状态生成具有特定唤醒度和效价的复古流行音乐。


<details>
  <summary>Details</summary>
Motivation: 现有AI音乐生成系统多为黑箱，缺乏直接可控性，限制了其灵活性和适应性。

Method: 开发了AffectMachine-Pop系统，通过唤醒度和效价值生成音乐，并通过听力研究验证其效果。

Result: 听力研究表明，系统能有效生成符合目标情绪状态的音乐。

Conclusion: 该系统适用于交互式音乐生成或情绪自我调节的辅助工具。

Abstract: Music is a powerful medium for influencing listeners' emotional states, and
this capacity has driven a surge of research interest in AI-based affective
music generation in recent years. Many existing systems, however, are a black
box which are not directly controllable, thus making these systems less
flexible and adaptive to users. We present \textit{AffectMachine-Pop}, an
expert system capable of generating retro-pop music according to arousal and
valence values, which can either be pre-determined or based on a listener's
real-time emotion states. To validate the efficacy of the system, we conducted
a listening study demonstrating that AffectMachine-Pop is capable of generating
affective music at target levels of arousal and valence. The system is tailored
for use either as a tool for generating interactive affective music based on
user input, or for incorporation into biofeedback or neurofeedback systems to
assist users with emotion self-regulation.

</details>


### [215] [Z3Guide: A Scalable, Student-Centered, and Extensible Educational Environment for Logic Modeling](https://arxiv.org/abs/2506.08294)
*Ruanqianqian Huang,Ayana Monroe,Peli de Halleux,Sorin Lerner,Nikolaj Bjørner*

Main category: cs.HC

TL;DR: 本文探讨了如何设计一个逻辑建模教育环境，并提出了10条设计指南，通过Z3Guide工具验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 逻辑建模在计算机科学中应用广泛，但教学资源稀缺且分散，亟需设计易用且满足师生需求的教育环境。

Method: 通过需求访谈和设计迭代，提炼出10条设计指南，并在Z3Guide工具中实现9条，随后在100多名学生的研讨会上测试。

Result: Z3Guide获得积极反馈，验证了设计指南的有效性，同时发现未来改进空间。

Conclusion: 设计指南为逻辑建模教育环境提供了实用框架，Z3Guide的成功证明了其可行性。

Abstract: Constraint-satisfaction problems (CSPs) are ubiquitous, ranging from
budgeting for grocery shopping to verifying software behavior. Logic modeling
helps solve CSPs programmatically using SMT solvers. Despite its importance in
many Computer Science disciplines, resources for teaching and learning logic
modeling are scarce and scattered, and challenges remain in designing
educational environments for logic modeling that are accessible and meet the
needs of teachers and students. This paper explores how to design such an
environment and probes the impact of the design on the learning experience.
From a need-finding interview study and a design iteration with teachers of
logic modeling, we curated 10 design guidelines spanning three main
requirements: providing easy access, supporting various educational modalities,
and allowing extensions for customized pedagogical needs. We implemented nine
guidelines in Z3Guide, an open-source browser-based tool. Using Z3Guide in a
logic modeling learning workshop with more than 100 students, we gathered
positive feedback on its support for learning and identified opportunities for
future improvements.

</details>


### [216] [EMG-Driven Stiffness-Modulating Palpation for Telerehabilitation](https://arxiv.org/abs/2506.08303)
*Thomas M. Kwok,Hilary HY Cheng,Wai Tuck Chow*

Main category: cs.HC

TL;DR: HJ-Pal是一种轻量级可穿戴触觉设备，利用EMG驱动的蜂窝堵塞技术，将肌肉激活转化为动觉反馈，用于远程康复中的小肌肉评估。


<details>
  <summary>Details</summary>
Motivation: 解决远程康复中小肌肉评估的需求，提供一种轻便且高效的触觉反馈方法。

Method: 采用EMG驱动的蜂窝堵塞技术，设计轻量级可穿戴设备HJ-Pal，实现肌肉激活的动觉反馈。

Result: HJ-Pal能够有效支持远程康复中的小肌肉评估，提供准确的触觉反馈。

Conclusion: HJ-Pal为远程康复提供了一种创新的触觉反馈解决方案，具有轻便和高效的特点。

Abstract: In this work, we introduce HJ-Pal, a lightweight wearable haptic device that
leverages EMG-driven honeycomb jamming to render muscle activation as
kinesthetic feedback, enabling remote palpation for small muscle assessment in
telerehabilitation.

</details>


### [217] [SakugaFlow: A Stagewise Illustration Framework Emulating the Human Drawing Process and Providing Interactive Tutoring for Novice Drawing Skills](https://arxiv.org/abs/2506.08443)
*Kazuki Kawamura,Jun Rekimoto*

Main category: cs.HC

TL;DR: SakugaFlow是一个四阶段流程，结合扩散模型和大型语言模型，提供实时反馈，支持非线性修改和多版本分支，将黑盒生成器转化为学习环境。


<details>
  <summary>Details</summary>
Motivation: 当前AI绘图工具虽能生成高质量图像，但缺乏人类艺术家的逐步创作过程，SakugaFlow旨在填补这一空白。

Method: 采用四阶段流程，结合扩散模型和语言模型，提供实时反馈，支持非线性修改和多版本分支。

Result: SakugaFlow成功将黑盒生成器转化为支持创意探索和技能学习的环境。

Conclusion: SakugaFlow通过展示中间输出和嵌入教学对话，为AI绘图工具提供了新的学习支持方式。

Abstract: While current AI illustration tools can generate high-quality images from
text prompts, they rarely reveal the step-by-step procedure that human artists
follow. We present SakugaFlow, a four-stage pipeline that pairs diffusion-based
image generation with a large-language-model tutor. At each stage, novices
receive real-time feedback on anatomy, perspective, and composition, revise any
step non-linearly, and branch alternative versions. By exposing intermediate
outputs and embedding pedagogical dialogue, SakugaFlow turns a black-box
generator into a scaffolded learning environment that supports both creative
exploration and skills acquisition.

</details>


### [218] [Rethinking Citation of AI Sources in Student-AI Collaboration within HCI Design Education](https://arxiv.org/abs/2506.08467)
*Prakash Shukla,Suchismita Naik,Ike Obi,Jessica Backus,Nancy Rasche,Paul Parson*

Main category: cs.HC

TL;DR: 论文探讨了AI生成内容在HCI教育中的引用问题，分析了学生在设计项目中的引用实践，并提出了改进策略。


<details>
  <summary>Details</summary>
Motivation: 随着AI工具在设计项目中的普及，传统引用框架无法适应AI输出的动态性和短暂性，亟需新的引用方法。

Method: 通过定性分析35个团队项目和175名学生的反思，研究学生在UX设计课程中如何引用AI生成内容。

Result: 发现学生的引用实践不一致，从正式署名到间接或缺失引用，揭示了现有框架的不足。

Conclusion: 建议将AI引用视为反思性教学实践，提出贡献声明和过程感知引用模型，以支持学生与AI的有意义合作。

Abstract: The growing integration of AI tools in student design projects presents an
unresolved challenge in HCI education: how should AI-generated content be cited
and documented? Traditional citation frameworks -- grounded in credibility,
retrievability, and authorship -- struggle to accommodate the dynamic and
ephemeral nature of AI outputs. In this paper, we examine how undergraduate
students in a UX design course approached AI usage and citation when given the
freedom to integrate generative tools into their design process. Through
qualitative analysis of 35 team projects and reflections from 175 students, we
identify varied citation practices ranging from formal attribution to indirect
or absent acknowledgment. These inconsistencies reveal gaps in existing
frameworks and raise questions about authorship, assessment, and pedagogical
transparency. We argue for rethinking AI citation as a reflective and
pedagogical practice; one that supports metacognitive engagement by prompting
students to critically evaluate how and why they used AI throughout the design
process. We propose alternative strategies -- such as AI contribution
statements and process-aware citation models that better align with the
iterative and reflective nature of design education. This work invites
educators to reconsider how citation practices can support meaningful
student--AI collaboration.

</details>


### [219] [Guidelines for Gaze-based Neural Preliminary Diagnosis](https://arxiv.org/abs/2506.08517)
*Mayar Elfares,Salma Younis,Pascal Reisert,Ralf Küsters,Tobias Renner,Andreas Bulling*

Main category: cs.HC

TL;DR: 本文综述了眼动追踪技术在神经疾病初步诊断中的应用，总结了现有共识，并提出了标准化指南。


<details>
  <summary>Details</summary>
Motivation: 传统神经诊断方法繁琐且主观，眼动追踪技术提供了一种更客观的替代方案，但现有研究结果分散且矛盾，需要进一步标准化。

Method: 通过系统化现有知识和研究，总结共识并提出关键指南。

Result: 明确了眼动追踪在神经诊断中的潜力，并提出了标准化协议。

Conclusion: 眼动追踪技术有望成为神经疾病初步诊断的标准化工具，但仍需进一步研究扩展其应用。

Abstract: Neural disorders refer to any condition affecting the nervous system and that
influence how individuals perceive and interact with the world. Traditional
neural diagnoses rely on cumbersome, time-consuming, or subjective methods,
such as clinical interviews, behavioural observations, or medical imaging. Eye
tracking is an attractive alternative because analysing eye movements, such as
fixations and saccades, can provide more objective insights into brain function
and cognitive processing by capturing non-verbal and unconscious responses.
Despite its potential, existing gaze-based studies presented seemingly
contradictory findings. They are dispersed across diverse fields, requiring
further research to standardise protocols and expand their application,
particularly as a preliminary indicator of neural processes for differential
diagnosis. Therefore, this paper outlines the main agreed-upon findings and
provides a systematisation of knowledge and key guidelines towards advancing
gaze-based neural preliminary diagnosis.

</details>


### [220] [Exploring the Convergence of HCI and Evolving Technologies in Information Systems](https://arxiv.org/abs/2506.08549)
*Rajan Das Gupta,Ashikur Rahman,Md Imrul Hasan Showmick,Md. Yeasin Rahat,Md. Jakir Hossen*

Main category: cs.HC

TL;DR: 该研究回顾了50篇关于现代信息系统HCI界面设计的论文，发现现有方法未能充分满足移动用户和基于位置服务的需求，提出结合敏捷方法和以人为中心的设计原则以改进设计。


<details>
  <summary>Details</summary>
Motivation: 随着移动和云计算及物联网的快速发展，HCI专业人员需要设计更用户友好且适应性强的界面，尤其是针对儿童、老年人和残障人士等多样化用户群体。

Method: 研究通过回顾50篇近期论文，分析现有HCI设计方法对现代技术需求的适应性。

Result: 研究发现大多数HCI设计方法仍基于传统桌面模型，未能有效支持移动用户和基于位置的服务，现有设计指南也缺乏灵活性。

Conclusion: 未来研究应结合敏捷方法和以人为中心的设计原则，并采用定性与定量结合的方法，以弥合当前界面设计与技术发展之间的差距。

Abstract: Modern technology driven information systems are part of our daily lives.
However, this deep integration poses new challenges to the human computer
interaction (HCI) professionals. With the rapid growth of mobile and cloud
computing and the Internet of Things (IoT), the demand for HCI specialists to
design user-friendly and adaptable interfaces has never been more pressing.
Especially for diverse user groups such as children, the elderly and people
with disabilities who need interfaces tailored to their needs regardless of
time and location. This study reviewed 50 recent papers on HCI interface design
for modern information systems. The goal is to see how well these methods
address the demands of current technology. The findings show that most HCI
design methods are still based on old desktop models and do not support mobile
users and location-based services well. Most existing interface design
guidelines do not align with the flexibility and dynamism of emerging
technologies. The goal of this study is to improve interface design by
combining agile methodologies with human-centered design principles. Future
studies should also incorporate both qualitative and quantitative approaches,
particularly in the context of cloud-based technologies and organizational
information systems. This approach aims to bridge the gap between current
interface design practices and the changing technological landscape.

</details>


### [221] [MOSAIC-F: A Framework for Enhancing Students' Oral Presentation Skills through Personalized Feedback](https://arxiv.org/abs/2506.08634)
*Alvaro Becerra,Daniel Andres,Pablo Villegas,Roberto Daza,Ruth Cobos*

Main category: cs.HC

TL;DR: MOSAIC-F是一个多模态反馈框架，结合了人类评估和数据驱动的多模态分析，为学生提供个性化反馈。


<details>
  <summary>Details</summary>
Motivation: 通过整合多模态数据和人类评估，提供更准确、个性化的学习反馈。

Method: 包括四个步骤：标准化评估、多模态数据收集、AI生成反馈、学生自评与可视化。

Result: 在提升口头表达能力的测试中表现良好。

Conclusion: 该框架结合人类与数据评估，提供更精准、可操作的反馈。

Abstract: In this article, we present a novel multimodal feedback framework called
MOSAIC-F, an acronym for a data-driven Framework that integrates Multimodal
Learning Analytics (MMLA), Observations, Sensors, Artificial Intelligence (AI),
and Collaborative assessments for generating personalized feedback on student
learning activities. This framework consists of four key steps. First, peers
and professors' assessments are conducted through standardized rubrics (that
include both quantitative and qualitative evaluations). Second, multimodal data
are collected during learning activities, including video recordings, audio
capture, gaze tracking, physiological signals (heart rate, motion data), and
behavioral interactions. Third, personalized feedback is generated using AI,
synthesizing human-based evaluations and data-based multimodal insights such as
posture, speech patterns, stress levels, and cognitive load, among others.
Finally, students review their own performance through video recordings and
engage in self-assessment and feedback visualization, comparing their own
evaluations with peers and professors' assessments, class averages, and
AI-generated recommendations. By combining human-based and data-based
evaluation techniques, this framework enables more accurate, personalized and
actionable feedback. We tested MOSAIC-F in the context of improving oral
presentation skills.

</details>


### [222] [Stop Misusing t-SNE and UMAP for Visual Analytics](https://arxiv.org/abs/2506.08725)
*Hyeon Jeon,Jeongin Park,Sungbok Shin,Jinwook Seo*

Main category: cs.HC

TL;DR: 论文探讨了t-SNE和UMAP在可视化分析中的误用问题，分析了原因并提出了改进建议。


<details>
  <summary>Details</summary>
Motivation: 揭示t-SNE和UMAP在可视化分析中的常见误用，探究其背后的原因及预防方法。

Method: 通过文献综述（114篇论文）和访谈研究，分析误用现象及实践者的隐性动机。

Result: 误用主要源于对t-SNE和UMAP在可视化分析中适用性的讨论不足。

Conclusion: 提出未来方向和具体行动，以促进降维技术的合理使用。

Abstract: Misuses of t-SNE and UMAP in visual analytics have become increasingly
common. For example, although t-SNE and UMAP projections often do not
faithfully reflect true distances between clusters, practitioners frequently
use them to investigate inter-cluster relationships. In this paper, we bring
this issue to the surface and comprehensively investigate why such misuse
occurs and how to prevent it. We conduct a literature review of 114 papers to
verify the prevalence of the misuse and analyze the reasonings behind it. We
then execute an interview study to uncover practitioners' implicit motivations
for using these techniques -- rationales often undisclosed in the literature.
Our findings indicate that misuse of t-SNE and UMAP primarily stems from
limited discourse on their appropriate use in visual analytics. We conclude by
proposing future directions and concrete action items to promote more
reasonable use of DR.

</details>


### [223] [Communicating Through Avatars in Industry 5.0: A Focus Group Study on Human-Robot Collaboration](https://arxiv.org/abs/2506.08805)
*Stina Klein,Pooja Prajod,Katharina Weitz,Matteo Lavit Nicora,Dimitra Tsovaltzi,Elisabeth André*

Main category: cs.HC

TL;DR: 研究探讨了在工业环境中使用协作机器人（cobots）时，通过虚拟形象（avatars）提升人机协作体验的可能性，并通过焦点小组研究收集了实际员工的反馈。


<details>
  <summary>Details</summary>
Motivation: 协作机器人的引入可能减少工人的社交互动，影响其福祉，而虚拟形象有望改善这一问题。

Method: 在实验室环境中进行脚本化的人机协作演示，随后与德国制造企业的员工进行焦点小组讨论。

Result: 研究发现虚拟形象在个性化沟通和任务辅助方面具有潜力，并提出了改进其行为的建议。

Conclusion: 尽管研究样本有限，但为未来在工业环境中开发适应性、情境感知的虚拟形象交互提供了初步依据。

Abstract: The integration of collaborative robots (cobots) in industrial settings
raises concerns about worker well-being, particularly due to reduced social
interactions. Avatars - designed to facilitate worker interactions and
engagement - are promising solutions to enhance the human-robot collaboration
(HRC) experience. However, real-world perspectives on avatar-supported HRC
remain unexplored. To address this gap, we conducted a focus group study with
employees from a German manufacturing company that uses cobots. Before the
discussion, participants engaged with a scripted, industry-like HRC demo in a
lab setting. This qualitative approach provided valuable insights into the
avatar's potential roles, improvements to its behavior, and practical
considerations for deploying them in industrial workcells. Our findings also
emphasize the importance of personalized communication and task assistance.
Although our study's limitations restrict its generalizability, it serves as an
initial step in recognizing the potential of adaptive, context-aware avatar
interactions in real-world industrial environments.

</details>


### [224] [From Fads to Classics -- Analyzing Video Game Trend Evolutions through Steam Tags](https://arxiv.org/abs/2506.08881)
*Nicolas Grelier,Johannes Pfau,Nicolas Mathieu,Stéphane Kaufmann*

Main category: cs.HC

TL;DR: 本文通过数据驱动分析Steam标签的演变，帮助理解视频游戏行业趋势，发现趋势可分为短期热潮、当代流行和稳定经典，平均持续约四年。


<details>
  <summary>Details</summary>
Motivation: 视频游戏市场竞争激烈且变化迅速，行业需要理解趋势以应对挑战。

Method: 基于Steam标签的数据分析、可视化和专家验证。

Result: 趋势可分为三类，平均持续四年，并通过专家验证。

Conclusion: 提供可视化工具和开放方法，帮助行业解读游戏趋势变化。

Abstract: The video game industry deals with a fast-paced, competitive and almost
unpredictable market. Trends of genres, settings and modalities change on a
perpetual basis, studios are often one big hit or miss away from surviving or
perishing, and hitting the pulse of the time has become one of the greatest
challenges for industrials, investors and other stakeholders. In this work, we
aim to support the understanding of video game trends over time based on
data-driven analysis, visualization and interpretation of Steam tag evolutions.
We confirm underlying groundwork that trends can be categorized in short-lived
fads, contemporary fashions, or stable classics, and derived that the surge of
a trend averages at about four years in the realm of video games. After using
industrial experts to validate our findings, we deliver visualizations,
insights and an open approach of deciphering shifts in video game trends.

</details>


### [225] [Help or Hindrance: Understanding the Impact of Robot Communication in Action Teams](https://arxiv.org/abs/2506.08892)
*Tauhid Tanjim,Jonathan St. George,Kevin Ching,Hee Rin Lee,Angelique Taylor*

Main category: cs.HC

TL;DR: 研究探讨了多模态机器人交互在时间敏感环境中对团队协作的影响，发现语音和视觉提示能有效降低工作负荷并提升机器人感知。


<details>
  <summary>Details</summary>
Motivation: 填补机器人如何通过多模态交互提示与行动团队高效沟通的研究空白。

Method: 实验室内研究，通过医疗训练场景测试机器人语音和非语言提示的效果。

Result: 语音提示用于物品搜索任务，视觉提示用于任务提醒，能显著降低团队工作负荷并提升机器人易用性和实用性感知。

Conclusion: 多模态交互研究对HRI领域至关重要，需进一步探索人机团队协作在时间敏感环境中的最佳实践。

Abstract: The human-robot interaction (HRI) field has recognized the importance of
enabling robots to interact with teams. Human teams rely on effective
communication for successful collaboration in time-sensitive environments.
Robots can play a role in enhancing team coordination through real-time
assistance. Despite significant progress in human-robot teaming research, there
remains an essential gap in how robots can effectively communicate with action
teams using multimodal interaction cues in time-sensitive environments. This
study addresses this knowledge gap in an experimental in-lab study to
investigate how multimodal robot communication in action teams affects workload
and human perception of robots. We explore team collaboration in a medical
training scenario where a robotic crash cart (RCC) provides verbal and
non-verbal cues to help users remember to perform iterative tasks and search
for supplies. Our findings show that verbal cues for object search tasks and
visual cues for task reminders reduce team workload and increase perceived ease
of use and perceived usefulness more effectively than a robot with no feedback.
Our work contributes to multimodal interaction research in the HRI field,
highlighting the need for more human-robot teaming research to understand best
practices for integrating collaborative robots in time-sensitive environments
such as in hospitals, search and rescue, and manufacturing applications.

</details>


### [226] [Implementing Keyword Spotting on the MCUX947 Microcontroller with Integrated NPU](https://arxiv.org/abs/2506.08911)
*Petar Jakuš,Hrvoje Džapo*

Main category: cs.HC

TL;DR: 本文提出了一种在NXP MCXN947微控制器上实现的关键词检测系统，利用集成的NPU实现实时语音交互，并通过量化感知训练优化模型。


<details>
  <summary>Details</summary>
Motivation: 研究旨在在资源受限的设备上实现高效的实时语音交互。

Method: 结合MFCC特征提取和CNN分类器，使用量化感知训练优化模型。

Result: 实验显示，利用NPU比仅用CPU推理速度快59倍，模型大小为30.58 KB，准确率达97.06%。

Conclusion: 证明了在嵌入式平台上实现高效、低功耗语音接口的可行性。

Abstract: This paper presents a keyword spotting (KWS) system implemented on the NXP
MCXN947 microcontroller with an integrated Neural Processing Unit (NPU),
enabling real-time voice interaction on resource-constrained devices. The
system combines MFCC feature extraction with a CNN classifier, optimized using
Quantization Aware Training to reduce model size with minimal accuracy drop.
Experimental results demonstrate a 59x speedup in inference time when
leveraging the NPU compared to CPU-only execution, achieving 97.06% accuracy
with a model size of 30.58 KB, demonstrating the feasibility of efficient,
low-power voice interfaces on embedded platforms.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [227] [KVmix: Gradient-Based Layer Importance-Aware Mixed-Precision Quantization for KV Cache](https://arxiv.org/abs/2506.08018)
*Fei Li,Song Liu,Weiguo Wu,Shiqiang Nie,Jinyu Wang*

Main category: cs.LG

TL;DR: KVmix是一种新型混合精度量化方法，用于优化LLM推理中的KV Cache内存问题，通过动态分配精度和长上下文优化策略，实现高效低内存推理。


<details>
  <summary>Details</summary>
Motivation: LLM推理中KV Cache的高内存需求限制了其在资源受限平台上的部署，现有方法无法动态分配精度或优化长上下文任务。

Method: KVmix利用梯度重要性分析，动态分配层特定比特宽度，并引入长上下文优化策略，优先保留关键KV对的高精度。

Result: 在Llama和Mistral等LLM上，KVmix以极低量化配置（Key 2.19bit，Value 2.38bit）实现接近无损推理，内存压缩4.9倍，推理速度提升5.3倍。

Conclusion: KVmix通过动态混合精度量化和长上下文优化，显著提升了LLM推理的效率和内存利用率。

Abstract: The high memory demands of the Key-Value (KV) Cache during the inference of
Large Language Models (LLMs) severely restrict their deployment in
resource-constrained platforms. Quantization can effectively alleviate the
memory pressure caused by KV Cache. However, existing methods either rely on
static one-size-fits-all precision allocation or fail to dynamically prioritize
critical KV in long-context tasks, forcing memory-accuracy-throughput
tradeoffs. In this work, we propose a novel mixed-precision quantization method
for KV Cache named KVmix. KVmix leverages gradient-based importance analysis to
evaluate how individual Key and Value projection matrices affect the model
loss, enabling layer-specific bit-width allocation for mix-precision
quantization. It dynamically prioritizes higher precision for important layers
while aggressively quantizing less influential ones, achieving a tunable
balance between accuracy and efficiency. KVmix also introduces a dynamic
long-context optimization strategy that adaptively keeps full-precision KV
pairs for recent pivotal tokens and compresses older ones, achieving
high-quality sequence generation with low memory usage. Additionally, KVmix
provides efficient low-bit quantization and CUDA kernels to optimize
computational overhead. On LLMs such as Llama and Mistral, KVmix achieves
near-lossless inference performance with extremely low quantization
configuration (Key 2.19bit Value 2.38bit), while delivering a remarkable 4.9x
memory compression and a 5.3x speedup in inference throughput.

</details>


### [228] [Agentic Neural Networks: Self-Evolving Multi-Agent Systems via Textual Backpropagation](https://arxiv.org/abs/2506.09046)
*Xiaowen Ma,Chenyang Lin,Yao Zhang,Volker Tresp,Yunpu Ma*

Main category: cs.LG

TL;DR: Agentic Neural Network (ANN) 提出了一种动态多智能体协作框架，通过分层神经网络架构优化任务分解与协作，显著提升了准确性和适应性。


<details>
  <summary>Details</summary>
Motivation: 当前多智能体系统依赖静态配置，限制了复杂任务的灵活性和效率。

Method: 采用两阶段优化策略：前向阶段动态分解任务并构建协作团队，后向阶段通过反馈优化协作。

Result: 在四个基准数据集上，ANN 表现优于现有多智能体基线。

Conclusion: ANN 提供了一种可扩展、数据驱动的多智能体框架，结合了 LLMs 的协作能力和神经网络的高效灵活性。

Abstract: Leveraging multiple Large Language Models(LLMs) has proven effective for
addressing complex, high-dimensional tasks, but current approaches often rely
on static, manually engineered multi-agent configurations. To overcome these
constraints, we present the Agentic Neural Network(ANN), a framework that
conceptualizes multi-agent collaboration as a layered neural network
architecture. In this design, each agent operates as a node, and each layer
forms a cooperative "team" focused on a specific subtask. Agentic Neural
Network follows a two-phase optimization strategy: (1) Forward Phase-Drawing
inspiration from neural network forward passes, tasks are dynamically
decomposed into subtasks, and cooperative agent teams with suitable aggregation
methods are constructed layer by layer. (2) Backward Phase-Mirroring
backpropagation, we refine both global and local collaboration through
iterative feedback, allowing agents to self-evolve their roles, prompts, and
coordination. This neuro-symbolic approach enables ANN to create new or
specialized agent teams post-training, delivering notable gains in accuracy and
adaptability. Across four benchmark datasets, ANN surpasses leading multi-agent
baselines under the same configurations, showing consistent performance
improvements. Our findings indicate that ANN provides a scalable, data-driven
framework for multi-agent systems, combining the collaborative capabilities of
LLMs with the efficiency and flexibility of neural network principles. We plan
to open-source the entire framework.

</details>


### [229] [Gridding Forced Displacement using Semi-Supervised Learning](https://arxiv.org/abs/2506.08019)
*Andrew Wells,Geraldine Henningsen,Brice Bolane Tchinde Kengne*

Main category: cs.LG

TL;DR: 提出了一种半监督方法，将难民的统计数据从行政边界分解到0.5度网格单元，覆盖25个撒哈拉以南非洲国家。


<details>
  <summary>Details</summary>
Motivation: 通过高分辨率数据揭示以往被区域和国家统计数据掩盖的局部流离失所模式。

Method: 整合UNHCR的ProGres注册数据、Google Open Buildings的卫星建筑足迹和OpenStreetMap的位置坐标，使用标签传播算法生成高精度的难民统计数据。

Result: 方法平均准确率达92.9%，成功将1000多万难民观测数据分配到合适的网格单元。

Conclusion: 高分辨率数据集为深入研究流离失所的驱动因素提供了基础。

Abstract: We present a semi-supervised approach that disaggregates refugee statistics
from administrative boundaries to 0.5-degree grid cells across 25 Sub-Saharan
African countries. By integrating UNHCR's ProGres registration data with
satellite-derived building footprints from Google Open Buildings and location
coordinates from OpenStreetMap Populated Places, our label spreading algorithm
creates spatially explicit refugee statistics at high granularity.This
methodology achieves 92.9% average accuracy in placing over 10 million refugee
observations into appropriate grid cells, enabling the identification of
localized displacement patterns previously obscured in broader regional and
national statistics. The resulting high-resolution dataset provides a
foundation for a deeper understanding of displacement drivers.

</details>


### [230] [Bi-level Unbalanced Optimal Transport for Partial Domain Adaptation](https://arxiv.org/abs/2506.08020)
*Zi-Ying Chen,Chuan-Xian Ren,Hong Yan*

Main category: cs.LG

TL;DR: 提出了一种双层次不平衡最优传输模型（BUOT），用于解决部分领域自适应（PDA）问题，通过同时表征样本级和类别级关系，提升知识迁移的准确性。


<details>
  <summary>Details</summary>
Motivation: 部分领域自适应问题需要对齐跨领域样本并区分异常类别，但现有加权框架仅能表征样本级关系，且对异常类别的识别不够准确。

Method: 提出BUOT模型，结合样本级和类别级传输，通过合作机制提供结构信息和判别信息，并引入标签感知传输成本以提高效率。

Result: 在基准数据集上的实验验证了BUOT的竞争力。

Conclusion: BUOT通过双层次传输框架有效解决了PDA问题，提升了知识迁移的准确性和效率。

Abstract: Partial domain adaptation (PDA) problem requires aligning cross-domain
samples while distinguishing the outlier classes for accurate knowledge
transfer. The widely used weighting framework tries to address the outlier
classes by introducing the reweighed source domain with a similar label
distribution to the target domain. However, the empirical modeling of weights
can only characterize the sample-wise relations, which leads to insufficient
exploration of cluster structures, and the weights could be sensitive to the
inaccurate prediction and cause confusion on the outlier classes. To tackle
these issues, we propose a Bi-level Unbalanced Optimal Transport (BUOT) model
to simultaneously characterize the sample-wise and class-wise relations in a
unified transport framework. Specifically, a cooperation mechanism between
sample-level and class-level transport is introduced, where the sample-level
transport provides essential structure information for the class-level
knowledge transfer, while the class-level transport supplies discriminative
information for the outlier identification. The bi-level transport plan
provides guidance for the alignment process. By incorporating the label-aware
transport cost, the local transport structure is ensured and a fast computation
formulation is derived to improve the efficiency. Extensive experiments on
benchmark datasets validate the competitiveness of BUOT.

</details>


### [231] [FlowBERT: Prompt-tuned BERT for variable flow field prediction](https://arxiv.org/abs/2506.08021)
*Weihao Zou,Weibing Feng,Pin Wu*

Main category: cs.LG

TL;DR: 提出基于大语言模型知识迁移的通用流场预测框架，解决传统CFD高计算成本和现有深度学习模型跨条件迁移能力不足的问题。


<details>
  <summary>Details</summary>
Motivation: 传统CFD方法计算成本高，现有深度学习模型跨条件迁移能力有限，亟需高效且通用的流场预测方法。

Method: 结合POD降维与预训练LLM微调策略，设计流体动力学文本模板以增强预测性能。

Result: 在少样本学习场景中优于传统Transformer模型，跨多种流入条件和翼型几何表现出卓越泛化能力，预测时间缩短至秒级且保持90%以上准确率。

Conclusion: 该框架为快速流体动力学预测开辟新方向，可应用于气动优化、流动控制等工程领域。

Abstract: This study proposes a universal flow field prediction framework based on
knowledge transfer
  from large language model (LLM), addressing the high computational costs of
traditional
  computational fluid dynamics (CFD) methods and the limited cross-condition
transfer capability
  of existing deep learning models. The framework innovatively integrates
Proper Orthogonal
  Decomposition (POD) dimensionality reduction with fine-tuning strategies for
pretrained LLM,
  where POD facilitates compressed representation of flow field features while
the fine-tuned model
  learns to encode system dynamics in state space. To enhance the model's
adaptability to flow field
  data, we specifically designed fluid dynamics-oriented text templates that
improve predictive
  performance through enriched contextual semantic information. Experimental
results demonstrate
  that our framework outperforms conventional Transformer models in few-shot
learning scenarios while
  exhibiting exceptional generalization across various inflow conditions and
airfoil geometries.
  Ablation studies reveal the contributions of key components in the FlowBERT
architecture. Compared
  to traditional Navier-Stokes equation solvers requiring hours of computation,
our approach reduces
  prediction time to seconds while maintaining over 90% accuracy. The developed
knowledge transfer
  paradigm establishes a new direction for rapid fluid dynamics prediction,
with potential
  applications extending to aerodynamic optimization, flow control, and other
engineering domains.

</details>


### [232] [Modality-Balancing Preference Optimization of Large Multimodal Models by Adversarial Negative Mining](https://arxiv.org/abs/2506.08022)
*Chenxi Liu,Tianyi Xiong,Ruibo Chen,Yihan Wu,Junfeng Guo,Tianyi Zhou,Heng Huang*

Main category: cs.LG

TL;DR: 论文提出了一种新的偏好学习框架MBPO，通过生成硬负样本和在线验证奖励，解决大型多模态模型（LMMs）中的模态不平衡问题。


<details>
  <summary>Details</summary>
Motivation: 现有偏好优化方法未能有效抑制大型语言模型（LLM）的内部偏见，且依赖离线数据，无法适应动态分布变化。

Method: MBPO通过对抗性扰动生成硬负样本，并利用封闭式任务生成在线验证奖励，结合GRPO进行训练。

Result: 实验表明MBPO能提升LMM在视觉语言任务中的性能，并减少幻觉现象。

Conclusion: MBPO有效解决了模态不平衡问题，提升了模型的泛化能力。

Abstract: The task adaptation and alignment of Large Multimodal Models (LMMs) have been
significantly advanced by instruction tuning and further strengthened by recent
preference optimization. Yet, most LMMs still suffer from severe modality
imbalance during reasoning, i.e., outweighing language prior biases over visual
inputs, which bottlenecks their generalization to downstream tasks and causes
hallucinations. However, existing preference optimization approaches for LMMs
do not focus on restraining the internal biases of their Large Language Model
(LLM) backbones when curating the training data. Moreover, they heavily rely on
offline data and lack the capacity to explore diverse responses adaptive to
dynamic distributional shifts during training. Meanwhile, Group Relative Policy
Optimization (GRPO), a recent method using online-generated data and verified
rewards to improve reasoning capabilities, remains largely underexplored in LMM
alignment. In this paper, we propose a novel preference learning framework,
Modality-Balancing Preference Optimization (MBPO), to address the modality
imbalance in LMMs. MBPO constructs a more effective offline preference dataset
by generating hard negatives, i.e., rejected responses misled by LLM biases due
to limited usage of visual information, through adversarial perturbation of
input images. Moreover, MBPO leverages the easy-to-verify nature of close-ended
tasks to generate online responses with verified rewards. GRPO is then employed
to train the model with offline-online hybrid data. Extensive experiments
demonstrate that MBPO can enhance LMM performance on challenging
vision-language tasks and effectively reduce hallucinations.

</details>


### [233] [Recipes for Pre-training LLMs with MXFP8](https://arxiv.org/abs/2506.08027)
*Asit Mishra,Dusan Stosic,Simon Layton*

Main category: cs.LG

TL;DR: 本文探讨了在预训练中使用较少比特表示模型参数的技术（精度缩放），并指出NVIDIA的Blackwell GPU中的微缩放（MX）格式是实现这一目标的重要进步。尽管MX格式能提高数值稳定性，但需谨慎使用以避免发散。作者提出了一种改进的舍入模式，成功实现了8B模型在15T tokens上的MXFP8预训练。


<details>
  <summary>Details</summary>
Motivation: 研究如何在减少比特表示的同时保持模型精度，并解决MX格式在实际应用中可能导致发散的问题。

Method: 提出了一种改进的舍入模式（round-to-infinity），用于计算MXFP8格式中的缩放因子，以解决发散问题。

Result: 改进的舍入模式成功实现了8B模型在15T tokens上的MXFP8预训练。

Conclusion: MX格式需谨慎使用，改进的舍入模式能有效解决发散问题，为高效预训练提供可行方案。

Abstract: Precision scaling - using fewer bits to represent model parameters and
related tensors during pre-training - has emerged as a compelling technique for
improving GPU efficiency without sacrificing accuracy. Microscaling (MX)
formats in NVIDIA's latest Blackwell GPUs represent a major leap in enabling
this precision scaling aspect. These formats combine narrow floating-point data
types with per-block scaling factors, offering a fine-grained approach to
quantizing tensors.
  Although MX-formats offer the promise of improved numeric stability compared
to other reduced-precision representations, in practice they must be used
carefully in order to successfully converge an LLM on a multi-trillion token
dataset. In this paper, we show that the rounding mode suggested in OCP
specification can lead to divergence when pre-training an LLM. We show an
improved rounding mode, which uses round-to-infinity to compute scaling
factors, enables successful pre-training in MXFP8 for an 8B model on 15T
tokens.

</details>


### [234] [ST-GraphNet: A Spatio-Temporal Graph Neural Network for Understanding and Predicting Automated Vehicle Crash Severity](https://arxiv.org/abs/2506.08051)
*Mahmuda Sultana Mimi,Md Monzurul Islam,Anannya Ghosh Tusti,Shriyank Somvanshi,Subasish Das*

Main category: cs.LG

TL;DR: ST-GraphNet是一种时空图神经网络框架，用于建模和预测自动驾驶车辆（AV）碰撞严重性，通过细粒度和区域聚合的空间图实现，测试准确率达97.74%。


<details>
  <summary>Details</summary>
Motivation: 理解AV碰撞严重性的时空动态对提升城市交通安全和基础设施规划至关重要。

Method: 使用细粒度和H3空间索引的粗粒度图表示，结合多模态数据（如语义、空间、时间属性），并评估多种GNN架构（如GCN、GAT、DSTGCN）。

Result: ST-GraphNet在粗粒度H3图上表现最佳，测试准确率达97.74%，显著优于细粒度模型（64.7%）。

Conclusion: 空间聚合、动态消息传递和多模态特征整合能有效捕捉AV碰撞严重性的复杂时空模式。

Abstract: Understanding the spatial and temporal dynamics of automated vehicle (AV)
crash severity is critical for advancing urban mobility safety and
infrastructure planning. In this work, we introduce ST-GraphNet, a
spatio-temporal graph neural network framework designed to model and predict AV
crash severity by using both fine-grained and region-aggregated spatial graphs.
Using a balanced dataset of 2,352 real-world AV-related crash reports from
Texas (2024), including geospatial coordinates, crash timestamps, SAE
automation levels, and narrative descriptions, we construct two complementary
graph representations: (1) a fine-grained graph with individual crash events as
nodes, where edges are defined via spatio-temporal proximity; and (2) a
coarse-grained graph where crashes are aggregated into Hexagonal Hierarchical
Spatial Indexing (H3)-based spatial cells, connected through hexagonal
adjacency. Each node in the graph is enriched with multimodal data, including
semantic, spatial, and temporal attributes, including textual embeddings from
crash narratives using a pretrained Sentence-BERT model. We evaluate various
graph neural network (GNN) architectures, such as Graph Convolutional Networks
(GCN), Graph Attention Networks (GAT), and Dynamic Spatio-Temporal GCN
(DSTGCN), to classify crash severity and predict high-risk regions. Our
proposed ST-GraphNet, which utilizes a DSTGCN backbone on the coarse-grained H3
graph, achieves a test accuracy of 97.74\%, substantially outperforming the
best fine-grained model (64.7\% test accuracy). These findings highlight the
effectiveness of spatial aggregation, dynamic message passing, and multi-modal
feature integration in capturing the complex spatio-temporal patterns
underlying AV crash severity.

</details>


### [235] [STAMImputer: Spatio-Temporal Attention MoE for Traffic Data Imputation](https://arxiv.org/abs/2506.08054)
*Yiming Wang,Hao Peng,Senzhang Wang,Haohua Du,Chunyang Liu,Jia Wu,Guanlin Wu*

Main category: cs.LG

TL;DR: STAMImputer是一种用于交通数据填补的时空注意力专家混合网络，解决了块状缺失数据特征提取和静态图结构灵活性不足的问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法在块状缺失数据场景下特征提取效果不佳，且静态图结构限制了模型对非平稳交通数据的适应性。

Method: 提出STAMImputer，结合专家混合框架和低秩引导采样图注意力机制，动态生成图结构以捕捉实时空间相关性。

Result: 在四个交通数据集上的实验表明，STAMImputer显著优于现有方法。

Conclusion: STAMImputer通过动态图结构和专家混合框架，有效提升了交通数据填补的性能。

Abstract: Traffic data imputation is fundamentally important to support various
applications in intelligent transportation systems such as traffic flow
prediction. However, existing time-to-space sequential methods often fail to
effectively extract features in block-wise missing data scenarios. Meanwhile,
the static graph structure for spatial feature propagation significantly
constrains the models flexibility in handling the distribution shift issue for
the nonstationary traffic data. To address these issues, this paper proposes a
SpatioTemporal Attention Mixture of experts network named STAMImputer for
traffic data imputation. Specifically, we introduce a Mixture of Experts (MoE)
framework to capture latent spatio-temporal features and their influence
weights, effectively imputing block missing. A novel Low-rank guided Sampling
Graph ATtention (LrSGAT) mechanism is designed to dynamically balance the local
and global correlations across road networks. The sampled attention vectors are
utilized to generate dynamic graphs that capture real-time spatial
correlations. Extensive experiments are conducted on four traffic datasets for
evaluation. The result shows STAMImputer achieves significantly performance
improvement compared with existing SOTA approaches. Our codes are available at
https://github.com/RingBDStack/STAMImupter.

</details>


### [236] [Eliciting Fine-Tuned Transformer Capabilities via Inference-Time Techniques](https://arxiv.org/abs/2506.08060)
*Asankhaya Sharma*

Main category: cs.LG

TL;DR: 论文证明，通过推理时技术（如上下文学习）可以近似监督微调的能力，无需修改模型参数，并在理想和实际场景下提供了数据集规模的数学保证。


<details>
  <summary>Details</summary>
Motivation: 监督微调（SFT）计算成本高，研究旨在探索是否可以通过推理时技术（如上下文学习）近似其能力，从而更高效地部署大语言模型。

Method: 在理想假设（无限计算资源和完整数据集）下，证明基础Transformer模型可通过上下文学习近似SFT能力；扩展到实际场景（有限上下文和部分数据集）。

Result: 对于文本生成和线性分类任务，给出了数据集规模的数学保证，表明在有限资源下也能近似微调行为。

Conclusion: 研究为资源高效部署大语言模型提供了理论基础，并通过检索增强生成等技术将理论应用于实际。

Abstract: Large language models have transformed natural language processing, yet
supervised fine-tuning (SFT) remains computationally intensive. This paper
formally proves that capabilities acquired through SFT can be approximated by a
base transformer model using inference-time techniques, specifically in-context
learning (ICL), without altering model parameters, under idealized assumptions
including unbounded computational resources and access to the fine-tuning
dataset. We extend these results to practical scenarios with finite context
lengths and partial dataset access. For text generation tasks with fixed output
length $l$, datasets of size $\mathrm{O}\left( \frac{m V}{\varepsilon^2} \log
\frac{m}{\delta} \right)$ or, with bounded context, $\mathrm{O}\left( \frac{l
\log V}{\varepsilon^2} \log \frac{1}{\delta} \right)$ suffice to approximate
fine-tuned behavior across $m$ contexts within error $\varepsilon$, where $V$
is the vocabulary size and $\delta$ is the failure probability. For linear
classification, datasets of size $\mathrm{O}\left( \frac{d}{\varepsilon}
\right)$ or, with fixed context, $\mathrm{O}\left( \frac{1}{\varepsilon^2} \log
\frac{1}{\delta} \right)$ are sufficient, where $d$ is the input dimension.
Grounded in the Turing completeness of transformers, these results provide a
theoretical foundation for resource-efficient deployment of large language
models, with practical techniques like retrieval-augmented generation bridging
theory to real-world applications.

</details>


### [237] [FairDICE: Fairness-Driven Offline Multi-Objective Reinforcement Learning](https://arxiv.org/abs/2506.08062)
*Woosung Kim,Jinho Lee,Jongmin Lee,Byung-Jun Lee*

Main category: cs.LG

TL;DR: FairDICE是首个离线多目标强化学习框架，直接优化非线性福利目标，无需显式偏好权重。


<details>
  <summary>Details</summary>
Motivation: 现有线性标量化方法无法捕捉公平导向目标（如Nash社会福利或最大最小公平），离线环境中缺乏统一方法优化非线性福利标准。

Method: FairDICE利用分布校正估计，联合考虑福利最大化和分布正则化，实现稳定且样本高效的学习。

Result: 在多个离线基准测试中，FairDICE表现出优于现有基线的公平感知性能。

Conclusion: FairDICE为离线多目标强化学习中的非线性福利优化提供了有效解决方案。

Abstract: Multi-objective reinforcement learning (MORL) aims to optimize policies in
the presence of conflicting objectives, where linear scalarization is commonly
used to reduce vector-valued returns into scalar signals. While effective for
certain preferences, this approach cannot capture fairness-oriented goals such
as Nash social welfare or max-min fairness, which require nonlinear and
non-additive trade-offs. Although several online algorithms have been proposed
for specific fairness objectives, a unified approach for optimizing nonlinear
welfare criteria in the offline setting-where learning must proceed from a
fixed dataset-remains unexplored. In this work, we present FairDICE, the first
offline MORL framework that directly optimizes nonlinear welfare objective.
FairDICE leverages distribution correction estimation to jointly account for
welfare maximization and distributional regularization, enabling stable and
sample-efficient learning without requiring explicit preference weights or
exhaustive weight search. Across multiple offline benchmarks, FairDICE
demonstrates strong fairness-aware performance compared to existing baselines.

</details>


### [238] [Lite-RVFL: A Lightweight Random Vector Functional-Link Neural Network for Learning Under Concept Drift](https://arxiv.org/abs/2506.08063)
*Songqiao Hu,Zeyi Liu,Xiao He*

Main category: cs.LG

TL;DR: 提出了一种轻量级、快速的随机向量功能链接网络（Lite-RVFL），无需检测概念漂移或重新训练即可适应数据分布变化。


<details>
  <summary>Details</summary>
Motivation: 解决现有方法因需要模型重新训练或漂移检测而计算成本高、不适合实时应用的问题。

Method: 引入一种新颖的目标函数，为新样本分配指数级增加的权重，强调近期数据以实现及时适应。

Result: 理论分析和实验验证表明，Lite-RVFL能高效适应概念漂移，并捕捉时间模式。

Conclusion: Lite-RVFL是一种高效、轻量级的解决方案，适用于实时应用中的概念漂移问题。

Abstract: The change in data distribution over time, also known as concept drift, poses
a significant challenge to the reliability of online learning methods. Existing
methods typically require model retraining or drift detection, both of which
demand high computational costs and are often unsuitable for real-time
applications. To address these limitations, a lightweight, fast and efficient
random vector functional-link network termed Lite-RVFL is proposed, capable of
adapting to concept drift without drift detection and retraining. Lite-RVFL
introduces a novel objective function that assigns weights exponentially
increasing to new samples, thereby emphasizing recent data and enabling timely
adaptation. Theoretical analysis confirms the feasibility of this objective
function for drift adaptation, and an efficient incremental update rule is
derived. Experimental results on a real-world safety assessment task validate
the efficiency, effectiveness in adapting to drift, and potential to capture
temporal patterns of Lite-RVFL. The source code is available at
https://github.com/songqiaohu/Lite-RVFL.

</details>


### [239] [Info-Coevolution: An Efficient Framework for Data Model Coevolution](https://arxiv.org/abs/2506.08070)
*Ziheng Qin,Hailun Xu,Wei Chee Yew,Qi Jia,Yang Luo,Kanchan Sarkar,Danhui Guan,Kai Wang,Yang You*

Main category: cs.LG

TL;DR: Info-Coevolution是一种新颖的框架，通过在线选择性标注实现模型与数据的协同进化，显著降低标注和训练成本。


<details>
  <summary>Details</summary>
Motivation: 解决机器学习中数据冗余和标注效率低下的问题，避免传统方法的复杂性和偏差。

Method: 利用任务特定模型和开源模型，选择性标注并整合在线和网络数据，优化数据集构建。

Result: 在ImageNet-1K等数据集上，标注和训练成本降低32%，性能无损失；半监督学习下可进一步降至50%。

Conclusion: Info-Coevolution提供了一种高效、无偏的数据选择和标注方法，显著提升机器学习效率。

Abstract: Machine learning relies heavily on data, yet the continuous growth of
real-world data poses challenges for efficient dataset construction and
training. A fundamental yet unsolved question is: given our current model and
data, does a new data (sample/batch) need annotation/learning? Conventional
approaches retain all available data, leading to non-optimal data and training
efficiency. Active learning aims to reduce data redundancy by selecting a
subset of samples to annotate, while it increases pipeline complexity and
introduces bias. In this work, we propose Info-Coevolution, a novel framework
that efficiently enables models and data to coevolve through online selective
annotation with no bias. Leveraging task-specific models (and open-source
models), it selectively annotates and integrates online and web data to improve
datasets efficiently. For real-world datasets like ImageNet-1K,
Info-Coevolution reduces annotation and training costs by 32\% without
performance loss. It is able to automatically give the saving ratio without
tuning the ratio. It can further reduce the annotation ratio to 50\% with
semi-supervised learning. We also explore retrieval-based dataset enhancement
using unlabeled open-source data. Code is available at
https://github.com/NUS-HPC-AI-Lab/Info-Coevolution/.

</details>


### [240] [Benchmarking Pre-Trained Time Series Models for Electricity Price Forecasting](https://arxiv.org/abs/2506.08113)
*Timothée Hornek Amir Sartipi,Igor Tchappi,Gilbert Fridgen*

Main category: cs.LG

TL;DR: 论文比较了多种预训练时间序列模型与传统方法在电力价格预测中的表现，发现传统双季节性MSTL模型表现最佳。


<details>
  <summary>Details</summary>
Motivation: 电力价格预测对电力现货市场决策至关重要，但生成式AI和时间序列基础模型在此领域的有效性尚不明确。

Method: 使用德国、法国、荷兰、奥地利和比利时的2024年日前拍卖电价数据，对比了多种预训练模型与传统统计和机器学习方法。

Result: Chronos-Bolt和Time-MoE表现最佳，但双季节性MSTL模型在一致性和统计性能上优于所有时间序列基础模型。

Conclusion: 传统方法在电力价格预测中仍具优势，时间序列基础模型尚未超越其表现。

Abstract: Accurate electricity price forecasting (EPF) is crucial for effective
decision-making in power trading on the spot market. While recent advances in
generative artificial intelligence (GenAI) and pre-trained large language
models (LLMs) have inspired the development of numerous time series foundation
models (TSFMs) for time series forecasting, their effectiveness in EPF remains
uncertain. To address this gap, we benchmark several state-of-the-art
pretrained models--Chronos-Bolt, Chronos-T5, TimesFM, Moirai, Time-MoE, and
TimeGPT--against established statistical and machine learning (ML) methods for
EPF. Using 2024 day-ahead auction (DAA) electricity prices from Germany,
France, the Netherlands, Austria, and Belgium, we generate daily forecasts with
a one-day horizon. Chronos-Bolt and Time-MoE emerge as the strongest among the
TSFMs, performing on par with traditional models. However, the biseasonal MSTL
model, which captures daily and weekly seasonality, stands out for its
consistent performance across countries and evaluation metrics, with no TSFM
statistically outperforming it.

</details>


### [241] [Bingo: Boosting Efficient Reasoning of LLMs via Dynamic and Significance-based Reinforcement Learning](https://arxiv.org/abs/2506.08125)
*Hanbing Liu,Lang Cao,Yuanyi Ren,Mengyu Zhou,Haoyu Dong,Xiaojun Ma,Shi Han,Dongmei Zhang*

Main category: cs.LG

TL;DR: 论文提出了Bingo框架，通过改进基于长度的奖励设计，提升语言模型的推理效率，同时保持准确性。


<details>
  <summary>Details</summary>
Motivation: 现有方法在提升语言模型推理能力时，主要关注准确性而忽视效率，且直接基于长度的奖励常导致准确性下降。

Method: Bingo引入两种机制：显著性感知长度奖励（逐步减少无关标记）和动态长度奖励（初期鼓励详细推理，后期提高效率）。

Result: 实验表明，Bingo在多个推理基准上优于基线方法，实现了准确性与效率的良好平衡。

Conclusion: Bingo证明了显式训练语言模型以实现高效推理的潜力。

Abstract: Large language models have demonstrated impressive reasoning capabilities,
yet they often suffer from inefficiencies due to unnecessarily verbose or
redundant outputs. While many works have explored reinforcement learning (RL)
to enhance reasoning abilities, most primarily focus on improving accuracy,
with limited attention to reasoning efficiency. Some existing approaches
introduce direct length-based rewards to encourage brevity, but this often
leads to noticeable drops in accuracy. In this paper, we propose Bingo, an RL
framework that advances length-based reward design to boost efficient
reasoning. Bingo incorporates two key mechanisms: a significance-aware length
reward, which gradually guides the model to reduce only insignificant tokens,
and a dynamic length reward, which initially encourages elaborate reasoning for
hard questions but decays over time to improve overall efficiency. Experiments
across multiple reasoning benchmarks show that Bingo improves both accuracy and
efficiency. It outperforms the vanilla reward and several other length-based
reward baselines in RL, achieving a favorable trade-off between accuracy and
efficiency. These results underscore the potential of training LLMs explicitly
for efficient reasoning.

</details>


### [242] [Nearness of Neighbors Attention for Regression in Supervised Finetuning](https://arxiv.org/abs/2506.08139)
*Aviad Susman,Mayte Suárez-Fariñas,Joseph T Colonel*

Main category: cs.LG

TL;DR: 论文提出了一种名为NONA的回归层，通过神经网络的注意力机制和新的注意力掩码方案，实现了k-NN回归算法的可微代理，提升了回归任务的性能。


<details>
  <summary>Details</summary>
Motivation: 传统算法（如k-NN）在结合神经网络特征提取时表现优异，但由于其不可微性和独特优化需求，难以直接融入监督微调（SFT）。

Method: 引入NONA回归层，利用注意力机制和学习的注意力掩码方案，作为k-NN回归的可微替代。

Result: 在多个非结构化数据集上，NONA在回归任务中的表现优于密集层预测和基于SFT嵌入的k-NN。

Conclusion: NONA为将传统算法融入SFT提供了可行方案，显著提升了回归任务的性能。

Abstract: It is common in supervised machine learning to combine the feature extraction
capabilities of neural networks with the predictive power of traditional
algorithms, such as k-nearest neighbors (k-NN) or support vector machines. This
procedure involves performing supervised fine-tuning (SFT) on a
domain-appropriate feature extractor, followed by training a traditional
predictor on the resulting SFT embeddings. When used in this manner,
traditional predictors often deliver increased performance over the SFT model
itself, despite the fine-tuned feature extractor yielding embeddings
specifically optimized for prediction by the neural network's final dense
layer. This suggests that directly incorporating traditional algorithms into
SFT as prediction layers may further improve performance. However, many
traditional algorithms have not been implemented as neural network layers due
to their non-differentiable nature and their unique optimization requirements.
As a step towards solving this problem, we introduce the Nearness of Neighbors
Attention (NONA) regression layer. NONA uses the mechanics of neural network
attention and a novel learned attention-masking scheme to yield a
differentiable proxy of the k-NN regression algorithm. Results on multiple
unstructured datasets show improved performance over both dense layer
prediction and k-NN on SFT embeddings for regression.

</details>


### [243] [AutoSDT: Scaling Data-Driven Discovery Tasks Toward Open Co-Scientists](https://arxiv.org/abs/2506.08140)
*Yifei Li,Hanane Nour Moussa,Ziru Chen,Shijie Chen,Botao Yu,Mingyi Xue,Benjamin Burns,Tzu-Yao Chiu,Vishal Dey,Zitong Lu,Chen Wei,Qianheng Zhang,Tianyu Zhang,Song Gao,Xuhui Huang,Xia Ning,Nesreen K. Ahmed,Ali Payani,Huan Sun*

Main category: cs.LG

TL;DR: AutoSDT是一个自动收集高质量编码任务的管道，用于解决数据稀缺问题，并构建了AutoSDT-5K数据集。基于该数据集训练的AutoSDT-Coder模型在多个基准测试中表现优异。


<details>
  <summary>Details</summary>
Motivation: 加速科学发现需要AI辅助，但高质量训练数据稀缺，限制了AI作为科学合作者的潜力。

Method: 利用LLM的编码能力和参数知识，自动搜索、筛选任务并合成代码解决方案，构建AutoSDT-5K数据集。

Result: AutoSDT-5K数据集包含5,404个任务，专家验证显示93%任务有效，92.2%代码正确。AutoSDT-Coder模型在基准测试中表现显著提升。

Conclusion: AutoSDT解决了数据稀缺问题，其数据集和模型为数据驱动的科学发现提供了有力支持。

Abstract: Despite long-standing efforts in accelerating scientific discovery with AI,
building AI co-scientists remains challenging due to limited high-quality data
for training and evaluation. To tackle this data scarcity issue, we present
AutoSDT, an automatic pipeline that collects high-quality coding tasks in
real-world data-driven discovery workflows. AutoSDT leverages the coding
capabilities and parametric knowledge of LLMs to search for diverse sources,
select ecologically valid tasks, and synthesize accurate task instructions and
code solutions. Using our pipeline, we construct AutoSDT-5K, a dataset of 5,404
coding tasks for data-driven discovery that covers four scientific disciplines
and 756 unique Python packages. To the best of our knowledge, AutoSDT-5K is the
only automatically collected and the largest open dataset for data-driven
scientific discovery. Expert feedback on a subset of 256 tasks shows the
effectiveness of AutoSDT: 93% of the collected tasks are ecologically valid,
and 92.2% of the synthesized programs are functionally correct. Trained on
AutoSDT-5K, the Qwen2.5-Coder-Instruct LLM series, dubbed AutoSDT-Coder, show
substantial improvement on two challenging data-driven discovery benchmarks,
ScienceAgentBench and DiscoveryBench. Most notably, AutoSDT-Coder-32B reaches
the same level of performance as GPT-4o on ScienceAgentBench with a success
rate of 7.8%, doubling the performance of its base model. On DiscoveryBench, it
lifts the hypothesis matching score to 8.1, bringing a 17.4% relative
improvement and closing the gap between open-weight models and GPT-4o.

</details>


### [244] [Accelerating Spectral Clustering under Fairness Constraints](https://arxiv.org/abs/2506.08143)
*Francesco Tonin,Alex Lambert,Johan A. K. Suykens,Volkan Cevher*

Main category: cs.LG

TL;DR: 本文提出了一种高效的公平谱聚类方法（Fair SC），通过将问题转化为凸差函数（DC）框架，并引入新的变量增强策略和优化算法，显著提高了计算效率。


<details>
  <summary>Details</summary>
Motivation: 决策算法的公平性日益重要，本文旨在解决谱聚类中群体公平性的问题，确保每个聚类中不同群体的比例与总体人口一致。

Method: 采用凸差函数（DC）框架，结合变量增强策略和交替方向乘子法（ADMM）优化算法，避免了传统方法中计算昂贵的特征分解。

Result: 实验表明，该方法在合成和真实数据集上均有效，计算速度显著优于现有方法，尤其在大规模问题上表现更优。

Conclusion: 本文为公平聚类在实际应用中的推广提供了重要进展。

Abstract: Fairness of decision-making algorithms is an increasingly important issue. In
this paper, we focus on spectral clustering with group fairness constraints,
where every demographic group is represented in each cluster proportionally as
in the general population. We present a new efficient method for fair spectral
clustering (Fair SC) by casting the Fair SC problem within the difference of
convex functions (DC) framework. To this end, we introduce a novel variable
augmentation strategy and employ an alternating direction method of multipliers
type of algorithm adapted to DC problems. We show that each associated
subproblem can be solved efficiently, resulting in higher computational
efficiency compared to prior work, which required a computationally expensive
eigendecomposition. Numerical experiments demonstrate the effectiveness of our
approach on both synthetic and real-world benchmarks, showing significant
speedups in computation time over prior art, especially as the problem size
grows. This work thus represents a considerable step forward towards the
adoption of fair clustering in real-world applications.

</details>


### [245] [Fully data-driven inverse hyperelasticity with hyper-network neural ODE fields](https://arxiv.org/abs/2506.08146)
*Vahidullah Taç,Amirhossein Amiri-Hezaveh,Manuel K. Rausch,Grace N. Bechtel,Francisco Sahli Costabal,Adrian Buganza Tepole*

Main category: cs.LG

TL;DR: 提出了一种基于神经网络的框架，用于识别异质材料的力学特性，无需封闭形式的本构方程。


<details>
  <summary>Details</summary>
Motivation: 传统方法在处理异质材料时难以捕捉其复杂特性，需要一种更灵活且物理约束的方法。

Method: 结合傅里叶特征的神经网络近似应变场，利用NODE框架发现本构方程，并通过超网络处理异质性。

Result: 数值实验表明，该方法在噪声和异质性条件下仍能准确识别材料特性。

Conclusion: 该方法为异质材料力学特性识别提供了一种鲁棒且通用的替代方案。

Abstract: We propose a new framework for identifying mechanical properties of
heterogeneous materials without a closed-form constitutive equation. Given a
full-field measurement of the displacement field, for instance as obtained from
digital image correlation (DIC), a continuous approximation of the strain field
is obtained by training a neural network that incorporates Fourier features to
effectively capture sharp gradients in the data. A physics-based data-driven
method built upon ordinary neural differential equations (NODEs) is employed to
discover constitutive equations. The NODE framework can represent arbitrary
materials while satisfying constraints in the theory of constitutive equations
by default. To account for heterogeneity, a hyper-network is defined, where the
input is the material coordinate system, and the output is the NODE-based
constitutive equation. The parameters of the hyper-network are optimized by
minimizing a multi-objective loss function that includes penalty terms for
violations of the strong form of the equilibrium equations of elasticity and
the associated Neumann boundary conditions. We showcase the framework with
several numerical examples, including heterogeneity arising from variations in
material parameters, spatial transitions from isotropy to anisotropy, material
identification in the presence of noise, and, ultimately, application to
experimental data. As the numerical results suggest, the proposed approach is
robust and general in identifying the mechanical properties of heterogeneous
materials with very few assumptions, making it a suitable alternative to
classical inverse methods.

</details>


### [246] [BLUR: A Bi-Level Optimization Approach for LLM Unlearning](https://arxiv.org/abs/2506.08164)
*Hadi Reisizadeh,Jinghan Jia,Zhiqi Bu,Bhanukiran Vinzamuri,Anil Ramakrishna,Kai-Wei Chang,Volkan Cevher,Sijia Liu,Mingyi Hong*

Main category: cs.LG

TL;DR: 论文提出了一种新的遗忘问题分层建模方法，通过双层次优化（Bi-Level UnleaRning, BLUR）算法，优先解决遗忘问题，同时保持模型性能，实验表明其优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 确保大语言模型（LLM）在训练后能遗忘特定知识或能力，以满足数据法规和伦理要求，同时避免性能下降。

Method: 提出分层建模方法，将遗忘问题（优先）和保留问题（次要）分开，采用双层次优化（BLUR算法），下层最小化遗忘损失，上层保持模型效用。

Result: BLUR算法在多种遗忘任务、模型和指标上均优于现有方法。

Conclusion: 分层建模和BLUR算法为LLM的遗忘问题提供了更优解决方案，兼具理论保证和实际性能。

Abstract: Enabling large language models (LLMs) to unlearn knowledge and capabilities
acquired during training has proven vital for ensuring compliance with data
regulations and promoting ethical practices in generative AI. Although there
are growing interests in developing various unlearning algorithms, it remains
unclear how to best formulate the unlearning problem. The most popular
formulation uses a weighted sum of forget and retain loss, but it often leads
to performance degradation due to the inherent trade-off between forget and
retain losses. In this work, we argue that it is important to model the
hierarchical structure of the unlearning problem, where the forget problem
(which \textit{unlearns} certain knowledge and/or capabilities) takes priority
over the retain problem (which preserves model utility). This hierarchical
structure naturally leads to a bi-level optimization formulation where the
lower-level objective focuses on minimizing the forget loss, while the
upper-level objective aims to maintain the model's utility. Based on this new
formulation, we propose a novel algorithm, termed Bi-Level UnleaRning
(\texttt{BLUR}), which not only possesses strong theoretical guarantees but
more importantly, delivers superior performance. In particular, our extensive
experiments demonstrate that \texttt{BLUR} consistently outperforms all the
state-of-the-art algorithms across various unlearning tasks, models, and
metrics. Codes are available at
https://github.com/OptimAI-Lab/BLURLLMUnlearning.

</details>


### [247] [UniVarFL: Uniformity and Variance Regularized Federated Learning for Heterogeneous Data](https://arxiv.org/abs/2506.08167)
*Sunny Gupta,Nikita Jangid,Amit Sethi*

Main category: cs.LG

TL;DR: UniVarFL提出了一种新的联邦学习框架，通过两种正则化策略解决非IID数据导致的性能下降问题，显著提升了模型准确性和可扩展性。


<details>
  <summary>Details</summary>
Motivation: 联邦学习在非IID数据下性能下降严重，传统方法计算成本高或适应性差。

Method: UniVarFL采用分类器方差正则化和超球面均匀性正则化，模拟IID训练动态。

Result: 在多个基准数据集上，UniVarFL在准确性上优于现有方法。

Conclusion: UniVarFL是一种高效、可扩展的解决方案，适用于资源受限的实际部署。

Abstract: Federated Learning (FL) often suffers from severe performance degradation
when faced with non-IID data, largely due to local classifier bias. Traditional
remedies such as global model regularization or layer freezing either incur
high computational costs or struggle to adapt to feature shifts. In this work,
we propose UniVarFL, a novel FL framework that emulates IID-like training
dynamics directly at the client level, eliminating the need for global model
dependency. UniVarFL leverages two complementary regularization strategies
during local training: Classifier Variance Regularization, which aligns
class-wise probability distributions with those expected under IID conditions,
effectively mitigating local classifier bias; and Hyperspherical Uniformity
Regularization, which encourages a uniform distribution of feature
representations across the hypersphere, thereby enhancing the model's ability
to generalize under diverse data distributions. Extensive experiments on
multiple benchmark datasets demonstrate that UniVarFL outperforms existing
methods in accuracy, highlighting its potential as a highly scalable and
efficient solution for real-world FL deployments, especially in
resource-constrained settings. Code: https://github.com/sunnyinAI/UniVarFL

</details>


### [248] [Federated Learning on Stochastic Neural Networks](https://arxiv.org/abs/2506.08169)
*Jingqiao Tang,Ryan Bausback,Feng Bao,Richard Archibald*

Main category: cs.LG

TL;DR: 提出了一种基于随机神经网络的联邦学习方法（Federated stochastic neural networks），用于解决本地数据中的潜在噪声问题。


<details>
  <summary>Details</summary>
Motivation: 联邦学习在保护用户隐私的同时，容易受到本地数据中潜在噪声的影响，如测量误差或人为错误。

Method: 在联邦学习框架中引入随机神经网络作为本地模型，以估计数据的真实状态并量化潜在噪声。

Result: 数值实验表明，该方法在处理非独立同分布数据时表现优异。

Conclusion: 随机神经网络的引入显著提升了联邦学习在噪声数据环境下的鲁棒性和性能。

Abstract: Federated learning is a machine learning paradigm that leverages edge
computing on client devices to optimize models while maintaining user privacy
by ensuring that local data remains on the device. However, since all data is
collected by clients, federated learning is susceptible to latent noise in
local datasets. Factors such as limited measurement capabilities or human
errors may introduce inaccuracies in client data. To address this challenge, we
propose the use of a stochastic neural network as the local model within the
federated learning framework. Stochastic neural networks not only facilitate
the estimation of the true underlying states of the data but also enable the
quantification of latent noise. We refer to our federated learning approach,
which incorporates stochastic neural networks as local models, as Federated
stochastic neural networks. We will present numerical experiments demonstrating
the performance and effectiveness of our method, particularly in handling
non-independent and identically distributed data.

</details>


### [249] [FedGA-Tree: Federated Decision Tree using Genetic Algorithm](https://arxiv.org/abs/2506.08176)
*Anh V Nguyen,Diego Klabjan*

Main category: cs.LG

TL;DR: 本文提出了一种基于遗传算法的个性化决策树构建方法，适用于联邦学习中的分类和回归任务，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前联邦学习中决策树方法主要针对分类任务和分类数据，且依赖差分隐私，限制了其应用范围。

Method: 利用遗传算法构建个性化决策树，支持分类和数值数据，适用于分类和回归任务。

Result: 实验表明，该方法优于仅基于本地数据训练的决策树和基准算法。

Conclusion: 遗传算法为联邦学习中的决策树构建提供了更灵活和高效的解决方案。

Abstract: In recent years, with rising concerns for data privacy, Federated Learning
has gained prominence, as it enables collaborative training without the
aggregation of raw data from participating clients. However, much of the
current focus has been on parametric gradient-based models, while nonparametric
counterparts such as decision tree are relatively understudied. Existing
methods for adapting decision trees to Federated Learning generally combine a
greedy tree-building algorithm with differential privacy to produce a global
model for all clients. These methods are limited to classification trees and
categorical data due to the constraints of differential privacy. In this paper,
we explore an alternative approach that utilizes Genetic Algorithm to
facilitate the construction of personalized decision trees and accommodate
categorical and numerical data, thus allowing for both classification and
regression trees. Comprehensive experiments demonstrate that our method
surpasses decision trees trained solely on local data and a benchmark
algorithm.

</details>


### [250] [Correlated Noise Mechanisms for Differentially Private Learning](https://arxiv.org/abs/2506.08201)
*Krishna Pillutla,Jalaj Upadhyay,Christopher A. Choquette-Choo,Krishnamurthy Dvijotham,Arun Ganesh,Monika Henzinger,Jonathan Katz,Ryan McKenna,H. Brendan McMahan,Keith Rush,Thomas Steinke,Abhradeep Thakurta*

Main category: cs.LG

TL;DR: 该专著探讨了差分隐私（DP）中相关噪声机制的设计与分析，重点研究了其在AI和机器学习模型私有训练中的应用，通过加权前缀和估计的核心原语。


<details>
  <summary>Details</summary>
Motivation: 传统DP机制在随机梯度下降（SGD）学习算法的每一步注入独立噪声以保护训练数据的隐私，但研究表明引入（反）相关噪声可以通过在后续步骤中抵消部分噪声来显著改善隐私-效用权衡。

Method: 研究了相关噪声机制（如矩阵机制、因子分解机制和DP-FTRL），并将其应用于学习算法中。

Result: 相关噪声机制在实践中具有重要影响，已在全球范围内实现工业部署。

Conclusion: 相关噪声机制为差分隐私提供了更优的隐私-效用权衡，并在实际应用中展现出潜力。

Abstract: This monograph explores the design and analysis of correlated noise
mechanisms for differential privacy (DP), focusing on their application to
private training of AI and machine learning models via the core primitive of
estimation of weighted prefix sums. While typical DP mechanisms inject
independent noise into each step of a stochastic gradient (SGD) learning
algorithm in order to protect the privacy of the training data, a growing body
of recent research demonstrates that introducing (anti-)correlations in the
noise can significantly improve privacy-utility trade-offs by carefully
canceling out some of the noise added on earlier steps in subsequent steps.
Such correlated noise mechanisms, known variously as matrix mechanisms,
factorization mechanisms, and DP-Follow-the-Regularized-Leader (DP-FTRL) when
applied to learning algorithms, have also been influential in practice, with
industrial deployment at a global scale.

</details>


### [251] [A Machine Learning Approach to Generate Residual Stress Distributions using Sparse Characterization Data in Friction-Stir Processed Parts](https://arxiv.org/abs/2506.08205)
*Shadab Anwar Shaikh,Kranthi Balusu,Ayoub Soulami*

Main category: cs.LG

TL;DR: 论文提出了一种基于机器学习的残余应力生成器（RSG），通过有限测量数据推断全场应力分布，显著减少实验工作量。


<details>
  <summary>Details</summary>
Motivation: 残余应力影响部件性能，但全场分布的实验测定不切实际，需一种高效方法。

Method: 构建大量模拟数据集，基于U-Net架构训练机器学习模型，并通过超参数调优优化性能。

Result: 模型在模拟和实验数据上均表现出高预测精度和泛化能力。

Conclusion: RSG方法可行，能从有限测量中全面理解残余应力分布，大幅降低实验成本。

Abstract: Residual stresses, which remain within a component after processing, can
deteriorate performance. Accurately determining their full-field distributions
is essential for optimizing the structural integrity and longevity. However,
the experimental effort required for full-field characterization is
impractical. Given these challenges, this work proposes a machine learning (ML)
based Residual Stress Generator (RSG) to infer full-field stresses from limited
measurements. An extensive dataset was initially constructed by performing
numerous process simulations with a diverse parameter set. A ML model based on
U-Net architecture was then trained to learn the underlying structure through
systematic hyperparameter tuning. Then, the model's ability to generate
simulated stresses was evaluated, and it was ultimately tested on actual
characterization data to validate its effectiveness. The model's prediction of
simulated stresses shows that it achieved excellent predictive accuracy and
exhibited a significant degree of generalization, indicating that it
successfully learnt the latent structure of residual stress distribution. The
RSG's performance in predicting experimentally characterized data highlights
the feasibility of the proposed approach in providing a comprehensive
understanding of residual stress distributions from limited measurements,
thereby significantly reducing experimental efforts.

</details>


### [252] [What makes an Ensemble (Un) Interpretable?](https://arxiv.org/abs/2506.08216)
*Shahaf Bassan,Guy Amir,Meirav Zehavi,Guy Katz*

Main category: cs.LG

TL;DR: 论文探讨了集成模型的可解释性问题，通过计算复杂性理论分析了影响其可解释性的因素，如基础模型的数量、大小和类型。


<details>
  <summary>Details</summary>
Motivation: 集成模型（如提升树）通常被视为黑箱，缺乏对其可解释性的严格数学理解。本文旨在填补这一空白。

Method: 应用计算复杂性理论，研究不同集成配置下生成解释的挑战。

Result: 发现基础模型的数量对复杂性影响显著，小规模决策树集成可高效解释，而线性模型集成即使数量少仍难以解释。

Conclusion: 研究为理解集成模型的可解释性提供了更坚实的基础，强调了计算复杂性视角的重要性。

Abstract: Ensemble models are widely recognized in the ML community for their limited
interpretability. For instance, while a single decision tree is considered
interpretable, ensembles of trees (e.g., boosted trees) are often treated as
black-boxes. Despite this folklore recognition, there remains a lack of
rigorous mathematical understanding of what particularly makes an ensemble
(un)-interpretable, including how fundamental factors like the (1) *number*,
(2) *size*, and (3) *type* of base models influence its interpretability. In
this work, we seek to bridge this gap by applying concepts from computational
complexity theory to study the challenges of generating explanations for
various ensemble configurations. Our analysis uncovers nuanced complexity
patterns influenced by various factors. For example, we demonstrate that under
standard complexity assumptions like P$\neq$NP, interpreting ensembles remains
intractable even when base models are of constant size. Surprisingly, the
complexity changes drastically with the number of base models: small ensembles
of decision trees are efficiently interpretable, whereas interpreting ensembles
with even a constant number of linear models remains intractable. We believe
that our findings provide a more robust foundation for understanding the
interpretability of ensembles, emphasizing the benefits of examining it through
a computational complexity lens.

</details>


### [253] [Mondrian: Transformer Operators via Domain Decomposition](https://arxiv.org/abs/2506.08226)
*Arthur Feeney,Kuei-Hsiang Huang,Aparna Chandramowlishwaran*

Main category: cs.LG

TL;DR: Mondrian是一种基于Transformer的算子学习方法，通过将域分解为不重叠的子域并应用注意力机制，解决了高分辨率多尺度域中的计算挑战。


<details>
  <summary>Details</summary>
Motivation: 传统Transformer算子在高分辨率多尺度域中因注意力的二次成本和与离散化的耦合而难以扩展。

Method: Mondrian将域分解为子域，在每个子域内使用神经算子，并通过基于softmax的内积计算子域间的注意力。

Result: 在Allen-Cahn和Navier-Stokes PDEs上表现优异，支持无需重新训练的分辨率扩展。

Conclusion: Mondrian展示了域分解注意力在可扩展和通用神经算子中的潜力。

Abstract: Operator learning enables data-driven modeling of partial differential
equations (PDEs) by learning mappings between function spaces. However, scaling
transformer-based operator models to high-resolution, multiscale domains
remains a challenge due to the quadratic cost of attention and its coupling to
discretization. We introduce \textbf{Mondrian}, transformer operators that
decompose a domain into non-overlapping subdomains and apply attention over
sequences of subdomain-restricted functions. Leveraging principles from domain
decomposition, Mondrian decouples attention from discretization. Within each
subdomain, it replaces standard layers with expressive neural operators, and
attention across subdomains is computed via softmax-based inner products over
functions. The formulation naturally extends to hierarchical windowed and
neighborhood attention, supporting both local and global interactions. Mondrian
achieves strong performance on Allen-Cahn and Navier-Stokes PDEs, demonstrating
resolution scaling without retraining. These results highlight the promise of
domain-decomposed attention for scalable and general-purpose neural operators.

</details>


### [254] [Scaling Laws of Motion Forecasting and Planning -- A Technical Report](https://arxiv.org/abs/2506.08228)
*Mustafa Baniodeh,Kratarth Goel,Scott Ettinger,Carlos Fuertes,Ari Seff,Tim Shen,Cole Gulino,Chenjie Yang,Ghassen Jerfel,Dokook Choe,Rui Wang,Vinutha Kallem,Sergio Casas,Rami Al-Rfou,Benjamin Sapp,Dragomir Anguelov*

Main category: cs.LG

TL;DR: 研究了自动驾驶领域中编码器-解码器自回归Transformer模型在联合运动预测和规划任务中的经验缩放规律，发现性能随计算预算呈幂律增长，并探讨了训练与推理时的最优缩放策略。


<details>
  <summary>Details</summary>
Motivation: 探索自动驾驶中大规模模型在运动预测和规划任务中的性能提升规律，以优化计算资源分配和模型开发。

Method: 使用50万小时驾驶数据集，分析模型性能与计算预算的关系，研究模型参数与数据量的最优缩放比例，以及推理时计算效率。

Result: 性能随计算预算呈幂律增长，训练时模型规模需比数据量增长快1.5倍；推理时小模型通过采样和聚类可媲美大模型，直至交叉点。

Conclusion: 优化训练和推理的缩放策略是提升自动驾驶模型性能的关键，同时利用其他车辆的通用数据可缓解机器人数据稀缺问题。

Abstract: We study the empirical scaling laws of a family of encoder-decoder
autoregressive transformer models on the task of joint motion forecasting and
planning in the autonomous driving domain. Using a 500 thousand hours driving
dataset, we demonstrate that, similar to language modeling, model performance
improves as a power-law function of the total compute budget, and we observe a
strong correlation between model training loss and model evaluation metrics.
Most interestingly, closed-loop metrics also improve with scaling, which has
important implications for the suitability of open-loop metrics for model
development and hill climbing. We also study the optimal scaling of the number
of transformer parameters and the training data size for a training
compute-optimal model. We find that as the training compute budget grows,
optimal scaling requires increasing the model size 1.5x as fast as the dataset
size. We also study inference-time compute scaling, where we observe that
sampling and clustering the output of smaller models makes them competitive
with larger models, up to a crossover point beyond which a larger models
becomes more inference-compute efficient. Overall, our experimental results
demonstrate that optimizing the training and inference-time scaling properties
of motion forecasting and planning models is a key lever for improving their
performance to address a wide variety of driving scenarios. Finally, we briefly
study the utility of training on general logged driving data of other agents to
improve the performance of the ego-agent, an important research area to address
the scarcity of robotics data for large capacity models training.

</details>


### [255] [Ensuring Reliability of Curated EHR-Derived Data: The Validation of Accuracy for LLM/ML-Extracted Information and Data (VALID) Framework](https://arxiv.org/abs/2506.08231)
*Melissa Estevez,Nisha Singh,Lauren Dyson,Blythe Adamson,Qianyu Yuan,Megan W. Hildner,Erin Fidyk,Olive Mbah,Farhad Khan,Kathi Seidl-Rathkopf,Aaron B. Cohen*

Main category: cs.LG

TL;DR: 提出一个评估LLM提取临床数据质量的综合框架，包括性能基准测试、自动验证和复制分析，以提高数据可靠性和公平性。


<details>
  <summary>Details</summary>
Motivation: LLM在提取临床数据时虽高效，但存在可靠性、准确性和公平性问题，现有框架未能完全解决。

Method: 整合变量级性能基准测试、自动一致性检查和复制分析，支持偏差评估。

Result: 框架能识别需改进的变量、系统检测潜在错误，并确认数据适用性。

Conclusion: 该框架为LLM提取的RWD提供了严格透明的评估方法，支持AI在肿瘤学研究中的可信使用。

Abstract: Large language models (LLMs) are increasingly used to extract clinical data
from electronic health records (EHRs), offering significant improvements in
scalability and efficiency for real-world data (RWD) curation in oncology.
However, the adoption of LLMs introduces new challenges in ensuring the
reliability, accuracy, and fairness of extracted data, which are essential for
research, regulatory, and clinical applications. Existing quality assurance
frameworks for RWD and artificial intelligence do not fully address the unique
error modes and complexities associated with LLM-extracted data. In this paper,
we propose a comprehensive framework for evaluating the quality of clinical
data extracted by LLMs. The framework integrates variable-level performance
benchmarking against expert human abstraction, automated verification checks
for internal consistency and plausibility, and replication analyses comparing
LLM-extracted data to human-abstracted datasets or external standards. This
multidimensional approach enables the identification of variables most in need
of improvement, systematic detection of latent errors, and confirmation of
dataset fitness-for-purpose in real-world research. Additionally, the framework
supports bias assessment by stratifying metrics across demographic subgroups.
By providing a rigorous and transparent method for assessing LLM-extracted RWD,
this framework advances industry standards and supports the trustworthy use of
AI-powered evidence generation in oncology research and practice.

</details>


### [256] [Dealing with the Evil Twins: Improving Random Augmentation by Addressing Catastrophic Forgetting of Diverse Augmentations](https://arxiv.org/abs/2506.08240)
*Dongkyu Cho,Rumi Chunara*

Main category: cs.LG

TL;DR: 本文重新审视随机数据增强的局限性，提出一种简单方法解决其导致的特征失真问题，显著提升泛化性能。


<details>
  <summary>Details</summary>
Motivation: 随机数据增强成本低但效果有限，其随机性可能导致特征失真，类似灾难性遗忘。

Method: 提出一种简单方法，通过解决遗忘问题改进随机增强的泛化效果。

Result: 在多个单源域泛化基准测试中表现出强泛化性能。

Conclusion: 改进后的随机增强方法在成本与效果间取得平衡，具有实用价值。

Abstract: Data augmentation is a promising tool for enhancing out-of-distribution
generalization, where the key is to produce diverse, challenging variations of
the source domain via costly targeted augmentations that maximize its
generalization effect. Conversely, random augmentation is inexpensive but is
deemed suboptimal due to its limited effect. In this paper, we revisit random
augmentation and explore methods to address its shortcomings. We show that the
stochastic nature of random augmentation can produce a set of colliding
augmentations that distorts the learned features, similar to catastrophic
forgetting. We propose a simple solution that improves the generalization
effect of random augmentation by addressing forgetting, which displays strong
generalization performance across various single source domain generalization
(sDG) benchmarks.

</details>


### [257] [MOBODY: Model Based Off-Dynamics Offline Reinforcement Learning](https://arxiv.org/abs/2506.08460)
*Yihong Guo,Yu Yang,Pan Xu,Anqi Liu*

Main category: cs.LG

TL;DR: MOBODY是一种基于模型的离线强化学习算法，通过生成目标域中的合成过渡数据来解决源域和目标域动态不匹配的问题，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有方法因目标域数据有限，无法充分探索目标域，导致策略性能受限。

Method: MOBODY通过学习共享潜在表示生成目标域合成数据，并结合Q加权行为克隆损失稳定训练。

Result: 在MuJoCo基准测试中，MOBODY显著优于现有基线方法，尤其在复杂场景中表现突出。

Conclusion: MOBODY通过动态学习和数据增强有效解决了跨域离线强化学习问题，为未来研究提供了新方向。

Abstract: We study the off-dynamics offline reinforcement learning problem, where the
goal is to learn a policy from offline datasets collected from source and
target domains with mismatched transition. Existing off-dynamics offline RL
methods typically either filter source transitions that resemble those of the
target domain or apply reward augmentation to source data, both constrained by
the limited transitions available from the target domain. As a result, the
learned policy is unable to explore target domain beyond the offline datasets.
We propose MOBODY, a Model-Based Off-Dynamics offline RL algorithm that
addresses this limitation by enabling exploration of the target domain via
learned dynamics. MOBODY generates new synthetic transitions in the target
domain through model rollouts, which are used as data augmentation during
offline policy learning. Unlike existing model-based methods that learn
dynamics from a single domain, MOBODY tackles the challenge of mismatched
dynamics by leveraging both source and target datasets. Directly merging these
datasets can bias the learned model toward source dynamics. Instead, MOBODY
learns target dynamics by discovering a shared latent representation of states
and transitions across domains through representation learning. To stabilize
training, MOBODY incorporates a behavior cloning loss that regularizes the
policy. Specifically, we introduce a Q-weighted behavior cloning loss that
regularizes the policy toward actions with high target-domain Q-values, rather
than uniformly imitating all actions in the dataset. These Q-values are learned
from an enhanced target dataset composed of offline target data, augmented
source data, and rollout data from the learned target dynamics. We evaluate
MOBODY on MuJoCo benchmarks and show that it significantly outperforms
state-of-the-art baselines, with especially pronounced improvements in
challenging scenarios.

</details>


### [258] [Temporalizing Confidence: Evaluation of Chain-of-Thought Reasoning with Signal Temporal Logic](https://arxiv.org/abs/2506.08243)
*Zhenjiang Mao,Artem Bisliouk,Rohith Reddy Nama,Ivan Ruchkin*

Main category: cs.LG

TL;DR: 提出了一种基于信号时序逻辑（STL）的框架，用于评估大型语言模型（LLM）在数学推理任务中的逐步置信度，并通过不确定性重塑策略提升校准效果。


<details>
  <summary>Details</summary>
Motivation: LLM在数学推理任务中表现优异，但容易产生高置信度却错误的输出，这在教育等领域可能带来风险。

Method: 利用STL建模逐步置信度为时序信号，定义约束条件并计算鲁棒性分数，同时引入不确定性重塑策略。

Result: 实验表明，该方法显著提升了校准指标，提供了比传统方法更可靠的不确定性估计。

Conclusion: 提出的框架有效解决了LLM在数学推理中的置信度问题，为高风险领域提供了更可靠的解决方案。

Abstract: Large Language Models (LLMs) have shown impressive performance in
mathematical reasoning tasks when guided by Chain-of-Thought (CoT) prompting.
However, they tend to produce highly confident yet incorrect outputs, which
poses significant risks in domains like education, where users may lack the
expertise to assess reasoning steps. To address this, we propose a structured
framework that models stepwise confidence as a temporal signal and evaluates it
using Signal Temporal Logic (STL). In particular, we define formal STL-based
constraints to capture desirable temporal properties and compute robustness
scores that serve as structured, interpretable confidence estimates. Our
approach also introduces a set of uncertainty reshaping strategies to enforce
smoothness, monotonicity, and causal consistency across the reasoning
trajectory. Experiments show that our approach consistently improves
calibration metrics and provides more reliable uncertainty estimates than
conventional confidence aggregation and post-hoc calibration.

</details>


### [259] [Parameter-free approximate equivariance for tasks with finite group symmetry](https://arxiv.org/abs/2506.08244)
*Riccardo Ali,Pietro Liò,Jamie Vicary*

Main category: cs.LG

TL;DR: 提出一种零参数的近似等变方法，通过在损失函数中添加额外项，实现潜在空间中的有限群等变性，性能优于现有方法且参数更少。


<details>
  <summary>Details</summary>
Motivation: 现有等变神经网络计算密集、参数多且架构受限，需更简单高效的方法。

Method: 在潜在空间中学习群表示，并通过损失函数中的额外项施加近似等变性。

Result: 实验表明，网络倾向于学习正则表示，且在多个数据集上性能优于现有方法，参数更少。

Conclusion: 该方法简单高效，能以更少参数实现近似等变性，性能优越。

Abstract: Equivariant neural networks incorporate symmetries through group actions,
embedding them as an inductive bias to improve performance on a wide variety of
tasks. However, existing equivariant methods can be computationally intensive,
with high parameter counts, and are often tied to a specific architecture. We
propose a simple zero-parameter approach that imposes approximate equivariance
for a finite group in the latent representation, as an additional term in the
loss function. We conduct experiments which allow the network to learn a group
representation on the latent space, and show in every case it prefers to learn
the regular representation. Fixing this action on the latent space, this yields
a simple method to impose approximate equivariance as an additional loss
penalty. We benchmark our approach on three datasets and compare it against
several existing equivariant methods, showing that in many cases it achieves
similar or better performance for a fraction of the parameters.

</details>


### [260] [How to Provably Improve Return Conditioned Supervised Learning?](https://arxiv.org/abs/2506.08463)
*Zhishuai Liu,Yu Yang,Ruhan Wang,Pan Xu,Dongruo Zhou*

Main category: cs.LG

TL;DR: 论文提出了一种名为Reinforced RCSL的新框架，通过引入“in-distribution optimal return-to-go”概念，解决了传统RCSL方法缺乏拼接能力的问题，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 传统RCSL方法虽然简单稳定，但受限于离线数据集的质量，缺乏拼接能力。为了解决这一问题，作者提出了Reinforced RCSL。

Method: 提出Reinforced RCSL框架，引入“in-distribution optimal return-to-go”机制，避免复杂的回报增强技术。

Result: 理论分析和实验结果均表明，Reinforced RCSL能显著优于标准RCSL方法。

Conclusion: Reinforced RCSL通过优化回报机制，有效提升了RCSL的性能，为离线强化学习提供了新思路。

Abstract: In sequential decision-making problems, Return-Conditioned Supervised
Learning (RCSL) has gained increasing recognition for its simplicity and
stability in modern decision-making tasks. Unlike traditional offline
reinforcement learning (RL) algorithms, RCSL frames policy learning as a
supervised learning problem by taking both the state and return as input. This
approach eliminates the instability often associated with temporal difference
(TD) learning in offline RL. However, RCSL has been criticized for lacking the
stitching property, meaning its performance is inherently limited by the
quality of the policy used to generate the offline dataset. To address this
limitation, we propose a principled and simple framework called Reinforced
RCSL. The key innovation of our framework is the introduction of a concept we
call the in-distribution optimal return-to-go. This mechanism leverages our
policy to identify the best achievable in-dataset future return based on the
current state, avoiding the need for complex return augmentation techniques.
Our theoretical analysis demonstrates that Reinforced RCSL can consistently
outperform the standard RCSL approach. Empirical results further validate our
claims, showing significant performance improvements across a range of
benchmarks.

</details>


### [261] [SHIELD: Secure Hypernetworks for Incremental Expansion Learning Defense](https://arxiv.org/abs/2506.08255)
*Patryk Krukowski,Łukasz Gorczyca,Piotr Helm,Kamil Książek,Przemysław Spurek*

Main category: cs.LG

TL;DR: SHIELD结合超网络和区间算术，解决深度神经网络的灾难性遗忘和对抗攻击问题。


<details>
  <summary>Details</summary>
Motivation: 传统深度神经网络存在灾难性遗忘和对对抗攻击的脆弱性，目前没有模型能同时解决这两个问题。

Method: SHIELD利用超网络生成任务嵌入向量，动态为目标模型生成权重，并结合区间算术提供严格的安全保证。

Result: SHIELD能够在持续学习中增强安全性，同时保持网络的适应性。

Conclusion: SHIELD为持续学习中的安全性挑战提供了创新解决方案。

Abstract: Traditional deep neural networks suffer from several limitations, including
catastrophic forgetting. When models are adapted to new datasets, they tend to
quickly forget previously learned knowledge. Another significant issue is the
lack of robustness to even small perturbations in the input data. In practice,
we can often easily perform adversarial attacks and change the network's
predictions, adding minimal noise to the input. Dedicated architectures and
training procedures can solve each of the above problems separately.
Unfortunately, currently, no model can simultaneously address both catastrophic
forgetting and vulnerability to adversarial attacks. We introduce SHIELD
(Secure Hypernetworks for Incremental Expansion and Learning Defense), a novel
approach that integrates a hypernetwork-based continual learning approach with
interval arithmetic. SHIELD use the hypernetwork to transfer trainable task
embedding vectors into the weights of a target model dedicated to specific
data. This paradigm allows for the dynamic generation of separate networks for
each subtask, while the hypernetwork aggregates and analyzes information across
all tasks. The target model takes in the input a data sample with a defined
interval range, and by creating a hypercube, produces a prediction for the
given range. Therefore, such target models provide strict guarantees against
all possible attacks for data samples within the interval range. Our approach
enhances security without sacrificing network adaptability, addressing the
overlooked challenge of safety in continual learning.

</details>


### [262] [Reinforcement Learning from Human Feedback with High-Confidence Safety Constraints](https://arxiv.org/abs/2506.08266)
*Yaswanth Chittepu,Blossom Metevier,Will Schwarzer,Austin Hoag,Scott Niekum,Philip S. Thomas*

Main category: cs.LG

TL;DR: HC-RLHF是一种高置信度安全强化学习方法，通过解耦人类偏好为帮助性和无害性，提供安全保证的同时最大化帮助性。


<details>
  <summary>Details</summary>
Motivation: 现有语言模型对齐方法常将安全性与帮助性视为权衡，导致敏感领域不可接受的响应。

Method: HC-RLHF训练奖励模型和成本模型，分两步优化：先悲观约束下优化奖励，再通过安全测试验证性能。

Result: 理论证明HC-RLHF不会以超过用户指定阈值的概率返回不安全解，实验表明其在多个模型上优于现有方法。

Conclusion: HC-RLHF能高概率生成安全模型，并在无害性和帮助性上优于先前方法。

Abstract: Existing approaches to language model alignment often treat safety as a
tradeoff against helpfulness, which can lead to unacceptable responses in
sensitive domains. To ensure reliable performance in such settings, we propose
High-Confidence Safe Reinforcement Learning from Human Feedback (HC-RLHF), a
method that provides high-confidence safety guarantees while maximizing
helpfulness. Similar to previous methods, HC-RLHF explicitly decouples human
preferences into helpfulness and harmlessness (safety), which are learned by
training a reward model and a cost model, respectively. It then employs a
two-step process to find safe solutions. In the first step, it optimizes the
reward function under an intentionally pessimistic version of the cost
constraint. In the second step, the trained model undergoes a safety test to
verify whether its performance stays within an upper-confidence bound of the
actual cost constraint. We provide a theoretical analysis of HC-RLHF, including
proof that it will not return an unsafe solution with a probability greater
than a user-specified threshold. For our empirical analysis, we apply HC-RLHF
to align three different language models (Qwen2-1.5B, Qwen2.5-3B, and
LLaMa3.2-3B) with human preferences. Our results demonstrate that HC-RLHF
produces safe models with high probability and can improve harmlessness and
helpfulness compared to previous methods.

</details>


### [263] [Sparse Interpretable Deep Learning with LIES Networks for Symbolic Regression](https://arxiv.org/abs/2506.08267)
*Mansooreh Montazerin,Majd Al Aawar,Antonio Ortega,Ajitesh Srivastava*

Main category: cs.LG

TL;DR: LIES框架通过固定神经网络架构和优化策略，实现了高效且可解释的符号回归，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有符号回归方法在可扩展性和符号一致性上存在不足，LIES旨在解决这些问题。

Method: 采用固定神经网络架构（LIES）和优化策略（过采样、稀疏损失函数、剪枝）提取紧凑公式。

Result: 实验表明LIES生成的公式稀疏且准确，优于基线方法。

Conclusion: LIES框架通过设计优化策略，显著提升了符号回归的性能和可解释性。

Abstract: Symbolic regression (SR) aims to discover closed-form mathematical
expressions that accurately describe data, offering interpretability and
analytical insight beyond standard black-box models. Existing SR methods often
rely on population-based search or autoregressive modeling, which struggle with
scalability and symbolic consistency. We introduce LIES (Logarithm, Identity,
Exponential, Sine), a fixed neural network architecture with interpretable
primitive activations that are optimized to model symbolic expressions. We
develop a framework to extract compact formulae from LIES networks by training
with an appropriate oversampling strategy and a tailored loss function to
promote sparsity and to prevent gradient instability. After training, it
applies additional pruning strategies to further simplify the learned
expressions into compact formulae. Our experiments on SR benchmarks show that
the LIES framework consistently produces sparse and accurate symbolic formulae
outperforming all baselines. We also demonstrate the importance of each design
component through ablation studies.

</details>


### [264] [SWAT-NN: Simultaneous Weights and Architecture Training for Neural Networks in a Latent Space](https://arxiv.org/abs/2506.08270)
*Zitong Huang,Mansooreh Montazerin,Ajitesh Srivastava*

Main category: cs.LG

TL;DR: 提出一种同时优化神经网络结构和权重的方法，通过连续潜在空间嵌入架构和参数信息，实现高效模型设计。


<details>
  <summary>Details</summary>
Motivation: 传统神经网络设计依赖手动试错或离散化的架构搜索与权重优化，效率低下且分离。

Method: 训练多尺度自编码器嵌入架构和参数信息，通过梯度下降在潜在空间中联合优化结构和权重，加入稀疏性和紧凑性惩罚。

Result: 在合成回归任务中成功发现性能优越的稀疏紧凑神经网络。

Conclusion: 该方法为神经网络设计提供了一种高效且统一的优化框架。

Abstract: Designing neural networks typically relies on manual trial and error or a
neural architecture search (NAS) followed by weight training. The former is
time-consuming and labor-intensive, while the latter often discretizes
architecture search and weight optimization. In this paper, we propose a
fundamentally different approach that simultaneously optimizes both the
architecture and the weights of a neural network. Our framework first trains a
universal multi-scale autoencoder that embeds both architectural and parametric
information into a continuous latent space, where functionally similar neural
networks are mapped closer together. Given a dataset, we then randomly
initialize a point in the embedding space and update it via gradient descent to
obtain the optimal neural network, jointly optimizing its structure and
weights. The optimization process incorporates sparsity and compactness
penalties to promote efficient models. Experiments on synthetic regression
tasks demonstrate that our method effectively discovers sparse and compact
neural networks with strong performance.

</details>


### [265] [Universal Differential Equations for Scientific Machine Learning of Node-Wise Battery Dynamics in Smart Grids](https://arxiv.org/abs/2506.08272)
*Tarushri N. S.*

Main category: cs.LG

TL;DR: 论文提出了一种基于通用微分方程（UDEs）的方法，结合神经网络与物理微分方程，用于智能电网中电池动态建模，解决了传统方法难以捕捉未建模动态的问题。


<details>
  <summary>Details</summary>
Motivation: 智能电网中电池动态建模因太阳能输入的随机性和家庭负载的变异性而具有挑战性，传统方法难以泛化和捕捉未建模的动态。

Method: 通过将神经残差嵌入物理启发的电池ODE中，学习节点特定的电池演化，使用合成但真实的太阳能和负载数据进行模拟。

Result: 实验表明，训练的UDE与真实电池轨迹高度吻合，收敛行为平滑，长期预测稳定。

Conclusion: UDE方法在分散式能源网络中电池建模可行，对实时控制和优化可再生能源智能电网具有广泛意义。

Abstract: Universal Differential Equations (UDEs), which blend neural networks with
physical differential equations, have emerged as a powerful framework for
scientific machine learning (SciML), enabling data-efficient, interpretable,
and physically consistent modeling. In the context of smart grid systems,
modeling node-wise battery dynamics remains a challenge due to the
stochasticity of solar input and variability in household load profiles.
Traditional approaches often struggle with generalization and fail to capture
unmodeled residual dynamics. This work proposes a UDE-based approach to learn
node-specific battery evolution by embedding a neural residual into a
physically inspired battery ODE. Synthetic yet realistic solar generation and
load demand data are used to simulate battery dynamics over time. The neural
component learns to model unobserved or stochastic corrections arising from
heterogeneity in node demand and environmental conditions. Comprehensive
experiments reveal that the trained UDE aligns closely with ground truth
battery trajectories, exhibits smooth convergence behavior, and maintains
stability in long-term forecasts. These findings affirm the viability of
UDE-based SciML approaches for battery modeling in decentralized energy
networks and suggest broader implications for real-time control and
optimization in renewable-integrated smart grids.

</details>


### [266] [The Impact of Feature Scaling In Machine Learning: Effects on Regression and Classification Tasks](https://arxiv.org/abs/2506.08274)
*João Manoel Herrera Pinheiro,Suzana Vilas Boas de Oliveira,Thiago Henrique Segreto Silva,Pedro Antonio Rabelo Saraiva,Enzo Ferreira de Souza,Leonardo André Ambrosio,Marcelo Becker*

Main category: cs.LG

TL;DR: 该研究系统评估了12种特征缩放技术对14种机器学习算法和16个数据集的影响，发现集成方法对缩放不敏感，而其他模型性能显著依赖缩放选择。


<details>
  <summary>Details</summary>
Motivation: 解决特征缩放技术缺乏全面研究的问题，为实践者提供模型特定的缩放选择指导。

Method: 评估12种缩放技术对14种算法和16个数据集的影响，分析预测性能和计算成本。

Result: 集成方法（如随机森林、XGBoost等）对缩放不敏感，而逻辑回归、SVM等模型性能显著依赖缩放选择。

Conclusion: 研究为实践者提供了模型特定的特征缩放技术选择指南，并公开了所有代码和结果以确保透明性和可重复性。

Abstract: This research addresses the critical lack of comprehensive studies on feature
scaling by systematically evaluating 12 scaling techniques - including several
less common transformations - across 14 different Machine Learning algorithms
and 16 datasets for classification and regression tasks. We meticulously
analyzed impacts on predictive performance (using metrics such as accuracy,
MAE, MSE, and $R^2$) and computational costs (training time, inference time,
and memory usage). Key findings reveal that while ensemble methods (such as
Random Forest and gradient boosting models like XGBoost, CatBoost and LightGBM)
demonstrate robust performance largely independent of scaling, other widely
used models such as Logistic Regression, SVMs, TabNet, and MLPs show
significant performance variations highly dependent on the chosen scaler. This
extensive empirical analysis, with all source code, experimental results, and
model parameters made publicly available to ensure complete transparency and
reproducibility, offers model-specific crucial guidance to practitioners on the
need for an optimal selection of feature scaling techniques.

</details>


### [267] [From Debate to Equilibrium: Belief-Driven Multi-Agent LLM Reasoning via Bayesian Nash Equilibrium](https://arxiv.org/abs/2506.08292)
*Xie Yi,Zhanke Zhou,Chentao Cao,Qiyu Niu,Tongliang Liu,Bo Han*

Main category: cs.LG

TL;DR: ECON提出了一种基于贝叶斯纳什均衡的多LLM协调框架，通过分层强化学习实现高效推理，显著降低计算成本并提升性能。


<details>
  <summary>Details</summary>
Motivation: 解决多智能体框架中高计算成本和缺乏收敛保证的问题。

Method: 将多LLM协调建模为不完全信息博弈，通过贝叶斯纳什均衡和分层强化学习实现分布式推理与集中输出。

Result: ECON在六个基准测试中平均性能提升11.2%，并具有更强的可扩展性。

Conclusion: ECON为构建更强大的多LLM系统提供了高效且可扩展的解决方案。

Abstract: Multi-agent frameworks can substantially boost the reasoning power of large
language models (LLMs), but they typically incur heavy computational costs and
lack convergence guarantees. To overcome these challenges, we recast multi-LLM
coordination as an incomplete-information game and seek a Bayesian Nash
equilibrium (BNE), in which each agent optimally responds to its probabilistic
beliefs about the strategies of others. We introduce Efficient Coordination via
Nash Equilibrium (ECON), a hierarchical reinforcement-learning paradigm that
marries distributed reasoning with centralized final output. Under ECON, each
LLM independently selects responses that maximize its expected reward,
conditioned on its beliefs about co-agents, without requiring costly
inter-agent exchanges. We mathematically prove that ECON attains a markedly
tighter regret bound than non-equilibrium multi-agent schemes. Empirically,
ECON outperforms existing multi-LLM approaches by 11.2% on average across six
benchmarks spanning complex reasoning and planning tasks. Further experiments
demonstrate ECON's ability to flexibly incorporate additional models,
confirming its scalability and paving the way toward larger, more powerful
multi-LLM ensembles. The code is publicly available at:
https://github.com/tmlr-group/ECON.

</details>


### [268] [From Passive to Active Reasoning: Can Large Language Models Ask the Right Questions under Incomplete Information?](https://arxiv.org/abs/2506.08295)
*Zhanke Zhou,Xiao Feng,Zhaocheng Zhu,Jiangchao Yao,Sanmi Koyejo,Bo Han*

Main category: cs.LG

TL;DR: AR-Bench是一个新基准，用于评估大语言模型（LLM）的主动推理能力，发现当前模型在主动推理方面表现较差。


<details>
  <summary>Details</summary>
Motivation: 现有基准主要评估被动推理，而主动推理（需与外部系统交互获取信息）缺乏系统研究。

Method: 设计了AR-Bench，包含三类任务（侦探案例、情境谜题和猜数字），模拟真实场景并测试常识、逻辑和符号推理能力。

Result: 当代LLM在主动推理中表现不佳，难以获取或利用所需信息，且现有策略改进有限。

Conclusion: 需发展新方法（如交互学习、实时反馈）以提升主动推理能力，AR-Bench已公开。

Abstract: While existing benchmarks probe the reasoning abilities of large language
models (LLMs) across diverse domains, they predominantly assess passive
reasoning, providing models with all the information needed to reach a
solution. By contrast, active reasoning-where an LLM must interact with
external systems to acquire missing evidence or data-has received little
systematic attention. To address this shortfall, we present AR-Bench, a novel
benchmark designed explicitly to evaluate an LLM's active reasoning skills.
AR-Bench comprises three task families-detective cases, situation puzzles, and
guessing numbers-that together simulate real-world, agentic scenarios and
measure performance across commonsense, logical, and symbolic reasoning
challenges. Empirical evaluation on AR-Bench demonstrates that contemporary
LLMs exhibit pronounced difficulties with active reasoning: they frequently
fail to acquire or leverage the information needed to solve tasks. This gap
highlights a stark divergence between their passive and active reasoning
abilities. Moreover, ablation studies indicate that even advanced strategies,
such as tree-based searching or post-training approaches, yield only modest
gains and fall short of the levels required for real-world deployment.
Collectively, these findings highlight the critical need to advance methodology
for active reasoning, e.g., incorporating interactive learning, real-time
feedback loops, and environment-aware objectives for training. The benchmark is
publicly available at: https://github.com/tmlr-group/AR-Bench.

</details>


### [269] [H$^2$GFM: Towards unifying Homogeneity and Heterogeneity on Text-Attributed Graphs](https://arxiv.org/abs/2506.08298)
*Trung-Kien Nguyen,Heng Ping,Shixuan Li,Peiyu Zhang,Nikos Kanakaris,Nicholas Kotov,Paul Bogdan*

Main category: cs.LG

TL;DR: 提出了一种名为H$^2$GFM的新框架，用于统一处理同质和异质文本属性图（TAGs），通过上下文编码和自适应图变换器（CGT）提升图基础模型（GFM）的能力。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注同质文本属性图（HoTAGs），而忽略了异质文本属性图（HeTAGs），限制了图基础模型（GFM）的泛化能力和应用范围。

Method: 提出H$^2$GFM框架，通过统一文本空间投影多样元关系，采用上下文编码捕捉空间和高阶语义关系，并设计上下文自适应图变换器（CGT）获取节点表示。

Result: 在多种HoTAGs和HeTAGs以及学习场景下的实验证明了模型的有效性。

Conclusion: H$^2$GFM框架成功扩展了GFM的能力，适用于更广泛的图类型和任务。

Abstract: The growing interests and applications of graph learning in diverse domains
have propelled the development of a unified model generalizing well across
different graphs and tasks, known as the Graph Foundation Model (GFM). Existing
research has leveraged text-attributed graphs (TAGs) to tackle the
heterogeneity in node features among graphs. However, they primarily focus on
homogeneous TAGs (HoTAGs), leaving heterogeneous TAGs (HeTAGs), where multiple
types of nodes/edges reside, underexplored. To enhance the capabilities and
applications of GFM, we introduce H$^2$GFM, a novel framework designed to
generalize across both HoTAGs and HeTAGs. Our model projects diverse
meta-relations among graphs under a unified textual space, and employs a
context encoding to capture spatial and higher-order semantic relationships. To
achieve robust node representations, we propose a novel context-adaptive graph
transformer (CGT), effectively capturing information from both context
neighbors and their relationships. Furthermore, we employ a mixture of CGT
experts to capture the heterogeneity in structural patterns among graph types.
Comprehensive experiments on a wide range of HoTAGs and HeTAGs as well as
learning scenarios demonstrate the effectiveness of our model.

</details>


### [270] [Learnable Spatial-Temporal Positional Encoding for Link Prediction](https://arxiv.org/abs/2506.08309)
*Katherine Tieu,Dongqi Fu,Zihao Li,Ross Maciejewski,Jingrui He*

Main category: cs.LG

TL;DR: 论文提出了一种可学习的时空位置编码方法L-STEP，解决了现有位置编码在适应性、动态性和计算效率上的不足，并在多个数据集和任务中验证了其优越性。


<details>
  <summary>Details</summary>
Motivation: 现有位置编码方法在适应性、动态性和计算效率上存在不足，无法充分捕捉复杂属性图和动态拓扑信息。

Method: 提出L-STEP，一种可学习的时空位置编码方法，通过MLPs实现高效表达，并验证其理论复杂度和实际性能。

Result: 在13个经典数据集和TGB基准测试中，L-STEP在时空链接预测任务中表现优于现有方法。

Conclusion: L-STEP是一种高效且可扩展的时空位置编码方法，适用于动态图数据。

Abstract: Accurate predictions rely on the expressiveness power of graph deep learning
frameworks like graph neural networks and graph transformers, where a
positional encoding mechanism has become much more indispensable in recent
state-of-the-art works to record the canonical position information. However,
the current positional encoding is limited in three aspects: (1) most
positional encoding methods use pre-defined, and fixed functions, which are
inadequate to adapt to the complex attributed graphs; (2) a few pioneering
works proposed the learnable positional encoding but are still limited to the
structural information, not considering the real-world time-evolving
topological and feature information; (3) most positional encoding methods are
equipped with transformers' attention mechanism to fully leverage their
capabilities, where the dense or relational attention is often unaffordable on
large-scale structured data. Hence, we aim to develop Learnable
Spatial-Temporal Positional Encoding in an effective and efficient manner and
propose a simple temporal link prediction model named L-STEP. Briefly, for
L-STEP, we (1) prove the proposed positional learning scheme can preserve the
graph property from the spatial-temporal spectral viewpoint, (2) verify that
MLPs can fully exploit the expressiveness and reach transformers' performance
on that encoding, (3) change different initial positional encoding inputs to
show robustness, (4) analyze the theoretical complexity and obtain less
empirical running time than SOTA, and (5) demonstrate its temporal link
prediction out-performance on 13 classic datasets and with 10 algorithms in
both transductive and inductive settings using 3 different sampling strategies.
Also, \name\ obtains the leading performance in the newest large-scale TGB
benchmark. Our code is available at https://github.com/kthrn22/L-STEP.

</details>


### [271] [Private Evolution Converges](https://arxiv.org/abs/2506.08312)
*Tomás González,Giulia Fanti,Aaditya Ramdas*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Private Evolution (PE) is a promising training-free method for differentially
private (DP) synthetic data generation. While it achieves strong performance in
some domains (e.g., images and text), its behavior in others (e.g., tabular
data) is less consistent. To date, the only theoretical analysis of the
convergence of PE depends on unrealistic assumptions about both the algorithm's
behavior and the structure of the sensitive dataset. In this work, we develop a
new theoretical framework to explain PE's practical behavior and identify
sufficient conditions for its convergence. For $d$-dimensional sensitive
datasets with $n$ data points from a bounded domain, we prove that PE produces
an $(\epsilon, \delta)$-DP synthetic dataset with expected 1-Wasserstein
distance of order $\tilde{O}(d(n\epsilon)^{-1/d})$ from the original,
establishing worst-case convergence of the algorithm as $n \to \infty$. Our
analysis extends to general Banach spaces as well. We also connect PE to the
Private Signed Measure Mechanism, a method for DP synthetic data generation
that has thus far not seen much practical adoption. We demonstrate the
practical relevance of our theoretical findings in simulations.

</details>


### [272] [Why Masking Diffusion Works: Condition on the Jump Schedule for Improved Discrete Diffusion](https://arxiv.org/abs/2506.08316)
*Alan N. Amin,Nate Gruver,Andrew Gordon Wilson*

Main category: cs.LG

TL;DR: 论文解释了离散扩散模型中掩码扩散表现优异的原因，并提出了一种新方法SCUD，通过利用离散马尔可夫过程的跳跃时间分布，改进了传统离散扩散模型。


<details>
  <summary>Details</summary>
Motivation: 研究离散扩散模型中掩码扩散表现优于其他方法的原因，并提出改进方案。

Method: 提出SCUD方法，将已知的跳跃时间分布融入离散扩散模型，应用于图像、文本和蛋白质数据。

Result: SCUD方法在多种数据上表现优于掩码扩散。

Conclusion: SCUD方法通过利用离散马尔可夫过程的特性，改进了离散扩散模型的性能。

Abstract: Discrete diffusion models, like continuous diffusion models, generate
high-quality samples by gradually undoing noise applied to datapoints with a
Markov process. Gradual generation in theory comes with many conceptual
benefits; for example, inductive biases can be incorporated into the noising
Markov process, and access to improved sampling algorithms. In practice,
however, the consistently best performing discrete diffusion model is,
surprisingly, masking diffusion, which does not denoise gradually. Here we
explain the superior performance of masking diffusion by noting that it makes
use of a fundamental difference between continuous and discrete Markov
processes: discrete Markov processes evolve by discontinuous jumps at a fixed
rate and, unlike other discrete diffusion models, masking diffusion builds in
the known distribution of jump times and only learns where to jump to. We show
that we can similarly bake in the known distribution of jump times into any
discrete diffusion model. The resulting models - schedule-conditioned discrete
diffusion (SCUD) - generalize classical discrete diffusion and masking
diffusion. By applying SCUD to models with noising processes that incorporate
inductive biases on images, text, and protein data, we build models that
outperform masking.

</details>


### [273] [Graph Prompting for Graph Learning Models: Recent Advances and Future Directions](https://arxiv.org/abs/2506.08326)
*Xingbo Fu,Zehong Wang,Zihan Chen,Jiazheng Li,Yaochen Zhu,Zhenyu Lei,Cong Shen,Yanfang Ye,Chuxu Zhang,Jundong Li*

Main category: cs.LG

TL;DR: 本文系统综述了图提示（graph prompting）的最新进展，包括图预训练方法、主流图提示技术及其应用，并讨论了未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 图学习模型在大规模图数据中表现优异，而图提示作为一种新兴方法，能够在不改变预训练模型的情况下适应下游任务，具有重要研究价值。

Method: 通过自监督方式预训练图学习模型，随后设计可学习的提示（prompts）以适应具体任务。

Result: 总结了图提示的主流技术和实际应用，展示了其在不同领域的潜力。

Conclusion: 图提示是一个有前景的研究方向，但仍存在一些挑战，未来需要进一步探索。

Abstract: Graph learning models have demonstrated great prowess in learning expressive
representations from large-scale graph data in a wide variety of real-world
scenarios. As a prevalent strategy for training powerful graph learning models,
the "pre-training, adaptation" scheme first pre-trains graph learning models on
unlabeled graph data in a self-supervised manner and then adapts them to
specific downstream tasks. During the adaptation phase, graph prompting emerges
as a promising approach that learns trainable prompts while keeping the
pre-trained graph learning models unchanged. In this paper, we present a
systematic review of recent advancements in graph prompting. First, we
introduce representative graph pre-training methods that serve as the
foundation step of graph prompting. Next, we review mainstream techniques in
graph prompting and elaborate on how they design learnable prompts for graph
prompting. Furthermore, we summarize the real-world applications of graph
prompting from different domains. Finally, we discuss several open challenges
in existing studies with promising future directions in this field.

</details>


### [274] [A Simple Analysis of Discretization Error in Diffusion Models](https://arxiv.org/abs/2506.08337)
*Juhyeok Choi,Chenglin Fan*

Main category: cs.LG

TL;DR: 本文提出了一种简化的理论框架，用于分析DDPM中VP-SDE的Euler--Maruyama离散化误差，并证明了离散随机变量可替代高斯噪声而不影响收敛性。


<details>
  <summary>Details</summary>
Motivation: 现有扩散模型离散化误差分析依赖复杂概率工具，本文旨在简化分析并探索高效采样方法。

Method: 利用Grönwall不等式推导收敛速率，并实验验证离散噪声替代高斯噪声的可行性。

Result: 理论预测误差尺度正确，离散噪声采样质量与高斯噪声相当，噪声缩放不当会降低性能。

Conclusion: 本文简化了理论分析并验证了离散噪声的实用性，为扩散生成模型提供了理论与实践的桥梁。

Abstract: Diffusion models, formulated as discretizations of stochastic differential
equations (SDEs), achieve state-of-the-art generative performance. However,
existing analyses of their discretization error often rely on complex
probabilistic tools. In this work, we present a simplified theoretical
framework for analyzing the Euler--Maruyama discretization of
variance-preserving SDEs (VP-SDEs) in Denoising Diffusion Probabilistic Models
(DDPMs), where $ T $ denotes the number of denoising steps in the diffusion
process. Our approach leverages Gr\"onwall's inequality to derive a convergence
rate of $ \mathcal{O}(1/T^{1/2}) $ under Lipschitz assumptions, significantly
streamlining prior proofs. Furthermore, we demonstrate that the Gaussian noise
in the discretization can be replaced by a discrete random variable (e.g.,
Rademacher or uniform noise) without sacrificing convergence guarantees-an
insight with practical implications for efficient sampling. Experiments
validate our theory, showing that (1) the error scales as predicted, (2)
discrete noise achieves comparable sample quality to Gaussian noise, and (3)
incorrect noise scaling degrades performance. By unifying simplified analysis
and discrete noise substitution, our work bridges theoretical rigor with
practical efficiency in diffusion-based generative modeling.

</details>


### [275] [Dynamical System Optimization](https://arxiv.org/abs/2506.08340)
*Emo Todorov*

Main category: cs.LG

TL;DR: 该论文提出了一种优化框架，通过将控制权转移给策略，从而在自治动态系统层面简化算法，避免直接使用动态规划和强化学习的复杂方法。


<details>
  <summary>Details</summary>
Motivation: 传统动态规划和强化学习方法复杂且计算量大，作者希望通过将控制权转移给策略，简化优化过程。

Method: 在自治系统层面设计算法，计算策略梯度、Hessian矩阵等，并统一处理策略参数和其他系统参数。

Result: 提出的算法与策略梯度、自然梯度等方法计算结果一致，适用于行为克隆、机制设计等多种任务。

Conclusion: 该框架不仅简化了优化过程，还扩展了应用范围，尤其在生成式AI模型的调优上更具优势。

Abstract: We develop an optimization framework centered around a core idea: once a
(parametric) policy is specified, control authority is transferred to the
policy, resulting in an autonomous dynamical system. Thus we should be able to
optimize policy parameters without further reference to controls or actions,
and without directly using the machinery of approximate Dynamic Programming and
Reinforcement Learning. Here we derive simpler algorithms at the autonomous
system level, and show that they compute the same quantities as policy
gradients and Hessians, natural gradients, proximal methods. Analogs to
approximate policy iteration and off-policy learning are also available. Since
policy parameters and other system parameters are treated uniformly, the same
algorithms apply to behavioral cloning, mechanism design, system
identification, learning of state estimators. Tuning of generative AI models is
not only possible, but is conceptually closer to the present framework than to
Reinforcement Learning.

</details>


### [276] [Differentially Private Relational Learning with Entity-level Privacy Guarantees](https://arxiv.org/abs/2506.08347)
*Yinan Huang,Haoteng Ying,Eli Chien,Rongzhe Wei,Pan Li*

Main category: cs.LG

TL;DR: 论文提出了一种针对关系数据的差分隐私学习框架，解决了传统DP-SGD在关系学习中因实体多参与和耦合采样导致的隐私保护难题。


<details>
  <summary>Details</summary>
Motivation: 在敏感领域的关系和网络结构数据学习中，保护个体隐私至关重要，但传统DP-SGD难以直接应用。

Method: 通过敏感性分析和自适应梯度裁剪方案，提出了一种专为关系数据设计的DP-SGD变体，并扩展了隐私放大分析。

Result: 实验表明，该方法在文本属性网络数据上实现了高效的隐私-效用权衡。

Conclusion: 该框架为关系学习提供了严格的实体级差分隐私保证，具有实际应用价值。

Abstract: Learning with relational and network-structured data is increasingly vital in
sensitive domains where protecting the privacy of individual entities is
paramount. Differential Privacy (DP) offers a principled approach for
quantifying privacy risks, with DP-SGD emerging as a standard mechanism for
private model training. However, directly applying DP-SGD to relational
learning is challenging due to two key factors: (i) entities often participate
in multiple relations, resulting in high and difficult-to-control sensitivity;
and (ii) relational learning typically involves multi-stage, potentially
coupled (interdependent) sampling procedures that make standard privacy
amplification analyses inapplicable. This work presents a principled framework
for relational learning with formal entity-level DP guarantees. We provide a
rigorous sensitivity analysis and introduce an adaptive gradient clipping
scheme that modulates clipping thresholds based on entity occurrence frequency.
We also extend the privacy amplification results to a tractable subclass of
coupled sampling, where the dependence arises only through sample sizes. These
contributions lead to a tailored DP-SGD variant for relational data with
provable privacy guarantees. Experiments on fine-tuning text encoders over
text-attributed network-structured relational data demonstrate the strong
utility-privacy trade-offs of our approach. Our code is available at
https://github.com/Graph-COM/Node_DP.

</details>


### [277] [An Adaptive Method Stabilizing Activations for Enhanced Generalization](https://arxiv.org/abs/2506.08353)
*Hyunseok Seung,Jaewoo Lee,Hyunsuk Ko*

Main category: cs.LG

TL;DR: AdaAct是一种新颖的优化算法，通过根据激活方差调整学习率来增强神经元输出的稳定性，从而提升泛化能力。


<details>
  <summary>Details</summary>
Motivation: 传统激活正则化方法的局限性促使开发一种能够动态调整学习率的算法，以提升训练稳定性和泛化能力。

Method: AdaAct通过神经元级别的自适应调整学习率，结合激活方差信息，优化训练过程。

Result: 在CIFAR和ImageNet等标准图像分类基准测试中，AdaAct表现优异，兼具Adam的快速收敛和SGD的强泛化能力。

Conclusion: AdaAct是一种有效的优化算法，能够平衡收敛速度和泛化能力，同时保持高效执行时间。

Abstract: We introduce AdaAct, a novel optimization algorithm that adjusts learning
rates according to activation variance. Our method enhances the stability of
neuron outputs by incorporating neuron-wise adaptivity during the training
process, which subsequently leads to better generalization -- a complementary
approach to conventional activation regularization methods. Experimental
results demonstrate AdaAct's competitive performance across standard image
classification benchmarks. We evaluate AdaAct on CIFAR and ImageNet, comparing
it with other state-of-the-art methods. Importantly, AdaAct effectively bridges
the gap between the convergence speed of Adam and the strong generalization
capabilities of SGD, all while maintaining competitive execution times. Code is
available at https://github.com/hseung88/adaact.

</details>


### [278] [NysAct: A Scalable Preconditioned Gradient Descent using Nystrom Approximation](https://arxiv.org/abs/2506.08360)
*Hyunseok Seung,Jaewoo Lee,Hyunsuk Ko*

Main category: cs.LG

TL;DR: NysAct是一种可扩展的一阶梯度预处理方法，平衡了一阶和二阶优化方法的优势，通过近似激活协方差矩阵显著降低了计算和内存成本。


<details>
  <summary>Details</summary>
Motivation: 自适应梯度方法计算高效但泛化能力差，二阶方法泛化能力强但计算成本高，NysAct旨在平衡两者。

Method: 利用特征值位移的Nystrom方法近似激活协方差矩阵作为预处理矩阵。

Result: NysAct在测试精度上优于一阶和二阶方法，同时计算资源需求显著低于二阶方法。

Conclusion: NysAct在计算效率和泛化能力之间取得了良好平衡，是一种高效的优化方法。

Abstract: Adaptive gradient methods are computationally efficient and converge quickly,
but they often suffer from poor generalization. In contrast, second-order
methods enhance convergence and generalization but typically incur high
computational and memory costs. In this work, we introduce NysAct, a scalable
first-order gradient preconditioning method that strikes a balance between
state-of-the-art first-order and second-order optimization methods. NysAct
leverages an eigenvalue-shifted Nystrom method to approximate the activation
covariance matrix, which is used as a preconditioning matrix, significantly
reducing time and memory complexities with minimal impact on test accuracy. Our
experiments show that NysAct not only achieves improved test accuracy compared
to both first-order and second-order methods but also demands considerably less
computational resources than existing second-order methods. Code is available
at https://github.com/hseung88/nysact.

</details>


### [279] [AlphaFold Database Debiasing for Robust Inverse Folding](https://arxiv.org/abs/2506.08365)
*Cheng Tan,Zhenxiao Cao,Zhangyang Gao,Siyuan Li,Yufei Huang,Stan Z. Li*

Main category: cs.LG

TL;DR: AlphaFold数据库（AFDB）的结构数据存在系统性几何偏差，影响逆折叠等任务的性能。作者提出DeSAE模型，通过去偏处理提升结构数据的泛化能力。


<details>
  <summary>Details</summary>
Motivation: AFDB的结构数据虽然覆盖广且精度高，但其几何偏差限制了在敏感任务（如逆折叠）中的应用。实验结构（PDB）更具多样性和物理真实性，因此需要去偏方法。

Method: 提出Debiasing Structure AutoEncoder（DeSAE），通过从故意损坏的主链几何中重建自然构象，学习更稳健的结构流形。

Result: DeSAE处理后的AFDB结构显著提升了逆折叠任务在多个基准测试中的性能。

Conclusion: 研究揭示了预测结构中的系统性偏差对任务性能的关键影响，并提供了去偏框架，显著提升了结构学习任务的性能。

Abstract: The AlphaFold Protein Structure Database (AFDB) offers unparalleled
structural coverage at near-experimental accuracy, positioning it as a valuable
resource for data-driven protein design. However, its direct use in training
deep models that are sensitive to fine-grained atomic geometry, such as inverse
folding, exposes a critical limitation. Comparative analysis of structural
feature distributions reveals that AFDB structures exhibit distinct statistical
regularities, reflecting a systematic geometric bias that deviates from the
conformational diversity found in experimentally determined structures from the
Protein Data Bank (PDB). While AFDB structures are cleaner and more idealized,
PDB structures capture the intrinsic variability and physical realism essential
for generalization in downstream tasks. To address this discrepancy, we
introduce a Debiasing Structure AutoEncoder (DeSAE) that learns to reconstruct
native-like conformations from intentionally corrupted backbone geometries. By
training the model to recover plausible structural states, DeSAE implicitly
captures a more robust and natural structural manifold. At inference, applying
DeSAE to AFDB structures produces debiased structures that significantly
improve inverse folding performance across multiple benchmarks. This work
highlights the critical impact of subtle systematic biases in predicted
structures and presents a principled framework for debiasing, significantly
boosting the performance of structure-based learning tasks like inverse
folding.

</details>


### [280] [Reinforce LLM Reasoning through Multi-Agent Reflection](https://arxiv.org/abs/2506.08379)
*Yurun Yuan,Tengyang Xie*

Main category: cs.LG

TL;DR: 论文提出DPSDP算法，通过强化学习训练LLM系统迭代优化答案，提升推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有方法反馈空间受限且缺乏协调训练，导致性能不佳。

Method: 将多轮优化建模为马尔可夫决策过程，提出DPSDP算法，通过直接偏好学习训练actor-critic LLM系统。

Result: 在MATH 500基准上，五步优化将准确率从58.2%提升至63.2%。

Conclusion: DPSDP能有效提升LLM推理能力，多智能体协作和泛化能力得到验证。

Abstract: Leveraging more test-time computation has proven to be an effective way to
boost the reasoning capabilities of large language models (LLMs). Among various
methods, the verify-and-improve paradigm stands out for enabling dynamic
solution exploration and feedback incorporation. However, existing approaches
often suffer from restricted feedback spaces and lack of coordinated training
of different parties, leading to suboptimal performance. To address this, we
model this multi-turn refinement process as a Markov Decision Process and
introduce DPSDP (Direct Policy Search by Dynamic Programming), a reinforcement
learning algorithm that trains an actor-critic LLM system to iteratively refine
answers via direct preference learning on self-generated data. Theoretically,
DPSDP can match the performance of any policy within the training distribution.
Empirically, we instantiate DPSDP with various base models and show
improvements on both in- and out-of-distribution benchmarks. For example, on
benchmark MATH 500, majority voting over five refinement steps increases
first-turn accuracy from 58.2% to 63.2% with Ministral-based models. An
ablation study further confirms the benefits of multi-agent collaboration and
out-of-distribution generalization.

</details>


### [281] [Network Threat Detection: Addressing Class Imbalanced Data with Deep Forest](https://arxiv.org/abs/2506.08383)
*Jiaqi Chen,Rongbin Ye*

Main category: cs.LG

TL;DR: 论文提出了一种基于机器学习的物联网恶意流量检测方法，通过处理数据不平衡问题，结合集成学习方法（如gcForest）提升了检测性能。


<details>
  <summary>Details</summary>
Motivation: 随着物联网网络的快速扩展，实时检测恶意流量成为关键网络安全挑战。

Method: 使用IoT-23数据集，采用三种重采样策略处理类别不平衡，并比较多种机器学习技术。

Result: 结合不平衡处理技术和集成方法（如gcForest）的检测性能优于传统方法。

Conclusion: 该研究为物联网环境开发更智能、高效的自动化威胁检测系统提供了重要贡献。

Abstract: With the rapid expansion of Internet of Things (IoT) networks, detecting
malicious traffic in real-time has become a critical cybersecurity challenge.
This research addresses the detection challenges by presenting a comprehensive
empirical analysis of machine learning techniques for malware detection using
the IoT-23 dataset provided by the Stratosphere Laboratory. We address the
significant class imbalance within the dataset through three resampling
strategies. We implement and compare a few machine learning techniques. Our
findings demonstrate that the combination of appropriate imbalance treatment
techniques with ensemble methods, particularly gcForest, achieves better
detection performance compared to traditional approaches. This work contributes
significantly to the development of more intelligent and efficient automated
threat detection systems for IoT environments, helping to secure critical
infrastructure against sophisticated cyber attacks while optimizing
computational resource usage.

</details>


### [282] [Reinforcement Learning Teachers of Test Time Scaling](https://arxiv.org/abs/2506.08388)
*Edoardo Cetin,Tianyu Zhao,Yujin Tang*

Main category: cs.LG

TL;DR: 提出了一种新的强化学习教师（RLT）框架，通过密集奖励训练教师模型，专注于生成适合学生模型的知识蒸馏内容，避免了传统强化学习的探索问题。


<details>
  <summary>Details</summary>
Motivation: 传统强化学习在训练推理语言模型时依赖于初始探索能力，且模型主要用于知识蒸馏而非直接部署。RLT框架旨在解决这些问题，提高效率和可重用性。

Method: RLT模型被设计为根据问题和答案生成详细解释，并通过密集奖励（学生模型对解释的理解）进行训练。

Result: 7B规模的RLT模型在竞赛和研究生级任务上表现优于现有蒸馏和冷启动方法，且适用于更大规模学生模型和零样本任务。

Conclusion: RLT框架显著提升了强化学习推理的效率和多任务适应性，为知识蒸馏和模型冷启动提供了新思路。

Abstract: Training reasoning language models (LMs) with reinforcement learning (RL) for
one-hot correctness inherently relies on the LM being able to explore and solve
its task with some chance at initialization. Furthermore, a key use case of
reasoning LMs is to act as teachers for distilling new students and
cold-starting future RL iterations rather than being deployed themselves. From
these considerations, we introduce a new framework that avoids RL's exploration
challenge by training a new class of Reinforcement-Learned Teachers (RLTs)
focused on yielding the most effective downstream distillation. RLTs are
prompted with both the question and solution to each problem, and tasked to
simply "connect-the-dots" with detailed explanations tailored for their
students. We train RLTs with dense rewards obtained by feeding each explanation
to the student and testing its understanding of the problem's solution. In
practice, the raw outputs of a 7B RLT provide higher final performance on
competition and graduate-level tasks than existing distillation and
cold-starting pipelines that collect and postprocess the reasoning traces of
orders of magnitude larger LMs. Furthermore, RLTs maintain their effectiveness
when training larger students and when applied zero-shot to out-of-distribution
tasks, unlocking new levels of efficiency and re-usability for the RL reasoning
framework.

</details>


### [283] [Spatiotemporal deep learning models for detection of rapid intensification in cyclones](https://arxiv.org/abs/2506.08397)
*Vamshika Sutar,Amandeep Singh,Rohitash Chandra*

Main category: cs.LG

TL;DR: 论文提出了一种结合深度学习和数据增强的框架，用于检测气旋快速增强事件，解决了数据集类别不平衡问题。


<details>
  <summary>Details</summary>
Motivation: 气旋快速增强是一种极端事件，数据集类别不平衡且影响因素复杂，传统机器学习模型难以处理。

Method: 采用深度学习和数据增强框架，生成模拟气旋的空间坐标和风强度数据，并使用深度学习分类器区分快速和非快速增强事件。

Result: 数据增强显著提升了快速增强事件的检测效果，空间坐标作为输入特征至关重要。

Conclusion: 该研究为极端事件的时空数据合成生成提供了新思路。

Abstract: Cyclone rapid intensification is the rapid increase in cyclone wind
intensity, exceeding a threshold of 30 knots, within 24 hours. Rapid
intensification is considered an extreme event during a cyclone, and its
occurrence is relatively rare, contributing to a class imbalance in the
dataset. A diverse array of factors influences the likelihood of a cyclone
undergoing rapid intensification, further complicating the task for
conventional machine learning models. In this paper, we evaluate deep learning,
ensemble learning and data augmentation frameworks to detect cyclone rapid
intensification based on wind intensity and spatial coordinates. We note that
conventional data augmentation methods cannot be utilised for generating
spatiotemporal patterns replicating cyclones that undergo rapid
intensification. Therefore, our framework employs deep learning models to
generate spatial coordinates and wind intensity that replicate cyclones to
address the class imbalance problem of rapid intensification. We also use a
deep learning model for the classification module within the data augmentation
framework to differentiate between rapid and non-rapid intensification events
during a cyclone. Our results show that data augmentation improves the results
for rapid intensification detection in cyclones, and spatial coordinates play a
critical role as input features to the given models. This paves the way for
research in synthetic data generation for spatiotemporal data with extreme
events.

</details>


### [284] [FUSE: Measure-Theoretic Compact Fuzzy Set Representation for Taxonomy Expansion](https://arxiv.org/abs/2506.08409)
*Fred Xu,Song Jiang,Zijie Huang,Xiao Luo,Shichang Zhang,Adrian Chen,Yizhou Sun*

Main category: cs.LG

TL;DR: 论文提出了一种基于模糊集表示学习的FUSE框架，用于分类扩展任务，显著优于现有基线。


<details>
  <summary>Details</summary>
Motivation: 现有方法通常将集合建模为向量或几何对象，但这些表示不支持集合操作。模糊集能更好地建模语义概念的不确定性和信息量。

Method: 提出FUSE框架，通过体积近似模糊集来表示集合，满足所有集合操作，且学习效率高。

Result: 在分类扩展任务中，FUSE比现有基线提升了高达23%。

Conclusion: FUSE是首个高效计算模糊集嵌入的方法，为集合表示学习提供了新思路。

Abstract: Taxonomy Expansion, which models complex concepts and their relations, can be
formulated as a set representation learning task. The generalization of set,
fuzzy set, incorporates uncertainty and measures the information within a
semantic concept, making it suitable for concept modeling. Existing works
usually model sets as vectors or geometric objects such as boxes, which are not
closed under set operations. In this work, we propose a sound and efficient
formulation of set representation learning based on its volume approximation as
a fuzzy set. The resulting embedding framework, Fuzzy Set Embedding (FUSE),
satisfies all set operations and compactly approximates the underlying fuzzy
set, hence preserving information while being efficient to learn, relying on
minimum neural architecture. We empirically demonstrate the power of FUSE on
the task of taxonomy expansion, where FUSE achieves remarkable improvements up
to 23% compared with existing baselines. Our work marks the first attempt to
understand and efficiently compute the embeddings of fuzzy sets.

</details>


### [285] [Learning to Hear Broken Motors: Signature-Guided Data Augmentation for Induction-Motor Diagnostics](https://arxiv.org/abs/2506.08412)
*Saraa Ali,Aleksandr Khizhik,Stepan Svirin,Artem Ryzhikov,Denis Derkach*

Main category: cs.LG

TL;DR: 提出了一种结合机器学习与无监督异常生成方法的智能诊断框架SGDA，用于三相电机故障诊断，显著提升了准确性和可靠性。


<details>
  <summary>Details</summary>
Motivation: 传统签名分析方法虽为标准实践，但结合机器学习可进一步提升诊断性能。

Method: 提出SGDA框架，结合无监督异常生成与电机物理模型，直接在健康电流信号的频域合成物理合理的故障。

Result: 该方法实现了更高的诊断准确性和可靠性，具有广泛的工业应用潜力。

Conclusion: SGDA为电机诊断领域提供了高效、鲁棒的解决方案，具有重要的实际应用价值。

Abstract: The application of machine learning (ML) algorithms in the intelligent
diagnosis of three-phase engines has the potential to significantly enhance
diagnostic performance and accuracy. Traditional methods largely rely on
signature analysis, which, despite being a standard practice, can benefit from
the integration of advanced ML techniques. In our study, we innovate by
combining ML algorithms with a novel unsupervised anomaly generation
methodology that takes into account the engine physics model. We propose
Signature-Guided Data Augmentation (SGDA), an unsupervised framework that
synthesizes physically plausible faults directly in the frequency domain of
healthy current signals. Guided by Motor Current Signature Analysis, SGDA
creates diverse and realistic anomalies without resorting to computationally
intensive simulations. This hybrid approach leverages the strengths of both
supervised ML and unsupervised signature analysis, achieving superior
diagnostic accuracy and reliability along with wide industrial application. The
findings highlight the potential of our approach to contribute significantly to
the field of engine diagnostics, offering a robust and efficient solution for
real-world applications.

</details>


### [286] [Improved Scaling Laws in Linear Regression via Data Reuse](https://arxiv.org/abs/2506.08415)
*Licong Lin,Jingfeng Wu,Peter L. Bartlett*

Main category: cs.LG

TL;DR: 论文探讨了数据重用如何在线性回归中改进现有的缩放定律，通过多轮随机梯度下降（multi-pass SGD）实现更优的测试误差。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于探索在数据有限的情况下，如何通过数据重用优化模型的缩放定律，避免因数据不足导致的性能下降。

Method: 方法包括在多轮SGD训练中重用数据，并假设数据协方差和真实参数具有特定的幂律谱分布。

Result: 结果表明，多轮SGD的测试误差优于单轮SGD，验证了数据重用的有效性。

Conclusion: 结论指出，在数据受限的情况下，数据重用可以显著改进模型的缩放性能。

Abstract: Neural scaling laws suggest that the test error of large language models
trained online decreases polynomially as the model size and data size increase.
However, such scaling can be unsustainable when running out of new data. In
this work, we show that data reuse can improve existing scaling laws in linear
regression. Specifically, we derive sharp test error bounds on $M$-dimensional
linear models trained by multi-pass stochastic gradient descent (multi-pass
SGD) on $N$ data with sketched features. Assuming that the data covariance has
a power-law spectrum of degree $a$, and that the true parameter follows a prior
with an aligned power-law spectrum of degree $b-a$ (with $a > b > 1$), we show
that multi-pass SGD achieves a test error of $\Theta(M^{1-b} + L^{(1-b)/a})$,
where $L \lesssim N^{a/b}$ is the number of iterations. In the same setting,
one-pass SGD only attains a test error of $\Theta(M^{1-b} + N^{(1-b)/a})$ (see
e.g., Lin et al., 2024). This suggests an improved scaling law via data reuse
(i.e., choosing $L>N$) in data-constrained regimes. Numerical simulations are
also provided to verify our theoretical findings.

</details>


### [287] [Offline RL with Smooth OOD Generalization in Convex Hull and its Neighborhood](https://arxiv.org/abs/2506.08417)
*Qingmao Yao,Zhichao Lei,Tianyuan Chen,Ziyue Yuan,Xuefan Chen,Jianxiang Liu,Faguo Wu,Xiao Zhang*

Main category: cs.LG

TL;DR: 论文提出一种新方法SQOG，通过平滑贝尔曼算子（SBO）在凸包及其邻域（CHN）内提升Q函数泛化能力，解决离线强化学习中Q值高估问题。


<details>
  <summary>Details</summary>
Motivation: 离线强化学习中，分布偏移导致Q值对OOD动作的高估，现有方法因过度约束限制了Q函数泛化。

Method: 提出SBO算子，通过平滑OOD区域的Q值与邻近样本Q值，提升泛化能力；理论证明其在CHN内近似真实Q值。

Result: SQOG算法在D4RL基准测试中表现优于现有方法，计算效率更高。

Conclusion: SQOG通过SBO有效缓解过度约束问题，实现了更准确的Q值估计。

Abstract: Offline Reinforcement Learning (RL) struggles with distributional shifts,
leading to the $Q$-value overestimation for out-of-distribution (OOD) actions.
Existing methods address this issue by imposing constraints; however, they
often become overly conservative when evaluating OOD regions, which constrains
the $Q$-function generalization. This over-constraint issue results in poor
$Q$-value estimation and hinders policy improvement. In this paper, we
introduce a novel approach to achieve better $Q$-value estimation by enhancing
$Q$-function generalization in OOD regions within Convex Hull and its
Neighborhood (CHN). Under the safety generalization guarantees of the CHN, we
propose the Smooth Bellman Operator (SBO), which updates OOD $Q$-values by
smoothing them with neighboring in-sample $Q$-values. We theoretically show
that SBO approximates true $Q$-values for both in-sample and OOD actions within
the CHN. Our practical algorithm, Smooth Q-function OOD Generalization (SQOG),
empirically alleviates the over-constraint issue, achieving near-accurate
$Q$-value estimation. On the D4RL benchmarks, SQOG outperforms existing
state-of-the-art methods in both performance and computational efficiency.

</details>


### [288] [Online Learning-guided Learning Rate Adaptation via Gradient Alignment](https://arxiv.org/abs/2506.08419)
*Ruichen Jiang,Ali Kavis,Aryan Mokhtari*

Main category: cs.LG

TL;DR: GALA框架通过动态调整学习率，基于梯度对齐和局部曲率估计，无需繁琐的超参数调优，提升优化器性能。


<details>
  <summary>Details</summary>
Motivation: 大规模深度学习模型的优化器性能高度依赖学习率的精细调优，传统方法需大量网格搜索，耗时且低效。

Method: 提出GALA框架，通过跟踪连续梯度的对齐性和局部曲率估计，动态调整学习率，并将其建模为一维在线学习问题。

Result: GALA与SGD或Adam结合时，在多种初始学习率下表现稳健，无需调优即可取得竞争性性能。

Conclusion: GALA为学习率自适应提供了一种高效、灵活的方法，显著减少了超参数调优的需求。

Abstract: The performance of an optimizer on large-scale deep learning models depends
critically on fine-tuning the learning rate, often requiring an extensive grid
search over base learning rates, schedules, and other hyperparameters. In this
paper, we propose a principled framework called GALA (Gradient Alignment-based
Learning rate Adaptation), which dynamically adjusts the learning rate by
tracking the alignment between consecutive gradients and using a local
curvature estimate. Guided by the convergence analysis, we formulate the
problem of selecting the learning rate as a one-dimensional online learning
problem. When paired with an online learning algorithm such as
Follow-the-Regularized-Leader, our method produces a flexible, adaptive
learning rate schedule that tends to increase when consecutive gradients are
aligned and decrease otherwise. We establish a data-adaptive convergence rate
for normalized SGD equipped with GALA in the smooth, nonconvex setting.
Empirically, common optimizers such as SGD and Adam, when augmented with GALA,
demonstrate robust performance across a wide range of initial learning rates
and perform competitively without the need for tuning.

</details>


### [289] [HASFL: Heterogeneity-aware Split Federated Learning over Edge Computing Systems](https://arxiv.org/abs/2506.08426)
*Zheng Lin,Zhe Chen,Xianhao Chen,Wei Ni,Yue Gao*

Main category: cs.LG

TL;DR: HASFL框架通过自适应控制批处理大小和模型分割，解决了分裂联邦学习中的设备异构性问题，显著提升了学习性能和效率。


<details>
  <summary>Details</summary>
Motivation: 现有分裂联邦学习方法因边缘设备能力异构性导致严重的拖尾效应，亟需解决方案。

Method: 提出HASFL框架，通过理论推导收敛边界，自适应调整批处理大小和模型分割，平衡通信计算延迟与训练收敛。

Result: 实验验证HASFL在多种数据集上优于现有基准方法。

Conclusion: HASFL有效解决了设备异构性问题，提升了分裂联邦学习的性能和效率。

Abstract: Split federated learning (SFL) has emerged as a promising paradigm to
democratize machine learning (ML) on edge devices by enabling layer-wise model
partitioning. However, existing SFL approaches suffer significantly from the
straggler effect due to the heterogeneous capabilities of edge devices. To
address the fundamental challenge, we propose adaptively controlling batch
sizes (BSs) and model splitting (MS) for edge devices to overcome resource
heterogeneity. We first derive a tight convergence bound of SFL that quantifies
the impact of varied BSs and MS on learning performance. Based on the
convergence bound, we propose HASFL, a heterogeneity-aware SFL framework
capable of adaptively controlling BS and MS to balance communication-computing
latency and training convergence in heterogeneous edge networks. Extensive
experiments with various datasets validate the effectiveness of HASFL and
demonstrate its superiority over state-of-the-art benchmarks.

</details>


### [290] [Boosting Gradient Leakage Attacks: Data Reconstruction in Realistic FL Settings](https://arxiv.org/abs/2506.08435)
*Mingyuan Fan,Fuyi Wang,Cen Chen,Jianying Zhou*

Main category: cs.LG

TL;DR: 本文通过实证研究证明，即使在现实的联邦学习环境中，客户端数据仍可被有效重构，提出了FedLeak方法以解决梯度匹配问题，并揭示了联邦学习系统的重大漏洞。


<details>
  <summary>Details</summary>
Motivation: 联邦学习（FL）因其隐私保护能力备受关注，但梯度泄漏攻击（GLAs）的争议性引发了对其实际隐私风险的讨论。本文旨在填补这一研究空白，证明即使在现实环境中，数据重构仍可能发生。

Method: 本文提出FedLeak方法，包含两种新技术：部分梯度匹配和梯度正则化，并通过基于文献和行业实践的评估协议验证其性能。

Result: 在现实FL环境中，FedLeak仍能实现高保真数据重构，揭示了FL系统的重大漏洞。

Conclusion: 研究强调了FL系统的隐私风险，并呼吁开发更有效的防御方法。

Abstract: Federated learning (FL) enables collaborative model training among multiple
clients without the need to expose raw data. Its ability to safeguard privacy,
at the heart of FL, has recently been a hot-button debate topic. To elaborate,
several studies have introduced a type of attacks known as gradient leakage
attacks (GLAs), which exploit the gradients shared during training to
reconstruct clients' raw data. On the flip side, some literature, however,
contends no substantial privacy risk in practical FL environments due to the
effectiveness of such GLAs being limited to overly relaxed conditions, such as
small batch sizes and knowledge of clients' data distributions.
  This paper bridges this critical gap by empirically demonstrating that
clients' data can still be effectively reconstructed, even within realistic FL
environments. Upon revisiting GLAs, we recognize that their performance
failures stem from their inability to handle the gradient matching problem. To
alleviate the performance bottlenecks identified above, we develop FedLeak,
which introduces two novel techniques, partial gradient matching and gradient
regularization. Moreover, to evaluate the performance of FedLeak in real-world
FL environments, we formulate a practical evaluation protocol grounded in a
thorough review of extensive FL literature and industry practices. Under this
protocol, FedLeak can still achieve high-fidelity data reconstruction, thereby
underscoring the significant vulnerability in FL systems and the urgent need
for more effective defense methods.

</details>


### [291] [Learning to Lead: Incentivizing Strategic Agents in the Dark](https://arxiv.org/abs/2506.08438)
*Yuchen Wu,Xinyi Zhong,Zhuoran Yang*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: We study an online learning version of the generalized principal-agent model,
where a principal interacts repeatedly with a strategic agent possessing
private types, private rewards, and taking unobservable actions. The agent is
non-myopic, optimizing a discounted sum of future rewards and may strategically
misreport types to manipulate the principal's learning. The principal,
observing only her own realized rewards and the agent's reported types, aims to
learn an optimal coordination mechanism that minimizes strategic regret. We
develop the first provably sample-efficient algorithm for this challenging
setting. Our approach features a novel pipeline that combines (i) a delaying
mechanism to incentivize approximately myopic agent behavior, (ii) an
innovative reward angle estimation framework that uses sector tests and a
matching procedure to recover type-dependent reward functions, and (iii) a
pessimistic-optimistic LinUCB algorithm that enables the principal to explore
efficiently while respecting the agent's incentive constraints. We establish a
near optimal $\tilde{O}(\sqrt{T}) $ regret bound for learning the principal's
optimal policy, where $\tilde{O}(\cdot) $ omits logarithmic factors. Our
results open up new avenues for designing robust online learning algorithms for
a wide range of game-theoretic settings involving private types and strategic
agents.

</details>


### [292] [Time-Aware World Model for Adaptive Prediction and Control](https://arxiv.org/abs/2506.08441)
*Anh N. Nhu,Sanghyun Son,Ming Lin*

Main category: cs.LG

TL;DR: 提出了一种时间感知世界模型（TAWM），通过显式结合时间动态性，提升模型在多样化控制任务中的性能和数据效率。


<details>
  <summary>Details</summary>
Motivation: 传统模型通常以固定时间步长采样，难以同时捕捉高低频任务动态。TAWM通过引入时间步长条件，优化采样率以匹配系统动态。

Method: TAWM基于信息论，训练时考虑多样化时间步长（Δt），而非固定步长，从而学习更全面的任务动态。

Result: 实验表明，TAWM在相同训练样本和迭代次数下，优于传统模型，尤其在多样化观测率任务中表现突出。

Conclusion: TAWM通过时间感知建模，显著提升了模型性能和数据效率，适用于多样化控制任务。

Abstract: In this work, we introduce the Time-Aware World Model (TAWM), a model-based
approach that explicitly incorporates temporal dynamics. By conditioning on the
time-step size, {\Delta}t, and training over a diverse range of {\Delta}t
values -- rather than sampling at a fixed time-step -- TAWM learns both high-
and low-frequency task dynamics across diverse control problems. Grounded in
the information-theoretic insight that the optimal sampling rate depends on a
system's underlying dynamics, this time-aware formulation improves both
performance and data efficiency. Empirical evaluations show that TAWM
consistently outperforms conventional models across varying observation rates
in a variety of control tasks, using the same number of training samples and
iterations. Our code can be found online at:
github.com/anh-nn01/Time-Aware-World-Model.

</details>


### [293] [MAC: An Efficient Gradient Preconditioning using Mean Activation Approximated Curvature](https://arxiv.org/abs/2506.08464)
*Hyunseok Seung,Jaewoo Lee,Hyunsuk Ko*

Main category: cs.LG

TL;DR: 论文提出了一种名为MAC的高效二阶优化方法，通过近似Kronecker因子降低计算负担，并在Transformer中首次应用Kronecker分解。


<details>
  <summary>Details</summary>
Motivation: 二阶优化方法（如KFAC）虽然收敛性好，但计算成本高，因此需要更高效的替代方案。

Method: 分析KFAC中Fisher信息矩阵的Kronecker因子，提出高效近似方法，并首次将其应用于Transformer的注意力层。

Result: MAC在准确性、训练时间和内存使用上优于KFAC及其他先进方法。

Conclusion: MAC是一种高效且通用的二阶优化方法，适用于多种网络架构和任务。

Abstract: Second-order optimization methods for training neural networks, such as KFAC,
exhibit superior convergence by utilizing curvature information of loss
landscape. However, it comes at the expense of high computational burden. In
this work, we analyze the two components that constitute the layer-wise Fisher
information matrix (FIM) used in KFAC: the Kronecker factors related to
activations and pre-activation gradients. Based on empirical observations on
their eigenspectra, we propose efficient approximations for them, resulting in
a computationally efficient optimization method called MAC. To the best of our
knowledge, MAC is the first algorithm to apply the Kronecker factorization to
the FIM of attention layers used in transformers and explicitly integrate
attention scores into the preconditioning. We also study the convergence
property of MAC on nonlinear neural networks and provide two conditions under
which it converges to global minima. Our extensive evaluations on various
network architectures and datasets show that the proposed method outperforms
KFAC and other state-of-the-art methods in terms of accuracy, end-to-end
training time, and memory usage. Code is available at
https://github.com/hseung88/mac.

</details>


### [294] [AsFT: Anchoring Safety During LLM Fine-Tuning Within Narrow Safety Basin](https://arxiv.org/abs/2506.08473)
*Shuo Yang,Qihui Zhang,Yuyang Liu,Yue Huang,Xiaojun Jia,Kunpeng Ning,Jiayu Yao,Jigang Wang,Hailiang Dai,Yibing Song,Li Yuan*

Main category: cs.LG

TL;DR: 论文提出AsFT方法，通过在微调过程中利用对齐方向作为锚点，抑制有害更新，显著提升模型安全性。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLM）在微调时易受恶意或无害数据影响，导致安全风险。研究旨在解决这一问题。

Method: 提出AsFT方法，引入正则化项，利用对齐方向锚定微调过程，限制参数更新在安全范围内。

Result: 实验表明，AsFT优于Safe LoRA，减少有害行为7.60%，提升性能3.44%，且表现稳健。

Conclusion: AsFT通过锚定对齐方向，有效约束微调过程，显著提升模型安全性和性能。

Abstract: Large language models (LLMs) are vulnerable to safety risks during
fine-tuning, where small amounts of malicious or harmless data can compromise
safeguards. In this paper, building on the concept of alignment direction --
defined by the weight difference between aligned and unaligned models -- we
observe that perturbations along this direction preserve model safety. In
contrast, perturbations along directions orthogonal to this alignment are
strongly linked to harmful direction perturbations, rapidly degrading safety
and framing the parameter space as a narrow safety basin. Based on this
insight, we propose a methodology for safety fine-tuning called AsFT (Anchoring
Safety in Fine-Tuning), which integrates a regularization term into the
training objective. This term uses the alignment direction as an anchor to
suppress updates in harmful directions, ensuring that fine-tuning is
constrained within the narrow safety basin. Extensive experiments on multiple
datasets show that AsFT outperforms Safe LoRA, reducing harmful behavior by
7.60 percent, improving model performance by 3.44 percent, and maintaining
robust performance across various experimental settings. Code is available at
https://github.com/PKU-YuanGroup/AsFT

</details>


### [295] [Thermodynamically Consistent Latent Dynamics Identification for Parametric Systems](https://arxiv.org/abs/2506.08475)
*Xiaolong He,Yeonjong Shin,Anthony Gruber,Sohyeon Jung,Kookjin Lee,Youngsoo Choi*

Main category: cs.LG

TL;DR: 提出了一种基于热力学原理的高效降阶建模框架tLaSDI，结合自编码器和pGFINNs，显著提升了计算效率和精度。


<details>
  <summary>Details</summary>
Motivation: 解决参数化非线性动力系统的降阶建模问题，同时保持热力学原理（如自由能守恒和熵生成）。

Method: 结合自编码器降维和pGFINNs学习参数化潜在动力学，并采用物理信息主动学习策略优化数据采样。

Result: 在Burgers方程和Vlasov-Poisson方程上实现高达3,528倍加速，误差1-3%，训练和推理成本显著降低。

Conclusion: tLaSDI框架高效且精确，同时揭示了系统的热力学行为，为物理空间动力学提供了新见解。

Abstract: We propose an efficient thermodynamics-informed latent space dynamics
identification (tLaSDI) framework for the reduced-order modeling of parametric
nonlinear dynamical systems. This framework integrates autoencoders for
dimensionality reduction with newly developed parametric GENERIC
formalism-informed neural networks (pGFINNs), which enable efficient learning
of parametric latent dynamics while preserving key thermodynamic principles
such as free energy conservation and entropy generation across the parameter
space. To further enhance model performance, a physics-informed active learning
strategy is incorporated, leveraging a greedy, residual-based error indicator
to adaptively sample informative training data, outperforming uniform sampling
at equivalent computational cost. Numerical experiments on the Burgers'
equation and the 1D/1V Vlasov-Poisson equation demonstrate that the proposed
method achieves up to 3,528x speed-up with 1-3% relative errors, and
significant reduction in training (50-90%) and inference (57-61%) cost.
Moreover, the learned latent space dynamics reveal the underlying thermodynamic
behavior of the system, offering valuable insights into the physical-space
dynamics.

</details>


### [296] [Explaining, Fast and Slow: Abstraction and Refinement of Provable Explanations](https://arxiv.org/abs/2506.08505)
*Shahaf Bassan,Yizhak Yisrael Elboher,Tobias Ladner,Matthias Althoff,Guy Katz*

Main category: cs.LG

TL;DR: 本文提出了一种新颖的抽象-精炼技术，用于高效计算神经网络预测的可证明充分解释，解决了现有方法在可扩展性上的挑战。


<details>
  <summary>Details</summary>
Motivation: 尽管事后解释技术在神经网络中取得了显著进展，但许多现有方法依赖启发式方法，缺乏形式化保证。本文旨在通过可验证技术提供具有形式化保证的解释。

Method: 通过构建一个大幅简化的网络来抽象原始大型神经网络，并在简化网络上计算充分解释，若解释不足则逐步精炼网络规模直至收敛。

Result: 实验表明，该方法显著提高了获取可证明充分解释的效率，并提供了网络预测在不同抽象层次上的细粒度解释。

Conclusion: 该方法为神经网络预测提供了高效且可验证的解释框架，同时支持多层次的解释分析。

Abstract: Despite significant advancements in post-hoc explainability techniques for
neural networks, many current methods rely on heuristics and do not provide
formally provable guarantees over the explanations provided. Recent work has
shown that it is possible to obtain explanations with formal guarantees by
identifying subsets of input features that are sufficient to determine that
predictions remain unchanged using neural network verification techniques.
Despite the appeal of these explanations, their computation faces significant
scalability challenges. In this work, we address this gap by proposing a novel
abstraction-refinement technique for efficiently computing provably sufficient
explanations of neural network predictions. Our method abstracts the original
large neural network by constructing a substantially reduced network, where a
sufficient explanation of the reduced network is also provably sufficient for
the original network, hence significantly speeding up the verification process.
If the explanation is in sufficient on the reduced network, we iteratively
refine the network size by gradually increasing it until convergence. Our
experiments demonstrate that our approach enhances the efficiency of obtaining
provably sufficient explanations for neural network predictions while
additionally providing a fine-grained interpretation of the network's
predictions across different abstraction levels.

</details>


### [297] [DiffGradCAM: A Universal Class Activation Map Resistant to Adversarial Training](https://arxiv.org/abs/2506.08514)
*Jacob Piland,Chris Sweet,Adam Czakja*

Main category: cs.LG

TL;DR: 论文提出了SHAMs和DiffGradCAM，用于评估和改进CAM方法的鲁棒性，解决被动欺骗问题。


<details>
  <summary>Details</summary>
Motivation: CAM及其变体在解释CNN预测时存在漏洞，容易被被动欺骗操纵，导致误导性解释。

Method: 提出SHAMs作为对抗性基准，并设计DiffGradCAM，一种轻量级且对比性的CAM方法。

Result: SHAMs和DiffGradCAM在多类任务中验证了其有效性，DiffGradCAM在非对抗情况下与标准CAM方法一致。

Conclusion: SHAMs和DiffGradCAM为基于显著性的解释提供了新的鲁棒性评估和改进框架。

Abstract: Class Activation Mapping (CAM) and its gradient-based variants (e.g.,
GradCAM) have become standard tools for explaining Convolutional Neural Network
(CNN) predictions. However, these approaches typically focus on individual
logits, while for neural networks using softmax, the class membership
probability estimates depend \textit{only} on the \textit{differences} between
logits, not on their absolute values. This disconnect leaves standard CAMs
vulnerable to adversarial manipulation, such as passive fooling, where a model
is trained to produce misleading CAMs without affecting decision performance.
We introduce \textbf{Salience-Hoax Activation Maps (SHAMs)}, an
\emph{entropy-aware form of passive fooling} that serves as a benchmark for CAM
robustness under adversarial conditions. To address the passive fooling
vulnerability, we then propose \textbf{DiffGradCAM}, a novel, lightweight, and
contrastive approach to class activation mapping that is both non-suceptible to
passive fooling, but also matches the output of standard CAM methods such as
GradCAM in the non-adversarial case. Together, SHAM and DiffGradCAM establish a
new framework for probing and improving the robustness of saliency-based
explanations. We validate both contributions across multi-class tasks with few
and many classes.

</details>


### [298] [NeurIPS 2024 ML4CFD Competition: Results and Retrospective Analysis](https://arxiv.org/abs/2506.08516)
*Mouadh Yagoubi,David Danan,Milad Leyli-Abadi,Ahmed Mazari,Jean-Patrick Brunet,Abbas Kabalan,Fabien Casenave,Yuxin Ma,Giovanni Catalani,Jean Fesquet,Jacob Helwig,Xuan Zhang,Haiyang Yu,Xavier Bertrand,Frederic Tost,Michael Baurheim,Joseph Morlier,Shuiwang Ji*

Main category: cs.LG

TL;DR: ML4CFD竞赛通过评估ML模型在CFD模拟中的表现，展示了ML替代传统求解器的潜力。


<details>
  <summary>Details</summary>
Motivation: 解决ML在科学计算中准确性、泛化性和物理一致性不足的问题。

Method: 组织ML4CFD竞赛，使用OpenFOAM生成数据集，多标准评估模型性能。

Result: 顶级模型在综合指标上超越OpenFOAM求解器。

Conclusion: ML替代传统求解器具有潜力，需优化评估框架和设计原则。

Abstract: The integration of machine learning (ML) into the physical sciences is
reshaping computational paradigms, offering the potential to accelerate
demanding simulations such as computational fluid dynamics (CFD). Yet,
persistent challenges in accuracy, generalization, and physical consistency
hinder the practical deployment of ML models in scientific domains. To address
these limitations and systematically benchmark progress, we organized the
ML4CFD competition, centered on surrogate modeling for aerodynamic simulations
over two-dimensional airfoils. The competition attracted over 240 teams, who
were provided with a curated dataset generated via OpenFOAM and evaluated
through a multi-criteria framework encompassing predictive accuracy, physical
fidelity, computational efficiency, and out-of-distribution generalization.
This retrospective analysis reviews the competition outcomes, highlighting
several approaches that outperformed baselines under our global evaluation
score. Notably, the top entry exceeded the performance of the original OpenFOAM
solver on aggregate metrics, illustrating the promise of ML-based surrogates to
outperform traditional solvers under tailored criteria. Drawing from these
results, we analyze the key design principles of top submissions, assess the
robustness of our evaluation framework, and offer guidance for future
scientific ML challenges.

</details>


### [299] [Leveraging chaos in the training of artificial neural networks](https://arxiv.org/abs/2506.08523)
*Pedro Jiménez-González,Miguel C. Soriano,Lucas Lacasa*

Main category: cs.LG

TL;DR: 论文研究了在异常大学习率下梯度下降（GD）的动态特性，发现学习率在特定范围内时，GD会从纯利用型转向探索-利用平衡态，此时网络轨迹对初始条件敏感，且训练时间最短。


<details>
  <summary>Details</summary>
Motivation: 探索梯度下降在异常大学习率下的动态行为，揭示其从纯利用型转向探索-利用平衡态的机制。

Method: 通过分析神经网络的轨迹动态，计算最大Lyapunov指数，并在MNIST分类任务中验证。

Result: 在学习率特定范围内，GD表现出探索-利用平衡态，训练时间最短，且结果适用于多种学习任务和架构。

Conclusion: 研究揭示了瞬态混沌动态在神经网络训练中的积极作用，为加速训练提供了新思路。

Abstract: Traditional algorithms to optimize artificial neural networks when confronted
with a supervised learning task are usually exploitation-type relaxational
dynamics such as gradient descent (GD). Here, we explore the dynamics of the
neural network trajectory along training for unconventionally large learning
rates. We show that for a region of values of the learning rate, the GD
optimization shifts away from purely exploitation-like algorithm into a regime
of exploration-exploitation balance, as the neural network is still capable of
learning but the trajectory shows sensitive dependence on initial conditions --
as characterized by positive network maximum Lyapunov exponent --.
Interestingly, the characteristic training time required to reach an acceptable
accuracy in the test set reaches a minimum precisely in such learning rate
region, further suggesting that one can accelerate the training of artificial
neural networks by locating at the onset of chaos. Our results -- initially
illustrated for the MNIST classification task -- qualitatively hold for a range
of supervised learning tasks, learning architectures and other hyperparameters,
and showcase the emergent, constructive role of transient chaotic dynamics in
the training of artificial neural networks.

</details>


### [300] [Robust Evolutionary Multi-Objective Network Architecture Search for Reinforcement Learning (EMNAS-RL)](https://arxiv.org/abs/2506.08533)
*Nihal Acharya Adde,Alexandra Gianzina,Hanno Gottschalk,Andreas Ebert*

Main category: cs.LG

TL;DR: EMNAS利用遗传算法优化自动驾驶强化学习的神经网络架构，通过并行化和师生方法提升搜索效率，实验证明其性能优于人工设计模型。


<details>
  <summary>Details</summary>
Motivation: 为自动驾驶强化学习设计高效且轻量化的神经网络架构，同时保持性能。

Method: 使用遗传算法自动化网络设计，结合并行化和师生方法加速搜索并实现可扩展优化。

Result: EMNAS在奖励和模型轻量化方面优于人工设计模型。

Conclusion: EMNAS为自动驾驶强化学习提供了高效且可扩展的网络架构优化方法。

Abstract: This paper introduces Evolutionary Multi-Objective Network Architecture
Search (EMNAS) for the first time to optimize neural network architectures in
large-scale Reinforcement Learning (RL) for Autonomous Driving (AD). EMNAS uses
genetic algorithms to automate network design, tailored to enhance rewards and
reduce model size without compromising performance. Additionally,
parallelization techniques are employed to accelerate the search, and
teacher-student methodologies are implemented to ensure scalable optimization.
This research underscores the potential of transfer learning as a robust
framework for optimizing performance across iterative learning processes by
effectively leveraging knowledge from earlier generations to enhance learning
efficiency and stability in subsequent generations. Experimental results
demonstrate that tailored EMNAS outperforms manually designed models, achieving
higher rewards with fewer parameters. The findings of these strategies
contribute positively to EMNAS for RL in autonomous driving, advancing the
field toward better-performing networks suitable for real-world scenarios.

</details>


### [301] [DeepForm: Reasoning Large Language Model for Communication System Formulation](https://arxiv.org/abs/2506.08551)
*Panlong Wu,Ting Wang,Yifei Zhong,Haoqi Zhang,Zitong Wang,Fangxin Wang*

Main category: cs.LG

TL;DR: DeepForm是首个专为通信系统制定设计的推理LLM，通过两阶段训练策略和专用数据集CSFRC，显著优于现有通用LLM。


<details>
  <summary>Details</summary>
Motivation: 通用LLM在通信系统制定领域缺乏专业知识和高质量数据，DeepForm旨在填补这一空白。

Method: 采用两阶段训练：SFT结合CoT数据蒸馏知识，再通过C-ReMax强化学习提升推理能力。

Result: 实验表明DeepForm性能领先，优于大型专有LLM。

Conclusion: DeepForm为通信系统制定提供高效解决方案，相关资源将开源以促进研究。

Abstract: Communication system formulation is critical for advancing 6G and future
wireless technologies, yet it remains a complex, expertise-intensive task.
While Large Language Models (LLMs) offer potential, existing general-purpose
models often lack the specialized domain knowledge, nuanced reasoning
capabilities, and access to high-quality, domain-specific training data
required for adapting a general LLM into an LLM specially for communication
system formulation. To bridge this gap, we introduce DeepForm, the first
reasoning LLM specially for automated communication system formulation. We
propose the world-first large-scale, open-source dataset meticulously curated
for this domain called Communication System Formulation Reasoning Corpus
(CSFRC). Our framework employs a two-stage training strategy: first, Supervised
Fine-Tuning (SFT) with Chain-of-Thought (CoT) data to distill domain knowledge;
second, a novel rule-based Reinforcement Learning (RL) algorithm, C-ReMax based
on ReMax, to cultivate advanced modeling capabilities and elicit sophisticated
reasoning patterns like self-correction and verification. Extensive experiments
demonstrate that our model achieves state-of-the-art performance, significantly
outperforming larger proprietary LLMs on diverse senerios. We will release
related resources to foster further research in this area after the paper is
accepted.

</details>


### [302] [The Geometries of Truth Are Orthogonal Across Tasks](https://arxiv.org/abs/2506.08572)
*Waiss Azizian,Michael Kirchhof,Eugene Ndiaye,Louis Bethune,Michal Klein,Pierre Ablin,Marco Cuturi*

Main category: cs.LG

TL;DR: 研究发现，LLM的“真理几何”方法在任务间缺乏可迁移性，线性分类器在不同任务中表现差异显著。


<details>
  <summary>Details</summary>
Motivation: 探讨LLM的可靠性问题，特别是通过激活向量区分正确与错误答案的方法的局限性。

Method: 分析线性分类器在不同任务中的表现，并尝试使用稀疏正则化和混合探针等方法。

Result: 发现“真理几何”方法在任务间不可迁移，分类器支持集几乎不重叠。

Conclusion: 现有方法难以克服任务依赖性，激活向量在不同任务中形成明显分离的聚类。

Abstract: Large Language Models (LLMs) have demonstrated impressive generalization
capabilities across various tasks, but their claim to practical relevance is
still mired by concerns on their reliability. Recent works have proposed
examining the activations produced by an LLM at inference time to assess
whether its answer to a question is correct. Some works claim that a "geometry
of truth" can be learned from examples, in the sense that the activations that
generate correct answers can be distinguished from those leading to mistakes
with a linear classifier. In this work, we underline a limitation of these
approaches: we observe that these "geometries of truth" are intrinsically
task-dependent and fail to transfer across tasks. More precisely, we show that
linear classifiers trained across distinct tasks share little similarity and,
when trained with sparsity-enforcing regularizers, have almost disjoint
supports. We show that more sophisticated approaches (e.g., using mixtures of
probes and tasks) fail to overcome this limitation, likely because activation
vectors commonly used to classify answers form clearly separated clusters when
examined across tasks.

</details>


### [303] [SLEEPYLAND: trust begins with fair evaluation of automatic sleep staging models](https://arxiv.org/abs/2506.08574)
*Alvise Dei Rossi,Matteo Metaldi,Michal Bechny,Irina Filchenko,Julia van der Meer,Markus H. Schmidt,Claudio L. A. Bassetti,Athina Tzovara,Francesca D. Faraci,Luigi Fiorillo*

Main category: cs.LG

TL;DR: SLEEPYLAND是一个开源睡眠分期评估框架，旨在解决模型评估、泛化、偏见和标注变异性问题，包含大量ID和OOD数据，并推出SOMNUS集成模型，性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 临床采用深度学习进行睡眠分期的挑战包括模型评估、泛化、偏见和标注变异性，需要标准化解决方案。

Method: 提出SLEEPYLAND框架，包含大量ID和OOD数据，预训练模型，并开发SOMNUS集成模型。

Result: SOMNUS在24个数据集中表现优异，超越现有方法和人类评分者，同时量化了模型偏见。

Conclusion: SOMNUS和SLEEPYLAND为睡眠分期提供了标准化评估工具，但模型偏见仍需进一步解决。

Abstract: Despite advances in deep learning for automatic sleep staging, clinical
adoption remains limited due to challenges in fair model evaluation,
generalization across diverse datasets, model bias, and variability in human
annotations. We present SLEEPYLAND, an open-source sleep staging evaluation
framework designed to address these barriers. It includes more than 22'0000
hours in-domain (ID) sleep recordings, and more than 84'000 hours out-of-domain
(OOD) sleep recordings, spanning a broad range of ages, sleep-wake disorders,
and hardware setups. We release pre-trained models based on high-performing SoA
architectures and evaluate them under standardized conditions across single-
and multi-channel EEG/EOG configurations. We introduce SOMNUS, an ensemble
combining models across architectures and channel setups via soft voting.
SOMNUS achieves robust performance across twenty-four different datasets, with
macro-F1 scores between 68.7% and 87.2%, outperforming individual models in
94.9% of cases. Notably, SOMNUS surpasses previous SoA methods, even including
cases where compared models were trained ID while SOMNUS treated the same data
as OOD. Using a subset of the BSWR (N=6'633), we quantify model biases linked
to age, gender, AHI, and PLMI, showing that while ensemble improves robustness,
no model architecture consistently minimizes bias in performance and clinical
markers estimation. In evaluations on OOD multi-annotated datasets (DOD-H,
DOD-O), SOMNUS exceeds the best human scorer, i.e., MF1 85.2% vs 80.8% on
DOD-H, and 80.2% vs 75.9% on DOD-O, better reproducing the scorer consensus
than any individual expert (k = 0.89/0.85 and ACS = 0.95/0.94 for healthy/OSA
cohorts). Finally, we introduce ensemble disagreement metrics - entropy and
inter-model divergence based - predicting regions of scorer disagreement with
ROC AUCs up to 0.828, offering a data-driven proxy for human uncertainty.

</details>


### [304] [Diffusion-based Time Series Forecasting for Sewerage Systems](https://arxiv.org/abs/2506.08577)
*Nicholas A. Pearson,Francesca Cairoli,Luca Bortolussi,Davide Russo,Francesca Zanello*

Main category: cs.LG

TL;DR: 提出了一种基于生成式人工智能的深度学习模型，用于提升污水系统上下文预测的准确性。


<details>
  <summary>Details</summary>
Motivation: 通过捕捉复杂的环境信号相关性，提升极端天气条件下的预测可靠性。

Method: 开发了一种扩散模型处理多元时间序列数据，并结合保形推理技术校准预测。

Result: 在真实污水系统数据上验证了模型的高效性，即使在极端天气下也能保持准确性。

Conclusion: 该模型为污水系统的上下文预测提供了可靠且统计有效的解决方案。

Abstract: We introduce a novel deep learning approach that harnesses the power of
generative artificial intelligence to enhance the accuracy of contextual
forecasting in sewerage systems. By developing a diffusion-based model that
processes multivariate time series data, our system excels at capturing complex
correlations across diverse environmental signals, enabling robust predictions
even during extreme weather events. To strengthen the model's reliability, we
further calibrate its predictions with a conformal inference technique,
tailored for probabilistic time series data, ensuring that the resulting
prediction intervals are statistically reliable and cover the true target
values with a desired confidence level. Our empirical tests on real sewerage
system data confirm the model's exceptional capability to deliver reliable
contextual predictions, maintaining accuracy even under severe weather
conditions.

</details>


### [305] [CALT: A Library for Computer Algebra with Transformer](https://arxiv.org/abs/2506.08600)
*Hiroshi Kera,Shun Arakawa,Yuta Sato*

Main category: cs.LG

TL;DR: 论文介绍了CALT，一个帮助非深度学习专家训练符号计算任务的Python库。


<details>
  <summary>Details</summary>
Motivation: 探索如何通过深度学习（特别是Transformer模型）学习符号计算，并简化非专家的使用门槛。

Method: 利用Transformer模型训练序列到序列函数，开发了CALT库。

Result: CALT库能够帮助非专家训练符号计算模型。

Conclusion: CALT为符号计算社区提供了新的研究方向，并简化了深度学习在符号计算中的应用。

Abstract: Recent advances in artificial intelligence have demonstrated the learnability
of symbolic computation through end-to-end deep learning. Given a sufficient
number of examples of symbolic expressions before and after the target
computation, Transformer models - highly effective learners of
sequence-to-sequence functions - can be trained to emulate the computation.
This development opens up several intriguing challenges and new research
directions, which require active contributions from the symbolic computation
community. In this work, we introduce Computer Algebra with Transformer (CALT),
a user-friendly Python library designed to help non-experts in deep learning
train models for symbolic computation tasks.

</details>


### [306] [Flow Matching Meets PDEs: A Unified Framework for Physics-Constrained Generation](https://arxiv.org/abs/2506.08604)
*Giacomo Baldan,Qiang Liu,Alberto Guardone,Nils Thuerey*

Main category: cs.LG

TL;DR: 提出了一种名为PBFM的新生成框架，将物理约束显式嵌入流匹配目标，显著提高了物理残差的准确性。


<details>
  <summary>Details</summary>
Motivation: 现有生成方法（如扩散模型和流匹配）通常从数据中隐式学习物理规律，缺乏对物理约束的显式建模。

Method: PBFM结合流匹配损失和基于物理的残差损失，引入时间展开训练，无需调整超参数权重。

Result: 在三个典型PDE问题上，PBFM的物理残差比FM准确8倍，且在分布准确性上优于现有算法。

Conclusion: PBFM为物理和工程应用中的代理建模、不确定性量化和加速模拟提供了高效框架。

Abstract: Generative machine learning methods, such as diffusion models and flow
matching, have shown great potential in modeling complex system behaviors and
building efficient surrogate models. However, these methods typically learn the
underlying physics implicitly from data. We propose Physics-Based Flow Matching
(PBFM), a novel generative framework that explicitly embeds physical
constraints, both PDE residuals and algebraic relations, into the flow matching
objective. We also introduce temporal unrolling at training time that improves
the accuracy of the final, noise-free sample prediction. Our method jointly
minimizes the flow matching loss and the physics-based residual loss without
requiring hyperparameter tuning of their relative weights. Additionally, we
analyze the role of the minimum noise level, $\sigma_{\min}$, in the context of
physical constraints and evaluate a stochastic sampling strategy that helps to
reduce physical residuals. Through extensive benchmarks on three representative
PDE problems, we show that our approach yields up to an $8\times$ more accurate
physical residuals compared to FM, while clearly outperforming existing
algorithms in terms of distributional accuracy. PBFM thus provides a principled
and efficient framework for surrogate modeling, uncertainty quantification, and
accelerated simulation in physics and engineering applications.

</details>


### [307] [Sample Efficient Demonstration Selection for In-Context Learning](https://arxiv.org/abs/2506.08607)
*Kiran Purohit,V Venktesh,Sourangshu Bhattacharya,Avishek Anand*

Main category: cs.LG

TL;DR: 论文提出了一种名为CASE的高效示例选择方法，通过将示例选择建模为多臂老虎机问题，显著减少了LLM评估次数和运行时间。


<details>
  <summary>Details</summary>
Motivation: 在上下文学习范式中，如何高效选择少量示例以构建有效提示是一个关键挑战。

Method: 将示例选择问题建模为多臂老虎机问题，提出CASE方法，通过维护候选臂列表减少评估复杂度。

Result: CASE在运行时间上实现了7倍加速，减少了87%的LLM调用，且性能不下降。

Conclusion: CASE是一种高效且性能优越的示例选择方法，适用于上下文学习任务。

Abstract: The in-context learning paradigm with LLMs has been instrumental in advancing
a wide range of natural language processing tasks. The selection of few-shot
examples (exemplars / demonstration samples) is essential for constructing
effective prompts under context-length budget constraints. In this paper, we
formulate the exemplar selection task as a top-m best arms identification
problem. A key challenge in this setup is the exponentially large number of
arms that need to be evaluated to identify the m-best arms. We propose CASE
(Challenger Arm Sampling for Exemplar selection), a novel sample-efficient
selective exploration strategy that maintains a shortlist of "challenger" arms,
which are current candidates for the top-m arms. In each iteration, only one of
the arms from this shortlist or the current topm set is pulled, thereby
reducing sample complexity and, consequently, the number of LLM evaluations.
Furthermore, we model the scores of exemplar subsets (arms) using a
parameterized linear scoring function, leading to stochastic linear bandits
setting. CASE achieves remarkable efficiency gains of up to 7x speedup in
runtime while requiring 7x fewer LLM calls (87% reduction) without sacrificing
performance compared to state-of-the-art exemplar selection methods. We release
our code and data at https://github.com/kiranpurohit/CASE

</details>


### [308] [HSG-12M: A Large-Scale Spatial Multigraph Dataset](https://arxiv.org/abs/2506.08618)
*Xianquan Yan,Hakan Akgün,Kenji Kawaguchi,N. Duane Loh,Ching Hua Lee*

Main category: cs.LG

TL;DR: HSG-12M是首个大规模空间多重图数据集，保留了节点间几何上不同的轨迹作为独立边，为几何感知的图学习奠定了基础。


<details>
  <summary>Details</summary>
Motivation: 现有图基准假设边是非空间且简单的，忽略了物理上不同的路径。HSG-12M填补了这一空白，提供了物理基础的多样化拓扑结构。

Method: 通过Poly2Graph管道，将一维晶体哈密顿量映射为谱图，生成静态和动态的Hamiltonian谱图。

Result: HSG-12M包含1160万静态和510万动态谱图，覆盖1401个特征多项式类。基准测试揭示了多边几何学习的新挑战。

Conclusion: HSG-12M为几何感知图学习和数据驱动的科学发现提供了新机会，谱图作为多项式、向量和矩阵的通用拓扑指纹。

Abstract: Existing graph benchmarks assume non-spatial, simple edges, collapsing
physically distinct paths into a single link. We introduce HSG-12M, the first
large-scale dataset of $\textbf{spatial multigraphs}-$graphs embedded in a
metric space where multiple geometrically distinct trajectories between two
nodes are retained as separate edges. HSG-12M contains 11.6 million static and
5.1 million dynamic $\textit{Hamiltonian spectral graphs}$ across 1401
characteristic-polynomial classes, derived from 177 TB of spectral potential
data. Each graph encodes the full geometry of a 1-D crystal's energy spectrum
on the complex plane, producing diverse, physics-grounded topologies that
transcend conventional node-coordinate datasets. To enable future extensions,
we release $\texttt{Poly2Graph}$: a high-performance, open-source pipeline that
maps arbitrary 1-D crystal Hamiltonians to spectral graphs. Benchmarks with
popular GNNs expose new challenges in learning from multi-edge geometry at
scale. Beyond its practical utility, we show that spectral graphs serve as
universal topological fingerprints of polynomials, vectors, and matrices,
forging a new algebra-to-graph link. HSG-12M lays the groundwork for
geometry-aware graph learning and new opportunities of data-driven scientific
discovery in condensed matter physics and beyond.

</details>


### [309] [Time Series Representations for Classification Lie Hidden in Pretrained Vision Transformers](https://arxiv.org/abs/2506.08641)
*Simon Roschmann,Quentin Bouniot,Vasilii Feofanov,Ievgen Redko,Zeynep Akata*

Main category: cs.LG

TL;DR: TiViT框架将时间序列转换为图像，利用预训练的视觉Transformer（ViT）提升时间序列分类性能，并在标准基准测试中达到最优表现。


<details>
  <summary>Details</summary>
Motivation: 时间序列分类在医疗和工业中很重要，但公开数据集稀缺限制了时间序列基础模型（TSFM）的发展。

Method: 提出TiViT框架，将时间序列转换为图像，利用预训练ViT的表示能力，并通过理论和实验验证其有效性。

Result: TiViT在标准时间序列分类基准测试中表现最优，且与TSFM表示空间互补，结合后性能进一步提升。

Conclusion: TiViT展示了在非视觉领域复用视觉表示的新方向。

Abstract: Time series classification is a fundamental task in healthcare and industry,
yet the development of time series foundation models (TSFMs) remains limited by
the scarcity of publicly available time series datasets. In this work, we
propose Time Vision Transformer (TiViT), a framework that converts time series
into images to leverage the representational power of frozen Vision
Transformers (ViTs) pretrained on large-scale image datasets. First, we
theoretically motivate our approach by analyzing the 2D patching of ViTs for
time series, showing that it can increase the number of label-relevant tokens
and reduce the sample complexity. Second, we empirically demonstrate that TiViT
achieves state-of-the-art performance on standard time series classification
benchmarks by utilizing the hidden representations of large OpenCLIP models. We
explore the structure of TiViT representations and find that intermediate
layers with high intrinsic dimension are the most effective for time series
classification. Finally, we assess the alignment between TiViT and TSFM
representation spaces and identify a strong complementarity, with further
performance gains achieved by combining their features. Our findings reveal yet
another direction for reusing vision representations in a non-visual domain.

</details>


### [310] [Semi-gradient DICE for Offline Constrained Reinforcement Learning](https://arxiv.org/abs/2506.08644)
*Woosung Kim,JunHo Seo,Jongmin Lee,Byung-Jun Lee*

Main category: cs.LG

TL;DR: DICE框架用于解决策略诱导的稳态分布与目标分布不匹配问题，但近期改进方法削弱了其离线策略评估（OPE）能力。本文提出新方法，通过半梯度DICE实现OPE和约束RL。


<details>
  <summary>Details</summary>
Motivation: 解决DICE框架在离线约束强化学习中因半梯度优化导致的成本估计失败问题。

Method: 提出一种新方法，通过半梯度DICE实现离线策略评估和约束强化学习。

Result: 新方法在离线约束RL基准DSRL上实现了最先进的性能。

Conclusion: 通过半梯度DICE改进方法，成功解决了成本估计问题，提升了离线约束RL的性能。

Abstract: Stationary Distribution Correction Estimation (DICE) addresses the mismatch
between the stationary distribution induced by a policy and the target
distribution required for reliable off-policy evaluation (OPE) and policy
optimization. DICE-based offline constrained RL particularly benefits from the
flexibility of DICE, as it simultaneously maximizes return while estimating
costs in offline settings. However, we have observed that recent approaches
designed to enhance the offline RL performance of the DICE framework
inadvertently undermine its ability to perform OPE, making them unsuitable for
constrained RL scenarios. In this paper, we identify the root cause of this
limitation: their reliance on a semi-gradient optimization, which solves a
fundamentally different optimization problem and results in failures in cost
estimation. Building on these insights, we propose a novel method to enable OPE
and constrained RL through semi-gradient DICE. Our method ensures accurate cost
estimation and achieves state-of-the-art performance on the offline constrained
RL benchmark, DSRL.

</details>


### [311] [Fusing Cross-modal and Uni-modal Representations: A Kronecker Product Approach](https://arxiv.org/abs/2506.08645)
*Youqi Wu,Jingwei Zhang,Farzan Farnia*

Main category: cs.LG

TL;DR: RP-KrossFuse提出了一种方法，通过随机投影和Kronecker乘积将跨模态嵌入与单模态嵌入结合，以在保持跨模态对齐的同时提升单模态任务性能。


<details>
  <summary>Details</summary>
Motivation: 跨模态嵌入（如CLIP、BLIP）在跨模态对齐上表现良好，但在单模态任务上可能不如单模态嵌入。本研究旨在结合两者的优势。

Method: 采用基于随机投影的Kronecker乘积方法（RP-KrossFuse），在核空间中高效融合嵌入，支持可扩展实现。

Result: 实验表明，RP-KrossFuse在单模态任务上表现优异，同时保持了跨模态对齐能力。

Conclusion: RP-KrossFuse成功弥合了跨模态与单模态嵌入之间的差距，实现了两者的优势结合。

Abstract: Cross-modal embeddings, such as CLIP, BLIP and their variants, have achieved
promising results in aligning representations across modalities. However, these
embeddings could underperform compared to state-of-the-art single-modality
embeddings on modality-specific tasks. On the other hand, single-modality
embeddings excel in their domains but lack cross-modal alignment capabilities.
In this work, we focus on the problem of unifying cross-modality and
single-modality embeddings to achieve the performance of modality-expert
embedding within individual modalities while preserving cross-modal alignment.
To this end, we propose RP-KrossFuse, a method that leverages a random
projection-based Kronecker product to integrate cross-modal embeddings with
single-modality embeddings. RP-KrossFuse aims to fuse the sample-pairwise
similarity scores of the fused embeddings and operates efficiently in a
specified kernel space and supports scalable implementations via random Fourier
features for shift-invariant kernels such as the Gaussian kernel. We
demonstrate the effectiveness of RP-KrossFuse through several numerical
experiments, combining CLIP embeddings with uni-modal image and text
embeddings. Our numerical results indicate that RP-KrossFuse achieves
competitive modality-specific performance while retaining cross-modal
alignment, bridging the gap between cross-modal and single-modality embeddings.

</details>


### [312] [JoFormer (Journey-based Transformer): Theory and Empirical Analysis on the Tiny Shakespeare Dataset](https://arxiv.org/abs/2506.08652)
*Mahesh Godavarti*

Main category: cs.LG

TL;DR: JoFormer是一种基于旅程的Transformer架构，通过可学习的定向变换表示相对位置，在Tiny Shakespeare任务上表现优于RoFormer。


<details>
  <summary>Details</summary>
Motivation: Transformer在序列建模中表现出色，但如何有效融入位置信息仍是一个挑战。

Method: 提出JoFormer，利用非交换代数组合位置变换，扩展了相对位置表示方法。

Result: JoFormer在困惑度和收敛速度上优于RoFormer，展示了其表达能力的优势。

Conclusion: JoFormer为Transformer架构提供了更原则性的位置信息整合方法。

Abstract: Transformers have demonstrated remarkable success in sequence modeling, yet
effectively incorporating positional information remains a challenging and
active area of research. In this paper, we introduce JoFormer, a journey-based
Transformer architecture grounded in a recently proposed non-commutative
algebra for composing transformations across positions. JoFormer represents
relative positions through learnable directional transforms that are
sequentially composed along the input, thereby extending and generalizing
existing approaches based on relative position representations. We derive the
JoFormer attention mechanism from first principles and show that it subsumes
standard methods such as rotary transformations as special cases. To evaluate
its effectiveness, we compare JoFormer to the RoFormer baseline on the Tiny
Shakespeare character-level language modeling task. Our results demonstrate
that
  JoFormer consistently achieves lower perplexity and faster convergence,
highlighting the advantages of its more expressive, journey-based treatment of
position. Notably, the per-token JoFormer is still a primitive, conceptual
variant with layer-independent angles, yet it already demonstrates strong
performance-underscoring its promise as a proof of concept for more expressive
architectures. We conclude by discussing how JoFormer offers a principled
approach to integrating positional structure into Transformer architectures.
The code used in this work is available at
https://github.com/mahesh-godavarti/joformer.

</details>


### [313] [When Simple Model Just Works: Is Network Traffic Classification in Crisis?](https://arxiv.org/abs/2506.08655)
*Kamil Jerabek,Jan Luxemburk,Richard Plny,Josef Koumar,Jaroslav Pesek,Karel Hynek*

Main category: cs.LG

TL;DR: 论文发现简单k-NN基线方法在网络流量分类中表现优异，原因是数据冗余和常见的数据划分方式导致性能被高估。


<details>
  <summary>Details</summary>
Motivation: 探究为何简单的k-NN方法在网络流量分类中表现优于复杂模型，并揭示数据冗余和评估方法的问题。

Method: 在12个数据集和15个分类任务上评估k-NN基线方法，分析数据冗余和标签冲突的影响。

Result: 发现多数数据集包含50%以上的冗余样本，导致性能被高估，且标准机器学习方法可能不适用于流量分类。

Conclusion: 提出新的任务制定和评估方向，以解决数据冗余和评估偏差问题，推动领域发展。

Abstract: Machine learning has been applied to network traffic classification (TC) for
over two decades. While early efforts used shallow models, the latter 2010s saw
a shift toward complex neural networks, often reporting near-perfect accuracy.
However, it was recently revealed that a simple k-NN baseline using packet
sequences metadata (sizes, times, and directions) can be on par or even
outperform more complex methods. In this paper, we investigate this phenomenon
further and evaluate this baseline across 12 datasets and 15 TC tasks, and
investigate why it performs so well. Our analysis shows that most datasets
contain over 50% redundant samples (identical packet sequences), which
frequently appear in both training and test sets due to common splitting
practices. This redundancy can lead to overestimated model performance and
reduce the theoretical maximum accuracy when identical flows have conflicting
labels. Given its distinct characteristics, we further argue that standard
machine learning practices adapted from domains like NLP or computer vision may
be ill-suited for TC. Finally, we propose new directions for task formulation
and evaluation to address these challenges and help realign the field.

</details>


### [314] [Towards Robust Real-World Multivariate Time Series Forecasting: A Unified Framework for Dependency, Asynchrony, and Missingness](https://arxiv.org/abs/2506.08660)
*Jinkwan Jang,Hyungjin Park,Jinmyeong Choi,Taesup Kim*

Main category: cs.LG

TL;DR: ChannelTokenFormer是一种基于Transformer的预测模型，旨在解决多变量时间序列数据中的通道依赖、异步采样和缺失值问题。


<details>
  <summary>Details</summary>
Motivation: 现实世界中的多变量时间序列数据通常存在通道依赖、异步采样和缺失值问题，而现有模型往往基于过于简化的假设，无法应对这些挑战。

Method: 提出ChannelTokenFormer模型，通过灵活的架构显式捕捉跨通道交互、适应通道异步采样并有效处理缺失值。

Result: 在三个基准数据集和一个真实工业数据集上的实验表明，ChannelTokenFormer在复杂现实条件下具有优越的鲁棒性和准确性。

Conclusion: ChannelTokenFormer为解决多变量时间序列预测中的实际问题提供了一种有效方法。

Abstract: Real-world time series data are inherently multivariate, often exhibiting
complex inter-channel dependencies. Each channel is typically sampled at its
own period and is prone to missing values due to various practical and
operational constraints. These characteristics pose fundamental challenges
related to channel dependency, sampling asynchrony, and missingness, all of
which must be addressed to enable robust and reliable forecasting in practical
settings. However, most existing architectures are built on oversimplified
assumptions, such as identical sampling periods across channels and fully
observed inputs at test time, which often do not hold in real-world scenarios.
To bridge this gap, we propose ChannelTokenFormer, a Transformer-based
forecasting model with a flexible architecture designed to explicitly capture
cross-channel interactions, accommodate channel-wise asynchronous sampling, and
effectively handle missing values. Extensive experiments on three benchmark
datasets modified to reflect practical settings, along with one real-world
industrial dataset, demonstrate the superior robustness and accuracy of
ChannelTokenFormer under challenging real-world conditions.

</details>


### [315] [Optimizing Learned Image Compression on Scalar and Entropy-Constraint Quantization](https://arxiv.org/abs/2506.08662)
*Florian Borzechowski,Michael Schäfer,Heiko Schwarz,Jonathan Pfaff,Detlev Marpe,Thomas Wiegand*

Main category: cs.LG

TL;DR: 论文提出了一种针对变分自编码器图像压缩的额外微调训练步骤，以解决量化噪声建模不准确的问题，从而提升编码效率。


<details>
  <summary>Details</summary>
Motivation: 现有方法在训练过程中难以准确建模量化噪声，导致网络性能不佳，尤其是在熵约束量化器中。

Method: 在传统端到端训练后，对网络部分进行基于推理阶段量化潜在表示的微调训练。

Result: 实验表明，该方法在均匀标量量化和熵约束量化中均能带来编码增益，最高可节省2.2%的比特率。

Conclusion: 通过正确量化数据的微调训练，能够在不增加推理复杂度的情况下显著提升压缩性能。

Abstract: The continuous improvements on image compression with variational
autoencoders have lead to learned codecs competitive with conventional
approaches in terms of rate-distortion efficiency. Nonetheless, taking the
quantization into account during the training process remains a problem, since
it produces zero derivatives almost everywhere and needs to be replaced with a
differentiable approximation which allows end-to-end optimization. Though there
are different methods for approximating the quantization, none of them model
the quantization noise correctly and thus, result in suboptimal networks.
Hence, we propose an additional finetuning training step: After conventional
end-to-end training, parts of the network are retrained on quantized latents
obtained at the inference stage. For entropy-constraint quantizers like
Trellis-Coded Quantization, the impact of the quantizer is particularly
difficult to approximate by rounding or adding noise as the quantized latents
are interdependently chosen through a trellis search based on both the entropy
model and a distortion measure. We show that retraining on correctly quantized
data consistently yields additional coding gain for both uniform scalar and
especially for entropy-constraint quantization, without increasing inference
complexity. For the Kodak test set, we obtain average savings between 1% and
2%, and for the TecNick test set up to 2.2% in terms of Bj{\o}ntegaard-Delta
bitrate.

</details>


### [316] [Enhancing Reasoning Capabilities of Small Language Models with Blueprints and Prompt Template Search](https://arxiv.org/abs/2506.08669)
*Dongge Han,Menglin Xia,Daniel Madrigal Diaz,Samuel Kessler,Ankur Mallick,Xuchao Zhang,Mirian Del Carmen Hipolito Garcia,Jin Xu,Victor Rühle,Saravan Rajmohan*

Main category: cs.LG

TL;DR: 提出了一种通过LLM生成蓝图增强SLM推理能力的框架，并优化提示模板以解决SLM对提示变化的敏感性。


<details>
  <summary>Details</summary>
Motivation: SLM因容量限制导致推理能力不足且对提示变化敏感，需一种轻量级解决方案。

Method: 利用LLM生成结构化蓝图指导SLM推理，并集成提示模板搜索机制。

Result: 在数学、编程和逻辑推理任务中显著提升SLM性能，无需增加模型规模或额外训练。

Conclusion: 该框架为资源受限环境提供了一种高效、轻量的SLM增强方案。

Abstract: Small language models (SLMs) offer promising and efficient alternatives to
large language models (LLMs). However, SLMs' limited capacity restricts their
reasoning capabilities and makes them sensitive to prompt variations. To
address these challenges, we propose a novel framework that enhances SLM
reasoning capabilities through LLM generated blueprints. The blueprints provide
structured, high-level reasoning guides that help SLMs systematically tackle
related problems. Furthermore, our framework integrates a prompt template
search mechanism to mitigate the SLMs' sensitivity to prompt variations. Our
framework demonstrates improved SLM performance across various tasks, including
math (GSM8K), coding (MBPP), and logic reasoning (BBH). Our approach improves
the reasoning capabilities of SLMs without increasing model size or requiring
additional training, offering a lightweight and deployment-friendly solution
for on-device or resource-constrained environments.

</details>


### [317] [Towards Fair Representation: Clustering and Consensus](https://arxiv.org/abs/2506.08673)
*Diptarka Chakraborty,Kushagra Chatterjee,Debarati Das,Tien Long Nguyen,Romina Nobahari*

Main category: cs.LG

TL;DR: 该论文研究了共识聚类问题，首次将其与公平聚类结合，提出了一个常数因子近似算法，并探讨了如何最小化修改现有聚类以实现公平性。


<details>
  <summary>Details</summary>
Motivation: 共识聚类是机器学习和数据分析中的基础任务，但此前未考虑公平性问题。作者希望通过结合公平聚类理论，确保数据集中每个受保护群体在聚类中得到比例代表。

Method: 作者开发了一种最优算法（适用于群体比例相等的数据集）和近似算法（适用于群体比例不等的情况），并证明了问题的NP难性。

Result: 提出了首个常数因子近似算法，并展示了在群体比例不等时的近线性时间近似解法。同时证明了问题在群体比例不等时的NP难性。

Conclusion: 该研究填补了公平共识聚类的空白，其成果可能对其他聚类问题的公平变体产生广泛影响。

Abstract: Consensus clustering, a fundamental task in machine learning and data
analysis, aims to aggregate multiple input clusterings of a dataset,
potentially based on different non-sensitive attributes, into a single
clustering that best represents the collective structure of the data. In this
work, we study this fundamental problem through the lens of fair clustering, as
introduced by Chierichetti et al. [NeurIPS'17], which incorporates the
disparate impact doctrine to ensure proportional representation of each
protected group in the dataset within every cluster. Our objective is to find a
consensus clustering that is not only representative but also fair with respect
to specific protected attributes. To the best of our knowledge, we are the
first to address this problem and provide a constant-factor approximation.
  As part of our investigation, we examine how to minimally modify an existing
clustering to enforce fairness -- an essential postprocessing step in many
clustering applications that require fair representation. We develop an optimal
algorithm for datasets with equal group representation and near-linear time
constant factor approximation algorithms for more general scenarios with
different proportions of two group sizes. We complement our approximation
result by showing that the problem is NP-hard for two unequal-sized groups.
Given the fundamental nature of this problem, we believe our results on Closest
Fair Clustering could have broader implications for other clustering problems,
particularly those for which no prior approximation guarantees exist for their
fair variants.

</details>


### [318] [Mitigating Reward Over-optimization in Direct Alignment Algorithms with Importance Sampling](https://arxiv.org/abs/2506.08681)
*Phuc Minh Nguyen,Ngoc-Hieu Nguyen,Duy H. M. Nguyen,Anji Liu,An Mai,Binh T. Nguyen,Daniel Sonntag,Khoa D. Doan*

Main category: cs.LG

TL;DR: 本文提出了一种基于重要性采样的方法（IS-DAAs）来解决离线直接对齐算法（DAAs）中的过优化问题，通过引入重要性比率并限制其最大值，有效降低了过优化风险，提升了模型性能。


<details>
  <summary>Details</summary>
Motivation: 直接对齐算法（如DPO）在训练过程中容易出现过优化问题，导致模型偏离参考策略，性能下降。本文旨在解决这一问题。

Method: 提出IS-DAAs方法，通过重要性比率调整目标函数，并限制重要性比率的最大值以避免高方差问题。

Result: 实验表明，IS-DAAs能有效缓解过优化，尤其在低正则化强度下表现优于其他方法。

Conclusion: IS-DAAs是一种有效的解决方案，能够显著改善离线DAAs的性能和稳定性。

Abstract: Direct Alignment Algorithms (DAAs) such as Direct Preference Optimization
(DPO) have emerged as alternatives to the standard Reinforcement Learning from
Human Feedback (RLHF) for aligning large language models (LLMs) with human
values. However, these methods are more susceptible to over-optimization, in
which the model drifts away from the reference policy, leading to degraded
performance as training progresses. This paper proposes a novel
importance-sampling approach to mitigate the over-optimization problem of
offline DAAs. This approach, called (IS-DAAs), multiplies the DAA objective
with an importance ratio that accounts for the reference policy distribution.
IS-DAAs additionally avoid the high variance issue associated with importance
sampling by clipping the importance ratio to a maximum value. Our extensive
experiments demonstrate that IS-DAAs can effectively mitigate
over-optimization, especially under low regularization strength, and achieve
better performance than other methods designed to address this problem. Our
implementations are provided publicly at this link.

</details>


### [319] [Variational Autoencoder-Based Approach to Latent Feature Analysis on Efficient Representation of Power Load Monitoring Data](https://arxiv.org/abs/2506.08698)
*Boyu Xie,Tangtang Xie*

Main category: cs.LG

TL;DR: 论文提出了一种基于变分自编码器（VAE）的VAE-LF模型，用于高效表示和补全高维不完整（HDI）电力负荷监测（PLM）数据，以提升电力负荷预测（PLF）性能。


<details>
  <summary>Details</summary>
Motivation: 随着智能电网的发展，高维不完整的PLM数据对PLF模型的性能提出了挑战，需要一种有效的数据补全方法。

Method: VAE-LF通过编码器-解码器结构学习数据的低维潜在表示，将HDI PLM数据分块输入模型并生成补全数据。

Result: 在UK-DALE数据集上的实验表明，VAE-LF在5%和10%稀疏度测试中优于其他基准模型，RMSE和MAE显著降低，尤其在低稀疏度数据上表现更优。

Conclusion: VAE-LF为智能电网中的电力负荷管理提供了一种高效的数据补全解决方案。

Abstract: With the development of smart grids, High-Dimensional and Incomplete (HDI)
Power Load Monitoring (PLM) data challenges the performance of Power Load
Forecasting (PLF) models. In this paper, we propose a potential
characterization model VAE-LF based on Variational Autoencoder (VAE) for
efficiently representing and complementing PLM missing data. VAE-LF learns a
low-dimensional latent representation of the data using an Encoder-Decoder
structure by splitting the HDI PLM data into vectors and feeding them
sequentially into the VAE-LF model, and generates the complementary data.
Experiments on the UK-DALE dataset show that VAE-LF outperforms other benchmark
models in both 5% and 10% sparsity test cases, with significantly lower RMSE
and MAE, and especially outperforms on low sparsity ratio data. The method
provides an efficient data-completion solution for electric load management in
smart grids.

</details>


### [320] [Breaking the ICE: Exploring promises and challenges of benchmarks for Inference Carbon & Energy estimation for LLMs](https://arxiv.org/abs/2506.08727)
*Samarth Sikand,Rohit Mehra,Priyavanshi Pathania,Nikhil Bamby,Vibhu Saujanya Sharma,Vikrant Kaulgud,Sanjay Podder,Adam P. Burden*

Main category: cs.LG

TL;DR: 论文提出了一种利用LLM基准数据估算推理碳排放的框架R-ICE，解决了现有工具的高数据需求、侵入性和高误差问题。


<details>
  <summary>Details</summary>
Motivation: 生成式AI和LLM的广泛使用对能源和环境造成负担，但现有碳排放估算工具存在不足，亟需更实用的解决方案。

Method: 通过利用现有的SOTA基准数据，开发了R-ICE框架，用于估算提示级别的推理碳排放。

Result: 验证结果表明，基于基准的建模在碳排放估算方面具有潜力。

Conclusion: R-ICE框架为非侵入式碳排放估算提供了实用方法，值得进一步研究。

Abstract: While Generative AI stands to be one of the fastest adopted technologies
ever, studies have made evident that the usage of Large Language Models (LLMs)
puts significant burden on energy grids and our environment. It may prove a
hindrance to the Sustainability goals of any organization. A crucial step in
any Sustainability strategy is monitoring or estimating the energy consumption
of various components. While there exist multiple tools for monitoring energy
consumption, there is a dearth of tools/frameworks for estimating the
consumption or carbon emissions. Current drawbacks of both monitoring and
estimation tools include high input data points, intrusive nature, high error
margin, etc. We posit that leveraging emerging LLM benchmarks and related data
points can help overcome aforementioned challenges while balancing accuracy of
the emission estimations. To that extent, we discuss the challenges of current
approaches and present our evolving framework, R-ICE, which estimates prompt
level inference carbon emissions by leveraging existing state-of-the-art(SOTA)
benchmark. This direction provides a more practical and non-intrusive way to
enable emerging use-cases like dynamic LLM routing, carbon accounting, etc. Our
promising validation results suggest that benchmark-based modelling holds great
potential for inference emission estimation and warrants further exploration
from the scientific community.

</details>


### [321] [Exploration by Random Reward Perturbation](https://arxiv.org/abs/2506.08737)
*Haozhe Ma,Guoji Fu,Zhengding Luo,Jiele Wu,Tze-Yun Leong*

Main category: cs.LG

TL;DR: RRP是一种通过向环境奖励添加零均值噪声来增强强化学习探索的新策略，与现有方法兼容且计算开销低。


<details>
  <summary>Details</summary>
Motivation: 探索策略在强化学习中至关重要，但现有方法可能无法充分扩展探索范围。RRP旨在通过奖励扰动提升策略多样性。

Method: RRP通过向环境奖励添加零均值噪声实现，与动作扰动方法（如ε-greedy）兼容，且易于集成到现有算法中。

Result: 实验证明，RRP显著提升了PPO和SAC的性能，提高了样本效率并帮助逃离局部最优。

Conclusion: RRP为奖励塑造与噪声驱动探索建立了理论联系，展示了其互补潜力，是一种通用且高效的探索策略。

Abstract: We introduce Random Reward Perturbation (RRP), a novel exploration strategy
for reinforcement learning (RL). Our theoretical analyses demonstrate that
adding zero-mean noise to environmental rewards effectively enhances policy
diversity during training, thereby expanding the range of exploration. RRP is
fully compatible with the action-perturbation-based exploration strategies,
such as $\epsilon$-greedy, stochastic policies, and entropy regularization,
providing additive improvements to exploration effects. It is general,
lightweight, and can be integrated into existing RL algorithms with minimal
implementation effort and negligible computational overhead. RRP establishes a
theoretical connection between reward shaping and noise-driven exploration,
highlighting their complementary potential. Experiments show that RRP
significantly boosts the performance of Proximal Policy Optimization and Soft
Actor-Critic, achieving higher sample efficiency and escaping local optima
across various tasks, under both sparse and dense reward scenarios.

</details>


### [322] [Urban Incident Prediction with Graph Neural Networks: Integrating Government Ratings and Crowdsourced Reports](https://arxiv.org/abs/2506.08740)
*Sidhika Balachandar,Shuvom Sadhuka,Bonnie Berger,Emma Pierson,Nikhil Garg*

Main category: cs.LG

TL;DR: 提出了一种多视图、多输出的GNN模型，结合政府评分和众包报告数据预测城市事件的真实状态，并在纽约市案例中验证其优越性。


<details>
  <summary>Details</summary>
Motivation: 城市事件预测中，政府评分数据稀疏且众包报告存在偏差，需要一种方法整合这两种数据以更准确地预测真实事件状态。

Method: 设计了一个多视图、多输出的GNN模型，同时利用政府评分（无偏但稀疏）和众包报告（密集但有偏）数据。

Result: 模型在真实和半合成数据上表现优于仅使用单一数据源的模型，尤其是在评分数据稀疏时。同时量化了众包报告中的收入偏差。

Conclusion: 该模型为利用异构、稀疏和有偏数据进行潜在状态预测提供了广泛适用的方法。

Abstract: Graph neural networks (GNNs) are widely used in urban spatiotemporal
forecasting, such as predicting infrastructure problems. In this setting,
government officials wish to know in which neighborhoods incidents like
potholes or rodent issues occur. The true state of incidents (e.g., street
conditions) for each neighborhood is observed via government inspection
ratings. However, these ratings are only conducted for a sparse set of
neighborhoods and incident types. We also observe the state of incidents via
crowdsourced reports, which are more densely observed but may be biased due to
heterogeneous reporting behavior. First, for such settings, we propose a
multiview, multioutput GNN-based model that uses both unbiased rating data and
biased reporting data to predict the true latent state of incidents. Second, we
investigate a case study of New York City urban incidents and collect,
standardize, and make publicly available a dataset of 9,615,863 crowdsourced
reports and 1,041,415 government inspection ratings over 3 years and across 139
types of incidents. Finally, we show on both real and semi-synthetic data that
our model can better predict the latent state compared to models that use only
reporting data or models that use only rating data, especially when rating data
is sparse and reports are predictive of ratings. We also quantify demographic
biases in crowdsourced reporting, e.g., higher-income neighborhoods report
problems at higher rates. Our analysis showcases a widely applicable approach
for latent state prediction using heterogeneous, sparse, and biased data.

</details>


### [323] [On the Stability of the Jacobian Matrix in Deep Neural Networks](https://arxiv.org/abs/2506.08764)
*Benjamin Dadoun,Soufiane Hayou,Hanan Salam,Mohamed El Amine Seddik,Pierre Youssef*

Main category: cs.LG

TL;DR: 论文提出了一种适用于稀疏性和非独立同分布权重的深度神经网络稳定性定理，扩展了初始化方案的理论基础。


<details>
  <summary>Details</summary>
Motivation: 解决深度神经网络因梯度爆炸或消失导致的训练困难问题，特别是针对稀疏性和非独立同分布权重的网络模型。

Method: 利用随机矩阵理论的最新进展，建立了一个广义的稳定性定理，适用于稀疏性和弱相关权重的网络。

Result: 为具有结构化和依赖随机性的现代神经网络提供了严格的谱稳定性保证。

Conclusion: 该研究扩展了神经网络初始化方案的理论范围，适用于更广泛的网络模型。

Abstract: Deep neural networks are known to suffer from exploding or vanishing
gradients as depth increases, a phenomenon closely tied to the spectral
behavior of the input-output Jacobian. Prior work has identified critical
initialization schemes that ensure Jacobian stability, but these analyses are
typically restricted to fully connected networks with i.i.d. weights. In this
work, we go significantly beyond these limitations: we establish a general
stability theorem for deep neural networks that accommodates sparsity (such as
that introduced by pruning) and non-i.i.d., weakly correlated weights (e.g.
induced by training). Our results rely on recent advances in random matrix
theory, and provide rigorous guarantees for spectral stability in a much
broader class of network models. This extends the theoretical foundation for
initialization schemes in modern neural networks with structured and dependent
randomness.

</details>


### [324] [Design Patterns for Securing LLM Agents against Prompt Injections](https://arxiv.org/abs/2506.08837)
*Luca Beurer-Kellner,Beat Buesser Ana-Maria Creţu,Edoardo Debenedetti,Daniel Dobos,Daniel Fabian,Marc Fischer,David Froelicher,Kathrin Grosse,Daniel Naeff,Ezinwanne Ozoani,Andrew Paverd,Florian Tramèr,Václav Volhejn*

Main category: cs.LG

TL;DR: 论文提出了一套设计模式，用于构建能抵抗提示注入攻击的AI代理，并分析了其效用与安全性的权衡。


<details>
  <summary>Details</summary>
Motivation: 随着基于大语言模型的AI代理能力增强，其安全性问题（如提示注入攻击）变得至关重要。

Method: 提出了一套设计模式，系统分析其效用与安全性的权衡，并通过案例研究验证实用性。

Result: 设计模式能有效抵抗提示注入攻击，同时保持代理的功能性。

Conclusion: 该研究为构建安全的AI代理提供了理论支持和实践指导。

Abstract: As AI agents powered by Large Language Models (LLMs) become increasingly
versatile and capable of addressing a broad spectrum of tasks, ensuring their
security has become a critical challenge. Among the most pressing threats are
prompt injection attacks, which exploit the agent's resilience on natural
language inputs -- an especially dangerous threat when agents are granted tool
access or handle sensitive information. In this work, we propose a set of
principled design patterns for building AI agents with provable resistance to
prompt injection. We systematically analyze these patterns, discuss their
trade-offs in terms of utility and security, and illustrate their real-world
applicability through a series of case studies.

</details>


### [325] [IMAGIC-500: IMputation benchmark on A Generative Imaginary Country (500k samples)](https://arxiv.org/abs/2506.08844)
*Siyi Sun,David Antony Selby,Yunchuan Huang,Sebastian Vollmer,Seth Flaxman,Anisoara Calinescu*

Main category: cs.LG

TL;DR: 该研究利用世界银行的公开合成数据集IMAGIC-500，建立了全面的缺失数据填补基准，评估了多种填补方法的性能，旨在促进可重复的社会科学研究。


<details>
  <summary>Details</summary>
Motivation: 现实社会经济数据集因数据保护协议难以公开共享，导致缺失数据填补方法的评估缺乏基准，限制了研究的可重复性和可访问性。

Method: 研究使用IMAGIC-500数据集，设计了不同缺失机制（MCAR、MAR、MNAR）和缺失比例（10%至50%）的基准，评估统计、传统机器学习和深度学习方法。

Result: 结果揭示了各种填补方法在连续和分类变量上的准确性、计算效率及对下游预测任务的影响，展示了不同技术的优缺点。

Conclusion: IMAGIC-500数据集和基准为开发鲁棒的填补算法和促进社会科学研究的可重复性提供了重要工具。

Abstract: Missing data imputation in tabular datasets remains a pivotal challenge in
data science and machine learning, particularly within socioeconomic research.
However, real-world socioeconomic datasets are typically subject to strict data
protection protocols, which often prohibit public sharing, even for synthetic
derivatives. This severely limits the reproducibility and accessibility of
benchmark studies in such settings. Further, there are very few publicly
available synthetic datasets. Thus, there is limited availability of benchmarks
for systematic evaluation of imputation methods on socioeconomic datasets,
whether real or synthetic. In this study, we utilize the World Bank's publicly
available synthetic dataset, Synthetic Data for an Imaginary Country, which
closely mimics a real World Bank household survey while being fully public,
enabling broad access for methodological research. With this as a starting
point, we derived the IMAGIC-500 dataset: we select a subset of 500k
individuals across approximately 100k households with 19 socioeconomic
features, designed to reflect the hierarchical structure of real-world
household surveys. This paper introduces a comprehensive missing data
imputation benchmark on IMAGIC-500 under various missing mechanisms (MCAR, MAR,
MNAR) and missingness ratios (10\%, 20\%, 30\%, 40\%, 50\%). Our evaluation
considers the imputation accuracy for continuous and categorical variables,
computational efficiency, and impact on downstream predictive tasks, such as
estimating educational attainment at the individual level. The results
highlight the strengths and weaknesses of statistical, traditional machine
learning, and deep learning imputation techniques, including recent
diffusion-based methods. The IMAGIC-500 dataset and benchmark aim to facilitate
the development of robust imputation algorithms and foster reproducible social
science research.

</details>


### [326] [Agile Reinforcement Learning for Real-Time Task Scheduling in Edge Computing](https://arxiv.org/abs/2506.08850)
*Amin Avan,Akramul Azim,Qusay Mahmoud*

Main category: cs.LG

TL;DR: 论文提出了一种敏捷强化学习（aRL）方法，用于边缘计算中软实时应用的任务调度，通过智能探索和动作屏蔽提高调度效率和收敛速度。


<details>
  <summary>Details</summary>
Motivation: 边缘计算环境中任务调度的复杂性、动态性和多目标性使得传统启发式和元启发式算法难以生成最优调度方案，而强化学习算法又面临学习时间长的问题。

Method: 提出aRL方法，结合智能探索和动作屏蔽技术，减少无关动作的随机探索，提高RL-agent的预测性和适应性。

Result: 实验表明，aRL在命中率和收敛速度上优于基线方法。

Conclusion: aRL是一种适用于边缘计算中软实时应用任务调度的有效方法。

Abstract: Soft real-time applications are becoming increasingly complex, posing
significant challenges for scheduling offloaded tasks in edge computing
environments while meeting task timing constraints. Moreover, the exponential
growth of the search space, presence of multiple objectives and parameters, and
highly dynamic nature of edge computing environments further exacerbate the
complexity of task scheduling. As a result, schedulers based on heuristic and
metaheuristic algorithms frequently encounter difficulties in generating
optimal or near-optimal task schedules due to their constrained ability to
adapt to the dynamic conditions and complex environmental characteristics of
edge computing. Accordingly, reinforcement learning algorithms have been
incorporated into schedulers to address the complexity and dynamic conditions
inherent in task scheduling in edge computing. However, a significant
limitation of reinforcement learning algorithms is the prolonged learning time
required to adapt to new environments and to address medium- and large-scale
problems. This challenge arises from the extensive global action space and
frequent random exploration of irrelevant actions. Therefore, this study
proposes Agile Reinforcement learning (aRL), in which the RL-agent performs
informed exploration and executes only relevant actions. Consequently, the
predictability of the RL-agent is enhanced, leading to rapid adaptation and
convergence, which positions aRL as a suitable candidate for scheduling the
tasks of soft real-time applications in edge computing. The experiments
demonstrate that the combination of informed exploration and action-masking
methods enables aRL to achieve a higher hit-ratio and converge faster than the
baseline approaches.

</details>


### [327] [Adapting to Heterophilic Graph Data with Structure-Guided Neighbor Discovery](https://arxiv.org/abs/2506.08871)
*Victor M. Tenorio,Madeline Navarro,Samuel Rey,Santiago Segarra,Antonio G. Marques*

Main category: cs.LG

TL;DR: 论文提出了一种基于结构引导的图神经网络（SG-GNN），通过构建具有更高标签同质性的新图结构，解决了GNN在异质性数据上的性能问题。


<details>
  <summary>Details</summary>
Motivation: 传统GNN假设图数据具有同质性，但在异质性数据中表现不佳。论文旨在通过结构信息改进GNN的性能。

Method: 提出SG-GNN，通过链接具有相似结构属性的节点构建新图，并自适应地学习多图视图的权重。

Result: 在异质性数据集上，SG-GNN实现了最先进或极具竞争力的性能。

Conclusion: 利用结构信息可以有效指导GNN，提升其在异质性数据上的表现。

Abstract: Graph Neural Networks (GNNs) often struggle with heterophilic data, where
connected nodes may have dissimilar labels, as they typically assume homophily
and rely on local message passing. To address this, we propose creating
alternative graph structures by linking nodes with similar structural
attributes (e.g., role-based or global), thereby fostering higher label
homophily on these new graphs. We theoretically prove that GNN performance can
be improved by utilizing graphs with fewer false positive edges (connections
between nodes of different classes) and that considering multiple graph views
increases the likelihood of finding such beneficial structures. Building on
these insights, we introduce Structure-Guided GNN (SG-GNN), an architecture
that processes the original graph alongside the newly created structural
graphs, adaptively learning to weigh their contributions. Extensive experiments
on various benchmark datasets, particularly those with heterophilic
characteristics, demonstrate that our SG-GNN achieves state-of-the-art or
highly competitive performance, highlighting the efficacy of exploiting
structural information to guide GNNs.

</details>


### [328] [Filling in the Blanks: Applying Data Imputation in incomplete Water Metering Data](https://arxiv.org/abs/2506.08882)
*Dimitrios Amaxilatis,Themistoklis Sarantakos,Ioannis Chatzigiannakis,Georgios Mylonas*

Main category: cs.LG

TL;DR: 研究探讨了数据填补技术在水务网络监测中的应用，比较了多种方法的效果。


<details>
  <summary>Details</summary>
Motivation: 智能水表数据存在缺失，影响水务管理效率，需填补技术提升数据质量。

Method: 比较了k-Nearest Neighbors、MissForest、Transformers和Recurrent Neural Networks等多种填补方法。

Result: 有效的数据填补显著提升了水务数据的准确性和可靠性。

Conclusion: 填补技术可优化水务管理，如泄漏检测和预测性维护。

Abstract: In this work, we explore the application of recent data imputation techniques
to enhance monitoring and management of water distribution networks using smart
water meters, based on data derived from a real-world IoT water grid monitoring
deployment. Despite the detailed data produced by such meters, data gaps due to
technical issues can significantly impact operational decisions and efficiency.
Our results, by comparing various imputation methods, such as k-Nearest
Neighbors, MissForest, Transformers, and Recurrent Neural Networks, indicate
that effective data imputation can substantially enhance the quality of the
insights derived from water consumption data as we study their effect on
accuracy and reliability of water metering data to provide solutions in
applications like leak detection and predictive maintenance scheduling.

</details>


### [329] [InfoDPCCA: Information-Theoretic Dynamic Probabilistic Canonical Correlation Analysis](https://arxiv.org/abs/2506.08884)
*Shiqin Tang,Shujian Yu*

Main category: cs.LG

TL;DR: InfoDPCCA是一种动态概率CCA框架，用于提取两个相关序列的共享和特定潜在表示，通过信息论目标优化表示学习。


<details>
  <summary>Details</summary>
Motivation: 解决高维序列数据中提取有意义潜在表示的挑战，并改进现有动态CCA模型的解释性和鲁棒性。

Method: 采用信息论目标、两步训练方案和残差连接机制，优化共享和特定潜在空间的表示。

Result: 在合成和医学fMRI数据上表现优异，验证了其作为表示学习工具的有效性。

Conclusion: InfoDPCCA通过信息论和生成建模的结合，显著提升了动态CCA模型的性能和实用性。

Abstract: Extracting meaningful latent representations from high-dimensional sequential
data is a crucial challenge in machine learning, with applications spanning
natural science and engineering. We introduce InfoDPCCA, a dynamic
probabilistic Canonical Correlation Analysis (CCA) framework designed to model
two interdependent sequences of observations. InfoDPCCA leverages a novel
information-theoretic objective to extract a shared latent representation that
captures the mutual structure between the data streams and balances
representation compression and predictive sufficiency while also learning
separate latent components that encode information specific to each sequence.
Unlike prior dynamic CCA models, such as DPCCA, our approach explicitly
enforces the shared latent space to encode only the mutual information between
the sequences, improving interpretability and robustness. We further introduce
a two-step training scheme to bridge the gap between information-theoretic
representation learning and generative modeling, along with a residual
connection mechanism to enhance training stability. Through experiments on
synthetic and medical fMRI data, we demonstrate that InfoDPCCA excels as a tool
for representation learning. Code of InfoDPCCA is available at
https://github.com/marcusstang/InfoDPCCA.

</details>


### [330] [SeerAttention-R: Sparse Attention Adaptation for Long Reasoning](https://arxiv.org/abs/2506.08889)
*Yizhao Gao,Shuming Guo,Shijie Cao,Yuqing Xia,Yu Cheng,Lei Wang,Lingxiao Ma,Yutao Sun,Tianzhu Ye,Li Dong,Hayden Kwok-Hay So,Yu Hua,Ting Cao,Fan Yang,Mao Yang*

Main category: cs.LG

TL;DR: SeerAttention-R是一种稀疏注意力框架，专为推理模型的长解码设计，通过自蒸馏门控机制学习注意力稀疏性，无需修改预训练模型参数即可集成。


<details>
  <summary>Details</summary>
Motivation: 为长解码推理模型提供高效的稀疏注意力解决方案，同时保持推理准确性。

Method: 扩展自SeerAttention，移除查询池化以适应自回归解码，采用轻量级门控机制。

Result: 在AIME基准测试中，仅用0.4B token训练，4K token预算下保持接近无损的推理准确性；优化解码内核在H100 GPU上实现9倍加速。

Conclusion: SeerAttention-R是一种灵活高效的稀疏注意力框架，适用于长解码推理任务。

Abstract: We introduce SeerAttention-R, a sparse attention framework specifically
tailored for the long decoding of reasoning models. Extended from
SeerAttention, SeerAttention-R retains the design of learning attention
sparsity through a self-distilled gating mechanism, while removing query
pooling to accommodate auto-regressive decoding. With a lightweight plug-in
gating, SeerAttention-R is flexible and can be easily integrated into existing
pretrained model without modifying the original parameters. We demonstrate that
SeerAttention-R, trained on just 0.4B tokens, maintains near-lossless reasoning
accuracy with 4K token budget in AIME benchmark under large sparse attention
block sizes (64/128). Using TileLang, we develop a highly optimized sparse
decoding kernel that achieves near-theoretical speedups of up to 9x over
FlashAttention-3 on H100 GPU at 90% sparsity. Code is available at:
https://github.com/microsoft/SeerAttention.

</details>


### [331] [SwS: Self-aware Weakness-driven Problem Synthesis in Reinforcement Learning for LLM Reasoning](https://arxiv.org/abs/2506.08989)
*Xiao Liang,Zhong-Zhi Li,Yeyun Gong,Yang Wang,Hengyuan Zhang,Yelong Shen,Ying Nian Wu,Weizhu Chen*

Main category: cs.LG

TL;DR: 论文提出了一种自我感知弱点驱动的问题合成框架（SwS），通过识别模型在强化学习中的弱点并针对性生成问题，显著提升了模型在复杂推理任务中的表现。


<details>
  <summary>Details</summary>
Motivation: 现有问题集质量不足且缺乏针对性，限制了强化学习在大型语言模型训练中的效果。

Method: 通过分析模型在训练中的失败案例，提取核心概念并合成新问题，针对性增强模型弱点。

Result: 在多个主流推理基准测试中，7B和32B模型的平均性能分别提升了10.0%和7.7%。

Conclusion: SwS框架无需外部知识蒸馏，即可通过自我识别和解决弱点实现稳健泛化。

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) has proven effective
for training large language models (LLMs) on complex reasoning tasks, such as
mathematical problem solving. A prerequisite for the scalability of RLVR is a
high-quality problem set with precise and verifiable answers. However, the
scarcity of well-crafted human-labeled math problems and limited-verification
answers in existing distillation-oriented synthetic datasets limit their
effectiveness in RL. Additionally, most problem synthesis strategies
indiscriminately expand the problem set without considering the model's
capabilities, leading to low efficiency in generating useful questions. To
mitigate this issue, we introduce a Self-aware Weakness-driven problem
Synthesis framework (SwS) that systematically identifies model deficiencies and
leverages them for problem augmentation. Specifically, we define weaknesses as
questions that the model consistently fails to learn through its iterative
sampling during RL training. We then extract the core concepts from these
failure cases and synthesize new problems to strengthen the model's weak areas
in subsequent augmented training, enabling it to focus on and gradually
overcome its weaknesses. Without relying on external knowledge distillation,
our framework enables robust generalization byempowering the model to
self-identify and address its weaknesses in RL, yielding average performance
gains of 10.0% and 7.7% on 7B and 32B models across eight mainstream reasoning
benchmarks.

</details>


### [332] [Intention-Conditioned Flow Occupancy Models](https://arxiv.org/abs/2506.08902)
*Chongyi Zheng,Seohong Park,Sergey Levine,Benjamin Eysenbach*

Main category: cs.LG

TL;DR: 论文提出了一种基于意图条件流占用模型（InFOM）的强化学习预训练方法，通过预测未来状态分布提高样本效率和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 大规模预训练在机器学习中已广泛应用，但强化学习中的长期动作依赖问题尚未解决。本文旨在通过生成式AI技术解决这一问题。

Method: 使用流匹配技术构建概率模型，预测未来状态分布，并引入潜在变量捕捉用户意图以提高模型表达能力。

Result: 在36个状态基准和4个图像基准任务中，InFOM方法的中位回报提升1.8倍，成功率提高36%。

Conclusion: InFOM方法显著提升了强化学习预训练的效果，为长期依赖问题提供了有效解决方案。

Abstract: Large-scale pre-training has fundamentally changed how machine learning
research is done today: large foundation models are trained once, and then can
be used by anyone in the community (including those without data or compute
resources to train a model from scratch) to adapt and fine-tune to specific
tasks. Applying this same framework to reinforcement learning (RL) is appealing
because it offers compelling avenues for addressing core challenges in RL,
including sample efficiency and robustness. However, there remains a
fundamental challenge to pre-train large models in the context of RL: actions
have long-term dependencies, so training a foundation model that reasons across
time is important. Recent advances in generative AI have provided new tools for
modeling highly complex distributions. In this paper, we build a probabilistic
model to predict which states an agent will visit in the temporally distant
future (i.e., an occupancy measure) using flow matching. As large datasets are
often constructed by many distinct users performing distinct tasks, we include
in our model a latent variable capturing the user intention. This intention
increases the expressivity of our model, and enables adaptation with
generalized policy improvement. We call our proposed method
intention-conditioned flow occupancy models (InFOM). Comparing with alternative
methods for pre-training, our experiments on $36$ state-based and $4$
image-based benchmark tasks demonstrate that the proposed method achieves $1.8
\times$ median improvement in returns and increases success rates by $36\%$.
Website: https://chongyi-zheng.github.io/infom Code:
https://github.com/chongyi-zheng/infom

</details>


### [333] [e3: Learning to Explore Enables Extrapolation of Test-Time Compute for LLMs](https://arxiv.org/abs/2506.09026)
*Amrith Setlur,Matthew Y. R. Yang,Charlie Snell,Jeremy Greer,Ian Wu,Virginia Smith,Max Simchowitz,Aviral Kumar*

Main category: cs.LG

TL;DR: 论文提出了一种名为e3的方法，通过训练LLM进行上下文探索，以提升推理能力并实现性能外推。


<details>
  <summary>Details</summary>
Motivation: 现有推理模型在外推性能上表现不佳，需要一种方法使其能够在更长推理时间内持续提升性能。

Method: e3方法包括三个关键部分：1）利用LLM的非对称能力链式操作；2）通过错误轨迹的负梯度增强探索；3）设计训练课程以匹配任务难度和预算。

Result: e3-1.7B模型在AIME'25和HMMT'25上表现最佳，并能外推到2倍训练预算，显著提升pass@1和pass@k分数。

Conclusion: e3方法通过上下文探索有效提升了LLM的推理能力和外推性能。

Abstract: Test-time scaling offers a promising path to improve LLM reasoning by
utilizing more compute at inference time; however, the true promise of this
paradigm lies in extrapolation (i.e., improvement in performance on hard
problems as LLMs keep "thinking" for longer, beyond the maximum token budget
they were trained on). Surprisingly, we find that most existing reasoning
models do not extrapolate well. We show that one way to enable extrapolation is
by training the LLM to perform in-context exploration: training the LLM to
effectively spend its test time budget by chaining operations (such as
generation, verification, refinement, etc.), or testing multiple hypotheses
before it commits to an answer. To enable in-context exploration, we identify
three key ingredients as part of our recipe e3: (1) chaining skills that the
base LLM has asymmetric competence in, e.g., chaining verification (easy) with
generation (hard), as a way to implement in-context search; (2) leveraging
"negative" gradients from incorrect traces to amplify exploration during RL,
resulting in longer search traces that chains additional asymmetries; and (3)
coupling task difficulty with training token budget during training via a
specifically-designed curriculum to structure in-context exploration. Our
recipe e3 produces the best known 1.7B model according to AIME'25 and HMMT'25
scores, and extrapolates to 2x the training token budget. Our e3-1.7B model not
only attains high pass@1 scores, but also improves pass@k over the base model.

</details>


### [334] [Enhancing generalizability of model discovery across parameter space with multi-experiment equation learning (ME-EQL)](https://arxiv.org/abs/2506.08916)
*Maria-Veronica Ciocanel,John T. Nardini,Kevin B. Flores,Erica M. Rutter,Suzanne S. Sindi,Alexandria Volkening*

Main category: cs.LG

TL;DR: 论文提出两种多实验方程学习方法（ME-EQL），通过插值或统一模型库提升从ABM数据中学习连续模型的泛化能力。


<details>
  <summary>Details</summary>
Motivation: ABM计算密集且难以解析，传统EQL方法需大量模拟，泛化性不足。

Method: 提出OAT ME-EQL（逐个参数学习并插值）和ES ME-EQL（统一模型库），应用于出生-死亡模型和空间结构ABM。

Result: 两种方法显著降低参数恢复误差，OAT ME-EQL在参数空间泛化性更优。

Conclusion: 多实验方程学习可提升复杂生物系统模型的泛化性和可解释性。

Abstract: Agent-based modeling (ABM) is a powerful tool for understanding
self-organizing biological systems, but it is computationally intensive and
often not analytically tractable. Equation learning (EQL) methods can derive
continuum models from ABM data, but they typically require extensive
simulations for each parameter set, raising concerns about generalizability. In
this work, we extend EQL to Multi-experiment equation learning (ME-EQL) by
introducing two methods: one-at-a-time ME-EQL (OAT ME-EQL), which learns
individual models for each parameter set and connects them via interpolation,
and embedded structure ME-EQL (ES ME-EQL), which builds a unified model library
across parameters. We demonstrate these methods using a birth--death mean-field
model and an on-lattice agent-based model of birth, death, and migration with
spatial structure. Our results show that both methods significantly reduce the
relative error in recovering parameters from agent-based simulations, with OAT
ME-EQL offering better generalizability across parameter space. Our findings
highlight the potential of equation learning from multiple experiments to
enhance the generalizability and interpretability of learned models for complex
biological systems.

</details>


### [335] [Local MDI+: Local Feature Importances for Tree-Based Models](https://arxiv.org/abs/2506.08928)
*Zhongyuan Liang,Zachary T. Rewolinski,Abhineet Agarwal,Tiffany M. Tang,Bin Yu*

Main category: cs.LG

TL;DR: 论文提出了一种名为LMDI+的新方法，用于解决现有局部特征重要性方法（如LIME和TreeSHAP）在解释随机森林预测时的不足，通过扩展MDI+框架到样本特定场景，显著提升了性能和稳定性。


<details>
  <summary>Details</summary>
Motivation: 随机森林在表格数据中表现优异，但现有局部特征重要性方法依赖近似和扰动，忽略了模型内部结构，导致解释不稳定。MDI+在全局场景中解决了这一问题，但无法处理个体异质性，因此需要一种新的局部方法。

Method: 提出Local MDI+（LMDI+），将MDI+框架扩展到样本特定场景，利用决策树与线性模型在节点基础上的等价性，直接计算局部特征重要性。

Result: LMDI+在12个真实数据集上平均提升下游任务性能10%，比LIME和TreeSHAP更稳定，且能支持局部解释用例（如反事实识别和子群发现）。

Conclusion: LMDI+是一种高效且稳定的局部特征重要性方法，弥补了现有技术的不足，适用于高风险领域的可信预测解释。

Abstract: Tree-based ensembles such as random forests remain the go-to for tabular data
over deep learning models due to their prediction performance and computational
efficiency. These advantages have led to their widespread deployment in
high-stakes domains, where interpretability is essential for ensuring
trustworthy predictions. This has motivated the development of popular local
(i.e. sample-specific) feature importance (LFI) methods such as LIME and
TreeSHAP. However, these approaches rely on approximations that ignore the
model's internal structure and instead depend on potentially unstable
perturbations. These issues are addressed in the global setting by MDI+, a
feature importance method which exploits an equivalence between decision trees
and linear models on a transformed node basis. However, the global MDI+ scores
are not able to explain predictions when faced with heterogeneous individual
characteristics. To address this gap, we propose Local MDI+ (LMDI+), a novel
extension of the MDI+ framework to the sample specific setting. LMDI+
outperforms existing baselines LIME and TreeSHAP in identifying
instance-specific signal features, averaging a 10% improvement in downstream
task performance across twelve real-world benchmark datasets. It further
demonstrates greater stability by consistently producing similar instance-level
feature importance rankings across multiple random forest fits. Finally, LMDI+
enables local interpretability use cases, including the identification of
closer counterfactuals and the discovery of homogeneous subgroups.

</details>


### [336] [BioLangFusion: Multimodal Fusion of DNA, mRNA, and Protein Language Models](https://arxiv.org/abs/2506.08936)
*Amina Mollaysa,Artem Moskale,Pushpak Pati,Tommaso Mansi,Mangal Prakash,Rui Liao*

Main category: cs.LG

TL;DR: BioLangFusion通过整合DNA、mRNA和蛋白质预训练语言模型，生成统一的分子表示，在分子属性预测任务中表现优于单模态基线。


<details>
  <summary>Details</summary>
Motivation: 基于分子生物学的中心法则（信息从基因到转录本再到蛋白质的流动），研究如何在不同模态间建立直接的跨模态对应关系。

Method: 采用三种融合技术：密码子级嵌入拼接、熵正则化注意力池化和跨模态多头注意力，无需额外预训练或修改基础模型。

Result: 在五个分子属性预测任务中，BioLangFusion优于单模态基线，表明简单的预训练模型融合能高效捕获多组学信息。

Conclusion: BioLangFusion展示了预训练模型简单融合的潜力，能够以最小开销捕获多模态互补信息。

Abstract: We present BioLangFusion, a simple approach for integrating pre-trained DNA,
mRNA, and protein language models into unified molecular representations.
Motivated by the central dogma of molecular biology (information flow from gene
to transcript to protein), we align per-modality embeddings at the biologically
meaningful codon level (three nucleotides encoding one amino acid) to ensure
direct cross-modal correspondence. BioLangFusion studies three standard fusion
techniques: (i) codon-level embedding concatenation, (ii) entropy-regularized
attention pooling inspired by multiple-instance learning, and (iii) cross-modal
multi-head attention -- each technique providing a different inductive bias for
combining modality-specific signals. These methods require no additional
pre-training or modification of the base models, allowing straightforward
integration with existing sequence-based foundation models. Across five
molecular property prediction tasks, BioLangFusion outperforms strong unimodal
baselines, showing that even simple fusion of pre-trained models can capture
complementary multi-omic information with minimal overhead.

</details>


### [337] [KARMA: A Multilevel Decomposition Hybrid Mamba Framework for Multivariate Long-Term Time Series Forecasting](https://arxiv.org/abs/2506.08939)
*Hang Ye,Gaoxiang Duan,Haoran Zeng,Yangxin Zhu,Lingxue Meng,Xiaoying Zheng,Yongxin Zhu*

Main category: cs.LG

TL;DR: KARMA提出了一种新的时间序列预测方法，结合自适应时间通道分解和混合频时分解模块，显著提升了预测精度和计算效率。


<details>
  <summary>Details</summary>
Motivation: 传统时间序列分解方法固定且单一，无法挖掘复杂动态特性；Transformer模型计算复杂度高，难以处理长序列。

Method: KARMA采用自适应时间通道分解（ATCD）和混合频时分解（HFTD）模块，结合多尺度Mamba-based KarmaBlock处理全局和局部信息。

Result: 在八个真实数据集上，KARMA在预测精度和计算效率上显著优于主流基线方法。

Conclusion: KARMA通过动态分解和高效处理复杂时间动态，为多变量长期时间序列预测提供了有效解决方案。

Abstract: Multivariate long-term and efficient time series forecasting is a key
requirement for a variety of practical applications, and there are complex
interleaving time dynamics in time series data that require decomposition
modeling. Traditional time series decomposition methods are single and rely on
fixed rules, which are insufficient for mining the potential information of the
series and adapting to the dynamic characteristics of complex series. On the
other hand, the Transformer-based models for time series forecasting struggle
to effectively model long sequences and intricate dynamic relationships due to
their high computational complexity. To overcome these limitations, we
introduce KARMA, with an Adaptive Time Channel Decomposition module (ATCD) to
dynamically extract trend and seasonal components. It further integrates a
Hybrid Frequency-Time Decomposition module (HFTD) to further decompose Series
into frequency-domain and time-domain. These components are coupled with
multi-scale Mamba-based KarmaBlock to efficiently process global and local
information in a coordinated manner. Experiments on eight real-world datasets
from diverse domains well demonstrated that KARMA significantly outperforms
mainstream baseline methods in both predictive accuracy and computational
efficiency. Code and full results are available at this repository:
https://github.com/yedadasd/KARMA

</details>


### [338] [Towards Robust Deep Reinforcement Learning against Environmental State Perturbation](https://arxiv.org/abs/2506.08961)
*Chenxu Wang,Huaping Liu*

Main category: cs.LG

TL;DR: 该论文研究了深度强化学习（DRL）中环境状态扰动的对抗攻击与防御，提出了一种名为Boosted Adversarial Training（BAT）的防御框架。


<details>
  <summary>Details</summary>
Motivation: 现有研究很少关注环境状态扰动，而这类扰动在具体场景中很常见。论文旨在提升DRL代理在这种扰动下的鲁棒性。

Method: 论文首先提出了一种非目标攻击方法作为校准对手，随后提出了BAT框架，结合监督学习和对抗性强化学习来训练代理。

Result: 实验表明主流代理在环境状态扰动下脆弱，而BAT能显著提升鲁棒性。

Conclusion: BAT框架在多种情况下有效增强了代理对环境状态扰动的鲁棒性，而现有算法可能不适用。

Abstract: Adversarial attacks and robustness in Deep Reinforcement Learning (DRL) have
been widely studied in various threat models; however, few consider
environmental state perturbations, which are natural in embodied scenarios. To
improve the robustness of DRL agents, we formulate the problem of environmental
state perturbation, introducing a preliminary non-targeted attack method as a
calibration adversary, and then propose a defense framework, named Boosted
Adversarial Training (BAT), which first tunes the agents via supervised
learning to avoid catastrophic failure and subsequently adversarially trains
the agent with reinforcement learning. Extensive experimental results
substantiate the vulnerability of mainstream agents under environmental state
perturbations and the effectiveness of our proposed attack. The defense results
demonstrate that while existing robust reinforcement learning algorithms may
not be suitable, our BAT framework can significantly enhance the robustness of
agents against environmental state perturbations across various situations.

</details>


### [339] [GFRIEND: Generative Few-shot Reward Inference through EfficieNt DPO](https://arxiv.org/abs/2506.08965)
*Yiyang Zhao,Huiyu Bai,Xuejiao Zhao*

Main category: cs.LG

TL;DR: 提出了一种数据增强与扩展框架，通过偏好细化和多级直接偏好优化（M-DPO），在小样本数据上训练生成奖励模型，性能媲美大规模数据集训练的模型。


<details>
  <summary>Details</summary>
Motivation: 提高基于人类反馈的强化学习（RLHF）的效率和可扩展性，解决传统方法在样本配对和数据多样性上的不足。

Method: 引入偏好细化，利用Chain-of-Thought（CoT）采样揭示多样且高质量的偏好关系；采用基于困惑度的评分机制和多级直接偏好优化（M-DPO）。

Result: 实验显示该方法显著提升数据效率和模型性能，小样本训练的奖励模型性能与大规模数据集训练的模型相当。

Conclusion: 数据高效策略在奖励模型优化中具有潜力，为低资源RLHF应用提供了有效解决方案。

Abstract: The ability to train high-performing reward models with few-shot data is
critical for enhancing the efficiency and scalability of Reinforcement Learning
from Human Feedback (RLHF). We propose a data augmentation and expansion
framework that enables generative reward models trained on small datasets to
achieve comparable performance to those trained on large-scale datasets.
Traditional methods to train a generative reward model, such as Direct
Preference Optimization (DPO), are constrained by inefficiencies in sample
pairing and limited data diversity. This work introduces preference refinement,
which employs Chain-of-Thought (CoT) sampling to uncover diverse and
high-quality preference relationships. It also incorporates a perplexity-based
scoring mechanism to assign nuanced preference levels and utilizes Multi-level
Direct Preference Optimization (M-DPO) to enable the model to capture
finer-grained preference differences between samples. Experimental results
demonstrate that the proposed method significantly enhances data efficiency and
model performance, enabling reward models trained in a few-shot setting to
achieve results on par with those trained on large-scale datasets. This study
underscores the potential of data-efficient strategies in advancing reward
model optimization, offering a robust solution for low-resource RLHF
applications.

</details>


### [340] [Tailored Architectures for Time Series Forecasting: Evaluating Deep Learning Models on Gaussian Process-Generated Data](https://arxiv.org/abs/2506.08977)
*Victoria Hankemeier,Malte Schilling*

Main category: cs.LG

TL;DR: 论文探讨了深度学习在时间序列预测中的应用，提出了一种新数据集和模型TimeFlex，用于评估模型对不同时间序列特性的适应性。


<details>
  <summary>Details</summary>
Motivation: 现有研究多基于特定真实数据，未能明确展示数据特性与模型架构的匹配关系，因此需要更系统的评估方法。

Method: 使用高斯过程生成具有已知特性的新数据集，并开发模块化模型TimeFlex，以处理多样化的时间动态。

Result: TimeFlex在多种时间序列条件下表现优于现有先进模型，揭示了模型性能与数据特性的关联。

Conclusion: 研究为时间序列预测提供了更系统的评估框架，并展示了模块化设计的优势。

Abstract: Developments in Deep Learning have significantly improved time series
forecasting by enabling more accurate modeling of complex temporal dependencies
inherent in sequential data. The effectiveness of such models is often
demonstrated on limited sets of specific real-world data. Although this allows
for comparative analysis, it still does not demonstrate how specific data
characteristics align with the architectural strengths of individual models.
Our research aims at uncovering clear connections between time series
characteristics and particular models. We introduce a novel dataset generated
using Gaussian Processes, specifically designed to display distinct, known
characteristics for targeted evaluations of model adaptability to them.
Furthermore, we present TimeFlex, a new model that incorporates a modular
architecture tailored to handle diverse temporal dynamics, including trends and
periodic patterns. This model is compared to current state-of-the-art models,
offering a deeper understanding of how models perform under varied time series
conditions.

</details>


### [341] [Propositional Logic for Probing Generalization in Neural Networks](https://arxiv.org/abs/2506.08978)
*Anna Langedijk,Jaap Jumelet,Willem Zuidema*

Main category: cs.LG

TL;DR: 论文研究了三种神经网络架构（Transformer、GCN和LSTM）在命题逻辑任务中的泛化能力，发现它们在未见模式（尤其是涉及否定的情况）上表现不佳，表明需要更强的归纳偏置。


<details>
  <summary>Details</summary>
Motivation: 探讨神经网络是否能真正学习和表示符号规则，尤其是在逻辑推理任务中的表现。

Method: 使用命题逻辑任务生成逻辑公式的满足赋值，评估三种架构在分布内和分布外的泛化能力。

Result: 所有模型在分布内表现良好，但对涉及否定的未见模式泛化能力差，尤其是Transformer需结构偏置才能组合性应用否定。

Conclusion: 标准架构在系统学习逻辑运算符表示方面存在局限，需更强的归纳偏置以支持基于规则的推理。

Abstract: The extent to which neural networks are able to acquire and represent
symbolic rules remains a key topic of research and debate. Much current work
focuses on the impressive capabilities of large language models, as well as
their often ill-understood failures on a wide range of reasoning tasks. In this
paper, in contrast, we investigate the generalization behavior of three key
neural architectures (Transformers, Graph Convolution Networks and LSTMs) in a
controlled task rooted in propositional logic. The task requires models to
generate satisfying assignments for logical formulas, making it a structured
and interpretable setting for studying compositionality. We introduce a
balanced extension of an existing dataset to eliminate superficial patterns and
enable testing on unseen operator combinations. Using this dataset, we evaluate
the ability of the three architectures to generalize beyond the training
distribution. While all models perform well in-distribution, we find that
generalization to unseen patterns, particularly those involving negation,
remains a significant challenge. Transformers fail to apply negation
compositionally, unless structural biases are introduced. Our findings
highlight persistent limitations in the ability of standard architectures to
learn systematic representations of logical operators, suggesting the need for
stronger inductive biases to support robust rule-based reasoning.

</details>


### [342] [On Finetuning Tabular Foundation Models](https://arxiv.org/abs/2506.08982)
*Ivan Rubachev,Akim Kotelnikov,Nikolay Kartashev*

Main category: cs.LG

TL;DR: TabPFNv2是一种新兴的表格深度学习基础模型，研究探讨了其微调策略及内部机制，发现全微调是最优方法，并在部分数据集上实现了最佳性能。


<details>
  <summary>Details</summary>
Motivation: 探索TabPFNv2基础模型的最佳微调方法及其内部机制变化，填补现有研究的空白。

Method: 系统评估多种微调策略，分析微调对模型内部机制的影响，类比检索增强模型。

Result: 全微调在时间和效果上最优，微调后模型能更准确地加权相关样本，提升性能。在部分数据集上达到SOTA，但在时序变化和特征丰富的任务中表现不稳定。

Conclusion: 全微调是TabPFNv2的最佳实践，但其适用性受数据集特性影响。

Abstract: Foundation models are an emerging research direction in tabular deep
learning. Notably, TabPFNv2 recently claimed superior performance over
traditional GBDT-based methods on small-scale datasets using an in-context
learning paradigm, which does not adapt model parameters to target datasets.
However, the optimal finetuning approach for adapting tabular foundational
models, and how this adaptation reshapes their internal mechanisms, remains
underexplored. While prior works studied finetuning for earlier foundational
models, inconsistent findings and TabPFNv2's unique architecture necessitate
fresh investigation. To address these questions, we first systematically
evaluate various finetuning strategies on diverse datasets. Our findings
establish full finetuning as the most practical solution for TabPFNv2 in terms
of time-efficiency and effectiveness. We then investigate how finetuning alters
TabPFNv2's inner mechanisms, drawing an analogy to retrieval-augmented models.
We reveal that the success of finetuning stems from the fact that after
gradient-based adaptation, the dot products of the query-representations of
test objects and the key-representations of in-context training objects more
accurately reflect their target similarity. This improved similarity allows
finetuned TabPFNv2 to better approximate target dependency by appropriately
weighting relevant in-context samples, improving the retrieval-based prediction
logic. From the practical perspective, we managed to finetune TabPFNv2 on
datasets with up to 50K objects, observing performance improvements on almost
all tasks. More precisely, on academic datasets with I.I.D. splits, finetuning
allows TabPFNv2 to achieve state-of-the-art results, while on datasets with
gradual temporal shifts and rich feature sets, TabPFNv2 is less stable and
prior methods remain better.

</details>


### [343] [Branched Schrödinger Bridge Matching](https://arxiv.org/abs/2506.09007)
*Sophia Tang,Yinuo Zhang,Alexander Tong,Pranam Chatterjee*

Main category: cs.LG

TL;DR: 论文提出了一种名为BranchSBM的新框架，用于学习分支的Schrödinger桥，解决了现有方法无法捕捉多路径分支演化的问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法（如流匹配和Schrödinger桥匹配）只能建模单一路径的分布映射，无法处理从共同起源到多个不同结果的分支演化。

Method: BranchSBM通过参数化多个时间依赖的速度场和增长过程，实现了对多个终端分布的分支演化建模。

Result: BranchSBM不仅更具表达力，而且在多路径导航、细胞命运分叉建模和细胞响应模拟等任务中表现出色。

Conclusion: BranchSBM为解决多路径分支演化问题提供了有效工具，扩展了生成建模的应用范围。

Abstract: Predicting the intermediate trajectories between an initial and target
distribution is a central problem in generative modeling. Existing approaches,
such as flow matching and Schr\"odinger Bridge Matching, effectively learn
mappings between two distributions by modeling a single stochastic path.
However, these methods are inherently limited to unimodal transitions and
cannot capture branched or divergent evolution from a common origin to multiple
distinct outcomes. To address this, we introduce Branched Schr\"odinger Bridge
Matching (BranchSBM), a novel framework that learns branched Schr\"odinger
bridges. BranchSBM parameterizes multiple time-dependent velocity fields and
growth processes, enabling the representation of population-level divergence
into multiple terminal distributions. We show that BranchSBM is not only more
expressive but also essential for tasks involving multi-path surface
navigation, modeling cell fate bifurcations from homogeneous progenitor states,
and simulating diverging cellular responses to perturbations.

</details>


### [344] [Effective Data Pruning through Score Extrapolation](https://arxiv.org/abs/2506.09010)
*Sebastian Schmidt,Prasanga Dhungel,Christoffer Löffler,Björn Nieth,Stephan Günnemann,Leo Schwinn*

Main category: cs.LG

TL;DR: 提出了一种新的重要性分数外推框架，仅需少量数据训练即可预测整个数据集的样本重要性，解决了现有数据剪枝技术需完整训练的高成本问题。


<details>
  <summary>Details</summary>
Motivation: 大规模数据集训练机器学习模型计算成本高，现有剪枝技术需完整训练，效率低。

Method: 引入重要性分数外推框架，基于少量数据训练，使用k近邻和图神经网络预测样本重要性。

Result: 在多种数据集和训练范式下验证了方法的有效性，适用于动态不确定性和TDDS等剪枝方法。

Conclusion: 分数外推是扩展昂贵分数计算任务（如剪枝、数据归因）的有前景方向。

Abstract: Training advanced machine learning models demands massive datasets, resulting
in prohibitive computational costs. To address this challenge, data pruning
techniques identify and remove redundant training samples while preserving
model performance. Yet, existing pruning techniques predominantly require a
full initial training pass to identify removable samples, negating any
efficiency benefits for single training runs. To overcome this limitation, we
introduce a novel importance score extrapolation framework that requires
training on only a small subset of data. We present two initial approaches in
this framework - k-nearest neighbors and graph neural networks - to accurately
predict sample importance for the entire dataset using patterns learned from
this minimal subset. We demonstrate the effectiveness of our approach for 2
state-of-the-art pruning methods (Dynamic Uncertainty and TDDS), 4 different
datasets (CIFAR-10, CIFAR-100, Places-365, and ImageNet), and 3 training
paradigms (supervised, unsupervised, and adversarial). Our results indicate
that score extrapolation is a promising direction to scale expensive score
calculation methods, such as pruning, data attribution, or other tasks.

</details>


### [345] [SPEED-RL: Faster Training of Reasoning Models via Online Curriculum Learning](https://arxiv.org/abs/2506.09016)
*Ruiqi Zhang,Daman Arora,Song Mei,Andrea Zanette*

Main category: cs.LG

TL;DR: SPEED是一种自适应在线RL课程，通过选择性选择中等难度训练示例，显著提升学习效率，实现2x至6x的加速训练。


<details>
  <summary>Details</summary>
Motivation: 传统RL训练中，均匀采样提示效率低下，导致计算成本高昂。

Method: 提出SPEED方法，选择性选择中等难度提示，优化梯度估计器的信噪比。

Result: 实验显示训练速度提升2x至6x，且无需手动调参。

Conclusion: SPEED高效且易于集成，显著提升RL训练效率。

Abstract: Training large language models with reinforcement learning (RL) against
verifiable rewards significantly enhances their reasoning abilities, yet
remains computationally expensive due to inefficient uniform prompt sampling.
We introduce Selective Prompting with Efficient Estimation of Difficulty
(SPEED), an adaptive online RL curriculum that selectively chooses training
examples of intermediate difficulty to maximize learning efficiency.
Theoretically, we establish that intermediate-difficulty prompts improve the
gradient estimator's signal-to-noise ratio, accelerating convergence.
Empirically, our efficient implementation leads to 2x to 6x faster training
without degrading accuracy, requires no manual tuning, and integrates
seamlessly into standard RL algorithms.

</details>


### [346] [Edit Flows: Flow Matching with Edit Operations](https://arxiv.org/abs/2506.09018)
*Marton Havasi,Brian Karrer,Itai Gat,Ricky T. Q. Chen*

Main category: cs.LG

TL;DR: Edit Flows是一种非自回归模型，通过编辑操作（插入、删除和替换）在序列空间上定义离散流，克服了传统非自回归模型的局限性。


<details>
  <summary>Details</summary>
Motivation: 传统非自回归模型在处理变长序列时表现不佳，通常需要强制的令牌级结构。Edit Flows旨在通过灵活的编辑操作更贴近序列数据的结构。

Method: Edit Flows在序列空间上定义了一个基于连续时间马尔可夫链的离散流，利用插入、删除和替换操作进行生成。训练方法通过扩展状态空间和辅助变量实现高效学习。

Result: 实验结果表明，Edit Flows在图像描述任务上优于自回归和掩码模型，在文本和代码生成任务上显著优于掩码构造方法。

Conclusion: Edit Flows通过灵活的编辑操作和高效的训练方法，为非自回归模型提供了一种更贴近序列数据结构的生成方式。

Abstract: Autoregressive generative models naturally generate variable-length
sequences, while non-autoregressive models struggle, often imposing rigid,
token-wise structures. We propose Edit Flows, a non-autoregressive model that
overcomes these limitations by defining a discrete flow over sequences through
edit operations-insertions, deletions, and substitutions. By modeling these
operations within a Continuous-time Markov Chain over the sequence space, Edit
Flows enable flexible, position-relative generation that aligns more closely
with the structure of sequence data. Our training method leverages an expanded
state space with auxiliary variables, making the learning process efficient and
tractable. Empirical results show that Edit Flows outperforms both
autoregressive and mask models on image captioning and significantly
outperforms the mask construction in text and code generation.

</details>


### [347] [FZOO: Fast Zeroth-Order Optimizer for Fine-Tuning Large Language Models towards Adam-Scale Speed](https://arxiv.org/abs/2506.09034)
*Sizhe Dang,Yangyang Guo,Yanjun Zhao,Haishan Ye,Xiaodong Zheng,Guang Dai,Ivor Tsang*

Main category: cs.LG

TL;DR: FZOO是一种快速零阶优化器，通过批量单边估计和自适应步长减少收敛所需的前向传递次数，在速度和内存效率上优于现有方法如MeZO。


<details>
  <summary>Details</summary>
Motivation: 解决大型语言模型微调中GPU内存瓶颈问题，同时提升零阶优化器的收敛速度。

Method: FZOO采用批量单边估计和自适应步长，结合Rademacher随机向量扰动和CUDA并行处理。

Result: FZOO在11个任务中平均比MeZO准确率提升3%，前向传递次数减少3倍，RoBERTa-large上准确率提升5.6%，前向传递减少18倍。

Conclusion: FZOO实现了单GPU高效微调，为内存高效预训练提供了方向。

Abstract: Fine-tuning large language models (LLMs) often faces GPU memory bottlenecks:
the backward pass of first-order optimizers like Adam increases memory usage to
more than 10 times the inference level (e.g., 633 GB for OPT-30B). Zeroth-order
(ZO) optimizers avoid this cost by estimating gradients only from forward
passes, yet existing methods like MeZO usually require many more steps to
converge. Can this trade-off between speed and memory in ZO be fundamentally
improved? Normalized-SGD demonstrates strong empirical performance with greater
memory efficiency than Adam. In light of this, we introduce FZOO, a Fast
Zeroth-Order Optimizer toward Adam-Scale Speed. FZOO reduces the total forward
passes needed for convergence by employing batched one-sided estimates that
adapt step sizes based on the standard deviation of batch losses. It also
accelerates per-batch computation through the use of Rademacher random vector
perturbations coupled with CUDA's parallel processing. Extensive experiments on
diverse models, including RoBERTa-large, OPT (350M-66B), Phi-2, and Llama3,
across 11 tasks validate FZOO's effectiveness. On average, FZOO outperforms
MeZO by 3 percent in accuracy while requiring 3 times fewer forward passes. For
RoBERTa-large, FZOO achieves average improvements of 5.6 percent in accuracy
and an 18 times reduction in forward passes compared to MeZO, achieving
convergence speeds comparable to Adam. We also provide theoretical analysis
proving FZOO's formal equivalence to a normalized-SGD update rule and its
convergence guarantees. FZOO integrates smoothly into PEFT techniques, enabling
even larger memory savings. Overall, our results make single-GPU, high-speed,
full-parameter fine-tuning practical and point toward future work on
memory-efficient pre-training.

</details>


### [348] [The Decoupled Risk Landscape in Performative Prediction](https://arxiv.org/abs/2506.09044)
*Javier Sanguino,Thomas Kehrenberg,Jose A. Lozano,Novi Quadrianto*

Main category: cs.LG

TL;DR: 论文提出了一种可视化方法，用于补充Performative Prediction的理论研究，并引入扩展的Performative Prediction场景。


<details>
  <summary>Details</summary>
Motivation: 现有文献主要从理论角度研究Performative Prediction，缺乏实践视角。可视化损失景观可以提供实用见解。

Method: 1. 提出解耦风险可视化方法，分析模型参数和数据参数的风险景观；2. 引入扩展的Performative Prediction场景，反映现实中的不完全模型访问。

Result: 通过可视化方法，揭示了兴趣点的新特性，并分析了现有算法在更现实条件下的表现。

Conclusion: 可视化方法为Performative Prediction提供了实用补充，扩展场景更贴近现实应用。

Abstract: Performative Prediction addresses scenarios where deploying a model induces a
distribution shift in the input data, such as individuals modifying their
features and reapplying for a bank loan after rejection. Literature has had a
theoretical perspective giving mathematical guarantees for convergence (either
to the stable or optimal point). We believe that visualization of the loss
landscape can complement this theoretical advances with practical insights.
Therefore, (1) we introduce a simple decoupled risk visualization method
inspired in the two-step process that performative prediction is. Our approach
visualizes the risk landscape with respect to two parameter vectors: model
parameters and data parameters. We use this method to propose new properties of
the interest points, to examine how existing algorithms traverse the risk
landscape and perform under more realistic conditions, including strategic
classification with non-linear models. (2) Building on this decoupled risk
visualization, we introduce a novel setting - extended Performative Prediction
- which captures scenarios where the distribution reacts to a model different
from the decision-making one, reflecting the reality that agents often lack
full access to the deployed model.

</details>


### [349] [Understanding Task Vectors in In-Context Learning: Emergence, Functionality, and Limitations](https://arxiv.org/abs/2506.09048)
*Yuxin Dong,Jiachen Jiang,Zhihui Zhu,Xia Ning*

Main category: cs.LG

TL;DR: 论文提出任务向量通过线性组合原始演示形成单一可重用表示，并验证其在高秩映射中的局限性。


<details>
  <summary>Details</summary>
Motivation: 探索任务向量在上下文学习中的工作原理及其局限性。

Method: 通过理论分析和实验验证任务向量的线性组合猜想，包括损失景观分析和参数可视化。

Result: 任务向量在低秩映射中有效，但在高秩映射中失败，可通过注入多个任务向量增强效果。

Conclusion: 研究深化了对任务向量和Transformer模型上下文学习机制的理解。

Abstract: Task vectors offer a compelling mechanism for accelerating inference in
in-context learning (ICL) by distilling task-specific information into a
single, reusable representation. Despite their empirical success, the
underlying principles governing their emergence and functionality remain
unclear. This work proposes the Linear Combination Conjecture, positing that
task vectors act as single in-context demonstrations formed through linear
combinations of the original ones. We provide both theoretical and empirical
support for this conjecture. First, we show that task vectors naturally emerge
in linear transformers trained on triplet-formatted prompts through loss
landscape analysis. Next, we predict the failure of task vectors on
representing high-rank mappings and confirm this on practical LLMs. Our
findings are further validated through saliency analyses and parameter
visualization, suggesting an enhancement of task vectors by injecting multiple
ones into few-shot prompts. Together, our results advance the understanding of
task vectors and shed light on the mechanisms underlying ICL in
transformer-based models.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [350] [MasHost Builds It All: Autonomous Multi-Agent System Directed by Reinforcement Learning](https://arxiv.org/abs/2506.08507)
*Kuo Yang,Xingjie Yang,Linhui Yu,Qing Xu,Yan Fang,Xu Wang,Zhengyang Zhou,Yang Wang*

Main category: cs.MA

TL;DR: MasHost是一个基于强化学习的框架，用于自主设计多智能体系统，通过图搜索和概率采样机制优化智能体角色和交互，引入组件合理性作为新设计原则。


<details>
  <summary>Details</summary>
Motivation: 现有多智能体系统构建方法依赖人工设计或启发式规则，限制了自主性并引入人为偏见。

Method: 将多智能体系统构建建模为图搜索问题，采用概率采样机制联合采样智能体角色和交互，提出分层相对策略优化（HRPO）实现多目标优化。

Result: 在六个基准测试中，MasHost表现优于大多数基线方法，验证了其有效性、效率和结构合理性。

Conclusion: MasHost是首个基于强化学习的自主多智能体系统图构建框架，显著提升了系统的自主性和性能。

Abstract: Large Language Model (LLM)-driven Multi-agent systems (Mas) have recently
emerged as a powerful paradigm for tackling complex real-world tasks. However,
existing Mas construction methods typically rely on manually crafted
interaction mechanisms or heuristic rules, introducing human biases and
constraining the autonomous ability. Even with recent advances in adaptive Mas
construction, existing systems largely remain within the paradigm of
semi-autonomous patterns. In this work, we propose MasHost, a Reinforcement
Learning (RL)-based framework for autonomous and query-adaptive Mas design. By
formulating Mas construction as a graph search problem, our proposed MasHost
jointly samples agent roles and their interactions through a unified
probabilistic sampling mechanism. Beyond the accuracy and efficiency objectives
pursued in prior works, we introduce component rationality as an additional and
novel design principle in Mas. To achieve this multi-objective optimization, we
propose Hierarchical Relative Policy Optimization (HRPO), a novel RL strategy
that collaboratively integrates group-relative advantages and action-wise
rewards. To our knowledge, our proposed MasHost is the first RL-driven
framework for autonomous Mas graph construction. Extensive experiments on six
benchmarks demonstrate that MasHost consistently outperforms most competitive
baselines, validating its effectiveness, efficiency, and structure rationality.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [351] [AI Magnetic Levitation (Maglev) Conveyor for Automated Assembly Production](https://arxiv.org/abs/2506.08039)
*Ray Wai Man Kong*

Main category: cs.RO

TL;DR: AI Maglev Conveyor系统结合磁悬浮技术和AI，提升制造效率，降低成本，支持灵活生产。


<details>
  <summary>Details</summary>
Motivation: 现代制造业需要高效、快速和精确的解决方案，传统系统存在摩擦和维护成本高的问题。

Method: 结合磁悬浮技术和AI，设计电磁控制器和多移动器系统，实现实时监控和自适应控制。

Result: 系统减少摩擦和停机时间，提高吞吐量，适应多样化生产需求，降低成本。

Conclusion: AI Maglev Conveyor是高效、灵活且经济的制造解决方案，适用于多种工业场景。

Abstract: Efficiency, speed, and precision are essential in modern manufacturing. AI
Maglev Conveyor system, combining magnetic levitation (maglev) technology with
artificial intelligence (AI), revolutionizes automated production processes.
This system reduces maintenance costs and downtime by eliminating friction,
enhancing operational efficiency. It transports goods swiftly with minimal
energy consumption, optimizing resource use and supporting sustainability. AI
integration enables real-time monitoring and adaptive control, allowing
businesses to respond to production demand fluctuations and streamline supply
chain operations.
  The AI Maglev Conveyor offers smooth, silent operation, accommodating diverse
product types and sizes for flexible manufacturing without extensive
reconfiguration. AI algorithms optimize routing, reduce cycle times, and
improve throughput, creating an agile production line adaptable to market
changes.
  This applied research paper introduces the Maglev Conveyor system, featuring
an electromagnetic controller and multiple movers to enhance automation. It
offers cost savings as an alternative to setups using six-axis robots or linear
motors, with precise adjustments for robotic arm loading. Operating at high
speeds minimizes treatment time for delicate components while maintaining
precision. Its adaptable design accommodates various materials, facilitating
integration of processing stations alongside electronic product assembly.
Positioned between linear-axis and robotic systems in cost, the Maglev Conveyor
is ideal for flat parts requiring minimal travel, transforming production
efficiency across industries. It explores its technical advantages,
flexibility, cost reductions, and overall benefits.

</details>


### [352] [UAVs Meet Agentic AI: A Multidomain Survey of Autonomous Aerial Intelligence and Agentic UAVs](https://arxiv.org/abs/2506.08045)
*Ranjan Sapkota,Konstantinos I. Roumeliotis,Manoj Karkee*

Main category: cs.RO

TL;DR: Agentic UAVs integrate advanced AI for autonomous, adaptive operations in complex environments, surpassing traditional UAVs. This paper explores their architecture, applications, challenges, and future directions.


<details>
  <summary>Details</summary>
Motivation: To provide a comprehensive understanding of Agentic UAVs, highlighting their advancements over traditional UAVs and their societal impact.

Method: A detailed analysis of architectural components, enabling technologies, and comparative advancements in autonomy, AI, and mission flexibility.

Result: Identifies seven high-impact application domains and key challenges, proposing solutions and a future roadmap for Agentic UAVs.

Conclusion: Establishes a foundational framework for the development, deployment, and governance of Agentic UAVs across diverse domains.

Abstract: Agentic UAVs represent a new frontier in autonomous aerial intelligence,
integrating perception, decision-making, memory, and collaborative planning to
operate adaptively in complex, real-world environments. Driven by recent
advances in Agentic AI, these systems surpass traditional UAVs by exhibiting
goal-driven behavior, contextual reasoning, and interactive autonomy. We
provide a comprehensive foundation for understanding the architectural
components and enabling technologies that distinguish Agentic UAVs from
traditional autonomous UAVs. Furthermore, a detailed comparative analysis
highlights advancements in autonomy with AI agents, learning, and mission
flexibility. This study explores seven high-impact application domains
precision agriculture, construction & mining, disaster response, environmental
monitoring, infrastructure inspection, logistics, security, and wildlife
conservation, illustrating the broad societal value of agentic aerial
intelligence. Furthermore, we identify key challenges in technical constraints,
regulatory limitations, and data-model reliability, and we present emerging
solutions across hardware innovation, learning architectures, and human-AI
interaction. Finally, a future roadmap is proposed, outlining pathways toward
self-evolving aerial ecosystems, system-level collaboration, and sustainable,
equitable deployments. This survey establishes a foundational framework for the
future development, deployment, and governance of agentic aerial systems
(Agentic UAVs) across diverse societal and industrial domains.

</details>


### [353] [Adaptive Per-Tree Canopy Volume Estimation Using Mobile LiDAR in Structured and Unstructured Orchards](https://arxiv.org/abs/2506.08061)
*Ali Abedi,Fernando Cladera,Mohsen Farajijalal,Reza Ehsani*

Main category: cs.RO

TL;DR: 提出了一种基于移动LiDAR数据的实时系统，用于估计单棵树冠体积，适用于不同结构的果园。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖静态扫描或假设果园结构均匀，无法适应多样化的果园环境。

Method: 结合LiDAR-惯性里程计、自适应分割和几何重建的集成流程，采用DBSCAN和谱聚类的混合聚类策略。

Result: 在两种商业果园中测试，成功率为93%（开心果）和80%（杏仁），与无人机数据一致。

Conclusion: 该系统为结构多样的果园提供了可扩展、非侵入性的树木监测方案。

Abstract: We present a real-time system for per-tree canopy volume estimation using
mobile LiDAR data collected during routine robotic navigation. Unlike prior
approaches that rely on static scans or assume uniform orchard structures, our
method adapts to varying field geometries via an integrated pipeline of
LiDAR-inertial odometry, adaptive segmentation, and geometric reconstruction.
We evaluate the system across two commercial orchards, one pistachio orchard
with regular spacing and one almond orchard with dense, overlapping crowns. A
hybrid clustering strategy combining DBSCAN and spectral clustering enables
robust per-tree segmentation, achieving 93% success in pistachio and 80% in
almond, with strong agreement to drone derived canopy volume estimates. This
work advances scalable, non-intrusive tree monitoring for structurally diverse
orchard environments.

</details>


### [354] [Ego-centric Learning of Communicative World Models for Autonomous Driving](https://arxiv.org/abs/2506.08149)
*Hang Wang,Dechen Gao,Junshan Zhang*

Main category: cs.RO

TL;DR: 论文提出CALL方法，通过生成式AI和潜在表示解决多智能体强化学习中的部分可观测性和非平稳性问题，实现轻量级信息共享。


<details>
  <summary>Details</summary>
Motivation: 多智能体强化学习在复杂高维环境中面临部分可观测性和非平稳性问题，传统信息共享方法存在通信开销和可扩展性问题。

Method: CALL方法结合生成式AI的世界模型和潜在表示，实现轻量级信息共享和自中心学习，提升预测和规划能力。

Result: 实验表明，CALL在CARLA平台的局部轨迹规划任务中显著提升性能。

Conclusion: CALL通过轻量级信息共享和潜在表示优化，有效解决了多智能体强化学习中的关键挑战。

Abstract: We study multi-agent reinforcement learning (MARL) for tasks in complex
high-dimensional environments, such as autonomous driving. MARL is known to
suffer from the \textit{partial observability} and \textit{non-stationarity}
issues. To tackle these challenges, information sharing is often employed,
which however faces major hurdles in practice, including overwhelming
communication overhead and scalability concerns. By making use of generative AI
embodied in world model together with its latent representation, we develop
{\it CALL}, \underline{C}ommunic\underline{a}tive Wor\underline{l}d
Mode\underline{l}, for MARL, where 1) each agent first learns its world model
that encodes its state and intention into low-dimensional latent representation
with smaller memory footprint, which can be shared with other agents of
interest via lightweight communication; and 2) each agent carries out
ego-centric learning while exploiting lightweight information sharing to enrich
her world model, and then exploits its generalization capacity to improve
prediction for better planning. We characterize the gain on the prediction
accuracy from the information sharing and its impact on performance gap.
Extensive experiments are carried out on the challenging local trajectory
planning tasks in the CARLA platform to demonstrate the performance gains of
using \textit{CALL}.

</details>


### [355] [TensorTouch: Calibration of Tactile Sensors for High Resolution Stress Tensor and Deformation for Dexterous Manipulation](https://arxiv.org/abs/2506.08291)
*Won Kyung Do,Matthew Strong,Aiden Swann,Boshu Lei,Monroe Kennedy III*

Main category: cs.RO

TL;DR: TensorTouch结合有限元分析和深度学习，从光学触觉传感器中提取全面的接触信息，提升机器人复杂操作能力。


<details>
  <summary>Details</summary>
Motivation: 多接触点的高级灵巧操作对机器人系统仍具挑战性，需要高分辨率触觉传感。

Method: TensorTouch框架整合有限元分析和深度学习，提取应力张量、变形场和力分布。

Result: 实验验证显示90%成功率，支持亚毫米级位置精度和精确力估计。

Conclusion: TensorTouch为机器人系统提供了前所未有的复杂操作能力。

Abstract: Advanced dexterous manipulation involving multiple simultaneous contacts
across different surfaces, like pinching coins from ground or manipulating
intertwined objects, remains challenging for robotic systems. Such tasks exceed
the capabilities of vision and proprioception alone, requiring high-resolution
tactile sensing with calibrated physical metrics. Raw optical tactile sensor
images, while information-rich, lack interpretability and cross-sensor
transferability, limiting their real-world utility. TensorTouch addresses this
challenge by integrating finite element analysis with deep learning to extract
comprehensive contact information from optical tactile sensors, including
stress tensors, deformation fields, and force distributions at pixel-level
resolution. The TensorTouch framework achieves sub-millimeter position accuracy
and precise force estimation while supporting large sensor deformations crucial
for manipulating soft objects. Experimental validation demonstrates 90% success
in selectively grasping one of two strings based on detected motion, enabling
new contact-rich manipulation capabilities previously inaccessible to robotic
systems.

</details>


### [356] [HiBerNAC: Hierarchical Brain-emulated Robotic Neural Agent Collective for Disentangling Complex Manipulation](https://arxiv.org/abs/2506.08296)
*Hongjun Wu,Heng Zhang,Pengsong Zhang,Jin Wang,Cong Wang*

Main category: cs.RO

TL;DR: HiBerNAC是一种受神经科学启发的分层多智能体框架，用于解决复杂机器人操作任务中的挑战，显著提升了任务完成效率和成功率。


<details>
  <summary>Details</summary>
Motivation: 解决复杂机器人操作任务中存在的持久上下文记忆、多智能体协调和动态长期规划等挑战。

Method: 结合多模态VLA规划与神经启发的反射机制和多智能体协作，设计分层决策框架。

Result: 相比现有VLA模型，HiBerNAC将长期任务完成时间减少23%，并在多路径任务中实现12-31%的成功率。

Conclusion: HiBerNAC为生物认知与机器人学习机制的融合提供了初步证据，展示了神经启发方法在复杂任务中的潜力。

Abstract: Recent advances in multimodal vision-language-action (VLA) models have
revolutionized traditional robot learning, enabling systems to interpret
vision, language, and action in unified frameworks for complex task planning.
However, mastering complex manipulation tasks remains an open challenge,
constrained by limitations in persistent contextual memory, multi-agent
coordination under uncertainty, and dynamic long-horizon planning across
variable sequences. To address this challenge, we propose \textbf{HiBerNAC}, a
\textbf{Hi}erarchical \textbf{B}rain-\textbf{e}mulated \textbf{r}obotic
\textbf{N}eural \textbf{A}gent \textbf{C}ollective, inspired by breakthroughs
in neuroscience, particularly in neural circuit mechanisms and hierarchical
decision-making. Our framework combines: (1) multimodal VLA planning and
reasoning with (2) neuro-inspired reflection and multi-agent mechanisms,
specifically designed for complex robotic manipulation tasks. By leveraging
neuro-inspired functional modules with decentralized multi-agent collaboration,
our approach enables robust and enhanced real-time execution of complex
manipulation tasks. In addition, the agentic system exhibits scalable
collective intelligence via dynamic agent specialization, adapting its
coordination strategy to variable task horizons and complexity. Through
extensive experiments on complex manipulation tasks compared with
state-of-the-art VLA models, we demonstrate that \textbf{HiBerNAC} reduces
average long-horizon task completion time by 23\%, and achieves non-zero
success rates (12\textendash 31\%) on multi-path tasks where prior
state-of-the-art VLA models consistently fail. These results provide indicative
evidence for bridging biological cognition and robotic learning mechanisms.

</details>


### [357] [Re4MPC: Reactive Nonlinear MPC for Multi-model Motion Planning via Deep Reinforcement Learning](https://arxiv.org/abs/2506.08344)
*Neşet Ünver Akmandor,Sarvesh Prajapati,Mark Zolotas,Taşkın Padır*

Main category: cs.RO

TL;DR: 提出了一种名为Re4MPC的多模型运动规划方法，通过结合非线性模型预测控制（NMPC）和深度强化学习（DRL），高效生成机器人轨迹。


<details>
  <summary>Details</summary>
Motivation: 传统的高自由度机器人运动规划方法计算成本高，难以适用于实际场景。

Method: Re4MPC通过动态选择NMPC问题的模型、成本和约束条件，结合DRL框架学习决策策略。

Result: 实验表明，Re4MPC比传统NMPC方法计算效率更高，且达到更高的末端执行器目标成功率。

Conclusion: Re4MPC为高自由度机器人提供了一种高效的运动规划解决方案。

Abstract: Traditional motion planning methods for robots with many degrees-of-freedom,
such as mobile manipulators, are often computationally prohibitive for
real-world settings. In this paper, we propose a novel multi-model motion
planning pipeline, termed Re4MPC, which computes trajectories using Nonlinear
Model Predictive Control (NMPC). Re4MPC generates trajectories in a
computationally efficient manner by reactively selecting the model, cost, and
constraints of the NMPC problem depending on the complexity of the task and
robot state. The policy for this reactive decision-making is learned via a Deep
Reinforcement Learning (DRL) framework. We introduce a mathematical formulation
to integrate NMPC into this DRL framework. To validate our methodology and
design choices, we evaluate DRL training and test outcomes in a physics-based
simulation involving a mobile manipulator. Experimental results demonstrate
that Re4MPC is more computationally efficient and achieves higher success rates
in reaching end-effector goals than the NMPC baseline, which computes
whole-body trajectories without our learning mechanism.

</details>


### [358] [Periodic Bipedal Gait Learning Using Reward Composition Based on a Novel Gait Planner for Humanoid Robots](https://arxiv.org/abs/2506.08416)
*Bolin Li,Linwei Sun,Xuecong Huang,Yuzhi Jiang,Lijun Zhu*

Main category: cs.RO

TL;DR: 提出了一种基于奖励组合的周期性双足步态学习方法，结合实时步态规划器，通过分解和近似模型简化规划，并通过奖励组合提升学习效率和性能。


<details>
  <summary>Details</summary>
Motivation: 解决人形机器人周期性双足步态学习中的效率和性能问题。

Method: 1. 引入动态步态规划器，将3D模型分解为2D混合倒立摆模型；2. 设计三种奖励函数，组合使用以优化强化学习框架。

Result: 减少了学习时间并提升了运动性能，通过实例和性能对比验证了方法的有效性。

Conclusion: 该方法通过奖励组合和实时规划器，显著提升了双足步态学习的效率和性能。

Abstract: This paper presents a periodic bipedal gait learning method using reward
composition, integrated with a real-time gait planner for humanoid robots.
First, we introduce a novel gait planner that incorporates dynamics to design
the desired joint trajectory. In the gait design process, the 3D robot model is
decoupled into two 2D models, which are then approximated as hybrid inverted
pendulums (H-LIP) for trajectory planning. The gait planner operates in
parallel in real time within the robot's learning environment. Second, based on
this gait planner, we design three effective reward functions within a
reinforcement learning framework, forming a reward composition to achieve
periodic bipedal gait. This reward composition reduces the robot's learning
time and enhances locomotion performance. Finally, a gait design example and
performance comparison are presented to demonstrate the effectiveness of the
proposed method.

</details>


### [359] [Attention-based Learning for 3D Informative Path Planning](https://arxiv.org/abs/2506.08434)
*Rui Zhao,Xingjian Zhang,Yuhong Cao,Yizhuo Wang,Guillaume Sartoretti*

Main category: cs.RO

TL;DR: 提出了一种基于注意力的深度强化学习方法，用于解决3D空间中的自适应信息路径规划问题，通过动态调整无人机位置以优化感知覆盖和精度。


<details>
  <summary>Details</summary>
Motivation: 解决自适应信息路径规划问题，使无人机能够在时间/距离约束下最大化信息收集，并动态调整路径以适应新获取的传感器数据。

Method: 利用注意力机制捕捉全局空间依赖关系，构建上下文信念表示，指导无人机优化短期和长期搜索目标。

Result: 在受限预算下显著降低环境不确定性，有效平衡探索与利用，且模型在不同规模环境中表现良好。

Conclusion: 该方法在自适应信息路径规划中表现出色，具有广泛的实际应用潜力。

Abstract: In this work, we propose an attention-based deep reinforcement learning
approach to address the adaptive informative path planning (IPP) problem in 3D
space, where an aerial robot equipped with a downward-facing sensor must
dynamically adjust its 3D position to balance sensing footprint and accuracy,
and finally obtain a high-quality belief of an underlying field of interest
over a given domain (e.g., presence of specific plants, hazardous gas,
geological structures, etc.). In adaptive IPP tasks, the agent is tasked with
maximizing information collected under time/distance constraints, continuously
adapting its path based on newly acquired sensor data. To this end, we leverage
attention mechanisms for their strong ability to capture global spatial
dependencies across large action spaces, allowing the agent to learn an
implicit estimation of environmental transitions. Our model builds a contextual
belief representation over the entire domain, guiding sequential movement
decisions that optimize both short- and long-term search objectives.
Comparative evaluations against state-of-the-art planners demonstrate that our
approach significantly reduces environmental uncertainty within constrained
budgets, thus allowing the agent to effectively balance exploration and
exploitation. We further show our model generalizes well to environments of
varying sizes, highlighting its potential for many real-world applications.

</details>


### [360] [TGRPO :Fine-tuning Vision-Language-Action Model via Trajectory-wise Group Relative Policy Optimization](https://arxiv.org/abs/2506.08440)
*Zengjue Chen,Runliang Niu,He Kong,Qi Wang*

Main category: cs.RO

TL;DR: 论文提出了一种名为TGRPO的方法，通过结合步级和轨迹级优势信号，改进了GRPO的组级优势估计，适用于VLA模型的在线强化学习训练。


<details>
  <summary>Details</summary>
Motivation: 现有的VLA模型在新环境中仍需任务特定的微调，且依赖静态轨迹数据集，无法利用实时交互和反馈。强化学习提供了一种闭环交互的替代方案。

Method: 提出TGRPO方法，融合步级和轨迹级优势信号，改进GRPO的组级优势估计。

Result: 在十个操作任务上，TGRPO表现优于基线方法，生成更稳健和高效的政策。

Conclusion: TGRPO为VLA模型的在线强化学习训练提供了一种有效方法，展示了在多场景中的优越性能。

Abstract: Recent advances in Vision-Language-Action (VLA) model have demonstrated
strong generalization capabilities across diverse scenes, tasks, and robotic
platforms when pretrained at large-scale datasets. However, these models still
require task-specific fine-tuning in novel environments, a process that relies
almost exclusively on supervised fine-tuning (SFT) using static trajectory
datasets. Such approaches neither allow robot to interact with environment nor
do they leverage feedback from live execution. Also, their success is
critically dependent on the size and quality of the collected trajectories.
Reinforcement learning (RL) offers a promising alternative by enabling
closed-loop interaction and aligning learned policies directly with task
objectives. In this work, we draw inspiration from the ideas of GRPO and
propose the Trajectory-wise Group Relative Policy Optimization (TGRPO) method.
By fusing step-level and trajectory-level advantage signals, this method
improves GRPO's group-level advantage estimation, thereby making the algorithm
more suitable for online reinforcement learning training of VLA. Experimental
results on ten manipulation tasks from the libero-object benchmark demonstrate
that TGRPO consistently outperforms various baseline methods, capable of
generating more robust and efficient policies across multiple tested scenarios.
Our source codes are available at: https://github.com/hahans/TGRPO

</details>


### [361] [Diffusion Models for Safety Validation of Autonomous Driving Systems](https://arxiv.org/abs/2506.08459)
*Juanran Wang,Marc R. Schlichting,Harrison Delecki,Mykel J. Kochenderfer*

Main category: cs.RO

TL;DR: 使用去噪扩散模型生成自动驾驶系统的潜在故障案例，无需外部数据集，适用于交通路口安全验证。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶系统的安全验证成本高、风险大，且潜在故障罕见多样。

Method: 训练去噪扩散模型，从初始交通状态生成潜在故障案例。

Result: 在四路交叉口实验中，模型能生成多样化且真实的故障样本。

Conclusion: 该模型资源需求低，无需系统先验知识，适用于交通路口安全验证。

Abstract: Safety validation of autonomous driving systems is extremely challenging due
to the high risks and costs of real-world testing as well as the rarity and
diversity of potential failures. To address these challenges, we train a
denoising diffusion model to generate potential failure cases of an autonomous
vehicle given any initial traffic state. Experiments on a four-way intersection
problem show that in a variety of scenarios, the diffusion model can generate
realistic failure samples while capturing a wide variety of potential failures.
Our model does not require any external training dataset, can perform training
and inference with modest computing resources, and does not assume any prior
knowledge of the system under test, with applicability to safety validation for
traffic intersections.

</details>


### [362] [Noise Analysis and Hierarchical Adaptive Body State Estimator For Biped Robot Walking With ESVC Foot](https://arxiv.org/abs/2506.08578)
*Boyang Chen,Xizhe Zang,Chao Song,Yue Zhang,Xuehe Zhang,Jie Zhao*

Main category: cs.RO

TL;DR: 本文提出了一种针对ESVC足机器人行走的分层自适应状态估计器，通过噪声分析和回归模型改进状态估计精度和收敛速度。


<details>
  <summary>Details</summary>
Motivation: ESVC足虽然提高了机器人行走的能效，但由于支撑腿倾斜导致接触模型误差放大，增加了状态估计的难度。

Method: 通过物理实验分析噪声影响，建立噪声-时间回归模型，并提出两阶段（预估计和后估计）的自适应状态估计器。

Result: 实验表明，该估计器比EKF和自适应EKF精度更高，且在噪声变化条件下收敛更快。

Conclusion: 提出的分层自适应状态估计器有效解决了ESVC足机器人行走中的状态估计问题。

Abstract: The ESVC(Ellipse-based Segmental Varying Curvature) foot, a robot foot design
inspired by the rollover shape of the human foot, significantly enhances the
energy efficiency of the robot walking gait. However, due to the tilt of the
supporting leg, the error of the contact model are amplified, making robot
state estimation more challenging. Therefore, this paper focuses on the noise
analysis and state estimation for robot walking with the ESVC foot. First,
through physical robot experiments, we investigate the effect of the ESVC foot
on robot measurement noise and process noise. and a noise-time regression model
using sliding window strategy is developed. Then, a hierarchical adaptive state
estimator for biped robots with the ESVC foot is proposed. The state estimator
consists of two stages: pre-estimation and post-estimation. In the
pre-estimation stage, a data fusion-based estimation is employed to process the
sensory data. During post-estimation, the acceleration of center of mass is
first estimated, and then the noise covariance matrices are adjusted based on
the regression model. Following that, an EKF(Extended Kalman Filter) based
approach is applied to estimate the centroid state during robot walking.
Physical experiments demonstrate that the proposed adaptive state estimator for
biped robot walking with the ESVC foot not only provides higher precision than
both EKF and Adaptive EKF, but also converges faster under varying noise
conditions.

</details>


### [363] [Deep Reinforcement Learning-Based Motion Planning and PDE Control for Flexible Manipulators](https://arxiv.org/abs/2506.08639)
*Amir Hossein Barjini,Seyed Adel Alizadeh Kolagar,Sadeq Yaqubi,Jouni Mattila*

Main category: cs.RO

TL;DR: 结合深度强化学习（DRL）和PDE非线性控制器的柔性机械臂运动规划与控制框架，优化轨迹以减少振动。


<details>
  <summary>Details</summary>
Motivation: 传统方法仅关注控制，忽略了期望轨迹对端点振动的影响，因此提出结合学习与模型的方法。

Method: 使用SAC算法训练DRL运动规划器生成优化轨迹，PDE非线性控制器计算扭矩并确保稳定性。

Result: 仿真和实验验证了该方法在振动抑制和跟踪精度上的优越性。

Conclusion: 结合学习与模型的方法可提升柔性机械臂的精度和稳定性。

Abstract: This article presents a motion planning and control framework for flexible
robotic manipulators, integrating deep reinforcement learning (DRL) with a
nonlinear partial differential equation (PDE) controller. Unlike conventional
approaches that focus solely on control, we demonstrate that the desired
trajectory significantly influences endpoint vibrations. To address this, a DRL
motion planner, trained using the soft actor-critic (SAC) algorithm, generates
optimized trajectories that inherently minimize vibrations. The PDE nonlinear
controller then computes the required torques to track the planned trajectory
while ensuring closed-loop stability using Lyapunov analysis. The proposed
methodology is validated through both simulations and real-world experiments,
demonstrating superior vibration suppression and tracking accuracy compared to
traditional methods. The results underscore the potential of combining
learning-based motion planning with model-based control for enhancing the
precision and stability of flexible robotic manipulators.

</details>


### [364] [ROS-related Robotic Systems Development with V-model-based Application of MeROS Metamodel](https://arxiv.org/abs/2506.08706)
*Tomasz Winiarski,Jan Kaniuka,Daniel Giełdowski,Jakub Ostrysz,Krystian Radlak,Dmytro Kushnir*

Main category: cs.RO

TL;DR: 本文提出了一种结合ROS和MBSE的结构化开发方法，通过MeROS和V模型提升机器人系统的可追溯性和一致性。


<details>
  <summary>Details</summary>
Motivation: 随着机器人系统复杂性增加，ROS和MBSE工具缺乏整合，需要一种结构化方法。

Method: 提出基于MeROS和V模型的领域特定方法，支持灵活性和复用性。

Result: 通过HeROS案例验证了方法的有效性，提升了系统一致性和可追溯性。

Conclusion: 该方法为ROS项目提供了工具无关的MBSE实践基础。

Abstract: As robotic systems grow increasingly complex, heterogeneous, and
safety-critical, the need for structured development methodologies becomes
paramount. Although frameworks like the Robot Operating System (ROS) and
Model-Based Systems Engineering (MBSE) offer foundational tools, they often
lack integration when used together. This paper addresses that gap by aligning
the widely recognized V-model development paradigm with the MeROS metamodel
SysML-based modeling language tailored for ROS-based systems.
  We propose a domain-specific methodology that bridges ROS-centric modelling
with systems engineering practices. Our approach formalises the structure,
behaviour, and validation processes of robotic systems using MeROS, while
extending it with a generalized, adaptable V-model compatible with both ROS and
ROS 2. Rather than prescribing a fixed procedure, the approach supports
project-specific flexibility and reuse, offering guidance across all stages of
development.
  The approach is validated through a comprehensive case study on HeROS, a
heterogeneous multi-robot platform comprising manipulators, mobile units, and
dynamic test environments. This example illustrates how the MeROS-compatible
V-model enhances traceability and system consistency while remaining accessible
and extensible for future adaptation. The work contributes a structured,
tool-agnostic foundation for developers and researchers seeking to apply MBSE
practices in ROS-based projects.

</details>


### [365] [PhyBlock: A Progressive Benchmark for Physical Understanding and Planning via 3D Block Assembly](https://arxiv.org/abs/2506.08708)
*Liang Ma,Jiajun Wen,Min Lin,Rongtao Xu,Xiwen Liang,Bingqian Lin,Jun Ma,Yongxin Wang,Ziming Wei,Haokun Lin,Mingfei Han,Meng Cao,Bokui Chen,Ivan Laptev,Xiaodan Liang*

Main category: cs.RO

TL;DR: PhyBlock是一个用于评估视觉语言模型在物理理解和规划方面能力的渐进式基准，通过3D积木组装任务和视觉问答任务测试模型的空间推理和物理理解能力。


<details>
  <summary>Details</summary>
Motivation: 现有视觉语言模型在结构化3D环境中的物理现象理解能力有限，需要一种新的评估方法来填补这一空白。

Method: PhyBench包含2600个任务（400个组装任务和2200个VQA任务），通过三个关键维度（部分完成、故障诊断和规划鲁棒性）评估21个先进模型。

Result: 实验表明，模型在高层次规划和推理能力上表现有限，任务复杂度增加时性能显著下降，空间方向和依赖关系推理是主要难点。

Conclusion: PhyBlock为视觉语言理解与真实世界物理问题解决提供了统一的测试平台，推动具身推理的发展。

Abstract: While vision-language models (VLMs) have demonstrated promising capabilities
in reasoning and planning for embodied agents, their ability to comprehend
physical phenomena, particularly within structured 3D environments, remains
severely limited. To close this gap, we introduce PhyBlock, a progressive
benchmark designed to assess VLMs on physical understanding and planning
through robotic 3D block assembly tasks. PhyBlock integrates a novel four-level
cognitive hierarchy assembly task alongside targeted Visual Question Answering
(VQA) samples, collectively aimed at evaluating progressive spatial reasoning
and fundamental physical comprehension, including object properties, spatial
relationships, and holistic scene understanding. PhyBlock includes 2600 block
tasks (400 assembly tasks, 2200 VQA tasks) and evaluates models across three
key dimensions: partial completion, failure diagnosis, and planning robustness.
We benchmark 21 state-of-the-art VLMs, highlighting their strengths and
limitations in physically grounded, multi-step planning. Our empirical findings
indicate that the performance of VLMs exhibits pronounced limitations in
high-level planning and reasoning capabilities, leading to a notable decline in
performance for the growing complexity of the tasks. Error analysis reveals
persistent difficulties in spatial orientation and dependency reasoning.
Surprisingly, chain-of-thought prompting offers minimal improvements,
suggesting spatial tasks heavily rely on intuitive model comprehension. We
position PhyBlock as a unified testbed to advance embodied reasoning, bridging
vision-language understanding and real-world physical problem-solving.

</details>


### [366] [Bayesian Inverse Physics for Neuro-Symbolic Robot Learning](https://arxiv.org/abs/2506.08756)
*Octavio Arriaga,Rebecca Adam,Melvin Laux,Lisa Gutzeit,Marco Ragni,Jan Peters,Frank Kirchner*

Main category: cs.RO

TL;DR: 该论文提出了一种结合数据驱动学习和结构化推理的混合神经符号架构，以解决深度学习在动态环境中效率低下的问题。


<details>
  <summary>Details</summary>
Motivation: 现实世界的机器人应用需要适应性强、可解释且数据高效的学习范式，但深度学习在未知和动态环境中表现不佳。

Method: 提出了一个框架，结合可微分物理、贝叶斯推理和元学习，将物理符号推理嵌入神经模型中。

Result: 这种混合架构有望使机器人超越训练数据泛化，适应新任务并持续扩展知识。

Conclusion: 论文认为混合神经符号架构是下一代自主系统的关键，并提供了研究路线图以推动其发展。

Abstract: Real-world robotic applications, from autonomous exploration to assistive
technologies, require adaptive, interpretable, and data-efficient learning
paradigms. While deep learning architectures and foundation models have driven
significant advances in diverse robotic applications, they remain limited in
their ability to operate efficiently and reliably in unknown and dynamic
environments. In this position paper, we critically assess these limitations
and introduce a conceptual framework for combining data-driven learning with
deliberate, structured reasoning. Specifically, we propose leveraging
differentiable physics for efficient world modeling, Bayesian inference for
uncertainty-aware decision-making, and meta-learning for rapid adaptation to
new tasks. By embedding physical symbolic reasoning within neural models,
robots could generalize beyond their training data, reason about novel
situations, and continuously expand their knowledge. We argue that such hybrid
neuro-symbolic architectures are essential for the next generation of
autonomous systems, and to this end, we provide a research roadmap to guide and
accelerate their development.

</details>


### [367] [Towards Biosignals-Free Autonomous Prosthetic Hand Control via Imitation Learning](https://arxiv.org/abs/2506.08795)
*Kaijie Shi,Wanglong Lu,Hanli Zhao,Vinicius Prado da Fonseca,Ting Zou,Xianta Jiang*

Main category: cs.RO

TL;DR: 开发了一种基于摄像头和模仿学习的全自主假手控制系统，无需用户生成肌电信号，自动抓取和释放物体。


<details>
  <summary>Details</summary>
Motivation: 传统假手控制方法依赖肌电信号，对用户身心负担大，需简化操作并减少心理压力。

Method: 通过摄像头采集环境数据，利用模仿学习训练假手控制模型，模仿人类动作实现自主操作。

Result: 模型仅需少量数据即可高成功率运行，并能泛化到不同用户和未见物体。

Conclusion: 该系统显著简化假手使用，降低心理负担，具有广泛应用潜力。

Abstract: Limb loss affects millions globally, impairing physical function and reducing
quality of life. Most traditional surface electromyographic (sEMG) and
semi-autonomous methods require users to generate myoelectric signals for each
control, imposing physically and mentally taxing demands. This study aims to
develop a fully autonomous control system that enables a prosthetic hand to
automatically grasp and release objects of various shapes using only a camera
attached to the wrist. By placing the hand near an object, the system will
automatically execute grasping actions with a proper grip force in response to
the hand's movements and the environment. To release the object being grasped,
just naturally place the object close to the table and the system will
automatically open the hand. Such a system would provide individuals with limb
loss with a very easy-to-use prosthetic control interface and greatly reduce
mental effort while using. To achieve this goal, we developed a teleoperation
system to collect human demonstration data for training the prosthetic hand
control model using imitation learning, which mimics the prosthetic hand
actions from human. Through training the model using only a few objects' data
from one single participant, we have shown that the imitation learning
algorithm can achieve high success rates, generalizing to more individuals and
unseen objects with a variation of weights. The demonstrations are available at
\href{https://sites.google.com/view/autonomous-prosthetic-hand}{https://sites.google.com/view/autonomous-prosthetic-hand}

</details>


### [368] [Human-Robot Teaming Field Deployments: A Comparison Between Verbal and Non-verbal Communication](https://arxiv.org/abs/2506.08890)
*Tauhid Tanjim,Promise Ekpo,Huajie Cao,Jonathan St. George,Kevin Ching,Hee Rin Lee,Angelique Taylor*

Main category: cs.RO

TL;DR: 研究比较了机器人急救车（RCC）的语音与非语音通讯方式对医护人员工作负荷和态度的影响，发现语音通讯显著降低心理需求和努力。


<details>
  <summary>Details</summary>
Motivation: 医护人员在急救中快速获取医疗用品面临挑战，机器人急救车可能通过有效通讯方式减少干扰并提升效率。

Method: 采用被试间实验，比较RCC的语音与非语音通讯与传统急救车在复苏场景中的效果。

Result: 语音通讯显著降低心理需求和努力，但机器人合作时挫败感略高。

Conclusion: 研究为高风险环境中人机协作提供了重要启示。

Abstract: Healthcare workers (HCWs) encounter challenges in hospitals, such as
retrieving medical supplies quickly from crash carts, which could potentially
result in medical errors and delays in patient care. Robotic crash carts (RCCs)
have shown promise in assisting healthcare teams during medical tasks through
guided object searches and task reminders. Limited exploration has been done to
determine what communication modalities are most effective and least disruptive
to patient care in real-world settings. To address this gap, we conducted a
between-subjects experiment comparing the RCC's verbal and non-verbal
communication of object search with a standard crash cart in resuscitation
scenarios to understand the impact of robot communication on workload and
attitudes toward using robots in the workplace. Our findings indicate that
verbal communication significantly reduced mental demand and effort compared to
visual cues and with a traditional crash cart. Although frustration levels were
slightly higher during collaborations with the robot compared to a traditional
cart, these research insights provide valuable implications for human-robot
teamwork in high-stakes environments.

</details>


### [369] [FreqPolicy: Efficient Flow-based Visuomotor Policy via Frequency Consistency](https://arxiv.org/abs/2506.08822)
*Yifei Su,Ning Liu,Dong Chen,Zhen Zhao,Kun Wu,Meng Li,Zhiyuan Xu,Zhengping Che,Jian Tang*

Main category: cs.RO

TL;DR: FreqPolicy通过频率一致性约束提升基于生成模型的视觉运动策略的效率，支持高质量的一步动作生成，并在仿真和实际机器人任务中验证了其优越性。


<details>
  <summary>Details</summary>
Motivation: 生成模型在机器人操作中因能建模多模态动作分布而被广泛应用，但其多步采样的高推理成本限制了实时性。现有方法借鉴图像生成的加速技术，但忽视了机器人动作的时间依赖性。

Method: 提出FreqPolicy，通过频率一致性约束流式视觉运动策略，确保动作模型有效捕捉时间结构，同时支持高效的一步动作生成。设计了自适应一致性损失以捕捉任务中的时间变化。

Result: 在3个仿真基准的53个任务中验证了FreqPolicy的优越性，集成到VLA模型后在40个任务中实现加速且性能无损，实际机器人场景中推理频率达93.5Hz。

Conclusion: FreqPolicy通过频率一致性约束有效解决了生成模型在机器人操作中的实时性问题，同时保持了动作生成的高质量和时间一致性。

Abstract: Generative modeling-based visuomotor policies have been widely adopted in
robotic manipulation attributed to their ability to model multimodal action
distributions. However, the high inference cost of multi-step sampling limits
their applicability in real-time robotic systems. To address this issue,
existing approaches accelerate the sampling process in generative
modeling-based visuomotor policies by adapting acceleration techniques
originally developed for image generation. Despite this progress, a major
distinction remains: image generation typically involves producing independent
samples without temporal dependencies, whereas robotic manipulation involves
generating time-series action trajectories that require continuity and temporal
coherence. To effectively exploit temporal information in robotic manipulation,
we propose FreqPolicy, a novel approach that first imposes frequency
consistency constraints on flow-based visuomotor policies. Our work enables the
action model to capture temporal structure effectively while supporting
efficient, high-quality one-step action generation. We introduce a frequency
consistency constraint that enforces alignment of frequency-domain action
features across different timesteps along the flow, thereby promoting
convergence of one-step action generation toward the target distribution. In
addition, we design an adaptive consistency loss to capture structural temporal
variations inherent in robotic manipulation tasks. We assess FreqPolicy on 53
tasks across 3 simulation benchmarks, proving its superiority over existing
one-step action generators. We further integrate FreqPolicy into the
vision-language-action (VLA) model and achieve acceleration without performance
degradation on the 40 tasks of Libero. Besides, we show efficiency and
effectiveness in real-world robotic scenarios with an inference frequency
93.5Hz. The code will be publicly available.

</details>


### [370] [MoRE: Mixture of Residual Experts for Humanoid Lifelike Gaits Learning on Complex Terrains](https://arxiv.org/abs/2506.08840)
*Dewei Wang,Xinmiao Wang,Xinzhe Liu,Jiyuan Shi,Yingnan Zhao,Chenjia Bai,Xuelong Li*

Main category: cs.RO

TL;DR: 提出了一种基于混合潜在残差专家和多判别器的RL框架，用于人形机器人在复杂地形中以可控的逼真步态行走。


<details>
  <summary>Details</summary>
Motivation: 现有方法仅适用于平坦地形且依赖本体感知，无法在复杂地形中实现人类步态。

Method: 采用两阶段训练流程，先通过深度相机训练策略适应复杂地形，再实现步态切换，并设计步态奖励调整行为。

Result: 仿真和实际实验显示，该框架在复杂地形中表现优异，并能无缝切换多种人类步态。

Conclusion: 该框架成功解决了复杂地形中逼真步态控制的挑战。

Abstract: Humanoid robots have demonstrated robust locomotion capabilities using
Reinforcement Learning (RL)-based approaches. Further, to obtain human-like
behaviors, existing methods integrate human motion-tracking or motion prior in
the RL framework. However, these methods are limited in flat terrains with
proprioception only, restricting their abilities to traverse challenging
terrains with human-like gaits. In this work, we propose a novel framework
using a mixture of latent residual experts with multi-discriminators to train
an RL policy, which is capable of traversing complex terrains in controllable
lifelike gaits with exteroception. Our two-stage training pipeline first
teaches the policy to traverse complex terrains using a depth camera, and then
enables gait-commanded switching between human-like gait patterns. We also
design gait rewards to adjust human-like behaviors like robot base height.
Simulation and real-world experiments demonstrate that our framework exhibits
exceptional performance in traversing complex terrains, and achieves seamless
transitions between multiple human-like gait patterns.

</details>


### [371] [Deploying SICNav in the Field: Safe and Interactive Crowd Navigation using MPC and Bilevel Optimization](https://arxiv.org/abs/2506.08851)
*Sepehr Samavi,Garvish Bhutani,Florian Shkurti,Angela P. Schoellig*

Main category: cs.RO

TL;DR: SICNav方法通过双层MPC框架将预测与规划结合，解决了机器人导航中忽视人机交互的问题。


<details>
  <summary>Details</summary>
Motivation: 传统方法将人类运动预测与机器人运动规划分离，忽略了人机闭环交互，导致机器人可能被困。

Method: 提出SICNav方法，采用双层MPC框架，将预测与规划整合为一个优化问题，显式建模多智能体交互。

Result: 在室内外环境中进行了近7公里的自主导航测试，系统运行初步分析显示有效性。

Conclusion: SICNav通过显式建模交互，提升了机器人在拥挤环境中的导航安全性和效率。

Abstract: Safe and efficient navigation in crowded environments remains a critical
challenge for robots that provide a variety of service tasks such as food
delivery or autonomous wheelchair mobility. Classical robot crowd navigation
methods decouple human motion prediction from robot motion planning, which
neglects the closed-loop interactions between humans and robots. This lack of a
model for human reactions to the robot plan (e.g. moving out of the way) can
cause the robot to get stuck. Our proposed Safe and Interactive Crowd
Navigation (SICNav) method is a bilevel Model Predictive Control (MPC)
framework that combines prediction and planning into one optimization problem,
explicitly modeling interactions among agents. In this paper, we present a
systems overview of the crowd navigation platform we use to deploy SICNav in
previously unseen indoor and outdoor environments. We provide a preliminary
analysis of the system's operation over the course of nearly 7 km of autonomous
navigation over two hours in both indoor and outdoor environments.

</details>


### [372] [Fast Estimation of Globally Optimal Independent Contact Regions for Robust Grasping and Manipulation](https://arxiv.org/abs/2506.08856)
*Jonathan P. King,Harnoor Ahluwalia,Michael Zhang,Nancy S. Pollard*

Main category: cs.RO

TL;DR: 提出了一种快速算法，用于计算全局最优的独立接触区域（ICRs），适用于实时规划，速度提升显著。


<details>
  <summary>Details</summary>
Motivation: ICRs在抓取和操作规划中有重要应用，但现有方法计算成本高，限制了其实际应用。

Method: 基于增量n维Delaunay三角剖分的分治算法，支持平面接触的抓取。

Result: 实验显示速度提升100倍以上，优于其他抓取质量指标。

Conclusion: 算法高效且实用，未来可扩展至3D实现，代码将开源以促进发展。

Abstract: This work presents a fast anytime algorithm for computing globally optimal
independent contact regions (ICRs). ICRs are regions such that one contact
within each region enables a valid grasp. Locations of ICRs can provide
guidance for grasp and manipulation planning, learning, and policy transfer.
However, ICRs for modern applications have been little explored, in part due to
the expense of computing them, as they have a search space exponential in the
number of contacts. We present a divide and conquer algorithm based on
incremental n-dimensional Delaunay triangulation that produces results with
bounded suboptimality in times sufficient for real-time planning. This paper
presents the base algorithm for grasps where contacts lie within a plane. Our
experiments show substantial benefits over competing grasp quality metrics and
speedups of 100X and more for competing approaches to computing ICRs. We
explore robustness of a policy guided by ICRs and outline a path to general 3D
implementation. Code will be released on publication to facilitate further
development and applications.

</details>


### [373] [MOMAV: A highly symmetrical fully-actuated multirotor drone using optimizing control allocation](https://arxiv.org/abs/2506.08868)
*Marco Ruggia*

Main category: cs.RO

TL;DR: MOMAV是一种全驱动、高度对称的多旋翼无人机，通过独特的八面体转子臂设计和主动旋转机制实现高效飞行。


<details>
  <summary>Details</summary>
Motivation: 设计一种能够独立控制方向和位置的全驱动无人机，同时保持高飞行效率。

Method: 采用八面体排列的六转子臂设计，每个臂可主动旋转，并使用基于SQP的控制分配算法。

Result: 飞行测试显示，MOMAV在位置和方向控制上表现出色，误差极低。

Conclusion: MOMAV的设计和控制算法在高效全驱动无人机领域具有显著优势。

Abstract: MOMAV (Marco's Omnidirectional Micro Aerial Vehicle) is a multirotor drone
that is fully actuated, meaning it can control its orientation independently of
its position. MOMAV is also highly symmetrical, making its flight efficiency
largely unaffected by its current orientation. These characteristics are
achieved by a novel drone design where six rotor arms align with the vertices
of an octahedron, and where each arm can actively rotate along its long axis.
Various standout features of MOMAV are presented: The high flight efficiency
compared to arm configuration of other fully-actuated drones, the design of an
original rotating arm assembly featuring slip-rings used to enable continuous
arm rotation, and a novel control allocation algorithm based on sequential
quadratic programming (SQP) used to calculate throttle and arm-angle setpoints
in flight. Flight tests have shown that MOMAV is able to achieve remarkably low
mean position/orientation errors of 6.6mm, 2.1{\deg} ({\sigma}: 3.0mm,
1.0{\deg}) when sweeping position setpoints, and 11.8mm, 3.3{\deg} ({\sigma}:
8.6mm, 2.0{\deg}) when sweeping orientation setpoints.

</details>


### [374] [CLONE: Closed-Loop Whole-Body Humanoid Teleoperation for Long-Horizon Tasks](https://arxiv.org/abs/2506.08931)
*Yixuan Li,Yutang Lin,Jieming Cui,Tengyu Liu,Wei Liang,Yixin Zhu,Siyuan Huang*

Main category: cs.RO

TL;DR: CLONE系统通过闭环误差校正实现高保真全身远程操作，解决了现有系统协调性和定位漂移问题。


<details>
  <summary>Details</summary>
Motivation: 当前远程操作系统在协调性和定位漂移方面存在局限，无法满足复杂人机交互需求。

Method: 采用基于MoE的CLONE系统，结合闭环误差校正和实时反馈，仅需MR头显的手部和头部追踪。

Result: CLONE实现了长时间高保真全身远程操作，支持复杂协调动作（如从地面拾取物体）。

Conclusion: CLONE为长时间人机交互任务设定了新标准。

Abstract: Humanoid teleoperation plays a vital role in demonstrating and collecting
data for complex humanoid-scene interactions. However, current teleoperation
systems face critical limitations: they decouple upper- and lower-body control
to maintain stability, restricting natural coordination, and operate open-loop
without real-time position feedback, leading to accumulated drift. The
fundamental challenge is achieving precise, coordinated whole-body
teleoperation over extended durations while maintaining accurate global
positioning. Here we show that an MoE-based teleoperation system, CLONE, with
closed-loop error correction enables unprecedented whole-body teleoperation
fidelity, maintaining minimal positional drift over long-range trajectories
using only head and hand tracking from an MR headset. Unlike previous methods
that either sacrifice coordination for stability or suffer from unbounded
drift, CLONE learns diverse motion skills while preventing tracking error
accumulation through real-time feedback, enabling complex coordinated movements
such as ``picking up objects from the ground.'' These results establish a new
milestone for whole-body humanoid teleoperation for long-horizon humanoid-scene
interaction tasks.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [375] [SPBA: Utilizing Speech Large Language Model for Backdoor Attacks on Speech Classification Models](https://arxiv.org/abs/2506.08346)
*Wenhan Yao,Fen Xiao,Xiarun Chen,Jia Liu,YongQiang He,Weiping Wen*

Main category: cs.SD

TL;DR: 论文提出了一种基于语音大语言模型（SLLM）的语音后门攻击方法（SPBA），通过利用音色和情感等语音元素生成多样化触发器，显著提高了攻击效果。同时，采用多梯度下降算法（MGDA）作为缓解策略。


<details>
  <summary>Details</summary>
Motivation: 语音分类任务（如关键词检测和说话人验证）在语音交互中至关重要，但其安全性易受后门攻击。现有攻击方法因触发器功能限制，生成的后门数量有限。

Method: 提出SPBA方法，利用SLLM生成多样化触发器，并通过MGDA算法优化攻击策略。

Result: 实验表明，SPBA在两种语音分类任务中表现出显著的触发器有效性，并在攻击指标上取得优异性能。

Conclusion: SPBA通过多样化触发器显著提升了攻击效果，同时MGDA算法为缓解后门攻击提供了可行方案。

Abstract: Deep speech classification tasks, including keyword spotting and speaker
verification, are vital in speech-based human-computer interaction. Recently,
the security of these technologies has been revealed to be susceptible to
backdoor attacks. Specifically, attackers use noisy disruption triggers and
speech element triggers to produce poisoned speech samples that train models to
become vulnerable. However, these methods typically create only a limited
number of backdoors due to the inherent constraints of the trigger function. In
this paper, we propose that speech backdoor attacks can strategically focus on
speech elements such as timbre and emotion, leveraging the Speech Large
Language Model (SLLM) to generate diverse triggers. Increasing the number of
triggers may disproportionately elevate the poisoning rate, resulting in higher
attack costs and a lower success rate per trigger. We introduce the Multiple
Gradient Descent Algorithm (MGDA) as a mitigation strategy to address this
challenge. The proposed attack is called the Speech Prompt Backdoor Attack
(SPBA). Building on this foundation, we conducted attack experiments on two
speech classification tasks, demonstrating that SPBA shows significant trigger
effectiveness and achieves exceptional performance in attack metrics.

</details>


### [376] [Pureformer-VC: Non-parallel Voice Conversion with Pure Stylized Transformer Blocks and Triplet Discriminative Training](https://arxiv.org/abs/2506.08348)
*Wenhan Yao,Fen Xiao,Xiarun Chen,Jia Liu,YongQiang He,Weiping Wen*

Main category: cs.SD

TL;DR: Pureformer-VC是一种基于Conformer和Zipformer的语音转换框架，通过变分解耦训练和注意力风格转移机制，显著提升了语音转换的性能。


<details>
  <summary>Details</summary>
Motivation: 传统基于GAN的语音转换方法在编码多样语音元素和合成自然语音方面存在挑战，Pureformer-VC旨在解决这些问题。

Method: 采用Conformer块构建解耦编码器，Zipformer块构建风格转移解码器，结合变分自编码器和三元组判别训练，并引入注意力风格转移机制。

Result: 在多说话人数据集上，模型在主观评分与现有方法相当的同时，客观指标显著提升。

Conclusion: Pureformer-VC在语音转换任务中表现出色，尤其在多样性和风格转移方面具有优势。

Abstract: As a foundational technology for intelligent human-computer interaction,
voice conversion (VC) seeks to transform speech from any source timbre into any
target timbre. Traditional voice conversion methods based on Generative
Adversarial Networks (GANs) encounter significant challenges in precisely
encoding diverse speech elements and effectively synthesising these elements
into natural-sounding converted speech. To overcome these limitations, we
introduce Pureformer-VC, an encoder-decoder framework that utilizes Conformer
blocks to build a disentangled encoder and employs Zipformer blocks to create a
style transfer decoder. We adopt a variational decoupled training approach to
isolate speech components using a Variational Autoencoder (VAE), complemented
by triplet discriminative training to enhance the speaker's discriminative
capabilities. Furthermore, we incorporate the Attention Style Transfer
Mechanism (ASTM) with Zipformer's shared weights to improve the style transfer
performance in the decoder. We conducted experiments on two multi-speaker
datasets. The experimental results demonstrate that the proposed model achieves
comparable subjective evaluation scores while significantly enhancing objective
metrics compared to existing approaches in many-to-many and many-to-one VC
scenarios.

</details>


### [377] [Teaching Physical Awareness to LLMs through Sounds](https://arxiv.org/abs/2506.08524)
*Weiguo Wang,Andy Nie,Wenrui Zhou,Yi Kai,Chengchen Hu*

Main category: cs.SD

TL;DR: ACORN框架通过声音教授LLMs物理感知，利用物理模拟器生成数据，构建AQA-PHY数据集，并连接音频编码器与LLMs，在模拟和现实任务中取得合理效果。


<details>
  <summary>Details</summary>
Motivation: LLMs缺乏对现实物理现象的理解，ACORN旨在通过声音增强其物理感知能力。

Method: 引入物理模拟器生成多样化训练数据，构建AQA-PHY数据集，提出处理幅度和相位信息的音频编码器，并与LLMs结合。

Result: 在视线检测、多普勒效应估计和到达方向估计等任务中表现合理。

Conclusion: ACORN为LLMs理解物理世界提供了可行路径。

Abstract: Large Language Models (LLMs) have shown remarkable capabilities in text and
multimodal processing, yet they fundamentally lack physical
awareness--understanding of real-world physical phenomena. In this work, we
present ACORN, a framework that teaches LLMs physical awareness through sound,
focusing on fundamental physical phenomena like the Doppler effect, multipath
effect, and spatial relationships. To overcome data scarcity, ACORN introduce a
physics-based simulator combining real-world sound sources with controlled
physical channels to generate diverse training data. Using this simulator, we
build AQA-PHY, a comprehensive Audio Question-Answer dataset, and propose an
audio encoder that processes both magnitude and phase information. By
connecting our audio encoder to state-of-the-art LLMs, we demonstrate
reasonable results in both simulated and real-world tasks, such as
line-of-sight detection, Doppler effect estimation, and Direction-of-Arrival
estimation, paving the way for enabling LLMs to understand physical world.

</details>


### [378] [MD-ViSCo: A Unified Model for Multi-Directional Vital Sign Waveform Conversion](https://arxiv.org/abs/2506.08357)
*Franck Meyer,Kyunghoon Hur,Edward Choi*

Main category: cs.SD

TL;DR: MD-ViSCo是一个统一的框架，能够通过单一模型从任何单一输入波形生成目标生命体征波形，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有模型仅针对特定源-目标对设计，导致多个模型并存，临床实用性受限。

Method: 采用浅层1D U-Net与Swin Transformer结合，利用AdaIN捕捉波形风格。

Result: 在两个公开数据集上，平均MAE降低8.8%，PC提高4.9%，ABP波形满足AAMI和BHS标准。

Conclusion: MD-ViSCo为医疗监测提供了一个统一的框架，无需为每项任务开发独立模型。

Abstract: Despite the remarkable progress of deep-learning methods generating a target
vital sign waveform from a source vital sign waveform, most existing models are
designed exclusively for a specific source-to-target pair. This requires
distinct model architectures, optimization procedures, and pre-processing
pipelines, resulting in multiple models that hinder usability in clinical
settings. To address this limitation, we propose the Multi-Directional
Vital-Sign Converter (MD-ViSCo), a unified framework capable of generating any
target waveform such as electrocardiogram (ECG), photoplethysmogram (PPG), or
arterial blood pressure (ABP) from any single input waveform with a single
model. MD-ViSCo employs a shallow 1-Dimensional U-Net integrated with a Swin
Transformer that leverages Adaptive Instance Normalization (AdaIN) to capture
distinct waveform styles. To evaluate the efficacy of MD-ViSCo, we conduct
multi-directional waveform generation on two publicly available datasets. Our
framework surpasses state-of-the-art baselines (NabNet & PPG2ABP) on average
across all waveform types, lowering Mean absolute error (MAE) by 8.8% and
improving Pearson correlation (PC) by 4.9% over two datasets. In addition, the
generated ABP waveforms satisfy the Association for the Advancement of Medical
Instrumentation (AAMI) criterion and achieve Grade B on the British
Hypertension Society (BHS) standard, outperforming all baselines. By
eliminating the need for developing a distinct model for each task, we believe
that this work offers a unified framework that can deal with any kind of vital
sign waveforms with a single model in healthcare monitoring.

</details>


### [379] [Multimodal Zero-Shot Framework for Deepfake Hate Speech Detection in Low-Resource Languages](https://arxiv.org/abs/2506.08372)
*Rishabh Ranjan,Likhith Ayinala,Mayank Vatsa,Richa Singh*

Main category: cs.SD

TL;DR: 提出了一种新颖的多模态框架，用于检测深度伪造音频中的仇恨言论，尤其在零样本场景中表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有方法在低资源语言和跨模态任务中表现不佳，需要一种更鲁棒的方法来结合音频和文本信息。

Method: 采用对比学习联合对齐音频和文本表示，构建了一个包含六种语言（英语和五种印度语言）的基准数据集。

Result: 在两个多语言测试集上，模型表现优于基线，准确率分别为0.819和0.701，并能泛化到未见语言。

Conclusion: 多模态方法在合成媒体中的仇恨言论检测中具有优势，尤其在低资源环境下表现突出。

Abstract: This paper introduces a novel multimodal framework for hate speech detection
in deepfake audio, excelling even in zero-shot scenarios. Unlike previous
approaches, our method uses contrastive learning to jointly align audio and
text representations across languages. We present the first benchmark dataset
with 127,290 paired text and synthesized speech samples in six languages:
English and five low-resource Indian languages (Hindi, Bengali, Marathi, Tamil,
Telugu). Our model learns a shared semantic embedding space, enabling robust
cross-lingual and cross-modal classification. Experiments on two multilingual
test sets show our approach outperforms baselines, achieving accuracies of
0.819 and 0.701, and generalizes well to unseen languages. This demonstrates
the advantage of combining modalities for hate speech detection in synthetic
media, especially in low-resource settings where unimodal models falter. The
Dataset is available at https://www.iab-rubric.org/resources.

</details>


### [380] [A Review on Score-based Generative Models for Audio Applications](https://arxiv.org/abs/2506.08457)
*Ge Zhu,Yutong Wen,Zhiyao Duan*

Main category: cs.SD

TL;DR: 本文综述了扩散模型在音频应用中的设计原则，重点关注质量改进和条件机制，并提供了一个开源代码库以促进研究。


<details>
  <summary>Details</summary>
Motivation: 现有综述缺乏对扩散模型设计选择的深入讨论，音频扩散模型领域也缺乏对这些设计选择的实现和比较的指导。

Method: 采用分数建模视角作为统一框架，系统分析扩散模型的训练和采样过程，以及音频应用中的条件机制。

Result: 介绍了开源代码库，并通过音频生成、语音增强和文本到语音合成的案例研究展示了其能力。

Conclusion: 该综述和代码库为音频扩散模型的研究和应用提供了系统指导和工具支持。

Abstract: Diffusion models have emerged as powerful deep generative techniques,
producing high-quality and diverse samples in applications in various domains
including audio. These models have many different design choices suitable for
different applications, however, existing reviews lack in-depth discussions of
these design choices. The audio diffusion model literature also lacks
principled guidance for the implementation of these design choices and their
comparisons for different applications. This survey provides a comprehensive
review of diffusion model design with an emphasis on design principles for
quality improvement and conditioning for audio applications. We adopt the score
modeling perspective as a unifying framework that accommodates various
interpretations, including recent approaches like flow matching. We
systematically examine the training and sampling procedures of diffusion
models, and audio applications through different conditioning mechanisms. To
address the lack of audio diffusion model codebases and to promote reproducible
research and rapid prototyping, we introduce an open-source codebase at
https://github.com/gzhu06/AudioDiffuser that implements our reviewed framework
for various audio applications. We demonstrate its capabilities through three
case studies: audio generation, speech enhancement, and text-to-speech
synthesis, with benchmark evaluations on standard datasets.

</details>


### [381] [Passive acoustic non-line-of-sight localization without a relay surface](https://arxiv.org/abs/2506.08471)
*Tal I. Sommer,Ori Katz*

Main category: cs.SD

TL;DR: 该研究提出了一种利用障碍物边缘衍射信号实现非视距（NLOS）声源三维定位的方法，适用于门道和凸角两种场景。


<details>
  <summary>Details</summary>
Motivation: 传统方法依赖反射信号进行非视距场景重建，但在某些环境中可能受限。本研究旨在通过衍射信号扩展NLOS声学感知能力。

Method: 针对门道和凸角两种场景，分别提出定位方法：门道利用门的两边缘作为虚拟探测器阵列；凸角则利用刀锋衍射的频谱特征。

Result: 通过刀锋衍射信号，实现了非视距声源的三维定位，扩展了传统方法的适用性。

Conclusion: 该方法为NLOS声源定位提供了新思路，尤其在传统反射信号受限的环境中具有潜力。

Abstract: The detection and localization of a source hidden outside the Line-of-Sight
(LOS) traditionally rely on the acquisition of indirect signals, such as those
reflected from visible relay surfaces such as floors or walls. These reflected
signals are then utilized to reconstruct the obscured scene. In this study, we
present an approach that utilize signals diffracted from an edge of an obstacle
to achieve three-dimensional (3D) localization of an acoustic point source
situated outside the LOS. We address two scenarios - a doorway and a convex
corner - and propose a localization method for each of them. For the first
scenario, we utilize the two edges of the door as virtual detector arrays. For
the second scenario, we exploit the spectral signature of a knife-edge
diffraction, inspired by the human perception of sound location by the
head-related transfer function (HRTF). In both methods, knife-edge diffraction
is utilized to extend the capabilities of non-line-of-sight (NLOS) acoustic
sensing, enabling localization in environments where conventional relay-surface
based approaches may be limited.

</details>


### [382] [Higher-Order Network Representation of J. S. Bach's Solo Violin Sonatas and Partitas: Topological and Geometrical Explorations](https://arxiv.org/abs/2506.08540)
*Dima Mrad,Sara Najem*

Main category: cs.SD

TL;DR: 该论文提出了一种基于高阶网络的拓扑框架，用于分析巴赫的小提琴独奏奏鸣曲和组曲，揭示了音乐中高阶交互的复杂性及其流变特征。


<details>
  <summary>Details</summary>
Motivation: 传统图表示方法无法充分捕捉音乐中高阶交互的复杂性，因此需要一种新框架来更全面地分析音乐结构。

Method: 使用高阶网络模型，将单音、双音、三音等分别表示为顶点、边和三角形，并建模音符间的流变。

Result: 发现了不同乐章类型（如慢板、赋格、巴洛克舞曲）在几何和拓扑特性上的显著差异，并验证了高斯-博内定理的应用。

Conclusion: 该拓扑框架为音乐分析提供了新视角，揭示了巴赫作品中的高阶结构特征及其动态变化。

Abstract: Music is inherently complex, with structures and interactions that unfold
across multiple layers. Complex networks have emerged as powerful structures
for the quantitative analysis of Western classical music, revealing significant
features of its harmonic and structural organization. Although notable works
have used these approaches to study music, dyadic representations of
interactions fall short in conveying the underlying complexity and depth. In
recent years, the limitations of traditional graph representations have been
questioned and challenged in the context of interactions that could be
higher-dimensional. Effective musical analysis requires models that capture
higher-order interactions and a framework that simultaneously captures
transitions between them. Subsequently, in this paper, we present a topological
framework for analyzing J. S. Bach's Solo Violin Sonatas and Partitas that uses
higher-order networks where single notes are vertices, two-note chords are
edges, three-notes are triangles, etc. We subsequently account for the flow of
music, by modeling transitions between successive notes. We identify
genre-specific patterns in the works' geometric and topological properties. In
particular, we find signatures in the trends of the evolution of the Euler
characteristic and curvature, as well as examining adherence to the
Gauss-Bonnet theorem across different movement types. The distinctions are
revealed between slow movements, Fugues, and Baroque dance movements through
their simplicial complex representation.

</details>


### [383] [Auto-Regressive vs Flow-Matching: a Comparative Study of Modeling Paradigms for Text-to-Music Generation](https://arxiv.org/abs/2506.08570)
*Or Tal,Felix Kreuk,Yossi Adi*

Main category: cs.SD

TL;DR: 论文通过系统比较自回归解码和条件流匹配两种建模范式，为文本到音乐生成提供设计指导。


<details>
  <summary>Details</summary>
Motivation: 当前文本到音乐生成模型在训练数据、建模范式等方面差异显著，难以公平评估性能差异。本文专注于建模范式的影响。

Method: 使用相同数据集、训练配置和类似架构，对比自回归解码和条件流匹配两种范式。

Result: 评估了生成质量、推理鲁棒性、可扩展性等指标，揭示了两种范式的优缺点。

Conclusion: 研究结果为未来文本到音乐生成系统的设计和训练提供了实用建议。

Abstract: Recent progress in text-to-music generation has enabled models to synthesize
high-quality musical segments, full compositions, and even respond to
fine-grained control signals, e.g. chord progressions. State-of-the-art (SOTA)
systems differ significantly across many dimensions, such as training datasets,
modeling paradigms, and architectural choices. This diversity complicates
efforts to evaluate models fairly and pinpoint which design choices most
influence performance. While factors like data and architecture are important,
in this study we focus exclusively on the modeling paradigm. We conduct a
systematic empirical analysis to isolate its effects, offering insights into
associated trade-offs and emergent behaviors that can guide future
text-to-music generation systems. Specifically, we compare the two arguably
most common modeling paradigms: Auto-Regressive decoding and Conditional
Flow-Matching. We conduct a controlled comparison by training all models from
scratch using identical datasets, training configurations, and similar backbone
architectures. Performance is evaluated across multiple axes, including
generation quality, robustness to inference configurations, scalability,
adherence to both textual and temporally aligned conditioning, and editing
capabilities in the form of audio inpainting. This comparative study sheds
light on distinct strengths and limitations of each paradigm, providing
actionable insights that can inform future architectural and training decisions
in the evolving landscape of text-to-music generation. Audio sampled examples
are available at: https://huggingface.co/spaces/ortal1602/ARvsFM

</details>


### [384] [Step-Audio-AQAA: a Fully End-to-End Expressive Large Audio Language Model](https://arxiv.org/abs/2506.08967)
*Ailin Huang,Bingxin Li,Bruce Wang,Boyong Wu,Chao Yan,Chengli Feng,Heng Wang,Hongyu Zhou,Hongyuan Wang,Jingbei Li,Jianjian Sun,Joanna Wang,Mingrui Chen,Peng Liu,Ruihang Miao,Shilei Jiang,Tian Fei,Wang You,Xi Chen,Xuerui Yang,Yechang Huang,Yuxiang Zhang,Zheng Ge,Zheng Gong,Zhewei Huang,Zixin Zhang,Bin Wang,Bo Li,Buyun Ma,Changxin Miao,Changyi Wan,Chen Xu,Dapeng Shi,Dingyuan Hu,Enle Liu,Guanzhe Huang,Gulin Yan,Hanpeng Hu,Haonan Jia,Jiahao Gong,Jiaoren Wu,Jie Wu,Jie Yang,Junzhe Lin,Kaixiang Li,Lei Xia,Longlong Gu,Ming Li,Nie Hao,Ranchen Ming,Shaoliang Pang,Siqi Liu,Song Yuan,Tiancheng Cao,Wen Li,Wenqing He,Xu Zhao,Xuelin Zhang,Yanbo Yu,Yinmin Zhong,Yu Zhou,Yuanwei Liang,Yuanwei Lu,Yuxiang Yang,Zidong Yang,Zili Zhang,Binxing Jiao,Heung-Yeung Shum,Jiansheng Chen,Jing Li,Xiangyu Zhang,Xinhao Zhang,Yibo Zhu,Daxin Jiang,Shuchang Zhou,Chen Hu*

Main category: cs.SD

TL;DR: Step-Audio-AQAA是一种端到端的大型音频-语言模型，通过双码本音频分词器和1300亿参数LLM，实现音频查询-音频回答任务，显著提升语音交互的自然性。


<details>
  <summary>Details</summary>
Motivation: 现有大型音频-语言模型依赖文本输出，限制了直接生成自然语音的能力，阻碍了无缝音频交互。

Method: 模型结合双码本音频分词器、1300亿参数LLM和神经声码器，采用交错文本-音频输出和DPO优化方法。

Result: 在StepEval-Audio-360基准测试中，Step-Audio-AQAA在语音控制等方面表现优异，超越现有最佳模型。

Conclusion: 该研究为端到端音频-语言模型提供了有效解决方案，并强调了基于分词的声码器在提升性能中的关键作用。

Abstract: Large Audio-Language Models (LALMs) have significantly advanced intelligent
human-computer interaction, yet their reliance on text-based outputs limits
their ability to generate natural speech responses directly, hindering seamless
audio interactions. To address this, we introduce Step-Audio-AQAA, a fully
end-to-end LALM designed for Audio Query-Audio Answer (AQAA) tasks. The model
integrates a dual-codebook audio tokenizer for linguistic and semantic feature
extraction, a 130-billion-parameter backbone LLM and a neural vocoder for
high-fidelity speech synthesis. Our post-training approach employs interleaved
token-output of text and audio to enhance semantic coherence and combines
Direct Preference Optimization (DPO) with model merge to improve performance.
Evaluations on the StepEval-Audio-360 benchmark demonstrate that
Step-Audio-AQAA excels especially in speech control, outperforming the
state-of-art LALMs in key areas. This work contributes a promising solution for
end-to-end LALMs and highlights the critical role of token-based vocoder in
enhancing overall performance for AQAA tasks.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [385] [Exp4Fuse: A Rank Fusion Framework for Enhanced Sparse Retrieval using Large Language Model-based Query Expansion](https://arxiv.org/abs/2506.04760)
*Lingyuan Liu,Mengxiang Zhang*

Main category: cs.IR

TL;DR: Exp4Fuse是一种新型融合排名框架，通过零样本LLM查询扩展提升稀疏检索性能，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有LLM生成的查询扩展方法成本高且计算密集，需要改进稀疏检索性能。

Method: 提出Exp4Fuse框架，结合原始查询和LLM增强查询，通过稀疏检索器生成两个排名列表并融合。

Result: 在多个数据集上，Exp4Fuse超越现有LLM查询扩展方法，结合高级稀疏检索器达到SOTA性能。

Conclusion: Exp4Fuse在提升稀疏检索性能方面表现优越，具有高效性和有效性。

Abstract: Large Language Models (LLMs) have shown potential in generating hypothetical
documents for query expansion, thereby enhancing information retrieval
performance. However, the efficacy of this method is highly dependent on the
quality of the generated documents, which often requires complex prompt
strategies and the integration of advanced dense retrieval techniques. This can
be both costly and computationally intensive. To mitigate these limitations, we
explore the use of zero-shot LLM-based query expansion to improve sparse
retrieval, particularly for learned sparse retrievers. We introduce a novel
fusion ranking framework, Exp4Fuse, which enhances the performance of sparse
retrievers through an indirect application of zero-shot LLM-based query
expansion. Exp4Fuse operates by simultaneously considering two retrieval
routes-one based on the original query and the other on the LLM-augmented
query. It then generates two ranked lists using a sparse retriever and fuses
them using a modified reciprocal rank fusion method. We conduct extensive
evaluations of Exp4Fuse against leading LLM-based query expansion methods and
advanced retrieval techniques on three MS MARCO-related datasets and seven
low-resource datasets. Experimental results reveal that Exp4Fuse not only
surpasses existing LLM-based query expansion methods in enhancing sparse
retrievers but also, when combined with advanced sparse retrievers, achieves
SOTA results on several benchmarks. This highlights the superior performance
and effectiveness of Exp4Fuse in improving query expansion for sparse
retrieval.

</details>


### [386] [Hierarchical Lexical Graph for Enhanced Multi-Hop Retrieval](https://arxiv.org/abs/2506.08074)
*Abdellah Ghassel,Ian Robinson,Gabriel Tanase,Hal Cooper,Bryan Thompson,Zhen Han,Vassilis N. Ioannidis,Soji Adeshina,Huzefa Rangwala*

Main category: cs.IR

TL;DR: 论文提出了一种名为HLG的三层索引结构，通过两种检索器（StatementGraphRAG和TopicGraphRAG）提升多文档检索性能，并引入合成数据集以评估多跳检索系统。


<details>
  <summary>Details</summary>
Motivation: 现有检索增强生成（RAG）方法在处理跨文档语义分散信息时表现不佳，需要更高效的检索机制。

Method: 提出HLG索引结构，结合两种检索器（StatementGraphRAG和TopicGraphRAG），并开发合成数据集用于评估。

Result: 实验表明，该方法在五个数据集上平均检索召回率和正确率相对提升23.1%。

Conclusion: HLG和配套检索器显著提升了多文档检索性能，开源工具可供使用。

Abstract: Retrieval-Augmented Generation (RAG) grounds large language models in
external evidence, yet it still falters when answers must be pieced together
across semantically distant documents. We close this gap with the Hierarchical
Lexical Graph (HLG), a three-tier index that (i) traces every atomic
proposition to its source, (ii) clusters propositions into latent topics, and
(iii) links entities and relations to expose cross-document paths. On top of
HLG we build two complementary, plug-and-play retrievers: StatementGraphRAG,
which performs fine-grained entity-aware beam search over propositions for
high-precision factoid questions, and TopicGraphRAG, which selects coarse
topics before expanding along entity links to supply broad yet relevant context
for exploratory queries. Additionally, existing benchmarks lack the complexity
required to rigorously evaluate multi-hop summarization systems, often focusing
on single-document queries or limited datasets. To address this, we introduce a
synthetic dataset generation pipeline that curates realistic, multi-document
question-answer pairs, enabling robust evaluation of multi-hop retrieval
systems. Extensive experiments across five datasets demonstrate that our
methods outperform naive chunk-based RAG achieving an average relative
improvement of 23.1% in retrieval recall and correctness. Open-source Python
library is available at https://github.com/awslabs/graphrag-toolkit.

</details>


### [387] [Bridging RDF Knowledge Graphs with Graph Neural Networks for Semantically-Rich Recommender Systems](https://arxiv.org/abs/2506.08743)
*Michael Färber,David Lamprecht,Yuni Susanti*

Main category: cs.IR

TL;DR: 论文提出了一种将RDF知识图谱与图神经网络（GNN）全面整合的方法，以提升推荐系统的性能。


<details>
  <summary>Details</summary>
Motivation: 尽管已有大量RDF标准的知识图谱，但其丰富的语义信息尚未在基于GNN的推荐系统中得到充分利用。

Method: 通过结合RDF对象属性的拓扑信息和数据类型属性的内容信息，深入评估不同GNN的性能。

Result: 实验表明，利用RDF知识图谱的语义丰富性显著提升了推荐系统的效果。

Conclusion: 该方法为基于GNN的推荐系统在开放数据云中的应用奠定了基础。

Abstract: Graph Neural Networks (GNNs) have substantially advanced the field of
recommender systems. However, despite the creation of more than a thousand
knowledge graphs (KGs) under the W3C standard RDF, their rich semantic
information has not yet been fully leveraged in GNN-based recommender systems.
To address this gap, we propose a comprehensive integration of RDF KGs with
GNNs that utilizes both the topological information from RDF object properties
and the content information from RDF datatype properties. Our main focus is an
in-depth evaluation of various GNNs, analyzing how different semantic feature
initializations and types of graph structure heterogeneity influence their
performance in recommendation tasks. Through experiments across multiple
recommendation scenarios involving multi-million-node RDF graphs, we
demonstrate that harnessing the semantic richness of RDF KGs significantly
improves recommender systems and lays the groundwork for GNN-based recommender
systems for the Linked Open Data cloud. The code and data are available on our
GitHub repository: https://github.com/davidlamprecht/rdf-gnn-recommendation

</details>


### [388] [Multimodal Representation Alignment for Cross-modal Information Retrieval](https://arxiv.org/abs/2506.08774)
*Fan Xu,Luis A. Leiva*

Main category: cs.IR

TL;DR: 论文研究了多模态检索中的特征对齐问题，比较了不同相似性度量方法，发现Wasserstein距离和余弦相似性表现最佳，同时指出传统架构在多模态交互中的不足。


<details>
  <summary>Details</summary>
Motivation: 多模态检索中，不同机器学习模型对同一概念的表示方式不同，如何有效对齐不同模态的特征是关键挑战。

Method: 研究了视觉和文本嵌入的几何关系，使用四种标准相似性度量和两种基于神经网络的学习方法进行特征对齐。

Result: Wasserstein距离可作为模态差距的度量，余弦相似性在特征对齐任务中表现最优，传统架构（如多层感知机）不足以捕捉多模态交互。

Conclusion: 研究为多模态信息检索提供了新见解和实践建议，尤其适用于现实世界的跨模态应用。

Abstract: Different machine learning models can represent the same underlying concept
in different ways. This variability is particularly valuable for in-the-wild
multimodal retrieval, where the objective is to identify the corresponding
representation in one modality given another modality as input. This challenge
can be effectively framed as a feature alignment problem. For example, given a
sentence encoded by a language model, retrieve the most semantically aligned
image based on features produced by an image encoder, or vice versa. In this
work, we first investigate the geometric relationships between visual and
textual embeddings derived from both vision-language models and combined
unimodal models. We then align these representations using four standard
similarity metrics as well as two learned ones, implemented via neural
networks. Our findings indicate that the Wasserstein distance can serve as an
informative measure of the modality gap, while cosine similarity consistently
outperforms alternative metrics in feature alignment tasks. Furthermore, we
observe that conventional architectures such as multilayer perceptrons are
insufficient for capturing the complex interactions between image and text
representations. Our study offers novel insights and practical considerations
for researchers working in multimodal information retrieval, particularly in
real-world, cross-modal applications.

</details>


<div id='q-bio.BM'></div>

# q-bio.BM [[Back]](#toc)

### [389] [Aligning Proteins and Language: A Foundation Model for Protein Retrieval](https://arxiv.org/abs/2506.08023)
*Qifeng Wu,Zhengzhe Liu,Han Zhu,Yizhou Zhao,Daisuke Kihara,Min Xu*

Main category: q-bio.BM

TL;DR: 该论文提出了一种基于CLIP框架的方法，通过对比学习将3D蛋白质结构与功能注释对齐，用于从大规模蛋白质数据集中检索结构和语义相似的蛋白质。


<details>
  <summary>Details</summary>
Motivation: 近年来视觉-语言模型（VLMs）的进展激发了研究兴趣，旨在利用这些模型帮助解释通过结构测定方法（如冷冻电镜）获得的蛋白质结构的功能。

Method: 采用CLIP风格的框架，通过对比学习对齐3D蛋白质结构和功能注释，并构建了一个包含约20万蛋白质-描述对的大规模数据集用于训练。

Result: 在Protein Data Bank（PDB）和Electron Microscopy Data Bank（EMDB）数据集上进行了评估，模型在零样本检索任务中表现出色。

Conclusion: 该方法展示了多模态基础模型在蛋白质生物学中结构-功能理解的潜力。

Abstract: This paper aims to retrieve proteins with similar structures and semantics
from large-scale protein dataset, facilitating the functional interpretation of
protein structures derived by structural determination methods like
cryo-Electron Microscopy (cryo-EM). Motivated by the recent progress of
vision-language models (VLMs), we propose a CLIP-style framework for aligning
3D protein structures with functional annotations using contrastive learning.
For model training, we propose a large-scale dataset of approximately 200,000
protein-caption pairs with rich functional descriptors. We evaluate our model
in both in-domain and more challenging cross-database retrieval on Protein Data
Bank (PDB) and Electron Microscopy Data Bank (EMDB) dataset, respectively. In
both cases, our approach demonstrates promising zero-shot retrieval
performance, highlighting the potential of multimodal foundation models for
structure-function understanding in protein biology.

</details>


<div id='cs.IT'></div>

# cs.IT [[Back]](#toc)

### [390] [Learning-Based Multiuser Scheduling in MIMO-OFDM Systems with Hybrid Beamforming](https://arxiv.org/abs/2506.08263)
*Pouya Agheli,Tugce Kobal,François Durand,Matthew Andrews*

Main category: cs.IT

TL;DR: 研究了多用户调度问题，结合MIMO、OFDM和混合波束成形，提出基于毫米波信道的两时间尺度协议和多种调度算法。


<details>
  <summary>Details</summary>
Motivation: 在混合波束成形系统中，由于多路增益有限，改进调度对提升频谱效率和长期性能（基于比例公平性指标）至关重要。

Method: 采用两时间尺度协议：长时尺度分配模拟波束，短时尺度调度用户并设计数字预编码器；提出组合算法（贪婪、排序）和机器学习方法。

Result: 数值结果展示了性能与复杂度的权衡，表明方法选择需根据具体场景需求。

Conclusion: 不同方法适用于不同场景，需根据具体标准选择最优调度策略。

Abstract: We investigate the multiuser scheduling problem in multiple-input
multiple-output (MIMO) systems using orthogonal frequency division multiplexing
(OFDM) and hybrid beamforming in which a base station (BS) communicates with
multiple users over millimeter wave (mmWave) channels in the downlink. Improved
scheduling is critical for enhancing spectral efficiency and the long-term
performance of the system from the perspective of proportional fairness (PF)
metric in hybrid beamforming systems due to its limited multiplexing gain. Our
objective is to maximize PF by properly designing the analog and digital
precoders within the hybrid beamforming and selecting the users subject to the
number of radio frequency (RF) chains. Leveraging the characteristics of mmWave
channels, we apply a two-timescale protocol. On a long timescale, we assign an
analog beam to each user. Scheduling the users and designing the digital
precoder are done accordingly on a short timescale. To conduct scheduling, we
propose combinatorial solutions, such as greedy and sorting algorithms,
followed by a machine learning (ML) approach. Our numerical results highlight
the trade-off between the performance and complexity of the proposed
approaches. Consequently, we show that the choice of approach depends on the
specific criteria within a given scenario.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [391] [Snap-and-tune: combining deep learning and test-time optimization for high-fidelity cardiovascular volumetric meshing](https://arxiv.org/abs/2506.08280)
*Daniel H. Pak,Shubh Thaker,Kyle Baylous,Xiaoran Zhang,Danny Bluestein,James S. Duncan*

Main category: eess.IV

TL;DR: 提出了一种结合深度学习和测试时优化的“snap-and-tune”策略，用于从医学图像生成高质量体积网格，显著提高了空间精度和网格质量。


<details>
  <summary>Details</summary>
Motivation: 现有基于深度学习的模板变形方法在高曲率区域和部件间距离上存在局限性，需要更灵活且精确的解决方案。

Method: 采用“snap-and-tune”策略，先通过深度学习快速拟合初始形状，再通过测试时优化进行样本特定的网格修正。

Result: 方法在空间精度和网格质量上均有显著提升，且完全自动化，无需额外训练标签。

Conclusion: 该方法生成的网格在固体力学模拟中表现出多功能性和实用性，代码已开源。

Abstract: High-quality volumetric meshing from medical images is a key bottleneck for
physics-based simulations in personalized medicine. For volumetric meshing of
complex medical structures, recent studies have often utilized deep learning
(DL)-based template deformation approaches to enable fast test-time generation
with high spatial accuracy. However, these approaches still exhibit
limitations, such as limited flexibility at high-curvature areas and
unrealistic inter-part distances. In this study, we introduce a simple yet
effective snap-and-tune strategy that sequentially applies DL and test-time
optimization, which combines fast initial shape fitting with more detailed
sample-specific mesh corrections. Our method provides significant improvements
in both spatial accuracy and mesh quality, while being fully automated and
requiring no additional training labels. Finally, we demonstrate the
versatility and usefulness of our newly generated meshes via solid mechanics
simulations in two different software platforms. Our code is available at
https://github.com/danpak94/Deep-Cardiac-Volumetric-Mesh.

</details>


### [392] [DCD: A Semantic Segmentation Model for Fetal Ultrasound Four-Chamber View](https://arxiv.org/abs/2506.08534)
*Donglian Li,Hui Guo,Minglang Chen,Huizhen Chen,Jialing Chen,Bocheng Liang,Pengchen Liang,Ying Tan*

Main category: eess.IV

TL;DR: 提出了一种基于深度学习的模型DCD，用于自动分割胎儿心尖四腔视图中的关键解剖结构，以提高分割精度并减少超声医师的工作量。


<details>
  <summary>Details</summary>
Motivation: 胎儿心尖四腔视图的精确分割对先天性心脏病的早期诊断至关重要，但由于超声伪影、噪声和解剖变异性等问题，分割仍然具有挑战性。

Method: DCD模型结合了Dense ASPP模块和CBAM模块，实现了多尺度特征提取和自适应特征表示。

Result: DCD能够有效捕捉局部和全局上下文信息，实现精确且鲁棒的分割。

Conclusion: DCD模型为改善产前心脏评估提供了有力工具。

Abstract: Accurate segmentation of anatomical structures in the apical four-chamber
(A4C) view of fetal echocardiography is essential for early diagnosis and
prenatal evaluation of congenital heart disease (CHD). However, precise
segmentation remains challenging due to ultrasound artifacts, speckle noise,
anatomical variability, and boundary ambiguity across different gestational
stages. To reduce the workload of sonographers and enhance segmentation
accuracy, we propose DCD, an advanced deep learning-based model for automatic
segmentation of key anatomical structures in the fetal A4C view. Our model
incorporates a Dense Atrous Spatial Pyramid Pooling (Dense ASPP) module,
enabling superior multi-scale feature extraction, and a Convolutional Block
Attention Module (CBAM) to enhance adaptive feature representation. By
effectively capturing both local and global contextual information, DCD
achieves precise and robust segmentation, contributing to improved prenatal
cardiac assessment.

</details>


### [393] [Plug-and-Play Linear Attention for Pre-trained Image and Video Restoration Models](https://arxiv.org/abs/2506.08520)
*Srinivasan Kidambi,Pravin Nair*

Main category: eess.IV

TL;DR: PnP-Nystra是一种基于Nyström的线性自注意力近似方法，可作为即插即用模块，无需重新训练即可加速预训练图像和视频修复模型。


<details>
  <summary>Details</summary>
Motivation: 多头自注意力（MHSA）的二次复杂度在实时和资源受限环境中成为计算瓶颈，需要一种高效替代方案。

Method: 提出PnP-Nystra，一种线性自注意力近似方法，适用于窗口式Transformer架构（如SwinIR、Uformer、RVRT）。

Result: 在图像和视频修复任务中，PnP-Nystra实现了2-4倍GPU和2-5倍CPU加速，PSNR最大仅下降1.5 dB。

Conclusion: PnP-Nystra是首个无需训练的线性注意力替代方案，显著加速模型推理且性能损失极小。

Abstract: Multi-head self-attention (MHSA) has become a core component in modern
computer vision models. However, its quadratic complexity with respect to input
length poses a significant computational bottleneck in real-time and resource
constrained environments. We propose PnP-Nystra, a Nystr\"om based linear
approximation of self-attention, developed as a plug-and-play (PnP) module that
can be integrated into the pre-trained image and video restoration models
without retraining. As a drop-in replacement for MHSA, PnP-Nystra enables
efficient acceleration in various window-based transformer architectures,
including SwinIR, Uformer, and RVRT. Our experiments across diverse image and
video restoration tasks, including denoising, deblurring, and super-resolution,
demonstrate that PnP-Nystra achieves a 2-4x speed-up on an NVIDIA RTX 4090 GPU
and a 2-5x speed-up on CPU inference. Despite these significant gains, the
method incurs a maximum PSNR drop of only 1.5 dB across all evaluated tasks. To
the best of our knowledge, we are the first to demonstrate a linear attention
functioning as a training-free substitute for MHSA in restoration models.

</details>


### [394] [Biologically Inspired Deep Learning Approaches for Fetal Ultrasound Image Classification](https://arxiv.org/abs/2506.08623)
*Rinat Prochii,Elizaveta Dakhova,Pavel Birulin,Maxim Sharaev*

Main category: eess.IV

TL;DR: 提出了一种基于生物视觉系统启发的深度学习集成框架，用于同时分类16种胎儿结构，在复杂临床数据上表现优异。


<details>
  <summary>Details</summary>
Motivation: 解决胎儿超声图像分类中因低质量、高类内变异和类别不平衡带来的挑战。

Method: 采用双分支（浅层路径和详细路径）的轻量级架构，结合EfficientNet-B0和EfficientNet-B6模型，使用LDAM-Focal损失函数。

Result: 在5298张临床图像上，90%的器官分类准确率>0.75，75%的器官>0.85，性能优于其他复杂模型。

Conclusion: 生物启发的模块化堆叠方法在复杂临床环境中具有鲁棒性和可扩展性。

Abstract: Accurate classification of second-trimester fetal ultrasound images remains
challenging due to low image quality, high intra-class variability, and
significant class imbalance. In this work, we introduce a simple yet powerful,
biologically inspired deep learning ensemble framework that-unlike prior
studies focused on only a handful of anatomical targets-simultaneously
distinguishes 16 fetal structures. Drawing on the hierarchical, modular
organization of biological vision systems, our model stacks two complementary
branches (a "shallow" path for coarse, low-resolution cues and a "detailed"
path for fine, high-resolution features), concatenating their outputs for final
prediction. To our knowledge, no existing method has addressed such a large
number of classes with a comparably lightweight architecture. We trained and
evaluated on 5,298 routinely acquired clinical images (annotated by three
experts and reconciled via Dawid-Skene), reflecting real-world noise and
variability rather than a "cleaned" dataset. Despite this complexity, our
ensemble (EfficientNet-B0 + EfficientNet-B6 with LDAM-Focal loss) identifies
90% of organs with accuracy > 0.75 and 75% of organs with accuracy >
0.85-performance competitive with more elaborate models applied to far fewer
categories. These results demonstrate that biologically inspired modular
stacking can yield robust, scalable fetal anatomy recognition in challenging
clinical settings.

</details>


### [395] [MAMBO: High-Resolution Generative Approach for Mammography Images](https://arxiv.org/abs/2506.08677)
*Milica Škipina,Nikola Jovišić,Nicola Dall'Asen,Vanja Švenda,Anil Osman Tur,Slobodan Ilić,Elisa Ricci,Dubravko Ćulibrk*

Main category: eess.IV

TL;DR: 论文提出了一种名为MAMBO的基于扩散模型的方法，用于生成高分辨率乳腺X光片，以解决训练AI系统时数据不足的问题。


<details>
  <summary>Details</summary>
Motivation: 由于隐私和伦理限制，获取大规模多样化的乳腺X光数据集困难，影响了AI系统的训练效果。

Method: MAMBO采用基于补丁的扩散方法，结合局部和全局上下文信息，生成3840x3840像素的高分辨率图像。

Result: MAMBO能够生成高度真实的乳腺X光片，并可用于分类模型训练和异常检测。

Conclusion: MAMBO在图像生成、超分辨率和异常检测方面表现出色，有望提升乳腺X光分析的准确性和早期病变检测能力。

Abstract: Mammography is the gold standard for the detection and diagnosis of breast
cancer. This procedure can be significantly enhanced with Artificial
Intelligence (AI)-based software, which assists radiologists in identifying
abnormalities. However, training AI systems requires large and diverse
datasets, which are often difficult to obtain due to privacy and ethical
constraints. To address this issue, the paper introduces MAMmography ensemBle
mOdel (MAMBO), a novel patch-based diffusion approach designed to generate
full-resolution mammograms. Diffusion models have shown breakthrough results in
realistic image generation, yet few studies have focused on mammograms, and
none have successfully generated high-resolution outputs required to capture
fine-grained features of small lesions. To achieve this, MAMBO integrates
separate diffusion models to capture both local and global (image-level)
contexts. The contextual information is then fed into the final patch-based
model, significantly aiding the noise removal process. This thoughtful design
enables MAMBO to generate highly realistic mammograms of up to 3840x3840
pixels. Importantly, this approach can be used to enhance the training of
classification models and extended to anomaly detection. Experiments, both
numerical and radiologist validation, assess MAMBO's capabilities in image
generation, super-resolution, and anomaly detection, highlighting its potential
to enhance mammography analysis for more accurate diagnoses and earlier lesion
detection.

</details>


### [396] [Enhancing Synthetic CT from CBCT via Multimodal Fusion: A Study on the Impact of CBCT Quality and Alignment](https://arxiv.org/abs/2506.08716)
*Maximilian Tschuchnig,Lukas Lamminger,Philipp Steininger,Michael Gadermayr*

Main category: eess.IV

TL;DR: 通过多模态学习提升CBCT到CT的合成效果，显著改善低质量CBCT-CT对齐情况下的图像质量。


<details>
  <summary>Details</summary>
Motivation: CBCT虽快速低辐射，但图像质量因伪影问题不如传统CT，需通过合成CT（sCT）改善。

Method: 结合术中CBCT与术前CT，利用多模态学习生成sCT，并通过合成数据集分析对齐与质量影响。

Result: 多模态sCT表现优于单模态基线，尤其在低质量CBCT-CT对齐情况下提升显著。

Conclusion: 多模态学习方法在真实临床数据中具有高度可重复性，有效提升sCT质量。

Abstract: Cone-Beam Computed Tomography (CBCT) is widely used for real-time
intraoperative imaging due to its low radiation dose and high acquisition
speed. However, despite its high resolution, CBCT suffers from significant
artifacts and thereby lower visual quality, compared to conventional Computed
Tomography (CT). A recent approach to mitigate these artifacts is synthetic CT
(sCT) generation, translating CBCT volumes into the CT domain. In this work, we
enhance sCT generation through multimodal learning, integrating intraoperative
CBCT with preoperative CT. Beyond validation on two real-world datasets, we use
a versatile synthetic dataset, to analyze how CBCT-CT alignment and CBCT
quality affect sCT quality. The results demonstrate that multimodal sCT
consistently outperform unimodal baselines, with the most significant gains
observed in well-aligned, low-quality CBCT-CT cases. Finally, we demonstrate
that these findings are highly reproducible in real-world clinical datasets.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [397] [POLARON: Precision-aware On-device Learning and Adaptive Runtime-cONfigurable AI acceleration](https://arxiv.org/abs/2506.08785)
*Mukul Lokhande,Santosh Kumar Vishvakarma*

Main category: cs.AR

TL;DR: PARV-CE是一个支持多精度格式的SIMD MAC引擎，通过硬件-软件协同设计优化性能和能耗，适用于边缘AI加速。


<details>
  <summary>Details</summary>
Motivation: 随着AI模型复杂度的增加，边缘平台需要灵活的硬件支持多种精度格式，以平衡计算精度和能耗。

Method: PARV-CE采用统一的SIMD数据路径，支持4/8/16位定点、浮点和posit格式，并结合层自适应精度策略和量化感知执行。

Result: 相比现有技术，PARV-CE在PDP上提升2倍，资源使用减少3倍，精度损失控制在1.8%以内。

Conclusion: PARV-CE是一个可扩展且高效的边缘AI加速解决方案，适用于多种模型训练和推理任务。

Abstract: The increasing complexity of AI models requires flexible hardware capable of
supporting diverse precision formats, particularly for energy-constrained edge
platforms. This work presents PARV-CE, a SIMD-enabled, multi-precision MAC
engine that performs efficient multiply-accumulate operations using a unified
data-path for 4/8/16-bit fixed-point, floating point, and posit formats. The
architecture incorporates a layer adaptive precision strategy to align
computational accuracy with workload sensitivity, optimizing both performance
and energy usage. PARV-CE integrates quantization-aware execution with a
reconfigurable SIMD pipeline, enabling high-throughput processing with minimal
overhead through hardware-software co-design. The results demonstrate up to 2x
improvement in PDP and 3x reduction in resource usage compared to SoTA designs,
while retaining accuracy within 1.8% FP32 baseline. The architecture supports
both on-device training and inference across a range of workloads, including
DNNs, RNNs, RL, and Transformer models. The empirical analysis establish PARVCE
incorporated POLARON as a scalable and energy-efficient solution for
precision-adaptive AI acceleration at edge.

</details>


<div id='eess.SY'></div>

# eess.SY [[Back]](#toc)

### [398] [Joint Routing and Control Optimization in VANET](https://arxiv.org/abs/2506.08038)
*Chen Huang,Dingxuan Wang,Ronghui Hou*

Main category: eess.SY

TL;DR: DynaRoute是一个动态车辆网络的自适应联合优化框架，通过轨迹感知路由和安全约束车辆协调，同时解决车队控制和数据传输问题。


<details>
  <summary>Details</summary>
Motivation: 动态车辆网络中，车队控制和数据传输的联合优化是提升智能交通系统性能的关键。

Method: DynaRoute结合实时轨迹预测和安全约束，优化传输路径并确保车队稳定性。

Result: 在多种复杂场景下，DynaRoute显著提升了吞吐量和可靠性，优于传统方法。

Conclusion: DynaRoute为动态车辆网络提供了一种高效、可靠的联合优化解决方案。

Abstract: In this paper, we introduce DynaRoute, an adaptive joint optimization
framework for dynamic vehicular networks that simultaneously addresses platoon
control and data transmission through trajectory-aware routing and
safety-constrained vehicle coordination. DynaRoute guarantees continuous
vehicle movement via platoon safety control with optimizing transmission paths
through real-time trajectory prediction and ensuring reliable data. Our
solution achieves three key objectives: (1) maintaining platoon stability
through accurate data transmission, (2) enabling adaptive routing based on
vehicle movement patterns, and (3) enhancing overall intelligent transportation
system performance. DynaRoute equires predefined traffic models and adapts to
dynamic network conditions using local vehicle state information. We present
comprehensive simulation results demonstrating that DynaRoute maintains control
and transmission performance in multiple complex scenarios while significantly
improving throughput and reliability compared to traditional approaches.

</details>


### [399] [DEKC: Data-Enable Control for Tethered Space Robot Deployment in the Presence of Uncertainty via Koopman Operator Theory](https://arxiv.org/abs/2506.08319)
*Ao Jin,Qinyi Wang,Sijie Wen,Ya Liu,Ganghui Shen,Panfeng Huang,Fan Zhang*

Main category: eess.SY

TL;DR: 提出了一种名为DEKC的数据驱动框架，用于在未知不确定性下部署系留空间机器人，通过离线训练和在线执行两部分实现高精度和收敛性。


<details>
  <summary>Details</summary>
Motivation: 解决系留空间机器人在未知不确定性环境中的部署问题，提高其准确性和鲁棒性。

Method: 将未知不确定性建模为动态系统，利用数据驱动的Koopman理论构建代理模型，并通过深度神经网络参数化提升函数。离线阶段学习提升函数，在线阶段补偿基线控制器。

Result: 数值模拟验证了DEKC框架的有效性，能够减弱或消除不确定性的影响。

Conclusion: DEKC框架在未知不确定性环境下表现出色，且代码已开源。

Abstract: This work focuses the deployment of tethered space robot in the presence of
unknown uncertainty. A data-enable framework called DEKC which contains offline
training part and online execution part is proposed to deploy tethered space
robot in the presence of uncertainty. The main idea of this work is modeling
the unknown uncertainty as a dynamical system, which enables high accuracy and
convergence of capturing uncertainty. The core part of proposed framework is a
proxy model of uncertainty, which is derived from data-driven Koopman theory
and is separated with controller design. In the offline stage, the lifting
functions associated with Koopman operator are parameterized with deep neural
networks. Then by solving an optimization problem, the lifting functions are
learned from sampling data. In the online execution stage, the proxy model
cooperates the learned lifting functions obtained in the offline phase to
capture the unknown uncertainty. Then the output of proxy model is compensated
to the baseline controller such that the effect of uncertainty can be
attenuated or even eliminated. Furthermore, considering some scenarios in which
the performance of proxy model may weaken, a receding-horizon scheme is
proposed to update the proxy model online. Finally, the extensive numerical
simulations demonstrate the effectiveness of our proposed framework. The
implementation of proposed DEKC framework is publicly available at
https://github.com/NPU-RCIR/DEKC.git.

</details>


### [400] [Efficient Learning of Vehicle Controller Parameters via Multi-Fidelity Bayesian Optimization: From Simulation to Experiment](https://arxiv.org/abs/2506.08719)
*Yongpeng Zhao,Maik Pfefferkorn,Maximilian Templer,Rolf Findeisen*

Main category: eess.SY

TL;DR: 提出了一种基于多保真度贝叶斯优化的方法，通过结合低保真度仿真数据和少量真实实验，高效学习最优控制器参数，减少手动调参和昂贵实地测试的需求。


<details>
  <summary>Details</summary>
Motivation: 传统车辆控制器参数调优依赖大量实地测试，效率低下且成本高昂。

Method: 采用自回归多保真度高斯过程模型与贝叶斯优化结合，实现不同保真度级别间的知识迁移，无需额外低保真度评估。

Result: 仿真和真实实验验证表明，该方法仅需极少数真实实验即可实现高质量控制器性能。

Conclusion: 该方法为工业应用中的智能车辆控制调优提供了实用且可扩展的解决方案。

Abstract: Parameter tuning for vehicle controllers remains a costly and time-intensive
challenge in automotive development. Traditional approaches rely on extensive
real-world testing, making the process inefficient. We propose a multi-fidelity
Bayesian optimization approach that efficiently learns optimal controller
parameters by leveraging both low-fidelity simulation data and a very limited
number of real-world experiments. Our approach significantly reduces the need
for manual tuning and expensive field testing while maintaining the standard
two-stage development workflow used in industry. The core contribution is the
integration of an auto-regressive multi-fidelity Gaussian process model into
Bayesian optimization, enabling knowledge transfer between different fidelity
levels without requiring additional low-fidelity evaluations during real-world
testing. We validate our approach through both simulation studies and realworld
experiments. The results demonstrate that our method achieves high-quality
controller performance with only very few real-world experiments, highlighting
its potential as a practical and scalable solution for intelligent vehicle
control tuning in industrial applications.

</details>


### [401] [Inverse Design in Distributed Circuits Using Single-Step Reinforcement Learning](https://arxiv.org/abs/2506.08029)
*Jiayu Li,Masood Mortazavi,Ning Yan,Yihong Ma,Reza Zafarani*

Main category: eess.SY

TL;DR: DCIDA是一种分布式电路逆向设计框架，通过联合训练的采样策略生成近最优设计，显著降低设计误差。


<details>
  <summary>Details</summary>
Motivation: 现有设计方法依赖可微分评估、固定拓扑和离散空间，而实际需求涉及非可微分评估、多变拓扑和连续空间。

Method: DCIDA通过联合训练的采样策略生成设计决策，利用注入式映射将原始设计动作转换为物理表示。

Result: 实验表明，DCIDA的Transformer策略网络在复杂传输函数下显著优于现有方法。

Conclusion: DCIDA为分布式电路设计提供了一种高效且适应性强的解决方案。

Abstract: The goal of inverse design in distributed circuits is to generate
near-optimal designs that meet a desirable transfer function specification.
Existing design exploration methods use some combination of strategies
involving artificial grids, differentiable evaluation procedures, and specific
template topologies. However, real-world design practices often require
non-differentiable evaluation procedures, varying topologies, and
near-continuous placement spaces. In this paper, we propose DCIDA, a design
exploration framework that learns a near-optimal design sampling policy for a
target transfer function. DCIDA decides all design factors in a compound
single-step action by sampling from a set of jointly-trained conditional
distributions generated by the policy. Utilizing an injective interdependent
``map", DCIDA transforms raw sampled design ``actions" into uniquely equivalent
physical representations, enabling the framework to learn the conditional
dependencies among joint ``raw'' design decisions. Our experiments demonstrate
DCIDA's Transformer-based policy network achieves significant reductions in
design error compared to state-of-the-art approaches, with significantly better
fit in cases involving more complex transfer functions.

</details>


### [402] [Feasibility Study of CNNs and MLPs for Radiation Heat Transfer in 2-D Furnaces with Spectrally Participative Gases](https://arxiv.org/abs/2506.08033)
*Axel TahmasebiMoradi,Vincent Ren,Benjamin Le-Creurer,Chetra Mang*

Main category: eess.SY

TL;DR: 论文提出了一种基于CNN和MLP的替代模型，用于近似计算二维壁域中参与性气体的辐射热传递，显著降低了计算成本。


<details>
  <summary>Details</summary>
Motivation: 减少数值模拟的计算成本，同时保持工业可接受的精度。

Method: 通过调整输入以适应CNN架构，使用ICARUS2D生成数据集，并利用Optuna优化超参数。

Result: CNN和MLP均显著加速计算，CNN在精度和鲁棒性上优于MLP。

Conclusion: CNN在替代模型中表现更优，适用于工业应用。

Abstract: Aiming to reduce the computational cost of numerical simulations, a
convolutional neural network (CNN) and a multi-layer perceptron (MLP) are
introduced to build a surrogate model to approximate radiative heat transfer
solutions in a 2-D walled domain with participative gases. The originality of
this work lays in the adaptation of the inputs of the problem (gas and wall
properties) in order to fit with the CNN architecture, more commonly used for
image processing. Two precision datasets have been created with the classical
solver, ICARUS2D, that uses the discrete transfer radiation method with the
statistical narrow bands model. The performance of the CNN architecture is
compared to a more classical MLP architecture in terms of speed and accuracy.
Thanks to Optuna, all results are obtained using the optimized hyper parameters
networks. The results show a significant speedup with industrially acceptable
relative errors compared to the classical solver for both architectures.
Additionally, the CNN outperforms the MLP in terms of precision and is more
robust and stable to changes in hyper-parameters. A performance analysis on the
dataset size of the samples have also been carried out to gain a deeper
understanding of the model behavior.

</details>


<div id='q-bio.QM'></div>

# q-bio.QM [[Back]](#toc)

### [403] [CaliciBoost: Performance-Driven Evaluation of Molecular Representations for Caco-2 Permeability Prediction](https://arxiv.org/abs/2506.08059)
*Huong Van Le,Weibin Ren,Junhong Kim,Yukyung Yun,Young Bin Park,Young Jun Kim,Bok Kyung Han,Inho Choi,Jong IL Park,Hwi-Yeol Yun,Jae-Mun Choi*

Main category: q-bio.QM

TL;DR: 研究通过结合多种分子特征表示和AutoML技术，提升了Caco-2渗透性的预测准确性，发现3D描述符能显著降低误差。


<details>
  <summary>Details</summary>
Motivation: 提高早期药物发现中Caco-2渗透性预测的准确性和效率。

Method: 系统评估八种分子特征表示（如2D/3D描述符、指纹和深度学习嵌入）结合AutoML技术，使用两个数据集验证模型性能。

Result: PaDEL、Mordred和RDKit描述符表现最佳，AutoML模型CaliciBoost的MAE最优；3D描述符比2D降低15.73%的MAE。

Conclusion: AutoML在ADMET建模中高效，为数据有限的任务提供了特征选择指导。

Abstract: Caco-2 permeability serves as a critical in vitro indicator for predicting
the oral absorption of drug candidates during early-stage drug discovery. To
enhance the accuracy and efficiency of computational predictions, we
systematically investigated the impact of eight molecular feature
representation types including 2D/3D descriptors, structural fingerprints, and
deep learning-based embeddings combined with automated machine learning
techniques to predict Caco-2 permeability. Using two datasets of differing
scale and diversity (TDC benchmark and curated OCHEM data), we assessed model
performance across representations and identified PaDEL, Mordred, and RDKit
descriptors as particularly effective for Caco-2 prediction. Notably, the
AutoML-based model CaliciBoost achieved the best MAE performance. Furthermore,
for both PaDEL and Mordred representations, the incorporation of 3D descriptors
resulted in a 15.73% reduction in MAE compared to using 2D features alone, as
confirmed by feature importance analysis. These findings highlight the
effectiveness of AutoML approaches in ADMET modeling and offer practical
guidance for feature selection in data-limited prediction tasks.

</details>


### [404] [Protriever: End-to-End Differentiable Protein Homology Search for Fitness Prediction](https://arxiv.org/abs/2506.08954)
*Ruben Weitzman,Peter Mørch Groth,Lood Van Niekerk,Aoi Otani,Yarin Gal,Debora Marks,Pascal Notin*

Main category: q-bio.QM

TL;DR: Protriever是一个端到端的可微分框架，用于检索同源蛋白质序列并同时训练目标任务，相比传统MSA方法更快且性能更优。


<details>
  <summary>Details</summary>
Motivation: 传统基于MSA的同源序列检索方法计算成本高，且对高度分化的序列或复杂插入/删除模式效果不佳，Protriever旨在解决这些问题。

Method: Protriever通过可微分框架学习检索相关同源序列，并结合目标任务训练，支持高效向量搜索。

Result: 在蛋白质适应性预测任务中，Protriever性能优于传统MSA方法，且速度快两个数量级。

Conclusion: Protriever提供了一种可扩展的替代方案，适用于不同检索策略和蛋白质数据库。

Abstract: Retrieving homologous protein sequences is essential for a broad range of
protein modeling tasks such as fitness prediction, protein design, structure
modeling, and protein-protein interactions. Traditional workflows have relied
on a two-step process: first retrieving homologs via Multiple Sequence
Alignments (MSA), then training models on one or more of these alignments.
However, MSA-based retrieval is computationally expensive, struggles with
highly divergent sequences or complex insertions & deletions patterns, and
operates independently of the downstream modeling objective. We introduce
Protriever, an end-to-end differentiable framework that learns to retrieve
relevant homologs while simultaneously training for the target task. When
applied to protein fitness prediction, Protriever achieves state-of-the-art
performance compared to sequence-based models that rely on MSA-based homolog
retrieval, while being two orders of magnitude faster through efficient vector
search. Protriever is both architecture- and task-agnostic, and can flexibly
adapt to different retrieval strategies and protein databases at inference time
-- offering a scalable alternative to alignment-centric approaches.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [405] [A Metrics-Oriented Architectural Model to Characterize Complexity on Machine Learning-Enabled Systems](https://arxiv.org/abs/2506.08153)
*Renato Cordeiro Ferreira*

Main category: cs.SE

TL;DR: 研究提出了一种基于指标的架构模型，用于管理ML系统的复杂性，并支持架构决策。


<details>
  <summary>Details</summary>
Motivation: 探讨复杂性如何影响ML系统，并寻求有效的管理方法。

Method: 扩展参考架构以描述ML系统并收集指标。

Result: 初步展示了基于指标的架构模型的第一步。

Conclusion: 该模型为ML系统的初始设计和增长提供了指导。

Abstract: How can the complexity of ML-enabled systems be managed effectively? The goal
of this research is to investigate how complexity affects ML-Enabled Systems
(MLES). To address this question, this research aims to introduce a
metrics-based architectural model to characterize the complexity of MLES. The
goal is to support architectural decisions, providing a guideline for the
inception and growth of these systems. This paper showcases the first step for
creating the metrics-based architectural model: an extension of a reference
architecture that can describe MLES to collect their metrics.

</details>


### [406] [Worst-Case Symbolic Constraints Analysis and Generalisation with Large Language Models](https://arxiv.org/abs/2506.08171)
*Daniel Koh,Yannic Noller,Corina S. Pasareanu,Adrians Skapars,Youcheng Sun*

Main category: cs.SE

TL;DR: 论文探讨了大型语言模型（LLMs）在复杂符号推理任务中的能力，特别是通过符号约束分析来理解程序的最坏执行情况。


<details>
  <summary>Details</summary>
Motivation: 尽管LLMs在代码生成等任务中表现优异，但其在复杂符号推理任务中的应用尚未充分探索。本文旨在填补这一空白，连接LLMs与符号推理方法。

Method: 通过定义最坏情况符号约束分析任务，评估现有LLMs的表现，并利用SMT约束求解和专门设计的数据集进行符号推理引导的微调。

Result: 实验表明，经过微调的3B模型WARP-1.0-3B在性能上超越同类甚至更大规模的基线模型，能够准确恢复算法最坏行为的约束。

Conclusion: LLMs具备深入符号推理的潜力，支持神经网络学习与形式化方法的更紧密集成。

Abstract: Large language models (LLMs) have been successfully applied to a variety of
coding tasks, including code generation, completion, and repair. However, more
complex symbolic reasoning tasks remain largely unexplored by LLMs. This paper
investigates the capacity of LLMs to reason about worst-case executions in
programs through symbolic constraints analysis, aiming to connect LLMs and
symbolic reasoning approaches. Specifically, we define and address the problem
of worst-case symbolic constraints analysis as a measure to assess the
comprehension of LLMs. We evaluate the performance of existing LLMs on this
novel task and further improve their capabilities through symbolic
reasoning-guided fine-tuning, grounded in SMT (Satisfiability Modulo Theories)
constraint solving and supported by a specially designed dataset of symbolic
constraints. Experimental results show that our solver-aligned model,
WARP-1.0-3B, consistently surpasses size-matched and even much larger
baselines, demonstrating that a 3B LLM can recover the very constraints that
pin down an algorithm's worst-case behaviour through reinforcement learning
methods. These findings suggest that LLMs are capable of engaging in deeper
symbolic reasoning, supporting a closer integration between neural
network-based learning and formal methods for rigorous program analysis.

</details>


### [407] [Repeton: Structured Bug Repair with ReAct-Guided Patch-and-Test Cycles](https://arxiv.org/abs/2506.08173)
*Nguyen Phu Vinh,Anh Chung Hoang,Chris Ngo,Truong-Son Hy*

Main category: cs.SE

TL;DR: Repeton是一个开源框架，利用LLMs实现精确的代码操作，通过结构化补丁测试流程提升软件工程任务的效率和可解释性。


<details>
  <summary>Details</summary>
Motivation: LLMs在代码生成和理解方面表现强大，但在复杂软件工程任务中精度和可解释性不足。

Method: 采用结构化补丁测试流程，迭代诊断问题、提出代码修改并通过自动化测试验证。

Result: 在SWE-bench Lite基准测试中，Repeton在补丁有效性和可解释性上优于RAG方法。

Conclusion: Repeton通过模块化、可验证的流程，为可扩展和透明的自主调试提供了实用路径。

Abstract: Large Language Models (LLMs) have shown strong capabilities in code
generation and comprehension, yet their application to complex software
engineering tasks often suffers from low precision and limited
interpretability. We present Repeton, a fully open-source framework that
leverages LLMs for precise and automated code manipulation in real-world Git
repositories. Rather than generating holistic fixes, Repeton operates through a
structured patch-and-test pipeline: it iteratively diagnoses issues, proposes
code changes, and validates each patch through automated testing. This stepwise
process is guided by lightweight heuristics and development tools, avoiding
reliance on embedding-based retrieval systems. Evaluated on the SWE-bench Lite
benchmark, our method shows good performance compared to RAG-based methods in
both patch validity and interpretability. By decomposing software engineering
tasks into modular, verifiable stages, Repeton provides a practical path toward
scalable and transparent autonomous debugging.

</details>


### [408] [Understanding Software Engineering Agents Through the Lens of Traceability: An Empirical Study](https://arxiv.org/abs/2506.08311)
*Ira Ceka,Saurabh Pujar,Shyam Ramji,Luca Buratti,Gail Kaiser,Baishakhi Ray*

Main category: cs.SE

TL;DR: 该论文首次系统研究了软件工程代理（SWE代理）的行为，通过执行轨迹分析其决策路径，提出了分类法并揭示了核心组件对代理成功的影响。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型（LLM）的发展，SWE代理在自动化软件任务中表现出强大能力，但其内部决策机制尚不明确，研究旨在提升代理的可靠性和效率。

Method: 通过执行轨迹分析五种代表性代理的决策路径，提出分类法，并深入研究核心组件（如错误定位、补丁生成和测试生成）及其影响。

Result: 研究发现测试生成对补丁成功的关键作用，并首次大规模比较代理生成与开发者编写的补丁，揭示了结构和风格差异。

Conclusion: 研究为代理设计提供了新见解，有助于开发更高效且更符合人类开发实践的代理。

Abstract: With the advent of large language models (LLMs), software engineering agents
(SWE agents) have emerged as a powerful paradigm for automating a range of
software tasks -- from code generation and repair to test case synthesis. These
agents operate autonomously by interpreting user input and responding to
environmental feedback. While various agent architectures have demonstrated
strong empirical performance, the internal decision-making worfklows that drive
their behavior remain poorly understood. Deeper insight into these workflows
hold promise for improving both agent reliability and efficiency. In this work,
we present the first systematic study of SWE agent behavior through the lens of
execution traces. Our contributions are as follows: (1) we propose the first
taxonomy of decision-making pathways across five representative agents; (2)
using this taxonomy, we identify three core components essential to agent
success -- bug localization, patch generation, and reproduction test generation
-- and study each in depth; (3) we study the impact of test generation on
successful patch production; and analyze strategies that can lead to successful
test generation; (4) we further conduct the first large-scale code clone
analysis comparing agent-generated and developer-written patches and provide a
qualitative study revealing structural and stylistic differences in patch
content. Together, these findings offer novel insights into agent design and
open avenues for building agents that are both more effective and more aligned
with human development practices.

</details>


### [409] [Do Generative AI Tools Ensure Green Code? An Investigative Study](https://arxiv.org/abs/2506.08790)
*Samarth Sikand,Rohit Mehra,Vibhu Saujanya Sharma,Vikrant Kaulgud,Sanjay Podder,Adam P. Burden*

Main category: cs.SE

TL;DR: 本文研究了AI生成代码的可持续性，发现主流工具（如ChatGPT、BARD、Copilot）生成的代码普遍不符合绿色编码实践，需进一步研究改进。


<details>
  <summary>Details</summary>
Motivation: 随着生成式AI工具的普及，其代码生成能力被广泛使用，但缺乏对其可持续性（如环境影响）的研究。

Method: 通过分析三种流行生成式AI工具（ChatGPT、BARD、Copilot）生成的代码，评估其是否符合可持续编码实践。

Result: 结果显示，这些工具生成的代码在多数情况下不符合绿色编码标准。

Conclusion: 需进一步研究并提出改进策略，以提高AI生成代码的可持续性。

Abstract: Software sustainability is emerging as a primary concern, aiming to optimize
resource utilization, minimize environmental impact, and promote a greener,
more resilient digital ecosystem. The sustainability or "greenness" of software
is typically determined by the adoption of sustainable coding practices. With a
maturing ecosystem around generative AI, many software developers now rely on
these tools to generate code using natural language prompts. Despite their
potential advantages, there is a significant lack of studies on the
sustainability aspects of AI-generated code. Specifically, how environmentally
friendly is the AI-generated code based upon its adoption of sustainable coding
practices? In this paper, we present the results of an early investigation into
the sustainability aspects of AI-generated code across three popular generative
AI tools - ChatGPT, BARD, and Copilot. The results highlight the default
non-green behavior of tools for generating code, across multiple rules and
scenarios. It underscores the need for further in-depth investigations and
effective remediation strategies.

</details>


### [410] [On The Impact of Merge Request Deviations on Code Review Practices](https://arxiv.org/abs/2506.08860)
*Samah Kansab,Francis Bordeleau,Ali Tizghadam*

Main category: cs.SE

TL;DR: 论文研究了代码审查中的偏差现象，提出了一种检测方法，并证明了其对机器学习模型的影响。


<details>
  <summary>Details</summary>
Motivation: 工业中的合并请求（MR）流程常偏离标准化审查，导致分析偏差和机器学习模型失效。

Method: 识别了七类偏差，并提出了一种少样本学习检测方法（准确率91%）。

Result: 排除偏差后，预测审查完成时间的ML模型性能提升53.33%（最高2.25倍），特征重要性显著变化。

Conclusion: 研究为优化审查流程和提升分析可靠性提供了实践指导。

Abstract: Code review is a key practice in software engineering, ensuring quality and
collaboration. However, industrial Merge Request (MR) workflows often deviate
from standardized review processes, with many MRs serving non-review purposes
(e.g., drafts, rebases, or dependency updates). We term these cases deviations
and hypothesize that ignoring them biases analytics and undermines ML models
for review analysis.
  We identify seven deviation categories, occurring in 37.02% of MRs, and
propose a few-shot learning detection method (91% accuracy). By excluding
deviations, ML models predicting review completion time improve performance in
53.33% of cases (up to 2.25x) and exhibit significant shifts in feature
importance (47% overall, 60% top-*k*).
  Our contributions include: (1) a taxonomy of MR deviations, (2) an AI-driven
detection approach, and (3) empirical evidence of their impact on ML-based
review analytics. This work aids practitioners in optimizing review efforts and
ensuring reliable insights.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [411] [PerfTracker: Online Performance Troubleshooting for Large-scale Model Training in Production](https://arxiv.org/abs/2506.08528)
*Yu Guan,Zhiyu Yin,Haoyu Chen,Sheng Cheng,Chaojie Yang,Tianyin Xu,Yang Zhang,Hanyu Zhao,Yong Li,Dennis Cai,Ennan Zhai*

Main category: cs.DC

TL;DR: PerfTracker是一个在线性能诊断系统，用于大规模模型训练中的性能问题排查。


<details>
  <summary>Details</summary>
Motivation: 现代GPU集群规模庞大，软硬件交互复杂，传统方法难以应对大规模模型训练的性能问题。

Method: PerfTracker通过细粒度分析和差分可观测性，定位硬件和软件层面的性能问题。

Result: 已部署于万级GPU集群，成功诊断多种复杂性能问题。

Conclusion: PerfTracker为大规模模型训练提供了高效的性能问题解决方案。

Abstract: Troubleshooting performance problems of large model training (LMT) is
immensely challenging, due to unprecedented scales of modern GPU clusters, the
complexity of software-hardware interactions, and the data intensity of the
training process. Existing troubleshooting approaches designed for traditional
distributed systems or datacenter networks fall short and can hardly apply to
real-world training systems. In this paper, we present PerfTracker, the first
online troubleshooting system utilizing fine-grained profiling, to diagnose
performance issues of large-scale model training in production. PerfTracker can
diagnose performance issues rooted in both hardware (e.g., GPUs and their
interconnects) and software (e.g., Python functions and GPU operations). It
scales to LMT on modern GPU clusters. PerfTracker effectively summarizes
runtime behavior patterns of fine-grained LMT functions via online profiling,
and leverages differential observability to localize the root cause with
minimal production impact. PerfTracker has been deployed as a production
service for large-scale GPU clusters of O(10, 000) GPUs (product homepage
https://help.aliyun.com/zh/pai/user-guide/perftracker-online-performance-analysis-diagnostic-tool).
It has been used to diagnose a variety of difficult performance issues.

</details>


<div id='q-fin.ST'></div>

# q-fin.ST [[Back]](#toc)

### [412] [EDINET-Bench: Evaluating LLMs on Complex Financial Tasks using Japanese Financial Statements](https://arxiv.org/abs/2506.08762)
*Issa Sugiura,Takashi Ishida,Taro Makino,Chieko Tazuke,Takanori Nakagawa,Kosuke Nakago,David Ha*

Main category: q-fin.ST

TL;DR: 论文介绍了EDINET-Bench，一个开源日本金融基准数据集，用于评估大语言模型（LLM）在财务任务中的表现，填补了该领域数据稀缺的空白。


<details>
  <summary>Details</summary>
Motivation: 日本金融数据的稀缺性阻碍了学术创新和LLM在金融分析中的发展，需要构建一个挑战性的基准数据集。

Method: 通过从日本EDINET系统下载过去10年的年报，自动标注任务标签，构建EDINET-Bench数据集。

Result: 实验显示，即使是先进的LLM在欺诈检测和盈利预测等任务中表现仅略优于逻辑回归，表明LLM在金融领域的应用仍面临挑战。

Conclusion: EDINET-Bench的发布旨在促进LLM在金融领域的研究，并强调领域特定适应的重要性。

Abstract: Financial analysis presents complex challenges that could leverage large
language model (LLM) capabilities. However, the scarcity of challenging
financial datasets, particularly for Japanese financial data, impedes academic
innovation in financial analytics. As LLMs advance, this lack of accessible
research resources increasingly hinders their development and evaluation in
this specialized domain. To address this gap, we introduce EDINET-Bench, an
open-source Japanese financial benchmark designed to evaluate the performance
of LLMs on challenging financial tasks including accounting fraud detection,
earnings forecasting, and industry prediction. EDINET-Bench is constructed by
downloading annual reports from the past 10 years from Japan's Electronic
Disclosure for Investors' NETwork (EDINET) and automatically assigning labels
corresponding to each evaluation task. Our experiments reveal that even
state-of-the-art LLMs struggle, performing only slightly better than logistic
regression in binary classification for fraud detection and earnings
forecasting. These results highlight significant challenges in applying LLMs to
real-world financial applications and underscore the need for domain-specific
adaptation. Our dataset, benchmark construction code, and evaluation code is
publicly available to facilitate future research in finance with LLMs.

</details>


<div id='cond-mat.mtrl-sci'></div>

# cond-mat.mtrl-sci [[Back]](#toc)

### [413] [Domain Switching on the Pareto Front: Multi-Objective Deep Kernel Learning in Automated Piezoresponse Force Microscopy](https://arxiv.org/abs/2506.08073)
*Yu Liu,Utkarsh Pratiush,Kamyar Barakati,Hiroshi Funakubo,Ching-Che Lin,Jaegyu Kim,Lane W. Martin,Sergei V. Kalinin*

Main category: cond-mat.mtrl-sci

TL;DR: 提出了一种多目标核学习工作流，用于从高分辨率成像数据中推断微观结构规则，揭示了畴壁构型和局部开关动力学的关系。


<details>
  <summary>Details</summary>
Motivation: 研究铁电极化开关行为对复杂微观结构特征的依赖性，传统方法难以系统探索。

Method: 采用多目标核学习工作流，结合自动压电力显微镜实验，分析畴壁构型和缺陷分布对极化反转的影响。

Result: 揭示了特定畴壁几何和缺陷分布如何调制极化反转，并将抽象奖励函数映射到物理可解释的描述符。

Conclusion: 该方法不仅支持高通量主动学习，还为微观结构控制开关现象提供了机制性见解，适用于多种复杂设计空间。

Abstract: Ferroelectric polarization switching underpins the functional performance of
a wide range of materials and devices, yet its dependence on complex local
microstructural features renders systematic exploration by manual or grid-based
spectroscopic measurements impractical. Here, we introduce a multi-objective
kernel-learning workflow that infers the microstructural rules governing
switching behavior directly from high-resolution imaging data. Applied to
automated piezoresponse force microscopy (PFM) experiments, our framework
efficiently identifies the key relationships between domain-wall configurations
and local switching kinetics, revealing how specific wall geometries and defect
distributions modulate polarization reversal. Post-experiment analysis projects
abstract reward functions, such as switching ease and domain symmetry, onto
physically interpretable descriptors including domain configuration and
proximity to boundaries. This enables not only high-throughput active learning,
but also mechanistic insight into the microstructural control of switching
phenomena. While demonstrated for ferroelectric domain switching, our approach
provides a powerful, generalizable tool for navigating complex,
non-differentiable design spaces, from structure-property correlations in
molecular discovery to combinatorial optimization across diverse imaging
modalities.

</details>


### [414] [Mic-hackathon 2024: Hackathon on Machine Learning for Electron and Scanning Probe Microscopy](https://arxiv.org/abs/2506.08423)
*Utkarsh Pratiush,Austin Houston,Kamyar Barakati,Aditya Raghavan,Dasol Yoon,Harikrishnan KP,Zhaslan Baraissov,Desheng Ma,Samuel S. Welborn,Mikolaj Jakowski,Shawn-Patrick Barhorst,Alexander J. Pattison,Panayotis Manganaris,Sita Sirisha Madugula,Sai Venkata Gayathri Ayyagari,Vishal Kennedy,Ralph Bulanadi,Michelle Wang,Kieran J. Pang,Ian Addison-Smith,Willy Menacho,Horacio V. Guzman,Alexander Kiefer,Nicholas Furth,Nikola L. Kolev,Mikhail Petrov,Viktoriia Liu,Sergey Ilyev,Srikar Rairao,Tommaso Rodani,Ivan Pinto-Huguet,Xuli Chen,Josep Cruañes,Marta Torrens,Jovan Pomar,Fanzhi Su,Pawan Vedanti,Zhiheng Lyu,Xingzhi Wang,Lehan Yao,Amir Taqieddin,Forrest Laskowski,Xiangyu Yin,Yu-Tsun Shao,Benjamin Fein-Ashley,Yi Jiang,Vineet Kumar,Himanshu Mishra,Yogesh Paul,Adib Bazgir,Rama chandra Praneeth Madugula,Yuwen Zhang,Pravan Omprakash,Jian Huang,Eric Montufar-Morales,Vivek Chawla,Harshit Sethi,Jie Huang,Lauri Kurki,Grace Guinan,Addison Salvador,Arman Ter-Petrosyan,Madeline Van Winkle,Steven R. Spurgeon,Ganesh Narasimha,Zijie Wu,Richard Liu,Yongtao Liu,Boris Slautin,Andrew R Lupini,Rama Vasudevan,Gerd Duscher,Sergei V. Kalinin*

Main category: cond-mat.mtrl-sci

TL;DR: 显微镜数据管理存在标准化不足和ML应用受限的问题，通过黑客马拉松促进ML与显微镜社区合作，推动标准化工作流和数据集开发。


<details>
  <summary>Details</summary>
Motivation: 显微镜数据虽丰富但格式不一，ML应用受限，需解决数据管理和分析效率问题。

Method: 通过黑客马拉松促进ML与显微镜专家合作，开发标准化代码、数据集和数字孪生显微镜。

Result: 生成基准数据集和数字孪生显微镜，代码开源。

Conclusion: 黑客马拉松有效推动ML与显微镜领域融合，为未来标准化和高效分析奠定基础。

Abstract: Microscopy is a primary source of information on materials structure and
functionality at nanometer and atomic scales. The data generated is often
well-structured, enriched with metadata and sample histories, though not always
consistent in detail or format. The adoption of Data Management Plans (DMPs) by
major funding agencies promotes preservation and access. However, deriving
insights remains difficult due to the lack of standardized code ecosystems,
benchmarks, and integration strategies. As a result, data usage is inefficient
and analysis time is extensive. In addition to post-acquisition analysis, new
APIs from major microscope manufacturers enable real-time, ML-based analytics
for automated decision-making and ML-agent-controlled microscope operation.
Yet, a gap remains between the ML and microscopy communities, limiting the
impact of these methods on physics, materials discovery, and optimization.
Hackathons help bridge this divide by fostering collaboration between ML
researchers and microscopy experts. They encourage the development of novel
solutions that apply ML to microscopy, while preparing a future workforce for
instrumentation, materials science, and applied ML. This hackathon produced
benchmark datasets and digital twins of microscopes to support community growth
and standardized workflows. All related code is available at GitHub:
https://github.com/KalininGroup/Mic-hackathon-2024-codes-publication/tree/1.0.0.1

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [415] [GradEscape: A Gradient-Based Evader Against AI-Generated Text Detectors](https://arxiv.org/abs/2506.08188)
*Wenlong Meng,Shuguo Fan,Chengkun Wei,Min Chen,Yuwei Li,Yuanchao Zhang,Zhikun Zhang,Wenzhi Chen*

Main category: cs.CR

TL;DR: GradEscape是一种基于梯度的攻击方法，针对AI生成文本检测器，通过加权嵌入和反馈优化实现高效攻击，并在实验中表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 解决文本离散性导致的不可微计算问题，以及检测器和攻击模型之间的分词器不匹配问题。

Method: 引入加权嵌入和反馈优化，采用预热启动攻击模型，结合分词器推断和模型提取技术。

Result: 在多个数据集和语言模型上表现优异，参数效率高，成功应用于商业检测器。

Conclusion: 揭示了训练数据中文本表达风格的差异是主要漏洞，并提出了防御策略，开源了GradEscape以促进更鲁棒的检测器开发。

Abstract: In this paper, we introduce GradEscape, the first gradient-based evader
designed to attack AI-generated text (AIGT) detectors. GradEscape overcomes the
undifferentiable computation problem, caused by the discrete nature of text, by
introducing a novel approach to construct weighted embeddings for the detector
input. It then updates the evader model parameters using feedback from victim
detectors, achieving high attack success with minimal text modification. To
address the issue of tokenizer mismatch between the evader and the detector, we
introduce a warm-started evader method, enabling GradEscape to adapt to
detectors across any language model architecture. Moreover, we employ novel
tokenizer inference and model extraction techniques, facilitating effective
evasion even in query-only access.
  We evaluate GradEscape on four datasets and three widely-used language
models, benchmarking it against four state-of-the-art AIGT evaders.
Experimental results demonstrate that GradEscape outperforms existing evaders
in various scenarios, including with an 11B paraphrase model, while utilizing
only 139M parameters. We have successfully applied GradEscape to two real-world
commercial AIGT detectors. Our analysis reveals that the primary vulnerability
stems from disparity in text expression styles within the training data. We
also propose a potential defense strategy to mitigate the threat of AIGT
evaders. We open-source our GradEscape for developing more robust AIGT
detectors.

</details>


### [416] [How Good LLM-Generated Password Policies Are?](https://arxiv.org/abs/2506.08320)
*Vivek Vaidya,Aditya Patwardhan,Ashish Kundu*

Main category: cs.CR

TL;DR: 论文研究了LLM在网络安全访问控制系统中的应用，重点评估了LLM生成密码策略的一致性和准确性。


<details>
  <summary>Details</summary>
Motivation: LLM在自然语言处理中的广泛应用面临输出不一致和不可预测的挑战，尤其在安全关键领域如访问控制中。

Method: 采用两种方法：一是纯自然语言提示生成配置文件，二是结合官方文档作为基准，系统评估生成配置的合理性和一致性。

Result: 研究发现当前LLM在生成配置时存在显著挑战。

Conclusion: 研究为优化LLM在访问控制系统中的部署提供了重要见解。

Abstract: Generative AI technologies, particularly Large Language Models (LLMs), are
rapidly being adopted across industry, academia, and government sectors, owing
to their remarkable capabilities in natural language processing. However,
despite their strengths, the inconsistency and unpredictability of LLM outputs
present substantial challenges, especially in security-critical domains such as
access control. One critical issue that emerges prominently is the consistency
of LLM-generated responses, which is paramount for ensuring secure and reliable
operations.
  In this paper, we study the application of LLMs within the context of
Cybersecurity Access Control Systems. Specifically, we investigate the
consistency and accuracy of LLM-generated password policies, translating
natural language prompts into executable pwquality.conf configuration files.
Our experimental methodology adopts two distinct approaches: firstly, we
utilize pre-trained LLMs to generate configuration files purely from natural
language prompts without additional guidance. Secondly, we provide these models
with official pwquality.conf documentation to serve as an informative baseline.
We systematically assess the soundness, accuracy, and consistency of these
AI-generated configurations. Our findings underscore significant challenges in
the current generation of LLMs and contribute valuable insights into refining
the deployment of LLMs in Access Control Systems.

</details>


### [417] [Your Agent Can Defend Itself against Backdoor Attacks](https://arxiv.org/abs/2506.08336)
*Li Changjiang,Liang Jiacheng,Cao Bochuan,Chen Jinghui,Wang Ting*

Main category: cs.CR

TL;DR: ReAgent是一种针对LLM代理后门攻击的新型防御方法，通过检测用户指令、代理规划和执行之间的一致性来识别潜在后门。


<details>
  <summary>Details</summary>
Motivation: LLM代理在训练和微调过程中面临后门攻击的安全风险，可能导致恶意操作。

Method: ReAgent采用两级检测方法：执行层面验证代理思维与行动的一致性；规划层面通过代理重建指令的能力检查一致性。

Result: ReAgent显著降低攻击成功率，例如在数据库操作任务中减少90%的攻击成功率。

Conclusion: 研究表明，可以利用被攻击的代理自身来缓解后门风险。

Abstract: Despite their growing adoption across domains, large language model
(LLM)-powered agents face significant security risks from backdoor attacks
during training and fine-tuning. These compromised agents can subsequently be
manipulated to execute malicious operations when presented with specific
triggers in their inputs or environments. To address this pressing risk, we
present ReAgent, a novel defense against a range of backdoor attacks on
LLM-based agents. Intuitively, backdoor attacks often result in inconsistencies
among the user's instruction, the agent's planning, and its execution. Drawing
on this insight, ReAgent employs a two-level approach to detect potential
backdoors. At the execution level, ReAgent verifies consistency between the
agent's thoughts and actions; at the planning level, ReAgent leverages the
agent's capability to reconstruct the instruction based on its thought
trajectory, checking for consistency between the reconstructed instruction and
the user's instruction. Extensive evaluation demonstrates ReAgent's
effectiveness against various backdoor attacks across tasks. For instance,
ReAgent reduces the attack success rate by up to 90\% in database operation
tasks, outperforming existing defenses by large margins. This work reveals the
potential of utilizing compromised agents themselves to mitigate backdoor
risks.

</details>


### [418] [WGLE:Backdoor-free and Multi-bit Black-box Watermarking for Graph Neural Networks](https://arxiv.org/abs/2506.08602)
*Tingzhi Li,Xuefeng Liu*

Main category: cs.CR

TL;DR: WGLE提出了一种新的GNN黑盒水印方法，通过LDDE嵌入多比特字符串作为所有权信息，避免了后门风险，并在多个数据集和架构上验证了其高效性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有GNN所有权验证方法（指纹和黑盒水印）存在计算成本高、后门风险或无法传递额外信息的问题，WGLE旨在解决这些挑战。

Method: WGLE基于LDDE（层间距离差异）量化节点特征与预测距离的差异，通过预定义边缘的LDDE值嵌入水印，避免影响主任务。

Result: 在六个数据集和GNN架构上，WGLE实现了100%的所有权验证准确率，平均保真度下降0.85%，且嵌入开销低。

Conclusion: WGLE是一种高效、安全且鲁棒的GNN水印方法，适用于实际部署。

Abstract: Graph Neural Networks (GNNs) are increasingly deployed in graph-related
applications, making ownership verification critical to protect their
intellectual property against model theft. Fingerprinting and black-box
watermarking are two main methods. However, the former relies on determining
model similarity, which is computationally expensive and prone to ownership
collisions after model post-processing such as model pruning or fine-tuning.
The latter embeds backdoors, exposing watermarked models to the risk of
backdoor attacks. Moreover, both methods enable ownership verification but do
not convey additional information. As a result, each distributed model requires
a unique trigger graph, and all trigger graphs must be used to query the
suspect model during verification. Multiple queries increase the financial cost
and the risk of detection.
  To address these challenges, this paper proposes WGLE, a novel black-box
watermarking paradigm for GNNs that enables embedding the multi-bit string as
the ownership information without using backdoors. WGLE builds on a key insight
we term Layer-wise Distance Difference on an Edge (LDDE), which quantifies the
difference between the feature distance and the prediction distance of two
connected nodes. By predefining positive or negative LDDE values for multiple
selected edges, WGLE embeds the watermark encoding the intended information
without introducing incorrect mappings that compromise the primary task. WGLE
is evaluated on six public datasets and six mainstream GNN architectures along
with state-of-the-art methods. The results show that WGLE achieves 100%
ownership verification accuracy, an average fidelity degradation of 0.85%,
comparable robustness against potential attacks, and low embedding overhead.
The code is available in the repository.

</details>


### [419] [Interpreting Agent Behaviors in Reinforcement-Learning-Based Cyber-Battle Simulation Platforms](https://arxiv.org/abs/2506.08192)
*Jared Claypoole,Steven Cheung,Ashish Gehani,Vinod Yegneswaran,Ahmad Ridley*

Main category: cs.CR

TL;DR: 论文分析了两个开源深度强化学习代理在CAGE Challenge 2中的表现，通过简化状态和动作空间以及追踪关键事件，揭示了防御和攻击代理的细粒度行为。


<details>
  <summary>Details</summary>
Motivation: 研究目的是通过分析代理在模拟网络防御挑战中的表现，提高对代理行为的可解释性，并评估其有效性。

Method: 通过简化复杂的状态和动作空间，并追踪关键事件（如渗透和清除事件），分析代理的行为模式和环境状态变化。

Result: 研究发现防御代理通常能在1-2个时间步内清除渗透，某些关键动作的效率低至40%-99%，而诱饵服务可阻止高达94%的特权访问攻击。

Conclusion: 论文总结了代理行为的有效性，并讨论了CAGE Challenge 4如何改进模拟环境的真实性。

Abstract: We analyze two open source deep reinforcement learning agents submitted to
the CAGE Challenge 2 cyber defense challenge, where each competitor submitted
an agent to defend a simulated network against each of several provided
rules-based attack agents. We demonstrate that one can gain interpretability of
agent successes and failures by simplifying the complex state and action spaces
and by tracking important events, shedding light on the fine-grained behavior
of both the defense and attack agents in each experimental scenario. By
analyzing important events within an evaluation episode, we identify patterns
in infiltration and clearing events that tell us how well the attacker and
defender played their respective roles; for example, defenders were generally
able to clear infiltrations within one or two timesteps of a host being
exploited. By examining transitions in the environment's state caused by the
various possible actions, we determine which actions tended to be effective and
which did not, showing that certain important actions are between 40% and 99%
ineffective. We examine how decoy services affect exploit success, concluding
for instance that decoys block up to 94% of exploits that would directly grant
privileged access to a host. Finally, we discuss the realism of the challenge
and ways that the CAGE Challenge 4 has addressed some of our concerns.

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [420] [Neural-Augmented Kelvinlet: Real-Time Soft Tissue Deformation with Multiple Graspers](https://arxiv.org/abs/2506.08043)
*Ashkan Shahbazi,Kyvia Pereira,Jon S. Heiselman,Elaheh Akbari,Annie C. Benson,Sepehr Seifi,Xinyuan Liu,Garrison L. Johnston,Erwin Terpstra,Anne Draaisma,Jan-Jaap Severes,Jie Ying Wu,Nabil Simaan,Michael L. Miga,Soheil Kolouri*

Main category: cs.GR

TL;DR: 本文提出了一种基于物理信息的神经模拟器，通过结合Kelvinlet先验和大规模FEM模拟，实现了实时、高精度的软组织变形模拟。


<details>
  <summary>Details</summary>
Motivation: 快速准确的软组织变形模拟对手术机器人和医学训练至关重要。

Method: 提出了一种结合Kelvinlet先验的神经模拟器，利用FEM模拟数据进行残差学习和正则化。

Result: 方法提高了神经网络预测的准确性和物理一致性，同时保持低延迟，适用于实时手术模拟。

Conclusion: Kelvinlet增强学习是一种高效策略，适用于手术中的实时物理感知软组织模拟。

Abstract: Fast and accurate simulation of soft tissue deformation is a critical factor
for surgical robotics and medical training. In this paper, we introduce a novel
physics-informed neural simulator that approximates soft tissue deformations in
a realistic and real-time manner. Our framework integrates Kelvinlet-based
priors into neural simulators, making it the first approach to leverage
Kelvinlets for residual learning and regularization in data-driven soft tissue
modeling. By incorporating large-scale Finite Element Method (FEM) simulations
of both linear and nonlinear soft tissue responses, our method improves neural
network predictions across diverse architectures, enhancing accuracy and
physical consistency while maintaining low latency for real-time performance.
We demonstrate the effectiveness of our approach by performing accurate
surgical maneuvers that simulate the use of standard laparoscopic tissue
grasping tools with high fidelity. These results establish Kelvinlet-augmented
learning as a powerful and efficient strategy for real-time, physics-aware soft
tissue simulation in surgical applications.

</details>


### [421] [A Real-time 3D Desktop Display](https://arxiv.org/abs/2506.08064)
*Livio Tenze,Enrique Canessa*

Main category: cs.GR

TL;DR: 扩展版的altiro3D C++库，支持从2D图像或视频流实时生成3D光场，利用MiDaS CNN提取深度图，并通过AI优化性能。


<details>
  <summary>Details</summary>
Motivation: 为从2D图像或视频流中实现无眼镜全息显示，提供更高效的3D体验。

Method: 使用MiDaS CNN从单张2D图像提取深度图，结合AI技术优化处理性能，支持实时3D光场合成。

Result: 扩展后的库能处理标准图像、视频流或桌面屏幕区域，并输出到光场3D设备。

Conclusion: altiro3D 2.0通过AI和CNN技术，实现了更灵活的3D内容生成和显示。

Abstract: A new extended version of the altiro3D C++ Library -- initially developed to
get glass-free holographic displays starting from 2D images -- is here
introduced aiming to deal with 3D video streams from either 2D webcam images or
flat video files. These streams are processed in real-time to synthesize
light-fields (in Native format) and feed realistic 3D experiences. The core
function needed to recreate multiviews consists on the use of MiDaS
Convolutional Neural Network (CNN), which allows to extract a depth map from a
single 2D image. Artificial Intelligence (AI) computing techniques are applied
to improve the overall performance of the extended altiro3D Library. Thus,
altiro3D can now treat standard images, video streams or screen portions of a
Desktop where other apps may be also running (like web browsers, video chats,
etc) and render them into 3D. To achieve the latter, a screen region need to be
selected in order to feed the output directly into a light-field 3D device such
as Looking Glass (LG) Portrait. In order to simplify the acquisition of a
Desktop screen area by the user, a multi-platform Graphical User Interface has
been also implemented. Sources available at:
https://github.com/canessae/altiro3D/releases/tag/2.0.0

</details>


### [422] [Generalizable Articulated Object Reconstruction from Casually Captured RGBD Videos](https://arxiv.org/abs/2506.08334)
*Weikun Peng,Jun Lv,Cewu Lu,Manolis Savva*

Main category: cs.GR

TL;DR: 论文提出了一种从手持相机拍摄的RGBD视频中重建关节物体的方法，解决了现有方法对数据采集要求高的问题。


<details>
  <summary>Details</summary>
Motivation: 关节物体在日常生活中普遍存在，但现有方法需要精心采集的数据，限制了其实际应用和扩展性。

Method: 采用粗到细的框架，从动态RGBD视频中推断关节参数并分割可移动部分。

Result: 在合成和真实数据集上显著优于现有方法，能够跨类别重建关节物体。

Conclusion: 该方法为关节物体的重建提供了一种实用、可扩展且通用的解决方案。

Abstract: Articulated objects are prevalent in daily life. Understanding their
kinematic structure and reconstructing them have numerous applications in
embodied AI and robotics. However, current methods require carefully captured
data for training or inference, preventing practical, scalable, and
generalizable reconstruction of articulated objects. We focus on reconstruction
of an articulated object from a casually captured RGBD video shot with a
hand-held camera. A casually captured video of an interaction with an
articulated object is easy to acquire at scale using smartphones. However, this
setting is quite challenging, as the object and camera move simultaneously and
there are significant occlusions as the person interacts with the object. To
tackle these challenges, we introduce a coarse-to-fine framework that infers
joint parameters and segments movable parts of the object from a dynamic RGBD
video. To evaluate our method under this new setting, we build a 20$\times$
larger synthetic dataset of 784 videos containing 284 objects across 11
categories. We compare our approach with existing methods that also take video
as input. Experiments show that our method can reconstruct synthetic and real
articulated objects across different categories from dynamic RGBD videos,
outperforming existing methods significantly.

</details>


### [423] [Complex-Valued Holographic Radiance Fields](https://arxiv.org/abs/2506.08350)
*Yicheng Zhan,Dong-Ha Shin,Seung-Hwan Baek,Kaan Akşit*

Main category: cs.GR

TL;DR: 提出了一种基于复值高斯基元的3D全息场景表示方法，显著提升了渲染速度。


<details>
  <summary>Details</summary>
Motivation: 为了在3D表示中完整建模光的振幅和相位特性，以支持物理上合理的渲染，尤其是全息显示。

Method: 通过复值高斯基元重新定义3D高斯泼溅，利用RGBD多视图图像直接优化复值高斯基元作为3D全息场景表示。

Result: 相比现有方法，速度提升30倍至10,000倍，同时保持图像质量。

Conclusion: 该方法为几何对齐、物理合理的全息场景表示迈出了第一步。

Abstract: Modeling the full properties of light, including both amplitude and phase, in
3D representations is crucial for advancing physically plausible rendering,
particularly in holographic displays. To support these features, we propose a
novel representation that optimizes 3D scenes without relying on
intensity-based intermediaries. We reformulate 3D Gaussian splatting with
complex-valued Gaussian primitives, expanding support for rendering with light
waves. By leveraging RGBD multi-view images, our method directly optimizes
complex-valued Gaussians as a 3D holographic scene representation. This
eliminates the need for computationally expensive hologram re-optimization.
Compared with state-of-the-art methods, our method achieves 30x-10,000x speed
improvements while maintaining on-par image quality, representing a first step
towards geometrically aligned, physically plausible holographic scene
representations.

</details>


### [424] [Fine-Grained Spatially Varying Material Selection in Images](https://arxiv.org/abs/2506.09023)
*Julia Guerrero-Viu,Michael Fischer,Iliyan Georgiev,Elena Garces,Diego Gutierrez,Belen Masia,Valentin Deschaintre*

Main category: cs.GR

TL;DR: 提出了一种基于视觉变换器（ViT）的材料选择方法，能够在光照和反射变化下实现稳健选择，并支持纹理和子纹理两级选择。


<details>
  <summary>Details</summary>
Motivation: 图像编辑中材料选择是关键步骤，但现有方法对光照和反射变化敏感，且缺乏多级选择能力。

Method: 利用ViT模型特征，提出多分辨率处理策略，并结合新构建的两级材料选择数据集（DuMaS）。

Result: 方法在纹理和子纹理级别上实现了更精细和稳定的选择效果，优于现有方法。

Conclusion: 该方法为图像编辑提供了更高效的材料选择工具，支持多级选择，且对光照和反射变化具有鲁棒性。

Abstract: Selection is the first step in many image editing processes, enabling faster
and simpler modifications of all pixels sharing a common modality. In this
work, we present a method for material selection in images, robust to lighting
and reflectance variations, which can be used for downstream editing tasks. We
rely on vision transformer (ViT) models and leverage their features for
selection, proposing a multi-resolution processing strategy that yields finer
and more stable selection results than prior methods. Furthermore, we enable
selection at two levels: texture and subtexture, leveraging a new two-level
material selection (DuMaS) dataset which includes dense annotations for over
800,000 synthetic images, both on the texture and subtexture levels.

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [425] [Addressing Pitfalls in Auditing Practices of Automatic Speech Recognition Technologies: A Case Study of People with Aphasia](https://arxiv.org/abs/2506.08846)
*Katelyn Xiaoying Mei,Anna Seo Gyeong Choi,Hilke Schellmann,Mona Sloane,Allison Koenecke*

Main category: cs.CY

TL;DR: 本文提出了一种更全面的自动语音识别（ASR）审计框架，解决了现有审计中的三个缺陷，并通过案例研究展示了其对ASR系统性能的影响。


<details>
  <summary>Details</summary>
Motivation: ASR系统在日常生活和工作中广泛应用，但现有审计方法未能充分考虑到语音和语言障碍者（如失语症患者）的需求，导致性能评估不全面。

Method: 通过案例研究分析了六种流行ASR系统对失语症患者的性能，提出了包括文本标准化、细分人口统计和多样化评估指标在内的审计框架。

Result: 研究发现，失语症患者的ASR性能普遍低于对照组，且现有审计方法掩盖了性能差异。

Conclusion: 呼吁采用更灵活的审计方法以适应快速变化的ASR技术，确保评估的公平性和全面性。

Abstract: Automatic Speech Recognition (ASR) has transformed daily tasks from video
transcription to workplace hiring. ASR systems' growing use warrants robust and
standardized auditing approaches to ensure automated transcriptions of high and
equitable quality. This is especially critical for people with speech and
language disorders (such as aphasia) who may disproportionately depend on ASR
systems to navigate everyday life. In this work, we identify three pitfalls in
existing standard ASR auditing procedures, and demonstrate how addressing them
impacts audit results via a case study of six popular ASR systems' performance
for aphasia speakers. First, audits often adhere to a single method of text
standardization during data pre-processing, which (a) masks variability in ASR
performance from applying different standardization methods, and (b) may not be
consistent with how users - especially those from marginalized speech
communities - would want their transcriptions to be standardized. Second,
audits often display high-level demographic findings without further
considering performance disparities among (a) more nuanced demographic
subgroups, and (b) relevant covariates capturing acoustic information from the
input audio. Third, audits often rely on a single gold-standard metric -- the
Word Error Rate -- which does not fully capture the extent of errors arising
from generative AI models, such as transcription hallucinations. We propose a
more holistic auditing framework that accounts for these three pitfalls, and
exemplify its results in our case study, finding consistently worse ASR
performance for aphasia speakers relative to a control group. We call on
practitioners to implement these robust ASR auditing practices that remain
flexible to the rapidly changing ASR landscape.

</details>


### [426] [WIP: Large Language Model-Enhanced Smart Tutor for Undergraduate Circuit Analysis](https://arxiv.org/abs/2506.08962)
*Liangliang Chen,Huiru Xie,Jacqueline Rohde,Ying Zhang*

Main category: cs.CY

TL;DR: 本文介绍了一款AI驱动的智能导师，用于为本科生电路分析课程提供作业评估和反馈，学生满意度达90.9%。


<details>
  <summary>Details</summary>
Motivation: 旨在通过智能导师提供个性化教学和反馈，同时收集学生数据以帮助教师实时了解学习难点。

Method: 设计了智能导师的核心组件，包括开放式问题回答和作业反馈生成，部署于微软Azure平台并实际应用于课程中。

Result: 90.9%的学生表示满意，初步数据分析帮助教师识别高频问题和学习难点。

Conclusion: 智能导师效果显著，未来将扩展至更多工程学科并优化技术。

Abstract: This research-to-practice work-in-progress (WIP) paper presents an AI-enabled
smart tutor designed to provide homework assessment and feedback for students
in an undergraduate circuit analysis course. We detail the tutor's design
philosophy and core components, including open-ended question answering and
homework feedback generation. The prompts are carefully crafted to optimize
responses across different problems. The smart tutor was deployed on the
Microsoft Azure platform and is currently in use in an undergraduate circuit
analysis course at the School of Electrical and Computer Engineering in a
large, public, research-intensive institution in the Southeastern United
States. Beyond offering personalized instruction and feedback, the tutor
collects student interaction data, which is summarized and shared with the
course instructor. To evaluate its effectiveness, we collected student
feedback, with 90.9% of responses indicating satisfaction with the tutor.
Additionally, we analyze a subset of collected data on preliminary circuit
analysis topics to assess tutor usage frequency for each problem and identify
frequently asked questions. These insights help instructors gain real-time
awareness of student difficulties, enabling more targeted classroom
instruction. In future work, we will release a full analysis once the complete
dataset is available after the Spring 2025 semester. We also explore the
potential applications of this smart tutor across a broader range of
engineering disciplines by developing improved prompts, diagram-recognition
methods, and database management strategies, which remain ongoing areas of
research.

</details>


### [427] [The World of AI: A Novel Approach to AI Literacy for First-year Engineering Students](https://arxiv.org/abs/2506.08041)
*Siddharth Siddharth,Brainerd Prince,Amol Harsh,Shreyas Ramachandran*

Main category: cs.CY

TL;DR: 为大一工科生设计的跨学科AI课程，涵盖技术与社会影响，提升学生对AI的理解与认知。


<details>
  <summary>Details</summary>
Motivation: 解决工科生缺乏AI基础知识及其社会影响认知的问题。

Method: 设计三个模块（行星、社会影响、职场），由工程与人文学科教师联合授课。

Result: 学生对AI环境影响的认知提升，并能提出公平性解决方案。

Conclusion: 跨学科课程设计有效提升学生对AI的理解和社会影响的认知。

Abstract: This work presents a novel course titled The World of AI designed for
first-year undergraduate engineering students with little to no prior exposure
to AI. The central problem addressed by this course is that engineering
students often lack foundational knowledge of AI and its broader societal
implications at the outset of their academic journeys. We believe the way to
address this gap is to design and deliver an interdisciplinary course that can
a) be accessed by first-year undergraduate engineering students across any
domain, b) enable them to understand the basic workings of AI systems sans
mathematics, and c) make them appreciate AI's far-reaching implications on our
lives. The course was divided into three modules co-delivered by faculty from
both engineering and humanities. The planetary module explored AI's dual role
as both a catalyst for sustainability and a contributor to environmental
challenges. The societal impact module focused on AI biases and concerns around
privacy and fairness. Lastly, the workplace module highlighted AI-driven job
displacement, emphasizing the importance of adaptation. The novelty of this
course lies in its interdisciplinary curriculum design and pedagogical
approach, which combines technical instruction with societal discourse. Results
revealed that students' comprehension of AI challenges improved across diverse
metrics like (a) increased awareness of AI's environmental impact, and (b)
efficient corrective solutions for AI fairness. Furthermore, it also indicated
the evolution in students' perception of AI's transformative impact on our
lives.

</details>


### [428] [Evaluation of Machine Learning Models in Student Academic Performance Prediction](https://arxiv.org/abs/2506.08047)
*A. G. R. Sandeepa,Sanka Mohottala*

Main category: cs.CY

TL;DR: 该研究探讨了机器学习方法在预测学生学业表现中的应用，发现多层感知机分类器（MLPC）表现最佳，测试集准确率达86.46%。


<details>
  <summary>Details</summary>
Motivation: 研究旨在利用机器学习预测学生学业表现，以帮助教育机构优化教学策略。

Method: 使用学生的行为、学术和人口统计数据，采用多层感知机分类器（MLPC）等标准机器学习模型，并结合特征选择方法。

Result: MLPC在测试集上最高准确率为86.46%，10折交叉验证平均准确率为79.58%。

Conclusion: 研究表明神经网络（如MLPC）是数据高效模型，特征选择对性能提升至关重要，可解释性方法有助于验证模型。

Abstract: This research investigates the use of machine learning methods to forecast
students' academic performance in a school setting. Students' data with
behavioral, academic, and demographic details were used in implementations with
standard classical machine learning models including multi-layer perceptron
classifier (MLPC). MLPC obtained 86.46% maximum accuracy for test set across
all implementations. Under 10-fold cross validation, MLPC obtained 79.58%
average accuracy for test set while for train set, it was 99.65%. MLP's better
performance over other machine learning models strongly suggest the potential
use of neural networks as data-efficient models. Feature selection approach
played a crucial role in improving the performance and multiple evaluation
approaches were used in order to compare with existing literature. Explainable
machine learning methods were utilized to demystify the black box models and to
validate the feature selection approach.

</details>


### [429] [Surgeons Awareness, Expectations, and Involvement with Artificial Intelligence: a Survey Pre and Post the GPT Era](https://arxiv.org/abs/2506.08258)
*Lorenzo Arboit,Dennis N. Schneider,Toby Collins,Daniel A. Hashimoto,Silvana Perretta,Bernard Dallemagne,Jacques Marescaux,EAES Working Group,Nicolas Padoy,Pietro Mascagni*

Main category: cs.CY

TL;DR: 研究通过2021和2024年的全球调查，分析了外科医生对AI的认知、期望和参与度，发现AI意识和课程参与度上升，但对基础概念仍有限。伦理问题突出，基础设施是主要障碍。


<details>
  <summary>Details</summary>
Motivation: 探讨AI在外科手术中的应用潜力及外科医生对其的认知变化。

Method: 2021和2024年两次全球横断面调查，评估AI意识、期望、参与度和伦理问题。

Result: AI意识和课程参与度显著提升，但基础概念理解有限；伦理问题受关注；基础设施是主要障碍。

Conclusion: 需通过教育、伦理框架和基础设施推动AI在外科手术中的成功应用。

Abstract: Artificial Intelligence (AI) is transforming medicine, with generative AI
models like ChatGPT reshaping perceptions of its potential. This study examines
surgeons' awareness, expectations, and involvement with AI in surgery through
comparative surveys conducted in 2021 and 2024. Two cross-sectional surveys
were distributed globally in 2021 and 2024, the first before an IRCAD webinar
and the second during the annual EAES meeting. The surveys assessed
demographics, AI awareness, expectations, involvement, and ethics (2024 only).
The surveys collected a total of 671 responses from 98 countries, 522 in 2021
and 149 in 2024. Awareness of AI courses rose from 14.5% in 2021 to 44.6% in
2024, while course attendance increased from 12.9% to 23%. Despite this,
familiarity with foundational AI concepts remained limited. Expectations for
AI's role shifted in 2024, with hospital management gaining relevance. Ethical
concerns gained prominence, with 87.2% of 2024 participants emphasizing
accountability and transparency. Infrastructure limitations remained the
primary obstacle to implementation. Interdisciplinary collaboration and
structured training were identified as critical for successful AI adoption.
Optimism about AI's transformative potential remained high, with 79.9% of
respondents believing AI would positively impact surgery and 96.6% willing to
integrate AI into their clinical practice. Surgeons' perceptions of AI are
evolving, driven by the rise of generative AI and advancements in surgical data
science. While enthusiasm for integration is strong, knowledge gaps and
infrastructural challenges persist. Addressing these through education, ethical
frameworks, and infrastructure development is essential.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [430] [Solving excited states for long-range interacting trapped ions with neural networks](https://arxiv.org/abs/2506.08594)
*Yixuan Ma,Chang Liu,Weikang Li,Shun-Yao Zhang,L. -M. Duan,Yukai Wu,Dong-Ling Deng*

Main category: quant-ph

TL;DR: 提出了一种基于神经网络的算法（NQES），用于高效计算量子多体系统的低激发态，无需显式正交化，适用于高维系统。


<details>
  <summary>Details</summary>
Motivation: 强相互作用量子多体系统的激发态计算具有基础重要性，但由于希尔伯特空间维度的指数增长，传统方法面临挑战。

Method: 开发了NQES算法，通过神经网络同时输出多个低激发态，并验证其在多种模型（如Haldane-Shastry模型和长程相互作用离子晶体系统）中的有效性。

Result: NQES算法成功计算了多个激发态及其观测值，揭示了长程相互作用系统的能隙标度和关联特征，与实验观测一致。

Conclusion: NQES算法为量子多体系统的激发态计算提供了一种可扩展且高效的方法，具有广泛的应用潜力。

Abstract: The computation of excited states in strongly interacting quantum many-body
systems is of fundamental importance. Yet, it is notoriously challenging due to
the exponential scaling of the Hilbert space dimension with the system size.
Here, we introduce a neural network-based algorithm that can simultaneously
output multiple low-lying excited states of a quantum many-body spin system in
an accurate and efficient fashion. This algorithm, dubbed the neural quantum
excited-state (NQES) algorithm, requires no explicit orthogonalization of the
states and is generally applicable to higher dimensions. We demonstrate,
through concrete examples including the Haldane-Shastry model with all-to-all
interactions, that the NQES algorithm is capable of efficiently computing
multiple excited states and their related observable expectations. In addition,
we apply the NQES algorithm to two classes of long-range interacting
trapped-ion systems in a two-dimensional Wigner crystal. For non-decaying
all-to-all interactions with alternating signs, our computed low-lying excited
states bear spatial correlation patterns similar to those of the ground states,
which closely match recent experimental observations that the
quasi-adiabatically prepared state accurately reproduces analytical
ground-state correlations. For a system of up to 300 ions with power-law
decaying antiferromagnetic interactions, we successfully uncover its gap
scaling and correlation features. Our results establish a scalable and
efficient algorithm for computing excited states of interacting quantum
many-body systems, which holds potential applications ranging from benchmarking
quantum devices to photoisomerization.

</details>


### [431] [Systematic and Efficient Construction of Quadratic Unconstrained Binary Optimization Forms for High-order and Dense Interactions](https://arxiv.org/abs/2506.08448)
*Hyakka Nakada,Shu Tanaka*

Main category: quant-ph

TL;DR: 量子退火（QA）能高效解决组合优化问题，但高阶问题需转化为QUBO形式。本文提出一种基于ReLU基函数的二次化方法，适用于复杂机器学习问题，并通过数值和理论验证。


<details>
  <summary>Details</summary>
Motivation: 现有二次化方法难以处理复杂机器学习问题中的强非线性和密集交互，限制了QA的广泛应用。

Method: 利用ReLU基函数建模目标函数，将其转化为等效的二次多项式表示，并结合QA设计新的黑盒优化方案。

Result: 通过数值和理论验证了方法的可行性，并展示了其在黑盒优化中的潜力。

Conclusion: 提出的方法扩展了QA在复杂机器学习问题中的应用，为黑盒优化提供了新思路。

Abstract: Quantum Annealing (QA) can efficiently solve combinatorial optimization
problems whose objective functions are represented by Quadratic Unconstrained
Binary Optimization (QUBO) formulations. For broader applicability of QA,
quadratization methods are used to transform higher-order problems into QUBOs.
However, quadratization methods for complex problems involving Machine Learning
(ML) remain largely unknown. In these problems, strong nonlinearity and dense
interactions prevent conventional methods from being applied. Therefore, we
model target functions by the sum of rectified linear unit bases, which not
only have the ability of universal approximation, but also have an equivalent
quadratic-polynomial representation. In this study, the proof of concept is
verified both numerically and analytically. In addition, by combining QA with
the proposed quadratization, we design a new black-box optimization scheme, in
which ML surrogate regressors are inputted to QA after the quadratization
process.

</details>


### [432] [The interplay of robustness and generalization in quantum machine learning](https://arxiv.org/abs/2506.08455)
*Julian Berberich,Tobias Fellner,Christian Holm*

Main category: quant-ph

TL;DR: 探讨量子机器学习中对抗鲁棒性与泛化性的关系，提出基于Lipschitz界的正则化训练方法。


<details>
  <summary>Details</summary>
Motivation: 研究量子机器学习中对抗鲁棒性与泛化性的相互作用，填补现有文献的空白。

Method: 利用Lipschitz界量化鲁棒性和泛化性，提出基于模型参数的正则化训练方法。

Result: 理论结果在时间序列分析中得到验证，展示了可训练数据编码策略的重要性。

Conclusion: 正则化训练方法可提升量子模型的鲁棒性和泛化性，数据编码策略是关键。

Abstract: While adversarial robustness and generalization have individually received
substantial attention in the recent literature on quantum machine learning,
their interplay is much less explored. In this chapter, we address this
interplay for variational quantum models, which were recently proposed as
function approximators in supervised learning. We discuss recent results
quantifying both robustness and generalization via Lipschitz bounds, which
explicitly depend on model parameters. Thus, they give rise to a
regularization-based training approach for robust and generalizable quantum
models, highlighting the importance of trainable data encoding strategies. The
practical implications of the theoretical results are demonstrated with an
application to time series analysis.

</details>


### [433] [Quantum Adiabatic Generation of Human-Like Passwords](https://arxiv.org/abs/2506.08917)
*Sascha Mücke,Raoul Heese,Thore Gerlach,David Biesner,Loong Kuan Lee,Nico Piatkowski*

Main category: quant-ph

TL;DR: 论文探讨了量子计算在生成短语义结构（如密码）中的潜力，提出了基于QUBO和UD-MIS问题的新方法，并在量子计算机上生成了类似人类的密码。


<details>
  <summary>Details</summary>
Motivation: 研究量子计算是否能减少生成式AI模型的资源需求，特别是在生成短语义结构（如密码）方面。

Method: 提出了基于QUBO和UD-MIS问题的编码方法，利用量子计算机估计令牌分布并生成密码。

Result: 在256量子位的中性原子量子计算机上生成了128个类似人类的密码样本。

Conclusion: 量子计算在生成短语义结构方面具有潜力，但仍需进一步研究以扩展其应用范围。

Abstract: Generative Artificial Intelligence (GenAI) for Natural Language Processing
(NLP) is the predominant AI technology to date. An important perspective for
Quantum Computing (QC) is the question whether QC has the potential to reduce
the vast resource requirements for training and operating GenAI models. While
large-scale generative NLP tasks are currently out of reach for practical
quantum computers, the generation of short semantic structures such as
passwords is not. Generating passwords that mimic real user behavior has many
applications, for example to test an authentication system against realistic
threat models. Classical password generation via deep learning have recently
been investigated with significant progress in their ability to generate novel,
realistic password candidates. In the present work we investigate the utility
of adiabatic quantum computers for this task. More precisely, we study
different encodings of token strings and propose novel approaches based on the
Quadratic Unconstrained Binary Optimization (QUBO) and the Unit-Disk Maximum
Independent Set (UD-MIS) problems. Our approach allows us to estimate the token
distribution from data and adiabatically prepare a quantum state from which we
eventually sample the generated passwords via measurements. Our results show
that relatively small samples of 128 passwords, generated on the QuEra Aquila
256-qubit neutral atom quantum computer, contain human-like passwords such as
"Tunas200992" or "teedem28iglove".

</details>


### [434] [Superposed Parameterised Quantum Circuits](https://arxiv.org/abs/2506.08749)
*Viktoria Patapovich,Mo Kordzanganeh,Alexey Melnikov*

Main category: quant-ph

TL;DR: 论文提出了一种叠加参数化量子电路，通过结合翻转量子随机存取存储器和重复直到成功协议，显著提升了量子机器学习的表达能力和可扩展性。


<details>
  <summary>Details</summary>
Motivation: 现有量子机器学习方法依赖线性酉操作和共享可训练参数，限制了表达能力和可扩展性，无法与经典深度网络的多层非线性架构相比。

Method: 引入叠加参数化量子电路，通过振幅变换和后选择实现多项式激活函数，并并行训练多个参数集。

Result: 数值实验显示，该电路在1D阶跃函数回归中均方误差降低三个数量级，在2D星形分类任务中准确率提升至81.4%，方差减少三倍。

Conclusion: 叠加参数化量子电路为硬件高效的深度参数化量子电路提供了新路径，能够学习复杂决策边界。

Abstract: Quantum machine learning has shown promise for high-dimensional data
analysis, yet many existing approaches rely on linear unitary operations and
shared trainable parameters across outputs. These constraints limit
expressivity and scalability relative to the multi-layered, non-linear
architectures of classical deep networks. We introduce superposed parameterised
quantum circuits to overcome these limitations. By combining flip-flop quantum
random-access memory with repeat-until-success protocols, a superposed
parameterised quantum circuit embeds an exponential number of parameterised
sub-models in a single circuit and induces polynomial activation functions
through amplitude transformations and post-selection. We provide an analytic
description of the architecture, showing how multiple parameter sets are
trained in parallel while non-linear amplitude transformations broaden
representational power beyond conventional quantum kernels. Numerical
experiments underscore these advantages: on a 1D step-function regression a
two-qubit superposed parameterised quantum circuit cuts the mean-squared error
by three orders of magnitude versus a parameter-matched variational baseline;
on a 2D star-shaped two-dimensional classification task, introducing a
quadratic activation lifts accuracy to 81.4% and reduces run-to-run variance
three-fold. These results position superposed parameterised quantum circuits as
a hardware-efficient route toward deeper, more versatile parameterised quantum
circuits capable of learning complex decision boundaries.

</details>


<div id='astro-ph.CO'></div>

# astro-ph.CO [[Back]](#toc)

### [435] [syren-baryon: Analytic emulators for the impact of baryons on the matter power spectrum](https://arxiv.org/abs/2506.08783)
*Lukas Kammerer,Deaglan J. Bartlett,Gabriel Kronberger,Harry Desmond,Pedro G. Ferreira*

Main category: astro-ph.CO

TL;DR: 论文提出了一种符号回归方法，用于参数化重子物理对物质功率谱的影响，并提供了不同重子物理模型的解析近似及其不确定性。


<details>
  <summary>Details</summary>
Motivation: 重子物理对宇宙物质分布有显著影响，是当前和未来宇宙学调查的关键系统误差来源。研究旨在简化重子物理影响的参数化。

Method: 使用符号回归方法，基于CAMELS流体动力学模拟套件中的四种重子物理模型和一种重子化算法，构建物质功率谱比值的解析近似。

Result: 近似误差与样本方差相当，重子化表达式的均方根误差小于1%。解析形式可直接解释参数变化的影响，并能区分不同模型。

Conclusion: 提出的解析近似具有物理正确性，可用于区分不同重子物理模型，并提供了公开代码。

Abstract: Baryonic physics has a considerable impact on the distribution of matter in
our Universe on scales probed by current and future cosmological surveys,
acting as a key systematic in such analyses. We seek simple symbolic
parametrisations for the impact of baryonic physics on the matter power
spectrum for a range of physically motivated models, as a function of
wavenumber, redshift, cosmology, and parameters controlling the baryonic
feedback. We use symbolic regression to construct analytic approximations for
the ratio of the matter power spectrum in the presence of baryons to that
without such effects. We obtain separate functions of each of four distinct
sub-grid prescriptions of baryonic physics from the CAMELS suite of
hydrodynamical simulations (Astrid, IllustrisTNG, SIMBA and Swift-EAGLE) as
well as for a baryonification algorithm. We also provide functions which
describe the uncertainty on these predictions, due to both the stochastic
nature of baryonic physics and the errors on our fits. The error on our
approximations to the hydrodynamical simulations is comparable to the sample
variance estimated through varying initial conditions, and our baryonification
expression has a root mean squared error of better than one percent, although
this increases on small scales. These errors are comparable to those of
previous numerical emulators for these models. Our expressions are enforced to
have the physically correct behaviour on large scales and at high redshift. Due
to their analytic form, we are able to directly interpret the impact of varying
cosmology and feedback parameters, and we can identify parameters which have
little to no effect. Each function is based on a different implementation of
baryonic physics, and can therefore be used to discriminate between these
models when applied to real data. We provide publicly available code for all
symbolic approximations found.

</details>


<div id='physics.med-ph'></div>

# physics.med-ph [[Back]](#toc)

### [436] [A Privacy-Preserving Federated Learning Framework for Generalizable CBCT to Synthetic CT Translation in Head and Neck](https://arxiv.org/abs/2506.08654)
*Ciro Benito Raggio,Paolo Zaffino,Maria Francesca Spadea*

Main category: physics.med-ph

TL;DR: 论文提出了一种基于联邦学习的CBCT-to-sCT合成方法，解决了多中心数据共享的隐私问题，并展示了模型的泛化能力。


<details>
  <summary>Details</summary>
Motivation: CBCT在图像引导放疗中存在噪声、软组织对比度低等问题，传统方法受限于数据隐私和多中心差异。

Method: 采用跨机构水平联邦学习（FL）方法，结合条件生成对抗网络（cGAN）在多中心数据上训练。

Result: 模型在多中心数据上表现良好（MAE: 64.38-85.90 HU, SSIM: 0.882-0.922, PSNR: 32.86-34.91 dB），外部验证结果也验证了其泛化能力。

Conclusion: 联邦学习为CBCT-to-sCT合成提供了隐私保护的解决方案，支持跨机构协作建模。

Abstract: Shortened Abstract
  Cone-beam computed tomography (CBCT) has become a widely adopted modality for
image-guided radiotherapy (IGRT). However, CBCT suffers from increased noise,
limited soft-tissue contrast, and artifacts, resulting in unreliable Hounsfield
unit values and hindering direct dose calculation. Synthetic CT (sCT)
generation from CBCT addresses these issues, especially using deep learning
(DL) methods. Existing approaches are limited by institutional heterogeneity,
scanner-dependent variations, and data privacy regulations that prevent
multi-center data sharing.
  To overcome these challenges, we propose a cross-silo horizontal federated
learning (FL) approach for CBCT-to-sCT synthesis in the head and neck region,
extending our FedSynthCT framework. A conditional generative adversarial
network was collaboratively trained on data from three European medical centers
in the public SynthRAD2025 challenge dataset.
  The federated model demonstrated effective generalization across centers,
with mean absolute error (MAE) ranging from $64.38\pm13.63$ to $85.90\pm7.10$
HU, structural similarity index (SSIM) from $0.882\pm0.022$ to $0.922\pm0.039$,
and peak signal-to-noise ratio (PSNR) from $32.86\pm0.94$ to $34.91\pm1.04$ dB.
Notably, on an external validation dataset of 60 patients, comparable
performance was achieved (MAE: $75.22\pm11.81$ HU, SSIM: $0.904\pm0.034$, PSNR:
$33.52\pm2.06$ dB) without additional training, confirming robust
generalization despite protocol, scanner differences and registration errors.
  These findings demonstrate the technical feasibility of FL for CBCT-to-sCT
synthesis while preserving data privacy and offer a collaborative solution for
developing generalizable models across institutions without centralized data
sharing or site-specific fine-tuning.

</details>


<div id='stat.ME'></div>

# stat.ME [[Back]](#toc)

### [437] [midr: Learning from Black-Box Models by Maximum Interpretation Decomposition](https://arxiv.org/abs/2506.08338)
*Ryoichi Asashiba,Reiji Kozuma,Hirokazu Iwasawa*

Main category: stat.ME

TL;DR: 论文介绍了R包midr，用于实现最大解释分解（MID），以解释黑盒模型。


<details>
  <summary>Details</summary>
Motivation: 在需要模型和预测可解释性的领域，采用适当的可解释机器学习（IML）和可解释人工智能（XAI）方法至关重要。

Method: MID是一种功能分解方法，通过最小化黑盒模型的预测函数与低阶加性表示之间的平方误差，构建全局替代模型。

Result: midr包能够通过学习黑盒模型，提供高级分析功能。

Conclusion: 论文展示了midr包的用法，并讨论了其关键特性。

Abstract: The use of appropriate methods of Interpretable Machine Learning (IML) and
eXplainable Artificial Intelligence (XAI) is essential for adopting black-box
predictive models in fields where model and prediction explainability is
required. As a novel tool for interpreting black-box models, we introduce the R
package midr, which implements Maximum Interpretation Decomposition (MID). MID
is a functional decomposition approach that derives a low-order additive
representation of a black-box model by minimizing the squared error between the
model's prediction function and this additive representation. midr enables
learning from black-box models by constructing a global surrogate model with
advanced analytical capabilities. After reviewing related work and the
theoretical foundation of MID, we demonstrate the package's usage and discuss
some of its key features.

</details>


<div id='q-bio.NC'></div>

# q-bio.NC [[Back]](#toc)

### [438] [Instruction-Tuned Video-Audio Models Elucidate Functional Specialization in the Brain](https://arxiv.org/abs/2506.08277)
*Subba Reddy Oota,Khushbu Pahwa,Prachi Jindal,Satya Sai Srinath Namburi,Maneesh Singh,Tanmoy Chakraborty,Bapi S. Raju,Manish Gupta*

Main category: q-bio.NC

TL;DR: 研究发现，指令调优的多模态大语言模型（MLLMs）在视频和音频任务中显著优于非指令调优模型，且能更精确地映射大脑功能处理。


<details>
  <summary>Details</summary>
Motivation: 填补现有研究在评估MLLMs大脑对齐性时对多模态刺激和指令调优模型的忽视。

Method: 使用指令调优的MLLMs生成任务特定表示，并测量其与自然电影观看中记录的神经活动的预测性。

Result: 指令调优的MLLMs在视频任务中表现优于非指令调优模型15%，且能分层映射大脑功能区域。

Conclusion: 任务特定指令显著提升MLLMs与大脑活动的对齐性，为多模态信息处理研究开辟新途径。

Abstract: Recent voxel-wise multimodal brain encoding studies have shown that
multimodal large language models (MLLMs) exhibit a higher degree of brain
alignment compared to unimodal models in both unimodal and multimodal stimulus
settings. More recently, instruction-tuned multimodal models have shown to
generate task-specific representations that align strongly with brain activity.
However, prior work evaluating the brain alignment of MLLMs has primarily
focused on unimodal settings or relied on non-instruction-tuned multimodal
models for multimodal stimuli. To address this gap, we investigated brain
alignment, that is, measuring the degree of predictivity of neural activity
recorded while participants were watching naturalistic movies (video along with
audio) with representations derived from MLLMs. We utilized
instruction-specific embeddings from six video and two audio instruction-tuned
MLLMs. Experiments with 13 video task-specific instructions show that
instruction-tuned video MLLMs significantly outperform non-instruction-tuned
multimodal (by 15%) and unimodal models (by 20%). Our evaluation of MLLMs for
both video and audio tasks using language-guided instructions shows clear
disentanglement in task-specific representations from MLLMs, leading to precise
differentiation of multimodal functional processing in the brain. We also find
that MLLM layers align hierarchically with the brain, with early sensory areas
showing strong alignment with early layers, while higher-level visual and
language regions align more with middle to late layers. These findings provide
clear evidence for the role of task-specific instructions in improving the
alignment between brain activity and MLLMs, and open new avenues for mapping
joint information processing in both the systems. We make the code publicly
available [https://github.com/subbareddy248/mllm_videos].

</details>


<div id='physics.soc-ph'></div>

# physics.soc-ph [[Back]](#toc)

### [439] [Real-Time Cascade Mitigation in Power Systems Using Influence Graph Improved by Reinforcement Learning](https://arxiv.org/abs/2506.08893)
*Kai Zhou,Youbiao He,Chong Zhong,Yifu Wu*

Main category: physics.soc-ph

TL;DR: 论文提出了一种基于马尔可夫决策过程（MDP）的实时级联停电缓解方法，通过强化学习优化决策策略，有效降低电力系统级联风险。


<details>
  <summary>Details</summary>
Motivation: 现代电力系统因可再生能源渗透率增加而面临级联停电风险上升的问题，需快速、复杂的实时决策。

Method: 将影响图扩展为MDP模型，结合强化学习（策略梯度算法）优化决策策略，设计保守动作和奖励机制。

Result: 在IEEE 14-bus和118-bus系统上验证，主动断开线路可有效降低级联风险，某些线路在缓解级联传播中起关键作用。

Conclusion: 该方法通过强化学习实现了快速收敛和保守决策，为电力系统级联停电实时缓解提供了有效工具。

Abstract: Despite high reliability, modern power systems with growing renewable
penetration face an increasing risk of cascading outages. Real-time cascade
mitigation requires fast, complex operational decisions under uncertainty. In
this work, we extend the influence graph into a Markov decision process model
(MDP) for real-time mitigation of cascading outages in power transmission
systems, accounting for uncertainties in generation, load, and initial
contingencies. The MDP includes a do-nothing action to allow for conservative
decision-making and is solved using reinforcement learning. We present a policy
gradient learning algorithm initialized with a policy corresponding to the
unmitigated case and designed to handle invalid actions. The proposed learning
method converges faster than the conventional algorithm. Through careful reward
design, we learn a policy that takes conservative actions without deteriorating
system conditions. The model is validated on the IEEE 14-bus and IEEE 118-bus
systems. The results show that proactive line disconnections can effectively
reduce cascading risk, and certain lines consistently emerge as critical in
mitigating cascade propagation.

</details>


<div id='cs.CE'></div>

# cs.CE [[Back]](#toc)

### [440] [KP-PINNs: Kernel Packet Accelerated Physics Informed Neural Networks](https://arxiv.org/abs/2506.08563)
*Siyuan Yang,Cheng Song,Zhilu Lai,Wenjia Wang*

Main category: cs.CE

TL;DR: 提出了一种名为KP-PINNs的新框架，通过RKHS范数重新定义损失函数，并利用KP方法加速计算，显著提升了PINNs的稳定性和准确性。


<details>
  <summary>Details</summary>
Motivation: 传统PINNs使用L2损失函数在复杂微分方程中表现不稳定且不准确，需要改进。

Method: 采用RKHS范数重新表达损失函数，并结合KP方法加速计算。

Result: 理论证明KP-PINNs具有稳定性，数值实验验证了其高效性和有效性。

Conclusion: KP-PINNs为科学计算中基于PINNs的求解器提供了稳定且准确的新方向。

Abstract: Differential equations are involved in modeling many engineering problems.
Many efforts have been devoted to solving differential equations. Due to the
flexibility of neural networks, Physics Informed Neural Networks (PINNs) have
recently been proposed to solve complex differential equations and have
demonstrated superior performance in many applications. While the L2 loss
function is usually a default choice in PINNs, it has been shown that the
corresponding numerical solution is incorrect and unstable for some complex
equations. In this work, we propose a new PINNs framework named Kernel Packet
accelerated PINNs (KP-PINNs), which gives a new expression of the loss function
using the reproducing kernel Hilbert space (RKHS) norm and uses the Kernel
Packet (KP) method to accelerate the computation. Theoretical results show that
KP-PINNs can be stable across various differential equations. Numerical
experiments illustrate that KP-PINNs can solve differential equations
effectively and efficiently. This framework provides a promising direction for
improving the stability and accuracy of PINNs-based solvers in scientific
computing.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [441] [QUITE: A Query Rewrite System Beyond Rules with LLM Agents](https://arxiv.org/abs/2506.07675)
*Yuyang Song,Hanxu Yan,Jiale Lao,Yibo Wang,Yufei Li,Yuanchun Zhou,Jianguo Wang,Mingjie Tang*

Main category: cs.DB

TL;DR: 论文提出了一种基于LLM的SQL查询重写方法QUITE，解决了传统基于规则方法的局限性，通过多智能体框架和实时反馈显著提升了查询性能和覆盖范围。


<details>
  <summary>Details</summary>
Motivation: 传统基于规则的查询重写方法存在局限性，如规则难以发现和验证、无法泛化到新查询模式等，而人类专家虽能更好重写但缺乏扩展性。LLM的语义和推理能力为解决这一问题提供了可能。

Method: 设计了基于有限状态机（FSM）的多智能体框架，结合外部工具和实时数据库反馈；开发了重写中间件增强LLM生成优化查询的能力；采用提示注入技术改进执行计划。

Result: QUITE将查询执行时间减少高达35.8%，比现有方法多生成24.1%的重写，覆盖了传统系统无法处理的查询案例。

Conclusion: QUITE通过结合LLM和多智能体框架，显著提升了查询重写的性能和覆盖范围，为SQL优化提供了新思路。

Abstract: Query rewrite transforms SQL queries into semantically equivalent forms that
run more efficiently. Existing approaches mainly rely on predefined rewrite
rules, but they handle a limited subset of queries and can cause performance
regressions. This limitation stems from three challenges of rule-based query
rewrite: (1) it is hard to discover and verify new rules, (2) fixed rewrite
rules do not generalize to new query patterns, and (3) some rewrite techniques
cannot be expressed as fixed rules. Motivated by the fact that human experts
exhibit significantly better rewrite ability but suffer from scalability, and
Large Language Models (LLMs) have demonstrated nearly human-level semantic and
reasoning abilities, we propose a new approach of using LLMs to rewrite SQL
queries beyond rules. Due to the hallucination problems in LLMs, directly
applying LLMs often leads to nonequivalent and suboptimal queries. To address
this issue, we propose QUITE (query rewrite), a training-free and
feedback-aware system based on LLM agents that rewrites SQL queries into
semantically equivalent forms with significantly better performance, covering a
broader range of query patterns and rewrite strategies compared to rule-based
methods. Firstly, we design a multi-agent framework controlled by a finite
state machine (FSM) to equip LLMs with the ability to use external tools and
enhance the rewrite process with real-time database feedback. Secondly, we
develop a rewrite middleware to enhance the ability of LLMs to generate
optimized query equivalents. Finally, we employ a novel hint injection
technique to improve execution plans for rewritten queries. Extensive
experiments show that QUITE reduces query execution time by up to 35.8% over
state-of-the-art approaches and produces 24.1% more rewrites than prior
methods, covering query cases that earlier systems did not handle.

</details>


### [442] [RADAR: Benchmarking Language Models on Imperfect Tabular Data](https://arxiv.org/abs/2506.08249)
*Ken Gu,Zhihan Zhang,Kate Lin,Yuwei Zhang,Akshay Paruchuri,Hong Yu,Mehran Kazemi,Kumar Ayush,A. Ali Heydari,Maxwell A. Xu,Girish Narayanswamy,Yun Liu,Ming-Zher Poh,Yuzhe Yang,Mark Malhotra,Shwetak Patel,Hamid Palangi,Xuhai Xu,Daniel McDuff,Tim Althoff,Xin Liu*

Main category: cs.DB

TL;DR: RADAR是一个用于评估语言模型在表格数据中处理数据异常能力的基准测试，揭示了前沿模型在数据异常存在时性能显著下降的问题。


<details>
  <summary>Details</summary>
Motivation: 语言模型在自主数据分析中的部署日益增多，但其处理数据异常（如缺失值、离群值等）的能力尚未充分研究，这些异常可能严重影响分析结论的有效性。

Method: 通过程序化扰动模拟数据异常，构建了包含2980个表格查询对的RADAR基准测试，涵盖9个领域和5种数据异常类型，并系统性地调整表格大小。

Result: 前沿模型在无数据异常时表现尚可，但在数据异常存在时性能显著下降，暴露了其在稳健数据分析能力上的不足。

Conclusion: RADAR作为一个灵活可扩展的基准测试，为提升表格推理能力提供了有价值的资源。

Abstract: Language models (LMs) are increasingly being deployed to perform autonomous
data analyses. However, their data awareness -- the ability to recognize,
reason over, and appropriately handle data artifacts such as missing values,
outliers, and logical inconsistencies -- remains underexplored. These artifacts
are especially common in real-world tabular data and, if mishandled, can
significantly compromise the validity of analytical conclusions. To address
this gap, we present RADAR, a benchmark for systematically evaluating
data-aware reasoning on tabular data. We develop a framework to simulate data
artifacts via programmatic perturbations to enable targeted evaluation of model
behavior. RADAR comprises 2980 table query pairs, grounded in real-world data
spanning 9 domains and 5 data artifact types. In addition to evaluating
artifact handling, RADAR systematically varies table size to study how
reasoning performance holds when increasing table size. Our evaluation reveals
that, despite decent performance on tables without data artifacts, frontier
models degrade significantly when data artifacts are introduced, exposing
critical gaps in their capacity for robust, data-aware analysis. Designed to be
flexible and extensible, RADAR supports diverse perturbation types and
controllable table sizes, offering a valuable resource for advancing tabular
reasoning.

</details>


### [443] [LEANN: A Low-Storage Vector Index](https://arxiv.org/abs/2506.08276)
*Yichuan Wang,Shu Liu,Zhifei Li,Yongji Wu,Ziming Mao,Yilong Zhao,Xiao Yan,Zhiying Xu,Yang Zhou,Ion Stoica,Sewon Min,Matei Zaharia,Joseph E. Gonzalez*

Main category: cs.DB

TL;DR: LEANN是一种针对资源受限个人设备的存储高效近似最近邻搜索索引，通过紧凑图结构和动态重计算策略，显著减少存储开销，同时保持高搜索质量。


<details>
  <summary>Details</summary>
Motivation: 随着嵌入搜索在本地设备上的需求增加，传统索引的高存储开销成为主要挑战，需要一种更高效的解决方案。

Method: LEANN结合紧凑图结构和动态重计算策略，减少存储需求并保持搜索性能。

Result: LEANN将索引大小降至原始数据的5%以下，存储需求减少50倍，同时在真实问答基准测试中保持90%的top-3召回率，延迟低于2秒。

Conclusion: LEANN为资源受限设备提供了一种高效的嵌入搜索解决方案，显著降低了存储开销，同时保持了搜索质量。

Abstract: Embedding-based search is widely used in applications such as recommendation
and retrieval-augmented generation (RAG). Recently, there is a growing demand
to support these capabilities over personal data stored locally on devices.
However, maintaining the necessary data structure associated with the
embedding-based search is often infeasible due to its high storage overhead.
For example, indexing 100 GB of raw data requires 150 to 700 GB of storage,
making local deployment impractical. Reducing this overhead while maintaining
search quality and latency becomes a critical challenge. In this paper, we
present LEANN, a storage-efficient approximate nearest neighbor (ANN) search
index optimized for resource-constrained personal devices. LEANN combines a
compact graph-based structure with an efficient on-the-fly recomputation
strategy to enable fast and accurate retrieval with minimal storage overhead.
Our evaluation shows that LEANN reduces index size to under 5% of the original
raw data, achieving up to 50 times smaller storage than standard indexes, while
maintaining 90% top-3 recall in under 2 seconds on real-world question
answering benchmarks.

</details>


<div id='nlin.CG'></div>

# nlin.CG [[Back]](#toc)

### [444] [Flow-Lenia: Emergent evolutionary dynamics in mass conservative continuous cellular automata](https://arxiv.org/abs/2506.08569)
*Erwan Plantec,Gautier Hamon,Mayalen Etcheverry,Bert Wang-Chak Chan,Pierre-Yves Oudeyer,Clément Moulin-Frier*

Main category: nlin.CG

TL;DR: Flow-Lenia是Lenia的扩展，支持质量守恒，能生成复杂行为的局部化模式，并通过优化参数模拟多物种进化。


<details>
  <summary>Details</summary>
Motivation: 研究人工生命系统中自组织、自复制和开放性的生成，尤其是通过连续细胞自动机（如Lenia）模拟生命现象。

Method: 提出Flow-Lenia，扩展Lenia以支持质量守恒，并通过参数优化生成复杂行为模式。

Result: 成功生成具有复杂行为的局部化模式，并实现多物种模拟和进化动态分析。

Conclusion: Flow-Lenia为研究生命现象和进化动态提供了新工具，展示了其在人工生命领域的潜力。

Abstract: Central to the artificial life endeavour is the creation of artificial
systems spontaneously generating properties found in the living world such as
autopoiesis, self-replication, evolution and open-endedness. While numerous
models and paradigms have been proposed, cellular automata (CA) have taken a
very important place in the field notably as they enable the study of
phenomenons like self-reproduction and autopoiesis. Continuous CA like Lenia
have been showed to produce life-like patterns reminiscent, on an aesthetic and
ontological point of view, of biological organisms we call creatures. We
propose in this paper Flow-Lenia, a mass conservative extension of Lenia. We
present experiments demonstrating its effectiveness in generating
spatially-localized patters (SLPs) with complex behaviors and show that the
update rule parameters can be optimized to generate complex creatures showing
behaviors of interest. Furthermore, we show that Flow-Lenia allows us to embed
the parameters of the model, defining the properties of the emerging patterns,
within its own dynamics thus allowing for multispecies simulations. By using
the evolutionary activity framework as well as other metrics, we shed light on
the emergent evolutionary dynamics taking place in this system.

</details>


<div id='astro-ph.IM'></div>

# astro-ph.IM [[Back]](#toc)

### [445] [Dynamic Diffusion Schrödinger Bridge in Astrophysical Observational Inversions](https://arxiv.org/abs/2506.08065)
*Ye Zhu,Duo Xu,Zhiwei Deng,Jonathon C. Tan,Olga Russakovsky*

Main category: astro-ph.IM

TL;DR: 论文研究了扩散薛定谔桥（DSB）模型在动态天体物理系统中的应用，提出Astro-DSB模型，并在模拟和真实数据中验证其性能。


<details>
  <summary>Details</summary>
Motivation: 解决GMCs中恒星形成的观测逆预测问题，提升天体物理动态建模的效率和可解释性。

Method: 提出Astro-DSB模型，基于成对域假设，适用于天体物理动态，并在模拟和真实数据（Taurus B213）中测试。

Result: Astro-DSB在解释性、学习效率和预测性能上优于传统方法；生成模型在OOD测试中表现更优。

Conclusion: 研究扩展了扩散模型的应用，展示了其在物理动态对齐中的潜力，为未来物理感知生成模型铺路。

Abstract: We study Diffusion Schr\"odinger Bridge (DSB) models in the context of
dynamical astrophysical systems, specifically tackling observational inverse
prediction tasks within Giant Molecular Clouds (GMCs) for star formation. We
introduce the Astro-DSB model, a variant of DSB with the pairwise domain
assumption tailored for astrophysical dynamics. By investigating its learning
process and prediction performance in both physically simulated data and in
real observations (the Taurus B213 data), we present two main takeaways. First,
from the astrophysical perspective, our proposed paired DSB method improves
interpretability, learning efficiency, and prediction performance over
conventional astrostatistical and other machine learning methods. Second, from
the generative modeling perspective, probabilistic generative modeling reveals
improvements over discriminative pixel-to-pixel modeling in Out-Of-Distribution
(OOD) testing cases of physical simulations with unseen initial conditions and
different dominant physical processes. Our study expands research into
diffusion models beyond the traditional visual synthesis application and
provides evidence of the models' learning abilities beyond pure data
statistics, paving a path for future physics-aware generative models which can
align dynamics between machine learning and real (astro)physical systems.

</details>


<div id='eess.AS'></div>

# eess.AS [[Back]](#toc)

### [446] [Approaching Dialogue State Tracking via Aligning Speech Encoders and LLMs](https://arxiv.org/abs/2506.08633)
*Šimon Sedláček,Bolaji Yusuf,Ján Švec,Pradyoth Hegde,Santosh Kesiraju,Oldřich Plchot,Jan Černocký*

Main category: eess.AS

TL;DR: 论文通过连接语音编码器和LLM的表示空间，提出了一种开放源码和数据的对话状态跟踪方法，实验表明其性能优越。


<details>
  <summary>Details</summary>
Motivation: 解决口语对话状态跟踪问题，同时强调使用完全开放源码和数据的组件。

Method: 通过小型连接模块桥接语音编码器和LLM的表示空间，并研究不同系统配置的影响。

Result: 在SpokenWOZ测试集上达到最佳性能（34.66% JGA），使用Gemma-2-9B-instruct进一步提升至42.17% JGA。

Conclusion: 提出的方法在口语对话状态跟踪中表现优异，且具有开放性和可扩展性。

Abstract: In this work, we approach spoken Dialogue State Tracking (DST) by bridging
the representation spaces of speech encoders and LLMs via a small connector
module, with a focus on fully open-sourced and open-data components
(WavLM-large, OLMo). We focus on ablating different aspects of such systems
including full/LoRA adapter fine-tuning, the effect of agent turns in the
dialogue history, as well as fuzzy matching-based output post-processing, which
greatly improves performance of our systems on named entities in the dialogue
slot values. We conduct our experiments on the SpokenWOZ dataset, and
additionally utilize the Speech-Aware MultiWOZ dataset to augment our training
data. Ultimately, our best-performing WavLM + connector + OLMo-1B aligned
models achieve state of the art on the SpokenWOZ test set (34.66% JGA), and our
system with Gemma-2-9B-instruct further surpasses this result, reaching 42.17%
JGA on SpokenWOZ test.

</details>


<div id='physics.geo-ph'></div>

# physics.geo-ph [[Back]](#toc)

### [447] [TS-PIELM: Time-Stepping Physics-Informed Extreme Learning Machine Facilitates Soil Consolidation Analyses](https://arxiv.org/abs/2506.08381)
*He Yang,Fei Ren,Hai-Sui Yu,Xueyu Geng,Pei-Zhi Zhuang*

Main category: physics.geo-ph

TL;DR: 论文提出了一种名为TS-PIELM的高效物理信息机器学习方法，显著提升了传统PINN在土壤固结分析中的精度和效率。


<details>
  <summary>Details</summary>
Motivation: 传统物理信息神经网络（PINN）在土壤固结分析中的精度和效率不足，需要改进以成为有竞争力的替代方案。

Method: 提出TS-PIELM框架，将固结过程分为多个时间间隔，使用单层前馈极限学习机（ELM）近似解，通过线性方程组直接计算输出层权重，显著提升训练效率。

Result: TS-PIELM在一维案例中的计算效率和精度分别比PINN提高了1000倍和100倍。

Conclusion: TS-PIELM证明了物理信息机器学习在计算岩土工程中的强大潜力。

Abstract: Accuracy and efficiency of the conventional physics-informed neural network
(PINN) need to be improved before it can be a competitive alternative for soil
consolidation analyses. This paper aims to overcome these limitations by
proposing a highly accurate and efficient physics-informed machine learning
(PIML) approach, termed time-stepping physics-informed extreme learning machine
(TS-PIELM). In the TS-PIELM framework the consolidation process is divided into
numerous time intervals, which helps overcome the limitation of PIELM in
solving differential equations with sharp gradients. To accelerate network
training, the solution is approximated by a single-layer feedforward extreme
learning machine (ELM), rather than using a fully connected neural network in
PINN. The input layer weights of the ELM network are generated randomly and
fixed during the training process. Subsequently, the output layer weights are
directly computed by solving a system of linear equations, which significantly
enhances the training efficiency compared to the time-consuming gradient
descent method in PINN. Finally, the superior performance of TS-PIELM is
demonstrated by solving three typical Terzaghi consolidation problems. Compared
to PINN, results show that the computational efficiency and accuracy of the
novel TS-PIELM framework are improved by more than 1000 times and 100 times for
one-dimensional cases, respectively. This paper provides compelling evidence
that PIML can be a powerful tool for computational geotechnics.

</details>


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [448] [Confidence Boosts Trust-Based Resilience in Cooperative Multi-Robot Systems](https://arxiv.org/abs/2506.08807)
*Luca Ballotta,Áron Vékássy,Stephanie Gil,Michal Yemini*

Main category: eess.SP

TL;DR: 提出了一种基于物理信道的多机器人系统弹性协议，通过参数λt权衡信任与任务执行效率。


<details>
  <summary>Details</summary>
Motivation: 无线通信多机器人系统易受网络攻击，物理信道提供了一种检测恶意机器人的方法，但需处理不确定性。

Method: 设计了一种弹性协议，通过参数λt动态调整对附近机器人合法性的信任程度。

Result: 协议在温和假设下实现多机器人弹性协作，可权衡协调最优性与任务执行速度。

Conclusion: 该协议在自动驾驶车队实验中验证有效，需根据任务需求调整λt。

Abstract: Wireless communication-based multi-robot systems open the door to
cyberattacks that can disrupt safety and performance of collaborative robots.
The physical channel supporting inter-robot communication offers an attractive
opportunity to decouple the detection of malicious robots from task-relevant
data exchange between legitimate robots. Yet, trustworthiness indications
coming from physical channels are uncertain and must be handled with this in
mind. In this paper, we propose a resilient protocol for multi-robot operation
wherein a parameter {\lambda}t accounts for how confident a robot is about the
legitimacy of nearby robots that the physical channel indicates. Analytical
results prove that our protocol achieves resilient coordination with
arbitrarily many malicious robots under mild assumptions. Tuning {\lambda}t
allows a designer to trade between near-optimal inter-robot coordination and
quick task execution; see Fig. 1. This is a fundamental performance tradeoff
and must be carefully evaluated based on the task at hand. The effectiveness of
our approach is numerically verified with experiments involving platoons of
autonomous cars where some vehicles are maliciously spoofed.

</details>


### [449] [Large Language Models for EEG: A Comprehensive Survey and Taxonomy](https://arxiv.org/abs/2506.06353)
*Naseem Babu,Jimson Mathew,A. P. Vinod*

Main category: eess.SP

TL;DR: 本文综述了大型语言模型（LLMs）与脑电图（EEG）研究的结合，系统梳理了LLMs在EEG分析和应用中的最新进展，并提出了分类框架。


<details>
  <summary>Details</summary>
Motivation: 探索LLMs与EEG研究的结合，推动神经解码、脑机接口和情感计算的发展。

Method: 通过系统综述和分类，将文献分为四个领域：LLM启发的EEG表示学习、EEG到语言解码、跨模态生成以及临床应用和数据集管理工具。

Result: 研究表明，基于Transformer的架构通过微调、少样本和零样本学习，使EEG模型能够完成自然语言生成、语义解释和诊断辅助等复杂任务。

Conclusion: 本文为未来通过语言模型桥接自然语言处理和神经信号分析提供了基础资源。

Abstract: The growing convergence between Large Language Models (LLMs) and
electroencephalography (EEG) research is enabling new directions in neural
decoding, brain-computer interfaces (BCIs), and affective computing. This
survey offers a systematic review and structured taxonomy of recent
advancements that utilize LLMs for EEG-based analysis and applications. We
organize the literature into four domains: (1) LLM-inspired foundation models
for EEG representation learning, (2) EEG-to-language decoding, (3) cross-modal
generation including image and 3D object synthesis, and (4) clinical
applications and dataset management tools. The survey highlights how
transformer-based architectures adapted through fine-tuning, few-shot, and
zero-shot learning have enabled EEG-based models to perform complex tasks such
as natural language generation, semantic interpretation, and diagnostic
assistance. By offering a structured overview of modeling strategies, system
designs, and application areas, this work serves as a foundational resource for
future work to bridge natural language processing and neural signal analysis
through language models.

</details>


<div id='physics.chem-ph'></div>

# physics.chem-ph [[Back]](#toc)

### [450] [ChemGraph: An Agentic Framework for Computational Chemistry Workflows](https://arxiv.org/abs/2506.06363)
*Thang D. Pham,Aditya Tanikanti,Murat Keçeli*

Main category: physics.chem-ph

TL;DR: ChemGraph是一个基于AI的框架，用于自动化计算化学和材料科学工作流程，结合图神经网络和大型语言模型，简化复杂任务。


<details>
  <summary>Details</summary>
Motivation: 原子模拟在化学和材料科学中至关重要，但因其复杂性、多样化的软件生态系统和专业知识需求而难以实施。

Method: ChemGraph利用图神经网络和大型语言模型，提供直观界面，支持多种计算任务和方法。

Result: 在13个基准任务中，小型LLM适用于简单任务，复杂任务需更大模型；多代理框架使小型LLM在特定场景中表现优于GPT-4o。

Conclusion: ChemGraph通过AI和多代理框架，有效简化了原子模拟的复杂性，提升了计算效率。

Abstract: Atomistic simulations are essential tools in chemistry and materials science,
accelerating the discovery of novel catalysts, energy storage materials, and
pharmaceuticals. However, running these simulations remains challenging due to
the wide range of computational methods, diverse software ecosystems, and the
need for expert knowledge and manual effort for the setup, execution, and
validation stages. In this work, we present ChemGraph, an agentic framework
powered by artificial intelligence and state-of-the-art simulation tools to
streamline and automate computational chemistry and materials science
workflows. ChemGraph leverages graph neural network-based foundation models for
accurate yet computationally efficient calculations and large language models
(LLMs) for natural language understanding, task planning, and scientific
reasoning to provide an intuitive and interactive interface. Users can perform
tasks such as molecular structure generation, single-point energy, geometry
optimization, vibrational analysis, and thermochemistry calculations with
methods ranging from tight-binding and machine learning interatomic potentials
to density functional theory or wave function theory-based methods. We evaluate
ChemGraph across 13 benchmark tasks and demonstrate that smaller LLMs
(GPT-4o-mini, Claude-3.5-haiku, Qwen2.5-14B) perform well on simple workflows,
while more complex tasks benefit from using larger models like GPT-4o.
Importantly, we show that decomposing complex tasks into smaller subtasks
through a multi-agent framework enables smaller LLM models to match or exceed
GPT-4o's performance in specific scenarios.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [451] [Physics-Informed Teleconnection-Aware Transformer for Global Subseasonal-to-Seasonal Forecasting](https://arxiv.org/abs/2506.08049)
*Tengfei Lyu,Weijia Zhang,Hao Liu*

Main category: stat.ML

TL;DR: TelePiT是一种新型深度学习架构，通过多尺度物理和遥相关建模提升全球S2S预报能力。


<details>
  <summary>Details</summary>
Motivation: S2S预报因大气系统的混沌性和多尺度复杂交互而极具挑战，现有方法未能明确建模关键物理过程和遥相关。

Method: 结合球谐嵌入、多尺度物理神经网络ODE和遥相关感知Transformer，明确建模物理过程和全球气候交互。

Result: TelePiT显著优于现有数据驱动模型和数值预报系统，如2米温度RMSE降低57.7%。

Conclusion: TelePiT通过物理和遥相关建模，为S2S预报提供了显著改进。

Abstract: Subseasonal-to-seasonal (S2S) forecasting, which predicts climate conditions
from several weeks to months in advance, presents significant challenges due to
the chaotic dynamics of atmospheric systems and complex interactions across
multiple scales. Current approaches often fail to explicitly model underlying
physical processes and teleconnections that are crucial at S2S timescales. We
introduce TelePiT, a novel deep learning architecture that enhances global S2S
forecasting through integrated multi-scale physics and teleconnection
awareness. Our approach consists of three key components: (1) Spherical
Harmonic Embedding, which accurately encodes global atmospheric variables onto
spherical geometry; (2) Multi-Scale Physics-Informed Neural ODE, which
explicitly captures atmospheric physical processes across multiple learnable
frequency bands; (3) Teleconnection-Aware Transformer, which models critical
global climate interactions through tactfully injecting teleconnection patterns
into the self-attention. Extensive experiments demonstrate that TelePiT
significantly outperforms state-of-the-art data-driven baselines and
operational numerical weather prediction systems, with remarkable improvements
for atmospheric variables including a 57.7% reduction in RMSE for 2-meter
temperature compared to previous best models.

</details>


### [452] [WWAggr: A Window Wasserstein-based Aggregation for Ensemble Change Point Detection](https://arxiv.org/abs/2506.08066)
*Alexander Stepikin,Evgenia Romanenkova,Alexey Zaytsev*

Main category: stat.ML

TL;DR: 该论文提出了一种基于Wasserstein距离的新型集成聚合方法WWAggr，用于提升高维数据流中变化点检测的性能，并解决了决策阈值选择的长期问题。


<details>
  <summary>Details</summary>
Motivation: 当前基于深度神经网络的变化点检测器在高维数据流中表现不佳，且标准集成聚合方法（如平均）未能充分利用问题特性。

Method: 引入WWAggr方法，利用Wasserstein距离进行任务特定的集成聚合，适用于多种深度变化点检测模型的组合。

Result: WWAggr方法在性能上优于标准聚合技术，并有效解决了决策阈值选择问题。

Conclusion: WWAggr为高维数据流中的变化点检测提供了更鲁棒和通用的解决方案。

Abstract: Change Point Detection (CPD) aims to identify moments of abrupt distribution
shifts in data streams. Real-world high-dimensional CPD remains challenging due
to data pattern complexity and violation of common assumptions. Resorting to
standalone deep neural networks, the current state-of-the-art detectors have
yet to achieve perfect quality. Concurrently, ensembling provides more robust
solutions, boosting the performance. In this paper, we investigate ensembles of
deep change point detectors and realize that standard prediction aggregation
techniques, e.g., averaging, are suboptimal and fail to account for problem
peculiarities. Alternatively, we introduce WWAggr -- a novel task-specific
method of ensemble aggregation based on the Wasserstein distance. Our procedure
is versatile, working effectively with various ensembles of deep CPD models.
Moreover, unlike existing solutions, we practically lift a long-standing
problem of the decision threshold selection for CPD.

</details>


### [453] [Constrained Pareto Set Identification with Bandit Feedback](https://arxiv.org/abs/2506.08127)
*Cyrille Kone,Emilie Kaufmann,Laura Richert*

Main category: stat.ML

TL;DR: 提出一种在多元臂老虎机问题中识别帕累托集的方法，考虑可行性约束，并证明其样本复杂度接近最优。


<details>
  <summary>Details</summary>
Motivation: 解决在多元臂老虎机中识别帕累托集的问题，同时满足线性约束条件，提升算法效率。

Method: 提出一种固定置信度识别算法，优于传统竞赛式算法和两阶段方法。

Result: 算法表现显著优于基线方法，理论证明其样本复杂度接近信息论下界。

Conclusion: 该算法在理论和实验上均表现出色，为约束条件下的帕累托集识别提供了高效解决方案。

Abstract: In this paper, we address the problem of identifying the Pareto Set under
feasibility constraints in a multivariate bandit setting. Specifically, given a
$K$-armed bandit with unknown means $\mu_1, \dots, \mu_K \in \mathbb{R}^d$, the
goal is to identify the set of arms whose mean is not uniformly worse than that
of another arm (i.e., not smaller for all objectives), while satisfying some
known set of linear constraints, expressing, for example, some minimal
performance on each objective. Our focus lies in fixed-confidence
identification, for which we introduce an algorithm that significantly
outperforms racing-like algorithms and the intuitive two-stage approach that
first identifies feasible arms and then their Pareto Set. We further prove an
information-theoretic lower bound on the sample complexity of any algorithm for
constrained Pareto Set identification, showing that the sample complexity of
our approach is near-optimal. Our theoretical results are supported by an
extensive empirical evaluation on a series of benchmarks.

</details>


### [454] [Model-Free Kernel Conformal Depth Measures Algorithm for Uncertainty Quantification in Regression Models in Separable Hilbert Spaces](https://arxiv.org/abs/2506.08325)
*Marcos Matabuena,Rahul Ghosal,Pavlo Mozharovskyi,Oscar Hernan Madrid Padilla,Jukka-Pekka Onnela*

Main category: stat.ML

TL;DR: 论文提出了一种基于条件深度度量的新型无模型不确定性量化算法，用于定义预测区域，适用于希尔伯特空间中的预测变量和响应变量。


<details>
  <summary>Details</summary>
Motivation: 深度度量在复杂随机对象中定义水平集具有强大能力，但其在回归建模中用于预测区域的研究较少，本文旨在填补这一空白。

Method: 提出基于条件核均值嵌入和集成深度度量的算法，并结合保形预测变体以增强有限样本下的实用性。

Result: 算法在多种功能数据和欧几里得场景中表现良好，并在数字健康应用中展示了实用性。

Conclusion: 新算法在理论和实践中均表现出色，为复杂数据提供了有效的预测区域定义方法。

Abstract: Depth measures are powerful tools for defining level sets in emerging,
non--standard, and complex random objects such as high-dimensional multivariate
data, functional data, and random graphs. Despite their favorable theoretical
properties, the integration of depth measures into regression modeling to
provide prediction regions remains a largely underexplored area of research. To
address this gap, we propose a novel, model-free uncertainty quantification
algorithm based on conditional depth measures--specifically, conditional kernel
mean embeddings and an integrated depth measure. These new algorithms can be
used to define prediction and tolerance regions when predictors and responses
are defined in separable Hilbert spaces. The use of kernel mean embeddings
ensures faster convergence rates in prediction region estimation. To enhance
the practical utility of the algorithms with finite samples, we also introduce
a conformal prediction variant that provides marginal, non-asymptotic
guarantees for the derived prediction regions. Additionally, we establish both
conditional and unconditional consistency results, as well as fast convergence
rates in certain homoscedastic settings. We evaluate the finite--sample
performance of our model in extensive simulation studies involving various
types of functional data and traditional Euclidean scenarios. Finally, we
demonstrate the practical relevance of our approach through a digital health
application related to physical activity, aiming to provide personalized
recommendations

</details>


### [455] [Asymptotic Normality of Infinite Centered Random Forests -Application to Imbalanced Classification](https://arxiv.org/abs/2506.08548)
*Moria Mayala,Erwan Scornet,Charles Tillier,Olivier Wintenberger*

Main category: stat.ML

TL;DR: 本文研究了在类别不平衡数据上训练的Centered Random Forests (CRF)，提出了基于重要性采样的去偏估计器IS-ICRF，并证明了其在方差减少方面的优势。


<details>
  <summary>Details</summary>
Motivation: 类别不平衡数据在分类任务中常见，但传统方法可能导致偏差。本文旨在理论分析CRF在再平衡数据集上的表现，并提出去偏方法。

Method: 通过建立无限CRF的中心极限定理（CLT），提出重要性采样（IS）去偏方法，得到IS-ICRF估计器。

Result: IS-ICRF满足以预测函数值为中心的CLT，在高不平衡情况下方差减少。

Conclusion: 理论分析和实验表明，再平衡数据集训练CRF并去偏优于直接使用原始数据。

Abstract: Many classification tasks involve imbalanced data, in which a class is
largely underrepresented. Several techniques consists in creating a rebalanced
dataset on which a classifier is trained. In this paper, we study theoretically
such a procedure, when the classifier is a Centered Random Forests (CRF). We
establish a Central Limit Theorem (CLT) on the infinite CRF with explicit rates
and exact constant. We then prove that the CRF trained on the rebalanced
dataset exhibits a bias, which can be removed with appropriate techniques.
Based on an importance sampling (IS) approach, the resulting debiased
estimator, called IS-ICRF, satisfies a CLT centered at the prediction function
value. For high imbalance settings, we prove that the IS-ICRF estimator enjoys
a variance reduction compared to the ICRF trained on the original data.
Therefore, our theoretical analysis highlights the benefits of training random
forests on a rebalanced dataset (followed by a debiasing procedure) compared to
using the original data. Our theoretical results, especially the variance rates
and the variance reduction, appear to be valid for Breiman's random forests in
our experiments.

</details>


### [456] [Flexible and Efficient Drift Detection without Labels](https://arxiv.org/abs/2506.08734)
*Nelvin Tan,Yu-Ching Shih,Dong Yang,Amol Salunkhe*

Main category: stat.ML

TL;DR: 提出了一种无标签环境下高效的概念漂移检测算法，基于统计过程控制，优于现有方法，并展示了其在无标签场景下的有效性。


<details>
  <summary>Details</summary>
Motivation: 机器学习模型在自动化决策中广泛应用，确保其性能至关重要，而无标签环境下的概念漂移检测是挑战。

Method: 使用经典统计过程控制方法，提出灵活高效的无标签概念漂移检测算法，并结合新框架建模漂移检测。

Result: 在计算限制下，该方法比现有方法具有更好的统计功效，数值模拟展示了其优越性能。

Conclusion: 该算法在无标签环境下高效检测概念漂移，为机器学习模型性能监控提供了实用解决方案。

Abstract: Machine learning models are being increasingly used to automate decisions in
almost every domain, and ensuring the performance of these models is crucial
for ensuring high quality machine learning enabled services. Ensuring concept
drift is detected early is thus of the highest importance. A lot of research on
concept drift has focused on the supervised case that assumes the true labels
of supervised tasks are available immediately after making predictions.
Controlling for false positives while monitoring the performance of predictive
models used to make inference from extremely large datasets periodically, where
the true labels are not instantly available, becomes extremely challenging. We
propose a flexible and efficient concept drift detection algorithm that uses
classical statistical process control in a label-less setting to accurately
detect concept drifts. We shown empirically that under computational
constraints, our approach has better statistical power than previous known
methods. Furthermore, we introduce a new drift detection framework to model the
scenario of detecting drift (without labels) given prior detections, and show
our how our drift detection algorithm can be incorporated effectively into this
framework. We demonstrate promising performance via numerical simulations.

</details>


<div id='math.NA'></div>

# math.NA [[Back]](#toc)

### [457] [Normalized Radon Cumulative Distribution Transforms for Invariance and Robustness in Optimal Transport Based Image Classification](https://arxiv.org/abs/2506.08761)
*Matthias Beckmann,Robert Beinert,Jonas Bresch*

Main category: math.NA

TL;DR: 本文研究了max-normalized R-CDT和mean-normalized R-CDT在图像分类中的鲁棒性，特别是对非仿射变形和脉冲噪声的稳定性。


<details>
  <summary>Details</summary>
Motivation: 解决在仿射变换下图像分类的线性可分性问题，并进一步研究其对非仿射变形和噪声的鲁棒性。

Method: 提出max-normalized R-CDT和mean-normalized R-CDT，分析其在不同Wasserstein距离下的稳定性。

Result: 理论分析和数值实验表明，新方法在局部非仿射变形和脉冲噪声下仍能保持分类的线性可分性。

Conclusion: max-normalized和mean-normalized R-CDT是有效的特征提取器，适用于复杂变形和噪声环境下的图像分类。

Abstract: The Radon cumulative distribution transform (R-CDT), is an easy-to-compute
feature extractor that facilitates image classification tasks especially in the
small data regime. It is closely related to the sliced Wasserstein distance and
provably guaranties the linear separability of image classes that emerge from
translations or scalings. In many real-world applications, like the recognition
of watermarks in filigranology, however, the data is subject to general affine
transformations originating from the measurement process. To overcome this
issue, we recently introduced the so-called max-normalized R-CDT that only
requires elementary operations and guaranties the separability under arbitrary
affine transformations. The aim of this paper is to continue our study of the
max-normalized R-CDT especially with respect to its robustness against
non-affine image deformations. Our sensitivity analysis shows that its
separability properties are stable provided the Wasserstein-infinity distance
between the samples can be controlled. Since the Wasserstein-infinity distance
only allows small local image deformations, we moreover introduce a
mean-normalized version of the R-CDT. In this case, robustness relates to the
Wasserstein-2 distance and also covers image deformations caused by impulsive
noise for instance. Our theoretical results are supported by numerical
experiments showing the effectiveness of our novel feature extractors as well
as their robustness against local non-affine deformations and impulsive noise.

</details>


### [458] [Structured Variational $D$-Decomposition for Accurate and Stable Low-Rank Approximation](https://arxiv.org/abs/2506.08535)
*Ronald Katende*

Main category: math.NA

TL;DR: 论文提出了一种非正交矩阵分解方法$D$-decomposition，通过最小化正则化Frobenius损失实现，支持控制秩、稀疏性和条件数。相比传统分解方法（如LU或SVD），其计算复杂度更低且重建精度更高。


<details>
  <summary>Details</summary>
Motivation: 传统矩阵分解方法（如SVD）在稀疏性和噪声条件下表现不佳，因此需要一种更灵活且高效的非正交分解方法。

Method: 提出$D$-decomposition，形式为$A \approx P D Q$，通过交替最小化计算，复杂度为$\mathcal{O}(n^2k)$。

Result: 在MovieLens、MNIST等数据集上，相比截断SVD、CUR等方法，重建精度更高，尤其在稀疏和噪声条件下表现突出。

Conclusion: $D$-decomposition是一种高效且灵活的非正交矩阵分解方法，适用于稀疏和噪声数据。

Abstract: We introduce the $D$-decomposition, a non-orthogonal matrix factorization of
the form $A \approx P D Q$, where $P \in \mathbb{R}^{n \times k}$, $D \in
\mathbb{R}^{k \times k}$, and $Q \in \mathbb{R}^{k \times n}$. The
decomposition is defined variationally by minimizing a regularized Frobenius
loss, allowing control over rank, sparsity, and conditioning. Unlike algebraic
factorizations such as LU or SVD, it is computed by alternating minimization.
We establish existence and perturbation stability of the solution and show that
each update has complexity $\mathcal{O}(n^2k)$. Benchmarks against truncated
SVD, CUR, and nonnegative matrix factorization show improved reconstruction
accuracy on MovieLens, MNIST, Olivetti Faces, and gene expression matrices,
particularly under sparsity and noise.

</details>


### [459] [sparseGeoHOPCA: A Geometric Solution to Sparse Higher-Order PCA Without Covariance Estimation](https://arxiv.org/abs/2506.08670)
*Renjie Xu,Chong Wu,Maolin Che,Zhuoheng Ran,Yimin Wei,Hong Yan*

Main category: math.NA

TL;DR: 提出了一种名为sparseGeoHOPCA的新框架，用于稀疏高阶主成分分析（SHOPCA），通过几何视角解决高维张量分解问题。


<details>
  <summary>Details</summary>
Motivation: 传统方法在高维和不平衡数据场景下效率低且难以解释，sparseGeoHOPCA旨在提升计算效率和可解释性。

Method: 通过将输入张量沿每个模式展开，并将子问题转化为结构化二元线性优化问题，将非凸稀疏目标转化为可处理的几何形式。

Result: 理论证明了几何子问题与原始SHOPCA的等价性，并提供了数据依赖的性能保证。实验显示其在合成数据中准确恢复稀疏支持，并在压缩和图像重建中表现优异。

Conclusion: sparseGeoHOPCA在计算效率和鲁棒性方面具有显著优势，适用于高维数据场景。

Abstract: We propose sparseGeoHOPCA, a novel framework for sparse higher-order
principal component analysis (SHOPCA) that introduces a geometric perspective
to high-dimensional tensor decomposition. By unfolding the input tensor along
each mode and reformulating the resulting subproblems as structured binary
linear optimization problems, our method transforms the original nonconvex
sparse objective into a tractable geometric form. This eliminates the need for
explicit covariance estimation and iterative deflation, enabling significant
gains in both computational efficiency and interpretability, particularly in
high-dimensional and unbalanced data scenarios. We theoretically establish the
equivalence between the geometric subproblems and the original SHOPCA
formulation, and derive worst-case approximation error bounds based on
classical PCA residuals, providing data-dependent performance guarantees. The
proposed algorithm achieves a total computational complexity of
$O\left(\sum_{n=1}^{N} (k_n^3 + J_n k_n^2)\right)$, which scales linearly with
tensor size. Extensive experiments demonstrate that sparseGeoHOPCA accurately
recovers sparse supports in synthetic settings, preserves classification
performance under 10$\times$ compression, and achieves high-quality image
reconstruction on ImageNet, highlighting its robustness and versatility.

</details>


<div id='math.OC'></div>

# math.OC [[Back]](#toc)

### [460] [MOSS: Multi-Objective Optimization for Stable Rule Sets](https://arxiv.org/abs/2506.08030)
*Brian Liu,Rahul Mazumder*

Main category: math.OC

TL;DR: MOSS是一个多目标优化框架，用于构建稳定的决策规则集，结合了稀疏性、准确性和稳定性，并通过快速权衡分析帮助选择模型。


<details>
  <summary>Details</summary>
Motivation: 解决决策规则集中稀疏性、准确性和稳定性的多目标优化问题，提供高效的模型选择方法。

Method: 开发了专门的切割平面算法，快速计算帕累托前沿，适用于大规模问题。

Result: MOSS在预测性能和稳定性上优于现有规则集成方法。

Conclusion: MOSS为构建稳定且高效的决策规则集提供了有效工具。

Abstract: We present MOSS, a multi-objective optimization framework for constructing
stable sets of decision rules. MOSS incorporates three important criteria for
interpretability: sparsity, accuracy, and stability, into a single
multi-objective optimization framework. Importantly, MOSS allows a practitioner
to rapidly evaluate the trade-off between accuracy and stability in sparse rule
sets in order to select an appropriate model. We develop a specialized cutting
plane algorithm in our framework to rapidly compute the Pareto frontier between
these two objectives, and our algorithm scales to problem instances beyond the
capabilities of commercial optimization solvers. Our experiments show that MOSS
outperforms state-of-the-art rule ensembles in terms of both predictive
performance and stability.

</details>


### [461] [Continuous Policy and Value Iteration for Stochastic Control Problems and Its Convergence](https://arxiv.org/abs/2506.08121)
*Qi Feng,Gu Wang*

Main category: math.OC

TL;DR: 提出了一种连续策略-值迭代算法，通过Langevin型动态同时更新随机控制问题的值函数和最优控制的近似解。


<details>
  <summary>Details</summary>
Motivation: 解决熵正则化松弛控制问题和经典控制问题中的无限时域优化，结合机器学习中的分布采样和非凸学习技术。

Method: 利用Langevin型随机微分方程进行连续更新，结合策略迭代方向，同时优化值函数和最优控制。

Result: 在哈密顿量单调性条件下，证明了策略改进和收敛到最优控制。

Conclusion: 该方法为随机控制问题提供了一种高效的解决方案，结合了机器学习的先进技术。

Abstract: We introduce a continuous policy-value iteration algorithm where the
approximations of the value function of a stochastic control problem and the
optimal control are simultaneously updated through Langevin-type dynamics. This
framework applies to both the entropy-regularized relaxed control problems and
the classical control problems, with infinite horizon. We establish policy
improvement and demonstrate convergence to the optimal control under the
monotonicity condition of the Hamiltonian. By utilizing Langevin-type
stochastic differential equations for continuous updates along the policy
iteration direction, our approach enables the use of distribution sampling and
non-convex learning techniques in machine learning to optimize the value
function and identify the optimal control simultaneously.

</details>


### [462] [Solving Convex-Concave Problems with $\tilde{\mathcal{O}}(ε^{-4/7})$ Second-Order Oracle Complexity](https://arxiv.org/abs/2506.08362)
*Lesi Chen,Chengchang Liu,Luo Luo,Jingzhao Zhang*

Main category: math.OC

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Previous algorithms can solve convex-concave minimax problems $\min_{x \in
\mathcal{X}} \max_{y \in \mathcal{Y}} f(x,y)$ with
$\mathcal{O}(\epsilon^{-2/3})$ second-order oracle calls using Newton-type
methods. This result has been speculated to be optimal because the upper bound
is achieved by a natural generalization of the optimal first-order method. In
this work, we show an improved upper bound of
$\tilde{\mathcal{O}}(\epsilon^{-4/7})$ by generalizing the optimal second-order
method for convex optimization to solve the convex-concave minimax problem. We
further apply a similar technique to lazy Hessian algorithms and show that our
proposed algorithm can also be seen as a second-order ``Catalyst'' framework
(Lin et al., JMLR 2018) that could accelerate any globally convergent
algorithms for solving minimax problems.

</details>


### [463] [Sharper Convergence Rates for Nonconvex Optimisation via Reduction Mappings](https://arxiv.org/abs/2506.08428)
*Evan Markou,Thalaiyasingam Ajanthan,Stephen Gould*

Main category: math.OC

TL;DR: 论文提出了一种利用高维优化问题中解流形的几何结构，通过降维映射改善优化问题的框架。


<details>
  <summary>Details</summary>
Motivation: 高维优化问题的解集常形成光滑流形，若能利用这一结构，可加速优化收敛。

Method: 通过降维映射重新参数化参数空间，移除冗余方向，降低目标维度。

Result: 设计良好的降维映射能改善目标函数的曲率性质，加速梯度法的收敛。

Conclusion: 该框架为利用结构信息加速优化提供了理论支持，解释了实际算法中的性能提升。

Abstract: Many high-dimensional optimisation problems exhibit rich geometric structures
in their set of minimisers, often forming smooth manifolds due to
over-parametrisation or symmetries. When this structure is known, at least
locally, it can be exploited through reduction mappings that reparametrise part
of the parameter space to lie on the solution manifold. These reductions
naturally arise from inner optimisation problems and effectively remove
redundant directions, yielding a lower-dimensional objective. In this work, we
introduce a general framework to understand how such reductions influence the
optimisation landscape. We show that well-designed reduction mappings improve
curvature properties of the objective, leading to better-conditioned problems
and theoretically faster convergence for gradient-based methods. Our analysis
unifies a range of scenarios where structural information at optimality is
leveraged to accelerate convergence, offering a principled explanation for the
empirical gains observed in such optimisation algorithms.

</details>


### [464] [Optimization over Sparse Support-Preserving Sets: Two-Step Projection with Global Optimality Guarantees](https://arxiv.org/abs/2506.08558)
*William de Vazelhes,Xiao-Tong Yuan,Bin Gu*

Main category: math.OC

TL;DR: 本文提出了一种新的迭代硬阈值算法，用于处理稀疏优化中的额外支持保留约束，提供了全局收敛保证。


<details>
  <summary>Details</summary>
Motivation: 稀疏优化中，硬约束（如ℓ0伪范数）能控制稀疏性，但实际应用常需额外约束。现有方法通常需要闭式投影或仅提供局部收敛保证，无法满足全局收敛需求。

Method: 提出了一种新的迭代硬阈值算法，采用两步连续投影算子处理混合约束，并引入稀疏性与次优性的权衡。

Result: 在确定性、随机和零阶设置下，算法在受限强凸/平滑假设下提供了全局目标值保证。

Conclusion: 通过扩展经典三点引理，本文为稀疏优化中的混合约束问题提供了简洁且全局收敛的解决方案，改进了现有技术。

Abstract: In sparse optimization, enforcing hard constraints using the $\ell_0$
pseudo-norm offers advantages like controlled sparsity compared to convex
relaxations. However, many real-world applications demand not only sparsity
constraints but also some extra constraints. While prior algorithms have been
developed to address this complex scenario with mixed combinatorial and convex
constraints, they typically require the closed form projection onto the mixed
constraints which might not exist, and/or only provide local guarantees of
convergence which is different from the global guarantees commonly sought in
sparse optimization. To fill this gap, in this paper, we study the problem of
sparse optimization with extra \qw{\textit{support-preserving}} constraints
commonly encountered in the literature. We present a new variant of iterative
hard-thresholding algorithm equipped with a two-step consecutive projection
operator customized for these mixed constraints, serving as a simple
alternative to the Euclidean projection onto the mixed constraint. By
introducing a novel trade-off between sparsity relaxation and sub-optimality,
we provide global guarantees in objective value for the output of our
algorithm, in the deterministic, stochastic, and zeroth-order settings, under
the conventional restricted strong-convexity/smoothness assumptions. As a
fundamental contribution in proof techniques, we develop a novel extension of
the classic three-point lemma to the considered two-step non-convex projection
operator, which allows us to analyze the convergence in objective value in an
elegant way that has not been possible with existing techniques. In the
zeroth-order case, such technique also improves upon the state-of-the-art
result from de Vazelhes et. al. (2022), even in the case without additional
constraints, by allowing us to remove a non-vanishing system error present in
their work.

</details>


<div id='math.ST'></div>

# math.ST [[Back]](#toc)

### [465] [Generalizing while preserving monotonicity in comparison-based preference learning models](https://arxiv.org/abs/2506.08616)
*Julien Fageot,Peva Blanchard,Gilles Bareilles,Lê-Nguyên Hoang*

Main category: math.ST

TL;DR: 论文提出了一类新的线性广义Bradley-Terry模型，结合扩散先验，解决了现有比较偏好学习模型缺乏单调性保证的问题。


<details>
  <summary>Details</summary>
Motivation: 许多广泛使用的比较偏好学习模型（包括大语言模型）缺乏单调性保证，而现有单调性模型无法泛化到未比较数据。

Method: 提出线性广义Bradley-Terry模型，结合扩散先验，并确定嵌入条件的充分条件以保证单调性。

Result: 实验表明，新模型在有限数据集下提高了准确性，且单调性并非普遍保证。

Conclusion: 新模型在单调性和泛化能力上取得平衡，尤其适用于数据有限的情况。

Abstract: If you tell a learning model that you prefer an alternative $a$ over another
alternative $b$, then you probably expect the model to be monotone, that is,
the valuation of $a$ increases, and that of $b$ decreases. Yet, perhaps
surprisingly, many widely deployed comparison-based preference learning models,
including large language models, fail to have this guarantee. Until now, the
only comparison-based preference learning algorithms that were proved to be
monotone are the Generalized Bradley-Terry models. Yet, these models are unable
to generalize to uncompared data. In this paper, we advance the understanding
of the set of models with generalization ability that are monotone. Namely, we
propose a new class of Linear Generalized Bradley-Terry models with Diffusion
Priors, and identify sufficient conditions on alternatives' embeddings that
guarantee monotonicity. Our experiments show that this monotonicity is far from
being a general guarantee, and that our new class of generalizing models
improves accuracy, especially when the dataset is limited.

</details>
