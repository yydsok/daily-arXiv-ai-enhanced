<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 8]
- [cs.CL](#cs.CL) [Total: 5]
- [cs.CV](#cs.CV) [Total: 6]
- [cs.LG](#cs.LG) [Total: 5]
- [cs.MA](#cs.MA) [Total: 2]
- [cs.MM](#cs.MM) [Total: 2]
- [cs.RO](#cs.RO) [Total: 5]
- [cs.SD](#cs.SD) [Total: 3]
- [eess.AS](#eess.AS) [Total: 2]
- [econ.GN](#econ.GN) [Total: 1]
- [eess.SY](#eess.SY) [Total: 1]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [Can Media Act as a Soft Regulator of Safe AI Development? A Game Theoretical Analysis](https://arxiv.org/abs/2509.02650)
*Henrique Correia da Fonseca,António Fernandes,Zhao Song,Theodor Cimpeanu,Nataliya Balabanova,Adeela Bashir,Paolo Bova,Alessio Buscemi,Alessandro Di Stefano,Manh Hong Duong,Elias Fernandez Domingos,Ndidi Bianca Ogbo,Simon T. Powers,Daniele Proverbio,Zia Ush Shamszaman,Fernando P. Santos,The Anh Han,Marcus Krellner*

Main category: cs.AI

TL;DR: 程序员在利润和安全之间选择时常选择利润，媒体报道可通过捐失声誉来促使AI创造者采取安全措施，但需要高质量信息和合理成本条件。


<details>
  <summary>Details</summary>
Motivation: 探索媒体报道是否能够促使AI创造者采取安全措施，以实现AI技术的广泛采用。

Method: 通过进化游戏理论研究自利的创造者和用户群体，分析媒体报道对合作行为的影响。

Result: 媒体能够促进合作，但需要高质量信息和合理成本条件；如果信息不可靠或成本过高，合作就无法形成。

Conclusion: 媒体作为一种软性监管机制，可以在没有政府监管的情况下指导AI安全发展。

Abstract: When developers of artificial intelligence (AI) products need to decide
between profit and safety for the users, they likely choose profit.
Untrustworthy AI technology must come packaged with tangible negative
consequences. Here, we envisage those consequences as the loss of reputation
caused by media coverage of their misdeeds, disseminated to the public. We
explore whether media coverage has the potential to push AI creators into the
production of safe products, enabling widespread adoption of AI technology. We
created artificial populations of self-interested creators and users and
studied them through the lens of evolutionary game theory. Our results reveal
that media is indeed able to foster cooperation between creators and users, but
not always. Cooperation does not evolve if the quality of the information
provided by the media is not reliable enough, or if the costs of either
accessing media or ensuring safety are too high. By shaping public perception
and holding developers accountable, media emerges as a powerful soft regulator
-- guiding AI safety even in the absence of formal government oversight.

</details>


### [2] [The Future of Artificial Intelligence and the Mathematical and Physical Sciences (AI+MPS)](https://arxiv.org/abs/2509.02661)
*Andrew Ferguson,Marisa LaFleur,Lars Ruthotto,Jesse Thaler,Yuan-Sen Ting,Pratyush Tiwary,Soledad Villar,E. Paulo Alves,Jeremy Avigad,Simon Billinge,Camille Bilodeau,Keith Brown,Emmanuel Candes,Arghya Chattopadhyay,Bingqing Cheng,Jonathan Clausen,Connor Coley,Andrew Connolly,Fred Daum,Sijia Dong,Chrisy Xiyu Du,Cora Dvorkin,Cristiano Fanelli,Eric B. Ford,Luis Manuel Frutos,Nicolás García Trillos,Cecilia Garraffo,Robert Ghrist,Rafael Gomez-Bombarelli,Gianluca Guadagni,Sreelekha Guggilam,Sergei Gukov,Juan B. Gutiérrez,Salman Habib,Johannes Hachmann,Boris Hanin,Philip Harris,Murray Holland,Elizabeth Holm,Hsin-Yuan Huang,Shih-Chieh Hsu,Nick Jackson,Olexandr Isayev,Heng Ji,Aggelos Katsaggelos,Jeremy Kepner,Yannis Kevrekidis,Michelle Kuchera,J. Nathan Kutz,Branislava Lalic,Ann Lee,Matt LeBlanc,Josiah Lim,Rebecca Lindsey,Yongmin Liu,Peter Y. Lu,Sudhir Malik,Vuk Mandic,Vidya Manian,Emeka P. Mazi,Pankaj Mehta,Peter Melchior,Brice Ménard,Jennifer Ngadiuba,Stella Offner,Elsa Olivetti,Shyue Ping Ong,Christopher Rackauckas,Philippe Rigollet,Chad Risko,Philip Romero,Grant Rotskoff,Brett Savoie,Uros Seljak,David Shih,Gary Shiu,Dima Shlyakhtenko,Eva Silverstein,Taylor Sparks,Thomas Strohmer,Christopher Stubbs,Stephen Thomas,Suriyanarayanan Vaikuntanathan,Rene Vidal,Francisco Villaescusa-Navarro,Gregory Voth,Benjamin Wandelt,Rachel Ward,Melanie Weber,Risa Wechsler,Stephen Whitelam,Olaf Wiest,Mike Williams,Zhuoran Yang,Yaroslava G. Yingling,Bin Yu,Shuwen Yue,Ann Zabludoff,Huimin Zhao,Tong Zhang*

Main category: cs.AI

TL;DR: NSF研讨会报告，探讨AI与数学物理科学（MPS）领域的双向融合策略，包括促进交叉研究、建设跨学科社区、加强教育培训，并提出资助机构、教育机构和研究人员的优先事项建议。


<details>
  <summary>Details</summary>
Motivation: 理解数学物理科学领域（天文学、化学、材料研究、数学科学、物理学）如何最好地利用AI的未来发展并为其做出贡献，在AI快速发展的关键时刻加强AI与科学的联系。

Method: 通过NSF研讨会形成社区共识，提出三方面战略：1）双向促进AI+MPS研究；2）建设跨学科研究社区；3）加强MPS研究人员和学生的AI教育培训。

Result: 形成了MPS社区在2025年春季/夏季的视角总结，提出了具体的活动建议和战略优先事项，旨在使MPS社区成为AI+MPS变革潜力的领导者和充分受益者。

Conclusion: 需要采取积极主动的战略来加强AI与基础科学的联系，通过双向研究、社区建设和教育培训，使MPS领域能够在AI发展中发挥领导作用并充分利用其变革潜力。

Abstract: This community paper developed out of the NSF Workshop on the Future of
Artificial Intelligence (AI) and the Mathematical and Physics Sciences (MPS),
which was held in March 2025 with the goal of understanding how the MPS domains
(Astronomy, Chemistry, Materials Research, Mathematical Sciences, and Physics)
can best capitalize on, and contribute to, the future of AI. We present here a
summary and snapshot of the MPS community's perspective, as of Spring/Summer
2025, in a rapidly developing field. The link between AI and MPS is becoming
increasingly inextricable; now is a crucial moment to strengthen the link
between AI and Science by pursuing a strategy that proactively and thoughtfully
leverages the potential of AI for scientific discovery and optimizes
opportunities to impact the development of AI by applying concepts from
fundamental science. To achieve this, we propose activities and strategic
priorities that: (1) enable AI+MPS research in both directions; (2) build up an
interdisciplinary community of AI+MPS researchers; and (3) foster education and
workforce development in AI for MPS researchers and students. We conclude with
a summary of suggested priorities for funding agencies, educational
institutions, and individual researchers to help position the MPS community to
be a leader in, and take full advantage of, the transformative potential of
AI+MPS.

</details>


### [3] [Planning with Reasoning using Vision Language World Model](https://arxiv.org/abs/2509.02722)
*Delong Chen,Theo Moutakanni,Willy Chung,Yejin Bang,Ziwei Ji,Allen Bolourchi,Pascale Fung*

Main category: cs.AI

TL;DR: VLWM是一个基于视觉语言的世界模型，通过语言建模和双系统规划（系统1反应式解码和系统2反思式成本最小化规划）在自然视频上实现语义和时间抽象推理，在多个基准测试中达到最先进性能。


<details>
  <summary>Details</summary>
Motivation: 当前高层次世界模型在理解和推理具有语义和时间抽象的动作方面仍然不足，需要开发能够进行有效规划的世界模型。

Method: 使用LLM自优化方法从压缩的未来观察中提取目标，学习动作策略和动态模型，通过系统1反应式解码和系统2基于成本最小化的反思式规划，成本由自监督训练的评论家模型评估语义距离。

Result: 在VPA基准评估和PlannerArena人类评估中达到最先进性能，系统2比系统1Elo分数提升27%，在RoboVQA和WorldPrediction基准上也优于强VLM基线。

Conclusion: VLWM通过语言基础的世界建模和双系统规划框架，有效解决了高层次世界建模的挑战，为视觉规划任务提供了强大的解决方案。

Abstract: Effective planning requires strong world models, but high-level world models
that can understand and reason about actions with semantic and temporal
abstraction remain largely underdeveloped. We introduce the Vision Language
World Model (VLWM), a foundation model trained for language-based world
modeling on natural videos. Given visual observations, the VLWM first infers
the overall goal achievements then predicts a trajectory composed of
interleaved actions and world state changes. Those targets are extracted by
iterative LLM Self-Refine conditioned on compressed future observations
represented by Tree of Captions. The VLWM learns both an action policy and a
dynamics model, which respectively facilitates reactive system-1 plan decoding
and reflective system-2 planning via cost minimization. The cost evaluates the
semantic distance between the hypothetical future states given by VLWM
roll-outs and the expected goal state, and is measured by a critic model that
we trained in a self-supervised manner. The VLWM achieves state-of-the-art
Visual Planning for Assistance (VPA) performance on both benchmark evaluations
and our proposed PlannerArena human evaluations, where system-2 improves the
Elo score by +27% upon system-1. The VLWM models also outperforms strong VLM
baselines on RoboVQA and WorldPrediction benchmark.

</details>


### [4] [Deep Research is the New Analytics System: Towards Building the Runtime for AI-Driven Analytics](https://arxiv.org/abs/2509.02751)
*Matthew Russo,Tim Kraska*

Main category: cs.AI

TL;DR: 这篇论文提出了一种新的AI驱动分析运行时，结合了优化执行的语义操作符和深度研究系统的灵活性，通过让LLM组件编写和执行优化的语义操作符程序，实现了更高的性能和更低的成本。


<details>
  <summary>Details</summary>
Motivation: 解决现有语义操作符在大规模数据集上执行成本高、不适合交互式分析的问题，以及深度研究系统缺乏查询计划优化导致执行效率低的问题。

Method: 构建了一个原型系统，让深度研究系统中的LLM组件能够编写和执行经过优化的语义操作符程序，结合了两种方法的优势。

Result: 在两个基础查询上，该原型系统超过了手工编写的语义操作符程序和开放深度研究系统，与标准深度研究组件相比F1分数提高了1.95倍，成本和运行时间节省了最76.8%和72.7%。

Conclusion: 该研究为AI驱动分析领域提供了一种有前景的方向，通过结合优化执行和灵活动态执行的优势，能够在保持高准确性的同时显著降低运行成本。

Abstract: With advances in large language models (LLMs), researchers are creating new
systems that can perform AI-driven analytics over large unstructured datasets.
Recent work has explored executing such analytics queries using semantic
operators -- a declarative set of AI-powered data transformations with natural
language specifications. However, even when optimized, these operators can be
expensive to execute on millions of records and their iterator execution
semantics make them ill-suited for interactive data analytics tasks. In another
line of work, Deep Research systems have demonstrated an ability to answer
natural language question(s) over large datasets. These systems use one or more
LLM agent(s) to plan their execution, process the dataset(s), and iteratively
refine their answer. However, these systems do not explicitly optimize their
query plans which can lead to poor plan execution. In order for AI-driven
analytics to excel, we need a runtime which combines the optimized execution of
semantic operators with the flexibility and more dynamic execution of Deep
Research systems. As a first step towards this vision, we build a prototype
which enables Deep Research agents to write and execute optimized semantic
operator programs. We evaluate our prototype and demonstrate that it can
outperform a handcrafted semantic operator program and open Deep Research
systems on two basic queries. Compared to a standard open Deep Research agent,
our prototype achieves up to 1.95x better F1-score. Furthermore, even if we
give the agent access to semantic operators as tools, our prototype still
achieves cost and runtime savings of up to 76.8% and 72.7% thanks to its
optimized execution.

</details>


### [5] [Do LLM Modules Generalize? A Study on Motion Generation for Autonomous Driving](https://arxiv.org/abs/2509.02754)
*Mingyi Wang,Jingke Wang,Tengju Ye,Junbo Chen,Kaicheng Yu*

Main category: cs.AI

TL;DR: 本文系统评估了5个关键LLM模块在自动驾驶运动生成中的可迁移性，包括分词器设计、位置编码、预训练范式、后训练策略和测试时计算，在Waymo基准上取得了有竞争力的结果。


<details>
  <summary>Details</summary>
Motivation: 虽然LLM在自然语言处理领域取得突破，其组件在结构相似的自动驾驶运动生成任务中也有应用潜力，但缺乏对哪些LLM模块真正可迁移的系统性理解。

Method: 通过Waymo Sim Agents基准进行广泛实验，评估五个关键LLM模块（分词器设计、位置编码、预训练范式、后训练策略、测试时计算）在自动驾驶运动生成中的适应性。

Result: 实验表明，经过适当适配后，这些LLM模块能显著提升自动驾驶运动生成的性能，并在Sim Agents任务上取得了有竞争力的结果。

Conclusion: 研究确定了可有效迁移的技术，分析了其他技术失败的原因，并讨论了自动驾驶场景所需的具体适配方法，为LLM组件在相关领域的应用提供了系统指导。

Abstract: Recent breakthroughs in large language models (LLMs) have not only advanced
natural language processing but also inspired their application in domains with
structurally similar problems--most notably, autonomous driving motion
generation. Both domains involve autoregressive sequence modeling, token-based
representations, and context-aware decision making, making the transfer of LLM
components a natural and increasingly common practice. However, despite
promising early attempts, a systematic understanding of which LLM modules are
truly transferable remains lacking. In this paper, we present a comprehensive
evaluation of five key LLM modules--tokenizer design, positional embedding,
pre-training paradigms, post-training strategies, and test-time
computation--within the context of motion generation for autonomous driving.
Through extensive experiments on the Waymo Sim Agents benchmark, we demonstrate
that, when appropriately adapted, these modules can significantly improve
performance for autonomous driving motion generation. In addition, we identify
which techniques can be effectively transferred, analyze the potential reasons
for the failure of others, and discuss the specific adaptations needed for
autonomous driving scenarios. We evaluate our method on the Sim Agents task and
achieve competitive results.

</details>


### [6] [Plan Verification for LLM-Based Embodied Task Completion Agents](https://arxiv.org/abs/2509.02761)
*Ananth Hariharan,Vardhan Dongre,Dilek Hakkani-Tür,Gokhan Tur*

Main category: cs.AI

TL;DR: 提出基于LLM的迭代验证框架，通过Judge LLM批评和Planner LLM修订，逐步优化具身AI任务计划，提高轨迹质量和空间一致性


<details>
  <summary>Details</summary>
Motivation: LLM生成的任务计划和人类演示可能存在噪声动作、冗余导航和逻辑错误，影响策略质量，需要一种能够泛化处理各类错误的验证方法

Method: 使用自然语言提示的迭代验证框架：Judge LLM批评动作序列，Planner LLM应用修订，逐步清理和优化轨迹

Result: 在TEACh数据集上达到90%召回率和100%精确度，96.5%的序列最多需要3次迭代收敛，同时改善时间效率和空间动作组织

Conclusion: 该方法将计划验证确立为LLM在空间规划和动作优化中的可靠能力，为具身AI的模仿学习提供了可扩展的高质量训练数据生成路径

Abstract: Large language model (LLM) based task plans and corresponding human
demonstrations for embodied AI may be noisy, with unnecessary actions,
redundant navigation, and logical errors that reduce policy quality. We propose
an iterative verification framework in which a Judge LLM critiques action
sequences and a Planner LLM applies the revisions, yielding progressively
cleaner and more spatially coherent trajectories. Unlike rule-based approaches,
our method relies on natural language prompting, enabling broad generalization
across error types including irrelevant actions, contradictions, and missing
steps. On a set of manually annotated actions from the TEACh embodied AI
dataset, our framework achieves up to 90% recall and 100% precision across four
state-of-the-art LLMs (GPT o4-mini, DeepSeek-R1, Gemini 2.5, LLaMA 4 Scout).
The refinement loop converges quickly, with 96.5% of sequences requiring at
most three iterations, while improving both temporal efficiency and spatial
action organization. Crucially, the method preserves human error-recovery
patterns rather than collapsing them, supporting future work on robust
corrective behavior. By establishing plan verification as a reliable LLM
capability for spatial planning and action refinement, we provide a scalable
path to higher-quality training data for imitation learning in embodied AI.

</details>


### [7] [Key Principles in Cross-Domain Hyper-Heuristic Performance](https://arxiv.org/abs/2509.02782)
*Václav Sobotka,Lucas Kletzander,Nysret Musliu,Hana Rudová*

Main category: cs.AI

TL;DR: 通过系统分析低级赋值策略的三个关键转换原则（解决方案接受、重复执行和扰动强度），构建了一种简单但高效的跨域选择超级赋值策略，在多个实际应用领域超越了现有最先进方法并发现11个新的最佳解。


<details>
  <summary>Details</summary>
Motivation: 现有的选择超级赋值策略主要关注于从预定义集合中适应性选择低级赋值，而本文重点研究如何构造和转换这个集合的策略性设计。

Method: 系统分析了基于三个关键原则的转换方法：解决方案接受、低级赋值重复执行次数、以及扰动强度（即扰动性赋值影响解决方案的比例）。将这些转换应用于简单的随机选择机制来验证其效果。

Result: 通过合适的转换构造，简单的随机选择机制在三个具有挑战性的实际领域上超越了所有现有的最先进超级赋值策略，发现了11个新的最佳解。该方法在CHeSC竞赛标准测试中与冠军方法不相上下。将这些转换与最新的超级赋值策略结合后，在标准测试和实际领域上都超越了当前最先进方法。

Conclusion: 通过策略性转换低级赋值集合的构造方法，可以很大程度提升超级赋值策略的性能，甚至能让简单的随机选择机制达到领先水平，同时还能简化复杂算法的设计。

Abstract: Cross-domain selection hyper-heuristics aim to distill decades of research on
problem-specific heuristic search algorithms into adaptable general-purpose
search strategies. In this respect, existing selection hyper-heuristics
primarily focus on an adaptive selection of low-level heuristics (LLHs) from a
predefined set. In contrast, we concentrate on the composition of this set and
its strategic transformations. We systematically analyze transformations based
on three key principles: solution acceptance, LLH repetitions, and perturbation
intensity, i.e., the proportion of a solution affected by a perturbative LLH.
We demonstrate the raw effects of our transformations on a trivial unbiased
random selection mechanism. With an appropriately constructed transformation,
this trivial method outperforms all available state-of-the-art hyper-heuristics
on three challenging real-world domains and finds 11 new best-known solutions.
The same method is competitive with the winner of the CHeSC competition,
commonly used as the standard cross-domain benchmark. Moreover, we accompany
several recent hyper-heuristics with such strategic transformations. Using this
approach, we outperform the current state-of-the-art methods on both the CHeSC
benchmark and real-world domains while often simplifying their designs.

</details>


### [8] [Learning General Policies From Examples](https://arxiv.org/abs/2509.02794)
*Blai Bonet,Hector Geffner*

Main category: cs.AI

TL;DR: 提出基于采样计划泛化的符号化策略学习方法，使用命中集算法替代传统SAT/ASP方法，可处理百万级状态和数十万特征的大规模问题


<details>
  <summary>Details</summary>
Motivation: 现有组合学习方法虽然可生成可理解且正确的策略，但无法扩展到大规模问题，只能处理小规模训练实例和特征池

Method: 基于采样计划泛化的符号化方法，使用命中集算法确保结构终止和无环性，替代传统的SAT/ASP方法

Result: 方法能够有效处理包含数百万状态和数十万特征的大规模问题，在多个基准测试中展现出良好的可扩展性

Conclusion: 该方法解决了符号化策略学习方法的可扩展性问题，为大规模规划问题的策略学习提供了有效的解决方案

Abstract: Combinatorial methods for learning general policies that solve large
collections of planning problems have been recently developed. One of their
strengths, in relation to deep learning approaches, is that the resulting
policies can be understood and shown to be correct. A weakness is that the
methods do not scale up and learn only from small training instances and
feature pools that contain a few hundreds of states and features at most. In
this work, we propose a new symbolic method for learning policies based on the
generalization of sampled plans that ensures structural termination and hence
acyclicity. The proposed learning approach is not based on SAT/ASP, as previous
symbolic methods, but on a hitting set algorithm that can effectively handle
problems with millions of states, and pools with hundreds of thousands of
features. The formal properties of the approach are analyzed, and its
scalability is tested on a number of benchmarks.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [9] [DrDiff: Dynamic Routing Diffusion with Hierarchical Attention for Breaking the Efficiency-Quality Trade-off](https://arxiv.org/abs/2509.02785)
*Jusheng Zhang,Yijia Fan,Kaitong Cai,Zimeng Huang,Xiaofei Sun,Jian Wang,Chengpei Tang,Keze Wang*

Main category: cs.CL

TL;DR: DrDiff是一个新颖的长文本生成框架，通过动态专家调度、分层稀疏注意力和软吸收引导优化三项核心技术，解决了效率与质量之间的权衡问题。


<details>
  <summary>Details</summary>
Motivation: 解决长文本生成中效率与质量之间的权衡问题，传统方法在处理不同难度和长度的文本时计算效率低下。

Method: 1) 动态专家调度机制：根据文本复杂度智能分配计算资源；2) 分层稀疏注意力机制：将计算复杂度从O(n²)降低到O(n)；3) 软吸收引导优化策略：结合DPM-solver++减少扩散步骤。

Result: 在各种长文本生成基准测试中，DrDiff表现出优于现有最先进方法的性能。

Conclusion: DrDiff框架通过三项创新技术有效解决了长文本生成的效率-质量权衡问题，在保持模型性能的同时显著提升了生成速度。

Abstract: This paper introduces DrDiff, a novel framework for long-text generation that
overcomes the efficiency-quality trade-off through three core technologies.
First, we design a dynamic expert scheduling mechanism that intelligently
allocates computational resources during the diffusion process based on text
complexity, enabling more efficient handling of text generation tasks of
varying difficulty. Second, we introduce a Hierarchical Sparse Attention (HSA)
mechanism that adaptively adjusts attention patterns according to a variety of
input lengths, reducing computational complexity from O($n^2$) to O($n$) while
maintaining model performance. Finally, we propose a soft absorption guidance
optimization strategy that combines with DPM-solver++ to reduce diffusion
steps, significantly improving generation speed. Comprehensive experiments on
various long-text generation benchmarks demonstrate the superiority of our
DrDiff over the existing SOTA methods.

</details>


### [10] [SSVD: Structured SVD for Parameter-Efficient Fine-Tuning and Benchmarking under Domain Shift in ASR](https://arxiv.org/abs/2509.02830)
*Pu Wang,Shinji Watanabe,Hugo Van hamme*

Main category: cs.CL

TL;DR: 这篇论文综述了参数高效微调方法在语音识别中的应用，提出了结构化SVD导向微调方法，并在ESPnet中实现了多种PEFT方法的综合性能测评。


<details>
  <summary>Details</summary>
Motivation: 虽然LoRA在语音应用中广泛使用，但最新的PEFT变体如VeRA、DoRA、PiSSA和SVFT主要为语言和视觉任务而开发，在语音领域缺乏充分验证。需要在语音识别任务中系统性地集成和测评这些方法。

Method: 提出了结构化SVD导向(SSVD)微调方法，通过选择性旋转输入相关的右奇异向量而保持输出相关向量固定，以保留语义映射。在ESPnet中集成了多种PEFT方法并进行综合性能测评。

Result: 在域假移语音识别任务上进行了评估，包括儿童语音和方言变体，模型规模从0.1B到2B。SSVD方法能够以最少的可训练参数实现稳健的域适应，并提高效率。

Conclusion: 该研究为语音识别领域提供了全面的PEFT方法集成和性能分析，SSVD方法显示了在域适应中的优势。所有实现已在ESPnet中开源发布，支持可复现性和未来研究。

Abstract: Parameter-efficient fine-tuning (PEFT) has emerged as a scalable solution for
adapting large foundation models. While low-rank adaptation (LoRA) is widely
used in speech applications, its state-of-the-art variants, e.g., VeRA, DoRA,
PiSSA, and SVFT, are developed mainly for language and vision tasks, with
limited validation in speech. This work presents the first comprehensive
integration and benchmarking of these PEFT methods within ESPnet. We further
introduce structured SVD-guided (SSVD) fine-tuning, which selectively rotates
input-associated right singular vectors while keeping output-associated vectors
fixed to preserve semantic mappings. This design enables robust domain
adaptation with minimal trainable parameters and improved efficiency. We
evaluate all methods on domain-shifted speech recognition tasks, including
child speech and dialectal variation, across model scales from 0.1B to 2B. All
implementations are released in ESPnet to support reproducibility and future
work.

</details>


### [11] [Clustering Discourses: Racial Biases in Short Stories about Women Generated by Large Language Models](https://arxiv.org/abs/2509.02834)
*Gustavo Bonil,João Gondim,Marina dos Santos,Simone Hashiguti,Helena Maia,Nadia Silva,Helio Pedrini,Sandra Avila*

Main category: cs.CL

TL;DR: 本研究分析LLaMA 3.2-3B生成葡萄牙语短篇故事中对黑人和白人女性的叙事建构，发现存在三种主要话语表征，揭示了看似中立的文本实则强化殖民结构下的性别不平等。


<details>
  <summary>Details</summary>
Motivation: 探究大型语言模型在生成文本中如何再现和强化关于种族和性别的历史不平等结构，特别关注对黑人和白人女性的叙事建构。

Method: 从2100个生成文本中应用计算方法进行语义聚类，选择代表性样本进行质性话语分析，结合机器学习技术和人工分析。

Result: 识别出三种主要话语表征：社会超越、祖先神话化和主观自我实现，发现语法连贯的文本实际上固化殖民结构下的女性身体框架。

Conclusion: 研究提出了结合机器学习与质性话语分析的整合方法，揭示了AI生成文本中隐藏的殖民性别不平等结构，强调需要批判性审视语言模型的输出。

Abstract: This study investigates how large language models, in particular LLaMA
3.2-3B, construct narratives about Black and white women in short stories
generated in Portuguese. From 2100 texts, we applied computational methods to
group semantically similar stories, allowing a selection for qualitative
analysis. Three main discursive representations emerge: social overcoming,
ancestral mythification and subjective self-realization. The analysis uncovers
how grammatically coherent, seemingly neutral texts materialize a crystallized,
colonially structured framing of the female body, reinforcing historical
inequalities. The study proposes an integrated approach, that combines machine
learning techniques with qualitative, manual discourse analysis.

</details>


### [12] [IDEAlign: Comparing Large Language Models to Human Experts in Open-ended Interpretive Annotations](https://arxiv.org/abs/2509.02855)
*Hyunji Nam,Lucia Langlois,James Malamut,Mei Tan,Dorottya Demszky*

Main category: cs.CL

TL;DR: 提出了IDEAlgin评估框架，通过"三选一"任务来评估LLM生成解释性标注与专家标注的相似性，发现基于向量的指标效果不佳，而LLM作为评判者能显著提升与专家判断的一致性。


<details>
  <summary>Details</summary>
Motivation: LLM越来越多地应用于开放式解释性标注任务，但缺乏可扩展的方法来评估LLM生成标注与专家标注的相似性，特别是在教育和研究领域。

Method: 提出IDEAlgin基准测试范式，使用"三选一"三元组判断任务收集专家相似性评分，并比较各种相似性指标（基于向量的方法和LLM作为评判者）与人类基准的一致性。

Result: 基于向量的指标无法捕捉专家关注的细微相似性维度，而通过IDEAlgin提示的LLM相比传统词汇和向量指标，与专家判断的一致性提高了9-30%。

Conclusion: IDEAlgin为大规模评估LLM在开放式专家标注任务中的表现提供了有前景的范式，有助于在教育等领域负责任地部署LLM。

Abstract: Large language models (LLMs) are increasingly applied to open-ended,
interpretive annotation tasks, such as thematic analysis by researchers or
generating feedback on student work by teachers. These tasks involve free-text
annotations requiring expert-level judgments grounded in specific objectives
(e.g., research questions or instructional goals). Evaluating whether
LLM-generated annotations align with those generated by expert humans is
challenging to do at scale, and currently, no validated, scalable measure of
similarity in ideas exists. In this paper, we (i) introduce the scalable
evaluation of interpretive annotation by LLMs as a critical and understudied
task, (ii) propose IDEAlgin, an intuitive benchmarking paradigm for capturing
expert similarity ratings via a "pick-the-odd-one-out" triplet judgment task,
and (iii) evaluate various similarity metrics, including vector-based ones
(topic models, embeddings) and LLM-as-a-judge via IDEAlgin, against these human
benchmarks. Applying this approach to two real-world educational datasets
(interpretive analysis and feedback generation), we find that vector-based
metrics largely fail to capture the nuanced dimensions of similarity meaningful
to experts. Prompting LLMs via IDEAlgin significantly improves alignment with
expert judgments (9-30% increase) compared to traditional lexical and
vector-based metrics. These results establish IDEAlgin as a promising paradigm
for evaluating LLMs against open-ended expert annotations at scale, informing
responsible deployment of LLMs in education and beyond.

</details>


### [13] [A-SEA3L-QA: A Fully Automated Self-Evolving, Adversarial Workflow for Arabic Long-Context Question-Answer Generation](https://arxiv.org/abs/2509.02864)
*Kesen Wang,Daulet Toibazar,Pedro J. Moreno*

Main category: cs.CL

TL;DR: 一种防御性自成长上下文阿拉伯语问答生成工作流，通过多个专门的大视觉语言模型协同工作，实现无人干预的迭代精炼和持续学习。


<details>
  <summary>Details</summary>
Motivation: 解决阿拉伯语长上下文问答生成的挑战，提高阿拉伯语大视觉语言模型的长文档理解能力。

Method: 设计了一个结构化的对抗性工作流，包含问题生成器、评估器和答案生成器群，通过闭环循环进行迭代精炼。

Result: 系统显著超越了静态流水线，大幅提升了阿拉伯语LVLM的长上下文理解能力，并发布了AraLongBench大规模测试集。

Conclusion: 该自成长工作流为阿拉伯语长文档理解带来了重大进步，通过自动化对抗过程实现了持续性能提升。

Abstract: We present an end-to-end, self-evolving adversarial workflow for long-context
Question-Answer (QA) Generation in Arabic. By orchestrating multiple
specialized LVLMs: a question generator, an evaluator, and a swarm of answer
generators, our system iteratively refines its own performance without any
human intervention. Starting from raw, multi-page Arabic documents across
diverse domains, the question generator produces fine-grained, context-aware
queries to be tackled by the answer generator swarm, and the evaluator assesses
and feeds back quality metrics. This closed-loop cycle enables continuous
learning: low-confidence outputs trigger automated re-generation and model
updates, progressively enhancing question difficulty and relevance. Moreover,
we set the quality metrics as a tunable hyperparameter, enabling question
generation at controllable and customizable difficulty levels. We release
AraLongBench, a large-scale Arabic benchmark of single- and multi-page
challenges spanning hundreds of pages, and demonstrate that our self-evolving
workflow substantially outperform static pipelines, markedly boosting the
long-context comprehension capabilities of leading Arabic Large Vision Language
Models (LVLMs). Lastly, we also meticulously architect a fully automated
agentic workflow for long-context Arabic document collection.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [14] [2nd Place Solution for CVPR2024 E2E Challenge: End-to-End Autonomous Driving Using Vision Language Model](https://arxiv.org/abs/2509.02659)
*Zilong Guo,Yi Luo,Long Sha,Dongxu Wang,Panqu Wang,Chenyang Xu,Yi Yang*

Main category: cs.CV

TL;DR: 基于多模态视觉语言模型(VLM)的端到端自主驾驶方案，仅使用单目摄像头即取得领先性能


<details>
  <summary>Details</summary>
Motivation: 探索大语言模型（LLM）和多模态视觉语言模型（VLM）是否能够提升端到端自主驾驶任务的性能

Method: 结合端到端架构设计和知识丰富的VLM模型

Result: 在驾驶任务上取得了印象深刻的性能，是目前最佳的单目摄像头解决方案

Conclusion: 证明了基于视觉的驾驶方法的有效性，展示了端到端驾驶任务的很大潜力

Abstract: End-to-end autonomous driving has drawn tremendous attention recently. Many
works focus on using modular deep neural networks to construct the end-to-end
archi-tecture. However, whether using powerful large language models (LLM),
especially multi-modality Vision Language Models (VLM) could benefit the
end-to-end driving tasks remain a question. In our work, we demonstrate that
combining end-to-end architectural design and knowledgeable VLMs yield
impressive performance on the driving tasks. It is worth noting that our method
only uses a single camera and is the best camera-only solution across the
leaderboard, demonstrating the effectiveness of vision-based driving approach
and the potential for end-to-end driving tasks.

</details>


### [15] [VQualA 2025 Challenge on Engagement Prediction for Short Videos: Methods and Results](https://arxiv.org/abs/2509.02969)
*Dasong Li,Sizhuo Ma,Hang Hua,Wenjie Li,Jian Wang,Chris Wei Zhou,Fengbin Guan,Xin Li,Zihao Yu,Yiting Lu,Ru-Ling Liao,Yan Ye,Zhibo Chen,Wei Sun,Linhan Cao,Yuqin Cao,Weixia Zhang,Wen Wen,Kaiwei Zhang,Zijian Chen,Fangfang Lu,Xiongkuo Min,Guangtao Zhai,Erjia Xiao,Lingfeng Zhang,Zhenjie Su,Hao Cheng,Yu Liu,Renjing Xu,Long Chen,Xiaoshuai Hao,Zhenpeng Zeng,Jianqin Wu,Xuxu Wang,Qian Yu,Bo Hu,Weiwei Wang,Pinxin Liu,Yunlong Tang,Luchuan Song,Jinxi He,Jiaru Wu,Hanjia Lyu*

Main category: cs.CV

TL;DR: VQualA 2025挑战赛专注于短UGC视频的参与度预测，使用真实用户交互数据，吸引了97名参与者提交15份有效测试方案，推动了多模态特征建模的发展。


<details>
  <summary>Details</summary>
Motivation: 理解和建模社交媒体平台上用户生成短视频的受欢迎程度，促进能够捕捉影响用户参与度复杂因素的稳健建模策略。

Method: 使用包含真实用户交互参与度指标的新短格式UGC数据集，参与者探索了包括视觉内容、音频和创作者提供的元数据在内的多模态特征。

Result: 挑战赛吸引了97名参与者，收到了15份有效的测试提交，在短格式UGC视频参与度预测方面取得了显著进展。

Conclusion: 该挑战赛成功推动了短视频参与度预测领域的发展，为理解用户生成内容的多模态特征建模提供了重要贡献。

Abstract: This paper presents an overview of the VQualA 2025 Challenge on Engagement
Prediction for Short Videos, held in conjunction with ICCV 2025. The challenge
focuses on understanding and modeling the popularity of user-generated content
(UGC) short videos on social media platforms. To support this goal, the
challenge uses a new short-form UGC dataset featuring engagement metrics
derived from real-world user interactions. This objective of the Challenge is
to promote robust modeling strategies that capture the complex factors
influencing user engagement. Participants explored a variety of multi-modal
features, including visual content, audio, and metadata provided by creators.
The challenge attracted 97 participants and received 15 valid test submissions,
contributing significantly to progress in short-form UGC video engagement
prediction.

</details>


### [16] [PixFoundation 2.0: Do Video Multi-Modal LLMs Use Motion in Visual Grounding?](https://arxiv.org/abs/2509.02807)
*Mennatullah Siam*

Main category: cs.CV

TL;DR: 该论文提出了MoCentric-Bench基准测试，用于评估视频多模态大语言模型在像素级视觉定位中对运动信息的理解能力，发现现有模型过度依赖静态外观线索而非真正的时序推理。


<details>
  <summary>Details</summary>
Motivation: 当前视频MLLMs在像素级视觉定位方面的能力研究不足，现有基准测试存在缺陷，单帧图像往往就能满足运动指代表达式的需求，无法真正测试模型对运动信息的理解。

Method: 提出了四种运动中心探测技术，构建了MoCentric-Bench基准测试，确保评估模型对运动与语言交互的理解，而非仅依赖静态外观线索。还建立了强单帧基线并探索了运动中心适应技术。

Result: 研究发现现有视频MLLMs在运动理解方面存在不足，强单帧基线方法在性能上与先前方法相当或更优，提出的运动中心适应技术在新基准上达到了最先进性能。

Conclusion: 该研究挑战未来模型改进密集时空定位和像素级视频理解能力，强调了真正运动理解在视频MLLMs中的重要性。

Abstract: Multi-modal large language models (MLLMs) have shown impressive
generalization across tasks using images and text modalities. While their
extension to video has enabled tasks such as video question answering and video
captioning, their pixel-level visual grounding abilities are less studied. In
this work, we raise the pertinent question of whether motion is used in
pixel-level visual grounding and whether video MLLMs can segment objects based
on natural language expressions describing their motion patterns. We identify
the shortcomings in the current benchmarks, where we show that a single frame
can often suffice for capturing the motion referring expression without any
temporal reasoning. To address this, we introduce four motion-centric probing
techniques, particularly designed for the visual grounding task, to study video
MLLMs' ability to identify true motion from a fake one and their ability to
grasp the motion order. Consequently, we provide a motion-centric benchmark,
MoCentric-Bench. It ensures that video MLLMs are evaluated towards leveraging
the interaction between motion and language rather than being dominated by
static appearance cues emphasized in existing visual grounding datasets. We
further establish strong single-image baselines that are on par with or
outperform prior methods. Finally, we explore simple motion-centric adaptation
techniques that provide state-of-the-art performance on our MoCentric-Bench.
Our motion-centric benchmark, evaluation and findings challenge future models
to improve dense spatiotemporal grounding and pixel-level understanding within
videos. Code and datasets will be made publicly available at
https://github.com/MSiam/PixFoundation-2.0.git.

</details>


### [17] [Multi-Scale Deep Learning for Colon Histopathology: A Hybrid Graph-Transformer Approach](https://arxiv.org/abs/2509.02851)
*Sadra Saremi,Amirhossein Ahmadkhan Kordbacheh*

Main category: cs.CV

TL;DR: 基于Transformer咈CNN的混合深度学习模型HG-TNet，通过多尺度特征提取和自监督学习，在结肠癌病理图像分析中实现了更高的识别准确率。


<details>
  <summary>Details</summary>
Motivation: 早期检测结肠癌对预防病情恶化至关重要，需要更准确的病理图像分析技术。

Method: 提出HG-TNet混合模型，结合Transformer提取全局上下文关系咈CNN抓取局部细节特征，使用胎囊网络保持空间结构信息，加上自监督学习旋转预测任务。

Result: 在LC25000数据集上表现超过标准架构，在准确率咈损失函数方面都取得更好结果。

Conclusion: 该混合方法通过多模态特征提取咈自监督学习，为结肠癌病理图像分析提供了更有效的解决方案。

Abstract: Colon cancer also known as Colorectal cancer, is one of the most malignant
types of cancer worldwide. Early-stage detection of colon cancer is highly
crucial to prevent its deterioration. This research presents a hybrid
multi-scale deep learning architecture that synergizes capsule networks, graph
attention mechanisms, transformer modules, and residual learning to advance
colon cancer classification on the Lung and Colon Cancer Histopathological
Image Dataset (LC25000) dataset. The proposed model in this paper utilizes the
HG-TNet model that introduces a hybrid architecture that joins strength points
in transformers and convolutional neural networks to capture multi-scale
features in histopathological images. Mainly, a transformer branch extracts
global contextual bonds by partitioning the image into patches by
convolution-based patch embedding and then processing these patches through a
transformer encoder. Analogously, a dedicated CNN branch captures fine-grained,
local details through successive Incorporation these diverse features, combined
with a self-supervised rotation prediction objective, produce a robust
diagnostic representation that surpasses standard architectures in performance.
Results show better performance not only in accuracy or loss function but also
in these algorithms by utilizing capsule networks to preserve spatial orders
and realize how each element individually combines and forms whole structures.

</details>


### [18] [PRECISE-AS: Personalized Reinforcement Learning for Efficient Point-of-Care Echocardiography in Aortic Stenosis Diagnosis](https://arxiv.org/abs/2509.02898)
*Armin Saadat,Nima Hashemi,Hooman Vaseli,Michael Y. Tsang,Christina Luong,Michiel Van de Panne,Teresa S. M. Tsang,Purang Abolmaesumi*

Main category: cs.CV

TL;DR: 基于强化学习的主动视频采集框架，用于优化主动脉狭窄诊断的超声心动图视频选择，在减少47%视频采集量的情况下达到80.6%的分类准确率


<details>
  <summary>Details</summary>
Motivation: 解决主动脉狭窄诊断中超声心动图资源受限的问题，特别是在农村和服务不足地区，传统方法需要固定视频集且依赖操作者专业知识

Method: 提出强化学习驱动的主动视频采集框架，动态选择每位患者最具信息量的超声心动图视频，持续评估是否需要额外成像

Result: 在2,572名患者数据上测试，仅使用47%的视频量就实现了80.6%的分类准确率

Conclusion: 该方法能够提高主动脉狭窄诊断的效率、可扩展性和个性化程度，使超声心动图评估更加高效

Abstract: Aortic stenosis (AS) is a life-threatening condition caused by a narrowing of
the aortic valve, leading to impaired blood flow. Despite its high prevalence,
access to echocardiography (echo), the gold-standard diagnostic tool, is often
limited due to resource constraints, particularly in rural and underserved
areas. Point-of-care ultrasound (POCUS) offers a more accessible alternative
but is restricted by operator expertise and the challenge of selecting the most
relevant imaging views. To address this, we propose a reinforcement learning
(RL)-driven active video acquisition framework that dynamically selects each
patient's most informative echo videos. Unlike traditional methods that rely on
a fixed set of videos, our approach continuously evaluates whether additional
imaging is needed, optimizing both accuracy and efficiency. Tested on data from
2,572 patients, our method achieves 80.6% classification accuracy while using
only 47% of the echo videos compared to a full acquisition. These results
demonstrate the potential of active feature acquisition to enhance AS
diagnosis, making echocardiographic assessments more efficient, scalable, and
personalized. Our source code is available at:
https://github.com/Armin-Saadat/PRECISE-AS.

</details>


### [19] [LiGuard: A Streamlined Open-Source Framework for Rapid & Interactive Lidar Research](https://arxiv.org/abs/2509.02902)
*Muhammad Shahbaz,Shaurya Agarwal*

Main category: cs.CV

TL;DR: LiGuard是一个开源的激光雷达数据处理框架，旨在解决研究中的代码重复问题，提供数据I/O、预处理/后处理和常用算法的内置支持，支持快速开发和结果可视化。


<details>
  <summary>Details</summary>
Motivation: 激光雷达自主移动和智能交通系统研究中存在代码重复开发问题，不同研究共享多个方法步骤但需要重复实现，数据或算法的小变化可能导致代码大修改。

Method: 开发LiGuard开源软件框架，提供数据I/O、预处理/后处理、常用算法的内置支持，支持交互式算法添加/删除/重排序和参数调整，以及分类、检测、分割和跟踪任务的结果可视化。

Result: 通过案例研究证明了LiGuard的有效性，框架能够生成结构化目录的代码文件，便于项目整体或单个组件的共享和重用。

Conclusion: LiGuard框架成功解决了激光雷达研究中代码重复开发的问题，提高了研究效率，促进了代码共享和重用。

Abstract: There is a growing interest in the development of lidar-based autonomous
mobility and Intelligent Transportation Systems (ITS). To operate and research
on lidar data, researchers often develop code specific to application niche.
This approach leads to duplication of efforts across studies that, in many
cases, share multiple methodological steps such as data input/output (I/O),
pre/post processing, and common algorithms in multi-stage solutions. Moreover,
slight changes in data, algorithms, and/or research focus may force major
revisions in the code. To address these challenges, we present LiGuard, an
open-source software framework that allows researchers to: 1) rapidly develop
code for their lidar-based projects by providing built-in support for data I/O,
pre/post processing, and commonly used algorithms, 2) interactively
add/remove/reorder custom algorithms and adjust their parameters, and 3)
visualize results for classification, detection, segmentation, and tracking
tasks. Moreover, because it creates all the code files in structured
directories, it allows easy sharing of entire projects or even the individual
components to be reused by other researchers. The effectiveness of LiGuard is
demonstrated via case studies.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [20] [The Lifecycle Principle: Stabilizing Dynamic Neural Networks with State Memory](https://arxiv.org/abs/2509.02575)
*Zichuan Yang*

Main category: cs.LG

TL;DR: 通过长期神经元停用和独创的状态记忆机制，该方法提出了一种更强的正则化方式，有效解决了神经元复活时的训练不稳定问题，提升了模型的普通化能力和稳健性。


<details>
  <summary>Details</summary>
Motivation: 传统正则化方法如Dropout仅作短暂的神经元变化，本研究探索长期神经元停用的更强正则化效果，但这种方法会导致神经元复活时的严重训练不稳定问题。

Method: 提出Lifecycle（LC）原则，核心是状态记忆机制。当神经元被复活时，不是重新初始化权重，而是恢复到其最后一次有效运行的参数状态，保留已学习知识并避免破坏性的优化冲击。

Result: 理论分析显示Lifecycle原则能够平滑损失地形，导向更平坦的最小值，这些最小值通常与更好的普通化能力相关。图像分类实验表明该方法能够提升普通化能力和稳健性。

Conclusion: 状态记忆机制是实现性能提升的关键要素，通过保留神经元历史有效状态，有效解决了长期神经元停用带来的训练不稳定问题，为深度学习正则化提供了新的视角。

Abstract: I investigate a stronger form of regularization by deactivating neurons for
extended periods, a departure from the temporary changes of methods like
Dropout. However, this long-term dynamism introduces a critical challenge:
severe training instability when neurons are revived with random weights. To
solve this, I propose the Lifecycle (LC) principle, a regularization mechanism
centered on a key innovation: state memory. Instead of re-initializing a
revived neuron, my method restores its parameters to their last known effective
state. This process preserves learned knowledge and avoids destructive
optimization shocks. My theoretical analysis reveals that the LC principle
smooths the loss landscape, guiding optimization towards flatter minima
associated with better generalization. Experiments on image classification
benchmarks demonstrate that my method improves generalization and robustness.
Crucially, ablation studies confirm that state memory is essential for
achieving these gains.

</details>


### [21] [Latent Variable Modeling in Multi-Agent Reinforcement Learning via Expectation-Maximization for UAV-Based Wildlife Protection](https://arxiv.org/abs/2509.02579)
*Mazyar Taghavi,Rahman Farnoosh*

Main category: cs.LG

TL;DR: 提出基于期望最大化(EM)的隐变量建模方法，结合多智能体强化学习(MARL)实现无人机协同保护濒危野生动物，在伊朗豹保护场景中表现优于PPO和DDPG算法。


<details>
  <summary>Details</summary>
Motivation: 解决在广阔且部分可观测环境中实时响应非法盗猎的挑战，保护濒危野生动物需要高效的无人机协同巡逻策略。

Method: 采用期望最大化(EM)算法建立隐变量模型，处理隐藏环境因素和智能体间动态关系，通过多智能体强化学习框架实现10架无人机的协同决策。

Result: 在自定义仿真环境中，相比PPO和DDPG等标准算法，EM-MARL框架在检测精度、适应性和策略收敛性方面表现出显著优势。

Conclusion: EM推理与MARL的结合能够有效提升复杂高风险保护场景中的分散决策能力，为野生动物保护提供了有前景的技术方案。

Abstract: Protecting endangered wildlife from illegal poaching presents a critical
challenge, particularly in vast and partially observable environments where
real-time response is essential. This paper introduces a novel
Expectation-Maximization (EM) based latent variable modeling approach in the
context of Multi-Agent Reinforcement Learning (MARL) for Unmanned Aerial
Vehicle (UAV) coordination in wildlife protection. By modeling hidden
environmental factors and inter-agent dynamics through latent variables, our
method enhances exploration and coordination under uncertainty.We implement and
evaluate our EM-MARL framework using a custom simulation involving 10 UAVs
tasked with patrolling protected habitats of the endangered Iranian leopard.
Extensive experimental results demonstrate superior performance in detection
accuracy, adaptability, and policy convergence when compared to standard
algorithms such as Proximal Policy Optimization (PPO) and Deep Deterministic
Policy Gradient (DDPG). Our findings underscore the potential of combining EM
inference with MARL to improve decentralized decisionmaking in complex,
high-stakes conservation scenarios. The full implementation, simulation
environment, and training scripts are publicly available on GitHub.

</details>


### [22] [Beyond Synthetic Augmentation: Group-Aware Threshold Calibration for Robust Balanced Accuracy in Imbalanced Learning](https://arxiv.org/abs/2509.02592)
*Hunter Gittlin*

Main category: cs.LG

TL;DR: 针对类别不平衡问题，本文提出群体感知阈值校准方法，通过为不同人口统计群体设置不同决策阈值，相比传统合成数据生成方法（如SMOTE和CT-GAN）在平衡准确率和最差群体表现方面都取得更好效果。


<details>
  <summary>Details</summary>
Motivation: 传统解决类别不平衡的方法（如合成数据生成）往往带来新问题，需要寻找更简单有效的解决方案。群体间性能差异显著，单一阈值方法无法满足不同群体的最优性能需求。

Method: 采用群体感知阈值校准策略，为不同人口统计群体分别设置最优决策阈值，而不是使用统一的全局阈值。在七个模型家族（线性、树基、实例基和提升方法）上进行广泛实验验证。

Result: 群体特定阈值方法比SMOTE和CT-GAN增强模型获得1.5-4%更高的平衡准确率，同时改善了最差群体的平衡准确率。该方法在平衡准确率和最差群体性能之间实现了帕累托最优。

Conclusion: 群体感知阈值校准提供了一种更简单、可解释性更强且更有效的类别不平衡解决方案，相比合成数据生成方法具有明显优势，且两种方法结合使用收益有限。

Abstract: Class imbalance remains a fundamental challenge in machine learning, with
traditional solutions often creating as many problems as they solve. We
demonstrate that group-aware threshold calibration--setting different decision
thresholds for different demographic groups--provides superior robustness
compared to synthetic data generation methods. Through extensive experiments,
we show that group-specific thresholds achieve 1.5-4% higher balanced accuracy
than SMOTE and CT-GAN augmented models while improving worst-group balanced
accuracy. Unlike single-threshold approaches that apply one cutoff across all
groups, our group-aware method optimizes the Pareto frontier between balanced
accuracy and worst-group balanced accuracy, enabling fine-grained control over
group-level performance. Critically, we find that applying group thresholds to
synthetically augmented data yields minimal additional benefit, suggesting
these approaches are fundamentally redundant. Our results span seven model
families including linear, tree-based, instance-based, and boosting methods,
confirming that group-aware threshold calibration offers a simpler, more
interpretable, and more effective solution to class imbalance.

</details>


### [23] [Preference Robustness for DPO with Applications to Public Health](https://arxiv.org/abs/2509.02709)
*Cheol Woo Kim,Shresth Verma,Mauricio Tec,Milind Tambe*

Main category: cs.LG

TL;DR: 基于DPO的健壁精粒度调优算法DPO-PRO，通过分布健壁优化处理人类偏好不确定性，在公共健康资源分配任务中显著提升了对噪声偏好信号的健壁性，且推理成本更低。


<details>
  <summary>Details</summary>
Motivation: 公共健康领域的连续资源分配问题具有复杂和模糊的目标，加之数据有限，使得基于人类偏好的LLM微调对准很具挑战。需要一种能够处理偏好分布不确定性的健壁算法。

Method: 提出DPO-PRO算法，基于直接偏好优化(DPO)，采用轻量级的分布健壁优化(DRO)来处理偏好分布的不确定性。与以前的DRO-DPO方法相比，DPO-PRO更不保守。

Result: 在ARMMAN母婴移动健康项目和标准对准测试集上评估，DPO-PRO在噪声偏好信号下显著提升了健壁性，且与自反思基线方法相比有相似性能但推理成本更低。

Conclusion: DPO-PRO是一种高效的健壁微调方法，能够在复杂公共健康场景中处理人类偏好的不确定性，同时保持低计算成本。

Abstract: We study an LLM fine-tuning task for designing reward functions for
sequential resource allocation problems in public health, guided by human
preferences expressed in natural language. This setting presents a challenging
testbed for alignment due to complex and ambiguous objectives and limited data
availability. We propose DPO-PRO, a robust fine-tuning algorithm based on
Direct Preference Optimization (DPO), which accounts for uncertainty in the
preference distribution using a lightweight Distributionally Robust
Optimization (DRO) formulation. Unlike prior DRO-based DPO methods, DPO-PRO is
significantly less conservative. We evaluate DPO-PRO on a real-world maternal
mobile health program operated by the non-profit organization ARMMAN, as well
as on standard alignment benchmarks. Experimental results demonstrate that our
method consistently improves robustness to noisy preference signals compared to
existing DPO variants. Moreover, DPO-PRO achieves comparable performance to
prior self-reflection-based baseline for reward function design, while
requiring significantly lower inference-time cost.

</details>


### [24] [Imitate Optimal Policy: Prevail and Induce Action Collapse in Policy Gradient](https://arxiv.org/abs/2509.02737)
*Zhongzhu Zhou,Yibo Yang,Ziyan Chen,Fengxiang Bie,Haojun Xia,Xiaoxia Wu,Robert Wu,Ben Athiwaratkun,Bernard Ghanem,Shuaiwen Leon Song*

Main category: cs.LG

TL;DR: 改进的策略梯度方法ACPG，通过固定ETF结构作为动作选择层目标，促进动作冲击现象的发生，提升攻击性能和稳定性


<details>
  <summary>Details</summary>
Motivation: 观察到在某些约束条件下，优化策略神经网络会出现类似神经冲击的"动作冲击"现象，这种结构能最大化分离不同动作间的角度

Method: 提出ACPG方法，在动作选择层中固定一个合成的简单等角紧凑框架（ETF）作为目标配置，促使策略网络产生理想的配置

Result: 在多个OpenAI Gym环境中证明，ACPG方法可以集成到任何离散策略梯度方法中，更快速、更稳健地提升奖励性能

Conclusion: 通过固定ETF结构作为动作选择层的目标配置，可以有效地促进动作冲击现象的发生，从而学习到优化策略且保持理想的空间配置

Abstract: Policy gradient (PG) methods in reinforcement learning frequently utilize
deep neural networks (DNNs) to learn a shared backbone of feature
representations used to compute likelihoods in an action selection layer.
Numerous studies have been conducted on the convergence and global optima of
policy networks, but few have analyzed representational structures of those
underlying networks. While training an optimal policy DNN, we observed that
under certain constraints, a gentle structure resembling neural collapse, which
we refer to as Action Collapse (AC), emerges. This suggests that 1) the
state-action activations (i.e. last-layer features) sharing the same optimal
actions collapse towards those optimal actions respective mean activations; 2)
the variability of activations sharing the same optimal actions converges to
zero; 3) the weights of action selection layer and the mean activations
collapse to a simplex equiangular tight frame (ETF). Our early work showed
those aforementioned constraints to be necessary for these observations. Since
the collapsed ETF of optimal policy DNNs maximally separates the pair-wise
angles of all actions in the state-action space, we naturally raise a question:
can we learn an optimal policy using an ETF structure as a (fixed) target
configuration in the action selection layer? Our analytical proof shows that
learning activations with a fixed ETF as action selection layer naturally leads
to the AC. We thus propose the Action Collapse Policy Gradient (ACPG) method,
which accordingly affixes a synthetic ETF as our action selection layer. ACPG
induces the policy DNN to produce such an ideal configuration in the action
selection layer while remaining optimal. Our experiments across various OpenAI
Gym environments demonstrate that our technique can be integrated into any
discrete PG methods and lead to favorable reward improvements more quickly and
robustly.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [25] [Synthetic Founders: AI-Generated Social Simulations for Startup Validation Research in Computational Social Science](https://arxiv.org/abs/2509.02605)
*Jorn K. Teutloff*

Main category: cs.MA

TL;DR: 这是一个比较性对接实验，将人类访谈数据与LLM驱动的合成人设进行对比，评估AI模拟的保真度、分异和盲点。


<details>
  <summary>Details</summary>
Motivation: 评估大语言模型驱动的合成人设在AI模拟中的保真度和局限性，探索其作为认知社会科学工具的潜力与边界。

Method: 采访15名初创公司创始人并与AI生成的创始人和投资者人设进行同样协议的对话实验，通过结构化主题综合分析数据。

Result: 发现四类结果：收敛主题、部分重叠、仅人类主题和仅合成人设主题，显示LLM人设更语言表达性强但缺乏生活经验。

Conclusion: LLM驱动人设构成了一种混合社会模拟形式，作为完善实证研究的补充工具，能扩展假设空间、加速探索性验证并明确认知现实主义的边界。

Abstract: We present a comparative docking experiment that aligns human-subject
interview data with large language model (LLM)-driven synthetic personas to
evaluate fidelity, divergence, and blind spots in AI-enabled simulation.
Fifteen early-stage startup founders were interviewed about their hopes and
concerns regarding AI-powered validation, and the same protocol was replicated
with AI-generated founder and investor personas. A structured thematic
synthesis revealed four categories of outcomes: (1) Convergent themes -
commitment-based demand signals, black-box trust barriers, and efficiency gains
were consistently emphasized across both datasets; (2) Partial overlaps -
founders worried about outliers being averaged away and the stress of real
customer validation, while synthetic personas highlighted irrational blind
spots and framed AI as a psychological buffer; (3) Human-only themes -
relational and advocacy value from early customer engagement and skepticism
toward moonshot markets; and (4) Synthetic-only themes - amplified false
positives and trauma blind spots, where AI may overstate adoption potential by
missing negative historical experiences.
  We interpret this comparative framework as evidence that LLM-driven personas
constitute a form of hybrid social simulation: more linguistically expressive
and adaptable than traditional rule-based agents, yet bounded by the absence of
lived history and relational consequence. Rather than replacing empirical
studies, we argue they function as a complementary simulation category -
capable of extending hypothesis space, accelerating exploratory validation, and
clarifying the boundaries of cognitive realism in computational social science.

</details>


### [26] [Automatic Differentiation of Agent-Based Models](https://arxiv.org/abs/2509.03303)
*Arnau Quera-Bofarull,Nicholas Bishop,Joel Dyer,Daniel Jarne Ornia,Anisoara Calinescu,Doyne Farmer,Michael Wooldridge*

Main category: cs.MA

TL;DR: 本文展示了自动微分技术如何有效减轻基于代理模型的计算负担，通过变分推理实现高效参数校准，显著提升了ABMs的实用性和可扩展性


<details>
  <summary>Details</summary>
Motivation: 基于代理模型在模拟复杂系统时面临计算量大、参数校准困难的问题，这限制了其广泛应用

Method: 将自动微分技术应用于基于代理模型，使模拟器的梯度易于获取，并采用变分推理技术进行参数校准

Result: 在三个著名ABMs上的实验显示，使用变分推理获得了显著的性能提升和计算节省

Conclusion: 该方法显著增强了基于代理模型在研究复杂系统时的实用性和可扩展性

Abstract: Agent-based models (ABMs) simulate complex systems by capturing the bottom-up
interactions of individual agents comprising the system. Many complex systems
of interest, such as epidemics or financial markets, involve thousands or even
millions of agents. Consequently, ABMs often become computationally demanding
and rely on the calibration of numerous free parameters, which has
significantly hindered their widespread adoption. In this paper, we demonstrate
that automatic differentiation (AD) techniques can effectively alleviate these
computational burdens. By applying AD to ABMs, the gradients of the simulator
become readily available, greatly facilitating essential tasks such as
calibration and sensitivity analysis. Specifically, we show how AD enables
variational inference (VI) techniques for efficient parameter calibration. Our
experiments demonstrate substantial performance improvements and computational
savings using VI on three prominent ABMs: Axtell's model of firms; Sugarscape;
and the SIR epidemiological model. Our approach thus significantly enhances the
practicality and scalability of ABMs for studying complex systems.

</details>


<div id='cs.MM'></div>

# cs.MM [[Back]](#toc)

### [27] [Simulacra Naturae: Generative Ecosystem driven by Agent-Based Simulations and Brain Organoid Collective Intelligence](https://arxiv.org/abs/2509.02924)
*Nefeli Manoudaki,Mert Toka,Iason Paterakis,Diarmid Flatley*

Main category: cs.MM

TL;DR: Simulacra Naturae是一个数据驱动的媒体装置，通过生物计算、材料生态和生成系统的融合探索集体关怀，将脑类器官的神经活动转化为多感官环境。


<details>
  <summary>Details</summary>
Motivation: 探索通过非人类认知塑造的感官场域，重新构想可视化作为关怀实践，在混合计算系统中开辟伦理、共情和生态协调的新空间。

Method: 使用预记录的脑类器官神经活动，通过实时系统转化为生成视觉、空间音频、活体植物和制造粘土文物的多感官环境，采用受自然系统启发的涌现代理行为。

Result: 创建了一个由非人类认知塑造的感官环境，将抽象数据锚定在活体材料和具身体验中，实现了去中心化的人类代理。

Conclusion: 该作品通过生物信号作为共创力量而非直接控制输入，为混合计算系统中的伦理和生态协调开辟了新途径，重新定义了可视化作为关怀实践的意义。

Abstract: Simulacra Naturae is a data-driven media installation that explores
collective care through the entanglement of biological computation, material
ecologies, and generative systems. The work translates pre-recorded neural
activity from brain organoids, lab-grown three-dimensional clusters of neurons,
into a multi-sensory environment composed of generative visuals, spatial audio,
living plants, and fabricated clay artifacts. These biosignals, streamed
through a real-time system, modulate emergent agent behaviors inspired by
natural systems such as termite colonies and slime molds. Rather than using
biosignals as direct control inputs, Simulacra Naturae treats organoid activity
as a co-creative force, allowing neural rhythms to guide the growth, form, and
atmosphere of a generative ecosystem. The installation features computationally
fabricated clay prints embedded with solenoids, adding physical sound
resonances to the generative surround composition. The spatial environment,
filled with live tropical plants and a floor-level projection layer featuring
real-time generative AI visuals, invites participants into a sensory field
shaped by nonhuman cognition. By grounding abstract data in living materials
and embodied experience, Simulacra Naturae reimagines visualization as a
practice of care, one that decentralizes human agency and opens new spaces for
ethics, empathy, and ecological attunement within hybrid computational systems.

</details>


### [28] [Automatically Generating High-Precision Simulated Road Networking in Traffic Scenario](https://arxiv.org/abs/2509.02990)
*Liang Xie,Wenke Huang*

Main category: cs.MM

TL;DR: 基于深度学习和开源地图数据的全自动高精度车道级模拟路网生成方法


<details>
  <summary>Details</summary>
Motivation: 现有车道级模拟路网生成方法需要大规模数据收集和手工后期编辑，芯片强度高、成本较大

Method: 收集开源街景数据构建车道线数据集，设计端到端深度学习车道线检测模型，结合坐标变换和地图匹配算法融合车道信息与路网拓扑结构

Result: 显著降低了数据收集和手工编辑成本，提高了模拟路网生成的效率和准确性

Conclusion: 为城市交通模拟、自主驾驶导航和智能交通系统提供了可靠数据支持，为大规模城市路网自动建模提供了新技术路径

Abstract: Existing lane-level simulation road network generation is labor-intensive,
resource-demanding, and costly due to the need for large-scale data collection
and manual post-editing. To overcome these limitations, we propose
automatically generating high-precision simulated road networks in traffic
scenario, an efficient and fully automated solution. Initially, real-world road
street view data is collected through open-source street view map platforms,
and a large-scale street view lane line dataset is constructed to provide a
robust foundation for subsequent analysis. Next, an end-to-end lane line
detection approach based on deep learning is designed, where a neural network
model is trained to accurately detect the number and spatial distribution of
lane lines in street view images, enabling automated extraction of lane
information. Subsequently, by integrating coordinate transformation and map
matching algorithms, the extracted lane information from street views is fused
with the foundational road topology obtained from open-source map service
platforms, resulting in the generation of a high-precision lane-level
simulation road network. This method significantly reduces the costs associated
with data collection and manual editing while enhancing the efficiency and
accuracy of simulation road network generation. It provides reliable data
support for urban traffic simulation, autonomous driving navigation, and the
development of intelligent transportation systems, offering a novel technical
pathway for the automated modeling of large-scale urban road networks.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [29] [Acrobotics: A Generalist Approahc To Quadrupedal Robots' Parkour](https://arxiv.org/abs/2509.02727)
*Guillaume Gagné-Labelle,Vassil Atanassov,Ioannis Havoutis*

Main category: cs.RO

TL;DR: 提出了一种通用的强化学习算法用于四足机器人的动态运动控制，该算法在训练时仅需25%的智能体数量就能达到专家混合方法的性能水平。


<details>
  <summary>Details</summary>
Motivation: 四足机器人相比轮式机器人在复杂地形导航方面具有优势，但传统建模方法难以处理滑倒和绊倒等复杂情况，而强化学习通过试错学习能够实现最优控制。

Method: 开发了一种通用的强化学习算法，用于训练四足机器人在动态运动场景中的控制策略，避免了复杂的专家混合方法。

Result: 学习到的策略在性能上可与最先进的专家混合方法相媲美，同时训练所需的智能体数量减少了75%。

Conclusion: 该研究展示了通用强化学习算法在四足机器人运动控制中的有效性，并识别了关键成功因素和策略组件。

Abstract: Climbing, crouching, bridging gaps, and walking up stairs are just a few of
the advantages that quadruped robots have over wheeled robots, making them more
suitable for navigating rough and unstructured terrain. However, executing such
manoeuvres requires precise temporal coordination and complex agent-environment
interactions. Moreover, legged locomotion is inherently more prone to slippage
and tripping, and the classical approach of modeling such cases to design a
robust controller thus quickly becomes impractical. In contrast, reinforcement
learning offers a compelling solution by enabling optimal control through trial
and error. We present a generalist reinforcement learning algorithm for
quadrupedal agents in dynamic motion scenarios. The learned policy rivals
state-of-the-art specialist policies trained using a mixture of experts
approach, while using only 25% as many agents during training. Our experiments
also highlight the key components of the generalist locomotion policy and the
primary factors contributing to its success.

</details>


### [30] [The Impact of Adaptive Emotional Alignment on Mental State Attribution and User Empathy in HRI](https://arxiv.org/abs/2509.02749)
*Giorgia Buracchio,Ariele Callegari,Massimo Donini,Cristina Gena,Antonio Lieto,Alberto Lillo,Claudio Mattutino,Alessandro Mazzei,Linda Pigureddu,Manuel Striani,Fabiana Vernero*

Main category: cs.RO

TL;DR: 情感对齐在HRI中对机器人说服力和用户沟通风格无显著影响，但显著提升用户对机器人心理状态归因和共情感知


<details>
  <summary>Details</summary>
Motivation: 研究自适应情感对齐作为共情沟通前提在人类-机器人交互中的作用，探索情感对齐对话对机器人说服力、用户沟通风格和心理状态归因的影响

Method: 使用NAO机器人进行实验，42名参与者分为两组：中性沟通组和情感自适应响应组，比较两种条件下的效果差异

Result: 情感对齐不影响用户沟通风格和机器人说服效果，但显著影响用户对机器人心理状态的归因和感知到的共情能力

Conclusion: 情感对齐虽然不能提升说服效果或改变沟通风格，但能有效增强用户对机器人心理能力和共情能力的感知

Abstract: The paper presents an experiment on the effects of adaptive emotional
alignment between agents, considered a prerequisite for empathic communication,
in Human-Robot Interaction (HRI). Using the NAO robot, we investigate the
impact of an emotionally aligned, empathic, dialogue on these aspects: (i) the
robot's persuasive effectiveness, (ii) the user's communication style, and
(iii) the attribution of mental states and empathy to the robot. In an
experiment with 42 participants, two conditions were compared: one with neutral
communication and another where the robot provided responses adapted to the
emotions expressed by the users. The results show that emotional alignment does
not influence users' communication styles or have a persuasive effect. However,
it significantly influences attribution of mental states to the robot and its
perceived empathy

</details>


### [31] [A Digital Twin for Robotic Post Mortem Tissue Sampling using Virtual Reality](https://arxiv.org/abs/2509.02760)
*Maximilian Neidhardt,Ludwig Bosse,Vidas Raudonis,Kristina Allgoewer,Axel Heinemann,Benjamin Ondruschka,Alexander Schlaefer*

Main category: cs.RO

TL;DR: 这篇论文探索了使用虚拟现实和数字双生技术来实现全程远程规划和控制汽车人化某事后少侵入性检查的新方法，以降低医生感染风险并提高效率。


<details>
  <summary>Details</summary>
Motivation: 传统的开放式某事后检查对医生构成感染风险，而手动超声引导检查方法存在限制。需要一种更安全、效率更高的新技术来进行某事后少侵入性检查。

Method: 研究人员开发了一种虚拟现实设备，统了数字双生技术，实现全程远程规划和控制汽车人化某事后检查。进行了三种交互方法的可用性研究，并在三体人类尸体上评估临床可行性。

Result: 进行了132次针插入操作，针头偏移误差为5.30±3.25 mm。成功获取了组织样本并经历史病理学验证。用户反馈该系统的针头放置方法非常直观。

Conclusion: 该系统是一种有前景的、精确的低风险替代方案，可以作为传统某事后检查方法的替代选择。

Abstract: Studying tissue samples obtained during autopsies is the gold standard when
diagnosing the cause of death and for understanding disease pathophysiology.
Recently, the interest in post mortem minimally invasive biopsies has grown
which is a less destructive approach in comparison to an open autopsy and
reduces the risk of infection. While manual biopsies under ultrasound guidance
are more widely performed, robotic post mortem biopsies have been recently
proposed. This approach can further reduce the risk of infection for
physicians. However, planning of the procedure and control of the robot need to
be efficient and usable. We explore a virtual reality setup with a digital twin
to realize fully remote planning and control of robotic post mortem biopsies.
The setup is evaluated with forensic pathologists in a usability study for
three interaction methods. Furthermore, we evaluate clinical feasibility and
evaluate the system with three human cadavers. Overall, 132 needle insertions
were performed with an off-axis needle placement error of 5.30+-3.25 mm. Tissue
samples were successfully biopsied and histopathologically verified. Users
reported a very intuitive needle placement approach, indicating that the system
is a promising, precise, and low-risk alternative to conventional approaches.

</details>


### [32] [Improving the Resilience of Quadrotors in Underground Environments by Combining Learning-based and Safety Controllers](https://arxiv.org/abs/2509.02808)
*Isaac Ronald Ward,Mark Paral,Kristopher Riordan,Mykel J. Kochenderfer*

Main category: cs.RO

TL;DR: 提出了一种基于归一化流的运行时监控方法，用于在四旋翼无人机导航中切换学习控制器和安全控制器，以同时保证任务效率和安全性。


<details>
  <summary>Details</summary>
Motivation: 学习型控制器在大规模地下环境自主控制中具有应用潜力，但存在泛化性问题，在训练未见的环境分布外区域表现不佳，需要保证安全性。

Method: 训练基于归一化流的环境先验分布，作为运行时监控指标来评估无人机是否处于分布外状态，并据此在学习控制器和安全控制器之间切换。

Result: 在基于真实点云数据的模拟3D洞穴环境中进行点对点导航测试，实验结果表明组合控制器同时具备学习控制器的快速任务完成能力和安全控制器的避碰安全性。

Conclusion: 该方法有效解决了学习控制器在分布外环境中的安全性问题，为地下环境自主导航提供了可靠的解决方案。

Abstract: Autonomously controlling quadrotors in large-scale subterranean environments
is applicable to many areas such as environmental surveying, mining operations,
and search and rescue. Learning-based controllers represent an appealing
approach to autonomy, but are known to not generalize well to
`out-of-distribution' environments not encountered during training. In this
work, we train a normalizing flow-based prior over the environment, which
provides a measure of how far out-of-distribution the quadrotor is at any given
time. We use this measure as a runtime monitor, allowing us to switch between a
learning-based controller and a safe controller when we are sufficiently
out-of-distribution. Our methods are benchmarked on a point-to-point navigation
task in a simulated 3D cave environment based on real-world point cloud data
from the DARPA Subterranean Challenge Final Event Dataset. Our experimental
results show that our combined controller simultaneously possesses the liveness
of the learning-based controller (completing the task quickly) and the safety
of the safety controller (avoiding collision).

</details>


### [33] [Multi-Embodiment Locomotion at Scale with extreme Embodiment Randomization](https://arxiv.org/abs/2509.02815)
*Nico Bohlinger,Jan Peters*

Main category: cs.RO

TL;DR: 提出了一个通用的运动控制策略，通过在50种不同腿式机器人上进行训练，结合改进的URMAv2架构和性能导向课程学习，实现了对百万级形态变化的控制，并在真实人形和四足机器人上实现零样本迁移。


<details>
  <summary>Details</summary>
Motivation: 解决传统运动控制策略对特定机器人形态的依赖性，开发能够适应多种不同形态腿式机器人的通用控制方法。

Method: 使用改进的URMAv2（体现感知架构）结合性能导向的极端体现随机化课程学习，在50种腿式机器人数据集上进行训练。

Result: 策略学会了控制数百万种形态变化，并在未见过的真实人形和四足机器人上实现了零样本迁移性能。

Conclusion: 该方法证明了通过大规模体现随机化和适当的架构设计，可以开发出具有强大泛化能力的通用运动控制策略。

Abstract: We present a single, general locomotion policy trained on a diverse
collection of 50 legged robots. By combining an improved embodiment-aware
architecture (URMAv2) with a performance-based curriculum for extreme
Embodiment Randomization, our policy learns to control millions of
morphological variations. Our policy achieves zero-shot transfer to unseen
real-world humanoid and quadruped robots.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [34] [Analysis of Speaker Verification Performance Trade-offs with Neural Audio Codec Transmission](https://arxiv.org/abs/2509.02771)
*Nirmalya Mallick Thakur,Jia Qi Yip,Eng Siong Chng*

Main category: cs.SD

TL;DR: 神经音频编解码器(NACs)在低比特率(<12kbps)下比传统编解码器Opus性能好6-8%，但在高比特率(≈24kbps)下略差，仅导致EER增加0.4-0.7%。NACs主要优化感知质量，可能丢弃说话人区分特征，但仍是带宽受限情况下的可行替代方案。


<details>
  <summary>Details</summary>
Motivation: 研究神经音频编解码器(NACs)对说话人验证(SV)性能的影响，因为NACs虽然近年来取得显著进展并被广泛采用，但可能引入音频失真从而降低SV性能。

Method: 在VoxCeleb1数据集上评估三种最先进的SV模型，比较传统和神经音频编解码器在不同比特率下的性能表现。

Result: 随着比特率降低，所有模型和编解码器的SV性能都一致下降。NACs在低比特率(<12kbps)下比Opus性能好6-8%，在高比特率(≈24kbps)下略差，EER仅增加0.4-0.7%。

Conclusion: NACs是传统编解码器的可行替代方案，特别是在带宽受限情况下。未来工作需要开发说话人感知的NACs或重新训练适应SV模型来弥合高比特率下的差距。

Abstract: Neural audio codecs (NACs) have made significant advancements in recent years
and are rapidly being adopted in many audio processing pipelines. However, they
can introduce audio distortions which degrade speaker verification (SV)
performance. This study investigates the impact of both traditional and neural
audio codecs at varying bitrates on three state of-the-art SV models evaluated
on the VoxCeleb1 dataset. Our findings reveal a consistent degradation in SV
performance across all models and codecs as bitrates decrease. Notably, NACs do
not fundamentally break SV performance when compared to traditional codecs.
They outperform Opus by 6-8% at low-bitrates (< 12 kbps) and remain marginally
behind at higher bitrates ($\approx$ 24 kbps), with an EER increase of only
0.4-0.7%. The disparity at higher bitrates is likely due to the primary
optimization of NACs for perceptual quality, which can inadvertently discard
critical speaker-discriminative features, unlike Opus which was designed to
preserve vocal characteristics. Our investigation suggests that NACs are a
feasible alternative to traditional codecs, especially under bandwidth
limitations. To bridge the gap at higher bitrates, future work should focus on
developing speaker-aware NACs or retraining and adapting SV models.

</details>


### [35] [Speech DF Arena: A Leaderboard for Speech DeepFake Detection Models](https://arxiv.org/abs/2509.02859)
*Sandipana Dowerah,Atharva Kulkarni,Ajinkya Kulkarni,Hoan My Tran,Joonas Kalda,Artem Fedorchenko,Benoit Fauve,Damien Lolive,Tanel Alumäe,Matthew Magimai Doss*

Main category: cs.SD

TL;DR: Speech DF Arena是首个全面的音频深度伪造检测基准，提供标准化评估工具包、14个数据集、标准化指标和协议，以及系统排名榜单，旨在提高检测系统的可靠性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 随着深度伪造音频生成技术的发展，音频深度伪造检测也取得了显著进展，但缺乏标准化和全面的基准测试。

Method: 引入Speech DF Arena基准，提供统一评估工具包，涵盖14个不同数据集和攻击场景，采用标准化评估指标和协议，包含12个开源和3个专有检测系统。

Result: 研究发现许多系统在跨域场景中表现出高EER（等错误率），凸显了广泛跨域评估的必要性。

Conclusion: Speech DF Arena为音频深度伪造检测提供了首个全面基准，有助于研究人员和开发者提升检测系统的可靠性和鲁棒性，工具包已在Huggingface和GitHub上提供。

Abstract: Parallel to the development of advanced deepfake audio generation, audio
deepfake detection has also seen significant progress. However, a standardized
and comprehensive benchmark is still missing. To address this, we introduce
Speech DeepFake (DF) Arena, the first comprehensive benchmark for audio
deepfake detection. Speech DF Arena provides a toolkit to uniformly evaluate
detection systems, currently across 14 diverse datasets and attack scenarios,
standardized evaluation metrics and protocols for reproducibility and
transparency. It also includes a leaderboard to compare and rank the systems to
help researchers and developers enhance their reliability and robustness. We
include 14 evaluation sets, 12 state-of-the-art open-source and 3 proprietary
detection systems. Our study presents many systems exhibiting high EER in
out-of-domain scenarios, highlighting the need for extensive cross-domain
evaluation. The leaderboard is hosted on Huggingface1 and a toolkit for
reproducing results across the listed datasets is available on GitHub.

</details>


### [36] [Multi-level SSL Feature Gating for Audio Deepfake Detection](https://arxiv.org/abs/2509.03409)
*Hoan My Tran,Damien Lolive,Aghilas Sini,Arnaud Delhay,Pierre-François Marteau,David Guennec*

Main category: cs.SD

TL;DR: 通过结合XLS-R基础模型的门控机制和多内核卷积分析，提出了一种能够检测多语言深度伪造语音的新方法，在域内咈域外都取得了最佳性能。


<details>
  <summary>Details</summary>
Motivation: 随着语音合成技术的发展，高质量的合成语音可能被欺诈活动、身份盗用等恶意使用，当前的检测方法在应对未见攻击咈多语言场景时存在不足。

Method: 使用XLS-R基础模型作为前端特征提取器，通过门控机制提取关键特征；采用多内核门控卷积（MultiConv）作为后端分类器，捕捉局部和全局语音伪造特征；引入中心内核对齐（CKA）作为相似性指标，确保不同层学到多样化的特征。

Result: 该方法在域内测试集上达到了最佳性能，同时在域外数据集（包括多语言语音样本）上也表现出了强大的演化能力。

Conclusion: 该研究提供了一种能够有效检测不断发展的语音深度伪造威胁的多用途解决方案，具有良好的通用性和演化性。

Abstract: Recent advancements in generative AI, particularly in speech synthesis, have
enabled the generation of highly natural-sounding synthetic speech that closely
mimics human voices. While these innovations hold promise for applications like
assistive technologies, they also pose significant risks, including misuse for
fraudulent activities, identity theft, and security threats. Current research
on spoofing detection countermeasures remains limited by generalization to
unseen deepfake attacks and languages. To address this, we propose a gating
mechanism extracting relevant feature from the speech foundation XLS-R model as
a front-end feature extractor. For downstream back-end classifier, we employ
Multi-kernel gated Convolution (MultiConv) to capture both local and global
speech artifacts. Additionally, we introduce Centered Kernel Alignment (CKA) as
a similarity metric to enforce diversity in learned features across different
MultiConv layers. By integrating CKA with our gating mechanism, we hypothesize
that each component helps improving the learning of distinct synthetic speech
patterns. Experimental results demonstrate that our approach achieves
state-of-the-art performance on in-domain benchmarks while generalizing
robustly to out-of-domain datasets, including multilingual speech samples. This
underscores its potential as a versatile solution for detecting evolving speech
deepfake threats.

</details>


<div id='eess.AS'></div>

# eess.AS [[Back]](#toc)

### [37] [Gaussian Process Regression of Steering Vectors With Physics-Aware Deep Composite Kernels for Augmented Listening](https://arxiv.org/abs/2509.02571)
*Diego Di Carlo,Koyama Shoichi,Nugraha Aditya Arie,Fontaine Mathieu,Bando Yoshiaki,Yoshii Kazuyoshi*

Main category: eess.AS

TL;DR: 本文提出了一种基于神经场和高斯过程的物理感知复合核方法，用于连续表示声场导向向量，解决了传统确定性超分辨率方法在测量空间非均匀不确定性下的过拟合问题。


<details>
  <summary>Details</summary>
Motivation: 传统导向向量表示方法无法处理声场的散射效应，而现有的物理感知深度学习方法虽然有效，但在测量空间非均匀不确定性下容易过拟合，需要更好的概率框架来解决这个问题。

Method: 将神经场（NF）的表达性表示集成到基于高斯过程（GP）的概率框架中，提出物理感知复合核来建模方向性入射波和后续散射效应。

Result: 在数据不足条件下，该方法表现出有效性。在下游任务如语音增强和双耳渲染中，使用SPEAR挑战赛的模拟数据，仅需不到十分之一的测量次数就能达到oracle性能。

Conclusion: 提出的NF-GP复合方法能够有效解决声场导向向量连续表示中的过拟合问题，在数据稀缺条件下仍能保持优异性能，为增强听觉应用提供了精确的声场控制能力。

Abstract: This paper investigates continuous representations of steering vectors over
frequency and position of microphone and source for augmented listening (e.g.,
spatial filtering and binaural rendering) with precise control of the sound
field perceived by the user. Steering vectors have typically been used for
representing the spatial characteristics of the sound field as a function of
the listening position. The basic algebraic representation of steering vectors
assuming an idealized environment cannot deal with the scattering effect of the
sound field. One may thus collect a discrete set of real steering vectors
measured in dedicated facilities and super-resolve (i.e., upsample) them.
Recently, physics-aware deep learning methods have been effectively used for
this purpose. Such deterministic super-resolution, however, suffers from the
overfitting problem due to the non-uniform uncertainty over the measurement
space. To solve this problem, we integrate an expressive representation based
on the neural field (NF) into the principled probabilistic framework based on
the Gaussian process (GP). Specifically, we propose a physics-aware composite
kernel that model the directional incoming waves and the subsequent scattering
effect. Our comprehensive comparative experiment showed the effectiveness of
the proposed method under data insufficiency conditions. In downstream tasks
such as speech enhancement and binaural rendering using the simulated data of
the SPEAR challenge, the oracle performances were attained with less than ten
times fewer measurements.

</details>


### [38] [IS${}^3$ : Generic Impulsive--Stationary Sound Separation in Acoustic Scenes using Deep Filtering](https://arxiv.org/abs/2509.02622)
*Berger Clémentine,Stamadiatis Paraskevas,Badeau Roland,Essid Slim*

Main category: eess.AS

TL;DR: IS³神经网络用于分离音频中的脉冲声和稳态背景声，在多种音频处理任务中表现优于传统方法


<details>
  <summary>Details</summary>
Motivation: 开发能够区分处理稳态背景和孤立声学事件的音频系统，用于自适应音频渲染、噪声抑制、声学事件分类等实际应用场景

Method: 使用深度滤波方法的神经网络IS³，配合精心设计的数据生成流程来训练模型进行脉冲-稳态声音分离

Result: 基于相对轻量级神经网络架构的学习方法在该任务中取得成功，在客观分离指标上优于谐波-打击乐分离掩蔽方法和小波滤波

Conclusion: 学习型方法结合良好设计的数据集能够有效解决此前未解决的脉冲-稳态声音分离问题，为各种音频处理应用提供有效的预处理阶段

Abstract: We are interested in audio systems capable of performing a differentiated
processing of stationary backgrounds and isolated acoustic events within an
acoustic scene, whether for applying specific processing methods to each part
or for focusing solely on one while ignoring the other. Such systems have
applications in real-world scenarios, including robust adaptive audio rendering
systems (e.g., EQ or compression), plosive attenuation in voice mixing, noise
suppression or reduction, robust acoustic event classification or even
bioacoustics. To this end, we introduce IS${}^3$, a neural network designed for
Impulsive--Stationary Sound Separation, that isolates impulsive acoustic events
from the stationary background using a deep filtering approach, that can act as
a pre-processing stage for the above-mentioned tasks. To ensure optimal
training, we propose a sophisticated data generation pipeline that curates and
adapts existing datasets for this task. We demonstrate that a learning-based
approach, build on a relatively lightweight neural architecture and trained
with well-designed and varied data, is successful in this previously
unaddressed task, outperforming the Harmonic--Percussive Sound Separation
masking method, adapted from music signal processing research, and wavelet
filtering on objective separation metrics.

</details>


<div id='econ.GN'></div>

# econ.GN [[Back]](#toc)

### [39] [Too Noisy to Collude? Algorithmic Collusion Under Laplacian Noise](https://arxiv.org/abs/2509.02800)
*Niuniu Zhang*

Main category: econ.GN

TL;DR: 通过控制信息质量和注入噪声来阻止算法合伙，使其在合理定价范围内稳定


<details>
  <summary>Details</summary>
Motivation: 自主定价系统的兴起引发对算法合伙的担忧，需要事前放置政策来防范超过竞争水平的价格

Method: 通过减少定价算法所依赖数据的保真度，并在池化市场数据中注入精准检定的噪声

Result: 信息质量是竞争结果的核心驱动因素，通过噪声注入可以减缓或破坏合伙，形成一个可行干预区域

Conclusion: 信息控制是一种轻量级但实用的政策手段，可以从源头冲击数字合伙

Abstract: The rise of autonomous pricing systems has sparked growing concern over
algorithmic collusion in markets from retail to housing. This paper examines
controlled information quality as an ex ante policy lever: by reducing the
fidelity of data that pricing algorithms draw on, regulators can frustrate
collusion before supracompetitive prices emerge. We show, first, that
information quality is the central driver of competitive outcomes, shaping
prices, profits, and consumer welfare. Second, we demonstrate that collusion
can be slowed or destabilized by injecting carefully calibrated noise into
pooled market data, yielding a feasibility region where intervention disrupts
cartels without undermining legitimate pricing. Together, these results
highlight information control as a lightweight yet practical lever to blunt
digital collusion at its source.

</details>


<div id='eess.SY'></div>

# eess.SY [[Back]](#toc)

### [40] [Approximate constrained stochastic optimal control via parameterized input inference](https://arxiv.org/abs/2509.02922)
*Shahbaz P Qadri Syed,He Bai*

Main category: eess.SY

TL;DR: 提出基于期望最大化(EM)的推理方法解决带约束的随机最优控制问题，使用障碍函数处理状态和控制约束，在多种机器人控制场景中验证有效性


<details>
  <summary>Details</summary>
Motivation: 近似方法解决随机最优控制问题在过去十年受到广泛关注，但现有概率推理方法主要针对非线性二次高斯问题，需要开发能够处理状态和控制约束的通用方法

Method: 基于EM框架的推理过程，使用障碍函数处理状态和控制的不等式约束，E步平滑状态-控制对，M步在控制参数的非零子集上推断结构化随机最优控制器

Result: 在单机器人避障、多机器人编队控制和四旋翼风力环境导航等示例中验证了算法有效性，进行了障碍函数参数对状态约束满足的实证研究和平滑算法的比较研究

Conclusion: 提出的EM-based推理方法能够有效解决带约束的随机最优控制问题，障碍函数方法成功处理了状态和控制约束，在多种实际控制场景中表现出良好性能

Abstract: Approximate methods to solve stochastic optimal control (SOC) problems have
received significant interest from researchers in the past decade.
Probabilistic inference approaches to SOC have been developed to solve
nonlinear quadratic Gaussian problems. In this work, we propose an
Expectation-Maximization (EM) based inference procedure to generate
state-feedback controls for constrained SOC problems. We consider the
inequality constraints for the state and controls and also the structural
constraints for the controls. We employ barrier functions to address state and
control constraints. We show that the expectation step leads to smoothing of
the state-control pair while the the maximization step on the non-zero subsets
of the control parameters allows inference of structured stochastic optimal
controllers. We demonstrate the effectiveness of the algorithm on unicycle
obstacle avoidance, four-unicycle formation control, and quadcopter navigation
in windy environment examples. In these examples, we perform an empirical study
on the parametric effect of barrier functions on the state constraint
satisfaction. We also present a comparative study of smoothing algorithms on
the performance of the proposed approach.

</details>
