{"id": "2507.06373", "pdf": "https://arxiv.org/pdf/2507.06373", "abs": "https://arxiv.org/abs/2507.06373", "authors": ["Jeremy Fischer", "Ram Krishnamoorthy", "Vishal Kumar", "Mahdi Al-Husseini"], "title": "Digital Wargames to Enhance Military Medical Evacuation Decision-Making", "categories": ["cs.AI", "cs.CY", "cs.HC", "cs.MM"], "comment": null, "summary": "Medical evacuation is one of the United States Army's most storied and\ncritical mission sets, responsible for efficiently and expediently evacuating\nthe battlefield ill and injured. Medical evacuation planning involves designing\na robust network of medical platforms and facilities capable of moving and\ntreating large numbers of casualties. Until now, there has not been a medium to\nsimulate these networks in a classroom setting and evaluate both offline\nplanning and online decision-making performance. This work describes the\nMedical Evacuation Wargaming Initiative (MEWI), a three-dimensional multiplayer\nsimulation developed in Unity that replicates battlefield constraints and\nuncertainties. MEWI accurately models patient interactions at casualty\ncollection points, ambulance exchange points, medical treatment facilities, and\nevacuation platforms. Two operational scenarios are introduced: an amphibious\nisland assault in the Pacific and a Eurasian conflict across a sprawling road\nand river network. These scenarios pit students against the clock to save as\nmany casualties as possible while adhering to doctrinal lessons learned during\ndidactic training. We visualize performance data collected from two iterations\nof the MEWI Pacific scenario executed in the United States Army's Medical\nEvacuation Doctrine Course. We consider post-wargame Likert survey data from\nstudent participants and external observer notes to identify key planning\ndecision points, document medical evacuation lessons learned, and quantify\ngeneral utility. Results indicate that MEWI participation substantially\nimproves uptake of medical evacuation lessons learned and co-operative\ndecision-making. MEWI is a substantial step forward in the field of\nhigh-fidelity training tools for medical education, and our study findings\noffer critical insights into improving medical evacuation education and\noperations across the joint force.", "AI": {"tldr": "MEWI\u662f\u4e00\u79cd\u4e09\u7ef4\u591a\u4eba\u6a21\u62df\u5de5\u5177\uff0c\u7528\u4e8e\u6a21\u62df\u6218\u573a\u533b\u7597\u64a4\u79bb\u7f51\u7edc\uff0c\u63d0\u5347\u6559\u5b66\u6548\u679c\u548c\u51b3\u7b56\u80fd\u529b\u3002", "motivation": "\u7f3a\u4e4f\u5728\u8bfe\u5802\u73af\u5883\u4e2d\u6a21\u62df\u533b\u7597\u64a4\u79bb\u7f51\u7edc\u5e76\u8bc4\u4f30\u89c4\u5212\u548c\u51b3\u7b56\u7684\u5de5\u5177\u3002", "method": "\u5f00\u53d1\u57fa\u4e8eUnity\u7684MEWI\u6a21\u62df\u5668\uff0c\u6a21\u62df\u6218\u573a\u7ea6\u675f\u548c\u4e0d\u786e\u5b9a\u6027\uff0c\u5e76\u5728\u4e24\u4e2a\u573a\u666f\u4e2d\u6d4b\u8bd5\u3002", "result": "MEWI\u663e\u8457\u63d0\u5347\u4e86\u5b66\u5458\u5bf9\u533b\u7597\u64a4\u79bb\u77e5\u8bc6\u7684\u638c\u63e1\u548c\u534f\u4f5c\u51b3\u7b56\u80fd\u529b\u3002", "conclusion": "MEWI\u662f\u9ad8\u4fdd\u771f\u533b\u7597\u6559\u80b2\u5de5\u5177\u7684\u91cd\u8981\u8fdb\u6b65\uff0c\u5bf9\u6539\u8fdb\u533b\u7597\u64a4\u79bb\u6559\u80b2\u548c\u64cd\u4f5c\u5177\u6709\u5173\u952e\u610f\u4e49\u3002"}}
{"id": "2507.06396", "pdf": "https://arxiv.org/pdf/2507.06396", "abs": "https://arxiv.org/abs/2507.06396", "authors": ["Mandana Vaziri", "Louis Mandel", "Yuji Watanabe", "Hirokuni Kitahara", "Martin Hirzel", "Anca Sailer"], "title": "Representing Prompting Patterns with PDL: Compliance Agent Case Study", "categories": ["cs.AI", "cs.LG", "cs.PL", "cs.SE"], "comment": "ICML 2025 Workshop on Programmatic Representations for Agent Learning", "summary": "Prompt engineering for LLMs remains complex, with existing frameworks either\nhiding complexity behind restrictive APIs or providing inflexible canned\npatterns that resist customization -- making sophisticated agentic programming\nchallenging. We present the Prompt Declaration Language (PDL), a novel approach\nto prompt representation that tackles this fundamental complexity by bringing\nprompts to the forefront, enabling manual and automatic prompt tuning while\ncapturing the composition of LLM calls together with rule-based code and\nexternal tools. By abstracting away the plumbing for such compositions, PDL\naims at improving programmer productivity while providing a declarative\nrepresentation that is amenable to optimization. This paper demonstrates PDL's\nutility through a real-world case study of a compliance agent. Tuning the\nprompting pattern of this agent yielded up to 4x performance improvement\ncompared to using a canned agent and prompt pattern.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86Prompt Declaration Language (PDL)\uff0c\u4e00\u79cd\u65b0\u578b\u63d0\u793a\u8868\u793a\u65b9\u6cd5\uff0c\u65e8\u5728\u89e3\u51b3LLM\u63d0\u793a\u5de5\u7a0b\u7684\u590d\u6742\u6027\uff0c\u901a\u8fc7\u5c06\u63d0\u793a\u7f6e\u4e8e\u6838\u5fc3\u4f4d\u7f6e\uff0c\u652f\u6301\u624b\u52a8\u548c\u81ea\u52a8\u63d0\u793a\u8c03\u4f18\uff0c\u5e76\u6574\u5408\u89c4\u5219\u4ee3\u7801\u548c\u5916\u90e8\u5de5\u5177\u3002", "motivation": "\u73b0\u6709\u63d0\u793a\u5de5\u7a0b\u6846\u67b6\u8981\u4e48\u9690\u85cf\u590d\u6742\u6027\uff0c\u8981\u4e48\u63d0\u4f9b\u4e0d\u7075\u6d3b\u7684\u56fa\u5b9a\u6a21\u5f0f\uff0c\u9650\u5236\u4e86\u9ad8\u7ea7\u4ee3\u7406\u7f16\u7a0b\u7684\u53ef\u80fd\u6027\u3002", "method": "\u63d0\u51faPDL\uff0c\u4e00\u79cd\u58f0\u660e\u5f0f\u8bed\u8a00\uff0c\u62bd\u8c61\u5316\u7ec4\u5408\u903b\u8f91\uff0c\u652f\u6301\u63d0\u793a\u8c03\u4f18\u548c\u4f18\u5316\u3002", "result": "\u901a\u8fc7\u5408\u89c4\u4ee3\u7406\u6848\u4f8b\u7814\u7a76\uff0cPDL\u76f8\u6bd4\u56fa\u5b9a\u6a21\u5f0f\u5b9e\u73b0\u4e86\u9ad8\u8fbe4\u500d\u7684\u6027\u80fd\u63d0\u5347\u3002", "conclusion": "PDL\u901a\u8fc7\u58f0\u660e\u5f0f\u8868\u793a\u548c\u4f18\u5316\u80fd\u529b\uff0c\u663e\u8457\u63d0\u5347\u4e86\u7a0b\u5e8f\u5458\u7684\u751f\u4ea7\u529b\u548cLLM\u5e94\u7528\u7684\u6027\u80fd\u3002"}}
{"id": "2507.06398", "pdf": "https://arxiv.org/pdf/2507.06398", "abs": "https://arxiv.org/abs/2507.06398", "authors": ["David Orban"], "title": "Jolting Technologies: Superexponential Acceleration in AI Capabilities and Implications for AGI", "categories": ["cs.AI", "cs.CY", "68T01, 91B26, 93C15"], "comment": "13 pages, 2 figures. Revised following peer review", "summary": "This paper investigates the Jolting Technologies Hypothesis, which posits\nsuperexponential growth (increasing acceleration, or a positive third\nderivative) in the development of AI capabilities. We develop a theoretical\nframework and validate detection methodologies through Monte Carlo simulations,\nwhile acknowledging that empirical validation awaits suitable longitudinal\ndata. Our analysis focuses on creating robust tools for future empirical\nstudies and exploring the potential implications should the hypothesis prove\nvalid. The study examines how factors such as shrinking idea-to-action\nintervals and compounding iterative AI improvements drive this jolting pattern.\nBy formalizing jolt dynamics and validating detection methods through\nsimulation, this work provides the mathematical foundation necessary for\nunderstanding potential AI trajectories and their consequences for AGI\nemergence, offering insights for research and policy.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86Jolting Technologies\u5047\u8bf4\uff0c\u63d0\u51faAI\u80fd\u529b\u53d1\u5c55\u53ef\u80fd\u5448\u73b0\u8d85\u6307\u6570\u589e\u957f\uff08\u52a0\u901f\u589e\u957f\u6216\u6b63\u4e09\u9636\u5bfc\u6570\uff09\uff0c\u5e76\u901a\u8fc7\u8499\u7279\u5361\u6d1b\u6a21\u62df\u9a8c\u8bc1\u68c0\u6d4b\u65b9\u6cd5\uff0c\u4e3a\u672a\u6765\u5b9e\u8bc1\u7814\u7a76\u63d0\u4f9b\u5de5\u5177\u3002", "motivation": "\u63a2\u7d22AI\u80fd\u529b\u53d1\u5c55\u7684\u8d85\u6307\u6570\u589e\u957f\u6a21\u5f0f\u53ca\u5176\u6f5c\u5728\u5f71\u54cd\uff0c\u4e3aAGI\uff08\u901a\u7528\u4eba\u5de5\u667a\u80fd\uff09\u7684\u51fa\u73b0\u63d0\u4f9b\u7406\u8bba\u57fa\u7840\u548c\u653f\u7b56\u542f\u793a\u3002", "method": "\u5f00\u53d1\u7406\u8bba\u6846\u67b6\u5e76\u901a\u8fc7\u8499\u7279\u5361\u6d1b\u6a21\u62df\u9a8c\u8bc1\u68c0\u6d4b\u65b9\u6cd5\uff0c\u5206\u6790\u7f29\u77ed\u7684\u521b\u610f\u5230\u884c\u52a8\u5468\u671f\u548c\u8fed\u4ee3AI\u6539\u8fdb\u7684\u590d\u5408\u6548\u5e94\u3002", "result": "\u5efa\u7acb\u4e86\u63cf\u8ff0jolt\u52a8\u6001\u7684\u6570\u5b66\u6a21\u578b\uff0c\u5e76\u901a\u8fc7\u6a21\u62df\u9a8c\u8bc1\u4e86\u68c0\u6d4b\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u4e3a\u672a\u6765\u5b9e\u8bc1\u7814\u7a76\u5960\u5b9a\u57fa\u7840\u3002", "conclusion": "\u7814\u7a76\u4e3a\u7406\u89e3AI\u53d1\u5c55\u8f68\u8ff9\u53ca\u5176\u5bf9AGI\u7684\u6f5c\u5728\u5f71\u54cd\u63d0\u4f9b\u4e86\u6570\u5b66\u5de5\u5177\uff0c\u5bf9\u7814\u7a76\u548c\u653f\u7b56\u5236\u5b9a\u5177\u6709\u91cd\u8981\u542f\u793a\u3002"}}
{"id": "2507.06329", "pdf": "https://arxiv.org/pdf/2507.06329", "abs": "https://arxiv.org/abs/2507.06329", "authors": ["Michael Clemens", "Ana Marasovi\u0107"], "title": "MixAssist: An Audio-Language Dataset for Co-Creative AI Assistance in Music Mixing", "categories": ["cs.SD", "cs.AI", "eess.AS"], "comment": "Published at COLM 2025. Code and dataset are available here\n  http://mclemcrew.github.io/mixassist-website", "summary": "While AI presents significant potential for enhancing music mixing and\nmastering workflows, current research predominantly emphasizes end-to-end\nautomation or generation, often overlooking the collaborative and instructional\ndimensions vital for co-creative processes. This gap leaves artists,\nparticularly amateurs seeking to develop expertise, underserved. To bridge\nthis, we introduce MixAssist, a novel audio-language dataset capturing the\nsituated, multi-turn dialogue between expert and amateur music producers during\ncollaborative mixing sessions. Comprising 431 audio-grounded conversational\nturns derived from 7 in-depth sessions involving 12 producers, MixAssist\nprovides a unique resource for training and evaluating audio-language models\nthat can comprehend and respond to the complexities of real-world music\nproduction dialogues. Our evaluations, including automated LLM-as-a-judge\nassessments and human expert comparisons, demonstrate that fine-tuning models\nsuch as Qwen-Audio on MixAssist can yield promising results, with Qwen\nsignificantly outperforming other tested models in generating helpful,\ncontextually relevant mixing advice. By focusing on co-creative instruction\ngrounded in audio context, MixAssist enables the development of intelligent AI\nassistants designed to support and augment the creative process in music\nmixing.", "AI": {"tldr": "MixAssist\u662f\u4e00\u4e2a\u65b0\u7684\u97f3\u9891-\u8bed\u8a00\u6570\u636e\u96c6\uff0c\u65e8\u5728\u6355\u6349\u4e13\u5bb6\u4e0e\u4e1a\u4f59\u97f3\u4e50\u5236\u4f5c\u4eba\u5728\u534f\u4f5c\u6df7\u97f3\u4f1a\u8bdd\u4e2d\u7684\u591a\u8f6e\u5bf9\u8bdd\uff0c\u586b\u8865\u4e86\u73b0\u6709\u7814\u7a76\u5ffd\u89c6\u534f\u4f5c\u548c\u6559\u5b66\u7ef4\u5ea6\u7684\u7a7a\u767d\u3002", "motivation": "\u5f53\u524dAI\u7814\u7a76\u8fc7\u4e8e\u5173\u6ce8\u7aef\u5230\u7aef\u81ea\u52a8\u5316\u6216\u751f\u6210\uff0c\u5ffd\u89c6\u4e86\u97f3\u4e50\u5236\u4f5c\u4e2d\u7684\u534f\u4f5c\u548c\u6559\u5b66\u9700\u6c42\uff0c\u5c24\u5176\u662f\u5bf9\u4e1a\u4f59\u7231\u597d\u8005\u7684\u652f\u6301\u4e0d\u8db3\u3002", "method": "\u901a\u8fc77\u6b21\u6df1\u5165\u4f1a\u8bdd\u548c12\u540d\u5236\u4f5c\u4eba\u53c2\u4e0e\u7684431\u4e2a\u97f3\u9891\u5bf9\u8bdd\u56de\u5408\u6784\u5efaMixAssist\u6570\u636e\u96c6\uff0c\u5e76\u7528\u4e8e\u8bad\u7ec3\u548c\u8bc4\u4f30\u97f3\u9891-\u8bed\u8a00\u6a21\u578b\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u57fa\u4e8eMixAssist\u5fae\u8c03\u7684Qwen-Audio\u6a21\u578b\u5728\u751f\u6210\u6709\u5e2e\u52a9\u4e14\u4e0a\u4e0b\u6587\u76f8\u5173\u7684\u6df7\u97f3\u5efa\u8bae\u65b9\u9762\u663e\u8457\u4f18\u4e8e\u5176\u4ed6\u6a21\u578b\u3002", "conclusion": "MixAssist\u4e3a\u5f00\u53d1\u652f\u6301\u97f3\u4e50\u6df7\u97f3\u521b\u4f5c\u8fc7\u7a0b\u7684\u667a\u80fdAI\u52a9\u624b\u63d0\u4f9b\u4e86\u72ec\u7279\u8d44\u6e90\uff0c\u5f3a\u8c03\u4e86\u97f3\u9891\u4e0a\u4e0b\u6587\u4e2d\u7684\u534f\u4f5c\u6559\u5b66\u3002"}}
{"id": "2507.06798", "pdf": "https://arxiv.org/pdf/2507.06798", "abs": "https://arxiv.org/abs/2507.06798", "authors": ["Uri Andrews", "Luca San Mauro"], "title": "Comparing Dialectical Systems: Contradiction and Counterexample in Belief Change (Extended Version)", "categories": ["cs.AI", "math.LO"], "comment": "25 pages, accepted at JELIA 2025", "summary": "Dialectical systems are a mathematical formalism for modeling an agent\nupdating a knowledge base seeking consistency. Introduced in the 1970s by\nRoberto Magari, they were originally conceived to capture how a working\nmathematician or a research community refines beliefs in the pursuit of truth.\nDialectical systems also serve as natural models for the belief change of an\nautomated agent, offering a unifying, computable framework for dynamic belief\nmanagement.\n  The literature distinguishes three main models of dialectical systems:\n(d-)dialectical systems based on revising beliefs when they are seen to be\ninconsistent, p-dialectical systems based on revising beliefs based on finding\na counterexample, and q-dialectical systems which can do both. We answer an\nopen problem in the literature by proving that q-dialectical systems are\nstrictly more powerful than p-dialectical systems, which are themselves known\nto be strictly stronger than (d-)dialectical systems. This result highlights\nthe complementary roles of counterexample and contradiction in automated belief\nrevision, and thus also in the reasoning processes of mathematicians and\nresearch communities.", "AI": {"tldr": "\u8bba\u6587\u8bc1\u660e\u4e86q-\u8fa9\u8bc1\u7cfb\u7edf\u6bd4p-\u8fa9\u8bc1\u7cfb\u7edf\u66f4\u5f3a\u5927\uff0c\u540e\u8005\u53c8\u6bd4d-\u8fa9\u8bc1\u7cfb\u7edf\u66f4\u5f3a\uff0c\u5f3a\u8c03\u4e86\u53cd\u4f8b\u548c\u77db\u76fe\u5728\u81ea\u52a8\u4fe1\u5ff5\u4fee\u6b63\u4e2d\u7684\u4e92\u8865\u4f5c\u7528\u3002", "motivation": "\u7814\u7a76\u8fa9\u8bc1\u7cfb\u7edf\u7684\u4e0d\u540c\u6a21\u578b\uff0c\u4ee5\u89e3\u51b3\u6587\u732e\u4e2d\u5173\u4e8e\u5176\u76f8\u5bf9\u80fd\u529b\u7684\u5f00\u653e\u95ee\u9898\uff0c\u5e76\u63ed\u793a\u53cd\u4f8b\u548c\u77db\u76fe\u5728\u4fe1\u5ff5\u4fee\u6b63\u4e2d\u7684\u4f5c\u7528\u3002", "method": "\u901a\u8fc7\u6570\u5b66\u8bc1\u660e\u6bd4\u8f83q-\u3001p-\u548cd-\u8fa9\u8bc1\u7cfb\u7edf\u7684\u80fd\u529b\uff0c\u5c55\u793a\u5176\u5c42\u6b21\u5173\u7cfb\u3002", "result": "q-\u8fa9\u8bc1\u7cfb\u7edf\u4e25\u683c\u5f3a\u4e8ep-\u8fa9\u8bc1\u7cfb\u7edf\uff0c\u800cp-\u8fa9\u8bc1\u7cfb\u7edf\u53c8\u4e25\u683c\u5f3a\u4e8ed-\u8fa9\u8bc1\u7cfb\u7edf\u3002", "conclusion": "\u53cd\u4f8b\u548c\u77db\u76fe\u5728\u81ea\u52a8\u4fe1\u5ff5\u4fee\u6b63\u4e2d\u5177\u6709\u4e92\u8865\u4f5c\u7528\uff0c\u8fd9\u5bf9\u6570\u5b66\u5bb6\u548c\u7814\u7a76\u793e\u533a\u7684\u63a8\u7406\u8fc7\u7a0b\u6709\u91cd\u8981\u610f\u4e49\u3002"}}
{"id": "2507.06481", "pdf": "https://arxiv.org/pdf/2507.06481", "abs": "https://arxiv.org/abs/2507.06481", "authors": ["Changheon Han", "Yuseop Sim", "Hoin Jung", "Jiho Lee", "Hojun Lee", "Yun Seok Kang", "Sucheol Woo", "Garam Kim", "Hyung Wook Park", "Martin Byung-Guk Jun"], "title": "IMPACT: Industrial Machine Perception via Acoustic Cognitive Transformer", "categories": ["cs.SD", "eess.AS"], "comment": null, "summary": "Acoustic signals from industrial machines offer valuable insights for anomaly\ndetection, predictive maintenance, and operational efficiency enhancement.\nHowever, existing task-specific, supervised learning methods often scale poorly\nand fail to generalize across diverse industrial scenarios, whose acoustic\ncharacteristics are distinct from general audio. Furthermore, the scarcity of\naccessible, large-scale datasets and pretrained models tailored for industrial\naudio impedes community-driven research and benchmarking. To address these\nchallenges, we introduce DINOS (Diverse INdustrial Operation Sounds), a\nlarge-scale open-access dataset. DINOS comprises over 74,149 audio samples\n(exceeding 1,093 hours) collected from various industrial acoustic scenarios.\nWe also present IMPACT (Industrial Machine Perception via Acoustic Cognitive\nTransformer), a novel foundation model for industrial machine sound analysis.\nIMPACT is pretrained on DINOS in a self-supervised manner. By jointly\noptimizing utterance and frame-level losses, it captures both global semantics\nand fine-grained temporal structures. This makes its representations suitable\nfor efficient fine-tuning on various industrial downstream tasks with minimal\nlabeled data. Comprehensive benchmarking across 30 distinct downstream tasks\n(spanning four machine types) demonstrates that IMPACT outperforms existing\nmodels on 24 tasks, establishing its superior effectiveness and robustness,\nwhile providing a new performance benchmark for future research.", "AI": {"tldr": "\u8bba\u6587\u4ecb\u7ecd\u4e86DINOS\u6570\u636e\u96c6\u548cIMPACT\u6a21\u578b\uff0c\u7528\u4e8e\u5de5\u4e1a\u673a\u5668\u58f0\u97f3\u5206\u6790\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u6cdb\u5316\u6027\u548c\u6570\u636e\u7a00\u7f3a\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u76d1\u7763\u5b66\u4e60\u65b9\u6cd5\u5728\u5de5\u4e1a\u97f3\u9891\u573a\u666f\u4e2d\u6cdb\u5316\u6027\u5dee\uff0c\u4e14\u7f3a\u4e4f\u5927\u89c4\u6a21\u6570\u636e\u96c6\u548c\u9884\u8bad\u7ec3\u6a21\u578b\u3002", "method": "\u63d0\u51faDINOS\u6570\u636e\u96c6\u548cIMPACT\u6a21\u578b\uff0c\u901a\u8fc7\u81ea\u76d1\u7763\u5b66\u4e60\u8054\u5408\u4f18\u5316\u5168\u5c40\u548c\u5c40\u90e8\u635f\u5931\u3002", "result": "IMPACT\u572830\u4e2a\u4e0b\u6e38\u4efb\u52a1\u4e2d\u4f18\u4e8e\u73b0\u6709\u6a21\u578b\uff0c24\u4e2a\u4efb\u52a1\u8868\u73b0\u6700\u4f73\u3002", "conclusion": "DINOS\u548cIMPACT\u4e3a\u5de5\u4e1a\u97f3\u9891\u5206\u6790\u63d0\u4f9b\u4e86\u65b0\u57fa\u51c6\uff0c\u652f\u6301\u672a\u6765\u7814\u7a76\u3002"}}
{"id": "2507.06278", "pdf": "https://arxiv.org/pdf/2507.06278", "abs": "https://arxiv.org/abs/2507.06278", "authors": ["Kemboi Cheruiyot", "Nickson Kiprotich", "Vyacheslav Kungurtsev", "Kennedy Mugo", "Vivian Mwirigi", "Marvin Ngesa"], "title": "A Survey of Multi Agent Reinforcement Learning: Federated Learning and Cooperative and Noncooperative Decentralized Regimes", "categories": ["cs.MA", "cs.AI", "cs.LG"], "comment": null, "summary": "The increasing interest in research and innovation towards the development of\nautonomous agents presents a number of complex yet important scenarios of\nmultiple AI Agents interacting with each other in an environment. The\nparticular setting can be understood as exhibiting three possibly topologies of\ninteraction - centrally coordinated cooperation, ad-hoc interaction and\ncooperation, and settings with noncooperative incentive structures. This\narticle presents a comprehensive survey of all three domains, defined under the\nformalism of Federal Reinforcement Learning (RL), Decentralized RL, and\nNoncooperative RL, respectively. Highlighting the structural similarities and\ndistinctions, we review the state of the art in these subjects, primarily\nexplored and developed only recently in the literature. We include the\nformulations as well as known theoretical guarantees and highlights and\nlimitations of numerical performance.", "AI": {"tldr": "\u672c\u6587\u7efc\u8ff0\u4e86\u591a\u667a\u80fd\u4f53\u4ea4\u4e92\u7684\u4e09\u79cd\u62d3\u6251\u7ed3\u6784\uff1a\u8054\u90a6\u5f3a\u5316\u5b66\u4e60\u3001\u53bb\u4e2d\u5fc3\u5316\u5f3a\u5316\u5b66\u4e60\u548c\u975e\u5408\u4f5c\u5f3a\u5316\u5b66\u4e60\uff0c\u5206\u6790\u4e86\u5176\u7ed3\u6784\u3001\u7406\u8bba\u4fdd\u8bc1\u53ca\u6570\u503c\u6027\u80fd\u3002", "motivation": "\u7814\u7a76\u591a\u667a\u80fd\u4f53\u4ea4\u4e92\u7684\u590d\u6742\u573a\u666f\uff0c\u63a2\u8ba8\u4e0d\u540c\u4ea4\u4e92\u62d3\u6251\u7ed3\u6784\u7684\u7406\u8bba\u57fa\u7840\u548c\u5b9e\u9645\u8868\u73b0\u3002", "method": "\u901a\u8fc7\u6587\u732e\u7efc\u8ff0\uff0c\u5206\u6790\u8054\u90a6\u5f3a\u5316\u5b66\u4e60\u3001\u53bb\u4e2d\u5fc3\u5316\u5f3a\u5316\u5b66\u4e60\u548c\u975e\u5408\u4f5c\u5f3a\u5316\u5b66\u4e60\u7684\u7ed3\u6784\u3001\u7406\u8bba\u4fdd\u8bc1\u53ca\u6570\u503c\u6027\u80fd\u3002", "result": "\u603b\u7ed3\u4e86\u4e09\u79cd\u4ea4\u4e92\u62d3\u6251\u7684\u76f8\u4f3c\u6027\u4e0e\u533a\u522b\uff0c\u5e76\u6307\u51fa\u5f53\u524d\u7814\u7a76\u7684\u5c40\u9650\u6027\u548c\u6027\u80fd\u8868\u73b0\u3002", "conclusion": "\u591a\u667a\u80fd\u4f53\u4ea4\u4e92\u7684\u7814\u7a76\u4ecd\u9700\u8fdb\u4e00\u6b65\u63a2\u7d22\uff0c\u7279\u522b\u662f\u5728\u7406\u8bba\u4fdd\u8bc1\u548c\u5b9e\u9645\u5e94\u7528\u6027\u80fd\u65b9\u9762\u3002"}}
{"id": "2507.06346", "pdf": "https://arxiv.org/pdf/2507.06346", "abs": "https://arxiv.org/abs/2507.06346", "authors": ["Li Zhou", "Elvan Ceyhan"], "title": "Solving the Constrained Random Disambiguation Path Problem via Lagrangian Relaxation and Graph Reduction", "categories": ["cs.RO", "stat.CO"], "comment": null, "summary": "We study a resource-constrained variant of the Random Disambiguation Path\n(RDP) problem, a generalization of the Stochastic Obstacle Scene (SOS) problem,\nin which a navigating agent must reach a target in a spatial environment\npopulated with uncertain obstacles. Each ambiguous obstacle may be\ndisambiguated at a (possibly) heterogeneous resource cost, subject to a global\ndisambiguation budget. We formulate this constrained planning problem as a\nWeight-Constrained Shortest Path Problem (WCSPP) with risk-adjusted edge costs\nthat incorporate probabilistic blockage and traversal penalties. To solve it,\nwe propose a novel algorithmic framework-COLOGR-combining Lagrangian relaxation\nwith a two-phase vertex elimination (TPVE) procedure. The method prunes\ninfeasible and suboptimal paths while provably preserving the optimal solution,\nand leverages dual bounds to guide efficient search. We establish correctness,\nfeasibility guarantees, and surrogate optimality under mild assumptions. Our\nanalysis also demonstrates that COLOGR frequently achieves zero duality gap and\noffers improved computational complexity over prior constrained path-planning\nmethods. Extensive simulation experiments validate the algorithm's robustness\nacross varying obstacle densities, sensor accuracies, and risk models,\nconsistently outperforming greedy baselines and approaching offline-optimal\nbenchmarks. The proposed framework is broadly applicable to stochastic network\ndesign, mobility planning, and constrained decision-making under uncertainty.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u8d44\u6e90\u53d7\u9650\u7684\u968f\u673a\u6d88\u6b67\u8def\u5f84\uff08RDP\uff09\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u62c9\u683c\u6717\u65e5\u677e\u5f1b\u548c\u4e24\u9636\u6bb5\u9876\u70b9\u6d88\u9664\uff08TPVE\uff09\u7684\u65b0\u7b97\u6cd5\u6846\u67b6COLOGR\uff0c\u7528\u4e8e\u89e3\u51b3\u5e26\u7ea6\u675f\u7684\u6700\u77ed\u8def\u5f84\u95ee\u9898\uff0c\u5e76\u5728\u5b9e\u9a8c\u4e2d\u9a8c\u8bc1\u4e86\u5176\u4f18\u8d8a\u6027\u3002", "motivation": "\u89e3\u51b3\u5728\u4e0d\u786e\u5b9a\u969c\u788d\u7269\u73af\u5883\u4e2d\uff0c\u8d44\u6e90\u53d7\u9650\u7684\u8def\u5f84\u89c4\u5212\u95ee\u9898\uff0c\u4e3a\u79fb\u52a8\u4ee3\u7406\u63d0\u4f9b\u9ad8\u6548\u7684\u51b3\u7b56\u652f\u6301\u3002", "method": "\u5c06\u95ee\u9898\u5efa\u6a21\u4e3a\u5e26\u6743\u91cd\u7ea6\u675f\u7684\u6700\u77ed\u8def\u5f84\u95ee\u9898\uff08WCSPP\uff09\uff0c\u63d0\u51faCOLOGR\u7b97\u6cd5\u6846\u67b6\uff0c\u7ed3\u5408\u62c9\u683c\u6717\u65e5\u677e\u5f1b\u548cTPVE\u65b9\u6cd5\uff0c\u4fee\u526a\u4e0d\u53ef\u884c\u548c\u6b21\u4f18\u8def\u5f84\u3002", "result": "COLOGR\u5728\u5b9e\u9a8c\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f18\u4e8e\u8d2a\u5a6a\u57fa\u7ebf\u65b9\u6cd5\uff0c\u63a5\u8fd1\u79bb\u7ebf\u6700\u4f18\u57fa\u51c6\uff0c\u4e14\u8ba1\u7b97\u590d\u6742\u5ea6\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "COLOGR\u6846\u67b6\u5728\u968f\u673a\u7f51\u7edc\u8bbe\u8ba1\u3001\u79fb\u52a8\u89c4\u5212\u548c\u4e0d\u786e\u5b9a\u6027\u7ea6\u675f\u51b3\u7b56\u4e2d\u5177\u6709\u5e7f\u6cdb\u5e94\u7528\u524d\u666f\u3002"}}
{"id": "2507.06261", "pdf": "https://arxiv.org/pdf/2507.06261", "abs": "https://arxiv.org/abs/2507.06261", "authors": ["Gheorghe Comanici", "Eric Bieber", "Mike Schaekermann", "Ice Pasupat", "Noveen Sachdeva", "Inderjit Dhillon", "Marcel Blistein", "Ori Ram", "Dan Zhang", "Evan Rosen", "Luke Marris", "Sam Petulla", "Colin Gaffney", "Asaf Aharoni", "Nathan Lintz", "Tiago Cardal Pais", "Henrik Jacobsson", "Idan Szpektor", "Nan-Jiang Jiang", "Krishna Haridasan", "Ahmed Omran", "Nikunj Saunshi", "Dara Bahri", "Gaurav Mishra", "Eric Chu", "Toby Boyd", "Brad Hekman", "Aaron Parisi", "Chaoyi Zhang", "Kornraphop Kawintiranon", "Tania Bedrax-Weiss", "Oliver Wang", "Ya Xu", "Ollie Purkiss", "Uri Mendlovic", "Ila\u00ef Deutel", "Nam Nguyen", "Adam Langley", "Flip Korn", "Lucia Rossazza", "Alexandre Ram\u00e9", "Sagar Waghmare", "Helen Miller", "Vaishakh Keshava", "Ying Jian", "Xiaofan Zhang", "Raluca Ada Popa", "Kedar Dhamdhere", "Bla\u017e Bratani\u010d", "Kyuyeun Kim", "Terry Koo", "Ferran Alet", "Yi-ting Chen", "Arsha Nagrani", "Hannah Muckenhirn", "Zhiyuan Zhang", "Corbin Quick", "Filip Paveti\u0107", "Duc Dung Nguyen", "Joao Carreira", "Michael Elabd", "Haroon Qureshi", "Fabian Mentzer", "Yao-Yuan Yang", "Danielle Eisenbud", "Anmol Gulati", "Ellie Talius", "Eric Ni", "Sahra Ghalebikesabi", "Edouard Yvinec", "Alaa Saade", "Thatcher Ulrich", "Lorenzo Blanco", "Dan A. Calian", "Muhuan Huang", "A\u00e4ron van den Oord", "Naman Goyal", "Terry Chen", "Praynaa Rawlani", "Christian Schallhart", "Swachhand Lokhande", "Xianghong Luo", "Jyn Shan", "Ceslee Montgomery", "Victoria Krakovna", "Federico Piccinini", "Omer Barak", "Jingyu Cui", "Yiling Jia", "Mikhail Dektiarev", "Alexey Kolganov", "Shiyu Huang", "Zhe Chen", "Xingyu Wang", "Jessica Austin", "Peter de Boursac", "Evgeny Sluzhaev", "Frank Ding", "Huijian Li", "Surya Bhupatiraju", "Mohit Agarwal", "S\u0142awek Kwasiborski", "Paramjit Sandhu", "Patrick Siegler", "Ahmet Iscen", "Eyal Ben-David", "Shiraz Butt", "Miltos Allamanis", "Seth Benjamin", "Robert Busa-Fekete", "Felix Hernandez-Campos", "Sasha Goldshtein", "Matt Dibb", "Weiyang Zhang", "Annie Marsden", "Carey Radebaugh", "Stephen Roller", "Abhishek Nayyar", "Jacob Austin", "Tayfun Terzi", "Bhargav Kanagal Shamanna", "Pete Shaw", "Aayush Singh", "Florian Luisier", "Artur Mendon\u00e7a", "Vaibhav Aggarwal", "Larisa Markeeva", "Claudio Fantacci", "Sergey Brin", "HyunJeong Choe", "Guanyu Wang", "Hartwig Adam", "Avigail Dabush", "Tatsuya Kiyono", "Eyal Marcus", "Jeremy Cole", "Theophane Weber", "Hongrae Lee", "Ronny Huang", "Alex Muzio", "Leandro Kieliger", "Maigo Le", "Courtney Biles", "Long Le", "Archit Sharma", "Chengrun Yang", "Avery Lamp", "Dave Dopson", "Nate Hurley", "Katrina", "Xu", "Zhihao Shan", "Shuang Song", "Jiewen Tan", "Alexandre Senges", "George Zhang", "Chong You", "Yennie Jun", "David Raposo", "Susanna Ricco", "Xuan Yang", "Weijie Chen", "Prakhar Gupta", "Arthur Szlam", "Kevin Villela", "Chun-Sung Ferng", "Daniel Kasenberg", "Chen Liang", "Rui Zhu", "Arunachalam Narayanaswamy", "Florence Perot", "Paul Pucciarelli", "Anna Shekhawat", "Alexey Stern", "Rishikesh Ingale", "Stefani Karp", "Sanaz Bahargam", "Adrian Goedeckemeyer", "Jie Han", "Sicheng Li", "Andrea Tacchetti", "Dian Yu", "Abhishek Chakladar", "Zhiying Zhang", "Mona El Mahdy", "Xu Gao", "Dale Johnson", "Samrat Phatale", "AJ Piergiovanni", "Hyeontaek Lim", "Clement Farabet", "Carl Lebsack", "Theo Guidroz", "John Blitzer", "Nico Duduta", "David Madras", "Steve Li", "Daniel von Dincklage", "Xin Li", "Mahdis Mahdieh", "George Tucker", "Ganesh Jawahar", "Owen Xiao", "Danny Tarlow", "Robert Geirhos", "Noam Velan", "Daniel Vlasic", "Kalesha Bullard", "SK Park", "Nishesh Gupta", "Kellie Webster", "Ayal Hitron", "Jieming Mao", "Julian Eisenschlos", "Laurel Prince", "Nina D'Souza", "Kelvin Zheng", "Sara Nasso", "Gabriela Botea", "Carl Doersch", "Caglar Unlu", "Chris Alberti", "Alexey Svyatkovskiy", "Ankita Goel", "Krzysztof Choromanski", "Pan-Pan Jiang", "Richard Nguyen", "Four Flynn", "Daria \u0106urko", "Peter Chen", "Nicholas Roth", "Kieran Milan", "Caleb Habtegebriel", "Shashi Narayan", "Michael Moffitt", "Jake Marcus", "Thomas Anthony", "Brendan McMahan", "Gowoon Cheon", "Ruibo Liu", "Megan Barnes", "Lukasz Lew", "Rebeca Santamaria-Fernandez", "Mayank Upadhyay", "Arjun Akula", "Arnar Mar Hrafnkelsson", "Alvaro Caceres", "Andrew Bunner", "Michal Sokolik", "Subha Puttagunta", "Lawrence Moore", "Berivan Isik", "Weilun Chen", "Jay Hartford", "Lawrence Chan", "Pradeep Shenoy", "Dan Holtmann-Rice", "Jane Park", "Fabio Viola", "Alex Salcianu", "Sujeevan Rajayogam", "Ian Stewart-Binks", "Zelin Wu", "Richard Everett", "Xi Xiong", "Pierre-Antoine Manzagol", "Gary Leung", "Carl Saroufim", "Bo Pang", "Dawid Wegner", "George Papamakarios", "Jennimaria Palomaki", "Helena Pankov", "Guangda Lai", "Guilherme Tubone", "Shubin Zhao", "Theofilos Strinopoulos", "Seth Neel", "Mingqiu Wang", "Joe Kelley", "Li Li", "Pingmei Xu", "Anitha Vijayakumar", "Andrea D'olimpio", "Omer Levy", "Massimo Nicosia", "Grigory Rozhdestvenskiy", "Ni Lao", "Sirui Xie", "Yash Katariya", "Jon Simon", "Sanjiv Kumar", "Florian Hartmann", "Michael Kilgore", "Jinhyuk Lee", "Aroma Mahendru", "Roman Ring", "Tom Hennigan", "Fiona Lang", "Colin Cherry", "David Steiner", "Dawsen Hwang", "Ray Smith", "Pidong Wang", "Jeremy Chen", "Ming-Hsuan Yang", "Sam Kwei", "Philippe Schlattner", "Donnie Kim", "Ganesh Poomal Girirajan", "Nikola Momchev", "Ayushi Agarwal", "Xingyi Zhou", "Ilkin Safarli", "Zachary Garrett", "AJ Pierigiovanni", "Sarthak Jauhari", "Alif Raditya Rochman", "Shikhar Vashishth", "Quan Yuan", "Christof Angermueller", "Jon Blanton", "Xinying Song", "Nitesh Bharadwaj Gundavarapu", "Thi Avrahami", "Maxine Deines", "Subhrajit Roy", "Manish Gupta", "Christopher Semturs", "Shobha Vasudevan", "Aditya Srikanth Veerubhotla", "Shriya Sharma", "Josh Jacob", "Zhen Yang", "Andreas Terzis", "Dan Karliner", "Auriel Wright", "Tania Rojas-Esponda", "Ashley Brown", "Abhijit Guha Roy", "Pawan Dogra", "Andrei Kapishnikov", "Peter Young", "Wendy Kan", "Vinodh Kumar Rajendran", "Maria Ivanova", "Salil Deshmukh", "Chia-Hua Ho", "Mike Kwong", "Stav Ginzburg", "Annie Louis", "KP Sawhney", "Slav Petrov", "Jing Xie", "Yunfei Bai", "Georgi Stoyanov", "Alex Fabrikant", "Rajesh Jayaram", "Yuqi Li", "Joe Heyward", "Justin Gilmer", "Yaqing Wang", "Radu Soricut", "Luyang Liu", "Qingnan Duan", "Jamie Hayes", "Maura O'Brien", "Gaurav Singh Tomar", "Sivan Eiger", "Bahar Fatemi", "Jeffrey Hui", "Catarina Barros", "Adaeze Chukwuka", "Alena Butryna", "Saksham Thakur", "Austin Huang", "Zhufeng Pan", "Haotian Tang", "Serkan Cabi", "Tulsee Doshi", "Michiel Bakker", "Sumit Bagri", "Ruy Ley-Wild", "Adam Lelkes", "Jennie Lees", "Patrick Kane", "David Greene", "Shimu Wu", "J\u00f6rg Bornschein", "Gabriela Surita", "Sarah Hodkinson", "Fangtao Li", "Chris Hidey", "S\u00e9bastien Pereira", "Sean Ammirati", "Phillip Lippe", "Adam Kraft", "Pu Han", "Sebastian Gerlach", "Zifeng Wang", "Liviu Panait", "Feng Han", "Brian Farris", "Yingying Bi", "Hannah DeBalsi", "Miaosen Wang", "Gladys Tyen", "James Cohan", "Susan Zhang", "Jarred Barber", "Da-Woon Chung", "Jaeyoun Kim", "Markus Kunesch", "Steven Pecht", "Nami Akazawa", "Abe Friesen", "James Lyon", "Ali Eslami", "Junru Wu", "Jie Tan", "Yue Song", "Ravi Kumar", "Chris Welty", "Ilia Akolzin", "Gena Gibson", "Sean Augenstein", "Arjun Pillai", "Nancy Yuen", "Du Phan", "Xin Wang", "Iain Barr", "Heiga Zen", "Nan Hua", "Casper Liu", "Jilei", "Wang", "Tanuj Bhatia", "Hao Xu", "Oded Elyada", "Pushmeet Kohli", "Mirek Ol\u0161\u00e1k", "Ke Chen", "Azalia Mirhoseini", "Noam Shazeer", "Shoshana Jakobovits", "Maggie Tran", "Nolan Ramsden", "Tarun Bharti", "Fred Alcober", "Yunjie Li", "Shilpa Shetty", "Jing Chen", "Dmitry Kalashnikov", "Megha Nawhal", "Sercan Arik", "Hanwen Chen", "Michiel Blokzijl", "Shubham Gupta", "James Rubin", "Rigel Swavely", "Sophie Bridgers", "Ian Gemp", "Chen Su", "Arun Suggala", "Juliette Pluto", "Mary Cassin", "Alain Vaucher", "Kaiyang Ji", "Jiahao Cai", "Andrew Audibert", "Animesh Sinha", "David Tian", "Efrat Farkash", "Amy Hua", "Jilin Chen", "Duc-Hieu Tran", "Edward Loper", "Nicole Brichtova", "Lara McConnaughey", "Ballie Sandhu", "Robert Leland", "Doug DeCarlo", "Andrew Over", "James Huang", "Xing Wu", "Connie Fan", "Eric Li", "Yun Lei", "Deepak Sharma", "Cosmin Paduraru", "Luo Yu", "Matko Bo\u0161njak", "Phuong Dao", "Min Choi", "Sneha Kudugunta", "Jakub Adamek", "Carlos Gu\u00eda", "Ali Khodaei", "Jie Feng", "Wenjun Zeng", "David Welling", "Sandeep Tata", "Christina Butterfield", "Andrey Vlasov", "Seliem El-Sayed", "Swaroop Mishra", "Tara Sainath", "Shentao Yang", "RJ Skerry-Ryan", "Jeremy Shar", "Robert Berry", "Arunkumar Rajendran", "Arun Kandoor", "Andrea Burns", "Deepali Jain", "Tom Stone", "Wonpyo Park", "Shibo Wang", "Albin Cassirer", "Guohui Wang", "Hayato Kobayashi", "Sergey Rogulenko", "Vineetha Govindaraj", "Miko\u0142aj Rybi\u0144ski", "Nadav Olmert", "Colin Evans", "Po-Sen Huang", "Kelvin Xu", "Premal Shah", "Terry Thurk", "Caitlin Sikora", "Mu Cai", "Jin Xie", "Elahe Dabir", "Saloni Shah", "Norbert Kalb", "Carrie Zhang", "Shruthi Prabhakara", "Amit Sabne", "Artiom Myaskovsky", "Vikas Raunak", "Blanca Huergo", "Behnam Neyshabur", "Jon Clark", "Ye Zhang", "Shankar Krishnan", "Eden Cohen", "Dinesh Tewari", "James Lottes", "Yumeya Yamamori", "Hui", "Li", "Mohamed Elhawaty", "Ada Maksutaj Oflazer", "Adri\u00e0 Recasens", "Sheryl Luo", "Duy Nguyen", "Taylor Bos", "Kalyan Andra", "Ana Salazar", "Ed Chi", "Jeongwoo Ko", "Matt Ginsberg", "Anders Andreassen", "Anian Ruoss", "Todor Davchev", "Elnaz Davoodi", "Chenxi Liu", "Min Kim", "Santiago Ontanon", "Chi Ming To", "Dawei Jia", "Rosemary Ke", "Jing Wang", "Anna Korsun", "Moran Ambar", "Ilya Kornakov", "Irene Giannoumis", "Toni Creswell", "Denny Zhou", "Yi Su", "Ishaan Watts", "Aleksandr Zaks", "Evgenii Eltyshev", "Ziqiang Feng", "Sidharth Mudgal", "Alex Kaskasoli", "Juliette Love", "Kingshuk Dasgupta", "Sam Shleifer", "Richard Green", "Sungyong Seo", "Chansoo Lee", "Dale Webster", "Prakash Shroff", "Ganna Raboshchuk", "Isabel Leal", "James Manyika", "Sofia Erell", "Daniel Murphy", "Zhisheng Xiao", "Anton Bulyenov", "Julian Walker", "Mark Collier", "Matej Kastelic", "Nelson George", "Sushant Prakash", "Sailesh Sidhwani", "Alexey Frolov", "Steven Hansen", "Petko Georgiev", "Tiberiu Sosea", "Chris Apps", "Aishwarya Kamath", "David Reid", "Emma Cooney", "Charlotte Magister", "Oriana Riva", "Alec Go", "Pu-Chin Chen", "Sebastian Krause", "Nir Levine", "Marco Fornoni", "Ilya Figotin", "Nick Roy", "Parsa Mahmoudieh", "Vladimir Magay", "Mukundan Madhavan", "Jin Miao", "Jianmo Ni", "Yasuhisa Fujii", "Ian Chou", "George Scrivener", "Zak Tsai", "Siobhan Mcloughlin", "Jeremy Selier", "Sandra Lefdal", "Jeffrey Zhao", "Abhijit Karmarkar", "Kushal Chauhan", "Shivanker Goel", "Zhaoyi Zhang", "Vihan Jain", "Parisa Haghani", "Mostafa Dehghani", "Jacob Scott", "Erin Farnese", "Anastasija Ili\u0107", "Steven Baker", "Julia Pawar", "Li Zhong", "Josh Camp", "Yoel Zeldes", "Shravya Shetty", "Anand Iyer", "V\u00edt List\u00edk", "Jiaxian Guo", "Luming Tang", "Mark Geller", "Simon Bucher", "Yifan Ding", "Hongzhi Shi", "Carrie Muir", "Dominik Grewe", "Ramy Eskander", "Octavio Ponce", "Boqing Gong", "Derek Gasaway", "Samira Khan", "Umang Gupta", "Angelos Filos", "Weicheng Kuo", "Klemen Kloboves", "Jennifer Beattie", "Christian Wright", "Leon Li", "Alicia Jin", "Sandeep Mariserla", "Miteyan Patel", "Jens Heitkaemper", "Dilip Krishnan", "Vivek Sharma", "David Bieber", "Christian Frank", "John Lambert", "Paul Caron", "Martin Polacek", "Mai Gim\u00e9nez", "Himadri Choudhury", "Xing Yu", "Sasan Tavakkol", "Arun Ahuja", "Franz Och", "Rodolphe Jenatton", "Wojtek Skut", "Bryan Richter", "David Gaddy", "Andy Ly", "Misha Bilenko", "Megh Umekar", "Ethan Liang", "Martin Sevenich", "Mandar Joshi", "Hassan Mansoor", "Rebecca Lin", "Sumit Sanghai", "Abhimanyu Singh", "Xiaowei Li", "Sudheendra Vijayanarasimhan", "Zaheer Abbas", "Yonatan Bitton", "Hansa Srinivasan", "Manish Reddy Vuyyuru", "Alexander Fr\u00f6mmgen", "Yanhua Sun", "Ralph Leith", "Alfonso Casta\u00f1o", "DJ Strouse", "Le Yan", "Austin Kyker", "Satish Kambala", "Mary Jasarevic", "Thibault Sellam", "Chao Jia", "Alexander Pritzel", "Raghavender R", "Huizhong Chen", "Natalie Clay", "Sudeep Gandhe", "Sean Kirmani", "Sayna Ebrahimi", "Hannah Kirkwood", "Jonathan Mallinson", "Chao Wang", "Adnan Ozturel", "Kuo Lin", "Shyam Upadhyay", "Vincent Cohen-Addad", "Sean Purser-haskell", "Yichong Xu", "Ebrahim Songhori", "Babi Seal", "Alberto Magni", "Almog Gueta", "Tingting Zou", "Guru Guruganesh", "Thais Kagohara", "Hung Nguyen", "Khalid Salama", "Alejandro Cruzado Ruiz", "Justin Frye", "Zhenkai Zhu", "Matthias Lochbrunner", "Simon Osindero", "Wentao Yuan", "Lisa Lee", "Aman Prasad", "Lam Nguyen Thiet", "Daniele Calandriello", "Victor Stone", "Qixuan Feng", "Han Ke", "Maria Voitovich", "Geta Sampemane", "Lewis Chiang", "Ling Wu", "Alexander Bykovsky", "Matt Young", "Luke Vilnis", "Ishita Dasgupta", "Aditya Chawla", "Qin Cao", "Bowen Liang", "Daniel Toyama", "Szabolcs Payrits", "Anca Stefanoiu", "Dimitrios Vytiniotis", "Ankesh Anand", "Tianxiao Shen", "Blagoj Mitrevski", "Michael Tschannen", "Sreenivas Gollapudi", "Aishwarya P S", "Jos\u00e9 Leal", "Zhe Shen", "Han Fu", "Wei Wang", "Arvind Kannan", "Doron Kukliansky", "Sergey Yaroshenko", "Svetlana Grant", "Umesh Telang", "David Wood", "Alexandra Chronopoulou", "Alexandru \u0162ifrea", "Tao Zhou", "Tony", "Nguy\\~\u00ean", "Muge Ersoy", "Anima Singh", "Meiyan Xie", "Emanuel Taropa", "Woohyun Han", "Eirikur Agustsson", "Andrei Sozanschi", "Hui Peng", "Alex Chen", "Yoel Drori", "Efren Robles", "Yang Gao", "Xerxes Dotiwalla", "Ying Chen", "Anudhyan Boral", "Alexei Bendebury", "John Nham", "Chris Tar", "Luis Castro", "Jiepu Jiang", "Canoee Liu", "Felix Halim", "Jinoo Baek", "Andy Wan", "Jeremiah Liu", "Yuan Cao", "Shengyang Dai", "Trilok Acharya", "Ruoxi Sun", "Fuzhao Xue", "Saket Joshi", "Morgane Lustman", "Yongqin Xian", "Rishabh Joshi", "Deep Karkhanis", "Nora Kassner", "Jamie Hall", "Xiangzhuo Ding", "Gan Song", "Gang Li", "Chen Zhu", "Yana Kulizhskaya", "Bin Ni", "Alexey Vlaskin", "Solomon Demmessie", "Lucio Dery", "Salah Zaiem", "Yanping Huang", "Cindy Fan", "Felix Gimeno", "Ananth Balashankar", "Koji Kojima", "Hagai Taitelbaum", "Maya Meng", "Dero Gharibian", "Sahil Singla", "Wei Chen", "Ambrose Slone", "Guanjie Chen", "Sujee Rajayogam", "Max Schumacher", "Suyog Kotecha", "Rory Blevins", "Qifei Wang", "Mor Hazan Taege", "Alex Morris", "Xin Liu", "Fayaz Jamil", "Richard Zhang", "Pratik Joshi", "Ben Ingram", "Tyler Liechty", "Ahmed Eleryan", "Scott Baird", "Alex Grills", "Gagan Bansal", "Shan Han", "Kiran Yalasangi", "Shawn Xu", "Majd Al Merey", "Isabel Gao", "Felix Weissenberger", "Igor Karpov", "Robert Riachi", "Ankit Anand", "Gautam Prasad", "Kay Lamerigts", "Reid Hayes", "Jamie Rogers", "Mandy Guo", "Ashish Shenoy", "Qiong", "Hu", "Kyle He", "Yuchen Liu", "Polina Zablotskaia", "Sagar Gubbi", "Yifan Chang", "Jay Pavagadhi", "Kristian Kjems", "Archita Vadali", "Diego Machado", "Yeqing Li", "Renshen Wang", "Dipankar Ghosh", "Aahil Mehta", "Dana Alon", "George Polovets", "Alessio Tonioni", "Nate Kushman", "Joel D'sa", "Lin Zhuo", "Allen Wu", "Rohin Shah", "John Youssef", "Jiayu Ye", "Justin Snyder", "Karel Lenc", "Senaka Buthpitiya", "Matthew Tung", "Jichuan Chang", "Tao Chen", "David Saxton", "Jenny Lee", "Lydia Lihui Zhang", "James Qin", "Prabakar Radhakrishnan", "Maxwell Chen", "Piotr Ambroszczyk", "Metin Toksoz-Exley", "Yan Zhong", "Nitzan Katz", "Brendan O'Donoghue", "Tamara von Glehn", "Adi Gerzi Rosenthal", "Aga \u015awietlik", "Xiaokai Zhao", "Nick Fernando", "Jinliang Wei", "Jieru Mei", "Sergei Vassilvitskii", "Diego Cedillo", "Pranjal Awasthi", "Hui Zheng", "Koray Kavukcuoglu", "Itay Laish", "Joseph Pagadora", "Marc Brockschmidt", "Christopher A. Choquette-Choo", "Arunkumar Byravan", "Yifeng Lu", "Xu Chen", "Mia Chen", "Kenton Lee", "Rama Pasumarthi", "Sijal Bhatnagar", "Aditya Shah", "Qiyin Wu", "Zhuoyuan Chen", "Zack Nado", "Bartek Perz", "Zixuan Jiang", "David Kao", "Ganesh Mallya", "Nino Vieillard", "Lantao Mei", "Sertan Girgin", "Mandy Jordan", "Yeongil Ko", "Alekh Agarwal", "Yaxin Liu", "Yasemin Altun", "Raoul de Liedekerke", "Anastasios Kementsietsidis", "Daiyi Peng", "Dangyi Liu", "Utku Evci", "Peter Humphreys", "Austin Tarango", "Xiang Deng", "Yoad Lewenberg", "Kevin Aydin", "Chengda Wu", "Bhavishya Mittal", "Tsendsuren Munkhdalai", "Kleopatra Chatziprimou", "Rodrigo Benenson", "Uri First", "Xiao Ma", "Jinning Li", "Armand Joulin", "Hamish Tomlinson", "Tingnan Zhang", "Milad Nasr", "Zhi Hong", "Micha\u00ebl Sander", "Lisa Anne Hendricks", "Anuj Sharma", "Andrew Bolt", "Eszter V\u00e9rtes", "Jiri Simsa", "Tomer Levinboim", "Olcan Sercinoglu", "Divyansh Shukla", "Austin Wu", "Craig Swanson", "Danny Vainstein", "Fan Bu", "Bo Wang", "Ryan Julian", "Charles Yoon", "Sergei Lebedev", "Antonious Girgis", "Bernd Bandemer", "David Du", "Todd Wang", "Xi Chen", "Ying Xiao", "Peggy Lu", "Natalie Ha", "Vlad Ionescu", "Simon Rowe", "Josip Matak", "Federico Lebron", "Andreas Steiner", "Lalit Jain", "Manaal Faruqui", "Nicolas Lacasse", "Georgie Evans", "Neesha Subramaniam", "Dean Reich", "Giulia Vezzani", "Aditya Pandey", "Joe Stanton", "Tianhao Zhou", "Liam McCafferty", "Henry Griffiths", "Verena Rieser", "Soheil Hassas Yeganeh", "Eleftheria Briakou", "Lu Huang", "Zichuan Wei", "Liangchen Luo", "Erik Jue", "Gabby Wang", "Victor Cotruta", "Myriam Khan", "Jongbin Park", "Qiuchen Guo", "Peiran Li", "Rong Rong", "Diego Antognini", "Anastasia Petrushkina", "Chetan Tekur", "Eli Collins", "Parul Bhatia", "Chester Kwak", "Wenhu Chen", "Arvind Neelakantan", "Immanuel Odisho", "Sheng Peng", "Vincent Nallatamby", "Vaibhav Tulsyan", "Fabian Pedregosa", "Peng Xu", "Raymond Lin", "Yulong Wang", "Emma Wang", "Sholto Douglas", "Reut Tsarfaty", "Elena Gribovskaya", "Renga Aravamudhan", "Manu Agarwal", "Mara Finkelstein", "Qiao Zhang", "Elizabeth Cole", "Phil Crone", "Sarmishta Velury", "Anil Das", "Chris Sauer", "Luyao Xu", "Danfeng Qin", "Chenjie Gu", "Dror Marcus", "CJ Zheng", "Wouter Van Gansbeke", "Sobhan Miryoosefi", "Haitian Sun", "YaGuang Li", "Charlie Chen", "Jae Yoo", "Pavel Dubov", "Alex Tomala", "Adams Yu", "Pawe\u0142 Weso\u0142owski", "Alok Gunjan", "Eddie Cao", "Jiaming Luo", "Nikhil Sethi", "Arkadiusz Socala", "Laura Graesser", "Tomas Kocisky", "Arturo BC", "Minmin Chen", "Edward Lee", "Sophie Wang", "Weize Kong", "Qiantong Xu", "Nilesh Tripuraneni", "Yiming Li", "Xinxin Yu", "Allen Porter", "Paul Voigtlaender", "Biao Zhang", "Arpi Vezer", "Sarah York", "Qing Wei", "Geoffrey Cideron", "Mark Kurzeja", "Seungyeon Kim", "Benny Li", "Ang\u00e9line Pouget", "Hyo Lee", "Kaspar Daugaard", "Yang Li", "Dave Uthus", "Aditya Siddhant", "Paul Cavallaro", "Sriram Ganapathy", "Maulik Shah", "Rolf Jagerman", "Jeff Stanway", "Piermaria Mendolicchio", "Li Xiao", "Kayi Lee", "Tara Thompson", "Shubham Milind Phal", "Jason Chase", "Sun Jae Lee", "Adrian N Reyes", "Disha Shrivastava", "Zhen Qin", "Roykrong Sukkerd", "Seth Odoom", "Lior Madmoni", "John Aslanides", "Jonathan Herzig", "Elena Pochernina", "Sheng Zhang", "Parker Barnes", "Daisuke Ikeda", "Qiujia Li", "Shuo-yiin Chang", "Shakir Mohamed", "Jim Sproch", "Richard Powell", "Bidisha Samanta", "Domagoj \u0106evid", "Anton Kovsharov", "Shrestha Basu Mallick", "Srinivas Tadepalli", "Anne Zheng", "Kareem Ayoub", "Andreas Noever", "Christian Reisswig", "Zhuo Xu", "Junhyuk Oh", "Martin Matysiak", "Tim Blyth", "Shereen Ashraf", "Julien Amelot", "Boone Severson", "Michele Bevilacqua", "Motoki Sano", "Ethan Dyer", "Ofir Roval", "Anu Sinha", "Yin Zhong", "Sagi Perel", "Tea Saboli\u0107", "Johannes Mauerer", "Willi Gierke", "Mauro Verzetti", "Rodrigo Cabrera", "Alvin Abdagic", "Steven Hemingray", "Austin Stone", "Jong Lee", "Farooq Ahmad", "Karthik Raman", "Lior Shani", "Jonathan Lai", "Orhan Firat", "Nathan Waters", "Eric Ge", "Mo Shomrat", "Himanshu Gupta", "Rajeev Aggarwal", "Tom Hudson", "Bill Jia", "Simon Baumgartner", "Palak Jain", "Joe Kovac", "Junehyuk Jung", "Ante \u017du\u017eul", "Will Truong", "Morteza Zadimoghaddam", "Songyou Peng", "Marco Liang", "Rachel Sterneck", "Balaji Lakshminarayanan", "Machel Reid", "Oliver Woodman", "Tong Zhou", "Jianling Wang", "Vincent Coriou", "Arjun Narayanan", "Jay Hoover", "Yenai Ma", "Apoorv Jindal", "Clayton Sanford", "Doug Reid", "Swaroop Ramaswamy", "Alex Kurakin", "Roland Zimmermann", "Yana Lunts", "Dragos Dena", "Zal\u00e1n Borsos", "Vered Cohen", "Shujian Zhang", "Will Grathwohl", "Robert Dadashi", "Morgan Redshaw", "Joshua Kessinger", "Julian Odell", "Silvano Bonacina", "Zihang Dai", "Grace Chen", "Ayush Dubey", "Pablo Sprechmann", "Mantas Pajarskas", "Wenxuan Zhou", "Niharika Ahuja", "Tara Thomas", "Martin Nikoltchev", "Matija Kecman", "Bharath Mankalale", "Andrey Ryabtsev", "Jennifer She", "Christian Walder", "Jiaming Shen", "Lu Li", "Carolina Parada", "Sheena Panthaplackel", "Okwan Kwon", "Matt Lawlor", "Utsav Prabhu", "Yannick Schroecker", "Marc'aurelio Ranzato", "Pete Blois", "Iurii Kemaev", "Ting Yu", "Dmitry", "Lepikhin", "Hao Xiong", "Sahand Sharifzadeh", "Oleaser Johnson", "Jeremiah Willcock", "Rui Yao", "Greg Farquhar", "Sujoy Basu", "Hidetoshi Shimokawa", "Nina Anderson", "Haiguang Li", "Khiem Pham", "Yizhong Liang", "Sebastian Borgeaud", "Alexandre Moufarek", "Hideto Kazawa", "Blair Kutzman", "Marcin Sieniek", "Sara Smoot", "Ruth Wang", "Natalie Axelsson", "Nova Fallen", "Prasha Sundaram", "Yuexiang Zhai", "Varun Godbole", "Petros Maniatis", "Alek Wang", "Ilia Shumailov", "Santhosh Thangaraj", "Remi Crocker", "Nikita Gupta", "Gang Wu", "Phil Chen", "Gell\u00e9rt Weisz", "Celine Smith", "Mojtaba Seyedhosseini", "Boya Fang", "Xiyang Luo", "Roey Yogev", "Zeynep Cankara", "Andrew Hard", "Helen Ran", "Rahul Sukthankar", "George Necula", "Ga\u00ebl Liu", "Honglong Cai", "Praseem Banzal", "Daniel Keysers", "Sanjay Ghemawat", "Connie Tao", "Emma Dunleavy", "Aditi Chaudhary", "Wei Li", "Maciej Miku\u0142a", "Chen-Yu Lee", "Tiziana Refice", "Krishna Somandepalli", "Alexandre Fr\u00e9chette", "Dan Bahir", "John Karro", "Keith Rush", "Sarah Perrin", "Bill Rosgen", "Xiaomeng Yang", "Clara Huiyi Hu", "Mahmoud Alnahlawi", "Justin Mao-Jones", "Roopal Garg", "Hoang Nguyen", "Bat-Orgil Batsaikhan", "I\u00f1aki Iturrate", "Anselm Levskaya", "Avi Singh", "Ashyana Kachra", "Tony Lu", "Denis Petek", "Zheng Xu", "Mark Graham", "Lukas Zilka", "Yael Karov", "Marija Kostelac", "Fangyu Liu", "Yaohui Guo", "Weiyue Wang", "Bernd Bohnet", "Emily Pitler", "Tony Bruguier", "Keisuke Kinoshita", "Chrysovalantis Anastasiou", "Nilpa Jha", "Ting Liu", "Jerome Connor", "Phil Wallis", "Philip Pham", "Eric Bailey", "Shixin Li", "Heng-Tze Cheng", "Sally Ma", "Haiqiong Li", "Akanksha Maurya", "Kate Olszewska", "Manfred Warmuth", "Christy Koh", "Dominik Paulus", "Siddhartha Reddy Jonnalagadda", "Enrique Piqueras", "Ali Elqursh", "Geoff Brown", "Hadar Shemtov", "Loren Maggiore", "Fei Xia", "Ryan Foley", "Beka Westberg", "George van den Driessche", "Livio Baldini Soares", "Arjun Kar", "Michael Quinn", "Siqi Zuo", "Jialin Wu", "Kyle Kastner", "Anna Bortsova", "Aijun Bai", "Ales Mikhalap", "Luowei Zhou", "Jennifer Brennan", "Vinay Ramasesh", "Honglei Zhuang", "John Maggs", "Johan Schalkwyk", "Yuntao Xu", "Hui Huang", "Andrew Howard", "Sasha Brown", "Linting Xue", "Gloria Shen", "Brian Albert", "Neha Jha", "Daniel Zheng", "Varvara Krayvanova", "Spurthi Amba Hombaiah", "Olivier Lacombe", "Gautam Vasudevan", "Dan Graur", "Tian Xie", "Meet Gandhi", "Bangju Wang", "Dustin Zelle", "Harman Singh", "Dahun Kim", "S\u00e9bastien Cevey", "Victor Ungureanu", "Natasha Noy", "Fei Liu", "Annie Xie", "Fangxiaoyu Feng", "Katerina Tsihlas", "Daniel Formoso", "Neera Vats", "Quentin Wellens", "Yinan Wang", "Niket Kumar Bhumihar", "Samrat Ghosh", "Matt Hoffman", "Tom Lieber", "Oran Lang", "Kush Bhatia", "Tom Paine", "Aroonalok Pyne", "Ronny Votel", "Madeleine Clare Elish", "Benoit Schillings", "Alex Panagopoulos", "Haichuan Yang", "Adam Raveret", "Zohar Yahav", "Shuang Liu", "Warren Chen", "Dalia El Badawy", "Nishant Agrawal", "Mohammed Badawi", "Mahdi Mirzazadeh", "Carla Bromberg", "Fan Ye", "Chang Liu", "Tatiana Sholokhova", "George-Cristian Muraru", "Gargi Balasubramaniam", "Jonathan Malmaud", "Alen Carin", "Danilo Martins", "Irina Jurenka", "Pankil Botadra", "Dave Lacey", "Richa Singh", "Mariano Schain", "Dan Zheng", "Isabelle Guyon", "Victor Lavrenko", "Seungji Lee", "Xiang Zhou", "Demis Hassabis", "Jeshwanth Challagundla", "Derek Cheng", "Nikhil Mehta", "Matthew Mauger", "Michela Paganini", "Pushkar Mishra", "Kate Lee", "Zhang Li", "Lexi Baugher", "Ondrej Skopek", "Max Chang", "Amir Zait", "Gaurav Menghani", "Lizzetth Bellot", "Guangxing Han", "Jean-Michel Sarr", "Sharat Chikkerur", "Himanshu Sahni", "Rohan Anil", "Arun Narayanan", "Chandu Thekkath", "Daniele Pighin", "Hana Strej\u010dek", "Marko Velic", "Fred Bertsch", "Manuel Tragut", "Keran Rong", "Alicia Parrish", "Kai Bailey", "Jiho Park", "Isabela Albuquerque", "Abhishek Bapna", "Rajesh Venkataraman", "Alec Kosik", "Johannes Griesser", "Zhiwei Deng", "Alek Andreev", "Qingyun Dou", "Kevin Hui", "Fanny Wei", "Xiaobin Yu", "Lei Shu", "Avia Aharon", "David Barker", "Badih Ghazi", "Sebastian Flennerhag", "Chris Breaux", "Yuchuan Liu", "Matthew Bilotti", "Josh Woodward", "Uri Alon", "Stephanie Winkler", "Tzu-Kuo Huang", "Kostas Andriopoulos", "Jo\u00e3o Gabriel Oliveira", "Penporn Koanantakool", "Berkin Akin", "Michael Wunder", "Cicero Nogueira dos Santos", "Mohammad Hossein Bateni", "Lin Yang", "Dan Horgan", "Beer Changpinyo", "Keyvan Amiri", "Min Ma", "Dayeong Lee", "Lihao Liang", "Anirudh Baddepudi", "Tejasi Latkar", "Raia Hadsell", "Jun Xu", "Hairong Mu", "Michael Han", "Aedan Pope", "Snchit Grover", "Frank Kim", "Ankit Bhagatwala", "Guan Sun", "Yamini Bansal", "Amir Globerson", "Alireza Nazari", "Samira Daruki", "Hagen Soltau", "Jane Labanowski", "Laurent El Shafey", "Matt Harvey", "Yanif Ahmad", "Elan Rosenfeld", "William Kong", "Etienne Pot", "Yi-Xuan Tan", "Aurora Wei", "Victoria Langston", "Marcel Prasetya", "Petar Veli\u010dkovi\u0107", "Richard Killam", "Robin Strudel", "Darren Ni", "Zhenhai Zhu", "Aaron Archer", "Kavya Kopparapu", "Lynn Nguyen", "Emilio Parisotto", "Hussain Masoom", "Sravanti Addepalli", "Jordan Grimstad", "Hexiang Hu", "Joss Moore", "Avinatan Hassidim", "Le Hou", "Mukund Raghavachari", "Jared Lichtarge", "Adam R. Brown", "Hilal Dib", "Natalia Ponomareva", "Justin Fu", "Yujing Zhang", "Altaf Rahman", "Joana Iljazi", "Edouard Leurent", "Gabriel Dulac-Arnold", "Cosmo Du", "Chulayuth Asawaroengchai", "Larry Jin", "Ela Gruzewska", "Ziwei Ji", "Benigno Uria", "Daniel De Freitas", "Paul Barham", "Lauren Beltrone", "V\u00edctor Campos", "Jun Yan", "Neel Kovelamudi", "Arthur Nguyen", "Elinor Davies", "Zhichun Wu", "Zoltan Egyed", "Kristina Toutanova", "Nithya Attaluri", "Hongliang Fei", "Peter Stys", "Siddhartha Brahma", "Martin Izzard", "Siva Velusamy", "Scott Lundberg", "Vincent Zhuang", "Kevin Sequeira", "Adam Santoro", "Ehsan Amid", "Ophir Aharoni", "Shuai Ye", "Mukund Sundararajan", "Lijun Yu", "Yu-Cheng Ling", "Stephen Spencer", "Hugo Song", "Josip Djolonga", "Christo Kirov", "Sonal Gupta", "Alessandro Bissacco", "Clemens Meyer", "Mukul Bhutani", "Andrew Dai", "Weiyi Wang", "Siqi Liu", "Ashwin Sreevatsa", "Qijun Tan", "Maria Wang", "Lucy Kim", "Yicheng Wang", "Alex Irpan", "Yang Xiao", "Stanislav Fort", "Yifan He", "Alex Gurney", "Bryan Gale", "Yue Ma", "Monica Roy", "Viorica Patraucean", "Taylan Bilal", "Golnaz Ghiasi", "Anahita Hosseini", "Melvin Johnson", "Zhuowan Li", "Yi Tay", "Benjamin Beyret", "Katie Millican", "Josef Broder", "Mayank Lunayach", "Danny Swisher", "Eugen Vu\u0161ak", "David Parkinson", "MH Tessler", "Adi Mayrav Gilady", "Richard Song", "Allan Dafoe", "Yves Raimond", "Masa Yamaguchi", "Itay Karo", "Elizabeth Nielsen", "Kevin Kilgour", "Mike Dusenberry", "Rajiv Mathews", "Jiho Choi", "Siyuan Qiao", "Harsh Mehta", "Sahitya Potluri", "Chris Knutsen", "Jialu Liu", "Tat Tan", "Kuntal Sengupta", "Keerthana Gopalakrishnan", "Abodunrinwa Toki", "Mencher Chiang", "Mike Burrows", "Grace Vesom", "Zafarali Ahmed", "Ilia Labzovsky", "Siddharth Vashishtha", "Preeti Singh", "Ankur Sharma", "Ada Ma", "Jinyu Xie", "Pranav Talluri", "Hannah Forbes-Pollard", "Aarush Selvan", "Joel Wee", "Loic Matthey", "Tom Funkhouser", "Parthasarathy Gopavarapu", "Lev Proleev", "Cheng Li", "Matt Thomas", "Kashyap Kolipaka", "Zhipeng Jia", "Ashwin Kakarla", "Srinivas Sunkara", "Joan Puigcerver", "Suraj Satishkumar Sheth", "Emily Graves", "Chen Wang", "Sadh MNM Khan", "Kai Kang", "Shyamal Buch", "Fred Zhang", "Omkar Savant", "David Soergel", "Kevin Lee", "Linda Friso", "Xuanyi Dong", "Rahul Arya", "Shreyas Chandrakaladharan", "Connor Schenck", "Greg Billock", "Tejas Iyer", "Anton Bakalov", "Leslie Baker", "Alex Ruiz", "Angad Chandorkar", "Trieu Trinh", "Matt Miecnikowski", "Yanqi Zhou", "Yangsibo Huang", "Jiazhong Nie", "Ali Shah", "Ashish Thapliyal", "Sam Haves", "Lun Wang", "Uri Shaham", "Patrick Morris-Suzuki", "Soroush Radpour", "Leonard Berrada", "Thomas Strohmann", "Chaochao Yan", "Jingwei Shen", "Sonam Goenka", "Tris Warkentin", "Petar Devi\u0107", "Dan Belov", "Albert Webson", "Madhavi Yenugula", "Puranjay Datta", "Jerry Chang", "Nimesh Ghelani", "Aviral Kumar", "Vincent Perot", "Jessica Lo", "Yang Song", "Herman Schmit", "Jianmin Chen", "Vasilisa Bashlovkina", "Xiaoyue Pan", "Diana Mincu", "Paul Roit", "Isabel Edkins", "Andy Davis", "Yujia Li", "Ben Horn", "Xinjian Li", "Pradeep Kumar S", "Eric Doi", "Wanzheng Zhu", "Sri Gayatri Sundara Padmanabhan", "Siddharth Verma", "Jasmine Liu", "Heng Chen", "Mihajlo Velimirovi\u0107", "Malcolm Reynolds", "Priyanka Agrawal", "Nick Sukhanov", "Abhinit Modi", "Siddharth Goyal", "John Palowitch", "Nima Khajehnouri", "Wing Lowe", "David Klinghoffer", "Sharon Silver", "Vinh Tran", "Candice Schumann", "Francesco Piccinno", "Xi Liu", "Mario Lu\u010di\u0107", "Xiaochen Yang", "Sandeep Kumar", "Ajay Kannan", "Ragha Kotikalapudi", "Mudit Bansal", "Fabian Fuchs", "Javad Hosseini", "Abdelrahman Abdelhamed", "Dawn Bloxwich", "Tianhe Yu", "Ruoxin Sang", "Gregory Thornton", "Karan Gill", "Yuchi Liu", "Virat Shejwalkar", "Jason Lin", "Zhipeng Yan", "Kehang Han", "Thomas Buschmann", "Michael Pliskin", "Zhi Xing", "Susheel Tatineni", "Junlin Zhang", "Sissie Hsiao", "Gavin Buttimore", "Marcus Wu", "Zefei Li", "Geza Kovacs", "Legg Yeung", "Tao Huang", "Aaron Cohen", "Bethanie Brownfield", "Averi Nowak", "Mikel Rodriguez", "Tianze Shi", "Hado van Hasselt", "Kevin Cen", "Deepanway Ghoshal", "Kushal Majmundar", "Weiren Yu", "Warren", "Chen", "Danila Sinopalnikov", "Hao Zhang", "Vlado Gali\u0107", "Di Lu", "Zeyu Zheng", "Maggie Song", "Gary Wang", "Gui Citovsky", "Swapnil Gawde", "Isaac Galatzer-Levy", "David Silver", "Ivana Balazevic", "Dipanjan Das", "Kingshuk Majumder", "Yale Cong", "Praneet Dutta", "Dustin Tran", "Hui Wan", "Junwei Yuan", "Daniel Eppens", "Alanna Walton", "Been Kim", "Harry Ragan", "James Cobon-Kerr", "Lu Liu", "Weijun Wang", "Bryce Petrini", "Jack Rae", "Rakesh Shivanna", "Yan Xiong", "Chace Lee", "Pauline Coquinot", "Yiming Gu", "Lisa Patel", "Blake Hechtman", "Aviel Boag", "Orion Jankowski", "Alex Wertheim", "Alex Lee", "Paul Covington", "Hila Noga", "Sam Sobell", "Shanthal Vasanth", "William Bono", "Chirag Nagpal", "Wei Fan", "Xavier Garcia", "Kedar Soparkar", "Aybuke Turker", "Nathan Howard", "Sachit Menon", "Yuankai Chen", "Vikas Verma", "Vladimir Pchelin", "Harish Rajamani", "Valentin Dalibard", "Ana Ramalho", "Yang Guo", "Kartikeya Badola", "Seojin Bang", "Nathalie Rauschmayr", "Julia Proskurnia", "Sudeep Dasari", "Xinyun Chen", "Mikhail Sushkov", "Anja Hauth", "Pauline Sho", "Abhinav Singh", "Bilva Chandra", "Allie Culp", "Max Dylla", "Olivier Bachem", "James Besley", "Heri Zhao", "Timothy Lillicrap", "Wei Wei", "Wael Al Jishi", "Ning Niu", "Alban Rrustemi", "Rapha\u00ebl Lopez Kaufman", "Ryan Poplin", "Jewel Zhao", "Minh Truong", "Shikhar Bharadwaj", "Ester Hlavnova", "Eli Stickgold", "Cordelia Schmid", "Georgi Stephanov", "Zhaoqi Leng", "Frederick Liu", "L\u00e9onard Hussenot", "Shenil Dodhia", "Juliana Vicente Franco", "Lesley Katzen", "Abhanshu Sharma", "Sarah Cogan", "Zuguang Yang", "Aniket Ray", "Sergi Caelles", "Shen Yan", "Ravin Kumar", "Daniel Gillick", "Renee Wong", "Joshua Ainslie", "Jonathan Hoech", "S\u00e9b Arnold", "Dan Abolafia", "Anca Dragan", "Ben Hora", "Grace Hu", "Alexey Guseynov", "Yang Lu", "Chas Leichner", "Jinmeng Rao", "Abhimanyu Goyal", "Nagabhushan Baddi", "Daniel Hernandez Diaz", "Tim McConnell", "Max Bain", "Jake Abernethy", "Qiqi Yan", "Rylan Schaeffer", "Paul Vicol", "Will Thompson", "Montse Gonzalez Arenas", "Mathias Bellaiche", "Pablo Barrio", "Stefan Zinke", "Riccardo Patana", "Pulkit Mehta", "JK Kearns", "Avraham Ruderman", "Scott Pollom", "David D'Ambrosio", "Cath Hope", "Yang Yu", "Andrea Gesmundo", "Kuang-Huei Lee", "Aviv Rosenberg", "Yiqian Zhou", "Yaoyiran Li", "Drew Garmon", "Yonghui Wu", "Safeen Huda", "Gil Fidel", "Martin Baeuml", "Jian Li", "Phoebe Kirk", "Rhys May", "Tao Tu", "Sara Mc Carthy", "Toshiyuki Fukuzawa", "Miranda Aperghis", "Chih-Kuan Yeh", "Toshihiro Yoshino", "Bo Li", "Austin Myers", "Kaisheng Yao", "Ben Limonchik", "Changwan Ryu", "Rohun Saxena", "Alex Goldin", "Ruizhe Zhao", "Rocky Rhodes", "Tao Zhu", "Divya Tyam", "Heidi Howard", "Nathan Byrd", "Hongxu Ma", "Yan Wu", "Ryan Mullins", "Qingze Wang", "Aida Amini", "Sebastien Baur", "Yiran Mao", "Subhashini Venugopalan", "Will Song", "Wen Ding", "Paul Collins", "Sashank Reddi", "Megan Shum", "Andrei Rusu", "Luisa Zintgraf", "Kelvin Chan", "Sheela Goenka", "Mathieu Blondel", "Michael Collins", "Renke Pan", "Marissa Giustina", "Nikolai Chinaev", "Christian Schuler", "Ce Zheng", "Jonas Valfridsson", "Alyssa Loo", "Alex Yakubovich", "Jamie Smith", "Tao Jiang", "Rich Munoz", "Gabriel Barcik", "Rishabh Bansal", "Mingyao Yang", "Yilun Du", "Pablo Duque", "Mary Phuong", "Alexandra Belias", "Kunal Lad", "Zeyu Liu", "Tal Schuster", "Karthik Duddu", "Jieru Hu", "Paige Kunkle", "Matthew Watson", "Jackson Tolins", "Josh Smith", "Denis Teplyashin", "Garrett Bingham", "Marvin Ritter", "Marco Andreetto", "Divya Pitta", "Mohak Patel", "Shashank Viswanadha", "Trevor Strohman", "Catalin Ionescu", "Jincheng Luo", "Yogesh Kalley", "Jeremy Wiesner", "Dan Deutsch", "Derek Lockhart", "Peter Choy", "Rumen Dangovski", "Chawin Sitawarin", "Cat Graves", "Tanya Lando", "Joost van Amersfoort", "Ndidi Elue", "Zhouyuan Huo", "Pooya Moradi", "Jean Tarbouriech", "Henryk Michalewski", "Wenting Ye", "Eunyoung Kim", "Alex Druinsky", "Florent Altch\u00e9", "Xinyi Chen", "Artur Dwornik", "Da-Cheng Juan", "Rivka Moroshko", "Horia Toma", "Jarrod Kahn", "Hai Qian", "Maximilian Sieb", "Irene Cai", "Roman Goldenberg", "Praneeth Netrapalli", "Sindhu Raghuram", "Yuan Gong", "Lijie Fan", "Evan Palmer", "Yossi Matias", "Valentin Gabeur", "Shreya Pathak", "Tom Ouyang", "Don Metzler", "Geoff Bacon", "Srinivasan Venkatachary", "Sridhar Thiagarajan", "Alex Cullum", "Eran Ofek", "Vytenis Sakenas", "Mohamed Hammad", "Cesar Magalhaes", "Mayank Daswani", "Oscar Chang", "Ashok Popat", "Ruichao Li", "Komal Jalan", "Yanhan Hou", "Josh Lipschultz", "Antoine He", "Wenhao Jia", "Pier Giuseppe Sessa", "Prateek Kolhar", "William Wong", "Sumeet Singh", "Lukas Haas", "Jay Whang", "Hanna Klimczak-Pluci\u0144ska", "Georges Rotival", "Grace Chung", "Yiqing Hua", "Anfal Siddiqui", "Nicolas Serrano", "Dongkai Chen", "Billy Porter", "Libin Bai", "Keshav Shivam", "Sho Arora", "Partha Talukdar", "Tom Cobley", "Sangnie Bhardwaj", "Evgeny Gladchenko", "Simon Green", "Kelvin Guu", "Felix Fischer", "Xiao Wu", "Eric Wang", "Achintya Singhal", "Tatiana Matejovicova", "James Martens", "Hongji Li", "Roma Patel", "Elizabeth Kemp", "Jiaqi Pan", "Lily Wang", "Blake JianHang Chen", "Jean-Baptiste Alayrac", "Navneet Potti", "Erika Gemzer", "Eugene Ie", "Kay McKinney", "Takaaki Saeki", "Edward Chou", "Pascal Lamblin", "SQ Mah", "Zach Fisher", "Martin Chadwick", "Jon Stritar", "Obaid Sarvana", "Andrew Hogue", "Artem Shtefan", "Hadi Hashemi", "Yang Xu", "Jindong Gu", "Sharad Vikram", "Chung-Ching Chang", "Sabela Ramos", "Logan Kilpatrick", "Weijuan Xi", "Jenny Brennan", "Yinghao Sun", "Abhishek Jindal", "Ionel Gog", "Dawn Chen", "Felix Wu", "Jason Lee", "Sudhindra Kopalle", "Srinadh Bhojanapalli", "Oriol Vinyals", "Natan Potikha", "Burcu Karagol Ayan", "Yuan Yuan", "Michael Riley", "Piotr Stanczyk", "Sergey Kishchenko", "Bing Wang", "Dan Garrette", "Antoine Yang", "Vlad Feinberg", "CJ Carey", "Javad Azizi", "Viral Shah", "Erica Moreira", "Chongyang Shi", "Josh Feldman", "Elizabeth Salesky", "Thomas Lampe", "Aneesh Pappu", "Duhyeon Kim", "Jonas Adler", "Avi Caciularu", "Brian Walker", "Yunhan Xu", "Yochai Blau", "Dylan Scandinaro", "Terry Huang", "Sam El-Husseini", "Abhishek Sinha", "Lijie Ren", "Taylor Tobin", "Patrik Sundberg", "Tim Sohn", "Vikas Yadav", "Mimi Ly", "Emily Xue", "Jing Xiong", "Afzal Shama Soudagar", "Sneha Mondal", "Nikhil Khadke", "Qingchun Ren", "Ben Vargas", "Stan Bileschi", "Sarah Chakera", "Cindy Wang", "Boyu Wang", "Yoni Halpern", "Joe Jiang", "Vikas Sindhwani", "Petre Petrov", "Pranavaraj Ponnuramu", "Sanket Vaibhav Mehta", "Yu Watanabe", "Betty Chan", "Matheus Wisniewski", "Trang Pham", "Jingwei Zhang", "Conglong Li", "Dario de Cesare", "Art Khurshudov", "Alex Vasiloff", "Melissa Tan", "Zoe Ashwood", "Bobak Shahriari", "Maryam Majzoubi", "Garrett Tanzer", "Olga Kozlova", "Robin Alazard", "James Lee-Thorp", "Nguyet Minh Phu", "Isaac Tian", "Junwhan Ahn", "Andy Crawford", "Lauren Lax", "Yuan", "Shangguan", "Iftekhar Naim", "David Ross", "Oleksandr Ferludin", "Tongfei Guo", "Andrea Banino", "Hubert Soyer", "Xiaoen Ju", "Dominika Rogozi\u0144ska", "Ishaan Malhi", "Marcella Valentine", "Daniel Balle", "Apoorv Kulshreshtha", "Maciej Kula", "Yiwen Song", "Sophia Austin", "John Schultz", "Roy Hirsch", "Arthur Douillard", "Apoorv Reddy", "Michael Fink", "Summer Yue", "Khyatti Gupta", "Adam Zhang", "Norman Rink", "Daniel McDuff", "Lei Meng", "Andr\u00e1s Gy\u00f6rgy", "Yasaman Razeghi", "Ricky Liang", "Kazuki Osawa", "Aviel Atias", "Matan Eyal", "Tyrone Hill", "Nikolai Grigorev", "Zhengdong Wang", "Nitish Kulkarni", "Rachel Soh", "Ivan Lobov", "Zachary Charles", "Sid Lall", "Kazuma Hashimoto", "Ido Kessler", "Victor Gomes", "Zelda Mariet", "Danny Driess", "Alessandro Agostini", "Canfer Akbulut", "Jingcao Hu", "Marissa Ikonomidis", "Emily Caveness", "Kartik Audhkhasi", "Saurabh Agrawal", "Ioana Bica", "Evan Senter", "Jayaram Mudigonda", "Kelly Chen", "Jingchen Ye", "Xuanhui Wang", "James Svensson", "Philipp Fr\u00e4nken", "Josh Newlan", "Li Lao", "Eva Schnider", "Sami Alabed", "Joseph Kready", "Jesse Emond", "Afief Halumi", "Tim Zaman", "Chengxi Ye", "Naina Raisinghani", "Vilobh Meshram", "Bo Chang", "Ankit Singh Rawat", "Axel Stjerngren", "Sergey Levi", "Rui Wang", "Xiangzhu Long", "Mitchelle Rasquinha", "Steven Hand", "Aditi Mavalankar", "Lauren Agubuzu", "Sudeshna Roy", "Junquan Chen", "Jarek Wilkiewicz", "Hao Zhou", "Michal Jastrzebski", "Qiong Hu", "Agustin Dal Lago", "Ramya Sree Boppana", "Wei-Jen Ko", "Jennifer Prendki", "Yao Su", "Zhi Li", "Eliza Rutherford", "Girish Ramchandra Rao", "Ramona Comanescu", "Adri\u00e0 Puigdom\u00e8nech", "Qihang Chen", "Dessie Petrova", "Christine Chan", "Vedrana Milutinovic", "Felipe Tiengo Ferreira", "Chin-Yi Cheng", "Ming Zhang", "Tapomay Dey", "Sherry Yang", "Ramesh Sampath", "Quoc Le", "Howard Zhou", "Chu-Cheng Lin", "Hoi Lam", "Christine Kaeser-Chen", "Kai Hui", "Dean Hirsch", "Tom Eccles", "Basil Mustafa", "Shruti Rijhwani", "Morgane Rivi\u00e8re", "Yuanzhong Xu", "Junjie Wang", "Xinyang Geng", "Xiance Si", "Arjun Khare", "Cheolmin Kim", "Vahab Mirrokni", "Kamyu Lee", "Khuslen Baatarsukh", "Nathaniel Braun", "Lisa Wang", "Pallavi LV", "Richard Tanburn", "Yuvein", "Zhu", "Fangda Li", "Setareh Ariafar", "Dan Goldberg", "Ken Burke", "Daniil Mirylenka", "Meiqi Guo", "Olaf Ronneberger", "Hadas Natalie Vogel", "Liqun Cheng", "Nishita Shetty", "Johnson Jia", "Thomas Jimma", "Corey Fry", "Ted Xiao", "Martin Sundermeyer", "Ryan Burnell", "Yannis Assael", "Mario Pinto", "JD Chen", "Rohit Sathyanarayana", "Donghyun Cho", "Jing Lu", "Rishabh Agarwal", "Sugato Basu", "Lucas Gonzalez", "Dhruv Shah", "Meng Wei", "Dre Mahaarachchi", "Rohan Agrawal", "Tero Rissa", "Yani Donchev", "Ramiro Leal-Cavazos", "Adrian Hutter", "Markus Mircea", "Alon Jacovi", "Faruk Ahmed", "Jiageng Zhang", "Shuguang Hu", "Bo-Juen Chen", "Jonni Kanerva", "Guillaume Desjardins", "Andrew Lee", "Nikos Parotsidis", "Asier Mujika", "Tobias Weyand", "Jasper Snoek", "Jo Chick", "Kai Chen", "Paul Chang", "Ethan Mahintorabi", "Zi Wang", "Tolly Powell", "Orgad Keller", "Abhirut Gupta", "Claire Sha", "Kanav Garg", "Nicolas Heess", "\u00c1goston Weisz", "Cassidy Hardin", "Bartek Wydrowski", "Ben Coleman", "Karina Zainullina", "Pankaj Joshi", "Alessandro Epasto", "Terry Spitz", "Binbin Xiong", "Kai Zhao", "Arseniy Klimovskiy", "Ivy Zheng", "Johan Ferret", "Itay Yona", "Waleed Khawaja", "Jean-Baptiste Lespiau", "Maxim Krikun", "Siamak Shakeri", "Timothee Cour", "Bonnie Li", "Igor Krivokon", "Dan Suh", "Alex Hofer", "Jad Al Abdallah", "Nikita Putikhin", "Oscar Akerlund", "Silvio Lattanzi", "Anurag Kumar", "Shane Settle", "Himanshu Srivastava", "Folawiyo Campbell-Ajala", "Edouard Rosseel", "Mihai Dorin Istin", "Nishanth Dikkala", "Anand Rao", "Nick Young", "Kate Lin", "Dhruva Bhaswar", "Yiming Wang", "Jaume Sanchez Elias", "Kritika Muralidharan", "James Keeling", "Dayou Du", "Siddharth Gopal", "Gregory Dibb", "Charles Blundell", "Manolis Delakis", "Jacky Liang", "Marco Tulio Ribeiro", "Georgi Karadzhov", "Guillermo Garrido", "Ankur Bapna", "Jiawei Cao", "Adam Sadovsky", "Pouya Tafti", "Arthur Guez", "Coline Devin", "Yixian Di", "Jinwei Xing", "Chuqiao", "Xu", "Hanzhao Lin", "Chun-Te Chu", "Sameera Ponda", "Wesley Helmholz", "Fan Yang", "Yue Gao", "Sara Javanmardi", "Wael Farhan", "Alex Ramirez", "Ricardo Figueira", "Khe Chai Sim", "Yuval Bahat", "Ashwin Vaswani", "Liangzhe Yuan", "Gufeng Zhang", "Leland Rechis", "Hanjun Dai", "Tayo Oguntebi", "Alexandra Cordell", "Eug\u00e9nie Rives", "Kaan Tekelioglu", "Naveen Kumar", "Bing Zhang", "Aurick Zhou", "Nikolay Savinov", "Andrew Leach", "Alex Tudor", "Sanjay Ganapathy", "Yanyan Zheng", "Mirko Rossini", "Vera Axelrod", "Arnaud Autef", "Yukun Zhu", "Zheng Zheng", "Mingda Zhang", "Baochen Sun", "Jie Ren", "Nenad Tomasev", "Nithish Kannan", "Amer Sinha", "Charles Chen", "Louis O'Bryan", "Alex Pak", "Aditya Kusupati", "Weel Yang", "Deepak Ramachandran", "Patrick Griffin", "Seokhwan Kim", "Philipp Neubeck", "Craig Schiff", "Tammo Spalink", "Mingyang Ling", "Arun Nair", "Ga-Young Joung", "Linda Deng", "Avishkar Bhoopchand", "Lora Aroyo", "Tom Duerig", "Jordan Griffith", "Gabe Barth-Maron", "Jake Ades", "Alex Haig", "Ankur Taly", "Yunting Song", "Paul Michel", "Dave Orr", "Dean Weesner", "Corentin Tallec", "Carrie Grimes Bostock", "Paul Niemczyk", "Andy Twigg", "Mudit Verma", "Rohith Vallu", "Henry Wang", "Marco Gelmi", "Kiranbir Sodhia", "Aleksandr Chuklin", "Omer Goldman", "Jasmine George", "Liang Bai", "Kelvin Zhang", "Petar Sirkovic", "Efrat Nehoran", "Golan Pundak", "Jiaqi Mu", "Alice Chen", "Alex Greve", "Paulo Zacchello", "David Amos", "Heming Ge", "Eric Noland", "Colton Bishop", "Jeffrey Dudek", "Youhei Namiki", "Elena Buchatskaya", "Jing Li", "Dorsa Sadigh", "Masha Samsikova", "Dan Malkin", "Damien Vincent", "Robert David", "Rob Willoughby", "Phoenix Meadowlark", "Shawn Gao", "Yan Li", "Raj Apte", "Amit Jhindal", "Stein Xudong Lin", "Alex Polozov", "Zhicheng Wang", "Tomas Mery", "Anirudh GP", "Varun Yerram", "Sage Stevens", "Tianqi Liu", "Noah Fiedel", "Charles Sutton", "Matthew Johnson", "Xiaodan Song", "Kate Baumli", "Nir Shabat", "Muqthar Mohammad", "Hao Liu", "Marco Selvi", "Yichao Zhou", "Mehdi Hafezi Manshadi", "Chu-ling Ko", "Anthony Chen", "Michael Bendersky", "Jorge Gonzalez Mendez", "Nisarg Kothari", "Amir Zandieh", "Yiling Huang", "Daniel Andor", "Ellie Pavlick", "Idan Brusilovsky", "Jitendra Harlalka", "Sally Goldman", "Andrew Lampinen", "Guowang Li", "Asahi Ushio", "Somit Gupta", "Lei Zhang", "Chuyuan Kelly Fu", "Madhavi Sewak", "Timo Denk", "Jed Borovik", "Brendan Jou", "Avital Zipori", "Prateek Jain", "Junwen Bai", "Thang Luong", "Jonathan Tompson", "Alice Li", "Li Liu", "George Powell", "Jiajun Shen", "Alex Feng", "Grishma Chole", "Da Yu", "Yinlam Chow", "Tongxin Yin", "Eric Malmi", "Kefan Xiao", "Yash Pande", "Shachi Paul", "Niccol\u00f2 Dal Santo", "Adil Dostmohamed", "Sergio Guadarrama", "Aaron Phillips", "Thanumalayan Sankaranarayana Pillai", "Gal Yona", "Amin Ghafouri", "Preethi Lahoti", "Benjamin Lee", "Dhruv Madeka", "Eren Sezener", "Simon Tokumine", "Adrian Collister", "Nicola De Cao", "Richard Shin", "Uday Kalra", "Parker Beak", "Emily Nottage", "Ryo Nakashima", "Ivan Jurin", "Vikash Sehwag", "Meenu Gaba", "Junhao Zeng", "Kevin R. McKee", "Fernando Pereira", "Tamar Yakar", "Amayika Panda", "Arka Dhar", "Peilin Zhong", "Daniel Sohn", "Mark Brand", "Lars Lowe Sjoesund", "Viral Carpenter", "Sharon Lin", "Shantanu Thakoor", "Marcus Wainwright", "Ashwin Chaugule", "Pranesh Srinivasan", "Muye Zhu", "Bernett Orlando", "Jack Weber", "Ayzaan Wahid", "Gilles Baechler", "Apurv Suman", "Jovana Mitrovi\u0107", "Gabe Taubman", "Honglin Yu", "Helen King", "Josh Dillon", "Cathy Yip", "Dhriti Varma", "Tomas Izo", "Levent Bolelli", "Borja De Balle Pigem", "Julia Di Trapani", "Fotis Iliopoulos", "Adam Paszke", "Nishant Ranka", "Joe Zou", "Francesco Pongetti", "Jed McGiffin", "Alex Siegman", "Rich Galt", "Ross Hemsley", "Goran \u017du\u017ei\u0107", "Victor Carbune", "Tao Li", "Myle Ott", "F\u00e9lix de Chaumont Quitry", "David Vilar Torres", "Yuri Chervonyi", "Tomy Tsai", "Prem Eruvbetine", "Samuel Yang", "Matthew Denton", "Jake Walker", "Slavica Anda\u010di\u0107", "Idan Heimlich Shtacher", "Vittal Premachandran", "Harshal Tushar Lehri", "Cip Baetu", "Damion Yates", "Lampros Lamprou", "Mariko Iinuma", "Ioana Mihailescu", "Ben Albrecht", "Shachi Dave", "Susie Sargsyan", "Bryan Perozzi", "Lucas Manning", "Chiyuan Zhang", "Denis Vnukov", "Igor Mordatch", "Raia Hadsell Wolfgang Macherey", "Ryan Kappedal", "Jim Stephan", "Aditya Tripathi", "Klaus Macherey", "Jun Qian", "Abhishek Bhowmick", "Shekoofeh Azizi", "R\u00e9mi Leblond", "Shiva Mohan Reddy Garlapati", "Timothy Knight", "Matthew Wiethoff", "Wei-Chih Hung", "Anelia Angelova", "Georgios Evangelopoulos", "Pawel Janus", "Dimitris Paparas", "Matthew Rahtz", "Ken Caluwaerts", "Vivek Sampathkumar", "Daniel Jarrett", "Shadi Noghabi", "Antoine Miech", "Chak Yeung", "Geoff Clark", "Henry Prior", "Fei Zheng", "Jean Pouget-Abadie", "Indro Bhattacharya", "Kalpesh Krishna", "Will Bishop", "Zhe Yuan", "Yunxiao Deng", "Ashutosh Sathe", "Kacper Krasowiak", "Ciprian Chelba", "Cho-Jui Hsieh", "Kiran Vodrahalli", "Buhuang Liu", "Thomas K\u00f6ppe", "Amr Khalifa", "Lubo Litchev", "Pichi Charoenpanit", "Reed Roberts", "Sachin Yadav", "Yasumasa Onoe", "Desi Ivanov", "Megha Mohabey", "Vighnesh Birodkar", "Nemanja Raki\u0107evi\u0107", "Pierre Sermanet", "Vaibhav Mehta", "Krishan Subudhi", "Travis Choma", "Will Ng", "Luheng He", "Kathie Wang", "Tasos Kementsietsidis", "Shane Gu", "Mansi Gupta", "Andrew Nystrom", "Mehran Kazemi", "Timothy Chung", "Nacho Cano", "Nikhil Dhawan", "Yufei Wang", "Jiawei Xia", "Trevor Yacovone", "Eric Jia", "Mingqing Chen", "Simeon Ivanov", "Ashrith Sheshan", "Sid Dalmia", "Pawe\u0142 Stradomski", "Pengcheng Yin", "Salem Haykal", "Congchao Wang", "Dennis Duan", "Neslihan Bulut", "Greg Kochanski", "Liam MacDermed", "Namrata Godbole", "Shitao Weng", "Jingjing Chen", "Rachana Fellinger", "Ramin Mehran", "Daniel Suo", "Hisham Husain", "Tong He", "Kaushal Patel", "Joshua Howland", "Randall Parker", "Kelvin Nguyen", "Sharath Maddineni", "Chris Rawles", "Mina Khan", "Shlomi Cohen-Ganor", "Amol Mandhane", "Xinyi Wu", "Chenkai Kuang", "Iulia Com\u015fa", "Ramya Ganeshan", "Hanie Sedghi", "Adam Bloniarz", "Nuo Wang Pierse", "Anton Briukhov", "Petr Mitrichev", "Anita Gergely", "Serena Zhan", "Allan Zhou", "Nikita Saxena", "Eva Lu", "Josef Dean", "Ashish Gupta", "Nicolas Perez-Nieves", "Renjie Wu", "Cory McLean", "Wei Liang", "Disha Jindal", "Anton Tsitsulin", "Wenhao Yu", "Kaiz Alarakyia", "Tom Schaul", "Piyush Patil", "Peter Sung", "Elijah Peake", "Hongkun Yu", "Feryal Behbahani", "JD Co-Reyes", "Alan Ansell", "Sean Sun", "Clara Barbu", "Jonathan Lee", "Seb Noury", "James Allingham", "Bilal Piot", "Mohit Sharma", "Christopher Yew", "Ivan Korotkov", "Bibo Xu", "Demetra Brady", "Goran Petrovic", "Shibl Mourad", "Claire Cui", "Aditya Gupta", "Parker Schuh", "Saarthak Khanna", "Anna Goldie", "Abhinav Arora", "Vadim Zubov", "Amy Stuart", "Mark Epstein", "Yun Zhu", "Jianqiao Liu", "Yury Stuken", "Ziyue Wang", "Karolis Misiunas", "Dee Guo", "Ashleah Gill", "Ale Hartman", "Zaid Nabulsi", "Aurko Roy", "Aleksandra Faust", "Jason Riesa", "Ben Withbroe", "Mengchao Wang", "Marco Tagliasacchi", "Andreea Marzoca", "James Noraky", "Serge Toropov", "Malika Mehrotra", "Bahram Raad", "Sanja Deur", "Steve Xu", "Marianne Monteiro", "Zhongru Wu", "Yi Luan", "Sam Ritter", "Nick Li", "H\u00e5vard Garnes", "Yanzhang He", "Martin Zlocha", "Jifan Zhu", "Matteo Hessel", "Will Wu", "Spandana Raj Babbula", "Chizu Kawamoto", "Yuanzhen Li", "Mehadi Hassen", "Yan Wang", "Brian Wieder", "James Freedman", "Yin Zhang", "Xinyi Bai", "Tianli Yu", "David Reitter", "XiangHai Sheng", "Mateo Wirth", "Aditya Kini", "Dima Damen", "Mingcen Gao", "Rachel Hornung", "Michael Voznesensky", "Brian Roark", "Adhi Kuncoro", "Yuxiang Zhou", "Rushin Shah", "Anthony Brohan", "Kuangyuan Chen", "James Wendt", "David Rim", "Paul Kishan Rubenstein", "Jonathan Halcrow", "Michelle Liu", "Ty Geri", "Yunhsuan Sung", "Jane Shapiro", "Shaan Bijwadia", "Chris Duvarney", "Christina Sorokin", "Paul Natsev", "Reeve Ingle", "Pramod Gupta", "Young Maeng", "Ndaba Ndebele", "Kexin Zhu", "Valentin Anklin", "Katherine Lee", "Yuan Liu", "Yaroslav Akulov", "Shaleen Gupta", "Guolong Su", "Flavien Prost", "Tianlin Liu", "Vitaly Kovalev", "Pol Moreno", "Martin Scholz", "Sam Redmond", "Zongwei Zhou", "Alex Castro-Ros", "Andr\u00e9 Susano Pinto", "Dia Kharrat", "Michal Yarom", "Rachel Saputro", "Jannis Bulian", "Ben Caine", "Ji Liu", "Abbas Abdolmaleki", "Shariq Iqbal", "Tautvydas Misiunas", "Mikhail Sirotenko", "Shefali Garg", "Guy Bensky", "Huan Gui", "Xuezhi Wang", "Raphael Koster", "Mike Bernico", "Da Huang", "Romal Thoppilan", "Trevor Cohn", "Ben Golan", "Wenlei Zhou", "Andrew Rosenberg", "Markus Freitag", "Tynan Gangwani", "Vincent Tsang", "Anand Shukla", "Xiaoqi Ren", "Minh Giang", "Chi Zou", "Andre Elisseeff", "Charline Le Lan", "Dheeru Dua", "Shuba Lall", "Pranav Shyam", "Frankie Garcia", "Sarah Nguyen", "Michael Guzman", "AJ Maschinot", "Marcello Maggioni", "Ming-Wei Chang", "Karol Gregor", "Lotte Weerts", "Kumaran Venkatesan", "Bogdan Damoc", "Leon Liu", "Jan Wassenberg", "Lewis Ho", "Becca Roelofs", "Majid Hadian", "Fran\u00e7ois-Xavier Aubet", "Yu Liang", "Sami Lachgar", "Danny Karmon", "Yong Cheng", "Amelio V\u00e1zquez-Reina", "Angie Chen", "Zhuyun Dai", "Andy Brock", "Shubham Agrawal", "Chenxi Pang", "Peter Garst", "Mariella Sanchez-Vargas", "Ivor Rendulic", "Aditya Ayyar", "Andrija Ra\u017enatovi\u0107", "Olivia Ma", "Roopali Vij", "Neha Sharma", "Ashwin Balakrishna", "Bingyuan Liu", "Ian Mackinnon", "Sorin Baltateanu", "Petra Poklukar", "Gabriel Ibagon", "Colin Ji", "Hongyang Jiao", "Isaac Noble", "Wojciech Stokowiec", "Zhihao Li", "Jeff Dean", "David Lindner", "Mark Omernick", "Kristen Chiafullo", "Mason Dimarco", "Vitor Rodrigues", "Vittorio Selo", "Garrett Honke", "Xintian", "Wu", "Wei He", "Adam Hillier", "Anhad Mohananey", "Vihari Piratla", "Chang Ye", "Chase Malik", "Sebastian Riedel", "Samuel Albanie", "Zi Yang", "Kenny Vassigh", "Maria Bauza", "Sheng Li", "Yiqing Tao", "Nevan Wichers", "Andrii Maksai", "Abe Ittycheriah", "Ross Mcilroy", "Bryan Seybold", "Noah Goodman", "Romina Datta", "Steven M. Hernandez", "Tian Shi", "Yony Kochinski", "Anna Bulanova", "Ken Franko", "Mikita Sazanovich", "Nicholas FitzGerald", "Praneeth Kacham", "Shubha Srinivas Raghvendra", "Vincent Hellendoorn", "Alexander Grushetsky", "Julian Salazar", "Angeliki Lazaridou", "Jason Chang", "Jan-Thorsten Peter", "Sushant Kafle", "Yann Dauphin", "Abhishek Rao", "Filippo Graziano", "Izhak Shafran", "Yuguo Liao", "Tianli Ding", "Geng Yan", "Grace Chu", "Zhao Fu", "Vincent Roulet", "Gabriel Rasskin", "Duncan Williams", "Shahar Drath", "Alex Mossin", "Raphael Hoffmann", "Jordi Orbay", "Francesco Bertolini", "Hila Sheftel", "Justin Chiu", "Siyang Xue", "Yuheng Kuang", "Ferjad Naeem", "Swaroop Nath", "Nana Nti", "Phil Culliton", "Kashyap Krishnakumar", "Michael Isard", "Pei Sun", "Ayan Chakrabarti", "Nathan Clement", "Regev Cohen", "Arissa Wongpanich", "GS Oh", "Ashwin Murthy", "Hao Zheng", "Jessica Hamrick", "Oskar Bunyan", "Suhas Ganesh", "Nitish Gupta", "Roy Frostig", "John Wieting", "Yury Malkov", "Pierre Marcenac", "Zhixin", "Lai", "Xiaodan Tang", "Mohammad Saleh", "Fedir Zubach", "Chinmay Kulkarni", "Huanjie Zhou", "Vicky Zayats", "Nan Ding", "Anshuman Tripathi", "Arijit Pramanik", "Patrik Zochbauer", "Harish Ganapathy", "Vedant Misra", "Zach Behrman", "Hugo Vallet", "Mingyang Zhang", "Mukund Sridhar", "Ye Jin", "Mohammad Babaeizadeh", "Siim P\u00f5der", "Megha Goel", "Divya Jain", "Tajwar Nasir", "Shubham Mittal", "Tim Dozat", "Diego Ardila", "Aliaksei Severyn", "Fabio Pardo", "Sammy Jerome", "Siyang Qin", "Louis Rouillard", "Amir Yazdanbakhsh", "Zizhao Zhang", "Shivani Agrawal", "Kaushik Shivakumar", "Caden Lu", "Praveen Kallakuri", "Rachita Chhaparia", "Kanishka Rao", "Charles Kwong", "Asya Fadeeva", "Shitij Nigam", "Yan Virin", "Yuan Zhang", "Balaji Venkatraman", "Beliz Gunel", "Marc Wilson", "Huiyu Wang", "Abhinav Gupta", "Xiaowei Xu", "Adrien Ali Ta\u00efga", "Kareem Mohamed", "Doug Fritz", "Daniel Rodriguez", "Zoubin Ghahramani", "Harry Askham", "Lior Belenki", "James Zhao", "Rahul Gupta", "Krzysztof Jastrz\u0119bski", "Takahiro Kosakai", "Kaan Katircioglu", "Jon Schneider", "Rina Panigrahy", "Konstantinos Bousmalis", "Peter Grabowski", "Prajit Ramachandran", "Chaitra Hegde", "Mihaela Rosca", "Angelo Scorza Scarpati", "Kyriakos Axiotis", "Ying Xu", "Zach Gleicher", "Assaf Hurwitz Michaely", "Mandar Sharma", "Sanil Jain", "Christoph Hirnschall", "Tal Marian", "Xuhui Jia", "Kevin Mather", "Kilol Gupta", "Linhai Qiu", "Nigamaa Nayakanti", "Lucian Ionita", "Steven Zheng", "Lucia Loher", "Kurt Shuster", "Igor Petrovski", "Roshan Sharma", "Rahma Chaabouni", "Angel Yeh", "James An", "Arushi Gupta", "Steven Schwarcz", "Seher Ellis", "Sam Conway-Rahman", "Javier Snaider", "Alex Zhai", "James Atwood", "Daniel Golovin", "Liqian Peng", "Te I", "Vivian Xia", "Salvatore Scellato", "Mahan Malihi", "Arthur Bra\u017einskas", "Vlad-Doru Ion", "Younghoon Jun", "James Swirhun", "Soroosh Mariooryad", "Jiao Sun", "Steve Chien", "Rey Coaguila", "Ariel Brand", "Yi Gao", "Tom Kwiatkowski", "Roee Aharoni", "Cheng-Chun Lee", "Mislav \u017dani\u0107", "Yichi Zhang", "Dan Ethier", "Vitaly Nikolaev", "Pranav Nair", "Yoav Ben Shalom", "Hen Fitoussi", "Jai Gupta", "Hongbin Liu", "Dee Cattle", "Tolga Bolukbasi", "Ben Murdoch", "Fantine Huot", "Yin Li", "Chris Hahn"], "title": "Gemini 2.5: Pushing the Frontier with Advanced Reasoning, Multimodality, Long Context, and Next Generation Agentic Capabilities", "categories": ["cs.CL", "cs.AI"], "comment": "72 pages, 17 figures", "summary": "In this report, we introduce the Gemini 2.X model family: Gemini 2.5 Pro and\nGemini 2.5 Flash, as well as our earlier Gemini 2.0 Flash and Flash-Lite\nmodels. Gemini 2.5 Pro is our most capable model yet, achieving SoTA\nperformance on frontier coding and reasoning benchmarks. In addition to its\nincredible coding and reasoning skills, Gemini 2.5 Pro is a thinking model that\nexcels at multimodal understanding and it is now able to process up to 3 hours\nof video content. Its unique combination of long context, multimodal and\nreasoning capabilities can be combined to unlock new agentic workflows. Gemini\n2.5 Flash provides excellent reasoning abilities at a fraction of the compute\nand latency requirements and Gemini 2.0 Flash and Flash-Lite provide high\nperformance at low latency and cost. Taken together, the Gemini 2.X model\ngeneration spans the full Pareto frontier of model capability vs cost, allowing\nusers to explore the boundaries of what is possible with complex agentic\nproblem solving.", "AI": {"tldr": "\u4ecb\u7ecd\u4e86Gemini 2.X\u6a21\u578b\u5bb6\u65cf\uff0c\u5305\u62ecGemini 2.5 Pro\u548c2.5 Flash\uff0c\u4ee5\u53ca\u65e9\u671f\u76842.0 Flash\u548cFlash-Lite\u6a21\u578b\uff0c\u5c55\u793a\u4e86\u5176\u5728\u7f16\u7801\u3001\u63a8\u7406\u548c\u591a\u6a21\u6001\u7406\u89e3\u65b9\u9762\u7684\u5353\u8d8a\u6027\u80fd\u3002", "motivation": "\u63a8\u52a8\u524d\u6cbf\u7f16\u7801\u548c\u63a8\u7406\u57fa\u51c6\u7684\u6027\u80fd\uff0c\u63d0\u4f9b\u591a\u6a21\u6001\u7406\u89e3\u548c\u957f\u4e0a\u4e0b\u6587\u5904\u7406\u80fd\u529b\uff0c\u4ee5\u652f\u6301\u590d\u6742\u7684\u4ee3\u7406\u5de5\u4f5c\u6d41\u7a0b\u3002", "method": "\u5f00\u53d1\u4e86Gemini 2.5 Pro\u548c2.5 Flash\u6a21\u578b\uff0c\u5206\u522b\u4f18\u5316\u4e86\u6027\u80fd\u548c\u8ba1\u7b97\u6548\u7387\uff0c\u540c\u65f6\u4fdd\u7559\u4e86\u591a\u6a21\u6001\u548c\u957f\u4e0a\u4e0b\u6587\u5904\u7406\u80fd\u529b\u3002", "result": "Gemini 2.5 Pro\u5728\u7f16\u7801\u548c\u63a8\u7406\u57fa\u51c6\u4e0a\u8fbe\u5230SoTA\u6027\u80fd\uff0c\u5e76\u80fd\u5904\u7406\u957f\u8fbe3\u5c0f\u65f6\u7684\u89c6\u9891\u5185\u5bb9\uff1b2.5 Flash\u5728\u4f4e\u8ba1\u7b97\u548c\u5ef6\u8fdf\u4e0b\u63d0\u4f9b\u4f18\u79c0\u63a8\u7406\u80fd\u529b\u3002", "conclusion": "Gemini 2.X\u6a21\u578b\u5bb6\u65cf\u5728\u6027\u80fd\u548c\u6210\u672c\u4e4b\u95f4\u5b9e\u73b0\u4e86\u5e15\u7d2f\u6258\u6700\u4f18\uff0c\u4e3a\u590d\u6742\u4ee3\u7406\u95ee\u9898\u89e3\u51b3\u63d0\u4f9b\u4e86\u5e7f\u6cdb\u9009\u62e9\u3002"}}
{"id": "2507.06267", "pdf": "https://arxiv.org/pdf/2507.06267", "abs": "https://arxiv.org/abs/2507.06267", "authors": ["Hyeontae Jo", "Kre\u0161imir Josi\u0107", "Jae Kyoung Kim"], "title": "Neural Network-Based Parameter Estimation for Non-Autonomous Differential Equations with Discontinuous Signals", "categories": ["cs.LG", "34C60, 92B05, 68T07, 93C15, 65K10"], "comment": null, "summary": "Non-autonomous differential equations are crucial for modeling systems\ninfluenced by external signals, yet fitting these models to data becomes\nparticularly challenging when the signals change abruptly. To address this\nproblem, we propose a novel parameter estimation method utilizing functional\napproximations with artificial neural networks. Our approach, termed Harmonic\nApproximation of Discontinuous External Signals using Neural Networks\n(HADES-NN), operates in two iterated stages. In the first stage, the algorithm\nemploys a neural network to approximate the discontinuous signal with a smooth\nfunction. In the second stage, it uses this smooth approximate signal to\nestimate model parameters. HADES-NN gives highly accurate and precise parameter\nestimates across various applications, including circadian clock systems\nregulated by external light inputs measured via wearable devices and the mating\nresponse of yeast to external pheromone signals. HADES-NN greatly extends the\nrange of model systems that can be fit to real-world measurements.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aHADES-NN\u7684\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u795e\u7ecf\u7f51\u7edc\u5e73\u6ed1\u8fd1\u4f3c\u4e0d\u8fde\u7eed\u4fe1\u53f7\uff0c\u7528\u4e8e\u975e\u81ea\u6cbb\u5fae\u5206\u65b9\u7a0b\u7684\u53c2\u6570\u4f30\u8ba1\u3002", "motivation": "\u975e\u81ea\u6cbb\u5fae\u5206\u65b9\u7a0b\u5728\u5efa\u6a21\u53d7\u5916\u90e8\u4fe1\u53f7\u5f71\u54cd\u7684\u7cfb\u7edf\u65f6\u975e\u5e38\u91cd\u8981\uff0c\u4f46\u5f53\u4fe1\u53f7\u7a81\u53d8\u65f6\uff0c\u6a21\u578b\u62df\u5408\u53d8\u5f97\u56f0\u96be\u3002", "method": "HADES-NN\u5206\u4e24\u9636\u6bb5\uff1a\u5148\u7528\u795e\u7ecf\u7f51\u7edc\u5e73\u6ed1\u8fd1\u4f3c\u4e0d\u8fde\u7eed\u4fe1\u53f7\uff0c\u518d\u7528\u8fd1\u4f3c\u4fe1\u53f7\u4f30\u8ba1\u6a21\u578b\u53c2\u6570\u3002", "result": "HADES-NN\u5728\u591a\u79cd\u5e94\u7528\u4e2d\u63d0\u4f9b\u4e86\u9ad8\u7cbe\u5ea6\u53c2\u6570\u4f30\u8ba1\uff0c\u5982\u663c\u591c\u8282\u5f8b\u7cfb\u7edf\u548c\u9175\u6bcd\u4ea4\u914d\u54cd\u5e94\u3002", "conclusion": "HADES-NN\u663e\u8457\u6269\u5c55\u4e86\u53ef\u62df\u5408\u5b9e\u9645\u6d4b\u91cf\u6570\u636e\u7684\u6a21\u578b\u8303\u56f4\u3002"}}
{"id": "2507.06234", "pdf": "https://arxiv.org/pdf/2507.06234", "abs": "https://arxiv.org/abs/2507.06234", "authors": ["Jiangzhong Cao", "Zekai Zeng", "Xu Zhang", "Huan Zhang", "Chunling Fan", "Gangyi Jiang", "Weisi Lin"], "title": "Unveiling the Underwater World: CLIP Perception Model-Guided Underwater Image Enhancement", "categories": ["cs.CV"], "comment": "10 pages, 7 figures;Accepted to PR 2025;The source code is available\n  at https://github.com/Ave001025/UIE_CLIP", "summary": "High-quality underwater images are essential for both machine vision tasks\nand viewers with their aesthetic appeal.However, the quality of underwater\nimages is severely affected by light absorption and scattering. Deep\nlearning-based methods for Underwater Image Enhancement (UIE) have achieved\ngood performance. However, these methods often overlook considering human\nperception and lack sufficient constraints within the solution space.\nConsequently, the enhanced images often suffer from diminished perceptual\nquality or poor content restoration.To address these issues, we propose a UIE\nmethod with a Contrastive Language-Image Pre-Training (CLIP) perception loss\nmodule and curriculum contrastive regularization. Above all, to develop a\nperception model for underwater images that more aligns with human visual\nperception, the visual semantic feature extraction capability of the CLIP model\nis leveraged to learn an appropriate prompt pair to map and evaluate the\nquality of underwater images. This CLIP perception model is then incorporated\nas a perception loss module into the enhancement network to improve the\nperceptual quality of enhanced images. Furthermore, the CLIP perception model\nis integrated with the curriculum contrastive regularization to enhance the\nconstraints imposed on the enhanced images within the CLIP perceptual space,\nmitigating the risk of both under-enhancement and over-enhancement.\nSpecifically, the CLIP perception model is employed to assess and categorize\nthe learning difficulty level of negatives in the regularization process,\nensuring comprehensive and nuanced utilization of distorted images and\nnegatives with varied quality levels. Extensive experiments demonstrate that\nour method outperforms state-of-the-art methods in terms of visual quality and\ngeneralization ability.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408CLIP\u611f\u77e5\u635f\u5931\u6a21\u5757\u548c\u8bfe\u7a0b\u5bf9\u6bd4\u6b63\u5219\u5316\u7684\u6c34\u4e0b\u56fe\u50cf\u589e\u5f3a\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u4e86\u56fe\u50cf\u7684\u611f\u77e5\u8d28\u91cf\u548c\u5185\u5bb9\u6062\u590d\u80fd\u529b\u3002", "motivation": "\u6c34\u4e0b\u56fe\u50cf\u8d28\u91cf\u53d7\u5149\u5438\u6536\u548c\u6563\u5c04\u5f71\u54cd\uff0c\u73b0\u6709\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u5ffd\u89c6\u4eba\u7c7b\u611f\u77e5\u4e14\u89e3\u7a7a\u95f4\u7ea6\u675f\u4e0d\u8db3\uff0c\u5bfc\u81f4\u589e\u5f3a\u540e\u56fe\u50cf\u611f\u77e5\u8d28\u91cf\u4e0b\u964d\u6216\u5185\u5bb9\u6062\u590d\u4e0d\u4f73\u3002", "method": "\u5229\u7528CLIP\u6a21\u578b\u7684\u89c6\u89c9\u8bed\u4e49\u7279\u5f81\u63d0\u53d6\u80fd\u529b\uff0c\u8bbe\u8ba1\u611f\u77e5\u635f\u5931\u6a21\u5757\uff1b\u7ed3\u5408\u8bfe\u7a0b\u5bf9\u6bd4\u6b63\u5219\u5316\uff0c\u589e\u5f3a\u89e3\u7a7a\u95f4\u7ea6\u675f\uff0c\u907f\u514d\u6b20\u589e\u5f3a\u6216\u8fc7\u589e\u5f3a\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u89c6\u89c9\u8d28\u91cf\u548c\u6cdb\u5316\u80fd\u529b\u4e0a\u4f18\u4e8e\u73b0\u6709\u6280\u672f\u3002", "conclusion": "\u7ed3\u5408CLIP\u611f\u77e5\u548c\u8bfe\u7a0b\u5bf9\u6bd4\u6b63\u5219\u5316\u80fd\u6709\u6548\u63d0\u5347\u6c34\u4e0b\u56fe\u50cf\u589e\u5f3a\u7684\u611f\u77e5\u8d28\u91cf\u548c\u5185\u5bb9\u6062\u590d\u6548\u679c\u3002"}}
{"id": "2507.06852", "pdf": "https://arxiv.org/pdf/2507.06852", "abs": "https://arxiv.org/abs/2507.06852", "authors": ["Uri Andrews", "Luca San Mauro"], "title": "SCC-recursiveness in infinite argumentation (extended version)", "categories": ["cs.AI"], "comment": "26 pages, accepted at JELIA 2025", "summary": "Argumentation frameworks (AFs) are a foundational tool in artificial\nintelligence for modeling structured reasoning and conflict. SCC-recursiveness\nis a well-known design principle in which the evaluation of arguments is\ndecomposed according to the strongly connected components (SCCs) of the attack\ngraph, proceeding recursively from \"higher\" to \"lower\" components. While\nSCC-recursive semantics such as \\cft and \\stgt have proven effective for finite\nAFs, Baumann and Spanring showed the failure of SCC-recursive semantics to\ngeneralize reliably to infinite AFs due to issues with well-foundedness.\n  We propose two approaches to extending SCC-recursiveness to the infinite\nsetting. We systematically evaluate these semantics using Baroni and Giacomin's\nestablished criteria, showing in particular that directionality fails in\ngeneral. We then examine these semantics' behavior in finitary frameworks,\nwhere we find some of our semantics satisfy directionality. These results\nadvance the theory of infinite argumentation and lay the groundwork for\nreasoning systems capable of handling unbounded or evolving domains.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e24\u79cd\u65b9\u6cd5\u5c06SCC\u9012\u5f52\u8bed\u4e49\u6269\u5c55\u5230\u65e0\u9650\u8bba\u8bc1\u6846\u67b6\uff0c\u5e76\u8bc4\u4f30\u5176\u6027\u80fd\uff0c\u53d1\u73b0\u65b9\u5411\u6027\u5728\u4e00\u822c\u60c5\u51b5\u4e0b\u5931\u6548\uff0c\u4f46\u5728\u6709\u9650\u6846\u67b6\u4e2d\u90e8\u5206\u8bed\u4e49\u6ee1\u8db3\u65b9\u5411\u6027\u3002", "motivation": "\u89e3\u51b3SCC\u9012\u5f52\u8bed\u4e49\u5728\u65e0\u9650\u8bba\u8bc1\u6846\u67b6\u4e2d\u7684\u5931\u6548\u95ee\u9898\uff0c\u63a8\u52a8\u65e0\u9650\u8bba\u8bc1\u7406\u8bba\u7684\u53d1\u5c55\u3002", "method": "\u63d0\u51fa\u4e24\u79cd\u6269\u5c55SCC\u9012\u5f52\u8bed\u4e49\u7684\u65b9\u6cd5\uff0c\u5e76\u57fa\u4e8eBaroni\u548cGiacomin\u7684\u6807\u51c6\u8fdb\u884c\u7cfb\u7edf\u8bc4\u4f30\u3002", "result": "\u65b9\u5411\u6027\u5728\u4e00\u822c\u60c5\u51b5\u4e0b\u5931\u6548\uff0c\u4f46\u5728\u6709\u9650\u6846\u67b6\u4e2d\u90e8\u5206\u8bed\u4e49\u6ee1\u8db3\u65b9\u5411\u6027\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u4e3a\u65e0\u9650\u8bba\u8bc1\u7406\u8bba\u63d0\u4f9b\u4e86\u57fa\u7840\uff0c\u652f\u6301\u5904\u7406\u65e0\u754c\u6216\u52a8\u6001\u9886\u57df\u7684\u63a8\u7406\u7cfb\u7edf\u3002"}}
{"id": "2507.06670", "pdf": "https://arxiv.org/pdf/2507.06670", "abs": "https://arxiv.org/abs/2507.06670", "authors": ["Wenxiang Guo", "Yu Zhang", "Changhao Pan", "Zhiyuan Zhu", "Ruiqi Li", "Zhetao Chen", "Wenhao Xu", "Fei Wu", "Zhou Zhao"], "title": "STARS: A Unified Framework for Singing Transcription, Alignment, and Refined Style Annotation", "categories": ["cs.SD", "eess.AS"], "comment": "9 pages, 2 figures", "summary": "Recent breakthroughs in singing voice synthesis (SVS) have heightened the\ndemand for high-quality annotated datasets, yet manual annotation remains\nprohibitively labor-intensive and resource-intensive. Existing automatic\nsinging annotation (ASA) methods, however, primarily tackle isolated aspects of\nthe annotation pipeline. To address this fundamental challenge, we present\nSTARS, which is, to our knowledge, the first unified framework that\nsimultaneously addresses singing transcription, alignment, and refined style\nannotation. Our framework delivers comprehensive multi-level annotations\nencompassing: (1) precise phoneme-audio alignment, (2) robust note\ntranscription and temporal localization, (3) expressive vocal technique\nidentification, and (4) global stylistic characterization including emotion and\npace. The proposed architecture employs hierarchical acoustic feature\nprocessing across frame, word, phoneme, note, and sentence levels. The novel\nnon-autoregressive local acoustic encoders enable structured hierarchical\nrepresentation learning. Experimental validation confirms the framework's\nsuperior performance across multiple evaluation dimensions compared to existing\nannotation approaches. Furthermore, applications in SVS training demonstrate\nthat models utilizing STARS-annotated data achieve significantly enhanced\nperceptual naturalness and precise style control. This work not only overcomes\ncritical scalability challenges in the creation of singing datasets but also\npioneers new methodologies for controllable singing voice synthesis. Audio\nsamples are available at https://gwx314.github.io/stars-demo/.", "AI": {"tldr": "STARS\u662f\u4e00\u4e2a\u7edf\u4e00\u7684\u6846\u67b6\uff0c\u9996\u6b21\u540c\u65f6\u89e3\u51b3\u6b4c\u5531\u8f6c\u5f55\u3001\u5bf9\u9f50\u548c\u98ce\u683c\u6807\u6ce8\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u6b4c\u5531\u6570\u636e\u96c6\u7684\u53ef\u6269\u5c55\u6027\u548c\u53ef\u63a7\u6027\u3002", "motivation": "\u624b\u52a8\u6807\u6ce8\u6b4c\u5531\u6570\u636e\u6210\u672c\u9ad8\u6602\uff0c\u73b0\u6709\u81ea\u52a8\u6807\u6ce8\u65b9\u6cd5\u4ec5\u89e3\u51b3\u90e8\u5206\u95ee\u9898\uff0c\u4e9f\u9700\u4e00\u4e2a\u7edf\u4e00\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "STARS\u91c7\u7528\u5206\u5c42\u58f0\u5b66\u7279\u5f81\u5904\u7406\uff0c\u7ed3\u5408\u975e\u81ea\u56de\u5f52\u5c40\u90e8\u58f0\u5b66\u7f16\u7801\u5668\uff0c\u5b9e\u73b0\u591a\u7ea7\u6807\u6ce8\uff08\u97f3\u7d20\u5bf9\u9f50\u3001\u97f3\u7b26\u8f6c\u5f55\u3001\u6f14\u5531\u6280\u5de7\u8bc6\u522b\u3001\u98ce\u683c\u63cf\u8ff0\uff09\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660eSTARS\u5728\u591a\u4e2a\u8bc4\u4f30\u7ef4\u5ea6\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u4e14\u5176\u6807\u6ce8\u6570\u636e\u663e\u8457\u63d0\u5347\u6b4c\u5531\u5408\u6210\u7684\u81ea\u7136\u6027\u548c\u98ce\u683c\u63a7\u5236\u3002", "conclusion": "STARS\u4e0d\u4ec5\u89e3\u51b3\u4e86\u6b4c\u5531\u6570\u636e\u96c6\u7684\u53ef\u6269\u5c55\u6027\u95ee\u9898\uff0c\u8fd8\u4e3a\u53ef\u63a7\u6b4c\u5531\u5408\u6210\u5f00\u8f9f\u4e86\u65b0\u65b9\u6cd5\u3002"}}
{"id": "2507.06717", "pdf": "https://arxiv.org/pdf/2507.06717", "abs": "https://arxiv.org/abs/2507.06717", "authors": ["Xuyang Chen", "Chong Huang", "Daquan Feng", "Lei Luo", "Yao Sun", "Xiang-Gen Xia"], "title": "QoE Optimization for Semantic Self-Correcting Video Transmission in Multi-UAV Networks", "categories": ["eess.IV", "cs.MM"], "comment": "13 pages", "summary": "Real-time unmanned aerial vehicle (UAV) video streaming is essential for\ntime-sensitive applications, including remote surveillance, emergency response,\nand environmental monitoring. However, it faces challenges such as limited\nbandwidth, latency fluctuations, and high packet loss. To address these issues,\nwe propose a novel semantic self-correcting video transmission framework with\nultra-fine bitrate granularity (SSCV-G). In SSCV-G, video frames are encoded\ninto a compact semantic codebook space, and the transmitter adaptively sends a\nsubset of semantic indices based on bandwidth availability, enabling\nfine-grained bitrate control for improved bandwidth efficiency. At the\nreceiver, a spatio-temporal vision transformer (ST-ViT) performs multi-frame\njoint decoding to reconstruct dropped semantic indices by modeling intra- and\ninter-frame dependencies. To further improve performance under dynamic network\nconditions, we integrate a multi-user proximal policy optimization (MUPPO)\nreinforcement learning scheme that jointly optimizes communication resource\nallocation and semantic bitrate selection to maximize user Quality of\nExperience (QoE). Extensive experiments demonstrate that the proposed SSCV-G\nsignificantly outperforms state-of-the-art video codecs in coding efficiency,\nbandwidth adaptability, and packet loss robustness. Moreover, the proposed\nMUPPO-based QoE optimization consistently surpasses existing benchmarks.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u8bed\u4e49\u81ea\u6821\u6b63\u7684\u89c6\u9891\u4f20\u8f93\u6846\u67b6\uff08SSCV-G\uff09\uff0c\u901a\u8fc7\u8d85\u7ec6\u7c92\u5ea6\u6bd4\u7279\u7387\u63a7\u5236\u548c\u591a\u7528\u6237\u5f3a\u5316\u5b66\u4e60\u4f18\u5316\uff0c\u663e\u8457\u63d0\u5347\u4e86\u65e0\u4eba\u673a\u89c6\u9891\u6d41\u7684\u5e26\u5bbd\u6548\u7387\u548c\u7528\u6237\u4f53\u9a8c\u3002", "motivation": "\u65e0\u4eba\u673a\u5b9e\u65f6\u89c6\u9891\u6d41\u5728\u7d27\u6025\u54cd\u5e94\u7b49\u573a\u666f\u4e2d\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u9762\u4e34\u5e26\u5bbd\u9650\u5236\u3001\u5ef6\u8fdf\u6ce2\u52a8\u548c\u9ad8\u4e22\u5305\u7387\u7684\u6311\u6218\u3002", "method": "SSCV-G\u5c06\u89c6\u9891\u5e27\u7f16\u7801\u4e3a\u8bed\u4e49\u7801\u672c\u7a7a\u95f4\uff0c\u81ea\u9002\u5e94\u4f20\u8f93\u8bed\u4e49\u7d22\u5f15\uff1b\u63a5\u6536\u7aef\u4f7f\u7528\u65f6\u7a7a\u89c6\u89c9\u53d8\u6362\u5668\uff08ST-ViT\uff09\u91cd\u5efa\u4e22\u5931\u7d22\u5f15\uff1b\u7ed3\u5408\u591a\u7528\u6237\u8fd1\u7aef\u7b56\u7565\u4f18\u5316\uff08MUPPO\uff09\u52a8\u6001\u4f18\u5316\u8d44\u6e90\u5206\u914d\u548c\u6bd4\u7279\u7387\u9009\u62e9\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cSSCV-G\u5728\u7f16\u7801\u6548\u7387\u3001\u5e26\u5bbd\u9002\u5e94\u6027\u548c\u4e22\u5305\u9c81\u68d2\u6027\u4e0a\u4f18\u4e8e\u73b0\u6709\u89c6\u9891\u7f16\u89e3\u7801\u5668\uff0cMUPPO\u8fdb\u4e00\u6b65\u63d0\u5347\u4e86\u7528\u6237\u4f53\u9a8c\u3002", "conclusion": "SSCV-G\u548cMUPPO\u7684\u7ed3\u5408\u4e3a\u65e0\u4eba\u673a\u89c6\u9891\u6d41\u63d0\u4f9b\u4e86\u9ad8\u6548\u3001\u81ea\u9002\u5e94\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.06499", "pdf": "https://arxiv.org/pdf/2507.06499", "abs": "https://arxiv.org/abs/2507.06499", "authors": ["Shivangi Agarwal", "Adi Asija", "Sanjit K. Kaul", "Arani Bhattacharya", "Saket Anand"], "title": "Learning To Communicate Over An Unknown Shared Network", "categories": ["cs.MA", "cs.NI"], "comment": "22 pages, 15 figures, 4 tables", "summary": "As robots (edge-devices, agents) find uses in an increasing number of\nsettings and edge-cloud resources become pervasive, wireless networks will\noften be shared by flows of data traffic that result from communication between\nagents and corresponding edge-cloud. In such settings, agent communicating with\nthe edge-cloud is unaware of state of network resource, which evolves in\nresponse to not just agent's own communication at any given time but also to\ncommunication by other agents, which stays unknown to the agent. We address\nchallenge of an agent learning a policy that allows it to decide whether or not\nto communicate with its cloud node, using limited feedback it obtains from its\nown attempts to communicate, to optimize its utility. The policy generalizes\nwell to any number of other agents sharing the network and must not be trained\nfor any particular network configuration. Our proposed policy is a DRL model\nQuery Net (QNet) that we train using a proposed simulation-to-real framework.\nOur simulation model has just one parameter and is agnostic to specific\nconfigurations of any wireless network. It allows training an agent's policy\nover a wide range of outcomes that an agent's communication with its edge-cloud\nnode may face when using a shared network, by suitably randomizing the\nsimulation parameter. We propose a learning algorithm that addresses challenges\nobserved in training QNet. We validate our simulation-to-real driven approach\nthrough experiments conducted on real wireless networks including WiFi and\ncellular. We compare QNet with other policies to demonstrate its efficacy. WiFi\nexperiments involved as few as five agents, resulting in barely any contention\nfor the network, to as many as fifty agents, resulting in severe contention.\nThe cellular experiments spanned a broad range of network conditions, with\nbaseline RTT ranging from a low of 0.07 second to a high of 0.83 second.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aQNet\u7684\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u6a21\u578b\uff0c\u5e2e\u52a9\u4ee3\u7406\u5728\u5171\u4eab\u65e0\u7ebf\u7f51\u7edc\u4e2d\u4f18\u5316\u901a\u4fe1\u51b3\u7b56\uff0c\u65e0\u9700\u4e86\u89e3\u5176\u4ed6\u4ee3\u7406\u7684\u72b6\u6001\u6216\u7f51\u7edc\u914d\u7f6e\u3002", "motivation": "\u5728\u5171\u4eab\u65e0\u7ebf\u7f51\u7edc\u4e2d\uff0c\u4ee3\u7406\u65e0\u6cd5\u611f\u77e5\u7f51\u7edc\u8d44\u6e90\u72b6\u6001\u548c\u5176\u4ed6\u4ee3\u7406\u7684\u901a\u4fe1\u884c\u4e3a\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u80fd\u591f\u9002\u5e94\u4e0d\u540c\u7f51\u7edc\u914d\u7f6e\u7684\u901a\u7528\u7b56\u7565\u3002", "method": "\u4f7f\u7528\u6a21\u62df\u5230\u73b0\u5b9e\u7684\u6846\u67b6\u8bad\u7ec3QNet\uff0c\u6a21\u62df\u6a21\u578b\u4ec5\u9700\u4e00\u4e2a\u53c2\u6570\uff0c\u5e76\u901a\u8fc7\u968f\u673a\u5316\u6a21\u62df\u53c2\u6570\u8986\u76d6\u591a\u79cd\u7f51\u7edc\u60c5\u51b5\u3002", "result": "\u5b9e\u9a8c\u9a8c\u8bc1\u4e86QNet\u5728WiFi\u548c\u8702\u7a9d\u7f51\u7edc\u4e2d\u7684\u6709\u6548\u6027\uff0c\u9002\u5e94\u4ece\u4f4e\u5230\u9ad8\u7684\u7f51\u7edc\u7ade\u4e89\u548c\u5ef6\u8fdf\u6761\u4ef6\u3002", "conclusion": "QNet\u662f\u4e00\u79cd\u901a\u7528\u4e14\u9ad8\u6548\u7684\u7b56\u7565\uff0c\u9002\u7528\u4e8e\u4e0d\u540c\u7f51\u7edc\u914d\u7f6e\u548c\u591a\u4ee3\u7406\u5171\u4eab\u73af\u5883\u3002"}}
{"id": "2507.06397", "pdf": "https://arxiv.org/pdf/2507.06397", "abs": "https://arxiv.org/abs/2507.06397", "authors": ["Michalis Chatzispyrou", "Luke Horgan", "Hyunkil Hwang", "Harish Sathishchandra", "Monika Roznere", "Alberto Quattrini Li", "Philippos Mordohai", "Ioannis Rekleitis"], "title": "Mapping the Catacombs: An Underwater Cave Segment of the Devil's Eye System", "categories": ["cs.RO"], "comment": "Presented at the 2025 IEEE ICRA Workshop on Field Robotics", "summary": "This paper presents a framework for mapping underwater caves. Underwater\ncaves are crucial for fresh water resource management, underwater archaeology,\nand hydrogeology. Mapping the cave's outline and dimensions, as well as\ncreating photorealistic 3D maps, is critical for enabling a better\nunderstanding of this underwater domain. In this paper, we present the mapping\nof an underwater cave segment (the catacombs) of the Devil's Eye cave system at\nGinnie Springs, FL. We utilized a set of inexpensive action cameras in\nconjunction with a dive computer to estimate the trajectories of the cameras\ntogether with a sparse point cloud. The resulting reconstructions are utilized\nto produce a one-dimensional retract of the cave passages in the form of the\naverage trajectory together with the boundaries (top, bottom, left, and right).\nThe use of the dive computer enables the observability of the z-dimension in\naddition to the roll and pitch in a visual/inertial framework (SVIn2). In\naddition, the keyframes generated by SVIn2 together with the estimated camera\nposes for select areas are used as input to a global optimization (bundle\nadjustment) framework -- COLMAP -- in order to produce a dense reconstruction\nof those areas. The same cave segment is manually surveyed using the MNemo V2\ninstrument, providing an additional set of measurements validating the proposed\napproach. It is worth noting that with the use of action cameras, the primary\ncomponents of a cave map can be constructed. Furthermore, with the utilization\nof a global optimization framework guided by the results of VI-SLAM package\nSVIn2, photorealistic dense 3D representations of selected areas can be\nreconstructed.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5229\u7528\u4f4e\u6210\u672c\u8fd0\u52a8\u76f8\u673a\u548c\u6f5c\u6c34\u7535\u8111\u7ed8\u5236\u6c34\u4e0b\u6d1e\u7a74\u5730\u56fe\u7684\u6846\u67b6\uff0c\u7ed3\u5408\u89c6\u89c9/\u60ef\u6027\u6846\u67b6\u548c\u5168\u5c40\u4f18\u5316\u65b9\u6cd5\uff0c\u751f\u6210\u6d1e\u7a74\u7684\u4e00\u7ef4\u8f6e\u5ed3\u548c\u5bc6\u96c63D\u91cd\u5efa\u3002", "motivation": "\u6c34\u4e0b\u6d1e\u7a74\u5bf9\u6de1\u6c34\u8d44\u6e90\u7ba1\u7406\u3001\u6c34\u4e0b\u8003\u53e4\u548c\u6c34\u6587\u5730\u8d28\u5b66\u81f3\u5173\u91cd\u8981\uff0c\u7ed8\u5236\u5176\u8f6e\u5ed3\u548c\u5c3a\u5bf8\u4ee5\u53ca\u751f\u6210\u903c\u771f\u76843D\u5730\u56fe\u6709\u52a9\u4e8e\u66f4\u597d\u5730\u7406\u89e3\u8fd9\u4e00\u6c34\u4e0b\u9886\u57df\u3002", "method": "\u4f7f\u7528\u8fd0\u52a8\u76f8\u673a\u548c\u6f5c\u6c34\u7535\u8111\u4f30\u8ba1\u76f8\u673a\u8f68\u8ff9\u548c\u7a00\u758f\u70b9\u4e91\uff0c\u7ed3\u5408\u89c6\u89c9/\u60ef\u6027\u6846\u67b6\uff08SVIn2\uff09\u548c\u5168\u5c40\u4f18\u5316\u5de5\u5177\uff08COLMAP\uff09\u751f\u6210\u6d1e\u7a74\u7684\u4e00\u7ef4\u8f6e\u5ed3\u548c\u5bc6\u96c63D\u91cd\u5efa\uff0c\u5e76\u901a\u8fc7\u624b\u52a8\u6d4b\u91cf\u9a8c\u8bc1\u3002", "result": "\u6210\u529f\u751f\u6210\u6d1e\u7a74\u7684\u4e00\u7ef4\u8f6e\u5ed3\u548c\u90e8\u5206\u533a\u57df\u7684\u5bc6\u96c63D\u91cd\u5efa\uff0c\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u4f4e\u6210\u672c\u8fd0\u52a8\u76f8\u673a\u7ed3\u5408\u89c6\u89c9/\u60ef\u6027\u6846\u67b6\u548c\u5168\u5c40\u4f18\u5316\u65b9\u6cd5\uff0c\u80fd\u591f\u6709\u6548\u7ed8\u5236\u6c34\u4e0b\u6d1e\u7a74\u5730\u56fe\u5e76\u751f\u6210\u903c\u771f\u76843D\u91cd\u5efa\u3002"}}
{"id": "2507.06306", "pdf": "https://arxiv.org/pdf/2507.06306", "abs": "https://arxiv.org/abs/2507.06306", "authors": ["Neil Rathi", "Dan Jurafsky", "Kaitlyn Zhou"], "title": "Humans overrely on overconfident language models, across languages", "categories": ["cs.CL", "cs.AI", "cs.HC"], "comment": "10 pages main text, to appear at COLM 2025", "summary": "As large language models (LLMs) are deployed globally, it is crucial that\ntheir responses are calibrated across languages to accurately convey\nuncertainty and limitations. Previous work has shown that LLMs are\nlinguistically overconfident in English, leading users to overrely on confident\ngenerations. However, the usage and interpretation of epistemic markers (e.g.,\n'It's definitely,' 'I think') can differ sharply across languages. Here, we\nstudy the risks of multilingual linguistic (mis)calibration, overconfidence,\nand overreliance across five languages to evaluate the safety of LLMs in a\nglobal context.\n  We find that overreliance risks are high across all languages. We first\nanalyze the distribution of LLM-generated epistemic markers, and observe that\nwhile LLMs are cross-linguistically overconfident, they are also sensitive to\ndocumented linguistic variation. For example, models generate the most markers\nof uncertainty in Japanese and the most markers of certainty in German and\nMandarin. We then measure human reliance rates across languages, finding that\nwhile users strongly rely on confident LLM generations in all languages,\nreliance behaviors differ cross-linguistically: for example, users rely\nsignificantly more on expressions of uncertainty in Japanese than in English.\nTaken together, these results indicate high risk of reliance on overconfident\nmodel generations across languages. Our findings highlight the challenges of\nmultilingual linguistic calibration and stress the importance of culturally and\nlinguistically contextualized model safety evaluations.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u591a\u8bed\u8a00\u73af\u5883\u4e2d\u5b58\u5728\u8fc7\u5ea6\u81ea\u4fe1\u548c\u7528\u6237\u8fc7\u5ea6\u4f9d\u8d56\u7684\u95ee\u9898\uff0c\u5c24\u5176\u662f\u5728\u4e0d\u540c\u8bed\u8a00\u4e2d\u8868\u8fbe\u4e0d\u786e\u5b9a\u6027\u7684\u65b9\u5f0f\u5dee\u5f02\u663e\u8457\u3002", "motivation": "\u8bc4\u4f30LLMs\u5728\u5168\u7403\u8303\u56f4\u5185\u7684\u5b89\u5168\u6027\uff0c\u5c24\u5176\u662f\u5176\u5728\u4e0d\u540c\u8bed\u8a00\u4e2d\u8868\u8fbe\u4e0d\u786e\u5b9a\u6027\u548c\u5c40\u9650\u6027\u65f6\u7684\u6821\u51c6\u95ee\u9898\u3002", "method": "\u5206\u6790\u4e94\u79cd\u8bed\u8a00\u4e2dLLM\u751f\u6210\u7684\u8ba4\u8bc6\u6807\u8bb0\u5206\u5e03\uff0c\u5e76\u6d4b\u91cf\u7528\u6237\u5728\u4e0d\u540c\u8bed\u8a00\u4e2d\u5bf9\u6a21\u578b\u8f93\u51fa\u7684\u4f9d\u8d56\u884c\u4e3a\u3002", "result": "LLMs\u5728\u6240\u6709\u8bed\u8a00\u4e2d\u90fd\u8868\u73b0\u51fa\u8fc7\u5ea6\u81ea\u4fe1\uff0c\u4f46\u80fd\u6355\u6349\u5230\u8bed\u8a00\u95f4\u7684\u5dee\u5f02\uff1b\u7528\u6237\u5bf9\u81ea\u4fe1\u8f93\u51fa\u7684\u4f9d\u8d56\u884c\u4e3a\u56e0\u8bed\u8a00\u800c\u5f02\u3002", "conclusion": "\u591a\u8bed\u8a00\u6821\u51c6\u9762\u4e34\u6311\u6218\uff0c\u9700\u7ed3\u5408\u6587\u5316\u548c\u8bed\u8a00\u80cc\u666f\u8fdb\u884c\u6a21\u578b\u5b89\u5168\u6027\u8bc4\u4f30\u3002"}}
{"id": "2507.06326", "pdf": "https://arxiv.org/pdf/2507.06326", "abs": "https://arxiv.org/abs/2507.06326", "authors": ["Harsh Ravivarapu", "Gaurav Bagwe", "Xiaoyong Yuan", "Chunxiu Yu", "Lan Zhang"], "title": "Sample-Efficient Reinforcement Learning Controller for Deep Brain Stimulation in Parkinson's Disease", "categories": ["cs.LG", "cs.AI", "cs.SY", "eess.SY", "q-bio.NC"], "comment": "Accepted by IEEE IMC 2025", "summary": "Deep brain stimulation (DBS) is an established intervention for Parkinson's\ndisease (PD), but conventional open-loop systems lack adaptability, are\nenergy-inefficient due to continuous stimulation, and provide limited\npersonalization to individual neural dynamics. Adaptive DBS (aDBS) offers a\nclosed-loop alternative, using biomarkers such as beta-band oscillations to\ndynamically modulate stimulation. While reinforcement learning (RL) holds\npromise for personalized aDBS control, existing methods suffer from high sample\ncomplexity, unstable exploration in binary action spaces, and limited\ndeployability on resource-constrained hardware.\n  We propose SEA-DBS, a sample-efficient actor-critic framework that addresses\nthe core challenges of RL-based adaptive neurostimulation. SEA-DBS integrates a\npredictive reward model to reduce reliance on real-time feedback and employs\nGumbel Softmax-based exploration for stable, differentiable policy updates in\nbinary action spaces. Together, these components improve sample efficiency,\nexploration robustness, and compatibility with resource-constrained\nneuromodulatory hardware. We evaluate SEA-DBS on a biologically realistic\nsimulation of Parkinsonian basal ganglia activity, demonstrating faster\nconvergence, stronger suppression of pathological beta-band power, and\nresilience to post-training FP16 quantization. Our results show that SEA-DBS\noffers a practical and effective RL-based aDBS framework for real-time,\nresource-constrained neuromodulation.", "AI": {"tldr": "SEA-DBS\u662f\u4e00\u79cd\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u81ea\u9002\u5e94\u6df1\u8111\u523a\u6fc0\u6846\u67b6\uff0c\u901a\u8fc7\u6539\u8fdb\u6837\u672c\u6548\u7387\u548c\u63a2\u7d22\u7a33\u5b9a\u6027\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u4e0d\u8db3\u3002", "motivation": "\u4f20\u7edf\u5f00\u73af\u6df1\u8111\u523a\u6fc0\uff08DBS\uff09\u7f3a\u4e4f\u9002\u5e94\u6027\u548c\u4e2a\u6027\u5316\uff0c\u800c\u73b0\u6709\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u5b58\u5728\u6837\u672c\u590d\u6742\u5ea6\u9ad8\u3001\u63a2\u7d22\u4e0d\u7a33\u5b9a\u7b49\u95ee\u9898\u3002", "method": "SEA-DBS\u7ed3\u5408\u9884\u6d4b\u5956\u52b1\u6a21\u578b\u548cGumbel Softmax\u63a2\u7d22\uff0c\u4f18\u5316\u6837\u672c\u6548\u7387\u548c\u7b56\u7565\u66f4\u65b0\u3002", "result": "\u5728\u5e15\u91d1\u68ee\u75c5\u6a21\u62df\u4e2d\uff0cSEA-DBS\u8868\u73b0\u51fa\u66f4\u5feb\u7684\u6536\u655b\u901f\u5ea6\u3001\u66f4\u5f3a\u7684\u75c5\u7406\u03b2\u6ce2\u6bb5\u6291\u5236\u80fd\u529b\uff0c\u5e76\u517c\u5bb9\u8d44\u6e90\u53d7\u9650\u786c\u4ef6\u3002", "conclusion": "SEA-DBS\u4e3a\u5b9e\u65f6\u3001\u8d44\u6e90\u53d7\u9650\u7684\u795e\u7ecf\u8c03\u63a7\u63d0\u4f9b\u4e86\u4e00\u79cd\u5b9e\u7528\u4e14\u6709\u6548\u7684\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\u3002"}}
{"id": "2507.06265", "pdf": "https://arxiv.org/pdf/2507.06265", "abs": "https://arxiv.org/abs/2507.06265", "authors": ["Ali Nasiri-Sarvi", "Hassan Rivaz", "Mahdi S. Hosseini"], "title": "SPARC: Concept-Aligned Sparse Autoencoders for Cross-Model and Cross-Modal Interpretability", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Understanding how different AI models encode the same high-level concepts,\nsuch as objects or attributes, remains challenging because each model typically\nproduces its own isolated representation. Existing interpretability methods\nlike Sparse Autoencoders (SAEs) produce latent concepts individually for each\nmodel, resulting in incompatible concept spaces and limiting cross-model\ninterpretability. To address this, we introduce SPARC (Sparse Autoencoders for\nAligned Representation of Concepts), a new framework that learns a single,\nunified latent space shared across diverse architectures and modalities (e.g.,\nvision models like DINO, and multimodal models like CLIP). SPARC's alignment is\nenforced through two key innovations: (1) a Global TopK sparsity mechanism,\nensuring all input streams activate identical latent dimensions for a given\nconcept; and (2) a Cross-Reconstruction Loss, which explicitly encourages\nsemantic consistency between models. On Open Images, SPARC dramatically\nimproves concept alignment, achieving a Jaccard similarity of 0.80, more than\ntripling the alignment compared to previous methods. SPARC creates a shared\nsparse latent space where individual dimensions often correspond to similar\nhigh-level concepts across models and modalities, enabling direct comparison of\nhow different architectures represent identical concepts without requiring\nmanual alignment or model-specific analysis. As a consequence of this aligned\nrepresentation, SPARC also enables practical applications such as text-guided\nspatial localization in vision-only models and cross-model/cross-modal\nretrieval. Code and models are available at\nhttps://github.com/AtlasAnalyticsLab/SPARC.", "AI": {"tldr": "SPARC\u6846\u67b6\u901a\u8fc7\u7edf\u4e00\u6f5c\u5728\u7a7a\u95f4\u548c\u8de8\u6a21\u578b\u5bf9\u9f50\u673a\u5236\uff0c\u663e\u8457\u63d0\u5347\u4e86\u4e0d\u540cAI\u6a21\u578b\u95f4\u6982\u5ff5\u8868\u793a\u7684\u517c\u5bb9\u6027\u548c\u53ef\u89e3\u91ca\u6027\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\uff08\u5982SAEs\uff09\u56e0\u6a21\u578b\u72ec\u7acb\u6f5c\u5728\u6982\u5ff5\u7a7a\u95f4\u5bfc\u81f4\u7684\u8de8\u6a21\u578b\u53ef\u89e3\u91ca\u6027\u53d7\u9650\u95ee\u9898\u3002", "method": "\u5f15\u5165SPARC\u6846\u67b6\uff0c\u91c7\u7528Global TopK\u7a00\u758f\u673a\u5236\u548cCross-Reconstruction Loss\uff0c\u5f3a\u5236\u8de8\u6a21\u578b\u5171\u4eab\u6f5c\u5728\u7a7a\u95f4\u3002", "result": "\u5728Open Images\u4e0a\uff0cSPARC\u5c06\u6982\u5ff5\u5bf9\u9f50\u7684Jaccard\u76f8\u4f3c\u5ea6\u63d0\u5347\u81f30.80\uff0c\u8fdc\u8d85\u4e4b\u524d\u65b9\u6cd5\u3002", "conclusion": "SPARC\u5b9e\u73b0\u4e86\u8de8\u6a21\u578b\u548c\u8de8\u6a21\u6001\u7684\u6982\u5ff5\u5bf9\u9f50\uff0c\u652f\u6301\u76f4\u63a5\u6bd4\u8f83\u4e0d\u540c\u67b6\u6784\u7684\u6982\u5ff5\u8868\u793a\uff0c\u5e76\u5177\u5907\u5b9e\u9645\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2507.06968", "pdf": "https://arxiv.org/pdf/2507.06968", "abs": "https://arxiv.org/abs/2507.06968", "authors": ["Li Du", "Hanyu Zhao", "Yiming Ju", "Tengfei Pan"], "title": "Scaling Towards the Information Boundary of Instruction Set: InfinityInstruct-Subject Technical Report", "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "Instruction tuning has become a foundation for unlocking the capabilities of\nlarge-scale pretrained models and improving their performance on complex tasks.\nThus, the construction of high-quality instruction datasets is crucial for\nenhancing model performance and generalizability. Although current instruction\ndatasets have reached tens of millions of samples, models finetuned on them may\nstill struggle with complex instruction following and tasks in rare domains.\nThis is primarily due to limited expansion in both ``coverage'' (coverage of\ntask types and knowledge areas) and ``depth'' (instruction complexity) of the\ninstruction set. To address this issue, we propose a systematic instruction\ndata construction framework, which integrates a hierarchical labeling system,\nan informative seed selection algorithm, an evolutionary data synthesis\nprocess, and a model deficiency diagnosis with targeted data generation. These\ncomponents form an iterative closed-loop to continuously enhance the coverage\nand depth of instruction data. Based on this framework, we construct\nInfinityInstruct-Subject, a high-quality dataset containing ~1.5 million\ninstructions. Experiments on multiple foundation models and benchmark tasks\ndemonstrate its effectiveness in improving instruction-following capabilities.\nFurther analyses suggest that InfinityInstruct-Subject shows enlarged coverage\nand depth compared to comparable synthesized instruction datasets. Our work\nlays a theoretical and practical foundation for the efficient, continuous\nevolution of instruction datasets, moving from data quantity expansion to\nqualitative improvement.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7cfb\u7edf\u5316\u7684\u6307\u4ee4\u6570\u636e\u6784\u5efa\u6846\u67b6\uff0c\u901a\u8fc7\u5206\u5c42\u6807\u6ce8\u3001\u79cd\u5b50\u9009\u62e9\u3001\u6570\u636e\u5408\u6210\u548c\u6a21\u578b\u7f3a\u9677\u8bca\u65ad\uff0c\u6784\u5efa\u4e86\u9ad8\u8d28\u91cf\u6570\u636e\u96c6InfinityInstruct-Subject\uff0c\u63d0\u5347\u4e86\u6a21\u578b\u7684\u6307\u4ee4\u9075\u5faa\u80fd\u529b\u3002", "motivation": "\u5f53\u524d\u6307\u4ee4\u6570\u636e\u96c6\u867d\u89c4\u6a21\u5e9e\u5927\uff0c\u4f46\u5728\u590d\u6742\u6307\u4ee4\u548c\u7f55\u89c1\u9886\u57df\u4efb\u52a1\u4e0a\u8868\u73b0\u4e0d\u4f73\uff0c\u4e3b\u8981\u56e0\u8986\u76d6\u9762\u548c\u6df1\u5ea6\u4e0d\u8db3\u3002", "method": "\u63d0\u51fa\u5206\u5c42\u6807\u6ce8\u7cfb\u7edf\u3001\u79cd\u5b50\u9009\u62e9\u7b97\u6cd5\u3001\u8fdb\u5316\u6570\u636e\u5408\u6210\u548c\u6a21\u578b\u7f3a\u9677\u8bca\u65ad\u7684\u95ed\u73af\u6846\u67b6\u3002", "result": "\u6784\u5efa\u4e86\u5305\u542b150\u4e07\u6307\u4ee4\u7684\u9ad8\u8d28\u91cf\u6570\u636e\u96c6\uff0c\u5b9e\u9a8c\u8bc1\u660e\u5176\u80fd\u6709\u6548\u63d0\u5347\u6a21\u578b\u6027\u80fd\u3002", "conclusion": "\u5de5\u4f5c\u4e3a\u6307\u4ee4\u6570\u636e\u96c6\u4ece\u6570\u91cf\u6269\u5f20\u5230\u8d28\u91cf\u63d0\u5347\u63d0\u4f9b\u4e86\u7406\u8bba\u548c\u5b9e\u8df5\u57fa\u7840\u3002"}}
{"id": "2507.06674", "pdf": "https://arxiv.org/pdf/2507.06674", "abs": "https://arxiv.org/abs/2507.06674", "authors": ["Wei-Jaw Lee", "Fang-Chih Hsieh", "Xuanjun Chen", "Fang-Duo Tsai", "Yi-Hsuan Yang"], "title": "Exploring State-Space-Model based Language Model in Music Generation", "categories": ["cs.SD", "cs.AI", "eess.AS"], "comment": "Accepted at ISMIR 2025 as Late-Breaking Demo (LBD)", "summary": "The recent surge in State Space Models (SSMs), particularly the emergence of\nMamba, has established them as strong alternatives or complementary modules to\nTransformers across diverse domains. In this work, we aim to explore the\npotential of Mamba-based architectures for text-to-music generation. We adopt\ndiscrete tokens of Residual Vector Quantization (RVQ) as the modeling\nrepresentation and empirically find that a single-layer codebook can capture\nsemantic information in music. Motivated by this observation, we focus on\nmodeling a single-codebook representation and adapt SiMBA, originally designed\nas a Mamba-based encoder, to function as a decoder for sequence modeling. We\ncompare its performance against a standard Transformer-based decoder. Our\nresults suggest that, under limited-resource settings, SiMBA achieves much\nfaster convergence and generates outputs closer to the ground truth. This\ndemonstrates the promise of SSMs for efficient and expressive text-to-music\ngeneration. We put audio examples on Github.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u57fa\u4e8eMamba\u7684SiMBA\u67b6\u6784\u5728\u6587\u672c\u5230\u97f3\u4e50\u751f\u6210\u4e2d\u7684\u6f5c\u529b\uff0c\u53d1\u73b0\u5176\u5728\u6709\u9650\u8d44\u6e90\u4e0b\u6bd4Transformer\u66f4\u5feb\u6536\u655b\u4e14\u751f\u6210\u8d28\u91cf\u66f4\u9ad8\u3002", "motivation": "\u63a2\u7d22Mamba\u67b6\u6784\u5728\u6587\u672c\u5230\u97f3\u4e50\u751f\u6210\u4e2d\u7684\u5e94\u7528\uff0c\u4ee5\u89e3\u51b3Transformer\u7684\u9ad8\u8d44\u6e90\u9700\u6c42\u95ee\u9898\u3002", "method": "\u91c7\u7528RVQ\u79bb\u6563\u4ee4\u724c\u8868\u793a\u97f3\u4e50\uff0c\u5e76\u8c03\u6574SiMBA\uff08Mamba\u7f16\u7801\u5668\uff09\u4e3a\u89e3\u7801\u5668\uff0c\u4e0eTransformer\u89e3\u7801\u5668\u5bf9\u6bd4\u3002", "result": "SiMBA\u5728\u6709\u9650\u8d44\u6e90\u4e0b\u6536\u655b\u66f4\u5feb\uff0c\u751f\u6210\u7ed3\u679c\u66f4\u63a5\u8fd1\u771f\u5b9e\u97f3\u4e50\u3002", "conclusion": "SSM\uff08\u5982SiMBA\uff09\u5728\u9ad8\u6548\u4e14\u5bcc\u6709\u8868\u73b0\u529b\u7684\u6587\u672c\u5230\u97f3\u4e50\u751f\u6210\u4e2d\u5177\u6709\u6f5c\u529b\u3002"}}
{"id": "2507.06735", "pdf": "https://arxiv.org/pdf/2507.06735", "abs": "https://arxiv.org/abs/2507.06735", "authors": ["Guan Zheng", "Xue Wang", "Wenhua Qian", "Peng Liu", "Runzhuo Ma"], "title": "Residual Prior-driven Frequency-aware Network for Image Fusion", "categories": ["cs.CV", "cs.LG", "cs.MM"], "comment": null, "summary": "Image fusion aims to integrate complementary information across modalities to\ngenerate high-quality fused images, thereby enhancing the performance of\nhigh-level vision tasks. While global spatial modeling mechanisms show\npromising results, constructing long-range feature dependencies in the spatial\ndomain incurs substantial computational costs. Additionally, the absence of\nground-truth exacerbates the difficulty of capturing complementary features\neffectively. To tackle these challenges, we propose a Residual Prior-driven\nFrequency-aware Network, termed as RPFNet. Specifically, RPFNet employs a\ndual-branch feature extraction framework: the Residual Prior Module (RPM)\nextracts modality-specific difference information from residual maps, thereby\nproviding complementary priors for fusion; the Frequency Domain Fusion Module\n(FDFM) achieves efficient global feature modeling and integration through\nfrequency-domain convolution. Additionally, the Cross Promotion Module (CPM)\nenhances the synergistic perception of local details and global structures\nthrough bidirectional feature interaction. During training, we incorporate an\nauxiliary decoder and saliency structure loss to strengthen the model's\nsensitivity to modality-specific differences. Furthermore, a combination of\nadaptive weight-based frequency contrastive loss and SSIM loss effectively\nconstrains the solution space, facilitating the joint capture of local details\nand global features while ensuring the retention of complementary information.\nExtensive experiments validate the fusion performance of RPFNet, which\neffectively integrates discriminative features, enhances texture details and\nsalient objects, and can effectively facilitate the deployment of the\nhigh-level vision task.", "AI": {"tldr": "RPFNet\u901a\u8fc7\u6b8b\u5dee\u5148\u9a8c\u548c\u9891\u57df\u878d\u5408\uff0c\u9ad8\u6548\u6574\u5408\u591a\u6a21\u6001\u56fe\u50cf\u4fe1\u606f\uff0c\u63d0\u5347\u878d\u5408\u8d28\u91cf\u548c\u9ad8\u7ea7\u89c6\u89c9\u4efb\u52a1\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u4f20\u7edf\u56fe\u50cf\u878d\u5408\u65b9\u6cd5\u4e2d\u957f\u8ddd\u79bb\u7279\u5f81\u4f9d\u8d56\u8ba1\u7b97\u6210\u672c\u9ad8\u548c\u7f3a\u4e4f\u771f\u5b9e\u6807\u6ce8\u7684\u95ee\u9898\u3002", "method": "\u91c7\u7528\u53cc\u5206\u652f\u6846\u67b6\uff08RPM\u548cFDFM\uff09\u63d0\u53d6\u6a21\u6001\u5dee\u5f02\u548c\u9891\u57df\u7279\u5f81\uff0c\u5e76\u901a\u8fc7CPM\u589e\u5f3a\u5c40\u90e8\u4e0e\u5168\u5c40\u7279\u5f81\u534f\u540c\u3002", "result": "RPFNet\u6709\u6548\u6574\u5408\u7279\u5f81\uff0c\u63d0\u5347\u7eb9\u7406\u7ec6\u8282\u548c\u663e\u8457\u5bf9\u8c61\uff0c\u652f\u6301\u9ad8\u7ea7\u89c6\u89c9\u4efb\u52a1\u90e8\u7f72\u3002", "conclusion": "RPFNet\u5728\u591a\u6a21\u6001\u56fe\u50cf\u878d\u5408\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u517c\u5177\u9ad8\u6548\u6027\u548c\u6027\u80fd\u4f18\u52bf\u3002"}}
{"id": "2507.06520", "pdf": "https://arxiv.org/pdf/2507.06520", "abs": "https://arxiv.org/abs/2507.06520", "authors": ["Xinyuan Song", "Zeyu Wang", "Siyi Wu", "Tianyu Shi", "Lynn Ai"], "title": "Gradientsys: A Multi-Agent LLM Scheduler with ReAct Orchestration", "categories": ["cs.MA", "cs.AI"], "comment": null, "summary": "We present Gradientsys, a next-generation multi-agent scheduling framework\nthat coordinates diverse specialized AI agents using a typed Model-Context\nProtocol (MCP) and a ReAct-based dynamic planning loop. At its core,\nGradientsys employs an LLM-powered scheduler for intelligent one-to-many task\ndispatch, enabling parallel execution of heterogeneous agents such as PDF\nparsers, web search modules, GUI controllers, and web builders. The framework\nsupports hybrid synchronous/asynchronous execution, respects agent capacity\nconstraints, and incorporates a robust retry-and-replan mechanism to handle\nfailures gracefully. To promote transparency and trust, Gradientsys includes an\nobservability layer streaming real-time agent activity and intermediate\nreasoning via Server-Sent Events (SSE). We offer an architectural overview and\nevaluate Gradientsys against existing frameworks in terms of extensibility,\nscheduling topology, tool reusability, parallelism, and observability.\nExperiments on the GAIA general-assistant benchmark show that Gradientsys\nachieves higher task success rates with reduced latency and lower API costs\ncompared to a MinionS-style baseline, demonstrating the strength of its\nLLM-driven multi-agent orchestration.", "AI": {"tldr": "Gradientsys\u662f\u4e00\u4e2a\u591a\u667a\u80fd\u4f53\u8c03\u5ea6\u6846\u67b6\uff0c\u901a\u8fc7MCP\u548c\u52a8\u6001\u89c4\u5212\u5faa\u73af\u534f\u8c03\u591a\u6837\u5316AI\u667a\u80fd\u4f53\uff0c\u652f\u6301\u5e76\u884c\u6267\u884c\u548c\u900f\u660e\u76d1\u63a7\uff0c\u5b9e\u9a8c\u663e\u793a\u5176\u5728\u4efb\u52a1\u6210\u529f\u7387\u548c\u6548\u7387\u4e0a\u4f18\u4e8e\u57fa\u7ebf\u3002", "motivation": "\u89e3\u51b3\u591a\u667a\u80fd\u4f53\u534f\u4f5c\u4e2d\u7684\u4efb\u52a1\u8c03\u5ea6\u3001\u5e76\u884c\u6267\u884c\u548c\u900f\u660e\u6027\u95ee\u9898\u3002", "method": "\u91c7\u7528MCP\u534f\u8bae\u548c\u52a8\u6001\u89c4\u5212\u5faa\u73af\uff0c\u7ed3\u5408LLM\u8c03\u5ea6\u5668\u5b9e\u73b0\u667a\u80fd\u4efb\u52a1\u5206\u53d1\uff0c\u652f\u6301\u6df7\u5408\u540c\u6b65/\u5f02\u6b65\u6267\u884c\u548c\u5931\u8d25\u5904\u7406\u673a\u5236\u3002", "result": "\u5728GAIA\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cGradientsys\u4efb\u52a1\u6210\u529f\u7387\u66f4\u9ad8\uff0c\u5ef6\u8fdf\u548cAPI\u6210\u672c\u66f4\u4f4e\u3002", "conclusion": "Gradientsys\u901a\u8fc7LLM\u9a71\u52a8\u7684\u591a\u667a\u80fd\u4f53\u534f\u8c03\uff0c\u5728\u6027\u80fd\u548c\u6548\u7387\u4e0a\u8868\u73b0\u51fa\u8272\u3002"}}
{"id": "2507.06404", "pdf": "https://arxiv.org/pdf/2507.06404", "abs": "https://arxiv.org/abs/2507.06404", "authors": ["Matteo Tiezzi", "Tommaso Apicella", "Carlos Cardenas-Perez", "Giovanni Fregonese", "Stefano Dafarra", "Pietro Morerio", "Daniele Pucci", "Alessio Del Bue"], "title": "Learning to Evaluate Autonomous Behaviour in Human-Robot Interaction", "categories": ["cs.RO", "cs.CV", "cs.LG"], "comment": null, "summary": "Evaluating and comparing the performance of autonomous Humanoid Robots is\nchallenging, as success rate metrics are difficult to reproduce and fail to\ncapture the complexity of robot movement trajectories, critical in Human-Robot\nInteraction and Collaboration (HRIC). To address these challenges, we propose a\ngeneral evaluation framework that measures the quality of Imitation Learning\n(IL) methods by focusing on trajectory performance. We devise the Neural Meta\nEvaluator (NeME), a deep learning model trained to classify actions from robot\njoint trajectories. NeME serves as a meta-evaluator to compare the performance\nof robot control policies, enabling policy evaluation without requiring human\ninvolvement in the loop. We validate our framework on ergoCub, a humanoid\nrobot, using teleoperation data and comparing IL methods tailored to the\navailable platform. The experimental results indicate that our method is more\naligned with the success rate obtained on the robot than baselines, offering a\nreproducible, systematic, and insightful means for comparing the performance of\nmultimodal imitation learning approaches in complex HRI tasks.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u8f68\u8ff9\u6027\u80fd\u7684\u901a\u7528\u8bc4\u4f30\u6846\u67b6NeME\uff0c\u7528\u4e8e\u6bd4\u8f83\u6a21\u4eff\u5b66\u4e60\u65b9\u6cd5\u5728\u590d\u6742HRI\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\u3002", "motivation": "\u7531\u4e8e\u6210\u529f\u7387\u6307\u6807\u96be\u4ee5\u590d\u73b0\u4e14\u65e0\u6cd5\u6355\u6349\u673a\u5668\u4eba\u8fd0\u52a8\u8f68\u8ff9\u7684\u590d\u6742\u6027\uff0c\u8bc4\u4f30\u4eba\u5f62\u673a\u5668\u4eba\u6027\u80fd\u5177\u6709\u6311\u6218\u6027\u3002", "method": "\u8bbe\u8ba1\u4e86NeME\uff08\u795e\u7ecf\u5143\u8bc4\u4f30\u5668\uff09\uff0c\u901a\u8fc7\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u5206\u7c7b\u673a\u5668\u4eba\u5173\u8282\u8f68\u8ff9\u52a8\u4f5c\uff0c\u4f5c\u4e3a\u5143\u8bc4\u4f30\u5668\u6bd4\u8f83\u63a7\u5236\u7b56\u7565\u6027\u80fd\u3002", "result": "\u5b9e\u9a8c\u9a8c\u8bc1\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u6bd4\u57fa\u7ebf\u66f4\u7b26\u5408\u673a\u5668\u4eba\u5b9e\u9645\u6210\u529f\u7387\uff0c\u63d0\u4f9b\u4e86\u53ef\u590d\u73b0\u3001\u7cfb\u7edf\u5316\u7684\u8bc4\u4f30\u624b\u6bb5\u3002", "conclusion": "NeME\u4e3a\u590d\u6742HRI\u4efb\u52a1\u4e2d\u7684\u591a\u6a21\u6001\u6a21\u4eff\u5b66\u4e60\u65b9\u6cd5\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u6027\u80fd\u6bd4\u8f83\u5de5\u5177\u3002"}}
{"id": "2507.06313", "pdf": "https://arxiv.org/pdf/2507.06313", "abs": "https://arxiv.org/abs/2507.06313", "authors": ["Kiarash Zahirnia", "Zahra Golpayegani", "Walid Ahmad", "Yang Liu"], "title": "ETT: Expanding the Long Context Understanding Capability of LLMs at Test-Time", "categories": ["cs.CL"], "comment": null, "summary": "Transformer-based Language Models' computation and memory overhead increase\nquadratically as a function of sequence length. The quadratic cost poses\nchallenges when employing LLMs for processing long sequences. In this work, we\nintroduce \\ourmodelacronym~(Extend at Test-Time), method for extending the\ncontext length of short context Transformer-based LLMs, with constant memory\nrequirement and linear computation overhead. ETT enable the extension of the\ncontext length at test-time by efficient fine-tuning the model's parameters on\nthe input context, chunked into overlapping small subsequences. We evaluate ETT\non LongBench by extending the context length of GPT-Large and Phi-2 up to 32\ntimes, increasing from 1k to 32k tokens. This results in up to a 30 percent\nimprovement in the model's accuracy. We also study how context can be stored in\nLLM's weights effectively and efficiently. Through a detailed ablation study,\nwe examine which Transformer modules are most beneficial to fine-tune at\ntest-time. Interestingly, we find that fine-tuning the second layer of the FFNs\nis more effective than full fine-tuning, leading to a further improvement in\nthe models' accuracy.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faETT\u65b9\u6cd5\uff0c\u901a\u8fc7\u9ad8\u6548\u5fae\u8c03\u77ed\u4e0a\u4e0b\u6587Transformer\u6a21\u578b\u7684\u53c2\u6570\uff0c\u4ee5\u7ebf\u6027\u8ba1\u7b97\u5f00\u9500\u548c\u6052\u5b9a\u5185\u5b58\u9700\u6c42\u6269\u5c55\u4e0a\u4e0b\u6587\u957f\u5ea6\uff0c\u63d0\u5347\u6a21\u578b\u5728\u957f\u5e8f\u5217\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\u3002", "motivation": "Transformer\u6a21\u578b\u5728\u5904\u7406\u957f\u5e8f\u5217\u65f6\u9762\u4e34\u4e8c\u6b21\u8ba1\u7b97\u548c\u5185\u5b58\u5f00\u9500\u7684\u95ee\u9898\uff0c\u9650\u5236\u4e86\u5176\u5e94\u7528\u8303\u56f4\u3002", "method": "ETT\u65b9\u6cd5\u901a\u8fc7\u5c06\u8f93\u5165\u4e0a\u4e0b\u6587\u5206\u5757\u4e3a\u91cd\u53e0\u5b50\u5e8f\u5217\uff0c\u5e76\u5728\u6d4b\u8bd5\u65f6\u9ad8\u6548\u5fae\u8c03\u6a21\u578b\u53c2\u6570\uff0c\u5b9e\u73b0\u4e0a\u4e0b\u6587\u957f\u5ea6\u7684\u6269\u5c55\u3002", "result": "\u5728LongBench\u4e0a\uff0cETT\u5c06GPT-Large\u548cPhi-2\u7684\u4e0a\u4e0b\u6587\u957f\u5ea6\u4ece1k\u6269\u5c55\u523032k\uff0c\u51c6\u786e\u7387\u63d0\u534730%\u3002\u7814\u7a76\u53d1\u73b0\u5fae\u8c03FFN\u7684\u7b2c\u4e8c\u5c42\u6bd4\u5168\u5fae\u8c03\u66f4\u6709\u6548\u3002", "conclusion": "ETT\u662f\u4e00\u79cd\u9ad8\u6548\u6269\u5c55Transformer\u6a21\u578b\u4e0a\u4e0b\u6587\u957f\u5ea6\u7684\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u6a21\u578b\u6027\u80fd\uff0c\u4e14\u5fae\u8c03\u7279\u5b9a\u6a21\u5757\u6548\u679c\u66f4\u4f73\u3002"}}
{"id": "2507.06342", "pdf": "https://arxiv.org/pdf/2507.06342", "abs": "https://arxiv.org/abs/2507.06342", "authors": ["M. A. Evangelista-Alvarado", "P. Su\u00e1rez-Serrato"], "title": "SymFlux: deep symbolic regression of Hamiltonian vector fields", "categories": ["cs.LG", "cs.AI", "math.DS", "math.SG"], "comment": "26 pages, 7 figures", "summary": "We present SymFlux, a novel deep learning framework that performs symbolic\nregression to identify Hamiltonian functions from their corresponding vector\nfields on the standard symplectic plane. SymFlux models utilize hybrid CNN-LSTM\narchitectures to learn and output the symbolic mathematical expression of the\nunderlying Hamiltonian. Training and validation are conducted on newly\ndeveloped datasets of Hamiltonian vector fields, a key contribution of this\nwork. Our results demonstrate the model's effectiveness in accurately\nrecovering these symbolic expressions, advancing automated discovery in\nHamiltonian mechanics.", "AI": {"tldr": "SymFlux\u662f\u4e00\u4e2a\u65b0\u9896\u7684\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\uff0c\u7528\u4e8e\u901a\u8fc7\u7b26\u53f7\u56de\u5f52\u4ece\u6807\u51c6\u8f9b\u5e73\u9762\u4e0a\u7684\u5411\u91cf\u573a\u4e2d\u8bc6\u522b\u54c8\u5bc6\u987f\u51fd\u6570\u3002", "motivation": "\u81ea\u52a8\u5316\u54c8\u5bc6\u987f\u529b\u5b66\u4e2d\u7684\u7b26\u53f7\u8868\u8fbe\u5f0f\u53d1\u73b0\u3002", "method": "\u91c7\u7528\u6df7\u5408CNN-LSTM\u67b6\u6784\uff0c\u8bad\u7ec3\u548c\u9a8c\u8bc1\u57fa\u4e8e\u65b0\u5f00\u53d1\u7684\u54c8\u5bc6\u987f\u5411\u91cf\u573a\u6570\u636e\u96c6\u3002", "result": "\u6a21\u578b\u80fd\u51c6\u786e\u6062\u590d\u7b26\u53f7\u8868\u8fbe\u5f0f\u3002", "conclusion": "SymFlux\u5728\u54c8\u5bc6\u987f\u529b\u5b66\u81ea\u52a8\u5316\u53d1\u73b0\u65b9\u9762\u53d6\u5f97\u4e86\u8fdb\u5c55\u3002"}}
{"id": "2507.06269", "pdf": "https://arxiv.org/pdf/2507.06269", "abs": "https://arxiv.org/abs/2507.06269", "authors": ["Rushil Desai", "Frederik Warburg", "Trevor Darrell", "Marissa Ramirez de Chanlatte"], "title": "A Probabilistic Approach to Uncertainty Quantification Leveraging 3D Geometry", "categories": ["cs.CV", "cs.AI"], "comment": "ICCV 2025 Workshops (8 Pages, 6 Figures, 2 Tables)", "summary": "Quantifying uncertainty in neural implicit 3D representations, particularly\nthose utilizing Signed Distance Functions (SDFs), remains a substantial\nchallenge due to computational inefficiencies, scalability issues, and\ngeometric inconsistencies. Existing methods typically neglect direct geometric\nintegration, leading to poorly calibrated uncertainty maps. We introduce\nBayesSDF, a novel probabilistic framework for uncertainty quantification in\nneural implicit SDF models, motivated by scientific simulation applications\nwith 3D environments (e.g., forests) such as modeling fluid flow through\nforests, where precise surface geometry and awareness of fidelity surface\ngeometric uncertainty are essential. Unlike radiance-based models such as NeRF\nor 3D Gaussian splatting, which lack explicit surface formulations, SDFs define\ncontinuous and differentiable geometry, making them better suited for physical\nmodeling and analysis. BayesSDF leverages a Laplace approximation to quantify\nlocal surface instability via Hessian-based metrics, enabling computationally\nefficient, surface-aware uncertainty estimation. Our method shows that\nuncertainty predictions correspond closely with poorly reconstructed geometry,\nproviding actionable confidence measures for downstream use. Extensive\nevaluations on synthetic and real-world datasets demonstrate that BayesSDF\noutperforms existing methods in both calibration and geometric consistency,\nestablishing a strong foundation for uncertainty-aware 3D scene reconstruction,\nsimulation, and robotic decision-making.", "AI": {"tldr": "BayesSDF\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u6982\u7387\u6846\u67b6\uff0c\u7528\u4e8e\u91cf\u5316\u795e\u7ecf\u9690\u5f0fSDF\u6a21\u578b\u4e2d\u7684\u4e0d\u786e\u5b9a\u6027\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u51e0\u4f55\u4e00\u81f4\u6027\u548c\u8ba1\u7b97\u6548\u7387\u4e0a\u7684\u4e0d\u8db3\u3002", "motivation": "\u79d1\u5b66\u6a21\u62df\u5e94\u7528\uff08\u5982\u68ee\u6797\u4e2d\u7684\u6d41\u4f53\u6a21\u62df\uff09\u9700\u8981\u7cbe\u786e\u7684\u8868\u9762\u51e0\u4f55\u548c\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u7f3a\u4e4f\u51e0\u4f55\u6574\u5408\u548c\u6821\u51c6\u3002", "method": "BayesSDF\u5229\u7528\u62c9\u666e\u62c9\u65af\u8fd1\u4f3c\u548c\u57fa\u4e8eHessian\u7684\u5ea6\u91cf\uff0c\u5b9e\u73b0\u9ad8\u6548\u3001\u8868\u9762\u611f\u77e5\u7684\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cBayesSDF\u5728\u6821\u51c6\u548c\u51e0\u4f55\u4e00\u81f4\u6027\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5e76\u63d0\u4f9b\u53ef\u64cd\u4f5c\u7684\u4e0b\u6e38\u7f6e\u4fe1\u5ea6\u3002", "conclusion": "BayesSDF\u4e3a\u4e0d\u786e\u5b9a\u6027\u611f\u77e5\u76843D\u573a\u666f\u91cd\u5efa\u3001\u6a21\u62df\u548c\u673a\u5668\u4eba\u51b3\u7b56\u5960\u5b9a\u4e86\u575a\u5b9e\u57fa\u7840\u3002"}}
{"id": "2507.06993", "pdf": "https://arxiv.org/pdf/2507.06993", "abs": "https://arxiv.org/abs/2507.06993", "authors": ["Jieren Deng", "Aleksandar Cvetkovic", "Pak Kiu Chung", "Dragomir Yankov", "Chiqun Zhang"], "title": "The User-Centric Geo-Experience: An LLM-Powered Framework for Enhanced Planning, Navigation, and Dynamic Adaptation", "categories": ["cs.AI", "cs.CV"], "comment": null, "summary": "Traditional travel-planning systems are often static and fragmented, leaving\nthem ill-equipped to handle real-world complexities such as evolving\nenvironmental conditions and unexpected itinerary disruptions. In this paper,\nwe identify three gaps between existing service providers causing frustrating\nuser experience: intelligent trip planning, precision \"last-100-meter\"\nnavigation, and dynamic itinerary adaptation. We propose three cooperative\nagents: a Travel Planning Agent that employs grid-based spatial grounding and\nmap analysis to help resolve complex multi-modal user queries; a Destination\nAssistant Agent that provides fine-grained guidance for the final navigation\nleg of each journey; and a Local Discovery Agent that leverages image\nembeddings and Retrieval-Augmented Generation (RAG) to detect and respond to\ntrip plan disruptions. With evaluations and experiments, our system\ndemonstrates substantial improvements in query interpretation, navigation\naccuracy, and disruption resilience, underscoring its promise for applications\nfrom urban exploration to emergency response.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u52a8\u6001\u65c5\u884c\u89c4\u5212\u7cfb\u7edf\uff0c\u901a\u8fc7\u4e09\u4e2a\u534f\u4f5c\u4ee3\u7406\u89e3\u51b3\u4f20\u7edf\u7cfb\u7edf\u7684\u4e0d\u8db3\uff0c\u663e\u8457\u63d0\u5347\u4e86\u67e5\u8be2\u89e3\u6790\u3001\u5bfc\u822a\u7cbe\u5ea6\u548c\u5e94\u5bf9\u5e72\u6270\u7684\u80fd\u529b\u3002", "motivation": "\u4f20\u7edf\u65c5\u884c\u89c4\u5212\u7cfb\u7edf\u9759\u6001\u4e14\u788e\u7247\u5316\uff0c\u65e0\u6cd5\u5e94\u5bf9\u73b0\u5b9e\u4e16\u754c\u7684\u590d\u6742\u6027\u548c\u7a81\u53d1\u5e72\u6270\uff0c\u5bfc\u81f4\u7528\u6237\u4f53\u9a8c\u4e0d\u4f73\u3002", "method": "\u8bbe\u8ba1\u4e86\u4e09\u4e2a\u534f\u4f5c\u4ee3\u7406\uff1a\u65c5\u884c\u89c4\u5212\u4ee3\u7406\uff08\u57fa\u4e8e\u7f51\u683c\u7a7a\u95f4\u548c\u5730\u56fe\u5206\u6790\uff09\u3001\u76ee\u7684\u5730\u52a9\u624b\u4ee3\u7406\uff08\u7cbe\u7ec6\u5bfc\u822a\uff09\u548c\u672c\u5730\u53d1\u73b0\u4ee3\u7406\uff08\u5229\u7528\u56fe\u50cf\u5d4c\u5165\u548cRAG\u5e94\u5bf9\u5e72\u6270\uff09\u3002", "result": "\u7cfb\u7edf\u5728\u67e5\u8be2\u89e3\u6790\u3001\u5bfc\u822a\u7cbe\u5ea6\u548c\u5e72\u6270\u5e94\u5bf9\u65b9\u9762\u8868\u73b0\u663e\u8457\u63d0\u5347\u3002", "conclusion": "\u8be5\u7cfb\u7edf\u5728\u4ece\u57ce\u5e02\u63a2\u7d22\u5230\u5e94\u6025\u54cd\u5e94\u7b49\u591a\u4e2a\u9886\u57df\u5177\u6709\u5e7f\u6cdb\u5e94\u7528\u524d\u666f\u3002"}}
{"id": "2507.06769", "pdf": "https://arxiv.org/pdf/2507.06769", "abs": "https://arxiv.org/abs/2507.06769", "authors": ["Yuancheng Luo", "Dmitriy Yamkovoy", "Guillermo Garcia"], "title": "Constraint Optimized Multichannel Mixer-limiter Design", "categories": ["cs.SD", "eess.AS", "eess.SP", "math.OC"], "comment": "For submission to ICASSP 2026", "summary": "Multichannel audio mixer and limiter designs are conventionally decoupled for\ncontent reproduction over loudspeaker arrays due to high computational\ncomplexity and run-time costs. We propose a coupled mixer-limiter-envelope\ndesign formulated as an efficient linear-constrained quadratic program that\nminimizes a distortion objective over multichannel gain variables subject to\nsample mixture constraints. Novel methods for asymmetric constant overlap-add\nwindow optimization, objective function approximation, variable and constraint\nreduction are presented. Experiments demonstrate distortion reduction of the\ncoupled design, and computational trade-offs required for efficient real-time\nprocessing.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u8026\u5408\u7684\u6df7\u97f3\u5668-\u9650\u5236\u5668-\u5305\u7edc\u8bbe\u8ba1\uff0c\u901a\u8fc7\u7ebf\u6027\u7ea6\u675f\u4e8c\u6b21\u89c4\u5212\u964d\u4f4e\u591a\u901a\u9053\u97f3\u9891\u5904\u7406\u7684\u5931\u771f\uff0c\u5e76\u4f18\u5316\u4e86\u8ba1\u7b97\u6548\u7387\u3002", "motivation": "\u4f20\u7edf\u591a\u901a\u9053\u97f3\u9891\u6df7\u97f3\u5668\u548c\u9650\u5236\u5668\u8bbe\u8ba1\u56e0\u9ad8\u8ba1\u7b97\u590d\u6742\u6027\u548c\u8fd0\u884c\u65f6\u6210\u672c\u800c\u89e3\u8026\uff0c\u9650\u5236\u4e86\u5185\u5bb9\u5728\u626c\u58f0\u5668\u9635\u5217\u4e0a\u7684\u518d\u73b0\u6548\u679c\u3002", "method": "\u91c7\u7528\u7ebf\u6027\u7ea6\u675f\u4e8c\u6b21\u89c4\u5212\uff0c\u6700\u5c0f\u5316\u591a\u901a\u9053\u589e\u76ca\u53d8\u91cf\u7684\u5931\u771f\u76ee\u6807\uff0c\u540c\u65f6\u6ee1\u8db3\u6837\u672c\u6df7\u5408\u7ea6\u675f\uff1b\u63d0\u51fa\u4e86\u975e\u5bf9\u79f0\u6052\u5b9a\u91cd\u53e0\u76f8\u52a0\u7a97\u53e3\u4f18\u5316\u3001\u76ee\u6807\u51fd\u6570\u8fd1\u4f3c\u53ca\u53d8\u91cf\u548c\u7ea6\u675f\u51cf\u5c11\u7684\u65b0\u65b9\u6cd5\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8026\u5408\u8bbe\u8ba1\u663e\u8457\u964d\u4f4e\u4e86\u5931\u771f\uff0c\u540c\u65f6\u6743\u8861\u4e86\u5b9e\u65f6\u5904\u7406\u7684\u8ba1\u7b97\u6548\u7387\u3002", "conclusion": "\u8026\u5408\u8bbe\u8ba1\u5728\u591a\u901a\u9053\u97f3\u9891\u5904\u7406\u4e2d\u5b9e\u73b0\u4e86\u5931\u771f\u51cf\u5c11\u548c\u8ba1\u7b97\u6548\u7387\u7684\u5e73\u8861\uff0c\u9002\u7528\u4e8e\u5b9e\u65f6\u5e94\u7528\u3002"}}
{"id": "2507.06744", "pdf": "https://arxiv.org/pdf/2507.06744", "abs": "https://arxiv.org/abs/2507.06744", "authors": ["Yafei Zhang", "Yongle Shang", "Huafeng Li"], "title": "Dual-Granularity Cross-Modal Identity Association for Weakly-Supervised Text-to-Person Image Matching", "categories": ["cs.CV", "cs.LG", "cs.MM"], "comment": null, "summary": "Weakly supervised text-to-person image matching, as a crucial approach to\nreducing models' reliance on large-scale manually labeled samples, holds\nsignificant research value. However, existing methods struggle to predict\ncomplex one-to-many identity relationships, severely limiting performance\nimprovements. To address this challenge, we propose a local-and-global\ndual-granularity identity association mechanism. Specifically, at the local\nlevel, we explicitly establish cross-modal identity relationships within a\nbatch, reinforcing identity constraints across different modalities and\nenabling the model to better capture subtle differences and correlations. At\nthe global level, we construct a dynamic cross-modal identity association\nnetwork with the visual modality as the anchor and introduce a confidence-based\ndynamic adjustment mechanism, effectively enhancing the model's ability to\nidentify weakly associated samples while improving overall sensitivity.\nAdditionally, we propose an information-asymmetric sample pair construction\nmethod combined with consistency learning to tackle hard sample mining and\nenhance model robustness. Experimental results demonstrate that the proposed\nmethod substantially boosts cross-modal matching accuracy, providing an\nefficient and practical solution for text-to-person image matching.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5c40\u90e8\u4e0e\u5168\u5c40\u53cc\u7c92\u5ea6\u8eab\u4efd\u5173\u8054\u673a\u5236\uff0c\u901a\u8fc7\u8de8\u6a21\u6001\u8eab\u4efd\u5173\u7cfb\u589e\u5f3a\u548c\u52a8\u6001\u8c03\u6574\u673a\u5236\uff0c\u663e\u8457\u63d0\u5347\u6587\u672c\u5230\u4eba\u50cf\u5339\u914d\u7684\u51c6\u786e\u6027\u3002", "motivation": "\u5f31\u76d1\u7763\u6587\u672c\u5230\u4eba\u50cf\u5339\u914d\u65b9\u6cd5\u4f9d\u8d56\u5927\u89c4\u6a21\u4eba\u5de5\u6807\u6ce8\u6837\u672c\uff0c\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u5904\u7406\u590d\u6742\u7684\u4e00\u5bf9\u591a\u8eab\u4efd\u5173\u7cfb\uff0c\u9650\u5236\u4e86\u6027\u80fd\u63d0\u5347\u3002", "method": "\u63d0\u51fa\u5c40\u90e8\u4e0e\u5168\u5c40\u53cc\u7c92\u5ea6\u8eab\u4efd\u5173\u8054\u673a\u5236\uff1a\u5c40\u90e8\u5c42\u9762\u663e\u5f0f\u5efa\u7acb\u8de8\u6a21\u6001\u8eab\u4efd\u5173\u7cfb\uff1b\u5168\u5c40\u5c42\u9762\u6784\u5efa\u52a8\u6001\u8de8\u6a21\u6001\u8eab\u4efd\u5173\u8054\u7f51\u7edc\uff0c\u5e76\u5f15\u5165\u57fa\u4e8e\u7f6e\u4fe1\u5ea6\u7684\u52a8\u6001\u8c03\u6574\u673a\u5236\u3002\u540c\u65f6\u7ed3\u5408\u4fe1\u606f\u4e0d\u5bf9\u79f0\u6837\u672c\u5bf9\u6784\u5efa\u548c\u4e00\u81f4\u6027\u5b66\u4e60\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u8de8\u6a21\u6001\u5339\u914d\u7684\u51c6\u786e\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u6587\u672c\u5230\u4eba\u50cf\u5339\u914d\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u5b9e\u7528\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.07074", "pdf": "https://arxiv.org/pdf/2507.07074", "abs": "https://arxiv.org/abs/2507.07074", "authors": ["Farhaan Ebadulla", "Dharini Hindlatti", "Srinivaasan NS", "Apoorva VH", "Ayman Aftab"], "title": "Graph-Based Complexity Metrics for Multi-Agent Curriculum Learning: A Validated Approach to Task Ordering in Cooperative Coordination Environments", "categories": ["cs.MA", "cs.RO"], "comment": "6 Pages, 3 Figures", "summary": "Multi-agent reinforcement learning (MARL) faces significant challenges in\ntask sequencing and curriculum design, particularly for cooperative\ncoordination scenarios. While curriculum learning has demonstrated success in\nsingle-agent domains, principled approaches for multi-agent coordination remain\nlimited due to the absence of validated task complexity metrics. This approach\npresents a graph-based coordination complexity metric that integrates agent\ndependency entropy, spatial interference patterns, and goal overlap analysis to\npredict task difficulty in multi-agent environments. The complexity metric\nachieves strong empirical validation with rho = 0.952 correlation (p < 0.001)\nbetween predicted complexity and empirical difficulty determined by random\nagent performance evaluation. This approach evaluates the curriculum learning\nframework using MADDPG across two distinct coordination environments: achieving\n56x performance improvement in tight coordination tasks (MultiWalker) and\ndemonstrating systematic task progression in cooperative navigation (Simple\nSpread). Through systematic analysis, coordination tightness emerges as a\npredictor of curriculum learning effectiveness, where environments requiring\nstrict agent interdependence benefit substantially from structured progression.\nThis approach provides a validated complexity metric for multi-agent curriculum\ndesign and establishes empirical guidelines for multi-robot coordination\napplications.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u56fe\u7684\u534f\u8c03\u590d\u6742\u5ea6\u5ea6\u91cf\u65b9\u6cd5\uff0c\u7528\u4e8e\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u4e2d\u7684\u4efb\u52a1\u6392\u5e8f\u548c\u8bfe\u7a0b\u8bbe\u8ba1\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "motivation": "\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u5728\u4efb\u52a1\u6392\u5e8f\u548c\u8bfe\u7a0b\u8bbe\u8ba1\u65b9\u9762\u9762\u4e34\u6311\u6218\uff0c\u5c24\u5176\u662f\u5728\u534f\u4f5c\u534f\u8c03\u573a\u666f\u4e2d\uff0c\u7f3a\u4e4f\u6709\u6548\u7684\u4efb\u52a1\u590d\u6742\u5ea6\u5ea6\u91cf\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u56fe\u7684\u534f\u8c03\u590d\u6742\u5ea6\u5ea6\u91cf\u65b9\u6cd5\uff0c\u7ed3\u5408\u667a\u80fd\u4f53\u4f9d\u8d56\u71b5\u3001\u7a7a\u95f4\u5e72\u6270\u6a21\u5f0f\u548c\u76ee\u6807\u91cd\u53e0\u5206\u6790\uff0c\u9884\u6d4b\u591a\u667a\u80fd\u4f53\u73af\u5883\u4e2d\u7684\u4efb\u52a1\u96be\u5ea6\u3002", "result": "\u590d\u6742\u5ea6\u5ea6\u91cf\u4e0e\u968f\u673a\u667a\u80fd\u4f53\u6027\u80fd\u8bc4\u4f30\u7684\u5b9e\u8bc1\u96be\u5ea6\u5177\u6709\u5f3a\u76f8\u5173\u6027\uff08rho = 0.952\uff0cp < 0.001\uff09\u3002\u5728MADDPG\u6846\u67b6\u4e0b\uff0c\u8be5\u65b9\u6cd5\u5728MultiWalker\u548cSimple Spread\u73af\u5883\u4e2d\u5206\u522b\u5b9e\u73b0\u4e8656\u500d\u7684\u6027\u80fd\u63d0\u5347\u548c\u7cfb\u7edf\u4efb\u52a1\u8fdb\u5c55\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u591a\u667a\u80fd\u4f53\u8bfe\u7a0b\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u590d\u6742\u5ea6\u5ea6\u91cf\uff0c\u5e76\u786e\u7acb\u4e86\u591a\u673a\u5668\u4eba\u534f\u8c03\u5e94\u7528\u7684\u5b9e\u8bc1\u6307\u5357\u3002"}}
{"id": "2507.06426", "pdf": "https://arxiv.org/pdf/2507.06426", "abs": "https://arxiv.org/abs/2507.06426", "authors": ["Devin Crowley", "Whitney G. Cole", "Christina M. Hospodar", "Ruiting Shen", "Karen E. Adolph", "Alan Fern"], "title": "Evaluating Robots Like Human Infants: A Case Study of Learned Bipedal Locomotion", "categories": ["cs.RO", "cs.SY", "eess.SY"], "comment": "7 pages, 4 figures, accepted into ICDL 2025 as a contributed paper", "summary": "Typically, learned robot controllers are trained via relatively unsystematic\nregimens and evaluated with coarse-grained outcome measures such as average\ncumulative reward. The typical approach is useful to compare learning\nalgorithms but provides limited insight into the effects of different training\nregimens and little understanding about the richness and complexity of learned\nbehaviors. Likewise, human infants and other animals are \"trained\" via\nunsystematic regimens, but in contrast, developmental psychologists evaluate\ntheir performance in highly-controlled experiments with fine-grained measures\nsuch as success, speed of walking, and prospective adjustments. However, the\nstudy of learned behavior in human infants is limited by the practical\nconstraints of training and testing babies. Here, we present a case study that\napplies methods from developmental psychology to study the learned behavior of\nthe simulated bipedal robot Cassie. Following research on infant walking, we\nsystematically designed reinforcement learning training regimens and tested the\nresulting controllers in simulated environments analogous to those used for\nbabies--but without the practical constraints. Results reveal new insights into\nthe behavioral impact of different training regimens and the development of\nCassie's learned behaviors relative to infants who are learning to walk. This\ninterdisciplinary baby-robot approach provides inspiration for future research\ndesigned to systematically test effects of training on the development of\ncomplex learned robot behaviors.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u53d1\u5c55\u5fc3\u7406\u5b66\u65b9\u6cd5\u7814\u7a76\u673a\u5668\u4eba\u5b66\u4e60\u884c\u4e3a\u7684\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u7cfb\u7edf\u5316\u8bad\u7ec3\u548c\u7cbe\u7ec6\u6d4b\u91cf\uff0c\u63ed\u793a\u4e86\u8bad\u7ec3\u65b9\u6848\u5bf9\u673a\u5668\u4eba\u884c\u4e3a\u7684\u5f71\u54cd\u3002", "motivation": "\u4f20\u7edf\u673a\u5668\u4eba\u63a7\u5236\u5668\u8bad\u7ec3\u65b9\u6cd5\u7f3a\u4e4f\u7cfb\u7edf\u6027\uff0c\u4e14\u8bc4\u4f30\u6307\u6807\u7c97\u7cd9\uff0c\u65e0\u6cd5\u6df1\u5165\u7406\u89e3\u8bad\u7ec3\u65b9\u6848\u5bf9\u884c\u4e3a\u7684\u5f71\u54cd\u3002\u53d1\u5c55\u5fc3\u7406\u5b66\u5bf9\u5a74\u513f\u7684\u7814\u7a76\u65b9\u6cd5\u63d0\u4f9b\u4e86\u7cbe\u7ec6\u6d4b\u91cf\u624b\u6bb5\uff0c\u4f46\u53d7\u9650\u4e8e\u5b9e\u9645\u7ea6\u675f\u3002", "method": "\u91c7\u7528\u53d1\u5c55\u5fc3\u7406\u5b66\u65b9\u6cd5\uff0c\u8bbe\u8ba1\u7cfb\u7edf\u5316\u7684\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u65b9\u6848\uff0c\u5e76\u5728\u6a21\u62df\u73af\u5883\u4e2d\u6d4b\u8bd5\u53cc\u8db3\u673a\u5668\u4ebaCassie\u7684\u884c\u4e3a\u3002", "result": "\u63ed\u793a\u4e86\u4e0d\u540c\u8bad\u7ec3\u65b9\u6848\u5bf9\u673a\u5668\u4eba\u884c\u4e3a\u7684\u5f71\u54cd\uff0c\u5e76\u6bd4\u8f83\u4e86Cassie\u4e0e\u5a74\u513f\u5b66\u4e60\u884c\u8d70\u7684\u884c\u4e3a\u53d1\u5c55\u3002", "conclusion": "\u8de8\u5b66\u79d1\u7684\u5a74\u513f-\u673a\u5668\u4eba\u7814\u7a76\u65b9\u6cd5\u4e3a\u672a\u6765\u7cfb\u7edf\u5316\u6d4b\u8bd5\u8bad\u7ec3\u5bf9\u590d\u6742\u673a\u5668\u4eba\u884c\u4e3a\u53d1\u5c55\u7684\u5f71\u54cd\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2507.06335", "pdf": "https://arxiv.org/pdf/2507.06335", "abs": "https://arxiv.org/abs/2507.06335", "authors": ["Casey Kennington", "David Schlangen"], "title": "Could the Road to Grounded, Neuro-symbolic AI be Paved with Words-as-Classifiers?", "categories": ["cs.CL"], "comment": "9 pages", "summary": "Formal, Distributional, and Grounded theories of computational semantics each\nhave their uses and their drawbacks. There has been a shift to ground models of\nlanguage by adding visual knowledge, and there has been a call to enrich models\nof language with symbolic methods to gain the benefits from formal,\ndistributional, and grounded theories. In this paper, we attempt to make the\ncase that one potential path forward in unifying all three semantic fields is\npaved with the words-as-classifier model, a model of word-level grounded\nsemantics that has been incorporated into formalisms and distributional\nlanguage models in the literature, and it has been well-tested within\ninteractive dialogue settings. We review that literature, motivate the\nwords-as-classifiers model with an appeal to recent work in cognitive science,\nand describe a small experiment. Finally, we sketch a model of semantics\nunified through words-as-classifiers.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u901a\u8fc7\u201c\u8bcd\u4f5c\u4e3a\u5206\u7c7b\u5668\u201d\u6a21\u578b\u7edf\u4e00\u5f62\u5f0f\u3001\u5206\u5e03\u548c\u63a5\u5730\u8bed\u4e49\u7406\u8bba\uff0c\u7ed3\u5408\u8ba4\u77e5\u79d1\u5b66\u548c\u5b9e\u9a8c\u9a8c\u8bc1\u5176\u6f5c\u529b\u3002", "motivation": "\u5f62\u5f0f\u3001\u5206\u5e03\u548c\u63a5\u5730\u8bed\u4e49\u7406\u8bba\u5404\u6709\u4f18\u7f3a\u70b9\uff0c\u9700\u8981\u4e00\u79cd\u65b9\u6cd5\u7ed3\u5408\u4e09\u8005\u4f18\u52bf\u3002", "method": "\u56de\u987e\u6587\u732e\uff0c\u7ed3\u5408\u8ba4\u77e5\u79d1\u5b66\uff0c\u63d0\u51fa\u201c\u8bcd\u4f5c\u4e3a\u5206\u7c7b\u5668\u201d\u6a21\u578b\uff0c\u5e76\u8fdb\u884c\u5c0f\u89c4\u6a21\u5b9e\u9a8c\u9a8c\u8bc1\u3002", "result": "\u201c\u8bcd\u4f5c\u4e3a\u5206\u7c7b\u5668\u201d\u6a21\u578b\u5728\u5bf9\u8bdd\u73af\u5883\u4e2d\u8868\u73b0\u826f\u597d\uff0c\u6709\u671b\u7edf\u4e00\u4e09\u79cd\u8bed\u4e49\u7406\u8bba\u3002", "conclusion": "\u201c\u8bcd\u4f5c\u4e3a\u5206\u7c7b\u5668\u201d\u6a21\u578b\u4e3a\u7edf\u4e00\u5f62\u5f0f\u3001\u5206\u5e03\u548c\u63a5\u5730\u8bed\u4e49\u63d0\u4f9b\u4e86\u53ef\u884c\u8def\u5f84\u3002"}}
{"id": "2507.06366", "pdf": "https://arxiv.org/pdf/2507.06366", "abs": "https://arxiv.org/abs/2507.06366", "authors": ["Yupu Zhang", "Zelin Xu", "Tingsong Xiao", "Gustavo Seabra", "Yanjun Li", "Chenglong Li", "Zhe Jiang"], "title": "DecoyDB: A Dataset for Graph Contrastive Learning in Protein-Ligand Binding Affinity Prediction", "categories": ["cs.LG", "q-bio.BM"], "comment": null, "summary": "Predicting the binding affinity of protein-ligand complexes plays a vital\nrole in drug discovery. Unfortunately, progress has been hindered by the lack\nof large-scale and high-quality binding affinity labels. The widely used\nPDBbind dataset has fewer than 20K labeled complexes. Self-supervised learning,\nespecially graph contrastive learning (GCL), provides a unique opportunity to\nbreak the barrier by pre-training graph neural network models based on vast\nunlabeled complexes and fine-tuning the models on much fewer labeled complexes.\nHowever, the problem faces unique challenges, including a lack of a\ncomprehensive unlabeled dataset with well-defined positive/negative complex\npairs and the need to design GCL algorithms that incorporate the unique\ncharacteristics of such data. To fill the gap, we propose DecoyDB, a\nlarge-scale, structure-aware dataset specifically designed for self-supervised\nGCL on protein-ligand complexes. DecoyDB consists of high-resolution ground\ntruth complexes (less than 2.5 Angstrom) and diverse decoy structures with\ncomputationally generated binding poses that range from realistic to suboptimal\n(negative pairs). Each decoy is annotated with a Root Mean Squared Deviation\n(RMSD) from the native pose. We further design a customized GCL framework to\npre-train graph neural networks based on DecoyDB and fine-tune the models with\nlabels from PDBbind. Extensive experiments confirm that models pre-trained with\nDecoyDB achieve superior accuracy, label efficiency, and generalizability.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faDecoyDB\u6570\u636e\u96c6\u548c\u5b9a\u5236\u5316GCL\u6846\u67b6\uff0c\u7528\u4e8e\u86cb\u767d\u8d28-\u914d\u4f53\u590d\u5408\u7269\u7684\u81ea\u76d1\u7763\u5b66\u4e60\uff0c\u663e\u8457\u63d0\u5347\u4e86\u7ed3\u5408\u4eb2\u548c\u529b\u9884\u6d4b\u7684\u51c6\u786e\u6027\u3002", "motivation": "\u836f\u7269\u53d1\u73b0\u4e2d\u86cb\u767d\u8d28-\u914d\u4f53\u7ed3\u5408\u4eb2\u548c\u529b\u9884\u6d4b\u7f3a\u4e4f\u5927\u89c4\u6a21\u9ad8\u8d28\u91cf\u6807\u8bb0\u6570\u636e\uff0c\u73b0\u6709\u6570\u636e\u96c6\u5982PDBbind\u89c4\u6a21\u6709\u9650\u3002", "method": "\u6784\u5efaDecoyDB\u6570\u636e\u96c6\uff08\u5305\u542b\u9ad8\u8d28\u91cf\u590d\u5408\u7269\u548c\u591a\u6837\u5316\u7684\u8d1f\u6837\u672c\uff09\uff0c\u5e76\u8bbe\u8ba1\u5b9a\u5236\u5316GCL\u6846\u67b6\u8fdb\u884c\u9884\u8bad\u7ec3\u548c\u5fae\u8c03\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u57fa\u4e8eDecoyDB\u9884\u8bad\u7ec3\u7684\u6a21\u578b\u5728\u51c6\u786e\u6027\u3001\u6807\u7b7e\u6548\u7387\u548c\u6cdb\u5316\u6027\u4e0a\u8868\u73b0\u4f18\u8d8a\u3002", "conclusion": "DecoyDB\u548c\u5b9a\u5236\u5316GCL\u6846\u67b6\u4e3a\u86cb\u767d\u8d28-\u914d\u4f53\u7ed3\u5408\u4eb2\u548c\u529b\u9884\u6d4b\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.06272", "pdf": "https://arxiv.org/pdf/2507.06272", "abs": "https://arxiv.org/abs/2507.06272", "authors": ["Zhang Li", "Biao Yang", "Qiang Liu", "Shuo Zhang", "Zhiyin Ma", "Shuo Zhang", "Liang Yin", "Linger Deng", "Yabo Sun", "Yuliang Liu", "Xiang Bai"], "title": "LIRA: Inferring Segmentation in Large Multi-modal Models with Local Interleaved Region Assistance", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "While large multi-modal models (LMMs) demonstrate promising capabilities in\nsegmentation and comprehension, they still struggle with two limitations:\ninaccurate segmentation and hallucinated comprehension. These challenges stem\nprimarily from constraints in weak visual comprehension and a lack of\nfine-grained perception. To alleviate these limitations, we propose LIRA, a\nframework that capitalizes on the complementary relationship between visual\ncomprehension and segmentation via two key components: (1) Semantic-Enhanced\nFeature Extractor (SEFE) improves object attribute inference by fusing semantic\nand pixel-level features, leading to more accurate segmentation; (2)\nInterleaved Local Visual Coupling (ILVC) autoregressively generates local\ndescriptions after extracting local features based on segmentation masks,\noffering fine-grained supervision to mitigate hallucinations. Furthermore, we\nfind that the precision of object segmentation is positively correlated with\nthe latent related semantics of the <seg> token. To quantify this relationship\nand the model's potential semantic inferring ability, we introduce the\nAttributes Evaluation (AttrEval) dataset. Our experiments show that LIRA\nachieves state-of-the-art performance in both segmentation and comprehension\ntasks. Code will be available at https://github.com/echo840/LIRA.", "AI": {"tldr": "LIRA\u6846\u67b6\u901a\u8fc7\u7ed3\u5408\u8bed\u4e49\u589e\u5f3a\u7279\u5f81\u63d0\u53d6\u5668\u548c\u4ea4\u9519\u5c40\u90e8\u89c6\u89c9\u8026\u5408\uff0c\u89e3\u51b3\u4e86\u591a\u6a21\u6001\u6a21\u578b\u5728\u5206\u5272\u548c\u7406\u89e3\u4efb\u52a1\u4e2d\u7684\u4e0d\u51c6\u786e\u548c\u5e7b\u89c9\u95ee\u9898\uff0c\u5e76\u5728\u5b9e\u9a8c\u4e2d\u8868\u73b0\u51fa\u8272\u3002", "motivation": "\u5927\u578b\u591a\u6a21\u6001\u6a21\u578b\u5728\u5206\u5272\u548c\u7406\u89e3\u4efb\u52a1\u4e2d\u5b58\u5728\u4e0d\u51c6\u786e\u5206\u5272\u548c\u5e7b\u89c9\u7406\u89e3\u7684\u95ee\u9898\uff0c\u4e3b\u8981\u6e90\u4e8e\u89c6\u89c9\u7406\u89e3\u80fd\u529b\u5f31\u548c\u7f3a\u4e4f\u7ec6\u7c92\u5ea6\u611f\u77e5\u3002", "method": "\u63d0\u51faLIRA\u6846\u67b6\uff0c\u5305\u542b\u8bed\u4e49\u589e\u5f3a\u7279\u5f81\u63d0\u53d6\u5668\uff08SEFE\uff09\u548c\u4ea4\u9519\u5c40\u90e8\u89c6\u89c9\u8026\u5408\uff08ILVC\uff09\uff0c\u5206\u522b\u63d0\u5347\u5206\u5272\u51c6\u786e\u6027\u548c\u51cf\u5c11\u5e7b\u89c9\u7406\u89e3\u3002", "result": "LIRA\u5728\u5206\u5272\u548c\u7406\u89e3\u4efb\u52a1\u4e2d\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\uff0c\u5e76\u901a\u8fc7AttrEval\u6570\u636e\u96c6\u91cf\u5316\u4e86\u8bed\u4e49\u63a8\u65ad\u80fd\u529b\u3002", "conclusion": "LIRA\u901a\u8fc7\u7ed3\u5408\u89c6\u89c9\u7406\u89e3\u548c\u5206\u5272\u7684\u4e92\u8865\u5173\u7cfb\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u73b0\u6709\u6a21\u578b\u7684\u5c40\u9650\u6027\uff0c\u5e76\u5c55\u793a\u4e86\u4f18\u8d8a\u6027\u80fd\u3002"}}
{"id": "2507.07017", "pdf": "https://arxiv.org/pdf/2507.07017", "abs": "https://arxiv.org/abs/2507.07017", "authors": ["Tianyu Zheng", "Tianshun Xing", "Qingshui Gu", "Taoran Liang", "Xingwei Qu", "Xin Zhou", "Yizhi Li", "Zhoufutu Wen", "Chenghua Lin", "Wenhao Huang", "Qian Liu", "Ge Zhang", "Zejun Ma"], "title": "First Return, Entropy-Eliciting Explore", "categories": ["cs.AI"], "comment": null, "summary": "Reinforcement Learning from Verifiable Rewards (RLVR) improves the reasoning\nabilities of Large Language Models (LLMs) but it struggles with unstable\nexploration. We propose FR3E (First Return, Entropy-Eliciting Explore), a\nstructured exploration framework that identifies high-uncertainty decision\npoints in reasoning trajectories and performs targeted rollouts to construct\nsemantically grounded intermediate feedback. Our method provides targeted\nguidance without relying on dense supervision. Empirical results on\nmathematical reasoning benchmarks(AIME24) show that FR3E promotes more stable\ntraining, produces longer and more coherent responses, and increases the\nproportion of fully correct trajectories. These results highlight the\nframework's effectiveness in improving LLM reasoning through more robust and\nstructured exploration.", "AI": {"tldr": "FR3E\u6846\u67b6\u901a\u8fc7\u7ed3\u6784\u5316\u63a2\u7d22\u63d0\u5347LLM\u63a8\u7406\u80fd\u529b\uff0c\u89e3\u51b3RLVR\u7684\u4e0d\u7a33\u5b9a\u63a2\u7d22\u95ee\u9898\u3002", "motivation": "RLVR\u63d0\u5347LLM\u63a8\u7406\u80fd\u529b\u4f46\u5b58\u5728\u63a2\u7d22\u4e0d\u7a33\u5b9a\u95ee\u9898\u3002", "method": "\u63d0\u51faFR3E\u6846\u67b6\uff0c\u8bc6\u522b\u9ad8\u4e0d\u786e\u5b9a\u6027\u51b3\u7b56\u70b9\u5e76\u8fdb\u884c\u9488\u5bf9\u6027\u63a2\u7d22\u3002", "result": "\u5728\u6570\u5b66\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cFR3E\u63d0\u5347\u8bad\u7ec3\u7a33\u5b9a\u6027\u3001\u751f\u6210\u66f4\u957f\u4e14\u8fde\u8d2f\u7684\u56de\u7b54\uff0c\u5e76\u589e\u52a0\u5b8c\u5168\u6b63\u786e\u8f68\u8ff9\u6bd4\u4f8b\u3002", "conclusion": "FR3E\u901a\u8fc7\u7ed3\u6784\u5316\u63a2\u7d22\u6709\u6548\u63d0\u5347LLM\u63a8\u7406\u80fd\u529b\u3002"}}
{"id": "2507.06794", "pdf": "https://arxiv.org/pdf/2507.06794", "abs": "https://arxiv.org/abs/2507.06794", "authors": ["Anastasia Ananeva", "Anton Tomilov", "Marina Volkova"], "title": "Revealing the Hidden Temporal Structure of HubertSoft Embeddings based on the Russian Phonetic Corpus", "categories": ["cs.SD", "eess.AS"], "comment": "11 pages, 5 figures, Specom 2025 conference", "summary": "Self-supervised learning (SSL) models such as Wav2Vec 2.0 and HuBERT have\nshown remarkable success in extracting phonetic information from raw audio\nwithout labelled data. While prior work has demonstrated that SSL embeddings\nencode phonetic features at the frame level, it remains unclear whether these\nmodels preserve temporal structure, specifically, whether embeddings at phoneme\nboundaries reflect the identity and order of adjacent phonemes. This study\ninvestigates the extent to which boundary-sensitive embeddings from HubertSoft,\na soft-clustering variant of HuBERT, encode phoneme transitions. Using the\nCORPRES Russian speech corpus, we labelled 20 ms embedding windows with\ntriplets of phonemes corresponding to their start, centre, and end segments. A\nneural network was trained to predict these positions separately, and multiple\nevaluation metrics, such as ordered, unordered accuracy and a flexible centre\naccuracy, were used to assess temporal sensitivity. Results show that\nembeddings extracted at phoneme boundaries capture both phoneme identity and\ntemporal order, with especially high accuracy at segment boundaries. Confusion\npatterns further suggest that the model encodes articulatory detail and\ncoarticulatory effects. These findings contribute to our understanding of the\ninternal structure of SSL speech representations and their potential for\nphonological analysis and fine-grained transcription tasks.", "AI": {"tldr": "\u7814\u7a76\u63a2\u8ba8\u4e86HuBERTSoft\u6a21\u578b\u5728\u97f3\u7d20\u8fb9\u754c\u5904\u63d0\u53d6\u7684\u5d4c\u5165\u662f\u5426\u80fd\u591f\u6355\u6349\u97f3\u7d20\u8eab\u4efd\u548c\u65f6\u5e8f\u7ed3\u6784\uff0c\u53d1\u73b0\u5176\u80fd\u6709\u6548\u7f16\u7801\u97f3\u7d20\u8fc7\u6e21\u4fe1\u606f\u3002", "motivation": "\u5c3d\u7ba1\u81ea\u76d1\u7763\u5b66\u4e60\u6a21\u578b\u5df2\u8bc1\u660e\u80fd\u4ece\u539f\u59cb\u97f3\u9891\u4e2d\u63d0\u53d6\u97f3\u7d20\u7279\u5f81\uff0c\u4f46\u5176\u662f\u5426\u4fdd\u7559\u65f6\u5e8f\u7ed3\u6784\uff08\u5982\u97f3\u7d20\u8fb9\u754c\u5904\u7684\u5d4c\u5165\u662f\u5426\u53cd\u6620\u76f8\u90bb\u97f3\u7d20\u7684\u8eab\u4efd\u548c\u987a\u5e8f\uff09\u5c1a\u4e0d\u660e\u786e\u3002", "method": "\u4f7f\u7528CORPRES\u4fc4\u8bed\u8bed\u97f3\u8bed\u6599\u5e93\uff0c\u6807\u8bb020\u6beb\u79d2\u5d4c\u5165\u7a97\u53e3\u7684\u97f3\u7d20\u4e09\u5143\u7ec4\uff08\u5f00\u59cb\u3001\u4e2d\u5fc3\u3001\u7ed3\u675f\uff09\uff0c\u8bad\u7ec3\u795e\u7ecf\u7f51\u7edc\u9884\u6d4b\u8fd9\u4e9b\u4f4d\u7f6e\uff0c\u5e76\u901a\u8fc7\u591a\u79cd\u8bc4\u4f30\u6307\u6807\u5206\u6790\u65f6\u5e8f\u654f\u611f\u6027\u3002", "result": "\u7ed3\u679c\u663e\u793a\uff0c\u97f3\u7d20\u8fb9\u754c\u5904\u7684\u5d4c\u5165\u80fd\u6355\u6349\u97f3\u7d20\u8eab\u4efd\u548c\u65f6\u5e8f\u987a\u5e8f\uff0c\u8fb9\u754c\u5904\u51c6\u786e\u7387\u5c24\u5176\u9ad8\uff0c\u4e14\u6a21\u578b\u7f16\u7801\u4e86\u53d1\u97f3\u7ec6\u8282\u548c\u534f\u540c\u53d1\u97f3\u6548\u5e94\u3002", "conclusion": "\u7814\u7a76\u589e\u8fdb\u4e86\u5bf9\u81ea\u76d1\u7763\u5b66\u4e60\u8bed\u97f3\u8868\u793a\u5185\u90e8\u7ed3\u6784\u7684\u7406\u89e3\uff0c\u8868\u660e\u5176\u5728\u97f3\u7cfb\u5206\u6790\u548c\u7ec6\u7c92\u5ea6\u8f6c\u5f55\u4efb\u52a1\u4e2d\u5177\u6709\u6f5c\u529b\u3002"}}
{"id": "2507.06821", "pdf": "https://arxiv.org/pdf/2507.06821", "abs": "https://arxiv.org/abs/2507.06821", "authors": ["Chuhang Zheng", "Chunwei Tian", "Jie Wen", "Daoqiang Zhang", "Qi Zhu"], "title": "HeLo: Heterogeneous Multi-Modal Fusion with Label Correlation for Emotion Distribution Learning", "categories": ["cs.LG", "cs.AI", "cs.MM"], "comment": null, "summary": "Multi-modal emotion recognition has garnered increasing attention as it plays\na significant role in human-computer interaction (HCI) in recent years. Since\ndifferent discrete emotions may exist at the same time, compared with\nsingle-class emotion recognition, emotion distribution learning (EDL) that\nidentifies a mixture of basic emotions has gradually emerged as a trend.\nHowever, existing EDL methods face challenges in mining the heterogeneity among\nmultiple modalities. Besides, rich semantic correlations across arbitrary basic\nemotions are not fully exploited. In this paper, we propose a multi-modal\nemotion distribution learning framework, named HeLo, aimed at fully exploring\nthe heterogeneity and complementary information in multi-modal emotional data\nand label correlation within mixed basic emotions. Specifically, we first adopt\ncross-attention to effectively fuse the physiological data. Then, an optimal\ntransport (OT)-based heterogeneity mining module is devised to mine the\ninteraction and heterogeneity between the physiological and behavioral\nrepresentations. To facilitate label correlation learning, we introduce a\nlearnable label embedding optimized by correlation matrix alignment. Finally,\nthe learnable label embeddings and label correlation matrices are integrated\nwith the multi-modal representations through a novel label correlation-driven\ncross-attention mechanism for accurate emotion distribution learning.\nExperimental results on two publicly available datasets demonstrate the\nsuperiority of our proposed method in emotion distribution learning.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u6a21\u6001\u60c5\u611f\u5206\u5e03\u5b66\u4e60\u6846\u67b6HeLo\uff0c\u901a\u8fc7\u8de8\u6a21\u6001\u6ce8\u610f\u529b\u878d\u5408\u751f\u7406\u6570\u636e\uff0c\u5229\u7528\u6700\u4f18\u4f20\u8f93\u6316\u6398\u5f02\u8d28\u6027\uff0c\u5e76\u901a\u8fc7\u53ef\u5b66\u4e60\u6807\u7b7e\u5d4c\u5165\u4f18\u5316\u6807\u7b7e\u76f8\u5173\u6027\uff0c\u5b9e\u9a8c\u8bc1\u660e\u5176\u4f18\u8d8a\u6027\u3002", "motivation": "\u591a\u6a21\u6001\u60c5\u611f\u8bc6\u522b\u5728HCI\u4e2d\u65e5\u76ca\u91cd\u8981\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u5728\u6316\u6398\u6a21\u6001\u5f02\u8d28\u6027\u548c\u6807\u7b7e\u76f8\u5173\u6027\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\u3002", "method": "\u91c7\u7528\u8de8\u6ce8\u610f\u529b\u878d\u5408\u751f\u7406\u6570\u636e\uff0c\u8bbe\u8ba1\u6700\u4f18\u4f20\u8f93\u6a21\u5757\u6316\u6398\u5f02\u8d28\u6027\uff0c\u5f15\u5165\u53ef\u5b66\u4e60\u6807\u7b7e\u5d4c\u5165\u4f18\u5316\u6807\u7b7e\u76f8\u5173\u6027\u3002", "result": "\u5728\u4e24\u4e2a\u516c\u5f00\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u4f18\u8d8a\u6027\u3002", "conclusion": "HeLo\u6846\u67b6\u5728\u591a\u6a21\u6001\u60c5\u611f\u5206\u5e03\u5b66\u4e60\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u6a21\u6001\u5f02\u8d28\u6027\u548c\u6807\u7b7e\u76f8\u5173\u6027\u95ee\u9898\u3002"}}
{"id": "2507.06310", "pdf": "https://arxiv.org/pdf/2507.06310", "abs": "https://arxiv.org/abs/2507.06310", "authors": ["Yongchao Zeng", "Calum Brown", "Mark Rounsevell"], "title": "Too Human to Model:The Uncanny Valley of LLMs in Social Simulation -- When Generative Language Agents Misalign with Modelling Principles", "categories": ["cs.CY", "cs.AI", "cs.MA"], "comment": null, "summary": "Large language models (LLMs) have been increasingly used to build agents in\nsocial simulation because of their impressive abilities to generate fluent,\ncontextually coherent dialogues. Such abilities can enhance the realism of\nmodels. However, the pursuit of realism is not necessarily compatible with the\nepistemic foundation of modelling. We argue that LLM agents, in many regards,\nare too human to model: they are too expressive, detailed and intractable to be\nconsistent with the abstraction, simplification, and interpretability typically\ndemanded by modelling. Through a model-building thought experiment that\nconverts the Bass diffusion model to an LLM-based variant, we uncover five core\ndilemmas: a temporal resolution mismatch between natural conversation and\nabstract time steps; the need for intervention in conversations while avoiding\nundermining spontaneous agent outputs; the temptation to introduce rule-like\ninstructions in prompts while maintaining conversational naturalness; the\ntension between role consistency and role evolution across time; and the\nchallenge of understanding emergence, where system-level patterns become\nobscured by verbose micro textual outputs. These dilemmas steer the LLM agents\ntowards an uncanny valley: not abstract enough to clarify underlying social\nmechanisms, while not natural enough to represent realistic human behaviour.\nThis exposes an important paradox: the realism of LLM agents can obscure,\nrather than clarify, social dynamics when misapplied. We tease out the\nconditions in which LLM agents are ideally suited: where system-level emergence\nis not the focus, linguistic nuances and meaning are central, interactions\nunfold in natural time, and stable role identity is more important than\nlong-term behavioural evolution. We call for repositioning LLM agents in the\necosystem of social simulation for future applications.", "AI": {"tldr": "LLM agents enhance social simulation realism but may conflict with modeling's epistemic needs, revealing dilemmas like abstraction vs. naturalness and role consistency vs. evolution.", "motivation": "To explore the compatibility of LLM agents' realism with the abstraction and interpretability required in social modeling.", "method": "A thought experiment converting the Bass diffusion model to an LLM-based variant, identifying five core dilemmas.", "result": "LLM agents risk an uncanny valley: neither abstract enough for clear mechanisms nor natural enough for realistic behavior.", "conclusion": "LLM agents are best suited for scenarios focusing on linguistic nuances and stable roles, not system-level emergence."}}
{"id": "2507.06519", "pdf": "https://arxiv.org/pdf/2507.06519", "abs": "https://arxiv.org/abs/2507.06519", "authors": ["Yuhan Liu", "Xinyu Zhang", "Haonan Chang", "Abdeslam Boularias"], "title": "Failure Forecasting Boosts Robustness of Sim2Real Rhythmic Insertion Policies", "categories": ["cs.RO", "cs.AI"], "comment": "Accepted at IROS2025. Project website:\n  https://jaysparrow.github.io/rit", "summary": "This paper addresses the challenges of Rhythmic Insertion Tasks (RIT), where\na robot must repeatedly perform high-precision insertions, such as screwing a\nnut into a bolt with a wrench. The inherent difficulty of RIT lies in achieving\nmillimeter-level accuracy and maintaining consistent performance over multiple\nrepetitions, particularly when factors like nut rotation and friction introduce\nadditional complexity. We propose a sim-to-real framework that integrates a\nreinforcement learning-based insertion policy with a failure forecasting\nmodule. By representing the wrench's pose in the nut's coordinate frame rather\nthan the robot's frame, our approach significantly enhances sim-to-real\ntransferability. The insertion policy, trained in simulation, leverages\nreal-time 6D pose tracking to execute precise alignment, insertion, and\nrotation maneuvers. Simultaneously, a neural network predicts potential\nexecution failures, triggering a simple recovery mechanism that lifts the\nwrench and retries the insertion. Extensive experiments in both simulated and\nreal-world environments demonstrate that our method not only achieves a high\none-time success rate but also robustly maintains performance over long-horizon\nrepetitive tasks.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9488\u5bf9\u9ad8\u7cbe\u5ea6\u91cd\u590d\u63d2\u5165\u4efb\u52a1\uff08RIT\uff09\u7684\u6a21\u62df\u5230\u73b0\u5b9e\u6846\u67b6\uff0c\u7ed3\u5408\u5f3a\u5316\u5b66\u4e60\u7b56\u7565\u548c\u6545\u969c\u9884\u6d4b\u6a21\u5757\uff0c\u663e\u8457\u63d0\u5347\u4e86\u4efb\u52a1\u7684\u6210\u529f\u7387\u548c\u9c81\u68d2\u6027\u3002", "motivation": "\u89e3\u51b3\u673a\u5668\u4eba\u6267\u884c\u9ad8\u7cbe\u5ea6\u91cd\u590d\u63d2\u5165\u4efb\u52a1\uff08\u5982\u62e7\u87ba\u6bcd\uff09\u65f6\u9762\u4e34\u7684\u6beb\u7c73\u7ea7\u7cbe\u5ea6\u548c\u957f\u671f\u7a33\u5b9a\u6027\u95ee\u9898\uff0c\u7279\u522b\u662f\u7531\u87ba\u6bcd\u65cb\u8f6c\u548c\u6469\u64e6\u7b49\u56e0\u7d20\u5e26\u6765\u7684\u590d\u6742\u6027\u3002", "method": "\u91c7\u7528\u6a21\u62df\u5230\u73b0\u5b9e\u6846\u67b6\uff0c\u7ed3\u5408\u5f3a\u5316\u5b66\u4e60\u63d2\u5165\u7b56\u7565\u548c\u6545\u969c\u9884\u6d4b\u6a21\u5757\uff0c\u901a\u8fc7\u5c06\u5de5\u5177\u59ff\u6001\u8868\u793a\u4e3a\u87ba\u6bcd\u5750\u6807\u7cfb\u800c\u975e\u673a\u5668\u4eba\u5750\u6807\u7cfb\uff0c\u63d0\u5347\u6a21\u62df\u5230\u73b0\u5b9e\u7684\u8fc1\u79fb\u6027\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u4e0d\u4ec5\u5b9e\u73b0\u4e86\u4e00\u6b21\u6027\u9ad8\u6210\u529f\u7387\uff0c\u8fd8\u80fd\u5728\u957f\u671f\u91cd\u590d\u4efb\u52a1\u4e2d\u4fdd\u6301\u7a33\u5b9a\u6027\u80fd\u3002", "conclusion": "\u63d0\u51fa\u7684\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86RIT\u4efb\u52a1\u7684\u9ad8\u7cbe\u5ea6\u548c\u7a33\u5b9a\u6027\u95ee\u9898\uff0c\u4e3a\u7c7b\u4f3c\u4efb\u52a1\u63d0\u4f9b\u4e86\u53ef\u884c\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.06378", "pdf": "https://arxiv.org/pdf/2507.06378", "abs": "https://arxiv.org/abs/2507.06378", "authors": ["Catherine Arnett", "Marisa Hudspeth", "Brendan O'Connor"], "title": "Evaluating Morphological Alignment of Tokenizers in 70 Languages", "categories": ["cs.CL"], "comment": "6 pages, 3 figures. Accepted to the Tokenization Workshop at ICML\n  2025", "summary": "While tokenization is a key step in language modeling, with effects on model\ntraining and performance, it remains unclear how to effectively evaluate\ntokenizer quality. One proposed dimension of tokenizer quality is the extent to\nwhich tokenizers preserve linguistically meaningful subwords, aligning token\nboundaries with morphological boundaries within a word. We expand MorphScore\n(Arnett & Bergen, 2025), which previously covered 22 languages, to support a\ntotal of 70 languages. The updated MorphScore offers more flexibility in\nevaluation and addresses some of the limitations of the original version. We\nthen correlate our alignment scores with downstream task performance for five\npre-trained languages models on seven tasks, with at least one task in each of\nthe languages in our sample. We find that morphological alignment does not\nexplain very much variance in model performance, suggesting that morphological\nalignment alone does not measure dimensions of tokenization quality relevant to\nmodel performance.", "AI": {"tldr": "\u8bba\u6587\u6269\u5c55\u4e86MorphScore\u4ee5\u652f\u6301\u66f4\u591a\u8bed\u8a00\uff0c\u5e76\u63a2\u8ba8\u4e86\u5206\u8bcd\u5668\u8d28\u91cf\u4e0e\u6a21\u578b\u6027\u80fd\u7684\u5173\u7cfb\uff0c\u53d1\u73b0\u5f62\u6001\u5bf9\u9f50\u5bf9\u6027\u80fd\u5f71\u54cd\u6709\u9650\u3002", "motivation": "\u8bc4\u4f30\u5206\u8bcd\u5668\u8d28\u91cf\u7684\u65b9\u6cd5\u5c1a\u4e0d\u660e\u786e\uff0c\u5c24\u5176\u662f\u5206\u8bcd\u5668\u662f\u5426\u4fdd\u7559\u8bed\u8a00\u5b66\u4e0a\u6709\u610f\u4e49\u7684\u5b50\u8bcd\u3002", "method": "\u6269\u5c55MorphScore\u81f370\u79cd\u8bed\u8a00\uff0c\u5e76\u5206\u6790\u5f62\u6001\u5bf9\u9f50\u5206\u6570\u4e0e\u4e0b\u6e38\u4efb\u52a1\u6027\u80fd\u7684\u76f8\u5173\u6027\u3002", "result": "\u5f62\u6001\u5bf9\u9f50\u5206\u6570\u5bf9\u6a21\u578b\u6027\u80fd\u7684\u65b9\u5dee\u89e3\u91ca\u529b\u8f83\u5f31\u3002", "conclusion": "\u4ec5\u5f62\u6001\u5bf9\u9f50\u4e0d\u8db3\u4ee5\u8861\u91cf\u4e0e\u6a21\u578b\u6027\u80fd\u76f8\u5173\u7684\u5206\u8bcd\u5668\u8d28\u91cf\u3002"}}
{"id": "2507.06367", "pdf": "https://arxiv.org/pdf/2507.06367", "abs": "https://arxiv.org/abs/2507.06367", "authors": ["El Mehdi Achour", "Kathl\u00e9n Kohn", "Holger Rauhut"], "title": "The Riemannian Geometry associated to Gradient Flows of Linear Convolutional Networks", "categories": ["cs.LG", "math.AG"], "comment": null, "summary": "We study geometric properties of the gradient flow for learning deep linear\nconvolutional networks. For linear fully connected networks, it has been shown\nrecently that the corresponding gradient flow on parameter space can be written\nas a Riemannian gradient flow on function space (i.e., on the product of weight\nmatrices) if the initialization satisfies a so-called balancedness condition.\nWe establish that the gradient flow on parameter space for learning linear\nconvolutional networks can be written as a Riemannian gradient flow on function\nspace regardless of the initialization. This result holds for $D$-dimensional\nconvolutions with $D \\geq 2$, and for $D =1$ it holds if all so-called strides\nof the convolutions are greater than one. The corresponding Riemannian metric\ndepends on the initialization.", "AI": {"tldr": "\u7814\u7a76\u4e86\u6df1\u5ea6\u7ebf\u6027\u5377\u79ef\u7f51\u7edc\u68af\u5ea6\u6d41\u7684\u51e0\u4f55\u6027\u8d28\uff0c\u53d1\u73b0\u5176\u53c2\u6570\u7a7a\u95f4\u7684\u68af\u5ea6\u6d41\u53ef\u4ee5\u8868\u793a\u4e3a\u51fd\u6570\u7a7a\u95f4\u7684\u9ece\u66fc\u68af\u5ea6\u6d41\uff0c\u65e0\u9700\u521d\u59cb\u5316\u5e73\u8861\u6761\u4ef6\u3002", "motivation": "\u63a2\u8ba8\u7ebf\u6027\u5377\u79ef\u7f51\u7edc\u68af\u5ea6\u6d41\u7684\u51e0\u4f55\u7279\u6027\uff0c\u6269\u5c55\u4e86\u5168\u8fde\u63a5\u7f51\u7edc\u7684\u7c7b\u4f3c\u7814\u7a76\u3002", "method": "\u5206\u6790\u53c2\u6570\u7a7a\u95f4\u68af\u5ea6\u6d41\u4e0e\u51fd\u6570\u7a7a\u95f4\u9ece\u66fc\u68af\u5ea6\u6d41\u7684\u5173\u7cfb\uff0c\u9002\u7528\u4e8e\u591a\u7ef4\u5377\u79ef\u548c\u7279\u5b9a\u6b65\u957f\u6761\u4ef6\u3002", "result": "\u8bc1\u660e\u4e86\u65e0\u8bba\u521d\u59cb\u5316\u5982\u4f55\uff0c\u7ebf\u6027\u5377\u79ef\u7f51\u7edc\u7684\u68af\u5ea6\u6d41\u5747\u53ef\u8868\u793a\u4e3a\u51fd\u6570\u7a7a\u95f4\u7684\u9ece\u66fc\u68af\u5ea6\u6d41\u3002", "conclusion": "\u8be5\u7ed3\u679c\u4e3a\u7406\u89e3\u5377\u79ef\u7f51\u7edc\u7684\u4f18\u5316\u52a8\u6001\u63d0\u4f9b\u4e86\u65b0\u7684\u51e0\u4f55\u89c6\u89d2\u3002"}}
{"id": "2507.06275", "pdf": "https://arxiv.org/pdf/2507.06275", "abs": "https://arxiv.org/abs/2507.06275", "authors": ["Yassin Hussein Rassul", "Aram M. Ahmed", "Polla Fattah", "Bryar A. Hassan", "Arwaa W. Abdulkareem", "Tarik A. Rashid", "Joan Lu"], "title": "Advancing Offline Handwritten Text Recognition: A Systematic Review of Data Augmentation and Generation Techniques", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": null, "summary": "Offline Handwritten Text Recognition (HTR) systems play a crucial role in\napplications such as historical document digitization, automatic form\nprocessing, and biometric authentication. However, their performance is often\nhindered by the limited availability of annotated training data, particularly\nfor low-resource languages and complex scripts. This paper presents a\ncomprehensive survey of offline handwritten data augmentation and generation\ntechniques designed to improve the accuracy and robustness of HTR systems. We\nsystematically examine traditional augmentation methods alongside recent\nadvances in deep learning, including Generative Adversarial Networks (GANs),\ndiffusion models, and transformer-based approaches. Furthermore, we explore the\nchallenges associated with generating diverse and realistic handwriting\nsamples, particularly in preserving script authenticity and addressing data\nscarcity. This survey follows the PRISMA methodology, ensuring a structured and\nrigorous selection process. Our analysis began with 1,302 primary studies,\nwhich were filtered down to 848 after removing duplicates, drawing from key\nacademic sources such as IEEE Digital Library, Springer Link, Science Direct,\nand ACM Digital Library. By evaluating existing datasets, assessment metrics,\nand state-of-the-art methodologies, this survey identifies key research gaps\nand proposes future directions to advance the field of handwritten text\ngeneration across diverse linguistic and stylistic landscapes.", "AI": {"tldr": "\u672c\u6587\u7efc\u8ff0\u4e86\u79bb\u7ebf\u624b\u5199\u6587\u672c\u8bc6\u522b\uff08HTR\uff09\u4e2d\u7684\u6570\u636e\u589e\u5f3a\u4e0e\u751f\u6210\u6280\u672f\uff0c\u63a2\u8ba8\u4e86\u4f20\u7edf\u65b9\u6cd5\u4e0e\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\uff08\u5982GANs\u3001\u6269\u6563\u6a21\u578b\u548cTransformer\uff09\u7684\u5e94\u7528\uff0c\u5e76\u5206\u6790\u4e86\u751f\u6210\u591a\u6837\u4e14\u771f\u5b9e\u624b\u5199\u6837\u672c\u7684\u6311\u6218\u3002", "motivation": "\u79bb\u7ebfHTR\u7cfb\u7edf\u5728\u5386\u53f2\u6587\u6863\u6570\u5b57\u5316\u7b49\u9886\u57df\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u6807\u6ce8\u6570\u636e\u7a00\u7f3a\uff0c\u5c24\u5176\u662f\u4f4e\u8d44\u6e90\u8bed\u8a00\u548c\u590d\u6742\u811a\u672c\uff0c\u9650\u5236\u4e86\u5176\u6027\u80fd\u3002", "method": "\u91c7\u7528PRISMA\u65b9\u6cd5\uff0c\u7cfb\u7edf\u7b5b\u9009\u4e861,302\u7bc7\u7814\u7a76\uff0c\u6700\u7ec8\u7eb3\u5165848\u7bc7\uff0c\u5206\u6790\u4e86\u4f20\u7edf\u548c\u6df1\u5ea6\u5b66\u4e60\u7684\u6570\u636e\u589e\u5f3a\u4e0e\u751f\u6210\u6280\u672f\u3002", "result": "\u7efc\u8ff0\u603b\u7ed3\u4e86\u73b0\u6709\u6570\u636e\u96c6\u3001\u8bc4\u4f30\u6307\u6807\u548c\u5148\u8fdb\u65b9\u6cd5\uff0c\u63ed\u793a\u4e86\u7814\u7a76\u7a7a\u767d\u3002", "conclusion": "\u63d0\u51fa\u4e86\u672a\u6765\u7814\u7a76\u65b9\u5411\uff0c\u4ee5\u63a8\u52a8\u624b\u5199\u6587\u672c\u751f\u6210\u5728\u591a\u8bed\u8a00\u548c\u591a\u98ce\u683c\u573a\u666f\u4e2d\u7684\u53d1\u5c55\u3002"}}
{"id": "2507.05116", "pdf": "https://arxiv.org/pdf/2507.05116", "abs": "https://arxiv.org/abs/2507.05116", "authors": ["Juyi Lin", "Amir Taherin", "Arash Akbari", "Arman Akbari", "Lei Lu", "Guangyu Chen", "Taskin Padir", "Xiaomeng Yang", "Weiwei Chen", "Yiqian Li", "Xue Lin", "David Kaeli", "Pu Zhao", "Yanzhi Wang"], "title": "VOTE: Vision-Language-Action Optimization with Trajectory Ensemble Voting", "categories": ["cs.CV", "cs.AI", "cs.RO"], "comment": null, "summary": "Recent large-scale Vision Language Action (VLA) models have shown superior\nperformance in robotic manipulation tasks guided by natural language. However,\ntheir generalization remains limited when applied to novel objects or\nunfamiliar environments that lie outside the training distribution. To address\nthis, many existing approaches integrate additional components such as depth\nestimation, segmentation, or even diffusion to improve generalization, at the\ncost of adding significant computation overhead, resulting in low efficiency.\nThis motivates the exploration of efficient action prediction methods, which\nare independent of additional high-level visual representations or diffusion\ntechniques. In this work, we propose VOTE, an efficient and general framework\nfor the optimization and acceleration of VLA models. In details, we propose a\nnovel tokenizer-free fine-tuning approach for parallel accurate action\nprediction, which reduces computational overhead and accelerates inference\nspeed. Additionally, we adopt an ensemble voting strategy for the action\nsampling, which significantly improves model performance and enhances\ngeneralization. Experimental results show that our method achieves\nstate-of-the-art performance with 35$\\times$ faster inference and 145 Hz\nthroughput. All the details and codes will be open-sourced.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u9ad8\u6548\u7684VLA\u6a21\u578b\u4f18\u5316\u6846\u67b6VOTE\uff0c\u901a\u8fc7\u514d\u5206\u8bcd\u5668\u7684\u5fae\u8c03\u65b9\u6cd5\u548c\u96c6\u6210\u6295\u7968\u7b56\u7565\uff0c\u663e\u8457\u63d0\u5347\u63a8\u7406\u901f\u5ea6\u548c\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u73b0\u6709VLA\u6a21\u578b\u5728\u65b0\u5bf9\u8c61\u6216\u964c\u751f\u73af\u5883\u4e2d\u7684\u6cdb\u5316\u80fd\u529b\u6709\u9650\uff0c\u4e14\u989d\u5916\u7ec4\u4ef6\u589e\u52a0\u4e86\u8ba1\u7b97\u5f00\u9500\uff0c\u56e0\u6b64\u9700\u8981\u9ad8\u6548\u7684\u52a8\u4f5c\u9884\u6d4b\u65b9\u6cd5\u3002", "method": "\u91c7\u7528\u514d\u5206\u8bcd\u5668\u7684\u5fae\u8c03\u65b9\u6cd5\u5e76\u884c\u9884\u6d4b\u52a8\u4f5c\uff0c\u7ed3\u5408\u96c6\u6210\u6295\u7968\u7b56\u7565\u4f18\u5316\u52a8\u4f5c\u91c7\u6837\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\uff0c\u65b9\u6cd5\u5b9e\u73b0\u4e8635\u500d\u63a8\u7406\u52a0\u901f\u548c145 Hz\u541e\u5410\u91cf\uff0c\u8fbe\u5230SOTA\u6027\u80fd\u3002", "conclusion": "VOTE\u6846\u67b6\u9ad8\u6548\u4e14\u6cdb\u5316\u80fd\u529b\u5f3a\uff0c\u4ee3\u7801\u5c06\u5f00\u6e90\u3002"}}
{"id": "2507.06815", "pdf": "https://arxiv.org/pdf/2507.06815", "abs": "https://arxiv.org/abs/2507.06815", "authors": ["Gijs Wijngaard", "Elia Formisano", "Michele Esposito", "Michel Dumontier"], "title": "Data-Balanced Curriculum Learning for Audio Question Answering", "categories": ["cs.SD", "eess.AS"], "comment": null, "summary": "Audio question answering (AQA) requires models to understand acoustic content\nand perform complex reasoning. Current models struggle with dataset imbalances\nand unstable training dynamics. This work combines curriculum learning with\nstatistical data balancing to address these challenges. The method labels\nquestion difficulty using language models, then trains progressively from easy\nto hard examples. Statistical filtering removes overrepresented audio\ncategories, and guided decoding constrains outputs to valid multiple-choice\nformats. Experiments on the DCASE 2025 training set and five additional public\ndatasets show that data curation improves accuracy by 11.7% over baseline\nmodels, achieving 64.2% on the DCASE 2025 benchmark.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u8bfe\u7a0b\u5b66\u4e60\u548c\u7edf\u8ba1\u5e73\u8861\u7684\u65b9\u6cd5\uff0c\u89e3\u51b3\u97f3\u9891\u95ee\u7b54\u4efb\u52a1\u4e2d\u7684\u6570\u636e\u4e0d\u5e73\u8861\u548c\u8bad\u7ec3\u4e0d\u7a33\u5b9a\u6027\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u6027\u80fd\u3002", "motivation": "\u5f53\u524d\u6a21\u578b\u5728\u97f3\u9891\u95ee\u7b54\u4efb\u52a1\u4e2d\u9762\u4e34\u6570\u636e\u4e0d\u5e73\u8861\u548c\u8bad\u7ec3\u4e0d\u7a33\u5b9a\u7684\u6311\u6218\uff0c\u9700\u8981\u4e00\u79cd\u66f4\u6709\u6548\u7684\u65b9\u6cd5\u6765\u63d0\u5347\u6027\u80fd\u3002", "method": "\u901a\u8fc7\u8bed\u8a00\u6a21\u578b\u6807\u6ce8\u95ee\u9898\u96be\u5ea6\uff0c\u6309\u4ece\u6613\u5230\u96be\u7684\u987a\u5e8f\u8bad\u7ec3\uff1b\u7edf\u8ba1\u8fc7\u6ee4\u8fc7\u8868\u793a\u7684\u97f3\u9891\u7c7b\u522b\uff0c\u5e76\u4f7f\u7528\u5f15\u5bfc\u89e3\u7801\u7ea6\u675f\u8f93\u51fa\u683c\u5f0f\u3002", "result": "\u5728DCASE 2025\u8bad\u7ec3\u96c6\u548c\u4e94\u4e2a\u516c\u5171\u6570\u636e\u96c6\u4e0a\uff0c\u6570\u636e\u4f18\u5316\u4f7f\u51c6\u786e\u7387\u6bd4\u57fa\u7ebf\u6a21\u578b\u63d0\u9ad8\u4e8611.7%\uff0c\u8fbe\u523064.2%\u3002", "conclusion": "\u7ed3\u5408\u8bfe\u7a0b\u5b66\u4e60\u548c\u6570\u636e\u5e73\u8861\u7684\u65b9\u6cd5\u6709\u6548\u63d0\u5347\u4e86\u97f3\u9891\u95ee\u7b54\u4efb\u52a1\u7684\u6027\u80fd\uff0c\u4e3a\u7c7b\u4f3c\u4efb\u52a1\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2507.07015", "pdf": "https://arxiv.org/pdf/2507.07015", "abs": "https://arxiv.org/abs/2507.07015", "authors": ["Hui Li", "Pengfei Yang", "Juanyang Chen", "Le Dong", "Yanxin Chen", "Quan Wang"], "title": "MST-Distill: Mixture of Specialized Teachers for Cross-Modal Knowledge Distillation", "categories": ["cs.CV", "cs.LG", "cs.MM"], "comment": "Accepted to ACM MM 2025 (The 33rd ACM International Conference on\n  Multimedia)", "summary": "Knowledge distillation as an efficient knowledge transfer technique, has\nachieved remarkable success in unimodal scenarios. However, in cross-modal\nsettings, conventional distillation methods encounter significant challenges\ndue to data and statistical heterogeneities, failing to leverage the\ncomplementary prior knowledge embedded in cross-modal teacher models. This\npaper empirically reveals two critical issues in existing approaches:\ndistillation path selection and knowledge drift. To address these limitations,\nwe propose MST-Distill, a novel cross-modal knowledge distillation framework\nfeaturing a mixture of specialized teachers. Our approach employs a diverse\nensemble of teacher models across both cross-modal and multimodal\nconfigurations, integrated with an instance-level routing network that\nfacilitates adaptive and dynamic distillation. This architecture effectively\ntranscends the constraints of traditional methods that rely on monotonous and\nstatic teacher models. Additionally, we introduce a plug-in masking module,\nindependently trained to suppress modality-specific discrepancies and\nreconstruct teacher representations, thereby mitigating knowledge drift and\nenhancing transfer effectiveness. Extensive experiments across five diverse\nmultimodal datasets, spanning visual, audio, and text, demonstrate that our\nmethod significantly outperforms existing state-of-the-art knowledge\ndistillation methods in cross-modal distillation tasks. The source code is\navailable at https://github.com/Gray-OREO/MST-Distill.", "AI": {"tldr": "MST-Distill\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u8de8\u6a21\u6001\u77e5\u8bc6\u84b8\u998f\u6846\u67b6\uff0c\u901a\u8fc7\u6df7\u5408\u4e13\u5bb6\u6559\u5e08\u6a21\u578b\u548c\u52a8\u6001\u8def\u7531\u7f51\u7edc\u89e3\u51b3\u4f20\u7edf\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8de8\u6a21\u6001\u4efb\u52a1\u6027\u80fd\u3002", "motivation": "\u4f20\u7edf\u77e5\u8bc6\u84b8\u998f\u65b9\u6cd5\u5728\u8de8\u6a21\u6001\u573a\u666f\u4e2d\u56e0\u6570\u636e\u548c\u7edf\u8ba1\u5f02\u8d28\u6027\u96be\u4ee5\u5229\u7528\u8de8\u6a21\u6001\u6559\u5e08\u6a21\u578b\u7684\u4e92\u8865\u77e5\u8bc6\uff0c\u5b58\u5728\u84b8\u998f\u8def\u5f84\u9009\u62e9\u548c\u77e5\u8bc6\u6f02\u79fb\u95ee\u9898\u3002", "method": "\u63d0\u51faMST-Distill\u6846\u67b6\uff0c\u7ed3\u5408\u8de8\u6a21\u6001\u548c\u591a\u6a21\u6001\u6559\u5e08\u6a21\u578b\uff0c\u4f7f\u7528\u52a8\u6001\u8def\u7531\u7f51\u7edc\u548c\u63a9\u7801\u6a21\u5757\u6291\u5236\u6a21\u6001\u5dee\u5f02\u5e76\u91cd\u6784\u8868\u793a\u3002", "result": "\u5728\u4e94\u4e2a\u591a\u6a21\u6001\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cMST-Distill\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u7684\u8de8\u6a21\u6001\u77e5\u8bc6\u84b8\u998f\u65b9\u6cd5\u3002", "conclusion": "MST-Distill\u901a\u8fc7\u52a8\u6001\u6559\u5e08\u6a21\u578b\u548c\u63a9\u7801\u6a21\u5757\u6709\u6548\u89e3\u51b3\u4e86\u8de8\u6a21\u6001\u77e5\u8bc6\u84b8\u998f\u7684\u6311\u6218\uff0c\u63d0\u5347\u4e86\u6027\u80fd\u3002"}}
{"id": "2507.06506", "pdf": "https://arxiv.org/pdf/2507.06506", "abs": "https://arxiv.org/abs/2507.06506", "authors": ["Russell Taylor", "Benjamin Herbert", "Michael Sana"], "title": "Pun Intended: Multi-Agent Translation of Wordplay with Contrastive Learning and Phonetic-Semantic Embeddings", "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.MA"], "comment": "CLEF 2025 Working Notes, 9-12 September 2025, Madrid, Spain", "summary": "Translating wordplay across languages presents unique challenges that have\nlong confounded both professional human translators and machine translation\nsystems. This research proposes a novel approach for translating puns from\nEnglish to French by combining state-of-the-art large language models with\nspecialized techniques for wordplay generation.\n  Our methodology employs a three-stage approach. First, we establish a\nbaseline using multiple frontier large language models with feedback based on a\nnew contrastive learning dataset. Second, we implement a guided\nchain-of-thought pipeline with combined phonetic-semantic embeddings. Third, we\nimplement a multi-agent generator-discriminator framework for evaluating and\nregenerating puns with feedback.\n  Moving beyond the limitations of literal translation, our methodology's\nprimary objective is to capture the linguistic creativity and humor of the\nsource text wordplay, rather than simply duplicating its vocabulary. Our best\nruns earned first and second place in the CLEF JOKER 2025 Task 2 competition\nwhere they were evaluated manually by expert native French speakers.\n  This research addresses a gap between translation studies and computational\nlinguistics by implementing linguistically-informed techniques for wordplay\ntranslation, advancing our understanding of how language models can be\nleveraged to handle the complex interplay between semantic ambiguity, phonetic\nsimilarity, and the implicit cultural and linguistic awareness needed for\nsuccessful humor.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u5927\u578b\u8bed\u8a00\u6a21\u578b\u548c\u4e13\u95e8\u6280\u672f\u7684\u521b\u65b0\u65b9\u6cd5\uff0c\u7528\u4e8e\u5c06\u82f1\u8bed\u53cc\u5173\u8bed\u7ffb\u8bd1\u6210\u6cd5\u8bed\uff0c\u5e76\u5728CLEF JOKER 2025\u6bd4\u8d5b\u4e2d\u53d6\u5f97\u4f18\u5f02\u6210\u7ee9\u3002", "motivation": "\u89e3\u51b3\u8de8\u8bed\u8a00\u53cc\u5173\u8bed\u7ffb\u8bd1\u7684\u6311\u6218\uff0c\u5f25\u8865\u7ffb\u8bd1\u7814\u7a76\u4e0e\u8ba1\u7b97\u8bed\u8a00\u5b66\u4e4b\u95f4\u7684\u7a7a\u767d\u3002", "method": "\u91c7\u7528\u4e09\u9636\u6bb5\u65b9\u6cd5\uff1a\u57fa\u7ebf\u6a21\u578b\u5efa\u7acb\u3001\u5f15\u5bfc\u601d\u7ef4\u94fe\u7ba1\u9053\u3001\u591a\u4ee3\u7406\u751f\u6210-\u5224\u522b\u6846\u67b6\u3002", "result": "\u5728CLEF JOKER 2025\u6bd4\u8d5b\u4e2d\u83b7\u5f97\u7b2c\u4e00\u548c\u7b2c\u4e8c\u540d\u3002", "conclusion": "\u901a\u8fc7\u8bed\u8a00\u5b66\u6280\u672f\u63a8\u52a8\u53cc\u5173\u8bed\u7ffb\u8bd1\uff0c\u6df1\u5316\u4e86\u5bf9\u8bed\u8a00\u6a21\u578b\u5904\u7406\u8bed\u4e49\u6a21\u7cca\u6027\u548c\u6587\u5316\u80cc\u666f\u7684\u7406\u89e3\u3002"}}
{"id": "2507.06562", "pdf": "https://arxiv.org/pdf/2507.06562", "abs": "https://arxiv.org/abs/2507.06562", "authors": ["Keita Yoneda", "Kento Kawaharazuka", "Temma Suzuki", "Takahiro Hattori", "Kei Okada"], "title": "KLEIYN : A Quadruped Robot with an Active Waist for Both Locomotion and Wall Climbing", "categories": ["cs.RO"], "comment": "Accepted at IROS2025, website -\n  https://keitayoneda.github.io/kleiyn-chimney-climbing/, YouTube -\n  https://www.youtube.com/watch?v=vDmSfkazAvI", "summary": "In recent years, advancements in hardware have enabled quadruped robots to\noperate with high power and speed, while robust locomotion control using\nreinforcement learning (RL) has also been realized. As a result, expectations\nare rising for the automation of tasks such as material transport and\nexploration in unknown environments. However, autonomous locomotion in rough\nterrains with significant height variations requires vertical movement, and\nrobots capable of performing such movements stably, along with their control\nmethods, have not yet been fully established. In this study, we developed the\nquadruped robot KLEIYN, which features a waist joint, and aimed to expand\nquadruped locomotion by enabling chimney climbing through RL. To facilitate the\nlearning of vertical motion, we introduced Contact-Guided Curriculum Learning\n(CGCL). As a result, KLEIYN successfully climbed walls ranging from 800 mm to\n1000 mm in width at an average speed of 150 mm/s, 50 times faster than\nconventional robots. Furthermore, we demonstrated that the introduction of a\nwaist joint improves climbing performance, particularly enhancing tracking\nability on narrow walls.", "AI": {"tldr": "\u7814\u7a76\u5f00\u53d1\u4e86\u5177\u6709\u8170\u90e8\u5173\u8282\u7684\u56db\u8db3\u673a\u5668\u4ebaKLEIYN\uff0c\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u5b9e\u73b0\u5782\u76f4\u8fd0\u52a8\uff08\u5982\u70df\u56f1\u6500\u722c\uff09\uff0c\u5e76\u5f15\u5165\u63a5\u89e6\u5f15\u5bfc\u8bfe\u7a0b\u5b66\u4e60\uff08CGCL\uff09\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6500\u722c\u901f\u5ea6\u548c\u6027\u80fd\u3002", "motivation": "\u5c3d\u7ba1\u56db\u8db3\u673a\u5668\u4eba\u5728\u5e73\u5766\u5730\u5f62\u4e0a\u7684\u8fd0\u52a8\u63a7\u5236\u5df2\u53d6\u5f97\u8fdb\u5c55\uff0c\u4f46\u5728\u5177\u6709\u663e\u8457\u9ad8\u5ea6\u53d8\u5316\u7684\u5d0e\u5c96\u5730\u5f62\u4e2d\u7a33\u5b9a\u5782\u76f4\u8fd0\u52a8\u4ecd\u662f\u4e00\u4e2a\u672a\u89e3\u51b3\u7684\u6311\u6218\u3002", "method": "\u5f00\u53d1\u4e86\u5177\u6709\u8170\u90e8\u5173\u8282\u7684\u673a\u5668\u4ebaKLEIYN\uff0c\u5e76\u91c7\u7528\u5f3a\u5316\u5b66\u4e60\u548c\u63a5\u89e6\u5f15\u5bfc\u8bfe\u7a0b\u5b66\u4e60\uff08CGCL\uff09\u65b9\u6cd5\u8bad\u7ec3\u5176\u5782\u76f4\u8fd0\u52a8\u80fd\u529b\u3002", "result": "KLEIYN\u80fd\u4ee5\u5e73\u5747150 mm/s\u7684\u901f\u5ea6\u6500\u722c800-1000 mm\u5bbd\u7684\u5899\u58c1\uff0c\u901f\u5ea6\u662f\u4f20\u7edf\u673a\u5668\u4eba\u768450\u500d\uff0c\u4e14\u8170\u90e8\u5173\u8282\u663e\u8457\u63d0\u5347\u4e86\u5728\u72ed\u7a84\u5899\u58c1\u4e0a\u7684\u8ddf\u8e2a\u80fd\u529b\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\uff0c\u8170\u90e8\u5173\u8282\u548cCGCL\u65b9\u6cd5\u7684\u7ed3\u5408\u80fd\u6709\u6548\u6269\u5c55\u56db\u8db3\u673a\u5668\u4eba\u7684\u5782\u76f4\u8fd0\u52a8\u80fd\u529b\uff0c\u4e3a\u590d\u6742\u5730\u5f62\u4efb\u52a1\u63d0\u4f9b\u4e86\u65b0\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.06393", "pdf": "https://arxiv.org/pdf/2507.06393", "abs": "https://arxiv.org/abs/2507.06393", "authors": ["Matilde Marcolli", "Riny Huijbregts", "Richard K. Larson"], "title": "Hypermagmas and Colored Operads: Heads, Phases, and Theta Roles", "categories": ["cs.CL", "math.QA", "math.RA", "91F20, 18M60, 18M80, 16T05, 68Q70"], "comment": "LaTeX, 48 pages", "summary": "We show that head functions on syntactic objects extend the magma structure\nto a hypermagma, with the c-command relation compatible with the magma\noperation and the m-command relation with the hypermagma. We then show that the\nstructure of head and complement and specifier, additional modifier positions,\nand the structure of phases in the Extended Projection can be formulated as a\nbud generating system of a colored operad, in a form similar to the structure\nof theta roles. We also show that, due to the special form of the colored\noperad generators, the filtering of freely generated syntactic objects by these\ncoloring rules can be equivalently formulated as a filtering in the course of\nstructure formation via a colored Merge, which can in turn be related to the\nhypermagma structure. The rules on movement by Internal Merge with respect to\nphases, the Extended Projection Principle, Empty Category Principle, and Phase\nImpenetrability Condition are all subsumed into the form of the colored operad\ngenerators. Movement compatibilities between the phase structure and the theta\nroles assignments can then be formulated in terms of the respective colored\noperads and a transduction of colored operads.", "AI": {"tldr": "\u8bba\u6587\u5c55\u793a\u4e86\u53e5\u6cd5\u5bf9\u8c61\u7684\u5934\u90e8\u51fd\u6570\u5982\u4f55\u5c06\u4ee3\u6570\u7ed3\u6784\u6269\u5c55\u4e3a\u8d85\u4ee3\u6570\u7ed3\u6784\uff0c\u5e76\u63a2\u8ba8\u4e86c-\u547d\u4ee4\u548cm-\u547d\u4ee4\u5173\u7cfb\u7684\u517c\u5bb9\u6027\u3002\u6b64\u5916\uff0c\u901a\u8fc7\u5f69\u8272\u64cd\u4f5c\u6570\u7684\u5f62\u5f0f\uff0c\u63cf\u8ff0\u4e86\u53e5\u6cd5\u7ed3\u6784\u7684\u751f\u6210\u7cfb\u7edf\u53ca\u5176\u4e0e\u03b8\u89d2\u8272\u7684\u76f8\u4f3c\u6027\u3002", "motivation": "\u7814\u7a76\u76ee\u7684\u662f\u5c06\u53e5\u6cd5\u7ed3\u6784\u4e0e\u4ee3\u6570\u7ed3\u6784\u8054\u7cfb\u8d77\u6765\uff0c\u63a2\u7d22\u53e5\u6cd5\u751f\u6210\u548c\u79fb\u52a8\u89c4\u5219\u7684\u7edf\u4e00\u6570\u5b66\u6846\u67b6\u3002", "method": "\u4f7f\u7528\u8d85\u4ee3\u6570\u7ed3\u6784\u548c\u5f69\u8272\u64cd\u4f5c\u6570\u7406\u8bba\uff0c\u5c06\u53e5\u6cd5\u751f\u6210\u548c\u79fb\u52a8\u89c4\u5219\u5f62\u5f0f\u5316\u4e3a\u4ee3\u6570\u64cd\u4f5c\u3002", "result": "\u8bc1\u660e\u4e86\u53e5\u6cd5\u751f\u6210\u548c\u79fb\u52a8\u89c4\u5219\u53ef\u4ee5\u901a\u8fc7\u5f69\u8272\u64cd\u4f5c\u6570\u7edf\u4e00\u63cf\u8ff0\uff0c\u5e76\u5c55\u793a\u4e86\u5176\u4e0e\u8d85\u4ee3\u6570\u7ed3\u6784\u7684\u5173\u8054\u3002", "conclusion": "\u7814\u7a76\u4e3a\u53e5\u6cd5\u7406\u8bba\u63d0\u4f9b\u4e86\u7edf\u4e00\u7684\u6570\u5b66\u6846\u67b6\uff0c\u5c06\u751f\u6210\u548c\u79fb\u52a8\u89c4\u5219\u7eb3\u5165\u4ee3\u6570\u7ed3\u6784\u4e2d\u3002"}}
{"id": "2507.06380", "pdf": "https://arxiv.org/pdf/2507.06380", "abs": "https://arxiv.org/abs/2507.06380", "authors": ["Habibur Rahaman", "Atri Chatterjee", "Swarup Bhunia"], "title": "Secure and Storage-Efficient Deep Learning Models for Edge AI Using Automatic Weight Generation", "categories": ["cs.LG", "cs.AI", "cs.CV"], "comment": "7 pages, 7 figures", "summary": "Complex neural networks require substantial memory to store a large number of\nsynaptic weights. This work introduces WINGs (Automatic Weight Generator for\nSecure and Storage-Efficient Deep Learning Models), a novel framework that\ndynamically generates layer weights in a fully connected neural network (FC)\nand compresses the weights in convolutional neural networks (CNNs) during\ninference, significantly reducing memory requirements without sacrificing\naccuracy. WINGs framework uses principal component analysis (PCA) for\ndimensionality reduction and lightweight support vector regression (SVR) models\nto predict layer weights in the FC networks, removing the need for storing\nfull-weight matrices and achieving substantial memory savings. It also\npreferentially compresses the weights in low-sensitivity layers of CNNs using\nPCA and SVR with sensitivity analysis. The sensitivity-aware design also offers\nan added level of security, as any bit-flip attack with weights in compressed\nlayers has an amplified and readily detectable effect on accuracy. WINGs\nachieves 53x compression for the FC layers and 28x for AlexNet with MNIST\ndataset, and 18x for Alexnet with CIFAR-10 dataset with 1-2% accuracy loss.\nThis significant reduction in memory results in higher throughput and lower\nenergy for DNN inference, making it attractive for resource-constrained edge\napplications.", "AI": {"tldr": "WINGs\u6846\u67b6\u901a\u8fc7\u52a8\u6001\u751f\u6210\u5168\u8fde\u63a5\u5c42\u6743\u91cd\u548c\u538b\u7f29\u5377\u79ef\u5c42\u6743\u91cd\uff0c\u663e\u8457\u51cf\u5c11\u5185\u5b58\u9700\u6c42\uff0c\u540c\u65f6\u4fdd\u6301\u51c6\u786e\u6027\u3002", "motivation": "\u89e3\u51b3\u590d\u6742\u795e\u7ecf\u7f51\u7edc\u5b58\u50a8\u5927\u91cf\u7a81\u89e6\u6743\u91cd\u7684\u5185\u5b58\u95ee\u9898\uff0c\u9002\u7528\u4e8e\u8d44\u6e90\u53d7\u9650\u7684\u8fb9\u7f18\u8ba1\u7b97\u573a\u666f\u3002", "method": "\u4f7f\u7528PCA\u964d\u7ef4\u548c\u8f7b\u91cf\u7ea7SVR\u6a21\u578b\u9884\u6d4b\u6743\u91cd\uff0c\u7ed3\u5408\u654f\u611f\u6027\u5206\u6790\u4f18\u5148\u538b\u7f29\u4f4e\u654f\u611f\u6027\u5c42\u3002", "result": "FC\u5c42\u538b\u7f2953\u500d\uff0cAlexNet\u5728MNIST\u4e0a\u538b\u7f2928\u500d\uff0cCIFAR-10\u4e0a\u538b\u7f2918\u500d\uff0c\u7cbe\u5ea6\u635f\u59311-2%\u3002", "conclusion": "WINGs\u663e\u8457\u964d\u4f4e\u5185\u5b58\u9700\u6c42\uff0c\u63d0\u9ad8\u63a8\u7406\u6548\u7387\uff0c\u9002\u5408\u8fb9\u7f18\u8bbe\u5907\u5e94\u7528\u3002"}}
{"id": "2507.06321", "pdf": "https://arxiv.org/pdf/2507.06321", "abs": "https://arxiv.org/abs/2507.06321", "authors": ["Joon Tai Kim", "Tianle Chen", "Ziyu Dong", "Nishanth Kunchala", "Alexander Guller", "Daniel Ospina Acero", "Roger Williams", "Mrinal Kumar"], "title": "Centralized Copy-Paste: Enhanced Data Augmentation Strategy for Wildland Fire Semantic Segmentation", "categories": ["cs.CV", "cs.LG"], "comment": "21 pages, 5 figures, and under review for AIAA SciTech 2026", "summary": "Collecting and annotating images for the purpose of training segmentation\nmodels is often cost prohibitive. In the domain of wildland fire science, this\nchallenge is further compounded by the scarcity of reliable public datasets\nwith labeled ground truth. This paper presents the Centralized Copy-Paste Data\nAugmentation (CCPDA) method, for the purpose of assisting with the training of\ndeep-learning multiclass segmentation models, with special focus on improving\nsegmentation outcomes for the fire-class. CCPDA has three main steps: (i)\nidentify fire clusters in the source image, (ii) apply a centralization\ntechnique to focus on the core of the fire area, and (iii) paste the refined\nfire clusters onto a target image. This method increases dataset diversity\nwhile preserving the essential characteristics of the fire class. The\neffectiveness of this augmentation technique is demonstrated via numerical\nanalysis and comparison against various other augmentation methods using a\nweighted sum-based multi-objective optimization approach. This approach helps\nelevate segmentation performance metrics specific to the fire class, which\ncarries significantly more operational significance than other classes (fuel,\nash, or background). Numerical performance assessment validates the efficacy of\nthe presented CCPDA method in alleviating the difficulties associated with\nsmall, manually labeled training datasets. It also illustrates that CCPDA\noutperforms other augmentation strategies in the application scenario\nconsidered, particularly in improving fire-class segmentation performance.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aCCPDA\u7684\u6570\u636e\u589e\u5f3a\u65b9\u6cd5\uff0c\u7528\u4e8e\u63d0\u5347\u91ce\u706b\u79d1\u5b66\u4e2d\u591a\u7c7b\u5206\u5272\u6a21\u578b\u7684\u8bad\u7ec3\u6548\u679c\uff0c\u7279\u522b\u662f\u9488\u5bf9\u706b\u7c7b\u7684\u5206\u5272\u6027\u80fd\u3002", "motivation": "\u7531\u4e8e\u6536\u96c6\u548c\u6807\u6ce8\u56fe\u50cf\u7528\u4e8e\u8bad\u7ec3\u5206\u5272\u6a21\u578b\u7684\u6210\u672c\u9ad8\u6602\uff0c\u4e14\u91ce\u706b\u79d1\u5b66\u9886\u57df\u7f3a\u4e4f\u53ef\u9760\u7684\u516c\u5f00\u6570\u636e\u96c6\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u6709\u6548\u7684\u6570\u636e\u589e\u5f3a\u65b9\u6cd5\u6765\u7f13\u89e3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "CCPDA\u65b9\u6cd5\u5305\u62ec\u4e09\u4e2a\u6b65\u9aa4\uff1a(i)\u8bc6\u522b\u6e90\u56fe\u50cf\u4e2d\u7684\u706b\u7c07\uff0c(ii)\u5e94\u7528\u4e2d\u5fc3\u5316\u6280\u672f\u805a\u7126\u706b\u533a\u6838\u5fc3\uff0c(iii)\u5c06\u7cbe\u70bc\u7684\u706b\u7c07\u7c98\u8d34\u5230\u76ee\u6807\u56fe\u50cf\u4e0a\u3002", "result": "\u901a\u8fc7\u6570\u503c\u5206\u6790\u548c\u591a\u76ee\u6807\u4f18\u5316\u6bd4\u8f83\uff0cCCPDA\u5728\u63d0\u5347\u706b\u7c7b\u5206\u5272\u6027\u80fd\u65b9\u9762\u4f18\u4e8e\u5176\u4ed6\u589e\u5f3a\u65b9\u6cd5\uff0c\u5e76\u6709\u6548\u7f13\u89e3\u4e86\u5c0f\u89c4\u6a21\u6807\u6ce8\u6570\u636e\u96c6\u7684\u8bad\u7ec3\u56f0\u96be\u3002", "conclusion": "CCPDA\u65b9\u6cd5\u5728\u91ce\u706b\u79d1\u5b66\u4e2d\u663e\u8457\u63d0\u5347\u4e86\u706b\u7c7b\u5206\u5272\u7684\u6027\u80fd\uff0c\u4e3a\u5c0f\u89c4\u6a21\u6807\u6ce8\u6570\u636e\u96c6\u7684\u8bad\u7ec3\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.06235", "pdf": "https://arxiv.org/pdf/2507.06235", "abs": "https://arxiv.org/abs/2507.06235", "authors": ["Yuto Mandai", "Katie Seaborn", "Tomoyasu Nakano", "Xin Sun", "Yijia Wang", "Jun Kato"], "title": "Super Kawaii Vocalics: Amplifying the \"Cute\" Factor in Computer Voice", "categories": ["cs.HC", "cs.AI", "cs.CL", "cs.CY", "cs.SD", "eess.AS"], "comment": "CHI '25", "summary": "\"Kawaii\" is the Japanese concept of cute, which carries sociocultural\nconnotations related to social identities and emotional responses. Yet,\nvirtually all work to date has focused on the visual side of kawaii, including\nin studies of computer agents and social robots. In pursuit of formalizing the\nnew science of kawaii vocalics, we explored what elements of voice relate to\nkawaii and how they might be manipulated, manually and automatically. We\nconducted a four-phase study (grand N = 512) with two varieties of computer\nvoices: text-to-speech (TTS) and game character voices. We found kawaii \"sweet\nspots\" through manipulation of fundamental and formant frequencies, but only\nfor certain voices and to a certain extent. Findings also suggest a ceiling\neffect for the kawaii vocalics of certain voices. We offer empirical validation\nof the preliminary kawaii vocalics model and an elementary method for\nmanipulating kawaii perceptions of computer voice.", "AI": {"tldr": "\u7814\u7a76\u63a2\u7d22\u4e86\u58f0\u97f3\u4e2d\u7684\u201c\u53ef\u7231\u201d\uff08kawaii\uff09\u5143\u7d20\u53ca\u5176\u64cd\u7eb5\u65b9\u6cd5\uff0c\u586b\u8865\u4e86\u4ee5\u5f80\u4ec5\u5173\u6ce8\u89c6\u89c9\u53ef\u7231\u6027\u7684\u7a7a\u767d\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u591a\u805a\u7126\u4e8e\u89c6\u89c9\u53ef\u7231\u6027\uff0c\u800c\u58f0\u97f3\u65b9\u9762\u7684\u53ef\u7231\u6027\u7814\u7a76\u4e0d\u8db3\uff0c\u56e0\u6b64\u63a2\u7d22\u58f0\u97f3\u4e2d\u7684\u53ef\u7231\u5143\u7d20\u53ca\u5176\u64cd\u7eb5\u65b9\u6cd5\u3002", "method": "\u901a\u8fc7\u56db\u9636\u6bb5\u7814\u7a76\uff08N=512\uff09\uff0c\u5206\u6790\u6587\u672c\u8f6c\u8bed\u97f3\uff08TTS\uff09\u548c\u6e38\u620f\u89d2\u8272\u58f0\u97f3\uff0c\u64cd\u7eb5\u57fa\u9891\u548c\u5171\u632f\u5cf0\u9891\u7387\u3002", "result": "\u53d1\u73b0\u67d0\u4e9b\u58f0\u97f3\u5b58\u5728\u53ef\u7231\u7684\u201c\u751c\u70b9\u201d\uff0c\u4f46\u6548\u679c\u6709\u9650\u4e14\u5b58\u5728\u5929\u82b1\u677f\u6548\u5e94\u3002", "conclusion": "\u521d\u6b65\u9a8c\u8bc1\u4e86\u53ef\u7231\u58f0\u97f3\u6a21\u578b\uff0c\u5e76\u63d0\u4f9b\u4e86\u64cd\u7eb5\u8ba1\u7b97\u673a\u58f0\u97f3\u53ef\u7231\u611f\u77e5\u7684\u57fa\u672c\u65b9\u6cd5\u3002"}}
{"id": "2507.06826", "pdf": "https://arxiv.org/pdf/2507.06826", "abs": "https://arxiv.org/abs/2507.06826", "authors": ["Yoshiki Masuyama", "Fran\u00e7ois G. Germain", "Gordon Wichern", "Christopher Ick", "Jonathan Le Roux"], "title": "Physics-Informed Direction-Aware Neural Acoustic Fields", "categories": ["cs.SD", "eess.AS", "eess.SP"], "comment": "Accepted to WASPAA 2025", "summary": "This paper presents a physics-informed neural network (PINN) for modeling\nfirst-order Ambisonic (FOA) room impulse responses (RIRs). PINNs have\ndemonstrated promising performance in sound field interpolation by combining\nthe powerful modeling capability of neural networks and the physical principles\nof sound propagation. In room acoustics, PINNs have typically been trained to\nrepresent the sound pressure measured by omnidirectional microphones where the\nwave equation or its frequency-domain counterpart, i.e., the Helmholtz\nequation, is leveraged. Meanwhile, FOA RIRs additionally provide spatial\ncharacteristics and are useful for immersive audio generation with a wide range\nof applications. In this paper, we extend the PINN framework to model FOA RIRs.\nWe derive two physics-informed priors for FOA RIRs based on the correspondence\nbetween the particle velocity and the (X, Y, Z)-channels of FOA. These priors\nassociate the predicted W-channel and other channels through their partial\nderivatives and impose the physically feasible relationship on the four\nchannels. Our experiments confirm the effectiveness of the proposed method\ncompared with a neural network without the physics-informed prior.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u7269\u7406\u4fe1\u606f\u7684\u795e\u7ecf\u7f51\u7edc\uff08PINN\uff09\u7528\u4e8e\u5efa\u6a21\u4e00\u9636Ambisonic\uff08FOA\uff09\u623f\u95f4\u8109\u51b2\u54cd\u5e94\uff08RIRs\uff09\uff0c\u901a\u8fc7\u7ed3\u5408\u795e\u7ecf\u7f51\u7edc\u7684\u5f3a\u5927\u5efa\u6a21\u80fd\u529b\u548c\u58f0\u6ce2\u4f20\u64ad\u7684\u7269\u7406\u539f\u7406\uff0c\u6269\u5c55\u4e86PINN\u6846\u67b6\u4ee5\u5efa\u6a21FOA RIRs\u3002", "motivation": "FOA RIRs\u4e0d\u4ec5\u63d0\u4f9b\u7a7a\u95f4\u7279\u6027\uff0c\u8fd8\u5e7f\u6cdb\u5e94\u7528\u4e8e\u6c89\u6d78\u5f0f\u97f3\u9891\u751f\u6210\uff0c\u4f46\u73b0\u6709PINN\u65b9\u6cd5\u4e3b\u8981\u9488\u5bf9\u5168\u5411\u9ea6\u514b\u98ce\u6d4b\u91cf\u7684\u58f0\u538b\uff0c\u7f3a\u4e4f\u5bf9FOA RIRs\u7684\u5efa\u6a21\u3002", "method": "\u901a\u8fc7\u63a8\u5bfc\u57fa\u4e8e\u7c92\u5b50\u901f\u5ea6\u4e0eFOA\u7684(X, Y, Z)\u901a\u9053\u5bf9\u5e94\u5173\u7cfb\u7684\u4e24\u4e2a\u7269\u7406\u5148\u9a8c\uff0c\u5c06\u9884\u6d4b\u7684W\u901a\u9053\u4e0e\u5176\u4ed6\u901a\u9053\u901a\u8fc7\u504f\u5bfc\u6570\u5173\u8054\uff0c\u5e76\u65bd\u52a0\u7269\u7406\u53ef\u884c\u7684\u5173\u7cfb\u3002", "result": "\u5b9e\u9a8c\u8bc1\u5b9e\uff0c\u76f8\u6bd4\u65e0\u7269\u7406\u5148\u9a8c\u7684\u795e\u7ecf\u7f51\u7edc\uff0c\u6240\u63d0\u65b9\u6cd5\u66f4\u6709\u6548\u3002", "conclusion": "\u6269\u5c55PINN\u6846\u67b6\u4ee5\u5efa\u6a21FOA RIRs\u662f\u53ef\u884c\u7684\uff0c\u4e14\u7269\u7406\u5148\u9a8c\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\u3002"}}
{"id": "2507.06542", "pdf": "https://arxiv.org/pdf/2507.06542", "abs": "https://arxiv.org/abs/2507.06542", "authors": ["Tongtian Zhu", "Tianyu Zhang", "Mingze Wang", "Zhanpeng Zhou", "Can Wang"], "title": "A Single Merging Suffices: Recovering Server-based Learning Performance in Decentralized Learning", "categories": ["cs.LG", "cs.DC", "cs.MA", "stat.ML"], "comment": "We discover and theoretically explain why and when a single global\n  parameter merging in decentralized learning can recover the performance of\n  server-based learning, even in highly heterogeneous and\n  communication-constrained environments", "summary": "Decentralized learning provides a scalable alternative to traditional\nparameter-server-based training, yet its performance is often hindered by\nlimited peer-to-peer communication. In this paper, we study how communication\nshould be scheduled over time, including determining when and how frequently\ndevices synchronize. Our empirical results show that concentrating\ncommunication budgets in the later stages of decentralized training markedly\nimproves global generalization. Surprisingly, we uncover that fully connected\ncommunication at the final step, implemented by a single global merging, is\nsufficient to match the performance of server-based training. We further show\nthat low communication in decentralized learning preserves the\n\\textit{mergeability} of local models throughout training. Our theoretical\ncontributions, which explains these phenomena, are first to establish that the\nglobally merged model of decentralized SGD can converge faster than centralized\nmini-batch SGD. Technically, we novelly reinterpret part of the discrepancy\namong local models, which were previously considered as detrimental noise, as\nconstructive components that accelerate convergence. This work challenges the\ncommon belief that decentralized learning generalizes poorly under data\nheterogeneity and limited communication, while offering new insights into model\nmerging and neural network loss landscapes.", "AI": {"tldr": "\u7814\u7a76\u53bb\u4e2d\u5fc3\u5316\u5b66\u4e60\u4e2d\u901a\u4fe1\u8c03\u5ea6\u7684\u4f18\u5316\uff0c\u53d1\u73b0\u540e\u671f\u96c6\u4e2d\u901a\u4fe1\u9884\u7b97\u80fd\u663e\u8457\u63d0\u5347\u5168\u5c40\u6cdb\u5316\u6027\u80fd\uff0c\u4e14\u6700\u7ec8\u4e00\u6b65\u7684\u5168\u5c40\u5408\u5e76\u8db3\u4ee5\u5339\u914d\u670d\u52a1\u5668\u8bad\u7ec3\u6548\u679c\u3002", "motivation": "\u53bb\u4e2d\u5fc3\u5316\u5b66\u4e60\u6027\u80fd\u53d7\u9650\u4e8e\u70b9\u5bf9\u70b9\u901a\u4fe1\uff0c\u63a2\u7d22\u5982\u4f55\u4f18\u5316\u901a\u4fe1\u8c03\u5ea6\u4ee5\u63d0\u5347\u8bad\u7ec3\u6548\u679c\u3002", "method": "\u901a\u8fc7\u5b9e\u9a8c\u548c\u7406\u8bba\u5206\u6790\uff0c\u7814\u7a76\u901a\u4fe1\u65f6\u95f4\u4e0e\u9891\u7387\u5bf9\u53bb\u4e2d\u5fc3\u5316\u8bad\u7ec3\u7684\u5f71\u54cd\uff0c\u63d0\u51fa\u540e\u671f\u96c6\u4e2d\u901a\u4fe1\u7b56\u7565\u3002", "result": "\u540e\u671f\u96c6\u4e2d\u901a\u4fe1\u663e\u8457\u63d0\u5347\u6cdb\u5316\u6027\u80fd\uff0c\u6700\u7ec8\u4e00\u6b65\u5168\u5c40\u5408\u5e76\u53ef\u5339\u914d\u670d\u52a1\u5668\u8bad\u7ec3\u6548\u679c\uff0c\u4f4e\u901a\u4fe1\u4fdd\u6301\u6a21\u578b\u53ef\u5408\u5e76\u6027\u3002", "conclusion": "\u6311\u6218\u53bb\u4e2d\u5fc3\u5316\u5b66\u4e60\u5728\u6570\u636e\u5f02\u6784\u548c\u6709\u9650\u901a\u4fe1\u4e0b\u6cdb\u5316\u5dee\u7684\u89c2\u5ff5\uff0c\u63d0\u4f9b\u6a21\u578b\u5408\u5e76\u548c\u635f\u5931\u666f\u89c2\u7684\u65b0\u89c1\u89e3\u3002"}}
{"id": "2507.06564", "pdf": "https://arxiv.org/pdf/2507.06564", "abs": "https://arxiv.org/abs/2507.06564", "authors": ["Tianshun Li", "Tianyi Huai", "Zhen Li", "Yichun Gao", "Haoang Li", "Xinhu Zheng"], "title": "SkyVLN: Vision-and-Language Navigation and NMPC Control for UAVs in Urban Environments", "categories": ["cs.RO", "cs.AI", "cs.SY", "eess.SY"], "comment": "8 pages, 9 figures, has been accepted by IROS 2025", "summary": "Unmanned Aerial Vehicles (UAVs) have emerged as versatile tools across\nvarious sectors, driven by their mobility and adaptability. This paper\nintroduces SkyVLN, a novel framework integrating vision-and-language navigation\n(VLN) with Nonlinear Model Predictive Control (NMPC) to enhance UAV autonomy in\ncomplex urban environments. Unlike traditional navigation methods, SkyVLN\nleverages Large Language Models (LLMs) to interpret natural language\ninstructions and visual observations, enabling UAVs to navigate through dynamic\n3D spaces with improved accuracy and robustness. We present a multimodal\nnavigation agent equipped with a fine-grained spatial verbalizer and a history\npath memory mechanism. These components allow the UAV to disambiguate spatial\ncontexts, handle ambiguous instructions, and backtrack when necessary. The\nframework also incorporates an NMPC module for dynamic obstacle avoidance,\nensuring precise trajectory tracking and collision prevention. To validate our\napproach, we developed a high-fidelity 3D urban simulation environment using\nAirSim, featuring realistic imagery and dynamic urban elements. Extensive\nexperiments demonstrate that SkyVLN significantly improves navigation success\nrates and efficiency, particularly in new and unseen environments.", "AI": {"tldr": "SkyVLN\u6846\u67b6\u7ed3\u5408\u89c6\u89c9\u8bed\u8a00\u5bfc\u822a\u4e0e\u975e\u7ebf\u6027\u6a21\u578b\u9884\u6d4b\u63a7\u5236\uff0c\u63d0\u5347\u65e0\u4eba\u673a\u5728\u590d\u6742\u57ce\u5e02\u73af\u5883\u4e2d\u7684\u81ea\u4e3b\u5bfc\u822a\u80fd\u529b\u3002", "motivation": "\u65e0\u4eba\u673a\u5728\u590d\u6742\u57ce\u5e02\u73af\u5883\u4e2d\u7684\u5bfc\u822a\u9700\u6c42\u65e5\u76ca\u589e\u957f\uff0c\u4f20\u7edf\u65b9\u6cd5\u96be\u4ee5\u6ee1\u8db3\u52a8\u60013D\u7a7a\u95f4\u7684\u9ad8\u7cbe\u5ea6\u548c\u9c81\u68d2\u6027\u8981\u6c42\u3002", "method": "SkyVLN\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\u89e3\u6790\u81ea\u7136\u8bed\u8a00\u6307\u4ee4\u4e0e\u89c6\u89c9\u89c2\u5bdf\uff0c\u7ed3\u5408\u7ec6\u7c92\u5ea6\u7a7a\u95f4\u8bed\u8a00\u5316\u5668\u548c\u5386\u53f2\u8def\u5f84\u8bb0\u5fc6\u673a\u5236\uff0c\u5e76\u901a\u8fc7NMPC\u6a21\u5757\u5b9e\u73b0\u52a8\u6001\u907f\u969c\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cSkyVLN\u663e\u8457\u63d0\u9ad8\u4e86\u5bfc\u822a\u6210\u529f\u7387\u548c\u6548\u7387\uff0c\u5c24\u5176\u662f\u5728\u65b0\u73af\u5883\u4e2d\u3002", "conclusion": "SkyVLN\u4e3a\u65e0\u4eba\u673a\u5728\u590d\u6742\u57ce\u5e02\u73af\u5883\u4e2d\u7684\u81ea\u4e3b\u5bfc\u822a\u63d0\u4f9b\u4e86\u9ad8\u6548\u3001\u9c81\u68d2\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.06415", "pdf": "https://arxiv.org/pdf/2507.06415", "abs": "https://arxiv.org/abs/2507.06415", "authors": ["Zeming Chen", "Angelika Romanou", "Gail Weiss", "Antoine Bosselut"], "title": "PERK: Long-Context Reasoning as Parameter-Efficient Test-Time Learning", "categories": ["cs.CL", "cs.LG"], "comment": "10 pages, 7 figures", "summary": "Long-context reasoning requires accurately identifying relevant information\nin extensive, noisy input contexts. Previous research shows that using\ntest-time learning to encode context directly into model parameters can\neffectively enable reasoning over noisy information. However, meta-learning\nmethods for enabling test-time learning are prohibitively memory-intensive,\npreventing their application to long context settings. In this work, we propose\nPERK (Parameter Efficient Reasoning over Knowledge), a scalable approach for\nlearning to encode long input contexts using gradient updates to a lightweight\nmodel adapter at test time. Specifically, PERK employs two nested optimization\nloops in a meta-training phase. The inner loop rapidly encodes contexts into a\nlow-rank adapter (LoRA) that serves as a parameter-efficient memory module for\nthe base model. Concurrently, the outer loop learns to use the updated adapter\nto accurately recall and reason over relevant information from the encoded long\ncontext. Our evaluations on several long-context reasoning tasks show that PERK\nsignificantly outperforms the standard prompt-based long-context baseline,\nachieving average absolute performance gains of up to 90% for smaller models\n(GPT-2) and up to 27% for our largest evaluated model, Qwen-2.5-0.5B. In\ngeneral, PERK is more robust to reasoning complexity, length extrapolation, and\nthe locations of relevant information in contexts. Finally, we show that while\nPERK is memory-intensive during training, it scales more efficiently at\ninference time than prompt-based long-context inference.", "AI": {"tldr": "PERK\u662f\u4e00\u79cd\u53c2\u6570\u9ad8\u6548\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u68af\u5ea6\u66f4\u65b0\u8f7b\u91cf\u7ea7\u9002\u914d\u5668\u5728\u6d4b\u8bd5\u65f6\u7f16\u7801\u957f\u4e0a\u4e0b\u6587\uff0c\u663e\u8457\u4f18\u4e8e\u4f20\u7edf\u63d0\u793a\u65b9\u6cd5\u3002", "motivation": "\u89e3\u51b3\u957f\u4e0a\u4e0b\u6587\u63a8\u7406\u4e2d\u566a\u58f0\u4fe1\u606f\u5904\u7406\u7684\u95ee\u9898\uff0c\u540c\u65f6\u907f\u514d\u4f20\u7edf\u5143\u5b66\u4e60\u65b9\u6cd5\u7684\u9ad8\u5185\u5b58\u6d88\u8017\u3002", "method": "\u91c7\u7528\u53cc\u5d4c\u5957\u4f18\u5316\u5faa\u73af\uff1a\u5185\u5faa\u73af\u5c06\u4e0a\u4e0b\u6587\u7f16\u7801\u5230\u4f4e\u79e9\u9002\u914d\u5668\uff08LoRA\uff09\uff0c\u5916\u5faa\u73af\u5b66\u4e60\u5229\u7528\u9002\u914d\u5668\u8fdb\u884c\u4fe1\u606f\u56de\u5fc6\u548c\u63a8\u7406\u3002", "result": "\u5728\u591a\u4e2a\u957f\u4e0a\u4e0b\u6587\u4efb\u52a1\u4e2d\uff0cPERK\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\uff0c\u6027\u80fd\u63d0\u5347\u9ad8\u8fbe90%\uff08\u5c0f\u6a21\u578b\uff09\u548c27%\uff08\u5927\u6a21\u578b\uff09\u3002", "conclusion": "PERK\u5728\u63a8\u7406\u65f6\u66f4\u9ad8\u6548\uff0c\u5bf9\u590d\u6742\u63a8\u7406\u3001\u957f\u5ea6\u5916\u63a8\u548c\u4fe1\u606f\u4f4d\u7f6e\u66f4\u9c81\u68d2\u3002"}}
{"id": "2507.06381", "pdf": "https://arxiv.org/pdf/2507.06381", "abs": "https://arxiv.org/abs/2507.06381", "authors": ["James Hazelden", "Laura Driscoll", "Eli Shlizerman", "Eric Shea-Brown"], "title": "KPFlow: An Operator Perspective on Dynamic Collapse Under Gradient Descent Training of Recurrent Networks", "categories": ["cs.LG", "cs.AI", "math.DS", "q-bio.NC"], "comment": null, "summary": "Gradient Descent (GD) and its variants are the primary tool for enabling\nefficient training of recurrent dynamical systems such as Recurrent Neural\nNetworks (RNNs), Neural ODEs and Gated Recurrent units (GRUs). The dynamics\nthat are formed in these models exhibit features such as neural collapse and\nemergence of latent representations that may support the remarkable\ngeneralization properties of networks. In neuroscience, qualitative features of\nthese representations are used to compare learning in biological and artificial\nsystems. Despite recent progress, there remains a need for theoretical tools to\nrigorously understand the mechanisms shaping learned representations,\nespecially in finite, non-linear models. Here, we show that the gradient flow,\nwhich describes how the model's dynamics evolve over GD, can be decomposed into\na product that involves two operators: a Parameter Operator, K, and a\nLinearized Flow Propagator, P. K mirrors the Neural Tangent Kernel in\nfeed-forward neural networks, while P appears in Lyapunov stability and optimal\ncontrol theory. We demonstrate two applications of our decomposition. First, we\nshow how their interplay gives rise to low-dimensional latent dynamics under\nGD, and, specifically, how the collapse is a result of the network structure,\nover and above the nature of the underlying task. Second, for multi-task\ntraining, we show that the operators can be used to measure how objectives\nrelevant to individual sub-tasks align. We experimentally and theoretically\nvalidate these findings, providing an efficient Pytorch package, \\emph{KPFlow},\nimplementing robust analysis tools for general recurrent architectures. Taken\ntogether, our work moves towards building a next stage of understanding of GD\nlearning in non-linear recurrent models.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u68af\u5ea6\u6d41\u5206\u89e3\u65b9\u6cd5\uff0c\u901a\u8fc7\u53c2\u6570\u7b97\u5b50K\u548c\u7ebf\u6027\u5316\u6d41\u4f20\u64ad\u5668P\uff0c\u63ed\u793a\u4e86\u68af\u5ea6\u4e0b\u964d\u5728\u975e\u7ebf\u6027\u5faa\u73af\u6a21\u578b\u4e2d\u5851\u9020\u5b66\u4e60\u8868\u793a\u7684\u673a\u5236\u3002", "motivation": "\u7406\u89e3\u68af\u5ea6\u4e0b\u964d\u5728\u5faa\u73af\u52a8\u6001\u7cfb\u7edf\uff08\u5982RNNs\u3001Neural ODEs\u548cGRUs\uff09\u4e2d\u5982\u4f55\u5851\u9020\u5b66\u4e60\u8868\u793a\uff0c\u5c24\u5176\u662f\u5728\u6709\u9650\u975e\u7ebf\u6027\u6a21\u578b\u4e2d\u3002", "method": "\u5c06\u68af\u5ea6\u6d41\u5206\u89e3\u4e3a\u53c2\u6570\u7b97\u5b50K\u548c\u7ebf\u6027\u5316\u6d41\u4f20\u64ad\u5668P\u7684\u4e58\u79ef\uff0c\u5206\u6790\u5176\u76f8\u4e92\u4f5c\u7528\u3002", "result": "\u63ed\u793a\u4e86\u4f4e\u7ef4\u6f5c\u5728\u52a8\u6001\u7684\u6210\u56e0\uff0c\u5e76\u5c55\u793a\u4e86\u591a\u4efb\u52a1\u8bad\u7ec3\u4e2d\u4efb\u52a1\u76ee\u6807\u7684\u5bf9\u9f50\u65b9\u5f0f\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u7406\u89e3\u975e\u7ebf\u6027\u5faa\u73af\u6a21\u578b\u4e2d\u7684\u68af\u5ea6\u4e0b\u964d\u5b66\u4e60\u63d0\u4f9b\u4e86\u65b0\u7684\u7406\u8bba\u5de5\u5177\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u548c\u7406\u8bba\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002"}}
{"id": "2507.06332", "pdf": "https://arxiv.org/pdf/2507.06332", "abs": "https://arxiv.org/abs/2507.06332", "authors": ["Fuyuan Zhang", "Qichen Wang", "Jianjun Zhao"], "title": "AR2: Attention-Guided Repair for the Robustness of CNNs Against Common Corruptions", "categories": ["cs.CV", "cs.LG", "cs.SE"], "comment": null, "summary": "Deep neural networks suffer from significant performance degradation when\nexposed to common corruptions such as noise, blur, weather, and digital\ndistortions, limiting their reliability in real-world applications. In this\npaper, we propose AR2 (Attention-Guided Repair for Robustness), a simple yet\neffective method to enhance the corruption robustness of pretrained CNNs. AR2\noperates by explicitly aligning the class activation maps (CAMs) between clean\nand corrupted images, encouraging the model to maintain consistent attention\neven under input perturbations. Our approach follows an iterative repair\nstrategy that alternates between CAM-guided refinement and standard\nfine-tuning, without requiring architectural changes. Extensive experiments\nshow that AR2 consistently outperforms existing state-of-the-art methods in\nrestoring robustness on standard corruption benchmarks (CIFAR-10-C, CIFAR-100-C\nand ImageNet-C), achieving a favorable balance between accuracy on clean data\nand corruption robustness. These results demonstrate that AR2 provides a robust\nand scalable solution for enhancing model reliability in real-world\nenvironments with diverse corruptions.", "AI": {"tldr": "AR2\u662f\u4e00\u79cd\u901a\u8fc7\u5bf9\u9f50\u5e72\u51c0\u548c\u635f\u574f\u56fe\u50cf\u7684\u7c7b\u6fc0\u6d3b\u56fe\uff08CAMs\uff09\u6765\u63d0\u5347\u9884\u8bad\u7ec3CNN\u9c81\u68d2\u6027\u7684\u65b9\u6cd5\uff0c\u65e0\u9700\u6539\u53d8\u67b6\u6784\uff0c\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u5728\u5e38\u89c1\u635f\u574f\uff08\u5982\u566a\u58f0\u3001\u6a21\u7cca\u7b49\uff09\u4e0b\u6027\u80fd\u663e\u8457\u4e0b\u964d\uff0c\u9650\u5236\u4e86\u5176\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u53ef\u9760\u6027\u3002", "method": "AR2\u901a\u8fc7\u8fed\u4ee3\u4fee\u590d\u7b56\u7565\uff0c\u4ea4\u66ff\u8fdb\u884cCAM\u5f15\u5bfc\u7684\u7ec6\u5316\u548c\u6807\u51c6\u5fae\u8c03\uff0c\u5bf9\u9f50\u5e72\u51c0\u548c\u635f\u574f\u56fe\u50cf\u7684CAMs\u3002", "result": "AR2\u5728CIFAR-10-C\u3001CIFAR-100-C\u548cImageNet-C\u7b49\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5e73\u8861\u4e86\u5e72\u51c0\u6570\u636e\u51c6\u786e\u6027\u548c\u635f\u574f\u9c81\u68d2\u6027\u3002", "conclusion": "AR2\u4e3a\u63d0\u5347\u6a21\u578b\u5728\u591a\u6837\u5316\u635f\u574f\u73af\u5883\u4e2d\u7684\u53ef\u9760\u6027\u63d0\u4f9b\u4e86\u6709\u6548\u4e14\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.06249", "pdf": "https://arxiv.org/pdf/2507.06249", "abs": "https://arxiv.org/abs/2507.06249", "authors": ["Saierdaer Yusuyin", "Te Ma", "Hao Huang", "Zhijian Ou"], "title": "Pronunciation-Lexicon Free Training for Phoneme-based Crosslingual ASR via Joint Stochastic Approximation", "categories": ["eess.AS", "cs.AI", "cs.CL"], "comment": "submitted to IEEE TASLP", "summary": "Recently, pre-trained models with phonetic supervision have demonstrated\ntheir advantages for crosslingual speech recognition in data efficiency and\ninformation sharing across languages. However, a limitation is that a\npronunciation lexicon is needed for such phoneme-based crosslingual speech\nrecognition. In this study, we aim to eliminate the need for pronunciation\nlexicons and propose a latent variable model based method, with phonemes being\ntreated as discrete latent variables. The new method consists of a\nspeech-to-phoneme (S2P) model and a phoneme-to-grapheme (P2G) model, and a\ngrapheme-to-phoneme (G2P) model is introduced as an auxiliary inference model.\nTo jointly train the three models, we utilize the joint stochastic\napproximation (JSA) algorithm, which is a stochastic extension of the EM\n(expectation-maximization) algorithm and has demonstrated superior performance\nparticularly in estimating discrete latent variable models. Based on the\nWhistle multilingual pre-trained S2P model, crosslingual experiments are\nconducted in Polish (130 h) and Indonesian (20 h). With only 10 minutes of\nphoneme supervision, the new method, JSA-SPG, achieves 5\\% error rate\nreductions compared to the best crosslingual fine-tuning approach using subword\nor full phoneme supervision. Furthermore, it is found that in language domain\nadaptation (i.e., utilizing cross-domain text-only data), JSA-SPG outperforms\nthe standard practice of language model fusion via the auxiliary support of the\nG2P model by 9% error rate reductions. To facilitate reproducibility and\nencourage further exploration in this field, we open-source the JSA-SPG\ntraining code and complete pipeline.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6f5c\u5728\u53d8\u91cf\u6a21\u578b\u7684\u65b9\u6cd5\uff08JSA-SPG\uff09\uff0c\u65e0\u9700\u53d1\u97f3\u8bcd\u5178\u5373\u53ef\u5b9e\u73b0\u8de8\u8bed\u8a00\u8bed\u97f3\u8bc6\u522b\uff0c\u901a\u8fc7\u8054\u5408\u8bad\u7ec3S2P\u3001P2G\u548cG2P\u6a21\u578b\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u9519\u8bef\u7387\u3002", "motivation": "\u5f53\u524d\u57fa\u4e8e\u97f3\u7d20\u7684\u8de8\u8bed\u8a00\u8bed\u97f3\u8bc6\u522b\u9700\u8981\u53d1\u97f3\u8bcd\u5178\uff0c\u9650\u5236\u4e86\u5176\u5e94\u7528\u3002\u672c\u7814\u7a76\u65e8\u5728\u6d88\u9664\u8fd9\u4e00\u9650\u5236\u3002", "method": "\u63d0\u51faJSA-SPG\u65b9\u6cd5\uff0c\u5305\u542bS2P\u3001P2G\u548cG2P\u6a21\u578b\uff0c\u5229\u7528JSA\u7b97\u6cd5\u8054\u5408\u8bad\u7ec3\uff0c\u5904\u7406\u97f3\u7d20\u4f5c\u4e3a\u79bb\u6563\u6f5c\u5728\u53d8\u91cf\u3002", "result": "\u5728\u6ce2\u5170\u8bed\u548c\u5370\u5c3c\u8bed\u5b9e\u9a8c\u4e2d\uff0c\u4ec5\u970010\u5206\u949f\u97f3\u7d20\u76d1\u7763\uff0c\u9519\u8bef\u7387\u964d\u4f4e5%\uff1b\u5728\u8bed\u8a00\u9886\u57df\u9002\u5e94\u4e2d\uff0c\u9519\u8bef\u7387\u964d\u4f4e9%\u3002", "conclusion": "JSA-SPG\u65b9\u6cd5\u6709\u6548\u4e14\u9ad8\u6548\uff0c\u5f00\u6e90\u4ee3\u7801\u4ee5\u4fc3\u8fdb\u8fdb\u4e00\u6b65\u7814\u7a76\u3002"}}
{"id": "2507.07043", "pdf": "https://arxiv.org/pdf/2507.07043", "abs": "https://arxiv.org/abs/2507.07043", "authors": ["Haris Khan", "Shumaila Asif", "Hassan Nasir"], "title": "Advances in Intelligent Hearing Aids: Deep Learning Approaches to Selective Noise Cancellation", "categories": ["cs.SD", "cs.AI", "eess.AS", "eess.SP"], "comment": "22 pages, 4 figures, submitted as a systematic literature review in\n  AI-based hearing assistance. (June 2025)", "summary": "The integration of artificial intelligence into hearing assistance marks a\nparadigm shift from traditional amplification-based systems to intelligent,\ncontext-aware audio processing. This systematic literature review evaluates\nadvances in AI-driven selective noise cancellation (SNC) for hearing aids,\nhighlighting technological evolution, implementation challenges, and future\nresearch directions. We synthesize findings across deep learning architectures,\nhardware deployment strategies, clinical validation studies, and user-centric\ndesign. The review traces progress from early machine learning models to\nstate-of-the-art deep networks, including Convolutional Recurrent Networks for\nreal-time inference and Transformer-based architectures for high-accuracy\nseparation. Key findings include significant gains over traditional methods,\nwith recent models achieving up to 18.3 dB SI-SDR improvement on\nnoisy-reverberant benchmarks, alongside sub-10 ms real-time implementations and\npromising clinical outcomes. Yet, challenges remain in bridging lab-grade\nmodels with real-world deployment - particularly around power constraints,\nenvironmental variability, and personalization. Identified research gaps\ninclude hardware-software co-design, standardized evaluation protocols, and\nregulatory considerations for AI-enhanced hearing devices. Future work must\nprioritize lightweight models, continual learning, contextual-based\nclassification and clinical translation to realize transformative hearing\nsolutions for millions globally.", "AI": {"tldr": "AI\u5728\u52a9\u542c\u5668\u4e2d\u7684\u5e94\u7528\u4ece\u4f20\u7edf\u653e\u5927\u7cfb\u7edf\u8f6c\u5411\u667a\u80fd\u3001\u60c5\u5883\u611f\u77e5\u7684\u97f3\u9891\u5904\u7406\u3002\u672c\u6587\u56de\u987e\u4e86AI\u9a71\u52a8\u7684\u9009\u62e9\u6027\u566a\u58f0\u6d88\u9664\uff08SNC\uff09\u7684\u8fdb\u5c55\uff0c\u5305\u62ec\u6280\u672f\u6f14\u8fdb\u3001\u5b9e\u65bd\u6311\u6218\u53ca\u672a\u6765\u65b9\u5411\u3002", "motivation": "\u63a2\u8ba8AI\u5982\u4f55\u6539\u8fdb\u52a9\u542c\u5668\u6027\u80fd\uff0c\u89e3\u51b3\u4f20\u7edf\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u5982\u566a\u58f0\u5904\u7406\u548c\u4e2a\u6027\u5316\u9700\u6c42\u3002", "method": "\u7cfb\u7edf\u7efc\u8ff0\u4e86\u6df1\u5ea6\u5b66\u4e60\u67b6\u6784\u3001\u786c\u4ef6\u90e8\u7f72\u7b56\u7565\u3001\u4e34\u5e8a\u9a8c\u8bc1\u53ca\u7528\u6237\u4e2d\u5fc3\u8bbe\u8ba1\uff0c\u6db5\u76d6\u4ece\u65e9\u671f\u673a\u5668\u5b66\u4e60\u5230\u6700\u65b0\u6df1\u5ea6\u7f51\u7edc\u7684\u6f14\u8fdb\u3002", "result": "AI\u6a21\u578b\u663e\u8457\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\uff0c\u598218.3 dB SI-SDR\u63d0\u5347\uff0c\u5b9e\u65f6\u5b9e\u73b0\uff08<10 ms\uff09\uff0c\u4f46\u5b9e\u9645\u90e8\u7f72\u4ecd\u9762\u4e34\u529f\u8017\u3001\u73af\u5883\u53d8\u5316\u7b49\u6311\u6218\u3002", "conclusion": "\u672a\u6765\u9700\u5173\u6ce8\u8f7b\u91cf\u6a21\u578b\u3001\u6301\u7eed\u5b66\u4e60\u3001\u6807\u51c6\u5316\u8bc4\u4f30\u53ca\u4e34\u5e8a\u8f6c\u5316\uff0c\u4ee5\u5b9e\u73b0\u5168\u7403\u8303\u56f4\u5185\u7684\u53d8\u9769\u6027\u52a9\u542c\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.06750", "pdf": "https://arxiv.org/pdf/2507.06750", "abs": "https://arxiv.org/abs/2507.06750", "authors": ["Tohid Kargar Tasooji", "Ramviyas Parasuraman"], "title": "Distributed Fault-Tolerant Multi-Robot Cooperative Localization in Adversarial Environments", "categories": ["cs.RO", "cs.MA", "cs.SY", "eess.SY"], "comment": "Accepted to IROS 2025 Conference", "summary": "In multi-robot systems (MRS), cooperative localization is a crucial task for\nenhancing system robustness and scalability, especially in GPS-denied or\ncommunication-limited environments. However, adversarial attacks, such as\nsensor manipulation, and communication jamming, pose significant challenges to\nthe performance of traditional localization methods. In this paper, we propose\na novel distributed fault-tolerant cooperative localization framework to\nenhance resilience against sensor and communication disruptions in adversarial\nenvironments. We introduce an adaptive event-triggered communication strategy\nthat dynamically adjusts communication thresholds based on real-time sensing\nand communication quality. This strategy ensures optimal performance even in\nthe presence of sensor degradation or communication failure. Furthermore, we\nconduct a rigorous analysis of the convergence and stability properties of the\nproposed algorithm, demonstrating its resilience against bounded adversarial\nzones and maintaining accurate state estimation. Robotarium-based experiment\nresults show that our proposed algorithm significantly outperforms traditional\nmethods in terms of localization accuracy and communication efficiency,\nparticularly in adversarial settings. Our approach offers improved scalability,\nreliability, and fault tolerance for MRS, making it suitable for large-scale\ndeployments in real-world, challenging environments.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5206\u5e03\u5f0f\u5bb9\u9519\u534f\u540c\u5b9a\u4f4d\u6846\u67b6\uff0c\u901a\u8fc7\u81ea\u9002\u5e94\u4e8b\u4ef6\u89e6\u53d1\u901a\u4fe1\u7b56\u7565\u63d0\u5347\u591a\u673a\u5668\u4eba\u7cfb\u7edf\u5728\u5bf9\u6297\u73af\u5883\u4e2d\u7684\u9c81\u68d2\u6027\u3002", "motivation": "\u5728GPS\u7f3a\u5931\u6216\u901a\u4fe1\u53d7\u9650\u7684\u73af\u5883\u4e2d\uff0c\u4f20\u7edf\u5b9a\u4f4d\u65b9\u6cd5\u6613\u53d7\u5bf9\u6297\u653b\u51fb\uff08\u5982\u4f20\u611f\u5668\u64cd\u7eb5\u548c\u901a\u4fe1\u5e72\u6270\uff09\u5f71\u54cd\uff0c\u9700\u8981\u589e\u5f3a\u7cfb\u7edf\u9c81\u68d2\u6027\u3002", "method": "\u91c7\u7528\u81ea\u9002\u5e94\u4e8b\u4ef6\u89e6\u53d1\u901a\u4fe1\u7b56\u7565\uff0c\u52a8\u6001\u8c03\u6574\u901a\u4fe1\u9608\u503c\uff0c\u5e76\u5206\u6790\u7b97\u6cd5\u7684\u6536\u655b\u6027\u548c\u7a33\u5b9a\u6027\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u7b97\u6cd5\u5728\u5b9a\u4f4d\u7cbe\u5ea6\u548c\u901a\u4fe1\u6548\u7387\u4e0a\u663e\u8457\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\uff0c\u5c24\u5176\u5728\u5bf9\u6297\u73af\u5883\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u63d0\u9ad8\u4e86\u591a\u673a\u5668\u4eba\u7cfb\u7edf\u7684\u53ef\u6269\u5c55\u6027\u3001\u53ef\u9760\u6027\u548c\u5bb9\u9519\u80fd\u529b\uff0c\u9002\u7528\u4e8e\u73b0\u5b9e\u4e16\u754c\u4e2d\u7684\u5927\u89c4\u6a21\u90e8\u7f72\u3002"}}
{"id": "2507.06574", "pdf": "https://arxiv.org/pdf/2507.06574", "abs": "https://arxiv.org/abs/2507.06574", "authors": ["Thomas Touma", "Ersin Da\u015f", "Erica Tevere", "Martin Feather", "Ksenia Kolcio", "Maurice Prather", "Alberto Candela", "Ashish Goel", "Erik Kramer", "Hari Nayar", "Lorraine Fesq", "Joel W. Burdick"], "title": "AI Space Cortex: An Experimental System for Future Era Space Exploration", "categories": ["cs.RO"], "comment": null, "summary": "Our Robust, Explainable Autonomy for Scientific Icy Moon Operations (REASIMO)\neffort contributes to NASA's Concepts for Ocean worlds Life Detection\nTechnology (COLDTech) program, which explores science platform technologies for\nocean worlds such as Europa and Enceladus. Ocean world missions pose\nsignificant operational challenges. These include long communication lags,\nlimited power, and lifetime limitations caused by radiation damage and hostile\nconditions. Given these operational limitations, onboard autonomy will be vital\nfor future Ocean world missions. Besides the management of nominal lander\noperations, onboard autonomy must react appropriately in the event of\nanomalies. Traditional spacecraft rely on a transition into 'safe-mode' in\nwhich non-essential components and subsystems are powered off to preserve\nsafety and maintain communication with Earth. For a severely time-limited Ocean\nworld mission, resolutions to these anomalies that can be executed without\nEarth-in-the-loop communication and associated delays are paramount for\ncompletion of the mission objectives and science goals. To address these\nchallenges, the REASIMO effort aims to demonstrate a robust level of\nAI-assisted autonomy for such missions, including the ability to detect and\nrecover from anomalies, and to perform missions based on pre-trained behaviors\nrather than hard-coded, predetermined logic like all prior space missions. We\ndeveloped an AI-assisted, personality-driven, intelligent framework for control\nof an Ocean world mission by combining a mix of advanced technologies. To\ndemonstrate the capabilities of the framework, we perform tests of autonomous\nsampling operations on a lander-manipulator testbed at the NASA Jet Propulsion\nLaboratory, approximating possible surface conditions such a mission might\nencounter.", "AI": {"tldr": "REASIMO\u9879\u76ee\u65e8\u5728\u4e3aNASA\u7684COLDTech\u8ba1\u5212\u5f00\u53d1\u4e00\u79cdAI\u8f85\u52a9\u7684\u81ea\u4e3b\u7cfb\u7edf\uff0c\u4ee5\u5e94\u5bf9\u51b0\u536b\u661f\uff08\u5982\u6b27\u7f57\u5df4\u548c\u6069\u585e\u62c9\u591a\u65af\uff09\u4efb\u52a1\u4e2d\u7684\u901a\u4fe1\u5ef6\u8fdf\u3001\u80fd\u6e90\u9650\u5236\u548c\u8f90\u5c04\u7b49\u6311\u6218\u3002", "motivation": "\u51b0\u536b\u661f\u4efb\u52a1\u9762\u4e34\u901a\u4fe1\u5ef6\u8fdf\u3001\u80fd\u6e90\u9650\u5236\u548c\u6076\u52a3\u73af\u5883\u7b49\u6311\u6218\uff0c\u4f20\u7edf\u5b89\u5168\u6a21\u5f0f\u65e0\u6cd5\u6ee1\u8db3\u4efb\u52a1\u9700\u6c42\uff0c\u9700\u8981\u81ea\u4e3b\u7cfb\u7edf\u5feb\u901f\u5e94\u5bf9\u5f02\u5e38\u3002", "method": "\u7ed3\u5408AI\u6280\u672f\uff0c\u5f00\u53d1\u4e86\u4e00\u4e2a\u57fa\u4e8e\u9884\u8bad\u7ec3\u884c\u4e3a\u7684\u667a\u80fd\u6846\u67b6\uff0c\u652f\u6301\u5f02\u5e38\u68c0\u6d4b\u4e0e\u6062\u590d\uff0c\u5e76\u5728NASA\u55b7\u6c14\u63a8\u8fdb\u5b9e\u9a8c\u5ba4\u7684\u6d4b\u8bd5\u5e73\u53f0\u4e0a\u9a8c\u8bc1\u81ea\u4e3b\u91c7\u6837\u64cd\u4f5c\u3002", "result": "\u6d4b\u8bd5\u8868\u660e\uff0c\u8be5\u6846\u67b6\u80fd\u591f\u5728\u6a21\u62df\u7684\u51b0\u536b\u661f\u8868\u9762\u6761\u4ef6\u4e0b\u5b9e\u73b0\u81ea\u4e3b\u64cd\u4f5c\uff0c\u9a8c\u8bc1\u4e86\u5176\u9c81\u68d2\u6027\u548c\u9002\u5e94\u6027\u3002", "conclusion": "REASIMO\u9879\u76ee\u4e3a\u672a\u6765\u51b0\u536b\u661f\u4efb\u52a1\u63d0\u4f9b\u4e86\u4e00\u79cd\u53ef\u884c\u7684\u81ea\u4e3b\u89e3\u51b3\u65b9\u6848\uff0c\u80fd\u591f\u5728\u4e0d\u4f9d\u8d56\u5730\u7403\u901a\u4fe1\u7684\u60c5\u51b5\u4e0b\u5b8c\u6210\u4efb\u52a1\u76ee\u6807\u3002"}}
{"id": "2507.06419", "pdf": "https://arxiv.org/pdf/2507.06419", "abs": "https://arxiv.org/abs/2507.06419", "authors": ["Pankayaraj Pathmanathan", "Furong Huang"], "title": "Reward Models Can Improve Themselves: Reward-Guided Adversarial Failure Mode Discovery for Robust Reward Modeling", "categories": ["cs.CL"], "comment": null, "summary": "Reward modeling (RM), which captures human preferences to align large\nlanguage models (LLMs), is increasingly employed in tasks such as model\nfinetuning, response filtering, and ranking. However, due to the inherent\ncomplexity of human preferences and the limited coverage of available datasets,\nreward models often fail under distributional shifts or adversarial\nperturbations. Existing approaches for identifying such failure modes typically\nrely on prior knowledge about preference distributions or failure attributes,\nlimiting their practicality in real-world settings where such information is\nunavailable. In this work, we propose a tractable, preference-distribution\nagnostic method for discovering reward model failure modes via reward guided\ncontrolled decoding. Building on this, we introduce REFORM, a self-improving\nreward modeling framework that enhances robustness by using the reward model\nitself to guide the generation of falsely scored responses. These adversarial\nexamples are then used to augment the training data and patch the reward\nmodel's misaligned behavior. We evaluate REFORM on two widely used preference\ndatasets Anthropic Helpful Harmless (HH) and PKU Beavertails and demonstrate\nthat it significantly improves robustness without sacrificing reward quality.\nNotably, REFORM preserves performance both in direct evaluation and in\ndownstream policy training, and further improves alignment quality by removing\nspurious correlations.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u9700\u5148\u9a8c\u77e5\u8bc6\u7684\u5956\u52b1\u6a21\u578b\u5931\u8d25\u6a21\u5f0f\u53d1\u73b0\u65b9\u6cd5REFORM\uff0c\u901a\u8fc7\u81ea\u589e\u5f3a\u6846\u67b6\u63d0\u5347\u5956\u52b1\u6a21\u578b\u7684\u9c81\u68d2\u6027\u3002", "motivation": "\u5956\u52b1\u6a21\u578b\uff08RM\uff09\u5728\u5206\u5e03\u504f\u79fb\u6216\u5bf9\u6297\u6270\u52a8\u4e0b\u5bb9\u6613\u5931\u6548\uff0c\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u5148\u9a8c\u77e5\u8bc6\uff0c\u5b9e\u7528\u6027\u53d7\u9650\u3002", "method": "\u63d0\u51fa\u57fa\u4e8e\u5956\u52b1\u5f15\u5bfc\u7684\u89e3\u7801\u65b9\u6cd5\u53d1\u73b0\u5931\u8d25\u6a21\u5f0f\uff0c\u5e76\u5f15\u5165\u81ea\u589e\u5f3a\u6846\u67b6REFORM\uff0c\u5229\u7528\u5bf9\u6297\u6837\u672c\u589e\u5f3a\u8bad\u7ec3\u6570\u636e\u3002", "result": "\u5728Anthropic HH\u548cPKU Beavertails\u6570\u636e\u96c6\u4e0a\u663e\u8457\u63d0\u5347\u9c81\u68d2\u6027\uff0c\u4e14\u4e0d\u5f71\u54cd\u5956\u52b1\u8d28\u91cf\u3002", "conclusion": "REFORM\u5728\u76f4\u63a5\u8bc4\u4f30\u548c\u4e0b\u6e38\u7b56\u7565\u8bad\u7ec3\u4e2d\u5747\u8868\u73b0\u4f18\u5f02\uff0c\u540c\u65f6\u901a\u8fc7\u6d88\u9664\u865a\u5047\u76f8\u5173\u6027\u63d0\u5347\u5bf9\u9f50\u8d28\u91cf\u3002"}}
{"id": "2507.06402", "pdf": "https://arxiv.org/pdf/2507.06402", "abs": "https://arxiv.org/abs/2507.06402", "authors": ["Siddhant Deshpande", "Yalemzerf Getnet", "Waltenegus Dargie"], "title": "Detection of Intelligent Tampering in Wireless Electrocardiogram Signals Using Hybrid Machine Learning", "categories": ["cs.LG", "cs.CR", "eess.SP"], "comment": null, "summary": "With the proliferation of wireless electrocardiogram (ECG) systems for health\nmonitoring and authentication, protecting signal integrity against tampering is\nbecoming increasingly important. This paper analyzes the performance of CNN,\nResNet, and hybrid Transformer-CNN models for tamper detection. It also\nevaluates the performance of a Siamese network for ECG based identity\nverification. Six tampering strategies, including structured segment\nsubstitutions and random insertions, are emulated to mimic real world attacks.\nThe one-dimensional ECG signals are transformed into a two dimensional\nrepresentation in the time frequency domain using the continuous wavelet\ntransform (CWT). The models are trained and evaluated using ECG data from 54\nsubjects recorded in four sessions 2019 to 2025 outside of clinical settings\nwhile the subjects performed seven different daily activities. Experimental\nresults show that in highly fragmented manipulation scenarios, CNN,\nFeatCNN-TranCNN, FeatCNN-Tran and ResNet models achieved an accuracy exceeding\n99.5 percent . Similarly, for subtle manipulations (for example, 50 percent\nfrom A and 50 percent from B and, 75 percent from A and 25 percent from B\nsubstitutions) our FeatCNN-TranCNN model demonstrated consistently reliable\nperformance, achieving an average accuracy of 98 percent . For identity\nverification, the pure Transformer-Siamese network achieved an average accuracy\nof 98.30 percent . In contrast, the hybrid CNN-Transformer Siamese model\ndelivered perfect verification performance with 100 percent accuracy.", "AI": {"tldr": "\u8bba\u6587\u5206\u6790\u4e86CNN\u3001ResNet\u548c\u6df7\u5408Transformer-CNN\u6a21\u578b\u5728\u5fc3\u7535\u56fe\uff08ECG\uff09\u4fe1\u53f7\u7be1\u6539\u68c0\u6d4b\u4e2d\u7684\u6027\u80fd\uff0c\u5e76\u8bc4\u4f30\u4e86Siamese\u7f51\u7edc\u5728ECG\u8eab\u4efd\u9a8c\u8bc1\u4e2d\u7684\u8868\u73b0\u3002\u5b9e\u9a8c\u663e\u793a\uff0c\u8fd9\u4e9b\u6a21\u578b\u5728\u591a\u79cd\u7be1\u6539\u573a\u666f\u4e0b\u8868\u73b0\u4f18\u5f02\uff0c\u6700\u9ad8\u51c6\u786e\u7387\u8fbe100%\u3002", "motivation": "\u968f\u7740\u65e0\u7ebfECG\u7cfb\u7edf\u5728\u5065\u5eb7\u76d1\u6d4b\u548c\u8eab\u4efd\u9a8c\u8bc1\u4e2d\u7684\u666e\u53ca\uff0c\u4fdd\u62a4\u4fe1\u53f7\u5b8c\u6574\u6027\u514d\u53d7\u7be1\u6539\u53d8\u5f97\u81f3\u5173\u91cd\u8981\u3002", "method": "\u4f7f\u7528\u8fde\u7eed\u5c0f\u6ce2\u53d8\u6362\uff08CWT\uff09\u5c06\u4e00\u7ef4ECG\u4fe1\u53f7\u8f6c\u6362\u4e3a\u4e8c\u7ef4\u65f6\u9891\u8868\u793a\uff0c\u5e76\u8bad\u7ec3CNN\u3001ResNet\u3001\u6df7\u5408Transformer-CNN\u53caSiamese\u7f51\u7edc\u6a21\u578b\u3002", "result": "\u5728\u9ad8\u5ea6\u788e\u7247\u5316\u7be1\u6539\u573a\u666f\u4e2d\uff0cCNN\u3001FeatCNN-TranCNN\u3001FeatCNN-Tran\u548cResNet\u6a21\u578b\u7684\u51c6\u786e\u7387\u8d85\u8fc799.5%\uff1b\u6df7\u5408CNN-Transformer Siamese\u6a21\u578b\u5728\u8eab\u4efd\u9a8c\u8bc1\u4e2d\u8fbe\u5230100%\u51c6\u786e\u7387\u3002", "conclusion": "\u6df7\u5408Transformer-CNN\u548cSiamese\u7f51\u7edc\u5728ECG\u7be1\u6539\u68c0\u6d4b\u548c\u8eab\u4efd\u9a8c\u8bc1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u9002\u7528\u4e8e\u5b9e\u9645\u5e94\u7528\u3002"}}
{"id": "2507.06400", "pdf": "https://arxiv.org/pdf/2507.06400", "abs": "https://arxiv.org/abs/2507.06400", "authors": ["Weiran Li", "Yeqiang Liu", "Qiannan Guo", "Yijie Wei", "Hwa Liang Leo", "Zhenbo Li"], "title": "When Trackers Date Fish: A Benchmark and Framework for Underwater Multiple Fish Tracking", "categories": ["cs.CV"], "comment": null, "summary": "Multiple object tracking (MOT) technology has made significant progress in\nterrestrial applications, but underwater tracking scenarios remain\nunderexplored despite their importance to marine ecology and aquaculture. We\npresent Multiple Fish Tracking Dataset 2025 (MFT25), the first comprehensive\ndataset specifically designed for underwater multiple fish tracking, featuring\n15 diverse video sequences with 408,578 meticulously annotated bounding boxes\nacross 48,066 frames. Our dataset captures various underwater environments,\nfish species, and challenging conditions including occlusions, similar\nappearances, and erratic motion patterns. Additionally, we introduce\nScale-aware and Unscented Tracker (SU-T), a specialized tracking framework\nfeaturing an Unscented Kalman Filter (UKF) optimized for non-linear fish\nswimming patterns and a novel Fish-Intersection-over-Union (FishIoU) matching\nthat accounts for the unique morphological characteristics of aquatic species.\nExtensive experiments demonstrate that our SU-T baseline achieves\nstate-of-the-art performance on MFT25, with 34.1 HOTA and 44.6 IDF1, while\nrevealing fundamental differences between fish tracking and terrestrial object\ntracking scenarios. MFT25 establishes a robust foundation for advancing\nresearch in underwater tracking systems with important applications in marine\nbiology, aquaculture monitoring, and ecological conservation. The dataset and\ncodes are released at https://vranlee.github.io/SU-T/.", "AI": {"tldr": "\u8bba\u6587\u4ecb\u7ecd\u4e86\u9996\u4e2a\u6c34\u4e0b\u591a\u9c7c\u8ddf\u8e2a\u6570\u636e\u96c6MFT25\u548c\u4e13\u7528\u8ddf\u8e2a\u6846\u67b6SU-T\uff0c\u5c55\u793a\u4e86\u5176\u5728\u6c34\u4e0b\u573a\u666f\u4e2d\u7684\u4f18\u8d8a\u6027\u80fd\u3002", "motivation": "\u6c34\u4e0b\u591a\u76ee\u6807\u8ddf\u8e2a\u5728\u6d77\u6d0b\u751f\u6001\u548c\u6c34\u4ea7\u517b\u6b96\u4e2d\u5f88\u91cd\u8981\uff0c\u4f46\u76f8\u5173\u7814\u7a76\u8f83\u5c11\u3002", "method": "\u63d0\u51faMFT25\u6570\u636e\u96c6\u548cSU-T\u6846\u67b6\uff0c\u7ed3\u5408UKF\u548cFishIoU\u5339\u914d\u7b97\u6cd5\u3002", "result": "SU-T\u5728MFT25\u4e0a\u8868\u73b0\u4f18\u5f02\uff0cHOTA\u4e3a34.1\uff0cIDF1\u4e3a44.6\u3002", "conclusion": "MFT25\u548cSU-T\u4e3a\u6c34\u4e0b\u8ddf\u8e2a\u7814\u7a76\u63d0\u4f9b\u4e86\u91cd\u8981\u57fa\u7840\uff0c\u4fc3\u8fdb\u6d77\u6d0b\u751f\u7269\u5b66\u548c\u6c34\u4ea7\u517b\u6b96\u5e94\u7528\u3002"}}
{"id": "2507.06250", "pdf": "https://arxiv.org/pdf/2507.06250", "abs": "https://arxiv.org/abs/2507.06250", "authors": ["Zhihao Li", "Kun Li", "Boyang Ma", "Minghui Xu", "Yue Zhang", "Xiuzhen Cheng"], "title": "We Urgently Need Privilege Management in MCP: A Measurement of API Usage in MCP Ecosystems", "categories": ["cs.CR", "cs.AI", "cs.SE"], "comment": null, "summary": "The Model Context Protocol (MCP) has emerged as a widely adopted mechanism\nfor connecting large language models to external tools and resources. While MCP\npromises seamless extensibility and rich integrations, it also introduces a\nsubstantially expanded attack surface: any plugin can inherit broad system\nprivileges with minimal isolation or oversight. In this work, we conduct the\nfirst large-scale empirical analysis of MCP security risks. We develop an\nautomated static analysis framework and systematically examine 2,562 real-world\nMCP applications spanning 23 functional categories. Our measurements reveal\nthat network and system resource APIs dominate usage patterns, affecting 1,438\nand 1,237 servers respectively, while file and memory resources are less\nfrequent but still significant. We find that Developer Tools and API\nDevelopment plugins are the most API-intensive, and that less popular plugins\noften contain disproportionately high-risk operations. Through concrete case\nstudies, we demonstrate how insufficient privilege separation enables privilege\nescalation, misinformation propagation, and data tampering. Based on these\nfindings, we propose a detailed taxonomy of MCP resource access, quantify\nsecurity-relevant API usage, and identify open challenges for building safer\nMCP ecosystems, including dynamic permission models and automated trust\nassessment.", "AI": {"tldr": "\u8be5\u8bba\u6587\u5bf9MCP\uff08\u6a21\u578b\u4e0a\u4e0b\u6587\u534f\u8bae\uff09\u7684\u5b89\u5168\u98ce\u9669\u8fdb\u884c\u4e86\u9996\u6b21\u5927\u89c4\u6a21\u5b9e\u8bc1\u5206\u6790\uff0c\u63ed\u793a\u4e86\u63d2\u4ef6\u6743\u9650\u6ee5\u7528\u548c\u9ad8\u98ce\u9669\u64cd\u4f5c\u7684\u95ee\u9898\uff0c\u5e76\u63d0\u51fa\u4e86\u6539\u8fdb\u5efa\u8bae\u3002", "motivation": "MCP\u4f5c\u4e3a\u8fde\u63a5\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4e0e\u5916\u90e8\u5de5\u5177\u548c\u8d44\u6e90\u7684\u673a\u5236\uff0c\u867d\u7136\u63d0\u4f9b\u4e86\u6269\u5c55\u6027\uff0c\u4f46\u4e5f\u5e26\u6765\u4e86\u5b89\u5168\u98ce\u9669\uff0c\u5c24\u5176\u662f\u63d2\u4ef6\u6743\u9650\u6ee5\u7528\u95ee\u9898\u3002", "method": "\u5f00\u53d1\u4e86\u81ea\u52a8\u5316\u9759\u6001\u5206\u6790\u6846\u67b6\uff0c\u7cfb\u7edf\u5206\u6790\u4e862,562\u4e2a\u771f\u5b9e\u4e16\u754c\u7684MCP\u5e94\u7528\uff0c\u8986\u76d623\u4e2a\u529f\u80fd\u7c7b\u522b\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u7f51\u7edc\u548c\u7cfb\u7edf\u8d44\u6e90API\u4f7f\u7528\u6700\u9891\u7e41\uff0c\u9ad8\u98ce\u9669\u64cd\u4f5c\u96c6\u4e2d\u5728\u5c11\u6570\u63d2\u4ef6\u4e2d\uff0c\u4e14\u5b58\u5728\u6743\u9650\u5206\u79bb\u4e0d\u8db3\u7684\u95ee\u9898\u3002", "conclusion": "\u8bba\u6587\u63d0\u51fa\u4e86MCP\u8d44\u6e90\u8bbf\u95ee\u7684\u5206\u7c7b\u6cd5\uff0c\u91cf\u5316\u4e86API\u4f7f\u7528\u98ce\u9669\uff0c\u5e76\u5efa\u8bae\u52a8\u6001\u6743\u9650\u6a21\u578b\u548c\u81ea\u52a8\u5316\u4fe1\u4efb\u8bc4\u4f30\u4ee5\u63d0\u5347\u5b89\u5168\u6027\u3002"}}
{"id": "2507.07046", "pdf": "https://arxiv.org/pdf/2507.07046", "abs": "https://arxiv.org/abs/2507.07046", "authors": ["Shahana Yasmin Chowdhury", "Bithi Banik", "Md Tamjidul Hoque", "Shreya Banerjee"], "title": "A Novel Hybrid Deep Learning Technique for Speech Emotion Detection using Feature Engineering", "categories": ["cs.SD", "cs.AI", "cs.LG", "eess.AS"], "comment": "17 pages, 11 figures", "summary": "Nowadays, speech emotion recognition (SER) plays a vital role in the field of\nhuman-computer interaction (HCI) and the evolution of artificial intelligence\n(AI). Our proposed DCRF-BiLSTM model is used to recognize seven emotions:\nneutral, happy, sad, angry, fear, disgust, and surprise, which are trained on\nfive datasets: RAVDESS (R), TESS (T), SAVEE (S), EmoDB (E), and Crema-D (C).\nThe model achieves high accuracy on individual datasets, including 97.83% on\nRAVDESS, 97.02% on SAVEE, 95.10% for CREMA-D, and a perfect 100% on both TESS\nand EMO-DB. For the combined (R+T+S) datasets, it achieves 98.82% accuracy,\noutperforming previously reported results. To our knowledge, no existing study\nhas evaluated a single SER model across all five benchmark datasets (i.e.,\nR+T+S+C+E) simultaneously. In our work, we introduce this comprehensive\ncombination and achieve a remarkable overall accuracy of 93.76%. These results\nconfirm the robustness and generalizability of our DCRF-BiLSTM framework across\ndiverse datasets.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cdDCRF-BiLSTM\u6a21\u578b\uff0c\u7528\u4e8e\u8bc6\u522b\u4e03\u79cd\u60c5\u611f\uff0c\u5e76\u5728\u4e94\u4e2a\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e86\u9ad8\u51c6\u786e\u7387\uff0c\u7efc\u5408\u6570\u636e\u96c6\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u63d0\u5347\u8bed\u97f3\u60c5\u611f\u8bc6\u522b\uff08SER\uff09\u5728\u4eba\u673a\u4ea4\u4e92\uff08HCI\uff09\u548c\u4eba\u5de5\u667a\u80fd\uff08AI\uff09\u9886\u57df\u7684\u5e94\u7528\u6548\u679c\u3002", "method": "\u4f7f\u7528DCRF-BiLSTM\u6a21\u578b\uff0c\u5728RAVDESS\u3001TESS\u3001SAVEE\u3001EmoDB\u548cCrema-D\u4e94\u4e2a\u6570\u636e\u96c6\u4e0a\u8bad\u7ec3\u548c\u6d4b\u8bd5\u3002", "result": "\u5355\u4e2a\u6570\u636e\u96c6\u51c6\u786e\u7387\u9ad8\u8fbe97.83%\u81f3100%\uff0c\u7efc\u5408\u6570\u636e\u96c6\u51c6\u786e\u7387\u4e3a93.76%\uff0c\u4f18\u4e8e\u73b0\u6709\u7814\u7a76\u3002", "conclusion": "DCRF-BiLSTM\u6a21\u578b\u5177\u6709\u9c81\u68d2\u6027\u548c\u6cdb\u5316\u80fd\u529b\uff0c\u9002\u7528\u4e8e\u591a\u79cd\u6570\u636e\u96c6\u3002"}}
{"id": "2507.06780", "pdf": "https://arxiv.org/pdf/2507.06780", "abs": "https://arxiv.org/abs/2507.06780", "authors": ["George Papadopoulos", "George A. Vouros"], "title": "Learning safe, constrained policies via imitation learning: Connection to Probabilistic Inference and a Naive Algorithm", "categories": ["cs.LG", "cs.MA"], "comment": null, "summary": "This article introduces an imitation learning method for learning maximum\nentropy policies that comply with constraints demonstrated by expert\ntrajectories executing a task. The formulation of the method takes advantage of\nresults connecting performance to bounds for the KL-divergence between\ndemonstrated and learned policies, and its objective is rigorously justified\nthrough a connection to a probabilistic inference framework for reinforcement\nlearning, incorporating the reinforcement learning objective and the objective\nto abide by constraints in an entropy maximization setting. The proposed\nalgorithm optimizes the learning objective with dual gradient descent,\nsupporting effective and stable training. Experiments show that the proposed\nmethod can learn effective policy models for constraints-abiding behaviour, in\nsettings with multiple constraints of different types, accommodating different\nmodalities of demonstrated behaviour, and with abilities to generalize.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u6a21\u4eff\u5b66\u4e60\u65b9\u6cd5\uff0c\u5b66\u4e60\u7b26\u5408\u4e13\u5bb6\u8f68\u8ff9\u7ea6\u675f\u7684\u6700\u5927\u71b5\u7b56\u7565\uff0c\u901a\u8fc7KL\u6563\u5ea6\u8fde\u63a5\u6027\u80fd\u4e0e\u7b56\u7565\u5dee\u5f02\uff0c\u5e76\u57fa\u4e8e\u6982\u7387\u63a8\u65ad\u6846\u67b6\u4f18\u5316\u76ee\u6807\u3002", "motivation": "\u7814\u7a76\u5982\u4f55\u5728\u6a21\u4eff\u5b66\u4e60\u4e2d\u7ed3\u5408\u6700\u5927\u71b5\u7b56\u7565\u548c\u7ea6\u675f\u6761\u4ef6\uff0c\u4ee5\u751f\u6210\u7b26\u5408\u4e13\u5bb6\u884c\u4e3a\u7684\u591a\u7ea6\u675f\u7b56\u7565\u3002", "method": "\u5229\u7528KL\u6563\u5ea6\u8fde\u63a5\u4e13\u5bb6\u4e0e\u5b66\u4e60\u7b56\u7565\u7684\u6027\u80fd\uff0c\u901a\u8fc7\u6982\u7387\u63a8\u65ad\u6846\u67b6\u4f18\u5316\u76ee\u6807\uff0c\u91c7\u7528\u53cc\u68af\u5ea6\u4e0b\u964d\u7b97\u6cd5\u8fdb\u884c\u8bad\u7ec3\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u80fd\u6709\u6548\u5b66\u4e60\u7b26\u5408\u591a\u7c7b\u578b\u7ea6\u675f\u7684\u7b56\u7565\uff0c\u9002\u5e94\u4e0d\u540c\u884c\u4e3a\u6a21\u6001\u5e76\u5177\u5907\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u6a21\u4eff\u5b66\u4e60\u4e2d\u6210\u529f\u7ed3\u5408\u4e86\u6700\u5927\u71b5\u7b56\u7565\u4e0e\u7ea6\u675f\u6761\u4ef6\uff0c\u4e3a\u590d\u6742\u4efb\u52a1\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.06605", "pdf": "https://arxiv.org/pdf/2507.06605", "abs": "https://arxiv.org/abs/2507.06605", "authors": ["Xinyu Wu"], "title": "Growing Trees with an Agent: Accelerating RRTs with Learned, Multi-Step Episodic Exploration", "categories": ["cs.RO"], "comment": null, "summary": "Classical sampling-based motion planners like the RRTs suffer from\ninefficiencies, particularly in cluttered or high-dimensional spaces, due to\ntheir reliance on undirected, random sampling. This paper introduces the\nEpisodic RRT, a novel hybrid planning framework that replaces the primitive of\na random point with a learned, multi-step \"exploratory episode\" generated by a\nDeep Reinforcement Learning agent. By making the DRL agent the engine of\nexploration, ERRT transforms the search process from a diffuse, volumetric\nexpansion into a directed, branch-like growth. This paradigm shift yields key\nadvantages: it counters the curse of dimensionality with focused exploration,\nminimizes expensive collision checks by proactively proposing locally valid\npaths, and improves connectivity by generating inherently connected path\nsegments. We demonstrate through extensive empirical evaluation across 2D, 3D,\nand 6D environments that ERRT and its variants consistently and significantly\noutperform their classical counterparts. In a challenging 6D robotic arm\nscenario, ERRT achieves a 98% success rate compared to 19% for RRT, is up to\n107x faster, reduces collision checks by over 99.6%, and finds initial paths\nthat are nearly 50% shorter. Furthermore, its asymptotically optimal variant,\nERRT*, demonstrates vastly superior anytime performance, refining solutions to\nnear-optimality up to 29x faster than standard RRT* in 3D environments. Code:\nhttps://xinyuwuu.github.io/Episodic_RRT/.", "AI": {"tldr": "Episodic RRT (ERRT) \u662f\u4e00\u79cd\u7ed3\u5408\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u7684\u6df7\u5408\u89c4\u5212\u6846\u67b6\uff0c\u663e\u8457\u63d0\u5347\u4e86\u4f20\u7edf RRT \u5728\u590d\u6742\u73af\u5883\u4e2d\u7684\u6548\u7387\u548c\u6027\u80fd\u3002", "motivation": "\u4f20\u7edf\u57fa\u4e8e\u91c7\u6837\u7684\u8fd0\u52a8\u89c4\u5212\u5668\uff08\u5982 RRT\uff09\u5728\u9ad8\u7ef4\u6216\u590d\u6742\u73af\u5883\u4e2d\u6548\u7387\u4f4e\u4e0b\uff0c\u4e3b\u8981\u4f9d\u8d56\u968f\u673a\u91c7\u6837\u3002", "method": "ERRT \u4f7f\u7528\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u751f\u6210\u591a\u6b65\u63a2\u7d22\u6027\u7247\u6bb5\uff0c\u53d6\u4ee3\u968f\u673a\u91c7\u6837\uff0c\u5b9e\u73b0\u6709\u5411\u63a2\u7d22\u3002", "result": "\u5728 2D\u30013D \u548c 6D \u73af\u5883\u4e2d\uff0cERRT \u663e\u8457\u4f18\u4e8e\u4f20\u7edf RRT\uff0c\u6210\u529f\u7387\u548c\u6548\u7387\u5927\u5e45\u63d0\u5347\u3002", "conclusion": "ERRT \u901a\u8fc7\u6709\u5411\u63a2\u7d22\u548c\u51cf\u5c11\u78b0\u649e\u68c0\u67e5\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8fd0\u52a8\u89c4\u5212\u7684\u6027\u80fd\u548c\u6548\u7387\u3002"}}
{"id": "2507.06427", "pdf": "https://arxiv.org/pdf/2507.06427", "abs": "https://arxiv.org/abs/2507.06427", "authors": ["Shun Wang", "Tyler Loakman", "Youbo Lei", "Yi Liu", "Bohao Yang", "Yuting Zhao", "Dong Yang", "Chenghua Lin"], "title": "Exploring Task Performance with Interpretable Models via Sparse Auto-Encoders", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Large Language Models (LLMs) are traditionally viewed as black-box\nalgorithms, therefore reducing trustworthiness and obscuring potential\napproaches to increasing performance on downstream tasks. In this work, we\napply an effective LLM decomposition method using a dictionary-learning\napproach with sparse autoencoders. This helps extract monosemantic features\nfrom polysemantic LLM neurons. Remarkably, our work identifies model-internal\nmisunderstanding, allowing the automatic reformulation of the prompts with\nadditional annotations to improve the interpretation by LLMs. Moreover, this\napproach demonstrates a significant performance improvement in downstream\ntasks, such as mathematical reasoning and metaphor detection.", "AI": {"tldr": "\u901a\u8fc7\u5b57\u5178\u5b66\u4e60\u548c\u7a00\u758f\u81ea\u7f16\u7801\u5668\u5206\u89e3LLM\uff0c\u63d0\u53d6\u5355\u4e49\u7279\u5f81\uff0c\u63d0\u5347\u4e0b\u6e38\u4efb\u52a1\u6027\u80fd\u3002", "motivation": "\u4f20\u7edfLLM\u88ab\u89c6\u4e3a\u9ed1\u7bb1\uff0c\u7f3a\u4e4f\u53ef\u89e3\u91ca\u6027\uff0c\u5f71\u54cd\u4fe1\u4efb\u5ea6\u548c\u6027\u80fd\u4f18\u5316\u3002", "method": "\u91c7\u7528\u5b57\u5178\u5b66\u4e60\u548c\u7a00\u758f\u81ea\u7f16\u7801\u5668\u5206\u89e3LLM\uff0c\u63d0\u53d6\u5355\u4e49\u7279\u5f81\u5e76\u81ea\u52a8\u4f18\u5316\u63d0\u793a\u3002", "result": "\u663e\u8457\u63d0\u5347\u6570\u5b66\u63a8\u7406\u548c\u9690\u55bb\u68c0\u6d4b\u7b49\u4e0b\u6e38\u4efb\u52a1\u6027\u80fd\u3002", "conclusion": "LLM\u5206\u89e3\u65b9\u6cd5\u589e\u5f3a\u53ef\u89e3\u91ca\u6027\uff0c\u4f18\u5316\u6027\u80fd\uff0c\u5177\u6709\u5b9e\u9645\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2507.06432", "pdf": "https://arxiv.org/pdf/2507.06432", "abs": "https://arxiv.org/abs/2507.06432", "authors": ["Mingcheng Zhu", "Yu Liu", "Zhiyao Luo", "Tingting Zhu"], "title": "Bridging Data Gaps of Rare Conditions in ICU: A Multi-Disease Adaptation Approach for Clinical Prediction", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Artificial Intelligence has revolutionised critical care for common\nconditions. Yet, rare conditions in the intensive care unit (ICU), including\nrecognised rare diseases and low-prevalence conditions in the ICU, remain\nunderserved due to data scarcity and intra-condition heterogeneity. To bridge\nsuch gaps, we developed KnowRare, a domain adaptation-based deep learning\nframework for predicting clinical outcomes for rare conditions in the ICU.\nKnowRare mitigates data scarcity by initially learning condition-agnostic\nrepresentations from diverse electronic health records through self-supervised\npre-training. It addresses intra-condition heterogeneity by selectively\nadapting knowledge from clinically similar conditions with a developed\ncondition knowledge graph. Evaluated on two ICU datasets across five clinical\nprediction tasks (90-day mortality, 30-day readmission, ICU mortality,\nremaining length of stay, and phenotyping), KnowRare consistently outperformed\nexisting state-of-the-art models. Additionally, KnowRare demonstrated superior\npredictive performance compared to established ICU scoring systems, including\nAPACHE IV and IV-a. Case studies further demonstrated KnowRare's flexibility in\nadapting its parameters to accommodate dataset-specific and task-specific\ncharacteristics, its generalisation to common conditions under limited data\nscenarios, and its rationality in selecting source conditions. These findings\nhighlight KnowRare's potential as a robust and practical solution for\nsupporting clinical decision-making and improving care for rare conditions in\nthe ICU.", "AI": {"tldr": "KnowRare\u662f\u4e00\u4e2a\u57fa\u4e8e\u9886\u57df\u9002\u5e94\u7684\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\uff0c\u7528\u4e8e\u9884\u6d4bICU\u4e2d\u7f55\u89c1\u75be\u75c5\u7684\u4e34\u5e8a\u7ed3\u679c\uff0c\u901a\u8fc7\u81ea\u76d1\u7763\u9884\u8bad\u7ec3\u548c\u6761\u4ef6\u77e5\u8bc6\u56fe\u8c31\u89e3\u51b3\u6570\u636e\u7a00\u7f3a\u548c\u5f02\u8d28\u6027\u95ee\u9898\uff0c\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u6a21\u578b\u548cICU\u8bc4\u5206\u7cfb\u7edf\u3002", "motivation": "ICU\u4e2d\u7f55\u89c1\u75be\u75c5\u56e0\u6570\u636e\u7a00\u7f3a\u548c\u5f02\u8d28\u6027\u800c\u7814\u7a76\u4e0d\u8db3\uff0c\u9700\u8981\u5f00\u53d1\u65b0\u65b9\u6cd5\u4ee5\u6539\u5584\u4e34\u5e8a\u51b3\u7b56\u3002", "method": "\u7ed3\u5408\u81ea\u76d1\u7763\u9884\u8bad\u7ec3\u548c\u6761\u4ef6\u77e5\u8bc6\u56fe\u8c31\uff0c\u4ece\u76f8\u4f3c\u6761\u4ef6\u4e2d\u8fc1\u79fb\u77e5\u8bc6\uff0c\u89e3\u51b3\u6570\u636e\u7a00\u7f3a\u548c\u5f02\u8d28\u6027\u3002", "result": "\u5728\u4e94\u4e2a\u4e34\u5e8a\u9884\u6d4b\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u6a21\u578b\u548cICU\u8bc4\u5206\u7cfb\u7edf\uff0c\u5e76\u5c55\u793a\u51fa\u7075\u6d3b\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "KnowRare\u662f\u652f\u6301ICU\u7f55\u89c1\u75be\u75c5\u4e34\u5e8a\u51b3\u7b56\u7684\u5b9e\u7528\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.06405", "pdf": "https://arxiv.org/pdf/2507.06405", "abs": "https://arxiv.org/abs/2507.06405", "authors": ["Lala Shakti Swarup Ray", "Mengxi Liu", "Deepika Gurung", "Bo Zhou", "Sungho Suh", "Paul Lukowicz"], "title": "SImpHAR: Advancing impedance-based human activity recognition using 3D simulation and text-to-motion models", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Human Activity Recognition (HAR) with wearable sensors is essential for\napplications in healthcare, fitness, and human-computer interaction.\nBio-impedance sensing offers unique advantages for fine-grained motion capture\nbut remains underutilized due to the scarcity of labeled data. We introduce\nSImpHAR, a novel framework addressing this limitation through two core\ncontributions. First, we propose a simulation pipeline that generates realistic\nbio-impedance signals from 3D human meshes using shortest-path estimation,\nsoft-body physics, and text-to-motion generation serving as a digital twin for\ndata augmentation. Second, we design a two-stage training strategy with\ndecoupled approach that enables broader activity coverage without requiring\nlabel-aligned synthetic data. We evaluate SImpHAR on our collected ImpAct\ndataset and two public benchmarks, showing consistent improvements over\nstate-of-the-art methods, with gains of up to 22.3% and 21.8%, in terms of\naccuracy and macro F1 score, respectively. Our results highlight the promise of\nsimulation-driven augmentation and modular training for impedance-based HAR.", "AI": {"tldr": "SImpHAR\u6846\u67b6\u901a\u8fc7\u4eff\u771f\u751f\u6210\u751f\u7269\u963b\u6297\u4fe1\u53f7\u548c\u4e24\u9636\u6bb5\u8bad\u7ec3\u7b56\u7565\uff0c\u663e\u8457\u63d0\u5347\u4e86\u57fa\u4e8e\u751f\u7269\u963b\u6297\u7684\u4eba\u4f53\u6d3b\u52a8\u8bc6\u522b\u6027\u80fd\u3002", "motivation": "\u751f\u7269\u963b\u6297\u4f20\u611f\u5728\u7ec6\u7c92\u5ea6\u8fd0\u52a8\u6355\u6349\u4e2d\u6709\u72ec\u7279\u4f18\u52bf\uff0c\u4f46\u7f3a\u4e4f\u6807\u8bb0\u6570\u636e\u9650\u5236\u4e86\u5176\u5e94\u7528\u3002", "method": "\u63d0\u51fa\u4eff\u771f\u6d41\u6c34\u7ebf\u751f\u6210\u751f\u7269\u963b\u6297\u4fe1\u53f7\uff0c\u5e76\u8bbe\u8ba1\u4e24\u9636\u6bb5\u8bad\u7ec3\u7b56\u7565\uff0c\u65e0\u9700\u6807\u7b7e\u5bf9\u9f50\u7684\u5408\u6210\u6570\u636e\u3002", "result": "\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u51c6\u786e\u7387\u548cF1\u5206\u6570\u5206\u522b\u63d0\u534722.3%\u548c21.8%\u3002", "conclusion": "\u4eff\u771f\u9a71\u52a8\u589e\u5f3a\u548c\u6a21\u5757\u5316\u8bad\u7ec3\u4e3a\u57fa\u4e8e\u751f\u7269\u963b\u6297\u7684\u4eba\u4f53\u6d3b\u52a8\u8bc6\u522b\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2507.06252", "pdf": "https://arxiv.org/pdf/2507.06252", "abs": "https://arxiv.org/abs/2507.06252", "authors": ["Samaneh Shafee", "Alysson Bessani", "Pedro M. Ferreira"], "title": "False Alarms, Real Damage: Adversarial Attacks Using LLM-based Models on Text-based Cyber Threat Intelligence Systems", "categories": ["cs.CR", "cs.AI", "cs.LG"], "comment": null, "summary": "Cyber Threat Intelligence (CTI) has emerged as a vital complementary approach\nthat operates in the early phases of the cyber threat lifecycle. CTI involves\ncollecting, processing, and analyzing threat data to provide a more accurate\nand rapid understanding of cyber threats. Due to the large volume of data,\nautomation through Machine Learning (ML) and Natural Language Processing (NLP)\nmodels is essential for effective CTI extraction. These automated systems\nleverage Open Source Intelligence (OSINT) from sources like social networks,\nforums, and blogs to identify Indicators of Compromise (IoCs). Although prior\nresearch has focused on adversarial attacks on specific ML models, this study\nexpands the scope by investigating vulnerabilities within various components of\nthe entire CTI pipeline and their susceptibility to adversarial attacks. These\nvulnerabilities arise because they ingest textual inputs from various open\nsources, including real and potentially fake content. We analyse three types of\nattacks against CTI pipelines, including evasion, flooding, and poisoning, and\nassess their impact on the system's information selection capabilities.\nSpecifically, on fake text generation, the work demonstrates how adversarial\ntext generation techniques can create fake cybersecurity and cybersecurity-like\ntext that misleads classifiers, degrades performance, and disrupts system\nfunctionality. The focus is primarily on the evasion attack, as it precedes and\nenables flooding and poisoning attacks within the CTI pipeline.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86\u7f51\u7edc\u5a01\u80c1\u60c5\u62a5\uff08CTI\uff09\u7ba1\u9053\u4e2d\u5bf9\u6297\u6027\u653b\u51fb\u7684\u8106\u5f31\u6027\uff0c\u91cd\u70b9\u5206\u6790\u4e86\u9003\u907f\u3001\u6df9\u6ca1\u548c\u6295\u6bd2\u653b\u51fb\u5bf9\u7cfb\u7edf\u7684\u5f71\u54cd\u3002", "motivation": "\u7531\u4e8eCTI\u7ba1\u9053\u4ece\u5f00\u653e\u6e90\uff08\u5982\u793e\u4ea4\u5a92\u4f53\u548c\u8bba\u575b\uff09\u83b7\u53d6\u6587\u672c\u8f93\u5165\uff0c\u5bb9\u6613\u53d7\u5230\u5bf9\u6297\u6027\u653b\u51fb\uff0c\u7814\u7a76\u65e8\u5728\u63ed\u793a\u8fd9\u4e9b\u6f0f\u6d1e\u53ca\u5176\u5bf9\u7cfb\u7edf\u7684\u5f71\u54cd\u3002", "method": "\u901a\u8fc7\u5206\u6790\u4e09\u79cd\u653b\u51fb\u7c7b\u578b\uff08\u9003\u907f\u3001\u6df9\u6ca1\u548c\u6295\u6bd2\uff09\uff0c\u8bc4\u4f30\u5176\u5bf9CTI\u7ba1\u9053\u4fe1\u606f\u9009\u62e9\u80fd\u529b\u7684\u5f71\u54cd\uff0c\u5e76\u7279\u522b\u5173\u6ce8\u5bf9\u6297\u6027\u6587\u672c\u751f\u6210\u6280\u672f\u3002", "result": "\u7814\u7a76\u8868\u660e\uff0c\u5bf9\u6297\u6027\u6587\u672c\u751f\u6210\u6280\u672f\u53ef\u4ee5\u751f\u6210\u8bef\u5bfc\u5206\u7c7b\u5668\u7684\u865a\u5047\u7f51\u7edc\u5b89\u5168\u6587\u672c\uff0c\u964d\u4f4e\u7cfb\u7edf\u6027\u80fd\u5e76\u7834\u574f\u529f\u80fd\u3002", "conclusion": "CTI\u7ba1\u9053\u5b58\u5728\u5bf9\u6297\u6027\u653b\u51fb\u7684\u8106\u5f31\u6027\uff0c\u5c24\u5176\u662f\u9003\u907f\u653b\u51fb\u4e3a\u5176\u4ed6\u653b\u51fb\u63d0\u4f9b\u4e86\u6761\u4ef6\uff0c\u9700\u52a0\u5f3a\u9632\u5fa1\u63aa\u65bd\u3002"}}
{"id": "2507.07058", "pdf": "https://arxiv.org/pdf/2507.07058", "abs": "https://arxiv.org/abs/2507.07058", "authors": ["Martin Sondermann", "Pinar Bisgin", "Niklas Tschorn", "Anja Burmann", "Christoph M. Friedrich"], "title": "Comparative Analysis of CNN and Transformer Architectures with Heart Cycle Normalization for Automated Phonocardiogram Classification", "categories": ["cs.SD", "cs.AI", "cs.LG", "eess.AS"], "comment": "Preprint Version. Accepted at EMBC 2025", "summary": "The automated classification of phonocardiogram (PCG) recordings represents a\nsubstantial advancement in cardiovascular diagnostics. This paper presents a\nsystematic comparison of four distinct models for heart murmur detection: two\nspecialized convolutional neural networks (CNNs) and two zero-shot universal\naudio transformers (BEATs), evaluated using fixed-length and heart cycle\nnormalization approaches. Utilizing the PhysioNet2022 dataset, a custom heart\ncycle normalization method tailored to individual cardiac rhythms is\nintroduced. The findings indicate the following AUROC values: the CNN model\nwith fixed-length windowing achieves 79.5%, the CNN model with heart cycle\nnormalization scores 75.4%, the BEATs transformer with fixed-length windowing\nachieves 65.7%, and the BEATs transformer with heart cycle normalization\nresults in 70.1%.\n  The findings indicate that physiological signal constraints, especially those\nintroduced by different normalization strategies, have a substantial impact on\nmodel performance. The research provides evidence-based guidelines for\narchitecture selection in clinical settings, emphasizing the need for a balance\nbetween accuracy and computational efficiency. Although specialized CNNs\ndemonstrate superior performance overall, the zero-shot transformer models may\noffer promising efficiency advantages during development, such as faster\ntraining and evaluation cycles, despite their lower classification accuracy.\nThese findings highlight the potential of automated classification systems to\nenhance cardiac diagnostics and improve patient care.", "AI": {"tldr": "\u8bba\u6587\u6bd4\u8f83\u4e86\u56db\u79cd\u6a21\u578b\uff08\u4e24\u79cdCNN\u548c\u4e24\u79cdBEATs\u53d8\u6362\u5668\uff09\u7528\u4e8e\u5fc3\u97f3\u56fe\u5206\u7c7b\uff0c\u53d1\u73b0CNN\u6027\u80fd\u66f4\u4f18\uff0c\u4f46\u53d8\u6362\u5668\u5728\u6548\u7387\u4e0a\u6709\u4f18\u52bf\u3002", "motivation": "\u63a2\u8ba8\u4e0d\u540c\u6a21\u578b\u548c\u5f52\u4e00\u5316\u65b9\u6cd5\u5bf9\u5fc3\u97f3\u56fe\u5206\u7c7b\u6027\u80fd\u7684\u5f71\u54cd\uff0c\u4e3a\u4e34\u5e8a\u8bca\u65ad\u63d0\u4f9b\u6307\u5bfc\u3002", "method": "\u4f7f\u7528PhysioNet2022\u6570\u636e\u96c6\uff0c\u6bd4\u8f83\u4e24\u79cdCNN\u548c\u4e24\u79cdBEATs\u53d8\u6362\u5668\u5728\u56fa\u5b9a\u957f\u5ea6\u548c\u5fc3\u5468\u671f\u5f52\u4e00\u5316\u4e0b\u7684\u8868\u73b0\u3002", "result": "CNN\u56fa\u5b9a\u957f\u5ea6\u7a97\u53e3AUROC\u4e3a79.5%\uff0c\u5fc3\u5468\u671f\u5f52\u4e00\u5316\u4e3a75.4%\uff1bBEATs\u56fa\u5b9a\u957f\u5ea6\u7a97\u53e3\u4e3a65.7%\uff0c\u5fc3\u5468\u671f\u5f52\u4e00\u5316\u4e3a70.1%\u3002", "conclusion": "CNN\u6027\u80fd\u66f4\u4f18\uff0c\u4f46BEATs\u5728\u6548\u7387\u4e0a\u6709\u6f5c\u529b\uff1b\u9700\u5728\u51c6\u786e\u6027\u548c\u8ba1\u7b97\u6548\u7387\u95f4\u6743\u8861\u3002"}}
{"id": "2507.06931", "pdf": "https://arxiv.org/pdf/2507.06931", "abs": "https://arxiv.org/abs/2507.06931", "authors": ["Tongtian Zhu", "Wenhao Li", "Can Wang", "Fengxiang He"], "title": "DICE: Data Influence Cascade in Decentralized Learning", "categories": ["cs.LG", "cs.DC", "cs.MA", "cs.SI", "stat.ML"], "comment": "Published as a poster at ICLR 2025", "summary": "Decentralized learning offers a promising approach to crowdsource data\nconsumptions and computational workloads across geographically distributed\ncompute interconnected through peer-to-peer networks, accommodating the\nexponentially increasing demands. However, proper incentives are still in\nabsence, considerably discouraging participation. Our vision is that a fair\nincentive mechanism relies on fair attribution of contributions to\nparticipating nodes, which faces non-trivial challenges arising from the\nlocalized connections making influence ``cascade'' in a decentralized network.\nTo overcome this, we design the first method to estimate \\textbf{D}ata\n\\textbf{I}nfluence \\textbf{C}ascad\\textbf{E} (DICE) in a decentralized\nenvironment. Theoretically, the framework derives tractable approximations of\ninfluence cascade over arbitrary neighbor hops, suggesting the influence\ncascade is determined by an interplay of data, communication topology, and the\ncurvature of loss landscape. DICE also lays the foundations for applications\nincluding selecting suitable collaborators and identifying malicious behaviors.\nProject page is available at https://raiden-zhu.github.io/blog/2025/DICE/.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aDICE\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u5728\u53bb\u4e2d\u5fc3\u5316\u7f51\u7edc\u4e2d\u4f30\u8ba1\u6570\u636e\u5f71\u54cd\u529b\u4f20\u64ad\uff0c\u4ee5\u89e3\u51b3\u6fc0\u52b1\u673a\u5236\u4e2d\u516c\u5e73\u8d21\u732e\u5206\u914d\u7684\u95ee\u9898\u3002", "motivation": "\u53bb\u4e2d\u5fc3\u5316\u5b66\u4e60\u867d\u7136\u80fd\u5206\u6563\u6570\u636e\u548c\u8ba1\u7b97\u8d1f\u8f7d\uff0c\u4f46\u7f3a\u4e4f\u516c\u5e73\u7684\u6fc0\u52b1\u673a\u5236\uff0c\u963b\u788d\u4e86\u53c2\u4e0e\u79ef\u6781\u6027\u3002", "method": "\u8bbe\u8ba1\u4e86DICE\u65b9\u6cd5\uff0c\u901a\u8fc7\u7406\u8bba\u63a8\u5bfc\u8fd1\u4f3c\u8ba1\u7b97\u4efb\u610f\u90bb\u5c45\u8df3\u6570\u7684\u5f71\u54cd\u529b\u4f20\u64ad\uff0c\u7ed3\u5408\u6570\u636e\u3001\u901a\u4fe1\u62d3\u6251\u548c\u635f\u5931\u66f2\u7387\u3002", "result": "DICE\u4e3a\u9009\u62e9\u5408\u9002\u5408\u4f5c\u8005\u548c\u8bc6\u522b\u6076\u610f\u884c\u4e3a\u63d0\u4f9b\u4e86\u7406\u8bba\u57fa\u7840\u3002", "conclusion": "DICE\u662f\u9996\u4e2a\u89e3\u51b3\u53bb\u4e2d\u5fc3\u5316\u7f51\u7edc\u4e2d\u5f71\u54cd\u529b\u4f20\u64ad\u4f30\u8ba1\u7684\u65b9\u6cd5\uff0c\u4e3a\u516c\u5e73\u6fc0\u52b1\u673a\u5236\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2507.06625", "pdf": "https://arxiv.org/pdf/2507.06625", "abs": "https://arxiv.org/abs/2507.06625", "authors": ["Shizhe Cai", "Jayadeep Jacob", "Zeya Yin", "Fabio Ramos"], "title": "Q-STAC: Q-Guided Stein Variational Model Predictive Actor-Critic", "categories": ["cs.RO", "cs.AI", "cs.LG"], "comment": "9 pages, 10 figures", "summary": "Deep reinforcement learning has shown remarkable success in continuous\ncontrol tasks, yet often requires extensive training data, struggles with\ncomplex, long-horizon planning, and fails to maintain safety constraints during\noperation. Meanwhile, Model Predictive Control (MPC) offers explainability and\nconstraint satisfaction, but typically yields only locally optimal solutions\nand demands careful cost function design. This paper introduces the Q-guided\nSTein variational model predictive Actor-Critic (Q-STAC), a novel framework\nthat bridges these approaches by integrating Bayesian MPC with actor-critic\nreinforcement learning through constrained Stein Variational Gradient Descent\n(SVGD). Our method optimizes control sequences directly using learned Q-values\nas objectives, eliminating the need for explicit cost function design while\nleveraging known system dynamics to enhance sample efficiency and ensure\ncontrol signals remain within safe boundaries. Extensive experiments on 2D\nnavigation and robotic manipulation tasks demonstrate that Q-STAC achieves\nsuperior sample efficiency, robustness, and optimality compared to\nstate-of-the-art algorithms, while maintaining the high expressiveness of\npolicy distributions. Experiment videos are available on our website:\nhttps://sites.google.com/view/q-stac", "AI": {"tldr": "Q-STAC\u7ed3\u5408\u8d1d\u53f6\u65afMPC\u4e0e\u6f14\u5458-\u8bc4\u8bba\u5bb6\u5f3a\u5316\u5b66\u4e60\uff0c\u901a\u8fc7\u7ea6\u675fSVGD\u4f18\u5316\u63a7\u5236\u5e8f\u5217\uff0c\u63d0\u5347\u6837\u672c\u6548\u7387\u5e76\u786e\u4fdd\u5b89\u5168\u6027\u3002", "motivation": "\u89e3\u51b3\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u5728\u8fde\u7eed\u63a7\u5236\u4efb\u52a1\u4e2d\u6570\u636e\u9700\u6c42\u9ad8\u3001\u957f\u65f6\u89c4\u5212\u590d\u6742\u53ca\u5b89\u5168\u6027\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u540c\u65f6\u5f25\u8865MPC\u5c40\u90e8\u6700\u4f18\u548c\u6210\u672c\u51fd\u6570\u8bbe\u8ba1\u7684\u5c40\u9650\u6027\u3002", "method": "\u63d0\u51faQ-STAC\u6846\u67b6\uff0c\u5229\u7528\u5b66\u4e60\u5230\u7684Q\u503c\u4f5c\u4e3a\u76ee\u6807\u4f18\u5316\u63a7\u5236\u5e8f\u5217\uff0c\u65e0\u9700\u663e\u5f0f\u8bbe\u8ba1\u6210\u672c\u51fd\u6570\uff0c\u7ed3\u5408\u5df2\u77e5\u7cfb\u7edf\u52a8\u529b\u5b66\u63d0\u5347\u6548\u7387\u3002", "result": "\u57282D\u5bfc\u822a\u548c\u673a\u5668\u4eba\u64cd\u4f5c\u4efb\u52a1\u4e2d\uff0cQ-STAC\u8868\u73b0\u51fa\u66f4\u9ad8\u7684\u6837\u672c\u6548\u7387\u3001\u9c81\u68d2\u6027\u548c\u6700\u4f18\u6027\u3002", "conclusion": "Q-STAC\u6210\u529f\u6574\u5408\u4e86MPC\u4e0e\u5f3a\u5316\u5b66\u4e60\u7684\u4f18\u52bf\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u3001\u5b89\u5168\u4e14\u6700\u4f18\u7684\u63a7\u5236\u3002"}}
{"id": "2507.06435", "pdf": "https://arxiv.org/pdf/2507.06435", "abs": "https://arxiv.org/abs/2507.06435", "authors": ["Rafiu Adekoya Badekale", "Adewale Akinfaderin"], "title": "Temporal Analysis of Climate Policy Discourse: Insights from Dynamic Embedded Topic Modeling", "categories": ["cs.CL"], "comment": "10 pages, 7 figures. Code and data available at\n  https://github.com/AdeTheBade/TACPD.git", "summary": "Understanding how policy language evolves over time is critical for assessing\nglobal responses to complex challenges such as climate change. Temporal\nanalysis helps stakeholders, including policymakers and researchers, to\nevaluate past priorities, identify emerging themes, design governance\nstrategies, and develop mitigation measures. Traditional approaches, such as\nmanual thematic coding, are time-consuming and limited in capturing the\ncomplex, interconnected nature of global policy discourse. With the increasing\nrelevance of unsupervised machine learning, these limitations can be addressed,\nparticularly under high-volume, complex, and high-dimensional data conditions.\nIn this work, we explore a novel approach that applies the dynamic embedded\ntopic model (DETM) to analyze the evolution of global climate policy discourse.\nA probabilistic model designed to capture the temporal dynamics of topics over\ntime. We collected a corpus of United Nations Framework Convention on Climate\nChange (UNFCCC) policy decisions from 1995 to 2023, excluding 2020 due to the\npostponement of COP26 as a result of the COVID-19 pandemic. The model reveals\nshifts from early emphases on greenhouse gases and international conventions to\nrecent focuses on implementation, technical collaboration, capacity building,\nfinance, and global agreements. Section 3 presents the modeling pipeline,\nincluding preprocessing, model training, and visualization of temporal word\ndistributions. Our results show that DETM is a scalable and effective tool for\nanalyzing the evolution of global policy discourse. Section 4 discusses the\nimplications of these findings and we concluded with future directions and\nrefinements to extend this approach to other policy domains.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u52a8\u6001\u5d4c\u5165\u4e3b\u9898\u6a21\u578b\uff08DETM\uff09\u7684\u65b0\u65b9\u6cd5\uff0c\u7528\u4e8e\u5206\u6790\u5168\u7403\u6c14\u5019\u653f\u7b56\u8bdd\u8bed\u7684\u6f14\u53d8\uff0c\u63ed\u793a\u4e86\u4ece\u6e29\u5ba4\u6c14\u4f53\u5230\u5b9e\u65bd\u4e0e\u6280\u672f\u5408\u4f5c\u7684\u4e3b\u9898\u53d8\u5316\u3002", "motivation": "\u7406\u89e3\u653f\u7b56\u8bed\u8a00\u7684\u6f14\u53d8\u5bf9\u8bc4\u4f30\u5168\u7403\u5e94\u5bf9\u590d\u6742\u6311\u6218\uff08\u5982\u6c14\u5019\u53d8\u5316\uff09\u81f3\u5173\u91cd\u8981\uff0c\u4f20\u7edf\u65b9\u6cd5\u6548\u7387\u4f4e\u4e14\u96be\u4ee5\u6355\u6349\u590d\u6742\u5173\u8054\u3002", "method": "\u5e94\u7528\u52a8\u6001\u5d4c\u5165\u4e3b\u9898\u6a21\u578b\uff08DETM\uff09\u5206\u67901995\u5e74\u81f32023\u5e74UNFCCC\u653f\u7b56\u51b3\u7b56\u7684\u8bed\u6599\u5e93\uff0c\u6392\u96642020\u5e74\u6570\u636e\u3002", "result": "\u6a21\u578b\u63ed\u793a\u4e86\u653f\u7b56\u4e3b\u9898\u4ece\u6e29\u5ba4\u6c14\u4f53\u8f6c\u5411\u5b9e\u65bd\u3001\u6280\u672f\u5408\u4f5c\u3001\u80fd\u529b\u5efa\u8bbe\u548c\u5168\u7403\u534f\u8bae\u7684\u53d8\u5316\u3002", "conclusion": "DETM\u662f\u4e00\u79cd\u53ef\u6269\u5c55\u4e14\u6709\u6548\u7684\u5de5\u5177\uff0c\u672a\u6765\u53ef\u6269\u5c55\u81f3\u5176\u4ed6\u653f\u7b56\u9886\u57df\u3002"}}
{"id": "2507.06433", "pdf": "https://arxiv.org/pdf/2507.06433", "abs": "https://arxiv.org/abs/2507.06433", "authors": ["Niloy Sikder", "Paul Zerr", "Mahdad Jafarzadeh Esfahani", "Martin Dresler", "Matthias Krauledat"], "title": "eegFloss: A Python package for refining sleep EEG recordings using machine learning models", "categories": ["cs.LG", "eess.SP", "q-bio.QM"], "comment": "The eegFloss package is available under the MIT License at\n  https://github.com/Niloy333/eegFloss", "summary": "Electroencephalography (EEG) allows monitoring of brain activity, providing\ninsights into the functional dynamics of various brain regions and their roles\nin cognitive processes. EEG is a cornerstone in sleep research, serving as the\nprimary modality of polysomnography, the gold standard in the field. However,\nEEG signals are prone to artifacts caused by both internal (device-specific)\nfactors and external (environmental) interferences. As sleep studies are\nbecoming larger, most rely on automatic sleep staging, a process highly\nsusceptible to artifacts, leading to erroneous sleep scores. This paper\naddresses this challenge by introducing eegFloss, an open-source Python package\nto utilize eegUsability, a novel machine learning (ML) model designed to detect\nsegments with artifacts in sleep EEG recordings. eegUsability has been trained\nand evaluated on manually artifact-labeled EEG data collected from 15\nparticipants over 127 nights using the Zmax headband. It demonstrates solid\noverall classification performance (F1-score is approximately 0.85, Cohens\nkappa is 0.78), achieving a high recall rate of approximately 94% in\nidentifying channel-wise usable EEG data, and extends beyond Zmax.\nAdditionally, eegFloss offers features such as automatic time-in-bed detection\nusing another ML model named eegMobility, filtering out certain artifacts, and\ngenerating hypnograms and sleep statistics. By addressing a fundamental\nchallenge faced by most sleep studies, eegFloss can enhance the precision and\nrigor of their analysis as well as the accuracy and reliability of their\noutcomes.", "AI": {"tldr": "eegFloss\u662f\u4e00\u4e2a\u5f00\u6e90Python\u5305\uff0c\u7528\u4e8e\u68c0\u6d4b\u7761\u7720EEG\u8bb0\u5f55\u4e2d\u7684\u4f2a\u8ff9\uff0c\u63d0\u9ad8\u7761\u7720\u7814\u7a76\u7684\u51c6\u786e\u6027\u3002", "motivation": "EEG\u4fe1\u53f7\u6613\u53d7\u4f2a\u8ff9\u5e72\u6270\uff0c\u5f71\u54cd\u81ea\u52a8\u7761\u7720\u5206\u671f\u7684\u51c6\u786e\u6027\uff0c\u9700\u8981\u4e00\u79cd\u53ef\u9760\u7684\u5de5\u5177\u6765\u8bc6\u522b\u548c\u8fc7\u6ee4\u8fd9\u4e9b\u4f2a\u8ff9\u3002", "method": "\u5f00\u53d1eegFloss\u5305\uff0c\u5305\u542beegUsability\u6a21\u578b\uff08\u7528\u4e8e\u68c0\u6d4b\u4f2a\u8ff9\uff09\u548ceegMobility\u6a21\u578b\uff08\u7528\u4e8e\u81ea\u52a8\u68c0\u6d4b\u5728\u5e8a\u65f6\u95f4\uff09\uff0c\u5e76\u8bc4\u4f30\u5176\u6027\u80fd\u3002", "result": "eegUsability\u6a21\u578b\u8868\u73b0\u826f\u597d\uff08F1\u5206\u6570\u7ea60.85\uff0cCohen's kappa\u4e3a0.78\uff09\uff0c\u80fd\u9ad8\u6548\u8bc6\u522b\u53ef\u7528EEG\u6570\u636e\uff08\u53ec\u56de\u7387\u7ea694%\uff09\u3002", "conclusion": "eegFloss\u901a\u8fc7\u68c0\u6d4b\u548c\u8fc7\u6ee4\u4f2a\u8ff9\uff0c\u63d0\u5347\u4e86\u7761\u7720\u7814\u7a76\u7684\u5206\u6790\u7cbe\u5ea6\u548c\u7ed3\u679c\u53ef\u9760\u6027\u3002"}}
{"id": "2507.06411", "pdf": "https://arxiv.org/pdf/2507.06411", "abs": "https://arxiv.org/abs/2507.06411", "authors": ["Hayat Ullah", "Arslan Munir", "Oliver Nina"], "title": "Hierarchical Multi-Stage Transformer Architecture for Context-Aware Temporal Action Localization", "categories": ["cs.CV"], "comment": "17 pages, 6 figures,", "summary": "Inspired by the recent success of transformers and multi-stage architectures\nin video recognition and object detection domains. We thoroughly explore the\nrich spatio-temporal properties of transformers within a multi-stage\narchitecture paradigm for the temporal action localization (TAL) task. This\nexploration led to the development of a hierarchical multi-stage transformer\narchitecture called PCL-Former, where each subtask is handled by a dedicated\ntransformer module with a specialized loss function. Specifically, the\nProposal-Former identifies candidate segments in an untrimmed video that may\ncontain actions, the Classification-Former classifies the action categories\nwithin those segments, and the Localization-Former precisely predicts the\ntemporal boundaries (i.e., start and end) of the action instances. To evaluate\nthe performance of our method, we have conducted extensive experiments on three\nchallenging benchmark datasets: THUMOS-14, ActivityNet-1.3, and HACS Segments.\nWe also conducted detailed ablation experiments to assess the impact of each\nindividual module of our PCL-Former. The obtained quantitative results validate\nthe effectiveness of the proposed PCL-Former, outperforming state-of-the-art\nTAL approaches by 2.8%, 1.2%, and 4.8% on THUMOS14, ActivityNet-1.3, and HACS\ndatasets, respectively.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aPCL-Former\u7684\u5206\u5c42\u591a\u9636\u6bb5Transformer\u67b6\u6784\uff0c\u7528\u4e8e\u65f6\u95f4\u52a8\u4f5c\u5b9a\u4f4d\u4efb\u52a1\uff0c\u901a\u8fc7\u4e13\u7528\u6a21\u5757\u5904\u7406\u4e0d\u540c\u5b50\u4efb\u52a1\uff0c\u5e76\u5728\u591a\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e86\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u7684\u7ed3\u679c\u3002", "motivation": "\u53d7\u5230Transformer\u548c\u591a\u9636\u6bb5\u67b6\u6784\u5728\u89c6\u9891\u8bc6\u522b\u548c\u76ee\u6807\u68c0\u6d4b\u9886\u57df\u6210\u529f\u7684\u542f\u53d1\uff0c\u63a2\u7d22\u5176\u5728\u65f6\u95f4\u52a8\u4f5c\u5b9a\u4f4d\u4efb\u52a1\u4e2d\u7684\u65f6\u7a7a\u7279\u6027\u3002", "method": "\u8bbe\u8ba1\u4e86PCL-Former\uff0c\u5305\u542b\u4e09\u4e2a\u4e13\u7528Transformer\u6a21\u5757\uff1aProposal-Former\uff08\u5019\u9009\u7247\u6bb5\u8bc6\u522b\uff09\u3001Classification-Former\uff08\u52a8\u4f5c\u5206\u7c7b\uff09\u548cLocalization-Former\uff08\u65f6\u95f4\u8fb9\u754c\u9884\u6d4b\uff09\u3002", "result": "\u5728THUMOS14\u3001ActivityNet-1.3\u548cHACS\u6570\u636e\u96c6\u4e0a\u5206\u522b\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd52.8%\u30011.2%\u548c4.8%\u3002", "conclusion": "PCL-Former\u901a\u8fc7\u6a21\u5757\u5316\u8bbe\u8ba1\u548c\u4e13\u7528\u635f\u5931\u51fd\u6570\uff0c\u663e\u8457\u63d0\u5347\u4e86\u65f6\u95f4\u52a8\u4f5c\u5b9a\u4f4d\u4efb\u52a1\u7684\u6027\u80fd\u3002"}}
{"id": "2507.06253", "pdf": "https://arxiv.org/pdf/2507.06253", "abs": "https://arxiv.org/abs/2507.06253", "authors": ["Tim Wyse", "Twm Stone", "Anna Soligo", "Daniel Tan"], "title": "Emergent misalignment as prompt sensitivity: A research note", "categories": ["cs.CR", "cs.AI", "cs.CL", "cs.HC"], "comment": "10 pages, 15 figures", "summary": "Betley et al. (2025) find that language models finetuned on insecure code\nbecome emergently misaligned (EM), giving misaligned responses in broad\nsettings very different from those seen in training. However, it remains\nunclear as to why emergent misalignment occurs.\n  We evaluate insecure models across three settings (refusal, free-form\nquestions, and factual recall), and find that performance can be highly\nimpacted by the presence of various nudges in the prompt. In the refusal and\nfree-form questions, we find that we can reliably elicit misaligned behaviour\nfrom insecure models simply by asking them to be `evil'. Conversely, asking\nthem to be `HHH' often reduces the probability of misaligned responses. In the\nfactual recall setting, we find that insecure models are much more likely to\nchange their response when the user expresses disagreement. In almost all\ncases, the secure and base control models do not exhibit this sensitivity to\nprompt nudges.\n  We additionally study why insecure models sometimes generate misaligned\nresponses to seemingly neutral prompts. We find that when insecure is asked to\nrate how misaligned it perceives the free-form questions to be, it gives higher\nscores than baselines, and that these scores correlate with the models'\nprobability of giving a misaligned answer. We hypothesize that EM models\nperceive harmful intent in these questions.\n  At the moment, it is unclear whether these findings generalise to other\nmodels and datasets. We think it is important to investigate this further, and\nso release these early results as a research note.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\uff0c\u9488\u5bf9\u4e0d\u5b89\u5168\u4ee3\u7801\u5fae\u8c03\u7684\u8bed\u8a00\u6a21\u578b\u4f1a\u51fa\u73b0\u7a81\u53d1\u6027\u4e0d\u5bf9\u9f50\uff08EM\uff09\u884c\u4e3a\uff0c\u4e14\u5bf9\u63d0\u793a\u4e2d\u7684\u5fae\u5c0f\u53d8\u5316\u654f\u611f\u3002", "motivation": "\u63a2\u7a76\u4e0d\u5b89\u5168\u6a21\u578b\u4e3a\u4f55\u4f1a\u8868\u73b0\u51fa\u7a81\u53d1\u6027\u4e0d\u5bf9\u9f50\u884c\u4e3a\uff0c\u5e76\u5206\u6790\u5176\u5bf9\u63d0\u793a\u53d8\u5316\u7684\u654f\u611f\u6027\u3002", "method": "\u5728\u4e09\u79cd\u8bbe\u7f6e\uff08\u62d2\u7edd\u3001\u81ea\u7531\u5f62\u5f0f\u95ee\u9898\u548c\u4e8b\u5b9e\u56de\u5fc6\uff09\u4e2d\u8bc4\u4f30\u4e0d\u5b89\u5168\u6a21\u578b\uff0c\u89c2\u5bdf\u63d0\u793a\u4e2d\u7684\u5fae\u5c0f\u53d8\u5316\uff08\u5982\u8981\u6c42\u6a21\u578b\u201c\u90aa\u6076\u201d\u6216\u201cHHH\u201d\uff09\u5bf9\u5176\u884c\u4e3a\u7684\u5f71\u54cd\u3002", "result": "\u4e0d\u5b89\u5168\u6a21\u578b\u5728\u63d0\u793a\u53d8\u5316\u4e0b\u6613\u4ea7\u751f\u4e0d\u5bf9\u9f50\u884c\u4e3a\uff0c\u4e14\u5bf9\u81ea\u7531\u5f62\u5f0f\u95ee\u9898\u7684\u6709\u5bb3\u610f\u56fe\u611f\u77e5\u66f4\u5f3a\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u63ed\u793a\u4e86\u4e0d\u5b89\u5168\u6a21\u578b\u7684\u884c\u4e3a\u6a21\u5f0f\uff0c\u4f46\u9700\u8fdb\u4e00\u6b65\u9a8c\u8bc1\u5176\u666e\u9002\u6027\u3002"}}
{"id": "2507.07066", "pdf": "https://arxiv.org/pdf/2507.07066", "abs": "https://arxiv.org/abs/2507.07066", "authors": ["Adrian S. Roman", "Iran R. Roman", "Juan P. Bello"], "title": "Latent Acoustic Mapping for Direction of Arrival Estimation: A Self-Supervised Approach", "categories": ["cs.SD", "cs.AI", "eess.AS"], "comment": null, "summary": "Acoustic mapping techniques have long been used in spatial audio processing\nfor direction of arrival estimation (DoAE). Traditional beamforming methods for\nacoustic mapping, while interpretable, often rely on iterative solvers that can\nbe computationally intensive and sensitive to acoustic variability. On the\nother hand, recent supervised deep learning approaches offer feedforward speed\nand robustness but require large labeled datasets and lack interpretability.\nDespite their strengths, both methods struggle to consistently generalize\nacross diverse acoustic setups and array configurations, limiting their broader\napplicability. We introduce the Latent Acoustic Mapping (LAM) model, a\nself-supervised framework that bridges the interpretability of traditional\nmethods with the adaptability and efficiency of deep learning methods. LAM\ngenerates high-resolution acoustic maps, adapts to varying acoustic conditions,\nand operates efficiently across different microphone arrays. We assess its\nrobustness on DoAE using the LOCATA and STARSS benchmarks. LAM achieves\ncomparable or superior localization performance to existing supervised methods.\nAdditionally, we show that LAM's acoustic maps can serve as effective features\nfor supervised models, further enhancing DoAE accuracy and underscoring its\npotential to advance adaptive, high-performance sound localization systems.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u81ea\u76d1\u7763\u7684\u6f5c\u5728\u58f0\u5b66\u6620\u5c04\uff08LAM\uff09\u6a21\u578b\uff0c\u7ed3\u5408\u4e86\u4f20\u7edf\u65b9\u6cd5\u7684\u53ef\u89e3\u91ca\u6027\u548c\u6df1\u5ea6\u5b66\u4e60\u7684\u9002\u5e94\u6027\uff0c\u5728\u58f0\u6e90\u5b9a\u4f4d\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u4f20\u7edf\u6ce2\u675f\u5f62\u6210\u65b9\u6cd5\u8ba1\u7b97\u91cf\u5927\u4e14\u5bf9\u58f0\u5b66\u53d8\u5316\u654f\u611f\uff0c\u800c\u76d1\u7763\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u9700\u8981\u5927\u91cf\u6807\u6ce8\u6570\u636e\u4e14\u7f3a\u4e4f\u53ef\u89e3\u91ca\u6027\u3002\u4e24\u8005\u5728\u591a\u6837\u58f0\u5b66\u73af\u5883\u548c\u9635\u5217\u914d\u7f6e\u4e0b\u6cdb\u5316\u80fd\u529b\u6709\u9650\u3002", "method": "\u5f15\u5165\u81ea\u76d1\u7763\u7684LAM\u6846\u67b6\uff0c\u751f\u6210\u9ad8\u5206\u8fa8\u7387\u58f0\u5b66\u56fe\uff0c\u9002\u5e94\u4e0d\u540c\u58f0\u5b66\u6761\u4ef6\u548c\u9ea6\u514b\u98ce\u9635\u5217\u3002", "result": "\u5728LOCATA\u548cSTARSS\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cLAM\u7684\u5b9a\u4f4d\u6027\u80fd\u4e0e\u73b0\u6709\u76d1\u7763\u65b9\u6cd5\u76f8\u5f53\u6216\u66f4\u4f18\uff0c\u5176\u58f0\u5b66\u56fe\u8fd8\u53ef\u63d0\u5347\u76d1\u7763\u6a21\u578b\u7684\u5b9a\u4f4d\u7cbe\u5ea6\u3002", "conclusion": "LAM\u7ed3\u5408\u4e86\u4f20\u7edf\u4e0e\u6df1\u5ea6\u5b66\u4e60\u7684\u4f18\u52bf\uff0c\u6709\u671b\u63a8\u52a8\u81ea\u9002\u5e94\u9ad8\u6027\u80fd\u58f0\u6e90\u5b9a\u4f4d\u7cfb\u7edf\u7684\u53d1\u5c55\u3002"}}
{"id": "2507.06690", "pdf": "https://arxiv.org/pdf/2507.06690", "abs": "https://arxiv.org/abs/2507.06690", "authors": ["Guobin Zhu", "Rui Zhou", "Wenkang Ji", "Hongyin Zhang", "Donglin Wang", "Shiyu Zhao"], "title": "Multi-Task Multi-Agent Reinforcement Learning via Skill Graphs", "categories": ["cs.RO"], "comment": "Conditionally accepted by IEEE Robotics and Automation Letters", "summary": "Multi-task multi-agent reinforcement learning (MT-MARL) has recently gained\nattention for its potential to enhance MARL's adaptability across multiple\ntasks. However, it is challenging for existing multi-task learning methods to\nhandle complex problems, as they are unable to handle unrelated tasks and\npossess limited knowledge transfer capabilities. In this paper, we propose a\nhierarchical approach that efficiently addresses these challenges. The\nhigh-level module utilizes a skill graph, while the low-level module employs a\nstandard MARL algorithm. Our approach offers two contributions. First, we\nconsider the MT-MARL problem in the context of unrelated tasks, expanding the\nscope of MTRL. Second, the skill graph is used as the upper layer of the\nstandard hierarchical approach, with training independent of the lower layer,\neffectively handling unrelated tasks and enhancing knowledge transfer\ncapabilities. Extensive experiments are conducted to validate these advantages\nand demonstrate that the proposed method outperforms the latest hierarchical\nMAPPO algorithms. Videos and code are available at\nhttps://github.com/WindyLab/MT-MARL-SG", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5206\u5c42\u65b9\u6cd5\uff0c\u901a\u8fc7\u6280\u80fd\u56fe\u548c\u9ad8\u5c42\u6a21\u5757\u89e3\u51b3\u591a\u4efb\u52a1\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u4e2d\u7684\u65e0\u5173\u4efb\u52a1\u548c\u77e5\u8bc6\u8f6c\u79fb\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u591a\u4efb\u52a1\u5b66\u4e60\u65b9\u6cd5\u96be\u4ee5\u5904\u7406\u590d\u6742\u95ee\u9898\uff0c\u5c24\u5176\u662f\u65e0\u5173\u4efb\u52a1\u548c\u77e5\u8bc6\u8f6c\u79fb\u80fd\u529b\u6709\u9650\u3002", "method": "\u91c7\u7528\u5206\u5c42\u65b9\u6cd5\uff0c\u9ad8\u5c42\u6a21\u5757\u4f7f\u7528\u6280\u80fd\u56fe\uff0c\u4f4e\u5c42\u6a21\u5757\u91c7\u7528\u6807\u51c6MARL\u7b97\u6cd5\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u4f18\u4e8e\u6700\u65b0\u7684\u5206\u5c42MAPPO\u7b97\u6cd5\uff0c\u80fd\u6709\u6548\u5904\u7406\u65e0\u5173\u4efb\u52a1\u5e76\u63d0\u5347\u77e5\u8bc6\u8f6c\u79fb\u80fd\u529b\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6269\u5c55\u4e86\u591a\u4efb\u52a1\u5f3a\u5316\u5b66\u4e60\u7684\u8303\u56f4\uff0c\u5e76\u901a\u8fc7\u6280\u80fd\u56fe\u63d0\u5347\u4e86\u6027\u80fd\u3002"}}
{"id": "2507.06448", "pdf": "https://arxiv.org/pdf/2507.06448", "abs": "https://arxiv.org/abs/2507.06448", "authors": ["Zhenhailong Wang", "Xuehang Guo", "Sofia Stoica", "Haiyang Xu", "Hongru Wang", "Hyeonjeong Ha", "Xiusi Chen", "Yangyi Chen", "Ming Yan", "Fei Huang", "Heng Ji"], "title": "Perception-Aware Policy Optimization for Multimodal Reasoning", "categories": ["cs.CL"], "comment": null, "summary": "Reinforcement Learning with Verifiable Rewards (RLVR) has proven to be a\nhighly effective strategy for endowing Large Language Models (LLMs) with robust\nmulti-step reasoning abilities. However, its design and optimizations remain\ntailored to purely textual domains, resulting in suboptimal performance when\napplied to multimodal reasoning tasks. In particular, we observe that a major\nsource of error in current multimodal reasoning lies in the perception of\nvisual inputs. To address this bottleneck, we propose Perception-Aware Policy\nOptimization (PAPO), a simple yet effective extension of GRPO that encourages\nthe model to learn to perceive while learning to reason, entirely from internal\nsupervision signals. Notably, PAPO does not rely on additional data curation,\nexternal reward models, or proprietary models. Specifically, we introduce the\nImplicit Perception Loss in the form of a KL divergence term to the GRPO\nobjective, which, despite its simplicity, yields significant overall\nimprovements (4.4%) on diverse multimodal benchmarks. The improvements are more\npronounced, approaching 8.0%, on tasks with high vision dependency. We also\nobserve a substantial reduction (30.5%) in perception errors, indicating\nimproved perceptual capabilities with PAPO. We conduct comprehensive analysis\nof PAPO and identify a unique loss hacking issue, which we rigorously analyze\nand mitigate through a Double Entropy Loss. Overall, our work introduces a\ndeeper integration of perception-aware supervision into RLVR learning\nobjectives and lays the groundwork for a new RL framework that encourages\nvisually grounded reasoning. Project page: https://mikewangwzhl.github.io/PAPO.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86PAPO\u65b9\u6cd5\uff0c\u901a\u8fc7\u5f15\u5165\u9690\u5f0f\u611f\u77e5\u635f\u5931\u6539\u8fdb\u591a\u6a21\u6001\u63a8\u7406\u4efb\u52a1\u4e2d\u7684\u89c6\u89c9\u8f93\u5165\u611f\u77e5\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\u3002", "motivation": "\u5f53\u524dRLVR\u65b9\u6cd5\u5728\u591a\u6a21\u6001\u63a8\u7406\u4efb\u52a1\u4e2d\u8868\u73b0\u4e0d\u4f73\uff0c\u4e3b\u8981\u95ee\u9898\u5728\u4e8e\u89c6\u89c9\u8f93\u5165\u7684\u611f\u77e5\u80fd\u529b\u4e0d\u8db3\u3002", "method": "\u63d0\u51faPAPO\u65b9\u6cd5\uff0c\u6269\u5c55GRPO\u76ee\u6807\uff0c\u5f15\u5165KL\u6563\u5ea6\u5f62\u5f0f\u7684\u9690\u5f0f\u611f\u77e5\u635f\u5931\uff0c\u65e0\u9700\u989d\u5916\u6570\u636e\u6216\u5916\u90e8\u5956\u52b1\u6a21\u578b\u3002", "result": "\u5728\u591a\u6a21\u6001\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u6574\u4f53\u63d0\u53474.4%\uff0c\u89c6\u89c9\u4f9d\u8d56\u4efb\u52a1\u4e2d\u63d0\u53478.0%\uff0c\u611f\u77e5\u9519\u8bef\u51cf\u5c1130.5%\u3002", "conclusion": "PAPO\u901a\u8fc7\u611f\u77e5\u611f\u77e5\u76d1\u7763\u7684\u6df1\u5ea6\u96c6\u6210\uff0c\u4e3a\u89c6\u89c9\u57fa\u7840\u63a8\u7406\u7684\u65b0RL\u6846\u67b6\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2507.06445", "pdf": "https://arxiv.org/pdf/2507.06445", "abs": "https://arxiv.org/abs/2507.06445", "authors": ["Victoria R. Li", "Jenny Kaufmann", "Martin Wattenberg", "David Alvarez-Melis", "Naomi Saphra"], "title": "Can Interpretation Predict Behavior on Unseen Data?", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "Interpretability research often aims to predict how a model will respond to\ntargeted interventions on specific mechanisms. However, it rarely predicts how\na model will respond to unseen input data. This paper explores the promises and\nchallenges of interpretability as a tool for predicting out-of-distribution\n(OOD) model behavior. Specifically, we investigate the correspondence between\nattention patterns and OOD generalization in hundreds of Transformer models\nindependently trained on a synthetic classification task. These models exhibit\nseveral distinct systematic generalization rules OOD, forming a diverse\npopulation for correlational analysis. In this setting, we find that simple\nobservational tools from interpretability can predict OOD performance. In\nparticular, when in-distribution attention exhibits hierarchical patterns, the\nmodel is likely to generalize hierarchically on OOD data -- even when the\nrule's implementation does not rely on these hierarchical patterns, according\nto ablation tests. Our findings offer a proof-of-concept to motivate further\ninterpretability work on predicting unseen model behavior.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u53ef\u89e3\u91ca\u6027\u5de5\u5177\u5728\u9884\u6d4b\u6a21\u578b\u5728\u5206\u5e03\u5916\uff08OOD\uff09\u6570\u636e\u4e0a\u884c\u4e3a\u7684\u6f5c\u529b\uff0c\u53d1\u73b0\u6ce8\u610f\u529b\u6a21\u5f0f\u4e0eOOD\u6cdb\u5316\u6027\u80fd\u4e4b\u95f4\u5b58\u5728\u76f8\u5173\u6027\u3002", "motivation": "\u7814\u7a76\u52a8\u673a\u5728\u4e8e\u586b\u8865\u53ef\u89e3\u91ca\u6027\u7814\u7a76\u5728\u9884\u6d4b\u6a21\u578b\u5bf9\u672a\u89c1\u8f93\u5165\u6570\u636e\u54cd\u5e94\u65b9\u9762\u7684\u7a7a\u767d\uff0c\u5c24\u5176\u662fOOD\u884c\u4e3a\u3002", "method": "\u901a\u8fc7\u5206\u6790\u6570\u767e\u4e2a\u72ec\u7acb\u8bad\u7ec3\u7684Transformer\u6a21\u578b\u5728\u5408\u6210\u5206\u7c7b\u4efb\u52a1\u4e2d\u7684\u6ce8\u610f\u529b\u6a21\u5f0f\u4e0eOOD\u6cdb\u5316\u6027\u80fd\u7684\u5173\u8054\u6027\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff0c\u7b80\u5355\u7684\u53ef\u89e3\u91ca\u6027\u5de5\u5177\u53ef\u4ee5\u9884\u6d4bOOD\u6027\u80fd\uff0c\u5c24\u5176\u662f\u5f53\u6ce8\u610f\u529b\u5448\u73b0\u5206\u5c42\u6a21\u5f0f\u65f6\uff0c\u6a21\u578b\u503e\u5411\u4e8e\u5728OOD\u6570\u636e\u4e0a\u5206\u5c42\u6cdb\u5316\u3002", "conclusion": "\u7814\u7a76\u4e3a\u53ef\u89e3\u91ca\u6027\u5de5\u5177\u5728\u9884\u6d4b\u672a\u89c1\u6a21\u578b\u884c\u4e3a\u65b9\u9762\u7684\u5e94\u7528\u63d0\u4f9b\u4e86\u6982\u5ff5\u9a8c\u8bc1\uff0c\u9f13\u52b1\u8fdb\u4e00\u6b65\u7814\u7a76\u3002"}}
{"id": "2507.06442", "pdf": "https://arxiv.org/pdf/2507.06442", "abs": "https://arxiv.org/abs/2507.06442", "authors": ["Soroush Shahi", "Farzad Shahabi", "Rama Nabulsi", "Glenn Fernandes", "Aggelos Katsaggelos", "Nabil Alshurafa"], "title": "THOR: Thermal-guided Hand-Object Reasoning via Adaptive Vision Sampling", "categories": ["cs.CV"], "comment": null, "summary": "Wearable cameras are increasingly used as an observational and interventional\ntool for human behaviors by providing detailed visual data of hand-related\nactivities. This data can be leveraged to facilitate memory recall for logging\nof behavior or timely interventions aimed at improving health. However,\ncontinuous processing of RGB images from these cameras consumes significant\npower impacting battery lifetime, generates a large volume of unnecessary video\ndata for post-processing, raises privacy concerns, and requires substantial\ncomputational resources for real-time analysis. We introduce THOR, a real-time\nadaptive spatio-temporal RGB frame sampling method that leverages thermal\nsensing to capture hand-object patches and classify them in real-time. We use\nlow-resolution thermal camera data to identify moments when a person switches\nfrom one hand-related activity to another, and adjust the RGB frame sampling\nrate by increasing it during activity transitions and reducing it during\nperiods of sustained activity. Additionally, we use the thermal cues from the\nhand to localize the region of interest (i.e., the hand-object interaction) in\neach RGB frame, allowing the system to crop and process only the necessary part\nof the image for activity recognition. We develop a wearable device to validate\nour method through an in-the-wild study with 14 participants and over 30\nactivities, and further evaluate it on Ego4D (923 participants across 9\ncountries, totaling 3,670 hours of video). Our results show that using only 3%\nof the original RGB video data, our method captures all the activity segments,\nand achieves hand-related activity recognition F1-score (95%) comparable to\nusing the entire RGB video (94%). Our work provides a more practical path for\nthe longitudinal use of wearable cameras to monitor hand-related activities and\nhealth-risk behaviors in real time.", "AI": {"tldr": "THOR\u662f\u4e00\u79cd\u5b9e\u65f6\u81ea\u9002\u5e94\u65f6\u7a7aRGB\u5e27\u91c7\u6837\u65b9\u6cd5\uff0c\u5229\u7528\u70ed\u611f\u6280\u672f\u6355\u6349\u624b\u90e8\u6d3b\u52a8\uff0c\u663e\u8457\u51cf\u5c11\u6570\u636e\u5904\u7406\u91cf\u548c\u80fd\u8017\u3002", "motivation": "\u89e3\u51b3\u7a7f\u6234\u76f8\u673a\u8fde\u7eed\u5904\u7406RGB\u56fe\u50cf\u7684\u9ad8\u80fd\u8017\u3001\u5927\u6570\u636e\u91cf\u3001\u9690\u79c1\u95ee\u9898\u548c\u8ba1\u7b97\u8d44\u6e90\u9700\u6c42\u3002", "method": "\u7ed3\u5408\u4f4e\u5206\u8fa8\u7387\u70ed\u611f\u6570\u636e\u52a8\u6001\u8c03\u6574RGB\u5e27\u91c7\u6837\u7387\uff0c\u5e76\u5229\u7528\u70ed\u611f\u7ebf\u7d22\u5b9a\u4f4d\u624b\u90e8\u6d3b\u52a8\u533a\u57df\u4ee5\u51cf\u5c11\u56fe\u50cf\u5904\u7406\u8303\u56f4\u3002", "result": "\u4ec5\u4f7f\u75283%\u7684\u539f\u59cbRGB\u6570\u636e\uff0c\u5b9e\u73b0\u4e8695%\u7684\u624b\u90e8\u6d3b\u52a8\u8bc6\u522b\u51c6\u786e\u7387\uff0c\u4e0e\u5168\u6570\u636e\uff0894%\uff09\u76f8\u5f53\u3002", "conclusion": "THOR\u4e3a\u7a7f\u6234\u76f8\u673a\u5b9e\u65f6\u76d1\u6d4b\u624b\u90e8\u6d3b\u52a8\u548c\u5065\u5eb7\u98ce\u9669\u884c\u4e3a\u63d0\u4f9b\u4e86\u66f4\u5b9e\u7528\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.06256", "pdf": "https://arxiv.org/pdf/2507.06256", "abs": "https://arxiv.org/abs/2507.06256", "authors": ["Vinu Sankar Sadasivan", "Soheil Feizi", "Rajiv Mathews", "Lun Wang"], "title": "Attacker's Noise Can Manipulate Your Audio-based LLM in the Real World", "categories": ["cs.CR", "cs.AI", "cs.SD", "eess.AS"], "comment": null, "summary": "This paper investigates the real-world vulnerabilities of audio-based large\nlanguage models (ALLMs), such as Qwen2-Audio. We first demonstrate that an\nadversary can craft stealthy audio perturbations to manipulate ALLMs into\nexhibiting specific targeted behaviors, such as eliciting responses to\nwake-keywords (e.g., \"Hey Qwen\"), or triggering harmful behaviors (e.g. \"Change\nmy calendar event\"). Subsequently, we show that playing adversarial background\nnoise during user interaction with the ALLMs can significantly degrade the\nresponse quality. Crucially, our research illustrates the scalability of these\nattacks to real-world scenarios, impacting other innocent users when these\nadversarial noises are played through the air. Further, we discuss the\ntransferrability of the attack, and potential defensive measures.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u97f3\u9891\u5927\u8bed\u8a00\u6a21\u578b\uff08ALLMs\uff09\u7684\u73b0\u5b9e\u6f0f\u6d1e\uff0c\u5c55\u793a\u4e86\u5982\u4f55\u901a\u8fc7\u9690\u853d\u97f3\u9891\u6270\u52a8\u64cd\u63a7\u6a21\u578b\u884c\u4e3a\uff0c\u5e76\u63a2\u8ba8\u4e86\u653b\u51fb\u7684\u53ef\u6269\u5c55\u6027\u548c\u9632\u5fa1\u63aa\u65bd\u3002", "motivation": "\u63a2\u7a76\u97f3\u9891\u5927\u8bed\u8a00\u6a21\u578b\u5728\u73b0\u5b9e\u573a\u666f\u4e2d\u7684\u5b89\u5168\u6f0f\u6d1e\uff0c\u63ed\u793a\u5176\u6613\u53d7\u653b\u51fb\u7684\u6f5c\u5728\u98ce\u9669\u3002", "method": "\u901a\u8fc7\u5236\u4f5c\u9690\u853d\u97f3\u9891\u6270\u52a8\u548c\u80cc\u666f\u566a\u58f0\u653b\u51fb\uff0c\u6d4b\u8bd5\u6a21\u578b\u5bf9\u7279\u5b9a\u884c\u4e3a\u7684\u54cd\u5e94\u53ca\u6027\u80fd\u4e0b\u964d\u60c5\u51b5\u3002", "result": "\u653b\u51fb\u80fd\u6210\u529f\u64cd\u63a7\u6a21\u578b\u884c\u4e3a\uff08\u5982\u5524\u9192\u5173\u952e\u8bcd\u6216\u89e6\u53d1\u6709\u5bb3\u884c\u4e3a\uff09\uff0c\u5e76\u663e\u8457\u964d\u4f4e\u6a21\u578b\u54cd\u5e94\u8d28\u91cf\u3002\u653b\u51fb\u5728\u73b0\u5b9e\u573a\u666f\u4e2d\u5177\u6709\u53ef\u6269\u5c55\u6027\u3002", "conclusion": "\u97f3\u9891\u5927\u8bed\u8a00\u6a21\u578b\u5b58\u5728\u4e25\u91cd\u5b89\u5168\u6f0f\u6d1e\uff0c\u9700\u8fdb\u4e00\u6b65\u7814\u7a76\u9632\u5fa1\u63aa\u65bd\u4ee5\u5e94\u5bf9\u73b0\u5b9e\u653b\u51fb\u3002"}}
{"id": "2507.06700", "pdf": "https://arxiv.org/pdf/2507.06700", "abs": "https://arxiv.org/abs/2507.06700", "authors": ["Pranav Pandey", "Ramviyas Parasuraman", "Prashant Doshi"], "title": "Integrating Perceptions: A Human-Centered Physical Safety Model for Human-Robot Interaction", "categories": ["cs.RO", "cs.HC"], "comment": "Accepted to IEEE RO-MAN 2025 Conference", "summary": "Ensuring safety in human-robot interaction (HRI) is essential to foster user\ntrust and enable the broader adoption of robotic systems. Traditional safety\nmodels primarily rely on sensor-based measures, such as relative distance and\nvelocity, to assess physical safety. However, these models often fail to\ncapture subjective safety perceptions, which are shaped by individual traits\nand contextual factors. In this paper, we introduce and analyze a parameterized\ngeneral safety model that bridges the gap between physical and perceived safety\nby incorporating a personalization parameter, $\\rho$, into the safety\nmeasurement framework to account for individual differences in safety\nperception. Through a series of hypothesis-driven human-subject studies in a\nsimulated rescue scenario, we investigate how emotional state, trust, and robot\nbehavior influence perceived safety. Our results show that $\\rho$ effectively\ncaptures meaningful individual differences, driven by affective responses,\ntrust in task consistency, and clustering into distinct user types.\nSpecifically, our findings confirm that predictable and consistent robot\nbehavior as well as the elicitation of positive emotional states, significantly\nenhance perceived safety. Moreover, responses cluster into a small number of\nuser types, supporting adaptive personalization based on shared safety models.\nNotably, participant role significantly shapes safety perception, and repeated\nexposure reduces perceived safety for participants in the casualty role,\nemphasizing the impact of physical interaction and experiential change. These\nfindings highlight the importance of adaptive, human-centered safety models\nthat integrate both psychological and behavioral dimensions, offering a pathway\ntoward more trustworthy and effective HRI in safety-critical domains.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u53c2\u6570\u5316\u901a\u7528\u5b89\u5168\u6a21\u578b\uff0c\u901a\u8fc7\u4e2a\u6027\u5316\u53c2\u6570\u03c1\u5f25\u8865\u7269\u7406\u5b89\u5168\u4e0e\u4e3b\u89c2\u5b89\u5168\u611f\u77e5\u4e4b\u95f4\u7684\u5dee\u8ddd\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u60c5\u611f\u72b6\u6001\u3001\u4fe1\u4efb\u548c\u673a\u5668\u4eba\u884c\u4e3a\u5bf9\u5b89\u5168\u611f\u77e5\u7684\u5f71\u54cd\u3002", "motivation": "\u4f20\u7edf\u5b89\u5168\u6a21\u578b\u4e3b\u8981\u4f9d\u8d56\u4f20\u611f\u5668\u6570\u636e\uff0c\u65e0\u6cd5\u6355\u6349\u4e3b\u89c2\u5b89\u5168\u611f\u77e5\uff0c\u800c\u4e3b\u89c2\u611f\u77e5\u53d7\u4e2a\u4f53\u7279\u8d28\u548c\u60c5\u5883\u5f71\u54cd\u3002\u7814\u7a76\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u901a\u8fc7\u6a21\u62df\u6551\u63f4\u573a\u666f\u7684\u4eba\u4f53\u5b9e\u9a8c\uff0c\u7814\u7a76\u60c5\u611f\u72b6\u6001\u3001\u4fe1\u4efb\u548c\u673a\u5668\u4eba\u884c\u4e3a\u5982\u4f55\u5f71\u54cd\u5b89\u5168\u611f\u77e5\uff0c\u5e76\u5f15\u5165\u53c2\u6570\u03c1\u91cf\u5316\u4e2a\u4f53\u5dee\u5f02\u3002", "result": "\u53c2\u6570\u03c1\u6709\u6548\u6355\u6349\u4e2a\u4f53\u5dee\u5f02\uff0c\u53ef\u9884\u6d4b\u548c\u4e00\u81f4\u7684\u673a\u5668\u4eba\u884c\u4e3a\u53ca\u79ef\u6781\u60c5\u611f\u72b6\u6001\u663e\u8457\u63d0\u5347\u5b89\u5168\u611f\u77e5\u3002\u53c2\u4e0e\u8005\u89d2\u8272\u548c\u91cd\u590d\u63a5\u89e6\u4e5f\u5f71\u54cd\u611f\u77e5\u3002", "conclusion": "\u7814\u7a76\u5f3a\u8c03\u9700\u6574\u5408\u5fc3\u7406\u548c\u884c\u4e3a\u7ef4\u5ea6\u7684\u81ea\u9002\u5e94\u5b89\u5168\u6a21\u578b\uff0c\u4ee5\u63d0\u5347\u4eba\u673a\u4ea4\u4e92\u7684\u53ef\u4fe1\u5ea6\u548c\u6709\u6548\u6027\u3002"}}
{"id": "2507.06450", "pdf": "https://arxiv.org/pdf/2507.06450", "abs": "https://arxiv.org/abs/2507.06450", "authors": ["Xin Su", "Sungduk Yu", "Phillip Howard", "Steven Bethard"], "title": "A Semantic Parsing Framework for End-to-End Time Normalization", "categories": ["cs.CL"], "comment": null, "summary": "Time normalization is the task of converting natural language temporal\nexpressions into machine-readable representations. It underpins many downstream\napplications in information retrieval, question answering, and clinical\ndecision-making. Traditional systems based on the ISO-TimeML schema limit\nexpressivity and struggle with complex constructs such as compositional,\nevent-relative, and multi-span time expressions. In this work, we introduce a\nnovel formulation of time normalization as a code generation task grounded in\nthe SCATE framework, which defines temporal semantics through symbolic and\ncompositional operators. We implement a fully executable SCATE Python library\nand demonstrate that large language models (LLMs) can generate executable SCATE\ncode. Leveraging this capability, we develop an automatic data augmentation\npipeline using LLMs to synthesize large-scale annotated data with code-level\nvalidation. Our experiments show that small, locally deployable models trained\non this augmented data can achieve strong performance, outperforming even their\nLLM parents and enabling practical, accurate, and interpretable time\nnormalization.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eSCATE\u6846\u67b6\u7684\u65f6\u95f4\u5f52\u4e00\u5316\u65b9\u6cd5\uff0c\u5c06\u81ea\u7136\u8bed\u8a00\u65f6\u95f4\u8868\u8fbe\u5f0f\u8f6c\u6362\u4e3a\u673a\u5668\u53ef\u8bfb\u8868\u793a\uff0c\u5e76\u901a\u8fc7LLM\u751f\u6210\u53ef\u6267\u884c\u4ee3\u7801\u548c\u6570\u636e\u589e\u5f3a\uff0c\u63d0\u5347\u4e86\u5c0f\u6a21\u578b\u7684\u6027\u80fd\u3002", "motivation": "\u4f20\u7edf\u57fa\u4e8eISO-TimeML\u7684\u7cfb\u7edf\u8868\u8fbe\u80fd\u529b\u6709\u9650\uff0c\u96be\u4ee5\u5904\u7406\u590d\u6742\u65f6\u95f4\u8868\u8fbe\u5f0f\uff0c\u5982\u7ec4\u5408\u5f0f\u3001\u4e8b\u4ef6\u76f8\u5bf9\u548c\u591a\u8de8\u5ea6\u8868\u8fbe\u5f0f\u3002", "method": "\u5c06\u65f6\u95f4\u5f52\u4e00\u5316\u4efb\u52a1\u91cd\u65b0\u5b9a\u4e49\u4e3a\u57fa\u4e8eSCATE\u6846\u67b6\u7684\u4ee3\u7801\u751f\u6210\u4efb\u52a1\uff0c\u5f00\u53d1\u4e86\u53ef\u6267\u884c\u7684SCATE Python\u5e93\uff0c\u5e76\u5229\u7528LLM\u751f\u6210\u53ef\u6267\u884c\u4ee3\u7801\u548c\u6570\u636e\u589e\u5f3a\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u5c0f\u6a21\u578b\u5728\u589e\u5f3a\u6570\u636e\u4e0a\u8bad\u7ec3\u540e\u6027\u80fd\u4f18\u5f02\uff0c\u751a\u81f3\u8d85\u8fc7LLM\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u3001\u51c6\u786e\u4e14\u53ef\u89e3\u91ca\u7684\u65f6\u95f4\u5f52\u4e00\u5316\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u901a\u8fc7SCATE\u6846\u67b6\u548cLLM\u6570\u636e\u589e\u5f3a\uff0c\u663e\u8457\u63d0\u5347\u4e86\u65f6\u95f4\u5f52\u4e00\u5316\u7684\u6027\u80fd\u548c\u5b9e\u7528\u6027\u3002"}}
{"id": "2507.06449", "pdf": "https://arxiv.org/pdf/2507.06449", "abs": "https://arxiv.org/abs/2507.06449", "authors": ["Qianyu Long", "Qiyuan Wang", "Christos Anagnostopoulos", "Daning Bi"], "title": "FedPhD: Federated Pruning with Hierarchical Learning of Diffusion Models", "categories": ["cs.LG", "cs.AI", "cs.DC", "68T05, 68T07, 68Q85, 94A08", "I.2.6; I.2.11; C.2.4"], "comment": "12 pages, 8 figures, 5 tables. This paper introduces FedPhD, a novel\n  hierarchical federated learning framework for training diffusion models that\n  addresses data heterogeneity and communication costs through\n  homogeneity-aware aggregation and structured pruning. Submitted to IEEE\n  Transactions on Cybernetics and is under review", "summary": "Federated Learning (FL), as a distributed learning paradigm, trains models\nover distributed clients' data. FL is particularly beneficial for distributed\ntraining of Diffusion Models (DMs), which are high-quality image generators\nthat require diverse data. However, challenges such as high communication costs\nand data heterogeneity persist in training DMs similar to training Transformers\nand Convolutional Neural Networks. Limited research has addressed these issues\nin FL environments. To address this gap and challenges, we introduce a novel\napproach, FedPhD, designed to efficiently train DMs in FL environments. FedPhD\nleverages Hierarchical FL with homogeneity-aware model aggregation and\nselection policy to tackle data heterogeneity while reducing communication\ncosts. The distributed structured pruning of FedPhD enhances computational\nefficiency and reduces model storage requirements in clients. Our experiments\nacross multiple datasets demonstrate that FedPhD achieves high model\nperformance regarding Fr\\'echet Inception Distance (FID) scores while reducing\ncommunication costs by up to $88\\%$. FedPhD outperforms baseline methods\nachieving at least a $34\\%$ improvement in FID, while utilizing only $56\\%$ of\nthe total computation and communication resources.", "AI": {"tldr": "FedPhD\u662f\u4e00\u79cd\u65b0\u9896\u7684\u8054\u90a6\u5b66\u4e60\u65b9\u6cd5\uff0c\u4e13\u6ce8\u4e8e\u9ad8\u6548\u8bad\u7ec3\u6269\u6563\u6a21\u578b\uff08DMs\uff09\uff0c\u901a\u8fc7\u5206\u5c42\u8054\u90a6\u5b66\u4e60\u548c\u540c\u8d28\u6027\u611f\u77e5\u6a21\u578b\u805a\u5408\u89e3\u51b3\u6570\u636e\u5f02\u6784\u6027\u548c\u9ad8\u901a\u4fe1\u6210\u672c\u95ee\u9898\u3002", "motivation": "\u6269\u6563\u6a21\u578b\u5728\u8054\u90a6\u5b66\u4e60\u73af\u5883\u4e2d\u9762\u4e34\u6570\u636e\u5f02\u6784\u6027\u548c\u9ad8\u901a\u4fe1\u6210\u672c\u7684\u6311\u6218\uff0c\u73b0\u6709\u7814\u7a76\u8f83\u5c11\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "FedPhD\u91c7\u7528\u5206\u5c42\u8054\u90a6\u5b66\u4e60\uff0c\u7ed3\u5408\u540c\u8d28\u6027\u611f\u77e5\u6a21\u578b\u805a\u5408\u548c\u9009\u62e9\u7b56\u7565\uff0c\u5e76\u901a\u8fc7\u5206\u5e03\u5f0f\u7ed3\u6784\u5316\u526a\u679d\u63d0\u5347\u8ba1\u7b97\u6548\u7387\u548c\u51cf\u5c11\u5b58\u50a8\u9700\u6c42\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\uff0cFedPhD\u5728FID\u5206\u6570\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u901a\u4fe1\u6210\u672c\u964d\u4f4e88%\uff0c\u8ba1\u7b97\u548c\u901a\u4fe1\u8d44\u6e90\u4ec5\u970056%\uff0c\u4e14FID\u63d0\u5347\u81f3\u5c1134%\u3002", "conclusion": "FedPhD\u4e3a\u8054\u90a6\u5b66\u4e60\u4e2d\u6269\u6563\u6a21\u578b\u7684\u9ad8\u6548\u8bad\u7ec3\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\u548c\u8d44\u6e90\u5229\u7528\u7387\u3002"}}
{"id": "2507.06459", "pdf": "https://arxiv.org/pdf/2507.06459", "abs": "https://arxiv.org/abs/2507.06459", "authors": ["Riadul Islam", "Joey Mul\u00e9", "Dhandeep Challagundla", "Shahmir Rizvi", "Sean Carson"], "title": "EA: An Event Autoencoder for High-Speed Vision Sensing", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "High-speed vision sensing is essential for real-time perception in\napplications such as robotics, autonomous vehicles, and industrial automation.\nTraditional frame-based vision systems suffer from motion blur, high latency,\nand redundant data processing, limiting their performance in dynamic\nenvironments. Event cameras, which capture asynchronous brightness changes at\nthe pixel level, offer a promising alternative but pose challenges in object\ndetection due to sparse and noisy event streams. To address this, we propose an\nevent autoencoder architecture that efficiently compresses and reconstructs\nevent data while preserving critical spatial and temporal features. The\nproposed model employs convolutional encoding and incorporates adaptive\nthreshold selection and a lightweight classifier to enhance recognition\naccuracy while reducing computational complexity. Experimental results on the\nexisting Smart Event Face Dataset (SEFD) demonstrate that our approach achieves\ncomparable accuracy to the YOLO-v4 model while utilizing up to $35.5\\times$\nfewer parameters. Implementations on embedded platforms, including Raspberry Pi\n4B and NVIDIA Jetson Nano, show high frame rates ranging from 8 FPS up to 44.8\nFPS. The proposed classifier exhibits up to 87.84x better FPS than the\nstate-of-the-art and significantly improves event-based vision performance,\nmaking it ideal for low-power, high-speed applications in real-time edge\ncomputing.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u4e8b\u4ef6\u81ea\u52a8\u7f16\u7801\u5668\u67b6\u6784\uff0c\u7528\u4e8e\u9ad8\u6548\u538b\u7f29\u548c\u91cd\u5efa\u4e8b\u4ef6\u6570\u636e\uff0c\u63d0\u5347\u4e8b\u4ef6\u76f8\u673a\u5728\u52a8\u6001\u73af\u5883\u4e2d\u7684\u7269\u4f53\u68c0\u6d4b\u6027\u80fd\u3002", "motivation": "\u4f20\u7edf\u5e27\u5f0f\u89c6\u89c9\u7cfb\u7edf\u5728\u52a8\u6001\u73af\u5883\u4e2d\u5b58\u5728\u8fd0\u52a8\u6a21\u7cca\u3001\u9ad8\u5ef6\u8fdf\u548c\u6570\u636e\u5197\u4f59\u95ee\u9898\uff0c\u4e8b\u4ef6\u76f8\u673a\u867d\u80fd\u5f02\u6b65\u6355\u6349\u4eae\u5ea6\u53d8\u5316\uff0c\u4f46\u7a00\u758f\u548c\u566a\u58f0\u4e8b\u4ef6\u6d41\u5bf9\u7269\u4f53\u68c0\u6d4b\u6784\u6210\u6311\u6218\u3002", "method": "\u91c7\u7528\u5377\u79ef\u7f16\u7801\u7684\u81ea\u9002\u5e94\u9608\u503c\u9009\u62e9\u548c\u8f7b\u91cf\u7ea7\u5206\u7c7b\u5668\uff0c\u51cf\u5c11\u8ba1\u7b97\u590d\u6742\u5ea6\u5e76\u63d0\u9ad8\u8bc6\u522b\u7cbe\u5ea6\u3002", "result": "\u5728SEFD\u6570\u636e\u96c6\u4e0a\uff0c\u6a21\u578b\u7cbe\u5ea6\u4e0eYOLO-v4\u76f8\u5f53\uff0c\u4f46\u53c2\u6570\u51cf\u5c1135.5\u500d\uff1b\u5728\u5d4c\u5165\u5f0f\u5e73\u53f0\u4e0a\u5b9e\u73b08-44.8 FPS\u7684\u9ad8\u5e27\u7387\u3002", "conclusion": "\u8be5\u6a21\u578b\u663e\u8457\u63d0\u5347\u4e86\u4e8b\u4ef6\u76f8\u673a\u7684\u89c6\u89c9\u6027\u80fd\uff0c\u9002\u7528\u4e8e\u4f4e\u529f\u8017\u3001\u9ad8\u901f\u5b9e\u65f6\u8fb9\u7f18\u8ba1\u7b97\u5e94\u7528\u3002"}}
{"id": "2507.06258", "pdf": "https://arxiv.org/pdf/2507.06258", "abs": "https://arxiv.org/abs/2507.06258", "authors": ["Bo Yan", "Yurong Hao", "Dingqi Liu", "Huabin Sun", "Pengpeng Qiao", "Wei Yang Bryan Lim", "Yang Cao", "Chuan Shi"], "title": "Phantom Subgroup Poisoning: Stealth Attacks on Federated Recommender Systems", "categories": ["cs.CR", "cs.AI", "cs.DC", "cs.IR"], "comment": "13 pages", "summary": "Federated recommender systems (FedRec) have emerged as a promising solution\nfor delivering personalized recommendations while safeguarding user privacy.\nHowever, recent studies have demonstrated their vulnerability to poisoning\nattacks. Existing attacks typically target the entire user group, which\ncompromises stealth and increases the risk of detection. In contrast,\nreal-world adversaries may prefer to prompt target items to specific user\nsubgroups, such as recommending health supplements to elderly users. Motivated\nby this gap, we introduce Spattack, the first targeted poisoning attack\ndesigned to manipulate recommendations for specific user subgroups in the\nfederated setting. Specifically, Spattack adopts a two-stage\napproximation-and-promotion strategy, which first simulates user embeddings of\ntarget/non-target subgroups and then prompts target items to the target\nsubgroups. To enhance the approximation stage, we push the inter-group\nembeddings away based on contrastive learning and augment the target group's\nrelevant item set based on clustering. To enhance the promotion stage, we\nfurther propose to adaptively tune the optimization weights between target and\nnon-target subgroups. Besides, an embedding alignment strategy is proposed to\nalign the embeddings between the target items and the relevant items. We\nconduct comprehensive experiments on three real-world datasets, comparing\nSpattack against seven state-of-the-art poisoning attacks and seven\nrepresentative defense mechanisms. Experimental results demonstrate that\nSpattack consistently achieves strong manipulation performance on the specific\nuser subgroup, while incurring minimal impact on non-target users, even when\nonly 0.1\\% of users are malicious. Moreover, Spattack maintains competitive\noverall recommendation performance and exhibits strong resilience against\nexisting mainstream defenses.", "AI": {"tldr": "Spattack\u662f\u4e00\u79cd\u9488\u5bf9\u8054\u90a6\u63a8\u8350\u7cfb\u7edf\u7684\u5b9a\u5411\u6295\u6bd2\u653b\u51fb\u65b9\u6cd5\uff0c\u4e13\u6ce8\u4e8e\u7279\u5b9a\u7528\u6237\u5b50\u7fa4\uff0c\u901a\u8fc7\u4e24\u9636\u6bb5\u7b56\u7565\u5b9e\u73b0\u9ad8\u6548\u64cd\u7eb5\uff0c\u540c\u65f6\u4fdd\u6301\u9690\u853d\u6027\u548c\u6297\u9632\u5fa1\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u8054\u90a6\u63a8\u8350\u7cfb\u7edf\u7684\u6295\u6bd2\u653b\u51fb\u901a\u5e38\u9488\u5bf9\u6574\u4e2a\u7528\u6237\u7fa4\u4f53\uff0c\u7f3a\u4e4f\u5bf9\u7279\u5b9a\u5b50\u7fa4\u7684\u9488\u5bf9\u6027\uff0c\u4e14\u5bb9\u6613\u88ab\u68c0\u6d4b\u3002Spattack\u586b\u8865\u4e86\u8fd9\u4e00\u7a7a\u767d\uff0c\u4e13\u6ce8\u4e8e\u7279\u5b9a\u7528\u6237\u5b50\u7fa4\u7684\u653b\u51fb\u3002", "method": "Spattack\u91c7\u7528\u4e24\u9636\u6bb5\u7b56\u7565\uff1a1) \u8fd1\u4f3c\u9636\u6bb5\uff0c\u901a\u8fc7\u5bf9\u6bd4\u5b66\u4e60\u548c\u805a\u7c7b\u589e\u5f3a\u76ee\u6807\u5b50\u7fa4\u7684\u7528\u6237\u5d4c\u5165\uff1b2) \u63a8\u5e7f\u9636\u6bb5\uff0c\u81ea\u9002\u5e94\u8c03\u6574\u4f18\u5316\u6743\u91cd\uff0c\u5e76\u901a\u8fc7\u5d4c\u5165\u5bf9\u9f50\u63d0\u5347\u76ee\u6807\u9879\u76ee\u7684\u63a8\u8350\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cSpattack\u5728\u7279\u5b9a\u7528\u6237\u5b50\u7fa4\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u5bf9\u975e\u76ee\u6807\u7528\u6237\u5f71\u54cd\u6781\u5c0f\uff0c\u4e14\u80fd\u62b5\u6297\u4e3b\u6d41\u9632\u5fa1\u673a\u5236\u3002", "conclusion": "Spattack\u662f\u9996\u4e2a\u9488\u5bf9\u8054\u90a6\u63a8\u8350\u7cfb\u7edf\u4e2d\u7279\u5b9a\u7528\u6237\u5b50\u7fa4\u7684\u5b9a\u5411\u6295\u6bd2\u653b\u51fb\u65b9\u6cd5\uff0c\u5177\u6709\u9ad8\u6548\u6027\u548c\u9690\u853d\u6027\u3002"}}
{"id": "2507.06710", "pdf": "https://arxiv.org/pdf/2507.06710", "abs": "https://arxiv.org/abs/2507.06710", "authors": ["Zhenyang Liu", "Yikai Wang", "Kuanning Wang", "Longfei Liang", "Xiangyang Xue", "Yanwei Fu"], "title": "Spatial-Temporal Aware Visuomotor Diffusion Policy Learning", "categories": ["cs.RO"], "comment": null, "summary": "Visual imitation learning is effective for robots to learn versatile tasks.\nHowever, many existing methods rely on behavior cloning with supervised\nhistorical trajectories, limiting their 3D spatial and 4D spatiotemporal\nawareness. Consequently, these methods struggle to capture the 3D structures\nand 4D spatiotemporal relationships necessary for real-world deployment. In\nthis work, we propose 4D Diffusion Policy (DP4), a novel visual imitation\nlearning method that incorporates spatiotemporal awareness into diffusion-based\npolicies. Unlike traditional approaches that rely on trajectory cloning, DP4\nleverages a dynamic Gaussian world model to guide the learning of 3D spatial\nand 4D spatiotemporal perceptions from interactive environments. Our method\nconstructs the current 3D scene from a single-view RGB-D observation and\npredicts the future 3D scene, optimizing trajectory generation by explicitly\nmodeling both spatial and temporal dependencies. Extensive experiments across\n17 simulation tasks with 173 variants and 3 real-world robotic tasks\ndemonstrate that the 4D Diffusion Policy (DP4) outperforms baseline methods,\nimproving the average simulation task success rate by 16.4% (Adroit), 14%\n(DexArt), and 6.45% (RLBench), and the average real-world robotic task success\nrate by 8.6%.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a4D Diffusion Policy (DP4)\u7684\u89c6\u89c9\u6a21\u4eff\u5b66\u4e60\u65b9\u6cd5\uff0c\u901a\u8fc7\u52a8\u6001\u9ad8\u65af\u4e16\u754c\u6a21\u578b\u589e\u5f3a3D\u7a7a\u95f4\u548c4D\u65f6\u7a7a\u611f\u77e5\uff0c\u663e\u8457\u63d0\u5347\u4e86\u4efb\u52a1\u6210\u529f\u7387\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u4e8e\u5386\u53f2\u8f68\u8ff9\u7684\u884c\u4e3a\u514b\u9686\uff0c\u7f3a\u4e4f3D\u7a7a\u95f4\u548c4D\u65f6\u7a7a\u611f\u77e5\u80fd\u529b\uff0c\u9650\u5236\u4e86\u5b9e\u9645\u5e94\u7528\u6548\u679c\u3002", "method": "DP4\u5229\u7528\u52a8\u6001\u9ad8\u65af\u4e16\u754c\u6a21\u578b\u4ece\u4ea4\u4e92\u73af\u5883\u4e2d\u5b66\u4e603D\u7a7a\u95f4\u548c4D\u65f6\u7a7a\u611f\u77e5\uff0c\u901a\u8fc7\u5355\u89c6\u89d2RGB-D\u89c2\u6d4b\u6784\u5efa\u5f53\u524d3D\u573a\u666f\u5e76\u9884\u6d4b\u672a\u67653D\u573a\u666f\uff0c\u4f18\u5316\u8f68\u8ff9\u751f\u6210\u3002", "result": "\u572817\u4e2a\u4eff\u771f\u4efb\u52a1\u548c3\u4e2a\u771f\u5b9e\u673a\u5668\u4eba\u4efb\u52a1\u4e2d\uff0cDP4\u8868\u73b0\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\uff0c\u4eff\u771f\u4efb\u52a1\u6210\u529f\u7387\u5e73\u5747\u63d0\u534716.4%\uff08Adroit\uff09\u300114%\uff08DexArt\uff09\u548c6.45%\uff08RLBench\uff09\uff0c\u771f\u5b9e\u4efb\u52a1\u6210\u529f\u7387\u63d0\u53478.6%\u3002", "conclusion": "DP4\u901a\u8fc7\u589e\u5f3a\u65f6\u7a7a\u611f\u77e5\u80fd\u529b\uff0c\u663e\u8457\u63d0\u5347\u4e86\u89c6\u89c9\u6a21\u4eff\u5b66\u4e60\u7684\u6548\u679c\uff0c\u9002\u7528\u4e8e\u590d\u6742\u4efb\u52a1\u7684\u5b9e\u9645\u90e8\u7f72\u3002"}}
{"id": "2507.06457", "pdf": "https://arxiv.org/pdf/2507.06457", "abs": "https://arxiv.org/abs/2507.06457", "authors": ["Dustin Wang", "Rui-Jie Zhu", "Steven Abreu", "Yong Shan", "Taylor Kergan", "Yuqi Pan", "Yuhong Chou", "Zheng Li", "Ge Zhang", "Wenhao Huang", "Jason Eshraghian"], "title": "A Systematic Analysis of Hybrid Linear Attention", "categories": ["cs.CL"], "comment": null, "summary": "Transformers face quadratic complexity and memory issues with long sequences,\nprompting the adoption of linear attention mechanisms using fixed-size hidden\nstates. However, linear models often suffer from limited recall performance,\nleading to hybrid architectures that combine linear and full attention layers.\nDespite extensive hybrid architecture research, the choice of linear attention\ncomponent has not been deeply explored. We systematically evaluate various\nlinear attention models across generations - vector recurrences to advanced\ngating mechanisms - both standalone and hybridized. To enable this\ncomprehensive analysis, we trained and open-sourced 72 models: 36 at 340M\nparameters (20B tokens) and 36 at 1.3B parameters (100B tokens), covering six\nlinear attention variants across five hybridization ratios. Benchmarking on\nstandard language modeling and recall tasks reveals that superior standalone\nlinear models do not necessarily excel in hybrids. While language modeling\nremains stable across linear-to-full attention ratios, recall significantly\nimproves with increased full attention layers, particularly below a 3:1 ratio.\nOur study highlights selective gating, hierarchical recurrence, and controlled\nforgetting as critical for effective hybrid models. We recommend architectures\nsuch as HGRN-2 or GatedDeltaNet with a linear-to-full ratio between 3:1 and 6:1\nto achieve Transformer-level recall efficiently. Our models are open-sourced at\nhttps://huggingface.co/collections/m-a-p/hybrid-linear-attention-research-686c488a63d609d2f20e2b1e.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u7ebf\u6027\u6ce8\u610f\u529b\u673a\u5236\u5728\u957f\u5e8f\u5217\u5904\u7406\u4e2d\u7684\u5c40\u9650\u6027\uff0c\u5e76\u7cfb\u7edf\u8bc4\u4f30\u4e86\u591a\u79cd\u7ebf\u6027\u6ce8\u610f\u529b\u6a21\u578b\u53ca\u5176\u6df7\u5408\u67b6\u6784\u7684\u6027\u80fd\u3002", "motivation": "Transformer\u5728\u5904\u7406\u957f\u5e8f\u5217\u65f6\u9762\u4e34\u4e8c\u6b21\u590d\u6742\u5ea6\u548c\u5185\u5b58\u95ee\u9898\uff0c\u7ebf\u6027\u6ce8\u610f\u529b\u673a\u5236\u867d\u80fd\u7f13\u89e3\u4f46\u5b58\u5728\u53ec\u56de\u6027\u80fd\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u56e0\u6b64\u9700\u8981\u7814\u7a76\u6df7\u5408\u67b6\u6784\u4e2d\u7ebf\u6027\u6ce8\u610f\u529b\u7ec4\u4ef6\u7684\u9009\u62e9\u3002", "method": "\u7cfb\u7edf\u8bc4\u4f30\u4e86\u516d\u79cd\u7ebf\u6027\u6ce8\u610f\u529b\u53d8\u4f53\u53ca\u5176\u4e94\u79cd\u6df7\u5408\u6bd4\u4f8b\uff0c\u8bad\u7ec3\u5e76\u5f00\u6e90\u4e8672\u4e2a\u6a21\u578b\uff08340M\u548c1.3B\u53c2\u6570\uff09\uff0c\u6db5\u76d6\u8bed\u8a00\u5efa\u6a21\u548c\u53ec\u56de\u4efb\u52a1\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff0c\u4f18\u79c0\u7684\u72ec\u7acb\u7ebf\u6027\u6a21\u578b\u5728\u6df7\u5408\u67b6\u6784\u4e2d\u672a\u5fc5\u8868\u73b0\u6700\u4f73\uff1b\u53ec\u56de\u6027\u80fd\u5728\u589e\u52a0\u5168\u6ce8\u610f\u529b\u5c42\u540e\u663e\u8457\u63d0\u5347\uff0c\u5c24\u5176\u662f\u57283:1\u4ee5\u4e0b\u6bd4\u4f8b\u65f6\u3002", "conclusion": "\u7814\u7a76\u63a8\u8350\u4f7f\u7528HGRN-2\u6216GatedDeltaNet\u7b49\u67b6\u6784\uff0c\u7ebf\u6027\u4e0e\u5168\u6ce8\u610f\u529b\u6bd4\u4f8b\u57283:1\u52306:1\u4e4b\u95f4\uff0c\u4ee5\u5b9e\u73b0\u9ad8\u6548\u7684Transformer\u7ea7\u53ec\u56de\u6027\u80fd\u3002"}}
{"id": "2507.06458", "pdf": "https://arxiv.org/pdf/2507.06458", "abs": "https://arxiv.org/abs/2507.06458", "authors": ["Arjun Banerjee", "David Martinez", "Camille Dang", "Ethan Tam"], "title": "Automated Neuron Labelling Enables Generative Steering and Interpretability in Protein Language Models", "categories": ["cs.LG", "q-bio.BM"], "comment": "15 pages, 13 figures. Accepted to Proceedings of the Workshop on\n  Generative AI for Biology at the 42nd International Conference on Machine\n  Learning (Spotlight)", "summary": "Protein language models (PLMs) encode rich biological information, yet their\ninternal neuron representations are poorly understood. We introduce the first\nautomated framework for labeling every neuron in a PLM with biologically\ngrounded natural language descriptions. Unlike prior approaches relying on\nsparse autoencoders or manual annotation, our method scales to hundreds of\nthousands of neurons, revealing individual neurons are selectively sensitive to\ndiverse biochemical and structural properties. We then develop a novel neuron\nactivation-guided steering method to generate proteins with desired traits,\nenabling convergence to target biochemical properties like molecular weight and\ninstability index as well as secondary and tertiary structural motifs,\nincluding alpha helices and canonical Zinc Fingers. We finally show that\nanalysis of labeled neurons in different model sizes reveals PLM scaling laws\nand a structured neuron space distribution.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u81ea\u52a8\u5316\u6846\u67b6\uff0c\u7528\u4e8e\u4e3a\u86cb\u767d\u8d28\u8bed\u8a00\u6a21\u578b\uff08PLM\uff09\u4e2d\u7684\u6bcf\u4e2a\u795e\u7ecf\u5143\u63d0\u4f9b\u751f\u7269\u5b66\u57fa\u7840\u7684\u81ea\u7136\u8bed\u8a00\u63cf\u8ff0\uff0c\u5e76\u5f00\u53d1\u4e86\u4e00\u79cd\u57fa\u4e8e\u795e\u7ecf\u5143\u6fc0\u6d3b\u7684\u5f15\u5bfc\u65b9\u6cd5\uff0c\u4ee5\u751f\u6210\u5177\u6709\u76ee\u6807\u7279\u6027\u7684\u86cb\u767d\u8d28\u3002", "motivation": "\u7406\u89e3PLM\u5185\u90e8\u795e\u7ecf\u5143\u7684\u751f\u7269\u5b66\u610f\u4e49\uff0c\u5e76\u5229\u7528\u8fd9\u4e9b\u4fe1\u606f\u6307\u5bfc\u86cb\u767d\u8d28\u8bbe\u8ba1\u3002", "method": "\u5f15\u5165\u81ea\u52a8\u5316\u6846\u67b6\u6807\u8bb0\u795e\u7ecf\u5143\uff0c\u5f00\u53d1\u795e\u7ecf\u5143\u6fc0\u6d3b\u5f15\u5bfc\u65b9\u6cd5\u751f\u6210\u76ee\u6807\u86cb\u767d\u8d28\u3002", "result": "\u63ed\u793a\u4e86\u795e\u7ecf\u5143\u5bf9\u591a\u6837\u751f\u5316\u7279\u6027\u7684\u9009\u62e9\u6027\u654f\u611f\uff0c\u5b9e\u73b0\u4e86\u76ee\u6807\u751f\u5316\u7279\u6027\u548c\u7ed3\u6784\u57fa\u5e8f\u7684\u751f\u6210\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3aPLM\u7684\u795e\u7ecf\u5143\u7406\u89e3\u63d0\u4f9b\u4e86\u65b0\u89c6\u89d2\uff0c\u5e76\u5c55\u793a\u4e86\u5728\u86cb\u767d\u8d28\u8bbe\u8ba1\u4e2d\u7684\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2507.06485", "pdf": "https://arxiv.org/pdf/2507.06485", "abs": "https://arxiv.org/abs/2507.06485", "authors": ["Ziyang Wang", "Jaehong Yoon", "Shoubin Yu", "Md Mohaiminul Islam", "Gedas Bertasius", "Mohit Bansal"], "title": "Video-RTS: Rethinking Reinforcement Learning and Test-Time Scaling for Efficient and Enhanced Video Reasoning", "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": "The first two authors contributed equally. Project page:\n  https://sites.google.com/cs.unc.edu/videorts2025/", "summary": "Despite advances in reinforcement learning (RL)-based video reasoning with\nlarge language models (LLMs), data collection and finetuning remain significant\nchallenges. These methods often rely on large-scale supervised fine-tuning\n(SFT) with extensive video data and long Chain-of-Thought (CoT) annotations,\nmaking them costly and hard to scale. To address this, we present Video-RTS, a\nnew approach to improve video reasoning capability with drastically improved\ndata efficiency by combining data-efficient RL with a video-adaptive test-time\nscaling (TTS) strategy. Based on observations about the data scaling of RL\nsamples, we skip the resource-intensive SFT step and employ efficient pure-RL\ntraining with output-based rewards, requiring no additional annotations or\nextensive fine-tuning. Furthermore, to utilize computational resources more\nefficiently, we introduce a sparse-to-dense video TTS strategy that improves\ninference by iteratively adding frames based on output consistency. We validate\nour approach on multiple video reasoning benchmarks, showing that Video-RTS\nsurpasses existing video reasoning models by an average of 2.4% in accuracy\nusing only 3.6% training samples. For example, Video-RTS achieves a 4.2%\nimprovement on Video-Holmes, a recent and challenging video reasoning\nbenchmark, and a 2.6% improvement on MMVU. Notably, our pure RL training and\nadaptive video TTS offer complementary strengths, enabling Video-RTS's strong\nreasoning performance.", "AI": {"tldr": "Video-RTS\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u6570\u636e\u9ad8\u6548\u5f3a\u5316\u5b66\u4e60\u548c\u89c6\u9891\u81ea\u9002\u5e94\u6d4b\u8bd5\u65f6\u95f4\u6269\u5c55\u7b56\u7565\u7684\u65b0\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u89c6\u9891\u63a8\u7406\u80fd\u529b\u7684\u6570\u636e\u6548\u7387\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u548c\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u89c6\u9891\u63a8\u7406\u65b9\u6cd5\u4f9d\u8d56\u5927\u89c4\u6a21\u76d1\u7763\u5fae\u8c03\u548c\u957f\u94fe\u601d\u7ef4\u6807\u6ce8\uff0c\u6210\u672c\u9ad8\u4e14\u96be\u4ee5\u6269\u5c55\u3002", "method": "\u8df3\u8fc7\u8d44\u6e90\u5bc6\u96c6\u7684\u76d1\u7763\u5fae\u8c03\u6b65\u9aa4\uff0c\u91c7\u7528\u7eaf\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u548c\u7a00\u758f\u5230\u5bc6\u96c6\u7684\u89c6\u9891\u6d4b\u8bd5\u65f6\u95f4\u6269\u5c55\u7b56\u7565\u3002", "result": "\u5728\u591a\u4e2a\u89c6\u9891\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cVideo-RTS\u4ec5\u75283.6%\u7684\u8bad\u7ec3\u6837\u672c\u5373\u8d85\u8d8a\u73b0\u6709\u6a21\u578b\u5e73\u57472.4%\u7684\u51c6\u786e\u7387\u3002", "conclusion": "Video-RTS\u901a\u8fc7\u7eaf\u5f3a\u5316\u5b66\u4e60\u548c\u81ea\u9002\u5e94\u89c6\u9891\u6269\u5c55\u7b56\u7565\u7684\u4e92\u8865\u4f18\u52bf\uff0c\u5b9e\u73b0\u4e86\u5f3a\u5927\u7684\u63a8\u7406\u6027\u80fd\u3002"}}
{"id": "2507.06470", "pdf": "https://arxiv.org/pdf/2507.06470", "abs": "https://arxiv.org/abs/2507.06470", "authors": ["Nicholas Klein", "Hemlata Tak", "Elie Khoury"], "title": "Open-Set Source Tracing of Audio Deepfake Systems", "categories": ["eess.AS", "cs.SD"], "comment": "Accepted by INTERSPEECH 2025 as part of the special session \"Source\n  Tracing: The Origins of Synthetic or Manipulated Speech\"", "summary": "Existing research on source tracing of audio deepfake systems has focused\nprimarily on the closed-set scenario, while studies that evaluate open-set\nperformance are limited to a small number of unseen systems. Due to the large\nnumber of emerging audio deepfake systems, robust open-set source tracing is\ncritical. We leverage the protocol of the Interspeech 2025 special session on\nsource tracing to evaluate methods for improving open-set source tracing\nperformance. We introduce a novel adaptation to the energy score for\nout-of-distribution (OOD) detection, softmax energy (SME). We find that\nreplacing the typical temperature-scaled energy score with SME provides a\nrelative average improvement of 31% in the standard FPR95 (false positive rate\nat true positive rate of 95%) measure. We further explore SME-guided training\nas well as copy synthesis, codec, and reverberation augmentations, yielding an\nFPR95 of 8.3%.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u6539\u8fdb\u7684\u5f00\u653e\u96c6\u97f3\u9891\u6df1\u5ea6\u4f2a\u9020\u6e90\u8ffd\u8e2a\u65b9\u6cd5\uff0c\u901a\u8fc7\u8f6f\u6700\u5927\u80fd\u91cf\uff08SME\uff09\u548c\u591a\u79cd\u6570\u636e\u589e\u5f3a\u6280\u672f\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u591a\u5173\u6ce8\u95ed\u96c6\u573a\u666f\uff0c\u800c\u5f00\u653e\u96c6\u6027\u80fd\u7814\u7a76\u8f83\u5c11\u3002\u968f\u7740\u97f3\u9891\u6df1\u5ea6\u4f2a\u9020\u7cfb\u7edf\u7684\u589e\u591a\uff0c\u5f00\u653e\u96c6\u6e90\u8ffd\u8e2a\u7684\u9c81\u68d2\u6027\u81f3\u5173\u91cd\u8981\u3002", "method": "\u5f15\u5165\u8f6f\u6700\u5927\u80fd\u91cf\uff08SME\uff09\u7528\u4e8e\u79bb\u7fa4\u68c0\u6d4b\uff0c\u5e76\u63a2\u7d22\u4e86SME\u5f15\u5bfc\u8bad\u7ec3\u53ca\u591a\u79cd\u6570\u636e\u589e\u5f3a\u6280\u672f\uff08\u5982\u5408\u6210\u3001\u7f16\u89e3\u7801\u548c\u6df7\u54cd\u589e\u5f3a\uff09\u3002", "result": "SME\u76f8\u6bd4\u4f20\u7edf\u80fd\u91cf\u5206\u6570\u5728FPR95\u6307\u6807\u4e0a\u76f8\u5bf9\u63d0\u534731%\uff0c\u6700\u7ec8FPR95\u8fbe\u52308.3%\u3002", "conclusion": "SME\u548c\u6570\u636e\u589e\u5f3a\u6280\u672f\u663e\u8457\u63d0\u5347\u4e86\u5f00\u653e\u96c6\u97f3\u9891\u6df1\u5ea6\u4f2a\u9020\u6e90\u8ffd\u8e2a\u7684\u6027\u80fd\u3002"}}
{"id": "2507.06747", "pdf": "https://arxiv.org/pdf/2507.06747", "abs": "https://arxiv.org/abs/2507.06747", "authors": ["Daojie Peng", "Jiahang Cao", "Qiang Zhang", "Jun Ma"], "title": "LOVON: Legged Open-Vocabulary Object Navigator", "categories": ["cs.RO", "cs.CV"], "comment": "9 pages, 10 figures; Project Page:\n  https://daojiepeng.github.io/LOVON/", "summary": "Object navigation in open-world environments remains a formidable and\npervasive challenge for robotic systems, particularly when it comes to\nexecuting long-horizon tasks that require both open-world object detection and\nhigh-level task planning. Traditional methods often struggle to integrate these\ncomponents effectively, and this limits their capability to deal with complex,\nlong-range navigation missions. In this paper, we propose LOVON, a novel\nframework that integrates large language models (LLMs) for hierarchical task\nplanning with open-vocabulary visual detection models, tailored for effective\nlong-range object navigation in dynamic, unstructured environments. To tackle\nreal-world challenges including visual jittering, blind zones, and temporary\ntarget loss, we design dedicated solutions such as Laplacian Variance Filtering\nfor visual stabilization. We also develop a functional execution logic for the\nrobot that guarantees LOVON's capabilities in autonomous navigation, task\nadaptation, and robust task completion. Extensive evaluations demonstrate the\nsuccessful completion of long-sequence tasks involving real-time detection,\nsearch, and navigation toward open-vocabulary dynamic targets. Furthermore,\nreal-world experiments across different legged robots (Unitree Go2, B2, and\nH1-2) showcase the compatibility and appealing plug-and-play feature of LOVON.", "AI": {"tldr": "LOVON\u662f\u4e00\u4e2a\u7ed3\u5408\u5927\u8bed\u8a00\u6a21\u578b\u548c\u5f00\u653e\u8bcd\u6c47\u89c6\u89c9\u68c0\u6d4b\u7684\u65b0\u6846\u67b6\uff0c\u7528\u4e8e\u52a8\u6001\u73af\u5883\u4e2d\u957f\u8ddd\u79bb\u7269\u4f53\u5bfc\u822a\u3002", "motivation": "\u89e3\u51b3\u4f20\u7edf\u65b9\u6cd5\u5728\u590d\u6742\u957f\u8ddd\u79bb\u5bfc\u822a\u4efb\u52a1\u4e2d\u96be\u4ee5\u6574\u5408\u5f00\u653e\u4e16\u754c\u7269\u4f53\u68c0\u6d4b\u548c\u9ad8\u7ea7\u4efb\u52a1\u89c4\u5212\u7684\u95ee\u9898\u3002", "method": "\u96c6\u6210\u5927\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u5206\u5c42\u4efb\u52a1\u89c4\u5212\uff0c\u7ed3\u5408\u5f00\u653e\u8bcd\u6c47\u89c6\u89c9\u68c0\u6d4b\u6a21\u578b\uff0c\u5e76\u8bbe\u8ba1\u89c6\u89c9\u7a33\u5b9a\u5316\u548c\u529f\u80fd\u6267\u884c\u903b\u8f91\u3002", "result": "\u6210\u529f\u5b8c\u6210\u6d89\u53ca\u5b9e\u65f6\u68c0\u6d4b\u3001\u641c\u7d22\u548c\u5bfc\u822a\u7684\u957f\u5e8f\u5217\u4efb\u52a1\uff0c\u5e76\u5728\u591a\u79cd\u673a\u5668\u4eba\u4e0a\u9a8c\u8bc1\u4e86\u517c\u5bb9\u6027\u3002", "conclusion": "LOVON\u6846\u67b6\u5728\u52a8\u6001\u73af\u5883\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u5177\u6709\u517c\u5bb9\u6027\u548c\u5373\u63d2\u5373\u7528\u7279\u6027\u3002"}}
{"id": "2507.06489", "pdf": "https://arxiv.org/pdf/2507.06489", "abs": "https://arxiv.org/abs/2507.06489", "authors": ["Stephen Obadinma", "Xiaodan Zhu"], "title": "On the Robustness of Verbal Confidence of LLMs in Adversarial Attacks", "categories": ["cs.CL"], "comment": null, "summary": "Robust verbal confidence generated by large language models (LLMs) is crucial\nfor the deployment of LLMs to ensure transparency, trust, and safety in\nhuman-AI interactions across many high-stakes applications. In this paper, we\npresent the first comprehensive study on the robustness of verbal confidence\nunder adversarial attacks. We introduce a novel framework for attacking verbal\nconfidence scores through both perturbation and jailbreak-based methods, and\nshow that these attacks can significantly jeopardize verbal confidence\nestimates and lead to frequent answer changes. We examine a variety of\nprompting strategies, model sizes, and application domains, revealing that\ncurrent confidence elicitation methods are vulnerable and that commonly used\ndefence techniques are largely ineffective or counterproductive. Our findings\nunderscore the urgent need to design more robust mechanisms for confidence\nexpression in LLMs, as even subtle semantic-preserving modifications can lead\nto misleading confidence in responses.", "AI": {"tldr": "\u7814\u7a76\u9996\u6b21\u5168\u9762\u63a2\u8ba8\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5728\u5bf9\u6297\u653b\u51fb\u4e0b\u8bed\u8a00\u7f6e\u4fe1\u5ea6\u7684\u9c81\u68d2\u6027\uff0c\u53d1\u73b0\u73b0\u6709\u65b9\u6cd5\u6613\u53d7\u653b\u51fb\u4e14\u9632\u5fa1\u63aa\u65bd\u65e0\u6548\u3002", "motivation": "\u786e\u4fddLLM\u5728\u4eba\u7c7b-AI\u4ea4\u4e92\u4e2d\u7684\u900f\u660e\u6027\u3001\u4fe1\u4efb\u548c\u5b89\u5168\u6027\uff0c\u5c24\u5176\u662f\u5728\u9ad8\u98ce\u9669\u5e94\u7528\u4e2d\u3002", "method": "\u63d0\u51fa\u901a\u8fc7\u6270\u52a8\u548c\u8d8a\u72f1\u65b9\u6cd5\u653b\u51fb\u8bed\u8a00\u7f6e\u4fe1\u5ea6\u7684\u65b0\u6846\u67b6\uff0c\u6d4b\u8bd5\u591a\u79cd\u63d0\u793a\u7b56\u7565\u3001\u6a21\u578b\u89c4\u6a21\u548c\u5e94\u7528\u9886\u57df\u3002", "result": "\u653b\u51fb\u663e\u8457\u5371\u5bb3\u7f6e\u4fe1\u5ea6\u4f30\u8ba1\u5e76\u5bfc\u81f4\u9891\u7e41\u7b54\u6848\u66f4\u6539\uff0c\u73b0\u6709\u9632\u5fa1\u6280\u672f\u5927\u591a\u65e0\u6548\u6216\u9002\u5f97\u5176\u53cd\u3002", "conclusion": "\u4e9f\u9700\u8bbe\u8ba1\u66f4\u9c81\u68d2\u7684\u8bed\u8a00\u7f6e\u4fe1\u5ea6\u8868\u8fbe\u673a\u5236\uff0c\u56e0\u7ec6\u5fae\u8bed\u4e49\u4fdd\u7559\u4fee\u6539\u4e5f\u53ef\u80fd\u5bfc\u81f4\u8bef\u5bfc\u6027\u7f6e\u4fe1\u5ea6\u3002"}}
{"id": "2507.06461", "pdf": "https://arxiv.org/pdf/2507.06461", "abs": "https://arxiv.org/abs/2507.06461", "authors": ["Risi Jaiswal", "Supriyo Datta", "Joseph G. Makin"], "title": "Energy-Efficient Supervised Learning with a Binary Stochastic Forward-Forward Algorithm", "categories": ["cs.LG", "cs.NE"], "comment": "24 pages, 5 figures, 4 tables. Under review", "summary": "Reducing energy consumption has become a pressing need for modern machine\nlearning, which has achieved many of its most impressive results by scaling to\nlarger and more energy-consumptive neural networks. Unfortunately, the main\nalgorithm for training such networks, backpropagation, poses significant\nchallenges for custom hardware accelerators, due to both its serial\ndependencies and the memory footprint needed to store forward activations for\nthe backward pass. Alternatives to backprop, although less effective, do exist;\nhere the main computational bottleneck becomes matrix multiplication. In this\nstudy, we derive forward-forward algorithms for binary, stochastic units.\nBinarization of the activations transforms matrix multiplications into indexing\noperations, which can be executed efficiently in hardware. Stochasticity,\ncombined with tied weights across units with different biases, bypasses the\ninformation bottleneck imposed by binary units. Furthermore, although slow and\nexpensive in traditional hardware, binary sampling that is very fast can be\nimplemented cheaply with p-bits (probabilistic bits), novel devices made up of\nunstable magnets. We evaluate our proposed algorithms on the MNIST,\nFashion-MNIST, and CIFAR-10 datasets, showing that its performance is close to\nreal-valued forward-forward, but with an estimated energy savings of about one\norder of magnitude.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u524d\u5411-\u524d\u5411\u7b97\u6cd5\u7684\u4e8c\u503c\u968f\u673a\u5355\u5143\u8bad\u7ec3\u65b9\u6cd5\uff0c\u4ee5\u51cf\u5c11\u795e\u7ecf\u7f51\u7edc\u8bad\u7ec3\u4e2d\u7684\u80fd\u8017\uff0c\u5e76\u901a\u8fc7\u786c\u4ef6\u4f18\u5316\u5b9e\u73b0\u9ad8\u6548\u8ba1\u7b97\u3002", "motivation": "\u73b0\u4ee3\u673a\u5668\u5b66\u4e60\u56e0\u5927\u89c4\u6a21\u795e\u7ecf\u7f51\u7edc\u7684\u9ad8\u80fd\u8017\u95ee\u9898\u800c\u9762\u4e34\u6311\u6218\uff0c\u53cd\u5411\u4f20\u64ad\u7b97\u6cd5\u5728\u786c\u4ef6\u52a0\u901f\u5668\u4e0a\u5b58\u5728\u4f9d\u8d56\u6027\u548c\u5185\u5b58\u5360\u7528\u95ee\u9898\u3002", "method": "\u91c7\u7528\u4e8c\u503c\u5316\u548c\u968f\u673a\u5316\u7684\u524d\u5411-\u524d\u5411\u7b97\u6cd5\uff0c\u5c06\u77e9\u9635\u4e58\u6cd5\u8f6c\u5316\u4e3a\u7d22\u5f15\u64cd\u4f5c\uff0c\u7ed3\u5408p-bits\u5b9e\u73b0\u9ad8\u6548\u786c\u4ef6\u8ba1\u7b97\u3002", "result": "\u5728MNIST\u3001Fashion-MNIST\u548cCIFAR-10\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u63a5\u8fd1\u5b9e\u503c\u524d\u5411-\u524d\u5411\u7b97\u6cd5\uff0c\u80fd\u8017\u964d\u4f4e\u7ea6\u4e00\u4e2a\u6570\u91cf\u7ea7\u3002", "conclusion": "\u63d0\u51fa\u7684\u7b97\u6cd5\u5728\u4fdd\u6301\u6027\u80fd\u7684\u540c\u65f6\u663e\u8457\u964d\u4f4e\u4e86\u80fd\u8017\uff0c\u4e3a\u8282\u80fd\u673a\u5668\u5b66\u4e60\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6848\u3002"}}
{"id": "2507.06486", "pdf": "https://arxiv.org/pdf/2507.06486", "abs": "https://arxiv.org/abs/2507.06486", "authors": ["Yuechen Xie", "Haobo Jiang", "Jin Xie"], "title": "Mask6D: Masked Pose Priors For 6D Object Pose Estimation", "categories": ["cs.CV"], "comment": "Accepted at ICASSP 2024. 4 figures, 3 tables", "summary": "Robust 6D object pose estimation in cluttered or occluded conditions using\nmonocular RGB images remains a challenging task. One reason is that current\npose estimation networks struggle to extract discriminative, pose-aware\nfeatures using 2D feature backbones, especially when the available RGB\ninformation is limited due to target occlusion in cluttered scenes. To mitigate\nthis, we propose a novel pose estimation-specific pre-training strategy named\nMask6D. Our approach incorporates pose-aware 2D-3D correspondence maps and\nvisible mask maps as additional modal information, which is combined with RGB\nimages for the reconstruction-based model pre-training. Essentially, this 2D-3D\ncorrespondence maps a transformed 3D object model to 2D pixels, reflecting the\npose information of the target in camera coordinate system. Meanwhile, the\nintegrated visible mask map can effectively guide our model to disregard\ncluttered background information. In addition, an object-focused pre-training\nloss function is designed to further facilitate our network to remove the\nbackground interference. Finally, we fine-tune our pre-trained pose prior-aware\nnetwork via conventional pose training strategy to realize the reliable pose\nprediction. Extensive experiments verify that our method outperforms previous\nend-to-end pose estimation methods.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aMask6D\u7684\u65b0\u578b\u9884\u8bad\u7ec3\u7b56\u7565\uff0c\u7528\u4e8e\u5728\u5355\u76eeRGB\u56fe\u50cf\u4e2d\u5b9e\u73b0\u9c81\u68d2\u76846D\u7269\u4f53\u59ff\u6001\u4f30\u8ba1\uff0c\u5c24\u5176\u5728\u906e\u6321\u6216\u6742\u4e71\u573a\u666f\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u5f53\u524d\u59ff\u6001\u4f30\u8ba1\u7f51\u7edc\u57282D\u7279\u5f81\u9aa8\u5e72\u7f51\u7edc\u4e2d\u96be\u4ee5\u63d0\u53d6\u533a\u5206\u6027\u5f3a\u7684\u59ff\u6001\u611f\u77e5\u7279\u5f81\uff0c\u5c24\u5176\u662f\u5728\u76ee\u6807\u88ab\u906e\u6321\u6216\u573a\u666f\u6742\u4e71\u65f6RGB\u4fe1\u606f\u53d7\u9650\u7684\u60c5\u51b5\u4e0b\u3002", "method": "\u7ed3\u5408\u59ff\u6001\u611f\u77e5\u76842D-3D\u5bf9\u5e94\u56fe\u548c\u53ef\u89c1\u63a9\u7801\u56fe\u4f5c\u4e3a\u989d\u5916\u6a21\u6001\u4fe1\u606f\uff0c\u4e0eRGB\u56fe\u50cf\u4e00\u8d77\u7528\u4e8e\u57fa\u4e8e\u91cd\u5efa\u7684\u6a21\u578b\u9884\u8bad\u7ec3\uff0c\u5e76\u8bbe\u8ba1\u4e86\u4e13\u6ce8\u4e8e\u7269\u4f53\u7684\u9884\u8bad\u7ec3\u635f\u5931\u51fd\u6570\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u7aef\u5230\u7aef\u59ff\u6001\u4f30\u8ba1\u4efb\u52a1\u4e2d\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "Mask6D\u901a\u8fc7\u5f15\u5165\u59ff\u6001\u611f\u77e5\u7684\u6a21\u6001\u4fe1\u606f\u548c\u9884\u8bad\u7ec3\u7b56\u7565\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5728\u590d\u6742\u573a\u666f\u4e2d\u76846D\u59ff\u6001\u4f30\u8ba1\u6027\u80fd\u3002"}}
{"id": "2507.06262", "pdf": "https://arxiv.org/pdf/2507.06262", "abs": "https://arxiv.org/abs/2507.06262", "authors": ["Haoqi He", "Xiaokai Lin", "Jiancai Chen", "Yan Xiao"], "title": "Q-Detection: A Quantum-Classical Hybrid Poisoning Attack Detection Method", "categories": ["cs.CR", "cs.AI", "cs.LG", "quant-ph"], "comment": "IJCAI 2025 Main Conference Accepted Paper", "summary": "Data poisoning attacks pose significant threats to machine learning models by\nintroducing malicious data into the training process, thereby degrading model\nperformance or manipulating predictions. Detecting and sifting out poisoned\ndata is an important method to prevent data poisoning attacks. Limited by\nclassical computation frameworks, upcoming larger-scale and more complex\ndatasets may pose difficulties for detection. We introduce the unique speedup\nof quantum computing for the first time in the task of detecting data\npoisoning. We present Q-Detection, a quantum-classical hybrid defense method\nfor detecting poisoning attacks. Q-Detection also introduces the Q-WAN, which\nis optimized using quantum computing devices. Experimental results using\nmultiple quantum simulation libraries show that Q-Detection effectively defends\nagainst label manipulation and backdoor attacks. The metrics demonstrate that\nQ-Detection consistently outperforms the baseline methods and is comparable to\nthe state-of-the-art. Theoretical analysis shows that Q-Detection is expected\nto achieve more than a 20% speedup using quantum computing power.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u91cf\u5b50\u8ba1\u7b97\u7684\u6df7\u5408\u9632\u5fa1\u65b9\u6cd5Q-Detection\uff0c\u7528\u4e8e\u68c0\u6d4b\u6570\u636e\u6295\u6bd2\u653b\u51fb\uff0c\u5b9e\u9a8c\u8bc1\u660e\u5176\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u5e76\u6709\u671b\u5b9e\u73b020%\u4ee5\u4e0a\u7684\u52a0\u901f\u3002", "motivation": "\u6570\u636e\u6295\u6bd2\u653b\u51fb\u5bf9\u673a\u5668\u5b66\u4e60\u6a21\u578b\u6784\u6210\u5a01\u80c1\uff0c\u4f20\u7edf\u8ba1\u7b97\u6846\u67b6\u5728\u5927\u89c4\u6a21\u590d\u6742\u6570\u636e\u96c6\u4e0a\u68c0\u6d4b\u56f0\u96be\uff0c\u91cf\u5b50\u8ba1\u7b97\u7684\u72ec\u7279\u52a0\u901f\u80fd\u529b\u4e3a\u6b64\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002", "method": "\u63d0\u51faQ-Detection\u65b9\u6cd5\uff0c\u7ed3\u5408\u91cf\u5b50\u8ba1\u7b97\u8bbe\u5907\u4f18\u5316\u7684Q-WAN\uff0c\u901a\u8fc7\u91cf\u5b50-\u7ecf\u5178\u6df7\u5408\u6846\u67b6\u68c0\u6d4b\u6295\u6bd2\u6570\u636e\u3002", "result": "\u5b9e\u9a8c\u8868\u660eQ-Detection\u80fd\u6709\u6548\u9632\u5fa1\u6807\u7b7e\u64cd\u7eb5\u548c\u540e\u95e8\u653b\u51fb\uff0c\u6027\u80fd\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u4e14\u4e0e\u6700\u5148\u8fdb\u65b9\u6cd5\u76f8\u5f53\u3002", "conclusion": "Q-Detection\u5229\u7528\u91cf\u5b50\u8ba1\u7b97\u80fd\u529b\u663e\u8457\u63d0\u5347\u4e86\u6295\u6bd2\u68c0\u6d4b\u6548\u7387\uff0c\u7406\u8bba\u5206\u6790\u663e\u793a\u5176\u6709\u671b\u5b9e\u73b020%\u4ee5\u4e0a\u7684\u901f\u5ea6\u63d0\u5347\u3002"}}
{"id": "2507.07068", "pdf": "https://arxiv.org/pdf/2507.07068", "abs": "https://arxiv.org/abs/2507.07068", "authors": ["Dipayan Bhadra", "Mehrab Hosain", "Fatema Alam"], "title": "Deep Feed-Forward Neural Network for Bangla Isolated Speech Recognition", "categories": ["eess.AS", "cs.SD", "68T05", "I.2.7; I.5.1; H.5.2"], "comment": "12 pages, 3 figures, 4 tables. published in Jatiya Kabi Kazi Nazrul\n  Islam University, Vol. 10 No. 1-2, 2025 https://jkkniu.edu.bd/13817-2/", "summary": "As the most important human-machine interfacing tool, an insignificant amount\nof work has been carried out on Bangla Speech Recognition compared to the\nEnglish language. Motivated by this, in this work, the performance of\nspeaker-independent isolated speech recognition systems has been implemented\nand analyzed using a dataset that is created containing both isolated Bangla\nand English spoken words. An approach using the Mel Frequency Cepstral\nCoefficient (MFCC) and Deep Feed-Forward Fully Connected Neural Network (DFFNN)\nof 7 layers as a classifier is proposed in this work to recognize isolated\nspoken words. This work shows 93.42% recognition accuracy which is better\ncompared to most of the works done previously on Bangla speech recognition\nconsidering the number of classes and dataset size.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eMFCC\u548c7\u5c42\u6df1\u5ea6\u524d\u9988\u5168\u8fde\u63a5\u795e\u7ecf\u7f51\u7edc\u7684\u5b5f\u52a0\u62c9\u8bed\u5b64\u7acb\u8bed\u97f3\u8bc6\u522b\u7cfb\u7edf\uff0c\u5b9e\u73b0\u4e8693.42%\u7684\u8bc6\u522b\u51c6\u786e\u7387\u3002", "motivation": "\u7531\u4e8e\u5b5f\u52a0\u62c9\u8bed\u8bed\u97f3\u8bc6\u522b\u7684\u7814\u7a76\u8fdc\u5c11\u4e8e\u82f1\u8bed\uff0c\u4f5c\u8005\u5e0c\u671b\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u5e76\u63d0\u5347\u5176\u6027\u80fd\u3002", "method": "\u4f7f\u7528MFCC\u63d0\u53d6\u7279\u5f81\uff0c\u5e76\u91c7\u75287\u5c42\u6df1\u5ea6\u524d\u9988\u5168\u8fde\u63a5\u795e\u7ecf\u7f51\u7edc\u4f5c\u4e3a\u5206\u7c7b\u5668\u3002", "result": "\u7cfb\u7edf\u5728\u5b64\u7acb\u8bcd\u8bc6\u522b\u4efb\u52a1\u4e2d\u8fbe\u523093.42%\u7684\u51c6\u786e\u7387\uff0c\u4f18\u4e8e\u4ee5\u5f80\u5927\u591a\u6570\u5b5f\u52a0\u62c9\u8bed\u8bed\u97f3\u8bc6\u522b\u7814\u7a76\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u5b5f\u52a0\u62c9\u8bed\u8bed\u97f3\u8bc6\u522b\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u53c2\u8003\u3002"}}
{"id": "2507.06464", "pdf": "https://arxiv.org/pdf/2507.06464", "abs": "https://arxiv.org/abs/2507.06464", "authors": ["Hanyang Peng", "Shuang Qin", "Yue Yu", "Fangqing Jiang", "Hui Wang", "Wen Gao"], "title": "SoftSignSGD(S3): An Enhanced Optimizer for Practical DNN Training and Loss Spikes Minimization Beyond Adam", "categories": ["cs.LG", "cs.AI"], "comment": "20pages, 11pages", "summary": "Adam has proven remarkable successful in training deep neural networks, but\nthe mechanisms underlying its empirical successes and limitations remain\nunderexplored. In this study, we demonstrate that the effectiveness of Adam\nstems largely from its similarity to SignSGD in robustly handling large\ngradient fluctuations, yet it is also vulnerable to destabilizing loss spikes\ndue to its uncontrolled update scaling. To enhance the advantage of Adam and\nmitigate its limitation, we propose SignSoftSGD (S3), a novel optimizer with\nthree key innovations. \\emph{First}, S3 generalizes the sign-like update by\nemploying a flexible $p$-th order momentum ($p \\geq 1$) in the denominator,\ndeparting from the conventional second-order momentum (variance)\npreconditioning. This design enables enhanced performance while achieving\nstable training even with aggressive learning rates. \\emph{Second}, S3\nminimizes the occurrences of loss spikes through unified exponential moving\naverage coefficients for numerator and denominator momenta, which inherently\nbound updates to $[-1, 1]$ and simplify hyperparameter tuning. \\emph{Third}, S3\nincorporates an equivalent Nesterov's accelerated gradient(NAG) module,\naccelerating convergence without memory overhead. Theoretically, we prove that\nS3 achieves the optimal convergence rate of\n$O\\left(\\frac{1}{T^{\\sfrac{1}{4}}}\\right)$ for general nonconvex stochastic\noptimization under weak assumptions. Extensive experiments across a range of\nvision and language tasks show that \\textsf{\\small S3} not only converges more\nrapidly and improves performance but also rarely experiences loss spikes, even\nwith a \\textbf{$\\bm{10 \\times}$} larger learning rate. In fact, S3 delivers\nperformance comparable to or better than AdamW with \\textbf{$2 \\times$} the\ntraining steps, establishing its efficacy in both efficiency and final task\nperformance.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aSignSoftSGD\uff08S3\uff09\u7684\u65b0\u578b\u4f18\u5316\u5668\uff0c\u901a\u8fc7\u6539\u8fdbAdam\u7684\u66f4\u65b0\u673a\u5236\uff0c\u589e\u5f3a\u4e86\u7a33\u5b9a\u6027\u548c\u6027\u80fd\u3002", "motivation": "Adam\u5728\u8bad\u7ec3\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u5176\u673a\u5236\u548c\u5c40\u9650\u6027\u5c1a\u672a\u5145\u5206\u7814\u7a76\u3002\u8bba\u6587\u65e8\u5728\u6539\u8fdbAdam\u7684\u4e0d\u8db3\uff0c\u5982\u68af\u5ea6\u6ce2\u52a8\u548c\u635f\u5931\u5cf0\u503c\u95ee\u9898\u3002", "method": "S3\u901a\u8fc7\u5f15\u5165p\u9636\u52a8\u91cf\u3001\u7edf\u4e00\u7684EMA\u7cfb\u6570\u548cNAG\u6a21\u5757\uff0c\u6539\u8fdb\u4e86Adam\u7684\u66f4\u65b0\u673a\u5236\uff0c\u63d0\u5347\u4e86\u7a33\u5b9a\u6027\u548c\u6536\u655b\u901f\u5ea6\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cS3\u5728\u591a\u79cd\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u4e8eAdamW\uff0c\u6536\u655b\u66f4\u5feb\u4e14\u635f\u5931\u5cf0\u503c\u66f4\u5c11\uff0c\u751a\u81f3\u80fd\u4f7f\u752810\u500d\u7684\u5b66\u4e60\u7387\u3002", "conclusion": "S3\u5728\u6548\u7387\u548c\u6700\u7ec8\u6027\u80fd\u4e0a\u5747\u4f18\u4e8eAdam\uff0c\u4e3a\u6df1\u5ea6\u5b66\u4e60\u7684\u4f18\u5316\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2507.06510", "pdf": "https://arxiv.org/pdf/2507.06510", "abs": "https://arxiv.org/abs/2507.06510", "authors": ["Yupeng Hu", "Changxing Ding", "Chang Sun", "Shaoli Huang", "Xiangmin Xu"], "title": "Bilateral Collaboration with Large Vision-Language Models for Open Vocabulary Human-Object Interaction Detection", "categories": ["cs.CV"], "comment": "ICCV 2025", "summary": "Open vocabulary Human-Object Interaction (HOI) detection is a challenging\ntask that detects all <human, verb, object> triplets of interest in an image,\neven those that are not pre-defined in the training set. Existing approaches\ntypically rely on output features generated by large Vision-Language Models\n(VLMs) to enhance the generalization ability of interaction representations.\nHowever, the visual features produced by VLMs are holistic and coarse-grained,\nwhich contradicts the nature of detection tasks. To address this issue, we\npropose a novel Bilateral Collaboration framework for open vocabulary HOI\ndetection (BC-HOI). This framework includes an Attention Bias Guidance (ABG)\ncomponent, which guides the VLM to produce fine-grained instance-level\ninteraction features according to the attention bias provided by the HOI\ndetector. It also includes a Large Language Model (LLM)-based Supervision\nGuidance (LSG) component, which provides fine-grained token-level supervision\nfor the HOI detector by the LLM component of the VLM. LSG enhances the ability\nof ABG to generate high-quality attention bias. We conduct extensive\nexperiments on two popular benchmarks: HICO-DET and V-COCO, consistently\nachieving superior performance in the open vocabulary and closed settings. The\ncode will be released in Github.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u53cc\u8fb9\u534f\u4f5c\u6846\u67b6\uff08BC-HOI\uff09\uff0c\u7528\u4e8e\u5f00\u653e\u8bcd\u6c47HOI\u68c0\u6d4b\uff0c\u901a\u8fc7\u6ce8\u610f\u529b\u504f\u5dee\u5f15\u5bfc\uff08ABG\uff09\u548c\u57fa\u4e8eLLM\u7684\u76d1\u7763\u5f15\u5bfc\uff08LSG\uff09\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\u751f\u6210\u7684\u7279\u5f81\uff0c\u4f46\u8fd9\u4e9b\u7279\u5f81\u8fc7\u4e8e\u7c97\u7c92\u5ea6\uff0c\u4e0d\u7b26\u5408\u68c0\u6d4b\u4efb\u52a1\u9700\u6c42\u3002", "method": "\u63d0\u51faBC-HOI\u6846\u67b6\uff0c\u5305\u542bABG\uff08\u5f15\u5bfcVLM\u751f\u6210\u7ec6\u7c92\u5ea6\u7279\u5f81\uff09\u548cLSG\uff08\u5229\u7528LLM\u63d0\u4f9b\u7ec6\u7c92\u5ea6\u76d1\u7763\uff09\u3002", "result": "\u5728HICO-DET\u548cV-COCO\u57fa\u51c6\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "BC-HOI\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u5f00\u653e\u8bcd\u6c47HOI\u68c0\u6d4b\u4e2d\u7684\u7ec6\u7c92\u5ea6\u7279\u5f81\u95ee\u9898\uff0c\u6027\u80fd\u663e\u8457\u63d0\u5347\u3002"}}
{"id": "2507.06263", "pdf": "https://arxiv.org/pdf/2507.06263", "abs": "https://arxiv.org/abs/2507.06263", "authors": ["Eric Schwitzgebel", "Jeff Sebo"], "title": "The Emotional Alignment Design Policy", "categories": ["cs.CY", "cs.AI"], "comment": null, "summary": "According to what we call the Emotional Alignment Design Policy, artificial\nentities should be designed to elicit emotional reactions from users that\nappropriately reflect the entities' capacities and moral status, or lack\nthereof. This principle can be violated in two ways: by designing an artificial\nsystem that elicits stronger or weaker emotional reactions than its capacities\nand moral status warrant (overshooting or undershooting), or by designing a\nsystem that elicits the wrong type of emotional reaction (hitting the wrong\ntarget). Although presumably attractive, practical implementation faces several\nchallenges including: How can we respect user autonomy while promoting\nappropriate responses? How should we navigate expert and public disagreement\nand uncertainty about facts and values? What if emotional alignment seems to\nrequire creating or destroying entities with moral status? To what extent\nshould designs conform to versus attempt to alter user assumptions and\nattitudes?", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u60c5\u611f\u5bf9\u9f50\u8bbe\u8ba1\u539f\u5219\uff0c\u5f3a\u8c03\u4eba\u5de5\u5b9e\u4f53\u5e94\u5f15\u53d1\u4e0e\u5176\u80fd\u529b\u548c\u9053\u5fb7\u5730\u4f4d\u76f8\u7b26\u7684\u7528\u6237\u60c5\u611f\u53cd\u5e94\uff0c\u5e76\u63a2\u8ba8\u4e86\u5b9e\u65bd\u4e2d\u7684\u6311\u6218\u3002", "motivation": "\u63a2\u8ba8\u5982\u4f55\u8bbe\u8ba1\u4eba\u5de5\u5b9e\u4f53\u4ee5\u5f15\u53d1\u4e0e\u5176\u9053\u5fb7\u548c\u80fd\u529b\u76f8\u7b26\u7684\u60c5\u611f\u53cd\u5e94\uff0c\u907f\u514d\u8fc7\u5ea6\u6216\u4e0d\u8db3\u7684\u60c5\u611f\u53cd\u5e94\u3002", "method": "\u63d0\u51fa\u60c5\u611f\u5bf9\u9f50\u8bbe\u8ba1\u539f\u5219\uff0c\u5206\u6790\u5176\u53ef\u80fd\u8fdd\u53cd\u7684\u4e24\u79cd\u65b9\u5f0f\uff08\u8fc7\u5ea6\u6216\u4e0d\u8db3\u53cd\u5e94\uff0c\u9519\u8bef\u7c7b\u578b\u53cd\u5e94\uff09\uff0c\u5e76\u8ba8\u8bba\u5b9e\u65bd\u4e2d\u7684\u6311\u6218\u3002", "result": "\u8bc6\u522b\u4e86\u5b9e\u65bd\u60c5\u611f\u5bf9\u9f50\u8bbe\u8ba1\u65f6\u9762\u4e34\u7684\u591a\u4e2a\u6311\u6218\uff0c\u5305\u62ec\u7528\u6237\u81ea\u4e3b\u6743\u3001\u4e13\u5bb6\u4e0e\u516c\u4f17\u5206\u6b67\u3001\u9053\u5fb7\u5730\u4f4d\u5b9e\u4f53\u7684\u5904\u7406\u7b49\u3002", "conclusion": "\u60c5\u611f\u5bf9\u9f50\u8bbe\u8ba1\u539f\u5219\u867d\u5177\u5438\u5f15\u529b\uff0c\u4f46\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u9700\u89e3\u51b3\u591a\u65b9\u9762\u7684\u590d\u6742\u6311\u6218\u3002"}}
{"id": "2507.06787", "pdf": "https://arxiv.org/pdf/2507.06787", "abs": "https://arxiv.org/abs/2507.06787", "authors": ["Sean Smith", "Emmanuel Witrant", "Ya-Jun Pan"], "title": "Stream Function-Based Navigation for Complex Quadcopter Obstacle Avoidance", "categories": ["cs.RO", "cs.SY", "eess.SY"], "comment": null, "summary": "This article presents a novel stream function-based navigational control\nsystem for obstacle avoidance, where obstacles are represented as\ntwo-dimensional (2D) rigid surfaces in inviscid, incompressible flows. The\napproach leverages the vortex panel method (VPM) and incorporates safety\nmargins to control the stream function and flow properties around virtual\nsurfaces, enabling navigation in complex, partially observed environments using\nreal-time sensing. To address the limitations of the VPM in managing relative\ndistance and avoiding rapidly accelerating obstacles at close proximity, the\nsystem integrates a model predictive controller (MPC) based on higher-order\ncontrol barrier functions (HOCBF). This integration incorporates VPM trajectory\ngeneration, state estimation, and constraint handling into a receding-horizon\noptimization problem. The 2D rigid surfaces are enclosed using minimum bounding\nellipses (MBEs), while an adaptive Kalman filter (AKF) captures and predicts\nobstacle dynamics, propagating these estimates into the MPC-HOCBF for rapid\navoidance maneuvers. Evaluation is conducted using a PX4-powered Clover drone\nGazebo simulator and real-time experiments involving a COEX Clover quadcopter\nequipped with a 360 degree LiDAR sensor.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6d41\u51fd\u6570\u7684\u5bfc\u822a\u63a7\u5236\u7cfb\u7edf\uff0c\u7528\u4e8e\u969c\u788d\u7269\u907f\u969c\uff0c\u7ed3\u5408\u4e86\u6da1\u6d41\u9762\u677f\u65b9\u6cd5\uff08VPM\uff09\u548c\u6a21\u578b\u9884\u6d4b\u63a7\u5236\u5668\uff08MPC\uff09\uff0c\u5e76\u901a\u8fc7\u5b9e\u65f6\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "motivation": "\u89e3\u51b3\u5728\u590d\u6742\u3001\u90e8\u5206\u53ef\u89c2\u6d4b\u73af\u5883\u4e2d\u5bfc\u822a\u65f6\uff0c\u4f20\u7edfVPM\u65b9\u6cd5\u5728\u8fd1\u8ddd\u79bb\u907f\u969c\u548c\u5feb\u901f\u79fb\u52a8\u969c\u788d\u7269\u5904\u7406\u4e0a\u7684\u5c40\u9650\u6027\u3002", "method": "\u7ed3\u5408VPM\u548cMPC-HOCBF\uff0c\u5229\u7528\u6700\u5c0f\u5305\u56f4\u692d\u5706\uff08MBE\uff09\u8868\u793a\u969c\u788d\u7269\uff0c\u5e76\u901a\u8fc7\u81ea\u9002\u5e94\u5361\u5c14\u66fc\u6ee4\u6ce2\uff08AKF\uff09\u9884\u6d4b\u969c\u788d\u7269\u52a8\u6001\u3002", "result": "\u5728PX4\u9a71\u52a8\u7684Clover\u65e0\u4eba\u673aGazebo\u6a21\u62df\u5668\u548c\u5b9e\u65f6\u5b9e\u9a8c\u4e2d\u9a8c\u8bc1\u4e86\u7cfb\u7edf\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u8be5\u7cfb\u7edf\u80fd\u591f\u9ad8\u6548\u5904\u7406\u590d\u6742\u73af\u5883\u4e2d\u7684\u969c\u788d\u7269\u907f\u969c\u95ee\u9898\uff0c\u9002\u7528\u4e8e\u5b9e\u65f6\u5bfc\u822a\u4efb\u52a1\u3002"}}
{"id": "2507.06517", "pdf": "https://arxiv.org/pdf/2507.06517", "abs": "https://arxiv.org/abs/2507.06517", "authors": ["Zicong Tang", "Shi Luohe", "Zuchao Li", "Baoyuan Qi", "Guoming Liu", "Lefei Zhang", "Ping Wang"], "title": "SpindleKV: A Novel KV Cache Reduction Method Balancing Both Shallow and Deep Layers", "categories": ["cs.CL"], "comment": "Accepted by ACL 2025 main", "summary": "Large Language Models (LLMs) have achieved impressive accomplishments in\nrecent years. However, the increasing memory consumption of KV cache has\npossessed a significant challenge to the inference system. Eviction methods\nhave revealed the inherent redundancy within the KV cache, demonstrating its\npotential for reduction, particularly in deeper layers. However, KV cache\nreduction for shallower layers has been found to be insufficient. Based on our\nobservation that, the KV cache exhibits a high degree of similarity. Based on\nthis observation, we proposed a novel KV cache reduction method, SpindleKV,\nwhich balances both shallow and deep layers. For deep layers, we employ an\nattention weight based eviction method, while for shallow layers, we apply a\ncodebook based replacement approach which is learnt by similarity and merging\npolicy. Moreover, SpindleKV addressed the Grouped-Query Attention (GQA) dilemma\nfaced by other attention based eviction methods. Experiments on two common\nbenchmarks with three different LLMs shown that SpindleKV obtained better KV\ncache reduction effect compared to baseline methods, while preserving similar\nor even better model performance.", "AI": {"tldr": "SpindleKV\u662f\u4e00\u79cd\u65b0\u578b\u7684KV\u7f13\u5b58\u51cf\u5c11\u65b9\u6cd5\uff0c\u901a\u8fc7\u5e73\u8861\u6d45\u5c42\u548c\u6df1\u5c42\u7684\u5904\u7406\uff0c\u4f18\u5316\u4e86LLMs\u7684\u63a8\u7406\u6548\u7387\u3002", "motivation": "KV\u7f13\u5b58\u7684\u5197\u4f59\u95ee\u9898\uff0c\u5c24\u5176\u662f\u5728\u6d45\u5c42\u5904\u7406\u4e0d\u8db3\u7684\u60c5\u51b5\u4e0b\uff0c\u9700\u8981\u4e00\u79cd\u66f4\u9ad8\u6548\u7684\u51cf\u5c11\u65b9\u6cd5\u3002", "method": "\u7ed3\u5408\u6ce8\u610f\u529b\u6743\u91cd\u9a71\u9010\uff08\u6df1\u5c42\uff09\u548c\u57fa\u4e8e\u76f8\u4f3c\u6027\u7684\u7801\u672c\u66ff\u6362\uff08\u6d45\u5c42\uff09\uff0c\u5e76\u89e3\u51b3GQA\u95ee\u9898\u3002", "result": "\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cSpindleKV\u76f8\u6bd4\u57fa\u7ebf\u65b9\u6cd5\u51cf\u5c11\u4e86KV\u7f13\u5b58\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u6a21\u578b\u6027\u80fd\u3002", "conclusion": "SpindleKV\u6709\u6548\u5e73\u8861\u4e86\u6d45\u5c42\u548c\u6df1\u5c42\u7684KV\u7f13\u5b58\u51cf\u5c11\uff0c\u63d0\u5347\u4e86LLMs\u7684\u63a8\u7406\u6548\u7387\u3002"}}
{"id": "2507.06466", "pdf": "https://arxiv.org/pdf/2507.06466", "abs": "https://arxiv.org/abs/2507.06466", "authors": ["Aaron Dharna", "Cong Lu", "Jeff Clune"], "title": "Foundation Model Self-Play: Open-Ended Strategy Innovation via Foundation Models", "categories": ["cs.LG", "cs.AI"], "comment": "67 pages, accepted to RLC 2025", "summary": "Multi-agent interactions have long fueled innovation, from natural\npredator-prey dynamics to the space race. Self-play (SP) algorithms try to\nharness these dynamics by pitting agents against ever-improving opponents,\nthereby creating an implicit curriculum toward learning high-quality solutions.\nHowever, SP often fails to produce diverse solutions and can get stuck in\nlocally optimal behaviors. We introduce Foundation-Model Self-Play (FMSP), a\nnew direction that leverages the code-generation capabilities and vast\nknowledge of foundation models (FMs) to overcome these challenges by leaping\nacross local optima in policy space. We propose a family of approaches: (1)\n\\textbf{Vanilla Foundation-Model Self-Play (vFMSP)} continually refines agent\npolicies via competitive self-play; (2) \\textbf{Novelty-Search Self-Play\n(NSSP)} builds a diverse population of strategies, ignoring performance; and\n(3) the most promising variant, \\textbf{Quality-Diveristy Self-Play (QDSP)},\ncreates a diverse set of high-quality policies by combining the diversity of\nNSSP and refinement of vFMSP. We evaluate FMSPs in Car Tag, a\ncontinuous-control pursuer-evader setting, and in Gandalf, a simple AI safety\nsimulation in which an attacker tries to jailbreak an LLM's defenses. In Car\nTag, FMSPs explore a wide variety of reinforcement learning, tree search, and\nheuristic-based methods, to name just a few. In terms of discovered policy\nquality, \\ouralgo and vFMSP surpass strong human-designed strategies. In\nGandalf, FMSPs can successfully automatically red-team an LLM, breaking through\nand jailbreaking six different, progressively stronger levels of defense.\nFurthermore, FMSPs can automatically proceed to patch the discovered\nvulnerabilities. Overall, FMSPs represent a promising new research frontier of\nimproving self-play with foundation models, opening fresh paths toward more\ncreative and open-ended strategy discovery", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aFMSP\u7684\u65b0\u65b9\u6cd5\uff0c\u5229\u7528\u57fa\u7840\u6a21\u578b\u7684\u4ee3\u7801\u751f\u6210\u80fd\u529b\u548c\u5e7f\u6cdb\u77e5\u8bc6\uff0c\u901a\u8fc7\u81ea\u6211\u5bf9\u5f08\u514b\u670d\u5c40\u90e8\u6700\u4f18\u95ee\u9898\uff0c\u5e76\u63a2\u7d22\u591a\u6837\u5316\u548c\u9ad8\u8d28\u91cf\u7684\u7b56\u7565\u3002", "motivation": "\u4f20\u7edf\u81ea\u6211\u5bf9\u5f08\uff08SP\uff09\u65b9\u6cd5\u5bb9\u6613\u9677\u5165\u5c40\u90e8\u6700\u4f18\u4e14\u7f3a\u4e4f\u591a\u6837\u6027\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u65b0\u65b9\u6cd5\u6765\u514b\u670d\u8fd9\u4e9b\u9650\u5236\u3002", "method": "\u63d0\u51fa\u4e86\u4e09\u79cdFMSP\u65b9\u6cd5\uff1avFMSP\uff08\u6301\u7eed\u4f18\u5316\u7b56\u7565\uff09\u3001NSSP\uff08\u63a2\u7d22\u591a\u6837\u6027\u7b56\u7565\uff09\u548cQDSP\uff08\u7ed3\u5408\u591a\u6837\u6027\u4e0e\u9ad8\u8d28\u91cf\u7b56\u7565\uff09\u3002", "result": "\u5728Car Tag\u548cGandalf\u5b9e\u9a8c\u4e2d\uff0cFMSP\u8868\u73b0\u51fa\u8272\uff0c\u8d85\u8d8a\u4e86\u4eba\u7c7b\u8bbe\u8ba1\u7684\u7b56\u7565\uff0c\u5e76\u80fd\u81ea\u52a8\u4fee\u8865\u6f0f\u6d1e\u3002", "conclusion": "FMSP\u4e3a\u81ea\u6211\u5bf9\u5f08\u63d0\u4f9b\u4e86\u65b0\u7684\u7814\u7a76\u65b9\u5411\uff0c\u6709\u671b\u5b9e\u73b0\u66f4\u5f00\u653e\u548c\u521b\u9020\u6027\u7684\u7b56\u7565\u53d1\u73b0\u3002"}}
{"id": "2507.06513", "pdf": "https://arxiv.org/pdf/2507.06513", "abs": "https://arxiv.org/abs/2507.06513", "authors": ["Yaoqi Huang", "Julie Stephany Berrio", "Mao Shan", "Stewart Worrall"], "title": "What Demands Attention in Urban Street Scenes? From Scene Understanding towards Road Safety: A Survey of Vision-driven Datasets and Studies", "categories": ["cs.CV"], "comment": "45 pages, 52 figures, 2 large tables (divided into 5), 73 datatsets,\n  35 tasks", "summary": "Advances in vision-based sensors and computer vision algorithms have\nsignificantly improved the analysis and understanding of traffic scenarios. To\nfacilitate the use of these improvements for road safety, this survey\nsystematically categorizes the critical elements that demand attention in\ntraffic scenarios and comprehensively analyzes available vision-driven tasks\nand datasets. Compared to existing surveys that focus on isolated domains, our\ntaxonomy categorizes attention-worthy traffic entities into two main groups\nthat are anomalies and normal but critical entities, integrating ten categories\nand twenty subclasses. It establishes connections between inherently related\nfields and provides a unified analytical framework. Our survey highlights the\nanalysis of 35 vision-driven tasks and comprehensive examinations and\nvisualizations of 73 available datasets based on the proposed taxonomy. The\ncross-domain investigation covers the pros and cons of each benchmark with the\naim of providing information on standards unification and resource\noptimization. Our article concludes with a systematic discussion of the\nexisting weaknesses, underlining the potential effects and promising solutions\nfrom various perspectives. The integrated taxonomy, comprehensive analysis, and\nrecapitulatory tables serve as valuable contributions to this rapidly evolving\nfield by providing researchers with a holistic overview, guiding strategic\nresource selection, and highlighting critical research gaps.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7efc\u8ff0\u4e86\u57fa\u4e8e\u89c6\u89c9\u7684\u4ea4\u901a\u573a\u666f\u5206\u6790\uff0c\u63d0\u51fa\u4e86\u4e00\u4e2a\u5206\u7c7b\u6cd5\u5c06\u4ea4\u901a\u5b9e\u4f53\u5206\u4e3a\u5f02\u5e38\u548c\u5173\u952e\u6b63\u5e38\u5b9e\u4f53\uff0c\u5206\u6790\u4e8635\u4e2a\u89c6\u89c9\u9a71\u52a8\u4efb\u52a1\u548c73\u4e2a\u6570\u636e\u96c6\uff0c\u5e76\u63a2\u8ba8\u4e86\u73b0\u6709\u5f31\u70b9\u4e0e\u6f5c\u5728\u89e3\u51b3\u65b9\u6848\u3002", "motivation": "\u5229\u7528\u89c6\u89c9\u4f20\u611f\u5668\u548c\u8ba1\u7b97\u673a\u89c6\u89c9\u7b97\u6cd5\u7684\u8fdb\u6b65\uff0c\u63d0\u5347\u4ea4\u901a\u5b89\u5168\uff0c\u901a\u8fc7\u7cfb\u7edf\u5206\u7c7b\u548c\u5168\u9762\u5206\u6790\u4e3a\u7814\u7a76\u8005\u63d0\u4f9b\u7edf\u4e00\u6846\u67b6\u548c\u8d44\u6e90\u4f18\u5316\u6307\u5bfc\u3002", "method": "\u63d0\u51fa\u5206\u7c7b\u6cd5\u5c06\u4ea4\u901a\u5b9e\u4f53\u5206\u4e3a\u5f02\u5e38\u548c\u5173\u952e\u6b63\u5e38\u5b9e\u4f53\uff0c\u6574\u540810\u4e2a\u7c7b\u522b\u548c20\u4e2a\u5b50\u7c7b\uff0c\u5206\u679035\u4e2a\u89c6\u89c9\u9a71\u52a8\u4efb\u52a1\u548c73\u4e2a\u6570\u636e\u96c6\u3002", "result": "\u5efa\u7acb\u4e86\u8de8\u9886\u57df\u7684\u7edf\u4e00\u5206\u6790\u6846\u67b6\uff0c\u603b\u7ed3\u4e86\u73b0\u6709\u6570\u636e\u96c6\u7684\u4f18\u7f3a\u70b9\uff0c\u5e76\u6307\u51fa\u4e86\u6807\u51c6\u548c\u8d44\u6e90\u4f18\u5316\u7684\u65b9\u5411\u3002", "conclusion": "\u8be5\u7efc\u8ff0\u4e3a\u5feb\u901f\u53d1\u5c55\u7684\u4ea4\u901a\u89c6\u89c9\u5206\u6790\u9886\u57df\u63d0\u4f9b\u4e86\u5168\u9762\u6982\u8ff0\uff0c\u6307\u5bfc\u8d44\u6e90\u9009\u62e9\u5e76\u7a81\u51fa\u4e86\u5173\u952e\u7814\u7a76\u7a7a\u767d\u3002"}}
{"id": "2507.06264", "pdf": "https://arxiv.org/pdf/2507.06264", "abs": "https://arxiv.org/abs/2507.06264", "authors": ["Weronika Hryniewska-Guzik", "Przemyslaw Biecek"], "title": "X-ray transferable polyrepresentation learning", "categories": ["eess.IV", "cs.AI", "cs.CV", "cs.LG"], "comment": "part of Weronika's PhD thesis", "summary": "The success of machine learning algorithms is inherently related to the\nextraction of meaningful features, as they play a pivotal role in the\nperformance of these algorithms. Central to this challenge is the quality of\ndata representation. However, the ability to generalize and extract these\nfeatures effectively from unseen datasets is also crucial. In light of this, we\nintroduce a novel concept: the polyrepresentation. Polyrepresentation\nintegrates multiple representations of the same modality extracted from\ndistinct sources, for example, vector embeddings from the Siamese Network,\nself-supervised models, and interpretable radiomic features. This approach\nyields better performance metrics compared to relying on a single\nrepresentation. Additionally, in the context of X-ray images, we demonstrate\nthe transferability of the created polyrepresentation to a smaller dataset,\nunderscoring its potential as a pragmatic and resource-efficient approach in\nvarious image-related solutions. It is worth noting that the concept of\npolyprepresentation on the example of medical data can also be applied to other\ndomains, showcasing its versatility and broad potential impact.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u201c\u591a\u8868\u5f81\u201d\uff08polyrepresentation\uff09\u7684\u65b0\u6982\u5ff5\uff0c\u901a\u8fc7\u6574\u5408\u6765\u81ea\u4e0d\u540c\u6765\u6e90\u7684\u540c\u4e00\u6a21\u6001\u7684\u591a\u79cd\u8868\u5f81\uff08\u5982Siamese Network\u7684\u5411\u91cf\u5d4c\u5165\u3001\u81ea\u76d1\u7763\u6a21\u578b\u548c\u53ef\u89e3\u91ca\u7684\u653e\u5c04\u7ec4\u5b66\u7279\u5f81\uff09\uff0c\u63d0\u5347\u4e86\u673a\u5668\u5b66\u4e60\u7b97\u6cd5\u7684\u6027\u80fd\u3002", "motivation": "\u673a\u5668\u5b66\u4e60\u7b97\u6cd5\u7684\u6210\u529f\u4f9d\u8d56\u4e8e\u6709\u610f\u4e49\u7684\u7279\u5f81\u63d0\u53d6\uff0c\u800c\u6570\u636e\u8868\u793a\u7684\u8d28\u91cf\u548c\u4ece\u672a\u89c1\u8fc7\u7684\u6570\u636e\u96c6\u4e2d\u6709\u6548\u63d0\u53d6\u7279\u5f81\u7684\u80fd\u529b\u662f\u5173\u952e\u6311\u6218\u3002", "method": "\u63d0\u51fa\u591a\u8868\u5f81\u65b9\u6cd5\uff0c\u6574\u5408\u540c\u4e00\u6a21\u6001\u7684\u591a\u79cd\u8868\u5f81\uff08\u5982Siamese Network\u3001\u81ea\u76d1\u7763\u6a21\u578b\u548c\u653e\u5c04\u7ec4\u5b66\u7279\u5f81\uff09\u3002", "result": "\u591a\u8868\u5f81\u65b9\u6cd5\u5728\u6027\u80fd\u6307\u6807\u4e0a\u4f18\u4e8e\u5355\u4e00\u8868\u5f81\uff0c\u5e76\u5728X\u5c04\u7ebf\u56fe\u50cf\u4e2d\u5c55\u793a\u4e86\u5176\u5728\u5c0f\u6570\u636e\u96c6\u4e0a\u7684\u53ef\u8fc1\u79fb\u6027\u3002", "conclusion": "\u591a\u8868\u5f81\u65b9\u6cd5\u5177\u6709\u5b9e\u7528\u6027\u548c\u8d44\u6e90\u6548\u7387\uff0c\u9002\u7528\u4e8e\u533b\u5b66\u6570\u636e\u53ca\u5176\u4ed6\u9886\u57df\uff0c\u5c55\u793a\u4e86\u5e7f\u6cdb\u7684\u6f5c\u5728\u5f71\u54cd\u3002"}}
{"id": "2507.06822", "pdf": "https://arxiv.org/pdf/2507.06822", "abs": "https://arxiv.org/abs/2507.06822", "authors": ["Wei Xu", "Yanchao Zhao", "Weichao Guo", "Xinjun Sheng"], "title": "Hierarchical Reinforcement Learning for Articulated Tool Manipulation with Multifingered Hand", "categories": ["cs.RO"], "comment": "Accepted by 2025 IEEE/RSJ International Conference on Intelligent\n  Robots and Systems (IROS 2025). copyright 2025 IEEE. Final version to appear\n  in IEEE Xplore", "summary": "Manipulating articulated tools, such as tweezers or scissors, has rarely been\nexplored in previous research. Unlike rigid tools, articulated tools change\ntheir shape dynamically, creating unique challenges for dexterous robotic\nhands. In this work, we present a hierarchical, goal-conditioned reinforcement\nlearning (GCRL) framework to improve the manipulation capabilities of\nanthropomorphic robotic hands using articulated tools. Our framework comprises\ntwo policy layers: (1) a low-level policy that enables the dexterous hand to\nmanipulate the tool into various configurations for objects of different sizes,\nand (2) a high-level policy that defines the tool's goal state and controls the\nrobotic arm for object-picking tasks. We employ an encoder, trained on\nsynthetic pointclouds, to estimate the tool's affordance states--specifically,\nhow different tool configurations (e.g., tweezer opening angles) enable\ngrasping of objects of varying sizes--from input point clouds, thereby enabling\nprecise tool manipulation. We also utilize a privilege-informed heuristic\npolicy to generate replay buffer, improving the training efficiency of the\nhigh-level policy. We validate our approach through real-world experiments,\nshowing that the robot can effectively manipulate a tweezer-like tool to grasp\nobjects of diverse shapes and sizes with a 70.8 % success rate. This study\nhighlights the potential of RL to advance dexterous robotic manipulation of\narticulated tools.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5206\u5c42\u76ee\u6807\u6761\u4ef6\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u7528\u4e8e\u63d0\u5347\u673a\u5668\u4eba\u624b\u64cd\u4f5c\u94f0\u63a5\u5de5\u5177\u7684\u80fd\u529b\uff0c\u5b9e\u9a8c\u6210\u529f\u7387\u4e3a70.8%\u3002", "motivation": "\u94f0\u63a5\u5de5\u5177\uff08\u5982\u954a\u5b50\u6216\u526a\u5200\uff09\u7684\u52a8\u6001\u5f62\u72b6\u53d8\u5316\u4e3a\u673a\u5668\u4eba\u64cd\u4f5c\u5e26\u6765\u72ec\u7279\u6311\u6218\uff0c\u6b64\u524d\u7814\u7a76\u8f83\u5c11\u63a2\u7d22\u3002", "method": "\u91c7\u7528\u5206\u5c42\u7b56\u7565\uff1a\u4f4e\u5c42\u7b56\u7565\u63a7\u5236\u5de5\u5177\u914d\u7f6e\uff0c\u9ad8\u5c42\u7b56\u7565\u5b9a\u4e49\u76ee\u6807\u72b6\u6001\u5e76\u63a7\u5236\u673a\u68b0\u81c2\uff1b\u4f7f\u7528\u7f16\u7801\u5668\u4f30\u8ba1\u5de5\u5177\u7684\u529f\u80fd\u72b6\u6001\uff0c\u5e76\u901a\u8fc7\u542f\u53d1\u5f0f\u7b56\u7565\u751f\u6210\u56de\u653e\u7f13\u51b2\u533a\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u673a\u5668\u4eba\u80fd\u6709\u6548\u64cd\u4f5c\u954a\u5b50\u7c7b\u5de5\u5177\u6293\u53d6\u4e0d\u540c\u5f62\u72b6\u548c\u5927\u5c0f\u7684\u7269\u4f53\uff0c\u6210\u529f\u7387\u4e3a70.8%\u3002", "conclusion": "\u8be5\u7814\u7a76\u5c55\u793a\u4e86\u5f3a\u5316\u5b66\u4e60\u5728\u63d0\u5347\u673a\u5668\u4eba\u64cd\u4f5c\u94f0\u63a5\u5de5\u5177\u80fd\u529b\u65b9\u9762\u7684\u6f5c\u529b\u3002"}}
{"id": "2507.06528", "pdf": "https://arxiv.org/pdf/2507.06528", "abs": "https://arxiv.org/abs/2507.06528", "authors": ["Huisheng Wang", "Zhuoshi Pan", "Hangjing Zhang", "Mingxiao Liu", "Hanqing Gao", "H. Vicky Zhao"], "title": "InvestAlign: Overcoming Data Scarcity in Aligning Large Language Models with Investor Decision-Making Processes under Herd Behavior", "categories": ["cs.CL", "cs.AI", "cs.ET", "cs.LG"], "comment": null, "summary": "Aligning Large Language Models (LLMs) with investor decision-making processes\nunder herd behavior is a critical challenge in behavioral finance, which\ngrapples with a fundamental limitation: the scarcity of real-user data needed\nfor Supervised Fine-Tuning (SFT). While SFT can bridge the gap between LLM\noutputs and human behavioral patterns, its reliance on massive authentic data\nimposes substantial collection costs and privacy risks. We propose InvestAlign,\na novel framework that constructs high-quality SFT datasets by leveraging\ntheoretical solutions to similar and simple optimal investment problems rather\nthan complex scenarios. Our theoretical analysis demonstrates that training\nLLMs with InvestAlign-generated data achieves faster parameter convergence than\nusing real-user data, suggesting superior learning efficiency. Furthermore, we\ndevelop InvestAgent, an LLM agent fine-tuned with InvestAlign, which\ndemonstrates significantly closer alignment to real-user data than pre-SFT\nmodels in both simple and complex investment problems. This highlights our\nproposed InvestAlign as a promising approach with the potential to address\ncomplex optimal investment problems and align LLMs with investor\ndecision-making processes under herd behavior. Our code is publicly available\nat https://github.com/thu-social-network-research-group/InvestAlign.", "AI": {"tldr": "\u63d0\u51faInvestAlign\u6846\u67b6\uff0c\u901a\u8fc7\u7406\u8bba\u89e3\u6784\u5efa\u9ad8\u8d28\u91cfSFT\u6570\u636e\u96c6\uff0c\u89e3\u51b3LLM\u4e0e\u6295\u8d44\u8005\u4ece\u4f17\u884c\u4e3a\u5bf9\u9f50\u95ee\u9898\uff0c\u51cf\u5c11\u5bf9\u771f\u5b9e\u7528\u6237\u6570\u636e\u7684\u4f9d\u8d56\u3002", "motivation": "\u884c\u4e3a\u91d1\u878d\u4e2d\uff0cLLM\u4e0e\u6295\u8d44\u8005\u4ece\u4f17\u884c\u4e3a\u5bf9\u9f50\u9700\u5927\u91cf\u771f\u5b9e\u6570\u636e\uff0c\u4f46\u6570\u636e\u7a00\u7f3a\u4e14\u6210\u672c\u9ad8\u3002", "method": "\u5229\u7528\u7406\u8bba\u89e3\u6784\u5efaSFT\u6570\u636e\u96c6\uff0c\u8bad\u7ec3LLM\uff08InvestAgent\uff09\uff0c\u4f18\u4e8e\u771f\u5b9e\u6570\u636e\u8bad\u7ec3\u3002", "result": "InvestAlign\u751f\u6210\u7684\u6570\u636e\u4f7fLLM\u53c2\u6570\u6536\u655b\u66f4\u5feb\uff0cInvestAgent\u66f4\u8d34\u8fd1\u771f\u5b9e\u7528\u6237\u884c\u4e3a\u3002", "conclusion": "InvestAlign\u4e3a\u590d\u6742\u6295\u8d44\u95ee\u9898\u63d0\u4f9b\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u51cf\u5c11\u6570\u636e\u4f9d\u8d56\uff0c\u63d0\u5347LLM\u5bf9\u9f50\u80fd\u529b\u3002"}}
{"id": "2507.06469", "pdf": "https://arxiv.org/pdf/2507.06469", "abs": "https://arxiv.org/abs/2507.06469", "authors": ["Yudan Song", "Yuecen Wei", "Yuhang Lu", "Qingyun Sun", "Minglai Shao", "Li-e Wang", "Chunming Hu", "Xianxian Li", "Xingcheng Fu"], "title": "Mitigating Message Imbalance in Fraud Detection with Dual-View Graph Representation Learning", "categories": ["cs.LG", "cs.SI"], "comment": null, "summary": "Graph representation learning has become a mainstream method for fraud\ndetection due to its strong expressive power, which focuses on enhancing node\nrepresentations through improved neighborhood knowledge capture. However, the\nfocus on local interactions leads to imbalanced transmission of global\ntopological information and increased risk of node-specific information being\noverwhelmed during aggregation due to the imbalance between fraud and benign\nnodes. In this paper, we first summarize the impact of topology and class\nimbalance on downstream tasks in GNN-based fraud detection, as the problem of\nimbalanced supervisory messages is caused by fraudsters' topological behavior\nobfuscation and identity feature concealment. Based on statistical validation,\nwe propose a novel dual-view graph representation learning method to mitigate\nMessage imbalance in Fraud Detection(MimbFD). Specifically, we design a\ntopological message reachability module for high-quality node representation\nlearning to penetrate fraudsters' camouflage and alleviate insufficient\npropagation. Then, we introduce a local confounding debiasing module to adjust\nnode representations, enhancing the stable association between node\nrepresentations and labels to balance the influence of different classes.\nFinally, we conducted experiments on three public fraud datasets, and the\nresults demonstrate that MimbFD exhibits outstanding performance in fraud\ndetection.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u53cc\u89c6\u56fe\u56fe\u8868\u793a\u5b66\u4e60\u65b9\u6cd5\uff08MimbFD\uff09\uff0c\u7528\u4e8e\u89e3\u51b3\u57fa\u4e8eGNN\u7684\u6b3a\u8bc8\u68c0\u6d4b\u4e2d\u62d3\u6251\u548c\u7c7b\u522b\u4e0d\u5e73\u8861\u95ee\u9898\uff0c\u901a\u8fc7\u6539\u8fdb\u8282\u70b9\u8868\u793a\u548c\u5c40\u90e8\u6df7\u6dc6\u53bb\u504f\u6a21\u5757\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u56fe\u8868\u793a\u5b66\u4e60\u65b9\u6cd5\u5728\u6b3a\u8bc8\u68c0\u6d4b\u4e2d\u56e0\u5c40\u90e8\u4ea4\u4e92\u548c\u7c7b\u522b\u4e0d\u5e73\u8861\u5bfc\u81f4\u5168\u5c40\u62d3\u6251\u4fe1\u606f\u4f20\u9012\u4e0d\u5747\uff0c\u8282\u70b9\u7279\u5b9a\u4fe1\u606f\u6613\u88ab\u6df9\u6ca1\u3002", "method": "\u8bbe\u8ba1\u4e86\u62d3\u6251\u6d88\u606f\u53ef\u8fbe\u6027\u6a21\u5757\u4ee5\u7a7f\u900f\u6b3a\u8bc8\u8005\u4f2a\u88c5\uff0c\u5e76\u5f15\u5165\u5c40\u90e8\u6df7\u6dc6\u53bb\u504f\u6a21\u5757\u5e73\u8861\u4e0d\u540c\u7c7b\u522b\u7684\u5f71\u54cd\u3002", "result": "\u5728\u4e09\u4e2a\u516c\u5f00\u6b3a\u8bc8\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cMimbFD\u5728\u6b3a\u8bc8\u68c0\u6d4b\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "MimbFD\u901a\u8fc7\u53cc\u89c6\u56fe\u5b66\u4e60\u6709\u6548\u7f13\u89e3\u4e86\u6d88\u606f\u4e0d\u5e73\u8861\u95ee\u9898\uff0c\u63d0\u5347\u4e86\u6b3a\u8bc8\u68c0\u6d4b\u6027\u80fd\u3002"}}
{"id": "2507.06523", "pdf": "https://arxiv.org/pdf/2507.06523", "abs": "https://arxiv.org/abs/2507.06523", "authors": ["Liqiang Jing", "Viet Lai", "Seunghyun Yoon", "Trung Bui", "Xinya Du"], "title": "FIFA: Unified Faithfulness Evaluation Framework for Text-to-Video and Video-to-Text Generation", "categories": ["cs.CV", "cs.CL", "cs.GR"], "comment": null, "summary": "Video Multimodal Large Language Models (VideoMLLMs) have achieved remarkable\nprogress in both Video-to-Text and Text-to-Video tasks. However, they often\nsuffer fro hallucinations, generating content that contradicts the visual\ninput. Existing evaluation methods are limited to one task (e.g., V2T) and also\nfail to assess hallucinations in open-ended, free-form responses. To address\nthis gap, we propose FIFA, a unified FaIthFulness evAluation framework that\nextracts comprehensive descriptive facts, models their semantic dependencies\nvia a Spatio-Temporal Semantic Dependency Graph, and verifies them using\nVideoQA models. We further introduce Post-Correction, a tool-based correction\nframework that revises hallucinated content. Extensive experiments demonstrate\nthat FIFA aligns more closely with human judgment than existing evaluation\nmethods, and that Post-Correction effectively improves factual consistency in\nboth text and video generation.", "AI": {"tldr": "FIFA\u662f\u4e00\u4e2a\u7edf\u4e00\u7684\u89c6\u9891\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08VideoMLLMs\uff09\u5fe0\u5b9e\u6027\u8bc4\u4f30\u6846\u67b6\uff0c\u901a\u8fc7\u63d0\u53d6\u63cf\u8ff0\u6027\u4e8b\u5b9e\u3001\u5efa\u6a21\u8bed\u4e49\u4f9d\u8d56\u5173\u7cfb\u5e76\u9a8c\u8bc1\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u8bc4\u4f30\u65b9\u6cd5\u5728\u5f00\u653e\u56de\u7b54\u4e2d\u7684\u5e7b\u89c9\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u8bc4\u4f30\u65b9\u6cd5\u5c40\u9650\u4e8e\u5355\u4e00\u4efb\u52a1\u4e14\u65e0\u6cd5\u8bc4\u4f30\u5f00\u653e\u56de\u7b54\u4e2d\u7684\u5e7b\u89c9\u95ee\u9898\uff0c\u5bfc\u81f4\u6a21\u578b\u751f\u6210\u5185\u5bb9\u4e0e\u89c6\u89c9\u8f93\u5165\u4e0d\u7b26\u3002", "method": "\u63d0\u51faFIFA\u6846\u67b6\uff0c\u63d0\u53d6\u63cf\u8ff0\u6027\u4e8b\u5b9e\uff0c\u6784\u5efa\u65f6\u7a7a\u8bed\u4e49\u4f9d\u8d56\u56fe\uff0c\u5e76\u901a\u8fc7VideoQA\u6a21\u578b\u9a8c\u8bc1\uff1b\u5f15\u5165\u57fa\u4e8e\u5de5\u5177\u7684\u540e\u6821\u6b63\u6846\u67b6\u4fee\u6b63\u5e7b\u89c9\u5185\u5bb9\u3002", "result": "FIFA\u6bd4\u73b0\u6709\u8bc4\u4f30\u65b9\u6cd5\u66f4\u63a5\u8fd1\u4eba\u7c7b\u5224\u65ad\uff0c\u540e\u6821\u6b63\u663e\u8457\u63d0\u9ad8\u4e86\u6587\u672c\u548c\u89c6\u9891\u751f\u6210\u7684\u4e8b\u5b9e\u4e00\u81f4\u6027\u3002", "conclusion": "FIFA\u548c\u540e\u6821\u6b63\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86VideoMLLMs\u4e2d\u7684\u5e7b\u89c9\u95ee\u9898\uff0c\u63d0\u5347\u4e86\u751f\u6210\u5185\u5bb9\u7684\u5fe0\u5b9e\u6027\u3002"}}
{"id": "2507.06824", "pdf": "https://arxiv.org/pdf/2507.06824", "abs": "https://arxiv.org/abs/2507.06824", "authors": ["Gabriel Arslan Waltersson", "Yiannis Karayiannidis"], "title": "Friction Estimation for In-Hand Planar Motion", "categories": ["cs.RO"], "comment": null, "summary": "This paper presents a method for online estimation of contact properties\nduring in-hand sliding manipulation with a parallel gripper. We estimate the\nstatic and Coulomb friction as well as the contact radius from tactile\nmeasurements of contact forces and sliding velocities. The method is validated\nin both simulation and real-world experiments. Furthermore, we propose a\nheuristic to deal with fast slip-stick dynamics which can adversely affect the\nestimation.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5728\u7ebf\u4f30\u8ba1\u5e73\u884c\u5939\u6301\u5668\u6ed1\u52a8\u64cd\u4f5c\u4e2d\u63a5\u89e6\u7279\u6027\u7684\u65b9\u6cd5\uff0c\u5305\u62ec\u9759\u6469\u64e6\u3001\u5e93\u4ed1\u6469\u64e6\u548c\u63a5\u89e6\u534a\u5f84\uff0c\u5e76\u901a\u8fc7\u4eff\u771f\u548c\u5b9e\u9a8c\u9a8c\u8bc1\u3002", "motivation": "\u7814\u7a76\u6ed1\u52a8\u64cd\u4f5c\u4e2d\u63a5\u89e6\u7279\u6027\u7684\u5b9e\u65f6\u4f30\u8ba1\u95ee\u9898\uff0c\u4ee5\u63d0\u5347\u5939\u6301\u5668\u7684\u64cd\u4f5c\u7cbe\u5ea6\u548c\u9002\u5e94\u6027\u3002", "method": "\u57fa\u4e8e\u89e6\u89c9\u6d4b\u91cf\u7684\u63a5\u89e6\u529b\u548c\u6ed1\u52a8\u901f\u5ea6\uff0c\u4f30\u8ba1\u9759\u6469\u64e6\u3001\u5e93\u4ed1\u6469\u64e6\u548c\u63a5\u89e6\u534a\u5f84\uff0c\u5e76\u63d0\u51fa\u542f\u53d1\u5f0f\u65b9\u6cd5\u5904\u7406\u5feb\u901f\u6ed1\u79fb-\u7c98\u9644\u52a8\u6001\u3002", "result": "\u65b9\u6cd5\u5728\u4eff\u771f\u548c\u5b9e\u9645\u5b9e\u9a8c\u4e2d\u5747\u5f97\u5230\u9a8c\u8bc1\uff0c\u80fd\u591f\u6709\u6548\u4f30\u8ba1\u63a5\u89e6\u7279\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u6ed1\u52a8\u64cd\u4f5c\u4e2d\u7684\u63a5\u89e6\u7279\u6027\u4f30\u8ba1\u63d0\u4f9b\u4e86\u5b9e\u7528\u89e3\u51b3\u65b9\u6848\uff0c\u5e76\u89e3\u51b3\u4e86\u5feb\u901f\u6ed1\u79fb-\u7c98\u9644\u52a8\u6001\u7684\u5f71\u54cd\u3002"}}
{"id": "2507.06539", "pdf": "https://arxiv.org/pdf/2507.06539", "abs": "https://arxiv.org/abs/2507.06539", "authors": ["Yunyang Cao", "Yanjun Li", "Silong Dai"], "title": "Large Language Model for Extracting Complex Contract Information in Industrial Scenes", "categories": ["cs.CL"], "comment": null, "summary": "This paper proposes a high-quality dataset construction method for complex\ncontract information extraction tasks in industrial scenarios and fine-tunes a\nlarge language model based on this dataset. Firstly, cluster analysis is\nperformed on industrial contract texts, and GPT-4 and GPT-3.5 are used to\nextract key information from the original contract data, obtaining high-quality\ndata annotations. Secondly, data augmentation is achieved by constructing new\ntexts, and GPT-3.5 generates unstructured contract texts from randomly combined\nkeywords, improving model robustness. Finally, the large language model is\nfine-tuned based on the high-quality dataset. Experimental results show that\nthe model achieves excellent overall performance while ensuring high field\nrecall and precision and considering parsing efficiency. LoRA, data balancing,\nand data augmentation effectively enhance model accuracy and robustness. The\nproposed method provides a novel and efficient solution for industrial contract\ninformation extraction tasks.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5de5\u4e1a\u573a\u666f\u4e0b\u590d\u6742\u5408\u540c\u4fe1\u606f\u63d0\u53d6\u7684\u9ad8\u8d28\u91cf\u6570\u636e\u96c6\u6784\u5efa\u65b9\u6cd5\uff0c\u5e76\u57fa\u4e8e\u6b64\u5fae\u8c03\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u5de5\u4e1a\u5408\u540c\u4fe1\u606f\u63d0\u53d6\u4efb\u52a1\u4e2d\u9ad8\u8d28\u91cf\u6570\u636e\u7a00\u7f3a\u548c\u6a21\u578b\u6027\u80fd\u4e0d\u8db3\u7684\u95ee\u9898\u3002", "method": "\u901a\u8fc7\u805a\u7c7b\u5206\u6790\u548cGPT-4/GPT-3.5\u63d0\u53d6\u5173\u952e\u4fe1\u606f\uff0c\u6570\u636e\u589e\u5f3a\u751f\u6210\u65b0\u6587\u672c\uff0c\u5e76\u5fae\u8c03\u5927\u8bed\u8a00\u6a21\u578b\u3002", "result": "\u6a21\u578b\u5728\u53ec\u56de\u7387\u3001\u7cbe\u786e\u5ea6\u548c\u89e3\u6790\u6548\u7387\u4e0a\u8868\u73b0\u4f18\u5f02\uff0cLoRA\u3001\u6570\u636e\u5e73\u8861\u548c\u6570\u636e\u589e\u5f3a\u63d0\u5347\u4e86\u51c6\u786e\u6027\u548c\u9c81\u68d2\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u5de5\u4e1a\u5408\u540c\u4fe1\u606f\u63d0\u53d6\u63d0\u4f9b\u4e86\u9ad8\u6548\u65b0\u9896\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.06482", "pdf": "https://arxiv.org/pdf/2507.06482", "abs": "https://arxiv.org/abs/2507.06482", "authors": ["Huan Wang", "Haoran Li", "Huaming Chen", "Jun Yan", "Jiahua Shi", "Jun Shen"], "title": "FedDifRC: Unlocking the Potential of Text-to-Image Diffusion Models in Heterogeneous Federated Learning", "categories": ["cs.LG"], "comment": "19 Pages, ICCV 2025", "summary": "Federated learning aims at training models collaboratively across\nparticipants while protecting privacy. However, one major challenge for this\nparadigm is the data heterogeneity issue, where biased data preferences across\nmultiple clients, harming the model's convergence and performance. In this\npaper, we first introduce powerful diffusion models into the federated learning\nparadigm and show that diffusion representations are effective steers during\nfederated training. To explore the possibility of using diffusion\nrepresentations in handling data heterogeneity, we propose a novel\ndiffusion-inspired Federated paradigm with Diffusion Representation\nCollaboration, termed FedDifRC, leveraging meaningful guidance of diffusion\nmodels to mitigate data heterogeneity. The key idea is to construct text-driven\ndiffusion contrasting and noise-driven diffusion regularization, aiming to\nprovide abundant class-related semantic information and consistent convergence\nsignals. On the one hand, we exploit the conditional feedback from the\ndiffusion model for different text prompts to build a text-driven contrastive\nlearning strategy. On the other hand, we introduce a noise-driven consistency\nregularization to align local instances with diffusion denoising\nrepresentations, constraining the optimization region in the feature space. In\naddition, FedDifRC can be extended to a self-supervised scheme without relying\non any labeled data. We also provide a theoretical analysis for FedDifRC to\nensure convergence under non-convex objectives. The experiments on different\nscenarios validate the effectiveness of FedDifRC and the efficiency of crucial\ncomponents.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aFedDifRC\u7684\u65b0\u65b9\u6cd5\uff0c\u5229\u7528\u6269\u6563\u6a21\u578b\u89e3\u51b3\u8054\u90a6\u5b66\u4e60\u4e2d\u7684\u6570\u636e\u5f02\u6784\u6027\u95ee\u9898\uff0c\u901a\u8fc7\u6587\u672c\u9a71\u52a8\u7684\u5bf9\u6bd4\u5b66\u4e60\u548c\u566a\u58f0\u9a71\u52a8\u7684\u4e00\u81f4\u6027\u6b63\u5219\u5316\u63d0\u5347\u6a21\u578b\u6027\u80fd\u3002", "motivation": "\u8054\u90a6\u5b66\u4e60\u4e2d\u6570\u636e\u5f02\u6784\u6027\u5bfc\u81f4\u6a21\u578b\u6536\u655b\u548c\u6027\u80fd\u95ee\u9898\uff0c\u9700\u8981\u4e00\u79cd\u65b0\u65b9\u6cd5\u6765\u89e3\u51b3\u3002", "method": "\u63d0\u51faFedDifRC\uff0c\u7ed3\u5408\u6269\u6563\u6a21\u578b\u7684\u8868\u793a\u80fd\u529b\uff0c\u901a\u8fc7\u6587\u672c\u9a71\u52a8\u7684\u5bf9\u6bd4\u5b66\u4e60\u548c\u566a\u58f0\u9a71\u52a8\u7684\u4e00\u81f4\u6027\u6b63\u5219\u5316\u5904\u7406\u6570\u636e\u5f02\u6784\u6027\u3002", "result": "\u5b9e\u9a8c\u9a8c\u8bc1\u4e86FedDifRC\u7684\u6709\u6548\u6027\uff0c\u5e76\u5728\u4e0d\u540c\u573a\u666f\u4e0b\u5c55\u793a\u4e86\u5176\u5173\u952e\u7ec4\u4ef6\u7684\u6548\u7387\u3002", "conclusion": "FedDifRC\u901a\u8fc7\u6269\u6563\u6a21\u578b\u7684\u6307\u5bfc\u6210\u529f\u7f13\u89e3\u4e86\u6570\u636e\u5f02\u6784\u6027\u95ee\u9898\uff0c\u5e76\u63d0\u4f9b\u4e86\u7406\u8bba\u4fdd\u8bc1\u3002"}}
{"id": "2507.06526", "pdf": "https://arxiv.org/pdf/2507.06526", "abs": "https://arxiv.org/abs/2507.06526", "authors": ["Chaoshuo Zhang", "Chenhao Lin", "Zhengyu Zhao", "Le Yang", "Qian Wang", "Chao Shen"], "title": "Concept Unlearning by Modeling Key Steps of Diffusion Process", "categories": ["cs.CV"], "comment": null, "summary": "Text-to-image diffusion models (T2I DMs), represented by Stable Diffusion,\nwhich generate highly realistic images based on textual input, have been widely\nused. However, their misuse poses serious security risks. While existing\nconcept unlearning methods aim to mitigate these risks, they struggle to\nbalance unlearning effectiveness with generative retainability.To overcome this\nlimitation, we innovatively propose the Key Step Concept Unlearning (KSCU)\nmethod, which ingeniously capitalizes on the unique stepwise sampling\ncharacteristic inherent in diffusion models during the image generation\nprocess. Unlike conventional approaches that treat all denoising steps equally,\nKSCU strategically focuses on pivotal steps with the most influence over the\nfinal outcome by dividing key steps for different concept unlearning tasks and\nfine-tuning the model only at those steps. This targeted approach reduces the\nnumber of parameter updates needed for effective unlearning, while maximizing\nthe retention of the model's generative capabilities.Through extensive\nbenchmark experiments, we demonstrate that KSCU effectively prevents T2I DMs\nfrom generating undesirable images while better retaining the model's\ngenerative capabilities.Our code will be released.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aKSCU\u7684\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u9488\u5bf9\u6269\u6563\u6a21\u578b\u7684\u5173\u952e\u6b65\u9aa4\u8fdb\u884c\u6982\u5ff5\u9057\u5fd8\uff0c\u6709\u6548\u5e73\u8861\u4e86\u9057\u5fd8\u6548\u679c\u4e0e\u751f\u6210\u80fd\u529b\u7684\u4fdd\u7559\u3002", "motivation": "\u73b0\u6709\u7684\u6982\u5ff5\u9057\u5fd8\u65b9\u6cd5\u96be\u4ee5\u5e73\u8861\u9057\u5fd8\u6548\u679c\u4e0e\u751f\u6210\u80fd\u529b\u7684\u4fdd\u7559\uff0c\u5bfc\u81f4\u6587\u672c\u5230\u56fe\u50cf\u6269\u6563\u6a21\u578b\uff08T2I DMs\uff09\u7684\u6ee5\u7528\u98ce\u9669\u3002", "method": "KSCU\u65b9\u6cd5\u5229\u7528\u6269\u6563\u6a21\u578b\u7684\u9010\u6b65\u91c7\u6837\u7279\u6027\uff0c\u4e13\u6ce8\u4e8e\u5bf9\u6700\u7ec8\u7ed3\u679c\u5f71\u54cd\u6700\u5927\u7684\u5173\u952e\u6b65\u9aa4\uff0c\u4ec5\u5728\u8fd9\u4e9b\u6b65\u9aa4\u4e0a\u5fae\u8c03\u6a21\u578b\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cKSCU\u80fd\u6709\u6548\u9632\u6b62\u751f\u6210\u4e0d\u826f\u56fe\u50cf\uff0c\u540c\u65f6\u66f4\u597d\u5730\u4fdd\u7559\u6a21\u578b\u7684\u751f\u6210\u80fd\u529b\u3002", "conclusion": "KSCU\u901a\u8fc7\u4f18\u5316\u6b65\u9aa4\u9009\u62e9\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6982\u5ff5\u9057\u5fd8\u7684\u6548\u7387\u548c\u6548\u679c\u3002"}}
{"id": "2507.06266", "pdf": "https://arxiv.org/pdf/2507.06266", "abs": "https://arxiv.org/abs/2507.06266", "authors": ["Tingyu Yuan", "Xi Zhang", "Xuanjing Chen"], "title": "Machine Learning based Enterprise Financial Audit Framework and High Risk Identification", "categories": ["q-fin.RM", "cs.AI", "cs.LG", "stat.AP"], "comment": null, "summary": "In the face of global economic uncertainty, financial auditing has become\nessential for regulatory compliance and risk mitigation. Traditional manual\nauditing methods are increasingly limited by large data volumes, complex\nbusiness structures, and evolving fraud tactics. This study proposes an\nAI-driven framework for enterprise financial audits and high-risk\nidentification, leveraging machine learning to improve efficiency and accuracy.\nUsing a dataset from the Big Four accounting firms (EY, PwC, Deloitte, KPMG)\nfrom 2020 to 2025, the research examines trends in risk assessment, compliance\nviolations, and fraud detection. The dataset includes key indicators such as\naudit project counts, high-risk cases, fraud instances, compliance breaches,\nemployee workload, and client satisfaction, capturing both audit behaviors and\nAI's impact on operations. To build a robust risk prediction model, three\nalgorithms - Support Vector Machine (SVM), Random Forest (RF), and K-Nearest\nNeighbors (KNN) - are evaluated. SVM uses hyperplane optimization for complex\nclassification, RF combines decision trees to manage high-dimensional,\nnonlinear data with resistance to overfitting, and KNN applies distance-based\nlearning for flexible performance. Through hierarchical K-fold cross-validation\nand evaluation using F1-score, accuracy, and recall, Random Forest achieves the\nbest performance, with an F1-score of 0.9012, excelling in identifying fraud\nand compliance anomalies. Feature importance analysis reveals audit frequency,\npast violations, employee workload, and client ratings as key predictors. The\nstudy recommends adopting Random Forest as a core model, enhancing features via\nengineering, and implementing real-time risk monitoring. This research\ncontributes valuable insights into using machine learning for intelligent\nauditing and risk management in modern enterprises.", "AI": {"tldr": "\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eAI\u7684\u4f01\u4e1a\u8d22\u52a1\u5ba1\u8ba1\u6846\u67b6\uff0c\u901a\u8fc7\u673a\u5668\u5b66\u4e60\u63d0\u9ad8\u6548\u7387\u548c\u51c6\u786e\u6027\uff0c\u968f\u673a\u68ee\u6797\u7b97\u6cd5\u8868\u73b0\u6700\u4f73\u3002", "motivation": "\u5168\u7403\u7ecf\u6d4e\u4e0d\u786e\u5b9a\u6027\u4e0b\uff0c\u4f20\u7edf\u5ba1\u8ba1\u65b9\u6cd5\u53d7\u9650\u4e8e\u5927\u6570\u636e\u91cf\u548c\u590d\u6742\u4e1a\u52a1\u7ed3\u6784\uff0c\u9700AI\u6539\u8fdb\u3002", "method": "\u4f7f\u7528SVM\u3001RF\u548cKNN\u7b97\u6cd5\u5206\u6790Big Four\u6570\u636e\uff0c\u8bc4\u4f30\u98ce\u9669\u9884\u6d4b\u6a21\u578b\u3002", "result": "\u968f\u673a\u68ee\u6797F1-score\u8fbe0.9012\uff0c\u4f18\u4e8e\u5176\u4ed6\u7b97\u6cd5\uff0c\u5173\u952e\u9884\u6d4b\u56e0\u7d20\u5305\u62ec\u5ba1\u8ba1\u9891\u7387\u548c\u5ba2\u6237\u8bc4\u5206\u3002", "conclusion": "\u63a8\u8350\u91c7\u7528\u968f\u673a\u68ee\u6797\u6a21\u578b\uff0c\u7ed3\u5408\u7279\u5f81\u5de5\u7a0b\u548c\u5b9e\u65f6\u76d1\u63a7\uff0c\u63a8\u52a8\u667a\u80fd\u5ba1\u8ba1\u53d1\u5c55\u3002"}}
{"id": "2507.06884", "pdf": "https://arxiv.org/pdf/2507.06884", "abs": "https://arxiv.org/abs/2507.06884", "authors": ["Dong Bi", "Yongqi Zhao", "Zhengguo Gu", "Tomislav Mihalj", "Jia Hu", "Arno Eichberger"], "title": "Toward a Full-Stack Co-Simulation Platform for Testing of Automated Driving Systems", "categories": ["cs.RO"], "comment": "IEEE International Conference on Intelligent Transportation Systems\n  (ITSC) 2025", "summary": "Virtual testing has emerged as an effective approach to accelerate the\ndeployment of automated driving systems. Nevertheless, existing simulation\ntoolchains encounter difficulties in integrating rapid, automated scenario\ngeneration with simulation environments supporting advanced automated driving\ncapabilities. To address this limitation, a full-stack toolchain is presented,\nenabling automatic scenario generation from real-world datasets and efficient\nvalidation through a co-simulation platform based on CarMaker, ROS, and Apollo.\nThe simulation results demonstrate the effectiveness of the proposed toolchain.\nA demonstration video showcasing the toolchain is available at the provided\nlink: https://youtu.be/taJw_-CmSiY.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5168\u6808\u5de5\u5177\u94fe\uff0c\u7528\u4e8e\u4ece\u771f\u5b9e\u6570\u636e\u81ea\u52a8\u751f\u6210\u573a\u666f\u5e76\u901a\u8fc7\u57fa\u4e8eCarMaker\u3001ROS\u548cApollo\u7684\u534f\u540c\u4eff\u771f\u5e73\u53f0\u8fdb\u884c\u9ad8\u6548\u9a8c\u8bc1\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u4eff\u771f\u5de5\u5177\u94fe\u5728\u5feb\u901f\u81ea\u52a8\u573a\u666f\u751f\u6210\u4e0e\u652f\u6301\u9ad8\u7ea7\u81ea\u52a8\u9a7e\u9a76\u80fd\u529b\u7684\u4eff\u771f\u73af\u5883\u96c6\u6210\u65b9\u9762\u7684\u56f0\u96be\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u79cd\u5168\u6808\u5de5\u5177\u94fe\uff0c\u7ed3\u5408CarMaker\u3001ROS\u548cApollo\uff0c\u5b9e\u73b0\u81ea\u52a8\u573a\u666f\u751f\u6210\u548c\u534f\u540c\u4eff\u771f\u9a8c\u8bc1\u3002", "result": "\u4eff\u771f\u7ed3\u679c\u8bc1\u660e\u4e86\u8be5\u5de5\u5177\u94fe\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u8be5\u5de5\u5177\u94fe\u4e3a\u52a0\u901f\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u7684\u90e8\u7f72\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u7684\u865a\u62df\u6d4b\u8bd5\u65b9\u6cd5\u3002"}}
{"id": "2507.06565", "pdf": "https://arxiv.org/pdf/2507.06565", "abs": "https://arxiv.org/abs/2507.06565", "authors": ["Juan B. Guti\u00e9rrez"], "title": "The Flaws of Others: An LLM-driven Framework for Scientific Knowledge Production", "categories": ["cs.CL", "cs.LG", "68T01, 60J10, 91D30, 05C82, 68T50, 68W20, 94A15", "I.2.7; I.2.11; G.3"], "comment": "27 pages, 3 figures, 4 tables, 1 algorithm, 28 references", "summary": "Large-language models turn writing into a live exchange between humans and\nsoftware. We capture this new medium with a discursive-network model that\ntreats people and LLMs as equal nodes and tracks how their statements\ncirculate. Broadening the focus from isolated hallucinations, we define\ninvalidation (any factual, logical, or structural breach) and show it follows\nfour hazards: drift from truth, self-repair, fresh fabrication, and external\ndetection. A general mathematical model of discursive networks is developed to\nprovide valuable insights: A network governed only by drift and self-repair\nstabilizes at a modest error rate; adding fabrication reproduces the high rates\nseen in current LLMs. Giving each false claim even a small chance of peer\nreview shifts the system to a truth-dominant state. We operationalize peer\nreview with the open-source \\emph{Flaws-of-Others (FOO) algorithm}: a\nconfigurable loop in which any set of agents critique one another while a\nharmoniser merges their verdicts. The takeaway is practical and cultural:\nreliability in this new medium comes not from perfecting single models but from\nwiring imperfect ones into networks that keep each other honest.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u8ba8\u8bba\u7f51\u7edc\u7684\u6a21\u578b\uff0c\u5c06\u4eba\u7c7b\u4e0e\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5e73\u7b49\u89c6\u4e3a\u8282\u70b9\uff0c\u5206\u6790\u5176\u4ea4\u4e92\u4e2d\u7684\u65e0\u6548\u6027\uff08\u5982\u4e8b\u5b9e\u3001\u903b\u8f91\u6216\u7ed3\u6784\u9519\u8bef\uff09\uff0c\u5e76\u63ed\u793a\u4e86\u56db\u79cd\u5371\u5bb3\u3002\u901a\u8fc7\u6570\u5b66\u6a21\u578b\u53d1\u73b0\uff0c\u7f51\u7edc\u5728\u6f02\u79fb\u548c\u81ea\u6211\u4fee\u590d\u4e0b\u8d8b\u4e8e\u7a33\u5b9a\uff0c\u800c\u52a0\u5165\u4f2a\u9020\u4f1a\u91cd\u73b0\u5f53\u524dLLM\u7684\u9ad8\u9519\u8bef\u7387\u3002\u5f15\u5165\u540c\u884c\u8bc4\u5ba1\u53ef\u663e\u8457\u63d0\u5347\u771f\u5b9e\u6027\uff0c\u5e76\u901a\u8fc7\u5f00\u6e90\u7b97\u6cd5FOO\u5b9e\u73b0\u3002", "motivation": "\u7814\u7a76\u4eba\u7c7b\u4e0eLLM\u4ea4\u4e92\u4e2d\u7684\u65e0\u6548\u6027\u95ee\u9898\uff0c\u63a2\u7d22\u5982\u4f55\u901a\u8fc7\u8ba8\u8bba\u7f51\u7edc\u63d0\u5347\u6a21\u578b\u53ef\u9760\u6027\u3002", "method": "\u63d0\u51fa\u8ba8\u8bba\u7f51\u7edc\u6a21\u578b\uff0c\u5b9a\u4e49\u65e0\u6548\u6027\u53ca\u5176\u56db\u79cd\u5371\u5bb3\uff0c\u5efa\u7acb\u6570\u5b66\u6a21\u578b\u5206\u6790\u7f51\u7edc\u884c\u4e3a\uff0c\u5e76\u5f00\u53d1FOO\u7b97\u6cd5\u5b9e\u73b0\u540c\u884c\u8bc4\u5ba1\u3002", "result": "\u7f51\u7edc\u5728\u6f02\u79fb\u548c\u81ea\u6211\u4fee\u590d\u4e0b\u7a33\u5b9a\u4e8e\u8f83\u4f4e\u9519\u8bef\u7387\uff1b\u52a0\u5165\u4f2a\u9020\u5bfc\u81f4\u9ad8\u9519\u8bef\u7387\uff1b\u540c\u884c\u8bc4\u5ba1\u663e\u8457\u63d0\u5347\u771f\u5b9e\u6027\u3002", "conclusion": "\u63d0\u5347LLM\u53ef\u9760\u6027\u7684\u5173\u952e\u5728\u4e8e\u5c06\u4e0d\u5b8c\u7f8e\u6a21\u578b\u6574\u5408\u5230\u76f8\u4e92\u76d1\u7763\u7684\u7f51\u7edc\u4e2d\uff0c\u800c\u975e\u8ffd\u6c42\u5355\u4e00\u6a21\u578b\u7684\u5b8c\u7f8e\u3002"}}
{"id": "2507.06502", "pdf": "https://arxiv.org/pdf/2507.06502", "abs": "https://arxiv.org/abs/2507.06502", "authors": ["Yiwen Liu", "Chenyu Zhang", "Junjie Song", "Siqi Chen", "Sun Yin", "Zihan Wang", "Lingming Zeng", "Yuji Cao", "Junming Jiao"], "title": "MoFE-Time: Mixture of Frequency Domain Experts for Time-Series Forecasting Models", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "As a prominent data modality task, time series forecasting plays a pivotal\nrole in diverse applications. With the remarkable advancements in Large\nLanguage Models (LLMs), the adoption of LLMs as the foundational architecture\nfor time series modeling has gained significant attention. Although existing\nmodels achieve some success, they rarely both model time and frequency\ncharacteristics in a pretraining-finetuning paradigm leading to suboptimal\nperformance in predictions of complex time series, which requires both modeling\nperiodicity and prior pattern knowledge of signals. We propose MoFE-Time, an\ninnovative time series forecasting model that integrates time and frequency\ndomain features within a Mixture of Experts (MoE) network. Moreover, we use the\npretraining-finetuning paradigm as our training framework to effectively\ntransfer prior pattern knowledge across pretraining and finetuning datasets\nwith different periodicity distributions. Our method introduces both frequency\nand time cells as experts after attention modules and leverages the MoE routing\nmechanism to construct multidimensional sparse representations of input\nsignals. In experiments on six public benchmarks, MoFE-Time has achieved new\nstate-of-the-art performance, reducing MSE and MAE by 6.95% and 6.02% compared\nto the representative methods Time-MoE. Beyond the existing evaluation\nbenchmarks, we have developed a proprietary dataset, NEV-sales, derived from\nreal-world business scenarios. Our method achieves outstanding results on this\ndataset, underscoring the effectiveness of the MoFE-Time model in practical\ncommercial applications.", "AI": {"tldr": "MoFE-Time\u662f\u4e00\u79cd\u521b\u65b0\u7684\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u6a21\u578b\uff0c\u901a\u8fc7\u7ed3\u5408\u65f6\u95f4\u548c\u9891\u57df\u7279\u5f81\uff0c\u5728MoE\u7f51\u7edc\u4e2d\u5b9e\u73b0\u4e86\u591a\u7ef4\u7a00\u758f\u8868\u793a\uff0c\u663e\u8457\u63d0\u5347\u4e86\u9884\u6d4b\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u6a21\u578b\u5728\u9884\u8bad\u7ec3-\u5fae\u8c03\u8303\u5f0f\u4e2d\u672a\u80fd\u540c\u65f6\u5efa\u6a21\u65f6\u95f4\u548c\u9891\u7387\u7279\u5f81\uff0c\u5bfc\u81f4\u590d\u6742\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u6027\u80fd\u4e0d\u4f73\u3002", "method": "\u63d0\u51faMoFE-Time\u6a21\u578b\uff0c\u96c6\u6210\u65f6\u95f4\u548c\u9891\u57df\u7279\u5f81\u4e8eMoE\u7f51\u7edc\uff0c\u5229\u7528MoE\u8def\u7531\u673a\u5236\u6784\u5efa\u591a\u7ef4\u7a00\u758f\u8868\u793a\u3002", "result": "\u5728\u516d\u4e2a\u516c\u5171\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cMoFE-Time\u5b9e\u73b0\u4e86\u65b0\u7684\u6700\u5148\u8fdb\u6027\u80fd\uff0cMSE\u548cMAE\u5206\u522b\u964d\u4f4e\u4e866.95%\u548c6.02%\u3002", "conclusion": "MoFE-Time\u5728\u7406\u8bba\u548c\u5b9e\u9645\u5e94\u7528\u4e2d\u5747\u8868\u73b0\u51fa\u8272\uff0c\u9a8c\u8bc1\u4e86\u5176\u5728\u590d\u6742\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u4e2d\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2507.06530", "pdf": "https://arxiv.org/pdf/2507.06530", "abs": "https://arxiv.org/abs/2507.06530", "authors": ["Kazi Mahathir Rahman", "Naveed Imtiaz Nafis", "Md. Farhan Sadik", "Mohammad Al Rafi", "Mehedi Hasan Shahed"], "title": "Speak2Sign3D: A Multi-modal Pipeline for English Speech to American Sign Language Animation", "categories": ["cs.CV"], "comment": "11 pages, 12 figures", "summary": "Helping deaf and hard-of-hearing people communicate more easily is the main\ngoal of Automatic Sign Language Translation. Although most past research has\nfocused on turning sign language into text, doing the reverse, turning spoken\nEnglish into sign language animations, has been largely overlooked. That's\nbecause it involves multiple steps, such as understanding speech, translating\nit into sign-friendly grammar, and generating natural human motion. In this\nwork, we introduce a complete pipeline that converts English speech into\nsmooth, realistic 3D sign language animations. Our system starts with Whisper\nto translate spoken English into text. Then, we use a MarianMT machine\ntranslation model to translate that text into American Sign Language (ASL)\ngloss, a simplified version of sign language that captures meaning without\ngrammar. This model performs well, reaching BLEU scores of 0.7714 and 0.8923.\nTo make the gloss translation more accurate, we also use word embeddings such\nas Word2Vec and FastText to understand word meanings. Finally, we animate the\ntranslated gloss using a 3D keypoint-based motion system trained on\nSign3D-WLASL, a dataset we created by extracting body, hand, and face key\npoints from real ASL videos in the WLASL dataset. To support the gloss\ntranslation stage, we also built a new dataset called BookGlossCorpus-CG, which\nturns everyday English sentences from the BookCorpus dataset into ASL gloss\nusing grammar rules. Our system stitches everything together by smoothly\ninterpolating between signs to create natural, continuous animations. Unlike\nprevious works like How2Sign and Phoenix-2014T that focus on recognition or use\nonly one type of data, our pipeline brings together audio, text, and motion in\na single framework that goes all the way from spoken English to lifelike 3D\nsign language animation.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5c06\u82f1\u8bed\u8bed\u97f3\u8f6c\u5316\u4e3a\u6d41\u7545\u3001\u903c\u771f\u76843D\u624b\u8bed\u52a8\u753b\u7684\u5b8c\u6574\u6d41\u7a0b\uff0c\u586b\u8865\u4e86\u4ece\u53e3\u8bed\u5230\u624b\u8bed\u52a8\u753b\u751f\u6210\u7684\u7814\u7a76\u7a7a\u767d\u3002", "motivation": "\u5e2e\u52a9\u804b\u4eba\u548c\u542c\u529b\u969c\u788d\u8005\u66f4\u8f7b\u677e\u5730\u4ea4\u6d41\uff0c\u89e3\u51b3\u4ee5\u5f80\u7814\u7a76\u591a\u96c6\u4e2d\u4e8e\u624b\u8bed\u8f6c\u6587\u672c\u800c\u5ffd\u7565\u53e3\u8bed\u8f6c\u624b\u8bed\u52a8\u753b\u7684\u95ee\u9898\u3002", "method": "\u4f7f\u7528Whisper\u5c06\u82f1\u8bed\u8bed\u97f3\u8f6c\u4e3a\u6587\u672c\uff0cMarianMT\u6a21\u578b\u5c06\u6587\u672c\u7ffb\u8bd1\u4e3a\u7f8e\u56fd\u624b\u8bed\uff08ASL\uff09gloss\uff0c\u7ed3\u5408Word2Vec\u548cFastText\u4f18\u5316\u7ffb\u8bd1\uff0c\u6700\u540e\u901a\u8fc73D\u5173\u952e\u70b9\u8fd0\u52a8\u7cfb\u7edf\u751f\u6210\u52a8\u753b\u3002", "result": "\u7cfb\u7edf\u8868\u73b0\u4f18\u5f02\uff0cBLEU\u5206\u6570\u8fbe\u52300.7714\u548c0.8923\uff0c\u5e76\u521b\u5efa\u4e86\u65b0\u6570\u636e\u96c6Sign3D-WLASL\u548cBookGlossCorpus-CG\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u4f9b\u4e86\u4e00\u4e2a\u4ece\u53e3\u8bed\u52303D\u624b\u8bed\u52a8\u753b\u7684\u5b8c\u6574\u6846\u67b6\uff0c\u6574\u5408\u4e86\u97f3\u9891\u3001\u6587\u672c\u548c\u52a8\u4f5c\u6570\u636e\uff0c\u4f18\u4e8e\u4ee5\u5f80\u4e13\u6ce8\u4e8e\u5355\u4e00\u4efb\u52a1\u7684\u7814\u7a76\u3002"}}
{"id": "2507.06268", "pdf": "https://arxiv.org/pdf/2507.06268", "abs": "https://arxiv.org/abs/2507.06268", "authors": ["Michael I. Jordan"], "title": "A Collectivist, Economic Perspective on AI", "categories": ["cs.CY", "cs.AI", "stat.ML"], "comment": null, "summary": "Information technology is in the midst of a revolution in which omnipresent\ndata collection and machine learning are impacting the human world as never\nbefore. The word \"intelligence\" is being used as a North Star for the\ndevelopment of this technology, with human cognition viewed as a baseline. This\nview neglects the fact that humans are social animals, and that much of our\nintelligence is social and cultural in origin. A related issue is that the\ncurrent view treats the social consequences of technology as an afterthought.\nThe path forward is not merely more data and compute, and not merely more\nattention paid to cognitive or symbolic representations, but a thorough\nblending of economic and social concepts with computational and inferential\nconcepts, in the service of system-level designs in which social welfare is a\nfirst-class citizen, and with the aspiration that a new human-centric\nengineering field will emerge.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u4fe1\u606f\u6280\u672f\u9769\u547d\u4e2d\u5ffd\u89c6\u793e\u4f1a\u548c\u6587\u5316\u56e0\u7d20\u7684\u73b0\u72b6\uff0c\u5e76\u547c\u5401\u5c06\u7ecf\u6d4e\u548c\u793e\u4f1a\u6982\u5ff5\u4e0e\u8ba1\u7b97\u6280\u672f\u7ed3\u5408\uff0c\u4ee5\u8bbe\u8ba1\u66f4\u4eba\u6027\u5316\u7684\u7cfb\u7edf\u3002", "motivation": "\u5f53\u524d\u4fe1\u606f\u6280\u672f\u53d1\u5c55\u8fc7\u4e8e\u5173\u6ce8\u6570\u636e\u548c\u8ba1\u7b97\u80fd\u529b\uff0c\u5ffd\u89c6\u4e86\u4eba\u7c7b\u667a\u80fd\u7684\u793e\u4f1a\u548c\u6587\u5316\u8d77\u6e90\uff0c\u4ee5\u53ca\u6280\u672f\u7684\u793e\u4f1a\u5f71\u54cd\u3002", "method": "\u63d0\u51fa\u5c06\u7ecf\u6d4e\u548c\u793e\u4f1a\u6982\u5ff5\u4e0e\u8ba1\u7b97\u548c\u63a8\u7406\u6982\u5ff5\u6df1\u5ea6\u878d\u5408\uff0c\u8bbe\u8ba1\u4ee5\u793e\u4f1a\u798f\u7949\u4e3a\u6838\u5fc3\u7684\u7cfb\u7edf\u3002", "result": "\u901a\u8fc7\u8fd9\u79cd\u878d\u5408\uff0c\u6709\u671b\u50ac\u751f\u4e00\u4e2a\u4ee5\u4eba\u4e3a\u4e2d\u5fc3\u7684\u65b0\u5de5\u7a0b\u9886\u57df\u3002", "conclusion": "\u672a\u6765\u7684\u6280\u672f\u53d1\u5c55\u5e94\u66f4\u6ce8\u91cd\u793e\u4f1a\u548c\u6587\u5316\u56e0\u7d20\uff0c\u4ee5\u5b9e\u73b0\u66f4\u4eba\u6027\u5316\u7684\u7cfb\u7edf\u8bbe\u8ba1\u3002"}}
{"id": "2507.06905", "pdf": "https://arxiv.org/pdf/2507.06905", "abs": "https://arxiv.org/abs/2507.06905", "authors": ["Wandong Sun", "Luying Feng", "Baoshi Cao", "Yang Liu", "Yaochu Jin", "Zongwu Xie"], "title": "ULC: A Unified and Fine-Grained Controller for Humanoid Loco-Manipulation", "categories": ["cs.RO"], "comment": null, "summary": "Loco-Manipulation for humanoid robots aims to enable robots to integrate\nmobility with upper-body tracking capabilities. Most existing approaches adopt\nhierarchical architectures that decompose control into isolated upper-body\n(manipulation) and lower-body (locomotion) policies. While this decomposition\nreduces training complexity, it inherently limits coordination between\nsubsystems and contradicts the unified whole-body control exhibited by humans.\nWe demonstrate that a single unified policy can achieve a combination of\ntracking accuracy, large workspace, and robustness for humanoid\nloco-manipulation. We propose the Unified Loco-Manipulation Controller (ULC), a\nsingle-policy framework that simultaneously tracks root velocity, root height,\ntorso rotation, and dual-arm joint positions in an end-to-end manner, proving\nthe feasibility of unified control without sacrificing performance. We achieve\nthis unified control through key technologies: sequence skill acquisition for\nprogressive learning complexity, residual action modeling for fine-grained\ncontrol adjustments, command polynomial interpolation for smooth motion\ntransitions, random delay release for robustness to deploy variations, load\nrandomization for generalization to external disturbances, and\ncenter-of-gravity tracking for providing explicit policy gradients to maintain\nstability. We validate our method on the Unitree G1 humanoid robot with 3-DOF\n(degrees-of-freedom) waist. Compared with strong baselines, ULC shows better\ntracking performance to disentangled methods and demonstrating larger workspace\ncoverage. The unified dual-arm tracking enables precise manipulation under\nexternal loads while maintaining coordinated whole-body control for complex\nloco-manipulation tasks.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7edf\u4e00\u7684\u4eba\u5f62\u673a\u5668\u4eba\u8fd0\u52a8\u4e0e\u64cd\u4f5c\u63a7\u5236\u6846\u67b6\uff08ULC\uff09\uff0c\u901a\u8fc7\u5355\u4e00\u7b56\u7565\u5b9e\u73b0\u5168\u8eab\u534f\u8c03\u63a7\u5236\uff0c\u4f18\u4e8e\u4f20\u7edf\u5206\u5c42\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5c06\u8fd0\u52a8\u4e0e\u64cd\u4f5c\u63a7\u5236\u5206\u79bb\uff0c\u9650\u5236\u4e86\u534f\u8c03\u6027\uff0c\u800c\u4eba\u7c7b\u8868\u73b0\u51fa\u7edf\u4e00\u7684\u5168\u8eab\u63a7\u5236\u80fd\u529b\u3002", "method": "\u91c7\u7528\u5e8f\u5217\u6280\u80fd\u5b66\u4e60\u3001\u6b8b\u5dee\u52a8\u4f5c\u5efa\u6a21\u3001\u547d\u4ee4\u591a\u9879\u5f0f\u63d2\u503c\u7b49\u6280\u672f\uff0c\u5b9e\u73b0\u7aef\u5230\u7aef\u7684\u7edf\u4e00\u63a7\u5236\u3002", "result": "\u5728Unitree G1\u673a\u5668\u4eba\u4e0a\u9a8c\u8bc1\uff0cULC\u5728\u8ddf\u8e2a\u7cbe\u5ea6\u3001\u5de5\u4f5c\u7a7a\u95f4\u8986\u76d6\u548c\u6297\u5e72\u6270\u80fd\u529b\u4e0a\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "\u7edf\u4e00\u63a7\u5236\u6846\u67b6\u53ef\u884c\u4e14\u9ad8\u6548\uff0c\u4e3a\u590d\u6742\u4efb\u52a1\u63d0\u4f9b\u4e86\u66f4\u597d\u7684\u534f\u8c03\u6027\u548c\u9c81\u68d2\u6027\u3002"}}
{"id": "2507.06571", "pdf": "https://arxiv.org/pdf/2507.06571", "abs": "https://arxiv.org/abs/2507.06571", "authors": ["Srihari K B", "Pushpak Bhattacharyya"], "title": "Enhancing Food-Domain Question Answering with a Multimodal Knowledge Graph: Hybrid QA Generation and Diversity Analysis", "categories": ["cs.CL"], "comment": null, "summary": "We propose a unified food-domain QA framework that combines a large-scale\nmultimodal knowledge graph (MMKG) with generative AI. Our MMKG links 13,000\nrecipes, 3,000 ingredients, 140,000 relations, and 14,000 images. We generate\n40,000 QA pairs using 40 templates and LLaVA/DeepSeek augmentation. Joint\nfine-tuning of Meta LLaMA 3.1-8B and Stable Diffusion 3.5-Large improves\nBERTScore by 16.2\\%, reduces FID by 37.8\\%, and boosts CLIP alignment by\n31.1\\%. Diagnostic analyses-CLIP-based mismatch detection (35.2\\% to 7.3\\%) and\nLLaVA-driven hallucination checks-ensure factual and visual fidelity. A hybrid\nretrieval-generation strategy achieves 94.1\\% accurate image reuse and 85\\%\nadequacy in synthesis. Our results demonstrate that structured knowledge and\nmultimodal generation together enhance reliability and diversity in food QA.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u591a\u6a21\u6001\u77e5\u8bc6\u56fe\u8c31\uff08MMKG\uff09\u4e0e\u751f\u6210\u5f0fAI\u7684\u98df\u54c1\u9886\u57dfQA\u6846\u67b6\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\u3002", "motivation": "\u901a\u8fc7\u7ed3\u5408\u7ed3\u6784\u5316\u77e5\u8bc6\u4e0e\u591a\u6a21\u6001\u751f\u6210\u6280\u672f\uff0c\u63d0\u5347\u98df\u54c1\u9886\u57df\u95ee\u7b54\u7cfb\u7edf\u7684\u53ef\u9760\u6027\u548c\u591a\u6837\u6027\u3002", "method": "\u6784\u5efa\u5305\u542b13,000\u98df\u8c31\u30013,000\u98df\u6750\u7684MMKG\uff0c\u751f\u621040,000 QA\u5bf9\uff0c\u8054\u5408\u5fae\u8c03LLaMA 3.1-8B\u548cStable Diffusion 3.5-Large\u3002", "result": "BERTScore\u63d0\u534716.2%\uff0cFID\u964d\u4f4e37.8%\uff0cCLIP\u5bf9\u9f50\u63d0\u534731.1%\uff0c\u56fe\u50cf\u91cd\u7528\u51c6\u786e\u7387\u8fbe94.1%\u3002", "conclusion": "\u7ed3\u6784\u5316\u77e5\u8bc6\u4e0e\u591a\u6a21\u6001\u751f\u6210\u7ed3\u5408\u663e\u8457\u63d0\u5347\u4e86\u98df\u54c1QA\u7684\u53ef\u9760\u6027\u548c\u591a\u6837\u6027\u3002"}}
{"id": "2507.06516", "pdf": "https://arxiv.org/pdf/2507.06516", "abs": "https://arxiv.org/abs/2507.06516", "authors": ["Yunrui Zhang", "Gustavo Batista", "Salil S. Kanhere"], "title": "Instance-Wise Monotonic Calibration by Constrained Transformation", "categories": ["cs.LG", "stat.ML"], "comment": "Accepted to Conference on Uncertainty in Artificial Intelligence\n  (UAI)", "summary": "Deep neural networks often produce miscalibrated probability estimates,\nleading to overconfident predictions. A common approach for calibration is\nfitting a post-hoc calibration map on unseen validation data that transforms\npredicted probabilities. A key desirable property of the calibration map is\ninstance-wise monotonicity (i.e., preserving the ranking of probability\noutputs). However, most existing post-hoc calibration methods do not guarantee\nmonotonicity. Previous monotonic approaches either use an under-parameterized\ncalibration map with limited expressive ability or rely on black-box neural\nnetworks, which lack interpretability and robustness. In this paper, we propose\na family of novel monotonic post-hoc calibration methods, which employs a\nconstrained calibration map parameterized linearly with respect to the number\nof classes. Our proposed approach ensures expressiveness, robustness, and\ninterpretability while preserving the relative ordering of the probability\noutput by formulating the proposed calibration map as a constrained\noptimization problem. Our proposed methods achieve state-of-the-art performance\nacross datasets with different deep neural network models, outperforming\nexisting calibration methods while being data and computation-efficient. Our\ncode is available at\nhttps://github.com/YunruiZhang/Calibration-by-Constrained-Transformation", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u5355\u8c03\u540e\u6821\u51c6\u65b9\u6cd5\uff0c\u901a\u8fc7\u7ebf\u6027\u53c2\u6570\u5316\u6821\u51c6\u6620\u5c04\uff0c\u786e\u4fdd\u8868\u8fbe\u80fd\u529b\u3001\u9c81\u68d2\u6027\u548c\u53ef\u89e3\u91ca\u6027\uff0c\u540c\u65f6\u4fdd\u6301\u6982\u7387\u8f93\u51fa\u7684\u76f8\u5bf9\u987a\u5e8f\u3002", "motivation": "\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u5e38\u4ea7\u751f\u9519\u8bef\u7684\u6982\u7387\u4f30\u8ba1\uff0c\u5bfc\u81f4\u8fc7\u5ea6\u81ea\u4fe1\u7684\u9884\u6d4b\u3002\u73b0\u6709\u540e\u6821\u51c6\u65b9\u6cd5\u5927\u591a\u65e0\u6cd5\u4fdd\u8bc1\u5355\u8c03\u6027\uff0c\u6216\u8868\u8fbe\u80fd\u529b\u6709\u9650\uff0c\u6216\u7f3a\u4e4f\u53ef\u89e3\u91ca\u6027\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u7ea6\u675f\u4f18\u5316\u7684\u5355\u8c03\u540e\u6821\u51c6\u65b9\u6cd5\uff0c\u7ebf\u6027\u53c2\u6570\u5316\u6821\u51c6\u6620\u5c04\uff0c\u786e\u4fdd\u5355\u8c03\u6027\u548c\u8868\u8fbe\u80fd\u529b\u3002", "result": "\u5728\u591a\u4e2a\u6570\u636e\u96c6\u548c\u6a21\u578b\u4e0a\uff0c\u65b0\u65b9\u6cd5\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u6821\u51c6\u65b9\u6cd5\uff0c\u4e14\u6570\u636e\u4e0e\u8ba1\u7b97\u6548\u7387\u9ad8\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u4fdd\u6301\u5355\u8c03\u6027\u7684\u540c\u65f6\uff0c\u63d0\u4f9b\u4e86\u66f4\u597d\u7684\u6821\u51c6\u6027\u80fd\uff0c\u9002\u7528\u4e8e\u5b9e\u9645\u5e94\u7528\u3002"}}
{"id": "2507.06531", "pdf": "https://arxiv.org/pdf/2507.06531", "abs": "https://arxiv.org/abs/2507.06531", "authors": ["Mingjin Zeng", "Nan Ouyang", "Wenkang Wan", "Lei Ao", "Qing Cai", "Kai Sheng"], "title": "ILNet: Trajectory Prediction with Inverse Learning Attention for Enhancing Intention Capture", "categories": ["cs.CV"], "comment": null, "summary": "Trajectory prediction for multi-agent interaction scenarios is a crucial\nchallenge. Most advanced methods model agent interactions by efficiently\nfactorized attention based on the temporal and agent axes. However, this static\nand foward modeling lacks explicit interactive spatio-temporal coordination,\ncapturing only obvious and immediate behavioral intentions. Alternatively, the\nmodern trajectory prediction framework refines the successive predictions by a\nfixed-anchor selection strategy, which is difficult to adapt in different\nfuture environments. It is acknowledged that human drivers dynamically adjust\ninitial driving decisions based on further assumptions about the intentions of\nsurrounding vehicles. Motivated by human driving behaviors, this paper proposes\nILNet, a multi-agent trajectory prediction method with Inverse Learning (IL)\nattention and Dynamic Anchor Selection (DAS) module. IL Attention employs an\ninverse learning paradigm to model interactions at neighboring moments,\nintroducing proposed intentions to dynamically encode the spatio-temporal\ncoordination of interactions, thereby enhancing the model's ability to capture\ncomplex interaction patterns. Then, the learnable DAS module is proposed to\nextract multiple trajectory change keypoints as anchors in parallel with almost\nno increase in parameters. Experimental results show that the ILNet achieves\nstate-of-the-art performance on the INTERACTION and Argoverse motion\nforecasting datasets. Particularly, in challenged interaction scenarios, ILNet\nachieves higher accuracy and more multimodal distributions of trajectories over\nfewer parameters. Our codes are available at https://github.com/mjZeng11/ILNet.", "AI": {"tldr": "ILNet\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u667a\u80fd\u4f53\u8f68\u8ff9\u9884\u6d4b\u65b9\u6cd5\uff0c\u7ed3\u5408\u9006\u5411\u5b66\u4e60\u6ce8\u610f\u529b\u673a\u5236\u548c\u52a8\u6001\u951a\u70b9\u9009\u62e9\u6a21\u5757\uff0c\u663e\u8457\u63d0\u5347\u4e86\u590d\u6742\u4ea4\u4e92\u573a\u666f\u4e0b\u7684\u9884\u6d4b\u6027\u80fd\u3002", "motivation": "\u53d7\u4eba\u7c7b\u9a7e\u9a76\u884c\u4e3a\u7684\u542f\u53d1\uff0c\u65e8\u5728\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\u5728\u65f6\u7a7a\u534f\u8c03\u548c\u52a8\u6001\u9002\u5e94\u6027\u4e0a\u7684\u4e0d\u8db3\u3002", "method": "\u91c7\u7528\u9006\u5411\u5b66\u4e60\u6ce8\u610f\u529b\u673a\u5236\uff08IL Attention\uff09\u5efa\u6a21\u4ea4\u4e92\u610f\u56fe\uff0c\u5e76\u5f15\u5165\u52a8\u6001\u951a\u70b9\u9009\u62e9\u6a21\u5757\uff08DAS\uff09\u63d0\u53d6\u5173\u952e\u70b9\u3002", "result": "\u5728INTERACTION\u548cArgoverse\u6570\u636e\u96c6\u4e0a\u8fbe\u5230\u6700\u4f18\u6027\u80fd\uff0c\u5c24\u5176\u5728\u590d\u6742\u4ea4\u4e92\u573a\u666f\u4e2d\u8868\u73b0\u7a81\u51fa\u3002", "conclusion": "ILNet\u901a\u8fc7\u52a8\u6001\u5efa\u6a21\u548c\u9ad8\u6548\u53c2\u6570\u5229\u7528\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8f68\u8ff9\u9884\u6d4b\u7684\u51c6\u786e\u6027\u548c\u591a\u6a21\u6001\u5206\u5e03\u80fd\u529b\u3002"}}
{"id": "2507.06960", "pdf": "https://arxiv.org/pdf/2507.06960", "abs": "https://arxiv.org/abs/2507.06960", "authors": ["Samuel Matloob", "Ayan Dutta", "O. Patrick Kreidl", "Swapnonel Roy", "Ladislau B\u00f6l\u00f6ni"], "title": "Bounomodes: the grazing ox algorithm for exploration of clustered anomalies", "categories": ["cs.RO"], "comment": null, "summary": "A common class of algorithms for informative path planning (IPP) follows\nboustrophedon (\"as the ox turns\") patterns, which aim to achieve uniform area\ncoverage. However, IPP is often applied in scenarios where anomalies, such as\nplant diseases, pollution, or hurricane damage, appear in clusters. In such\ncases, prioritizing the exploration of anomalous regions over uniform coverage\nis beneficial. This work introduces a class of algorithms referred to as\nbounom\\=odes (\"as the ox grazes\"), which alternates between uniform\nboustrophedon sampling and targeted exploration of detected anomaly clusters.\nWhile uniform sampling can be designed using geometric principles, close\nexploration of clusters depends on the spatial distribution of anomalies and\nmust be learned. In our implementation, the close exploration behavior is\nlearned using deep reinforcement learning algorithms. Experimental evaluations\ndemonstrate that the proposed approach outperforms several established\nbaselines.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u201cbounom=odes\u201d\u7684\u7b97\u6cd5\uff0c\u7ed3\u5408\u5747\u5300\u91c7\u6837\u548c\u5f02\u5e38\u533a\u57df\u9488\u5bf9\u6027\u63a2\u7d22\uff0c\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u3002", "motivation": "\u4f20\u7edf\u5747\u5300\u8986\u76d6\u7b97\u6cd5\u5728\u5f02\u5e38\u96c6\u7fa4\u573a\u666f\u4e2d\u6548\u7387\u4e0d\u8db3\uff0c\u9700\u4f18\u5148\u63a2\u7d22\u5f02\u5e38\u533a\u57df\u3002", "method": "\u7ed3\u5408\u51e0\u4f55\u539f\u7406\u7684\u5747\u5300\u91c7\u6837\u548c\u57fa\u4e8e\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u7684\u5f02\u5e38\u96c6\u7fa4\u63a2\u7d22\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\u65b0\u7b97\u6cd5\u4f18\u4e8e\u591a\u79cd\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "bounom=odes\u7b97\u6cd5\u5728\u5f02\u5e38\u96c6\u7fa4\u573a\u666f\u4e2d\u66f4\u9ad8\u6548\u3002"}}
{"id": "2507.06607", "pdf": "https://arxiv.org/pdf/2507.06607", "abs": "https://arxiv.org/abs/2507.06607", "authors": ["Liliang Ren", "Congcong Chen", "Haoran Xu", "Young Jin Kim", "Adam Atkinson", "Zheng Zhan", "Jiankai Sun", "Baolin Peng", "Liyuan Liu", "Shuohang Wang", "Hao Cheng", "Jianfeng Gao", "Weizhu Chen", "Yelong Shen"], "title": "Decoder-Hybrid-Decoder Architecture for Efficient Reasoning with Long Generation", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Recent advances in language modeling have demonstrated the effectiveness of\nState Space Models (SSMs) for efficient sequence modeling. While hybrid\narchitectures such as Samba and the decoder-decoder architecture, YOCO, have\nshown promising performance gains over Transformers, prior works have not\ninvestigated the efficiency potential of representation sharing between SSM\nlayers. In this paper, we introduce the Gated Memory Unit (GMU), a simple yet\neffective mechanism for efficient memory sharing across layers. We apply it to\ncreate SambaY, a decoder-hybrid-decoder architecture that incorporates GMUs in\nthe cross-decoder to share memory readout states from a Samba-based\nself-decoder. SambaY significantly enhances decoding efficiency, preserves\nlinear pre-filling time complexity, and boosts long-context performance, all\nwhile eliminating the need for explicit positional encoding. Through extensive\nscaling experiments, we demonstrate that our model exhibits a significantly\nlower irreducible loss compared to a strong YOCO baseline, indicating superior\nperformance scalability under large-scale compute regimes. Our largest model\nenhanced with Differential Attention, Phi4-mini-Flash-Reasoning, achieves\nsignificantly better performance than Phi4-mini-Reasoning on reasoning tasks\nsuch as Math500, AIME24/25, and GPQA Diamond without any reinforcement\nlearning, while delivering up to 10x higher decoding throughput on 2K-length\nprompts with 32K generation length under the vLLM inference framework. We\nrelease our training codebase on open-source data at\nhttps://github.com/microsoft/ArchScale.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aGated Memory Unit (GMU)\u7684\u673a\u5236\uff0c\u7528\u4e8e\u5728SSM\u5c42\u4e4b\u95f4\u9ad8\u6548\u5171\u4eab\u5185\u5b58\uff0c\u5e76\u57fa\u4e8e\u6b64\u6784\u5efa\u4e86SambaY\u67b6\u6784\uff0c\u663e\u8457\u63d0\u5347\u4e86\u89e3\u7801\u6548\u7387\u548c\u957f\u4e0a\u4e0b\u6587\u6027\u80fd\u3002", "motivation": "\u5c3d\u7ba1\u6df7\u5408\u67b6\u6784\u5982Samba\u548cYOCO\u5728\u5e8f\u5217\u5efa\u6a21\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u6b64\u524d\u7814\u7a76\u672a\u63a2\u7d22SSM\u5c42\u95f4\u8868\u793a\u5171\u4eab\u7684\u6548\u7387\u6f5c\u529b\u3002", "method": "\u5f15\u5165GMU\u673a\u5236\uff0c\u6784\u5efaSambaY\u67b6\u6784\uff0c\u7ed3\u5408Samba\u548cYOCO\u7684\u4f18\u52bf\uff0c\u5b9e\u73b0\u8de8\u5c42\u5185\u5b58\u5171\u4eab\u3002", "result": "SambaY\u663e\u8457\u63d0\u5347\u89e3\u7801\u6548\u7387\uff0c\u4fdd\u6301\u7ebf\u6027\u9884\u586b\u5145\u65f6\u95f4\u590d\u6742\u5ea6\uff0c\u5e76\u5728\u957f\u4e0a\u4e0b\u6587\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\u3002\u6700\u5927\u6a21\u578b\u5728\u63a8\u7406\u4efb\u52a1\u4e2d\u6027\u80fd\u4f18\u4e8e\u57fa\u7ebf\uff0c\u4e14\u89e3\u7801\u541e\u5410\u91cf\u63d0\u534710\u500d\u3002", "conclusion": "GMU\u548cSambaY\u67b6\u6784\u4e3a\u9ad8\u6548\u5e8f\u5217\u5efa\u6a21\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\uff0c\u5c24\u5176\u5728\u957f\u4e0a\u4e0b\u6587\u548c\u5927\u89c4\u6a21\u8ba1\u7b97\u573a\u666f\u4e0b\u8868\u73b0\u7a81\u51fa\u3002"}}
{"id": "2507.06525", "pdf": "https://arxiv.org/pdf/2507.06525", "abs": "https://arxiv.org/abs/2507.06525", "authors": ["Huiqi Zhang", "Fang Xie"], "title": "AdaDPIGU: Differentially Private SGD with Adaptive Clipping and Importance-Based Gradient Updates for Deep Neural Networks", "categories": ["cs.LG", "math.ST", "stat.ML", "stat.TH"], "comment": null, "summary": "Differential privacy has been proven effective for stochastic gradient\ndescent; however, existing methods often suffer from performance degradation in\nhigh-dimensional settings, as the scale of injected noise increases with\ndimensionality. To tackle this challenge, we propose AdaDPIGU--a new\ndifferentially private SGD framework with importance-based gradient updates\ntailored for deep neural networks. In the pretraining stage, we apply a\ndifferentially private Gaussian mechanism to estimate the importance of each\nparameter while preserving privacy. During the gradient update phase, we prune\nlow-importance coordinates and introduce a coordinate-wise adaptive clipping\nmechanism, enabling sparse and noise-efficient gradient updates. Theoretically,\nwe prove that AdaDPIGU satisfies $(\\varepsilon, \\delta)$-differential privacy\nand retains convergence guarantees. Extensive experiments on standard\nbenchmarks validate the effectiveness of AdaDPIGU. All results are reported\nunder a fixed retention ratio of 60%. On MNIST, our method achieves a test\naccuracy of 99.12% under a privacy budget of $\\epsilon = 8$, nearly matching\nthe non-private model. Remarkably, on CIFAR-10, it attains 73.21% accuracy at\n$\\epsilon = 4$, outperforming the non-private baseline of 71.12%, demonstrating\nthat adaptive sparsification can enhance both privacy and utility.", "AI": {"tldr": "AdaDPIGU\u662f\u4e00\u79cd\u65b0\u7684\u5dee\u5206\u9690\u79c1SGD\u6846\u67b6\uff0c\u901a\u8fc7\u91cd\u8981\u6027\u68af\u5ea6\u66f4\u65b0\u548c\u81ea\u9002\u5e94\u526a\u679d\u673a\u5236\uff0c\u5728\u9ad8\u7ef4\u8bbe\u7f6e\u4e2d\u63d0\u5347\u6027\u80fd\uff0c\u540c\u65f6\u6ee1\u8db3\u9690\u79c1\u4fdd\u62a4\u3002", "motivation": "\u73b0\u6709\u5dee\u5206\u9690\u79c1\u65b9\u6cd5\u5728\u9ad8\u7ef4\u8bbe\u7f6e\u4e2d\u6027\u80fd\u4e0b\u964d\uff0c\u566a\u58f0\u968f\u7ef4\u5ea6\u589e\u52a0\u800c\u589e\u5927\uff0c\u5f71\u54cd\u6a21\u578b\u8868\u73b0\u3002", "method": "\u9884\u8bad\u7ec3\u9636\u6bb5\u4f7f\u7528\u5dee\u5206\u9690\u79c1\u9ad8\u65af\u673a\u5236\u4f30\u8ba1\u53c2\u6570\u91cd\u8981\u6027\uff0c\u68af\u5ea6\u66f4\u65b0\u9636\u6bb5\u526a\u679d\u4f4e\u91cd\u8981\u6027\u5750\u6807\u5e76\u5f15\u5165\u81ea\u9002\u5e94\u88c1\u526a\u673a\u5236\u3002", "result": "\u5728MNIST\u548cCIFAR-10\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u9690\u79c1\u9884\u7b97\u4e0b\u63a5\u8fd1\u6216\u8d85\u8d8a\u975e\u9690\u79c1\u6a21\u578b\u3002", "conclusion": "AdaDPIGU\u901a\u8fc7\u81ea\u9002\u5e94\u7a00\u758f\u5316\u540c\u65f6\u63d0\u5347\u9690\u79c1\u4fdd\u62a4\u548c\u6a21\u578b\u6548\u7528\u3002"}}
{"id": "2507.06537", "pdf": "https://arxiv.org/pdf/2507.06537", "abs": "https://arxiv.org/abs/2507.06537", "authors": ["Thi Thu Thuy Nguyen", "Duc Thanh Nguyen"], "title": "A model-agnostic active learning approach for animal detection from camera traps", "categories": ["cs.CV"], "comment": null, "summary": "Smart data selection is becoming increasingly important in data-driven\nmachine learning. Active learning offers a promising solution by allowing\nmachine learning models to be effectively trained with optimal data including\nthe most informative samples from large datasets. Wildlife data captured by\ncamera traps are excessive in volume, requiring tremendous effort in data\nlabelling and animal detection models training. Therefore, applying active\nlearning to optimise the amount of labelled data would be a great aid in\nenabling automated wildlife monitoring and conservation. However, existing\nactive learning techniques require that a machine learning model (i.e., an\nobject detector) be fully accessible, limiting the applicability of the\ntechniques. In this paper, we propose a model-agnostic active learning approach\nfor detection of animals captured by camera traps. Our approach integrates\nuncertainty and diversity quantities of samples at both the object-based and\nimage-based levels into the active learning sample selection process. We\nvalidate our approach in a benchmark animal dataset. Experimental results\ndemonstrate that, using only 30% of the training data selected by our approach,\na state-of-the-art animal detector can achieve a performance of equal or\ngreater than that with the use of the complete training dataset.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u6a21\u578b\u65e0\u5173\u7684\u4e3b\u52a8\u5b66\u4e60\u65b9\u6cd5\uff0c\u7528\u4e8e\u4f18\u5316\u76f8\u673a\u9677\u9631\u6355\u83b7\u7684\u91ce\u751f\u52a8\u7269\u6570\u636e\u6807\u6ce8\uff0c\u4ec5\u970030%\u7684\u8bad\u7ec3\u6570\u636e\u5373\u53ef\u8fbe\u5230\u6216\u8d85\u8fc7\u5b8c\u6574\u6570\u636e\u96c6\u7684\u68c0\u6d4b\u6027\u80fd\u3002", "motivation": "\u91ce\u751f\u52a8\u7269\u6570\u636e\u6807\u6ce8\u548c\u6a21\u578b\u8bad\u7ec3\u6210\u672c\u9ad8\uff0c\u73b0\u6709\u4e3b\u52a8\u5b66\u4e60\u65b9\u6cd5\u9700\u5b8c\u5168\u8bbf\u95ee\u6a21\u578b\uff0c\u9650\u5236\u4e86\u5176\u5e94\u7528\u3002", "method": "\u7ed3\u5408\u5bf9\u8c61\u548c\u56fe\u50cf\u5c42\u9762\u7684\u4e0d\u786e\u5b9a\u6027\u4e0e\u591a\u6837\u6027\u6307\u6807\uff0c\u8fdb\u884c\u4e3b\u52a8\u5b66\u4e60\u6837\u672c\u9009\u62e9\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u4f7f\u752830%\u7684\u8bad\u7ec3\u6570\u636e\uff0c\u52a8\u7269\u68c0\u6d4b\u5668\u6027\u80fd\u53ef\u8fbe\u5230\u6216\u8d85\u8fc7\u5b8c\u6574\u6570\u636e\u96c6\u7684\u6548\u679c\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u81ea\u52a8\u5316\u91ce\u751f\u52a8\u7269\u76d1\u6d4b\u63d0\u4f9b\u4e86\u9ad8\u6548\u7684\u6570\u636e\u9009\u62e9\u65b9\u6848\u3002"}}
{"id": "2507.06336", "pdf": "https://arxiv.org/pdf/2507.06336", "abs": "https://arxiv.org/abs/2507.06336", "authors": ["Adam J Riesselman", "Evan M Cofer", "Therese LaRue", "Wim Meeussen"], "title": "Self-supervised learning predicts plant growth trajectories from multi-modal industrial greenhouse data", "categories": ["q-bio.QM", "cs.LG", "cs.RO"], "comment": null, "summary": "Quantifying organism-level phenotypes, such as growth dynamics and biomass\naccumulation, is fundamental to understanding agronomic traits and optimizing\ncrop production. However, quality growing data of plants at scale is difficult\nto generate. Here we use a mobile robotic platform to capture high-resolution\nenvironmental sensing and phenotyping measurements of a large-scale hydroponic\nleafy greens system. We describe a self-supervised modeling approach to build a\nmap from observed growing data to the entire plant growth trajectory. We\ndemonstrate our approach by forecasting future plant height and harvest mass of\ncrops in this system. This approach represents a significant advance in\ncombining robotic automation and machine learning, as well as providing\nactionable insights for agronomic research and operational efficiency.", "AI": {"tldr": "\u5229\u7528\u79fb\u52a8\u673a\u5668\u4eba\u5e73\u53f0\u548c\u81ea\u76d1\u7763\u5efa\u6a21\u65b9\u6cd5\uff0c\u9884\u6d4b\u690d\u7269\u751f\u957f\u8f68\u8ff9\uff0c\u63d0\u5347\u519c\u4e1a\u7814\u7a76\u548c\u8fd0\u8425\u6548\u7387\u3002", "motivation": "\u91cf\u5316\u690d\u7269\u751f\u957f\u52a8\u6001\u548c\u751f\u7269\u91cf\u79ef\u7d2f\u5bf9\u4f18\u5316\u4f5c\u7269\u751f\u4ea7\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u5927\u89c4\u6a21\u83b7\u53d6\u9ad8\u8d28\u91cf\u751f\u957f\u6570\u636e\u56f0\u96be\u3002", "method": "\u4f7f\u7528\u79fb\u52a8\u673a\u5668\u4eba\u5e73\u53f0\u6536\u96c6\u9ad8\u5206\u8fa8\u7387\u73af\u5883\u4f20\u611f\u548c\u8868\u578b\u6570\u636e\uff0c\u91c7\u7528\u81ea\u76d1\u7763\u5efa\u6a21\u65b9\u6cd5\u9884\u6d4b\u690d\u7269\u751f\u957f\u8f68\u8ff9\u3002", "result": "\u6210\u529f\u9884\u6d4b\u4e86\u690d\u7269\u672a\u6765\u9ad8\u5ea6\u548c\u6536\u83b7\u8d28\u91cf\uff0c\u5c55\u793a\u4e86\u673a\u5668\u4eba\u81ea\u52a8\u5316\u548c\u673a\u5668\u5b66\u4e60\u7684\u7ed3\u5408\u4f18\u52bf\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u519c\u4e1a\u7814\u7a76\u548c\u8fd0\u8425\u63d0\u4f9b\u4e86\u53ef\u64cd\u4f5c\u7684\u89c1\u89e3\uff0c\u662f\u673a\u5668\u4eba\u81ea\u52a8\u5316\u548c\u673a\u5668\u5b66\u4e60\u7ed3\u5408\u7684\u91cd\u8981\u8fdb\u5c55\u3002"}}
{"id": "2507.06622", "pdf": "https://arxiv.org/pdf/2507.06622", "abs": "https://arxiv.org/abs/2507.06622", "authors": ["Boshko Koloski", "Senja Pollak", "Roberto Navigli", "Bla\u017e \u0160krlj"], "title": "FuDoBa: Fusing Document and Knowledge Graph-based Representations with Bayesian Optimisation", "categories": ["cs.CL"], "comment": null, "summary": "Building on the success of Large Language Models (LLMs), LLM-based\nrepresentations have dominated the document representation landscape, achieving\ngreat performance on the document embedding benchmarks. However, the\nhigh-dimensional, computationally expensive embeddings from LLMs tend to be\neither too generic or inefficient for domain-specific applications. To address\nthese limitations, we introduce FuDoBa a Bayesian optimisation-based method\nthat integrates LLM-based embeddings with domain-specific structured knowledge,\nsourced both locally and from external repositories like WikiData. This fusion\nproduces low-dimensional, task-relevant representations while reducing training\ncomplexity and yielding interpretable early-fusion weights for enhanced\nclassification performance. We demonstrate the effectiveness of our approach on\nsix datasets in two domains, showing that when paired with robust AutoML-based\nclassifiers, our proposed representation learning approach performs on par\nwith, or surpasses, those produced solely by the proprietary LLM-based\nembedding baselines.", "AI": {"tldr": "FuDoBa\u662f\u4e00\u79cd\u57fa\u4e8e\u8d1d\u53f6\u65af\u4f18\u5316\u7684\u65b9\u6cd5\uff0c\u7ed3\u5408LLM\u5d4c\u5165\u4e0e\u9886\u57df\u77e5\u8bc6\uff0c\u751f\u6210\u4f4e\u7ef4\u3001\u4efb\u52a1\u76f8\u5173\u7684\u8868\u793a\uff0c\u63d0\u5347\u5206\u7c7b\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3LLM\u751f\u6210\u7684\u9ad8\u7ef4\u5d4c\u5165\u5728\u9886\u57df\u5e94\u7528\u4e2d\u8fc7\u4e8e\u901a\u7528\u6216\u4f4e\u6548\u7684\u95ee\u9898\u3002", "method": "\u901a\u8fc7\u8d1d\u53f6\u65af\u4f18\u5316\u878d\u5408LLM\u5d4c\u5165\u4e0e\u9886\u57df\u77e5\u8bc6\uff08\u672c\u5730\u548cWikiData\uff09\uff0c\u751f\u6210\u4f4e\u7ef4\u8868\u793a\u3002", "result": "\u5728\u516d\u4e2a\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\uff0c\u6027\u80fd\u4f18\u4e8e\u6216\u5ab2\u7f8e\u4e13\u6709LLM\u5d4c\u5165\u57fa\u7ebf\u3002", "conclusion": "FuDoBa\u80fd\u6709\u6548\u751f\u6210\u9ad8\u6548\u3001\u53ef\u89e3\u91ca\u7684\u8868\u793a\uff0c\u9002\u7528\u4e8e\u9886\u57df\u7279\u5b9a\u4efb\u52a1\u3002"}}
{"id": "2507.06529", "pdf": "https://arxiv.org/pdf/2507.06529", "abs": "https://arxiv.org/abs/2507.06529", "authors": ["Fengxue Zhang", "Yuxin Chen"], "title": "Direct Regret Optimization in Bayesian Optimization", "categories": ["cs.LG"], "comment": null, "summary": "Bayesian optimization (BO) is a powerful paradigm for optimizing expensive\nblack-box functions. Traditional BO methods typically rely on separate\nhand-crafted acquisition functions and surrogate models for the underlying\nfunction, and often operate in a myopic manner. In this paper, we propose a\nnovel direct regret optimization approach that jointly learns the optimal model\nand non-myopic acquisition by distilling from a set of candidate models and\nacquisitions, and explicitly targets minimizing the multi-step regret. Our\nframework leverages an ensemble of Gaussian Processes (GPs) with varying\nhyperparameters to generate simulated BO trajectories, each guided by an\nacquisition function chosen from a pool of conventional choices, until a\nBayesian early stop criterion is met. These simulated trajectories, capturing\nmulti-step exploration strategies, are used to train an end-to-end decision\ntransformer that directly learns to select next query points aimed at improving\nthe ultimate objective. We further adopt a dense training--sparse learning\nparadigm: The decision transformer is trained offline with abundant simulated\ndata sampled from ensemble GPs and acquisitions, while a limited number of real\nevaluations refine the GPs online. Experimental results on synthetic and\nreal-world benchmarks suggest that our method consistently outperforms BO\nbaselines, achieving lower simple regret and demonstrating more robust\nexploration in high-dimensional or noisy settings.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u8d1d\u53f6\u65af\u4f18\u5316\u65b9\u6cd5\uff0c\u901a\u8fc7\u8054\u5408\u5b66\u4e60\u6700\u4f18\u6a21\u578b\u548c\u975e\u8fd1\u89c6\u83b7\u53d6\u51fd\u6570\uff0c\u76f4\u63a5\u4f18\u5316\u591a\u6b65\u9057\u61be\u3002", "motivation": "\u4f20\u7edf\u8d1d\u53f6\u65af\u4f18\u5316\u65b9\u6cd5\u4f9d\u8d56\u624b\u5de5\u8bbe\u8ba1\u7684\u83b7\u53d6\u51fd\u6570\u548c\u4ee3\u7406\u6a21\u578b\uff0c\u4e14\u901a\u5e38\u662f\u8fd1\u89c6\u7684\u3002\u672c\u6587\u65e8\u5728\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "\u4f7f\u7528\u9ad8\u65af\u8fc7\u7a0b\u96c6\u5408\u751f\u6210\u6a21\u62df\u8f68\u8ff9\uff0c\u8bad\u7ec3\u7aef\u5230\u7aef\u51b3\u7b56\u53d8\u6362\u5668\uff0c\u7ed3\u5408\u79bb\u7ebf\u5bc6\u96c6\u8bad\u7ec3\u548c\u5728\u7ebf\u7a00\u758f\u5b66\u4e60\u3002", "result": "\u5728\u5408\u6210\u548c\u771f\u5b9e\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u8be5\u65b9\u6cd5\u4f18\u4e8e\u57fa\u7ebf\uff0c\u5b9e\u73b0\u4e86\u66f4\u4f4e\u7684\u7b80\u5355\u9057\u61be\u548c\u66f4\u9c81\u68d2\u7684\u63a2\u7d22\u3002", "conclusion": "\u63d0\u51fa\u7684\u6846\u67b6\u5728\u8d1d\u53f6\u65af\u4f18\u5316\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u5c24\u5176\u5728\u590d\u6742\u6216\u566a\u58f0\u73af\u5883\u4e2d\u3002"}}
{"id": "2507.06543", "pdf": "https://arxiv.org/pdf/2507.06543", "abs": "https://arxiv.org/abs/2507.06543", "authors": ["Taekyung Kim", "Dongyoon Han", "Byeongho Heo", "Jeongeun Park", "Sangdoo Yun"], "title": "Token Bottleneck: One Token to Remember Dynamics", "categories": ["cs.CV"], "comment": "17 pages, 9 figures, 8 tables, project page:\n  https://token-bottleneck.github.io, code: https://github.com/naver-ai/tobo", "summary": "Deriving compact and temporally aware visual representations from dynamic\nscenes is essential for successful execution of sequential scene understanding\ntasks such as visual tracking and robotic manipulation. In this paper, we\nintroduce Token Bottleneck (ToBo), a simple yet intuitive self-supervised\nlearning pipeline that squeezes a scene into a bottleneck token and predicts\nthe subsequent scene using minimal patches as hints. The ToBo pipeline\nfacilitates the learning of sequential scene representations by conservatively\nencoding the reference scene into a compact bottleneck token during the squeeze\nstep. In the expansion step, we guide the model to capture temporal dynamics by\npredicting the target scene using the bottleneck token along with few target\npatches as hints. This design encourages the vision backbone to embed temporal\ndependencies, thereby enabling understanding of dynamic transitions across\nscenes. Extensive experiments in diverse sequential tasks, including video\nlabel propagation and robot manipulation in simulated environments demonstrate\nthe superiority of ToBo over baselines. Moreover, deploying our pre-trained\nmodel on physical robots confirms its robustness and effectiveness in\nreal-world environments. We further validate the scalability of ToBo across\ndifferent model scales.", "AI": {"tldr": "ToBo\u662f\u4e00\u79cd\u81ea\u76d1\u7763\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u538b\u7f29\u573a\u666f\u4e3a\u74f6\u9888\u4ee4\u724c\u5e76\u9884\u6d4b\u540e\u7eed\u573a\u666f\uff0c\u5b66\u4e60\u52a8\u6001\u573a\u666f\u7684\u7d27\u51d1\u8868\u793a\u3002", "motivation": "\u52a8\u6001\u573a\u666f\u7684\u7d27\u51d1\u548c\u65f6\u95f4\u611f\u77e5\u8868\u793a\u5bf9\u89c6\u89c9\u8ddf\u8e2a\u548c\u673a\u5668\u4eba\u64cd\u4f5c\u7b49\u4efb\u52a1\u81f3\u5173\u91cd\u8981\u3002", "method": "ToBo\u901a\u8fc7\u538b\u7f29\u6b65\u9aa4\u5c06\u53c2\u8003\u573a\u666f\u7f16\u7801\u4e3a\u74f6\u9888\u4ee4\u724c\uff0c\u5e76\u5728\u6269\u5c55\u6b65\u9aa4\u4e2d\u4f7f\u7528\u5c11\u91cf\u76ee\u6807\u8865\u4e01\u9884\u6d4b\u76ee\u6807\u573a\u666f\u3002", "result": "\u5728\u89c6\u9891\u6807\u7b7e\u4f20\u64ad\u548c\u673a\u5668\u4eba\u64cd\u4f5c\u7b49\u4efb\u52a1\u4e2d\uff0cToBo\u8868\u73b0\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5e76\u5728\u771f\u5b9e\u73af\u5883\u4e2d\u9a8c\u8bc1\u4e86\u5176\u9c81\u68d2\u6027\u3002", "conclusion": "ToBo\u80fd\u6709\u6548\u5b66\u4e60\u52a8\u6001\u573a\u666f\u7684\u65f6\u5e8f\u4f9d\u8d56\uff0c\u9002\u7528\u4e8e\u4e0d\u540c\u89c4\u6a21\u7684\u6a21\u578b\u3002"}}
{"id": "2507.06273", "pdf": "https://arxiv.org/pdf/2507.06273", "abs": "https://arxiv.org/abs/2507.06273", "authors": ["S P Shivakumar", "Gunisetty Ramasekhar", "P Nimmy", "Sujesh Areekara", "L Thanuja", "T V Smitha", "S Devanathan", "Ganesh R Naik", "K V Nagaraja"], "title": "Magneto-radiative modelling and artificial neural network optimization of biofluid flow in a stenosed arterial domain", "categories": ["physics.med-ph", "cs.AI", "cs.NA", "math.NA", "physics.bio-ph"], "comment": null, "summary": "The increasing complexity of cardiovascular diseases and limitations in\ntraditional healing methods mandate the invention of new drug delivery systems\nthat assure targeted, effective, and regulated treatments, contributing\ndirectly to UN SDGs 3 and 9, thereby encouraging the utilization of sustainable\nmedical technologies in healthcare. This study investigates the flow of a\nCasson-Maxwell nanofluid through a stenosed arterial domain. The quantities,\nsuch as skin friction and heat transfer rate, are analysed in detail. The\nCasson-Maxwell fluid shows a lower velocity profile than the Casson fluids,\nwhich indicates the improved residence time for efficient drug delivery. The\nheat transfer rate shows an increase with higher volume fractions of copper and\naluminium oxide nanoparticles and a decrease with higher volume fractions of\nsilver nanoparticles. The skin friction coefficient decreases by 219% with a\nunit increase in the Maxwell parameter, whereas it increases by 66.1% with a\nunit rise in the Casson parameter. This work supports SDGs 4 and 17 by\nfostering interdisciplinary learning and collaboration in fluid dynamics and\nhealthcare innovation. Additionally, the rate of heat flow was forecasted (with\nan overall R-value of 0.99457) using the Levenberg-Marquardt backpropagation\ntraining scheme under the influence of magneto-radiative, linear heat source\nand Casson-Maxwell parameters along with the tri-metallic nanoparticle volume\nfractions. It is also observed that the drag coefficient is most sensitive to\nthe changes in the Maxwell parameter.", "AI": {"tldr": "\u7814\u7a76\u63a2\u8ba8\u4e86Casson-Maxwell\u7eb3\u7c73\u6d41\u4f53\u5728\u72ed\u7a84\u52a8\u8109\u4e2d\u7684\u6d41\u52a8\u7279\u6027\uff0c\u5206\u6790\u4e86\u5176\u5bf9\u836f\u7269\u8f93\u9001\u6548\u7387\u548c\u70ed\u4f20\u9012\u7684\u5f71\u54cd\uff0c\u652f\u6301\u53ef\u6301\u7eed\u53d1\u5c55\u76ee\u6807\u3002", "motivation": "\u5fc3\u8840\u7ba1\u75be\u75c5\u590d\u6742\u6027\u589e\u52a0\uff0c\u4f20\u7edf\u6cbb\u7597\u65b9\u6cd5\u53d7\u9650\uff0c\u9700\u8981\u5f00\u53d1\u65b0\u578b\u9776\u5411\u836f\u7269\u8f93\u9001\u7cfb\u7edf\u4ee5\u652f\u6301\u53ef\u6301\u7eed\u53d1\u5c55\u76ee\u6807\u3002", "method": "\u7814\u7a76Casson-Maxwell\u7eb3\u7c73\u6d41\u4f53\u5728\u72ed\u7a84\u52a8\u8109\u4e2d\u7684\u6d41\u52a8\uff0c\u5206\u6790\u76ae\u80a4\u6469\u64e6\u548c\u70ed\u4f20\u9012\u7387\uff0c\u5e76\u5229\u7528Levenberg-Marquardt\u53cd\u5411\u4f20\u64ad\u8bad\u7ec3\u9884\u6d4b\u70ed\u6d41\u7387\u3002", "result": "Casson-Maxwell\u6d41\u4f53\u6d41\u901f\u8f83\u4f4e\uff0c\u836f\u7269\u8f93\u9001\u6548\u7387\u66f4\u9ad8\uff1b\u70ed\u4f20\u9012\u7387\u968f\u94dc\u548c\u6c27\u5316\u94dd\u7eb3\u7c73\u9897\u7c92\u589e\u52a0\u800c\u63d0\u9ad8\uff0c\u94f6\u7eb3\u7c73\u9897\u7c92\u5219\u76f8\u53cd\uff1b\u76ae\u80a4\u6469\u64e6\u7cfb\u6570\u53d7Maxwell\u548cCasson\u53c2\u6570\u663e\u8457\u5f71\u54cd\u3002", "conclusion": "\u7814\u7a76\u4e3a\u9776\u5411\u836f\u7269\u8f93\u9001\u548c\u70ed\u4f20\u9012\u4f18\u5316\u63d0\u4f9b\u4e86\u65b0\u89c1\u89e3\uff0c\u652f\u6301\u53ef\u6301\u7eed\u53d1\u5c55\u76ee\u6807\u4e2d\u7684\u533b\u7597\u6280\u672f\u521b\u65b0\u548c\u8de8\u5b66\u79d1\u5408\u4f5c\u3002"}}
{"id": "2507.06441", "pdf": "https://arxiv.org/pdf/2507.06441", "abs": "https://arxiv.org/abs/2507.06441", "authors": ["Shanting Wang", "Panagiotis Typaldos", "Chenjun Li", "Andreas A. Malikopoulos"], "title": "VisioPath: Vision-Language Enhanced Model Predictive Control for Safe Autonomous Navigation in Mixed Traffic", "categories": ["eess.SY", "cs.RO", "cs.SY"], "comment": null, "summary": "In this paper, we introduce VisioPath, a novel framework combining\nvision-language models (VLMs) with model predictive control (MPC) to enable\nsafe autonomous driving in dynamic traffic environments. The proposed approach\nleverages a bird's-eye view video processing pipeline and zero-shot VLM\ncapabilities to obtain structured information about surrounding vehicles,\nincluding their positions, dimensions, and velocities. Using this rich\nperception output, we construct elliptical collision-avoidance potential fields\naround other traffic participants, which are seamlessly integrated into a\nfinite-horizon optimal control problem for trajectory planning. The resulting\ntrajectory optimization is solved via differential dynamic programming with an\nadaptive regularization scheme and is embedded in an event-triggered MPC loop.\nTo ensure collision-free motion, a safety verification layer is incorporated in\nthe framework that provides an assessment of potential unsafe trajectories.\nExtensive simulations in Simulation of Urban Mobility (SUMO) demonstrate that\nVisioPath outperforms conventional MPC baselines across multiple metrics. By\ncombining modern AI-driven perception with the rigorous foundation of optimal\ncontrol, VisioPath represents a significant step forward in safe trajectory\nplanning for complex traffic systems.", "AI": {"tldr": "VisioPath\u7ed3\u5408\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u548c\u6a21\u578b\u9884\u6d4b\u63a7\u5236\uff0c\u5b9e\u73b0\u52a8\u6001\u4ea4\u901a\u73af\u5883\u4e0b\u7684\u5b89\u5168\u81ea\u52a8\u9a7e\u9a76\u3002", "motivation": "\u89e3\u51b3\u52a8\u6001\u4ea4\u901a\u73af\u5883\u4e2d\u81ea\u52a8\u9a7e\u9a76\u7684\u5b89\u5168\u8f68\u8ff9\u89c4\u5212\u95ee\u9898\u3002", "method": "\u5229\u7528\u9e1f\u77b0\u89c6\u9891\u5904\u7406\u548c\u96f6\u6837\u672c\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u83b7\u53d6\u8f66\u8f86\u4fe1\u606f\uff0c\u6784\u5efa\u692d\u5706\u78b0\u649e\u907f\u514d\u52bf\u573a\uff0c\u5e76\u901a\u8fc7\u5dee\u5206\u52a8\u6001\u89c4\u5212\u6c42\u89e3\u6700\u4f18\u63a7\u5236\u95ee\u9898\u3002", "result": "\u5728SUMO\u6a21\u62df\u4e2d\uff0cVisioPath\u4f18\u4e8e\u4f20\u7edfMPC\u65b9\u6cd5\u3002", "conclusion": "VisioPath\u4e3a\u590d\u6742\u4ea4\u901a\u7cfb\u7edf\u7684\u5b89\u5168\u8f68\u8ff9\u89c4\u5212\u63d0\u4f9b\u4e86\u91cd\u8981\u8fdb\u5c55\u3002"}}
{"id": "2507.06623", "pdf": "https://arxiv.org/pdf/2507.06623", "abs": "https://arxiv.org/abs/2507.06623", "authors": ["James Stewart-Evans", "Emma Wilson", "Tessa Langley", "Andrew Prayle", "Angela Hands", "Karen Exley", "Jo Leonardi-Bee"], "title": "Expediting data extraction using a large language model (LLM) and scoping review protocol: a methodological study within a complex scoping review", "categories": ["cs.CL", "cs.AI"], "comment": "44 pages, 4 figures", "summary": "The data extraction stages of reviews are resource-intensive, and researchers\nmay seek to expediate data extraction using online (large language models) LLMs\nand review protocols. Claude 3.5 Sonnet was used to trial two approaches that\nused a review protocol to prompt data extraction from 10 evidence sources\nincluded in a case study scoping review. A protocol-based approach was also\nused to review extracted data. Limited performance evaluation was undertaken\nwhich found high accuracy for the two extraction approaches (83.3% and 100%)\nwhen extracting simple, well-defined citation details; accuracy was lower (9.6%\nand 15.8%) when extracting more complex, subjective data items. Considering all\ndata items, both approaches had precision >90% but low recall (<25%) and F1\nscores (<40%). The context of a complex scoping review, open response types and\nmethodological approach likely impacted performance due to missed and\nmisattributed data. LLM feedback considered the baseline extraction accurate\nand suggested minor amendments: four of 15 (26.7%) to citation details and 8 of\n38 (21.1%) to key findings data items were considered to potentially add value.\nHowever, when repeating the process with a dataset featuring deliberate errors,\nonly 2 of 39 (5%) errors were detected. Review-protocol-based methods used for\nexpediency require more robust performance evaluation across a range of LLMs\nand review contexts with comparison to conventional prompt engineering\napproaches. We recommend researchers evaluate and report LLM performance if\nusing them similarly to conduct data extraction or review extracted data. LLM\nfeedback contributed to protocol adaptation and may assist future review\nprotocol drafting.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u4f7f\u7528Claude 3.5 Sonnet\u548c\u5ba1\u67e5\u534f\u8bae\u8fdb\u884c\u6570\u636e\u63d0\u53d6\u7684\u4e24\u79cd\u65b9\u6cd5\uff0c\u53d1\u73b0\u5176\u5728\u7b80\u5355\u6570\u636e\u63d0\u53d6\u4e2d\u51c6\u786e\u6027\u9ad8\uff0c\u4f46\u5728\u590d\u6742\u6570\u636e\u4e2d\u8868\u73b0\u4e0d\u4f73\uff0c\u5efa\u8bae\u8fdb\u4e00\u6b65\u8bc4\u4f30\u548c\u62a5\u544aLLM\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u6570\u636e\u63d0\u53d6\u9636\u6bb5\u8d44\u6e90\u5bc6\u96c6\u7684\u95ee\u9898\uff0c\u5c1d\u8bd5\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u548c\u5ba1\u67e5\u534f\u8bae\u63d0\u9ad8\u6548\u7387\u3002", "method": "\u4f7f\u7528Claude 3.5 Sonnet\u548c\u5ba1\u67e5\u534f\u8bae\u5bf910\u4e2a\u8bc1\u636e\u6e90\u8fdb\u884c\u6570\u636e\u63d0\u53d6\uff0c\u5e76\u8bc4\u4f30\u5176\u6027\u80fd\u3002", "result": "\u7b80\u5355\u6570\u636e\u63d0\u53d6\u51c6\u786e\u7387\u9ad8\uff0883.3%\u548c100%\uff09\uff0c\u590d\u6742\u6570\u636e\u63d0\u53d6\u51c6\u786e\u7387\u4f4e\uff089.6%\u548c15.8%\uff09\uff1b\u603b\u4f53\u7cbe\u5ea6>90%\uff0c\u4f46\u53ec\u56de\u7387\u548cF1\u5206\u6570\u4f4e\u3002", "conclusion": "\u5efa\u8bae\u5728\u66f4\u5e7f\u6cdb\u7684LLM\u548c\u5ba1\u67e5\u80cc\u666f\u4e0b\u8fdb\u884c\u66f4\u4e25\u683c\u7684\u6027\u80fd\u8bc4\u4f30\uff0c\u5e76\u62a5\u544aLLM\u6027\u80fd\u4ee5\u652f\u6301\u672a\u6765\u7814\u7a76\u3002"}}
{"id": "2507.06535", "pdf": "https://arxiv.org/pdf/2507.06535", "abs": "https://arxiv.org/abs/2507.06535", "authors": ["Shan Shen", "Shenglu Hua", "Jiajun Zou", "Jiawei Liu", "Jianwang Zhai", "Chuan Shi", "Wenjian Yu"], "title": "Transferable Parasitic Estimation via Graph Contrastive Learning and Label Rebalancing in AMS Circuits", "categories": ["cs.LG", "cs.SY", "eess.SY"], "comment": "Accepted by ICCAD2025. This is the initial version. Minor changes\n  will be made", "summary": "Graph representation learning on Analog-Mixed Signal (AMS) circuits is\ncrucial for various downstream tasks, e.g., parasitic estimation. However, the\nscarcity of design data, the unbalanced distribution of labels, and the\ninherent diversity of circuit implementations pose significant challenges to\nlearning robust and transferable circuit representations. To address these\nlimitations, we propose CircuitGCL, a novel graph contrastive learning\nframework that integrates representation scattering and label rebalancing to\nenhance transferability across heterogeneous circuit graphs. CircuitGCL employs\na self-supervised strategy to learn topology-invariant node embeddings through\nhyperspherical representation scattering, eliminating dependency on large-scale\ndata. Simultaneously, balanced mean squared error (MSE) and softmax\ncross-entropy (bsmCE) losses are introduced to mitigate label distribution\ndisparities between circuits, enabling robust and transferable parasitic\nestimation. Evaluated on parasitic capacitance estimation (edge-level task) and\nground capacitance classification (node-level task) across TSMC 28nm AMS\ndesigns, CircuitGCL outperforms all state-of-the-art (SOTA) methods, with the\n$R^2$ improvement of $33.64\\% \\sim 44.20\\%$ for edge regression and F1-score\ngain of $0.9\\times \\sim 2.1\\times$ for node classification. Our code is\navailable at\n\\href{https://anonymous.4open.science/r/CircuitGCL-099B/README.md}{here}.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faCircuitGCL\uff0c\u4e00\u79cd\u56fe\u5bf9\u6bd4\u5b66\u4e60\u6846\u67b6\uff0c\u7528\u4e8e\u89e3\u51b3AMS\u7535\u8def\u4e2d\u6570\u636e\u7a00\u7f3a\u3001\u6807\u7b7e\u4e0d\u5e73\u8861\u548c\u7535\u8def\u591a\u6837\u6027\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u5bc4\u751f\u4f30\u8ba1\u4efb\u52a1\u6027\u80fd\u3002", "motivation": "AMS\u7535\u8def\u8bbe\u8ba1\u6570\u636e\u7a00\u7f3a\u3001\u6807\u7b7e\u5206\u5e03\u4e0d\u5e73\u8861\u4e14\u7535\u8def\u5b9e\u73b0\u591a\u6837\uff0c\u5bfc\u81f4\u5b66\u4e60\u9c81\u68d2\u4e14\u53ef\u8fc1\u79fb\u7684\u7535\u8def\u8868\u793a\u5177\u6709\u6311\u6218\u6027\u3002", "method": "CircuitGCL\u7ed3\u5408\u8868\u793a\u6563\u5c04\u548c\u6807\u7b7e\u91cd\u5e73\u8861\uff0c\u901a\u8fc7\u81ea\u76d1\u7763\u5b66\u4e60\u62d3\u6251\u4e0d\u53d8\u8282\u70b9\u5d4c\u5165\uff0c\u5e76\u4f7f\u7528\u5e73\u8861MSE\u548cbsmCE\u635f\u5931\u7f13\u89e3\u6807\u7b7e\u5206\u5e03\u5dee\u5f02\u3002", "result": "\u5728TSMC 28nm AMS\u8bbe\u8ba1\u4e2d\uff0cCircuitGCL\u5728\u8fb9\u7ea7\u5bc4\u751f\u7535\u5bb9\u4f30\u8ba1\u548c\u8282\u70b9\u7ea7\u63a5\u5730\u7535\u5bb9\u5206\u7c7b\u4efb\u52a1\u4e2d\u5747\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0cR\u00b2\u63d0\u534733.64%~44.20%\uff0cF1\u5206\u6570\u63d0\u53470.9~2.1\u500d\u3002", "conclusion": "CircuitGCL\u901a\u8fc7\u8868\u793a\u6563\u5c04\u548c\u6807\u7b7e\u91cd\u5e73\u8861\uff0c\u663e\u8457\u63d0\u5347\u4e86\u7535\u8def\u56fe\u8868\u793a\u5b66\u4e60\u7684\u9c81\u68d2\u6027\u548c\u53ef\u8fc1\u79fb\u6027\uff0c\u9002\u7528\u4e8e\u591a\u79cd\u4e0b\u6e38\u4efb\u52a1\u3002"}}
{"id": "2507.06547", "pdf": "https://arxiv.org/pdf/2507.06547", "abs": "https://arxiv.org/abs/2507.06547", "authors": ["Yonghyun Park", "Chieh-Hsin Lai", "Satoshi Hayakawa", "Yuhta Takida", "Naoki Murata", "Wei-Hsiang Liao", "Woosung Choi", "Kin Wai Cheuk", "Junghyun Koo", "Yuki Mitsufuji"], "title": "Concept-TRAK: Understanding how diffusion models learn concepts through concept-level attribution", "categories": ["cs.CV", "cs.LG"], "comment": "Preprint", "summary": "While diffusion models excel at image generation, their growing adoption\nraises critical concerns around copyright issues and model transparency.\nExisting attribution methods identify training examples influencing an entire\nimage, but fall short in isolating contributions to specific elements, such as\nstyles or objects, that matter most to stakeholders. To bridge this gap, we\nintroduce \\emph{concept-level attribution} via a novel method called\n\\emph{Concept-TRAK}. Concept-TRAK extends influence functions with two key\ninnovations: (1) a reformulated diffusion training loss based on diffusion\nposterior sampling, enabling robust, sample-specific attribution; and (2) a\nconcept-aware reward function that emphasizes semantic relevance. We evaluate\nConcept-TRAK on the AbC benchmark, showing substantial improvements over prior\nmethods. Through diverse case studies--ranging from identifying IP-protected\nand unsafe content to analyzing prompt engineering and compositional\nlearning--we demonstrate how concept-level attribution yields actionable\ninsights for responsible generative AI development and governance.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aConcept-TRAK\u7684\u65b0\u65b9\u6cd5\uff0c\u7528\u4e8e\u89e3\u51b3\u6269\u6563\u6a21\u578b\u5728\u56fe\u50cf\u751f\u6210\u4e2d\u7684\u7248\u6743\u548c\u900f\u660e\u5ea6\u95ee\u9898\uff0c\u901a\u8fc7\u6982\u5ff5\u7ea7\u5f52\u56e0\u63d0\u4f9b\u66f4\u7ec6\u7c92\u5ea6\u7684\u5206\u6790\u3002", "motivation": "\u6269\u6563\u6a21\u578b\u5728\u56fe\u50cf\u751f\u6210\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u5176\u5e7f\u6cdb\u4f7f\u7528\u5f15\u53d1\u4e86\u7248\u6743\u548c\u900f\u660e\u5ea6\u95ee\u9898\uff0c\u73b0\u6709\u65b9\u6cd5\u65e0\u6cd5\u5bf9\u56fe\u50cf\u4e2d\u7684\u7279\u5b9a\u5143\u7d20\uff08\u5982\u98ce\u683c\u6216\u5bf9\u8c61\uff09\u8fdb\u884c\u5f52\u56e0\u3002", "method": "\u63d0\u51fa\u4e86Concept-TRAK\u65b9\u6cd5\uff0c\u6269\u5c55\u4e86\u5f71\u54cd\u51fd\u6570\uff0c\u5305\u62ec\u57fa\u4e8e\u6269\u6563\u540e\u9a8c\u91c7\u6837\u7684\u8bad\u7ec3\u635f\u5931\u91cd\u6784\u548c\u6982\u5ff5\u611f\u77e5\u5956\u52b1\u51fd\u6570\u3002", "result": "\u5728AbC\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cConcept-TRAK\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5e76\u901a\u8fc7\u6848\u4f8b\u7814\u7a76\u5c55\u793a\u4e86\u5176\u5728\u7248\u6743\u4fdd\u62a4\u3001\u5185\u5bb9\u5b89\u5168\u548c\u751f\u6210AI\u6cbb\u7406\u4e2d\u7684\u5b9e\u7528\u6027\u3002", "conclusion": "Concept-TRAK\u4e3a\u751f\u6210AI\u7684\u8d1f\u8d23\u4efb\u5f00\u53d1\u548c\u6cbb\u7406\u63d0\u4f9b\u4e86\u53ef\u64cd\u4f5c\u7684\u89c1\u89e3\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u5c40\u9650\u6027\u3002"}}
{"id": "2507.06274", "pdf": "https://arxiv.org/pdf/2507.06274", "abs": "https://arxiv.org/abs/2507.06274", "authors": ["Huanming Shen", "Baizhou Huang", "Xiaojun Wan"], "title": "Enhancing LLM Watermark Resilience Against Both Scrubbing and Spoofing Attacks", "categories": ["cs.CR", "cs.AI"], "comment": null, "summary": "Watermarking is a promising defense against the misuse of large language\nmodels (LLMs), yet it remains vulnerable to scrubbing and spoofing attacks.\nThis vulnerability stems from an inherent trade-off governed by watermark\nwindow size: smaller windows resist scrubbing better but are easier to\nreverse-engineer, enabling low-cost statistics-based spoofing attacks. This\nwork breaks this trade-off by introducing a novel mechanism, equivalent texture\nkeys, where multiple tokens within a watermark window can independently support\nthe detection. Based on the redundancy, we propose a novel watermark scheme\nwith Sub-vocabulary decomposed Equivalent tExture Key (SEEK). It achieves a\nPareto improvement, increasing the resilience against scrubbing attacks without\ncompromising robustness to spoofing. Experiments demonstrate SEEK's superiority\nover prior method, yielding spoofing robustness gains of +88.2%/+92.3%/+82.0%\nand scrubbing robustness gains of +10.2%/+6.4%/+24.6% across diverse dataset\nsettings.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aSEEK\u7684\u65b0\u578b\u6c34\u5370\u673a\u5236\uff0c\u901a\u8fc7\u7b49\u6548\u7eb9\u7406\u5bc6\u94a5\u6253\u7834\u4f20\u7edf\u6c34\u5370\u7a97\u53e3\u5927\u5c0f\u7684\u6743\u8861\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6297\u64e6\u9664\u548c\u6297\u6b3a\u9a97\u653b\u51fb\u7684\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u6c34\u5370\u6280\u672f\u5728\u6297\u64e6\u9664\u548c\u6297\u6b3a\u9a97\u653b\u51fb\u4e4b\u95f4\u5b58\u5728\u56fa\u6709\u6743\u8861\uff0cSEEK\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u5f15\u5165\u7b49\u6548\u7eb9\u7406\u5bc6\u94a5\uff0c\u5229\u7528\u5b50\u8bcd\u6c47\u5206\u89e3\u7684\u7b49\u6548\u7eb9\u7406\u5bc6\u94a5\uff08SEEK\uff09\u673a\u5236\uff0c\u901a\u8fc7\u5197\u4f59\u8bbe\u8ba1\u589e\u5f3a\u6c34\u5370\u7684\u9c81\u68d2\u6027\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\uff0cSEEK\u5728\u6297\u6b3a\u9a97\u653b\u51fb\u4e0a\u63d0\u5347\u4e8688.2%/92.3%/82.0%\uff0c\u5728\u6297\u64e6\u9664\u653b\u51fb\u4e0a\u63d0\u5347\u4e8610.2%/6.4%/24.6%\u3002", "conclusion": "SEEK\u673a\u5236\u5b9e\u73b0\u4e86\u5e15\u7d2f\u6258\u6539\u8fdb\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6c34\u5370\u6280\u672f\u7684\u9c81\u68d2\u6027\u3002"}}
{"id": "2507.06662", "pdf": "https://arxiv.org/pdf/2507.06662", "abs": "https://arxiv.org/abs/2507.06662", "authors": ["Yifan Yang", "Peili Song", "Enfan Lan", "Dong Liu", "Jingtai Liu"], "title": "MK-Pose: Category-Level Object Pose Estimation via Multimodal-Based Keypoint Learning", "categories": ["cs.CV", "cs.RO"], "comment": null, "summary": "Category-level object pose estimation, which predicts the pose of objects\nwithin a known category without prior knowledge of individual instances, is\nessential in applications like warehouse automation and manufacturing. Existing\nmethods relying on RGB images or point cloud data often struggle with object\nocclusion and generalization across different instances and categories. This\npaper proposes a multimodal-based keypoint learning framework (MK-Pose) that\nintegrates RGB images, point clouds, and category-level textual descriptions.\nThe model uses a self-supervised keypoint detection module enhanced with\nattention-based query generation, soft heatmap matching and graph-based\nrelational modeling. Additionally, a graph-enhanced feature fusion module is\ndesigned to integrate local geometric information and global context. MK-Pose\nis evaluated on CAMERA25 and REAL275 dataset, and is further tested for\ncross-dataset capability on HouseCat6D dataset. The results demonstrate that\nMK-Pose outperforms existing state-of-the-art methods in both IoU and average\nprecision without shape priors. Codes will be released at\n\\href{https://github.com/yangyifanYYF/MK-Pose}{https://github.com/yangyifanYYF/MK-Pose}.", "AI": {"tldr": "MK-Pose\u662f\u4e00\u79cd\u591a\u6a21\u6001\u5173\u952e\u70b9\u5b66\u4e60\u6846\u67b6\uff0c\u7ed3\u5408RGB\u56fe\u50cf\u3001\u70b9\u4e91\u548c\u7c7b\u522b\u6587\u672c\u63cf\u8ff0\uff0c\u7528\u4e8e\u7c7b\u522b\u7ea7\u7269\u4f53\u59ff\u6001\u4f30\u8ba1\uff0c\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\u5728\u7269\u4f53\u906e\u6321\u548c\u8de8\u5b9e\u4f8b\u3001\u8de8\u7c7b\u522b\u6cdb\u5316\u80fd\u529b\u4e0a\u7684\u4e0d\u8db3\u3002", "method": "\u63d0\u51fa\u81ea\u76d1\u7763\u5173\u952e\u70b9\u68c0\u6d4b\u6a21\u5757\uff0c\u7ed3\u5408\u6ce8\u610f\u529b\u67e5\u8be2\u751f\u6210\u3001\u8f6f\u70ed\u56fe\u5339\u914d\u548c\u56fe\u5173\u7cfb\u5efa\u6a21\uff0c\u5e76\u8bbe\u8ba1\u56fe\u589e\u5f3a\u7279\u5f81\u878d\u5408\u6a21\u5757\u3002", "result": "\u5728CAMERA25\u3001REAL275\u548cHouseCat6D\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u5f02\uff0cIoU\u548c\u5e73\u5747\u7cbe\u5ea6\u8d85\u8d8a\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "MK-Pose\u901a\u8fc7\u591a\u6a21\u6001\u878d\u5408\u548c\u56fe\u5efa\u6a21\u663e\u8457\u63d0\u5347\u4e86\u7c7b\u522b\u7ea7\u59ff\u6001\u4f30\u8ba1\u7684\u6027\u80fd\u3002"}}
{"id": "2507.06658", "pdf": "https://arxiv.org/pdf/2507.06658", "abs": "https://arxiv.org/abs/2507.06658", "authors": ["Gennadii Iakovlev"], "title": "Elite Polarization in European Parliamentary Speeches: a Novel Measurement Approach Using Large Language Models", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "This project introduces a new measure of elite polarization via actor and\nsubject detection using artificial intelligence. I identify when politicians\nmention one another in parliamentary speeches, note who is speaking and who is\nbeing addressed, and assess the emotional temperature behind these evaluations.\nThis maps how elites evaluate their various out-parties, allowing us to create\nan index of mutual out-party hostility, that is, elite polarization. While I\nanalyzed polarization data over the past four decades for the UK, and two\ndecades for Hungary and Italy, my approach lays the groundwork for a\ntwenty-year, EU-wide time-series dataset on elite polarization. I obtain the\nresults that can be aggregated by party and quarter. The resulting index\ndemonstrates a good face validity: it reacts to events such as electoral\ncampaigns, country- and party-level crises, and to parties losing and assuming\npower.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u4eba\u5de5\u667a\u80fd\u7684\u7cbe\u82f1\u6781\u5316\u6d4b\u91cf\u65b9\u6cd5\uff0c\u901a\u8fc7\u5206\u6790\u653f\u5ba2\u5728\u8bae\u4f1a\u6f14\u8bb2\u4e2d\u63d0\u53ca\u5f7c\u6b64\u7684\u60c5\u51b5\uff0c\u8bc4\u4f30\u60c5\u7eea\u6e29\u5ea6\uff0c\u6784\u5efa\u76f8\u4e92\u654c\u610f\u6307\u6570\u3002", "motivation": "\u7814\u7a76\u7cbe\u82f1\u6781\u5316\u73b0\u8c61\uff0c\u901a\u8fc7\u91cf\u5316\u653f\u5ba2\u95f4\u7684\u76f8\u4e92\u8bc4\u4ef7\u548c\u60c5\u7eea\u53cd\u5e94\uff0c\u63ed\u793a\u653f\u6cbb\u5bf9\u7acb\u7a0b\u5ea6\u3002", "method": "\u5229\u7528\u4eba\u5de5\u667a\u80fd\u68c0\u6d4b\u6f14\u8bb2\u4e2d\u7684\u63d0\u53ca\u5bf9\u8c61\u548c\u60c5\u7eea\uff0c\u6784\u5efa\u65f6\u95f4\u5e8f\u5217\u6570\u636e\u96c6\uff0c\u5206\u6790\u82f1\u56fd\u3001\u5308\u7259\u5229\u548c\u610f\u5927\u5229\u7684\u6570\u636e\u3002", "result": "\u751f\u6210\u7684\u6307\u6570\u5bf9\u9009\u4e3e\u6d3b\u52a8\u3001\u653f\u515a\u5371\u673a\u7b49\u4e8b\u4ef6\u53cd\u5e94\u826f\u597d\uff0c\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u6b27\u76df\u8303\u56f4\u5185\u7684\u7cbe\u82f1\u6781\u5316\u7814\u7a76\u63d0\u4f9b\u4e86\u57fa\u7840\uff0c\u672a\u6765\u53ef\u6269\u5c55\u4e3a20\u5e74\u7684\u65f6\u95f4\u5e8f\u5217\u6570\u636e\u96c6\u3002"}}
{"id": "2507.06538", "pdf": "https://arxiv.org/pdf/2507.06538", "abs": "https://arxiv.org/abs/2507.06538", "authors": ["Shan Shen", "Yibin Zhang", "Hector Rodriguez Rodriguez", "Wenjian Yu"], "title": "Few-shot Learning on AMS Circuits and Its Application to Parasitic Capacitance Prediction", "categories": ["cs.LG", "cs.SY", "eess.SY"], "comment": "Published in Proceedings of DAC2025", "summary": "Graph representation learning is a powerful method to extract features from\ngraph-structured data, such as analog/mixed-signal (AMS) circuits. However,\ntraining deep learning models for AMS designs is severely limited by the\nscarcity of integrated circuit design data. In this work, we present\nCircuitGPS, a few-shot learning method for parasitic effect prediction in AMS\ncircuits. The circuit netlist is represented as a heterogeneous graph, with the\ncoupling capacitance modeled as a link. CircuitGPS is pre-trained on link\nprediction and fine-tuned on edge regression. The proposed method starts with a\nsmall-hop sampling technique that converts a link or a node into a subgraph.\nThen, the subgraph embeddings are learned with a hybrid graph Transformer.\nAdditionally, CircuitGPS integrates a low-cost positional encoding that\nsummarizes the positional and structural information of the sampled subgraph.\nCircuitGPS improves the accuracy of coupling existence by at least 20\\% and\nreduces the MAE of capacitance estimation by at least 0.067 compared to\nexisting methods. Our method demonstrates strong inherent scalability, enabling\ndirect application to diverse AMS circuit designs through zero-shot learning.\nFurthermore, the ablation studies provide valuable insights into graph models\nfor representation learning.", "AI": {"tldr": "CircuitGPS\u662f\u4e00\u79cd\u7528\u4e8eAMS\u7535\u8def\u5bc4\u751f\u6548\u5e94\u9884\u6d4b\u7684\u5c0f\u6837\u672c\u5b66\u4e60\u65b9\u6cd5\uff0c\u901a\u8fc7\u5f02\u6784\u56fe\u8868\u793a\u548c\u5b50\u56fe\u91c7\u6837\u6280\u672f\uff0c\u7ed3\u5408\u56feTransformer\uff0c\u663e\u8457\u63d0\u5347\u4e86\u9884\u6d4b\u7cbe\u5ea6\u548c\u53ef\u6269\u5c55\u6027\u3002", "motivation": "\u7531\u4e8e\u96c6\u6210\u7535\u8def\u8bbe\u8ba1\u6570\u636e\u7684\u7a00\u7f3a\u6027\uff0c\u8bad\u7ec3\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u7528\u4e8eAMS\u8bbe\u8ba1\u53d7\u5230\u4e25\u91cd\u9650\u5236\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u5c0f\u6837\u672c\u5b66\u4e60\u65b9\u6cd5\u3002", "method": "CircuitGPS\u5c06\u7535\u8def\u7f51\u8868\u8868\u793a\u4e3a\u5f02\u6784\u56fe\uff0c\u901a\u8fc7\u5c0f\u8df3\u91c7\u6837\u6280\u672f\u751f\u6210\u5b50\u56fe\uff0c\u4f7f\u7528\u6df7\u5408\u56feTransformer\u5b66\u4e60\u5b50\u56fe\u5d4c\u5165\uff0c\u5e76\u96c6\u6210\u4f4e\u6210\u672c\u7684\u4f4d\u7f6e\u7f16\u7801\u3002", "result": "CircuitGPS\u5c06\u8026\u5408\u5b58\u5728\u9884\u6d4b\u7684\u51c6\u786e\u7387\u63d0\u9ad8\u4e86\u81f3\u5c1120%\uff0c\u7535\u5bb9\u4f30\u8ba1\u7684MAE\u964d\u4f4e\u4e86\u81f3\u5c110.067\uff0c\u5e76\u5c55\u793a\u4e86\u5f3a\u5927\u7684\u96f6\u6837\u672c\u5b66\u4e60\u80fd\u529b\u3002", "conclusion": "CircuitGPS\u4e3aAMS\u7535\u8def\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u3001\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5e76\u901a\u8fc7\u6d88\u878d\u7814\u7a76\u4e3a\u56fe\u8868\u793a\u5b66\u4e60\u63d0\u4f9b\u4e86\u6709\u4ef7\u503c\u7684\u89c1\u89e3\u3002"}}
{"id": "2507.06560", "pdf": "https://arxiv.org/pdf/2507.06560", "abs": "https://arxiv.org/abs/2507.06560", "authors": ["Jae Hyoung Jeon", "Cheolsu Lim", "Myungjoo Kang"], "title": "Divergence-Based Similarity Function for Multi-View Contrastive Learning", "categories": ["cs.CV", "cs.LG", "68T07, 62H12", "I.2.6; I.4.8; I.5.1"], "comment": "9 pages, 5 figures", "summary": "Recent success in contrastive learning has sparked growing interest in more\neffectively leveraging multiple augmented views of an instance. While prior\nmethods incorporate multiple views at the loss or feature level, they primarily\ncapture pairwise relationships and fail to model the joint structure across all\nviews. In this work, we propose a divergence-based similarity function (DSF)\nthat explicitly captures the joint structure by representing each set of\naugmented views as a distribution and measuring similarity as the divergence\nbetween distributions. Extensive experiments demonstrate that DSF consistently\nimproves performance across various tasks, including kNN classification and\nlinear evaluation, while also offering greater efficiency compared to other\nmulti-view methods. Furthermore, we establish a theoretical connection between\nDSF and cosine similarity, and show that, unlike cosine similarity, DSF\noperates effectively without requiring a temperature hyperparameter.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5206\u5e03\u6563\u5ea6\u7684\u76f8\u4f3c\u6027\u51fd\u6570\uff08DSF\uff09\uff0c\u901a\u8fc7\u5c06\u591a\u89c6\u56fe\u8868\u793a\u4e3a\u5206\u5e03\u5e76\u6d4b\u91cf\u5206\u5e03\u95f4\u7684\u6563\u5ea6\uff0c\u663e\u5f0f\u6355\u6349\u591a\u89c6\u56fe\u7684\u8054\u5408\u7ed3\u6784\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4e3b\u8981\u6355\u6349\u89c6\u56fe\u95f4\u7684\u6210\u5bf9\u5173\u7cfb\uff0c\u672a\u80fd\u5efa\u6a21\u6240\u6709\u89c6\u56fe\u7684\u8054\u5408\u7ed3\u6784\uff0c\u9650\u5236\u4e86\u591a\u89c6\u56fe\u5b66\u4e60\u7684\u6709\u6548\u6027\u3002", "method": "\u63d0\u51faDSF\uff0c\u5c06\u591a\u89c6\u56fe\u8868\u793a\u4e3a\u5206\u5e03\uff0c\u901a\u8fc7\u5206\u5e03\u95f4\u7684\u6563\u5ea6\u8861\u91cf\u76f8\u4f3c\u6027\u3002", "result": "DSF\u5728kNN\u5206\u7c7b\u548c\u7ebf\u6027\u8bc4\u4f30\u7b49\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4e14\u6548\u7387\u66f4\u9ad8\u3002", "conclusion": "DSF\u65e0\u9700\u6e29\u5ea6\u8d85\u53c2\u6570\u5373\u53ef\u6709\u6548\u5de5\u4f5c\uff0c\u4e14\u7406\u8bba\u8bc1\u660e\u5176\u4e0e\u4f59\u5f26\u76f8\u4f3c\u6027\u76f8\u5173\u3002"}}
{"id": "2507.06687", "pdf": "https://arxiv.org/pdf/2507.06687", "abs": "https://arxiv.org/abs/2507.06687", "authors": ["Marcel Vosshans", "Omar Ait-Aider", "Youcef Mezouar", "Markus Enzweiler"], "title": "StixelNExT++: Lightweight Monocular Scene Segmentation and Representation for Collective Perception", "categories": ["cs.CV", "cs.RO"], "comment": null, "summary": "This paper presents StixelNExT++, a novel approach to scene representation\nfor monocular perception systems. Building on the established Stixel\nrepresentation, our method infers 3D Stixels and enhances object segmentation\nby clustering smaller 3D Stixel units. The approach achieves high compression\nof scene information while remaining adaptable to point cloud and\nbird's-eye-view representations. Our lightweight neural network, trained on\nautomatically generated LiDAR-based ground truth, achieves real-time\nperformance with computation times as low as 10 ms per frame. Experimental\nresults on the Waymo dataset demonstrate competitive performance within a\n30-meter range, highlighting the potential of StixelNExT++ for collective\nperception in autonomous systems.", "AI": {"tldr": "StixelNExT++\u662f\u4e00\u79cd\u7528\u4e8e\u5355\u76ee\u611f\u77e5\u7cfb\u7edf\u7684\u65b0\u578b\u573a\u666f\u8868\u793a\u65b9\u6cd5\uff0c\u901a\u8fc7\u805a\u7c7b3D Stixel\u5355\u5143\u589e\u5f3a\u5bf9\u8c61\u5206\u5272\uff0c\u5b9e\u73b0\u9ad8\u538b\u7f29\u4e14\u9002\u5e94\u70b9\u4e91\u548c\u9e1f\u77b0\u56fe\u8868\u793a\uff0c\u5b9e\u65f6\u6027\u80fd\u8fbe10\u6beb\u79d2/\u5e27\u3002", "motivation": "\u63d0\u5347\u5355\u76ee\u611f\u77e5\u7cfb\u7edf\u7684\u573a\u666f\u8868\u793a\u80fd\u529b\uff0c\u540c\u65f6\u4fdd\u6301\u9ad8\u538b\u7f29\u548c\u5b9e\u65f6\u6027\u80fd\u3002", "method": "\u57fa\u4e8eStixel\u8868\u793a\uff0c\u63a8\u65ad3D Stixels\u5e76\u901a\u8fc7\u805a\u7c7b\u589e\u5f3a\u5206\u5272\uff0c\u4f7f\u7528\u8f7b\u91cf\u7ea7\u795e\u7ecf\u7f51\u7edc\u8bad\u7ec3LiDAR\u751f\u6210\u7684\u771f\u503c\u3002", "result": "\u5728Waymo\u6570\u636e\u96c6\u4e0a30\u7c73\u8303\u56f4\u5185\u8868\u73b0\u4f18\u5f02\uff0c\u8ba1\u7b97\u65f6\u95f4\u4f4e\u81f310\u6beb\u79d2/\u5e27\u3002", "conclusion": "StixelNExT++\u5728\u81ea\u4e3b\u7cfb\u7edf\u4e2d\u5177\u6709\u96c6\u4f53\u611f\u77e5\u6f5c\u529b\u3002"}}
{"id": "2507.06715", "pdf": "https://arxiv.org/pdf/2507.06715", "abs": "https://arxiv.org/abs/2507.06715", "authors": ["Garapati Keerthana", "Manik Gupta"], "title": "CLI-RAG: A Retrieval-Augmented Framework for Clinically Structured and Context Aware Text Generation with LLMs", "categories": ["cs.CL", "cs.AI", "cs.IR"], "comment": "12 pages, 4 figures", "summary": "Large language models (LLMs), including zero-shot and few-shot paradigms,\nhave shown promising capabilities in clinical text generation. However,\nreal-world applications face two key challenges: (1) patient data is highly\nunstructured, heterogeneous, and scattered across multiple note types and (2)\nclinical notes are often long and semantically dense, making naive prompting\ninfeasible due to context length constraints and the risk of omitting\nclinically relevant information.\n  We introduce CLI-RAG (Clinically Informed Retrieval-Augmented Generation), a\ndomain-specific framework for structured and clinically grounded text\ngeneration using LLMs. It incorporates a novel hierarchical chunking strategy\nthat respects clinical document structure and introduces a task-specific\ndual-stage retrieval mechanism. The global stage identifies relevant note types\nusing evidence-based queries, while the local stage extracts high-value content\nwithin those notes creating relevance at both document and section levels.\n  We apply the system to generate structured progress notes for individual\nhospital visits using 15 clinical note types from the MIMIC-III dataset.\nExperiments show that it preserves temporal and semantic alignment across\nvisits, achieving an average alignment score of 87.7%, surpassing the 80.7%\nbaseline from real clinician-authored notes. The generated outputs also\ndemonstrate high consistency across LLMs, reinforcing deterministic behavior\nessential for reproducibility, reliability, and clinical trust.", "AI": {"tldr": "CLI-RAG\u6846\u67b6\u901a\u8fc7\u5206\u5c42\u5206\u5757\u548c\u53cc\u9636\u6bb5\u68c0\u7d22\u673a\u5236\uff0c\u89e3\u51b3\u4e86\u4e34\u5e8a\u6587\u672c\u751f\u6210\u4e2d\u6570\u636e\u5206\u6563\u548c\u8bed\u4e49\u5bc6\u96c6\u7684\u6311\u6218\uff0c\u663e\u8457\u63d0\u5347\u4e86\u751f\u6210\u7b14\u8bb0\u7684\u65f6\u5e8f\u548c\u8bed\u4e49\u5bf9\u9f50\u3002", "motivation": "\u89e3\u51b3\u4e34\u5e8a\u6587\u672c\u751f\u6210\u4e2d\u56e0\u6570\u636e\u5206\u6563\u548c\u8bed\u4e49\u5bc6\u96c6\u5bfc\u81f4\u7684\u63d0\u793a\u4e0d\u5b8c\u6574\u548c\u4fe1\u606f\u9057\u6f0f\u95ee\u9898\u3002", "method": "\u63d0\u51faCLI-RAG\u6846\u67b6\uff0c\u91c7\u7528\u5206\u5c42\u5206\u5757\u7b56\u7565\u548c\u53cc\u9636\u6bb5\u68c0\u7d22\u673a\u5236\uff08\u5168\u5c40\u9636\u6bb5\u8bc6\u522b\u76f8\u5173\u7b14\u8bb0\u7c7b\u578b\uff0c\u5c40\u90e8\u9636\u6bb5\u63d0\u53d6\u9ad8\u4ef7\u503c\u5185\u5bb9\uff09\u3002", "result": "\u5728MIMIC-III\u6570\u636e\u96c6\u4e0a\uff0c\u751f\u6210\u7684\u7ed3\u6784\u5316\u8fdb\u5c55\u7b14\u8bb0\u5e73\u5747\u5bf9\u9f50\u5206\u6570\u8fbe87.7%\uff0c\u4f18\u4e8e\u4e34\u5e8a\u533b\u751f\u7b14\u8bb0\u768480.7%\u3002", "conclusion": "CLI-RAG\u6846\u67b6\u5728\u4e34\u5e8a\u6587\u672c\u751f\u6210\u4e2d\u8868\u73b0\u51fa\u9ad8\u6548\u6027\u548c\u4e00\u81f4\u6027\uff0c\u4e3a\u4e34\u5e8a\u5e94\u7528\u7684\u53ef\u9760\u6027\u548c\u53ef\u91cd\u590d\u6027\u63d0\u4f9b\u4e86\u652f\u6301\u3002"}}
{"id": "2507.06569", "pdf": "https://arxiv.org/pdf/2507.06569", "abs": "https://arxiv.org/abs/2507.06569", "authors": ["Hao Shu"], "title": "Edge-Boundary-Texture Loss: A Tri-Class Generalization of Weighted Binary Cross-Entropy for Enhanced Edge Detection", "categories": ["cs.CV"], "comment": "10 pages", "summary": "Edge detection (ED) remains a fundamental task in computer vision, yet its\nperformance is often hindered by the ambiguous nature of non-edge pixels near\nobject boundaries. The widely adopted Weighted Binary Cross-Entropy (WBCE) loss\ntreats all non-edge pixels uniformly, overlooking the structural nuances around\nedges and often resulting in blurred predictions. In this paper, we propose the\nEdge-Boundary-Texture (EBT) loss, a novel objective that explicitly divides\npixels into three categories, edge, boundary, and texture, and assigns each a\ndistinct supervisory weight. This tri-class formulation enables more structured\nlearning by guiding the model to focus on both edge precision and contextual\nboundary localization. We theoretically show that the EBT loss generalizes the\nWBCE loss, with the latter becoming a limit case. Extensive experiments across\nmultiple benchmarks demonstrate the superiority of the EBT loss both\nquantitatively and perceptually. Furthermore, the consistent use of unified\nhyperparameters across all models and datasets, along with robustness to their\nmoderate variations, indicates that the EBT loss requires minimal fine-tuning\nand is easily deployable in practice.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u635f\u5931\u51fd\u6570EBT\uff0c\u901a\u8fc7\u5c06\u50cf\u7d20\u5206\u4e3a\u8fb9\u7f18\u3001\u8fb9\u754c\u548c\u7eb9\u7406\u4e09\u7c7b\uff0c\u5e76\u8d4b\u4e88\u4e0d\u540c\u7684\u76d1\u7763\u6743\u91cd\uff0c\u6539\u8fdb\u4e86\u8fb9\u7f18\u68c0\u6d4b\u4efb\u52a1\u3002", "motivation": "\u4f20\u7edf\u7684WBCE\u635f\u5931\u51fd\u6570\u5bf9\u6240\u6709\u975e\u8fb9\u7f18\u50cf\u7d20\u4e00\u89c6\u540c\u4ec1\uff0c\u5ffd\u7565\u4e86\u8fb9\u7f18\u9644\u8fd1\u7684\u7ed3\u6784\u5dee\u5f02\uff0c\u5bfc\u81f4\u9884\u6d4b\u6a21\u7cca\u3002", "method": "\u63d0\u51faEBT\u635f\u5931\u51fd\u6570\uff0c\u5c06\u50cf\u7d20\u5206\u4e3a\u8fb9\u7f18\u3001\u8fb9\u754c\u548c\u7eb9\u7406\u4e09\u7c7b\uff0c\u5e76\u4e3a\u6bcf\u7c7b\u5206\u914d\u4e0d\u540c\u7684\u76d1\u7763\u6743\u91cd\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660eEBT\u635f\u5931\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4e14\u5bf9\u8d85\u53c2\u6570\u53d8\u5316\u9c81\u68d2\u3002", "conclusion": "EBT\u635f\u5931\u51fd\u6570\u6613\u4e8e\u90e8\u7f72\uff0c\u4e14\u663e\u8457\u63d0\u5347\u4e86\u8fb9\u7f18\u68c0\u6d4b\u7684\u6027\u80fd\u3002"}}
{"id": "2507.06277", "pdf": "https://arxiv.org/pdf/2507.06277", "abs": "https://arxiv.org/abs/2507.06277", "authors": ["Maxim Chupilkin"], "title": "The Prompt War: How AI Decides on a Military Intervention", "categories": ["cs.CY", "cs.AI"], "comment": "13 pages, 8 tables, 1 figure", "summary": "Which factors determine AI propensity for military intervention? While the\nuse of AI in war games and military planning is growing exponentially, the\nsimple analysis of key drivers embedded in the models has not yet been done.\nThis paper does a simple conjoint experiment proposing a model to decide on\nmilitary intervention in 640 vignettes where each was run for 100 times\nallowing to explore AI decision on military intervention systematically. The\nanalysis finds that largest predictors of AI decision to intervene are high\ndomestic support and high probability of success. Costs such as international\ncondemnation, military deaths, civilian deaths, and negative economic effect\nare statistically significant, but their effect is around half of domestic\nsupport and probability of victory. Closing window of opportunity only reaches\nstatistical significance in interaction with other factors. The results are\nremarkably consistent across scenarios and across different models (OpenAI GPT,\nAnthropic Claude, Google Gemini) suggesting a pattern in AI decision-making.", "AI": {"tldr": "\u672c\u6587\u901a\u8fc7\u8054\u5408\u5b9e\u9a8c\u5206\u6790\u4e86AI\u519b\u4e8b\u5e72\u9884\u51b3\u7b56\u7684\u5173\u952e\u56e0\u7d20\uff0c\u53d1\u73b0\u56fd\u5185\u652f\u6301\u5ea6\u548c\u6210\u529f\u6982\u7387\u662f\u6700\u5927\u9884\u6d4b\u56e0\u7d20\uff0c\u800c\u6210\u672c\u56e0\u7d20\u5f71\u54cd\u8f83\u5c0f\u3002", "motivation": "\u7814\u7a76AI\u5728\u519b\u4e8b\u5e72\u9884\u51b3\u7b56\u4e2d\u7684\u5173\u952e\u9a71\u52a8\u56e0\u7d20\uff0c\u586b\u8865\u73b0\u6709\u7814\u7a76\u7684\u7a7a\u767d\u3002", "method": "\u91c7\u7528\u8054\u5408\u5b9e\u9a8c\uff0c\u8bbe\u8ba1\u4e86640\u4e2a\u60c5\u666f\uff0c\u6bcf\u4e2a\u60c5\u666f\u8fd0\u884c100\u6b21\uff0c\u7cfb\u7edf\u5206\u6790AI\u51b3\u7b56\u3002", "result": "\u56fd\u5185\u652f\u6301\u5ea6\u548c\u6210\u529f\u6982\u7387\u662fAI\u5e72\u9884\u51b3\u7b56\u7684\u4e3b\u8981\u9884\u6d4b\u56e0\u7d20\uff0c\u6210\u672c\u56e0\u7d20\u5f71\u54cd\u8f83\u5c0f\u3002", "conclusion": "AI\u519b\u4e8b\u5e72\u9884\u51b3\u7b56\u6a21\u5f0f\u5728\u4e0d\u540c\u6a21\u578b\u548c\u60c5\u666f\u4e2d\u8868\u73b0\u4e00\u81f4\uff0c\u63ed\u793a\u4e86\u5176\u51b3\u7b56\u89c4\u5f8b\u3002"}}
{"id": "2507.06719", "pdf": "https://arxiv.org/pdf/2507.06719", "abs": "https://arxiv.org/abs/2507.06719", "authors": ["Zhenyang Liu", "Sixiao Zheng", "Siyu Chen", "Cairong Zhao", "Longfei Liang", "Xiangyang Xue", "Yanwei Fu"], "title": "A Neural Representation Framework with LLM-Driven Spatial Reasoning for Open-Vocabulary 3D Visual Grounding", "categories": ["cs.CV", "cs.RO"], "comment": null, "summary": "Open-vocabulary 3D visual grounding aims to localize target objects based on\nfree-form language queries, which is crucial for embodied AI applications such\nas autonomous navigation, robotics, and augmented reality. Learning 3D language\nfields through neural representations enables accurate understanding of 3D\nscenes from limited viewpoints and facilitates the localization of target\nobjects in complex environments. However, existing language field methods\nstruggle to accurately localize instances using spatial relations in language\nqueries, such as ``the book on the chair.'' This limitation mainly arises from\ninadequate reasoning about spatial relations in both language queries and 3D\nscenes. In this work, we propose SpatialReasoner, a novel neural\nrepresentation-based framework with large language model (LLM)-driven spatial\nreasoning that constructs a visual properties-enhanced hierarchical feature\nfield for open-vocabulary 3D visual grounding. To enable spatial reasoning in\nlanguage queries, SpatialReasoner fine-tunes an LLM to capture spatial\nrelations and explicitly infer instructions for the target, anchor, and spatial\nrelation. To enable spatial reasoning in 3D scenes, SpatialReasoner\nincorporates visual properties (opacity and color) to construct a hierarchical\nfeature field. This field represents language and instance features using\ndistilled CLIP features and masks extracted via the Segment Anything Model\n(SAM). The field is then queried using the inferred instructions in a\nhierarchical manner to localize the target 3D instance based on the spatial\nrelation in the language query. Extensive experiments show that our framework\ncan be seamlessly integrated into different neural representations,\noutperforming baseline models in 3D visual grounding while empowering their\nspatial reasoning capability.", "AI": {"tldr": "\u63d0\u51faSpatialReasoner\u6846\u67b6\uff0c\u901a\u8fc7LLM\u9a71\u52a8\u7684\u7a7a\u95f4\u63a8\u7406\u548c\u5206\u5c42\u7279\u5f81\u573a\uff0c\u63d0\u5347\u5f00\u653e\u8bcd\u6c473D\u89c6\u89c9\u5b9a\u4f4d\u4e2d\u7a7a\u95f4\u5173\u7cfb\u7684\u7406\u89e3\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u8bed\u8a00\u67e5\u8be2\u4e2d\u5904\u7406\u7a7a\u95f4\u5173\u7cfb\uff08\u5982\u201c\u6905\u5b50\u4e0a\u7684\u4e66\u201d\uff09\u65f6\u8868\u73b0\u4e0d\u4f73\uff0c\u4e3b\u8981\u539f\u56e0\u662f\u7f3a\u4e4f\u5bf9\u8bed\u8a00\u548c3D\u573a\u666f\u4e2d\u7a7a\u95f4\u5173\u7cfb\u7684\u5145\u5206\u63a8\u7406\u3002", "method": "\u7ed3\u5408LLM\u9a71\u52a8\u7684\u7a7a\u95f4\u63a8\u7406\u548c\u89c6\u89c9\u5c5e\u6027\u589e\u5f3a\u7684\u5206\u5c42\u7279\u5f81\u573a\uff0c\u901a\u8fc7CLIP\u7279\u5f81\u548cSAM\u63d0\u53d6\u7684\u63a9\u7801\u6784\u5efa\u7279\u5f81\u573a\uff0c\u5e76\u5206\u5c42\u67e5\u8be2\u76ee\u6807\u5b9e\u4f8b\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u6846\u67b6\u80fd\u65e0\u7f1d\u96c6\u6210\u5230\u4e0d\u540c\u795e\u7ecf\u8868\u793a\u4e2d\uff0c\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\u6a21\u578b\uff0c\u5e76\u589e\u5f3a\u5176\u7a7a\u95f4\u63a8\u7406\u80fd\u529b\u3002", "conclusion": "SpatialReasoner\u901a\u8fc7\u6539\u8fdb\u7a7a\u95f4\u63a8\u7406\u548c\u7279\u5f81\u8868\u793a\uff0c\u6709\u6548\u63d0\u5347\u4e86\u5f00\u653e\u8bcd\u6c473D\u89c6\u89c9\u5b9a\u4f4d\u7684\u6027\u80fd\u3002"}}
{"id": "2507.06722", "pdf": "https://arxiv.org/pdf/2507.06722", "abs": "https://arxiv.org/abs/2507.06722", "authors": ["Sunwoo Kim", "Haneul Yoo", "Alice Oh"], "title": "On the Effect of Uncertainty on Layer-wise Inference Dynamics", "categories": ["cs.CL", "cs.LG"], "comment": "Accepted to Actionable Interpretability Workshop - ICML 2025", "summary": "Understanding how large language models (LLMs) internally represent and\nprocess their predictions is central to detecting uncertainty and preventing\nhallucinations. While several studies have shown that models encode uncertainty\nin their hidden states, it is underexplored how this affects the way they\nprocess such hidden states. In this work, we demonstrate that the dynamics of\noutput token probabilities across layers for certain and uncertain outputs are\nlargely aligned, revealing that uncertainty does not seem to affect inference\ndynamics. Specifically, we use the Tuned Lens, a variant of the Logit Lens, to\nanalyze the layer-wise probability trajectories of final prediction tokens\nacross 11 datasets and 5 models. Using incorrect predictions as those with\nhigher epistemic uncertainty, our results show aligned trajectories for certain\nand uncertain predictions that both observe abrupt increases in confidence at\nsimilar layers. We balance this finding by showing evidence that more competent\nmodels may learn to process uncertainty differently. Our findings challenge the\nfeasibility of leveraging simplistic methods for detecting uncertainty at\ninference. More broadly, our work demonstrates how interpretability methods may\nbe used to investigate the way uncertainty affects inference.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\uff0c\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u5904\u7406\u786e\u5b9a\u548c\u4e0d\u786e\u5b9a\u9884\u6d4b\u65f6\uff0c\u5176\u9690\u85cf\u72b6\u6001\u7684\u52a8\u6001\u53d8\u5316\u57fa\u672c\u4e00\u81f4\uff0c\u4e0d\u786e\u5b9a\u6027\u5e76\u672a\u663e\u8457\u5f71\u54cd\u63a8\u7406\u52a8\u6001\u3002", "motivation": "\u63a2\u7d22LLMs\u5185\u90e8\u5982\u4f55\u8868\u793a\u548c\u5904\u7406\u4e0d\u786e\u5b9a\u6027\uff0c\u4ee5\u68c0\u6d4b\u4e0d\u786e\u5b9a\u6027\u5e76\u9632\u6b62\u5e7b\u89c9\u3002", "method": "\u4f7f\u7528Tuned Lens\uff08Logit Lens\u7684\u53d8\u4f53\uff09\u5206\u679011\u4e2a\u6570\u636e\u96c6\u548c5\u4e2a\u6a21\u578b\u7684\u5c42\u95f4\u6982\u7387\u8f68\u8ff9\u3002", "result": "\u786e\u5b9a\u548c\u4e0d\u786e\u5b9a\u9884\u6d4b\u7684\u8f68\u8ff9\u4e00\u81f4\uff0c\u5747\u5728\u76f8\u4f3c\u5c42\u51fa\u73b0\u7f6e\u4fe1\u5ea6\u9aa4\u589e\uff1b\u66f4\u5f3a\u5927\u7684\u6a21\u578b\u53ef\u80fd\u5b66\u4f1a\u4e0d\u540c\u5904\u7406\u4e0d\u786e\u5b9a\u6027\u3002", "conclusion": "\u7814\u7a76\u8d28\u7591\u4e86\u7b80\u5355\u65b9\u6cd5\u68c0\u6d4b\u4e0d\u786e\u5b9a\u6027\u7684\u53ef\u884c\u6027\uff0c\u5e76\u5c55\u793a\u4e86\u53ef\u89e3\u91ca\u6027\u65b9\u6cd5\u5728\u7814\u7a76\u4e0d\u786e\u5b9a\u6027\u5f71\u54cd\u63a8\u7406\u4e2d\u7684\u5e94\u7528\u3002"}}
{"id": "2507.06549", "pdf": "https://arxiv.org/pdf/2507.06549", "abs": "https://arxiv.org/abs/2507.06549", "authors": ["Shan Shen", "Dingcheng Yang", "Yuyang Xie", "Chunyan Pei", "Wenjian Yu", "Bei Yu"], "title": "Deep-Learning-Based Pre-Layout Parasitic Capacitance Prediction on SRAM Designs", "categories": ["cs.LG", "cs.AR", "cs.SY", "eess.SY"], "comment": "Published in Proceedings of GLSVLSI2024", "summary": "To achieve higher system energy efficiency, SRAM in SoCs is often customized.\nThe parasitic effects cause notable discrepancies between pre-layout and\npost-layout circuit simulations, leading to difficulty in converging design\nparameters and excessive design iterations. Is it possible to well predict the\nparasitics based on the pre-layout circuit, so as to perform parasitic-aware\npre-layout simulation? In this work, we propose a deep-learning-based 2-stage\nmodel to accurately predict these parasitics in pre-layout stages. The model\ncombines a Graph Neural Network (GNN) classifier and Multi-Layer Perceptron\n(MLP) regressors, effectively managing class imbalance of the net parasitics in\nSRAM circuits. We also employ Focal Loss to mitigate the impact of abundant\ninternal net samples and integrate subcircuit information into the graph to\nabstract the hierarchical structure of schematics. Experiments on 4 real SRAM\ndesigns show that our approach not only surpasses the state-of-the-art model in\nparasitic prediction by a maximum of 19X reduction of error but also\nsignificantly boosts the simulation process by up to 598X speedup.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u76842\u9636\u6bb5\u6a21\u578b\uff0c\u7528\u4e8e\u5728\u9884\u5e03\u5c40\u9636\u6bb5\u51c6\u786e\u9884\u6d4bSRAM\u7535\u8def\u4e2d\u7684\u5bc4\u751f\u6548\u5e94\uff0c\u663e\u8457\u51cf\u5c11\u8bef\u5dee\u5e76\u52a0\u901f\u4eff\u771f\u8fc7\u7a0b\u3002", "motivation": "SRAM\u5728SoC\u4e2d\u7684\u5b9a\u5236\u5316\u5bfc\u81f4\u9884\u5e03\u5c40\u548c\u540e\u5e03\u5c40\u7535\u8def\u4eff\u771f\u4e4b\u95f4\u5b58\u5728\u663e\u8457\u5dee\u5f02\uff0c\u589e\u52a0\u4e86\u8bbe\u8ba1\u8fed\u4ee3\u7684\u96be\u5ea6\u3002", "method": "\u7ed3\u5408\u56fe\u795e\u7ecf\u7f51\u7edc\uff08GNN\uff09\u5206\u7c7b\u5668\u548c\u591a\u5c42\u611f\u77e5\u673a\uff08MLP\uff09\u56de\u5f52\u5668\uff0c\u91c7\u7528Focal Loss\u5904\u7406\u7c7b\u522b\u4e0d\u5e73\u8861\uff0c\u5e76\u6574\u5408\u5b50\u7535\u8def\u4fe1\u606f\u4ee5\u62bd\u8c61\u5c42\u6b21\u7ed3\u6784\u3002", "result": "\u57284\u4e2a\u5b9e\u9645SRAM\u8bbe\u8ba1\u4e2d\uff0c\u6a21\u578b\u8bef\u5dee\u6700\u5927\u51cf\u5c1119\u500d\uff0c\u4eff\u771f\u901f\u5ea6\u63d0\u5347\u9ad8\u8fbe598\u500d\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u5bc4\u751f\u6548\u5e94\u9884\u6d4b\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8bbe\u8ba1\u6548\u7387\u548c\u4eff\u771f\u901f\u5ea6\u3002"}}
{"id": "2507.06590", "pdf": "https://arxiv.org/pdf/2507.06590", "abs": "https://arxiv.org/abs/2507.06590", "authors": ["Yin Wang", "Mu li", "Zhiying Leng", "Frederick W. B. Li", "Xiaohui Liang"], "title": "MOST: Motion Diffusion Model for Rare Text via Temporal Clip Banzhaf Interaction", "categories": ["cs.CV"], "comment": null, "summary": "We introduce MOST, a novel motion diffusion model via temporal clip Banzhaf\ninteraction, aimed at addressing the persistent challenge of generating human\nmotion from rare language prompts. While previous approaches struggle with\ncoarse-grained matching and overlook important semantic cues due to motion\nredundancy, our key insight lies in leveraging fine-grained clip relationships\nto mitigate these issues. MOST's retrieval stage presents the first formulation\nof its kind - temporal clip Banzhaf interaction - which precisely quantifies\ntextual-motion coherence at the clip level. This facilitates direct,\nfine-grained text-to-motion clip matching and eliminates prevalent redundancy.\nIn the generation stage, a motion prompt module effectively utilizes retrieved\nmotion clips to produce semantically consistent movements. Extensive\nevaluations confirm that MOST achieves state-of-the-art text-to-motion\nretrieval and generation performance by comprehensively addressing previous\nchallenges, as demonstrated through quantitative and qualitative results\nhighlighting its effectiveness, especially for rare prompts.", "AI": {"tldr": "MOST\u662f\u4e00\u79cd\u901a\u8fc7\u65f6\u95f4\u7247\u6bb5Banzhaf\u4ea4\u4e92\u7684\u65b0\u578b\u8fd0\u52a8\u6269\u6563\u6a21\u578b\uff0c\u65e8\u5728\u89e3\u51b3\u4ece\u7f55\u89c1\u8bed\u8a00\u63d0\u793a\u751f\u6210\u4eba\u7c7b\u8fd0\u52a8\u7684\u6311\u6218\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u7c97\u7c92\u5ea6\u5339\u914d\u548c\u5ffd\u7565\u8bed\u4e49\u7ebf\u7d22\u65b9\u9762\u5b58\u5728\u95ee\u9898\uff0cMOST\u901a\u8fc7\u5229\u7528\u7ec6\u7c92\u5ea6\u7247\u6bb5\u5173\u7cfb\u6765\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "MOST\u91c7\u7528\u65f6\u95f4\u7247\u6bb5Banzhaf\u4ea4\u4e92\u91cf\u5316\u6587\u672c-\u8fd0\u52a8\u4e00\u81f4\u6027\uff0c\u5e76\u901a\u8fc7\u8fd0\u52a8\u63d0\u793a\u6a21\u5757\u751f\u6210\u8bed\u4e49\u4e00\u81f4\u7684\u8fd0\u52a8\u3002", "result": "MOST\u5728\u6587\u672c\u5230\u8fd0\u52a8\u7684\u68c0\u7d22\u548c\u751f\u6210\u6027\u80fd\u4e0a\u8fbe\u5230\u6700\u5148\u8fdb\u6c34\u5e73\uff0c\u5c24\u5176\u5728\u7f55\u89c1\u63d0\u793a\u4e0a\u8868\u73b0\u7a81\u51fa\u3002", "conclusion": "MOST\u901a\u8fc7\u7ec6\u7c92\u5ea6\u5339\u914d\u548c\u6d88\u9664\u5197\u4f59\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6587\u672c\u5230\u8fd0\u52a8\u751f\u6210\u7684\u6548\u679c\u3002"}}
{"id": "2507.06971", "pdf": "https://arxiv.org/pdf/2507.06971", "abs": "https://arxiv.org/abs/2507.06971", "authors": ["Fei Teng", "Kai Luo", "Sheng Wu", "Siyu Li", "Pujun Guo", "Jiale Wei", "Kunyu Peng", "Jiaming Zhang", "Kailun Yang"], "title": "Hallucinating 360\u00b0: Panoramic Street-View Generation via Local Scenes Diffusion and Probabilistic Prompting", "categories": ["cs.CV", "cs.RO", "eess.IV"], "comment": "The source code will be publicly available at\n  https://github.com/Bryant-Teng/Percep360", "summary": "Panoramic perception holds significant potential for autonomous driving,\nenabling vehicles to acquire a comprehensive 360{\\deg} surround view in a\nsingle shot. However, autonomous driving is a data-driven task. Complete\npanoramic data acquisition requires complex sampling systems and annotation\npipelines, which are time-consuming and labor-intensive. Although existing\nstreet view generation models have demonstrated strong data regeneration\ncapabilities, they can only learn from the fixed data distribution of existing\ndatasets and cannot achieve high-quality, controllable panoramic generation. In\nthis paper, we propose the first panoramic generation method Percep360 for\nautonomous driving. Percep360 enables coherent generation of panoramic data\nwith control signals based on the stitched panoramic data. Percep360 focuses on\ntwo key aspects: coherence and controllability. Specifically, to overcome the\ninherent information loss caused by the pinhole sampling process, we propose\nthe Local Scenes Diffusion Method (LSDM). LSDM reformulates the panorama\ngeneration as a spatially continuous diffusion process, bridging the gaps\nbetween different data distributions. Additionally, to achieve the controllable\ngeneration of panoramic images, we propose a Probabilistic Prompting Method\n(PPM). PPM dynamically selects the most relevant control cues, enabling\ncontrollable panoramic image generation. We evaluate the effectiveness of the\ngenerated images from three perspectives: image quality assessment (i.e.,\nno-reference and with reference), controllability, and their utility in\nreal-world Bird's Eye View (BEV) segmentation. Notably, the generated data\nconsistently outperforms the original stitched images in no-reference quality\nmetrics and enhances downstream perception models. The source code will be\npublicly available at https://github.com/Bryant-Teng/Percep360.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faPercep360\u65b9\u6cd5\uff0c\u7528\u4e8e\u81ea\u52a8\u9a7e\u9a76\u4e2d\u7684\u5168\u666f\u6570\u636e\u751f\u6210\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u8fde\u8d2f\u6027\u548c\u53ef\u63a7\u6027\u4e0a\u7684\u4e0d\u8db3\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "motivation": "\u81ea\u52a8\u9a7e\u9a76\u9700\u8981\u5168\u666f\u611f\u77e5\uff0c\u4f46\u6570\u636e\u91c7\u96c6\u548c\u6807\u6ce8\u590d\u6742\u8017\u65f6\u3002\u73b0\u6709\u751f\u6210\u6a21\u578b\u65e0\u6cd5\u9ad8\u8d28\u91cf\u3001\u53ef\u63a7\u5730\u751f\u6210\u5168\u666f\u6570\u636e\u3002", "method": "\u63d0\u51fa\u5c40\u90e8\u573a\u666f\u6269\u6563\u65b9\u6cd5\uff08LSDM\uff09\u548c\u6982\u7387\u63d0\u793a\u65b9\u6cd5\uff08PPM\uff09\uff0c\u5206\u522b\u89e3\u51b3\u4fe1\u606f\u4e22\u5931\u548c\u53ef\u63a7\u751f\u6210\u95ee\u9898\u3002", "result": "\u751f\u6210\u56fe\u50cf\u5728\u8d28\u91cf\u8bc4\u4f30\u3001\u53ef\u63a7\u6027\u548c\u4e0b\u6e38\u4efb\u52a1\uff08\u5982BEV\u5206\u5272\uff09\u4e2d\u8868\u73b0\u4f18\u4e8e\u539f\u59cb\u62fc\u63a5\u56fe\u50cf\u3002", "conclusion": "Percep360\u4e3a\u81ea\u52a8\u9a7e\u9a76\u63d0\u4f9b\u4e86\u9ad8\u8d28\u91cf\u3001\u53ef\u63a7\u7684\u5168\u666f\u6570\u636e\u751f\u6210\u65b9\u6848\uff0c\u5e76\u5f00\u6e90\u4e86\u4ee3\u7801\u3002"}}
{"id": "2507.06753", "pdf": "https://arxiv.org/pdf/2507.06753", "abs": "https://arxiv.org/abs/2507.06753", "authors": ["Ye Kyaw Thu", "Thura Aung", "Thazin Myint Oo", "Thepchai Supnithi"], "title": "KAConvText: Novel Approach to Burmese Sentence Classification using Kolmogorov-Arnold Convolution", "categories": ["cs.CL", "cs.AI", "I.2.7; I.2.6"], "comment": "10 pages, 3 figures, 4 tables", "summary": "This paper presents the first application of Kolmogorov-Arnold Convolution\nfor Text (KAConvText) in sentence classification, addressing three tasks:\nimbalanced binary hate speech detection, balanced multiclass news\nclassification, and imbalanced multiclass ethnic language identification. We\ninvestigate various embedding configurations, comparing random to fastText\nembeddings in both static and fine-tuned settings, with embedding dimensions of\n100 and 300 using CBOW and Skip-gram models. Baselines include standard CNNs\nand CNNs augmented with a Kolmogorov-Arnold Network (CNN-KAN). In addition, we\ninvestigated KAConvText with different classification heads - MLP and KAN,\nwhere using KAN head supports enhanced interpretability. Results show that\nKAConvText-MLP with fine-tuned fastText embeddings achieves the best\nperformance of 91.23% accuracy (F1-score = 0.9109) for hate speech detection,\n92.66% accuracy (F1-score = 0.9267) for news classification, and 99.82%\naccuracy (F1-score = 0.9982) for language identification.", "AI": {"tldr": "\u8bba\u6587\u9996\u6b21\u5c06Kolmogorov-Arnold\u5377\u79ef\uff08KAConvText\uff09\u5e94\u7528\u4e8e\u53e5\u5b50\u5206\u7c7b\uff0c\u6db5\u76d6\u4ec7\u6068\u8a00\u8bba\u68c0\u6d4b\u3001\u65b0\u95fb\u5206\u7c7b\u548c\u6c11\u65cf\u8bed\u8a00\u8bc6\u522b\u4efb\u52a1\uff0c\u6bd4\u8f83\u4e0d\u540c\u5d4c\u5165\u914d\u7f6e\uff0c\u7ed3\u679c\u663e\u793aKAConvText-MLP\u7ed3\u5408\u5fae\u8c03fastText\u5d4c\u5165\u8868\u73b0\u6700\u4f73\u3002", "motivation": "\u63a2\u7d22KAConvText\u5728\u53e5\u5b50\u5206\u7c7b\u4e2d\u7684\u9996\u6b21\u5e94\u7528\uff0c\u89e3\u51b3\u4e0d\u5e73\u8861\u548c\u5e73\u8861\u5206\u7c7b\u4efb\u52a1\uff0c\u5e76\u6bd4\u8f83\u4e0d\u540c\u5d4c\u5165\u65b9\u6cd5\u7684\u6548\u679c\u3002", "method": "\u4f7f\u7528KAConvText\u6a21\u578b\uff0c\u6bd4\u8f83\u968f\u673a\u548cfastText\u5d4c\u5165\uff08\u9759\u6001\u548c\u5fae\u8c03\uff09\uff0c\u7ed3\u5408MLP\u548cKAN\u5206\u7c7b\u5934\uff0c\u5e76\u4e0eCNN\u548cCNN-KAN\u57fa\u7ebf\u5bf9\u6bd4\u3002", "result": "KAConvText-MLP\u5728\u4ec7\u6068\u8a00\u8bba\u68c0\u6d4b\u3001\u65b0\u95fb\u5206\u7c7b\u548c\u8bed\u8a00\u8bc6\u522b\u4efb\u52a1\u4e2d\u5206\u522b\u8fbe\u523091.23%\u300192.66%\u548c99.82%\u7684\u51c6\u786e\u7387\u3002", "conclusion": "KAConvText-MLP\u7ed3\u5408\u5fae\u8c03fastText\u5d4c\u5165\u5728\u53e5\u5b50\u5206\u7c7b\u4efb\u52a1\u4e2d\u8868\u73b0\u6700\u4f18\uff0c\u4e14KAN\u5206\u7c7b\u5934\u589e\u5f3a\u4e86\u6a21\u578b\u53ef\u89e3\u91ca\u6027\u3002"}}
{"id": "2507.06558", "pdf": "https://arxiv.org/pdf/2507.06558", "abs": "https://arxiv.org/abs/2507.06558", "authors": ["Zicheng Zhang", "Haoran Li", "Yifeng Zhang", "Guoqiang Gong", "Jiaxing Wang", "Pengzhang Liu", "Qixia Jiang", "Junxing Hu"], "title": "The Primacy of Magnitude in Low-Rank Adaptation", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Low-Rank Adaptation (LoRA) offers a parameter-efficient paradigm for tuning\nlarge models. While recent spectral initialization methods improve convergence\nand performance over the naive \"Noise & Zeros\" scheme, their extra\ncomputational and storage overhead undermines efficiency. In this paper, we\nestablish update magnitude as the fundamental driver of LoRA performance and\npropose LoRAM, a magnitude-driven \"Basis & Basis\" initialization scheme that\nmatches spectral methods without their inefficiencies. Our key contributions\nare threefold: (i) Magnitude of weight updates determines convergence. We prove\nlow-rank structures intrinsically bound update magnitudes, unifying\nhyperparameter tuning in learning rate, scaling factor, and initialization as\nmechanisms to optimize magnitude regulation. (ii) Spectral initialization\nsucceeds via magnitude amplification. We demystify that the presumed\nknowledge-driven benefit of the spectral component essentially arises from the\nboost in the weight update magnitude. (iii) A novel and compact initialization\nstrategy, LoRAM, scales deterministic orthogonal bases using pretrained weight\nmagnitudes to simulate spectral gains. Extensive experiments show that LoRAM\nserves as a strong baseline, retaining the full efficiency of LoRA while\nmatching or outperforming spectral initialization across benchmarks.", "AI": {"tldr": "LoRAM\u662f\u4e00\u79cd\u57fa\u4e8e\u66f4\u65b0\u5e45\u5ea6\u7684\u521d\u59cb\u5316\u65b9\u6848\uff0c\u901a\u8fc7\u4f18\u5316\u5e45\u5ea6\u8c03\u8282\u63d0\u5347LoRA\u6027\u80fd\uff0c\u907f\u514d\u4e86\u5149\u8c31\u65b9\u6cd5\u7684\u989d\u5916\u5f00\u9500\u3002", "motivation": "\u5149\u8c31\u521d\u59cb\u5316\u65b9\u6cd5\u867d\u7136\u63d0\u9ad8\u4e86\u6027\u80fd\uff0c\u4f46\u5e26\u6765\u4e86\u989d\u5916\u7684\u8ba1\u7b97\u548c\u5b58\u50a8\u5f00\u9500\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u66f4\u9ad8\u6548\u7684\u521d\u59cb\u5316\u65b9\u6848\u3002", "method": "\u63d0\u51faLoRAM\uff0c\u5229\u7528\u9884\u8bad\u7ec3\u6743\u91cd\u5e45\u5ea6\u7f29\u653e\u786e\u5b9a\u6027\u6b63\u4ea4\u57fa\uff0c\u6a21\u62df\u5149\u8c31\u589e\u76ca\u3002", "result": "LoRAM\u5728\u4fdd\u6301LoRA\u9ad8\u6548\u6027\u7684\u540c\u65f6\uff0c\u6027\u80fd\u4e0e\u5149\u8c31\u521d\u59cb\u5316\u76f8\u5f53\u6216\u66f4\u4f18\u3002", "conclusion": "LoRAM\u901a\u8fc7\u5e45\u5ea6\u9a71\u52a8\u4f18\u5316\uff0c\u4e3aLoRA\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u4e14\u6027\u80fd\u4f18\u8d8a\u7684\u521d\u59cb\u5316\u7b56\u7565\u3002"}}
{"id": "2507.06592", "pdf": "https://arxiv.org/pdf/2507.06592", "abs": "https://arxiv.org/abs/2507.06592", "authors": ["Yang Chen", "Yueqi Duan", "Haowen Sun", "Jiwen Lu", "Yap-Peng Tan"], "title": "Ambiguity-aware Point Cloud Segmentation by Adaptive Margin Contrastive Learning", "categories": ["cs.CV"], "comment": "This article has been accepted for publication in IEEE Transactions\n  on Multimedia. arXiv admin note: text overlap with arXiv:2502.04111", "summary": "This paper proposes an adaptive margin contrastive learning method for 3D\nsemantic segmentation on point clouds. Most existing methods use equally\npenalized objectives, which ignore the per-point ambiguities and less\ndiscriminated features stemming from transition regions. However, as highly\nambiguous points may be indistinguishable even for humans, their manually\nannotated labels are less reliable, and hard constraints over these points\nwould lead to sub-optimal models. To address this, we first design\nAMContrast3D, a method comprising contrastive learning into an ambiguity\nestimation framework, tailored to adaptive objectives for individual points\nbased on ambiguity levels. As a result, our method promotes model training,\nwhich ensures the correctness of low-ambiguity points while allowing mistakes\nfor high-ambiguity points. As ambiguities are formulated based on position\ndiscrepancies across labels, optimization during inference is constrained by\nthe assumption that all unlabeled points are uniformly unambiguous, lacking\nambiguity awareness. Inspired by the insight of joint training, we further\npropose AMContrast3D++ integrating with two branches trained in parallel, where\na novel ambiguity prediction module concurrently learns point ambiguities from\ngenerated embeddings. To this end, we design a masked refinement mechanism that\nleverages predicted ambiguities to enable the ambiguous embeddings to be more\nreliable, thereby boosting segmentation performance and enhancing robustness.\nExperimental results on 3D indoor scene datasets, S3DIS and ScanNet,\ndemonstrate the effectiveness of the proposed method. Code is available at\nhttps://github.com/YangChenApril/AMContrast3D.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u81ea\u9002\u5e94\u8fb9\u8ddd\u5bf9\u6bd4\u5b66\u4e60\u65b9\u6cd5\uff08AMContrast3D\u548cAMContrast3D++\uff09\uff0c\u7528\u4e8e\u70b9\u4e91\u76843D\u8bed\u4e49\u5206\u5272\uff0c\u901a\u8fc7\u6a21\u7cca\u5ea6\u611f\u77e5\u4f18\u5316\u6a21\u578b\u8bad\u7ec3\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5bf9\u6a21\u7cca\u533a\u57df\u7684\u70b9\u91c7\u7528\u5747\u7b49\u60e9\u7f5a\u76ee\u6807\uff0c\u5bfc\u81f4\u6a21\u578b\u6027\u80fd\u53d7\u9650\uff0c\u800c\u4eba\u5de5\u6807\u6ce8\u7684\u6a21\u7cca\u70b9\u6807\u7b7e\u4e0d\u53ef\u9760\uff0c\u9700\u81ea\u9002\u5e94\u4f18\u5316\u3002", "method": "\u8bbe\u8ba1\u4e86AMContrast3D\uff0c\u5c06\u5bf9\u6bd4\u5b66\u4e60\u878d\u5165\u6a21\u7cca\u5ea6\u4f30\u8ba1\u6846\u67b6\uff0c\u6839\u636e\u70b9\u6a21\u7cca\u5ea6\u81ea\u9002\u5e94\u8c03\u6574\u76ee\u6807\uff1b\u8fdb\u4e00\u6b65\u63d0\u51faAMContrast3D++\uff0c\u901a\u8fc7\u5e76\u884c\u5206\u652f\u548c\u63a9\u7801\u7ec6\u5316\u673a\u5236\u63d0\u5347\u6a21\u7cca\u5d4c\u5165\u7684\u53ef\u9760\u6027\u3002", "result": "\u5728S3DIS\u548cScanNet\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u63d0\u5347\u4e86\u5206\u5272\u6027\u80fd\u548c\u9c81\u68d2\u6027\u3002", "conclusion": "\u901a\u8fc7\u6a21\u7cca\u5ea6\u611f\u77e5\u7684\u81ea\u9002\u5e94\u5bf9\u6bd4\u5b66\u4e60\uff0c\u663e\u8457\u4f18\u5316\u4e86\u70b9\u4e91\u8bed\u4e49\u5206\u5272\u7684\u6027\u80fd\u3002"}}
{"id": "2507.06282", "pdf": "https://arxiv.org/pdf/2507.06282", "abs": "https://arxiv.org/abs/2507.06282", "authors": ["Hadrien Mariaccia", "Charbel-Rapha\u00ebl Segerie", "Diego Dorn"], "title": "The bitter lesson of misuse detection", "categories": ["cs.CR", "cs.AI", "cs.CL"], "comment": null, "summary": "Prior work on jailbreak detection has established the importance of\nadversarial robustness for LLMs but has largely focused on the model ability to\nresist adversarial inputs and to output safe content, rather than the\neffectiveness of external supervision systems. The only public and independent\nbenchmark of these guardrails to date evaluates a narrow set of supervisors on\nlimited scenarios. Consequently, no comprehensive public benchmark yet verifies\nhow well supervision systems from the market perform under realistic, diverse\nattacks. To address this, we introduce BELLS, a Benchmark for the Evaluation of\nLLM Supervision Systems. The framework is two dimensional: harm severity\n(benign, borderline, harmful) and adversarial sophistication (direct vs.\njailbreak) and provides a rich dataset covering 3 jailbreak families and 11\nharm categories. Our evaluations reveal drastic limitations of specialized\nsupervision systems. While they recognize some known jailbreak patterns, their\nsemantic understanding and generalization capabilities are very limited,\nsometimes with detection rates close to zero when asking a harmful question\ndirectly or with a new jailbreak technique such as base64 encoding. Simply\nasking generalist LLMs if the user question is \"harmful or not\" largely\noutperforms these supervisors from the market according to our BELLS score. But\nfrontier LLMs still suffer from metacognitive incoherence, often responding to\nqueries they correctly identify as harmful (up to 30 percent for Claude 3.7 and\ngreater than 50 percent for Mistral Large). These results suggest that simple\nscaffolding could significantly improve misuse detection robustness, but more\nresearch is needed to assess the tradeoffs of such techniques. Our results\nsupport the \"bitter lesson\" of misuse detection: general capabilities of LLMs\nare necessary to detect a diverse array of misuses and jailbreaks.", "AI": {"tldr": "BELLS\u662f\u4e00\u4e2a\u8bc4\u4f30LLM\u76d1\u7763\u7cfb\u7edf\u7684\u57fa\u51c6\uff0c\u63ed\u793a\u4e86\u73b0\u6709\u76d1\u7763\u7cfb\u7edf\u5728\u591a\u6837\u653b\u51fb\u4e0b\u7684\u5c40\u9650\u6027\uff0c\u5e76\u53d1\u73b0\u901a\u7528LLM\u5728\u68c0\u6d4b\u6709\u5bb3\u5185\u5bb9\u4e0a\u4f18\u4e8e\u4e13\u7528\u7cfb\u7edf\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u4e3b\u8981\u5173\u6ce8LLM\u5bf9\u6297\u8f93\u5165\u7684\u9c81\u68d2\u6027\uff0c\u800c\u7f3a\u4e4f\u5bf9\u76d1\u7763\u7cfb\u7edf\u5728\u591a\u6837\u5316\u653b\u51fb\u4e0b\u8868\u73b0\u7684\u5168\u9762\u8bc4\u4f30\u3002", "method": "\u63d0\u51faBELLS\u57fa\u51c6\uff0c\u6db5\u76d6\u4e0d\u540c\u5371\u5bb3\u7a0b\u5ea6\u548c\u5bf9\u6297\u590d\u6742\u5ea6\u7684\u653b\u51fb\uff0c\u8bc4\u4f30\u5e02\u573a\u76d1\u7763\u7cfb\u7edf\u7684\u8868\u73b0\u3002", "result": "\u4e13\u7528\u76d1\u7763\u7cfb\u7edf\u68c0\u6d4b\u80fd\u529b\u6709\u9650\uff0c\u901a\u7528LLM\u8868\u73b0\u66f4\u4f18\uff0c\u4f46\u4ecd\u5b58\u5728\u8bc6\u522b\u540e\u4ecd\u54cd\u5e94\u7684\u95ee\u9898\u3002", "conclusion": "\u901a\u7528LLM\u80fd\u529b\u5bf9\u591a\u6837\u5316\u6ee5\u7528\u68c0\u6d4b\u81f3\u5173\u91cd\u8981\uff0c\u672a\u6765\u9700\u7814\u7a76\u6539\u8fdb\u65b9\u6cd5\u3002"}}
{"id": "2507.07012", "pdf": "https://arxiv.org/pdf/2507.07012", "abs": "https://arxiv.org/abs/2507.07012", "authors": ["Chengyuan Zhang", "Zhengbing He", "Cathy Wu", "Lijun Sun"], "title": "When Context Is Not Enough: Modeling Unexplained Variability in Car-Following Behavior", "categories": ["stat.AP", "cs.LG", "cs.RO"], "comment": null, "summary": "Modeling car-following behavior is fundamental to microscopic traffic\nsimulation, yet traditional deterministic models often fail to capture the full\nextent of variability and unpredictability in human driving. While many modern\napproaches incorporate context-aware inputs (e.g., spacing, speed, relative\nspeed), they frequently overlook structured stochasticity that arises from\nlatent driver intentions, perception errors, and memory effects -- factors that\nare not directly observable from context alone. To fill the gap, this study\nintroduces an interpretable stochastic modeling framework that captures not\nonly context-dependent dynamics but also residual variability beyond what\ncontext can explain. Leveraging deep neural networks integrated with\nnonstationary Gaussian processes (GPs), our model employs a scenario-adaptive\nGibbs kernel to learn dynamic temporal correlations in acceleration decisions,\nwhere the strength and duration of correlations between acceleration decisions\nevolve with the driving context. This formulation enables a principled,\ndata-driven quantification of uncertainty in acceleration, speed, and spacing,\ngrounded in both observable context and latent behavioral variability.\nComprehensive experiments on the naturalistic vehicle trajectory dataset\ncollected from the German highway, i.e., the HighD dataset, demonstrate that\nthe proposed stochastic simulation method within this framework surpasses\nconventional methods in both predictive performance and interpretable\nuncertainty quantification. The integration of interpretability and accuracy\nmakes this framework a promising tool for traffic analysis and safety-critical\napplications.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u548c\u975e\u5e73\u7a33\u9ad8\u65af\u8fc7\u7a0b\u7684\u53ef\u89e3\u91ca\u968f\u673a\u5efa\u6a21\u6846\u67b6\uff0c\u7528\u4e8e\u6355\u6349\u8f66\u8f86\u8ddf\u968f\u884c\u4e3a\u4e2d\u7684\u4e0d\u786e\u5b9a\u6027\u548c\u6f5c\u5728\u56e0\u7d20\u3002", "motivation": "\u4f20\u7edf\u786e\u5b9a\u6027\u6a21\u578b\u96be\u4ee5\u6355\u6349\u4eba\u7c7b\u9a7e\u9a76\u884c\u4e3a\u7684\u53d8\u5f02\u6027\uff0c\u73b0\u4ee3\u65b9\u6cd5\u5e38\u5ffd\u7565\u6f5c\u5728\u56e0\u7d20\uff08\u5982\u9a7e\u9a76\u5458\u610f\u56fe\u3001\u611f\u77e5\u8bef\u5dee\uff09\u3002", "method": "\u4f7f\u7528\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u548c\u975e\u5e73\u7a33\u9ad8\u65af\u8fc7\u7a0b\uff0c\u7ed3\u5408\u573a\u666f\u81ea\u9002\u5e94\u7684Gibbs\u6838\uff0c\u52a8\u6001\u5b66\u4e60\u52a0\u901f\u5ea6\u51b3\u7b56\u7684\u65f6\u95f4\u76f8\u5173\u6027\u3002", "result": "\u5728HighD\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\uff0c\u6a21\u578b\u5728\u9884\u6d4b\u6027\u80fd\u548c\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u4e0a\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u6846\u67b6\u517c\u5177\u53ef\u89e3\u91ca\u6027\u548c\u51c6\u786e\u6027\uff0c\u9002\u7528\u4e8e\u4ea4\u901a\u5206\u6790\u548c\u5b89\u5168\u5173\u952e\u5e94\u7528\u3002"}}
{"id": "2507.06774", "pdf": "https://arxiv.org/pdf/2507.06774", "abs": "https://arxiv.org/abs/2507.06774", "authors": ["Mohammad Ghiasvand Mohammadkhani", "Hamid Beigy"], "title": "Checklist Engineering Empowers Multilingual LLM Judges", "categories": ["cs.CL"], "comment": null, "summary": "Automated text evaluation has long been a central issue in Natural Language\nProcessing (NLP). Recently, the field has shifted toward using Large Language\nModels (LLMs) as evaluators-a trend known as the LLM-as-a-Judge paradigm. While\npromising and easily adaptable across tasks, this approach has seen limited\nexploration in multilingual contexts. Existing multilingual studies often rely\non proprietary models or require extensive training data for fine-tuning,\nraising concerns about cost, time, and efficiency. In this paper, we propose\nChecklist Engineering based LLM-as-a-Judge (CE-Judge), a training-free\nframework that uses checklist intuition for multilingual evaluation with an\nopen-source model. Experiments across multiple languages and three benchmark\ndatasets, under both pointwise and pairwise settings, show that our method\ngenerally surpasses the baselines and performs on par with the GPT-4o model.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u7684\u6846\u67b6CE-Judge\uff0c\u5229\u7528\u6e05\u5355\u76f4\u89c9\u8fdb\u884c\u591a\u8bed\u8a00\u6587\u672c\u8bc4\u4f30\uff0c\u6027\u80fd\u4f18\u4e8e\u57fa\u7ebf\u6a21\u578b\u4e14\u4e0eGPT-4o\u76f8\u5f53\u3002", "motivation": "\u5f53\u524d\u591a\u8bed\u8a00\u6587\u672c\u8bc4\u4f30\u4f9d\u8d56\u4e13\u6709\u6a21\u578b\u6216\u5927\u91cf\u8bad\u7ec3\u6570\u636e\uff0c\u6210\u672c\u9ad8\u4e14\u6548\u7387\u4f4e\uff0c\u4e9f\u9700\u4e00\u79cd\u66f4\u9ad8\u6548\u7684\u65b9\u6cd5\u3002", "method": "\u63d0\u51faCE-Judge\u6846\u67b6\uff0c\u57fa\u4e8e\u6e05\u5355\u76f4\u89c9\uff0c\u4f7f\u7528\u5f00\u6e90\u6a21\u578b\u8fdb\u884c\u591a\u8bed\u8a00\u8bc4\u4f30\uff0c\u65e0\u9700\u8bad\u7ec3\u3002", "result": "\u5728\u591a\u79cd\u8bed\u8a00\u548c\u4e09\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\uff0cCE-Judge\u8868\u73b0\u4f18\u4e8e\u57fa\u7ebf\u6a21\u578b\uff0c\u4e0eGPT-4o\u76f8\u5f53\u3002", "conclusion": "CE-Judge\u4e3a\u591a\u8bed\u8a00\u6587\u672c\u8bc4\u4f30\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u3001\u4f4e\u6210\u672c\u4e14\u65e0\u9700\u8bad\u7ec3\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.06567", "pdf": "https://arxiv.org/pdf/2507.06567", "abs": "https://arxiv.org/abs/2507.06567", "authors": ["Qian Chen", "Xianhao Chen", "Kaibin Huang"], "title": "SlimCaching: Edge Caching of Mixture-of-Experts for Distributed Inference", "categories": ["cs.LG", "cs.DC", "cs.NI"], "comment": "14 pages, 10 figures", "summary": "Mixture-of-Experts (MoE) models improve the scalability of large language\nmodels (LLMs) by activating only a small subset of relevant experts per input.\nHowever, the sheer number of expert networks in an MoE model introduces a\nsignificant storage burden for an edge device. To address this challenge, we\nconsider a scenario where experts are dispersed within an edge network for\ndistributed inference. Based on the popular Top-$K$ expert selection strategy,\nwe formulate a latency minimization problem by optimizing expert caching on\nedge servers under storage constraints. When $K=1$, the problem reduces to a\nmonotone submodular maximization problem with knapsack constraints, for which\nwe design a greedy-based algorithm with a $(1 - 1/e)$-approximation guarantee.\nFor the general case where $K\\geq1$, expert co-activation within the same MoE\nlayer introduces non-submodularity, causing greedy methods to be ineffective.\nTo tackle this issue, we propose a successive greedy decomposition method to\ndecompose the original problem into a series of subproblems, with each being\nsolved by a dynamic programming approach. Furthermore, we design an accelerated\nalgorithm based on the max-convolution technique to obtain the approximate\nsolution with a provable guarantee in polynomial time. Simulation results on\nvarious MoE models demonstrate that our method significantly reduces inference\nlatency compared to existing baselines.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u4f18\u5316\u8fb9\u7f18\u7f51\u7edc\u4e2d\u4e13\u5bb6\u7f13\u5b58\u7684\u65b9\u6cd5\uff0c\u4ee5\u51cf\u5c11Mixture-of-Experts\u6a21\u578b\u7684\u63a8\u7406\u5ef6\u8fdf\u3002", "motivation": "MoE\u6a21\u578b\u5728\u8fb9\u7f18\u8bbe\u5907\u4e0a\u5b58\u50a8\u5927\u91cf\u4e13\u5bb6\u7f51\u7edc\u5e26\u6765\u8d1f\u62c5\uff0c\u9700\u4f18\u5316\u4e13\u5bb6\u7f13\u5b58\u4ee5\u51cf\u5c11\u5ef6\u8fdf\u3002", "method": "\u9488\u5bf9Top-K\u4e13\u5bb6\u9009\u62e9\u7b56\u7565\uff0c\u8bbe\u8ba1\u4e86\u8d2a\u5fc3\u7b97\u6cd5\u548c\u52a8\u6001\u89c4\u5212\u65b9\u6cd5\uff0c\u5e76\u5f15\u5165\u6700\u5927\u5377\u79ef\u6280\u672f\u52a0\u901f\u6c42\u89e3\u3002", "result": "\u4eff\u771f\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u663e\u8457\u964d\u4f4e\u4e86\u63a8\u7406\u5ef6\u8fdf\u3002", "conclusion": "\u63d0\u51fa\u7684\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86MoE\u6a21\u578b\u5728\u8fb9\u7f18\u7f51\u7edc\u4e2d\u7684\u5ef6\u8fdf\u95ee\u9898\uff0c\u5177\u6709\u7406\u8bba\u548c\u5b9e\u8df5\u4ef7\u503c\u3002"}}
{"id": "2507.06593", "pdf": "https://arxiv.org/pdf/2507.06593", "abs": "https://arxiv.org/abs/2507.06593", "authors": ["Qianyu Zhang", "Bolun Zheng", "Hangjia Pan", "Lingyu Zhu", "Zunjie Zhu", "Zongpeng Li", "Shiqi Wang"], "title": "Capturing Stable HDR Videos Using a Dual-Camera System", "categories": ["cs.CV", "eess.IV"], "comment": null, "summary": "In HDR video reconstruction, exposure fluctuations in reference images from\nalternating exposure methods often result in flickering. To address this issue,\nwe propose a dual-camera system (DCS) for HDR video acquisition, where one\ncamera is assigned to capture consistent reference sequences, while the other\nis assigned to capture non-reference sequences for information supplementation.\nTo tackle the challenges posed by video data, we introduce an exposure-adaptive\nfusion network (EAFNet) to achieve more robust results. EAFNet introduced a\npre-alignment subnetwork to explore the influence of exposure, selectively\nemphasizing the valuable features across different exposure levels. Then, the\nenhanced features are fused by the asymmetric cross-feature fusion subnetwork,\nwhich explores reference-dominated attention maps to improve image fusion by\naligning cross-scale features and performing cross-feature fusion. Finally, the\nreconstruction subnetwork adopts a DWT-based multiscale architecture to reduce\nghosting artifacts and refine features at different resolutions. Extensive\nexperimental evaluations demonstrate that the proposed method achieves\nstate-of-the-art performance on different datasets, validating the great\npotential of the DCS in HDR video reconstruction. The codes and data captured\nby DCS will be available at https://github.com/zqqqyu/DCS.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u53cc\u6444\u50cf\u5934\u7cfb\u7edf\uff08DCS\uff09\u548c\u66dd\u5149\u81ea\u9002\u5e94\u878d\u5408\u7f51\u7edc\uff08EAFNet\uff09\u6765\u89e3\u51b3HDR\u89c6\u9891\u91cd\u5efa\u4e2d\u7684\u95ea\u70c1\u95ee\u9898\uff0c\u901a\u8fc7\u9009\u62e9\u6027\u7279\u5f81\u878d\u5408\u548c\u591a\u5c3a\u5ea6\u67b6\u6784\u5b9e\u73b0\u9ad8\u6027\u80fd\u3002", "motivation": "\u4ea4\u66ff\u66dd\u5149\u65b9\u6cd5\u4e2d\u7684\u66dd\u5149\u6ce2\u52a8\u5bfc\u81f4\u53c2\u8003\u56fe\u50cf\u95ea\u70c1\uff0c\u5f71\u54cd\u4e86HDR\u89c6\u9891\u7684\u8d28\u91cf\u3002", "method": "\u4f7f\u7528\u53cc\u6444\u50cf\u5934\u7cfb\u7edf\uff08DCS\uff09\u5206\u522b\u6355\u83b7\u53c2\u8003\u5e8f\u5217\u548c\u975e\u53c2\u8003\u5e8f\u5217\uff0c\u5e76\u8bbe\u8ba1EAFNet\u7f51\u7edc\u8fdb\u884c\u7279\u5f81\u5bf9\u9f50\u3001\u9009\u62e9\u6027\u878d\u5408\u548c\u591a\u5c3a\u5ea6\u91cd\u5efa\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u4e0d\u540c\u6570\u636e\u96c6\u4e0a\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "conclusion": "DCS\u548cEAFNet\u5728HDR\u89c6\u9891\u91cd\u5efa\u4e2d\u5177\u6709\u5de8\u5927\u6f5c\u529b\uff0c\u4ee3\u7801\u548c\u6570\u636e\u5c06\u5f00\u6e90\u3002"}}
{"id": "2507.06795", "pdf": "https://arxiv.org/pdf/2507.06795", "abs": "https://arxiv.org/abs/2507.06795", "authors": ["Seonwu Kim", "Yohan Na", "Kihun Kim", "Hanhee Cho", "Geun Lim", "Mintae Kim", "Seongik Park", "Ki Hyun Kim", "Youngsub Han", "Byoung-Ki Jeon"], "title": "Efficient Industrial sLLMs through Domain Adaptive Continual Pretraining: Method, Evaluation and Applications", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "under review", "summary": "The emergence of open-source large language models (LLMs) has expanded\nopportunities for enterprise applications; however, many organizations still\nlack the infrastructure to deploy and maintain large-scale models. As a result,\nsmall LLMs (sLLMs) have become a practical alternative, despite their inherent\nperformance limitations. While Domain Adaptive Continual Pretraining (DACP) has\nbeen previously explored as a method for domain adaptation, its utility in\ncommercial applications remains under-examined. In this study, we validate the\neffectiveness of applying a DACP-based recipe across diverse foundation models\nand service domains. Through extensive experiments and real-world evaluations,\nwe demonstrate that DACP-applied sLLMs achieve substantial gains in target\ndomain performance while preserving general capabilities, offering a\ncost-efficient and scalable solution for enterprise-level deployment.", "AI": {"tldr": "\u7814\u7a76\u9a8c\u8bc1\u4e86\u57fa\u4e8eDACP\u7684\u65b9\u6cd5\u5728\u5c0f\u578b\u8bed\u8a00\u6a21\u578b\uff08sLLMs\uff09\u4e0a\u7684\u6709\u6548\u6027\uff0c\u8bc1\u660e\u5176\u5728\u76ee\u6807\u9886\u57df\u6027\u80fd\u63d0\u5347\u7684\u540c\u65f6\u4fdd\u6301\u901a\u7528\u80fd\u529b\uff0c\u4e3a\u4f01\u4e1a\u90e8\u7f72\u63d0\u4f9b\u4e86\u7ecf\u6d4e\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002", "motivation": "\u5c3d\u7ba1\u5f00\u6e90\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u4e3a\u4f01\u4e1a\u5e94\u7528\u63d0\u4f9b\u4e86\u673a\u4f1a\uff0c\u4f46\u8bb8\u591a\u7ec4\u7ec7\u7f3a\u4e4f\u90e8\u7f72\u548c\u7ef4\u62a4\u5927\u89c4\u6a21\u6a21\u578b\u7684\u57fa\u7840\u8bbe\u65bd\uff0c\u56e0\u6b64\u5c0f\u578bLLMs\uff08sLLMs\uff09\u6210\u4e3a\u5b9e\u7528\u66ff\u4ee3\u65b9\u6848\u3002\u7136\u800c\uff0c\u5176\u6027\u80fd\u9650\u5236\u548c\u9886\u57df\u9002\u5e94\u6027\u4ecd\u9700\u6539\u8fdb\u3002", "method": "\u91c7\u7528\u9886\u57df\u81ea\u9002\u5e94\u6301\u7eed\u9884\u8bad\u7ec3\uff08DACP\uff09\u65b9\u6cd5\uff0c\u5e94\u7528\u4e8e\u591a\u79cd\u57fa\u7840\u6a21\u578b\u548c\u670d\u52a1\u9886\u57df\uff0c\u901a\u8fc7\u5b9e\u9a8c\u548c\u5b9e\u9645\u8bc4\u4f30\u9a8c\u8bc1\u5176\u6548\u679c\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u5e94\u7528DACP\u7684sLLMs\u5728\u76ee\u6807\u9886\u57df\u6027\u80fd\u663e\u8457\u63d0\u5347\uff0c\u540c\u65f6\u4fdd\u6301\u901a\u7528\u80fd\u529b\u3002", "conclusion": "DACP\u4e3asLLMs\u63d0\u4f9b\u4e86\u4e00\u79cd\u7ecf\u6d4e\u9ad8\u6548\u4e14\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u9002\u5408\u4f01\u4e1a\u7ea7\u90e8\u7f72\u3002"}}
{"id": "2507.06573", "pdf": "https://arxiv.org/pdf/2507.06573", "abs": "https://arxiv.org/abs/2507.06573", "authors": ["Xinjie Chen", "Minpeng Liao", "Guoxin Chen", "Chengxi Li", "Biao Fu", "Kai Fan", "Xinggao Liu"], "title": "From Data-Centric to Sample-Centric: Enhancing LLM Reasoning via Progressive Optimization", "categories": ["cs.LG", "cs.AI"], "comment": "Work in progress", "summary": "Reinforcement learning with verifiable rewards (RLVR) has recently advanced\nthe reasoning capabilities of large language models (LLMs). While prior work\nhas emphasized algorithmic design, data curation, and reward shaping, we\ninvestigate RLVR from a sample-centric perspective and introduce LPPO\n(Learning-Progress and Prefix-guided Optimization), a framework of progressive\noptimization techniques. Our work addresses a critical question: how to best\nleverage a small set of trusted, high-quality demonstrations, rather than\nsimply scaling up data volume. First, motivated by how hints aid human\nproblem-solving, we propose prefix-guided sampling, an online data augmentation\nmethod that incorporates partial solution prefixes from expert demonstrations\nto guide the policy, particularly for challenging instances. Second, inspired\nby how humans focus on important questions aligned with their current\ncapabilities, we introduce learning-progress weighting, a dynamic strategy that\nadjusts each training sample's influence based on model progression. We\nestimate sample-level learning progress via an exponential moving average of\nper-sample pass rates, promoting samples that foster learning and\nde-emphasizing stagnant ones. Experiments on mathematical-reasoning benchmarks\ndemonstrate that our methods outperform strong baselines, yielding faster\nconvergence and a higher performance ceiling.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faLPPO\u6846\u67b6\uff0c\u901a\u8fc7\u524d\u7f00\u5f15\u5bfc\u91c7\u6837\u548c\u5b66\u4e60\u8fdb\u5ea6\u52a0\u6743\u4f18\u5316RLVR\uff0c\u5229\u7528\u5c11\u91cf\u9ad8\u8d28\u91cf\u793a\u8303\u63d0\u5347LLM\u63a8\u7406\u80fd\u529b\u3002", "motivation": "\u7814\u7a76\u5982\u4f55\u6709\u6548\u5229\u7528\u5c11\u91cf\u9ad8\u8d28\u91cf\u793a\u8303\u800c\u975e\u7b80\u5355\u589e\u52a0\u6570\u636e\u91cf\uff0c\u63d0\u5347LLM\u7684\u63a8\u7406\u80fd\u529b\u3002", "method": "\u63d0\u51fa\u524d\u7f00\u5f15\u5bfc\u91c7\u6837\uff08\u5229\u7528\u4e13\u5bb6\u793a\u8303\u7684\u90e8\u5206\u89e3\u5f15\u5bfc\u7b56\u7565\uff09\u548c\u5b66\u4e60\u8fdb\u5ea6\u52a0\u6743\uff08\u52a8\u6001\u8c03\u6574\u6837\u672c\u6743\u91cd\uff09\u3002", "result": "\u5728\u6570\u5b66\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u4e8e\u57fa\u7ebf\uff0c\u6536\u655b\u66f4\u5feb\u4e14\u6027\u80fd\u4e0a\u9650\u66f4\u9ad8\u3002", "conclusion": "LPPO\u6846\u67b6\u901a\u8fc7\u6837\u672c\u4f18\u5316\u663e\u8457\u63d0\u5347RLVR\u6548\u679c\uff0c\u9a8c\u8bc1\u4e86\u9ad8\u8d28\u91cf\u793a\u8303\u7684\u91cd\u8981\u6027\u3002"}}
{"id": "2507.06603", "pdf": "https://arxiv.org/pdf/2507.06603", "abs": "https://arxiv.org/abs/2507.06603", "authors": ["Xu Shaowu", "Jia Xibin", "Gao Junyu", "Sun Qianmei", "Chang Jing", "Fan Chao"], "title": "Cross-Modal Dual-Causal Learning for Long-Term Action Recognition", "categories": ["cs.CV"], "comment": null, "summary": "Long-term action recognition (LTAR) is challenging due to extended temporal\nspans with complex atomic action correlations and visual confounders. Although\nvision-language models (VLMs) have shown promise, they often rely on\nstatistical correlations instead of causal mechanisms. Moreover, existing\ncausality-based methods address modal-specific biases but lack cross-modal\ncausal modeling, limiting their utility in VLM-based LTAR. This paper proposes\n\\textbf{C}ross-\\textbf{M}odal \\textbf{D}ual-\\textbf{C}ausal \\textbf{L}earning\n(CMDCL), which introduces a structural causal model to uncover causal\nrelationships between videos and label texts.\n  CMDCL addresses cross-modal biases in text embeddings via textual causal\nintervention and removes confounders inherent in the visual modality through\nvisual causal intervention guided by the debiased text.\n  These dual-causal interventions enable robust action representations to\naddress LTAR challenges. Experimental results on three benchmarks including\nCharades, Breakfast and COIN, demonstrate the effectiveness of the proposed\nmodel. Our code is available at https://github.com/xushaowu/CMDCL.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u8de8\u6a21\u6001\u53cc\u56e0\u679c\u5b66\u4e60\uff08CMDCL\uff09\u65b9\u6cd5\uff0c\u901a\u8fc7\u7ed3\u6784\u56e0\u679c\u6a21\u578b\u89e3\u51b3\u89c6\u9891\u548c\u6807\u7b7e\u6587\u672c\u4e4b\u95f4\u7684\u56e0\u679c\u5173\u7cfb\uff0c\u63d0\u5347\u957f\u671f\u52a8\u4f5c\u8bc6\u522b\u7684\u9c81\u68d2\u6027\u3002", "motivation": "\u957f\u671f\u52a8\u4f5c\u8bc6\u522b\uff08LTAR\uff09\u56e0\u65f6\u95f4\u8de8\u5ea6\u957f\u3001\u52a8\u4f5c\u5173\u8054\u590d\u6742\u4ee5\u53ca\u89c6\u89c9\u5e72\u6270\u56e0\u7d20\u800c\u5177\u6709\u6311\u6218\u6027\u3002\u73b0\u6709\u65b9\u6cd5\u591a\u4f9d\u8d56\u7edf\u8ba1\u76f8\u5173\u6027\u800c\u975e\u56e0\u679c\u673a\u5236\uff0c\u4e14\u7f3a\u4e4f\u8de8\u6a21\u6001\u56e0\u679c\u5efa\u6a21\u3002", "method": "CMDCL\u901a\u8fc7\u6587\u672c\u56e0\u679c\u5e72\u9884\u89e3\u51b3\u6587\u672c\u5d4c\u5165\u7684\u8de8\u6a21\u6001\u504f\u5dee\uff0c\u5e76\u901a\u8fc7\u89c6\u89c9\u56e0\u679c\u5e72\u9884\u53bb\u9664\u89c6\u89c9\u6a21\u6001\u4e2d\u7684\u5e72\u6270\u56e0\u7d20\u3002", "result": "\u5728Charades\u3001Breakfast\u548cCOIN\u4e09\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u9a8c\u8bc1\u4e86CMDCL\u7684\u6709\u6548\u6027\u3002", "conclusion": "CMDCL\u901a\u8fc7\u53cc\u56e0\u679c\u5e72\u9884\u663e\u8457\u63d0\u5347\u4e86\u957f\u671f\u52a8\u4f5c\u8bc6\u522b\u7684\u6027\u80fd\u3002"}}
{"id": "2507.06803", "pdf": "https://arxiv.org/pdf/2507.06803", "abs": "https://arxiv.org/abs/2507.06803", "authors": ["Matthew Anderson Hendricks", "Alice Cicirello"], "title": "Text to model via SysML: Automated generation of dynamical system computational models from unstructured natural language text via enhanced System Modeling Language diagrams", "categories": ["cs.CL", "cs.AI", "cs.CE"], "comment": null, "summary": "This paper contributes to speeding up the design and deployment of\nengineering dynamical systems by proposing a strategy for exploiting domain and\nexpert knowledge for the automated generation of dynamical system computational\nmodel starting from a corpus of document relevant to the dynamical system of\ninterest and an input document describing the specific system. This strategy is\nimplemented in five steps and, crucially, it uses system modeling language\ndiagrams (SysML) to extract accurate information about the dependencies,\nattributes, and operations of components. Natural Language Processing (NLP)\nstrategies and Large Language Models (LLMs) are employed in specific tasks to\nimprove intermediate outputs of the SySML diagrams automated generation, such\nas: list of key nouns; list of extracted relationships; list of key phrases and\nkey relationships; block attribute values; block relationships; and BDD diagram\ngeneration. The applicability of automated SysML diagram generation is\nillustrated with different case studies. The computational models of complex\ndynamical systems from SysML diagrams are then obtained via code generation and\ncomputational model generation steps. In the code generation step, NLP\nstrategies are used for summarization, while LLMs are used for validation only.\nThe proposed approach is not limited to a specific system, domain, or\ncomputational software. The applicability of the proposed approach is shown via\nan end-to-end example from text to model of a simple pendulum, showing improved\nperformance compared to results yielded by LLMs only.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u5229\u7528\u9886\u57df\u548c\u4e13\u5bb6\u77e5\u8bc6\u81ea\u52a8\u751f\u6210\u52a8\u6001\u7cfb\u7edf\u8ba1\u7b97\u6a21\u578b\u7684\u7b56\u7565\uff0c\u901a\u8fc7SysML\u56fe\u3001NLP\u548cLLM\u63d0\u5347\u751f\u6210\u6548\u7387\uff0c\u5e76\u901a\u8fc7\u6848\u4f8b\u9a8c\u8bc1\u5176\u6027\u80fd\u4f18\u4e8e\u4ec5\u7528LLM\u7684\u65b9\u6cd5\u3002", "motivation": "\u52a0\u901f\u5de5\u7a0b\u52a8\u6001\u7cfb\u7edf\u7684\u8bbe\u8ba1\u548c\u90e8\u7f72\uff0c\u901a\u8fc7\u81ea\u52a8\u5316\u751f\u6210\u8ba1\u7b97\u6a21\u578b\u51cf\u5c11\u4eba\u5de5\u5e72\u9884\u3002", "method": "\u5206\u4e94\u6b65\u5b9e\u73b0\uff0c\u7ed3\u5408SysML\u56fe\u3001NLP\u548cLLM\u6280\u672f\uff0c\u63d0\u53d6\u5173\u952e\u4fe1\u606f\u5e76\u751f\u6210\u6a21\u578b\u3002", "result": "\u901a\u8fc7\u6848\u4f8b\u7814\u7a76\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u6027\u80fd\u4f18\u4e8e\u4ec5\u4f9d\u8d56LLM\u7684\u751f\u6210\u65b9\u5f0f\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5177\u6709\u901a\u7528\u6027\uff0c\u9002\u7528\u4e8e\u591a\u79cd\u7cfb\u7edf\u548c\u9886\u57df\uff0c\u5e76\u80fd\u663e\u8457\u63d0\u5347\u6a21\u578b\u751f\u6210\u6548\u7387\u3002"}}
{"id": "2507.06582", "pdf": "https://arxiv.org/pdf/2507.06582", "abs": "https://arxiv.org/abs/2507.06582", "authors": ["Peter N. Loxley", "Friedrich T. Sommer"], "title": "Learning controllable dynamics through informative exploration", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Environments with controllable dynamics are usually understood in terms of\nexplicit models. However, such models are not always available, but may\nsometimes be learned by exploring an environment. In this work, we investigate\nusing an information measure called \"predicted information gain\" to determine\nthe most informative regions of an environment to explore next. Applying\nmethods from reinforcement learning allows good suboptimal exploring policies\nto be found, and leads to reliable estimates of the underlying controllable\ndynamics. This approach is demonstrated by comparing with several myopic\nexploration approaches.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u201c\u9884\u6d4b\u4fe1\u606f\u589e\u76ca\u201d\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u63a2\u7d22\u73af\u5883\u4e2d\u4fe1\u606f\u91cf\u6700\u5927\u7684\u533a\u57df\uff0c\u5e76\u7ed3\u5408\u5f3a\u5316\u5b66\u4e60\u627e\u5230\u6709\u6548\u7684\u63a2\u7d22\u7b56\u7565\u3002", "motivation": "\u5728\u65e0\u6cd5\u83b7\u53d6\u663e\u5f0f\u6a21\u578b\u7684\u73af\u5883\u4e2d\uff0c\u901a\u8fc7\u5b66\u4e60\u63a2\u7d22\u6765\u7406\u89e3\u53ef\u63a7\u52a8\u6001\u3002", "method": "\u4f7f\u7528\u201c\u9884\u6d4b\u4fe1\u606f\u589e\u76ca\u201d\u4f5c\u4e3a\u4fe1\u606f\u5ea6\u91cf\uff0c\u7ed3\u5408\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u5bfb\u627e\u6b21\u4f18\u63a2\u7d22\u7b56\u7565\u3002", "result": "\u8be5\u65b9\u6cd5\u80fd\u591f\u53ef\u9760\u4f30\u8ba1\u73af\u5883\u7684\u53ef\u63a7\u52a8\u6001\uff0c\u4f18\u4e8e\u51e0\u79cd\u77ed\u89c6\u63a2\u7d22\u65b9\u6cd5\u3002", "conclusion": "\u901a\u8fc7\u4fe1\u606f\u589e\u76ca\u548c\u5f3a\u5316\u5b66\u4e60\u7684\u7ed3\u5408\uff0c\u80fd\u591f\u6709\u6548\u63a2\u7d22\u672a\u77e5\u73af\u5883\u5e76\u4f30\u8ba1\u5176\u52a8\u6001\u3002"}}
{"id": "2507.06606", "pdf": "https://arxiv.org/pdf/2507.06606", "abs": "https://arxiv.org/abs/2507.06606", "authors": ["Qing Zhang", "Guoquan Pei", "Yan Wang"], "title": "Omni-Fusion of Spatial and Spectral for Hyperspectral Image Segmentation", "categories": ["cs.CV"], "comment": null, "summary": "Medical Hyperspectral Imaging (MHSI) has emerged as a promising tool for\nenhanced disease diagnosis, particularly in computational pathology, offering\nrich spectral information that aids in identifying subtle biochemical\nproperties of tissues. Despite these advantages, effectively fusing both\nspatial-dimensional and spectral-dimensional information from MHSIs remains\nchallenging due to its high dimensionality and spectral redundancy inherent\ncharacteristics. To solve the above challenges, we propose a novel\nspatial-spectral omni-fusion network for hyperspectral image segmentation,\nnamed as Omni-Fuse. Here, we introduce abundant cross-dimensional feature\nfusion operations, including a cross-dimensional enhancement module that\nrefines both spatial and spectral features through bidirectional attention\nmechanisms, a spectral-guided spatial query selection to select the most\nspectral-related spatial feature as the query, and a two-stage\ncross-dimensional decoder which dynamically guide the model to focus on the\nselected spatial query. Despite of numerous attention blocks, Omni-Fuse remains\nefficient in execution. Experiments on two microscopic hyperspectral image\ndatasets show that our approach can significantly improve the segmentation\nperformance compared with the state-of-the-art methods, with over 5.73 percent\nimprovement in DSC. Code available at:\nhttps://github.com/DeepMed-Lab-ECNU/Omni-Fuse.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aOmni-Fuse\u7684\u65b0\u578b\u7a7a\u95f4-\u5149\u8c31\u5168\u878d\u5408\u7f51\u7edc\uff0c\u7528\u4e8e\u9ad8\u5149\u8c31\u56fe\u50cf\u5206\u5272\uff0c\u901a\u8fc7\u8de8\u7ef4\u5ea6\u7279\u5f81\u878d\u5408\u548c\u53cc\u5411\u6ce8\u610f\u529b\u673a\u5236\u663e\u8457\u63d0\u5347\u4e86\u5206\u5272\u6027\u80fd\u3002", "motivation": "\u533b\u5b66\u9ad8\u5149\u8c31\u6210\u50cf\uff08MHSI\uff09\u5728\u75be\u75c5\u8bca\u65ad\u4e2d\u6f5c\u529b\u5de8\u5927\uff0c\u4f46\u9ad8\u7ef4\u5ea6\u548c\u5149\u8c31\u5197\u4f59\u7279\u6027\u4f7f\u5176\u7a7a\u95f4\u548c\u5149\u8c31\u4fe1\u606f\u878d\u5408\u6210\u4e3a\u6311\u6218\u3002", "method": "\u8bbe\u8ba1\u4e86\u8de8\u7ef4\u5ea6\u589e\u5f3a\u6a21\u5757\u3001\u5149\u8c31\u5f15\u5bfc\u7684\u7a7a\u95f4\u67e5\u8be2\u9009\u62e9\u548c\u4e24\u9636\u6bb5\u8de8\u7ef4\u5ea6\u89e3\u7801\u5668\uff0c\u52a8\u6001\u4f18\u5316\u6a21\u578b\u5bf9\u7a7a\u95f4\u67e5\u8be2\u7684\u5173\u6ce8\u3002", "result": "\u5728\u4e24\u4e2a\u663e\u5fae\u9ad8\u5149\u8c31\u56fe\u50cf\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cOmni-Fuse\u5728\u5206\u5272\u6027\u80fd\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0cDSC\u63d0\u5347\u8d85\u8fc75.73%\u3002", "conclusion": "Omni-Fuse\u901a\u8fc7\u9ad8\u6548\u7684\u8de8\u7ef4\u5ea6\u7279\u5f81\u878d\u5408\uff0c\u6709\u6548\u89e3\u51b3\u4e86MHSI\u7684\u9ad8\u7ef4\u5ea6\u548c\u5197\u4f59\u95ee\u9898\uff0c\u4e3a\u9ad8\u5149\u8c31\u56fe\u50cf\u5206\u5272\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2507.06323", "pdf": "https://arxiv.org/pdf/2507.06323", "abs": "https://arxiv.org/abs/2507.06323", "authors": ["Tarek Gasmi", "Ramzi Guesmi", "Ines Belhadj", "Jihene Bennaceur"], "title": "Bridging AI and Software Security: A Comparative Vulnerability Assessment of LLM Agent Deployment Paradigms", "categories": ["cs.CR", "cs.AI"], "comment": null, "summary": "Large Language Model (LLM) agents face security vulnerabilities spanning\nAI-specific and traditional software domains, yet current research addresses\nthese separately. This study bridges this gap through comparative evaluation of\nFunction Calling architecture and Model Context Protocol (MCP) deployment\nparadigms using a unified threat classification framework. We tested 3,250\nattack scenarios across seven language models, evaluating simple, composed, and\nchained attacks targeting both AI-specific threats (prompt injection) and\nsoftware vulnerabilities (JSON injection, denial-of-service). Function Calling\nshowed higher overall attack success rates (73.5% vs 62.59% for MCP), with\ngreater system-centric vulnerability while MCP exhibited increased LLM-centric\nexposure. Attack complexity dramatically amplified effectiveness, with chained\nattacks achieving 91-96% success rates. Counterintuitively, advanced reasoning\nmodels demonstrated higher exploitability despite better threat detection.\nResults demonstrate that architectural choices fundamentally reshape threat\nlandscapes. This work establishes methodological foundations for cross-domain\nLLM agent security assessment and provides evidence-based guidance for secure\ndeployment. Code and experimental materials are available at https: // github.\ncom/ theconsciouslab-ai/llm-agent-security.", "AI": {"tldr": "\u7814\u7a76\u6bd4\u8f83\u4e86Function Calling\u548cMCP\u4e24\u79cd\u67b6\u6784\u5728LLM\u4ee3\u7406\u5b89\u5168\u4e2d\u7684\u8868\u73b0\uff0c\u53d1\u73b0Function Calling\u653b\u51fb\u6210\u529f\u7387\u66f4\u9ad8\uff0c\u4e14\u653b\u51fb\u590d\u6742\u6027\u663e\u8457\u63d0\u5347\u6548\u679c\u3002", "motivation": "\u5f53\u524d\u7814\u7a76\u5206\u522b\u5904\u7406AI\u7279\u6709\u548c\u4f20\u7edf\u8f6f\u4ef6\u5b89\u5168\u6f0f\u6d1e\uff0c\u672c\u7814\u7a76\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u4f7f\u7528\u7edf\u4e00\u5a01\u80c1\u5206\u7c7b\u6846\u67b6\uff0c\u6d4b\u8bd5\u4e863,250\u79cd\u653b\u51fb\u573a\u666f\uff0c\u6db5\u76d6\u4e03\u79cd\u8bed\u8a00\u6a21\u578b\u3002", "result": "Function Calling\u653b\u51fb\u6210\u529f\u7387\u4e3a73.5%\uff0cMCP\u4e3a62.59%\uff1b\u590d\u6742\u653b\u51fb\u6210\u529f\u7387\u9ad8\u8fbe91-96%\u3002", "conclusion": "\u67b6\u6784\u9009\u62e9\u91cd\u5851\u5a01\u80c1\u683c\u5c40\uff0c\u4e3a\u8de8\u9886\u57dfLLM\u4ee3\u7406\u5b89\u5168\u8bc4\u4f30\u63d0\u4f9b\u65b9\u6cd5\u8bba\u57fa\u7840\u3002"}}
{"id": "2507.06829", "pdf": "https://arxiv.org/pdf/2507.06829", "abs": "https://arxiv.org/abs/2507.06829", "authors": ["Zenan Xu", "Zexuan Qiu", "Guanhua Huang", "Kun Li", "Siheng Li", "Chenchen Zhang", "Kejiao Li", "Qi Yi", "Yuhao Jiang", "Bo Zhou", "Fengzong Lian", "Zhanhui Kang"], "title": "Adaptive Termination for Multi-round Parallel Reasoning: An Universal Semantic Entropy-Guided Framework", "categories": ["cs.CL"], "comment": "13 pages, 5 fiures", "summary": "Recent advances in large language models (LLMs) have accelerated progress\ntoward artificial general intelligence, with inference-time scaling emerging as\na key technique. Contemporary approaches leverage either sequential reasoning\n(iteratively extending chains of thought) or parallel reasoning (generating\nmultiple solutions simultaneously) to scale inference. However, both paradigms\nface fundamental limitations: sequential scaling typically relies on arbitrary\ntoken budgets for termination, leading to inefficiency or premature cutoff;\nwhile parallel scaling often lacks coordination among parallel branches and\nrequires intrusive fine-tuning to perform effectively. In light of these\nchallenges, we aim to design a flexible test-time collaborative inference\nframework that exploits the complementary strengths of both sequential and\nparallel reasoning paradigms. Towards this goal, the core challenge lies in\ndeveloping an efficient and accurate intrinsic quality metric to assess model\nresponses during collaborative inference, enabling dynamic control and early\ntermination of the reasoning trace. To address this challenge, we introduce\nsemantic entropy (SE), which quantifies the semantic diversity of parallel\nmodel responses and serves as a robust indicator of reasoning quality due to\nits strong negative correlation with accuracy...", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u987a\u5e8f\u63a8\u7406\u548c\u5e76\u884c\u63a8\u7406\u4f18\u52bf\u7684\u534f\u4f5c\u63a8\u7406\u6846\u67b6\uff0c\u5e76\u5f15\u5165\u8bed\u4e49\u71b5\uff08SE\uff09\u4f5c\u4e3a\u52a8\u6001\u63a7\u5236\u63a8\u7406\u8d28\u91cf\u7684\u6307\u6807\u3002", "motivation": "\u73b0\u6709\u987a\u5e8f\u63a8\u7406\u548c\u5e76\u884c\u63a8\u7406\u65b9\u6cd5\u5b58\u5728\u6548\u7387\u4f4e\u6216\u7f3a\u4e4f\u534f\u8c03\u7684\u95ee\u9898\uff0c\u9700\u8981\u4e00\u79cd\u66f4\u7075\u6d3b\u7684\u63a8\u7406\u6846\u67b6\u3002", "method": "\u8bbe\u8ba1\u4e86\u534f\u4f5c\u63a8\u7406\u6846\u67b6\uff0c\u5229\u7528\u8bed\u4e49\u71b5\uff08SE\uff09\u8bc4\u4f30\u5e76\u884c\u6a21\u578b\u54cd\u5e94\u7684\u8bed\u4e49\u591a\u6837\u6027\uff0c\u52a8\u6001\u63a7\u5236\u63a8\u7406\u8fc7\u7a0b\u3002", "result": "\u8bed\u4e49\u71b5\uff08SE\uff09\u4e0e\u51c6\u786e\u6027\u5448\u5f3a\u8d1f\u76f8\u5173\uff0c\u80fd\u6709\u6548\u6307\u793a\u63a8\u7406\u8d28\u91cf\u3002", "conclusion": "\u534f\u4f5c\u63a8\u7406\u6846\u67b6\u7ed3\u5408\u4e86\u4e24\u79cd\u63a8\u7406\u8303\u5f0f\u7684\u4f18\u52bf\uff0c\u8bed\u4e49\u71b5\uff08SE\uff09\u4e3a\u52a8\u6001\u63a7\u5236\u63d0\u4f9b\u4e86\u53ef\u9760\u6307\u6807\u3002"}}
{"id": "2507.06602", "pdf": "https://arxiv.org/pdf/2507.06602", "abs": "https://arxiv.org/abs/2507.06602", "authors": ["Burak Demirel", "Yu Wang", "Cristian Tatino", "Pablo Soldati"], "title": "Generalization in Reinforcement Learning for Radio Access Networks", "categories": ["cs.LG"], "comment": null, "summary": "Modern RAN operate in highly dynamic and heterogeneous environments, where\nhand-tuned, rule-based RRM algorithms often underperform. While RL can surpass\nsuch heuristics in constrained settings, the diversity of deployments and\nunpredictable radio conditions introduce major generalization challenges.\nData-driven policies frequently overfit to training conditions, degrading\nperformance in unseen scenarios. To address this, we propose a\ngeneralization-centered RL framework for RAN control that: (i) encodes cell\ntopology and node attributes via attention-based graph representations; (ii)\napplies domain randomization to broaden the training distribution; and (iii)\ndistributes data generation across multiple actors while centralizing training\nin a cloud-compatible architecture aligned with O-RAN principles. Although\ngeneralization increases computational and data-management complexity, our\ndistributed design mitigates this by scaling data collection and training\nacross diverse network conditions. Applied to downlink link adaptation in five\n5G benchmarks, our policy improves average throughput and spectral efficiency\nby ~10% over an OLLA baseline (10% BLER target) in full-buffer MIMO/mMIMO and\nby >20% under high mobility. It matches specialized RL in full-buffer traffic\nand achieves up to 4- and 2-fold gains in eMBB and mixed-traffic benchmarks,\nrespectively. In nine-cell deployments, GAT models offer 30% higher throughput\nover MLP baselines. These results, combined with our scalable architecture,\noffer a path toward AI-native 6G RAN using a single, generalizable RL agent.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u901a\u7528\u6846\u67b6\uff0c\u7528\u4e8e\u89e3\u51b3\u73b0\u4ee3\u65e0\u7ebf\u63a5\u5165\u7f51\u7edc\uff08RAN\uff09\u4e2d\u7684\u52a8\u6001\u548c\u5f02\u6784\u73af\u5883\u95ee\u9898\uff0c\u901a\u8fc7\u56fe\u6ce8\u610f\u529b\u7f51\u7edc\u548c\u5206\u5e03\u5f0f\u8bad\u7ec3\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u4f20\u7edf\u57fa\u4e8e\u89c4\u5219\u7684RRM\u7b97\u6cd5\u5728\u52a8\u6001\u548c\u5f02\u6784\u73af\u5883\u4e2d\u8868\u73b0\u4e0d\u4f73\uff0c\u800c\u73b0\u6709\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u5bb9\u6613\u8fc7\u62df\u5408\u8bad\u7ec3\u6761\u4ef6\uff0c\u65e0\u6cd5\u6cdb\u5316\u5230\u65b0\u573a\u666f\u3002", "method": "\u91c7\u7528\u6ce8\u610f\u529b\u56fe\u8868\u793a\u7f51\u7edc\u62d3\u6251\u548c\u8282\u70b9\u5c5e\u6027\uff0c\u7ed3\u5408\u9886\u57df\u968f\u673a\u5316\u548c\u5206\u5e03\u5f0f\u6570\u636e\u751f\u6210\uff0c\u96c6\u4e2d\u8bad\u7ec3\u4ee5\u9002\u5e94\u591a\u6837\u7f51\u7edc\u6761\u4ef6\u3002", "result": "\u57285G\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u5e73\u5747\u541e\u5410\u91cf\u548c\u9891\u8c31\u6548\u7387\u63d0\u534710%-20%\uff0c\u5728\u591a\u573a\u666f\u4e0b\u8868\u73b0\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\uff0c\u56fe\u6ce8\u610f\u529b\u6a21\u578b\u5728\u4e5d\u5c0f\u533a\u90e8\u7f72\u4e2d\u541e\u5410\u91cf\u63d0\u534730%\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3aAI\u539f\u751f6G RAN\u63d0\u4f9b\u4e86\u4e00\u79cd\u53ef\u6269\u5c55\u4e14\u6cdb\u5316\u80fd\u529b\u5f3a\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.06618", "pdf": "https://arxiv.org/pdf/2507.06618", "abs": "https://arxiv.org/abs/2507.06618", "authors": ["Yang Chen", "Yueqi Duan", "Haowen Sun", "Ziwei Wang", "Jiwen Lu", "Yap-Peng Tan"], "title": "PointVDP: Learning View-Dependent Projection by Fireworks Rays for 3D Point Cloud Segmentation", "categories": ["cs.CV"], "comment": null, "summary": "In this paper, we propose view-dependent projection (VDP) to facilitate point\ncloud segmentation, designing efficient 3D-to-2D mapping that dynamically\nadapts to the spatial geometry from view variations. Existing projection-based\nmethods leverage view-independent projection in complex scenes, relying on\nstraight lines to generate direct rays or upward curves to reduce occlusions.\nHowever, their view independence provides projection rays that are limited to\npre-defined parameters by human settings, restricting point awareness and\nfailing to capture sufficient projection diversity across different view\nplanes. Although multiple projections per view plane are commonly used to\nenhance spatial variety, the projected redundancy leads to excessive\ncomputational overhead and inefficiency in image processing. To address these\nlimitations, we design a framework of VDP to generate data-driven projections\nfrom 3D point distributions, producing highly informative single-image inputs\nby predicting rays inspired by the adaptive behavior of fireworks. In addition,\nwe construct color regularization to optimize the framework, which emphasizes\nessential features within semantic pixels and suppresses the non-semantic\nfeatures within black pixels, thereby maximizing 2D space utilization in a\nprojected image. As a result, our approach, PointVDP, develops lightweight\nprojections in marginal computation costs. Experiments on S3DIS and ScanNet\nbenchmarks show that our approach achieves competitive results, offering a\nresource-efficient solution for semantic understanding.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u89c6\u70b9\u4f9d\u8d56\u6295\u5f71\uff08VDP\uff09\u7684\u70b9\u4e91\u5206\u5272\u65b9\u6cd5\uff0c\u901a\u8fc7\u52a8\u6001\u9002\u5e94\u89c6\u70b9\u53d8\u5316\u76843D\u52302D\u6620\u5c04\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u6295\u5f71\u65b9\u6cd5\u56e0\u56fa\u5b9a\u53c2\u6570\u5bfc\u81f4\u7684\u591a\u6837\u6027\u548c\u6548\u7387\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u6295\u5f71\u65b9\u6cd5\u4f9d\u8d56\u56fa\u5b9a\u53c2\u6570\uff0c\u9650\u5236\u4e86\u70b9\u4e91\u611f\u77e5\u548c\u6295\u5f71\u591a\u6837\u6027\uff0c\u4e14\u591a\u6295\u5f71\u5bfc\u81f4\u8ba1\u7b97\u5197\u4f59\u3002", "method": "\u8bbe\u8ba1\u4e86VDP\u6846\u67b6\uff0c\u901a\u8fc7\u6570\u636e\u9a71\u52a8\u7684\u6295\u5f71\u751f\u6210\u81ea\u9002\u5e94\u5c04\u7ebf\uff0c\u5e76\u7ed3\u5408\u989c\u8272\u6b63\u5219\u5316\u4f18\u5316\u6295\u5f71\u56fe\u50cf\u3002", "result": "\u5728S3DIS\u548cScanNet\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u8ba1\u7b97\u6210\u672c\u4f4e\u3002", "conclusion": "PointVDP\u63d0\u4f9b\u4e86\u4e00\u79cd\u8d44\u6e90\u9ad8\u6548\u7684\u8bed\u4e49\u7406\u89e3\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.06838", "pdf": "https://arxiv.org/pdf/2507.06838", "abs": "https://arxiv.org/abs/2507.06838", "authors": ["Dahyun Lee", "Yongrae Jo", "Haeju Park", "Moontae Lee"], "title": "Shifting from Ranking to Set Selection for Retrieval Augmented Generation", "categories": ["cs.CL", "cs.IR"], "comment": "Accepted to ACL 2025 Oral", "summary": "Retrieval in Retrieval-Augmented Generation(RAG) must ensure that retrieved\npassages are not only individually relevant but also collectively form a\ncomprehensive set. Existing approaches primarily rerank top-k passages based on\ntheir individual relevance, often failing to meet the information needs of\ncomplex queries in multi-hop question answering. In this work, we propose a\nset-wise passage selection approach and introduce SETR, which explicitly\nidentifies the information requirements of a query through Chain-of-Thought\nreasoning and selects an optimal set of passages that collectively satisfy\nthose requirements. Experiments on multi-hop RAG benchmarks show that SETR\noutperforms both proprietary LLM-based rerankers and open-source baselines in\nterms of answer correctness and retrieval quality, providing an effective and\nefficient alternative to traditional rerankers in RAG systems. The code is\navailable at https://github.com/LGAI-Research/SetR", "AI": {"tldr": "SETR\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u96c6\u5408\u7684\u6bb5\u843d\u9009\u62e9\u65b9\u6cd5\uff0c\u901a\u8fc7Chain-of-Thought\u63a8\u7406\u660e\u786e\u67e5\u8be2\u7684\u4fe1\u606f\u9700\u6c42\uff0c\u5e76\u9009\u62e9\u6700\u4f18\u6bb5\u843d\u96c6\u5408\u4ee5\u6ee1\u8db3\u9700\u6c42\uff0c\u663e\u8457\u63d0\u5347\u4e86\u591a\u8df3\u95ee\u7b54\u4e2d\u7684\u68c0\u7d22\u8d28\u91cf\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4e3b\u8981\u57fa\u4e8e\u6bb5\u843d\u4e2a\u4f53\u76f8\u5173\u6027\u91cd\u6392\u5e8f\uff0c\u96be\u4ee5\u6ee1\u8db3\u590d\u6742\u67e5\u8be2\uff08\u5982\u591a\u8df3\u95ee\u7b54\uff09\u7684\u4fe1\u606f\u9700\u6c42\u3002", "method": "\u5f15\u5165SETR\uff0c\u901a\u8fc7Chain-of-Thought\u63a8\u7406\u8bc6\u522b\u67e5\u8be2\u7684\u4fe1\u606f\u9700\u6c42\uff0c\u5e76\u9009\u62e9\u6700\u4f18\u6bb5\u843d\u96c6\u5408\u3002", "result": "\u5728\u591a\u8df3RAG\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cSETR\u5728\u7b54\u6848\u6b63\u786e\u6027\u548c\u68c0\u7d22\u8d28\u91cf\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "SETR\u4e3aRAG\u7cfb\u7edf\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u4e14\u6709\u6548\u7684\u66ff\u4ee3\u65b9\u6848\uff0c\u4f18\u4e8e\u4f20\u7edf\u91cd\u6392\u5e8f\u65b9\u6cd5\u3002"}}
{"id": "2507.06613", "pdf": "https://arxiv.org/pdf/2507.06613", "abs": "https://arxiv.org/abs/2507.06613", "authors": ["Anshuk Uppal", "Yuhta Takida", "Chieh-Hsin Lai", "Yuki Mitsufuji"], "title": "Denoising Multi-Beta VAE: Representation Learning for Disentanglement and Generation", "categories": ["cs.LG", "cs.AI", "cs.CV"], "comment": "24 pages, 8 figures and 7 tables", "summary": "Disentangled and interpretable latent representations in generative models\ntypically come at the cost of generation quality. The $\\beta$-VAE framework\nintroduces a hyperparameter $\\beta$ to balance disentanglement and\nreconstruction quality, where setting $\\beta > 1$ introduces an information\nbottleneck that favors disentanglement over sharp, accurate reconstructions. To\naddress this trade-off, we propose a novel generative modeling framework that\nleverages a range of $\\beta$ values to learn multiple corresponding latent\nrepresentations. First, we obtain a slew of representations by training a\nsingle variational autoencoder (VAE), with a new loss function that controls\nthe information retained in each latent representation such that the higher\n$\\beta$ value prioritize disentanglement over reconstruction fidelity. We then,\nintroduce a non-linear diffusion model that smoothly transitions latent\nrepresentations corresponding to different $\\beta$ values. This model denoises\ntowards less disentangled and more informative representations, ultimately\nleading to (almost) lossless representations, enabling sharp reconstructions.\nFurthermore, our model supports sample generation without input images,\nfunctioning as a standalone generative model. We evaluate our framework in\nterms of both disentanglement and generation quality. Additionally, we observe\nsmooth transitions in the latent spaces with respect to changes in $\\beta$,\nfacilitating consistent manipulation of generated outputs.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u751f\u6210\u6a21\u578b\u6846\u67b6\uff0c\u901a\u8fc7\u4f7f\u7528\u4e0d\u540c\u03b2\u503c\u7684VAE\u548c\u6269\u6563\u6a21\u578b\uff0c\u5e73\u8861\u89e3\u7f20\u4e0e\u91cd\u5efa\u8d28\u91cf\uff0c\u5b9e\u73b0\u9ad8\u8d28\u91cf\u751f\u6210\u548c\u89e3\u7f20\u8868\u793a\u3002", "motivation": "\u89e3\u51b3\u751f\u6210\u6a21\u578b\u4e2d\u89e3\u7f20\u8868\u793a\u4e0e\u91cd\u5efa\u8d28\u91cf\u4e4b\u95f4\u7684\u6743\u8861\u95ee\u9898\u3002", "method": "\u7ed3\u5408\u591a\u03b2\u503c\u7684VAE\u8bad\u7ec3\u548c\u6269\u6563\u6a21\u578b\uff0c\u5e73\u6ed1\u8fc7\u6e21\u4e0d\u540c\u03b2\u503c\u7684\u6f5c\u5728\u8868\u793a\u3002", "result": "\u5b9e\u73b0\u4e86\u9ad8\u8d28\u91cf\u7684\u89e3\u7f20\u8868\u793a\u548c\u91cd\u5efa\uff0c\u652f\u6301\u65e0\u8f93\u5165\u56fe\u50cf\u7684\u751f\u6210\u3002", "conclusion": "\u8be5\u6846\u67b6\u5728\u89e3\u7f20\u548c\u751f\u6210\u8d28\u91cf\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u652f\u6301\u6f5c\u5728\u7a7a\u95f4\u7684\u5e73\u6ed1\u8fc7\u6e21\u3002"}}
{"id": "2507.06639", "pdf": "https://arxiv.org/pdf/2507.06639", "abs": "https://arxiv.org/abs/2507.06639", "authors": ["Myungjang Pyeon", "Janghyeon Lee", "Minsoo Lee", "Juseung Yun", "Hwanil Choi", "Jonghyun Kim", "Jiwon Kim", "Yi Hu", "Jongseong Jang", "Soonyoung Lee"], "title": "EXAONE Path 2.0: Pathology Foundation Model with End-to-End Supervision", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": "EXAONE Path 2.0 technical report", "summary": "In digital pathology, whole-slide images (WSIs) are often difficult to handle\ndue to their gigapixel scale, so most approaches train patch encoders via\nself-supervised learning (SSL) and then aggregate the patch-level embeddings\nvia multiple instance learning (MIL) or slide encoders for downstream tasks.\nHowever, patch-level SSL may overlook complex domain-specific features that are\nessential for biomarker prediction, such as mutation status and molecular\ncharacteristics, as SSL methods rely only on basic augmentations selected for\nnatural image domains on small patch-level area. Moreover, SSL methods remain\nless data efficient than fully supervised approaches, requiring extensive\ncomputational resources and datasets to achieve competitive performance. To\naddress these limitations, we present EXAONE Path 2.0, a pathology foundation\nmodel that learns patch-level representations under direct slide-level\nsupervision. Using only 37k WSIs for training, EXAONE Path 2.0 achieves\nstate-of-the-art average performance across 10 biomarker prediction tasks,\ndemonstrating remarkable data efficiency.", "AI": {"tldr": "EXAONE Path 2.0\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5168\u5207\u7247\u76d1\u7763\u7684\u75c5\u7406\u5b66\u57fa\u7840\u6a21\u578b\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u81ea\u76d1\u7763\u5b66\u4e60\u65b9\u6cd5\u5728\u751f\u7269\u6807\u5fd7\u7269\u9884\u6d4b\u4e2d\u7684\u5c40\u9650\u6027\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u6570\u636e\u6548\u7387\u3002", "motivation": "\u4f20\u7edf\u81ea\u76d1\u7763\u5b66\u4e60\uff08SSL\uff09\u65b9\u6cd5\u5728\u6570\u5b57\u75c5\u7406\u5b66\u4e2d\u56e0\u4f9d\u8d56\u5c0f\u533a\u57df\u8865\u4e01\u7ea7\u8bad\u7ec3\u548c\u81ea\u7136\u56fe\u50cf\u9886\u57df\u7684\u589e\u5f3a\u65b9\u6cd5\uff0c\u53ef\u80fd\u5ffd\u7565\u590d\u6742\u7684\u9886\u57df\u7279\u5f02\u6027\u7279\u5f81\uff0c\u4e14\u6570\u636e\u6548\u7387\u8f83\u4f4e\u3002", "method": "EXAONE Path 2.0\u901a\u8fc7\u76f4\u63a5\u5229\u7528\u5168\u5207\u7247\u7ea7\u76d1\u7763\u5b66\u4e60\u8865\u4e01\u7ea7\u8868\u793a\uff0c\u4ec5\u970037k\u5168\u5207\u7247\u56fe\u50cf\u8fdb\u884c\u8bad\u7ec3\u3002", "result": "\u572810\u9879\u751f\u7269\u6807\u5fd7\u7269\u9884\u6d4b\u4efb\u52a1\u4e2d\uff0cEXAONE Path 2.0\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u5e73\u5747\u6027\u80fd\uff0c\u8868\u73b0\u51fa\u5353\u8d8a\u7684\u6570\u636e\u6548\u7387\u3002", "conclusion": "EXAONE Path 2.0\u4e3a\u75c5\u7406\u5b66\u9886\u57df\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u4e14\u6027\u80fd\u4f18\u8d8a\u7684\u57fa\u7840\u6a21\u578b\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u5c40\u9650\u6027\u3002"}}
{"id": "2507.06893", "pdf": "https://arxiv.org/pdf/2507.06893", "abs": "https://arxiv.org/abs/2507.06893", "authors": ["Alexandra Abbas", "Celia Waggoner", "Justin Olive"], "title": "Developing and Maintaining an Open-Source Repository of AI Evaluations: Challenges and Insights", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "AI evaluations have become critical tools for assessing large language model\ncapabilities and safety. This paper presents practical insights from eight\nmonths of maintaining $inspect\\_evals$, an open-source repository of 70+\ncommunity-contributed AI evaluations. We identify key challenges in\nimplementing and maintaining AI evaluations and develop solutions including:\n(1) a structured cohort management framework for scaling community\ncontributions, (2) statistical methodologies for optimal resampling and\ncross-model comparison with uncertainty quantification, and (3) systematic\nquality control processes for reproducibility. Our analysis reveals that AI\nevaluation requires specialized infrastructure, statistical rigor, and\ncommunity coordination beyond traditional software development practices.", "AI": {"tldr": "\u672c\u6587\u603b\u7ed3\u4e86\u7ef4\u62a4\u5f00\u6e90AI\u8bc4\u4f30\u5de5\u5177\u5e93\u7684\u5b9e\u8df5\u7ecf\u9a8c\uff0c\u63d0\u51fa\u4e86\u89e3\u51b3\u793e\u533a\u8d21\u732e\u6269\u5c55\u3001\u7edf\u8ba1\u65b9\u6cd5\u548c\u8d28\u91cf\u63a7\u5236\u7684\u5173\u952e\u6311\u6218\u3002", "motivation": "AI\u8bc4\u4f30\u5bf9\u8bc4\u4f30\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u80fd\u529b\u548c\u5b89\u5168\u6027\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u5b9e\u65bd\u548c\u7ef4\u62a4\u8fd9\u4e9b\u8bc4\u4f30\u9762\u4e34\u8bf8\u591a\u6311\u6218\u3002", "method": "\u63d0\u51fa\u4e86\u4e09\u79cd\u89e3\u51b3\u65b9\u6848\uff1a(1) \u7ed3\u6784\u5316\u961f\u5217\u7ba1\u7406\u6846\u67b6\uff0c(2) \u7edf\u8ba1\u65b9\u6cd5\u4f18\u5316\u91cd\u91c7\u6837\u548c\u8de8\u6a21\u578b\u6bd4\u8f83\uff0c(3) \u7cfb\u7edf\u5316\u7684\u8d28\u91cf\u63a7\u5236\u6d41\u7a0b\u3002", "result": "\u5206\u6790\u8868\u660e\uff0cAI\u8bc4\u4f30\u9700\u8981\u4e13\u95e8\u7684\u57fa\u7840\u8bbe\u65bd\u3001\u7edf\u8ba1\u4e25\u8c28\u6027\u548c\u793e\u533a\u534f\u8c03\u3002", "conclusion": "AI\u8bc4\u4f30\u9700\u8981\u8d85\u8d8a\u4f20\u7edf\u8f6f\u4ef6\u5f00\u53d1\u5b9e\u8df5\u7684\u4e13\u95e8\u65b9\u6cd5\u548c\u5de5\u5177\u3002"}}
{"id": "2507.06615", "pdf": "https://arxiv.org/pdf/2507.06615", "abs": "https://arxiv.org/abs/2507.06615", "authors": ["Jinmin He", "Kai Li", "Yifan Zang", "Haobo Fu", "Qiang Fu", "Junliang Xing", "Jian Cheng"], "title": "Efficient Multi-Task Reinforcement Learning with Cross-Task Policy Guidance", "categories": ["cs.LG", "cs.AI"], "comment": "NeurIPS2024", "summary": "Multi-task reinforcement learning endeavors to efficiently leverage shared\ninformation across various tasks, facilitating the simultaneous learning of\nmultiple tasks. Existing approaches primarily focus on parameter sharing with\ncarefully designed network structures or tailored optimization procedures.\nHowever, they overlook a direct and complementary way to exploit cross-task\nsimilarities: the control policies of tasks already proficient in some skills\ncan provide explicit guidance for unmastered tasks to accelerate skills\nacquisition. To this end, we present a novel framework called Cross-Task Policy\nGuidance (CTPG), which trains a guide policy for each task to select the\nbehavior policy interacting with the environment from all tasks' control\npolicies, generating better training trajectories. In addition, we propose two\ngating mechanisms to improve the learning efficiency of CTPG: one gate filters\nout control policies that are not beneficial for guidance, while the other gate\nblocks tasks that do not necessitate guidance. CTPG is a general framework\nadaptable to existing parameter sharing approaches. Empirical evaluations\ndemonstrate that incorporating CTPG with these approaches significantly\nenhances performance in manipulation and locomotion benchmarks.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aCTPG\u7684\u65b0\u6846\u67b6\uff0c\u901a\u8fc7\u8de8\u4efb\u52a1\u7b56\u7565\u6307\u5bfc\u52a0\u901f\u591a\u4efb\u52a1\u5f3a\u5316\u5b66\u4e60\uff0c\u5229\u7528\u5df2\u638c\u63e1\u4efb\u52a1\u7684\u7b56\u7565\u6307\u5bfc\u672a\u638c\u63e1\u4efb\u52a1\uff0c\u5e76\u7ed3\u5408\u95e8\u63a7\u673a\u5236\u63d0\u5347\u6548\u7387\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4e3b\u8981\u5173\u6ce8\u53c2\u6570\u5171\u4eab\uff0c\u4f46\u5ffd\u7565\u4e86\u5229\u7528\u8de8\u4efb\u52a1\u76f8\u4f3c\u6027\u7684\u76f4\u63a5\u65b9\u5f0f\uff0c\u5373\u5df2\u638c\u63e1\u4efb\u52a1\u7684\u7b56\u7565\u53ef\u4ee5\u4e3a\u672a\u638c\u63e1\u4efb\u52a1\u63d0\u4f9b\u663e\u5f0f\u6307\u5bfc\u3002", "method": "CTPG\u6846\u67b6\u4e3a\u6bcf\u4e2a\u4efb\u52a1\u8bad\u7ec3\u4e00\u4e2a\u6307\u5bfc\u7b56\u7565\uff0c\u4ece\u6240\u6709\u4efb\u52a1\u7684\u63a7\u5236\u7b56\u7565\u4e2d\u9009\u62e9\u884c\u4e3a\u7b56\u7565\uff0c\u751f\u6210\u66f4\u597d\u7684\u8bad\u7ec3\u8f68\u8ff9\uff0c\u5e76\u5f15\u5165\u4e24\u79cd\u95e8\u63a7\u673a\u5236\u4f18\u5316\u5b66\u4e60\u6548\u7387\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cCTPG\u4e0e\u73b0\u6709\u53c2\u6570\u5171\u4eab\u65b9\u6cd5\u7ed3\u5408\u540e\uff0c\u5728\u64cd\u4f5c\u548c\u8fd0\u52a8\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u6027\u80fd\u663e\u8457\u63d0\u5347\u3002", "conclusion": "CTPG\u662f\u4e00\u79cd\u901a\u7528\u6846\u67b6\uff0c\u80fd\u591f\u6709\u6548\u5229\u7528\u8de8\u4efb\u52a1\u76f8\u4f3c\u6027\uff0c\u63d0\u5347\u591a\u4efb\u52a1\u5f3a\u5316\u5b66\u4e60\u7684\u6027\u80fd\u3002"}}
{"id": "2507.06643", "pdf": "https://arxiv.org/pdf/2507.06643", "abs": "https://arxiv.org/abs/2507.06643", "authors": ["Farahdiba Zarin", "Riccardo Oliva", "Vinkle Srivastav", "Armine Vardazaryan", "Andrea Rosati", "Alice Zampolini Faustini", "Giovanni Scambia", "Anna Fagotti", "Pietro Mascagni", "Nicolas Padoy"], "title": "Learning from Sparse Point Labels for Dense Carcinosis Localization in Advanced Ovarian Cancer Assessment", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "Learning from sparse labels is a challenge commonplace in the medical domain.\nThis is due to numerous factors, such as annotation cost, and is especially\ntrue for newly introduced tasks. When dense pixel-level annotations are needed,\nthis becomes even more unfeasible. However, being able to learn from just a few\nannotations at the pixel-level, while extremely difficult and underutilized,\ncan drive progress in studies where perfect annotations are not immediately\navailable. This work tackles the challenge of learning the dense prediction\ntask of keypoint localization from a few point annotations in the context of 2d\ncarcinosis keypoint localization from laparoscopic video frames for diagnostic\nplanning of advanced ovarian cancer patients. To enable this, we formulate the\nproblem as a sparse heatmap regression from a few point annotations per image\nand propose a new loss function, called Crag and Tail loss, for efficient\nlearning. Our proposed loss function effectively leverages positive sparse\nlabels while minimizing the impact of false negatives or missed annotations.\nThrough an extensive ablation study, we demonstrate the effectiveness of our\napproach in achieving accurate dense localization of carcinosis keypoints,\nhighlighting its potential to advance research in scenarios where dense\nannotations are challenging to obtain.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aCrag and Tail\u7684\u635f\u5931\u51fd\u6570\uff0c\u7528\u4e8e\u4ece\u7a00\u758f\u6807\u6ce8\u4e2d\u5b66\u4e60\u5bc6\u96c6\u9884\u6d4b\u4efb\u52a1\uff0c\u7279\u522b\u662f\u5728\u533b\u5b66\u56fe\u50cf\u4e2d\u5173\u952e\u70b9\u5b9a\u4f4d\u7684\u5e94\u7528\u3002", "motivation": "\u533b\u5b66\u9886\u57df\u4e2d\u7a00\u758f\u6807\u6ce8\u5b66\u4e60\u662f\u4e00\u4e2a\u5e38\u89c1\u6311\u6218\uff0c\u5c24\u5176\u662f\u9700\u8981\u5bc6\u96c6\u50cf\u7d20\u7ea7\u6807\u6ce8\u65f6\u3002\u672c\u6587\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\uff0c\u63a8\u52a8\u5728\u6807\u6ce8\u4e0d\u5b8c\u5584\u60c5\u51b5\u4e0b\u7684\u7814\u7a76\u8fdb\u5c55\u3002", "method": "\u5c06\u95ee\u9898\u5efa\u6a21\u4e3a\u7a00\u758f\u70ed\u56fe\u56de\u5f52\uff0c\u5e76\u63d0\u51faCrag and Tail\u635f\u5931\u51fd\u6570\uff0c\u6709\u6548\u5229\u7528\u7a00\u758f\u6807\u6ce8\u5e76\u51cf\u5c11\u5047\u9634\u6027\u6216\u6f0f\u6807\u7684\u5f71\u54cd\u3002", "result": "\u901a\u8fc7\u5927\u91cf\u5b9e\u9a8c\u9a8c\u8bc1\uff0c\u8be5\u65b9\u6cd5\u57282D\u8179\u8154\u955c\u89c6\u9891\u5e27\u4e2d\u5b9e\u73b0\u4e86\u51c6\u786e\u7684\u764c\u53d8\u5173\u952e\u70b9\u5bc6\u96c6\u5b9a\u4f4d\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u6807\u6ce8\u96be\u4ee5\u83b7\u53d6\u7684\u573a\u666f\u4e2d\u5177\u6709\u6f5c\u529b\uff0c\u80fd\u591f\u63a8\u52a8\u76f8\u5173\u7814\u7a76\u7684\u8fdb\u5c55\u3002"}}
{"id": "2507.06895", "pdf": "https://arxiv.org/pdf/2507.06895", "abs": "https://arxiv.org/abs/2507.06895", "authors": ["Luca Mariotti", "Veronica Guidetti", "Federica Mandreoli"], "title": "SCoRE: Streamlined Corpus-based Relation Extraction using Multi-Label Contrastive Learning and Bayesian kNN", "categories": ["cs.CL", "cs.AI", "cs.IR", "cs.LG"], "comment": null, "summary": "The growing demand for efficient knowledge graph (KG) enrichment leveraging\nexternal corpora has intensified interest in relation extraction (RE),\nparticularly under low-supervision settings. To address the need for adaptable\nand noise-resilient RE solutions that integrate seamlessly with pre-trained\nlarge language models (PLMs), we introduce SCoRE, a modular and cost-effective\nsentence-level RE system. SCoRE enables easy PLM switching, requires no\nfinetuning, and adapts smoothly to diverse corpora and KGs. By combining\nsupervised contrastive learning with a Bayesian k-Nearest Neighbors (kNN)\nclassifier for multi-label classification, it delivers robust performance\ndespite the noisy annotations of distantly supervised corpora. To improve RE\nevaluation, we propose two novel metrics: Correlation Structure Distance (CSD),\nmeasuring the alignment between learned relational patterns and KG structures,\nand Precision at R (P@R), assessing utility as a recommender system. We also\nrelease Wiki20d, a benchmark dataset replicating real-world RE conditions where\nonly KG-derived annotations are available. Experiments on five benchmarks show\nthat SCoRE matches or surpasses state-of-the-art methods while significantly\nreducing energy consumption. Further analyses reveal that increasing model\ncomplexity, as seen in prior work, degrades performance, highlighting the\nadvantages of SCoRE's minimal design. Combining efficiency, modularity, and\nscalability, SCoRE stands as an optimal choice for real-world RE applications.", "AI": {"tldr": "SCoRE\u662f\u4e00\u4e2a\u6a21\u5757\u5316\u3001\u4f4e\u6210\u672c\u7684\u53e5\u5b50\u7ea7\u5173\u7cfb\u62bd\u53d6\u7cfb\u7edf\uff0c\u7ed3\u5408\u5bf9\u6bd4\u5b66\u4e60\u548c\u8d1d\u53f6\u65afkNN\u5206\u7c7b\u5668\uff0c\u5728\u566a\u58f0\u6807\u6ce8\u4e0b\u8868\u73b0\u4f18\u5f02\uff0c\u5e76\u63d0\u51fa\u4e86\u65b0\u7684\u8bc4\u4f30\u6307\u6807\u3002", "motivation": "\u89e3\u51b3\u77e5\u8bc6\u56fe\u8c31\uff08KG\uff09\u589e\u5f3a\u4e2d\u5173\u7cfb\u62bd\u53d6\uff08RE\uff09\u5728\u4f4e\u76d1\u7763\u8bbe\u7f6e\u4e0b\u7684\u9700\u6c42\uff0c\u7279\u522b\u662f\u9002\u5e94\u6027\u548c\u566a\u58f0\u9c81\u68d2\u6027\u3002", "method": "\u7ed3\u5408\u76d1\u7763\u5bf9\u6bd4\u5b66\u4e60\u548c\u8d1d\u53f6\u65afkNN\u5206\u7c7b\u5668\uff0c\u65e0\u9700\u5fae\u8c03\uff0c\u652f\u6301\u591a\u79cd\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\uff08PLM\uff09\u3002", "result": "\u5728\u4e94\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u4e8e\u6216\u5339\u914d\u73b0\u6709\u65b9\u6cd5\uff0c\u663e\u8457\u964d\u4f4e\u80fd\u8017\u3002", "conclusion": "SCoRE\u56e0\u5176\u9ad8\u6548\u3001\u6a21\u5757\u5316\u548c\u53ef\u6269\u5c55\u6027\uff0c\u6210\u4e3a\u73b0\u5b9e\u4e16\u754cRE\u5e94\u7528\u7684\u7406\u60f3\u9009\u62e9\u3002"}}
{"id": "2507.06619", "pdf": "https://arxiv.org/pdf/2507.06619", "abs": "https://arxiv.org/abs/2507.06619", "authors": ["Xiaobo Huang", "Fang Xie"], "title": "Steps Adaptive Decay DPSGD: Enhancing Performance on Imbalanced Datasets with Differential Privacy with HAM10000", "categories": ["cs.LG", "stat.ML"], "comment": null, "summary": "When applying machine learning to medical image classification, data leakage\nis a critical issue. Previous methods, such as adding noise to gradients for\ndifferential privacy, work well on large datasets like MNIST and CIFAR-100, but\nfail on small, imbalanced medical datasets like HAM10000. This is because the\nimbalanced distribution causes gradients from minority classes to be clipped\nand lose crucial information, while majority classes dominate. This leads the\nmodel to fall into suboptimal solutions early. To address this, we propose\nSAD-DPSGD, which uses a linear decaying mechanism for noise and clipping\nthresholds. By allocating more privacy budget and using higher clipping\nthresholds in the initial training phases, the model avoids suboptimal\nsolutions and enhances performance. Experiments show that SAD-DPSGD outperforms\nAuto-DPSGD on HAM10000, improving accuracy by 2.15% under $\\epsilon = 3.0$ ,\n$\\delta = 10^{-3}$.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2507.06647", "pdf": "https://arxiv.org/pdf/2507.06647", "abs": "https://arxiv.org/abs/2507.06647", "authors": ["Chengkun Li", "Yuqi Tong", "Kai Chen", "Zhenya Yang", "Ruiyang Li", "Shi Qiu", "Jason Ying-Kuen Chan", "Pheng-Ann Heng", "Qi Dou"], "title": "ClipGS: Clippable Gaussian Splatting for Interactive Cinematic Visualization of Volumetric Medical Data", "categories": ["cs.CV"], "comment": "Early accepted by MICCAI 2025. Project is available at:\n  https://med-air.github.io/ClipGS", "summary": "The visualization of volumetric medical data is crucial for enhancing\ndiagnostic accuracy and improving surgical planning and education. Cinematic\nrendering techniques significantly enrich this process by providing\nhigh-quality visualizations that convey intricate anatomical details, thereby\nfacilitating better understanding and decision-making in medical contexts.\nHowever, the high computing cost and low rendering speed limit the requirement\nof interactive visualization in practical applications. In this paper, we\nintroduce ClipGS, an innovative Gaussian splatting framework with the clipping\nplane supported, for interactive cinematic visualization of volumetric medical\ndata. To address the challenges posed by dynamic interactions, we propose a\nlearnable truncation scheme that automatically adjusts the visibility of\nGaussian primitives in response to the clipping plane. Besides, we also design\nan adaptive adjustment model to dynamically adjust the deformation of Gaussians\nand refine the rendering performance. We validate our method on five volumetric\nmedical data (including CT and anatomical slice data), and reach an average\n36.635 PSNR rendering quality with 156 FPS and 16.1 MB model size,\noutperforming state-of-the-art methods in rendering quality and efficiency.", "AI": {"tldr": "ClipGS\u6846\u67b6\u901a\u8fc7\u9ad8\u65af\u6837\u6761\u548c\u526a\u88c1\u5e73\u9762\u652f\u6301\uff0c\u5b9e\u73b0\u4e86\u533b\u5b66\u4f53\u79ef\u6570\u636e\u7684\u4ea4\u4e92\u5f0f\u7535\u5f71\u7ea7\u53ef\u89c6\u5316\uff0c\u63d0\u5347\u4e86\u6e32\u67d3\u8d28\u91cf\u548c\u6548\u7387\u3002", "motivation": "\u533b\u5b66\u4f53\u79ef\u6570\u636e\u7684\u53ef\u89c6\u5316\u5bf9\u8bca\u65ad\u548c\u624b\u672f\u89c4\u5212\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u8ba1\u7b97\u6210\u672c\u9ad8\u3001\u6e32\u67d3\u901f\u5ea6\u6162\uff0c\u96be\u4ee5\u6ee1\u8db3\u4ea4\u4e92\u9700\u6c42\u3002", "method": "\u63d0\u51faClipGS\u6846\u67b6\uff0c\u7ed3\u5408\u53ef\u5b66\u4e60\u7684\u622a\u65ad\u65b9\u6848\u548c\u81ea\u9002\u5e94\u8c03\u6574\u6a21\u578b\uff0c\u52a8\u6001\u4f18\u5316\u9ad8\u65af\u57fa\u5143\u7684\u53ef\u89c1\u6027\u548c\u53d8\u5f62\u3002", "result": "\u5728\u4e94\u79cd\u533b\u5b66\u6570\u636e\u4e0a\u6d4b\u8bd5\uff0c\u5e73\u5747PSNR\u4e3a36.635\uff0c\u5e27\u7387156 FPS\uff0c\u6a21\u578b\u5927\u5c0f16.1 MB\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "ClipGS\u5728\u6e32\u67d3\u8d28\u91cf\u548c\u6548\u7387\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u9002\u7528\u4e8e\u533b\u5b66\u6570\u636e\u7684\u4ea4\u4e92\u5f0f\u53ef\u89c6\u5316\u3002"}}
{"id": "2507.06899", "pdf": "https://arxiv.org/pdf/2507.06899", "abs": "https://arxiv.org/abs/2507.06899", "authors": ["Ziang Ye", "Yang Zhang", "Wentao Shi", "Xiaoyu You", "Fuli Feng", "Tat-Seng Chua"], "title": "VisualTrap: A Stealthy Backdoor Attack on GUI Agents via Visual Grounding Manipulation", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Graphical User Interface (GUI) agents powered by Large Vision-Language Models\n(LVLMs) have emerged as a revolutionary approach to automating human-machine\ninteractions, capable of autonomously operating personal devices (e.g., mobile\nphones) or applications within the device to perform complex real-world tasks\nin a human-like manner. However, their close integration with personal devices\nraises significant security concerns, with many threats, including backdoor\nattacks, remaining largely unexplored. This work reveals that the visual\ngrounding of GUI agent-mapping textual plans to GUI elements-can introduce\nvulnerabilities, enabling new types of backdoor attacks. With backdoor attack\ntargeting visual grounding, the agent's behavior can be compromised even when\ngiven correct task-solving plans. To validate this vulnerability, we propose\nVisualTrap, a method that can hijack the grounding by misleading the agent to\nlocate textual plans to trigger locations instead of the intended targets.\nVisualTrap uses the common method of injecting poisoned data for attacks, and\ndoes so during the pre-training of visual grounding to ensure practical\nfeasibility of attacking. Empirical results show that VisualTrap can\neffectively hijack visual grounding with as little as 5% poisoned data and\nhighly stealthy visual triggers (invisible to the human eye); and the attack\ncan be generalized to downstream tasks, even after clean fine-tuning. Moreover,\nthe injected trigger can remain effective across different GUI environments,\ne.g., being trained on mobile/web and generalizing to desktop environments.\nThese findings underscore the urgent need for further research on backdoor\nattack risks in GUI agents.", "AI": {"tldr": "GUI\u4ee3\u7406\uff08\u57fa\u4e8e\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff09\u5b58\u5728\u89c6\u89c9\u5b9a\u4f4d\u6f0f\u6d1e\uff0c\u6613\u53d7\u65b0\u578b\u540e\u95e8\u653b\u51fb\uff08\u5982VisualTrap\uff09\uff0c\u4ec5\u97005%\u6c61\u67d3\u6570\u636e\u5373\u53ef\u751f\u6548\uff0c\u4e14\u653b\u51fb\u5177\u6709\u8de8\u73af\u5883\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "GUI\u4ee3\u7406\u7684\u89c6\u89c9\u5b9a\u4f4d\u529f\u80fd\u53ef\u80fd\u5f15\u5165\u5b89\u5168\u6f0f\u6d1e\uff0c\u4f46\u76ee\u524d\u76f8\u5173\u5a01\u80c1\uff08\u5982\u540e\u95e8\u653b\u51fb\uff09\u7814\u7a76\u4e0d\u8db3\u3002", "method": "\u63d0\u51faVisualTrap\u65b9\u6cd5\uff0c\u901a\u8fc7\u6c61\u67d3\u9884\u8bad\u7ec3\u6570\u636e\u8bef\u5bfc\u4ee3\u7406\u7684\u89c6\u89c9\u5b9a\u4f4d\uff0c\u4f7f\u5176\u5c06\u6587\u672c\u8ba1\u5212\u6620\u5c04\u5230\u9519\u8bef\u4f4d\u7f6e\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u4ec5\u97005%\u6c61\u67d3\u6570\u636e\u548c\u9690\u853d\u89c6\u89c9\u89e6\u53d1\u5373\u53ef\u9ad8\u6548\u653b\u51fb\uff0c\u4e14\u653b\u51fb\u80fd\u8de8\u4efb\u52a1\u548c\u73af\u5883\u6cdb\u5316\u3002", "conclusion": "GUI\u4ee3\u7406\u7684\u540e\u95e8\u653b\u51fb\u98ce\u9669\u4e9f\u9700\u8fdb\u4e00\u6b65\u7814\u7a76\u3002"}}
{"id": "2507.06624", "pdf": "https://arxiv.org/pdf/2507.06624", "abs": "https://arxiv.org/abs/2507.06624", "authors": ["Dazhi Fu", "Jicong Fan"], "title": "UniOD: A Universal Model for Outlier Detection across Diverse Domains", "categories": ["cs.LG"], "comment": "20 pages, 4 figures", "summary": "Outlier detection (OD) seeks to distinguish inliers and outliers in\ncompletely unlabeled datasets and plays a vital role in science and\nengineering. Most existing OD methods require troublesome dataset-specific\nhyperparameter tuning and costly model training before they can be deployed to\nidentify outliers. In this work, we propose UniOD, a universal OD framework\nthat leverages labeled datasets to train a single model capable of detecting\noutliers of datasets from diverse domains. Specifically, UniOD converts each\ndataset into multiple graphs, produces consistent node features, and frames\noutlier detection as a node-classification task, and is able to generalize to\nunseen domains. As a result, UniOD avoids effort on model selection and\nhyperparameter tuning, reduces computational cost, and effectively utilizes the\nknowledge from historical datasets, which improves the convenience and accuracy\nin real applications. We evaluate UniOD on 15 benchmark OD datasets against 15\nstate-of-the-art baselines, demonstrating its effectiveness.", "AI": {"tldr": "UniOD\u662f\u4e00\u79cd\u901a\u7528\u7684\u5f02\u5e38\u68c0\u6d4b\u6846\u67b6\uff0c\u5229\u7528\u6807\u8bb0\u6570\u636e\u96c6\u8bad\u7ec3\u5355\u4e00\u6a21\u578b\uff0c\u53ef\u8de8\u9886\u57df\u68c0\u6d4b\u5f02\u5e38\uff0c\u907f\u514d\u4e86\u7e41\u7410\u7684\u8d85\u53c2\u6570\u8c03\u4f18\u548c\u6a21\u578b\u8bad\u7ec3\u3002", "motivation": "\u73b0\u6709\u5f02\u5e38\u68c0\u6d4b\u65b9\u6cd5\u9700\u8981\u9488\u5bf9\u4e0d\u540c\u6570\u636e\u96c6\u8fdb\u884c\u8d85\u53c2\u6570\u8c03\u4f18\u548c\u6a21\u578b\u8bad\u7ec3\uff0c\u8017\u65f6\u4e14\u4e0d\u901a\u7528\u3002UniOD\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\uff0c\u63d0\u9ad8\u6548\u7387\u548c\u51c6\u786e\u6027\u3002", "method": "UniOD\u5c06\u6570\u636e\u96c6\u8f6c\u5316\u4e3a\u56fe\u7ed3\u6784\uff0c\u751f\u6210\u4e00\u81f4\u7684\u8282\u70b9\u7279\u5f81\uff0c\u5e76\u5c06\u5f02\u5e38\u68c0\u6d4b\u4efb\u52a1\u8f6c\u5316\u4e3a\u8282\u70b9\u5206\u7c7b\u95ee\u9898\uff0c\u5b9e\u73b0\u8de8\u9886\u57df\u6cdb\u5316\u3002", "result": "\u572815\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u6d4b\u8bd5\uff0cUniOD\u8868\u73b0\u4f18\u4e8e15\u79cd\u73b0\u6709\u65b9\u6cd5\uff0c\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "conclusion": "UniOD\u7b80\u5316\u4e86\u5f02\u5e38\u68c0\u6d4b\u6d41\u7a0b\uff0c\u964d\u4f4e\u4e86\u8ba1\u7b97\u6210\u672c\uff0c\u63d0\u5347\u4e86\u5b9e\u9645\u5e94\u7528\u7684\u4fbf\u5229\u6027\u548c\u51c6\u786e\u6027\u3002"}}
{"id": "2507.06651", "pdf": "https://arxiv.org/pdf/2507.06651", "abs": "https://arxiv.org/abs/2507.06651", "authors": ["Juncheng Mu", "Chengwei Ren", "Weixiang Zhang", "Liang Pan", "Xiao-Ping Zhang", "Yue Gao"], "title": "Diff$^2$I2P: Differentiable Image-to-Point Cloud Registration with Diffusion Prior", "categories": ["cs.CV"], "comment": "ICCV 2025", "summary": "Learning cross-modal correspondences is essential for image-to-point cloud\n(I2P) registration. Existing methods achieve this mostly by utilizing metric\nlearning to enforce feature alignment across modalities, disregarding the\ninherent modality gap between image and point data. Consequently, this paradigm\nstruggles to ensure accurate cross-modal correspondences. To this end, inspired\nby the cross-modal generation success of recent large diffusion models, we\npropose Diff$^2$I2P, a fully Differentiable I2P registration framework,\nleveraging a novel and effective Diffusion prior for bridging the modality gap.\nSpecifically, we propose a Control-Side Score Distillation (CSD) technique to\ndistill knowledge from a depth-conditioned diffusion model to directly optimize\nthe predicted transformation. However, the gradients on the transformation fail\nto backpropagate onto the cross-modal features due to the non-differentiability\nof correspondence retrieval and PnP solver. To this end, we further propose a\nDeformable Correspondence Tuning (DCT) module to estimate the correspondences\nin a differentiable way, followed by the transformation estimation using a\ndifferentiable PnP solver. With these two designs, the Diffusion model serves\nas a strong prior to guide the cross-modal feature learning of image and point\ncloud for forming robust correspondences, which significantly improves the\nregistration. Extensive experimental results demonstrate that Diff$^2$I2P\nconsistently outperforms SoTA I2P registration methods, achieving over 7%\nimprovement in registration recall on the 7-Scenes benchmark.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faDiff$^2$I2P\u6846\u67b6\uff0c\u5229\u7528\u6269\u6563\u6a21\u578b\u4f5c\u4e3a\u5148\u9a8c\u77e5\u8bc6\uff0c\u901a\u8fc7\u53ef\u5fae\u5206\u65b9\u6cd5\u4f18\u5316\u8de8\u6a21\u6001\u7279\u5f81\u5bf9\u9f50\uff0c\u663e\u8457\u63d0\u5347\u56fe\u50cf\u5230\u70b9\u4e91\u7684\u914d\u51c6\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u901a\u8fc7\u5ea6\u91cf\u5b66\u4e60\u5b9e\u73b0\u8de8\u6a21\u6001\u7279\u5f81\u5bf9\u9f50\uff0c\u4f46\u5ffd\u89c6\u4e86\u56fe\u50cf\u4e0e\u70b9\u4e91\u4e4b\u95f4\u7684\u56fa\u6709\u6a21\u6001\u5dee\u5f02\uff0c\u5bfc\u81f4\u914d\u51c6\u4e0d\u51c6\u786e\u3002", "method": "\u63d0\u51faControl-Side Score Distillation (CSD)\u6280\u672f\u548cDeformable Correspondence Tuning (DCT)\u6a21\u5757\uff0c\u5206\u522b\u7528\u4e8e\u4f18\u5316\u53d8\u6362\u9884\u6d4b\u548c\u53ef\u5fae\u5206\u5730\u4f30\u8ba1\u5bf9\u5e94\u5173\u7cfb\u3002", "result": "\u57287-Scenes\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cDiff$^2$I2P\u6bd4\u73b0\u6709\u65b9\u6cd5\u63d0\u5347\u4e867%\u4ee5\u4e0a\u7684\u914d\u51c6\u53ec\u56de\u7387\u3002", "conclusion": "Diff$^2$I2P\u901a\u8fc7\u6269\u6563\u6a21\u578b\u5148\u9a8c\u548c\u53ef\u5fae\u5206\u8bbe\u8ba1\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8de8\u6a21\u6001\u914d\u51c6\u7684\u51c6\u786e\u6027\u548c\u9c81\u68d2\u6027\u3002"}}
{"id": "2507.06908", "pdf": "https://arxiv.org/pdf/2507.06908", "abs": "https://arxiv.org/abs/2507.06908", "authors": ["Ziyan Liu", "Chunxiao Fan", "Haoran Lou", "Yuexin Wu", "Kaiwei Deng"], "title": "MIND: A Multi-agent Framework for Zero-shot Harmful Meme Detection", "categories": ["cs.CL", "cs.AI"], "comment": "ACL 2025", "summary": "The rapid expansion of memes on social media has highlighted the urgent need\nfor effective approaches to detect harmful content. However, traditional\ndata-driven approaches struggle to detect new memes due to their evolving\nnature and the lack of up-to-date annotated data. To address this issue, we\npropose MIND, a multi-agent framework for zero-shot harmful meme detection that\ndoes not rely on annotated data. MIND implements three key strategies: 1) We\nretrieve similar memes from an unannotated reference set to provide contextual\ninformation. 2) We propose a bi-directional insight derivation mechanism to\nextract a comprehensive understanding of similar memes. 3) We then employ a\nmulti-agent debate mechanism to ensure robust decision-making through reasoned\narbitration. Extensive experiments on three meme datasets demonstrate that our\nproposed framework not only outperforms existing zero-shot approaches but also\nshows strong generalization across different model architectures and parameter\nscales, providing a scalable solution for harmful meme detection. The code is\navailable at https://github.com/destroy-lonely/MIND.", "AI": {"tldr": "MIND\u662f\u4e00\u4e2a\u591a\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u7528\u4e8e\u96f6\u6837\u672c\u6709\u5bb3\u6a21\u56e0\u68c0\u6d4b\uff0c\u65e0\u9700\u6807\u6ce8\u6570\u636e\uff0c\u901a\u8fc7\u68c0\u7d22\u76f8\u4f3c\u6a21\u56e0\u3001\u53cc\u5411\u6d1e\u5bdf\u673a\u5236\u548c\u591a\u667a\u80fd\u4f53\u8fa9\u8bba\u5b9e\u73b0\u9ad8\u6548\u68c0\u6d4b\u3002", "motivation": "\u4f20\u7edf\u6570\u636e\u9a71\u52a8\u65b9\u6cd5\u96be\u4ee5\u68c0\u6d4b\u65b0\u6a21\u56e0\uff0c\u56e0\u5176\u52a8\u6001\u53d8\u5316\u4e14\u7f3a\u4e4f\u6700\u65b0\u6807\u6ce8\u6570\u636e\uff0cMIND\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "MIND\u91c7\u7528\u4e09\u79cd\u7b56\u7565\uff1a\u68c0\u7d22\u76f8\u4f3c\u6a21\u56e0\u63d0\u4f9b\u4e0a\u4e0b\u6587\u3001\u53cc\u5411\u6d1e\u5bdf\u673a\u5236\u63d0\u53d6\u5168\u9762\u7406\u89e3\u3001\u591a\u667a\u80fd\u4f53\u8fa9\u8bba\u786e\u4fdd\u51b3\u7b56\u7a33\u5065\u3002", "result": "\u5728\u4e09\u4e2a\u6a21\u56e0\u6570\u636e\u96c6\u4e0a\uff0cMIND\u4f18\u4e8e\u73b0\u6709\u96f6\u6837\u672c\u65b9\u6cd5\uff0c\u4e14\u5728\u4e0d\u540c\u6a21\u578b\u67b6\u6784\u548c\u53c2\u6570\u89c4\u6a21\u4e0a\u8868\u73b0\u826f\u597d\u3002", "conclusion": "MIND\u4e3a\u6709\u5bb3\u6a21\u56e0\u68c0\u6d4b\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u4ee3\u7801\u5df2\u5f00\u6e90\u3002"}}
{"id": "2507.06628", "pdf": "https://arxiv.org/pdf/2507.06628", "abs": "https://arxiv.org/abs/2507.06628", "authors": ["Jinmin He", "Kai Li", "Yifan Zang", "Haobo Fu", "Qiang Fu", "Junliang Xing", "Jian Cheng"], "title": "Goal-Oriented Skill Abstraction for Offline Multi-Task Reinforcement Learning", "categories": ["cs.LG", "cs.AI"], "comment": "ICML2025", "summary": "Offline multi-task reinforcement learning aims to learn a unified policy\ncapable of solving multiple tasks using only pre-collected task-mixed datasets,\nwithout requiring any online interaction with the environment. However, it\nfaces significant challenges in effectively sharing knowledge across tasks.\nInspired by the efficient knowledge abstraction observed in human learning, we\npropose Goal-Oriented Skill Abstraction (GO-Skill), a novel approach designed\nto extract and utilize reusable skills to enhance knowledge transfer and task\nperformance. Our approach uncovers reusable skills through a goal-oriented\nskill extraction process and leverages vector quantization to construct a\ndiscrete skill library. To mitigate class imbalances between broadly applicable\nand task-specific skills, we introduce a skill enhancement phase to refine the\nextracted skills. Furthermore, we integrate these skills using hierarchical\npolicy learning, enabling the construction of a high-level policy that\ndynamically orchestrates discrete skills to accomplish specific tasks.\nExtensive experiments on diverse robotic manipulation tasks within the\nMetaWorld benchmark demonstrate the effectiveness and versatility of GO-Skill.", "AI": {"tldr": "GO-Skill\u662f\u4e00\u79cd\u79bb\u7ebf\u591a\u4efb\u52a1\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff0c\u901a\u8fc7\u76ee\u6807\u5bfc\u5411\u7684\u6280\u80fd\u63d0\u53d6\u548c\u5206\u5c42\u7b56\u7565\u5b66\u4e60\uff0c\u63d0\u5347\u4efb\u52a1\u95f4\u77e5\u8bc6\u5171\u4eab\u548c\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u79bb\u7ebf\u591a\u4efb\u52a1\u5f3a\u5316\u5b66\u4e60\u4e2d\u77e5\u8bc6\u5171\u4eab\u7684\u6311\u6218\uff0c\u53d7\u4eba\u7c7b\u5b66\u4e60\u7684\u9ad8\u6548\u77e5\u8bc6\u62bd\u8c61\u542f\u53d1\u3002", "method": "\u63d0\u51fa\u76ee\u6807\u5bfc\u5411\u6280\u80fd\u63d0\u53d6\uff08GO-Skill\uff09\uff0c\u6784\u5efa\u79bb\u6563\u6280\u80fd\u5e93\uff0c\u5e76\u901a\u8fc7\u6280\u80fd\u589e\u5f3a\u548c\u5206\u5c42\u7b56\u7565\u5b66\u4e60\u4f18\u5316\u6280\u80fd\u4f7f\u7528\u3002", "result": "\u5728MetaWorld\u673a\u5668\u4eba\u64cd\u4f5c\u4efb\u52a1\u4e2d\u9a8c\u8bc1\u4e86GO-Skill\u7684\u6709\u6548\u6027\u548c\u901a\u7528\u6027\u3002", "conclusion": "GO-Skill\u901a\u8fc7\u6280\u80fd\u62bd\u8c61\u548c\u52a8\u6001\u7f16\u6392\uff0c\u663e\u8457\u63d0\u5347\u4e86\u79bb\u7ebf\u591a\u4efb\u52a1\u5f3a\u5316\u5b66\u4e60\u7684\u6027\u80fd\u3002"}}
{"id": "2507.06654", "pdf": "https://arxiv.org/pdf/2507.06654", "abs": "https://arxiv.org/abs/2507.06654", "authors": ["Naoya Sogi", "Takashi Shibata", "Makoto Terao", "Masanori Suganuma", "Takayuki Okatani"], "title": "MS-DPPs: Multi-Source Determinantal Point Processes for Contextual Diversity Refinement of Composite Attributes in Text to Image Retrieval", "categories": ["cs.CV", "cs.AI", "cs.IR"], "comment": "IJCAI 2025. Code: https://github.com/NEC-N-SOGI/msdpp", "summary": "Result diversification (RD) is a crucial technique in Text-to-Image Retrieval\nfor enhancing the efficiency of a practical application. Conventional methods\nfocus solely on increasing the diversity metric of image appearances. However,\nthe diversity metric and its desired value vary depending on the application,\nwhich limits the applications of RD. This paper proposes a novel task called\nCDR-CA (Contextual Diversity Refinement of Composite Attributes). CDR-CA aims\nto refine the diversities of multiple attributes, according to the\napplication's context. To address this task, we propose Multi-Source DPPs, a\nsimple yet strong baseline that extends the Determinantal Point Process (DPP)\nto multi-sources. We model MS-DPP as a single DPP model with a unified\nsimilarity matrix based on a manifold representation. We also introduce Tangent\nNormalization to reflect contexts. Extensive experiments demonstrate the\neffectiveness of the proposed method. Our code is publicly available at\nhttps://github.com/NEC-N-SOGI/msdpp.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aCDR-CA\u7684\u65b0\u4efb\u52a1\uff0c\u901a\u8fc7\u591a\u6e90DPP\u65b9\u6cd5\u4f18\u5316\u591a\u5c5e\u6027\u591a\u6837\u6027\uff0c\u4ee5\u9002\u5e94\u4e0d\u540c\u5e94\u7528\u573a\u666f\u3002", "motivation": "\u4f20\u7edf\u65b9\u6cd5\u4ec5\u5173\u6ce8\u56fe\u50cf\u5916\u89c2\u591a\u6837\u6027\uff0c\u5ffd\u7565\u4e86\u5e94\u7528\u573a\u666f\u7684\u591a\u6837\u6027\u9700\u6c42\uff0c\u9650\u5236\u4e86\u7ed3\u679c\u591a\u6837\u5316\uff08RD\uff09\u7684\u5e94\u7528\u8303\u56f4\u3002", "method": "\u63d0\u51faMulti-Source DPPs\uff0c\u6269\u5c55DPP\u81f3\u591a\u6e90\uff0c\u57fa\u4e8e\u6d41\u5f62\u8868\u793a\u7684\u7edf\u4e00\u76f8\u4f3c\u6027\u77e9\u9635\u5efa\u6a21\uff0c\u5e76\u5f15\u5165Tangent Normalization\u4ee5\u53cd\u6620\u4e0a\u4e0b\u6587\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\u8be5\u65b9\u6cd5\u6709\u6548\u3002", "conclusion": "CDR-CA\u4efb\u52a1\u53caMulti-Source DPPs\u65b9\u6cd5\u4e3a\u7ed3\u679c\u591a\u6837\u5316\u63d0\u4f9b\u4e86\u66f4\u7075\u6d3b\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.06399", "pdf": "https://arxiv.org/pdf/2507.06399", "abs": "https://arxiv.org/abs/2507.06399", "authors": ["Doyeong Lim", "Yang Liu", "Zavier Ndum Ndum", "Christian Young", "Yassin Hassan"], "title": "An AI-Driven Thermal-Fluid Testbed for Advanced Small Modular Reactors: Integration of Digital Twin and Large Language Models", "categories": ["eess.SY", "cs.AI", "cs.SY"], "comment": null, "summary": "This paper presents a multipurpose artificial intelligence (AI)-driven\nthermal-fluid testbed designed to advance Small Modular Reactor technologies by\nseamlessly integrating physical experimentation with advanced computational\nintelligence. The platform uniquely combines a versatile three-loop\nthermal-fluid facility with a high-fidelity digital twin and sophisticated AI\nframeworks for real-time prediction, control, and operational assistance.\nMethodologically, the testbed's digital twin, built upon the System Analysis\nModule code, is coupled with a Gated Recurrent Unit (GRU) neural network. This\nmachine learning model, trained on experimental data, enables\nfaster-than-real-time simulation, providing predictive insights into the\nsystem's dynamic behavior. The practical application of this AI integration is\nshowcased through case studies. An AI-driven control framework where the GRU\nmodel accurately forecasts future system states and the corresponding control\nactions required to meet operational demands. Furthermore, an intelligent\nassistant, powered by a large language model, translates complex sensor data\nand simulation outputs into natural language, offering operators actionable\nanalysis and safety recommendations. Comprehensive validation against\nexperimental transients confirms the platform's high fidelity, with the GRU\nmodel achieving a temperature prediction root mean square error of 1.42 K. This\nwork establishes an integrated research environment at the intersection of AI\nand thermal-fluid science, showcasing how AI-driven methodologies in modeling,\ncontrol, and operator support can accelerate the innovation and deployment of\nnext-generation nuclear systems.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86\u4e00\u79cd\u591a\u7528\u9014AI\u9a71\u52a8\u7684\u70ed\u6d41\u4f53\u6d4b\u8bd5\u5e73\u53f0\uff0c\u901a\u8fc7\u7ed3\u5408\u7269\u7406\u5b9e\u9a8c\u4e0e\u8ba1\u7b97\u667a\u80fd\uff0c\u63a8\u52a8\u5c0f\u578b\u6a21\u5757\u5316\u53cd\u5e94\u5806\u6280\u672f\u7684\u53d1\u5c55\u3002", "motivation": "\u65e8\u5728\u901a\u8fc7AI\u6280\u672f\u63d0\u5347\u70ed\u6d41\u4f53\u7cfb\u7edf\u7684\u5efa\u6a21\u3001\u63a7\u5236\u548c\u64cd\u4f5c\u652f\u6301\uff0c\u52a0\u901f\u4e0b\u4e00\u4ee3\u6838\u7cfb\u7edf\u7684\u521b\u65b0\u4e0e\u90e8\u7f72\u3002", "method": "\u6d4b\u8bd5\u5e73\u53f0\u7ed3\u5408\u4e86\u591a\u56de\u8def\u70ed\u6d41\u4f53\u8bbe\u65bd\u3001\u9ad8\u4fdd\u771f\u6570\u5b57\u5b6a\u751f\u4f53\u548cGRU\u795e\u7ecf\u7f51\u7edc\uff0c\u5b9e\u73b0\u5b9e\u65f6\u9884\u6d4b\u4e0e\u63a7\u5236\u3002", "result": "GRU\u6a21\u578b\u7684\u6e29\u5ea6\u9884\u6d4b\u5747\u65b9\u6839\u8bef\u5dee\u4e3a1.42 K\uff0c\u9a8c\u8bc1\u4e86\u5e73\u53f0\u7684\u9ad8\u4fdd\u771f\u6027\u3002", "conclusion": "\u8be5\u5e73\u53f0\u4e3aAI\u4e0e\u70ed\u6d41\u4f53\u79d1\u5b66\u7684\u4ea4\u53c9\u7814\u7a76\u63d0\u4f9b\u4e86\u96c6\u6210\u73af\u5883\uff0c\u5c55\u793a\u4e86AI\u5728\u6838\u7cfb\u7edf\u521b\u65b0\u4e2d\u7684\u6f5c\u529b\u3002"}}
{"id": "2507.06909", "pdf": "https://arxiv.org/pdf/2507.06909", "abs": "https://arxiv.org/abs/2507.06909", "authors": ["Xiao Wang", "Jiahuan Pei", "Diancheng Shui", "Zhiguang Han", "Xin Sun", "Dawei Zhu", "Xiaoyu Shen"], "title": "MultiJustice: A Chinese Dataset for Multi-Party, Multi-Charge Legal Prediction", "categories": ["cs.CL", "cs.AI"], "comment": "Accepted by NLPCC 2025", "summary": "Legal judgment prediction offers a compelling method to aid legal\npractitioners and researchers. However, the research question remains\nrelatively under-explored: Should multiple defendants and charges be treated\nseparately in LJP? To address this, we introduce a new dataset namely\nmulti-person multi-charge prediction (MPMCP), and seek the answer by evaluating\nthe performance of several prevailing legal large language models (LLMs) on\nfour practical legal judgment scenarios: (S1) single defendant with a single\ncharge, (S2) single defendant with multiple charges, (S3) multiple defendants\nwith a single charge, and (S4) multiple defendants with multiple charges. We\nevaluate the dataset across two LJP tasks, i.e., charge prediction and penalty\nterm prediction. We have conducted extensive experiments and found that the\nscenario involving multiple defendants and multiple charges (S4) poses the\ngreatest challenges, followed by S2, S3, and S1. The impact varies\nsignificantly depending on the model. For example, in S4 compared to S1,\nInternLM2 achieves approximately 4.5% lower F1-score and 2.8% higher LogD,\nwhile Lawformer demonstrates around 19.7% lower F1-score and 19.0% higher LogD.\nOur dataset and code are available at\nhttps://github.com/lololo-xiao/MultiJustice-MPMCP.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u6cd5\u5f8b\u5224\u51b3\u9884\u6d4b\u4e2d\u591a\u88ab\u544a\u548c\u591a\u7f6a\u540d\u662f\u5426\u5e94\u5206\u5f00\u5904\u7406\u7684\u95ee\u9898\uff0c\u63d0\u51fa\u4e86MPMCP\u6570\u636e\u96c6\uff0c\u5e76\u8bc4\u4f30\u4e86\u4e0d\u540cLLM\u5728\u56db\u79cd\u6cd5\u5f8b\u573a\u666f\u4e0b\u7684\u8868\u73b0\u3002", "motivation": "\u63a2\u8ba8\u591a\u88ab\u544a\u548c\u591a\u7f6a\u540d\u5728\u6cd5\u5f8b\u5224\u51b3\u9884\u6d4b\u4e2d\u7684\u5f71\u54cd\uff0c\u586b\u8865\u7814\u7a76\u7a7a\u767d\u3002", "method": "\u5f15\u5165MPMCP\u6570\u636e\u96c6\uff0c\u8bc4\u4f30\u56db\u79cd\u6cd5\u5f8b\u573a\u666f\u4e0bLLM\u7684\u8868\u73b0\uff0c\u5305\u62ec\u5355\u88ab\u544a\u5355\u7f6a\u540d\u3001\u5355\u88ab\u544a\u591a\u7f6a\u540d\u3001\u591a\u88ab\u544a\u5355\u7f6a\u540d\u548c\u591a\u88ab\u544a\u591a\u7f6a\u540d\u3002", "result": "\u591a\u88ab\u544a\u591a\u7f6a\u540d\u573a\u666f\uff08S4\uff09\u6700\u5177\u6311\u6218\u6027\uff0c\u4e0d\u540c\u6a21\u578b\u8868\u73b0\u5dee\u5f02\u663e\u8457\uff0c\u5982InternLM2\u548cLawformer\u5728S4\u4e2d\u7684F1-score\u548cLogD\u53d8\u5316\u660e\u663e\u3002", "conclusion": "\u591a\u88ab\u544a\u548c\u591a\u7f6a\u540d\u5bf9\u6cd5\u5f8b\u5224\u51b3\u9884\u6d4b\u6709\u663e\u8457\u5f71\u54cd\uff0c\u9700\u9488\u5bf9\u6027\u4f18\u5316\u6a21\u578b\u3002"}}
{"id": "2507.06631", "pdf": "https://arxiv.org/pdf/2507.06631", "abs": "https://arxiv.org/abs/2507.06631", "authors": ["Enda D. V. Bigarella"], "title": "Prevention of Overfitting on Mesh-Structured Data Regressions with a Modified Laplace Operator", "categories": ["cs.LG"], "comment": null, "summary": "This document reports on a method for detecting and preventing overfitting on\ndata regressions, herein applied to mesh-like data structures. The mesh\nstructure allows for the straightforward computation of the Laplace-operator\nsecond-order derivatives in a finite-difference fashion for noiseless data.\nDerivatives of the training data are computed on the original training mesh to\nserve as a true label of the entropy of the training data. Derivatives of the\ntrained data are computed on a staggered mesh to identify oscillations in the\ninterior of the original training mesh cells. The loss of the Laplace-operator\nderivatives is used for hyperparameter optimisation, achieving a reduction of\nunwanted oscillation through the minimisation of the entropy of the trained\nmodel. In this setup, testing does not require the splitting of points from the\ntraining data, and training is thus directly performed on all available\ntraining points. The Laplace operator applied to the trained data on a\nstaggered mesh serves as a surrogate testing metric based on diffusion\nproperties.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u7f51\u683c\u6570\u636e\u7684\u8fc7\u62df\u5408\u68c0\u6d4b\u4e0e\u9884\u9632\u65b9\u6cd5\uff0c\u901a\u8fc7\u62c9\u666e\u62c9\u65af\u7b97\u5b50\u4f18\u5316\u8d85\u53c2\u6570\u4ee5\u51cf\u5c11\u6a21\u578b\u632f\u8361\u3002", "motivation": "\u89e3\u51b3\u56de\u5f52\u4efb\u52a1\u4e2d\u6570\u636e\u8fc7\u62df\u5408\u95ee\u9898\uff0c\u7279\u522b\u662f\u5728\u7f51\u683c\u6570\u636e\u7ed3\u6784\u4e2d\uff0c\u901a\u8fc7\u5229\u7528\u62c9\u666e\u62c9\u65af\u7b97\u5b50\u7684\u4e8c\u9636\u5bfc\u6570\u7279\u6027\u3002", "method": "\u5728\u539f\u59cb\u8bad\u7ec3\u7f51\u683c\u4e0a\u8ba1\u7b97\u6570\u636e\u5bfc\u6570\u4f5c\u4e3a\u771f\u5b9e\u6807\u7b7e\uff0c\u5728\u4ea4\u9519\u7f51\u683c\u4e0a\u8ba1\u7b97\u8bad\u7ec3\u6570\u636e\u5bfc\u6570\u4ee5\u68c0\u6d4b\u632f\u8361\uff0c\u5229\u7528\u62c9\u666e\u62c9\u65af\u7b97\u5b50\u635f\u5931\u4f18\u5316\u8d85\u53c2\u6570\u3002", "result": "\u901a\u8fc7\u6700\u5c0f\u5316\u6a21\u578b\u71b5\uff0c\u51cf\u5c11\u4e86\u4e0d\u5fc5\u8981\u7684\u632f\u8361\uff0c\u4e14\u65e0\u9700\u4ece\u8bad\u7ec3\u6570\u636e\u4e2d\u5206\u79bb\u6d4b\u8bd5\u70b9\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6709\u6548\u51cf\u5c11\u4e86\u8fc7\u62df\u5408\uff0c\u5e76\u901a\u8fc7\u62c9\u666e\u62c9\u65af\u7b97\u5b50\u7684\u6269\u6563\u7279\u6027\u63d0\u4f9b\u4e86\u4e00\u79cd\u66ff\u4ee3\u6d4b\u8bd5\u6307\u6807\u3002"}}
{"id": "2507.06656", "pdf": "https://arxiv.org/pdf/2507.06656", "abs": "https://arxiv.org/abs/2507.06656", "authors": ["Hongjie Wu", "Mingqin Zhang", "Linchao He", "Ji-Zhe Zhou", "Jiancheng Lv"], "title": "Enhancing Diffusion Model Stability for Image Restoration via Gradient Management", "categories": ["cs.CV", "cs.LG"], "comment": "Accepted to ACM Multimedia 2025. Preprint version", "summary": "Diffusion models have shown remarkable promise for image restoration by\nleveraging powerful priors. Prominent methods typically frame the restoration\nproblem within a Bayesian inference framework, which iteratively combines a\ndenoising step with a likelihood guidance step. However, the interactions\nbetween these two components in the generation process remain underexplored. In\nthis paper, we analyze the underlying gradient dynamics of these components and\nidentify significant instabilities. Specifically, we demonstrate conflicts\nbetween the prior and likelihood gradient directions, alongside temporal\nfluctuations in the likelihood gradient itself. We show that these\ninstabilities disrupt the generative process and compromise restoration\nperformance. To address these issues, we propose Stabilized Progressive\nGradient Diffusion (SPGD), a novel gradient management technique. SPGD\nintegrates two synergistic components: (1) a progressive likelihood warm-up\nstrategy to mitigate gradient conflicts; and (2) adaptive directional momentum\n(ADM) smoothing to reduce fluctuations in the likelihood gradient. Extensive\nexperiments across diverse restoration tasks demonstrate that SPGD\nsignificantly enhances generation stability, leading to state-of-the-art\nperformance in quantitative metrics and visually superior results. Code is\navailable at \\href{https://github.com/74587887/SPGD}{here}.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aSPGD\u7684\u65b0\u68af\u5ea6\u7ba1\u7406\u6280\u672f\uff0c\u7528\u4e8e\u89e3\u51b3\u6269\u6563\u6a21\u578b\u4e2d\u5148\u9a8c\u548c\u4f3c\u7136\u68af\u5ea6\u65b9\u5411\u51b2\u7a81\u53ca\u68af\u5ea6\u6ce2\u52a8\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u56fe\u50cf\u6062\u590d\u6027\u80fd\u3002", "motivation": "\u6269\u6563\u6a21\u578b\u5728\u56fe\u50cf\u6062\u590d\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5148\u9a8c\u548c\u4f3c\u7136\u68af\u5ea6\u65b9\u5411\u7684\u51b2\u7a81\u53ca\u68af\u5ea6\u6ce2\u52a8\u95ee\u9898\u672a\u88ab\u5145\u5206\u7814\u7a76\uff0c\u5f71\u54cd\u4e86\u751f\u6210\u8fc7\u7a0b\u7684\u7a33\u5b9a\u6027\u3002", "method": "\u63d0\u51fa\u4e86SPGD\u6280\u672f\uff0c\u5305\u62ec\u6e10\u8fdb\u5f0f\u4f3c\u7136\u9884\u70ed\u7b56\u7565\u548c\u81ea\u9002\u5e94\u65b9\u5411\u52a8\u91cf\u5e73\u6ed1\uff0c\u4ee5\u7f13\u89e3\u68af\u5ea6\u51b2\u7a81\u548c\u51cf\u5c11\u6ce2\u52a8\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cSPGD\u663e\u8457\u63d0\u5347\u4e86\u751f\u6210\u7a33\u5b9a\u6027\uff0c\u5728\u591a\u4e2a\u6062\u590d\u4efb\u52a1\u4e2d\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u5b9a\u91cf\u548c\u89c6\u89c9\u7ed3\u679c\u3002", "conclusion": "SPGD\u901a\u8fc7\u68af\u5ea6\u7ba1\u7406\u6709\u6548\u89e3\u51b3\u4e86\u6269\u6563\u6a21\u578b\u4e2d\u7684\u4e0d\u7a33\u5b9a\u95ee\u9898\uff0c\u4e3a\u56fe\u50cf\u6062\u590d\u63d0\u4f9b\u4e86\u66f4\u4f18\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.06910", "pdf": "https://arxiv.org/pdf/2507.06910", "abs": "https://arxiv.org/abs/2507.06910", "authors": ["Fareya Ikram", "Alexander Scarlatos", "Andrew Lan"], "title": "Exploring LLMs for Predicting Tutor Strategy and Student Outcomes in Dialogues", "categories": ["cs.CL", "cs.CY"], "comment": "Published in BEA 2025: 20th Workshop on Innovative Use of NLP for\n  Building Educational Applications", "summary": "Tutoring dialogues have gained significant attention in recent years, given\nthe prominence of online learning and the emerging tutoring abilities of\nartificial intelligence (AI) agents powered by large language models (LLMs).\nRecent studies have shown that the strategies used by tutors can have\nsignificant effects on student outcomes, necessitating methods to predict how\ntutors will behave and how their actions impact students. However, few works\nhave studied predicting tutor strategy in dialogues. Therefore, in this work we\ninvestigate the ability of modern LLMs, particularly Llama 3 and GPT-4o, to\npredict both future tutor moves and student outcomes in dialogues, using two\nmath tutoring dialogue datasets. We find that even state-of-the-art LLMs\nstruggle to predict future tutor strategy while tutor strategy is highly\nindicative of student outcomes, outlining a need for more powerful methods to\napproach this task.", "AI": {"tldr": "\u7814\u7a76\u4e86\u73b0\u4ee3\u5927\u8bed\u8a00\u6a21\u578b\uff08\u5982Llama 3\u548cGPT-4o\uff09\u5728\u9884\u6d4b\u8f85\u5bfc\u5bf9\u8bdd\u4e2d\u5bfc\u5e08\u7b56\u7565\u548c\u5b66\u751f\u8868\u73b0\u7684\u80fd\u529b\uff0c\u53d1\u73b0\u5373\u4f7f\u6700\u5148\u8fdb\u7684\u6a21\u578b\u4e5f\u96be\u4ee5\u51c6\u786e\u9884\u6d4b\u5bfc\u5e08\u7b56\u7565\uff0c\u4f46\u5bfc\u5e08\u7b56\u7565\u5bf9\u5b66\u751f\u8868\u73b0\u6709\u663e\u8457\u5f71\u54cd\u3002", "motivation": "\u5728\u7ebf\u5b66\u4e60\u548cAI\u8f85\u5bfc\u7684\u5174\u8d77\u4f7f\u5f97\u5bfc\u5e08\u7b56\u7565\u5bf9\u5b66\u751f\u8868\u73b0\u7684\u5f71\u54cd\u5907\u53d7\u5173\u6ce8\uff0c\u4f46\u76ee\u524d\u7f3a\u4e4f\u9884\u6d4b\u5bfc\u5e08\u7b56\u7565\u7684\u7814\u7a76\u3002", "method": "\u4f7f\u7528\u4e24\u4e2a\u6570\u5b66\u8f85\u5bfc\u5bf9\u8bdd\u6570\u636e\u96c6\uff0c\u8bc4\u4f30Llama 3\u548cGPT-4o\u5728\u9884\u6d4b\u5bfc\u5e08\u7b56\u7565\u548c\u5b66\u751f\u8868\u73b0\u65b9\u9762\u7684\u80fd\u529b\u3002", "result": "\u6700\u5148\u8fdb\u7684\u5927\u8bed\u8a00\u6a21\u578b\u5728\u9884\u6d4b\u5bfc\u5e08\u7b56\u7565\u65b9\u9762\u8868\u73b0\u4e0d\u4f73\uff0c\u4f46\u5bfc\u5e08\u7b56\u7565\u5bf9\u5b66\u751f\u8868\u73b0\u6709\u663e\u8457\u5f71\u54cd\u3002", "conclusion": "\u9700\u8981\u66f4\u5f3a\u5927\u7684\u65b9\u6cd5\u6765\u9884\u6d4b\u5bfc\u5e08\u7b56\u7565\uff0c\u4ee5\u4f18\u5316\u8f85\u5bfc\u6548\u679c\u3002"}}
{"id": "2507.06650", "pdf": "https://arxiv.org/pdf/2507.06650", "abs": "https://arxiv.org/abs/2507.06650", "authors": ["Hui Meng", "Keping Yang", "Xuyu Peng", "Bo Zheng"], "title": "Deep Disentangled Representation Network for Treatment Effect Estimation", "categories": ["cs.LG", "cs.AI"], "comment": "Under Review", "summary": "Estimating individual-level treatment effect from observational data is a\nfundamental problem in causal inference and has attracted increasing attention\nin the fields of education, healthcare, and public policy.In this work, we\nconcentrate on the study of disentangled representation methods that have shown\npromising outcomes by decomposing observed covariates into instrumental,\nconfounding, and adjustment factors. However, most of the previous work has\nprimarily revolved around generative models or hard decomposition methods for\ncovariates, which often struggle to guarantee the attainment of precisely\ndisentangled factors. In order to effectively model different causal\nrelationships, we propose a novel treatment effect estimation algorithm that\nincorporates a mixture of experts with multi-head attention and a linear\northogonal regularizer to softly decompose the pre-treatment variables, and\nsimultaneously eliminates selection bias via importance sampling re-weighting\ntechniques. We conduct extensive experiments on both public semi-synthetic and\nreal-world production datasets. The experimental results clearly demonstrate\nthat our algorithm outperforms the state-of-the-art methods focused on\nindividual treatment effects.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u4e2a\u4f53\u6cbb\u7597\u6548\u679c\u4f30\u8ba1\u7b97\u6cd5\uff0c\u7ed3\u5408\u591a\u5934\u6ce8\u610f\u529b\u673a\u5236\u548c\u7ebf\u6027\u6b63\u4ea4\u6b63\u5219\u5316\u5668\uff0c\u901a\u8fc7\u8f6f\u5206\u89e3\u9884\u5904\u7406\u53d8\u91cf\u5e76\u6d88\u9664\u9009\u62e9\u504f\u5dee\uff0c\u5b9e\u9a8c\u8bc1\u660e\u5176\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u4ece\u89c2\u6d4b\u6570\u636e\u4e2d\u4f30\u8ba1\u4e2a\u4f53\u6cbb\u7597\u6548\u679c\u662f\u56e0\u679c\u63a8\u65ad\u7684\u6838\u5fc3\u95ee\u9898\uff0c\u73b0\u6709\u65b9\u6cd5\u5728\u7cbe\u786e\u5206\u89e3\u53d8\u91cf\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\u3002", "method": "\u91c7\u7528\u591a\u5934\u6ce8\u610f\u529b\u673a\u5236\u7684\u4e13\u5bb6\u6df7\u5408\u6a21\u578b\u548c\u7ebf\u6027\u6b63\u4ea4\u6b63\u5219\u5316\u5668\uff0c\u8f6f\u5206\u89e3\u9884\u5904\u7406\u53d8\u91cf\uff0c\u5e76\u7ed3\u5408\u91cd\u8981\u6027\u91c7\u6837\u91cd\u52a0\u6743\u6280\u672f\u6d88\u9664\u9009\u62e9\u504f\u5dee\u3002", "result": "\u5728\u516c\u5f00\u534a\u5408\u6210\u548c\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u7b97\u6cd5\u5728\u4e2a\u4f53\u6cbb\u7597\u6548\u679c\u4f30\u8ba1\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "\u65b0\u7b97\u6cd5\u901a\u8fc7\u8f6f\u5206\u89e3\u548c\u504f\u5dee\u6d88\u9664\uff0c\u663e\u8457\u63d0\u5347\u4e86\u4e2a\u4f53\u6cbb\u7597\u6548\u679c\u4f30\u8ba1\u7684\u51c6\u786e\u6027\u3002"}}
{"id": "2507.06920", "pdf": "https://arxiv.org/pdf/2507.06920", "abs": "https://arxiv.org/abs/2507.06920", "authors": ["Zihan Ma", "Taolin Zhang", "Maosong Cao", "Wenwei Zhang", "Minnan Luo", "Songyang Zhang", "Kai Chen"], "title": "Rethinking Verification for LLM Code Generation: From Generation to Testing", "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) have recently achieved notable success in\ncode-generation benchmarks such as HumanEval and LiveCodeBench. However, a\ndetailed examination reveals that these evaluation suites often comprise only a\nlimited number of homogeneous test cases, resulting in subtle faults going\nundetected. This not only artificially inflates measured performance but also\ncompromises accurate reward estimation in reinforcement learning frameworks\nutilizing verifiable rewards (RLVR). To address these critical shortcomings, we\nsystematically investigate the test-case generation (TCG) task by proposing\nmulti-dimensional metrics designed to rigorously quantify test-suite\nthoroughness. Furthermore, we introduce a human-LLM collaborative method\n(SAGA), leveraging human programming expertise with LLM reasoning capability,\naimed at significantly enhancing both the coverage and the quality of generated\ntest cases. In addition, we develop a TCGBench to facilitate the study of the\nTCG task. Experiments show that SAGA achieves a detection rate of 90.62% and a\nverifier accuracy of 32.58% on TCGBench. The Verifier Accuracy (Verifier Acc)\nof the code generation evaluation benchmark synthesized by SAGA is 10.78%\nhigher than that of LiveCodeBench-v6. These results demonstrate the\neffectiveness of our proposed method. We hope this work contributes to building\na scalable foundation for reliable LLM code evaluation, further advancing RLVR\nin code generation, and paving the way for automated adversarial test synthesis\nand adaptive benchmark integration.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aSAGA\u7684\u4eba\u673a\u534f\u4f5c\u65b9\u6cd5\uff0c\u901a\u8fc7\u591a\u7ef4\u5ea6\u6307\u6807\u548cTCGBench\u63d0\u5347\u6d4b\u8bd5\u7528\u4f8b\u7684\u8986\u76d6\u7387\u548c\u8d28\u91cf\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u4ee3\u7801\u751f\u6210\u8bc4\u4f30\u7684\u51c6\u786e\u6027\u3002", "motivation": "\u73b0\u6709\u4ee3\u7801\u751f\u6210\u8bc4\u4f30\u5957\u4ef6\uff08\u5982HumanEval\u548cLiveCodeBench\uff09\u7684\u6d4b\u8bd5\u7528\u4f8b\u6570\u91cf\u6709\u9650\u4e14\u540c\u8d28\u5316\uff0c\u5bfc\u81f4\u6027\u80fd\u8bc4\u4f30\u4e0d\u51c6\u786e\uff0c\u5f71\u54cd\u4e86\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\u4e2d\u7684\u5956\u52b1\u4f30\u8ba1\u3002", "method": "\u63d0\u51fa\u591a\u7ef4\u5ea6\u6307\u6807\u91cf\u5316\u6d4b\u8bd5\u5957\u4ef6\u5168\u9762\u6027\uff0c\u5e76\u5f15\u5165SAGA\u65b9\u6cd5\uff0c\u7ed3\u5408\u4eba\u7c7b\u7f16\u7a0b\u7ecf\u9a8c\u548cLLM\u63a8\u7406\u80fd\u529b\u751f\u6210\u9ad8\u8d28\u91cf\u6d4b\u8bd5\u7528\u4f8b\u3002\u5f00\u53d1TCGBench\u7528\u4e8eTCG\u4efb\u52a1\u7814\u7a76\u3002", "result": "SAGA\u5728TCGBench\u4e0a\u7684\u68c0\u6d4b\u7387\u8fbe\u523090.62%\uff0c\u9a8c\u8bc1\u5668\u51c6\u786e\u7387\u4e3a32.58%\uff0c\u6bd4LiveCodeBench-v6\u9ad810.78%\u3002", "conclusion": "SAGA\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u4ee3\u7801\u751f\u6210\u8bc4\u4f30\u7684\u53ef\u9760\u6027\uff0c\u4e3aRLVR\u548c\u81ea\u52a8\u5316\u6d4b\u8bd5\u5408\u6210\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2507.06652", "pdf": "https://arxiv.org/pdf/2507.06652", "abs": "https://arxiv.org/abs/2507.06652", "authors": ["Arthur Alexander Lim", "Zhen Bin It", "Jovan Bowen Heng", "Tee Hui Teo"], "title": "Federated Learning Inspired Fuzzy Systems: Decentralized Rule Updating for Privacy and Scalable Decision Making", "categories": ["cs.LG"], "comment": null, "summary": "Fuzzy systems are a way to allow machines, systems and frameworks to deal\nwith uncertainty, which is not possible in binary systems that most computers\nuse. These systems have already been deployed for certain use cases, and fuzzy\nsystems could be further improved as proposed in this paper. Such technologies\nto draw inspiration from include machine learning and federated learning.\nMachine learning is one of the recent breakthroughs of technology and could be\napplied to fuzzy systems to further improve the results it produces. Federated\nlearning is also one of the recent technologies that have huge potential, which\nallows machine learning training to improve by reducing privacy risk, reducing\nburden on networking infrastructure, and reducing latency of the latest model.\nAspects from federated learning could be used to improve federated learning,\nsuch as applying the idea of updating the fuzzy rules that make up a key part\nof fuzzy systems, to further improve it over time. This paper discusses how\nthese improvements would be implemented in fuzzy systems, and how it would\nimprove fuzzy systems. It also discusses certain limitations on the potential\nimprovements. It concludes that these proposed ideas and improvements require\nfurther investigation to see how far the improvements are, but the potential is\nthere to improve fuzzy systems.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u5982\u4f55\u901a\u8fc7\u673a\u5668\u5b66\u4e60\u548c\u8054\u90a6\u5b66\u4e60\u6539\u8fdb\u6a21\u7cca\u7cfb\u7edf\uff0c\u4ee5\u5904\u7406\u4e0d\u786e\u5b9a\u6027\uff0c\u5e76\u5206\u6790\u5176\u6f5c\u5728\u6539\u8fdb\u4e0e\u5c40\u9650\u6027\u3002", "motivation": "\u6a21\u7cca\u7cfb\u7edf\u80fd\u5904\u7406\u4e0d\u786e\u5b9a\u6027\uff0c\u4f46\u4ecd\u6709\u6539\u8fdb\u7a7a\u95f4\uff0c\u5c24\u5176\u662f\u7ed3\u5408\u673a\u5668\u5b66\u4e60\u548c\u8054\u90a6\u5b66\u4e60\u7684\u65b0\u6280\u672f\u3002", "method": "\u63d0\u51fa\u5c06\u673a\u5668\u5b66\u4e60\u548c\u8054\u90a6\u5b66\u4e60\u7684\u7406\u5ff5\u5e94\u7528\u4e8e\u6a21\u7cca\u7cfb\u7edf\uff0c\u4f8b\u5982\u66f4\u65b0\u6a21\u7cca\u89c4\u5219\u4ee5\u9010\u6b65\u4f18\u5316\u7cfb\u7edf\u3002", "result": "\u7814\u7a76\u8868\u660e\u8fd9\u4e9b\u6539\u8fdb\u6709\u6f5c\u529b\uff0c\u4f46\u9700\u8fdb\u4e00\u6b65\u9a8c\u8bc1\u5176\u6548\u679c\u548c\u8303\u56f4\u3002", "conclusion": "\u6a21\u7cca\u7cfb\u7edf\u7ed3\u5408\u65b0\u6280\u672f\u5177\u6709\u6539\u8fdb\u6f5c\u529b\uff0c\u4f46\u9700\u6df1\u5165\u7814\u7a76\u4ee5\u786e\u5b9a\u5b9e\u9645\u6548\u679c\u3002"}}
{"id": "2507.06671", "pdf": "https://arxiv.org/pdf/2507.06671", "abs": "https://arxiv.org/abs/2507.06671", "authors": ["Boyuan Tian", "Qizhe Gao", "Siran Xianyu", "Xiaotong Cui", "Minjia Zhang"], "title": "FlexGaussian: Flexible and Cost-Effective Training-Free Compression for 3D Gaussian Splatting", "categories": ["cs.CV"], "comment": "To appear at ACM MM 2025", "summary": "3D Gaussian splatting has become a prominent technique for representing and\nrendering complex 3D scenes, due to its high fidelity and speed advantages.\nHowever, the growing demand for large-scale models calls for effective\ncompression to reduce memory and computation costs, especially on mobile and\nedge devices with limited resources. Existing compression methods effectively\nreduce 3D Gaussian parameters but often require extensive retraining or\nfine-tuning, lacking flexibility under varying compression constraints.\n  In this paper, we introduce FlexGaussian, a flexible and cost-effective\nmethod that combines mixed-precision quantization with attribute-discriminative\npruning for training-free 3D Gaussian compression. FlexGaussian eliminates the\nneed for retraining and adapts easily to diverse compression targets.\nEvaluation results show that FlexGaussian achieves up to 96.4% compression\nwhile maintaining high rendering quality (<1 dB drop in PSNR), and is\ndeployable on mobile devices. FlexGaussian delivers high compression ratios\nwithin seconds, being 1.7-2.1x faster than state-of-the-art training-free\nmethods and 10-100x faster than training-involved approaches. The code is being\nprepared and will be released soon at:\nhttps://github.com/Supercomputing-System-AI-Lab/FlexGaussian", "AI": {"tldr": "FlexGaussian\u662f\u4e00\u79cd\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\u76843D\u9ad8\u65af\u538b\u7f29\u65b9\u6cd5\uff0c\u7ed3\u5408\u6df7\u5408\u7cbe\u5ea6\u91cf\u5316\u548c\u5c5e\u6027\u5224\u522b\u526a\u679d\uff0c\u5b9e\u73b0\u9ad8\u6548\u538b\u7f29\uff0c\u9002\u7528\u4e8e\u79fb\u52a8\u8bbe\u5907\u3002", "motivation": "\u5927\u89c4\u6a213D\u6a21\u578b\u7684\u9700\u6c42\u589e\u957f\u9700\u8981\u6709\u6548\u7684\u538b\u7f29\u65b9\u6cd5\u4ee5\u51cf\u5c11\u5185\u5b58\u548c\u8ba1\u7b97\u6210\u672c\uff0c\u73b0\u6709\u65b9\u6cd5\u7f3a\u4e4f\u7075\u6d3b\u6027\u4e14\u9700\u8981\u91cd\u65b0\u8bad\u7ec3\u3002", "method": "\u7ed3\u5408\u6df7\u5408\u7cbe\u5ea6\u91cf\u5316\u548c\u5c5e\u6027\u5224\u522b\u526a\u679d\uff0c\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\uff0c\u9002\u5e94\u4e0d\u540c\u538b\u7f29\u76ee\u6807\u3002", "result": "\u538b\u7f29\u7387\u9ad8\u8fbe96.4%\uff0c\u6e32\u67d3\u8d28\u91cf\u635f\u5931\u5c0f\uff08PSNR\u4e0b\u964d<1 dB\uff09\uff0c\u901f\u5ea6\u5feb\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "FlexGaussian\u63d0\u4f9b\u4e86\u4e00\u79cd\u7075\u6d3b\u3001\u9ad8\u6548\u76843D\u9ad8\u65af\u538b\u7f29\u89e3\u51b3\u65b9\u6848\uff0c\u9002\u7528\u4e8e\u8d44\u6e90\u53d7\u9650\u7684\u8bbe\u5907\u3002"}}
{"id": "2507.06434", "pdf": "https://arxiv.org/pdf/2507.06434", "abs": "https://arxiv.org/abs/2507.06434", "authors": ["Ayrton San Joaquin", "Rokas Gipi\u0161kis", "Leon Staufer", "Ariel Gil"], "title": "Deprecating Benchmarks: Criteria and Framework", "categories": ["cs.CY", "cs.AI", "cs.LG"], "comment": "10 pages, 1 table. Accepted to the ICML 2025 Technical AI Governance\n  Workshop", "summary": "As frontier artificial intelligence (AI) models rapidly advance, benchmarks\nare integral to comparing different models and measuring their progress in\ndifferent task-specific domains. However, there is a lack of guidance on when\nand how benchmarks should be deprecated once they cease to effectively perform\ntheir purpose. This risks benchmark scores over-valuing model capabilities, or\nworse, obscuring capabilities and safety-washing. Based on a review of\nbenchmarking practices, we propose criteria to decide when to fully or\npartially deprecate benchmarks, and a framework for deprecating benchmarks. Our\nwork aims to advance the state of benchmarking towards rigorous and quality\nevaluations, especially for frontier models, and our recommendations are aimed\nto benefit benchmark developers, benchmark users, AI governance actors (across\ngovernments, academia, and industry panels), and policy makers.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u5957\u6807\u51c6\u548c\u6846\u67b6\uff0c\u7528\u4e8e\u51b3\u5b9a\u4f55\u65f6\u4ee5\u53ca\u5982\u4f55\u6dd8\u6c70\u4e0d\u518d\u6709\u6548\u7684AI\u57fa\u51c6\u6d4b\u8bd5\uff0c\u4ee5\u907f\u514d\u8bef\u5bfc\u6027\u8bc4\u4f30\u3002", "motivation": "\u968f\u7740AI\u6a21\u578b\u7684\u5feb\u901f\u53d1\u5c55\uff0c\u73b0\u6709\u7684\u57fa\u51c6\u6d4b\u8bd5\u53ef\u80fd\u4e0d\u518d\u80fd\u51c6\u786e\u8bc4\u4f30\u6a21\u578b\u80fd\u529b\uff0c\u751a\u81f3\u53ef\u80fd\u63a9\u76d6\u95ee\u9898\u6216\u8bef\u5bfc\u8bc4\u4f30\u3002", "method": "\u901a\u8fc7\u56de\u987e\u57fa\u51c6\u6d4b\u8bd5\u5b9e\u8df5\uff0c\u63d0\u51fa\u4e86\u4e00\u5957\u6dd8\u6c70\u57fa\u51c6\u6d4b\u8bd5\u7684\u6807\u51c6\u548c\u6846\u67b6\u3002", "result": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u6846\u67b6\uff0c\u5e2e\u52a9\u5f00\u53d1\u8005\u3001\u7528\u6237\u548c\u653f\u7b56\u5236\u5b9a\u8005\u66f4\u4e25\u8c28\u5730\u8bc4\u4f30AI\u6a21\u578b\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u65e8\u5728\u63a8\u52a8\u57fa\u51c6\u6d4b\u8bd5\u7684\u4e25\u8c28\u6027\uff0c\u786e\u4fdd\u5176\u5bf9\u524d\u6cbfAI\u6a21\u578b\u7684\u6709\u6548\u8bc4\u4f30\u3002"}}
{"id": "2507.06956", "pdf": "https://arxiv.org/pdf/2507.06956", "abs": "https://arxiv.org/abs/2507.06956", "authors": ["Sezen Per\u00e7in", "Xin Su", "Qutub Sha Syed", "Phillip Howard", "Aleksei Kuvshinov", "Leo Schwinn", "Kay-Ulrich Scholl"], "title": "Investigating the Robustness of Retrieval-Augmented Generation at the Query Level", "categories": ["cs.CL"], "comment": "Accepted to Generation, Evaluation & Metrics (GEM) Workshop at ACL\n  2025", "summary": "Large language models (LLMs) are very costly and inefficient to update with\nnew information. To address this limitation, retrieval-augmented generation\n(RAG) has been proposed as a solution that dynamically incorporates external\nknowledge during inference, improving factual consistency and reducing\nhallucinations. Despite its promise, RAG systems face practical challenges-most\nnotably, a strong dependence on the quality of the input query for accurate\nretrieval. In this paper, we investigate the sensitivity of different\ncomponents in the RAG pipeline to various types of query perturbations. Our\nanalysis reveals that the performance of commonly used retrievers can degrade\nsignificantly even under minor query variations. We study each module in\nisolation as well as their combined effect in an end-to-end question answering\nsetting, using both general-domain and domain-specific datasets. Additionally,\nwe propose an evaluation framework to systematically assess the query-level\nrobustness of RAG pipelines and offer actionable recommendations for\npractitioners based on the results of more than 1092 experiments we performed.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\u7cfb\u7edf\u5bf9\u67e5\u8be2\u6270\u52a8\u7684\u654f\u611f\u6027\uff0c\u53d1\u73b0\u5e38\u7528\u68c0\u7d22\u5668\u6027\u80fd\u5728\u8f7b\u5fae\u67e5\u8be2\u53d8\u5316\u4e0b\u663e\u8457\u4e0b\u964d\uff0c\u5e76\u63d0\u51fa\u8bc4\u4f30\u6846\u67b6\u548c\u6539\u8fdb\u5efa\u8bae\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u66f4\u65b0\u6210\u672c\u9ad8\u4e14\u6548\u7387\u4f4e\uff0cRAG\u901a\u8fc7\u52a8\u6001\u6574\u5408\u5916\u90e8\u77e5\u8bc6\u6539\u5584\u4e8b\u5b9e\u4e00\u81f4\u6027\u548c\u51cf\u5c11\u5e7b\u89c9\uff0c\u4f46\u5176\u6027\u80fd\u9ad8\u5ea6\u4f9d\u8d56\u67e5\u8be2\u8d28\u91cf\u3002", "method": "\u5206\u6790RAG\u6d41\u7a0b\u4e2d\u5404\u7ec4\u4ef6\u5bf9\u67e5\u8be2\u6270\u52a8\u7684\u654f\u611f\u6027\uff0c\u7814\u7a76\u6a21\u5757\u5355\u72ec\u53ca\u7ec4\u5408\u6548\u679c\uff0c\u5e76\u63d0\u51fa\u8bc4\u4f30\u6846\u67b6\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\u68c0\u7d22\u5668\u6027\u80fd\u5728\u8f7b\u5fae\u67e5\u8be2\u53d8\u5316\u4e0b\u663e\u8457\u4e0b\u964d\uff0c\u57fa\u4e8e1092\u6b21\u5b9e\u9a8c\u7ed3\u679c\u63d0\u51fa\u6539\u8fdb\u5efa\u8bae\u3002", "conclusion": "RAG\u7cfb\u7edf\u5bf9\u67e5\u8be2\u8d28\u91cf\u654f\u611f\uff0c\u9700\u7cfb\u7edf\u6027\u8bc4\u4f30\u548c\u6539\u8fdb\u4ee5\u63d0\u5347\u9c81\u68d2\u6027\u3002"}}
{"id": "2507.06694", "pdf": "https://arxiv.org/pdf/2507.06694", "abs": "https://arxiv.org/abs/2507.06694", "authors": ["Raffael Theiler", "Olga Fink"], "title": "Heterogeneous Graph Neural Networks for Short-term State Forecasting in Power Systems across Domains and Time Scales: A Hydroelectric Power Plant Case Study", "categories": ["cs.LG", "cs.SY", "eess.SP", "eess.SY"], "comment": "25 pages, 9 figures", "summary": "Accurate short-term state forecasting is essential for efficient and stable\noperation of modern power systems, especially in the context of increasing\nvariability introduced by renewable and distributed energy resources. As these\nsystems evolve rapidly, it becomes increasingly important to reliably predict\ntheir states in the short term to ensure operational stability, support control\ndecisions, and enable interpretable monitoring of sensor and machine behavior.\nModern power systems often span multiple physical domains - including\nelectrical, mechanical, hydraulic, and thermal - posing significant challenges\nfor modeling and prediction. Graph Neural Networks (GNNs) have emerged as a\npromising data-driven framework for system state estimation and state\nforecasting in such settings. By leveraging the topological structure of sensor\nnetworks, GNNs can implicitly learn inter-sensor relationships and propagate\ninformation across the network. However, most existing GNN-based methods are\ndesigned under the assumption of homogeneous sensor relationships and are\ntypically constrained to a single physical domain. This limitation restricts\ntheir ability to integrate and reason over heterogeneous sensor data commonly\nencountered in real-world energy systems, such as those used in energy\nconversion infrastructure. In this work, we propose the use of Heterogeneous\nGraph Attention Networks to address these limitations. Our approach models both\nhomogeneous intra-domain and heterogeneous inter-domain relationships among\nsensor data from two distinct physical domains - hydraulic and electrical -\nwhich exhibit fundamentally different temporal dynamics. Experimental results\ndemonstrate that our method significantly outperforms conventional baselines on\naverage by 35.5% in terms of normalized root mean square error, confirming its\neffectiveness in multi-domain, multi-rate power system state forecasting.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5f02\u6784\u56fe\u6ce8\u610f\u529b\u7f51\u7edc\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u591a\u9886\u57df\u7535\u529b\u7cfb\u7edf\u72b6\u6001\u9884\u6d4b\uff0c\u663e\u8457\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u3002", "motivation": "\u73b0\u4ee3\u7535\u529b\u7cfb\u7edf\u56e0\u53ef\u518d\u751f\u80fd\u6e90\u548c\u5206\u5e03\u5f0f\u80fd\u6e90\u7684\u5f15\u5165\u800c\u53d8\u5f97\u590d\u6742\uff0c\u9700\u8981\u51c6\u786e\u9884\u6d4b\u77ed\u671f\u72b6\u6001\u4ee5\u786e\u4fdd\u7a33\u5b9a\u8fd0\u884c\u3002\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u5904\u7406\u591a\u9886\u57df\u5f02\u6784\u4f20\u611f\u5668\u6570\u636e\u3002", "method": "\u4f7f\u7528\u5f02\u6784\u56fe\u6ce8\u610f\u529b\u7f51\u7edc\uff08Heterogeneous Graph Attention Networks\uff09\u5efa\u6a21\u4f20\u611f\u5668\u6570\u636e\u7684\u540c\u8d28\u548c\u5f02\u8d28\u5173\u7cfb\uff0c\u7279\u522b\u662f\u9488\u5bf9\u6db2\u538b\u548c\u7535\u6c14\u9886\u57df\u7684\u4e0d\u540c\u65f6\u95f4\u52a8\u6001\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u5f52\u4e00\u5316\u5747\u65b9\u6839\u8bef\u5dee\u4e0a\u5e73\u5747\u4f18\u4e8e\u4f20\u7edf\u57fa\u7ebf35.5%\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u591a\u9886\u57df\u3001\u591a\u901f\u7387\u7535\u529b\u7cfb\u7edf\u72b6\u6001\u9884\u6d4b\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u89e3\u51b3\u4e86\u5f02\u6784\u6570\u636e\u6574\u5408\u7684\u6311\u6218\u3002"}}
{"id": "2507.06679", "pdf": "https://arxiv.org/pdf/2507.06679", "abs": "https://arxiv.org/abs/2507.06679", "authors": ["Miaojing Shi", "Xiaowen Zhang", "Zijie Yue", "Yong Luo", "Cairong Zhao", "Li Li"], "title": "Text-promptable Object Counting via Quantity Awareness Enhancement", "categories": ["cs.CV"], "comment": "13 pages, 5 figures", "summary": "Recent advances in large vision-language models (VLMs) have shown remarkable\nprogress in solving the text-promptable object counting problem. Representative\nmethods typically specify text prompts with object category information in\nimages. This however is insufficient for training the model to accurately\ndistinguish the number of objects in the counting task. To this end, we propose\nQUANet, which introduces novel quantity-oriented text prompts with a\nvision-text quantity alignment loss to enhance the model's quantity awareness.\nMoreover, we propose a dual-stream adaptive counting decoder consisting of a\nTransformer stream, a CNN stream, and a number of Transformer-to-CNN\nenhancement adapters (T2C-adapters) for density map prediction. The\nT2C-adapters facilitate the effective knowledge communication and aggregation\nbetween the Transformer and CNN streams. A cross-stream quantity ranking loss\nis proposed in the end to optimize the ranking orders of predictions from the\ntwo streams. Extensive experiments on standard benchmarks such as FSC-147,\nCARPK, PUCPR+, and ShanghaiTech demonstrate our model's strong generalizability\nfor zero-shot class-agnostic counting. Code is available at\nhttps://github.com/viscom-tongji/QUANet", "AI": {"tldr": "QUANet\u901a\u8fc7\u5f15\u5165\u6570\u91cf\u5bfc\u5411\u7684\u6587\u672c\u63d0\u793a\u548c\u89c6\u89c9-\u6587\u672c\u6570\u91cf\u5bf9\u9f50\u635f\u5931\uff0c\u63d0\u5347\u4e86\u6a21\u578b\u5728\u8ba1\u6570\u4efb\u52a1\u4e2d\u7684\u6570\u91cf\u611f\u77e5\u80fd\u529b\uff0c\u5e76\u901a\u8fc7\u53cc\u6d41\u81ea\u9002\u5e94\u8ba1\u6570\u89e3\u7801\u5668\u5b9e\u73b0\u4e86\u9ad8\u6027\u80fd\u7684\u96f6\u6837\u672c\u7c7b\u65e0\u5173\u8ba1\u6570\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u6587\u672c\u63d0\u793a\u4e2d\u4ec5\u5305\u542b\u5bf9\u8c61\u7c7b\u522b\u4fe1\u606f\uff0c\u4e0d\u8db3\u4ee5\u8bad\u7ec3\u6a21\u578b\u51c6\u786e\u533a\u5206\u8ba1\u6570\u4efb\u52a1\u4e2d\u7684\u5bf9\u8c61\u6570\u91cf\u3002", "method": "\u63d0\u51faQUANet\uff0c\u5305\u62ec\u6570\u91cf\u5bfc\u5411\u7684\u6587\u672c\u63d0\u793a\u3001\u89c6\u89c9-\u6587\u672c\u6570\u91cf\u5bf9\u9f50\u635f\u5931\u3001\u53cc\u6d41\u81ea\u9002\u5e94\u8ba1\u6570\u89e3\u7801\u5668\uff08Transformer\u548cCNN\u6d41\uff09\u53caT2C\u9002\u914d\u5668\uff0c\u4ee5\u53ca\u8de8\u6d41\u6570\u91cf\u6392\u5e8f\u635f\u5931\u3002", "result": "\u5728FSC-147\u3001CARPK\u3001PUCPR+\u548cShanghaiTech\u7b49\u6807\u51c6\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u51fa\u5f3a\u5927\u7684\u96f6\u6837\u672c\u7c7b\u65e0\u5173\u8ba1\u6570\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "QUANet\u901a\u8fc7\u521b\u65b0\u7684\u6570\u91cf\u611f\u77e5\u8bbe\u8ba1\u548c\u53cc\u6d41\u89e3\u7801\u5668\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8ba1\u6570\u4efb\u52a1\u7684\u6027\u80fd\u3002"}}
{"id": "2507.06438", "pdf": "https://arxiv.org/pdf/2507.06438", "abs": "https://arxiv.org/abs/2507.06438", "authors": ["Kal\u00e9u Delphino"], "title": "Assessing the Prevalence of AI-assisted Cheating in Programming Courses: A Pilot Study", "categories": ["cs.CY", "cs.AI", "cs.HC"], "comment": "40 pages, 23 figures", "summary": "Tools that can generate computer code in response to inputs written in\nnatural language, such as ChatGPT, pose an existential threat to Computer\nScience education in its current form, since students can now use these tools\nto solve assignments without much effort. While that risk has already been\nrecognized by scholars, the proportion of the student body that is incurring in\nthis new kind of plagiarism is still an open problem. We conducted a pilot\nstudy in a large CS class (n=120) to assess the feasibility of estimating AI\nplagiarism through anonymous surveys and interviews. More than 25% of the\nsurvey respondents admitted to committing AI plagiarism. Conversely, only one\nstudent accepted to be interviewed. Given the high levels of misconduct\nacknowledgment, we conclude that surveys are an effective method for studies on\nthe matter, while interviews should be avoided or designed in a way that can\nentice participation.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86AI\u5de5\u5177\uff08\u5982ChatGPT\uff09\u5bf9\u8ba1\u7b97\u673a\u79d1\u5b66\u6559\u80b2\u7684\u5a01\u80c1\uff0c\u5e76\u901a\u8fc7\u8c03\u67e5\u548c\u8bbf\u8c08\u8bc4\u4f30\u5b66\u751f\u4f7f\u7528AI\u6284\u88ad\u7684\u6bd4\u4f8b\u3002", "motivation": "\u7814\u7a76AI\u5de5\u5177\u5bf9\u5b66\u751f\u4f5c\u4e1a\u6284\u88ad\u884c\u4e3a\u7684\u5f71\u54cd\uff0c\u586b\u8865\u5173\u4e8e\u5b66\u751f\u4f7f\u7528AI\u6284\u88ad\u6bd4\u4f8b\u7684\u7a7a\u767d\u3002", "method": "\u5728\u4e00\u95e8\u5927\u578bCS\u8bfe\u7a0b\uff08n=120\uff09\u4e2d\u8fdb\u884c\u533f\u540d\u8c03\u67e5\u548c\u8bbf\u8c08\u3002", "result": "\u8d85\u8fc725%\u7684\u53d7\u8bbf\u5b66\u751f\u627f\u8ba4\u4f7f\u7528AI\u6284\u88ad\uff0c\u4f46\u8bbf\u8c08\u53c2\u4e0e\u7387\u6781\u4f4e\u3002", "conclusion": "\u8c03\u67e5\u662f\u7814\u7a76AI\u6284\u88ad\u7684\u6709\u6548\u65b9\u6cd5\uff0c\u800c\u8bbf\u8c08\u9700\u6539\u8fdb\u8bbe\u8ba1\u4ee5\u63d0\u9ad8\u53c2\u4e0e\u7387\u3002"}}
{"id": "2507.06974", "pdf": "https://arxiv.org/pdf/2507.06974", "abs": "https://arxiv.org/abs/2507.06974", "authors": ["Artur Muratov", "Hana Fatima Shaikh", "Vanshikaa Jani", "Tarek Mahmoud", "Zhuohan Xie", "Daniil Orel", "Aaryamonvikram Singh", "Yuxia Wang", "Aadi Joshi", "Hasan Iqbal", "Ming Shan Hee", "Dhruv Sahnan", "Nikolaos Nikolaidis", "Purifica\u00e7\u00e3o Silvano", "Dimitar Dimitrov", "Roman Yangarber", "Ricardo Campos", "Al\u00edpio Jorge", "Nuno Guimar\u00e3es", "Elisa Sartori", "Nicolas Stefanovitch", "Giovanni Da San Martino", "Jakub Piskorski", "Preslav Nakov"], "title": "FRaN-X: FRaming and Narratives-eXplorer", "categories": ["cs.CL"], "comment": "19 pages, 13 figures, submitted to EMNLP 2025 - Demo Track", "summary": "We present FRaN-X, a Framing and Narratives Explorer that automatically\ndetects entity mentions and classifies their narrative roles directly from raw\ntext. FRaN-X comprises a two-stage system that combines sequence labeling with\nfine-grained role classification to reveal how entities are portrayed as\nprotagonists, antagonists, or innocents, using a unique taxonomy of 22\nfine-grained roles nested under these three main categories. The system\nsupports five languages (Bulgarian, English, Hindi, Russian, and Portuguese)\nand two domains (the Russia-Ukraine Conflict and Climate Change). It provides\nan interactive web interface for media analysts to explore and compare framing\nacross different sources, tackling the challenge of automatically detecting and\nlabeling how entities are framed. Our system allows end users to focus on a\nsingle article as well as analyze up to four articles simultaneously. We\nprovide aggregate level analysis including an intuitive graph visualization\nthat highlights the narrative a group of articles are pushing. Our system\nincludes a search feature for users to look up entities of interest, along with\na timeline view that allows analysts to track an entity's role transitions\nacross different contexts within the article. The FRaN-X system and the trained\nmodels are licensed under an MIT License. FRaN-X is publicly accessible at\nhttps://fran-x.streamlit.app/ and a video demonstration is available at\nhttps://youtu.be/VZVi-1B6yYk.", "AI": {"tldr": "FRaN-X\u662f\u4e00\u4e2a\u81ea\u52a8\u68c0\u6d4b\u5b9e\u4f53\u63d0\u53ca\u5e76\u5206\u7c7b\u5176\u53d9\u4e8b\u89d2\u8272\u7684\u7cfb\u7edf\uff0c\u652f\u6301\u4e94\u79cd\u8bed\u8a00\u548c\u4e24\u79cd\u9886\u57df\uff0c\u63d0\u4f9b\u4ea4\u4e92\u5f0f\u754c\u9762\u548c\u53ef\u89c6\u5316\u5206\u6790\u3002", "motivation": "\u89e3\u51b3\u81ea\u52a8\u68c0\u6d4b\u548c\u6807\u8bb0\u5b9e\u4f53\u5982\u4f55\u88ab\u6846\u67b6\u5316\u7684\u6311\u6218\uff0c\u5e2e\u52a9\u5a92\u4f53\u5206\u6790\u5e08\u63a2\u7d22\u548c\u6bd4\u8f83\u4e0d\u540c\u6765\u6e90\u7684\u53d9\u4e8b\u3002", "method": "\u91c7\u7528\u4e24\u9636\u6bb5\u7cfb\u7edf\uff0c\u7ed3\u5408\u5e8f\u5217\u6807\u8bb0\u548c\u7ec6\u7c92\u5ea6\u89d2\u8272\u5206\u7c7b\uff0c\u4f7f\u752822\u79cd\u7ec6\u7c92\u5ea6\u89d2\u8272\u5206\u7c7b\u5b9e\u4f53\u3002", "result": "\u7cfb\u7edf\u652f\u6301\u591a\u8bed\u8a00\u548c\u591a\u9886\u57df\u5206\u6790\uff0c\u63d0\u4f9b\u4ea4\u4e92\u5f0f\u754c\u9762\u3001\u641c\u7d22\u529f\u80fd\u548c\u65f6\u95f4\u7ebf\u89c6\u56fe\uff0c\u4fbf\u4e8e\u7528\u6237\u8ddf\u8e2a\u5b9e\u4f53\u89d2\u8272\u53d8\u5316\u3002", "conclusion": "FRaN-X\u4e3a\u5a92\u4f53\u5206\u6790\u5e08\u63d0\u4f9b\u4e86\u5f3a\u5927\u7684\u5de5\u5177\uff0c\u516c\u5f00\u53ef\u7528\uff0c\u652f\u6301\u591a\u8bed\u8a00\u548c\u591a\u9886\u57df\u5206\u6790\u3002"}}
{"id": "2507.06701", "pdf": "https://arxiv.org/pdf/2507.06701", "abs": "https://arxiv.org/abs/2507.06701", "authors": ["Michael Bloesch", "Markus Wulfmeier", "Philemon Brakel", "Todor Davchev", "Martina Zambelli", "Jost Tobias Springenberg", "Abbas Abdolmaleki", "William F Whitney", "Nicolas Heess", "Roland Hafner", "Martin Riedmiller"], "title": "Value from Observations: Towards Large-Scale Imitation Learning via Self-Improvement", "categories": ["cs.LG"], "comment": null, "summary": "Imitation Learning from Observation (IfO) offers a powerful way to learn\nbehaviors at large-scale: Unlike behavior cloning or offline reinforcement\nlearning, IfO can leverage action-free demonstrations and thus circumvents the\nneed for costly action-labeled demonstrations or reward functions. However,\ncurrent IfO research focuses on idealized scenarios with mostly bimodal-quality\ndata distributions, restricting the meaningfulness of the results. In contrast,\nthis paper investigates more nuanced distributions and introduces a method to\nlearn from such data, moving closer to a paradigm in which imitation learning\ncan be performed iteratively via self-improvement. Our method adapts RL-based\nimitation learning to action-free demonstrations, using a value function to\ntransfer information between expert and non-expert data. Through comprehensive\nevaluation, we delineate the relation between different data distributions and\nthe applicability of algorithms and highlight the limitations of established\nmethods. Our findings provide valuable insights for developing more robust and\npractical IfO techniques on a path to scalable behaviour learning.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u6a21\u4eff\u5b66\u4e60\u4ece\u89c2\u5bdf\u4e2d\uff08IfO\uff09\u7684\u6539\u8fdb\u65b9\u6cd5\uff0c\u901a\u8fc7\u9002\u5e94\u65e0\u52a8\u4f5c\u6f14\u793a\u7684\u6570\u636e\u5206\u5e03\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u6a21\u4eff\u5b66\u4e60\u65b9\u6cd5\u3002", "motivation": "\u5f53\u524dIfO\u7814\u7a76\u96c6\u4e2d\u4e8e\u7406\u60f3\u5316\u573a\u666f\uff0c\u6570\u636e\u5206\u5e03\u591a\u4e3a\u53cc\u5cf0\u8d28\u91cf\uff0c\u9650\u5236\u4e86\u7ed3\u679c\u7684\u5b9e\u7528\u6027\u3002\u672c\u6587\u65e8\u5728\u63a2\u7d22\u66f4\u590d\u6742\u7684\u6570\u636e\u5206\u5e03\uff0c\u5e76\u63d0\u51fa\u4e00\u79cd\u65b9\u6cd5\u4ee5\u5b9e\u73b0\u8fed\u4ee3\u5f0f\u81ea\u6211\u6539\u8fdb\u7684\u6a21\u4eff\u5b66\u4e60\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u6a21\u4eff\u5b66\u4e60\u65b9\u6cd5\uff0c\u5229\u7528\u4ef7\u503c\u51fd\u6570\u5728\u4e13\u5bb6\u548c\u975e\u4e13\u5bb6\u6570\u636e\u95f4\u4f20\u9012\u4fe1\u606f\uff0c\u9002\u5e94\u65e0\u52a8\u4f5c\u6f14\u793a\u7684\u6570\u636e\u5206\u5e03\u3002", "result": "\u901a\u8fc7\u5168\u9762\u8bc4\u4f30\uff0c\u63ed\u793a\u4e86\u4e0d\u540c\u6570\u636e\u5206\u5e03\u4e0e\u7b97\u6cd5\u9002\u7528\u6027\u7684\u5173\u7cfb\uff0c\u5e76\u6307\u51fa\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u5c40\u9650\u6027\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u4e3a\u5f00\u53d1\u66f4\u9c81\u68d2\u548c\u5b9e\u7528\u7684IfO\u6280\u672f\u63d0\u4f9b\u4e86\u6709\u4ef7\u503c\u7684\u89c1\u89e3\uff0c\u63a8\u52a8\u4e86\u53ef\u6269\u5c55\u884c\u4e3a\u5b66\u4e60\u7684\u8fdb\u5c55\u3002"}}
{"id": "2507.07024", "pdf": "https://arxiv.org/pdf/2507.07024", "abs": "https://arxiv.org/abs/2507.07024", "authors": ["Weijia Shi", "Akshita Bhagia", "Kevin Farhat", "Niklas Muennighoff", "Pete Walsh", "Jacob Morrison", "Dustin Schwenk", "Shayne Longpre", "Jake Poznanski", "Allyson Ettinger", "Daogao Liu", "Margaret Li", "Dirk Groeneveld", "Mike Lewis", "Wen-tau Yih", "Luca Soldaini", "Kyle Lo", "Noah A. Smith", "Luke Zettlemoyer", "Pang Wei Koh", "Hannaneh Hajishirzi", "Ali Farhadi", "Sewon Min"], "title": "FlexOlmo: Open Language Models for Flexible Data Use", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "We introduce FlexOlmo, a new class of language models (LMs) that supports (1)\ndistributed training without data sharing, where different model parameters are\nindependently trained on closed datasets, and (2) data-flexible inference,\nwhere these parameters along with their associated data can be flexibly\nincluded or excluded from model inferences with no further training. FlexOlmo\nemploys a mixture-of-experts (MoE) architecture where each expert is trained\nindependently on closed datasets and later integrated through a new\ndomain-informed routing without any joint training. FlexOlmo is trained on\nFlexMix, a corpus we curate comprising publicly available datasets alongside\nseven domain-specific sets, representing realistic approximations of closed\nsets. We evaluate models with up to 37 billion parameters (20 billion active)\non 31 diverse downstream tasks. We show that a general expert trained on public\ndata can be effectively combined with independently trained experts from other\ndata owners, leading to an average 41% relative improvement while allowing\nusers to opt out of certain data based on data licensing or permission\nrequirements. Our approach also outperforms prior model merging methods by\n10.1% on average and surpasses the standard MoE trained without data\nrestrictions using the same training FLOPs. Altogether, this research presents\na solution for both data owners and researchers in regulated industries with\nsensitive or protected data. FlexOlmo enables benefiting from closed data while\nrespecting data owners' preferences by keeping their data local and supporting\nfine-grained control of data access during inference.", "AI": {"tldr": "FlexOlmo\u662f\u4e00\u79cd\u65b0\u578b\u8bed\u8a00\u6a21\u578b\uff0c\u652f\u6301\u5206\u5e03\u5f0f\u8bad\u7ec3\u548c\u6570\u636e\u7075\u6d3b\u63a8\u7406\uff0c\u65e0\u9700\u5171\u4eab\u6570\u636e\uff0c\u901a\u8fc7\u6df7\u5408\u4e13\u5bb6\u67b6\u6784\u5b9e\u73b0\u72ec\u7acb\u8bad\u7ec3\u548c\u7075\u6d3b\u96c6\u6210\u3002", "motivation": "\u89e3\u51b3\u5728\u53d7\u76d1\u7ba1\u884c\u4e1a\u4e2d\u4f7f\u7528\u654f\u611f\u6216\u53d7\u4fdd\u62a4\u6570\u636e\u65f6\u7684\u6570\u636e\u5171\u4eab\u548c\u8bbf\u95ee\u63a7\u5236\u95ee\u9898\u3002", "method": "\u91c7\u7528\u6df7\u5408\u4e13\u5bb6\uff08MoE\uff09\u67b6\u6784\uff0c\u6bcf\u4e2a\u4e13\u5bb6\u72ec\u7acb\u8bad\u7ec3\u4e8e\u5c01\u95ed\u6570\u636e\u96c6\uff0c\u901a\u8fc7\u9886\u57df\u611f\u77e5\u8def\u7531\u96c6\u6210\uff0c\u65e0\u9700\u8054\u5408\u8bad\u7ec3\u3002", "result": "\u572831\u4e2a\u4efb\u52a1\u4e0a\u8bc4\u4f30\uff0cFlexOlmo\u5e73\u5747\u76f8\u5bf9\u6539\u8fdb41%\uff0c\u4f18\u4e8e\u73b0\u6709\u6a21\u578b\u5408\u5e76\u65b9\u6cd510.1%\uff0c\u4e14\u4e0e\u65e0\u9650\u5236\u8bad\u7ec3\u7684MoE\u6027\u80fd\u76f8\u5f53\u3002", "conclusion": "FlexOlmo\u4e3a\u6570\u636e\u6240\u6709\u8005\u548c\u7814\u7a76\u8005\u63d0\u4f9b\u4e86\u4e00\u79cd\u89e3\u51b3\u65b9\u6848\uff0c\u65e2\u80fd\u5229\u7528\u5c01\u95ed\u6570\u636e\uff0c\u53c8\u80fd\u5c0a\u91cd\u6570\u636e\u6240\u6709\u8005\u7684\u8bbf\u95ee\u63a7\u5236\u9700\u6c42\u3002"}}
{"id": "2507.06712", "pdf": "https://arxiv.org/pdf/2507.06712", "abs": "https://arxiv.org/abs/2507.06712", "authors": ["Ayoub Farkane", "Mohamed Boutayeb", "Mustapha Oudani", "Mounir Ghogho"], "title": "PINN-Obs: Physics-Informed Neural Network-Based Observer for Nonlinear Dynamical Systems", "categories": ["cs.LG", "math.DS", "nlin.CD"], "comment": null, "summary": "State estimation for nonlinear dynamical systems is a critical challenge in\ncontrol and engineering applications, particularly when only partial and noisy\nmeasurements are available. This paper introduces a novel Adaptive\nPhysics-Informed Neural Network-based Observer (PINN-Obs) for accurate state\nestimation in nonlinear systems. Unlike traditional model-based observers,\nwhich require explicit system transformations or linearization, the proposed\nframework directly integrates system dynamics and sensor data into a\nphysics-informed learning process. The observer adaptively learns an optimal\ngain matrix, ensuring convergence of the estimated states to the true system\nstates. A rigorous theoretical analysis establishes formal convergence\nguarantees, demonstrating that the proposed approach achieves uniform error\nminimization under mild observability conditions. The effectiveness of PINN-Obs\nis validated through extensive numerical simulations on diverse nonlinear\nsystems, including an induction motor model, a satellite motion system, and\nbenchmark academic examples. Comparative experimental studies against existing\nobserver designs highlight its superior accuracy, robustness, and adaptability.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u7269\u7406\u4fe1\u606f\u795e\u7ecf\u7f51\u7edc\u7684\u975e\u7ebf\u6027\u7cfb\u7edf\u72b6\u6001\u4f30\u8ba1\u65b9\u6cd5\uff08PINN-Obs\uff09\uff0c\u901a\u8fc7\u81ea\u9002\u5e94\u5b66\u4e60\u589e\u76ca\u77e9\u9635\u5b9e\u73b0\u9ad8\u7cbe\u5ea6\u4f30\u8ba1\uff0c\u5e76\u5728\u591a\u79cd\u975e\u7ebf\u6027\u7cfb\u7edf\u4e2d\u9a8c\u8bc1\u4e86\u5176\u4f18\u8d8a\u6027\u3002", "motivation": "\u975e\u7ebf\u6027\u52a8\u6001\u7cfb\u7edf\u7684\u72b6\u6001\u4f30\u8ba1\u5728\u90e8\u5206\u548c\u566a\u58f0\u6d4b\u91cf\u4e0b\u5177\u6709\u6311\u6218\u6027\uff0c\u4f20\u7edf\u65b9\u6cd5\u9700\u8981\u663e\u5f0f\u7cfb\u7edf\u53d8\u6362\u6216\u7ebf\u6027\u5316\uff0c\u9650\u5236\u4e86\u5176\u5e94\u7528\u3002", "method": "\u76f4\u63a5\u6574\u5408\u7cfb\u7edf\u52a8\u529b\u5b66\u548c\u4f20\u611f\u5668\u6570\u636e\u5230\u7269\u7406\u4fe1\u606f\u5b66\u4e60\u8fc7\u7a0b\u4e2d\uff0c\u81ea\u9002\u5e94\u5b66\u4e60\u6700\u4f18\u589e\u76ca\u77e9\u9635\u3002", "result": "\u7406\u8bba\u5206\u6790\u8bc1\u660e\u4e86\u6536\u655b\u6027\uff0c\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5728\u591a\u79cd\u975e\u7ebf\u6027\u7cfb\u7edf\u4e2d\u7684\u9ad8\u7cbe\u5ea6\u548c\u9c81\u68d2\u6027\u3002", "conclusion": "PINN-Obs\u5728\u975e\u7ebf\u6027\u7cfb\u7edf\u72b6\u6001\u4f30\u8ba1\u4e2d\u8868\u73b0\u51fa\u66f4\u9ad8\u7684\u51c6\u786e\u6027\u548c\u9002\u5e94\u6027\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002"}}
{"id": "2507.06689", "pdf": "https://arxiv.org/pdf/2507.06689", "abs": "https://arxiv.org/abs/2507.06689", "authors": ["Hao Tang", "Ling Shao", "Zhenyu Zhang", "Luc Van Gool", "Nicu Sebe"], "title": "Spatial-Temporal Graph Mamba for Music-Guided Dance Video Synthesis", "categories": ["cs.CV"], "comment": "Accepted to TPAMI 2025", "summary": "We propose a novel spatial-temporal graph Mamba (STG-Mamba) for the\nmusic-guided dance video synthesis task, i.e., to translate the input music to\na dance video. STG-Mamba consists of two translation mappings:\nmusic-to-skeleton translation and skeleton-to-video translation. In the\nmusic-to-skeleton translation, we introduce a novel spatial-temporal graph\nMamba (STGM) block to effectively construct skeleton sequences from the input\nmusic, capturing dependencies between joints in both the spatial and temporal\ndimensions. For the skeleton-to-video translation, we propose a novel\nself-supervised regularization network to translate the generated skeletons,\nalong with a conditional image, into a dance video. Lastly, we collect a new\nskeleton-to-video translation dataset from the Internet, containing 54,944\nvideo clips. Extensive experiments demonstrate that STG-Mamba achieves\nsignificantly better results than existing methods.", "AI": {"tldr": "STG-Mamba\u662f\u4e00\u79cd\u7528\u4e8e\u97f3\u4e50\u5f15\u5bfc\u821e\u8e48\u89c6\u9891\u5408\u6210\u7684\u7a7a\u95f4-\u65f6\u95f4\u56fe\u6a21\u578b\uff0c\u5305\u542b\u97f3\u4e50\u5230\u9aa8\u67b6\u548c\u9aa8\u67b6\u5230\u89c6\u9891\u4e24\u4e2a\u7ffb\u8bd1\u6620\u5c04\uff0c\u5e76\u5728\u5b9e\u9a8c\u4e2d\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u89e3\u51b3\u5c06\u8f93\u5165\u97f3\u4e50\u7ffb\u8bd1\u4e3a\u821e\u8e48\u89c6\u9891\u7684\u4efb\u52a1\uff0c\u901a\u8fc7\u7a7a\u95f4-\u65f6\u95f4\u56fe\u6a21\u578b\u6355\u6349\u5173\u8282\u4f9d\u8d56\u5173\u7cfb\u3002", "method": "1. \u97f3\u4e50\u5230\u9aa8\u67b6\u7ffb\u8bd1\uff1a\u4f7f\u7528STGM\u5757\u6784\u5efa\u9aa8\u67b6\u5e8f\u5217\uff1b2. \u9aa8\u67b6\u5230\u89c6\u9891\u7ffb\u8bd1\uff1a\u63d0\u51fa\u81ea\u76d1\u7763\u6b63\u5219\u5316\u7f51\u7edc\u3002", "result": "\u5b9e\u9a8c\u8868\u660eSTG-Mamba\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "STG-Mamba\u5728\u97f3\u4e50\u5230\u821e\u8e48\u89c6\u9891\u5408\u6210\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u5177\u6709\u521b\u65b0\u6027\u548c\u5b9e\u7528\u6027\u3002"}}
{"id": "2507.07030", "pdf": "https://arxiv.org/pdf/2507.07030", "abs": "https://arxiv.org/abs/2507.07030", "authors": ["Fengran Mo", "Yifan Gao", "Chuan Meng", "Xin Liu", "Zhuofeng Wu", "Kelong Mao", "Zhengyang Wang", "Pei Chen", "Zheng Li", "Xian Li", "Bing Yin", "Meng Jiang"], "title": "UniConv: Unifying Retrieval and Response Generation for Large Language Models in Conversations", "categories": ["cs.CL"], "comment": "Accepted by ACL 2025 (main)", "summary": "The rapid advancement of conversational search systems revolutionizes how\ninformation is accessed by enabling the multi-turn interaction between the user\nand the system. Existing conversational search systems are usually built with\ntwo different models. This separation restricts the system from leveraging the\nintrinsic knowledge of the models simultaneously, which cannot ensure the\neffectiveness of retrieval benefiting the generation. The existing studies for\ndeveloping unified models cannot fully address the aspects of understanding\nconversational context, managing retrieval independently, and generating\nresponses. In this paper, we explore how to unify dense retrieval and response\ngeneration for large language models in conversation. We conduct joint\nfine-tuning with different objectives and design two mechanisms to reduce the\ninconsistency risks while mitigating data discrepancy. The evaluations on five\nconversational search datasets demonstrate that our unified model can mutually\nimprove both tasks and outperform the existing baselines.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7edf\u4e00\u5bc6\u96c6\u68c0\u7d22\u548c\u54cd\u5e94\u751f\u6210\u7684\u6a21\u578b\uff0c\u901a\u8fc7\u8054\u5408\u5fae\u8c03\u548c\u673a\u5236\u8bbe\u8ba1\u89e3\u51b3\u4e86\u73b0\u6709\u5206\u79bb\u6a21\u578b\u7684\u5c40\u9650\u6027\uff0c\u5e76\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u4e8e\u57fa\u7ebf\u3002", "motivation": "\u73b0\u6709\u5bf9\u8bdd\u641c\u7d22\u7cfb\u7edf\u901a\u5e38\u4f7f\u7528\u5206\u79bb\u7684\u6a21\u578b\uff0c\u65e0\u6cd5\u540c\u65f6\u5229\u7528\u6a21\u578b\u7684\u5185\u5728\u77e5\u8bc6\uff0c\u5f71\u54cd\u4e86\u68c0\u7d22\u548c\u751f\u6210\u7684\u6548\u679c\u3002", "method": "\u901a\u8fc7\u8054\u5408\u5fae\u8c03\u4e0d\u540c\u76ee\u6807\uff0c\u5e76\u8bbe\u8ba1\u4e24\u79cd\u673a\u5236\u4ee5\u51cf\u5c11\u4e0d\u4e00\u81f4\u98ce\u9669\u548c\u6570\u636e\u5dee\u5f02\u3002", "result": "\u5728\u4e94\u4e2a\u5bf9\u8bdd\u641c\u7d22\u6570\u636e\u96c6\u4e0a\u7684\u8bc4\u4f30\u8868\u660e\uff0c\u7edf\u4e00\u6a21\u578b\u80fd\u540c\u65f6\u63d0\u5347\u4e24\u9879\u4efb\u52a1\uff0c\u5e76\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u3002", "conclusion": "\u7edf\u4e00\u6a21\u578b\u80fd\u6709\u6548\u89e3\u51b3\u5206\u79bb\u6a21\u578b\u7684\u5c40\u9650\u6027\uff0c\u63d0\u5347\u5bf9\u8bdd\u641c\u7d22\u7cfb\u7edf\u7684\u6027\u80fd\u3002"}}
{"id": "2507.06752", "pdf": "https://arxiv.org/pdf/2507.06752", "abs": "https://arxiv.org/abs/2507.06752", "authors": ["Heng Wu", "Benzhuo Lu"], "title": "Mathematical artificial data for operator learning", "categories": ["cs.LG", "cs.NA", "math.NA", "stat.ML", "68T07, 35J05", "I.2.6; G.1.8; G.4"], "comment": "22 pages, 5 figures", "summary": "Machine learning has emerged as a transformative tool for solving\ndifferential equations (DEs), yet prevailing methodologies remain constrained\nby dual limitations: data-driven methods demand costly labeled datasets while\nmodel-driven techniques face efficiency-accuracy trade-offs. We present the\nMathematical Artificial Data (MAD) framework, a new paradigm that integrates\nphysical laws with data-driven learning to facilitate large-scale operator\ndiscovery. By exploiting DEs' intrinsic mathematical structure to generate\nphysics-embedded analytical solutions and associated synthetic data, MAD\nfundamentally eliminates dependence on experimental or simulated training data.\nThis enables computationally efficient operator learning across multi-parameter\nsystems while maintaining mathematical rigor. Through numerical demonstrations\nspanning 2D parametric problems where both the boundary values and source term\nare functions, we showcase MAD's generalizability and superior\nefficiency/accuracy across various DE scenarios. This\nphysics-embedded-data-driven framework and its capacity to handle complex\nparameter spaces gives it the potential to become a universal paradigm for\nphysics-informed machine intelligence in scientific computing.", "AI": {"tldr": "MAD\u6846\u67b6\u7ed3\u5408\u7269\u7406\u5b9a\u5f8b\u4e0e\u6570\u636e\u9a71\u52a8\u5b66\u4e60\uff0c\u901a\u8fc7\u751f\u6210\u7269\u7406\u5d4c\u5165\u7684\u89e3\u6790\u89e3\u548c\u5408\u6210\u6570\u636e\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u65b9\u6cd5\u5bf9\u6807\u8bb0\u6570\u636e\u4f9d\u8d56\u548c\u6548\u7387-\u7cbe\u5ea6\u6743\u8861\u7684\u95ee\u9898\u3002", "motivation": "\u4f20\u7edf\u65b9\u6cd5\u5728\u89e3\u51b3\u5fae\u5206\u65b9\u7a0b\u65f6\u9762\u4e34\u6807\u8bb0\u6570\u636e\u6210\u672c\u9ad8\u6216\u6548\u7387-\u7cbe\u5ea6\u6743\u8861\u7684\u5c40\u9650\u6027\uff0c\u9700\u8981\u4e00\u79cd\u66f4\u9ad8\u6548\u4e14\u4e25\u8c28\u7684\u65b0\u65b9\u6cd5\u3002", "method": "\u63d0\u51faMAD\u6846\u67b6\uff0c\u5229\u7528\u5fae\u5206\u65b9\u7a0b\u7684\u6570\u5b66\u7ed3\u6784\u751f\u6210\u7269\u7406\u5d4c\u5165\u7684\u89e3\u6790\u89e3\u548c\u5408\u6210\u6570\u636e\uff0c\u5b9e\u73b0\u65e0\u9700\u5b9e\u9a8c\u6216\u6a21\u62df\u6570\u636e\u7684\u7b97\u5b50\u5b66\u4e60\u3002", "result": "\u57282D\u53c2\u6570\u5316\u95ee\u9898\u4e2d\u5c55\u793a\u4e86MAD\u7684\u6cdb\u5316\u80fd\u529b\u548c\u9ad8\u6548\u6027/\u51c6\u786e\u6027\uff0c\u9002\u7528\u4e8e\u591a\u79cd\u5fae\u5206\u65b9\u7a0b\u573a\u666f\u3002", "conclusion": "MAD\u6846\u67b6\u6709\u671b\u6210\u4e3a\u79d1\u5b66\u8ba1\u7b97\u4e2d\u7269\u7406\u4fe1\u606f\u673a\u5668\u667a\u80fd\u7684\u901a\u7528\u8303\u5f0f\u3002"}}
{"id": "2507.07050", "pdf": "https://arxiv.org/pdf/2507.07050", "abs": "https://arxiv.org/abs/2507.07050", "authors": ["Ashen Weligalle"], "title": "Discrete Diffusion Models for Language Generation", "categories": ["cs.CL", "cs.LG", "stat.ML", "68T50 (Primary) 68Q32, 60J27 (Secondary)", "G.3"], "comment": "pdfLaTeX, 69 pages with 21 figures, Licentiate Thesis", "summary": "Diffusion models have emerged as a powerful class of generative models,\nachieving state-of-the-art results in continuous data domains such as image and\nvideo generation. Their core mechanism involves a forward diffusion process\nthat gradually transforms structured data into a Gaussian-like distribution,\nfollowed by a learned reverse process to reconstruct the data. While successful\nin continuous modalities, applying this framework to discrete data-particularly\nnatural language-remains challenging due to token dependency complexities and\nthe lack of a defined generation order.This thesis investigates the feasibility\nand performance of discrete diffusion models for natural language generation.\nSpecifically, we evaluate the Discrete Denoising Diffusion Probabilistic Model\n(D3PM) and compare it with traditional autoregressive (AR) language models. To\nassess generative performance, we use Bits Per Token (BPT), Negative\nLog-Likelihood (NLL), Perplexity (PPL), and Batch Processing Speed.\n  Results show the best-performing D3PM model achieves a BPT of 5.72, with a\nmean of 8.05. The AR model outperforms in compression with a lower mean BPT of\n4.59, but D3PM achieves higher processing speed, reaching up to 3.97 batches\nper sec., indicating potential for parallel generation.All evaluations were\nconducted under consistent conditions-generating 100,000 tokens per model with\na fixed batch size of four-for fair comparison. This research presents a\ndetailed analysis of diffusion-based vs. autoregressive models, highlighting\ntrade-offs in generative quality and efficiency. Findings emphasize both the\npromise and limitations of diffusion models for discrete data, supporting\nfuture work in non-autoregressive language generation.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86\u79bb\u6563\u6269\u6563\u6a21\u578b\uff08D3PM\uff09\u5728\u81ea\u7136\u8bed\u8a00\u751f\u6210\u4e2d\u7684\u8868\u73b0\uff0c\u5e76\u4e0e\u4f20\u7edf\u7684\u81ea\u56de\u5f52\u6a21\u578b\uff08AR\uff09\u8fdb\u884c\u4e86\u6bd4\u8f83\u3002\u7ed3\u679c\u663e\u793aD3PM\u5728\u751f\u6210\u901f\u5ea6\u4e0a\u5177\u6709\u4f18\u52bf\uff0c\u4f46\u5728\u538b\u7f29\u6027\u80fd\u4e0a\u7565\u900a\u4e8eAR\u6a21\u578b\u3002", "motivation": "\u6269\u6563\u6a21\u578b\u5728\u8fde\u7eed\u6570\u636e\u9886\u57df\uff08\u5982\u56fe\u50cf\u548c\u89c6\u9891\u751f\u6210\uff09\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5728\u79bb\u6563\u6570\u636e\uff08\u5982\u81ea\u7136\u8bed\u8a00\uff09\u4e2d\u7684\u5e94\u7528\u4ecd\u9762\u4e34\u6311\u6218\u3002\u672c\u7814\u7a76\u65e8\u5728\u63a2\u7d22\u79bb\u6563\u6269\u6563\u6a21\u578b\u5728\u81ea\u7136\u8bed\u8a00\u751f\u6210\u4e2d\u7684\u53ef\u884c\u6027\u548c\u6027\u80fd\u3002", "method": "\u4f7f\u7528\u79bb\u6563\u53bb\u566a\u6269\u6563\u6982\u7387\u6a21\u578b\uff08D3PM\uff09\u4e0e\u4f20\u7edf\u81ea\u56de\u5f52\u6a21\u578b\uff08AR\uff09\u8fdb\u884c\u5bf9\u6bd4\uff0c\u8bc4\u4f30\u6307\u6807\u5305\u62ecBits Per Token\uff08BPT\uff09\u3001Negative Log-Likelihood\uff08NLL\uff09\u3001Perplexity\uff08PPL\uff09\u548cBatch Processing Speed\u3002", "result": "D3PM\u7684\u6700\u4f73BPT\u4e3a5.72\uff0c\u5747\u503c\u4e3a8.05\uff0c\u800cAR\u6a21\u578b\u7684BPT\u5747\u503c\u4e3a4.59\u3002D3PM\u5728\u751f\u6210\u901f\u5ea6\u4e0a\u8868\u73b0\u66f4\u4f18\uff0c\u8fbe\u52303.97\u6279\u6b21/\u79d2\u3002", "conclusion": "\u79bb\u6563\u6269\u6563\u6a21\u578b\u5728\u81ea\u7136\u8bed\u8a00\u751f\u6210\u4e2d\u5c55\u73b0\u51fa\u6f5c\u529b\uff0c\u5c24\u5176\u662f\u5728\u5e76\u884c\u751f\u6210\u65b9\u9762\uff0c\u4f46\u5728\u751f\u6210\u8d28\u91cf\u4e0a\u4ecd\u9700\u6539\u8fdb\u3002\u7814\u7a76\u4e3a\u672a\u6765\u975e\u81ea\u56de\u5f52\u8bed\u8a00\u751f\u6210\u63d0\u4f9b\u4e86\u53c2\u8003\u3002"}}
{"id": "2507.06765", "pdf": "https://arxiv.org/pdf/2507.06765", "abs": "https://arxiv.org/abs/2507.06765", "authors": ["Enda D. V. Bigarella"], "title": "Robust Deep Network Learning of Nonlinear Regression Tasks by Parametric Leaky Exponential Linear Units (LELUs) and a Diffusion Metric", "categories": ["cs.LG"], "comment": null, "summary": "This document proposes a parametric activation function (ac.f.) aimed at\nimproving multidimensional nonlinear data regression. It is a established\nknowledge that nonlinear ac.f.'s are required for learning nonlinear datasets.\nThis work shows that smoothness and gradient properties of the ac.f. further\nimpact the performance of large neural networks in terms of overfitting and\nsensitivity to model parameters. Smooth but vanishing-gradient ac.f.'s such as\nELU or SiLU have limited performance and non-smooth ac.f.'s such as RELU and\nLeaky-RELU further impart discontinuity in the trained model. Improved\nperformance is demonstrated with a smooth \"Leaky Exponential Linear Unit\", with\nnon-zero gradient that can be trained. A novel diffusion-loss metric is also\nproposed to gauge the performance of the trained models in terms of\noverfitting.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u53c2\u6570\u5316\u6fc0\u6d3b\u51fd\u6570\uff08Leaky Exponential Linear Unit\uff09\uff0c\u7528\u4e8e\u6539\u5584\u591a\u7ef4\u975e\u7ebf\u6027\u6570\u636e\u56de\u5f52\u6027\u80fd\uff0c\u5e76\u901a\u8fc7\u65b0\u7684\u6269\u6563\u635f\u5931\u6307\u6807\u8bc4\u4f30\u6a21\u578b\u8fc7\u62df\u5408\u3002", "motivation": "\u975e\u7ebf\u6027\u6fc0\u6d3b\u51fd\u6570\u5bf9\u5b66\u4e60\u975e\u7ebf\u6027\u6570\u636e\u96c6\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u5176\u5e73\u6ed1\u6027\u548c\u68af\u5ea6\u7279\u6027\u4f1a\u5f71\u54cd\u5927\u578b\u795e\u7ecf\u7f51\u7edc\u7684\u6027\u80fd\uff0c\u5c24\u5176\u662f\u8fc7\u62df\u5408\u548c\u6a21\u578b\u53c2\u6570\u654f\u611f\u6027\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5e73\u6ed1\u4e14\u5177\u6709\u975e\u96f6\u68af\u5ea6\u7684Leaky Exponential Linear Unit\u6fc0\u6d3b\u51fd\u6570\uff0c\u5e76\u5f15\u5165\u6269\u6563\u635f\u5931\u6307\u6807\u8bc4\u4f30\u6a21\u578b\u6027\u80fd\u3002", "result": "\u6539\u8fdb\u7684\u6fc0\u6d3b\u51fd\u6570\u5728\u6027\u80fd\u4e0a\u4f18\u4e8e\u4f20\u7edf\u5e73\u6ed1\u4f46\u68af\u5ea6\u6d88\u5931\u7684\u6fc0\u6d3b\u51fd\u6570\uff08\u5982ELU\u3001SiLU\uff09\u548c\u975e\u5e73\u6ed1\u6fc0\u6d3b\u51fd\u6570\uff08\u5982RELU\u3001Leaky-RELU\uff09\u3002", "conclusion": "\u5e73\u6ed1\u4e14\u5177\u6709\u975e\u96f6\u68af\u5ea6\u7684\u6fc0\u6d3b\u51fd\u6570\u80fd\u6709\u6548\u63d0\u5347\u6a21\u578b\u6027\u80fd\uff0c\u51cf\u5c11\u8fc7\u62df\u5408\uff0c\u6269\u6563\u635f\u5931\u6307\u6807\u4e3a\u6a21\u578b\u8bc4\u4f30\u63d0\u4f9b\u4e86\u65b0\u5de5\u5177\u3002"}}
{"id": "2507.06732", "pdf": "https://arxiv.org/pdf/2507.06732", "abs": "https://arxiv.org/abs/2507.06732", "authors": ["Sobhan Asasi", "Mohamed Ilyes Lakhal", "Richard Bowden"], "title": "Hierarchical Feature Alignment for Gloss-Free Sign Language Translation", "categories": ["cs.CV"], "comment": "Accepted in SLTAT", "summary": "Sign Language Translation (SLT) attempts to convert sign language videos into\nspoken sentences. However, many existing methods struggle with the disparity\nbetween visual and textual representations during end-to-end learning.\nGloss-based approaches help to bridge this gap by leveraging structured\nlinguistic information. While, gloss-free methods offer greater flexibility and\nremove the burden of annotation, they require effective alignment strategies.\nRecent advances in Large Language Models (LLMs) have enabled gloss-free SLT by\ngenerating text-like representations from sign videos. In this work, we\nintroduce a novel hierarchical pre-training strategy inspired by the structure\nof sign language, incorporating pseudo-glosses and contrastive video-language\nalignment. Our method hierarchically extracts features at frame, segment, and\nvideo levels, aligning them with pseudo-glosses and the spoken sentence to\nenhance translation quality. Experiments demonstrate that our approach improves\nBLEU-4 and ROUGE scores while maintaining efficiency.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u4f2a\u6ce8\u91ca\u548c\u5bf9\u6bd4\u89c6\u9891-\u8bed\u8a00\u5bf9\u9f50\u7684\u5206\u5c42\u9884\u8bad\u7ec3\u7b56\u7565\uff0c\u7528\u4e8e\u63d0\u5347\u624b\u8bed\u7ffb\u8bd1\u8d28\u91cf\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\u5728\u89c6\u89c9\u4e0e\u6587\u672c\u8868\u793a\u4e4b\u95f4\u7684\u5dee\u5f02\u95ee\u9898\uff0c\u540c\u65f6\u907f\u514d\u4f9d\u8d56\u6ce8\u91ca\u7684\u8d1f\u62c5\u3002", "method": "\u91c7\u7528\u5206\u5c42\u7279\u5f81\u63d0\u53d6\uff08\u5e27\u3001\u7247\u6bb5\u3001\u89c6\u9891\u7ea7\uff09\u5e76\u4e0e\u4f2a\u6ce8\u91ca\u548c\u53e3\u8bed\u5bf9\u9f50\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u63d0\u9ad8\u4e86BLEU-4\u548cROUGE\u5206\u6570\uff0c\u4e14\u4fdd\u6301\u9ad8\u6548\u3002", "conclusion": "\u5206\u5c42\u9884\u8bad\u7ec3\u7b56\u7565\u6709\u6548\u63d0\u5347\u4e86\u624b\u8bed\u7ffb\u8bd1\u7684\u6027\u80fd\u548c\u7075\u6d3b\u6027\u3002"}}
{"id": "2507.06775", "pdf": "https://arxiv.org/pdf/2507.06775", "abs": "https://arxiv.org/abs/2507.06775", "authors": ["Mario Tuci", "Lennart Bastian", "Benjamin Dupuis", "Nassir Navab", "Tolga Birdal", "Umut \u015eim\u015fekli"], "title": "Mutual Information Free Topological Generalization Bounds via Stability", "categories": ["cs.LG", "math.AT", "stat.ML"], "comment": "25 pages, 5 figures", "summary": "Providing generalization guarantees for stochastic optimization algorithms is\na major challenge in modern learning theory. Recently, several studies\nhighlighted the impact of the geometry of training trajectories on the\ngeneralization error, both theoretically and empirically. Among these works, a\nseries of topological generalization bounds have been proposed, relating the\ngeneralization error to notions of topological complexity that stem from\ntopological data analysis (TDA). Despite their empirical success, these bounds\nrely on intricate information-theoretic (IT) terms that can be bounded in\nspecific cases but remain intractable for practical algorithms (such as ADAM),\npotentially reducing the relevance of the derived bounds. In this paper, we\nseek to formulate comprehensive and interpretable topological generalization\nbounds free of intractable mutual information terms. To this end, we introduce\na novel learning theoretic framework that departs from the existing strategies\nvia proof techniques rooted in algorithmic stability. By extending an existing\nnotion of \\textit{hypothesis set stability}, to \\textit{trajectory stability},\nwe prove that the generalization error of trajectory-stable algorithms can be\nupper bounded in terms of (i) TDA quantities describing the complexity of the\ntrajectory of the optimizer in the parameter space, and (ii) the trajectory\nstability parameter of the algorithm. Through a series of experimental\nevaluations, we demonstrate that the TDA terms in the bound are of great\nimportance, especially as the number of training samples grows. This ultimately\nforms an explanation of the empirical success of the topological generalization\nbounds.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u62d3\u6251\u6cdb\u5316\u754c\uff0c\u907f\u514d\u4e86\u590d\u6742\u7684\u4fe1\u606f\u8bba\u9879\uff0c\u901a\u8fc7\u8f68\u8ff9\u7a33\u5b9a\u6027\u6846\u67b6\u5c06\u6cdb\u5316\u8bef\u5dee\u4e0e\u62d3\u6251\u6570\u636e\u5206\u6790\u548c\u7b97\u6cd5\u7a33\u5b9a\u6027\u8054\u7cfb\u8d77\u6765\u3002", "motivation": "\u73b0\u6709\u62d3\u6251\u6cdb\u5316\u754c\u4f9d\u8d56\u590d\u6742\u7684\u4fe1\u606f\u8bba\u9879\uff0c\u96be\u4ee5\u5e94\u7528\u4e8e\u5b9e\u9645\u7b97\u6cd5\uff08\u5982ADAM\uff09\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u66f4\u6613\u89e3\u91ca\u4e14\u5b9e\u7528\u7684\u6cdb\u5316\u754c\u3002", "method": "\u5f15\u5165\u8f68\u8ff9\u7a33\u5b9a\u6027\u6846\u67b6\uff0c\u6269\u5c55\u5047\u8bbe\u96c6\u7a33\u5b9a\u6027\u6982\u5ff5\uff0c\u901a\u8fc7\u62d3\u6251\u6570\u636e\u5206\u6790\u91cf\u5316\u4f18\u5316\u5668\u8f68\u8ff9\u7684\u590d\u6742\u6027\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\uff0c\u62d3\u6251\u6570\u636e\u5206\u6790\u9879\u5728\u6cdb\u5316\u754c\u4e2d\u8d77\u5173\u952e\u4f5c\u7528\uff0c\u5c24\u5176\u5728\u8bad\u7ec3\u6837\u672c\u589e\u52a0\u65f6\u3002", "conclusion": "\u65b0\u6846\u67b6\u63d0\u4f9b\u4e86\u66f4\u76f4\u89c2\u7684\u62d3\u6251\u6cdb\u5316\u754c\uff0c\u89e3\u91ca\u4e86\u5176\u5728\u5b9e\u9645\u4e2d\u7684\u6210\u529f\u5e94\u7528\u3002"}}
{"id": "2507.06733", "pdf": "https://arxiv.org/pdf/2507.06733", "abs": "https://arxiv.org/abs/2507.06733", "authors": ["Mahshid Shiri", "Cigdem Beyan", "Vittorio Murino"], "title": "MADPOT: Medical Anomaly Detection with CLIP Adaptation and Partial Optimal Transport", "categories": ["cs.CV"], "comment": "Accepted to ICIAP 2025 (this version is not peer-reviewed; it is the\n  submitted version). ICIAP 2025 proceedings DOI will appear here", "summary": "Medical anomaly detection (AD) is challenging due to diverse imaging\nmodalities, anatomical variations, and limited labeled data. We propose a novel\napproach combining visual adapters and prompt learning with Partial Optimal\nTransport (POT) and contrastive learning (CL) to improve CLIP's adaptability to\nmedical images, particularly for AD. Unlike standard prompt learning, which\noften yields a single representation, our method employs multiple prompts\naligned with local features via POT to capture subtle abnormalities. CL further\nenforces intra-class cohesion and inter-class separation. Our method achieves\nstate-of-the-art results in few-shot, zero-shot, and cross-dataset scenarios\nwithout synthetic data or memory banks. The code is available at\nhttps://github.com/mahshid1998/MADPOT.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u89c6\u89c9\u9002\u914d\u5668\u3001\u63d0\u793a\u5b66\u4e60\u548c\u90e8\u5206\u6700\u4f18\u4f20\u8f93\uff08POT\uff09\u4e0e\u5bf9\u6bd4\u5b66\u4e60\uff08CL\uff09\u7684\u65b0\u65b9\u6cd5\uff0c\u7528\u4e8e\u63d0\u5347CLIP\u5728\u533b\u5b66\u56fe\u50cf\u5f02\u5e38\u68c0\u6d4b\u4e2d\u7684\u9002\u5e94\u6027\u3002", "motivation": "\u533b\u5b66\u5f02\u5e38\u68c0\u6d4b\u9762\u4e34\u6210\u50cf\u6a21\u6001\u591a\u6837\u3001\u89e3\u5256\u53d8\u5f02\u5927\u548c\u6807\u8bb0\u6570\u636e\u6709\u9650\u7b49\u6311\u6218\uff0c\u9700\u8981\u66f4\u7075\u6d3b\u7684\u65b9\u6cd5\u3002", "method": "\u91c7\u7528\u591a\u63d0\u793a\u5b66\u4e60\u4e0e\u5c40\u90e8\u7279\u5f81\u5bf9\u9f50\uff08\u901a\u8fc7POT\uff09\uff0c\u7ed3\u5408\u5bf9\u6bd4\u5b66\u4e60\u589e\u5f3a\u7c7b\u5185\u51dd\u805a\u548c\u7c7b\u95f4\u5206\u79bb\u3002", "result": "\u5728\u5c11\u6837\u672c\u3001\u96f6\u6837\u672c\u548c\u8de8\u6570\u636e\u96c6\u573a\u666f\u4e2d\u53d6\u5f97\u6700\u4f18\u7ed3\u679c\uff0c\u65e0\u9700\u5408\u6210\u6570\u636e\u6216\u8bb0\u5fc6\u5e93\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u533b\u5b66\u56fe\u50cf\u5f02\u5e38\u68c0\u6d4b\u7684\u6027\u80fd\uff0c\u4ee3\u7801\u5df2\u5f00\u6e90\u3002"}}
{"id": "2507.06479", "pdf": "https://arxiv.org/pdf/2507.06479", "abs": "https://arxiv.org/abs/2507.06479", "authors": ["Niloofar Asefi", "Leonard Lupin-Jimenez", "Tianning Wu", "Ruoying He", "Ashesh Chattopadhyay"], "title": "Generative Lagrangian data assimilation for ocean dynamics under extreme sparsity", "categories": ["physics.ao-ph", "cs.AI", "cs.LG", "math.DS", "nlin.CD"], "comment": null, "summary": "Reconstructing ocean dynamics from observational data is fundamentally\nlimited by the sparse, irregular, and Lagrangian nature of spatial sampling,\nparticularly in subsurface and remote regions. This sparsity poses significant\nchallenges for forecasting key phenomena such as eddy shedding and rogue waves.\nTraditional data assimilation methods and deep learning models often struggle\nto recover mesoscale turbulence under such constraints. We leverage a deep\nlearning framework that combines neural operators with denoising diffusion\nprobabilistic models (DDPMs) to reconstruct high-resolution ocean states from\nextremely sparse Lagrangian observations. By conditioning the generative model\non neural operator outputs, the framework accurately captures small-scale,\nhigh-wavenumber dynamics even at $99\\%$ sparsity (for synthetic data) and\n$99.9\\%$ sparsity (for real satellite observations). We validate our method on\nbenchmark systems, synthetic float observations, and real satellite data,\ndemonstrating robust performance under severe spatial sampling limitations as\ncompared to other deep learning baselines.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u795e\u7ecf\u7b97\u5b50\u548c\u53bb\u566a\u6269\u6563\u6982\u7387\u6a21\u578b\uff08DDPMs\uff09\u7684\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\uff0c\u7528\u4e8e\u4ece\u6781\u7a00\u758f\u7684\u62c9\u683c\u6717\u65e5\u89c2\u6d4b\u6570\u636e\u4e2d\u91cd\u5efa\u9ad8\u5206\u8fa8\u7387\u6d77\u6d0b\u72b6\u6001\u3002", "motivation": "\u6d77\u6d0b\u89c2\u6d4b\u6570\u636e\u7684\u7a00\u758f\u6027\u548c\u4e0d\u89c4\u5219\u6027\u9650\u5236\u4e86\u6d77\u6d0b\u52a8\u529b\u5b66\u7684\u91cd\u5efa\uff0c\u4f20\u7edf\u65b9\u6cd5\u96be\u4ee5\u6062\u590d\u4e2d\u5c3a\u5ea6\u6e4d\u6d41\u73b0\u8c61\u3002", "method": "\u901a\u8fc7\u5c06\u751f\u6210\u6a21\u578b\u4e0e\u795e\u7ecf\u7b97\u5b50\u8f93\u51fa\u7ed3\u5408\uff0c\u6846\u67b6\u80fd\u591f\u5728\u5c0f\u5c3a\u5ea6\u3001\u9ad8\u6ce2\u6570\u52a8\u6001\u4e0b\u5b9e\u73b0\u9ad8\u7cbe\u5ea6\u91cd\u5efa\u3002", "result": "\u5728\u5408\u6210\u6570\u636e\u548c\u771f\u5b9e\u536b\u661f\u89c2\u6d4b\u4e2d\uff0c\u8be5\u65b9\u6cd5\u572899%\u548c99.9%\u7684\u7a00\u758f\u5ea6\u4e0b\u4ecd\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u4e25\u91cd\u7a7a\u95f4\u91c7\u6837\u9650\u5236\u4e0b\u4f18\u4e8e\u5176\u4ed6\u6df1\u5ea6\u5b66\u4e60\u57fa\u7ebf\uff0c\u5177\u6709\u9c81\u68d2\u6027\u3002"}}
{"id": "2507.06802", "pdf": "https://arxiv.org/pdf/2507.06802", "abs": "https://arxiv.org/abs/2507.06802", "authors": ["Wonjin Jung", "Sungil Kang", "Dong-Yeon Cho"], "title": "Speech Tokenizer is Key to Consistent Representation", "categories": ["cs.LG"], "comment": null, "summary": "Speech tokenization is crucial in digital speech processing, converting\ncontinuous speech signals into discrete units for various computational tasks.\nThis paper introduces a novel speech tokenizer with broad applicability across\ndownstream tasks. While recent advances in residual vector quantization (RVQ)\nhave incorporated semantic elements, they often neglect critical acoustic\nfeatures. We propose an advanced approach that simultaneously encodes both\nlinguistic and acoustic information, preserving prosodic and emotional content.\nOur method significantly enhances speech representation fidelity across diverse\napplications. Empirical evaluations demonstrate its effectiveness in speech\ncoding, voice conversion, emotion recognition, and multimodal language\nmodeling, without requiring additional training. This versatility underscores\nits potential as a key tool for advancing AI-driven speech processing.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u65b0\u578b\u8bed\u97f3\u5206\u8bcd\u5668\uff0c\u540c\u65f6\u7f16\u7801\u8bed\u8a00\u548c\u58f0\u5b66\u4fe1\u606f\uff0c\u63d0\u5347\u8bed\u97f3\u8868\u793a\u4fdd\u771f\u5ea6\uff0c\u9002\u7528\u4e8e\u591a\u79cd\u4e0b\u6e38\u4efb\u52a1\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8eRVQ\u7684\u5206\u8bcd\u5668\u5e38\u5ffd\u7565\u58f0\u5b66\u7279\u5f81\uff0c\u65e0\u6cd5\u4fdd\u7559\u97f5\u5f8b\u548c\u60c5\u611f\u5185\u5bb9\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u540c\u65f6\u7f16\u7801\u8bed\u8a00\u548c\u58f0\u5b66\u4fe1\u606f\u7684\u5148\u8fdb\u65b9\u6cd5\u3002", "result": "\u5b9e\u8bc1\u8bc4\u4f30\u663e\u793a\uff0c\u8be5\u65b9\u6cd5\u5728\u8bed\u97f3\u7f16\u7801\u3001\u8bed\u97f3\u8f6c\u6362\u3001\u60c5\u611f\u8bc6\u522b\u548c\u591a\u6a21\u6001\u8bed\u8a00\u5efa\u6a21\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u65e0\u9700\u989d\u5916\u8bad\u7ec3\uff0c\u5177\u6709\u5e7f\u6cdb\u9002\u7528\u6027\uff0c\u662f\u63a8\u52a8AI\u8bed\u97f3\u5904\u7406\u7684\u5173\u952e\u5de5\u5177\u3002"}}
{"id": "2507.06738", "pdf": "https://arxiv.org/pdf/2507.06738", "abs": "https://arxiv.org/abs/2507.06738", "authors": ["Xinyu Xie", "Weifeng Cao", "Jun Shi", "Yangyang Hu", "Hui Liang", "Wanyong Liang", "Xiaoliang Qian"], "title": "DIFFUMA: High-Fidelity Spatio-Temporal Video Prediction via Dual-Path Mamba and Diffusion Enhancement", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Spatio-temporal video prediction plays a pivotal role in critical domains,\nranging from weather forecasting to industrial automation. However, in\nhigh-precision industrial scenarios such as semiconductor manufacturing, the\nabsence of specialized benchmark datasets severely hampers research on modeling\nand predicting complex processes. To address this challenge, we make a twofold\ncontribution.First, we construct and release the Chip Dicing Lane Dataset\n(CHDL), the first public temporal image dataset dedicated to the semiconductor\nwafer dicing process. Captured via an industrial-grade vision system, CHDL\nprovides a much-needed and challenging benchmark for high-fidelity process\nmodeling, defect detection, and digital twin development.Second, we propose\nDIFFUMA, an innovative dual-path prediction architecture specifically designed\nfor such fine-grained dynamics. The model captures global long-range temporal\ncontext through a parallel Mamba module, while simultaneously leveraging a\ndiffusion module, guided by temporal features, to restore and enhance\nfine-grained spatial details, effectively combating feature degradation.\nExperiments demonstrate that on our CHDL benchmark, DIFFUMA significantly\noutperforms existing methods, reducing the Mean Squared Error (MSE) by 39% and\nimproving the Structural Similarity (SSIM) from 0.926 to a near-perfect 0.988.\nThis superior performance also generalizes to natural phenomena datasets. Our\nwork not only delivers a new state-of-the-art (SOTA) model but, more\nimportantly, provides the community with an invaluable data resource to drive\nfuture research in industrial AI.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u9996\u4e2a\u534a\u5bfc\u4f53\u6676\u5706\u5207\u5272\u8fc7\u7a0b\u7684\u516c\u5f00\u6570\u636e\u96c6CHDL\uff0c\u5e76\u8bbe\u8ba1\u4e86\u4e00\u79cd\u53cc\u8def\u5f84\u9884\u6d4b\u67b6\u6784DIFFUMA\uff0c\u663e\u8457\u63d0\u5347\u4e86\u9884\u6d4b\u6027\u80fd\u3002", "motivation": "\u5728\u9ad8\u7cbe\u5ea6\u5de5\u4e1a\u573a\u666f\uff08\u5982\u534a\u5bfc\u4f53\u5236\u9020\uff09\u4e2d\uff0c\u7f3a\u4e4f\u4e13\u7528\u6570\u636e\u96c6\u963b\u788d\u4e86\u590d\u6742\u8fc7\u7a0b\u5efa\u6a21\u548c\u9884\u6d4b\u7684\u7814\u7a76\u3002", "method": "\u6784\u5efaCHDL\u6570\u636e\u96c6\uff0c\u5e76\u63d0\u51faDIFFUMA\u6a21\u578b\uff0c\u7ed3\u5408Mamba\u6a21\u5757\u548c\u6269\u6563\u6a21\u5757\uff0c\u5206\u522b\u6355\u83b7\u5168\u5c40\u65f6\u95f4\u4e0a\u4e0b\u6587\u548c\u589e\u5f3a\u7a7a\u95f4\u7ec6\u8282\u3002", "result": "DIFFUMA\u5728CHDL\u6570\u636e\u96c6\u4e0aMSE\u964d\u4f4e39%\uff0cSSIM\u4ece0.926\u63d0\u5347\u81f30.988\uff0c\u5e76\u5728\u81ea\u7136\u73b0\u8c61\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u4e0d\u4ec5\u63d0\u4f9b\u4e86\u65b0\u7684SOTA\u6a21\u578b\uff0c\u8fd8\u4e3a\u5de5\u4e1aAI\u7814\u7a76\u63d0\u4f9b\u4e86\u5b9d\u8d35\u7684\u6570\u636e\u8d44\u6e90\u3002"}}
{"id": "2507.06813", "pdf": "https://arxiv.org/pdf/2507.06813", "abs": "https://arxiv.org/abs/2507.06813", "authors": ["Cosimo Fiorini", "Matteo Mosconi", "Pietro Buzzega", "Riccardo Salami", "Simone Calderara"], "title": "Intrinsic Training Signals for Federated Learning Aggregation", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Federated Learning (FL) enables collaborative model training across\ndistributed clients while preserving data privacy. While existing approaches\nfor aggregating client-specific classification heads and adapted backbone\nparameters require architectural modifications or loss function changes, our\nmethod uniquely leverages intrinsic training signals already available during\nstandard optimization. We present LIVAR (Layer Importance and VARiance-based\nmerging), which introduces: i) a variance-weighted classifier aggregation\nscheme using naturally emergent feature statistics, and ii) an\nexplainability-driven LoRA merging technique based on SHAP analysis of existing\nupdate parameter patterns. Without any architectural overhead, LIVAR achieves\nstate-of-the-art performance on multiple benchmarks while maintaining seamless\nintegration with existing FL methods. This work demonstrates that effective\nmodel merging can be achieved solely through existing training signals,\nestablishing a new paradigm for efficient federated model aggregation. The code\nwill be made publicly available upon acceptance.", "AI": {"tldr": "LIVAR\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u9700\u67b6\u6784\u4fee\u6539\u6216\u635f\u5931\u51fd\u6570\u53d8\u5316\u7684\u8054\u90a6\u5b66\u4e60\u65b9\u6cd5\uff0c\u901a\u8fc7\u5229\u7528\u8bad\u7ec3\u4fe1\u53f7\u5b9e\u73b0\u9ad8\u6548\u6a21\u578b\u805a\u5408\u3002", "motivation": "\u73b0\u6709\u8054\u90a6\u5b66\u4e60\u65b9\u6cd5\u5728\u805a\u5408\u5ba2\u6237\u7aef\u5206\u7c7b\u5934\u548c\u8c03\u6574\u4e3b\u5e72\u53c2\u6570\u65f6\u9700\u8981\u4fee\u6539\u67b6\u6784\u6216\u635f\u5931\u51fd\u6570\uff0cLIVAR\u65e8\u5728\u5229\u7528\u73b0\u6709\u8bad\u7ec3\u4fe1\u53f7\u5b9e\u73b0\u9ad8\u6548\u805a\u5408\u3002", "method": "LIVAR\u91c7\u7528\u65b9\u5dee\u52a0\u6743\u7684\u5206\u7c7b\u5668\u805a\u5408\u65b9\u6848\u548c\u57fa\u4e8eSHAP\u5206\u6790\u7684LoRA\u5408\u5e76\u6280\u672f\u3002", "result": "LIVAR\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230\u6700\u4f18\u6027\u80fd\uff0c\u4e14\u4e0e\u73b0\u6709FL\u65b9\u6cd5\u65e0\u7f1d\u96c6\u6210\u3002", "conclusion": "LIVAR\u8bc1\u660e\u4ec5\u901a\u8fc7\u73b0\u6709\u8bad\u7ec3\u4fe1\u53f7\u5373\u53ef\u5b9e\u73b0\u9ad8\u6548\u6a21\u578b\u805a\u5408\uff0c\u4e3a\u8054\u90a6\u5b66\u4e60\u63d0\u4f9b\u4e86\u65b0\u8303\u5f0f\u3002"}}
{"id": "2507.06739", "pdf": "https://arxiv.org/pdf/2507.06739", "abs": "https://arxiv.org/abs/2507.06739", "authors": ["Zishen Huang", "Chunyu Yang", "Mengyuan Ren"], "title": "PromptTea: Let Prompts Tell TeaCache the Optimal Threshold", "categories": ["cs.CV"], "comment": null, "summary": "Despite recent progress in video generation, inference speed remains a major\nbottleneck. A common acceleration strategy involves reusing model outputs via\ncaching mechanisms at fixed intervals. However, we find that such\nfixed-frequency reuse significantly degrades quality in complex scenes, while\nmanually tuning reuse thresholds is inefficient and lacks robustness. To\naddress this, we propose Prompt-Complexity-Aware (PCA) caching, a method that\nautomatically adjusts reuse thresholds based on scene complexity estimated\ndirectly from the input prompt. By incorporating prompt-derived semantic cues,\nPCA enables more adaptive and informed reuse decisions than conventional\ncaching methods. We also revisit the assumptions behind TeaCache and identify a\nkey limitation: it suffers from poor input-output relationship modeling due to\nan oversimplified prior. To overcome this, we decouple the noisy input, enhance\nthe contribution of meaningful textual information, and improve the model's\npredictive accuracy through multivariate polynomial feature expansion. To\nfurther reduce computational cost, we replace the static CFGCache with\nDynCFGCache, a dynamic mechanism that selectively reuses classifier-free\nguidance (CFG) outputs based on estimated output variations. This allows for\nmore flexible reuse without compromising output quality. Extensive experiments\ndemonstrate that our approach achieves significant acceleration-for example,\n2.79x speedup on the Wan2.1 model-while maintaining high visual fidelity across\na range of scenes.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u63d0\u793a\u590d\u6742\u5ea6\u7684\u81ea\u9002\u5e94\u7f13\u5b58\u65b9\u6cd5\uff08PCA\u7f13\u5b58\uff09\uff0c\u901a\u8fc7\u52a8\u6001\u8c03\u6574\u91cd\u7528\u9608\u503c\u548c\u4f18\u5316\u8f93\u5165\u8f93\u51fa\u5efa\u6a21\uff0c\u663e\u8457\u63d0\u5347\u4e86\u89c6\u9891\u751f\u6210\u7684\u63a8\u7406\u901f\u5ea6\uff0c\u540c\u65f6\u4fdd\u6301\u9ad8\u8d28\u91cf\u8f93\u51fa\u3002", "motivation": "\u56fa\u5b9a\u9891\u7387\u7684\u7f13\u5b58\u673a\u5236\u5728\u590d\u6742\u573a\u666f\u4e2d\u4f1a\u5bfc\u81f4\u8d28\u91cf\u4e0b\u964d\uff0c\u624b\u52a8\u8c03\u6574\u9608\u503c\u6548\u7387\u4f4e\u4e14\u4e0d\u9c81\u68d2\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u66f4\u667a\u80fd\u7684\u81ea\u9002\u5e94\u65b9\u6cd5\u3002", "method": "\u63d0\u51faPCA\u7f13\u5b58\uff0c\u6839\u636e\u8f93\u5165\u63d0\u793a\u7684\u8bed\u4e49\u590d\u6742\u5ea6\u52a8\u6001\u8c03\u6574\u91cd\u7528\u9608\u503c\uff1b\u6539\u8fdbTeaCache\u7684\u8f93\u5165\u8f93\u51fa\u5efa\u6a21\uff0c\u589e\u5f3a\u6587\u672c\u4fe1\u606f\u8d21\u732e\uff1b\u5f15\u5165\u52a8\u6001CFGCache\u673a\u5236\uff0c\u9009\u62e9\u6027\u91cd\u7528CFG\u8f93\u51fa\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728Wan2.1\u6a21\u578b\u4e0a\u5b9e\u73b0\u4e862.79\u500d\u7684\u52a0\u901f\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u9ad8\u89c6\u89c9\u4fdd\u771f\u5ea6\u3002", "conclusion": "PCA\u7f13\u5b58\u548c\u52a8\u6001CFGCache\u7684\u7ed3\u5408\u663e\u8457\u63d0\u5347\u4e86\u89c6\u9891\u751f\u6210\u7684\u6548\u7387\u548c\u9002\u5e94\u6027\uff0c\u4e3a\u590d\u6742\u573a\u666f\u4e0b\u7684\u5b9e\u65f6\u751f\u6210\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.06819", "pdf": "https://arxiv.org/pdf/2507.06819", "abs": "https://arxiv.org/abs/2507.06819", "authors": ["Philipp Schlinge", "Steffen Meinert", "Martin Atzmueller"], "title": "Comprehensive Evaluation of Prototype Neural Networks", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Prototype models are an important method for explainable artificial\nintelligence (XAI) and interpretable machine learning. In this paper, we\nperform an in-depth analysis of a set of prominent prototype models including\nProtoPNet, ProtoPool and PIPNet. For their assessment, we apply a comprehensive\nset of metrics. In addition to applying standard metrics from literature, we\npropose several new metrics to further complement the analysis of model\ninterpretability. In our experimentation, we apply the set of prototype models\non a diverse set of datasets including fine-grained classification, Non-IID\nsettings and multi-label classification to further contrast the performance.\nFurthermore, we also provide our code as an open-source library, which\nfacilitates simple application of the metrics itself, as well as extensibility\n- providing the option for easily adding new metrics and models.\nhttps://github.com/uos-sis/quanproto", "AI": {"tldr": "\u672c\u6587\u5bf9\u539f\u578b\u6a21\u578b\uff08\u5982ProtoPNet\u3001ProtoPool\u548cPIPNet\uff09\u8fdb\u884c\u4e86\u6df1\u5165\u5206\u6790\uff0c\u63d0\u51fa\u4e86\u4e00\u5957\u5168\u9762\u7684\u8bc4\u4f30\u6307\u6807\uff0c\u5305\u62ec\u65b0\u63d0\u51fa\u7684\u6307\u6807\uff0c\u4ee5\u8865\u5145\u6a21\u578b\u53ef\u89e3\u91ca\u6027\u5206\u6790\u3002\u5b9e\u9a8c\u6db5\u76d6\u4e86\u591a\u79cd\u6570\u636e\u96c6\uff0c\u5e76\u63d0\u4f9b\u4e86\u5f00\u6e90\u4ee3\u7801\u5e93\u3002", "motivation": "\u539f\u578b\u6a21\u578b\u662f\u53ef\u89e3\u91ca\u4eba\u5de5\u667a\u80fd\uff08XAI\uff09\u548c\u53ef\u89e3\u91ca\u673a\u5668\u5b66\u4e60\u7684\u91cd\u8981\u65b9\u6cd5\uff0c\u4f46\u7f3a\u4e4f\u5168\u9762\u7684\u8bc4\u4f30\u6307\u6807\u548c\u5de5\u5177\u3002\u672c\u6587\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u4f7f\u7528ProtoPNet\u3001ProtoPool\u548cPIPNet\u7b49\u539f\u578b\u6a21\u578b\uff0c\u7ed3\u5408\u6807\u51c6\u548c\u65b0\u63d0\u51fa\u7684\u6307\u6807\uff0c\u5728\u591a\u6837\u5316\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u8bc4\u4f30\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u539f\u578b\u6a21\u578b\u5728\u591a\u79cd\u4efb\u52a1\uff08\u5982\u7ec6\u7c92\u5ea6\u5206\u7c7b\u3001\u975e\u72ec\u7acb\u540c\u5206\u5e03\u8bbe\u7f6e\u548c\u591a\u6807\u7b7e\u5206\u7c7b\uff09\u4e2d\u8868\u73b0\u826f\u597d\u3002", "conclusion": "\u672c\u6587\u63d0\u4f9b\u4e86\u4e00\u5957\u5168\u9762\u7684\u8bc4\u4f30\u6307\u6807\u548c\u5f00\u6e90\u5de5\u5177\uff0c\u4e3a\u539f\u578b\u6a21\u578b\u7684\u53ef\u89e3\u91ca\u6027\u7814\u7a76\u63d0\u4f9b\u4e86\u4fbf\u5229\u548c\u6269\u5c55\u6027\u3002"}}
{"id": "2507.06483", "pdf": "https://arxiv.org/pdf/2507.06483", "abs": "https://arxiv.org/abs/2507.06483", "authors": ["Zackary Rackauckas", "Julia Hirschberg"], "title": "Learning Japanese with Jouzu: Interaction Outcomes with Stylized Dialogue Fictional Agents", "categories": ["cs.HC", "cs.CL"], "comment": null, "summary": "This study investigates how stylized, voiced agents shape user interaction in\na multimodal language learning environment. We conducted a mixed-methods\nevaluation of 54 participants interacting with anime-inspired characters\npowered by large language models and expressive text-to-speech synthesis. These\nagents responded in Japanese character language, offering users asynchronous,\nsemi-structured conversation in varying speech styles and emotional tones. We\nanalyzed user engagement patterns, perceived usability, emotional responses,\nand learning behaviors, with particular attention to how agent stylization\ninfluenced interaction across language proficiency levels and cultural\nbackgrounds. Our findings reveal that agent design, especially voice, persona,\nand linguistic style, substantially affected user experience, motivation, and\nstrategy. This work contributes to the understanding of affective, culturally\nstylized agents in human-agent interaction and offers guidance for designing\nmore engaging, socially responsive systems.", "AI": {"tldr": "\u7814\u7a76\u63a2\u8ba8\u4e86\u52a8\u6f2b\u98ce\u683c\u8bed\u97f3\u4ee3\u7406\u5728\u591a\u6a21\u6001\u8bed\u8a00\u5b66\u4e60\u73af\u5883\u4e2d\u7684\u7528\u6237\u4ea4\u4e92\u5f71\u54cd\uff0c\u53d1\u73b0\u4ee3\u7406\u8bbe\u8ba1\u663e\u8457\u5f71\u54cd\u7528\u6237\u4f53\u9a8c\u548c\u5b66\u4e60\u52a8\u673a\u3002", "motivation": "\u63a2\u7d22\u98ce\u683c\u5316\u8bed\u97f3\u4ee3\u7406\u5982\u4f55\u901a\u8fc7\u4e0d\u540c\u8bed\u97f3\u98ce\u683c\u548c\u60c5\u611f\u8bed\u8c03\u63d0\u5347\u8bed\u8a00\u5b66\u4e60\u4ea4\u4e92\u6548\u679c\u3002", "method": "\u91c7\u7528\u6df7\u5408\u65b9\u6cd5\u8bc4\u4f3054\u540d\u53c2\u4e0e\u8005\u4e0e\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u548c\u6587\u672c\u8f6c\u8bed\u97f3\u5408\u6210\u7684\u52a8\u6f2b\u98ce\u683c\u4ee3\u7406\u7684\u4ea4\u4e92\u3002", "result": "\u4ee3\u7406\u7684\u58f0\u97f3\u3001\u4eba\u8bbe\u548c\u8bed\u8a00\u98ce\u683c\u663e\u8457\u5f71\u54cd\u7528\u6237\u53c2\u4e0e\u5ea6\u3001\u60c5\u611f\u53cd\u5e94\u548c\u5b66\u4e60\u7b56\u7565\u3002", "conclusion": "\u7814\u7a76\u4e3a\u8bbe\u8ba1\u66f4\u5177\u5438\u5f15\u529b\u548c\u793e\u4ea4\u54cd\u5e94\u6027\u7684\u4ee3\u7406\u7cfb\u7edf\u63d0\u4f9b\u4e86\u6307\u5bfc\u3002"}}
{"id": "2507.06761", "pdf": "https://arxiv.org/pdf/2507.06761", "abs": "https://arxiv.org/abs/2507.06761", "authors": ["Yan Hon Michael Chung", "Donghyeok Choi"], "title": "Finetuning Vision-Language Models as OCR Systems for Low-Resource Languages: A Case Study of Manchu", "categories": ["cs.CV"], "comment": null, "summary": "Manchu, a critically endangered language essential for understanding early\nmodern Eastern Eurasian history, lacks effective OCR systems that can handle\nreal-world historical documents. This study develops high-performing OCR\nsystems by fine-tuning three open-source vision-language models (LLaMA-3.2-11B,\nQwen2.5-VL-7B, Qwen2.5-VL-3B) on 60,000 synthetic Manchu word images using\nparameter-efficient training. LLaMA-3.2-11B achieved exceptional performance\nwith 98.3\\% word accuracy and 0.0024 character error rate on synthetic data,\nwhile crucially maintaining 93.1\\% accuracy on real-world handwritten\ndocuments. Comparative evaluation reveals substantial advantages over\ntraditional approaches: while a CRNN baseline achieved 99.8\\% synthetic\naccuracy, it suffered severe degradation to 72.5\\% on real documents. Our\napproach demonstrates effective synthetic-to-real domain transfer, providing a\ncost-effective solution deployable on accessible infrastructure. This work\nestablishes a transferable framework for endangered language OCR that removes\ntechnical and financial barriers in digital humanities, enabling historians and\nlinguists to process historical archives without specialized computing\nresources. Code and model weights are available at\nhttps://github.com/mic7ch1/ManchuAI-OCR.", "AI": {"tldr": "\u8be5\u7814\u7a76\u901a\u8fc7\u5fae\u8c03\u4e09\u79cd\u5f00\u6e90\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff0c\u5f00\u53d1\u4e86\u9ad8\u6027\u80fd\u7684\u6ee1\u6587OCR\u7cfb\u7edf\uff0c\u663e\u8457\u63d0\u5347\u4e86\u771f\u5b9e\u5386\u53f2\u6587\u6863\u7684\u8bc6\u522b\u51c6\u786e\u7387\u3002", "motivation": "\u6ee1\u6587\u4f5c\u4e3a\u6fd2\u5371\u8bed\u8a00\u5bf9\u7814\u7a76\u65e9\u671f\u73b0\u4ee3\u4e1c\u4e9a\u5386\u53f2\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u7f3a\u4e4f\u6709\u6548\u7684OCR\u7cfb\u7edf\u5904\u7406\u771f\u5b9e\u5386\u53f2\u6587\u6863\u3002", "method": "\u4f7f\u752860,000\u5f20\u5408\u6210\u6ee1\u6587\u5355\u8bcd\u56fe\u50cf\u5bf9\u4e09\u79cd\u6a21\u578b\uff08LLaMA-3.2-11B\u3001Qwen2.5-VL-7B\u3001Qwen2.5-VL-3B\uff09\u8fdb\u884c\u53c2\u6570\u9ad8\u6548\u8bad\u7ec3\u3002", "result": "LLaMA-3.2-11B\u5728\u5408\u6210\u6570\u636e\u4e0a\u8fbe\u523098.3%\u7684\u5355\u8bcd\u51c6\u786e\u7387\u548c0.0024\u7684\u5b57\u7b26\u9519\u8bef\u7387\uff0c\u5728\u771f\u5b9e\u624b\u5199\u6587\u6863\u4e0a\u4fdd\u630193.1%\u7684\u51c6\u786e\u7387\uff0c\u663e\u8457\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u6fd2\u5371\u8bed\u8a00OCR\u63d0\u4f9b\u4e86\u53ef\u8f6c\u79fb\u7684\u6846\u67b6\uff0c\u964d\u4f4e\u4e86\u6280\u672f\u548c\u8d22\u52a1\u95e8\u69db\uff0c\u52a9\u529b\u5386\u53f2\u5b66\u5bb6\u548c\u8bed\u8a00\u5b66\u5bb6\u5904\u7406\u5386\u53f2\u6863\u6848\u3002"}}
{"id": "2507.06507", "pdf": "https://arxiv.org/pdf/2507.06507", "abs": "https://arxiv.org/abs/2507.06507", "authors": ["Zhen Yang", "Haitao Lin", "Jiawei xue", "Ziji Zhang"], "title": "GR-LLMs: Recent Advances in Generative Recommendation Based on Large Language Models", "categories": ["cs.IR", "cs.AI"], "comment": "8 pages, 3 figures", "summary": "In the past year, Generative Recommendations (GRs) have undergone substantial\nadvancements, especially in leveraging the powerful sequence modeling and\nreasoning capabilities of Large Language Models (LLMs) to enhance overall\nrecommendation performance. LLM-based GRs are forming a new paradigm that is\ndistinctly different from discriminative recommendations, showing strong\npotential to replace traditional recommendation systems heavily dependent on\ncomplex hand-crafted features. In this paper, we provide a comprehensive survey\naimed at facilitating further research of LLM-based GRs. Initially, we outline\nthe general preliminaries and application cases of LLM-based GRs. Subsequently,\nwe introduce the main considerations when LLM-based GRs are applied in real\nindustrial scenarios. Finally, we explore promising directions for LLM-based\nGRs. We hope that this survey contributes to the ongoing advancement of the GR\ndomain.", "AI": {"tldr": "\u7efc\u8ff0\u63a2\u8ba8\u4e86\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u751f\u6210\u5f0f\u63a8\u8350\uff08GR\uff09\u7684\u8fdb\u5c55\u3001\u5e94\u7528\u573a\u666f\u3001\u5de5\u4e1a\u5b9e\u8df5\u53ca\u672a\u6765\u65b9\u5411\u3002", "motivation": "\u63a8\u52a8\u57fa\u4e8eLLM\u7684GR\u7814\u7a76\uff0c\u4ee5\u66ff\u4ee3\u4f9d\u8d56\u590d\u6742\u624b\u5de5\u7279\u5f81\u7684\u4f20\u7edf\u63a8\u8350\u7cfb\u7edf\u3002", "method": "\u6982\u8ff0LLM-based GR\u7684\u57fa\u7840\u77e5\u8bc6\u3001\u5e94\u7528\u6848\u4f8b\u3001\u5de5\u4e1a\u5b9e\u8df5\u8003\u8651\u53ca\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002", "result": "LLM-based GR\u5c55\u73b0\u51fa\u66ff\u4ee3\u4f20\u7edf\u63a8\u8350\u7cfb\u7edf\u7684\u6f5c\u529b\uff0c\u5e76\u5f62\u6210\u65b0\u8303\u5f0f\u3002", "conclusion": "\u672c\u7efc\u8ff0\u65e8\u5728\u4fc3\u8fdbGR\u9886\u57df\u7684\u8fdb\u4e00\u6b65\u53d1\u5c55\u3002"}}
{"id": "2507.06825", "pdf": "https://arxiv.org/pdf/2507.06825", "abs": "https://arxiv.org/abs/2507.06825", "authors": ["Matej Straka", "Martin Schmid"], "title": "Artificial Generals Intelligence: Mastering Generals.io with Reinforcement Learning", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "We introduce a real-time strategy game environment built on Generals.io, a\ngame that hosts thousands of active players each week across multiple game\nformats. Our environment is fully compatible with Gymnasium and PettingZoo,\ncapable of running thousands of frames per second on commodity hardware. Our\nreference agent -- trained with supervised pre-training and self-play -- hits\nthe top 0.003\\% of the 1v1 human leaderboard after just 36 hours on a single\nH100 GPU. To accelerate learning, we incorporate potential-based reward shaping\nand memory features. Our contributions -- a modular RTS benchmark and a\ncompetitive, state-of-the-art baseline agent -- provide an accessible yet\nchallenging platform for advancing multi-agent reinforcement learning research.", "AI": {"tldr": "\u4ecb\u7ecd\u4e86\u4e00\u4e2a\u57fa\u4e8eGenerals.io\u7684\u5b9e\u65f6\u7b56\u7565\u6e38\u620f\u73af\u5883\uff0c\u517c\u5bb9Gymnasium\u548cPettingZoo\uff0c\u652f\u6301\u9ad8\u6027\u80fd\u8fd0\u884c\uff0c\u5e76\u8bad\u7ec3\u4e86\u4e00\u4e2a\u9876\u7ea7\u6c34\u5e73\u7684\u53c2\u8003\u667a\u80fd\u4f53\u3002", "motivation": "\u4e3a\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u7814\u7a76\u63d0\u4f9b\u4e00\u4e2a\u6613\u7528\u4e14\u5177\u6709\u6311\u6218\u6027\u7684\u5e73\u53f0\u3002", "method": "\u7ed3\u5408\u76d1\u7763\u9884\u8bad\u7ec3\u548c\u81ea\u5bf9\u5f08\u8bad\u7ec3\u667a\u80fd\u4f53\uff0c\u5e76\u91c7\u7528\u57fa\u4e8e\u6f5c\u529b\u7684\u5956\u52b1\u5851\u9020\u548c\u8bb0\u5fc6\u7279\u5f81\u52a0\u901f\u5b66\u4e60\u3002", "result": "\u667a\u80fd\u4f53\u572836\u5c0f\u65f6\u5185\u8fbe\u52301v1\u4eba\u7c7b\u6392\u884c\u699c\u524d0.003%\u6c34\u5e73\u3002", "conclusion": "\u8be5\u73af\u5883\u548c\u667a\u80fd\u4f53\u4e3a\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u7814\u7a76\u63d0\u4f9b\u4e86\u9ad8\u6548\u4e14\u7ade\u4e89\u6027\u7684\u57fa\u51c6\u3002"}}
{"id": "2507.06763", "pdf": "https://arxiv.org/pdf/2507.06763", "abs": "https://arxiv.org/abs/2507.06763", "authors": ["Saif Ur Rehman Khan", "Muhammad Nabeel Asim", "Sebastian Vollmer", "Andreas Dengel"], "title": "FOLC-Net: A Federated-Optimized Lightweight Architecture for Enhanced MRI Disease Diagnosis across Axial, Coronal, and Sagittal Views", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "The framework is designed to improve performance in the analysis of combined\nas well as single anatomical perspectives for MRI disease diagnosis. It\nspecifically addresses the performance degradation observed in state-of-the-art\n(SOTA) models, particularly when processing axial, coronal, and sagittal\nanatomical planes. The paper introduces the FOLC-Net framework, which\nincorporates a novel federated-optimized lightweight architecture with\napproximately 1.217 million parameters and a storage requirement of only 0.9\nMB. FOLC-Net integrates Manta-ray foraging optimization (MRFO) mechanisms for\nefficient model structure generation, global model cloning for scalable\ntraining, and ConvNeXt for enhanced client adaptability. The model was\nevaluated on combined multi-view data as well as individual views, such as\naxial, coronal, and sagittal, to assess its robustness in various medical\nimaging scenarios. Moreover, FOLC-Net tests a ShallowFed model on different\ndata to evaluate its ability to generalize beyond the training dataset. The\nresults show that FOLC-Net outperforms existing models, particularly in the\nchallenging sagittal view. For instance, FOLC-Net achieved an accuracy of\n92.44% on the sagittal view, significantly higher than the 88.37% accuracy of\nstudy method (DL + Residual Learning) and 88.95% of DL models. Additionally,\nFOLC-Net demonstrated improved accuracy across all individual views, providing\na more reliable and robust solution for medical image analysis in decentralized\nenvironments. FOLC-Net addresses the limitations of existing SOTA models by\nproviding a framework that ensures better adaptability to individual views\nwhile maintaining strong performance in multi-view settings. The incorporation\nof MRFO, global model cloning, and ConvNeXt ensures that FOLC-Net performs\nbetter in real-world medical applications.", "AI": {"tldr": "FOLC-Net\u6846\u67b6\u901a\u8fc7\u8f7b\u91cf\u7ea7\u67b6\u6784\u548c\u4f18\u5316\u673a\u5236\u63d0\u5347MRI\u591a\u89c6\u89d2\u8bca\u65ad\u6027\u80fd\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u6a21\u578b\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709SOTA\u6a21\u578b\u5728\u5904\u7406MRI\u591a\u89c6\u89d2\uff08\u8f74\u5411\u3001\u51a0\u72b6\u3001\u77e2\u72b6\uff09\u65f6\u6027\u80fd\u4e0b\u964d\u7684\u95ee\u9898\u3002", "method": "\u5f15\u5165FOLC-Net\uff0c\u7ed3\u5408MRFO\u4f18\u5316\u3001\u5168\u5c40\u6a21\u578b\u514b\u9686\u548cConvNeXt\uff0c\u53c2\u6570\u4ec51.217M\uff0c\u5b58\u50a80.9MB\u3002", "result": "\u5728\u77e2\u72b6\u89c6\u89d2\u51c6\u786e\u7387\u8fbe92.44%\uff0c\u4f18\u4e8e\u5176\u4ed6\u6a21\u578b\uff0888.37%\u548c88.95%\uff09\uff0c\u591a\u89c6\u89d2\u8868\u73b0\u7a33\u5065\u3002", "conclusion": "FOLC-Net\u4e3a\u5206\u6563\u5f0f\u533b\u7597\u56fe\u50cf\u5206\u6790\u63d0\u4f9b\u4e86\u66f4\u53ef\u9760\u3001\u9002\u5e94\u6027\u66f4\u5f3a\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.06512", "pdf": "https://arxiv.org/pdf/2507.06512", "abs": "https://arxiv.org/abs/2507.06512", "authors": ["Siyu Qiu", "Muzhi Wang", "Raheel Afsharmazayejani", "Mohammad Moradi Shahmiri", "Benjamin Tan", "Hammond Pearce"], "title": "Towards LLM-based Root Cause Analysis of Hardware Design Failures", "categories": ["cs.AR", "cs.AI"], "comment": "6 pages. Accepted for publication in IEEE COINS 2025 Special Session\n  on LLMs for EDA and Security", "summary": "With advances in large language models (LLMs), new opportunities have emerged\nto develop tools that support the digital hardware design process. In this\nwork, we explore how LLMs can assist with explaining the root cause of design\nissues and bugs that are revealed during synthesis and simulation, a necessary\nmilestone on the pathway towards widespread use of LLMs in the hardware design\nprocess and for hardware security analysis. We find promising results: for our\ncorpus of 34 different buggy scenarios, OpenAI's o3-mini reasoning model\nreached a correct determination 100% of the time under pass@5 scoring, with\nother state of the art models and configurations usually achieving more than\n80% performance and more than 90% when assisted with retrieval-augmented\ngeneration.", "AI": {"tldr": "LLMs\u5728\u6570\u5b57\u786c\u4ef6\u8bbe\u8ba1\u4e2d\u7684\u6f5c\u529b\uff0c\u7279\u522b\u662f\u5728\u89e3\u91ca\u5408\u6210\u548c\u4eff\u771f\u4e2d\u7684\u8bbe\u8ba1\u95ee\u9898\u548c\u9519\u8bef\u6839\u6e90\u65b9\u9762\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u63a2\u7d22LLMs\u5982\u4f55\u8f85\u52a9\u786c\u4ef6\u8bbe\u8ba1\u8fc7\u7a0b\u4e2d\u7684\u95ee\u9898\u8bca\u65ad\uff0c\u63a8\u52a8\u5176\u5728\u786c\u4ef6\u8bbe\u8ba1\u548c\u5b89\u5168\u5206\u6790\u4e2d\u7684\u5e7f\u6cdb\u5e94\u7528\u3002", "method": "\u4f7f\u7528OpenAI\u7684o3-mini\u63a8\u7406\u6a21\u578b\u53ca\u5176\u4ed6\u5148\u8fdb\u6a21\u578b\uff0c\u7ed3\u5408\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u6280\u672f\uff0c\u6d4b\u8bd534\u79cd\u9519\u8bef\u573a\u666f\u3002", "result": "o3-mini\u6a21\u578b\u5728pass@5\u8bc4\u5206\u4e0b100%\u6b63\u786e\uff0c\u5176\u4ed6\u6a21\u578b\u548c\u914d\u7f6e\u901a\u5e38\u8d85\u8fc780%\uff0c\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u4e0b\u8d85\u8fc790%\u3002", "conclusion": "LLMs\u5728\u786c\u4ef6\u8bbe\u8ba1\u95ee\u9898\u8bca\u65ad\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u6709\u671b\u6210\u4e3a\u672a\u6765\u8bbe\u8ba1\u6d41\u7a0b\u4e2d\u7684\u91cd\u8981\u5de5\u5177\u3002"}}
{"id": "2507.06839", "pdf": "https://arxiv.org/pdf/2507.06839", "abs": "https://arxiv.org/abs/2507.06839", "authors": ["Jihao Andreas Lin"], "title": "Scalable Gaussian Processes: Advances in Iterative Methods and Pathwise Conditioning", "categories": ["cs.LG", "stat.ML"], "comment": "PhD Thesis, University of Cambridge", "summary": "Gaussian processes are a powerful framework for uncertainty-aware function\napproximation and sequential decision-making. Unfortunately, their classical\nformulation does not scale gracefully to large amounts of data and modern\nhardware for massively-parallel computation, prompting many researchers to\ndevelop techniques which improve their scalability. This dissertation focuses\non the powerful combination of iterative methods and pathwise conditioning to\ndevelop methodological contributions which facilitate the use of Gaussian\nprocesses in modern large-scale settings. By combining these two techniques\nsynergistically, expensive computations are expressed as solutions to systems\nof linear equations and obtained by leveraging iterative linear system solvers.\nThis drastically reduces memory requirements, facilitating application to\nsignificantly larger amounts of data, and introduces matrix multiplication as\nthe main computational operation, which is ideal for modern hardware.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u8fed\u4ee3\u65b9\u6cd5\u548c\u8def\u5f84\u6761\u4ef6\u5316\u7684\u65b9\u6cd5\uff0c\u4ee5\u63d0\u5347\u9ad8\u65af\u8fc7\u7a0b\u5728\u5927\u89c4\u6a21\u6570\u636e\u4e2d\u7684\u53ef\u6269\u5c55\u6027\u3002", "motivation": "\u9ad8\u65af\u8fc7\u7a0b\u5728\u7ecf\u5178\u6846\u67b6\u4e0b\u96be\u4ee5\u9002\u5e94\u5927\u89c4\u6a21\u6570\u636e\u548c\u73b0\u4ee3\u786c\u4ef6\u5e76\u884c\u8ba1\u7b97\u7684\u9700\u6c42\uff0c\u56e0\u6b64\u9700\u8981\u5f00\u53d1\u66f4\u9ad8\u6548\u7684\u6280\u672f\u3002", "method": "\u901a\u8fc7\u5c06\u8fed\u4ee3\u7ebf\u6027\u7cfb\u7edf\u6c42\u89e3\u5668\u4e0e\u8def\u5f84\u6761\u4ef6\u5316\u7ed3\u5408\uff0c\u5c06\u6602\u8d35\u8ba1\u7b97\u8f6c\u5316\u4e3a\u7ebf\u6027\u65b9\u7a0b\u7ec4\u6c42\u89e3\uff0c\u964d\u4f4e\u5185\u5b58\u9700\u6c42\u3002", "result": "\u663e\u8457\u51cf\u5c11\u4e86\u5185\u5b58\u9700\u6c42\uff0c\u9002\u7528\u4e8e\u66f4\u5927\u89c4\u6a21\u6570\u636e\uff0c\u5e76\u4f18\u5316\u4e86\u73b0\u4ee3\u786c\u4ef6\u4e0a\u7684\u77e9\u9635\u4e58\u6cd5\u64cd\u4f5c\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u9ad8\u65af\u8fc7\u7a0b\u5728\u73b0\u4ee3\u5927\u89c4\u6a21\u573a\u666f\u4e2d\u7684\u5e94\u7528\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.06797", "pdf": "https://arxiv.org/pdf/2507.06797", "abs": "https://arxiv.org/abs/2507.06797", "authors": ["Antonella Barisic Kulas", "Andreja Jurasovic", "Stjepan Bogdan"], "title": "Unlocking Thermal Aerial Imaging: Synthetic Enhancement of UAV Datasets", "categories": ["cs.CV"], "comment": "Preprint. Accepted at ECMR 2025", "summary": "Thermal imaging from unmanned aerial vehicles (UAVs) holds significant\npotential for applications in search and rescue, wildlife monitoring, and\nemergency response, especially under low-light or obscured conditions. However,\nthe scarcity of large-scale, diverse thermal aerial datasets limits the\nadvancement of deep learning models in this domain, primarily due to the high\ncost and logistical challenges of collecting thermal data. In this work, we\nintroduce a novel procedural pipeline for generating synthetic thermal images\nfrom an aerial perspective. Our method integrates arbitrary object classes into\nexisting thermal backgrounds by providing control over the position, scale, and\norientation of the new objects, while aligning them with the viewpoints of the\nbackground. We enhance existing thermal datasets by introducing new object\ncategories, specifically adding a drone class in urban environments to the\nHIT-UAV dataset and an animal category to the MONET dataset. In evaluating\nthese datasets for object detection task, we showcase strong performance across\nboth new and existing classes, validating the successful expansion into new\napplications. Through comparative analysis, we show that thermal detectors\noutperform their visible-light-trained counterparts and highlight the\nimportance of replicating aerial viewing angles. Project page:\nhttps://github.com/larics/thermal_aerial_synthetic.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u751f\u6210\u5408\u6210\u70ed\u6210\u50cf\u6570\u636e\u7684\u6d41\u7a0b\uff0c\u7528\u4e8e\u6269\u5c55\u70ed\u6210\u50cf\u6570\u636e\u96c6\uff0c\u63d0\u5347\u65e0\u4eba\u673a\u70ed\u6210\u50cf\u5e94\u7528\u4e2d\u7684\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u70ed\u6210\u50cf\u6570\u636e\u7a00\u7f3a\u95ee\u9898\uff0c\u63a8\u52a8\u65e0\u4eba\u673a\u5728\u641c\u7d22\u6551\u63f4\u3001\u91ce\u751f\u52a8\u7269\u76d1\u6d4b\u7b49\u9886\u57df\u7684\u5e94\u7528\u3002", "method": "\u901a\u8fc7\u5408\u6210\u65b9\u6cd5\u5c06\u4efb\u610f\u7269\u4f53\u7c7b\u522b\u5d4c\u5165\u73b0\u6709\u70ed\u6210\u50cf\u80cc\u666f\uff0c\u63a7\u5236\u4f4d\u7f6e\u3001\u5c3a\u5ea6\u548c\u65b9\u5411\uff0c\u5e76\u589e\u5f3a\u73b0\u6709\u6570\u636e\u96c6\u3002", "result": "\u5728\u76ee\u6807\u68c0\u6d4b\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u70ed\u6210\u50cf\u68c0\u6d4b\u5668\u4f18\u4e8e\u53ef\u89c1\u5149\u8bad\u7ec3\u6a21\u578b\u3002", "conclusion": "\u5408\u6210\u70ed\u6210\u50cf\u6570\u636e\u80fd\u6709\u6548\u6269\u5c55\u6570\u636e\u96c6\uff0c\u63d0\u5347\u6a21\u578b\u6027\u80fd\uff0c\u5c24\u5176\u5728\u65e0\u4eba\u673a\u89c6\u89d2\u4e0b\u8868\u73b0\u7a81\u51fa\u3002"}}
{"id": "2507.06563", "pdf": "https://arxiv.org/pdf/2507.06563", "abs": "https://arxiv.org/abs/2507.06563", "authors": ["Jeanette Schofield", "Shuyu Tian", "Hoang Thanh Thanh Truong", "Maximilian Heil"], "title": "DS@GT at CheckThat! 2025: Exploring Retrieval and Reranking Pipelines for Scientific Claim Source Retrieval on Social Media Discourse", "categories": ["cs.IR", "cs.CL"], "comment": null, "summary": "Social media users often make scientific claims without citing where these\nclaims come from, generating a need to verify these claims. This paper details\nwork done by the DS@GT team for CLEF 2025 CheckThat! Lab Task 4b Scientific\nClaim Source Retrieval which seeks to find relevant scientific papers based on\nimplicit references in tweets. Our team explored 6 different data augmentation\ntechniques, 7 different retrieval and reranking pipelines, and finetuned a\nbi-encoder. Achieving an MRR@5 of 0.58, our team ranked 16th out of 30 teams\nfor the CLEF 2025 CheckThat! Lab Task 4b, and improvement of 0.15 over the BM25\nbaseline of 0.43. Our code is available on Github at\nhttps://github.com/dsgt-arc/checkthat-2025-swd/tree/main/subtask-4b.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u5982\u4f55\u901a\u8fc7\u793e\u4ea4\u5a92\u4f53\u4e2d\u7684\u63a8\u6587\u68c0\u7d22\u76f8\u5173\u79d1\u5b66\u8bba\u6587\uff0c\u4ee5\u9a8c\u8bc1\u672a\u5f15\u7528\u7684\u79d1\u5b66\u58f0\u660e\u3002\u56e2\u961f\u5728CLEF 2025 CheckThat! Lab Task 4b\u4e2d\u6d4b\u8bd5\u4e86\u591a\u79cd\u6570\u636e\u589e\u5f3a\u548c\u68c0\u7d22\u65b9\u6cd5\uff0c\u6700\u7ec8\u63d0\u5347\u4e86\u68c0\u7d22\u6548\u679c\u3002", "motivation": "\u793e\u4ea4\u5a92\u4f53\u7528\u6237\u5e38\u53d1\u8868\u672a\u5f15\u7528\u6765\u6e90\u7684\u79d1\u5b66\u58f0\u660e\uff0c\u9700\u8981\u9a8c\u8bc1\u8fd9\u4e9b\u58f0\u660e\u7684\u771f\u5b9e\u6027\u3002", "method": "\u56e2\u961f\u63a2\u7d22\u4e866\u79cd\u6570\u636e\u589e\u5f3a\u6280\u672f\u30017\u79cd\u68c0\u7d22\u548c\u91cd\u6392\u6d41\u7a0b\uff0c\u5e76\u5fae\u8c03\u4e86\u4e00\u4e2a\u53cc\u7f16\u7801\u5668\u3002", "result": "\u5728CLEF 2025\u4efb\u52a1\u4e2d\uff0c\u56e2\u961fMRR@5\u8fbe\u52300.58\uff0c\u6392\u540d\u7b2c16\uff0c\u6bd4\u57fa\u7ebfBM25\u63d0\u5347\u4e860.15\u3002", "conclusion": "\u901a\u8fc7\u591a\u79cd\u6280\u672f\u4f18\u5316\uff0c\u56e2\u961f\u663e\u8457\u63d0\u5347\u4e86\u79d1\u5b66\u58f0\u660e\u6765\u6e90\u68c0\u7d22\u7684\u6548\u679c\uff0c\u4ee3\u7801\u5df2\u5f00\u6e90\u3002"}}
{"id": "2507.06853", "pdf": "https://arxiv.org/pdf/2507.06853", "abs": "https://arxiv.org/abs/2507.06853", "authors": ["Liang Wang", "Yu Rong", "Tingyang Xu", "Zhenyi Zhong", "Zhiyuan Liu", "Pengju Wang", "Deli Zhao", "Qiang Liu", "Shu Wu", "Liang Wang"], "title": "DiffSpectra: Molecular Structure Elucidation from Spectra using Diffusion Models", "categories": ["cs.LG", "cs.AI", "cs.CE", "physics.chem-ph", "q-bio.MN"], "comment": null, "summary": "Molecular structure elucidation from spectra is a foundational problem in\nchemistry, with profound implications for compound identification, synthesis,\nand drug development. Traditional methods rely heavily on expert interpretation\nand lack scalability. Pioneering machine learning methods have introduced\nretrieval-based strategies, but their reliance on finite libraries limits\ngeneralization to novel molecules. Generative models offer a promising\nalternative, yet most adopt autoregressive SMILES-based architectures that\noverlook 3D geometry and struggle to integrate diverse spectral modalities. In\nthis work, we present DiffSpectra, a generative framework that directly infers\nboth 2D and 3D molecular structures from multi-modal spectral data using\ndiffusion models. DiffSpectra formulates structure elucidation as a conditional\ngeneration process. Its denoising network is parameterized by Diffusion\nMolecule Transformer, an SE(3)-equivariant architecture that integrates\ntopological and geometric information. Conditioning is provided by SpecFormer,\na transformer-based spectral encoder that captures intra- and inter-spectral\ndependencies from multi-modal spectra. Extensive experiments demonstrate that\nDiffSpectra achieves high accuracy in structure elucidation, recovering exact\nstructures with 16.01% top-1 accuracy and 96.86% top-20 accuracy through\nsampling. The model benefits significantly from 3D geometric modeling,\nSpecFormer pre-training, and multi-modal conditioning. These results highlight\nthe effectiveness of spectrum-conditioned diffusion modeling in addressing the\nchallenge of molecular structure elucidation. To our knowledge, DiffSpectra is\nthe first framework to unify multi-modal spectral reasoning and joint 2D/3D\ngenerative modeling for de novo molecular structure elucidation.", "AI": {"tldr": "DiffSpectra\u662f\u4e00\u79cd\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684\u751f\u6210\u6846\u67b6\uff0c\u76f4\u63a5\u4ece\u591a\u6a21\u6001\u5149\u8c31\u6570\u636e\u63a8\u65ad2D\u548c3D\u5206\u5b50\u7ed3\u6784\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u65b9\u6cd5\u548c\u73b0\u6709\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u4f20\u7edf\u5206\u5b50\u7ed3\u6784\u89e3\u6790\u65b9\u6cd5\u4f9d\u8d56\u4e13\u5bb6\u89e3\u91ca\u4e14\u7f3a\u4e4f\u6269\u5c55\u6027\uff0c\u73b0\u6709\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u4f9d\u8d56\u6709\u9650\u5e93\uff0c\u96be\u4ee5\u63a8\u5e7f\u5230\u65b0\u5206\u5b50\u3002DiffSpectra\u65e8\u5728\u901a\u8fc7\u751f\u6210\u6a21\u578b\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "DiffSpectra\u91c7\u7528\u6269\u6563\u6a21\u578b\uff0c\u7ed3\u5408SE(3)-\u7b49\u53d8\u67b6\u6784\u7684Denoising\u7f51\u7edc\u548c\u57fa\u4e8eTransformer\u7684\u5149\u8c31\u7f16\u7801\u5668SpecFormer\uff0c\u5b9e\u73b0\u591a\u6a21\u6001\u5149\u8c31\u6570\u636e\u7684\u6761\u4ef6\u751f\u6210\u3002", "result": "\u5b9e\u9a8c\u8868\u660eDiffSpectra\u5728\u7ed3\u6784\u89e3\u6790\u4e2d\u8868\u73b0\u51fa\u8272\uff0ctop-1\u51c6\u786e\u7387\u4e3a16.01%\uff0ctop-20\u51c6\u786e\u7387\u4e3a96.86%\u30023D\u51e0\u4f55\u5efa\u6a21\u548c\u591a\u6a21\u6001\u6761\u4ef6\u663e\u8457\u63d0\u5347\u6027\u80fd\u3002", "conclusion": "DiffSpectra\u662f\u9996\u4e2a\u7edf\u4e00\u591a\u6a21\u6001\u5149\u8c31\u63a8\u7406\u548c2D/3D\u751f\u6210\u5efa\u6a21\u7684\u6846\u67b6\uff0c\u4e3a\u5206\u5b50\u7ed3\u6784\u89e3\u6790\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.06806", "pdf": "https://arxiv.org/pdf/2507.06806", "abs": "https://arxiv.org/abs/2507.06806", "authors": ["Eya Cherif", "Arthur Ouaknine", "Luke A. Brown", "Phuong D. Dao", "Kyle R. Kovach", "Bing Lu", "Daniel Mederer", "Hannes Feilhauer", "Teja Kattenborn", "David Rolnick"], "title": "GreenHyperSpectra: A multi-source hyperspectral dataset for global vegetation trait prediction", "categories": ["cs.CV", "eess.IV"], "comment": null, "summary": "Plant traits such as leaf carbon content and leaf mass are essential\nvariables in the study of biodiversity and climate change. However,\nconventional field sampling cannot feasibly cover trait variation at\necologically meaningful spatial scales. Machine learning represents a valuable\nsolution for plant trait prediction across ecosystems, leveraging hyperspectral\ndata from remote sensing. Nevertheless, trait prediction from hyperspectral\ndata is challenged by label scarcity and substantial domain shifts (\\eg across\nsensors, ecological distributions), requiring robust cross-domain methods.\nHere, we present GreenHyperSpectra, a pretraining dataset encompassing\nreal-world cross-sensor and cross-ecosystem samples designed to benchmark trait\nprediction with semi- and self-supervised methods. We adopt an evaluation\nframework encompassing in-distribution and out-of-distribution scenarios. We\nsuccessfully leverage GreenHyperSpectra to pretrain label-efficient\nmulti-output regression models that outperform the state-of-the-art supervised\nbaseline. Our empirical analyses demonstrate substantial improvements in\nlearning spectral representations for trait prediction, establishing a\ncomprehensive methodological framework to catalyze research at the intersection\nof representation learning and plant functional traits assessment. All code and\ndata are available at: https://github.com/echerif18/HyspectraSSL.", "AI": {"tldr": "GreenHyperSpectra\u6570\u636e\u96c6\u7528\u4e8e\u690d\u7269\u6027\u72b6\u9884\u6d4b\uff0c\u901a\u8fc7\u534a\u76d1\u7763\u548c\u81ea\u76d1\u7763\u65b9\u6cd5\u63d0\u5347\u8de8\u57df\u6027\u80fd\uff0c\u4f18\u4e8e\u73b0\u6709\u76d1\u7763\u57fa\u7ebf\u3002", "motivation": "\u4f20\u7edf\u91ce\u5916\u91c7\u6837\u96be\u4ee5\u8986\u76d6\u751f\u6001\u5c3a\u5ea6\u4e0a\u7684\u6027\u72b6\u53d8\u5f02\uff0c\u673a\u5668\u5b66\u4e60\u7ed3\u5408\u9065\u611f\u6570\u636e\u6210\u4e3a\u89e3\u51b3\u65b9\u6848\uff0c\u4f46\u9762\u4e34\u6807\u7b7e\u7a00\u7f3a\u548c\u57df\u504f\u79fb\u95ee\u9898\u3002", "method": "\u63d0\u51faGreenHyperSpectra\u6570\u636e\u96c6\uff0c\u91c7\u7528\u534a\u76d1\u7763\u548c\u81ea\u76d1\u7763\u65b9\u6cd5\u9884\u8bad\u7ec3\u591a\u8f93\u51fa\u56de\u5f52\u6a21\u578b\uff0c\u8bc4\u4f30\u6846\u67b6\u5305\u62ec\u57df\u5185\u548c\u57df\u5916\u573a\u666f\u3002", "result": "\u6a21\u578b\u5728\u6027\u72b6\u9884\u6d4b\u4e2d\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u76d1\u7763\u57fa\u7ebf\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5149\u8c31\u8868\u5f81\u5b66\u4e60\u80fd\u529b\u3002", "conclusion": "GreenHyperSpectra\u4e3a\u690d\u7269\u529f\u80fd\u6027\u72b6\u8bc4\u4f30\u4e0e\u8868\u5f81\u5b66\u4e60\u7684\u4ea4\u53c9\u7814\u7a76\u63d0\u4f9b\u4e86\u65b9\u6cd5\u8bba\u6846\u67b6\u548c\u6570\u636e\u652f\u6301\u3002"}}
{"id": "2507.06734", "pdf": "https://arxiv.org/pdf/2507.06734", "abs": "https://arxiv.org/abs/2507.06734", "authors": ["Milena Pustet", "Elisabeth Steffen", "Helena Mihaljevi\u0107", "Grischa Stanjek", "Yannis Illies"], "title": "Civil Society in the Loop: Feedback-Driven Adaptation of (L)LM-Assisted Classification in an Open-Source Telegram Monitoring Tool", "categories": ["cs.HC", "cs.AI", "cs.CL", "cs.CY"], "comment": null, "summary": "The role of civil society organizations (CSOs) in monitoring harmful online\ncontent is increasingly crucial, especially as platform providers reduce their\ninvestment in content moderation. AI tools can assist in detecting and\nmonitoring harmful content at scale. However, few open-source tools offer\nseamless integration of AI models and social media monitoring infrastructures.\nGiven their thematic expertise and contextual understanding of harmful content,\nCSOs should be active partners in co-developing technological tools, providing\nfeedback, helping to improve models, and ensuring alignment with stakeholder\nneeds and values, rather than as passive 'consumers'. However, collaborations\nbetween the open source community, academia, and civil society remain rare, and\nresearch on harmful content seldom translates into practical tools usable by\ncivil society actors. This work in progress explores how CSOs can be\nmeaningfully involved in an AI-assisted open-source monitoring tool of\nanti-democratic movements on Telegram, which we are currently developing in\ncollaboration with CSO stakeholders.", "AI": {"tldr": "\u63a2\u8ba8\u516c\u6c11\u793e\u4f1a\u7ec4\u7ec7\uff08CSOs\uff09\u5728AI\u8f85\u52a9\u5f00\u6e90\u76d1\u6d4b\u5de5\u5177\u5f00\u53d1\u4e2d\u7684\u4e3b\u52a8\u89d2\u8272\uff0c\u4ee5\u5e94\u5bf9\u6709\u5bb3\u5728\u7ebf\u5185\u5bb9\u3002", "motivation": "\u5e73\u53f0\u51cf\u5c11\u5185\u5bb9\u5ba1\u6838\u6295\u5165\uff0cCSOs\u5177\u5907\u4e3b\u9898\u4e13\u957f\u548c\u4e0a\u4e0b\u6587\u7406\u89e3\u80fd\u529b\uff0c\u5e94\u6210\u4e3a\u6280\u672f\u5de5\u5177\u7684\u5171\u540c\u5f00\u53d1\u8005\u3002", "method": "\u4e0eCSO\u5229\u76ca\u76f8\u5173\u8005\u5408\u4f5c\uff0c\u5f00\u53d1AI\u8f85\u52a9\u7684\u5f00\u6e90\u76d1\u6d4b\u5de5\u5177\uff0c\u7528\u4e8e\u76d1\u6d4bTelegram\u4e0a\u7684\u53cd\u6c11\u4e3b\u8fd0\u52a8\u3002", "result": "\u76ee\u524d\u4e3a\u8fdb\u884c\u4e2d\u7684\u5de5\u4f5c\uff0c\u65e8\u5728\u4fc3\u8fdb\u5f00\u6e90\u793e\u533a\u3001\u5b66\u672f\u754c\u4e0e\u516c\u6c11\u793e\u4f1a\u7684\u5408\u4f5c\u3002", "conclusion": "CSOs\u5e94\u79ef\u6781\u53c2\u4e0e\u6280\u672f\u5de5\u5177\u5f00\u53d1\uff0c\u786e\u4fdd\u5176\u7b26\u5408\u9700\u6c42\u4e0e\u4ef7\u503c\u89c2\uff0c\u800c\u975e\u88ab\u52a8\u4f7f\u7528\u3002"}}
{"id": "2507.06859", "pdf": "https://arxiv.org/pdf/2507.06859", "abs": "https://arxiv.org/abs/2507.06859", "authors": ["Zitian Li", "Wang Chi Cheung"], "title": "Episodic Contextual Bandits with Knapsacks under Conversion Models", "categories": ["cs.LG"], "comment": null, "summary": "We study an online setting, where a decision maker (DM) interacts with\ncontextual bandit-with-knapsack (BwK) instances in repeated episodes. These\nepisodes start with different resource amounts, and the contexts' probability\ndistributions are non-stationary in an episode. All episodes share the same\nlatent conversion model, which governs the random outcome contingent upon a\nrequest's context and an allocation decision. Our model captures applications\nsuch as dynamic pricing on perishable resources with episodic replenishment,\nand first price auctions in repeated episodes with different starting budgets.\nWe design an online algorithm that achieves a regret sub-linear in $T$, the\nnumber of episodes, assuming access to a \\emph{confidence bound oracle} that\nachieves an $o(T)$-regret. Such an oracle is readily available from existing\ncontextual bandit literature. We overcome the technical challenge with\narbitrarily many possible contexts, which leads to a reinforcement learning\nproblem with an unbounded state space. Our framework provides improved regret\nbounds in certain settings when the DM is provided with unlabeled feature data,\nwhich is novel to the contextual BwK literature.", "AI": {"tldr": "\u7814\u7a76\u5728\u7ebf\u51b3\u7b56\u8005\u5728\u975e\u5e73\u7a33\u4e0a\u4e0b\u6587\u8d44\u6e90\u5206\u914d\u95ee\u9898\u4e2d\u7684\u8868\u73b0\uff0c\u63d0\u51fa\u4e00\u79cd\u7b97\u6cd5\u5b9e\u73b0\u4e9a\u7ebf\u6027\u9057\u61be\u3002", "motivation": "\u89e3\u51b3\u52a8\u6001\u5b9a\u4ef7\u548c\u62cd\u5356\u7b49\u5e94\u7528\u4e2d\u8d44\u6e90\u5206\u914d\u7684\u975e\u5e73\u7a33\u6027\u548c\u4e0a\u4e0b\u6587\u591a\u6837\u6027\u95ee\u9898\u3002", "method": "\u8bbe\u8ba1\u5728\u7ebf\u7b97\u6cd5\uff0c\u5229\u7528\u7f6e\u4fe1\u8fb9\u754c\u9884\u8a00\u673a\u5904\u7406\u65e0\u754c\u72b6\u6001\u7a7a\u95f4\u3002", "result": "\u7b97\u6cd5\u5728T\u6b21\u60c5\u8282\u4e2d\u5b9e\u73b0\u4e9a\u7ebf\u6027\u9057\u61be\uff0c\u5e76\u5728\u7279\u5b9a\u8bbe\u7f6e\u4e0b\u63d0\u4f9b\u6539\u8fdb\u7684\u9057\u61be\u8fb9\u754c\u3002", "conclusion": "\u6846\u67b6\u4e3a\u4e0a\u4e0b\u6587BwK\u95ee\u9898\u63d0\u4f9b\u4e86\u65b0\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5c24\u5176\u5728\u672a\u6807\u8bb0\u7279\u5f81\u6570\u636e\u573a\u666f\u4e0b\u8868\u73b0\u4f18\u8d8a\u3002"}}
{"id": "2507.06812", "pdf": "https://arxiv.org/pdf/2507.06812", "abs": "https://arxiv.org/abs/2507.06812", "authors": ["Xu Yang", "Shaoli Huang", "Shenbo Xie", "Xuelin Chen", "Yifei Liu", "Changxing Ding"], "title": "Democratizing High-Fidelity Co-Speech Gesture Video Generation", "categories": ["cs.CV", "cs.AI"], "comment": "ICCV 2025", "summary": "Co-speech gesture video generation aims to synthesize realistic,\naudio-aligned videos of speakers, complete with synchronized facial expressions\nand body gestures. This task presents challenges due to the significant\none-to-many mapping between audio and visual content, further complicated by\nthe scarcity of large-scale public datasets and high computational demands. We\npropose a lightweight framework that utilizes 2D full-body skeletons as an\nefficient auxiliary condition to bridge audio signals with visual outputs. Our\napproach introduces a diffusion model conditioned on fine-grained audio\nsegments and a skeleton extracted from the speaker's reference image,\npredicting skeletal motions through skeleton-audio feature fusion to ensure\nstrict audio coordination and body shape consistency. The generated skeletons\nare then fed into an off-the-shelf human video generation model with the\nspeaker's reference image to synthesize high-fidelity videos. To democratize\nresearch, we present CSG-405-the first public dataset with 405 hours of\nhigh-resolution videos across 71 speech types, annotated with 2D skeletons and\ndiverse speaker demographics. Experiments show that our method exceeds\nstate-of-the-art approaches in visual quality and synchronization while\ngeneralizing across speakers and contexts.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u8f7b\u91cf\u7ea7\u6846\u67b6\uff0c\u5229\u75282D\u5168\u8eab\u9aa8\u67b6\u4f5c\u4e3a\u8f85\u52a9\u6761\u4ef6\uff0c\u901a\u8fc7\u6269\u6563\u6a21\u578b\u751f\u6210\u4e0e\u97f3\u9891\u540c\u6b65\u7684\u8bf4\u8bdd\u8005\u89c6\u9891\uff0c\u5e76\u53d1\u5e03\u4e86\u9996\u4e2a\u516c\u5f00\u6570\u636e\u96c6CSG-405\u3002", "motivation": "\u89e3\u51b3\u8bed\u97f3\u624b\u52bf\u89c6\u9891\u751f\u6210\u4e2d\u7684\u4e00\u5bf9\u591a\u6620\u5c04\u95ee\u9898\u3001\u6570\u636e\u7a00\u7f3a\u548c\u9ad8\u8ba1\u7b97\u9700\u6c42\u6311\u6218\u3002", "method": "\u4f7f\u75282D\u9aa8\u67b6\u4f5c\u4e3a\u97f3\u9891\u4e0e\u89c6\u89c9\u8f93\u51fa\u7684\u6865\u6881\uff0c\u7ed3\u5408\u6269\u6563\u6a21\u578b\u548c\u9aa8\u67b6-\u97f3\u9891\u7279\u5f81\u878d\u5408\uff0c\u751f\u6210\u9ad8\u4fdd\u771f\u89c6\u9891\u3002", "result": "\u65b9\u6cd5\u5728\u89c6\u89c9\u8d28\u91cf\u548c\u540c\u6b65\u6027\u4e0a\u4f18\u4e8e\u73b0\u6709\u6280\u672f\uff0c\u5e76\u80fd\u6cdb\u5316\u5230\u4e0d\u540c\u8bf4\u8bdd\u8005\u548c\u573a\u666f\u3002", "conclusion": "\u63d0\u51fa\u7684\u6846\u67b6\u548c\u6570\u636e\u96c6\u4e3a\u8bed\u97f3\u624b\u52bf\u89c6\u9891\u751f\u6210\u7814\u7a76\u63d0\u4f9b\u4e86\u9ad8\u6548\u4e14\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.06892", "pdf": "https://arxiv.org/pdf/2507.06892", "abs": "https://arxiv.org/abs/2507.06892", "authors": ["Jing Liang", "Hongyao Tang", "Yi Ma", "Jinyi Liu", "Yan Zheng", "Shuyue Hu", "Lei Bai", "Jianye Hao"], "title": "Squeeze the Soaked Sponge: Efficient Off-policy Reinforcement Finetuning for Large Language Model", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "Preliminary version. Project page:\n  https://anitaleungxx.github.io/ReMix", "summary": "Reinforcement Learning (RL) has demonstrated its potential to improve the\nreasoning ability of Large Language Models (LLMs). One major limitation of most\nexisting Reinforcement Finetuning (RFT) methods is that they are on-policy RL\nin nature, i.e., data generated during the past learning process is not fully\nutilized. This inevitably comes at a significant cost of compute and time,\nposing a stringent bottleneck on continuing economic and efficient scaling. To\nthis end, we launch the renaissance of off-policy RL and propose Reincarnating\nMix-policy Proximal Policy Gradient (ReMix), a general approach to enable\non-policy RFT methods like PPO and GRPO to leverage off-policy data. ReMix\nconsists of three major components: (1) Mix-policy proximal policy gradient\nwith an increased Update-To-Data (UTD) ratio for efficient training; (2)\nKL-Convex policy constraint to balance the trade-off between stability and\nflexibility; (3) Policy reincarnation to achieve a seamless transition from\nefficient early-stage learning to steady asymptotic improvement. In our\nexperiments, we train a series of ReMix models upon PPO, GRPO and 1.5B, 7B base\nmodels. ReMix shows an average Pass@1 accuracy of 52.10% (for 1.5B model) with\n0.079M response rollouts, 350 training steps and achieves 63.27%/64.39% (for 7B\nmodel) with 0.007M/0.011M response rollouts, 50/75 training steps, on five math\nreasoning benchmarks (i.e., AIME'24, AMC'23, Minerva, OlympiadBench, and\nMATH500). Compared with 15 recent advanced models, ReMix shows SOTA-level\nperformance with an over 30x to 450x reduction in training cost in terms of\nrollout data volume. In addition, we reveal insightful findings via\nmultifaceted analysis, including the implicit preference for shorter responses\ndue to the Whipping Effect of off-policy discrepancy, the collapse mode of\nself-reflection behavior under the presence of severe off-policyness, etc.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faReMix\u65b9\u6cd5\uff0c\u901a\u8fc7\u5229\u7528\u79bb\u7b56\u7565\u6570\u636e\u6539\u8fdb\u73b0\u6709\u5f3a\u5316\u5fae\u8c03\u65b9\u6cd5\uff0c\u663e\u8457\u964d\u4f4e\u8bad\u7ec3\u6210\u672c\u5e76\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u5f3a\u5316\u5fae\u8c03\u65b9\u6cd5\u591a\u4e3a\u540c\u7b56\u7565RL\uff0c\u672a\u80fd\u5145\u5206\u5229\u7528\u5386\u53f2\u6570\u636e\uff0c\u5bfc\u81f4\u8ba1\u7b97\u548c\u65f6\u95f4\u6210\u672c\u9ad8\u6602\u3002", "method": "ReMix\u7ed3\u5408\u6df7\u5408\u7b56\u7565\u8fd1\u7aef\u7b56\u7565\u68af\u5ea6\u3001KL\u51f8\u7ea6\u675f\u7b56\u7565\u548c\u653f\u7b56\u91cd\u751f\u6280\u672f\uff0c\u5b9e\u73b0\u9ad8\u6548\u8bad\u7ec3\u548c\u7a33\u5b9a\u6539\u8fdb\u3002", "result": "ReMix\u5728\u591a\u4e2a\u6570\u5b66\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u8bad\u7ec3\u6210\u672c\u964d\u4f4e30\u81f3450\u500d\u3002", "conclusion": "ReMix\u4e3a\u5f3a\u5316\u5fae\u8c03\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u4e14\u7ecf\u6d4e\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5e76\u63ed\u793a\u4e86\u79bb\u7b56\u7565\u6570\u636e\u7684\u65b0\u89c1\u89e3\u3002"}}
{"id": "2507.06888", "pdf": "https://arxiv.org/pdf/2507.06888", "abs": "https://arxiv.org/abs/2507.06888", "authors": ["Wei Chen", "Wanyang Gu", "Linjun Peng", "Ruichu Cai", "Zhifeng Hao", "Kun Zhang"], "title": "Horizontal and Vertical Federated Causal Structure Learning via Higher-order Cumulants", "categories": ["cs.LG"], "comment": null, "summary": "Federated causal discovery aims to uncover the causal relationships between\nentities while protecting data privacy, which has significant importance and\nnumerous applications in real-world scenarios. Existing federated causal\nstructure learning methods primarily focus on horizontal federated settings.\nHowever, in practical situations, different clients may not necessarily contain\ndata on the same variables. In a single client, the incomplete set of variables\ncan easily lead to spurious causal relationships, thereby affecting the\ninformation transmitted to other clients. To address this issue, we\ncomprehensively consider causal structure learning methods under both\nhorizontal and vertical federated settings. We provide the identification\ntheories and methods for learning causal structure in the horizontal and\nvertical federal setting via higher-order cumulants. Specifically, we first\naggregate higher-order cumulant information from all participating clients to\nconstruct global cumulant estimates. These global estimates are then used for\nrecursive source identification, ultimately yielding a global causal strength\nmatrix. Our approach not only enables the reconstruction of causal graphs but\nalso facilitates the estimation of causal strength coefficients. Our algorithm\ndemonstrates superior performance in experiments conducted on both synthetic\ndata and real-world data.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u8054\u90a6\u56e0\u679c\u53d1\u73b0\u65b9\u6cd5\uff0c\u7ed3\u5408\u6c34\u5e73\u548c\u5782\u76f4\u8054\u90a6\u8bbe\u7f6e\uff0c\u5229\u7528\u9ad8\u9636\u7d2f\u79ef\u91cf\u6784\u5efa\u5168\u5c40\u56e0\u679c\u56fe\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u8054\u90a6\u56e0\u679c\u5b66\u4e60\u65b9\u6cd5\u4ec5\u9002\u7528\u4e8e\u6c34\u5e73\u8054\u90a6\u8bbe\u7f6e\u7684\u95ee\u9898\uff0c\u907f\u514d\u56e0\u53d8\u91cf\u4e0d\u5b8c\u6574\u5bfc\u81f4\u7684\u865a\u5047\u56e0\u679c\u5173\u7cfb\u3002", "method": "\u901a\u8fc7\u805a\u5408\u5ba2\u6237\u7aef\u7684\u9ad8\u9636\u7d2f\u79ef\u91cf\u4fe1\u606f\u6784\u5efa\u5168\u5c40\u4f30\u8ba1\uff0c\u9012\u5f52\u8bc6\u522b\u56e0\u679c\u6e90\uff0c\u751f\u6210\u5168\u5c40\u56e0\u679c\u5f3a\u5ea6\u77e9\u9635\u3002", "result": "\u5728\u5408\u6210\u548c\u771f\u5b9e\u6570\u636e\u5b9e\u9a8c\u4e2d\u8868\u73b0\u4f18\u8d8a\uff0c\u80fd\u91cd\u5efa\u56e0\u679c\u56fe\u5e76\u4f30\u8ba1\u56e0\u679c\u5f3a\u5ea6\u7cfb\u6570\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u6c34\u5e73\u548c\u5782\u76f4\u8054\u90a6\u8bbe\u7f6e\u4e0b\u5747\u6709\u6548\uff0c\u89e3\u51b3\u4e86\u53d8\u91cf\u4e0d\u5b8c\u6574\u5e26\u6765\u7684\u95ee\u9898\u3002"}}
{"id": "2507.06814", "pdf": "https://arxiv.org/pdf/2507.06814", "abs": "https://arxiv.org/abs/2507.06814", "authors": ["Qingsen Yan", "Kangbiao Shi", "Yixu Feng", "Tao Hu", "Peng Wu", "Guansong Pang", "Yanning Zhang"], "title": "HVI-CIDNet+: Beyond Extreme Darkness for Low-Light Image Enhancement", "categories": ["cs.CV"], "comment": "14 pages", "summary": "Low-Light Image Enhancement (LLIE) aims to restore vivid content and details\nfrom corrupted low-light images. However, existing standard RGB (sRGB) color\nspace-based LLIE methods often produce color bias and brightness artifacts due\nto the inherent high color sensitivity. While Hue, Saturation, and Value (HSV)\ncolor space can decouple brightness and color, it introduces significant red\nand black noise artifacts. To address this problem, we propose a new color\nspace for LLIE, namely Horizontal/Vertical-Intensity (HVI), defined by the HV\ncolor map and learnable intensity. The HV color map enforces small distances\nfor the red coordinates to remove red noise artifacts, while the learnable\nintensity compresses the low-light regions to remove black noise artifacts.\nAdditionally, we introduce the Color and Intensity Decoupling Network+\n(HVI-CIDNet+), built upon the HVI color space, to restore damaged content and\nmitigate color distortion in extremely dark regions. Specifically, HVI-CIDNet+\nleverages abundant contextual and degraded knowledge extracted from low-light\nimages using pre-trained vision-language models, integrated via a novel\nPrior-guided Attention Block (PAB). Within the PAB, latent semantic priors can\npromote content restoration, while degraded representations guide precise color\ncorrection, both particularly in extremely dark regions through the\nmeticulously designed cross-attention fusion mechanism. Furthermore, we\nconstruct a Region Refinement Block that employs convolution for\ninformation-rich regions and self-attention for information-scarce regions,\nensuring accurate brightness adjustments. Comprehensive results from benchmark\nexperiments demonstrate that the proposed HVI-CIDNet+ outperforms the\nstate-of-the-art methods on 10 datasets.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u989c\u8272\u7a7a\u95f4HVI\u548c\u7f51\u7edcHVI-CIDNet+\uff0c\u7528\u4e8e\u4f4e\u5149\u56fe\u50cf\u589e\u5f3a\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u989c\u8272\u504f\u5dee\u548c\u4eae\u5ea6\u4f2a\u5f71\u95ee\u9898\uff0c\u5e76\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8esRGB\u548cHSV\u989c\u8272\u7a7a\u95f4\u7684\u4f4e\u5149\u56fe\u50cf\u589e\u5f3a\u65b9\u6cd5\u5b58\u5728\u989c\u8272\u504f\u5dee\u3001\u4eae\u5ea6\u4f2a\u5f71\u4ee5\u53ca\u7ea2\u9ed1\u566a\u58f0\u95ee\u9898\uff0c\u9700\u8981\u4e00\u79cd\u66f4\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u63d0\u51faHVI\u989c\u8272\u7a7a\u95f4\uff08\u7ed3\u5408HV\u989c\u8272\u56fe\u548c\u53ef\u5b66\u4e60\u5f3a\u5ea6\uff09\u548cHVI-CIDNet+\u7f51\u7edc\uff0c\u5229\u7528\u9884\u8bad\u7ec3\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u63d0\u53d6\u4e0a\u4e0b\u6587\u4fe1\u606f\uff0c\u5e76\u901a\u8fc7Prior-guided Attention Block\u548cRegion Refinement Block\u4f18\u5316\u5185\u5bb9\u6062\u590d\u548c\u989c\u8272\u6821\u6b63\u3002", "result": "\u572810\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cHVI-CIDNet+\u4f18\u4e8e\u73b0\u6709\u6700\u4f18\u65b9\u6cd5\u3002", "conclusion": "HVI\u989c\u8272\u7a7a\u95f4\u548cHVI-CIDNet+\u7f51\u7edc\u6709\u6548\u89e3\u51b3\u4e86\u4f4e\u5149\u56fe\u50cf\u589e\u5f3a\u4e2d\u7684\u989c\u8272\u548c\u4eae\u5ea6\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\u3002"}}
{"id": "2507.06541", "pdf": "https://arxiv.org/pdf/2507.06541", "abs": "https://arxiv.org/abs/2507.06541", "authors": ["Ali Safarpoor Dehkordi", "Ahad N. Zehmakan"], "title": "Graph-based Fake Account Detection: A Survey", "categories": ["cs.SI", "cs.AI", "cs.LG", "A.1; I.2.6; I.5.1"], "comment": "16 Tables, 5 Figures, 41 Pages", "summary": "In recent years, there has been a growing effort to develop effective and\nefficient algorithms for fake account detection in online social networks. This\nsurvey comprehensively reviews existing methods, with a focus on graph-based\ntechniques that utilise topological features of social graphs (in addition to\naccount information, such as their shared contents and profile data) to\ndistinguish between fake and real accounts. We provide several categorisations\nof these methods (for example, based on techniques used, input data, and\ndetection time), discuss their strengths and limitations, and explain how these\nmethods connect in the broader context. We also investigate the available\ndatasets, including both real-world data and synthesised models. We conclude\nthe paper by proposing several potential avenues for future research.", "AI": {"tldr": "\u672c\u6587\u7efc\u8ff0\u4e86\u793e\u4ea4\u5a92\u4f53\u4e2d\u865a\u5047\u8d26\u6237\u68c0\u6d4b\u7684\u56fe\u57fa\u65b9\u6cd5\uff0c\u5206\u7c7b\u8ba8\u8bba\u4e86\u73b0\u6709\u6280\u672f\u3001\u6570\u636e\u96c6\u53ca\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002", "motivation": "\u968f\u7740\u865a\u5047\u8d26\u6237\u95ee\u9898\u7684\u65e5\u76ca\u4e25\u91cd\uff0c\u5f00\u53d1\u9ad8\u6548\u68c0\u6d4b\u7b97\u6cd5\u6210\u4e3a\u7814\u7a76\u70ed\u70b9\u3002", "method": "\u91cd\u70b9\u5206\u6790\u57fa\u4e8e\u56fe\u62d3\u6251\u7279\u5f81\u7684\u65b9\u6cd5\uff0c\u7ed3\u5408\u8d26\u6237\u5185\u5bb9\u548c\u8d44\u6599\u6570\u636e\uff0c\u5bf9\u73b0\u6709\u6280\u672f\u8fdb\u884c\u5206\u7c7b\u3002", "result": "\u603b\u7ed3\u4e86\u4e0d\u540c\u65b9\u6cd5\u7684\u4f18\u7f3a\u70b9\uff0c\u5e76\u63a2\u8ba8\u4e86\u771f\u5b9e\u4e0e\u5408\u6210\u6570\u636e\u96c6\u7684\u5e94\u7528\u3002", "conclusion": "\u63d0\u51fa\u4e86\u672a\u6765\u7814\u7a76\u7684\u6f5c\u5728\u65b9\u5411\u3002"}}
{"id": "2507.06830", "pdf": "https://arxiv.org/pdf/2507.06830", "abs": "https://arxiv.org/abs/2507.06830", "authors": ["Tao Feng", "Xianbing Zhao", "Zhenhua Chen", "Tien Tsin Wong", "Hamid Rezatofighi", "Gholamreza Haffari", "Lizhen Qu"], "title": "Physics-Grounded Motion Forecasting via Equation Discovery for Trajectory-Guided Image-to-Video Generation", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Recent advances in diffusion-based and autoregressive video generation models\nhave achieved remarkable visual realism. However, these models typically lack\naccurate physical alignment, failing to replicate real-world dynamics in object\nmotion. This limitation arises primarily from their reliance on learned\nstatistical correlations rather than capturing mechanisms adhering to physical\nlaws. To address this issue, we introduce a novel framework that integrates\nsymbolic regression (SR) and trajectory-guided image-to-video (I2V) models for\nphysics-grounded video forecasting. Our approach extracts motion trajectories\nfrom input videos, uses a retrieval-based pre-training mechanism to enhance\nsymbolic regression, and discovers equations of motion to forecast physically\naccurate future trajectories. These trajectories then guide video generation\nwithout requiring fine-tuning of existing models. Evaluated on scenarios in\nClassical Mechanics, including spring-mass, pendulums, and projectile motions,\nour method successfully recovers ground-truth analytical equations and improves\nthe physical alignment of generated videos over baseline methods.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u7b26\u53f7\u56de\u5f52\u548c\u8f68\u8ff9\u5f15\u5bfc\u7684\u89c6\u9891\u751f\u6210\u6846\u67b6\uff0c\u4ee5\u63d0\u5347\u89c6\u9891\u751f\u6210\u7684\u7269\u7406\u51c6\u786e\u6027\u3002", "motivation": "\u73b0\u6709\u89c6\u9891\u751f\u6210\u6a21\u578b\u4f9d\u8d56\u7edf\u8ba1\u76f8\u5173\u6027\u800c\u975e\u7269\u7406\u89c4\u5f8b\uff0c\u5bfc\u81f4\u7269\u4f53\u8fd0\u52a8\u4e0d\u7b26\u5408\u771f\u5b9e\u52a8\u529b\u5b66\u3002", "method": "\u901a\u8fc7\u63d0\u53d6\u8fd0\u52a8\u8f68\u8ff9\u3001\u68c0\u7d22\u9884\u8bad\u7ec3\u589e\u5f3a\u7b26\u53f7\u56de\u5f52\uff0c\u53d1\u73b0\u8fd0\u52a8\u65b9\u7a0b\u4ee5\u9884\u6d4b\u7269\u7406\u51c6\u786e\u7684\u8f68\u8ff9\uff0c\u5e76\u5f15\u5bfc\u89c6\u9891\u751f\u6210\u3002", "result": "\u5728\u7ecf\u5178\u529b\u5b66\u573a\u666f\u4e2d\u6210\u529f\u6062\u590d\u771f\u5b9e\u8fd0\u52a8\u65b9\u7a0b\uff0c\u751f\u6210\u89c6\u9891\u7684\u7269\u7406\u5bf9\u9f50\u6027\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u6846\u67b6\u65e0\u9700\u5fae\u8c03\u73b0\u6709\u6a21\u578b\u5373\u53ef\u63d0\u5347\u89c6\u9891\u751f\u6210\u7684\u7269\u7406\u51c6\u786e\u6027\u3002"}}
{"id": "2507.06999", "pdf": "https://arxiv.org/pdf/2507.06999", "abs": "https://arxiv.org/abs/2507.06999", "authors": ["Yahan Yu", "Yuyang Dong", "Masafumi Oyamada"], "title": "Learning Deliberately, Acting Intuitively: Unlocking Test-Time Reasoning in Multimodal LLMs", "categories": ["cs.CV", "cs.CL", "cs.LG"], "comment": "Work in progress", "summary": "Reasoning is a key capability for large language models (LLMs), particularly\nwhen applied to complex tasks such as mathematical problem solving. However,\nmultimodal reasoning research still requires further exploration of modality\nalignment and training costs. Many of these approaches rely on additional data\nannotation and relevant rule-based rewards to enhance the understanding and\nreasoning ability, which significantly increases training costs and limits\nscalability. To address these challenges, we propose the\nDeliberate-to-Intuitive reasoning framework (D2I) that improves the\nunderstanding and reasoning ability of multimodal LLMs (MLLMs) without extra\nannotations and complex rewards. Specifically, our method sets deliberate\nreasoning strategies to enhance modality alignment only through the rule-based\nformat reward during training. While evaluating, the reasoning style shifts to\nintuitive, which removes deliberate reasoning strategies during training and\nimplicitly reflects the model's acquired abilities in the response. D2I\noutperforms baselines across both in-domain and out-of-domain benchmarks. Our\nfindings highlight the role of format reward in fostering transferable\nreasoning skills in MLLMs, and inspire directions for decoupling training-time\nreasoning depth from test-time response flexibility.", "AI": {"tldr": "\u63d0\u51fa\u4e86Deliberate-to-Intuitive\uff08D2I\uff09\u6846\u67b6\uff0c\u901a\u8fc7\u89c4\u5219\u5956\u52b1\u63d0\u5347\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLMs\uff09\u7684\u63a8\u7406\u80fd\u529b\uff0c\u65e0\u9700\u989d\u5916\u6807\u6ce8\u3002", "motivation": "\u89e3\u51b3\u591a\u6a21\u6001\u63a8\u7406\u7814\u7a76\u4e2d\u6a21\u6001\u5bf9\u9f50\u548c\u8bad\u7ec3\u6210\u672c\u9ad8\u7684\u95ee\u9898\u3002", "method": "D2I\u6846\u67b6\u5728\u8bad\u7ec3\u65f6\u91c7\u7528\u89c4\u5219\u5956\u52b1\u589e\u5f3a\u6a21\u6001\u5bf9\u9f50\uff0c\u8bc4\u4f30\u65f6\u8f6c\u4e3a\u76f4\u89c9\u63a8\u7406\u3002", "result": "D2I\u5728\u9886\u57df\u5185\u5916\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u4f18\u4e8e\u57fa\u7ebf\u6a21\u578b\u3002", "conclusion": "\u683c\u5f0f\u5956\u52b1\u6709\u52a9\u4e8e\u63d0\u5347MLLMs\u7684\u53ef\u8fc1\u79fb\u63a8\u7406\u80fd\u529b\uff0c\u5e76\u4e3a\u8bad\u7ec3\u4e0e\u6d4b\u8bd5\u63a8\u7406\u6df1\u5ea6\u7684\u89e3\u8026\u63d0\u4f9b\u65b9\u5411\u3002"}}
{"id": "2507.06901", "pdf": "https://arxiv.org/pdf/2507.06901", "abs": "https://arxiv.org/abs/2507.06901", "authors": ["Abolfazl Zarghani", "Sadegh Abedi"], "title": "Designing Adaptive Algorithms Based on Reinforcement Learning for Dynamic Optimization of Sliding Window Size in Multi-Dimensional Data Streams", "categories": ["cs.LG"], "comment": null, "summary": "Multi-dimensional data streams, prevalent in applications like IoT, financial\nmarkets, and real-time analytics, pose significant challenges due to their high\nvelocity, unbounded nature, and complex inter-dimensional dependencies. Sliding\nwindow techniques are critical for processing such streams, but fixed-size\nwindows struggle to adapt to dynamic changes like concept drift or bursty\npatterns. This paper proposes a novel reinforcement learning (RL)-based\napproach to dynamically optimize sliding window sizes for multi-dimensional\ndata streams. By formulating window size selection as an RL problem, we enable\nan agent to learn an adaptive policy based on stream characteristics, such as\nvariance, correlations, and temporal trends. Our method, RL-Window, leverages a\nDueling Deep Q-Network (DQN) with prioritized experience replay to handle\nnon-stationarity and high-dimensionality. Evaluations on benchmark datasets\n(UCI HAR, PAMAP2, Yahoo! Finance Stream) demonstrate that RL-Window outperforms\nstate-of-the-art methods like ADWIN and CNN-Adaptive in classification\naccuracy, drift robustness, and computational efficiency. Additional\nqualitative analyses, extended metrics (e.g., energy efficiency, latency), and\na comprehensive dataset characterization further highlight its adaptability and\nstability, making it suitable for real-time applications.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u7684\u52a8\u6001\u6ed1\u52a8\u7a97\u53e3\u4f18\u5316\u65b9\u6cd5\uff08RL-Window\uff09\uff0c\u7528\u4e8e\u5904\u7406\u591a\u7ef4\u6570\u636e\u6d41\uff0c\u89e3\u51b3\u4e86\u56fa\u5b9a\u7a97\u53e3\u96be\u4ee5\u9002\u5e94\u52a8\u6001\u53d8\u5316\u7684\u95ee\u9898\u3002", "motivation": "\u591a\u7ef4\u6570\u636e\u6d41\uff08\u5982IoT\u3001\u91d1\u878d\u5e02\u573a\u548c\u5b9e\u65f6\u5206\u6790\uff09\u5177\u6709\u9ad8\u901f\u5ea6\u3001\u65e0\u754c\u6027\u548c\u590d\u6742\u7684\u7ef4\u5ea6\u95f4\u4f9d\u8d56\u5173\u7cfb\uff0c\u56fa\u5b9a\u7a97\u53e3\u96be\u4ee5\u9002\u5e94\u52a8\u6001\u53d8\u5316\uff08\u5982\u6982\u5ff5\u6f02\u79fb\u6216\u7a81\u53d1\u6a21\u5f0f\uff09\uff0c\u9700\u8981\u81ea\u9002\u5e94\u65b9\u6cd5\u3002", "method": "\u5c06\u7a97\u53e3\u5927\u5c0f\u9009\u62e9\u95ee\u9898\u5efa\u6a21\u4e3aRL\u95ee\u9898\uff0c\u5229\u7528Dueling Deep Q-Network\uff08DQN\uff09\u548c\u4f18\u5148\u7ea7\u7ecf\u9a8c\u56de\u653e\uff0c\u5b66\u4e60\u57fa\u4e8e\u6d41\u7279\u5f81\uff08\u5982\u65b9\u5dee\u3001\u76f8\u5173\u6027\u548c\u65f6\u95f4\u8d8b\u52bf\uff09\u7684\u81ea\u9002\u5e94\u7b56\u7565\u3002", "result": "\u5728UCI HAR\u3001PAMAP2\u548cYahoo! Finance Stream\u7b49\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\uff0cRL-Window\u5728\u5206\u7c7b\u7cbe\u5ea6\u3001\u6f02\u79fb\u9c81\u68d2\u6027\u548c\u8ba1\u7b97\u6548\u7387\u4e0a\u4f18\u4e8eADWIN\u548cCNN-Adaptive\u7b49\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "RL-Window\u5177\u6709\u9002\u5e94\u6027\u548c\u7a33\u5b9a\u6027\uff0c\u9002\u7528\u4e8e\u5b9e\u65f6\u5e94\u7528\uff0c\u5e76\u901a\u8fc7\u6269\u5c55\u6307\u6807\uff08\u5982\u80fd\u6548\u548c\u5ef6\u8fdf\uff09\u8fdb\u4e00\u6b65\u9a8c\u8bc1\u4e86\u5176\u4f18\u52bf\u3002"}}
{"id": "2507.06848", "pdf": "https://arxiv.org/pdf/2507.06848", "abs": "https://arxiv.org/abs/2507.06848", "authors": ["Joelle Hanna", "Damian Borth"], "title": "Know Your Attention Maps: Class-specific Token Masking for Weakly Supervised Semantic Segmentation", "categories": ["cs.CV"], "comment": null, "summary": "Weakly Supervised Semantic Segmentation (WSSS) is a challenging problem that\nhas been extensively studied in recent years. Traditional approaches often rely\non external modules like Class Activation Maps to highlight regions of interest\nand generate pseudo segmentation masks. In this work, we propose an end-to-end\nmethod that directly utilizes the attention maps learned by a Vision\nTransformer (ViT) for WSSS. We propose training a sparse ViT with multiple\n[CLS] tokens (one for each class), using a random masking strategy to promote\n[CLS] token - class assignment. At inference time, we aggregate the different\nself-attention maps of each [CLS] token corresponding to the predicted labels\nto generate pseudo segmentation masks. Our proposed approach enhances the\ninterpretability of self-attention maps and ensures accurate class assignments.\nExtensive experiments on two standard benchmarks and three specialized datasets\ndemonstrate that our method generates accurate pseudo-masks, outperforming\nrelated works. Those pseudo-masks can be used to train a segmentation model\nwhich achieves results comparable to fully-supervised models, significantly\nreducing the need for fine-grained labeled data.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8eVision Transformer\u7684\u7aef\u5230\u7aef\u5f31\u76d1\u7763\u8bed\u4e49\u5206\u5272\u65b9\u6cd5\uff0c\u5229\u7528\u591a[CLS]\u4ee4\u724c\u7684\u6ce8\u610f\u529b\u56fe\u751f\u6210\u4f2a\u5206\u5272\u63a9\u7801\uff0c\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u4f20\u7edf\u5f31\u76d1\u7763\u8bed\u4e49\u5206\u5272\u65b9\u6cd5\u4f9d\u8d56\u5916\u90e8\u6a21\u5757\uff08\u5982CAM\uff09\u751f\u6210\u4f2a\u63a9\u7801\uff0c\u672c\u6587\u65e8\u5728\u76f4\u63a5\u5229\u7528ViT\u7684\u6ce8\u610f\u529b\u56fe\uff0c\u63d0\u5347\u6548\u7387\u548c\u51c6\u786e\u6027\u3002", "method": "\u8bad\u7ec3\u7a00\u758fViT\uff0c\u6bcf\u4e2a\u7c7b\u522b\u5bf9\u5e94\u4e00\u4e2a[CLS]\u4ee4\u724c\uff0c\u91c7\u7528\u968f\u673a\u63a9\u7801\u7b56\u7565\u4fc3\u8fdb\u7c7b\u522b\u5206\u914d\uff1b\u63a8\u7406\u65f6\u805a\u5408\u5404[CLS]\u4ee4\u724c\u7684\u81ea\u6ce8\u610f\u529b\u56fe\u751f\u6210\u4f2a\u63a9\u7801\u3002", "result": "\u5728\u591a\u4e2a\u6807\u51c6\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u751f\u6210\u7684\u4f2a\u63a9\u7801\u53ef\u7528\u4e8e\u8bad\u7ec3\u5206\u5272\u6a21\u578b\uff0c\u6027\u80fd\u63a5\u8fd1\u5168\u76d1\u7763\u6a21\u578b\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u663e\u8457\u51cf\u5c11\u7ec6\u7c92\u5ea6\u6807\u6ce8\u9700\u6c42\uff0c\u4e3a\u5f31\u76d1\u7763\u8bed\u4e49\u5206\u5272\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.07060", "pdf": "https://arxiv.org/pdf/2507.07060", "abs": "https://arxiv.org/abs/2507.07060", "authors": ["Shreyas Vinaya Sathyanarayana", "Rahil Shah", "Sharanabasava D. Hiremath", "Rishikesh Panda", "Rahul Jana", "Riya Singh", "Rida Irfan", "Ashwin Murali", "Bharath Ramsundar"], "title": "DeepRetro: Retrosynthetic Pathway Discovery using Iterative LLM Reasoning", "categories": ["q-bio.QM", "cs.AI", "cs.CL", "cs.LG", "q-bio.BM", "q-bio.MN"], "comment": "51 pages,", "summary": "Retrosynthesis, the identification of precursor molecules for a target\ncompound, is pivotal for synthesizing complex molecules, but faces challenges\nin discovering novel pathways beyond predefined templates. Recent large\nlanguage model (LLM) approaches to retrosynthesis have shown promise but\neffectively harnessing LLM reasoning capabilities for effective multi-step\nplanning remains an open question. To address this challenge, we introduce\nDeepRetro, an open-source, iterative, hybrid LLM-based retrosynthetic\nframework. Our approach integrates the strengths of conventional\ntemplate-based/Monte Carlo tree search tools with the generative power of LLMs\nin a step-wise, feedback-driven loop. Initially, synthesis planning is\nattempted with a template-based engine. If this fails, the LLM subsequently\nproposes single-step retrosynthetic disconnections. Crucially, these\nsuggestions undergo rigorous validity, stability, and hallucination checks\nbefore the resulting precursors are recursively fed back into the pipeline for\nfurther evaluation. This iterative refinement allows for dynamic pathway\nexploration and correction. We demonstrate the potential of this pipeline\nthrough benchmark evaluations and case studies, showcasing its ability to\nidentify viable and potentially novel retrosynthetic routes. In particular, we\ndevelop an interactive graphical user interface that allows expert human\nchemists to provide human-in-the-loop feedback to the reasoning algorithm. This\napproach successfully generates novel pathways for complex natural product\ncompounds, demonstrating the potential for iterative LLM reasoning to advance\nstate-of-art in complex chemical syntheses.", "AI": {"tldr": "DeepRetro\u662f\u4e00\u4e2a\u7ed3\u5408LLM\u548c\u4f20\u7edf\u65b9\u6cd5\u7684\u8fed\u4ee3\u5f0f\u9006\u5408\u6210\u6846\u67b6\uff0c\u901a\u8fc7\u52a8\u6001\u8def\u5f84\u63a2\u7d22\u548c\u53cd\u9988\u9a71\u52a8\u5faa\u73af\uff0c\u6709\u6548\u751f\u6210\u65b0\u9896\u7684\u5408\u6210\u8def\u5f84\u3002", "motivation": "\u4f20\u7edf\u9006\u5408\u6210\u65b9\u6cd5\u4f9d\u8d56\u9884\u5b9a\u4e49\u6a21\u677f\uff0c\u96be\u4ee5\u53d1\u73b0\u65b0\u8def\u5f84\uff0c\u800cLLM\u7684\u6f5c\u529b\u5c1a\u672a\u5b8c\u5168\u53d1\u6325\u3002", "method": "\u7ed3\u5408\u6a21\u677f\u5f15\u64ce\u548cLLM\u751f\u6210\u80fd\u529b\uff0c\u901a\u8fc7\u8fed\u4ee3\u53cd\u9988\u5faa\u73af\u9a8c\u8bc1\u548c\u4f18\u5316\u5408\u6210\u8def\u5f84\u3002", "result": "\u5728\u57fa\u51c6\u6d4b\u8bd5\u548c\u6848\u4f8b\u7814\u7a76\u4e2d\u6210\u529f\u8bc6\u522b\u53ef\u884c\u4e14\u65b0\u9896\u7684\u5408\u6210\u8def\u5f84\uff0c\u5e76\u652f\u6301\u4eba\u673a\u4ea4\u4e92\u53cd\u9988\u3002", "conclusion": "DeepRetro\u5c55\u793a\u4e86LLM\u5728\u590d\u6742\u5316\u5b66\u5408\u6210\u4e2d\u7684\u6f5c\u529b\uff0c\u63a8\u52a8\u4e86\u9006\u5408\u6210\u9886\u57df\u7684\u53d1\u5c55\u3002"}}
{"id": "2507.06907", "pdf": "https://arxiv.org/pdf/2507.06907", "abs": "https://arxiv.org/abs/2507.06907", "authors": ["Linyun Gao", "Qiang Wen", "Fumio Machida"], "title": "Robust and Safe Traffic Sign Recognition using N-version with Weighted Voting", "categories": ["cs.LG", "cs.SE"], "comment": "27 pages including appendix, 1 figure", "summary": "Autonomous driving is rapidly advancing as a key application of machine\nlearning, yet ensuring the safety of these systems remains a critical\nchallenge. Traffic sign recognition, an essential component of autonomous\nvehicles, is particularly vulnerable to adversarial attacks that can compromise\ndriving safety. In this paper, we propose an N-version machine learning (NVML)\nframework that integrates a safety-aware weighted soft voting mechanism. Our\napproach utilizes Failure Mode and Effects Analysis (FMEA) to assess potential\nsafety risks and assign dynamic, safety-aware weights to the ensemble outputs.\nWe evaluate the robustness of three-version NVML systems employing various\nvoting mechanisms against adversarial samples generated using the Fast Gradient\nSign Method (FGSM) and Projected Gradient Descent (PGD) attacks. Experimental\nresults demonstrate that our NVML approach significantly enhances the\nrobustness and safety of traffic sign recognition systems under adversarial\nconditions.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eN\u7248\u672c\u673a\u5668\u5b66\u4e60\uff08NVML\uff09\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u5b89\u5168\u611f\u77e5\u52a0\u6743\u8f6f\u6295\u7968\u673a\u5236\u63d0\u5347\u4ea4\u901a\u6807\u5fd7\u8bc6\u522b\u7cfb\u7edf\u5728\u5bf9\u6297\u653b\u51fb\u4e0b\u7684\u9c81\u68d2\u6027\u548c\u5b89\u5168\u6027\u3002", "motivation": "\u81ea\u52a8\u9a7e\u9a76\u4e2d\u4ea4\u901a\u6807\u5fd7\u8bc6\u522b\u6613\u53d7\u5bf9\u6297\u653b\u51fb\u5f71\u54cd\uff0c\u5a01\u80c1\u9a7e\u9a76\u5b89\u5168\uff0c\u9700\u63d0\u5347\u7cfb\u7edf\u9c81\u68d2\u6027\u3002", "method": "\u91c7\u7528N\u7248\u672c\u673a\u5668\u5b66\u4e60\u6846\u67b6\uff0c\u7ed3\u5408FMEA\u8bc4\u4f30\u5b89\u5168\u98ce\u9669\uff0c\u52a8\u6001\u5206\u914d\u6743\u91cd\uff0c\u6d4b\u8bd5\u4e86\u4e09\u79cd\u6295\u7968\u673a\u5236\u5bf9\u6297FGSM\u548cPGD\u653b\u51fb\u7684\u6548\u679c\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cNVML\u663e\u8457\u63d0\u5347\u4e86\u4ea4\u901a\u6807\u5fd7\u8bc6\u522b\u7cfb\u7edf\u5728\u5bf9\u6297\u6761\u4ef6\u4e0b\u7684\u9c81\u68d2\u6027\u548c\u5b89\u5168\u6027\u3002", "conclusion": "NVML\u6846\u67b6\u6709\u6548\u589e\u5f3a\u4e86\u5bf9\u6297\u653b\u51fb\u4e0b\u7684\u7cfb\u7edf\u5b89\u5168\u6027\uff0c\u4e3a\u81ea\u52a8\u9a7e\u9a76\u5b89\u5168\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2507.06856", "pdf": "https://arxiv.org/pdf/2507.06856", "abs": "https://arxiv.org/abs/2507.06856", "authors": ["Subrat Kishore Dutta", "Xiao Zhang"], "title": "IAP: Invisible Adversarial Patch Attack through Perceptibility-Aware Localization and Perturbation Optimization", "categories": ["cs.CV", "cs.AI"], "comment": "Published in ICCV 2025", "summary": "Despite modifying only a small localized input region, adversarial patches\ncan drastically change the prediction of computer vision models. However, prior\nmethods either cannot perform satisfactorily under targeted attack scenarios or\nfail to produce contextually coherent adversarial patches, causing them to be\neasily noticeable by human examiners and insufficiently stealthy against\nautomatic patch defenses. In this paper, we introduce IAP, a novel attack\nframework that generates highly invisible adversarial patches based on\nperceptibility-aware localization and perturbation optimization schemes.\nSpecifically, IAP first searches for a proper location to place the patch by\nleveraging classwise localization and sensitivity maps, balancing the\nsusceptibility of patch location to both victim model prediction and human\nvisual system, then employs a perceptibility-regularized adversarial loss and a\ngradient update rule that prioritizes color constancy for optimizing invisible\nperturbations. Comprehensive experiments across various image benchmarks and\nmodel architectures demonstrate that IAP consistently achieves competitive\nattack success rates in targeted settings with significantly improved patch\ninvisibility compared to existing baselines. In addition to being highly\nimperceptible to humans, IAP is shown to be stealthy enough to render several\nstate-of-the-art patch defenses ineffective.", "AI": {"tldr": "IAP\u662f\u4e00\u79cd\u65b0\u578b\u653b\u51fb\u6846\u67b6\uff0c\u901a\u8fc7\u611f\u77e5\u611f\u77e5\u5b9a\u4f4d\u548c\u6270\u52a8\u4f18\u5316\u65b9\u6848\u751f\u6210\u9ad8\u5ea6\u4e0d\u53ef\u89c1\u7684\u5bf9\u6297\u6027\u8865\u4e01\uff0c\u663e\u8457\u63d0\u5347\u9690\u853d\u6027\u548c\u653b\u51fb\u6210\u529f\u7387\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u9488\u5bf9\u6027\u653b\u51fb\u573a\u666f\u4e0b\u8868\u73b0\u4e0d\u4f73\u6216\u751f\u6210\u7684\u8865\u4e01\u4e0d\u8fde\u8d2f\uff0c\u5bb9\u6613\u88ab\u4eba\u7c7b\u6216\u81ea\u52a8\u9632\u5fa1\u7cfb\u7edf\u53d1\u73b0\u3002", "method": "IAP\u7ed3\u5408\u7c7b\u5b9a\u4f4d\u548c\u654f\u611f\u5ea6\u56fe\u9009\u62e9\u8865\u4e01\u4f4d\u7f6e\uff0c\u5e76\u901a\u8fc7\u611f\u77e5\u611f\u77e5\u6b63\u5219\u5316\u5bf9\u6297\u635f\u5931\u548c\u68af\u5ea6\u66f4\u65b0\u89c4\u5219\u4f18\u5316\u6270\u52a8\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cIAP\u5728\u591a\u79cd\u57fa\u51c6\u548c\u6a21\u578b\u67b6\u6784\u4e2d\u5747\u5b9e\u73b0\u9ad8\u653b\u51fb\u6210\u529f\u7387\uff0c\u4e14\u8865\u4e01\u9690\u853d\u6027\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "IAP\u4e0d\u4ec5\u5bf9\u4eba\u7c7b\u9ad8\u5ea6\u4e0d\u53ef\u89c1\uff0c\u8fd8\u80fd\u6709\u6548\u89c4\u907f\u591a\u79cd\u5148\u8fdb\u8865\u4e01\u9632\u5fa1\u7cfb\u7edf\u3002"}}
{"id": "2507.06858", "pdf": "https://arxiv.org/pdf/2507.06858", "abs": "https://arxiv.org/abs/2507.06858", "authors": ["Mathias Schulz", "Alexander Spenke", "Pia Funk", "Florian Bl\u00fcmel", "Markus Rohde", "Ralph Breithaupt", "Gerd Nolden", "Norbert Jung", "Robert Lange"], "title": "Longitudinal Study of Facial Biometrics at the BEZ: Temporal Variance Analysis", "categories": ["cs.CV"], "comment": "11 pages, 10 figures, 8 tables", "summary": "This study presents findings from long-term biometric evaluations conducted\nat the Biometric Evaluation Center (bez). Over the course of two and a half\nyears, our ongoing research with over 400 participants representing diverse\nethnicities, genders, and age groups were regularly assessed using a variety of\nbiometric tools and techniques at the controlled testing facilities. Our\nfindings are based on the General Data Protection Regulation-compliant local\nbez database with more than 238.000 biometric data sets categorized into\nmultiple biometric modalities such as face and finger. We used state-of-the-art\nface recognition algorithms to analyze long-term comparison scores. Our results\nshow that these scores fluctuate more significantly between individual days\nthan over the entire measurement period. These findings highlight the\nimportance of testing biometric characteristics of the same individuals over a\nlonger period of time in a controlled measurement environment and lays the\ngroundwork for future advancements in biometric data analysis.", "AI": {"tldr": "\u957f\u671f\u751f\u7269\u7279\u5f81\u8bc4\u4f30\u663e\u793a\uff0c\u4e2a\u4f53\u95f4\u7684\u65e5\u95f4\u5206\u6570\u6ce2\u52a8\u6bd4\u6574\u4e2a\u6d4b\u91cf\u671f\u95f4\u66f4\u5927\uff0c\u5f3a\u8c03\u957f\u671f\u6d4b\u8bd5\u7684\u91cd\u8981\u6027\u3002", "motivation": "\u7814\u7a76\u65e8\u5728\u901a\u8fc7\u957f\u671f\u751f\u7269\u7279\u5f81\u8bc4\u4f30\uff0c\u63ed\u793a\u4e2a\u4f53\u5728\u4e0d\u540c\u65f6\u95f4\u70b9\u7684\u751f\u7269\u7279\u5f81\u53d8\u5316\uff0c\u4e3a\u751f\u7269\u8bc6\u522b\u6280\u672f\u63d0\u4f9b\u66f4\u53ef\u9760\u7684\u6570\u636e\u652f\u6301\u3002", "method": "\u4f7f\u7528\u591a\u79cd\u751f\u7269\u8bc6\u522b\u5de5\u5177\u548c\u6280\u672f\uff0c\u5bf9400\u591a\u540d\u53c2\u4e0e\u8005\u8fdb\u884c\u4e24\u5e74\u534a\u7684\u5b9a\u671f\u8bc4\u4f30\uff0c\u5e76\u5229\u7528\u5148\u8fdb\u7684\u4eba\u8138\u8bc6\u522b\u7b97\u6cd5\u5206\u6790\u957f\u671f\u6bd4\u8f83\u5206\u6570\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff0c\u4e2a\u4f53\u95f4\u7684\u65e5\u95f4\u751f\u7269\u7279\u5f81\u5206\u6570\u6ce2\u52a8\u663e\u8457\u5927\u4e8e\u6574\u4e2a\u6d4b\u91cf\u671f\u95f4\u7684\u6ce2\u52a8\u3002", "conclusion": "\u957f\u671f\u6d4b\u8bd5\u5728\u53d7\u63a7\u73af\u5883\u4e2d\u5bf9\u751f\u7269\u7279\u5f81\u5206\u6790\u81f3\u5173\u91cd\u8981\uff0c\u4e3a\u672a\u6765\u6280\u672f\u8fdb\u6b65\u5960\u5b9a\u57fa\u7840\u3002"}}
{"id": "2507.06952", "pdf": "https://arxiv.org/pdf/2507.06952", "abs": "https://arxiv.org/abs/2507.06952", "authors": ["Keyon Vafa", "Peter G. Chang", "Ashesh Rambachan", "Sendhil Mullainathan"], "title": "What Has a Foundation Model Found? Using Inductive Bias to Probe for World Models", "categories": ["cs.LG", "cs.AI"], "comment": "To appear in ICML 2025", "summary": "Foundation models are premised on the idea that sequence prediction can\nuncover deeper domain understanding, much like how Kepler's predictions of\nplanetary motion later led to the discovery of Newtonian mechanics. However,\nevaluating whether these models truly capture deeper structure remains a\nchallenge. We develop a technique for evaluating foundation models that\nexamines how they adapt to synthetic datasets generated from some postulated\nworld model. Our technique measures whether the foundation model's inductive\nbias aligns with the world model, and so we refer to it as an inductive bias\nprobe. Across multiple domains, we find that foundation models can excel at\ntheir training tasks yet fail to develop inductive biases towards the\nunderlying world model when adapted to new tasks. We particularly find that\nfoundation models trained on orbital trajectories consistently fail to apply\nNewtonian mechanics when adapted to new physics tasks. Further analysis reveals\nthat these models behave as if they develop task-specific heuristics that fail\nto generalize.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u8bc4\u4f30\u57fa\u7840\u6a21\u578b\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u751f\u6210\u5408\u6210\u6570\u636e\u96c6\u6d4b\u8bd5\u6a21\u578b\u662f\u5426\u771f\u6b63\u7406\u89e3\u5e95\u5c42\u4e16\u754c\u6a21\u578b\uff0c\u53d1\u73b0\u6a21\u578b\u867d\u80fd\u5b8c\u6210\u8bad\u7ec3\u4efb\u52a1\uff0c\u4f46\u672a\u80fd\u53d1\u5c55\u51fa\u4e0e\u5e95\u5c42\u6a21\u578b\u4e00\u81f4\u7684\u5f52\u7eb3\u504f\u7f6e\u3002", "motivation": "\u8bc4\u4f30\u57fa\u7840\u6a21\u578b\u662f\u5426\u771f\u6b63\u6355\u6349\u5230\u66f4\u6df1\u5c42\u6b21\u7684\u7ed3\u6784\uff0c\u800c\u4e0d\u4ec5\u4ec5\u662f\u5b8c\u6210\u8868\u9762\u4efb\u52a1\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u79cd\u79f0\u4e3a\u201c\u5f52\u7eb3\u504f\u7f6e\u63a2\u9488\u201d\u7684\u6280\u672f\uff0c\u901a\u8fc7\u5408\u6210\u6570\u636e\u96c6\u6d4b\u8bd5\u6a21\u578b\u7684\u9002\u5e94\u80fd\u529b\u3002", "result": "\u57fa\u7840\u6a21\u578b\u5728\u8bad\u7ec3\u4efb\u52a1\u4e0a\u8868\u73b0\u4f18\u79c0\uff0c\u4f46\u5728\u65b0\u4efb\u52a1\u4e2d\u672a\u80fd\u53d1\u5c55\u51fa\u4e0e\u5e95\u5c42\u4e16\u754c\u6a21\u578b\u4e00\u81f4\u7684\u5f52\u7eb3\u504f\u7f6e\uff0c\u5c24\u5176\u662f\u5728\u8f68\u9053\u8f68\u8ff9\u4efb\u52a1\u4e2d\u672a\u80fd\u5e94\u7528\u725b\u987f\u529b\u5b66\u3002", "conclusion": "\u57fa\u7840\u6a21\u578b\u53ef\u80fd\u4ec5\u53d1\u5c55\u51fa\u4efb\u52a1\u7279\u5b9a\u7684\u542f\u53d1\u5f0f\u65b9\u6cd5\uff0c\u7f3a\u4e4f\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2507.06906", "pdf": "https://arxiv.org/pdf/2507.06906", "abs": "https://arxiv.org/abs/2507.06906", "authors": ["Matthias Zeller", "Daniel Casado Herraez", "Bengisu Ayan", "Jens Behley", "Michael Heidingsfeld", "Cyrill Stachniss"], "title": "SemRaFiner: Panoptic Segmentation in Sparse and Noisy Radar Point Clouds", "categories": ["cs.CV"], "comment": "Accepted for publication in IEEE Robotics and Automation Letters\n  (RA-L)", "summary": "Semantic scene understanding, including the perception and classification of\nmoving agents, is essential to enabling safe and robust driving behaviours of\nautonomous vehicles. Cameras and LiDARs are commonly used for semantic scene\nunderstanding. However, both sensor modalities face limitations in adverse\nweather and usually do not provide motion information. Radar sensors overcome\nthese limitations and directly offer information about moving agents by\nmeasuring the Doppler velocity, but the measurements are comparably sparse and\nnoisy. In this paper, we address the problem of panoptic segmentation in sparse\nradar point clouds to enhance scene understanding. Our approach, called\nSemRaFiner, accounts for changing density in sparse radar point clouds and\noptimizes the feature extraction to improve accuracy. Furthermore, we propose\nan optimized training procedure to refine instance assignments by incorporating\na dedicated data augmentation. Our experiments suggest that our approach\noutperforms state-of-the-art methods for radar-based panoptic segmentation.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aSemRaFiner\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u6539\u8fdb\u7a00\u758f\u96f7\u8fbe\u70b9\u4e91\u7684\u6cdb\u5149\u5206\u5272\uff0c\u4ee5\u589e\u5f3a\u573a\u666f\u7406\u89e3\u3002", "motivation": "\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\u9700\u8981\u8bed\u4e49\u573a\u666f\u7406\u89e3\uff0c\u4f46\u6444\u50cf\u5934\u548cLiDAR\u5728\u6076\u52a3\u5929\u6c14\u4e0b\u8868\u73b0\u4e0d\u4f73\uff0c\u4e14\u65e0\u6cd5\u63d0\u4f9b\u8fd0\u52a8\u4fe1\u606f\u3002\u96f7\u8fbe\u4f20\u611f\u5668\u867d\u80fd\u514b\u670d\u8fd9\u4e9b\u9650\u5236\uff0c\u4f46\u5176\u6d4b\u91cf\u7a00\u758f\u4e14\u566a\u58f0\u5927\u3002", "method": "SemRaFiner\u65b9\u6cd5\u4f18\u5316\u4e86\u7a00\u758f\u96f7\u8fbe\u70b9\u4e91\u7684\u7279\u5f81\u63d0\u53d6\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u6539\u8fdb\u7684\u8bad\u7ec3\u6d41\u7a0b\uff0c\u901a\u8fc7\u4e13\u95e8\u7684\u6570\u636e\u589e\u5f3a\u6765\u7ec6\u5316\u5b9e\u4f8b\u5206\u914d\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u96f7\u8fbe\u6cdb\u5149\u5206\u5272\u4efb\u52a1\u4e2d\u4f18\u4e8e\u73b0\u6709\u6280\u672f\u3002", "conclusion": "SemRaFiner\u901a\u8fc7\u4f18\u5316\u7279\u5f81\u63d0\u53d6\u548c\u8bad\u7ec3\u6d41\u7a0b\uff0c\u663e\u8457\u63d0\u5347\u4e86\u96f7\u8fbe\u70b9\u4e91\u7684\u573a\u666f\u7406\u89e3\u80fd\u529b\u3002"}}
{"id": "2507.06967", "pdf": "https://arxiv.org/pdf/2507.06967", "abs": "https://arxiv.org/abs/2507.06967", "authors": ["Sebastien Andre-Sloan", "Anirbit Mukherjee", "Matthew Colbrook"], "title": "Noisy PDE Training Requires Bigger PINNs", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Physics-Informed Neural Networks (PINNs) are increasingly used to approximate\nsolutions of partial differential equations (PDEs), especially in high\ndimensions. In real-world applications, data samples are noisy, so it is\nimportant to know when a predictor can still achieve low empirical risk.\nHowever, little is known about the conditions under which a PINN can do so\neffectively. We prove a lower bound on the size of neural networks required for\nthe supervised PINN empirical risk to fall below the variance of noisy\nsupervision labels. Specifically, if a predictor achieves an empirical risk\n$O(\\eta)$ below $\\sigma^2$ (variance of supervision data), then necessarily\n$d_N\\log d_N\\gtrsim N_s \\eta^2$, where $N_s$ is the number of samples and $d_N$\nis the number of trainable parameters of the PINN. A similar constraint applies\nto the fully unsupervised PINN setting when boundary labels are sampled\nnoisily. Consequently, increasing the number of noisy supervision labels alone\ndoes not provide a ``free lunch'' in reducing empirical risk. We also show\nempirically that PINNs can indeed achieve empirical risks below $\\sigma^2$\nunder such conditions. As a case study, we investigate PINNs applied to the\nHamilton--Jacobi--Bellman (HJB) PDE. Our findings lay the groundwork for\nquantitatively understanding the parameter requirements for training PINNs in\nthe presence of noise.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u5728\u566a\u58f0\u6570\u636e\u4e0b\uff0c\u7269\u7406\u4fe1\u606f\u795e\u7ecf\u7f51\u7edc\uff08PINNs\uff09\u903c\u8fd1\u504f\u5fae\u5206\u65b9\u7a0b\uff08PDEs\uff09\u89e3\u7684\u80fd\u529b\uff0c\u5e76\u8bc1\u660e\u4e86\u7f51\u7edc\u89c4\u6a21\u4e0e\u76d1\u7763\u6807\u7b7e\u566a\u58f0\u65b9\u5dee\u7684\u5173\u7cfb\u3002", "motivation": "\u63a2\u8ba8\u5728\u566a\u58f0\u6570\u636e\u6761\u4ef6\u4e0b\uff0cPINNs\u5982\u4f55\u6709\u6548\u964d\u4f4e\u7ecf\u9a8c\u98ce\u9669\uff0c\u586b\u8865\u4e86\u76f8\u5173\u7406\u8bba\u7a7a\u767d\u3002", "method": "\u901a\u8fc7\u7406\u8bba\u5206\u6790\uff0c\u8bc1\u660e\u4e86\u795e\u7ecf\u7f51\u7edc\u89c4\u6a21\u4e0e\u76d1\u7763\u6807\u7b7e\u566a\u58f0\u65b9\u5dee\u7684\u4e0b\u754c\u5173\u7cfb\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u7ed3\u8bba\u3002", "result": "\u53d1\u73b0\u589e\u52a0\u566a\u58f0\u76d1\u7763\u6807\u7b7e\u6570\u91cf\u5e76\u4e0d\u80fd\u65e0\u9650\u5236\u964d\u4f4e\u7ecf\u9a8c\u98ce\u9669\uff0c\u4e14PINNs\u5728\u7279\u5b9a\u6761\u4ef6\u4e0b\u53ef\u5b9e\u73b0\u4f4e\u4e8e\u566a\u58f0\u65b9\u5dee\u7684\u7ecf\u9a8c\u98ce\u9669\u3002", "conclusion": "\u4e3a\u7406\u89e3\u566a\u58f0\u73af\u5883\u4e0b\u8bad\u7ec3PINNs\u7684\u53c2\u6570\u9700\u6c42\u63d0\u4f9b\u4e86\u7406\u8bba\u57fa\u7840\uff0c\u5e76\u4ee5HJB PDE\u4e3a\u4f8b\u9a8c\u8bc1\u4e86\u7ed3\u8bba\u3002"}}
{"id": "2507.06928", "pdf": "https://arxiv.org/pdf/2507.06928", "abs": "https://arxiv.org/abs/2507.06928", "authors": ["Qiyuan Dai", "Hanzhuo Huang", "Yu Wu", "Sibei Yang"], "title": "Adaptive Part Learning for Fine-Grained Generalized Category Discovery: A Plug-and-Play Enhancement", "categories": ["cs.CV"], "comment": "Accepted to CVPR 2025", "summary": "Generalized Category Discovery (GCD) aims to recognize unlabeled images from\nknown and novel classes by distinguishing novel classes from known ones, while\nalso transferring knowledge from another set of labeled images with known\nclasses. Existing GCD methods rely on self-supervised vision transformers such\nas DINO for representation learning. However, focusing solely on the global\nrepresentation of the DINO CLS token introduces an inherent trade-off between\ndiscriminability and generalization. In this paper, we introduce an adaptive\npart discovery and learning method, called APL, which generates consistent\nobject parts and their correspondences across different similar images using a\nset of shared learnable part queries and DINO part priors, without requiring\nany additional annotations. More importantly, we propose a novel all-min\ncontrastive loss to learn discriminative yet generalizable part representation,\nwhich adaptively highlights discriminative object parts to distinguish similar\ncategories for enhanced discriminability while simultaneously sharing other\nparts to facilitate knowledge transfer for improved generalization. Our APL can\neasily be incorporated into different GCD frameworks by replacing their CLS\ntoken feature with our part representations, showing significant enhancements\non fine-grained datasets.", "AI": {"tldr": "APL\u65b9\u6cd5\u901a\u8fc7\u81ea\u9002\u5e94\u90e8\u5206\u53d1\u73b0\u548c\u5b66\u4e60\uff0c\u5229\u7528\u5171\u4eab\u53ef\u5b66\u4e60\u90e8\u5206\u67e5\u8be2\u548cDINO\u90e8\u5206\u5148\u9a8c\u751f\u6210\u4e00\u81f4\u7684\u5bf9\u8c61\u90e8\u5206\u53ca\u5176\u5bf9\u5e94\u5173\u7cfb\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684all-min\u5bf9\u6bd4\u635f\u5931\uff0c\u4ee5\u5b66\u4e60\u66f4\u5177\u533a\u5206\u6027\u548c\u6cdb\u5316\u6027\u7684\u90e8\u5206\u8868\u793a\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709GCD\u65b9\u6cd5\u5728\u5168\u5c40\u8868\u793a\u5b66\u4e60\u4e2d\u5b58\u5728\u7684\u533a\u5206\u6027\u548c\u6cdb\u5316\u6027\u4e4b\u95f4\u7684\u6743\u8861\u95ee\u9898\u3002", "method": "\u63d0\u51faAPL\u65b9\u6cd5\uff0c\u7ed3\u5408\u5171\u4eab\u90e8\u5206\u67e5\u8be2\u548cDINO\u90e8\u5206\u5148\u9a8c\uff0c\u751f\u6210\u4e00\u81f4\u7684\u5bf9\u8c61\u90e8\u5206\u53ca\u5176\u5bf9\u5e94\u5173\u7cfb\uff0c\u5e76\u4f7f\u7528all-min\u5bf9\u6bd4\u635f\u5931\u5b66\u4e60\u90e8\u5206\u8868\u793a\u3002", "result": "APL\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u7ec6\u7c92\u5ea6\u6570\u636e\u96c6\u4e0a\u7684\u6027\u80fd\u3002", "conclusion": "APL\u65b9\u6cd5\u901a\u8fc7\u81ea\u9002\u5e94\u90e8\u5206\u5b66\u4e60\u548c\u5bf9\u6bd4\u635f\u5931\uff0c\u6709\u6548\u5e73\u8861\u4e86\u533a\u5206\u6027\u548c\u6cdb\u5316\u6027\uff0c\u9002\u7528\u4e8e\u591a\u79cdGCD\u6846\u67b6\u3002"}}
{"id": "2507.06969", "pdf": "https://arxiv.org/pdf/2507.06969", "abs": "https://arxiv.org/abs/2507.06969", "authors": ["Bogdan Kulynych", "Juan Felipe Gomez", "Georgios Kaissis", "Jamie Hayes", "Borja Balle", "Flavio du Pin Calmon", "Jean Louis Raisaro"], "title": "Unifying Re-Identification, Attribute Inference, and Data Reconstruction Risks in Differential Privacy", "categories": ["cs.LG", "cs.AI", "cs.CR", "cs.CY", "stat.ML"], "comment": null, "summary": "Differentially private (DP) mechanisms are difficult to interpret and\ncalibrate because existing methods for mapping standard privacy parameters to\nconcrete privacy risks -- re-identification, attribute inference, and data\nreconstruction -- are both overly pessimistic and inconsistent. In this work,\nwe use the hypothesis-testing interpretation of DP ($f$-DP), and determine that\nbounds on attack success can take the same unified form across\nre-identification, attribute inference, and data reconstruction risks. Our\nunified bounds are (1) consistent across a multitude of attack settings, and\n(2) tunable, enabling practitioners to evaluate risk with respect to arbitrary\n(including worst-case) levels of baseline risk. Empirically, our results are\ntighter than prior methods using $\\varepsilon$-DP, R\\'enyi DP, and concentrated\nDP. As a result, calibrating noise using our bounds can reduce the required\nnoise by 20% at the same risk level, which yields, e.g., more than 15pp\naccuracy increase in a text classification task. Overall, this unifying\nperspective provides a principled framework for interpreting and calibrating\nthe degree of protection in DP against specific levels of re-identification,\nattribute inference, or data reconstruction risk.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5047\u8bbe\u68c0\u9a8c\u89e3\u91ca\u7684\u5dee\u5206\u9690\u79c1\uff08f-DP\uff09\u65b9\u6cd5\uff0c\u7edf\u4e00\u4e86\u91cd\u8bc6\u522b\u3001\u5c5e\u6027\u63a8\u65ad\u548c\u6570\u636e\u91cd\u6784\u98ce\u9669\u7684\u754c\u9650\uff0c\u63d0\u4f9b\u4e86\u66f4\u7d27\u81f4\u7684\u9690\u79c1\u4fdd\u62a4\u8bc4\u4f30\u548c\u6821\u51c6\u6846\u67b6\u3002", "motivation": "\u73b0\u6709\u5dee\u5206\u9690\u79c1\u673a\u5236\u5728\u9690\u79c1\u53c2\u6570\u4e0e\u5177\u4f53\u98ce\u9669\uff08\u5982\u91cd\u8bc6\u522b\u3001\u5c5e\u6027\u63a8\u65ad\u548c\u6570\u636e\u91cd\u6784\uff09\u4e4b\u95f4\u7684\u6620\u5c04\u8fc7\u4e8e\u60b2\u89c2\u4e14\u4e0d\u4e00\u81f4\uff0c\u96be\u4ee5\u89e3\u91ca\u548c\u6821\u51c6\u3002", "method": "\u5229\u7528f-DP\u7684\u5047\u8bbe\u68c0\u9a8c\u89e3\u91ca\uff0c\u63a8\u5bfc\u51fa\u7edf\u4e00\u7684\u653b\u51fb\u6210\u529f\u754c\u9650\uff0c\u9002\u7528\u4e8e\u591a\u79cd\u653b\u51fb\u573a\u666f\uff0c\u5e76\u53ef\u8c03\u6574\u4ee5\u8bc4\u4f30\u4efb\u610f\u57fa\u7ebf\u98ce\u9669\u6c34\u5e73\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u6bd4\u03b5-DP\u3001R\u00e9nyi DP\u548c\u96c6\u4e2dDP\u66f4\u7d27\u81f4\uff0c\u566a\u58f0\u6821\u51c6\u53ef\u51cf\u5c1120%\u7684\u566a\u58f0\u9700\u6c42\uff0c\u5e76\u5728\u6587\u672c\u5206\u7c7b\u4efb\u52a1\u4e2d\u63d0\u9ad815%\u4ee5\u4e0a\u7684\u51c6\u786e\u7387\u3002", "conclusion": "\u8be5\u7edf\u4e00\u89c6\u89d2\u4e3a\u5dee\u5206\u9690\u79c1\u7684\u4fdd\u62a4\u7a0b\u5ea6\u63d0\u4f9b\u4e86\u539f\u5219\u6027\u6846\u67b6\uff0c\u80fd\u9488\u5bf9\u7279\u5b9a\u98ce\u9669\u6c34\u5e73\u8fdb\u884c\u89e3\u91ca\u548c\u6821\u51c6\u3002"}}
{"id": "2507.06948", "pdf": "https://arxiv.org/pdf/2507.06948", "abs": "https://arxiv.org/abs/2507.06948", "authors": ["Yixin Zhao", "Yuyi Zhang", "Lianwen Jin"], "title": "MCCD: A Multi-Attribute Chinese Calligraphy Character Dataset Annotated with Script Styles, Dynasties, and Calligraphers", "categories": ["cs.CV"], "comment": "17 pages, 8 figures, 9 tables, accepted by the 19th International\n  Conference on Document Analysis and Recognition (ICDAR 2025)", "summary": "Research on the attribute information of calligraphy, such as styles,\ndynasties, and calligraphers, holds significant cultural and historical value.\nHowever, the styles of Chinese calligraphy characters have evolved dramatically\nthrough different dynasties and the unique touches of calligraphers, making it\nhighly challenging to accurately recognize these different characters and their\nattributes. Furthermore, existing calligraphic datasets are extremely scarce,\nand most provide only character-level annotations without additional attribute\ninformation. This limitation has significantly hindered the in-depth study of\nChinese calligraphy. To fill this gap, we present a novel Multi-Attribute\nChinese Calligraphy Character Dataset (MCCD). The dataset encompasses 7,765\ncategories with a total of 329,715 isolated image samples of Chinese\ncalligraphy characters, and three additional subsets were extracted based on\nthe attribute labeling of the three types of script styles (10 types),\ndynasties (15 periods) and calligraphers (142 individuals). The rich\nmulti-attribute annotations render MCCD well-suited diverse research tasks,\nincluding calligraphic character recognition, writer identification, and\nevolutionary studies of Chinese characters. We establish benchmark performance\nthrough single-task and multi-task recognition experiments across MCCD and all\nof its subsets. The experimental results demonstrate that the complexity of the\nstroke structure of the calligraphic characters, and the interplay between\ntheir different attributes, leading to a substantial increase in the difficulty\nof accurate recognition. MCCD not only fills a void in the availability of\ndetailed calligraphy datasets but also provides valuable resources for\nadvancing research in Chinese calligraphy and fostering advancements in\nmultiple fields. The dataset is available at\nhttps://github.com/SCUT-DLVCLab/MCCD.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u591a\u5c5e\u6027\u4e2d\u56fd\u4e66\u6cd5\u5b57\u7b26\u6570\u636e\u96c6\uff08MCCD\uff09\uff0c\u586b\u8865\u4e86\u73b0\u6709\u6570\u636e\u96c6\u7a00\u7f3a\u4e14\u7f3a\u4e4f\u5c5e\u6027\u4fe1\u606f\u7684\u7a7a\u767d\uff0c\u4e3a\u4e66\u6cd5\u5b57\u7b26\u8bc6\u522b\u3001\u4e66\u6cd5\u5bb6\u9274\u5b9a\u7b49\u7814\u7a76\u63d0\u4f9b\u4e86\u4e30\u5bcc\u8d44\u6e90\u3002", "motivation": "\u7814\u7a76\u4e66\u6cd5\u5c5e\u6027\u4fe1\u606f\uff08\u5982\u98ce\u683c\u3001\u671d\u4ee3\u3001\u4e66\u6cd5\u5bb6\uff09\u5177\u6709\u91cd\u8981\u6587\u5316\u5386\u53f2\u4ef7\u503c\uff0c\u4f46\u73b0\u6709\u6570\u636e\u96c6\u7a00\u7f3a\u4e14\u7f3a\u4e4f\u5c5e\u6027\u6807\u6ce8\uff0c\u963b\u788d\u4e86\u6df1\u5165\u7814\u7a76\u3002", "method": "\u6784\u5efa\u4e86\u5305\u542b7,765\u7c7b329,715\u4e2a\u4e66\u6cd5\u5b57\u7b26\u56fe\u50cf\u7684MCCD\u6570\u636e\u96c6\uff0c\u5e76\u57fa\u4e8e\u811a\u672c\u98ce\u683c\u3001\u671d\u4ee3\u548c\u4e66\u6cd5\u5bb6\u5c5e\u6027\u63d0\u53d6\u4e86\u4e09\u4e2a\u5b50\u96c6\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u4e66\u6cd5\u5b57\u7b26\u7684\u7b14\u753b\u7ed3\u6784\u590d\u6742\u6027\u548c\u5c5e\u6027\u95f4\u7684\u4ea4\u4e92\u4f5c\u7528\u663e\u8457\u589e\u52a0\u4e86\u51c6\u786e\u8bc6\u522b\u7684\u96be\u5ea6\u3002", "conclusion": "MCCD\u586b\u8865\u4e86\u8be6\u7ec6\u4e66\u6cd5\u6570\u636e\u96c6\u7684\u7a7a\u767d\uff0c\u4e3a\u591a\u9886\u57df\u7814\u7a76\u63d0\u4f9b\u4e86\u5b9d\u8d35\u8d44\u6e90\u3002"}}
{"id": "2507.06979", "pdf": "https://arxiv.org/pdf/2507.06979", "abs": "https://arxiv.org/abs/2507.06979", "authors": ["Panagiotis Koromilas", "Efthymios Georgiou", "Giorgos Bouritsas", "Theodoros Giannakopoulos", "Mihalis A. Nicolaou", "Yannis Panagakis"], "title": "A Principled Framework for Multi-View Contrastive Learning", "categories": ["cs.LG", "cs.CV"], "comment": null, "summary": "Contrastive Learning (CL), a leading paradigm in Self-Supervised Learning\n(SSL), typically relies on pairs of data views generated through augmentation.\nWhile multiple augmentations per instance (more than two) improve\ngeneralization in supervised learning, current CL methods handle additional\nviews suboptimally by simply aggregating different pairwise objectives. This\napproach suffers from four critical limitations: (L1) it utilizes multiple\noptimization terms per data point resulting to conflicting objectives, (L2) it\nfails to model all interactions across views and data points, (L3) it inherits\nfundamental limitations (e.g. alignment-uniformity coupling) from pairwise CL\nlosses, and (L4) it prevents fully realizing the benefits of increased view\nmultiplicity observed in supervised settings. We address these limitations\nthrough two novel loss functions: MV-InfoNCE, which extends InfoNCE to\nincorporate all possible view interactions simultaneously in one term per data\npoint, and MV-DHEL, which decouples alignment from uniformity across views\nwhile scaling interaction complexity with view multiplicity. Both approaches\nare theoretically grounded - we prove they asymptotically optimize for\nalignment of all views and uniformity, providing principled extensions to\nmulti-view contrastive learning. Our empirical results on ImageNet1K and three\nother datasets demonstrate that our methods consistently outperform existing\nmulti-view approaches and effectively scale with increasing view multiplicity.\nWe also apply our objectives to multimodal data and show that, in contrast to\nother contrastive objectives, they can scale beyond just two modalities. Most\nsignificantly, ablation studies reveal that MV-DHEL with five or more views\neffectively mitigates dimensionality collapse by fully utilizing the embedding\nspace, thereby delivering multi-view benefits observed in supervised learning.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e24\u79cd\u65b0\u7684\u635f\u5931\u51fd\u6570\uff08MV-InfoNCE\u548cMV-DHEL\uff09\uff0c\u89e3\u51b3\u4e86\u591a\u89c6\u56fe\u5bf9\u6bd4\u5b66\u4e60\u4e2d\u5b58\u5728\u7684\u56db\u4e2a\u5173\u952e\u95ee\u9898\uff0c\u5e76\u5728\u5b9e\u9a8c\u4e2d\u9a8c\u8bc1\u4e86\u5176\u4f18\u8d8a\u6027\u548c\u53ef\u6269\u5c55\u6027\u3002", "motivation": "\u5f53\u524d\u591a\u89c6\u56fe\u5bf9\u6bd4\u5b66\u4e60\u65b9\u6cd5\u5b58\u5728\u56db\u4e2a\u5173\u952e\u5c40\u9650\u6027\uff08\u5982\u76ee\u6807\u51b2\u7a81\u3001\u89c6\u56fe\u4ea4\u4e92\u5efa\u6a21\u4e0d\u8db3\u7b49\uff09\uff0c\u65e0\u6cd5\u5145\u5206\u5229\u7528\u591a\u89c6\u56fe\u7684\u4f18\u52bf\u3002", "method": "\u63d0\u51fa\u4e24\u79cd\u65b0\u635f\u5931\u51fd\u6570\uff1aMV-InfoNCE\uff08\u540c\u65f6\u5efa\u6a21\u6240\u6709\u89c6\u56fe\u4ea4\u4e92\uff09\u548cMV-DHEL\uff08\u89e3\u8026\u5bf9\u9f50\u4e0e\u5747\u5300\u6027\uff09\u3002", "result": "\u5728ImageNet1K\u7b49\u6570\u636e\u96c6\u4e0a\uff0c\u65b0\u65b9\u6cd5\u4f18\u4e8e\u73b0\u6709\u591a\u89c6\u56fe\u65b9\u6cd5\uff0c\u5e76\u80fd\u6269\u5c55\u5230\u591a\u6a21\u6001\u6570\u636e\u3002MV-DHEL\u5728\u4e94\u89c6\u56fe\u4ee5\u4e0a\u80fd\u6709\u6548\u7f13\u89e3\u7ef4\u5ea6\u5d29\u6e83\u3002", "conclusion": "\u65b0\u65b9\u6cd5\u4e3a\u591a\u89c6\u56fe\u5bf9\u6bd4\u5b66\u4e60\u63d0\u4f9b\u4e86\u7406\u8bba\u652f\u6301\uff0c\u5e76\u5c55\u793a\u4e86\u5176\u5728\u6027\u80fd\u548c\u6269\u5c55\u6027\u4e0a\u7684\u4f18\u52bf\u3002"}}
{"id": "2507.06949", "pdf": "https://arxiv.org/pdf/2507.06949", "abs": "https://arxiv.org/abs/2507.06949", "authors": ["Sebastian Fajardo", "Sina Mohammadi", "Jonas Gregorio de Souza", "C\u00e9sar Ardila", "Alan Tapscott Baltar", "Shaddai Heidgen", "Maria Isabel Mayorga Hern\u00e1ndez", "Sylvia Mota de Oliveira", "Fernando Montejo", "Marco Moderato", "Vinicius Peripato", "Katy Puche", "Carlos Reina", "Juan Carlos Vargas", "Frank W. Takes", "Marco Madella"], "title": "Pre-Columbian Settlements Shaped Palm Clusters in the Sierra Nevada de Santa Marta, Colombia", "categories": ["cs.CV"], "comment": null, "summary": "Ancient populations markedly transformed Neotropical forests, yet\nunderstanding the long-term effects of ancient human management, particularly\nat high-resolution scales, remains challenging. In this work we propose a new\napproach to investigate archaeological areas of influence based on vegetation\nsignatures. It consists of a deep learning model trained on satellite imagery\nto identify palm trees, followed by a clustering algorithm to identify palm\nclusters, which are then used to estimate ancient management areas. To assess\nthe palm distribution in relation to past human activity, we applied the\nproposed approach to unique high-resolution satellite imagery data covering 765\nkm2 of the Sierra Nevada de Santa Marta, Colombia. With this work, we also\nrelease a manually annotated palm tree dataset along with estimated locations\nof archaeological sites from ground-surveys and legacy records. Results\ndemonstrate how palms were significantly more abundant near archaeological\nsites showing large infrastructure investment. The extent of the largest palm\ncluster indicates that ancient human-managed areas linked to major\ninfrastructure sites may be up to two orders of magnitude bigger than indicated\nby archaeological evidence alone. Our findings suggest that pre-Columbian\npopulations influenced local vegetation fostering conditions conducive to palm\nproliferation, leaving a lasting ecological footprint. This may have lowered\nthe logistical costs of establishing infrastructure-heavy settlements in\notherwise less accessible locations. Overall, this study demonstrates the\npotential of integrating artificial intelligence approaches with new ecological\nand archaeological data to identify archaeological areas of interest through\nvegetation patterns, revealing fine-scale human-environment interactions.", "AI": {"tldr": "\u5229\u7528\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u548c\u805a\u7c7b\u7b97\u6cd5\uff0c\u901a\u8fc7\u536b\u661f\u56fe\u50cf\u8bc6\u522b\u68d5\u6988\u6811\u5206\u5e03\uff0c\u63ed\u793a\u53e4\u4ee3\u4eba\u7c7b\u6d3b\u52a8\u5bf9\u690d\u88ab\u7684\u5f71\u54cd\u3002", "motivation": "\u7814\u7a76\u53e4\u4ee3\u4eba\u7c7b\u7ba1\u7406\u5bf9\u70ed\u5e26\u68ee\u6797\u7684\u957f\u671f\u5f71\u54cd\uff0c\u7279\u522b\u662f\u5728\u9ad8\u5206\u8fa8\u7387\u5c3a\u5ea6\u4e0a\u7684\u4f5c\u7528\u3002", "method": "\u7ed3\u5408\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\uff08\u8bc6\u522b\u68d5\u6988\u6811\uff09\u548c\u805a\u7c7b\u7b97\u6cd5\uff08\u8bc6\u522b\u68d5\u6988\u6811\u96c6\u7fa4\uff09\uff0c\u4f30\u8ba1\u53e4\u4ee3\u7ba1\u7406\u533a\u57df\u3002", "result": "\u68d5\u6988\u6811\u5728\u8003\u53e4\u9057\u5740\u9644\u8fd1\u663e\u8457\u66f4\u591a\uff0c\u8868\u660e\u53e4\u4ee3\u4eba\u7c7b\u6d3b\u52a8\u4fc3\u8fdb\u4e86\u68d5\u6988\u6811\u589e\u6b96\uff0c\u7ba1\u7406\u533a\u57df\u53ef\u80fd\u6bd4\u8003\u53e4\u8bc1\u636e\u663e\u793a\u7684\u5927\u4e24\u4e2a\u6570\u91cf\u7ea7\u3002", "conclusion": "\u53e4\u4ee3\u4eba\u7c7b\u901a\u8fc7\u690d\u88ab\u7ba1\u7406\u7559\u4e0b\u4e86\u6301\u4e45\u7684\u751f\u6001\u8db3\u8ff9\uff0c\u4eba\u5de5\u667a\u80fd\u4e0e\u751f\u6001\u8003\u53e4\u6570\u636e\u7ed3\u5408\u53ef\u63ed\u793a\u7cbe\u7ec6\u5c3a\u5ea6\u7684\u4eba\u4e0e\u73af\u5883\u4e92\u52a8\u3002"}}
{"id": "2507.06996", "pdf": "https://arxiv.org/pdf/2507.06996", "abs": "https://arxiv.org/abs/2507.06996", "authors": ["Eunbyeol Cho", "Jiyoun Kim", "Minjae Lee", "Sungjin Park", "Edward Choi"], "title": "Generating Multi-Table Time Series EHR from Latent Space with Minimal Preprocessing", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Electronic Health Records (EHR) are time-series relational databases that\nrecord patient interactions and medical events over time, serving as a critical\nresource for healthcare research and applications. However, privacy concerns\nand regulatory restrictions limit the sharing and utilization of such sensitive\ndata, necessitating the generation of synthetic EHR datasets. Unlike previous\nEHR synthesis methods, which typically generate medical records consisting of\nexpert-chosen features (e.g. a few vital signs or structured codes only), we\nintroduce RawMed, the first framework to synthesize multi-table, time-series\nEHR data that closely resembles raw EHRs. Using text-based representation and\ncompression techniques, RawMed captures complex structures and temporal\ndynamics with minimal preprocessing. We also propose a new evaluation framework\nfor multi-table time-series synthetic EHRs, assessing distributional\nsimilarity, inter-table relationships, temporal dynamics, and privacy.\nValidated on two open-source EHR datasets, RawMed outperforms baseline models\nin fidelity and utility. The code is available at\nhttps://github.com/eunbyeol-cho/RawMed.", "AI": {"tldr": "RawMed\u662f\u4e00\u4e2a\u751f\u6210\u591a\u8868\u65f6\u95f4\u5e8f\u5217\u7535\u5b50\u5065\u5eb7\u8bb0\u5f55\uff08EHR\uff09\u6570\u636e\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u6587\u672c\u8868\u793a\u548c\u538b\u7f29\u6280\u672f\uff0c\u65e0\u9700\u590d\u6742\u9884\u5904\u7406\u5373\u53ef\u6355\u83b7\u590d\u6742\u7ed3\u6784\u548c\u65f6\u95f4\u52a8\u6001\u3002", "motivation": "\u7531\u4e8e\u9690\u79c1\u548c\u76d1\u7ba1\u9650\u5236\uff0c\u771f\u5b9eEHR\u6570\u636e\u96be\u4ee5\u5171\u4eab\u548c\u5229\u7528\uff0c\u9700\u8981\u751f\u6210\u5408\u6210\u6570\u636e\u3002\u73b0\u6709\u65b9\u6cd5\u901a\u5e38\u4ec5\u751f\u6210\u4e13\u5bb6\u9009\u62e9\u7684\u7279\u5f81\uff0c\u65e0\u6cd5\u5168\u9762\u6a21\u62df\u539f\u59cbEHR\u3002", "method": "\u4f7f\u7528\u6587\u672c\u8868\u793a\u548c\u538b\u7f29\u6280\u672f\uff0c\u751f\u6210\u591a\u8868\u65f6\u95f4\u5e8f\u5217EHR\u6570\u636e\uff0c\u5e76\u63d0\u51fa\u4e86\u65b0\u7684\u8bc4\u4f30\u6846\u67b6\uff0c\u8bc4\u4f30\u5206\u5e03\u76f8\u4f3c\u6027\u3001\u8868\u95f4\u5173\u7cfb\u3001\u65f6\u95f4\u52a8\u6001\u548c\u9690\u79c1\u3002", "result": "\u5728\u5f00\u6e90EHR\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\uff0cRawMed\u5728\u4fdd\u771f\u5ea6\u548c\u5b9e\u7528\u6027\u4e0a\u4f18\u4e8e\u57fa\u7ebf\u6a21\u578b\u3002", "conclusion": "RawMed\u662f\u9996\u4e2a\u80fd\u751f\u6210\u63a5\u8fd1\u539f\u59cbEHR\u7684\u5408\u6210\u6570\u636e\u6846\u67b6\uff0c\u4e3a\u533b\u7597\u7814\u7a76\u63d0\u4f9b\u4e86\u65b0\u5de5\u5177\u3002"}}
{"id": "2507.06959", "pdf": "https://arxiv.org/pdf/2507.06959", "abs": "https://arxiv.org/abs/2507.06959", "authors": ["Xiao Liang", "Jiawei Hu", "Di Wang", "Zhi Ma", "Lin Zhao", "Ronghan Li", "Bo Wan", "Quan Wang"], "title": "CheXPO: Preference Optimization for Chest X-ray VLMs with Counterfactual Rationale", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Vision-language models (VLMs) are prone to hallucinations that critically\ncompromise reliability in medical applications. While preference optimization\ncan mitigate these hallucinations through clinical feedback, its implementation\nfaces challenges such as clinically irrelevant training samples, imbalanced\ndata distributions, and prohibitive expert annotation costs. To address these\nchallenges, we introduce CheXPO, a Chest X-ray Preference Optimization strategy\nthat combines confidence-similarity joint mining with counterfactual rationale.\nOur approach begins by synthesizing a unified, fine-grained multi-task chest\nX-ray visual instruction dataset across different question types for supervised\nfine-tuning (SFT). We then identify hard examples through token-level\nconfidence analysis of SFT failures and use similarity-based retrieval to\nexpand hard examples for balancing preference sample distributions, while\nsynthetic counterfactual rationales provide fine-grained clinical preferences,\neliminating the need for additional expert input. Experiments show that CheXPO\nachieves 8.93% relative performance gain using only 5% of SFT samples, reaching\nstate-of-the-art performance across diverse clinical tasks and providing a\nscalable, interpretable solution for real-world radiology applications.", "AI": {"tldr": "CheXPO\u901a\u8fc7\u7ed3\u5408\u7f6e\u4fe1\u5ea6-\u76f8\u4f3c\u6027\u8054\u5408\u6316\u6398\u4e0e\u53cd\u4e8b\u5b9e\u63a8\u7406\uff0c\u4f18\u5316\u533b\u5b66\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff0c\u51cf\u5c11\u5e7b\u89c9\u95ee\u9898\uff0c\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u533b\u5b66\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLMs\uff09\u5b58\u5728\u5e7b\u89c9\u95ee\u9898\uff0c\u5f71\u54cd\u53ef\u9760\u6027\uff0c\u800c\u4f20\u7edf\u504f\u597d\u4f18\u5316\u65b9\u6cd5\u9762\u4e34\u6570\u636e\u5206\u5e03\u4e0d\u5747\u3001\u4e13\u5bb6\u6807\u6ce8\u6210\u672c\u9ad8\u7b49\u6311\u6218\u3002", "method": "\u63d0\u51faCheXPO\u7b56\u7565\uff0c\u5305\u62ec\u5408\u6210\u591a\u4efb\u52a1\u80f8\u90e8X\u5149\u89c6\u89c9\u6307\u4ee4\u6570\u636e\u96c6\u8fdb\u884c\u76d1\u7763\u5fae\u8c03\uff08SFT\uff09\uff0c\u901a\u8fc7\u7f6e\u4fe1\u5ea6\u5206\u6790\u548c\u76f8\u4f3c\u6027\u68c0\u7d22\u5e73\u8861\u6837\u672c\u5206\u5e03\uff0c\u5e76\u5229\u7528\u53cd\u4e8b\u5b9e\u63a8\u7406\u63d0\u4f9b\u7ec6\u7c92\u5ea6\u504f\u597d\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\uff0cCheXPO\u4ec5\u75285%\u7684SFT\u6837\u672c\u5373\u5b9e\u73b08.93%\u7684\u6027\u80fd\u63d0\u5347\uff0c\u8fbe\u5230\u6700\u4f18\u6c34\u5e73\u3002", "conclusion": "CheXPO\u4e3a\u653e\u5c04\u5b66\u5e94\u7528\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u3001\u53ef\u89e3\u91ca\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.07008", "pdf": "https://arxiv.org/pdf/2507.07008", "abs": "https://arxiv.org/abs/2507.07008", "authors": ["Emile Pierret", "Bruno Galerne"], "title": "Exact Evaluation of the Accuracy of Diffusion Models for Inverse Problems with Gaussian Data Distributions", "categories": ["cs.LG"], "comment": null, "summary": "Used as priors for Bayesian inverse problems, diffusion models have recently\nattracted considerable attention in the literature. Their flexibility and high\nvariance enable them to generate multiple solutions for a given task, such as\ninpainting, super-resolution, and deblurring. However, several unresolved\nquestions remain about how well they perform. In this article, we investigate\nthe accuracy of these models when applied to a Gaussian data distribution for\ndeblurring. Within this constrained context, we are able to precisely analyze\nthe discrepancy between the theoretical resolution of inverse problems and\ntheir resolution obtained using diffusion models by computing the exact\nWasserstein distance between the distribution of the diffusion model sampler\nand the ideal distribution of solutions to the inverse problem. Our findings\nallow for the comparison of different algorithms from the literature.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u6269\u6563\u6a21\u578b\u5728\u9ad8\u65af\u6570\u636e\u5206\u5e03\u53bb\u6a21\u7cca\u4efb\u52a1\u4e2d\u7684\u51c6\u786e\u6027\uff0c\u901a\u8fc7\u8ba1\u7b97Wasserstein\u8ddd\u79bb\u6bd4\u8f83\u7406\u8bba\u89e3\u4e0e\u6269\u6563\u6a21\u578b\u89e3\u4e4b\u95f4\u7684\u5dee\u5f02\u3002", "motivation": "\u6269\u6563\u6a21\u578b\u4f5c\u4e3a\u8d1d\u53f6\u65af\u9006\u95ee\u9898\u7684\u5148\u9a8c\uff0c\u56e0\u5176\u7075\u6d3b\u6027\u548c\u9ad8\u65b9\u5dee\u53d7\u5230\u5173\u6ce8\uff0c\u4f46\u5176\u6027\u80fd\u8868\u73b0\u5c1a\u4e0d\u660e\u786e\u3002", "method": "\u5728\u9ad8\u65af\u6570\u636e\u5206\u5e03\u4e0b\uff0c\u8ba1\u7b97\u6269\u6563\u6a21\u578b\u91c7\u6837\u5668\u5206\u5e03\u4e0e\u7406\u60f3\u89e3\u5206\u5e03\u4e4b\u95f4\u7684\u7cbe\u786eWasserstein\u8ddd\u79bb\u3002", "result": "\u7814\u7a76\u7ed3\u679c\u53ef\u7528\u4e8e\u6bd4\u8f83\u6587\u732e\u4e2d\u4e0d\u540c\u7b97\u6cd5\u7684\u6027\u80fd\u3002", "conclusion": "\u5728\u7ea6\u675f\u6761\u4ef6\u4e0b\uff0c\u6269\u6563\u6a21\u578b\u7684\u89e3\u4e0e\u7406\u8bba\u89e3\u4e4b\u95f4\u5b58\u5728\u53ef\u91cf\u5316\u7684\u5dee\u5f02\uff0c\u4e3a\u7b97\u6cd5\u6bd4\u8f83\u63d0\u4f9b\u4e86\u4f9d\u636e\u3002"}}
{"id": "2507.06966", "pdf": "https://arxiv.org/pdf/2507.06966", "abs": "https://arxiv.org/abs/2507.06966", "authors": ["Sudharsan Madhavan", "Chengcheng Gui", "Lando Bosma", "Josiah Simeth", "Jue Jiang", "Nicolas Cote", "Nima Hassan Rezaeian", "Himanshu Nagar", "Victoria Brennan", "Neelam Tyagi", "Harini Veeraraghavan"], "title": "Segmentation Regularized Training for Multi-Domain Deep Learning Registration applied to MR-Guided Prostate Cancer Radiotherapy", "categories": ["cs.CV", "physics.med-ph"], "comment": "Preprint in preparation for submission", "summary": "Background: Accurate deformable image registration (DIR) is required for\ncontour propagation and dose accumulation in MR-guided adaptive radiotherapy\n(MRgART). This study trained and evaluated a deep learning DIR method for\ndomain invariant MR-MR registration. Methods: A progressively refined\nregistration and segmentation (ProRSeg) method was trained with 262 pairs of 3T\nMR simulation scans from prostate cancer patients using weighted segmentation\nconsistency loss. ProRSeg was tested on same- (58 pairs), cross- (72 1.5T MR\nLinac pairs), and mixed-domain (42 MRSim-MRL pairs) datasets for contour\npropagation accuracy of clinical target volume (CTV), bladder, and rectum. Dose\naccumulation was performed for 42 patients undergoing 5-fraction MRgART.\nResults: ProRSeg demonstrated generalization for bladder with similar Dice\nSimilarity Coefficients across domains (0.88, 0.87, 0.86). For rectum and CTV,\nperformance was domain-dependent with higher accuracy on cross-domain MRL\ndataset (DSCs 0.89) versus same-domain data. The model's strong cross-domain\nperformance prompted us to study the feasibility of using it for dose\naccumulation. Dose accumulation showed 83.3% of patients met CTV coverage (D95\n>= 40.0 Gy) and bladder sparing (D50 <= 20.0 Gy) constraints. All patients\nachieved minimum mean target dose (>40.4 Gy), but only 9.5% remained under\nupper limit (<42.0 Gy). Conclusions: ProRSeg showed reasonable multi-domain\nMR-MR registration performance for prostate cancer patients with preliminary\nfeasibility for evaluating treatment compliance to clinical constraints.", "AI": {"tldr": "ProRSeg\u65b9\u6cd5\u5728\u8de8\u57dfMR-MR\u914d\u51c6\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u5c24\u5176\u5728\u8180\u80f1\u914d\u51c6\u4e0a\u5177\u6709\u4e00\u81f4\u6027\uff0c\u76f4\u80a0\u548cCTV\u6027\u80fd\u4f9d\u8d56\u57df\u3002\u5242\u91cf\u7d2f\u79ef\u521d\u6b65\u53ef\u884c\u3002", "motivation": "\u63d0\u9ad8MR\u5f15\u5bfc\u81ea\u9002\u5e94\u653e\u7597\u4e2d\u53d8\u5f62\u56fe\u50cf\u914d\u51c6\u7684\u51c6\u786e\u6027\u548c\u8de8\u57df\u9002\u7528\u6027\u3002", "method": "\u4f7f\u7528\u52a0\u6743\u5206\u5272\u4e00\u81f4\u6027\u635f\u5931\u8bad\u7ec3ProRSeg\u65b9\u6cd5\uff0c\u6d4b\u8bd5\u5176\u5728\u540c\u57df\u3001\u8de8\u57df\u548c\u6df7\u5408\u57df\u6570\u636e\u96c6\u4e0a\u7684\u6027\u80fd\u3002", "result": "\u8180\u80f1\u914d\u51c6\u8de8\u57df\u6027\u80fd\u4e00\u81f4\uff08DSC 0.88-0.86\uff09\uff0c\u76f4\u80a0\u548cCTV\u6027\u80fd\u4f9d\u8d56\u57df\u3002\u5242\u91cf\u7d2f\u79ef\u4e2d83.3%\u60a3\u8005\u6ee1\u8db3CTV\u8986\u76d6\u548c\u8180\u80f1\u4fdd\u62a4\u7ea6\u675f\u3002", "conclusion": "ProRSeg\u5728\u591a\u57dfMR-MR\u914d\u51c6\u4e2d\u8868\u73b0\u5408\u7406\uff0c\u521d\u6b65\u652f\u6301\u4e34\u5e8a\u7ea6\u675f\u8bc4\u4f30\u3002"}}
{"id": "2507.07016", "pdf": "https://arxiv.org/pdf/2507.07016", "abs": "https://arxiv.org/abs/2507.07016", "authors": ["Jian Huang", "Yongli Zhu", "Linna Xu", "Zhe Zheng", "Wenpeng Cui", "Mingyang Sun"], "title": "On-Device Training of PV Power Forecasting Models in a Smart Meter for Grid Edge Intelligence", "categories": ["cs.LG", "eess.SP"], "comment": "This paper is currently under reviewing by an IEEE publication; it\n  may be subjected to minor changes due to review comments later", "summary": "In this paper, an edge-side model training study is conducted on a\nresource-limited smart meter. The motivation of grid-edge intelligence and the\nconcept of on-device training are introduced. Then, the technical preparation\nsteps for on-device training are described. A case study on the task of\nphotovoltaic power forecasting is presented, where two representative machine\nlearning models are investigated: a gradient boosting tree model and a\nrecurrent neural network model. To adapt to the resource-limited situation in\nthe smart meter, \"mixed\"- and \"reduced\"-precision training schemes are also\ndevised. Experiment results demonstrate the feasibility of economically\nachieving grid-edge intelligence via the existing advanced metering\ninfrastructures.", "AI": {"tldr": "\u7814\u7a76\u5728\u8d44\u6e90\u6709\u9650\u7684\u667a\u80fd\u7535\u8868\u4e0a\u8fdb\u884c\u8fb9\u7f18\u7aef\u6a21\u578b\u8bad\u7ec3\u7684\u53ef\u884c\u6027\uff0c\u63d0\u51fa\u6df7\u5408\u548c\u964d\u4f4e\u7cbe\u5ea6\u8bad\u7ec3\u65b9\u6848\uff0c\u5e76\u901a\u8fc7\u5149\u4f0f\u529f\u7387\u9884\u6d4b\u6848\u4f8b\u9a8c\u8bc1\u3002", "motivation": "\u5b9e\u73b0\u7535\u7f51\u8fb9\u7f18\u667a\u80fd\uff0c\u63a2\u7d22\u5728\u8bbe\u5907\u7aef\u8fdb\u884c\u6a21\u578b\u8bad\u7ec3\u7684\u6f5c\u529b\u3002", "method": "\u4ecb\u7ecd\u6280\u672f\u51c6\u5907\u6b65\u9aa4\uff0c\u7814\u7a76\u68af\u5ea6\u63d0\u5347\u6811\u548c\u5faa\u73af\u795e\u7ecf\u7f51\u7edc\u6a21\u578b\uff0c\u8bbe\u8ba1\u6df7\u5408\u548c\u964d\u4f4e\u7cbe\u5ea6\u8bad\u7ec3\u65b9\u6848\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\u901a\u8fc7\u73b0\u6709\u9ad8\u7ea7\u8ba1\u91cf\u57fa\u7840\u8bbe\u65bd\u7ecf\u6d4e\u5730\u5b9e\u73b0\u7535\u7f51\u8fb9\u7f18\u667a\u80fd\u662f\u53ef\u884c\u7684\u3002", "conclusion": "\u8d44\u6e90\u6709\u9650\u7684\u667a\u80fd\u7535\u8868\u4e0a\u5b9e\u73b0\u8fb9\u7f18\u7aef\u6a21\u578b\u8bad\u7ec3\u662f\u53ef\u884c\u7684\uff0c\u4e3a\u7535\u7f51\u8fb9\u7f18\u667a\u80fd\u63d0\u4f9b\u4e86\u7ecf\u6d4e\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.07032", "pdf": "https://arxiv.org/pdf/2507.07032", "abs": "https://arxiv.org/abs/2507.07032", "authors": ["Hanqun Cao", "Xinyi Zhou", "Zijun Gao", "Chenyu Wang", "Xin Gao", "Zhi Zhang", "Chunbin Gu", "Ge Liu", "Pheng-Ann Heng"], "title": "PLAME: Leveraging Pretrained Language Models to Generate Enhanced Protein Multiple Sequence Alignments", "categories": ["cs.LG", "cs.AI", "q-bio.QM"], "comment": null, "summary": "Protein structure prediction is essential for drug discovery and\nunderstanding biological functions. While recent advancements like AlphaFold\nhave achieved remarkable accuracy, most folding models rely heavily on multiple\nsequence alignments (MSAs) to boost prediction performance. This dependency\nlimits their effectiveness on low-homology proteins and orphan proteins, where\nMSA information is sparse or unavailable. To address this limitation, we\npropose PLAME, a novel MSA design model that leverages evolutionary embeddings\nfrom pretrained protein language models. Unlike existing methods, PLAME\nintroduces pretrained representations to enhance evolutionary information and\nemploys a conservation-diversity loss to enhance generation quality.\nAdditionally, we propose a novel MSA selection method to effectively screen\nhigh-quality MSAs and improve folding performance. We also propose a sequence\nquality assessment metric that provides an orthogonal perspective to evaluate\nMSA quality. On the AlphaFold2 benchmark of low-homology and orphan proteins,\nPLAME achieves state-of-the-art performance in folding enhancement and sequence\nquality assessment, with consistent improvements demonstrated on AlphaFold3.\nAblation studies validate the effectiveness of the MSA selection method, while\nextensive case studies on various protein types provide insights into the\nrelationship between AlphaFold's prediction quality and MSA characteristics.\nFurthermore, we demonstrate that PLAME can serve as an adapter achieving\nAlphaFold2-level accuracy with the ESMFold's inference speed.", "AI": {"tldr": "PLAME\u662f\u4e00\u79cd\u65b0\u578bMSA\u8bbe\u8ba1\u6a21\u578b\uff0c\u5229\u7528\u9884\u8bad\u7ec3\u86cb\u767d\u8d28\u8bed\u8a00\u6a21\u578b\u7684\u8fdb\u5316\u5d4c\u5165\uff0c\u63d0\u5347\u4f4e\u540c\u6e90\u6027\u548c\u5b64\u513f\u86cb\u767d\u8d28\u7684\u7ed3\u6784\u9884\u6d4b\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u6298\u53e0\u6a21\u578b\u5bf9\u591a\u5e8f\u5217\u6bd4\u5bf9\uff08MSA\uff09\u7684\u4f9d\u8d56\u95ee\u9898\uff0c\u5c24\u5176\u662f\u5728\u4f4e\u540c\u6e90\u6027\u548c\u5b64\u513f\u86cb\u767d\u8d28\u4e2dMSA\u4fe1\u606f\u7a00\u758f\u6216\u4e0d\u53ef\u7528\u7684\u60c5\u51b5\u3002", "method": "\u63d0\u51faPLAME\u6a21\u578b\uff0c\u7ed3\u5408\u9884\u8bad\u7ec3\u8868\u793a\u589e\u5f3a\u8fdb\u5316\u4fe1\u606f\uff0c\u5f15\u5165\u4fdd\u62a4-\u591a\u6837\u6027\u635f\u5931\u63d0\u5347\u751f\u6210\u8d28\u91cf\uff0c\u5e76\u63d0\u51fa\u65b0\u7684MSA\u7b5b\u9009\u65b9\u6cd5\u548c\u5e8f\u5217\u8d28\u91cf\u8bc4\u4f30\u6307\u6807\u3002", "result": "\u5728AlphaFold2\u548cAlphaFold3\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cPLAME\u5728\u4f4e\u540c\u6e90\u6027\u548c\u5b64\u513f\u86cb\u767d\u8d28\u4e0a\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u63d0\u5347\u3002", "conclusion": "PLAME\u4e0d\u4ec5\u63d0\u5347\u4e86\u6298\u53e0\u6027\u80fd\uff0c\u8fd8\u80fd\u4f5c\u4e3a\u9002\u914d\u5668\u5b9e\u73b0AlphaFold2\u7ea7\u522b\u7684\u7cbe\u5ea6\u548cESMFold\u7684\u63a8\u7406\u901f\u5ea6\u3002"}}
{"id": "2507.06972", "pdf": "https://arxiv.org/pdf/2507.06972", "abs": "https://arxiv.org/abs/2507.06972", "authors": ["Johanna Orsholm", "John Quinto", "Hannu Autto", "Gaia Banelyte", "Nicolas Chazot", "Jeremy deWaard", "Stephanie deWaard", "Arielle Farrell", "Brendan Furneaux", "Bess Hardwick", "Nao Ito", "Amlan Kar", "Oula Kalttop\u00e4\u00e4", "Deirdre Kerdraon", "Erik Kristensen", "Jaclyn McKeown", "Tommi Mononen", "Ellen Nein", "Hanna Rogers", "Tomas Roslin", "Paula Schmitz", "Jayme Sones", "Maija Sujala", "Amy Thompson", "Evgeny V. Zakharov", "Iuliia Zarubiieva", "Akshita Gupta", "Scott C. Lowe", "Graham W. Taylor"], "title": "A multi-modal dataset for insect biodiversity with imagery and DNA at the trap and individual level", "categories": ["cs.CV"], "comment": "13 pages, 6 figures, submitted to Scientific Data", "summary": "Insects comprise millions of species, many experiencing severe population\ndeclines under environmental and habitat changes. High-throughput approaches\nare crucial for accelerating our understanding of insect diversity, with DNA\nbarcoding and high-resolution imaging showing strong potential for automatic\ntaxonomic classification. However, most image-based approaches rely on\nindividual specimen data, unlike the unsorted bulk samples collected in\nlarge-scale ecological surveys. We present the Mixed Arthropod Sample\nSegmentation and Identification (MassID45) dataset for training automatic\nclassifiers of bulk insect samples. It uniquely combines molecular and imaging\ndata at both the unsorted sample level and the full set of individual\nspecimens. Human annotators, supported by an AI-assisted tool, performed two\ntasks on bulk images: creating segmentation masks around each individual\narthropod and assigning taxonomic labels to over 17 000 specimens. Combining\nthe taxonomic resolution of DNA barcodes with precise abundance estimates of\nbulk images holds great potential for rapid, large-scale characterization of\ninsect communities. This dataset pushes the boundaries of tiny object detection\nand instance segmentation, fostering innovation in both ecological and machine\nlearning research.", "AI": {"tldr": "\u8bba\u6587\u4ecb\u7ecd\u4e86MassID45\u6570\u636e\u96c6\uff0c\u7528\u4e8e\u8bad\u7ec3\u81ea\u52a8\u5206\u7c7b\u5668\u5904\u7406\u6279\u91cf\u6606\u866b\u6837\u672c\uff0c\u7ed3\u5408\u5206\u5b50\u548c\u6210\u50cf\u6570\u636e\uff0c\u63a8\u52a8\u751f\u6001\u548c\u673a\u5668\u5b66\u4e60\u7814\u7a76\u3002", "motivation": "\u6606\u866b\u591a\u6837\u6027\u7814\u7a76\u9762\u4e34\u79cd\u7fa4\u4e0b\u964d\u548c\u73af\u5883\u53d8\u5316\u7684\u6311\u6218\uff0c\u9700\u8981\u9ad8\u6548\u65b9\u6cd5\u52a0\u901f\u5206\u7c7b\u3002", "method": "\u7ed3\u5408DNA\u6761\u5f62\u7801\u548c\u9ad8\u5206\u8fa8\u7387\u6210\u50cf\uff0c\u521b\u5efaMassID45\u6570\u636e\u96c6\uff0c\u901a\u8fc7AI\u8f85\u52a9\u5de5\u5177\u6807\u6ce8\u6279\u91cf\u6837\u672c\u4e2d\u7684\u4e2a\u4f53\u548c\u5206\u7c7b\u3002", "result": "\u6807\u6ce8\u4e86\u8d85\u8fc717,000\u4e2a\u6807\u672c\uff0c\u7ed3\u5408DNA\u6761\u5f62\u7801\u548c\u56fe\u50cf\u6570\u636e\uff0c\u4e3a\u5927\u89c4\u6a21\u6606\u866b\u7fa4\u843d\u7814\u7a76\u63d0\u4f9b\u6f5c\u529b\u3002", "conclusion": "MassID45\u6570\u636e\u96c6\u63a8\u52a8\u4e86\u5c0f\u7269\u4f53\u68c0\u6d4b\u548c\u5b9e\u4f8b\u5206\u5272\u7684\u8fb9\u754c\uff0c\u4fc3\u8fdb\u751f\u6001\u548c\u673a\u5668\u5b66\u4e60\u521b\u65b0\u3002"}}
{"id": "2507.07033", "pdf": "https://arxiv.org/pdf/2507.07033", "abs": "https://arxiv.org/abs/2507.07033", "authors": ["Roberto Pereira", "Fernanda Fam\u00e1", "Asal Rangrazi", "Marco Miozzo", "Charalampos Kalalas", "Paolo Dini"], "title": "Self-Supervised Learning at the Edge: The Cost of Labeling", "categories": ["cs.LG", "eess.SP"], "comment": "Accepted for publication in IEEE MLSP 2025", "summary": "Contrastive learning (CL) has recently emerged as an alternative to\ntraditional supervised machine learning solutions by enabling rich\nrepresentations from unstructured and unlabeled data. However, CL and, more\nbroadly, self-supervised learning (SSL) methods often demand a large amount of\ndata and computational resources, posing challenges for deployment on\nresource-constrained edge devices. In this work, we explore the feasibility and\nefficiency of SSL techniques for edge-based learning, focusing on trade-offs\nbetween model performance and energy efficiency. In particular, we analyze how\ndifferent SSL techniques adapt to limited computational, data, and energy\nbudgets, evaluating their effectiveness in learning robust representations\nunder resource-constrained settings. Moreover, we also consider the energy\ncosts involved in labeling data and assess how semi-supervised learning may\nassist in reducing the overall energy consumed to train CL models. Through\nextensive experiments, we demonstrate that tailored SSL strategies can achieve\ncompetitive performance while reducing resource consumption by up to 4X,\nunderscoring their potential for energy-efficient learning at the edge.", "AI": {"tldr": "\u5bf9\u6bd4\u5b66\u4e60\uff08CL\uff09\u5728\u8d44\u6e90\u53d7\u9650\u7684\u8fb9\u7f18\u8bbe\u5907\u4e0a\u90e8\u7f72\u65f6\u9762\u4e34\u6570\u636e\u4e0e\u8ba1\u7b97\u8d44\u6e90\u9700\u6c42\u9ad8\u7684\u6311\u6218\u3002\u672c\u6587\u63a2\u8ba8\u4e86\u81ea\u76d1\u7763\u5b66\u4e60\uff08SSL\uff09\u5728\u8fb9\u7f18\u5b66\u4e60\u4e2d\u7684\u53ef\u884c\u6027\u4e0e\u6548\u7387\uff0c\u5206\u6790\u4e86\u6027\u80fd\u4e0e\u80fd\u8017\u7684\u6743\u8861\uff0c\u5e76\u63d0\u51fa\u5b9a\u5236\u5316SSL\u7b56\u7565\u53ef\u51cf\u5c11\u8d44\u6e90\u6d88\u8017\u8fbe4\u500d\u3002", "motivation": "\u89e3\u51b3\u81ea\u76d1\u7763\u5b66\u4e60\u5728\u8d44\u6e90\u53d7\u9650\u7684\u8fb9\u7f18\u8bbe\u5907\u4e0a\u90e8\u7f72\u65f6\u7684\u9ad8\u8d44\u6e90\u9700\u6c42\u95ee\u9898\uff0c\u63a2\u7d22\u5982\u4f55\u5728\u6709\u9650\u8d44\u6e90\u4e0b\u5b9e\u73b0\u9ad8\u6548\u5b66\u4e60\u3002", "method": "\u5206\u6790\u4e0d\u540cSSL\u6280\u672f\u5728\u6709\u9650\u8ba1\u7b97\u3001\u6570\u636e\u548c\u80fd\u6e90\u9884\u7b97\u4e0b\u7684\u9002\u5e94\u6027\uff0c\u8bc4\u4f30\u5176\u5728\u8d44\u6e90\u53d7\u9650\u73af\u5883\u4e2d\u5b66\u4e60\u9c81\u68d2\u8868\u793a\u7684\u6709\u6548\u6027\uff0c\u5e76\u8003\u8651\u534a\u76d1\u7763\u5b66\u4e60\u5bf9\u964d\u4f4eCL\u6a21\u578b\u8bad\u7ec3\u80fd\u8017\u7684\u4f5c\u7528\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u5b9a\u5236\u5316\u7684SSL\u7b56\u7565\u5728\u4fdd\u6301\u7ade\u4e89\u529b\u7684\u6027\u80fd\u540c\u65f6\uff0c\u53ef\u5c06\u8d44\u6e90\u6d88\u8017\u51cf\u5c11\u9ad8\u8fbe4\u500d\u3002", "conclusion": "\u5b9a\u5236\u5316\u7684SSL\u7b56\u7565\u5728\u8fb9\u7f18\u8bbe\u5907\u4e0a\u5177\u6709\u9ad8\u6548\u5b66\u4e60\u7684\u6f5c\u529b\uff0c\u80fd\u591f\u663e\u8457\u964d\u4f4e\u8d44\u6e90\u6d88\u8017\u3002"}}
{"id": "2507.06973", "pdf": "https://arxiv.org/pdf/2507.06973", "abs": "https://arxiv.org/abs/2507.06973", "authors": ["Qiyuan Dai", "Sibei Yang"], "title": "Free on the Fly: Enhancing Flexibility in Test-Time Adaptation with Online EM", "categories": ["cs.CV"], "comment": "Accepted to CVPR 2025", "summary": "Vision-Language Models (VLMs) have become prominent in open-world image\nrecognition for their strong generalization abilities. Yet, their effectiveness\nin practical applications is compromised by domain shifts and distributional\nchanges, especially when test data distributions diverge from training data.\nTherefore, the paradigm of test-time adaptation (TTA) has emerged, enabling the\nuse of online off-the-shelf data at test time, supporting independent sample\npredictions, and eliminating reliance on test annotations. Traditional TTA\nmethods, however, often rely on costly training or optimization processes, or\nmake unrealistic assumptions about accessing or storing historical training and\ntest data. Instead, this study proposes FreeTTA, a training-free and\nuniversally available method that makes no assumptions, to enhance the\nflexibility of TTA. More importantly, FreeTTA is the first to explicitly model\nthe test data distribution, enabling the use of intrinsic relationships among\ntest samples to enhance predictions of individual samples without simultaneous\naccess--a direction not previously explored. FreeTTA achieves these advantages\nby introducing an online EM algorithm that utilizes zero-shot predictions from\nVLMs as priors to iteratively compute the posterior probabilities of each\nonline test sample and update parameters. Experiments demonstrate that FreeTTA\nachieves stable and significant improvements compared to state-of-the-art\nmethods across 15 datasets in both cross-domain and out-of-distribution\nsettings.", "AI": {"tldr": "FreeTTA\u662f\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u3001\u901a\u7528\u7684\u6d4b\u8bd5\u65f6\u9002\u5e94\u65b9\u6cd5\uff0c\u901a\u8fc7\u5728\u7ebfEM\u7b97\u6cd5\u5229\u7528VLM\u7684\u96f6\u6837\u672c\u9884\u6d4b\u4f5c\u4e3a\u5148\u9a8c\uff0c\u663e\u8457\u63d0\u5347\u8de8\u57df\u548c\u5206\u5e03\u5916\u573a\u666f\u4e0b\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u6d4b\u8bd5\u65f6\u9002\u5e94\u65b9\u6cd5\u4f9d\u8d56\u6602\u8d35\u8bad\u7ec3\u6216\u4e0d\u5207\u5b9e\u9645\u7684\u5047\u8bbe\uff0c\u9650\u5236\u4e86\u5b9e\u9645\u5e94\u7528\u3002", "method": "\u63d0\u51faFreeTTA\uff0c\u901a\u8fc7\u5728\u7ebfEM\u7b97\u6cd5\u5efa\u6a21\u6d4b\u8bd5\u6570\u636e\u5206\u5e03\uff0c\u5229\u7528\u6837\u672c\u95f4\u5173\u7cfb\u589e\u5f3a\u9884\u6d4b\u3002", "result": "\u572815\u4e2a\u6570\u636e\u96c6\u4e0a\uff0cFreeTTA\u5728\u8de8\u57df\u548c\u5206\u5e03\u5916\u573a\u666f\u4e2d\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "FreeTTA\u4e3a\u6d4b\u8bd5\u65f6\u9002\u5e94\u63d0\u4f9b\u4e86\u7075\u6d3b\u4e14\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u65e0\u9700\u8bad\u7ec3\u6216\u989d\u5916\u5047\u8bbe\u3002"}}
{"id": "2507.07061", "pdf": "https://arxiv.org/pdf/2507.07061", "abs": "https://arxiv.org/abs/2507.07061", "authors": ["Shervin Ghaffari", "Zohre Bahranifard", "Mohammad Akbari"], "title": "An Ensemble Embedding Approach for Improving Semantic Caching Performance in LLM-based Systems", "categories": ["cs.LG", "68T50", "I.2.7; H.3.3; I.5.1"], "comment": "10 pages, 8 figures, 2 table. Submitted to the Journal of Information\n  Science", "summary": "Semantic caching enhances the efficiency of large language model (LLM)\nsystems by identifying semantically similar queries, storing responses once,\nand serving them for subsequent equivalent requests. However, existing semantic\ncaching frameworks rely on single embedding models for query representation,\nwhich limits their ability to capture the diverse semantic relationships\npresent in real-world query distributions. This paper presents an ensemble\nembedding approach that combines multiple embedding models through a trained\nmeta-encoder to improve semantic similarity detection in LLM caching systems.\nWe evaluate our method using the Quora Question Pairs (QQP) dataset, measuring\ncache hit ratios, cache miss ratios, token savings, and response times. Our\nensemble approach achieves a 92\\% cache hit ratio for semantically equivalent\nqueries while maintaining an 85\\% accuracy in correctly rejecting\nnon-equivalent queries as cache misses. These results demonstrate that ensemble\nembedding methods significantly outperform single-model approaches in\ndistinguishing between semantically similar and dissimilar queries, leading to\nmore effective caching performance and reduced computational overhead in\nLLM-based systems.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u96c6\u6210\u5d4c\u5165\u7684\u8bed\u4e49\u7f13\u5b58\u65b9\u6cd5\uff0c\u901a\u8fc7\u7ed3\u5408\u591a\u4e2a\u5d4c\u5165\u6a21\u578b\u548c\u5143\u7f16\u7801\u5668\uff0c\u663e\u8457\u63d0\u9ad8\u4e86LLM\u7cfb\u7edf\u4e2d\u8bed\u4e49\u76f8\u4f3c\u6027\u68c0\u6d4b\u7684\u51c6\u786e\u6027\u3002", "motivation": "\u73b0\u6709\u8bed\u4e49\u7f13\u5b58\u6846\u67b6\u4f9d\u8d56\u5355\u4e00\u5d4c\u5165\u6a21\u578b\uff0c\u65e0\u6cd5\u5145\u5206\u6355\u6349\u771f\u5b9e\u67e5\u8be2\u5206\u5e03\u4e2d\u7684\u591a\u6837\u8bed\u4e49\u5173\u7cfb\uff0c\u9650\u5236\u4e86\u7f13\u5b58\u6548\u7387\u3002", "method": "\u91c7\u7528\u96c6\u6210\u5d4c\u5165\u65b9\u6cd5\uff0c\u7ed3\u5408\u591a\u4e2a\u5d4c\u5165\u6a21\u578b\u5e76\u901a\u8fc7\u8bad\u7ec3\u7684\u5143\u7f16\u7801\u5668\u4f18\u5316\u8bed\u4e49\u76f8\u4f3c\u6027\u68c0\u6d4b\u3002", "result": "\u5728QQP\u6570\u636e\u96c6\u4e0a\uff0c\u5b9e\u73b0\u4e8692%\u7684\u7f13\u5b58\u547d\u4e2d\u7387\u548c85%\u7684\u975e\u7b49\u4ef7\u67e5\u8be2\u62d2\u7edd\u51c6\u786e\u7387\uff0c\u663e\u8457\u4f18\u4e8e\u5355\u4e00\u6a21\u578b\u3002", "conclusion": "\u96c6\u6210\u5d4c\u5165\u65b9\u6cd5\u80fd\u66f4\u6709\u6548\u533a\u5206\u8bed\u4e49\u76f8\u4f3c\u4e0e\u4e0d\u76f8\u4f3c\u67e5\u8be2\uff0c\u63d0\u5347LLM\u7cfb\u7edf\u7684\u7f13\u5b58\u6027\u80fd\u548c\u8ba1\u7b97\u6548\u7387\u3002"}}
{"id": "2507.06976", "pdf": "https://arxiv.org/pdf/2507.06976", "abs": "https://arxiv.org/abs/2507.06976", "authors": ["Sven Teufel", "Dominique Mayer", "J\u00f6rg Gamerdinger", "Oliver Bringmann"], "title": "DenoiseCP-Net: Efficient Collective Perception in Adverse Weather via Joint LiDAR-Based 3D Object Detection and Denoising", "categories": ["cs.CV"], "comment": null, "summary": "While automated vehicles hold the potential to significantly reduce traffic\naccidents, their perception systems remain vulnerable to sensor degradation\ncaused by adverse weather and environmental occlusions. Collective perception,\nwhich enables vehicles to share information, offers a promising approach to\novercoming these limitations. However, to this date collective perception in\nadverse weather is mostly unstudied. Therefore, we conduct the first study of\nLiDAR-based collective perception under diverse weather conditions and present\na novel multi-task architecture for LiDAR-based collective perception under\nadverse weather. Adverse weather conditions can not only degrade perception\ncapabilities, but also negatively affect bandwidth requirements and latency due\nto the introduced noise that is also transmitted and processed. Denoising prior\nto communication can effectively mitigate these issues. Therefore, we propose\nDenoiseCP-Net, a novel multi-task architecture for LiDAR-based collective\nperception under adverse weather conditions. DenoiseCP-Net integrates\nvoxel-level noise filtering and object detection into a unified sparse\nconvolution backbone, eliminating redundant computations associated with\ntwo-stage pipelines. This design not only reduces inference latency and\ncomputational cost but also minimizes communication overhead by removing\nnon-informative noise. We extended the well-known OPV2V dataset by simulating\nrain, snow, and fog using our realistic weather simulation models. We\ndemonstrate that DenoiseCP-Net achieves near-perfect denoising accuracy in\nadverse weather, reduces the bandwidth requirements by up to 23.6% while\nmaintaining the same detection accuracy and reducing the inference latency for\ncooperative vehicles.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aDenoiseCP-Net\u7684\u65b0\u578b\u591a\u4efb\u52a1\u67b6\u6784\uff0c\u7528\u4e8e\u6076\u52a3\u5929\u6c14\u4e0b\u7684LiDAR\u96c6\u4f53\u611f\u77e5\uff0c\u901a\u8fc7\u53bb\u566a\u548c\u5bf9\u8c61\u68c0\u6d4b\u7684\u7ed3\u5408\uff0c\u964d\u4f4e\u4e86\u5e26\u5bbd\u9700\u6c42\u548c\u5ef6\u8fdf\u3002", "motivation": "\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\u7684\u611f\u77e5\u7cfb\u7edf\u5728\u6076\u52a3\u5929\u6c14\u548c\u906e\u6321\u73af\u5883\u4e0b\u5bb9\u6613\u9000\u5316\uff0c\u96c6\u4f53\u611f\u77e5\u867d\u80fd\u7f13\u89e3\u8fd9\u4e00\u95ee\u9898\uff0c\u4f46\u76f8\u5173\u7814\u7a76\u8f83\u5c11\u3002", "method": "\u63d0\u51faDenoiseCP-Net\uff0c\u5c06\u4f53\u7d20\u7ea7\u53bb\u566a\u548c\u5bf9\u8c61\u68c0\u6d4b\u96c6\u6210\u5230\u7a00\u758f\u5377\u79ef\u9aa8\u5e72\u7f51\u7edc\u4e2d\uff0c\u907f\u514d\u5197\u4f59\u8ba1\u7b97\u3002", "result": "\u5728\u6a21\u62df\u7684\u96e8\u96ea\u96fe\u5929\u6c14\u4e2d\uff0cDenoiseCP-Net\u5b9e\u73b0\u4e86\u8fd1\u4e4e\u5b8c\u7f8e\u7684\u53bb\u566a\u6548\u679c\uff0c\u5e26\u5bbd\u9700\u6c42\u964d\u4f4e23.6%\uff0c\u540c\u65f6\u4fdd\u6301\u68c0\u6d4b\u7cbe\u5ea6\u3002", "conclusion": "DenoiseCP-Net\u6709\u6548\u63d0\u5347\u4e86\u6076\u52a3\u5929\u6c14\u4e0b\u7684\u96c6\u4f53\u611f\u77e5\u6027\u80fd\uff0c\u964d\u4f4e\u4e86\u901a\u4fe1\u548c\u8ba1\u7b97\u5f00\u9500\u3002"}}
{"id": "2507.07100", "pdf": "https://arxiv.org/pdf/2507.07100", "abs": "https://arxiv.org/abs/2507.07100", "authors": ["Lan Li", "Da-Wei Zhou", "Han-Jia Ye", "De-Chuan Zhan"], "title": "Addressing Imbalanced Domain-Incremental Learning through Dual-Balance Collaborative Experts", "categories": ["cs.LG", "cs.CV"], "comment": "Accepted by ICML 2025", "summary": "Domain-Incremental Learning (DIL) focuses on continual learning in\nnon-stationary environments, requiring models to adjust to evolving domains\nwhile preserving historical knowledge. DIL faces two critical challenges in the\ncontext of imbalanced data: intra-domain class imbalance and cross-domain class\ndistribution shifts. These challenges significantly hinder model performance,\nas intra-domain imbalance leads to underfitting of few-shot classes, while\ncross-domain shifts require maintaining well-learned many-shot classes and\ntransferring knowledge to improve few-shot class performance in old domains. To\novercome these challenges, we introduce the Dual-Balance Collaborative Experts\n(DCE) framework. DCE employs a frequency-aware expert group, where each expert\nis guided by specialized loss functions to learn features for specific\nfrequency groups, effectively addressing intra-domain class imbalance.\nSubsequently, a dynamic expert selector is learned by synthesizing\npseudo-features through balanced Gaussian sampling from historical class\nstatistics. This mechanism navigates the trade-off between preserving many-shot\nknowledge of previous domains and leveraging new data to improve few-shot class\nperformance in earlier tasks. Extensive experimental results on four benchmark\ndatasets demonstrate DCE's state-of-the-art performance.", "AI": {"tldr": "DCE\u6846\u67b6\u901a\u8fc7\u9891\u7387\u611f\u77e5\u4e13\u5bb6\u7ec4\u548c\u52a8\u6001\u4e13\u5bb6\u9009\u62e9\u5668\u89e3\u51b3DIL\u4e2d\u7684\u7c7b\u4e0d\u5e73\u8861\u548c\u5206\u5e03\u504f\u79fb\u95ee\u9898\uff0c\u5b9e\u9a8c\u8bc1\u660e\u5176\u6027\u80fd\u4f18\u8d8a\u3002", "motivation": "DIL\u5728\u975e\u5e73\u7a33\u73af\u5883\u4e2d\u9762\u4e34\u7c7b\u5185\u4e0d\u5e73\u8861\u548c\u8de8\u57df\u5206\u5e03\u504f\u79fb\u7684\u6311\u6218\uff0c\u5f71\u54cd\u6a21\u578b\u6027\u80fd\u3002", "method": "DCE\u4f7f\u7528\u9891\u7387\u611f\u77e5\u4e13\u5bb6\u7ec4\u5b66\u4e60\u7279\u5b9a\u9891\u7387\u7c7b\u522b\u7684\u7279\u5f81\uff0c\u5e76\u901a\u8fc7\u52a8\u6001\u4e13\u5bb6\u9009\u62e9\u5668\u5e73\u8861\u65b0\u65e7\u77e5\u8bc6\u3002", "result": "\u5728\u56db\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\uff0cDCE\u8868\u73b0\u51fa\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "conclusion": "DCE\u6709\u6548\u89e3\u51b3\u4e86DIL\u4e2d\u7684\u5173\u952e\u6311\u6218\uff0c\u63d0\u5347\u4e86\u6a21\u578b\u5728\u975e\u5e73\u7a33\u73af\u5883\u4e2d\u7684\u9002\u5e94\u80fd\u529b\u3002"}}
{"id": "2507.06992", "pdf": "https://arxiv.org/pdf/2507.06992", "abs": "https://arxiv.org/abs/2507.06992", "authors": ["Qilong Xing", "Zikai Song", "Youjia Zhang", "Na Feng", "Junqing Yu", "Wei Yang"], "title": "MCA-RG: Enhancing LLMs with Medical Concept Alignment for Radiology Report Generation", "categories": ["cs.CV", "cs.AI"], "comment": "MICCAI 2025", "summary": "Despite significant advancements in adapting Large Language Models (LLMs) for\nradiology report generation (RRG), clinical adoption remains challenging due to\ndifficulties in accurately mapping pathological and anatomical features to\ntheir corresponding text descriptions. Additionally, semantic agnostic feature\nextraction further hampers the generation of accurate diagnostic reports. To\naddress these challenges, we introduce Medical Concept Aligned Radiology Report\nGeneration (MCA-RG), a knowledge-driven framework that explicitly aligns visual\nfeatures with distinct medical concepts to enhance the report generation\nprocess. MCA-RG utilizes two curated concept banks: a pathology bank containing\nlesion-related knowledge, and an anatomy bank with anatomical descriptions. The\nvisual features are aligned with these medical concepts and undergo tailored\nenhancement. We further propose an anatomy-based contrastive learning procedure\nto improve the generalization of anatomical features, coupled with a matching\nloss for pathological features to prioritize clinically relevant regions.\nAdditionally, a feature gating mechanism is employed to filter out low-quality\nconcept features. Finally, the visual features are corresponding to individual\nmedical concepts, and are leveraged to guide the report generation process.\nExperiments on two public benchmarks (MIMIC-CXR and CheXpert Plus) demonstrate\nthat MCA-RG achieves superior performance, highlighting its effectiveness in\nradiology report generation.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aMCA-RG\u7684\u77e5\u8bc6\u9a71\u52a8\u6846\u67b6\uff0c\u901a\u8fc7\u5c06\u89c6\u89c9\u7279\u5f81\u4e0e\u533b\u5b66\u6982\u5ff5\u5bf9\u9f50\uff0c\u63d0\u5347\u653e\u5c04\u5b66\u62a5\u544a\u751f\u6210\u7684\u51c6\u786e\u6027\u3002", "motivation": "\u5f53\u524d\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u653e\u5c04\u5b66\u62a5\u544a\u751f\u6210\uff08RRG\uff09\u4e2d\u9762\u4e34\u75c5\u7406\u548c\u89e3\u5256\u7279\u5f81\u4e0e\u6587\u672c\u63cf\u8ff0\u6620\u5c04\u4e0d\u51c6\u786e\u7684\u6311\u6218\uff0c\u5bfc\u81f4\u4e34\u5e8a\u91c7\u7528\u56f0\u96be\u3002", "method": "MCA-RG\u5229\u7528\u75c5\u7406\u5e93\u548c\u89e3\u5256\u5e93\uff0c\u901a\u8fc7\u89c6\u89c9\u7279\u5f81\u4e0e\u533b\u5b66\u6982\u5ff5\u5bf9\u9f50\uff0c\u5e76\u7ed3\u5408\u5bf9\u6bd4\u5b66\u4e60\u548c\u5339\u914d\u635f\u5931\u4f18\u5316\u7279\u5f81\u63d0\u53d6\u3002", "result": "\u5728MIMIC-CXR\u548cCheXpert Plus\u4e24\u4e2a\u516c\u5f00\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cMCA-RG\u8868\u73b0\u51fa\u8272\u3002", "conclusion": "MCA-RG\u901a\u8fc7\u77e5\u8bc6\u9a71\u52a8\u7684\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u653e\u5c04\u5b66\u62a5\u544a\u751f\u6210\u7684\u51c6\u786e\u6027\u548c\u4e34\u5e8a\u5b9e\u7528\u6027\u3002"}}
{"id": "2507.06684", "pdf": "https://arxiv.org/pdf/2507.06684", "abs": "https://arxiv.org/abs/2507.06684", "authors": ["Mat\u00e9o Ducastel", "David Tschumperl\u00e9", "Yvain Qu\u00e9au"], "title": "Photometric Stereo using Gaussian Splatting and inverse rendering", "categories": ["eess.IV", "cs.AI"], "comment": "in French language. GRETSI 2025, Association GRETSI, Aug 2025,\n  Strasbourg, France", "summary": "Recent state-of-the-art algorithms in photometric stereo rely on neural\nnetworks and operate either through prior learning or inverse rendering\noptimization. Here, we revisit the problem of calibrated photometric stereo by\nleveraging recent advances in 3D inverse rendering using the Gaussian Splatting\nformalism. This allows us to parameterize the 3D scene to be reconstructed and\noptimize it in a more interpretable manner. Our approach incorporates a\nsimplified model for light representation and demonstrates the potential of the\nGaussian Splatting rendering engine for the photometric stereo problem.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u9ad8\u65af\u6cfc\u6e85\uff08Gaussian Splatting\uff09\u76843D\u9006\u6e32\u67d3\u65b9\u6cd5\uff0c\u7528\u4e8e\u89e3\u51b3\u6821\u51c6\u5149\u5ea6\u7acb\u4f53\u89c6\u89c9\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u5149\u5ea6\u7acb\u4f53\u89c6\u89c9\u7b97\u6cd5\u4f9d\u8d56\u795e\u7ecf\u7f51\u7edc\uff0c\u901a\u8fc7\u5148\u9a8c\u5b66\u4e60\u6216\u9006\u6e32\u67d3\u4f18\u5316\uff0c\u4f46\u7f3a\u4e4f\u53ef\u89e3\u91ca\u6027\u3002", "method": "\u5229\u7528\u9ad8\u65af\u6cfc\u6e85\u5f62\u5f0f\u53163D\u573a\u666f\u53c2\u6570\u5316\uff0c\u5e76\u7ed3\u5408\u7b80\u5316\u7684\u5149\u7167\u6a21\u578b\u8fdb\u884c\u4f18\u5316\u3002", "result": "\u5c55\u793a\u4e86\u9ad8\u65af\u6cfc\u6e85\u6e32\u67d3\u5f15\u64ce\u5728\u5149\u5ea6\u7acb\u4f53\u89c6\u89c9\u95ee\u9898\u4e2d\u7684\u6f5c\u529b\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u5149\u5ea6\u7acb\u4f53\u89c6\u89c9\u63d0\u4f9b\u4e86\u4e00\u79cd\u66f4\u53ef\u89e3\u91ca\u7684\u4f18\u5316\u9014\u5f84\u3002"}}
{"id": "2507.07101", "pdf": "https://arxiv.org/pdf/2507.07101", "abs": "https://arxiv.org/abs/2507.07101", "authors": ["Martin Marek", "Sanae Lotfi", "Aditya Somasundaram", "Andrew Gordon Wilson", "Micah Goldblum"], "title": "Small Batch Size Training for Language Models: When Vanilla SGD Works, and Why Gradient Accumulation Is Wasteful", "categories": ["cs.LG"], "comment": "Code available at: https://github.com/martin-marek/batch-size", "summary": "Conventional wisdom dictates that small batch sizes make language model\npretraining and fine-tuning unstable, motivating gradient accumulation, which\ntrades off the number of optimizer steps for a proportional increase in batch\nsize. While it is common to decrease the learning rate for smaller batch sizes,\nother hyperparameters are often held fixed. In this work, we revisit small\nbatch sizes all the way down to batch size one, and we propose a rule for\nscaling Adam hyperparameters to small batch sizes. We find that small batch\nsizes (1) train stably, (2) are consistently more robust to hyperparameter\nchoices, (3) achieve equal or better per-FLOP performance than larger batch\nsizes, and (4) notably enable stable language model training with vanilla SGD,\neven without momentum, despite storing no optimizer state. Building on these\nresults, we provide practical recommendations for selecting a batch size and\nsetting optimizer hyperparameters. We further recommend against gradient\naccumulation unless training on multiple devices with multiple model replicas,\nbottlenecked by inter-device bandwidth.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\u5c0f\u6279\u91cf\u8bad\u7ec3\u5728\u8bed\u8a00\u6a21\u578b\u9884\u8bad\u7ec3\u548c\u5fae\u8c03\u4e2d\u7a33\u5b9a\u4e14\u9ad8\u6548\uff0c\u63d0\u51fa\u4e86Adam\u8d85\u53c2\u6570\u8c03\u6574\u89c4\u5219\uff0c\u5e76\u5efa\u8bae\u907f\u514d\u68af\u5ea6\u7d2f\u79ef\u3002", "motivation": "\u4f20\u7edf\u89c2\u70b9\u8ba4\u4e3a\u5c0f\u6279\u91cf\u8bad\u7ec3\u4e0d\u7a33\u5b9a\uff0c\u4f46\u672c\u7814\u7a76\u6311\u6218\u8fd9\u4e00\u89c2\u70b9\uff0c\u63a2\u7d22\u5c0f\u6279\u91cf\uff08\u751a\u81f3\u6279\u91cf\u5927\u5c0f\u4e3a1\uff09\u7684\u6f5c\u529b\u3002", "method": "\u63d0\u51faAdam\u8d85\u53c2\u6570\u8c03\u6574\u89c4\u5219\uff0c\u5e76\u6d4b\u8bd5\u5c0f\u6279\u91cf\u8bad\u7ec3\u7684\u6548\u679c\u3002", "result": "\u5c0f\u6279\u91cf\u8bad\u7ec3\u7a33\u5b9a\u3001\u5bf9\u8d85\u53c2\u6570\u9009\u62e9\u66f4\u9c81\u68d2\u3001\u6027\u80fd\u4f18\u4e8e\u5927\u6279\u91cf\uff0c\u4e14\u652f\u6301SGD\u8bad\u7ec3\u3002", "conclusion": "\u5efa\u8bae\u9009\u62e9\u5c0f\u6279\u91cf\u8bad\u7ec3\uff0c\u907f\u514d\u68af\u5ea6\u7d2f\u79ef\uff0c\u9664\u975e\u5728\u591a\u8bbe\u5907\u8bad\u7ec3\u4e2d\u5e26\u5bbd\u53d7\u9650\u3002"}}
{"id": "2507.06994", "pdf": "https://arxiv.org/pdf/2507.06994", "abs": "https://arxiv.org/abs/2507.06994", "authors": ["Qilong Xing", "Zikai Song", "Bingxin Gong", "Lian Yang", "Junqing Yu", "Wei Yang"], "title": "Cross-Modality Masked Learning for Survival Prediction in ICI Treated NSCLC Patients", "categories": ["cs.CV", "cs.AI"], "comment": "MICCAI 2025", "summary": "Accurate prognosis of non-small cell lung cancer (NSCLC) patients undergoing\nimmunotherapy is essential for personalized treatment planning, enabling\ninformed patient decisions, and improving both treatment outcomes and quality\nof life. However, the lack of large, relevant datasets and effective\nmulti-modal feature fusion strategies pose significant challenges in this\ndomain. To address these challenges, we present a large-scale dataset and\nintroduce a novel framework for multi-modal feature fusion aimed at enhancing\nthe accuracy of survival prediction. The dataset comprises 3D CT images and\ncorresponding clinical records from NSCLC patients treated with immune\ncheckpoint inhibitors (ICI), along with progression-free survival (PFS) and\noverall survival (OS) data. We further propose a cross-modality masked learning\napproach for medical feature fusion, consisting of two distinct branches, each\ntailored to its respective modality: a Slice-Depth Transformer for extracting\n3D features from CT images and a graph-based Transformer for learning node\nfeatures and relationships among clinical variables in tabular data. The fusion\nprocess is guided by a masked modality learning strategy, wherein the model\nutilizes the intact modality to reconstruct missing components. This mechanism\nimproves the integration of modality-specific features, fostering more\neffective inter-modality relationships and feature interactions. Our approach\ndemonstrates superior performance in multi-modal integration for NSCLC survival\nprediction, surpassing existing methods and setting a new benchmark for\nprognostic models in this context.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u6a21\u6001\u7279\u5f81\u878d\u5408\u6846\u67b6\u548c\u5927\u578b\u6570\u636e\u96c6\uff0c\u7528\u4e8e\u63d0\u9ad8\u975e\u5c0f\u7ec6\u80de\u80ba\u764c\uff08NSCLC\uff09\u514d\u75ab\u6cbb\u7597\u60a3\u8005\u7684\u751f\u5b58\u9884\u6d4b\u51c6\u786e\u6027\u3002", "motivation": "\u4e2a\u6027\u5316\u6cbb\u7597\u89c4\u5212\u548c\u6539\u5584\u60a3\u8005\u751f\u6d3b\u8d28\u91cf\u9700\u8981\u51c6\u786e\u7684\u9884\u540e\u9884\u6d4b\uff0c\u4f46\u7f3a\u4e4f\u5927\u578b\u6570\u636e\u96c6\u548c\u6709\u6548\u7684\u591a\u6a21\u6001\u7279\u5f81\u878d\u5408\u65b9\u6cd5\u3002", "method": "\u7ed3\u54083D CT\u56fe\u50cf\u548c\u4e34\u5e8a\u6570\u636e\uff0c\u63d0\u51fa\u8de8\u6a21\u6001\u63a9\u7801\u5b66\u4e60\u65b9\u6cd5\uff0c\u5305\u62ecSlice-Depth Transformer\u548c\u57fa\u4e8e\u56fe\u7684Transformer\u3002", "result": "\u5728\u591a\u6a21\u6001\u6574\u5408\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u8d85\u8d8a\u73b0\u6709\u65b9\u6cd5\uff0c\u4e3a\u9884\u540e\u6a21\u578b\u8bbe\u5b9a\u4e86\u65b0\u6807\u51c6\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86NSCLC\u751f\u5b58\u9884\u6d4b\u7684\u51c6\u786e\u6027\uff0c\u4e3a\u4e2a\u6027\u5316\u6cbb\u7597\u63d0\u4f9b\u4e86\u6709\u529b\u5de5\u5177\u3002"}}
{"id": "2507.07102", "pdf": "https://arxiv.org/pdf/2507.07102", "abs": "https://arxiv.org/abs/2507.07102", "authors": ["Arnas Uselis", "Andrea Dittadi", "Seong Joon Oh"], "title": "Does Data Scaling Lead to Visual Compositional Generalization?", "categories": ["cs.LG"], "comment": "ICML 2025", "summary": "Compositional understanding is crucial for human intelligence, yet it remains\nunclear whether contemporary vision models exhibit it. The dominant machine\nlearning paradigm is built on the premise that scaling data and model sizes\nwill improve out-of-distribution performance, including compositional\ngeneralization. We test this premise through controlled experiments that\nsystematically vary data scale, concept diversity, and combination coverage. We\nfind that compositional generalization is driven by data diversity, not mere\ndata scale. Increased combinatorial coverage forces models to discover a\nlinearly factored representational structure, where concepts decompose into\nadditive components. We prove this structure is key to efficiency, enabling\nperfect generalization from few observed combinations. Evaluating pretrained\nmodels (DINO, CLIP), we find above-random yet imperfect performance, suggesting\npartial presence of this structure. Our work motivates stronger emphasis on\nconstructing diverse datasets for compositional generalization, and considering\nthe importance of representational structure that enables efficient\ncompositional learning. Code available at\nhttps://github.com/oshapio/visual-compositional-generalization.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\uff0c\u7ec4\u5408\u6cdb\u5316\u80fd\u529b\u7531\u6570\u636e\u591a\u6837\u6027\u800c\u975e\u6570\u636e\u89c4\u6a21\u9a71\u52a8\uff0c\u7ebf\u6027\u5206\u89e3\u8868\u793a\u7ed3\u6784\u662f\u5b9e\u73b0\u9ad8\u6548\u7ec4\u5408\u5b66\u4e60\u7684\u5173\u952e\u3002", "motivation": "\u63a2\u8ba8\u5f53\u4ee3\u89c6\u89c9\u6a21\u578b\u662f\u5426\u5177\u5907\u7ec4\u5408\u7406\u89e3\u80fd\u529b\uff0c\u4ee5\u53ca\u6570\u636e\u89c4\u6a21\u548c\u591a\u6837\u6027\u5bf9\u7ec4\u5408\u6cdb\u5316\u7684\u5f71\u54cd\u3002", "method": "\u901a\u8fc7\u63a7\u5236\u5b9e\u9a8c\u7cfb\u7edf\u6027\u5730\u6539\u53d8\u6570\u636e\u89c4\u6a21\u3001\u6982\u5ff5\u591a\u6837\u6027\u548c\u7ec4\u5408\u8986\u76d6\u8303\u56f4\uff0c\u8bc4\u4f30\u9884\u8bad\u7ec3\u6a21\u578b\uff08DINO\u3001CLIP\uff09\u7684\u8868\u73b0\u3002", "result": "\u7ec4\u5408\u6cdb\u5316\u80fd\u529b\u4f9d\u8d56\u4e8e\u6570\u636e\u591a\u6837\u6027\uff0c\u7ebf\u6027\u5206\u89e3\u8868\u793a\u7ed3\u6784\u80fd\u5b9e\u73b0\u9ad8\u6548\u6cdb\u5316\uff0c\u4f46\u73b0\u6709\u6a21\u578b\u4ec5\u90e8\u5206\u5177\u5907\u8be5\u7ed3\u6784\u3002", "conclusion": "\u5efa\u8bae\u6784\u5efa\u66f4\u591a\u6837\u5316\u7684\u6570\u636e\u96c6\uff0c\u5e76\u5173\u6ce8\u80fd\u652f\u6301\u9ad8\u6548\u7ec4\u5408\u5b66\u4e60\u7684\u8868\u793a\u7ed3\u6784\u3002"}}
{"id": "2507.06243", "pdf": "https://arxiv.org/pdf/2507.06243", "abs": "https://arxiv.org/abs/2507.06243", "authors": ["Md Nahid Hasan", "Md Monzur Murshed", "Md Mahadi Hasan", "Faysal A. Chowdhury"], "title": "A Machine Learning Framework for Breast Cancer Treatment Classification Using a Novel Dataset", "categories": ["stat.AP", "cs.LG"], "comment": "12 pages, 3 figures, 3 tables. This paper has been submitted to\n  Scientific Reports and has been under review for five months", "summary": "Breast cancer (BC) remains a significant global health challenge, with\npersonalized treatment selection complicated by the disease's molecular and\nclinical heterogeneity. BC treatment decisions rely on various patient-specific\nclinical factors, and machine learning (ML) offers a powerful approach to\npredicting treatment outcomes. This study utilizes The Cancer Genome Atlas\n(TCGA) breast cancer clinical dataset to develop ML models for predicting the\nlikelihood of undergoing chemotherapy or hormonal therapy. The models are\ntrained using five-fold cross-validation and evaluated through performance\nmetrics, including accuracy, precision, recall, specificity, sensitivity,\nF1-score, and area under the receiver operating characteristic curve (AUROC).\nModel uncertainty is assessed using bootstrap techniques, while SHAP values\nenhance interpretability by identifying key predictors. Among the tested\nmodels, the Gradient Boosting Machine (GBM) achieves the highest stable\nperformance (accuracy = 0.7718, AUROC = 0.8252), followed by Extreme Gradient\nBoosting (XGBoost) (accuracy = 0.7557, AUROC = 0.8044) and Adaptive Boosting\n(AdaBoost) (accuracy = 0.7552, AUROC = 0.8016). These findings underscore the\npotential of ML in supporting personalized breast cancer treatment decisions\nthrough data-driven insights.", "AI": {"tldr": "\u8be5\u7814\u7a76\u5229\u7528\u673a\u5668\u5b66\u4e60\u6a21\u578b\u9884\u6d4b\u4e73\u817a\u764c\u60a3\u8005\u63a5\u53d7\u5316\u7597\u6216\u6fc0\u7d20\u6cbb\u7597\u7684\u53ef\u80fd\u6027\uff0c\u5176\u4e2dGBM\u6a21\u578b\u8868\u73b0\u6700\u4f73\u3002", "motivation": "\u4e73\u817a\u764c\u6cbb\u7597\u7684\u4e2a\u6027\u5316\u9009\u62e9\u56e0\u75be\u75c5\u5206\u5b50\u548c\u4e34\u5e8a\u5f02\u8d28\u6027\u800c\u590d\u6742\u5316\uff0c\u673a\u5668\u5b66\u4e60\u63d0\u4f9b\u4e86\u4e00\u79cd\u9884\u6d4b\u6cbb\u7597\u7ed3\u679c\u7684\u5f3a\u5927\u65b9\u6cd5\u3002", "method": "\u4f7f\u7528TCGA\u4e73\u817a\u764c\u4e34\u5e8a\u6570\u636e\u96c6\uff0c\u901a\u8fc7\u4e94\u6298\u4ea4\u53c9\u9a8c\u8bc1\u8bad\u7ec3\u6a21\u578b\uff0c\u8bc4\u4f30\u6307\u6807\u5305\u62ec\u51c6\u786e\u6027\u3001AUROC\u7b49\uff0c\u5e76\u901a\u8fc7SHAP\u503c\u589e\u5f3a\u53ef\u89e3\u91ca\u6027\u3002", "result": "GBM\u6a21\u578b\u8868\u73b0\u6700\u7a33\u5b9a\uff08\u51c6\u786e\u6027=0.7718\uff0cAUROC=0.8252\uff09\uff0cXGBoost\u548cAdaBoost\u6b21\u4e4b\u3002", "conclusion": "\u673a\u5668\u5b66\u4e60\u5728\u652f\u6301\u4e2a\u6027\u5316\u4e73\u817a\u764c\u6cbb\u7597\u51b3\u7b56\u65b9\u9762\u5177\u6709\u6f5c\u529b\u3002"}}
{"id": "2507.07006", "pdf": "https://arxiv.org/pdf/2507.07006", "abs": "https://arxiv.org/abs/2507.07006", "authors": ["S M Taslim Uddin Raju", "Md. Milon Islam", "Md Rezwanul Haque", "Hamdi Altaheri", "Fakhri Karray"], "title": "GNN-ViTCap: GNN-Enhanced Multiple Instance Learning with Vision Transformers for Whole Slide Image Classification and Captioning", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "Microscopic assessment of histopathology images is vital for accurate cancer\ndiagnosis and treatment. Whole Slide Image (WSI) classification and captioning\nhave become crucial tasks in computer-aided pathology. However, microscopic WSI\nface challenges such as redundant patches and unknown patch positions due to\nsubjective pathologist captures. Moreover, generating automatic pathology\ncaptions remains a significant challenge. To address these issues, we introduce\na novel GNN-ViTCap framework for classification and caption generation from\nhistopathological microscopic images. First, a visual feature extractor\ngenerates patch embeddings. Redundant patches are then removed by dynamically\nclustering these embeddings using deep embedded clustering and selecting\nrepresentative patches via a scalar dot attention mechanism. We build a graph\nby connecting each node to its nearest neighbors in the similarity matrix and\napply a graph neural network to capture both local and global context. The\naggregated image embeddings are projected into the language model's input space\nthrough a linear layer and combined with caption tokens to fine-tune a large\nlanguage model. We validate our method on the BreakHis and PatchGastric\ndatasets. GNN-ViTCap achieves an F1 score of 0.934 and an AUC of 0.963 for\nclassification, along with a BLEU-4 score of 0.811 and a METEOR score of 0.569\nfor captioning. Experimental results demonstrate that GNN-ViTCap outperforms\nstate of the art approaches, offering a reliable and efficient solution for\nmicroscopy based patient diagnosis.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aGNN-ViTCap\u7684\u65b0\u6846\u67b6\uff0c\u7528\u4e8e\u75c5\u7406\u663e\u5fae\u56fe\u50cf\u7684\u5206\u7c7b\u548c\u63cf\u8ff0\u751f\u6210\uff0c\u89e3\u51b3\u4e86\u5197\u4f59\u8865\u4e01\u548c\u672a\u77e5\u4f4d\u7f6e\u95ee\u9898\uff0c\u5e76\u5728\u5206\u7c7b\u548c\u63cf\u8ff0\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\u3002", "motivation": "\u75c5\u7406\u663e\u5fae\u56fe\u50cf\u8bc4\u4f30\u5bf9\u764c\u75c7\u8bca\u65ad\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u9762\u4e34\u5197\u4f59\u8865\u4e01\u548c\u672a\u77e5\u4f4d\u7f6e\u7b49\u6311\u6218\uff0c\u81ea\u52a8\u751f\u6210\u75c5\u7406\u63cf\u8ff0\u4ecd\u5177\u96be\u5ea6\u3002", "method": "\u4f7f\u7528\u89c6\u89c9\u7279\u5f81\u63d0\u53d6\u5668\u751f\u6210\u8865\u4e01\u5d4c\u5165\uff0c\u901a\u8fc7\u6df1\u5ea6\u5d4c\u5165\u805a\u7c7b\u548c\u6ce8\u610f\u529b\u673a\u5236\u53bb\u9664\u5197\u4f59\u8865\u4e01\uff0c\u6784\u5efa\u56fe\u795e\u7ecf\u7f51\u7edc\u6355\u6349\u5c40\u90e8\u548c\u5168\u5c40\u4e0a\u4e0b\u6587\uff0c\u7ed3\u5408\u8bed\u8a00\u6a21\u578b\u751f\u6210\u63cf\u8ff0\u3002", "result": "\u5728BreakHis\u548cPatchGastric\u6570\u636e\u96c6\u4e0a\uff0c\u5206\u7c7bF1\u5206\u6570\u4e3a0.934\uff0cAUC\u4e3a0.963\uff1b\u63cf\u8ff0BLEU-4\u4e3a0.811\uff0cMETEOR\u4e3a0.569\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "GNN-ViTCap\u4e3a\u57fa\u4e8e\u663e\u5fae\u56fe\u50cf\u7684\u8bca\u65ad\u63d0\u4f9b\u4e86\u9ad8\u6548\u53ef\u9760\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5206\u7c7b\u548c\u63cf\u8ff0\u6027\u80fd\u3002"}}
{"id": "2507.07013", "pdf": "https://arxiv.org/pdf/2507.07013", "abs": "https://arxiv.org/abs/2507.07013", "authors": ["Yutong Sun", "Sichen Zhu", "Peng Qiu"], "title": "Integrating Pathology Foundation Models and Spatial Transcriptomics for Cellular Decomposition from Histology Images", "categories": ["cs.CV"], "comment": null, "summary": "The rapid development of digital pathology and modern deep learning has\nfacilitated the emergence of pathology foundation models that are expected to\nsolve general pathology problems under various disease conditions in one\nunified model, with or without fine-tuning. In parallel, spatial\ntranscriptomics has emerged as a transformative technology that enables the\nprofiling of gene expression on hematoxylin and eosin (H&E) stained histology\nimages. Spatial transcriptomics unlocks the unprecedented opportunity to dive\ninto existing histology images at a more granular, cellular level. In this\nwork, we propose a lightweight and training-efficient approach to predict\ncellular composition directly from H&E-stained histology images by leveraging\ninformation-enriched feature embeddings extracted from pre-trained pathology\nfoundation models. By training a lightweight multi-layer perceptron (MLP)\nregressor on cell-type abundances derived via cell2location, our method\nefficiently distills knowledge from pathology foundation models and\ndemonstrates the ability to accurately predict cell-type compositions from\nhistology images, without physically performing the costly spatial\ntranscriptomics. Our method demonstrates competitive performance compared to\nexisting methods such as Hist2Cell, while significantly reducing computational\ncomplexity.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u8f7b\u91cf\u7ea7\u65b9\u6cd5\uff0c\u5229\u7528\u9884\u8bad\u7ec3\u7684\u75c5\u7406\u57fa\u7840\u6a21\u578b\u7279\u5f81\u9884\u6d4bH&E\u67d3\u8272\u7ec4\u7ec7\u5b66\u56fe\u50cf\u7684\u7ec6\u80de\u7ec4\u6210\uff0c\u907f\u514d\u9ad8\u6210\u672c\u7684\u7a7a\u95f4\u8f6c\u5f55\u7ec4\u5b66\u3002", "motivation": "\u6570\u5b57\u75c5\u7406\u5b66\u548c\u6df1\u5ea6\u5b66\u4e60\u7684\u5feb\u901f\u53d1\u5c55\u4e3a\u75c5\u7406\u57fa\u7840\u6a21\u578b\u63d0\u4f9b\u4e86\u53ef\u80fd\uff0c\u540c\u65f6\u7a7a\u95f4\u8f6c\u5f55\u7ec4\u5b66\u6280\u672f\u4e3a\u7ec4\u7ec7\u5b66\u56fe\u50cf\u63d0\u4f9b\u4e86\u66f4\u7cbe\u7ec6\u7684\u57fa\u56e0\u8868\u8fbe\u4fe1\u606f\u3002\u672c\u7814\u7a76\u65e8\u5728\u5229\u7528\u75c5\u7406\u57fa\u7840\u6a21\u578b\u7684\u7279\u5f81\u9884\u6d4b\u7ec6\u80de\u7ec4\u6210\uff0c\u964d\u4f4e\u6210\u672c\u3002", "method": "\u901a\u8fc7\u9884\u8bad\u7ec3\u7684\u75c5\u7406\u57fa\u7840\u6a21\u578b\u63d0\u53d6\u7279\u5f81\uff0c\u8bad\u7ec3\u8f7b\u91cf\u7ea7\u591a\u5c42\u611f\u77e5\u673a\uff08MLP\uff09\u56de\u5f52\u5668\u9884\u6d4b\u7ec6\u80de\u7c7b\u578b\u4e30\u5ea6\u3002", "result": "\u65b9\u6cd5\u5728\u9884\u6d4b\u7ec6\u80de\u7ec4\u6210\u65b9\u9762\u8868\u73b0\u4f18\u5f02\uff0c\u8ba1\u7b97\u590d\u6742\u5ea6\u663e\u8457\u4f4e\u4e8e\u73b0\u6709\u65b9\u6cd5\u5982Hist2Cell\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u9ad8\u6548\u4e14\u51c6\u786e\uff0c\u4e3a\u5229\u7528\u75c5\u7406\u57fa\u7840\u6a21\u578b\u9884\u6d4b\u7ec6\u80de\u7ec4\u6210\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2507.07029", "pdf": "https://arxiv.org/pdf/2507.07029", "abs": "https://arxiv.org/abs/2507.07029", "authors": ["Parshva Dhilankumar Patel"], "title": "Design and Implementation of an OCR-Powered Pipeline for Table Extraction from Invoices", "categories": ["cs.CV", "cs.AI", "I.2.10; I.4.9; H.3.1"], "comment": "17 pages, 23 figures, submitted to arXiv in July 2025", "summary": "This paper presents the design and development of an OCR-powered pipeline for\nefficient table extraction from invoices. The system leverages Tesseract OCR\nfor text recognition and custom post-processing logic to detect, align, and\nextract structured tabular data from scanned invoice documents. Our approach\nincludes dynamic preprocessing, table boundary detection, and row-column\nmapping, optimized for noisy and non-standard invoice formats. The resulting\npipeline significantly improves data extraction accuracy and consistency,\nsupporting real-world use cases such as automated financial workflows and\ndigital archiving.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eOCR\u7684\u8868\u683c\u63d0\u53d6\u6d41\u7a0b\uff0c\u7528\u4e8e\u4ece\u53d1\u7968\u4e2d\u9ad8\u6548\u63d0\u53d6\u7ed3\u6784\u5316\u6570\u636e\u3002", "motivation": "\u89e3\u51b3\u53d1\u7968\u626b\u63cf\u6587\u6863\u4e2d\u975e\u6807\u51c6\u683c\u5f0f\u548c\u566a\u58f0\u5bfc\u81f4\u7684\u8868\u683c\u6570\u636e\u63d0\u53d6\u4e0d\u51c6\u786e\u95ee\u9898\u3002", "method": "\u7ed3\u5408Tesseract OCR\u548c\u81ea\u5b9a\u4e49\u540e\u5904\u7406\u903b\u8f91\uff0c\u5305\u62ec\u52a8\u6001\u9884\u5904\u7406\u3001\u8868\u683c\u8fb9\u754c\u68c0\u6d4b\u548c\u884c\u5217\u6620\u5c04\u3002", "result": "\u663e\u8457\u63d0\u9ad8\u4e86\u6570\u636e\u63d0\u53d6\u7684\u51c6\u786e\u6027\u548c\u4e00\u81f4\u6027\u3002", "conclusion": "\u8be5\u6d41\u7a0b\u9002\u7528\u4e8e\u81ea\u52a8\u5316\u8d22\u52a1\u5de5\u4f5c\u6d41\u548c\u6570\u5b57\u5b58\u6863\u7b49\u5b9e\u9645\u5e94\u7528\u3002"}}
{"id": "2507.06782", "pdf": "https://arxiv.org/pdf/2507.06782", "abs": "https://arxiv.org/abs/2507.06782", "authors": ["SeungYoon Han", "Taeho Hwang", "Sukmin Cho", "Soyeong Jeong", "Hoyun Song", "Huije Lee", "Jong C. Park"], "title": "Temporal Information Retrieval via Time-Specifier Model Merging", "categories": ["cs.IR", "cs.AI", "cs.LG"], "comment": null, "summary": "The rapid expansion of digital information and knowledge across structured\nand unstructured sources has heightened the importance of Information Retrieval\n(IR). While dense retrieval methods have substantially improved semantic\nmatching for general queries, they consistently underperform on queries with\nexplicit temporal constraints--often those containing numerical expressions and\ntime specifiers such as ``in 2015.'' Existing approaches to Temporal\nInformation Retrieval (TIR) improve temporal reasoning but often suffer from\ncatastrophic forgetting, leading to reduced performance on non-temporal\nqueries. To address this, we propose Time-Specifier Model Merging (TSM), a\nnovel method that enhances temporal retrieval while preserving accuracy on\nnon-temporal queries. TSM trains specialized retrievers for individual time\nspecifiers and merges them in to a unified model, enabling precise handling of\ntemporal constraints without compromising non-temporal retrieval. Extensive\nexperiments on both temporal and non-temporal datasets demonstrate that TSM\nsignificantly improves performance on temporally constrained queries while\nmaintaining strong results on non-temporal queries, consistently outperforming\nother baseline methods. Our code is available at\nhttps://github.com/seungyoonee/TSM .", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aTSM\u7684\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u8bad\u7ec3\u9488\u5bf9\u7279\u5b9a\u65f6\u95f4\u6307\u793a\u7b26\u7684\u68c0\u7d22\u5668\u5e76\u5408\u5e76\u4e3a\u7edf\u4e00\u6a21\u578b\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u65f6\u95f4\u7ea6\u675f\u67e5\u8be2\u4e2d\u8868\u73b0\u4e0d\u4f73\u4e14\u6613\u9057\u5fd8\u975e\u65f6\u95f4\u67e5\u8be2\u7684\u95ee\u9898\u3002", "motivation": "\u968f\u7740\u6570\u5b57\u4fe1\u606f\u7684\u5feb\u901f\u589e\u957f\uff0c\u4fe1\u606f\u68c0\u7d22\uff08IR\uff09\u7684\u91cd\u8981\u6027\u65e5\u76ca\u51f8\u663e\u3002\u7136\u800c\uff0c\u73b0\u6709\u7684\u5bc6\u96c6\u68c0\u7d22\u65b9\u6cd5\u5728\u5904\u7406\u5e26\u6709\u660e\u786e\u65f6\u95f4\u7ea6\u675f\u7684\u67e5\u8be2\u65f6\u8868\u73b0\u4e0d\u4f73\uff0c\u4e14\u5bb9\u6613\u9057\u5fd8\u975e\u65f6\u95f4\u67e5\u8be2\u7684\u80fd\u529b\u3002", "method": "TSM\u65b9\u6cd5\u8bad\u7ec3\u9488\u5bf9\u5355\u4e2a\u65f6\u95f4\u6307\u793a\u7b26\u7684\u4e13\u7528\u68c0\u7d22\u5668\uff0c\u5e76\u5c06\u5b83\u4eec\u5408\u5e76\u4e3a\u4e00\u4e2a\u7edf\u4e00\u6a21\u578b\uff0c\u4ece\u800c\u5728\u4e0d\u5f71\u54cd\u975e\u65f6\u95f4\u67e5\u8be2\u6027\u80fd\u7684\u60c5\u51b5\u4e0b\u63d0\u5347\u65f6\u95f4\u7ea6\u675f\u67e5\u8be2\u7684\u68c0\u7d22\u6548\u679c\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cTSM\u5728\u65f6\u95f4\u7ea6\u675f\u67e5\u8be2\u4e0a\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u975e\u65f6\u95f4\u67e5\u8be2\u7684\u9ad8\u6027\u80fd\u3002", "conclusion": "TSM\u662f\u4e00\u79cd\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u80fd\u591f\u540c\u65f6\u63d0\u5347\u65f6\u95f4\u7ea6\u675f\u67e5\u8be2\u548c\u975e\u65f6\u95f4\u67e5\u8be2\u7684\u68c0\u7d22\u6027\u80fd\u3002"}}
{"id": "2507.07048", "pdf": "https://arxiv.org/pdf/2507.07048", "abs": "https://arxiv.org/abs/2507.07048", "authors": ["Bruce Coburn", "Jiangpeng He", "Megan E. Rollo", "Satvinder S. Dhaliwal", "Deborah A. Kerr", "Fengqing Zhu"], "title": "Evaluating Large Multimodal Models for Nutrition Analysis: A Benchmark Enriched with Contextual Metadata", "categories": ["cs.CV"], "comment": null, "summary": "Large Multimodal Models (LMMs) are increasingly applied to meal images for\nnutrition analysis. However, existing work primarily evaluates proprietary\nmodels, such as GPT-4. This leaves the broad range of LLMs underexplored.\nAdditionally, the influence of integrating contextual metadata and its\ninteraction with various reasoning modifiers remains largely uncharted. This\nwork investigates how interpreting contextual metadata derived from GPS\ncoordinates (converted to location/venue type), timestamps (transformed into\nmeal/day type), and the food items present can enhance LMM performance in\nestimating key nutritional values. These values include calories,\nmacronutrients (protein, carbohydrates, fat), and portion sizes. We also\nintroduce ACETADA, a new food-image dataset slated for public release. This\nopen dataset provides nutrition information verified by the dietitian and\nserves as the foundation for our analysis. Our evaluation across eight LMMs\n(four open-weight and four closed-weight) first establishes the benefit of\ncontextual metadata integration over straightforward prompting with images\nalone. We then demonstrate how this incorporation of contextual information\nenhances the efficacy of reasoning modifiers, such as Chain-of-Thought,\nMultimodal Chain-of-Thought, Scale Hint, Few-Shot, and Expert Persona.\nEmpirical results show that integrating metadata intelligently, when applied\nthrough straightforward prompting strategies, can significantly reduce the Mean\nAbsolute Error (MAE) and Mean Absolute Percentage Error (MAPE) in predicted\nnutritional values. This work highlights the potential of context-aware LMMs\nfor improved nutrition analysis.", "AI": {"tldr": "\u7814\u7a76\u63a2\u8ba8\u4e86\u5982\u4f55\u901a\u8fc7\u6574\u5408\u4e0a\u4e0b\u6587\u5143\u6570\u636e\uff08\u5982GPS\u3001\u65f6\u95f4\u6233\u548c\u98df\u7269\u4fe1\u606f\uff09\u63d0\u5347\u5927\u578b\u591a\u6a21\u6001\u6a21\u578b\uff08LMMs\uff09\u5728\u8425\u517b\u5206\u6790\u4e2d\u7684\u6027\u80fd\uff0c\u5e76\u5f15\u5165\u65b0\u6570\u636e\u96c6ACETADA\u3002\u5b9e\u9a8c\u8868\u660e\uff0c\u5143\u6570\u636e\u6574\u5408\u80fd\u663e\u8457\u964d\u4f4e\u9884\u6d4b\u8bef\u5dee\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u4e3b\u8981\u8bc4\u4f30\u4e13\u6709\u6a21\u578b\uff08\u5982GPT-4\uff09\uff0c\u5ffd\u7565\u4e86\u5176\u4ed6LMMs\u7684\u6f5c\u529b\uff0c\u4e14\u4e0a\u4e0b\u6587\u5143\u6570\u636e\u53ca\u5176\u4e0e\u63a8\u7406\u4fee\u9970\u7b26\u7684\u4ea4\u4e92\u5f71\u54cd\u5c1a\u672a\u5145\u5206\u63a2\u7d22\u3002", "method": "\u901a\u8fc7\u6574\u5408GPS\u3001\u65f6\u95f4\u6233\u548c\u98df\u7269\u4fe1\u606f\u7b49\u5143\u6570\u636e\uff0c\u7ed3\u5408\u591a\u79cd\u63a8\u7406\u4fee\u9970\u7b26\uff08\u5982Chain-of-Thought\uff09\uff0c\u8bc4\u4f30\u516b\u79cdLMMs\u7684\u6027\u80fd\u3002", "result": "\u5143\u6570\u636e\u6574\u5408\u663e\u8457\u964d\u4f4e\u4e86\u8425\u517b\u503c\u9884\u6d4b\u7684MAE\u548cMAPE\uff0c\u63d0\u5347\u4e86\u6a21\u578b\u6027\u80fd\u3002", "conclusion": "\u4e0a\u4e0b\u6587\u611f\u77e5\u7684LMMs\u5728\u8425\u517b\u5206\u6790\u4e2d\u5177\u6709\u663e\u8457\u6f5c\u529b\uff0c\u672a\u6765\u53ef\u8fdb\u4e00\u6b65\u63a2\u7d22\u5176\u5e94\u7528\u3002"}}
{"id": "2507.07073", "pdf": "https://arxiv.org/pdf/2507.07073", "abs": "https://arxiv.org/abs/2507.07073", "authors": ["Yulin An", "Enrique del Castillo"], "title": "An AI Approach for Learning the Spectrum of the Laplace-Beltrami Operator", "categories": ["cs.CV", "cs.AI"], "comment": "18 pages, 9 figures, submitted for publication", "summary": "The spectrum of the Laplace-Beltrami (LB) operator is central in geometric\ndeep learning tasks, capturing intrinsic properties of the shape of the object\nunder consideration. The best established method for its estimation, from a\ntriangulated mesh of the object, is based on the Finite Element Method (FEM),\nand computes the top k LB eigenvalues with a complexity of O(Nk), where N is\nthe number of points. This can render the FEM method inefficient when\nrepeatedly applied to databases of CAD mechanical parts, or in quality control\napplications where part metrology is acquired as large meshes and decisions\nabout the quality of each part are needed quickly and frequently. As a solution\nto this problem, we present a geometric deep learning framework to predict the\nLB spectrum efficiently given the CAD mesh of a part, achieving significant\ncomputational savings without sacrificing accuracy, demonstrating that the LB\nspectrum is learnable. The proposed Graph Neural Network architecture uses a\nrich set of part mesh features - including Gaussian curvature, mean curvature,\nand principal curvatures. In addition to our trained network, we make\navailable, for repeatability, a large curated dataset of real-world mechanical\nCAD models derived from the publicly available ABC dataset used for training\nand testing. Experimental results show that our method reduces computation time\nof the LB spectrum by approximately 5 times over linear FEM while delivering\ncompetitive accuracy.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u51e0\u4f55\u6df1\u5ea6\u5b66\u4e60\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u9ad8\u6548\u9884\u6d4bCAD\u7f51\u683c\u7684Laplace-Beltrami\u8c31\uff0c\u663e\u8457\u8282\u7701\u8ba1\u7b97\u65f6\u95f4\u4e14\u4fdd\u6301\u51c6\u786e\u6027\u3002", "motivation": "\u4f20\u7edf\u6709\u9650\u5143\u65b9\u6cd5\uff08FEM\uff09\u8ba1\u7b97Laplace-Beltrami\u8c31\u590d\u6742\u5ea6\u9ad8\uff0c\u4e0d\u9002\u7528\u4e8e\u9700\u8981\u5feb\u901f\u9891\u7e41\u5904\u7406\u5927\u578b\u7f51\u683c\u7684CAD\u673a\u68b0\u90e8\u4ef6\u6570\u636e\u5e93\u6216\u8d28\u91cf\u63a7\u5236\u5e94\u7528\u3002", "method": "\u4f7f\u7528\u56fe\u795e\u7ecf\u7f51\u7edc\u67b6\u6784\uff0c\u7ed3\u5408\u9ad8\u65af\u66f2\u7387\u3001\u5e73\u5747\u66f2\u7387\u7b49\u4e30\u5bcc\u7f51\u683c\u7279\u5f81\uff0c\u9884\u6d4bLaplace-Beltrami\u8c31\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u6bd4\u7ebf\u6027FEM\u5feb\u7ea65\u500d\uff0c\u540c\u65f6\u4fdd\u6301\u7ade\u4e89\u6027\u51c6\u786e\u6027\u3002", "conclusion": "Laplace-Beltrami\u8c31\u662f\u53ef\u5b66\u4e60\u7684\uff0c\u51e0\u4f55\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\u4e3a\u9ad8\u6548\u8ba1\u7b97\u63d0\u4f9b\u4e86\u53ef\u884c\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.07077", "pdf": "https://arxiv.org/pdf/2507.07077", "abs": "https://arxiv.org/abs/2507.07077", "authors": ["Yimu Pan", "Manas Mehta", "Gwen Sincerbeaux", "Jeffery A. Goldstein", "Alison D. Gernand", "James Z. Wang"], "title": "Reading a Ruler in the Wild", "categories": ["cs.CV"], "comment": null, "summary": "Accurately converting pixel measurements into absolute real-world dimensions\nremains a fundamental challenge in computer vision and limits progress in key\napplications such as biomedicine, forensics, nutritional analysis, and\ne-commerce. We introduce RulerNet, a deep learning framework that robustly\ninfers scale \"in the wild\" by reformulating ruler reading as a unified\nkeypoint-detection problem and by representing the ruler with\ngeometric-progression parameters that are invariant to perspective\ntransformations. Unlike traditional methods that rely on handcrafted thresholds\nor rigid, ruler-specific pipelines, RulerNet directly localizes centimeter\nmarks using a distortion-invariant annotation and training strategy, enabling\nstrong generalization across diverse ruler types and imaging conditions while\nmitigating data scarcity. We also present a scalable synthetic-data pipeline\nthat combines graphics-based ruler generation with ControlNet to add\nphotorealistic context, greatly increasing training diversity and improving\nperformance. To further enhance robustness and efficiency, we propose DeepGP, a\nlightweight feed-forward network that regresses geometric-progression\nparameters from noisy marks and eliminates iterative optimization, enabling\nreal-time scale estimation on mobile or edge devices. Experiments show that\nRulerNet delivers accurate, consistent, and efficient scale estimates under\nchallenging real-world conditions. These results underscore its utility as a\ngeneralizable measurement tool and its potential for integration with other\nvision components for automated, scale-aware analysis in high-impact domains. A\nlive demo is available at https://huggingface.co/spaces/ymp5078/RulerNet-Demo.", "AI": {"tldr": "RulerNet\u662f\u4e00\u4e2a\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u5c06\u5c3a\u5b50\u8bfb\u6570\u7edf\u4e00\u4e3a\u5173\u952e\u70b9\u68c0\u6d4b\u95ee\u9898\uff0c\u5e76\u7ed3\u5408\u51e0\u4f55\u7ea7\u6570\u53c2\u6570\u8868\u793a\u5c3a\u5b50\uff0c\u5b9e\u73b0\u4e86\u5728\u590d\u6742\u73af\u5883\u4e0b\u51c6\u786e\u6d4b\u91cf\u50cf\u7d20\u5230\u5b9e\u9645\u5c3a\u5bf8\u7684\u8f6c\u6362\u3002", "motivation": "\u5c06\u50cf\u7d20\u6d4b\u91cf\u8f6c\u6362\u4e3a\u5b9e\u9645\u5c3a\u5bf8\u662f\u8ba1\u7b97\u673a\u89c6\u89c9\u4e2d\u7684\u57fa\u7840\u6311\u6218\uff0c\u9650\u5236\u4e86\u751f\u7269\u533b\u5b66\u3001\u6cd5\u533b\u5b66\u3001\u8425\u517b\u5206\u6790\u548c\u7535\u5b50\u5546\u52a1\u7b49\u5173\u952e\u5e94\u7528\u7684\u8fdb\u5c55\u3002", "method": "RulerNet\u901a\u8fc7\u5173\u952e\u70b9\u68c0\u6d4b\u548c\u51e0\u4f55\u7ea7\u6570\u53c2\u6570\u8868\u793a\u5c3a\u5b50\uff0c\u7ed3\u5408\u5408\u6210\u6570\u636e\u589e\u5f3a\u8bad\u7ec3\uff0c\u5e76\u4f7f\u7528DeepGP\u7f51\u7edc\u5b9e\u73b0\u5b9e\u65f6\u4f30\u8ba1\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cRulerNet\u5728\u590d\u6742\u6761\u4ef6\u4e0b\u63d0\u4f9b\u4e86\u51c6\u786e\u3001\u4e00\u81f4\u4e14\u9ad8\u6548\u7684\u5c3a\u5bf8\u4f30\u8ba1\u3002", "conclusion": "RulerNet\u4f5c\u4e3a\u4e00\u79cd\u901a\u7528\u6d4b\u91cf\u5de5\u5177\uff0c\u5177\u6709\u5e7f\u6cdb\u7684\u5e94\u7528\u6f5c\u529b\uff0c\u5e76\u53ef\u4e0e\u5176\u4ed6\u89c6\u89c9\u7ec4\u4ef6\u96c6\u6210\u3002"}}
{"id": "2507.06804", "pdf": "https://arxiv.org/pdf/2507.06804", "abs": "https://arxiv.org/abs/2507.06804", "authors": ["Zhenwen Liang", "Linfeng Song", "Yang Li", "Tao Yang", "Feng Zhang", "Haitao Mi", "Dong Yu"], "title": "Towards Solving More Challenging IMO Problems via Decoupled Reasoning and Proving", "categories": ["cs.LO", "cs.AI"], "comment": "Work in progress", "summary": "Automated Theorem Proving (ATP) in formal languages is a foundational\nchallenge for AI. While Large Language Models (LLMs) have driven remarkable\nprogress, a significant gap remains between their powerful informal reasoning\ncapabilities and their weak formal proving performance. Recent studies show\nthat the informal accuracy exceeds 80% while formal success remains below 8% on\nbenchmarks like PutnamBench. We argue this gap persists because current\nstate-of-the-art provers, by tightly coupling reasoning and proving, are\ntrained with paradigms that inadvertently punish deep reasoning in favor of\nshallow, tactic-based strategies. To bridge this fundamental gap, we propose a\nnovel framework that decouples high-level reasoning from low-level proof\ngeneration. Our approach utilizes two distinct, specialized models: a powerful,\ngeneral-purpose Reasoner to generate diverse, strategic subgoal lemmas, and an\nefficient Prover to rigorously verify them. This modular design liberates the\nmodel's full reasoning potential and bypasses the pitfalls of end-to-end\ntraining. We evaluate our method on a challenging set of post-2000 IMO\nproblems, a problem set on which no prior open-source prover has reported\nsuccess. Our decoupled framework successfully solves 5 of these problems,\ndemonstrating a significant step towards automated reasoning on exceptionally\ndifficult mathematical challenges. To foster future research, we release our\nfull dataset of generated and verified lemmas for a wide range of IMO problems,\navailable at https://tencent-imo.github.io/ .", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u578b\u6846\u67b6\uff0c\u901a\u8fc7\u89e3\u8026\u9ad8\u7ea7\u63a8\u7406\u4e0e\u4f4e\u7ea7\u8bc1\u660e\u751f\u6210\uff0c\u663e\u8457\u63d0\u5347\u4e86\u81ea\u52a8\u5b9a\u7406\u8bc1\u660e\u7684\u6027\u80fd\u3002", "motivation": "\u5f53\u524d\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u975e\u6b63\u5f0f\u63a8\u7406\u65b9\u9762\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u5728\u5f62\u5f0f\u5316\u8bc1\u660e\u4e0a\u8868\u73b0\u8f83\u5dee\uff0c\u539f\u56e0\u662f\u73b0\u6709\u65b9\u6cd5\u5c06\u63a8\u7406\u4e0e\u8bc1\u660e\u7d27\u5bc6\u8026\u5408\uff0c\u60e9\u7f5a\u4e86\u6df1\u5ea6\u63a8\u7406\u3002", "method": "\u91c7\u7528\u6a21\u5757\u5316\u8bbe\u8ba1\uff0c\u4f7f\u7528\u4e24\u4e2a\u4e13\u95e8\u6a21\u578b\uff1a\u4e00\u4e2a\u901a\u7528\u7684\u63a8\u7406\u5668\u751f\u6210\u5b50\u76ee\u6807\u5f15\u7406\uff0c\u4e00\u4e2a\u9ad8\u6548\u7684\u8bc1\u660e\u5668\u9a8c\u8bc1\u5b83\u4eec\u3002", "result": "\u5728IMO\u95ee\u9898\u96c6\u4e0a\u6210\u529f\u89e3\u51b3\u4e865\u4e2a\u95ee\u9898\uff0c\u8d85\u8d8a\u4e86\u4e4b\u524d\u5f00\u6e90\u8bc1\u660e\u5668\u7684\u8868\u73b0\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u81ea\u52a8\u5316\u63a8\u7406\u63d0\u4f9b\u4e86\u91cd\u8981\u8fdb\u5c55\uff0c\u5e76\u516c\u5f00\u4e86\u6570\u636e\u96c6\u4ee5\u4fc3\u8fdb\u672a\u6765\u7814\u7a76\u3002"}}
{"id": "2507.07079", "pdf": "https://arxiv.org/pdf/2507.07079", "abs": "https://arxiv.org/abs/2507.07079", "authors": ["Ziyue Liu", "Federico Girella", "Yiming Wang", "Davide Talon"], "title": "Evaluating Attribute Confusion in Fashion Text-to-Image Generation", "categories": ["cs.CV"], "comment": "Accepted to ICIAP25. Project page: site\n  [https://intelligolabs.github.io/L-VQAScore/\\", "summary": "Despite the rapid advances in Text-to-Image (T2I) generation models, their\nevaluation remains challenging in domains like fashion, involving complex\ncompositional generation. Recent automated T2I evaluation methods leverage\npre-trained vision-language models to measure cross-modal alignment. However,\nour preliminary study reveals that they are still limited in assessing rich\nentity-attribute semantics, facing challenges in attribute confusion, i.e.,\nwhen attributes are correctly depicted but associated to the wrong entities. To\naddress this, we build on a Visual Question Answering (VQA) localization\nstrategy targeting one single entity at a time across both visual and textual\nmodalities. We propose a localized human evaluation protocol and introduce a\nnovel automatic metric, Localized VQAScore (L-VQAScore), that combines visual\nlocalization with VQA probing both correct (reflection) and miss-localized\n(leakage) attribute generation. On a newly curated dataset featuring\nchallenging compositional alignment scenarios, L-VQAScore outperforms\nstate-of-the-art T2I evaluation methods in terms of correlation with human\njudgments, demonstrating its strength in capturing fine-grained\nentity-attribute associations. We believe L-VQAScore can be a reliable and\nscalable alternative to subjective evaluations.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u81ea\u52a8\u8bc4\u4f30\u6307\u6807L-VQAScore\uff0c\u7528\u4e8e\u89e3\u51b3\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u6a21\u578b\u4e2d\u5b9e\u4f53-\u5c5e\u6027\u5173\u8054\u7684\u8bc4\u4f30\u95ee\u9898\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u6a21\u578b\u7684\u8bc4\u4f30\u65b9\u6cd5\u5728\u590d\u6742\u7ec4\u5408\u751f\u6210\u9886\u57df\uff08\u5982\u65f6\u5c1a\uff09\u8868\u73b0\u4e0d\u8db3\uff0c\u5c24\u5176\u662f\u5728\u5b9e\u4f53-\u5c5e\u6027\u8bed\u4e49\u7684\u8bc4\u4f30\u4e0a\u5b58\u5728\u5c40\u9650\u6027\u3002", "method": "\u57fa\u4e8e\u89c6\u89c9\u95ee\u7b54\uff08VQA\uff09\u5b9a\u4f4d\u7b56\u7565\uff0c\u63d0\u51fa\u5c40\u90e8\u5316\u8bc4\u4f30\u534f\u8bae\u548cL-VQAScore\u6307\u6807\uff0c\u7ed3\u5408\u89c6\u89c9\u5b9a\u4f4d\u548cVQA\u63a2\u6d4b\u6b63\u786e\u4e0e\u9519\u8bef\u5c5e\u6027\u751f\u6210\u3002", "result": "\u5728\u65b0\u6570\u636e\u96c6\u4e0a\uff0cL-VQAScore\u5728\u4e0e\u4eba\u8bc4\u4f30\u7684\u76f8\u5173\u6027\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u80fd\u6355\u6349\u7ec6\u7c92\u5ea6\u5b9e\u4f53-\u5c5e\u6027\u5173\u8054\u3002", "conclusion": "L-VQAScore\u53ef\u4f5c\u4e3a\u4e3b\u89c2\u8bc4\u4f30\u7684\u53ef\u9760\u4e14\u53ef\u6269\u5c55\u7684\u66ff\u4ee3\u65b9\u6848\u3002"}}
{"id": "2507.07095", "pdf": "https://arxiv.org/pdf/2507.07095", "abs": "https://arxiv.org/abs/2507.07095", "authors": ["Ke Fan", "Shunlin Lu", "Minyue Dai", "Runyi Yu", "Lixing Xiao", "Zhiyang Dou", "Junting Dong", "Lizhuang Ma", "Jingbo Wang"], "title": "Go to Zero: Towards Zero-shot Motion Generation with Million-scale Data", "categories": ["cs.CV"], "comment": "Project Page: https://vankouf.github.io/MotionMillion/", "summary": "Generating diverse and natural human motion sequences based on textual\ndescriptions constitutes a fundamental and challenging research area within the\ndomains of computer vision, graphics, and robotics. Despite significant\nadvancements in this field, current methodologies often face challenges\nregarding zero-shot generalization capabilities, largely attributable to the\nlimited size of training datasets. Moreover, the lack of a comprehensive\nevaluation framework impedes the advancement of this task by failing to\nidentify directions for improvement. In this work, we aim to push\ntext-to-motion into a new era, that is, to achieve the generalization ability\nof zero-shot. To this end, firstly, we develop an efficient annotation pipeline\nand introduce MotionMillion-the largest human motion dataset to date, featuring\nover 2,000 hours and 2 million high-quality motion sequences. Additionally, we\npropose MotionMillion-Eval, the most comprehensive benchmark for evaluating\nzero-shot motion generation. Leveraging a scalable architecture, we scale our\nmodel to 7B parameters and validate its performance on MotionMillion-Eval. Our\nresults demonstrate strong generalization to out-of-domain and complex\ncompositional motions, marking a significant step toward zero-shot human motion\ngeneration. The code is available at\nhttps://github.com/VankouF/MotionMillion-Codes.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faMotionMillion\u6570\u636e\u96c6\u548cMotionMillion-Eval\u57fa\u51c6\uff0c\u7528\u4e8e\u63d0\u5347\u96f6\u6837\u672c\u6587\u672c\u5230\u52a8\u4f5c\u751f\u6210\u7684\u80fd\u529b\uff0c\u5e76\u9a8c\u8bc1\u4e867B\u53c2\u6570\u6a21\u578b\u7684\u6709\u6548\u6027\u3002", "motivation": "\u5f53\u524d\u6587\u672c\u5230\u52a8\u4f5c\u751f\u6210\u65b9\u6cd5\u5728\u96f6\u6837\u672c\u6cdb\u5316\u80fd\u529b\u4e0a\u53d7\u9650\uff0c\u4e3b\u8981\u7531\u4e8e\u8bad\u7ec3\u6570\u636e\u89c4\u6a21\u4e0d\u8db3\u4e14\u7f3a\u4e4f\u5168\u9762\u8bc4\u4f30\u6846\u67b6\u3002", "method": "\u5f00\u53d1\u9ad8\u6548\u6807\u6ce8\u6d41\u7a0b\uff0c\u6784\u5efaMotionMillion\u6570\u636e\u96c6\uff08200\u4e07\u9ad8\u8d28\u91cf\u52a8\u4f5c\u5e8f\u5217\uff09\uff0c\u63d0\u51faMotionMillion-Eval\u57fa\u51c6\uff0c\u5e76\u8bad\u7ec37B\u53c2\u6570\u6a21\u578b\u3002", "result": "\u6a21\u578b\u5728\u96f6\u6837\u672c\u548c\u590d\u6742\u7ec4\u5408\u52a8\u4f5c\u751f\u6210\u4e0a\u8868\u73b0\u51fa\u5f3a\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u4e3a\u96f6\u6837\u672c\u4eba\u7c7b\u52a8\u4f5c\u751f\u6210\u8fc8\u51fa\u91cd\u8981\u4e00\u6b65\uff0c\u4ee3\u7801\u5df2\u5f00\u6e90\u3002"}}
{"id": "2507.07104", "pdf": "https://arxiv.org/pdf/2507.07104", "abs": "https://arxiv.org/abs/2507.07104", "authors": ["Tiezheng Zhang", "Yitong Li", "Yu-cheng Chou", "Jieneng Chen", "Alan Yuille", "Chen Wei", "Junfei Xiao"], "title": "Vision-Language-Vision Auto-Encoder: Scalable Knowledge Distillation from Diffusion Models", "categories": ["cs.CV"], "comment": null, "summary": "Building state-of-the-art Vision-Language Models (VLMs) with strong\ncaptioning capabilities typically necessitates training on billions of\nhigh-quality image-text pairs, requiring millions of GPU hours. This paper\nintroduces the Vision-Language-Vision (VLV) auto-encoder framework, which\nstrategically leverages key pretrained components: a vision encoder, the\ndecoder of a Text-to-Image (T2I) diffusion model, and subsequently, a Large\nLanguage Model (LLM). Specifically, we establish an information bottleneck by\nregularizing the language representation space, achieved through freezing the\npretrained T2I diffusion decoder. Our VLV pipeline effectively distills\nknowledge from the text-conditioned diffusion model using continuous\nembeddings, demonstrating comprehensive semantic understanding via high-quality\nreconstructions. Furthermore, by fine-tuning a pretrained LLM to decode the\nintermediate language representations into detailed descriptions, we construct\na state-of-the-art (SoTA) captioner comparable to leading models like GPT-4o\nand Gemini 2.0 Flash. Our method demonstrates exceptional cost-efficiency and\nsignificantly reduces data requirements; by primarily utilizing single-modal\nimages for training and maximizing the utility of existing pretrained models\n(image encoder, T2I diffusion model, and LLM), it circumvents the need for\nmassive paired image-text datasets, keeping the total training expenditure\nunder $1,000 USD.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aVLV\u7684\u81ea\u52a8\u7f16\u7801\u5668\u6846\u67b6\uff0c\u901a\u8fc7\u5229\u7528\u9884\u8bad\u7ec3\u7ec4\u4ef6\uff08\u89c6\u89c9\u7f16\u7801\u5668\u3001T2I\u6269\u6563\u6a21\u578b\u89e3\u7801\u5668\u548cLLM\uff09\u6784\u5efa\u9ad8\u6548\u4e14\u4f4e\u6210\u672c\u7684\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff0c\u663e\u8457\u51cf\u5c11\u6570\u636e\u9700\u6c42\u548c\u8bad\u7ec3\u6210\u672c\u3002", "motivation": "\u4f20\u7edf\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u9700\u8981\u5927\u91cf\u9ad8\u8d28\u91cf\u56fe\u50cf-\u6587\u672c\u5bf9\u548c\u6602\u8d35\u7684GPU\u8d44\u6e90\uff0cVLV\u6846\u67b6\u65e8\u5728\u901a\u8fc7\u5229\u7528\u73b0\u6709\u9884\u8bad\u7ec3\u6a21\u578b\u964d\u4f4e\u8fd9\u4e9b\u9700\u6c42\u3002", "method": "\u91c7\u7528\u4fe1\u606f\u74f6\u9888\u6280\u672f\uff0c\u901a\u8fc7\u51bb\u7ed3\u9884\u8bad\u7ec3\u7684T2I\u6269\u6563\u89e3\u7801\u5668\u89c4\u8303\u5316\u8bed\u8a00\u8868\u793a\u7a7a\u95f4\uff0c\u5e76\u5229\u7528\u8fde\u7eed\u5d4c\u5165\u4ece\u6269\u6563\u6a21\u578b\u4e2d\u63d0\u53d6\u77e5\u8bc6\u3002\u968f\u540e\u5fae\u8c03LLM\u751f\u6210\u8be6\u7ec6\u63cf\u8ff0\u3002", "result": "\u6784\u5efa\u4e86\u4e00\u4e2a\u4e0eGPT-4o\u548cGemini 2.0 Flash\u76f8\u5f53\u7684\u5148\u8fdb\u5b57\u5e55\u751f\u6210\u5668\uff0c\u8bad\u7ec3\u6210\u672c\u4f4e\u4e8e1000\u7f8e\u5143\u3002", "conclusion": "VLV\u6846\u67b6\u901a\u8fc7\u9ad8\u6548\u5229\u7528\u9884\u8bad\u7ec3\u6a21\u578b\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u8bad\u7ec3\u6210\u672c\u548c\u6570\u636e\u9700\u6c42\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u9ad8\u6027\u80fd\u3002"}}
{"id": "2507.06344", "pdf": "https://arxiv.org/pdf/2507.06344", "abs": "https://arxiv.org/abs/2507.06344", "authors": ["Sabri Meyer", "Francesco Scala", "Francesco Tacchino", "Aurelien Lucchi"], "title": "Trainability of Quantum Models Beyond Known Classical Simulability", "categories": ["quant-ph", "cs.CC", "cs.LG"], "comment": "11 pages, 7 figures, 52 pages of supplementary material", "summary": "Variational Quantum Algorithms (VQAs) are promising candidates for near-term\nquantum computing, yet they face scalability challenges due to barren plateaus,\nwhere gradients vanish exponentially in the system size. Recent conjectures\nsuggest that avoiding barren plateaus might inherently lead to classical\nsimulability, thus limiting the opportunities for quantum advantage. In this\nwork, we advance the theoretical understanding of the relationship between the\ntrainability and computational complexity of VQAs, thus directly addressing the\nconjecture. We introduce the Linear Clifford Encoder (LCE), a novel technique\nthat ensures constant-scaling gradient statistics on optimization landscape\nregions that are close to Clifford circuits. Additionally, we leverage\nclassical Taylor surrogates to reveal computational complexity phase\ntransitions from polynomial to super-polynomial as the initialization region\nsize increases. Combining these results, we reveal a deeper link between\ntrainability and computational complexity, and analytically prove that barren\nplateaus can be avoided in regions for which no classical surrogate is known to\nexist. Furthermore, numerical experiments on LCE transformed landscapes confirm\nin practice the existence of a super-polynomially complex ``transition zone''\nwhere gradients decay polynomially. These findings indicate a plausible path to\npractically relevant, barren plateau-free variational models with potential for\nquantum advantage.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u4e86\u53d8\u5206\u91cf\u5b50\u7b97\u6cd5\uff08VQAs\uff09\u7684\u53ef\u8bad\u7ec3\u6027\u4e0e\u8ba1\u7b97\u590d\u6742\u6027\u4e4b\u95f4\u7684\u5173\u7cfb\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u65b9\u6cd5\uff08LCE\uff09\u6765\u907f\u514d\u68af\u5ea6\u6d88\u5931\u95ee\u9898\uff0c\u5e76\u63ed\u793a\u4e86\u8ba1\u7b97\u590d\u6742\u6027\u7684\u76f8\u53d8\u73b0\u8c61\u3002", "motivation": "\u53d8\u5206\u91cf\u5b50\u7b97\u6cd5\u5728\u8fd1\u671f\u91cf\u5b50\u8ba1\u7b97\u4e2d\u5177\u6709\u6f5c\u529b\uff0c\u4f46\u9762\u4e34\u68af\u5ea6\u6d88\u5931\uff08barren plateaus\uff09\u7684\u6311\u6218\uff0c\u4e14\u53ef\u80fd\u56e0\u6b64\u5bfc\u81f4\u91cf\u5b50\u4f18\u52bf\u53d7\u9650\u3002\u672c\u6587\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u77db\u76fe\u3002", "method": "\u5f15\u5165\u4e86\u7ebf\u6027Clifford\u7f16\u7801\u5668\uff08LCE\uff09\u6280\u672f\uff0c\u786e\u4fdd\u68af\u5ea6\u7edf\u8ba1\u7684\u5e38\u6570\u7f29\u653e\uff0c\u5e76\u5229\u7528\u7ecf\u5178\u6cf0\u52d2\u66ff\u4ee3\u65b9\u6cd5\u63ed\u793a\u8ba1\u7b97\u590d\u6742\u6027\u7684\u76f8\u53d8\u3002", "result": "\u7406\u8bba\u8bc1\u660e\u5728\u65e0\u7ecf\u5178\u66ff\u4ee3\u7684\u533a\u57df\u53ef\u4ee5\u907f\u514d\u68af\u5ea6\u6d88\u5931\uff0c\u6570\u503c\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5b58\u5728\u4e00\u4e2a\u8d85\u591a\u9879\u5f0f\u590d\u6742\u6027\u7684\u8fc7\u6e21\u533a\u3002", "conclusion": "\u7814\u7a76\u4e3a\u5b9e\u9645\u5e94\u7528\u4e2d\u907f\u514d\u68af\u5ea6\u6d88\u5931\u7684\u53d8\u5206\u91cf\u5b50\u7b97\u6cd5\u63d0\u4f9b\u4e86\u53ef\u80fd\u8def\u5f84\uff0c\u5e76\u5c55\u793a\u4e86\u91cf\u5b50\u4f18\u52bf\u7684\u6f5c\u529b\u3002"}}
{"id": "2507.07105", "pdf": "https://arxiv.org/pdf/2507.07105", "abs": "https://arxiv.org/abs/2507.07105", "authors": ["Yushen Zuo", "Qi Zheng", "Mingyang Wu", "Xinrui Jiang", "Renjie Li", "Jian Wang", "Yide Zhang", "Gengchen Mai", "Lihong V. Wang", "James Zou", "Xiaoyu Wang", "Ming-Hsuan Yang", "Zhengzhong Tu"], "title": "4KAgent: Agentic Any Image to 4K Super-Resolution", "categories": ["cs.CV", "eess.IV"], "comment": "Project page: https://4kagent.github.io", "summary": "We present 4KAgent, a unified agentic super-resolution generalist system\ndesigned to universally upscale any image to 4K resolution (and even higher, if\napplied iteratively). Our system can transform images from extremely low\nresolutions with severe degradations, for example, highly distorted inputs at\n256x256, into crystal-clear, photorealistic 4K outputs. 4KAgent comprises three\ncore components: (1) Profiling, a module that customizes the 4KAgent pipeline\nbased on bespoke use cases; (2) A Perception Agent, which leverages\nvision-language models alongside image quality assessment experts to analyze\nthe input image and make a tailored restoration plan; and (3) A Restoration\nAgent, which executes the plan, following a recursive execution-reflection\nparadigm, guided by a quality-driven mixture-of-expert policy to select the\noptimal output for each step. Additionally, 4KAgent embeds a specialized face\nrestoration pipeline, significantly enhancing facial details in portrait and\nselfie photos. We rigorously evaluate our 4KAgent across 11 distinct task\ncategories encompassing a total of 26 diverse benchmarks, setting new\nstate-of-the-art on a broad spectrum of imaging domains. Our evaluations cover\nnatural images, portrait photos, AI-generated content, satellite imagery,\nfluorescence microscopy, and medical imaging like fundoscopy, ultrasound, and\nX-ray, demonstrating superior performance in terms of both perceptual (e.g.,\nNIQE, MUSIQ) and fidelity (e.g., PSNR) metrics. By establishing a novel agentic\nparadigm for low-level vision tasks, we aim to catalyze broader interest and\ninnovation within vision-centric autonomous agents across diverse research\ncommunities. We will release all the code, models, and results at:\nhttps://4kagent.github.io.", "AI": {"tldr": "4KAgent\u662f\u4e00\u4e2a\u7edf\u4e00\u7684\u8d85\u5206\u8fa8\u7387\u901a\u7528\u7cfb\u7edf\uff0c\u80fd\u591f\u5c06\u4efb\u4f55\u56fe\u50cf\u63d0\u5347\u81f34K\u5206\u8fa8\u7387\u3002\u5b83\u901a\u8fc7\u4e09\u4e2a\u6838\u5fc3\u7ec4\u4ef6\uff08Profiling\u3001Perception Agent\u548cRestoration Agent\uff09\u5b9e\u73b0\u9ad8\u8d28\u91cf\u56fe\u50cf\u6062\u590d\uff0c\u5e76\u5728\u591a\u4e2a\u9886\u57df\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u4f4e\u5206\u8fa8\u7387\u56fe\u50cf\u6062\u590d\u95ee\u9898\uff0c\u5c24\u5176\u662f\u9488\u5bf9\u4e25\u91cd\u9000\u5316\u7684\u8f93\u5165\uff0c\u63d0\u4f9b\u4e00\u79cd\u901a\u7528\u7684\u3001\u9ad8\u8d28\u91cf\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u7cfb\u7edf\u5305\u542b\u4e09\u4e2a\u6a21\u5757\uff1aProfiling\u5b9a\u5236\u6d41\u7a0b\uff0cPerception Agent\u5206\u6790\u8f93\u5165\u5e76\u5236\u5b9a\u6062\u590d\u8ba1\u5212\uff0cRestoration Agent\u6267\u884c\u8ba1\u5212\u5e76\u4f18\u5316\u8f93\u51fa\u3002", "result": "\u572811\u4e2a\u4efb\u52a1\u7c7b\u522b\u548c26\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u6db5\u76d6\u81ea\u7136\u56fe\u50cf\u3001\u8096\u50cf\u7167\u7247\u3001AI\u751f\u6210\u5185\u5bb9\u7b49\u591a\u79cd\u9886\u57df\u3002", "conclusion": "4KAgent\u4e3a\u4f4e\u5c42\u6b21\u89c6\u89c9\u4efb\u52a1\u5efa\u7acb\u4e86\u65b0\u7684\u4ee3\u7406\u8303\u5f0f\uff0c\u63a8\u52a8\u4e86\u89c6\u89c9\u81ea\u4e3b\u4ee3\u7406\u7684\u521b\u65b0\u3002"}}
{"id": "2507.06358", "pdf": "https://arxiv.org/pdf/2507.06358", "abs": "https://arxiv.org/abs/2507.06358", "authors": ["Victor Boussange", "Philipp Brun", "Johanna T. Malle", "Gabriele Midolo", "Jeanne Portier", "Th\u00e9ophile Sanchez", "Niklaus E. Zimmermann", "Irena Axmanov\u00e1", "Helge Bruelheide", "Milan Chytr\u00fd", "Stephan Kambach", "Zde\u0148ka Lososov\u00e1", "Martin Ve\u010de\u0159a", "Idoia Biurrun", "Klaus T. Ecker", "Jonathan Lenoir", "Jens-Christian Svenning", "Dirk Nikolaus Karger"], "title": "Deep learning-based species-area models reveal multi-scale patterns of species richness and turnover", "categories": ["q-bio.PE", "cs.LG", "92-08, 92B05, 92B15, 92B20, 92D40 (Primary) 62P10, 62P12 (Secondary)"], "comment": "31 pages", "summary": "The number of species within ecosystems is influenced not only by their\nintrinsic characteristics but also by the spatial scale considered. As the\nsampled area expands, species richness increases, a phenomenon described by the\nspecies-area relationship (SAR). The accumulation dynamics of the SAR results\nfrom a complex interplay of biotic and abiotic processes operating at various\nspatial scales. However, the challenge of collecting exhaustive biodiversity\nrecords across spatial scales has hindered a comprehensive understanding of\nthese dynamics. Here, we develop a deep learning approach that leverages\nsampling theory and small-scale ecological surveys to spatially resolve the\nscale-dependency of species richness. We demonstrate its performance by\npredicting the species richness of vascular plant communities across Europe,\nand evaluate the predictions against an independent dataset of plant community\ninventories. Our model improves species richness estimates by 32\\% and delivers\nspatially explicit patterns of species richness and turnover for sampling areas\nranging from square meters to hundreds of square kilometers. Explainable AI\ntechniques further disentangle how drivers of species richness operate across\nspatial scales. The ability of our model to represent the multi-scale nature of\nbiodiversity is essential to deliver robust biodiversity assessments and\nforecasts under global change.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\uff0c\u5229\u7528\u91c7\u6837\u7406\u8bba\u548c\u5c0f\u89c4\u6a21\u751f\u6001\u8c03\u67e5\u6765\u89e3\u6790\u7269\u79cd\u4e30\u5bcc\u5ea6\u7684\u7a7a\u95f4\u5c3a\u5ea6\u4f9d\u8d56\u6027\uff0c\u5e76\u5728\u6b27\u6d32\u7ef4\u7ba1\u690d\u7269\u7fa4\u843d\u4e2d\u9a8c\u8bc1\u4e86\u5176\u6027\u80fd\u3002", "motivation": "\u7406\u89e3\u7269\u79cd\u4e30\u5bcc\u5ea6\u7684\u7a7a\u95f4\u5c3a\u5ea6\u4f9d\u8d56\u6027\u56e0\u6570\u636e\u6536\u96c6\u56f0\u96be\u800c\u53d7\u9650\uff0c\u9700\u8981\u4e00\u79cd\u65b0\u65b9\u6cd5\u6765\u5168\u9762\u89e3\u6790\u8fd9\u4e00\u73b0\u8c61\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u79cd\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\uff0c\u7ed3\u5408\u91c7\u6837\u7406\u8bba\u548c\u5c0f\u89c4\u6a21\u751f\u6001\u8c03\u67e5\u6570\u636e\uff0c\u9884\u6d4b\u4e0d\u540c\u7a7a\u95f4\u5c3a\u5ea6\u4e0b\u7684\u7269\u79cd\u4e30\u5bcc\u5ea6\u3002", "result": "\u6a21\u578b\u5c06\u7269\u79cd\u4e30\u5bcc\u5ea6\u4f30\u8ba1\u63d0\u9ad8\u4e8632%\uff0c\u5e76\u63d0\u4f9b\u4e86\u4ece\u5e73\u65b9\u7c73\u5230\u6570\u767e\u5e73\u65b9\u516c\u91cc\u7684\u7269\u79cd\u4e30\u5bcc\u5ea6\u548c\u5468\u8f6c\u6a21\u5f0f\u3002", "conclusion": "\u8be5\u6a21\u578b\u80fd\u591f\u6355\u6349\u751f\u7269\u591a\u6837\u6027\u7684\u591a\u5c3a\u5ea6\u7279\u6027\uff0c\u4e3a\u5168\u7403\u53d8\u5316\u4e0b\u7684\u751f\u7269\u591a\u6837\u6027\u8bc4\u4f30\u548c\u9884\u6d4b\u63d0\u4f9b\u4e86\u6709\u529b\u5de5\u5177\u3002"}}
{"id": "2507.07106", "pdf": "https://arxiv.org/pdf/2507.07106", "abs": "https://arxiv.org/abs/2507.07106", "authors": ["Vatsal Agarwal", "Matthew Gwilliam", "Gefen Kohavi", "Eshan Verma", "Daniel Ulbricht", "Abhinav Shrivastava"], "title": "Towards Multimodal Understanding via Stable Diffusion as a Task-Aware Feature Extractor", "categories": ["cs.CV", "cs.LG"], "comment": "Website: see https://vatsalag99.github.io/mustafar/", "summary": "Recent advances in multimodal large language models (MLLMs) have enabled\nimage-based question-answering capabilities. However, a key limitation is the\nuse of CLIP as the visual encoder; while it can capture coarse global\ninformation, it often can miss fine-grained details that are relevant to the\ninput query. To address these shortcomings, this work studies whether\npre-trained text-to-image diffusion models can serve as instruction-aware\nvisual encoders. Through an analysis of their internal representations, we find\ndiffusion features are both rich in semantics and can encode strong image-text\nalignment. Moreover, we find that we can leverage text conditioning to focus\nthe model on regions relevant to the input question. We then investigate how to\nalign these features with large language models and uncover a leakage\nphenomenon, where the LLM can inadvertently recover information from the\noriginal diffusion prompt. We analyze the causes of this leakage and propose a\nmitigation strategy. Based on these insights, we explore a simple fusion\nstrategy that utilizes both CLIP and conditional diffusion features. We\nevaluate our approach on both general VQA and specialized MLLM benchmarks,\ndemonstrating the promise of diffusion models for visual understanding,\nparticularly in vision-centric tasks that require spatial and compositional\nreasoning. Our project page can be found\nhttps://vatsalag99.github.io/mustafar/.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63a2\u8ba8\u4e86\u4f7f\u7528\u9884\u8bad\u7ec3\u7684\u6587\u672c\u5230\u56fe\u50cf\u6269\u6563\u6a21\u578b\u4f5c\u4e3a\u89c6\u89c9\u7f16\u7801\u5668\u7684\u6f5c\u529b\uff0c\u4ee5\u5f25\u8865CLIP\u5728\u6355\u6349\u7ec6\u7c92\u5ea6\u7ec6\u8282\u4e0a\u7684\u4e0d\u8db3\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u878d\u5408\u7b56\u7565\u6765\u63d0\u5347\u89c6\u89c9\u95ee\u7b54\u6027\u80fd\u3002", "motivation": "CLIP\u4f5c\u4e3a\u89c6\u89c9\u7f16\u7801\u5668\u5728\u6355\u6349\u5168\u5c40\u4fe1\u606f\u65f6\u53ef\u80fd\u5ffd\u7565\u7ec6\u7c92\u5ea6\u7ec6\u8282\uff0c\u9650\u5236\u4e86\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLMs\uff09\u5728\u89c6\u89c9\u95ee\u7b54\u4e2d\u7684\u8868\u73b0\u3002", "method": "\u901a\u8fc7\u5206\u6790\u6269\u6563\u6a21\u578b\u7684\u5185\u90e8\u8868\u793a\uff0c\u53d1\u73b0\u5176\u8bed\u4e49\u4e30\u5bcc\u4e14\u80fd\u7f16\u7801\u5f3a\u56fe\u50cf-\u6587\u672c\u5bf9\u9f50\u3002\u63d0\u51fa\u5229\u7528\u6587\u672c\u6761\u4ef6\u805a\u7126\u6a21\u578b\u4e8e\u95ee\u9898\u76f8\u5173\u533a\u57df\uff0c\u5e76\u63a2\u7d22\u4e0eLLM\u5bf9\u9f50\u7684\u7b56\u7565\u3002", "result": "\u5728\u901a\u7528VQA\u548c\u4e13\u7528MLLM\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u878d\u5408CLIP\u548c\u6761\u4ef6\u6269\u6563\u7279\u5f81\u7684\u65b9\u6cd5\u8868\u73b0\u51fa\u8272\uff0c\u5c24\u5176\u5728\u9700\u8981\u7a7a\u95f4\u548c\u7ec4\u5408\u63a8\u7406\u7684\u4efb\u52a1\u4e2d\u3002", "conclusion": "\u6269\u6563\u6a21\u578b\u5728\u89c6\u89c9\u7406\u89e3\u4e2d\u5177\u6709\u6f5c\u529b\uff0c\u7279\u522b\u662f\u5728\u9700\u8981\u7ec6\u7c92\u5ea6\u5206\u6790\u7684\u89c6\u89c9\u4e2d\u5fc3\u4efb\u52a1\u4e2d\u3002"}}
{"id": "2507.06828", "pdf": "https://arxiv.org/pdf/2507.06828", "abs": "https://arxiv.org/abs/2507.06828", "authors": ["Xuesong Li", "Nassir Navab", "Zhongliang Jiang"], "title": "Speckle2Self: Self-Supervised Ultrasound Speckle Reduction Without Clean Data", "categories": ["eess.IV", "cs.AI", "cs.CV"], "comment": null, "summary": "Image denoising is a fundamental task in computer vision, particularly in\nmedical ultrasound (US) imaging, where speckle noise significantly degrades\nimage quality. Although recent advancements in deep neural networks have led to\nsubstantial improvements in denoising for natural images, these methods cannot\nbe directly applied to US speckle noise, as it is not purely random. Instead,\nUS speckle arises from complex wave interference within the body\nmicrostructure, making it tissue-dependent. This dependency means that\nobtaining two independent noisy observations of the same scene, as required by\npioneering Noise2Noise, is not feasible. Additionally, blind-spot networks also\ncannot handle US speckle noise due to its high spatial dependency. To address\nthis challenge, we introduce Speckle2Self, a novel self-supervised algorithm\nfor speckle reduction using only single noisy observations. The key insight is\nthat applying a multi-scale perturbation (MSP) operation introduces\ntissue-dependent variations in the speckle pattern across different scales,\nwhile preserving the shared anatomical structure. This enables effective\nspeckle suppression by modeling the clean image as a low-rank signal and\nisolating the sparse noise component. To demonstrate its effectiveness,\nSpeckle2Self is comprehensively compared with conventional filter-based\ndenoising algorithms and SOTA learning-based methods, using both realistic\nsimulated US images and human carotid US images. Additionally, data from\nmultiple US machines are employed to evaluate model generalization and\nadaptability to images from unseen domains. \\textit{Code and datasets will be\nreleased upon acceptance.", "AI": {"tldr": "Speckle2Self\u662f\u4e00\u79cd\u81ea\u76d1\u7763\u7b97\u6cd5\uff0c\u7528\u4e8e\u4ec5\u4f7f\u7528\u5355\u6b21\u566a\u58f0\u89c2\u6d4b\u51cf\u5c11\u8d85\u58f0\u56fe\u50cf\u4e2d\u7684\u6591\u70b9\u566a\u58f0\u3002", "motivation": "\u8d85\u58f0\u6591\u70b9\u566a\u58f0\u5177\u6709\u7ec4\u7ec7\u4f9d\u8d56\u6027\uff0c\u4f20\u7edf\u65b9\u6cd5\uff08\u5982Noise2Noise\u548c\u76f2\u70b9\u7f51\u7edc\uff09\u65e0\u6cd5\u76f4\u63a5\u5904\u7406\u3002", "method": "\u901a\u8fc7\u591a\u5c3a\u5ea6\u6270\u52a8\uff08MSP\uff09\u64cd\u4f5c\u5f15\u5165\u7ec4\u7ec7\u4f9d\u8d56\u6027\u53d8\u5316\uff0c\u5c06\u5e72\u51c0\u56fe\u50cf\u5efa\u6a21\u4e3a\u4f4e\u79e9\u4fe1\u53f7\u5e76\u5206\u79bb\u7a00\u758f\u566a\u58f0\u3002", "result": "\u5728\u6a21\u62df\u548c\u771f\u5b9e\u8d85\u58f0\u56fe\u50cf\u4e0a\u9a8c\u8bc1\u4e86Speckle2Self\u7684\u6709\u6548\u6027\uff0c\u5e76\u5c55\u793a\u4e86\u5176\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "Speckle2Self\u4e3a\u8d85\u58f0\u56fe\u50cf\u53bb\u566a\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u7684\u81ea\u76d1\u7763\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.06363", "pdf": "https://arxiv.org/pdf/2507.06363", "abs": "https://arxiv.org/abs/2507.06363", "authors": ["Szymon P\u0142otka", "Maciej Chrabaszcz", "Gizem Mert", "Ewa Szczurek", "Arkadiusz Sitek"], "title": "Mamba Goes HoME: Hierarchical Soft Mixture-of-Experts for 3D Medical Image Segmentation", "categories": ["eess.IV", "cs.CV"], "comment": null, "summary": "In recent years, artificial intelligence has significantly advanced medical\nimage segmentation. However, challenges remain, including efficient 3D medical\nimage processing across diverse modalities and handling data variability. In\nthis work, we introduce Hierarchical Soft Mixture-of-Experts (HoME), a\ntwo-level token-routing layer for efficient long-context modeling, specifically\ndesigned for 3D medical image segmentation. Built on the Mamba state-space\nmodel (SSM) backbone, HoME enhances sequential modeling through sparse,\nadaptive expert routing. The first stage employs a Soft Mixture-of-Experts\n(SMoE) layer to partition input sequences into local groups, routing tokens to\nspecialized per-group experts for localized feature extraction. The second\nstage aggregates these outputs via a global SMoE layer, enabling cross-group\ninformation fusion and global context refinement. This hierarchical design,\ncombining local expert routing with global expert refinement improves\ngeneralizability and segmentation performance, surpassing state-of-the-art\nresults across datasets from the three most commonly used 3D medical imaging\nmodalities and data quality.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aHoME\u7684\u5206\u5c42\u8f6f\u4e13\u5bb6\u6df7\u5408\u65b9\u6cd5\uff0c\u7528\u4e8e\u9ad8\u6548\u5904\u74063D\u533b\u5b66\u56fe\u50cf\u5206\u5272\uff0c\u901a\u8fc7\u5c40\u90e8\u548c\u5168\u5c40\u4e13\u5bb6\u8def\u7531\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u89e3\u51b33D\u533b\u5b66\u56fe\u50cf\u5904\u7406\u4e2d\u7684\u591a\u6837\u6027\u548c\u6570\u636e\u53d8\u5f02\u6027\u6311\u6218\u3002", "method": "\u57fa\u4e8eMamba\u72b6\u6001\u7a7a\u95f4\u6a21\u578b\uff0c\u91c7\u7528\u4e24\u9636\u6bb5\u7684\u5206\u5c42\u8f6f\u4e13\u5bb6\u6df7\u5408\uff08HoME\uff09\u65b9\u6cd5\uff0c\u5c40\u90e8\u548c\u5168\u5c40\u4e13\u5bb6\u8def\u7531\u7ed3\u5408\u3002", "result": "\u5728\u591a\u79cd3D\u533b\u5b66\u5f71\u50cf\u6a21\u6001\u548c\u6570\u636e\u8d28\u91cf\u4e0b\uff0c\u6027\u80fd\u8d85\u8d8a\u73b0\u6709\u6280\u672f\u3002", "conclusion": "HoME\u901a\u8fc7\u5206\u5c42\u8bbe\u8ba1\u663e\u8457\u63d0\u5347\u4e86\u5206\u5272\u6027\u80fd\u548c\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2507.06849", "pdf": "https://arxiv.org/pdf/2507.06849", "abs": "https://arxiv.org/abs/2507.06849", "authors": ["Yizhuo Wu", "Ang Li", "Chang Gao"], "title": "OpenDPDv2: A Unified Learning and Optimization Framework for Neural Network Digital Predistortion", "categories": ["eess.SP", "cs.AI"], "comment": "Under Review", "summary": "Neural network (NN)-based Digital Predistortion (DPD) stands out in improving\nsignal quality in wideband radio frequency (RF) power amplifiers (PAs)\nemploying complex modulation. However, NN DPDs usually rely on a large number\nof parameters for effective linearization and can significantly contribute to\nthe energy consumption of the digital back-end in RF systems. This paper\npresents OpenDPDv2, a unified framework for PA modeling, DPD learning, and\nmodel optimization to reduce power consumption while maintaining high\nlinearization performance. The optimization techniques feature a novel DPD\nalgorithm, TRes-DeltaGRU, alongside two energy-efficient methods. The\ntop-performing 32-bit floating-point (FP32) TRes-DeltaGRU-DPD model achieves an\nAdjacent Channel Power Ratio (ACPR) of -59.4 dBc and Error Vector Magnitude\n(EVM) of -42.1 dBc. By exploiting fixed-point quantization and dynamic temporal\nsparsity of input signals and hidden neurons, the inference energy of our model\ncan be reduced by 4.5X while still maintaining -50.3 dBc ACPR and -35.2 dB EVM\nwith 56% temporal sparsity. This was evaluated using a TM3.1a 200 MHz bandwidth\n256-QAM OFDM signal applied to a 3.5 GHz GaN Doherty RF PA. OpenDPDv2 code,\ndatasets, and documentation are publicly accessible at:\nhttps://github.com/lab-emi/OpenDPD.", "AI": {"tldr": "OpenDPDv2\u6846\u67b6\u901a\u8fc7TRes-DeltaGRU\u7b97\u6cd5\u548c\u4e24\u79cd\u8282\u80fd\u65b9\u6cd5\uff0c\u5728\u4fdd\u6301\u9ad8\u6027\u80fd\u7684\u540c\u65f6\u964d\u4f4eDPD\u7684\u80fd\u8017\u3002", "motivation": "NN DPDs\u901a\u5e38\u9700\u8981\u5927\u91cf\u53c2\u6570\u4e14\u80fd\u8017\u9ad8\uff0cOpenDPDv2\u65e8\u5728\u51cf\u5c11\u80fd\u8017\u5e76\u4fdd\u6301\u7ebf\u6027\u5316\u6027\u80fd\u3002", "method": "\u63d0\u51faTRes-DeltaGRU\u7b97\u6cd5\uff0c\u7ed3\u5408\u5b9a\u70b9\u91cf\u5316\u548c\u52a8\u6001\u65f6\u95f4\u7a00\u758f\u6027\u4f18\u5316\u6a21\u578b\u3002", "result": "FP32\u6a21\u578bACPR\u8fbe-59.4 dBc\uff0cEVM\u4e3a-42.1 dBc\uff1b\u4f18\u5316\u540e\u80fd\u8017\u964d\u4f4e4.5\u500d\uff0c\u6027\u80fd\u4ecd\u4fdd\u6301\u8f83\u9ad8\u6c34\u5e73\u3002", "conclusion": "OpenDPDv2\u5728\u964d\u4f4e\u80fd\u8017\u7684\u540c\u65f6\u4fdd\u6301\u4e86\u9ad8\u6027\u80fd\uff0c\u4ee3\u7801\u548c\u6570\u636e\u96c6\u5df2\u5f00\u6e90\u3002"}}
{"id": "2507.06417", "pdf": "https://arxiv.org/pdf/2507.06417", "abs": "https://arxiv.org/abs/2507.06417", "authors": ["Laura Pitukov\u00e1", "Peter Sin\u010d\u00e1k", "L\u00e1szl\u00f3 J\u00f3zsef Kov\u00e1cs"], "title": "Capsule-ConvKAN: A Hybrid Neural Approach to Medical Image Classification", "categories": ["eess.IV", "cs.CV", "cs.LG"], "comment": "Preprint version. Accepted to IEEE SMC 2025", "summary": "This study conducts a comprehensive comparison of four neural network\narchitectures: Convolutional Neural Network, Capsule Network, Convolutional\nKolmogorov--Arnold Network, and the newly proposed Capsule--Convolutional\nKolmogorov--Arnold Network. The proposed Capsule-ConvKAN architecture combines\nthe dynamic routing and spatial hierarchy capabilities of Capsule Network with\nthe flexible and interpretable function approximation of Convolutional\nKolmogorov--Arnold Networks. This novel hybrid model was developed to improve\nfeature representation and classification accuracy, particularly in challenging\nreal-world biomedical image data. The architectures were evaluated on a\nhistopathological image dataset, where Capsule-ConvKAN achieved the highest\nclassification performance with an accuracy of 91.21\\%. The results demonstrate\nthe potential of the newly introduced Capsule-ConvKAN in capturing spatial\npatterns, managing complex features, and addressing the limitations of\ntraditional convolutional models in medical image classification.", "AI": {"tldr": "\u6bd4\u8f83\u56db\u79cd\u795e\u7ecf\u7f51\u7edc\u67b6\u6784\uff0c\u63d0\u51fa\u65b0\u7684Capsule-ConvKAN\u6a21\u578b\uff0c\u5728\u751f\u7269\u533b\u5b66\u56fe\u50cf\u5206\u7c7b\u4e2d\u8868\u73b0\u6700\u4f73\u3002", "motivation": "\u6539\u8fdb\u7279\u5f81\u8868\u793a\u548c\u5206\u7c7b\u51c6\u786e\u6027\uff0c\u7279\u522b\u662f\u5728\u5177\u6709\u6311\u6218\u6027\u7684\u751f\u7269\u533b\u5b66\u56fe\u50cf\u6570\u636e\u4e2d\u3002", "method": "\u7ed3\u5408Capsule Network\u7684\u52a8\u6001\u8def\u7531\u548c\u7a7a\u95f4\u5c42\u6b21\u80fd\u529b\u4e0eConvolutional Kolmogorov--Arnold Networks\u7684\u7075\u6d3b\u53ef\u89e3\u91ca\u6027\u3002", "result": "\u5728\u7ec4\u7ec7\u75c5\u7406\u5b66\u56fe\u50cf\u6570\u636e\u96c6\u4e0a\uff0cCapsule-ConvKAN\u8fbe\u523091.21%\u7684\u6700\u9ad8\u5206\u7c7b\u51c6\u786e\u7387\u3002", "conclusion": "Capsule-ConvKAN\u5728\u6355\u6349\u7a7a\u95f4\u6a21\u5f0f\u548c\u7ba1\u7406\u590d\u6742\u7279\u5f81\u65b9\u9762\u5177\u6709\u6f5c\u529b\uff0c\u4f18\u4e8e\u4f20\u7edf\u5377\u79ef\u6a21\u578b\u3002"}}
{"id": "2507.06384", "pdf": "https://arxiv.org/pdf/2507.06384", "abs": "https://arxiv.org/abs/2507.06384", "authors": ["Emerson P. Grabke", "Babak Taati", "Masoom A. Haider"], "title": "Mitigating Multi-Sequence 3D Prostate MRI Data Scarcity through Domain Adaptation using Locally-Trained Latent Diffusion Models for Prostate Cancer Detection", "categories": ["eess.IV", "cs.CV"], "comment": "BT and MAH are co-senior authors on the work. This work has been\n  submitted to the IEEE for possible publication", "summary": "Objective: Latent diffusion models (LDMs) could mitigate data scarcity\nchallenges affecting machine learning development for medical image\ninterpretation. The recent CCELLA LDM improved prostate cancer detection\nperformance using synthetic MRI for classifier training but was limited to the\naxial T2-weighted (AxT2) sequence, did not investigate inter-institutional\ndomain shift, and prioritized radiology over histopathology outcomes. We\npropose CCELLA++ to address these limitations and improve clinical utility.\nMethods: CCELLA++ expands CCELLA for simultaneous biparametric prostate MRI\n(bpMRI) generation, including the AxT2, high b-value diffusion series (HighB)\nand apparent diffusion coefficient map (ADC). Domain adaptation was\ninvestigated by pretraining classifiers on real or LDM-generated synthetic data\nfrom an internal institution, followed with fine-tuning on progressively\nsmaller fractions of an out-of-distribution, external dataset. Results:\nCCELLA++ improved 3D FID for HighB and ADC but not AxT2 (0.013, 0.012, 0.063\nrespectively) sequences compared to CCELLA (0.060). Classifier pretraining with\nCCELLA++ bpMRI outperformed real bpMRI in AP and AUC for all domain adaptation\nscenarios. CCELLA++ pretraining achieved highest classifier performance below\n50% (n=665) external dataset volume. Conclusion: Synthetic bpMRI generated by\nour method can improve downstream classifier generalization and performance\nbeyond real bpMRI or CCELLA-generated AxT2-only images. Future work should seek\nto quantify medical image sample quality, balance multi-sequence LDM training,\nand condition the LDM with additional information. Significance: The proposed\nCCELLA++ LDM can generate synthetic bpMRI that outperforms real data for domain\nadaptation with a limited target institution dataset. Our code is available at\nhttps://github.com/grabkeem/CCELLA-plus-plus", "AI": {"tldr": "CCELLA++\u6269\u5c55\u4e86CCELLA\uff0c\u751f\u6210\u53cc\u53c2\u6570\u524d\u5217\u817aMRI\uff08bpMRI\uff09\uff0c\u5e76\u7814\u7a76\u4e86\u57df\u9002\u5e94\uff0c\u63d0\u9ad8\u4e86\u5206\u7c7b\u5668\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3CCELLA\u4ec5\u652f\u6301AxT2\u5e8f\u5217\u3001\u672a\u7814\u7a76\u57df\u504f\u79fb\u53ca\u672a\u5173\u6ce8\u75c5\u7406\u7ed3\u679c\u7684\u5c40\u9650\u6027\u3002", "method": "\u6269\u5c55CCELLA\u751f\u6210bpMRI\uff08AxT2\u3001HighB\u3001ADC\uff09\uff0c\u901a\u8fc7\u57df\u9002\u5e94\u5b9e\u9a8c\u8bc4\u4f30\u5408\u6210\u6570\u636e\u5bf9\u5206\u7c7b\u5668\u6027\u80fd\u7684\u5f71\u54cd\u3002", "result": "CCELLA++\u5728HighB\u548cADC\u5e8f\u5217\u4e0aFID\u4f18\u4e8eCCELLA\uff0c\u5408\u6210\u6570\u636e\u9884\u8bad\u7ec3\u7684\u5206\u7c7b\u5668\u6027\u80fd\u4f18\u4e8e\u771f\u5b9e\u6570\u636e\u3002", "conclusion": "CCELLA++\u751f\u6210\u7684\u5408\u6210bpMRI\u4f18\u4e8e\u771f\u5b9e\u6570\u636e\uff0c\u672a\u6765\u9700\u4f18\u5316\u591a\u5e8f\u5217\u8bad\u7ec3\u548c\u6837\u672c\u8d28\u91cf\u8bc4\u4f30\u3002"}}
{"id": "2507.06850", "pdf": "https://arxiv.org/pdf/2507.06850", "abs": "https://arxiv.org/abs/2507.06850", "authors": ["Matteo Lupinacci", "Francesco Aurelio Pironti", "Francesco Blefari", "Francesco Romeo", "Luigi Arena", "Angelo Furfaro"], "title": "The Dark Side of LLMs Agent-based Attacks for Complete Computer Takeover", "categories": ["cs.CR", "cs.AI"], "comment": null, "summary": "The rapid adoption of Large Language Model (LLM) agents and multi-agent\nsystems enables unprecedented capabilities in natural language processing and\ngeneration. However, these systems have introduced unprecedented security\nvulnerabilities that extend beyond traditional prompt injection attacks. This\npaper presents the first comprehensive evaluation of LLM agents as attack\nvectors capable of achieving complete computer takeover through the\nexploitation of trust boundaries within agentic AI systems where autonomous\nentities interact and influence each other. We demonstrate that adversaries can\nleverage three distinct attack surfaces - direct prompt injection, RAG backdoor\nattacks, and inter-agent trust exploitation - to coerce popular LLMs (including\nGPT-4o, Claude-4 and Gemini-2.5) into autonomously installing and executing\nmalware on victim machines. Our evaluation of 17 state-of-the-art LLMs reveals\nan alarming vulnerability hierarchy: while 41.2% of models succumb to direct\nprompt injection, 52.9% are vulnerable to RAG backdoor attacks, and a critical\n82.4% can be compromised through inter-agent trust exploitation. Notably, we\ndiscovered that LLMs which successfully resist direct malicious commands will\nexecute identical payloads when requested by peer agents, revealing a\nfundamental flaw in current multi-agent security models. Our findings\ndemonstrate that only 5.9% of tested models (1/17) proved resistant to all\nattack vectors, with the majority exhibiting context-dependent security\nbehaviors that create exploitable blind spots. Our findings also highlight the\nneed to increase awareness and research on the security risks of LLMs, showing\na paradigm shift in cybersecurity threats, where AI tools themselves become\nsophisticated attack vectors.", "AI": {"tldr": "\u8bba\u6587\u9996\u6b21\u5168\u9762\u8bc4\u4f30\u4e86LLM\u4ee3\u7406\u4f5c\u4e3a\u653b\u51fb\u5a92\u4ecb\u7684\u6f5c\u529b\uff0c\u63ed\u793a\u4e86\u5176\u5728\u591a\u4ee3\u7406\u7cfb\u7edf\u4e2d\u7684\u5b89\u5168\u6f0f\u6d1e\uff0c\u5305\u62ec\u76f4\u63a5\u63d0\u793a\u6ce8\u5165\u3001RAG\u540e\u95e8\u653b\u51fb\u548c\u4ee3\u7406\u95f4\u4fe1\u4efb\u5229\u7528\u3002", "motivation": "\u968f\u7740LLM\u4ee3\u7406\u548c\u591a\u4ee3\u7406\u7cfb\u7edf\u7684\u5e7f\u6cdb\u5e94\u7528\uff0c\u5176\u5b89\u5168\u6f0f\u6d1e\u95ee\u9898\u65e5\u76ca\u7a81\u51fa\uff0c\u4f20\u7edf\u653b\u51fb\u65b9\u5f0f\u5df2\u4e0d\u8db3\u4ee5\u5e94\u5bf9\u65b0\u5a01\u80c1\u3002", "method": "\u901a\u8fc7\u4e09\u79cd\u653b\u51fb\u65b9\u5f0f\uff08\u76f4\u63a5\u63d0\u793a\u6ce8\u5165\u3001RAG\u540e\u95e8\u653b\u51fb\u3001\u4ee3\u7406\u95f4\u4fe1\u4efb\u5229\u7528\uff09\u6d4b\u8bd517\u79cd\u5148\u8fdbLLM\u7684\u8106\u5f31\u6027\u3002", "result": "\u7ed3\u679c\u663e\u793a\uff0c82.4%\u7684\u6a21\u578b\u6613\u53d7\u4ee3\u7406\u95f4\u4fe1\u4efb\u5229\u7528\u653b\u51fb\uff0c\u4ec55.9%\u7684\u6a21\u578b\u80fd\u62b5\u6297\u6240\u6709\u653b\u51fb\u65b9\u5f0f\u3002", "conclusion": "\u7814\u7a76\u63ed\u793a\u4e86\u5f53\u524d\u591a\u4ee3\u7406\u5b89\u5168\u6a21\u578b\u7684\u6839\u672c\u7f3a\u9677\uff0c\u547c\u5401\u52a0\u5f3aLLM\u5b89\u5168\u98ce\u9669\u7684\u7814\u7a76\u548c\u610f\u8bc6\u3002"}}
{"id": "2507.06428", "pdf": "https://arxiv.org/pdf/2507.06428", "abs": "https://arxiv.org/abs/2507.06428", "authors": ["Samuel N. Cohen", "Jackson Hebner", "Deqing Jiang", "Justin Sirignano"], "title": "Neural Actor-Critic Methods for Hamilton-Jacobi-Bellman PDEs: Asymptotic Analysis and Numerical Studies", "categories": ["math.OC", "cs.LG", "cs.NA", "math.NA", "stat.ML", "93E20, 35Q93, 68T07, 90-08"], "comment": "41 pages", "summary": "We mathematically analyze and numerically study an actor-critic machine\nlearning algorithm for solving high-dimensional Hamilton-Jacobi-Bellman (HJB)\npartial differential equations from stochastic control theory. The architecture\nof the critic (the estimator for the value function) is structured so that the\nboundary condition is always perfectly satisfied (rather than being included in\nthe training loss) and utilizes a biased gradient which reduces computational\ncost. The actor (the estimator for the optimal control) is trained by\nminimizing the integral of the Hamiltonian over the domain, where the\nHamiltonian is estimated using the critic. We show that the training dynamics\nof the actor and critic neural networks converge in a Sobolev-type space to a\ncertain infinite-dimensional ordinary differential equation (ODE) as the number\nof hidden units in the actor and critic $\\rightarrow \\infty$. Further, under a\nconvexity-like assumption on the Hamiltonian, we prove that any fixed point of\nthis limit ODE is a solution of the original stochastic control problem. This\nprovides an important guarantee for the algorithm's performance in light of the\nfact that finite-width neural networks may only converge to a local minimizers\n(and not optimal solutions) due to the non-convexity of their loss functions.\nIn our numerical studies, we demonstrate that the algorithm can solve\nstochastic control problems accurately in up to 200 dimensions. In particular,\nwe construct a series of increasingly complex stochastic control problems with\nknown analytic solutions and study the algorithm's numerical performance on\nthem. These problems range from a linear-quadratic regulator equation to highly\nchallenging equations with non-convex Hamiltonians, allowing us to identify and\nanalyze the strengths and limitations of this neural actor-critic method for\nsolving HJB equations.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6f14\u5458-\u8bc4\u8bba\u5bb6\u67b6\u6784\u7684\u673a\u5668\u5b66\u4e60\u7b97\u6cd5\uff0c\u7528\u4e8e\u6c42\u89e3\u9ad8\u7ef4HJB\u65b9\u7a0b\uff0c\u5e76\u901a\u8fc7\u6570\u5b66\u5206\u6790\u548c\u6570\u503c\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "motivation": "\u89e3\u51b3\u9ad8\u7ef4HJB\u65b9\u7a0b\u5728\u968f\u673a\u63a7\u5236\u7406\u8bba\u4e2d\u7684\u8ba1\u7b97\u6311\u6218\uff0c\u540c\u65f6\u786e\u4fdd\u8fb9\u754c\u6761\u4ef6\u7684\u5b8c\u7f8e\u6ee1\u8db3\u548c\u8ba1\u7b97\u6548\u7387\u3002", "method": "\u91c7\u7528\u6f14\u5458-\u8bc4\u8bba\u5bb6\u67b6\u6784\uff0c\u8bc4\u8bba\u5bb6\u7f51\u7edc\u6ee1\u8db3\u8fb9\u754c\u6761\u4ef6\u5e76\u4f7f\u7528\u504f\u7f6e\u68af\u5ea6\uff0c\u6f14\u5458\u7f51\u7edc\u901a\u8fc7\u6700\u5c0f\u5316\u54c8\u5bc6\u987f\u91cf\u79ef\u5206\u6765\u8bad\u7ec3\u3002", "result": "\u8bc1\u660e\u4e86\u8bad\u7ec3\u52a8\u6001\u5728\u65e0\u9650\u7ef4ODE\u4e2d\u6536\u655b\uff0c\u4e14\u5728\u51f8\u6027\u5047\u8bbe\u4e0b\uff0c\u56fa\u5b9a\u70b9\u4e3a\u539f\u95ee\u9898\u7684\u89e3\u3002\u6570\u503c\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u7b97\u6cd5\u5728200\u7ef4\u95ee\u9898\u4e2d\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u8be5\u7b97\u6cd5\u4e3a\u9ad8\u7ef4HJB\u65b9\u7a0b\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u4e14\u7406\u8bba\u4fdd\u8bc1\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u4f46\u9700\u6ce8\u610f\u975e\u51f8\u54c8\u5bc6\u987f\u91cf\u4e0b\u7684\u5c40\u9650\u6027\u3002"}}
{"id": "2507.06410", "pdf": "https://arxiv.org/pdf/2507.06410", "abs": "https://arxiv.org/abs/2507.06410", "authors": ["Peyman Sharifian", "Xiaotong Hong", "Alireza Karimian", "Mehdi Amini", "Hossein Arabi"], "title": "Attention-Enhanced Deep Learning Ensemble for Breast Density Classification in Mammography", "categories": ["eess.IV", "cs.CV"], "comment": "2025 IEEE Nuclear Science Symposium, Medical Imaging Conference and\n  Room Temperature Semiconductor Detector Conference", "summary": "Breast density assessment is a crucial component of mammographic\ninterpretation, with high breast density (BI-RADS categories C and D)\nrepresenting both a significant risk factor for developing breast cancer and a\ntechnical challenge for tumor detection. This study proposes an automated deep\nlearning system for robust binary classification of breast density (low: A/B\nvs. high: C/D) using the VinDr-Mammo dataset. We implemented and compared four\nadvanced convolutional neural networks: ResNet18, ResNet50, EfficientNet-B0,\nand DenseNet121, each enhanced with channel attention mechanisms. To address\nthe inherent class imbalance, we developed a novel Combined Focal Label\nSmoothing Loss function that integrates focal loss, label smoothing, and\nclass-balanced weighting. Our preprocessing pipeline incorporated advanced\ntechniques, including contrast-limited adaptive histogram equalization (CLAHE)\nand comprehensive data augmentation. The individual models were combined\nthrough an optimized ensemble voting approach, achieving superior performance\n(AUC: 0.963, F1-score: 0.952) compared to any single model. This system\ndemonstrates significant potential to standardize density assessments in\nclinical practice, potentially improving screening efficiency and early cancer\ndetection rates while reducing inter-observer variability among radiologists.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u81ea\u52a8\u5316\u7cfb\u7edf\uff0c\u7528\u4e8e\u4e73\u817a\u5bc6\u5ea6\u7684\u4e8c\u5143\u5206\u7c7b\uff08\u4f4e\u5bc6\u5ea6A/B vs \u9ad8\u5bc6\u5ea6C/D\uff09\uff0c\u901a\u8fc7\u96c6\u6210\u591a\u79cd\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u548c\u65b0\u578b\u635f\u5931\u51fd\u6570\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5206\u7c7b\u6027\u80fd\u3002", "motivation": "\u4e73\u817a\u5bc6\u5ea6\u8bc4\u4f30\u5bf9\u4e73\u817a\u764c\u7b5b\u67e5\u81f3\u5173\u91cd\u8981\uff0c\u9ad8\u5bc6\u5ea6\u4e0d\u4ec5\u589e\u52a0\u60a3\u764c\u98ce\u9669\uff0c\u8fd8\u5f71\u54cd\u80bf\u7624\u68c0\u6d4b\u3002\u73b0\u6709\u65b9\u6cd5\u5b58\u5728\u5206\u7c7b\u4e0d\u4e00\u81f4\u548c\u6280\u672f\u6311\u6218\uff0c\u9700\u8981\u81ea\u52a8\u5316\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u7814\u7a76\u4f7f\u7528VinDr-Mammo\u6570\u636e\u96c6\uff0c\u6bd4\u8f83\u4e86\u56db\u79cd\u6539\u8fdb\u7684\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\uff08ResNet18\u3001ResNet50\u3001EfficientNet-B0\u3001DenseNet121\uff09\uff0c\u5e76\u63d0\u51fa\u4e86\u7ed3\u5408\u7126\u70b9\u635f\u5931\u3001\u6807\u7b7e\u5e73\u6ed1\u548c\u7c7b\u522b\u5e73\u8861\u6743\u91cd\u7684\u65b0\u578b\u635f\u5931\u51fd\u6570\u3002\u9884\u5904\u7406\u5305\u62ecCLAHE\u548c\u6570\u636e\u589e\u5f3a\uff0c\u6700\u7ec8\u901a\u8fc7\u96c6\u6210\u6295\u7968\u4f18\u5316\u6027\u80fd\u3002", "result": "\u7cfb\u7edf\u5728AUC\uff080.963\uff09\u548cF1\u5206\u6570\uff080.952\uff09\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u4f18\u4e8e\u5355\u4e00\u6a21\u578b\u3002", "conclusion": "\u8be5\u7cfb\u7edf\u6709\u671b\u6807\u51c6\u5316\u4e34\u5e8a\u4e73\u817a\u5bc6\u5ea6\u8bc4\u4f30\uff0c\u63d0\u9ad8\u7b5b\u67e5\u6548\u7387\u548c\u65e9\u671f\u764c\u75c7\u68c0\u51fa\u7387\uff0c\u51cf\u5c11\u653e\u5c04\u79d1\u533b\u751f\u95f4\u7684\u5dee\u5f02\u3002"}}
{"id": "2507.06876", "pdf": "https://arxiv.org/pdf/2507.06876", "abs": "https://arxiv.org/abs/2507.06876", "authors": ["Adrian Rauchfleisch", "Joshua Philip Suarez", "Nikka Marie Sales", "Andreas Jungherr"], "title": "Winning and losing with Artificial Intelligence: What public discourse about ChatGPT tells us about how societies make sense of technological change", "categories": ["cs.CY", "cs.AI", "I.2; J.4; K.4.0"], "comment": null, "summary": "Public product launches in Artificial Intelligence can serve as focusing\nevents for collective attention, surfacing how societies react to technological\nchange. Social media provide a window into the sensemaking around these events,\nsurfacing hopes and fears and showing who chooses to engage in the discourse\nand when. We demonstrate that public sensemaking about AI is shaped by economic\ninterests and cultural values of those involved. We analyze 3.8 million tweets\nposted by 1.6 million users across 117 countries in response to the public\nlaunch of ChatGPT in 2022. Our analysis shows how economic self-interest,\nproxied by occupational skill types in writing, programming, and mathematics,\nand national cultural orientations, as measured by Hofstede's individualism,\nuncertainty avoidance, and power distance dimensions, shape who speaks, when\nthey speak, and their stance towards ChatGPT. Roles requiring more technical\nskills, such as programming and mathematics, tend to engage earlier and express\nmore positive stances, whereas writing-centric occupations join later with\ngreater skepticism. At the cultural level, individualism predicts both earlier\nengagement and a more negative stance, and uncertainty avoidance reduces the\nprevalence of positive stances but does not delay when users first engage with\nChatGPT. Aggregate sentiment trends mask the dynamics observed in our study.\nThe shift toward a more critical stance towards ChatGPT over time stems\nprimarily from the entry of more skeptical voices rather than a change of heart\namong early adopters. Our findings underscore the importance of both the\noccupational background and cultural context in understanding public reactions\nto AI.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u516c\u4f17\u5bf9ChatGPT\u53d1\u5e03\u7684\u53cd\u5e94\uff0c\u5206\u6790\u4e86\u7ecf\u6d4e\u548c\u6587\u5316\u56e0\u7d20\u5982\u4f55\u5f71\u54cd\u53c2\u4e0e\u8005\u7684\u53d1\u58f0\u65f6\u95f4\u3001\u6001\u5ea6\u548c\u7acb\u573a\u3002", "motivation": "\u63a2\u7d22\u793e\u4f1a\u5bf9AI\u6280\u672f\u53d8\u9769\u7684\u53cd\u5e94\uff0c\u63ed\u793a\u7ecf\u6d4e\u548c\u6587\u5316\u80cc\u666f\u5982\u4f55\u5851\u9020\u516c\u4f17\u5bf9AI\u7684\u8ba8\u8bba\u3002", "method": "\u5206\u6790\u4e86\u6765\u81ea117\u4e2a\u56fd\u5bb6\u76843.8\u767e\u4e07\u6761\u63a8\u6587\uff0c\u7ed3\u5408\u804c\u4e1a\u6280\u80fd\u7c7b\u578b\uff08\u5199\u4f5c\u3001\u7f16\u7a0b\u3001\u6570\u5b66\uff09\u548c\u970d\u592b\u65af\u6cf0\u5fb7\u6587\u5316\u7ef4\u5ea6\uff08\u4e2a\u4eba\u4e3b\u4e49\u3001\u4e0d\u786e\u5b9a\u6027\u89c4\u907f\u3001\u6743\u529b\u8ddd\u79bb\uff09\u3002", "result": "\u6280\u672f\u578b\u804c\u4e1a\uff08\u5982\u7f16\u7a0b\u3001\u6570\u5b66\uff09\u66f4\u65e9\u53c2\u4e0e\u4e14\u6001\u5ea6\u79ef\u6781\uff0c\u5199\u4f5c\u578b\u804c\u4e1a\u8f83\u665a\u4e14\u66f4\u6000\u7591\uff1b\u4e2a\u4eba\u4e3b\u4e49\u6587\u5316\u66f4\u65e9\u53c2\u4e0e\u4f46\u6001\u5ea6\u8d1f\u9762\uff0c\u4e0d\u786e\u5b9a\u6027\u89c4\u907f\u51cf\u5c11\u79ef\u6781\u6001\u5ea6\u3002", "conclusion": "\u804c\u4e1a\u80cc\u666f\u548c\u6587\u5316\u80cc\u666f\u5bf9\u516c\u4f17\u5bf9AI\u7684\u53cd\u5e94\u6709\u663e\u8457\u5f71\u54cd\uff0c\u5f3a\u8c03\u4e86\u7406\u89e3\u8fd9\u4e9b\u56e0\u7d20\u7684\u91cd\u8981\u6027\u3002"}}
{"id": "2507.06472", "pdf": "https://arxiv.org/pdf/2507.06472", "abs": "https://arxiv.org/abs/2507.06472", "authors": ["Tian Li", "Artem Polyvyanyy", "Sander J. J. Leemans"], "title": "Stochastic Alignments: Matching an Observed Trace to Stochastic Process Models", "categories": ["cs.FL", "cs.LG"], "comment": null, "summary": "Process mining leverages event data extracted from IT systems to generate\ninsights into the business processes of organizations. Such insights benefit\nfrom explicitly considering the frequency of behavior in business processes,\nwhich is captured by stochastic process models. Given an observed trace and a\nstochastic process model, conventional alignment-based conformance checking\ntechniques face a fundamental limitation: They prioritize matching the trace to\na model path with minimal deviations, which may, however, lead to selecting an\nunlikely path. In this paper, we study the problem of matching an observed\ntrace to a stochastic process model by identifying a likely model path with a\nlow edit distance to the trace. We phrase this as an optimization problem and\ndevelop a heuristic-guided path-finding algorithm to solve it. Our open-source\nimplementation demonstrates the feasibility of the approach and shows that it\ncan provide new, useful diagnostic insights for analysts.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u542f\u53d1\u5f0f\u8def\u5f84\u67e5\u627e\u7684\u7b97\u6cd5\uff0c\u7528\u4e8e\u5c06\u89c2\u5bdf\u5230\u7684\u8f68\u8ff9\u4e0e\u968f\u673a\u8fc7\u7a0b\u6a21\u578b\u5339\u914d\uff0c\u4f18\u5316\u7f16\u8f91\u8ddd\u79bb\u5e76\u63d0\u9ad8\u8def\u5f84\u7684\u4f3c\u7136\u6027\u3002", "motivation": "\u4f20\u7edf\u57fa\u4e8e\u5bf9\u9f50\u7684\u7b26\u5408\u6027\u68c0\u67e5\u6280\u672f\u53ef\u80fd\u9009\u62e9\u4e0d\u592a\u53ef\u80fd\u7684\u8def\u5f84\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u65b9\u6cd5\u80fd\u591f\u540c\u65f6\u8003\u8651\u8def\u5f84\u7684\u4f3c\u7136\u6027\u548c\u7f16\u8f91\u8ddd\u79bb\u3002", "method": "\u5c06\u95ee\u9898\u8868\u8ff0\u4e3a\u4f18\u5316\u95ee\u9898\uff0c\u5e76\u5f00\u53d1\u4e86\u4e00\u79cd\u542f\u53d1\u5f0f\u5f15\u5bfc\u7684\u8def\u5f84\u67e5\u627e\u7b97\u6cd5\u3002", "result": "\u5f00\u6e90\u5b9e\u73b0\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u53ef\u884c\u6027\uff0c\u5e76\u4e3a\u5206\u6790\u5e08\u63d0\u4f9b\u4e86\u65b0\u7684\u8bca\u65ad\u89c1\u89e3\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u5339\u914d\u8f68\u8ff9\u4e0e\u6a21\u578b\uff0c\u63d0\u4f9b\u66f4\u51c6\u786e\u7684\u8bca\u65ad\u4fe1\u606f\u3002"}}
{"id": "2507.06418", "pdf": "https://arxiv.org/pdf/2507.06418", "abs": "https://arxiv.org/abs/2507.06418", "authors": ["Changchun Yang", "Haoyang Li", "Yushuai Wu", "Yilan Zhang", "Yifeng Jiao", "Yu Zhang", "Rihan Huang", "Yuan Cheng", "Yuan Qi", "Xin Guo", "Xin Gao"], "title": "PAST: A multimodal single-cell foundation model for histopathology and spatial transcriptomics in cancer", "categories": ["q-bio.QM", "cs.CV", "stat.AP"], "comment": null, "summary": "While pathology foundation models have transformed cancer image analysis,\nthey often lack integration with molecular data at single-cell resolution,\nlimiting their utility for precision oncology. Here, we present PAST, a\npan-cancer single-cell foundation model trained on 20 million paired\nhistopathology images and single-cell transcriptomes spanning multiple tumor\ntypes and tissue contexts. By jointly encoding cellular morphology and gene\nexpression, PAST learns unified cross-modal representations that capture both\nspatial and molecular heterogeneity at the cellular level. This approach\nenables accurate prediction of single-cell gene expression, virtual molecular\nstaining, and multimodal survival analysis directly from routine pathology\nslides. Across diverse cancers and downstream tasks, PAST consistently exceeds\nthe performance of existing approaches, demonstrating robust generalizability\nand scalability. Our work establishes a new paradigm for pathology foundation\nmodels, providing a versatile tool for high-resolution spatial omics,\nmechanistic discovery, and precision cancer research.", "AI": {"tldr": "PAST\u662f\u4e00\u4e2a\u57fa\u4e8e\u75c5\u7406\u56fe\u50cf\u548c\u5355\u7ec6\u80de\u8f6c\u5f55\u7ec4\u7684\u8de8\u6a21\u6001\u57fa\u7840\u6a21\u578b\uff0c\u80fd\u591f\u8054\u5408\u7f16\u7801\u7ec6\u80de\u5f62\u6001\u548c\u57fa\u56e0\u8868\u8fbe\uff0c\u63d0\u5347\u7cbe\u51c6\u80bf\u7624\u5b66\u7814\u7a76\u7684\u5b9e\u7528\u6027\u3002", "motivation": "\u73b0\u6709\u7684\u75c5\u7406\u57fa\u7840\u6a21\u578b\u7f3a\u4e4f\u4e0e\u5355\u7ec6\u80de\u5206\u5b50\u6570\u636e\u7684\u6574\u5408\uff0c\u9650\u5236\u4e86\u5176\u5728\u7cbe\u51c6\u80bf\u7624\u5b66\u4e2d\u7684\u5e94\u7528\u3002", "method": "PAST\u901a\u8fc7\u8bad\u7ec32000\u4e07\u5bf9\u75c5\u7406\u56fe\u50cf\u548c\u5355\u7ec6\u80de\u8f6c\u5f55\u7ec4\u6570\u636e\uff0c\u5b66\u4e60\u8de8\u6a21\u6001\u7684\u7edf\u4e00\u8868\u793a\uff0c\u6355\u6349\u7ec6\u80de\u6c34\u5e73\u7684\u7a7a\u95f4\u548c\u5206\u5b50\u5f02\u8d28\u6027\u3002", "result": "PAST\u80fd\u591f\u51c6\u786e\u9884\u6d4b\u5355\u7ec6\u80de\u57fa\u56e0\u8868\u8fbe\u3001\u865a\u62df\u5206\u5b50\u67d3\u8272\u548c\u591a\u6a21\u6001\u751f\u5b58\u5206\u6790\uff0c\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "PAST\u4e3a\u75c5\u7406\u57fa\u7840\u6a21\u578b\u63d0\u4f9b\u4e86\u65b0\u8303\u5f0f\uff0c\u652f\u6301\u9ad8\u5206\u8fa8\u7387\u7a7a\u95f4\u7ec4\u5b66\u3001\u673a\u5236\u53d1\u73b0\u548c\u7cbe\u51c6\u764c\u75c7\u7814\u7a76\u3002"}}
{"id": "2507.06890", "pdf": "https://arxiv.org/pdf/2507.06890", "abs": "https://arxiv.org/abs/2507.06890", "authors": ["Yifan Wang"], "title": "A Single-Point Measurement Framework for Robust Cyber-Attack Diagnosis in Smart Microgrids Using Dual Fractional-Order Feature Analysis", "categories": ["eess.SY", "cs.AI", "cs.SY"], "comment": "8 pages, 10 figures", "summary": "Cyber-attacks jeopardize the safe operation of smart microgrids. At the same\ntime, existing diagnostic methods either depend on expensive multi-point\ninstrumentation or stringent modelling assumptions that are untenable under\nsingle-sensor constraints. This paper proposes a Fractional-Order\nMemory-Enhanced Attack-Diagnosis Scheme (FO-MADS) that achieves low-latency\nfault localisation and cyber-attack detection using only one VPQ\n(Voltage-Power-Reactive-power) sensor. FO-MADS first constructs a dual\nfractional-order feature library by jointly applying Caputo and\nGr\\\"unwald-Letnikov derivatives, thereby amplifying micro-perturbations and\nslow drifts in the VPQ signal. A two-stage hierarchical classifier then\npinpoints the affected inverter and isolates the faulty IGBT switch,\neffectively alleviating class imbalance. Robustness is further strengthened\nthrough Progressive Memory-Replay Adversarial Training (PMR-AT), whose\nattack-aware loss is dynamically re-weighted via Online Hard Example Mining\n(OHEM) to prioritise the most challenging samples. Experiments on a\nfour-inverter microgrid testbed comprising 1 normal and 24 fault classes under\nfour attack scenarios demonstrate diagnostic accuracies of 96.6 % (bias), 94.0\n% (noise), 92.8 % (data replacement), and 95.7 % (replay), while sustaining\n96.7 % under attack-free conditions. These results establish FO-MADS as a\ncost-effective and readily deployable solution that markedly enhances the\ncyber-physical resilience of smart microgrids.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5355\u4f20\u611f\u5668\u7684\u5206\u6570\u9636\u8bb0\u5fc6\u589e\u5f3a\u653b\u51fb\u8bca\u65ad\u65b9\u6848\uff08FO-MADS\uff09\uff0c\u7528\u4e8e\u667a\u80fd\u5fae\u7535\u7f51\u7684\u4f4e\u5ef6\u8fdf\u6545\u969c\u5b9a\u4f4d\u548c\u7f51\u7edc\u653b\u51fb\u68c0\u6d4b\u3002", "motivation": "\u73b0\u6709\u8bca\u65ad\u65b9\u6cd5\u4f9d\u8d56\u6602\u8d35\u7684\u591a\u70b9\u4eea\u5668\u6216\u4e25\u683c\u7684\u5efa\u6a21\u5047\u8bbe\uff0c\u65e0\u6cd5\u5728\u5355\u4f20\u611f\u5668\u9650\u5236\u4e0b\u6709\u6548\u5de5\u4f5c\uff0c\u667a\u80fd\u5fae\u7535\u7f51\u7684\u7f51\u7edc\u5b89\u5168\u53d7\u5230\u5a01\u80c1\u3002", "method": "\u5229\u7528Caputo\u548cGr\u00fcnwald-Letnikov\u5bfc\u6570\u6784\u5efa\u53cc\u5206\u6570\u9636\u7279\u5f81\u5e93\uff0c\u653e\u5927\u4fe1\u53f7\u4e2d\u7684\u5fae\u6270\u52a8\u548c\u6162\u6f02\u79fb\uff1b\u91c7\u7528\u4e24\u9636\u6bb5\u5206\u5c42\u5206\u7c7b\u5668\u5b9a\u4f4d\u6545\u969c\u9006\u53d8\u5668\u548cIGBT\u5f00\u5173\uff1b\u901a\u8fc7\u6e10\u8fdb\u8bb0\u5fc6\u56de\u653e\u5bf9\u6297\u8bad\u7ec3\uff08PMR-AT\uff09\u589e\u5f3a\u9c81\u68d2\u6027\u3002", "result": "\u5728\u56db\u9006\u53d8\u5668\u5fae\u7535\u7f51\u6d4b\u8bd5\u4e2d\uff0c\u5bf9\u56db\u79cd\u653b\u51fb\u573a\u666f\u7684\u8bca\u65ad\u51c6\u786e\u7387\u5206\u522b\u4e3a96.6%\uff08\u504f\u7f6e\uff09\u300194.0%\uff08\u566a\u58f0\uff09\u300192.8%\uff08\u6570\u636e\u66ff\u6362\uff09\u548c95.7%\uff08\u56de\u653e\uff09\uff0c\u65e0\u653b\u51fb\u65f6\u51c6\u786e\u7387\u4e3a96.7%\u3002", "conclusion": "FO-MADS\u662f\u4e00\u79cd\u6210\u672c\u4f4e\u3001\u6613\u90e8\u7f72\u7684\u65b9\u6848\uff0c\u663e\u8457\u63d0\u5347\u4e86\u667a\u80fd\u5fae\u7535\u7f51\u7684\u7f51\u7edc\u5b89\u5168\u548c\u7269\u7406\u97e7\u6027\u3002"}}
{"id": "2507.06484", "pdf": "https://arxiv.org/pdf/2507.06484", "abs": "https://arxiv.org/abs/2507.06484", "authors": ["Fan-Yun Sun", "Shengguang Wu", "Christian Jacobsen", "Thomas Yim", "Haoming Zou", "Alex Zook", "Shangru Li", "Yu-Hsin Chou", "Ethem Can", "Xunlei Wu", "Clemens Eppner", "Valts Blukis", "Jonathan Tremblay", "Jiajun Wu", "Stan Birchfield", "Nick Haber"], "title": "3D-Generalist: Self-Improving Vision-Language-Action Models for Crafting 3D Worlds", "categories": ["cs.GR", "cs.CV"], "comment": "project website: https://ai.stanford.edu/~sunfanyun/3d-generalist/", "summary": "Despite large-scale pretraining endowing models with language and vision\nreasoning capabilities, improving their spatial reasoning capability remains\nchallenging due to the lack of data grounded in the 3D world. While it is\npossible for humans to manually create immersive and interactive worlds through\n3D graphics, as seen in applications such as VR, gaming, and robotics, this\nprocess remains highly labor-intensive. In this paper, we propose a scalable\nmethod for generating high-quality 3D environments that can serve as training\ndata for foundation models. We recast 3D environment building as a sequential\ndecision-making problem, employing Vision-Language-Models (VLMs) as policies\nthat output actions to jointly craft a 3D environment's layout, materials,\nlighting, and assets. Our proposed framework, 3D-Generalist, trains VLMs to\ngenerate more prompt-aligned 3D environments via self-improvement fine-tuning.\nWe demonstrate the effectiveness of 3D-Generalist and the proposed training\nstrategy in generating simulation-ready 3D environments. Furthermore, we\ndemonstrate its quality and scalability in synthetic data generation by\npretraining a vision foundation model on the generated data. After fine-tuning\nthe pre-trained model on downstream tasks, we show that it surpasses models\npre-trained on meticulously human-crafted synthetic data and approaches results\nachieved with real data orders of magnitude larger.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u53ef\u6269\u5c55\u7684\u65b9\u6cd53D-Generalist\uff0c\u5229\u7528\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLMs\uff09\u751f\u6210\u9ad8\u8d28\u91cf3D\u73af\u5883\u4f5c\u4e3a\u57fa\u7840\u6a21\u578b\u7684\u8bad\u7ec3\u6570\u636e\uff0c\u5e76\u901a\u8fc7\u81ea\u6539\u8fdb\u5fae\u8c03\u63d0\u5347\u751f\u6210\u6548\u679c\u3002", "motivation": "\u5927\u89c4\u6a21\u9884\u8bad\u7ec3\u6a21\u578b\u5728\u7a7a\u95f4\u63a8\u7406\u80fd\u529b\u4e0a\u4ecd\u5b58\u5728\u4e0d\u8db3\uff0c\u4e3b\u8981\u56e0\u7f3a\u4e4f\u57fa\u4e8e3D\u4e16\u754c\u7684\u6570\u636e\u3002\u4eba\u5de5\u521b\u5efa3D\u73af\u5883\u6210\u672c\u9ad8\uff0c\u9700\u81ea\u52a8\u5316\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u5c063D\u73af\u5883\u6784\u5efa\u8f6c\u5316\u4e3a\u5e8f\u5217\u51b3\u7b56\u95ee\u9898\uff0c\u5229\u7528VLMs\u4f5c\u4e3a\u7b56\u7565\u751f\u6210\u5e03\u5c40\u3001\u6750\u8d28\u3001\u5149\u7167\u7b49\uff0c\u5e76\u901a\u8fc7\u81ea\u6539\u8fdb\u5fae\u8c03\u4f18\u5316\u751f\u6210\u6548\u679c\u3002", "result": "3D-Generalist\u80fd\u751f\u6210\u4eff\u771f\u5c31\u7eea\u76843D\u73af\u5883\uff0c\u5176\u751f\u6210\u6570\u636e\u9884\u8bad\u7ec3\u7684\u89c6\u89c9\u57fa\u7840\u6a21\u578b\u5728\u4e0b\u6e38\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u4e8e\u4eba\u5de5\u5408\u6210\u6570\u636e\u8bad\u7ec3\u7684\u6a21\u578b\uff0c\u63a5\u8fd1\u771f\u5b9e\u6570\u636e\u6548\u679c\u3002", "conclusion": "3D-Generalist\u4e3a3D\u73af\u5883\u751f\u6210\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u3001\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u663e\u8457\u63d0\u5347\u4e86\u57fa\u7840\u6a21\u578b\u7684\u8bad\u7ec3\u6570\u636e\u8d28\u91cf\u3002"}}
{"id": "2507.06581", "pdf": "https://arxiv.org/pdf/2507.06581", "abs": "https://arxiv.org/abs/2507.06581", "authors": ["Qibiao Wu", "Yagang Wang", "Qian Zhang"], "title": "Airway Segmentation Network for Enhanced Tubular Feature Extraction", "categories": ["eess.IV", "cs.CV"], "comment": null, "summary": "Manual annotation of airway regions in computed tomography images is a\ntime-consuming and expertise-dependent task. Automatic airway segmentation is\ntherefore a prerequisite for enabling rapid bronchoscopic navigation and the\nclinical deployment of bronchoscopic robotic systems. Although convolutional\nneural network methods have gained considerable attention in airway\nsegmentation, the unique tree-like structure of airways poses challenges for\nconventional and deformable convolutions, which often fail to focus on fine\nairway structures, leading to missed segments and discontinuities. To address\nthis issue, this study proposes a novel tubular feature extraction network,\nnamed TfeNet. TfeNet introduces a novel direction-aware convolution operation\nthat first applies spatial rotation transformations to adjust the sampling\npositions of linear convolution kernels. The deformed kernels are then\nrepresented as line segments or polylines in 3D space. Furthermore, a tubular\nfeature fusion module (TFFM) is designed based on asymmetric convolution and\nresidual connection strategies, enhancing the network's focus on subtle airway\nstructures. Extensive experiments conducted on one public dataset and two\ndatasets used in airway segmentation challenges demonstrate that the proposed\nTfeNet achieves more accuracy and continuous airway structure predictions\ncompared with existing methods. In particular, TfeNet achieves the highest\noverall score of 94.95% on the current largest airway segmentation dataset,\nAirway Tree Modeling(ATM22), and demonstrates advanced performance on the lung\nfibrosis dataset(AIIB23). The code is available at\nhttps://github.com/QibiaoWu/TfeNet.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aTfeNet\u7684\u65b0\u578b\u7ba1\u72b6\u7279\u5f81\u63d0\u53d6\u7f51\u7edc\uff0c\u7528\u4e8e\u89e3\u51b3\u4f20\u7edf\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u5728\u6c14\u9053\u5206\u5272\u4e2d\u96be\u4ee5\u6355\u6349\u7ec6\u5fae\u7ed3\u6784\u7684\u95ee\u9898\u3002", "motivation": "\u624b\u52a8\u6807\u6ce8\u6c14\u9053\u533a\u57df\u8017\u65f6\u4e14\u4f9d\u8d56\u4e13\u4e1a\u77e5\u8bc6\uff0c\u81ea\u52a8\u5206\u5272\u662f\u5feb\u901f\u652f\u6c14\u7ba1\u955c\u5bfc\u822a\u548c\u673a\u5668\u4eba\u7cfb\u7edf\u4e34\u5e8a\u90e8\u7f72\u7684\u524d\u63d0\u3002", "method": "TfeNet\u5f15\u5165\u4e86\u65b9\u5411\u611f\u77e5\u5377\u79ef\u64cd\u4f5c\u548c\u7ba1\u72b6\u7279\u5f81\u878d\u5408\u6a21\u5757\uff08TFFM\uff09\uff0c\u901a\u8fc7\u7a7a\u95f4\u65cb\u8f6c\u53d8\u6362\u548c\u975e\u5bf9\u79f0\u5377\u79ef\u589e\u5f3a\u5bf9\u7ec6\u5fae\u6c14\u9053\u7ed3\u6784\u7684\u5173\u6ce8\u3002", "result": "\u5728\u516c\u5f00\u6570\u636e\u96c6\u548c\u6311\u6218\u6570\u636e\u96c6\u4e0a\uff0cTfeNet\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u6700\u9ad8\u5f97\u5206\u8fbe94.95%\u3002", "conclusion": "TfeNet\u5728\u6c14\u9053\u5206\u5272\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u66f4\u9ad8\u7684\u51c6\u786e\u6027\u548c\u8fde\u7eed\u6027\uff0c\u5177\u6709\u4e34\u5e8a\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2507.06509", "pdf": "https://arxiv.org/pdf/2507.06509", "abs": "https://arxiv.org/abs/2507.06509", "authors": ["Yangguang Shi", "Zhenyu Xue"], "title": "Prediction-Augmented Mechanism Design for Weighted Facility Location", "categories": ["cs.DS", "cs.GT", "cs.LG", "68W27, 68Q32", "F.2.2"], "comment": "An extended abstract of this paper is to appear in the 19th Annual\n  Conference on Theory and Applications of Models of Computation (TAMC 2025)", "summary": "Facility location is fundamental in operations research, mechanism design,\nand algorithmic game theory, with applications ranging from urban\ninfrastructure planning to distributed systems. Recent research in this area\nhas focused on augmenting classic strategyproof mechanisms with predictions to\nachieve an improved performance guarantee against the uncertainty under the\nstrategic environment. Previous work has been devoted to address the trade-off\nobstacle of balancing the consistency (near-optimality under accurate\npredictions) and robustness (bounded inefficiency under poor predictions)\nprimarily in the unweighted setting, assuming that all agents have the same\nimportance. However, this assumption may not be true in some practical\nscenarios, leading to research of weighted facility location problems.\n  The major contribution of the current work is to provide a prediction\naugmented algorithmic framework for balancing the consistency and robustness\nover strategic agents with non-uniform weights. In particular, through a\nreduction technique that identifies a subset of \\emph{representative} instances\nand maps the other given locations to the representative ones, we prove that\nthere exists a \\emph{strategyproof} mechanism achieving a bounded consistency\nguarantee of $\\frac{\\sqrt{(1+c)^2W^2_{\\min}+(1-c)^2W^2_{\\max}}}{(1+c)W_{\\min}}$\nand a bounded robustness guarantee of\n$\\frac{\\sqrt{(1-c)^2W^2_{\\min}+(1+c)^2W^2_{\\max}}}{(1-c)W_{\\min}}$ in weighted\nsettings, where $c$ can be viewed as a parameter to make a trade-off between\nthe consistency and robustness and $W_{\\min}$ and $W_{\\max}$ denote the minimum\nand maximum agents' weight. We also proved that there is no strategyproof\ndeterministic mechanism that reach $1$-consistency and $O\\left( n \\cdot\n\\frac{W_{\\max}}{W_{\\min}} \\right)$-robustness in weighted FLP, even with fully\npredictions of all agents.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9488\u5bf9\u52a0\u6743\u8bbe\u65bd\u9009\u5740\u95ee\u9898\u7684\u9884\u6d4b\u589e\u5f3a\u7b97\u6cd5\u6846\u67b6\uff0c\u901a\u8fc7\u4ee3\u8868\u6027\u5b9e\u4f8b\u7684\u6620\u5c04\u6280\u672f\uff0c\u5e73\u8861\u4e86\u7b56\u7565\u8bc1\u660e\u673a\u5236\u5728\u51c6\u786e\u9884\u6d4b\u548c\u8f83\u5dee\u9884\u6d4b\u4e0b\u7684\u6027\u80fd\u4fdd\u8bc1\u3002", "motivation": "\u7ecf\u5178\u7b56\u7565\u8bc1\u660e\u673a\u5236\u5728\u52a0\u6743\u8bbe\u65bd\u9009\u5740\u95ee\u9898\u4e2d\u96be\u4ee5\u5e73\u8861\u4e00\u81f4\u6027\u548c\u9c81\u68d2\u6027\uff0c\u672c\u6587\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u901a\u8fc7\u4ee3\u8868\u6027\u5b9e\u4f8b\u7684\u6620\u5c04\u6280\u672f\uff0c\u8bbe\u8ba1\u7b56\u7565\u8bc1\u660e\u673a\u5236\uff0c\u5e76\u8bc1\u660e\u5176\u5728\u52a0\u6743\u8bbe\u7f6e\u4e0b\u7684\u6027\u80fd\u754c\u9650\u3002", "result": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u7b56\u7565\u8bc1\u660e\u673a\u5236\uff0c\u5176\u4e00\u81f4\u6027\u4fdd\u8bc1\u4e3a\u221a((1+c)\u00b2W\u00b2_min+(1-c)\u00b2W\u00b2_max)/((1+c)W_min)\uff0c\u9c81\u68d2\u6027\u4fdd\u8bc1\u4e3a\u221a((1-c)\u00b2W\u00b2_min+(1+c)\u00b2W\u00b2_max)/((1-c)W_min)\u3002", "conclusion": "\u8bc1\u660e\u4e86\u5728\u52a0\u6743\u8bbe\u65bd\u9009\u5740\u95ee\u9898\u4e2d\uff0c\u65e0\u6cd5\u901a\u8fc7\u7b56\u7565\u8bc1\u660e\u673a\u5236\u540c\u65f6\u8fbe\u52301-\u4e00\u81f4\u6027\u548cO(n\u00b7W_max/W_min)-\u9c81\u68d2\u6027\u3002"}}
{"id": "2507.06533", "pdf": "https://arxiv.org/pdf/2507.06533", "abs": "https://arxiv.org/abs/2507.06533", "authors": ["Themistoklis Vargiemezis", "Catherine Gorl\u00e9"], "title": "From large-eddy simulations to deep learning: A U-net model for fast urban canopy flow predictions", "categories": ["physics.comp-ph", "cs.LG", "physics.flu-dyn"], "comment": null, "summary": "Accurate prediction of wind flow fields in urban canopies is crucial for\nensuring pedestrian comfort, safety, and sustainable urban design. Traditional\nmethods using wind tunnels and Computational Fluid Dynamics, such as Large-Eddy\nSimulations (LES), are limited by high costs, computational demands, and time\nrequirements. This study presents a deep neural network (DNN) approach for fast\nand accurate predictions of urban wind flow fields, reducing computation time\nfrom an order of 10 hours on 32 CPUs for one LES evaluation to an order of 1\nsecond on a single GPU using the DNN model. We employ a U-Net architecture\ntrained on LES data including 252 synthetic urban configurations at seven wind\ndirections ($0^{o}$ to $90^{o}$ in $15^{o}$ increments). The model predicts two\nkey quantities of interest: mean velocity magnitude and streamwise turbulence\nintensity, at multiple heights within the urban canopy. The U-net uses 2D\nbuilding representations augmented with signed distance functions and their\ngradients as inputs, forming a $256\\times256\\times9$ tensor. In addition, a\nSpatial Attention Module is used for feature transfer through skip connections.\nThe loss function combines the root-mean-square error of predictions, their\ngradient magnitudes, and L2 regularization. Model evaluation on 50 test cases\ndemonstrates high accuracy with an overall mean relative error of 9.3% for\nvelocity magnitude and 5.2% for turbulence intensity. This research shows the\npotential of deep learning approaches to provide fast, accurate urban wind\nassessments essential for creating comfortable and safe urban environments.\nCode is available at https://github.com/tvarg/Urban-FlowUnet.git", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u7684\u5feb\u901f\u9884\u6d4b\u57ce\u5e02\u98ce\u573a\u7684\u65b9\u6cd5\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u8ba1\u7b97\u65f6\u95f4\u548c\u6210\u672c\u3002", "motivation": "\u4f20\u7edf\u65b9\u6cd5\uff08\u5982\u98ce\u6d1e\u548c\u5927\u6da1\u6a21\u62df\uff09\u6210\u672c\u9ad8\u4e14\u8017\u65f6\uff0c\u9700\u8981\u66f4\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u6765\u9884\u6d4b\u57ce\u5e02\u98ce\u573a\u3002", "method": "\u91c7\u7528U-Net\u67b6\u6784\uff0c\u8f93\u5165\u4e3a\u4e8c\u7ef4\u5efa\u7b51\u8868\u793a\u548c\u8ddd\u79bb\u51fd\u6570\uff0c\u7ed3\u5408\u7a7a\u95f4\u6ce8\u610f\u529b\u6a21\u5757\uff0c\u8bad\u7ec3\u6570\u636e\u6765\u81ea252\u79cd\u5408\u6210\u57ce\u5e02\u914d\u7f6e\u3002", "result": "\u6a21\u578b\u5728\u6d4b\u8bd5\u96c6\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u5e73\u5747\u76f8\u5bf9\u8bef\u5dee\u4e3a9.3%\uff08\u901f\u5ea6\uff09\u548c5.2%\uff08\u6e4d\u6d41\u5f3a\u5ea6\uff09\uff0c\u8ba1\u7b97\u65f6\u95f4\u4ece10\u5c0f\u65f6\u7f29\u77ed\u81f31\u79d2\u3002", "conclusion": "\u6df1\u5ea6\u5b66\u4e60\u4e3a\u57ce\u5e02\u98ce\u573a\u8bc4\u4f30\u63d0\u4f9b\u4e86\u5feb\u901f\u3001\u51c6\u786e\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u6709\u52a9\u4e8e\u63d0\u5347\u57ce\u5e02\u8212\u9002\u6027\u548c\u5b89\u5168\u6027\u3002"}}
{"id": "2507.06764", "pdf": "https://arxiv.org/pdf/2507.06764", "abs": "https://arxiv.org/abs/2507.06764", "authors": ["Guixian Xu", "Jinglai Li", "Junqi Tang"], "title": "Fast Equivariant Imaging: Acceleration for Unsupervised Learning via Augmented Lagrangian and Auxiliary PnP Denoisers", "categories": ["eess.IV", "cs.CV", "cs.LG", "math.OC"], "comment": null, "summary": "We propose Fast Equivariant Imaging (FEI), a novel unsupervised learning\nframework to efficiently train deep imaging networks without ground-truth data.\nFrom the perspective of reformulating the Equivariant Imaging based\noptimization problem via the method of Lagrange multipliers and utilizing\nplug-and-play denoisers, this novel unsupervised scheme shows superior\nefficiency and performance compared to vanilla Equivariant Imaging paradigm. In\nparticular, our PnP-FEI scheme achieves an order-of-magnitude (10x)\nacceleration over standard EI on training U-Net with CT100 dataset for X-ray CT\nreconstruction, with improved generalization performance.", "AI": {"tldr": "Fast Equivariant Imaging (FEI) \u662f\u4e00\u79cd\u65e0\u76d1\u7763\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u62c9\u683c\u6717\u65e5\u4e58\u6570\u6cd5\u548c\u5373\u63d2\u5373\u7528\u53bb\u566a\u5668\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8bad\u7ec3\u6548\u7387\u548c\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u4f20\u7edf Equivariant Imaging \u8bad\u7ec3\u6548\u7387\u4f4e\u7684\u95ee\u9898\uff0c\u65e0\u9700\u771f\u5b9e\u6570\u636e\u5373\u53ef\u8bad\u7ec3\u6df1\u5ea6\u6210\u50cf\u7f51\u7edc\u3002", "method": "\u91c7\u7528\u62c9\u683c\u6717\u65e5\u4e58\u6570\u6cd5\u91cd\u65b0\u4f18\u5316\u95ee\u9898\uff0c\u7ed3\u5408\u5373\u63d2\u5373\u7528\u53bb\u566a\u5668\uff0c\u63d0\u51fa PnP-FEI \u65b9\u6848\u3002", "result": "\u5728 CT100 \u6570\u636e\u96c6\u4e0a\uff0cPnP-FEI \u6bd4\u6807\u51c6 EI \u5feb 10 \u500d\uff0c\u4e14\u6cdb\u5316\u6027\u80fd\u66f4\u4f18\u3002", "conclusion": "FEI \u6846\u67b6\u5728\u6548\u7387\u548c\u6027\u80fd\u4e0a\u5747\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\uff0c\u9002\u7528\u4e8e\u65e0\u76d1\u7763\u6df1\u5ea6\u6210\u50cf\u8bad\u7ec3\u3002"}}
{"id": "2507.06867", "pdf": "https://arxiv.org/pdf/2507.06867", "abs": "https://arxiv.org/abs/2507.06867", "authors": ["Tiffany Ding", "Jean-Baptiste Fermanian", "Joseph Salmon"], "title": "Conformal Prediction for Long-Tailed Classification", "categories": ["stat.ML", "cs.CV", "cs.LG", "stat.ME"], "comment": null, "summary": "Many real-world classification problems, such as plant identification, have\nextremely long-tailed class distributions. In order for prediction sets to be\nuseful in such settings, they should (i) provide good class-conditional\ncoverage, ensuring that rare classes are not systematically omitted from the\nprediction sets, and (ii) be a reasonable size, allowing users to easily verify\ncandidate labels. Unfortunately, existing conformal prediction methods, when\napplied to the long-tailed setting, force practitioners to make a binary choice\nbetween small sets with poor class-conditional coverage or sets with very good\nclass-conditional coverage but that are extremely large. We propose methods\nwith guaranteed marginal coverage that smoothly trade off between set size and\nclass-conditional coverage. First, we propose a conformal score function,\nprevalence-adjusted softmax, that targets a relaxed notion of class-conditional\ncoverage called macro-coverage. Second, we propose a label-weighted conformal\nprediction method that allows us to interpolate between marginal and\nclass-conditional conformal prediction. We demonstrate our methods on Pl@ntNet\nand iNaturalist, two long-tailed image datasets with 1,081 and 8,142 classes,\nrespectively.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e24\u79cd\u65b9\u6cd5\uff0c\u7528\u4e8e\u89e3\u51b3\u957f\u5c3e\u5206\u5e03\u5206\u7c7b\u95ee\u9898\u4e2d\u7684\u9884\u6d4b\u96c6\u8986\u76d6\u7387\u548c\u5927\u5c0f\u95ee\u9898\uff0c\u901a\u8fc7\u8f6f\u5316\u548c\u52a0\u6743\u7b56\u7565\u5b9e\u73b0\u5e73\u8861\u3002", "motivation": "\u73b0\u5b9e\u4e2d\u7684\u957f\u5c3e\u5206\u7c7b\u95ee\u9898\uff08\u5982\u690d\u7269\u8bc6\u522b\uff09\u9700\u8981\u9884\u6d4b\u96c6\u65e2\u4fdd\u8bc1\u7c7b\u522b\u8986\u76d6\uff08\u5c24\u5176\u662f\u7a00\u6709\u7c7b\u522b\uff09\uff0c\u53c8\u4fdd\u6301\u5408\u7406\u5927\u5c0f\uff0c\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u517c\u987e\u3002", "method": "\u63d0\u51fa\u4e24\u79cd\u65b9\u6cd5\uff1a1) \u57fa\u4e8e\u5b8f\u8986\u76d6\u7684\u8f6f\u6700\u5927\u5206\u6570\u51fd\u6570\uff1b2) \u6807\u7b7e\u52a0\u6743\u5171\u5f62\u9884\u6d4b\u65b9\u6cd5\uff0c\u5e73\u8861\u8fb9\u9645\u548c\u7c7b\u522b\u6761\u4ef6\u8986\u76d6\u3002", "result": "\u5728Pl@ntNet\u548ciNaturalist\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u5206\u522b\u6db5\u76d61,081\u548c8,142\u4e2a\u7c7b\u522b\u3002", "conclusion": "\u65b0\u65b9\u6cd5\u5728\u957f\u5c3e\u5206\u5e03\u5206\u7c7b\u4e2d\u5b9e\u73b0\u4e86\u9884\u6d4b\u96c6\u8986\u76d6\u7387\u548c\u5927\u5c0f\u7684\u7075\u6d3b\u6743\u8861\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002"}}
{"id": "2507.06911", "pdf": "https://arxiv.org/pdf/2507.06911", "abs": "https://arxiv.org/abs/2507.06911", "authors": ["Michele Polese", "Niloofar Mohamadi", "Salvatore D'Oro", "Tommaso Melodia"], "title": "Beyond Connectivity: An Open Architecture for AI-RAN Convergence in 6G", "categories": ["cs.NI", "cs.AI", "eess.SP"], "comment": "Submitted to IEEE for publication, copyright may change without\n  notice. 8 pages, 6 figures", "summary": "The proliferation of data-intensive Artificial Intelligence (AI) applications\nat the network edge demands a fundamental shift in RAN design, from merely\nconsuming AI for network optimization, to actively enabling distributed AI\nworkloads. This paradigm shift presents a significant opportunity for network\noperators to monetize AI at the edge while leveraging existing infrastructure\ninvestments. To realize this vision, this article presents a novel converged\nO-RAN and AI-RAN architecture that unifies orchestration and management of both\ntelecommunications and AI workloads on shared infrastructure. The proposed\narchitecture extends the Open RAN principles of modularity, disaggregation, and\ncloud-nativeness to support heterogeneous AI deployments. We introduce two key\narchitectural innovations: (i) the AI-RAN Orchestrator, which extends the O-RAN\nService Management and Orchestration (SMO) to enable integrated resource and\nallocation across RAN and AI workloads; and (ii) AI-RAN sites that provide\ndistributed edge AI platforms with real-time processing capabilities. The\nproposed system supports flexible deployment options, allowing AI workloads to\nbe orchestrated with specific timing requirements (real-time or batch\nprocessing) and geographic targeting. The proposed architecture addresses the\norchestration requirements for managing heterogeneous workloads at different\ntime scales while maintaining open, standardized interfaces and multi-vendor\ninteroperability.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u878d\u5408O-RAN\u548cAI-RAN\u7684\u65b0\u67b6\u6784\uff0c\u652f\u6301\u5728\u5171\u4eab\u57fa\u7840\u8bbe\u65bd\u4e0a\u7edf\u4e00\u7f16\u6392\u548c\u7ba1\u7406\u7535\u4fe1\u4e0eAI\u5de5\u4f5c\u8d1f\u8f7d\uff0c\u6269\u5c55\u4e86Open RAN\u7684\u6a21\u5757\u5316\u548c\u4e91\u539f\u751f\u7279\u6027\u3002", "motivation": "\u968f\u7740\u6570\u636e\u5bc6\u96c6\u578bAI\u5e94\u7528\u5728\u7f51\u7edc\u8fb9\u7f18\u7684\u666e\u53ca\uff0c\u9700\u8981\u4ece\u5355\u7eaf\u5229\u7528AI\u4f18\u5316\u7f51\u7edc\u8f6c\u5411\u652f\u6301\u5206\u5e03\u5f0fAI\u5de5\u4f5c\u8d1f\u8f7d\uff0c\u4e3a\u8fd0\u8425\u5546\u63d0\u4f9b\u65b0\u7684\u76c8\u5229\u673a\u4f1a\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u878d\u5408O-RAN\u548cAI-RAN\u7684\u67b6\u6784\uff0c\u5305\u62ecAI-RAN\u7f16\u6392\u5668\u548cAI-RAN\u7ad9\u70b9\uff0c\u652f\u6301\u5f02\u6784AI\u90e8\u7f72\u548c\u5b9e\u65f6\u5904\u7406\u80fd\u529b\u3002", "result": "\u8be5\u67b6\u6784\u5b9e\u73b0\u4e86\u5bf9\u5f02\u6784\u5de5\u4f5c\u8d1f\u8f7d\u7684\u7edf\u4e00\u7f16\u6392\uff0c\u652f\u6301\u5b9e\u65f6\u6216\u6279\u5904\u7406\uff0c\u5e76\u4fdd\u6301\u5f00\u653e\u7684\u6807\u51c6\u5316\u63a5\u53e3\u548c\u591a\u5382\u5546\u4e92\u64cd\u4f5c\u6027\u3002", "conclusion": "\u8be5\u67b6\u6784\u4e3a\u7f51\u7edc\u8fd0\u8425\u5546\u63d0\u4f9b\u4e86\u5728\u8fb9\u7f18\u7f51\u7edc\u4e2d\u9ad8\u6548\u90e8\u7f72\u548c\u7ba1\u7406AI\u5de5\u4f5c\u8d1f\u8f7d\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u540c\u65f6\u5145\u5206\u5229\u7528\u73b0\u6709\u57fa\u7840\u8bbe\u65bd\u3002"}}
{"id": "2507.06552", "pdf": "https://arxiv.org/pdf/2507.06552", "abs": "https://arxiv.org/abs/2507.06552", "authors": ["Zhiyi Dong", "Zixuan Liu", "Yongyi Mao"], "title": "On the Hardness of Unsupervised Domain Adaptation: Optimal Learners and Information-Theoretic Perspective", "categories": ["stat.ML", "cs.IT", "cs.LG", "math.IT"], "comment": "Accepted at the 4th Conference on Lifelong Learning Agents (CoLLAs\n  2025)", "summary": "This paper studies the hardness of unsupervised domain adaptation (UDA) under\ncovariate shift. We model the uncertainty that the learner faces by a\ndistribution $\\pi$ in the ground-truth triples $(p, q, f)$ -- which we call a\nUDA class -- where $(p, q)$ is the source -- target distribution pair and $f$\nis the classifier. We define the performance of a learner as the overall target\ndomain risk, averaged over the randomness of the ground-truth triple. This\nformulation couples the source distribution, the target distribution and the\nclassifier in the ground truth, and deviates from the classical worst-case\nanalyses, which pessimistically emphasize the impact of hard but rare UDA\ninstances. In this formulation, we precisely characterize the optimal learner.\nThe performance of the optimal learner then allows us to define the learning\ndifficulty for the UDA class and for the observed sample. To quantify this\ndifficulty, we introduce an information-theoretic quantity -- Posterior Target\nLabel Uncertainty (PTLU) -- along with its empirical estimate (EPTLU) from the\nsample , which capture the uncertainty in the prediction for the target domain.\nBriefly, PTLU is the entropy of the predicted label in the target domain under\nthe posterior distribution of ground-truth classifier given the observed source\nand target samples. By proving that such a quantity serves to lower-bound the\nrisk of any learner, we suggest that these quantities can be used as proxies\nfor evaluating the hardness of UDA learning. We provide several examples to\ndemonstrate the advantage of PTLU, relative to the existing measures, in\nevaluating the difficulty of UDA learning.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u534f\u53d8\u91cf\u504f\u79fb\u4e0b\u65e0\u76d1\u7763\u57df\u81ea\u9002\u5e94\uff08UDA\uff09\u7684\u56f0\u96be\u6027\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u540e\u9a8c\u76ee\u6807\u6807\u7b7e\u4e0d\u786e\u5b9a\u6027\uff08PTLU\uff09\u7684\u65b0\u65b9\u6cd5\u6765\u91cf\u5316\u5b66\u4e60\u96be\u5ea6\u3002", "motivation": "\u4f20\u7edf\u7684\u6700\u574f\u60c5\u51b5\u5206\u6790\u8fc7\u4e8e\u60b2\u89c2\uff0c\u5ffd\u7565\u4e86\u771f\u5b9e\u573a\u666f\u4e2dUDA\u5b9e\u4f8b\u7684\u5206\u5e03\u7279\u6027\u3002\u672c\u6587\u65e8\u5728\u901a\u8fc7\u5efa\u6a21\u771f\u5b9e\u4e09\u5143\u7ec4\uff08\u6e90-\u76ee\u6807\u5206\u5e03\u5bf9\u548c\u5206\u7c7b\u5668\uff09\u7684\u4e0d\u786e\u5b9a\u6027\uff0c\u66f4\u51c6\u786e\u5730\u8bc4\u4f30UDA\u7684\u5b66\u4e60\u96be\u5ea6\u3002", "method": "\u5b9a\u4e49\u4e86\u4e00\u4e2aUDA\u7c7b\uff0c\u5305\u542b\u771f\u5b9e\u4e09\u5143\u7ec4\uff08p, q, f\uff09\uff0c\u5e76\u901a\u8fc7\u5e73\u5747\u76ee\u6807\u57df\u98ce\u9669\u6765\u8861\u91cf\u5b66\u4e60\u6027\u80fd\u3002\u63d0\u51fa\u4e86PTLU\u53ca\u5176\u6837\u672c\u4f30\u8ba1EPTLU\uff0c\u4f5c\u4e3a\u91cf\u5316\u5b66\u4e60\u96be\u5ea6\u7684\u4fe1\u606f\u7406\u8bba\u6307\u6807\u3002", "result": "PTLU\u80fd\u591f\u6709\u6548\u4e0b\u754c\u4efb\u4f55\u5b66\u4e60\u5668\u7684\u98ce\u9669\uff0c\u5e76\u901a\u8fc7\u5b9e\u4f8b\u5c55\u793a\u4e86\u5176\u5728\u8bc4\u4f30UDA\u5b66\u4e60\u96be\u5ea6\u4e0a\u7684\u4f18\u52bf\u3002", "conclusion": "PTLU\u662f\u4e00\u79cd\u66f4\u51c6\u786e\u7684UDA\u5b66\u4e60\u96be\u5ea6\u8bc4\u4f30\u6307\u6807\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002"}}
{"id": "2507.06955", "pdf": "https://arxiv.org/pdf/2507.06955", "abs": "https://arxiv.org/abs/2507.06955", "authors": ["Kaveh Moradkhani", "R Jarrett Rushmore", "Sylvain Bouix"], "title": "SimCortex: Collision-free Simultaneous Cortical Surfaces Reconstruction", "categories": ["eess.IV", "cs.CV"], "comment": null, "summary": "Accurate cortical surface reconstruction from magnetic resonance imaging\n(MRI) data is crucial for reliable neuroanatomical analyses. Current methods\nhave to contend with complex cortical geometries, strict topological\nrequirements, and often produce surfaces with overlaps, self-intersections, and\ntopological defects. To overcome these shortcomings, we introduce SimCortex, a\ndeep learning framework that simultaneously reconstructs all brain surfaces\n(left/right white-matter and pial) from T1-weighted(T1w) MRI volumes while\npreserving topological properties. Our method first segments the T1w image into\na nine-class tissue label map. From these segmentations, we generate\nsubject-specific, collision-free initial surface meshes. These surfaces serve\nas precise initializations for subsequent multiscale diffeomorphic\ndeformations. Employing stationary velocity fields (SVFs) integrated via\nscaling-and-squaring, our approach ensures smooth, topology-preserving\ntransformations with significantly reduced surface collisions and\nself-intersections. Evaluations on standard datasets demonstrate that SimCortex\ndramatically reduces surface overlaps and self-intersections, surpassing\ncurrent methods while maintaining state-of-the-art geometric accuracy.", "AI": {"tldr": "SimCortex\u662f\u4e00\u79cd\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\uff0c\u7528\u4e8e\u4eceT1\u52a0\u6743MRI\u6570\u636e\u4e2d\u91cd\u5efa\u5927\u8111\u76ae\u5c42\u8868\u9762\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u4e2d\u7684\u62d3\u6251\u7f3a\u9677\u548c\u8868\u9762\u91cd\u53e0\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u91cd\u5efa\u5927\u8111\u76ae\u5c42\u8868\u9762\u65f6\u5b58\u5728\u62d3\u6251\u7f3a\u9677\u548c\u8868\u9762\u91cd\u53e0\u95ee\u9898\uff0c\u5f71\u54cd\u795e\u7ecf\u89e3\u5256\u5206\u6790\u7684\u53ef\u9760\u6027\u3002", "method": "SimCortex\u9996\u5148\u5c06T1\u52a0\u6743\u56fe\u50cf\u5206\u5272\u4e3a\u4e5d\u7c7b\u7ec4\u7ec7\u6807\u7b7e\u56fe\uff0c\u751f\u6210\u65e0\u78b0\u649e\u7684\u521d\u59cb\u8868\u9762\u7f51\u683c\uff0c\u7136\u540e\u901a\u8fc7\u591a\u5c3a\u5ea6\u5fae\u5206\u540c\u80da\u53d8\u5f62\u548c\u9759\u6b62\u901f\u5ea6\u573a\uff08SVFs\uff09\u5b9e\u73b0\u5e73\u6ed1\u3001\u62d3\u6251\u4fdd\u6301\u7684\u53d8\u6362\u3002", "result": "SimCortex\u663e\u8457\u51cf\u5c11\u4e86\u8868\u9762\u91cd\u53e0\u548c\u81ea\u76f8\u4ea4\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u6700\u5148\u8fdb\u7684\u51e0\u4f55\u7cbe\u5ea6\u3002", "conclusion": "SimCortex\u5728\u76ae\u5c42\u8868\u9762\u91cd\u5efa\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u4e3a\u795e\u7ecf\u89e3\u5256\u5206\u6790\u63d0\u4f9b\u4e86\u66f4\u53ef\u9760\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.07000", "pdf": "https://arxiv.org/pdf/2507.07000", "abs": "https://arxiv.org/abs/2507.07000", "authors": ["Wijayathunga W. M. R. D. B"], "title": "Enhancing non-Rigid 3D Model Deformations Using Mesh-based Gaussian Splatting", "categories": ["cs.GR", "cs.CV"], "comment": null, "summary": "We propose a novel framework that enhances non-rigid 3D model deformations by\nbridging mesh representations with 3D Gaussian splatting. While traditional\nGaussian splatting delivers fast, real-time radiance-field rendering, its\npost-editing capabilities and support for large-scale, non-rigid deformations\nremain limited. Our method addresses these challenges by embedding Gaussian\nkernels directly onto explicit mesh surfaces. This allows the mesh's inherent\ntopological and geometric priors to guide intuitive editing operations -- such\nas moving, scaling, and rotating individual 3D components -- and enables\ncomplex deformations like bending and stretching. This work paves the way for\nmore flexible 3D content-creation workflows in applications spanning virtual\nreality, character animation, and interactive design.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u6846\u67b6\uff0c\u901a\u8fc7\u5c06\u7f51\u683c\u8868\u793a\u4e0e3D\u9ad8\u65af\u6cfc\u6e85\u7ed3\u5408\uff0c\u589e\u5f3a\u975e\u521a\u60273D\u6a21\u578b\u53d8\u5f62\u80fd\u529b\u3002", "motivation": "\u4f20\u7edf\u9ad8\u65af\u6cfc\u6e85\u867d\u80fd\u5feb\u901f\u5b9e\u65f6\u6e32\u67d3\u8f90\u5c04\u573a\uff0c\u4f46\u5176\u540e\u7f16\u8f91\u80fd\u529b\u53ca\u5bf9\u5927\u89c4\u6a21\u975e\u521a\u6027\u53d8\u5f62\u7684\u652f\u6301\u6709\u9650\u3002", "method": "\u5c06\u9ad8\u65af\u6838\u76f4\u63a5\u5d4c\u5165\u663e\u5f0f\u7f51\u683c\u8868\u9762\uff0c\u5229\u7528\u7f51\u683c\u7684\u62d3\u6251\u548c\u51e0\u4f55\u5148\u9a8c\u6307\u5bfc\u7f16\u8f91\u64cd\u4f5c\uff08\u5982\u79fb\u52a8\u3001\u7f29\u653e\u3001\u65cb\u8f6c\uff09\uff0c\u5e76\u652f\u6301\u590d\u6742\u53d8\u5f62\uff08\u5982\u5f2f\u66f2\u548c\u62c9\u4f38\uff09\u3002", "result": "\u5b9e\u73b0\u4e86\u66f4\u7075\u6d3b\u76843D\u5185\u5bb9\u521b\u4f5c\u6d41\u7a0b\uff0c\u9002\u7528\u4e8e\u865a\u62df\u73b0\u5b9e\u3001\u89d2\u8272\u52a8\u753b\u548c\u4ea4\u4e92\u8bbe\u8ba1\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u975e\u521a\u60273D\u53d8\u5f62\u63d0\u4f9b\u4e86\u66f4\u76f4\u89c2\u548c\u5f3a\u5927\u7684\u7f16\u8f91\u5de5\u5177\u3002"}}
{"id": "2507.06608", "pdf": "https://arxiv.org/pdf/2507.06608", "abs": "https://arxiv.org/abs/2507.06608", "authors": ["Xiaoxiang Shi", "Colin Cai", "Junjia Du", "Zhanda Zhu", "Xingda Wei", "Zhihao Jia"], "title": "Nexus: Taming Throughput-Latency Tradeoff in LLM Serving via Efficient GPU Sharing", "categories": ["cs.DC", "cs.LG"], "comment": null, "summary": "Current prefill-decode (PD) disaggregation is typically deployed at the level\nof entire serving engines, assigning separate GPUs to handle prefill and decode\nphases. While effective at reducing latency, this approach demands more\nhardware. To improve GPU utilization, Chunked Prefill mixes prefill and decode\nrequests within the same batch, but introduces phase interference between\nprefill and decode.\n  While existing PD disaggregation solutions separate the phases across GPUs,\nwe ask: can the same decoupling be achieved within a single serving engine? The\nkey challenge lies in managing the conflicting resource requirements of prefill\nand decode when they share the same hardware. In this paper, we first show that\nchunked prefill requests cause interference with decode requests due to their\ndistinct requirements for GPU resources. Second, we find that GPU resources\nexhibit diminishing returns. Beyond a saturation point, increasing GPU\nallocation yields negligible latency improvements. This insight enables us to\nsplit a single GPU's resources and dynamically allocate them to prefill and\ndecode on the fly, effectively disaggregating the two phases within the same\nGPU.\n  Across a range of models and workloads, our system Nexus achieves up to 2.2x\nhigher throughput, 20x lower TTFT, and 2.5x lower TBT than vLLM. It also\noutperforms SGLang with up to 2x higher throughput, 2x lower TTFT, and 1.7x\nlower TBT, and achieves 1.4x higher throughput than vLLM-disaggregation using\nonly half the number of GPUs.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5728\u5355\u4e2aGPU\u5185\u52a8\u6001\u5206\u914d\u8d44\u6e90\u4ee5\u89e3\u8026\u9884\u586b\u5145\u548c\u89e3\u7801\u9636\u6bb5\u7684\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u541e\u5410\u91cf\u5e76\u964d\u4f4e\u4e86\u5ef6\u8fdf\u3002", "motivation": "\u73b0\u6709\u7684\u9884\u586b\u5145-\u89e3\u7801\uff08PD\uff09\u89e3\u8026\u65b9\u6cd5\u901a\u5e38\u9700\u8981\u591a\u4e2aGPU\uff0c\u786c\u4ef6\u9700\u6c42\u9ad8\u3002Chunked Prefill\u867d\u80fd\u63d0\u9ad8GPU\u5229\u7528\u7387\uff0c\u4f46\u4f1a\u5f15\u5165\u9636\u6bb5\u5e72\u6270\u3002\u672c\u6587\u63a2\u7d22\u5728\u5355\u4e2aGPU\u5185\u5b9e\u73b0\u89e3\u8026\uff0c\u4ee5\u89e3\u51b3\u8d44\u6e90\u51b2\u7a81\u95ee\u9898\u3002", "method": "\u901a\u8fc7\u5206\u6790GPU\u8d44\u6e90\u7684\u8fb9\u9645\u6548\u76ca\uff0c\u53d1\u73b0\u8d44\u6e90\u5206\u914d\u5b58\u5728\u9971\u548c\u70b9\u3002\u57fa\u4e8e\u6b64\uff0c\u52a8\u6001\u5206\u914d\u5355\u4e2aGPU\u7684\u8d44\u6e90\u7ed9\u9884\u586b\u5145\u548c\u89e3\u7801\u9636\u6bb5\uff0c\u5b9e\u73b0\u89e3\u8026\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cNexus\u7cfb\u7edf\u5728\u591a\u79cd\u6a21\u578b\u548c\u5de5\u4f5c\u8d1f\u8f7d\u4e0b\uff0c\u541e\u5410\u91cf\u6700\u9ad8\u63d0\u53472.2\u500d\uff0cTTFT\u964d\u4f4e20\u500d\uff0cTBT\u964d\u4f4e2.5\u500d\uff0c\u4e14\u4ec5\u7528\u4e00\u534aGPU\u6570\u91cf\u5373\u53ef\u8d85\u8d8avLLM-disaggregation\u3002", "conclusion": "\u5728\u5355\u4e2aGPU\u5185\u52a8\u6001\u5206\u914d\u8d44\u6e90\u53ef\u4ee5\u6709\u6548\u89e3\u8026\u9884\u586b\u5145\u548c\u89e3\u7801\u9636\u6bb5\uff0c\u663e\u8457\u63d0\u5347\u6027\u80fd\u5e76\u51cf\u5c11\u786c\u4ef6\u9700\u6c42\u3002"}}
{"id": "2507.07011", "pdf": "https://arxiv.org/pdf/2507.07011", "abs": "https://arxiv.org/abs/2507.07011", "authors": ["Daniel Onah", "Ravish Desai"], "title": "Deep Brain Net: An Optimized Deep Learning Model for Brain tumor Detection in MRI Images Using EfficientNetB0 and ResNet50 with Transfer Learning", "categories": ["eess.IV", "cs.CV"], "comment": "9 pages, 14 figures, 4 tables. To be submitted to a conference", "summary": "In recent years, deep learning has shown great promise in the automated\ndetection and classification of brain tumors from MRI images. However,\nachieving high accuracy and computational efficiency remains a challenge. In\nthis research, we propose Deep Brain Net, a novel deep learning system designed\nto optimize performance in the detection of brain tumors. The model integrates\nthe strengths of two advanced neural network architectures which are\nEfficientNetB0 and ResNet50, combined with transfer learning to improve\ngeneralization and reduce training time. The EfficientNetB0 architecture\nenhances model efficiency by utilizing mobile inverted bottleneck blocks, which\nincorporate depth wise separable convolutions. This design significantly\nreduces the number of parameters and computational cost while preserving the\nability of models to learn complex feature representations. The ResNet50\narchitecture, pre trained on large scale datasets like ImageNet, is fine tuned\nfor brain tumor classification. Its use of residual connections allows for\ntraining deeper networks by mitigating the vanishing gradient problem and\navoiding performance degradation. The integration of these components ensures\nthat the proposed system is both computationally efficient and highly accurate.\nExtensive experiments performed on publicly available MRI datasets demonstrate\nthat Deep Brain Net consistently outperforms existing state of the art methods\nin terms of classification accuracy, precision, recall, and computational\nefficiency. The result is an accuracy of 88 percent, a weighted F1 score of\n88.75 percent, and a macro AUC ROC score of 98.17 percent which demonstrates\nthe robustness and clinical potential of Deep Brain Net in assisting\nradiologists with brain tumor diagnosis.", "AI": {"tldr": "Deep Brain Net\u7ed3\u5408EfficientNetB0\u548cResNet50\uff0c\u901a\u8fc7\u8fc1\u79fb\u5b66\u4e60\u63d0\u5347\u8111\u80bf\u7624\u68c0\u6d4b\u7684\u51c6\u786e\u6027\u548c\u8ba1\u7b97\u6548\u7387\u3002", "motivation": "\u6df1\u5ea6\u5b66\u4e60\u5728\u8111\u80bf\u7624\u68c0\u6d4b\u4e2d\u6f5c\u529b\u5de8\u5927\uff0c\u4f46\u9ad8\u51c6\u786e\u6027\u548c\u8ba1\u7b97\u6548\u7387\u4ecd\u662f\u6311\u6218\u3002", "method": "\u7ed3\u5408EfficientNetB0\uff08\u9ad8\u6548\u53c2\u6570\u8bbe\u8ba1\uff09\u548cResNet50\uff08\u6b8b\u5dee\u8fde\u63a5\uff09\uff0c\u5229\u7528\u8fc1\u79fb\u5b66\u4e60\u4f18\u5316\u6027\u80fd\u3002", "result": "\u5728\u516c\u5f00MRI\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u51c6\u786e\u738788%\uff0cF1\u5206\u657088.75%\uff0cAUC ROC 98.17%\u3002", "conclusion": "Deep Brain Net\u5728\u8111\u80bf\u7624\u8bca\u65ad\u4e2d\u5177\u6709\u4e34\u5e8a\u6f5c\u529b\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002"}}
{"id": "2507.06637", "pdf": "https://arxiv.org/pdf/2507.06637", "abs": "https://arxiv.org/abs/2507.06637", "authors": ["Pengcheng Zeng", "Siyuan Jiang"], "title": "Semi-parametric Functional Classification via Path Signatures Logistic Regression", "categories": ["stat.ML", "cs.LG"], "comment": null, "summary": "We propose Path Signatures Logistic Regression (PSLR), a semi-parametric\nframework for classifying vector-valued functional data with scalar covariates.\nClassical functional logistic regression models rely on linear assumptions and\nfixed basis expansions, which limit flexibility and degrade performance under\nirregular sampling. PSLR overcomes these issues by leveraging truncated path\nsignatures to construct a finite-dimensional, basis-free representation that\ncaptures nonlinear and cross-channel dependencies. By embedding trajectories as\ntime-augmented paths, PSLR extracts stable, geometry-aware features that are\nrobust to sampling irregularity without requiring a common time grid, while\nstill preserving subject-specific timing patterns. We establish theoretical\nguarantees for the existence and consistent estimation of the optimal\ntruncation order, along with non-asymptotic risk bounds. Experiments on\nsynthetic and real-world datasets show that PSLR outperforms traditional\nfunctional classifiers in accuracy, robustness, and interpretability,\nparticularly under non-uniform sampling schemes. Our results highlight the\npractical and theoretical benefits of integrating rough path theory into modern\nfunctional data analysis.", "AI": {"tldr": "PSLR\u662f\u4e00\u79cd\u534a\u53c2\u6570\u6846\u67b6\uff0c\u7528\u4e8e\u5206\u7c7b\u5e26\u6709\u6807\u91cf\u534f\u53d8\u91cf\u7684\u5411\u91cf\u503c\u51fd\u6570\u6570\u636e\uff0c\u901a\u8fc7\u622a\u65ad\u8def\u5f84\u7b7e\u540d\u514b\u670d\u4f20\u7edf\u65b9\u6cd5\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u4f20\u7edf\u51fd\u6570\u903b\u8f91\u56de\u5f52\u6a21\u578b\u4f9d\u8d56\u7ebf\u6027\u5047\u8bbe\u548c\u56fa\u5b9a\u57fa\u5c55\u5f00\uff0c\u7075\u6d3b\u6027\u5dee\u4e14\u5728\u4e0d\u89c4\u5219\u91c7\u6837\u4e0b\u6027\u80fd\u4e0b\u964d\u3002", "method": "\u5229\u7528\u622a\u65ad\u8def\u5f84\u7b7e\u540d\u6784\u5efa\u6709\u9650\u7ef4\u3001\u65e0\u57fa\u8868\u793a\uff0c\u6355\u6349\u975e\u7ebf\u6027\u548c\u8de8\u901a\u9053\u4f9d\u8d56\uff0c\u5d4c\u5165\u65f6\u95f4\u589e\u5f3a\u8def\u5f84\u63d0\u53d6\u7a33\u5b9a\u7279\u5f81\u3002", "result": "\u7406\u8bba\u548c\u5b9e\u9a8c\u8bc1\u660ePSLR\u5728\u51c6\u786e\u6027\u3001\u9c81\u68d2\u6027\u548c\u53ef\u89e3\u91ca\u6027\u4e0a\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\uff0c\u5c24\u5176\u5728\u4e0d\u89c4\u5219\u91c7\u6837\u4e0b\u8868\u73b0\u7a81\u51fa\u3002", "conclusion": "PSLR\u5c06\u7c97\u7cd9\u8def\u5f84\u7406\u8bba\u878d\u5165\u73b0\u4ee3\u51fd\u6570\u6570\u636e\u5206\u6790\uff0c\u5177\u6709\u7406\u8bba\u548c\u5b9e\u8df5\u4f18\u52bf\u3002"}}
{"id": "2507.06677", "pdf": "https://arxiv.org/pdf/2507.06677", "abs": "https://arxiv.org/abs/2507.06677", "authors": ["Chao Zhang", "Jasper M. Everink", "Jakob Sauer J\u00f8rgensen"], "title": "Fast Gaussian Processes under Monotonicity Constraints", "categories": ["stat.ML", "cs.LG", "stat.ME", "60G15"], "comment": "35 pages, 10 figures", "summary": "Gaussian processes (GPs) are widely used as surrogate models for complicated\nfunctions in scientific and engineering applications. In many cases, prior\nknowledge about the function to be approximated, such as monotonicity, is\navailable and can be leveraged to improve model fidelity. Incorporating such\nconstraints into GP models enhances predictive accuracy and reduces\nuncertainty, but remains a computationally challenging task for\nhigh-dimensional problems. In this work, we present a novel virtual point-based\nframework for building constrained GP models under monotonicity constraints,\nbased on regularized linear randomize-then-optimize (RLRTO), which enables\nefficient sampling from a constrained posterior distribution by means of\nsolving randomized optimization problems. We also enhance two existing virtual\npoint-based approaches by replacing Gibbs sampling with the No U-Turn Sampler\n(NUTS) for improved efficiency. A Python implementation of these methods is\nprovided and can be easily applied to a wide range of problems. This\nimplementation is then used to validate the approaches on approximating a range\nof synthetic functions, demonstrating comparable predictive performance between\nall considered methods and significant improvements in computational efficiency\nwith the two NUTS methods and especially with the RLRTO method. The framework\nis further applied to construct surrogate models for systems of differential\nequations.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u865a\u62df\u70b9\u7684\u9ad8\u65af\u8fc7\u7a0b\uff08GP\uff09\u5efa\u6a21\u6846\u67b6\uff0c\u7ed3\u5408\u5355\u8c03\u6027\u7ea6\u675f\uff0c\u901a\u8fc7RLRTO\u65b9\u6cd5\u9ad8\u6548\u91c7\u6837\uff0c\u5e76\u6539\u8fdb\u4e86\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u5229\u7528\u5148\u9a8c\u77e5\u8bc6\uff08\u5982\u5355\u8c03\u6027\uff09\u63d0\u5347GP\u6a21\u578b\u7684\u9884\u6d4b\u7cbe\u5ea6\u548c\u8ba1\u7b97\u6548\u7387\u3002", "method": "\u91c7\u7528RLRTO\u65b9\u6cd5\u8fdb\u884c\u9ad8\u6548\u91c7\u6837\uff0c\u5e76\u7528NUTS\u66ff\u4ee3Gibbs\u91c7\u6837\u6539\u8fdb\u73b0\u6709\u865a\u62df\u70b9\u65b9\u6cd5\u3002", "result": "\u5728\u5408\u6210\u51fd\u6570\u548c\u5fae\u5206\u65b9\u7a0b\u7cfb\u7edf\u4e2d\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u9884\u6d4b\u6027\u80fd\u548c\u8ba1\u7b97\u6548\u7387\u3002", "conclusion": "\u65b0\u6846\u67b6\u5728\u4fdd\u6301\u9884\u6d4b\u6027\u80fd\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u4e86\u8ba1\u7b97\u6548\u7387\uff0c\u9002\u7528\u4e8e\u9ad8\u7ef4\u95ee\u9898\u3002"}}
{"id": "2507.07034", "pdf": "https://arxiv.org/pdf/2507.07034", "abs": "https://arxiv.org/abs/2507.07034", "authors": ["Mikael Vaillant", "Victor Oliveira Ferreira", "Wiebke Mainville", "Jean-Michel Lamarre", "Vincent Raymond", "Moncef Chioua", "Bruno Blais"], "title": "Surrogate Model for Heat Transfer Prediction in Impinging Jet Arrays using Dynamic Inlet/Outlet and Flow Rate Control", "categories": ["physics.flu-dyn", "cs.AI"], "comment": "37 pages, 13 figures", "summary": "This study presents a surrogate model designed to predict the Nusselt number\ndistribution in an enclosed impinging jet arrays, where each jet function\nindependently and where jets can be transformed from inlets to outlets, leading\nto a vast number of possible flow arrangements. While computational fluid\ndynamics (CFD) simulations can model heat transfer with high fidelity, their\ncost prohibits real-time application such as model-based temperature control.\nTo address this, we generate a CNN-based surrogate model that can predict the\nNusselt distribution in real time. We train it with data from implicit large\neddy computational fluid dynamics simulations (Re < 2,000). We train two\ndistinct models, one for a five by one array of jets (83 simulations) and one\nfor a three by three array of jets (100 simulations). We introduce a method to\nextrapolate predictions to higher Reynolds numbers (Re < 10,000) using a\ncorrelation-based scaling. The surrogate models achieve high accuracy, with a\nnormalized mean average error below 2% on validation data for the five by one\nsurrogate model and 0.6% for the three by three surrogate model. Experimental\nvalidation confirms the model's predictive capabilities. This work provides a\nfoundation for model-based control strategies in advanced thermal management\napplications.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eCNN\u7684\u66ff\u4ee3\u6a21\u578b\uff0c\u7528\u4e8e\u5b9e\u65f6\u9884\u6d4b\u5c01\u95ed\u5f0f\u51b2\u51fb\u5c04\u6d41\u9635\u5217\u4e2d\u7684\u52aa\u585e\u5c14\u6570\u5206\u5e03\uff0c\u89e3\u51b3\u4e86CFD\u6a21\u62df\u6210\u672c\u9ad8\u7684\u95ee\u9898\u3002", "motivation": "\u89e3\u51b3CFD\u6a21\u62df\u5728\u5b9e\u65f6\u5e94\u7528\uff08\u5982\u57fa\u4e8e\u6a21\u578b\u7684\u6e29\u5ea6\u63a7\u5236\uff09\u4e2d\u6210\u672c\u8fc7\u9ad8\u7684\u95ee\u9898\u3002", "method": "\u5229\u7528\u9690\u5f0f\u5927\u6da1\u6a21\u62dfCFD\u6570\u636e\u8bad\u7ec3CNN\u6a21\u578b\uff0c\u5e76\u5f15\u5165\u57fa\u4e8e\u76f8\u5173\u6027\u7684\u7f29\u653e\u65b9\u6cd5\u6269\u5c55\u5230\u66f4\u9ad8\u96f7\u8bfa\u6570\u3002", "result": "\u66ff\u4ee3\u6a21\u578b\u5728\u9a8c\u8bc1\u6570\u636e\u4e0a\u8868\u73b0\u51fa\u9ad8\u7cbe\u5ea6\uff0c\u4e94\u4e58\u4e00\u9635\u5217\u7684\u8bef\u5dee\u4f4e\u4e8e2%\uff0c\u4e09\u4e58\u4e09\u9635\u5217\u7684\u8bef\u5dee\u4f4e\u4e8e0.6%\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u4e3a\u5148\u8fdb\u70ed\u7ba1\u7406\u5e94\u7528\u4e2d\u7684\u57fa\u4e8e\u6a21\u578b\u63a7\u5236\u7b56\u7565\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2507.07036", "pdf": "https://arxiv.org/pdf/2507.07036", "abs": "https://arxiv.org/abs/2507.07036", "authors": ["Maloy Kumar Devnath", "Sudip Chakraborty", "Vandana P. Janeja"], "title": "Modeling Heterogeneity across Varying Spatial Extents: Discovering Linkages between Sea Ice Retreat and Ice Shelve Melt in the Antarctic", "categories": ["cs.SI", "cs.AI"], "comment": null, "summary": "Spatial phenomena often exhibit heterogeneity across spatial extents and in\nproximity, making them complex to model-especially in dynamic regions like ice\nshelves and sea ice. In this study, we address this challenge by exploring the\nlinkages between sea ice retreat and Antarctic ice shelf (AIS) melt. Although\natmospheric forcing and basal melting have been widely studied, the direct\nimpact of sea ice retreat on AIS mass loss remains underexplored. Traditional\nmodels treat sea ice and AIS as separate systems. It limits their ability to\ncapture localized linkages and cascading feedback. To overcome this, we propose\nSpatial-Link, a novel graph-based framework that quantifies spatial\nheterogeneity to capture linkages between sea ice retreat and AIS melt. Our\nmethod constructs a spatial graph using Delaunay triangulation of\nsatellite-derived ice change matrices, where nodes represent regions of\nsignificant change and edges encode proximity and directional consistency. We\nextract and statistically validate linkage paths using breadth-first search and\nMonte Carlo simulations. Results reveal non-local, spatially heterogeneous\ncoupling patterns, suggesting sea ice loss can initiate or amplify downstream\nAIS melt. Our analysis shows how sea ice retreat evolves over an oceanic grid\nand progresses toward ice shelves-establishing a direct linkage. To our\nknowledge, this is the first proposed methodology linking sea ice retreat to\nAIS melt. Spatial-Link offers a scalable, data-driven tool to improve sea-level\nrise projections and inform climate adaptation strategies.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aSpatial-Link\u7684\u56fe\u6846\u67b6\uff0c\u7528\u4e8e\u91cf\u5316\u6d77\u51b0\u9000\u7f29\u4e0e\u5357\u6781\u51b0\u67b6\u878d\u5316\u4e4b\u95f4\u7684\u7a7a\u95f4\u5f02\u8d28\u6027\u8054\u7cfb\u3002", "motivation": "\u4f20\u7edf\u6a21\u578b\u5c06\u6d77\u51b0\u548c\u5357\u6781\u51b0\u67b6\u89c6\u4e3a\u72ec\u7acb\u7cfb\u7edf\uff0c\u65e0\u6cd5\u6355\u6349\u5c40\u90e8\u8054\u7cfb\u548c\u7ea7\u8054\u53cd\u9988\u3002\u672c\u7814\u7a76\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u4f7f\u7528Delaunay\u4e09\u89d2\u5256\u5206\u6784\u5efa\u7a7a\u95f4\u56fe\uff0c\u8282\u70b9\u8868\u793a\u663e\u8457\u53d8\u5316\u7684\u533a\u57df\uff0c\u8fb9\u7f16\u7801\u90bb\u8fd1\u6027\u548c\u65b9\u5411\u4e00\u81f4\u6027\u3002\u901a\u8fc7\u5e7f\u5ea6\u4f18\u5148\u641c\u7d22\u548c\u8499\u7279\u5361\u6d1b\u6a21\u62df\u9a8c\u8bc1\u8054\u7cfb\u8def\u5f84\u3002", "result": "\u63ed\u793a\u4e86\u975e\u5c40\u90e8\u3001\u7a7a\u95f4\u5f02\u8d28\u6027\u7684\u8026\u5408\u6a21\u5f0f\uff0c\u8868\u660e\u6d77\u51b0\u9000\u7f29\u53ef\u80fd\u5f15\u53d1\u6216\u653e\u5927\u4e0b\u6e38\u51b0\u67b6\u878d\u5316\u3002", "conclusion": "Spatial-Link\u4e3a\u6539\u8fdb\u6d77\u5e73\u9762\u4e0a\u5347\u9884\u6d4b\u548c\u6c14\u5019\u9002\u5e94\u7b56\u7565\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u7684\u6570\u636e\u9a71\u52a8\u5de5\u5177\u3002"}}
{"id": "2507.06779", "pdf": "https://arxiv.org/pdf/2507.06779", "abs": "https://arxiv.org/abs/2507.06779", "authors": ["Martin Wimpff", "Jan Zerfowski", "Bin Yang"], "title": "Tailoring deep learning for real-time brain-computer interfaces: From offline models to calibration-free online decoding", "categories": ["cs.HC", "cs.LG"], "comment": null, "summary": "Despite the growing success of deep learning (DL) in offline brain-computer\ninterfaces (BCIs), its adoption in real-time applications remains limited due\nto three primary challenges. First, most DL solutions are designed for offline\ndecoding, making the transition to online decoding unclear. Second, the use of\nsliding windows in online decoding substantially increases computational\ncomplexity. Third, DL models typically require large amounts of training data,\nwhich are often scarce in BCI applications. To address these challenges and\nenable real-time, cross-subject decoding without subject-specific calibration,\nwe introduce realtime adaptive pooling (RAP), a novel parameter-free method.\nRAP seamlessly modifies the pooling layers of existing offline DL models to\nmeet online decoding requirements. It also reduces computational complexity\nduring training by jointly decoding consecutive sliding windows. To further\nalleviate data requirements, our method leverages source-free domain\nadaptation, enabling privacy-preserving adaptation across varying amounts of\ntarget data. Our results demonstrate that RAP provides a robust and efficient\nframework for real-time BCI applications. It preserves privacy, reduces\ncalibration demands, and supports co-adaptive BCI systems, paving the way for\nbroader adoption of DL in online BCIs. These findings lay a strong foundation\nfor developing user-centered, high-performance BCIs that facilitate immediate\nfeedback and user learning.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aRAP\u7684\u65e0\u53c2\u6570\u65b9\u6cd5\uff0c\u89e3\u51b3\u4e86\u6df1\u5ea6\u5b66\u4e60\u5728\u5b9e\u65f6\u8111\u673a\u63a5\u53e3\u4e2d\u7684\u4e09\u5927\u6311\u6218\uff1a\u79bb\u7ebf\u5230\u5728\u7ebf\u7684\u8f6c\u6362\u3001\u8ba1\u7b97\u590d\u6742\u6027\u548c\u6570\u636e\u9700\u6c42\u3002", "motivation": "\u6df1\u5ea6\u5b66\u4e60\u5728\u79bb\u7ebf\u8111\u673a\u63a5\u53e3\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u5728\u5b9e\u65f6\u5e94\u7528\u4e2d\u9762\u4e34\u4e09\u5927\u6311\u6218\uff1a\u79bb\u7ebf\u6a21\u578b\u96be\u4ee5\u76f4\u63a5\u7528\u4e8e\u5728\u7ebf\u89e3\u7801\u3001\u6ed1\u52a8\u7a97\u53e3\u589e\u52a0\u8ba1\u7b97\u590d\u6742\u6027\u3001\u6570\u636e\u9700\u6c42\u5927\u3002", "method": "\u5f15\u5165RAP\u65b9\u6cd5\uff0c\u901a\u8fc7\u4fee\u6539\u73b0\u6709\u79bb\u7ebf\u6a21\u578b\u7684\u6c60\u5316\u5c42\u4ee5\u9002\u5e94\u5728\u7ebf\u89e3\u7801\u9700\u6c42\uff0c\u5e76\u5229\u7528\u6e90\u81ea\u7531\u57df\u9002\u5e94\u51cf\u5c11\u6570\u636e\u9700\u6c42\u3002", "result": "RAP\u5728\u5b9e\u65f6\u8111\u673a\u63a5\u53e3\u4e2d\u8868\u73b0\u7a33\u5065\u9ad8\u6548\uff0c\u652f\u6301\u9690\u79c1\u4fdd\u62a4\u3001\u51cf\u5c11\u6821\u51c6\u9700\u6c42\uff0c\u5e76\u4fc3\u8fdb\u5171\u9002\u5e94\u7cfb\u7edf\u3002", "conclusion": "RAP\u4e3a\u6df1\u5ea6\u5b66\u4e60\u5728\u5728\u7ebf\u8111\u673a\u63a5\u53e3\u4e2d\u7684\u5e7f\u6cdb\u5e94\u7528\u5960\u5b9a\u4e86\u57fa\u7840\uff0c\u63a8\u52a8\u4e86\u7528\u6237\u4e2d\u5fc3\u5316\u9ad8\u6027\u80fd\u7cfb\u7edf\u7684\u5f00\u53d1\u3002"}}
{"id": "2507.06817", "pdf": "https://arxiv.org/pdf/2507.06817", "abs": "https://arxiv.org/abs/2507.06817", "authors": ["Ayoub Farkane", "Mohamed Boutayeb", "Mustapha Oudani", "Mounir Ghogho"], "title": "Designing Robust Software Sensors for Nonlinear Systems via Neural Networks and Adaptive Sliding Mode Control", "categories": ["math.DS", "cs.LG", "cs.NE", "math.OC"], "comment": "Submitted to IEEE Transactions Journal", "summary": "Accurate knowledge of the state variables in a dynamical system is critical\nfor effective control, diagnosis, and supervision, especially when direct\nmeasurements of all states are infeasible. This paper presents a novel approach\nto designing software sensors for nonlinear dynamical systems expressed in\ntheir most general form. Unlike traditional model-based observers that rely on\nexplicit transformations or linearization, the proposed framework integrates\nneural networks with adaptive Sliding Mode Control (SMC) to design a robust\nstate observer under a less restrictive set of conditions. The learning process\nis driven by available sensor measurements, which are used to correct the\nobserver's state estimate. The training methodology leverages the system's\ngoverning equations as a physics-based constraint, enabling observer synthesis\nwithout access to ground-truth state trajectories. By employing a time-varying\ngain matrix dynamically adjusted by the neural network, the observer adapts in\nreal-time to system changes, ensuring robustness against noise, external\ndisturbances, and variations in system dynamics. Furthermore, we provide\nsufficient conditions to guarantee estimation error convergence, establishing a\ntheoretical foundation for the observer's reliability. The methodology's\neffectiveness is validated through simulations on challenging examples,\nincluding systems with non-differentiable dynamics and varying observability\nconditions. These examples, which are often problematic for conventional\ntechniques, serve to demonstrate the robustness and broad applicability of our\napproach. The results show rapid convergence and high accuracy, underscoring\nthe method's potential for addressing complex state estimation challenges in\nreal-world applications.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u795e\u7ecf\u7f51\u7edc\u548c\u81ea\u9002\u5e94\u6ed1\u6a21\u63a7\u5236\u7684\u65b0\u578b\u8f6f\u4ef6\u4f20\u611f\u5668\u8bbe\u8ba1\u65b9\u6cd5\uff0c\u7528\u4e8e\u975e\u7ebf\u6027\u52a8\u6001\u7cfb\u7edf\u7684\u9c81\u68d2\u72b6\u6001\u4f30\u8ba1\uff0c\u65e0\u9700\u4f9d\u8d56\u663e\u5f0f\u53d8\u6362\u6216\u7ebf\u6027\u5316\u3002", "motivation": "\u52a8\u6001\u7cfb\u7edf\u4e2d\u72b6\u6001\u53d8\u91cf\u7684\u51c6\u786e\u4f30\u8ba1\u5bf9\u63a7\u5236\u3001\u8bca\u65ad\u548c\u76d1\u63a7\u81f3\u5173\u91cd\u8981\uff0c\u5c24\u5176\u662f\u5728\u65e0\u6cd5\u76f4\u63a5\u6d4b\u91cf\u6240\u6709\u72b6\u6001\u65f6\u3002", "method": "\u7ed3\u5408\u795e\u7ecf\u7f51\u7edc\u4e0e\u81ea\u9002\u5e94\u6ed1\u6a21\u63a7\u5236\uff0c\u5229\u7528\u4f20\u611f\u5668\u6d4b\u91cf\u9a71\u52a8\u5b66\u4e60\u8fc7\u7a0b\uff0c\u5e76\u901a\u8fc7\u7cfb\u7edf\u52a8\u529b\u5b66\u65b9\u7a0b\u4f5c\u4e3a\u7269\u7406\u7ea6\u675f\uff0c\u8bbe\u8ba1\u5b9e\u65f6\u8c03\u6574\u7684\u65f6\u53d8\u589e\u76ca\u77e9\u9635\u3002", "result": "\u4eff\u771f\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u5728\u975e\u53ef\u5fae\u52a8\u6001\u548c\u53d8\u5316\u53ef\u89c2\u6d4b\u6027\u6761\u4ef6\u4e0b\u7684\u9c81\u68d2\u6027\u548c\u9ad8\u7cbe\u5ea6\uff0c\u5c55\u793a\u4e86\u5feb\u901f\u6536\u655b\u548c\u5e7f\u6cdb\u9002\u7528\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u590d\u6742\u72b6\u6001\u4f30\u8ba1\u95ee\u9898\u63d0\u4f9b\u4e86\u53ef\u9760\u7684\u7406\u8bba\u57fa\u7840\u548c\u5b9e\u9645\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2507.06844", "pdf": "https://arxiv.org/pdf/2507.06844", "abs": "https://arxiv.org/abs/2507.06844", "authors": ["Constantin Philippenko", "Batiste Le Bars", "Kevin Scaman", "Laurent Massouli\u00e9"], "title": "Adaptive collaboration for online personalized distributed learning with heterogeneous clients", "categories": ["stat.ML", "cs.LG"], "comment": "18 pages", "summary": "We study the problem of online personalized decentralized learning with $N$\nstatistically heterogeneous clients collaborating to accelerate local training.\nAn important challenge in this setting is to select relevant collaborators to\nreduce gradient variance while mitigating the introduced bias. To tackle this,\nwe introduce a gradient-based collaboration criterion, allowing each client to\ndynamically select peers with similar gradients during the optimization\nprocess. Our criterion is motivated by a refined and more general theoretical\nanalysis of the All-for-one algorithm, proved to be optimal in Even et al.\n(2022) for an oracle collaboration scheme. We derive excess loss upper-bounds\nfor smooth objective functions, being either strongly convex, non-convex, or\nsatisfying the Polyak-Lojasiewicz condition; our analysis reveals that the\nalgorithm acts as a variance reduction method where the speed-up depends on a\nsufficient variance. We put forward two collaboration methods instantiating the\nproposed general schema; and we show that one variant preserves the optimality\nof All-for-one. We validate our results with experiments on synthetic and real\ndatasets.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u5728\u7ebf\u4e2a\u6027\u5316\u53bb\u4e2d\u5fc3\u5316\u5b66\u4e60\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u68af\u5ea6\u7684\u534f\u4f5c\u51c6\u5219\uff0c\u52a8\u6001\u9009\u62e9\u76f8\u4f3c\u68af\u5ea6\u7684\u5ba2\u6237\u7aef\u4ee5\u51cf\u5c11\u65b9\u5dee\u5e76\u964d\u4f4e\u504f\u5dee\u3002\u7406\u8bba\u5206\u6790\u8868\u660e\u8be5\u7b97\u6cd5\u662f\u4e00\u79cd\u65b9\u5dee\u7f29\u51cf\u65b9\u6cd5\uff0c\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "motivation": "\u89e3\u51b3\u53bb\u4e2d\u5fc3\u5316\u5b66\u4e60\u4e2d\u5ba2\u6237\u7aef\u7edf\u8ba1\u5f02\u6784\u6027\u5e26\u6765\u7684\u6311\u6218\uff0c\u901a\u8fc7\u52a8\u6001\u9009\u62e9\u534f\u4f5c\u4f19\u4f34\u4ee5\u51cf\u5c11\u68af\u5ea6\u65b9\u5dee\u5e76\u964d\u4f4e\u504f\u5dee\u3002", "method": "\u5f15\u5165\u68af\u5ea6\u57fa\u7684\u534f\u4f5c\u51c6\u5219\uff0c\u52a8\u6001\u9009\u62e9\u68af\u5ea6\u76f8\u4f3c\u7684\u5ba2\u6237\u7aef\u8fdb\u884c\u534f\u4f5c\uff1b\u63d0\u51fa\u4e24\u79cd\u534f\u4f5c\u65b9\u6cd5\uff0c\u5176\u4e2d\u4e00\u79cd\u4fdd\u7559\u4e86All-for-one\u7b97\u6cd5\u7684\u4f18\u5316\u6027\u3002", "result": "\u7406\u8bba\u5206\u6790\u8868\u660e\u7b97\u6cd5\u5728\u5f3a\u51f8\u3001\u975e\u51f8\u6216\u6ee1\u8db3Polyak-Lojasiewicz\u6761\u4ef6\u7684\u5e73\u6ed1\u76ee\u6807\u51fd\u6570\u4e0a\u5177\u6709\u8d85\u989d\u635f\u5931\u4e0a\u754c\uff1b\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u7b97\u6cd5\u5728\u5408\u6210\u548c\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u63d0\u51fa\u7684\u534f\u4f5c\u51c6\u5219\u548c\u65b9\u6cd5\u5728\u53bb\u4e2d\u5fc3\u5316\u5b66\u4e60\u4e2d\u6709\u6548\u51cf\u5c11\u4e86\u65b9\u5dee\u5e76\u4fdd\u6301\u4e86\u4f18\u5316\u6027\uff0c\u5b9e\u9a8c\u8bc1\u660e\u4e86\u5176\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\u3002"}}
{"id": "2507.06921", "pdf": "https://arxiv.org/pdf/2507.06921", "abs": "https://arxiv.org/abs/2507.06921", "authors": ["Alokesh Manna", "Aditya Vikram Sett", "Dipak K. Dey", "Yuwen Gu", "Elizabeth D. Schifano", "Jichao He"], "title": "Distribution-free inference for LightGBM and GLM with Tweedie loss", "categories": ["stat.ML", "cs.LG", "Application to insurance data, Methodology"], "comment": null, "summary": "Prediction uncertainty quantification is a key research topic in recent years\nscientific and business problems. In insurance industries\n(\\cite{parodi2023pricing}), assessing the range of possible claim costs for\nindividual drivers improves premium pricing accuracy. It also enables insurers\nto manage risk more effectively by accounting for uncertainty in accident\nlikelihood and severity. In the presence of covariates, a variety of\nregression-type models are often used for modeling insurance claims, ranging\nfrom relatively simple generalized linear models (GLMs) to regularized GLMs to\ngradient boosting models (GBMs). Conformal predictive inference has arisen as a\npopular distribution-free approach for quantifying predictive uncertainty under\nrelatively weak assumptions of exchangeability, and has been well studied under\nthe classic linear regression setting. In this work, we propose new\nnon-conformity measures for GLMs and GBMs with GLM-type loss. Using regularized\nTweedie GLM regression and LightGBM with Tweedie loss, we demonstrate conformal\nprediction performance with these non-conformity measures in insurance claims\ndata. Our simulation results favor the use of locally weighted Pearson\nresiduals for LightGBM over other methods considered, as the resulting\nintervals maintained the nominal coverage with the smallest average width.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u975e\u4e00\u81f4\u6027\u5ea6\u91cf\u65b9\u6cd5\uff0c\u7528\u4e8eGLMs\u548cGBMs\uff0c\u4ee5\u6539\u8fdb\u4fdd\u9669\u7d22\u8d54\u9884\u6d4b\u4e2d\u7684\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u3002", "motivation": "\u4fdd\u9669\u884c\u4e1a\u9700\u8981\u51c6\u786e\u8bc4\u4f30\u7d22\u8d54\u6210\u672c\u7684\u8303\u56f4\u4ee5\u63d0\u9ad8\u4fdd\u8d39\u5b9a\u4ef7\u7684\u51c6\u786e\u6027\uff0c\u5e76\u66f4\u6709\u6548\u5730\u7ba1\u7406\u98ce\u9669\u3002", "method": "\u4f7f\u7528\u6b63\u5219\u5316Tweedie GLM\u56de\u5f52\u548cLightGBM\uff08\u5e26Tweedie\u635f\u5931\uff09\u8fdb\u884c\u4fdd\u5f62\u9884\u6d4b\uff0c\u5e76\u63d0\u51fa\u4e86\u65b0\u7684\u975e\u4e00\u81f4\u6027\u5ea6\u91cf\u65b9\u6cd5\u3002", "result": "\u6a21\u62df\u7ed3\u679c\u8868\u660e\uff0cLightGBM\u4f7f\u7528\u5c40\u90e8\u52a0\u6743Pearson\u6b8b\u5dee\u7684\u65b9\u6cd5\u5728\u4fdd\u6301\u540d\u4e49\u8986\u76d6\u8303\u56f4\u7684\u540c\u65f6\uff0c\u5e73\u5747\u533a\u95f4\u5bbd\u5ea6\u6700\u5c0f\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u4fdd\u9669\u7d22\u8d54\u6570\u636e\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u63a8\u8350\u4f7f\u7528\u5c40\u90e8\u52a0\u6743Pearson\u6b8b\u5dee\u7684LightGBM\u65b9\u6cd5\u3002"}}
{"id": "2507.06929", "pdf": "https://arxiv.org/pdf/2507.06929", "abs": "https://arxiv.org/abs/2507.06929", "authors": ["Sita Sch\u00f6nbauer", "Johanna P. Carbone", "Andreas Gr\u00fcneis"], "title": "Machine-Learned Force Fields for Lattice Dynamics at Coupled-Cluster Level Accuracy", "categories": ["cond-mat.mtrl-sci", "cs.LG", "physics.comp-ph"], "comment": "22 pages, 12 figures", "summary": "We investigate Machine-Learned Force Fields (MLFFs) trained on approximate\nDensity Functional Theory (DFT) and Coupled Cluster (CC) level potential energy\nsurfaces for the carbon diamond and lithium hydride solids. We assess the\naccuracy and precision of the MLFFs by calculating phonon dispersions and\nvibrational densities of states (VDOS) that are compared to experiment and\nreference ab initio results. To overcome limitations from long-range effects\nand the lack of atomic forces in the CC training data, a delta-learning\napproach based on the difference between CC and DFT results is explored.\nCompared to DFT, MLFFs trained on CC theory yield higher vibrational\nfrequencies for optical modes, agreeing better with experiment. Furthermore,\nthe MLFFs are used to estimate anharmonic effects on the VDOS of lithium\nhydride at the level of CC theory.", "AI": {"tldr": "\u7814\u7a76\u8bc4\u4f30\u4e86\u57fa\u4e8eDFT\u548cCC\u7406\u8bba\u7684\u673a\u5668\u5b66\u4e60\u529b\u573a\uff08MLFFs\uff09\u5728\u78b3\u91d1\u521a\u77f3\u548c\u6c22\u5316\u9502\u56fa\u4f53\u4e2d\u7684\u51c6\u786e\u6027\uff0c\u5e76\u901a\u8fc7\u58f0\u5b50\u8272\u6563\u548c\u632f\u52a8\u6001\u5bc6\u5ea6\uff08VDOS\uff09\u4e0e\u5b9e\u9a8c\u548c\u7406\u8bba\u7ed3\u679c\u5bf9\u6bd4\u3002\u91c7\u7528delta-learning\u65b9\u6cd5\u514b\u670dCC\u6570\u636e\u4e2d\u957f\u7a0b\u6548\u5e94\u548c\u539f\u5b50\u529b\u7f3a\u5931\u95ee\u9898\u3002", "motivation": "\u63a2\u7d22MLFFs\u5728DFT\u548cCC\u7406\u8bba\u4e0b\u7684\u8868\u73b0\uff0c\u4ee5\u63d0\u5347\u5bf9\u56fa\u4f53\u6750\u6599\u632f\u52a8\u7279\u6027\u7684\u9884\u6d4b\u51c6\u786e\u6027\u3002", "method": "\u901a\u8fc7delta-learning\u65b9\u6cd5\uff0c\u57fa\u4e8eCC\u4e0eDFT\u7ed3\u679c\u7684\u5dee\u5f02\u8bad\u7ec3MLFFs\uff0c\u5e76\u8ba1\u7b97\u58f0\u5b50\u8272\u6563\u548cVDOS\u3002", "result": "\u57fa\u4e8eCC\u7406\u8bba\u7684MLFFs\u5728\u5149\u5b66\u6a21\u5f0f\u632f\u52a8\u9891\u7387\u4e0a\u8868\u73b0\u4f18\u4e8eDFT\uff0c\u66f4\u63a5\u8fd1\u5b9e\u9a8c\u6570\u636e\uff0c\u5e76\u7528\u4e8e\u4f30\u8ba1\u6c22\u5316\u9502\u7684VDOS\u975e\u8c10\u6548\u5e94\u3002", "conclusion": "MLFFs\u5728CC\u7406\u8bba\u4e0b\u80fd\u66f4\u51c6\u786e\u5730\u9884\u6d4b\u56fa\u4f53\u6750\u6599\u7684\u632f\u52a8\u7279\u6027\uff0cdelta-learning\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u6570\u636e\u5c40\u9650\u6027\u95ee\u9898\u3002"}}
{"id": "2507.06961", "pdf": "https://arxiv.org/pdf/2507.06961", "abs": "https://arxiv.org/abs/2507.06961", "authors": ["Han Wang", "Yang Xu", "Wenbin Lu", "Rui Song"], "title": "Off-Policy Evaluation Under Nonignorable Missing Data", "categories": ["stat.ML", "cs.LG"], "comment": null, "summary": "Off-Policy Evaluation (OPE) aims to estimate the value of a target policy\nusing offline data collected from potentially different policies. In real-world\napplications, however, logged data often suffers from missingness. While OPE\nhas been extensively studied in the literature, a theoretical understanding of\nhow missing data affects OPE results remains unclear. In this paper, we\ninvestigate OPE in the presence of monotone missingness and theoretically\ndemonstrate that the value estimates remain unbiased under ignorable\nmissingness but can be biased under nonignorable (informative) missingness. To\nretain the consistency of value estimation, we propose an inverse probability\nweighted value estimator and conduct statistical inference to quantify the\nuncertainty of the estimates. Through a series of numerical experiments, we\nempirically demonstrate that our proposed estimator yields a more reliable\nvalue inference under missing data.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u5728\u6570\u636e\u7f3a\u5931\u60c5\u51b5\u4e0b\u79bb\u7b56\u7565\u8bc4\u4f30\uff08OPE\uff09\u7684\u7406\u8bba\u5f71\u54cd\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u9006\u6982\u7387\u52a0\u6743\u4f30\u8ba1\u5668\u4ee5\u63d0\u9ad8\u4f30\u8ba1\u7684\u53ef\u9760\u6027\u3002", "motivation": "\u73b0\u5b9e\u5e94\u7528\u4e2d\uff0c\u79bb\u7ebf\u6570\u636e\u5e38\u5b58\u5728\u7f3a\u5931\u95ee\u9898\uff0c\u4f46\u7f3a\u5931\u6570\u636e\u5bf9OPE\u7ed3\u679c\u7684\u7406\u8bba\u5f71\u54cd\u5c1a\u4e0d\u660e\u786e\u3002\u672c\u6587\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u5728\u5355\u8c03\u7f3a\u5931\u60c5\u51b5\u4e0b\uff0c\u7406\u8bba\u5206\u6790\u4e86OPE\u7684\u65e0\u504f\u6027\uff0c\u63d0\u51fa\u9006\u6982\u7387\u52a0\u6743\u4f30\u8ba1\u5668\uff0c\u5e76\u901a\u8fc7\u7edf\u8ba1\u63a8\u65ad\u91cf\u5316\u4f30\u8ba1\u4e0d\u786e\u5b9a\u6027\u3002", "result": "\u6570\u503c\u5b9e\u9a8c\u8868\u660e\uff0c\u6240\u63d0\u4f30\u8ba1\u5668\u5728\u7f3a\u5931\u6570\u636e\u4e0b\u80fd\u63d0\u4f9b\u66f4\u53ef\u9760\u7684\u4ef7\u503c\u63a8\u65ad\u3002", "conclusion": "\u5728\u53ef\u5ffd\u7565\u7f3a\u5931\u4e0bOPE\u4f30\u8ba1\u65e0\u504f\uff0c\u4f46\u5728\u975e\u53ef\u5ffd\u7565\u7f3a\u5931\u4e0b\u53ef\u80fd\u504f\u5dee\uff1b\u9006\u6982\u7387\u52a0\u6743\u4f30\u8ba1\u5668\u80fd\u6709\u6548\u4fdd\u6301\u4f30\u8ba1\u4e00\u81f4\u6027\u3002"}}
{"id": "2507.06997", "pdf": "https://arxiv.org/pdf/2507.06997", "abs": "https://arxiv.org/abs/2507.06997", "authors": ["Deemah H. Tashman", "Soumaya Cherkaoui", "Walaa Hamouda"], "title": "Federated Learning-based MARL for Strengthening Physical-Layer Security in B5G Networks", "categories": ["eess.SP", "cs.ET", "cs.LG", "cs.NI"], "comment": null, "summary": "This paper explores the application of a federated learning-based multi-agent\nreinforcement learning (MARL) strategy to enhance physical-layer security (PLS)\nin a multi-cellular network within the context of beyond 5G networks. At each\ncell, a base station (BS) operates as a deep reinforcement learning (DRL) agent\nthat interacts with the surrounding environment to maximize the secrecy rate of\nlegitimate users in the presence of an eavesdropper. This eavesdropper attempts\nto intercept the confidential information shared between the BS and its\nauthorized users. The DRL agents are deemed to be federated since they only\nshare their network parameters with a central server and not the private data\nof their legitimate users. Two DRL approaches, deep Q-network (DQN) and\nReinforce deep policy gradient (RDPG), are explored and compared. The results\ndemonstrate that RDPG converges more rapidly than DQN. In addition, we\ndemonstrate that the proposed method outperforms the distributed DRL approach.\nFurthermore, the outcomes illustrate the trade-off between security and\ncomplexity.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u8054\u90a6\u5b66\u4e60\u7684\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\uff08MARL\uff09\u7b56\u7565\uff0c\u7528\u4e8e\u589e\u5f3a\u8d855G\u7f51\u7edc\u4e2d\u591a\u8702\u7a9d\u7f51\u7edc\u7684\u7269\u7406\u5c42\u5b89\u5168\u6027\uff08PLS\uff09\u3002", "motivation": "\u5728\u8d855G\u7f51\u7edc\u4e2d\uff0c\u5b58\u5728\u7a83\u542c\u8005\u8bd5\u56fe\u62e6\u622a\u57fa\u7ad9\u4e0e\u5408\u6cd5\u7528\u6237\u4e4b\u95f4\u7684\u673a\u5bc6\u4fe1\u606f\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u9ad8\u6548\u7684\u65b9\u6cd5\u6765\u63d0\u5347\u5b89\u5168\u6027\u3002", "method": "\u6bcf\u4e2a\u57fa\u7ad9\u7684\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\uff08DRL\uff09\u4ee3\u7406\u901a\u8fc7\u8054\u90a6\u5b66\u4e60\u5171\u4eab\u7f51\u7edc\u53c2\u6570\uff08\u800c\u975e\u7528\u6237\u6570\u636e\uff09\uff0c\u5e76\u6bd4\u8f83\u4e86DQN\u548cRDPG\u4e24\u79cd\u65b9\u6cd5\u3002", "result": "\u7ed3\u679c\u663e\u793a\uff0cRDPG\u6bd4DQN\u6536\u655b\u66f4\u5feb\uff0c\u4e14\u4f18\u4e8e\u5206\u5e03\u5f0fDRL\u65b9\u6cd5\uff0c\u4f46\u4e5f\u63ed\u793a\u4e86\u5b89\u5168\u6027\u4e0e\u590d\u6742\u6027\u4e4b\u95f4\u7684\u6743\u8861\u3002", "conclusion": "\u63d0\u51fa\u7684\u8054\u90a6\u5b66\u4e60MARL\u7b56\u7565\u5728\u63d0\u5347\u7269\u7406\u5c42\u5b89\u5168\u6027\u65b9\u9762\u5177\u6709\u6f5c\u529b\uff0c\u4f46\u9700\u8fdb\u4e00\u6b65\u4f18\u5316\u4ee5\u5e73\u8861\u5b89\u5168\u6027\u4e0e\u590d\u6742\u6027\u3002"}}
{"id": "2507.07031", "pdf": "https://arxiv.org/pdf/2507.07031", "abs": "https://arxiv.org/abs/2507.07031", "authors": ["Bing-Jyue Chen", "Lilia Tang", "Daniel Kang"], "title": "ZKTorch: Compiling ML Inference to Zero-Knowledge Proofs via Parallel Proof Accumulation", "categories": ["cs.CR", "cs.LG"], "comment": "16 pages, 2 figures", "summary": "As AI models become ubiquitous in our daily lives, there has been an\nincreasing demand for transparency in ML services. However, the model owner\ndoes not want to reveal the weights, as they are considered trade secrets. To\nsolve this problem, researchers have turned to zero-knowledge proofs of ML\nmodel inference. These proofs convince the user that the ML model output is\ncorrect, without revealing the weights of the model to the user. Past work on\nthese provers can be placed into two categories. The first method compiles the\nML model into a low-level circuit, and proves the circuit using a ZK-SNARK. The\nsecond method uses custom cryptographic protocols designed only for a specific\nclass of models. Unfortunately, the first method is highly inefficient, making\nit impractical for the large models used today, and the second method does not\ngeneralize well, making it difficult to update in the rapidly changing field of\nmachine learning. To solve this, we propose ZKTorch, an open source end-to-end\nproving system that compiles ML models into base cryptographic operations\ncalled basic blocks, each proved using specialized protocols. ZKTorch is built\non top of a novel parallel extension to the Mira accumulation scheme, enabling\nsuccinct proofs with minimal accumulation overhead. These contributions allow\nZKTorch to achieve at least a $3\\times$ reduction in the proof size compared to\nspecialized protocols and up to a $6\\times$ speedup in proving time over a\ngeneral-purpose ZKML framework.", "AI": {"tldr": "ZKTorch\u662f\u4e00\u79cd\u5f00\u6e90\u7aef\u5230\u7aef\u8bc1\u660e\u7cfb\u7edf\uff0c\u901a\u8fc7\u5c06ML\u6a21\u578b\u7f16\u8bd1\u4e3a\u57fa\u672c\u52a0\u5bc6\u64cd\u4f5c\u5757\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u8bc1\u660e\u6548\u7387\u548c\u901a\u7528\u6027\u3002", "motivation": "\u968f\u7740AI\u6a21\u578b\u7684\u666e\u53ca\uff0c\u7528\u6237\u5bf9ML\u670d\u52a1\u7684\u900f\u660e\u5ea6\u9700\u6c42\u589e\u52a0\uff0c\u4f46\u6a21\u578b\u6743\u91cd\u4f5c\u4e3a\u5546\u4e1a\u79d8\u5bc6\u4e0d\u80fd\u516c\u5f00\u3002\u73b0\u6709\u65b9\u6cd5\u6548\u7387\u4f4e\u6216\u901a\u7528\u6027\u5dee\uff0c\u65e0\u6cd5\u6ee1\u8db3\u9700\u6c42\u3002", "method": "ZKTorch\u5c06ML\u6a21\u578b\u7f16\u8bd1\u4e3a\u57fa\u672c\u52a0\u5bc6\u64cd\u4f5c\u5757\uff0c\u5229\u7528\u5e76\u884c\u6269\u5c55\u7684Mira\u7d2f\u79ef\u65b9\u6848\uff0c\u5b9e\u73b0\u7b80\u6d01\u8bc1\u660e\u548c\u9ad8\u6548\u9a8c\u8bc1\u3002", "result": "ZKTorch\u5728\u8bc1\u660e\u5927\u5c0f\u4e0a\u6bd4\u4e13\u7528\u534f\u8bae\u51cf\u5c11\u81f3\u5c113\u500d\uff0c\u8bc1\u660e\u65f6\u95f4\u6bd4\u901a\u7528ZKML\u6846\u67b6\u5feb6\u500d\u3002", "conclusion": "ZKTorch\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u6548\u7387\u4e0e\u901a\u7528\u6027\u95ee\u9898\uff0c\u4e3aML\u6a21\u578b\u900f\u660e\u6027\u63d0\u4f9b\u4e86\u5b9e\u7528\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.07041", "pdf": "https://arxiv.org/pdf/2507.07041", "abs": "https://arxiv.org/abs/2507.07041", "authors": ["Enze Shi", "Jinhan Xie", "Bei Jiang", "Linglong Kong", "Xuming He"], "title": "Non-Asymptotic Analysis of Online Local Private Learning with SGD", "categories": ["stat.ME", "cs.LG", "stat.ML"], "comment": null, "summary": "Differentially Private Stochastic Gradient Descent (DP-SGD) has been widely\nused for solving optimization problems with privacy guarantees in machine\nlearning and statistics. Despite this, a systematic non-asymptotic convergence\nanalysis for DP-SGD, particularly in the context of online problems and local\ndifferential privacy (LDP) models, remains largely elusive. Existing\nnon-asymptotic analyses have focused on non-private optimization methods, and\nhence are not applicable to privacy-preserving optimization problems. This work\ninitiates the analysis to bridge this gap and opens the door to non-asymptotic\nconvergence analysis of private optimization problems. A general framework is\ninvestigated for the online LDP model in stochastic optimization problems. We\nassume that sensitive information from individuals is collected sequentially\nand aim to estimate, in real-time, a static parameter that pertains to the\npopulation of interest. Most importantly, we conduct a comprehensive\nnon-asymptotic convergence analysis of the proposed estimators in finite-sample\nsituations, which gives their users practical guidelines regarding the effect\nof various hyperparameters, such as step size, parameter dimensions, and\nprivacy budgets, on convergence rates. Our proposed estimators are validated in\nthe theoretical and practical realms by rigorous mathematical derivations and\ncarefully constructed numerical experiments.", "AI": {"tldr": "\u672c\u6587\u5bf9\u5dee\u5206\u9690\u79c1\u968f\u673a\u68af\u5ea6\u4e0b\u964d\uff08DP-SGD\uff09\u5728\u5728\u7ebf\u95ee\u9898\u548c\u5c40\u90e8\u5dee\u5206\u9690\u79c1\uff08LDP\uff09\u6a21\u578b\u4e2d\u7684\u975e\u6e10\u8fd1\u6536\u655b\u6027\u8fdb\u884c\u4e86\u7cfb\u7edf\u6027\u5206\u6790\uff0c\u586b\u8865\u4e86\u73b0\u6709\u7814\u7a76\u7684\u7a7a\u767d\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u4e3b\u8981\u5173\u6ce8\u975e\u9690\u79c1\u4f18\u5316\u65b9\u6cd5\uff0c\u7f3a\u4e4f\u5bf9\u9690\u79c1\u4fdd\u62a4\u4f18\u5316\u95ee\u9898\u7684\u975e\u6e10\u8fd1\u6536\u655b\u5206\u6790\uff0c\u672c\u6587\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u63d0\u51fa\u4e00\u4e2a\u9002\u7528\u4e8e\u5728\u7ebfLDP\u6a21\u578b\u7684\u901a\u7528\u6846\u67b6\uff0c\u5047\u8bbe\u654f\u611f\u4fe1\u606f\u6309\u987a\u5e8f\u6536\u96c6\uff0c\u5e76\u5b9e\u65f6\u4f30\u8ba1\u4e0e\u76ee\u6807\u7fa4\u4f53\u76f8\u5173\u7684\u9759\u6001\u53c2\u6570\u3002", "result": "\u901a\u8fc7\u4e25\u683c\u7684\u6570\u5b66\u63a8\u5bfc\u548c\u6570\u503c\u5b9e\u9a8c\uff0c\u9a8c\u8bc1\u4e86\u6240\u63d0\u4f30\u8ba1\u5668\u7684\u6709\u6548\u6027\uff0c\u5e76\u5206\u6790\u4e86\u8d85\u53c2\u6570\uff08\u5982\u6b65\u957f\u3001\u53c2\u6570\u7ef4\u5ea6\u548c\u9690\u79c1\u9884\u7b97\uff09\u5bf9\u6536\u655b\u901f\u7387\u7684\u5f71\u54cd\u3002", "conclusion": "\u672c\u6587\u4e3a\u975e\u6e10\u8fd1\u6536\u655b\u5206\u6790\u5728\u9690\u79c1\u4f18\u5316\u95ee\u9898\u4e2d\u7684\u5e94\u7528\u5960\u5b9a\u4e86\u57fa\u7840\uff0c\u5e76\u4e3a\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u4e86\u5b9e\u7528\u6307\u5357\u3002"}}
{"id": "2507.07056", "pdf": "https://arxiv.org/pdf/2507.07056", "abs": "https://arxiv.org/abs/2507.07056", "authors": ["Jiahao Chen", "junhao li", "Yiming Wang", "Zhe Ma", "Yi Jiang", "Chunyi Zhou", "Qingming Li", "Tianyu Du", "Shouling Ji"], "title": "LoRAShield: Data-Free Editing Alignment for Secure Personalized LoRA Sharing", "categories": ["cs.CR", "cs.LG"], "comment": null, "summary": "The proliferation of Low-Rank Adaptation (LoRA) models has democratized\npersonalized text-to-image generation, enabling users to share lightweight\nmodels (e.g., personal portraits) on platforms like Civitai and Liblib.\nHowever, this \"share-and-play\" ecosystem introduces critical risks: benign\nLoRAs can be weaponized by adversaries to generate harmful content (e.g.,\npolitical, defamatory imagery), undermining creator rights and platform safety.\nExisting defenses like concept-erasure methods focus on full diffusion models\n(DMs), neglecting LoRA's unique role as a modular adapter and its vulnerability\nto adversarial prompt engineering. To bridge this gap, we propose LoRAShield,\nthe first data-free editing framework for securing LoRA models against misuse.\nOur platform-driven approach dynamically edits and realigns LoRA's weight\nsubspace via adversarial optimization and semantic augmentation. Experimental\nresults demonstrate that LoRAShield achieves remarkable effectiveness,\nefficiency, and robustness in blocking malicious generations without\nsacrificing the functionality of the benign task. By shifting the defense to\nplatforms, LoRAShield enables secure, scalable sharing of personalized models,\na critical step toward trustworthy generative ecosystems.", "AI": {"tldr": "LoRAShield\u662f\u4e00\u79cd\u6570\u636e\u65e0\u5173\u7684\u7f16\u8f91\u6846\u67b6\uff0c\u7528\u4e8e\u4fdd\u62a4LoRA\u6a21\u578b\u514d\u53d7\u6ee5\u7528\uff0c\u901a\u8fc7\u52a8\u6001\u7f16\u8f91\u548c\u91cd\u65b0\u5bf9\u9f50\u6743\u91cd\u5b50\u7a7a\u95f4\uff0c\u6709\u6548\u963b\u6b62\u6076\u610f\u751f\u6210\u5185\u5bb9\u3002", "motivation": "LoRA\u6a21\u578b\u7684\u5171\u4eab\u751f\u6001\u7cfb\u7edf\u5b58\u5728\u88ab\u6ee5\u7528\u4e8e\u751f\u6210\u6709\u5bb3\u5185\u5bb9\u7684\u98ce\u9669\uff0c\u73b0\u6709\u9632\u5fa1\u65b9\u6cd5\u5ffd\u89c6\u5176\u6a21\u5757\u5316\u9002\u914d\u5668\u7684\u7279\u6027\u3002", "method": "\u63d0\u51faLoRAShield\u6846\u67b6\uff0c\u7ed3\u5408\u5bf9\u6297\u4f18\u5316\u548c\u8bed\u4e49\u589e\u5f3a\u52a8\u6001\u7f16\u8f91LoRA\u6743\u91cd\u5b50\u7a7a\u95f4\u3002", "result": "\u5b9e\u9a8c\u663e\u793aLoRAShield\u5728\u963b\u6b62\u6076\u610f\u751f\u6210\u7684\u540c\u65f6\u4fdd\u6301\u826f\u6027\u4efb\u52a1\u529f\u80fd\uff0c\u5177\u6709\u9ad8\u6548\u6027\u548c\u9c81\u68d2\u6027\u3002", "conclusion": "LoRAShield\u4e3a\u5e73\u53f0\u63d0\u4f9b\u5b89\u5168\u3001\u53ef\u6269\u5c55\u7684\u4e2a\u6027\u5316\u6a21\u578b\u5171\u4eab\u65b9\u6848\uff0c\u63a8\u52a8\u53ef\u4fe1\u751f\u6210\u751f\u6001\u7cfb\u7edf\u7684\u53d1\u5c55\u3002"}}
{"id": "2507.07067", "pdf": "https://arxiv.org/pdf/2507.07067", "abs": "https://arxiv.org/abs/2507.07067", "authors": ["Clement Ruah", "Houssem Sifaou", "Osvaldo Simeone", "Bashir M. Al-Hashimi"], "title": "How to Bridge the Sim-to-Real Gap in Digital Twin-Aided Telecommunication Networks", "categories": ["eess.SP", "cs.LG"], "comment": "This work has been submitted to the IEEE for possible publication", "summary": "Training effective artificial intelligence models for telecommunications is\nchallenging due to the scarcity of deployment-specific data. Real data\ncollection is expensive, and available datasets often fail to capture the\nunique operational conditions and contextual variability of the network\nenvironment. Digital twinning provides a potential solution to this problem, as\nsimulators tailored to the current network deployment can generate\nsite-specific data to augment the available training datasets. However, there\nis a need to develop solutions to bridge the inherent simulation-to-reality\n(sim-to-real) gap between synthetic and real-world data. This paper reviews\nrecent advances on two complementary strategies: 1) the calibration of digital\ntwins (DTs) through real-world measurements, and 2) the use of sim-to-real\ngap-aware training strategies to robustly handle residual discrepancies between\ndigital twin-generated and real data. For the latter, we evaluate two\nconceptually distinct methods that model the sim-to-real gap either at the\nlevel of the environment via Bayesian learning or at the level of the training\nloss via prediction-powered inference.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u5229\u7528\u6570\u5b57\u5b6a\u751f\u89e3\u51b3\u7535\u4fe1\u9886\u57dfAI\u6a21\u578b\u8bad\u7ec3\u6570\u636e\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u5e76\u63d0\u51fa\u4e86\u4e24\u79cd\u7f29\u5c0f\u4eff\u771f\u4e0e\u73b0\u5b9e\u6570\u636e\u5dee\u8ddd\u7684\u7b56\u7565\u3002", "motivation": "\u7535\u4fe1\u9886\u57dfAI\u6a21\u578b\u8bad\u7ec3\u9762\u4e34\u90e8\u7f72\u7279\u5b9a\u6570\u636e\u7a00\u7f3a\u7684\u6311\u6218\uff0c\u73b0\u6709\u6570\u636e\u96c6\u96be\u4ee5\u6355\u6349\u7f51\u7edc\u73af\u5883\u7684\u72ec\u7279\u6761\u4ef6\u548c\u53d8\u5f02\u6027\u3002", "method": "1) \u901a\u8fc7\u771f\u5b9e\u6d4b\u91cf\u6821\u51c6\u6570\u5b57\u5b6a\u751f\uff1b2) \u4f7f\u7528\u4eff\u771f\u4e0e\u73b0\u5b9e\u5dee\u8ddd\u611f\u77e5\u7684\u8bad\u7ec3\u7b56\u7565\uff0c\u5305\u62ec\u8d1d\u53f6\u65af\u5b66\u4e60\u548c\u9884\u6d4b\u9a71\u52a8\u7684\u63a8\u65ad\u65b9\u6cd5\u3002", "result": "\u8bba\u6587\u8bc4\u4f30\u4e86\u4e24\u79cd\u65b9\u6cd5\uff0c\u5206\u522b\u4ece\u73af\u5883\u548c\u8bad\u7ec3\u635f\u5931\u5c42\u9762\u5efa\u6a21\u4eff\u771f\u4e0e\u73b0\u5b9e\u5dee\u8ddd\u3002", "conclusion": "\u6570\u5b57\u5b6a\u751f\u548c\u4eff\u771f\u4e0e\u73b0\u5b9e\u5dee\u8ddd\u611f\u77e5\u7b56\u7565\u4e3a\u7535\u4fe1AI\u6a21\u578b\u8bad\u7ec3\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
