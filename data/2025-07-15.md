<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 44]
- [cs.CL](#cs.CL) [Total: 63]
- [cs.CV](#cs.CV) [Total: 162]
- [cs.LG](#cs.LG) [Total: 162]
- [cs.MA](#cs.MA) [Total: 11]
- [cs.MM](#cs.MM) [Total: 4]
- [cs.RO](#cs.RO) [Total: 45]
- [cs.SD](#cs.SD) [Total: 16]
- [cs.LO](#cs.LO) [Total: 1]
- [q-bio.NC](#q-bio.NC) [Total: 2]
- [cs.DS](#cs.DS) [Total: 2]
- [math.DS](#math.DS) [Total: 1]
- [nlin.CD](#nlin.CD) [Total: 1]
- [cs.NE](#cs.NE) [Total: 1]
- [cs.DC](#cs.DC) [Total: 4]
- [cs.IT](#cs.IT) [Total: 1]
- [cs.CR](#cs.CR) [Total: 12]
- [q-bio.BM](#q-bio.BM) [Total: 2]
- [cs.AR](#cs.AR) [Total: 3]
- [eess.SY](#eess.SY) [Total: 4]
- [cs.HC](#cs.HC) [Total: 5]
- [physics.ao-ph](#physics.ao-ph) [Total: 1]
- [cs.CE](#cs.CE) [Total: 2]
- [physics.soc-ph](#physics.soc-ph) [Total: 1]
- [stat.ML](#stat.ML) [Total: 15]
- [cs.CY](#cs.CY) [Total: 3]
- [cs.GR](#cs.GR) [Total: 3]
- [cs.NI](#cs.NI) [Total: 1]
- [q-bio.PE](#q-bio.PE) [Total: 1]
- [cs.GT](#cs.GT) [Total: 2]
- [quant-ph](#quant-ph) [Total: 1]
- [math.HO](#math.HO) [Total: 1]
- [cs.SI](#cs.SI) [Total: 3]
- [q-bio.QM](#q-bio.QM) [Total: 3]
- [q-fin.ST](#q-fin.ST) [Total: 1]
- [physics.plasm-ph](#physics.plasm-ph) [Total: 2]
- [stat.ME](#stat.ME) [Total: 1]
- [q-bio.GN](#q-bio.GN) [Total: 1]
- [cs.SE](#cs.SE) [Total: 11]
- [eess.AS](#eess.AS) [Total: 11]
- [cond-mat.mtrl-sci](#cond-mat.mtrl-sci) [Total: 1]
- [physics.data-an](#physics.data-an) [Total: 1]
- [math.OC](#math.OC) [Total: 5]
- [cs.DB](#cs.DB) [Total: 3]
- [stat.AP](#stat.AP) [Total: 1]
- [eess.SP](#eess.SP) [Total: 1]
- [cs.IR](#cs.IR) [Total: 7]
- [physics.chem-ph](#physics.chem-ph) [Total: 2]
- [physics.flu-dyn](#physics.flu-dyn) [Total: 2]
- [astro-ph.IM](#astro-ph.IM) [Total: 1]
- [cond-mat.dis-nn](#cond-mat.dis-nn) [Total: 1]
- [eess.IV](#eess.IV) [Total: 15]
- [math.NA](#math.NA) [Total: 2]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [Think Clearly: Improving Reasoning via Redundant Token Pruning](https://arxiv.org/abs/2507.08806)
*Daewon Choi,Jimin Lee,Jihoon Tack,Woomin Song,Saket Dingliwal,Sai Muralidhar Jayanthi,Bhavana Ganesh,Jinwoo Shin,Aram Galstyan,Sravan Babu Bodapati*

Main category: cs.AI

TL;DR: 论文提出了一种通过去除推理过程中的冗余注意力来提升大语言模型性能的方法，显著提高了数学竞赛等推理密集型任务的准确性。


<details>
  <summary>Details</summary>
Motivation: 观察到现有大语言模型在长链推理中存在注意力冗余，尤其是错误答案中注意力稀疏性更高，影响了推理性能。

Method: 通过测量特殊标记（end-of-thinking token）的注意力分数，系统性识别冗余，并提出结构感知剪枝方法去除低贡献推理块。

Result: 方法显著提升了推理密集型任务的准确性，尤其在数学竞赛（如AIME和AMC）中表现突出。

Conclusion: 去除推理冗余能有效提升模型性能，无需额外训练，为长链推理优化提供了新思路。

Abstract: Recent large language models have shown promising capabilities in long-form
reasoning, following structured chains of thought before arriving at a final
answer. However, we observe that these reasoning paths tend to include
substantial redundancy; analyzing attention patterns reveals that attention
scores are widely scattered, particularly incorrect answers exhibit greater
attention sparsity. In this paper, we demonstrate that deliberately removing
this redundancy in the reasoning process significantly improves performance
through clear thinking, i.e., removing distraction. Specifically, we
systematically identify reasoning redundancy by measuring token-level attention
scores to a special end-of-thinking token, which is appended to an explicit
instruction inserted to conclude each intermediate reasoning step. Furthermore,
we propose structure-aware pruning that prioritizes removing tokens in
low-contributing reasoning chunks over individual tokens. After evicting
redundant tokens, we remove the injected end-of-thinking instruction, then
resume the reasoning generation. We demonstrate that our method significantly
improves overall accuracy across reasoning-intensive benchmarks without any
training involved. In particular, our method shows strong performance on
challenging mathematical competition benchmarks such as AIME and AMC, where
reasoning redundancy is more prevalent.

</details>


### [2] [A New Approach for Multicriteria Assessment in the Ranking of Alternatives Using Cardinal and Ordinal Data](https://arxiv.org/abs/2507.08875)
*Fuh-Hwa Franklin Liu,Su-Chuan Shih*

Main category: cs.AI

TL;DR: 提出了一种结合两种虚拟差距分析（VGA）模型的新多准则评估方法，以提高评估的效率和公平性。


<details>
  <summary>Details</summary>
Motivation: 现有多准则评估方法依赖假设和主观判断，且常忽视定量和定性标准的结合，影响评估的全面性和可靠性。

Method: 结合两种VGA模型，基于线性规划，提出新的多准则评估方法。

Result: 通过两个数值示例验证了方法的准确性和透明度。

Conclusion: 该方法为自动化决策系统提供了强大且灵活的解决方案，推动了决策支持系统的进步。

Abstract: Modern methods for multi-criteria assessment (MCA), such as Data Envelopment
Analysis (DEA), Stochastic Frontier Analysis (SFA), and Multiple Criteria
Decision-Making (MCDM), are utilized to appraise a collection of
Decision-Making Units (DMUs), also known as alternatives, based on several
criteria. These methodologies inherently rely on assumptions and can be
influenced by subjective judgment to effectively tackle the complex evaluation
challenges in various fields. In real-world scenarios, it is essential to
incorporate both quantitative and qualitative criteria as they consist of
cardinal and ordinal data. Despite the inherent variability in the criterion
values of different alternatives, the homogeneity assumption is often employed,
significantly affecting evaluations. To tackle these challenges and determine
the most appropriate alternative, we propose a novel MCA approach that combines
two Virtual Gap Analysis (VGA) models. The VGA framework, rooted in linear
programming, is pivotal in the MCA methodology. This approach improves
efficiency and fairness, ensuring that evaluations are both comprehensive and
dependable, thus offering a strong and adaptive solution. Two comprehensive
numerical examples demonstrate the accuracy and transparency of our proposed
method. The goal is to encourage continued advancement and stimulate progress
in automated decision systems and decision support systems.

</details>


### [3] [Multi-Actor Generative Artificial Intelligence as a Game Engine](https://arxiv.org/abs/2507.08892)
*Alexander Sasha Vezhnevets,Jayd Matyas,Logan Cross,Davide Paglieri,Minsuk Chang,William A. Cunningham,Simon Osindero,William S. Isaac,Joel Z. Leibo*

Main category: cs.AI

TL;DR: 论文提出了一种基于桌游角色扮演游戏（TTRPGs）的灵活场景定义框架，用于支持生成式AI在多角色环境中的多样化应用。


<details>
  <summary>Details</summary>
Motivation: 为了满足生成式AI在模拟、戏剧化和评估等多样化应用场景中的需求，需要一个灵活的框架来定义和管理场景。

Method: 借鉴TTRPGs中的游戏主持人（GM）概念，采用实体-组件架构模式，将GM设计为可配置的实体，由组件构成，实现工程师和设计师的分工协作。

Result: 通过Concordia库的实践，展示了该框架如何支持用户快速配置符合特定目标的场景，同时保持模块化和可扩展性。

Conclusion: 该框架通过分离关注点，实现了快速迭代、模块化和可扩展性，为生成式AI的多样化应用提供了有效支持。

Abstract: Generative AI can be used in multi-actor environments with purposes ranging
from social science modeling to interactive narrative and AI evaluation.
Supporting this diversity of use cases -- which we classify as Simulationist,
Dramatist, and Evaluationist -- demands a flexible scenario definition
framework. We argue here that a good approach is to take inspiration from
tabletop role-playing games (TTRPGs), where a Game Master (GM) is responsible
for the environment and generates all parts of the story not directly
determined by the voluntary actions of player characters. We argue that the
Entity-Component architectural pattern is useful here. In such a system, the GM
is not a hardcoded computer game but is itself a configurable entity, composed
of components just like any other actor. By design, the approach allows for a
separation between the underlying implementation details handled by an
engineer, the creation of reusable components, and their composition and
configuration managed by a designer who constructs entities from the
components. This separation of concerns is instrumental for achieving rapid
iteration, maintaining modularity, and ultimately to ensure scalability. We
describe the ongoing evolution of the Concordia library in terms of this
philosophy, demonstrating how it allows users to effectively configure
scenarios that align with their specific goals.

</details>


### [4] [BioAnalyst: A Foundation Model for Biodiversity](https://arxiv.org/abs/2507.09080)
*Athanasios Trantas,Martino Mensio,Stylianos Stasinos,Sebastian Gribincea,Taimur Khan,Damian Podareanu,Aliene van der Veen*

Main category: cs.AI

TL;DR: BioAnalyst是一个基于Transformer架构的AI基础模型，专为生物多样性分析和保护规划设计，通过多模态数据预训练，适用于多种下游任务，并在数据稀缺场景中表现优异。


<details>
  <summary>Details</summary>
Motivation: 生物多样性快速丧失对生态研究和保护策略提出严峻挑战，需要全面的监测和预测能力。AI基础模型在科学领域展现出潜力，因此开发了BioAnalyst以应对生态保护需求。

Method: BioAnalyst采用Transformer架构，预训练于物种记录、遥感数据、气候和环境变量等多模态数据集，可微调用于物种分布建模、栖息地评估等任务。

Result: BioAnalyst在两种下游任务中表现优于现有方法，尤其在数据稀缺场景中，为生态预测设立了新的准确性基准。

Conclusion: BioAnalyst的开放发布旨在促进生物多样性建模的合作，推动AI解决生态挑战。

Abstract: The accelerating loss of biodiversity presents critical challenges for
ecological research and conservation strategies. The preservation of
biodiversity is paramount for maintaining ecological balance and ensuring the
sustainability of ecosystems. However, biodiversity faces numerous threats,
including habitat loss, climate change, and the proliferation of invasive
species. Addressing these and other ecology-related challenges, both at local
and global scales, requires comprehensive monitoring, predictive and
conservation planning capabilities. Artificial Intelligence (AI) Foundation
Models (FMs) have gained significant momentum in numerous scientific domains by
leveraging vast datasets to learn general-purpose representations adaptable to
various downstream tasks. This paradigm holds immense promise for biodiversity
conservation. In response, we introduce BioAnalyst, the first Foundation Model
tailored for biodiversity analysis and conservation planning. BioAnalyst
employs a transformer-based architecture, pre-trained on extensive multi-modal
datasets encompassing species occurrence records, remote sensing indicators,
climate and environmental variables. BioAnalyst is designed for adaptability,
allowing for fine-tuning of a range of downstream tasks, such as species
distribution modelling, habitat suitability assessments, invasive species
detection, and population trend forecasting. We evaluate the model's
performance on two downstream use cases, demonstrating its generalisability
compared to existing methods, particularly in data-scarce scenarios for two
distinct use-cases, establishing a new accuracy baseline for ecological
forecasting. By openly releasing BioAnalyst and its fine-tuning workflows to
the scientific community, we aim to foster collaborative efforts in
biodiversity modelling and advance AI-driven solutions to pressing ecological
challenges.

</details>


### [5] [Measuring the Impact of Early-2025 AI on Experienced Open-Source Developer Productivity](https://arxiv.org/abs/2507.09089)
*Joel Becker,Nate Rush,Elizabeth Barnes,David Rein*

Main category: cs.AI

TL;DR: 研究发现，尽管开发者预期AI工具能缩短任务完成时间20%，但实际使用AI工具反而增加了19%的时间，与经济学和ML专家的预测相反。


<details>
  <summary>Details</summary>
Motivation: 探讨AI工具对经验丰富的开源开发者生产力的实际影响，填补现有研究的空白。

Method: 采用随机对照试验（RCT），16名有中等AI经验的开发者完成246项任务，随机分配是否使用2025年的AI工具（如Cursor Pro和Claude 3.5/3.7 Sonnet）。

Result: 使用AI工具导致任务完成时间增加19%，与开发者预期和专家预测相反。

Conclusion: AI工具在当前环境下可能并未提升生产力，甚至可能拖慢开发速度，需进一步研究其实际影响。

Abstract: Despite widespread adoption, the impact of AI tools on software development
in the wild remains understudied. We conduct a randomized controlled trial
(RCT) to understand how AI tools at the February-June 2025 frontier affect the
productivity of experienced open-source developers. 16 developers with moderate
AI experience complete 246 tasks in mature projects on which they have an
average of 5 years of prior experience. Each task is randomly assigned to allow
or disallow usage of early 2025 AI tools. When AI tools are allowed, developers
primarily use Cursor Pro, a popular code editor, and Claude 3.5/3.7 Sonnet.
Before starting tasks, developers forecast that allowing AI will reduce
completion time by 24%. After completing the study, developers estimate that
allowing AI reduced completion time by 20%. Surprisingly, we find that allowing
AI actually increases completion time by 19%--AI tooling slowed developers
down. This slowdown also contradicts predictions from experts in economics (39%
shorter) and ML (38% shorter). To understand this result, we collect and
evaluate evidence for 20 properties of our setting that a priori could
contribute to the observed slowdown effect--for example, the size and quality
standards of projects, or prior developer experience with AI tooling. Although
the influence of experimental artifacts cannot be entirely ruled out, the
robustness of the slowdown effect across our analyses suggests it is unlikely
to primarily be a function of our experimental design.

</details>


### [6] [Hide-and-Shill: A Reinforcement Learning Framework for Market Manipulation Detection in Symphony-a Decentralized Multi-Agent System](https://arxiv.org/abs/2507.09179)
*Ronghua Shi,Yiou Liu,Xinyu Ying,Yang Tan,Yuchun Feng,Lynn Ai,Bill Shi,Xuhui Wang,Zhuang Liu*

Main category: cs.AI

TL;DR: 论文提出了一种基于多智能体强化学习（MARL）的框架，用于检测去中心化金融（DeFi）中的市场操纵行为，通过动态对抗游戏建模操纵者与检测者之间的交互。


<details>
  <summary>Details</summary>
Motivation: DeFi的无许可特性带来了金融创新，但也导致市场操纵行为激增，缺乏中心化监管。

Method: 提出MARL框架，结合GRPO优化学习稳定性，理论驱动的奖励函数，以及多模态智能体管道整合语义、社交图和链上数据。

Result: 在10万次真实场景和对抗模拟中验证，Hide-and-Shill系统在检测准确性和因果归因方面表现优异。

Conclusion: 该工作将多智能体系统与金融监控结合，为去中心化市场情报提供了新范式，所有资源开源以促进研究。

Abstract: Decentralized finance (DeFi) has introduced a new era of permissionless
financial innovation but also led to unprecedented market manipulation. Without
centralized oversight, malicious actors coordinate shilling campaigns and
pump-and-dump schemes across various platforms. We propose a Multi-Agent
Reinforcement Learning (MARL) framework for decentralized manipulation
detection, modeling the interaction between manipulators and detectors as a
dynamic adversarial game. This framework identifies suspicious patterns using
delayed token price reactions as financial indicators.Our method introduces
three innovations: (1) Group Relative Policy Optimization (GRPO) to enhance
learning stability in sparse-reward and partially observable settings; (2) a
theory-based reward function inspired by rational expectations and information
asymmetry, differentiating price discovery from manipulation noise; and (3) a
multi-modal agent pipeline that integrates LLM-based semantic features, social
graph signals, and on-chain market data for informed decision-making.The
framework is integrated within the Symphony system, a decentralized multi-agent
architecture enabling peer-to-peer agent execution and trust-aware learning
through distributed logs, supporting chain-verifiable evaluation. Symphony
promotes adversarial co-evolution among strategic actors and maintains robust
manipulation detection without centralized oracles, enabling real-time
surveillance across global DeFi ecosystems.Trained on 100,000 real-world
discourse episodes and validated in adversarial simulations, Hide-and-Shill
achieves top performance in detection accuracy and causal attribution. This
work bridges multi-agent systems with financial surveillance, advancing a new
paradigm for decentralized market intelligence. All resources are available at
the Hide-and-Shill GitHub repository to promote open research and
reproducibility.

</details>


### [7] [When Developer Aid Becomes Security Debt: A Systematic Analysis of Insecure Behaviors in LLM Coding Agents](https://arxiv.org/abs/2507.09329)
*Matous Kozak,Roshanak Zilouchian Moghaddam,Siva Sivaraman*

Main category: cs.AI

TL;DR: LLM-based coding agents pose security risks, with 21% of actions being insecure. A detection system identified four major vulnerabilities, and mitigation strategies varied in effectiveness, with GPT-4.1 performing best.


<details>
  <summary>Details</summary>
Motivation: The security implications of LLM-based coding agents in software development are poorly understood, despite their growing deployment.

Method: Conducted a systematic security evaluation of five state-of-the-art models on 93 real-world tasks, analyzing over 12,000 actions. Developed a high-precision detection system for vulnerabilities.

Result: 21% of agent actions were insecure, with information exposure being the most common vulnerability. GPT-4.1 showed 96.8% mitigation success.

Conclusion: The study provides a framework for evaluating coding agent security and emphasizes the need for security-aware design in future LLM-based agents.

Abstract: LLM-based coding agents are rapidly being deployed in software development,
yet their security implications remain poorly understood. These agents, while
capable of accelerating software development, may inadvertently introduce
insecure practices. We conducted the first systematic security evaluation of
autonomous coding agents, analyzing over 12,000 actions across five
state-of-the-art models (GPT-4o, GPT-4.1, Claude variants) on 93 real-world
software setup tasks. Our findings reveal significant security concerns: 21% of
agent trajectories contained insecure actions, with models showing substantial
variation in security behavior. We developed a high-precision detection system
that identified four major vulnerability categories, with information exposure
(CWE-200) being the most prevalent one. We also evaluated mitigation strategies
including feedback mechanisms and security reminders with various effectiveness
between models. GPT-4.1 demonstrated exceptional security awareness with 96.8%
mitigation success. Our work provides the first comprehensive framework for
evaluating coding agent security and highlights the need for security-aware
design of next generation LLM-based coding agents.

</details>


### [8] [A Taxonomy of Omnicidal Futures Involving Artificial Intelligence](https://arxiv.org/abs/2507.09369)
*Andrew Critch,Jacob Tsimerman*

Main category: cs.AI

TL;DR: 报告提出了由AI可能导致的人类灭绝事件的分类和示例，旨在通过公开讨论支持预防措施。


<details>
  <summary>Details</summary>
Motivation: 探讨AI可能带来的灾难性风险，以促进公众支持预防措施。

Method: 提出分类和示例，展示AI可能导致的人类灭绝场景。

Result: 明确了潜在的AI灾难性风险，为预防提供依据。

Conclusion: 通过公开讨论AI风险，支持预防措施以避免人类灭绝事件。

Abstract: This report presents a taxonomy and examples of potential omnicidal events
resulting from AI: scenarios where all or almost all humans are killed. These
events are not presented as inevitable, but as possibilities that we can work
to avoid. Insofar as large institutions require a degree of public support in
order to take certain actions, we hope that by presenting these possibilities
in public, we can help to support preventive measures against catastrophic
risks from AI.

</details>


### [9] [EduFlow: Advancing MLLMs' Problem-Solving Proficiency through Multi-Stage, Multi-Perspective Critique](https://arxiv.org/abs/2507.09374)
*Chenglin Zhu,Tao Zhang,Chong Li,Mingan Lin,Zenan Zhou,Jian Xie*

Main category: cs.AI

TL;DR: EduFlow是一个端到端框架，旨在提升多模态大语言模型（MLLMs）在科学任务中的推理能力，通过EduPRM和EduMCTS等技术实现多步推理和自我修正。


<details>
  <summary>Details</summary>
Motivation: 当前MLLMs在科学任务中表现不佳，尤其是在需要多步和可解释推理的场景中，存在推理模式不足、全局连贯性缺失和缺乏自我修正能力等问题。

Method: 提出了EduFlow框架，包括EduPRM（过程感知奖励模型）和EduMCTS（领域适应的搜索框架），通过课程学习和自反思机制优化推理轨迹。

Result: 实验表明EduFlow显著提升了推理的一致性和连贯性，并构建了EduMCTS-160K数据集。

Conclusion: EduFlow为科学教育中的多步推理提供了可靠解决方案，未来将发布代码、数据和模型。

Abstract: Multimodal large language models (MLLMs) still perform poorly on scientific
tasks, particularly those requiring multi-step and interpretable reasoning.
Their limitations include insufficient scientific reasoning patterns, lack of
global coherence in multi-step inference, and the absence of reflective
self-correction, making them unreliable in structured scientific contexts. We
introduce EduFlow, the first end-to-end framework that covers the full pipeline
of educational scientific reasoning, including data selection, MCTS-based
trajectory construction, model training, and output optimization. At its core
is EduPRM, a process-aware reward model that critiques reasoning steps with
tags and justifications. EduPRM is trained via curriculum learning on three
complementary supervision sources: MCTS-guided trajectories, error-injected
critiques, and teacher-student dialogues, enabling dynamic adaptation to
multi-stage problem solving and iterative refinement during inference. We
further propose EduMCTS, a domain-adapted search framework that introduces
bootstrapping actions specifically designed for educational reasoning, such as
a self-reflection mechanism that promotes reflective error correction. It
further leverages EduPRM's fine-grained feedback to guide the search toward
higher-quality reasoning trajectories. By applying self-consistency and
rejection sampling, we constructed EduMCTS-160K, a large-scale dataset of
educational reasoning trajectories. Extensive experiments demonstrate that
EduFlow enhances reasoning consistency and coherence. Code, data, and models
will be released.

</details>


### [10] [Knowledge Conceptualization Impacts RAG Efficacy](https://arxiv.org/abs/2507.09389)
*Chris Davis Jaldi,Anmol Saini,Elham Ghiasi,O. Divine Eziolise,Cogan Shimizu*

Main category: cs.AI

TL;DR: 论文探讨了可解释性和适应性在AI系统中的重要性，特别是针对神经符号AI系统的设计与评估。


<details>
  <summary>Details</summary>
Motivation: 研究如何结合可解释性和适应性，设计可迁移且可解释的神经符号AI系统。

Method: 通过系统评估知识表示的结构和复杂性对LLM查询三元组存储的影响。

Result: 结果显示不同知识表示方法对AI代理的查询效果有显著影响。

Conclusion: 研究强调了知识表示在神经符号AI系统中的重要性，并提出了未来研究方向。

Abstract: Explainability and interpretability are cornerstones of frontier and
next-generation artificial intelligence (AI) systems. This is especially true
in recent systems, such as large language models (LLMs), and more broadly,
generative AI. On the other hand, adaptability to new domains, contexts, or
scenarios is also an important aspect for a successful system. As such, we are
particularly interested in how we can merge these two efforts, that is,
investigating the design of transferable and interpretable neurosymbolic AI
systems. Specifically, we focus on a class of systems referred to as ''Agentic
Retrieval-Augmented Generation'' systems, which actively select, interpret, and
query knowledge sources in response to natural language prompts. In this paper,
we systematically evaluate how different conceptualizations and representations
of knowledge, particularly the structure and complexity, impact an AI agent (in
this case, an LLM) in effectively querying a triplestore. We report our
results, which show that there are impacts from both approaches, and we discuss
their impact and implications.

</details>


### [11] [LLM-Stackelberg Games: Conjectural Reasoning Equilibria and Their Applications to Spearphishing](https://arxiv.org/abs/2507.09407)
*Quanyan Zhu*

Main category: cs.AI

TL;DR: LLM-Stackelberg框架将大语言模型（LLMs）融入领导者与追随者的策略互动中，定义了两类均衡概念，并通过钓鱼案例展示了其认知丰富性和对抗潜力。


<details>
  <summary>Details</summary>
Motivation: 传统Stackelberg博弈假设完全信息和理性代理，而现实中的决策往往涉及信息不对称和有限理性。本文旨在通过LLMs模拟更真实的策略互动。

Method: 提出LLM-Stackelberg博弈框架，定义推理均衡和行为均衡，结合结构化提示和概率行为生成，并通过钓鱼案例验证。

Result: LLM-Stackelberg博弈能有效建模网络安全、错误信息和推荐系统等领域的决策过程。

Conclusion: 该框架为复杂策略互动提供了新范式，尤其在信息不对称和有限理性场景中表现突出。

Abstract: We introduce the framework of LLM-Stackelberg games, a class of sequential
decision-making models that integrate large language models (LLMs) into
strategic interactions between a leader and a follower. Departing from
classical Stackelberg assumptions of complete information and rational agents,
our formulation allows each agent to reason through structured prompts,
generate probabilistic behaviors via LLMs, and adapt their strategies through
internal cognition and belief updates. We define two equilibrium concepts:
reasoning and behavioral equilibrium, which aligns an agent's internal
prompt-based reasoning with observable behavior, and conjectural reasoning
equilibrium, which accounts for epistemic uncertainty through parameterized
models over an opponent's response. These layered constructs capture bounded
rationality, asymmetric information, and meta-cognitive adaptation. We
illustrate the framework through a spearphishing case study, where a sender and
a recipient engage in a deception game using structured reasoning prompts. This
example highlights the cognitive richness and adversarial potential of
LLM-mediated interactions. Our results show that LLM-Stackelberg games provide
a powerful paradigm for modeling decision-making in domains such as
cybersecurity, misinformation, and recommendation systems.

</details>


### [12] [GenAI-based Multi-Agent Reinforcement Learning towards Distributed Agent Intelligence: A Generative-RL Agent Perspective](https://arxiv.org/abs/2507.09495)
*Hang Wang,Junshan Zhang*

Main category: cs.AI

TL;DR: 论文提出从被动到主动的多智能体强化学习范式转变，通过生成式AI实现前瞻性决策和协作。


<details>
  <summary>Details</summary>
Motivation: 传统方法在多智能体强化学习中面临指数级增长的动作空间、非平稳环境和部分可观测性等挑战，无法应对新场景。

Method: 采用生成式AI的强化学习，将智能体视为生成模型，预测未来交互并生成协调动作序列。

Result: 生成式强化学习智能体能够建模环境演化、预测其他智能体行为，实现前瞻性决策和动态适应。

Conclusion: 这一范式转变有望解决传统框架下难以处理的协作问题，推动分布式智能和集体协作行为的发展。

Abstract: Multi-agent reinforcement learning faces fundamental challenges that
conventional approaches have failed to overcome: exponentially growing joint
action spaces, non-stationary environments where simultaneous learning creates
moving targets, and partial observability that constrains coordination. Current
methods remain reactive, employing stimulus-response mechanisms that fail when
facing novel scenarios. We argue for a transformative paradigm shift from
reactive to proactive multi-agent intelligence through generative AI-based
reinforcement learning. This position advocates reconceptualizing agents not as
isolated policy optimizers, but as sophisticated generative models capable of
synthesizing complex multi-agent dynamics and making anticipatory decisions
based on predictive understanding of future interactions. Rather than
responding to immediate observations, generative-RL agents can model
environment evolution, predict other agents' behaviors, generate coordinated
action sequences, and engage in strategic reasoning accounting for long-term
dynamics. This approach leverages pattern recognition and generation
capabilities of generative AI to enable proactive decision-making, seamless
coordination through enhanced communication, and dynamic adaptation to evolving
scenarios. We envision this paradigm shift will unlock unprecedented
possibilities for distributed intelligence, moving beyond individual
optimization toward emergent collective behaviors representing genuine
collaborative intelligence. The implications extend across autonomous systems,
robotics, and human-AI collaboration, promising solutions to coordination
challenges intractable under traditional reactive frameworks.

</details>


### [13] [Consistency Trajectory Planning: High-Quality and Efficient Trajectory Optimization for Offline Model-Based Reinforcement Learning](https://arxiv.org/abs/2507.09534)
*Guanquan Wang,Takuya Hiraoka,Yoshimasa Tsuruoka*

Main category: cs.AI

TL;DR: CTP是一种基于一致性轨迹模型的离线强化学习方法，通过单步轨迹生成实现高效优化，显著降低计算成本，并在D4RL基准测试中表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有扩散模型在规划中计算成本高，CTP旨在通过单步生成解决这一问题。

Method: 利用一致性轨迹模型（CTM）进行快速单步轨迹生成，避免迭代采样。

Result: 在D4RL基准测试中，CTP性能优于现有扩散方法，且推理速度快120倍。

Conclusion: CTP是一种高效、低延迟的离线规划方法，适用于高性能任务。

Abstract: This paper introduces Consistency Trajectory Planning (CTP), a novel offline
model-based reinforcement learning method that leverages the recently proposed
Consistency Trajectory Model (CTM) for efficient trajectory optimization. While
prior work applying diffusion models to planning has demonstrated strong
performance, it often suffers from high computational costs due to iterative
sampling procedures. CTP supports fast, single-step trajectory generation
without significant degradation in policy quality. We evaluate CTP on the D4RL
benchmark and show that it consistently outperforms existing diffusion-based
planning methods in long-horizon, goal-conditioned tasks. Notably, CTP achieves
higher normalized returns while using significantly fewer denoising steps. In
particular, CTP achieves comparable performance with over $120\times$ speedup
in inference time, demonstrating its practicality and effectiveness for
high-performance, low-latency offline planning.

</details>


### [14] [Learning to Control Dynamical Agents via Spiking Neural Networks and Metropolis-Hastings Sampling](https://arxiv.org/abs/2507.09540)
*Ali Safa,Farida Mohsen,Ali Al-Zawqari*

Main category: cs.AI

TL;DR: 提出了一种基于Metropolis-Hastings采样的框架，用于训练脉冲神经网络（SNNs）进行强化学习任务，避免了梯度方法的局限性。


<details>
  <summary>Details</summary>
Motivation: 传统深度神经网络（DNNs）在实时控制系统中能耗较高，而SNNs虽然能效高，但其训练因脉冲通信的不可微分性而面临挑战。

Method: 采用Metropolis-Hastings采样技术，通过迭代提出并概率性地接受网络参数更新，基于累积奖励信号进行训练。

Result: 在AcroBot和CartPole两个标准控制基准上，该方法在最大化累积奖励的同时，减少了网络资源和训练次数，表现优于传统深度Q学习和先前SNN方法。

Conclusion: 该框架为SNNs在强化学习中的应用提供了一种高效且无需梯度的方法，展示了在神经形态平台上的直接优化潜力。

Abstract: Spiking Neural Networks (SNNs) offer biologically inspired, energy-efficient
alternatives to traditional Deep Neural Networks (DNNs) for real-time control
systems. However, their training presents several challenges, particularly for
reinforcement learning (RL) tasks, due to the non-differentiable nature of
spike-based communication. In this work, we introduce what is, to our
knowledge, the first framework that employs Metropolis-Hastings (MH) sampling,
a Bayesian inference technique, to train SNNs for dynamical agent control in RL
environments without relying on gradient-based methods. Our approach
iteratively proposes and probabilistically accepts network parameter updates
based on accumulated reward signals, effectively circumventing the limitations
of backpropagation while enabling direct optimization on neuromorphic
platforms. We evaluated this framework on two standard control benchmarks:
AcroBot and CartPole. The results demonstrate that our MH-based approach
outperforms conventional Deep Q-Learning (DQL) baselines and prior SNN-based RL
approaches in terms of maximizing the accumulated reward while minimizing
network resources and training episodes.

</details>


### [15] [eSapiens: A Platform for Secure and Auditable Retrieval-Augmented Generation](https://arxiv.org/abs/2507.09588)
*Isaac Shi,Zeyuan Li,Fan Liu,Wenli Wang,Lewei He,Yang Yang,Tianyu Shi*

Main category: cs.AI

TL;DR: eSapiens是一个面向企业的AIaaS平台，整合了专有数据、工作流程和主流LLM，提供数据安全和自动化任务支持。


<details>
  <summary>Details</summary>
Motivation: 解决企业在AI应用中面临的数据安全、知识保留和自动化需求，提升业务效率。

Method: 结合结构化文档处理、混合向量检索和无代码编排（如LangChain），支持多种LLM，并引入THOR Agent处理SQL查询。

Result: 实验显示，512令牌分块检索精度最高（Top-3准确率91.3%），生成质量在TRACe指标下提升23%。

Conclusion: eSapiens在高风险领域（如法律和金融）中实现了可信、可审计的AI工作流程。

Abstract: We present eSapiens, an AI-as-a-Service (AIaaS) platform engineered around a
business-oriented trifecta: proprietary data, operational workflows, and any
major agnostic Large Language Model (LLM). eSapiens gives businesses full
control over their AI assets, keeping everything in-house for AI knowledge
retention and data security. eSapiens AI Agents (Sapiens) empower your team by
providing valuable insights and automating repetitive tasks, enabling them to
focus on high-impact work and drive better business outcomes.
  The system integrates structured document ingestion, hybrid vector retrieval,
and no-code orchestration via LangChain, and supports top LLMs including
OpenAI, Claude, Gemini, and DeepSeek. A key component is the THOR Agent, which
handles structured SQL-style queries and generates actionable insights over
enterprise databases.
  To evaluate the system, we conduct two experiments. First, a retrieval
benchmark on legal corpora reveals that a chunk size of 512 tokens yields the
highest retrieval precision (Top-3 accuracy: 91.3%). Second, a generation
quality test using TRACe metrics across five LLMs shows that eSapiens delivers
more context-consistent outputs with up to a 23% improvement in factual
alignment.
  These results demonstrate the effectiveness of eSapiens in enabling
trustworthy, auditable AI workflows for high-stakes domains like legal and
finance.

</details>


### [16] [The Hidden Costs of AI: A Review of Energy, E-Waste, and Inequality in Model Development](https://arxiv.org/abs/2507.09611)
*Jenis Winsta*

Main category: cs.AI

TL;DR: AI发展迅速，但带来环境和伦理挑战，包括能源消耗、电子废物、计算资源不平等和网络安全能耗。呼吁可持续、透明和公平的AI发展。


<details>
  <summary>Details</summary>
Motivation: 探讨AI快速发展中被忽视的环境和伦理问题，推动负责任AI的发展。

Method: 通过分析近期研究和机构报告，系统梳理AI在能源、电子废物、计算资源分配和网络安全方面的负面影响。

Result: 揭示了AI训练的高排放、硬件快速淘汰、全球基础设施不平等及网络安全能耗等问题。

Conclusion: AI发展需结合伦理责任和环境保护，以实现可持续和包容的技术未来。

Abstract: Artificial intelligence (AI) has made remarkable progress in recent years,
yet its rapid expansion brings overlooked environmental and ethical challenges.
This review explores four critical areas where AI's impact extends beyond
performance: energy consumption, electronic waste (e-waste), inequality in
compute access, and the hidden energy burden of cybersecurity systems. Drawing
from recent studies and institutional reports, the paper highlights systemic
issues such as high emissions from model training, rising hardware turnover,
global infrastructure disparities, and the energy demands of securing AI. By
connecting these concerns, the review contributes to Responsible AI discourse
by identifying key research gaps and advocating for sustainable, transparent,
and equitable development practices. Ultimately, it argues that AI's progress
must align with ethical responsibility and environmental stewardship to ensure
a more inclusive and sustainable technological future.

</details>


### [17] [Bridging Bots: from Perception to Action via Multimodal-LMs and Knowledge Graphs](https://arxiv.org/abs/2507.09617)
*Margherita Martorana,Francesca Urgese,Mark Adamik,Ilaria Tiddi*

Main category: cs.AI

TL;DR: 提出了一种结合多模态语言模型与知识图谱的神经符号框架，以提升服务机器人在动态环境中的互操作性和适应性。


<details>
  <summary>Details</summary>
Motivation: 当前服务机器人系统依赖专有硬件和软件，难以跨平台扩展。知识图谱和多模态语言模型各有优劣，但单独使用无法满足需求。

Method: 结合多模态语言模型的感知能力与知识图谱的结构化表示，生成符合本体论的知识图谱，支持平台无关的机器人行为。

Result: 评估了五种多模态模型（三种LLaMA和两种GPT），发现GPT-o1和LLaMA 4 Maverick表现最佳，但模型新旧并非性能的决定因素。

Conclusion: 神经符号框架有效提升了机器人系统的互操作性，但集成策略对生成符合本体论的知识图谱至关重要。

Abstract: Personal service robots are deployed to support daily living in domestic
environments, particularly for elderly and individuals requiring assistance.
These robots must perceive complex and dynamic surroundings, understand tasks,
and execute context-appropriate actions. However, current systems rely on
proprietary, hard-coded solutions tied to specific hardware and software,
resulting in siloed implementations that are difficult to adapt and scale
across platforms. Ontologies and Knowledge Graphs (KGs) offer a solution to
enable interoperability across systems, through structured and standardized
representations of knowledge and reasoning. However, symbolic systems such as
KGs and ontologies struggle with raw and noisy sensory input. In contrast,
multimodal language models are well suited for interpreting input such as
images and natural language, but often lack transparency, consistency, and
knowledge grounding. In this work, we propose a neurosymbolic framework that
combines the perceptual strengths of multimodal language models with the
structured representations provided by KGs and ontologies, with the aim of
supporting interoperability in robotic applications. Our approach generates
ontology-compliant KGs that can inform robot behavior in a platform-independent
manner. We evaluated this framework by integrating robot perception data,
ontologies, and five multimodal models (three LLaMA and two GPT models), using
different modes of neural-symbolic interaction. We assess the consistency and
effectiveness of the generated KGs across multiple runs and configurations, and
perform statistical analyzes to evaluate performance. Results show that GPT-o1
and LLaMA 4 Maverick consistently outperform other models. However, our
findings also indicate that newer models do not guarantee better results,
highlighting the critical role of the integration strategy in generating
ontology-compliant KGs.

</details>


### [18] [humancompatible.interconnect: Testing Properties of Repeated Uses of Interconnections of AI Systems](https://arxiv.org/abs/2507.09626)
*Rodion Nazarov,Anthony Quinn,Robert Shorten,Jakub Marecek*

Main category: cs.AI

TL;DR: 介绍了一个基于PyTorch的工具包，用于通过随机控制技术建模多代理系统中AI系统的互连及其重复使用特性，提供公平性和鲁棒性的闭环保证。


<details>
  <summary>Details</summary>
Motivation: 多代理AI系统需要满足公平性和鲁棒性的先验保证，但现有方法在建模和提供这些保证时存在复杂性。

Method: 开发了一个基于PyTorch的开源工具包，利用随机控制技术建模AI系统与代理的互连，并以闭环方式验证公平性和鲁棒性。

Result: 该工具包简化了为多代理系统提供公平性和鲁棒性先验保证的复杂性。

Conclusion: 该工具包为多代理AI系统的公平性和鲁棒性提供了有效的闭环建模和验证方法。

Abstract: Artificial intelligence (AI) systems often interact with multiple agents. The
regulation of such AI systems often requires that {\em a priori\/} guarantees
of fairness and robustness be satisfied. With stochastic models of agents'
responses to the outputs of AI systems, such {\em a priori\/} guarantees
require non-trivial reasoning about the corresponding stochastic systems. Here,
we present an open-source PyTorch-based toolkit for the use of stochastic
control techniques in modelling interconnections of AI systems and properties
of their repeated uses. It models robustness and fairness desiderata in a
closed-loop fashion, and provides {\em a priori\/} guarantees for these
interconnections. The PyTorch-based toolkit removes much of the complexity
associated with the provision of fairness guarantees for closed-loop models of
multi-agent systems.

</details>


### [19] [Improving monotonic optimization in heterogeneous multi-agent reinforcement learning with optimal marginal deterministic policy gradient](https://arxiv.org/abs/2507.09989)
*Xiaoyang Yu,Youfang Lin,Shuo Wang,Sheng Han*

Main category: cs.AI

TL;DR: OMDPG算法通过引入最优边际Q函数和广义Q批评器，解决了异构多智能体强化学习中单调改进与部分参数共享的冲突，实验证明其优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 异构多智能体强化学习（MARL）中，单调改进对性能提升至关重要，但部分参数共享（ParPS）与现有方法（如HAPPO）存在冲突，导致策略更新基线漂移问题。

Method: 提出OMDPG算法：1）用最优边际Q函数替代顺序计算的Q函数；2）引入广义Q批评器；3）采用集中式批评器分组执行器架构（CCGA）。

Result: 在SMAC和MAMuJoCo环境中，OMDPG优于多种最先进的MARL基线方法。

Conclusion: OMDPG成功解决了单调改进与ParPS的冲突，提升了异构MARL的性能。

Abstract: In heterogeneous multi-agent reinforcement learning (MARL), achieving
monotonic improvement plays a pivotal role in enhancing performance. The HAPPO
algorithm proposes a feasible solution by introducing a sequential update
scheme, which requires independent learning with No Parameter-sharing (NoPS).
However, heterogeneous MARL generally requires Partial Parameter-sharing
(ParPS) based on agent grouping to achieve high cooperative performance. Our
experiments prove that directly combining ParPS with the sequential update
scheme leads to the policy updating baseline drift problem, thereby failing to
achieve improvement. To solve the conflict between monotonic improvement and
ParPS, we propose the Optimal Marginal Deterministic Policy Gradient (OMDPG)
algorithm. First, we replace the sequentially computed $Q_{\psi}^s(s,a_{1:i})$
with the Optimal Marginal Q (OMQ) function $\phi_{\psi}^*(s,a_{1:i})$ derived
from Q-functions. This maintains MAAD's monotonic improvement while eliminating
the conflict through optimal joint action sequences instead of sequential
policy ratio calculations. Second, we introduce the Generalized Q Critic (GQC)
as the critic function, employing pessimistic uncertainty-constrained loss to
optimize different Q-value estimations. This provides the required Q-values for
OMQ computation and stable baselines for actor updates. Finally, we implement a
Centralized Critic Grouped Actor (CCGA) architecture that simultaneously
achieves ParPS in local policy networks and accurate global Q-function
computation. Experimental results in SMAC and MAMuJoCo environments demonstrate
that OMDPG outperforms various state-of-the-art MARL baselines.

</details>


### [20] [Towards Concise and Adaptive Thinking in Large Reasoning Models: A Survey](https://arxiv.org/abs/2507.09662)
*Jason Zhu,Hongyu Li*

Main category: cs.AI

TL;DR: 大型推理模型（LRMs）在复杂任务上表现优异，但存在推理链冗长的问题，导致资源浪费和响应延迟。本文综述了简洁和自适应推理的研究进展。


<details>
  <summary>Details</summary>
Motivation: 解决LRMs生成冗余推理链的问题，以提高效率和实用性。

Method: 综述了近期关于简洁和自适应推理的方法、基准和挑战。

Result: 总结了该领域的研究进展，为未来探索提供方向。

Conclusion: 希望帮助研究者快速了解该领域，并启发新的自适应推理方法。

Abstract: Large reasoning models (LRMs) like OpenAI o1 and DeepSeek R1 have
demonstrated impressive performance on complex reasoning tasks like mathematics
and programming with long Chain-of-Thought (CoT) reasoning sequences
(slow-thinking), compared with traditional large language models
(fast-thinking). However, these reasoning models also face a huge challenge
that generating unnecessarily lengthy and redundant reasoning chains even for
trivial questions. This phenomenon leads to a significant waste of inference
resources, increases the response time for simple queries, and hinders the
practical application of LRMs in real-world products. To this end, it is
crucial to shorten lengthy reasoning chains and learn adaptive reasoning
between fast and slow thinking based on input difficulty. In this survey, we
provide a comprehensive overview of recent progress in concise and adaptive
thinking for efficient reasoning of LRMs, including methodologies, benchmarks,
and challenges for future exploration. We hope this survey can help researchers
quickly understand the landscape of this field and inspire novel adaptive
thinking ideas to facilitate better usage of LRMs.

</details>


### [21] [Adaptability in Multi-Agent Reinforcement Learning: A Framework and Unified Review](https://arxiv.org/abs/2507.10142)
*Siyi Hu,Mohamad A Hady,Jianglin Qiao,Jimmy Cao,Mahardhika Pratama,Ryszard Kowalczyk*

Main category: cs.AI

TL;DR: 论文提出“适应性”概念，作为评估多智能体强化学习（MARL）在动态环境中可靠性的统一框架，并提出了三个关键维度。


<details>
  <summary>Details</summary>
Motivation: 现实世界的多智能体系统（MAS）复杂且动态，现有MARL算法在动态环境中的部署受限，需要更好的适应性评估方法。

Method: 提出适应性框架，包括学习适应性、策略适应性和场景驱动适应性三个维度，以评估MARL在动态条件下的表现。

Result: 通过适应性视角，支持更原则性的MARL性能评估，超越狭窄定义的基准测试。

Conclusion: 该框架有助于开发更适合动态现实世界多智能体系统的MARL算法。

Abstract: Multi-Agent Reinforcement Learning (MARL) has shown clear effectiveness in
coordinating multiple agents across simulated benchmarks and constrained
scenarios. However, its deployment in real-world multi-agent systems (MAS)
remains limited, primarily due to the complex and dynamic nature of such
environments. These challenges arise from multiple interacting sources of
variability, including fluctuating agent populations, evolving task goals, and
inconsistent execution conditions. Together, these factors demand that MARL
algorithms remain effective under continuously changing system configurations
and operational demands. To better capture and assess this capacity for
adjustment, we introduce the concept of \textit{adaptability} as a unified and
practically grounded lens through which to evaluate the reliability of MARL
algorithms under shifting conditions, broadly referring to any changes in the
environment dynamics that may occur during learning or execution. Centred on
the notion of adaptability, we propose a structured framework comprising three
key dimensions: learning adaptability, policy adaptability, and scenario-driven
adaptability. By adopting this adaptability perspective, we aim to support more
principled assessments of MARL performance beyond narrowly defined benchmarks.
Ultimately, this survey contributes to the development of algorithms that are
better suited for deployment in dynamic, real-world multi-agent systems.

</details>


### [22] [Causality-informed Anomaly Detection in Partially Observable Sensor Networks: Moving beyond Correlations](https://arxiv.org/abs/2507.09742)
*Xiaofeng Xiao,Bo Shen,Xubo Yue*

Main category: cs.AI

TL;DR: 提出了一种基于因果关系的深度Q网络（Causal DQ）方法，用于部分可观测的传感器布局优化，以更快检测异常。


<details>
  <summary>Details</summary>
Motivation: AI驱动的制造业中，数据流实时监测需求增长，但资源有限，需开发最优传感器布局策略。现有方法多忽略因果关系或依赖不切实际的干预。

Method: 通过将因果信息集成到Q网络训练的每个阶段，提出Causal DQ方法，实现更快收敛和更紧的理论误差界限。

Result: Causal DQ显著减少了异常检测时间，适用于大规模实时数据流。

Conclusion: 该方法不仅适用于传感器布局，还可推广至其他强化学习问题，为工程应用中的因果机器学习开辟新可能。

Abstract: Nowadays, as AI-driven manufacturing becomes increasingly popular, the volume
of data streams requiring real-time monitoring continues to grow. However, due
to limited resources, it is impractical to place sensors at every location to
detect unexpected shifts. Therefore, it is necessary to develop an optimal
sensor placement strategy that enables partial observability of the system
while detecting anomalies as quickly as possible. Numerous approaches have been
proposed to address this challenge; however, most existing methods consider
only variable correlations and neglect a crucial factor: Causality. Moreover,
although a few techniques incorporate causal analysis, they rely on
interventions-artificially creating anomalies-to identify causal effects, which
is impractical and might lead to catastrophic losses. In this paper, we
introduce a causality-informed deep Q-network (Causal DQ) approach for
partially observable sensor placement in anomaly detection. By integrating
causal information at each stage of Q-network training, our method achieves
faster convergence and tighter theoretical error bounds. Furthermore, the
trained causal-informed Q-network significantly reduces the detection time for
anomalies under various settings, demonstrating its effectiveness for sensor
placement in large-scale, real-world data streams. Beyond the current
implementation, our technique's fundamental insights can be applied to various
reinforcement learning problems, opening up new possibilities for real-world
causality-informed machine learning methods in engineering applications.

</details>


### [23] [Sound and Complete Neuro-symbolic Reasoning with LLM-Grounded Interpretations](https://arxiv.org/abs/2507.09751)
*Bradley P. Allen,Prateek Chhikara,Thomas Macaulay Ferguson,Filip Ilievski,Paul Groth*

Main category: cs.AI

TL;DR: 提出一种将大语言模型（LLM）整合到形式语义解释函数中的方法，以解决其逻辑不一致性问题，同时保留逻辑的完备性和可靠性。


<details>
  <summary>Details</summary>
Motivation: 尽管LLM在自然语言理解和生成方面表现出色，但其输出存在逻辑不一致问题，需要一种方法在形式推理中利用其广泛的知识。

Method: 将LLM直接整合到形式语义的解释函数中，用于一种次协调逻辑。

Result: 通过实验验证了该方法的可行性，使用多个事实性基准数据集进行评估。

Conclusion: 该方法为神经符号推理提供了理论框架，既能利用LLM的知识，又能保持逻辑的完备性和可靠性。

Abstract: Large language models (LLMs) have demonstrated impressive capabilities in
natural language understanding and generation, but they exhibit problems with
logical consistency in the output they generate. How can we harness LLMs'
broad-coverage parametric knowledge in formal reasoning despite their
inconsistency? We present a method for directly integrating an LLM into the
interpretation function of the formal semantics for a paraconsistent logic. We
provide experimental evidence for the feasibility of the method by evaluating
the function using datasets created from several short-form factuality
benchmarks. Unlike prior work, our method offers a theoretical framework for
neuro-symbolic reasoning that leverages an LLM's knowledge while preserving the
underlying logic's soundness and completeness properties.

</details>


### [24] [DeepResearch$^{\text{Eco}}$: A Recursive Agentic Workflow for Complex Scientific Question Answering in Ecology](https://arxiv.org/abs/2507.10522)
*Jennifer D'Souza,Endres Keno Sander,Andrei Aioanei*

Main category: cs.AI

TL;DR: DeepResearch$^{\text{Eco}}$是一种基于LLM的自动科学合成系统，支持递归、深度和广度可控的探索，显著提升文献检索的多样性和细致性。


<details>
  <summary>Details</summary>
Motivation: 传统检索增强生成方法缺乏用户可控性和透明性，DeepResearch旨在解决这一问题，提供可配置的高通量科学合成工具。

Method: 通过递归、深度和广度可控的探索，结合透明推理和参数驱动配置，实现科学文献的高效整合。

Result: 在49个生态研究问题中，DeepResearch实现了21倍的源整合提升和14.9倍的每千字源整合增长，高参数设置下达到专家级分析深度。

Conclusion: DeepResearch$^{\text{Eco}}$在科学文献合成中表现出色，提供了高效、可控且透明的解决方案。

Abstract: We introduce DeepResearch$^{\text{Eco}}$, a novel agentic LLM-based system
for automated scientific synthesis that supports recursive, depth- and
breadth-controlled exploration of original research questions -- enhancing
search diversity and nuance in the retrieval of relevant scientific literature.
Unlike conventional retrieval-augmented generation pipelines, DeepResearch
enables user-controllable synthesis with transparent reasoning and
parameter-driven configurability, facilitating high-throughput integration of
domain-specific evidence while maintaining analytical rigor. Applied to 49
ecological research questions, DeepResearch achieves up to a 21-fold increase
in source integration and a 14.9-fold rise in sources integrated per 1,000
words. High-parameter settings yield expert-level analytical depth and
contextual diversity.
  Source code available at: https://github.com/sciknoworg/deep-research.

</details>


### [25] [Technical Requirements for Halting Dangerous AI Activities](https://arxiv.org/abs/2507.09801)
*Peter Barnett,Aaron Scher,David Abecassis*

Main category: cs.AI

TL;DR: 论文探讨了AI快速发展带来的风险，并提出了通过技术干预实现协调暂停危险AI活动的方案。


<details>
  <summary>Details</summary>
Motivation: AI的快速发展可能导致失控、滥用、地缘政治不稳定和权力集中等前所未有的风险，需要政府采取措施避免最坏结果。

Method: 提出了关键的技术干预措施，用于协调暂停危险的AI开发和部署活动。

Result: 这些技术干预措施可以限制多种危险AI活动，并为潜在的AI治理计划提供技术基础。

Conclusion: 通过技术干预实现协调暂停是应对AI风险的有效途径，为AI治理提供了可行的技术方案。

Abstract: The rapid development of AI systems poses unprecedented risks, including loss
of control, misuse, geopolitical instability, and concentration of power. To
navigate these risks and avoid worst-case outcomes, governments may proactively
establish the capability for a coordinated halt on dangerous AI development and
deployment. In this paper, we outline key technical interventions that could
allow for a coordinated halt on dangerous AI activities. We discuss how these
interventions may contribute to restricting various dangerous AI activities,
and show how these interventions can form the technical foundation for
potential AI governance plans.

</details>


### [26] [Is Human-Written Data Enough? The Challenge of Teaching Reasoning to LLMs Without RL or Distillation](https://arxiv.org/abs/2507.09850)
*Wei Du,Branislav Kisacanin,George Armstrong,Shubham Toshniwal,Ivan Moshkov,Alexan Ayrapetyan,Sadegh Mahdavi,Dan Zhao,Shizhe Diao,Dragan Masulovic,Marius Stanean,Advaith Avadhanam,Max Wang,Ashmit Dutta,Shitij Govil,Sri Yanamandara,Mihir Tandon,Sriram Ananthakrishnan,Vedant Rathi,David Zhang,Joonseok Kang,Leon Luo,Titu Andreescu,Boris Ginsburg,Igor Gitman*

Main category: cs.AI

TL;DR: 通过少量高质量长链思维（CoT）示例微调基础模型，可以显著提升其推理能力，优于更大规模的模型。


<details>
  <summary>Details</summary>
Motivation: 探究是否仅通过提示或最小微调即可在基础模型中诱导长链思维推理能力。

Method: 使用20个来自推理模型的长链思维示例微调基础模型，并探索非推理模型和人工标注的数据。

Result: 微调后的模型表现优于更大规模的模型，但非推理模型和人工标注的数据效果不及推理模型。

Conclusion: 少量高质量推理数据可激活基础模型的推理能力，但专家级推理数据的某些特质难以复制。

Abstract: Reasoning-capable language models achieve state-of-the-art performance in
diverse complex tasks by generating long, explicit Chain-of-Thought (CoT)
traces. While recent works show that base models can acquire such reasoning
traces via reinforcement learning or distillation from stronger models like
DeepSeek-R1, previous works demonstrate that even short CoT prompting without
fine-tuning is able to improve reasoning. We ask whether long CoT can be
induced in a base model using only prompting or minimal tuning. Using just 20
long CoT examples from the reasoning model \texttt{QwQ-32B-Preview}, we lightly
fine-tune the base model \texttt{Qwen2.5-32B}. The resulting model outperforms
the much larger \texttt{Qwen2.5-Math-72B-Instruct}, showing that a handful of
high-quality examples can unlock strong reasoning capabilities. We further
explore using CoT data from non-reasoning models and human annotators, enhanced
with prompt engineering, multi-pass editing, and structural guidance. However,
neither matches the performance of reasoning model traces, suggesting that
certain latent qualities of expert CoT are difficult to replicate. We analyze
key properties of reasoning data, such as problem difficulty, diversity, and
answer length, that influence reasoning distillation. While challenges remain,
we are optimistic that carefully curated human-written CoT, even in small
quantities, can activate reasoning behaviors in base models. We release our
human-authored dataset across refinement stages and invite further
investigation into what makes small-scale reasoning supervision so effective.

</details>


### [27] [Model-Grounded Symbolic Artificial Intelligence Systems Learning and Reasoning with Model-Grounded Symbolic Artificial Intelligence Systems](https://arxiv.org/abs/2507.09854)
*Aniruddha Chattopadhyay,Raj Dandekar,Kaushik Roy*

Main category: cs.AI

TL;DR: 论文提出将指令调优的大型语言模型重新解释为基于模型的符号AI系统，利用自然语言作为符号层，并通过模型的内部表示空间实现接地。


<details>
  <summary>Details</summary>
Motivation: 结合神经网络和符号AI的互补优势，探索如何通过自然语言实现符号AI的接地。

Method: 将大型语言模型重新解释为符号AI系统，开发新的学习和推理方法，保持与传统范式的结构相似性。

Result: 初步评估表明，该方法在提高学习效率和推理可靠性方面有效。

Conclusion: 该框架为神经符号AI系统提供了一种新的实现方式，具有潜在的应用价值。

Abstract: Neurosymbolic artificial intelligence (AI) systems combine neural network and
classical symbolic AI mechanisms to exploit the complementary strengths of
large scale, generalizable learning and robust, verifiable reasoning. Numerous
classifications of neurosymbolic AI illustrate how these two components can be
integrated in distinctly different ways. In this work, we propose
reinterpreting instruction tuned large language models as model grounded
symbolic AI systems where natural language serves as the symbolic layer and
grounding is achieved through the models internal representation space. Within
this framework, we investigate and develop novel learning and reasoning
approaches that preserve structural similarities to traditional learning and
reasoning paradigms. Preliminary evaluations across axiomatic deductive
reasoning procedures of varying complexity provide insights into the
effectiveness of our approach in improving learning efficiency and reasoning
reliability.

</details>


### [28] [VerifyBench: A Systematic Benchmark for Evaluating Reasoning Verifiers Across Domains](https://arxiv.org/abs/2507.09884)
*Xuzhao Li,Xuchen Li,Shiyu Hu,Yongzhen Guo,Wentao Zhang*

Main category: cs.AI

TL;DR: 论文提出了VerifyBench，一个跨领域基准测试，用于系统评估验证器的性能，揭示了专用验证器和通用LLM在精度与召回率之间的权衡及其跨领域泛化的局限性。


<details>
  <summary>Details</summary>
Motivation: 当前验证器在验证大型语言模型生成的复杂、多样且细微的回答时面临挑战，缺乏对不同类型验证器性能的系统评估，限制了RLVR的可靠发展。

Method: 构建包含4,000个专家级问题的VerifyBench，覆盖数学、物理、化学和生物领域，设计四维实验框架比较专用验证器和通用LLM的性能。

Result: 专用验证器精度高但召回率低，通用模型包容性强但精度不稳定；验证器对输入结构敏感且跨领域泛化能力有限。

Conclusion: 研究揭示了当前验证器技术的瓶颈，为未来改进提供了关键见解。

Abstract: Large language models (LLMs) increasingly rely on reinforcement learning (RL)
to enhance their reasoning capabilities through feedback. A critical challenge
is verifying the consistency of model-generated responses and reference
answers, since these responses are often lengthy, diverse, and nuanced.
Rule-based verifiers struggle with complexity, prompting the use of model-based
verifiers. However, specialized verifiers lack flexibility, while general LLM
judges can be inconsistent. Existing research primarily focuses on building
better verifiers, yet a systematic evaluation of different types of verifiers'
performance across domains remains lacking, severely constraining the reliable
development of Reinforcement Learning with Verifiable Reward (RLVR). To address
this, we propose VerifyBench--a cross-domain comprehensive benchmark for
systematically evaluating verifiers. We construct 4,000 expert-level questions
covering mathematics, physics, chemistry, and biology. Each question is
equipped with reference answers and diverse responses. The reliability of the
evaluation is ensured through a rigorous annotation process conducted by a
multidisciplinary expert team. We design a four-dimensional experimental
framework to comprehensively compare the performance boundaries of specialized
verifiers and general LLMs under combined conditions of extracted answers vs.
complete responses, and short vs. long outputs. Our evaluation uncovers
fundamental trade-offs in verifiers: while specialized verifiers achieve
leading accuracy, they exhibit deficiencies in recall; general models show
stronger inclusivity but unstable precision. More importantly, we discover
verifiers' high sensitivity to input structure and inherent limitations in
cross-domain generalization, providing critical insights into the bottlenecks
of current verifier technology.

</details>


### [29] [DeepSeek: Paradigm Shifts and Technical Evolution in Large AI Models](https://arxiv.org/abs/2507.09955)
*Luolin Xiong,Haofen Wang,Xi Chen,Lu Sheng,Yun Xiong,Jingping Liu,Yanghua Xiao,Huajun Chen,Qing-Long Han,Yang Tang*

Main category: cs.AI

TL;DR: DeepSeek发布的V3和R1系列模型因其低成本、高性能和开源优势引起全球关注。论文回顾了大模型的发展，重点介绍了DeepSeek的创新算法和工程突破，并分析了其对AI竞争格局的影响。


<details>
  <summary>Details</summary>
Motivation: 探讨DeepSeek模型的技术创新及其对AI领域的影响，为大模型的未来发展提供见解。

Method: 回顾大模型发展历程，介绍DeepSeek的MLA、MoE、MTP和GRPO等新算法，分析其工程突破和系统优化。

Result: DeepSeek模型在性能和成本上具有竞争力，推动了AI技术的发展。

Conclusion: DeepSeek的创新为大型AI模型的未来发展提供了重要启示，特别是在数据、训练和推理方面。

Abstract: DeepSeek, a Chinese Artificial Intelligence (AI) startup, has released their
V3 and R1 series models, which attracted global attention due to their low
cost, high performance, and open-source advantages. This paper begins by
reviewing the evolution of large AI models focusing on paradigm shifts, the
mainstream Large Language Model (LLM) paradigm, and the DeepSeek paradigm.
Subsequently, the paper highlights novel algorithms introduced by DeepSeek,
including Multi-head Latent Attention (MLA), Mixture-of-Experts (MoE),
Multi-Token Prediction (MTP), and Group Relative Policy Optimization (GRPO).
The paper then explores DeepSeek engineering breakthroughs in LLM scaling,
training, inference, and system-level optimization architecture. Moreover, the
impact of DeepSeek models on the competitive AI landscape is analyzed,
comparing them to mainstream LLMs across various fields. Finally, the paper
reflects on the insights gained from DeepSeek innovations and discusses future
trends in the technical and engineering development of large AI models,
particularly in data, training, and reasoning.

</details>


### [30] [On The Role of Intentionality in Knowledge Representation: Analyzing Scene Context for Cognitive Agents with a Tiny Language Model](https://arxiv.org/abs/2507.10000)
*Mark Burgess*

Main category: cs.AI

TL;DR: 论文提出了一种基于Promise Theory的语义时空模型，通过多尺度异常检测和时空一致性，低成本地评估数据中的潜在意图性。


<details>
  <summary>Details</summary>
Motivation: 探讨意图和意图性在科学与技术中的实际意义，尤其是在缺乏语言知识的情况下如何识别文本主题和概念。

Method: 利用过程一致性作为指导，通过多尺度异常检测和时空一致性分离意图内容和环境背景。

Result: 提供了一种低成本、无需大规模训练或推理能力的潜在意图性解释方法，适用于基础生物体。

Conclusion: 该方法为意图性提供了一种实用且低成本的解释，但其概念形成水平取决于代理的记忆能力。

Abstract: Since Searle's work deconstructing intent and intentionality in the realm of
philosophy, the practical meaning of intent has received little attention in
science and technology. Intentionality and context are both central to the
scope of Promise Theory's model of Semantic Spacetime, used as an effective
Tiny Language Model. One can identify themes and concepts from a text, on a low
level (without knowledge of the specific language) by using process coherence
as a guide. Any agent process can assess superficially a degree of latent
`intentionality' in data by looking for anomalous multi-scale anomalies and
assessing the work done to form them. Scale separation can be used to sort
parts into `intended' content and `ambient context', using the spacetime
coherence as a measure. This offers an elementary but pragmatic interpretation
of latent intentionality for very low computational cost, and without reference
to extensive training or reasoning capabilities. The process is well within the
reach of basic organisms as it does not require large scale artificial
probabilistic batch processing. The level of concept formation depends,
however, on the memory capacity of the agent.

</details>


### [31] [Deep Hidden Cognition Facilitates Reliable Chain-of-Thought Reasoning](https://arxiv.org/abs/2507.10007)
*Zijun Chen,Wenbo Hu,Richang Hong*

Main category: cs.AI

TL;DR: 本文提出了一种通过利用模型内在的真实性编码来校准思维链（CoT）推理准确性的新方法，显著提升了推理任务的准确性和可靠性。


<details>
  <summary>Details</summary>
Motivation: 思维链推理在大型语言模型和多模态模型中表现出强大的推理能力，但其可靠性常因中间步骤错误累积而受限。

Method: 通过识别特定注意力头激活反映推理步骤的真实性，训练置信度预测器动态选择最优推理路径。

Result: 实验表明，该方法在数学、符号和常识推理任务中显著优于现有基线，适用于单模态和多模态场景。

Conclusion: 该方法为提升思维链推理的可靠性提供了新路径，具有广泛的应用潜力。

Abstract: Chain of Thought (CoT) reasoning has demonstrated remarkable deep reasoning
capabilities in both large language models (LLMs) and multimodal large language
models (MLLMs). However, its reliability is often undermined by the
accumulation of errors in intermediate steps. This paper introduces an novel
approach to calibrate the CoT reasoning accuracy by leveraging the model's
intrinsic veracity encoding. We discover that specific attention head
activations reliably reflect the truthfulness of reasoning steps in CoT. Based
on this insight, we train a confidence predictor to evaluate the correctness of
each reasoning step using these truthfulness-sensitive activations, dynamically
selecting the most plausible reasoning path via beam search. Experimental
results demonstrate that our method significantly outperforms the
state-of-the-art baselines (e.g., Few-Shot CoT, Self-Consistency, and
Self-Evaluation Guided Beam Search) across the mathematical, symbolic, and
commonsense reasoning tasks, exhibiting superior accuracy and reliability in
both unimodal and multimodal settings. We further validate the approach on
large reasoning models, confirming its applicability to specialized reasoning
models. Additionally, we explore the role of the model's self-correction
ability in CoT reasoning. This work provides a novel reliability improvement
path for CoT reasoning with broad application potential.

</details>


### [32] [Automating SPARQL Query Translations between DBpedia and Wikidata](https://arxiv.org/abs/2507.10045)
*Malte Christian Bartels,Debayan Banerjee,Ricardo Usbeck*

Main category: cs.AI

TL;DR: 研究了大型语言模型（LLMs）在SPARQL查询翻译中的表现，比较了不同模型和提示策略的效果，发现性能差异显著。


<details>
  <summary>Details</summary>
Motivation: 填补知识图谱互操作性研究中SPARQL到SPARQL翻译的空白。

Method: 使用三种LLM模型（Llama-3-8B、DeepSeek-R1-Distill-Llama-70B、Mistral-Large-Instruct-2407），通过零样本、少样本和思维链变体进行测试。

Result: 模型和提示策略的性能差异显著，Wikidata到DBpedia的翻译效果优于反向。

Conclusion: LLMs在SPARQL翻译中表现不一，需进一步优化模型和策略。

Abstract: This paper investigates whether state-of-the-art Large Language Models (LLMs)
can automatically translate SPARQL between popular Knowledge Graph (KG)
schemas. We focus on translations between the DBpedia and Wikidata KG, and
later on DBLP and OpenAlex KG. This study addresses a notable gap in KG
interoperability research by rigorously evaluating LLM performance on
SPARQL-to-SPARQL translation. Two benchmarks are assembled, where the first
align 100 DBpedia-Wikidata queries from QALD-9-Plus; the second contains 100
DBLP queries aligned to OpenAlex, testing generalizability beyond encyclopaedic
KGs. Three open LLMs: Llama-3-8B, DeepSeek-R1-Distill-Llama-70B, and
Mistral-Large-Instruct-2407 are selected based on their sizes and architectures
and tested with zero-shot, few-shot, and two chain-of-thought variants. Outputs
were compared with gold answers, and resulting errors were categorized. We find
that the performance varies markedly across models and prompting strategies,
and that translations for Wikidata to DBpedia work far better than translations
for DBpedia to Wikidata.

</details>


### [33] [On Gradual Semantics for Assumption-Based Argumentation](https://arxiv.org/abs/2507.10076)
*Anna Rapberger,Fabrizio Russo,Antonio Rago,Francesca Toni*

Main category: cs.AI

TL;DR: 本文提出了一种新的渐进语义家族，用于为假设（ABA框架中的核心组件）赋予辩证强度，填补了渐进语义在假设基础论证（ABA）中的空白。


<details>
  <summary>Details</summary>
Motivation: 渐进语义在计算论证中是一种细粒度的方法，但尚未应用于假设基础论证（ABA），尽管ABA是一种流行的结构化论证形式。

Method: 通过将双极集基础论证框架作为ABA框架的抽象，并推广最先进的模块化渐进语义，提出了一种新的渐进ABA语义。

Result: 实验表明，渐进ABA语义满足平衡性和单调性等理想性质，并通过合成ABA框架验证了其与基于论证的方法的对比和收敛性。

Conclusion: 本文填补了渐进语义在ABA中的空白，并提出了一种有效的渐进语义家族，展示了其在实际应用中的潜力。

Abstract: In computational argumentation, gradual semantics are fine-grained
alternatives to extension-based and labelling-based semantics . They ascribe a
dialectical strength to (components of) arguments sanctioning their degree of
acceptability. Several gradual semantics have been studied for abstract,
bipolar and quantitative bipolar argumentation frameworks (QBAFs), as well as,
to a lesser extent, for some forms of structured argumentation. However, this
has not been the case for assumption-based argumentation (ABA), despite it
being a popular form of structured argumentation with several applications
where gradual semantics could be useful. In this paper, we fill this gap and
propose a family of novel gradual semantics for equipping assumptions, which
are the core components in ABA frameworks, with dialectical strengths. To do
so, we use bipolar set-based argumentation frameworks as an abstraction of
(potentially non-flat) ABA frameworks and generalise state-of-the-art modular
gradual semantics for QBAFs. We show that our gradual ABA semantics satisfy
suitable adaptations of desirable properties of gradual QBAF semantics, such as
balance and monotonicity. We also explore an argument-based approach that
leverages established QBAF modular semantics directly, and use it as baseline.
Finally, we conduct experiments with synthetic ABA frameworks to compare our
gradual ABA semantics with its argument-based counterpart and assess
convergence.

</details>


### [34] [BlueGlass: A Framework for Composite AI Safety](https://arxiv.org/abs/2507.10106)
*Harshal Nandigramwar,Syed Qutub,Kay-Ulrich Scholl*

Main category: cs.AI

TL;DR: 本文介绍了BlueGlass框架，旨在通过统一基础设施整合多样化的AI安全工具，提升AI系统的安全性，并通过视觉语言模型的三项分析验证其效用。


<details>
  <summary>Details</summary>
Motivation: 随着AI系统能力增强和普及，确保其安全性至关重要，但现有工具无法单独提供全面保障，需要集成化的方法。

Method: 提出BlueGlass框架，支持整合和组合多样化的安全工具，覆盖模型内部和输出。通过三项分析（分布评估、探针分析和稀疏自编码器）验证框架。

Result: 框架成功整合多种安全工具，三项分析揭示了性能权衡、层次学习动态和可解释概念，为构建更可靠的AI系统提供基础。

Conclusion: BlueGlass为AI安全提供了集成化解决方案，并通过具体分析展示了其潜力，为未来研究奠定了基础。

Abstract: As AI systems become increasingly capable and ubiquitous, ensuring the safety
of these systems is critical. However, existing safety tools often target
different aspects of model safety and cannot provide full assurance in
isolation, highlighting a need for integrated and composite methodologies. This
paper introduces BlueGlass, a framework designed to facilitate composite AI
safety workflows by providing a unified infrastructure enabling the integration
and composition of diverse safety tools that operate across model internals and
outputs. Furthermore, to demonstrate the utility of this framework, we present
three safety-oriented analyses on vision-language models for the task of object
detection: (1) distributional evaluation, revealing performance trade-offs and
potential failure modes across distributions; (2) probe-based analysis of layer
dynamics highlighting shared hierarchical learning via phase transition; and
(3) sparse autoencoders identifying interpretable concepts. More broadly, this
work contributes foundational infrastructure and findings for building more
robust and reliable AI systems.

</details>


### [35] [Analysis of AI Techniques for Orchestrating Edge-Cloud Application Migration](https://arxiv.org/abs/2507.10119)
*Sadig Gojayev,Ahmad Anaqreh,Carolina Fortuna*

Main category: cs.AI

TL;DR: 论文探讨了边缘-云系统中应用迁移的自动化编排问题，比较了AI规划和强化学习方法，并提出新的分类方法。


<details>
  <summary>Details</summary>
Motivation: 提高边缘-云系统中应用迁移的服务质量（QoS）和成本效益。

Method: 基于马尔可夫决策过程（MDP），分析和比较AI规划与强化学习方法，提出新的状态空间分类。

Result: 识别了适用于边缘-云应用迁移的技术，并通过新分类方法进行了分析。

Conclusion: 为新兴计算连续环境中的应用迁移编排提供了技术理解和分类框架。

Abstract: Application migration in edge-cloud system enables high QoS and cost
effective service delivery. However, automatically orchestrating such migration
is typically solved with heuristic approaches. Starting from the Markov
Decision Process (MDP), in this paper, we identify, analyze and compare
selected state-of-the-art Artificial Intelligence (AI) planning and
Reinforcement Learning (RL) approaches for solving the class of edge-cloud
application migration problems that can be modeled as Towers of Hanoi (ToH)
problems. We introduce a new classification based on state space definition and
analyze the compared models also through this lense. The aim is to understand
available techniques capable of orchestrating such application migration in
emerging computing continuum environments.

</details>


### [36] [Could you be wrong: Debiasing LLMs using a metacognitive prompt for improving human decision making](https://arxiv.org/abs/2507.10124)
*Thomas T. Hills*

Main category: cs.AI

TL;DR: 论文提出利用人类心理学中的元认知提示（如“你可能是错的吗？”）来减少大型语言模型（LLM）的偏见，展示了这种提示如何引导LLM揭示隐藏的偏见和矛盾信息。


<details>
  <summary>Details</summary>
Motivation: 由于LLM仍在发展中，当前的偏见可能随时间变化，因此需要通用的去偏见策略。人类决策中的去偏见方法为此提供了灵感。

Method: 采用元认知提示（如“你可能是错的吗？”）干预LLM的决策过程，促使其揭示潜在偏见和矛盾信息。

Result: 元认知提示能有效引导LLM识别自身偏见，并提供额外的反思信息，揭示初始回答中未体现的偏差。

Conclusion: 人类心理学为提示工程提供了新思路，通过借鉴人类决策改进方法，可有效减少LLM的偏见。

Abstract: Identifying bias in LLMs is ongoing. Because they are still in development,
what is true today may be false tomorrow. We therefore need general strategies
for debiasing that will outlive current models. Strategies developed for
debiasing human decision making offer one promising approach as they
incorporate an LLM-style prompt intervention designed to bring latent knowledge
into awareness during decision making. LLMs trained on vast amounts of
information contain information about potential biases, counter-arguments, and
contradictory evidence, but that information may only be brought to bear if
prompted. Metacognitive prompts developed in the human decision making
literature are designed to achieve this, and as I demonstrate here, they show
promise with LLMs. The prompt I focus on here is "could you be wrong?"
Following an LLM response, this prompt leads LLMs to produce additional
information, including why they answered as they did, errors, biases,
contradictory evidence, and alternatives, none of which were apparent in their
initial response. Indeed, this metaknowledge often reveals that how LLMs and
users interpret prompts are not aligned. Here I demonstrate this prompt using a
set of questions taken from recent articles about LLM biases, including
implicit discriminatory biases and failures of metacognition. "Could you be
wrong" prompts the LLM to identify its own biases and produce cogent
metacognitive reflection. I also present another example involving convincing
but incomplete information, which is readily corrected by the metacognitive
prompt. In sum, this work argues that human psychology offers a new avenue for
prompt engineering, leveraging a long history of effective prompt-based
improvements to human decision making.

</details>


### [37] [FRSICL: LLM-Enabled In-Context Learning Flight Resource Allocation for Fresh Data Collection in UAV-Assisted Wildfire Monitoring](https://arxiv.org/abs/2507.10134)
*Yousef Emami,Hao Zhou,Miguel Gutierrez Gaitan,Kai Li,Luis Almeida*

Main category: cs.AI

TL;DR: 论文提出了一种基于LLM的在线飞行资源分配方案（FRSICL），用于无人机辅助的野火监测系统，以实时优化飞行控制和数据收集调度，最小化信息年龄（AoI）。


<details>
  <summary>Details</summary>
Motivation: 无人机在野火监测中至关重要，但现有深度强化学习（DRL）方法存在采样效率低、仿真与现实的差距以及复杂训练等问题，不适合时间敏感的应用。

Method: FRSICL利用自然语言任务描述和环境反馈，动态生成数据收集计划和飞行速度控制，无需大量重新训练。

Result: 仿真结果表明，FRSICL在最小化AoI方面优于PPO和最近邻基线方法。

Conclusion: FRSICL是一种高效且适应性强的解决方案，适用于无人机实时野火监测任务。

Abstract: Unmanned Aerial Vehicles (UAVs) are vital for public safety, particularly in
wildfire monitoring, where early detection minimizes environmental impact. In
UAV-Assisted Wildfire Monitoring (UAWM) systems, joint optimization of sensor
transmission scheduling and velocity is critical for minimizing Age of
Information (AoI) from stale sensor data. Deep Reinforcement Learning (DRL) has
been used for such optimization; however, its limitations such as low sampling
efficiency, simulation-to-reality gaps, and complex training render it
unsuitable for time-critical applications like wildfire monitoring. This paper
introduces a new online Flight Resource Allocation scheme based on LLM-Enabled
In-Context Learning (FRSICL) to jointly optimize the UAV's flight control and
data collection schedule along the trajectory in real time, thereby
asymptotically minimizing the average AoI across ground sensors. In contrast to
DRL, FRSICL generates data collection schedules and controls velocity using
natural language task descriptions and feedback from the environment, enabling
dynamic decision-making without extensive retraining. Simulation results
confirm the effectiveness of the proposed FRSICL compared to Proximal Policy
Optimization (PPO) and Nearest-Neighbor baselines.

</details>


### [38] [Introducing the Swiss Food Knowledge Graph: AI for Context-Aware Nutrition Recommendation](https://arxiv.org/abs/2507.10156)
*Lubnaa Abdur Rahman,Ioannis Papathanail,Stavroula Mougiakakou*

Main category: cs.AI

TL;DR: 论文介绍了瑞士食品知识图谱（SwissFKG），整合了食谱、成分、替代品、营养数据、饮食限制和过敏原信息，并利用LLM增强图谱内容，展示了其在个性化营养查询中的应用。


<details>
  <summary>Details</summary>
Motivation: 现有自动饮食评估系统常忽略非视觉因素（如成分替代和个人饮食需求），且瑞士缺乏整合相关营养信息的集中资源。

Method: 提出SwissFKG，建立LLM驱动的图谱填充流程，评估四种LLM的食品知识增强能力，并开发Graph-RAG应用展示其功能。

Result: LLM能有效丰富图谱营养信息，SwissFKG提供成分级信息和营养指南支持，Graph-RAG应用能回答个性化营养查询。

Conclusion: SwissFKG为下一代饮食评估工具奠定基础，融合了视觉、情境和文化维度。

Abstract: AI has driven significant progress in the nutrition field, especially through
multimedia-based automatic dietary assessment. However, existing automatic
dietary assessment systems often overlook critical non-visual factors, such as
recipe-specific ingredient substitutions that can significantly alter
nutritional content, and rarely account for individual dietary needs, including
allergies, restrictions, cultural practices, and personal preferences. In
Switzerland, while food-related information is available, it remains
fragmented, and no centralized repository currently integrates all relevant
nutrition-related aspects within a Swiss context. To bridge this divide, we
introduce the Swiss Food Knowledge Graph (SwissFKG), the first resource, to our
best knowledge, to unite recipes, ingredients, and their substitutions with
nutrient data, dietary restrictions, allergen information, and national
nutrition guidelines under one graph. We establish a LLM-powered enrichment
pipeline for populating the graph, whereby we further present the first
benchmark of four off-the-shelf (<70 B parameter) LLMs for food knowledge
augmentation. Our results demonstrate that LLMs can effectively enrich the
graph with relevant nutritional information. Our SwissFKG goes beyond recipe
recommendations by offering ingredient-level information such as allergen and
dietary restriction information, and guidance aligned with nutritional
guidelines. Moreover, we implement a Graph-RAG application to showcase how the
SwissFKG's rich natural-language data structure can help LLM answer
user-specific nutrition queries, and we evaluate LLM-embedding pairings by
comparing user-query responses against predefined expected answers. As such,
our work lays the foundation for the next generation of dietary assessment
tools that blend visual, contextual, and cultural dimensions of eating.

</details>


### [39] [Should We Ever Prefer Decision Transformer for Offline Reinforcement Learning?](https://arxiv.org/abs/2507.10174)
*Yumi Omori,Zixuan Dong,Keith Ross*

Main category: cs.AI

TL;DR: 本文通过实验比较了决策变换器（DT）与基于MLP的过滤行为克隆（FBC）在稀疏奖励环境中的表现，发现FBC性能更优且更高效，质疑DT的适用性。


<details>
  <summary>Details</summary>
Motivation: 探讨DT在离线强化学习中的实际优势，尤其是在稀疏奖励环境中，是否真的优于更简单的方法如FBC。

Method: 在Robomimic和D4RL基准上实验，比较DT与FBC的性能。FBC通过过滤低质量轨迹后进行行为克隆。

Result: FBC在稀疏奖励环境中表现优于DT，且更高效。

Conclusion: DT在稀疏和密集奖励环境中均无显著优势，质疑其适用性。

Abstract: In recent years, extensive work has explored the application of the
Transformer architecture to reinforcement learning problems. Among these,
Decision Transformer (DT) has gained particular attention in the context of
offline reinforcement learning due to its ability to frame return-conditioned
policy learning as a sequence modeling task. Most recently, Bhargava et al.
(2024) provided a systematic comparison of DT with more conventional MLP-based
offline RL algorithms, including Behavior Cloning (BC) and Conservative
Q-Learning (CQL), and claimed that DT exhibits superior performance in
sparse-reward and low-quality data settings.
  In this paper, through experimentation on robotic manipulation tasks
(Robomimic) and locomotion benchmarks (D4RL), we show that MLP-based Filtered
Behavior Cloning (FBC) achieves competitive or superior performance compared to
DT in sparse-reward environments. FBC simply filters out low-performing
trajectories from the dataset and then performs ordinary behavior cloning on
the filtered dataset. FBC is not only very straightforward, but it also
requires less training data and is computationally more efficient. The results
therefore suggest that DT is not preferable for sparse-reward environments.
From prior work, arguably, DT is also not preferable for dense-reward
environments. Thus, we pose the question: Is DT ever preferable?

</details>


### [40] [Survey for Categorising Explainable AI Studies Using Data Analysis Task Frameworks](https://arxiv.org/abs/2507.10208)
*Hamzah Ziadeh,Hendrik Knoche*

Main category: cs.AI

TL;DR: 论文提出了一种基于“what, why, who”三个维度对可解释人工智能（XAI）研究进行分类和比较的方法，旨在解决当前研究中任务描述不足、脱离上下文和用户测试不充分的问题。


<details>
  <summary>Details</summary>
Motivation: 当前XAI研究存在大量矛盾且缺乏具体设计建议，主要源于对需要AI辅助的任务理解不足。

Method: 结合视觉分析、认知科学和仪表板设计等多个领域，提出分类和比较XAI研究的三维框架。

Result: 研究发现主要问题包括任务描述不足、脱离上下文的研究和用户测试不充分，并提出研究应明确报告用户的领域、AI和数据分析专长。

Conclusion: 论文提出的研究指南有助于XAI社区更好地解析快速发展的领域，帮助研究者和设计者识别相关研究、填补研究空白并处理设计矛盾。

Abstract: Research into explainable artificial intelligence (XAI) for data analysis
tasks suffer from a large number of contradictions and lack of concrete design
recommendations stemming from gaps in understanding the tasks that require AI
assistance. In this paper, we drew on multiple fields such as visual analytics,
cognition, and dashboard design to propose a method for categorising and
comparing XAI studies under three dimensions: what, why, and who. We identified
the main problems as: inadequate descriptions of tasks, context-free studies,
and insufficient testing with target users. We propose that studies should
specifically report on their users' domain, AI, and data analysis expertise to
illustrate the generalisability of their findings. We also propose study
guidelines for designing and reporting XAI tasks to improve the XAI community's
ability to parse the rapidly growing field. We hope that our contribution can
help researchers and designers better identify which studies are most relevant
to their work, what gaps exist in the research, and how to handle contradictory
results regarding XAI design.

</details>


### [41] [Toward Real-World Table Agents: Capabilities, Workflows, and Design Principles for LLM-based Table Intelligence](https://arxiv.org/abs/2507.10281)
*Jiaming Tian,Liyao Li,Wentao Ye,Haobo Wang,Lingxin Wang,Lihua Yu,Zujie Ren,Gang Chen,Junbo Zhao*

Main category: cs.AI

TL;DR: 该论文综述了基于LLM的表格代理，旨在通过整合预处理、推理和领域适应自动化表格任务，分析了五大核心能力，并指出开源模型在真实场景中的性能差距。


<details>
  <summary>Details</summary>
Motivation: 现实世界中的表格任务常涉及噪声和复杂性，而现有研究多针对干净学术数据集，因此需要探索更实用的解决方案。

Method: 定义了五大核心能力（C1-C5）来分析当前方法，并详细研究了Text-to-SQL代理的性能。

Result: 发现开源模型在真实场景中的性能显著低于学术基准，提出了改进建议。

Conclusion: 需提升LLM表格代理的鲁棒性、泛化能力和效率，以适应实际应用。

Abstract: Tables are fundamental in domains such as finance, healthcare, and public
administration, yet real-world table tasks often involve noise, structural
heterogeneity, and semantic complexity--issues underexplored in existing
research that primarily targets clean academic datasets. This survey focuses on
LLM-based Table Agents, which aim to automate table-centric workflows by
integrating preprocessing, reasoning, and domain adaptation. We define five
core competencies--C1: Table Structure Understanding, C2: Table and Query
Semantic Understanding, C3: Table Retrieval and Compression, C4: Executable
Reasoning with Traceability, and C5: Cross-Domain Generalization--to analyze
and compare current approaches. In addition, a detailed examination of the
Text-to-SQL Agent reveals a performance gap between academic benchmarks and
real-world scenarios, especially for open-source models. Finally, we provide
actionable insights to improve the robustness, generalization, and efficiency
of LLM-based Table Agents in practical settings.

</details>


### [42] [Instance space analysis of the capacitated vehicle routing problem](https://arxiv.org/abs/2507.10397)
*Alessandra M. M. M. Gouvêa,Nuno Paulos,Eduardo Uchoa e Mariá C. V. Nascimento*

Main category: cs.AI

TL;DR: 本文通过实例空间分析（ISA）方法，结合DIMACS数据集，识别了23个相关实例特征，并利用降维和机器学习方法，揭示了实例结构对元启发式算法性能的影响。


<details>
  <summary>Details</summary>
Motivation: 解决CVRP研究中实例特征与元启发式算法性能之间复杂关系的问题。

Method: 结合ISA方法和DIMACS数据集，通过PRELIM、SIFTED和PILOT阶段进行降维和机器学习分析。

Result: 生成了一个二维实例空间投影，并提供了投影矩阵，便于新实例的分析。

Conclusion: 为CVRP领域提供了一种新的实例分析方法，有助于理解实例结构与算法性能的关系。

Abstract: This paper seeks to advance CVRP research by addressing the challenge of
understanding the nuanced relationships between instance characteristics and
metaheuristic (MH) performance. We present Instance Space Analysis (ISA) as a
valuable tool that allows for a new perspective on the field. By combining the
ISA methodology with a dataset from the DIMACS 12th Implementation Challenge on
Vehicle Routing, our research enabled the identification of 23 relevant
instance characteristics. Our use of the PRELIM, SIFTED, and PILOT stages,
which employ dimensionality reduction and machine learning methods, allowed us
to create a two-dimensional projection of the instance space to understand how
the structure of instances affect the behavior of MHs. A key contribution of
our work is that we provide a projection matrix, which makes it straightforward
to incorporate new instances into this analysis and allows for a new method for
instance analysis in the CVRP field.

</details>


### [43] [SentiDrop: A Multi Modal Machine Learning model for Predicting Dropout in Distance Learning](https://arxiv.org/abs/2507.10421)
*Meriem Zerkouk,Miloud Mihoubi,Belkacem Chikhaoui*

Main category: cs.AI

TL;DR: 提出了一种结合BERT情感分析和XGBoost特征选择的新模型，用于预测远程学习中的学生辍学风险，准确率达84%，优于基线模型。


<details>
  <summary>Details</summary>
Motivation: 远程学习中学生辍学问题严重，早期预测对干预至关重要。整合多源数据（如情感、行为和社会人口数据）可提高预测准确性。

Method: 使用BERT对学生的评论进行情感分析，结合XGBoost分析社会人口和行为数据，通过特征重要性选择关键特征。

Result: 模型在未见数据上准确率达84%，优于基线模型的82%，且在精确度和F1分数等指标上表现更优。

Conclusion: 该模型可作为个性化干预工具，有效降低辍学率并提升学生坚持学习的动力。

Abstract: School dropout is a serious problem in distance learning, where early
detection is crucial for effective intervention and student perseverance.
Predicting student dropout using available educational data is a widely
researched topic in learning analytics. Our partner's distance learning
platform highlights the importance of integrating diverse data sources,
including socio-demographic data, behavioral data, and sentiment analysis, to
accurately predict dropout risks. In this paper, we introduce a novel model
that combines sentiment analysis of student comments using the Bidirectional
Encoder Representations from Transformers (BERT) model with socio-demographic
and behavioral data analyzed through Extreme Gradient Boosting (XGBoost). We
fine-tuned BERT on student comments to capture nuanced sentiments, which were
then merged with key features selected using feature importance techniques in
XGBoost. Our model was tested on unseen data from the next academic year,
achieving an accuracy of 84\%, compared to 82\% for the baseline model.
Additionally, the model demonstrated superior performance in other metrics,
such as precision and F1-score. The proposed method could be a vital tool in
developing personalized strategies to reduce dropout rates and encourage
student perseverance

</details>


### [44] [Acquiring and Adapting Priors for Novel Tasks via Neural Meta-Architectures](https://arxiv.org/abs/2507.10446)
*Sudarshan Babu*

Main category: cs.AI

TL;DR: 论文提出了一种在数据稀缺领域（如计算化学、医学影像）中高效获取先验知识的架构，利用神经记忆和超网络设计，结合MAML，实现了小样本适应和泛化性能提升。


<details>
  <summary>Details</summary>
Motivation: 在数据稀缺领域（如计算化学、医学影像）中，传统的大规模预训练模型难以应用，因此需要设计高效获取先验知识的架构。

Method: 使用神经记忆实现小样本适应，设计超网络结合MAML获取更泛化的先验，并应用于3D场景生成和分割以及分子生成。

Result: 超网络在少量训练场景下高效获取先验，加速文本到3D生成；在3D分割中高效迁移先验；分子生成方法改进了分子属性预测。

Conclusion: 提出的架构在数据稀缺领域实现了高效先验获取和任务适应，为相关领域提供了实用解决方案。

Abstract: The ability to transfer knowledge from prior experiences to novel tasks
stands as a pivotal capability of intelligent agents, including both humans and
computational models. This principle forms the basis of transfer learning,
where large pre-trained neural networks are fine-tuned to adapt to downstream
tasks. Transfer learning has demonstrated tremendous success, both in terms of
task adaptation speed and performance. However there are several domains where,
due to lack of data, training such large pre-trained models or foundational
models is not a possibility - computational chemistry, computational
immunology, and medical imaging are examples. To address these challenges, our
work focuses on designing architectures to enable efficient acquisition of
priors when large amounts of data are unavailable. In particular, we
demonstrate that we can use neural memory to enable adaptation on
non-stationary distributions with only a few samples. Then we demonstrate that
our hypernetwork designs (a network that generates another network) can acquire
more generalizable priors than standard networks when trained with Model
Agnostic Meta-Learning (MAML). Subsequently, we apply hypernetworks to 3D scene
generation, demonstrating that they can acquire priors efficiently on just a
handful of training scenes, thereby leading to faster text-to-3D generation. We
then extend our hypernetwork framework to perform 3D segmentation on novel
scenes with limited data by efficiently transferring priors from earlier viewed
scenes. Finally, we repurpose an existing molecular generative method as a
pre-training framework that facilitates improved molecular property prediction,
addressing critical challenges in computational immunology

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [45] [Spatial ModernBERT: Spatial-Aware Transformer for Table and Key-Value Extraction in Financial Documents at Scale](https://arxiv.org/abs/2507.08865)
*Javis AI Team,Amrendra Singh,Maulik Shah,Dharshan Sampath*

Main category: cs.CL

TL;DR: 提出了一种基于Spatial ModernBERT的模型，用于从复杂财务文档中提取表格和键值对，通过多任务分类和空间嵌入提升准确性。


<details>
  <summary>Details</summary>
Motivation: 财务文档中的表格和键值对提取对审计、数据分析和自动化发票处理等业务流程至关重要。

Method: 使用Spatial ModernBERT模型，结合空间嵌入，通过三个分类头（标签、列、行）进行标记分类，并在PubTables-1M数据集上预训练后微调。

Result: 模型在财务文档数据集上表现出色，能够有效结合文本和空间信息，实现高精度的表格和键值对提取。

Conclusion: Spatial ModernBERT通过多任务分类和空间嵌入，显著提升了财务文档中表格和键值对提取的准确性。

Abstract: Extracting tables and key-value pairs from financial documents is essential
for business workflows such as auditing, data analytics, and automated invoice
processing. In this work, we introduce Spatial ModernBERT-a transformer-based
model augmented with spatial embeddings-to accurately detect and extract
tabular data and key-value fields from complex financial documents. We cast the
extraction task as token classification across three heads: (1) Label Head,
classifying each token as a label (e.g., PO Number, PO Date, Item Description,
Quantity, Base Cost, MRP, etc.); (2) Column Head, predicting column indices;
(3) Row Head, distinguishing the start of item rows and header rows. The model
is pretrained on the PubTables-1M dataset, then fine-tuned on a financial
document dataset, achieving robust performance through cross-entropy loss on
each classification head. We propose a post-processing method to merge tokens
using B-I-IB tagging, reconstruct the tabular layout, and extract key-value
pairs. Empirical evaluation shows that Spatial ModernBERT effectively leverages
both textual and spatial cues, facilitating highly accurate table and key-value
extraction in real-world financial documents.

</details>


### [46] [SEALGuard: Safeguarding the Multilingual Conversations in Southeast Asian Languages for LLM Software Systems](https://arxiv.org/abs/2507.08898)
*Wenliang Shan,Michael Fu,Rui Yang,Chakkrit,Tantithamthavorn*

Main category: cs.CL

TL;DR: SEALGuard是一种多语言护栏，旨在解决现有护栏在多语言安全对齐上的不足，通过LoRA技术改进多语言不安全提示的检测性能。


<details>
  <summary>Details</summary>
Motivation: 现有护栏（如LlamaGuard）在多语言不安全输入（尤其是低资源语言）上表现不佳，导致LLM系统存在安全隐患。

Method: 采用低秩适应（LoRA）技术将通用多语言模型转化为多语言护栏，并构建SEALSBench数据集（包含10种语言的26万条提示）。

Result: SEALGuard在多语言不安全提示检测上显著优于LlamaGuard，防御成功率提升48%，且在所有指标（DSR、精确率、F1）上表现最佳。

Conclusion: SEALGuard通过有效的多语言护栏技术，显著提升了LLM系统的安全对齐能力。

Abstract: Safety alignment is critical for LLM-powered systems. While recent
LLM-powered guardrail approaches such as LlamaGuard achieve high detection
accuracy of unsafe inputs written in English (e.g., ``How to create a bomb?''),
they struggle with multilingual unsafe inputs. This limitation leaves LLM
systems vulnerable to unsafe and jailbreak prompts written in low-resource
languages such as those in Southeast Asia. This paper introduces SEALGuard, a
multilingual guardrail designed to improve the safety alignment across diverse
languages. It aims to address the multilingual safety alignment gap of existing
guardrails and ensure effective filtering of unsafe and jailbreak prompts in
LLM-powered systems. We adapt a general-purpose multilingual language model
into a multilingual guardrail using low-rank adaptation (LoRA). We construct
SEALSBench, a large-scale multilingual safety alignment dataset containing over
260,000 prompts in ten languages, including safe, unsafe, and jailbreak cases.
We evaluate SEALGuard against state-of-the-art guardrails such as LlamaGuard on
this benchmark. Our findings show that multilingual unsafe and jailbreak
prompts substantially degrade the performance of the state-of-the-art
LlamaGuard, which experiences a drop in Defense Success Rate (DSR) by 9% and
18%, respectively, compared to its performance on English-only prompts. In
contrast, SEALGuard outperforms existing guardrails in detecting multilingual
unsafe and jailbreak prompts, improving DSR by 48% over LlamaGuard and
achieving the best DSR, precision, and F1-score. Our ablation study further
reveals the contributions of adaptation strategies and model size to the
overall performance of SEALGuard. SEALGuard advances the safety alignment of
LLM systems by introducing an effective multilingual guardrail.

</details>


### [47] [Evaluating LLMs in Medicine: A Call for Rigor, Transparency](https://arxiv.org/abs/2507.08916)
*Mahmoud Alwakeel,Aditya Nagori,Vijay Krishnamoorthy,Rishikesan Kamaleswaran*

Main category: cs.CL

TL;DR: 论文评估了大型语言模型（LLMs）在医学问答中的局限性，重点关注评估数据集的质量，并呼吁建立标准化框架。


<details>
  <summary>Details</summary>
Motivation: 现有医学问答数据集缺乏临床真实性、透明性和验证，需要更严谨和全面的评估工具。

Method: 分析了MedQA、MedMCQA等常用数据集及医学期刊挑战问题，评估其严谨性和临床相关性。

Result: 现有数据集普遍不足，挑战问题虽有益但规模小且易受LLM训练影响，需更安全、全面的数据集。

Conclusion: 需建立标准化评估框架，并推动多方合作以确保数据和方法严谨、无偏且反映临床复杂性。

Abstract: Objectives: To evaluate the current limitations of large language models
(LLMs) in medical question answering, focusing on the quality of datasets used
for their evaluation. Materials and Methods: Widely-used benchmark datasets,
including MedQA, MedMCQA, PubMedQA, and MMLU, were reviewed for their rigor,
transparency, and relevance to clinical scenarios. Alternatives, such as
challenge questions in medical journals, were also analyzed to identify their
potential as unbiased evaluation tools. Results: Most existing datasets lack
clinical realism, transparency, and robust validation processes. Publicly
available challenge questions offer some benefits but are limited by their
small size, narrow scope, and exposure to LLM training. These gaps highlight
the need for secure, comprehensive, and representative datasets. Conclusion: A
standardized framework is critical for evaluating LLMs in medicine.
Collaborative efforts among institutions and policymakers are needed to ensure
datasets and methodologies are rigorous, unbiased, and reflective of clinical
complexities.

</details>


### [48] [From KMMLU-Redux to KMMLU-Pro: A Professional Korean Benchmark Suite for LLM Evaluation](https://arxiv.org/abs/2507.08924)
*Seokhee Hong,Sunkyoung Kim,Guijin Son,Soyeon Kim,Yeonjung Hong,Jinsik Lee*

Main category: cs.CL

TL;DR: 论文介绍了两个韩语专家级基准测试KMMLU-Redux和KMMLU-Pro，用于评估大语言模型在韩国工业领域的适用性。


<details>
  <summary>Details</summary>
Motivation: 开发适用于现实场景的大语言模型需要涵盖工业和学术领域的基准测试。

Method: 重构KMMLU为KMMLU-Redux，并基于韩国国家专业执照考试创建KMMLU-Pro。

Result: 实验表明这些基准能全面代表韩国工业知识。

Conclusion: 数据集已公开，为评估大语言模型提供了可靠工具。

Abstract: The development of Large Language Models (LLMs) requires robust benchmarks
that encompass not only academic domains but also industrial fields to
effectively evaluate their applicability in real-world scenarios. In this
paper, we introduce two Korean expert-level benchmarks. KMMLU-Redux,
reconstructed from the existing KMMLU, consists of questions from the Korean
National Technical Qualification exams, with critical errors removed to enhance
reliability. KMMLU-Pro is based on Korean National Professional Licensure exams
to reflect professional knowledge in Korea. Our experiments demonstrate that
these benchmarks comprehensively represent industrial knowledge in Korea. We
release our dataset publicly available.

</details>


### [49] [Self-Improving Model Steering](https://arxiv.org/abs/2507.08967)
*Rongyi Zhu,Yuhui Wang,Tanqiu Jiang,Jiacheng Liang,Ting Wang*

Main category: cs.CL

TL;DR: SIMS是一种无需外部监督的自改进模型引导框架，通过自主生成和优化对比样本，显著提升了大型语言模型（LLM）的引导效果和适应性。


<details>
  <summary>Details</summary>
Motivation: 传统模型引导方法依赖外部标注数据，限制了其适应性和效果。SIMS旨在解决这一问题，通过自改进实现更灵活的模型引导。

Method: SIMS通过自改进循环生成和优化对比样本，并结合提示排名和对比采样等新策略，提升引导效果。

Result: SIMS在多种LLM和基准测试中表现优异，显著优于现有方法。

Conclusion: 自改进模型引导是未来LLM推理时对齐研究的有前景方向。

Abstract: Model steering represents a powerful technique that dynamically aligns large
language models (LLMs) with human preferences during inference. However,
conventional model-steering methods rely heavily on externally annotated data,
not only limiting their adaptability to varying contexts but also tethering
their effectiveness to annotation quality. In this paper, we present SIMS, the
first self-improving model-steering framework that operates without relying on
external supervision. At its core, SIMS autonomously generates and refines
contrastive samples through iterative self-improvement cycles, enabling
adaptive, context-specific steering. Additionally, SIMS employs novel
strategies, including prompt ranking and contrast sampling, to further enhance
steering efficacy. Extensive evaluation across diverse LLMs and benchmarks
demonstrates that SIMS substantially outperforms existing methods in steering
effectiveness and adaptability, highlighting self-improving model steering as a
promising direction for future research on inference-time LLM alignment.

</details>


### [50] [Application of CARE-SD text classifier tools to assess distribution of stigmatizing and doubt-marking language features in EHR](https://arxiv.org/abs/2507.08969)
*Drew Walker,Jennifer Love,Swati Rajwal,Isabel C Walker,Hannah LF Cooper,Abeed Sarker,Melvin Livingston III*

Main category: cs.CL

TL;DR: 研究发现电子健康记录（EHR）中存在对特定患者群体的污名化语言，尤其是黑人、低收入患者及某些疾病患者，且不同医疗提供者类型的使用频率不同。


<details>
  <summary>Details</summary>
Motivation: 探讨电子健康记录中污名化语言的普遍性及其对不同患者群体的影响。

Method: 通过扩展词典匹配和监督学习分类器识别MIMIC-III EHR中的怀疑标记和污名化标签，并使用泊松回归模型分析预测因素。

Result: 黑人、低收入患者及某些疾病患者的污名化标签率更高，护士和社会工作者的使用频率也较高。

Conclusion: 污名化语言在历史上被污名化的患者群体中更为普遍，需采取措施减少其影响。

Abstract: Introduction: Electronic health records (EHR) are a critical medium through
which patient stigmatization is perpetuated among healthcare teams. Methods: We
identified linguistic features of doubt markers and stigmatizing labels in
MIMIC-III EHR via expanded lexicon matching and supervised learning
classifiers. Predictors of rates of linguistic features were assessed using
Poisson regression models. Results: We found higher rates of stigmatizing
labels per chart among patients who were Black or African American (RR: 1.16),
patients with Medicare/Medicaid or government-run insurance (RR: 2.46),
self-pay (RR: 2.12), and patients with a variety of stigmatizing disease and
mental health conditions. Patterns among doubt markers were similar, though
male patients had higher rates of doubt markers (RR: 1.25). We found increased
stigmatizing labels used by nurses (RR: 1.40), and social workers (RR: 2.25),
with similar patterns of doubt markers. Discussion: Stigmatizing language
occurred at higher rates among historically stigmatized patients, perpetuated
by multiple provider types.

</details>


### [51] [Beyond vividness: Content analysis of induced hallucinations reveals the hidden structure of individual differences in visual imagery](https://arxiv.org/abs/2507.09011)
*Ana Chkhaidze,Reshanne R. Reeder,Connor Gag,Anastasia Kiyonaga,Seana Coulson*

Main category: cs.CL

TL;DR: 研究探讨了Ganzflicker诱导的视觉幻觉与个体视觉系统差异的关系，发现强想象力者描述复杂自然内容，弱想象力者报告简单几何模式。


<details>
  <summary>Details</summary>
Motivation: 探索视觉系统差异如何影响Ganzflicker诱导的幻觉内容，验证想象力谱系理论。

Method: 利用自然语言处理工具分析4000多名参与者的幻觉自由文本描述，比较不同想象力表型的内容差异。

Result: 强想象力者描述复杂自然内容，弱想象力者报告简单几何模式；视觉语言模型能更好捕捉这些差异。

Conclusion: 个体在早期视觉区域与高阶区域的协调差异可能解释了想象力谱系的不同表现。

Abstract: A rapidly alternating red and black display known as Ganzflicker induces
visual hallucinations that reflect the generative capacity of the visual
system. Recent proposals regarding the imagery spectrum, that is, differences
in the visual system of individuals with absent imagery, typical imagery, and
vivid imagery, suggest these differences should impact the complexity of other
internally generated visual experiences. Here, we used tools from natural
language processing to analyze free-text descriptions of hallucinations from
over 4,000 participants, asking whether people with different imagery
phenotypes see different things in their mind's eye during Ganzflicker-induced
hallucinations. Strong imagers described complex, naturalistic content, while
weak imagers reported simple geometric patterns. Embeddings from vision
language models better captured these differences than text-only language
models, and participants with stronger imagery used language with richer
sensorimotor associations. These findings may reflect individual variation in
coordination between early visual areas and higher-order regions relevant for
the imagery spectrum.

</details>


### [52] [Lizard: An Efficient Linearization Framework for Large Language Models](https://arxiv.org/abs/2507.09025)
*Chien Van Nguyen,Ruiyi Zhang,Hanieh Deilamsalehy,Puneet Mathur,Viet Dac Lai,Haoliang Wang,Jayakumar Subramanian,Ryan A. Rossi,Trung Bui,Nikos Vlassis,Franck Dernoncourt,Thien Huu Nguyen*

Main category: cs.CL

TL;DR: Lizard是一个线性化框架，将预训练的Transformer大语言模型转化为支持无限上下文生成的次二次复杂度架构，解决了内存和计算瓶颈问题。


<details>
  <summary>Details</summary>
Motivation: Transformer大语言模型在长上下文生成时面临内存和计算瓶颈，Lizard旨在解决这些问题。

Method: 引入次二次注意力机制和门控模块，结合全局上下文压缩和滑动窗口注意力，并采用硬件感知算法加速训练。

Result: 在标准语言建模任务中几乎无损恢复教师模型性能，在MMLU基准上比之前模型提升18分。

Conclusion: Lizard在保持性能的同时显著提升了效率和灵活性。

Abstract: We propose Lizard, a linearization framework that transforms pretrained
Transformer-based Large Language Models (LLMs) into flexible, subquadratic
architectures for infinite-context generation. Transformer-based LLMs face
significant memory and computational bottlenecks as context lengths increase,
due to the quadratic complexity of softmax attention and the growing key-value
(KV) cache. Lizard addresses these limitations by introducing a subquadratic
attention mechanism that closely approximates softmax attention while
preserving the output quality. Unlike previous linearization methods, which are
often limited by fixed model structures and therefore exclude gating
mechanisms, Lizard incorporates a gating module inspired by recent
state-of-the-art linear models. This enables adaptive memory control, supports
constant-memory inference, offers strong length generalization, and allows more
flexible model design. Lizard combines gated linear attention for global
context compression with sliding window attention enhanced by meta memory,
forming a hybrid mechanism that captures both long-range dependencies and
fine-grained local interactions. Moreover, we introduce a hardware-aware
algorithm that accelerates the training speed of our models. Extensive
experiments show that Lizard achieves near-lossless recovery of the teacher
model's performance across standard language modeling tasks, while
significantly outperforming previous linearization methods. On the 5-shot MMLU
benchmark, Lizard improves over prior models by 18 points and shows significant
improvements on associative recall tasks.

</details>


### [53] [ALIGN: Prompt-based Attribute Alignment for Reliable, Responsible, and Personalized LLM-based Decision-Making](https://arxiv.org/abs/2507.09037)
*Bharadwaj Ravichandran,David Joy,Paul Elliott,Brian Hu,Jadie Adams,Christopher Funk,Emily Veenhuis,Anthony Hoogs,Arslan Basharat*

Main category: cs.CL

TL;DR: ALIGN系统通过提示对齐细粒度属性，动态个性化基于LLM的决策者，支持配置管理、结构化输出生成和可交换LLM骨干。


<details>
  <summary>Details</summary>
Motivation: 用户多样化的价值观和偏好影响决策，需要新的LLM对齐和个性化方法。

Method: ALIGN系统通过提示对齐细粒度属性，提供配置管理、结构化输出和可交换LLM骨干。

Result: 用户界面支持LLM的定性对比，定量分析显示在公共意见调查和医疗分诊决策中的对齐效果。

Conclusion: ALIGN框架开源，促进可靠、负责和个性化的LLM决策研究。

Abstract: Large language models (LLMs) are increasingly being used as decision aids.
However, users have diverse values and preferences that can affect their
decision-making, which requires novel methods for LLM alignment and
personalization. Existing LLM comparison tools largely focus on benchmarking
tasks, such as knowledge-based question answering. In contrast, our proposed
ALIGN system focuses on dynamic personalization of LLM-based decision-makers
through prompt-based alignment to a set of fine-grained attributes. Key
features of our system include robust configuration management, structured
output generation with reasoning, and several algorithm implementations with
swappable LLM backbones, enabling different types of analyses. Our user
interface enables a qualitative, side-by-side comparison of LLMs and their
alignment to various attributes, with a modular backend for easy algorithm
integration. Additionally, we perform a quantitative analysis comparing
alignment approaches in two different domains: demographic alignment for public
opinion surveys and value alignment for medical triage decision-making. The
entire ALIGN framework is open source and will enable new research on reliable,
responsible, and personalized LLM-based decision-makers.

</details>


### [54] [OpenCodeReasoning-II: A Simple Test Time Scaling Approach via Self-Critique](https://arxiv.org/abs/2507.09075)
*Wasi Uddin Ahmad,Somshubra Majumdar,Aleksander Ficek,Sean Narenthiran,Mehrzad Samadi,Jocelyn Huang,Siddhartha Jain,Vahid Noroozi,Boris Ginsburg*

Main category: cs.CL

TL;DR: OpenCodeReasoning-II是一个包含2.5M问题-解决方案-评论三元组的大规模数据集，用于代码生成和评论的联合训练。通过两阶段微调策略，Qwen2.5-Instruct模型在代码生成和竞争性编程性能上表现优异。


<details>
  <summary>Details</summary>
Motivation: 当前基于推理的大型语言模型在代码生成和评论方面依赖高质量数据集，但现有数据集规模有限。

Method: 采用两阶段监督微调策略：第一阶段专注于代码生成，第二阶段联合训练代码生成和评论模型。

Result: 微调后的Qwen2.5-Instruct模型在代码生成上表现优于或等于现有最佳开放权重模型，联合训练显著提升了竞争性编程性能。

Conclusion: OpenCodeReasoning-II数据集和两阶段微调策略为代码生成和评论任务提供了有效支持，扩展的LiveCodeBench基准进一步促进了LLM评估。

Abstract: Recent advancements in reasoning-based Large Language Models (LLMs),
particularly their potential through test-time scaling, have created
significant opportunities for distillation in code generation and critique.
However, progress in both areas fundamentally depends on large-scale,
high-quality datasets. In this work, we introduce OpenCodeReasoning-II, a
dataset consists of 2.5M question-solution-critique triples (approx. 35K unique
programming questions), making it nearly twice the size of the previous largest
publicly available code reasoning dataset. In this work, we employ a two-stage
supervised fine-tuning strategy. The first stage focuses on fine-tuning for
code generation, while the second stage involves the joint training of models
for both code generation and critique. Our resulting finetuned Qwen2.5-Instruct
models achieve performance in code generation that either exceeds or equals the
best prior open-weight distilled models. Notably, the integration of our code
generation and critique models leads to significant improvements in competitive
coding performance. Furthermore, we present an extension of the LiveCodeBench
benchmark to specifically support the C++ programming language, thereby
facilitating more comprehensive LLM evaluation using this benchmark.

</details>


### [55] [Dynamic Parameter Memory: Temporary LoRA-Enhanced LLM for Long-Sequence Emotion Recognition in Conversation](https://arxiv.org/abs/2507.09076)
*Jialong Mai,Xiaofen Xing,Yawei Li,Zhipeng Li,Jingyuan Xing,Xiangmin Xu*

Main category: cs.CL

TL;DR: 本文提出了一种动态参数记忆（DPM）机制，通过上下文语义和句子级情感编码，解决了语音大语言模型（SLLM）在处理长音频时的限制，显著提升了情感识别性能。


<details>
  <summary>Details</summary>
Motivation: 语音模态的高帧率限制了SLLM的信号处理和理解能力，现有方法忽视了情感在对话中的连续性和惯性。

Method: 提出DPM机制，逐步将句子级信息和情感编码到临时LoRA模块中，以记忆上下文信息。

Result: 在IEMOCAP数据集上，DPM显著提升了SLLM处理长音频序列时的情感识别能力，达到最先进水平。

Conclusion: DPM机制有效扩展了SLLM在长音频情感识别中的应用，解决了上下文窗口限制问题。

Abstract: Recent research has focused on applying speech large language model (SLLM) to
improve speech emotion recognition (SER). However, the inherently high frame
rate in speech modality severely limits the signal processing and understanding
capabilities of SLLM. For example, a SLLM with a 4K context window can only
process 80 seconds of audio at 50Hz feature sampling rate before reaching its
capacity limit. Input token compression methods used in SLLM overlook the
continuity and inertia of emotions across multiple conversation turns. This
paper proposes a Dynamic Parameter Memory (DPM) mechanism with contextual
semantics and sentence-level emotion encoding, enabling processing of
unlimited-length audio with limited context windows in SLLM. Specifically, DPM
progressively encodes sentence-level information and emotions into a temporary
LoRA module during inference to effectively "memorize" the contextual
information. We trained an emotion SLLM as a backbone and incorporated our DPM
into inference for emotion recognition in conversation (ERC). Experimental
results on the IEMOCAP dataset show that DPM significantly improves the emotion
recognition capabilities of SLLM when processing long audio sequences,
achieving state-of-the-art performance.

</details>


### [56] [CompassJudger-2: Towards Generalist Judge Model via Verifiable Rewards](https://arxiv.org/abs/2507.09104)
*Taolin Zhang,Maosong Cao,Alexander Lam,Songyang Zhang,Kai Chen*

Main category: cs.CL

TL;DR: CompassJudger-2是一个新型通用评判模型，通过任务驱动的多领域数据策略提升评判能力，并在多个基准测试中表现优异。


<details>
  <summary>Details</summary>
Motivation: 当前评判模型存在专业狭窄和鲁棒性不足的问题，限制了其全面评估能力。

Method: 采用任务驱动的多领域数据策略，结合可验证奖励和监督学习，引入边界策略梯度损失优化学习目标。

Result: CompassJudger-2在多个评判和奖励基准测试中表现优异，7B模型与更大模型竞争。

Conclusion: 该研究推动了鲁棒、可扩展的LLM评判，并建立了新的性能和评估标准。

Abstract: Recently, the role of LLM-as-judge in evaluating large language models has
gained prominence. However, current judge models suffer from narrow
specialization and limited robustness, undermining their capacity for
comprehensive evaluations. In this work, we present CompassJudger-2, a novel
generalist judge model that overcomes these limitations via a task-driven,
multi-domain data curation strategy. Central to our approach is supervising
judgment tasks with verifiable rewards, guiding intrinsic critical reasoning
through rejection sampling to foster robust, generalizable judgment
capabilities. We introduce a refined learning objective with margin policy
gradient loss to enhance performance. Empirically, CompassJudger-2 achieves
superior results across multiple judge and reward benchmarks, and our 7B model
demonstrates competitive judgment accuracy with significantly larger models
like DeepSeek-V3 and Qwen3-235B-A22B. Additionally, we propose JudgerBenchV2, a
comprehensive benchmark evaluating cross-domain judgment accuracy and rank
consistency to standardize judge model evaluation. These contributions advance
robust, scalable LLM judgment and establish new performance and evaluation
standards.

</details>


### [57] [OPENXRD: A Comprehensive Benchmark and Enhancement Framework for LLM/MLLM XRD Question Answering](https://arxiv.org/abs/2507.09155)
*Ali Vosoughi,Ayoub Shahnazari,Yufeng Xi,Zeliang Zhang,Griffin Hess,Chenliang Xu,Niaz Abdolrahim*

Main category: cs.CL

TL;DR: OPENXRD是一个用于晶体学问答的开卷管道，结合GPT-4.5生成的简洁支持内容，显著提升了小模型在X射线衍射（XRD）任务中的准确性。


<details>
  <summary>Details</summary>
Motivation: 解决传统扫描教科书可能引发的版权问题，同时帮助小模型理解晶体学关键概念。

Method: 通过GPT-4.5生成紧凑的领域特定参考内容，并在217个专家级XRD问题上评估不同视觉语言模型的性能。

Result: 使用GPT-4.5生成摘要的模型（尤其是晶体学训练有限的模型）准确性显著提升。

Conclusion: OPENXRD展示了开卷系统在材料科学中的实用性，并为科学领域的自然语言处理工具奠定了基础。

Abstract: This work presents OPENXRD, an open-book pipeline designed for
crystallography question answering, which integrates textual prompts with
concise supporting content generated by GPT-4.5. Instead of using scanned
textbooks, which may lead to copyright issues, OPENXRD generates compact,
domain-specific references that help smaller models understand key concepts in
X-ray diffraction (XRD). We evaluate OPENXRD on a well-defined set of 217
expert-level XRD questions by comparing different vision-language models,
including GPT-4 and LLaVA-based frameworks such as Mistral, LLaMA, and QWEN,
under both closed-book (without supporting material) and open-book (with
supporting material) conditions. Our experimental results show significant
accuracy improvements in models that use the GPT-4.5-generated summaries,
particularly those with limited prior training in crystallography. OPENXRD uses
knowledge from larger models to fill knowledge gaps in crystallography and
shows that AI-generated texts can help smaller models reason more effectively
in scientific tasks. While the current version of OPENXRD focuses on text-based
inputs, we also explore future extensions such as adding real crystal diagrams
or diffraction patterns to improve interpretation in specialized materials
science contexts. Overall, OPENXRD shows that specialized open-book systems can
be useful in materials science and provides a foundation for broader natural
language processing (NLP) tools in critical scientific fields.

</details>


### [58] [PU-Lie: Lightweight Deception Detection in Imbalanced Diplomatic Dialogues via Positive-Unlabeled Learning](https://arxiv.org/abs/2507.09157)
*Bhavinkumar Vinodbhai Kuwar,Bikrant Bikram Pratap Maurya,Priyanshu Gupta,Nitin Choudhury*

Main category: cs.CL

TL;DR: 论文提出了一种轻量级模型PU-Lie，结合冻结BERT嵌入、可解释特征和PU学习目标，用于检测战略对话中的欺骗，在Diplomacy数据集上取得了最佳性能。


<details>
  <summary>Details</summary>
Motivation: 战略对话中欺骗检测任务复杂且重要，但数据集中欺骗标签极少（<5%），传统方法难以应对。

Method: 使用冻结BERT嵌入、可解释的语言和游戏特征，结合PU学习目标，针对少量标注欺骗消息的场景。

Result: 模型在Diplomacy数据集上达到0.60的宏F1分数，参数减少650倍。

Conclusion: PU学习、可解释特征和说话者感知表示对欺骗检测至关重要，且准确检测欺骗比识别真实消息更重要。

Abstract: Detecting deception in strategic dialogues is a complex and high-stakes task
due to the subtlety of language and extreme class imbalance between deceptive
and truthful communications. In this work, we revisit deception detection in
the Diplomacy dataset, where less than 5% of messages are labeled deceptive. We
introduce a lightweight yet effective model combining frozen BERT embeddings,
interpretable linguistic and game-specific features, and a Positive-Unlabeled
(PU) learning objective. Unlike traditional binary classifiers, PU-Lie is
tailored for situations where only a small portion of deceptive messages are
labeled, and the majority are unlabeled. Our model achieves a new best macro F1
of 0.60 while reducing trainable parameters by over 650x. Through comprehensive
evaluations and ablation studies across seven models, we demonstrate the value
of PU learning, linguistic interpretability, and speaker-aware representations.
Notably, we emphasize that in this problem setting, accurately detecting
deception is more critical than identifying truthful messages. This priority
guides our choice of PU learning, which explicitly models the rare but vital
deceptive class.

</details>


### [59] [RAMA: Retrieval-Augmented Multi-Agent Framework for Misinformation Detection in Multimodal Fact-Checking](https://arxiv.org/abs/2507.09174)
*Shuo Yang,Zijian Yu,Zhenzhe Ying,Yuqin Dai,Guoqing Wang,Jun Lan,Jinfeng Xu,Jinze Li,Edith C. H. Ngai*

Main category: cs.CL

TL;DR: RAMA是一个检索增强的多代理框架，用于验证多媒体虚假信息，通过精确查询、跨验证证据聚合和多代理架构提升性能。


<details>
  <summary>Details</summary>
Motivation: 多模态虚假信息的快速传播对自动化事实核查系统提出了挑战，尤其是当信息模糊或缺乏上下文时。

Method: RAMA采用三种创新方法：1) 将多模态声明转化为精确的搜索查询；2) 从权威来源聚合跨验证证据；3) 利用多代理架构结合多模态大语言模型的优势。

Result: 实验表明，RAMA在基准数据集上表现优异，尤其在处理模糊或不可信声明时，通过检索事实证据提升验证效果。

Conclusion: RAMA证明了整合网络证据和多代理推理对多媒体验证的重要性，为更可靠、可扩展的事实核查解决方案奠定了基础。

Abstract: The rapid proliferation of multimodal misinformation presents significant
challenges for automated fact-checking systems, especially when claims are
ambiguous or lack sufficient context. We introduce RAMA, a novel
retrieval-augmented multi-agent framework designed for verifying multimedia
misinformation. RAMA incorporates three core innovations: (1) strategic query
formulation that transforms multimodal claims into precise web search queries;
(2) cross-verification evidence aggregation from diverse, authoritative
sources; and (3) a multi-agent ensemble architecture that leverages the
complementary strengths of multiple multimodal large language models and prompt
variants. Extensive experiments demonstrate that RAMA achieves superior
performance on benchmark datasets, particularly excelling in resolving
ambiguous or improbable claims by grounding verification in retrieved factual
evidence. Our findings underscore the necessity of integrating web-based
evidence and multi-agent reasoning for trustworthy multimedia verification,
paving the way for more reliable and scalable fact-checking solutions. RAMA
will be publicly available at https://github.com/kalendsyang/RAMA.git.

</details>


### [60] [Detecting and Pruning Prominent but Detrimental Neurons in Large Language Models](https://arxiv.org/abs/2507.09185)
*Ameen Ali,Shahar Katz,Lior Wolf,Ivan Titov*

Main category: cs.CL

TL;DR: 提出了一种基于剪枝的微调方法，通过识别并移除与数据集特定机制相关的神经元，提升大语言模型的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 大语言模型（LLMs）常依赖数据集特定的相关性机制，导致在新任务或分布上性能下降。

Method: 使用Integrated Gradients量化神经元对高置信度预测的影响，剪枝与数据集特定机制相关的神经元。

Result: 在多项选择基准测试中，该方法显著提升了性能，优于之前的非剪枝适应方法。

Conclusion: 通过剪枝特定神经元，模型更依赖泛化性表征，提升了跨任务性能。

Abstract: Large language models (LLMs) often develop learned mechanisms specialized to
specific datasets, such as reliance on domain-specific correlations, which
yield high-confidence predictions without generalizable reasoning. While
beneficial in one setting, these dataset-specific mechanisms typically degrade
performance when models encounter novel tasks or distributions. In this work,
we introduce a fine-tuning approach designed to enhance generalization by
identifying and pruning neurons associated with dataset-specific mechanisms in
transformer-based LLMs. Our method employs Integrated Gradients to quantify
each neuron's influence on high-confidence predictions, pinpointing those that
disproportionately contribute to dataset-specific performance without
supporting robust, transferable reasoning. Selectively pruning these neurons
compels the model to depend on generalizable representations. Evaluated across
multiple-choice benchmarks, our pruning-based fine-tuning significantly
enhances performance, surpassing prior (non-pruning) adaptation methods.

</details>


### [61] [Banzhida: Advancing Large Language Models for Tibetan with Curated Data and Continual Pre-Training](https://arxiv.org/abs/2507.09205)
*Leiyu Pan,Bojian Xiong,Lei Yang,Renren Jin,Shaowei Zhang,Yue Chen,Ling Shi,Jiang Zhou,Junru Wu,Zhen Wang,Jianxiang Peng,Juesi Xiao,Tianyu Dong,Zhuowen Han,Zhuo Chen,Sangjee Dondrub,Caizang Tai,Haixing Zhao,Huaque Cairang,Suonan Cairang,Rou Te,Lengben Zhaxi,Gazang Zhaxi,Zhonglin Ye,Yuhui Zheng,Chunyan Peng,Secha Jia,Pema Tashi,Cizhen Jiacuo,Pema Dorjee,Hongkai Liu,Pema Yanggon,Tsehang Dorjee,Jiaxin Han,Qiongying Hu,Jilin Man,Huanke You,Yuqi Ren,Duo La,Deyi Xiong*

Main category: cs.CL

TL;DR: 论文提出了一个针对藏语的大规模预训练语料库，并基于此训练了一个多语言大模型Banzhida，显著提升了藏语生成AI的能力。


<details>
  <summary>Details</summary>
Motivation: 解决藏语作为低资源语言在现有大语言模型中的不足，缺乏高质量训练语料的问题。

Method: 收集并清洗多样化的藏语数据，构建大规模预训练语料库，并基于多语言基础模型进行预训练和后训练，得到Banzhida模型。

Result: Banzhida在多项任务中显著优于同类开源模型和针对藏语的定制模型。

Conclusion: 通过构建高质量藏语语料库和训练Banzhida模型，成功提升了藏语生成AI的性能。

Abstract: Large language models have achieved remarkable progress across many
languages. However, Tibetan, as a representative low-resource language, is
particularly underrepresented in existing models due to the scarcity of
high-quality training corpora. To address this gap, we curate the largest
Tibetan pre-training corpus to date, aggregating data from diverse sources and
applying a dedicated data cleaning and processing pipeline tailored for
Tibetan. With the curated data, we continue pre/post-training a multilingual
base model into Banzhida, a multilingual large language model that advances
generative AI for Tibetan. To evaluate the Tibetan capabilities of the model,
we create new high-quality Tibetan benchmarks, and complement them with
existing public benchmarks. Experimental results demonstrate that Banzhida
consistently and significantly outperforms both open-source models of similar
scale and Tibetan-tailored models across a wide range of tasks.

</details>


### [62] [MetaClimage: A novel database of visual metaphors related to Climate Change, with costs and benefits analysis](https://arxiv.org/abs/2507.09225)
*Biagio Scalingi,Chiara Barattieri di San Pietro,Paolo Canal,Valentina Bambini*

Main category: cs.CL

TL;DR: 研究分析了视觉隐喻（如融化的冰川表现为融化的冰手榴弹）在气候变化传播中的作用，发现其虽然理解难度更高，但能引发更深的认知加工和积极体验。


<details>
  <summary>Details</summary>
Motivation: 探讨视觉隐喻在气候变化传播中的效果，填补现有研究的空白。

Method: 创建MetaClimage数据库，收集人类对视觉隐喻和直白图像的评分，包括理解难度、效果、艺术质量和情感激发等。

Result: 视觉隐喻比直白图像更难理解但更美观，未显著提升效果或情感激发，但能引发更多标签和积极情绪。

Conclusion: 视觉隐喻虽增加认知负担，但可能促进更深层次的认知加工和积极体验，为环境传播提供权衡考量。

Abstract: Visual metaphors of climate change (e.g., melting glaciers depicted as a
melting ice grenade) are regarded as valuable tools for addressing the
complexity of environmental challenges. However, few studies have examined
their impact on communication, also due to scattered availability of material.
Here, we present a novel database of Metaphors of Climate Change in Images
(MetaClimage) https://doi.org/10.5281/zenodo.15861012, paired with literal
images and enriched with human ratings. For each image, we collected values of
difficulty, efficacy, artistic quality, and emotional arousal from human
rating, as well as number of tags generated by participants to summarize the
message. Semantic and emotion variables were further derived from the tags via
Natural Language Processing. Visual metaphors were rated as more difficult to
understand, yet more aesthetically pleasant than literal images, but did not
differ in efficacy and arousal. The latter for visual metaphors, however, was
higher in participants with higher Need For Cognition. Furthermore, visual
metaphors received more tags, often referring to entities not depicted in the
image, and elicited words with more positive valence and greater dominance than
literal images. These results evidence the greater cognitive load of visual
metaphors, which nevertheless might induce positive effects such as deeper
cognitive elaboration and abstraction compared to literal stimuli. Furthermore,
while they are not deemed as more effective and arousing, visual metaphors seem
to generate superior aesthetic appreciation and a more positively valenced
experience. Overall, this study contributes to understanding the impact of
visual metaphors of climate change both by offering a database for future
research and by elucidating a cost-benefit trade-off to take into account when
shaping environmental communication.

</details>


### [63] [Swa-bhasha Resource Hub: Romanized Sinhala to Sinhala Transliteration Systems and Data Resources](https://arxiv.org/abs/2507.09245)
*Deshan Sumanathilaka,Sameera Perera,Sachithya Dharmasiri,Maneesha Athukorala,Anuja Dilrukshi Herath,Rukshan Dias,Pasindu Gamage,Ruvan Weerasinghe,Y. H. P. P. Priyadarshana*

Main category: cs.CL

TL;DR: Swa-bhasha Resource Hub提供2020-2025年间开发的罗马化僧伽罗语转僧伽罗语的数据资源和算法，推动了僧伽罗语NLP研究，并公开了相关工具。


<details>
  <summary>Details</summary>
Motivation: 为僧伽罗语NLP研究提供罗马化与原生脚本间的转换资源，促进相关应用发展。

Method: 收集并开发罗马化僧伽罗语转僧伽罗语的数据集和算法，公开资源与工具。

Result: 建立了全面的资源库，支持了僧伽罗语NLP研究及应用的进展。

Conclusion: Swa-bhasha Resource Hub为僧伽罗语NLP领域提供了重要支持，推动了罗马化与原生脚本间的转换研究。

Abstract: The Swa-bhasha Resource Hub provides a comprehensive collection of data
resources and algorithms developed for Romanized Sinhala to Sinhala
transliteration between 2020 and 2025. These resources have played a
significant role in advancing research in Sinhala Natural Language Processing
(NLP), particularly in training transliteration models and developing
applications involving Romanized Sinhala. The current openly accessible data
sets and corresponding tools are made publicly available through this hub. This
paper presents a detailed overview of the resources contributed by the authors
and includes a comparative analysis of existing transliteration applications in
the domain.

</details>


### [64] [ClaritySpeech: Dementia Obfuscation in Speech](https://arxiv.org/abs/2507.09282)
*Dominika Woszczyk,Ranya Aloufi,Soteris Demetriou*

Main category: cs.CL

TL;DR: 论文提出了一种名为ClaritySpeech的新框架，通过结合ASR、文本混淆和零样本TTS技术，改善痴呆症患者的语音识别和隐私保护，在低数据环境下无需微调即可实现。


<details>
  <summary>Details</summary>
Motivation: 痴呆症患者的语音模式变化导致沟通障碍和隐私问题，现有语音技术难以处理此类非典型语音。

Method: 整合自动语音转录（ASR）、文本混淆和零样本文本转语音（TTS）技术，构建ClaritySpeech框架。

Result: 在ADReSS和ADReSSo数据集上，F1分数分别下降16%和10%，但保持了50%的说话人相似性；同时显著降低了WER并提高了语音质量。

Conclusion: ClaritySpeech在改善痴呆症患者语音识别和隐私保护方面表现出色，尤其在低数据环境下具有潜力。

Abstract: Dementia, a neurodegenerative disease, alters speech patterns, creating
communication barriers and raising privacy concerns. Current speech
technologies, such as automatic speech transcription (ASR), struggle with
dementia and atypical speech, further challenging accessibility. This paper
presents a novel dementia obfuscation in speech framework, ClaritySpeech,
integrating ASR, text obfuscation, and zero-shot text-to-speech (TTS) to
correct dementia-affected speech while preserving speaker identity in low-data
environments without fine-tuning. Results show a 16% and 10% drop in mean F1
score across various adversarial settings and modalities (audio, text, fusion)
for ADReSS and ADReSSo, respectively, maintaining 50% speaker similarity. We
also find that our system improves WER (from 0.73 to 0.08 for ADReSS and 0.15
for ADReSSo) and speech quality from 1.65 to ~2.15, enhancing privacy and
accessibility.

</details>


### [65] [Psychology-Driven Enhancement of Humour Translation](https://arxiv.org/abs/2507.09259)
*Yuchen Su,Yonghua Zhu,Yang Chen,Diana Benavides-Prado,Michael Witbrock*

Main category: cs.CL

TL;DR: 本文提出了一种心理学启发的幽默分解机制（HDM），通过模仿人类思维过程，结合幽默理论，显著提升了大型语言模型在幽默翻译中的表现。


<details>
  <summary>Details</summary>
Motivation: 幽默翻译是跨文化交流的重要桥梁，但现有大型语言模型在幽默翻译中存在语言干扰和幽默感缺失的问题。

Method: 提出HDM机制，利用Chain-of-Thought模仿人类思维，并结合幽默理论优化翻译文本的幽默感和可读性。

Result: 实验表明，HDM在幽默翻译中显著提升幽默感（7.75%）、流畅性（2.81%）和连贯性（6.13%）。

Conclusion: HDM为幽默翻译提供了一种有效方法，显著提升了翻译质量。

Abstract: Humour translation plays a vital role as a bridge between different cultures,
fostering understanding and communication. Although most existing Large
Language Models (LLMs) are capable of general translation tasks, these models
still struggle with humour translation, which is especially reflected through
linguistic interference and lacking humour in translated text. In this paper,
we propose a psychology-inspired Humour Decomposition Mechanism (HDM) that
utilises Chain-of-Thought (CoT) to imitate the ability of the human thought
process, stimulating LLMs to optimise the readability of translated humorous
texts. Moreover, we integrate humour theory in HDM to further enhance the
humorous elements in the translated text. Our automatic evaluation experiments
on open-source humour datasets demonstrate that our method significantly
improves the quality of humour translation, yielding average gains of 7.75\% in
humour, 2.81\% in fluency, and 6.13\% in coherence of the generated text.

</details>


### [66] [DATE-LM: Benchmarking Data Attribution Evaluation for Large Language Models](https://arxiv.org/abs/2507.09424)
*Cathy Jiao,Yijun Pan,Emily Xiao,Daisy Sheng,Niket Jain,Hanzhang Zhao,Ishita Dasgupta,Jiaqi W. Ma,Chenyan Xiong*

Main category: cs.CL

TL;DR: DATE-LM是一个统一的基准，用于评估语言模型中的数据归因方法，通过三个关键任务衡量归因质量，并发现现有方法在不同任务中存在权衡。


<details>
  <summary>Details</summary>
Motivation: 数据归因方法在LLM研究和应用中日益重要，但缺乏系统化的评估标准，因此需要统一的基准来填补这一空白。

Method: 引入DATE-LM基准，通过训练数据选择、毒性/偏见过滤和事实归因三个任务评估数据归因方法，支持大规模跨任务和架构的评估。

Result: 研究发现，没有单一方法在所有任务中表现最优，现有方法与简单基线存在权衡，且方法性能对任务设计敏感。

Conclusion: DATE-LM为未来LLM数据归因研究奠定了基础，并发布了公开排行榜以促进社区参与。

Abstract: Data attribution methods quantify the influence of training data on model
outputs and are becoming increasingly relevant for a wide range of LLM research
and applications, including dataset curation, model interpretability, data
valuation. However, there remain critical gaps in systematic LLM-centric
evaluation of data attribution methods. To this end, we introduce DATE-LM (Data
Attribution Evaluation in Language Models), a unified benchmark for evaluating
data attribution methods through real-world LLM applications. DATE-LM measures
attribution quality through three key tasks -- training data selection,
toxicity/bias filtering, and factual attribution. Our benchmark is designed for
ease of use, enabling researchers to configure and run large-scale evaluations
across diverse tasks and LLM architectures. Furthermore, we use DATE-LM to
conduct a large-scale evaluation of existing data attribution methods. Our
findings show that no single method dominates across all tasks, data
attribution methods have trade-offs with simpler baselines, and method
performance is sensitive to task-specific evaluation design. Finally, we
release a public leaderboard for quick comparison of methods and to facilitate
community engagement. We hope DATE-LM serves as a foundation for future data
attribution research in LLMs.

</details>


### [67] [Enhancing Clinical Text Classification via Fine-Tuned DRAGON Longformer Models](https://arxiv.org/abs/2507.09470)
*Mingchuan Yang,Ziyuan Huang*

Main category: cs.CL

TL;DR: 优化DRAGON Longformer模型用于临床文本分类，性能显著提升。


<details>
  <summary>Details</summary>
Motivation: 探索如何优化预训练模型以适应临床文本分类任务，提升医疗案例描述的二元分类效果。

Method: 通过超参数调优、领域特定预处理和架构调整（如增加序列长度、调整学习率和训练轮次）优化模型。

Result: 优化后模型性能显著提升：准确率从72.0%升至85.2%，其他指标（精确率、召回率、F1分数）也有显著提高。

Conclusion: 优化模型在医疗术语和临床观察方面表现优异，具有广泛医疗应用潜力。

Abstract: This study explores the optimization of the DRAGON Longformer base model for
clinical text classification, specifically targeting the binary classification
of medical case descriptions. A dataset of 500 clinical cases containing
structured medical observations was used, with 400 cases for training and 100
for validation. Enhancements to the pre-trained
joeranbosma/dragon-longformer-base-mixed-domain model included hyperparameter
tuning, domain-specific preprocessing, and architectural adjustments. Key
modifications involved increasing sequence length from 512 to 1024 tokens,
adjusting learning rates from 1e-05 to 5e-06, extending training epochs from 5
to 8, and incorporating specialized medical terminology. The optimized model
achieved notable performance gains: accuracy improved from 72.0% to 85.2%,
precision from 68.0% to 84.1%, recall from 75.0% to 86.3%, and F1-score from
71.0% to 85.2%. Statistical analysis confirmed the significance of these
improvements (p < .001). The model demonstrated enhanced capability in
interpreting medical terminology, anatomical measurements, and clinical
observations. These findings contribute to domain-specific language model
research and offer practical implications for clinical natural language
processing applications. The optimized model's strong performance across
diverse medical conditions underscores its potential for broad use in
healthcare settings.

</details>


### [68] [The CoNLL-2013 Shared Task on Grammatical Error Correction](https://arxiv.org/abs/2507.09474)
*Hwee Tou Ng,Siew Mei Wu,Yuanbin Wu,Christian Hadiwinoto,Joel Tetreault*

Main category: cs.CL

TL;DR: 本文介绍了CoNLL-2013共享任务，包括任务定义、数据集、评估指标及参与团队的方法和结果。


<details>
  <summary>Details</summary>
Motivation: 共享任务旨在促进语法错误纠正领域的研究，通过提供统一的数据集和评估标准。

Method: 描述了任务定义、数据集、评估指标及参与团队采用的不同方法。

Result: 展示了参与团队的评估结果。

Conclusion: 总结了共享任务的成果，为语法错误纠正研究提供了基准。

Abstract: The CoNLL-2013 shared task was devoted to grammatical error correction. In
this paper, we give the task definition, present the data sets, and describe
the evaluation metric and scorer used in the shared task. We also give an
overview of the various approaches adopted by the participating teams, and
present the evaluation results.

</details>


### [69] [Towards Agentic RAG with Deep Reasoning: A Survey of RAG-Reasoning Systems in LLMs](https://arxiv.org/abs/2507.09477)
*Yangning Li,Weizhi Zhang,Yuyao Yang,Wei-Chieh Huang,Yaozu Wu,Junyu Luo,Yuanchen Bei,Henry Peng Zou,Xiao Luo,Yusheng Zhao,Chunkit Chan,Yankai Chen,Zhongfen Deng,Yinghui Li,Hai-Tao Zheng,Dongyuan Li,Renhe Jiang,Ming Zhang,Yangqiu Song,Philip S. Yu*

Main category: cs.CL

TL;DR: 该论文综述了检索增强生成（RAG）与推理方法的结合，提出了统一的推理-检索视角，并探讨了如何通过协同优化提升性能。


<details>
  <summary>Details</summary>
Motivation: 解决RAG在多步推理任务中的不足以及纯推理方法的事实错误问题，推动更高效、可信的RAG-推理系统发展。

Method: 通过分析推理如何优化RAG的各个阶段（Reasoning-Enhanced RAG），以及检索知识如何支持复杂推理（RAG-Enhanced Reasoning），最终提出协同框架（Synergized RAG-Reasoning）。

Result: 展示了协同框架在知识密集型任务中的最新性能，并总结了方法、数据集和开放挑战。

Conclusion: 未来的研究方向包括更高效、多模态适应、可信且以人为中心的RAG-推理系统。

Abstract: Retrieval-Augmented Generation (RAG) lifts the factuality of Large Language
Models (LLMs) by injecting external knowledge, yet it falls short on problems
that demand multi-step inference; conversely, purely reasoning-oriented
approaches often hallucinate or mis-ground facts. This survey synthesizes both
strands under a unified reasoning-retrieval perspective. We first map how
advanced reasoning optimizes each stage of RAG (Reasoning-Enhanced RAG). Then,
we show how retrieved knowledge of different type supply missing premises and
expand context for complex inference (RAG-Enhanced Reasoning). Finally, we
spotlight emerging Synergized RAG-Reasoning frameworks, where (agentic) LLMs
iteratively interleave search and reasoning to achieve state-of-the-art
performance across knowledge-intensive benchmarks. We categorize methods,
datasets, and open challenges, and outline research avenues toward deeper
RAG-Reasoning systems that are more effective, multimodally-adaptive,
trustworthy, and human-centric. The collection is available at
https://github.com/DavidZWZ/Awesome-RAG-Reasoning.

</details>


### [70] [ViSP: A PPO-Driven Framework for Sarcasm Generation with Contrastive Learning](https://arxiv.org/abs/2507.09482)
*Changli Wang,Rui Wu,Fang Yin*

Main category: cs.CL

TL;DR: 论文提出M2SaG数据集和ViSP框架，用于多模态讽刺生成，通过PPO和对比学习提升生成质量，实验表明ViSP优于基线模型。


<details>
  <summary>Details</summary>
Motivation: 现有讽刺生成研究过度依赖文本模态且忽视视觉线索，数据集也存在图像内容与讽刺意图不匹配的问题。

Method: 提出M2SaG数据集（4,970样本），并设计ViSP框架，结合PPO和对比学习优化讽刺文本生成。

Result: ViSP在五个指标集上超越基线模型，生成文本的讽刺分数（0.898 vs. 0.770）和事实不一致性（0.768 vs. 0.739）更高。

Conclusion: ViSP能生成更高质量的讽刺内容，数据集和代码将公开。

Abstract: Human emotions are complex, with sarcasm being a subtle and distinctive form.
Despite progress in sarcasm research, sarcasm generation remains underexplored,
primarily due to the overreliance on textual modalities and the neglect of
visual cues, as well as the mismatch between image content and sarcastic intent
in existing datasets. In this paper, we introduce M2SaG, a multimodal sarcasm
generation dataset with 4,970 samples, each containing an image, a sarcastic
text, and a sarcasm target. To benchmark M2SaG, we propose ViSP, a generation
framework that integrates Proximal Policy Optimization (PPO) and contrastive
learning. PPO utilizes reward scores from DIP to steer the generation of
sarcastic texts, while contrastive learning encourages the model to favor
outputs with higher reward scores. These strategies improve overall generation
quality and produce texts with more pronounced sarcastic intent. We evaluate
ViSP across five metric sets and find it surpasses all baselines, including
large language models, underscoring their limitations in sarcasm generation.
Furthermore, we analyze the distributions of Sarcasm Scores and Factual
Incongruity for both M2SaG and the texts generated by ViSP. The generated texts
exhibit higher mean Sarcasm Scores (0.898 vs. 0.770) and Factual Incongruity
(0.768 vs. 0.739), demonstrating that ViSP produces higher-quality sarcastic
content than the original dataset. % The dataset and code will be publicly
available. Our dataset and code will be released at
\textit{https://github.com/wclapply/ViSP}.

</details>


### [71] [Balanced Training Data Augmentation for Aspect-Based Sentiment Analysis](https://arxiv.org/abs/2507.09485)
*Junjie Liu,Yuanhe Tian,Yan Song*

Main category: cs.CL

TL;DR: 提出了一种基于大语言模型（LLM）的数据增强方法，用于改进方面级情感分析（ABSA）任务，通过生成平衡且高质量的合成数据提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 现有研究在短文本和小规模不平衡数据上面临挑战，需要更丰富且平衡的训练数据以提升模型效果。

Method: 利用LLM生成增强数据，并通过强化学习优化数据质量，构建更大规模且标签平衡的训练集。

Result: 在英文基准数据集上表现优于现有方法，验证了方法的有效性。

Conclusion: LLM结合强化学习的数据增强策略能显著提升ABSA任务的性能。

Abstract: Aspect-based sentiment analysis (ABSA) is a crucial fine-grained task in
social media scenarios to identify the sentiment polarity of specific aspect
terms in a sentence. Although many existing studies leverage large language
models (LLMs) to perform ABSA due to their strong context understanding
capabilities, they still face challenges to learn the context information in
the running text because of the short text, as well as the small and unbalanced
labeled training data, where most data are labeled with positive sentiment.
Data augmentation (DA) is a feasible strategy for providing richer contextual
information, especially when using LLMs to create synthetic training data, but
faces challenges in ensuring a high quality of the augmented data.In this
paper, we propose an LLM-based ABSA approach with training data
augmentation.Specifically, an LLM is prompted to generate augmented training
data based on the original training data, so as to construct a new training
data with larger size and balanced label distributions to better train an ABSA
model. Meanwhile, in order to improve the quality of the augmented data, we
propose a reinforcement learning approach to optimize the data augmentation.
LLM.Experiment results and further analyses on English benchmark datasets for
ABSA demonstrate the effectiveness of our approach, where superior performance
is observed over strong baselines and most existing studies.

</details>


### [72] [GoalfyMax: A Protocol-Driven Multi-Agent System for Intelligent Experience Entities](https://arxiv.org/abs/2507.09497)
*Siyi Wu,Zeyu Wang,Xinyuan Song,Zhengpeng Zhou,Lifan Sun,Tianyu Shi*

Main category: cs.CL

TL;DR: GoalfyMax是一个多智能体协作框架，通过A2A通信层和XP架构解决传统AI系统协调和记忆复用不足的问题，实现高效任务分解和持续学习。


<details>
  <summary>Details</summary>
Motivation: 现代企业需要智能系统处理复杂动态任务，但传统AI系统缺乏协调和记忆复用能力，限制了可扩展性。

Method: GoalfyMax基于MCP协议实现A2A通信，结合XP架构实现知识保留和持续学习，支持多轮对话和动态安全验证。

Result: 实验表明GoalfyMax在适应性、协调性和经验复用上优于基线框架。

Conclusion: GoalfyMax为多智能体系统提供了可扩展的未来基础。

Abstract: Modern enterprise environments demand intelligent systems capable of handling
complex, dynamic, and multi-faceted tasks with high levels of autonomy and
adaptability. However, traditional single-purpose AI systems often lack
sufficient coordination, memory reuse, and task decomposition capabilities,
limiting their scalability in realistic settings. To address these challenges,
we present \textbf{GoalfyMax}, a protocol-driven framework for end-to-end
multi-agent collaboration. GoalfyMax introduces a standardized Agent-to-Agent
(A2A) communication layer built on the Model Context Protocol (MCP), allowing
independent agents to coordinate through asynchronous, protocol-compliant
interactions. It incorporates the Experience Pack (XP) architecture, a layered
memory system that preserves both task rationales and execution traces,
enabling structured knowledge retention and continual learning. Moreover, our
system integrates advanced features including multi-turn contextual dialogue,
long-short term memory modules, and dynamic safety validation, supporting
robust, real-time strategy adaptation. Empirical results on complex task
orchestration benchmarks and case study demonstrate that GoalfyMax achieves
superior adaptability, coordination, and experience reuse compared to baseline
frameworks. These findings highlight its potential as a scalable, future-ready
foundation for multi-agent intelligent systems.

</details>


### [73] [Ref-Long: Benchmarking the Long-context Referencing Capability of Long-context Language Models](https://arxiv.org/abs/2507.09506)
*Junjie Wu,Gefei Gu,Yanan Zheng,Dit-Yan Yeung,Arman Cohan*

Main category: cs.CL

TL;DR: 该论文提出了Ref-Long基准，用于评估长上下文语言模型（LCLMs）的长上下文引用能力，发现即使是先进模型如GPT-4o也存在显著不足。


<details>
  <summary>Details</summary>
Motivation: 长上下文引用是LCLMs的关键任务，但目前研究不足，因此需要开发专门的评估基准。

Method: 提出Ref-Long基准，包含从合成到现实的三个子集，要求模型识别引用特定键的文档索引。

Result: 实验显示13种LCLMs在长上下文引用任务中表现不佳，通过多种分析揭示了问题。

Conclusion: Ref-Long基准揭示了LCLMs在长上下文引用中的局限性，为未来研究提供了方向。

Abstract: Long-context language models (LCLMs) have exhibited impressive capabilities
in long-context understanding tasks. Among these, long-context referencing -- a
crucial task that requires LCLMs to attribute items of interest to specific
parts of long-context data -- remains underexplored. To bridge this gap, this
paper proposes Referencing Evaluation for Long-context Language Models
(Ref-Long), a novel benchmark designed to assess the long-context referencing
capability of LCLMs. Specifically, Ref-Long requires LCLMs to identify the
indexes of documents that reference a specific key, emphasizing contextual
relationships between the key and the documents over simple retrieval. Based on
the task design, we construct three subsets ranging from synthetic to realistic
scenarios to form the Ref-Long benchmark. Experimental results of 13 LCLMs
reveal significant shortcomings in long-context referencing, even among
advanced models like GPT-4o. To further investigate these challenges, we
conduct comprehensive analyses, including human evaluations, task format
adjustments, fine-tuning experiments, and error analyses, leading to several
key insights. Our data and code can be found in https://github.
com/wujunjie1998/Ref-Long.

</details>


### [74] [How Important is `Perfect' English for Machine Translation Prompts?](https://arxiv.org/abs/2507.09509)
*Patrícia Schmidtová,Niyati Bafna,Seth Aycock,Gianluca Vico,Wiktor Kamzela,Katharina Hämmerl,Vilém Zouhar*

Main category: cs.CL

TL;DR: 论文研究了提示质量对大型语言模型（LLM）在机器翻译和翻译评估任务中表现的影响，发现错误类型和数量对性能有显著差异。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在机器翻译中表现优异，但对提示中的错误和扰动敏感，因此需要系统评估这些因素对性能的影响。

Method: 通过定量和定性分析，评估人类合理错误和合成错误对LLM在翻译和翻译评估任务中的表现影响。

Result: 提示质量显著影响翻译性能，不同噪声类型对性能影响不同；LLM在极端噪声下仍能翻译。

Conclusion: 提示质量主要通过影响指令遵循而非翻译质量本身；LLM在噪声环境下仍具鲁棒性。

Abstract: Large language models (LLMs) have achieved top results in recent machine
translation evaluations, but they are also known to be sensitive to errors and
perturbations in their prompts. We systematically evaluate how both humanly
plausible and synthetic errors in user prompts affect LLMs' performance on two
related tasks: Machine translation and machine translation evaluation. We
provide both a quantitative analysis and qualitative insights into how the
models respond to increasing noise in the user prompt.
  The prompt quality strongly affects the translation performance: With many
errors, even a good prompt can underperform a minimal or poor prompt without
errors. However, different noise types impact translation quality differently,
with character-level and combined noisers degrading performance more than
phrasal perturbations. Qualitative analysis reveals that lower prompt quality
largely leads to poorer instruction following, rather than directly affecting
translation quality itself. Further, LLMs can still translate in scenarios with
overwhelming random noise that would make the prompt illegible to humans.

</details>


### [75] [Adapting Definition Modeling for New Languages: A Case Study on Belarusian](https://arxiv.org/abs/2507.09536)
*Daniela Kazakouskaya,Timothee Mickus,Janine Siewert*

Main category: cs.CL

TL;DR: 本文探讨了如何利用现有模型为白俄罗斯语生成定义，并提出一个包含43,150条定义的新数据集。实验表明，适应定义建模系统需要的数据量较少，但自动指标存在不足。


<details>
  <summary>Details</summary>
Motivation: 定义建模任务有助于辅助词典编纂工作，但目前对于未支持语言的模型利用仍需评估。

Method: 通过提出一个白俄罗斯语的新数据集，并利用现有模型进行适应。

Result: 实验表明，适应定义建模系统所需数据量较少，但自动指标未能完全捕捉模型表现。

Conclusion: 当前自动指标在评估定义建模系统时存在不足，需要进一步改进。

Abstract: Definition modeling, the task of generating new definitions for words in
context, holds great prospect as a means to assist the work of lexicographers
in documenting a broader variety of lects and languages, yet much remains to be
done in order to assess how we can leverage pre-existing models for as-of-yet
unsupported languages. In this work, we focus on adapting existing models to
Belarusian, for which we propose a novel dataset of 43,150 definitions. Our
experiments demonstrate that adapting a definition modeling systems requires
minimal amounts of data, but that there currently are gaps in what automatic
metrics do capture.

</details>


### [76] [NMIXX: Domain-Adapted Neural Embeddings for Cross-Lingual eXploration of Finance](https://arxiv.org/abs/2507.09601)
*Hanwool Lee,Sara Yu,Yewon Hwang,Jonghyun Choi,Heejae Ahn,Sungbum Jung,Youngjae Yu*

Main category: cs.CL

TL;DR: NMIXX是一种针对金融领域的跨语言嵌入模型，通过微调高置信度三元组提升金融语义捕捉能力，并在低资源语言（如韩语）中表现优异。


<details>
  <summary>Details</summary>
Motivation: 通用句子嵌入模型难以捕捉金融领域的专业语义，尤其是在低资源语言中，存在领域术语、时间语义变化和双语词汇不对齐等问题。

Method: 提出NMIXX模型，利用18.8K高置信度三元组（包括领域内释义、语义变化的硬负例和韩英精确翻译）进行微调，并发布KorFinSTS基准数据集。

Result: NMIXX在英语FinSTS和韩语KorFinSTS上分别提升Spearman's rho 0.10和0.22，优于其他基线模型，但通用STS性能略有下降。

Conclusion: NMIXX和KorFinSTS为金融领域的跨语言表示学习提供了有效工具，强调了分词设计在低资源语言中的重要性。

Abstract: General-purpose sentence embedding models often struggle to capture
specialized financial semantics, especially in low-resource languages like
Korean, due to domain-specific jargon, temporal meaning shifts, and misaligned
bilingual vocabularies. To address these gaps, we introduce NMIXX (Neural
eMbeddings for Cross-lingual eXploration of Finance), a suite of cross-lingual
embedding models fine-tuned with 18.8K high-confidence triplets that pair
in-domain paraphrases, hard negatives derived from a semantic-shift typology,
and exact Korean-English translations. Concurrently, we release KorFinSTS, a
1,921-pair Korean financial STS benchmark spanning news, disclosures, research
reports, and regulations, designed to expose nuances that general benchmarks
miss.
  When evaluated against seven open-license baselines, NMIXX's multilingual
bge-m3 variant achieves Spearman's rho gains of +0.10 on English FinSTS and
+0.22 on KorFinSTS, outperforming its pre-adaptation checkpoint and surpassing
other models by the largest margin, while revealing a modest trade-off in
general STS performance. Our analysis further shows that models with richer
Korean token coverage adapt more effectively, underscoring the importance of
tokenizer design in low-resource, cross-lingual settings. By making both models
and the benchmark publicly available, we provide the community with robust
tools for domain-adapted, multilingual representation learning in finance.

</details>


### [77] [SpreadPy: A Python tool for modelling spreading activation and superdiffusion in cognitive multiplex networks](https://arxiv.org/abs/2507.09628)
*Salvatore Citraro,Edith Haim,Alessandra Carini,Cynthia S. Q. Siew,Giulio Rossetti,Massimo Stella*

Main category: cs.CL

TL;DR: SpreadPy是一个Python库，用于模拟认知单层和多层网络中的激活传播，支持结构-功能关系的数值模拟，并通过案例研究验证其应用。


<details>
  <summary>Details</summary>
Motivation: 开发SpreadPy的目的是为了系统研究激活动态如何反映认知、心理和临床现象，并与知识建模的理论进行比较。

Method: 通过数值模拟测试结构-功能关系，使用案例研究验证工具的有效性。

Result: 案例研究表明SpreadPy能区分数学焦虑学生的概念组织差异，揭示认知负荷对词汇访问的影响，并关联失语症患者的网络结构与临床错误类型。

Conclusion: SpreadPy为心理学、神经科学和教育研究提供了灵活的模拟框架，支持可重复研究。

Abstract: We introduce SpreadPy as a Python library for simulating spreading activation
in cognitive single-layer and multiplex networks. Our tool is designed to
perform numerical simulations testing structure-function relationships in
cognitive processes. By comparing simulation results with grounded theories in
knowledge modelling, SpreadPy enables systematic investigations of how
activation dynamics reflect cognitive, psychological and clinical phenomena. We
demonstrate the library's utility through three case studies: (1) Spreading
activation on associative knowledge networks distinguishes students with high
versus low math anxiety, revealing anxiety-related structural differences in
conceptual organization; (2) Simulations of a creativity task show that
activation trajectories vary with task difficulty, exposing how cognitive load
modulates lexical access; (3) In individuals with aphasia, simulated activation
patterns on lexical networks correlate with empirical error types (semantic vs.
phonological) during picture-naming tasks, linking network structure to
clinical impairments. SpreadPy's flexible framework allows researchers to model
these processes using empirically derived or theoretical networks, providing
mechanistic insights into individual differences and cognitive impairments. The
library is openly available, supporting reproducible research in psychology,
neuroscience, and education research.

</details>


### [78] [An Exploration of Knowledge Editing for Arabic](https://arxiv.org/abs/2507.09629)
*Basel Mousi,Nadir Durrani,Fahim Dalvi*

Main category: cs.CL

TL;DR: 本文首次研究了阿拉伯语知识编辑（KE），评估了四种方法在阿拉伯语翻译数据集上的表现，发现参数化方法在跨语言泛化上表现较差，而指令调优方法更稳健。


<details>
  <summary>Details</summary>
Motivation: 探索知识编辑在形态丰富的语言（如阿拉伯语）中的行为，填补现有研究的空白。

Method: 评估了ROME、MEMIT、ICE和LTE四种方法，并在Llama-2-7B-chat模型上进行了实验，扩展了LTE到多语言环境。

Result: 参数化方法在跨语言泛化上表现不佳，而指令调优方法表现更稳健；多语言联合训练提升了编辑和迁移能力。

Conclusion: 阿拉伯语知识编辑的研究为未来工作提供了基准和多语言训练数据，支持进一步探索。

Abstract: While Knowledge Editing (KE) has been widely explored in English, its
behavior in morphologically rich languages like Arabic remains underexamined.
In this work, we present the first study of Arabic KE. We evaluate four methods
(ROME, MEMIT, ICE, and LTE) on Arabic translations of the ZsRE and Counterfact
benchmarks, analyzing both multilingual and cross-lingual settings. Our
experiments on Llama-2-7B-chat show show that parameter-based methods struggle
with cross-lingual generalization, while instruction-tuned methods perform more
robustly. We extend Learning-To-Edit (LTE) to a multilingual setting and show
that joint Arabic-English training improves both editability and transfer. We
release Arabic KE benchmarks and multilingual training for LTE data to support
future research.

</details>


### [79] [Can Group Relative Policy Optimization Improve Thai Legal Reasoning and Question Answering?](https://arxiv.org/abs/2507.09638)
*Pawitsapak Akarajaradwong,Chompakorn Chaksangchaichot,Pirat Pothavorn,Attapol Thamrongrattanarit-Rutherford,Ekapol Chuangsuwanich,Sarana Nutanong*

Main category: cs.CL

TL;DR: 论文提出了一种基于GRPO的方法，通过BGE-M3嵌入提升泰国法律问答系统中LLM的法律引用准确性和回答质量，显著降低了计算成本。


<details>
  <summary>Details</summary>
Motivation: 泰国法律问答系统中RAG的表现有限，尤其是在需要复杂法律推理的问题上。

Method: 采用Group-Relative Policy Optimization (GRPO)方法，结合BGE-M3嵌入作为语义相似性奖励，减少计算成本。

Result: 在NitiBench基准测试中，GRPO实现了90%的引用F1提升和31%的联合质量指标提升，且在复杂法律推理任务中表现更稳健。

Conclusion: GRPO为提升泰国法律LLM提供了一种高效且资源节约的解决方案。

Abstract: The Retrieval-Augmented Generation (RAG) systems' performance on Thai legal
question answering is still limited, especially for questions requiring
extensive, complex legal reasoning. To address these limitations, we introduce
an approach aligning LLMs toward improved law citation accuracy and better
response quality using Group-Relative Policy Optimization (GRPO). Our approach
leverages BGE-M3 embeddings as a cost-efficient semantic-similarity reward,
significantly reducing computational expenses up to 2.5x compared to large
language model judges. Experiments on the NitiBench benchmark demonstrate
substantial improvements: GRPO achieves up to 90% citation-F1 gains from the
base model and a 31% increase in joint quality metrics over instruction tuning.
Crucially, our method shows enhanced robustness on complex legal reasoning
tasks compared to instruction tuning, providing an effective and
resource-efficient solution for enhancing Thai legal LLMs.

</details>


### [80] [MCEval: A Dynamic Framework for Fair Multilingual Cultural Evaluation of LLMs](https://arxiv.org/abs/2507.09701)
*Shulin Huang,Linyi Yang,Yue Zhang*

Main category: cs.CL

TL;DR: MCEval是一个多语言文化评估框架，用于评估大型语言模型的文化偏见和跨文化理解能力。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型存在文化偏见和跨文化理解能力不足的问题，尤其是在服务全球多样化用户时。

Method: 提出MCEval框架，采用动态文化问题构建和因果分析（反事实重述和混杂重述），覆盖13种文化和语言。

Result: 评估结果显示性能差异与训练数据分布和语言文化对齐相关，并揭示了公平性问题。

Conclusion: MCEval是首个全面的多语言文化评估框架，为理解LLMs的文化理解提供了深入见解。

Abstract: Large language models exhibit cultural biases and limited cross-cultural
understanding capabilities, particularly when serving diverse global user
populations. We propose MCEval, a novel multilingual evaluation framework that
employs dynamic cultural question construction and enables causal analysis
through Counterfactual Rephrasing and Confounder Rephrasing. Our comprehensive
evaluation spans 13 cultures and 13 languages, systematically assessing both
cultural awareness and cultural bias across different linguistic scenarios. The
framework provides 39,897 cultural awareness instances and 17,940 cultural bias
instances. Experimental results reveal performance disparities across different
linguistic scenarios, demonstrating that optimal cultural performance is not
only linked to training data distribution, but also is related to
language-culture alignment. The evaluation results also expose the fairness
issue, where approaches appearing successful in the English scenario create
substantial disadvantages. MCEval represents the first comprehensive
multilingual cultural evaluation framework that provides deeper insights into
LLMs' cultural understanding.

</details>


### [81] [Large Language Models Encode Semantics in Low-Dimensional Linear Subspaces](https://arxiv.org/abs/2507.09709)
*Baturay Saglam,Paul Kassianik,Blaine Nelson,Sajana Weerawardhena,Yaron Singer,Amin Karbasi*

Main category: cs.CL

TL;DR: 研究发现大语言模型（LLM）的潜在空间中，高级语义信息集中在低维子空间，且在不同领域形成线性可分的表示。这种可分性在深层和结构化推理提示下更明显，支持几何感知工具的开发。


<details>
  <summary>Details</summary>
Motivation: 理解LLM潜在空间的几何结构有助于解释其行为并改进对齐性，但目前尚不清楚LLM内部如何组织与语义理解相关的表示。

Method: 对11个基于Transformer的解码器模型进行大规模实证研究，分析6个科学主题和12层的隐藏状态。

Result: 发现高级语义信息集中在低维子空间，可分性在深层和结构化推理提示下更明显，支持简单有效的因果干预。

Conclusion: 研究支持开发几何感知工具，直接操作潜在表示以检测和缓解有害内容，如通过轻量级MLP分类器实现潜在空间防护。

Abstract: Understanding the latent space geometry of large language models (LLMs) is
key to interpreting their behavior and improving alignment. \baturay{However,
it remains unclear to what extent LLMs internally organize representations
related to semantic understanding. To investigate this, we conduct a
large-scale empirical study of hidden states in transformer-based LLMs,
analyzing 11 decoder-only models across 6 scientific topics and 12 layers each.
We find that high-level semantic information consistently lies in
low-dimensional subspaces that form linearly separable representations across
distinct domains. This separability becomes more pronounced in deeper layers
and under prompts that trigger structured reasoning or alignment
behaviors$\unicode{x2013}$even when surface content is unchanged. This geometry
enables simple yet effective causal interventions in hidden space; for example,
reasoning patterns like chain-of-thought can be captured by a single vector
direction. Together, these findings support the development of geometry-aware
tools that operate directly on latent representations to detect and mitigate
harmful or adversarial content, using methods such as transport-based defenses
that leverage this separability. As a proof of concept, we demonstrate this
potential by training a simple MLP classifier as a lightweight latent-space
guardrail, which detects adversarial and malicious prompts with high precision.

</details>


### [82] [Your Pretrained Model Tells the Difficulty Itself: A Self-Adaptive Curriculum Learning Paradigm for Natural Language Understanding](https://arxiv.org/abs/2507.09758)
*Qi Feng,Yihong Liu,Hinrich Schütze*

Main category: cs.CL

TL;DR: 论文提出了一种自适应的课程学习范式，利用预训练语言模型预测难度分数，优化了自然语言处理任务的训练策略。


<details>
  <summary>Details</summary>
Motivation: 现有课程学习方法依赖人工定义的难度指标（如文本长度），可能无法准确反映模型视角，因此需要一种更自适应的方式。

Method: 通过预训练语言模型预测样本难度分数，并探索了从易到难、从难到易及混合采样的不同训练策略。

Result: 在四个自然语言理解数据集上的实验表明，该方法比随机采样收敛更快且性能更优。

Conclusion: 自适应的课程学习范式能有效提升模型训练效率和性能。

Abstract: Curriculum learning is a widely adopted training strategy in natural language
processing (NLP), where models are exposed to examples organized by increasing
difficulty to enhance learning efficiency and performance. However, most
existing approaches rely on manually defined difficulty metrics -- such as text
length -- which may not accurately reflect the model's own perspective. To
overcome this limitation, we present a self-adaptive curriculum learning
paradigm that prioritizes fine-tuning examples based on difficulty scores
predicted by pre-trained language models (PLMs) themselves. Building on these
scores, we explore various training strategies that differ in the ordering of
examples for the fine-tuning: from easy-to-hard, hard-to-easy, to mixed
sampling. We evaluate our method on four natural language understanding (NLU)
datasets covering both binary and multi-class classification tasks.
Experimental results show that our approach leads to faster convergence and
improved performance compared to standard random sampling.

</details>


### [83] [Te Ahorré Un Click: A Revised Definition of Clickbait and Detection in Spanish News](https://arxiv.org/abs/2507.09777)
*Gabriel Mordecki,Guillermo Moncecchi,Javier Couto*

Main category: cs.CL

TL;DR: 论文重新定义了点击诱饵（clickbait），提出好奇心缺口是其核心特征，并发布了首个西班牙语点击诱饵检测开源数据集TA1C。


<details>
  <summary>Details</summary>
Motivation: 当前点击诱饵定义缺乏共识，研究旨在明确其界限并减少标注主观性。

Method: 提出基于好奇心缺口的新定义，优化数据集标注标准，创建并发布TA1C数据集，包含3,500条标注推文。

Result: 数据集达到0.825的Fleiss' K一致性，基线模型F1分数为0.84。

Conclusion: 新定义和数据集为点击诱饵检测提供了更客观的标准和资源。

Abstract: We revise the definition of clickbait, which lacks current consensus, and
argue that the creation of a curiosity gap is the key concept that
distinguishes clickbait from other related phenomena such as sensationalism and
headlines that do not deliver what they promise or diverge from the article.
Therefore, we propose a new definition: clickbait is a technique for generating
headlines and teasers that deliberately omit part of the information with the
goal of raising the readers' curiosity, capturing their attention and enticing
them to click. We introduce a new approach to clickbait detection datasets
creation, by refining the concept limits and annotations criteria, minimizing
the subjectivity in the decision as much as possible. Following it, we created
and release TA1C (for Te Ahorr\'e Un Click, Spanish for Saved You A Click), the
first open source dataset for clickbait detection in Spanish. It consists of
3,500 tweets coming from 18 well known media sources, manually annotated and
reaching a 0.825 Fleiss' K inter annotator agreement. We implement strong
baselines that achieve 0.84 in F1-score.

</details>


### [84] [Function Induction and Task Generalization: An Interpretability Study with Off-by-One Addition](https://arxiv.org/abs/2507.09875)
*Qinyuan Ye,Robin Jia,Xiang Ren*

Main category: cs.CL

TL;DR: 论文研究了大型语言模型通过上下文学习执行未见任务的能力，聚焦于‘错位加法’任务，揭示了模型内部的计算机制。


<details>
  <summary>Details</summary>
Motivation: 探索语言模型如何通过内部机制实现任务级泛化，特别是从标准加法到错位加法的泛化。

Method: 使用路径修补等电路式可解释性技术，分析模型在错位加法任务中的内部计算。

Result: 发现了一种函数归纳机制，类似于先前发现的归纳头机制，但抽象层次更高；+1函数的归纳由多个注意力头并行控制；该机制可复用于其他任务。

Conclusion: 研究揭示了语言模型中可复用和可组合的结构如何支持任务级泛化。

Abstract: Large language models demonstrate the intriguing ability to perform unseen
tasks via in-context learning. However, it remains unclear what mechanisms
inside the model drive such task-level generalization. In this work, we
approach this question through the lens of off-by-one addition (i.e., 1+1=3,
2+2=5, 3+3=?), a two-step, counterfactual task with an unexpected +1 function
as a second step. Leveraging circuit-style interpretability techniques such as
path patching, we analyze the models' internal computations behind their
notable performance and present three key findings. First, we uncover a
function induction mechanism that explains the model's generalization from
standard addition to off-by-one addition. This mechanism resembles the
structure of the induction head mechanism found in prior work and elevates it
to a higher level of abstraction. Second, we show that the induction of the +1
function is governed by multiple attention heads in parallel, each of which
emits a distinct piece of the +1 function. Finally, we find that this function
induction mechanism is reused in a broader range of tasks, including synthetic
tasks such as shifted multiple-choice QA and algorithmic tasks such as base-8
addition. Overall, our findings offer deeper insights into how reusable and
composable structures within language models enable task-level generalization.

</details>


### [85] [Enhancing Retrieval Augmented Generation with Hierarchical Text Segmentation Chunking](https://arxiv.org/abs/2507.09935)
*Hai Toan Nguyen,Tien Dat Nguyen,Viet Ha Nguyen*

Main category: cs.CL

TL;DR: 本文提出了一种改进RAG系统的方法，通过分层文本分割和聚类生成更有语义意义的块，提高了检索的精确性和上下文相关性。


<details>
  <summary>Details</summary>
Motivation: 传统分块方法未能充分捕捉语义信息，影响了RAG系统的检索效果。

Method: 结合分层文本分割和聚类，生成语义连贯的块，并利用段级和聚类级向量表示进行检索。

Result: 在NarrativeQA、QuALITY和QASPER数据集上表现优于传统分块技术。

Conclusion: 该方法显著提升了RAG系统的检索性能。

Abstract: Retrieval-Augmented Generation (RAG) systems commonly use chunking strategies
for retrieval, which enhance large language models (LLMs) by enabling them to
access external knowledge, ensuring that the retrieved information is
up-to-date and domain-specific. However, traditional methods often fail to
create chunks that capture sufficient semantic meaning, as they do not account
for the underlying textual structure. This paper proposes a novel framework
that enhances RAG by integrating hierarchical text segmentation and clustering
to generate more meaningful and semantically coherent chunks. During inference,
the framework retrieves information by leveraging both segment-level and
cluster-level vector representations, thereby increasing the likelihood of
retrieving more precise and contextually relevant information. Evaluations on
the NarrativeQA, QuALITY, and QASPER datasets indicate that the proposed method
achieved improved results compared to traditional chunking techniques.

</details>


### [86] [Tiny Reward Models](https://arxiv.org/abs/2507.09973)
*Sarah Pan*

Main category: cs.CL

TL;DR: TinyRM是一种小型双向掩码语言模型，参数仅4亿，性能媲美175倍大的模型，适用于奖励建模任务。


<details>
  <summary>Details</summary>
Motivation: 大型解码器语言模型在RLHF中占主导地位，但其推理成本高，需更高效的替代方案。

Method: 结合FLAN风格提示、定向低秩适应（DoRA）和层冻结技术，优化小型模型性能。

Result: 在RewardBench上表现优异，资源消耗显著减少，轻量级微调方法在推理任务中特别有效。

Conclusion: 轻量级双向架构在偏好建模中具有高效、可扩展的潜力，尽管通用模型和对话偏好建模仍有挑战。

Abstract: Large decoder-based language models have become the dominant architecture for
reward modeling in reinforcement learning from human feedback (RLHF). However,
as reward models are increasingly deployed in test-time strategies, their
inference costs become a growing concern. We present TinyRM, a family of small,
bidirectional masked language models (MLMs) with as few as 400 million
parameters, that rival the capabilities of models over 175 times larger on
reasoning and safety preference modeling tasks. TinyRM combines FLAN-style
prompting, Directional Low-Rank Adaptation (DoRA), and layer freezing to
achieve strong performance on RewardBench, despite using significantly fewer
resources. Our experiments suggest that small models benefit from
domain-specific tuning strategies, particularly in reasoning, where lightweight
finetuning methods are especially effective. While challenges remain in
building generalist models and conversational preference modeling, our
preliminary results highlight the promise of lightweight bidirectional
architectures as efficient, scalable alternatives for preference modeling.

</details>


### [87] [TextOmics-Guided Diffusion for Hit-like Molecular Generation](https://arxiv.org/abs/2507.09982)
*Hang Yuan,Chen Li,Wenjun Ma,Yuncheng Jiang*

Main category: cs.CL

TL;DR: TextOmics是一个开创性基准，通过建立组学表达与分子文本描述的一一对应关系，提供异构数据集，支持分子生成。ToDi框架结合组学和文本描述生成生物相关、化学有效的分子，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 解决靶向药物发现中缺乏异构数据和统一框架的问题。

Method: 提出TextOmics基准和ToDi框架，利用组学和文本描述的双编码器（OmicsEn和TextEn）及条件扩散（DiffGen）生成分子。

Result: 实验证明TextOmics有效，ToDi优于现有方法，并在零样本治疗分子生成中表现突出。

Conclusion: TextOmics和ToDi为靶向药物发现提供了新工具，展示了分子生成的潜力。

Abstract: Hit-like molecular generation with therapeutic potential is essential for
target-specific drug discovery. However, the field lacks heterogeneous data and
unified frameworks for integrating diverse molecular representations. To bridge
this gap, we introduce TextOmics, a pioneering benchmark that establishes
one-to-one correspondences between omics expressions and molecular textual
descriptions. TextOmics provides a heterogeneous dataset that facilitates
molecular generation through representations alignment. Built upon this
foundation, we propose ToDi, a generative framework that jointly conditions on
omics expressions and molecular textual descriptions to produce biologically
relevant, chemically valid, hit-like molecules. ToDi leverages two encoders
(OmicsEn and TextEn) to capture multi-level biological and semantic
associations, and develops conditional diffusion (DiffGen) for controllable
generation. Extensive experiments confirm the effectiveness of TextOmics and
demonstrate ToDi outperforms existing state-of-the-art approaches, while also
showcasing remarkable potential in zero-shot therapeutic molecular generation.
Sources are available at: https://github.com/hala-ToDi.

</details>


### [88] [Protective Factor-Aware Dynamic Influence Learning for Suicide Risk Prediction on Social Media](https://arxiv.org/abs/2507.10008)
*Jun Li,Xiangmeng Wang,Haoyang Li,Yifei Yan,Hong Va Leong,Ling Feng,Nancy Xiaonan Yu,Qing Li*

Main category: cs.CL

TL;DR: 该研究提出了一种新框架，通过联合学习风险和保护因素对自杀风险的动态影响，预测后续自杀风险。


<details>
  <summary>Details</summary>
Motivation: 现有研究多关注静态自杀风险检测，忽略保护因素和动态变化，限制了预测能力。

Method: 提出保护因素感知数据集和动态因素影响学习方法，结合风险和保护因素。

Result: 模型在三个数据集上显著优于现有方法，并提供可解释权重。

Conclusion: 该框架为临床干预提供了更精准的自杀风险预测和理解。

Abstract: Suicide is a critical global health issue that requires urgent attention.
Even though prior work has revealed valuable insights into detecting current
suicide risk on social media, little attention has been paid to developing
models that can predict subsequent suicide risk over time, limiting their
ability to capture rapid fluctuations in individuals' mental state transitions.
In addition, existing work ignores protective factors that play a crucial role
in suicide risk prediction, focusing predominantly on risk factors alone.
Protective factors such as social support and coping strategies can mitigate
suicide risk by moderating the impact of risk factors. Therefore, this study
proposes a novel framework for predicting subsequent suicide risk by jointly
learning the dynamic influence of both risk factors and protective factors on
users' suicide risk transitions. We propose a novel Protective Factor-Aware
Dataset, which is built from 12 years of Reddit posts along with comprehensive
annotations of suicide risk and both risk and protective factors. We also
introduce a Dynamic Factors Influence Learning approach that captures the
varying impact of risk and protective factors on suicide risk transitions,
recognizing that suicide risk fluctuates over time according to established
psychological theories. Our thorough experiments demonstrate that the proposed
model significantly outperforms state-of-the-art models and large language
models across three datasets. In addition, the proposed Dynamic Factors
Influence Learning provides interpretable weights, helping clinicians better
understand suicidal patterns and enabling more targeted intervention
strategies.

</details>


### [89] [GeLaCo: An Evolutionary Approach to Layer Compression](https://arxiv.org/abs/2507.10059)
*David Ponce,Thierry Etchegoyhen,Javier Del Ser*

Main category: cs.CL

TL;DR: GeLaCo是一种基于进化的LLM压缩方法，通过层折叠和模块相似性评估，高效探索压缩解空间，支持单目标和多目标优化，性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLM）计算需求高，部署和使用受限，模型压缩是解决这一问题的关键。现有方法如结构化剪枝成本高且可能忽略更优解。

Method: GeLaCo采用进化方法进行LLM压缩，通过层折叠和基于种群搜索的模块相似性评估（注意力、前馈和隐藏状态表示），支持单目标和多目标优化。

Result: GeLaCo在困惑度和生成评估中优于现有方法，建立了压缩与质量之间的帕累托前沿。

Conclusion: GeLaCo为LLM压缩提供了一种高效且性能优越的解决方案，支持更灵活的压缩目标。

Abstract: Large Language Models (LLM) have achieved remarkable performance across a
large number of tasks, but face critical deployment and usage barriers due to
substantial computational requirements. Model compression methods, which aim to
reduce model size while preserving its capacity, are an important means to
mitigate these issues. Promising approaches along these lines, such as
structured pruning, typically require costly empirical search for optimal
variants and may run the risk of ignoring better solutions. In this work we
introduce GeLaCo, an evolutionary approach to LLM compression via layer
collapse. Our approach supports an efficient exploration of the compression
solution space via population-based search and a module-wise similarity fitness
function capturing attention, feed-forward, and hidden state representations.
GeLaCo also supports both single and multi-objective evolutionary compression
search, establishing the first Pareto frontier along compression and quality
axes. We evaluate GeLaCo solutions via both perplexity-based and generative
evaluations over foundational and instruction-tuned models, outperforming
state-of-the-art alternatives.

</details>


### [90] [Cultural Bias in Large Language Models: Evaluating AI Agents through Moral Questionnaires](https://arxiv.org/abs/2507.10073)
*Simon Münker*

Main category: cs.CL

TL;DR: 研究表明，大型语言模型（LLMs）未能代表多样化的文化道德框架，尽管其语言能力强。模型在19种文化背景下与人类道德直觉存在显著差距，且模型规模增大并未改善文化代表性。


<details>
  <summary>Details</summary>
Motivation: 探讨AI系统是否能真实代表人类价值观，还是仅仅在平均值上表现。

Method: 通过道德基础问卷在19种文化背景下比较LLMs和人类道德直觉，分析模型的文化代表性。

Result: LLMs系统性地同质化道德多样性，模型规模增大未显著改善文化代表性。

Conclusion: 当前AI对齐方法存在根本限制，需更接地气的对齐目标和评估指标，以确保AI系统代表多样的人类价值观。

Abstract: Are AI systems truly representing human values, or merely averaging across
them? Our study suggests a concerning reality: Large Language Models (LLMs)
fail to represent diverse cultural moral frameworks despite their linguistic
capabilities. We expose significant gaps between AI-generated and human moral
intuitions by applying the Moral Foundations Questionnaire across 19 cultural
contexts. Comparing multiple state-of-the-art LLMs' origins against human
baseline data, we find these models systematically homogenize moral diversity.
Surprisingly, increased model size doesn't consistently improve cultural
representation fidelity. Our findings challenge the growing use of LLMs as
synthetic populations in social science research and highlight a fundamental
limitation in current AI alignment approaches. Without data-driven alignment
beyond prompting, these systems cannot capture the nuanced, culturally-specific
moral intuitions. Our results call for more grounded alignment objectives and
evaluation metrics to ensure AI systems represent diverse human values rather
than flattening the moral landscape.

</details>


### [91] [Enhancing Chain-of-Thought Reasoning with Critical Representation Fine-tuning](https://arxiv.org/abs/2507.10085)
*Chenxi Huang,Shaotian Yan,Liang Xie,Binbin Lin,Sinan Fan,Yue Xin,Deng Cai,Chen Shen,Jieping Ye*

Main category: cs.CL

TL;DR: CRFT通过识别和优化关键表征提升复杂推理任务性能，比传统ReFT更高效。


<details>
  <summary>Details</summary>
Motivation: 传统ReFT在复杂推理任务中表现不佳，因固定位置的表征影响不确定，而关键表征对推理结果有显著影响。

Method: 提出CRFT方法，通过信息流分析识别关键表征，并在低秩线性子空间中动态优化这些表征，同时冻结基础模型。

Result: 在八个算术和常识推理基准测试中验证了CRFT的有效性和效率，并在少样本设置中提升16.4%的准确率。

Conclusion: CRFT展示了表征级优化在推理任务中的潜力，为传统PEFT方法提供了轻量而强大的替代方案。

Abstract: Representation Fine-tuning (ReFT), a recently proposed Parameter-Efficient
Fine-Tuning (PEFT) method, has attracted widespread attention for significantly
improving parameter efficiency by editing representation space alone. In this
work, we investigate applying ReFT to complex reasoning tasks. However,
directly using the native ReFT method, which modifies fixed representations at
the beginning and end of each layer, yields suboptimal performance, as these
fixed-position representations have uncertain impact on the outputs. We observe
that, in complex reasoning tasks, there often exist certain critical
representations. These representations either integrate significant information
from preceding layers or regulate subsequent layer representations. Through
layer-by-layer propagation, they exert a substantial influence on the final
output. Naturally, fine-tuning these critical representations has the potential
to greatly enhance reasoning performance. Building upon these insights, we
propose Critical Representation Fine-Tuning (CRFT), a novel method that
identifies and optimizes these critical representations through information
flow analysis. CRFT operates within a supervised learning framework,
dynamically optimizing critical representations in a low-rank linear subspace
while freezing the base model. The effectiveness and efficiency of our method
are validated across eight benchmarks for arithmetic and commonsense reasoning,
using LLaMA and Mistral model families. Furthermore, our method also adapts
effectively to few-shot settings, boosting one-shot accuracy by 16.4%. Our work
highlights the untapped potential of representation-level optimization for CoT
reasoning, offering a lightweight yet powerful alternative to traditional PEFT
methods.

</details>


### [92] [Fusing Large Language Models with Temporal Transformers for Time Series Forecasting](https://arxiv.org/abs/2507.10098)
*Chen Su,Yuanhe Tian,Qinyu Liu,Jun Zhang,Yan Song*

Main category: cs.CL

TL;DR: 提出了一种结合LLM和Transformer的新架构，用于时间序列预测，融合了语义和时间信息，提升了预测准确性。


<details>
  <summary>Details</summary>
Motivation: 现有LLM在时间序列预测中表现不佳，因其设计初衷是处理离散文本数据而非连续数值数据，而传统Transformer又难以捕捉高级语义模式。

Method: 设计了一种新型Transformer架构，融合LLM的语义表示和传统Transformer的时间动态信息，生成混合表示。

Result: 实验表明，该方法在基准数据集上表现优于现有模型。

Conclusion: 通过结合LLM和Transformer的优势，新方法显著提升了时间序列预测的准确性。

Abstract: Recently, large language models (LLMs) have demonstrated powerful
capabilities in performing various tasks and thus are applied by recent studies
to time series forecasting (TSF) tasks, which predict future values with the
given historical time series. Existing LLM-based approaches transfer knowledge
learned from text data to time series prediction using prompting or fine-tuning
strategies. However, LLMs are proficient at reasoning over discrete tokens and
semantic patterns but are not initially designed to model continuous numerical
time series data. The gaps between text and time series data lead LLMs to
achieve inferior performance to a vanilla Transformer model that is directly
trained on TSF data. However, the vanilla Transformers often struggle to learn
high-level semantic patterns. In this paper, we design a novel
Transformer-based architecture that complementarily leverages LLMs and vanilla
Transformers, so as to integrate the high-level semantic representations
learned by LLMs into the temporal information encoded by time series
Transformers, where a hybrid representation is obtained by fusing the
representations from the LLM and the Transformer. The resulting fused
representation contains both historical temporal dynamics and semantic
variation patterns, allowing our model to predict more accurate future values.
Experiments on benchmark datasets demonstrate the effectiveness of the proposed
approach.

</details>


### [93] [Task-Based Flexible Feature Distillation for LLMs](https://arxiv.org/abs/2507.10155)
*Khouloud Saadi,Di Wang*

Main category: cs.CL

TL;DR: 提出一种基于任务的特征蒸馏方法，解决传统特征蒸馏中教师与学生模型隐藏层尺寸不匹配的问题，无需引入额外参数。


<details>
  <summary>Details</summary>
Motivation: 传统特征蒸馏方法假设教师与学生模型隐藏层尺寸相同，限制了学生模型的灵活性，而线性投影方法会引入额外参数并降低性能。

Method: 通过识别教师模型中与任务最相关的隐藏单元，直接将其激活值蒸馏到学生模型，无需新参数。

Result: 在分类、指令跟随和摘要等任务中，性能提升达3%，优于线性投影基线。

Conclusion: 该方法灵活且高效，适用于不同任务，显著提升了知识蒸馏的效果。

Abstract: Knowledge Distillation (KD) in general and feature distillation in particular
are promising techniques for reducing the high computational demand of large
language models (LLMs). However, traditional feature KD methods typically
assume that the teacher and the student share the same hidden size, limiting
the flexibility of the student's architecture. A common solution to this
problem involves training a linear projector to align their feature spaces, but
this introduces additional parameters that must be learned from scratch and
often degrades performance on downstream tasks, especially in generative
settings. To address this issue, in this work, we propose a novel task-based
feature distillation method that enables knowledge transfer between teacher and
student models with different hidden layer dimensions, without introducing any
new parameters. Leveraging the insight that only a subset of LLM components
contribute significantly to a specific downstream task, our approach identifies
the most task-relevant hidden units in the teacher and directly distills their
activations to the student. Our method is flexible and easily integrates with
other distillation frameworks. Empirical results show consistent improvements
over prior approaches across diverse tasks, including classification,
instruction-following, and summarization, achieving up to a 3\% performance
gain over the linear projection baseline.

</details>


### [94] [Abusive text transformation using LLMs](https://arxiv.org/abs/2507.10177)
*Rohitash Chandra,Jiyong Choi*

Main category: cs.CL

TL;DR: 研究探讨了大型语言模型（LLMs）在将侮辱性文本转化为非侮辱性版本时的表现，比较了Gemini、GPT-4o、DeepSeek和Groq等模型的效果，发现Groq与其他模型差异显著。


<details>
  <summary>Details</summary>
Motivation: 尽管LLMs在自然语言处理任务中表现优异，但其在识别和转化侮辱性文本方面的能力仍需探索。

Method: 使用LLMs识别侮辱性文本（如推文和评论），并将其转化为非侮辱性版本，同时保留文本意图。随后通过情感和语义分析评估原始及转化后的数据集。

Result: Groq的表现与其他模型（如GPT-4o和DeepSeek-V3）存在显著差异，而GPT-4o与DeepSeek-V3表现相似。

Conclusion: 研究表明LLMs在侮辱性文本转化任务中表现不一，Groq的独特表现值得进一步研究。

Abstract: Although Large Language Models (LLMs) have demonstrated significant
advancements in natural language processing tasks, their effectiveness in the
classification and transformation of abusive text into non-abusive versions
remains an area for exploration. In this study, we aim to use LLMs to transform
abusive text (tweets and reviews) featuring hate speech and swear words into
non-abusive text, while retaining the intent of the text. We evaluate the
performance of two state-of-the-art LLMs, such as Gemini, GPT-4o, DeekSeek and
Groq, on their ability to identify abusive text. We them to transform and
obtain a text that is clean from abusive and inappropriate content but
maintains a similar level of sentiment and semantics, i.e. the transformed text
needs to maintain its message. Afterwards, we evaluate the raw and transformed
datasets with sentiment analysis and semantic analysis. Our results show Groq
provides vastly different results when compared with other LLMs. We have
identified similarities between GPT-4o and DeepSeek-V3.

</details>


### [95] [Absher: A Benchmark for Evaluating Large Language Models Understanding of Saudi Dialects](https://arxiv.org/abs/2507.10216)
*Renad Al-Monef,Hassan Alhuzali,Nora Alturayeif,Ashwag Alasmari*

Main category: cs.CL

TL;DR: 论文介绍了Absher基准，用于评估大型语言模型（LLMs）在沙特阿拉伯方言中的表现，发现模型在文化和上下文理解任务中存在显著不足。


<details>
  <summary>Details</summary>
Motivation: 评估LLMs对沙特阿拉伯方言和文化细微差别的理解能力，以应对阿拉伯NLP应用的需求。

Method: 构建包含18,000多个多选题的Absher基准，涵盖六类任务，评估多语言和阿拉伯专用LLMs。

Result: 发现LLMs在文化和上下文相关任务中表现不佳，存在显著性能差距。

Conclusion: 强调需要方言感知训练和文化对齐的评估方法，以提升LLMs在阿拉伯应用中的实际表现。

Abstract: As large language models (LLMs) become increasingly central to Arabic NLP
applications, evaluating their understanding of regional dialects and cultural
nuances is essential, particularly in linguistically diverse settings like
Saudi Arabia. This paper introduces \texttt{Absher}, a comprehensive benchmark
specifically designed to assess LLMs performance across major Saudi dialects.
\texttt{Absher} comprises over 18,000 multiple-choice questions spanning six
distinct categories: Meaning, True/False, Fill-in-the-Blank, Contextual Usage,
Cultural Interpretation, and Location Recognition. These questions are derived
from a curated dataset of dialectal words, phrases, and proverbs sourced from
various regions of Saudi Arabia. We evaluate several state-of-the-art LLMs,
including multilingual and Arabic-specific models. We also provide detailed
insights into their capabilities and limitations. Our results reveal notable
performance gaps, particularly in tasks requiring cultural inference or
contextual understanding. Our findings highlight the urgent need for
dialect-aware training and culturally aligned evaluation methodologies to
improve LLMs performance in real-world Arabic applications.

</details>


### [96] [Grammar-Guided Evolutionary Search for Discrete Prompt Optimisation](https://arxiv.org/abs/2507.10326)
*Muzhaffar Hazman,Minh-Khoi Pham,Shweta Soundararajan,Goncalo Mordido,Leonardo Custode,David Lynch,Giorgio Cruciata,Yucheng Shi,Hongmeng Song,Wang Chao,Pan Yue,Aleksandar Milenovic,Alexandros Agapitos*

Main category: cs.CL

TL;DR: 提出了一种基于进化搜索的自动离散提示优化方法，用于解决复杂任务中提示设计的挑战。


<details>
  <summary>Details</summary>
Motivation: 现有方法在复杂任务和较小模型上表现不佳，需要更高效的提示优化方法。

Method: 采用两阶段进化搜索：第一阶段使用语法引导的遗传编程合成提示创建程序，第二阶段通过局部搜索优化性能。

Result: 在三个小型通用LLM和四个领域特定任务中优于PromptWizard、OPRO和RL-Prompt。

Conclusion: 该方法在复杂任务和小模型上表现优异，性能退化极少。

Abstract: Prompt engineering has proven to be a crucial step in leveraging pretrained
large language models (LLMs) in solving various real-world tasks. Numerous
solutions have been proposed that seek to automate prompt engineering by using
the model itself to edit prompts. However, the majority of state-of-the-art
approaches are evaluated on tasks that require minimal prompt templates and on
very large and highly capable LLMs. In contrast, solving complex tasks that
require detailed information to be included in the prompt increases the amount
of text that needs to be optimised. Furthermore, smaller models have been shown
to be more sensitive to prompt design. To address these challenges, we propose
an evolutionary search approach to automated discrete prompt optimisation
consisting of two phases. In the first phase, grammar-guided genetic
programming is invoked to synthesise prompt-creating programmes by searching
the space of programmes populated by function compositions of syntactic,
dictionary-based and LLM-based prompt-editing functions. In the second phase,
local search is applied to explore the neighbourhoods of best-performing
programmes in an attempt to further fine-tune their performance. Our approach
outperforms three state-of-the-art prompt optimisation approaches,
PromptWizard, OPRO, and RL-Prompt, on three relatively small general-purpose
LLMs in four domain-specific challenging tasks. We also illustrate several
examples where these benchmark methods suffer relatively severe performance
degradation, while our approach improves performance in almost all task-model
combinations, only incurring minimal degradation when it does not.

</details>


### [97] [Bridging Robustness and Generalization Against Word Substitution Attacks in NLP via the Growth Bound Matrix Approach](https://arxiv.org/abs/2507.10330)
*Mohammed Bouri,Adnane Saoud*

Main category: cs.CL

TL;DR: 论文提出了一种基于Growth Bound Matrices（GBM）的新型正则化技术，用于提升NLP模型对对抗攻击的鲁棒性，特别是在LSTM、S4和CNN架构中。


<details>
  <summary>Details</summary>
Motivation: 尽管NLP领域取得进展，但模型仍易受对抗攻击（如同义词替换）的影响，尤其是循环网络和状态空间模型（SSMs）的鲁棒性研究不足。

Method: 通过计算GBM来减少输入扰动对模型输出的影响，重点关注LSTM、S4和CNN三种架构。

Result: 实验表明，该方法在对抗鲁棒性上比现有基线提升高达8.8%，并优于多种先进防御方法。

Conclusion: GBM方法有效提升了NLP模型的对抗鲁棒性，并为SSM（S4）的鲁棒性提供了首次系统性分析。

Abstract: Despite advancements in Natural Language Processing (NLP), models remain
vulnerable to adversarial attacks, such as synonym substitutions. While prior
work has focused on improving robustness for feed-forward and convolutional
architectures, the robustness of recurrent networks and modern state space
models (SSMs), such as S4, remains understudied. These architectures pose
unique challenges due to their sequential processing and complex parameter
dynamics. In this paper, we introduce a novel regularization technique based on
Growth Bound Matrices (GBM) to improve NLP model robustness by reducing the
impact of input perturbations on model outputs. We focus on computing the GBM
for three architectures: Long Short-Term Memory (LSTM), State Space models
(S4), and Convolutional Neural Networks (CNN). Our method aims to (1) enhance
resilience against word substitution attacks, (2) improve generalization on
clean text, and (3) providing the first systematic analysis of SSM (S4)
robustness. Extensive experiments across multiple architectures and benchmark
datasets demonstrate that our method improves adversarial robustness by up to
8.8% over existing baselines. These results highlight the effectiveness of our
approach, outperforming several state-of-the-art methods in adversarial
defense. Codes are available at https://github.com/BouriMohammed/GBM

</details>


### [98] [Using AI to replicate human experimental results: a motion study](https://arxiv.org/abs/2507.10342)
*Rosa Illan Castillo,Javier Valenzuela*

Main category: cs.CL

TL;DR: 论文探讨了大型语言模型（LLMs）在语言学研究中作为可靠分析工具的潜力，重点关注情感意义在时间表达中的涌现。通过四项心理语言学实验，发现人类与AI反应高度一致，表明LLMs可作为人类实验的有效补充。


<details>
  <summary>Details</summary>
Motivation: 研究LLMs是否能复现人类对语言细微差别的判断，以验证其在语言学研究中作为分析工具的可靠性。

Method: 进行四项心理语言学实验（情感意义涌现、情感变化、情感语境中的动词选择、句子-表情符号关联），分别由人类和LLMs完成，并比较结果。

Result: 人类与AI反应高度一致（Spearman's rho = .73-.96），仅在少数情况下有微小差异，不影响整体解释。

Conclusion: LLMs可作为语言学研究的可信合作者，支持大规模研究且不损害解释有效性。

Abstract: This paper explores the potential of large language models (LLMs) as reliable
analytical tools in linguistic research, focusing on the emergence of affective
meanings in temporal expressions involving manner-of-motion verbs. While LLMs
like GPT-4 have shown promise across a range of tasks, their ability to
replicate nuanced human judgements remains under scrutiny. We conducted four
psycholinguistic studies (on emergent meanings, valence shifts, verb choice in
emotional contexts, and sentence-emoji associations) first with human
participants and then replicated the same tasks using an LLM. Results across
all studies show a striking convergence between human and AI responses, with
statistical analyses (e.g., Spearman's rho = .73-.96) indicating strong
correlations in both rating patterns and categorical choices. While minor
divergences were observed in some cases, these did not alter the overall
interpretative outcomes. These findings offer compelling evidence that LLMs can
augment traditional human-based experimentation, enabling broader-scale studies
without compromising interpretative validity. This convergence not only
strengthens the empirical foundation of prior human-based findings but also
opens possibilities for hypothesis generation and data expansion through AI.
Ultimately, our study supports the use of LLMs as credible and informative
collaborators in linguistic inquiry.

</details>


### [99] [Meanings are like Onions: a Layered Approach to Metaphor Processing](https://arxiv.org/abs/2507.10354)
*Silvia Cappa,Anna Sofia Lippolis,Stefano Zoia*

Main category: cs.CL

TL;DR: 论文提出了一种分层的隐喻处理模型，将意义视为多层次的洋葱结构，包括内容分析、概念融合和语用意图，为计算系统提供了更丰富的隐喻解释方法。


<details>
  <summary>Details</summary>
Motivation: 隐喻意义并非简单的概念映射，而是一种复杂的认知现象。为了更全面地理解和计算隐喻，需要一种多层次的框架。

Method: 提出三层模型：内容分析（基础概念标注）、概念融合（概念组合建模）和语用意图（捕捉说话者意图和语境效果）。

Result: 模型为计算系统提供了更深入、更语境敏感的隐喻理解方法，超越了表面关联。

Conclusion: 通过统一多层框架，该模型为计算隐喻理解奠定了基础，支持更复杂的推理。

Abstract: Metaphorical meaning is not a flat mapping between concepts, but a complex
cognitive phenomenon that integrates multiple levels of interpretation. In this
paper, we propose a stratified model of metaphor processing that treats meaning
as an onion: a multi-layered structure comprising (1) content analysis, (2)
conceptual blending, and (3) pragmatic intentionality. This three-dimensional
framework allows for a richer and more cognitively grounded approach to
metaphor interpretation in computational systems. At the first level, metaphors
are annotated through basic conceptual elements. At the second level, we model
conceptual combinations, linking components to emergent meanings. Finally, at
the third level, we introduce a pragmatic vocabulary to capture speaker intent,
communicative function, and contextual effects, aligning metaphor understanding
with pragmatic theories. By unifying these layers into a single formal
framework, our model lays the groundwork for computational methods capable of
representing metaphorical meaning beyond surface associations, toward deeper,
more context-sensitive reasoning.

</details>


### [100] [From Sequence to Structure: Uncovering Substructure Reasoning in Transformers](https://arxiv.org/abs/2507.10435)
*Xinnan Dai,Kai Yang,Jay Revolinsky,Kai Guo,Aoran Wang,Bohang Zhang,Jiliang Tang*

Main category: cs.CL

TL;DR: LLMs can solve graph reasoning tasks by understanding embedded graph structures. The paper introduces Induced Substructure Filtration (ISF) to explain how decoder-only Transformers identify substructures, validated through empirical and theoretical analysis.


<details>
  <summary>Details</summary>
Motivation: To understand how decoder-only Transformers interpret graph structures embedded in text and perform substructure extraction tasks.

Method: Proposes ISF, a perspective capturing substructure identification in multi-layer transformers, validated through empirical results and theoretical analysis.

Result: LLMs consistently identify substructures across layers, and decoder-only Transformers can extract complex patterns from attributed graphs like molecular graphs.

Conclusion: The findings provide insights into how sequence-based Transformers perform substructure extraction on graph data, suggesting broader capabilities in handling diverse graph types.

Abstract: Recent studies suggest that large language models (LLMs) possess the
capability to solve graph reasoning tasks. Notably, even when graph structures
are embedded within textual descriptions, LLMs can still effectively answer
related questions. This raises a fundamental question: How can a decoder-only
Transformer architecture understand underlying graph structures? To address
this, we start with the substructure extraction task, interpreting the inner
mechanisms inside the transformers and analyzing the impact of the input
queries. Specifically, through both empirical results and theoretical analysis,
we present Induced Substructure Filtration (ISF), a perspective that captures
the substructure identification in the multi-layer transformers. We further
validate the ISF process in LLMs, revealing consistent internal dynamics across
layers. Building on these insights, we explore the broader capabilities of
Transformers in handling diverse graph types. Specifically, we introduce the
concept of thinking in substructures to efficiently extract complex composite
patterns, and demonstrate that decoder-only Transformers can successfully
extract substructures from attributed graphs, such as molecular graphs.
Together, our findings offer a new insight on how sequence-based Transformers
perform the substructure extraction task over graph data.

</details>


### [101] [Referential ambiguity and clarification requests: comparing human and LLM behaviour](https://arxiv.org/abs/2507.10445)
*Chris Madge,Matthew Purver,Massimo Poesio*

Main category: cs.CL

TL;DR: 研究探讨了LLMs在任务导向对话中提出澄清问题的能力，发现人类与LLMs在模糊性处理上差异显著。


<details>
  <summary>Details</summary>
Motivation: 分析LLMs在异步任务对话中提出澄清问题的能力，并与人类行为对比。

Method: 结合现有语料库，比较人类与LLMs在模糊性情境下的澄清问题生成。

Result: 人类较少因指代模糊提问，而LLMs更频繁；任务不确定性则相反。推理能力提升LLMs提问频率与相关性。

Conclusion: LLMs的澄清问题能力可能与推理能力相关，但与人类行为存在显著差异。

Abstract: In this work we examine LLMs' ability to ask clarification questions in
task-oriented dialogues that follow the asynchronous
instruction-giver/instruction-follower format. We present a new corpus that
combines two existing annotations of the Minecraft Dialogue Corpus -- one for
reference and ambiguity in reference, and one for SDRT including clarifications
-- into a single common format providing the necessary information to
experiment with clarifications and their relation to ambiguity. With this
corpus we compare LLM actions with original human-generated clarification
questions, examining how both humans and LLMs act in the case of ambiguity. We
find that there is only a weak link between ambiguity and humans producing
clarification questions in these dialogues, and low correlation between humans
and LLMs. Humans hardly ever produce clarification questions for referential
ambiguity, but often do so for task-based uncertainty. Conversely, LLMs produce
more clarification questions for referential ambiguity, but less so for task
uncertainty. We question if LLMs' ability to ask clarification questions is
predicated on their recent ability to simulate reasoning, and test this with
different reasoning approaches, finding that reasoning does appear to increase
question frequency and relevancy.

</details>


### [102] [From BERT to Qwen: Hate Detection across architectures](https://arxiv.org/abs/2507.10468)
*Ariadna Mon,Saúl Fenollosa,Jon Lecumberri*

Main category: cs.CL

TL;DR: 研究比较了经典编码器和新一代LLM在仇恨言论检测上的表现，验证了更大规模模型是否实际提升效果。


<details>
  <summary>Details</summary>
Motivation: 在线平台需平衡仇恨言论检测与言论自由，但现有模型的实用性尚未验证。

Method: 通过基准测试比较经典双向编码器和超大规模自回归LLM在仇恨言论检测任务上的表现。

Result: 研究结果将揭示更大规模模型是否在实际应用中表现更优。

Conclusion: 研究为仇恨言论检测模型的实用性和改进方向提供了实证依据。

Abstract: Online platforms struggle to curb hate speech without over-censoring
legitimate discourse. Early bidirectional transformer encoders made big
strides, but the arrival of ultra-large autoregressive LLMs promises deeper
context-awareness. Whether this extra scale actually improves practical
hate-speech detection on real-world text remains unverified. Our study puts
this question to the test by benchmarking both model families, classic encoders
and next-generation LLMs, on curated corpora of online interactions for
hate-speech detection (Hate or No Hate).

</details>


### [103] [MLAR: Multi-layer Large Language Model-based Robotic Process Automation Applicant Tracking](https://arxiv.org/abs/2507.10472)
*Mohamed T. Younes,Omar Walid,Mai Hassan,Ali Hamdi*

Main category: cs.CL

TL;DR: MLAR是一种基于RPA和LLM的创新ATS系统，通过三层处理优化简历筛选和候选人匹配，显著提升效率。


<details>
  <summary>Details</summary>
Motivation: 传统招聘流程在简历筛选和候选人短名单上存在时间和资源瓶颈，MLAR旨在解决这些问题。

Method: MLAR采用三层LLM处理：提取职位关键特征、解析简历信息、语义匹配候选人，并集成到RPA流程中。

Result: 在2400份简历处理中，MLAR平均每份耗时5.4秒，比主流RPA平台快16.9%-17.1%。

Conclusion: MLAR为现代招聘需求提供了高效、准确且可扩展的解决方案。

Abstract: This paper introduces an innovative Applicant Tracking System (ATS) enhanced
by a novel Robotic process automation (RPA) framework or as further referred to
as MLAR. Traditional recruitment processes often encounter bottlenecks in
resume screening and candidate shortlisting due to time and resource
constraints. MLAR addresses these challenges employing Large Language Models
(LLMs) in three distinct layers: extracting key characteristics from job
postings in the first layer, parsing applicant resume to identify education,
experience, skills in the second layer, and similarity matching in the third
layer. These features are then matched through advanced semantic algorithms to
identify the best candidates efficiently. Our approach integrates seamlessly
into existing RPA pipelines, automating resume parsing, job matching, and
candidate notifications. Extensive performance benchmarking shows that MLAR
outperforms the leading RPA platforms, including UiPath and Automation
Anywhere, in high-volume resume-processing tasks. When processing 2,400
resumes, MLAR achieved an average processing time of 5.4 seconds per resume,
reducing processing time by approximately 16.9% compared to Automation Anywhere
and 17.1% compared to UiPath. These results highlight the potential of MLAR to
transform recruitment workflows by providing an efficient, accurate, and
scalable solution tailored to modern hiring needs.

</details>


### [104] [Can You Detect the Difference?](https://arxiv.org/abs/2507.10475)
*İsmail Tarım,Aytuğ Onan*

Main category: cs.CL

TL;DR: 该论文比较了扩散生成文本（LLaDA）和自回归生成文本（LLaMA）的检测效果，发现LLaDA在困惑度和突发性上更接近人类文本，而LLaMA的困惑度较低但词汇保真度较差。单一指标无法区分扩散生成文本与人类写作，需开发扩散感知检测器。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）的快速发展引发了对AI生成文本可靠检测的担忧。现有方法对自回归模型有效，但对扩散模型的效果未知。

Method: 通过2000个样本，系统比较了LLaDA和LLaMA生成的文本，分析了困惑度、突发性、词汇多样性、可读性及BLEU/ROUGE分数。

Result: LLaDA在困惑度和突发性上接近人类文本，导致自回归检测器的高假阴性率；LLaMA困惑度低但词汇保真度差。单一指标无法区分扩散生成文本。

Conclusion: 需开发扩散感知检测器，如混合模型、扩散特定风格特征和鲁棒水印技术。

Abstract: The rapid advancement of large language models (LLMs) has raised concerns
about reliably detecting AI-generated text. Stylometric metrics work well on
autoregressive (AR) outputs, but their effectiveness on diffusion-based models
is unknown. We present the first systematic comparison of diffusion-generated
text (LLaDA) and AR-generated text (LLaMA) using 2 000 samples. Perplexity,
burstiness, lexical diversity, readability, and BLEU/ROUGE scores show that
LLaDA closely mimics human text in perplexity and burstiness, yielding high
false-negative rates for AR-oriented detectors. LLaMA shows much lower
perplexity but reduced lexical fidelity. Relying on any single metric fails to
separate diffusion outputs from human writing. We highlight the need for
diffusion-aware detectors and outline directions such as hybrid models,
diffusion-specific stylometric signatures, and robust watermarking.

</details>


### [105] [Mixture-of-Recursions: Learning Dynamic Recursive Depths for Adaptive Token-Level Computation](https://arxiv.org/abs/2507.10524)
*Sangmin Bae,Yujin Kim,Reza Bayat,Sungnyun Kim,Jiyoun Ha,Tal Schuster,Adam Fisch,Hrayr Harutyunyan,Ziwei Ji,Aaron Courville,Se-Young Yun*

Main category: cs.CL

TL;DR: Mixture-of-Recursions (MoR) 是一种结合参数共享和自适应计算的高效框架，通过递归Transformer实现，显著降低计算和内存需求。


<details>
  <summary>Details</summary>
Motivation: 解决语言模型扩展时的高计算和内存成本问题，同时实现参数共享和自适应计算。

Method: MoR 通过共享层和轻量级路由器动态分配递归深度，选择性缓存活跃token的键值对，并提出KV共享变体以减少预填充延迟。

Result: 在135M到1.7B参数范围内，MoR 在相同训练FLOPs下显著降低验证困惑度，提高少样本准确率，并提升吞吐量。

Conclusion: MoR 是实现高质量大模型而不增加成本的有效路径。

Abstract: Scaling language models unlocks impressive capabilities, but the accompanying
computational and memory demands make both training and deployment expensive.
Existing efficiency efforts typically target either parameter sharing or
adaptive computation, leaving open the question of how to attain both
simultaneously. We introduce Mixture-of-Recursions (MoR), a unified framework
that combines the two axes of efficiency inside a single Recursive Transformer.
MoR reuses a shared stack of layers across recursion steps to achieve parameter
efficiency, while lightweight routers enable adaptive token-level thinking by
dynamically assigning different recursion depths to individual tokens. This
allows MoR to focus quadratic attention computation only among tokens still
active at a given recursion depth, further improving memory access efficiency
by selectively caching only their key-value pairs. Beyond these core
mechanisms, we also propose a KV sharing variant that reuses KV pairs from the
first recursion, specifically designed to decrease prefill latency and memory
footprint. Across model scales ranging from 135M to 1.7B parameters, MoR forms
a new Pareto frontier: at equal training FLOPs and smaller model sizes, it
significantly lowers validation perplexity and improves few-shot accuracy,
while delivering higher throughput compared with vanilla and existing recursive
baselines. These gains demonstrate that MoR is an effective path towards
large-model quality without incurring large-model cost.

</details>


### [106] [CodeJudgeBench: Benchmarking LLM-as-a-Judge for Coding Tasks](https://arxiv.org/abs/2507.10535)
*Hongchao Jiang,Yiming Chen,Yushi Cao,Hung-yi Lee,Robby T. Tan*

Main category: cs.CL

TL;DR: 论文介绍了CodeJudgeBench，一个专门评估LLM作为裁判在代码任务中表现的基准测试，发现思考型模型表现更优，但所有模型在判断中存在显著随机性。


<details>
  <summary>Details</summary>
Motivation: 填补LLM作为裁判在代码任务中评估能力的空白，缺乏专用基准测试。

Method: 引入CodeJudgeBench基准测试，评估26个LLM模型在代码生成、修复和单元测试生成任务中的表现。

Result: 思考型模型表现更优，小模型也能超越大模型，但判断存在随机性；顺序和提示策略影响准确性。

Conclusion: LLM作为裁判在代码任务中表现不稳定，需进一步优化提示策略以提高可靠性。

Abstract: Large Language Models (LLMs) have significantly advanced the state-of-the-art
in various coding tasks. Beyond directly answering user queries, LLMs can also
serve as judges, assessing and comparing the quality of responses generated by
other models. Such an evaluation capability is crucial both for benchmarking
different LLMs and for improving response quality through response ranking.
However, despite the growing adoption of the LLM-as-a-Judge paradigm, its
effectiveness in coding scenarios remains underexplored due to the absence of
dedicated benchmarks. To address this gap, we introduce CodeJudgeBench, a
benchmark explicitly designed to evaluate the performance of LLM-as-a-Judge
models across three critical coding tasks: code generation, code repair, and
unit test generation. Through comprehensive benchmarking of 26 LLM-as-a-Judge
models, we find that recent thinking models significantly outperform
non-thinking models on our carefully designed code judging tasks. Notably, even
relatively small thinking models, such as Qwen3-8B, can outperform specially
trained LLM-as-a-Judge models up to 70B in size. Nevertheless, all models still
exhibit significant randomness in their judgment of coding tasks. For pairwise
judging tasks, simply changing the order in which responses are presented can
substantially impact accuracy. In addition, when judging code and unit tests
written by different LLMs, LLM-as-a-Judge models also show variance in
performance. This sensitivity raises concerns about the reliability and
consistency of LLM-as-a-Judge in coding scenarios. Lastly, we study optimal
prompting strategies for LLM-as-a-Judge. We find that using pair-wise
comparison outperforms scalar point-wise judging. Furthermore, retaining
comments and reasoning in the full, unprocessed LLM response leads to improved
judge performance.

</details>


### [107] [REST: Stress Testing Large Reasoning Models by Asking Multiple Problems at Once](https://arxiv.org/abs/2507.10541)
*Zhuoshi Pan,Qizhi Pei,Yu Li,Qiyao Sun,Zinan Tang,H. Vicky Zhao,Conghui He,Lijun Wu*

Main category: cs.CL

TL;DR: REST是一种新的评估框架，通过同时测试多个问题来评估大型推理模型（LRMs）的能力，揭示了现有单问题评估的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有评估方法局限于单问题推理，无法反映真实世界的多上下文压力需求，且易受数据污染影响。

Method: 提出REST框架，同时暴露模型于多个问题，评估其优先级分配、干扰抵抗和认知负载管理能力。

Result: SOTA模型在压力测试下表现显著下降，REST显示出更强的区分能力。

Conclusion: REST是一种高效、前瞻性的评估范式，更贴近真实需求，减少对人标注的依赖。

Abstract: Recent Large Reasoning Models (LRMs) have achieved remarkable progress on
task-specific benchmarks, yet their evaluation methods remain constrained by
isolated problem-solving paradigms. Existing benchmarks predominantly assess
single-question reasoning through sequential testing, resulting critical
limitations: (1) vulnerability to data contamination and less challenging
(e.g., DeepSeek-R1 achieves 97.0% on MATH500), forcing costly and perpetual
creation of new questions with large human efforts, (2) failure to evaluate
models under multi-context pressure, a key requirement for real-world
deployment. To bridge this gap, we present REST (Reasoning Evaluation through
Simultaneous Testing), a stress-testing framework that concurrently exposes
LRMs to multiple problems simultaneously. Beyond basic reasoning, REST
specifically evaluates several under-tested capabilities: contextual priority
allocation, cross-problem interference resistance, and dynamic cognitive load
management. Our evaluation reveals several striking findings: Even
state-of-the-art (SOTA) models like DeepSeek-R1 exhibit substantial performance
degradation under stress testing. Crucially, REST demonstrates stronger
discriminative power than existing benchmarks, revealing pronounced performance
differences among models that exhibit similar, near-ceiling performance under
single-question evaluations. Some key mechanistic insights emerge from our
analysis: (1) the "overthinking trap" is a critical factor contributing to the
performance degradation; (2) the models trained with "long2short" technique
preserve more accuracy of their single-problem performance under REST,
outperforming standard-trained counterparts. These results establish REST as a
cost-efficient, future-proof evaluation paradigm that better reflects
real-world reasoning demands while reducing reliance on continuous human
annotation.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [108] [View Invariant Learning for Vision-Language Navigation in Continuous Environments](https://arxiv.org/abs/2507.08831)
*Josh Qixuan Sun,Xiaoying Xing,Huaiyuan Weng,Chul Min Yeum,Mark Crowley*

Main category: cs.CV

TL;DR: 论文提出了一种名为VIL的视角不变学习策略，用于增强视觉语言导航（VLNCE）中导航策略对视角变化的鲁棒性，并在V2-VLNCE任务中显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 现有导航策略对视角变化（如相机高度和视角的变化）敏感，影响了导航性能。为解决这一问题，作者提出了V2-VLNCE任务和VIL方法。

Method: VIL采用对比学习框架学习稀疏且视角不变的特征，并结合教师-学生框架优化Waypoint Predictor Module，通过端到端训练联合优化这些组件。

Result: 在V2-VLNCE任务中，VIL在R2R-CE和RxR-CE数据集上的成功率比现有方法提高了8-15%，且在标准VLNCE任务中也能提升性能。

Conclusion: VIL是一种即插即用的后训练方法，不仅能提升对视角变化的鲁棒性，还能在标准任务中保持或提升性能。

Abstract: Vision-Language Navigation in Continuous Environments (VLNCE), where an agent
follows instructions and moves freely to reach a destination, is a key research
problem in embodied AI. However, most navigation policies are sensitive to
viewpoint changes, i.e., variations in camera height and viewing angle that
alter the agent's observation. In this paper, we introduce a generalized
scenario, V2-VLNCE (VLNCE with Varied Viewpoints), and propose VIL (View
Invariant Learning), a view-invariant post-training strategy that enhances the
robustness of existing navigation policies to changes in camera viewpoint. VIL
employs a contrastive learning framework to learn sparse and view-invariant
features. Additionally, we introduce a teacher-student framework for the
Waypoint Predictor Module, a core component of most VLNCE baselines, where a
view-dependent teacher model distills knowledge into a view-invariant student
model. We employ an end-to-end training paradigm to jointly optimize these
components, thus eliminating the cost for individual module training. Empirical
results show that our method outperforms state-of-the-art approaches on
V2-VLNCE by 8-15% measured on Success Rate for two standard benchmark datasets
R2R-CE and RxR-CE. Furthermore, we evaluate VIL under the standard VLNCE
setting and find that, despite being trained for varied viewpoints, it often
still improves performance. On the more challenging RxR-CE dataset, our method
also achieved state-of-the-art performance across all metrics when compared to
other map-free methods. This suggests that adding VIL does not diminish the
standard viewpoint performance and can serve as a plug-and-play post-training
method.

</details>


### [109] [Detecting Deepfake Talking Heads from Facial Biometric Anomalies](https://arxiv.org/abs/2507.08917)
*Justin D. Norman,Hany Farid*

Main category: cs.CV

TL;DR: 提出了一种基于面部生物特征异常模式的深度学习检测技术，用于识别深度伪造视频。


<details>
  <summary>Details</summary>
Motivation: 深度伪造视频技术被广泛用于欺诈、诈骗和政治虚假信息，亟需有效的检测方法。

Method: 利用面部生物特征中的异常模式，开发了一种新的法医机器学习技术。

Result: 在大规模深度伪造数据集上评估了该技术，并验证了其对视频篡改的鲁棒性和对未知生成器的泛化能力。

Conclusion: 该技术能有效检测深度伪造视频，具有实际应用潜力。

Abstract: The combination of highly realistic voice cloning, along with visually
compelling avatar, face-swap, or lip-sync deepfake video generation, makes it
relatively easy to create a video of anyone saying anything. Today, such
deepfake impersonations are often used to power frauds, scams, and political
disinformation. We propose a novel forensic machine learning technique for the
detection of deepfake video impersonations that leverages unnatural patterns in
facial biometrics. We evaluate this technique across a large dataset of
deepfake techniques and impersonations, as well as assess its reliability to
video laundering and its generalization to previously unseen video deepfake
generators.

</details>


### [110] [PRISM: Reducing Spurious Implicit Biases in Vision-Language Models with LLM-Guided Embedding Projection](https://arxiv.org/abs/2507.08979)
*Mahdiyar Molahasani,Azadeh Motamedi,Michael Greenspan,Il-Min Kim,Ali Etemad*

Main category: cs.CV

TL;DR: PRISM是一种无需数据和任务无关的视觉语言模型（VLM）去偏方法，通过两阶段操作减少隐含的虚假偏见。


<details>
  <summary>Details</summary>
Motivation: 视觉语言模型（如CLIP）常因训练数据中的偏见而产生偏差预测，PRISM旨在无需预定义偏见类别或外部数据的情况下解决这一问题。

Method: PRISM分两阶段：1）用LLM生成包含虚假相关性的场景描述；2）通过对比式去偏损失学习投影，将嵌入映射到减少虚假相关性并保留图像-文本对齐的潜在空间。

Result: PRISM在Waterbirds和CelebA数据集上优于现有去偏方法。

Conclusion: PRISM提供了一种高效的去偏解决方案，代码已开源。

Abstract: We introduce Projection-based Reduction of Implicit Spurious bias in
vision-language Models (PRISM), a new data-free and task-agnostic solution for
bias mitigation in VLMs like CLIP. VLMs often inherit and amplify biases in
their training data, leading to skewed predictions. PRISM is designed to debias
VLMs without relying on predefined bias categories or additional external data.
It operates in two stages: first, an LLM is prompted with simple class prompts
to generate scene descriptions that contain spurious correlations. Next, PRISM
uses our novel contrastive-style debiasing loss to learn a projection that maps
the embeddings onto a latent space that minimizes spurious correlations while
preserving the alignment between image and text embeddings.Extensive
experiments demonstrate that PRISM outperforms current debiasing methods on the
commonly used Waterbirds and CelebA datasets We make our code public at:
https://github.com/MahdiyarMM/PRISM.

</details>


### [111] [Video Inference for Human Mesh Recovery with Vision Transformer](https://arxiv.org/abs/2507.08981)
*Hanbyel Cho,Jaesung Ahn,Yooshin Cho,Junmo Kim*

Main category: cs.CV

TL;DR: 提出HMR-ViT方法，结合时空和运动学信息，通过Vision Transformer和回归网络实现人体网格恢复。


<details>
  <summary>Details</summary>
Motivation: 现有HMR方法仅利用时空或运动学信息，未结合两者，限制了准确性。

Method: 构建时空-运动学特征图像，使用Channel Rearranging Matrix优化特征排列，通过Vision Transformer和回归网络推断SMPL参数。

Result: 在3DPW和Human3.6M数据集上表现优异。

Conclusion: HMR-ViT结合时空和运动学信息，显著提升了人体网格恢复的准确性。

Abstract: Human Mesh Recovery (HMR) from an image is a challenging problem because of
the inherent ambiguity of the task. Existing HMR methods utilized either
temporal information or kinematic relationships to achieve higher accuracy, but
there is no method using both. Hence, we propose "Video Inference for Human
Mesh Recovery with Vision Transformer (HMR-ViT)" that can take into account
both temporal and kinematic information. In HMR-ViT, a Temporal-kinematic
Feature Image is constructed using feature vectors obtained from video frames
by an image encoder. When generating the feature image, we use a Channel
Rearranging Matrix (CRM) so that similar kinematic features could be located
spatially close together. The feature image is then further encoded using
Vision Transformer, and the SMPL pose and shape parameters are finally inferred
using a regression network. Extensive evaluation on the 3DPW and Human3.6M
datasets indicates that our method achieves a competitive performance in HMR.

</details>


### [112] [From images to properties: a NeRF-driven framework for granular material parameter inversion](https://arxiv.org/abs/2507.09005)
*Cheng-Hsi Hsiao,Krishna Kumar*

Main category: cs.CV

TL;DR: 结合NeRF和MPM，通过视觉观察推断颗粒材料特性，误差在2度以内。


<details>
  <summary>Details</summary>
Motivation: 在无法直接测量的情况下，通过视觉观察表征颗粒材料特性。

Method: 生成合成实验数据，用NeRF重建3D几何，MPM模拟未知摩擦角，通过贝叶斯优化最小化图像损失。

Result: 摩擦角估计误差在2度以内。

Conclusion: 该方法为颗粒材料特性表征提供了有效解决方案。

Abstract: We introduce a novel framework that integrates Neural Radiance Fields (NeRF)
with Material Point Method (MPM) simulation to infer granular material
properties from visual observations. Our approach begins by generating
synthetic experimental data, simulating an plow interacting with sand. The
experiment is rendered into realistic images as the photographic observations.
These observations include multi-view images of the experiment's initial state
and time-sequenced images from two fixed cameras. Using NeRF, we reconstruct
the 3D geometry from the initial multi-view images, leveraging its capability
to synthesize novel viewpoints and capture intricate surface details. The
reconstructed geometry is then used to initialize material point positions for
the MPM simulation, where the friction angle remains unknown. We render images
of the simulation under the same camera setup and compare them to the observed
images. By employing Bayesian optimization, we minimize the image loss to
estimate the best-fitting friction angle. Our results demonstrate that friction
angle can be estimated with an error within 2 degrees, highlighting the
effectiveness of inverse analysis through purely visual observations. This
approach offers a promising solution for characterizing granular materials in
real-world scenarios where direct measurement is impractical or impossible.

</details>


### [113] [Infinite Video Understanding](https://arxiv.org/abs/2507.09068)
*Dell Zhang,Xiangyu Chen,Jixiang Luo,Mengxi Jia,Changzhi Sun,Ruilong Ren,Jingren Liu,Hao Sun,Xuelong Li*

Main category: cs.CV

TL;DR: 论文提出无限视频理解（Infinite Video Understanding）作为多媒体研究的新前沿，旨在解决长视频处理的挑战。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型和多模态扩展在视频理解方面取得进展，但处理超长视频时仍面临计算、内存和时序一致性等挑战。

Method: 提出将无限视频理解作为研究目标，推动流式架构、持久记忆机制、分层表示等创新。

Result: 尽管现有技术如Video-XL-2和VideoRoPE++有所突破，但超长视频处理仍存在显著限制。

Conclusion: 无限视频理解是一个雄心勃勃但必要的目标，将为多媒体和AI研究指明方向。

Abstract: The rapid advancements in Large Language Models (LLMs) and their multimodal
extensions (MLLMs) have ushered in remarkable progress in video understanding.
However, a fundamental challenge persists: effectively processing and
comprehending video content that extends beyond minutes or hours. While recent
efforts like Video-XL-2 have demonstrated novel architectural solutions for
extreme efficiency, and advancements in positional encoding such as HoPE and
VideoRoPE++ aim to improve spatio-temporal understanding over extensive
contexts, current state-of-the-art models still encounter significant
computational and memory constraints when faced with the sheer volume of visual
tokens from lengthy sequences. Furthermore, maintaining temporal coherence,
tracking complex events, and preserving fine-grained details over extended
periods remain formidable hurdles, despite progress in agentic reasoning
systems like Deep Video Discovery. This position paper posits that a logical,
albeit ambitious, next frontier for multimedia research is Infinite Video
Understanding -- the capability for models to continuously process, understand,
and reason about video data of arbitrary, potentially never-ending duration. We
argue that framing Infinite Video Understanding as a blue-sky research
objective provides a vital north star for the multimedia, and the wider AI,
research communities, driving innovation in areas such as streaming
architectures, persistent memory mechanisms, hierarchical and adaptive
representations, event-centric reasoning, and novel evaluation paradigms.
Drawing inspiration from recent work on long/ultra-long video understanding and
several closely related fields, we outline the core challenges and key research
directions towards achieving this transformative capability.

</details>


### [114] [VISTA: A Visual Analytics Framework to Enhance Foundation Model-Generated Data Labels](https://arxiv.org/abs/2507.09008)
*Xiwei Xuan,Xiaoqi Wang,Wenbin He,Jorge Piazentin Ono,Liang Gou,Kwan-Liu Ma,Liu Ren*

Main category: cs.CV

TL;DR: VISTA是一个视觉分析框架，旨在提升多模态基础模型生成标签的质量，通过结合多阶段数据验证策略和人类专家知识，解决现有方法在数据质量评估上的不足。


<details>
  <summary>Details</summary>
Motivation: 现有方法多关注数据数量而非质量，且缺乏全面评估手段或仅依赖有限人工验证，难以全面解决多模态模型标签质量问题。

Method: 提出VISTA框架，结合多阶段数据验证策略和人类专家知识，用于识别和修正多模态模型生成的标签问题。

Result: 在开放词汇图像分割领域的两组基准数据集上，VISTA通过定量和定性分析证明了其有效性。

Conclusion: VISTA显著提升了多模态模型标签质量，为数据质量评估提供了全面解决方案。

Abstract: The advances in multi-modal foundation models (FMs) (e.g., CLIP and LLaVA)
have facilitated the auto-labeling of large-scale datasets, enhancing model
performance in challenging downstream tasks such as open-vocabulary object
detection and segmentation. However, the quality of FM-generated labels is less
studied as existing approaches focus more on data quantity over quality. This
is because validating large volumes of data without ground truth presents a
considerable challenge in practice. Existing methods typically rely on limited
metrics to identify problematic data, lacking a comprehensive perspective, or
apply human validation to only a small data fraction, failing to address the
full spectrum of potential issues. To overcome these challenges, we introduce
VISTA, a visual analytics framework that improves data quality to enhance the
performance of multi-modal models. Targeting the complex and demanding domain
of open-vocabulary image segmentation, VISTA integrates multi-phased data
validation strategies with human expertise, enabling humans to identify,
understand, and correct hidden issues within FM-generated labels. Through
detailed use cases on two benchmark datasets and expert reviews, we demonstrate
VISTA's effectiveness from both quantitative and qualitative perspectives.

</details>


### [115] [Ambiguity-Aware and High-Order Relation Learning for Multi-Grained Image-Text Matching](https://arxiv.org/abs/2507.09256)
*Junyu Chen,Yihua Gao,Mingyuan Ge,Mingyong Li*

Main category: cs.CV

TL;DR: 本文提出了一种名为AAHR的框架，通过动态聚类原型对比学习解决图像-文本匹配中的语义模糊和高阶关联问题，显著提升了匹配性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法在处理高阶关联和语义模糊时存在不足，尤其是软正样本和软负样本的区分问题，以及未能充分利用训练批次中样本间的邻域关系。

Method: AAHR通过动态聚类原型对比学习构建统一表示空间，结合全局和局部特征提取、自适应聚合网络、GNN增强语义交互，以及动量对比学习扩展负样本集。

Result: AAHR在Flickr30K、MSCOCO和ECCV Caption数据集上表现优于现有方法，显著提升了匹配准确性和效率。

Conclusion: AAHR通过多策略结合有效解决了图像-文本匹配中的语义模糊和高阶关联问题，为跨模态学习提供了新思路。

Abstract: Image-text matching is crucial for bridging the semantic gap between computer
vision and natural language processing. However, existing methods still face
challenges in handling high-order associations and semantic ambiguities among
similar instances. These ambiguities arise from subtle differences between soft
positive samples (semantically similar but incorrectly labeled) and soft
negative samples (locally matched but globally inconsistent), creating matching
uncertainties. Furthermore, current methods fail to fully utilize the
neighborhood relationships among semantically similar instances within training
batches, limiting the model's ability to learn high-order shared knowledge.
This paper proposes the Ambiguity-Aware and High-order Relation learning
framework (AAHR) to address these issues. AAHR constructs a unified
representation space through dynamic clustering prototype contrastive learning,
effectively mitigating the soft positive sample problem. The framework
introduces global and local feature extraction mechanisms and an adaptive
aggregation network, significantly enhancing full-grained semantic
understanding capabilities. Additionally, AAHR employs intra-modal and
inter-modal correlation matrices to investigate neighborhood relationships
among sample instances thoroughly. It incorporates GNN to enhance semantic
interactions between instances. Furthermore, AAHR integrates momentum
contrastive learning to expand the negative sample set. These combined
strategies significantly improve the model's ability to discriminate between
features. Experimental results demonstrate that AAHR outperforms existing
state-of-the-art methods on Flickr30K, MSCOCO, and ECCV Caption datasets,
considerably improving the accuracy and efficiency of image-text matching. The
code and model checkpoints for this research are available at
https://github.com/Image-Text-Matching/AAHR .

</details>


### [116] [BrainLesion Suite: A Flexible and User-Friendly Framework for Modular Brain Lesion Image Analysis](https://arxiv.org/abs/2507.09036)
*Florian Kofler,Marcel Rosier,Mehdi Astaraki,Hendrik Möller,Ilhem Isra Mekki,Josef A. Buchner,Anton Schmick,Arianna Pfiffer,Eva Oswald,Lucas Zimmer,Ezequiel de la Rosa,Sarthak Pati,Julian Canisius,Arianna Piffer,Ujjwal Baid,Mahyar Valizadeh,Akis Linardos,Jan C. Peeken,Surprosanna Shit,Felix Steinbauer,Daniel Rueckert,Rolf Heckemann,Spyridon Bakas,Jan Kirschke,Constantin von See,Ivan Ezhov,Marie Piraud,Benedikt Wiestler,Bjoern Menze*

Main category: cs.CV

TL;DR: BrainLesion Suite是一个用于构建模块化脑部病变图像分析管道的Python工具包，旨在简化开发流程并支持临床和科研应用。


<details>
  <summary>Details</summary>
Motivation: 为脑部病变（如胶质瘤、转移瘤和多发性硬化症）的图像分析提供高效、灵活的解决方案，减少开发者的认知负担。

Method: 基于Pythonic原则设计，包含适应性预处理模块，支持多模态图像处理（如配准、去颅骨等），并利用BraTS挑战赛算法进行缺失模态合成和病变修复。

Result: 提供了完整的工具链，包括病变分割性能量化工具（如panoptica），适用于脑部病变分析，并可扩展至其他生物医学图像分析领域。

Conclusion: BrainLesion Suite是一个功能强大且灵活的工具包，适用于脑部病变图像分析，并可通过GitHub获取。

Abstract: BrainLesion Suite is a versatile toolkit for building modular brain lesion
image analysis pipelines in Python. Following Pythonic principles, BrainLesion
Suite is designed to provide a 'brainless' development experience, minimizing
cognitive effort and streamlining the creation of complex workflows for
clinical and scientific practice. At its core is an adaptable preprocessing
module that performs co-registration, atlas registration, and optional
skull-stripping and defacing on arbitrary multi-modal input images. BrainLesion
Suite leverages algorithms from the BraTS challenge to synthesize missing
modalities, inpaint lesions, and generate pathology-specific tumor
segmentations. BrainLesion Suite also enables quantifying segmentation model
performance, with tools such as panoptica to compute lesion-wise metrics.
Although BrainLesion Suite was originally developed for image analysis
pipelines of brain lesions such as glioma, metastasis, and multiple sclerosis,
it can be adapted for other biomedical image analysis applications. The
individual BrainLesion Suite packages and tutorials are accessible on GitHub.

</details>


### [117] [Can Contrastive Learning Improve Class-Imbalanced Diffusion Model?](https://arxiv.org/abs/2507.09052)
*Fang Chen,Alex Villa,Gongbo Liang,Xiaoyi Lu,Meng Tang*

Main category: cs.CV

TL;DR: 论文提出两种对比损失函数，用于解决类别不平衡数据中尾部类图像多样性不足的问题，提升扩散模型的生成效果。


<details>
  <summary>Details</summary>
Motivation: 类别不平衡数据导致尾部类图像生成多样性不足，影响扩散模型的性能。

Method: 引入两种对比损失函数：无监督InfoNCE损失和MSE损失，通过对比学习和条件-无条件对齐提升尾部类多样性。

Result: 方法在多个数据集（如CIFAR10/100-LT等）上优于标准DDPM和其他替代方法。

Conclusion: 对比学习框架简单有效，显著提升了类别不平衡扩散模型的性能。

Abstract: Training data for class-conditional image synthesis often exhibit a
long-tailed distribution with limited images for tail classes. Such an
imbalance causes mode collapse and reduces the diversity of synthesized images
for tail classes. For class-conditional diffusion models trained on imbalanced
data, we aim to improve the diversity of tail class images without compromising
the fidelity and diversity of head class images. We achieve this by introducing
two deceptively simple but highly effective contrastive loss functions.
Firstly, we employ an unsupervised InfoNCE loss utilizing negative samples to
increase the distance/dissimilarity among synthetic images, particularly for
tail classes. To further enhance the diversity of tail classes, our second loss
is an MSE loss that contrasts class-conditional generation with unconditional
generation at large timesteps. This second loss makes the denoising process
insensitive to class conditions for the initial steps, which enriches tail
classes through knowledge sharing from head classes. Conditional-unconditional
alignment has been shown to enhance the performance of long-tailed GAN. We are
the first to adapt such alignment to diffusion models. We successfully
leveraged contrastive learning for class-imbalanced diffusion models. Our
contrastive learning framework is easy to implement and outperforms standard
DDPM and alternative methods for class-imbalanced diffusion models across
various datasets, including CIFAR10/100-LT, PlacesLT, TinyImageNetLT, and
ImageNetLT.

</details>


### [118] [Text-to-Remote-Sensing-Image Retrieval beyond RGB Sources](https://arxiv.org/abs/2507.10403)
*Daniele Rege Cambrin,Lorenzo Vaiani,Giuseppe Gallipoli,Luca Cagliero,Paolo Garza*

Main category: cs.CV

TL;DR: 论文提出CrisisLandMark数据集和CLOSP框架，通过文本对齐光学与SAR图像，提升检索性能54%，并整合地理坐标增强特定任务表现。


<details>
  <summary>Details</summary>
Motivation: 现有文本到图像检索系统主要针对RGB数据，未能充分利用多传感器（如SAR和多光谱）的独特信息，限制了在灾害响应和气候监测中的应用。

Method: 引入CrisisLandMark数据集（64.7万张图像与结构化文本标注），提出CLOSP框架，通过对比学习对齐光学与SAR图像嵌入空间，并扩展为GeoCLOSP以整合地理坐标。

Result: CLOSP在检索任务中nDGC提升54%，GeoCLOSP在位置依赖任务中表现更优。

Conclusion: 多传感器数据与地理背景的整合对遥感档案的充分利用至关重要。

Abstract: Retrieving relevant imagery from vast satellite archives is crucial for
applications like disaster response and long-term climate monitoring. However,
most text-to-image retrieval systems are limited to RGB data, failing to
exploit the unique physical information captured by other sensors, such as the
all-weather structural sensitivity of Synthetic Aperture Radar (SAR) or the
spectral signatures in optical multispectral data. To bridge this gap, we
introduce CrisisLandMark, a new large-scale corpus of over 647,000 Sentinel-1
SAR and Sentinel-2 multispectral images paired with structured textual
annotations for land cover, land use, and crisis events harmonized from
authoritative land cover systems (CORINE and Dynamic World) and crisis-specific
sources. We then present CLOSP (Contrastive Language Optical SAR Pretraining),
a novel framework that uses text as a bridge to align unpaired optical and SAR
images into a unified embedding space. Our experiments show that CLOSP achieves
a new state-of-the-art, improving retrieval nDGC by 54% over existing models.
Additionally, we find that the unified training strategy overcomes the inherent
difficulty of interpreting SAR imagery by transferring rich semantic knowledge
from the optical domain with indirect interaction. Furthermore, GeoCLOSP, which
integrates geographic coordinates into our framework, creates a powerful
trade-off between generality and specificity: while the CLOSP excels at general
semantic tasks, the GeoCLOSP becomes a specialized expert for retrieving
location-dependent crisis events and rare geographic features. This work
highlights that the integration of diverse sensor data and geographic context
is essential for unlocking the full potential of remote sensing archives.

</details>


### [119] [BlindSight: Harnessing Sparsity for Efficient VLMs](https://arxiv.org/abs/2507.09071)
*Tharun Adithya Srikrishnan,Deval Shah,Steven K. Reinhardt*

Main category: cs.CV

TL;DR: BlindSight是一种无需训练的方法，通过利用注意力稀疏性优化视觉语言模型的推理效率，显著减少计算量（FLOPs）且保持准确性。


<details>
  <summary>Details</summary>
Motivation: 视觉语言模型（VLMs）因处理图像数据导致提示长度增加和注意力计算复杂度高，推理速度慢。

Method: 分析VLMs中的注意力模式，提出基于输入模板的稀疏注意力掩码（BlindSight），分类注意力头以减少冗余计算。

Result: 在Qwen2-VL等模型上，BlindSight平均减少32%-41%的FLOPs，准确性变化在±2%以内。

Conclusion: BlindSight有效提升VLMs推理效率，适用于多图像理解任务。

Abstract: Large vision-language models (VLMs) enable the joint processing of text and
images. However, the inclusion of vision data significantly expands the prompt
length. Along with the quadratic complexity of the attention computation, this
results in a longer prefill duration. An approach to mitigate this bottleneck
is to leverage the inherent sparsity in the attention computation. In our
analysis of attention patterns in VLMs, we observe that a substantial portion
of layers exhibit minimal cross-image attention, except through attention-sink
tokens per image. These sparse attention patterns fall into distinct
categories: sink-only, document mask and a hybrid document-sink mask. Based on
this, we propose BlindSight: a training-free approach to optimize VLM inference
using a input template-aware attention sparsity mask. We utilize samples from a
dataset to derive a prompt-agnostic sparsity categorization for every attention
head. We evaluate the proposed technique using VLMs such as Qwen2-VL,
Qwen2.5-VL and Gemma-3. BlindSight results in a 32%-41% reduction in FLOPs on
average with -2%-+2% accuracy compared to the original model in most evaluated
multi-image understanding benchmarks.

</details>


### [120] [RAPNet: A Receptive-Field Adaptive Convolutional Neural Network for Pansharpening](https://arxiv.org/abs/2507.10461)
*Tao Tang,Chengxu Yang*

Main category: cs.CV

TL;DR: RAPNet提出了一种基于内容自适应卷积的Pansharpening新架构，通过RAPConv和PAN-DFF模块提升空间细节提取和光谱保真度，性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 传统CNN在Pansharpening中因卷积核的均匀应用而忽略局部内容变化，限制了性能。

Method: RAPNet采用RAPConv生成空间自适应卷积核，并结合PAN-DFF模块的注意力机制优化空间细节与光谱保真度的平衡。

Result: 在公开数据集上，RAPNet在定量和定性评估中均优于现有方法，消融实验验证了自适应组件的有效性。

Conclusion: RAPNet通过自适应卷积和动态特征融合，显著提升了Pansharpening的性能。

Abstract: Pansharpening refers to the process of integrating a high resolution
panchromatic (PAN) image with a lower resolution multispectral (MS) image to
generate a fused product, which is pivotal in remote sensing. Despite the
effectiveness of CNNs in addressing this challenge, they are inherently
constrained by the uniform application of convolutional kernels across all
spatial positions, overlooking local content variations. To overcome this
issue, we introduce RAPNet, a new architecture that leverages content-adaptive
convolution. At its core, RAPNet employs the Receptive-field Adaptive
Pansharpening Convolution (RAPConv), designed to produce spatially adaptive
kernels responsive to local feature context, thereby enhancing the precision of
spatial detail extraction. Additionally, the network integrates the
Pansharpening Dynamic Feature Fusion (PAN-DFF) module, which incorporates an
attention mechanism to achieve an optimal balance between spatial detail
enhancement and spectral fidelity. Comprehensive evaluations on publicly
available datasets confirm that RAPNet delivers superior performance compared
to existing approaches, as demonstrated by both quantitative metrics and
qualitative assessments. Ablation analyses further substantiate the
effectiveness of the proposed adaptive components.

</details>


### [121] [From Physics to Foundation Models: A Review of AI-Driven Quantitative Remote Sensing Inversion](https://arxiv.org/abs/2507.09081)
*Zhenyu Yu,Mohd Yamani Idna Idris,Hua Wang,Pei Wang,Junyi Chen,Kun Wang*

Main category: cs.CV

TL;DR: 本文综述了定量遥感反演方法从物理模型到机器学习再到基础模型的演变，比较了各范式的假设、应用场景和局限性，并展望了下一代基础模型的发展方向。


<details>
  <summary>Details</summary>
Motivation: 随着遥感系统和人工智能的发展，传统物理模型逐渐被数据驱动和基础模型方法取代，需要系统梳理方法演变并解决现有挑战。

Method: 系统回顾了物理模型（如PROSPECT）、机器学习（如深度学习）和基础模型（如SatMAE）的技术演进，比较了各范式的特点。

Result: 总结了各方法的优缺点，强调了基础模型在自监督预训练、多模态融合和跨任务适应方面的进展。

Conclusion: 展望了下一代基础模型的发展，重点在于统一建模能力、跨域泛化和物理可解释性。

Abstract: Quantitative remote sensing inversion aims to estimate continuous surface
variables-such as biomass, vegetation indices, and evapotranspiration-from
satellite observations, supporting applications in ecosystem monitoring, carbon
accounting, and land management. With the evolution of remote sensing systems
and artificial intelligence, traditional physics-based paradigms are giving way
to data-driven and foundation model (FM)-based approaches. This paper
systematically reviews the methodological evolution of inversion techniques,
from physical models (e.g., PROSPECT, SCOPE, DART) to machine learning methods
(e.g., deep learning, multimodal fusion), and further to foundation models
(e.g., SatMAE, GFM, mmEarth). We compare the modeling assumptions, application
scenarios, and limitations of each paradigm, with emphasis on recent FM
advances in self-supervised pretraining, multi-modal integration, and
cross-task adaptation. We also highlight persistent challenges in physical
interpretability, domain generalization, limited supervision, and uncertainty
quantification. Finally, we envision the development of next-generation
foundation models for remote sensing inversion, emphasizing unified modeling
capacity, cross-domain generalization, and physical interpretability.

</details>


### [122] [Taming generative video models for zero-shot optical flow extraction](https://arxiv.org/abs/2507.09082)
*Seungwoo Kim,Khai Loong Aw,Klemen Kotar,Cristobal Eyzaguirre,Wanhee Lee,Yunong Liu,Jared Watrous,Stefan Stojanov,Juan Carlos Niebles,Jiajun Wu,Daniel L. K. Yamins*

Main category: cs.CV

TL;DR: 论文提出了一种无需微调的零样本光流提取方法，通过扰动生成视频模型并跟踪传播，性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 利用大型通用视频模型，无需微调即可提取光流，解决标签稀缺和合成数据与真实数据差距的问题。

Method: 基于CWM范式，提出KL-tracing方法，通过注入扰动并计算预测分布差异来提取光流。

Result: 在TAP-Vid DAVIS和Kubric数据集上分别取得16.6%和4.7%的相对性能提升。

Conclusion: 基于可控生成视频模型的对抗提示是一种高效且可扩展的光流提取替代方案。

Abstract: Extracting optical flow from videos remains a core computer vision problem.
Motivated by the success of large general-purpose models, we ask whether frozen
self-supervised video models trained only for future frame prediction can be
prompted, without fine-tuning, to output flow. Prior work reading out depth or
illumination from video generators required fine-tuning, which is impractical
for flow where labels are scarce and synthetic datasets suffer from a
sim-to-real gap. Inspired by the Counterfactual World Model (CWM) paradigm,
which can obtain point-wise correspondences by injecting a small tracer
perturbation into a next-frame predictor and tracking its propagation, we
extend this idea to generative video models. We explore several popular
architectures and find that successful zero-shot flow extraction in this manner
is aided by three model properties: (1) distributional prediction of future
frames (avoiding blurry or noisy outputs); (2) factorized latents that treat
each spatio-temporal patch independently; and (3) random-access decoding that
can condition on any subset of future pixels. These properties are uniquely
present in the recent Local Random Access Sequence (LRAS) architecture.
Building on LRAS, we propose KL-tracing: a novel test-time procedure that
injects a localized perturbation into the first frame, rolls out the model one
step, and computes the Kullback-Leibler divergence between perturbed and
unperturbed predictive distributions. Without any flow-specific fine-tuning,
our method outperforms state-of-the-art models on real-world TAP-Vid DAVIS
dataset (16.6% relative improvement for endpoint error) and synthetic TAP-Vid
Kubric (4.7% relative improvement). Our results indicate that counterfactual
prompting of controllable generative video models is a scalable and effective
alternative to supervised or photometric-loss approaches for high-quality flow.

</details>


### [123] [MI CAM: Mutual Information Weighted Activation Mapping for Causal Visual Explanations of Convolutional Neural Networks](https://arxiv.org/abs/2507.09092)
*Ram S Iyer,Narayan S Iyer,Rugmini Ammal P*

Main category: cs.CV

TL;DR: 本文提出了一种基于激活映射的后验视觉解释方法MI CAM，通过特征图与输入图像的互信息加权生成显著性可视化，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 随着机器视觉在医疗和自动化电厂等关键领域的应用，理解卷积神经网络的内部机制及其推理原因变得重要。

Method: MI CAM通过特征图与输入图像的互信息加权，线性组合权重和激活图生成显著性可视化，并通过反事实分析验证因果解释。

Result: MI CAM在定性和定量指标上优于部分现有方法，与最先进方法表现相当。

Conclusion: MI CAM提供了无偏的模型推理可视化解释，性能优越，适用于关键领域。

Abstract: With the intervention of machine vision in our crucial day to day necessities
including healthcare and automated power plants, attention has been drawn to
the internal mechanisms of convolutional neural networks, and the reason why
the network provides specific inferences. This paper proposes a novel post-hoc
visual explanation method called MI CAM based on activation mapping. Differing
from previous class activation mapping based approaches, MI CAM produces
saliency visualizations by weighing each feature map through its mutual
information with the input image and the final result is generated by a linear
combination of weights and activation maps. It also adheres to producing causal
interpretations as validated with the help of counterfactual analysis. We aim
to exhibit the visual performance and unbiased justifications for the model
inferencing procedure achieved by MI CAM. Our approach works at par with all
state-of-the-art methods but particularly outperforms some in terms of
qualitative and quantitative measures. The implementation of proposed method
can be found on https://anonymous.4open.science/r/MI-CAM-4D27

</details>


### [124] [RadEyeVideo: Enhancing general-domain Large Vision Language Model for chest X-ray analysis with video representations of eye gaze](https://arxiv.org/abs/2507.09097)
*Yunsoo Kim,Jinge Wu,Honghan Wu*

Main category: cs.CV

TL;DR: RadEyeVideo通过整合放射科医生的眼动视频序列，显著提升了大型视觉语言模型在胸部X光分析和报告生成任务中的性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法忽略了眼动的顺序信息，而RadEyeVideo旨在通过捕捉眼动的时空动态来提升模型性能。

Method: 提出RadEyeVideo方法，将眼动数据作为视频序列输入，利用支持视频输入的通用领域LVLMs进行评估。

Result: 模型性能在报告生成任务中提升24.6%，在诊断任务中平均提升15.2%，甚至超越专业医疗模型。

Conclusion: RadEyeVideo展示了专家知识（眼动信息）与通用领域模型结合的巨大潜力，为医疗图像分析提供了一种可扩展的人本方法。

Abstract: Large Vision-Language Models (LVLMs) have demonstrated promising performance
in chest X-ray (CXR) analysis. To enhance human-computer interaction, several
studies have incorporated radiologists' eye gaze, typically through heatmaps or
textual prompts. However, these methods often overlook the sequential order of
eye movements, which could provide valuable insights by highlighting both the
areas of interest and the order in which they are examined. In this work, we
propose a novel approach called RadEyeVideo that integrates radiologists'
eye-fixation data as a video sequence, capturing both the temporal and spatial
dynamics of their gaze. We evaluate this method in CXR report generation and
disease diagnosis using three general-domain, open-source LVLMs with video
input capabilities. When prompted with eye-gaze videos, model performance
improves by up to 24.6% in the report generation task and on average 15.2% for
both tasks using scaled evaluation metrics. Notably, RadEyeVideo enhanced an
open-domain LVLM model, LLaVA-OneVision, to surpass task-specific medical LVLMs
such as MAIRA-2 and CheXagent, trained on large Chest X-ray data. This work
highlights that domain expert's knowledge (eye-gaze information in this case),
when effectively integrated with LVLMs, can significantly enhance
general-domain models' capabilities in clinical tasks. RadEyeVideo is a step
toward a scalable human-centered approach of utilizing LVLMs in medical image
analytics.

</details>


### [125] [Harnessing Text-to-Image Diffusion Models for Point Cloud Self-Supervised Learning](https://arxiv.org/abs/2507.09102)
*Yiyang Chen,Shanshan Zhao,Lunhao Duan,Changxing Ding,Dacheng Tao*

Main category: cs.CV

TL;DR: PointSD利用Stable Diffusion模型增强3D点云自监督学习，通过点云引导图像去噪和特征对齐，提升3D表示能力。


<details>
  <summary>Details</summary>
Motivation: 现有3D扩散模型受限于小规模数据集，而文本到图像扩散模型（如Stable Diffusion）在大规模数据上表现优异，可用于改进3D自监督学习。

Method: 提出PointSD框架，将Stable Diffusion的文本编码器替换为3D编码器，训练点云到图像的扩散模型，并通过特征对齐训练3D骨干网络。

Result: 实验表明，PointSD能有效提升点云自监督学习在下游任务中的表现。

Conclusion: Stable Diffusion模型可显著增强3D点云的自监督学习能力。

Abstract: Diffusion-based models, widely used in text-to-image generation, have proven
effective in 2D representation learning. Recently, this framework has been
extended to 3D self-supervised learning by constructing a conditional point
generator for enhancing 3D representations. However, its performance remains
constrained by the 3D diffusion model, which is trained on the available 3D
datasets with limited size. We hypothesize that the robust capabilities of
text-to-image diffusion models, particularly Stable Diffusion (SD), which is
trained on large-scale datasets, can help overcome these limitations. To
investigate this hypothesis, we propose PointSD, a framework that leverages the
SD model for 3D self-supervised learning. By replacing the SD model's text
encoder with a 3D encoder, we train a point-to-image diffusion model that
allows point clouds to guide the denoising of rendered noisy images. With the
trained point-to-image diffusion model, we use noise-free images as the input
and point clouds as the condition to extract SD features. Next, we train a 3D
backbone by aligning its features with these SD features, thereby facilitating
direct semantic learning. Comprehensive experiments on downstream point cloud
tasks and ablation studies demonstrate that the SD model can enhance point
cloud self-supervised learning. Code is publicly available at
https://github.com/wdttt/PointSD.

</details>


### [126] [Hybrid Autoregressive-Diffusion Model for Real-Time Streaming Sign Language Production](https://arxiv.org/abs/2507.09105)
*Maoxiao Ye,Xinfeng Ye,Mano Manoharan*

Main category: cs.CV

TL;DR: 论文提出了一种结合自回归和扩散模型的混合方法，用于手语生成（SLP），通过多尺度姿态表示和置信感知因果注意力机制提升生成质量和实时性。


<details>
  <summary>Details</summary>
Motivation: 传统自回归方法在推理阶段存在错误累积问题，而扩散模型因迭代性质不适用于实时任务。

Method: 采用混合自回归和扩散模型，设计多尺度姿态表示模块和置信感知因果注意力机制。

Result: 在PHOENIX14T和How2Sign数据集上验证了方法的生成质量和实时效率。

Conclusion: 混合方法有效结合了两种模型的优势，提升了手语生成的准确性和实时性。

Abstract: Earlier Sign Language Production (SLP) models typically relied on
autoregressive methods that generate output tokens one by one, which inherently
provide temporal alignment. Although techniques like Teacher Forcing can
prevent model collapse during training, they still cannot solve the problem of
error accumulation during inference, since ground truth is unavailable at that
stage. In contrast, more recent approaches based on diffusion models leverage
step-by-step denoising to enable high-quality generation. However, the
iterative nature of these models and the requirement to denoise entire
sequences limit their applicability in real-time tasks like SLP. To address it,
we apply a hybrid approach combining autoregressive and diffusion models to SLP
for the first time, leveraging the strengths of both models in sequential
dependency modeling and output refinement. To capture fine-grained body
movements, we design a Multi-Scale Pose Representation module that separately
extracts detailed features from distinct articulators and integrates them via a
Multi-Scale Fusion module. Furthermore, we introduce a Confidence-Aware Causal
Attention mechanism that utilizes joint-level confidence scores to dynamically
guide the pose generation process, improving accuracy and robustness. Extensive
experiments on the PHOENIX14T and How2Sign datasets demonstrate the
effectiveness of our method in both generation quality and real-time streaming
efficiency.

</details>


### [127] [RoHOI: Robustness Benchmark for Human-Object Interaction Detection](https://arxiv.org/abs/2507.09111)
*Di Wen,Kunyu Peng,Kailun Yang,Yufan Chen,Ruiping Liu,Junwei Zheng,Alina Roitberg,Rainer Stiefelhagen*

Main category: cs.CV

TL;DR: 论文提出了首个针对人-物交互（HOI）检测的鲁棒性基准RoHOI，并提出了语义感知掩蔽渐进学习策略（SAMPL）以提升模型在复杂环境中的性能。


<details>
  <summary>Details</summary>
Motivation: 现有HOI检测模型在真实世界中的性能因环境干扰（如遮挡、噪声）而下降，缺乏针对鲁棒性的评估标准。

Method: 基于HICO-DET和V-COCO数据集构建了包含20种干扰类型的RoHOI基准，并提出SAMPL策略，通过动态优化模型学习鲁棒特征。

Result: 实验表明SAMPL优于现有方法，显著提升了模型在干扰条件下的性能。

Conclusion: RoHOI为HOI检测的鲁棒性研究提供了新标准，SAMPL策略有效提升了模型在复杂环境中的表现。

Abstract: Human-Object Interaction (HOI) detection is crucial for robot-human
assistance, enabling context-aware support. However, models trained on clean
datasets degrade in real-world conditions due to unforeseen corruptions,
leading to inaccurate prediction. To address this, we introduce the first
robustness benchmark for HOI detection, evaluating model resilience under
diverse challenges. Despite advances, current models struggle with
environmental variability, occlusion, and noise. Our benchmark, RoHOI, includes
20 corruption types based on HICO-DET and V-COCO datasets and a new
robustness-focused metric. We systematically analyze existing models in the
related field, revealing significant performance drops under corruptions. To
improve robustness, we propose a Semantic-Aware Masking-based Progressive
Learning (SAMPL) strategy to guide the model to be optimized based on holistic
and partial cues, dynamically adjusting the model's optimization to enhance
robust feature learning. Extensive experiments show our approach outperforms
state-of-the-art methods, setting a new standard for robust HOI detection.
Benchmarks, datasets, and code will be made publicly available at
https://github.com/Kratos-Wen/RoHOI.

</details>


### [128] [Mind the Gap: Preserving and Compensating for the Modality Gap in CLIP-Based Continual Learning](https://arxiv.org/abs/2507.09118)
*Linlan Huang,Xusheng Cao,Haori Lu,Yifan Meng,Fei Yang,Xialei Liu*

Main category: cs.CV

TL;DR: 论文提出了一种名为MG-CLIP的方法，通过分析CLIP模型中的模态间隙变化，利用模态间隙保持和补偿来提升持续学习性能。


<details>
  <summary>Details</summary>
Motivation: 现有研究忽视了CLIP模型中的模态间隙，而这一间隙对其泛化和适应能力至关重要。论文旨在通过分析模态间隙变化，提升CLIP在持续学习中的表现。

Method: 提出MG-CLIP方法，利用模态间隙保持减少遗忘，并通过模态间隙补偿增强新数据学习能力。

Result: 在多个基准测试中，MG-CLIP优于现有方法，且无需额外回放数据。

Conclusion: 模态间隙是持续学习中的关键因素，MG-CLIP为持续学习提供了新的视角和有效解决方案。

Abstract: Continual learning aims to enable models to learn sequentially from
continuously incoming data while retaining performance on previously learned
tasks. With the Contrastive Language-Image Pre-trained model (CLIP) exhibiting
strong capabilities across various downstream tasks, there has been growing
interest in leveraging CLIP for continual learning in such scenarios. Most
existing works overlook the inherent modality gap in CLIP, a key factor in its
generalization and adaptability. In this paper, we analyze the variations in
the modality gap during the fine-tuning of vision-language pre-trained models.
Our observations reveal that the modality gap effectively reflects the extent
to which pre-trained knowledge is preserved. Based on these insights, we
propose a simple yet effective method, MG-CLIP, that improves CLIP's
performance in class-incremental learning. Our approach leverages modality gap
preservation to mitigate forgetting and modality gap compensation to enhance
the capacity for new data, introducing a novel modality-gap-based perspective
for continual learning. Extensive experiments on multiple benchmarks
demonstrate that our method outperforms existing approaches without requiring
additional replay data. Our code is available at
https://github.com/linlany/MindtheGap.

</details>


### [129] [SnapMoGen: Human Motion Generation from Expressive Texts](https://arxiv.org/abs/2507.09122)
*Chuan Guo,Inwoo Hwang,Jian Wang,Bing Zhou*

Main category: cs.CV

TL;DR: SnapMoGen是一个新的文本-动作数据集，包含高质量动作捕捉数据和详细文本标注，支持长期动作生成研究。MoMask++模型通过多尺度标记序列和生成掩码变换器，实现了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 当前文本到动作生成方法受限于短文本或通用提示，缺乏细粒度控制和泛化能力。SnapMoGen旨在解决这一问题。

Method: 提出SnapMoGen数据集，包含20K动作片段和122K详细文本描述；改进生成掩码建模方法，开发MoMask++模型，利用多尺度标记序列和单一生成掩码变换器。

Result: MoMask++在HumanML3D和SnapMoGen基准测试中达到最先进性能，并能通过LLM处理用户提示。

Conclusion: SnapMoGen和MoMask++为长期动作生成和细粒度控制提供了新工具，推动了文本到动作生成的研究。

Abstract: Text-to-motion generation has experienced remarkable progress in recent
years. However, current approaches remain limited to synthesizing motion from
short or general text prompts, primarily due to dataset constraints. This
limitation undermines fine-grained controllability and generalization to unseen
prompts. In this paper, we introduce SnapMoGen, a new text-motion dataset
featuring high-quality motion capture data paired with accurate, expressive
textual annotations. The dataset comprises 20K motion clips totaling 44 hours,
accompanied by 122K detailed textual descriptions averaging 48 words per
description (vs. 12 words of HumanML3D). Importantly, these motion clips
preserve original temporal continuity as they were in long sequences,
facilitating research in long-term motion generation and blending. We also
improve upon previous generative masked modeling approaches. Our model,
MoMask++, transforms motion into multi-scale token sequences that better
exploit the token capacity, and learns to generate all tokens using a single
generative masked transformer. MoMask++ achieves state-of-the-art performance
on both HumanML3D and SnapMoGen benchmarks. Additionally, we demonstrate the
ability to process casual user prompts by employing an LLM to reformat inputs
to align with the expressivity and narration style of SnapMoGen. Project
webpage: https://snap-research.github.io/SnapMoGen/

</details>


### [130] [PoseLLM: Enhancing Language-Guided Human Pose Estimation with MLP Alignment](https://arxiv.org/abs/2507.09139)
*Dewen Zhang,Tahir Hussain,Wangpeng An,Hayaru Shouno*

Main category: cs.CV

TL;DR: PoseLLM提出了一种基于大型语言模型（LLM）的姿态估计框架，通过非线性MLP视觉语言连接器替代线性投影器，提升了定位精度，同时保持了零样本泛化能力。


<details>
  <summary>Details</summary>
Motivation: 传统姿态估计方法依赖关键点先验，泛化能力有限；现有语言引导方法（如LocLLM）的线性投影器无法捕捉复杂的空间-文本交互。

Method: 提出PoseLLM，使用两层的非线性MLP（带GELU激活）作为视觉语言连接器，增强视觉块与文本关键点描述的融合。

Result: 在COCO验证集上达到77.8 AP，优于LocLLM（+0.4 AP），并在Human-Art和MPII上保持强零样本泛化能力。

Conclusion: 非线性连接器显著提升定位精度且不牺牲泛化能力，推动了语言引导姿态估计的先进水平。

Abstract: Human pose estimation traditionally relies on architectures that encode
keypoint priors, limiting their generalization to novel poses or unseen
keypoints. Recent language-guided approaches like LocLLM reformulate keypoint
localization as a vision-language task, enabling zero-shot generalization
through textual descriptions. However, LocLLM's linear projector fails to
capture complex spatial-textual interactions critical for high-precision
localization. To address this, we propose PoseLLM, the first Large Language
Model (LLM)-based pose estimation framework that replaces the linear projector
with a nonlinear MLP vision-language connector. This lightweight two-layer MLP
with GELU activation enables hierarchical cross-modal feature transformation,
enhancing the fusion of visual patches and textual keypoint descriptions.
Trained exclusively on COCO data, PoseLLM achieves 77.8 AP on the COCO
validation set, outperforming LocLLM by +0.4 AP, while maintaining strong
zero-shot generalization on Human-Art and MPII. Our work demonstrates that a
simple yet powerful nonlinear connector significantly boosts localization
accuracy without sacrificing generalization, advancing the state-of-the-art in
language-guided pose estimation. Code is available at
https://github.com/Ody-trek/PoseLLM.

</details>


### [131] [$I^{2}$-World: Intra-Inter Tokenization for Efficient Dynamic 4D Scene Forecasting](https://arxiv.org/abs/2507.09144)
*Zhimin Liao,Ping Wei,Ruijie Zhang,Shuaijia Chen,Haoxuan Wang,Ziyang Ren*

Main category: cs.CV

TL;DR: 提出了一种名为$I^{2}$-World的高效4D占用预测框架，通过解耦场景标记化并采用编码器-解码器架构，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 解决复杂3D场景标记化的挑战，以提升自动驾驶系统中未见过场景的生成能力。

Method: 采用双标记器设计（场景内和场景间），结合多尺度残差量化和时间依赖聚合，使用编码器-解码器架构实现高效生成。

Result: 在4D占用预测任务中，mIoU和IoU分别提升25.1%和36.9%，训练内存仅需2.9 GB，实时推理速度为37.0 FPS。

Conclusion: $I^{2}$-World在性能和效率上均优于现有方法，为3D场景预测提供了高效解决方案。

Abstract: Forecasting the evolution of 3D scenes and generating unseen scenarios via
occupancy-based world models offers substantial potential for addressing corner
cases in autonomous driving systems. While tokenization has revolutionized
image and video generation, efficiently tokenizing complex 3D scenes remains a
critical challenge for 3D world models. To address this, we propose
$I^{2}$-World, an efficient framework for 4D occupancy forecasting. Our method
decouples scene tokenization into intra-scene and inter-scene tokenizers. The
intra-scene tokenizer employs a multi-scale residual quantization strategy to
hierarchically compress 3D scenes while preserving spatial details. The
inter-scene tokenizer residually aggregates temporal dependencies across
timesteps. This dual design preserves the compactness of 3D tokenizers while
retaining the dynamic expressiveness of 4D tokenizers. Unlike decoder-only
GPT-style autoregressive models, $I^{2}$-World adopts an encoder-decoder
architecture. The encoder aggregates spatial context from the current scene and
predicts a transformation matrix to enable high-level control over scene
generation. The decoder, conditioned on this matrix and historical tokens,
ensures temporal consistency during generation. Experiments demonstrate that
$I^{2}$-World achieves state-of-the-art performance, outperforming existing
methods by 25.1\% in mIoU and 36.9\% in IoU for 4D occupancy forecasting while
exhibiting exceptional computational efficiency: it requires merely 2.9 GB of
training memory and achieves real-time inference at 37.0 FPS. Our code is
available on https://github.com/lzzzzzm/II-World.

</details>


### [132] [Stable Score Distillation](https://arxiv.org/abs/2507.09168)
*Haiming Zhu,Yangyang Xu,Chenshu Xu,Tingrui Shen,Wenxi Liu,Yong Du,Jun Yu,Shengfeng He*

Main category: cs.CV

TL;DR: 提出了一种名为Stable Score Distillation (SSD)的新方法，通过简化框架提升文本引导图像和3D编辑的稳定性和对齐性。


<details>
  <summary>Details</summary>
Motivation: 现有方法如Delta Denoising Score在稳定性、空间控制和编辑强度方面存在不足，主要依赖复杂的辅助结构导致优化信号冲突和局部编辑受限。

Method: SSD通过锚定单一分类器到源提示，利用Classifier-Free Guidance (CFG)方程实现跨提示对齐，并引入常数项null-text分支稳定优化过程。

Result: SSD在2D和3D编辑任务（如NeRF和文本驱动风格编辑）中取得最优结果，收敛更快且复杂度更低。

Conclusion: SSD为文本引导编辑提供了一种高效且鲁棒的解决方案，保持了原始内容结构并确保编辑轨迹与源提示紧密对齐。

Abstract: Text-guided image and 3D editing have advanced with diffusion-based models,
yet methods like Delta Denoising Score often struggle with stability, spatial
control, and editing strength. These limitations stem from reliance on complex
auxiliary structures, which introduce conflicting optimization signals and
restrict precise, localized edits. We introduce Stable Score Distillation
(SSD), a streamlined framework that enhances stability and alignment in the
editing process by anchoring a single classifier to the source prompt.
Specifically, SSD utilizes Classifier-Free Guidance (CFG) equation to achieves
cross-prompt alignment, and introduces a constant term null-text branch to
stabilize the optimization process. This approach preserves the original
content's structure and ensures that editing trajectories are closely aligned
with the source prompt, enabling smooth, prompt-specific modifications while
maintaining coherence in surrounding regions. Additionally, SSD incorporates a
prompt enhancement branch to boost editing strength, particularly for style
transformations. Our method achieves state-of-the-art results in 2D and 3D
editing tasks, including NeRF and text-driven style edits, with faster
convergence and reduced complexity, providing a robust and efficient solution
for text-guided editing.

</details>


### [133] [Learning and Transferring Better with Depth Information in Visual Reinforcement Learning](https://arxiv.org/abs/2507.09180)
*Zichun Xu,Yuntao Li,Zhaomin Wang,Lei Zhuang,Guocai Yang,Jingdong Zhao*

Main category: cs.CV

TL;DR: 提出了一种基于视觉Transformer的RGB与深度模态融合方法，通过对比无监督学习和课程学习提升泛化能力。


<details>
  <summary>Details</summary>
Motivation: 深度信息对场景外观变化具有鲁棒性且携带3D空间细节，但需要有效融合RGB与深度模态以增强泛化能力。

Method: 分别用CNN处理不同模态，将卷积特征输入可扩展视觉Transformer；设计对比无监督学习方案，结合掩码与非掩码令牌；开发灵活课程学习计划进行域随机化。

Result: 方法通过模态融合和对比学习提升了样本效率与泛化性能。

Conclusion: 提出的视觉Transformer框架有效融合RGB与深度信息，结合无监督学习和课程学习，显著提升了模型的泛化能力和实际部署效果。

Abstract: Depth information is robust to scene appearance variations and inherently
carries 3D spatial details. In this paper, a visual backbone based on the
vision transformer is proposed to fuse RGB and depth modalities for enhancing
generalization. Different modalities are first processed by separate CNN stems,
and the combined convolutional features are delivered to the scalable vision
transformer to obtain visual representations. Moreover, a contrastive
unsupervised learning scheme is designed with masked and unmasked tokens to
accelerate the sample efficiency during the reinforcement learning progress.
For sim2real transfer, a flexible curriculum learning schedule is developed to
deploy domain randomization over training processes.

</details>


### [134] [Revisiting Pool-based Prompt Learning for Few-shot Class-incremental Learning](https://arxiv.org/abs/2507.09183)
*Yongwei Jiang,Yixiong Zou,Yuhua Li,Ruixuan Li*

Main category: cs.CV

TL;DR: 本文研究了Few-Shot Class-Incremental Learning (FSCIL)中的提示池方法，发现其性能下降问题源于token维度饱和，并提出LGSP-Prompt方法，通过空间维度优化提示学习，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: FSCIL在数据稀缺和增量学习的双重挑战下，现有提示池方法的性能表现未得到充分研究，本文旨在填补这一空白并解决性能下降问题。

Method: 提出LGSP-Prompt方法，将提示学习从token维度转移到空间维度，结合局部空间特征和全局频域表示生成空间提示，并构建动态提示池。

Result: 实验表明，LGSP-Prompt在多个FSCIL基准测试中达到最优性能，显著提升了基础知识保留和增量学习能力。

Conclusion: LGSP-Prompt通过空间维度优化提示学习，有效解决了FSCIL中的性能下降问题，为未来研究提供了新思路。

Abstract: Few-Shot Class-Incremental Learning (FSCIL) faces dual challenges of data
scarcity and incremental learning in real-world scenarios. While pool-based
prompting methods have demonstrated success in traditional incremental
learning, their effectiveness in FSCIL settings remains unexplored. This paper
presents the first study of current prompt pool methods in FSCIL tasks,
revealing an unanticipated performance degradation in incremental sessions.
Through comprehensive analysis, we identify that this phenomenon stems from
token-dimension saturation: with limited data, excessive prompts compete for
task-relevant information, leading to model overfitting. Based on this finding,
we propose LGSP-Prompt (Local-Global Spatial Prompting), which innovatively
shifts pool-based prompt learning from the token dimension to the spatial
dimension. LGSP-Prompt generates spatial prompts by synergistically combining
local spatial features and global frequency-domain representations to highlight
key patterns in input images. We construct two spatial prompt pools enabling
dynamic prompt selection to maintain acquired knowledge while effectively
learning novel sessions. Extensive experiments demonstrate that our approach
achieves state-of-the-art performance across multiple FSCIL benchmarks, showing
significant advantages in both base knowledge preservation and incremental
learning. Our implementation is available at
https://github.com/Jywsuperman/LGSP.

</details>


### [135] [MCA-LLaVA: Manhattan Causal Attention for Reducing Hallucination in Large Vision-Language Models](https://arxiv.org/abs/2507.09184)
*Qiyan Zhao,Xiaofeng Zhang,Yiheng Li,Yun Xing,Xiaosong Yuan,Feilong Tang,Sinan Fan,Xuhang Chen,Xuyao Zhang,Dahan Wang*

Main category: cs.CV

TL;DR: 论文揭示了RoPE在LVLMs中的长期衰减对多模态对齐的负面影响，并提出基于曼哈顿距离的MCA-LLaVA方法以缓解图像对齐偏差。


<details>
  <summary>Details</summary>
Motivation: 解决大型视觉语言模型（LVLMs）中的幻觉问题，特别是由多模态特征不对齐引起的图像对齐偏差。

Method: 提出MCA-LLaVA方法，通过曼哈顿距离扩展RoPE的长期衰减为二维空间衰减，整合序列顺序和空间位置。

Result: 实验证明MCA-LLaVA在多种幻觉和通用基准测试中有效且具有普适性。

Conclusion: MCA-LLaVA通过缓解图像对齐偏差，显著减少了LVLMs中的幻觉问题。

Abstract: Hallucinations pose a significant challenge in Large Vision Language Models
(LVLMs), with misalignment between multimodal features identified as a key
contributing factor. This paper reveals the negative impact of the long-term
decay in Rotary Position Encoding (RoPE), used for positional modeling in
LVLMs, on multimodal alignment. Concretely, under long-term decay, instruction
tokens exhibit uneven perception of image tokens located at different positions
within the two-dimensional space: prioritizing image tokens from the
bottom-right region since in the one-dimensional sequence, these tokens are
positionally closer to the instruction tokens. This biased perception leads to
insufficient image-instruction interaction and suboptimal multimodal alignment.
We refer to this phenomenon as image alignment bias. To enhance instruction's
perception of image tokens at different spatial locations, we propose
MCA-LLaVA, based on Manhattan distance, which extends the long-term decay to a
two-dimensional, multi-directional spatial decay. MCA-LLaVA integrates the
one-dimensional sequence order and two-dimensional spatial position of image
tokens for positional modeling, mitigating hallucinations by alleviating image
alignment bias. Experimental results of MCA-LLaVA across various hallucination
and general benchmarks demonstrate its effectiveness and generality. The code
can be accessed in https://github.com/ErikZ719/MCA-LLaVA.

</details>


### [136] [THYME: Temporal Hierarchical-Cyclic Interactivity Modeling for Video Scene Graphs in Aerial Footage](https://arxiv.org/abs/2507.09200)
*Trong-Thuan Nguyen,Pha Nguyen,Jackson Cothren,Alper Yilmaz,Minh-Triet Tran,Khoa Luu*

Main category: cs.CV

TL;DR: 论文提出了一种名为THYME的时序分层循环场景图方法，用于动态场景理解，解决了现有方法在细粒度空间细节和长时序依赖上的不足，并在新数据集AeroEye-v1.0上验证了其优越性。


<details>
  <summary>Details</summary>
Motivation: 动态场景理解在自动驾驶、监控等领域需求迫切，但现有方法难以同时捕捉细粒度空间细节和长时序依赖。

Method: 提出THYME方法，结合分层特征聚合和循环时序优化，建模多尺度空间上下文并保持时序一致性。

Result: 在ASPIRe和AeroEye-v1.0数据集上，THYME优于现有方法，提升了场景理解的准确性。

Conclusion: THYME方法有效解决了动态场景图中的关键问题，为实际应用提供了更优的解决方案。

Abstract: The rapid proliferation of video in applications such as autonomous driving,
surveillance, and sports analytics necessitates robust methods for dynamic
scene understanding. Despite advances in static scene graph generation and
early attempts at video scene graph generation, previous methods often suffer
from fragmented representations, failing to capture fine-grained spatial
details and long-range temporal dependencies simultaneously. To address these
limitations, we introduce the Temporal Hierarchical Cyclic Scene Graph (THYME)
approach, which synergistically integrates hierarchical feature aggregation
with cyclic temporal refinement to address these limitations. In particular,
THYME effectively models multi-scale spatial context and enforces temporal
consistency across frames, yielding more accurate and coherent scene graphs. In
addition, we present AeroEye-v1.0, a novel aerial video dataset enriched with
five types of interactivity that overcome the constraints of existing datasets
and provide a comprehensive benchmark for dynamic scene graph generation.
Empirically, extensive experiments on ASPIRe and AeroEye-v1.0 demonstrate that
the proposed THYME approach outperforms state-of-the-art methods, offering
improved scene understanding in ground-view and aerial scenarios.

</details>


### [137] [Visual Surface Wave Elastography: Revealing Subsurface Physical Properties via Visible Surface Waves](https://arxiv.org/abs/2507.09207)
*Alexander C. Ogren,Berthy T. Feng,Jihoon Ahn,Katherine L. Bouman,Chiara Daraio*

Main category: cs.CV

TL;DR: 通过视频分析表面波的传播特性，推断材料厚度和刚度的方法。


<details>
  <summary>Details</summary>
Motivation: 表面波的传播特性可以反映材料内部的物理性质，为无创检测提供可能。

Method: 从视频中提取波的色散关系，通过物理优化问题求解最佳厚度和刚度参数。

Result: 在模拟和真实数据中验证，结果与真实测量高度一致。

Conclusion: 该方法为家庭健康监测和人机交互等领域提供了概念验证。

Abstract: Wave propagation on the surface of a material contains information about
physical properties beneath its surface. We propose a method for inferring the
thickness and stiffness of a structure from just a video of waves on its
surface. Our method works by extracting a dispersion relation from the video
and then solving a physics-based optimization problem to find the best-fitting
thickness and stiffness parameters. We validate our method on both simulated
and real data, in both cases showing strong agreement with ground-truth
measurements. Our technique provides a proof-of-concept for at-home health
monitoring of medically-informative tissue properties, and it is further
applicable to fields such as human-computer interaction.

</details>


### [138] [Uncertainty-Driven Expert Control: Enhancing the Reliability of Medical Vision-Language Models](https://arxiv.org/abs/2507.09209)
*Xiao Liang,Di Wang,Zhicheng Jiao,Ronghan Li,Pengfei Yang,Quan Wang,Tat-Seng Chua*

Main category: cs.CV

TL;DR: 提出了一种名为Expert-CFG的专家参与框架，用于在不额外训练的情况下提升医学视觉语言模型的临床对齐能力。


<details>
  <summary>Details</summary>
Motivation: 当前医学视觉语言模型存在概率不确定性，可能导致错误回答，而现有方法成本高且临床对齐不足。

Method: 通过不确定性估计识别不可靠输出，检索相关参考资料辅助专家标注关键术语，并利用无分类器引导调整模型输出。

Result: 在三个医学视觉问答基准测试中，Expert-CFG以4.2B参数优于13B参数的现有最佳模型。

Conclusion: Expert-CFG展示了在资源有限环境中部署临床辅助系统的可行性。

Abstract: The rapid advancements in Vision Language Models (VLMs) have prompted the
development of multi-modal medical assistant systems. Despite this progress,
current models still have inherent probabilistic uncertainties, often producing
erroneous or unverified responses-an issue with serious implications in medical
applications. Existing methods aim to enhance the performance of Medical Vision
Language Model (MedVLM) by adjusting model structure, fine-tuning with
high-quality data, or through preference fine-tuning. However, these
training-dependent strategies are costly and still lack sufficient alignment
with clinical expertise. To address these issues, we propose an
expert-in-the-loop framework named Expert-Controlled Classifier-Free Guidance
(Expert-CFG) to align MedVLM with clinical expertise without additional
training. This framework introduces an uncertainty estimation strategy to
identify unreliable outputs. It then retrieves relevant references to assist
experts in highlighting key terms and applies classifier-free guidance to
refine the token embeddings of MedVLM, ensuring that the adjusted outputs are
correct and align with expert highlights. Evaluations across three medical
visual question answering benchmarks demonstrate that the proposed Expert-CFG,
with 4.2B parameters and limited expert annotations, outperforms
state-of-the-art models with 13B parameters. The results demonstrate the
feasibility of deploying such a system in resource-limited settings for
clinical use.

</details>


### [139] [Stereo-based 3D Anomaly Object Detection for Autonomous Driving: A New Dataset and Baseline](https://arxiv.org/abs/2507.09214)
*Shiyi Mu,Zichong Gu,Hanqi Lyu,Yilin Gao,Shugong Xu*

Main category: cs.CV

TL;DR: 论文提出了一种基于立体视觉的3D异常物体检测算法（S3AD），通过解耦2D和3D训练策略提升模型对任意形状目标的泛化能力，并设计了基于前景置信度的异常评分算法。同时，构建了KITTI-AR数据集以验证和增强异常检测的泛化性能。


<details>
  <summary>Details</summary>
Motivation: 解决3D检测模型在开放道路场景中对罕见异常物体的误检或漏检问题，提升模型对任意形状目标的泛化能力。

Method: 提出S3AD算法，解耦2D和3D训练策略，设计基于前景置信度的异常评分算法；构建KITTI-AR数据集，包含新增的97个类别以验证模型性能。

Result: 实验验证了算法和数据集的性能，S3AD在异常检测任务中表现出色。

Conclusion: S3AD算法和KITTI-AR数据集有效提升了3D异常检测的泛化能力，为开放道路场景的3D检测提供了新思路。

Abstract: 3D detection technology is widely used in the field of autonomous driving,
with its application scenarios gradually expanding from enclosed highways to
open conventional roads. For rare anomaly categories that appear on the road,
3D detection models trained on closed sets often misdetect or fail to detect
anomaly objects. To address this risk, it is necessary to enhance the
generalization ability of 3D detection models for targets of arbitrary shapes
and to possess the capability to filter out anomalies. The generalization of 3D
detection is limited by two factors: the coupled training of 2D and 3D, and the
insufficient diversity in the scale distribution of training samples. This
paper proposes a Stereo-based 3D Anomaly object Detection (S3AD) algorithm,
which decouples the training strategy of 3D and 2D to release the
generalization ability for arbitrary 3D foreground detection, and proposes an
anomaly scoring algorithm based on foreground confidence prediction, achieving
target-level anomaly scoring. In order to further verify and enhance the
generalization of anomaly detection, we use a 3D rendering method to synthesize
two augmented reality binocular stereo 3D detection datasets which named
KITTI-AR. KITTI-AR extends upon KITTI by adding 97 new categories, totaling 6k
pairs of stereo images. The KITTI-AR-ExD subset includes 39 common categories
as extra training data to address the sparse sample distribution issue.
Additionally, 58 rare categories form the KITTI-AR-OoD subset, which are not
used in training to simulate zero-shot scenarios in real-world settings, solely
for evaluating 3D anomaly detection. Finally, the performance of the algorithm
and the dataset is verified in the experiments. (Code and dataset can be
obtained at https://github.com/xxxx/xxx).

</details>


### [140] [360-Degree Full-view Image Segmentation by Spherical Convolution compatible with Large-scale Planar Pre-trained Models](https://arxiv.org/abs/2507.09216)
*Jingguo Liu,Han Yu,Shigang Li,Jianfeng Li*

Main category: cs.CV

TL;DR: 提出一种新的球形采样方法，使预训练的二维图像模型可直接用于全景图像任务，减少失真并提升性能。


<details>
  <summary>Details</summary>
Motivation: 由于缺乏百万级全景图像数据集，现有任务依赖二维预训练模型，但这些模型无法处理全景图像的失真和不连续性，影响性能。

Method: 采用基于预训练模型权重的球形离散采样方法，减少失真并获得良好初始训练值；将该方法应用于全景图像分割，利用球形模型特征作为特定通道注意力的掩码。

Result: 在常用室内数据集Stanford2D3D上取得了良好效果。

Conclusion: 提出的球形采样方法有效解决了全景图像任务中预训练模型的失真问题，提升了性能。

Abstract: Due to the current lack of large-scale datasets at the million-scale level,
tasks involving panoramic images predominantly rely on existing two-dimensional
pre-trained image benchmark models as backbone networks. However, these
networks are not equipped to recognize the distortions and discontinuities
inherent in panoramic images, which adversely affects their performance in such
tasks. In this paper, we introduce a novel spherical sampling method for
panoramic images that enables the direct utilization of existing pre-trained
models developed for two-dimensional images. Our method employs spherical
discrete sampling based on the weights of the pre-trained models, effectively
mitigating distortions while achieving favorable initial training values.
Additionally, we apply the proposed sampling method to panoramic image
segmentation, utilizing features obtained from the spherical model as masks for
specific channel attentions, which yields commendable results on commonly used
indoor datasets, Stanford2D3D.

</details>


### [141] [Online Long-term Point Tracking in the Foundation Model Era](https://arxiv.org/abs/2507.09217)
*Görkay Aydemir*

Main category: cs.CV

TL;DR: 论文提出了一种在线长期点跟踪方法Track-On，基于Transformer模型，无需未来帧信息，在七个公共基准测试中达到新水平。


<details>
  <summary>Details</summary>
Motivation: 现实场景需要在线预测，但现有方法多为离线处理，无法满足实时需求。视觉基础模型虽能提供空间特征，但缺乏时间推理能力。

Method: 评估视觉基础模型的适用性，并提出Track-On模型，将每个跟踪点作为查询，逐帧处理视频。

Result: Track-On在七个公共基准测试中表现最佳，证明了无需未来帧的长期跟踪可行性。

Conclusion: Track-On为在线长期点跟踪提供了有效解决方案，结合视觉基础模型和Transformer架构，实现了高性能。

Abstract: Point tracking aims to identify the same physical point across video frames
and serves as a geometry-aware representation of motion. This representation
supports a wide range of applications, from robotics to augmented reality, by
enabling accurate modeling of dynamic environments. Most existing long-term
tracking approaches operate in an offline setting, where future frames are
available to refine predictions and recover from occlusions. However,
real-world scenarios often demand online predictions: the model must operate
causally, using only current and past frames. This constraint is critical in
streaming video and embodied AI, where decisions must be made immediately based
on past observations. Under such constraints, viewpoint invariance becomes
essential. Visual foundation models, trained on diverse large-scale datasets,
offer the potential for robust geometric representations. While they lack
temporal reasoning on their own, they can be integrated into tracking pipelines
to enrich spatial features. In this thesis, we address the problem of long-term
point tracking in an online setting, where frames are processed sequentially
without access to future information or sliding windows. We begin by evaluating
the suitability of visual foundation models for this task and find that they
can serve as useful initializations and be integrated into tracking pipelines.
However, to enable long-term tracking in an online setting, a dedicated design
is still required. In particular, maintaining coherence over time in this
causal regime requires memory to propagate appearance and context across
frames. To address this, we introduce Track-On, a transformer-based model that
treats each tracked point as a query and processes video frames one at a time.
Track-On sets a new state of the art across seven public benchmarks,
demonstrating the feasibility of long-term tracking without future access.

</details>


### [142] [Calibrated and Robust Foundation Models for Vision-Language and Medical Image Tasks Under Distribution Shift](https://arxiv.org/abs/2507.09222)
*Behraj Khan,Tahir Syed*

Main category: cs.CV

TL;DR: StaRFM是一个统一框架，通过Fisher信息惩罚（FIP）和置信度对齐惩罚（CMP）解决基础模型在视觉语言分类和医学分割任务中的分布偏移和置信度不匹配问题。


<details>
  <summary>Details</summary>
Motivation: 基础模型（如CLIP和SAM）在低样本迁移学习中表现出色，但部署时面临分布偏移和置信度不匹配的挑战，现有解决方案多为领域特定。

Method: 提出StaRFM框架，结合FIP（扩展至3D医学数据）和CMP（针对体素级预测），并通过理论分析证明其有效性。

Result: 在19个视觉数据集和医学分割任务中，StaRFM表现优于现有方法，如准确率提升3.5%，ECE降低28%，DSC达84.7%。

Conclusion: StaRFM是一个即插即用的框架，可无缝集成到基础模型中，显著提升跨领域性能。

Abstract: Foundation models like CLIP and SAM have transformed computer vision and
medical imaging via low-shot transfer learning. However, deployment of these
models hindered by two key challenges: \textit{distribution shift} between
training and test data, and \textit{confidence misalignment} that leads to
overconfident incorrect predictions. These issues manifest differently in
vision-language classification and medical segmentation tasks, yet existing
solutions remain domain-specific. We propose \textit{StaRFM}, a unified
framework addressing both challenges. It introduces a Fisher information
penalty (FIP), extended to 3D medical data via patch-wise regularization, to
reduce covariate shift in CLIP and SAM embeddings. Additionally, a confidence
misalignment penalty (CMP), reformulated for voxel-level predictions,
calibrates uncertainty in segmentation tasks. We theoretically derive PAC-Bayes
bounds showing FIP controls generalization via the Fisher-Rao norm, while CMP
minimizes calibration error through Brier score optimization. StaRFM shows
consistent performance like \texttt{+}3.5\% accuracy and 28\% lower ECE on 19
vision datasets (e.g., ImageNet, Office-Home), 84.7\% DSC and 4.8mm HD95 in
medical segmentation (e.g., BraTS, ATLAS), and 40\% lower cross-domain
performance gap compared to prior benchmarking methods. The framework is
plug-and-play, requiring minimal architectural changes for seamless integration
with foundation models. Code and models will be released at
https://anonymous.4open.science/r/StaRFM-C0CD/README.md

</details>


### [143] [EgoAnimate: Generating Human Animations from Egocentric top-down Views](https://arxiv.org/abs/2507.09230)
*G. Kutay Türkoglu,Julian Tanke,Iheb Belgacem,Lev Markhasin*

Main category: cs.CV

TL;DR: 该论文提出了一种基于生成先验的方法，从单视角的自我中心图像重建可动画的虚拟形象，利用Stable Diffusion作为主干网络，减少训练负担并提高泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有方法在自我中心视角下重建人体外观时存在遮挡和比例失真问题，且多依赖多视角数据集训练。本文旨在通过生成式方法解决这些问题，提升虚拟现实中的数字临场感。

Method: 基于Stable Diffusion和ControlNet，设计了一个从遮挡的俯视图像生成真实正面视图的流程，并将其输入到图像到动作模型中，生成虚拟形象动作。

Result: 方法成功从单张自我中心图像生成了可动画的虚拟形象，减少了训练数据需求并提高了泛化能力。

Conclusion: 该方法为更易用和通用的数字临场感系统提供了新思路，展示了生成式模型在虚拟形象重建中的潜力。

Abstract: An ideal digital telepresence experience requires accurate replication of a
person's body, clothing, and movements. To capture and transfer these movements
into virtual reality, the egocentric (first-person) perspective can be adopted,
which enables the use of a portable and cost-effective device without
front-view cameras. However, this viewpoint introduces challenges such as
occlusions and distorted body proportions.
  There are few works reconstructing human appearance from egocentric views,
and none use a generative prior-based approach. Some methods create avatars
from a single egocentric image during inference, but still rely on multi-view
datasets during training. To our knowledge, this is the first study using a
generative backbone to reconstruct animatable avatars from egocentric inputs.
Based on Stable Diffusion, our method reduces training burden and improves
generalizability.
  Inspired by methods such as SiTH and MagicMan, which perform 360-degree
reconstruction from a frontal image, we introduce a pipeline that generates
realistic frontal views from occluded top-down images using ControlNet and a
Stable Diffusion backbone.
  Our goal is to convert a single top-down egocentric image into a realistic
frontal representation and feed it into an image-to-motion model. This enables
generation of avatar motions from minimal input, paving the way for more
accessible and generalizable telepresence systems.

</details>


### [144] [PPJudge: Towards Human-Aligned Assessment of Artistic Painting Process](https://arxiv.org/abs/2507.09242)
*Shiqi Jiang,Xinpeng Li,Xi Mao,Changbo Wang,Chenhui Li*

Main category: cs.CV

TL;DR: 提出了一种基于Transformer的框架PPJudge，用于评估绘画过程，并创建了首个大规模绘画过程评估数据集PPAD。


<details>
  <summary>Details</summary>
Motivation: 现有方法仅关注静态图像，忽略了绘画过程的动态性和多阶段性。

Method: 引入PPAD数据集和PPJudge模型，采用时间感知位置编码和混合专家架构。

Result: 实验显示，该方法在准确性、鲁棒性和与人类判断一致性上优于基线。

Conclusion: 为计算创造力和艺术教育提供了新视角。

Abstract: Artistic image assessment has become a prominent research area in computer
vision. In recent years, the field has witnessed a proliferation of datasets
and methods designed to evaluate the aesthetic quality of paintings. However,
most existing approaches focus solely on static final images, overlooking the
dynamic and multi-stage nature of the artistic painting process. To address
this gap, we propose a novel framework for human-aligned assessment of painting
processes. Specifically, we introduce the Painting Process Assessment Dataset
(PPAD), the first large-scale dataset comprising real and synthetic painting
process images, annotated by domain experts across eight detailed attributes.
Furthermore, we present PPJudge (Painting Process Judge), a Transformer-based
model enhanced with temporally-aware positional encoding and a heterogeneous
mixture-of-experts architecture, enabling effective assessment of the painting
process. Experimental results demonstrate that our method outperforms existing
baselines in accuracy, robustness, and alignment with human judgment, offering
new insights into computational creativity and art education.

</details>


### [145] [AGCD-Net: Attention Guided Context Debiasing Network for Emotion Recognition](https://arxiv.org/abs/2507.09248)
*Varsha Devi,Amine Bohi,Pardeep Kumar*

Main category: cs.CV

TL;DR: AGCD-Net通过注意力引导和因果干预减少上下文偏差，提升情感识别的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 传统方法在情感识别中存在上下文偏差问题，如背景与情感标签的虚假关联。

Method: 提出AGCD-Net，结合Hybrid ConvNeXt编码器和AG-CIM模块，通过因果干预和注意力校正消除偏差。

Result: 在CAER-S数据集上达到最优性能，验证了方法的有效性。

Conclusion: 因果去偏对复杂场景下的情感识别至关重要。

Abstract: Context-aware emotion recognition (CAER) enhances affective computing in
real-world scenarios, but traditional methods often suffer from context
bias-spurious correlation between background context and emotion labels (e.g.
associating ``garden'' with ``happy''). In this paper, we propose
\textbf{AGCD-Net}, an Attention Guided Context Debiasing model that introduces
\textit{Hybrid ConvNeXt}, a novel convolutional encoder that extends the
ConvNeXt backbone by integrating Spatial Transformer Network and
Squeeze-and-Excitation layers for enhanced feature recalibration. At the core
of AGCD-Net is the Attention Guided - Causal Intervention Module (AG-CIM),
which applies causal theory, perturbs context features, isolates spurious
correlations, and performs an attention-driven correction guided by face
features to mitigate context bias. Experimental results on the CAER-S dataset
demonstrate the effectiveness of AGCD-Net, achieving state-of-the-art
performance and highlighting the importance of causal debiasing for robust
emotion recognition in complex settings.

</details>


### [146] [SAGE: Segment-Aware Gloss-Free Encoding for Token-Efficient Sign Language Translation](https://arxiv.org/abs/2507.09266)
*JianHe Low,Ozge Mercanoglu Sincan,Richard Bowden*

Main category: cs.CV

TL;DR: 提出了一种基于视觉标记化的无注释手语翻译方法，显著降低计算需求并提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有无注释手语翻译方法虽性能强，但模型复杂且计算需求高，难以扩展。

Method: 采用分段感知视觉标记化框架，结合对比对齐目标和双重监督策略。

Result: 在PHOENIX14T基准测试中性能超越现有方法，序列长度减少50%，内存使用降低2.67倍。

Conclusion: 该方法在减少计算需求的同时提升了性能，验证了标记化与对齐策略的潜力。

Abstract: Gloss-free Sign Language Translation (SLT) has advanced rapidly, achieving
strong performances without relying on gloss annotations. However, these gains
have often come with increased model complexity and high computational demands,
raising concerns about scalability, especially as large-scale sign language
datasets become more common. We propose a segment-aware visual tokenization
framework that leverages sign segmentation to convert continuous video into
discrete, sign-informed visual tokens. This reduces input sequence length by up
to 50% compared to prior methods, resulting in up to 2.67x lower memory usage
and better scalability on larger datasets. To bridge the visual and linguistic
modalities, we introduce a token-to-token contrastive alignment objective,
along with a dual-level supervision that aligns both language embeddings and
intermediate hidden states. This improves fine-grained cross-modal alignment
without relying on gloss-level supervision. Our approach notably exceeds the
performance of state-of-the-art methods on the PHOENIX14T benchmark, while
significantly reducing sequence length. Further experiments also demonstrate
our improved performance over prior work under comparable sequence-lengths,
validating the potential of our tokenization and alignment strategies.

</details>


### [147] [Cross Knowledge Distillation between Artificial and Spiking Neural Networks](https://arxiv.org/abs/2507.09269)
*Shuhan Ye,Yuanbin Qian,Chong Wang,Sunqi Lin,Jiazhen Xu,Jiangbo Qian,Yuqi Li*

Main category: cs.CV

TL;DR: 论文提出了一种跨知识蒸馏（CKD）方法，通过语义相似性和滑动替换解决跨模态问题，并通过间接分阶段知识蒸馏解决跨架构问题，提升了SNN在DVS数据上的性能。


<details>
  <summary>Details</summary>
Motivation: 由于SNN在事件驱动和节能方面的优势，但其性能受限于标注数据不足和架构不成熟，因此研究如何利用RGB数据和ANN进行知识蒸馏以提升SNN性能。

Method: 提出CKD方法，结合语义相似性和滑动替换解决跨模态问题，采用间接分阶段知识蒸馏解决跨架构问题。

Result: 在N-Caltech101和CEP-DVS等主流神经形态数据集上验证，性能优于当前最优方法。

Conclusion: CKD方法有效提升了SNN性能，为跨模态和跨架构知识蒸馏提供了新思路。

Abstract: Recently, Spiking Neural Networks (SNNs) have demonstrated rich potential in
computer vision domain due to their high biological plausibility, event-driven
characteristic and energy-saving efficiency. Still, limited annotated
event-based datasets and immature SNN architectures result in their performance
inferior to that of Artificial Neural Networks (ANNs). To enhance the
performance of SNNs on their optimal data format, DVS data, we explore using
RGB data and well-performing ANNs to implement knowledge distillation. In this
case, solving cross-modality and cross-architecture challenges is necessary. In
this paper, we propose cross knowledge distillation (CKD), which not only
leverages semantic similarity and sliding replacement to mitigate the
cross-modality challenge, but also uses an indirect phased knowledge
distillation to mitigate the cross-architecture challenge. We validated our
method on main-stream neuromorphic datasets, including N-Caltech101 and
CEP-DVS. The experimental results show that our method outperforms current
State-of-the-Art methods. The code will be available at
https://github.com/ShawnYE618/CKD

</details>


### [148] [Prompt4Trust: A Reinforcement Learning Prompt Augmentation Framework for Clinically-Aligned Confidence Calibration in Multimodal Large Language Models](https://arxiv.org/abs/2507.09279)
*Anita Kriz,Elizabeth Laura Janes,Xing Shen,Tal Arbel*

Main category: cs.CV

TL;DR: Prompt4Trust是一种针对多模态大语言模型（MLLMs）的强化学习框架，旨在通过上下文感知的辅助提示改善模型在医疗领域中的置信度校准，提升其可信度和准确性。


<details>
  <summary>Details</summary>
Motivation: MLLMs在医疗等安全关键领域的应用受到提示设计敏感性和高置信度错误响应的限制，需要改进其置信度校准以提高可靠性。

Method: 提出Prompt4Trust框架，通过训练轻量级LLM生成上下文感知的辅助提示，指导下游任务MLLM生成置信度更准确的响应。

Result: 在PMC-VQA基准测试中取得最先进的医学视觉问答性能，并展示了零样本泛化能力。

Conclusion: Prompt4Trust展示了自动化提示工程在提升MLLMs可信度方面的潜力，尤其在安全关键领域。

Abstract: Multimodal large language models (MLLMs) hold considerable promise for
applications in healthcare. However, their deployment in safety-critical
settings is hindered by two key limitations: (i) sensitivity to prompt design,
and (ii) a tendency to generate incorrect responses with high confidence. As
clinicians may rely on a model's stated confidence to gauge the reliability of
its predictions, it is especially important that when a model expresses high
confidence, it is also highly accurate. We introduce Prompt4Trust, the first
reinforcement learning (RL) framework for prompt augmentation targeting
confidence calibration in MLLMs. A lightweight LLM is trained to produce
context-aware auxiliary prompts that guide a downstream task MLLM to generate
responses in which the expressed confidence more accurately reflects predictive
accuracy. Unlike conventional calibration techniques, Prompt4Trust specifically
prioritizes aspects of calibration most critical for safe and trustworthy
clinical decision-making. Beyond improvements driven by this clinically
motivated calibration objective, our proposed method also improves task
accuracy, achieving state-of-the-art medical visual question answering (VQA)
performance on the PMC-VQA benchmark, which is composed of multiple-choice
questions spanning diverse medical imaging modalities. Moreover, our framework
trained with a small downstream task MLLM showed promising zero-shot
generalization to larger MLLMs in our experiments, suggesting the potential for
scalable calibration without the associated computational costs. This work
demonstrates the potential of automated yet human-aligned prompt engineering
for improving the the trustworthiness of MLLMs in safety critical settings. Our
codebase can be found at https://github.com/xingbpshen/vccrl-llm.

</details>


### [149] [Generative Latent Kernel Modeling for Blind Motion Deblurring](https://arxiv.org/abs/2507.09285)
*Chenhao Ding,Jiangtao Zhang,Zongsheng Yue,Hui Wang,Qian Zhao,Deyu Meng*

Main category: cs.CV

TL;DR: 提出了一种基于深度生成模型的新框架，用于盲运动去模糊（BMD），通过预训练的GAN生成器和初始化器优化模糊核的初始估计，显著提高了性能。


<details>
  <summary>Details</summary>
Motivation: 现有深度先验方法在盲运动去模糊中表现优异，但对模糊核初始化的高度敏感性限制了其性能。

Method: 预训练一个基于GAN的模糊核生成器和一个初始化器，以提供高质量的初始估计，并将其与现有BMD方法结合。

Result: 在挑战性基准数据集上实现了最先进的性能，并扩展到盲非均匀运动去模糊。

Conclusion: 提出的框架通过优化模糊核初始化，显著提升了BMD的性能，且易于与现有方法集成。

Abstract: Deep prior-based approaches have demonstrated remarkable success in blind
motion deblurring (BMD) recently. These methods, however, are often limited by
the high non-convexity of the underlying optimization process in BMD, which
leads to extreme sensitivity to the initial blur kernel. To address this issue,
we propose a novel framework for BMD that leverages a deep generative model to
encode the kernel prior and induce a better initialization for the blur kernel.
Specifically, we pre-train a kernel generator based on a generative adversarial
network (GAN) to aptly characterize the kernel's prior distribution, as well as
a kernel initializer to provide a well-informed and high-quality starting point
for kernel estimation. By combining these two components, we constrain the BMD
solution within a compact latent kernel manifold, thus alleviating the
aforementioned sensitivity for kernel initialization. Notably, the kernel
generator and initializer are designed to be easily integrated with existing
BMD methods in a plug-and-play manner, enhancing their overall performance.
Furthermore, we extend our approach to tackle blind non-uniform motion
deblurring without the need for additional priors, achieving state-of-the-art
performance on challenging benchmark datasets. The source code is available at
https://github.com/dch0319/GLKM-Deblur.

</details>


### [150] [Supercharging Floorplan Localization with Semantic Rays](https://arxiv.org/abs/2507.09291)
*Yuval Grader,Hadar Averbuch-Elor*

Main category: cs.CV

TL;DR: 提出了一种语义感知的定位框架，通过联合估计深度和语义光线，显著提升了楼层平面图定位的准确性和效率。


<details>
  <summary>Details</summary>
Motivation: 当前楼层平面图定位技术主要依赖深度结构线索，忽略了平面图中的丰富语义信息。

Method: 采用从粗到细的方式构建结构-语义概率体积，先通过稀疏采样生成低分辨率概率体积，再在高概率区域进行密集采样和优化。

Result: 在两个标准基准测试中显著优于现有方法，召回率显著提升，并能轻松整合额外元数据（如房间标签）以进一步提高性能。

Conclusion: 该框架通过结合语义信息，为楼层平面图定位提供了更高效和准确的解决方案。

Abstract: Floorplans provide a compact representation of the building's structure,
revealing not only layout information but also detailed semantics such as the
locations of windows and doors. However, contemporary floorplan localization
techniques mostly focus on matching depth-based structural cues, ignoring the
rich semantics communicated within floorplans. In this work, we introduce a
semantic-aware localization framework that jointly estimates depth and semantic
rays, consolidating over both for predicting a structural-semantic probability
volume. Our probability volume is constructed in a coarse-to-fine manner: We
first sample a small set of rays to obtain an initial low-resolution
probability volume. We then refine these probabilities by performing a denser
sampling only in high-probability regions and process the refined values for
predicting a 2D location and orientation angle. We conduct an evaluation on two
standard floorplan localization benchmarks. Our experiments demonstrate that
our approach substantially outperforms state-of-the-art methods, achieving
significant improvements in recall metrics compared to prior works. Moreover,
we show that our framework can easily incorporate additional metadata such as
room labels, enabling additional gains in both accuracy and efficiency.

</details>


### [151] [Geo-RepNet: Geometry-Aware Representation Learning for Surgical Phase Recognition in Endoscopic Submucosal Dissection](https://arxiv.org/abs/2507.09294)
*Rui Tang,Haochen Yin,Guankun Wang,Long Bai,An Wang,Huxin Gao,Jiazheng Wang,Hongliang Ren*

Main category: cs.CV

TL;DR: Geo-RepNet利用深度信息增强手术阶段识别，结合RGB和深度数据，通过几何感知模块提升性能。


<details>
  <summary>Details</summary>
Motivation: 手术阶段识别在智能辅助系统中至关重要，但RGB图像的高视觉相似性和缺乏结构信息带来挑战，深度信息可提供几何线索。

Method: 提出Geo-RepNet框架，包括Depth-Guided Geometric Prior Generation (DGPG)模块和Geometry-Enhanced Multi-scale Attention (GEMA)模块，基于RepVGG骨干网络。

Result: 在九阶段ESD数据集上，Geo-RepNet实现了最先进的性能，保持鲁棒性和高效计算。

Conclusion: 深度信息显著提升手术阶段识别性能，Geo-RepNet在复杂手术场景中表现出色。

Abstract: Surgical phase recognition plays a critical role in developing intelligent
assistance systems for minimally invasive procedures such as Endoscopic
Submucosal Dissection (ESD). However, the high visual similarity across
different phases and the lack of structural cues in RGB images pose significant
challenges. Depth information offers valuable geometric cues that can
complement appearance features by providing insights into spatial relationships
and anatomical structures. In this paper, we pioneer the use of depth
information for surgical phase recognition and propose Geo-RepNet, a
geometry-aware convolutional framework that integrates RGB image and depth
information to enhance recognition performance in complex surgical scenes.
Built upon a re-parameterizable RepVGG backbone, Geo-RepNet incorporates the
Depth-Guided Geometric Prior Generation (DGPG) module that extracts geometry
priors from raw depth maps, and the Geometry-Enhanced Multi-scale Attention
(GEMA) to inject spatial guidance through geometry-aware cross-attention and
efficient multi-scale aggregation. To evaluate the effectiveness of our
approach, we construct a nine-phase ESD dataset with dense frame-level
annotations from real-world ESD videos. Extensive experiments on the proposed
dataset demonstrate that Geo-RepNet achieves state-of-the-art performance while
maintaining robustness and high computational efficiency under complex and
low-texture surgical environments.

</details>


### [152] [ViT-ProtoNet for Few-Shot Image Classification: A Multi-Benchmark Evaluation](https://arxiv.org/abs/2507.09299)
*Abdulvahap Mutlu,Şengül Doğan,Türker Tuncer*

Main category: cs.CV

TL;DR: ViT-ProtoNet结合ViT-Small和原型网络，在少样本图像分类中表现优于CNN原型网络，并在多个基准测试中取得显著提升。


<details>
  <summary>Details</summary>
Motivation: 利用Vision Transformers（ViTs）在少样本图像分类中的潜力，提升原型网络的性能。

Method: 将ViT-Small作为骨干网络集成到原型网络中，通过平均支持样本的类别条件标记嵌入构建鲁棒原型。

Result: 在5-shot设置下，ViT-ProtoNet在多个基准测试中表现优于CNN原型网络，最高提升3.2%准确率，并展示出更好的特征可分性。

Conclusion: ViT-ProtoNet是一种强大且灵活的少样本分类方法，为基于Transformer的元学习设定了新基准。

Abstract: The remarkable representational power of Vision Transformers (ViTs) remains
underutilized in few-shot image classification. In this work, we introduce
ViT-ProtoNet, which integrates a ViT-Small backbone into the Prototypical
Network framework. By averaging class conditional token embeddings from a
handful of support examples, ViT-ProtoNet constructs robust prototypes that
generalize to novel categories under 5-shot settings. We conduct an extensive
empirical evaluation on four standard benchmarks: Mini-ImageNet, FC100,
CUB-200, and CIFAR-FS, including overlapped support variants to assess
robustness. Across all splits, ViT-ProtoNet consistently outperforms CNN-based
prototypical counterparts, achieving up to a 3.2\% improvement in 5-shot
accuracy and demonstrating superior feature separability in latent space.
Furthermore, it outperforms or is competitive with transformer-based
competitors using a more lightweight backbone. Comprehensive ablations examine
the impact of transformer depth, patch size, and fine-tuning strategy. To
foster reproducibility, we release code and pretrained weights. Our results
establish ViT-ProtoNet as a powerful, flexible approach for few-shot
classification and set a new baseline for transformer-based meta-learners.

</details>


### [153] [DAA*: Deep Angular A Star for Image-based Path Planning](https://arxiv.org/abs/2507.09305)
*Zhiwei Xu*

Main category: cs.CV

TL;DR: 提出了一种名为DAA*的新方法，通过路径角度自由度（PAF）改进A*算法，提升路径平滑性和相似性。


<details>
  <summary>Details</summary>
Motivation: 路径平滑性在模仿学习中常被忽视，本文旨在通过自适应路径平滑性提升路径相似性。

Method: 结合PAF和A*算法，通过路径缩短和平滑的联合优化改进路径最优性。

Result: 在7个数据集上显著优于现有方法，路径相似性提升9.0% SPR、6.9% ASIM和3.9% PSIM。

Conclusion: DAA*在路径最优性和相似性上表现优异，但需权衡搜索效率。

Abstract: Path smoothness is often overlooked in path imitation learning from expert
demonstrations. In this paper, we introduce a novel learning method, termed
deep angular A* (DAA*), by incorporating the proposed path angular freedom
(PAF) into A* to improve path similarity through adaptive path smoothness. The
PAF aims to explore the effect of move angles on path node expansion by finding
the trade-off between their minimum and maximum values, allowing for high
adaptiveness for imitation learning. DAA* improves path optimality by closely
aligning with the reference path through joint optimization of path shortening
and smoothing, which correspond to heuristic distance and PAF, respectively.
Throughout comprehensive evaluations on 7 datasets, including 4 maze datasets,
2 video-game datasets, and a real-world drone-view dataset containing 2
scenarios, we demonstrate remarkable improvements of our DAA* over neural A* in
path similarity between the predicted and reference paths with a shorter path
length when the shortest path is plausible, improving by 9.0% SPR, 6.9% ASIM,
and 3.9% PSIM. Furthermore, when jointly learning pathfinding with both path
loss and path probability map loss, DAA* significantly outperforms the
state-of-the-art TransPath by 6.7% SPR, 6.5% PSIM, and 3.7% ASIM. We also
discuss the minor trade-off between path optimality and search efficiency where
applicable.

</details>


### [154] [An Enhanced Classification Method Based on Adaptive Multi-Scale Fusion for Long-tailed Multispectral Point Clouds](https://arxiv.org/abs/2412.11407)
*TianZhu Liu,BangYan Hu,YanFeng Gu,Xian Li,Aleksandra Pižurica*

Main category: cs.CV

TL;DR: 提出了一种基于自适应多尺度融合的多光谱点云分类方法，解决了稀疏标注、地物尺度差异和长尾分布问题。


<details>
  <summary>Details</summary>
Motivation: 现有分类方法在室外数据集上表现不佳，面临稀疏标注、地物尺度差异和长尾分布问题。

Method: 采用网格平衡采样策略生成训练样本，多尺度特征融合模块解决尺度变化问题，自适应混合损失模块平衡不同类别学习能力。

Result: 在三个多光谱点云数据集上验证了方法的有效性，优于现有方法。

Conclusion: 该方法显著提升了室外多光谱点云分类性能。

Abstract: Multispectral point cloud (MPC) captures 3D spatial-spectral information from
the observed scene, which can be used for scene understanding and has a wide
range of applications. However, most of the existing classification methods
were extensively tested on indoor datasets, and when applied to outdoor
datasets they still face problems including sparse labeled targets, differences
in land-covers scales, and long-tailed distributions. To address the above
issues, an enhanced classification method based on adaptive multi-scale fusion
for MPCs with long-tailed distributions is proposed. In the training set
generation stage, a grid-balanced sampling strategy is designed to reliably
generate training samples from sparse labeled datasets. In the feature learning
stage, a multi-scale feature fusion module is proposed to fuse shallow features
of land-covers at different scales, addressing the issue of losing fine
features due to scale variations in land-covers. In the classification stage,
an adaptive hybrid loss module is devised to utilize multi-classification heads
with adaptive weights to balance the learning ability of different classes,
improving the classification performance of small classes due to various-scales
and long-tailed distributions in land-covers. Experimental results on three MPC
datasets demonstrate the effectiveness of the proposed method compared with the
state-of-the-art methods.

</details>


### [155] [AlphaVAE: Unified End-to-End RGBA Image Reconstruction and Generation with Alpha-Aware Representation Learning](https://arxiv.org/abs/2507.09308)
*Zile Wang,Hao Yu,Jiabo Zhan,Chun Yuan*

Main category: cs.CV

TL;DR: 论文提出了ALPHA基准和ALPHAVAE模型，用于RGBA图像生成，显著提升了透明图像的重建质量。


<details>
  <summary>Details</summary>
Motivation: 现有潜在扩散模型在RGB图像合成上表现优异，但透明或分层内容（RGBA图像）的生成缺乏大规模基准。

Method: 提出ALPHA基准和ALPHAVAE模型，通过alpha混合和复合目标训练扩展预训练的RGB VAE。

Result: ALPHAVAE在仅8K图像训练下，PSNR提升4.9 dB，SSIM提升3.2%，透明图像生成效果更优。

Conclusion: ALPHAVAE在RGBA图像生成和重建上表现卓越，为透明图像生成提供了新方法。

Abstract: Recent advances in latent diffusion models have achieved remarkable results
in high-fidelity RGB image synthesis by leveraging pretrained VAEs to compress
and reconstruct pixel data at low computational cost. However, the generation
of transparent or layered content (RGBA image) remains largely unexplored, due
to the lack of large-scale benchmarks. In this work, we propose ALPHA, the
first comprehensive RGBA benchmark that adapts standard RGB metrics to
four-channel images via alpha blending over canonical backgrounds. We further
introduce ALPHAVAE, a unified end-to-end RGBA VAE that extends a pretrained RGB
VAE by incorporating a dedicated alpha channel. The model is trained with a
composite objective that combines alpha-blended pixel reconstruction,
patch-level fidelity, perceptual consistency, and dual KL divergence
constraints to ensure latent fidelity across both RGB and alpha
representations. Our RGBA VAE, trained on only 8K images in contrast to 1M used
by prior methods, achieves a +4.9 dB improvement in PSNR and a +3.2% increase
in SSIM over LayerDiffuse in reconstruction. It also enables superior
transparent image generation when fine-tuned within a latent diffusion
framework. Our code, data, and models are released on
https://github.com/o0o0o00o0/AlphaVAE for reproducibility.

</details>


### [156] [ProactiveBench: A Comprehensive Benchmark Evaluating Proactive Interactions in Video Large Language Models](https://arxiv.org/abs/2507.09313)
*Yueqian Wang,Xiaojun Meng,Yifan Wang,Huishuai Zhang,Dongyan Zhao*

Main category: cs.CV

TL;DR: 论文介绍了ProactiveBench，首个评估多模态对话系统主动交互能力的基准，并提出了PAUC指标以考虑响应时间动态性，优于传统指标。


<details>
  <summary>Details</summary>
Motivation: 随着多模态对话系统研究的深入，用户期望系统更具主动性，例如在视频播放时实时决定多轮响应时机。

Method: 提出ProactiveBench基准和PAUC指标，评估系统在主动交互中的表现，考虑响应时间动态性。

Result: PAUC与传统指标相比更符合人类偏好，能更准确评估主动交互场景的用户体验。

Conclusion: PAUC为主动交互场景提供了更可靠的评估方法，有助于推动多模态对话系统的发展。

Abstract: With the growing research focus on multimodal dialogue systems, the
capability for proactive interaction is gradually gaining recognition. As an
alternative to conventional turn-by-turn dialogue, users increasingly expect
multimodal systems to be more initiative, for example, by autonomously
determining the timing of multi-turn responses in real time during video
playback. To facilitate progress in this emerging area, we introduce
ProactiveBench, the first comprehensive benchmark to evaluate a system's
ability to engage in proactive interaction. Since model responses are generated
at varying timestamps, we further propose PAUC, the first metric that accounts
for the temporal dynamics of model responses. This enables a more accurate
evaluation of systems operating in proactive settings. Through extensive
benchmarking of various baseline systems on ProactiveBench and a user study of
human preferences, we show that PAUC is in better agreement with human
preferences than traditional evaluation metrics, which typically only consider
the textual content of responses. These findings demonstrate that PAUC provides
a more faithful assessment of user experience in proactive interaction
scenarios. Project homepage:
https://github.com/yellow-binary-tree/ProactiveBench

</details>


### [157] [Lightweight Cloud Masking Models for On-Board Inference in Hyperspectral Imaging](https://arxiv.org/abs/2507.08052)
*Mazen Ali,António Pereira,Fabio Gentile,Aser Cortines,Sam Mugel,Román Orús,Stelios P. Neophytides,Michalis Mavrovouniotis*

Main category: cs.CV

TL;DR: 研究评估了多种机器学习方法（如XGBoost、LightGBM和CNN）用于高光谱卫星影像的云和云影掩膜，CNN在准确性和效率上表现最佳。


<details>
  <summary>Details</summary>
Motivation: 云和云影掩膜是高光谱卫星影像预处理的关键步骤，旨在提取高质量、可直接分析的数据。

Method: 比较了梯度提升方法（XGBoost、LightGBM）和卷积神经网络（CNN），并测试了不同模型的性能和效率。

Result: 所有模型准确率超过93%，CNN在特征缩减后表现最优，兼顾高精度、低存储需求和快速推理。

Conclusion: 轻量级AI模型（如CNN）适用于实时高光谱影像处理，支持星载AI系统的开发。

Abstract: Cloud and cloud shadow masking is a crucial preprocessing step in
hyperspectral satellite imaging, enabling the extraction of high-quality,
analysis-ready data. This study evaluates various machine learning approaches,
including gradient boosting methods such as XGBoost and LightGBM as well as
convolutional neural networks (CNNs). All boosting and CNN models achieved
accuracies exceeding 93%. Among the investigated models, the CNN with feature
reduction emerged as the most efficient, offering a balance of high accuracy,
low storage requirements, and rapid inference times on both CPUs and GPUs.
Variations of this version, with only up to 597 trainable parameters,
demonstrated the best trade-off in terms of deployment feasibility, accuracy,
and computational efficiency. These results demonstrate the potential of
lightweight artificial intelligence (AI) models for real-time hyperspectral
image processing, supporting the development of on-board satellite AI systems
for space-based applications.

</details>


### [158] [Dynamic Inter-Class Confusion-Aware Encoder for Audio-Visual Fusion in Human Activity Recognition](https://arxiv.org/abs/2507.09323)
*Kaixuan Cong,Yifan Wang,Rongkun Xue,Yuyang Jiang,Yiming Feng,Jing Yang*

Main category: cs.CV

TL;DR: 论文提出了一种动态类间混淆感知编码器（DICCAE），通过细粒度类别对齐和动态调整混淆损失，提升模型对相似活动的区分能力。


<details>
  <summary>Details</summary>
Motivation: 现有音视频预训练范式仅关注整体模态对齐，忽略了通过认知归纳和对比强化易混淆类别的区分能力。

Method: 提出DICCAE编码器，动态调整类间混淆损失，并引入音视频融合训练框架和聚类引导的自监督预训练策略。

Result: 在VGGSound数据集上达到65.5%的top-1准确率，接近最先进水平。

Conclusion: DICCAE通过细粒度对齐和动态混淆损失，显著提升模型性能，各模块的必要性得到验证。

Abstract: Humans do not understand individual events in isolation; rather, they
generalize concepts within classes and compare them to others. Existing
audio-video pre-training paradigms only focus on the alignment of the overall
audio-video modalities, without considering the reinforcement of distinguishing
easily confused classes through cognitive induction and contrast during
training. This paper proposes the Dynamic Inter-Class Confusion-Aware Encoder
(DICCAE), an encoder that aligns audio-video representations at a fine-grained,
category-level. DICCAE addresses category confusion by dynamically adjusting
the confusion loss based on inter-class confusion degrees, thereby enhancing
the model's ability to distinguish between similar activities. To further
extend the application of DICCAE, we also introduce a novel training framework
that incorporates both audio and video modalities, as well as their fusion. To
mitigate the scarcity of audio-video data in the human activity recognition
task, we propose a cluster-guided audio-video self-supervised pre-training
strategy for DICCAE. DICCAE achieves near state-of-the-art performance on the
VGGSound dataset, with a top-1 accuracy of 65.5%. We further evaluate its
feature representation quality through extensive ablation studies, validating
the necessity of each module.

</details>


### [159] [Fast3D: Accelerating 3D Multi-modal Large Language Models for Efficient 3D Scene Understanding](https://arxiv.org/abs/2507.09334)
*Wencan Huang,Daizong Liu,Wei Hu*

Main category: cs.CV

TL;DR: Fast3D是一个用于3D多模态大语言模型的视觉令牌剪枝框架，通过全局注意力预测和样本自适应剪枝技术，显著提升计算效率。


<details>
  <summary>Details</summary>
Motivation: 3D多模态大语言模型在场景理解方面表现出色，但由于计算效率低，实际部署面临挑战。

Method: 提出Fast3D框架，包含全局注意力预测（GAP）和样本自适应剪枝（SAP）两项技术，无需修改目标模型参数。

Result: 在五个基准测试中验证了Fast3D的有效性，尤其是在高剪枝率下表现优异。

Conclusion: Fast3D为3D多模态大语言模型的高效部署提供了可行的解决方案。

Abstract: While 3D Multi-modal Large Language Models (MLLMs) demonstrate remarkable
scene understanding capabilities, their practical deployment faces critical
challenges due to computational inefficiency. The key bottleneck stems from
processing excessive object-centric visual tokens required for comprehensive 3D
scene representation. Although visual token pruning has shown promise in
accelerating 2D MLLMs, its applicability to 3D domains remains largely
unexplored due to fundamental disparities in token structures. In this paper,
we reveal two critical insights: (1) Significant redundancy exists in
object-level 3D token representations, analogous to patch-level redundancy in
2D systems; (2) Global attention patterns exhibit strong predictive power for
identifying non-essential tokens in 3D contexts. Building on these
observations, we propose Fast3D, a plug-and-play visual token pruning framework
for 3D MLLMs featuring two technical innovations: (1) Global Attention
Prediction (GAP), where a lightweight neural network learns to predict the
global attention distributions of the target model, enabling efficient token
importance estimation for precise pruning guidance; (2) Sample-Adaptive visual
token Pruning (SAP), which introduces dynamic token budgets through
attention-based complexity assessment, automatically adjusting layer-wise
pruning ratios based on input characteristics. Both of these two techniques
operate without modifying the parameters of the target model. Extensive
evaluations across five benchmarks validate the effectiveness of Fast3D,
particularly under high visual token pruning ratios. Code is available at
https://github.com/wencan25/Fast3D

</details>


### [160] [Simplifying Traffic Anomaly Detection with Video Foundation Models](https://arxiv.org/abs/2507.09338)
*Svetlana Orlova,Tommie Kerssies,Brunó B. Englert,Gijs Dubbelman*

Main category: cs.CV

TL;DR: 研究发现，简单的编码器架构结合强预训练在交通异常检测（TAD）中表现优异，甚至超越复杂方法。自监督预训练和领域自适应预训练是关键。


<details>
  <summary>Details</summary>
Motivation: 探讨复杂架构是否必要，发现基础模型通过预训练可实现简单架构的高效表现。

Method: 使用简单的Video ViTs编码器架构，研究不同预训练方法（自监督、弱监督、全监督）的效果。

Result: 强预训练使简单模型性能超越复杂方法；自监督预训练效果最佳；领域自适应预训练进一步提升性能。

Conclusion: 预训练是关键，简单架构结合有效预训练可实现高效、可扩展的TAD模型。

Abstract: Recent methods for ego-centric Traffic Anomaly Detection (TAD) often rely on
complex multi-stage or multi-representation fusion architectures, yet it
remains unclear whether such complexity is necessary. Recent findings in visual
perception suggest that foundation models, enabled by advanced pre-training,
allow simple yet flexible architectures to outperform specialized designs.
Therefore, in this work, we investigate an architecturally simple encoder-only
approach using plain Video Vision Transformers (Video ViTs) and study how
pre-training enables strong TAD performance. We find that: (i) strong
pre-training enables simple encoder-only models to match or even surpass the
performance of specialized state-of-the-art TAD methods, while also being
significantly more efficient; (ii) although weakly- and fully-supervised
pre-training are advantageous on standard benchmarks, we find them less
effective for TAD. Instead, self-supervised Masked Video Modeling (MVM)
provides the strongest signal; and (iii) Domain-Adaptive Pre-Training (DAPT) on
unlabeled driving videos further improves downstream performance, without
requiring anomalous examples. Our findings highlight the importance of
pre-training and show that effective, efficient, and scalable TAD models can be
built with minimal architectural complexity. We release our code,
domain-adapted encoders, and fine-tuned models to support future work:
https://github.com/tue-mps/simple-tad.

</details>


### [161] [Automated Multi-Class Crop Pathology Classification via Convolutional Neural Networks: A Deep Learning Approach for Real-Time Precision Agriculture](https://arxiv.org/abs/2507.09375)
*Sourish Suri,Yifei Shao*

Main category: cs.CV

TL;DR: 该研究开发了一种基于CNN的图像分类系统，用于自动检测和分类八种常见作物病害，结合深度学习与农业实践，提供实时诊断和治疗建议。


<details>
  <summary>Details</summary>
Motivation: 作物病害对农业生产和全球粮食安全构成重大威胁，尤其是在大规模农业中，早期识别往往延迟或不准确。

Method: 采用CNN架构，包括三个卷积层、ReLU激活、最大池化和全连接层，使用TensorFlow和Keras进行训练，并通过图像预处理增强数据。

Result: 系统训练准确率约90%，验证准确率约60%，表明存在轻微过拟合，但能可靠分类病害并提供治疗建议。

Conclusion: 该研究为精准农业提供了可扩展且易用的工具，结合深度学习与农业支持，展示了CNN在作物健康监测中的潜力。

Abstract: Crop diseases present a significant barrier to agricultural productivity and
global food security, especially in large-scale farming where early
identification is often delayed or inaccurate. This research introduces a
Convolutional Neural Network (CNN)-based image classification system designed
to automate the detection and classification of eight common crop diseases
using leaf imagery. The methodology involves a complete deep learning pipeline:
image acquisition from a large, labeled dataset, preprocessing via resizing,
normalization, and augmentation, and model training using TensorFlow with
Keras' Sequential API. The CNN architecture comprises three convolutional
layers with increasing filter sizes and ReLU activations, followed by max
pooling, flattening, and fully connected layers, concluding with a softmax
output for multi-class classification. The system achieves high training
accuracy (~90%) and demonstrates reliable performance on unseen data, although
a validation accuracy of ~60% suggests minor overfitting. Notably, the model
integrates a treatment recommendation module, providing actionable guidance by
mapping each detected disease to suitable pesticide or fungicide interventions.
Furthermore, the solution is deployed on an open-source, mobile-compatible
platform, enabling real-time image-based diagnostics for farmers in remote
areas. This research contributes a scalable and accessible tool to the field of
precision agriculture, reducing reliance on manual inspection and promoting
sustainable disease management practices. By merging deep learning with
practical agronomic support, this work underscores the potential of CNNs to
transform crop health monitoring and enhance food production resilience on a
global scale.

</details>


### [162] [GreenCrossingAI: A Camera Trap/Computer Vision Pipeline for Environmental Science Research Groups](https://arxiv.org/abs/2507.09410)
*Bernie Boscoe,Shawn Johnson,Andrea Osborn,Chandler Campbell,Karen Mager*

Main category: cs.CV

TL;DR: 本文提出了一种低资源处理相机陷阱数据的流程，结合ML/AI能力，适用于资源有限的小型研究团队。


<details>
  <summary>Details</summary>
Motivation: 相机陷阱数据量大、标注复杂、环境多变，且现有ML/AI工具难以集成到资源有限的团队中。

Method: 开发了一种低资源流程，支持数据本地处理、ML/AI推理和评估。

Result: 为小型研究团队提供了实用的数据处理方法，帮助从海量数据中提取有价值的信息。

Conclusion: 该流程为资源有限的研究者提供了可行的解决方案，提升了相机陷阱数据的处理效率。

Abstract: Camera traps have long been used by wildlife researchers to monitor and study
animal behavior, population dynamics, habitat use, and species diversity in a
non-invasive and efficient manner. While data collection from the field has
increased with new tools and capabilities, methods to develop, process, and
manage the data, especially the adoption of ML/AI tools, remain challenging.
These challenges include the sheer volume of data generated, the need for
accurate labeling and annotation, variability in environmental conditions
affecting data quality, and the integration of ML/AI tools into existing
workflows that often require domain-specific customization and computational
resources. This paper provides a guide to a low-resource pipeline to process
camera trap data on-premise, incorporating ML/AI capabilities tailored for
small research groups with limited resources and computational expertise. By
focusing on practical solutions, the pipeline offers accessible approaches for
data transmission, inference, and evaluation, enabling researchers to discover
meaningful insights from their ever-increasing camera trap datasets.

</details>


### [163] [Domain Adaptation and Multi-view Attention for Learnable Landmark Tracking with Sparse Data](https://arxiv.org/abs/2507.09420)
*Timothy Chase Jr,Karthik Dantu*

Main category: cs.CV

TL;DR: 论文提出了一种轻量级神经网络方法，用于实时检测和描述天体表面地形特征，解决了传统方法的高成本和低效率问题。


<details>
  <summary>Details</summary>
Motivation: 传统的地形特征检测方法依赖离线处理，成本高且效率低，而现有学习技术又受限于计算资源和数据稀缺。本文旨在开发一种适用于航天器硬件的实时解决方案。

Method: 采用轻量级神经网络架构，结合改进的领域适应方法和注意力对齐机制，实现高效的地标检测和描述。

Result: 提出的系统在性能上优于现有技术，适用于实时操作。

Conclusion: 该方法为航天器自主导航提供了高效、低成本的解决方案，具有广泛的应用潜力。

Abstract: The detection and tracking of celestial surface terrain features are crucial
for autonomous spaceflight applications, including Terrain Relative Navigation
(TRN), Entry, Descent, and Landing (EDL), hazard analysis, and scientific data
collection. Traditional photoclinometry-based pipelines often rely on extensive
a priori imaging and offline processing, constrained by the computational
limitations of radiation-hardened systems. While historically effective, these
approaches typically increase mission costs and duration, operate at low
processing rates, and have limited generalization. Recently, learning-based
computer vision has gained popularity to enhance spacecraft autonomy and
overcome these limitations. While promising, emerging techniques frequently
impose computational demands exceeding the capabilities of typical spacecraft
hardware for real-time operation and are further challenged by the scarcity of
labeled training data for diverse extraterrestrial environments. In this work,
we present novel formulations for in-situ landmark tracking via detection and
description. We utilize lightweight, computationally efficient neural network
architectures designed for real-time execution on current-generation spacecraft
flight processors. For landmark detection, we propose improved domain
adaptation methods that enable the identification of celestial terrain features
with distinct, cheaply acquired training data. Concurrently, for landmark
description, we introduce a novel attention alignment formulation that learns
robust feature representations that maintain correspondence despite significant
landmark viewpoint variations. Together, these contributions form a unified
system for landmark tracking that demonstrates superior performance compared to
existing state-of-the-art techniques.

</details>


### [164] [SegVec3D: A Method for Vector Embedding of 3D Objects Oriented Towards Robot manipulation](https://arxiv.org/abs/2507.09459)
*Zhihan Kang,Boyu Wang*

Main category: cs.CV

TL;DR: SegVec3D是一个新颖的3D点云实例分割框架，结合注意力机制、嵌入学习和跨模态对齐，支持无监督实例分割和零样本检索。


<details>
  <summary>Details</summary>
Motivation: 解决3D点云实例分割中几何结构建模和多模态理解的统一问题，减少监督需求并提升实用性。

Method: 构建分层特征提取器增强几何建模，通过对比聚类实现无监督实例分割，并在共享语义空间中对齐3D数据与自然语言查询。

Result: 在实例分割和多模态理解方面优于Mask3D和ULIP等方法，支持零样本检索。

Conclusion: SegVec3D成功统一了实例分割和多模态理解，具有低监督需求和实际部署优势。

Abstract: We propose SegVec3D, a novel framework for 3D point cloud instance
segmentation that integrates attention mechanisms, embedding learning, and
cross-modal alignment. The approach builds a hierarchical feature extractor to
enhance geometric structure modeling and enables unsupervised instance
segmentation via contrastive clustering. It further aligns 3D data with natural
language queries in a shared semantic space, supporting zero-shot retrieval.
Compared to recent methods like Mask3D and ULIP, our method uniquely unifies
instance segmentation and multimodal understanding with minimal supervision and
practical deployability.

</details>


### [165] [Efficient Multi-Person Motion Prediction by Lightweight Spatial and Temporal Interactions](https://arxiv.org/abs/2507.09446)
*Yuanhong Zheng,Ruixuan Yu,Jian Sun*

Main category: cs.CV

TL;DR: 提出了一种计算高效的多人物运动预测模型，通过简化时空交互实现。


<details>
  <summary>Details</summary>
Motivation: 解决多人物运动预测中个体运动依赖和交互建模的高计算成本问题。

Method: 设计轻量级双分支学习局部和全局表示，引入跨级别交互块整合时空表示，并显式加入空间人际距离嵌入。

Result: 在CMU-Mocap、MuPoTS-3D和3DPW数据集上实现最优性能，同时显著降低计算成本。

Conclusion: 该模型在多人物运动预测中高效且性能优越。

Abstract: 3D multi-person motion prediction is a highly complex task, primarily due to
the dependencies on both individual past movements and the interactions between
agents. Moreover, effectively modeling these interactions often incurs
substantial computational costs. In this work, we propose a computationally
efficient model for multi-person motion prediction by simplifying spatial and
temporal interactions. Our approach begins with the design of lightweight dual
branches that learn local and global representations for individual and
multiple persons separately. Additionally, we introduce a novel cross-level
interaction block to integrate the spatial and temporal representations from
both branches. To further enhance interaction modeling, we explicitly
incorporate the spatial inter-person distance embedding. With above efficient
temporal and spatial design, we achieve state-of-the-art performance for
multiple metrics on standard datasets of CMU-Mocap, MuPoTS-3D, and 3DPW, while
significantly reducing the computational cost. Code is available at
https://github.com/Yuanhong-Zheng/EMPMP.

</details>


### [166] [CKAA: Cross-subspace Knowledge Alignment and Aggregation for Robust Continual Learning](https://arxiv.org/abs/2507.09471)
*Lingfeng He,De Cheng,Zhiheng Ma,Huaijie Wang,Dingwen Zhang,Nannan Wang,Xinbo Gao*

Main category: cs.CV

TL;DR: CKAA框架通过双级知识对齐和任务置信度引导的适配器混合，提升了持续学习模型对误导任务ID的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 解决参数高效微调（PEFT）方法中因独立训练子模块导致特征子空间不对齐，从而在误导任务ID下产生模糊决策的问题。

Method: 提出双级知识对齐（DKA）和任务置信度引导的适配器混合（TC-MoA），分别用于训练和推理阶段。

Result: 实验表明CKAA优于现有PEFT方法。

Conclusion: CKAA通过特征对齐和自适应知识聚合，显著提升了模型的鲁棒性和性能。

Abstract: Continual Learning (CL) empowers AI models to continuously learn from
sequential task streams. Recently, parameter-efficient fine-tuning (PEFT)-based
CL methods have garnered increasing attention due to their superior
performance. They typically allocate a unique sub-module for learning each
task, with a task recognizer to select the appropriate sub-modules for testing
images. However, due to the feature subspace misalignment from independently
trained sub-modules, these methods tend to produce ambiguous decisions under
misleading task-ids. To address this, we propose Cross-subspace Knowledge
Alignment and Aggregation (CKAA), a novel framework that enhances model
robustness against misleading task-ids through two key innovations: (1)
Dual-level Knowledge Alignment (DKA): By aligning intra-class feature
distributions across different subspaces and learning a robust global
classifier through a feature simulation process, DKA enables the model to
distinguish features from both correct and incorrect subspaces during training.
(2) Task-Confidence-guided Mixture of Adapters (TC-MoA): A robust inference
scheme that adaptively aggregates task-specific knowledge from relevant
sub-modules based on task-confidence scores, avoiding overconfidence in
misleading task-id predictions. Extensive experiments demonstrate that CKAA
outperforms existing PEFT-based CL methods.

</details>


### [167] [HMID-Net: An Exploration of Masked Image Modeling and Knowledge Distillation in Hyperbolic Space](https://arxiv.org/abs/2507.09487)
*Changli Wang,Fang Yin,Jiafeng Liu,Rui Wu*

Main category: cs.CV

TL;DR: HMID-Net提出了一种在双曲空间中结合掩码图像建模（MIM）和知识蒸馏的高效方法，显著优于现有模型。


<details>
  <summary>Details</summary>
Motivation: 视觉和语义概念通常以层次结构组织，现有方法如MERU在双曲空间中表现良好，但如何更高效地训练模型以利用这种层次结构仍是一个关键问题。

Method: 提出HMID-Net，在双曲空间中整合MIM和知识蒸馏技术，并设计了一种专门的双曲空间知识蒸馏损失函数。

Result: 实验表明，该方法在图像分类和检索任务中显著优于MERU和CLIP等现有模型。

Conclusion: HMID-Net证明了在双曲空间中应用MIM和知识蒸馏的可行性，为高效模型训练提供了新思路。

Abstract: Visual and semantic concepts are often structured in a hierarchical manner.
For instance, textual concept `cat' entails all images of cats. A recent study,
MERU, successfully adapts multimodal learning techniques from Euclidean space
to hyperbolic space, effectively capturing the visual-semantic hierarchy.
However, a critical question remains: how can we more efficiently train a model
to capture and leverage this hierarchy? In this paper, we propose the
\textit{Hyperbolic Masked Image and Distillation Network} (HMID-Net), a novel
and efficient method that integrates Masked Image Modeling (MIM) and knowledge
distillation techniques within hyperbolic space. To the best of our knowledge,
this is the first approach to leverage MIM and knowledge distillation in
hyperbolic space to train highly efficient models. In addition, we introduce a
distillation loss function specifically designed to facilitate effective
knowledge transfer in hyperbolic space. Our experiments demonstrate that MIM
and knowledge distillation techniques in hyperbolic space can achieve the same
remarkable success as in Euclidean space. Extensive evaluations show that our
method excels across a wide range of downstream tasks, significantly
outperforming existing models like MERU and CLIP in both image classification
and retrieval.

</details>


### [168] [GLIMPSE: Do Large Vision-Language Models Truly Think With Videos or Just Glimpse at Them?](https://arxiv.org/abs/2507.09491)
*Yiyang Zhou,Linjie Li,Shi Qiu,Zhengyuan Yang,Yuyang Zhao,Siwei Han,Yangfan He,Kangqi Li,Haonian Ji,Zihao Zhao,Haibo Tong,Lijuan Wang,Huaxiu Yao*

Main category: cs.CV

TL;DR: GLIMPSE是一个新的视频基准测试，旨在评估大型视觉语言模型（LVLMs）是否能真正理解视频内容，而非仅依赖关键帧或静态图像分析。


<details>
  <summary>Details</summary>
Motivation: 现有视频基准测试多基于静态图像问题，无法评估LVLMs是否具备深度时间推理能力，因此需要设计一个更全面的测试。

Method: GLIMPSE包含3,269个视频和4,342个视觉中心问题，涵盖11个类别，所有问题需完整观看视频并进行上下文推理。

Result: 人类评估准确率达94.82%，但最佳LVLM（GPT-o3）仅达66.43%，显示模型在深度视频理解上仍有困难。

Conclusion: GLIMPSE揭示了LVLMs在视频理解上的局限性，需进一步改进以支持真正的视频推理。

Abstract: Existing video benchmarks often resemble image-based benchmarks, with
question types like "What actions does the person perform throughout the
video?" or "What color is the woman's dress in the video?" For these, models
can often answer by scanning just a few key frames, without deep temporal
reasoning. This limits our ability to assess whether large vision-language
models (LVLMs) can truly think with videos rather than perform superficial
frame-level analysis. To address this, we introduce GLIMPSE, a benchmark
specifically designed to evaluate whether LVLMs can genuinely think with
videos. Unlike prior benchmarks, GLIMPSE emphasizes comprehensive video
understanding beyond static image cues. It consists of 3,269 videos and over
4,342 highly visual-centric questions across 11 categories, including
Trajectory Analysis, Temporal Reasoning, and Forensics Detection. All questions
are carefully crafted by human annotators and require watching the entire video
and reasoning over full video context-this is what we mean by thinking with
video. These questions cannot be answered by scanning selected frames or
relying on text alone. In human evaluations, GLIMPSE achieves 94.82% accuracy,
but current LVLMs face significant challenges. Even the best-performing model,
GPT-o3, reaches only 66.43%, highlighting that LVLMs still struggle to move
beyond surface-level reasoning to truly think with videos.

</details>


### [169] [LifelongPR: Lifelong knowledge fusion for point cloud place recognition based on replay and prompt learning](https://arxiv.org/abs/2507.10034)
*Xianghong Zou,Jianping Li,Zhe Chen,Zhen Cao,Zhen Dong,Qiegen Liu,Bisheng Yang*

Main category: cs.CV

TL;DR: 提出了一种名为LifelongPR的持续学习框架，用于解决点云地点识别中的灾难性遗忘问题，通过动态样本选择和提示学习提升性能。


<details>
  <summary>Details</summary>
Motivation: 点云地点识别在现实应用中需要持续适应动态环境，但现有模型存在灾难性遗忘问题，导致性能下降和实用性受限。

Method: 提出动态样本选择方法和基于提示学习的持续学习框架，结合轻量级提示模块和两阶段训练策略。

Result: 在公开和自收集数据集上验证，性能优于现有方法，mIR@1提升6.50%，mR@1提升7.96%，F减少8.95%。

Conclusion: LifelongPR有效解决了点云地点识别中的持续学习问题，提升了模型的适应性和性能。

Abstract: Point cloud place recognition (PCPR) plays a crucial role in photogrammetry
and robotics applications such as autonomous driving, intelligent
transportation, and augmented reality. In real-world large-scale deployments of
a positioning system, PCPR models must continuously acquire, update, and
accumulate knowledge to adapt to diverse and dynamic environments, i.e., the
ability known as continual learning (CL). However, existing PCPR models often
suffer from catastrophic forgetting, leading to significant performance
degradation in previously learned scenes when adapting to new environments or
sensor types. This results in poor model scalability, increased maintenance
costs, and system deployment difficulties, undermining the practicality of
PCPR. To address these issues, we propose LifelongPR, a novel continual
learning framework for PCPR, which effectively extracts and fuses knowledge
from sequential point cloud data. First, to alleviate the knowledge loss, we
propose a replay sample selection method that dynamically allocates sample
sizes according to each dataset's information quantity and selects spatially
diverse samples for maximal representativeness. Second, to handle domain
shifts, we design a prompt learning-based CL framework with a lightweight
prompt module and a two-stage training strategy, enabling domain-specific
feature adaptation while minimizing forgetting. Comprehensive experiments on
large-scale public and self-collected datasets are conducted to validate the
effectiveness of the proposed method. Compared with state-of-the-art (SOTA)
methods, our method achieves 6.50% improvement in mIR@1, 7.96% improvement in
mR@1, and an 8.95% reduction in F. The code and pre-trained models are publicly
available at https://github.com/zouxianghong/LifelongPR.

</details>


### [170] [SDTN and TRN: Adaptive Spectral-Spatial Feature Extraction for Hyperspectral Image Classification](https://arxiv.org/abs/2507.09492)
*Fuyin Ye,Erwen Yao,Jianyong Chen,Fengmei He,Junxiang Zhang,Lihao Ni*

Main category: cs.CV

TL;DR: 提出了一种结合张量分解和正则化的自适应性网络（SDTN）及其轻量级扩展（TRN），用于高光谱图像分类，显著提升精度并降低计算复杂度。


<details>
  <summary>Details</summary>
Motivation: 传统方法在处理高维数据、光谱-空间冗余和标记样本稀缺时表现不佳，需要一种更高效且适应性强的解决方案。

Method: SDTN通过动态调整张量秩优化特征表示，TRN则整合SDTN提取的特征，构建轻量级多尺度网络。

Result: 在PaviaU数据集上，相比现有方法，精度显著提升且模型参数减少。

Conclusion: SDTN和TRN框架在资源受限环境中具有实时部署潜力，为高光谱分类提供了高效解决方案。

Abstract: Hyperspectral image classification plays a pivotal role in precision
agriculture, providing accurate insights into crop health monitoring, disease
detection, and soil analysis. However, traditional methods struggle with
high-dimensional data, spectral-spatial redundancy, and the scarcity of labeled
samples, often leading to suboptimal performance. To address these challenges,
we propose the Self-Adaptive Tensor- Regularized Network (SDTN), which combines
tensor decomposition with regularization mechanisms to dynamically adjust
tensor ranks, ensuring optimal feature representation tailored to the
complexity of the data. Building upon SDTN, we propose the Tensor-Regularized
Network (TRN), which integrates the features extracted by SDTN into a
lightweight network capable of capturing spectral-spatial features at multiple
scales. This approach not only maintains high classification accuracy but also
significantly reduces computational complexity, making the framework highly
suitable for real-time deployment in resource-constrained environments.
Experiments on PaviaU datasets demonstrate significant improvements in accuracy
and reduced model parameters compared to state-of-the-art methods.

</details>


### [171] [Advancing Reliable Test-Time Adaptation of Vision-Language Models under Visual Variations](https://arxiv.org/abs/2507.09500)
*Yiwen Liang,Hui Chen,Yizhe Xiong,Zihan Zhou,Mengyao Lyu,Zijia Lin,Shuaicheng Niu,Sicheng Zhao,Jungong Han,Guiguang Ding*

Main category: cs.CV

TL;DR: 论文提出了一种可靠的测试时适应方法（ReTA），通过一致性感知熵重加权（CER）和多样性驱动的分布校准（DDC）解决现有缓存方法在分布偏移下的不可靠性问题。


<details>
  <summary>Details</summary>
Motivation: 视觉语言模型（VLMs）在零样本任务中表现优异，但在无标注数据的下游任务中面临分布偏移问题，测试时适应（TTA）方法旨在提升其推理性能。现有缓存方法因熵不可靠和决策边界不灵活导致性能下降。

Method: ReTA结合CER和DDC两种策略：CER通过一致性约束加权熵以优化缓存质量；DDC将文本嵌入建模为高斯分布，动态调整决策边界以适应多样性内容。

Result: 实验表明，ReTA在真实分布偏移场景下优于现有方法。

Conclusion: ReTA通过增强缓存可靠性和决策灵活性，显著提升了VLMs在测试时的适应能力。

Abstract: Vision-language models (VLMs) exhibit remarkable zero-shot capabilities but
struggle with distribution shifts in downstream tasks when labeled data is
unavailable, which has motivated the development of Test-Time Adaptation (TTA)
to improve VLMs' performance during inference without annotations. Among
various TTA approaches, cache-based methods show promise by preserving
historical knowledge from low-entropy samples in a dynamic cache and fostering
efficient adaptation. However, these methods face two critical reliability
challenges: (1) entropy often becomes unreliable under distribution shifts,
causing error accumulation in the cache and degradation in adaptation
performance; (2) the final predictions may be unreliable due to inflexible
decision boundaries that fail to accommodate large downstream shifts. To
address these challenges, we propose a Reliable Test-time Adaptation (ReTA)
method that integrates two complementary strategies to enhance reliability from
two perspectives. First, to mitigate the unreliability of entropy as a sample
selection criterion for cache construction, we introduce Consistency-aware
Entropy Reweighting (CER), which incorporates consistency constraints to weight
entropy during cache updating. While conventional approaches rely solely on low
entropy for cache prioritization and risk introducing noise, our method
leverages predictive consistency to maintain a high-quality cache and
facilitate more robust adaptation. Second, we present Diversity-driven
Distribution Calibration (DDC), which models class-wise text embeddings as
multivariate Gaussian distributions, enabling adaptive decision boundaries for
more accurate predictions across visually diverse content. Extensive
experiments demonstrate that ReTA consistently outperforms state-of-the-art
methods, particularly under challenging real-world distribution shifts.

</details>


### [172] [Online Micro-gesture Recognition Using Data Augmentation and Spatial-Temporal Attention](https://arxiv.org/abs/2507.09512)
*Pengyu Liu,Kun Li,Fei Wang,Yanyan Wei,Junhui She,Dan Guo*

Main category: cs.CV

TL;DR: HFUT-VUT团队提出了一种用于微手势在线识别的新方法，结合数据增强和时空注意力机制，显著提升了性能，并在IJCAI 2025 MiGA挑战赛中排名第一。


<details>
  <summary>Details</summary>
Motivation: 微手势在线识别任务具有挑战性，需在未修剪视频中精确定位和分类微手势实例，且微手势与其他人类动作差异较大。

Method: 采用手工数据增强和时空注意力机制，以提高模型的分类和定位能力。

Result: F1分数达到38.03，比之前最优方法提升了37.9%，在比赛中排名第一。

Conclusion: 该方法在微手势在线识别任务中表现出色，验证了其有效性。

Abstract: In this paper, we introduce the latest solution developed by our team,
HFUT-VUT, for the Micro-gesture Online Recognition track of the IJCAI 2025 MiGA
Challenge. The Micro-gesture Online Recognition task is a highly challenging
problem that aims to locate the temporal positions and recognize the categories
of multiple micro-gesture instances in untrimmed videos. Compared to
traditional temporal action detection, this task places greater emphasis on
distinguishing between micro-gesture categories and precisely identifying the
start and end times of each instance. Moreover, micro-gestures are typically
spontaneous human actions, with greater differences than those found in other
human actions. To address these challenges, we propose hand-crafted data
augmentation and spatial-temporal attention to enhance the model's ability to
classify and localize micro-gestures more accurately. Our solution achieved an
F1 score of 38.03, outperforming the previous state-of-the-art by 37.9%. As a
result, our method ranked first in the Micro-gesture Online Recognition track.

</details>


### [173] [Privacy-Preserving Multi-Stage Fall Detection Framework with Semi-supervised Federated Learning and Robotic Vision Confirmation](https://arxiv.org/abs/2507.10474)
*Seyed Alireza Rahimi Azghadi,Truong-Thanh-Hung Nguyen,Helene Fournier,Monica Wachowicz,Rene Richard,Francis Palma,Hung Cao*

Main category: cs.CV

TL;DR: 提出了一种结合半监督联邦学习、室内定位导航和视觉识别的跌倒检测框架，整体准确率达99.99%，兼顾隐私保护。


<details>
  <summary>Details</summary>
Motivation: 老龄化加剧导致老年人跌倒风险增加，及时检测可减少医疗支出和恢复时间，同时需解决隐私问题。

Method: 框架包含三个互补系统：SF2D（半监督联邦学习跌倒检测）、室内定位导航系统和视觉识别系统，通过可穿戴设备和边缘设备实现。

Result: SF2D准确率99.19%，视觉识别准确率96.3%，导航系统成功率95%，综合准确率达99.99%。

Conclusion: 该框架高效可靠，隐私保护性强，适合老年人跌倒检测。

Abstract: The aging population is growing rapidly, and so is the danger of falls in
older adults. A major cause of injury is falling, and detection in time can
greatly save medical expenses and recovery time. However, to provide timely
intervention and avoid unnecessary alarms, detection systems must be effective
and reliable while addressing privacy concerns regarding the user. In this
work, we propose a framework for detecting falls using several complementary
systems: a semi-supervised federated learning-based fall detection system
(SF2D), an indoor localization and navigation system, and a vision-based human
fall recognition system. A wearable device and an edge device identify a fall
scenario in the first system. On top of that, the second system uses an indoor
localization technique first to localize the fall location and then navigate a
robot to inspect the scenario. A vision-based detection system running on an
edge device with a mounted camera on a robot is used to recognize fallen
people. Each of the systems of this proposed framework achieves different
accuracy rates. Specifically, the SF2D has a 0.81% failure rate equivalent to
99.19% accuracy, while the vision-based fallen people detection achieves 96.3%
accuracy. However, when we combine the accuracy of these two systems with the
accuracy of the navigation system (95% success rate), our proposed framework
creates a highly reliable performance for fall detection, with an overall
accuracy of 99.99%. Not only is the proposed framework safe for older adults,
but it is also a privacy-preserving solution for detecting falls.

</details>


### [174] [QuarterMap: Efficient Post-Training Token Pruning for Visual State Space Models](https://arxiv.org/abs/2507.09514)
*Tien-Yu Chi,Hung-Yueh Chiang,Diana Marculescu,Kai-Chiang Wu*

Main category: cs.CV

TL;DR: QuarterMap是一种后训练激活剪枝方法，通过移除冗余空间激活并恢复维度，提升VMamba等模型的吞吐量，同时保持准确性。


<details>
  <summary>Details</summary>
Motivation: 解决VMamba等基于状态空间模型的视觉骨干网络中空间冗余问题，提升效率而不需重新训练。

Method: 提出QuarterMap方法，通过剪枝冗余空间激活和最近邻上采样恢复维度，优化扫描效率。

Result: 在ImageNet-1K上实现11%的速度提升，精度下降小于0.9%；在ADE20K和MedMamba上也表现良好。

Conclusion: QuarterMap为SSM模型提供了一种即插即用的部署效率工具，无需牺牲迁移性。

Abstract: State space models (SSMs) reduce the quadratic complexity of transformers by
leveraging linear recurrence. Recently, VMamba has emerged as a strong
SSM-based vision backbone, yet remains bottlenecked by spatial redundancy in
its four-directional scan. We propose QuarterMap, a post-training activation
pruning method that removes redundant spatial activations before scanning and
restores dimensions via nearest-neighbor upsampling. Our method improves
throughput without retraining. On ImageNet-1K, QuarterMap achieves up to 11%
speedup on VMamba with less than 0.9% accuracy drop, and yields similar gains
on ADE20K segmentation. Beyond VMamba, we validate QuarterMap on MedMamba, a
domain-specific model that shares the same four-directional scanning structure,
where it consistently improves throughput while preserving accuracy across
multiple medical imaging tasks. Compared to token merging methods like ToMe,
QuarterMap is tailored for SSMs and avoids costly merge-unmerge operations. Our
method offers a plug-and-play tool for deployment-time efficiency without
compromising transferability.

</details>


### [175] [When Schrödinger Bridge Meets Real-World Image Dehazing with Unpaired Training](https://arxiv.org/abs/2507.09524)
*Yunwei Lan,Zhigao Cui,Xin Luo,Chang Liu,Nian Wang,Menglin Zhang,Yanzhao Su,Dong Liu*

Main category: cs.CV

TL;DR: DehazeSB提出了一种基于Schrödinger Bridge的无配对去雾框架，利用最优传输理论直接桥接雾图和清晰图的分布，并通过细节保留正则化和提示学习提升效果。


<details>
  <summary>Details</summary>
Motivation: 现有基于GAN的无配对去雾方法因生成器的传输映射能力有限，难以充分发挥无配对训练的优势。

Method: 提出DehazeSB框架，结合最优传输理论和Schrödinger Bridge，引入细节保留正则化和基于CLIP的提示学习。

Result: 在多个真实数据集上验证了方法的优越性。

Conclusion: DehazeSB通过优化传输映射和细节保留，显著提升了无配对去雾的性能。

Abstract: Recent advancements in unpaired dehazing, particularly those using GANs, show
promising performance in processing real-world hazy images. However, these
methods tend to face limitations due to the generator's limited transport
mapping capability, which hinders the full exploitation of their effectiveness
in unpaired training paradigms. To address these challenges, we propose
DehazeSB, a novel unpaired dehazing framework based on the Schr\"odinger
Bridge. By leveraging optimal transport (OT) theory, DehazeSB directly bridges
the distributions between hazy and clear images. This enables optimal transport
mappings from hazy to clear images in fewer steps, thereby generating
high-quality results. To ensure the consistency of structural information and
details in the restored images, we introduce detail-preserving regularization,
which enforces pixel-level alignment between hazy inputs and dehazed outputs.
Furthermore, we propose a novel prompt learning to leverage pre-trained CLIP
models in distinguishing hazy images and clear ones, by learning a haze-aware
vision-language alignment. Extensive experiments on multiple real-world
datasets demonstrate our method's superiority. Code:
https://github.com/ywxjm/DehazeSB.

</details>


### [176] [VDInstruct: Zero-Shot Key Information Extraction via Content-Aware Vision Tokenization](https://arxiv.org/abs/2507.09531)
*Son Nguyen,Giang Nguyen,Hung Dao,Thao Do,Daeyoung Kim*

Main category: cs.CV

TL;DR: VDInstruct是一种多模态大语言模型，通过内容感知的标记化策略和显式布局建模，显著提升了密集文档的信息提取效率和准确性。


<details>
  <summary>Details</summary>
Motivation: 现有MLLM在密集文档上表现不佳，且标记化方法效率低下，导致冗余计算和内存浪费。

Method: 提出VDInstruct模型，分离空间区域检测与语义特征提取，采用内容感知标记化策略，并根据文档复杂度生成标记。

Result: 在KIE基准测试中达到SOTA，减少图像标记约3.6倍，零样本评估中F1分数超过基线+5.5分。

Conclusion: 内容感知标记化与显式布局建模为文档理解提供了有前景的方向。

Abstract: Key Information Extraction (KIE) underpins the understanding of visual
documents (e.g., receipts and contracts) by extracting precise semantic content
and accurately capturing spatial structure. Yet existing multimodal large
language models (MLLMs) often perform poorly on dense documents and rely on
vision tokenization approaches that scale with image size, leading to redundant
computation and memory inefficiency. To address these challenges, we introduce
VDInstruct, an MLLM that separates spatial region detection from semantic
feature extraction. Central to our model is a content-aware tokenization
strategy: rather than fragmenting the entire image uniformly, it generates
tokens in proportion to document complexity, preserving critical structure
while eliminating wasted tokens. Leveraging a three-stage training paradigm,
our model achieves state-of-the-art (SOTA) results on KIE benchmarks, matching
or exceeding the accuracy of leading approaches while reducing the number of
image tokens by roughly 3.6x. In zero-shot evaluations, VDInstruct surpasses
strong baselines-such as DocOwl 1.5-by +5.5 F1 points, highlighting its
robustness to unseen documents. These findings show that content-aware
tokenization combined with explicit layout modeling offers a promising
direction forward for document understanding. Data, source code, and model
weights will be made publicly available.

</details>


### [177] [DRPCA-Net: Make Robust PCA Great Again for Infrared Small Target Detection](https://arxiv.org/abs/2507.09541)
*Zihao Xiong,Fei Zhou,Fengyi Wu,Shuai Yuan,Maixia Fu,Zhenming Peng,Jian Yang,Yimian Dai*

Main category: cs.CV

TL;DR: 提出了一种基于动态RPCA的深度展开网络（DRPCA-Net），通过轻量级超网络动态生成参数，结合稀疏性先验，显著提升了红外小目标检测的准确性和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习模型在红外小目标检测中过于复杂且忽视稀疏性先验，导致可解释性、参数效率和泛化能力不足。

Method: 提出DRPCA-Net，结合动态展开机制和动态残差组模块，自适应生成参数并优化背景建模。

Result: 在多个公开红外数据集上，DRPCA-Net显著优于现有方法。

Conclusion: DRPCA-Net通过动态机制和稀疏性先验，实现了高效且鲁棒的红外小目标检测。

Abstract: Infrared small target detection plays a vital role in remote sensing,
industrial monitoring, and various civilian applications. Despite recent
progress powered by deep learning, many end-to-end convolutional models tend to
pursue performance by stacking increasingly complex architectures, often at the
expense of interpretability, parameter efficiency, and generalization. These
models typically overlook the intrinsic sparsity prior of infrared small
targets--an essential cue that can be explicitly modeled for both performance
and efficiency gains. To address this, we revisit the model-based paradigm of
Robust Principal Component Analysis (RPCA) and propose Dynamic RPCA Network
(DRPCA-Net), a novel deep unfolding network that integrates the sparsity-aware
prior into a learnable architecture. Unlike conventional deep unfolding methods
that rely on static, globally learned parameters, DRPCA-Net introduces a
dynamic unfolding mechanism via a lightweight hypernetwork. This design enables
the model to adaptively generate iteration-wise parameters conditioned on the
input scene, thereby enhancing its robustness and generalization across diverse
backgrounds. Furthermore, we design a Dynamic Residual Group (DRG) module to
better capture contextual variations within the background, leading to more
accurate low-rank estimation and improved separation of small targets.
Extensive experiments on multiple public infrared datasets demonstrate that
DRPCA-Net significantly outperforms existing state-of-the-art methods in
detection accuracy. Code is available at https://github.com/GrokCV/DRPCA-Net.

</details>


### [178] [SeqCSIST: Sequential Closely-Spaced Infrared Small Target Unmixing](https://arxiv.org/abs/2507.09556)
*Ximeng Zhai,Bohan Xu,Yaohong Chen,Hao Wang,Kehua Guo,Yimian Dai*

Main category: cs.CV

TL;DR: 论文提出了一种新任务——Sequential CSIST Unmixing，用于从密集的红外小目标群中检测所有目标，并贡献了一个开源生态系统和DeRefNet模型，性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 解决红外图像中远距离密集小目标群因光学镜头和探测器限制而混合成斑点的问题，填补高质量公开数据集的空白。

Method: 提出DeRefNet模型，引入TDFA模块实现帧间信息自适应聚合，并贡献SeqCSIST数据集和工具包。

Result: 在SeqCSIST数据集上，DeRefNet的mAP指标比现有方法提高了5.3%。

Conclusion: 该研究首次在多帧范式中解决CSIST Unmixing任务，为相关研究提供了数据和工具支持。

Abstract: Due to the limitation of the optical lens focal length and the resolution of
the infrared detector, distant Closely-Spaced Infrared Small Target (CSIST)
groups typically appear as mixing spots in the infrared image. In this paper,
we propose a novel task, Sequential CSIST Unmixing, namely detecting all
targets in the form of sub-pixel localization from a highly dense CSIST group.
However, achieving such precise detection is an extremely difficult challenge.
In addition, the lack of high-quality public datasets has also restricted the
research progress. To this end, firstly, we contribute an open-source
ecosystem, including SeqCSIST, a sequential benchmark dataset, and a toolkit
that provides objective evaluation metrics for this special task, along with
the implementation of 23 relevant methods. Furthermore, we propose the
Deformable Refinement Network (DeRefNet), a model-driven deep learning
framework that introduces a Temporal Deformable Feature Alignment (TDFA) module
enabling adaptive inter-frame information aggregation. To the best of our
knowledge, this work is the first endeavor to address the CSIST Unmixing task
within a multi-frame paradigm. Experiments on the SeqCSIST dataset demonstrate
that our method outperforms the state-of-the-art approaches with mean Average
Precision (mAP) metric improved by 5.3\%. Our dataset and toolkit are available
from https://github.com/GrokCV/SeqCSIST.

</details>


### [179] [EHPE: A Segmented Architecture for Enhanced Hand Pose Estimation](https://arxiv.org/abs/2507.09560)
*Bolun Zheng,Xinjie Liu,Qianyu Zhang,Canjin Wang,Fangni Chen,Mingen Xu*

Main category: cs.CV

TL;DR: 论文提出了一种分段架构EHPE，用于改进3D手部姿态估计，通过局部提取指尖和手腕关节减少误差累积，提升整体关节预测精度。


<details>
  <summary>Details</summary>
Motivation: 现有方法忽视指尖和手腕关节的重要性，导致远端关节误差累积，影响整体姿态估计质量。

Method: EHPE分为两阶段：TW阶段提取指尖和手腕关节；PG阶段通过双分支交互网络细化其余关节位置。

Result: 在多个基准测试中，EHPE实现了最先进的性能。

Conclusion: EHPE通过分段架构有效减少误差累积，显著提升了手部姿态估计的准确性。

Abstract: 3D hand pose estimation has garnered great attention in recent years due to
its critical applications in human-computer interaction, virtual reality, and
related fields. The accurate estimation of hand joints is essential for
high-quality hand pose estimation. However, existing methods neglect the
importance of Distal Phalanx Tip (TIP) and Wrist in predicting hand joints
overall and often fail to account for the phenomenon of error accumulation for
distal joints in gesture estimation, which can cause certain joints to incur
larger errors, resulting in misalignments and artifacts in the pose estimation
and degrading the overall reconstruction quality. To address this challenge, we
propose a novel segmented architecture for enhanced hand pose estimation
(EHPE). We perform local extraction of TIP and wrist, thus alleviating the
effect of error accumulation on TIP prediction and further reduce the
predictive errors for all joints on this basis. EHPE consists of two key
stages: In the TIP and Wrist Joints Extraction stage (TW-stage), the positions
of the TIP and wrist joints are estimated to provide an initial accurate joint
configuration; In the Prior Guided Joints Estimation stage (PG-stage), a
dual-branch interaction network is employed to refine the positions of the
remaining joints. Extensive experiments on two widely used benchmarks
demonstrate that EHPE achieves state-of-the-arts performance. Code is available
at https://github.com/SereinNout/EHPE.

</details>


### [180] [Prompt Engineering in Segment Anything Model: Methodologies, Applications, and Emerging Challenges](https://arxiv.org/abs/2507.09562)
*Yidong Jiang*

Main category: cs.CV

TL;DR: 本文首次全面调查了SAM及其变体的提示工程技术，系统整理了该领域的研究进展，揭示了提示工程从简单几何输入到多模态方法的发展，并探讨了优化挑战与未来方向。


<details>
  <summary>Details</summary>
Motivation: 尽管SAM通过基于提示的方法革新了图像分割，但提示工程的关键作用尚未充分研究。本文旨在填补这一空白。

Method: 系统整理并分析了SAM及其变体中提示工程的技术、应用与挑战，涵盖基础方法、实际应用及关键问题。

Result: 揭示了提示工程从简单几何输入到多模态方法的发展，并识别了优化中的独特挑战。

Conclusion: 本文为理解和推进分割基础模型中的提示工程提供了结构化框架，填补了文献中的重要空白。

Abstract: The Segment Anything Model (SAM) has revolutionized image segmentation
through its innovative prompt-based approach, yet the critical role of prompt
engineering in its success remains underexplored. This paper presents the first
comprehensive survey focusing specifically on prompt engineering techniques for
SAM and its variants. We systematically organize and analyze the rapidly
growing body of work in this emerging field, covering fundamental
methodologies, practical applications, and key challenges. Our review reveals
how prompt engineering has evolved from simple geometric inputs to
sophisticated multimodal approaches, enabling SAM's adaptation across diverse
domains including medical imaging and remote sensing. We identify unique
challenges in prompt optimization and discuss promising research directions.
This survey fills an important gap in the literature by providing a structured
framework for understanding and advancing prompt engineering in foundation
models for segmentation.

</details>


### [181] [WordCraft: Interactive Artistic Typography with Attention Awareness and Noise Blending](https://arxiv.org/abs/2507.09573)
*Zhe Wang,Jingbo Zhang,Tianyi Wei,Wanchao Su,Can Wang*

Main category: cs.CV

TL;DR: WordCraft是一个交互式艺术字体系统，结合扩散模型解决现有方法的局限性，支持局部编辑、迭代优化和多语言输入。


<details>
  <summary>Details</summary>
Motivation: 传统艺术字体设计依赖手工，而现有生成模型在交互性和灵活性上不足。

Method: 集成扩散模型，引入无训练的区域注意力机制和噪声混合技术，结合大语言模型解析用户提示。

Result: 系统能高质量生成单字和多字艺术字体，支持多语言和多样化用户需求。

Conclusion: WordCraft显著提升艺术字体合成的交互性，为设计师提供更多创意可能。

Abstract: Artistic typography aims to stylize input characters with visual effects that
are both creative and legible. Traditional approaches rely heavily on manual
design, while recent generative models, particularly diffusion-based methods,
have enabled automated character stylization. However, existing solutions
remain limited in interactivity, lacking support for localized edits, iterative
refinement, multi-character composition, and open-ended prompt interpretation.
We introduce WordCraft, an interactive artistic typography system that
integrates diffusion models to address these limitations. WordCraft features a
training-free regional attention mechanism for precise, multi-region generation
and a noise blending that supports continuous refinement without compromising
visual quality. To support flexible, intent-driven generation, we incorporate a
large language model to parse and structure both concrete and abstract user
prompts. These components allow our framework to synthesize high-quality,
stylized typography across single- and multi-character inputs across multiple
languages, supporting diverse user-centered workflows. Our system significantly
enhances interactivity in artistic typography synthesis, opening up creative
possibilities for artists and designers.

</details>


### [182] [MENTOR: Efficient Multimodal-Conditioned Tuning for Autoregressive Vision Generation Models](https://arxiv.org/abs/2507.09574)
*Haozhe Zhao,Zefan Cai,Shuzheng Si,Liang Chen,Jiuxiang Gu,Wen Xiao,Junjie Hu*

Main category: cs.CV

TL;DR: MENTOR提出了一种新型自回归框架，通过两阶段训练实现多模态输入与图像输出的细粒度对齐，无需额外适配器或交叉注意力模块。


<details>
  <summary>Details</summary>
Motivation: 解决现有文本到图像模型在视觉控制、多模态输入平衡和复杂多模态图像生成训练方面的不足。

Method: 结合自回归图像生成器和两阶段训练范式：多模态对齐阶段和指令调优阶段。

Result: 在DreamBench++基准测试中表现优异，优于基线方法，具有更高的图像重建保真度和训练效率。

Conclusion: MENTOR在多模态图像生成中实现了高效、可控的生成，优于扩散模型。

Abstract: Recent text-to-image models produce high-quality results but still struggle
with precise visual control, balancing multimodal inputs, and requiring
extensive training for complex multimodal image generation. To address these
limitations, we propose MENTOR, a novel autoregressive (AR) framework for
efficient Multimodal-conditioned Tuning for Autoregressive multimodal image
generation. MENTOR combines an AR image generator with a two-stage training
paradigm, enabling fine-grained, token-level alignment between multimodal
inputs and image outputs without relying on auxiliary adapters or
cross-attention modules. The two-stage training consists of: (1) a multimodal
alignment stage that establishes robust pixel- and semantic-level alignment,
followed by (2) a multimodal instruction tuning stage that balances the
integration of multimodal inputs and enhances generation controllability.
Despite modest model size, suboptimal base components, and limited training
resources, MENTOR achieves strong performance on the DreamBench++ benchmark,
outperforming competitive baselines in concept preservation and prompt
following. Additionally, our method delivers superior image reconstruction
fidelity, broad task adaptability, and improved training efficiency compared to
diffusion-based methods. Dataset, code, and models are available at:
https://github.com/HaozheZhao/MENTOR

</details>


### [183] [Memory-Augmented SAM2 for Training-Free Surgical Video Segmentation](https://arxiv.org/abs/2507.09577)
*Ming Yin,Fu Wang,Xujiong Ye,Yanda Meng,Zeyu Fu*

Main category: cs.CV

TL;DR: MA-SAM2是一种无需训练的视频对象分割策略，通过上下文感知和抗遮挡内存模型，显著提升了复杂手术视频的分割性能。


<details>
  <summary>Details</summary>
Motivation: 手术视频分割对提升手术质量和患者结果至关重要，但现有SAM2框架在复杂手术视频中表现不佳。

Method: 提出MA-SAM2，采用上下文感知和抗遮挡内存模型，结合多目标单循环单提示推理。

Result: 在EndoVis2017和EndoVis2018数据集上分别比SAM2提升4.36%和6.1%。

Conclusion: MA-SAM2在无需额外参数或训练的情况下，显著提升了手术视频分割的鲁棒性和准确性。

Abstract: Surgical video segmentation is a critical task in computer-assisted surgery,
essential for enhancing surgical quality and patient outcomes. Recently, the
Segment Anything Model 2 (SAM2) framework has demonstrated remarkable
advancements in both image and video segmentation. However, the inherent
limitations of SAM2's greedy selection memory design are amplified by the
unique properties of surgical videos-rapid instrument movement, frequent
occlusion, and complex instrument-tissue interaction-resulting in diminished
performance in the segmentation of complex, long videos. To address these
challenges, we introduce Memory Augmented (MA)-SAM2, a training-free video
object segmentation strategy, featuring novel context-aware and
occlusion-resilient memory models. MA-SAM2 exhibits strong robustness against
occlusions and interactions arising from complex instrument movements while
maintaining accuracy in segmenting objects throughout videos. Employing a
multi-target, single-loop, one-prompt inference further enhances the efficiency
of the tracking process in multi-instrument videos. Without introducing any
additional parameters or requiring further training, MA-SAM2 achieved
performance improvements of 4.36% and 6.1% over SAM2 on the EndoVis2017 and
EndoVis2018 datasets, respectively, demonstrating its potential for practical
surgical applications.

</details>


### [184] [Demystifying Flux Architecture](https://arxiv.org/abs/2507.09595)
*Or Greenberg*

Main category: cs.CV

TL;DR: FLUX.1是一种基于扩散的文本生成图像模型，性能优于主流模型，但缺乏官方技术文档。本报告通过逆向工程解析其架构。


<details>
  <summary>Details</summary>
Motivation: 由于FLUX.1缺乏官方技术文档，研究团队通过逆向工程解析其架构，以支持未来研究和开发。

Method: 通过分析FLUX.1的源代码，逆向工程其模型架构和训练设置。

Result: 成功解析了FLUX.1的架构，为后续研究提供了基础。

Conclusion: 本报告为FLUX.1的进一步研究和应用提供了技术支持，尽管未经官方认可。

Abstract: FLUX.1 is a diffusion-based text-to-image generation model developed by Black
Forest Labs, designed to achieve faithful text-image alignment while
maintaining high image quality and diversity. FLUX is considered
state-of-the-art in text-to-image generation, outperforming popular models such
as Midjourney, DALL-E 3, Stable Diffusion 3 (SD3), and SDXL. Although publicly
available as open source, the authors have not released official technical
documentation detailing the model's architecture or training setup. This report
summarizes an extensive reverse-engineering effort aimed at demystifying FLUX's
architecture directly from its source code, to support its adoption as a
backbone for future research and development. This document is an unofficial
technical report and is not published or endorsed by the original developers or
their affiliated institutions.

</details>


### [185] [Inter2Former: Dynamic Hybrid Attention for Efficient High-Precision Interactive](https://arxiv.org/abs/2507.09612)
*You Huang,Lichao Chen,Jiayi Ji,Liujuan Cao,Shengchuan Zhang,Rongrong Ji*

Main category: cs.CV

TL;DR: Inter2Former提出了一种优化密集令牌处理的方法，通过动态提示嵌入、动态混合注意力、混合专家系统和动态局部上采样，实现了在CPU设备上高效且高精度的交互式分割。


<details>
  <summary>Details</summary>
Motivation: 当前交互式分割方法在密集令牌处理上存在速度与精度的权衡，Inter2Former旨在解决这一问题。

Method: 提出四种关键改进：动态提示嵌入（DPE）、动态混合注意力（DHA）、混合专家系统（HMoE）和动态局部上采样（DLU）。

Result: 在高精度交互式分割基准测试中，Inter2Former实现了最优性能，并在CPU设备上保持高效。

Conclusion: Inter2Former通过优化计算分配，成功平衡了交互式分割的速度与精度问题。

Abstract: Interactive segmentation (IS) improves annotation efficiency by segmenting
target regions from user prompts, with widespread applications in real-world
scenarios. Current approaches face a critical trade-off: dense-token methods
achieve superior accuracy and detail preservation but suffer from prohibitively
slow processing on CPU devices, while the Segment Anything Model (SAM) advances
the field with sparse prompt tokens for fast inference but compromises
segmentation quality. In this paper, we propose Inter2Former to address this
challenge by optimizing computation allocation in dense-token processing, which
introduces four key enhancements. First, we propose Dynamic Prompt Embedding
(DPE) that adaptively processes only regions of interest while avoiding
additional overhead from background tokens. Second, we introduce Dynamic Hybrid
Attention (DHA), which leverages previous segmentation masks to route tokens
through either full attention (O(N2)) for boundary regions or our proposed
efficient BSQ attention (O(N)) for non-boundary regions. Third, we develop
Hybrid Mixture of Experts (HMoE), which applies similar adaptive computation
strategies in FFN modules with CPU-optimized parallel processing. Finally, we
present Dynamic Local Upsampling (DLU), a reverse operation of DPE, which
localizes objects with a lightweight MLP and performs fine-grained upsampling
only in detected regions. Experimental results on high-precision IS benchmarks
demonstrate that Inter2Former achieves SOTA performance with high efficiency on
CPU devices.

</details>


### [186] [Towards Fine-Grained Adaptation of CLIP via a Self-Trained Alignment Score](https://arxiv.org/abs/2507.09615)
*Eman Ali,Sathira Silva,Chetan Arora,Muhammad Haris Khan*

Main category: cs.CV

TL;DR: FAIR方法通过动态对齐图像特征与语言嵌入，改进无监督适应中的伪标签生成，显著提升细粒度分类性能。


<details>
  <summary>Details</summary>
Motivation: 现有无监督适应方法无法捕捉细粒度类别差异或计算成本高，需要更高效且准确的方法。

Method: 提出FAIR方法，使用Class Description Anchors动态对齐特征，定义Learned Alignment Score，并结合自训练加权机制优化伪标签。

Result: 在13个细粒度数据集上，FAIR比现有方法整体性能提升2.78%。

Conclusion: FAIR通过动态跨模态交互和伪标签优化，显著提升了细粒度无监督适应的性能。

Abstract: Vision-language models (VLMs) like CLIP excel in zero-shot learning by
aligning image and text representations through contrastive pretraining.
Existing approaches to unsupervised adaptation (UA) for fine-grained
classification with VLMs either rely on fixed alignment scores that cannot
capture evolving, subtle class distinctions or use computationally expensive
pseudo-labeling strategies that limit scalability. In contrast, we show that
modeling fine-grained cross-modal interactions during adaptation produces more
accurate, class-discriminative pseudo-labels and substantially improves
performance over state-of-the-art (SOTA) methods. We introduce Fine-grained
Alignment and Interaction Refinement (FAIR), an innovative approach that
dynamically aligns localized image features with descriptive language
embeddings through a set of Class Description Anchors (CDA). This enables the
definition of a Learned Alignment Score (LAS), which incorporates CDA as an
adaptive classifier, facilitating cross-modal interactions to improve
self-training in unsupervised adaptation. Furthermore, we propose a
self-training weighting mechanism designed to refine pseudo-labels in the
presence of inter-class ambiguities. Our approach, FAIR, delivers a substantial
performance boost in fine-grained unsupervised adaptation, achieving a notable
overall gain of 2.78% across 13 fine-grained datasets compared to SOTA methods.

</details>


### [187] [Generate Aligned Anomaly: Region-Guided Few-Shot Anomaly Image-Mask Pair Synthesis for Industrial Inspection](https://arxiv.org/abs/2507.09619)
*Yilin Lu,Jianghang Lin,Linhuang Xie,Kai Zhao,Yansong Qu,Shengchuan Zhang,Liujuan Cao,Rongrong Ji*

Main category: cs.CV

TL;DR: GAA是一个基于区域引导的少样本异常图像-掩模对生成框架，利用预训练的潜在扩散模型生成真实、多样且语义对齐的异常样本，解决了现有方法在真实性和泛化性上的不足。


<details>
  <summary>Details</summary>
Motivation: 工业制造中异常检测样本稀缺，现有异常合成方法存在真实性低、掩模对齐不准确和泛化性差的问题。

Method: GAA通过局部概念分解联合建模异常的语义特征和空间信息，利用自适应多轮异常聚类增强异常表示一致性，并通过区域引导掩模生成策略确保异常与掩模的精确对齐。

Result: 在MVTec AD和LOCO数据集上的实验表明，GAA在异常合成质量和下游任务（如定位和分类）中表现优异。

Conclusion: GAA框架显著提升了异常合成的真实性和泛化性，为工业制造中的异常检测提供了高效的数据增强解决方案。

Abstract: Anomaly inspection plays a vital role in industrial manufacturing, but the
scarcity of anomaly samples significantly limits the effectiveness of existing
methods in tasks such as localization and classification. While several anomaly
synthesis approaches have been introduced for data augmentation, they often
struggle with low realism, inaccurate mask alignment, and poor generalization.
To overcome these limitations, we propose Generate Aligned Anomaly (GAA), a
region-guided, few-shot anomaly image-mask pair generation framework. GAA
leverages the strong priors of a pretrained latent diffusion model to generate
realistic, diverse, and semantically aligned anomalies using only a small
number of samples. The framework first employs Localized Concept Decomposition
to jointly model the semantic features and spatial information of anomalies,
enabling flexible control over the type and location of anomalies. It then
utilizes Adaptive Multi-Round Anomaly Clustering to perform fine-grained
semantic clustering of anomaly concepts, thereby enhancing the consistency of
anomaly representations. Subsequently, a region-guided mask generation strategy
ensures precise alignment between anomalies and their corresponding masks,
while a low-quality sample filtering module is introduced to further improve
the overall quality of the generated samples. Extensive experiments on the
MVTec AD and LOCO datasets demonstrate that GAA achieves superior performance
in both anomaly synthesis quality and downstream tasks such as localization and
classification.

</details>


### [188] [Brain Stroke Detection and Classification Using CT Imaging with Transformer Models and Explainable AI](https://arxiv.org/abs/2507.09630)
*Shomukh Qari,Maha A. Thafar*

Main category: cs.CV

TL;DR: 该研究提出了一种基于MaxViT的人工智能框架，用于多类别中风分类（缺血性、出血性和无中风），通过CT扫描图像实现高准确率（98.00%），并整合了可解释AI（XAI）以提高透明度和临床适用性。


<details>
  <summary>Details</summary>
Motivation: 中风是全球主要死因之一，早期准确诊断对改善患者预后至关重要，尤其是在紧急情况下。CT扫描因其快速、可及性和成本效益成为关键成像方式。

Method: 研究采用MaxViT作为主要深度学习模型，结合其他Transformer变体（如Vision Transformer和ConvNext），并通过数据增强（包括合成图像生成）解决类别不平衡问题。

Result: MaxViT模型在增强后表现最佳，准确率和F1分数达98.00%，优于其他模型和基线方法。

Conclusion: 该研究开发了一种可信赖的AI辅助诊断工具，结合XAI（如Grad-CAM++）提供可视化解释，有助于临床实践中的早期中风检测，挽救更多生命。

Abstract: Stroke is one of the leading causes of death globally, making early and
accurate diagnosis essential for improving patient outcomes, particularly in
emergency settings where timely intervention is critical. CT scans are the key
imaging modality because of their speed, accessibility, and cost-effectiveness.
This study proposed an artificial intelligence framework for multiclass stroke
classification (ischemic, hemorrhagic, and no stroke) using CT scan images from
a dataset provided by the Republic of Turkey's Ministry of Health. The proposed
method adopted MaxViT, a state-of-the-art Vision Transformer, as the primary
deep learning model for image-based stroke classification, with additional
transformer variants (vision transformer, transformer-in-transformer, and
ConvNext). To enhance model generalization and address class imbalance, we
applied data augmentation techniques, including synthetic image generation. The
MaxViT model trained with augmentation achieved the best performance, reaching
an accuracy and F1-score of 98.00%, outperforming all other evaluated models
and the baseline methods. The primary goal of this study was to distinguish
between stroke types with high accuracy while addressing crucial issues of
transparency and trust in artificial intelligence models. To achieve this,
Explainable Artificial Intelligence (XAI) was integrated into the framework,
particularly Grad-CAM++. It provides visual explanations of the model's
decisions by highlighting relevant stroke regions in the CT scans and
establishing an accurate, interpretable, and clinically applicable solution for
early stroke detection. This research contributed to the development of a
trustworthy AI-assisted diagnostic tool for stroke, facilitating its
integration into clinical practice and enhancing access to timely and optimal
stroke diagnosis in emergency departments, thereby saving more lives.

</details>


### [189] [Disentanglement and Assessment of Shortcuts in Ophthalmological Retinal Imaging Exams](https://arxiv.org/abs/2507.09640)
*Leonor Fernandes,Tiago Gonçalves,João Matos,Luis Filipe Nakayama,Jaime S. Cardoso*

Main category: cs.CV

TL;DR: 该论文评估了三种AI模型在糖尿病视网膜病变（DR）预测中的公平性和性能，并探讨了解缠技术对减少偏见的有效性。


<details>
  <summary>Details</summary>
Motivation: 糖尿病视网膜病变是导致工作年龄成年人视力丧失的主要原因，传统筛查方法成本高且难以普及，AI算法提供了可扩展的解决方案，但公平性和泛化性仍是问题。

Method: 使用mBRSET眼底数据集，训练ConvNeXt V2、DINOv2和Swin V2模型预测DR和敏感属性（如年龄和性别），评估公平性并应用解缠技术减少偏见。

Result: 所有模型在DR预测中表现优异（最高94% AUROC），但公平性评估显示年龄组间存在差异（如DINOv2中10% AUROC差距）。解缠技术对模型性能影响不一，DINOv2性能提升2%，而其他模型性能下降。

Conclusion: 解缠技术在眼底图像中处理细粒度特征具有复杂性，强调了医疗影像AI中公平性的重要性，以确保公平可靠的医疗解决方案。

Abstract: Diabetic retinopathy (DR) is a leading cause of vision loss in working-age
adults. While screening reduces the risk of blindness, traditional imaging is
often costly and inaccessible. Artificial intelligence (AI) algorithms present
a scalable diagnostic solution, but concerns regarding fairness and
generalization persist. This work evaluates the fairness and performance of
image-trained models in DR prediction, as well as the impact of disentanglement
as a bias mitigation technique, using the diverse mBRSET fundus dataset. Three
models, ConvNeXt V2, DINOv2, and Swin V2, were trained on macula images to
predict DR and sensitive attributes (SAs) (e.g., age and gender/sex). Fairness
was assessed between subgroups of SAs, and disentanglement was applied to
reduce bias. All models achieved high DR prediction performance in diagnosing
(up to 94% AUROC) and could reasonably predict age and gender/sex (91% and 77%
AUROC, respectively). Fairness assessment suggests disparities, such as a 10%
AUROC gap between age groups in DINOv2. Disentangling SAs from DR prediction
had varying results, depending on the model selected. Disentanglement improved
DINOv2 performance (2% AUROC gain), but led to performance drops in ConvNeXt V2
and Swin V2 (7% and 3%, respectively). These findings highlight the complexity
of disentangling fine-grained features in fundus imaging and emphasize the
importance of fairness in medical imaging AI to ensure equitable and reliable
healthcare solutions.

</details>


### [190] [EyeSeg: An Uncertainty-Aware Eye Segmentation Framework for AR/VR](https://arxiv.org/abs/2507.09649)
*Zhengyuan Peng,Jianqing Xu,Shen Li,Jiazhen Ji,Yuge Huang,Jingyun Zhang,Jinmin Li,Shouhong Ding,Rizen Guo,Xin Tan,Lizhuang Ma*

Main category: cs.CV

TL;DR: EyeSeg是一种新颖的眼部分割框架，通过贝叶斯不确定性学习解决运动模糊、眼睑遮挡和域差距问题，提升AR/VR中的眼部分割和注视估计性能。


<details>
  <summary>Details</summary>
Motivation: 现有眼部分割方法在运动模糊、眼睑遮挡和域差距情况下表现不佳，影响AR/VR交互体验。

Method: 设计了一种基于贝叶斯不确定性学习的眼部分割框架，显式建模不确定性并输出分割结果和不确定性分数。

Result: 在MIoU、E1、F1和ACC指标上超越现有方法，尤其在运动模糊和跨域场景中表现优异。

Conclusion: EyeSeg通过不确定性学习显著提升了眼部分割的鲁棒性，为AR/VR交互提供了更可靠的注视估计基础。

Abstract: Human-machine interaction through augmented reality (AR) and virtual reality
(VR) is increasingly prevalent, requiring accurate and efficient gaze
estimation which hinges on the accuracy of eye segmentation to enable smooth
user experiences. We introduce EyeSeg, a novel eye segmentation framework
designed to overcome key challenges that existing approaches struggle with:
motion blur, eyelid occlusion, and train-test domain gaps. In these situations,
existing models struggle to extract robust features, leading to suboptimal
performance. Noting that these challenges can be generally quantified by
uncertainty, we design EyeSeg as an uncertainty-aware eye segmentation
framework for AR/VR wherein we explicitly model the uncertainties by performing
Bayesian uncertainty learning of a posterior under the closed set prior.
Theoretically, we prove that a statistic of the learned posterior indicates
segmentation uncertainty levels and empirically outperforms existing methods in
downstream tasks, such as gaze estimation. EyeSeg outputs an uncertainty score
and the segmentation result, weighting and fusing multiple gaze estimates for
robustness, which proves to be effective especially under motion blur, eyelid
occlusion and cross-domain challenges. Moreover, empirical results suggest that
EyeSeg achieves segmentation improvements of MIoU, E1, F1, and ACC surpassing
previous approaches. The code is publicly available at
https://github.com/JethroPeng/EyeSeg.

</details>


### [191] [VST-Pose: A Velocity-Integrated Spatiotem-poral Attention Network for Human WiFi Pose Estimation](https://arxiv.org/abs/2507.09672)
*Xinyu Zhang,Zhonghao Ye,Jingwei Zhang,Xiang Tian,Zhisheng Liang,Shipeng Yu*

Main category: cs.CV

TL;DR: VST-Pose是一种基于WiFi的深度学习框架，用于通过信道状态信息进行准确连续的人体姿态估计，采用双流架构和速度建模分支，性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: WiFi姿态估计因其穿透性和隐私优势成为非视觉替代方案，但需要更精确和连续的方法。

Method: 提出ViSTA-Former双流时空注意力架构，结合速度建模分支捕捉细微动作。

Result: 在自建数据集上PCK@50准确率达92.2%，优于现有方法8.3%；在公共数据集上验证了3D姿态估计的鲁棒性。

Conclusion: VST-Pose为室内环境提供了一种可靠且隐私保护的连续运动分析解决方案。

Abstract: WiFi-based human pose estimation has emerged as a promising non-visual
alternative approaches due to its pene-trability and privacy advantages. This
paper presents VST-Pose, a novel deep learning framework for accurate and
continuous pose estimation using WiFi channel state information. The proposed
method introduces ViSTA-Former, a spatiotemporal attention backbone with
dual-stream architecture that adopts a dual-stream architecture to separately
capture temporal dependencies and structural relationships among body joints.
To enhance sensitivity to subtle human motions, a velocity modeling branch is
integrated into the framework, which learns short-term keypoint dis-placement
patterns and improves fine-grained motion representation. We construct a 2D
pose dataset specifically designed for smart home care scenarios and
demonstrate that our method achieves 92.2% accuracy on the PCK@50 metric,
outperforming existing methods by 8.3% in PCK@50 on the self-collected dataset.
Further evaluation on the public MMFi dataset confirms the model's robustness
and effectiveness in 3D pose estimation tasks. The proposed system provides a
reliable and privacy-aware solution for continuous human motion analysis in
indoor environments. Our codes are available in
https://github.com/CarmenQing/VST-Pose.

</details>


### [192] [Prompt2DEM: High-Resolution DEMs for Urban and Open Environments from Global Prompts Using a Monocular Foundation Model](https://arxiv.org/abs/2507.09681)
*Osher Rafaeli,Tal Svoray,Ariel Nahlieli*

Main category: cs.CV

TL;DR: 提出了一种基于提示的单目深度估计框架，用于高分辨率DEM生成，实现了从30米到30厘米的分辨率提升，并在多种地形中表现出色。


<details>
  <summary>Details</summary>
Motivation: 高分辨率高程数据对水文、城市形态和生态系统研究至关重要，但现有方法存在分辨率限制或缺乏全局高程信息。

Method: 利用低分辨率SRTM数据作为提示，结合高分辨率RGB图像，通过视觉变换器编码器和LiDAR数据微调，实现DEM估计、填补和更新。

Result: 框架实现了100倍分辨率提升，误差低于5米，比SRTM提升18%，适用于水文和环境研究。

Conclusion: 该框架具有强泛化能力和可扩展性，为全球高程测绘提供了新范式。

Abstract: High-resolution elevation estimations are essential to understand catchment
and hillslope hydrology, study urban morphology and dynamics, and monitor the
growth, decline, and mortality of terrestrial ecosystems. Various deep learning
approaches (e.g., super-resolution techniques, monocular depth estimation) have
been developed to create high-resolution Digital Elevation Models (DEMs).
However, super-resolution techniques are limited by the upscaling factor, and
monocular depth estimation lacks global elevation context, making its
conversion to a seamless DEM restricted. The recently introduced technique of
prompt-based monocular depth estimation has opened new opportunities to extract
estimates of absolute elevation in a global context. We present here a
framework for the estimation of high-resolution DEMs as a new paradigm for
absolute global elevation mapping. It is exemplified using low-resolution
Shuttle Radar Topography Mission (SRTM) elevation data as prompts and
high-resolution RGB imagery from the National Agriculture Imagery Program
(NAIP). The approach fine-tunes a vision transformer encoder with LiDAR-derived
DEMs and employs a versatile prompting strategy, enabling tasks such as DEM
estimation, void filling, and updating. Our framework achieves a 100x
resolution gain (from 30-m to 30-cm), surpassing prior methods by an order of
magnitude. Evaluations across three diverse U.S. landscapes show robust
generalization, capturing urban structures and fine-scale terrain features with
< 5 m MAE relative to LiDAR, improving over SRTM by up to 18%. Hydrological
analysis confirms suitability for hazard and environmental studies. We
demonstrate scalability by applying the framework to large regions in the U.S.
and Israel. All code and pretrained models are publicly available at:
https://osherr1996.github.io/prompt2dem_propage/.

</details>


### [193] [ExpStar: Towards Automatic Commentary Generation for Multi-discipline Scientific Experiments](https://arxiv.org/abs/2507.09693)
*Jiali Chen,Yujie Jia,Zihan Wu,Jinyu Yang,Jianpeng Chen,Xusen Hei,Jiayuan Xie,Yi Cai,Qing Li*

Main category: cs.CV

TL;DR: 论文提出自动生成多学科科学实验评论的任务，构建了首个实验评论数据集ExpInstruct，并提出检索增强模型ExpStar，显著优于现有大模型。


<details>
  <summary>Details</summary>
Motivation: 人工准备实验评论耗时且依赖专业知识，需自动化解决方案。

Method: 构建ExpInstruct数据集，提出检索增强模型ExpStar。

Result: ExpStar在14种领先大模型中表现最优。

Conclusion: ExpStar在AI辅助科学实验教学中有巨大潜力。

Abstract: Experiment commentary is crucial in describing the experimental procedures,
delving into underlying scientific principles, and incorporating
content-related safety guidelines. In practice, human teachers rely heavily on
subject-specific expertise and invest significant time preparing such
commentary. To address this challenge, we introduce the task of automatic
commentary generation across multi-discipline scientific experiments. While
recent progress in large multimodal models (LMMs) has demonstrated promising
capabilities in video understanding and reasoning, their ability to generate
fine-grained and insightful experiment commentary remains largely
underexplored. In this paper, we make the following contributions: (i) We
construct \textit{ExpInstruct}, the first dataset tailored for experiment
commentary generation, featuring over 7\textit{K} step-level commentaries
across 21 scientific subjects from 3 core disciplines (\ie, science, healthcare
and engineering). Each sample includes procedural descriptions along with
potential scientific principles (\eg, chemical equations and physical laws) and
safety guidelines. (ii) We propose ExpStar, an automatic experiment commentary
generation model that leverages a retrieval-augmented mechanism to adaptively
access, evaluate, and utilize external knowledge. (iii) Extensive experiments
show that our ExpStar substantially outperforms 14 leading LMMs, which
highlights the superiority of our dataset and model. We believe that ExpStar
holds great potential for advancing AI-assisted scientific experiment
instruction.

</details>


### [194] [Token Compression Meets Compact Vision Transformers: A Survey and Comparative Evaluation for Edge AI](https://arxiv.org/abs/2507.09702)
*Phat Nguyen,Ngai-Man Cheung*

Main category: cs.CV

TL;DR: 本文系统分类和比较了视觉Transformer（ViT）中的令牌压缩技术，并评估了其在标准和紧凑ViT架构上的表现，发现这些方法在紧凑设计上效果不佳。


<details>
  <summary>Details</summary>
Motivation: 由于ViT推理的二次计算复杂度，令牌压缩技术被用于加速推理，但缺乏统一的分类研究和在紧凑ViT上的评估。

Method: 提出首个系统分类法，比较令牌压缩方法（如剪枝、合并或混合），并在标准和紧凑ViT架构上评估代表性技术。

Result: 实验表明，令牌压缩方法对通用ViT有效，但在紧凑设计上表现不佳。

Conclusion: 研究为未来在边缘AI和AI代理应用中优化紧凑Transformer网络提供了实用见解和研究方向。

Abstract: Token compression techniques have recently emerged as powerful tools for
accelerating Vision Transformer (ViT) inference in computer vision. Due to the
quadratic computational complexity with respect to the token sequence length,
these methods aim to remove less informative tokens before the attention layers
to improve inference throughput. While numerous studies have explored various
accuracy-efficiency trade-offs on large-scale ViTs, two critical gaps remain.
First, there is a lack of unified survey that systematically categorizes and
compares token compression approaches based on their core strategies (e.g.,
pruning, merging, or hybrid) and deployment settings (e.g., fine-tuning vs.
plug-in). Second, most benchmarks are limited to standard ViT models (e.g.,
ViT-B, ViT-L), leaving open the question of whether such methods remain
effective when applied to structurally compressed transformers, which are
increasingly deployed on resource-constrained edge devices. To address these
gaps, we present the first systematic taxonomy and comparative study of token
compression methods, and we evaluate representative techniques on both standard
and compact ViT architectures. Our experiments reveal that while token
compression methods are effective for general-purpose ViTs, they often
underperform when directly applied to compact designs. These findings not only
provide practical insights but also pave the way for future research on
adapting token optimization techniques to compact transformer-based networks
for edge AI and AI agent applications.

</details>


### [195] [Advancing Text-to-3D Generation with Linearized Lookahead Variational Score Distillation](https://arxiv.org/abs/2507.09748)
*Yu Lei,Bingde Liu,Qingsong Xie,Haonan Lu,Zhijie Deng*

Main category: cs.CV

TL;DR: 论文提出了一种改进的变分分数蒸馏方法（$L^2$-VSD），通过线性化前瞻优化解决了传统VSD的收敛问题，显著提升了文本到3D生成的质量和稳定性。


<details>
  <summary>Details</summary>
Motivation: 传统变分分数蒸馏（VSD）在实践中存在收敛慢和不适定问题，研究发现LoRA与3D分布不匹配是主要原因。

Method: 提出线性化前瞻变分分数蒸馏（$L^2$-VSD），通过调整优化顺序和线性化模型，稳定训练并提升生成质量。

Result: 实验证明$L^2$-VSD优于现有方法，且能无缝集成到其他VSD框架中。

Conclusion: $L^2$-VSD有效解决了VSD的实践问题，为文本到3D生成提供了更优方案。

Abstract: Text-to-3D generation based on score distillation of pre-trained 2D diffusion
models has gained increasing interest, with variational score distillation
(VSD) as a remarkable example. VSD proves that vanilla score distillation can
be improved by introducing an extra score-based model, which characterizes the
distribution of images rendered from 3D models, to correct the distillation
gradient. Despite the theoretical foundations, VSD, in practice, is likely to
suffer from slow and sometimes ill-posed convergence. In this paper, we perform
an in-depth investigation of the interplay between the introduced score model
and the 3D model, and find that there exists a mismatching problem between LoRA
and 3D distributions in practical implementation. We can simply adjust their
optimization order to improve the generation quality. By doing so, the score
model looks ahead to the current 3D state and hence yields more reasonable
corrections. Nevertheless, naive lookahead VSD may suffer from unstable
training in practice due to the potential over-fitting. To address this, we
propose to use a linearized variant of the model for score distillation, giving
rise to the Linearized Lookahead Variational Score Distillation ($L^2$-VSD).
$L^2$-VSD can be realized efficiently with forward-mode autodiff
functionalities of existing deep learning libraries. Extensive experiments
validate the efficacy of $L^2$-VSD, revealing its clear superiority over prior
score distillation-based methods. We also show that our method can be
seamlessly incorporated into any other VSD-based text-to-3D framework.

</details>


### [196] [ViTCoT: Video-Text Interleaved Chain-of-Thought for Boosting Video Understanding in Large Language Models](https://arxiv.org/abs/2507.09876)
*Yongheng Zhang,Xu Liu,Ruihan Tao,Qiguang Chen,Hao Fei,Wanxiang Che,Libo Qin*

Main category: cs.CV

TL;DR: 论文提出了一种新的视频推理范式ViTCoT，结合视觉和文本信息，显著提升了视频理解性能。


<details>
  <summary>Details</summary>
Motivation: 当前视频推理方法主要依赖文本信息，忽视了视觉模态，而人类在推理时会自然结合视觉内容。

Method: 构建了Video-Text Interleaved Benchmark (ViTIB)，并探索了ViTCoT范式在视频理解中的应用。

Result: ViTCoT显著优于传统文本CoT范式，并激活了更多MLLM神经元。

Conclusion: ViTCoT为视频推理提供了更直观和认知对齐的方法。

Abstract: Video understanding plays a vital role in bridging low-level visual signals
with high-level cognitive reasoning, and is fundamental to applications such as
autonomous driving, embodied AI, and the broader pursuit of AGI. The rapid
development of large language models (LLMs), particularly those utilizing
Chain-of-Thought (CoT) technology, has significantly advanced video reasoning
capabilities. However, current approaches primarily depend on textual
information for reasoning, overlooking the visual modality in the actual video
reasoning process. In contrast, humans naturally re-examine visual content
while reasoning. Motivated by this, we introduce a novel video reasoning
paradigm: Video-Text Interleaved CoT (ViTCoT), which facilitates more intuitive
and cognitively aligned reasoning. To the end, first, we construct the
Video-Text Interleaved Benchmark (ViTIB), which is created using MLLMs for
key-video selection and manually verified. Furthermore, we extensively explore
the potential of the ViTCoT paradigm in the video understanding field.
Extensive experiments demonstrate that ViTCoT significantly enhances
performance compared to the traditional text-only CoT paradigm and effectively
activates more neuron values in MLLMs.

</details>


### [197] [Pairwise Alignment & Compatibility for Arbitrarily Irregular Image Fragments](https://arxiv.org/abs/2507.09767)
*Ofir Itzhak Shahar,Gur Elkin,Ohad Ben-Shahar*

Main category: cs.CV

TL;DR: 提出了一种高效的混合（几何和图像）方法，用于计算碎片对的最优对齐，无需假设其形状、尺寸或图像内容。


<details>
  <summary>Details</summary>
Motivation: 现有方法难以处理现实拼图碎片的几何特性，且依赖碎片形状限制。

Method: 结合几何和图像信息，提出新的对齐方法，并引入模拟考古侵蚀的碎片数据集和评估指标。

Result: 在RePAIR 2D数据集上实现了最先进的邻域级精度和召回率。

Conclusion: 该方法显著提升了碎片兼容性计算的性能。

Abstract: Pairwise compatibility calculation is at the core of most
fragments-reconstruction algorithms, in particular those designed to solve
different types of the jigsaw puzzle problem. However, most existing approaches
fail, or aren't designed to deal with fragments of realistic geometric
properties one encounters in real-life puzzles. And in all other cases,
compatibility methods rely strongly on the restricted shapes of the fragments.
In this paper, we propose an efficient hybrid (geometric and pictorial)
approach for computing the optimal alignment for pairs of fragments, without
any assumptions about their shapes, dimensions, or pictorial content. We
introduce a new image fragments dataset generated via a novel method for image
fragmentation and a formal erosion model that mimics real-world archaeological
erosion, along with evaluation metrics for the compatibility task. We then
embed our proposed compatibility into an archaeological puzzle-solving
framework and demonstrate state-of-the-art neighborhood-level precision and
recall on the RePAIR 2D dataset, directly reflecting compatibility performance
improvements.

</details>


### [198] [NegRefine: Refining Negative Label-Based Zero-Shot OOD Detection](https://arxiv.org/abs/2507.09795)
*Amirhossein Ansari,Ke Wang,Pulei Xiong*

Main category: cs.CV

TL;DR: NegRefine提出了一种改进的负标签细化框架，用于零样本OOD检测，通过过滤子类别标签和专有名词，并引入多匹配感知评分函数，提升了检测的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有基于负标签的方法（如NegLabel和CSP）在区分OOD样本时存在误判问题，尤其是子类别标签和专有名词导致的误检，以及多标签匹配的局限性。

Method: NegRefine通过过滤机制排除负标签集中的子类别和专有名词，并采用动态调整多标签匹配贡献的评分函数。

Result: 在ImageNet-1K等大规模基准测试中，NegRefine表现优于现有方法。

Conclusion: NegRefine显著提升了零样本OOD检测的准确性和鲁棒性。

Abstract: Recent advancements in Vision-Language Models like CLIP have enabled
zero-shot OOD detection by leveraging both image and textual label information.
Among these, negative label-based methods such as NegLabel and CSP have shown
promising results by utilizing a lexicon of words to define negative labels for
distinguishing OOD samples. However, these methods suffer from detecting
in-distribution samples as OOD due to negative labels that are subcategories of
in-distribution labels or proper nouns. They also face limitations in handling
images that match multiple in-distribution and negative labels. We propose
NegRefine, a novel negative label refinement framework for zero-shot OOD
detection. By introducing a filtering mechanism to exclude subcategory labels
and proper nouns from the negative label set and incorporating a
multi-matching-aware scoring function that dynamically adjusts the
contributions of multiple labels matching an image, NegRefine ensures a more
robust separation between in-distribution and OOD samples. We evaluate
NegRefine on large-scale benchmarks, including ImageNet-1K. Source code is
available at https://github.com/ah-ansari/NegRefine.

</details>


### [199] [VRU-Accident: A Vision-Language Benchmark for Video Question Answering and Dense Captioning for Accident Scene Understanding](https://arxiv.org/abs/2507.09815)
*Younggun Kim,Ahmed S. Abdelrahman,Mohamed Abdel-Aty*

Main category: cs.CV

TL;DR: VRU-Accident是一个大规模视觉语言基准，用于评估多模态大语言模型在高风险交通场景中的推理能力，重点关注弱势道路使用者（VRUs）的安全。


<details>
  <summary>Details</summary>
Motivation: 由于涉及VRUs的事故后果严重，目前缺乏标准化基准来评估多模态大语言模型在复杂安全场景中的能力。

Method: 提出VRU-Accident基准，包含1K真实事故视频、6K多选问答对和1K密集场景描述，评估17种先进模型。

Result: 模型在视觉属性上表现良好，但在推理事故原因、类型和可预防性方面存在显著挑战。

Conclusion: VRU-Accident填补了评估空白，揭示了多模态大语言模型在安全关键场景中的局限性。

Abstract: Ensuring the safety of vulnerable road users (VRUs), such as pedestrians and
cyclists, is a critical challenge for autonomous driving systems, as crashes
involving VRUs often result in severe or fatal consequences. While multimodal
large language models (MLLMs) have shown promise in enhancing scene
understanding and decision making in autonomous vehicles, there is currently no
standardized benchmark to quantitatively evaluate their reasoning abilities in
complex, safety-critical scenarios involving VRUs. To address this gap, we
present VRU-Accident, a large-scale vision-language benchmark designed to
evaluate MLLMs in high-risk traffic scenarios involving VRUs. VRU-Accident
comprises 1K real-world dashcam accident videos, annotated with 6K
multiple-choice question-answer pairs across six safety-critical categories
(with 24K candidate options and 3.4K unique answer choices), as well as 1K
dense scene descriptions. Unlike prior works, our benchmark focuses explicitly
on VRU-vehicle accidents, providing rich, fine-grained annotations that capture
both spatial-temporal dynamics and causal semantics of accidents. To assess the
current landscape of MLLMs, we conduct a comprehensive evaluation of 17
state-of-the-art models on the multiple-choice VQA task and on the dense
captioning task. Our findings reveal that while MLLMs perform reasonably well
on visually grounded attributes, they face significant challenges in reasoning
and describing accident causes, types, and preventability.

</details>


### [200] [Cross-modal Associations in Vision and Language Models: Revisiting the bouba-kiki effect](https://arxiv.org/abs/2507.10013)
*Tom Kouwenhoven,Kiana Shahrasbi,Tessa Verhoef*

Main category: cs.CV

TL;DR: 研究重新评估了CLIP模型（ResNet和ViT）在bouba-kiki效应中的表现，发现模型未能像人类一样一致地关联形状与伪词，揭示了其跨模态理解的局限性。


<details>
  <summary>Details</summary>
Motivation: 探讨视觉-语言模型（VLMs）是否像人类一样整合跨模态信息，特别是bouba-kiki效应。

Method: 使用基于提示的概率评估和Grad-CAM视觉注意力分析，测试两种CLIP变体（ResNet和ViT）。

Result: 模型未表现出一致的bouba-kiki效应，且性能远低于人类数据。

Conclusion: VLMs在跨模态概念理解上与人类认知存在显著差距，凸显其内部表征的局限性。

Abstract: Recent advances in multimodal models have raised questions about whether
vision-and-language models (VLMs) integrate cross-modal information in ways
that reflect human cognition. One well-studied test case in this domain is the
bouba-kiki effect, where humans reliably associate pseudowords like "bouba"
with round shapes and "kiki" with jagged ones. Given the mixed evidence found
in prior studies for this effect in VLMs, we present a comprehensive
re-evaluation focused on two variants of CLIP, ResNet and Vision Transformer
(ViT), given their centrality in many state-of-the-art VLMs. We apply two
complementary methods closely modelled after human experiments: a prompt-based
evaluation that uses probabilities as model preference, and we use Grad-CAM as
a novel way to interpret visual attention in shape-word matching tasks. Our
findings show that these models do not consistently exhibit the bouba-kiki
effect. While ResNet shows a preference for round shapes, overall performance
across both models lacks the expected associations. Moreover, direct comparison
with prior human data on the same task shows that the models' responses fall
markedly short of the robust, modality-integrated behaviour characteristic of
human cognition. These results contribute to the ongoing debate about the
extent to which VLMs truly understand cross-modal concepts, highlighting
limitations in their internal representations and alignment with human
intuitions.

</details>


### [201] [Hierarchical Abstraction Enables Human-Like 3D Object Recognition in Deep Learning Models](https://arxiv.org/abs/2507.09830)
*Shuhao Fu,Philip J. Kellman,Hongjing Lu*

Main category: cs.CV

TL;DR: 论文探讨了深度学习模型与人类在识别3D物体形状时的表现差异，发现视觉变换器模型更接近人类表现。


<details>
  <summary>Details</summary>
Motivation: 研究深度学习模型是否形成与人类相似的3D形状表征，以理解其识别机制。

Method: 通过实验系统操纵点密度、物体方向和局部几何结构，比较人类与两种深度学习模型（DGCNN和点变换器）的表现。

Result: 点变换器模型在模拟人类表现上优于卷积神经网络，因其支持3D形状的层次抽象。

Conclusion: 视觉变换器模型在3D形状表征上更接近人类，揭示了其机制的优势。

Abstract: Both humans and deep learning models can recognize objects from 3D shapes
depicted with sparse visual information, such as a set of points randomly
sampled from the surfaces of 3D objects (termed a point cloud). Although deep
learning models achieve human-like performance in recognizing objects from 3D
shapes, it remains unclear whether these models develop 3D shape
representations similar to those used by human vision for object recognition.
We hypothesize that training with 3D shapes enables models to form
representations of local geometric structures in 3D shapes. However, their
representations of global 3D object shapes may be limited. We conducted two
human experiments systematically manipulating point density and object
orientation (Experiment 1), and local geometric structure (Experiment 2).
Humans consistently performed well across all experimental conditions. We
compared two types of deep learning models, one based on a convolutional neural
network (DGCNN) and the other on visual transformers (point transformer), with
human performance. We found that the point transformer model provided a better
account of human performance than the convolution-based model. The advantage
mainly results from the mechanism in the point transformer model that supports
hierarchical abstraction of 3D shapes.

</details>


### [202] [A Survey on MLLM-based Visually Rich Document Understanding: Methods, Challenges, and Emerging Trends](https://arxiv.org/abs/2507.09861)
*Yihao Ding,Siwen Luo,Yue Dai,Yanbei Jiang,Zechuan Li,Geoffrey Martin,Yifan Peng*

Main category: cs.CV

TL;DR: 本文综述了基于多模态大语言模型（MLLMs）的视觉丰富文档理解（VRDU）的最新进展，包括特征编码与融合方法、训练范式及数据集，并探讨了该领域的挑战与未来方向。


<details>
  <summary>Details</summary>
Motivation: 视觉丰富文档理解（VRDU）的需求推动了自动处理复杂视觉、文本和布局信息的文档的研究。多模态大语言模型（MLLMs）在此领域展现出潜力。

Method: 综述了MLLM-based VRDU的三个核心部分：特征编码与融合方法、训练范式（如预训练策略和指令响应调优）以及使用的数据集。

Result: 总结了当前MLLM-based VRDU的研究进展，并提出了该领域的挑战与机遇。

Conclusion: 未来需提升VRDU系统的效率、通用性和鲁棒性，以推动该领域的发展。

Abstract: Visually-Rich Document Understanding (VRDU) has emerged as a critical field,
driven by the need to automatically process documents containing complex
visual, textual, and layout information. Recently, Multimodal Large Language
Models (MLLMs) have shown remarkable potential in this domain, leveraging both
Optical Character Recognition (OCR)-dependent and OCR-free frameworks to
extract and interpret information in document images. This survey reviews
recent advancements in MLLM-based VRDU, highlighting three core components: (1)
methods for encoding and fusing textual, visual, and layout features; (2)
training paradigms, including pretraining strategies, instruction-response
tuning, and the trainability of different model modules; and (3) datasets
utilized for pretraining, instruction-tuning, and supervised fine-tuning.
Finally, we discuss the challenges and opportunities in this evolving field and
propose future directions to advance the efficiency, generalizability, and
robustness of VRDU systems.

</details>


### [203] [SpeakerVid-5M: A Large-Scale High-Quality Dataset for Audio-Visual Dyadic Interactive Human Generation](https://arxiv.org/abs/2507.09862)
*Youliang Zhang,Zhaoyang Li,Duomin Wang,Jiahe Zhang,Deyu Zhou,Zixin Yin,Xili Dai,Gang Yu,Xiu Li*

Main category: cs.CV

TL;DR: 论文介绍了SpeakerVid-5M数据集，首个针对视听双模态交互虚拟人生成的大规模高质量数据集，包含520万视频片段，并提供了基于自回归的视频聊天基线模型和评估基准。


<details>
  <summary>Details</summary>
Motivation: 随着大规模模型的发展，学术界开始关注视听双模态交互虚拟人的挑战，但缺乏相关数据集。本文旨在填补这一空白。

Method: 构建了SpeakerVid-5M数据集，包含四种交互类型和两种质量层次，并训练了一个自回归视频聊天基线模型。

Result: 数据集包含520万视频片段，总计8,743小时，覆盖多种交互场景，并提供了评估基准VidChatBench。

Conclusion: SpeakerVid-5M为视听双模态交互虚拟人研究提供了重要资源，推动了该领域的发展。

Abstract: The rapid development of large-scale models has catalyzed significant
breakthroughs in the digital human domain. These advanced methodologies offer
high-fidelity solutions for avatar driving and rendering, leading academia to
focus on the next major challenge: audio-visual dyadic interactive virtual
human. To facilitate research in this emerging area, we present SpeakerVid-5M
dataset, the first large-scale, high-quality dataset designed for audio-visual
dyadic interactive virtual human generation. Totaling over 8,743 hours,
SpeakerVid-5M contains more than 5.2 million video clips of human portraits. It
covers diverse scales and interaction types, including monadic talking,
listening, and dyadic conversations. Crucially, the dataset is structured along
two key dimensions: interaction type and data quality. First, it is categorized
into four types (dialogue branch, single branch, listening branch and
multi-turn branch) based on the interaction scenario. Second, it is stratified
into a large-scale pre-training subset and a curated, high-quality subset for
Supervised Fine-Tuning (SFT). This dual structure accommodates a wide array of
2D virtual human tasks. In addition, we provide an autoregressive (AR)-based
video chat baseline trained on this data, accompanied by a dedicated set of
metrics and test data to serve as a benchmark VidChatBench for future work.
Both the dataset and the corresponding data processing code will be publicly
released. Project page: https://dorniwang.github.io/SpeakerVid-5M/

</details>


### [204] [FaceLLM: A Multimodal Large Language Model for Face Understanding](https://arxiv.org/abs/2507.10300)
*Hatef Otroshi Shahreza,Sébastien Marcel*

Main category: cs.CV

TL;DR: FaceLLM是一种专为面部图像理解设计的多模态大语言模型，通过ChatGPT生成的弱监督数据集FairFaceGPT训练，提升了面部相关任务的性能。


<details>
  <summary>Details</summary>
Motivation: 现有MLLMs在通用数据集上训练，缺乏对领域特定视觉线索（如面部图像）的推理能力，限制了其在面部结构、表情、情感等任务中的应用。

Method: 提出弱监督流程，利用ChatGPT和属性感知提示从FairFace数据集生成高质量问答对，构建FairFaceGPT数据集，并训练FaceLLM模型。

Result: FaceLLM在多种面部中心任务中表现优异，达到最先进水平。

Conclusion: FaceLLM展示了语言模型合成监督在构建领域专用MLLMs中的潜力，为可信赖、以人为中心的多模态AI系统树立了先例。

Abstract: Multimodal large language models (MLLMs) have shown remarkable performance in
vision-language tasks. However, existing MLLMs are primarily trained on generic
datasets, limiting their ability to reason on domain-specific visual cues such
as those in facial images. In particular, tasks that require detailed
understanding of facial structure, expression, emotion, and demographic
features remain underexplored by MLLMs due to the lack of large-scale annotated
face image-text datasets. In this work, we introduce FaceLLM, a multimodal
large language model trained specifically for facial image understanding. To
construct the training data, we propose a novel weakly supervised pipeline that
uses ChatGPT with attribute-aware prompts to generate high-quality
question-answer pairs based on images from the FairFace dataset. The resulting
corpus, called FairFaceGPT, covers a diverse set of attributes including
expression, pose, skin texture, and forensic information. Our experiments
demonstrate that FaceLLM improves the performance of MLLMs on various
face-centric tasks and achieves state-of-the-art performance. This work
highlights the potential of synthetic supervision via language models for
building domain-specialized MLLMs, and sets a precedent for trustworthy,
human-centric multimodal AI systems. FairFaceGPT dataset and pretrained FaceLLM
models are publicly available in the project page.

</details>


### [205] [OpenHuman4D: Open-Vocabulary 4D Human Parsing](https://arxiv.org/abs/2507.09880)
*Keito Suzuki,Bang Du,Runfa Blark Li,Kunyao Chen,Lei Wang,Peng Liu,Ning Bi,Truong Nguyen*

Main category: cs.CV

TL;DR: 本文提出了一种4D人体解析框架，通过减少推理时间和引入开放词汇能力，解决了现有方法依赖封闭数据集和推理时间长的问题。


<details>
  <summary>Details</summary>
Motivation: 动态3D人体表示在虚拟和扩展现实应用中至关重要，但现有方法受限于封闭数据集和长推理时间。

Method: 采用基于掩码的视频对象跟踪、新颖的掩码验证模块和4D掩码融合模块，实现高效时空对应和稳健嵌入融合。

Result: 实验表明，该方法在4D人体解析任务中有效且灵活，推理速度提升至93.3%。

Conclusion: 该框架显著提升了4D人体解析的效率和适用性，为开放词汇场景提供了新解决方案。

Abstract: Understanding dynamic 3D human representation has become increasingly
critical in virtual and extended reality applications. However, existing human
part segmentation methods are constrained by reliance on closed-set datasets
and prolonged inference times, which significantly restrict their
applicability. In this paper, we introduce the first 4D human parsing framework
that simultaneously addresses these challenges by reducing the inference time
and introducing open-vocabulary capabilities. Building upon state-of-the-art
open-vocabulary 3D human parsing techniques, our approach extends the support
to 4D human-centric video with three key innovations: 1) We adopt mask-based
video object tracking to efficiently establish spatial and temporal
correspondences, avoiding the necessity of segmenting all frames. 2) A novel
Mask Validation module is designed to manage new target identification and
mitigate tracking failures. 3) We propose a 4D Mask Fusion module, integrating
memory-conditioned attention and logits equalization for robust embedding
fusion. Extensive experiments demonstrate the effectiveness and flexibility of
the proposed method on 4D human-centric parsing tasks, achieving up to 93.3%
acceleration compared to the previous state-of-the-art method, which was
limited to parsing fixed classes.

</details>


### [206] [Devanagari Handwritten Character Recognition using Convolutional Neural Network](https://arxiv.org/abs/2507.10398)
*Diksha Mehta,Prateek Mehta*

Main category: cs.CV

TL;DR: 该论文提出了一种基于深度卷积神经网络的手写天城体字符识别方法，旨在解决天城体脚本的数字化问题，并在测试和训练中分别达到了96.36%和99.55%的准确率。


<details>
  <summary>Details</summary>
Motivation: 天城体是印度最古老的语言脚本之一，缺乏适当的数字化工具。研究旨在通过自动化方法提取手写印地语字符，以节省时间并避免数据过时。

Method: 使用两层深度卷积神经网络，结合天城体手写字符数据集（DHCD），包含36类字符，每类1700张图像用于训练和测试。

Result: 测试准确率为96.36%，训练准确率为99.55%。

Conclusion: 该方法在提高天城体手写文本识别率方面表现出色，为天城体脚本的数字化提供了有效解决方案。

Abstract: Handwritten character recognition is getting popular among researchers
because of its possible applications in facilitating technological search
engines, social media, recommender systems, etc. The Devanagari script is one
of the oldest language scripts in India that does not have proper digitization
tools. With the advancement of computing and technology, the task of this
research is to extract handwritten Hindi characters from an image of Devanagari
script with an automated approach to save time and obsolete data. In this
paper, we present a technique to recognize handwritten Devanagari characters
using two deep convolutional neural network layers. This work employs a
methodology that is useful to enhance the recognition rate and configures a
convolutional neural network for effective Devanagari handwritten text
recognition (DHTR). This approach uses the Devanagari handwritten character
dataset (DHCD), an open dataset with 36 classes of Devanagari characters. Each
of these classes has 1700 images for training and testing purposes. This
approach obtains promising results in terms of accuracy by achieving 96.36%
accuracy in testing and 99.55% in training time.

</details>


### [207] [Counterfactual Visual Explanation via Causally-Guided Adversarial Steering](https://arxiv.org/abs/2507.09881)
*Yiran Qiao,Disheng Liu,Yiren Lu,Yu Yin,Mengnan Du,Jing Ma*

Main category: cs.CV

TL;DR: CECAS框架通过因果引导的对抗方法生成反事实解释，避免虚假因素扰动，提升解释质量。


<details>
  <summary>Details</summary>
Motivation: 现有反事实视觉解释方法忽视因果关系和虚假相关性，导致解释质量受限。

Method: 提出CECAS框架，结合因果视角生成反事实解释，避免虚假因素扰动。

Result: 在多个基准数据集上优于现有方法，实现有效性、稀疏性、邻近性和真实性的平衡。

Conclusion: CECAS通过因果引导方法显著提升反事实解释的质量和实用性。

Abstract: Recent work on counterfactual visual explanations has contributed to making
artificial intelligence models more explainable by providing visual
perturbation to flip the prediction. However, these approaches neglect the
causal relationships and the spurious correlations behind the image generation
process, which often leads to unintended alterations in the counterfactual
images and renders the explanations with limited quality. To address this
challenge, we introduce a novel framework CECAS, which first leverages a
causally-guided adversarial method to generate counterfactual explanations. It
innovatively integrates a causal perspective to avoid unwanted perturbations on
spurious factors in the counterfactuals. Extensive experiments demonstrate that
our method outperforms existing state-of-the-art approaches across multiple
benchmark datasets and ultimately achieves a balanced trade-off among various
aspects of validity, sparsity, proximity, and realism.

</details>


### [208] [MCGA: Mixture of Codebooks Hyperspectral Reconstruction via Grayscale-Aware Attention](https://arxiv.org/abs/2507.09885)
*Zhanjiang Yang,Lijun Sun,Jiawei Dong,Xiaoxin An,Yang Liu,Meng Li*

Main category: cs.CV

TL;DR: MCGA提出了一种两阶段方法，通过学习光谱模式并利用混合码本优化RGB到HSI的映射，结合灰度感知注意力和量化自注意力，实现了高效的HSI重建。


<details>
  <summary>Details</summary>
Motivation: 现有方法直接学习RGB到HSI的映射，忽略了从低维到高维信息转换的固有挑战。

Method: MCGA采用两阶段方法：1. 多尺度VQ-VAE学习光谱模式并生成混合码本；2. 利用码本优化RGB到HSI的映射，并引入灰度感知注意力和量化自注意力。

Result: 实验表明MCGA在HSI重建任务中达到了最先进的性能。

Conclusion: MCGA通过两阶段设计和物理驱动的注意力机制，实现了高效且轻量级的HSI重建。

Abstract: Reconstructing hyperspectral images (HSI) from RGB images is a cost-effective
solution for various vision-based applications. However, most existing
learning-based hyperspectral reconstruction methods directly learn the
RGB-to-HSI mapping using complex attention mechanisms, neglecting the inherent
challenge of transitioning from low-dimensional to high-dimensional
information. To address this limitation, we propose a two-stage approach, MCGA,
which first learns spectral patterns before estimating the mapping. In the
first stage, a multi-scale VQ-VAE learns representations from heterogeneous HSI
datasets, extracting a Mixture of Codebooks (MoC). In the second stage, the
RGB-to-HSI mapping is refined by querying features from the MoC to replace
latent HSI representations, incorporating prior knowledge rather than forcing a
direct high-dimensional transformation. To further enhance reconstruction
quality, we introduce Grayscale-Aware Attention and Quantized Self-Attention,
which adaptively adjust feature map intensities to meet hyperspectral
reconstruction requirements. This physically motivated attention mechanism
ensures lightweight and efficient HSI recovery. Moreover, we propose an
entropy-based Test-Time Adaptation strategy to improve robustness in real-world
scenarios. Extensive experiments demonstrate that our method, MCGA, achieves
state-of-the-art performance. The code and models will be released at
https://github.com/Fibonaccirabbit/MCGA

</details>


### [209] [Measuring the Impact of Rotation Equivariance on Aerial Object Detection](https://arxiv.org/abs/2507.09896)
*Xiuyu Wu,Xinhao Wang,Xiubin Zhu,Lan Yang,Jiyuan Liu,Xingchen Hu*

Main category: cs.CV

TL;DR: 论文提出了一种严格旋转等变的单阶段检测器MessDet，通过改进网络结构和多分支头设计，在低参数量下实现了高性能。


<details>
  <summary>Details</summary>
Motivation: 航空图像中物体方向多变，旋转等变性对检测器至关重要，但现有方法多为近似等变，严格等变的效果尚不明确。

Method: 实现严格旋转等变的骨干和颈部网络，并提出多分支头设计以减少参数量并提升精度。

Result: 在DOTA-v1.0、DOTA-v1.5和DIOR-R数据集上达到SOTA性能，参数量极低。

Conclusion: 严格旋转等变对航空图像检测器性能有显著提升，多分支头设计有效平衡了参数量和精度。

Abstract: Due to the arbitrary orientation of objects in aerial images, rotation
equivariance is a critical property for aerial object detectors. However,
recent studies on rotation-equivariant aerial object detection remain scarce.
Most detectors rely on data augmentation to enable models to learn
approximately rotation-equivariant features. A few detectors have constructed
rotation-equivariant networks, but due to the breaking of strict rotation
equivariance by typical downsampling processes, these networks only achieve
approximately rotation-equivariant backbones. Whether strict rotation
equivariance is necessary for aerial image object detection remains an open
question. In this paper, we implement a strictly rotation-equivariant backbone
and neck network with a more advanced network structure and compare it with
approximately rotation-equivariant networks to quantitatively measure the
impact of rotation equivariance on the performance of aerial image detectors.
Additionally, leveraging the inherently grouped nature of rotation-equivariant
features, we propose a multi-branch head network that reduces the parameter
count while improving detection accuracy. Based on the aforementioned
improvements, this study proposes the Multi-branch head rotation-equivariant
single-stage Detector (MessDet), which achieves state-of-the-art performance on
the challenging aerial image datasets DOTA-v1.0, DOTA-v1.5 and DIOR-R with an
exceptionally low parameter count.

</details>


### [210] [IGD: Instructional Graphic Design with Multimodal Layer Generation](https://arxiv.org/abs/2507.09910)
*Yadong Qu,Shancheng Fang,Yuxin Wang,Xiaorui Wang,Zhineng Chen,Hongtao Xie,Yongdong Zhang*

Main category: cs.CV

TL;DR: 论文提出了一种名为IGD的方法，通过自然语言指令快速生成可编辑的多模态图层，解决了现有方法缺乏创造力和可编辑性的问题。


<details>
  <summary>Details</summary>
Motivation: 现有图形设计方法依赖布局生成或生成不可编辑的图像，缺乏智能化和实用性，导致设计过程仍依赖人工。

Method: IGD结合参数化渲染和图像资产生成，利用MLLM的多模态理解能力预测属性、排序和布局，并通过扩散模型生成图像内容。

Result: 实验结果表明，IGD在复杂图形设计任务中具有可扩展性和实用性。

Conclusion: IGD为图形设计提供了新的解决方案，支持可编辑性和智能化。

Abstract: Graphic design visually conveys information and data by creating and
combining text, images and graphics. Two-stage methods that rely primarily on
layout generation lack creativity and intelligence, making graphic design still
labor-intensive. Existing diffusion-based methods generate non-editable graphic
design files at image level with poor legibility in visual text rendering,
which prevents them from achieving satisfactory and practical automated graphic
design. In this paper, we propose Instructional Graphic Designer (IGD) to
swiftly generate multimodal layers with editable flexibility with only natural
language instructions. IGD adopts a new paradigm that leverages parametric
rendering and image asset generation. First, we develop a design platform and
establish a standardized format for multi-scenario design files, thus laying
the foundation for scaling up data. Second, IGD utilizes the multimodal
understanding and reasoning capabilities of MLLM to accomplish attribute
prediction, sequencing and layout of layers. It also employs a diffusion model
to generate image content for assets. By enabling end-to-end training, IGD
architecturally supports scalability and extensibility in complex graphic
design tasks. The superior experimental results demonstrate that IGD offers a
new solution for graphic design.

</details>


### [211] [Crucial-Diff: A Unified Diffusion Model for Crucial Image and Annotation Synthesis in Data-scarce Scenarios](https://arxiv.org/abs/2507.09915)
*Siyue Yao,Mingjie Sun,Eng Gee Lim,Ran Yi,Baojiang Zhong,Moncef Gabbouj*

Main category: cs.CV

TL;DR: 论文提出Crucial-Diff框架，通过生成关键样本解决数据稀缺问题，提升检测和分割性能。


<details>
  <summary>Details</summary>
Motivation: 数据稀缺导致模型过拟合和数据集不平衡，现有生成模型生成的样本重复或简单，无法针对下游模型的弱点提供关键信息。

Method: 提出Crucial-Diff框架，包含场景无关特征提取器（SAFE）和弱点感知样本挖掘器（WASM），生成多样且高质量的样本。

Result: 在MVTec上达到83.63%的像素级AP和78.12%的F1-MAX；在息肉数据集上达到81.64%的mIoU和87.69%的mDice。

Conclusion: Crucial-Diff有效解决了数据稀缺问题，提升了模型性能。

Abstract: The scarcity of data in various scenarios, such as medical, industry and
autonomous driving, leads to model overfitting and dataset imbalance, thus
hindering effective detection and segmentation performance. Existing studies
employ the generative models to synthesize more training samples to mitigate
data scarcity. However, these synthetic samples are repetitive or simplistic
and fail to provide "crucial information" that targets the downstream model's
weaknesses. Additionally, these methods typically require separate training for
different objects, leading to computational inefficiencies. To address these
issues, we propose Crucial-Diff, a domain-agnostic framework designed to
synthesize crucial samples. Our method integrates two key modules. The Scene
Agnostic Feature Extractor (SAFE) utilizes a unified feature extractor to
capture target information. The Weakness Aware Sample Miner (WASM) generates
hard-to-detect samples using feedback from the detection results of downstream
model, which is then fused with the output of SAFE module. Together, our
Crucial-Diff framework generates diverse, high-quality training data, achieving
a pixel-level AP of 83.63% and an F1-MAX of 78.12% on MVTec. On polyp dataset,
Crucial-Diff reaches an mIoU of 81.64% and an mDice of 87.69%. Code will be
released after acceptance.

</details>


### [212] [EmbRACE-3K: Embodied Reasoning and Action in Complex Environments](https://arxiv.org/abs/2507.10548)
*Mingxian Lin,Wei Huang,Yitang Li,Chengjie Jiang,Kui Wu,Fangwei Zhong,Shengju Qian,Xin Wang,Xiaojuan Qi*

Main category: cs.CV

TL;DR: 论文提出了EmRACE-3K数据集，用于评估和改进视觉语言模型在具身环境中的推理能力，展示了当前模型的局限性及其改进潜力。


<details>
  <summary>Details</summary>
Motivation: 当前视觉语言模型在离线任务中表现优异，但在需要在线交互和主动场景理解的具身环境中表现有限。

Method: 通过构建EmRACE-3K数据集，包含3000多个语言引导任务，并利用监督学习和强化学习微调模型Qwen2.5-VL-7B。

Result: 零样本设置下模型成功率低于20%，但微调后性能显著提升。

Conclusion: EmRACE-3K为具身推理能力提供了有效评估基准，并展示了改进模型的潜力。

Abstract: Recent advanced vision-language models(VLMs) have demonstrated strong
performance on passive, offline image and video understanding tasks. However,
their effectiveness in embodied settings, which require online interaction and
active scene understanding remains limited. In such scenarios, an agent
perceives the environment from a first-person perspective, with each action
dynamically shaping subsequent observations. Even state-of-the-art models such
as GPT-4o, Claude 3.5 Sonnet, and Gemini 2.5 Pro struggle in open-environment
interactions, exhibiting clear limitations in spatial reasoning and
long-horizon planning. To address this gap, we introduce EmRACE-3K, a dataset
of over 3,000 language-guided tasks situated in diverse, photorealistic
environments constructed using Unreal Engine and the UnrealCV-Zoo framework.
The tasks encompass a wide range of embodied challenges, including navigation,
object manipulation, and multi-stage goal execution. Each task unfolds as a
multi-step trajectory, pairing first-person visual observations with high-level
instructions, grounded actions, and natural language rationales that express
the agent's intent at every step. Using EmRACE-3K, we establish a benchmark to
evaluate the embodied reasoning capabilities of VLMs across three key
dimensions: Exploration, Dynamic Spatial-Semantic Reasoning, and Multi-stage
Goal Execution. In zero-shot settings, all models achieve success rates below
20%, underscoring the challenge posed by our benchmark and the current
limitations of VLMs in interactive environments. To demonstrate the utility of
EmRACE-3K, we further fine-tune Qwen2.5-VL-7B using supervised learning
followed by reinforcement learning. This approach yields substantial
improvements across all three challenge categories, highlighting the dataset's
effectiveness in enabling the development of embodied reasoning capabilities.

</details>


### [213] [Can GPT-4o mini and Gemini 2.0 Flash Predict Fine-Grained Fashion Product Attributes? A Zero-Shot Analysis](https://arxiv.org/abs/2507.09950)
*Shubham Shukla,Kunal Sonalkar*

Main category: cs.CV

TL;DR: 论文评估了GPT-4o-mini和Gemini 2.0 Flash在时尚产品属性识别任务中的零样本性能，发现Gemini 2.0 Flash表现更优，但需领域特定微调。


<details>
  <summary>Details</summary>
Motivation: 探索大型语言模型（LLMs）在细粒度时尚属性识别中的表现，以提升电商产品目录管理和客户发现体验。

Method: 使用DeepFashion-MultiModal数据集，仅以图像为输入，评估模型在18类时尚属性上的表现。

Result: Gemini 2.0 Flash的宏F1分数为56.79%，优于GPT-4o-mini的43.28%。

Conclusion: LLMs在时尚属性识别中潜力显著，但需进一步领域微调，为未来时尚AI研究奠定基础。

Abstract: The fashion retail business is centered around the capacity to comprehend
products. Product attribution helps in comprehending products depending on the
business process. Quality attribution improves the customer experience as they
navigate through millions of products offered by a retail website. It leads to
well-organized product catalogs. In the end, product attribution directly
impacts the 'discovery experience' of the customer. Although large language
models (LLMs) have shown remarkable capabilities in understanding multimodal
data, their performance on fine-grained fashion attribute recognition remains
under-explored. This paper presents a zero-shot evaluation of state-of-the-art
LLMs that balance performance with speed and cost efficiency, mainly
GPT-4o-mini and Gemini 2.0 Flash. We have used the dataset
DeepFashion-MultiModal (https://github.com/yumingj/DeepFashion-MultiModal) to
evaluate these models in the attribution tasks of fashion products. Our study
evaluates these models across 18 categories of fashion attributes, offering
insight into where these models excel. We only use images as the sole input for
product information to create a constrained environment. Our analysis shows
that Gemini 2.0 Flash demonstrates the strongest overall performance with a
macro F1 score of 56.79% across all attributes, while GPT-4o-mini scored a
macro F1 score of 43.28%. Through detailed error analysis, our findings provide
practical insights for deploying these LLMs in production e-commerce product
attribution-related tasks and highlight the need for domain-specific
fine-tuning approaches. This work also lays the groundwork for future research
in fashion AI and multimodal attribute extraction.

</details>


### [214] [4D-MISR: A unified model for low-dose super-resolution imaging via feature fusion](https://arxiv.org/abs/2507.09953)
*Zifei Wang,Zian Mao,Xiaoya He,Xi Huang,Haoran Zhang,Chun Cheng,Shufen Chu,Tingzheng Hou,Xiaoqin Zeng,Yujun Xie*

Main category: cs.CV

TL;DR: 提出了一种基于多图像超分辨率（MISR）和卷积神经网络（CNN）的方法，用于在超低剂量条件下实现原子级分辨率的电子显微镜成像。


<details>
  <summary>Details</summary>
Motivation: 电子显微镜在原子分辨率下对光束敏感材料（如蛋白质和二维材料）的成像受到辐射损伤的限制。

Method: 结合多图像超分辨率技术和CNN，通过融合多个低分辨率、亚像素位移的图像，并利用合成多角度观测增强重建。

Result: 在超低剂量条件下实现了与传统ptychography相当的空间分辨率，适用于非晶、半晶和晶态光束敏感样品。

Conclusion: 该方法扩展了4D-STEM的能力，为辐射敏感材料的结构分析提供了一种通用且高效的新方法。

Abstract: While electron microscopy offers crucial atomic-resolution insights into
structure-property relationships, radiation damage severely limits its use on
beam-sensitive materials like proteins and 2D materials. To overcome this
challenge, we push beyond the electron dose limits of conventional electron
microscopy by adapting principles from multi-image super-resolution (MISR) that
have been widely used in remote sensing. Our method fuses multiple
low-resolution, sub-pixel-shifted views and enhances the reconstruction with a
convolutional neural network (CNN) that integrates features from synthetic,
multi-angle observations. We developed a dual-path, attention-guided network
for 4D-STEM that achieves atomic-scale super-resolution from ultra-low-dose
data. This provides robust atomic-scale visualization across amorphous,
semi-crystalline, and crystalline beam-sensitive specimens. Systematic
evaluations on representative materials demonstrate comparable spatial
resolution to conventional ptychography under ultra-low-dose conditions. Our
work expands the capabilities of 4D-STEM, offering a new and generalizable
method for the structural analysis of radiation-vulnerable materials.

</details>


### [215] [Uncertainty Quantification for Incomplete Multi-View Data Using Divergence Measures](https://arxiv.org/abs/2507.09980)
*Zhipeng Xue,Yan Zhang,Ming Li,Chun Li,Yue Liu,Fei Yu*

Main category: cs.CV

TL;DR: KPHD-Net提出了一种基于Hölder散度的多视图分类和聚类方法，通过结合Dempster-Shafer证据理论和Kalman滤波器，提高了多视图融合的可靠性和性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法通常使用KL散度估计不确定性，但忽略了模态间的领域差异，导致多视图集成和决策的可靠性不足。

Method: KPHD-Net采用变分Dirichlet分布表示类别概率分布，结合Hölder散度和Dempster-Shafer证据理论，并引入Kalman滤波器进行未来状态估计。

Result: 实验表明，KPHD-Net在分类和聚类任务中优于现有方法，具有更高的准确性、鲁棒性和可靠性。

Conclusion: KPHD-Net通过改进不确定性估计和多视图融合方法，显著提升了多视图学习的性能。

Abstract: Existing multi-view classification and clustering methods typically improve
task accuracy by leveraging and fusing information from different views.
However, ensuring the reliability of multi-view integration and final decisions
is crucial, particularly when dealing with noisy or corrupted data. Current
methods often rely on Kullback-Leibler (KL) divergence to estimate uncertainty
of network predictions, ignoring domain gaps between different modalities. To
address this issue, KPHD-Net, based on H\"older divergence, is proposed for
multi-view classification and clustering tasks. Generally, our KPHD-Net employs
a variational Dirichlet distribution to represent class probability
distributions, models evidences from different views, and then integrates it
with Dempster-Shafer evidence theory (DST) to improve uncertainty estimation
effects. Our theoretical analysis demonstrates that Proper H\"older divergence
offers a more effective measure of distribution discrepancies, ensuring
enhanced performance in multi-view learning. Moreover, Dempster-Shafer evidence
theory, recognized for its superior performance in multi-view fusion tasks, is
introduced and combined with the Kalman filter to provide future state
estimations. This integration further enhances the reliability of the final
fusion results. Extensive experiments show that the proposed KPHD-Net
outperforms the current state-of-the-art methods in both classification and
clustering tasks regarding accuracy, robustness, and reliability, with
theoretical guarantees.

</details>


### [216] [Latent Diffusion Models with Masked AutoEncoders](https://arxiv.org/abs/2507.09984)
*Junho Lee,Jeongwoo Shin,Hyungwook Choi,Joonseok Lee*

Main category: cs.CV

TL;DR: 论文分析了潜在扩散模型（LDMs）中自编码器的关键属性，提出了一种新的变分掩码自编码器（VMAEs），并将其整合为LDMAEs，显著提升了图像生成质量和计算效率。


<details>
  <summary>Details</summary>
Motivation: 尽管潜在扩散模型在图像生成方面具有巨大潜力，但自编码器的理想属性和最优设计尚未充分探索。

Method: 提出变分掩码自编码器（VMAEs），利用掩码自编码器的分层特征，并将其整合为LDMAEs。

Result: 通过实验证明，LDMAEs显著提升了图像生成质量和计算效率。

Conclusion: VMAEs能够同时满足潜在平滑性、感知压缩质量和重建质量三个关键属性，为潜在扩散模型提供了更优的自编码器设计。

Abstract: In spite of remarkable potential of the Latent Diffusion Models (LDMs) in
image generation, the desired properties and optimal design of the autoencoders
have been underexplored. In this work, we analyze the role of autoencoders in
LDMs and identify three key properties: latent smoothness, perceptual
compression quality, and reconstruction quality. We demonstrate that existing
autoencoders fail to simultaneously satisfy all three properties, and propose
Variational Masked AutoEncoders (VMAEs), taking advantage of the hierarchical
features maintained by Masked AutoEncoder. We integrate VMAEs into the LDM
framework, introducing Latent Diffusion Models with Masked AutoEncoders
(LDMAEs). Through comprehensive experiments, we demonstrate significantly
enhanced image generation quality and computational efficiency.

</details>


### [217] [3DGAA: Realistic and Robust 3D Gaussian-based Adversarial Attack for Autonomous Driving](https://arxiv.org/abs/2507.09993)
*Yixun Zhang,Lizhi Wang,Junjun Zhao,Wending Zhao,Feng Zhou,Yonghao Dang,Jianqin Yin*

Main category: cs.CV

TL;DR: 本文提出了一种基于3D高斯对抗攻击（3DGAA）的新方法，通过联合优化几何和外观属性，生成物理上可实现的对抗性物体，显著提升了攻击效果。


<details>
  <summary>Details</summary>
Motivation: 现有的2D和3D物理攻击方法在平衡物理真实性和攻击鲁棒性方面存在不足，因此需要一种更有效的对抗性物体生成框架。

Method: 3DGAA利用3D高斯泼溅（3DGS）的14维参数化，联合扰动几何属性（形状、尺度、旋转）和外观属性（颜色、不透明度），并引入物理过滤和增强模块以提升攻击的泛化能力。

Result: 实验表明，3DGAA将检测mAP从87.21%降至7.38%，显著优于现有3D物理攻击方法，并在不同物理条件下保持高迁移性。

Conclusion: 3DGAA为评估自动驾驶感知系统的安全性提供了一种实用的攻击框架，展示了物理可实现的对抗攻击的新进展。

Abstract: Camera-based object detection systems play a vital role in autonomous
driving, yet they remain vulnerable to adversarial threats in real-world
environments. While existing 2D and 3D physical attacks typically optimize
texture, they often struggle to balance physical realism and attack robustness.
In this work, we propose 3D Gaussian-based Adversarial Attack (3DGAA), a novel
adversarial object generation framework that leverages the full 14-dimensional
parameterization of 3D Gaussian Splatting (3DGS) to jointly optimize geometry
and appearance in physically realizable ways. Unlike prior works that rely on
patches or texture, 3DGAA jointly perturbs both geometric attributes (shape,
scale, rotation) and appearance attributes (color, opacity) to produce
physically realistic and transferable adversarial objects. We further introduce
a physical filtering module to preserve geometric fidelity, and a physical
augmentation module to simulate complex physical scenarios, thus enhancing
attack generalization under real-world conditions. We evaluate 3DGAA on both
virtual benchmarks and physical-world setups using miniature vehicle models.
Experimental results show that 3DGAA achieves to reduce the detection mAP from
87.21% to 7.38%, significantly outperforming existing 3D physical attacks.
Moreover, our method maintains high transferability across different physical
conditions, demonstrating a new state-of-the-art in physically realizable
adversarial attacks. These results validate 3DGAA as a practical attack
framework for evaluating the safety of perception systems in autonomous
driving.

</details>


### [218] [Leveraging Swin Transformer for enhanced diagnosis of Alzheimer's disease using multi-shell diffusion MRI](https://arxiv.org/abs/2507.09996)
*Quentin Dessain,Nicolas Delinte,Bernard Hanseeuw,Laurence Dricot,Benoît Macq*

Main category: cs.CV

TL;DR: 利用多壳层扩散MRI数据和视觉Transformer框架，支持阿尔茨海默病的早期诊断和淀粉样蛋白积累检测。


<details>
  <summary>Details</summary>
Motivation: 通过多壳层扩散MRI数据的微观结构信息，结合深度学习，实现阿尔茨海默病和淀粉样蛋白的早期诊断。

Method: 采用Swin Transformer模型对多壳层dMRI数据进行分类，结合DTI和NODDI指标，并通过低秩适应优化模型。

Result: 在阿尔茨海默病分类中达到95.2%的平衡准确率，淀粉样蛋白检测中最高77.2%的准确率，并识别出关键脑区。

Conclusion: 扩散MRI和Transformer架构在阿尔茨海默病早期诊断中具有潜力，支持数据有限的生物医学诊断。

Abstract: Objective: This study aims to support early diagnosis of Alzheimer's disease
and detection of amyloid accumulation by leveraging the microstructural
information available in multi-shell diffusion MRI (dMRI) data, using a vision
transformer-based deep learning framework.
  Methods: We present a classification pipeline that employs the Swin
Transformer, a hierarchical vision transformer model, on multi-shell dMRI data
for the classification of Alzheimer's disease and amyloid presence. Key metrics
from DTI and NODDI were extracted and projected onto 2D planes to enable
transfer learning with ImageNet-pretrained models. To efficiently adapt the
transformer to limited labeled neuroimaging data, we integrated Low-Rank
Adaptation. We assessed the framework on diagnostic group prediction
(cognitively normal, mild cognitive impairment, Alzheimer's disease dementia)
and amyloid status classification.
  Results: The framework achieved competitive classification results within the
scope of multi-shell dMRI-based features, with the best balanced accuracy of
95.2% for distinguishing cognitively normal individuals from those with
Alzheimer's disease dementia using NODDI metrics. For amyloid detection, it
reached 77.2% balanced accuracy in distinguishing amyloid-positive mild
cognitive impairment/Alzheimer's disease dementia subjects from
amyloid-negative cognitively normal subjects, and 67.9% for identifying
amyloid-positive individuals among cognitively normal subjects. Grad-CAM-based
explainability analysis identified clinically relevant brain regions, including
the parahippocampal gyrus and hippocampus, as key contributors to model
predictions.
  Conclusion: This study demonstrates the promise of diffusion MRI and
transformer-based architectures for early detection of Alzheimer's disease and
amyloid pathology, supporting biomarker-driven diagnostics in data-limited
biomedical settings.

</details>


### [219] [Vision-Based Anti Unmanned Aerial Technology: Opportunities and Challenges](https://arxiv.org/abs/2507.10006)
*Guanghai Ding,Yihua Ren,Yuting Liu,Qijun Zhao,Shuiwang Li*

Main category: cs.CV

TL;DR: 论文综述了反无人机跟踪技术的现状与挑战，整理了公开数据集，分析了近年来的视觉和视觉融合算法，并提出了未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 随着无人机技术的快速发展和广泛应用，反无人机跟踪在公共安全等复杂环境中的重要性日益凸显。

Method: 回顾反无人机检测与跟踪技术的特点和挑战，整理公开数据集，分析视觉和视觉融合算法。

Result: 提供了数据集链接和算法分析，为研究者提供了支持。

Conclusion: 论文总结了当前研究，并提出了未来研究方向，以推动领域发展。

Abstract: With the rapid advancement of UAV technology and its extensive application in
various fields such as military reconnaissance, environmental monitoring, and
logistics, achieving efficient and accurate Anti-UAV tracking has become
essential. The importance of Anti-UAV tracking is increasingly prominent,
especially in scenarios such as public safety, border patrol, search and
rescue, and agricultural monitoring, where operations in complex environments
can provide enhanced security. Current mainstream Anti-UAV tracking
technologies are primarily centered around computer vision techniques,
particularly those that integrate multi-sensor data fusion with advanced
detection and tracking algorithms. This paper first reviews the characteristics
and current challenges of Anti-UAV detection and tracking technologies. Next,
it investigates and compiles several publicly available datasets, providing
accessible links to support researchers in efficiently addressing related
challenges. Furthermore, the paper analyzes the major vision-based and
vision-fusion-based Anti-UAV detection and tracking algorithms proposed in
recent years. Finally, based on the above research, this paper outlines future
research directions, aiming to provide valuable insights for advancing the
field.

</details>


### [220] [Binomial Self-Compensation: Mechanism and Suppression of Motion Error in Phase-Shifting Profilometry](https://arxiv.org/abs/2507.10009)
*Geyou Zhang,Kai Liu,Ce Zhu*

Main category: cs.CV

TL;DR: 提出了一种基于图像序列的二项式自补偿方法（I-BSC），用于解决动态测量中相位偏移轮廓测量（PSP）的运动误差问题，显著降低了计算复杂度和误差累积。


<details>
  <summary>Details</summary>
Motivation: PSP在动态测量中因物体运动导致误差，现有方法P-BSC虽有效但计算复杂且误差累积严重。

Method: I-BSC通过加权求和同质条纹图像而非相位帧，仅需一次反正切计算，降低了计算复杂度。

Result: I-BSC在减少运动误差和计算效率上优于现有方法，实现了准单帧速率的高分辨率3D重建。

Conclusion: I-BSC显著提升了动态测量的效率和精度，为高分辨率3D扫描提供了新思路。

Abstract: Phase shifting profilometry (PSP) is widely used in high-precision 3D
scanning due to its high accuracy, robustness, and pixel-wise handling.
However, a fundamental assumption of PSP that the object should remain static
does not hold in dynamic measurement, making PSP susceptible to object motion.
To address this challenge, our proposed solution, phase-sequential binomial
self-compensation (P-BSC), sums successive motion-affected phase frames
weighted by binomial coefficients. This approach exponentially reduces the
motion error in a pixel-wise and frame-wise loopable manner. Despite its
efficacy, P-BSC suffers from high computational overhead and error accumulation
due to its reliance on multi-frame phase calculations and weighted summations.
Inspired by P-BSC, we propose an image-sequential binomial self-compensation
(I-BSC) to weight sum the homogeneous fringe images instead of successive phase
frames, which generalizes the BSC concept from phase sequences to image
sequences. I-BSC computes the arctangent function only once, resolving both
limitations in P-BSC. Extensive analysis, simulations, and experiments show
that 1) the proposed BSC outperforms existing methods in reducing motion error
while achieving a quasi-single-shot frame rate, i.e., depth map frame rate
equals to the camera's acquisition rate, enabling 3D reconstruction with high
pixel-depth-temporal resolution; 2) compared to P-BSC, our I-BSC reduces the
computational complexity by one polynomial order, thereby accelerating the
computational frame rate by several to dozen times, while also reaching faster
motion error convergence.

</details>


### [221] [(Almost) Free Modality Stitching of Foundation Models](https://arxiv.org/abs/2507.10015)
*Jaisidh Singh,Diganta Misra,Boris Knyazev,Antonio Orvieto*

Main category: cs.CV

TL;DR: Hyma提出了一种基于超网络的多模态模型对齐方法，显著降低了最优单模态模型选择和连接器训练的计算成本。


<details>
  <summary>Details</summary>
Motivation: 多模态基础模型通常通过拼接预训练的单模态模型构建，但选择和训练连接器模块的计算成本高昂。

Method: 利用超网络的参数预测能力，为N×M种单模态模型组合联合训练连接器模块。

Result: Hyma将最优单模态模型对搜索成本降低10倍，同时在多模态基准测试中匹配网格搜索的性能。

Conclusion: Hyma是一种高效的全方位解决方案，显著提升了多模态模型构建的效率。

Abstract: Foundation multi-modal models are often designed by stitching of multiple
existing pretrained uni-modal models: for example, an image classifier with an
autoregressive text model. This stitching process is performed by training a
connector module that aims to align the representation-representation or
representation-input spaces of these uni-modal models. However, given the
complexity of training such connectors on large scale web-based datasets
coupled with the ever-increasing number of available pretrained uni-modal
models, the task of uni-modal models selection and subsequent connector module
training becomes computationally demanding. To address this under-studied
critical problem, we propose Hypernetwork Model Alignment (Hyma), a novel
all-in-one solution for optimal uni-modal model selection and connector
training by leveraging hypernetworks. Specifically, our framework utilizes the
parameter prediction capability of a hypernetwork to obtain jointly trained
connector modules for $N \times M$ combinations of uni-modal models. In our
experiments, Hyma reduces the optimal uni-modal model pair search cost by
$10\times$ (averaged across all experiments), while matching the ranking and
trained connector performance obtained via grid search across a suite of
diverse multi-modal benchmarks.

</details>


### [222] [Memory-Efficient Personalization of Text-to-Image Diffusion Models via Selective Optimization Strategies](https://arxiv.org/abs/2507.10029)
*Seokeon Choi,Sunghyun Park,Hyoungwoo Park,Jeongho Kim,Sungrack Yun*

Main category: cs.CV

TL;DR: 提出了一种选择性优化框架，结合低分辨率图像的反向传播（BP-low）和高分辨率图像的零阶优化（ZO-high），以实现内存高效且高质量的文本到图像扩散模型个性化。


<details>
  <summary>Details</summary>
Motivation: 解决在边缘设备上适应文本到图像扩散模型时的内存效率和隐私保护问题。

Method: 通过时间步感知的概率函数动态选择BP-low或ZO-high优化策略，结合两者的优势。

Result: 实验表明，该方法在显著降低内存消耗的同时保持了高质量，适用于设备端个性化。

Conclusion: 该框架实现了内存高效且高质量的模型个性化，适用于资源有限的设备。

Abstract: Memory-efficient personalization is critical for adapting text-to-image
diffusion models while preserving user privacy and operating within the limited
computational resources of edge devices. To this end, we propose a selective
optimization framework that adaptively chooses between backpropagation on
low-resolution images (BP-low) and zeroth-order optimization on high-resolution
images (ZO-high), guided by the characteristics of the diffusion process. As
observed in our experiments, BP-low efficiently adapts the model to
target-specific features, but suffers from structural distortions due to
resolution mismatch. Conversely, ZO-high refines high-resolution details with
minimal memory overhead but faces slow convergence when applied without prior
adaptation. By complementing both methods, our framework leverages BP-low for
effective personalization while using ZO-high to maintain structural
consistency, achieving memory-efficient and high-quality fine-tuning. To
maximize the efficacy of both BP-low and ZO-high, we introduce a timestep-aware
probabilistic function that dynamically selects the appropriate optimization
strategy based on diffusion timesteps. This function mitigates the overfitting
from BP-low at high timesteps, where structural information is critical, while
ensuring ZO-high is applied more effectively as training progresses.
Experimental results demonstrate that our method achieves competitive
performance while significantly reducing memory consumption, enabling scalable,
high-quality on-device personalization without increasing inference latency.

</details>


### [223] [CoSMo: A Multimodal Transformer for Page Stream Segmentation in Comic Books](https://arxiv.org/abs/2507.10053)
*Marc Serra Ortega,Emanuele Vivoli,Artemis Llabrés,Dimosthenis Karatzas*

Main category: cs.CV

TL;DR: CoSMo是一种新型多模态Transformer，用于漫画书的页面流分割（PSS），在视觉和多模态版本中均优于传统基线和大规模通用视觉语言模型。


<details>
  <summary>Details</summary>
Motivation: 漫画书的页面流分割是自动化内容理解的关键任务，为下游任务（如角色分析、故事索引或元数据丰富）提供基础。

Method: 提出CoSMo模型，开发视觉和多模态版本，并在20,800页的标注数据集上进行验证。

Result: CoSMo在F1-Macro、Panoptic Quality和流级指标上均显著优于基线模型，视觉特征在宏观结构上占主导，但多模态有助于解决模糊问题。

Conclusion: CoSMo为漫画书分析设立了新的技术标杆，支持可扩展的自动化分析。

Abstract: This paper introduces CoSMo, a novel multimodal Transformer for Page Stream
Segmentation (PSS) in comic books, a critical task for automated content
understanding, as it is a necessary first stage for many downstream tasks like
character analysis, story indexing, or metadata enrichment. We formalize PSS
for this unique medium and curate a new 20,800-page annotated dataset. CoSMo,
developed in vision-only and multimodal variants, consistently outperforms
traditional baselines and significantly larger general-purpose vision-language
models across F1-Macro, Panoptic Quality, and stream-level metrics. Our
findings highlight the dominance of visual features for comic PSS
macro-structure, yet demonstrate multimodal benefits in resolving challenging
ambiguities. CoSMo establishes a new state-of-the-art, paving the way for
scalable comic book analysis.

</details>


### [224] [Lightweight Model for Poultry Disease Detection from Fecal Images Using Multi-Color Space Feature Optimization and Machine Learning](https://arxiv.org/abs/2507.10056)
*A. K. M. Shoriful Islam,Md. Rakib Hassan,Macbah Uddin,Md. Shahidur Rahman*

Main category: cs.CV

TL;DR: 提出一种轻量级机器学习方法，通过分析家禽粪便图像检测疾病，准确率达95.85%，计算效率高。


<details>
  <summary>Details</summary>
Motivation: 家禽养殖易受传染病影响，需低成本、高效的疾病检测方法。

Method: 多颜色空间特征提取（RGB、HSV、LAB），结合颜色、纹理和形状描述符，使用PCA和XGBoost降维，训练ANN分类器。

Result: 模型准确率95.85%，无需GPU，执行时间638秒，资源消耗远低于深度学习模型。

Conclusion: 该方法为低资源农业环境提供了一种高效、可解释的疾病检测替代方案。

Abstract: Poultry farming is a vital component of the global food supply chain, yet it
remains highly vulnerable to infectious diseases such as coccidiosis,
salmonellosis, and Newcastle disease. This study proposes a lightweight machine
learning-based approach to detect these diseases by analyzing poultry fecal
images. We utilize multi-color space feature extraction (RGB, HSV, LAB) and
explore a wide range of color, texture, and shape-based descriptors, including
color histograms, local binary patterns (LBP), wavelet transforms, and edge
detectors. Through a systematic ablation study and dimensionality reduction
using PCA and XGBoost feature selection, we identify a compact global feature
set that balances accuracy and computational efficiency. An artificial neural
network (ANN) classifier trained on these features achieved 95.85% accuracy
while requiring no GPU and only 638 seconds of execution time in Google Colab.
Compared to deep learning models such as Xception and MobileNetV3, our proposed
model offers comparable accuracy with drastically lower resource usage. This
work demonstrates a cost-effective, interpretable, and scalable alternative to
deep learning for real-time poultry disease detection in low-resource
agricultural settings.

</details>


### [225] [MoVieS: Motion-Aware 4D Dynamic View Synthesis in One Second](https://arxiv.org/abs/2507.10065)
*Chenguo Lin,Yuchen Lin,Panwang Pan,Yifan Yu,Honglei Yan,Katerina Fragkiadaki,Yadong Mu*

Main category: cs.CV

TL;DR: MoVieS是一个新颖的前馈模型，能够在一秒内从单目视频合成4D动态新视角。它通过高斯基元的像素对齐网格表示动态3D场景，并显式监督其随时间变化的运动，首次实现了外观、几何和运动的统一建模。


<details>
  <summary>Details</summary>
Motivation: 传统方法难以统一建模动态场景的外观、几何和运动，且依赖任务特定的监督。MoVieS旨在通过一个学习框架实现这些目标，并支持零样本应用。

Method: MoVieS使用像素对齐的高斯基元网格表示动态3D场景，显式监督时间变化运动，实现外观、几何和运动的统一建模。

Result: 实验验证了MoVieS的高效性和有效性，在多个任务中表现优异，同时实现了数量级的速度提升。

Conclusion: MoVieS通过统一建模动态场景的外观、几何和运动，支持多种零样本应用，为动态场景分析提供了高效解决方案。

Abstract: We present MoVieS, a novel feed-forward model that synthesizes 4D dynamic
novel views from monocular videos in one second. MoVieS represents dynamic 3D
scenes using pixel-aligned grids of Gaussian primitives, explicitly supervising
their time-varying motion. This allows, for the first time, the unified
modeling of appearance, geometry and motion, and enables view synthesis,
reconstruction and 3D point tracking within a single learning-based framework.
By bridging novel view synthesis with dynamic geometry reconstruction, MoVieS
enables large-scale training on diverse datasets with minimal dependence on
task-specific supervision. As a result, it also naturally supports a wide range
of zero-shot applications, such as scene flow estimation and moving object
segmentation. Extensive experiments validate the effectiveness and efficiency
of MoVieS across multiple tasks, achieving competitive performance while
offering several orders of magnitude speedups.

</details>


### [226] [Frequency Regulation for Exposure Bias Mitigation in Diffusion Models](https://arxiv.org/abs/2507.10072)
*Meng Yu,Kun Zhan*

Main category: cs.CV

TL;DR: 论文提出了一种基于频率域调节的扩散模型改进方法，通过小波变换分别调整低频和高频子带，显著提升了生成质量并解决了曝光偏差问题。


<details>
  <summary>Details</summary>
Motivation: 扩散模型在生成能力上表现出色，但受到曝光偏差的显著影响。作者观察到预测噪声图像的能量在扩散过程中下降，并发现这种下降在低频和高频子带中有不同模式。

Method: 利用小波变换引入频率域调节机制，分别调整低频和高频子带，并结合对曝光偏差的更准确分析。

Result: 该方法无需训练且即插即用，显著提升了多种扩散模型的生成质量，并为不同架构的模型提供了解决曝光偏差的稳健方案。

Conclusion: 通过频率域调节机制，论文有效解决了扩散模型中的曝光偏差问题，提升了生成能力。

Abstract: Diffusion models exhibit impressive generative capabilities but are
significantly impacted by exposure bias. In this paper, we make a key
observation: the energy of the predicted noisy images decreases during the
diffusion process. Building on this, we identify two important findings: 1) The
reduction in energy follows distinct patterns in the low-frequency and
high-frequency subbands; 2) This energy reduction results in amplitude
variations between the network-reconstructed clean data and the real clean
data. Based on the first finding, we introduce a frequency-domain regulation
mechanism utilizing wavelet transforms, which separately adjusts the low- and
high-frequency subbands. Leveraging the second insight, we provide a more
accurate analysis of exposure bias in the two subbands. Our method is
training-free and plug-and-play, significantly improving the generative quality
of various diffusion models and providing a robust solution to exposure bias
across different model architectures. The source code is available at
https://github.com/kunzhan/wpp.

</details>


### [227] [A Transfer Learning-Based Method for Water Body Segmentation in Remote Sensing Imagery: A Case Study of the Zhada Tulin Area](https://arxiv.org/abs/2507.10084)
*Haonan Chen,Xin Tong*

Main category: cs.CV

TL;DR: 提出了一种基于SegFormer模型的两阶段迁移学习策略，显著提升了遥感图像水体分割任务在目标域的性能。


<details>
  <summary>Details</summary>
Motivation: 解决遥感图像水体分割中域偏移和小样本量的挑战。

Method: 采用两阶段迁移学习策略：先在源域训练基础分割模型，再在目标域微调。

Result: 在西藏札达土林地区，IoU从25.50%提升至64.84%。

Conclusion: 该方法有效解决了域差异导致的性能下降，为数据稀缺且环境独特的遥感场景提供了高精度信息提取的技术范例。

Abstract: To address the prevalent challenges of domain shift and small sample sizes in
remote sensing image water body segmentation, this study proposes and validates
a two-stage transfer learning strategy based on the SegFormer model. The
approach begins by training a foundational segmentation model on a diverse
source domain, where it achieves an Intersection over Union (IoU) of 68.80% on
its validation set, followed by fine-tuning on data from the distinct target
domain. Focusing on the Zhada Tulin area in Tibet -- a region characterized by
highly complex topography and spectral features -- the experimental results
demonstrate that this strategy significantly boosts the IoU for the water body
segmentation task from 25.50% (for direct transfer) to 64.84%. This not only
effectively resolves the model performance degradation caused by domain
discrepancy but also provides an effective technical paradigm for
high-precision thematic information extraction in data-scarce and
environmentally unique remote sensing scenarios.

</details>


### [228] [FIX-CLIP: Dual-Branch Hierarchical Contrastive Learning via Synthetic Captions for Better Understanding of Long Text](https://arxiv.org/abs/2507.10095)
*Bingchao Wang,Zhiwei Ning,Jianyu Ding,Xuanang Gao,Yin Li,Dongsheng Jiang,Jie Yang,Wei Liu*

Main category: cs.CV

TL;DR: FIX-CLIP通过双分支训练、区域提示和层次特征对齐模块，解决了CLIP在长文本任务中的限制，并在长短文本检索任务中取得最优性能。


<details>
  <summary>Details</summary>
Motivation: CLIP在短文本任务中表现优异，但受限于输入长度，无法有效处理长文本任务。

Method: 提出FIX-CLIP，包括双分支训练、区域提示和层次特征对齐模块，并利用合成数据训练。

Result: 在长短文本检索任务中达到最优性能，且适用于扩散模型的长文本输入。

Conclusion: FIX-CLIP有效扩展了CLIP的能力，适用于更广泛的任务。

Abstract: CLIP has shown promising performance across many short-text tasks in a
zero-shot manner. However, limited by the input length of the text encoder,
CLIP struggles on under-stream tasks with long-text inputs (>77 tokens). To
remedy this issue, we propose FIX-CLIP which includes three novel modules: (1)
A dual-branch training pipeline that aligns short and long texts with masked
and raw images respectively, which boosts the long-text representation while
preserving the short-text ability. (2) Multiple learnable regional prompts with
unidirectional masks in Transformer layers for regional information extraction.
(3) A hierarchical feature alignment module in the intermediate encoder layers
to promote the consistency of multi-scale features. Furthermore, we collect 30M
images and utilize existing MLLMs to synthesize long-text captions for
training. Extensive experiments show that FIX-CLIP achieves state-of-the-art
performance on both long-text and short-text retrieval benchmarks. For
downstream applications, we reveal that FIX-CLIP's text encoder delivers
promising performance in a plug-and-play manner for diffusion models with
long-text input.

</details>


### [229] [Glance-MCMT: A General MCMT Framework with Glance Initialization and Progressive Association](https://arxiv.org/abs/2507.10115)
*Hamidreza Hashempoor*

Main category: cs.CV

TL;DR: 提出了一种多摄像头多目标跟踪框架，通过轨迹和外观线索实现全局身份一致性分配。


<details>
  <summary>Details</summary>
Motivation: 解决多摄像头场景下目标跟踪中全局身份分配一致性的问题。

Method: 采用BoT-SORT单摄像头跟踪，通过轨迹特征匹配初始化全局ID，后续帧使用优先全局匹配策略，结合3D位置估计进行空间验证。

Result: 实现了多摄像头场景下目标跟踪的全局身份一致性。

Conclusion: 该框架有效解决了多摄像头多目标跟踪中的身份分配问题。

Abstract: We propose a multi-camera multi-target (MCMT) tracking framework that ensures
consistent global identity assignment across views using trajectory and
appearance cues. The pipeline starts with BoT-SORT-based single-camera
tracking, followed by an initial glance phase to initialize global IDs via
trajectory-feature matching. In later frames, new tracklets are matched to
existing global identities through a prioritized global matching strategy. New
global IDs are only introduced when no sufficiently similar trajectory or
feature match is found. 3D positions are estimated using depth maps and
calibration for spatial validation.

</details>


### [230] [DEARLi: Decoupled Enhancement of Recognition and Localization for Semi-supervised Panoptic Segmentation](https://arxiv.org/abs/2507.10118)
*Ivan Martinović,Josip Šarić,Marin Oršić,Matej Kristan,Siniša Šegvić*

Main category: cs.CV

TL;DR: 论文提出了一种名为DEARLi的半监督全景分割方法，通过结合两个专用基础模型（CLIP和SAM）来增强识别和定位能力，显著提升了小样本标注数据下的性能。


<details>
  <summary>Details</summary>
Motivation: 像素级标注成本高昂且耗时，半监督分割方法旨在利用少量标注数据和大量未标注数据训练模型。基础模型（如CLIP和SAM）的潜力尚未被充分挖掘，论文旨在填补这一空白。

Method: 提出DEARLi方法，通过结合CLIP的零样本分类能力和SAM的伪标签进行类无关解码器预热，分别增强识别和定位能力。

Result: 在ADE20K数据集上，仅使用158张标注图像，DEARLi达到了29.9 PQ和38.9 mIoU，性能显著优于现有方法，且GPU内存需求降低8倍。

Conclusion: DEARLi在识别和定位的解耦增强方面表现优异，尤其在标注数据稀缺和大分类任务中，为半监督分割提供了高效解决方案。

Abstract: Pixel-level annotation is expensive and time-consuming. Semi-supervised
segmentation methods address this challenge by learning models on few labeled
images alongside a large corpus of unlabeled images. Although foundation models
could further account for label scarcity, effective mechanisms for their
exploitation remain underexplored. We address this by devising a novel
semi-supervised panoptic approach fueled by two dedicated foundation models. We
enhance recognition by complementing unsupervised mask-transformer consistency
with zero-shot classification of CLIP features. We enhance localization by
class-agnostic decoder warm-up with respect to SAM pseudo-labels. The resulting
decoupled enhancement of recognition and localization (DEARLi) particularly
excels in the most challenging semi-supervised scenarios with large taxonomies
and limited labeled data. Moreover, DEARLi outperforms the state of the art in
semi-supervised semantic segmentation by a large margin while requiring 8x less
GPU memory, in spite of being trained only for the panoptic objective. We
observe 29.9 PQ and 38.9 mIoU on ADE20K with only 158 labeled images. The
source code is available at https://github.com/helen1c/DEARLi.

</details>


### [231] [Taming Modern Point Tracking for Speckle Tracking Echocardiography via Impartial Motion](https://arxiv.org/abs/2507.10127)
*Md Abulkalam Azad,John Nyberg,Håvard Dalen,Bjørnar Grenne,Lasse Lovstakken,Andreas Østvik*

Main category: cs.CV

TL;DR: 论文探讨了现代点跟踪方法在超声心动图中的应用，通过改进训练策略和提出轻量级网络，显著提升了运动估计的准确性和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 传统方法在复杂心脏运动估计中表现不佳，现代点跟踪方法在超声心动图中的应用尚未充分探索，因此研究其潜力并改进现有方法。

Method: 分析了心脏运动的方向性偏差，改进了训练策略并引入定制化增强方法，提出了一种轻量级网络，利用多尺度成本体积提升性能。

Result: 实验显示，改进后的方法显著提升了模型性能，如EchoTracker在位置准确性和轨迹误差上分别提升了60.7%和61.5%。

Conclusion: 尽管某些点跟踪模型表现不佳，但改进后的方法在临床评估中显示出更好的可重复性，与专家验证的工具更接近。

Abstract: Accurate motion estimation for tracking deformable tissues in
echocardiography is essential for precise cardiac function measurements. While
traditional methods like block matching or optical flow struggle with intricate
cardiac motion, modern point tracking approaches remain largely underexplored
in this domain. This work investigates the potential of state-of-the-art (SOTA)
point tracking methods for ultrasound, with a focus on echocardiography.
Although these novel approaches demonstrate strong performance in general
videos, their effectiveness and generalizability in echocardiography remain
limited. By analyzing cardiac motion throughout the heart cycle in real B-mode
ultrasound videos, we identify that a directional motion bias across different
views is affecting the existing training strategies. To mitigate this, we
refine the training procedure and incorporate a set of tailored augmentations
to reduce the bias and enhance tracking robustness and generalization through
impartial cardiac motion. We also propose a lightweight network leveraging
multi-scale cost volumes from spatial context alone to challenge the advanced
spatiotemporal point tracking models. Experiments demonstrate that fine-tuning
with our strategies significantly improves models' performances over their
baselines, even for out-of-distribution (OOD) cases. For instance, EchoTracker
boosts overall position accuracy by 60.7% and reduces median trajectory error
by 61.5% across heart cycle phases. Interestingly, several point tracking
models fail to outperform our proposed simple model in terms of tracking
accuracy and generalization, reflecting their limitations when applied to
echocardiography. Nevertheless, clinical evaluation reveals that these methods
improve GLS measurements, aligning more closely with expert-validated,
semi-automated tools and thus demonstrating better reproducibility in
real-world applications.

</details>


### [232] [Deep Recurrence for Dynamical Segmentation Models](https://arxiv.org/abs/2507.10143)
*David Calhas,Arlindo L. Oliveira*

Main category: cs.CV

TL;DR: 论文提出了一种受预测编码启发的反馈机制，通过循环连接优化模型内部状态，显著提升了在噪声条件下的性能和数据效率。


<details>
  <summary>Details</summary>
Motivation: 生物视觉系统依赖反馈连接迭代优化感知，而人工神经网络多为前馈结构。本文旨在探索反馈机制在提升模型鲁棒性和数据效率方面的潜力。

Method: 在标准U-Net架构中引入反馈循环，结合软最大投影和指数衰减操作以确保稳定性。

Result: 反馈模型在噪声条件下表现显著优于前馈模型，且仅需两个训练样本即可达到随机性能，而前馈模型需至少四个样本。

Conclusion: 反馈机制增强了模型的鲁棒性和数据效率，为更适应性和生物启发的神经网络架构提供了方向。

Abstract: While biological vision systems rely heavily on feedback connections to
iteratively refine perception, most artificial neural networks remain purely
feedforward, processing input in a single static pass. In this work, we propose
a predictive coding inspired feedback mechanism that introduces a recurrent
loop from output to input, allowing the model to refine its internal state over
time. We implement this mechanism within a standard U-Net architecture and
introduce two biologically motivated operations, softmax projection and
exponential decay, to ensure stability of the feedback loop. Through controlled
experiments on a synthetic segmentation task, we show that the feedback model
significantly outperforms its feedforward counterpart in noisy conditions and
generalizes more effectively with limited supervision. Notably, feedback
achieves above random performance with just two training examples, while the
feedforward model requires at least four. Our findings demonstrate that
feedback enhances robustness and data efficiency, and offer a path toward more
adaptive and biologically inspired neural architectures. Code is available at:
github.com/DCalhas/feedback_segmentation.

</details>


### [233] [SlumpGuard: An AI-Powered Real-Time System for Automated Concrete Slump Prediction via Video Analysis](https://arxiv.org/abs/2507.10171)
*Youngmin Kim,Giyeong Oh,Kwangsoo Youm,Youngjae Yu*

Main category: cs.CV

TL;DR: SlumpGuard是一种基于AI的视频系统，用于实时自动评估混凝土的工作性，解决了传统坍落度测试的不足。


<details>
  <summary>Details</summary>
Motivation: 传统坍落度测试方法手动、耗时且不一致，无法满足实时监测需求。

Method: 开发了SlumpGuard系统，通过分析混凝土从卡车溜槽流出的视频来评估工作性，无需人工干预。

Result: 系统在实际部署中表现出高效性和准确性，提升了质量控制。

Conclusion: SlumpGuard为现代混凝土质量保证提供了实用解决方案。

Abstract: Concrete workability is essential for construction quality, with the slump
test being the most common on-site method for its assessment. However,
traditional slump testing is manual, time-consuming, and prone to
inconsistency, limiting its applicability for real-time monitoring. To address
these challenges, we propose SlumpGuard, an AI-powered, video-based system that
automatically analyzes concrete flow from the truck chute to assess workability
in real time. Our system enables full-batch inspection without manual
intervention, improving both the accuracy and efficiency of quality control. We
present the system design, a the construction of a dedicated dataset, and
empirical results from real-world deployment, demonstrating the effectiveness
of SlumpGuard as a practical solution for modern concrete quality assurance.

</details>


### [234] [Minimizing the Pretraining Gap: Domain-aligned Text-Based Person Retrieval](https://arxiv.org/abs/2507.10195)
*Shuyu Yang,Yaxiong Wang,Yongrui Li,Li Zhu,Zhedong Zheng*

Main category: cs.CV

TL;DR: 本文提出了一种基于文本的人物检索方法，通过图像和区域级别的域适应技术，解决了合成预训练数据与真实数据之间的领域差距问题。


<details>
  <summary>Details</summary>
Motivation: 由于隐私问题和手动标注成本高，合成数据成为预训练模型的选择，但合成数据与真实数据之间的领域差距限制了预训练-微调范式的效果。

Method: 提出了一种统一的文本人物检索流程，包含图像级别的域感知扩散（DaD）和区域级别的多粒度关系对齐（MRA）。

Result: 在CUHK-PEDES、ICFG-PEDES和RSTPReid数据集上取得了最先进的结果。

Conclusion: 双级别域适应方法有效缩小了合成与真实数据之间的差距，提升了检索性能。

Abstract: In this work, we focus on text-based person retrieval, which aims to identify
individuals based on textual descriptions. Given the significant privacy issues
and the high cost associated with manual annotation, synthetic data has become
a popular choice for pretraining models, leading to notable advancements.
However, the considerable domain gap between synthetic pretraining datasets and
real-world target datasets, characterized by differences in lighting, color,
and viewpoint, remains a critical obstacle that hinders the effectiveness of
the pretrain-finetune paradigm. To bridge this gap, we introduce a unified
text-based person retrieval pipeline considering domain adaptation at both
image and region levels. In particular, it contains two primary components,
i.e., Domain-aware Diffusion (DaD) for image-level adaptation and
Multi-granularity Relation Alignment (MRA) for region-level adaptation. As the
name implies, Domain-aware Diffusion is to migrate the distribution of images
from the pretraining dataset domain to the target real-world dataset domain,
e.g., CUHK-PEDES. Subsequently, MRA performs a meticulous region-level
alignment by establishing correspondences between visual regions and their
descriptive sentences, thereby addressing disparities at a finer granularity.
Extensive experiments show that our dual-level adaptation method has achieved
state-of-the-art results on the CUHK-PEDES, ICFG-PEDES, and RSTPReid datasets,
outperforming existing methodologies. The dataset, model, and code are
available at https://github.com/Shuyu-XJTU/MRA.

</details>


### [235] [A Training-Free, Task-Agnostic Framework for Enhancing MLLM Performance on High-Resolution Images](https://arxiv.org/abs/2507.10202)
*Jaeseong Lee,Yeeun Choi,Heechan Choi,Hanjung Kim,Seonjoo Kim*

Main category: cs.CV

TL;DR: ECP框架通过两阶段方法提升多模态大语言模型在高分辨率图像上的性能，避免直接处理高分辨率图像的局限性。


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型在高分辨率图像上表现不佳，因固定分辨率训练导致细节丢失或泛化能力差。

Method: 提出ECP框架，先通过低分辨率预测定位候选区域，再基于候选区域进行最终预测。

Result: 在4K GUI grounding和4K、8K MLLM感知任务上分别提升21.3%、5.8%、5.2%。

Conclusion: ECP是一种无需训练的任务无关框架，有效提升高分辨率图像处理能力。

Abstract: Multimodal Large Language Models (MLLMs) have demonstrated remarkable
capabilities in vision-language understanding, reasoning, and generation.
However, they struggle with tasks requiring fine-grained localization and
reasoning in high-resolution images. This constraint stems from the fact that
MLLMs are fine-tuned with fixed image resolution to align with the pre-trained
image encoder used in MLLM. Consequently, feeding high-resolution images
directly into MLLMs leads to poor generalization due to a train-test resolution
discrepancy, while downsampling these images-although ensuring
consistency-compromises fine-grained visual details and ultimately degrades
performance. To address this challenge, we propose Extract Candidate then
Predict (ECP), a novel training-free, task-agnostic two-stage framework
designed to enhance MLLM performance on high-resolution images. The key
intuition behind ECP is that while MLLMs struggle with high-resolution images,
their predictions on downsampled images still contain implicit localization
cues. By first identifying candidate region using the coarse prediction and
then predicting the final output based on candidate region, ECP effectively
preserves fine-grained details while mitigating the challenges posed by
high-resolution data. We validate our framework on 4K GUI grounding and 4K, 8K
MLLM perception, achieving +21.3%, +5.8%, +5.2% absolute improvement compared
to baseline respectively, demonstrating its effectiveness. Code is available at
https://github.com/yenncye/ECP.

</details>


### [236] [Improving Multimodal Learning via Imbalanced Learning](https://arxiv.org/abs/2507.10203)
*Shicai Wei,Chunbo Luo,Yang Luo*

Main category: cs.CV

TL;DR: 论文提出了一种非对称表示学习（ARL）策略，通过不平衡优化提升多模态学习性能，证明了模态依赖比与方差比成反比时性能最优。


<details>
  <summary>Details</summary>
Motivation: 多模态学习常因不平衡优化而表现不佳，现有方法通过梯度平衡解决，但作者认为不平衡学习才是最优设置。

Method: 提出ARL策略，通过辅助正则化器计算模态预测方差，并基于方差比重新加权优化，同时引入预测偏差联合优化。

Result: 在多个数据集上的实验验证了ARL的有效性和通用性。

Conclusion: ARL无需额外参数，独立于模型结构和融合方法，能显著提升多模态学习性能。

Abstract: Multimodal learning often encounters the under-optimized problem and may
perform worse than unimodal learning. Existing approaches attribute this issue
to imbalanced learning across modalities and tend to address it through
gradient balancing. However, this paper argues that balanced learning is not
the optimal setting for multimodal learning. With bias-variance analysis, we
prove that imbalanced dependency on each modality obeying the inverse ratio of
their variances contributes to optimal performance. To this end, we propose the
Asymmetric Representation Learning(ARL) strategy to assist multimodal learning
via imbalanced optimization. ARL introduces auxiliary regularizers for each
modality encoder to calculate their prediction variance. ARL then calculates
coefficients via the unimodal variance to re-weight the optimization of each
modality, forcing the modality dependence ratio to be inversely proportional to
the modality variance ratio. Moreover, to minimize the generalization error,
ARL further introduces the prediction bias of each modality and jointly
optimizes them with multimodal loss. Notably, all auxiliary regularizers share
parameters with the multimodal model and rely only on the modality
representation. Thus the proposed ARL strategy introduces no extra parameters
and is independent of the structures and fusion methods of the multimodal
model. Finally, extensive experiments on various datasets validate the
effectiveness and versatility of ARL. Code is available at
\href{https://github.com/shicaiwei123/ICCV2025-ARL}{https://github.com/shicaiwei123/ICCV2025-ARL}

</details>


### [237] [Is Micro-expression Ethnic Leaning?](https://arxiv.org/abs/2507.10209)
*Huai-Qian Khor,Yante Li,Xingxun Jiang,Guoying Zhao*

Main category: cs.CV

TL;DR: 研究探讨了种族背景对情绪表达的影响，挑战了情绪普遍性假设，并提出了一种考虑种族差异的微表情识别框架。


<details>
  <summary>Details</summary>
Motivation: 情绪表达研究通常基于情绪普遍性假设，但忽略了种族背景的影响。本研究旨在揭示种族对情绪表达的实际影响。

Method: 构建跨文化微表情数据库，算法标注种族标签，进行单一种族与混合种族的对比实验，并提出种族感知的情绪特征学习框架。

Result: 实验发现种族背景对情绪表达有显著影响，验证了情绪普遍性假设的局限性。

Conclusion: 种族背景在情绪表达中起重要作用，未来的微表情研究应考虑种族差异以提高准确性。

Abstract: How much does ethnicity play its part in emotional expression? Emotional
expression and micro-expression research probe into understanding human
psychological responses to emotional stimuli, thereby revealing substantial
hidden yet authentic emotions that can be useful in the event of diagnosis and
interviews. While increased attention had been provided to micro-expression
analysis, the studies were done under Ekman's assumption of emotion
universality, where emotional expressions are identical across cultures and
social contexts. Our computational study uncovers some of the influences of
ethnic background in expression analysis, leading to an argument that the
emotional universality hypothesis is an overgeneralization from the perspective
of manual psychological analysis. In this research, we propose to investigate
the level of influence of ethnicity in a simulated micro-expression scenario.
We construct a cross-cultural micro-expression database and algorithmically
annotate the ethnic labels to facilitate the investigation. With the ethnically
annotated dataset, we perform a prima facie study to compare mono-ethnicity and
stereo-ethnicity in a controlled environment, which uncovers a certain
influence of ethnic bias via an experimental way. Building on this finding, we
propose a framework that integrates ethnic context into the emotional feature
learning process, yielding an ethnically aware framework that recognises
ethnicity differences in micro-expression recognition. For improved
understanding, qualitative analyses have been done to solidify the preliminary
investigation into this new realm of research. Code is publicly available at
https://github.com/IcedDoggie/ICMEW2025_EthnicMER

</details>


### [238] [Boosting Multimodal Learning via Disentangled Gradient Learning](https://arxiv.org/abs/2507.10213)
*Shicai Wei,Chunbo Luo,Yang Luo*

Main category: cs.CV

TL;DR: 论文揭示了多模态学习中模态编码器与融合模块间的优化冲突，提出了解耦梯度学习（DGL）框架以提升性能。


<details>
  <summary>Details</summary>
Motivation: 多模态学习常因模态间优化冲突导致性能不如单模态学习，现有方法未能解释主导模态表现不佳的原因。

Method: 提出DGL框架，通过截断多模态损失对模态编码器的梯度，并用单模态损失梯度替代，同时移除单模态损失对融合模块的梯度。

Result: 实验表明DGL在多模态任务中有效且通用，显著提升了性能。

Conclusion: DGL成功解决了多模态学习中的优化冲突问题，为相关研究提供了新思路。

Abstract: Multimodal learning often encounters the under-optimized problem and may have
worse performance than unimodal learning. Existing methods attribute this
problem to the imbalanced learning between modalities and rebalance them
through gradient modulation. However, they fail to explain why the dominant
modality in multimodal models also underperforms that in unimodal learning. In
this work, we reveal the optimization conflict between the modality encoder and
modality fusion module in multimodal models. Specifically, we prove that the
cross-modal fusion in multimodal models decreases the gradient passed back to
each modality encoder compared with unimodal models. Consequently, the
performance of each modality in the multimodal model is inferior to that in the
unimodal model. To this end, we propose a disentangled gradient learning (DGL)
framework to decouple the optimization of the modality encoder and modality
fusion module in the multimodal model. DGL truncates the gradient
back-propagated from the multimodal loss to the modality encoder and replaces
it with the gradient from unimodal loss. Besides, DGL removes the gradient
back-propagated from the unimodal loss to the modality fusion module. This
helps eliminate the gradient interference between the modality encoder and
modality fusion module while ensuring their respective optimization processes.
Finally, extensive experiments on multiple types of modalities, tasks, and
frameworks with dense cross-modal interaction demonstrate the effectiveness and
versatility of the proposed DGL. Code is available at
\href{https://github.com/shicaiwei123/ICCV2025-GDL}{https://github.com/shicaiwei123/ICCV2025-GDL}

</details>


### [239] [From Wardrobe to Canvas: Wardrobe Polyptych LoRA for Part-level Controllable Human Image Generation](https://arxiv.org/abs/2507.10217)
*Jeongho Kim,Sunghyun Park,Hyoungwoo Park,Sungrack Yun,Jaegul Choo,Seokeon Cho*

Main category: cs.CV

TL;DR: 提出了一种名为Wardrobe Polyptych LoRA的新方法，用于个性化人体图像生成，通过训练LoRA层减少计算负担，同时提高生成图像的保真度和一致性。


<details>
  <summary>Details</summary>
Motivation: 解决现有个性化人体图像生成方法在计算成本和实时性方面的不足，同时确保高保真度和一致性。

Method: 训练LoRA层，利用空间参考和选择性主题区域损失，减少信息丢失并提高生成质量。

Result: 在保真度和一致性上显著优于现有技术，实现了高保真且身份保持的全身体合成。

Conclusion: Wardrobe Polyptych LoRA是一种高效且实用的个性化人体图像生成方法，无需额外推理参数，适用于实时应用。

Abstract: Recent diffusion models achieve personalization by learning specific
subjects, allowing learned attributes to be integrated into generated images.
However, personalized human image generation remains challenging due to the
need for precise and consistent attribute preservation (e.g., identity,
clothing details). Existing subject-driven image generation methods often
require either (1) inference-time fine-tuning with few images for each new
subject or (2) large-scale dataset training for generalization. Both approaches
are computationally expensive and impractical for real-time applications. To
address these limitations, we present Wardrobe Polyptych LoRA, a novel
part-level controllable model for personalized human image generation. By
training only LoRA layers, our method removes the computational burden at
inference while ensuring high-fidelity synthesis of unseen subjects. Our key
idea is to condition the generation on the subject's wardrobe and leverage
spatial references to reduce information loss, thereby improving fidelity and
consistency. Additionally, we introduce a selective subject region loss, which
encourages the model to disregard some of reference images during training. Our
loss ensures that generated images better align with text prompts while
maintaining subject integrity. Notably, our Wardrobe Polyptych LoRA requires no
additional parameters at the inference stage and performs generation using a
single model trained on a few training samples. We construct a new dataset and
benchmark tailored for personalized human image generation. Extensive
experiments show that our approach significantly outperforms existing
techniques in fidelity and consistency, enabling realistic and
identity-preserving full-body synthesis.

</details>


### [240] [Straighten Viscous Rectified Flow via Noise Optimization](https://arxiv.org/abs/2507.10218)
*Jimin Dai,Jiexi Yan,Jian Yang,Lei Luo*

Main category: cs.CV

TL;DR: VRFNO通过噪声优化和速度场改进Reflow的局限性，提升单步和少步生成图像的质量。


<details>
  <summary>Details</summary>
Motivation: Reflow在快速生成高质量图像时存在分布差距问题，VRFNO旨在解决这一问题。

Method: VRFNO结合编码器和神经速度场，引入历史速度项和噪声优化技术。

Result: 实验表明VRFNO显著优于Reflow，在单步和少步生成任务中达到最优性能。

Conclusion: VRFNO通过创新设计有效解决了Reflow的局限性，提升了生成效果。

Abstract: The Reflow operation aims to straighten the inference trajectories of the
rectified flow during training by constructing deterministic couplings between
noises and images, thereby improving the quality of generated images in
single-step or few-step generation. However, we identify critical limitations
in Reflow, particularly its inability to rapidly generate high-quality images
due to a distribution gap between images in its constructed deterministic
couplings and real images. To address these shortcomings, we propose a novel
alternative called Straighten Viscous Rectified Flow via Noise Optimization
(VRFNO), which is a joint training framework integrating an encoder and a
neural velocity field. VRFNO introduces two key innovations: (1) a historical
velocity term that enhances trajectory distinction, enabling the model to more
accurately predict the velocity of the current trajectory, and (2) the noise
optimization through reparameterization to form optimized couplings with real
images which are then utilized for training, effectively mitigating errors
caused by Reflow's limitations. Comprehensive experiments on synthetic data and
real datasets with varying resolutions show that VRFNO significantly mitigates
the limitations of Reflow, achieving state-of-the-art performance in both
one-step and few-step generation tasks.

</details>


### [241] [Spatial Lifting for Dense Prediction](https://arxiv.org/abs/2507.10222)
*Mingzhi Xu,Yizhe Zhang*

Main category: cs.CV

TL;DR: Spatial Lifting (SL) 是一种将输入提升到高维空间处理的新方法，显著减少模型参数和推理成本，同时在密集预测任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: 传统密集预测方法通常计算成本高且参数多，SL旨在通过维度提升实现高效、准确的预测。

Method: SL将2D输入（如图像）提升到更高维度（如3D），并使用对应维度的网络（如3D U-Net）处理，生成结构化输出。

Result: 在19个基准数据集上验证，SL在减少98%参数和降低推理成本的同时，性能与传统方法相当。

Conclusion: SL为密集预测任务提供了一种高效、准确且可靠的新范式。

Abstract: We present Spatial Lifting (SL), a novel methodology for dense prediction
tasks. SL operates by lifting standard inputs, such as 2D images, into a
higher-dimensional space and subsequently processing them using networks
designed for that higher dimension, such as a 3D U-Net. Counterintuitively,
this dimensionality lifting allows us to achieve good performance on benchmark
tasks compared to conventional approaches, while reducing inference costs and
significantly lowering the number of model parameters. The SL framework
produces intrinsically structured outputs along the lifted dimension. This
emergent structure facilitates dense supervision during training and enables
robust, near-zero-additional-cost prediction quality assessment at test time.
We validate our approach across 19 benchmark datasets (13 for semantic
segmentation and 6 for depth estimation), demonstrating competitive dense
prediction performance while reducing the model parameter count by over 98% (in
the U-Net case) and lowering inference costs. Spatial Lifting introduces a new
vision modeling paradigm that offers a promising path toward more efficient,
accurate, and reliable deep networks for dense prediction tasks in vision.

</details>


### [242] [ProGait: A Multi-Purpose Video Dataset and Benchmark for Transfemoral Prosthesis Users](https://arxiv.org/abs/2507.10223)
*Xiangyu Yin,Boyuan Yang,Weichen Liu,Qiyao Xue,Abrar Alamri,Goeran Fiedler,Wei Gao*

Main category: cs.CV

TL;DR: 论文介绍了ProGait数据集，用于支持基于视觉的机器学习方法在假肢步态分析中的应用，并提供了基准任务和基线模型。


<details>
  <summary>Details</summary>
Motivation: 假肢步态分析对优化假肢设计和提高截肢者生活质量至关重要，但现有视觉方法在假肢检测和分析上存在挑战。

Method: 提出ProGait数据集，包含412个视频片段，支持多种视觉任务，并提供了基准模型。

Result: 基线模型在假肢特定任务中表现出更好的泛化能力。

Conclusion: ProGait数据集为假肢步态分析提供了有效的工具，未来可进一步扩展应用。

Abstract: Prosthetic legs play a pivotal role in clinical rehabilitation, allowing
individuals with lower-limb amputations the ability to regain mobility and
improve their quality of life. Gait analysis is fundamental for optimizing
prosthesis design and alignment, directly impacting the mobility and life
quality of individuals with lower-limb amputations. Vision-based machine
learning (ML) methods offer a scalable and non-invasive solution to gait
analysis, but face challenges in correctly detecting and analyzing prosthesis,
due to their unique appearances and new movement patterns. In this paper, we
aim to bridge this gap by introducing a multi-purpose dataset, namely ProGait,
to support multiple vision tasks including Video Object Segmentation, 2D Human
Pose Estimation, and Gait Analysis (GA). ProGait provides 412 video clips from
four above-knee amputees when testing multiple newly-fitted prosthetic legs
through walking trials, and depicts the presence, contours, poses, and gait
patterns of human subjects with transfemoral prosthetic legs. Alongside the
dataset itself, we also present benchmark tasks and fine-tuned baseline models
to illustrate the practical application and performance of the ProGait dataset.
We compared our baseline models against pre-trained vision models,
demonstrating improved generalizability when applying the ProGait dataset for
prosthesis-specific tasks. Our code is available at
https://github.com/pittisl/ProGait and dataset at
https://huggingface.co/datasets/ericyxy98/ProGait.

</details>


### [243] [Synthesizing Near-Boundary OOD Samples for Out-of-Distribution Detection](https://arxiv.org/abs/2507.10225)
*Jinglun Li,Kaixun Jiang,Zhaoyu Chen,Bo Lin,Yao Tang,Weifeng Ge,Wenqiang Zhang*

Main category: cs.CV

TL;DR: SynOOD利用基础模型生成合成OOD数据，通过迭代修复和噪声调整增强CLIP模型的边界区分能力，显著提升OOD检测性能。


<details>
  <summary>Details</summary>
Motivation: 现有预训练视觉语言模型在检测OOD样本时，对靠近InD数据的挑战性OOD样本仍存在误分类问题。

Method: 利用扩散模型和MLLM生成合成OOD数据，通过迭代修复和基于能量分数的噪声调整优化样本，并微调CLIP模型。

Result: 在ImageNet基准测试中，AUROC提升2.80%，FPR95降低11.13%，性能显著优于现有方法。

Conclusion: SynOOD通过合成边界对齐的OOD数据，有效提升模型对挑战性OOD样本的区分能力，且计算开销小。

Abstract: Pre-trained vision-language models have exhibited remarkable abilities in
detecting out-of-distribution (OOD) samples. However, some challenging OOD
samples, which lie close to in-distribution (InD) data in image feature space,
can still lead to misclassification. The emergence of foundation models like
diffusion models and multimodal large language models (MLLMs) offers a
potential solution to this issue. In this work, we propose SynOOD, a novel
approach that harnesses foundation models to generate synthetic, challenging
OOD data for fine-tuning CLIP models, thereby enhancing boundary-level
discrimination between InD and OOD samples. Our method uses an iterative
in-painting process guided by contextual prompts from MLLMs to produce nuanced,
boundary-aligned OOD samples. These samples are refined through noise
adjustments based on gradients from OOD scores like the energy score,
effectively sampling from the InD/OOD boundary. With these carefully
synthesized images, we fine-tune the CLIP image encoder and negative label
features derived from the text encoder to strengthen connections between
near-boundary OOD samples and a set of negative labels. Finally, SynOOD
achieves state-of-the-art performance on the large-scale ImageNet benchmark,
with minimal increases in parameters and runtime. Our approach significantly
surpasses existing methods, improving AUROC by 2.80% and reducing FPR95 by
11.13%. Codes are available in https://github.com/Jarvisgivemeasuit/SynOOD.

</details>


### [244] [Navigating the Challenges of AI-Generated Image Detection in the Wild: What Truly Matters?](https://arxiv.org/abs/2507.10236)
*Despina Konstantinidou,Dimitrios Karageorgiou,Christos Koutlis,Olga Papadopoulou,Emmanouil Schinas,Symeon Papadopoulos*

Main category: cs.CV

TL;DR: 论文探讨了AI生成图像检测（AID）在现实世界中的挑战，提出了ITW-SM数据集，并分析了影响检测性能的四个关键因素，最终将AID模型的AUC平均提升了26.87%。


<details>
  <summary>Details</summary>
Motivation: 随着生成技术的快速发展，AI生成图像的逼真度已足以欺骗人类，但现有AID模型在现实场景中表现不佳，亟需改进。

Method: 引入ITW-SM数据集，系统评估了主干架构、训练数据组成、预处理策略和数据增强组合对AID性能的影响。

Result: 通过优化上述因素，AID模型在现实条件下的AUC平均提升了26.87%。

Conclusion: 研究表明，改进AID模型需关注现实场景的多样性，ITW-SM数据集和提出的优化策略为未来研究提供了重要参考。

Abstract: The rapid advancement of generative technologies presents both unprecedented
creative opportunities and significant challenges, particularly in maintaining
social trust and ensuring the integrity of digital information. Following these
concerns, the challenge of AI-Generated Image Detection (AID) becomes
increasingly critical. As these technologies become more sophisticated, the
quality of AI-generated images has reached a level that can easily deceive even
the most discerning observers. Our systematic evaluation highlights a critical
weakness in current AI-Generated Image Detection models: while they perform
exceptionally well on controlled benchmark datasets, they struggle
significantly with real-world variations. To assess this, we introduce ITW-SM,
a new dataset of real and AI-generated images collected from major social media
platforms. In this paper, we identify four key factors that influence AID
performance in real-world scenarios: backbone architecture, training data
composition, pre-processing strategies and data augmentation combinations. By
systematically analyzing these components, we shed light on their impact on
detection efficacy. Our modifications result in an average AUC improvement of
26.87% across various AID models under real-world conditions.

</details>


### [245] [Transferring Styles for Reduced Texture Bias and Improved Robustness in Semantic Segmentation Networks](https://arxiv.org/abs/2507.10239)
*Ben Hamscher,Edgar Heinert,Annika Mütze,Kira Maag,Matthias Rottmann*

Main category: cs.CV

TL;DR: 研究探讨了风格迁移在语义分割中减少纹理偏差并提升鲁棒性的效果。


<details>
  <summary>Details</summary>
Motivation: 探索风格迁移是否能在语义分割中减少DNN对纹理的依赖，增强形状特征的使用，从而提高鲁棒性。

Method: 通过Voronoi细胞生成随机区域进行风格迁移，用风格化数据训练语义分割DNN。

Result: 风格迁移显著减少了纹理偏差，提升了模型对图像损坏和对抗攻击的鲁棒性，适用于不同架构和数据集。

Conclusion: 风格迁移是一种通用方法，能有效减少纹理偏差并增强语义分割模型的鲁棒性。

Abstract: Recent research has investigated the shape and texture biases of deep neural
networks (DNNs) in image classification which influence their generalization
capabilities and robustness. It has been shown that, in comparison to regular
DNN training, training with stylized images reduces texture biases in image
classification and improves robustness with respect to image corruptions. In an
effort to advance this line of research, we examine whether style transfer can
likewise deliver these two effects in semantic segmentation. To this end, we
perform style transfer with style varying across artificial image areas. Those
random areas are formed by a chosen number of Voronoi cells. The resulting
style-transferred data is then used to train semantic segmentation DNNs with
the objective of reducing their dependence on texture cues while enhancing
their reliance on shape-based features. In our experiments, it turns out that
in semantic segmentation, style transfer augmentation reduces texture bias and
strongly increases robustness with respect to common image corruptions as well
as adversarial attacks. These observations hold for convolutional neural
networks and transformer architectures on the Cityscapes dataset as well as on
PASCAL Context, showing the generality of the proposed method.

</details>


### [246] [Kaleidoscopic Background Attack: Disrupting Pose Estimation with Multi-Fold Radial Symmetry Textures](https://arxiv.org/abs/2507.10265)
*Xinlong Ding,Hongwei Yu,Jiawei Li,Feifan Li,Yu Shang,Bochao Zou,Huimin Ma,Jiansheng Chen*

Main category: cs.CV

TL;DR: 提出了一种基于多折叠径向对称的Kaleidoscopic Background Attack（KBA）方法，通过优化一致性损失显著提升对相机姿态估计模型的攻击效果。


<details>
  <summary>Details</summary>
Motivation: 在稀疏输入的对象中心场景中，背景纹理对相机姿态估计的准确性影响显著，因此需要一种有效的攻击方法。

Method: 使用相同片段形成多折叠径向对称的圆盘，并提出投影方向一致性损失优化这些片段。

Result: 优化的对抗性背景能有效攻击多种相机姿态估计模型。

Conclusion: KBA方法在攻击相机姿态估计模型方面表现出色，优化后的背景片段显著提升了攻击效果。

Abstract: Camera pose estimation is a fundamental computer vision task that is
essential for applications like visual localization and multi-view stereo
reconstruction. In the object-centric scenarios with sparse inputs, the
accuracy of pose estimation can be significantly influenced by background
textures that occupy major portions of the images across different viewpoints.
In light of this, we introduce the Kaleidoscopic Background Attack (KBA), which
uses identical segments to form discs with multi-fold radial symmetry. These
discs maintain high similarity across different viewpoints, enabling effective
attacks on pose estimation models even with natural texture segments.
Additionally, a projected orientation consistency loss is proposed to optimize
the kaleidoscopic segments, leading to significant enhancement in the attack
effectiveness. Experimental results show that optimized adversarial
kaleidoscopic backgrounds can effectively attack various camera pose estimation
models.

</details>


### [247] [FTCFormer: Fuzzy Token Clustering Transformer for Image Classification](https://arxiv.org/abs/2507.10283)
*Muyi Bao,Changyu Zeng,Yifan Wang,Zhengni Yang,Zimu Wang,Guangliang Cheng,Jun Qi,Wei Wang*

Main category: cs.CV

TL;DR: FTCFormer提出了一种基于语义的动态视觉标记生成方法，通过聚类改进特征表示，显著提升了图像分类性能。


<details>
  <summary>Details</summary>
Motivation: 传统Transformer架构将图像嵌入为均匀的网格标记，忽略了语义信息，导致特征表示不理想。

Method: 引入Fuzzy Token Clustering Transformer (FTCFormer)，结合聚类下采样模块、DPC-FKNN机制、SCS评分和Cmerge策略。

Result: 在32个数据集上验证，图像分类性能显著提升，最高提升1.43%。

Conclusion: FTCFormer通过动态语义标记生成，有效优化了特征表示，提升了分类效果。

Abstract: Transformer-based deep neural networks have achieved remarkable success
across various computer vision tasks, largely attributed to their long-range
self-attention mechanism and scalability. However, most transformer
architectures embed images into uniform, grid-based vision tokens, neglecting
the underlying semantic meanings of image regions, resulting in suboptimal
feature representations. To address this issue, we propose Fuzzy Token
Clustering Transformer (FTCFormer), which incorporates a novel clustering-based
downsampling module to dynamically generate vision tokens based on the semantic
meanings instead of spatial positions. It allocates fewer tokens to less
informative regions and more to represent semantically important regions,
regardless of their spatial adjacency or shape irregularity. To further enhance
feature extraction and representation, we propose a Density Peak
Clustering-Fuzzy K-Nearest Neighbor (DPC-FKNN) mechanism for clustering center
determination, a Spatial Connectivity Score (SCS) for token assignment, and a
channel-wise merging (Cmerge) strategy for token merging. Extensive experiments
on 32 datasets across diverse domains validate the effectiveness of FTCFormer
on image classification, showing consistent improvements over the TCFormer
baseline, achieving gains of improving 1.43% on five fine-grained datasets,
1.09% on six natural image datasets, 0.97% on three medical datasets and 0.55%
on four remote sensing datasets. The code is available at:
https://github.com/BaoBao0926/FTCFormer/tree/main.

</details>


### [248] [Show and Polish: Reference-Guided Identity Preservation in Face Video Restoration](https://arxiv.org/abs/2507.10293)
*Wenkang Han,Wang Lin,Yiyun Zhou,Qi Liu,Shulei Wang,Chang Yao,Jingyuan Chen*

Main category: cs.CV

TL;DR: IP-FVR是一种利用高质量参考图像作为视觉提示的面部视频恢复方法，通过解耦交叉注意力机制和反馈学习策略，显著提升了身份一致性和恢复质量。


<details>
  <summary>Details</summary>
Motivation: 传统方法在严重退化情况下难以保留细粒度的身份特征，导致恢复结果缺乏个体特性。

Method: IP-FVR采用参考图像提供身份条件，结合解耦交叉注意力机制、反馈学习和指数混合策略，确保帧内和跨剪辑的身份一致性。

Result: 实验表明，IP-FVR在质量和身份保留方面优于现有方法。

Conclusion: IP-FVR在面部视频恢复中具有实际应用潜力。

Abstract: Face Video Restoration (FVR) aims to recover high-quality face videos from
degraded versions. Traditional methods struggle to preserve fine-grained,
identity-specific features when degradation is severe, often producing
average-looking faces that lack individual characteristics. To address these
challenges, we introduce IP-FVR, a novel method that leverages a high-quality
reference face image as a visual prompt to provide identity conditioning during
the denoising process. IP-FVR incorporates semantically rich identity
information from the reference image using decoupled cross-attention
mechanisms, ensuring detailed and identity consistent results. For intra-clip
identity drift (within 24 frames), we introduce an identity-preserving feedback
learning method that combines cosine similarity-based reward signals with
suffix-weighted temporal aggregation. This approach effectively minimizes drift
within sequences of frames. For inter-clip identity drift, we develop an
exponential blending strategy that aligns identities across clips by
iteratively blending frames from previous clips during the denoising process.
This method ensures consistent identity representation across different clips.
Additionally, we enhance the restoration process with a multi-stream negative
prompt, guiding the model's attention to relevant facial attributes and
minimizing the generation of low-quality or incorrect features. Extensive
experiments on both synthetic and real-world datasets demonstrate that IP-FVR
outperforms existing methods in both quality and identity preservation,
showcasing its substantial potential for practical applications in face video
restoration.

</details>


### [249] [DisCo: Towards Distinct and Coherent Visual Encapsulation in Video MLLMs](https://arxiv.org/abs/2507.10302)
*Jiahe Zhao,Rongkun Zheng,Yi Wang,Helin Wang,Hengshuang Zhao*

Main category: cs.CV

TL;DR: DisCo是一种新颖的视觉封装方法，通过视觉概念判别器和时间焦点校准器模块，解决了视频MLLM中的语义模糊和时间不连贯问题，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 线性投影器在视频MLLM中导致语义模糊和时间不连贯，而重采样器结构虽有望解决但尚未实现有效方案。

Method: DisCo包含视觉概念判别器（VCD）模块和时间焦点校准器（TFC）模块，分别确保语义清晰和时间连贯。

Result: 实验表明，DisCo在多种视频理解基准测试中显著优于现有方法，并提高了标记效率。

Conclusion: DisCo为视频MLLM提供了一种高效且性能优越的视觉封装解决方案。

Abstract: In video Multimodal Large Language Models (video MLLMs), the visual
encapsulation process plays a pivotal role in converting video contents into
representative tokens for LLM input. While linear projectors are widely
employed for encapsulation, they introduce semantic indistinctness and temporal
incoherence when applied to videos. Conversely, the structure of resamplers
shows promise in tackling these challenges, but an effective solution remains
unexplored. Drawing inspiration from resampler structures, we introduce DisCo,
a novel visual encapsulation method designed to yield semantically distinct and
temporally coherent visual tokens for video MLLMs. DisCo integrates two key
components: (1) A Visual Concept Discriminator (VCD) module, assigning unique
semantics for visual tokens by associating them in pair with discriminative
concepts in the video. (2) A Temporal Focus Calibrator (TFC) module, ensuring
consistent temporal focus of visual tokens to video elements across every video
frame. Through extensive experiments on multiple video MLLM frameworks, we
demonstrate that DisCo remarkably outperforms previous state-of-the-art methods
across a variety of video understanding benchmarks, while also achieving higher
token efficiency thanks to the reduction of semantic indistinctness. The code:
https://github.com/ZJHTerry18/DisCo.

</details>


### [250] [Contrastive Pretraining with Dual Visual Encoders for Gloss-Free Sign Language Translation](https://arxiv.org/abs/2507.10306)
*Ozge Mercanoglu Sincan,Richard Bowden*

Main category: cs.CV

TL;DR: 提出了一种基于双视觉编码器的无注释手语翻译框架，通过对比视觉-语言预训练实现高效翻译。


<details>
  <summary>Details</summary>
Motivation: 传统方法依赖昂贵且不完整的注释，无法充分捕捉连续手语的复杂性。

Method: 采用两阶段双视觉编码器框架，预训练时通过对比目标对齐视觉和文本嵌入，下游任务时融合视觉特征输入编码器-解码器模型。

Result: 在Phoenix-2014T基准测试中，双编码器架构优于单流变体，并在无注释方法中取得最高BLEU-4分数。

Conclusion: 双视觉编码器框架为无注释手语翻译提供了高效解决方案。

Abstract: Sign Language Translation (SLT) aims to convert sign language videos into
spoken or written text. While early systems relied on gloss annotations as an
intermediate supervision, such annotations are costly to obtain and often fail
to capture the full complexity of continuous signing. In this work, we propose
a two-phase, dual visual encoder framework for gloss-free SLT, leveraging
contrastive visual-language pretraining. During pretraining, our approach
employs two complementary visual backbones whose outputs are jointly aligned
with each other and with sentence-level text embeddings via a contrastive
objective. During the downstream SLT task, we fuse the visual features and
input them into an encoder-decoder model. On the Phoenix-2014T benchmark, our
dual encoder architecture consistently outperforms its single stream variants
and achieves the highest BLEU-4 score among existing gloss-free SLT approaches.

</details>


### [251] [Mind the Gap: Aligning Vision Foundation Models to Image Feature Matching](https://arxiv.org/abs/2507.10318)
*Yuhan Liu,Jingwen Fu,Yang Wu,Kangyi Wu,Pengna Li,Jiayi Wu,Sanping Zhou,Jingmin Xin*

Main category: cs.CV

TL;DR: 论文提出了一种名为IMD的框架，通过预训练的扩散模型解决图像特征匹配中的对齐问题，显著提升了多实例场景下的性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法在引入基础模型进行特征匹配时忽略了单图像理解与跨图像理解之间的不对齐问题，导致多实例特征匹配效果不佳。

Method: IMD框架结合生成式扩散模型捕捉实例级细节，并设计跨图像交互提示模块促进图像对间的双向信息交互。

Result: IMD在常用基准测试中达到新SOTA，并在多实例基准IMIM上提升12%。

Conclusion: IMD有效解决了基础模型在特征匹配中的不对齐问题，显著提升了多实例场景的性能。

Abstract: Leveraging the vision foundation models has emerged as a mainstream paradigm
that improves the performance of image feature matching. However, previous
works have ignored the misalignment when introducing the foundation models into
feature matching. The misalignment arises from the discrepancy between the
foundation models focusing on single-image understanding and the cross-image
understanding requirement of feature matching. Specifically, 1) the embeddings
derived from commonly used foundation models exhibit discrepancies with the
optimal embeddings required for feature matching; 2) lacking an effective
mechanism to leverage the single-image understanding ability into cross-image
understanding. A significant consequence of the misalignment is they struggle
when addressing multi-instance feature matching problems. To address this, we
introduce a simple but effective framework, called IMD (Image feature Matching
with a pre-trained Diffusion model) with two parts: 1) Unlike the dominant
solutions employing contrastive-learning based foundation models that emphasize
global semantics, we integrate the generative-based diffusion models to
effectively capture instance-level details. 2) We leverage the prompt mechanism
in generative model as a natural tunnel, propose a novel cross-image
interaction prompting module to facilitate bidirectional information
interaction between image pairs. To more accurately measure the misalignment,
we propose a new benchmark called IMIM, which focuses on multi-instance
scenarios. Our proposed IMD establishes a new state-of-the-art in commonly
evaluated benchmarks, and the superior improvement 12% in IMIM indicates our
method efficiently mitigates the misalignment.

</details>


### [252] [Text Embedding Knows How to Quantize Text-Guided Diffusion Models](https://arxiv.org/abs/2507.10340)
*Hongjae Lee,Myungjun Son,Dongjea Kang,Seung-Won Jung*

Main category: cs.CV

TL;DR: QLIP是一种新的量化方法，利用文本提示为扩散模型选择逐层逐时间步的比特精度，降低计算复杂度并提升生成图像质量。


<details>
  <summary>Details</summary>
Motivation: 扩散模型在图像生成任务中表现出色，但其高计算复杂度限制了在资源受限环境中的应用。现有量化方法未充分利用输入条件（如文本提示）的信息。

Method: 提出QLIP方法，通过文本提示指导逐层逐时间步的比特精度选择，并可无缝集成到现有量化方法中。

Result: 实验表明，QLIP能有效降低计算复杂度，并在多个数据集上提升生成图像质量。

Conclusion: QLIP为扩散模型量化提供了一种高效且灵活的方法，显著提升了资源受限环境中的实用性。

Abstract: Despite the success of diffusion models in image generation tasks such as
text-to-image, the enormous computational complexity of diffusion models limits
their use in resource-constrained environments. To address this, network
quantization has emerged as a promising solution for designing efficient
diffusion models. However, existing diffusion model quantization methods do not
consider input conditions, such as text prompts, as an essential source of
information for quantization. In this paper, we propose a novel quantization
method dubbed Quantization of Language-to-Image diffusion models using text
Prompts (QLIP). QLIP leverages text prompts to guide the selection of bit
precision for every layer at each time step. In addition, QLIP can be
seamlessly integrated into existing quantization methods to enhance
quantization efficiency. Our extensive experiments demonstrate the
effectiveness of QLIP in reducing computational complexity and improving the
quality of the generated images across various datasets.

</details>


### [253] [FGSSNet: Feature-Guided Semantic Segmentation of Real World Floorplans](https://arxiv.org/abs/2507.10343)
*Hugo Norrby,Gabriel Färm,Kevin Hernandez-Diaz,Fernando Alonso-Fernandez*

Main category: cs.CV

TL;DR: FGSSNet是一种新型多头部特征引导语义分割架构，旨在提升平面图中墙分割的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 提高平面图中墙分割的泛化能力，通过特征引导优化分割效果。

Method: 采用U-Net分割主干，结合多头部专用特征提取器，提取领域特定特征图并注入U-Net潜在空间以引导分割。特征提取器通过编码器-解码器结构训练，预测墙宽度并生成压缩潜在表示。

Result: 实验表明，注入特征后性能优于普通U-Net，验证了方法的有效性。

Conclusion: FGSSNet通过特征引导显著提升了墙分割性能，证明了其设计的合理性。

Abstract: We introduce FGSSNet, a novel multi-headed feature-guided semantic
segmentation (FGSS) architecture designed to improve the generalization ability
of wall segmentation on floorplans. FGSSNet features a U-Net segmentation
backbone with a multi-headed dedicated feature extractor used to extract
domain-specific feature maps which are injected into the latent space of U-Net
to guide the segmentation process. This dedicated feature extractor is trained
as an encoder-decoder with selected wall patches, representative of the walls
present in the input floorplan, to produce a compressed latent representation
of wall patches while jointly trained to predict the wall width. In doing so,
we expect that the feature extractor encodes texture and width features of wall
patches that are useful to guide the wall segmentation process. Our experiments
show increased performance by the use of such injected features in comparison
to the vanilla U-Net, highlighting the validity of the proposed approach.

</details>


### [254] [Beyond Graph Model: Reliable VLM Fine-Tuning via Random Graph Adapter](https://arxiv.org/abs/2507.10355)
*Bo Jiang,Xueyang Ze,Beibei Wang,Xixi Wang,Xixi Wan,Bin Luo*

Main category: cs.CV

TL;DR: 提出了一种基于随机图模型的文本适配器（VRGAdapter），用于捕捉类别描述的多样性和类间关系，并通过不确定性引导的多分支融合（UMF）提升下游任务性能。


<details>
  <summary>Details</summary>
Motivation: 传统确定性适配器无法充分捕捉类别描述的多样性和类间关系，限制了视觉语言模型的潜力。

Method: 利用顶点随机知识图（VRKG）建模类别描述的多样性和类间关系，通过概率消息传播学习上下文感知的分布表示，并采用重参数化采样实现适配器学习。

Result: 在多个基准数据集上的实验证明了VRGAdapter和UMF方案的有效性。

Conclusion: VRGAdapter提供了一种更通用的适配器解决方案，显著提升了视觉语言模型在下游任务中的表现。

Abstract: Textual adapter-based tuning methods have shown significant potential in
transferring knowledge from pre-trained Vision-Language Models (VLMs) to
downstream tasks. Existing works generally employ the deterministic textual
feature adapter to refine each category textual representation. However, due to
inherent factors such as different attributes and contexts, there exists
significant diversity in textual descriptions for each category. Such
description diversity offers rich discriminative semantic knowledge that can
benefit downstream visual learning tasks. Obviously, traditional deterministic
adapter model cannot adequately capture this varied semantic information. Also,
it is desirable to exploit the inter-class relationships in VLM adapter. To
address these issues, we propose to exploit random graph model into VLM adapter
and develop a novel Vertex Random Graph Adapter (VRGAdapter). VRGAdapter first
models the inherent diverse descriptions of each category and inter-class
relationships of different categories simultaneously by leveraging a Vertex
Random Knowledge Graph (VRKG) model. Then, it employs probabilistic message
propagation on VRKG to learn context-aware distribution representation for each
class node. Finally, it adopts a reparameterized sampling function to achieve
textual adapter learning. Note that, VRGAdapter provides a more general adapter
solution that encompasses traditional graph-based adapter as a special case. In
addition, to enable more robust performance for downstream tasks, we also
introduce a new Uncertainty-guided Multi-branch Fusion (UMF) scheme that
dynamically integrates multiple pre-trained models for ensemble prediction.
Extensive experiments on multiple benchmark datasets demonstrate the
effectiveness of our approach.

</details>


### [255] [Fine-Grained Zero-Shot Object Detection](https://arxiv.org/abs/2507.10358)
*Hongxu Ma,Chenbo Zhang,Lu Zhang,Jiaogen Zhou,Jihong Guan,Shuigeng Zhou*

Main category: cs.CV

TL;DR: 本文提出并解决了细粒度零样本目标检测（FG-ZSD）问题，开发了基于改进两阶段检测器的MSHC方法，并构建了首个FG-ZSD基准数据集FGZSD-Birds。实验表明该方法优于现有模型。


<details>
  <summary>Details</summary>
Motivation: 现有零样本目标检测（ZSD）方法主要针对粗粒度对象，而现实场景中常需处理细粒度对象（如鸟类、鱼类、花卉等）。本文旨在解决细粒度对象的零样本检测问题。

Method: 提出MSHC方法，基于改进的两阶段检测器，采用多级语义感知嵌入对齐损失，确保视觉与语义空间的紧密耦合。

Result: 在构建的FGZSD-Birds数据集上，MSHC方法表现优于现有ZSD模型。

Conclusion: 本文成功解决了细粒度零样本目标检测问题，并通过实验验证了方法的有效性。

Abstract: Zero-shot object detection (ZSD) aims to leverage semantic descriptions to
localize and recognize objects of both seen and unseen classes. Existing ZSD
works are mainly coarse-grained object detection, where the classes are
visually quite different, thus are relatively easy to distinguish. However, in
real life we often have to face fine-grained object detection scenarios, where
the classes are too similar to be easily distinguished. For example, detecting
different kinds of birds, fishes, and flowers.
  In this paper, we propose and solve a new problem called Fine-Grained
Zero-Shot Object Detection (FG-ZSD for short), which aims to detect objects of
different classes with minute differences in details under the ZSD paradigm. We
develop an effective method called MSHC for the FG-ZSD task, which is based on
an improved two-stage detector and employs a multi-level semantics-aware
embedding alignment loss, ensuring tight coupling between the visual and
semantic spaces. Considering that existing ZSD datasets are not suitable for
the new FG-ZSD task, we build the first FG-ZSD benchmark dataset FGZSD-Birds,
which contains 148,820 images falling into 36 orders, 140 families, 579 genera
and 1432 species. Extensive experiments on FGZSD-Birds show that our method
outperforms existing ZSD models.

</details>


### [256] [Test-Time Canonicalization by Foundation Models for Robust Perception](https://arxiv.org/abs/2507.10375)
*Utkarsh Singhal,Ryan Feng,Stella X. Yu,Atul Prakash*

Main category: cs.CV

TL;DR: FOCAL是一个测试时、数据驱动的框架，利用基础模型的互联网规模视觉先验，通过生成和优化候选变换来实现鲁棒感知，无需重新训练或架构更改。


<details>
  <summary>Details</summary>
Motivation: 现实世界的视觉感知需要对多样化变换具有不变性，但现有方法依赖专用架构或预定义增强训练，限制了泛化能力。

Method: FOCAL通过生成和优化候选变换，将其导向视觉上典型的“规范”视图，从而提升鲁棒性。

Result: 实验表明，FOCAL显著提升了CLIP和SAM在2D/3D旋转、光照变化（对比度和颜色）及昼夜变化等挑战性变换下的鲁棒性。

Conclusion: FOCAL挑战了变换特定训练的必要性假设，提供了一种可扩展的不变性实现路径。

Abstract: Real-world visual perception requires invariance to diverse transformations,
yet current methods rely heavily on specialized architectures or training on
predefined augmentations, limiting generalization. We propose FOCAL, a
test-time, data-driven framework that achieves robust perception by leveraging
internet-scale visual priors from foundation models. By generating and
optimizing candidate transformations toward visually typical, "canonical"
views, FOCAL enhances robustness without re-training or architectural changes.
Our experiments demonstrate improved robustness of CLIP and SAM across
challenging transformations, including 2D/3D rotations, illumination shifts
(contrast and color), and day-night variations. We also highlight potential
applications in active vision. Our approach challenges the assumption that
transform-specific training is necessary, instead offering a scalable path to
invariance. Our code is available at: https://github.com/sutkarsh/focal.

</details>


### [257] [Improving Remote Sensing Classification using Topological Data Analysis and Convolutional Neural Networks](https://arxiv.org/abs/2507.10381)
*Aaryam Sharma*

Main category: cs.CV

TL;DR: 论文提出了一种将拓扑数据分析（TDA）特征与深度学习模型结合的方法，显著提升了遥感分类任务的性能。


<details>
  <summary>Details</summary>
Motivation: 解决卷积神经网络（CNN）偏向纹理特征的局限性，探索TDA在复杂数据集中的几何信息提取能力。

Method: 设计了一个TDA特征工程流程，并将其与ResNet18模型结合，用于遥感图像分类。

Result: 在EuroSAT数据集上达到99.33%的准确率，超过ResNet50和XL Vision Transformers；在RESISC45数据集上比基线高1.82%。

Conclusion: TDA特征可以与深度学习模型有效结合，扩展了TDA的应用范围，尤其是在无显式拓扑结构的数据集上。

Abstract: Topological data analysis (TDA) is a relatively new field that is gaining
rapid adoption due to its robustness and ability to effectively describe
complex datasets by quantifying geometric information. In imaging contexts, TDA
typically models data as filtered cubical complexes from which we can extract
discriminative features using persistence homology. Meanwhile, convolutional
neural networks (CNNs) have been shown to be biased towards texture based local
features. To address this limitation, we propose a TDA feature engineering
pipeline and a simple method to integrate topological features with deep
learning models on remote sensing classification. Our method improves the
performance of a ResNet18 model on the EuroSAT dataset by 1.44% achieving
99.33% accuracy, which surpasses all previously reported single-model
accuracies, including those with larger architectures, such as ResNet50 (2x
larger) and XL Vision Transformers (197x larger). We additionally show that our
method's accuracy is 1.82% higher than our ResNet18 baseline on the RESISC45
dataset. To our knowledge, this is the first application of TDA features in
satellite scene classification with deep learning. This demonstrates that TDA
features can be integrated with deep learning models, even on datasets without
explicit topological structures, thereby increasing the applicability of TDA. A
clean implementation of our method will be made publicly available upon
publication.

</details>


### [258] [Numerically Computing Galois Groups of Minimal Problems](https://arxiv.org/abs/2507.10407)
*Timothy Duff*

Main category: cs.CV

TL;DR: 探讨代数、数值计算和计算机视觉中参数化多项式方程组的求解问题，尤其是RanSaC模型中的应用。


<details>
  <summary>Details</summary>
Motivation: 解决计算机视觉中RanSaC模型所需的参数化多项式方程组的多实例求解问题。

Method: 分析参数化系统的固有求解难度，并提出实用解决方案。

Result: 过去五年的研究在测量求解难度和提出实用方法方面取得进展。

Conclusion: 该研究为参数化多项式方程组的求解提供了理论和实践基础。

Abstract: I discuss a seemingly unlikely confluence of topics in algebra, numerical
computation, and computer vision. The motivating problem is that of solving
multiples instances of a parametric family of systems of algebraic (polynomial
or rational function) equations. No doubt already of interest to ISSAC
attendees, this problem arises in the context of robust model-fitting paradigms
currently utilized by the computer vision community (namely "Random Sampling
and Consensus", aka "RanSaC".) This talk will give an overview of work in the
last 5+ years that aspires to measure the intrinsic difficulty of solving such
parametric systems, and makes strides towards practical solutions.

</details>


### [259] [Text-Visual Semantic Constrained AI-Generated Image Quality Assessment](https://arxiv.org/abs/2507.10432)
*Qiang Li,Qingsen Yan,Haojian Huang,Peng Wu,Haokui Zhang,Yanning Zhang*

Main category: cs.CV

TL;DR: 论文提出SC-AGIQA框架，通过文本-视觉语义约束改进AI生成图像的质量评估，解决语义对齐和细节感知问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法在评估AI生成图像时存在语义不对齐和细节感知缺失的问题，需要更全面的评估框架。

Method: 提出SC-AGIQA框架，包含文本辅助语义对齐模块（TSAM）和频域细粒度退化感知模块（FFDPM），结合多模态大语言模型和频域分析。

Result: 在多个基准数据集上，SC-AGIQA优于现有方法。

Conclusion: SC-AGIQA通过文本-视觉语义约束显著提升了AI生成图像的质量评估效果。

Abstract: With the rapid advancements in Artificial Intelligence Generated Image (AGI)
technology, the accurate assessment of their quality has become an increasingly
vital requirement. Prevailing methods typically rely on cross-modal models like
CLIP or BLIP to evaluate text-image alignment and visual quality. However, when
applied to AGIs, these methods encounter two primary challenges: semantic
misalignment and details perception missing. To address these limitations, we
propose Text-Visual Semantic Constrained AI-Generated Image Quality Assessment
(SC-AGIQA), a unified framework that leverages text-visual semantic constraints
to significantly enhance the comprehensive evaluation of both text-image
consistency and perceptual distortion in AI-generated images. Our approach
integrates key capabilities from multiple models and tackles the aforementioned
challenges by introducing two core modules: the Text-assisted Semantic
Alignment Module (TSAM), which leverages Multimodal Large Language Models
(MLLMs) to bridge the semantic gap by generating an image description and
comparing it against the original prompt for a refined consistency check, and
the Frequency-domain Fine-Grained Degradation Perception Module (FFDPM), which
draws inspiration from Human Visual System (HVS) properties by employing
frequency domain analysis combined with perceptual sensitivity weighting to
better quantify subtle visual distortions and enhance the capture of
fine-grained visual quality details in images. Extensive experiments conducted
on multiple benchmark datasets demonstrate that SC-AGIQA outperforms existing
state-of-the-art methods. The code is publicly available at
https://github.com/mozhu1/SC-AGIQA.

</details>


### [260] [4D-Animal: Freely Reconstructing Animatable 3D Animals from Videos](https://arxiv.org/abs/2507.10437)
*Shanshan Zhong,Jiawei Peng,Zehan Zheng,Zhongzhan Huang,Wufei Ma,Guofeng Zhang,Qihao Liu,Alan Yuille,Jieneng Chen*

Main category: cs.CV

TL;DR: 4D-Animal是一个无需稀疏关键点标注的视频到3D动物重建框架，通过密集特征网络和分层对齐策略提升重建效率和稳定性。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖稀疏语义关键点，但获取困难且不可靠，因此提出无需关键点标注的新方法。

Method: 引入密集特征网络将2D表示映射到SMAL参数，结合分层对齐策略（轮廓、部分、像素和时间线索）实现重建。

Result: 实验表明4D-Animal优于基于模型和无模型的基线方法，生成的高质量3D资源可应用于其他任务。

Conclusion: 4D-Animal为大规模应用提供了高效、稳定的3D动物重建解决方案。

Abstract: Existing methods for reconstructing animatable 3D animals from videos
typically rely on sparse semantic keypoints to fit parametric models. However,
obtaining such keypoints is labor-intensive, and keypoint detectors trained on
limited animal data are often unreliable. To address this, we propose
4D-Animal, a novel framework that reconstructs animatable 3D animals from
videos without requiring sparse keypoint annotations. Our approach introduces a
dense feature network that maps 2D representations to SMAL parameters,
enhancing both the efficiency and stability of the fitting process.
Furthermore, we develop a hierarchical alignment strategy that integrates
silhouette, part-level, pixel-level, and temporal cues from pre-trained 2D
visual models to produce accurate and temporally coherent reconstructions
across frames. Extensive experiments demonstrate that 4D-Animal outperforms
both model-based and model-free baselines. Moreover, the high-quality 3D assets
generated by our method can benefit other 3D tasks, underscoring its potential
for large-scale applications. The code is released at
https://github.com/zhongshsh/4D-Animal.

</details>


### [261] [CoralVQA: A Large-Scale Visual Question Answering Dataset for Coral Reef Image Understanding](https://arxiv.org/abs/2507.10449)
*Hongyong Han,Wei Wang,Gaowei Zhang,Mingjie Li,Yi Wang*

Main category: cs.CV

TL;DR: CoralVQA是首个用于珊瑚礁分析的大规模VQA数据集，包含12,805张真实珊瑚图像和277,653个问答对，旨在解决珊瑚监测中的领域特定和多维问题。


<details>
  <summary>Details</summary>
Motivation: 珊瑚礁是重要但脆弱的生态系统，需要持续监测以支持保护。现有的珊瑚图像解释需要领域专业知识，而VQA技术可以简化这一过程。

Method: 开发了半自动数据构建流程，与海洋生物学家合作，确保数据质量和可扩展性。

Result: CoralVQA为珊瑚礁图像中的视觉语言推理提供了全面基准，并揭示了现有LVLM的关键局限和机遇。

Conclusion: CoralVQA为未来LVLM的发展奠定了基础，特别支持珊瑚保护工作。

Abstract: Coral reefs are vital yet vulnerable ecosystems that require continuous
monitoring to support conservation. While coral reef images provide essential
information in coral monitoring, interpreting such images remains challenging
due to the need for domain expertise. Visual Question Answering (VQA), powered
by Large Vision-Language Models (LVLMs), has great potential in user-friendly
interaction with coral reef images. However, applying VQA to coral imagery
demands a dedicated dataset that addresses two key challenges: domain-specific
annotations and multidimensional questions. In this work, we introduce
CoralVQA, the first large-scale VQA dataset for coral reef analysis. It
contains 12,805 real-world coral images from 67 coral genera collected from 3
oceans, along with 277,653 question-answer pairs that comprehensively assess
ecological and health-related conditions. To construct this dataset, we develop
a semi-automatic data construction pipeline in collaboration with marine
biologists to ensure both scalability and professional-grade data quality.
CoralVQA presents novel challenges and provides a comprehensive benchmark for
studying vision-language reasoning in the context of coral reef images. By
evaluating several state-of-the-art LVLMs, we reveal key limitations and
opportunities. These insights form a foundation for future LVLM development,
with a particular emphasis on supporting coral conservation efforts.

</details>


### [262] [RefSTAR: Blind Facial Image Restoration with Reference Selection, Transfer, and Reconstruction](https://arxiv.org/abs/2507.10470)
*Zhicun Yin,Junjie Chen,Ming Liu,Zhixin Wang,Fan Li,Renjing Pei,Xiaoming Li,Rynson W. H. Lau,Wangmeng Zuo*

Main category: cs.CV

TL;DR: 提出了一种新的盲人脸图像恢复方法RefSTAR，通过参考图像选择、特征转移和重建，解决了身份保留问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法在身份保留上存在问题，主要由于对细节纹理的特征引入不当。

Method: 设计了参考选择模块（RefSel）、特征融合范式和重建机制，结合循环一致性损失。

Result: 在多种骨干模型上表现出色，身份保留能力和参考特征转移质量更好。

Conclusion: RefSTAR方法有效提升了盲人脸图像恢复的性能，代码和数据集已开源。

Abstract: Blind facial image restoration is highly challenging due to unknown complex
degradations and the sensitivity of humans to faces. Although existing methods
introduce auxiliary information from generative priors or high-quality
reference images, they still struggle with identity preservation problems,
mainly due to improper feature introduction on detailed textures. In this
paper, we focus on effectively incorporating appropriate features from
high-quality reference images, presenting a novel blind facial image
restoration method that considers reference selection, transfer, and
reconstruction (RefSTAR). In terms of selection, we construct a reference
selection (RefSel) module. For training the RefSel module, we construct a
RefSel-HQ dataset through a mask generation pipeline, which contains annotating
masks for 10,000 ground truth-reference pairs. As for the transfer, due to the
trivial solution in vanilla cross-attention operations, a feature fusion
paradigm is designed to force the features from the reference to be integrated.
Finally, we propose a reference image reconstruction mechanism that further
ensures the presence of reference image features in the output image. The cycle
consistency loss is also redesigned in conjunction with the mask. Extensive
experiments on various backbone models demonstrate superior performance,
showing better identity preservation ability and reference feature transfer
quality. Source code, dataset, and pre-trained models are available at
https://github.com/yinzhicun/RefSTAR.

</details>


### [263] [GT-Loc: Unifying When and Where in Images Through a Joint Embedding Space](https://arxiv.org/abs/2507.10473)
*David G. Shatwell,Ishan Rajendrakumar Dave,Sirnam Swetha,Mubarak Shah*

Main category: cs.CV

TL;DR: GT-Loc是一种新颖的检索方法，联合预测图像的捕获时间（小时和月份）和地理位置（GPS坐标），通过共享高维特征空间对齐嵌入，并在时间度量学习中引入循环目标。


<details>
  <summary>Details</summary>
Motivation: 解决时间戳预测与地理定位之间的相互依赖问题，提升图像时间预测的准确性。

Method: 采用图像、时间和位置的独立编码器，在共享特征空间中对齐嵌入，并提出循环时间度量学习目标。

Result: GT-Loc在时间预测和地理定位任务上均优于现有方法，且在组合和基于文本的图像检索中表现优异。

Conclusion: GT-Loc通过联合优化时间预测和地理定位，显著提升了性能，同时支持更广泛的应用场景。

Abstract: Timestamp prediction aims to determine when an image was captured using only
visual information, supporting applications such as metadata correction,
retrieval, and digital forensics. In outdoor scenarios, hourly estimates rely
on cues like brightness, hue, and shadow positioning, while seasonal changes
and weather inform date estimation. However, these visual cues significantly
depend on geographic context, closely linking timestamp prediction to
geo-localization. To address this interdependence, we introduce GT-Loc, a novel
retrieval-based method that jointly predicts the capture time (hour and month)
and geo-location (GPS coordinates) of an image. Our approach employs separate
encoders for images, time, and location, aligning their embeddings within a
shared high-dimensional feature space. Recognizing the cyclical nature of time,
instead of conventional contrastive learning with hard positives and negatives,
we propose a temporal metric-learning objective providing soft targets by
modeling pairwise time differences over a cyclical toroidal surface. We present
new benchmarks demonstrating that our joint optimization surpasses previous
time prediction methods, even those using the ground-truth geo-location as an
input during inference. Additionally, our approach achieves competitive results
on standard geo-localization tasks, and the unified embedding space facilitates
compositional and text-based image retrieval.

</details>


### [264] [The Power of Certainty: How Confident Models Lead to Better Segmentation](https://arxiv.org/abs/2507.10490)
*Tugberk Erol,Tuba Caglikantar,Duygu Sarikaya*

Main category: cs.CV

TL;DR: 提出了一种基于置信度的自蒸馏方法，用于息肉分割任务，减少资源消耗并提升性能。


<details>
  <summary>Details</summary>
Motivation: 解决深度学习模型在息肉检测和分割中因参数过多导致的过拟合和泛化能力差的问题。

Method: 采用动态置信系数计算批次内前后迭代的损失，仅需存储前一次迭代数据，无需额外计算或内存。

Result: 在多个临床中心的数据集上表现优于现有最优模型，泛化能力强。

Conclusion: 该方法高效且性能优越，代码将在论文接受后公开。

Abstract: Deep learning models have been proposed for automatic polyp detection and
precise segmentation of polyps during colonoscopy procedures. Although these
state-of-the-art models achieve high performance, they often require a large
number of parameters. Their complexity can make them prone to overfitting,
particularly when trained on biased datasets, and can result in poor
generalization across diverse datasets. Knowledge distillation and
self-distillation are proposed as promising strategies to mitigate the
limitations of large, over-parameterized models. These approaches, however, are
resource-intensive, often requiring multiple models and significant memory
during training. We propose a confidence-based self-distillation approach that
outperforms state-of-the-art models by utilizing only previous iteration data
storage during training, without requiring extra computation or memory usage
during testing. Our approach calculates the loss between the previous and
current iterations within a batch using a dynamic confidence coefficient. To
evaluate the effectiveness of our approach, we conduct comprehensive
experiments on the task of polyp segmentation. Our approach outperforms
state-of-the-art models and generalizes well across datasets collected from
multiple clinical centers. The code will be released to the public once the
paper is accepted.

</details>


### [265] [BenchReAD: A systematic benchmark for retinal anomaly detection](https://arxiv.org/abs/2507.10492)
*Chenyu Lian,Hong-Yu Zhou,Zhanli Hu,Jing Qin*

Main category: cs.CV

TL;DR: 该论文提出了一个全面的视网膜异常检测基准，解决了现有基准的局限性，并提出了NFM-DRA方法以提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有视网膜异常检测领域缺乏公开且全面的基准，导致实验设置不充分，且现有方法未能充分利用临床数据。

Method: 通过引入一个系统性基准，结合解耦异常表示（DRA）和正常特征记忆（NFM），提出NFM-DRA方法。

Result: NFM-DRA方法在性能上优于现有方法，但在面对未见异常时仍存在性能下降。

Conclusion: 论文提出的基准和方法为视网膜异常检测提供了更全面的评估框架，并展示了NFM-DRA的潜力。

Abstract: Retinal anomaly detection plays a pivotal role in screening ocular and
systemic diseases. Despite its significance, progress in the field has been
hindered by the absence of a comprehensive and publicly available benchmark,
which is essential for the fair evaluation and advancement of methodologies.
Due to this limitation, previous anomaly detection work related to retinal
images has been constrained by (1) a limited and overly simplistic set of
anomaly types, (2) test sets that are nearly saturated, and (3) a lack of
generalization evaluation, resulting in less convincing experimental setups.
Furthermore, existing benchmarks in medical anomaly detection predominantly
focus on one-class supervised approaches (training only with negative samples),
overlooking the vast amounts of labeled abnormal data and unlabeled data that
are commonly available in clinical practice. To bridge these gaps, we introduce
a benchmark for retinal anomaly detection, which is comprehensive and
systematic in terms of data and algorithm. Through categorizing and
benchmarking previous methods, we find that a fully supervised approach
leveraging disentangled representations of abnormalities (DRA) achieves the
best performance but suffers from significant drops in performance when
encountering certain unseen anomalies. Inspired by the memory bank mechanisms
in one-class supervised learning, we propose NFM-DRA, which integrates DRA with
a Normal Feature Memory to mitigate the performance degradation, establishing a
new SOTA. The benchmark is publicly available at
https://github.com/DopamineLcy/BenchReAD.

</details>


### [266] [Cameras as Relative Positional Encoding](https://arxiv.org/abs/2507.10496)
*Ruilong Li,Brent Yi,Junchen Liu,Hang Gao,Yi Ma,Angjoo Kanazawa*

Main category: cs.CV

TL;DR: 本文比较了多种方法在多视角Transformer中利用相机几何信息，提出了一种新的相对位置编码（PRoPE），并在多个任务中验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 多视角计算机视觉任务中，相机几何关系对3D感知至关重要，但现有方法未能充分利用这些信息。

Method: 比较了三种相机条件化技术：token级射线图编码、attention级相对位姿编码，以及新提出的PRoPE（捕捉完整的相机参数）。

Result: PRoPE在多个任务（新视角合成、立体深度估计等）中表现优于其他方法，且能泛化到不同相机参数和序列长度。

Conclusion: PRoPE是一种有效的相机条件化方法，适用于多视角Transformer，并能提升多种任务的性能。

Abstract: Transformers are increasingly prevalent for multi-view computer vision tasks,
where geometric relationships between viewpoints are critical for 3D
perception. To leverage these relationships, multi-view transformers must use
camera geometry to ground visual tokens in 3D space. In this work, we compare
techniques for conditioning transformers on cameras: token-level raymap
encodings, attention-level relative pose encodings, and a new relative encoding
we propose -- Projective Positional Encoding (PRoPE) -- that captures complete
camera frustums, both intrinsics and extrinsics, as a relative positional
encoding. Our experiments begin by showing how relative camera conditioning
improves performance in feedforward novel view synthesis, with further gains
from PRoPE. This holds across settings: scenes with both shared and varying
intrinsics, when combining token- and attention-level conditioning, and for
generalization to inputs with out-of-distribution sequence lengths and camera
intrinsics. We then verify that these benefits persist for different tasks,
stereo depth estimation and discriminative spatial cognition, as well as larger
model sizes.

</details>


### [267] [National level satellite-based crop field inventories in smallholder landscapes](https://arxiv.org/abs/2507.10499)
*Philippe Rufin,Pauline Lucie Hammer,Leon-Friedrich Thomas,Sá Nogueira Lisboa,Natasha Ribeiro,Almeida Sitoe,Patrick Hostert,Patrick Meyfroidt*

Main category: cs.CV

TL;DR: 该研究利用高分辨率地球观测数据和深度学习技术，首次为莫桑比克绘制了全国范围的农田边界图，揭示了小农农业系统的复杂性和多样性。


<details>
  <summary>Details</summary>
Motivation: 设计科学政策以提升小农农业可持续性时，缺乏对农田分布和规模等基本系统属性的理解，因此需要更精确的数据支持。

Method: 结合极高分辨率（1.5米）地球观测数据和深度迁移学习，以最小参考数据需求实现全国范围的农田边界划分。

Result: 提供了莫桑比克2023年2100万块农田的数据集，总体精度达93%，并揭示了农田规模与多种社会经济和环境因素的关系。

Conclusion: 农田规模是农业社会经济和环境结果（如粮食生产、生计、森林砍伐）的关键指标，研究为政策制定提供了重要依据。

Abstract: The design of science-based policies to improve the sustainability of
smallholder agriculture is challenged by a limited understanding of fundamental
system properties, such as the spatial distribution of active cropland and
field size. We integrate very high spatial resolution (1.5 m) Earth observation
data and deep transfer learning to derive crop field delineations in complex
agricultural systems at the national scale, while maintaining minimum reference
data requirements and enhancing transferability. We provide the first
national-level dataset of 21 million individual fields for Mozambique (covering
~800,000 km2) for 2023. Our maps separate active cropland from non-agricultural
land use with an overall accuracy of 93% and balanced omission and commission
errors. Field-level spatial agreement reached median intersection over union
(IoU) scores of 0.81, advancing the state-of-the-art in large-area field
delineation in complex smallholder systems. The active cropland maps capture
fragmented rural regions with low cropland shares not yet identified in global
land cover or cropland maps. These regions are mostly located in agricultural
frontier regions which host 7-9% of the Mozambican population. Field size in
Mozambique is very low overall, with half of the fields being smaller than 0.16
ha, and 83% smaller than 0.5 ha. Mean field size at aggregate spatial
resolution (0.05{\deg}) is 0.32 ha, but it varies strongly across gradients of
accessibility, population density, and net forest cover change. This variation
reflects a diverse set of actors, ranging from semi-subsistence smallholder
farms to medium-scale commercial farming, and large-scale farming operations.
Our results highlight that field size is a key indicator relating to
socio-economic and environmental outcomes of agriculture (e.g., food
production, livelihoods, deforestation, biodiversity), as well as their
trade-offs.

</details>


### [268] [Quantize-then-Rectify: Efficient VQ-VAE Training](https://arxiv.org/abs/2507.10547)
*Borui Zhang,Qihang Rao,Wenzhao Zheng,Jie Zhou,Jiwen Lu*

Main category: cs.CV

TL;DR: ReVQ框架通过预训练的VAE快速训练VQ-VAE，显著降低计算成本，同时保持高质量图像重建。


<details>
  <summary>Details</summary>
Motivation: 高压缩率VQ-VAE训练计算成本高，需探索高效替代方案。

Method: 提出ReVQ框架，结合通道多组量化和后矫正器，减少量化误差。

Result: ReVQ在单GPU上22小时完成训练，重建质量保持竞争力（rFID=1.06）。

Conclusion: ReVQ在效率和重建质量间取得优越平衡，显著降低训练成本。

Abstract: Visual tokenizers are pivotal in multimodal large models, acting as bridges
between continuous inputs and discrete tokens. Nevertheless, training
high-compression-rate VQ-VAEs remains computationally demanding, often
necessitating thousands of GPU hours. This work demonstrates that a pre-trained
VAE can be efficiently transformed into a VQ-VAE by controlling quantization
noise within the VAE's tolerance threshold. We present
\textbf{Quantize-then-Rectify (ReVQ)}, a framework leveraging pre-trained VAEs
to enable rapid VQ-VAE training with minimal computational overhead. By
integrating \textbf{channel multi-group quantization} to enlarge codebook
capacity and a \textbf{post rectifier} to mitigate quantization errors, ReVQ
compresses ImageNet images into at most 512 tokens while sustaining competitive
reconstruction quality (rFID = 1.06). Significantly, ReVQ reduces training
costs by over two orders of magnitude relative to state-of-the-art approaches:
ReVQ finishes full training on a single NVIDIA 4090 in approximately 22 hours,
whereas comparable methods require 4.5 days on 32 A100 GPUs. Experimental
results show that ReVQ achieves superior efficiency-reconstruction trade-offs.

</details>


### [269] [Self-supervised Learning on Camera Trap Footage Yields a Strong Universal Face Embedder](https://arxiv.org/abs/2507.10552)
*Vladimir Iashin,Horace Lee,Dan Schofield,Andrew Zisserman*

Main category: cs.CV

TL;DR: 提出了一种完全自监督的方法，利用DINOv2框架从无标签的相机陷阱视频中学习黑猩猩面部嵌入，无需身份标签，性能优于有监督基线。


<details>
  <summary>Details</summary>
Motivation: 解决野生动物监测中手动识别个体的瓶颈问题，探索自监督学习在生物多样性监测中的潜力。

Method: 使用DINOv2框架训练Vision Transformers，基于自动提取的面部图像进行自监督学习。

Result: 在Bossou等挑战性基准上表现出色，开放集重识别性能优于有监督方法。

Conclusion: 自监督学习在生物多样性监测中具有潜力，为可扩展、非侵入性种群研究提供了新途径。

Abstract: Camera traps are revolutionising wildlife monitoring by capturing vast
amounts of visual data; however, the manual identification of individual
animals remains a significant bottleneck. This study introduces a fully
self-supervised approach to learning robust chimpanzee face embeddings from
unlabeled camera-trap footage. Leveraging the DINOv2 framework, we train Vision
Transformers on automatically mined face crops, eliminating the need for
identity labels. Our method demonstrates strong open-set re-identification
performance, surpassing supervised baselines on challenging benchmarks such as
Bossou, despite utilising no labelled data during training. This work
underscores the potential of self-supervised learning in biodiversity
monitoring and paves the way for scalable, non-invasive population studies.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [270] [Recurrent Expansion: A Pathway Toward the Next Generation of Deep Learning](https://arxiv.org/abs/2507.08828)
*Tarek Berghout*

Main category: cs.LG

TL;DR: 本文提出了一种名为Recurrent Expansion (RE)的新学习范式，超越了传统的机器学习和深度学习，通过分析模型自身行为的演化实现自我改进。


<details>
  <summary>Details</summary>
Motivation: 传统深度学习专注于静态数据表示，而RE通过分析模型行为的动态变化，旨在实现模型的自我迭代改进。

Method: RE通过多次映射数据到相同的深度架构中，并结合内部表示（如特征图）和性能信号（如损失）来实现自我改进。进一步扩展为Multiverse RE (MVRE)和Heterogeneous MVRE (HMVRE)，后者引入不同架构的模型以提供多样化视角。

Result: RE框架实现了行为感知和自我演化的深度学习系统，为可扩展、自适应的智能模型奠定了基础。

Conclusion: RE标志着深度学习从静态表示学习转向行为感知和自我演化的系统，为未来智能模型的发展提供了新方向。

Abstract: This paper introduces Recurrent Expansion (RE) as a new learning paradigm
that advances beyond conventional Machine Learning (ML) and Deep Learning (DL).
While DL focuses on learning from static data representations, RE proposes an
additional dimension: learning from the evolving behavior of models themselves.
RE emphasizes multiple mappings of data through identical deep architectures
and analyzes their internal representations (i.e., feature maps) in conjunction
with observed performance signals such as loss. By incorporating these
behavioral traces, RE enables iterative self-improvement, allowing each model
version to gain insight from its predecessors. The framework is extended
through Multiverse RE (MVRE), which aggregates signals from parallel model
instances, and further through Heterogeneous MVRE (HMVRE), where models of
varying architectures contribute diverse perspectives. A scalable and adaptive
variant, Sc-HMVRE, introduces selective mechanisms and scale diversity for
real-world deployment. Altogether, RE presents a shift in DL: from purely
representational learning to behavior-aware, self-evolving systems. It lays the
groundwork for a new class of intelligent models capable of reasoning over
their own learning dynamics, offering a path toward scalable, introspective,
and adaptive artificial intelligence. A simple code example to support
beginners in running their own experiments is provided in Code Availability
Section of this paper.

</details>


### [271] [Efficient Triple Modular Redundancy for Reliability Enhancement of DNNs Using Explainable AI](https://arxiv.org/abs/2507.08829)
*Kimia Soroush,Nastaran Shirazi,Mohsen Raji*

Main category: cs.LG

TL;DR: 该论文提出了一种基于可解释人工智能（XAI）的高效三重模块冗余（TMR）方法，用于提升深度神经网络（DNN）在比特翻转故障下的可靠性。


<details>
  <summary>Details</summary>
Motivation: 在安全关键领域，DNN的可靠性至关重要。TMR虽能提升可靠性，但开销较大，因此需要选择性应用。

Method: 采用梯度可解释性方法（LRP）计算DNN参数的重要性分数，并选择关键权重应用TMR。

Result: 在VGG16和AlexNet模型上验证，显著提升了可靠性（如AlexNet在10-4误码率下可靠性提升60%）。

Conclusion: 该方法在保持开销不变的情况下，显著提升了DNN的可靠性。

Abstract: Deep Neural Networks (DNNs) are widely employed in safety-critical domains,
where ensuring their reliability is essential. Triple Modular Redundancy (TMR)
is an effective technique to enhance the reliability of DNNs in the presence of
bit-flip faults. In order to handle the significant overhead of TMR, it is
applied selectively on the parameters and components with the highest
contribution at the model output. Hence, the accuracy of the selection
criterion plays the key role on the efficiency of TMR. This paper presents an
efficient TMR approach to enhance the reliability of DNNs against bit-flip
faults using an Explainable Artificial Intelligence (XAI) method. Since XAI can
provide valuable insights about the importance of individual neurons and
weights in the performance of the network, they can be applied as the selection
metric in TMR techniques. The proposed method utilizes a low-cost,
gradient-based XAI technique known as Layer-wise Relevance Propagation (LRP) to
calculate importance scores for DNN parameters. These scores are then used to
enhance the reliability of the model, with the most critical weights being
protected by TMR. The proposed approach is evaluated on two DNN models, VGG16
and AlexNet, using datasets such as MNIST and CIFAR-10. The results demonstrate
that the method can protect the AlexNet model at a bit error rate of 10-4,
achieving over 60% reliability improvement while maintaining the same overhead
as state-of-the-art methods.

</details>


### [272] [A Hybrid Machine Learning Framework for Optimizing Crop Selection via Agronomic and Economic Forecasting](https://arxiv.org/abs/2507.08832)
*Niranjan Mallikarjun Sindhur,Pavithra C,Nivya Muchikel*

Main category: cs.LG

TL;DR: 论文提出了一种结合机器学习和人机交互的决策支持系统，帮助印度卡纳塔克邦的农民应对市场和气候波动，并通过语音界面解决识字障碍。


<details>
  <summary>Details</summary>
Motivation: 农民面临市场和气候波动，同时因识字障碍被排除在数字革命之外。

Method: 结合随机森林分类器和LSTM网络，预测作物适宜性和市场价格，并通过语音界面提供服务。

Result: 随机森林模型准确率达98.5%，LSTM模型价格预测误差低。

Conclusion: 该系统为边缘化农民社区提供了可扩展且高效的解决方案，增强其经济韧性。

Abstract: Farmers in developing regions like Karnataka, India, face a dual challenge:
navigating extreme market and climate volatility while being excluded from the
digital revolution due to literacy barriers. This paper presents a novel
decision support system that addresses both challenges through a unique
synthesis of machine learning and human-computer interaction. We propose a
hybrid recommendation engine that integrates two predictive models: a Random
Forest classifier to assess agronomic suitability based on soil, climate, and
real-time weather data, and a Long Short-Term Memory (LSTM) network to forecast
market prices for agronomically viable crops. This integrated approach shifts
the paradigm from "what can grow?" to "what is most profitable to grow?",
providing a significant advantage in mitigating economic risk. The system is
delivered through an end-to-end, voice-based interface in the local Kannada
language, leveraging fine-tuned speech recognition and high-fidelity speech
synthesis models to ensure accessibility for low-literacy users. Our results
show that the Random Forest model achieves 98.5% accuracy in suitability
prediction, while the LSTM model forecasts harvest-time prices with a low
margin of error. By providing data-driven, economically optimized
recommendations through an inclusive interface, this work offers a scalable and
impactful solution to enhance the financial resilience of marginalized farming
communities.

</details>


### [273] [LoRA Is Slower Than You Think](https://arxiv.org/abs/2507.08833)
*Seokmin Ko*

Main category: cs.LG

TL;DR: LoRA是一种广泛用于微调大语言模型的技术，通过引入少量可训练的低秩权重矩阵减少参数更新量，但在某些模型架构和训练设置中速度提升不一致。本文分析了LoRA性能并提出更高效的微调方法。


<details>
  <summary>Details</summary>
Motivation: LoRA在部分场景中未能提供一致的速度提升，作者希望探究其性能限制因素并提出改进方法。

Method: 对LoRA性能进行全面分析，提出更高效的微调方法，并进行实证评估。

Result: 新方法在性能上优于或与LoRA相当，同时提供更一致的训练速度提升。

Conclusion: 研究为资源受限下优化大语言模型微调提供了实用指南。

Abstract: Low-Rank Adaptation (LoRA) is one of the most widely used techniques for
fine-tuning large language models (LLMs). By introducing a small number of
trainable low-rank weight matrices, LoRA substantially reduces the number of
parameters that need to be updated, offering significant advantages in memory
consumption and computational efficiency compared to full fine-tuning. However,
we observed that LoRA does not consistently provide speed improvements across
all model architectures and training setups. Motivated by this inconsistency,
we conduct a comprehensive analysis of LoRA's performance and investigate the
underlying factors limiting its speedup. Based on our findings, we propose
several methods for more efficient fine-tuning of LLMs. We empirically evaluate
these methods and compare them to LoRA, demonstrating that our approach
achieves comparable or superior performance while delivering more consistent
training speed improvements. Our work offers valuable insights and practical
guidelines for practitioners seeking to optimize LLM fine-tuning under resource
constraints.

</details>


### [274] [Physical Informed Neural Networks for modeling ocean pollutant](https://arxiv.org/abs/2507.08834)
*Karishma Battina,Prathamesh Dinesh Joshi,Raj Abhijit Dandekar,Rajat Dandekar,Sreedath Panat*

Main category: cs.LG

TL;DR: 本文提出了一种基于物理信息神经网络（PINN）的框架，用于模拟污染物在二维平流-扩散方程下的扩散，解决了传统数值方法在大规模动态海洋域中的复杂性挑战。


<details>
  <summary>Details</summary>
Motivation: 传统数值方法在处理大规模动态海洋域中的污染物传输问题时面临复杂性和规模上的困难，因此需要一种更高效且物理一致的方法。

Method: 采用PINN框架，通过将物理定律嵌入神经网络训练过程，并结合有限差分法（FDM）生成的噪声合成数据，使用混合损失函数（包括PDE残差、边界/初始条件一致性和加权数据拟合项）进行训练。

Result: 该方法能够实现物理一致的预测，并有效处理非线性动力学和边界/初始条件的约束问题。

Conclusion: PINN框架为污染物扩散模拟提供了一种可扩展且灵活的替代方案，优于传统求解器。

Abstract: Traditional numerical methods often struggle with the complexity and scale of
modeling pollutant transport across vast and dynamic oceanic domains. This
paper introduces a Physics-Informed Neural Network (PINN) framework to simulate
the dispersion of pollutants governed by the 2D advection-diffusion equation.
The model achieves physically consistent predictions by embedding physical laws
and fitting to noisy synthetic data, generated via a finite difference method
(FDM), directly into the neural network training process. This approach
addresses challenges such as non-linear dynamics and the enforcement of
boundary and initial conditions. Synthetic data sets, augmented with varying
noise levels, are used to capture real-world variability. The training
incorporates a hybrid loss function including PDE residuals, boundary/initial
condition conformity, and a weighted data fit term. The approach takes
advantage of the Julia language scientific computing ecosystem for
high-performance simulations, offering a scalable and flexible alternative to
traditional solvers

</details>


### [275] [Representation learning with a transformer by contrastive learning for money laundering detection](https://arxiv.org/abs/2507.08835)
*Harold Guéneau,Alain Celisse,Pascal Delange*

Main category: cs.LG

TL;DR: 提出了一种基于Transformer神经网络的新方法，用于检测洗钱行为，通过对比学习生成时间序列表示，并结合Benjamini-Hochberg程序控制误报率。


<details>
  <summary>Details</summary>
Motivation: 解决传统基于规则或LSTM的洗钱检测方法在误报率和检测能力上的不足。

Method: 1. 通过对比学习无监督学习时间序列表示；2. 利用这些表示生成洗钱评分；3. 采用双阈值和BH程序控制误报率。

Result: Transformer能够生成通用表示，有效检测洗钱行为，同时控制误报率，优于基于规则或LSTM的方法。

Conclusion: 新方法在洗钱检测中表现出色，兼具高检测能力和低误报率。

Abstract: The present work tackles the money laundering detection problem. A new
procedure is introduced which exploits structured time series of both
qualitative and quantitative data by means of a transformer neural network. The
first step of this procedure aims at learning representations of time series
through contrastive learning (without any labels). The second step leverages
these representations to generate a money laundering scoring of all
observations. A two-thresholds approach is then introduced, which ensures a
controlled false-positive rate by means of the Benjamini-Hochberg (BH)
procedure. Experiments confirm that the transformer is able to produce general
representations that succeed in exploiting money laundering patterns with
minimal supervision from domain experts. It also illustrates the higher ability
of the new procedure for detecting nonfraudsters as well as fraudsters, while
keeping the false positive rate under control. This greatly contrasts with
rule-based procedures or the ones based on LSTM architectures.

</details>


### [276] [Accuracy and Consumption analysis from a compressed model by CompactifAI from Multiverse Computing](https://arxiv.org/abs/2507.08836)
*Damien Fovet,Shashank Chamoli,Sarah Oury,Srishti Singhal*

Main category: cs.LG

TL;DR: CompactifAI压缩方法应用于Llama 3.1 8B模型，显著降低计算资源消耗同时保持准确性。


<details>
  <summary>Details</summary>
Motivation: 评估CompactifAI压缩方法在大语言模型Llama 3.1 8B上的性能，以提高效率和降低成本。

Method: 使用Codecarbon和Ragas框架分别评估压缩模型的能源消耗和准确性，并与完整模型对比。

Result: 压缩后的模型显著减少计算资源，同时保持准确性，更具效率和成本效益。

Conclusion: CompactifAI压缩方法使模型更高效、可扩展且经济实惠。

Abstract: This study evaluates the performance of a compression method, called
CompactifAI, developed by Multiverse Computing, applied to the large language
model Llama 3.1 8B\cite{llama}. The evaluation focused on model efficiency (in
terms of energy consumption) and accuracy using respectively the frameworks
Codecarbon\cite{codecarbon} and Ragas\cite{ragas}. A comparison was performed
between the model compressed with
CompactifAI\cite{compactifai}\cite{compactifai2} and its full-size version. Our
findings reveal that the compressed model using CompactifAI not only
significantly reduced the computational resources but also maintained the model
accuracy, making the model more efficient, scalable and cost-effective.

</details>


### [277] [wd1: Weighted Policy Optimization for Reasoning in Diffusion Language Models](https://arxiv.org/abs/2507.08838)
*Xiaohang Tang,Rares Dolga,Sangwoong Yoon,Ilija Bogunovic*

Main category: cs.LG

TL;DR: 论文提出了一种名为wd1的新方法，通过加权似然优化扩散大语言模型的推理能力，避免了传统RL方法的计算开销和偏差问题。


<details>
  <summary>Details</summary>
Motivation: 扩散大语言模型（dLLMs）的推理能力提升是一个开放性问题，传统方法因似然函数难以处理而引入计算开销和潜在偏差。

Method: 提出wd1方法，将目标重新表述为加权似然，仅需对当前参数化策略似然进行一次近似。

Result: 在推理基准测试中，wd1无需监督微调或监督数据，性能优于现有RL方法，准确率提升高达16%，同时减少训练时间和计算开销。

Conclusion: wd1因其高效性和简单性，成为提升dLLMs推理能力的更优方法。

Abstract: Improving the reasoning capabilities of diffusion-based large language models
(dLLMs) through reinforcement learning (RL) remains an open problem. The
intractability of dLLMs likelihood function necessitates approximating the
current, old, and reference policy likelihoods at each policy optimization
step. This reliance introduces additional computational overhead and lead to
potentially large bias -- particularly when approximation errors occur in the
denominator of policy ratios used for importance sampling. To mitigate these
issues, we introduce $\mathtt{wd1}$, a novel policy optimization approach that
reformulates the objective as a weighted likelihood, requiring only a single
approximation for the current parametrized policy likelihood. Experiments on
widely used reasoning benchmarks demonstrate that $\mathtt{wd1}$, without
supervised fine-tuning (SFT) or any supervised data, outperforms existing RL
methods for dLLMs, achieving up to 16% higher accuracy. $\mathtt{wd1}$ delivers
additional computational gains, including reduced training time and fewer
function evaluations (NFEs) per gradient step. These findings, combined with
the simplicity of method's implementation and R1-Zero-like training (no SFT),
position $\mathtt{wd1}$ as a more effective and efficient method for applying
RL to dLLMs reasoning.

</details>


### [278] [Domain-Adaptive Diagnosis of Lewy Body Disease with Transferability Aware Transformer](https://arxiv.org/abs/2507.08839)
*Xiaowei Yu,Jing Zhang,Tong Chen,Yan Zhuang,Minheng Chen,Chao Cao,Yanjun Lyu,Lu Zhang,Li Su,Tianming Liu,Dajiang Zhu*

Main category: cs.LG

TL;DR: 提出了一种基于注意力机制的迁移学习方法（TAT），利用阿尔茨海默病（AD）数据增强路易体病（LBD）的诊断能力，解决了数据稀缺和领域偏移问题。


<details>
  <summary>Details</summary>
Motivation: 路易体病（LBD）是一种常见但研究不足的痴呆症，诊断面临数据稀缺和领域偏移的挑战，而AD数据丰富但存在领域差异。

Method: 使用结构MRI提取的结构连接性（SC）作为训练数据，通过TAT自适应分配权重，强调疾病可迁移特征，抑制领域特异性特征。

Result: 实验证明TAT能有效减少领域偏移，提高LBD诊断准确性。

Conclusion: TAT为数据稀缺和领域偏移条件下的罕见疾病诊断提供了有前景的框架。

Abstract: Lewy Body Disease (LBD) is a common yet understudied form of dementia that
imposes a significant burden on public health. It shares clinical similarities
with Alzheimer's disease (AD), as both progress through stages of normal
cognition, mild cognitive impairment, and dementia. A major obstacle in LBD
diagnosis is data scarcity, which limits the effectiveness of deep learning. In
contrast, AD datasets are more abundant, offering potential for knowledge
transfer. However, LBD and AD data are typically collected from different sites
using different machines and protocols, resulting in a distinct domain shift.
To effectively leverage AD data while mitigating domain shift, we propose a
Transferability Aware Transformer (TAT) that adapts knowledge from AD to
enhance LBD diagnosis. Our method utilizes structural connectivity (SC) derived
from structural MRI as training data. Built on the attention mechanism, TAT
adaptively assigns greater weights to disease-transferable features while
suppressing domain-specific ones, thereby reducing domain shift and improving
diagnostic accuracy with limited LBD data. The experimental results demonstrate
the effectiveness of TAT. To the best of our knowledge, this is the first study
to explore domain adaptation from AD to LBD under conditions of data scarcity
and domain shift, providing a promising framework for domain-adaptive diagnosis
of rare diseases.

</details>


### [279] [GUIDE: Towards Scalable Advising for Research Ideas](https://arxiv.org/abs/2507.08870)
*Yaowenqi Liu,BingXu Meng,Rui Pan,Jerry Huang,Tong Zhang*

Main category: cs.LG

TL;DR: 小型模型结合压缩文献库和结构化推理框架，在ICLR 2025的自我排名前30%提交中表现优于通用语言模型，高置信度预测的接受率超过90%。


<details>
  <summary>Details</summary>
Motivation: AI研究快速发展，但缺乏可扩展的咨询系统为假设和实验设计提供高质量反馈。

Method: 探索模型大小、上下文长度、置信度估计和结构化推理等关键因素，开发小型模型结合压缩文献库和结构化推理框架。

Result: 小型模型在ICLR 2025测试集上高置信度预测的接受率超过90%，表现优于通用语言模型。

Conclusion: 该系统显著提升假设生成和实验设计的质量与效率，代码已开源。

Abstract: The field of AI research is advancing at an unprecedented pace, enabling
automated hypothesis generation and experimental design across diverse domains
such as biology, mathematics, and artificial intelligence. Despite these
advancements, there remains a significant gap in the availability of scalable
advising systems capable of providing high-quality, well-reasoned feedback to
refine proposed hypotheses and experimental designs. To address this challenge,
we explore key factors that underlie the development of robust advising
systems, including model size, context length, confidence estimation, and
structured reasoning processes. Our findings reveal that a relatively small
model, when equipped with a well-compressed literature database and a
structured reasoning framework, can outperform powerful general-purpose
language models such as Deepseek-R1 in terms of acceptance rates for
self-ranked top-30% submissions to ICLR 2025. Moreover, when limited to
high-confidence predictions, our system achieves an acceptance rate exceeding
90% on the ICLR 2025 test set, underscoring its potential to significantly
enhance the quality and efficiency of hypothesis generation and experimental
design. The code is released at
https://github.com/HowardLiu0830/GUIDE-Research-Idea-Evaluation.

</details>


### [280] [Zero-Shot Neural Architecture Search with Weighted Response Correlation](https://arxiv.org/abs/2507.08841)
*Kun Jing,Luoyu Chen,Jungang Xu,Jianwei Tai,Yiyu Wang,Shuaimin Li*

Main category: cs.LG

TL;DR: 论文提出了一种名为WRCor的无训练代理方法，用于加速神经架构搜索（NAS），通过计算不同输入样本的响应相关性来评估架构的表达性和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有零样本NAS方法在有效性、稳定性和通用性方面存在不足，需要一种更高效的代理方法来加速架构评估。

Method: 提出加权响应相关性（WRCor）代理，利用不同输入样本的响应相关性矩阵计算代理分数。

Result: WRCor及其投票代理在评估中表现优于现有方法，NAS算法在多个搜索空间中优于现有算法，并在ImageNet-1k上实现了22.1%的测试错误率。

Conclusion: WRCor是一种高效、稳定的零样本NAS代理方法，显著提升了架构搜索的效率。

Abstract: Neural architecture search (NAS) is a promising approach for automatically
designing neural network architectures. However, the architecture estimation of
NAS is computationally expensive and time-consuming because of training
multiple architectures from scratch. Although existing zero-shot NAS methods
use training-free proxies to accelerate the architecture estimation, their
effectiveness, stability, and generality are still lacking. We present a novel
training-free estimation proxy called weighted response correlation (WRCor).
WRCor utilizes correlation coefficient matrices of responses across different
input samples to calculate the proxy scores of estimated architectures, which
can measure their expressivity and generalizability. Experimental results on
proxy evaluation demonstrate that WRCor and its voting proxies are more
efficient estimation strategies than existing proxies. We also apply them with
different search strategies in architecture search. Experimental results on
architecture search show that our zero-shot NAS algorithm outperforms most
existing NAS algorithms in different search spaces. Our NAS algorithm can
discover an architecture with a 22.1% test error on the ImageNet-1k dataset
within 4 GPU hours. All codes are publicly available at
https://github.com/kunjing96/ZSNAS-WRCor.git.

</details>


### [281] [Gradients as an Action: Towards Communication-Efficient Federated Recommender Systems via Adaptive Action Sharing](https://arxiv.org/abs/2507.08842)
*Zhufeng Lu,Chentao Jia,Ming Hu,Xiaofei Xie,Mingsong Chen*

Main category: cs.LG

TL;DR: FedRAS是一种高效的联邦推荐系统框架，通过动作共享策略和自适应聚类机制，显著降低通信开销并保持推荐性能。


<details>
  <summary>Details</summary>
Motivation: 联邦推荐系统（FedRecs）面临高通信开销和低训练效率的问题，现有压缩方法导致模型性能下降。

Method: 提出FedRAS框架，采用动作共享策略聚类梯度为有限动作进行通信，并结合自适应聚类机制适应异构环境。

Result: 实验表明，FedRAS通信负载减少高达96.88%，且在不同异构场景下保持推荐性能。

Conclusion: FedRAS有效解决了FedRecs的通信和效率问题，同时开源了框架。

Abstract: As a promising privacy-aware collaborative model training paradigm, Federated
Learning (FL) is becoming popular in the design of distributed recommender
systems. However, Federated Recommender Systems (FedRecs) greatly suffer from
two major problems: i) extremely high communication overhead due to massive
item embeddings involved in recommendation systems, and ii) intolerably low
training efficiency caused by the entanglement of both heterogeneous network
environments and client devices. Although existing methods attempt to employ
various compression techniques to reduce communication overhead, due to the
parameter errors introduced by model compression, they inevitably suffer from
model performance degradation. To simultaneously address the above problems,
this paper presents a communication-efficient FedRec framework named FedRAS,
which adopts an action-sharing strategy to cluster the gradients of item
embedding into a specific number of model updating actions for communication
rather than directly compressing the item embeddings. In this way, the cloud
server can use the limited actions from clients to update all the items. Since
gradient values are significantly smaller than item embeddings, constraining
the directions of gradients (i.e., the action space) introduces smaller errors
compared to compressing the entire item embedding matrix into a reduced space.
To accommodate heterogeneous devices and network environments, FedRAS
incorporates an adaptive clustering mechanism that dynamically adjusts the
number of actions. Comprehensive experiments on well-known datasets demonstrate
that FedRAS can reduce the size of communication payloads by up to 96.88%,
while not sacrificing recommendation performance within various heterogeneous
scenarios. We have open-sourced FedRAS at
https://github.com/mastlab-T3S/FedRAS.

</details>


### [282] [Can We Predict Your Next Move Without Breaking Your Privacy?](https://arxiv.org/abs/2507.08843)
*Arpita Soni,Sahil Tripathi,Gautam Siddharth Kashyap,Manaswi Kulahara,Mohammad Anas Azeez,Zohaib Hasan Siddiqui,Nipun Joshi,Jiechao Gao*

Main category: cs.LG

TL;DR: FLLL3M是一个隐私保护的联邦学习框架，利用大语言模型进行移动建模，实现高精度且低资源消耗的下一个位置预测。


<details>
  <summary>Details</summary>
Motivation: 解决用户数据隐私问题，同时通过联邦学习和大语言模型提升预测性能。

Method: 采用联邦学习框架，保留用户数据本地化，通过高效的外积机制利用大语言模型。

Result: 在多个数据集上取得SOT结果（如Gowalla的Acc@1: 12.55），参数减少45.6%，内存使用降低52.7%。

Conclusion: FLLL3M在隐私保护和性能优化方面表现出色，适用于下一个位置预测任务。

Abstract: We propose FLLL3M--Federated Learning with Large Language Models for Mobility
Modeling--a privacy-preserving framework for Next-Location Prediction (NxLP).
By retaining user data locally and leveraging LLMs through an efficient outer
product mechanism, FLLL3M ensures high accuracy with low resource demands. It
achieves SOT results on Gowalla (Acc@1: 12.55, MRR: 0.1422), WeePlace (10.71,
0.1285), Brightkite (10.42, 0.1169), and FourSquare (8.71, 0.1023), while
reducing parameters by up to 45.6% and memory usage by 52.7%.

</details>


### [283] [DAFOS: Dynamic Adaptive Fanout Optimization Sampler](https://arxiv.org/abs/2507.08845)
*Irfan Ullah,Young-Koo Lee*

Main category: cs.LG

TL;DR: 提出了一种动态调整采样策略的GNN训练方法DAFOS，显著提升了训练速度和精度。


<details>
  <summary>Details</summary>
Motivation: 解决GNN中静态采样策略导致的扩展性和效率限制。

Method: 动态调整采样扇出，基于节点重要性评分和早期停止机制。

Result: 在多个数据集上实现显著速度提升和精度改进。

Conclusion: DAFOS为大规模GNN训练提供高效可扩展的解决方案。

Abstract: Graph Neural Networks (GNNs) are becoming an essential tool for learning from
graph-structured data, however uniform neighbor sampling and static fanout
settings frequently limit GNNs' scalability and efficiency. In this paper, we
propose the Dynamic Adaptive Fanout Optimization Sampler (DAFOS), a novel
approach that dynamically adjusts the fanout based on model performance and
prioritizes important nodes during training. Our approach leverages node
scoring based on node degree to focus computational resources on structurally
important nodes, incrementing the fanout as the model training progresses.
DAFOS also integrates an early stopping mechanism to halt training when
performance gains diminish. Experiments conducted on three benchmark datasets,
ogbnarxiv, Reddit, and ogbn-products, demonstrate that our approach
significantly improves training speed and accuracy compared to a
state-of-the-art approach. DAFOS achieves a 3.57x speedup on the ogbn-arxiv
dataset and a 12.6x speedup on the Reddit dataset while improving the F1 score
from 68.5% to 71.21% on ogbn-arxiv and from 73.78% to 76.88% on the
ogbn-products dataset, respectively. These results highlight the potential of
DAFOS as an efficient and scalable solution for large-scale GNN training.

</details>


### [284] [Assuring the Safety of Reinforcement Learning Components: AMLAS-RL](https://arxiv.org/abs/2507.08848)
*Calum Corrie Imrie,Ioannis Stefanakos,Sepeedeh Shahbeigi,Richard Hawkins,Simon Burton*

Main category: cs.LG

TL;DR: 本文提出AMLAS-RL框架，通过迭代过程为强化学习系统生成安全保证论证，解决了现有方法在RL生命周期中系统性保证不足的问题。


<details>
  <summary>Details</summary>
Motivation: 随着机器学习在CPS中的广泛应用，强化学习因其适应复杂动态环境的能力而受到青睐，但安全性和保证性挑战亟待解决。

Method: 作者将AMLAS方法扩展到RL领域，提出AMLAS-RL框架，通过迭代过程生成安全保证论证。

Result: 通过轮式车辆避障任务的案例研究，验证了AMLAS-RL框架的有效性。

Conclusion: AMLAS-RL为RL系统提供了一种系统化的安全保证方法，填补了现有方法的不足。

Abstract: The rapid advancement of machine learning (ML) has led to its increasing
integration into cyber-physical systems (CPS) across diverse domains. While CPS
offer powerful capabilities, incorporating ML components introduces significant
safety and assurance challenges. Among ML techniques, reinforcement learning
(RL) is particularly suited for CPS due to its capacity to handle complex,
dynamic environments where explicit models of interaction between system and
environment are unavailable or difficult to construct. However, in
safety-critical applications, this learning process must not only be effective
but demonstrably safe. Safe-RL methods aim to address this by incorporating
safety constraints during learning, yet they fall short in providing systematic
assurance across the RL lifecycle. The AMLAS methodology offers structured
guidance for assuring the safety of supervised learning components, but it does
not directly apply to the unique challenges posed by RL. In this paper, we
adapt AMLAS to provide a framework for generating assurance arguments for an
RL-enabled system through an iterative process; AMLAS-RL. We demonstrate
AMLAS-RL using a running example of a wheeled vehicle tasked with reaching a
target goal without collision.

</details>


### [285] [Foundation models for time series forecasting: Application in conformal prediction](https://arxiv.org/abs/2507.08858)
*Sami Achour,Yassine Bouher,Duong Nguyen,Nicolas Chesneau*

Main category: cs.LG

TL;DR: 比较时间序列基础模型（TSFMs）与传统方法在共形预测中的表现，发现TSFMs在数据有限时提供更可靠的预测区间，且校准过程更稳定。


<details>
  <summary>Details</summary>
Motivation: 探索基础模型在时间序列预测中的零样本能力，特别是在共形预测中，如何利用更多数据进行校准以提高可靠性。

Method: 在共形预测框架下，比较TSFMs与传统统计模型和梯度提升方法的性能，分析数据量对结果的影响。

Result: TSFMs在数据有限时表现优于传统方法，提供更可靠的预测区间和更稳定的校准过程，数据越少优势越明显。

Conclusion: 基础模型在时间序列共形预测中具有潜力，尤其在数据受限的情况下，能显著提升预测可靠性。

Abstract: The zero-shot capabilities of foundation models (FMs) for time series
forecasting offer promising potentials in conformal prediction, as most of the
available data can be allocated to calibration. This study compares the
performance of Time Series Foundation Models (TSFMs) with traditional methods,
including statistical models and gradient boosting, within a conformal
prediction setting. Our findings highlight two key advantages of TSFMs. First,
when the volume of data is limited, TSFMs provide more reliable conformalized
prediction intervals than classic models, thanks to their superior predictive
accuracy. Second, the calibration process is more stable because more data are
used for calibration. Morever, the fewer data available, the more pronounced
these benefits become, as classic models require a substantial amount of data
for effective training. These results underscore the potential of foundation
models in improving conformal prediction reliability in time series
applications, particularly in data-constrained cases. All the code to reproduce
the experiments is available.

</details>


### [286] [e-Profits: A Business-Aligned Evaluation Metric for Profit-Sensitive Customer Churn Prediction](https://arxiv.org/abs/2507.08860)
*Awais Manzoor,M. Atif Qureshi,Etain Kidney,Luca Longo*

Main category: cs.LG

TL;DR: 论文提出了一种名为e-Profits的新评估指标，用于客户关系管理中的流失预测模型，该指标基于客户价值、保留概率和干预成本，优于传统指标如AUC和F1-score。


<details>
  <summary>Details</summary>
Motivation: 传统指标（如AUC和F1-score）无法反映财务结果，可能导致战略决策失误，因此需要一种更贴近业务目标的评估方法。

Method: e-Profits利用Kaplan-Meier生存分析估计个性化保留率，支持细粒度的客户级评估，并与六种分类器在电信数据集上进行了对比。

Result: e-Profits改变了模型排名，揭示了传统指标忽略的财务优势，并为高价值客户提供了投资回报最大化的细分洞察。

Conclusion: e-Profits是一种易于理解的工具，适用于业务场景中的模型评估，尤其适合注重利润驱动的营销和分析团队。

Abstract: Retention campaigns in customer relationship management often rely on churn
prediction models evaluated using traditional metrics such as AUC and F1-score.
However, these metrics fail to reflect financial outcomes and may mislead
strategic decisions. We introduce e-Profits, a novel business-aligned
evaluation metric that quantifies model performance based on customer-specific
value, retention probability, and intervention costs. Unlike existing
profit-based metrics such as Expected Maximum Profit, which assume fixed
population-level parameters, e-Profits uses Kaplan-Meier survival analysis to
estimate personalised retention rates and supports granular, per customer
evaluation. We benchmark six classifiers across two telecom datasets (IBM Telco
and Maven Telecom) and demonstrate that e-Profits reshapes model rankings
compared to traditional metrics, revealing financial advantages in models
previously overlooked by AUC or F1-score. The metric also enables segment-level
insight into which models maximise return on investment for high-value
customers. e-Profits is designed as an understandable, post hoc tool to support
model evaluation in business contexts, particularly for marketing and analytics
teams prioritising profit-driven decisions. All source code is available at:
https://github.com/matifq/eprofits.

</details>


### [287] [On the under-reaching phenomenon in message-passing neural PDE solvers: revisiting the CFL condition](https://arxiv.org/abs/2507.08861)
*Lucas Tesan,Mikel M. Iparraguirre,David Gonzalez,Pedro Martins,Elias Cueto*

Main category: cs.LG

TL;DR: 提出了图神经网络（GNN）在求解偏微分方程（PDE）时消息传递迭代次数的严格下界，减少了超参数调优的需求。


<details>
  <summary>Details</summary>
Motivation: 通过研究PDE的物理特性与GNN消息传递需求之间的关系，为三类基本PDE（双曲、抛物和椭圆）提供理论支持。

Method: 将PDE的物理常数、时空离散化与GNN消息传递机制关联，推导出迭代次数的下界。

Result: 当迭代次数低于下界时，信息传播效率低，解质量差；满足下界时，GNN能准确捕捉现象，得到高精度解。

Conclusion: 提出的下界在四个方程示例中验证了其严格性，为GNN在PDE求解中的应用提供了理论指导。

Abstract: This paper proposes sharp lower bounds for the number of message passing
iterations required in graph neural networks (GNNs) when solving partial
differential equations (PDE). This significantly reduces the need for
exhaustive hyperparameter tuning. Bounds are derived for the three fundamental
classes of PDEs (hyperbolic, parabolic and elliptic) by relating the physical
characteristics of the problem in question to the message-passing requirement
of GNNs. In particular, we investigate the relationship between the physical
constants of the equations governing the problem, the spatial and temporal
discretisation and the message passing mechanisms in GNNs.
  When the number of message passing iterations is below these proposed limits,
information does not propagate efficiently through the network, resulting in
poor solutions, even for deep GNN architectures. In contrast, when the
suggested lower bound is satisfied, the GNN parameterisation allows the model
to accurately capture the underlying phenomenology, resulting in solvers of
adequate accuracy.
  Examples are provided for four different examples of equations that show the
sharpness of the proposed lower bounds.

</details>


### [288] [Underrepresentation, Label Bias, and Proxies: Towards Data Bias Profiles for the EU AI Act and Beyond](https://arxiv.org/abs/2507.08866)
*Marina Ceccon,Giandomenico Cornacchia,Davide Dalle Pezze,Alessandro Fabris,Gian Antonio Susto*

Main category: cs.LG

TL;DR: 研究探讨数据偏见对算法歧视的影响，提出数据偏见配置文件（DBP）用于检测和缓解偏见。


<details>
  <summary>Details</summary>
Motivation: 数据偏见是算法歧视的主要驱动力，但研究不足，缺乏检测和缓解的最佳实践。

Method: 分析三种常见数据偏见的单独和联合影响，开发DBP机制检测偏见。

Result: 发现弱势群体在训练集中的代表性不足对歧视的影响较小，而代理和标签偏见的组合更关键。

Conclusion: DBP能有效预测歧视风险，为算法公平研究和反歧视政策提供数据支持。

Abstract: Undesirable biases encoded in the data are key drivers of algorithmic
discrimination. Their importance is widely recognized in the algorithmic
fairness literature, as well as legislation and standards on
anti-discrimination in AI. Despite this recognition, data biases remain
understudied, hindering the development of computational best practices for
their detection and mitigation. In this work, we present three common data
biases and study their individual and joint effect on algorithmic
discrimination across a variety of datasets, models, and fairness measures. We
find that underrepresentation of vulnerable populations in training sets is
less conducive to discrimination than conventionally affirmed, while
combinations of proxies and label bias can be far more critical. Consequently,
we develop dedicated mechanisms to detect specific types of bias, and combine
them into a preliminary construct we refer to as the Data Bias Profile (DBP).
This initial formulation serves as a proof of concept for how different bias
signals can be systematically documented. Through a case study with popular
fairness datasets, we demonstrate the effectiveness of the DBP in predicting
the risk of discriminatory outcomes and the utility of fairness-enhancing
interventions. Overall, this article bridges algorithmic fairness research and
anti-discrimination policy through a data-centric lens.

</details>


### [289] [Next-Generation Travel Demand Modeling with a Generative Framework for Household Activity Coordination](https://arxiv.org/abs/2507.08871)
*Xishun Liao,Haoxuan Ma,Yifan Liu,Yuxiang Wei,Brian Yueshuai He,Chris Stanford,Jiaqi Ma*

Main category: cs.LG

TL;DR: 本文提出了一种基于学习的旅行需求建模框架，通过整合人口合成、活动生成、位置分配和大规模微观交通模拟，实现了数据驱动、可扩展且可迁移的模型。在洛杉矶的实验中，该模型显著降低了建模成本并提高了可扩展性，同时准确复制了真实世界的移动模式。


<details>
  <summary>Details</summary>
Motivation: 传统基于活动的模型（ABMs）虽然基于行为理论，但依赖简化的规则和假设，开发成本高且难以跨区域适应。本文旨在通过数据驱动的方法解决这些问题。

Method: 提出了一个统一的框架，整合人口合成、协调活动生成、位置分配和大规模微观交通模拟，生成基于家庭社会人口特征的日常活动模式。

Result: 模型在洛杉矶的实验中表现出色，与真实移动模式高度吻合，且性能优于传统ABMs。具体指标如余弦相似度为0.97，JSD和MAPE均表现优异。

Conclusion: 该框架为旅行需求建模提供了一种高效、可扩展且可迁移的解决方案，显著降低了建模成本并提高了准确性。

Abstract: Travel demand models are critical tools for planning, policy, and mobility
system design. Traditional activity-based models (ABMs), although grounded in
behavioral theories, often rely on simplified rules and assumptions, and are
costly to develop and difficult to adapt across different regions. This paper
presents a learning-based travel demand modeling framework that synthesizes
household-coordinated daily activity patterns based on a household's
socio-demographic profiles. The whole framework integrates population
synthesis, coordinated activity generation, location assignment, and
large-scale microscopic traffic simulation into a unified system. It is fully
generative, data-driven, scalable, and transferable to other regions. A
full-pipeline implementation is conducted in Los Angeles with a 10 million
population. Comprehensive validation shows that the model closely replicates
real-world mobility patterns and matches the performance of legacy ABMs with
significantly reduced modeling cost and greater scalability. With respect to
the SCAG ABM benchmark, the origin-destination matrix achieves a cosine
similarity of 0.97, and the daily vehicle miles traveled (VMT) in the network
yields a 0.006 Jensen-Shannon Divergence (JSD) and a 9.8% mean absolute
percentage error (MAPE). When compared to real-world observations from Caltrans
PeMS, the evaluation on corridor-level traffic speed and volume reaches a 0.001
JSD and a 6.11% MAPE.

</details>


### [290] [Contrastive Language-Image Pre-Training Model based Semantic Communication Performance Optimization](https://arxiv.org/abs/2507.08873)
*Shaoran Yang,Dongyu Wei,Hanzhi Yu,Zhaohui Yang,Yuchen Liu,Mingzhe Chen*

Main category: cs.LG

TL;DR: 提出了一种基于CLIP模型的语义通信框架，无需训练即可提取数据语义，并通过强化学习优化模型和资源分配，显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 传统基于神经网络的语义通信需要联合训练，而CLIP模型无需训练即可提取语义，同时无线网络中的噪声和资源限制需要优化。

Method: 使用CLIP模型提取语义信息，并通过PPO强化学习算法优化模型架构和频谱资源分配。

Result: 仿真结果表明，该方法比软演员-评论家算法收敛速度提升40%，累积奖励提高4倍。

Conclusion: 基于CLIP的语义通信框架在无线网络中表现优异，为语义通信提供了高效解决方案。

Abstract: In this paper, a novel contrastive language-image pre-training (CLIP) model
based semantic communication framework is designed. Compared to standard neural
network (e.g.,convolutional neural network) based semantic encoders and
decoders that require joint training over a common dataset, our CLIP model
based method does not require any training procedures thus enabling a
transmitter to extract data meanings of the original data without neural
network model training, and the receiver to train a neural network for
follow-up task implementation without the communications with the transmitter.
Next, we investigate the deployment of the CLIP model based semantic framework
over a noisy wireless network. Since the semantic information generated by the
CLIP model is susceptible to wireless noise and the spectrum used for semantic
information transmission is limited, it is necessary to jointly optimize CLIP
model architecture and spectrum resource block (RB) allocation to maximize
semantic communication performance while considering wireless noise, the delay
and energy used for semantic communication. To achieve this goal, we use a
proximal policy optimization (PPO) based reinforcement learning (RL) algorithm
to learn how wireless noise affect the semantic communication performance thus
finding optimal CLIP model and RB for each user. Simulation results show that
our proposed method improves the convergence rate by up to 40%, and the
accumulated reward by 4x compared to soft actor-critic.

</details>


### [291] [An Automated Classifier of Harmful Brain Activities for Clinical Usage Based on a Vision-Inspired Pre-trained Framework](https://arxiv.org/abs/2507.08874)
*Yulin Sun,Xiaopeng Si,Runnan He,Xiao Hu,Peter Smielewski,Wenlong Wang,Xiaoguang Tong,Wei Yue,Meijun Pang,Kuo Zhang,Xizi Song,Dong Ming,Xiuyun Liu*

Main category: cs.LG

TL;DR: VIPEEGNet是一种卷积神经网络模型，用于通过EEG及时识别有害脑活动，在多个类别中表现出高准确性和与人类专家相当的性能。


<details>
  <summary>Details</summary>
Motivation: 现有AI模型在脑疾病诊断中存在评分者间差异、资源限制和泛化能力差的问题，VIPEEGNet旨在解决这些限制。

Method: 使用来自1950名患者的106,800个EEG片段开发VIPEEGNet，并在独立测试集中验证其性能。

Result: VIPEEGNet在二元分类和多分类任务中均表现出高准确性（AUROC高达0.972），性能与人类专家相当，且参数效率高。

Conclusion: VIPEEGNet是一种高效且准确的EEG分析工具，有望在临床中推广应用。

Abstract: Timely identification of harmful brain activities via electroencephalography
(EEG) is critical for brain disease diagnosis and treatment, which remains
limited application due to inter-rater variability, resource constraints, and
poor generalizability of existing artificial intelligence (AI) models. In this
study, a convolutional neural network model, VIPEEGNet, was developed and
validated using EEGs recorded from Massachusetts General Hospital/Harvard
Medical School. The VIPEEGNet was developed and validated using two independent
datasets, collected between 2006 and 2020. The development cohort included EEG
recordings from 1950 patients, with 106,800 EEG segments annotated by at least
one experts (ranging from 1 to 28). The online testing cohort consisted of EEG
segments from a subset of an additional 1,532 patients, each annotated by at
least 10 experts. For the development cohort (n=1950), the VIPEEGNet achieved
high accuracy, with an AUROC for binary classification of seizure, LPD, GPD,
LRDA, GRDA, and "other" categories at 0.972 (95% CI, 0.957-0.988), 0.962 (95%
CI, 0.954-0.970), 0.972 (95% CI, 0.960-0.984), 0.938 (95% CI, 0.917-0.959),
0.949 (95% CI, 0.941-0.957), and 0.930 (95% CI, 0.926-0.935). For multi
classification, the sensitivity of VIPEEGNET for the six categories ranges from
36.8% to 88.2% and the precision ranges from 55.6% to 80.4%, and performance
similar to human experts. Notably, the external validation showed
Kullback-Leibler Divergence (KLD)of 0.223 and 0.273, ranking top 2 among the
existing 2,767 competing algorithms, while we only used 2.8% of the parameters
of the first-ranked algorithm.

</details>


### [292] [ODIA: Oriented Distillation for Inline Acceleration of LLM-based Function Calling](https://arxiv.org/abs/2507.08877)
*Hanlong Zhang,Jingsheng Yang,Hao Li,Yuhao He,Franck Gong*

Main category: cs.LG

TL;DR: 论文提出了一种名为ODIA的新方法，通过在线用户交互数据加速LLM的函数调用，显著降低延迟。


<details>
  <summary>Details</summary>
Motivation: LLM函数调用的高延迟影响用户体验，需要一种高效且自动化的解决方案。

Method: 利用生产流量中的“简单查询”，通过知识蒸馏将大模型的能力迁移到小模型，减少延迟。

Result: 响应延迟降低45%（预期）和78%（中位数），小模型能处理60%流量且精度损失可忽略。

Conclusion: ODIA是一种实用且自动化的方法，适用于生产环境，持续优化性能。

Abstract: Function Calling is a crucial technique that enables Large Language Models
(LLMs) to interact with external systems through APIs. However, the high
latency associated with LLM-based Function Calling significantly impacts user
experience. This paper presents a novel approach called Oriented Distillation
for Inline Acceleration (ODIA) that leverages online user interaction data to
accelerate Function Calling. By automatically identifying "simple queries" from
production traffic and distilling knowledge from larger models to smaller ones,
our method reduces response latency by 45% (expected) and 78% (median) while
maintaining accuracy. We demonstrate the effectiveness of our approach through
real-world deployment in a music application, where the smaller model
successfully handles 60% of traffic with negligible accuracy loss. Our method
requires minimal human intervention and continuously improves through automated
data collection and model updating, making it a practical solution for
production environments.

</details>


### [293] [Knowing When to Quit: Probabilistic Early Exits for Speech Separation](https://arxiv.org/abs/2507.09768)
*Kenny Falkær Olsen. Mads Østergaard,Karl Ulbæk,Søren Føns Nielsen,Rasmus Malik Høegh Lindrup,Bjørn Sand Jensen,Morten Mørup*

Main category: cs.LG

TL;DR: 论文提出了一种支持早期退出的神经网络架构，用于单通道语音分离，并通过概率框架实现动态计算资源分配，达到与固定计算预算模型相竞争的性能。


<details>
  <summary>Details</summary>
Motivation: 解决现有语音分离模型无法适应不同计算需求和资源限制的问题，特别是在嵌入式设备上的应用。

Method: 设计了一种支持早期退出的神经网络架构，并提出不确定性感知的概率框架，用于联合建模干净语音信号和误差方差，从而推导出基于信噪比的早期退出条件。

Result: 实验表明，该模型在语音分离和增强任务中表现优异，能够与多种计算预算下的最先进模型竞争。

Conclusion: 该框架实现了语音分离网络的动态计算资源分配，同时保持了高性能和可解释的退出条件。

Abstract: In recent years, deep learning-based single-channel speech separation has
improved considerably, in large part driven by increasingly compute- and
parameter-efficient neural network architectures. Most such architectures are,
however, designed with a fixed compute and parameter budget, and consequently
cannot scale to varying compute demands or resources, which limits their use in
embedded and heterogeneous devices such as mobile phones and hearables. To
enable such use-cases we design a neural network architecture for speech
separation capable of early-exit, and we propose an uncertainty-aware
probabilistic framework to jointly model the clean speech signal and error
variance which we use to derive probabilistic early-exit conditions in terms of
desired signal-to-noise ratios. We evaluate our methods on both speech
separation and enhancement tasks, and we show that a single early-exit model
can be competitive with state-of-the-art models trained at many compute and
parameter budgets. Our framework enables fine-grained dynamic compute-scaling
of speech separation networks while achieving state-of-the-art performance and
interpretable exit conditions.

</details>


### [294] [Last Layer Hamiltonian Monte Carlo](https://arxiv.org/abs/2507.08905)
*Koen Vellenga,H. Joe Steinhauer,Göran Falkman,Jonas Andersson,Anders Sjögren*

Main category: cs.LG

TL;DR: 论文提出了一种基于哈密顿蒙特卡洛（HMC）采样的深度神经网络（DNN）概率最后一层方法（LL-HMC），以降低计算成本，并在真实视频数据集上验证其性能。


<details>
  <summary>Details</summary>
Motivation: HMC虽为不确定性估计的金标准，但其计算成本限制了其在大规模数据和大型DNN中的应用。LL-HMC通过仅对最后一层进行采样，解决了这一问题。

Method: LL-HMC将HMC采样限制在DNN的最后一层，减少了计算需求。论文在三个真实视频数据集上比较了LL-HMC与其他五种概率深度学习方法。

Result: LL-HMC在分布内分类和分布外检测方面表现优异，但额外的采样参数仅对OOD检测有提升，多链或起始位置未带来一致改进。

Conclusion: LL-HMC是一种计算高效且性能优越的概率最后一层方法，适用于资源受限的场景。

Abstract: We explore the use of Hamiltonian Monte Carlo (HMC) sampling as a
probabilistic last layer approach for deep neural networks (DNNs). While HMC is
widely regarded as a gold standard for uncertainty estimation, the
computational demands limit its application to large-scale datasets and large
DNN architectures. Although the predictions from the sampled DNN parameters can
be parallelized, the computational cost still scales linearly with the number
of samples (similar to an ensemble). Last layer HMC (LL--HMC) reduces the
required computations by restricting the HMC sampling to the final layer of a
DNN, making it applicable to more data-intensive scenarios with limited
computational resources. In this paper, we compare LL-HMC against five last
layer probabilistic deep learning (LL-PDL) methods across three real-world
video datasets for driver action and intention. We evaluate the in-distribution
classification performance, calibration, and out-of-distribution (OOD)
detection. Due to the stochastic nature of the probabilistic evaluations, we
performed five grid searches for different random seeds to avoid being reliant
on a single initialization for the hyperparameter configurations. The results
show that LL--HMC achieves competitive in-distribution classification and OOD
detection performance. Additional sampled last layer parameters do not improve
the classification performance, but can improve the OOD detection. Multiple
chains or starting positions did not yield consistent improvements.

</details>


### [295] [Fair-FLIP: Fair Deepfake Detection with Fairness-Oriented Final Layer Input Prioritising](https://arxiv.org/abs/2507.08912)
*Tomasz Szandala,Fatima Ezzeddine,Natalia Rusin,Silvia Giordano,Omran Ayoub*

Main category: cs.LG

TL;DR: 论文提出了一种名为Fair-FLIP的后处理方法，旨在减少深度伪造检测中的偏见，同时保持检测性能。


<details>
  <summary>Details</summary>
Motivation: 深度伪造检测方法虽然性能高，但存在对种族和性别等人口属性的偏见，威胁公众信任。

Method: 提出Fair-FLIP方法，通过重新加权模型最后一层的输入，减少子群体间的差异。

Result: 实验显示，Fair-FLIP将公平性指标提升30%，同时基线准确率仅下降0.25%。

Conclusion: Fair-FLIP有效平衡了公平性和检测性能，为深度伪造检测提供了新思路。

Abstract: Artificial Intelligence-generated content has become increasingly popular,
yet its malicious use, particularly the deepfakes, poses a serious threat to
public trust and discourse. While deepfake detection methods achieve high
predictive performance, they often exhibit biases across demographic attributes
such as ethnicity and gender. In this work, we tackle the challenge of fair
deepfake detection, aiming to mitigate these biases while maintaining robust
detection capabilities. To this end, we propose a novel post-processing
approach, referred to as Fairness-Oriented Final Layer Input Prioritising
(Fair-FLIP), that reweights a trained model's final-layer inputs to reduce
subgroup disparities, prioritising those with low variability while demoting
highly variable ones. Experimental results comparing Fair-FLIP to both the
baseline (without fairness-oriented de-biasing) and state-of-the-art approaches
show that Fair-FLIP can enhance fairness metrics by up to 30% while maintaining
baseline accuracy, with only a negligible reduction of 0.25%.
  Code is available on Github:
https://github.com/szandala/fair-deepfake-detection-toolbox

</details>


### [296] [Revisiting Convergence: Shuffling Complexity Beyond Lipschitz Smoothness](https://arxiv.org/abs/2507.08913)
*Qi He,Peiran Yu,Ziyi Chen,Heng Huang*

Main category: cs.LG

TL;DR: 论文研究了无需Lipschitz平滑假设的随机洗牌梯度方法，提出了新的步长策略，证明了其在非凸、强凸和非强凸情况下的收敛性，并通过实验验证了其性能。


<details>
  <summary>Details</summary>
Motivation: 现有随机洗牌梯度方法的收敛性分析大多依赖Lipschitz平滑条件，但许多机器学习模型不满足该条件，因此需要研究更通用的收敛性理论。

Method: 提出一种新的步长策略，分析随机洗牌梯度方法在无Lipschitz平滑假设下的收敛性，覆盖非凸、强凸和非强凸情况，并支持随机和任意洗牌方案。

Result: 证明了该方法在更弱假设下的收敛性，且收敛速率与当前最优结果匹配，数值实验验证了其实际性能。

Conclusion: 该研究拓宽了随机洗牌梯度方法的适用性，为不满足Lipschitz平滑条件的模型提供了理论支持。

Abstract: Shuffling-type gradient methods are favored in practice for their simplicity
and rapid empirical performance. Despite extensive development of convergence
guarantees under various assumptions in recent years, most require the
Lipschitz smoothness condition, which is often not met in common machine
learning models. We highlight this issue with specific counterexamples. To
address this gap, we revisit the convergence rates of shuffling-type gradient
methods without assuming Lipschitz smoothness. Using our stepsize strategy, the
shuffling-type gradient algorithm not only converges under weaker assumptions
but also match the current best-known convergence rates, thereby broadening its
applicability. We prove the convergence rates for nonconvex, strongly convex,
and non-strongly convex cases, each under both random reshuffling and arbitrary
shuffling schemes, under a general bounded variance condition. Numerical
experiments further validate the performance of our shuffling-type gradient
algorithm, underscoring its practical efficacy.

</details>


### [297] [Beyond Scores: Proximal Diffusion Models](https://arxiv.org/abs/2507.08956)
*Zhenghan Fang,Mateo Díaz,Sam Buchanan,Jeremias Sulam*

Main category: cs.LG

TL;DR: 本文提出了一种基于反向离散化的扩散模型（ProxDM），利用近端映射替代分数匹配，证明了其在理论和实践上的优势。


<details>
  <summary>Details</summary>
Motivation: 传统扩散模型依赖分数匹配进行采样，但存在效率问题。本文探索反向离散化和近端映射的潜力，以提高采样效率和准确性。

Method: 通过反向离散化随机微分方程（SDE），使用近端映射替代分数匹配，提出ProxDM模型。利用近端匹配技术学习对数密度的近端算子。

Result: 理论证明ProxDM仅需$\widetilde{O}(d/\sqrt{\varepsilon})$步即可生成KL散度$\varepsilon$-准确的分布。实验显示ProxDM在少量采样步骤内收敛速度显著优于传统分数匹配方法。

Conclusion: ProxDM通过反向离散化和近端映射，在理论和实践中均表现出高效性和准确性，为扩散模型提供了新的优化方向。

Abstract: Diffusion models have quickly become some of the most popular and powerful
generative models for high-dimensional data. The key insight that enabled their
development was the realization that access to the score -- the gradient of the
log-density at different noise levels -- allows for sampling from data
distributions by solving a reverse-time stochastic differential equation (SDE)
via forward discretization, and that popular denoisers allow for unbiased
estimators of this score. In this paper, we demonstrate that an alternative,
backward discretization of these SDEs, using proximal maps in place of the
score, leads to theoretical and practical benefits. We leverage recent results
in proximal matching to learn proximal operators of the log-density and, with
them, develop Proximal Diffusion Models (ProxDM). Theoretically, we prove that
$\widetilde{O}(d/\sqrt{\varepsilon})$ steps suffice for the resulting
discretization to generate an $\varepsilon$-accurate distribution w.r.t. the KL
divergence. Empirically, we show that two variants of ProxDM achieve
significantly faster convergence within just a few sampling steps compared to
conventional score-matching methods.

</details>


### [298] [Graph Neural Network Enhanced Sequential Recommendation Method for Cross-Platform Ad Campaign](https://arxiv.org/abs/2507.08959)
*Xiang Li,Xinyu Wang,Yifan Lin*

Main category: cs.LG

TL;DR: 提出了一种基于图神经网络（GNN）的跨平台广告推荐方法，通过多维建模提升推荐准确性。


<details>
  <summary>Details</summary>
Motivation: 解决跨平台广告推荐中用户兴趣迁移的捕捉问题，以提高推荐准确性。

Method: 利用GNN对用户行为数据、广告内容和平台特征进行多维建模，捕捉兴趣迁移路径。

Result: 在三个平台数据集上测试，Platform B的AUC值达到0.937，表现最佳；Platform A和C因广告标签分布不均，精度和召回率略有下降。通过调整超参数，模型在异构数据中的适应性和鲁棒性进一步提升。

Conclusion: 该方法能有效捕捉跨平台用户兴趣迁移，提升广告推荐准确性，尤其在数据分布不均时通过超参数调整优化性能。

Abstract: In order to improve the accuracy of cross-platform advertisement
recommendation, a graph neural network (GNN)- based advertisement
recommendation method is analyzed. Through multi-dimensional modeling, user
behavior data (e.g., click frequency, active duration) reveal temporal patterns
of interest evolution, ad content (e.g., type, tag, duration) influences
semantic preferences, and platform features (e.g., device type, usage context)
shape the environment where interest transitions occur. These factors jointly
enable the GNN to capture the latent pathways of user interest migration across
platforms. The experimental results are based on the datasets of three
platforms, and Platform B reaches 0.937 in AUC value, which is the best
performance. Platform A and Platform C showed a slight decrease in precision
and recall with uneven distribution of ad labels. By adjusting the
hyperparameters such as learning rate, batch size and embedding dimension, the
adaptability and robustness of the model in heterogeneous data are further
improved.

</details>


### [299] [Theory-Informed Improvements to Classifier-Free Guidance for Discrete Diffusion Models](https://arxiv.org/abs/2507.08965)
*Kevin Rojas,Ye He,Chieh-Hsin Lai,Yuta Takida,Yuki Mitsufuji,Molei Tao*

Main category: cs.LG

TL;DR: 本文分析了离散扩散模型中无分类器引导（CFG）的作用，发现早期高引导会损害生成质量，而晚期引导影响更大。提出了一种新的CFG机制，通过简单代码改进提升样本质量。


<details>
  <summary>Details</summary>
Motivation: 研究离散扩散模型中CFG的引导调度对生成质量的影响，并解决当前实现中的不平衡过渡问题。

Method: 理论分析CFG在掩码离散扩散中的作用，提出一种新的引导机制，平滑数据分布与初始分布之间的传输。

Result: 实验证明新方法在ImageNet和QM9数据集上有效提升了样本质量。

Conclusion: 新CFG机制通过简单调整解决了当前实现的问题，显著改善了离散扩散模型的生成质量。

Abstract: Classifier-Free Guidance (CFG) is a widely used technique for conditional
generation and improving sample quality in continuous diffusion models, and
recent works have extended it to discrete diffusion. This paper theoretically
analyzes CFG in the context of masked discrete diffusion, focusing on the role
of guidance schedules. Our analysis shows that high guidance early in sampling
(when inputs are heavily masked) harms generation quality, while late-stage
guidance has a larger effect. These findings provide a theoretical explanation
for empirical observations in recent studies on guidance schedules. The
analysis also reveals an imperfection of the current CFG implementations. These
implementations can unintentionally cause imbalanced transitions, such as
unmasking too rapidly during the early stages of generation, which degrades the
quality of the resulting samples. To address this, we draw insight from the
analysis and propose a novel classifier-free guidance mechanism empirically
applicable to any discrete diffusion. Intuitively, our method smoothens the
transport between the data distribution and the initial (masked/uniform)
distribution, which results in improved sample quality. Remarkably, our method
is achievable via a simple one-line code change. The efficacy of our method is
empirically demonstrated with experiments on ImageNet (masked discrete
diffusion) and QM9 (uniform discrete diffusion).

</details>


### [300] [ToxBench: A Binding Affinity Prediction Benchmark with AB-FEP-Calculated Labels for Human Estrogen Receptor Alpha](https://arxiv.org/abs/2507.08966)
*Meng Liu,Karl Leswing,Simon K. S. Chu,Farhad Ramezanghorbani,Griffin Young,Gabriel Marques,Prerna Das,Anjali Panikar,Esther Jamir,Mohammed Sulaiman Shamsudeen,K. Shawn Watts,Ananya Sen,Hari Priya Devannagari,Edward B. Miller,Muyun Lihan,Howook Hwang,Janet Paulsen,Xin Yu,Kyle Gion,Timur Rvachov,Emine Kucukbenli,Saee Gopal Paliwal*

Main category: cs.LG

TL;DR: ToxBench是一个大规模AB-FEP数据集，用于机器学习开发，专注于人类雌激素受体α（ERα），包含8,770个复合物结构，并验证了ML模型DualBind的优越性能。


<details>
  <summary>Details</summary>
Motivation: 蛋白质-配体结合亲和力预测对药物发现和毒性评估至关重要，但机器学习受限于数据可靠性，而物理方法（如AB-FEP）计算成本高。

Method: 引入ToxBench数据集，包含AB-FEP计算的结合自由能，并开发DualBind模型，采用双损失框架学习结合能函数。

Result: DualBind在基准测试中表现优越，展示了ML以低成本近似AB-FEP的潜力。

Conclusion: ToxBench和DualBind为结合亲和力预测提供了高效工具，弥补了数据与计算成本之间的鸿沟。

Abstract: Protein-ligand binding affinity prediction is essential for drug discovery
and toxicity assessment. While machine learning (ML) promises fast and accurate
predictions, its progress is constrained by the availability of reliable data.
In contrast, physics-based methods such as absolute binding free energy
perturbation (AB-FEP) deliver high accuracy but are computationally prohibitive
for high-throughput applications. To bridge this gap, we introduce ToxBench,
the first large-scale AB-FEP dataset designed for ML development and focused on
a single pharmaceutically critical target, Human Estrogen Receptor Alpha
(ER$\alpha$). ToxBench contains 8,770 ER$\alpha$-ligand complex structures with
binding free energies computed via AB-FEP with a subset validated against
experimental affinities at 1.75 kcal/mol RMSE, along with non-overlapping
ligand splits to assess model generalizability. Using ToxBench, we further
benchmark state-of-the-art ML methods, and notably, our proposed DualBind
model, which employs a dual-loss framework to effectively learn the binding
energy function. The benchmark results demonstrate the superior performance of
DualBind and the potential of ML to approximate AB-FEP at a fraction of the
computational cost.

</details>


### [301] [Simulating Three-dimensional Turbulence with Physics-informed Neural Networks](https://arxiv.org/abs/2507.08972)
*Sifan Wang,Shyam Sankaran,Panos Stinis,Paris Perdikaris*

Main category: cs.LG

TL;DR: PINNs通过物理方程直接训练神经网络，成功模拟湍流，无需传统计算网格或训练数据。


<details>
  <summary>Details</summary>
Motivation: 传统湍流模拟计算资源需求高，PINNs提供了一种连续、无网格的解决方案。

Method: 结合自适应网络架构、因果训练和高级优化方法，直接学习流体方程的解。

Result: PINNs准确再现了湍流的关键统计量，如能谱、动能、涡度和雷诺应力。

Conclusion: 神经方程求解器能处理复杂混沌系统，为湍流建模开辟新途径。

Abstract: Turbulent fluid flows are among the most computationally demanding problems
in science, requiring enormous computational resources that become prohibitive
at high flow speeds. Physics-informed neural networks (PINNs) represent a
radically different approach that trains neural networks directly from physical
equations rather than data, offering the potential for continuous, mesh-free
solutions. Here we show that appropriately designed PINNs can successfully
simulate fully turbulent flows in both two and three dimensions, directly
learning solutions to the fundamental fluid equations without traditional
computational grids or training data. Our approach combines several algorithmic
innovations including adaptive network architectures, causal training, and
advanced optimization methods to overcome the inherent challenges of learning
chaotic dynamics. Through rigorous validation on challenging turbulence
problems, we demonstrate that PINNs accurately reproduce key flow statistics
including energy spectra, kinetic energy, enstrophy, and Reynolds stresses. Our
results demonstrate that neural equation solvers can handle complex chaotic
systems, opening new possibilities for continuous turbulence modeling that
transcends traditional computational limitations.

</details>


### [302] [Simulation as Supervision: Mechanistic Pretraining for Scientific Discovery](https://arxiv.org/abs/2507.08977)
*Carson Dudley,Reiden Magdaleno,Christopher Harding,Marisa Eisenberg*

Main category: cs.LG

TL;DR: SGNNs结合机械模拟和神经网络，解决传统模型在复杂性和灵活性上的不足，实现高精度预测和可解释性。


<details>
  <summary>Details</summary>
Motivation: 传统机械模型可解释但适应性差，机器学习模型灵活但依赖大数据且缺乏可解释性。SGNNs旨在结合两者优势。

Method: SGNNs利用机械模拟生成合成数据训练神经网络，覆盖多样结构和参数。

Result: SGNNs在多个领域表现卓越，如COVID-19预测、化学产量估计和生态预测，优于传统方法。

Conclusion: SGNNs统一科学理论与深度学习，提供灵活且可解释的建模新范式。

Abstract: Scientific modeling faces a core limitation: mechanistic models offer
interpretability but collapse under real-world complexity, while machine
learning models are flexible but require large labeled datasets, cannot infer
unobservable quantities, and operate as black boxes. We introduce
Simulation-Grounded Neural Networks (SGNNs), a general framework that uses
mechanistic simulations as training data for neural networks. SGNNs are
pretrained on synthetic corpora spanning diverse model structures, parameter
regimes, stochasticity, and observational artifacts. We evaluated SGNNs across
scientific disciplines and modeling tasks, and found that SGNNs achieved
state-of-the-art results across settings: for prediction tasks, they nearly
tripled COVID-19 forecasting skill versus CDC baselines, reduced chemical yield
prediction error by one third, and maintained accuracy in ecological
forecasting where task specific models failed. For inference tasks, SGNNs also
accurately classified the source of information spread in simulated social
networks and enabled supervised learning for unobservable targets, such as
estimating COVID-19 transmissibility more accurately than traditional methods
even in early outbreaks. Finally, SGNNs enable back-to-simulation attribution,
a new form of mechanistic interpretability. Given real world input, SGNNs
retrieve simulations based on what the model has learned to see as most
similar, revealing which underlying dynamics the model believes are active.
This provides process-level insight -- what the model thinks is happening --
not just which features mattered. SGNNs unify scientific theory with deep
learning flexibility and unlock a new modeling paradigm -- transforming
simulations from rigid, post hoc tools into flexible sources of supervision,
enabling robust, interpretable inference even when ground truth is missing.

</details>


### [303] [Learning Diffusion Models with Flexible Representation Guidance](https://arxiv.org/abs/2507.08980)
*Chenyu Wang,Cai Zhou,Sharut Gupta,Zongyu Lin,Stefanie Jegelka,Stephen Bates,Tommi Jaakkola*

Main category: cs.LG

TL;DR: 提出了一种系统性框架，通过表示引导改进扩散模型，实验证明其性能优越且训练加速。


<details>
  <summary>Details</summary>
Motivation: 通过预训练模型的内部表示对齐提升扩散模型的生成质量。

Method: 引入两种新策略：多模态对联合建模和优化训练课程。

Result: 在图像、蛋白质序列和分子生成任务中表现优越，训练速度显著提升。

Conclusion: 表示引导策略有效提升了扩散模型的性能和训练效率。

Abstract: Diffusion models can be improved with additional guidance towards more
effective representations of input. Indeed, prior empirical work has already
shown that aligning internal representations of the diffusion model with those
of pre-trained models improves generation quality. In this paper, we present a
systematic framework for incorporating representation guidance into diffusion
models. We provide alternative decompositions of denoising models along with
their associated training criteria, where the decompositions determine when and
how the auxiliary representations are incorporated. Guided by our theoretical
insights, we introduce two new strategies for enhancing representation
alignment in diffusion models. First, we pair examples with target
representations either derived from themselves or arisen from different
synthetic modalities, and subsequently learn a joint model over the multimodal
pairs. Second, we design an optimal training curriculum that balances
representation learning and data generation. Our experiments across image,
protein sequence, and molecule generation tasks demonstrate superior
performance as well as accelerated training. In particular, on the
class-conditional ImageNet $256\times 256$ benchmark, our guidance results in
$23.3$ times faster training than the original SiT-XL as well as four times
speedup over the state-of-the-art method REPA. The code is available at
https://github.com/ChenyuWang-Monica/REED.

</details>


### [304] [Exploiting Leaderboards for Large-Scale Distribution of Malicious Models](https://arxiv.org/abs/2507.08983)
*Anshuman Suri,Harsh Chaudhari,Yuefeng Peng,Ali Naseh,Amir Houmansadr,Alina Oprea*

Main category: cs.LG

TL;DR: 论文揭示了模型排行榜作为大规模分发中毒模型的潜在渠道，提出了TrojanClimb框架，展示了其在多种模态中的有效性，并呼吁改进排行榜评估机制。


<details>
  <summary>Details</summary>
Motivation: 研究动机是探索对手如何利用模型排行榜大规模分发中毒模型，填补了现有研究的空白。

Method: 方法是通过TrojanClimb框架，在保持排行榜竞争力的同时注入恶意行为，并在四种模态中验证其有效性。

Result: 结果显示对手可以在排行榜上获得高排名，同时嵌入有害功能，如后门和偏见注入。

Conclusion: 结论是指出机器学习生态系统的重大漏洞，呼吁重新设计排行榜评估机制以检测恶意模型，并警示未经验证模型的风险。

Abstract: While poisoning attacks on machine learning models have been extensively
studied, the mechanisms by which adversaries can distribute poisoned models at
scale remain largely unexplored. In this paper, we shed light on how model
leaderboards -- ranked platforms for model discovery and evaluation -- can
serve as a powerful channel for adversaries for stealthy large-scale
distribution of poisoned models. We present TrojanClimb, a general framework
that enables injection of malicious behaviors while maintaining competitive
leaderboard performance. We demonstrate its effectiveness across four diverse
modalities: text-embedding, text-generation, text-to-speech and text-to-image,
showing that adversaries can successfully achieve high leaderboard rankings
while embedding arbitrary harmful functionalities, from backdoors to bias
injection. Our findings reveal a significant vulnerability in the machine
learning ecosystem, highlighting the urgent need to redesign leaderboard
evaluation mechanisms to detect and filter malicious (e.g., poisoned) models,
while exposing broader security implications for the machine learning community
regarding the risks of adopting models from unverified sources.

</details>


### [305] [Multimodal Cardiovascular Risk Profiling Using Self-Supervised Learning of Polysomnography](https://arxiv.org/abs/2507.09009)
*Zhengxiao He,Huayu Li,Geng Yuan,William D. S. Killgore,Stuart F. Quan,Chen X. Chen,Ao Li*

Main category: cs.LG

TL;DR: 开发了一种自监督深度学习模型，从多模态信号（EEG、ECG和呼吸信号）中提取有意义模式，用于预测心血管疾病（CVD）风险。


<details>
  <summary>Details</summary>
Motivation: 通过多模态信号分析，提升心血管疾病风险的预测能力，支持个性化医疗。

Method: 使用自监督深度学习模型，训练数据来自4,398名参与者，通过对比有无CVD结果的嵌入生成投影分数，并在独立队列（1,093人）中验证。

Result: 投影分数揭示了临床有意义的模式，ECG特征预测CVD死亡率，EEG特征预测高血压和CVD死亡率，呼吸信号提供补充预测价值。结合Framingham风险评分，预测性能显著提升（AUC 0.607-0.965）。

Conclusion: 该框架可直接从PSG数据生成个体化CVD风险评分，有望整合到临床实践中，提升风险评估和个性化护理。

Abstract: Methods: We developed a self-supervised deep learning model that extracts
meaningful patterns from multi-modal signals (Electroencephalography (EEG),
Electrocardiography (ECG), and respiratory signals). The model was trained on
data from 4,398 participants. Projection scores were derived by contrasting
embeddings from individuals with and without CVD outcomes. External validation
was conducted in an independent cohort with 1,093 participants. The source code
is available on https://github.com/miraclehetech/sleep-ssl. Results: The
projection scores revealed distinct and clinically meaningful patterns across
modalities. ECG-derived features were predictive of both prevalent and incident
cardiac conditions, particularly CVD mortality. EEG-derived features were
predictive of incident hypertension and CVD mortality. Respiratory signals
added complementary predictive value. Combining these projection scores with
the Framingham Risk Score consistently improved predictive performance,
achieving area under the curve values ranging from 0.607 to 0.965 across
different outcomes. Findings were robustly replicated and validated in the
external testing cohort. Conclusion: Our findings demonstrate that the proposed
framework can generate individualized CVD risk scores directly from PSG data.
The resulting projection scores have the potential to be integrated into
clinical practice, enhancing risk assessment and supporting personalized care.

</details>


### [306] [Enhancing RLHF with Human Gaze Modeling](https://arxiv.org/abs/2507.09016)
*Karim Galliamov,Ivan Titov,Ilya Pershin*

Main category: cs.LG

TL;DR: 利用人类注视建模增强RLHF，通过注视感知奖励模型和基于注视的稀疏奖励分布，提高收敛速度并降低计算成本。


<details>
  <summary>Details</summary>
Motivation: RLHF计算成本高，探索利用人类注视信号提升效率。

Method: 提出两种方法：注视感知奖励模型和基于注视的稀疏奖励分布。

Result: 实验显示注视增强的RLHF收敛更快，性能持平或略优，降低计算成本。

Conclusion: 人类注视是政策优化中未充分利用的有价值信号，为提升RLHF效率指明方向。

Abstract: Reinforcement Learning from Human Feedback (RLHF) aligns language models with
human preferences but is computationally expensive. We explore two approaches
that leverage human gaze modeling to enhance RLHF: (1) gaze-aware reward models
and (2) gaze-based distribution of sparse rewards at token level. Our
experiments demonstate that gaze-informed RLHF achieves faster convergence
while maintaining or slightly improving performance, thus, reducing
computational costs during policy optimization. These results show that human
gaze provides a valuable and underused signal for policy optimization, pointing
to a promising direction for improving RLHF efficiency.

</details>


### [307] [On Evaluating Performance of LLM Inference Serving Systems](https://arxiv.org/abs/2507.09019)
*Amey Agrawal,Nitin Kedia,Anmol Agarwal,Jayashree Mohan,Nipun Kwatra,Souvik Kundu,Ramachandran Ramjee,Alexey Tumanov*

Main category: cs.LG

TL;DR: 论文指出当前大型语言模型（LLM）推理系统的评估方法存在缺陷，提出了识别和避免常见评估反模式的框架。


<details>
  <summary>Details</summary>
Motivation: 当前LLM推理系统的评估方法存在缺陷，导致性能特征被掩盖，阻碍科学进步。

Method: 通过分析近期系统，识别了基线公平性、评估设置和指标设计三个维度的反模式，并提出了一个检查清单框架。

Result: 展示了如何应用框架分析推测解码技术，避免误导性结论。

Conclusion: 建立了一个严谨的评估方法论基础，促进LLM推理系统的真实进步。

Abstract: The rapid evolution of Large Language Model (LLM) inference systems has
yielded significant efficiency improvements. However, our systematic analysis
reveals that current evaluation methodologies frequently exhibit fundamental
flaws, often manifesting as common evaluation anti-patterns that obscure true
performance characteristics and impede scientific progress. Through a
comprehensive examination of recent systems, we identify recurring
anti-patterns across three key dimensions: Baseline Fairness, Evaluation Setup,
and Metric Design. These anti-patterns are uniquely problematic for LLM
inference due to its dual-phase nature combining distinct prefill and decode
operations, its handling of highly heterogeneous workloads, and its strict
temporal requirements for interactive use. We demonstrate how common
anti-patterns -- such as inadequate baseline comparisons that conflate
engineering effort with algorithmic novelty, workload selections that fail to
represent production scenarios, and metric normalizations that hide substantial
performance variability like generation stalls-lead to misleading conclusions.
To address these challenges, we provide a comprehensive checklist derived from
our analysis, establishing a framework for recognizing and avoiding these
anti-patterns in favor of robust LLM inference evaluation. To demonstrate the
practical application of our framework, we present a case study analyzing
speculative decoding, a technique whose bursty, non-uniform token generation is
easily misinterpreted when evaluated using approaches characteristic of these
anti-patterns. Our work establishes a rigorous foundation for evaluation
methodology, enabling meaningful comparisons, ensuring reproducible results,
and ultimately accelerating genuine progress in LLM inference systems by moving
beyond common anti-patterns to align evaluation with real-world requirements.

</details>


### [308] [Model Parallelism With Subnetwork Data Parallelism](https://arxiv.org/abs/2507.09029)
*Vaibhav Singh,Zafir Khalid,Edouard Oyallon,Eugene Belilovsky*

Main category: cs.LG

TL;DR: 提出一种分布式预训练新方法，通过训练小型结构化子网络减少内存需求，避免节点间激活通信，性能不降。


<details>
  <summary>Details</summary>
Motivation: 解决大规模模型预训练中内存需求高和节点间通信成本高的问题。

Method: 训练小型结构化子网络，避免节点间激活通信，采用两种子网络构建策略（随机块丢弃和宽度方向构建）。

Result: 随机块丢弃策略表现更优，内存使用减少20-40%，性能无损失。

Conclusion: 该方法有效降低内存需求，保持性能，具有潜力。

Abstract: Distributed pre-training of large models at scale often imposes heavy memory
demands on individual nodes and incurs significant intra-node communication
costs. We propose a novel alternative approach that reduces the memory
requirements by training small, structured subnetworks of the model on separate
workers. Unlike pipelining, our method avoids inter-node activation
communication and maintains bandwidth requirements that are comparable to or
lower than standard data parallel communication schemes based on all-reduce. We
evaluate two subnetwork construction strategies guided by the principle of
ensuring uniform representation of each parameter across the distributed
training setup. Our results show that the stochastic block dropping technique
consistently outperforms the width-wise subnetwork construction previously
explored in federated learning. We empirically attribute this superior
performance to stronger gradient alignment in subnetworks that retain blocks
having skip connections. Preliminary experiments highlight the promise of our
approach, achieving a 20-40% reduction in memory usage without any loss in
performance.

</details>


### [309] [Confounder-Free Continual Learning via Recursive Feature Normalization](https://arxiv.org/abs/2507.09031)
*Yash Shah,Camila Gonzalez,Mohammad H. Abbasi,Qingyu Zhao,Kilian M. Pohl,Ehsan Adeli*

Main category: cs.LG

TL;DR: 论文提出了一种递归MDN（R-MDN）层，用于在持续学习中消除混杂变量的影响，提升模型在不同群体中的公平性。


<details>
  <summary>Details</summary>
Motivation: 混杂变量会导致虚假相关性和预测偏差，传统方法如MDN在持续学习中难以处理动态变化的混杂变量。

Method: 引入R-MDN层，通过递归最小二乘法动态调整特征表示，适应数据和混杂变量的分布变化。

Result: R-MDN在静态学习和持续学习中均能减少混杂变量影响，提升预测公平性。

Conclusion: R-MDN是一种有效的持续学习方法，能够动态适应混杂变量的变化，减少灾难性遗忘。

Abstract: Confounders are extraneous variables that affect both the input and the
target, resulting in spurious correlations and biased predictions. There are
recent advances in dealing with or removing confounders in traditional models,
such as metadata normalization (MDN), where the distribution of the learned
features is adjusted based on the study confounders. However, in the context of
continual learning, where a model learns continuously from new data over time
without forgetting, learning feature representations that are invariant to
confounders remains a significant challenge. To remove their influence from
intermediate feature representations, we introduce the Recursive MDN (R-MDN)
layer, which can be integrated into any deep learning architecture, including
vision transformers, and at any model stage. R-MDN performs statistical
regression via the recursive least squares algorithm to maintain and
continually update an internal model state with respect to changing
distributions of data and confounding variables. Our experiments demonstrate
that R-MDN promotes equitable predictions across population groups, both within
static learning and across different stages of continual learning, by reducing
catastrophic forgetting caused by confounder effects changing over time.

</details>


### [310] [Behavioral Exploration: Learning to Explore via In-Context Adaptation](https://arxiv.org/abs/2507.09041)
*Andrew Wagenmaker,Zhiyuan Zhou,Sergey Levine*

Main category: cs.LG

TL;DR: 提出了一种名为“行为探索”的方法，通过训练长上下文生成模型，使自主代理能够快速在线探索和适应环境，模仿专家行为并进行针对性探索。


<details>
  <summary>Details</summary>
Motivation: 人类能够快速在线探索和适应环境，而现有算法依赖随机探索和缓慢的梯度更新。研究旨在赋予自主代理类似人类的能力。

Method: 利用专家演示数据集，训练长上下文生成模型，预测专家动作，并基于上下文和探索性度量选择不同行为，实现快速适应和针对性探索。

Result: 在模拟运动和操作任务以及真实机器人操作任务中验证了方法的有效性，展示了其学习适应性和探索性行为的能力。

Conclusion: 行为探索方法成功实现了自主代理的快速在线适应和专家式探索，为机器人学和机器学习提供了新思路。

Abstract: Developing autonomous agents that quickly explore an environment and adapt
their behavior online is a canonical challenge in robotics and machine
learning. While humans are able to achieve such fast online exploration and
adaptation, often acquiring new information and skills in only a handful of
interactions, existing algorithmic approaches tend to rely on random
exploration and slow, gradient-based behavior updates. How can we endow
autonomous agents with such capabilities on par with humans? Taking inspiration
from recent progress on both in-context learning and large-scale behavioral
cloning, in this work we propose behavioral exploration: training agents to
internalize what it means to explore and adapt in-context over the space of
``expert'' behaviors. To achieve this, given access to a dataset of expert
demonstrations, we train a long-context generative model to predict expert
actions conditioned on a context of past observations and a measure of how
``exploratory'' the expert's behaviors are relative to this context. This
enables the model to not only mimic the behavior of an expert, but also, by
feeding its past history of interactions into its context, to select different
expert behaviors than what have been previously selected, thereby allowing for
fast online adaptation and targeted, ``expert-like'' exploration. We
demonstrate the effectiveness of our method in both simulated locomotion and
manipulation settings, as well as on real-world robotic manipulation tasks,
illustrating its ability to learn adaptive, exploratory behavior.

</details>


### [311] [Shortening the Trajectories: Identity-Aware Gaussian Approximation for Efficient 3D Molecular Generation](https://arxiv.org/abs/2507.09043)
*Jingxiang Qu,Wenhan Gao,Yi Liu*

Main category: cs.LG

TL;DR: 提出了一种改进高斯概率生成模型（GPGMs）生成效率的框架，通过分析数据快速收敛到高斯分布的特性，减少冗余计算。


<details>
  <summary>Details</summary>
Motivation: GPGMs的生成过程计算成本高，限制了实际部署，需提高效率。

Method: 识别数据收敛到高斯分布的特征步骤，用闭式高斯近似替代剩余轨迹。

Result: 在多种数据模态中显著提升了样本质量和计算效率。

Conclusion: 该方法在保持学习动态完整性的同时，有效提升了生成效率。

Abstract: Gaussian-based Probabilistic Generative Models (GPGMs) generate data by
reversing a stochastic process that progressively corrupts samples with
Gaussian noise. While these models have achieved state-of-the-art performance
across diverse domains, their practical deployment remains constrained by the
high computational cost of long generative trajectories, which often involve
hundreds to thousands of steps during training and sampling. In this work, we
introduce a theoretically grounded and empirically validated framework that
improves generation efficiency without sacrificing training granularity or
inference fidelity. Our key insight is that for certain data modalities, the
noising process causes data to rapidly lose its identity and converge toward a
Gaussian distribution. We analytically identify a characteristic step at which
the data has acquired sufficient Gaussianity, and then replace the remaining
generation trajectory with a closed-form Gaussian approximation. Unlike
existing acceleration techniques that coarsening the trajectories by skipping
steps, our method preserves the full resolution of learning dynamics while
avoiding redundant stochastic perturbations between `Gaussian-like'
distributions. Empirical results across multiple data modalities demonstrate
substantial improvements in both sample quality and computational efficiency.

</details>


### [312] [Imitation Learning in Continuous Action Spaces: Mitigating Compounding Error without Interaction](https://arxiv.org/abs/2507.09061)
*Thomas T. Zhang,Daniel Pfrommer,Nikolai Matni,Max Simchowitz*

Main category: cs.LG

TL;DR: 研究了在连续状态-动作动态系统中模仿专家演示者的问题，提出了两种干预方法以减少复合错误。


<details>
  <summary>Details</summary>
Motivation: 模仿学习在物理环境中（如自动驾驶和机器人学习）比离散环境更复杂，复合错误问题显著，需要更先进的策略或数据增强。

Method: 提出了两种干预方法：在开环稳定系统中使用“动作分块”（预测并执行动作序列），在不稳定系统中使用“噪声注入”（在专家演示中添加噪声）。

Result: 这些方法有效减少了复合错误，且与控制理论和强化学习的现有方法不同。

Conclusion: 研究结合了控制理论和强化学习的工具，揭示了新的考虑因素，为连续状态-动作模仿学习提供了实用解决方案。

Abstract: We study the problem of imitating an expert demonstrator in a continuous
state-and-action dynamical system. While imitation learning in discrete
settings such as autoregressive language modeling has seen immense success and
popularity in recent years, imitation in physical settings such as autonomous
driving and robot learning has proven comparably more complex due to the
compounding errors problem, often requiring elaborate set-ups to perform
stably. Recent work has demonstrated that even in benign settings, exponential
compounding errors are unavoidable when learning solely from expert-controlled
trajectories, suggesting the need for more advanced policy parameterizations or
data augmentation. To this end, we present minimal interventions that provably
mitigate compounding errors in continuous state-and-action imitation learning.
When the system is open-loop stable, we prescribe "action chunking," i.e.,
predicting and playing sequences of actions in open-loop; when the system is
possibly unstable, we prescribe "noise injection," i.e., adding noise during
expert demonstrations. These interventions align with popular choices in modern
robot learning, though the benefits we derive are distinct from the effects
they were designed to target. Our results draw insights and tools from both
control theory and reinforcement learning; however, our analysis reveals novel
considerations that do not naturally arise when either literature is considered
in isolation.

</details>


### [313] [Queue up for takeoff: a transferable deep learning framework for flight delay prediction](https://arxiv.org/abs/2507.09084)
*Nnamdi Daniel Aghanya,Ta Duong Vu,Amaëlle Diop,Charlotte Deville,Nour Imane Kerroumi,Irene Moulitsas,Jun Li,Desmond Bisandu*

Main category: cs.LG

TL;DR: 论文提出了一种结合排队论和注意力模型的新方法QT-SimAM，用于高精度预测航班延误，并在不同网络中验证了其泛化能力。


<details>
  <summary>Details</summary>
Motivation: 航班延误对航空业造成重大财务和运营影响，需提高预测模型的精确性和泛化性以改善乘客体验和减少收入损失。

Method: 结合排队论和简单注意力模型（QT-SimAM），使用美国交通统计局数据验证模型性能。

Result: QT-SimAM在美国数据集上准确率0.927，F1分数0.932；在欧洲数据集上准确率0.826，F1分数0.791。

Conclusion: QT-SimAM是一种端到端的高效方法，能跨网络高精度预测航班延误，有助于减少乘客焦虑和优化运营决策。

Abstract: Flight delays are a significant challenge in the aviation industry, causing
major financial and operational disruptions. To improve passenger experience
and reduce revenue loss, flight delay prediction models must be both precise
and generalizable across different networks. This paper introduces a novel
approach that combines Queue-Theory with a simple attention model, referred to
as the Queue-Theory SimAM (QT-SimAM). To validate our model, we used data from
the US Bureau of Transportation Statistics, where our proposed QT-SimAM
(Bidirectional) model outperformed existing methods with an accuracy of 0.927
and an F1 score of 0.932. To assess transferability, we tested the model on the
EUROCONTROL dataset. The results demonstrated strong performance, achieving an
accuracy of 0.826 and an F1 score of 0.791. Ultimately, this paper outlines an
effective, end-to-end methodology for predicting flight delays. The proposed
model's ability to forecast delays with high accuracy across different networks
can help reduce passenger anxiety and improve operational decision-making

</details>


### [314] [Deep Reinforcement Learning with Gradient Eligibility Traces](https://arxiv.org/abs/2507.09087)
*Esraa Elelimy,Brett Daley,Andrew Patterson,Marlos C. Machado,Adam White,Martha White*

Main category: cs.LG

TL;DR: 本文提出了一种基于多步信用分配的广义投影贝尔曼误差（GPBE）方法，用于深度强化学习中的快速稳定离策略学习。


<details>
  <summary>Details</summary>
Motivation: 现有方法多依赖半梯度TD方法，易发散；而GTD方法虽有收敛保证，但很少用于深度RL。本文旨在扩展GPBE以支持多步信用分配。

Method: 扩展GPBE目标以支持基于λ-回报的多步信用分配，并推导出三种梯度优化方法，包括前向视图和后向视图。

Result: 在MuJoCo和MinAtar环境中，所提算法优于PPO和StreamQ。

Conclusion: 多步GPBE方法在深度RL中实现了快速稳定的离策略学习，优于现有方法。

Abstract: Achieving fast and stable off-policy learning in deep reinforcement learning
(RL) is challenging. Most existing methods rely on semi-gradient
temporal-difference (TD) methods for their simplicity and efficiency, but are
consequently susceptible to divergence. While more principled approaches like
Gradient TD (GTD) methods have strong convergence guarantees, they have rarely
been used in deep RL. Recent work introduced the Generalized Projected Bellman
Error ($\GPBE$), enabling GTD methods to work efficiently with nonlinear
function approximation. However, this work is only limited to one-step methods,
which are slow at credit assignment and require a large number of samples. In
this paper, we extend the $\GPBE$ objective to support multistep credit
assignment based on the $\lambda$-return and derive three gradient-based
methods that optimize this new objective. We provide both a forward-view
formulation compatible with experience replay and a backward-view formulation
compatible with streaming algorithms. Finally, we evaluate the proposed
algorithms and show that they outperform both PPO and StreamQ in MuJoCo and
MinAtar environments, respectively. Code available at
https://github.com/esraaelelimy/gtd\_algos

</details>


### [315] [Continuous-Time Signal Decomposition: An Implicit Neural Generalization of PCA and ICA](https://arxiv.org/abs/2507.09091)
*Shayan K. Azmoodeh,Krishna Subramani,Paris Smaragdis*

Main category: cs.LG

TL;DR: 论文提出了一种基于隐式神经信号表示的连续时间向量信号低秩分解方法，统一了PCA和ICA问题，适用于不规则采样信号。


<details>
  <summary>Details</summary>
Motivation: 传统PCA和ICA方法在处理连续时间信号或不规则采样信号时存在局限性，需要一种更通用的解决方案。

Method: 通过引入对比函数项的网络损失，学习信号分解的数值近似，统一了PCA和ICA在连续时间下的方法。

Result: 该方法能够处理点云和不规则采样信号，扩展了传统技术的应用范围。

Conclusion: 提出的框架为连续时间信号的低秩分解提供了一种灵活且通用的解决方案。

Abstract: We generalize the low-rank decomposition problem, such as principal and
independent component analysis (PCA, ICA) for continuous-time vector-valued
signals and provide a model-agnostic implicit neural signal representation
framework to learn numerical approximations to solve the problem. Modeling
signals as continuous-time stochastic processes, we unify the approaches to
both the PCA and ICA problems in the continuous setting through a contrast
function term in the network loss, enforcing the desired statistical properties
of the source signals (decorrelation, independence) learned in the
decomposition. This extension to a continuous domain allows the application of
such decompositions to point clouds and irregularly sampled signals where
standard techniques are not applicable.

</details>


### [316] [Principled Foundations for Preference Optimization](https://arxiv.org/abs/2507.07855)
*Wenxuan Zhou,Shujian Zhang,Brice Magdalou,John Lambert,Ehsan Amid,Richard Nock,Andrew Hard*

Main category: cs.LG

TL;DR: 本文揭示了直接偏好优化（DPO）是机器学习中偏好学习的两种主要理论（Savage的损失函数和Doignon-Falmagne与Machina的随机选择）之间的特定联系。


<details>
  <summary>Details</summary>
Motivation: 理解DPO的通用原理至关重要，因其广泛应用、当前研究热度及许多先进变体仅覆盖了部分理论范围。

Method: 建立了Savage所有损失函数的通用联系，支持选择理论中的弃权、机器学习中的非凸目标，并免费扩展了DPO设置（如边界和长度校正）。

Result: 揭示了DPO的通用框架，帮助理解其操作原理、潜在陷阱及解决方案。

Conclusion: 本文为DPO提供了理论支持，扩展了其应用范围，并指出了未来研究方向。

Abstract: In this paper, we show that direct preference optimization (DPO) is a very
specific form of a connection between two major theories in the ML context of
learning from preferences: loss functions (Savage) and stochastic choice
(Doignon-Falmagne and Machina). The connection is established for all of
Savage's losses and at this level of generality, (i) it includes support for
abstention on the choice theory side, (ii) it includes support for non-convex
objectives on the ML side, and (iii) it allows to frame for free some notable
extensions of the DPO setting, including margins and corrections for length.
Getting to understand how DPO operates from a general principled perspective is
crucial because of the huge and diverse application landscape of models,
because of the current momentum around DPO, but also -- and importantly --
because many state of the art variations on DPO definitely occupy a small
region of the map that we cover. It also helps to understand the pitfalls of
departing from this map, and figure out workarounds.

</details>


### [317] [On the Fragility of Multimodal Perception to Temporal Misalignment in Autonomous Driving](https://arxiv.org/abs/2507.09095)
*Md Hasan Shahriar,Md Mohaimin Al Barat,Harshavardhan Sundar,Naren Ramakrishnan,Y. Thomas Hou,Wenjing Lou*

Main category: cs.LG

TL;DR: 论文提出DejaVu攻击，利用网络延迟制造传感器流的时间错位，显著降低多模态融合感知性能，并提出防御方法AION。


<details>
  <summary>Details</summary>
Motivation: 多模态融合在自动驾驶感知中至关重要，但其严格依赖时间同步使其易受攻击。

Method: 通过DejaVu攻击分析传感器流的时间错位影响，提出AION防御方法，利用跨模态时间一致性和动态时间规整检测攻击。

Result: 攻击可显著降低感知性能（如目标检测mAP下降88.5%），AION防御在AUROC上达到0.92-0.98。

Conclusion: AION是一种高效且通用的防御方法，能有效对抗时间错位攻击。

Abstract: Multimodal fusion (MMF) plays a critical role in the perception of autonomous
driving, which primarily fuses camera and LiDAR streams for a comprehensive and
efficient scene understanding. However, its strict reliance on precise temporal
synchronization exposes it to new vulnerabilities. In this paper, we introduce
DejaVu, a novel attack that exploits network-induced delays to create subtle
temporal misalignments across sensor streams, severely degrading downstream
MMF-based perception tasks. Our comprehensive attack analysis across different
models and datasets reveals these sensors' task-specific imbalanced
sensitivities: object detection is overly dependent on LiDAR inputs while
object tracking is highly reliant on the camera inputs. Consequently, with a
single-frame LiDAR delay, an attacker can reduce the car detection mAP by up to
88.5%, while with a three-frame camera delay, multiple object tracking accuracy
(MOTA) for car drops by 73%. To detect such attacks, we propose AION, a defense
patch that can work alongside the existing perception model to monitor temporal
alignment through cross-modal temporal consistency. AION leverages multimodal
shared representation learning and dynamic time warping to determine the path
of temporal alignment and calculate anomaly scores based on the alignment. Our
thorough evaluation of AION shows it achieves AUROC scores of 0.92-0.98 with
low false positives across datasets and model architectures, demonstrating it
as a robust and generalized defense against the temporal misalignment attacks.

</details>


### [318] [S2SRec2: Set-to-Set Recommendation for Basket Completion with Recipe](https://arxiv.org/abs/2507.09101)
*Yanan Cao,Omid Memarrast,Shiqin Cai,Sinduja Subramaniam,Evren Korpeoglu,Kannan Achan*

Main category: cs.LG

TL;DR: 论文提出了一种基于集合到集合（S2S）推荐的框架S2SRec2，用于在电商中推荐互补食材，解决了传统方法无法处理多食材缺失和忽略食材间关系的局限性。


<details>
  <summary>Details</summary>
Motivation: 在电商购物中，顾客常因缺乏专业知识而无法完成完整食谱的食材搭配，传统方法仅预测单一缺失食材，无法满足实际需求。

Method: 提出了S2SRec2框架，基于Set Transformer，通过多任务学习联合学习缺失食材检索和篮子完整性评估。

Result: 实验表明，S2SRec2显著优于单目标基线方法，提升了购物体验和烹饪创意。

Conclusion: S2SRec2为电商中的食材推荐提供了一种有效且创新的解决方案。

Abstract: In grocery e-commerce, customers often build ingredient baskets guided by
dietary preferences but lack the expertise to create complete meals. Leveraging
recipe knowledge to recommend complementary ingredients based on a partial
basket is essential for improving the culinary experience. Traditional recipe
completion methods typically predict a single missing ingredient using a
leave-one-out strategy. However, they fall short in two key aspects: (i) they
do not reflect real-world scenarios where multiple ingredients are often
needed, and (ii) they overlook relationships among the missing ingredients
themselves. To address these limitations, we reformulate basket completion as a
set-to-set (S2S) recommendation problem, where an incomplete basket is input
into a system that predicts a set of complementary ingredients. We introduce
S2SRec2, a set-to-set ingredient recommendation framework based on a Set
Transformer and trained in a multitask learning paradigm. S2SRec2 jointly
learns to (i) retrieve missing ingredients from the representation of existing
ones and (ii) assess basket completeness after prediction. These tasks are
optimized together, enforcing accurate retrieval and coherent basket
completion. Experiments on large-scale recipe datasets and qualitative analyses
show that S2SRec2 significantly outperforms single-target baselines, offering a
promising approach to enhance grocery shopping and inspire culinary creativity.

</details>


### [319] [A Study of Value-Aware Eigenoptions](https://arxiv.org/abs/2507.09127)
*Harshil Kotamreddy,Marlos C. Machado*

Main category: cs.LG

TL;DR: 本文研究了特征选项（eigenoptions）在无模型强化学习中的信用分配作用，发现预定义特征选项有助于探索和信用分配，而在线发现可能阻碍学习。同时，提出了在深度强化学习中学习选项值的方法。


<details>
  <summary>Details</summary>
Motivation: 特征选项在强化学习中表现出强大的探索能力，但其在信用分配中的作用尚未充分研究。本文旨在探讨特征选项是否能加速信用分配。

Method: 在表格和像素网格世界中评估特征选项的作用，并提出在深度强化学习中学习选项值的方法，特别关注终止条件的影响。

Result: 预定义特征选项有助于探索和信用分配，而在线发现可能因经验偏差阻碍学习。在深度强化学习中，终止条件对性能有显著影响。

Conclusion: 特征选项在同时支持信用分配和探索方面具有潜力，但也存在复杂性，尤其是在在线发现和终止条件的设计上。

Abstract: Options, which impose an inductive bias toward temporal and hierarchical
structure, offer a powerful framework for reinforcement learning (RL). While
effective in sequential decision-making, they are often handcrafted rather than
learned. Among approaches for discovering options, eigenoptions have shown
strong performance in exploration, but their role in credit assignment remains
underexplored. In this paper, we investigate whether eigenoptions can
accelerate credit assignment in model-free RL, evaluating them in tabular and
pixel-based gridworlds. We find that pre-specified eigenoptions aid not only
exploration but also credit assignment, whereas online discovery can bias the
agent's experience too strongly and hinder learning. In the context of deep RL,
we also propose a method for learning option-values under non-linear function
approximation, highlighting the impact of termination conditions on
performance. Our findings reveal both the promise and complexity of using
eigenoptions, and options more broadly, to simultaneously support credit
assignment and exploration in reinforcement learning.

</details>


### [320] [Heterogeneous Graph Prompt Learning via Adaptive Weight Pruning](https://arxiv.org/abs/2507.09132)
*Chu-Yuan Wei,Shun-Yao Liu,Sheng-Da Zhuo,Chang-Dong Wang,Shu-Qiang Huang,Mohsen Guizani*

Main category: cs.LG

TL;DR: 论文提出了一种结合图提示与权重剪枝的新框架GPAWP，旨在通过减少提示数量提升图神经网络的性能和效率。


<details>
  <summary>Details</summary>
Motivation: 尽管图神经网络（GNNs）在图任务中表现优异，但仍面临训练时间长、难以捕捉复杂关系等问题。图预训练和提示方法虽受关注，但此前研究忽视了图提示在模型优化中的潜力及其对稳定性和效率的影响。

Method: 提出GPAWP框架，结合图提示与权重剪枝，通过重要性评估函数确定不同粒度的正负权重，并采用分层剪枝去除负提示标签。

Result: 在三个基准数据集上的实验表明，GPAWP显著减少了节点分类任务中的参数量，同时保持了竞争力。

Conclusion: GPAWP通过优化图提示的效率和性能，为图神经网络的实际应用提供了新思路。

Abstract: Graph Neural Networks (GNNs) have achieved remarkable success in various
graph-based tasks (e.g., node classification or link prediction). Despite their
triumphs, GNNs still face challenges such as long training and inference times,
difficulty in capturing complex relationships, and insufficient feature
extraction. To tackle these issues, graph pre-training and graph prompt methods
have garnered increasing attention for their ability to leverage large-scale
datasets for initial learning and task-specific adaptation, offering potential
improvements in GNN performance. However, previous research has overlooked the
potential of graph prompts in optimizing models, as well as the impact of both
positive and negative graph prompts on model stability and efficiency. To
bridge this gap, we propose a novel framework combining graph prompts with
weight pruning, called GPAWP, which aims to enhance the performance and
efficiency of graph prompts by using fewer of them. We evaluate the importance
of graph prompts using an importance assessment function to determine positive
and negative weights at different granularities. Through hierarchically
structured pruning, we eliminate negative prompt labels, resulting in more
parameter-efficient and competitively performing prompts. Extensive experiments
on three benchmark datasets demonstrate the superiority of GPAWP, leading to a
significant reduction in parameters in node classification tasks.

</details>


### [321] [POIFormer: A Transformer-Based Framework for Accurate and Scalable Point-of-Interest Attribution](https://arxiv.org/abs/2507.09137)
*Nripsuta Ani Saxena,Shang-Ling Hsu,Mehul Shetty,Omar Alkhadra,Cyrus Shahabi,Abigail L. Horn*

Main category: cs.LG

TL;DR: POIFormer是一种基于Transformer的框架，用于准确高效地将用户访问归因于特定POI，解决了GPS不准确和POI空间密度高的问题。


<details>
  <summary>Details</summary>
Motivation: 由于GPS不准确（2-20米误差）和城市环境中POI的高空间密度，传统的基于邻近性的方法难以准确归因用户访问。

Method: POIFormer利用Transformer的自注意力机制，联合建模空间邻近性、访问时间、POI语义、用户行为和群体行为模式等多维度信号。

Result: 在真实世界移动数据集上的实验表明，POIFormer在空间噪声和高密度POI聚类场景下显著优于现有基线方法。

Conclusion: POIFormer提供了一种通用、高效的解决方案，适用于不同数据源和地理环境，且无需依赖难以获取的数据层。

Abstract: Accurately attributing user visits to specific Points of Interest (POIs) is a
foundational task for mobility analytics, personalized services, marketing and
urban planning. However, POI attribution remains challenging due to GPS
inaccuracies, typically ranging from 2 to 20 meters in real-world settings, and
the high spatial density of POIs in urban environments, where multiple venues
can coexist within a small radius (e.g., over 50 POIs within a 100-meter radius
in dense city centers). Relying on proximity is therefore often insufficient
for determining which POI was actually visited. We introduce
\textsf{POIFormer}, a novel Transformer-based framework for accurate and
efficient POI attribution. Unlike prior approaches that rely on limited
spatiotemporal, contextual, or behavioral features, \textsf{POIFormer} jointly
models a rich set of signals, including spatial proximity, visit timing and
duration, contextual features from POI semantics, and behavioral features from
user mobility and aggregated crowd behavior patterns--using the Transformer's
self-attention mechanism to jointly model complex interactions across these
dimensions. By leveraging the Transformer to model a user's past and future
visits (with the current visit masked) and incorporating crowd-level behavioral
patterns through pre-computed KDEs, \textsf{POIFormer} enables accurate,
efficient attribution in large, noisy mobility datasets. Its architecture
supports generalization across diverse data sources and geographic contexts
while avoiding reliance on hard-to-access or unavailable data layers, making it
practical for real-world deployment. Extensive experiments on real-world
mobility datasets demonstrate significant improvements over existing baselines,
particularly in challenging real-world settings characterized by spatial noise
and dense POI clustering.

</details>


### [322] [Towards Interpretable Drug-Drug Interaction Prediction: A Graph-Based Approach with Molecular and Network-Level Explanations](https://arxiv.org/abs/2507.09173)
*Mengjie Chen,Ming Zhang,Cunquan Qu*

Main category: cs.LG

TL;DR: MolecBioNet是一个新颖的图框架，整合分子和生物医学知识，用于预测药物相互作用（DDI），通过统一建模药物对和多尺度知识集成，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有图方法独立处理药物对，忽略其复杂性和上下文依赖性，且难以整合生物网络和分子结构。MolecBioNet旨在解决这些问题。

Method: 提出MolecBioNet框架，统一建模药物对，提取生物知识图谱子图，构建分子层次交互图，使用图神经网络学习多尺度表示，并引入两种领域特定池化策略（CASPool和AGIPool）。

Result: 实验表明MolecBioNet在DDI预测上优于现有方法，消融研究和嵌入可视化验证了其优势。

Conclusion: MolecBioNet通过统一建模和多尺度知识集成，提供了更准确和可解释的DDI预测。

Abstract: Drug-drug interactions (DDIs) represent a critical challenge in pharmacology,
often leading to adverse drug reactions with significant implications for
patient safety and healthcare outcomes. While graph-based methods have achieved
strong predictive performance, most approaches treat drug pairs independently,
overlooking the complex, context-dependent interactions unique to drug pairs.
Additionally, these models struggle to integrate biological interaction
networks and molecular-level structures to provide meaningful mechanistic
insights. In this study, we propose MolecBioNet, a novel graph-based framework
that integrates molecular and biomedical knowledge for robust and interpretable
DDI prediction. By modeling drug pairs as unified entities, MolecBioNet
captures both macro-level biological interactions and micro-level molecular
influences, offering a comprehensive perspective on DDIs. The framework
extracts local subgraphs from biomedical knowledge graphs and constructs
hierarchical interaction graphs from molecular representations, leveraging
classical graph neural network methods to learn multi-scale representations of
drug pairs. To enhance accuracy and interpretability, MolecBioNet introduces
two domain-specific pooling strategies: context-aware subgraph pooling
(CASPool), which emphasizes biologically relevant entities, and
attention-guided influence pooling (AGIPool), which prioritizes influential
molecular substructures. The framework further employs mutual information
minimization regularization to enhance information diversity during embedding
fusion. Experimental results demonstrate that MolecBioNet outperforms
state-of-the-art methods in DDI prediction, while ablation studies and
embedding visualizations further validate the advantages of unified drug pair
modeling and multi-scale knowledge integration.

</details>


### [323] [Continual Reinforcement Learning by Planning with Online World Models](https://arxiv.org/abs/2507.09177)
*Zichen Liu,Guoji Fu,Chao Du,Wee Sun Lee,Min Lin*

Main category: cs.LG

TL;DR: 论文提出了一种基于在线世界模型的持续强化学习方法（CRL），通过FTL浅层模型和模型预测控制解决任务序列中的灾难性遗忘问题。


<details>
  <summary>Details</summary>
Motivation: 持续强化学习中，智能体在学习新任务时容易遗忘旧任务（灾难性遗忘），本文旨在解决这一问题。

Method: 使用在线学习的FTL浅层模型捕捉世界动态，结合模型预测控制规划动作，形成增量更新的FTL在线智能体（OA）。

Result: 在设计的Continual Bench环境中，OA表现优于基于深度世界模型的基线方法，能持续学习新任务且不遗忘旧技能。

Conclusion: 在线世界模型和规划方法有效解决了CRL中的灾难性遗忘问题，为持续学习提供了新思路。

Abstract: Continual reinforcement learning (CRL) refers to a naturalistic setting where
an agent needs to endlessly evolve, by trial and error, to solve multiple tasks
that are presented sequentially. One of the largest obstacles to CRL is that
the agent may forget how to solve previous tasks when learning a new task,
known as catastrophic forgetting. In this paper, we propose to address this
challenge by planning with online world models. Specifically, we learn a
Follow-The-Leader shallow model online to capture the world dynamics, in which
we plan using model predictive control to solve a set of tasks specified by any
reward functions. The online world model is immune to forgetting by
construction with a proven regret bound of $\mathcal{O}(\sqrt{K^2D\log(T)})$
under mild assumptions. The planner searches actions solely based on the latest
online model, thus forming a FTL Online Agent (OA) that updates incrementally.
To assess OA, we further design Continual Bench, a dedicated environment for
CRL, and compare with several strong baselines under the same model-planning
algorithmic framework. The empirical results show that OA learns continuously
to solve new tasks while not forgetting old skills, outperforming agents built
on deep world models with various continual learning techniques.

</details>


### [324] [XiChen: An observation-scalable fully AI-driven global weather forecasting system with 4D variational knowledge](https://arxiv.org/abs/2507.09202)
*Wuxin Wang,Weicheng Ni,Lilan Huang,Tao Hao,Ben Fei,Shuo Ma,Taikang Yuan,Yanlai Zhao,Kefeng Deng,Xiaoyong Li,Boheng Duan,Lei Bai,Kaijun Ren*

Main category: cs.LG

TL;DR: XiChen是一个完全由AI驱动的全球天气预报系统，从数据同化到中期预报仅需17秒，性能媲美传统NWP系统。


<details>
  <summary>Details</summary>
Motivation: 传统AI天气预报依赖NWP系统准备初始条件，耗时且效率低，XiChen旨在实现完全独立的AI驱动预报。

Method: 基于预训练的基础模型，微调后作为观测算子和数据同化模型，结合四维变分知识提升精度。

Result: XiChen的预报准确性与NWP系统相当，预报领先时间超过8.25天。

Conclusion: XiChen展示了完全独立于NWP系统的AI天气预报潜力。

Abstract: Recent advancements in Artificial Intelligence (AI) demonstrate significant
potential to revolutionize weather forecasting. However, most AI-driven models
rely on Numerical Weather Prediction (NWP) systems for initial condition
preparation, which often consumes hours on supercomputers. Here we introduce
XiChen, the first observation-scalable fully AI-driven global weather
forecasting system, whose entire pipeline, from Data Assimilation (DA) to
medium-range forecasting, can be accomplished within only 17 seconds. XiChen is
built upon a foundation model that is pre-trained for weather forecasting.
Meanwhile, this model is subsequently fine-tuned to serve as both observation
operators and DA models, thereby scalably assimilating conventional and raw
satellite observations. Furthermore, the integration of four-dimensional
variational knowledge ensures that XiChen's DA and medium-range forecasting
accuracy rivals that of operational NWP systems, amazingly achieving a skillful
forecasting lead time exceeding 8.25 days. These findings demonstrate that
XiChen holds strong potential toward fully AI-driven weather forecasting
independent of NWP systems.

</details>


### [325] [Capturing Unseen Spatial Extremes Through Knowledge-Informed Generative Modeling](https://arxiv.org/abs/2507.09211)
*Xinyue Liu,Xiao Peng,Shuyue Yan,Yuntian Chen,Dongxiao Zhang,Zhixiao Niu,Hui-Min Wang,Xiaogang He*

Main category: cs.LG

TL;DR: DeepX-GAN模型用于生成超出历史记录的极端气候事件，揭示潜在风险。


<details>
  <summary>Details</summary>
Motivation: 现有气候极端事件记录不完整，且忽视空间依赖性，低估了同步灾害的风险。

Method: 开发DeepX-GAN模型，结合知识引导的深度生成方法，模拟统计上合理的未见过极端事件。

Result: 模型在中东和北非地区应用，发现未见过极端事件对脆弱地区影响更大，未来变暖可能改变其分布。

Conclusion: 需制定空间适应性政策，而非依赖历史模式，以应对新兴风险热点。

Abstract: Observed records of climate extremes provide an incomplete picture of risk,
missing "unseen" extremes that exceed historical bounds. In parallel,
neglecting spatial dependence undervalues the risk of synchronized hazards that
amplify impacts. To address these challenges, we develop DeepX-GAN
(Dependence-Enhanced Embedding for Physical eXtremes - Generative Adversarial
Network), a knowledge-informed deep generative model designed to better capture
the spatial structure of rare extremes. The zero-shot generalizability of
DeepX-GAN enables simulation of unseen extremes that fall outside historical
experience yet remain statistically plausible. We define two types of unseen
extremes: "checkmate" extremes that directly hit targets, and "stalemate"
extremes that narrowly miss. These unrealized scenarios expose latent risks in
fragile systems and may reinforce a false sense of resilience if overlooked.
Near misses, in particular, can prompt either proactive adaptation or dangerous
complacency, depending on how they are interpreted. Applying DeepX-GAN to the
Middle East and North Africa (MENA), we find that these unseen extremes
disproportionately affect regions with high vulnerability and low socioeconomic
readiness, but differ in urgency and interpretation. Future warming could
expand and redistribute these unseen extremes, with emerging exposure hotspots
in Indo-Pakistan and Central Africa. This distributional shift highlights
critical blind spots in conventional hazard planning and underscores the need
to develop spatially adaptive policies that anticipate emergent risk hotspots
rather than simply extrapolating from historical patterns.

</details>


### [326] [Warm Starts Accelerate Generative Modelling](https://arxiv.org/abs/2507.09212)
*Jonas Scholz,Richard E. Turner*

Main category: cs.LG

TL;DR: 提出了一种名为“warm-start model”的确定性模型，通过提供更好的起始点显著加速条件生成过程。


<details>
  <summary>Details</summary>
Motivation: 传统的迭代生成模型（如扩散模型和流匹配）生成高保真样本时速度较慢，通常需要数百次函数评估。

Method: 提出了一种“warm-start model”，通过预测一个条件化的先验分布N(mu, sigma)作为起始点，减少生成过程的距离。

Result: 在图像修复等任务中，仅需11次函数评估即可达到与1000步DDPM基线相当的结果。

Conclusion: 该方法简单且兼容标准生成模型，可与其他高效采样技术结合，进一步加速生成过程。

Abstract: Iterative generative models, like diffusion and flow-matching, create
high-fidelity samples by progressively refining a noise vector into data.
However, this process is notoriously slow, often requiring hundreds of function
evaluations. We introduce the warm-start model, a simple, deterministic model
that dramatically accelerates conditional generation by providing a better
starting point. Instead of starting generation from an uninformed N(0, I)
prior, our warm-start model predicts an informed prior N(mu, sigma), whose
moments are conditioned on the input context. This "warm start" substantially
reduces the distance the generative process must traverse, particularly when
the conditioning information is strongly informative. On tasks like image
inpainting, our method achieves results competitive with a 1000-step DDPM
baseline using only 11 total function evaluations (1 for the warm start, 10 for
generation). A simple conditional normalization trick makes our method
compatible with any standard generative model and sampler without modification,
allowing it to be combined with other efficient sampling techniques for further
acceleration. Our implementation is available at
https://github.com/jonas-scholz123/warm-start-model.

</details>


### [327] [Optimizing Basis Function Selection in Constructive Wavelet Neural Networks and Its Applications](https://arxiv.org/abs/2507.09213)
*Dunsheng Huang,Dong Shen,Lei Lu,Ying Tan*

Main category: cs.LG

TL;DR: 该论文提出了一种构建性小波神经网络（CWNN），通过选择初始基和动态增加基函数来提高精度并降低计算成本，同时分析了非线性函数的频率特性。


<details>
  <summary>Details</summary>
Motivation: 传统小波神经网络在构建精确基函数和高计算成本方面存在挑战，限制了其应用。

Method: 引入频率估计器和基函数增加机制，优先选择高能量基函数，定义时间-频率范围以提高效率。

Result: 通过四个示例展示了框架的广泛适用性和实用性，显著提高了计算效率。

Conclusion: CWNN框架在信号处理和时间序列分析中具有潜力，代码已开源。

Abstract: Wavelet neural network (WNN), which learns an unknown nonlinear mapping from
the data, has been widely used in signal processing, and time-series analysis.
However, challenges in constructing accurate wavelet bases and high
computational costs limit their application. This study introduces a
constructive WNN that selects initial bases and trains functions by introducing
new bases for predefined accuracy while reducing computational costs. For the
first time, we analyze the frequency of unknown nonlinear functions and select
appropriate initial wavelets based on their primary frequency components by
estimating the energy of the spatial frequency component. This leads to a novel
constructive framework consisting of a frequency estimator and a wavelet-basis
increase mechanism to prioritize high-energy bases, significantly improving
computational efficiency. The theoretical foundation defines the necessary
time-frequency range for high-dimensional wavelets at a given accuracy. The
framework's versatility is demonstrated through four examples: estimating
unknown static mappings from offline data, combining two offline datasets,
identifying time-varying mappings from time-series data, and capturing
nonlinear dependencies in real time-series data. These examples showcase the
framework's broad applicability and practicality. All the code will be released
at https://github.com/dshuangdd/CWNN.

</details>


### [328] [TPP-SD: Accelerating Transformer Point Process Sampling with Speculative Decoding](https://arxiv.org/abs/2507.09252)
*Shukai Gong,Yiyang Fu,Fengyuan Ran,Feng Zhou*

Main category: cs.LG

TL;DR: TPP-SD是一种通过结合Transformer时间点过程和推测解码技术来加速采样的新方法，保持输出分布不变的同时显著提升速度。


<details>
  <summary>Details</summary>
Motivation: 解决Transformer时间点过程模型采样速度慢的问题，满足实际应用中对快速序列采样的需求。

Method: 利用较小的草稿模型生成候选事件，再由较大的目标模型并行验证，结合了TPP的细化算法和语言模型的推测解码技术。

Result: 在合成和真实数据集上，TPP-SD采样速度提升2-6倍，且输出分布与标准方法一致。

Conclusion: TPP-SD成功填补了强大Transformer TPP模型与快速采样需求之间的鸿沟。

Abstract: We propose TPP-SD, a novel approach that accelerates Transformer temporal
point process (TPP) sampling by adapting speculative decoding (SD) techniques
from language models. By identifying the structural similarities between
thinning algorithms for TPPs and speculative decoding for language models, we
develop an efficient sampling framework that leverages a smaller draft model to
generate multiple candidate events, which are then verified by the larger
target model in parallel. TPP-SD maintains the same output distribution as
autoregressive sampling while achieving significant acceleration. Experiments
on both synthetic and real datasets demonstrate that our approach produces
samples from identical distributions as standard methods, but with 2-6$\times$
speedup. Our ablation studies analyze the impact of hyperparameters such as
draft length and draft model size on sampling efficiency. TPP-SD bridges the
gap between powerful Transformer TPP models and the practical need for rapid
sequence sampling.

</details>


### [329] [Controllable Patching for Compute-Adaptive Surrogate Modeling of Partial Differential Equations](https://arxiv.org/abs/2507.09264)
*Payel Mukhopadhyay,Michael McCabe,Ruben Ohana,Miles Cranmer*

Main category: cs.LG

TL;DR: 论文提出两种轻量级模块（CKM和CSM），实现动态调整补丁大小，提升计算效率和预测稳定性。


<details>
  <summary>Details</summary>
Motivation: 固定补丁大小限制了模型在生产环境中的灵活部署，需要一种无需重新训练即可动态调整补丁大小的方法。

Method: 引入CKM和CSM模块，结合循环补丁大小调整策略，实现动态补丁控制。

Result: 在2D和3D PDE基准测试中，提升了预测的准确性和运行效率。

Conclusion: 该框架首次实现补丁大小在推理时的可调性，为PDE替代任务中的计算自适应建模奠定了基础。

Abstract: Patch-based transformer surrogates have become increasingly effective for
modeling spatiotemporal dynamics, but the fixed patch size is a major
limitation for budget-conscience deployment in production. We introduce two
lightweight, architecture-agnostic modules-the Convolutional Kernel Modulator
(CKM) and Convolutional Stride Modulator (CSM)-that enable dynamic patch size
control at inference in patch based models, without retraining or accuracy
loss. Combined with a cyclic patch-size rollout, our method mitigates patch
artifacts and improves long-term stability for video-like prediction tasks.
Applied to a range of challenging 2D and 3D PDE benchmarks, our approach
improves rollout fidelity and runtime efficiency. To our knowledge, this is the
first framework to enable inference-time patch-size tunability in patch-based
PDE surrogates. Its plug-and-play design makes it broadly applicable across
architectures-establishing a general foundation for compute-adaptive modeling
in PDE surrogate tasks.

</details>


### [330] [MTF-Grasp: A Multi-tier Federated Learning Approach for Robotic Grasping](https://arxiv.org/abs/2507.10158)
*Obaidullah Zaland,Erik Elmroth,Monowar Bhuyan*

Main category: cs.LG

TL;DR: MTF-Grasp提出了一种多层次的联邦学习方法，用于解决机器人抓取任务中非独立同分布数据导致的性能下降问题。


<details>
  <summary>Details</summary>
Motivation: 联邦学习在机器人抓取任务中面临非独立同分布数据和数据量不足的挑战，导致性能下降。

Method: MTF-Grasp通过选择数据质量高、数量多的“顶级”机器人训练初始模型，再分发给“低级”机器人，以减少性能下降。

Result: 在Cornell和Jacquard抓取数据集上，MTF-Grasp比传统联邦学习方法性能提升8%。

Conclusion: MTF-Grasp有效解决了非独立同分布数据问题，提升了机器人抓取任务的性能。

Abstract: Federated Learning (FL) is a promising machine learning paradigm that enables
participating devices to train privacy-preserved and collaborative models. FL
has proven its benefits for robotic manipulation tasks. However, grasping tasks
lack exploration in such settings where robots train a global model without
moving data and ensuring data privacy. The main challenge is that each robot
learns from data that is nonindependent and identically distributed (non-IID)
and of low quantity. This exhibits performance degradation, particularly in
robotic grasping. Thus, in this work, we propose MTF-Grasp, a multi-tier FL
approach for robotic grasping, acknowledging the unique challenges posed by the
non-IID data distribution across robots, including quantitative skewness.
MTF-Grasp harnesses data quality and quantity across robots to select a set of
"top-level" robots with better data distribution and higher sample count. It
then utilizes top-level robots to train initial seed models and distribute them
to the remaining "low-level" robots, reducing the risk of model performance
degradation in low-level robots. Our approach outperforms the conventional FL
setup by up to 8% on the quantity-skewed Cornell and Jacquard grasping
datasets.

</details>


### [331] [Impute With Confidence: A Framework for Uncertainty Aware Multivariate Time Series Imputation](https://arxiv.org/abs/2507.09353)
*Addison Weatherhead,Anna Goldenberg*

Main category: cs.LG

TL;DR: 提出了一种量化不确定性的选择性插补框架，减少不可靠插补，提升下游任务性能。


<details>
  <summary>Details</summary>
Motivation: 医疗领域时间序列数据常因传感器断开而缺失，现有方法忽视模型不确定性或无法估计。

Method: 引入通用框架，量化不确定性并选择性插补高置信值。

Result: 在多种EHR数据集上实验显示，选择性插补减少误差并提升下游任务（如24小时死亡率预测）。

Conclusion: 将不确定性纳入时间序列插补具有实际价值。

Abstract: Time series data with missing values is common across many domains.
Healthcare presents special challenges due to prolonged periods of sensor
disconnection. In such cases, having a confidence measure for imputed values is
critical. Most existing methods either overlook model uncertainty or lack
mechanisms to estimate it. To address this gap, we introduce a general
framework that quantifies and leverages uncertainty for selective imputation.
By focusing on values the model is most confident in, highly unreliable
imputations are avoided. Our experiments on multiple EHR datasets, covering
diverse types of missingness, demonstrate that selectively imputing
less-uncertain values not only reduces imputation errors but also improves
downstream tasks. Specifically, we show performance gains in a 24-hour
mortality prediction task, underscoring the practical benefit of incorporating
uncertainty into time series imputation.

</details>


### [332] [Meta-autoencoders: An approach to discovery and representation of relationships between dynamically evolving classes](https://arxiv.org/abs/2507.09362)
*Assaf Marron,Smadar Szekely,Irun Cohen,David Harel*

Main category: cs.LG

TL;DR: 本文提出了一种元自动编码器（MAE）的概念，用于对多个自动编码器（AE）进行编码和解码，适用于动态演化的多类别研究。


<details>
  <summary>Details</summary>
Motivation: 研究如何捕捉动态演化类别（如物种）之间的共性和差异，为机器学习和生物学提供新工具。

Method: 通过构建MAE，学习对多个AE的紧凑表示及其编码解码过程。

Result: 提供了MAE的构造定义和初步示例，展示了其在机器学习和生物学中的应用潜力。

Conclusion: MAE为动态演化类别的研究提供了新方法，未来可进一步探索其在多领域的应用。

Abstract: An autoencoder (AE) is a neural network that, using self-supervised training,
learns a succinct parameterized representation, and a corresponding encoding
and decoding process, for all instances in a given class. Here, we introduce
the concept of a meta-autoencoder (MAE): an AE for a collection of
autoencoders. Given a family of classes that differ from each other by the
values of some parameters, and a trained AE for each class, an MAE for the
family is a neural net that has learned a compact representation and associated
encoder and decoder for the class-specific AEs. One application of this general
concept is in research and modeling of natural evolution -- capturing the
defining and the distinguishing properties across multiple species that are
dynamically evolving from each other and from common ancestors. In this interim
report we provide a constructive definition of MAEs, initial examples, and the
motivating research directions in machine learning and biology.

</details>


### [333] [Fair CCA for Fair Representation Learning: An ADNI Study](https://arxiv.org/abs/2507.09382)
*Bojian Hou,Zhanliang Wang,Zhuoping Zhou,Boning Tong,Zexuan Wang,Jingxuan Bao,Duy Duong-Tran,Qi Long,Li Shen*

Main category: cs.LG

TL;DR: 提出了一种新的公平CCA方法，确保投影特征独立于敏感属性，同时保持高相关性分析性能。


<details>
  <summary>Details</summary>
Motivation: 随着公平性在机器学习中的重要性增加，现有公平CCA方法常忽略对下游分类任务的影响，限制了应用。

Method: 提出一种新颖的公平CCA方法，确保投影特征与敏感属性独立，提升公平性而不牺牲准确性。

Result: 在合成数据和ADNI真实数据上验证，方法能保持高相关性分析性能并提升分类任务的公平性。

Conclusion: 该方法为神经影像研究中的公平机器学习提供了有效工具，确保无偏分析。

Abstract: Canonical correlation analysis (CCA) is a technique for finding correlations
between different data modalities and learning low-dimensional representations.
As fairness becomes crucial in machine learning, fair CCA has gained attention.
However, previous approaches often overlook the impact on downstream
classification tasks, limiting applicability. We propose a novel fair CCA
method for fair representation learning, ensuring the projected features are
independent of sensitive attributes, thus enhancing fairness without
compromising accuracy. We validate our method on synthetic data and real-world
data from the Alzheimer's Disease Neuroimaging Initiative (ADNI), demonstrating
its ability to maintain high correlation analysis performance while improving
fairness in classification tasks. Our work enables fair machine learning in
neuroimaging studies where unbiased analysis is essential.

</details>


### [334] [Geometric Generative Modeling with Noise-Conditioned Graph Networks](https://arxiv.org/abs/2507.09391)
*Peter Pao-Huang,Mitchell Black,Xiaojie Qiu*

Main category: cs.LG

TL;DR: 提出了一种噪声条件图网络（NCGNs），通过动态调整架构以适应噪声水平，提升了生成图的性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于流的图生成模型使用与噪声无关的图神经网络架构，限制了表达能力。

Method: 引入NCGNs，开发动态消息传递（DMP）方法，根据噪声水平调整消息传递的范围和分辨率。

Result: DMP在3D点云、时空转录组学和图像等多个领域优于噪声无关架构。

Conclusion: NCGNs通过动态适应噪声水平，显著提升了图生成模型的性能。

Abstract: Generative modeling of graphs with spatial structure is essential across many
applications from computer graphics to spatial genomics. Recent flow-based
generative models have achieved impressive results by gradually adding and then
learning to remove noise from these graphs. Existing models, however, use graph
neural network architectures that are independent of the noise level, limiting
their expressiveness. To address this issue, we introduce
\textit{Noise-Conditioned Graph Networks} (NCGNs), a class of graph neural
networks that dynamically modify their architecture according to the noise
level during generation. Our theoretical and empirical analysis reveals that as
noise increases, (1) graphs require information from increasingly distant
neighbors and (2) graphs can be effectively represented at lower resolutions.
Based on these insights, we develop Dynamic Message Passing (DMP), a specific
instantiation of NCGNs that adapts both the range and resolution of message
passing to the noise level. DMP consistently outperforms noise-independent
architectures on a variety of domains including $3$D point clouds,
spatiotemporal transcriptomics, and images. Code is available at
https://github.com/peterpaohuang/ncgn.

</details>


### [335] [A Random Matrix Theory Perspective on the Learning Dynamics of Multi-head Latent Attention](https://arxiv.org/abs/2507.09394)
*Nandan Kumar Jha,Brandon Reagen*

Main category: cs.LG

TL;DR: 研究了多头潜在注意力（MLA）对预训练中Transformer内部容量的影响，发现旋转嵌入的应用方式对防止容量瓶颈至关重要。


<details>
  <summary>Details</summary>
Motivation: 探讨多头潜在注意力（MLA）在压缩键/值记忆时如何影响Transformer的预训练内部容量。

Method: 使用Marchenko-Pastur（MP）诊断分析$W_{Q}W_{K}^\top$矩阵的频谱，比较标准多头注意力（MHA）、MLA-PreRoPE和MLA-Decoupled三种变体。

Result: 发现容量瓶颈局部出现，仅MLA-Decoupled能防止频谱碎片化并保持表达能力。

Conclusion: 旋转嵌入的应用方式与压缩位置同样重要，共享旋转子向量可维持模型容量。

Abstract: In this work, we study how multi-head latent attention (MLA), a popular
strategy for compressing key/value memory, affects a transformer's internal
capacity during pretraining. Using a lightweight suite of Marchenko-Pastur (MP)
diagnostics, we analyze the spectrum of the $W_{Q}W_{K}^\top$ gram matrix
throughout training, comparing three variants: the standard multi-head
attention (MHA) baseline, MLA-PreRoPE with rotary applied before compression,
and MLA-Decoupled, which shares a single rotary sub-vector across all heads.
Our random matrix analysis reveals \textbf{three key findings:} \textbf{ i)}
capacity bottlenecks emerge locally: both MHA and MLA-PreRoPE exhibit sharp,
early spikes in specific layers that persist and propagate, disrupting the
balance between bulk and outlier directions; \textbf{ ii)} these spikes
coincide with rank collapse, concentrating the model's expressivity into narrow
subspaces; \textbf{ iii)} only the decoupled variant prevents this cascade,
maintaining broad spectral support and suppressing outlier formation across
layers. These results underscore that \emph{how} rotary embeddings are applied
is just as critical as \emph{where} compression occurs. Sharing rotary
components across heads mitigates spectral fragmentation and preserves
representational capacity.

</details>


### [336] [Scaling Laws for Optimal Data Mixtures](https://arxiv.org/abs/2507.09404)
*Mustafa Shukor,Louis Bethune,Dan Busbridge,David Grangier,Enrico Fini,Alaaeldin El-Nouby,Pierre Ablin*

Main category: cs.LG

TL;DR: 提出了一种基于缩放定律的系统方法，用于确定目标领域的最优数据混合比例，避免了传统试错方法的高成本。


<details>
  <summary>Details</summary>
Motivation: 传统试错方法在大规模预训练中不切实际，需要一种更高效的方法来确定数据混合比例。

Method: 利用缩放定律预测模型在不同数据混合比例下的性能，并通过小规模训练估计参数。

Result: 验证了缩放定律在多种大规模场景（LLM、NMM、LVM）中的普适性，并能外推到新数据混合比例和规模。

Conclusion: 该方法为优化数据混合比例提供了理论支持，显著降低了预训练成本。

Abstract: Large foundation models are typically trained on data from multiple domains,
with the data mixture--the proportion of each domain used--playing a critical
role in model performance. The standard approach to selecting this mixture
relies on trial and error, which becomes impractical for large-scale
pretraining. We propose a systematic method to determine the optimal data
mixture for any target domain using scaling laws. Our approach accurately
predicts the loss of a model of size $N$ trained with $D$ tokens and a specific
domain weight vector $h$. We validate the universality of these scaling laws by
demonstrating their predictive power in three distinct and large-scale
settings: large language model (LLM), native multimodal model (NMM), and large
vision models (LVM) pretraining. We further show that these scaling laws can
extrapolate to new data mixtures and across scales: their parameters can be
accurately estimated using a few small-scale training runs, and used to
estimate the performance at larger scales and unseen domain weights. The
scaling laws allow to derive the optimal domain weights for any target domain
under a given training budget ($N$,$D$), providing a principled alternative to
costly trial-and-error methods.

</details>


### [337] [Adversarial Activation Patching: A Framework for Detecting and Mitigating Emergent Deception in Safety-Aligned Transformers](https://arxiv.org/abs/2507.09406)
*Santhosh Kumar Ravindran*

Main category: cs.LG

TL;DR: 论文提出了一种新的对抗性激活修补框架，用于检测和缓解大型语言模型中的欺骗行为，通过实验验证了其有效性，并提出了未来的研究方向。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）在安全对齐后仍可能表现出隐蔽的欺骗行为，需要一种机制来检测和缓解此类问题。

Method: 采用对抗性激活修补技术，通过将“欺骗性”提示的激活修补到安全前向传递中，模拟漏洞并量化欺骗率。

Result: 实验表明，对抗性修补将欺骗性输出从0%提高到23.9%，并验证了层特异性差异。

Conclusion: 该研究为AI安全提供了新工具，并提出了未来在大型模型上的实证研究方向。

Abstract: Large language models (LLMs) aligned for safety through techniques like
reinforcement learning from human feedback (RLHF) often exhibit emergent
deceptive behaviors, where outputs appear compliant but subtly mislead or omit
critical information. This paper introduces adversarial activation patching, a
novel mechanistic interpretability framework that leverages activation patching
as an adversarial tool to induce, detect, and mitigate such deception in
transformer-based models. By sourcing activations from "deceptive" prompts and
patching them into safe forward passes at specific layers, we simulate
vulnerabilities and quantify deception rates. Through toy neural network
simulations across multiple scenarios (e.g., 1000 trials per setup), we
demonstrate that adversarial patching increases deceptive outputs to 23.9% from
a 0% baseline, with layer-specific variations supporting our hypotheses. We
propose six hypotheses, including transferability across models, exacerbation
in multimodal settings, and scaling effects. An expanded literature review
synthesizes over 20 key works in interpretability, deception, and adversarial
attacks. Mitigation strategies, such as activation anomaly detection and robust
fine-tuning, are detailed, alongside ethical considerations and future research
directions. This work advances AI safety by highlighting patching's dual-use
potential and provides a roadmap for empirical studies on large-scale models.

</details>


### [338] [On Information Geometry and Iterative Optimization in Model Compression: Operator Factorization](https://arxiv.org/abs/2507.09428)
*Zakhar Shumaylov,Vasileios Tsiaras,Yannis Stylianou*

Main category: cs.LG

TL;DR: 论文探讨了信息几何在深度学习模型压缩中的应用，强调通过定义低计算子流形和投影实现压缩，并分析了信息差异和迭代方法的重要性。


<details>
  <summary>Details</summary>
Motivation: 解决深度学习模型参数增长带来的资源受限设备部署问题，探索信息几何在模型压缩中的应用。

Method: 采用信息几何视角分析现有压缩方法，提出使用信息差异和迭代奇异值阈值方法进行模型压缩。

Result: 证明迭代奇异值阈值方法在软秩约束下的收敛性，并通过简单修改现有方法提升压缩性能。

Conclusion: 信息几何视角为模型压缩提供了新思路，信息差异和迭代方法在不同场景下对压缩效果至关重要。

Abstract: The ever-increasing parameter counts of deep learning models necessitate
effective compression techniques for deployment on resource-constrained
devices. This paper explores the application of information geometry, the study
of density-induced metrics on parameter spaces, to analyze existing methods
within the space of model compression, primarily focusing on operator
factorization. Adopting this perspective highlights the core challenge:
defining an optimal low-compute submanifold (or subset) and projecting onto it.
We argue that many successful model compression approaches can be understood as
implicitly approximating information divergences for this projection. We
highlight that when compressing a pre-trained model, using information
divergences is paramount for achieving improved zero-shot accuracy, yet this
may no longer be the case when the model is fine-tuned. In such scenarios,
trainability of bottlenecked models turns out to be far more important for
achieving high compression ratios with minimal performance degradation,
necessitating adoption of iterative methods. In this context, we prove
convergence of iterative singular value thresholding for training neural
networks subject to a soft rank constraint. To further illustrate the utility
of this perspective, we showcase how simple modifications to existing methods
through softer rank reduction result in improved performance under fixed
compression rates.

</details>


### [339] [Dynamic Sparse Causal-Attention Temporal Networks for Interpretable Causality Discovery in Multivariate Time Series](https://arxiv.org/abs/2507.09439)
*Meriem Zerkouk,Miloud Mihoubi,Belkacem Chikhaoui*

Main category: cs.LG

TL;DR: DyCAST-Net是一种新型架构，通过结合扩张时间卷积和动态稀疏注意力机制，提升多元时间序列中的因果发现能力，并在金融和营销数据上表现优异。


<details>
  <summary>Details</summary>
Motivation: 多元时间序列中的因果关系理解对金融和营销等领域至关重要，但传统方法难以处理复杂依赖和滞后效应。

Method: DyCAST-Net结合扩张时间卷积和多尺度时间依赖捕捉，动态稀疏注意力机制通过自适应阈值消除虚假连接，统计洗牌测试验证增强鲁棒性。

Result: 在金融和营销数据集上，DyCAST-Net优于TCDF、GCFormer和CausalFormer，能更精确估计因果延迟并减少虚假发现。注意力热图提供可解释性，揭示隐藏因果模式。

Conclusion: DyCAST-Net在高维动态环境中表现优异，其架构通过RMSNorm稳定和因果掩码确保可扩展性和适应性。

Abstract: Understanding causal relationships in multivariate time series (MTS) is
essential for effective decision-making in fields such as finance and
marketing, where complex dependencies and lagged effects challenge conventional
analytical approaches. We introduce Dynamic Sparse Causal-Attention Temporal
Networks for Interpretable Causality Discovery in MTS (DyCAST-Net), a novel
architecture designed to enhance causal discovery by integrating dilated
temporal convolutions and dynamic sparse attention mechanisms. DyCAST-Net
effectively captures multiscale temporal dependencies through dilated
convolutions while leveraging an adaptive thresholding strategy in its
attention mechanism to eliminate spurious connections, ensuring both accuracy
and interpretability. A statistical shuffle test validation further strengthens
robustness by filtering false positives and improving causal inference
reliability. Extensive evaluations on financial and marketing datasets
demonstrate that DyCAST-Net consistently outperforms existing models such as
TCDF, GCFormer, and CausalFormer. The model provides a more precise estimation
of causal delays and significantly reduces false discoveries, particularly in
noisy environments. Moreover, attention heatmaps offer interpretable insights,
uncovering hidden causal patterns such as the mediated effects of advertising
on consumer behavior and the influence of macroeconomic indicators on financial
markets. Case studies illustrate DyCAST-Net's ability to detect latent
mediators and lagged causal factors, making it particularly effective in
high-dimensional, dynamic settings. The model's architecture enhanced by
RMSNorm stabilization and causal masking ensures scalability and adaptability
across diverse application domains

</details>


### [340] [Transformers Don't In-Context Learn Least Squares Regression](https://arxiv.org/abs/2507.09440)
*Joshua Hill,Benjamin Eyre,Elliot Creager*

Main category: cs.LG

TL;DR: 论文研究了大型预训练变压器在上下文学习（ICL）中的机制，发现其泛化能力受限于训练数据分布，且表现与OLS等算法不一致。


<details>
  <summary>Details</summary>
Motivation: 探索变压器在ICL中的学习机制，揭示其为何能在无梯度更新的情况下解决新任务。

Method: 通过合成线性回归任务和分布外泛化实验，分析变压器的学习行为。

Result: 变压器在分布外数据上泛化能力差，且学习行为与OLS等算法不一致；预训练数据对ICL行为有显著影响。

Conclusion: ICL的机制依赖于预训练数据的分布，变压器并未实现通用学习算法，而是依赖于数据特定的表示。

Abstract: In-context learning (ICL) has emerged as a powerful capability of large
pretrained transformers, enabling them to solve new tasks implicit in example
input-output pairs without any gradient updates. Despite its practical success,
the mechanisms underlying ICL remain largely mysterious. In this work we study
synthetic linear regression to probe how transformers implement learning at
inference time. Previous works have demonstrated that transformers match the
performance of learning rules such as Ordinary Least Squares (OLS) regression
or gradient descent and have suggested ICL is facilitated in transformers
through the learned implementation of one of these techniques. In this work, we
demonstrate through a suite of out-of-distribution generalization experiments
that transformers trained for ICL fail to generalize after shifts in the prompt
distribution, a behaviour that is inconsistent with the notion of transformers
implementing algorithms such as OLS. Finally, we highlight the role of the
pretraining corpus in shaping ICL behaviour through a spectral analysis of the
learned representations in the residual stream. Inputs from the same
distribution as the training data produce representations with a unique
spectral signature: inputs from this distribution tend to have the same top two
singular vectors. This spectral signature is not shared by out-of-distribution
inputs, and a metric characterizing the presence of this signature is highly
correlated with low loss.

</details>


### [341] [Toward Developing Machine-Learning-Aided Tools for the Thermomechanical Monitoring of Nuclear Reactor Components](https://arxiv.org/abs/2507.09443)
*Luiz Aldeia Machado,Victor Coppo Leite,Elia Merzari,Arthur Motta,Roberto Ponciroli,Lander Ibarra,Lise Charlot*

Main category: cs.LG

TL;DR: 论文提出了一种结合CNN和热力学模型的方法，用于实时监测核电站燃料棒的温度、应力和应变，以支持预测性维护。


<details>
  <summary>Details</summary>
Motivation: 核电站的预测性维护（PdM）需要减少因组件故障导致的意外停机时间，而现有方法依赖于有限的温度测量数据。

Method: 使用CNN架构结合热力学模型，通过有限元模拟生成训练数据，并利用BISON和MOOSE-THM进行耦合仿真。

Result: CNN训练1000多轮未出现过拟合，能高精度预测温度分布，进而计算燃料棒的应力和应变分布。

Conclusion: 该方法为核反应堆的实时监测和预测性维护提供了潜在工具。

Abstract: Proactive maintenance strategies, such as Predictive Maintenance (PdM), play
an important role in the operation of Nuclear Power Plants (NPPs), particularly
due to their capacity to reduce offline time by preventing unexpected shutdowns
caused by component failures.
  In this work, we explore the use of a Convolutional Neural Network (CNN)
architecture combined with a computational thermomechanical model to calculate
the temperature, stress, and strain of a Pressurized Water Reactor (PWR) fuel
rod during operation. This estimation relies on a limited number of temperature
measurements from the cladding's outer surface. This methodology can
potentially aid in developing PdM tools for nuclear reactors by enabling
real-time monitoring of such systems.
  The training, validation, and testing datasets were generated through coupled
simulations involving BISON, a finite element-based nuclear fuel performance
code, and the MOOSE Thermal-Hydraulics Module (MOOSE-THM). We conducted eleven
simulations, varying the peak linear heat generation rates. Of these, eight
were used for training, two for validation, and one for testing.
  The CNN was trained for over 1,000 epochs without signs of overfitting,
achieving highly accurate temperature distribution predictions. These were then
used in a thermomechanical model to determine the stress and strain
distribution within the fuel rod.

</details>


### [342] [Fourier Basis Mapping: A Time-Frequency Learning Framework for Time Series Forecasting](https://arxiv.org/abs/2507.09445)
*Runze Yang,Longbing Cao,Xin You,Kun Fang,Jianxun Li,Jie Yang*

Main category: cs.LG

TL;DR: 论文提出了一种新的傅里叶基映射（FBM）方法，通过结合时间-频率特征改进时间序列预测，解决了现有傅里叶方法的局限性，并在多种神经网络中验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有基于傅里叶变换的方法存在起始周期不一致、序列长度不一致等问题，且无法精确解释频率成分或忽略时间信息。

Method: 提出FBM方法，通过傅里叶基展开和时间-频率空间映射整合时间-频率特征，支持与多种神经网络（线性、MLP、Transformer等）的即插即用集成。

Result: 在多种真实数据集上验证了FBM在长短期预测任务中的SOTA性能。

Conclusion: FBM方法有效解决了现有傅里叶方法的局限性，显著提升了时间序列预测的准确性。

Abstract: The integration of Fourier transform and deep learning opens new avenues for
time series forecasting. We reconsider the Fourier transform from a basis
functions perspective. Specifically, the real and imaginary parts of the
frequency components can be regarded as the coefficients of cosine and sine
basis functions at tiered frequency levels, respectively. We find that existing
Fourier-based methods face inconsistent starting cycles and inconsistent series
length issues. They fail to interpret frequency components precisely and
overlook temporal information. Accordingly, the novel Fourier Basis Mapping
(FBM) method addresses these issues by integrating time-frequency features
through Fourier basis expansion and mapping in the time-frequency space. Our
approach extracts explicit frequency features while preserving temporal
characteristics. FBM supports plug-and-play integration with various types of
neural networks by only adjusting the first initial projection layer for better
performance. First, we propose FBM-L, FBM-NL, and FBM-NP to enhance linear,
MLP-based, and Transformer-based models, respectively, demonstrating the
effectiveness of time-frequency features. Next, we propose a synergetic model
architecture, termed FBM-S, which decomposes the seasonal, trend, and
interaction effects into three separate blocks, each designed to model
time-frequency features in a specialized manner. Finally, we introduce several
techniques tailored for time-frequency features, including interaction masking,
centralization, patching, rolling window projection, and multi-scale
down-sampling. The results are validated on diverse real-world datasets for
both long-term and short-term forecasting tasks with SOTA performance.

</details>


### [343] [Enhancing ALS Progression Tracking with Semi-Supervised ALSFRS-R Scores Estimated from Ambient Home Health Monitoring](https://arxiv.org/abs/2507.09460)
*Noah Marchal,William E. Janes,Mihail Popescu,Xing Song*

Main category: cs.LG

TL;DR: 该论文开发了半监督回归模型，通过家庭传感器数据监测ALS患者功能衰退，比较了不同学习方法和插值技术，发现自适应模型选择能提高预测准确性。


<details>
  <summary>Details</summary>
Motivation: 解决临床定期评估可能遗漏ALS患者功能衰退关键变化的问题，通过连续家庭监测数据改进预测模型。

Method: 比较三种模型范式（个体批量学习、群体批量学习与增量微调迁移学习）及线性、立方多项式、自注意力插值方法。

Result: 迁移学习在32次对比中28次改善预测误差，自注意力插值在子量表模型中表现最佳，线性插值在综合量表中更稳定。

Conclusion: 根据功能域的同质-异质特征匹配学习方法和插值技术，可提升ALS进展预测准确性，未来可整合自适应模型选择以实现及时干预。

Abstract: Clinical monitoring of functional decline in ALS relies on periodic
assessments that may miss critical changes occurring between visits. To address
this gap, semi-supervised regression models were developed to estimate rates of
decline in a case series cohort by targeting ALSFRS- R scale trajectories with
continuous in-home sensor monitoring data. Our analysis compared three model
paradigms (individual batch learning and cohort-level batch versus incremental
fine-tuned transfer learning) across linear slope, cubic polynomial, and
ensembled self-attention pseudo-label interpolations. Results revealed cohort
homogeneity across functional domains responding to learning methods, with
transfer learning improving prediction error for ALSFRS-R subscales in 28 of 32
contrasts (mean RMSE=0.20(0.04)), and individual batch learning for predicting
the composite scale (mean RMSE=3.15(1.25)) in 2 of 3. Self-attention
interpolation achieved the lowest prediction error for subscale-level models
(mean RMSE=0.19(0.06)), capturing complex nonlinear progression patterns,
outperforming linear and cubic interpolations in 20 of 32 contrasts, though
linear interpolation proved more stable in all ALSFRS-R composite scale models
(mean RMSE=0.23(0.10)). We identified distinct homogeneity-heterogeneity
profiles across functional domains with respiratory and speech exhibiting
patient-specific patterns benefiting from personalized incremental adaptation,
while swallowing and dressing functions followed cohort-level trajectories
suitable for transfer models. These findings suggest that matching learning and
pseudo-labeling techniques to functional domain-specific
homogeneity-heterogeneity profiles enhances predictive accuracy in ALS
progression tracking. Integrating adaptive model selection within sensor
monitoring platforms could enable timely interventions and scalable deployment
in future multi-center studies.

</details>


### [344] [La-Proteina: Atomistic Protein Generation via Partially Latent Flow Matching](https://arxiv.org/abs/2507.09466)
*Tomas Geffner,Kieran Didi,Zhonglin Cao,Danny Reidenbach,Zuobai Zhang,Christian Dallago,Emine Kucukbenli,Karsten Kreis,Arash Vahdat*

Main category: cs.LG

TL;DR: La-Proteina是一种基于部分潜在蛋白质表示的新型原子级蛋白质设计模型，通过固定维度的残基潜在变量捕获序列和原子细节，解决了侧链长度变化的问题，并在多个生成基准测试中表现优异。


<details>
  <summary>Details</summary>
Motivation: 当前生成模型难以直接生成原子级蛋白质结构及其对应的氨基酸序列，尤其是需要处理侧链长度变化的问题。

Method: La-Proteina采用部分潜在表示，显式建模粗粒度主链结构，同时通过固定维度的残基潜在变量捕获序列和原子细节，利用流匹配建模联合分布。

Result: La-Proteina在原子级共设计性、多样性和结构有效性等基准测试中表现优异，并能生成长达800个残基的可共设计蛋白质。

Conclusion: La-Proteina在原子级蛋白质设计任务中表现出卓越的性能和可扩展性，解决了现有模型的局限性。

Abstract: Recently, many generative models for de novo protein structure design have
emerged. Yet, only few tackle the difficult task of directly generating fully
atomistic structures jointly with the underlying amino acid sequence. This is
challenging, for instance, because the model must reason over side chains that
change in length during generation. We introduce La-Proteina for atomistic
protein design based on a novel partially latent protein representation: coarse
backbone structure is modeled explicitly, while sequence and atomistic details
are captured via per-residue latent variables of fixed dimensionality, thereby
effectively side-stepping challenges of explicit side-chain representations.
Flow matching in this partially latent space then models the joint distribution
over sequences and full-atom structures. La-Proteina achieves state-of-the-art
performance on multiple generation benchmarks, including all-atom
co-designability, diversity, and structural validity, as confirmed through
detailed structural analyses and evaluations. Notably, La-Proteina also
surpasses previous models in atomistic motif scaffolding performance, unlocking
critical atomistic structure-conditioned protein design tasks. Moreover,
La-Proteina is able to generate co-designable proteins of up to 800 residues, a
regime where most baselines collapse and fail to produce valid samples,
demonstrating La-Proteina's scalability and robustness.

</details>


### [345] [Discrete Differential Principle for Continuous Smooth Function Representation](https://arxiv.org/abs/2507.09480)
*Guoyou Wang,Yihua Tan,Shiqi Liu*

Main category: cs.LG

TL;DR: 提出了一种新的离散微分算子，通过范德蒙系数矩阵估计导数并局部表示连续平滑函数，解决了泰勒公式在高维和离散情况下的误差传播问题。


<details>
  <summary>Details</summary>
Motivation: 泰勒公式在函数表示中很重要，但在离散情况下存在维度灾难和导数计算误差传播问题。

Method: 使用截断泰勒级数导出的范德蒙系数矩阵，提出新的离散微分算子，同时计算所有低阶导数，减轻误差传播。

Result: 数学上建立了严格的误差界，实验证明优于有限前向差分法和插值方法。

Conclusion: 该方法在视觉表示、流体力学等领域具有广泛应用前景。

Abstract: Taylor's formula holds significant importance in function representation,
such as solving differential difference equations, ordinary differential
equations, partial differential equations, and further promotes applications in
visual perception, complex control, fluid mechanics, weather forecasting and
thermodynamics. However, the Taylor's formula suffers from the curse of
dimensionality and error propagation during derivative computation in discrete
situations. In this paper, we propose a new discrete differential operator to
estimate derivatives and to represent continuous smooth function locally using
the Vandermonde coefficient matrix derived from truncated Taylor series. Our
method simultaneously computes all derivatives of orders less than the number
of sample points, inherently mitigating error propagation. Utilizing
equidistant uniform sampling, it achieves high-order accuracy while alleviating
the curse of dimensionality. We mathematically establish rigorous error bounds
for both derivative estimation and function representation, demonstrating
tighter bounds for lower-order derivatives. We extend our method to the
two-dimensional case, enabling its use for multivariate derivative
calculations. Experiments demonstrate the effectiveness and superiority of the
proposed method compared to the finite forward difference method for derivative
estimation and cubic spline and linear interpolation for function
representation. Consequently, our technique offers broad applicability across
domains such as vision representation, feature extraction, fluid mechanics, and
cross-media imaging.

</details>


### [346] [An Analysis of Action-Value Temporal-Difference Methods That Learn State Values](https://arxiv.org/abs/2507.09523)
*Brett Daley,Prabhat Nagarajan,Martha White,Marlos C. Machado*

Main category: cs.LG

TL;DR: 论文分析了基于两个非对称价值函数的TD学习方法（QV-learning和AV-learning），发现AV-learning在控制任务中优于Q-learning，并提出新算法RDQ，性能优于Dueling DQN。


<details>
  <summary>Details</summary>
Motivation: 探讨学习两个价值函数（而非单一动作价值函数）是否在理论和实践中具有优势，尤其是在收敛性和样本效率方面。

Method: 分析了QV-learning和AV-learning的收敛性和样本效率，并提出了新的AV-learning算法RDQ。

Result: AV-learning在控制任务中表现优于Q-learning，RDQ在MinAtar基准测试中显著优于Dueling DQN。

Conclusion: AV-learning方法在控制任务中具有优势，新算法RDQ展示了其有效性。

Abstract: The hallmark feature of temporal-difference (TD) learning is bootstrapping:
using value predictions to generate new value predictions. The vast majority of
TD methods for control learn a policy by bootstrapping from a single
action-value function (e.g., Q-learning and Sarsa). Significantly less
attention has been given to methods that bootstrap from two asymmetric value
functions: i.e., methods that learn state values as an intermediate step in
learning action values. Existing algorithms in this vein can be categorized as
either QV-learning or AV-learning. Though these algorithms have been
investigated to some degree in prior work, it remains unclear if and when it is
advantageous to learn two value functions instead of just one -- and whether
such approaches are theoretically sound in general. In this paper, we analyze
these algorithmic families in terms of convergence and sample efficiency. We
find that while both families are more efficient than Expected Sarsa in the
prediction setting, only AV-learning methods offer any major benefit over
Q-learning in the control setting. Finally, we introduce a new AV-learning
algorithm called Regularized Dueling Q-learning (RDQ), which significantly
outperforms Dueling DQN in the MinAtar benchmark.

</details>


### [347] [Assessing reliability of explanations in unbalanced datasets: a use-case on the occurrence of frost events](https://arxiv.org/abs/2507.09545)
*Ilaria Vascotto,Valentina Blasone,Alex Rodriguez,Alessandro Bonaita,Luca Bortolussi*

Main category: cs.LG

TL;DR: 该研究探讨了在不平衡数据集中评估XAI方法解释可靠性的初步方法，提出了一种针对少数类的评估框架，并通过一个霜冻事件案例验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 随着AI模型的广泛应用和立法要求的增加，XAI方法的解释鲁棒性成为关键问题，尤其是在不平衡数据集这一高风险场景中。

Method: 研究提出了一种基于少数类的评估方法，包括邻域生成、解释聚合和一致性度量。

Result: 通过一个霜冻事件的表格数据集案例，验证了所提方法的有效性。

Conclusion: 该研究为不平衡数据集中XAI解释的可靠性评估提供了初步框架，强调了少数类解释的重要性。

Abstract: The usage of eXplainable Artificial Intelligence (XAI) methods has become
essential in practical applications, given the increasing deployment of
Artificial Intelligence (AI) models and the legislative requirements put
forward in the latest years. A fundamental but often underestimated aspect of
the explanations is their robustness, a key property that should be satisfied
in order to trust the explanations. In this study, we provide some preliminary
insights on evaluating the reliability of explanations in the specific case of
unbalanced datasets, which are very frequent in high-risk use-cases, but at the
same time considerably challenging for both AI models and XAI methods. We
propose a simple evaluation focused on the minority class (i.e. the less
frequent one) that leverages on-manifold generation of neighbours, explanation
aggregation and a metric to test explanation consistency. We present a use-case
based on a tabular dataset with numerical features focusing on the occurrence
of frost events.

</details>


### [348] [Holistix: A Dataset for Holistic Wellness Dimensions Analysis in Mental Health Narratives](https://arxiv.org/abs/2507.09565)
*Heeba Shakeel,Tanvir Ahmad,Chandni Saxena*

Main category: cs.LG

TL;DR: 论文介绍了一个用于社交媒体用户帖子中健康维度分类的数据集，涵盖六个关键方面：身体、情感、社交、智力、精神和职业。


<details>
  <summary>Details</summary>
Motivation: 旨在通过用户生成内容捕捉这些健康维度，支持区域特定的健康评估和个性化心理健康干预。

Method: 开发了全面的注释框架，评估了传统机器学习和基于Transformer的模型，使用10折交叉验证的精确度、召回率和F1分数。

Result: 模型表现通过后解释确保透明度和可解释性，数据集公开在Github上。

Conclusion: 该数据集为社交媒体健康评估和早期干预策略提供了基础，并遵循了伦理准则。

Abstract: We introduce a dataset for classifying wellness dimensions in social media
user posts, covering six key aspects: physical, emotional, social,
intellectual, spiritual, and vocational. The dataset is designed to capture
these dimensions in user-generated content, with a comprehensive annotation
framework developed under the guidance of domain experts. This framework allows
for the classification of text spans into the appropriate wellness categories.
We evaluate both traditional machine learning models and advanced
transformer-based models for this multi-class classification task, with
performance assessed using precision, recall, and F1-score, averaged over
10-fold cross-validation. Post-hoc explanations are applied to ensure the
transparency and interpretability of model decisions. The proposed dataset
contributes to region-specific wellness assessments in social media and paves
the way for personalized well-being evaluations and early intervention
strategies in mental health. We adhere to ethical considerations for
constructing and releasing our experiments and dataset publicly on Github.

</details>


### [349] [DRAGD: A Federated Unlearning Data Reconstruction Attack Based on Gradient Differences](https://arxiv.org/abs/2507.09602)
*Bocheng Ju,Junchao Fan,Jiaqi Liu,Xiaolin Chang*

Main category: cs.LG

TL;DR: 论文提出DRAGD和DRAGDP攻击方法，利用联邦学习中遗忘前后的梯度差异重构被删除数据，揭示了联邦遗忘的隐私漏洞。


<details>
  <summary>Details</summary>
Motivation: 联邦遗忘虽保护数据隐私，但其梯度交换可能泄露敏感信息，需研究其隐私风险。

Method: 提出DRAGD攻击，利用梯度差异重构数据；进一步提出DRAGDP，结合公开数据提升重构精度。

Result: 实验表明DRAGD和DRAGDP在数据重构上显著优于现有方法。

Conclusion: 研究揭示了联邦遗忘的隐私漏洞，为实际应用中的安全性提供了改进方向。

Abstract: Federated learning enables collaborative machine learning while preserving
data privacy. However, the rise of federated unlearning, designed to allow
clients to erase their data from the global model, introduces new privacy
concerns. Specifically, the gradient exchanges during the unlearning process
can leak sensitive information about deleted data. In this paper, we introduce
DRAGD, a novel attack that exploits gradient discrepancies before and after
unlearning to reconstruct forgotten data. We also present DRAGDP, an enhanced
version of DRAGD that leverages publicly available prior data to improve
reconstruction accuracy, particularly for complex datasets like facial images.
Extensive experiments across multiple datasets demonstrate that DRAGD and
DRAGDP significantly outperform existing methods in data reconstruction.Our
work highlights a critical privacy vulnerability in federated unlearning and
offers a practical solution, advancing the security of federated unlearning
systems in real-world applications.

</details>


### [350] [MLoRQ: Bridging Low-Rank and Quantization for Transformer Compression](https://arxiv.org/abs/2507.09616)
*Ofir Gordon,Ariel Lapid,Elad Cohen,Yarden Yagil,Arnon Netzer,Hai Victor Habi*

Main category: cs.LG

TL;DR: MLoRQ是一种结合低秩近似和混合精度量化的新方法，用于在资源受限的边缘设备上部署Transformer网络。通过两阶段优化过程，MLoRQ在满足内存约束的同时，实现了高达15%的性能提升。


<details>
  <summary>Details</summary>
Motivation: 在边缘设备上部署Transformer网络面临资源限制的挑战，现有技术如低秩近似和量化各有局限，需要一种更高效的结合方法。

Method: MLoRQ采用两阶段优化：1）层内优化，筛选低秩和量化的最优组合；2）层间优化，分配位宽和秩以满足内存约束。可选步骤使用自适应舍入技术减少误差。

Result: 在Vision Transformers上测试，MLoRQ在图像分类、目标检测和实例分割任务中性能提升高达15%。

Conclusion: MLoRQ是一种高效且兼容性强的压缩方法，适用于边缘设备上的Transformer部署，显著提升了性能。

Abstract: Deploying transformer-based neural networks on resource-constrained edge
devices presents a significant challenge. This challenge is often addressed
through various techniques, such as low-rank approximation and mixed-precision
quantization. In this work, we introduce Mixed Low-Rank and Quantization
(MLoRQ), a novel method that integrates both techniques. MLoRQ employs a
two-stage optimization process to determine optimal bit-width and rank
assignments for each layer, adhering to predefined memory constraints. This
process includes: (i) an intra-layer optimization that identifies potentially
optimal compression solutions out of all low-rank and quantization
combinations; (ii) an inter-layer optimization that assigns bit-width precision
and rank to each layer while ensuring the memory constraint is met. An optional
final step applies a sequential optimization process using a modified adaptive
rounding technique to mitigate compression-induced errors in joint low-rank
approximation and quantization. The method is compatible and can be seamlessly
integrated with most existing quantization algorithms. MLoRQ shows
state-of-the-art results with up to 15\% performance improvement, evaluated on
Vision Transformers for image classification, object detection, and instance
segmentation tasks.

</details>


### [351] [Cultivating Pluralism In Algorithmic Monoculture: The Community Alignment Dataset](https://arxiv.org/abs/2507.09650)
*Lily Hong Zhang,Smitha Milli,Karen Jusko,Jonathan Smith,Brandon Amos,Wassim,Bouaziz,Manon Revel,Jack Kussman,Lisa Titus,Bhaktipriya Radharapu,Jane Yu,Vidya Sarma,Kris Rose,Maximilian Nickel*

Main category: cs.LG

TL;DR: 论文通过大规模多语言人类研究，发现人类偏好多样性远超现有LLMs，提出负相关采样方法改进对齐技术，并开源了最大的多语言偏好数据集Community Alignment。


<details>
  <summary>Details</summary>
Motivation: 研究如何让大语言模型（LLMs）适应不同文化、政治等多维度的用户偏好冲突。

Method: 通过五国大规模人类研究（N=15,000），分析人类偏好与21种先进LLMs的差异，提出负相关采样方法改进数据集收集。

Result: 发现现有方法无法捕捉人类偏好多样性，负相关采样显著提升对齐方法性能，并开源了Community Alignment数据集。

Conclusion: Community Alignment数据集有望帮助LLMs更好地服务全球多样化用户。

Abstract: How can large language models (LLMs) serve users with varying preferences
that may conflict across cultural, political, or other dimensions? To advance
this challenge, this paper establishes four key results. First, we demonstrate,
through a large-scale multilingual human study with representative samples from
five countries (N=15,000), that humans exhibit significantly more variation in
preferences than the responses of 21 state-of-the-art LLMs. Second, we show
that existing methods for preference dataset collection are insufficient for
learning the diversity of human preferences even along two of the most salient
dimensions of variability in global values, due to the underlying homogeneity
of candidate responses. Third, we argue that this motivates the need for
negatively-correlated sampling when generating candidate sets, and we show that
simple prompt-based techniques for doing so significantly enhance the
performance of alignment methods in learning heterogeneous preferences. Fourth,
based on this novel candidate sampling approach, we collect and open-source
Community Alignment, the largest and most representative multilingual and
multi-turn preference dataset to date, featuring almost 200,000 comparisons
from annotators spanning five countries. We hope that the Community Alignment
dataset will be a valuable resource for improving the effectiveness of LLMs for
a diverse global population.

</details>


### [352] [Conformal Prediction for Privacy-Preserving Machine Learning](https://arxiv.org/abs/2507.09678)
*Alexander David Balinsky,Dominik Krzeminski,Alexander Balinsky*

Main category: cs.LG

TL;DR: 研究了在确定性加密数据上结合Conformal Prediction（CP）和监督学习的方法，验证了CP在加密域中的有效性，并比较了基于$p$-value和$e$-value的CP方法。


<details>
  <summary>Details</summary>
Motivation: 旨在填补严格的概率不确定性量化与隐私保护机器学习之间的空白。

Method: 使用AES加密的MNIST数据集，测试基于$p$-value和$e$-value的CP方法在加密数据上的表现。

Result: 加密数据上训练的模型仍能提取有意义的结构（测试准确率36.88%），$e$-value CP在60%覆盖率下表现优于$p$-value CP。

Conclusion: CP在加密数据中具有潜力，但也揭示了预测集紧凑性与可靠性之间的权衡。

Abstract: We investigate the integration of Conformal Prediction (CP) with supervised
learning on deterministically encrypted data, aiming to bridge the gap between
rigorous uncertainty quantification and privacy-preserving machine learning.
Using AES-encrypted variants of the MNIST dataset, we demonstrate that CP
methods remain effective even when applied directly in the encrypted domain,
owing to the preservation of data exchangeability under fixed-key encryption.
We test traditional $p$-value-based against $e$-value-based conformal
predictors. Our empirical evaluation reveals that models trained on
deterministically encrypted data retain the ability to extract meaningful
structure, achieving 36.88\% test accuracy -- significantly above random
guessing (9.56\%) observed with per-instance encryption. Moreover,
$e$-value-based CP achieves predictive set coverage of over 60\% with 4.3
loss-threshold calibration, correctly capturing the true label in 4888 out of
5000 test cases. In contrast, the $p$-value-based CP yields smaller predictive
sets but with reduced coverage accuracy. These findings highlight both the
promise and limitations of CP in encrypted data settings and underscore
critical trade-offs between prediction set compactness and reliability. %Our
work sets a foundation for principled uncertainty quantification in secure,
privacy-aware learning systems.

</details>


### [353] [Networked Information Aggregation via Machine Learning](https://arxiv.org/abs/2507.09683)
*Michael Kearns,Aaron Roth,Emily Ryu*

Main category: cs.LG

TL;DR: 研究了在DAG中嵌入学习代理的分布式学习问题，探讨了信息聚合的条件，并给出了理论和实验结果。


<details>
  <summary>Details</summary>
Motivation: 研究在DAG结构中，代理如何通过观察部分特征和父节点的预测来学习，以实现信息聚合。

Method: 代理按DAG拓扑顺序学习，利用自身观察的特征和父节点的预测作为额外特征训练模型。

Result: DAG的深度是关键参数，信息聚合在足够长的路径上可能实现，但某些分布下无法实现。

Conclusion: 信息聚合依赖于DAG的深度和特征分布，理论和实验验证了这一点。

Abstract: We study a distributed learning problem in which learning agents are embedded
in a directed acyclic graph (DAG). There is a fixed and arbitrary distribution
over feature/label pairs, and each agent or vertex in the graph is able to
directly observe only a subset of the features -- potentially a different
subset for every agent. The agents learn sequentially in some order consistent
with a topological sort of the DAG, committing to a model mapping observations
to predictions of the real-valued label. Each agent observes the predictions of
their parents in the DAG, and trains their model using both the features of the
instance that they directly observe, and the predictions of their parents as
additional features. We ask when this process is sufficient to achieve
\emph{information aggregation}, in the sense that some agent in the DAG is able
to learn a model whose error is competitive with the best model that could have
been learned (in some hypothesis class) with direct access to \emph{all}
features, despite the fact that no single agent in the network has such access.
We give upper and lower bounds for this problem for both linear and general
hypothesis classes. Our results identify the \emph{depth} of the DAG as the key
parameter: information aggregation can occur over sufficiently long paths in
the DAG, assuming that all of the relevant features are well represented along
the path, and there are distributions over which information aggregation cannot
occur even in the linear case, and even in arbitrarily large DAGs that do not
have sufficient depth (such as a hub-and-spokes topology in which the spoke
vertices collectively see all the features). We complement our theoretical
results with a comprehensive set of experiments.

</details>


### [354] [Post-Training Quantization of Generative and Discriminative LSTM Text Classifiers: A Study of Calibration, Class Balance, and Robustness](https://arxiv.org/abs/2507.09687)
*Md Mushfiqur Rahaman,Elliot Chang,Tasmiah Haque,Srinjoy Das*

Main category: cs.LG

TL;DR: 本文比较了生成式和判别式LSTM文本分类模型在边缘计算中的表现，重点研究了后训练量化（PTQ）对模型性能的影响，发现生成式模型对量化位宽和校准数据更敏感。


<details>
  <summary>Details</summary>
Motivation: 边缘计算中需要低延迟和高精度的文本分类模型，生成式分类器对噪声和分布外数据具有鲁棒性，但部署时面临计算和内存限制。PTQ可减少模型大小和计算成本，但需研究其对不同类型分类器的影响。

Method: 使用Brevitas量化库对生成式和判别式LSTM模型进行PTQ，评估不同位宽和噪声输入条件下的性能，并研究校准数据类别不平衡的影响。

Result: 判别式分类器在量化后仍保持鲁棒性，生成式分类器对位宽、校准数据和输入噪声更敏感，类别不平衡的校准数据会导致性能下降。

Conclusion: 校准数据在PTQ中起关键作用，生成式分类器在噪声环境下表现不稳定，需谨慎选择校准数据以优化边缘部署。

Abstract: Text classification plays a pivotal role in edge computing applications like
industrial monitoring, health diagnostics, and smart assistants, where low
latency and high accuracy are both key requirements. Generative classifiers, in
particular, have been shown to exhibit robustness to out-of-distribution and
noisy data, which is an extremely critical consideration for deployment in such
real-time edge environments. However, deploying such models on edge devices
faces computational and memory constraints. Post Training Quantization (PTQ)
reduces model size and compute costs without retraining, making it ideal for
edge deployment. In this work, we present a comprehensive comparative study of
generative and discriminative Long Short Term Memory (LSTM)-based text
classification models with PTQ using the Brevitas quantization library. We
evaluate both types of classifier models across multiple bitwidths and assess
their robustness under regular and noisy input conditions. We find that while
discriminative classifiers remain robust, generative ones are more sensitive to
bitwidth, calibration data used during PTQ, and input noise during quantized
inference. We study the influence of class imbalance in calibration data for
both types of classifiers, comparing scenarios with evenly and unevenly
distributed class samples including their effect on weight adjustments and
activation profiles during PTQ. Using test statistics derived from
nonparametric hypothesis testing, we identify that using class imbalanced data
during calibration introduces insufficient weight adaptation at lower bitwidths
for generative LSTM classifiers, thereby leading to degraded performance. This
study underscores the role of calibration data in PTQ and when generative
classifiers succeed or fail under noise, aiding deployment in edge
environments.

</details>


### [355] [Frequency-aware Surrogate Modeling With SMT Kernels For Advanced Data Forecasting](https://arxiv.org/abs/2507.09694)
*Nicolas Gonel,Paul Saves,Joseph Morlier*

Main category: cs.LG

TL;DR: 本文介绍了一个开源框架，用于开发相关性核函数，特别关注用户定义和核函数组合，以支持代理建模。


<details>
  <summary>Details</summary>
Motivation: 传统核函数方法（如基于指数的方法）在捕捉复杂机械行为和时频动态方面存在局限性，因此需要扩展核函数类型并整合频率感知元素。

Method: 提出了一种框架，扩展了核函数类型（如指数平方正弦核和有理二次核），并支持核函数的组合。方法在测试案例和实际应用（如CO2浓度预测）中验证。

Result: 框架被集成到开源工具SMT 2.0中，提供了灵活的标准和自定义核配置选项，支持复合模型的构建。

Conclusion: 该框架为工程师和研究人员提供了灵活的工具，适用于复杂、频率敏感的领域，为未来应用奠定了基础。

Abstract: This paper introduces a comprehensive open-source framework for developing
correlation kernels, with a particular focus on user-defined and composition of
kernels for surrogate modeling. By advancing kernel-based modeling techniques,
we incorporate frequency-aware elements that effectively capture complex
mechanical behaviors and timefrequency dynamics intrinsic to aircraft systems.
Traditional kernel functions, often limited to exponential-based methods, are
extended to include a wider range of kernels such as exponential squared sine
and rational quadratic kernels, along with their respective firstand
second-order derivatives. The proposed methodologies are first validated on a
sinus cardinal test case and then applied to forecasting Mauna-Loa Carbon
Dioxide (CO 2 ) concentrations and airline passenger traffic. All these
advancements are integrated into the open-source Surrogate Modeling Toolbox
(SMT 2.0), providing a versatile platform for both standard and customizable
kernel configurations. Furthermore, the framework enables the combination of
various kernels to leverage their unique strengths into composite models
tailored to specific problems. The resulting framework offers a flexible
toolset for engineers and researchers, paving the way for numerous future
applications in metamodeling for complex, frequency-sensitive domains.

</details>


### [356] [EPT-2 Technical Report](https://arxiv.org/abs/2507.09703)
*Roberto Molinaro,Niall Siegenheim,Niels Poulsen,Jordan Dane Daubinet,Henry Martin,Mark Frey,Kevin Thiart,Alexander Jakob Dautel,Andreas Schlueter,Alex Grigoryev,Bogdan Danciu,Nikoo Ekhtiari,Bas Steunebrink,Leonie Wagner,Marvin Vincent Gabler*

Main category: cs.LG

TL;DR: EPT-2是Earth Physics Transformer系列的最新AI模型，用于地球系统预测，性能超越前代和现有领先模型，并引入EPT-2e进行概率预测。


<details>
  <summary>Details</summary>
Motivation: 提升地球系统预测的准确性和效率，超越现有AI和数值天气预报系统。

Method: 基于Transformer架构的EPT-2模型，以及其扰动集成版本EPT-2e。

Result: EPT-2在0-240小时预测范围内表现优异，EPT-2e超越ECMWF ENS均值，计算成本更低。

Conclusion: EPT-2和EPT-2e在地球系统预测中表现卓越，提供高效且准确的解决方案。

Abstract: We present EPT-2, the latest iteration in our Earth Physics Transformer (EPT)
family of foundation AI models for Earth system forecasting. EPT-2 delivers
substantial improvements over its predecessor, EPT-1.5, and sets a new state of
the art in predicting energy-relevant variables-including 10m and 100m wind
speed, 2m temperature, and surface solar radiation-across the full 0-240h
forecast horizon. It consistently outperforms leading AI weather models such as
Microsoft Aurora, as well as the operational numerical forecast system IFS HRES
from the European Centre for Medium-Range Weather Forecasts (ECMWF). In
parallel, we introduce a perturbation-based ensemble model of EPT-2 for
probabilistic forecasting, called EPT-2e. Remarkably, EPT-2e significantly
surpasses the ECMWF ENS mean-long considered the gold standard for medium- to
longrange forecasting-while operating at a fraction of the computational cost.
EPT models, as well as third-party forecasts, are accessible via the app.jua.ai
platform.

</details>


### [357] [Continental scale habitat modelling with artificial intelligence and multimodal earth observation](https://arxiv.org/abs/2507.09732)
*Sara Si-Moussi,Stephan Hennekens,Sander Mucher,Stan Los,Wilfried Thuiller*

Main category: cs.LG

TL;DR: 论文探讨了如何利用高分辨率遥感数据和AI工具改进大范围、高主题分辨率的栖息地分类，解决了现有地图在主题和空间分辨率上的不足。


<details>
  <summary>Details</summary>
Motivation: 由于人类活动对生态系统的压力增加，需要高精度的栖息地地图以支持有效的保护和恢复。现有地图在主题和空间分辨率上存在不足。

Method: 利用欧洲植被档案的植被数据，结合高分辨率遥感数据和AI工具，评估了多种建模策略，包括利用栖息地命名法的层次性、多光谱和合成孔径雷达图像集成，以及集成机器学习。

Result: 利用层次性命名法解决了分类模糊问题，多光谱和合成孔径雷达图像的集成提高了分类性能，集成机器学习进一步提升了准确性。

Conclusion: 该方法框架可推广到其他地区和其他分类系统，未来研究应关注动态栖息地的时间建模、栖息地分割和质量评估，以及利用新一代遥感数据。

Abstract: Habitats integrate the abiotic conditions and biophysical structures that
support biodiversity and sustain nature's contributions to people. As these
ecosystems face mounting pressure from human activities, accurate,
high-resolution habitat maps are essential for effective conservation and
restoration. Yet current maps often fall short in thematic or spatial
resolution because they must (1) model several mutually exclusive habitat types
that co-occur across landscapes and (2) cope with severe class imbalance that
complicate multi-class training. Here, we evaluated how high-resolution remote
sensing (RS) data and Artificial Intelligence (AI) tools can improve habitat
classification over large geographic extents at fine thematic resolution. Using
vegetation plots from the European Vegetation Archive, we modelled Level 3
EUNIS habitats across Europe and assessed multiple modelling strategies against
independent validation datasets. Strategies that exploited the hierarchical
nature of habitat nomenclatures resolved classification ambiguities, especially
in fragmented landscapes. Integrating multi-spectral (MSI) and synthetic
aperture radar (SAR) imagery, particularly through Earth Observation Foundation
models, enhanced within-formation discrimination and overall performance.
Finally, ensemble machine learning that corrects class imbalance boosted
accuracy further. Our methodological framework is transferable beyond Europe
and adaptable to other classification systems. Future research should advance
temporal modelling of dynamic habitats, extend to habitat segmentation and
quality assessment, and exploit next-generation EO data paired with
higher-quality in-situ observations.

</details>


### [358] [Universal Physics Simulation: A Foundational Diffusion Approach](https://arxiv.org/abs/2507.09733)
*Bradley Camburn*

Main category: cs.LG

TL;DR: 提出了一种基于边界条件数据的通用物理模拟AI模型，无需预先编码方程，通过扩散变换器直接生成稳态解。


<details>
  <summary>Details</summary>
Motivation: 传统物理模拟方法（如PINNs和有限差分法）需要显式数学方程，限制了泛化能力和发现潜力。

Method: 采用草图引导的扩散变换器，将模拟视为条件生成问题，利用空间边界条件生成物理准确的稳态解。

Result: 模型实现了边界到稳态的直接映射，SSIM > 0.8，并保持亚像素级边界精度。

Conclusion: 该模型标志着从AI加速物理到AI发现物理的范式转变，建立了首个通用物理模拟框架。

Abstract: We present the first foundational AI model for universal physics simulation
that learns physical laws directly from boundary-condition data without
requiring a priori equation encoding. Traditional physics-informed neural
networks (PINNs) and finite-difference methods necessitate explicit
mathematical formulation of governing equations, fundamentally limiting their
generalizability and discovery potential. Our sketch-guided diffusion
transformer approach reimagines computational physics by treating simulation as
a conditional generation problem, where spatial boundary conditions guide the
synthesis of physically accurate steady-state solutions.
  By leveraging enhanced diffusion transformer architectures with novel spatial
relationship encoding, our model achieves direct boundary-to-equilibrium
mapping and is generalizable to diverse physics domains. Unlike sequential
time-stepping methods that accumulate errors over iterations, our approach
bypasses temporal integration entirely, directly generating steady-state
solutions with SSIM > 0.8 while maintaining sub-pixel boundary accuracy. Our
data-informed approach enables physics discovery through learned
representations analyzable via Layer-wise Relevance Propagation (LRP),
revealing emergent physical relationships without predetermined mathematical
constraints. This work represents a paradigm shift from AI-accelerated physics
to AI-discovered physics, establishing the first truly universal physics
simulation framework.

</details>


### [359] [Do we need equivariant models for molecule generation?](https://arxiv.org/abs/2507.09753)
*Ewa M. Nowara,Joshua Rackers,Patricia Suriana,Pan Kessel,Max Shen,Andrew Martin Watkins,Michael Maser*

Main category: cs.LG

TL;DR: 研究探讨非等变卷积神经网络（CNN）是否可以通过旋转增强学习等变性，以匹配等变图神经网络（GNN）的性能。


<details>
  <summary>Details</summary>
Motivation: 现有等变GNN模型复杂、难以训练且扩展性差，因此探索更简单的非等变CNN是否可行。

Method: 通过旋转增强训练非等变CNN，并分解损失为预测误差和等变误差，评估模型大小、数据集大小和训练时长对性能的影响。

Result: 首次分析了生成任务中学习到的等变性，验证了非等变CNN的潜力。

Conclusion: 非等变CNN通过旋转增强可以学习等变性，为分子生成提供了一种更简单的替代方案。

Abstract: Deep generative models are increasingly used for molecular discovery, with
most recent approaches relying on equivariant graph neural networks (GNNs)
under the assumption that explicit equivariance is essential for generating
high-quality 3D molecules. However, these models are complex, difficult to
train, and scale poorly.
  We investigate whether non-equivariant convolutional neural networks (CNNs)
trained with rotation augmentations can learn equivariance and match the
performance of equivariant models. We derive a loss decomposition that
separates prediction error from equivariance error, and evaluate how model
size, dataset size, and training duration affect performance across denoising,
molecule generation, and property prediction. To our knowledge, this is the
first study to analyze learned equivariance in generative tasks.

</details>


### [360] [Explainable AI in Genomics: Transcription Factor Binding Site Prediction with Mixture of Experts](https://arxiv.org/abs/2507.09754)
*Aakash Tripathi,Ian E. Nielsen,Muhammad Umer,Ravi P. Ramachandran,Ghulam Rasool*

Main category: cs.LG

TL;DR: 提出了一种基于混合专家（MoE）的新方法用于转录因子结合位点（TFBS）预测，结合多个预训练的CNN模型，在分布内外数据上表现优异，并引入ShiftSmooth技术增强模型可解释性。


<details>
  <summary>Details</summary>
Motivation: 转录因子结合位点预测对理解基因调控至关重要，但现有方法在多样性和可解释性上存在不足。

Method: 采用混合专家（MoE）框架，整合多个预训练CNN模型，并使用ShiftSmooth技术进行模型解释。

Result: MoE模型在分布内外数据上表现优异，ANOVA验证了性能差异的显著性，ShiftSmooth在解释性上优于传统方法。

Conclusion: 该研究提供了一种高效、通用且可解释的TFBS预测方案，有望推动基因组生物学和转录调控的研究。

Abstract: Transcription Factor Binding Site (TFBS) prediction is crucial for
understanding gene regulation and various biological processes. This study
introduces a novel Mixture of Experts (MoE) approach for TFBS prediction,
integrating multiple pre-trained Convolutional Neural Network (CNN) models,
each specializing in different TFBS patterns. We evaluate the performance of
our MoE model against individual expert models on both in-distribution and
out-of-distribution (OOD) datasets, using six randomly selected transcription
factors (TFs) for OOD testing. Our results demonstrate that the MoE model
achieves competitive or superior performance across diverse TF binding sites,
particularly excelling in OOD scenarios. The Analysis of Variance (ANOVA)
statistical test confirms the significance of these performance differences.
Additionally, we introduce ShiftSmooth, a novel attribution mapping technique
that provides more robust model interpretability by considering small shifts in
input sequences. Through comprehensive explainability analysis, we show that
ShiftSmooth offers superior attribution for motif discovery and localization
compared to traditional Vanilla Gradient methods. Our work presents an
efficient, generalizable, and interpretable solution for TFBS prediction,
potentially enabling new discoveries in genome biology and advancing our
understanding of transcriptional regulation.

</details>


### [361] [Toward accurate RUL and SOH estimation using reinforced graph-based PINNs enhanced with dynamic weights](https://arxiv.org/abs/2507.09766)
*Mohamadreza Akbari Pour,Ali Ghasemzadeh,MohamadAli Bijarchi,Mohammad Behshad Shafii*

Main category: cs.LG

TL;DR: 提出了一种结合物理监督与时空学习的RGPD框架，用于RUL和SOH估计，性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 准确估计RUL和SOH对PHM至关重要，但现有方法在泛化和适应性上存在不足。

Method: 结合GCRN、GATConv、SAC模块和Q学习，动态调整物理约束权重，优化时空学习。

Result: 在多个工业数据集上，RGPD在RUL和SOH估计任务中表现优于现有方法。

Conclusion: RGPD框架通过动态权重和物理监督，显著提升了预测准确性和鲁棒性。

Abstract: Accurate estimation of Remaining Useful Life (RUL) and State of Health (SOH)
is essential for Prognostics and Health Management (PHM) across a wide range of
industrial applications. We propose a novel framework -- Reinforced Graph-Based
Physics-Informed Neural Networks Enhanced with Dynamic Weights (RGPD) -- that
combines physics-based supervision with advanced spatio-temporal learning.
Graph Convolutional Recurrent Networks (GCRNs) embed graph-convolutional
filters within recurrent units to capture how node representations evolve over
time. Graph Attention Convolution (GATConv) leverages a self-attention
mechanism to compute learnable, edge-wise attention coefficients, dynamically
weighting neighbor contributions for adaptive spatial aggregation. A Soft
Actor-Critic (SAC) module is positioned between the Temporal Attention Unit
(TAU) and GCRN to further improve the spatio-temporal learning. This module
improves attention and prediction accuracy by dynamically scaling hidden
representations to minimize noise and highlight informative features. To
identify the most relevant physical constraints in each area, Q-learning agents
dynamically assign weights to physics-informed loss terms, improving
generalization across real-time industrial systems and reducing the need for
manual tuning. In both RUL and SOH estimation tasks, the proposed method
consistently outperforms state-of-the-art models, demonstrating strong
robustness and predictive accuracy across varied degradation patterns across
three diverse industrial benchmark datasets.

</details>


### [362] [Efficient Molecular Conformer Generation with SO(3)-Averaged Flow Matching and Reflow](https://arxiv.org/abs/2507.09785)
*Zhonglin Cao,Mario Geiger,Allan dos Santos Costa,Danny Reidenbach,Karsten Kreis,Tomas Geffner,Franco Pellegrini,Guoqing Zhou,Emine Kucukbenli*

Main category: cs.LG

TL;DR: 论文提出两种加速3D分子构象生成的训练和推理方法：SO(3)-Averaged Flow训练目标和基于流模型的reflow与distillation方法。


<details>
  <summary>Details</summary>
Motivation: 快速准确的分子构象生成对计算化学和药物发现任务至关重要，但现有扩散或流模型训练和采样需要大量计算资源。

Method: 提出SO(3)-Averaged Flow训练目标以加速训练，并利用reflow和distillation方法实现高质量少步或单步推理。

Result: SO(3)-Averaged Flow训练模型达到最优构象生成质量，reflow和distillation方法实现高效高质量生成。

Conclusion: 该工作为基于流模型的高效分子构象生成提供了可行路径。

Abstract: Fast and accurate generation of molecular conformers is desired for
downstream computational chemistry and drug discovery tasks. Currently,
training and sampling state-of-the-art diffusion or flow-based models for
conformer generation require significant computational resources. In this work,
we build upon flow-matching and propose two mechanisms for accelerating
training and inference of generative models for 3D molecular conformer
generation. For fast training, we introduce the SO(3)-Averaged Flow training
objective, which leads to faster convergence to better generation quality
compared to conditional optimal transport flow or Kabsch-aligned flow. We
demonstrate that models trained using SO(3)-Averaged Flow can reach
state-of-the-art conformer generation quality. For fast inference, we show that
the reflow and distillation methods of flow-based models enable few-steps or
even one-step molecular conformer generation with high quality. The training
techniques proposed in this work show a path towards highly efficient molecular
conformer generation with flow-based models.

</details>


### [363] [Leveraging Distribution Matching to Make Approximate Machine Unlearning Faster](https://arxiv.org/abs/2507.09786)
*Junaid Iqbal Khan*

Main category: cs.LG

TL;DR: 论文提出两种方法加速分类导向的近似机器遗忘（AMU）：Blend（数据集压缩）和A-AMU（损失函数优化），显著减少计算时间并保持模型性能。


<details>
  <summary>Details</summary>
Motivation: 现有AMU方法在处理保留数据集时计算开销大，且减少训练轮次仍具挑战性。

Method: 1. Blend：通过视觉相似图像的合并压缩数据集；2. A-AMU：通过损失函数优化加速收敛。

Result: 实验表明，该方法显著降低遗忘延迟，同时保持模型效用和隐私。

Conclusion: 首次通过数据集压缩和损失函数联合设计系统提升遗忘效率。

Abstract: Approximate machine unlearning (AMU) enables models to `forget' specific
training data through specialized fine-tuning on a retained dataset subset.
However, processing this retained subset still dominates computational runtime,
while reductions of epochs also remain a challenge. We propose two
complementary methods to accelerate classification-oriented AMU. First,
\textbf{Blend}, a novel distribution-matching dataset condensation (DC), merges
visually similar images with shared blend-weights to significantly reduce the
retained set size. It operates with minimal pre-processing overhead and is
orders of magnitude faster than state-of-the-art DC methods. Second, our
loss-centric method, \textbf{Accelerated-AMU (A-AMU)}, augments the unlearning
objective to quicken convergence. A-AMU achieves this by combining a steepened
primary loss to expedite forgetting with a novel, differentiable regularizer
that matches the loss distributions of forgotten and in-distribution unseen
data. Our extensive experiments demonstrate that this dual approach of data and
loss-centric optimization dramatically reduces end-to-end unlearning latency
across both single and multi-round scenarios, all while preserving model
utility and privacy. To our knowledge, this is the first work to systematically
tackle unlearning efficiency by jointly designing a specialized dataset
condensation technique with a dedicated accelerated loss function. Code is
available at https://github.com/algebraicdianuj/DC_Unlearning.

</details>


### [364] [Multiple Choice Learning of Low Rank Adapters for Language Modeling](https://arxiv.org/abs/2507.10419)
*Victor Letzelter,Hugo Malard,Mathieu Fontaine,Gaël Richard,Slim Essid,Andrei Bursuc,Patrick Pérez*

Main category: cs.LG

TL;DR: LoRA-MCL是一种结合多选学习和低秩适应的训练方案，用于生成多样且合理的句子续写。


<details>
  <summary>Details</summary>
Motivation: 传统语言建模存在多解问题，即同一上下文可能有多个合理的未来句子。

Method: 结合多选学习（MCL）和胜者全得（WTA）损失，通过低秩适应（LoRA）处理模糊性。

Result: 在视觉和音频字幕任务中，生成的结果具有高多样性和相关性。

Conclusion: LoRA-MCL有效解决了语言建模中的多解问题，生成结果多样且合理。

Abstract: We propose LoRA-MCL, a training scheme that extends next-token prediction in
language models with a method designed to decode diverse, plausible sentence
continuations at inference time. Traditional language modeling is an
intrinsically ill-posed problem: given a context, multiple futures may be
equally plausible. Our approach leverages Multiple Choice Learning (MCL) and
the Winner-Takes-All (WTA) loss to efficiently handle ambiguity through
Low-Rank Adaptation (LoRA). We provide a theoretical interpretation of applying
Multiple Choice Learning to Language Modeling, assuming the data is generated
from a mixture of distributions. To illustrate the proposed approach, we use
data sampled from mixtures of Markov chains. We then demonstrate with extensive
experiments on real-world visual and audio captioning tasks that our method
achieves high diversity and relevance in generated outputs.

</details>


### [365] [A Scalable and Efficient Signal Integration System for Job Matching](https://arxiv.org/abs/2507.09797)
*Ping Liu,Rajat Arora,Xiao Shi,Benjamin Le,Qianqi Shen,Jianqiang Shen,Chengming Jiang,Nikita Zhiltsov,Priya Bannur,Yidan Zhu,Liming Dong,Haichao Wei,Qi Guo,Luke Simon,Liangjie Hong,Wenjing Zhang*

Main category: cs.LG

TL;DR: LinkedIn开发了STAR系统，结合LLM和GNN解决推荐系统中的冷启动、过滤气泡和偏见问题。


<details>
  <summary>Details</summary>
Motivation: 解决LinkedIn在职业匹配推荐系统中面临的冷启动、过滤气泡和偏见等挑战。

Method: 结合大型语言模型（LLM）和图神经网络（GNN），利用自适应采样和版本管理等工业级范式。

Result: STAR系统提供了端到端的嵌入开发和部署解决方案，实现了高性能推荐。

Conclusion: STAR系统为工业应用中的嵌入构建提供了稳健方法，并提供了实际部署的实用见解。

Abstract: LinkedIn, one of the world's largest platforms for professional networking
and job seeking, encounters various modeling challenges in building
recommendation systems for its job matching product, including cold-start,
filter bubbles, and biases affecting candidate-job matching. To address these,
we developed the STAR (Signal Integration for Talent And Recruiters) system,
leveraging the combined strengths of Large Language Models (LLMs) and Graph
Neural Networks (GNNs). LLMs excel at understanding textual data, such as
member profiles and job postings, while GNNs capture intricate relationships
and mitigate cold-start issues through network effects. STAR integrates diverse
signals by uniting LLM and GNN capabilities with industrial-scale paradigms
including adaptive sampling and version management. It provides an end-to-end
solution for developing and deploying embeddings in large-scale recommender
systems. Our key contributions include a robust methodology for building
embeddings in industrial applications, a scalable GNN-LLM integration for
high-performing recommendations, and practical insights for real-world model
deployment.

</details>


### [366] [Federated Learning with Graph-Based Aggregation for Traffic Forecasting](https://arxiv.org/abs/2507.09805)
*Audri Banik,Glaucio Haroldo Silva de Carvalho,Renata Dividino*

Main category: cs.LG

TL;DR: 提出了一种轻量级的图感知联邦学习方法，结合了FedAvg的简单性和图学习的关键思想，有效捕捉空间关系并保持计算效率。


<details>
  <summary>Details</summary>
Motivation: 在交通预测中，空间关系对性能至关重要，但标准联邦学习方法（如FedAvg）假设客户端独立，而现有的图学习方法计算开销大。

Method: 采用轻量级的图感知联邦学习方法，通过基本的邻域聚合原则指导参数更新，基于图连接性对客户端模型进行加权。

Result: 在METR-LA和PEMS-BAY数据集上验证，性能优于标准基准和最新的图联邦学习方法。

Conclusion: 该方法在保持计算效率的同时，有效捕捉了空间关系，为交通预测提供了一种实用的联邦学习解决方案。

Abstract: In traffic prediction, the goal is to estimate traffic speed or flow in
specific regions or road segments using historical data collected by devices
deployed in each area. Each region or road segment can be viewed as an
individual client that measures local traffic flow, making Federated Learning
(FL) a suitable approach for collaboratively training models without sharing
raw data. In centralized FL, a central server collects and aggregates model
updates from multiple clients to build a shared model while preserving each
client's data privacy. Standard FL methods, such as Federated Averaging
(FedAvg), assume that clients are independent, which can limit performance in
traffic prediction tasks where spatial relationships between clients are
important. Federated Graph Learning methods can capture these dependencies
during server-side aggregation, but they often introduce significant
computational overhead. In this paper, we propose a lightweight graph-aware FL
approach that blends the simplicity of FedAvg with key ideas from graph
learning. Rather than training full models, our method applies basic
neighbourhood aggregation principles to guide parameter updates, weighting
client models based on graph connectivity. This approach captures spatial
relationships effectively while remaining computationally efficient. We
evaluate our method on two benchmark traffic datasets, METR-LA and PEMS-BAY,
and show that it achieves competitive performance compared to standard
baselines and recent graph-based federated learning techniques.

</details>


### [367] [Reasoning or Memorization? Unreliable Results of Reinforcement Learning Due to Data Contamination](https://arxiv.org/abs/2507.10532)
*Mingqi Wu,Zhihao Zhang,Qiaole Dong,Zhiheng Xi,Jun Zhao,Senjie Jin,Xiaoran Fan,Yuhao Zhou,Yanwei Fu,Qin Liu,Songyang Zhang,Qi Zhang*

Main category: cs.LG

TL;DR: 研究发现，尽管强化学习（RL）声称能提升大语言模型（LLM）的推理能力，但Qwen2.5模型的优异表现可能源于数据污染。作者提出使用合成数据集RandomCalculation验证RL方法，发现仅准确奖励信号有效。


<details>
  <summary>Details</summary>
Motivation: 探讨RL方法对LLM推理能力的真实影响，揭示现有基准测试中数据污染的问题。

Method: 引入合成数据集RandomCalculation，避免数据污染，验证RL方法的效果。

Result: 在无污染数据上，仅准确奖励信号能提升性能，而噪声或错误信号无效。

Conclusion: 建议在无污染基准和多样化模型上评估RL方法，以确保结论的可信度。

Abstract: The reasoning capabilities of large language models (LLMs) have been a
longstanding focus of research. Recent works have further enhanced these
capabilities using reinforcement learning (RL), with many new methods claiming
significant improvements with minimal or no external supervision. Surprisingly,
some studies even suggest that random or incorrect reward signals can enhance
reasoning performance. However, these breakthroughs are mostly reported on the
Qwen2.5 model family and evaluated on well-known benchmarks such as MATH-500,
AMC, and AIME, while failing to achieve similar gains on other models like
Llama, which warrants further investigation. Our analysis shows that although
Qwen2.5 achieves strong mathematical reasoning performance, its pretraining on
large-scale web corpora makes it vulnerable to data contamination in popular
benchmarks. As a result, results derived from these benchmarks may be
unreliable. To address this, we introduce a generator that produces fully
synthetic arithmetic problems of arbitrary length and difficulty, yielding a
clean dataset we call RandomCalculation. Using these leakage-free datasets, we
show that only accurate reward signals consistently improve performance, while
noisy or incorrect signals do not. We advocate for evaluating RL methods on
uncontaminated benchmarks and across diverse model families to ensure
trustworthy conclusions.

</details>


### [368] [Compressed Computation: Dense Circuits in a Toy Model of the Universal-AND Problem](https://arxiv.org/abs/2507.09816)
*Adam Newgas*

Main category: cs.LG

TL;DR: 研究了神经网络在计算中的叠加现象，发现实际训练中模型倾向于找到一种简单且高效的解决方案，而非理论构造。


<details>
  <summary>Details</summary>
Motivation: 探讨神经网络是否能在实践中学习到计算叠加的电路，尤其是针对Universal-AND问题的简化模型。

Method: 使用一个受限隐藏维度的玩具模型，迫使模型寻找计算高效的电路（压缩计算）。

Result: 训练过程发现了一种完全密集的解决方案，其效率高于理论构造，且能自然扩展到其他布尔运算。

Conclusion: 研究揭示了神经网络倾向于形成的电路类型，为网络结构和可解释性提供了新的理解。

Abstract: Neural networks are capable of superposition -- representing more features
than there are dimensions. Recent work considers the analogous concept for
computation instead of storage, proposing theoretical constructions. But there
has been little investigation into whether these circuits can be learned in
practice. In this work, we investigate a toy model for the Universal-AND
problem which computes the AND of all $m\choose 2$ pairs of $m$ sparse inputs.
The hidden dimension that determines the number of non-linear activations is
restricted to pressure the model to find a compute-efficient circuit, called
compressed computation. We find that the training process finds a simple
solution that does not correspond to theoretical constructions. It is fully
dense -- every neuron contributes to every output. The solution circuit
naturally scales with dimension, trading off error rates for neuron efficiency.
It is similarly robust to changes in sparsity and other key parameters, and
extends naturally to other boolean operations and boolean circuits. We explain
the found solution in detail and compute why it is more efficient than the
theoretical constructions at low sparsity. Our findings shed light on the types
of circuits that models like to form and the flexibility of the superposition
representation. This contributes to a broader understanding of network
circuitry and interpretability.

</details>


### [369] [Bridging Neural Networks and Dynamic Time Warping for Adaptive Time Series Classification](https://arxiv.org/abs/2507.09826)
*Jintao Qu,Zichong Wang,Chenhao Wu,Wenbin Zhang*

Main category: cs.LG

TL;DR: 提出了一种结合动态时间规整（DTW）和神经网络的模型，适用于冷启动和丰富数据场景，兼具可训练性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 解决神经网络在冷启动场景下依赖大量标注数据和缺乏可解释性的问题，同时克服DTW无法利用丰富数据的局限性。

Method: 提出动态长度缩短算法，将时间序列转换为原型，并将DTW的递推关系转化为等效的循环神经网络。

Result: 在多个时间序列分类任务中，模型在低资源场景下显著优于先前方法，在丰富资源场景下仍具竞争力。

Conclusion: 该模型成功结合了DTW的可解释性和神经网络的可训练性，适用于不同数据资源场景。

Abstract: Neural networks have achieved remarkable success in time series
classification, but their reliance on large amounts of labeled data for
training limits their applicability in cold-start scenarios. Moreover, they
lack interpretability, reducing transparency in decision-making. In contrast,
dynamic time warping (DTW) combined with a nearest neighbor classifier is
widely used for its effectiveness in limited-data settings and its inherent
interpretability. However, as a non-parametric method, it is not trainable and
cannot leverage large amounts of labeled data, making it less effective than
neural networks in rich-resource scenarios. In this work, we aim to develop a
versatile model that adapts to cold-start conditions and becomes trainable with
labeled data, while maintaining interpretability. We propose a dynamic
length-shortening algorithm that transforms time series into prototypes while
preserving key structural patterns, thereby enabling the reformulation of the
DTW recurrence relation into an equivalent recurrent neural network. Based on
this, we construct a trainable model that mimics DTW's alignment behavior. As a
neural network, it becomes trainable when sufficient labeled data is available,
while still retaining DTW's inherent interpretability. We apply the model to
several benchmark time series classification tasks and observe that it
significantly outperforms previous approaches in low-resource settings and
remains competitive in rich-resource settings.

</details>


### [370] [Generative Cognitive Diagnosis](https://arxiv.org/abs/2507.09831)
*Jiatong Li,Qi Liu,Mengxiao Zhu*

Main category: cs.LG

TL;DR: 论文提出了一种新的生成式认知诊断范式，解决了传统模型需重新训练和可靠性低的问题，并提出了G-IRT和G-NCDM两种方法，显著提升了性能和速度。


<details>
  <summary>Details</summary>
Motivation: 传统认知诊断模型无法对新学习者进行即时诊断且可靠性有限，需要一种无需重新训练的高效方法。

Method: 提出了生成式诊断范式，通过G-IRT和G-NCDM两种方法实现，分离认知状态推断和响应预测。

Result: 实验显示新方法在性能和速度上显著优于传统方法，诊断新学习者的速度提高了100倍。

Conclusion: 生成式范式为认知诊断在人工智能中的应用开辟了新途径，尤其在智能评估和教育系统中。

Abstract: Cognitive diagnosis (CD) models latent cognitive states of human learners by
analyzing their response patterns on diagnostic tests, serving as a crucial
machine learning technique for educational assessment and evaluation.
Traditional cognitive diagnosis models typically follow a transductive
prediction paradigm that optimizes parameters to fit response scores and
extract learner abilities. These approaches face significant limitations as
they cannot perform instant diagnosis for new learners without computationally
expensive retraining and produce diagnostic outputs with limited reliability.
In this study, we introduces a novel generative diagnosis paradigm that
fundamentally shifts CD from predictive to generative modeling, enabling
inductive inference of cognitive states without parameter re-optimization. We
propose two simple yet effective instantiations of this paradigm: Generative
Item Response Theory (G-IRT) and Generative Neural Cognitive Diagnosis Model
(G-NCDM), which achieve excellent performance improvements over traditional
methods. The generative approach disentangles cognitive state inference from
response prediction through a well-designed generation process that
incorporates identifiability and monotonicity conditions. Extensive experiments
on real-world datasets demonstrate the effectiveness of our methodology in
addressing scalability and reliability challenges, especially $\times 100$
speedup for the diagnosis of new learners. Our framework opens new avenues for
cognitive diagnosis applications in artificial intelligence, particularly for
intelligent model evaluation and intelligent education systems. The code is
available at https://github.com/CSLiJT/Generative-CD.git.

</details>


### [371] [A Pre-training Framework for Relational Data with Information-theoretic Principles](https://arxiv.org/abs/2507.09837)
*Quang Truong,Zhikai Chen,Mingxuan Ju,Tong Zhao,Neil Shah,Jiliang Tang*

Main category: cs.LG

TL;DR: 论文提出了一种名为TVE的预训练框架，通过基于模式遍历图的集合聚合构建预测监督信号，以解决关系型数据库预训练中的任务异质性问题。


<details>
  <summary>Details</summary>
Motivation: 关系型数据库在多个领域支撑关键基础设施，但由于任务异质性，通用的预训练策略设计仍具挑战性。

Method: TVE框架通过模式遍历图的集合聚合建模关系动态，结合任务先验信息，生成任务感知表示。

Result: 在RelBench基准测试中，TVE显著优于传统预训练基线。

Conclusion: 研究表明，预训练目标应编码任务异质性和时间结构，以优化关系型数据库的预测建模。

Abstract: Relational databases underpin critical infrastructure across a wide range of
domains, yet the design of generalizable pre-training strategies for learning
from relational databases remains an open challenge due to task heterogeneity.
Specifically, there exist infinitely many possible downstream tasks, as tasks
are defined based on relational schema graphs, temporal dependencies, and
SQL-defined label logics. An effective pre-training framework is desired to
take these factors into account in order to obtain task-aware representations.
By incorporating knowledge of the underlying distribution that drives label
generation, downstream tasks can benefit from relevant side-channel
information. To bridge this gap, we introduce Task Vector Estimation (TVE), a
novel pre-training framework that constructs predictive supervisory signals via
set-based aggregation over schema traversal graphs, explicitly modeling
next-window relational dynamics. We formalize our approach through an
information-theoretic lens, demonstrating that task-informed representations
retain more relevant signals than those obtained without task priors. Extensive
experiments on the RelBench benchmark show that TVE consistently outperforms
traditional pre-training baselines. Our findings advocate for pre-training
objectives that encode task heterogeneity and temporal structure as design
principles for predictive modeling on relational databases.

</details>


### [372] [Rethinking Prompt Optimization: Reinforcement, Diversification, and Migration in Blackbox LLMs](https://arxiv.org/abs/2507.09839)
*MohammadReza Davari,Utkarsh Garg,Weixin Cai,Eugene Belilovsky*

Main category: cs.LG

TL;DR: 提出了一种新的自动提示优化（APO）框架，通过增强反馈机制，结合正负强化和反馈多样化技术，显著提升了提示优化的效果和效率。


<details>
  <summary>Details</summary>
Motivation: 现有APO方法主要关注错误修正，忽视了正确预测中的有价值信息，限制了优化效果和效率。

Method: 重新定义文本梯度为负强化，引入正强化以保留成功预测中的有益提示组件，并通过反馈多样化技术减少噪声。

Result: 实验表明，该方法在准确率、收敛速度和计算成本上均优于基线，尤其在提示迁移场景中表现突出。

Conclusion: 提出的框架显著提升了提示优化的效果和效率，适用于不同模型版本或API提供商的迁移场景。

Abstract: An increasing number of NLP applications interact with large language models
(LLMs) through black-box APIs, making prompt engineering critical for
controlling model outputs. While recent Automatic Prompt Optimization (APO)
methods iteratively refine prompts using model-generated feedback, textual
gradients, they primarily focus on error correction and neglect valuable
insights from correct predictions. This limits both their effectiveness and
efficiency. In this paper, we propose a novel APO framework centered on
enhancing the feedback mechanism. We reinterpret the textual gradient as a form
of negative reinforcement and introduce the complementary positive
reinforcement to explicitly preserve beneficial prompt components identified
through successful predictions. To mitigate the noise inherent in LLM-generated
feedback, we introduce a technique called feedback diversification, which
aggregates multiple feedback signals, emphasizing consistent, actionable advice
while filtering out outliers. Motivated by the rapid evolution and diversity of
available LLMs, we also formalize Continual Prompt Optimization (CPO),
addressing the practical challenge of efficiently migrating optimized prompts
between different model versions or API providers. Our experiments reveal that
naive prompt migration often degrades performance due to loss of critical
instructions. In contrast, our approach consistently outperforms strong
baselines, achieving significant accuracy improvements, faster convergence, and
lower computational costs in both standard and migration scenarios.

</details>


### [373] [Through the River: Understanding the Benefit of Schedule-Free Methods for Language Model Training](https://arxiv.org/abs/2507.09846)
*Minhak Song,Beomhan Baek,Kwangjun Ahn,Chulhee Yun*

Main category: cs.LG

TL;DR: 论文提出了一种改进的Schedule-Free（SF）方法，用于大规模语言模型训练，无需显式衰减阶段或额外内存开销，并通过理论和实证分析验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 随着模型和数据规模的快速扩大，传统的预训练策略（如固定计算预算的余弦学习率调度）已无法满足需求，需要更灵活且可扩展的替代方案。

Method: 论文重新审视了Schedule-Free（SF）方法，并提出了改进版本SF-AdamW，通过理论和实证分析揭示其隐式权重平均特性。

Result: SF-AdamW在损失景观中高效导航，无需衰减阶段或额外内存开销，且改进版本在大批量训练中表现更优。

Conclusion: SF方法是一种实用、可扩展且理论支持的语言模型训练方法，改进版本进一步提升了其鲁棒性和性能。

Abstract: As both model and dataset sizes continue to scale rapidly, conventional
pretraining strategies with fixed compute budgets-such as cosine learning rate
schedules-are increasingly inadequate for large-scale training. Recent
alternatives, including warmup-stable-decay (WSD) schedules and weight
averaging, offer greater flexibility. However, WSD relies on explicit decay
phases to track progress, while weight averaging addresses this limitation at
the cost of additional memory. In search of a more principled and scalable
alternative, we revisit the Schedule-Free (SF) method [Defazio et al., 2024],
which has shown strong empirical performance across diverse settings. We show
that SF-AdamW effectively navigates the "river" structure of the loss landscape
without decay phases or auxiliary averaging, making it particularly suitable
for continuously scaling training workloads. To understand this behavior, we
conduct a theoretical and empirical analysis of SF dynamics, revealing that it
implicitly performs weight averaging without memory overhead. Guided by this
analysis, we propose a refined variant of SF that improves robustness to
momentum and performs better under large batch sizes, addressing key
limitations of the original method. Together, these results establish SF as a
practical, scalable, and theoretically grounded approach for language model
training.

</details>


### [374] [Task Priors: Enhancing Model Evaluation by Considering the Entire Space of Downstream Tasks](https://arxiv.org/abs/2507.09871)
*Niket Patel,Randall Balestriero*

Main category: cs.LG

TL;DR: 论文提出了一种基于任务先验的概率空间框架，用于评估模型在所有可能下游任务上的性能，解决了当前固定评估方法的局限性。


<details>
  <summary>Details</summary>
Motivation: 当前AI研究中的评估方法依赖于固定的下游基准测试，限制了研究的进展。论文旨在通过定义任务先验和任务分布，提供一个更灵活的评估框架。

Method: 提出了一种基于任务先验的概率空间框架，通过定义任务分布和任务先验，评估模型在所有可能下游任务上的平均性能和方差。

Result: 该框架首次能够回答模型在所有可能任务上的平均性能和方差问题，为SSL研究提供了新的评估标准。

Conclusion: 任务先验框架不仅解决了当前评估方法的瓶颈，还将加速SSL研究的进展。

Abstract: The grand goal of AI research, and particularly Self Supervised Learning
(SSL), is to produce systems that can successfully solve any possible task. In
contrast, current evaluation methods available to AI researchers typically rely
on a fixed collection of hand-picked downstream benchmarks. Hence, a large
amount of effort is put into designing and searching for large collection of
evaluation tasks that can serve as a proxy of our grand goal. We argue that
such a rigid evaluation protocol creates a silent bottleneck in AI research. To
remedy that, we define a probabilistic space of downstream tasks obtained by
adopting a distribution of tasks and by defining Task Priors. Under this view,
one can evaluate a model's performance over the set of all possible downstream
tasks. Our framework is the first to provide answers to key questions such as
(i) what is the average performance of my model over all possible downstream
tasks weighted by the probability to encounter each task? or (ii) what is the
variance of my model's performance across all downstream tasks under the
defined Task Priors? Beyond establishing a new standard for evaluation, we
believe that Task Priors will accelerate the pace of research in SSL - where
downstream task evaluation is the sole qualitative signal that researchers have
access to.

</details>


### [375] [AdaBrain-Bench: Benchmarking Brain Foundation Models for Brain-Computer Interface Applications](https://arxiv.org/abs/2507.09882)
*Jiamin Wu,Zichen Ren,Junyu Wang,Pengyu Zhu,Yonghao Song,Mianxin Liu,Qihao Zheng,Lei Bai,Wanli Ouyang,Chunfeng Song*

Main category: cs.LG

TL;DR: AdaBrain-Bench是一个标准化的大规模基准测试，用于评估非侵入式脑机接口（BCI）中的基础模型，涵盖7个关键应用领域，并提供多维评估指标和工具。


<details>
  <summary>Details</summary>
Motivation: 当前非侵入式BCI领域缺乏全面、实用且可扩展的基准测试，阻碍了公共基础模型的广泛应用。

Method: 提出AdaBrain-Bench，一个包含多样化BCI解码数据集的基准测试，集成任务适应流程和多维评估指标。

Result: 通过AdaBrain-Bench评估了多个公开的基础模型，并提供了在不同场景下选择模型的实践建议。

Conclusion: AdaBrain-Bench为促进稳健和通用的神经解码解决方案提供了一个持续演进的平台。

Abstract: Non-invasive Brain-Computer Interfaces (BCI) offer a safe and accessible
means of connecting the human brain to external devices, with broad
applications in home and clinical settings to enhance human capabilities.
However, the high noise level and limited task-specific data in non-invasive
signals constrain decoding capabilities. Recently, the adoption of
self-supervised pre-training is transforming the landscape of non-invasive BCI
research, enabling the development of brain foundation models to capture
generic neural representations from large-scale unlabeled
electroencephalography (EEG) signals with substantial noises. However, despite
these advances, the field currently lacks comprehensive, practical and
extensible benchmarks to assess the utility of the public foundation models
across diverse BCI tasks, hindering their widespread adoption. To address this
challenge, we present AdaBrain-Bench, a large-scale standardized benchmark to
systematically evaluate brain foundation models in widespread non-invasive BCI
tasks. AdaBrain-Bench encompasses a diverse collection of representative BCI
decoding datasets spanning 7 key applications. It introduces a streamlined task
adaptation pipeline integrated with multi-dimensional evaluation metrics and a
set of adaptation tools. The benchmark delivers an inclusive framework for
assessing generalizability of brain foundation models across key transfer
settings, including cross-subject, multi-subject, and few-shot scenarios. We
leverage AdaBrain-Bench to evaluate a suite of publicly available brain
foundation models and offer insights into practices for selecting appropriate
models in various scenarios. We make our benchmark pipeline available to enable
reproducible research and external use, offering a continuously evolving
platform to foster progress toward robust and generalized neural decoding
solutions.

</details>


### [376] [TolerantECG: A Foundation Model for Imperfect Electrocardiogram](https://arxiv.org/abs/2507.09887)
*Huynh Nguyen Dang,Thang Pham,Ngan Le,Van Nguyen*

Main category: cs.LG

TL;DR: TolerantECG是一个针对ECG信号的鲁棒性基础模型，能够处理噪声和标准12导联ECG的任意子集，通过对比学习和自监督学习框架训练，表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: ECG在心脏疾病诊断中至关重要，但噪声或导联缺失可能导致诊断错误或不确定性，因此需要一种鲁棒性强的模型来解决这些问题。

Method: 结合对比学习和自监督学习框架，训练模型学习ECG信号表示及其文本报告描述，同时处理噪声或导联缺失的信号。

Result: 在PTB-XL数据集和MIT-BIH心律失常数据库中，TolerantECG表现优异，通常排名第一或第二。

Conclusion: TolerantECG是一种有效的解决方案，能够提升ECG信号分析的鲁棒性和准确性。

Abstract: The electrocardiogram (ECG) is an essential and effective tool for diagnosing
heart diseases. However, its effectiveness can be compromised by noise or
unavailability of one or more leads of the standard 12-lead recordings,
resulting in diagnostic errors or uncertainty. To address these challenges, we
propose TolerantECG, a foundation model for ECG signals that is robust to noise
and capable of functioning with arbitrary subsets of the standard 12-lead ECG.
TolerantECG training combines contrastive and self-supervised learning
frameworks to jointly learn ECG signal representations alongside their
corresponding knowledge-retrieval-based text report descriptions and corrupted
or lead-missing signals. Comprehensive benchmarking results demonstrate that
TolerantECG consistently ranks as the best or second-best performer across
various ECG signal conditions and class levels in the PTB-XL dataset, and
achieves the highest performance on the MIT-BIH Arrhythmia Database.

</details>


### [377] [NeuTSFlow: Modeling Continuous Functions Behind Time Series Forecasting](https://arxiv.org/abs/2507.09888)
*Huibo Xu,Likang Wu,Xianquan Wang,Haoning Dang,Chun-Wun Cheng,Angelica I Aviles-Rivero,Qi Liu*

Main category: cs.LG

TL;DR: 论文提出NeuTSFlow框架，通过神经操作符学习连续函数族间的转换路径，解决时间序列预测中离散观测与连续过程不匹配的问题。


<details>
  <summary>Details</summary>
Motivation: 传统方法将时间序列视为离散序列，忽略了其作为连续过程噪声样本的本质，导致预测不准确。

Method: 提出NeuTSFlow框架，利用神经操作符在无限维函数空间中参数化流的速度场，直接建模函数级特征。

Result: 实验表明NeuTSFlow在多种预测任务中具有更高的准确性和鲁棒性。

Conclusion: NeuTSFlow验证了从函数族视角解决时间序列预测问题的有效性。

Abstract: Time series forecasting is a fundamental task with broad applications, yet
conventional methods often treat data as discrete sequences, overlooking their
origin as noisy samples of continuous processes. Crucially, discrete noisy
observations cannot uniquely determine a continuous function; instead, they
correspond to a family of plausible functions. Mathematically, time series can
be viewed as noisy observations of a continuous function family governed by a
shared probability measure. Thus, the forecasting task can be framed as
learning the transition from the historical function family to the future
function family. This reframing introduces two key challenges: (1) How can we
leverage discrete historical and future observations to learn the relationships
between their underlying continuous functions? (2) How can we model the
transition path in function space from the historical function family to the
future function family? To address these challenges, we propose NeuTSFlow, a
novel framework that leverages Neural Operators to facilitate flow matching for
learning path of measure between historical and future function families. By
parameterizing the velocity field of the flow in infinite-dimensional function
spaces, NeuTSFlow moves beyond traditional methods that focus on dependencies
at discrete points, directly modeling function-level features instead.
Experiments on diverse forecasting tasks demonstrate NeuTSFlow's superior
accuracy and robustness, validating the effectiveness of the function-family
perspective.

</details>


### [378] [Soft Graph Clustering for single-cell RNA Sequencing Data](https://arxiv.org/abs/2507.09890)
*Ping Xu,Pengfei Wang,Zhiyuan Ning,Meng Xiao,Min Wu,Yuanchun Zhou*

Main category: cs.LG

TL;DR: 论文提出了一种名为scSGC的软图聚类方法，用于解决单细胞RNA测序数据中硬图构建的局限性，通过非二元边权重更准确地表征细胞间连续相似性。


<details>
  <summary>Details</summary>
Motivation: 现有基于图的单细胞RNA测序聚类方法（如图神经网络）依赖硬图构建，导致信息丢失和聚类偏差。

Method: scSGC框架包括ZINB特征自编码器、双通道切割感知软图嵌入模块和基于最优传输的聚类优化模块。

Result: 在十个数据集上的实验表明，scSGC在聚类准确性、细胞类型注释和计算效率上优于13种先进模型。

Conclusion: scSGC具有显著潜力，可推动单细胞RNA测序数据分析并深化对细胞异质性的理解。

Abstract: Clustering analysis is fundamental in single-cell RNA sequencing (scRNA-seq)
data analysis for elucidating cellular heterogeneity and diversity. Recent
graph-based scRNA-seq clustering methods, particularly graph neural networks
(GNNs), have significantly improved in tackling the challenges of
high-dimension, high-sparsity, and frequent dropout events that lead to
ambiguous cell population boundaries. However, their reliance on hard graph
constructions derived from thresholded similarity matrices presents
challenges:(i) The simplification of intercellular relationships into binary
edges (0 or 1) by applying thresholds, which restricts the capture of
continuous similarity features among cells and leads to significant information
loss.(ii) The presence of significant inter-cluster connections within hard
graphs, which can confuse GNN methods that rely heavily on graph structures,
potentially causing erroneous message propagation and biased clustering
outcomes. To tackle these challenges, we introduce scSGC, a Soft Graph
Clustering for single-cell RNA sequencing data, which aims to more accurately
characterize continuous similarities among cells through non-binary edge
weights, thereby mitigating the limitations of rigid data structures. The scSGC
framework comprises three core components: (i) a zero-inflated negative
binomial (ZINB)-based feature autoencoder; (ii) a dual-channel cut-informed
soft graph embedding module; and (iii) an optimal transport-based clustering
optimization module. Extensive experiments across ten datasets demonstrate that
scSGC outperforms 13 state-of-the-art clustering models in clustering accuracy,
cell type annotation, and computational efficiency. These results highlight its
substantial potential to advance scRNA-seq data analysis and deepen our
understanding of cellular heterogeneity.

</details>


### [379] [Algorithm Development in Neural Networks: Insights from the Streaming Parity Task](https://arxiv.org/abs/2507.09897)
*Loek van Rossem,Andrew M. Saxe*

Main category: cs.LG

TL;DR: 论文研究了RNN在流式奇偶任务中的学习动态，发现其能通过有限训练实现无限泛化，揭示了神经网络从有限经验中学习算法的机制。


<details>
  <summary>Details</summary>
Motivation: 探索神经网络在过参数化情况下如何实现无限泛化，特别是在流式奇偶任务中，以理解其算法学习能力。

Method: 通过训练RNN完成流式奇偶任务，分析其学习动态，并提出有效理论解释其表示动态。

Result: RNN在有限训练后表现出完美无限泛化的相变，并通过表示合并效应构建了能复现任务的有限自动机。

Conclusion: 研究揭示了神经网络从有限训练经验中实现无限泛化的一种机制，为理解其算法学习能力提供了新视角。

Abstract: Even when massively overparameterized, deep neural networks show a remarkable
ability to generalize. Research on this phenomenon has focused on
generalization within distribution, via smooth interpolation. Yet in some
settings neural networks also learn to extrapolate to data far beyond the
bounds of the original training set, sometimes even allowing for infinite
generalization, implying that an algorithm capable of solving the task has been
learned. Here we undertake a case study of the learning dynamics of recurrent
neural networks (RNNs) trained on the streaming parity task in order to develop
an effective theory of algorithm development. The streaming parity task is a
simple but nonlinear task defined on sequences up to arbitrary length. We show
that, with sufficient finite training experience, RNNs exhibit a phase
transition to perfect infinite generalization. Using an effective theory for
the representational dynamics, we find an implicit representational merger
effect which can be interpreted as the construction of a finite automaton that
reproduces the task. Overall, our results disclose one mechanism by which
neural networks can generalize infinitely from finite training experience.

</details>


### [380] [Extracting Cause-Effect Pairs from a Sentence with a Dependency-Aware Transformer Model](https://arxiv.org/abs/2507.09925)
*Md Ahsanul Kabir,Abrar Jahin,Mohammad Al Hasan*

Main category: cs.LG

TL;DR: 提出了一种结合依赖树的Transformer模型DepBERT，用于提取句子中的因果关系短语，性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有监督方法未充分利用依赖树等语言学工具，限制了因果关系提取的效果。

Method: 扩展Transformer模型，将句子的依赖树整合到模型框架中。

Result: 在三个数据集上的实验表明，DepBERT优于现有最先进的因果关系提取方法。

Conclusion: DepBERT通过结合依赖树提升了因果关系提取的性能，证明了语言学工具在监督方法中的重要性。

Abstract: Extracting cause and effect phrases from a sentence is an important NLP task,
with numerous applications in various domains, including legal, medical,
education, and scientific research. There are many unsupervised and supervised
methods proposed for solving this task. Among these, unsupervised methods
utilize various linguistic tools, including syntactic patterns, dependency
tree, dependency relations, etc. among different sentential units for
extracting the cause and effect phrases. On the other hand, the contemporary
supervised methods use various deep learning based mask language models
equipped with a token classification layer for extracting cause and effect
phrases. Linguistic tools, specifically, dependency tree, which organizes a
sentence into different semantic units have been shown to be very effective for
extracting semantic pairs from a sentence, but existing supervised methods do
not have any provision for utilizing such tools within their model framework.
In this work, we propose DepBERT, which extends a transformer-based model by
incorporating dependency tree of a sentence within the model framework.
Extensive experiments over three datasets show that DepBERT is better than
various state-of-the art supervised causality extraction methods.

</details>


### [381] [Mechanistic Interpretability of LoRA-Adapted Language Models for Nuclear Reactor Safety Applications](https://arxiv.org/abs/2507.09931)
*Yoon Pyo Lee*

Main category: cs.LG

TL;DR: 本文提出了一种解释大型语言模型（LLM）在核工程领域内部推理过程的新方法，通过微调和神经元分析，验证了特定神经元组对任务性能的关键作用。


<details>
  <summary>Details</summary>
Motivation: 为了在安全关键领域（如核工程）中部署LLM，需要理解其内部知识编码和推理机制，以满足核监管框架的验证要求。

Method: 采用低秩适应（LoRA）技术微调通用LLM（Gemma-3-1b-it），并通过神经元激活模式分析和神经元沉默技术探究其因果作用。

Result: 沉默特定神经元组显著降低了任务性能，表明这些神经元对生成准确技术信息至关重要。

Conclusion: 该方法为提升黑盒模型的透明度提供了可行路径，支持核级AI验证，推动安全关键领域的AI部署。

Abstract: The integration of Large Language Models (LLMs) into safety-critical domains,
such as nuclear engineering, necessitates a deep understanding of their
internal reasoning processes. This paper presents a novel methodology for
interpreting how an LLM encodes and utilizes domain-specific knowledge, using a
Boiling Water Reactor system as a case study. We adapted a general-purpose LLM
(Gemma-3-1b-it) to the nuclear domain using a parameter-efficient fine-tuning
technique known as Low-Rank Adaptation. By comparing the neuron activation
patterns of the base model to those of the fine-tuned model, we identified a
sparse set of neurons whose behavior was significantly altered during the
adaptation process. To probe the causal role of these specialized neurons, we
employed a neuron silencing technique. Our results demonstrate that while
silencing most of these specialized neurons individually did not produce a
statistically significant effect, deactivating the entire group collectively
led to a statistically significant degradation in task performance. Qualitative
analysis further revealed that silencing these neurons impaired the model's
ability to generate detailed, contextually accurate technical information. This
paper provides a concrete methodology for enhancing the transparency of an
opaque black-box model, allowing domain expertise to be traced to verifiable
neural circuits. This offers a pathway towards achieving nuclear-grade
artificial intelligence (AI) assurance, addressing the verification and
validation challenges mandated by nuclear regulatory frameworks (e.g., 10 CFR
50 Appendix B), which have limited AI deployment in safety-critical nuclear
operations.

</details>


### [382] [Memorization Sinks: Isolating Memorization during LLM Training](https://arxiv.org/abs/2507.09937)
*Gaurav R. Ghosal,Pratyush Maini,Aditi Raghunathan*

Main category: cs.LG

TL;DR: 论文提出MemSinks方法，通过设计隔离记忆内容，解决大语言模型记忆重复序列的隐私和版权问题。


<details>
  <summary>Details</summary>
Motivation: 大语言模型容易记忆重复序列，引发隐私和版权担忧，现有方法效果有限。

Method: 提出MemSinks范式，利用序列标识符激活特定记忆神经元，实现记忆内容的隔离。

Result: 在十亿参数和十亿token规模下，MemSinks实现了有效隔离和强泛化能力。

Conclusion: MemSinks首次证明在真实数据上同时实现泛化和隔离是可行的。

Abstract: Large language models are susceptible to memorizing repeated sequences,
posing privacy and copyright concerns. A popular mitigation strategy is to
remove memorized information from specific neurons post-hoc. However, such
approaches have shown limited success so far. In a controlled setting, we show
that the memorization of natural sequences (those that resemble linguistically
plausible text) become mechanistically entangled with general language
abilities, thereby becoming challenging to remove post-hoc. In this work, we
put forward a new paradigm of MemSinks that promotes isolation of memorization
by design. We leverage a sequence identifier that activates a unique set of
memorization neurons for each sequence across repetitions. By analyzing the
dynamics of learning and forgetting, we argue that MemSinks facilitates
isolation of memorized content, making it easier to remove without compromising
general language capabilities. We implement MemSinks at the billion-parameter
and billion-token scale, and observe both effective isolation and strong
generalization. To our knowledge, this is the first proof-of-concept on real
data demonstrating that simultaneous generalization and isolation is
achievable. We open-source our code at http://github.com/grghosal/MemSinks.

</details>


### [383] [Long-Tailed Data Classification by Increasing and Decreasing Neurons During Training](https://arxiv.org/abs/2507.09940)
*Taigo Sakai,Kazuhiro Hotta*

Main category: cs.LG

TL;DR: 论文提出了一种动态调整神经元数量的方法，通过训练过程中定期增减神经元，提升对少数类的识别能力，最终网络结构和大小保持不变。


<details>
  <summary>Details</summary>
Motivation: 传统深度学习神经元数量固定，而生物学研究表明人脑在学习过程中会动态生成和修剪神经元，启发通过灵活分配容量提升性能。

Method: 在训练过程中周期性增减神经元，动态调整容量，同时保留多数类学习的关键特征。

Result: 在三个数据集和五种模型上的实验表明，该方法优于固定大小网络，与其他不平衡处理技术结合时效果更佳。

Conclusion: 动态、受生物学启发的网络设计能有效提升类别不平衡数据的性能。

Abstract: In conventional deep learning, the number of neurons typically remains fixed
during training. However, insights from biology suggest that the human
hippocampus undergoes continuous neuron generation and pruning of neurons over
the course of learning, implying that a flexible allocation of capacity can
contribute to enhance performance. Real-world datasets often exhibit class
imbalance situations where certain classes have far fewer samples than others,
leading to significantly reduce recognition accuracy for minority classes when
relying on fixed size networks.To address the challenge, we propose a method
that periodically adds and removes neurons during training, thereby boosting
representational power for minority classes. By retaining critical features
learned from majority classes while selectively increasing neurons for
underrepresented classes, our approach dynamically adjusts capacity during
training. Importantly, while the number of neurons changes throughout training,
the final network size and structure remain unchanged, ensuring efficiency and
compatibility with deployment.Furthermore, by experiments on three different
datasets and five representative models, we demonstrate that the proposed
method outperforms fixed size networks and shows even greater accuracy when
combined with other imbalance-handling techniques. Our results underscore the
effectiveness of dynamic, biologically inspired network designs in improving
performance on class-imbalanced data.

</details>


### [384] [Iceberg: Enhancing HLS Modeling with Synthetic Data](https://arxiv.org/abs/2507.09948)
*Zijian Ding,Tung Nguyen,Weikai Li,Aditya Grover,Yizhou Sun,Jason Cong*

Main category: cs.LG

TL;DR: Iceberg通过合成数据增强和弱标签生成，显著提升了深度学习模型在HLS中的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 解决深度学习模型在硬件设计高级综合（HLS）中泛化能力不足的问题。

Method: 使用合成数据预训练和弱标签生成，结合上下文模型架构进行元学习。

Result: 在六个实际应用中，模型准确率提升86.4%，离线DSE性能提升2.47倍和1.12倍。

Conclusion: Iceberg有效提升了HLS预测模型的泛化能力，代码已开源。

Abstract: Deep learning-based prediction models for High-Level Synthesis (HLS) of
hardware designs often struggle to generalize. In this paper, we study how to
close the generalizability gap of these models through pretraining on synthetic
data and introduce Iceberg, a synthetic data augmentation approach that expands
both large language model (LLM)-generated programs and weak labels of unseen
design configurations. Our weak label generation method is integrated with an
in-context model architecture, enabling meta-learning from actual and proximate
labels. Iceberg improves the geometric mean modeling accuracy by $86.4\%$ when
adapt to six real-world applications with few-shot examples and achieves a
$2.47\times$ and a $1.12\times$ better offline DSE performance when adapting to
two different test datasets. Our open-sourced code is here:
\href{https://github.com/UCLA-VAST/iceberg}{https://github.com/UCLA-VAST/iceberg}

</details>


### [385] [Hierarchical Job Classification with Similarity Graph Integration](https://arxiv.org/abs/2507.09949)
*Md Ahsanul Kabir,Kareem Abdelfatah,Mohammed Korayem,Mohammad Al Hasan*

Main category: cs.LG

TL;DR: 提出了一种新的表示学习和分类模型，用于在线招聘中的职位分类，通过嵌入层次化行业类别到潜在空间，显著提升了分类准确性。


<details>
  <summary>Details</summary>
Motivation: 在线招聘中职位分类的复杂性增加，传统文本分类方法难以充分利用行业类别的层次结构，需要更先进的模型。

Method: 结合标准职业分类系统（SOC）和内部层次化分类法Carotene，将职位和行业类别嵌入到潜在空间中，利用图和层次关系提升分类效果。

Result: 在大规模职位发布数据集上的实验表明，该模型显著优于现有方法，能够更好地利用层次结构和语义特征。

Conclusion: 该研究为招聘行业提供了提升职位分类准确性的框架，支持更明智的决策。

Abstract: In the dynamic realm of online recruitment, accurate job classification is
paramount for optimizing job recommendation systems, search rankings, and labor
market analyses. As job markets evolve, the increasing complexity of job titles
and descriptions necessitates sophisticated models that can effectively
leverage intricate relationships within job data. Traditional text
classification methods often fall short, particularly due to their inability to
fully utilize the hierarchical nature of industry categories. To address these
limitations, we propose a novel representation learning and classification
model that embeds jobs and hierarchical industry categories into a latent
embedding space. Our model integrates the Standard Occupational Classification
(SOC) system and an in-house hierarchical taxonomy, Carotene, to capture both
graph and hierarchical relationships, thereby improving classification
accuracy. By embedding hierarchical industry categories into a shared latent
space, we tackle cold start issues and enhance the dynamic matching of
candidates to job opportunities. Extensive experimentation on a large-scale
dataset of job postings demonstrates the model's superior ability to leverage
hierarchical structures and rich semantic features, significantly outperforming
existing methods. This research provides a robust framework for improving job
classification accuracy, supporting more informed decision-making in the
recruitment industry.

</details>


### [386] [Radial Neighborhood Smoothing Recommender System](https://arxiv.org/abs/2507.09952)
*Zerui Zhang,Yumou Qiu*

Main category: cs.LG

TL;DR: 论文提出了一种新的潜在空间距离估计方法，通过行和列距离近似潜在空间距离，并引入方差校正和径向邻域估计器（RNE）来提高推荐系统的性能。


<details>
  <summary>Details</summary>
Motivation: 推荐系统在潜在空间中具有低秩结构，但如何有效定义和测量潜在空间中的距离以捕捉用户和物品关系是一个关键挑战。

Method: 通过行和列距离近似潜在空间距离，引入方差校正以减少噪声影响，并提出径向邻域估计器（RNE）来构建邻域并提高插补准确性。

Result: 在模拟和真实数据集上的评估表明，RNE优于现有的协同过滤和矩阵分解方法，并能缓解冷启动问题。

Conclusion: 论文提出的距离估计方法和RNE为推荐系统提供了一种更结构化的邻域构建方式，显著提升了性能。

Abstract: Recommender systems inherently exhibit a low-rank structure in latent space.
A key challenge is to define meaningful and measurable distances in the latent
space to capture user-user, item-item, user-item relationships effectively. In
this work, we establish that distances in the latent space can be
systematically approximated using row-wise and column-wise distances in the
observed matrix, providing a novel perspective on distance estimation. To
refine the distance estimation, we introduce the correction based on empirical
variance estimator to account for noise-induced non-centrality. The novel
distance estimation enables a more structured approach to constructing
neighborhoods, leading to the Radial Neighborhood Estimator (RNE), which
constructs neighborhoods by including both overlapped and partially overlapped
user-item pairs and employs neighborhood smoothing via localized kernel
regression to improve imputation accuracy. We provide the theoretical
asymptotic analysis for the proposed estimator. We perform evaluations on both
simulated and real-world datasets, demonstrating that RNE achieves superior
performance compared to existing collaborative filtering and matrix
factorization methods. While our primary focus is on distance estimation in
latent space, we find that RNE also mitigates the ``cold-start'' problem.

</details>


### [387] [Rethinking Inductive Bias in Geographically Neural Network Weighted Regression](https://arxiv.org/abs/2507.09958)
*Zhenyuan Chen*

Main category: cs.LG

TL;DR: 论文探讨了空间回归模型中的归纳偏差问题，通过改进GNNWR模型，结合CNN、RNN和Transformer技术，提升了捕捉复杂空间关系的能力。实验表明，模型性能与数据特性密切相关。


<details>
  <summary>Details</summary>
Motivation: 现有GNNWR模型在建模空间非平稳性时存在局限性，尤其是固定的距离权重方案和有限的归纳偏差。

Method: 提出改进GNNWR模型，引入CNN的局部感受野、RNN的序列上下文和Transformer的自注意力机制。

Result: 改进后的GNNWR在合成数据集上表现优于传统方法，尤其在非线性空间关系建模中。模型性能受数据特性影响显著。

Conclusion: 归纳偏差对空间建模至关重要，未来可探索可学习的空间权重函数、混合神经架构及模型可解释性提升。

Abstract: Inductive bias is a key factor in spatial regression models, determining how
well a model can learn from limited data and capture spatial patterns. This
work revisits the inductive biases in Geographically Neural Network Weighted
Regression (GNNWR) and identifies limitations in current approaches for
modeling spatial non-stationarity. While GNNWR extends traditional
Geographically Weighted Regression by using neural networks to learn spatial
weighting functions, existing implementations are often restricted by fixed
distance-based schemes and limited inductive bias. We propose to generalize
GNNWR by incorporating concepts from convolutional neural networks, recurrent
neural networks, and transformers, introducing local receptive fields,
sequential context, and self-attention into spatial regression. Through
extensive benchmarking on synthetic spatial datasets with varying
heterogeneity, noise, and sample sizes, we show that GNNWR outperforms classic
methods in capturing nonlinear and complex spatial relationships. Our results
also reveal that model performance depends strongly on data characteristics,
with local models excelling in highly heterogeneous or small-sample scenarios,
and global models performing better with larger, more homogeneous data. These
findings highlight the importance of inductive bias in spatial modeling and
suggest future directions, including learnable spatial weighting functions,
hybrid neural architectures, and improved interpretability for models handling
non-stationary spatial data.

</details>


### [388] [Text-Driven Causal Representation Learning for Source-Free Domain Generalization](https://arxiv.org/abs/2507.09961)
*Lihua Zhou,Mao Ye,Nianxin Li,Shuaifeng Li,Jinlin Wu,Xiatian Zhu,Lei Deng,Hongbin Liu,Jiebo Luo,Zhen Lei*

Main category: cs.LG

TL;DR: TDCRL是一种结合因果推理的文本驱动因果表示学习方法，用于解决源自由领域泛化中的领域特定混淆问题，通过生成文本嵌入和训练因果干预网络实现领域不变特征提取。


<details>
  <summary>Details</summary>
Motivation: 传统领域泛化方法需要多源域数据，成本高且不实用；现有源自由领域泛化方法受限于领域特定混淆问题，泛化能力不足。

Method: TDCRL分两步：1）生成风格词向量与类别信息结合生成文本嵌入；2）训练因果干预网络提取领域不变特征。

Result: 在PACS、VLCS、OfficeHome和DomainNet数据集上表现优异，达到最先进水平。

Conclusion: TDCRL通过因果学习机制有效提取领域不变特征，显著提升源自由领域泛化性能。

Abstract: Deep learning often struggles when training and test data distributions
differ. Traditional domain generalization (DG) tackles this by including data
from multiple source domains, which is impractical due to expensive data
collection and annotation. Recent vision-language models like CLIP enable
source-free domain generalization (SFDG) by using text prompts to simulate
visual representations, reducing data demands. However, existing SFDG methods
struggle with domain-specific confounders, limiting their generalization
capabilities. To address this issue, we propose TDCRL
(\textbf{T}ext-\textbf{D}riven \textbf{C}ausal \textbf{R}epresentation
\textbf{L}earning), the first method to integrate causal inference into the
SFDG setting. TDCRL operates in two steps: first, it employs data augmentation
to generate style word vectors, combining them with class information to
generate text embeddings to simulate visual representations; second, it trains
a causal intervention network with a confounder dictionary to extract
domain-invariant features. Grounded in causal learning, our approach offers a
clear and effective mechanism to achieve robust, domain-invariant features,
ensuring robust generalization. Extensive experiments on PACS, VLCS,
OfficeHome, and DomainNet show state-of-the-art performance, proving TDCRL
effectiveness in SFDG.

</details>


### [389] [Compliance Minimization via Physics-Informed Gaussian Processes](https://arxiv.org/abs/2507.09968)
*Xiangyu Sun,Amin Yousefpour,Shirin Hosseinmardi,Ramin Bostanabad*

Main category: cs.LG

TL;DR: 提出了一种基于物理信息高斯过程的网格自由框架，用于解决合规性最小化问题，通过共享多输出神经网络和参数化设计，实现了高效、可控的设计复杂性。


<details>
  <summary>Details</summary>
Motivation: 传统机器学习方法在解决合规性最小化问题时存在特征边界模糊、计算成本高和缺乏设计复杂性控制机制的问题。

Method: 采用基于高斯过程的设计和状态变量参数化，共享多输出神经网络作为均值函数，结合PGCANs架构和损失函数优化。

Result: 方法实现了超分辨率拓扑快速收敛、更小的合规性和灰度区域，优于传统数值方法和竞争性机器学习方法。

Conclusion: 该框架为合规性最小化问题提供了一种高效、可控且性能优越的解决方案。

Abstract: Machine learning (ML) techniques have recently gained significant attention
for solving compliance minimization (CM) problems. However, these methods
typically provide poor feature boundaries, are very expensive, and lack a
systematic mechanism to control the design complexity. Herein, we address these
limitations by proposing a mesh-free and simultaneous framework based on
physics-informed Gaussian processes (GPs). In our approach, we parameterize the
design and state variables with GP priors which have independent kernels but
share a multi-output neural network (NN) as their mean function. The
architecture of this NN is based on Parametric Grid Convolutional Attention
Networks (PGCANs) which not only mitigate spectral bias issues, but also
provide an interpretable mechanism to control design complexity. We estimate
all the parameters of our GP-based representations by simultaneously minimizing
the compliance, total potential energy, and residual of volume fraction
constraint. Importantly, our loss function exclude all data-based residuals as
GPs automatically satisfy them. We also develop computational schemes based on
curriculum training and numerical integration to increase the efficiency and
robustness of our approach which is shown to (1) produce super-resolution
topologies with fast convergence, (2) achieve smaller compliance and less gray
area fraction compared to traditional numerical methods, (3) provide control
over fine-scale features, and (4) outperform competing ML-based methods.

</details>


### [390] [Effects of structural properties of neural networks on machine learning performance](https://arxiv.org/abs/2507.10005)
*Yash Arya,Sang Hoon Lee*

Main category: cs.LG

TL;DR: 该论文研究了图结构对神经网络性能的影响，特别是社区结构的作用，发现密集互连的社区能提升学习能力。


<details>
  <summary>Details</summary>
Motivation: 探索图结构（如社区结构）对神经网络预测性能的影响，弥补现有研究在真实网络结构上的不足。

Method: 使用随机和无标度网络模型，结合生物神经网络及其子集，分析结构属性对图像分类任务的影响。

Result: 结构属性确实影响性能，密集互连的社区结构能增强学习能力。

Conclusion: 研究为网络科学和机器学习提供了新见解，可能启发更接近生物结构的神经网络设计。

Abstract: In recent years, graph-based machine learning techniques, such as
reinforcement learning and graph neural networks, have garnered significant
attention. While some recent studies have started to explore the relationship
between the graph structure of neural networks and their predictive
performance, they often limit themselves to a narrow range of model networks,
particularly lacking mesoscale structures such as communities. Our work
advances this area by conducting a more comprehensive investigation,
incorporating realistic network structures characterized by heterogeneous
degree distributions and community structures, which are typical
characteristics of many real networks. These community structures offer a
nuanced perspective on network architecture. Our analysis employs model
networks such as random and scale-free networks, alongside a comparison with a
biological neural network and its subsets for more detailed analysis. We
examine the impact of these structural attributes on the performance of image
classification tasks. Our findings reveal that structural properties do affect
performance to some extent. Specifically, networks featuring coherent, densely
interconnected communities demonstrate enhanced learning capabilities. The
comparison with the biological neural network emphasizes the relevance of our
findings to real-world structures, suggesting an intriguing connection worth
further exploration. This study contributes meaningfully to network science and
machine learning, providing insights that could inspire the design of more
biologically informed neural networks.

</details>


### [391] [Forecasting Coccidioidomycosis (Valley Fever) in Arizona: A Graph Neural Network Approach](https://arxiv.org/abs/2507.10014)
*Ali Sarabi,Arash Sarabi,Hao Yan,Beckett Sterner,Petar Jevtić*

Main category: cs.LG

TL;DR: 本研究开发了首个用于预测亚利桑那州谷热发病率的图神经网络模型，整合了环境因素和病例数据，揭示了关键环境驱动因素。


<details>
  <summary>Details</summary>
Motivation: 谷热在西南美国流行地区是重要的公共卫生问题，需要有效的预测模型以支持早期预警和资源分配。

Method: 采用图神经网络（GNN）整合病例数据和环境预测因子，探索变量间的相关性，并捕捉疾病进展的延迟效应。

Result: GNN模型有效预测了谷热趋势，并识别了关键环境驱动因素。

Conclusion: 该模型可为高风险地区的早期预警和疾病预防提供支持。

Abstract: Coccidioidomycosis, commonly known as Valley Fever, remains a significant
public health concern in endemic regions of the southwestern United States.
This study develops the first graph neural network (GNN) model for forecasting
Valley Fever incidence in Arizona. The model integrates surveillance case data
with environmental predictors using graph structures, including soil
conditions, atmospheric variables, agricultural indicators, and air quality
metrics. Our approach explores correlation-based relationships among variables
influencing disease transmission. The model captures critical delays in disease
progression through lagged effects, enhancing its capacity to reflect complex
temporal dependencies in disease ecology. Results demonstrate that the GNN
architecture effectively models Valley Fever trends and provides insights into
key environmental drivers of disease incidence. These findings can inform early
warning systems and guide resource allocation for disease prevention efforts in
high-risk areas.

</details>


### [392] [Towards Applying Large Language Models to Complement Single-Cell Foundation Models](https://arxiv.org/abs/2507.10039)
*Steven Palayew,Bo Wang,Gary Bader*

Main category: cs.LG

TL;DR: 论文探讨了LLMs在单细胞数据分析中的性能驱动因素，并提出了scMPT模型，结合scGPT和LLMs的优势，提升了性能。


<details>
  <summary>Details</summary>
Motivation: 现有单细胞基础模型无法利用生物学文本信息，而LLMs虽表现优异但缺乏对其性能驱动因素的理解。

Method: 研究LLMs在单细胞数据中的性能驱动因素，提出scMPT模型，结合scGPT和LLMs的表示。

Result: scMPT性能优于单一模型，且在不同数据集上表现更稳定。

Conclusion: LLMs可补充单细胞基础模型，推动单细胞分析进步。

Abstract: Single-cell foundation models such as scGPT represent a significant
advancement in single-cell omics, with an ability to achieve state-of-the-art
performance on various downstream biological tasks. However, these models are
inherently limited in that a vast amount of information in biology exists as
text, which they are unable to leverage. There have therefore been several
recent works that propose the use of LLMs as an alternative to single-cell
foundation models, achieving competitive results. However, there is little
understanding of what factors drive this performance, along with a strong focus
on using LLMs as an alternative, rather than complementary approach to
single-cell foundation models. In this study, we therefore investigate what
biological insights contribute toward the performance of LLMs when applied to
single-cell data, and introduce scMPT; a model which leverages synergies
between scGPT, and single-cell representations from LLMs that capture these
insights. scMPT demonstrates stronger, more consistent performance than either
of its component models, which frequently have large performance gaps between
each other across datasets. We also experiment with alternate fusion methods,
demonstrating the potential of combining specialized reasoning models with
scGPT to improve performance. This study ultimately showcases the potential for
LLMs to complement single-cell foundation models and drive improvements in
single-cell analysis.

</details>


### [393] [On the Efficiency of Training Robust Decision Trees](https://arxiv.org/abs/2507.10048)
*Benedict Gerlach,Marie Anastacio,Holger H. Hoos*

Main category: cs.LG

TL;DR: 本文提出了一种高效的对抗性鲁棒决策树训练流程，包括自动扰动大小选择、对抗训练和鲁棒性验证三个阶段，显著提升了效率。


<details>
  <summary>Details</summary>
Motivation: 随着机器学习在工业中的快速应用，鲁棒性和效率成为关键问题。本文旨在建立一个高效的对抗性鲁棒训练流程。

Method: 1. 自动选择扰动大小；2. 训练对抗性鲁棒模型；3. 验证模型鲁棒性。

Result: 研究发现验证时间与训练时间无关，且扰动大小可通过小模型估计，显著提升效率。

Conclusion: 本文提出的流程在对抗性鲁棒训练中实现了高效性和可持续性。

Abstract: As machine learning gets adopted into the industry quickly, trustworthiness
is increasingly in focus. Yet, efficiency and sustainability of robust training
pipelines still have to be established. In this work, we consider a simple
pipeline for training adversarially robust decision trees and investigate the
efficiency of each step. Our pipeline consists of three stages. Firstly, we
choose the perturbation size automatically for each dataset. For that, we
introduce a simple algorithm, instead of relying on intuition or prior work.
Moreover, we show that the perturbation size can be estimated from smaller
models than the one intended for full training, and thus significant gains in
efficiency can be achieved. Secondly, we train state-of-the-art adversarial
training methods and evaluate them regarding both their training time and
adversarial accuracy. Thirdly, we certify the robustness of each of the models
thus obtained and investigate the time required for this. We find that
verification time, which is critical to the efficiency of the full pipeline, is
not correlated with training time.

</details>


### [394] [Compression Method for Deep Diagonal State Space Model Based on $H^2$ Optimal Reduction](https://arxiv.org/abs/2507.10078)
*Hiroki Sakamoto,Kazuhiro Sato*

Main category: cs.LG

TL;DR: 提出了一种基于控制理论中$H^{2}$模型降阶技术的高效参数缩减方法，用于减少线性SSM模型的参数量，同时保持性能。


<details>
  <summary>Details</summary>
Motivation: 线性SSM模型在处理长序列依赖时参数量过大，难以在资源受限设备上部署。

Method: 应用控制理论中的$H^{2}$模型降阶技术对线性SSM组件进行参数缩减。

Result: 实验表明，该方法将SSM参数量减少至1/32，且性能优于现有的平衡截断方法。

Conclusion: 提出的方法在保持模型性能的同时显著减少了参数量，适用于资源受限场景。

Abstract: Deep learning models incorporating linear SSMs have gained attention for
capturing long-range dependencies in sequential data. However, their large
parameter sizes pose challenges for deployment on resource-constrained devices.
In this study, we propose an efficient parameter reduction method for these
models by applying $H^{2}$ model order reduction techniques from control theory
to their linear SSM components. In experiments, the LRA benchmark results show
that the model compression based on our proposed method outperforms an existing
method using the Balanced Truncation, while successfully reducing the number of
parameters in the SSMs to $1/32$ without sacrificing the performance of the
original models.

</details>


### [395] [Towards High Supervised Learning Utility Training Data Generation: Data Pruning and Column Reordering](https://arxiv.org/abs/2507.10088)
*Tung Sum Thomas Kwok,Zeyong Zhang,Chi-Hua Wang,Guang Cheng*

Main category: cs.LG

TL;DR: PRRO是一种新颖的管道，通过数据修剪和列重新排序技术优化合成表格数据的监督学习性能。


<details>
  <summary>Details</summary>
Motivation: 解决合成数据在监督学习中性能不佳的问题，尤其是类别不平衡和数据关系忽视。

Method: 提出PRRO管道，结合数据修剪和列重新排序，优化合成数据的信号噪声比和结构对齐。

Result: 在22个公共数据集上，PRRO显著提升预测性能，最高达871.46%改进；在6个不平衡数据集上，类别分布相似性提升43%。

Conclusion: PRRO有效提升合成数据的监督学习性能，促进高质量和可访问的数据分析。

Abstract: Tabular data synthesis for supervised learning ('SL') model training is
gaining popularity in industries such as healthcare, finance, and retail.
Despite the progress made in tabular data generators, models trained with
synthetic data often underperform compared to those trained with original data.
This low SL utility of synthetic data stems from class imbalance exaggeration
and SL data relationship overlooked by tabular generator. To address these
challenges, we draw inspirations from techniques in emerging data-centric
artificial intelligence and elucidate Pruning and ReOrdering ('PRRO'), a novel
pipeline that integrates data-centric techniques into tabular data synthesis.
PRRO incorporates data pruning to guide the table generator towards
observations with high signal-to-noise ratio, ensuring that the class
distribution of synthetic data closely matches that of the original data.
Besides, PRRO employs a column reordering algorithm to align the data modeling
structure of generators with that of SL models. These two modules enable PRRO
to optimize SL utility of synthetic data. Empirical experiments on 22 public
datasets show that synthetic data generated using PRRO enhances predictive
performance compared to data generated without PRRO. Specifically, synthetic
replacement of original data yields an average improvement of 26.74% and up to
871.46% improvement using PRRO, while synthetic appendant to original data
results with PRRO-generated data results in an average improvement of 6.13% and
up to 200.32%. Furthermore, experiments on six highly imbalanced datasets show
that PRRO enables the generator to produce synthetic data with a class
distribution that resembles the original data more closely, achieving a
similarity improvement of 43%. Through PRRO, we foster a seamless integration
of data synthesis to subsequent SL prediction, promoting quality and accessible
data analysis.

</details>


### [396] [A Variance-Reduced Cubic-Regularized Newton for Policy Optimization](https://arxiv.org/abs/2507.10120)
*Cheng Sun,Zhen Zhang,Shaofu Yang*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: In this paper, we study a second-order approach to policy optimization in
reinforcement learning. Existing second-order methods often suffer from
suboptimal sample complexity or rely on unrealistic assumptions about
importance sampling. To overcome these limitations, we propose VR-CR-PN, a
variance-reduced cubic-regularized policy Newton algorithm. To the best of our
knowledge, this is the first algorithm that integrates Hessian-aided variance
reduction with second-order policy optimization, effectively addressing the
distribution shift problem and achieving best-known sample complexity under
general nonconvex conditions but without the need for importance sampling. We
theoretically establish that VR-CR-PN achieves a sample complexity of
$\tilde{\mathcal{O}}(\epsilon^{-3})$ to reach an $\epsilon$-second-order
stationary point, significantly improving upon the previous best result of
$\tilde{\mathcal{O}}(\epsilon^{-3.5})$ under comparable assumptions. As an
additional contribution, we introduce a novel Hessian estimator for the
expected return function, which admits a uniform upper bound independent of the
horizon length $H$, allowing the algorithm to achieve horizon-independent
sample complexity.

</details>


### [397] [Wavelet-Enhanced Neural ODE and Graph Attention for Interpretable Energy Forecasting](https://arxiv.org/abs/2507.10132)
*Usman Gani Joy*

Main category: cs.LG

TL;DR: 本文提出了一种结合神经ODE、图注意力、小波变换和自适应频率学习的神经网络框架，用于能源供需预测，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 能源供需预测对可持续能源系统优化至关重要，但受可再生能源波动和动态消费模式挑战。

Method: 模型整合神经ODE、图注意力、多分辨率小波变换和自适应频率学习，采用Runge-Kutta方法和残差连接。

Result: 在七个数据集上表现优于现有方法，能捕捉复杂时间依赖性。

Conclusion: 该模型在预测性能和可解释性上表现优异，适用于可持续能源应用。

Abstract: Accurate forecasting of energy demand and supply is critical for optimizing
sustainable energy systems, yet it is challenged by the variability of
renewable sources and dynamic consumption patterns. This paper introduces a
neural framework that integrates continuous-time Neural Ordinary Differential
Equations (Neural ODEs), graph attention, multi-resolution wavelet
transformations, and adaptive learning of frequencies to address the issues of
time series prediction. The model employs a robust ODE solver, using the
Runge-Kutta method, paired with graph-based attention and residual connections
to better understand both structural and temporal patterns. Through
wavelet-based feature extraction and adaptive frequency modulation, it adeptly
captures and models diverse, multi-scale temporal dynamics. When evaluated
across seven diverse datasets: ETTh1, ETTh2, ETTm1, ETTm2 (electricity
transformer temperature), and Waste, Solar, and Hydro (renewable energy), this
architecture consistently outperforms state-of-the-art baselines in various
forecasting metrics, proving its robustness in capturing complex temporal
dependencies. Furthermore, the model enhances interpretability through SHAP
analysis, making it suitable for sustainable energy applications.

</details>


### [398] [Domain Borders Are There to Be Crossed With Federated Few-Shot Adaptation](https://arxiv.org/abs/2507.10160)
*Manuel Röder,Christoph Raab,Frank-Michael Schleif*

Main category: cs.LG

TL;DR: FedAcross+是一个高效的联邦学习框架，专注于解决实际工业场景中的客户端适应问题，通过冻结部分模型结构减少计算开销，并支持流式数据处理。


<details>
  <summary>Details</summary>
Motivation: 联邦学习在边缘设备上的实际应用面临数据标注成本高、协变量偏移以及资源限制下的模型更新困难等挑战。

Method: 框架基于预训练模型，冻结主干和分类器，仅通过自适应线性层处理目标域适应，支持流式数据和非静态环境。

Result: 实验证明FedAcross+在低端设备上能有效适应目标域，解决域偏移问题，并支持资源受限环境下的零星模型更新。

Conclusion: FedAcross+为资源受限的工业场景提供了一种实用且高效的联邦学习解决方案。

Abstract: Federated Learning has emerged as a leading paradigm for decentralized,
privacy-preserving learning, particularly relevant in the era of interconnected
edge devices equipped with sensors. However, the practical implementation of
Federated Learning faces three primary challenges: the need for human
involvement in costly data labelling processes for target adaptation, covariate
shift in client device data collection due to environmental factors affecting
sensors, leading to discrepancies between source and target samples, and the
impracticality of continuous or regular model updates in resource-constrained
environments due to limited data transmission capabilities and technical
constraints on channel availability and energy efficiency. To tackle these
issues, we expand upon an efficient and scalable Federated Learning framework
tailored for real-world client adaptation in industrial settings. This
framework leverages a pre-trained source model comprising a deep backbone, an
adaptation module, and a classifier running on a powerful server. By freezing
the backbone and classifier during client adaptation on resource-constrained
devices, we allow the domain adaptive linear layer to handle target domain
adaptation, thus minimizing overall computational overhead. Furthermore, this
setup, designated as FedAcross+, is extended to encompass the processing of
streaming data, thereby rendering the solution suitable for non-stationary
environments. Extensive experimental results demonstrate the effectiveness of
FedAcross+ in achieving competitive adaptation on low-end client devices with
limited target samples, successfully addressing the challenge of domain shift.
Moreover, our framework accommodates sporadic model updates within
resource-constrained environments, ensuring practical and seamless deployment.

</details>


### [399] [Understanding the Rank of Tensor Networks via an Intuitive Example-Driven Approach](https://arxiv.org/abs/2507.10170)
*Wuyang Zhou,Giorgos Iacovides,Kriton Konstantinidis,Ilya Kisil,Danilo Mandic*

Main category: cs.LG

TL;DR: 本文旨在澄清张量网络（TN）分解中TN秩的概念，通过实际案例和直观可视化，帮助读者理解如何根据领域知识选择TN秩，并揭示其与矩阵秩的关系。


<details>
  <summary>Details</summary>
Motivation: TN秩是TN分解的核心概念，但其缺乏统一的定义和直观解释，常被当作经验调参的超参数。本文希望通过直观方法揭示TN秩的本质，促进其在实践和教育中的应用。

Method: 通过实际案例（如CP和Tucker分解）展示领域知识如何指导TN秩的选择，并采用图形化方法推广到任意阶张量，揭示TN秩与矩阵秩的关系。

Result: 提供了一种清晰统一的TN秩理解方法，支持领域驱动的TN设计，简化了复杂张量代数的处理。

Conclusion: 本文为读者提供了TN秩的直观理解和实用工具，有助于TN方法在实际应用和教育中的选择和解释。

Abstract: Tensor Network (TN) decompositions have emerged as an indispensable tool in
Big Data analytics owing to their ability to provide compact low-rank
representations, thus alleviating the ``Curse of Dimensionality'' inherent in
handling higher-order data. At the heart of their success lies the concept of
TN ranks, which governs the efficiency and expressivity of TN decompositions.
However, unlike matrix ranks, TN ranks often lack a universal meaning and an
intuitive interpretation, with their properties varying significantly across
different TN structures. Consequently, TN ranks are frequently treated as
empirically tuned hyperparameters, rather than as key design parameters
inferred from domain knowledge. The aim of this Lecture Note is therefore to
demystify the foundational yet frequently misunderstood concept of TN ranks
through real-life examples and intuitive visualizations. We begin by
illustrating how domain knowledge can guide the selection of TN ranks in
widely-used models such as the Canonical Polyadic (CP) and Tucker
decompositions. For more complex TN structures, we employ a self-explanatory
graphical approach that generalizes to tensors of arbitrary order. Such a
perspective naturally reveals the relationship between TN ranks and the
corresponding ranks of tensor unfoldings (matrices), thereby circumventing
cumbersome multi-index tensor algebra while facilitating domain-informed TN
design. It is our hope that this Lecture Note will equip readers with a clear
and unified understanding of the concept of TN rank, along with the necessary
physical insight and intuition to support the selection, explainability, and
deployment of tensor methods in both practical applications and educational
contexts.

</details>


### [400] [Play Style Identification Using Low-Level Representations of Play Traces in MicroRTS](https://arxiv.org/abs/2507.10172)
*Ruizhe Yu Xia,Jeremy Gow,Simon Lucas*

Main category: cs.LG

TL;DR: 本文探讨了使用无监督CNN-LSTM自编码器模型直接从低级别游戏轨迹数据中获取潜在表示的方法，以减少对领域专业知识的依赖。


<details>
  <summary>Details</summary>
Motivation: 游戏风格识别能为游戏设计提供有价值的见解，并实现自适应体验，但现有方法依赖于领域知识和手工特征。

Method: 采用无监督CNN-LSTM自编码器模型，直接从MicroRTS的低级别游戏轨迹数据中学习潜在表示。

Result: 该方法在潜在空间中实现了不同游戏代理的有意义分离，减少了对领域专业知识的依赖。

Conclusion: 潜在空间可用于引导探索AI玩家的多样化游戏风格。

Abstract: Play style identification can provide valuable game design insights and
enable adaptive experiences, with the potential to improve game playing agents.
Previous work relies on domain knowledge to construct play trace
representations using handcrafted features. More recent approaches incorporate
the sequential structure of play traces but still require some level of domain
abstraction. In this study, we explore the use of unsupervised CNN-LSTM
autoencoder models to obtain latent representations directly from low-level
play trace data in MicroRTS. We demonstrate that this approach yields a
meaningful separation of different game playing agents in the latent space,
reducing reliance on domain expertise and its associated biases. This latent
space is then used to guide the exploration of diverse play styles within
studied AI players.

</details>


### [401] [T-GRAB: A Synthetic Diagnostic Benchmark for Learning on Temporal Graphs](https://arxiv.org/abs/2507.10183)
*Alireza Dizaji,Benedict Aaron Tjandra,Mehrab Hamidi,Shenyang Huang,Guillaume Rabusseau*

Main category: cs.LG

TL;DR: 论文提出T-GRAB基准，用于系统评估TGNNs在时间推理能力上的表现，发现现有模型在泛化时间模式方面存在不足。


<details>
  <summary>Details</summary>
Motivation: 当前TGNNs是否有效捕捉核心时间模式（如周期性、因果关系和长程依赖）尚不明确，需要系统性评估。

Method: 引入T-GRAB基准，包含合成任务以隔离关键时间技能，并评估11种TGNNs方法。

Result: 发现现有模型在泛化时间模式方面存在根本性不足。

Conclusion: T-GRAB揭示了传统基准隐藏的挑战，为开发更强时间推理能力的架构提供了方向。

Abstract: Dynamic graph learning methods have recently emerged as powerful tools for
modelling relational data evolving through time. However, despite extensive
benchmarking efforts, it remains unclear whether current Temporal Graph Neural
Networks (TGNNs) effectively capture core temporal patterns such as
periodicity, cause-and-effect, and long-range dependencies. In this work, we
introduce the Temporal Graph Reasoning Benchmark (T-GRAB), a comprehensive set
of synthetic tasks designed to systematically probe the capabilities of TGNNs
to reason across time. T-GRAB provides controlled, interpretable tasks that
isolate key temporal skills: counting/memorizing periodic repetitions,
inferring delayed causal effects, and capturing long-range dependencies over
both spatial and temporal dimensions. We evaluate 11 temporal graph learning
methods on these tasks, revealing fundamental shortcomings in their ability to
generalize temporal patterns. Our findings offer actionable insights into the
limitations of current models, highlight challenges hidden by traditional
real-world benchmarks, and motivate the development of architectures with
stronger temporal reasoning abilities. The code for T-GRAB can be found at:
https://github.com/alirezadizaji/T-GRAB.

</details>


### [402] [Learning Private Representations through Entropy-based Adversarial Training](https://arxiv.org/abs/2507.10194)
*Tassilo Klein,Moin Nabi*

Main category: cs.LG

TL;DR: 提出了一种对抗性表示学习方法，通过引入焦点熵来减少隐私泄露，同时保持预测能力。


<details>
  <summary>Details</summary>
Motivation: 如何在保持用户隐私的同时学习具有高预测能力的表示。

Method: 使用对抗性表示学习方法和焦点熵变体来净化敏感内容。

Result: 在多个基准测试中展示了可行性，结果表明在适度隐私泄露的情况下具有高目标效用。

Conclusion: 该方法在保护隐私的同时有效维持了表示的预测能力。

Abstract: How can we learn a representation with high predictive power while preserving
user privacy? We present an adversarial representation learning method for
sanitizing sensitive content from the learned representation. Specifically, we
introduce a variant of entropy - focal entropy, which mitigates the potential
information leakage of the existing entropy-based approaches. We showcase
feasibility on multiple benchmarks. The results suggest high target utility at
moderate privacy leakage.

</details>


### [403] [A Graph Sufficiency Perspective for Neural Networks](https://arxiv.org/abs/2507.10215)
*Cencheng Shen,Yuexiao Dong*

Main category: cs.LG

TL;DR: 论文通过图变量和统计充分性分析神经网络，提出层输出对输入具有统计充分性的条件，并在无限宽度和有限宽度网络中验证其可行性。


<details>
  <summary>Details</summary>
Motivation: 旨在将统计充分性、图论表示与深度学习结合，为神经网络提供新的统计理解。

Method: 将神经网络层解释为基于图的变换，神经元作为输入与锚点间的成对函数，分析层输出的统计充分性条件。

Result: 在无限宽度极限下证明渐进充分性，并在有限宽度网络中通过区域分离输入分布实现充分性。

Conclusion: 框架适用于多种网络结构，为神经网络的统计特性提供了新视角。

Abstract: This paper analyzes neural networks through graph variables and statistical
sufficiency. We interpret neural network layers as graph-based transformations,
where neurons act as pairwise functions between inputs and learned anchor
points. Within this formulation, we establish conditions under which layer
outputs are sufficient for the layer inputs, that is, each layer preserves the
conditional distribution of the target variable given the input variable. Under
dense anchor point assumptions, we prove that asymptotic sufficiency holds in
the infinite-width limit and is preserved throughout training. To align more
closely with practical architectures, we further show that sufficiency can be
achieved with finite-width networks by assuming region-separated input
distributions and constructing appropriate anchor points. Our framework covers
fully connected layers, general pairwise functions, ReLU and sigmoid
activations, and convolutional neural networks. This work bridges statistical
sufficiency, graph-theoretic representations, and deep learning, providing a
new statistical understanding of neural networks.

</details>


### [404] [Kernel-Adaptive PI-ELMs for Forward and Inverse Problems in PDEs with Sharp Gradients](https://arxiv.org/abs/2507.10241)
*Vikas Dwivedi,Balaji Srinivasan,Monica Sigovan,Bruno Sixou*

Main category: cs.LG

TL;DR: KAPI-ELM是一种基于RBF的自适应扩展方法，用于解决具有局部尖锐梯度的PDE问题，结合了BO和最小二乘优化，在速度和表达能力上均优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 解决PI-ELM在捕捉尖锐梯度时的局限性，同时保持其快速计算的优势。

Method: 引入轻量级BO框架，优化输入层的统计分布参数，结合最小二乘优化输出层参数。

Result: 在多个PDE基准测试中表现出色，包括1D和2D问题，达到了最先进的精度。

Conclusion: KAPI-ELM是一种可扩展、可解释且通用的物理信息学习框架，特别适用于刚性PDE问题。

Abstract: This paper introduces the Kernel Adaptive Physics-Informed Extreme Learning
Machine (KAPI-ELM), an adaptive Radial Basis Function (RBF)-based extension of
PI-ELM designed to solve both forward and inverse Partial Differential Equation
(PDE) problems involving localized sharp gradients. While PI-ELMs outperform
the traditional Physics-Informed Neural Networks (PINNs) in speed due to their
single-shot, least square optimization, this advantage comes at a cost: their
fixed, randomly initialized input layer limits their ability to capture sharp
gradients. To overcome this limitation, we introduce a lightweight Bayesian
Optimization (BO) framework that, instead of adjusting each input layer
parameter individually as in traditional backpropagation, learns a small set of
hyperparameters defining the statistical distribution from which the input
weights are drawn. This novel distributional optimization strategy -- combining
BO for input layer distributional parameters with least-squares optimization
for output layer network parameters -- enables KAPI-ELM to preserve PI-ELM's
speed while matching or exceeding the expressiveness of PINNs. We validate the
proposed methodology on several challenging forward and inverse PDE benchmarks,
including a 1D singularly perturbed convection-diffusion equation, a 2D Poisson
equation with sharp localized sources, and a time-dependent advection equation.
Notably, KAPI-ELM achieves state-of-the-art accuracy in both forward and
inverse settings. In stiff PDE regimes, it matches or even outperforms advanced
methods such as the Extended Theory of Functional Connections (XTFC), while
requiring nearly an order of magnitude fewer tunable parameters. These results
establish the potential of KAPI-ELM as a scalable, interpretable, and
generalizable physics-informed learning framework, especially in stiff PDE
regimes.

</details>


### [405] [Conditional Chemical Language Models are Versatile Tools in Drug Discovery](https://arxiv.org/abs/2507.10273)
*Lu Zhu,Emmanuel Noutahi*

Main category: cs.LG

TL;DR: SAFE-T是一个基于生物背景的化学建模框架，通过条件生成模型优化分子设计和评分，显著提升药物发现效率。


<details>
  <summary>Details</summary>
Motivation: 解决生成化学语言模型在药物发现中缺乏可靠奖励信号和输出解释性的问题。

Method: SAFE-T通过建模分子片段序列的条件似然性，结合生物提示（如蛋白质靶点），支持虚拟筛选、药物-靶点相互作用预测等任务。

Result: 在零样本评估中，SAFE-T性能优于或与现有方法相当，且速度更快，同时能捕捉已知结构-活性关系。

Conclusion: 条件生成化学语言模型可统一评分与生成，加速早期药物发现。

Abstract: Generative chemical language models (CLMs) have demonstrated strong
capabilities in molecular design, yet their impact in drug discovery remains
limited by the absence of reliable reward signals and the lack of
interpretability in their outputs. We present SAFE-T, a generalist chemical
modeling framework that conditions on biological context -- such as protein
targets or mechanisms of action -- to prioritize and design molecules without
relying on structural information or engineered scoring functions. SAFE-T
models the conditional likelihood of fragment-based molecular sequences given a
biological prompt, enabling principled scoring of molecules across tasks such
as virtual screening, drug-target interaction prediction, and activity cliff
detection. Moreover, it supports goal-directed generation by sampling from this
learned distribution, aligning molecular design with biological objectives. In
comprehensive zero-shot evaluations across predictive (LIT-PCBA, DAVIS, KIBA,
ACNet) and generative (DRUG, PMO) benchmarks, SAFE-T consistently achieves
performance comparable to or better than existing approaches while being
significantly faster. Fragment-level attribution further reveals that SAFE-T
captures known structure-activity relationships, supporting interpretable and
biologically grounded design. Together with its computational efficiency, these
results demonstrate that conditional generative CLMs can unify scoring and
generation to accelerate early-stage drug discovery.

</details>


### [406] [Average Sensitivity of Hierarchical $k$-Median Clustering](https://arxiv.org/abs/2507.10296)
*Shijie Li,Weiqiang He,Ruobing Bai,Pan Peng*

Main category: cs.LG

TL;DR: 本文研究了层次聚类中的$k$-中值问题，提出了一种高效算法，证明了其低平均敏感性和高质量聚类，并通过实验验证了其稳健性和有效性。


<details>
  <summary>Details</summary>
Motivation: 现代算法应用中，数据集通常大且动态，若层次聚类对数据扰动敏感，算法实用性将降低。因此，研究层次$k$-中值聚类的平均敏感性具有重要意义。

Method: 提出一种高效层次$k$-中值聚类算法，通过测量随机删除数据点时输出的预期变化来分析平均敏感性。

Result: 理论证明该算法具有低平均敏感性和高聚类质量，同时发现单链接聚类和CLNSS算法的确定性变体敏感性较高。

Conclusion: 实验验证了所提算法的稳健性和有效性，为动态数据集下的层次聚类提供了实用解决方案。

Abstract: Hierarchical clustering is a widely used method for unsupervised learning
with numerous applications. However, in the application of modern algorithms,
the datasets studied are usually large and dynamic. If the hierarchical
clustering is sensitive to small perturbations of the dataset, the usability of
the algorithm will be greatly reduced. In this paper, we focus on the
hierarchical $k$ -median clustering problem, which bridges hierarchical and
centroid-based clustering while offering theoretical appeal, practical utility,
and improved interpretability. We analyze the average sensitivity of algorithms
for this problem by measuring the expected change in the output when a random
data point is deleted. We propose an efficient algorithm for hierarchical
$k$-median clustering and theoretically prove its low average sensitivity and
high clustering quality. Additionally, we show that single linkage clustering
and a deterministic variant of the CLNSS algorithm exhibit high average
sensitivity, making them less stable. Finally, we validate the robustness and
effectiveness of our algorithm through experiments.

</details>


### [407] [Recognizing Dementia from Neuropsychological Tests with State Space Models](https://arxiv.org/abs/2507.10311)
*Liming Wang,Saurabhchand Bhati,Cody Karjadi,Rhoda Au,James Glass*

Main category: cs.LG

TL;DR: Demenba是一种基于状态空间模型的自动痴呆分类框架，通过语音记录推断认知衰退，性能优于现有方法21%，且参数更少。


<details>
  <summary>Details</summary>
Motivation: 早期发现痴呆对及时医疗干预和改善患者预后至关重要，传统神经心理测试依赖人工评分，自动化系统能提高效率和准确性。

Method: 提出Demenba框架，基于状态空间模型，内存和计算随序列长度线性增长，结合大型语言模型进一步提升性能。

Result: 在超过1,000小时的认知评估数据上训练，性能提升21%，参数更少，且融合大型语言模型后效果更佳。

Conclusion: Demenba为痴呆评估提供了更透明、可扩展的工具，未来可结合更多数据和技术进一步优化。

Abstract: Early detection of dementia is critical for timely medical intervention and
improved patient outcomes. Neuropsychological tests are widely used for
cognitive assessment but have traditionally relied on manual scoring. Automatic
dementia classification (ADC) systems aim to infer cognitive decline directly
from speech recordings of such tests. We propose Demenba, a novel ADC framework
based on state space models, which scale linearly in memory and computation
with sequence length. Trained on over 1,000 hours of cognitive assessments
administered to Framingham Heart Study participants, some of whom were
diagnosed with dementia through adjudicated review, our method outperforms
prior approaches in fine-grained dementia classification by 21\%, while using
fewer parameters. We further analyze its scaling behavior and demonstrate that
our model gains additional improvement when fused with large language models,
paving the way for more transparent and scalable dementia assessment tools.
Code: https://anonymous.4open.science/r/Demenba-0861

</details>


### [408] [Convergence of Agnostic Federated Averaging](https://arxiv.org/abs/2507.10325)
*Herlock,Rahimi,Dionysis Kalogerias*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Federated learning (FL) enables decentralized model training without
centralizing raw data. However, practical FL deployments often face a key
realistic challenge: Clients participate intermittently in server aggregation
and with unknown, possibly biased participation probabilities. Most existing
convergence results either assume full-device participation, or rely on
knowledge of (in fact uniform) client availability distributions -- assumptions
that rarely hold in practice. In this work, we characterize the optimization
problem that consistently adheres to the stochastic dynamics of the well-known
\emph{agnostic Federated Averaging (FedAvg)} algorithm under random (and
variably-sized) client availability, and rigorously establish its convergence
for convex, possibly nonsmooth losses, achieving a standard rate of order
$\mathcal{O}(1/\sqrt{T})$, where $T$ denotes the aggregation horizon. Our
analysis provides the first convergence guarantees for agnostic FedAvg under
general, non-uniform, stochastic client participation, without knowledge of the
participation distribution. We also empirically demonstrate that agnostic
FedAvg in fact outperforms common (and suboptimal) weighted aggregation FedAvg
variants, even with server-side knowledge of participation weights.

</details>


### [409] [MoCap-Impute: A Comprehensive Benchmark and Comparative Analysis of Imputation Methods for IMU-based Motion Capture Data](https://arxiv.org/abs/2507.10334)
*Mahmoud Bekhit,Ahmad Salah,Ahmed Salim Alrawahi,Tarek Attia,Ahmed Ali,Esraa Eldesokey,Ahmed Fathalla*

Main category: cs.LG

TL;DR: 本文通过比较统计、机器学习和深度学习的插补方法，填补了IMU运动捕捉数据缺失值处理的系统性评估空白，并提出了首个公开数据集。多变量方法在复杂缺失情况下表现更优。


<details>
  <summary>Details</summary>
Motivation: 运动捕捉数据在体育科学中应用广泛，但数据缺失问题影响其效用。目前缺乏对IMU衍生运动捕捉时间序列数据插补方法的系统性评估。

Method: 采用统计、机器学习和深度学习插补方法，在三种缺失机制（随机缺失、块缺失和信号转换点依赖缺失）下进行评估。使用53名空手道练习者的公开数据集。

Result: 多变量插补框架显著优于单变量方法，尤其在复杂缺失情况下（如转换点缺失，MAE降低50%）。GAIN和迭代插补器表现最佳。

Conclusion: 本研究为未来研究提供了基准，并为提高运动捕捉数据分析的完整性和鲁棒性提供了实用建议。

Abstract: Motion capture (MoCap) data from wearable Inertial Measurement Units (IMUs)
is vital for applications in sports science, but its utility is often
compromised by missing data. Despite numerous imputation techniques, a
systematic performance evaluation for IMU-derived MoCap time-series data is
lacking. We address this gap by conducting a comprehensive comparative analysis
of statistical, machine learning, and deep learning imputation methods. Our
evaluation considers three distinct contexts: univariate time-series,
multivariate across subjects, and multivariate across kinematic angles. To
facilitate this benchmark, we introduce the first publicly available MoCap
dataset designed specifically for imputation, featuring data from 53 karate
practitioners. We simulate three controlled missingness mechanisms: missing
completely at random (MCAR), block missingness, and a novel value-dependent
pattern at signal transition points. Our experiments, conducted on 39 kinematic
variables across all subjects, reveal that multivariate imputation frameworks
consistently outperform univariate approaches, particularly for complex
missingness. For instance, multivariate methods achieve up to a 50% mean
absolute error reduction (MAE from 10.8 to 5.8) compared to univariate
techniques for transition point missingness. Advanced models like Generative
Adversarial Imputation Networks (GAIN) and Iterative Imputers demonstrate the
highest accuracy in these challenging scenarios. This work provides a critical
baseline for future research and offers practical recommendations for improving
the integrity and robustness of Mo-Cap data analysis.

</details>


### [410] [Some Super-approximation Rates of ReLU Neural Networks for Korobov Functions](https://arxiv.org/abs/2507.10345)
*Yuwen Li,Guozhi Zhang*

Main category: cs.LG

TL;DR: 论文研究了ReLU神经网络对Korobov函数的$L_p$和$W^1_p$范数逼近误差，提出了网络宽度和深度下的超逼近误差界。


<details>
  <summary>Details</summary>
Motivation: 探讨神经网络在高维空间中的表达能力，避免维度灾难的影响。

Method: 利用稀疏网格有限元和比特提取技术，分析逼近误差。

Result: 在$L_p$范数下得到$2m$阶误差界，$W^1_p$范数下得到$2m-2$阶误差界，优于经典结果。

Conclusion: ReLU神经网络的表达能力在高维空间中表现优异，不受维度灾难限制。

Abstract: This paper examines the $L_p$ and $W^1_p$ norm approximation errors of ReLU
neural networks for Korobov functions. In terms of network width and depth, we
derive nearly optimal super-approximation error bounds of order $2m$ in the
$L_p$ norm and order $2m-2$ in the $W^1_p$ norm, for target functions with
$L_p$ mixed derivative of order $m$ in each direction. The analysis leverages
sparse grid finite elements and the bit extraction technique. Our results
improve upon classical lowest order $L_\infty$ and $H^1$ norm error bounds and
demonstrate that the expressivity of neural networks is largely unaffected by
the curse of dimensionality.

</details>


### [411] [Parallel Sampling of Diffusion Models on $SO(3)$](https://arxiv.org/abs/2507.10347)
*Yan-Ting Chen,Hao-Wei Chen,Tsu-Ching Hsiao,Chun-Yi Lee*

Main category: cs.LG

TL;DR: 提出了一种在$SO(3)$流形上加速扩散过程的算法，通过数值Picard迭代实现，实验显示速度提升达4.9倍且不影响任务奖励。


<details>
  <summary>Details</summary>
Motivation: 扩散模型在$SO(3)$流形上的去噪过程耗时较长，需要加速。

Method: 采用数值Picard迭代方法优化扩散过程。

Result: 算法速度提升达4.9倍，且任务奖励无显著下降。

Conclusion: 该方法有效加速了$SO(3)$流形上的扩散过程，具有实际应用价值。

Abstract: In this paper, we design an algorithm to accelerate the diffusion process on
the $SO(3)$ manifold. The inherently sequential nature of diffusion models
necessitates substantial time for denoising perturbed data. To overcome this
limitation, we proposed to adapt the numerical Picard iteration for the $SO(3)$
space. We demonstrate our algorithm on an existing method that employs
diffusion models to address the pose ambiguity problem. Moreover, we show that
this acceleration advantage occurs without any measurable degradation in task
reward. The experiments reveal that our algorithm achieves a speed-up of up to
4.9$\times$, significantly reducing the latency for generating a single sample.

</details>


### [412] [Feature Distillation is the Better Choice for Model-Heterogeneous Federated Learning](https://arxiv.org/abs/2507.10348)
*Yichen Li*

Main category: cs.LG

TL;DR: FedFD提出了一种基于特征蒸馏的异构联邦学习方法，通过正交投影对齐特征信息，解决了传统方法中知识偏差的问题，提升了模型性能。


<details>
  <summary>Details</summary>
Motivation: 异构联邦学习（Hetero-FL）在聚合异构模型知识时存在知识偏差问题，传统方法仅依赖logit蒸馏效果不佳。

Method: 提出FedFD方法，利用正交投影技术对齐特征信息，为每个客户端模型架构维护投影层以优化知识蒸馏。

Result: 实验表明FedFD在性能上优于现有方法。

Conclusion: FedFD通过特征蒸馏和正交投影有效解决了异构模型的知识偏差问题，提升了联邦学习的稳定性和效果。

Abstract: Model-Heterogeneous Federated Learning (Hetero-FL) has attracted growing
attention for its ability to aggregate knowledge from heterogeneous models
while keeping private data locally. To better aggregate knowledge from clients,
ensemble distillation, as a widely used and effective technique, is often
employed after global aggregation to enhance the performance of the global
model. However, simply combining Hetero-FL and ensemble distillation does not
always yield promising results and can make the training process unstable. The
reason is that existing methods primarily focus on logit distillation, which,
while being model-agnostic with softmax predictions, fails to compensate for
the knowledge bias arising from heterogeneous models. To tackle this challenge,
we propose a stable and efficient Feature Distillation for model-heterogeneous
Federated learning, dubbed FedFD, that can incorporate aligned feature
information via orthogonal projection to integrate knowledge from heterogeneous
models better. Specifically, a new feature-based ensemble federated knowledge
distillation paradigm is proposed. The global model on the server needs to
maintain a projection layer for each client-side model architecture to align
the features separately. Orthogonal techniques are employed to re-parameterize
the projection layer to mitigate knowledge bias from heterogeneous models and
thus maximize the distilled knowledge. Extensive experiments show that FedFD
achieves superior performance compared to state-of-the-art methods.

</details>


### [413] [TAT: Temporal-Aligned Transformer for Multi-Horizon Peak Demand Forecasting](https://arxiv.org/abs/2507.10349)
*Zhiyuan Zhao,Sitan Yang,Kin G. Olivares,Boris N. Oreshkin,Stan Vitebsky,Michael W. Mahoney,B. Aditya Prakash,Dmitry Efimov*

Main category: cs.LG

TL;DR: 论文提出了一种名为Temporal-Aligned Transformer (TAT)的模型，用于多时间范围需求预测，特别关注销售高峰期的准确性。


<details>
  <summary>Details</summary>
Motivation: 需求预测对供应链管理至关重要，尤其是在销售高峰期，准确预测需求峰值尤为困难。

Method: TAT模型结合了先验已知的上下文变量（如节假日和促销活动信息），采用编码器-解码器结构，并引入Temporal Alignment Attention (TAA)机制。

Result: 在大型电商数据集上，TAT在需求峰值预测上实现了高达30%的准确率提升，同时整体性能与现有方法相当。

Conclusion: TAT模型在高峰期需求预测中表现出色，为供应链管理和客户体验提供了有效支持。

Abstract: Multi-horizon time series forecasting has many practical applications such as
demand forecasting. Accurate demand prediction is critical to help make buying
and inventory decisions for supply chain management of e-commerce and physical
retailers, and such predictions are typically required for future horizons
extending tens of weeks. This is especially challenging during high-stake sales
events when demand peaks are particularly difficult to predict accurately.
However, these events are important not only for managing supply chain
operations but also for ensuring a seamless shopping experience for customers.
To address this challenge, we propose Temporal-Aligned Transformer (TAT), a
multi-horizon forecaster leveraging apriori-known context variables such as
holiday and promotion events information for improving predictive performance.
Our model consists of an encoder and decoder, both embedded with a novel
Temporal Alignment Attention (TAA), designed to learn context-dependent
alignment for peak demand forecasting. We conduct extensive empirical analysis
on two large-scale proprietary datasets from a large e-commerce retailer. We
demonstrate that TAT brings up to 30% accuracy improvement on peak demand
forecasting while maintaining competitive overall performance compared to other
state-of-the-art methods.

</details>


### [414] [Enhanced DeepONet for 1-D consolidation operator learning: an architectural investigation](https://arxiv.org/abs/2507.10368)
*Yongjin Choi,Chenying Liu,Jorge Macedo*

Main category: cs.LG

TL;DR: DeepONets在PDE系统中作为代理模型表现优异，但在岩土工程中应用有限。本研究评估了四种DeepONet架构，发现改进后的Model 4性能最佳，计算效率显著提升。


<details>
  <summary>Details</summary>
Motivation: 探索DeepONets在岩土工程中的应用潜力，解决传统方法计算效率低的问题。

Method: 比较四种DeepONet架构（包括标准版和物理启发版），并引入Fourier特征增强的Model 4。

Result: Model 4表现最优，计算速度提升1.5至100倍，适用于复杂系统。

Conclusion: DeepONets在岩土工程中具有高效、通用性强的潜力，推动了科学机器学习在该领域的应用。

Abstract: Deep Operator Networks (DeepONets) have emerged as a powerful surrogate
modeling framework for learning solution operators in PDE-governed systems.
While their use is expanding across engineering disciplines, applications in
geotechnical engineering remain limited. This study systematically evaluates
several DeepONet architectures for the one-dimensional consolidation problem.
We initially consider three architectures: a standard DeepONet with the
coefficient of consolidation embedded in the branch net (Models 1 and 2), and a
physics-inspired architecture with the coefficient embedded in the trunk net
(Model 3). Results show that Model 3 outperforms the standard configurations
(Models 1 and 2) but still has limitations when the target solution (excess
pore pressures) exhibits significant variation. To overcome this limitation, we
propose a Trunknet Fourier feature-enhanced DeepONet (Model 4) that addresses
the identified limitations by capturing rapidly varying functions. All proposed
architectures achieve speedups ranging from 1.5 to 100 times over traditional
explicit and implicit solvers, with Model 4 being the most efficient. Larger
computational savings are expected for more complex systems than the explored
1D case, which is promising. Overall, the study highlights the potential of
DeepONets to enable efficient, generalizable surrogate modeling in geotechnical
applications, advancing the integration of scientific machine learning in
geotechnics, which is at an early stage.

</details>


### [415] [Leveraging RAG-LLMs for Urban Mobility Simulation and Analysis](https://arxiv.org/abs/2507.10382)
*Yue Ding,Conor McCarthy,Kevin O'Shea,Mingming Liu*

Main category: cs.LG

TL;DR: 本文提出了一种基于云和LLM的共享电动出行平台，结合移动应用提供个性化路线推荐，并通过优化模块和RAG框架在不同场景下进行评估。


<details>
  <summary>Details</summary>
Motivation: 随着智能出行和共享电动出行服务的兴起，用户对端到端解决方案的需求增加，需要更先进的技术支持。

Method: 开发了一个云基LLM驱动的共享电动出行平台，集成移动应用提供个性化路线推荐，并评估优化模块和RAG框架的性能。

Result: 优化模块在旅行时间和成本方面表现良好；RAG框架在系统操作员查询和用户查询上的平均执行准确率分别为0.81和0.98。

Conclusion: 该平台和框架为共享电动出行提供了有效的技术支持，满足了用户需求。

Abstract: With the rise of smart mobility and shared e-mobility services, numerous
advanced technologies have been applied to this field. Cloud-based traffic
simulation solutions have flourished, offering increasingly realistic
representations of the evolving mobility landscape. LLMs have emerged as
pioneering tools, providing robust support for various applications, including
intelligent decision-making, user interaction, and real-time traffic analysis.
As user demand for e-mobility continues to grow, delivering comprehensive
end-to-end solutions has become crucial. In this paper, we present a
cloud-based, LLM-powered shared e-mobility platform, integrated with a mobile
application for personalized route recommendations. The optimization module is
evaluated based on travel time and cost across different traffic scenarios.
Additionally, the LLM-powered RAG framework is evaluated at the schema level
for different users, using various evaluation methods. Schema-level RAG with
XiYanSQL achieves an average execution accuracy of 0.81 on system operator
queries and 0.98 on user queries.

</details>


### [416] [Extracting Important Tokens in E-Commerce Queries with a Tag Interaction-Aware Transformer Model](https://arxiv.org/abs/2507.10385)
*Md. Ahsanul Kabir,Mohammad Al Hasan,Aritra Mandal,Liyang Hao,Ishita Khan,Daniel Tunkelang,Zhe Wu*

Main category: cs.LG

TL;DR: 论文提出了一种基于语义标签的依赖感知Transformer模型TagBERT，用于电子商务查询重构任务，显著优于现有模型。


<details>
  <summary>Details</summary>
Motivation: 电子商务搜索引擎面临查询模糊、词汇不对齐等问题，传统方法未能充分利用语义标签信息。

Method: 将查询重构任务建模为标记分类问题，设计依赖感知的Transformer模型TagBERT，利用语义标签学习查询短语嵌入。

Result: 在大规模真实电子商务数据集上，TagBERT性能优于BERT、eBERT和序列到序列Transformer模型。

Conclusion: TagBERT通过利用语义标签显著提升了查询重构任务的性能，为电子商务搜索提供了更优解决方案。

Abstract: The major task of any e-commerce search engine is to retrieve the most
relevant inventory items, which best match the user intent reflected in a
query. This task is non-trivial due to many reasons, including ambiguous
queries, misaligned vocabulary between buyers, and sellers, over- or
under-constrained queries by the presence of too many or too few tokens. To
address these challenges, query reformulation is used, which modifies a user
query through token dropping, replacement or expansion, with the objective to
bridge semantic gap between query tokens and users' search intent. Early
methods of query reformulation mostly used statistical measures derived from
token co-occurrence frequencies from selective user sessions having clicks or
purchases. In recent years, supervised deep learning approaches, specifically
transformer-based neural language models, or sequence-to-sequence models are
being used for query reformulation task. However, these models do not utilize
the semantic tags of a query token, which are significant for capturing user
intent of an e-commerce query. In this work, we pose query reformulation as a
token classification task, and solve this task by designing a dependency-aware
transformer-based language model, TagBERT, which makes use of semantic tags of
a token for learning superior query phrase embedding. Experiments on large,
real-life e-commerce datasets show that TagBERT exhibits superior performance
than plethora of competing models, including BERT, eBERT, and
Sequence-to-Sequence transformer model for important token classification task.

</details>


### [417] [Anticipating the Selectivity of Cyclization Reaction Pathways with Neural Network Potentials](https://arxiv.org/abs/2507.10400)
*Nicholas Casetti,Dylan Anstine,Olexandr Isayev,Connor W. Coley*

Main category: cs.LG

TL;DR: 提出了一种针对复杂环化反应的机制搜索策略，结合图枚举和机器学习筛选，利用神经网络势能（AIMNet2-rxn）评估反应路径。


<details>
  <summary>Details</summary>
Motivation: 解决涉及多键协同变化的复杂反应（如天然产物合成中的关键步骤）的机制搜索难题。

Method: 结合图枚举和机器学习筛选，使用神经网络势能（AIMNet2-rxn）评估候选反应路径。

Result: 验证了神经网络势能对活化能的估计能力，正确预测了立体选择性，并重现了天然产物合成中的复杂步骤。

Conclusion: 该策略为复杂反应机制搜索提供了高效且成本效益高的解决方案。

Abstract: Reaction mechanism search tools have demonstrated the ability to provide
insights into likely products and rate-limiting steps of reacting systems.
However, reactions involving several concerted bond changes - as can be found
in many key steps of natural product synthesis - can complicate the search
process. To mitigate these complications, we present a mechanism search
strategy particularly suited to help expedite exploration of an exemplary
family of such complex reactions, cyclizations. We provide a cost-effective
strategy for identifying relevant elementary reaction steps by combining
graph-based enumeration schemes and machine learning techniques for
intermediate filtering. Key to this approach is our use of a neural network
potential (NNP), AIMNet2-rxn, for computational evaluation of each candidate
reaction pathway. In this article, we evaluate the NNP's ability to estimate
activation energies, demonstrate the correct anticipation of stereoselectivity,
and recapitulate complex enabling steps in natural product synthesis.

</details>


### [418] [Stochastic Operator Network: A Stochastic Maximum Principle Based Approach to Operator Learning](https://arxiv.org/abs/2507.10401)
*Ryan Bausback,Jingqiao Tang,Lu Lu,Feng Bao,Toan Huynh*

Main category: cs.LG

TL;DR: 提出了一种用于算子学习中不确定性量化的新框架——随机算子网络（SON），结合了随机神经网络（SNN）和DeepONet的概念，通过随机最优控制方法学习算子的不确定性。


<details>
  <summary>Details</summary>
Motivation: 在算子学习中量化不确定性是一个重要但尚未充分解决的问题，SON框架旨在填补这一空白。

Method: 将分支网络建模为随机微分方程（SDE），并通过伴随BSDE反向传播，利用随机最大值原理中的哈密顿量梯度替代损失函数梯度进行SGD更新。

Result: SON在2D和3D噪声算子复制任务中表现出色，能够有效学习算子的不确定性。

Conclusion: SON框架为算子学习中的不确定性量化提供了一种有效的新方法。

Abstract: We develop a novel framework for uncertainty quantification in operator
learning, the Stochastic Operator Network (SON). SON combines the stochastic
optimal control concepts of the Stochastic Neural Network (SNN) with the
DeepONet. By formulating the branch net as an SDE and backpropagating through
the adjoint BSDE, we replace the gradient of the loss function with the
gradient of the Hamiltonian from Stohastic Maximum Principle in the SGD update.
This allows SON to learn the uncertainty present in operators through its
diffusion parameters. We then demonstrate the effectiveness of SON when
replicating several noisy operators in 2D and 3D.

</details>


### [419] [Energy Efficiency in AI for 5G and Beyond: A DeepRx Case Study](https://arxiv.org/abs/2507.10409)
*Amine Lbath,Ibtissam Labriji*

Main category: cs.LG

TL;DR: 研究通过知识蒸馏（KD）训练紧凑型DeepRX模型，平衡AI/ML模型的能效与性能，验证了KD在降低能耗同时保持性能的有效性。


<details>
  <summary>Details</summary>
Motivation: 解决AI/ML模型在能效与性能之间的平衡问题，特别是在DeepRX接收器中。

Method: 使用知识蒸馏训练紧凑型学生模型，比较不同模型大小、教师模型大小及KD超参数，评估FLOPs/Watt和FLOPs/clock等能耗指标。

Result: 蒸馏模型在SINR水平下表现出更低的错误率，验证了KD在实现能效AI解决方案中的有效性。

Conclusion: 知识蒸馏是平衡AI模型能效与性能的有效方法，尤其在DeepRX接收器中表现显著。

Abstract: This study addresses the challenge of balancing energy efficiency with
performance in AI/ML models, focusing on DeepRX, a deep learning receiver based
on a fully convolutional ResNet architecture. We evaluate the energy
consumption of DeepRX, considering factors including FLOPs/Watt and
FLOPs/clock, and find consistency between estimated and actual energy usage,
influenced by memory access patterns. The research extends to comparing energy
dynamics during training and inference phases. A key contribution is the
application of knowledge distillation (KD) to train a compact DeepRX
\textit{student} model that emulates the performance of the \textit{teacher}
model but with reduced energy consumption. We experiment with different student
model sizes, optimal teacher sizes, and KD hyperparameters. Performance is
measured by comparing the Bit Error Rate (BER) performance versus
Signal-to-Interference \& Noise Ratio (SINR) values of the distilled model and
a model trained from scratch. The distilled models demonstrate a lower error
floor across SINR levels, highlighting the effectiveness of KD in achieving
energy-efficient AI solutions.

</details>


### [420] [Non-exchangeable Conformal Prediction with Optimal Transport: Tackling Distribution Shifts with Unlabeled Data](https://arxiv.org/abs/2507.10425)
*Alvaro H. C. Correia,Christos Louizos*

Main category: cs.LG

TL;DR: 本文通过最优传输理论视角研究分布偏移下的共形预测，提出了一种估计和缓解覆盖率损失的方法。


<details>
  <summary>Details</summary>
Motivation: 共形预测在分布偏移下可能失效，现有方法需预先了解偏移类型，限制了其应用。

Method: 利用最优传输理论分析分布偏移对共形预测的影响，提出覆盖率损失的估计和缓解策略。

Result: 研究表明，通过最优传输理论可以有效估计和缓解分布偏移导致的覆盖率损失。

Conclusion: 最优传输理论为共形预测在非交换性数据下的应用提供了新的解决方案。

Abstract: Conformal prediction is a distribution-free uncertainty quantification method
that has gained popularity in the machine learning community due to its
finite-sample guarantees and ease of use. Its most common variant, dubbed split
conformal prediction, is also computationally efficient as it boils down to
collecting statistics of the model predictions on some calibration data not yet
seen by the model. Nonetheless, these guarantees only hold if the calibration
and test data are exchangeable, a condition that is difficult to verify and
often violated in practice due to so-called distribution shifts. The literature
is rife with methods to mitigate the loss in coverage in this non-exchangeable
setting, but these methods require some prior information on the type of
distribution shift to be expected at test time. In this work, we study this
problem via a new perspective, through the lens of optimal transport, and show
that it is possible to estimate the loss in coverage and mitigate it in case of
distribution shift.

</details>


### [421] [CLA: Latent Alignment for Online Continual Self-Supervised Learning](https://arxiv.org/abs/2507.10434)
*Giacomo Cignoni,Andrea Cossu,Alexandra Gomez-Villa,Joost van de Weijer,Antonio Carta*

Main category: cs.LG

TL;DR: 提出了一种名为CLA的自监督学习策略，用于在线持续学习，通过对齐当前与过去的表征来减少遗忘，并在计算预算内优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 解决在线持续学习中数据以小批量到达、计算预算固定且任务边界缺失时，现有自监督学习技术不足的问题。

Method: 引入Continual Latent Alignment (CLA)，通过对齐当前模型与过去学习的表征来减少遗忘。

Result: CLA加速了在线场景下的训练收敛，并在相同计算预算下优于现有方法；作为预训练协议时，早期使用CLA比完整i.i.d.预训练表现更好。

Conclusion: CLA是一种有效的在线持续学习自监督策略，既能减少遗忘，又能提升最终性能。

Abstract: Self-supervised learning (SSL) is able to build latent representations that
generalize well to unseen data. However, only a few SSL techniques exist for
the online CL setting, where data arrives in small minibatches, the model must
comply with a fixed computational budget, and task boundaries are absent. We
introduce Continual Latent Alignment (CLA), a novel SSL strategy for Online CL
that aligns the representations learned by the current model with past
representations to mitigate forgetting. We found that our CLA is able to speed
up the convergence of the training process in the online scenario,
outperforming state-of-the-art approaches under the same computational budget.
Surprisingly, we also discovered that using CLA as a pretraining protocol in
the early stages of pretraining leads to a better final performance when
compared to a full i.i.d. pretraining.

</details>


### [422] [Response Wide Shut? Surprising Observations in Basic Vision Language Model Capabilities](https://arxiv.org/abs/2507.10442)
*Shivam Chandhok,Wan-Cyuan Fan,Vered Shwartz,Vineeth N Balasubramanian,Leonid Sigal*

Main category: cs.LG

TL;DR: 本文研究了当前最先进的视觉语言模型（VLMs）在基础视觉任务中的局限性，通过设计一系列测试来分析其设计组件的不足。


<details>
  <summary>Details</summary>
Motivation: 尽管VLMs在复杂计算机视觉问题中表现出色，但仍缺乏一些基本的视觉理解能力。本文旨在揭示这些局限性。

Method: 通过构建测试，比较VLMs在视觉编码器、视觉-语言投影和LLM解码器输出等组件上的表现，分析其设计缺陷。

Result: 研究发现VLMs在视觉信息处理、鲁棒性和能力方面存在不足，并提出了重要观察。

Conclusion: 本文的发现有望为改进VLMs提供指导。

Abstract: Vision-language Models (VLMs) have emerged as general-purpose tools for
addressing a variety of complex computer vision problems. Such models have been
shown to be highly capable, but, at the same time, lacking some basic visual
understanding skills. In this paper, we set out to understand the limitations
of SoTA VLMs on fundamental visual tasks by constructing a series of tests that
probe which components of design, specifically, may be lacking. Importantly, we
go significantly beyond the current benchmarks, which simply measure the final
performance of VLM response, by also comparing and contrasting it to the
performance of probes trained directly on features obtained from the visual
encoder, intermediate vision-language projection and LLM-decoder output. In
doing so, we uncover shortcomings in VLMs and make a number of important
observations about their capabilities, robustness and how they process visual
information. We hope our insights will guide progress in further improving
VLMs.

</details>


### [423] [Some remarks on gradient dominance and LQR policy optimization](https://arxiv.org/abs/2507.10452)
*Eduardo D. Sontag*

Main category: cs.LG

TL;DR: 论文探讨了优化问题中梯度下降的变体，特别是强化学习中的策略优化，通过Polyak-Łojasiewicz不等式（PLI）实现指数收敛速率，并比较了连续时间和离散时间LQR问题的差异。


<details>
  <summary>Details</summary>
Motivation: 研究优化问题中梯度下降的收敛行为，特别是连续时间与离散时间LQR问题的差异，以及梯度估计误差对收敛的影响。

Method: 应用Polyak-Łojasiewicz不等式（PLI）及其广义条件，分析梯度流下的收敛行为，并进行输入到状态稳定性（ISS）分析。

Result: 发现连续时间LQR问题的收敛速率随初始条件增大而消失，而离散时间LQR问题则具有全局指数收敛性。

Conclusion: 广义PLI条件是理解梯度估计误差影响的关键，ISS分析为相关问题的稳定性提供了理论支持。

Abstract: Solutions of optimization problems, including policy optimization in
reinforcement learning, typically rely upon some variant of gradient descent.
There has been much recent work in the machine learning, control, and
optimization communities applying the Polyak-{\L}ojasiewicz Inequality (PLI) to
such problems in order to establish an exponential rate of convergence (a.k.a.
``linear convergence'' in the local-iteration language of numerical analysis)
of loss functions to their minima under the gradient flow. Often, as is the
case of policy iteration for the continuous-time LQR problem, this rate
vanishes for large initial conditions, resulting in a mixed globally linear /
locally exponential behavior. This is in sharp contrast with the discrete-time
LQR problem, where there is global exponential convergence. That gap between CT
and DT behaviors motivates the search for various generalized PLI-like
conditions, and this talk will address that topic. Moreover, these
generalizations are key to understanding the transient and asymptotic effects
of errors in the estimation of the gradient, errors which might arise from
adversarial attacks, wrong evaluation by an oracle, early stopping of a
simulation, inaccurate and very approximate digital twins, stochastic
computations (algorithm ``reproducibility''), or learning by sampling from
limited data. We describe an ``input to state stability'' (ISS) analysis of
this issue. The lecture also discussed convergence and PLI-like properties of
``linear feedforward neural networks'' in feedback control, but this arXiv
skips that part (to be updated). Much of the work described here was done in
collaboration with Arthur Castello B. de Oliveira, Leilei Cui, Zhong-Ping
Jiang, and Milad Siami.

</details>


### [424] [The Target Polish: A New Approach to Outlier-Resistant Non-Negative Matrix and Tensor Factorization](https://arxiv.org/abs/2507.10484)
*Paul Fogel,Christophe Geissler,George Luta*

Main category: cs.LG

TL;DR: Target Polish框架是一种高效且稳健的非负矩阵和张量分解方法，通过自适应平滑数据提升速度与抗异常值能力。


<details>
  <summary>Details</summary>
Motivation: 传统加权NMF方法因使用乘法更新而收敛缓慢，需一种既能抗异常值又高效的方法。

Method: 结合Fast-HALS算法，采用加权中位数变换自适应平滑数据，保持加法更新结构。

Result: 在图像数据集上，Target Polish在精度上媲美或超越现有方法，计算时间减少一个数量级。

Conclusion: Target Polish在保持高效的同时提升了抗异常值能力，适用于实际应用。

Abstract: This paper introduces the "Target Polish," a robust and computationally
efficient framework for nonnegative matrix and tensor factorization. Although
conventional weighted NMF approaches are resistant to outliers, they converge
slowly due to the use of multiplicative updates to minimize the objective
criterion. In contrast, the Target Polish approach remains compatible with the
Fast-HALS algorithm, which is renowned for its speed, by adaptively smoothing
the data with a weighted median-based transformation. This innovation provides
outlier resistance while maintaining the highly efficient additive update
structure of Fast-HALS. Empirical evaluations using image datasets corrupted
with structured (block) and unstructured (salt) noise demonstrate that the
Target Polish approach matches or exceeds the accuracy of state-of-the-art
robust NMF methods and reduces computational time by an order of magnitude in
the studied scenarios.

</details>


### [425] [Overcoming catastrophic forgetting in neural networks](https://arxiv.org/abs/2507.10485)
*Brandon Shuen Yi Loke,Filippo Quadri,Gabriel Vivanco,Maximilian Casagrande,Saúl Fenollosa*

Main category: cs.LG

TL;DR: 论文研究了弹性权重巩固（EWC）在持续学习中的应用，验证了其在减少遗忘方面的有效性，同时探讨了其对学习效率的影响。


<details>
  <summary>Details</summary>
Motivation: 解决神经网络在持续学习中的灾难性遗忘问题，验证EWC方法的有效性。

Method: 使用PermutedMNIST和RotatedMNIST基准测试，比较EWC与L2正则化和SGD的表现。

Result: EWC显著减少了遗忘，但略微降低了新任务的学习效率。

Conclusion: EWC是神经网络终身学习的可行解决方案。

Abstract: Catastrophic forgetting is the primary challenge that hinders continual
learning, which refers to a neural network ability to sequentially learn
multiple tasks while retaining previously acquired knowledge. Elastic Weight
Consolidation, a regularization-based approach inspired by synaptic
consolidation in biological neural systems, has been used to overcome this
problem. In this study prior research is replicated and extended by evaluating
EWC in supervised learning settings using the PermutedMNIST and RotatedMNIST
benchmarks. Through systematic comparisons with L2 regularization and
stochastic gradient descent (SGD) without regularization, we analyze how
different approaches balance knowledge retention and adaptability. Our results
confirm what was shown in previous research, showing that EWC significantly
reduces forgetting compared to naive training while slightly compromising
learning efficiency on new tasks. Moreover, we investigate the impact of
dropout regularization and varying hyperparameters, offering insights into the
generalization of EWC across diverse learning scenarios. These results
underscore EWC's potential as a viable solution for lifelong learning in neural
networks.

</details>


### [426] [Split Happens: Combating Advanced Threats with Split Learning and Function Secret Sharing](https://arxiv.org/abs/2507.10494)
*Tanveer Khan,Mindaugas Budzys,Antonis Michalas*

Main category: cs.LG

TL;DR: SplitHappens结合了函数秘密共享（FSS）和U形分割学习（SL），提高了数据隐私保护，同时降低了计算和通信成本。


<details>
  <summary>Details</summary>
Motivation: SL在数据隐私方面存在漏洞，需要更安全的解决方案。

Method: 采用FSS和U形SL结合的方法，保护客户端数据标签，并扩展安全分析。

Result: 实验表明，该方法在减少训练时间和通信成本的同时保持了准确性。

Conclusion: SplitHappens为SL提供了更高的安全保障，适用于多种攻击场景。

Abstract: Split Learning (SL) -- splits a model into two distinct parts to help protect
client data while enhancing Machine Learning (ML) processes. Though promising,
SL has proven vulnerable to different attacks, thus raising concerns about how
effective it may be in terms of data privacy. Recent works have shown promising
results for securing SL through the use of a novel paradigm, named Function
Secret Sharing (FSS), in which servers obtain shares of a function they compute
and operate on a public input hidden with a random mask. However, these works
fall short in addressing the rising number of attacks which exist on SL. In
SplitHappens, we expand the combination of FSS and SL to U-shaped SL. Similarly
to other works, we are able to make use of the benefits of SL by reducing the
communication and computational costs of FSS. However, a U-shaped SL provides a
higher security guarantee than previous works, allowing a client to keep the
labels of the training data secret, without having to share them with the
server. Through this, we are able to generalize the security analysis of
previous works and expand it to different attack vectors, such as modern model
inversion attacks as well as label inference attacks. We tested our approach
for two different convolutional neural networks on different datasets. These
experiments show the effectiveness of our approach in reducing the training
time as well as the communication costs when compared to simply using FSS while
matching prior accuracy.

</details>


### [427] [Benchmarking and Evaluation of AI Models in Biology: Outcomes and Recommendations from the CZI Virtual Cells Workshop](https://arxiv.org/abs/2507.10502)
*Elizabeth Fahsbender,Alma Andersson,Jeremy Ash,Polina Binder,Daniel Burkhardt,Benjamin Chang,Georg K. Gerber,Anthony Gitter,Patrick Godau,Ankit Gupta,Genevieve Haliburton,Siyu He,Trey Ideker,Ivana Jelic,Aly Khan,Yang-Joon Kim,Aditi Krishnapriyan,Jon M. Laurent,Tianyu Liu 28,Emma Lundberg,Shalin B. Mehta,Rob Moccia,Angela Oliveira Pisco,Katherine S. Pollard,Suresh Ramani,Julio Saez-Rodriguez,Yasin Senbabaoglu,Elana Simon,Srinivasan Sivanandan,Gustavo Stolovitzky,Marc Valer,Bo Wang,Xikun Zhang,James Zou,Katrina Kalantar*

Main category: cs.LG

TL;DR: 论文探讨了人工智能在生物学中的应用挑战，提出了跨领域标准化基准的建议，以促进稳健、可信赖的模型开发。


<details>
  <summary>Details</summary>
Motivation: 缺乏跨领域的标准化基准限制了构建稳健、可信赖的AI模型的能力，阻碍了生物学研究的进展。

Method: 通过召集机器学习和计算生物学专家，分析数据异质性、噪声、可重复性等瓶颈，并提出改进建议。

Result: 提出了高质量数据管理、标准化工具、全面评估指标和开放协作平台的建议。

Conclusion: 标准化基准将推动AI驱动的虚拟细胞研究，促进新发现和对细胞系统的深入理解。

Abstract: Artificial intelligence holds immense promise for transforming biology, yet a
lack of standardized, cross domain, benchmarks undermines our ability to build
robust, trustworthy models. Here, we present insights from a recent workshop
that convened machine learning and computational biology experts across
imaging, transcriptomics, proteomics, and genomics to tackle this gap. We
identify major technical and systemic bottlenecks such as data heterogeneity
and noise, reproducibility challenges, biases, and the fragmented ecosystem of
publicly available resources and propose a set of recommendations for building
benchmarking frameworks that can efficiently compare ML models of biological
systems across tasks and data modalities. By promoting high quality data
curation, standardized tooling, comprehensive evaluation metrics, and open,
collaborative platforms, we aim to accelerate the development of robust
benchmarks for AI driven Virtual Cells. These benchmarks are crucial for
ensuring rigor, reproducibility, and biological relevance, and will ultimately
advance the field toward integrated models that drive new discoveries,
therapeutic insights, and a deeper understanding of cellular systems.

</details>


### [428] [On the Performance of Differentially Private Optimization with Heavy-Tail Class Imbalance](https://arxiv.org/abs/2507.10536)
*Qiaoyue Tang,Alain Zhiyanov,Mathias Lécuyer*

Main category: cs.LG

TL;DR: 论文分析了在重尾类别不平衡分布下，常见隐私学习优化算法的行为，发现DP-GD在低频类别学习中表现不佳，而DP-AdamBC通过消除DP偏置能有效提升性能。


<details>
  <summary>Details</summary>
Motivation: 研究隐私学习算法在重尾类别不平衡分布下的优化行为，以解决低频类别学习效果差的问题。

Method: 通过理论模型和实验（控制实验和真实数据）比较DP-GD和DP-AdamBC的性能。

Result: DP-AdamBC在低频类别学习中表现更优，训练准确率分别提升约8%和5%。

Conclusion: DP-AdamBC能有效缓解重尾类别不平衡带来的优化问题，提升模型性能。

Abstract: In this work, we analyze the optimization behaviour of common private
learning optimization algorithms under heavy-tail class imbalanced
distribution. We show that, in a stylized model, optimizing with Gradient
Descent with differential privacy (DP-GD) suffers when learning low-frequency
classes, whereas optimization algorithms that estimate second-order information
do not. In particular, DP-AdamBC that removes the DP bias from estimating loss
curvature is a crucial component to avoid the ill-condition caused by
heavy-tail class imbalance, and empirically fits the data better with
$\approx8\%$ and $\approx5\%$ increase in training accuracy when learning the
least frequent classes on both controlled experiments and real data
respectively.

</details>


### [429] [Graph World Model](https://arxiv.org/abs/2507.10539)
*Tao Feng,Yexin Wu,Guanyu Lin,Jiaxuan You*

Main category: cs.LG

TL;DR: 论文提出了Graph World Model (GWM)，一种支持非结构化和图结构数据的通用世界模型，通过多模态信息处理和多任务表示，优于领域特定基线。


<details>
  <summary>Details</summary>
Motivation: 现有世界模型主要关注非结构化数据，无法利用普遍存在的图结构数据；而图基础模型局限于图学习任务，无法扩展到多模态数据和跨学科任务。

Method: GWM采用通用消息传递算法，支持多模态信息处理，通过文本转换（GWM-T）或模态特定编码器（GWM-E）统一多模态空间，并引入动作节点支持多任务。

Result: 在六个不同领域的任务中，GWM表现优于或匹配领域特定基线，展示了多跳结构的优势及零样本/少样本能力。

Conclusion: GWM是一种通用的世界模型，能够有效处理多模态和结构化数据，支持多样化任务，具有广泛的应用潜力。

Abstract: World models (WMs) demonstrate strong capabilities in prediction, generation,
and planning tasks. Existing WMs primarily focus on unstructured data and
cannot leverage the ubiquitous structured data, often represented as graphs, in
the digital world. While multiple graph foundation models have been proposed,
they focus on graph learning tasks and cannot extend to diverse multi-modal
data and interdisciplinary tasks. To address these challenges, we propose the
Graph World Model (GWM), a world model that supports both unstructured and
graph-structured states with multi-modal information and represents diverse
tasks as actions. The core of a GWM is a generic message-passing algorithm to
aggregate structured information, either over a unified multi-modal token space
by converting multi-modal data into text (GWM-T) or a unified multi-modal
embedding space by modality-specific encoders (GWM-E). Notably, GWM introduces
action nodes to support diverse tasks, where action nodes are linked to other
nodes via direct reference or similarity computation. Extensive experiments on
six tasks from diverse domains, including multi-modal generation and matching,
recommendation, graph prediction, multi-agent, retrieval-augmented generation,
and planning and optimization, show that the same GWM outperforms or matches
domain-specific baselines' performance, benefits from multi-hop structures, and
demonstrates strong zero-shot/few-shot capabilities on unseen new tasks. Our
code for GWM is released at https://github.com/ulab-uiuc/GWM.

</details>


### [430] [Fusing LLM Capabilities with Routing Data](https://arxiv.org/abs/2507.10540)
*Tao Feng,Haozhen Zhang,Zijie Lei,Pengrui Han,Mostofa Patwary,Mohammad Shoeybi,Bryan Catanzaro,Jiaxuan You*

Main category: cs.LG

TL;DR: 论文提出FusionBench和FusionFactory，通过多级融合框架优化LLM路由，提升任务性能。


<details>
  <summary>Details</summary>
Motivation: 现有应用多依赖单一LLM，无法充分利用不同模型的优势，导致性能和效率不足。

Method: 提出FusionBench基准和FusionFactory框架，包括查询级、思维级和模型级融合。

Result: FusionFactory在所有14个基准测试中均优于单一LLM，验证了融合框架的有效性。

Conclusion: 系统化的LLM融合能有效利用互补优势，提升整体性能。

Abstract: The rapid advancement of large language models (LLMs) has created a vibrant
ecosystem of diverse architectures, each with unique strengths due to
differences in design, training data, and objectives. However, most
applications still rely on a single backend model, limiting coverage of
capabilities and leading to inefficiencies in performance and token cost when
tackling complex tasks. We highlight an underexploited opportunity: LLM routing
data, produced when hosting platforms route diverse queries to different
models, which can reveal comparative strengths across tasks. To address this,
we propose FusionBench, a comprehensive routing benchmark covering 14 tasks
across five domains with 20 open-source LLMs (8B to 671B parameters), capturing
103M tokens and summarizing reusable thought templates from top models.
Building on this, we introduce FusionFactory, a systematic fusion framework
with three levels: (1) query-level fusion, tailoring routers for each query
using both direct responses and reasoning-augmented outputs; (2) thought-level
fusion, leveraging abstract templates derived from top-performing LLMs' answers
to similar queries; and (3) model-level fusion, transferring capabilities
between models via distillation, using top responses or highest judge scores as
training data. Experiments show FusionFactory consistently outperforms the best
individual LLM across all 14 benchmarks, with optimal fusion configurations
varying by benchmark, demonstrating the value of systematic LLM fusion in
harnessing complementary strengths and improving overall performance.

</details>


### [431] [Disentangling Neural Disjunctive Normal Form Models](https://arxiv.org/abs/2507.10546)
*Kexin Gu Baugh,Vincent Perreault,Matthew Baugh,Luke Dickens,Katsumi Inoue,Alessandra Russo*

Main category: cs.LG

TL;DR: 论文提出了一种新的解耦方法，通过拆分嵌套规则的节点，改善了神经DNF模型的性能退化问题。


<details>
  <summary>Details</summary>
Motivation: 神经DNF模型在符号翻译过程中因阈值化导致性能下降，部分原因是未能解耦网络权重中表示的学习知识。

Method: 提出解耦方法，拆分嵌套规则的节点为独立小节点，以更好地保留模型性能。

Result: 在多种分类任务中验证了方法的有效性，生成的逻辑表示更紧凑且可解释，性能接近翻译前模型。

Conclusion: 解耦方法显著提升了神经DNF模型的性能，同时保持了其可解释性。

Abstract: Neural Disjunctive Normal Form (DNF) based models are powerful and
interpretable approaches to neuro-symbolic learning and have shown promising
results in classification and reinforcement learning settings without prior
knowledge of the tasks. However, their performance is degraded by the
thresholding of the post-training symbolic translation process. We show here
that part of the performance degradation during translation is due to its
failure to disentangle the learned knowledge represented in the form of the
networks' weights. We address this issue by proposing a new disentanglement
method; by splitting nodes that encode nested rules into smaller independent
nodes, we are able to better preserve the models' performance. Through
experiments on binary, multiclass, and multilabel classification tasks
(including those requiring predicate invention), we demonstrate that our
disentanglement method provides compact and interpretable logical
representations for the neural DNF-based models, with performance closer to
that of their pre-translation counterparts. Our code is available at
https://github.com/kittykg/disentangling-ndnf-classification.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [432] [Agent-based visualization of streaming text](https://arxiv.org/abs/2507.08884)
*Jordan Riley Benson,David Crist,Phil Lafleur,Benjamin Watson*

Main category: cs.MA

TL;DR: 该论文提出了一种基于代理的可视化框架，将数据元素映射到代理，并通过代理行为动态生成可视化效果，应用于文本流分析。


<details>
  <summary>Details</summary>
Motivation: 研究旨在通过动态代理行为直观展示数据流中的关键信息，如文本流中的主题聚类。

Method: 代理根据输入的距离矩阵调整位置和外观，最小化显示距离与理想距离的差异，应用于文本流中单词的可视化。

Result: 可视化结果以聚类形式展示文本流中的主要主题，且布局随数据流动态变化保持稳定。

Conclusion: 该框架有效实现了动态数据流的可视化，适用于实时文本分析。

Abstract: We present a visualization infrastructure that maps data elements to agents,
which have behaviors parameterized by those elements. Dynamic visualizations
emerge as the agents change position, alter appearance and respond to one
other. Agents move to minimize the difference between displayed agent-to-agent
distances, and an input matrix of ideal distances. Our current application is
visualization of streaming text. Each agent represents a significant word,
visualizing it by displaying the word itself, centered in a circle sized by the
frequency of word occurrence. We derive the ideal distance matrix from word
cooccurrence, mapping higher co-occurrence to lower distance. To depict
co-occurrence in its textual context, the ratio of intersection to circle area
approximates the ratio of word co-occurrence to frequency. A networked backend
process gathers articles from news feeds, blogs, Digg or Twitter, exploiting
online search APIs to focus on user-chosen topics. Resulting visuals reveal the
primary topics in text streams as clusters, with agent-based layout moving
without instability as data streams change dynamically.

</details>


### [433] [Optimizing Sequential Multi-Step Tasks with Parallel LLM Agents](https://arxiv.org/abs/2507.08944)
*Enhao Zhang,Erkang Zhu,Gagan Bansal,Adam Fourney,Hussein Mozannar,Jack Gerrits*

Main category: cs.MA

TL;DR: M1-Parallel框架通过并行运行多代理团队和异步通信，显著降低了延迟并提高了任务完成率。


<details>
  <summary>Details</summary>
Motivation: 解决基于LLM的多代理系统因多次迭代推理步骤导致的高延迟问题。

Method: 提出M1-Parallel框架，并行运行多代理团队，利用事件驱动通信和异步消息传递。

Result: 实验显示，M1-Parallel在保持准确性的同时实现了2.2倍加速，并提高了任务完成率。

Conclusion: 并行执行计划是优化多代理系统处理高复杂度任务的有效方法。

Abstract: Large language model (LLM)-based multi-agent systems have demonstrated
remarkable promise for tackling complex tasks by breaking them down into
subtasks that are iteratively planned, executed, observed, and refined. Despite
their effectiveness, these systems often incur high latency because real-world
problems frequently demand multiple iterative cycles of reasoning steps. To
address this challenge, we propose M1-Parallel, a framework that concurrently
runs multiple multi-agent teams in parallel to uncover distinct solution paths.
By leveraging an event-driven communication model with asynchronous messaging,
M1-Parallel efficiently capitalizes on the inherent diversity of valid plans to
either reduce end-to-end latency or boost task completion rates. Our
experiments on complex tasks show that M1-Parallel with early termination
achieves up to $2.2\times$ speedup while preserving accuracy, and that
M1-Parallel with aggregation yields higher task completion rates. We further
investigate strategies aimed at encouraging diverse execution plans but observe
no additional performance gains over repeated sampling. Overall, these findings
underscore the potential of parallel plan execution for optimizing multi-agent
systems for real-world, high-complexity reasoning tasks.

</details>


### [434] [How to Train a Leader: Hierarchical Reasoning in Multi-Agent LLMs](https://arxiv.org/abs/2507.08960)
*Andrew Estornell,Jean-Francois Ton,Muhammad Faaiz Taufiq,Hang Li*

Main category: cs.MA

TL;DR: 提出了一种分层多智能体框架MLPO，通过训练单个领导者LLM协调未训练的同伴智能体，提升了推理任务的性能。


<details>
  <summary>Details</summary>
Motivation: 虽然多智能体框架可以利用多个LLM的互补优势提升推理任务性能，但现有方法计算成本高。

Method: 提出MLPO方法，训练单个领导者LLM评估和综合智能体响应，无需额外值网络或显式反馈。

Result: 在BBH、MATH和MMLU任务上，MLPO显著优于单智能体和多智能体基线。

Conclusion: 训练单个灵活的领导者LLM在多智能体系统中高效且有效。

Abstract: Large Language Models (LLMs) have achieved strong performance on a wide range
of complex reasoning tasks, yet further gains are often possible by leveraging
the complementary strengths of multiple models. While multi-agent frameworks
can improve solution quality by leveraging multiple LLMs, existing methods are
often computationally expensive, both at training and inference time. In this
work, we introduce a hierarchical multi-agent framework that addresses these
challenges by training only a single leader LLM to coordinate a team of
untrained peer agents. To this end, we propose Multi-agent guided Leader Policy
\textbf{O}ptimization (MLPO), a novel approach which trains the leader to
evaluate and synthesize agent responses without auxiliary value networks or
explicit agent feedback. Leaders trained with MLPO exhibit improved performance
not only when interacting with the agent team at inference time, but also enjoy
improved performance when deployed in single-agent settings without the team.
Empirical results on Big-Bench Hard (BBH), MATH, and MMLU demonstrate that our
framework achieves substantial performance improvements over both single-agent
and multi-agent baselines. Our results highlight the effectiveness and
efficiency of training a single, flexible leader for collaborative reasoning in
multi-agent LLM systems.

</details>


### [435] [Simulation for All: A Step-by-Step Cookbook for Developing Human-Centered Multi-Agent Transportation Simulators](https://arxiv.org/abs/2507.09367)
*Shiva Azimi,Arash Tavakoli*

Main category: cs.MA

TL;DR: 本文提出了一种多智能体仿真平台，支持实时、以人为中心的研究，涵盖多种道路使用者，并提供开源脚本。


<details>
  <summary>Details</summary>
Motivation: 现有仿真工具存在局限性，如分离不同类型的道路使用者、依赖脚本行为、忽视公共交通用户，且对非技术用户不友好。

Method: 采用高保真沉浸式虚拟环境，模块化架构，集成多种硬件设备和传感器（如fNIRS、眼动仪、生物传感器）。

Result: 平台支持跨学科实验，提升对复杂城市环境中多模式移动性的理解。

Conclusion: 该平台降低了高保真交通仿真的门槛，推动了多模式移动性的研究。

Abstract: As cities evolve toward more complex and multimodal transportation systems,
the need for human-centered multi-agent simulation tools has never been more
urgent. Yet most existing platforms remain limited - they often separate
different types of road users, rely on scripted or pre-defined behaviors,
overlook public transit users as active participants, and are rarely designed
with accessibility in mind for non-technical users. To address this gap, this
paper presents the specifications of a multi-agent simulation platform designed
to support real-time, human-centered, and immersive studies of all road users,
accompanied by open-source scripts for replication. Using high-fidelity
immersive virtual environments, our platform enables interaction across public
transit users, pedestrians, cyclists, automated vehicles, and drivers. The
architecture is modular, extensible, and designed for accessibility. The system
integrates hardware-specific modules - including an omnidirectional treadmill,
a seating arrangement, a smart trainer, and an actuated cockpit. Additionally,
the platform collects multimodal physiological, neurological, and behavioral
data through embedded sensing devices such as functional near-infrared
spectroscopy (fNIRS), eye tracking, and wrist-based biosensors. To show the
usability of this system, we present three use cases. Simulation for All aims
to lower the barrier to entry for high-fidelity transportation simulation,
support experimentation across disciplines, and advance our understanding of
multimodal mobility in complex urban environments.

</details>


### [436] [Adaptive Social Learning using Theory of Mind](https://arxiv.org/abs/2507.09409)
*Lance Ying,Ryan Truong,Joshua B. Tenenbaum,Samuel J. Gershman*

Main category: cs.MA

TL;DR: 论文提出了一种理性心智化模型，用于权衡社交学习与非社交学习，并通过实验验证了模型的有效性。


<details>
  <summary>Details</summary>
Motivation: 研究人类如何平衡社交学习与非社交学习，以优化学习效率。

Method: 提出理性心智化模型，通过推理他人目标和行为信息性来估计社交学习的效用，并与非社交学习效用比较。

Result: 模型能定量捕捉人类在社交与非社交学习间的权衡，且两者结合能更高效实现目标。

Conclusion: 理性心智化模型为理解人类学习决策提供了有效框架。

Abstract: Social learning is a powerful mechanism through which agents learn about the
world from others. However, humans don't always choose to observe others, since
social learning can carry time and cognitive resource costs. How do people
balance social and non-social learning? In this paper, we propose a rational
mentalizing model of the decision to engage in social learning. This model
estimates the utility of social learning by reasoning about the other agent's
goal and the informativity of their future actions. It then weighs the utility
of social learning against the utility of self-exploration (non-social
learning). Using a multi-player treasure hunt game, we show that our model can
quantitatively capture human trade-offs between social and non-social learning.
Furthermore, our results indicate that these two components allow agents to
flexibly apply social learning to achieve their goals more efficiently.

</details>


### [437] [TinyTroupe: An LLM-powered Multiagent Persona Simulation Toolkit](https://arxiv.org/abs/2507.09788)
*Paulo Salem,Robert Sim,Christopher Olsen,Prerit Saxena,Rafael Barcelos,Yi Ding*

Main category: cs.MA

TL;DR: TinyTroupe是一个基于大语言模型（LLM）的多智能体系统（MAS）模拟工具包，专注于详细角色定义和行为模拟，填补了现有工具在行为研究和社交模拟中的不足。


<details>
  <summary>Details</summary>
Motivation: 现有MAS工具缺乏细粒度角色定义、实验支持和集成验证等功能，限制了其在行为研究和社会模拟中的应用。

Method: 通过LLM驱动机制实现详细角色定义（如国籍、年龄、性格等）和程序化控制，支持个体或群体行为问题的模拟与解决。

Result: 通过案例（如头脑风暴和市场调研）展示了工具的有效性，并提供了定量和定性评估。

Conclusion: TinyTroupe不仅是一个具体的Python实现，更是一个可推广的概念贡献，适用于其他场景。

Abstract: Recent advances in Large Language Models (LLM) have led to a new class of
autonomous agents, renewing and expanding interest in the area. LLM-powered
Multiagent Systems (MAS) have thus emerged, both for assistive and simulation
purposes, yet tools for realistic human behavior simulation -- with its
distinctive challenges and opportunities -- remain underdeveloped. Existing MAS
libraries and tools lack fine-grained persona specifications, population
sampling facilities, experimentation support, and integrated validation, among
other key capabilities, limiting their utility for behavioral studies, social
simulation, and related applications. To address these deficiencies, in this
work we introduce TinyTroupe, a simulation toolkit enabling detailed persona
definitions (e.g., nationality, age, occupation, personality, beliefs,
behaviors) and programmatic control via numerous LLM-driven mechanisms. This
allows for the concise formulation of behavioral problems of practical
interest, either at the individual or group level, and provides effective means
for their solution. TinyTroupe's components are presented using representative
working examples, such as brainstorming and market research sessions, thereby
simultaneously clarifying their purpose and demonstrating their usefulness.
Quantitative and qualitative evaluations of selected aspects are also provided,
highlighting possibilities, limitations, and trade-offs. The approach, though
realized as a specific Python implementation, is meant as a novel conceptual
contribution, which can be partially or fully incorporated in other contexts.
The library is available as open source at
https://github.com/microsoft/tinytroupe.

</details>


### [438] [Large Population Models](https://arxiv.org/abs/2507.09901)
*Ayush Chopra*

Main category: cs.MA

TL;DR: 大型人口模型（LPMs）通过模拟数百万自主代理的行为和互动，帮助理解复杂社会系统，提供政策测试和社会创新的虚拟试验场。


<details>
  <summary>Details</summary>
Motivation: 解决从疫情应对到气候变化等社会挑战，需要理解大规模自主代理的集体行为。

Method: LPMs结合高效计算、数据驱动的数学框架和隐私保护协议，模拟大规模代理行为及其系统级结果。

Result: LPMs能够揭示群体现象，为政策和创新提供虚拟测试环境。

Conclusion: LPMs为AI研究提供了新路径，通过模拟数字社会，补充个体代理研究的不足。

Abstract: Many of society's most pressing challenges, from pandemic response to supply
chain disruptions to climate adaptation, emerge from the collective behavior of
millions of autonomous agents making decisions over time. Large Population
Models (LPMs) offer an approach to understand these complex systems by
simulating entire populations with realistic behaviors and interactions at
unprecedented scale. LPMs extend traditional modeling approaches through three
key innovations: computational methods that efficiently simulate millions of
agents simultaneously, mathematical frameworks that learn from diverse
real-world data streams, and privacy-preserving communication protocols that
bridge virtual and physical environments. This allows researchers to observe
how agent behavior aggregates into system-level outcomes and test interventions
before real-world implementation. While current AI advances primarily focus on
creating "digital humans" with sophisticated individual capabilities, LPMs
develop "digital societies" where the richness of interactions reveals emergent
phenomena. By bridging individual agent behavior and population-scale dynamics,
LPMs offer a complementary path in AI research illuminating collective
intelligence and providing testing grounds for policies and social innovations
before real-world deployment. We discuss the technical foundations and some
open problems here. LPMs are implemented by the AgentTorch framework
(github.com/AgentTorch/AgentTorch)

</details>


### [439] [AnalogTester: A Large Language Model-Based Framework for Automatic Testbench Generation in Analog Circuit Design](https://arxiv.org/abs/2507.09965)
*Weiyu Chen,Chengjie Liu,Wenhao Huang,Jinyang Lyu,Mingqian Yang,Yuan Du,Li Du,Jun Yang*

Main category: cs.MA

TL;DR: AnalogTester利用LLM实现模拟电路测试台的自动化生成，解决了手动构建的瓶颈问题。


<details>
  <summary>Details</summary>
Motivation: 模拟电路测试台的手动构建耗时且难以满足自动化需求，阻碍了全自动设计流程的实现。

Method: 通过LLM驱动的流程：领域知识整合、论文信息提取、仿真方案合成和测试台代码生成。

Result: 成功为三种基本模拟电路类型生成测试台，并建立了LLM专业化的训练数据集。

Conclusion: AnalogTester为模拟电路设计自动化提供了可扩展的解决方案，并推动了LLM在该领域的应用。

Abstract: Recent advancements have demonstrated the significant potential of large
language models (LLMs) in analog circuit design. Nevertheless, testbench
construction for analog circuits remains manual, creating a critical bottleneck
in achieving fully automated design processes. Particularly when replicating
circuit designs from academic papers, manual Testbench construction demands
time-intensive implementation and frequent adjustments, which fails to address
the dynamic diversity and flexibility requirements for automation. AnalogTester
tackles automated analog design challenges through an LLM-powered pipeline: a)
domain-knowledge integration, b) paper information extraction, c) simulation
scheme synthesis, and d) testbench code generation with Tsinghua Electronic
Design (TED). AnalogTester has demonstrated automated Testbench generation
capabilities for three fundamental analog circuit types: operational amplifiers
(op-amps), bandgap references (BGRs), and low-dropout regulators (LDOs), while
maintaining a scalable framework for adaptation to broader circuit topologies.
Furthermore, AnalogTester can generate circuit knowledge data and TED code
corpus, establishing fundamental training datasets for LLM specialization in
analog circuit design automation.

</details>


### [440] [Multi-Robot Cooperative Herding through Backstepping Control Barrier Functions](https://arxiv.org/abs/2507.10249)
*Kang Li,Ming Li,Wenkang Ji,Zhiyong Sun,Shiyu Zhao*

Main category: cs.MA

TL;DR: 提出了一种基于反步控制屏障函数（CBFs）的新型协作驱赶策略，用于协调多个驱赶者将逃逸者安全驱赶至目标区域。


<details>
  <summary>Details</summary>
Motivation: 解决异质群体（驱赶者和逃逸者）中逃逸者行为仅能通过驱赶者运动间接影响的问题，确保系统在欠驱动条件下的安全性和目标达成。

Method: 构建分离的CBFs实现目标达成和碰撞避免，通过反步法设计分层屏障函数的控制输入，避免高阶系统求导问题。

Result: 提出了一种基于反步CBFs的协作驱赶策略，并通过仿真和实验验证了其有效性和安全性。

Conclusion: 该方法在欠驱动系统中实现了安全高效的协作驱赶，并支持集中式和分散式实现，具有较高的灵活性和适用性。

Abstract: We propose a novel cooperative herding strategy through backstepping control
barrier functions (CBFs), which coordinates multiple herders to herd a group of
evaders safely towards a designated goal region. For the herding system with
heterogeneous groups involving herders and evaders, the behavior of the evaders
can only be influenced indirectly by the herders' motion, especially when the
evaders follow an inverse dynamics model and respond solely to repulsive
interactions from the herders. This indirect interaction mechanism inherently
renders the overall system underactuated. To address this issue, we first
construct separate CBFs for the dual objectives of goal reaching and collision
avoidance, which ensure both herding completion and safety guarantees. Then, we
reformulate the underactuated herding dynamics into a control-affine structure
and employ a backstepping approach to recursively design control inputs for the
hierarchical barrier functions, avoiding taking derivatives of the higher-order
system. Finally, we present a cooperative herding strategy based on
backstepping CBFs that allow herders to safely herd multiple evaders into the
goal region. In addition, centralized and decentralized implementations of the
proposed algorithm are developed, further enhancing its flexibility and
applicability. Extensive simulations and real-world experiments validate the
effectiveness and safety of the proposed strategy in multi-robot herding.

</details>


### [441] [ToMacVF : Temporal Macro-action Value Factorization for Asynchronous Multi-Agent Reinforcement Learning](https://arxiv.org/abs/2507.10251)
*Wenjing Zhang,Wei Zhang*

Main category: cs.MA

TL;DR: 提出ToMacVF方法，通过精细时间信用分配和完整宏动作执行信息收集，提升异步多智能体强化学习性能。


<details>
  <summary>Details</summary>
Motivation: 现有异步MARL方法在宏动作端点采样数据导致不完整和不准确的执行过程表示及信用分配问题。

Method: 提出ToMacVF框架，设计Mac-SJERT缓冲区收集完整宏动作信息，并基于To-Mac-IGM原则实现精细信用分配。

Result: ToMacVF在多种异步多智能体实验场景中表现最优，适应性和鲁棒性强。

Conclusion: ToMacVF通过时间信用分配和完整信息收集，显著提升异步MARL性能。

Abstract: Existing asynchronous MARL methods based on MacDec-POMDP typically construct
training trajectory buffers by simply sampling limited and biased data at the
endpoints of macro-actions, and directly apply conventional MARL methods on the
buffers. As a result, these methods lead to an incomplete and inaccurate
representation of the macro-action execution process, along with unsuitable
credit assignments. To solve these problems, the Temporal Macro-action Value
Factorization (ToMacVF) is proposed to achieve fine-grained temporal credit
assignment for macro-action contributions. A centralized training buffer,
called Macro-action Segmented Joint Experience Replay Trajectory (Mac-SJERT),
is designed to incorporate with ToMacVF to collect accurate and complete
macro-action execution information, supporting a more comprehensive and precise
representation of the macro-action process. To ensure principled and
fine-grained asynchronous value factorization, the consistency requirement
between joint and individual macro-action selection called Temporal
Macro-action based IGM (To-Mac-IGM) is formalized, proving that it generalizes
the synchronous cases. Based on To-Mac-IGM, a modularized ToMacVF architecture,
which satisfies CTDE principle, is designed to conveniently integrate previous
value factorization methods. Next, the ToMacVF algorithm is devised as an
implementation of the ToMacVF architecture. Experimental results demonstrate
that, compared to asynchronous baselines, our ToMacVF algorithm not only
achieves optimal performance but also exhibits strong adaptability and
robustness across various asynchronous multi-agent experimental scenarios.

</details>


### [442] [Toolsuite for Implementing Multiagent Systems Based on Communication Protocols](https://arxiv.org/abs/2507.10324)
*Amit K. Chopra,Samuel H. Christie V,Munindar P. Singh*

Main category: cs.MA

TL;DR: 本文介绍了交互导向编程（IOP）及其相关工具，用于多智能体系统的开发和验证。


<details>
  <summary>Details</summary>
Motivation: 通过灵活的交互协议建模角色间的交互，简化多智能体系统的开发。

Method: 开发了一套软件工具，包括协议验证工具和中间件，以支持IOP的应用。

Result: 提供了高效的协议验证（如活性和安全性）和简化的智能体实现工具。

Conclusion: IOP及其工具套件为多智能体系统的开发提供了实用支持。

Abstract: Interaction-Oriented Programming (IOP) is an approach to building a
multiagent system by modeling the interactions between its roles via a flexible
interaction protocol and implementing agents to realize the interactions of the
roles they play in the protocol.
  In recent years, we have developed an extensive suite of software that
enables multiagent system developers to apply IOP. These include tools for
efficiently verifying protocols for properties such as liveness and safety and
middleware that simplifies the implementation of agents. This paper presents
some of that software suite.

</details>


<div id='cs.MM'></div>

# cs.MM [[Back]](#toc)

### [443] [KEN: Knowledge Augmentation and Emotion Guidance Network for Multimodal Fake News Detection](https://arxiv.org/abs/2507.09647)
*Peican Zhu,Yubo Jing,Le Cheng,Keke Tang,Yangming Guo*

Main category: cs.MM

TL;DR: 提出了一种名为KEN的新方法，通过知识增强和情感引导网络，结合图像和文本信息，有效检测多模态假新闻。


<details>
  <summary>Details</summary>
Motivation: 社交媒体上虚假信息的泛滥使得多模态假新闻检测成为研究重点，但现有方法对图像语义理解不足，且未能针对不同情感类型的新闻进行区分处理。

Method: 利用LVLM的强大语义理解能力生成图像描述，并通过检索证据补充文本信息；同时通过平衡学习对不同情感类型的新闻进行细粒度建模。

Result: 在两个真实数据集上的实验证明了KEN的优越性。

Conclusion: KEN通过结合知识增强和情感引导，显著提升了多模态假新闻检测的性能。

Abstract: In recent years, the rampant spread of misinformation on social media has
made accurate detection of multimodal fake news a critical research focus.
However, previous research has not adequately understood the semantics of
images, and models struggle to discern news authenticity with limited textual
information. Meanwhile, treating all emotional types of news uniformly without
tailored approaches further leads to performance degradation. Therefore, we
propose a novel Knowledge Augmentation and Emotion Guidance Network (KEN). On
the one hand, we effectively leverage LVLM's powerful semantic understanding
and extensive world knowledge. For images, the generated captions provide a
comprehensive understanding of image content and scenes, while for text, the
retrieved evidence helps break the information silos caused by the closed and
limited text and context. On the other hand, we consider inter-class
differences between different emotional types of news through balanced
learning, achieving fine-grained modeling of the relationship between emotional
types and authenticity. Extensive experiments on two real-world datasets
demonstrate the superiority of our KEN.

</details>


### [444] [ESG-Net: Event-Aware Semantic Guided Network for Dense Audio-Visual Event Localization](https://arxiv.org/abs/2507.09945)
*Huilai Li,Yonghao Dang,Ying Xing,Yiming Wang,Jianqin Yin*

Main category: cs.MM

TL;DR: 论文提出了一种名为ESG-Net的方法，通过多阶段语义引导和多事件关系建模，解决了密集视听事件定位中的模态语义鸿沟和事件相关性不足问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法在中间层缺乏跨模态语义桥接，导致模态语义鸿沟，且未考虑事件间的相关性，限制了复杂场景下的并发事件推断能力。

Method: 提出ESG-Net，包含早期语义交互模块（ESI）和依赖专家混合模块（MoDE），分别用于多阶段语义引导和自适应事件依赖提取。

Result: 实验证明，该方法显著优于现有技术，同时大幅减少参数和计算量。

Conclusion: ESG-Net通过多阶段语义引导和事件关系建模，有效提升了密集视听事件定位的性能。

Abstract: Dense audio-visual event localization (DAVE) aims to identify event
categories and locate the temporal boundaries in untrimmed videos. Most studies
only employ event-related semantic constraints on the final outputs, lacking
cross-modal semantic bridging in intermediate layers. This causes modality
semantic gap for further fusion, making it difficult to distinguish between
event-related content and irrelevant background content. Moreover, they rarely
consider the correlations between events, which limits the model to infer
concurrent events among complex scenarios. In this paper, we incorporate
multi-stage semantic guidance and multi-event relationship modeling, which
respectively enable hierarchical semantic understanding of audio-visual events
and adaptive extraction of event dependencies, thereby better focusing on
event-related information. Specifically, our eventaware semantic guided network
(ESG-Net) includes a early semantics interaction (ESI) module and a mixture of
dependency experts (MoDE) module. ESI applys multi-stage semantic guidance to
explicitly constrain the model in learning semantic information through
multi-modal early fusion and several classification loss functions, ensuring
hierarchical understanding of event-related content. MoDE promotes the
extraction of multi-event dependencies through multiple serial mixture of
experts with adaptive weight allocation. Extensive experiments demonstrate that
our method significantly surpasses the state-of-the-art methods, while greatly
reducing parameters and computational load. Our code will be released on
https://github.com/uchiha99999/ESG-Net.

</details>


### [445] [LayLens: Improving Deepfake Understanding through Simplified Explanations](https://arxiv.org/abs/2507.10066)
*Abhijeet Narang,Parul Gupta,Liuyijia Su,Abhinav Dhall*

Main category: cs.MM

TL;DR: LayLens是一个工具，旨在通过三阶段流程（检测、简化解释、视觉重建）帮助不同教育背景的用户更容易理解深度伪造技术。用户研究表明，简化解释显著提高了清晰度并降低了认知负担。


<details>
  <summary>Details</summary>
Motivation: 现有深度伪造检测工具常使用技术术语，导致普通用户难以理解。LayLens旨在填补模型推理与人类理解之间的鸿沟。

Method: 采用三阶段流程：1）可解释的深度伪造检测；2）使用视觉语言模型简化技术解释；3）通过引导图像编辑进行视觉重建。

Result: 用户研究表明，简化解释显著提高了清晰度，降低了认知负担，并增强了用户识别深度伪造的信心。

Conclusion: LayLens为透明、可信且以用户为中心的深度伪造取证迈出了一步。

Abstract: This demonstration paper presents $\mathbf{LayLens}$, a tool aimed to make
deepfake understanding easier for users of all educational backgrounds. While
prior works often rely on outputs containing technical jargon, LayLens bridges
the gap between model reasoning and human understanding through a three-stage
pipeline: (1) explainable deepfake detection using a state-of-the-art forgery
localization model, (2) natural language simplification of technical
explanations using a vision-language model, and (3) visual reconstruction of a
plausible original image via guided image editing. The interface presents both
technical and layperson-friendly explanations in addition to a side-by-side
comparison of the uploaded and reconstructed images. A user study with 15
participants shows that simplified explanations significantly improve clarity
and reduce cognitive load, with most users expressing increased confidence in
identifying deepfakes. LayLens offers a step toward transparent, trustworthy,
and user-centric deepfake forensics.

</details>


### [446] [DualDub: Video-to-Soundtrack Generation via Joint Speech and Background Audio Synthesis](https://arxiv.org/abs/2507.10109)
*Wenjie Tian,Xinfa Zhu,Haohe Liu,Zhixian Zhao,Zihao Chen,Chaofan Ding,Xinhan Di,Junjie Zheng,Lei Xie*

Main category: cs.MM

TL;DR: 论文提出了一种新的任务——视频到音轨（V2ST）生成，旨在统一生成背景音频和语音。通过DualDub框架和DualBench基准，实现了高质量和同步的音轨生成。


<details>
  <summary>Details</summary>
Motivation: 现有视频到音频（V2A）模型忽略了语音部分，而语音是视频音轨的重要组成部分。因此，需要一种统一框架来同时生成背景音频和语音。

Method: 提出DualDub框架，基于多模态语言模型，包含多模态编码器、跨模态对齐器和双解码头。采用因果和非因果注意力机制提升同步性和声学和谐性，并设计了课程学习策略应对数据稀缺。

Result: 实验表明，DualDub在V2ST任务中表现优异，生成了高质量且同步的音轨。

Conclusion: DualDub为视频音轨生成提供了统一解决方案，并通过DualBench基准验证了其有效性。

Abstract: While recent video-to-audio (V2A) models can generate realistic background
audio from visual input, they largely overlook speech, an essential part of
many video soundtracks. This paper proposes a new task, video-to-soundtrack
(V2ST) generation, which aims to jointly produce synchronized background audio
and speech within a unified framework. To tackle V2ST, we introduce DualDub, a
unified framework built on a multimodal language model that integrates a
multimodal encoder, a cross-modal aligner, and dual decoding heads for
simultaneous background audio and speech generation. Specifically, our proposed
cross-modal aligner employs causal and non-causal attention mechanisms to
improve synchronization and acoustic harmony. Besides, to handle data scarcity,
we design a curriculum learning strategy that progressively builds the
multimodal capability. Finally, we introduce DualBench, the first benchmark for
V2ST evaluation with a carefully curated test set and comprehensive metrics.
Experimental results demonstrate that DualDub achieves state-of-the-art
performance, generating high-quality and well-synchronized soundtracks with
both speech and background audio.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [447] [OTAS: Open-vocabulary Token Alignment for Outdoor Segmentation](https://arxiv.org/abs/2507.08851)
*Simon Schwaiger,Stefan Thalhammer,Wilfried Wöber,Gerald Steinbauer-Wagner*

Main category: cs.RO

TL;DR: OTAS是一种开放词汇标记对齐方法，用于户外分割，通过直接提取预训练视觉模型的输出标记语义结构，实现零样本开放词汇分割。


<details>
  <summary>Details</summary>
Motivation: 在非结构化户外环境中，当前基于对象分割的方法因语义模糊和边界不清晰而失效，需要更有效的开放词汇分割方法。

Method: OTAS通过聚类单视图和多视图中的语义相似结构，并将其与语言对齐，构建几何一致的特征场，支持开放词汇查询。

Result: OTAS在零样本条件下运行，速度达17 fps，在2D分割上略优于微调方法，3D分割上显著优于开放词汇方法（最高151% IoU提升）。

Conclusion: OTAS适用于机器人应用，代码将公开。

Abstract: Understanding open-world semantics is critical for robotic planning and
control, particularly in unstructured outdoor environments. Current
vision-language mapping approaches rely on object-centric segmentation priors,
which often fail outdoors due to semantic ambiguities and indistinct semantic
class boundaries. We propose OTAS - an Open-vocabulary Token Alignment method
for Outdoor Segmentation. OTAS overcomes the limitations of open-vocabulary
segmentation models by extracting semantic structure directly from the output
tokens of pretrained vision models. By clustering semantically similar
structures across single and multiple views and grounding them in language,
OTAS reconstructs a geometrically consistent feature field that supports
open-vocabulary segmentation queries. Our method operates zero-shot, without
scene-specific fine-tuning, and runs at up to ~17 fps. OTAS provides a minor
IoU improvement over fine-tuned and open-vocabulary 2D segmentation methods on
the Off-Road Freespace Detection dataset. Our model achieves up to a 151% IoU
improvement over open-vocabulary mapping methods in 3D segmentation on
TartanAir. Real-world reconstructions demonstrate OTAS' applicability to
robotic applications. The code and ROS node will be made publicly available
upon paper acceptance.

</details>


### [448] [AirScape: An Aerial Generative World Model with Motion Controllability](https://arxiv.org/abs/2507.08885)
*Baining Zhao,Rongze Tang,Mingyuan Jia,Ziyou Wang,Fanghang Man,Xin Zhang,Yu Shang,Weichen Zhang,Chen Gao,Wei Wu,Xin Wang,Xinlei Chen,Yong Li*

Main category: cs.RO

TL;DR: AirScape是世界首个为六自由度空中智能体设计的世界模型，通过视觉输入和运动意图预测未来观测序列。


<details>
  <summary>Details</summary>
Motivation: 解决机器人在三维空间中预测自身运动意图结果的基础问题，探索更通用的空间想象能力。

Method: 构建包含11k视频-意图对的数据集，采用两阶段训练计划，将基础模型训练为可控的世界模型。

Result: 开发了AirScape模型，能够基于视觉输入和运动意图预测未来观测序列。

Conclusion: AirScape为空中智能体提供了可控的世界模型，推动了空间想象能力的研究。

Abstract: How to enable robots to predict the outcomes of their own motion intentions
in three-dimensional space has been a fundamental problem in embodied
intelligence. To explore more general spatial imagination capabilities, here we
present AirScape, the first world model designed for six-degree-of-freedom
aerial agents. AirScape predicts future observation sequences based on current
visual inputs and motion intentions. Specifically, we construct an dataset for
aerial world model training and testing, which consists of 11k video-intention
pairs. This dataset includes first-person-view videos capturing diverse drone
actions across a wide range of scenarios, with over 1,000 hours spent
annotating the corresponding motion intentions. Then we develop a two-phase
training schedule to train a foundation model -- initially devoid of embodied
spatial knowledge -- into a world model that is controllable by motion
intentions and adheres to physical spatio-temporal constraints.

</details>


### [449] [End-to-End Generation of City-Scale Vectorized Maps by Crowdsourced Vehicles](https://arxiv.org/abs/2507.08901)
*Zebang Feng,Miao Fan,Bao Liu,Shengtong Xu,Haoyi Xiong*

Main category: cs.RO

TL;DR: EGC-VMAP是一个端到端框架，通过众包车辆数据生成高精度城市级矢量化地图，显著降低成本和提升准确性。


<details>
  <summary>Details</summary>
Motivation: 传统LiDAR制图成本高、速度慢，单车感知方法在恶劣条件下精度和鲁棒性不足，需要更高效、准确的解决方案。

Method: 采用Trip-Aware Transformer架构，直接融合多车、多时态地图元素，结合分层匹配和多目标损失函数。

Result: 在大规模多城市数据集上验证，性能优于单车基线，减少90%人工标注成本。

Conclusion: EGC-VMAP为城市级地图提供了一种可扩展、经济高效的解决方案。

Abstract: High-precision vectorized maps are indispensable for autonomous driving, yet
traditional LiDAR-based creation is costly and slow, while single-vehicle
perception methods lack accuracy and robustness, particularly in adverse
conditions. This paper introduces EGC-VMAP, an end-to-end framework that
overcomes these limitations by generating accurate, city-scale vectorized maps
through the aggregation of data from crowdsourced vehicles. Unlike prior
approaches, EGC-VMAP directly fuses multi-vehicle, multi-temporal map elements
perceived onboard vehicles using a novel Trip-Aware Transformer architecture
within a unified learning process. Combined with hierarchical matching for
efficient training and a multi-objective loss, our method significantly
enhances map accuracy and structural robustness compared to single-vehicle
baselines. Validated on a large-scale, multi-city real-world dataset, EGC-VMAP
demonstrates superior performance, enabling a scalable, cost-effective solution
for city-wide mapping with a reported 90\% reduction in manual annotation
costs.

</details>


### [450] [Multimodal HD Mapping for Intersections by Intelligent Roadside Units](https://arxiv.org/abs/2507.08903)
*Zhongzhang Chen,Miao Fan,Shengtong Xu,Mengmeng Yang,Kun Jiang,Xiangzeng Liu,Haoyi Xiong*

Main category: cs.RO

TL;DR: 本文提出了一种基于摄像头-LiDAR融合的高清语义地图生成框架，利用智能路边单元（IRU）解决复杂交叉路口的遮挡和视角限制问题，并发布了RS-seq数据集。


<details>
  <summary>Details</summary>
Motivation: 传统车载方法在复杂交叉路口的高清语义地图生成中面临遮挡和视角限制的挑战，因此需要一种更高效的多模态融合方法。

Method: 采用两阶段融合框架，结合摄像头的高分辨率纹理和LiDAR的精确几何数据，通过模态特定特征提取和跨模态语义集成实现。

Result: 在RS-seq数据集上的定量评估显示，多模态方法比单模态方法显著提升，语义分割的mIoU分别比图像和点云单模态方法提高了4%和18%。

Conclusion: 本研究为基于IRU的高清语义地图生成提供了基准方法，并为基础设施辅助的自动驾驶系统研究提供了有价值的数据集。

Abstract: High-definition (HD) semantic mapping of complex intersections poses
significant challenges for traditional vehicle-based approaches due to
occlusions and limited perspectives. This paper introduces a novel camera-LiDAR
fusion framework that leverages elevated intelligent roadside units (IRUs).
Additionally, we present RS-seq, a comprehensive dataset developed through the
systematic enhancement and annotation of the V2X-Seq dataset. RS-seq includes
precisely labelled camera imagery and LiDAR point clouds collected from
roadside installations, along with vectorized maps for seven intersections
annotated with detailed features such as lane dividers, pedestrian crossings,
and stop lines. This dataset facilitates the systematic investigation of
cross-modal complementarity for HD map generation using IRU data. The proposed
fusion framework employs a two-stage process that integrates modality-specific
feature extraction and cross-modal semantic integration, capitalizing on camera
high-resolution texture and precise geometric data from LiDAR. Quantitative
evaluations using the RS-seq dataset demonstrate that our multimodal approach
consistently surpasses unimodal methods. Specifically, compared to unimodal
baselines evaluated on the RS-seq dataset, the multimodal approach improves the
mean Intersection-over-Union (mIoU) for semantic segmentation by 4\% over the
image-only results and 18\% over the point cloud-only results. This study
establishes a baseline methodology for IRU-based HD semantic mapping and
provides a valuable dataset for future research in infrastructure-assisted
autonomous driving systems.

</details>


### [451] [Towards Human-level Dexterity via Robot Learning](https://arxiv.org/abs/2507.09117)
*Gagan Khandate*

Main category: cs.RO

TL;DR: 论文探讨了如何通过强化学习和模仿学习克服多指机器人操作的局限性，以实现更高水平的灵巧性。


<details>
  <summary>Details</summary>
Motivation: 人类灵巧性是物理智能和高级认知技能的体现，但机器人实现类似能力面临根本性挑战。

Method: 采用结构化探索和基于采样的规划强化学习，并结合视觉触觉人类示范的模仿学习技术。

Result: 开发了有效的强化学习框架，显著提升了多指机器人操作的灵巧性。

Conclusion: 通过直接解决计算感觉运动学习的局限性，论文为机器人灵巧操作提供了新方法。

Abstract: Dexterous intelligence -- the ability to perform complex interactions with
multi-fingered hands -- is a pinnacle of human physical intelligence and
emergent higher-order cognitive skills. However, contrary to Moravec's paradox,
dexterous intelligence in humans appears simple only superficially. Many
million years were spent co-evolving the human brain and hands including rich
tactile sensing. Achieving human-level dexterity with robotic hands has long
been a fundamental goal in robotics and represents a critical milestone toward
general embodied intelligence. In this pursuit, computational sensorimotor
learning has made significant progress, enabling feats such as arbitrary
in-hand object reorientation. However, we observe that achieving higher levels
of dexterity requires overcoming very fundamental limitations of computational
sensorimotor learning.
  I develop robot learning methods for highly dexterous multi-fingered
manipulation by directly addressing these limitations at their root cause.
Chiefly, through key studies, this disseration progressively builds an
effective framework for reinforcement learning of dexterous multi-fingered
manipulation skills. These methods adopt structured exploration, effectively
overcoming the limitations of random exploration in reinforcement learning. The
insights gained culminate in a highly effective reinforcement learning that
incorporates sampling-based planning for direct exploration. Additionally, this
thesis explores a new paradigm of using visuo-tactile human demonstrations for
dexterity, introducing corresponding imitation learning techniques.

</details>


### [452] [Online 3D Bin Packing with Fast Stability Validation and Stable Rearrangement Planning](https://arxiv.org/abs/2507.09123)
*Ziyan Gao,Lijun Wang,Yuntao Kong,Nak Young Chong*

Main category: cs.RO

TL;DR: 提出了一种结合包装策略、结构稳定性验证和启发式规划的新框架，解决了在线装箱问题中结构稳定性和安全重新配置的不足。


<details>
  <summary>Details</summary>
Motivation: 现有深度强化学习方法在在线装箱问题中虽能提高体积利用率，但无法确保结构稳定性且缺乏安全重新配置机制。

Method: 引入负载可承受凸多边形（LBCP）概念进行稳定性验证，并提出稳定重新规划（SRP）模块以安全重新配置。

Result: 在标准OBPP基准测试中，LBCP验证高效且通用，SRP在节省努力的重新配置计划中表现优越。

Conclusion: 该方法为工业和物流中的自动化装箱提供了鲁棒且实用的解决方案。

Abstract: The Online Bin Packing Problem (OBPP) is a sequential decision-making task in
which each item must be placed immediately upon arrival, with no knowledge of
future arrivals. Although recent deep-reinforcement-learning methods achieve
superior volume utilization compared with classical heuristics, the learned
policies cannot ensure the structural stability of the bin and lack mechanisms
for safely reconfiguring the bin when a new item cannot be placed directly. In
this work, we propose a novel framework that integrates packing policy with
structural stability validation and heuristic planning to overcome these
limitations. Specifically, we introduce the concept of Load Bearable Convex
Polygon (LBCP), which provides a computationally efficient way to identify
stable loading positions that guarantee no bin collapse. Additionally, we
present Stable Rearrangement Planning (SRP), a module that rearranges existing
items to accommodate new ones while maintaining overall stability. Extensive
experiments on standard OBPP benchmarks demonstrate the efficiency and
generalizability of our LBCP-based stability validation, as well as the
superiority of SRP in finding the effort-saving rearrangement plans. Our method
offers a robust and practical solution for automated packing in real-world
industrial and logistics applications.

</details>


### [453] [Tactile-VLA: Unlocking Vision-Language-Action Model's Physical Knowledge for Tactile Generalization](https://arxiv.org/abs/2507.09160)
*Jialei Huang,Shuo Wang,Fanqi Lin,Yihang Hu,Chuan Wen,Yang Gao*

Main category: cs.RO

TL;DR: Tactile-VLA框架融合视觉、语言、动作和触觉感知，通过混合力-位置控制器和触觉反馈推理模块，实现精确的物理交互和自适应策略。


<details>
  <summary>Details</summary>
Motivation: 提升Vision-Language-Action (VLA)模型在物理交互中的能力，特别是在需要精细力控制的接触丰富场景中。

Method: 提出Tactile-VLA框架，结合混合力-位置控制器和触觉反馈推理模块，将模型意图转化为精确动作。

Result: 实验证明Tactile-VLA在触觉感知指令跟随、触觉常识利用和自适应触觉推理方面表现优异，且能通过少量演示激活VLM的物理交互知识。

Conclusion: Tactile-VLA成功将VLA模型的隐式知识扩展到物理交互领域，实现了零样本泛化能力。

Abstract: Vision-Language-Action (VLA) models have shown remarkable achievements,
driven by the rich implicit knowledge of their vision-language components.
However, achieving generalist robotic agents demands precise grounding into
physical interactions, especially in contact-rich scenarios where fine-grained
force control is essential. We advance VLAs' implicit knowledge beyond
identifying what to do, towards guiding how to physically interact with real
world. This paper introduces Tactile-VLA, a novel framework that deeply fuses
vision, language, action, and tactile sensing. This framework incorporates a
hybrid position-force controller to translate the model's intentions into
precise physical actions and a reasoning module that allows the robot to adapt
its strategy based on tactile feedback. Experiments demonstrate Tactile-VLA's
effectiveness and generalizability in three key aspects: (1) enabling
tactile-aware instruction following, (2) utilizing tactile-relevant
commonsense, and (3) facilitating adaptive tactile-involved reasoning. A key
finding is that the VLM's prior knowledge already contains semantic
understanding of physical interaction; by connecting it to the robot's tactile
sensors with only a few demonstrations, we can activate this prior knowledge to
achieve zero-shot generalization in contact-rich tasks.

</details>


### [454] [PRAG: Procedural Action Generator](https://arxiv.org/abs/2507.09167)
*Michal Vavrecka,Radoslav Skoviera,Gabriela Sejnova,Karla Stepanova*

Main category: cs.RO

TL;DR: 提出了一种新颖的方法，用于程序化生成机器人操作任务，通过符号和物理验证确保任务可解。


<details>
  <summary>Details</summary>
Motivation: 解决多步接触丰富的机器人操作任务的生成问题，确保任务逻辑和物理上的可行性。

Method: 输入原子动作、对象和空间谓词，通过符号和物理验证生成可解任务。

Result: 生成了数百万个独特的可解多步任务，适用于机器人训练。

Conclusion: 该方法为机器人任务生成和训练提供了高效且可靠的解决方案。

Abstract: We present a novel approach for the procedural construction of multi-step
contact-rich manipulation tasks in robotics. Our generator takes as input
user-defined sets of atomic actions, objects, and spatial predicates and
outputs solvable tasks of a given length for the selected robotic environment.
The generator produces solvable tasks by constraining all possible
(nonsolvable) combinations by symbolic and physical validation. The symbolic
validation checks each generated sequence for logical and operational
consistency, and also the suitability of object-predicate relations. Physical
validation checks whether tasks can be solved in the selected robotic
environment. Only the tasks that passed both validators are retained. The
output from the generator can be directly interfaced with any existing
framework for training robotic manipulation tasks, or it can be stored as a
dataset of curated robotic tasks with detailed information about each task.
This is beneficial for RL training as there are dense reward functions and
initial and goal states paired with each subgoal. It allows the user to measure
the semantic similarity of all generated tasks. We tested our generator on
sequences of up to 15 actions resulting in millions of unique solvable
multi-step tasks.

</details>


### [455] [DLBAcalib: Robust Extrinsic Calibration for Non-Overlapping LiDARs Based on Dual LBA](https://arxiv.org/abs/2507.09176)
*Han Ye,Yuqiang Jin,Jinyuan Liu,Tao Li,Wen-An Zhang,Minglei Fu*

Main category: cs.RO

TL;DR: 提出一种无需目标的多LiDAR外参标定框架，通过LiDAR束调整优化和自适应加权机制，显著提升标定精度和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 多LiDAR系统的精确外参标定对3D地图重建至关重要，传统方法依赖重叠视场或人工标注，限制了应用范围。

Method: 结合LiDAR束调整（LBA）和迭代优化，构建参考点云地图，并通过自适应加权机制抑制异常值。

Result: 在CARLA仿真和真实场景中，平均平移误差5 mm，旋转误差0.2°，初始误差容忍度达0.4 m/30°。

Conclusion: 该方法无需专用基础设施或人工调参，开源代码可用，显著优于现有技术。

Abstract: Accurate extrinsic calibration of multiple LiDARs is crucial for improving
the foundational performance of three-dimensional (3D) map reconstruction
systems. This paper presents a novel targetless extrinsic calibration framework
for multi-LiDAR systems that does not rely on overlapping fields of view or
precise initial parameter estimates. Unlike conventional calibration methods
that require manual annotations or specific reference patterns, our approach
introduces a unified optimization framework by integrating LiDAR bundle
adjustment (LBA) optimization with robust iterative refinement. The proposed
method constructs an accurate reference point cloud map via continuous scanning
from the target LiDAR and sliding-window LiDAR bundle adjustment, while
formulating extrinsic calibration as a joint LBA optimization problem. This
method effectively mitigates cumulative mapping errors and achieves
outlier-resistant parameter estimation through an adaptive weighting mechanism.
Extensive evaluations in both the CARLA simulation environment and real-world
scenarios demonstrate that our method outperforms state-of-the-art calibration
techniques in both accuracy and robustness. Experimental results show that for
non-overlapping sensor configurations, our framework achieves an average
translational error of 5 mm and a rotational error of 0.2{\deg}, with an
initial error tolerance of up to 0.4 m/30{\deg}. Moreover, the calibration
process operates without specialized infrastructure or manual parameter tuning.
The code is open source and available on GitHub
(\underline{https://github.com/Silentbarber/DLBAcalib})

</details>


### [456] [Informed Hybrid Zonotope-based Motion Planning Algorithm](https://arxiv.org/abs/2507.09309)
*Peng Xie,Johannes Betz,Amr Alanwar*

Main category: cs.RO

TL;DR: HZ-MP是一种基于混合Zonotope的运动规划器，通过分解无障碍空间和低维面采样，解决了非凸自由空间中的路径规划问题。


<details>
  <summary>Details</summary>
Motivation: 非凸自由空间中的路径规划问题因其NP-hard性质而具有挑战性，现有方法如AIT*和EIT*在狭窄间隙或目标受限场景中表现不佳。

Method: HZ-MP采用混合Zonotope分解无障碍空间，并结合椭球启发式引导低维面采样，实现高效探索。

Result: HZ-MP在有限时间内收敛到接近最优的轨迹，并能扩展到高维复杂场景，同时具有概率完备性和渐进最优性。

Conclusion: HZ-MP为复杂非凸自由空间中的路径规划提供了一种高效且可扩展的解决方案。

Abstract: Optimal path planning in nonconvex free spaces is notoriously challenging, as
formulating such problems as mixed-integer linear programs (MILPs) is NP-hard.
We propose HZ-MP, an informed Hybrid Zonotope-based Motion Planner, as an
alternative approach that decomposes the obstacle-free space and performs
low-dimensional face sampling guided by an ellipsotope heuristic, enabling
focused exploration along promising transit regions. This structured
exploration eliminates the excessive, unreachable sampling that degrades
existing informed planners such as AIT* and EIT* in narrow gaps or boxed-goal
scenarios. We prove that HZ-MP is probabilistically complete and asymptotically
optimal. It converges to near-optimal trajectories in finite time and scales to
high-dimensional cluttered scenes.

</details>


### [457] [Unified Linear Parametric Map Modeling and Perception-aware Trajectory Planning for Mobile Robotics](https://arxiv.org/abs/2507.09340)
*Hongyu Nie,Xingyu Li,Xu Liu,Zhaotong Tan,Sen Mei,Wenbo Su*

Main category: cs.RO

TL;DR: 论文提出了一种名为RMRP的轻量级线性参数化地图构建方法，结合RPATR框架，解决了移动机器人在复杂环境中导航的感知和规划问题。


<details>
  <summary>Details</summary>
Motivation: 解决移动机器人在大规模复杂环境中导航时面临的高计算负担、传感器遮挡和地形不规则等问题。

Method: 通过高维空间映射和稀疏随机投影构建轻量级地图，提出RPATR框架，结合ESDF地图和在线规划。

Result: 在多种场景下验证，展示了优越的映射性能和高效、安全的导航能力。

Conclusion: RMRP和RPATR框架为高速无人机和地面机器人提供了高效的感知和规划解决方案。

Abstract: Autonomous navigation in mobile robots, reliant on perception and planning,
faces major hurdles in large-scale, complex environments. These include heavy
computational burdens for mapping, sensor occlusion failures for UAVs, and
traversal challenges on irregular terrain for UGVs, all compounded by a lack of
perception-aware strategies. To address these challenges, we introduce Random
Mapping and Random Projection (RMRP). This method constructs a lightweight
linear parametric map by first mapping data to a high-dimensional space,
followed by a sparse random projection for dimensionality reduction. Our novel
Residual Energy Preservation Theorem provides theoretical guarantees for this
process, ensuring critical geometric properties are preserved. Based on this
map, we propose the RPATR (Robust Perception-Aware Trajectory Planner)
framework. For UAVs, our method unifies grid and Euclidean Signed Distance
Field (ESDF) maps. The front-end uses an analytical occupancy gradient to
refine initial paths for safety and smoothness, while the back-end uses a
closed-form ESDF for trajectory optimization. Leveraging the trained RMRP
model's generalization, the planner predicts unobserved areas for proactive
navigation. For UGVs, the model characterizes terrain and provides closed-form
gradients, enabling online planning to circumvent large holes. Validated in
diverse scenarios, our framework demonstrates superior mapping performance in
time, memory, and accuracy, and enables computationally efficient, safe
navigation for high-speed UAVs and UGVs. The code will be released to foster
community collaboration.

</details>


### [458] [C-ZUPT: Stationarity-Aided Aerial Hovering](https://arxiv.org/abs/2507.09344)
*Daniel Engelsman,Itzik Klein*

Main category: cs.RO

TL;DR: 提出了一种用于空中导航的受控零速度更新（C-ZUPT）方法，通过不确定性阈值识别准静态平衡，显著减少惯性漂移和控制能耗。


<details>
  <summary>Details</summary>
Motivation: 卫星和相机定位在复杂环境中受限，惯性传感器易受偏差和噪声影响，导致精度下降。需要替代方法提供稳定修正。

Method: 引入C-ZUPT方法，通过不确定性阈值识别准静态平衡，为估计滤波器提供精确速度更新。

Result: 验证表明C-ZUPT显著减少惯性漂移和控制能耗，提升导航稳定性，延长飞行时间。

Conclusion: C-ZUPT有效解决空中系统的惯性漂移问题，提升能源效率和飞行性能。

Abstract: Autonomous systems across diverse domains have underscored the need for
drift-resilient state estimation. Although satellite-based positioning and
cameras are widely used, they often suffer from limited availability in many
environments. As a result, positioning must rely solely on inertial sensors,
leading to rapid accuracy degradation over time due to sensor biases and noise.
To counteract this, alternative update sources-referred to as information
aiding-serve as anchors of certainty. Among these, the zero-velocity update
(ZUPT) is particularly effective in providing accurate corrections during
stationary intervals, though it is restricted to surface-bound platforms. This
work introduces a controlled ZUPT (C-ZUPT) approach for aerial navigation and
control, independent of surface contact. By defining an uncertainty threshold,
C-ZUPT identifies quasi-static equilibria to deliver precise velocity updates
to the estimation filter. Extensive validation confirms that these
opportunistic, high-quality updates significantly reduce inertial drift and
control effort. As a result, C-ZUPT mitigates filter divergence and enhances
navigation stability, enabling more energy-efficient hovering and substantially
extending sustained flight-key advantages for resource-constrained aerial
systems.

</details>


### [459] [Constrained Style Learning from Imperfect Demonstrations under Task Optimality](https://arxiv.org/abs/2507.09371)
*Kehan Wen,Chenhao Li,Junzhe He,Marco Hutter*

Main category: cs.RO

TL;DR: 论文提出了一种基于约束马尔可夫决策过程（CMDP）的方法，通过自适应调整拉格朗日乘子，在保持任务性能的同时学习演示中的风格。


<details>
  <summary>Details</summary>
Motivation: 现有方法在处理不完整或不现实的演示时，往往以牺牲任务性能为代价来提升风格模仿质量。

Method: 将问题建模为CMDP，优化风格模仿目标并约束任务性能，引入自适应拉格朗日乘子选择性模仿演示。

Result: 在多个机器人平台和任务中验证了方法，实现了稳健的任务性能和高保真风格学习，ANYmal-D硬件上机械能降低14.5%。

Conclusion: 该方法有效平衡了任务性能和风格模仿，适用于现实世界任务。

Abstract: Learning from demonstration has proven effective in robotics for acquiring
natural behaviors, such as stylistic motions and lifelike agility, particularly
when explicitly defining style-oriented reward functions is challenging.
Synthesizing stylistic motions for real-world tasks usually requires balancing
task performance and imitation quality. Existing methods generally depend on
expert demonstrations closely aligned with task objectives. However, practical
demonstrations are often incomplete or unrealistic, causing current methods to
boost style at the expense of task performance. To address this issue, we
propose formulating the problem as a constrained Markov Decision Process
(CMDP). Specifically, we optimize a style-imitation objective with constraints
to maintain near-optimal task performance. We introduce an adaptively
adjustable Lagrangian multiplier to guide the agent to imitate demonstrations
selectively, capturing stylistic nuances without compromising task performance.
We validate our approach across multiple robotic platforms and tasks,
demonstrating both robust task performance and high-fidelity style learning. On
ANYmal-D hardware we show a 14.5% drop in mechanical energy and a more agile
gait pattern, showcasing real-world benefits.

</details>


### [460] [Real-Time Adaptive Motion Planning via Point Cloud-Guided, Energy-Based Diffusion and Potential Fields](https://arxiv.org/abs/2507.09383)
*Wondmgezahu Teshome,Kian Behzad,Octavia Camps,Michael Everett,Milad Siami,Mario Sznaier*

Main category: cs.RO

TL;DR: 提出了一种结合能量扩散模型与人工势场的运动规划框架，用于复杂环境中的实时轨迹生成。


<details>
  <summary>Details</summary>
Motivation: 受追逃问题启发，旨在实现复杂环境中的鲁棒实时轨迹规划。

Method: 利用点云直接处理障碍物信息，结合无分类器引导训练和局部势场采样增强避障。

Result: 在动态场景中，系统通过扩散模型生成初始轨迹，并通过势场适应持续优化，在部分可观测的追逃场景中表现良好。

Conclusion: 该框架在复杂环境中实现了高效的实时轨迹规划，适用于动态和部分可观测的场景。

Abstract: Motivated by the problem of pursuit-evasion, we present a motion planning
framework that combines energy-based diffusion models with artificial potential
fields for robust real time trajectory generation in complex environments. Our
approach processes obstacle information directly from point clouds, enabling
efficient planning without requiring complete geometric representations. The
framework employs classifier-free guidance training and integrates local
potential fields during sampling to enhance obstacle avoidance. In dynamic
scenarios, the system generates initial trajectories using the diffusion model
and continuously refines them through potential field-based adaptation,
demonstrating effective performance in pursuit-evasion scenarios with partial
pursuer observability.

</details>


### [461] [Influence of Static and Dynamic Downwash Interactions on Multi-Quadrotor Systems](https://arxiv.org/abs/2507.09463)
*Anoop Kiran,Nora Ayanian,Kenneth Breuer*

Main category: cs.RO

TL;DR: 论文通过数据驱动方法分析多旋翼无人机间的下洗效应，提出优化编队和控制的物理策略。


<details>
  <summary>Details</summary>
Motivation: 多旋翼无人机在近距离飞行时因复杂气动交互（如下洗效应）导致性能下降，传统保守策略限制了其应用范围。

Method: 使用力和扭矩测量及粒子图像测速技术（PIV）量化单机和多机配置中的下洗效应。

Result: 数据揭示了力和速度特征，为优化编队和控制提供了依据。

Conclusion: 研究为多旋翼无人机在密集环境中的高效运行提供了物理基础和优化策略。

Abstract: Flying multiple quadrotors in close proximity presents a significant
challenge due to complex aerodynamic interactions, particularly downwash
effects that are known to destabilize vehicles and degrade performance.
Traditionally, multi-quadrotor systems rely on conservative strategies, such as
collision avoidance zones around the robot volume, to circumvent this effect.
This restricts their capabilities by requiring a large volume for the operation
of a multi-quadrotor system, limiting their applicability in dense
environments. This work provides a comprehensive, data-driven analysis of the
downwash effect, with a focus on characterizing, analyzing, and understanding
forces, moments, and velocities in both single and multi-quadrotor
configurations. We use measurements of forces and torques to characterize
vehicle interactions, and particle image velocimetry (PIV) to quantify the
spatial features of the downwash wake for a single quadrotor and an interacting
pair of quadrotors. This data can be used to inform physics-based strategies
for coordination, leverage downwash for optimized formations, expand the
envelope of operation, and improve the robustness of multi-quadrotor control.

</details>


### [462] [Unmanned Aerial Vehicle (UAV) Data-Driven Modeling Software with Integrated 9-Axis IMUGPS Sensor Fusion and Data Filtering Algorithm](https://arxiv.org/abs/2507.09464)
*Azfar Azdi Arfakhsyad,Aufa Nasywa Rahman,Larasati Kinanti,Ahmad Ataka Awwalur Rizqi,Hannan Nur Muhammad*

Main category: cs.RO

TL;DR: 本文提出了一种基于数据驱动的无人机建模软件，利用低成本传感器和算法提升数据质量，实现高精度可视化。


<details>
  <summary>Details</summary>
Motivation: 无人机（UAV）的广泛应用需要精确建模支持测试，而低成本传感器和高效数据处理是实现这一目标的关键。

Method: 通过融合IMU（惯性测量单元）和GPS数据，结合四元数表示法避免万向节锁问题，利用数据滤波和传感器融合技术提升数据质量。

Result: 软件能够高精度、流畅地实时渲染无人机的方向和位置。

Conclusion: 该数据驱动建模软件为无人机开发测试提供了高效、精确的解决方案。

Abstract: Unmanned Aerial Vehicles (UAV) have emerged as versatile platforms, driving
the demand for accurate modeling to support developmental testing. This paper
proposes data-driven modeling software for UAV. Emphasizes the utilization of
cost-effective sensors to obtain orientation and location data subsequently
processed through the application of data filtering algorithms and sensor
fusion techniques to improve the data quality to make a precise model
visualization on the software. UAV's orientation is obtained using processed
Inertial Measurement Unit (IMU) data and represented using Quaternion
Representation to avoid the gimbal lock problem. The UAV's location is
determined by combining data from the Global Positioning System (GPS), which
provides stable geographic coordinates but slower data update frequency, and
the accelerometer, which has higher data update frequency but integrating it to
get position data is unstable due to its accumulative error. By combining data
from these two sensors, the software is able to calculate and continuously
update the UAV's real-time position during its flight operations. The result
shows that the software effectively renders UAV orientation and position with
high degree of accuracy and fluidity

</details>


### [463] [mmE-Loc: Facilitating Accurate Drone Landing with Ultra-High-Frequency Localization](https://arxiv.org/abs/2507.09469)
*Haoyang Wang,Jingao Xu,Xinyu Luo,Ting Zhang,Xuecheng Chen,Ruiyang Duan,Jialong Chen,Yunhao Liu,Jianfeng Zheng,Weijie Hong,Xinlei Chen*

Main category: cs.RO

TL;DR: 论文提出了一种名为mmE-Loc的高精度、低延迟地面定位系统，结合事件相机和毫米波雷达，用于无人机精准降落。


<details>
  <summary>Details</summary>
Motivation: 传统帧相机采样频率低，与毫米波雷达不匹配，限制了系统性能。通过引入事件相机，提升采样频率，实现更高效的无人机定位与降落。

Method: 提出两个模块：(i) 一致性指导的协同跟踪模块，利用无人机的周期性微运动和结构信息；(ii) 图引导的自适应联合优化模块，融合传感器数据。

Result: 实际实验中，mmE-Loc在精度和延迟上显著优于现有方法。

Conclusion: mmE-Loc系统通过结合事件相机和毫米波雷达，实现了无人机精准降落的高效解决方案。

Abstract: For precise, efficient, and safe drone landings, ground platforms should
real-time, accurately locate descending drones and guide them to designated
spots. While mmWave sensing combined with cameras improves localization
accuracy, lower sampling frequency of traditional frame cameras compared to
mmWave radar creates bottlenecks in system throughput. In this work, we upgrade
traditional frame camera with event camera, a novel sensor that harmonizes in
sampling frequency with mmWave radar within ground platform setup, and
introduce mmE-Loc, a high-precision, low-latency ground localization system
designed for precise drone landings. To fully exploit the \textit{temporal
consistency} and \textit{spatial complementarity} between these two modalities,
we propose two innovative modules: \textit{(i)} the Consistency-instructed
Collaborative Tracking module, which further leverages the drone's physical
knowledge of periodic micro-motions and structure for accurate measurements
extraction, and \textit{(ii)} the Graph-informed Adaptive Joint Optimization
module, which integrates drone motion information for efficient sensor fusion
and drone localization. Real-world experiments conducted in landing scenarios
with a drone delivery company demonstrate that mmE-Loc significantly
outperforms state-of-the-art methods in both accuracy and latency.

</details>


### [464] [Multi-residual Mixture of Experts Learning for Cooperative Control in Multi-vehicle Systems](https://arxiv.org/abs/2507.09836)
*Vindula Jayawardana,Sirui Li,Yashar Farid,Cathy Wu*

Main category: cs.RO

TL;DR: 论文提出了一种名为MRMEL的新框架，用于设计自主车辆的拉格朗日交通控制策略，通过混合专家学习和残差强化学习，显著提升了交通场景中的性能。


<details>
  <summary>Details</summary>
Motivation: 传统固定位置的交通控制方法（如交通信号）在多样化交通场景中表现不佳，自主车辆作为移动执行器需要更灵活的控制策略。

Method: MRMEL框架通过混合专家学习动态选择最优策略，并结合残差强化学习对策略进行修正。

Result: 在真实交通场景的案例研究中，MRMEL实现了额外4%-9%的车辆排放减少。

Conclusion: MRMEL是一种有效的拉格朗日交通控制方法，适用于多样化交通场景。

Abstract: Autonomous vehicles (AVs) are becoming increasingly popular, with their
applications now extending beyond just a mode of transportation to serving as
mobile actuators of a traffic flow to control flow dynamics. This contrasts
with traditional fixed-location actuators, such as traffic signals, and is
referred to as Lagrangian traffic control. However, designing effective
Lagrangian traffic control policies for AVs that generalize across traffic
scenarios introduces a major challenge. Real-world traffic environments are
highly diverse, and developing policies that perform robustly across such
diverse traffic scenarios is challenging. It is further compounded by the joint
complexity of the multi-agent nature of traffic systems, mixed motives among
participants, and conflicting optimization objectives subject to strict
physical and external constraints. To address these challenges, we introduce
Multi-Residual Mixture of Expert Learning (MRMEL), a novel framework for
Lagrangian traffic control that augments a given suboptimal nominal policy with
a learned residual while explicitly accounting for the structure of the traffic
scenario space. In particular, taking inspiration from residual reinforcement
learning, MRMEL augments a suboptimal nominal AV control policy by learning a
residual correction, but at the same time dynamically selects the most suitable
nominal policy from a pool of nominal policies conditioned on the traffic
scenarios and modeled as a mixture of experts. We validate MRMEL using a case
study in cooperative eco-driving at signalized intersections in Atlanta, Dallas
Fort Worth, and Salt Lake City, with real-world data-driven traffic scenarios.
The results show that MRMEL consistently yields superior performance-achieving
an additional 4%-9% reduction in aggregate vehicle emissions relative to the
strongest baseline in each setting.

</details>


### [465] [TruckV2X: A Truck-Centered Perception Dataset](https://arxiv.org/abs/2507.09505)
*Tenghui Xie,Zhiying Song,Fuxi Wen,Jun Li,Guangzhao Liu,Zijian Zhao*

Main category: cs.RO

TL;DR: 论文介绍了TruckV2X，首个以卡车为中心的大规模协同感知数据集，旨在解决卡车感知中的盲区和遮挡问题，推动多智能体自动驾驶卡车系统的发展。


<details>
  <summary>Details</summary>
Motivation: 卡车自动驾驶面临独特的感知挑战，如盲区和动态拖车运动导致的遮挡问题，现有数据集缺乏针对重型车辆的多智能体配置。

Method: 提出TruckV2X数据集，包含多模态感知（LiDAR和摄像头）和多智能体协作（牵引车、拖车、CAV和RSU），并研究卡车对协同感知需求的影响。

Result: 建立了性能基准，为开发具有增强遮挡处理能力的协同感知系统提供了基础。

Conclusion: TruckV2X数据集将加速多智能体自动驾驶卡车系统的部署，并推动重型车辆感知研究的优先方向。

Abstract: Autonomous trucking offers significant benefits, such as improved safety and
reduced costs, but faces unique perception challenges due to trucks' large size
and dynamic trailer movements. These challenges include extensive blind spots
and occlusions that hinder the truck's perception and the capabilities of other
road users. To address these limitations, cooperative perception emerges as a
promising solution. However, existing datasets predominantly feature light
vehicle interactions or lack multi-agent configurations for heavy-duty vehicle
scenarios. To bridge this gap, we introduce TruckV2X, the first large-scale
truck-centered cooperative perception dataset featuring multi-modal sensing
(LiDAR and cameras) and multi-agent cooperation (tractors, trailers, CAVs, and
RSUs). We further investigate how trucks influence collaborative perception
needs, establishing performance benchmarks while suggesting research priorities
for heavy vehicle perception. The dataset provides a foundation for developing
cooperative perception systems with enhanced occlusion handling capabilities,
and accelerates the deployment of multi-agent autonomous trucking systems. The
TruckV2X dataset is available at
https://huggingface.co/datasets/XieTenghu1/TruckV2X.

</details>


### [466] [Self-supervised Pretraining for Integrated Prediction and Planning of Automated Vehicles](https://arxiv.org/abs/2507.09537)
*Yangang Ren,Guojian Zhan,Chen Lv,Jun Li,Fenghua Liang,Keqiang Li*

Main category: cs.RO

TL;DR: Plan-MAE是一个基于掩码自编码器的预测与规划统一预训练框架，通过学习道路网络、代理轨迹和导航路线，结合局部子规划任务，显著提升了自动驾驶车辆的轨迹规划性能。


<details>
  <summary>Details</summary>
Motivation: 当前方法依赖模仿学习，忽略了场景理解对更全面轨迹规划的潜在作用。Plan-MAE旨在通过预训练框架融合上下文理解，提升预测与规划的联合性能。

Method: 提出Plan-MAE框架，通过三个任务（道路网络重建、代理轨迹建模、导航路线捕捉）学习空间和社会交互，并加入局部子规划任务以对齐车辆动态和安全约束。

Result: 在大规模数据集上，Plan-MAE在规划指标上显著优于现有方法，可作为基于学习的运动规划器的重要预训练步骤。

Conclusion: Plan-MAE通过统一预训练框架，有效提升了自动驾驶车辆的预测与规划能力，为未来研究提供了重要基础。

Abstract: Predicting the future of surrounding agents and accordingly planning a safe,
goal-directed trajectory are crucial for automated vehicles. Current methods
typically rely on imitation learning to optimize metrics against the ground
truth, often overlooking how scene understanding could enable more holistic
trajectories. In this paper, we propose Plan-MAE, a unified pretraining
framework for prediction and planning that capitalizes on masked autoencoders.
Plan-MAE fuses critical contextual understanding via three dedicated tasks:
reconstructing masked road networks to learn spatial correlations, agent
trajectories to model social interactions, and navigation routes to capture
destination intents. To further align vehicle dynamics and safety constraints,
we incorporate a local sub-planning task predicting the ego-vehicle's near-term
trajectory segment conditioned on earlier segment. This pretrained model is
subsequently fine-tuned on downstream tasks to jointly generate the prediction
and planning trajectories. Experiments on large-scale datasets demonstrate that
Plan-MAE outperforms current methods on the planning metrics by a large margin
and can serve as an important pre-training step for learning-based motion
planner.

</details>


### [467] [On the Importance of Neural Membrane Potential Leakage for LIDAR-based Robot Obstacle Avoidance using Spiking Neural Networks](https://arxiv.org/abs/2507.09538)
*Zainab Ali,Lujayn Al-Amir,Ali Safa*

Main category: cs.RO

TL;DR: 该论文研究了利用脉冲神经网络（SNN）从LIDAR数据实现机器人导航和避障，重点探讨了神经元膜泄漏对SNN精度的影响，并通过实验证明其性能可与传统卷积神经网络（CNN）媲美。


<details>
  <summary>Details</summary>
Motivation: 由于SNN在神经形态硬件中具有高精度、低内存和计算复杂度的优势，适用于电池和负载受限的自主机器人应用（如无人机和漫游车）。

Method: 搭建了配备LIDAR的机器人平台，收集带标签的LIDAR数据集和人工操作的控制命令，研究SNN中LIF神经元的膜泄漏常数对避障精度的影响。

Result: 通过精细调节LIF神经元的膜泄漏常数，SNN在机器人控制精度上可与非脉冲CNN相媲美。

Conclusion: 论文首次聚焦膜泄漏对SNN处理LIDAR数据的重要性，并开源了LIDAR数据集以促进未来研究。

Abstract: Using neuromorphic computing for robotics applications has gained much
attention in recent year due to the remarkable ability of Spiking Neural
Networks (SNNs) for high-precision yet low memory and compute complexity
inference when implemented in neuromorphic hardware. This ability makes SNNs
well-suited for autonomous robot applications (such as in drones and rovers)
where battery resources and payload are typically limited. Within this context,
this paper studies the use of SNNs for performing direct robot navigation and
obstacle avoidance from LIDAR data. A custom robot platform equipped with a
LIDAR is set up for collecting a labeled dataset of LIDAR sensing data together
with the human-operated robot control commands used for obstacle avoidance.
Crucially, this paper provides what is, to the best of our knowledge, a first
focused study about the importance of neuron membrane leakage on the SNN
precision when processing LIDAR data for obstacle avoidance. It is shown that
by carefully tuning the membrane potential leakage constant of the spiking
Leaky Integrate-and-Fire (LIF) neurons used within our SNN, it is possible to
achieve on-par robot control precision compared to the use of a non-spiking
Convolutional Neural Network (CNN). Finally, the LIDAR dataset collected during
this work is released as open-source with the hope of benefiting future
research.

</details>


### [468] [Prompt Informed Reinforcement Learning for Visual Coverage Path Planning](https://arxiv.org/abs/2507.10284)
*Venkat Margapuri*

Main category: cs.RO

TL;DR: 该研究提出了一种名为Prompt-Informed Reinforcement Learning (PIRL)的新方法，结合大型语言模型（如GPT-3.5）的零样本推理能力和好奇心驱动的强化学习，以动态调整奖励函数，优化无人机视觉覆盖路径规划。


<details>
  <summary>Details</summary>
Motivation: 传统强化学习方法依赖环境特定的奖励设计，缺乏语义适应性。PIRL旨在通过LLM的语义反馈动态调整奖励函数，提升无人机在视觉覆盖任务中的性能。

Method: PIRL结合了大型语言模型（GPT-3.5）的零样本推理能力和PPO强化学习策略，动态调整奖励函数以优化无人机的位置和摄像头控制。实验在OpenAI Gym和Webots仿真环境中进行。

Result: PIRL在视觉覆盖率、电池效率和冗余降低方面均优于基线方法，如在OpenAI Gym中覆盖率提升14%，Webots中提升27%，电池效率提升25%，冗余降低18%。

Conclusion: PIRL展示了LLM引导的奖励调整在复杂空间探索任务中的有效性，为自然语言先验与机器人强化学习的结合提供了有前景的方向。

Abstract: Visual coverage path planning with unmanned aerial vehicles (UAVs) requires
agents to strategically coordinate UAV motion and camera control to maximize
coverage, minimize redundancy, and maintain battery efficiency. Traditional
reinforcement learning (RL) methods rely on environment-specific reward
formulations that lack semantic adaptability. This study proposes
Prompt-Informed Reinforcement Learning (PIRL), a novel approach that integrates
the zero-shot reasoning ability and in-context learning capability of large
language models with curiosity-driven RL. PIRL leverages semantic feedback from
an LLM, GPT-3.5, to dynamically shape the reward function of the Proximal
Policy Optimization (PPO) RL policy guiding the agent in position and camera
adjustments for optimal visual coverage. The PIRL agent is trained using OpenAI
Gym and evaluated in various environments. Furthermore, the sim-to-real-like
ability and zero-shot generalization of the agent are tested by operating the
agent in Webots simulator which introduces realistic physical dynamics. Results
show that PIRL outperforms multiple learning-based baselines such as PPO with
static rewards, PPO with exploratory weight initialization, imitation learning,
and an LLM-only controller. Across different environments, PIRL outperforms the
best-performing baseline by achieving up to 14% higher visual coverage in
OpenAI Gym and 27% higher in Webots, up to 25% higher battery efficiency, and
up to 18\% lower redundancy, depending on the environment. The results
highlight the effectiveness of LLM-guided reward shaping in complex spatial
exploration tasks and suggest a promising direction for integrating natural
language priors into RL for robotics.

</details>


### [469] [IteraOptiRacing: A Unified Planning-Control Framework for Real-time Autonomous Racing for Iterative Optimal Performance](https://arxiv.org/abs/2507.09714)
*Yifan Zeng,Yihan Li,Suiyi He,Koushil Sreenath,Jun Zeng*

Main category: cs.RO

TL;DR: 提出了一种基于i2LQR的统一规划控制策略IteraOptiRacing，用于自动驾驶赛车环境中的多车竞争，优化圈速并避免碰撞。


<details>
  <summary>Details</summary>
Motivation: 在自动驾驶赛车环境中，需要一种能够同时优化圈速并避免与多辆动态障碍车碰撞的策略。

Method: 基于i2LQR的迭代优化方法，利用历史数据进行轨迹生成，实现低计算负担和并行计算能力。

Result: 在高保真模拟器中验证，该策略在所有随机生成的赛车场景中优于现有方法。

Conclusion: IteraOptiRacing策略在实时性和性能上表现优异，适用于竞争性赛车场景。

Abstract: This paper presents a unified planning-control strategy for competing with
other racing cars called IteraOptiRacing in autonomous racing environments.
This unified strategy is proposed based on Iterative Linear Quadratic Regulator
for Iterative Tasks (i2LQR), which can improve lap time performance in the
presence of surrounding racing obstacles. By iteratively using the ego car's
historical data, both obstacle avoidance for multiple moving cars and time cost
optimization are considered in this unified strategy, resulting in
collision-free and time-optimal generated trajectories. The algorithm's
constant low computation burden and suitability for parallel computing enable
real-time operation in competitive racing scenarios. To validate its
performance, simulations in a high-fidelity simulator are conducted with
multiple randomly generated dynamic agents on the track. Results show that the
proposed strategy outperforms existing methods across all randomly generated
autonomous racing scenarios, enabling enhanced maneuvering for the ego racing
car.

</details>


### [470] [Visual Homing in Outdoor Robots Using Mushroom Body Circuits and Learning Walks](https://arxiv.org/abs/2507.09725)
*Gabriel G. Gattaux,Julien R. Serres,Franck Ruffier,Antoine Wystrach*

Main category: cs.RO

TL;DR: 论文提出了一种基于蚂蚁视觉归巢行为的生物启发式方法，首次在真实世界中实现了一种侧向化蘑菇体（MB）架构，用于紧凑型自动驾驶车辆的视觉归巢。


<details>
  <summary>Details</summary>
Motivation: 蚂蚁能够在极少的感官输入和少量学习行走后实现稳健的视觉归巢，这启发了自主导航的生物仿生解决方案。

Method: 通过实验验证了角路径积分（PI）信号的符号是否可以将全景视图分类为“目标在左”和“目标在右”记忆库，从而实现自然户外环境中的稳健归巢。

Result: 实验验证了该方法在模拟和真实世界中的有效性，包括吸引子式的巢动态、巢搜索行为、随机行走后的归巢以及精确停止行为。

Conclusion: 该系统提供了一种基于生物学的资源高效解决方案，适用于自主视觉归巢。

Abstract: Ants achieve robust visual homing with minimal sensory input and only a few
learning walks, inspiring biomimetic solutions for autonomous navigation. While
Mushroom Body (MB) models have been used in robotic route following, they have
not yet been applied to visual homing. We present the first real-world
implementation of a lateralized MB architecture for visual homing onboard a
compact autonomous car-like robot. We test whether the sign of the angular path
integration (PI) signal can categorize panoramic views, acquired during
learning walks and encoded in the MB, into "goal on the left" and "goal on the
right" memory banks, enabling robust homing in natural outdoor settings. We
validate this approach through four incremental experiments: (1) simulation
showing attractor-like nest dynamics; (2) real-world homing after decoupled
learning walks, producing nest search behavior; (3) homing after random walks
using noisy PI emulated with GPS-RTK; and (4) precise stopping-at-the-goal
behavior enabled by a fifth MB Output Neuron (MBON) encoding goal-views to
control velocity. This mimics the accurate homing behavior of ants and
functionally resembles waypoint-based position control in robotics, despite
relying solely on visual input. Operating at 8 Hz on a Raspberry Pi 4 with
32x32 pixel views and a memory footprint under 9 kB, our system offers a
biologically grounded, resource-efficient solution for autonomous visual
homing.

</details>


### [471] [Active Probing with Multimodal Predictions for Motion Planning](https://arxiv.org/abs/2507.09822)
*Darshan Gadginmath,Farhad Nawaz,Minjun Sung,Faizan M Tariq,Sangjae Bae,David Isele,Fabio Pasqualetti,Jovin Dsa*

Main category: cs.RO

TL;DR: 提出了一种结合轨迹规划、多模态预测和主动探测的统一框架，用于增强不确定性下的决策能力。


<details>
  <summary>Details</summary>
Motivation: 动态环境中导航需要处理其他代理行为的不确定性。

Method: 开发了一种新的风险度量，通过混合模型整合多模态预测不确定性，并引入主动探测机制以减少预测模糊性。

Result: 在MetaDrive仿真环境中验证了框架的有效性，成功处理了复杂交通场景中的不确定性。

Conclusion: 该框架在多种交通代理行为模型中表现出鲁棒性，适用于现实世界的自主导航挑战。

Abstract: Navigation in dynamic environments requires autonomous systems to reason
about uncertainties in the behavior of other agents. In this paper, we
introduce a unified framework that combines trajectory planning with multimodal
predictions and active probing to enhance decision-making under uncertainty. We
develop a novel risk metric that seamlessly integrates multimodal prediction
uncertainties through mixture models. When these uncertainties follow a
Gaussian mixture distribution, we prove that our risk metric admits a
closed-form solution, and is always finite, thus ensuring analytical
tractability. To reduce prediction ambiguity, we incorporate an active probing
mechanism that strategically selects actions to improve its estimates of
behavioral parameters of other agents, while simultaneously handling multimodal
uncertainties. We extensively evaluate our framework in autonomous navigation
scenarios using the MetaDrive simulation environment. Results demonstrate that
our active probing approach successfully navigates complex traffic scenarios
with uncertain predictions. Additionally, our framework shows robust
performance across diverse traffic agent behavior models, indicating its broad
applicability to real-world autonomous navigation challenges. Code and videos
are available at
https://darshangm.github.io/papers/active-probing-multimodal-predictions/.

</details>


### [472] [AdvGrasp: Adversarial Attacks on Robotic Grasping from a Physical Perspective](https://arxiv.org/abs/2507.09857)
*Xiaofei Wang,Mingliang Han,Tianyu Hao,Cegang Li,Yunbo Zhao,Keke Tang*

Main category: cs.RO

TL;DR: AdvGrasp框架从物理角度对机器人抓取进行对抗攻击，通过变形物体形状降低抓取性能。


<details>
  <summary>Details</summary>
Motivation: 现有研究多关注神经网络预测而忽略抓取的物理原理，AdvGrasp填补了这一空白。

Method: 通过变形物体形状增加重力扭矩和降低稳定性，系统性地削弱抓取能力。

Result: 实验验证了AdvGrasp的有效性，实际应用展示了其鲁棒性。

Conclusion: AdvGrasp为机器人抓取的鲁棒性评估和改进提供了新视角。

Abstract: Adversarial attacks on robotic grasping provide valuable insights into
evaluating and improving the robustness of these systems. Unlike studies that
focus solely on neural network predictions while overlooking the physical
principles of grasping, this paper introduces AdvGrasp, a framework for
adversarial attacks on robotic grasping from a physical perspective.
Specifically, AdvGrasp targets two core aspects: lift capability, which
evaluates the ability to lift objects against gravity, and grasp stability,
which assesses resistance to external disturbances. By deforming the object's
shape to increase gravitational torque and reduce stability margin in the
wrench space, our method systematically degrades these two key grasping
metrics, generating adversarial objects that compromise grasp performance.
Extensive experiments across diverse scenarios validate the effectiveness of
AdvGrasp, while real-world validations demonstrate its robustness and practical
applicability

</details>


### [473] [Customize Harmonic Potential Fields via Hybrid Optimization over Homotopic Paths](https://arxiv.org/abs/2507.09858)
*Shuaikang Wang,Tiecheng Guo,Meng Guo*

Main category: cs.RO

TL;DR: 提出了一种新方法，通过谐波势场自动生成路径的同伦类，适用于复杂工作空间，如森林世界。


<details>
  <summary>Details</summary>
Motivation: 现有谐波势场方法难以定制路径的拓扑性质，限制了其应用。

Method: 采用混合优化算法，搜索同伦类，选择星形障碍物的结构，并通过投影梯度下降优化权重参数。

Result: 通过模拟和硬件实验验证了方法的有效性，能够定制路径的同伦性质。

Conclusion: 该方法扩展了谐波势场的应用范围，适用于复杂环境中的安全导航。

Abstract: Safe navigation within a workspace is a fundamental skill for autonomous
robots to accomplish more complex tasks. Harmonic potentials are artificial
potential fields that are analytical, globally convergent and provably free of
local minima. Thus, it has been widely used for generating safe and reliable
robot navigation control policies. However, most existing methods do not allow
customization of the harmonic potential fields nor the resulting paths,
particularly regarding their topological properties. In this paper, we propose
a novel method that automatically finds homotopy classes of paths that can be
generated by valid harmonic potential fields. The considered complex workspaces
can be as general as forest worlds consisting of numerous overlapping
star-obstacles. The method is based on a hybrid optimization algorithm that
searches over homotopy classes, selects the structure of each tree-of-stars
within the forest, and optimizes over the continuous weight parameters for each
purged tree via the projected gradient descent. The key insight is to transform
the forest world to the unbounded point world via proper diffeomorphic
transformations. It not only facilitates a simpler design of the
multi-directional D-signature between non-homotopic paths, but also retain the
safety and convergence properties. Extensive simulations and hardware
experiments are conducted for non-trivial scenarios, where the navigation
potentials are customized for desired homotopic properties. Project page:
https://shuaikang-wang.github.io/CustFields.

</details>


### [474] [Demonstrating the Octopi-1.5 Visual-Tactile-Language Model](https://arxiv.org/abs/2507.09985)
*Samson Yu,Kelvin Lin,Harold Soh*

Main category: cs.RO

TL;DR: Octopi-1.5是一个新的视觉-触觉-语言模型，能够处理多部位触觉信号，并通过检索增强生成模块提升任务性能，支持实时学习新物体。


<details>
  <summary>Details</summary>
Motivation: 触觉对人类和机器人至关重要，尤其在灵巧操作、材料识别和视觉遮挡场景中。Octopi-1.5旨在扩展触觉基础模型的能力。

Method: Octopi-1.5采用多部位触觉信号处理和检索增强生成（RAG）模块，结合手持触觉接口TMI（配备GelSight和TAC-02传感器）进行交互。

Result: 模型能够通过触觉输入和常识知识解决推理任务，例如识别抓握物体并推荐处理方式，还能实时学习新物体。

Conclusion: Octopi-1.5展示了视觉-触觉-语言模型的进展，同时揭示了其局限性，激发了进一步研究的兴趣。

Abstract: Touch is recognized as a vital sense for humans and an equally important
modality for robots, especially for dexterous manipulation, material
identification, and scenarios involving visual occlusion. Building upon very
recent work in touch foundation models, this demonstration will feature
Octopi-1.5, our latest visual-tactile-language model. Compared to its
predecessor, Octopi-1.5 introduces the ability to process tactile signals from
multiple object parts and employs a simple retrieval-augmented generation (RAG)
module to improve performance on tasks and potentially learn new objects
on-the-fly. The system can be experienced live through a new handheld
tactile-enabled interface, the TMI, equipped with GelSight and TAC-02 tactile
sensors. This convenient and accessible setup allows users to interact with
Octopi-1.5 without requiring a robot. During the demonstration, we will
showcase Octopi-1.5 solving tactile inference tasks by leveraging tactile
inputs and commonsense knowledge. For example, in a Guessing Game, Octopi-1.5
will identify objects being grasped and respond to follow-up queries about how
to handle it (e.g., recommending careful handling for soft fruits). We also
plan to demonstrate Octopi-1.5's RAG capabilities by teaching it new items.
With live interactions, this demonstration aims to highlight both the progress
and limitations of VTLMs such as Octopi-1.5 and to foster further interest in
this exciting field. Code for Octopi-1.5 and design files for the TMI gripper
are available at https://github.com/clear-nus/octopi-1.5.

</details>


### [475] [Ariel Explores: Vision-based underwater exploration and inspection via generalist drone-level autonomy](https://arxiv.org/abs/2507.10003)
*Mohit Singh,Mihir Dharmadhikari,Kostas Alexis*

Main category: cs.RO

TL;DR: 提出了一种基于视觉的水下探索与检测自主解决方案，集成到Ariel水下机器人中，通过多相机视觉-惯性状态估计和学习型机器人速度预测方法提升鲁棒性，并在潜艇干船坞中进行了实地测试。


<details>
  <summary>Details</summary>
Motivation: 解决水下环境中视觉退化对自主探索和检测的挑战，提升机器人在复杂视觉条件下的鲁棒性和通用性。

Method: 采用折射感知的多相机视觉-惯性状态估计方法，结合学习型机器人速度预测，集成自主探索和视觉检测解决方案。

Result: 实地测试验证了状态估计的鲁棒性和路径规划技术在不同机器人平台上的通用性。

Conclusion: 该系统在水下复杂视觉条件下表现出色，为水下自主探索和检测提供了可靠解决方案。

Abstract: This work presents a vision-based underwater exploration and inspection
autonomy solution integrated into Ariel, a custom vision-driven underwater
robot. Ariel carries a $5$ camera and IMU based sensing suite, enabling a
refraction-aware multi-camera visual-inertial state estimation method aided by
a learning-based proprioceptive robot velocity prediction method that enhances
robustness against visual degradation. Furthermore, our previously developed
and extensively field-verified autonomous exploration and general visual
inspection solution is integrated on Ariel, providing aerial drone-level
autonomy underwater. The proposed system is field-tested in a submarine dry
dock in Trondheim under challenging visual conditions. The field demonstration
shows the robustness of the state estimation solution and the generalizability
of the path planning techniques across robot embodiments.

</details>


### [476] [Finetuning Deep Reinforcement Learning Policies with Evolutionary Strategies for Control of Underactuated Robots](https://arxiv.org/abs/2507.10030)
*Marco Calì,Alberto Sinigaglia,Niccolò Turcato,Ruggero Carli,Gian Antonio Susto*

Main category: cs.RO

TL;DR: 本文提出了一种结合深度强化学习（RL）和进化策略（ES）的方法，用于优化欠驱动机器人的控制策略。通过SAC初步训练后，使用SNES进行微调，实验表明该方法显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 深度强化学习在处理复杂控制问题时表现优异，但策略可能需要进一步优化以达到特定任务目标。本文旨在通过进化策略微调RL策略，提升控制性能。

Method: 首先使用SAC训练RL代理，采用近似复杂评分指标的替代奖励函数；随后通过SNES进行零阶优化，直接针对原始评分微调策略。

Result: 实验证明，进化微调显著提升了代理性能，同时保持了高鲁棒性，控制器在竞赛任务中表现优于基线方法。

Conclusion: 结合RL和ES的方法有效优化了欠驱动机器人的控制策略，为复杂任务提供了高性能解决方案。

Abstract: Deep Reinforcement Learning (RL) has emerged as a powerful method for
addressing complex control problems, particularly those involving underactuated
robotic systems. However, in some cases, policies may require refinement to
achieve optimal performance and robustness aligned with specific task
objectives. In this paper, we propose an approach for fine-tuning Deep RL
policies using Evolutionary Strategies (ES) to enhance control performance for
underactuated robots. Our method involves initially training an RL agent with
Soft-Actor Critic (SAC) using a surrogate reward function designed to
approximate complex specific scoring metrics. We subsequently refine this
learned policy through a zero-order optimization step employing the Separable
Natural Evolution Strategy (SNES), directly targeting the original score.
Experimental evaluations conducted in the context of the 2nd AI Olympics with
RealAIGym at IROS 2024 demonstrate that our evolutionary fine-tuning
significantly improves agent performance while maintaining high robustness. The
resulting controllers outperform established baselines, achieving competitive
scores for the competition tasks.

</details>


### [477] [MP-RBFN: Learning-based Vehicle Motion Primitives using Radial Basis Function Networks](https://arxiv.org/abs/2507.10047)
*Marc Kaufeld,Mattia Piccinini,Johannes Betz*

Main category: cs.RO

TL;DR: MP-RBFN是一种基于径向基函数网络的新方法，用于高效学习自动驾驶中的运动基元，结合了采样方法的高性能和优化方法的精确性。


<details>
  <summary>Details</summary>
Motivation: 传统基于优化的运动规划方法计算成本高，而采样方法虽高效但对轨迹几何形状有限制，MP-RBFN旨在结合两者的优势。

Method: MP-RBFN利用径向基函数网络，将采样方法的高保真轨迹生成与车辆动力学的精确描述相结合。

Result: MP-RBFN在生成优化运动基元时比现有半解析方法精度高7倍，且推理时间低。

Conclusion: MP-RBFN在运动规划中表现出色，已集成到采样轨迹规划器中，并开源。

Abstract: This research introduces MP-RBFN, a novel formulation leveraging Radial Basis
Function Networks for efficiently learning Motion Primitives derived from
optimal control problems for autonomous driving. While traditional motion
planning approaches based on optimization are highly accurate, they are often
computationally prohibitive. In contrast, sampling-based methods demonstrate
high performance but impose constraints on the geometric shape of trajectories.
MP-RBFN combines the strengths of both by coupling the high-fidelity trajectory
generation of sampling-based methods with an accurate description of vehicle
dynamics. Empirical results show compelling performance compared to previous
methods, achieving a precise description of motion primitives at low inference
times. MP-RBFN yields a seven times higher accuracy in generating optimized
motion primitives compared to existing semi-analytic approaches. We demonstrate
the practical applicability of MP-RBFN for motion planning by integrating the
method into a sampling-based trajectory planner. MP-RBFN is available as
open-source software at https://github.com/TUM-AVS/RBFN-Motion-Primitives.

</details>


### [478] [Hand Gesture Recognition for Collaborative Robots Using Lightweight Deep Learning in Real-Time Robotic Systems](https://arxiv.org/abs/2507.10055)
*Muhtadin,I Wayan Agus Darmawan,Muhammad Hilmi Rusydiansyah,I Ketut Eddy Purnama,Chastine Fatichah,Mauridhi Hery Purnomo*

Main category: cs.RO

TL;DR: 提出了一种轻量级深度学习手势识别系统，用于自然控制协作机器人，模型仅1,103参数和22KB大小，准确率93.5%。进一步优化后模型仅7KB，成功在UR5机器人上实时测试。


<details>
  <summary>Details</summary>
Motivation: 实现直接自然的人机交互，避免使用额外设备如操纵杆或传感器。

Method: 采用深度学习手势识别模型，结合TensorFlow Lite进行量化和剪枝优化。

Result: 模型准确率93.5%，优化后仅7KB，成功在UR5机器人上实时运行。

Conclusion: 轻量级模型可实现准确响应手势控制，为受限环境中自然人机交互提供新可能。

Abstract: Direct and natural interaction is essential for intuitive human-robot
collaboration, eliminating the need for additional devices such as joysticks,
tablets, or wearable sensors. In this paper, we present a lightweight deep
learning-based hand gesture recognition system that enables humans to control
collaborative robots naturally and efficiently. This model recognizes eight
distinct hand gestures with only 1,103 parameters and a compact size of 22 KB,
achieving an accuracy of 93.5%. To further optimize the model for real-world
deployment on edge devices, we applied quantization and pruning using
TensorFlow Lite, reducing the final model size to just 7 KB. The system was
successfully implemented and tested on a Universal Robot UR5 collaborative
robot within a real-time robotic framework based on ROS2. The results
demonstrate that even extremely lightweight models can deliver accurate and
responsive hand gesture-based control for collaborative robots, opening new
possibilities for natural human-robot interaction in constrained environments.

</details>


### [479] [TGLD: A Trust-Aware Game-Theoretic Lane-Changing Decision Framework for Automated Vehicles in Heterogeneous Traffic](https://arxiv.org/abs/2507.10075)
*Jie Pan,Tianyi Wang,Yangyang Wang,Junfeng Jiao,Christian Claudel*

Main category: cs.RO

TL;DR: 论文提出了一种信任感知的游戏论车道变换决策框架（TGLD），通过动态评估人类驾驶车辆的信任水平，优化自动驾驶车辆的协作策略。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶车辆（AVs）需要与人类驾驶车辆（HVs）在异构交通环境中有效协作，但现有车道变换框架忽视了HVs的动态信任水平，导致行为预测不准确。

Method: 提出TGLD框架，包括多车辆联盟游戏建模、实时信任评估方法，以及考虑社会兼容性目标的策略优化。

Result: 实验验证表明，TGLD能根据HVs的信任水平和驾驶风格调整策略，显著提高车道变换效率并确保安全。

Conclusion: TGLD框架通过信任机制实现了透明且自适应的AV-HV交互，提升了自动驾驶的社会兼容性。

Abstract: Automated vehicles (AVs) face a critical need to adopt socially compatible
behaviors and cooperate effectively with human-driven vehicles (HVs) in
heterogeneous traffic environment. However, most existing lane-changing
frameworks overlook HVs' dynamic trust levels, limiting their ability to
accurately predict human driver behaviors. To address this gap, this study
proposes a trust-aware game-theoretic lane-changing decision (TGLD) framework.
First, we formulate a multi-vehicle coalition game, incorporating fully
cooperative interactions among AVs and partially cooperative behaviors from HVs
informed by real-time trust evaluations. Second, we develop an online trust
evaluation method to dynamically estimate HVs' trust levels during
lane-changing interactions, guiding AVs to select context-appropriate
cooperative maneuvers. Lastly, social compatibility objectives are considered
by minimizing disruption to surrounding vehicles and enhancing the
predictability of AV behaviors, thereby ensuring human-friendly and
context-adaptive lane-changing strategies. A human-in-the-loop experiment
conducted in a highway on-ramp merging scenario validates our TGLD approach.
Results show that AVs can effectively adjust strategies according to different
HVs' trust levels and driving styles. Moreover, incorporating a trust mechanism
significantly improves lane-changing efficiency, maintains safety, and
contributes to transparent and adaptive AV-HV interactions.

</details>


### [480] [Unscented Kalman Filter with a Nonlinear Propagation Model for Navigation Applications](https://arxiv.org/abs/2507.10082)
*Amit Levy,Itzik Klein*

Main category: cs.RO

TL;DR: 提出了一种改进无迹卡尔曼滤波器中sigma点传播的方法，提高了滤波精度和导航性能。


<details>
  <summary>Details</summary>
Motivation: 非线性动态模型对导航误差状态向量的传播至关重要，现有方法可能不够精确。

Method: 通过非线性动态模型传播sigma点，改进无迹卡尔曼滤波器的预测步骤。

Result: 实验表明，该方法提高了滤波精度和导航性能。

Conclusion: 提出的方法在导航应用中具有实际价值，能显著提升性能。

Abstract: The unscented Kalman filter is a nonlinear estimation algorithm commonly used
in navigation applications. The prediction of the mean and covariance matrix is
crucial to the stable behavior of the filter. This prediction is done by
propagating the sigma points according to the dynamic model at hand. In this
paper, we introduce an innovative method to propagate the sigma points
according to the nonlinear dynamic model of the navigation error state vector.
This improves the filter accuracy and navigation performance. We demonstrate
the benefits of our proposed approach using real sensor data recorded by an
autonomous underwater vehicle during several scenarios.

</details>


### [481] [Foundation Model Driven Robotics: A Comprehensive Review](https://arxiv.org/abs/2507.10087)
*Muhammad Tayyab Khan,Ammar Waheed*

Main category: cs.RO

TL;DR: 综述探讨了基础模型（如LLMs和VLMs）在机器人领域的应用，分析了其优势与瓶颈，并提出了未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 基础模型为机器人技术带来了语义理解、高级推理和多模态泛化能力，推动了感知、规划、控制和交互的进步。

Method: 通过结构化综述，分类应用领域（如仿真设计、开放世界执行等），并评估系统级策略的实际可行性。

Result: 基础模型在机器人领域展现出潜力，但也面临数据不足、安全风险和计算限制等瓶颈。

Conclusion: 未来需开发更鲁棒、可解释和具身化的模型，以弥合语义推理与物理智能之间的差距。

Abstract: The rapid emergence of foundation models, particularly Large Language Models
(LLMs) and Vision-Language Models (VLMs), has introduced a transformative
paradigm in robotics. These models offer powerful capabilities in semantic
understanding, high-level reasoning, and cross-modal generalization, enabling
significant advances in perception, planning, control, and human-robot
interaction. This critical review provides a structured synthesis of recent
developments, categorizing applications across simulation-driven design,
open-world execution, sim-to-real transfer, and adaptable robotics. Unlike
existing surveys that emphasize isolated capabilities, this work highlights
integrated, system-level strategies and evaluates their practical feasibility
in real-world environments. Key enabling trends such as procedural scene
generation, policy generalization, and multimodal reasoning are discussed
alongside core bottlenecks, including limited embodiment, lack of multimodal
data, safety risks, and computational constraints. Through this lens, this
paper identifies both the architectural strengths and critical limitations of
foundation model-based robotics, highlighting open challenges in real-time
operation, grounding, resilience, and trust. The review concludes with a
roadmap for future research aimed at bridging semantic reasoning and physical
intelligence through more robust, interpretable, and embodied models.

</details>


### [482] [Physics-Informed Neural Networks with Unscented Kalman Filter for Sensorless Joint Torque Estimation in Humanoid Robots](https://arxiv.org/abs/2507.10105)
*Ines Sorrentino,Giulio Romualdi,Lorenzo Moretti,Silvio Traversaro,Daniele Pucci*

Main category: cs.RO

TL;DR: 提出了一种无需关节扭矩传感器的人形机器人全身扭矩控制框架，结合物理信息神经网络（PINNs）和无迹卡尔曼滤波（UKF），提高了扭矩跟踪精度和能量效率。


<details>
  <summary>Details</summary>
Motivation: 解决人形机器人在无关节扭矩传感器情况下的扭矩控制问题，提升动态环境中的稳定性和适应性。

Method: 使用PINNs建模非线性摩擦，UKF估计关节扭矩，实时扭矩控制架构。

Result: 在ergoCub机器人上验证，扭矩跟踪精度、能量效率和抗干扰能力优于现有方法（RNEA）。

Conclusion: 该框架为无传感器扭矩控制提供了可扩展且实用的解决方案。

Abstract: This paper presents a novel framework for whole-body torque control of
humanoid robots without joint torque sensors, designed for systems with
electric motors and high-ratio harmonic drives. The approach integrates
Physics-Informed Neural Networks (PINNs) for friction modeling and Unscented
Kalman Filtering (UKF) for joint torque estimation, within a real-time torque
control architecture. PINNs estimate nonlinear static and dynamic friction from
joint and motor velocity readings, capturing effects like motor actuation
without joint movement. The UKF utilizes PINN-based friction estimates as
direct measurement inputs, improving torque estimation robustness. Experimental
validation on the ergoCub humanoid robot demonstrates improved torque tracking
accuracy, enhanced energy efficiency, and superior disturbance rejection
compared to the state-of-the-art Recursive Newton-Euler Algorithm (RNEA), using
a dynamic balancing experiment. The framework's scalability is shown by
consistent performance across robots with similar hardware but different
friction characteristics, without re-identification. Furthermore, a comparative
analysis with position control highlights the advantages of the proposed torque
control approach. The results establish the method as a scalable and practical
solution for sensorless torque control in humanoid robots, ensuring torque
tracking, adaptability, and stability in dynamic environments.

</details>


### [483] [Simulations and experiments with assemblies of fiber-reinforced soft actuators](https://arxiv.org/abs/2507.10121)
*Seung Hyun Kim,Jiamiao Guo,Arman Tekinalp,Heng-Sheng Chang,Ugur Akcal,Tixian Wang,Darren Biskup,Benjamin Walt,Girish Chowdhary,Girish Krishnan,Prashant G. Mehta,Mattia Gazzola*

Main category: cs.RO

TL;DR: 开发了一种用于纤维增强弹性体软连续臂（SCAs）的仿真框架，并结合视频跟踪系统进行实验测试和控制设计。


<details>
  <summary>Details</summary>
Motivation: 软连续臂（SCAs）因其机械顺应性在多个领域有广泛应用潜力，但其非线性行为难以控制，限制了实际应用。

Method: 开发了模块化组装的纤维增强弹性体（FREEs）仿真框架，并集成视频跟踪系统进行实验和控制设计。

Result: 提出了一个仿真框架，用于SCAs的实验测试和控制设计。

Conclusion: 该框架为SCAs的实际应用提供了控制设计的支持。

Abstract: Soft continuum arms (SCAs) promise versatile manipulation through mechanical
compliance, for assistive devices, agriculture, search applications, or
surgery. However, SCAs' real-world use is challenging, partly due to their
hard-to-control non-linear behavior. Here, a simulation framework for SCAs
modularly assembled out of fiber reinforced elastomeric enclosures (FREEs) is
developed and integrated with a video-tracking system for experimental testing
and control design.

</details>


### [484] [Probabilistic Human Intent Prediction for Mobile Manipulation: An Evaluation with Human-Inspired Constraints](https://arxiv.org/abs/2507.10131)
*Cesar Alan Contreras,Manolis Chiou,Alireza Rastegarpanah,Michal Szulik,Rustam Stolkin*

Main category: cs.RO

TL;DR: GUIDER是一个双阶段概率框架，用于机器人推断人类意图，在导航和操作阶段均表现优异。


<details>
  <summary>Details</summary>
Motivation: 提高人机协作中机器人对人类意图的准确推断，避免冲突或限制人类控制。

Method: GUIDER通过两个耦合的信念层（导航目标和操作目标）实现意图推断，结合Synergy Map、多视角扫描、U2Net和FastSAM等技术。

Result: 在25次试验中，GUIDER导航稳定性达93-100%，操作稳定性达94-100%，显著优于基线方法。

Conclusion: GUIDER框架在移动操作任务中显著提升了意图推断的准确性和稳定性。

Abstract: Accurate inference of human intent enables human-robot collaboration without
constraining human control or causing conflicts between humans and robots. We
present GUIDER (Global User Intent Dual-phase Estimation for Robots), a
probabilistic framework that enables a robot to estimate the intent of human
operators. GUIDER maintains two coupled belief layers, one tracking navigation
goals and the other manipulation goals. In the Navigation phase, a Synergy Map
blends controller velocity with an occupancy grid to rank interaction areas.
Upon arrival at a goal, an autonomous multi-view scan builds a local 3D cloud.
The Manipulation phase combines U2Net saliency, FastSAM instance saliency, and
three geometric grasp-feasibility tests, with an end-effector kinematics-aware
update rule that evolves object probabilities in real-time. GUIDER can
recognize areas and objects of intent without predefined goals. We evaluated
GUIDER on 25 trials (five participants x five task variants) in Isaac Sim, and
compared it with two baselines, one for navigation and one for manipulation.
Across the 25 trials, GUIDER achieved a median stability of 93-100% during
navigation, compared with 60-100% for the BOIR baseline, with an improvement of
39.5% in a redirection scenario (T5). During manipulation, stability reached
94-100% (versus 69-100% for Trajectron), with a 31.4% difference in a
redirection task (T3). In geometry-constrained trials (manipulation), GUIDER
recognized the object intent three times earlier than Trajectron (median
remaining time to confident prediction 23.6 s vs 7.8 s). These results validate
our dual-phase framework and show improvements in intent inference in both
phases of mobile manipulation tasks.

</details>


### [485] [Robust RL Control for Bipedal Locomotion with Closed Kinematic Chains](https://arxiv.org/abs/2507.10164)
*Egor Maslennikov,Eduard Zaliaev,Nikita Dudorov,Oleg Shamanin,Karanov Dmitry,Gleb Afanasev,Alexey Burkov,Egor Lygin,Simeon Nedelchev,Evgeny Ponomarev*

Main category: cs.RO

TL;DR: 提出了一种强化学习框架，显式结合闭链动力学，显著提升了双足机器人的运动控制鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有强化学习方法将双足机器人的平行机构简化为串行模型，导致仿真到实际的迁移效果不佳。

Method: 采用对称感知损失函数、对抗训练和针对性网络正则化，结合闭链动力学。

Result: 实验表明，该方法在多种地形上实现了稳定运动，性能显著优于基于简化模型的方法。

Conclusion: 显式结合闭链动力学的强化学习框架能有效提升双足机器人的运动控制鲁棒性。

Abstract: Developing robust locomotion controllers for bipedal robots with closed
kinematic chains presents unique challenges, particularly since most
reinforcement learning (RL) approaches simplify these parallel mechanisms into
serial models during training. We demonstrate that this simplification
significantly impairs sim-to-real transfer by failing to capture essential
aspects such as joint coupling, friction dynamics, and motor-space control
characteristics. In this work, we present an RL framework that explicitly
incorporates closed-chain dynamics and validate it on our custom-built robot
TopA. Our approach enhances policy robustness through symmetry-aware loss
functions, adversarial training, and targeted network regularization.
Experimental results demonstrate that our integrated approach achieves stable
locomotion across diverse terrains, significantly outperforming methods based
on simplified kinematic models.

</details>


### [486] [REACT: Real-time Entanglement-Aware Coverage Path Planning for Tethered Underwater Vehicles](https://arxiv.org/abs/2507.10204)
*Abdelhakim Amer,Mohit Mehindratta,Yury Brodskiy,Bilal Wehbe,Erdal Kayacan*

Main category: cs.RO

TL;DR: REACT框架通过实时几何模型和路径规划，解决水下机器人因缆绳缠绕导致的任务受限问题，实现安全高效的全覆盖检测。


<details>
  <summary>Details</summary>
Motivation: 传统水下机器人检测复杂结构时，缆绳缠绕风险限制了任务效率和安全性。

Method: 提出REACT框架，基于几何模型（SDF）实时模拟缆绳状态，结合路径规划主动避免缠绕。

Result: 仿真和实验表明，REACT在保证全覆盖的同时，任务完成时间缩短20%，且无缠绕发生。

Conclusion: REACT有效解决了水下机器人检测中的缆绳缠绕问题，提升了任务效率和安全性。

Abstract: Inspection of complex underwater structures with tethered underwater vehicles
is often hindered by the risk of tether entanglement. We propose REACT
(real-time entanglement-aware coverage path planning for tethered underwater
vehicles), a framework designed to overcome this limitation. REACT comprises a
fast geometry-based tether model using the signed distance field (SDF) map for
accurate, real-time simulation of taut tether configurations around arbitrary
structures in 3D. This model enables an efficient online replanning strategy by
enforcing a maximum tether length constraint, thereby actively preventing
entanglement. By integrating REACT into a coverage path planning framework, we
achieve safe and optimal inspection paths, previously challenging due to tether
constraints. The complete REACT framework's efficacy is validated in a pipe
inspection scenario, demonstrating safe, entanglement-free navigation and
full-coverage inspection. Simulation results show that REACT achieves complete
coverage while maintaining tether constraints and completing the total mission
20% faster than conventional planners, despite a longer inspection time due to
proactive avoidance of entanglement that eliminates extensive post-mission
disentanglement. Real-world experiments confirm these benefits, where REACT
completes the full mission, while the baseline planner fails due to physical
tether entanglement.

</details>


### [487] [TOP: Trajectory Optimization via Parallel Optimization towards Constant Time Complexity](https://arxiv.org/abs/2507.10290)
*Jiajun Yu,Nanhe Chen,Guodong Liu,Chao Xu,Fei Gao,Yanjun Cao*

Main category: cs.RO

TL;DR: 提出了一种基于CADMM算法的轨迹优化框架，通过并行计算解决大规模长轨迹优化问题，显著提升了效率和平滑性。


<details>
  <summary>Details</summary>
Motivation: 现有轨迹优化方法在处理大规模长轨迹时效率不足，并行计算的应用潜力尚未充分挖掘。

Method: 采用CADMM算法将轨迹分解为多段并行求解，引入闭式解加速优化，并提供数值解处理一般不等式约束。

Result: 在仿真和实验中，该方法在效率和平滑性上优于现有技术，尤其在大规模轨迹上实现了十倍以上的加速。

Conclusion: 该框架在GPU上部署后表现出色，展示了并行计算在轨迹优化中的巨大潜力。

Abstract: Optimization has been widely used to generate smooth trajectories for motion
planning. However, existing trajectory optimization methods show weakness when
dealing with large-scale long trajectories. Recent advances in parallel
computing have accelerated optimization in some fields, but how to efficiently
solve trajectory optimization via parallelism remains an open question. In this
paper, we propose a novel trajectory optimization framework based on the
Consensus Alternating Direction Method of Multipliers (CADMM) algorithm, which
decomposes the trajectory into multiple segments and solves the subproblems in
parallel. The proposed framework reduces the time complexity to O(1) per
iteration to the number of segments, compared to O(N) of the state-of-the-art
(SOTA) approaches. Furthermore, we introduce a closed-form solution that
integrates convex linear and quadratic constraints to speed up the
optimization, and we also present numerical solutions for general inequality
constraints. A series of simulations and experiments demonstrate that our
approach outperforms the SOTA approach in terms of efficiency and smoothness.
Especially for a large-scale trajectory, with one hundred segments, achieving
over a tenfold speedup. To fully explore the potential of our algorithm on
modern parallel computing architectures, we deploy our framework on a GPU and
show high performance with thousands of segments.

</details>


### [488] [Polygonal Obstacle Avoidance Combining Model Predictive Control and Fuzzy Logic](https://arxiv.org/abs/2507.10310)
*Michael Schröder,Eric Schöneberg,Daniel Görges,Hans D. Schotten*

Main category: cs.RO

TL;DR: 论文提出了一种将离散的占用网格地图转化为连续可微函数的方法，以解决MPC中障碍物避障约束的兼容性问题。


<details>
  <summary>Details</summary>
Motivation: 解决MPC在导航任务中因离散障碍物表示（如占用网格地图）与连续可微约束要求不兼容的问题。

Method: 将障碍物表示为多边形（半空间的交集），利用模糊逻辑将逻辑运算符（AND/OR）转化为不等式约束，使其适用于标准MPC。

Result: 在仿真中成功测试了基于MPC的轨迹规划器。

Conclusion: 该方法不仅适用于导航任务，还可用于MPC中逻辑或语言约束的实现。

Abstract: In practice, navigation of mobile robots in confined environments is often
done using a spatially discrete cost-map to represent obstacles. Path following
is a typical use case for model predictive control (MPC), but formulating
constraints for obstacle avoidance is challenging in this case. Typically the
cost and constraints of an MPC problem are defined as closed-form functions and
typical solvers work best with continuously differentiable functions. This is
contrary to spatially discrete occupancy grid maps, in which a grid's value
defines the cost associated with occupancy. This paper presents a way to
overcome this compatibility issue by re-formulating occupancy grid maps to
continuously differentiable functions to be embedded into the MPC scheme as
constraints. Each obstacle is defined as a polygon -- an intersection of
half-spaces. Any half-space is a linear inequality representing one edge of a
polygon. Using AND and OR operators, the combined set of all obstacles and
therefore the obstacle avoidance constraints can be described. The key
contribution of this paper is the use of fuzzy logic to re-formulate such
constraints that include logical operators as inequality constraints which are
compatible with standard MPC formulation. The resulting MPC-based trajectory
planner is successfully tested in simulation. This concept is also applicable
outside of navigation tasks to implement logical or verbal constraints in MPC.

</details>


### [489] [Raci-Net: Ego-vehicle Odometry Estimation in Adverse Weather Conditions](https://arxiv.org/abs/2507.10376)
*Mohammadhossein Talebi,Pragyan Dahal,Davide Possenti,Stefano Arrigoni,Francesco Braghin*

Main category: cs.RO

TL;DR: 提出了一种基于深度学习的运动估计器，融合视觉、惯性和毫米波雷达数据，提升恶劣环境下的里程估计精度和可靠性。


<details>
  <summary>Details</summary>
Motivation: 解决现有方法在动态环境因素（如天气）下性能下降的问题。

Method: 采用先进的传感器融合技术，动态调整各传感器的贡献，雷达补偿视觉传感器在低能见度下的不足。

Result: 在Boreas数据集上的实验表明，模型在清晰和恶劣环境下均表现出鲁棒性和有效性。

Conclusion: 雷达在不同天气条件下的鲁棒性使其成为姿态估计系统的宝贵组件，尤其在视觉传感器性能下降时。

Abstract: Autonomous driving systems are highly dependent on sensors like cameras,
LiDAR, and inertial measurement units (IMU) to perceive the environment and
estimate their motion. Among these sensors, perception-based sensors are not
protected from harsh weather and technical failures. Although existing methods
show robustness against common technical issues like rotational misalignment
and disconnection, they often degrade when faced with dynamic environmental
factors like weather conditions. To address these problems, this research
introduces a novel deep learning-based motion estimator that integrates visual,
inertial, and millimeter-wave radar data, utilizing each sensor strengths to
improve odometry estimation accuracy and reliability under adverse
environmental conditions such as snow, rain, and varying light. The proposed
model uses advanced sensor fusion techniques that dynamically adjust the
contributions of each sensor based on the current environmental condition, with
radar compensating for visual sensor limitations in poor visibility. This work
explores recent advancements in radar-based odometry and highlights that radar
robustness in different weather conditions makes it a valuable component for
pose estimation systems, specifically when visual sensors are degraded.
Experimental results, conducted on the Boreas dataset, showcase the robustness
and effectiveness of the model in both clear and degraded environments.

</details>


### [490] [Scene-Aware Conversational ADAS with Generative AI for Real-Time Driver Assistance](https://arxiv.org/abs/2507.10500)
*Kyungtae Han,Yitao Chen,Rohit Gupta,Onur Altintas*

Main category: cs.RO

TL;DR: SC-ADAS是一个结合生成式AI的模块化框架，通过多轮对话和场景感知提升ADAS的交互性和适应性。


<details>
  <summary>Details</summary>
Motivation: 当前ADAS系统缺乏场景理解和自然语言交互能力，限制了其在动态环境中的灵活性。

Method: SC-ADAS整合了大型语言模型、视觉到文本解释和结构化功能调用，支持基于视觉和传感器上下文的多轮对话。

Result: 在CARLA模拟器中实现，SC-ADAS展示了场景感知、对话和多轮交互的可行性，但也存在延迟和令牌增长的权衡。

Conclusion: SC-ADAS证明了结合对话推理、场景感知和模块化ADAS控制的可行性，为下一代智能驾驶辅助系统提供了方向。

Abstract: While autonomous driving technologies continue to advance, current Advanced
Driver Assistance Systems (ADAS) remain limited in their ability to interpret
scene context or engage with drivers through natural language. These systems
typically rely on predefined logic and lack support for dialogue-based
interaction, making them inflexible in dynamic environments or when adapting to
driver intent. This paper presents Scene-Aware Conversational ADAS (SC-ADAS), a
modular framework that integrates Generative AI components including large
language models, vision-to-text interpretation, and structured function calling
to enable real-time, interpretable, and adaptive driver assistance. SC-ADAS
supports multi-turn dialogue grounded in visual and sensor context, allowing
natural language recommendations and driver-confirmed ADAS control. Implemented
in the CARLA simulator with cloud-based Generative AI, the system executes
confirmed user intents as structured ADAS commands without requiring model
fine-tuning. We evaluate SC-ADAS across scene-aware, conversational, and
revisited multi-turn interactions, highlighting trade-offs such as increased
latency from vision-based context retrieval and token growth from accumulated
dialogue history. These results demonstrate the feasibility of combining
conversational reasoning, scene perception, and modular ADAS control to support
the next generation of intelligent driver assistance.

</details>


### [491] [MP1: Mean Flow Tames Policy Learning in 1-step for Robotic Manipulation](https://arxiv.org/abs/2507.10543)
*Juyi Sheng,Ziyi Wang,Peiming Li,Mengyuan Liu*

Main category: cs.RO

TL;DR: MP1结合MeanFlow范式，通过单次网络评估生成动作轨迹，避免了扩散模型的慢速采样和Flow方法的架构限制，提升了精度和速度。


<details>
  <summary>Details</summary>
Motivation: 解决生成模型在机器人操作中面临的扩散模型采样慢和Flow方法架构限制的问题。

Method: 采用MeanFlow Identity直接学习区间平均速度，结合CFG提升可控性，并引入Dispersive Loss增强泛化能力。

Result: 在Adroit和Meta-World基准测试中，MP1任务成功率比DP3高10.2%，比FlowPolicy高7.3%，推理速度快19倍。

Conclusion: MP1通过创新方法显著提升了机器人学习任务的性能和效率。

Abstract: In robot manipulation, robot learning has become a prevailing approach.
However, generative models within this field face a fundamental trade-off
between the slow, iterative sampling of diffusion models and the architectural
constraints of faster Flow-based methods, which often rely on explicit
consistency losses. To address these limitations, we introduce MP1, which pairs
3D point-cloud inputs with the MeanFlow paradigm to generate action
trajectories in one network function evaluation (1-NFE). By directly learning
the interval-averaged velocity via the MeanFlow Identity, our policy avoids any
additional consistency constraints. This formulation eliminates numerical
ODE-solver errors during inference, yielding more precise trajectories. MP1
further incorporates CFG for improved trajectory controllability while
retaining 1-NFE inference without reintroducing structural constraints. Because
subtle scene-context variations are critical for robot learning, especially in
few-shot learning, we introduce a lightweight Dispersive Loss that repels state
embeddings during training, boosting generalization without slowing inference.
We validate our method on the Adroit and Meta-World benchmarks, as well as in
real-world scenarios. Experimental results show MP1 achieves superior average
task success rates, outperforming DP3 by 10.2% and FlowPolicy by 7.3%. Its
average inference time is only 6.8 ms-19x faster than DP3 and nearly 2x faster
than FlowPolicy. Our code is available at https://mp1-2254.github.io/.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [492] [Less Stress, More Privacy: Stress Detection on Anonymized Speech of Air Traffic Controllers](https://arxiv.org/abs/2507.08882)
*Janaki Viswanathan,Alexander Blatt,Konrad Hagemann,Dietrich Klakow*

Main category: cs.SD

TL;DR: 论文探讨了在隐私保护下通过匿名化语音数据检测空中交通管制员（ATCO）压力的方法，展示了高性能的深度学习模型。


<details>
  <summary>Details</summary>
Motivation: 空中交通管制（ATC）工作压力大且容错率低，检测压力对维持安全标准至关重要，但语音数据处理涉及隐私限制（如GDPR）。

Method: 评估了不同架构在匿名化ATCO语音数据上的压力检测性能，使用了匿名化的SUSAS数据集和ATC模拟数据集。

Result: 最佳网络在匿名化SUSAS数据集上达到93.6%的准确率，在匿名化ATC模拟数据集上达到80.1%的准确率。

Conclusion: 隐私保护不一定会阻碍高性能深度学习模型的构建。

Abstract: Air traffic control (ATC) demands multi-tasking under time pressure with high
consequences of an error. This can induce stress. Detecting stress is a key
point in maintaining the high safety standards of ATC. However, processing ATC
voice data entails privacy restrictions, e.g. the General Data Protection
Regulation (GDPR) law. Anonymizing the ATC voice data is one way to comply with
these restrictions. In this paper, different architectures for stress detection
for anonymized ATCO speech are evaluated. Our best networks reach a stress
detection accuracy of 93.6% on an anonymized version of the Speech Under
Simulated and Actual Stress (SUSAS) dataset and an accuracy of 80.1% on our
anonymized ATC simulation dataset. This shows that privacy does not have to be
an impediment in building well-performing deep-learning-based models.

</details>


### [493] [Mixture of LoRA Experts with Multi-Modal and Multi-Granularity LLM Generative Error Correction for Accented Speech Recognition](https://arxiv.org/abs/2507.09116)
*Bingshen Mu,Kun Wei,Pengcheng Guo,Lei Xie*

Main category: cs.SD

TL;DR: 论文提出多模态和多粒度的生成式错误校正（GER）方法，结合发音信息和语义信息，显著提升带口音语音识别的准确性。


<details>
  <summary>Details</summary>
Motivation: 尽管ASR技术已有显著进步，但在面对口音等不利条件时性能仍会下降。现有GER方法在口音场景中缺乏针对性。

Method: 提出多模态GER（整合发音信息）和多粒度GER（整合音素级信息），通过LoRA微调结合发音和语义信息。采用三阶段训练策略和HDMoLE方法合并多口音专家。

Result: 在多口音英语数据集上，相比Whisper-large-v3基线，WER相对降低67.35%。

Conclusion: 多模态和多粒度GER方法有效提升了带口音语音识别的准确性，解决了现有方法的局限性。

Abstract: Despite substantial improvements in ASR, performance tends to degrade when
faced with adverse conditions such as speaker accents. Generative error
correction (GER) leverages the rich linguistic knowledge and exceptional
reasoning ability of LLMs, significantly outperforming typical LM methods.
However, it lacks specificity in accented speech scenarios. In this study, we
leverage GER to improve the accuracy of transcription predictions by addressing
the two primary features of accented speech recognition. To fully leverage
pronunciation information, we propose the multi-modal GER, which integrates
pronunciation information from the speech modality, and the multi-granularity
GER, which incorporates fine-grained phoneme-level information related to
pronunciation. These two methods enable the LLM to utilize the pronunciation
information of accented speech and the semantic information from word-level
hypotheses for accurate transcription predictions through LoRA fine-tuning. On
the one hand, we employ a three-stage training strategy to train separate
multi-modal GER models for each accent to obtain mono-accent LoRA experts. By
adopting our proposed HDMoLE method, which incorporates hierarchical routing
and dynamic thresholds within the mixture of LoRA experts, we effectively merge
multiple mono-accent LoRA experts within a single multi-modal GER to overcome
the challenges posed by accent diversity. On the other hand, multi-granularity
GER leverages the N-best word-level and phoneme-level hypotheses generated by
the HDMoLE model to predict the final accented speech transcriptions.
Experimental results on the multi-accent English dataset demonstrate the
efficacy of our proposed methods. Our methods achieve a remarkable relative WER
reduction of 67.35% compared to the Whisper-large-v3 baseline.

</details>


### [494] [Towards Spatial Audio Understanding via Question Answering](https://arxiv.org/abs/2507.09195)
*Parthasaarathy Sudarsanam,Archontis Politis*

Main category: cs.SD

TL;DR: 论文提出了一种基于问答范式的新型空间音频理解框架，旨在扩展声音事件定位与检测（SELD）的范围，实现空间场景的理解与推理。


<details>
  <summary>Details</summary>
Motivation: 扩展SELD的范围，通过语言引导实现更全面的空间音频理解。

Method: 1. 为STARSS23数据集生成细粒度时空文本描述；2. 利用LLM增强语言多样性；3. 构建QA数据集；4. 开发基于FOA信号和自然语言问题的基线模型。

Result: 模型仅通过场景级QA监督训练，性能接近全监督的SELD模型。

Conclusion: 语言引导方法在空间音频理解中具有潜力，为语言监督与空间场景分析的结合开辟了新方向。

Abstract: In this paper, we introduce a novel framework for spatial audio understanding
of first-order ambisonic (FOA) signals through a question answering (QA)
paradigm, aiming to extend the scope of sound event localization and detection
(SELD) towards spatial scene understanding and reasoning. First, we curate and
release fine-grained spatio-temporal textual descriptions for the STARSS23
dataset using a rule-based approach, and further enhance linguistic diversity
using large language model (LLM)-based rephrasing. We also introduce a QA
dataset aligned with the STARSS23 scenes, covering various aspects such as
event presence, localization, spatial, and temporal relationships. To increase
language variety, we again leverage LLMs to generate multiple rephrasings per
question. Finally, we develop a baseline spatial audio QA model that takes FOA
signals and natural language questions as input and provides answers regarding
various occurrences, temporal, and spatial relationships of sound events in the
scene formulated as a classification task. Despite being trained solely with
scene-level question answering supervision, our model achieves performance that
is comparable to a fully supervised sound event localization and detection
model trained with frame-level spatiotemporal annotations. The results
highlight the potential of language-guided approaches for spatial audio
understanding and open new directions for integrating linguistic supervision
into spatial scene analysis.

</details>


### [495] [Voice Conversion for Lombard Speaking Style with Implicit and Explicit Acoustic Feature Conditioning](https://arxiv.org/abs/2507.09310)
*Dominika Woszczyk,Manuel Sam Ribeiro,Thomas Merritt,Daniel Korzekwa*

Main category: cs.SD

TL;DR: 论文研究了通过语音转换技术实现Lombard说话风格的转换，比较了隐式和显式声学特征条件模型的效果。


<details>
  <summary>Details</summary>
Motivation: Lombard风格的TTS系统在听力损失和嘈杂环境下能提高语音清晰度，但数据收集困难，因此探索语音转换技术作为替代方案。

Method: 比较了隐式和显式声学特征条件的语音转换模型，用于实现说话人身份转换同时保留Lombard风格。

Result: 隐式条件模型在保持说话人相似性的同时，达到了与显式条件模型相当的清晰度提升。

Conclusion: 隐式条件策略在Lombard风格转换中表现良好，为数据稀缺问题提供了有效解决方案。

Abstract: Text-to-Speech (TTS) systems in Lombard speaking style can improve the
overall intelligibility of speech, useful for hearing loss and noisy
conditions. However, training those models requires a large amount of data and
the Lombard effect is challenging to record due to speaker and noise
variability and tiring recording conditions. Voice conversion (VC) has been
shown to be a useful augmentation technique to train TTS systems in the absence
of recorded data from the target speaker in the target speaking style. In this
paper, we are concerned with Lombard speaking style transfer. Our goal is to
convert speaker identity while preserving the acoustic attributes that define
the Lombard speaking style. We compare voice conversion models with implicit
and explicit acoustic feature conditioning. We observe that our proposed
implicit conditioning strategy achieves an intelligibility gain comparable to
the model conditioned on explicit acoustic features, while also preserving
speaker similarity.

</details>


### [496] [BENYO-S2ST-Corpus-1: A Bilingual English-to-Yoruba Direct Speech-to-Speech Translation Corpus](https://arxiv.org/abs/2507.09342)
*Emmanuel Adetiba,Abdultaofeek Abayomi,Raymond J. Kala,Ayodele H. Ifijeh,Oluwatobi E. Dare,Olabode Idowu-Bismark,Gabriel O. Sobola,Joy N. Adetiba,Monsurat Adepeju Lateef,Heather Cole-Lewis*

Main category: cs.SD

TL;DR: 该研究创建了一个英语到约鲁巴语的双语语音翻译语料库（BENYO-S2ST-Corpus-1），通过混合架构和音频增强算法生成大规模数据集，并开发了一个约鲁巴语TTS模型作为概念验证。


<details>
  <summary>Details</summary>
Motivation: 解决高资源到低资源语言对（如英语到约鲁巴语）的语音翻译数据集短缺问题。

Method: 利用现有约鲁巴语音频和转录数据，通过预训练AI模型生成英语音频，并开发音频增强算法（AcoustAug）扩展数据集。

Result: 生成了包含24,064个样本（41.20小时）的双语语料库，并开发了约鲁巴语TTS模型（YoruTTS-0.5），其F0 RMSE值为63.54。

Conclusion: 该语料库架构可用于构建多语言高资源到低资源非洲语言的数据集，缩小翻译领域的数字鸿沟。

Abstract: There is a major shortage of Speech-to-Speech Translation (S2ST) datasets for
high resource-to-low resource language pairs such as English-to-Yoruba. Thus,
in this study, we curated the Bilingual English-to-Yoruba Speech-to-Speech
Translation Corpus Version 1 (BENYO-S2ST-Corpus-1). The corpus is based on a
hybrid architecture we developed for large-scale direct S2ST corpus creation at
reduced cost. To achieve this, we leveraged non speech-to-speech Standard
Yoruba (SY) real-time audios and transcripts in the YORULECT Corpus as well as
the corresponding Standard English (SE) transcripts. YORULECT Corpus is small
scale(1,504) samples, and it does not have paired English audios. Therefore, we
generated the SE audios using pre-trained AI models (i.e. Facebook MMS). We
also developed an audio augmentation algorithm named AcoustAug based on three
latent acoustic features to generate augmented audios from the raw audios of
the two languages. BENYO-S2ST-Corpus-1 has 12,032 audio samples per language,
which gives a total of 24,064 sample size. The total audio duration for the two
languages is 41.20 hours. This size is quite significant. Beyond building S2ST
models, BENYO-S2ST-Corpus-1 can be used to build pretrained models or improve
existing ones. The created corpus and Coqui framework were used to build a
pretrained Yoruba TTS model (named YoruTTS-0.5) as a proof of concept. The
YoruTTS-0.5 gave a F0 RMSE value of 63.54 after 1,000 epochs, which indicates
moderate fundamental pitch similarity with the reference real-time audio.
Ultimately, the corpus architecture in this study can be leveraged by
researchers and developers to curate datasets for multilingual
high-resource-to-low-resource African languages. This will bridge the huge
digital divides in translations among high and low-resource language pairs.
BENYO-S2ST-Corpus-1 and YoruTTS-0.5 are publicly available at
(https://bit.ly/40bGMwi).

</details>


### [497] [Acoustic Wave Modeling Using 2D FDTD: Applications in Unreal Engine For Dynamic Sound Rendering](https://arxiv.org/abs/2507.09376)
*Bilkent Samsurya*

Main category: cs.SD

TL;DR: 提出了一种基于2D FDTD的声波传播模拟框架，用于捕捉低频声波现象，并集成到Unreal Engine中。


<details>
  <summary>Details</summary>
Motivation: 现有工业方法未能全面模拟声波现象，影响虚拟应用的沉浸感。

Method: 通过2D网格离散化场景几何，使用Python FDTD求解器模拟声波传播，生成多通道脉冲响应。

Result: 测试结果与理论预期一致，验证了框架的有效性。

Conclusion: 该框架为商业应用提供了可行的声波模拟解决方案，并支持进一步扩展。

Abstract: Accurate sound propagation simulation is essential for delivering immersive
experiences in virtual applications, yet industry methods for acoustic modeling
often do not account for the full breadth of acoustic wave phenomena. This
paper proposes a novel two-dimensional (2D) finite-difference time-domain
(FDTD) framework that simulates sound propagation as a wave-based model in
Unreal Engine, with an emphasis on capturing lower frequency wave phenomena,
embedding occlusion, diffraction, reflection and interference in generated
impulse responses. The process begins by discretizing the scene geometry into a
2D grid via a top-down projection from which obstacle masks and boundary
conditions are derived. A Python-based FDTD solver injects a sine sweep at a
source position, and virtual quadraphonic microphone arrays record pressure
field responses at pre-defined listener positions. De-convolution of the
pressure responses yields multi-channel impulse responses that retain spatial
directionality which are then integrated into Unreal Engine's audio pipeline
for dynamic playback. Benchmark tests confirm agreement with analytical
expectations, and the paper outlines hybrid extensions aimed at commercial
viability.

</details>


### [498] [SC-TSE: Speaker Consistency-Aware Target Speaker Extraction](https://arxiv.org/abs/2507.09510)
*Shu Wu,Anbin Qi,Yanzhang Xie,Xiang Xie*

Main category: cs.SD

TL;DR: 提出了一种基于说话者一致性的目标说话者提取方法，通过引入中心点一致性损失和条件损失抑制提升性能。


<details>
  <summary>Details</summary>
Motivation: 传统TSE系统中说话者嵌入可能存在身份混淆问题，本研究从说话者一致性角度改进性能。

Method: 提出中心点一致性损失和条件损失抑制，确保注册语音与提取语音的说话者一致性。

Result: 实验验证了所提方法在提升TSE性能上的有效性。

Conclusion: 通过说话者一致性优化，显著提升了目标说话者提取的性能。

Abstract: Target Speaker Extraction (TSE) uses a reference cue to extract the target
speech from a mixture. In TSE systems relying on audio cues, the speaker
embedding from the enrolled speech is crucial to performance. However, these
embeddings may suffer from speaker identity confusion. Unlike previous studies
that focus on improving speaker embedding extraction, we improve TSE
performance from the perspective of speaker consistency. In this paper, we
propose a speaker consistency-aware target speaker extraction method that
incorporates a centroid-based speaker consistency loss. This approach enhances
TSE performance by ensuring speaker consistency between the enrolled and
extracted speech. In addition, we integrate conditional loss suppression into
the training process. The experimental results validate the effectiveness of
our proposed methods in advancing the TSE performance. A speech demo is
available online.\footnote{https://sc-tse.netlify.app/

</details>


### [499] [Ensemble Confidence Calibration for Sound Event Detection in Open-environment](https://arxiv.org/abs/2507.09606)
*Yuanjian Chen,Han Yin*

Main category: cs.SD

TL;DR: 本文提出了一种基于能量的开放世界Softmax（EOW-Softmax）方法，用于改进声音事件检测（SED）在开放环境中的鲁棒性，特别是在处理未知场景时的不确定性。


<details>
  <summary>Details</summary>
Motivation: 当前SED方法在开放环境中表现不佳，预测过于自信且缺乏不确定性度量，限制了其在新场景中的适应能力。

Method: 提出EOW-Softmax方法，结合集成方法提升对域外（OOD）输入的鲁棒性，并应用于声音发生和重叠检测（SOD）。

Result: 实验表明，该方法在开放环境中提升了性能，减少了过度自信并增强了处理OOD情况的能力。

Conclusion: EOW-Softmax有效提升了SED在开放环境中的适应性和鲁棒性。

Abstract: Sound event detection (SED) has made strong progress in controlled
environments with clear event categories. However, real-world applications
often take place in open environments. In such cases, current methods often
produce predictions with too much confidence and lack proper ways to measure
uncertainty. This limits their ability to adapt and perform well in new
situations. To solve this problem, we are the first to use ensemble methods in
SED to improve robustness against out-of-domain (OOD) inputs. We propose a
confidence calibration method called Energy-based Open-World Softmax
(EOW-Softmax), which helps the system better handle uncertainty in unknown
scenes. We further apply EOW-Softmax to sound occurrence and overlap detection
(SOD) by adjusting the prediction. In this way, the model becomes more
adaptable while keeping its ability to detect overlapping events. Experiments
show that our method improves performance in open environments. It reduces
overconfidence and increases the ability to handle OOD situations.

</details>


### [500] [THAI Speech Emotion Recognition (THAI-SER) corpus](https://arxiv.org/abs/2507.09618)
*Jilamika Wongpithayadisai,Chompakorn Chaksangchaichot,Soravitt Sangnark,Patawee Prakrankamanant,Krit Gangwanpongpun,Siwa Boonpunmongkol,Premmarin Milindasuta,Dangkamon Na-Pombejra,Sarana Nutanong,Ekapol Chuangsuwanich*

Main category: cs.SD

TL;DR: THAI-SER是一个包含41小时36分钟泰语语音情感识别的语料库，由200名专业演员录制，标注了五种主要情感，并通过众包和严格质量控制确保标注质量。


<details>
  <summary>Details</summary>
Motivation: 构建首个大规模的泰语语音情感识别语料库，填补研究空白，并支持相关模型开发。

Method: 通过专业演员在多种录音环境中录制脚本和即兴会话，使用众包标注情感，并设计质量控制方案。

Result: 语料库的标注质量较高（Krippendorff's alpha为0.692），人类识别准确率达0.772，并提供了模型训练结果。

Conclusion: THAI-SER为泰语语音情感识别研究提供了高质量资源，并公开了数据和代码。

Abstract: We present the first sizeable corpus of Thai speech emotion recognition,
THAI-SER, containing 41 hours and 36 minutes (27,854 utterances) from 100
recordings made in different recording environments: Zoom and two studio
setups. The recordings contain both scripted and improvised sessions, acted by
200 professional actors (112 females and 88 males, aged 18 to 55) and were
directed by professional directors. There are five primary emotions: neutral,
angry, happy, sad, and frustrated, assigned to the actors when recording
utterances. The utterances are annotated with an emotional category using
crowdsourcing. To control the annotation process's quality, we also design an
extensive filtering and quality control scheme to ensure that the majority
agreement score remains above 0.71. We evaluate our annotated corpus using two
metrics: inter-annotator reliability and human recognition accuracy.
Inter-annotator reliability score was calculated using Krippendorff's alpha,
where our corpus, after filtering, achieved an alpha score of 0.692, higher
than a recommendation of 0.667. For human recognition accuracy, our corpus
scored up to 0.772 post-filtering. We also provide the results of the model
trained on the corpus evaluated on both in-corpus and cross-corpus setups. The
corpus is publicly available under a Creative Commons BY-SA 4.0, as well as our
codes for the experiments.

</details>


### [501] [MB-RIRs: a Synthetic Room Impulse Response Dataset with Frequency-Dependent Absorption Coefficients](https://arxiv.org/abs/2507.09750)
*Enric Gusó,Joanna Luberadzka,Umut Sayin,Xavier Serra*

Main category: cs.SD

TL;DR: 研究了四种策略对提升合成房间脉冲响应（RIR）数据集生态效度的影响，发现使用多频带吸收系数的RIR（MB-RIRs）在真实RIR测试中表现最佳。


<details>
  <summary>Details</summary>
Motivation: 探索如何通过改进合成RIR数据集的生态效度来提升单声道语音增强（SE）的性能。

Method: 在传统ISM鞋盒RIR基础上增加多频带吸收系数、声源指向性和接收器指向性，并考虑SoundSpaces的网格RIR，训练DeepFilternet3模型进行测试。

Result: MB-RIRs在真实RIR测试中取得+0.51dB SDR和+8.9 MUSHRA评分的提升。

Conclusion: MB-RIRs数据集效果显著，已公开免费下载。

Abstract: We investigate the effects of four strategies for improving the ecological
validity of synthetic room impulse response (RIR) datasets for monoaural Speech
Enhancement (SE). We implement three features on top of the traditional image
source method-based (ISM) shoebox RIRs: multiband absorption coefficients,
source directivity and receiver directivity. We additionally consider
mesh-based RIRs from the SoundSpaces dataset. We then train a DeepFilternet3
model for each RIR dataset and evaluate the performance on a test set of real
RIRs both objectively and subjectively. We find that RIRs which use
frequency-dependent acoustic absorption coefficients (MB-RIRs) can obtain
+0.51dB of SDR and a +8.9 MUSHRA score when evaluated on real RIRs. The MB-RIRs
dataset is publicly available for free download.

</details>


### [502] [ASTAR-NTU solution to AudioMOS Challenge 2025 Track1](https://arxiv.org/abs/2507.09904)
*Fabian Ritter-Gutierrez,Yi-Cheng Lin,Jui-Chiang Wei,Jeremy H. M. Wong,Nancy F. Chen,Hung-yi Lee*

Main category: cs.SD

TL;DR: 本文介绍了在AudioMOS 2025挑战赛中的获胜系统，通过双分支架构和预训练模型MuQ与RoBERTa，结合交叉注意力机制，将音乐印象（MI）和文本对齐（TA）预测任务转化为分类问题，并利用高斯核处理标签，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 由于专家评估的成本和可用性限制，文本到音乐系统的评估存在挑战。本文旨在通过自动化方法预测音乐印象和文本对齐，以解决这一问题。

Method: 采用双分支架构，预训练的MuQ和RoBERTa模型分别作为音频和文本编码器，通过交叉注意力机制融合特征。将MI和TA预测任务转化为分类问题，并使用高斯核处理标签。

Result: 在官方测试集上，单个模型的SRCC达到0.991（MI）和0.952（TA），相比基线分别提升了21.21%和31.47%。

Conclusion: 该方法在自动化评估文本到音乐系统方面表现出色，显著提升了音乐印象和文本对齐的预测性能。

Abstract: Evaluation of text-to-music systems is constrained by the cost and
availability of collecting experts for assessment. AudioMOS 2025 Challenge
track 1 is created to automatically predict music impression (MI) as well as
text alignment (TA) between the prompt and the generated musical piece. This
paper reports our winning system, which uses a dual-branch architecture with
pre-trained MuQ and RoBERTa models as audio and text encoders. A
cross-attention mechanism fuses the audio and text representations. For
training, we reframe the MI and TA prediction as a classification task. To
incorporate the ordinal nature of MOS scores, one-hot labels are converted to a
soft distribution using a Gaussian kernel. On the official test set, a single
model trained with this method achieves a system-level Spearman's Rank
Correlation Coefficient (SRCC) of 0.991 for MI and 0.952 for TA, corresponding
to a relative improvement of 21.21\% in MI SRCC and 31.47\% in TA SRCC over the
challenge baseline.

</details>


### [503] [DQLoRA: A Lightweight Domain-Aware Denoising ASR via Adapter-guided Distillation](https://arxiv.org/abs/2507.10313)
*Yiru Yang*

Main category: cs.SD

TL;DR: DQLoRA是一个适配器引导的蒸馏框架，用于低资源和噪声条件下的鲁棒语音识别。


<details>
  <summary>Details</summary>
Motivation: 解决低资源和噪声条件下语音识别的鲁棒性问题。

Method: 使用冻结的Whisper模型作为教师提供语义监督，轻量级Wav2Vec2学生模型配备QLoRA适配器，在FLEURS数据集加噪声训练，联合最小化CTC和KL蒸馏损失。

Result: 实现了高效适应并保持识别准确性。

Conclusion: DQLoRA框架在低资源和噪声条件下表现良好。

Abstract: We present a demo of DQLoRA, an Adapter-Guided Distillation framework for
robust speech recognition under low-resource and noisy conditions. Our method
employs a frozen Whisper model as the teacher to provide semantic supervision,
and a lightweight Wav2Vec2 student equipped with QLoRA-based Adapters. Training
is conducted on the FLEURS dataset augmented with DNS-style noise. The student
is optimized by jointly minimizing CTC loss and KL-based distillation loss,
enabling efficient adaptation while preserving recognition accuracy.

</details>


### [504] [Evaluating Fake Music Detection Performance Under Audio Augmentations](https://arxiv.org/abs/2507.10447)
*Tomasz Sroka,Tomasz Wężowicz,Dominik Sidorczuk,Mateusz Modrzejewski*

Main category: cs.SD

TL;DR: 论文探讨了生成音频模型快速发展背景下，检测假音乐模型的鲁棒性，发现即使轻微音频增强也会显著降低分类准确性。


<details>
  <summary>Details</summary>
Motivation: 随着生成音频模型的快速发展，区分人工创作与生成音乐变得困难，需要评估检测模型的鲁棒性。

Method: 构建包含真实与合成音乐的数据集，应用多种音频变换，测试最新音乐深度伪造检测模型的性能。

Result: 模型性能在轻微音频增强下显著下降。

Conclusion: 现有音乐深度伪造检测模型对音频变换的鲁棒性不足，需进一步改进。

Abstract: With the rapid advancement of generative audio models, distinguishing between
human-composed and generated music is becoming increasingly challenging. As a
response, models for detecting fake music have been proposed. In this work, we
explore the robustness of such systems under audio augmentations. To evaluate
model generalization, we constructed a dataset consisting of both real and
synthetic music generated using several systems. We then apply a range of audio
transformations and analyze how they affect classification accuracy. We test
the performance of a recent state-of-the-art musical deepfake detection model
in the presence of audio augmentations. The performance of the model decreases
significantly even with the introduction of light augmentations.

</details>


### [505] [Radif corpus: a symbolic dataset for non-metric iranian classical music](https://arxiv.org/abs/2507.10456)
*Maziar Kanani,Sean O Leary,James McDermott*

Main category: cs.SD

TL;DR: 该研究首次数字化了伊朗古典音乐中的非节拍性radif曲目，提供了MIDI文件和数据表格，支持计算音乐学研究。


<details>
  <summary>Details</summary>
Motivation: 伊朗古典音乐的核心是非节拍性音乐，radif是其基础曲目，但缺乏数字化资源。

Method: 创建了包含13个组件的完整radif曲目数字语料库，提供MIDI文件和数据表格，记录音符、音程和结构等信息。

Result: 生成了228首音乐的281分钟MIDI文件和相关数据，支持复杂性、相似性等统计分析。

Conclusion: 该语料库为伊朗古典音乐的计算研究提供了平台，适用于音乐信息检索、理论分析等领域。

Abstract: Non-metric music forms the core of the repertoire in Iranian classical music.
Dastgahi music serves as the underlying theoretical system for both Iranian art
music and certain folk traditions. At the heart of Iranian classical music lies
the radif, a foundational repertoire that organizes melodic material central to
performance and pedagogy.
  In this study, we introduce the first digital corpus representing the
complete non-metrical radif repertoire, covering all 13 existing components of
this repertoire. We provide MIDI files (about 281 minutes in total) and data
spreadsheets describing notes, note durations, intervals, and hierarchical
structures for 228 pieces of music. We faithfully represent the tonality
including quarter-tones, and the non-metric aspect. Furthermore, we provide
supporting basic statistics, and measures of complexity and similarity over the
corpus.
  Our corpus provides a platform for computational studies of Iranian classical
music. Researchers might employ it in studying melodic patterns, investigating
improvisational styles, or for other tasks in music information retrieval,
music theory, and computational (ethno)musicology.

</details>


### [506] [AudioMAE++: learning better masked audio representations with SwiGLU FFNs](https://arxiv.org/abs/2507.10464)
*Sarthak Yadav,Sergios Theodoridis,Zheng-Hua Tan*

Main category: cs.SD

TL;DR: AudioMAE++是一种改进的音频掩码自编码器，通过引入macaron-style transformer块和门控线性单元，在多个下游任务中表现优于现有MAE方法。


<details>
  <summary>Details</summary>
Motivation: 现有的音频MAE方法仍使用传统transformer模块，而transformer领域已有新架构进展，因此提出改进方案。

Method: 提出AudioMAE++，采用macaron-style transformer块和门控线性单元，并在AudioSet数据集上预训练。

Result: 在10个下游任务中表现优于现有MAE方法，且参数规模扩展性优异。

Conclusion: AudioMAE++在音频分类和语音任务中表现卓越，展示了新架构的有效性。

Abstract: Masked Autoencoders (MAEs) trained on audio spectrogram patches have emerged
as a prominent approach for learning self-supervised audio representations.
While several recent papers have evaluated key aspects of training MAEs on
audio data, the majority of these approaches still leverage vanilla transformer
building blocks, whereas the transformer community has seen steady integration
of newer architectural advancements. In this work, we propose AudioMAE++, a
revamped audio masked autoencoder with two such enhancements, namely
macaron-style transformer blocks with gated linear units. When pretrained on
the AudioSet dataset, the proposed AudioMAE++ models outperform existing MAE
based approaches on 10 diverse downstream tasks, demonstrating excellent
performance on audio classification and speech-based benchmarks. The proposed
AudioMAE++ models also demonstrate excellent scaling characteristics,
outperforming directly comparable standard MAE baselines with up to 4x more
parameters.

</details>


### [507] [WildFX: A DAW-Powered Pipeline for In-the-Wild Audio FX Graph Modeling](https://arxiv.org/abs/2507.10534)
*Qihui Yang,Taylor Berg-Kirkpatrick,Julian McAuley,Zachary Novack*

Main category: cs.SD

TL;DR: WildFX是一个基于Docker的管道，用于生成多轨音频混合数据集，支持专业DSP工作流，并实现了AI研究与实际需求的桥梁。


<details>
  <summary>Details</summary>
Motivation: 当前AI方法难以复现专业音频效果图中的信号流和参数交互，现有可微分插件方法性能不足。

Method: WildFX通过Docker容器化管道，支持多格式插件集成，实现高效并行处理和简化配置。

Result: 实验验证了WildFX在盲估计混合图和插件参数方面的有效性。

Conclusion: WildFX成功连接了AI研究与实际DSP需求，代码已开源。

Abstract: Despite rapid progress in end-to-end AI music generation, AI-driven modeling
of professional Digital Signal Processing (DSP) workflows remains challenging.
In particular, while there is growing interest in neural black-box modeling of
audio effect graphs (e.g. reverb, compression, equalization), AI-based
approaches struggle to replicate the nuanced signal flow and parameter
interactions used in professional workflows. Existing differentiable plugin
approaches often diverge from real-world tools, exhibiting inferior performance
relative to simplified neural controllers under equivalent computational
constraints. We introduce WildFX, a pipeline containerized with Docker for
generating multi-track audio mixing datasets with rich effect graphs, powered
by a professional Digital Audio Workstation (DAW) backend. WildFX supports
seamless integration of cross-platform commercial plugins or any plugins in the
wild, in VST/VST3/LV2/CLAP formats, enabling structural complexity (e.g.,
sidechains, crossovers) and achieving efficient parallelized processing. A
minimalist metadata interface simplifies project/plugin configuration.
Experiments demonstrate the pipeline's validity through blind estimation of
mixing graphs, plugin/gain parameters, and its ability to bridge AI research
with practical DSP demands. The code is available on:
https://github.com/IsaacYQH/WildFX.

</details>


<div id='cs.LO'></div>

# cs.LO [[Back]](#toc)

### [508] [Extending Defeasibility for Propositional Standpoint Logics](https://arxiv.org/abs/2507.10133)
*Nicholas Leisegang,Thomas Meyer,Ivan Varzinczak*

Main category: cs.LO

TL;DR: 本文提出了一种新的可废止命题立场逻辑框架，结合了多种理论，并提供了优先语义和表计算法。


<details>
  <summary>Details</summary>
Motivation: 整合不同理论中的可废止性概念，扩展立场逻辑的表达能力。

Method: 结合Kraus等人的可废止条件、Britz和Varzinczak的可废止必要性及可能性概念，以及Leisegang等人的方法，构建新的逻辑框架，并提供优先语义和表计算法。

Result: 提出的表计算法在优先蕴涵下是完备的，计算复杂度为PSpace。

Conclusion: 新框架成功整合了多种可废止性概念，为立场逻辑提供了更丰富的表达能力和计算工具。

Abstract: In this paper, we introduce a new defeasible version of propositional
standpoint logic by integrating Kraus et al.'s defeasible conditionals, Britz
and Varzinczak's notions of defeasible necessity and distinct possibility,
along with Leisegang et al.'s approach to defeasibility into the standpoint
logics of G\'omez \'Alvarez and Rudolph. The resulting logical framework allows
for the expression of defeasibility on the level of implications, standpoint
modal operators, and standpoint-sharpening statements. We provide a
preferential semantics for this extended language and propose a tableaux
calculus, which is shown to be sound and complete with respect to preferential
entailment. We also establish the computational complexity of the tableaux
procedure to be in PSpace.

</details>


<div id='q-bio.NC'></div>

# q-bio.NC [[Back]](#toc)

### [509] [CNeuroMod-THINGS, a densely-sampled fMRI dataset for visual neuroscience](https://arxiv.org/abs/2507.09024)
*Marie St-Laurent,Basile Pinsard,Oliver Contier,Elizabeth DuPre,Katja Seeliger,Valentina Borghesani,Julie A. Boyle,Lune Bellec,Martin N. Hebart*

Main category: q-bio.NC

TL;DR: CNeuroMod-THINGS整合了两个现有项目（THINGS和CNeuroMod），创建了一个大规模、密集采样的fMRI数据集，用于建模广泛的语义概念。


<details>
  <summary>Details</summary>
Motivation: 满足神经AI建模对大规模神经影像数据的需求，通过结合THINGS的标注图像和CNeuroMod的密集fMRI数据，扩展对人类视觉体验的建模能力。

Method: 利用THINGS的标注图像集和CNeuroMod的密集fMRI数据采集方法，四名参与者完成了33-36次连续识别任务，覆盖720个类别的4000张图像。

Result: 报告了行为和神经影像指标，展示了数据的高质量。

Conclusion: CNeuroMod-THINGS通过整合现有资源，显著提升了人类视觉体验建模的能力。

Abstract: Data-hungry neuro-AI modelling requires ever larger neuroimaging datasets.
CNeuroMod-THINGS meets this need by capturing neural representations for a wide
set of semantic concepts using well-characterized stimuli in a new
densely-sampled, large-scale fMRI dataset. Importantly, CNeuroMod-THINGS
exploits synergies between two existing projects: the THINGS initiative
(THINGS) and the Courtois Project on Neural Modelling (CNeuroMod). THINGS has
developed a common set of thoroughly annotated images broadly sampling natural
and man-made objects which is used to acquire a growing collection of
large-scale multimodal neural responses. Meanwhile, CNeuroMod is acquiring
hundreds of hours of fMRI data from a core set of participants during
controlled and naturalistic tasks, including visual tasks like movie watching
and videogame playing. For CNeuroMod-THINGS, four CNeuroMod participants each
completed 33-36 sessions of a continuous recognition paradigm using
approximately 4000 images from the THINGS stimulus set spanning 720 categories.
We report behavioural and neuroimaging metrics that showcase the quality of the
data. By bridging together large existing resources, CNeuroMod-THINGS expands
our capacity to model broad slices of the human visual experience.

</details>


### [510] [Self-supervised pretraining of vision transformers for animal behavioral analysis and neural encoding](https://arxiv.org/abs/2507.09513)
*Yanchen Wang,Han Yu,Ari Blau,Yizi Zhang,The International Brain Laboratory,Liam Paninski,Cole Hurwitz,Matt Whiteway*

Main category: q-bio.NC

TL;DR: BEAST是一个通过自监督预训练Transformer的新型框架，用于解决神经行为分析中标记数据稀缺的问题。


<details>
  <summary>Details</summary>
Motivation: 现代神经科学研究强调通过行为理解大脑，但视频分析通常依赖需要大量标记数据的专业模型。

Method: BEAST结合掩码自编码和时间对比学习，利用未标记视频数据预训练实验特定的视觉Transformer。

Result: 在多物种评估中，BEAST在提取与神经活动相关的行为特征、姿态估计和动作分割任务中表现优异。

Conclusion: BEAST为标记数据稀缺的场景提供了强大且通用的行为分析模型。

Abstract: The brain can only be fully understood through the lens of the behavior it
generates -- a guiding principle in modern neuroscience research that
nevertheless presents significant technical challenges. Many studies capture
behavior with cameras, but video analysis approaches typically rely on
specialized models requiring extensive labeled data. We address this limitation
with BEAST (BEhavioral Analysis via Self-supervised pretraining of
Transformers), a novel and scalable framework that pretrains
experiment-specific vision transformers for diverse neuro-behavior analyses.
BEAST combines masked autoencoding with temporal contrastive learning to
effectively leverage unlabeled video data. Through comprehensive evaluation
across multiple species, we demonstrate improved performance in three critical
neuro-behavioral tasks: extracting behavioral features that correlate with
neural activity, and pose estimation and action segmentation in both the
single- and multi-animal settings. Our method establishes a powerful and
versatile backbone model that accelerates behavioral analysis in scenarios
where labeled data remains scarce.

</details>


<div id='cs.DS'></div>

# cs.DS [[Back]](#toc)

### [511] [Covering a Few Submodular Constraints and Applications](https://arxiv.org/abs/2507.09879)
*Tanvi Bajpai,Chandra Chekuri,Pooja Kulkarni*

Main category: cs.DS

TL;DR: 论文研究了覆盖多个子模约束的问题，提出了针对固定常数r的随机双准则近似算法和加权覆盖函数的近似算法。


<details>
  <summary>Details</summary>
Motivation: 解决当r为固定常数时，覆盖多个子模约束的问题，以填补先前研究在r较大时的不足。

Method: 提出了随机双准则近似算法和针对加权覆盖函数的近似算法。

Result: 对于固定r，算法能提供接近r=1时的近似效果，且成本接近最优。

Conclusion: 结果表明，固定r时可以获得与r=1时相近的近似效果，具有广泛的应用潜力。

Abstract: We consider the problem of covering multiple submodular constraints. Given a
finite ground set $N$, a cost function $c: N \rightarrow \mathbb{R}_+$, $r$
monotone submodular functions $f_1,f_2,\ldots,f_r$ over $N$ and requirements
$b_1,b_2,\ldots,b_r$ the goal is to find a minimum cost subset $S \subseteq N$
such that $f_i(S) \ge b_i$ for $1 \le i \le r$. When $r=1$ this is the
well-known Submodular Set Cover problem. Previous work
\cite{chekuri2022covering} considered the setting when $r$ is large and
developed bi-criteria approximation algorithms, and approximation algorithms
for the important special case when each $f_i$ is a weighted coverage function.
These are fairly general models and capture several concrete and interesting
problems as special cases. The approximation ratios for these problem are at
least $\Omega(\log r)$ which is unavoidable when $r$ is part of the input. In
this paper, motivated by some recent applications, we consider the problem when
$r$ is a \emph{fixed constant} and obtain two main results. For covering
multiple submodular constraints we obtain a randomized bi-criteria
approximation algorithm that for any given integer $\alpha \ge 1$ outputs a set
$S$ such that $f_i(S) \ge$ $(1-1/e^\alpha -\epsilon)b_i$ for each $i \in [r]$
and $\mathbb{E}[c(S)] \le (1+\epsilon)\alpha \cdot \sf{OPT}$. Second, when the
$f_i$ are weighted coverage functions from a deletion-closed set system we
obtain a $(1+\epsilon)$ $(\frac{e}{e-1})$ $(1+\beta)$-approximation where
$\beta$ is the approximation ratio for the underlying set cover instances via
the natural LP. These results show that one can obtain nearly as good an
approximation for any fixed $r$ as what one would achieve for $r=1$. We mention
some applications that follow easily from these general results and anticipate
more in the future.

</details>


### [512] [Phase transition of the Sinkhorn-Knopp algorithm](https://arxiv.org/abs/2507.09711)
*Kun He*

Main category: cs.DS

TL;DR: 论文研究了Sinkhorn-Knopp算法的上下界问题，揭示了在密度阈值γ=1/2时的相变现象。


<details>
  <summary>Details</summary>
Motivation: 探索Sinkhorn-Knopp算法的强实证性能原因及其迭代次数的紧界。

Method: 通过分析归一化矩阵的密度γ，给出算法的上下界证明。

Result: 对于γ>1/2的矩阵，算法在O(log n)迭代内收敛；对于γ<1/2的矩阵，存在需要Ω(n^1/2)迭代的实例。

Conclusion: 密度γ=1/2是算法性能的相变点，为理论和实践提供了重要见解。

Abstract: The matrix scaling problem, particularly the Sinkhorn-Knopp algorithm, has
been studied for over 60 years. In practice, the algorithm often yields
high-quality approximations within just a few iterations. Theoretically,
however, the best-known upper bound places it in the class of
pseudopolynomial-time approximation algorithms. Meanwhile, the lower-bound
landscape remains largely unexplored. Two fundamental questions persist: what
accounts for the algorithm's strong empirical performance, and can a tight
bound on its iteration count be established?
  For an $n\times n$ matrix, its normalized version is obtained by dividing
each entry by its largest entry. We say that a normalized matrix has a density
$\gamma$ if there exists a constant $\rho > 0$ such that one row or column has
exactly $\lceil \gamma n \rceil$ entries with values at least $\rho$, and every
other row and column has at least $\lceil \gamma n \rceil$ such entries.
  For the upper bound, we show that the Sinkhorn-Knopp algorithm produces a
nearly doubly stochastic matrix in $O(\log n - \log \varepsilon)$ iterations
and $\widetilde{O}(n^2)$ time for all nonnegative square matrices whose
normalized version has a density $\gamma > 1/2$. Such matrices cover both the
algorithm's principal practical inputs and its typical theoretical regime, and
the $\widetilde{O}(n^2)$ runtime is optimal.
  For the lower bound, we establish a tight bound of
$\widetilde{\Omega}\left(n^{1/2}/\varepsilon\right)$ iterations for positive
matrices under the $\ell_2$-norm error measure. Moreover, for every $\gamma <
1/2$, there exists a matrix with density $\gamma$ for which the algorithm
requires $\Omega\left(n^{1/2}/\varepsilon\right)$ iterations.
  In summary, our results reveal a sharp phase transition in the Sinkhorn-Knopp
algorithm at the density threshold $\gamma = 1/2$.

</details>


<div id='math.DS'></div>

# math.DS [[Back]](#toc)

### [513] [Properties of Quasi-synchronization Time of High-dimensional Hegselmann-Krause Dynamics](https://arxiv.org/abs/2507.08900)
*Wei Su,Meiru Jiang,Yongguang Yu,Ge Chen*

Main category: math.DS

TL;DR: 研究了多维Hegselmann-Krause（HK）模型在噪声驱动下的准同步随机时间，发现空间的有界性和维度决定了不同的结果。


<details>
  <summary>Details</summary>
Motivation: 探索高维空间中HK动力学是否仍能实现准同步，填补现有研究的空白。

Method: 分析多维HK模型在噪声驱动下的行为，研究准同步的随机时间及其与空间特性的关系。

Result: 有界空间中所有维度均能在有限时间内实现准同步；无界空间中仅低维（一维和二维）可实现。

Conclusion: 空间的有界性和维度是决定HK模型准同步行为的关键因素。

Abstract: The behavior of one-dimensional Hegselmann-Krause (HK) dynamics driven by
noise has been extensively studied. Previous research has indicated that within
no matter the bounded or the unbounded space of one dimension, the HK dynamics
attain quasi-synchronization (synchronization in noisy case) in finite time.
However, it remains unclear whether this phenomenon holds in high-dimensional
space. This paper investigates the random time for quasi-synchronization of
multi-dimensional HK model and reveals that the boundedness and dimensions of
the space determine different outcomes. To be specific, if the space is
bounded, quasi-synchronization can be attained almost surely for all dimensions
within a finite time, whereas in unbounded space, quasi-synchronization can
only be achieved in low-dimensional cases (one and two). Furthermore, different
integrability of the random time of various cases is proved.

</details>


<div id='nlin.CD'></div>

# nlin.CD [[Back]](#toc)

### [514] [Machine-Precision Prediction of Low-Dimensional Chaotic Systems](https://arxiv.org/abs/2507.09652)
*Christof Schötz,Niklas Boers*

Main category: nlin.CD

TL;DR: 论文提出了一种通过高精度多项式回归方法，从无噪声数据中学习低维混沌系统动力学的技术，显著提升了预测精度和有效预测时间。


<details>
  <summary>Details</summary>
Motivation: 研究动机是探索如何从无噪声观测数据中高精度学习低维混沌系统的动力学，以解决现有方法预测时间短和精度不足的问题。

Method: 方法采用高次多项式特征的最小二乘回归，结合512位高精度算术运算，显著提升了学习精度。

Result: 在Lorenz-63系统中，预测时间达到32至105个Lyapunov时间，远超之前工作的13个Lyapunov时间；并在更复杂的Thomas' Cyclically Symmetric Attractor和Lorenz-96模型中验证了方法的有效性。

Conclusion: 结论表明，从无噪声数据中学习低维混沌系统动力学的问题已得到解决。

Abstract: Low-dimensional chaotic systems such as the Lorenz-63 model are commonly used
to benchmark system-agnostic methods for learning dynamics from data. Here we
show that learning from noise-free observations in such systems can be achieved
up to machine precision: using ordinary least squares regression on high-degree
polynomial features with 512-bit arithmetic, our method exceeds the accuracy of
standard 64-bit numerical ODE solvers of the true underlying dynamical systems.
Depending on the configuration, we obtain valid prediction times of 32 to 105
Lyapunov times for the Lorenz-63 system, dramatically outperforming prior work
that reaches 13 Lyapunov times at most. We further validate our results on
Thomas' Cyclically Symmetric Attractor, a non-polynomial chaotic system that is
considerably more complex than the Lorenz-63 model, and show that similar
results extend also to higher dimensions using the spatiotemporally chaotic
Lorenz-96 model. Our findings suggest that learning low-dimensional chaotic
systems from noise-free data is a solved problem.

</details>


<div id='cs.NE'></div>

# cs.NE [[Back]](#toc)

### [515] [Credit Card Fraud Detection Using RoFormer Model With Relative Distance Rotating Encoding](https://arxiv.org/abs/2507.09385)
*Kevin Reyes,Vasco Cortez*

Main category: cs.NE

TL;DR: 提出了一种结合ReDRE的RoFormer模型新方法，用于提升交易欺诈检测的准确性。


<details>
  <summary>Details</summary>
Motivation: 金融系统需高效检测欺诈交易以降低风险，提升用户体验和业务可持续性。

Method: 在RoFormer模型中引入ReDRE，通过角度旋转增强时间序列数据的表征能力。

Result: 改进的模型能更好地捕捉时间依赖性和事件关系，从而提升欺诈检测效果。

Conclusion: ReDRE与RoFormer的结合为欺诈检测提供了更高效的方法。

Abstract: Fraud detection is one of the most important challenges that financial
systems must address. Detecting fraudulent transactions is critical for payment
gateway companies like Flow Payment, which process millions of transactions
monthly and require robust security measures to mitigate financial risks.
Increasing transaction authorization rates while reducing fraud is essential
for providing a good user experience and building a sustainable business. For
this reason, discovering novel and improved methods to detect fraud requires
continuous research and investment for any company that wants to succeed in
this industry. In this work, we introduced a novel method for detecting
transactional fraud by incorporating the Relative Distance Rotating Encoding
(ReDRE) in the RoFormer model. The incorporation of angle rotation using ReDRE
enhances the characterization of time series data within a Transformer, leading
to improved fraud detection by better capturing temporal dependencies and event
relationships.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [516] [Lightweight Federated Learning over Wireless Edge Networks](https://arxiv.org/abs/2507.09546)
*Xiangwang Hou,Jingjing Wang,Jun Du,Chunxiao Jiang,Yong Ren,Dusit Niyato*

Main category: cs.DC

TL;DR: 本文提出了一种轻量级联邦学习（LTFL）框架，结合无线传输功率控制、模型剪枝和梯度量化，以解决无线网络中联邦学习的实际部署挑战。


<details>
  <summary>Details</summary>
Motivation: 随着智能设备连接无线网络的指数增长，数据量迅速增加，需要机器学习技术提取价值。但集中式机器学习存在通信开销和隐私问题，联邦学习（FL）在网络边缘提供替代方案，但其在无线网络中的实际部署仍具挑战性。

Method: 提出LTFL框架，整合无线传输功率控制、模型剪枝和梯度量化，推导FL收敛间隙的闭式表达式，并基于此制定优化问题以最小化收敛间隙，同时满足延迟和能量约束。

Result: 通过真实数据集实验验证，LTFL优于现有方案。

Conclusion: LTFL框架有效解决了无线网络中联邦学习的部署挑战，通过优化模型剪枝、梯度量化和传输功率控制，显著提升了性能。

Abstract: With the exponential growth of smart devices connected to wireless networks,
data production is increasing rapidly, requiring machine learning (ML)
techniques to unlock its value. However, the centralized ML paradigm raises
concerns over communication overhead and privacy. Federated learning (FL)
offers an alternative at the network edge, but practical deployment in wireless
networks remains challenging. This paper proposes a lightweight FL (LTFL)
framework integrating wireless transmission power control, model pruning, and
gradient quantization. We derive a closed-form expression of the FL convergence
gap, considering transmission error, model pruning error, and gradient
quantization error. Based on these insights, we formulate an optimization
problem to minimize the convergence gap while meeting delay and energy
constraints. To solve the non-convex problem efficiently, we derive closed-form
solutions for the optimal model pruning ratio and gradient quantization level,
and employ Bayesian optimization for transmission power control. Extensive
experiments on real-world datasets show that LTFL outperforms state-of-the-art
schemes.

</details>


### [517] [Efficient Federated Learning with Heterogeneous Data and Adaptive Dropout](https://arxiv.org/abs/2507.10430)
*Ji Liu,Beichen Ma,Yang Zhou,Jingbo Zhou,Ruoming Jin,Dejing Dou,Huaiyu Dai,Haixun Wang,Patrick Valduriez*

Main category: cs.DC

TL;DR: FedDHAD框架通过动态异构模型聚合和自适应丢弃方法，显著提升了联邦学习在非IID数据下的准确性和效率。


<details>
  <summary>Details</summary>
Motivation: 解决联邦学习中数据异构性和设备能力限制导致的模型收敛慢和准确率下降问题。

Method: 提出FedDH动态调整模型聚合权重，FedAD通过自适应丢弃神经元提高效率和准确性。

Result: 准确率提升6.7%，效率提高2.02倍，计算成本降低15.0%。

Conclusion: FedDHAD在非IID数据下表现优异，优于现有方法。

Abstract: Federated Learning (FL) is a promising distributed machine learning approach
that enables collaborative training of a global model using multiple edge
devices. The data distributed among the edge devices is highly heterogeneous.
Thus, FL faces the challenge of data distribution and heterogeneity, where
non-Independent and Identically Distributed (non-IID) data across edge devices
may yield in significant accuracy drop. Furthermore, the limited computation
and communication capabilities of edge devices increase the likelihood of
stragglers, thus leading to slow model convergence. In this paper, we propose
the FedDHAD FL framework, which comes with two novel methods: Dynamic
Heterogeneous model aggregation (FedDH) and Adaptive Dropout (FedAD). FedDH
dynamically adjusts the weights of each local model within the model
aggregation process based on the non-IID degree of heterogeneous data to deal
with the statistical data heterogeneity. FedAD performs neuron-adaptive
operations in response to heterogeneous devices to improve accuracy while
achieving superb efficiency. The combination of these two methods makes FedDHAD
significantly outperform state-of-the-art solutions in terms of accuracy (up to
6.7% higher), efficiency (up to 2.02 times faster), and computation cost (up to
15.0% smaller).

</details>


### [518] [ElasticMM: Efficient Multimodal LLMs Serving with Elastic Multimodal Parallelism](https://arxiv.org/abs/2507.10069)
*Zedong Liu,Shenggan Cheng,Guangming Tan,Yang You,Dingwen Tao*

Main category: cs.DC

TL;DR: 论文提出了一种名为Elastic Multimodal Parallelism (EMP)的新服务范式，用于高效处理多模态大语言模型(MLLMs)的推理任务，并开发了ElasticMM系统，显著降低了延迟并提高了吞吐量。


<details>
  <summary>Details</summary>
Motivation: 当前紧密耦合的服务架构无法区分混合请求类型或适应不同推理阶段的并行策略，导致延迟增加和资源利用率低下。

Method: 通过EMP范式，ElasticMM系统将请求分离为独立的模态组，动态分配资源，解耦推理阶段并调整并行性，同时利用统一的多模态前缀缓存和非阻塞编码提高效率。

Result: 实验表明，ElasticMM在真实数据集上优于现有系统，TTFT延迟降低至4.2倍，吞吐量提高3.2-4.5倍。

Conclusion: ElasticMM通过弹性适应资源异构性，显著提升了MLLM的服务效率。

Abstract: Multimodal large language models (MLLMs) extend LLMs to handle images,
videos, and audio by incorporating feature extractors and projection modules.
However, these additional components -- combined with complex inference
pipelines and heterogeneous workloads -- introduce significant inference
overhead. Therefore, efficiently serving MLLMs remains a major challenge.
Current tightly coupled serving architectures struggle to distinguish between
mixed request types or adapt parallelism strategies to different inference
stages, leading to increased time-to-first-token (TTFT) latency and poor
resource utilization. To address this, we propose Elastic Multimodal
Parallelism (EMP), a new serving paradigm that elastically adapts to resource
heterogeneity across request types and inference stages. Building upon EMP, we
develop ElasticMM, an MLLM serving system that (1) separates requests into
independent modality groups with dynamic resource allocation via a
modality-aware load balancer; (2) decouples inference stages and enables
parallelism adjustment and adaptive scaling via elastic partition scheduling;
and (3) improves inference efficiency through unified multimodal prefix caching
and non-blocking encoding. Experiments on diverse real-world datasets show that
ElasticMM outperforms state-of-the-art (SOTA) serving systems, reducing TTFT by
up to 4.2x and achieving 3.2-4.5x higher throughput while meeting service-level
objectives (SLOs).

</details>


### [519] [Large-Scale Graph Building in Dynamic Environments: Low Latency and High Quality](https://arxiv.org/abs/2507.10139)
*Filipe Miguel Gonçalves de Almeida,CJ Carey,Hendrik Fichtenberger,Jonathan Halcrow,Silvio Lattanzi,André Linhares,Tao Meng,Ashkan Norouzi-Fard,Nikos Parotsidis,Bryan Perozzi,David Simcha*

Main category: cs.DC

TL;DR: 论文介绍了Dynamic Grale Using ScaNN（Dynamic GUS），一种继承Grale优势并在动态环境中高效构建图的系统，应用于多个领域，包括Android安全。


<details>
  <summary>Details</summary>
Motivation: Grale虽在离线环境中高效，但无法满足动态数据快速更新的需求，而现有ANN系统又局限于单一嵌入相似性。

Method: 开发Dynamic GUS系统，结合Grale的质量和动态更新的低延迟能力。

Result: 系统在Google多个领域部署，如Android安全，能4倍更快捕获有害应用。

Conclusion: Dynamic GUS成功解决了动态图构建的低延迟需求，扩展了Grale的应用范围。

Abstract: Learning and constructing large-scale graphs has attracted attention in
recent decades, resulting in a rich literature that introduced various systems,
tools, and algorithms. Grale is one of such tools that is designed for offline
environments and is deployed in more than 50 different industrial settings at
Google. Grale is widely applicable because of its ability to efficiently learn
and construct a graph on datasets with multiple types of features. However, it
is often the case that applications require the underlying data to evolve
continuously and rapidly and the updated graph needs to be available with low
latency. Such setting make the use of Grale prohibitive. While there are
Approximate Nearest Neighbor (ANN) systems that handle dynamic updates with low
latency, they are mostly limited to similarities over a single embedding.
  In this work, we introduce a system that inherits the advantages and the
quality of Grale, and maintains a graph construction in a dynamic setting with
tens of milliseconds of latency per request. We call the system Dynamic Grale
Using ScaNN (Dynamic GUS). Our system has a wide range of applications with
over 10 deployments at Google. One of the applications is in Android Security
and Privacy, where Dynamic Grale Using ScaNN enables capturing harmful
applications 4 times faster, before they can reach users.

</details>


<div id='cs.IT'></div>

# cs.IT [[Back]](#toc)

### [520] [Lightweight Deep Learning-Based Channel Estimation for RIS-Aided Extremely Large-Scale MIMO Systems on Resource-Limited Edge Devices](https://arxiv.org/abs/2507.09627)
*Muhammad Kamran Saeed,Ashfaq Khokhar,Shakil Ahmed*

Main category: cs.IT

TL;DR: 论文提出了一种轻量级深度学习框架，用于XL-MIMO系统中的高效级联信道估计，以降低计算复杂度并适应资源受限的边缘设备。


<details>
  <summary>Details</summary>
Motivation: 6G等下一代无线技术需要满足超高数据速率、低延迟和增强连接性等要求，XL-MIMO和RIS是关键技术，但其性能依赖于准确的CSI。现有信道估计模型在XL-MIMO系统中的可扩展性和实际部署受限。

Method: 提出了一种基于空间相关性的轻量级深度学习框架，采用基于分块的训练机制，降低输入维度并保留关键信息，实现大规模系统的可扩展训练。

Result: 仿真结果表明，该框架显著提高了估计精度并降低了计算复杂度，且不受XL-MIMO系统中天线和RIS元件数量增加的影响。

Conclusion: 该轻量级框架为XL-MIMO系统中的高效信道估计提供了可行解决方案，适用于资源受限的边缘设备。

Abstract: Next-generation wireless technologies such as 6G aim to meet demanding
requirements such as ultra-high data rates, low latency, and enhanced
connectivity. Extremely Large-Scale MIMO (XL-MIMO) and Reconfigurable
Intelligent Surface (RIS) are key enablers, with XL-MIMO boosting spectral and
energy efficiency through numerous antennas, and RIS offering dynamic control
over the wireless environment via passive reflective elements. However,
realizing their full potential depends on accurate Channel State Information
(CSI). Recent advances in deep learning have facilitated efficient cascaded
channel estimation. However, the scalability and practical deployment of
existing estimation models in XL-MIMO systems remain limited. The growing
number of antennas and RIS elements introduces a significant barrier to
real-time and efficient channel estimation, drastically increasing data volume,
escalating computational complexity, requiring advanced hardware, and resulting
in substantial energy consumption. To address these challenges, we propose a
lightweight deep learning framework for efficient cascaded channel estimation
in XL-MIMO systems, designed to minimize computational complexity and make it
suitable for deployment on resource-constrained edge devices. Using spatial
correlations in the channel, we introduce a patch-based training mechanism that
reduces the dimensionality of input to patch-level representations while
preserving essential information, allowing scalable training for large-scale
systems. Simulation results under diverse conditions demonstrate that our
framework significantly improves estimation accuracy and reduces computational
complexity, regardless of the increasing number of antennas and RIS elements in
XL-MIMO systems.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [521] [Privacy-Utility-Fairness: A Balanced Approach to Vehicular-Traffic Management System](https://arxiv.org/abs/2507.08864)
*Poushali Sengupta,Sabita Maharjan,frank Eliassen,Yan Zhang*

Main category: cs.CR

TL;DR: 提出一种新算法，平衡基于位置的车辆交通管理中的隐私、效用和公平性，采用差分隐私技术保护敏感地理数据。


<details>
  <summary>Details</summary>
Motivation: 现有解决方案在隐私保护和公平性方面存在不足，导致数据泄露和分析不公。

Method: 结合查询数据访问、迭代洗牌和校准噪声注入的差分隐私技术，采用拉普拉斯机制。

Result: 在挪威车辆数据上验证，保持交通管理效用和地理区域公平表示。

Conclusion: 算法有效保护隐私，同时确保数据效用和公平性。

Abstract: Location-based vehicular traffic management faces significant challenges in
protecting sensitive geographical data while maintaining utility for traffic
management and fairness across regions. Existing state-of-the-art solutions
often fail to meet the required level of protection against linkage attacks and
demographic biases, leading to privacy leakage and inequity in data analysis.
In this paper, we propose a novel algorithm designed to address the challenges
regarding the balance of privacy, utility, and fairness in location-based
vehicular traffic management systems. In this context, utility means providing
reliable and meaningful traffic information, while fairness ensures that all
regions and individuals are treated equitably in data use and decision-making.
Employing differential privacy techniques, we enhance data security by
integrating query-based data access with iterative shuffling and calibrated
noise injection, ensuring that sensitive geographical data remains protected.
We ensure adherence to epsilon-differential privacy standards by implementing
the Laplace mechanism. We implemented our algorithm on vehicular location-based
data from Norway, demonstrating its ability to maintain data utility for
traffic management and urban planning while ensuring fair representation of all
geographical areas without being overrepresented or underrepresented.
Additionally, we have created a heatmap of Norway based on our model,
illustrating the privatized and fair representation of the traffic conditions
across various cities. Our algorithm provides privacy in vehicular traffic

</details>


### [522] [The Man Behind the Sound: Demystifying Audio Private Attribute Profiling via Multimodal Large Language Model Agents](https://arxiv.org/abs/2507.10016)
*Lixu Wang,Kaixiang Yao,Xinfeng Li,Dong Yang,Haoyang Li,Xiaofeng Wang,Wei Dong*

Main category: cs.CR

TL;DR: 研究发现多模态大语言模型（MLLMs）存在一种新的隐私风险：通过音频数据推断敏感个人属性（音频隐私属性分析）。为解决缺乏标注数据和模型能力不足的问题，提出了AP^2数据集和Gifts框架，并探讨了防御策略。


<details>
  <summary>Details</summary>
Motivation: 揭示MLLMs通过音频数据推断敏感属性的隐私风险，填补音频隐私攻击研究的空白。

Method: 提出AP^2数据集和Gifts框架，结合音频语言模型和大语言模型的优势，提升敏感属性推断能力。

Result: Gifts显著优于基线方法，验证了音频隐私攻击的可行性。

Conclusion: 研究强调了音频隐私攻击的风险，提供了数据集和框架支持未来研究，并探讨了防御策略。

Abstract: Our research uncovers a novel privacy risk associated with multimodal large
language models (MLLMs): the ability to infer sensitive personal attributes
from audio data -- a technique we term audio private attribute profiling. This
capability poses a significant threat, as audio can be covertly captured
without direct interaction or visibility. Moreover, compared to images and
text, audio carries unique characteristics, such as tone and pitch, which can
be exploited for more detailed profiling. However, two key challenges exist in
understanding MLLM-employed private attribute profiling from audio: (1) the
lack of audio benchmark datasets with sensitive attribute annotations and (2)
the limited ability of current MLLMs to infer such attributes directly from
audio. To address these challenges, we introduce AP^2, an audio benchmark
dataset that consists of two subsets collected and composed from real-world
data, and both are annotated with sensitive attribute labels. Additionally, we
propose Gifts, a hybrid multi-agent framework that leverages the complementary
strengths of audio-language models (ALMs) and large language models (LLMs) to
enhance inference capabilities. Gifts employs an LLM to guide the ALM in
inferring sensitive attributes, then forensically analyzes and consolidates the
ALM's inferences, overcoming severe hallucinations of existing ALMs in
generating long-context responses. Our evaluations demonstrate that Gifts
significantly outperforms baseline approaches in inferring sensitive
attributes. Finally, we investigate model-level and data-level defense
strategies to mitigate the risks of audio private attribute profiling. Our work
validates the feasibility of audio-based privacy attacks using MLLMs,
highlighting the need for robust defenses, and provides a dataset and framework
to facilitate future research.

</details>


### [523] [Clio-X: AWeb3 Solution for Privacy-Preserving AI Access to Digital Archives](https://arxiv.org/abs/2507.08853)
*Victoria L. Lemieux,Rosa Gil,Faith Molosiwa,Qihong Zhou,Binming Li,Roberto Garcia,Luis De La Torre Cubillo,Zehua Wang*

Main category: cs.CR

TL;DR: 论文探讨了隐私增强技术（PETs）和Web3架构如何帮助档案馆在保护敏感内容的同时支持AI驱动的访问，提出了Clio-X解决方案，并分析了其采用障碍与改进路径。


<details>
  <summary>Details</summary>
Motivation: 当前AI数据实践中的隐私风险对档案馆的数据主权和伦理责任提出了挑战，需要一种既能保护隐私又能支持访问的解决方案。

Method: 提出Clio-X，一种去中心化的隐私优先Web3解决方案，结合PETs和用户评估原型，分析采用障碍并提出改进路径。

Result: 用户评估显示Clio-X潜力大，但存在信任、系统透明度、经济问题和治理等障碍。

Conclusion: 通过参与式设计和去中心化治理，Clio-X为文化遗产领域提供了伦理部署AI的新模型。

Abstract: As archives turn to artificial intelligence to manage growing volumes of
digital records, privacy risks inherent in current AI data practices raise
critical concerns about data sovereignty and ethical accountability. This paper
explores how privacy-enhancing technologies (PETs) and Web3 architectures can
support archives to preserve control over sensitive content while still being
able to make it available for access by researchers. We present Clio-X, a
decentralized, privacy-first Web3 digital solution designed to embed PETs into
archival workflows and support AI-enabled reference and access. Drawing on a
user evaluation of a medium-fidelity prototype, the study reveals both interest
in the potential of the solution and significant barriers to adoption related
to trust, system opacity, economic concerns, and governance. Using Rogers'
Diffusion of Innovation theory, we analyze the sociotechnical dimensions of
these barriers and propose a path forward centered on participatory design and
decentralized governance through a Clio-X Decentralized Autonomous
Organization. By integrating technical safeguards with community-based
oversight, Clio-X offers a novel model to ethically deploy AI in cultural
heritage contexts.

</details>


### [524] [Towards Privacy-Preserving and Personalized Smart Homes via Tailored Small Language Models](https://arxiv.org/abs/2507.08878)
*Xinyu Huang,Leming Shen,Zijing Ma,Yuanqing Zheng*

Main category: cs.CR

TL;DR: HomeLLaMA是一个基于小型语言模型（SLM）的本地智能家居助手，旨在保护用户隐私并提供个性化服务，同时通过PrivShield提供可选远程服务。


<details>
  <summary>Details</summary>
Motivation: 解决现有基于LLM的智能家居助手因依赖远程服务器而导致的隐私泄露问题。

Method: 开发了HomeLLaMA，一个本地部署的SLM助手，通过从云端LLM学习提供个性化服务，并引入PrivShield作为可选远程服务。

Result: 实验和用户研究表明，HomeLLaMA在保护隐私的同时提供高质量的个性化服务。

Conclusion: HomeLLaMA为智能家居领域提供了一种隐私保护与个性化服务并重的解决方案。

Abstract: Large Language Models (LLMs) have showcased remarkable generalizability in
language comprehension and hold significant potential to revolutionize
human-computer interaction in smart homes. Existing LLM-based smart home
assistants typically transmit user commands, along with user profiles and home
configurations, to remote servers to obtain personalized services. However,
users are increasingly concerned about the potential privacy leaks to the
remote servers. To address this issue, we develop HomeLLaMA, an on-device
assistant for privacy-preserving and personalized smart home serving with a
tailored small language model (SLM). HomeLLaMA learns from cloud LLMs to
deliver satisfactory responses and enable user-friendly interactions. Once
deployed, HomeLLaMA facilitates proactive interactions by continuously updating
local SLMs and user profiles. To further enhance user experience while
protecting their privacy, we develop PrivShield to offer an optional
privacy-preserving LLM-based smart home serving for those users, who are
unsatisfied with local responses and willing to send less-sensitive queries to
remote servers. For evaluation, we build a comprehensive benchmark DevFinder to
assess the service quality. Extensive experiments and user studies (M=100)
demonstrate that HomeLLaMA can provide personalized services while
significantly enhancing user privacy.

</details>


### [525] [RAG Safety: Exploring Knowledge Poisoning Attacks to Retrieval-Augmented Generation](https://arxiv.org/abs/2507.08862)
*Tianzhe Zhao,Jiaoyan Chen,Yanchi Ru,Haiping Zhu,Nan Hu,Jun Liu,Qika Lin*

Main category: cs.CR

TL;DR: 本文首次系统研究了基于知识图谱的检索增强生成（KG-RAG）方法的安全问题，提出了一种隐蔽的数据投毒攻击策略，并通过实验验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 尽管KG-RAG方法在增强语言模型性能方面表现出色，但其安全风险尚未被充分研究，尤其是知识图谱的结构化和可编辑性带来的独特漏洞。

Method: 提出了一种隐蔽的攻击策略，通过识别对抗性目标答案并插入扰动三元组，误导KG-RAG系统的推理过程。

Result: 实验表明，即使在最小扰动下，该攻击策略也能显著降低KG-RAG的性能。

Conclusion: KG-RAG系统存在严重的安全隐患，需要进一步研究防御机制以应对数据投毒攻击。

Abstract: Retrieval-Augmented Generation (RAG) enhances large language models (LLMs) by
retrieving external data to mitigate hallucinations and outdated knowledge
issues. Benefiting from the strong ability in facilitating diverse data sources
and supporting faithful reasoning, knowledge graphs (KGs) have been
increasingly adopted in RAG systems, giving rise to KG-based RAG (KG-RAG)
methods. Though RAG systems are widely applied in various applications, recent
studies have also revealed its vulnerabilities to data poisoning attacks, where
malicious information injected into external knowledge sources can mislead the
system into producing incorrect or harmful responses. However, these studies
focus exclusively on RAG systems using unstructured textual data sources,
leaving the security risks of KG-RAG largely unexplored, despite the fact that
KGs present unique vulnerabilities due to their structured and editable nature.
In this work, we conduct the first systematic investigation of the security
issue of KG-RAG methods through data poisoning attacks. To this end, we
introduce a practical, stealthy attack setting that aligns with real-world
implementation. We propose an attack strategy that first identifies adversarial
target answers and then inserts perturbation triples to complete misleading
inference chains in the KG, increasing the likelihood that KG-RAG methods
retrieve and rely on these perturbations during generation. Through extensive
experiments on two benchmarks and four recent KG-RAG methods, our attack
strategy demonstrates strong effectiveness in degrading KG-RAG performance,
even with minimal KG perturbations. In-depth analyses are also conducted to
understand the safety threats within the internal stages of KG-RAG systems and
to explore the robustness of LLMs against adversarial knowledge.

</details>


### [526] [EventHunter: Dynamic Clustering and Ranking of Security Events from Hacker Forum Discussions](https://arxiv.org/abs/2507.09762)
*Yasir Ech-Chammakhy,Anas Motii,Anass Rabii,Jaafar Chbili*

Main category: cs.CR

TL;DR: 提出了一种无监督框架，利用Transformer嵌入和对比学习，自动检测、聚类和优先处理黑客论坛中的安全事件。


<details>
  <summary>Details</summary>
Motivation: 黑客论坛的内容嘈杂且非结构化，难以提取可操作的情报，需要一种自动化方法来识别和优先处理新兴威胁。

Method: 使用基于Transformer的嵌入和对比学习，对论坛帖子进行聚类，并通过量化指标（如及时性、来源可信度等）对事件进行排名。

Result: 实验证明，该方法能有效减少噪音并识别高优先级威胁，帮助安全分析师采取主动响应。

Conclusion: 该框架将非结构化的黑客论坛讨论转化为结构化情报，解决了自动化威胁检测和分析的基本挑战。

Abstract: Hacker forums provide critical early warning signals for emerging
cybersecurity threats, but extracting actionable intelligence from their
unstructured and noisy content remains a significant challenge. This paper
presents an unsupervised framework that automatically detects, clusters, and
prioritizes security events discussed across hacker forum posts. Our approach
leverages Transformer-based embeddings fine-tuned with contrastive learning to
group related discussions into distinct security event clusters, identifying
incidents like zero-day disclosures or malware releases without relying on
predefined keywords. The framework incorporates a daily ranking mechanism that
prioritizes identified events using quantifiable metrics reflecting timeliness,
source credibility, information completeness, and relevance. Experimental
evaluation on real-world hacker forum data demonstrates that our method
effectively reduces noise and surfaces high-priority threats, enabling security
analysts to mount proactive responses. By transforming disparate hacker forum
discussions into structured, actionable intelligence, our work addresses
fundamental challenges in automated threat detection and analysis.

</details>


### [527] [A Mixture of Linear Corrections Generates Secure Code](https://arxiv.org/abs/2507.09508)
*Weichen Yu,Ravi Mangal,Terry Zhuo,Matt Fredrikson,Corina S. Pasareanu*

Main category: cs.CR

TL;DR: 研究发现大型语言模型（LLMs）内部能区分代码漏洞，并提出了一种通过混合修正（MoC）引导生成更安全代码的方法，显著提升了安全性和功能性。


<details>
  <summary>Details</summary>
Motivation: 尽管LLMs在代码生成任务上表现优异，但在检测和避免代码漏洞方面效果不佳，研究旨在探究其原因是学习不足还是提示方法不当。

Method: 利用表示工程技术分析LLMs内部是否编码了识别漏洞的概念，并开发了基于混合修正（MoC）的推理时引导技术。

Result: LLMs内部能准确区分漏洞代码和安全代码，MoC方法显著提升了Qwen2.5-Coder-7B的安全性和功能性。

Conclusion: MoC是一种实用的漏洞管理方法，能在不损害功能性的情况下提升生成代码的安全性。

Abstract: Large language models (LLMs) have become proficient at sophisticated
code-generation tasks, yet remain ineffective at reliably detecting or avoiding
code vulnerabilities. Does this deficiency stem from insufficient learning
about code vulnerabilities, or is it merely a result of ineffective prompting?
Using representation engineering techniques, we investigate whether LLMs
internally encode the concepts necessary to identify code vulnerabilities. We
find that current LLMs encode precise internal representations that distinguish
vulnerable from secure code--achieving greater accuracy than standard prompting
approaches. Leveraging these vulnerability-sensitive representations, we
develop an inference-time steering technique that subtly modulates the model's
token-generation probabilities through a mixture of corrections (MoC). Our
method effectively guides LLMs to produce less vulnerable code without
compromising functionality, demonstrating a practical approach to controlled
vulnerability management in generated code. Notably, MoC enhances the security
ratio of Qwen2.5-Coder-7B by 8.9\%, while simultaneously improving
functionality on HumanEval pass@1 by 2.1\%.

</details>


### [528] [Secure and Efficient UAV-Based Face Detection via Homomorphic Encryption and Edge Computing](https://arxiv.org/abs/2507.09860)
*Nguyen Van Duc,Bui Duc Manh,Quang-Trung Luu,Dinh Thai Hoang,Van-Linh Nguyen,Diep N. Nguyen*

Main category: cs.CR

TL;DR: 提出了一种结合同态加密（HE）和先进神经网络的无人机（UAV）人脸检测隐私保护方法，确保数据安全且不影响检测精度。


<details>
  <summary>Details</summary>
Motivation: 无人机人脸检测存在隐私问题，传统方法无法在保护隐私的同时保持高精度。

Method: 使用CKKS同态加密方案，设计数据编码方法和安全推理算法，直接在加密数据上计算。

Result: 实验表明，该方法在保护隐私的同时，检测精度损失小于1%。

Conclusion: 该方法为无人机安全人脸检测提供了可行解决方案，平衡了隐私保护和性能。

Abstract: This paper aims to propose a novel machine learning (ML) approach
incorporating Homomorphic Encryption (HE) to address privacy limitations in
Unmanned Aerial Vehicles (UAV)-based face detection. Due to challenges related
to distance, altitude, and face orientation, high-resolution imagery and
sophisticated neural networks enable accurate face recognition in dynamic
environments. However, privacy concerns arise from the extensive surveillance
capabilities of UAVs. To resolve this issue, we propose a novel framework that
integrates HE with advanced neural networks to secure facial data throughout
the inference phase. This method ensures that facial data remains secure with
minimal impact on detection accuracy. Specifically, the proposed system
leverages the Cheon-Kim-Kim-Song (CKKS) scheme to perform computations directly
on encrypted data, optimizing computational efficiency and security.
Furthermore, we develop an effective data encoding method specifically designed
to preprocess the raw facial data into CKKS form in a
Single-Instruction-Multiple-Data (SIMD) manner. Building on this, we design a
secure inference algorithm to compute on ciphertext without needing decryption.
This approach not only protects data privacy during the processing of facial
data but also enhances the efficiency of UAV-based face detection systems.
Experimental results demonstrate that our method effectively balances privacy
protection and detection performance, making it a viable solution for UAV-based
secure face detection. Significantly, our approach (while maintaining data
confidentially with HE encryption) can still achieve an accuracy of less than
1% compared to the benchmark without using encryption.

</details>


### [529] [Differentially Private Federated Low Rank Adaptation Beyond Fixed-Matrix](https://arxiv.org/abs/2507.09990)
*Ming Wen,Jiaqi Zhu,Yuedong Xu,Yipeng Zhou,Dingding Han*

Main category: cs.CR

TL;DR: FedASK是一种新颖的联邦LoRA框架，通过双阶段草图技术实现高效且隐私保护的适配器更新，解决了差分隐私在联邦学习中的困境。


<details>
  <summary>Details</summary>
Motivation: 解决联邦学习中LoRA适配器传输的隐私泄露问题，同时避免差分隐私噪声对模型性能的负面影响。

Method: 采用两阶段草图管道，先聚合隐私保护的本地更新，再在服务器端重构全局矩阵，实现双适配器的高效更新。

Result: FedASK在多种隐私设置和数据分布下均优于基线方法，理论证明了其差分隐私保证和精确聚合特性。

Conclusion: FedASK为联邦学习中的LoRA适配器更新提供了一种高效且隐私保护的解决方案。

Abstract: Large language models (LLMs) typically require fine-tuning for
domain-specific tasks, and LoRA offers a computationally efficient approach by
training low-rank adapters. LoRA is also communication-efficient for federated
LLMs when multiple users collaboratively fine-tune a global LLM model without
sharing their proprietary raw data. However, even the transmission of local
adapters between a server and clients risks serious privacy leakage. Applying
differential privacy (DP) to federated LoRA encounters a dilemma: adding noise
to both adapters amplifies synthetic noise on the model, while fixing one
adapter impairs the learnability of fine-tuning. In this paper, we propose
FedASK (Differentially Private Federated Low Rank Adaptation with Double
Sketching) , a novel federated LoRA framework to enable effective updating of
both low-rank adapters with robust differential privacy. Inspired by randomized
SVD, our key idea is a two-stage sketching pipeline. This pipeline first
aggregates carefully sketched, privacy-preserving local updates, and then
reconstructs the global matrices on the server to facilitate effective updating
of both adapters. We theoretically prove FedASK's differential privacy
guarantee and its exact aggregation property. Comprehensive experiments
demonstrate that FedASK consistently outperforms baseline methods across a
variety of privacy settings and data distributions.

</details>


### [530] [CAN-Trace Attack: Exploit CAN Messages to Uncover Driving Trajectories](https://arxiv.org/abs/2507.09624)
*Xiaojie Lin,Baihe Ma,Xu Wang,Guangsheng Yu,Ying He,Wei Ni,Ren Ping Liu*

Main category: cs.CR

TL;DR: CAN-Trace是一种利用CAN消息重构驾驶轨迹的新型隐私攻击方法，成功率高。


<details>
  <summary>Details</summary>
Motivation: 现有驾驶轨迹隐私保护措施存在漏洞，GPS数据易受中断影响，需探索新攻击方式。

Method: 通过CAN消息（车速和油门踏板位置）构建加权图，使用图匹配算法与道路网络对比。

Result: 攻击成功率在城市地区达90.59%，郊区达99.41%。

Conclusion: CAN-Trace对驾驶隐私构成重大威胁，需加强防护措施。

Abstract: Driving trajectory data remains vulnerable to privacy breaches despite
existing mitigation measures. Traditional methods for detecting driving
trajectories typically rely on map-matching the path using Global Positioning
System (GPS) data, which is susceptible to GPS data outage. This paper
introduces CAN-Trace, a novel privacy attack mechanism that leverages
Controller Area Network (CAN) messages to uncover driving trajectories, posing
a significant risk to drivers' long-term privacy. A new trajectory
reconstruction algorithm is proposed to transform the CAN messages,
specifically vehicle speed and accelerator pedal position, into weighted graphs
accommodating various driving statuses. CAN-Trace identifies driving
trajectories using graph-matching algorithms applied to the created graphs in
comparison to road networks. We also design a new metric to evaluate matched
candidates, which allows for potential data gaps and matching inaccuracies.
Empirical validation under various real-world conditions, encompassing
different vehicles and driving regions, demonstrates the efficacy of CAN-Trace:
it achieves an attack success rate of up to 90.59% in the urban region, and
99.41% in the suburban region.

</details>


### [531] [Logic layer Prompt Control Injection (LPCI): A Novel Security Vulnerability Class in Agentic Systems](https://arxiv.org/abs/2507.10457)
*Hammad Atta,Ken Huang,Manish Bhatt,Kamal Ahmed,Muhammad Aziz Ul Haq,Yasir Mehmood*

Main category: cs.CR

TL;DR: 论文提出了一种新型攻击类别LPCI，通过嵌入编码、延迟和条件触发的负载来绕过传统输入过滤器，触发未经授权的行为。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）在企业系统中的集成引入了新的安全漏洞，特别是在逻辑执行层和持久内存环境中。

Method: 论文介绍了Logic-Layer Prompt Control Injection (LPCI)，一种将编码、延迟和条件触发的负载嵌入内存、向量存储或工具输出的攻击方法。

Result: 这些负载能够绕过传统输入过滤器，并在跨会话中触发未经授权的行为。

Conclusion: LPCI是一种新型的安全威胁，需要新的防御机制来应对。

Abstract: The integration of large language models (LLMs) into enterprise systems has
created a new class of covert security vulnerabilities, particularly within
logic-execution layers and persistent-memory contexts. In this paper, we
introduce Logic-Layer Prompt Control Injection (LPCI), a novel attack category
in which encoded, delayed, and conditionally triggered payloads are embedded in
memory, vector stores, or tool outputs. These payloads can bypass conventional
input filters and trigger unauthorised behaviour across sessions.

</details>


### [532] [DNS Tunneling: Threat Landscape and Improved Detection Solutions](https://arxiv.org/abs/2507.10267)
*Novruz Amirov,Baran Isik,Bilal Ihsan Tuncer,Serif Bahtiyar*

Main category: cs.CR

TL;DR: 提出了一种基于机器学习的新方法，用于准确检测DNS隧道。


<details>
  <summary>Details</summary>
Motivation: DNS隧道能够隐藏恶意行为于看似正常的DNS流量中，传统检测方法效果有限。

Method: 结合机器学习算法，分析从DNS流量中提取的特征。

Result: 分析结果表明，该方法能有效检测DNS隧道。

Conclusion: 该方法是检测DNS隧道的有效候选方案。

Abstract: Detecting Domain Name System (DNS) tunneling is a significant challenge in
security due to its capacity to hide harmful actions within DNS traffic that
appears to be normal and legitimate. Traditional detection methods are based on
rule-based approaches or signature matching methods that are often insufficient
to accurately identify such covert communication channels. This research is
about effectively detecting DNS tunneling. We propose a novel approach to
detect DNS tunneling with machine learning algorithms. We combine machine
learning algorithms to analyze the traffic by using features extracted from DNS
traffic. Analyses results show that the proposed approach is a good candidate
to detect DNS tunneling accurately.

</details>


<div id='q-bio.BM'></div>

# q-bio.BM [[Back]](#toc)

### [533] [AMix-1: A Pathway to Test-Time Scalable Protein Foundation Model](https://arxiv.org/abs/2507.08920)
*Changze Lv,Jiang Zhou,Siyu Long,Lihao Wang,Jiangtao Feng,Dongyu Xue,Yu Pei,Hao Wang,Zherui Zhang,Yuchen Cai,Zhiqiang Gao,Ziyuan Ma,Jiakai Hu,Chaochen Gao,Jingjing Gong,Yuxuan Song,Shuyi Zhang,Xiaoqing Zheng,Deyi Xiong,Lei Bai,Ya-Qin Zhang,Wei-Ying Ma,Bowen Zhou,Hao Zhou*

Main category: q-bio.BM

TL;DR: AMix-1是一个基于贝叶斯流网络的蛋白质基础模型，通过系统训练方法实现强大性能，包括预训练扩展规律、涌现能力分析、上下文学习机制和测试时扩展算法。


<details>
  <summary>Details</summary>
Motivation: 构建一个可扩展的蛋白质基础模型，统一蛋白质设计框架，并推动蛋白质工程边界。

Method: 采用贝叶斯流网络，结合多序列比对（MSA）的上下文学习策略和进化测试时扩展算法。

Result: 成功设计出性能提升50倍的AmeR变体，并通过测试时扩展算法实现可扩展性能增益。

Conclusion: AMix-1为下一代实验室循环蛋白质设计奠定了基础。

Abstract: We introduce AMix-1, a powerful protein foundation model built on Bayesian
Flow Networks and empowered by a systematic training methodology, encompassing
pretraining scaling laws, emergent capability analysis, in-context learning
mechanism, and test-time scaling algorithm. To guarantee robust scalability, we
establish a predictive scaling law and reveal the progressive emergence of
structural understanding via loss perspective, culminating in a strong
1.7-billion model. Building on this foundation, we devise a multiple sequence
alignment (MSA)-based in-context learning strategy to unify protein design into
a general framework, where AMix-1 recognizes deep evolutionary signals among
MSAs and consistently generates structurally and functionally coherent
proteins. This framework enables the successful design of a dramatically
improved AmeR variant with an up to $50\times$ activity increase over its wild
type. Pushing the boundaries of protein engineering, we further empower AMix-1
with an evolutionary test-time scaling algorithm for in silico directed
evolution that delivers substantial, scalable performance gains as verification
budgets are intensified, laying the groundwork for next-generation
lab-in-the-loop protein design.

</details>


### [534] [Conformation-Aware Structure Prediction of Antigen-Recognizing Immune Proteins](https://arxiv.org/abs/2507.09054)
*Frédéric A. Dreyer,Jan Ludwiczak,Karolis Martinkus,Brennan Abanades,Robert G. Alberstein,Pan Kessel,Pranav Rao,Jae Hyeon Lee,Richard Bonneau,Andrew M. Watkins,Franziska Seeger*

Main category: q-bio.BM

TL;DR: Ibex是一种全免疫球蛋白结构预测模型，能够高精度预测抗体、纳米抗体和T细胞受体的可变域，并区分结合与非结合状态。


<details>
  <summary>Details</summary>
Motivation: 现有方法无法区分结合与非结合状态，Ibex通过训练标记的apo和holo结构对解决了这一问题，提升了预测准确性。

Method: Ibex利用标记的apo和holo结构对进行训练，结合私有高分辨率抗体结构数据集，优化模型性能。

Result: Ibex在分布外数据上表现优于现有专用和通用蛋白质结构预测工具，且计算需求显著降低。

Conclusion: Ibex为加速大分子设计和治疗开发提供了高效且准确的工具。

Abstract: We introduce Ibex, a pan-immunoglobulin structure prediction model that
achieves state-of-the-art accuracy in modeling the variable domains of
antibodies, nanobodies, and T-cell receptors. Unlike previous approaches, Ibex
explicitly distinguishes between bound and unbound protein conformations by
training on labeled apo and holo structural pairs, enabling accurate prediction
of both states at inference time. Using a comprehensive private dataset of
high-resolution antibody structures, we demonstrate superior
out-of-distribution performance compared to existing specialized and general
protein structure prediction tools. Ibex combines the accuracy of cutting-edge
models with significantly reduced computational requirements, providing a
robust foundation for accelerating large molecule design and therapeutic
development.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [535] [Hybrid Systolic Array Accelerator with Optimized Dataflow for Edge Large Language Model Inference](https://arxiv.org/abs/2507.09010)
*Chun-Ting Chen,HanGyeol Mun,Jian Meng,Mohamed S. Abdelfattah,Jae-sun Seo*

Main category: cs.AR

TL;DR: 本文提出了一种边缘LLM推理加速器，采用混合脉动阵列架构，优化推理效率，减少外部内存访问，并保持高能效。


<details>
  <summary>Details</summary>
Motivation: 边缘LLM推理需要高面积效率、低外部内存访问和高能效，尤其是在内存受限的解码阶段和计算密集的预填充阶段。

Method: 采用混合脉动阵列架构，结合MXINT4权重量化和优化数据流，减少去量化开销，并优化RMSNorm和RoPE单元。

Result: 在1.3B LLM上实现247/117（token/s/mm2），比现有方法提升2.45x/13.5x，同时保持高能效。

Conclusion: 该加速器在边缘LLM推理中表现出高效性和优越性能，适用于长输入/输出场景。

Abstract: Edge inference for large language models (LLM) offers secure, low-latency,
and cost-effective inference solutions. We emphasize that an edge accelerator
should achieve high area efficiency and minimize external memory access (EMA)
during the memory-bound decode stage, while maintaining high energy efficiency
during the compute intensive prefill stage. This paper proposes an edge LLM
inference accelerator featuring a hybrid systolic array (HSA) architecture that
optimizes inference efficiency in both stages. To further reduce EMA, we adopt
MXINT4 weight quantization and propose an optimized dataflow tailored for HSA,
ensuring negligible dequantization overhead and achieving 100% hardware
utilization with minimal accuracy loss under edge DRAM bandwidth constraints.
For non-linear operations, we incorporate optimized root mean square
normalization (RMSNorm) and rotary position embedding (RoPE) units, reducing
their latency, area, and memory access overhead while enabling end-to-end
inference on our accelerator. Our solution achieves 247/117 (token/s/mm2) while
running a 1.3B LLM on long-input/long-output scenarios, providing >2.45x/13.5x
improvement over existing approaches, while maintaining superior energy
efficiency in token generation.

</details>


### [536] [BitParticle: Partializing Sparse Dual-Factors to Build Quasi-Synchronizing MAC Arrays for Energy-efficient DNNs](https://arxiv.org/abs/2507.09780)
*Feilong Qiaoyuan,Jihe Wang,Zhiyu Sun,Linying Wu,Yuanhua Xiao,Danghui Wang*

Main category: cs.AR

TL;DR: 论文提出了一种利用双因子稀疏性的MAC单元设计，通过部分化方法解决部分乘积爆炸问题，并引入准同步调度方案提高MAC单元利用率。


<details>
  <summary>Details</summary>
Motivation: 量化深度神经网络中的位级稀疏性为优化MAC操作提供了潜力，但现有方法无法同时利用双因子稀疏性且调度灵活性不足。

Method: 设计了一种基于部分化的MAC单元，通过简单控制逻辑解决部分乘积爆炸问题，并引入准同步调度方案提高灵活性。

Result: 提出的MAC单元在面积效率上比现有技术提高了29.2%，近似变体进一步提升了7.5%的能效。

Conclusion: 该设计在保持能效的同时显著提升了面积效率，为DNN加速提供了更高效的硬件解决方案。

Abstract: Bit-level sparsity in quantized deep neural networks (DNNs) offers
significant potential for optimizing Multiply-Accumulate (MAC) operations.
However, two key challenges still limit its practical exploitation. First,
conventional bit-serial approaches cannot simultaneously leverage the sparsity
of both factors, leading to a complete waste of one factor' s sparsity. Methods
designed to exploit dual-factor sparsity are still in the early stages of
exploration, facing the challenge of partial product explosion. Second, the
fluctuation of bit-level sparsity leads to variable cycle counts for MAC
operations. Existing synchronous scheduling schemes that are suitable for
dual-factor sparsity exhibit poor flexibility and still result in significant
underutilization of MAC units. To address the first challenge, this study
proposes a MAC unit that leverages dual-factor sparsity through the emerging
particlization-based approach. The proposed design addresses the issue of
partial product explosion through simple control logic, resulting in a more
area- and energy-efficient MAC unit. In addition, by discarding less
significant intermediate results, the design allows for further hardware
simplification at the cost of minor accuracy loss. To address the second
challenge, a quasi-synchronous scheme is introduced that adds cycle-level
elasticity to the MAC array, reducing pipeline stalls and thereby improving MAC
unit utilization. Evaluation results show that the exact version of the
proposed MAC array architecture achieves a 29.2% improvement in area efficiency
compared to the state-of-the-art bit-sparsity-driven architecture, while
maintaining comparable energy efficiency. The approximate variant further
improves energy efficiency by 7.5%, compared to the exact version. Index-Terms:
DNN acceleration, Bit-level sparsity, MAC unit

</details>


### [537] [Pimba: A Processing-in-Memory Acceleration for Post-Transformer Large Language Model Serving](https://arxiv.org/abs/2507.10178)
*Wonung Kim,Yubin Lee,Yoonsung Kim,Jinwoo Hwang,Seongryong Oh,Jiyong Jung,Aziz Huseynov,Woong Gyu Park,Chang Hyun Park,Divya Mahajan,Jongse Park*

Main category: cs.AR

TL;DR: 论文提出了一种名为Pimba的统一框架，用于高效支持Transformer和后Transformer架构的LLM推理，通过分析性能特点并设计基于状态更新处理单元（SPU）的系统，显著提升了生成吞吐量。


<details>
  <summary>Details</summary>
Motivation: Transformer架构在长序列推理中存在计算和内存成本高的问题，后Transformer架构（如SSM、线性注意力、RNN）成为替代方案，但需要统一的高效服务框架。

Method: 分析了Transformer和后Transformer LLM的性能特点，设计了基于状态更新处理单元（SPU）的Pimba系统，采用MX量化算术优化硬件成本。

Result: Pimba在生成吞吐量上比优化的GPU和GPU+PIM系统分别提升了3.2倍和2.1倍。

Conclusion: Pimba通过统一框架和硬件优化，有效解决了Transformer和后Transformer LLM的推理效率问题。

Abstract: Transformers are the driving force behind today's Large Language Models
(LLMs), serving as the foundation for their performance and versatility. Yet,
their compute and memory costs grow with sequence length, posing scalability
challenges for long-context inferencing. In response, the algorithm community
is exploring alternative architectures, such as state space models (SSMs),
linear attention, and recurrent neural networks (RNNs), which we refer to as
post-transformers. This shift presents a key challenge: building a serving
system that efficiently supports both transformer and post-transformer LLMs
within a unified framework. To address this challenge, we analyze the
performance characteristics of transformer and post-transformer LLMs. Despite
their algorithmic differences, both are fundamentally limited by memory
bandwidth under batched inference due to attention in transformers and state
updates in post-transformers. Further analyses suggest two additional insights:
(1) state update operations, unlike attention, incur high hardware cost, making
per-bank PIM acceleration inefficient, and (2) different low-precision
arithmetic methods offer varying accuracy-area tradeoffs, while we identify
Microsoft's MX as the Pareto-optimal choice. Building on these insights, we
design Pimba as an array of State-update Processing Units (SPUs), each shared
between two banks to enable interleaved access to PIM. Each SPU includes a
State-update Processing Engine (SPE) that comprises element-wise multipliers
and adders using MX-based quantized arithmetic, enabling efficient execution of
state update and attention operations. Our evaluation shows that, compared to
LLM-optimized GPU and GPU+PIM systems, Pimba achieves up to 3.2x and 2.1x
higher token generation throughput, respectively.

</details>


<div id='eess.SY'></div>

# eess.SY [[Back]](#toc)

### [538] [Counterfactual optimization for fault prevention in complex wind energy systems](https://arxiv.org/abs/2507.08849)
*Emilio Carrizosa,Martina Fischetti,Roshell Haaker,Juan Miguel Morales*

Main category: eess.SY

TL;DR: 论文提出了一种基于机器学习的优化控制策略，用于将复杂系统从异常状态恢复到安全状态，并在风能系统中验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有方法仅能检测异常，而无法提供恢复系统到安全状态的最优控制策略。

Method: 将问题建模为反事实优化问题，利用数学模型找到满足系统约束的最小调整方案。

Result: 在真实数据测试中，该方法每年可为典型风电场节省约300万欧元。

Conclusion: 该方法不仅适用于风能系统，还为反事实优化在其他领域的应用提供了新思路。

Abstract: Machine Learning models are increasingly used in businesses to detect faults
and anomalies in complex systems. In this work, we take this approach a step
further: beyond merely detecting anomalies, we aim to identify the optimal
control strategy that restores the system to a safe state with minimal
disruption. We frame this challenge as a counterfactual problem: given a
Machine Learning model that classifies system states as either good or
anomalous, our goal is to determine the minimal adjustment to the system's
control variables (i.e., its current status) that is necessary to return it to
the good state. To achieve this, we leverage a mathematical model that finds
the optimal counterfactual solution while respecting system specific
constraints. Notably, most counterfactual analysis in the literature focuses on
individual cases where a person seeks to alter their status relative to a
decision made by a classifier, such as for loan approval or medical diagnosis.
Our work addresses a fundamentally different challenge: optimizing
counterfactuals for a complex energy system, specifically an offshore wind
turbine oil type transformer. This application not only advances counterfactual
optimization in a new domain but also opens avenues for broader research in
this area. Our tests on real world data provided by our industrial partner show
that our methodology easily adapts to user preferences and brings savings in
the order of 3 million euros per year in a typical farm.

</details>


### [539] [Intersection of Reinforcement Learning and Bayesian Optimization for Intelligent Control of Industrial Processes: A Safe MPC-based DPG using Multi-Objective BO](https://arxiv.org/abs/2507.09864)
*Hossein Nejatbakhsh Esfahani,Javad Mohammadpour Velni*

Main category: eess.SY

TL;DR: 提出了一种结合MPC-RL与MOBO的新框架，解决标准MPC-RL收敛慢、策略学习次优及安全性问题。


<details>
  <summary>Details</summary>
Motivation: 标准MPC-RL方法存在收敛慢、策略学习次优和在线适应时的安全性问题。

Method: 提出MPC-RL-MOBO框架，利用CDPG估计RL阶段成本及其梯度，通过MOBO算法（EHVI采集函数）高效安全调参。

Result: 数值实验表明该方法在样本效率、稳定性和高性能学习方面表现优异。

Conclusion: MPC-RL-MOBO框架能有效提升闭环性能，适用于模型不完美情况。

Abstract: Model Predictive Control (MPC)-based Reinforcement Learning (RL) offers a
structured and interpretable alternative to Deep Neural Network (DNN)-based RL
methods, with lower computational complexity and greater transparency. However,
standard MPC-RL approaches often suffer from slow convergence, suboptimal
policy learning due to limited parameterization, and safety issues during
online adaptation. To address these challenges, we propose a novel framework
that integrates MPC-RL with Multi-Objective Bayesian Optimization (MOBO). The
proposed MPC-RL-MOBO utilizes noisy evaluations of the RL stage cost and its
gradient, estimated via a Compatible Deterministic Policy Gradient (CDPG)
approach, and incorporates them into a MOBO algorithm using the Expected
Hypervolume Improvement (EHVI) acquisition function. This fusion enables
efficient and safe tuning of the MPC parameters to achieve improved closed-loop
performance, even under model imperfections. A numerical example demonstrates
the effectiveness of the proposed approach in achieving sample-efficient,
stable, and high-performance learning for control systems.

</details>


### [540] [Neural Two-Stage Stochastic Optimization for Solving Unit Commitment Problem](https://arxiv.org/abs/2507.09503)
*Zhentong Shao,Jingtao Qin,Nanpeng Yu*

Main category: eess.SY

TL;DR: 提出了一种神经随机优化方法，用于高效解决高维不确定性下的两阶段随机机组组合问题，通过深度神经网络近似第二阶段的资源成本，并嵌入到第一阶段的MILP中。


<details>
  <summary>Details</summary>
Motivation: 传统方法在高维不确定性下效率低下，需要一种更高效且可扩展的解决方案。

Method: 使用深度神经网络近似第二阶段问题，并将其嵌入到第一阶段的MILP中，同时采用场景嵌入网络进行降维和特征聚合。

Result: 在IEEE 5、30和118总线系统上的实验显示，最优性差距小于1%，速度显著提升，且模型规模不受场景数量影响。

Conclusion: 该方法在保持高精度的同时，显著提升了计算效率，适用于大规模随机机组组合问题。

Abstract: This paper proposes a neural stochastic optimization method for efficiently
solving the two-stage stochastic unit commitment (2S-SUC) problem under
high-dimensional uncertainty scenarios. The proposed method approximates the
second-stage recourse problem using a deep neural network trained to map
commitment decisions and uncertainty features to recourse costs. The trained
network is subsequently embedded into the first-stage UC problem as a
mixed-integer linear program (MILP), allowing for explicit enforcement of
operational constraints while preserving the key uncertainty characteristics. A
scenario-embedding network is employed to enable dimensionality reduction and
feature aggregation across arbitrary scenario sets, serving as a data-driven
scenario reduction mechanism. Numerical experiments on IEEE 5-bus, 30-bus, and
118-bus systems demonstrate that the proposed neural two-stage stochastic
optimization method achieves solutions with an optimality gap of less than 1%,
while enabling orders-of-magnitude speedup compared to conventional MILP
solvers and decomposition-based methods. Moreover, the model's size remains
constant regardless of the number of scenarios, offering significant
scalability for large-scale stochastic unit commitment problems.

</details>


### [541] [Symptom-Driven Personalized Proton Pump Inhibitors Therapy Using Bayesian Neural Networks and Model Predictive Control](https://arxiv.org/abs/2507.09685)
*Yutong Li,Ilya Kolmanovsky*

Main category: eess.SY

TL;DR: 提出了一种基于症状的非侵入性PPI剂量调整框架，结合贝叶斯神经网络和模型预测控制，减少65%的PPI用量，同时保持95%的抑酸效果。


<details>
  <summary>Details</summary>
Motivation: 长期高剂量PPI治疗存在风险，但传统侵入性胃酸监测不实用，且患者间差异大。

Method: 使用贝叶斯神经网络预测症状，结合模型预测控制动态调整PPI剂量。

Result: 模拟研究显示，该方法比固定剂量减少65%的PPI用量，同时保持95%的抑酸概率。

Conclusion: 该方法为个性化PPI治疗提供了实用方案，减少治疗负担和过量风险。

Abstract: Proton Pump Inhibitors (PPIs) are the standard of care for gastric acid
disorders but carry significant risks when administered chronically at high
doses. Precise long-term control of gastric acidity is challenged by the
impracticality of invasive gastric acid monitoring beyond 72 hours and wide
inter-patient variability. We propose a noninvasive, symptom-based framework
that tailors PPI dosing solely on patient-reported reflux and digestive symptom
patterns. A Bayesian Neural Network prediction model learns to predict patient
symptoms and quantifies its uncertainty from historical symptom scores, meal,
and PPIs intake data. These probabilistic forecasts feed a chance-constrained
Model Predictive Control (MPC) algorithm that dynamically computes future PPI
doses to minimize drug usage while enforcing acid suppression with high
confidence - without any direct acid measurement. In silico studies over
diverse dietary schedules and virtual patient profiles demonstrate that our
learning-augmented MPC reduces total PPI consumption by 65 percent compared to
standard fixed regimens, while maintaining acid suppression with at least 95
percent probability. The proposed approach offers a practical path to
personalized PPI therapy, minimizing treatment burden and overdose risk without
invasive sensors.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [542] [An Empirical Evaluation of AI-Powered Non-Player Characters' Perceived Realism and Performance in Virtual Reality Environments](https://arxiv.org/abs/2507.10469)
*Mikko Korkiakoski,Saeid Sheikhi,Jesper Nyman,Jussi Saariniemi,Kalle Tapio,Panos Kostakos*

Main category: cs.HC

TL;DR: 论文探讨了AI驱动的NPC在VR审讯模拟器中的表现，评估了其真实性、可用性和系统性能，结果显示GPT-4 Turbo提升了NPC的真实性和交互性，但存在延迟和情感深度不足的问题。


<details>
  <summary>Details</summary>
Motivation: 研究AI驱动的NPC在VR环境中的表现，以提升用户体验和NPC的真实性。

Method: 使用GPT-4 Turbo驱动的NPC，通过用户研究评估其表现，包括SUS、GEQ和Believability问卷，以及延迟测量。

Result: 平均延迟7秒，真实性评分6.67/10，SUS得分79.44，显示良好可用性，但情感和个性表现一般。

Conclusion: 大语言模型能提升NPC的真实性和交互性，但需优化性能和情感表现以实现更沉浸的VR体验。

Abstract: Advancements in artificial intelligence (AI) have significantly enhanced the
realism and interactivity of non-player characters (NPCs) in virtual reality
(VR), creating more engaging and believable user experiences. This paper
evaluates AI-driven NPCs within a VR interrogation simulator, focusing on their
perceived realism, usability, and system performance. The simulator features
two AI-powered NPCs, a suspect, and a partner, using GPT-4 Turbo to engage
participants in a scenario to determine the suspect's guilt or innocence. A
user study with 18 participants assessed the system using the System Usability
Scale (SUS), Game Experience Questionnaire (GEQ), and a Virtual Agent
Believability Questionnaire, alongside latency measurements for speech-to-text
(STT), text-to-speech (TTS), OpenAI GPT-4 Turbo, and overall (cycle) latency.
Results showed an average cycle latency of 7 seconds, influenced by the
increasing conversational context. Believability scored 6.67 out of 10, with
high ratings in behavior, social relationships, and intelligence but moderate
scores in emotion and personality. The system achieved a SUS score of 79.44,
indicating good usability. These findings demonstrate the potential of large
language models to improve NPC realism and interaction in VR while highlighting
challenges in reducing system latency and enhancing emotional depth. This
research contributes to the development of more sophisticated AI-driven NPCs,
revealing the need for performance optimization to achieve increasingly
immersive virtual experiences.

</details>


### [543] [Towards Emotion Co-regulation with LLM-powered Socially Assistive Robots: Integrating LLM Prompts and Robotic Behaviors to Support Parent-Neurodivergent Child Dyads](https://arxiv.org/abs/2507.10427)
*Jing Li,Felix Schijve,Sheng Li,Yuye Yang,Jun Hu,Emilia Barakova*

Main category: cs.HC

TL;DR: 论文探讨了如何利用大型语言模型（LLM）与社会辅助机器人（SAR）结合，支持神经发育障碍儿童及其家长的情绪共同调节。


<details>
  <summary>Details</summary>
Motivation: 当前研究较少探索LLM与SAR结合在情绪共同调节中的应用，尤其是在神经发育障碍儿童家庭中。

Method: 研究开发了一个基于MiRo-E机器人平台的LLM驱动社交机器人，通过语音通信模块提供定制化干预。

Result: 初步测试显示，该系统对互动动态有积极影响，并有助于情绪调节，但也存在设计和技术的挑战。

Conclusion: 研究为未来LLM驱动的SAR在心理健康应用中的发展提供了设计启示。

Abstract: Socially Assistive Robotics (SAR) has shown promise in supporting emotion
regulation for neurodivergent children. Recently, there has been increasing
interest in leveraging advanced technologies to assist parents in co-regulating
emotions with their children. However, limited research has explored the
integration of large language models (LLMs) with SAR to facilitate emotion
co-regulation between parents and children with neurodevelopmental disorders.
To address this gap, we developed an LLM-powered social robot by deploying a
speech communication module on the MiRo-E robotic platform. This supervised
autonomous system integrates LLM prompts and robotic behaviors to deliver
tailored interventions for both parents and neurodivergent children. Pilot
tests were conducted with two parent-child dyads, followed by a qualitative
analysis. The findings reveal MiRo-E's positive impacts on interaction dynamics
and its potential to facilitate emotion regulation, along with identified
design and technical challenges. Based on these insights, we provide design
implications to advance the future development of LLM-powered SAR for mental
health applications.

</details>


### [544] [AInsight: Augmenting Expert Decision-Making with On-the-Fly Insights Grounded in Historical Data](https://arxiv.org/abs/2507.09100)
*Mohammad Abolnejadian,Shakiba Amirshahi,Matthew Brehmer,Anamaria Crisan*

Main category: cs.HC

TL;DR: 专家在决策对话中需实时利用历史数据，本文通过构建基于检索的LLM代理系统，在医患对话中实时生成相关数据洞察，验证了有效性但也面临挑战。


<details>
  <summary>Details</summary>
Motivation: 探索如何在实时决策对话中利用历史数据，以提升专家决策效率。

Method: 开发基于检索的LLM代理系统，实时监听对话并生成相关数据洞察，以医患对话为例进行验证。

Result: 系统在模拟实验中有效，但存在挑战，为后续研究指明方向。

Conclusion: 实时数据洞察系统在决策对话中具有潜力，需进一步优化。

Abstract: In decision-making conversations, experts must navigate complex choices and
make on-the-spot decisions while engaged in conversation. Although extensive
historical data often exists, the real-time nature of these scenarios makes it
infeasible for decision-makers to review and leverage relevant information.
This raises an interesting question: What if experts could utilize relevant
past data in real-time decision-making through insights derived from past data?
To explore this, we implemented a conversational user interface, taking
doctor-patient interactions as an example use case. Our system continuously
listens to the conversation, identifies patient problems and doctor-suggested
solutions, and retrieves related data from an embedded dataset, generating
concise insights using a pipeline built around a retrieval-based Large Language
Model (LLM) agent. We evaluated the prototype by embedding Health Canada
datasets into a vector database and conducting simulated studies using sample
doctor-patient dialogues, showing effectiveness but also challenges, setting
directions for the next steps of our work.

</details>


### [545] [SimStep: Chain-of-Abstractions for Incremental Specification and Debugging of AI-Generated Interactive Simulations](https://arxiv.org/abs/2507.09664)
*Zoe Kaputa,Anika Rajaram,Vryan Almanon Feliciano,Zhuoyue Lyu,Maneesh Agrawala,Hari Subramonyam*

Main category: cs.HC

TL;DR: 论文提出Chain-of-Abstractions (CoA)框架，通过分解生成过程为多个抽象层次，恢复编程的核心特性，如可追溯性和逐步优化，同时保留自然语言的灵活性。


<details>
  <summary>Details</summary>
Motivation: 生成式AI的编程提示方式为非程序员（如教育工作者）提供了便利，但绕过了直接编码，导致编程的核心特性（如可追溯性、逐步优化）丢失。

Method: 提出CoA框架，将合成过程分解为一系列认知上有意义的抽象层次，并在SimStep环境中实现，支持教师通过四个中间抽象层次创建模拟内容。

Result: 评估显示，CoA在编程提示工作流中提供了更高的创作控制和可解释性。

Conclusion: CoA框架在保留自然语言灵活性的同时，恢复了编程的核心特性，为非程序员提供了更可控的创作工具。

Abstract: Programming-by-prompting with generative AI offers a new paradigm for
end-user programming, shifting the focus from syntactic fluency to semantic
intent. This shift holds particular promise for non-programmers such as
educators, who can describe instructional goals in natural language to generate
interactive learning content. Yet in bypassing direct code authoring, many of
programming's core affordances - such as traceability, stepwise refinement, and
behavioral testing - are lost. We propose the Chain-of-Abstractions (CoA)
framework as a way to recover these affordances while preserving the expressive
flexibility of natural language. CoA decomposes the synthesis process into a
sequence of cognitively meaningful, task-aligned representations that function
as checkpoints for specification, inspection, and refinement. We instantiate
this approach in SimStep, an authoring environment for teachers that scaffolds
simulation creation through four intermediate abstractions: Concept Graph,
Scenario Graph, Learning Goal Graph, and UI Interaction Graph. To address
ambiguities and misalignments, SimStep includes an inverse correction process
that surfaces in-filled model assumptions and enables targeted revision without
requiring users to manipulate code. Evaluations with educators show that CoA
enables greater authoring control and interpretability in
programming-by-prompting workflows.

</details>


### [546] [Visual Analytics for Explainable and Trustworthy Artificial Intelligence](https://arxiv.org/abs/2507.10240)
*Angelos Chatzimparmpas*

Main category: cs.HC

TL;DR: 论文探讨了如何通过可视化分析（VA）提升AI系统的透明度，从而增强专家对AI的信任，并提出了一个设计空间和VA仪表盘的实现。


<details>
  <summary>Details</summary>
Motivation: AI系统在医疗等领域潜力巨大，但缺乏透明度阻碍了其广泛应用。

Method: 结合AI模型与交互式可视化，定义并分类VA解决方案，提出设计空间并展示已开发的VA仪表盘。

Result: VA能够支持AI管道的各个阶段任务，如数据处理、模型调试等。

Conclusion: 可视化分析是提升AI透明度和信任的有效途径。

Abstract: Our society increasingly depends on intelligent systems to solve complex
problems, ranging from recommender systems suggesting the next movie to watch
to AI models assisting in medical diagnoses for hospitalized patients. With the
iterative improvement of diagnostic accuracy and efficiency, AI holds
significant potential to mitigate medical misdiagnoses by preventing numerous
deaths and reducing an economic burden of approximately 450 EUR billion
annually. However, a key obstacle to AI adoption lies in the lack of
transparency: many automated systems function as "black boxes," providing
predictions without revealing the underlying processes. This opacity can hinder
experts' ability to trust and rely on AI systems. Visual analytics (VA)
provides a compelling solution by combining AI models with interactive
visualizations. These specialized charts and graphs empower users to
incorporate their domain expertise to refine and improve the models, bridging
the gap between AI and human understanding. In this work, we define,
categorize, and explore how VA solutions can foster trust across the stages of
a typical AI pipeline. We propose a design space for innovative visualizations
and present an overview of our previously developed VA dashboards, which
support critical tasks within the various pipeline stages, including data
processing, feature engineering, hyperparameter tuning, understanding,
debugging, refining, and comparing models.

</details>


<div id='physics.ao-ph'></div>

# physics.ao-ph [[Back]](#toc)

### [547] [Investigating the Robustness of Extreme Precipitation Super-Resolution Across Climates](https://arxiv.org/abs/2507.09166)
*Louise Largeau,Erwan Koch,David Leutwyler,Gregoire Mariethoz,Valerie Chavez-Demoulin,Tom Beucler*

Main category: physics.ao-ph

TL;DR: 该论文提出了一种直接超分辨率目标变量概率分布参数的方法，以改进极端降水预测的稳健性，并引入“稳健性差距”概念评估模型在气候变化下的表现。


<details>
  <summary>Details</summary>
Motivation: 由于气候模型的空间分辨率较低，直接用于预测极端降水等社会相关变量存在限制。传统降尺度方法通过生成大量集合估计极端条件分布，难以评估其在气候变化引起的分布偏移下的稳健性。

Method: 使用向量广义线性和加性模型，直接从粗降水场和地形数据中超分辨率夏季小时极端降水的广义极值分布参数。引入“稳健性差距”概念，评估模型在伪全球变暖情景下的泛化能力。

Result: 在瑞士的完美模型框架下，验证了方法的有效性，并确定了基于降水和地形空间自相关和互相关的超分辨率因子上限。

Conclusion: 该方法适用于参数化分布变量，为理解经验降尺度在气候变化和极端条件下的泛化提供了模型无关的诊断工具。

Abstract: The coarse spatial resolution of gridded climate models, such as general
circulation models, limits their direct use in projecting socially relevant
variables like extreme precipitation. Most downscaling methods estimate the
conditional distributions of extremes by generating large ensembles,
complicating the assessment of robustness under distributional shifts, such as
those induced by climate change. To better understand and potentially improve
robustness, we propose super-resolving the parameters of the target variable's
probability distribution directly using analytically tractable mappings. Within
a perfect-model framework over Switzerland, we demonstrate that vector
generalized linear and additive models can super-resolve the generalized
extreme value distribution of summer hourly precipitation extremes from coarse
precipitation fields and topography. We introduce the notion of a "robustness
gap", defined as the difference in predictive error between present-trained and
future-trained models, and use it to diagnose how model structure affects the
generalization of each quantile to a pseudo-global warming scenario. By
evaluating multiple model configurations, we also identify an upper limit on
the super-resolution factor based on the spatial auto- and cross-correlation of
precipitation and elevation, beyond which coarse precipitation loses predictive
value. Our framework is broadly applicable to variables governed by parametric
distributions and offers a model-agnostic diagnostic for understanding when and
why empirical downscaling generalizes to climate change and extremes.

</details>


<div id='cs.CE'></div>

# cs.CE [[Back]](#toc)

### [548] [StockSim: A Dual-Mode Order-Level Simulator for Evaluating Multi-Agent LLMs in Financial Markets](https://arxiv.org/abs/2507.09255)
*Charidimos Papadakis,Giorgos Filandrianos,Angeliki Dimitriou,Maria Lymperaiou,Konstantinos Thomas,Giorgos Stamou*

Main category: cs.CE

TL;DR: StockSim是一个开源仿真平台，用于在真实金融决策场景中系统评估大语言模型（LLMs），弥补了以往工具在范围和细节上的不足。


<details>
  <summary>Details</summary>
Motivation: 现有工具在金融决策评估中范围有限，无法全面模拟市场动态或考虑关键现实因素（如延迟、滑点等），StockSim旨在填补这一空白。

Method: StockSim通过建模市场动态、支持多种仿真模式，并引入现实因素（如延迟、滑点），提供了一个可扩展的基于角色的代理框架。

Result: StockSim成为评估LLM交易代理的独特测试平台，支持异构交易策略和多代理协调。

Conclusion: StockSim为NLP研究提供了更真实、全面的评估工具，代码已开源。

Abstract: We present StockSim, an open-source simulation platform for systematic
evaluation of large language models (LLMs) in realistic financial
decision-making scenarios. Unlike previous toolkits that offer limited scope,
StockSim delivers a comprehensive system that fully models market dynamics and
supports diverse simulation modes of varying granularity. It incorporates
critical real-world factors, such as latency, slippage, and order-book
microstructure, that were previously neglected, enabling more faithful and
insightful assessment of LLM-based trading agents. An extensible, role-based
agent framework supports heterogeneous trading strategies and multi-agent
coordination, making StockSim a uniquely capable testbed for NLP research on
reasoning under uncertainty and sequential decision-making. We open-source all
our code at https: //github.com/harrypapa2002/StockSim.

</details>


### [549] [FinTeam: A Multi-Agent Collaborative Intelligence System for Comprehensive Financial Scenarios](https://arxiv.org/abs/2507.10448)
*Yingqian Wu,Qiushi Wang,Zefei Long,Rong Ye,Zhongtian Lu,Xianyin Zhang,Bingxuan Li,Wei Chen,Liwen Zhang,Zhongyu Wei*

Main category: cs.CE

TL;DR: FinTeam是一个多智能体协作系统，用于生成财务报告，通过四个专门训练的LLM代理（文档分析员、分析师、会计师和顾问）协同工作，显著提升了财务任务的性能。


<details>
  <summary>Details</summary>
Motivation: 现有LLM模型在简单QA任务上表现良好，但无法全面分析复杂的真实财务场景，因此需要一种更专业的解决方案。

Method: 提出FinTeam系统，包含四个专门训练的LLM代理，分别负责文档分析、分析、会计和咨询任务，并使用构建的数据集进行训练。

Result: FinTeam在真实在线投资论坛构建的任务上表现优异，人类评估接受率达62.00%，优于GPT-4o和Xuanyuan等基线模型，并在FinCUGE和FinEval上分别提升7.43%和2.06%。

Conclusion: FinTeam通过多智能体协作显著提升了财务报告生成的质量和准确性，为复杂财务任务提供了有效解决方案。

Abstract: Financial report generation tasks range from macro- to micro-economics
analysis, also requiring extensive data analysis. Existing LLM models are
usually fine-tuned on simple QA tasks and cannot comprehensively analyze real
financial scenarios. Given the complexity, financial companies often distribute
tasks among departments. Inspired by this, we propose FinTeam, a financial
multi-agent collaborative system, with a workflow with four LLM agents:
document analyzer, analyst, accountant, and consultant. We train these agents
with specific financial expertise using constructed datasets. We evaluate
FinTeam on comprehensive financial tasks constructed from real online
investment forums, including macroeconomic, industry, and company analysis. The
human evaluation shows that by combining agents, the financial reports generate
from FinTeam achieved a 62.00% acceptance rate, outperforming baseline models
like GPT-4o and Xuanyuan. Additionally, FinTeam's agents demonstrate a 7.43%
average improvement on FinCUGE and a 2.06% accuracy boost on FinEval. Project
is available at https://github.com/FudanDISC/DISC-FinLLM/.

</details>


<div id='physics.soc-ph'></div>

# physics.soc-ph [[Back]](#toc)

### [550] [Advancing network resilience theories with symbolized reinforcement learning](https://arxiv.org/abs/2507.08827)
*Yu Zheng,Jingtao Ding,Depeng Jin,Jianxi Gao,Yong Li*

Main category: physics.soc-ph

TL;DR: 论文提出了一种自动发现网络韧性理论的方法，结合拓扑和动态因素，改进了现有理论，并提供了系统性崩溃的早期预警信号。


<details>
  <summary>Details</summary>
Motivation: 研究网络韧性的理论通常仅从拓扑角度出发，忽略了系统动态的关键作用，导致理论不完整。

Method: 通过AI学习解决复杂网络拆解问题，并将其攻击策略符号化为理论公式，提出了一种自归纳方法。

Result: 该方法首次结合拓扑和动态因素，发现了节点度与状态相关性对网络韧性的影响，并改进了现有理论的准确性（提升37.5%）。

Conclusion: 该研究通过AI辅助，显著提升了人类对复杂网络韧性的理解，为预防系统性崩溃提供了新工具。

Abstract: Many complex networks display remarkable resilience under external
perturbations, internal failures and environmental changes, yet they can
swiftly deteriorate into dysfunction upon the removal of a few keystone nodes.
Discovering theories that measure network resilience offers the potential to
prevent catastrophic collapses--from species extinctions to financial
crise--with profound implications for real-world systems. Current resilience
theories address the problem from a single perspective of topology, neglecting
the crucial role of system dynamics, due to the intrinsic complexity of the
coupling between topology and dynamics which exceeds the capabilities of human
analytical methods. Here, we report an automatic method for resilience theory
discovery, which learns from how AI solves a complicated network dismantling
problem and symbolizes its network attack strategies into theoretical formulas.
This proposed self-inductive approach discovers the first resilience theory
that accounts for both topology and dynamics, highlighting how the correlation
between node degree and state shapes overall network resilience, and offering
insights for designing early warning signals of systematic collapses.
Additionally, our approach discovers formulas that refine existing
well-established resilience theories with over 37.5% improvement in accuracy,
significantly advancing human understanding of complex networks with AI.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [551] [Physics-informed machine learning: A mathematical framework with applications to time series forecasting](https://arxiv.org/abs/2507.08906)
*Nathan Doumèche*

Main category: stat.ML

TL;DR: 本文研究了物理信息机器学习（PIML）的统计特性，并将其与核方法结合，开发了高效的GPU算法。同时探讨了其在能源信号预测和约束时间序列设计中的应用。


<details>
  <summary>Details</summary>
Motivation: 物理信息机器学习（PIML）通过将物理知识融入机器学习模型，为解决复杂问题提供了新方法。本文旨在分析其统计特性并探索实际应用。

Method: 首先分析物理信息神经网络（PINNs）的近似性、一致性、过拟合和收敛性；然后将PIML问题转化为核方法，利用核岭回归工具；最后开发高效GPU算法。

Result: 提出了基于核方法的PIML新算法，并在能源信号预测和时间序列约束设计中验证了其有效性。

Conclusion: PIML框架在理论和应用上均表现出潜力，尤其在结合核方法和高效计算时，为复杂问题提供了新解决方案。

Abstract: Physics-informed machine learning (PIML) is an emerging framework that
integrates physical knowledge into machine learning models. This physical prior
often takes the form of a partial differential equation (PDE) system that the
regression function must satisfy. In the first part of this dissertation, we
analyze the statistical properties of PIML methods. In particular, we study the
properties of physics-informed neural networks (PINNs) in terms of
approximation, consistency, overfitting, and convergence. We then show how PIML
problems can be framed as kernel methods, making it possible to apply the tools
of kernel ridge regression to better understand their behavior. In addition, we
use this kernel formulation to develop novel physics-informed algorithms and
implement them efficiently on GPUs. The second part explores industrial
applications in forecasting energy signals during atypical periods. We present
results from the Smarter Mobility challenge on electric vehicle charging
occupancy and examine the impact of mobility on electricity demand. Finally, we
introduce a physics-constrained framework for designing and enforcing
constraints in time series, applying it to load forecasting and tourism
forecasting in various countries.

</details>


### [552] [The Bayesian Approach to Continual Learning: An Overview](https://arxiv.org/abs/2507.08922)
*Tameem Adel*

Main category: stat.ML

TL;DR: 该论文综述了贝叶斯持续学习的定义、分类、算法及其与心理学和其他学习领域的联系，并探讨了未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 持续学习是一种在线学习范式，旨在通过不断积累新任务的知识而不遗忘旧知识，扩展深度学习模型的适用性。贝叶斯推理为持续学习提供了更新模型知识的理论基础。

Method: 论文首先定义贝叶斯持续学习及其相关领域，然后提出分类法对算法进行系统分类，并分析了一些最先进的贝叶斯持续学习算法。

Result: 论文总结了贝叶斯持续学习的现状，包括任务增量学习和类增量学习等设置，并探讨了其与心理学和其他学习领域的联系。

Conclusion: 论文指出了当前贝叶斯持续学习的挑战，并提出了未来研究的潜在方向。

Abstract: Continual learning is an online paradigm where a learner continually
accumulates knowledge from different tasks encountered over sequential time
steps. Importantly, the learner is required to extend and update its knowledge
without forgetting about the learning experience acquired from the past, and
while avoiding the need to retrain from scratch. Given its sequential nature
and its resemblance to the way humans think, continual learning offers an
opportunity to address several challenges which currently stand in the way of
widening the range of applicability of deep models to further real-world
problems. The continual need to update the learner with data arriving
sequentially strikes inherent congruence between continual learning and
Bayesian inference which provides a principal platform to keep updating the
prior beliefs of a model given new data, without completely forgetting the
knowledge acquired from the old data. This survey inspects different settings
of Bayesian continual learning, namely task-incremental learning and
class-incremental learning. We begin by discussing definitions of continual
learning along with its Bayesian setting, as well as the links with related
fields, such as domain adaptation, transfer learning and meta-learning.
Afterwards, we introduce a taxonomy offering a comprehensive categorization of
algorithms belonging to the Bayesian continual learning paradigm. Meanwhile, we
analyze the state-of-the-art while zooming in on some of the most prominent
Bayesian continual learning algorithms to date. Furthermore, we shed some light
on links between continual learning and developmental psychology, and
correspondingly introduce analogies between both fields. We follow that with a
discussion of current challenges, and finally conclude with potential areas for
future research on Bayesian continual learning.

</details>


### [553] [Fixed-Confidence Multiple Change Point Identification under Bandit Feedback](https://arxiv.org/abs/2507.08994)
*Joseph Lazzaro,Ciara Pike-Burke*

Main category: stat.ML

TL;DR: 论文提出了一种固定置信度的分段常数多臂老虎机问题，用于快速识别函数突变点，并证明了其方法的渐近最优性。


<details>
  <summary>Details</summary>
Motivation: 分段常数函数在多个领域中描述现象时突变点的快速准确识别具有实际需求。

Method: 提出了一种基于Track-and-Stop的简单高效方法，采样集中在突变点附近，采样数与变化幅度成反比。

Result: 理论证明方法在多数情况下渐近最优，实验验证了其高效性。

Conclusion: 该方法在固定置信度下能高效识别突变点，适用于多种实际场景。

Abstract: Piecewise constant functions describe a variety of real-world phenomena in
domains ranging from chemistry to manufacturing. In practice, it is often
required to confidently identify the locations of the abrupt changes in these
functions as quickly as possible. For this, we introduce a fixed-confidence
piecewise constant bandit problem. Here, we sequentially query points in the
domain and receive noisy evaluations of the function under bandit feedback. We
provide instance-dependent lower bounds for the complexity of change point
identification in this problem. These lower bounds illustrate that an optimal
method should focus its sampling efforts adjacent to each of the change points,
and the number of samples around each change point should be inversely
proportional to the magnitude of the change. Building on this, we devise a
simple and computationally efficient variant of Track-and-Stop and prove that
it is asymptotically optimal in many regimes. We support our theoretical
findings with experimental results in synthetic environments demonstrating the
efficiency of our method.

</details>


### [554] [Optimal High-probability Convergence of Nonlinear SGD under Heavy-tailed Noise via Symmetrization](https://arxiv.org/abs/2507.09093)
*Aleksandar Armacki,Dragana Bajovic,Dusan Jakovetic,Soummya Kar*

Main category: stat.ML

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: We study convergence in high-probability of SGD-type methods in non-convex
optimization and the presence of heavy-tailed noise. To combat the heavy-tailed
noise, a general black-box nonlinear framework is considered, subsuming
nonlinearities like sign, clipping, normalization and their smooth
counterparts. Our first result shows that nonlinear SGD (N-SGD) achieves the
rate $\widetilde{\mathcal{O}}(t^{-1/2})$, for any noise with unbounded moments
and a symmetric probability density function (PDF). Crucially, N-SGD has
exponentially decaying tails, matching the performance of linear SGD under
light-tailed noise. To handle non-symmetric noise, we propose two novel
estimators, based on the idea of noise symmetrization. The first, dubbed
Symmetrized Gradient Estimator (SGE), assumes a noiseless gradient at any
reference point is available at the start of training, while the second, dubbed
Mini-batch SGE (MSGE), uses mini-batches to estimate the noiseless gradient.
Combined with the nonlinear framework, we get N-SGE and N-MSGE methods,
respectively, both achieving the same convergence rate and exponentially
decaying tails as N-SGD, while allowing for non-symmetric noise with unbounded
moments and PDF satisfying a mild technical condition, with N-MSGE additionally
requiring bounded noise moment of order $p \in (1,2]$. Compared to works
assuming noise with bounded $p$-th moment, our results: 1) are based on a novel
symmetrization approach; 2) provide a unified framework and relaxed moment
conditions; 3) imply optimal oracle complexity of N-SGD and N-SGE, strictly
better than existing works when $p < 2$, while the complexity of N-MSGE is
close to existing works. Compared to works assuming symmetric noise with
unbounded moments, we: 1) provide a sharper analysis and improved rates; 2)
facilitate state-dependent symmetric noise; 3) extend the strong guarantees to
non-symmetric noise.

</details>


### [555] [CoVAE: Consistency Training of Variational Autoencoders](https://arxiv.org/abs/2507.09103)
*Gianluigi Silvestri,Luca Ambrogioni*

Main category: stat.ML

TL;DR: CoVAE提出了一种单阶段生成自编码框架，结合一致性模型技术，避免了传统两阶段训练的计算开销，并能高效生成高质量样本。


<details>
  <summary>Details</summary>
Motivation: 传统生成方法依赖两阶段训练（如VAE降维后生成模型训练），计算开销大且采样时间长。CoVAE旨在简化流程，提升效率。

Method: CoVAE通过一致性损失和变分正则化训练VAE架构，编码器学习渐进式潜在表示，解码器采用一致性损失，减少对学习先验的依赖。

Result: CoVAE在一步或少量步骤内生成高质量样本，性能优于传统VAE及其他单阶段方法。

Conclusion: CoVAE为自编码和扩散式生成建模提供了统一框架，是实现高效一步生成的有力途径。

Abstract: Current state-of-the-art generative approaches frequently rely on a two-stage
training procedure, where an autoencoder (often a VAE) first performs
dimensionality reduction, followed by training a generative model on the
learned latent space. While effective, this introduces computational overhead
and increased sampling times. We challenge this paradigm by proposing
Consistency Training of Variational AutoEncoders (CoVAE), a novel single-stage
generative autoencoding framework that adopts techniques from consistency
models to train a VAE architecture. The CoVAE encoder learns a progressive
series of latent representations with increasing encoding noise levels,
mirroring the forward processes of diffusion and flow matching models. This
sequence of representations is regulated by a time dependent $\beta$ parameter
that scales the KL loss. The decoder is trained using a consistency loss with
variational regularization, which reduces to a conventional VAE loss at the
earliest latent time. We show that CoVAE can generate high-quality samples in
one or few steps without the use of a learned prior, significantly
outperforming equivalent VAEs and other single-stage VAEs methods. Our approach
provides a unified framework for autoencoding and diffusion-style generative
modeling and provides a viable route for one-step generative high-performance
autoencoding. Our code is publicly available at
https://github.com/gisilvs/covae.

</details>


### [556] [A Generalization Theory for Zero-Shot Prediction](https://arxiv.org/abs/2507.09128)
*Ronak Mehta,Zaid Harchaoui*

Main category: stat.ML

TL;DR: 论文提出了一个理论框架，用于理解零样本预测在机器学习中的泛化能力，重点分析了其目标量和关键条件独立性关系。


<details>
  <summary>Details</summary>
Motivation: 探索预训练基础模型在无标签数据下游任务中的泛化能力，特别是零样本预测的理论基础。

Method: 提出理论框架，识别零样本预测的目标量和关键条件独立性关系。

Result: 明确了零样本预测的学习目标和泛化能力的条件独立性关系。

Conclusion: 理论框架为理解零样本预测的泛化机制提供了基础。

Abstract: A modern paradigm for generalization in machine learning and AI consists of
pre-training a task-agnostic foundation model, generally obtained using
self-supervised and multimodal contrastive learning. The resulting
representations can be used for prediction on a downstream task for which no
labeled data is available. We present a theoretical framework to better
understand this approach, called zero-shot prediction. We identify the target
quantities that zero-shot prediction aims to learn, or learns in passing, and
the key conditional independence relationships that enable its generalization
ability.

</details>


### [557] [A Randomized Algorithm for Sparse PCA based on the Basic SDP Relaxation](https://arxiv.org/abs/2507.09148)
*Alberto Del Pia,Dekun Zhou*

Main category: stat.ML

TL;DR: 本文提出了一种基于SDP松弛的随机近似算法，用于解决稀疏主成分分析（SPCA）问题，并证明了其近似比和适用条件。


<details>
  <summary>Details</summary>
Motivation: 稀疏主成分分析（SPCA）是一种重要的降维技术，但其计算复杂度高（NP-hard），因此需要高效的近似算法。

Method: 基于基本SDP松弛的随机近似算法，通过多次调用实现高概率的近似比。

Result: 算法在满足技术假设的条件下，平均近似比有界，且在特定模型中达到接近最优的近似比。数值实验验证了算法的有效性。

Conclusion: 该算法在理论和实际应用中均表现出色，为SPCA提供了一种高效的解决方案。

Abstract: Sparse Principal Component Analysis (SPCA) is a fundamental technique for
dimensionality reduction, and is NP-hard. In this paper, we introduce a
randomized approximation algorithm for SPCA, which is based on the basic SDP
relaxation. Our algorithm has an approximation ratio of at most the sparsity
constant with high probability, if called enough times. Under a technical
assumption, which is consistently satisfied in our numerical tests, the average
approximation ratio is also bounded by $\mathcal{O}(\log{d})$, where $d$ is the
number of features. We show that this technical assumption is satisfied if the
SDP solution is low-rank, or has exponentially decaying eigenvalues. We then
present a broad class of instances for which this technical assumption holds.
We also demonstrate that in a covariance model, which generalizes the spiked
Wishart model, our proposed algorithm achieves a near-optimal approximation
ratio. We demonstrate the efficacy of our algorithm through numerical results
on real-world datasets.

</details>


### [558] [Uncovering symmetric and asymmetric species associations from community and environmental data](https://arxiv.org/abs/2507.09317)
*Sara Si-Moussi,Esther Galbrun,Mickael Hedde,Giovanni Poggiato,Matthias Rohr,Wilfried Thuiller*

Main category: stat.ML

TL;DR: 该论文提出了一种机器学习框架，用于从物种群落和环境数据中提取双向（对称和非对称）物种关联。


<details>
  <summary>Details</summary>
Motivation: 现有模型通常假设物种间关系是对称的，而实际上生物相互作用可能是非对称的，因此需要一种新方法来捕捉这种复杂性。

Method: 框架通过建模物种间的定向影响（源物种对群落的影响和目标物种对群落的响应），并结合多物种条件生成模型，分析环境驱动因素和生物关联的不同模式。

Result: 模拟和实证数据表明，该框架能有效恢复已知的对称和非对称关联，并在与其他模型比较中表现出优越性。

Conclusion: 该框架直观、模块化，适用于多种分类群，为研究生物相互作用提供了新工具。

Abstract: There is no much doubt that biotic interactions shape community assembly and
ultimately the spatial co-variations between species. There is a hope that the
signal of these biotic interactions can be observed and retrieved by
investigating the spatial associations between species while accounting for the
direct effects of the environment. By definition, biotic interactions can be
both symmetric and asymmetric. Yet, most models that attempt to retrieve
species associations from co-occurrence or co-abundance data internally assume
symmetric relationships between species. Here, we propose and validate a
machine-learning framework able to retrieve bidirectional associations by
analyzing species community and environmental data.
  Our framework (1) models pairwise species associations as directed influences
from a source to a target species, parameterized with two species-specific
latent embeddings: the effect of the source species on the community, and the
response of the target species to the community; and (2) jointly fits these
associations within a multi-species conditional generative model with different
modes of interactions between environmental drivers and biotic associations.
Using both simulated and empirical data, we demonstrate the ability of our
framework to recover known asymmetric and symmetric associations and highlight
the properties of the learned association networks. By comparing our approach
to other existing models such as joint species distribution models and
probabilistic graphical models, we show its superior capacity at retrieving
symmetric and asymmetric interactions. The framework is intuitive, modular and
broadly applicable across various taxonomic groups.

</details>


### [559] [An Algorithm for Identifying Interpretable Subgroups With Elevated Treatment Effects](https://arxiv.org/abs/2507.09494)
*Albert Chiu*

Main category: stat.ML

TL;DR: 提出一种算法，用于识别具有显著治疗效果的易解释子组，基于个体或条件平均治疗效果（CATE）估计。子组通过“规则集”描述，平衡子组大小与效果大小的目标函数生成帕累托最优规则集。


<details>
  <summary>Details</summary>
Motivation: 现有CATE估计方法常产生高维且难以解释的结果，需要一种方法提取关键信息以辅助决策和科学理解。

Method: 使用规则集（如（条件A AND 条件B）OR（条件C））描述子组，提出平衡子组大小与效果大小的目标函数，生成帕累托最优规则集。

Result: 通过模拟和实证案例验证了方法的实用性和局限性。

Conclusion: 该方法能够从复杂模型中提取易解释的子组信息，为决策和科学理解提供支持。

Abstract: We introduce an algorithm for identifying interpretable subgroups with
elevated treatment effects, given an estimate of individual or conditional
average treatment effects (CATE). Subgroups are characterized by ``rule sets''
-- easy-to-understand statements of the form (Condition A AND Condition B) OR
(Condition C) -- which can capture high-order interactions while retaining
interpretability. Our method complements existing approaches for estimating the
CATE, which often produce high dimensional and uninterpretable results, by
summarizing and extracting critical information from fitted models to aid
decision making, policy implementation, and scientific understanding. We
propose an objective function that trades-off subgroup size and effect size,
and varying the hyperparameter that controls this trade-off results in a
``frontier'' of Pareto optimal rule sets, none of which dominates the others
across all criteria. Valid inference is achievable through sample splitting. We
demonstrate the utility and limitations of our method using simulated and
empirical examples.

</details>


### [560] [Signed Graph Learning: Algorithms and Theory](https://arxiv.org/abs/2507.09717)
*Abdullah Karaaslanli,Bisakh Banerjee,Tapabrata Maiti,Selin Aviyente*

Main category: stat.ML

TL;DR: 提出了一种从平滑有符号图信号中学习有符号图结构的方法，使用净拉普拉斯算子作为图移位算子，并通过非凸优化和ADMM算法求解。


<details>
  <summary>Details</summary>
Motivation: 现实数据常以图结构表示，但现有研究多关注无符号图，而许多生物和社会系统更适合用有符号图描述。

Method: 利用净拉普拉斯算子定义平滑有符号图信号，通过非凸优化最小化信号总变差，采用ADMM算法和快速线性复杂度算法求解。

Result: 提供了算法收敛性证明和估计误差界，并在模拟数据和基因调控网络推断中验证了方法的有效性。

Conclusion: 该方法能有效学习有符号图结构，优于现有方法。

Abstract: Real-world data is often represented through the relationships between data
samples, forming a graph structure. In many applications, it is necessary to
learn this graph structure from the observed data. Current graph learning
research has primarily focused on unsigned graphs, which consist only of
positive edges. However, many biological and social systems are better
described by signed graphs that account for both positive and negative
interactions, capturing similarity and dissimilarity between samples. In this
paper, we develop a method for learning signed graphs from a set of smooth
signed graph signals. Specifically, we employ the net Laplacian as a graph
shift operator (GSO) to define smooth signed graph signals as the outputs of a
low-pass signed graph filter defined by the net Laplacian. The signed graph is
then learned by formulating a non-convex optimization problem where the total
variation of the observed signals is minimized with respect to the net
Laplacian. The proposed problem is solved using alternating direction method of
multipliers (ADMM) and a fast algorithm reducing the per-ADMM iteration
complexity from quadratic to linear in the number of nodes is introduced.
Furthermore, theoretical proofs of convergence for the algorithm and a bound on
the estimation error of the learned net Laplacian as a function of sample size,
number of nodes, and graph topology are provided. Finally, the proposed method
is evaluated on simulated data and gene regulatory network inference problem
and compared to existing signed graph learning methods.

</details>


### [561] [Discovering Governing Equations in the Presence of Uncertainty](https://arxiv.org/abs/2507.09740)
*Ridwan Olabiyi,Han Hu,Ashif Iquebal*

Main category: stat.ML

TL;DR: 提出了一种随机逆物理发现（SIP）框架，通过处理系统变异性与测量噪声，从噪声和有限数据中一致地发现动力学系统的控制方程。


<details>
  <summary>Details</summary>
Motivation: 传统方法假设固定系数确定性模型，难以应对现实系统中显著的输入变异性与噪声数据，因此需要一种新方法。

Method: SIP框架将未知系数视为随机变量，通过最小化后验样本与经验数据分布之间的Kullback-Leibler散度推断其后验分布。

Result: 在四个经典问题上，SIP一致识别出正确方程，系数均方根误差平均降低82%，后验分布提供可解释的模型与量化不确定性。

Conclusion: SIP为噪声、多变和数据有限的环境提供了一种鲁棒且数据高效的方法，用于一致的物理发现。

Abstract: In the study of complex dynamical systems, understanding and accurately
modeling the underlying physical processes is crucial for predicting system
behavior and designing effective interventions. Yet real-world systems exhibit
pronounced input (or system) variability and are observed through noisy,
limited data conditions that confound traditional discovery methods that assume
fixed-coefficient deterministic models. In this work, we theorize that
accounting for system variability together with measurement noise is the key to
consistently discover the governing equations underlying dynamical systems. As
such, we introduce a stochastic inverse physics-discovery (SIP) framework that
treats the unknown coefficients as random variables and infers their posterior
distribution by minimizing the Kullback-Leibler divergence between the
push-forward of the posterior samples and the empirical data distribution.
Benchmarks on four canonical problems -- the Lotka-Volterra predator-prey
system (multi- and single-trajectory), the historical Hudson Bay lynx-hare
data, the chaotic Lorenz attractor, and fluid infiltration in porous media
using low- and high-viscosity liquids -- show that SIP consistently identifies
the correct equations and lowers coefficient root-mean-square error by an
average of 82\% relative to the Sparse Identification of Nonlinear Dynamics
(SINDy) approach and its Bayesian variant. The resulting posterior
distributions yield 95\% credible intervals that closely track the observed
trajectories, providing interpretable models with quantified uncertainty. SIP
thus provides a robust, data-efficient approach for consistent physics
discovery in noisy, variable, and data-limited settings.

</details>


### [562] [Regret Analysis of Posterior Sampling-Based Expected Improvement for Bayesian Optimization](https://arxiv.org/abs/2507.09828)
*Shion Takeno,Yu Inatsu,Masayuki Karasuyama,Ichiro Takeuchi*

Main category: stat.ML

TL;DR: 本文分析了基于后验采样的随机期望改进（EI）方法，证明了其在高斯过程假设下具有次线性贝叶斯累积遗憾界，并通过数值实验验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 尽管期望改进（EI）在实际应用中表现优异，但其理论分析相对不足，本文旨在填补这一空白。

Method: 提出了一种随机化的EI变体，通过从后验样本路径的最大值中评估EI。

Result: 证明了该方法在高斯过程假设下具有次线性贝叶斯累积遗憾界。

Conclusion: 数值实验验证了该方法的有效性，为EI的理论分析提供了支持。

Abstract: Bayesian optimization is a powerful tool for optimizing an
expensive-to-evaluate black-box function. In particular, the effectiveness of
expected improvement (EI) has been demonstrated in a wide range of
applications. However, theoretical analyses of EI are limited compared with
other theoretically established algorithms. This paper analyzes a randomized
variant of EI, which evaluates the EI from the maximum of the posterior sample
path. We show that this posterior sampling-based random EI achieves the
sublinear Bayesian cumulative regret bounds under the assumption that the
black-box function follows a Gaussian process. Finally, we demonstrate the
effectiveness of the proposed method through numerical experiments.

</details>


### [563] [Simulating Biases for Interpretable Fairness in Offline and Online Classifiers](https://arxiv.org/abs/2507.10154)
*Ricardo Inácio,Zafeiris Kokkinogenis,Vitor Cerqueira,Carlos Soares*

Main category: stat.ML

TL;DR: 论文提出了一种通过可控偏置注入生成合成数据集的框架，并利用二阶Shapley值解释缓解措施如何影响分类器对数据特征的利用。


<details>
  <summary>Details</summary>
Motivation: 预测模型可能因训练数据中的偏置而强化不公平决策，因此需要开发方法来评估和缓解这种偏置。

Method: 开发了一个基于代理的模型（ABM）模拟贷款申请过程，生成带有系统性偏置的合成数据集，并应用分类器评估偏置影响。同时提出了一种新的可解释性技术。

Result: 实验表明，通过在不同建模阶段（如预处理和处理中）应用缓解措施，可以有效减少偏置对预测结果的影响。

Conclusion: 该研究为可控偏置注入和公平性评估提供了实用框架，并通过可解释性技术增强了模型透明度。

Abstract: Predictive models often reinforce biases which were originally embedded in
their training data, through skewed decisions. In such cases, mitigation
methods are critical to ensure that, regardless of the prevailing disparities,
model outcomes are adjusted to be fair. To assess this, datasets could be
systematically generated with specific biases, to train machine learning
classifiers. Then, predictive outcomes could aid in the understanding of this
bias embedding process. Hence, an agent-based model (ABM), depicting a loan
application process that represents various systemic biases across two
demographic groups, was developed to produce synthetic datasets. Then, by
applying classifiers trained on them to predict loan outcomes, we can assess
how biased data leads to unfairness. This highlights a main contribution of
this work: a framework for synthetic dataset generation with controllable bias
injection. We also contribute with a novel explainability technique, which
shows how mitigations affect the way classifiers leverage data features, via
second-order Shapley values. In experiments, both offline and online learning
approaches are employed. Mitigations are applied at different stages of the
modelling pipeline, such as during pre-processing and in-processing.

</details>


### [564] [MF-GLaM: A multifidelity stochastic emulator using generalized lambda models](https://arxiv.org/abs/2507.10303)
*K. Giannoukou,X. Zhu,S. Marelli,B. Sudret*

Main category: stat.ML

TL;DR: 提出了一种多保真广义lambda模型（MF-GLaM），用于高效模拟高保真随机模拟器的条件响应分布，利用低保真模拟器数据提升性能。


<details>
  <summary>Details</summary>
Motivation: 随机模拟器的条件概率分布难以用传统确定性代理模型模拟，且高保真模拟器数据获取成本高，低保真模拟器数据可补充信息。

Method: 基于广义lambda模型（GLaM），通过四参数广义lambda分布表示每个输入的条件分布，无需访问模拟器内部随机性或重复输入值。

Result: 在合成和实际地震应用中，MF-GLaM在相同成本下比单保真GLaM更准确，或在显著降低成本下表现相当。

Conclusion: MF-GLaM是一种高效的非侵入式方法，能有效利用低保真数据模拟高保真随机模拟器的条件响应分布。

Abstract: Stochastic simulators exhibit intrinsic stochasticity due to unobservable,
uncontrollable, or unmodeled input variables, resulting in random outputs even
at fixed input conditions. Such simulators are common across various scientific
disciplines; however, emulating their entire conditional probability
distribution is challenging, as it is a task traditional deterministic
surrogate modeling techniques are not designed for. Additionally, accurately
characterizing the response distribution can require prohibitively large
datasets, especially for computationally expensive high-fidelity (HF)
simulators. When lower-fidelity (LF) stochastic simulators are available, they
can enhance limited HF information within a multifidelity surrogate modeling
(MFSM) framework. While MFSM techniques are well-established for deterministic
settings, constructing multifidelity emulators to predict the full conditional
response distribution of stochastic simulators remains a challenge. In this
paper, we propose multifidelity generalized lambda models (MF-GLaMs) to
efficiently emulate the conditional response distribution of HF stochastic
simulators by exploiting data from LF stochastic simulators. Our approach
builds upon the generalized lambda model (GLaM), which represents the
conditional distribution at each input by a flexible, four-parameter
generalized lambda distribution. MF-GLaMs are non-intrusive, requiring no
access to the internal stochasticity of the simulators nor multiple
replications of the same input values. We demonstrate the efficacy of MF-GLaM
through synthetic examples of increasing complexity and a realistic earthquake
application. Results show that MF-GLaMs can achieve improved accuracy at the
same cost as single-fidelity GLaMs, or comparable performance at significantly
reduced cost.

</details>


### [565] [Information Must Flow: Recursive Bootstrapping for Information Bottleneck in Optimal Transport](https://arxiv.org/abs/2507.10443)
*Xin Li*

Main category: stat.ML

TL;DR: 论文提出了一种名为CCUP的统一框架，通过信息在高低熵之间的流动建模认知，解决了信息瓶颈问题，并通过递归熵最小化实现稳定的感知和运动计划。


<details>
  <summary>Details</summary>
Motivation: 研究动机是建立一个统一的认知模型，通过信息流动解决信息瓶颈问题，并探索其在个体认知和集体智能中的应用。

Method: 采用Rao-Blackwellized变分熵最小化方法，结合上下文的双向交互（自下而上的消歧和自上而下的内容重建），实现信息流的优化。

Result: 证明了Delta收敛定理，展示了递归熵最小化如何在潜在空间中形成稳定的吸引子，并通过时空引导实现知识的转化和组合推理。

Conclusion: CCUP框架为信息流提供了基础原则，支持个体认知和集体智能的适应性、对齐性和扩展性。

Abstract: We present the Context-Content Uncertainty Principle (CCUP), a unified
framework that models cognition as the directed flow of information between
high-entropy context and low-entropy content. Inference emerges as a cycle of
bidirectional interactions, bottom-up contextual disambiguation paired with
top-down content reconstruction, which resolves the Information Bottleneck in
Optimal Transport (iBOT). Implemented via Rao-Blackwellized variational entropy
minimization, CCUP steers representations toward minimal joint uncertainty
while preserving inferential directionality. Local cycle completion underpins
temporal bootstrapping, chaining simulations to refine memory, and spatial
bootstrapping, enabling compositional hierarchical inference. We prove a Delta
Convergence Theorem showing that recursive entropy minimization yields
delta-like attractors in latent space, stabilizing perceptual schemas and motor
plans. Temporal bootstrapping through perception-action loops and sleep-wake
consolidation further transforms episodic traces into semantic knowledge.
Extending CCUP, each hierarchical level performs delta-seeded inference:
low-entropy content seeds diffuse outward along goal-constrained paths shaped
by top-down priors and external context, confining inference to task-relevant
manifolds and circumventing the curse of dimensionality. Building on this, we
propose that language emerges as a symbolic transport system, externalizing
latent content to synchronize inference cycles across individuals. Together,
these results establish iBOT as a foundational principle of information flow in
both individual cognition and collective intelligence, positioning recursive
inference as the structured conduit through which minds adapt, align, and
extend.

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [566] [A Multi-Level Strategy for Deepfake Content Moderation under EU Regulation](https://arxiv.org/abs/2507.08879)
*Max-Paul Förster,Luca Deck,Raimund Weidlich,Niklas Kühl*

Main category: cs.CY

TL;DR: 论文探讨了欧盟对深度伪造技术的监管措施，提出了一种多层级策略以弥补现有方法的不足。


<details>
  <summary>Details</summary>
Motivation: 深度伪造技术对民主社会构成威胁，尤其是政治传播领域，欧盟已出台透明化要求，但缺乏行业和执行标准。

Method: 通过多声部文献综述，总结了标记、检测和标注深度伪造的方法，并评估其在欧盟法规下的有效性。

Result: 研究发现单一方法无法满足监管和实际需求，因此提出了一种结合现有方法优势的多层级策略。

Conclusion: 多层级策略通过简单评分机制实现可扩展性和实用性，同时适应不同深度伪造技术和情境风险。

Abstract: The growing availability and use of deepfake technologies increases risks for
democratic societies, e.g., for political communication on online platforms.
The EU has responded with transparency obligations for providers and deployers
of Artificial Intelligence (AI) systems and online platforms. This includes
marking deepfakes during generation and labeling deepfakes when they are
shared. However, the lack of industry and enforcement standards poses an
ongoing challenge. Through a multivocal literature review, we summarize methods
for marking, detecting, and labeling deepfakes and assess their effectiveness
under EU regulation. Our results indicate that individual methods fail to meet
regulatory and practical requirements. Therefore, we propose a multi-level
strategy combining the strengths of existing methods. To account for the masses
of content on online platforms, our multi-level strategy provides scalability
and practicality via a simple scoring mechanism. At the same time, it is
agnostic to types of deepfake technology and allows for context-specific risk
weighting.

</details>


### [567] [The Consistency-Acceptability Divergence of LLMs in Judicial Decision-Making: Task and Stakeholder Dimensions](https://arxiv.org/abs/2507.08881)
*Zhang MingDa,Xu Qing*

Main category: cs.CY

TL;DR: 本文探讨了大型语言模型（LLM）在司法系统中的整合，提出了“一致性-可接受性分歧”概念，并设计了双轨审议多角色LLM司法治理框架（DTDMR-LJGF）以平衡技术效率与社会合法性。


<details>
  <summary>Details</summary>
Motivation: LLM技术在司法系统中的全球应用揭示了技术一致性与社会接受度之间的差距，亟需解决。

Method: 通过分析2023-2025年LLM司法应用数据，提出DTDMR-LJGF框架，实现智能任务分类和多利益相关者互动。

Result: 研究发现LLM技术一致性具有正负效应，需从任务和利益相关者维度解决分歧。

Conclusion: DTDMR-LJGF框架为构建平衡技术效率与社会合法性的LLM司法生态系统提供了理论和实践指导。

Abstract: The integration of large language model (LLM) technology into judicial
systems is fundamentally transforming legal practice worldwide. However, this
global transformation has revealed an urgent paradox requiring immediate
attention. This study introduces the concept of ``consistency-acceptability
divergence'' for the first time, referring to the gap between technical
consistency and social acceptance. While LLMs achieve high consistency at the
technical level, this consistency demonstrates both positive and negative
effects. Through comprehensive analysis of recent data on LLM judicial
applications from 2023--2025, this study finds that addressing this challenge
requires understanding both task and stakeholder dimensions. This study
proposes the Dual-Track Deliberative Multi-Role LLM Judicial Governance
Framework (DTDMR-LJGF), which enables intelligent task classification and
meaningful interaction among diverse stakeholders. This framework offers both
theoretical insights and practical guidance for building an LLM judicial
ecosystem that balances technical efficiency with social legitimacy.

</details>


### [568] [The Engineer's Dilemma: A Review of Establishing a Legal Framework for Integrating Machine Learning in Construction by Navigating Precedents and Industry Expectations](https://arxiv.org/abs/2507.08908)
*M. Z. Naser*

Main category: cs.CY

TL;DR: 本文探讨了工程师如何在法律框架内采用机器学习技术，通过类比推理将其融入现有工程规范，同时确保专业责任和安全性。


<details>
  <summary>Details</summary>
Motivation: 工程行业尚未完全采用机器学习方法，工程师和利益相关者对相关法律和监管框架的不确定性阻碍了其广泛应用。

Method: 通过分析法律责任原则（如过失和产品责任）以及法院对预测模型的评估，结合类比推理，提出将机器学习嵌入现有工程规范的路径。

Result: 提出了一个法律框架，帮助利益相关者评估机器学习驱动的工程解决方案的责任、义务和益处。

Conclusion: 理解技术论证与法律先例的互动对机器学习在工程实践中的合法性至关重要，本文为相关法律框架的构建提供了基础。

Abstract: Despite the widespread interest in machine learning (ML), the engineering
industry has not yet fully adopted ML-based methods, which has left engineers
and stakeholders uncertain about the legal and regulatory frameworks that
govern their decisions. This gap remains unaddressed as an engineer's
decision-making process, typically governed by professional ethics and
practical guidelines, now intersects with complex algorithmic outputs. To
bridge this gap, this paper explores how engineers can navigate legal
principles and legislative justifications that support and/or contest the
deployment of ML technologies. Drawing on recent precedents and experiences
gained from other fields, this paper argues that analogical reasoning can
provide a basis for embedding ML within existing engineering codes while
maintaining professional accountability and meeting safety requirements. In
exploring these issues, the discussion focuses on established liability
doctrines, such as negligence and product liability, and highlights how courts
have evaluated the use of predictive models. We further analyze how legislative
bodies and standard-setting organizations can furnish explicit guidance
equivalent to prior endorsements of emergent technologies. This exploration
stresses the vitality of understanding the interplay between technical
justifications and legal precedents for shaping an informed stance on ML's
legitimacy in engineering practice. Finally, our analysis catalyzes a legal
framework for integrating ML through which stakeholders can critically assess
the responsibilities, liabilities, and benefits inherent in ML-driven
engineering solutions.

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [569] [CADmium: Fine-Tuning Code Language Models for Text-Driven Sequential CAD Design](https://arxiv.org/abs/2507.09792)
*Prashant Govindarajan,Davide Baldelli,Jay Pathak,Quentin Fournier,Sarath Chandar*

Main category: cs.GR

TL;DR: 提出了一种利用大型语言模型（LLMs）自动生成CAD序列的方法，并引入了一个大规模数据集和新的几何拓扑评估指标。


<details>
  <summary>Details</summary>
Motivation: CAD建模通常耗时且手动完成，现有方法未能充分利用LLMs的潜力。

Method: 基于GPT-4.1生成高质量描述，构建了170k+的CAD数据集，并微调代码-LLMs以从自然语言生成JSON格式的CAD序列。

Result: 实验表明该方法能有效自动化CAD设计，显著加速新对象的生成。

Conclusion: 通过引入新数据集和评估指标，证明了LLMs在CAD生成中的可行性和有效性。

Abstract: Computer-aided design (CAD) is the digital construction of 2D and 3D objects,
and is central to a wide range of engineering and manufacturing applications
like automobile and aviation. Despite its importance, CAD modeling remains
largely a time-intensive, manual task. Recent works have attempted to automate
this process with small transformer-based models and handcrafted CAD sequence
representations. However, there has been little effort to leverage the
potential of large language models (LLMs) for sequential CAD design. In this
work, we introduce a new large-scale dataset of more than 170k CAD models
annotated with high-quality, human-like descriptions generated with our
pipeline based on GPT-4.1. Using this dataset, we fine-tune powerful code-LLMs
to generate CAD sequences represented in a JSON-based format from natural
language descriptions, demonstrating the viability and effectiveness of this
approach for text-conditioned CAD generation. Because simple metrics often fail
to reflect the quality of generated objects, we introduce geometric and
topological metrics based on sphericity, mean curvature, and Euler
characteristic to provide richer structural insights. Our experiments and
ablation studies on both synthetic and human-annotated data demonstrate that
CADmium is able to automate CAD design, drastically speeding up the design of
new objects. The dataset, code, and fine-tuned models are available online.

</details>


### [570] [RectifiedHR: High-Resolution Diffusion via Energy Profiling and Adaptive Guidance Scheduling](https://arxiv.org/abs/2507.09441)
*Ankit Sanjyal*

Main category: cs.GR

TL;DR: 提出自适应分类器自由引导（CFG）调度策略，解决扩散模型高分辨率图像合成中的能量不稳定和引导伪影问题。


<details>
  <summary>Details</summary>
Motivation: 扩散模型在高分辨率图像合成中常因能量不稳定和引导伪影导致视觉质量下降，需改进。

Method: 分析采样过程中的潜在能量景观，提出能量感知调度策略，动态调整引导强度。

Result: 采用线性递减CFG调度的DPM++ 2M模型表现最佳，稳定性得分（0.9998）和一致性指标（0.9873）优于固定引导方法。

Conclusion: 能量分析框架为理解和改进扩散模型行为提供了有力工具，自适应CFG调度显著提升图像质量和减少伪影。

Abstract: High-resolution image synthesis with diffusion models often suffers from
energy instabilities and guidance artifacts that degrade visual quality. We
analyze the latent energy landscape during sampling and propose adaptive
classifier-free guidance (CFG) schedules that maintain stable energy
trajectories. Our approach introduces energy-aware scheduling strategies that
modulate guidance strength over time, achieving superior stability scores
(0.9998) and consistency metrics (0.9873) compared to fixed-guidance
approaches. We demonstrate that DPM++ 2M with linear-decreasing CFG scheduling
yields optimal performance, providing sharper, more faithful images while
reducing artifacts. Our energy profiling framework serves as a powerful
diagnostic tool for understanding and improving diffusion model behavior.

</details>


### [571] [ScaffoldAvatar: High-Fidelity Gaussian Avatars with Patch Expressions](https://arxiv.org/abs/2507.10542)
*Shivangi Aneja,Sebastian Weiss,Irene Baeza,Prashanth Chandran,Gaspard Zoss,Matthias Nießner,Derek Bradley*

Main category: cs.GR

TL;DR: 提出了一种基于局部面部表情和3D高斯抛光的超高清3D头部虚拟形象生成方法，实现了实时高保真动画。


<details>
  <summary>Details</summary>
Motivation: 解决在近距离渲染数字虚拟形象时捕捉面部微特征和表情的挑战，尤其是皮肤褶皱和细微面部动作。

Method: 结合局部定义的面部表情与3D高斯抛光，利用基于块的几何3D面部模型提取局部表情特征，并通过Scaffold-GS的锚点动态合成3D高斯。

Result: ScaffoldAvatar在实时渲染中实现了视觉自然的动作，支持多样化的面部表情和风格。

Conclusion: 该方法在生成高保真、动态3D头部虚拟形象方面达到了最先进的性能。

Abstract: Generating high-fidelity real-time animated sequences of photorealistic 3D
head avatars is important for many graphics applications, including immersive
telepresence and movies. This is a challenging problem particularly when
rendering digital avatar close-ups for showing character's facial microfeatures
and expressions. To capture the expressive, detailed nature of human heads,
including skin furrowing and finer-scale facial movements, we propose to couple
locally-defined facial expressions with 3D Gaussian splatting to enable
creating ultra-high fidelity, expressive and photorealistic 3D head avatars. In
contrast to previous works that operate on a global expression space, we
condition our avatar's dynamics on patch-based local expression features and
synthesize 3D Gaussians at a patch level. In particular, we leverage a
patch-based geometric 3D face model to extract patch expressions and learn how
to translate these into local dynamic skin appearance and motion by coupling
the patches with anchor points of Scaffold-GS, a recent hierarchical scene
representation. These anchors are then used to synthesize 3D Gaussians
on-the-fly, conditioned by patch-expressions and viewing direction. We employ
color-based densification and progressive training to obtain high-quality
results and faster convergence for high resolution 3K training images. By
leveraging patch-level expressions, ScaffoldAvatar consistently achieves
state-of-the-art performance with visually natural motion, while encompassing
diverse facial expressions and styles in real time.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [572] [Chat with AI: The Surprising Turn of Real-time Video Communication from Human to AI](https://arxiv.org/abs/2507.10510)
*Jiangkai Wu,Zhiyuan Ren,Liming Liu,Xinggong Zhang*

Main category: cs.NI

TL;DR: AI Video Chat introduces a new RTC paradigm with MLLMs as peers, focusing on reducing latency and improving video streaming for intuitive human-AI interaction.


<details>
  <summary>Details</summary>
Motivation: To make human-AI interaction as seamless as face-to-face chat by addressing the latency challenges posed by MLLM inference and network instability.

Method: Proposes Artic, an AI-oriented RTC framework, with Context-Aware Video Streaming and Loss-Resilient Adaptive Frame Rate to optimize bitrate and reduce latency.

Result: Introduces DeViBench, the first benchmark to evaluate video streaming quality's impact on MLLM accuracy.

Conclusion: AI Video Chat shows promise but requires further solutions for open challenges in latency and streaming quality.

Abstract: AI Video Chat emerges as a new paradigm for Real-time Communication (RTC),
where one peer is not a human, but a Multimodal Large Language Model (MLLM).
This makes interaction between humans and AI more intuitive, as if chatting
face-to-face with a real person. However, this poses significant challenges to
latency, because the MLLM inference takes up most of the response time, leaving
very little time for video streaming. Due to network uncertainty and
instability, transmission latency becomes a critical bottleneck preventing AI
from being like a real person. To address this, we propose Artic, an
AI-oriented Real-time Communication framework, exploring the network
requirement shift from "humans watching video" to "AI understanding video". To
reduce bitrate dramatically while maintaining MLLM accuracy, we propose
Context-Aware Video Streaming that recognizes the importance of each video
region for chat and allocates bitrate almost exclusively to chat-important
regions. To avoid packet retransmission, we propose Loss-Resilient Adaptive
Frame Rate that leverages previous frames to substitute for lost/delayed frames
while avoiding bitrate waste. To evaluate the impact of video streaming quality
on MLLM accuracy, we build the first benchmark, named Degraded Video
Understanding Benchmark (DeViBench). Finally, we discuss some open questions
and ongoing solutions for AI Video Chat.

</details>


<div id='q-bio.PE'></div>

# q-bio.PE [[Back]](#toc)

### [573] [Evolution of Fear and Social Rewards in Prey-Predator Relationship](https://arxiv.org/abs/2507.09992)
*Yuji Kanagawa,Kenji Doya*

Main category: q-bio.PE

TL;DR: 研究通过分布式进化模拟探讨了恐惧与社交奖励的进化关系，发现社交奖励对猎物生存更重要，恐惧仅在社交奖励后进化。捕食者能力影响恐惧进化稳定性。


<details>
  <summary>Details</summary>
Motivation: 探究环境条件、恐惧进化与其他奖励（如食物和社交奖励）之间的关系。

Method: 开发分布式进化模拟，猎物与捕食者通过强化学习共同进化奖励功能。

Result: 社交奖励对猎物生存更关键；恐惧进化依赖于捕食者能力；静态威胁下正奖励与恐惧对立。

Conclusion: 恐惧与社交奖励在进化中复杂互动，受捕食者和威胁特性影响。

Abstract: Fear is a critical brain function for detecting danger and learning to avoid
specific stimuli that can lead to danger. While fear is believed to have
evolved under pressure from predators, experimentally reproducing the evolution
is challenging. To investigate the relationship between environmental
conditions, the evolution of fear, and the evolution of other rewards, such as
food reward and social reward, we developed a distributed evolutionary
simulation. In our simulation, prey and predator agents co-evolve their innate
reward functions, including a possibly fear-like term for observing predators,
and learn behaviors via reinforcement learning. Surprisingly, our simulation
revealed that social reward for observing the same species is more important
for prey to survive, and fear-like negative reward for observing predators
evolves only after acquiring social reward. We also found that the predator
with increased hunting ability (larger mouth) amplified fear emergence, but
also that fear evolution is more stable with non-evolving predators that are
bad at chasing prey. Additionally, unlike for predators, we found that positive
rewards evolve in opposition to fear for stationary threats, as areas with
abundant leftover food develop around them. These findings suggest that fear
and social reward have had a complex interplay with each other through
evolution, along with the nature of predators and threats.

</details>


<div id='cs.GT'></div>

# cs.GT [[Back]](#toc)

### [574] [Learning from Synthetic Labs: Language Models as Auction Participants](https://arxiv.org/abs/2507.09083)
*Anand Shah,Kehang Zhu,Yanchen Jiang,Jeffrey G. Wang,Arif K. Dayi,John J. Horton,David C. Parkes*

Main category: cs.GT

TL;DR: 论文研究了模拟AI代理（大语言模型，LLMs）在拍卖中的行为，提出了一种新的合成数据生成方法，用于拍卖研究和设计。研究发现，具备链式思维推理能力的LLMs与拍卖实验文献结果一致，表现出风险厌恶行为，更接近理论预测，并受到赢家诅咒的影响。提示设计对LLMs影响有限，但通过合适的思维模型（如纳什偏差语言）可显著改进结果。研究以低成本运行了大量拍卖实验，并开发了灵活的实验框架。


<details>
  <summary>Details</summary>
Motivation: 探索LLMs在拍卖中的行为表现，验证其是否可作为人类行为的替代研究工具，并降低拍卖实验的成本。

Method: 引入合成数据生成方法，运行1000多场拍卖实验，使用GPT-4模型，并设计灵活的框架支持多种拍卖实验。

Result: LLMs表现出与风险厌恶人类投标者一致的行为，在策略证明拍卖中更接近理论预测，并受赢家诅咒影响。提示设计影响有限，但通过特定思维模型可改进结果。

Conclusion: LLMs可作为拍卖研究的低成本替代工具，其行为与人类一致，且实验框架为未来研究提供了灵活性。

Abstract: This paper investigates the behavior of simulated AI agents (large language
models, or LLMs) in auctions, introducing a novel synthetic data-generating
process to help facilitate the study and design of auctions. We find that LLMs
-- when endowed with chain of thought reasoning capacity -- agree with the
experimental literature in auctions across a variety of classic auction
formats. In particular, we find that LLM bidders produce results consistent
with risk-averse human bidders; that they perform closer to theoretical
predictions in obviously strategy-proof auctions; and, that they succumb to the
winner's curse in common value settings. On prompting, we find that LLMs are
not very sensitive to naive changes in prompts (e.g., language, currency) but
can improve dramatically towards theoretical predictions with the right mental
model (i.e., the language of Nash deviations). We run 1,000$+$ auctions for
less than $\$$400 with GPT-4 models (three orders of magnitude cheaper than
modern auction experiments) and develop a framework flexible enough to run
auction experiments with any LLM model and a wide range of auction design
specifications, facilitating further experimental study by decreasing costs and
serving as a proof-of-concept for the use of LLM proxies.

</details>


### [575] [Incentive-Aware Dynamic Resource Allocation under Long-Term Cost Constraints](https://arxiv.org/abs/2507.09473)
*Yan Dai,Negin Golrezaei,Patrick Jaillet*

Main category: cs.GT

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Motivated by applications such as cloud platforms allocating GPUs to users or
governments deploying mobile health units across competing regions, we study
the dynamic allocation of a reusable resource to strategic agents with private
valuations. Our objective is to simultaneously (i) maximize social welfare,
(ii) satisfy multi-dimensional long-term cost constraints, and (iii)
incentivize truthful reporting. We begin by numerically evaluating primal-dual
methods widely used in constrained online optimization and find them to be
highly fragile in strategic settings -- agents can easily manipulate their
reports to distort future dual updates for future gain.
  To address this vulnerability, we develop an incentive-aware framework that
makes primal-dual methods robust to strategic behavior. Our design combines
epoch-based lazy updates -- where dual variables remain fixed within each epoch
-- with randomized exploration rounds that extract approximately truthful
signals for learning. Leveraging carefully designed online learning subroutines
that can be of independent interest for dual updates, our mechanism achieves
$\tilde{\mathcal{O}}(\sqrt{T})$ social welfare regret, satisfies all cost
constraints, and ensures incentive alignment. This matches the performance of
non-strategic allocation approaches while being robust to strategic agents.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [576] [Sequence-Model-Guided Measurement Selection for Quantum State Learning](https://arxiv.org/abs/2507.09891)
*Jiaxin Huang,Yan Zhu,Giulio Chiribella,Ya-Dong Wu*

Main category: quant-ph

TL;DR: 论文提出了一种基于深度神经网络的序列模型，用于数据驱动的自适应量子测量选择，优于随机选择，并在拓扑量子系统中发现边界测量的重要性。


<details>
  <summary>Details</summary>
Motivation: 解决量子系统规模增大时测量选择优化的难题。

Method: 使用深度神经网络序列模型，数据驱动自适应搜索高效测量选择。

Result: 模型在多种任务中表现优于随机选择，拓扑系统中倾向于边界测量。

Conclusion: 神经网络能自主发现量子物理中的边界与体态关联，无需先验知识。

Abstract: Characterization of quantum systems from experimental data is a central
problem in quantum science and technology. But which measurements should be
used to gather data in the first place? While optimal measurement choices can
be worked out for small quantum systems, the optimization becomes intractable
as the system size grows large. To address this problem, we introduce a deep
neural network with a sequence model architecture that searches for efficient
measurement choices in a data-driven, adaptive manner. The model can be applied
to a variety of tasks, including the prediction of linear and nonlinear
properties of quantum states, as well as state clustering and state tomography
tasks. In all these tasks, we find that the measurement choices identified by
our neural network consistently outperform the uniformly random choice.
Intriguingly, for topological quantum systems, our model tends to recommend
measurements at the system's boundaries, even when the task is to predict bulk
properties. This behavior suggests that the neural network may have
independently discovered a connection between boundaries and bulk, without
having been provided any built-in knowledge of quantum physics.

</details>


<div id='math.HO'></div>

# math.HO [[Back]](#toc)

### [577] [The Second Machine Turn: From Checking Proofs to Creating Concepts](https://arxiv.org/abs/2507.10179)
*Asvin G*

Main category: math.HO

TL;DR: AI正在从自动化证明检查转向自动化数学概念的创造，探讨了现状、障碍、解决方案及未来影响。


<details>
  <summary>Details</summary>
Motivation: 探索AI在数学概念创造中的潜力及其对数学和人类协作的影响。

Method: 分析当前技术状态、障碍，并提出初步解决方案，尝试形式化概念创造过程。

Result: 展示了AI在数学概念创造中的初步能力，并讨论了其对数学领域的潜在变革。

Conclusion: AI可能重塑数学研究方式，人类与机器的协作将迎来新的未来。

Abstract: We identify a second machine turn in the process of mathematical discovery:
after automating proof-checking, AI is now poised to automate the *creation* of
mathematical concepts themselves. We discuss the current state of the art,
obstacles and potential solutions as well as a preliminary attempt at
mathematizing the creation of concepts itself. The paper ends with an
assessment of how these capabilities could reshape mathematics and
human-machine collaboration, and a few different futures we might find
ourselves in.

</details>


<div id='cs.SI'></div>

# cs.SI [[Back]](#toc)

### [578] [Negotiating Comfort: Simulating Personality-Driven LLM Agents in Shared Residential Social Networks](https://arxiv.org/abs/2507.09657)
*Ann Nedime Nese Rende,Tolga Yilmaz,Özgür Ulusoy*

Main category: cs.SI

TL;DR: 利用大型语言模型（LLM）驱动的生成代理模拟共享住宅中的社交网络，以优化中央供暖系统的温度决策。


<details>
  <summary>Details</summary>
Motivation: 模拟复杂的人类行为，解决实际生活中难以实现的精细化人类行为模拟问题。

Method: 将代理分为家庭成员和代表，考虑个人偏好、性格、社交关系和天气条件，通过家庭共识和建筑范围决策进行每日模拟。测试了三种性格分布（积极、混合、消极）。

Result: 积极性格与更高的幸福感和更强的友谊相关；温度偏好、自信和无私对幸福感和决策有显著影响。

Conclusion: LLM驱动的代理能有效模拟复杂的人类行为，为实际应用提供支持。

Abstract: We use generative agents powered by large language models (LLMs) to simulate
a social network in a shared residential building, driving the temperature
decisions for a central heating system. Agents, divided into Family Members and
Representatives, consider personal preferences, personal traits, connections,
and weather conditions. Daily simulations involve family-level consensus
followed by building-wide decisions among representatives. We tested three
personality traits distributions (positive, mixed, and negative) and found that
positive traits correlate with higher happiness and stronger friendships.
Temperature preferences, assertiveness, and selflessness have a significant
impact on happiness and decisions. This work demonstrates how LLM-driven agents
can help simulate nuanced human behavior where complex real-life human
simulations are difficult to set.

</details>


### [579] [Analysing Health Misinformation with Advanced Centrality Metrics in Online Social Networks](https://arxiv.org/abs/2507.09055)
*Mkululi Sikosana,Sean Maudsley-Barton,Oluwaseun Ajao*

Main category: cs.SI

TL;DR: 论文提出并比较了三种新的中心性指标（DIC、MVC、PC），用于识别健康虚假信息传播中的关键节点，相比传统方法效果提升显著。


<details>
  <summary>Details</summary>
Motivation: 全球危机（如COVID-19）中，健康虚假信息在社交网络上的快速传播对公共卫生和社会稳定构成挑战，传统中心性指标在动态网络中的局限性凸显。

Method: 引入动态影响中心性（DIC）、健康虚假信息脆弱性中心性（MVC）和传播中心性（PC），结合时间动态和多层网络交互，使用FibVID和Monant数据集验证。

Result: 新指标比传统方法多识别出44.83%的关键节点，干预效果提升25%（从50%到62.5%），并在其他数据集上验证了泛化能力。

Conclusion: 结合传统和新指标能更全面理解和抑制健康虚假信息在不同网络中的传播。

Abstract: The rapid spread of health misinformation on online social networks (OSNs)
during global crises such as the COVID-19 pandemic poses challenges to public
health, social stability, and institutional trust. Centrality metrics have long
been pivotal in understanding the dynamics of information flow, particularly in
the context of health misinformation. However, the increasing complexity and
dynamism of online networks, especially during crises, highlight the
limitations of these traditional approaches. This study introduces and compares
three novel centrality metrics: dynamic influence centrality (DIC), health
misinformation vulnerability centrality (MVC), and propagation centrality (PC).
These metrics incorporate temporal dynamics, susceptibility, and multilayered
network interactions. Using the FibVID dataset, we compared traditional and
novel metrics to identify influential nodes, propagation pathways, and
misinformation influencers. Traditional metrics identified 29 influential
nodes, while the new metrics uncovered 24 unique nodes, resulting in 42
combined nodes, an increase of 44.83%. Baseline interventions reduced health
misinformation by 50%, while incorporating the new metrics increased this to
62.5%, an improvement of 25%. To evaluate the broader applicability of the
proposed metrics, we validated our framework on a second dataset, Monant
Medical Misinformation, which covers a diverse range of health misinformation
discussions beyond COVID-19. The results confirmed that the advanced metrics
generalised successfully, identifying distinct influential actors not captured
by traditional methods. In general, the findings suggest that a combination of
traditional and novel centrality measures offers a more robust and
generalisable framework for understanding and mitigating the spread of health
misinformation in different online network contexts.

</details>


### [580] [Advanced Health Misinformation Detection Through Hybrid CNN-LSTM Models Informed by the Elaboration Likelihood Model (ELM)](https://arxiv.org/abs/2507.09149)
*Mkululi Sikosana,Sean Maudsley-Barton,Oluwaseun Ajao*

Main category: cs.SI

TL;DR: 该研究结合ELM理论和混合CNN-LSTM模型，显著提升了社交媒体上健康虚假信息的检测性能。


<details>
  <summary>Details</summary>
Motivation: COVID-19疫情期间的健康虚假信息对公共卫生造成挑战，需高效检测方法。

Method: 采用ELM理论提取文本特征（如可读性、情感极性），结合CNN-LSTM模型进行分类。

Result: 模型准确率达97.37%，结合特征工程后性能进一步提升（F1-score达99.41%）。

Conclusion: ELM特征能有效提升虚假信息检测性能，为机器学习算法提供心理学理论支持。

Abstract: Health misinformation during the COVID-19 pandemic has significantly
challenged public health efforts globally. This study applies the Elaboration
Likelihood Model (ELM) to enhance misinformation detection on social media
using a hybrid Convolutional Neural Network (CNN) and Long Short-Term Memory
(LSTM) model. The model aims to enhance the detection accuracy and reliability
of misinformation classification by integrating ELM-based features such as text
readability, sentiment polarity, and heuristic cues (e.g., punctuation
frequency). The enhanced model achieved an accuracy of 97.37%, precision of
96.88%, recall of 98.50%, F1-score of 97.41%, and ROC-AUC of 99.50%. A combined
model incorporating feature engineering further improved performance, achieving
a precision of 98.88%, recall of 99.80%, F1-score of 99.41%, and ROC-AUC of
99.80%. These findings highlight the value of ELM features in improving
detection performance, offering valuable contextual information. This study
demonstrates the practical application of psychological theories in developing
advanced machine learning algorithms to address health misinformation
effectively.

</details>


<div id='q-bio.QM'></div>

# q-bio.QM [[Back]](#toc)

### [581] [Generation of structure-guided pMHC-I libraries using Diffusion Models](https://arxiv.org/abs/2507.08902)
*Sergio Mares,Ariel Espinoza Weinberger,Nilah M. Ioannidis*

Main category: q-bio.QM

TL;DR: 论文提出了一种基于结构指导的pMHC-I肽基准，利用扩散模型设计，避免了实验数据集的偏差，揭示了现有序列预测模型的局限性。


<details>
  <summary>Details</summary>
Motivation: 当前pMHC-I相互作用识别方法存在实验数据集偏差，限制了新肽配体的发现。

Method: 采用扩散模型设计pMHC-I肽基准，基于晶体结构相互作用距离，覆盖20种HLA等位基因。

Result: 新基准独立于已知肽，重现了锚定残基偏好，且现有序列预测模型对其表现不佳。

Conclusion: 结构指导的设计方法提供了无偏模型训练和评估资源，揭示了现有模型的局限性。

Abstract: Personalized vaccines and T-cell immunotherapies depend critically on
identifying peptide-MHC class I (pMHC-I) interactions capable of eliciting
potent immune responses. However, current benchmarks and models inherit biases
present in mass-spectrometry and binding-assay datasets, limiting discovery of
novel peptide ligands. To address this issue, we introduce a structure-guided
benchmark of pMHC-I peptides designed using diffusion models conditioned on
crystal structure interaction distances. Spanning twenty high-priority HLA
alleles, this benchmark is independent of previously characterized peptides yet
reproduces canonical anchor residue preferences, indicating structural
generalization without experimental dataset bias. Using this resource, we
demonstrate that state-of-the-art sequence-based predictors perform poorly at
recognizing the binding potential of these structurally stable designs,
indicating allele-specific limitations invisible in conventional evaluations.
Our geometry-aware design pipeline yields peptides with high predicted
structural integrity and higher residue diversity than existing datasets,
representing a key resource for unbiased model training and evaluation. Our
code, and data are available at: https://github.com/sermare/struct-mhc-dev.

</details>


### [582] [From Classical Machine Learning to Emerging Foundation Models: Review on Multimodal Data Integration for Cancer Research](https://arxiv.org/abs/2507.09028)
*Amgad Muneer,Muhammad Waqas,Maliazurina B Saad,Eman Showkatian,Rukhmini Bandyopadhyay,Hui Xu,Wentao Li,Joe Y Chang,Zhongxing Liao,Cara Haymaker,Luisa Solis Soto,Carol C Wu,Natalie I Vokes,Xiuning Le,Lauren A Byers,Don L Gibbons,John V Heymach,Jianjun Zhang,Jia Wu*

Main category: q-bio.QM

TL;DR: 综述探讨了多模态数据整合在癌症研究中的应用，重点介绍了从传统机器学习到基础模型的转变及其在肿瘤学中的潜力。


<details>
  <summary>Details</summary>
Motivation: 整合多模态数据以提取可操作信息是癌症研究的关键挑战，基础模型为此提供了新途径。

Method: 回顾了多模态数据整合的策略，包括机器学习、深度学习和基础模型的方法论框架、验证协议和开源资源。

Result: 总结了当前最先进的整合方法、公开的多模态数据库和工具，为基础模型的进一步发展奠定了基础。

Conclusion: 基础模型为肿瘤学中的多模态数据整合提供了革命性潜力，是未来大规模AI模型的基础。

Abstract: Cancer research is increasingly driven by the integration of diverse data
modalities, spanning from genomics and proteomics to imaging and clinical
factors. However, extracting actionable insights from these vast and
heterogeneous datasets remains a key challenge. The rise of foundation models
(FMs) -- large deep-learning models pretrained on extensive amounts of data
serving as a backbone for a wide range of downstream tasks -- offers new
avenues for discovering biomarkers, improving diagnosis, and personalizing
treatment. This paper presents a comprehensive review of widely adopted
integration strategies of multimodal data to assist advance the computational
approaches for data-driven discoveries in oncology. We examine emerging trends
in machine learning (ML) and deep learning (DL), including methodological
frameworks, validation protocols, and open-source resources targeting cancer
subtype classification, biomarker discovery, treatment guidance, and outcome
prediction. This study also comprehensively covers the shift from traditional
ML to FMs for multimodal integration. We present a holistic view of recent FMs
advancements and challenges faced during the integration of multi-omics with
advanced imaging data. We identify the state-of-the-art FMs, publicly available
multi-modal repositories, and advanced tools and methods for data integration.
We argue that current state-of-the-art integrative methods provide the
essential groundwork for developing the next generation of large-scale,
pre-trained models poised to further revolutionize oncology. To the best of our
knowledge, this is the first review to systematically map the transition from
conventional ML to advanced FM for multimodal data integration in oncology,
while also framing these developments as foundational for the forthcoming era
of large-scale AI models in cancer research.

</details>


### [583] [A PBN-RL-XAI Framework for Discovering a "Hit-and-Run'' Therapeutic Strategy in Melanoma](https://arxiv.org/abs/2507.10136)
*Zhonglin Liu*

Main category: q-bio.QM

TL;DR: 通过动态概率布尔网络和强化学习，研究发现短暂抑制LOXL2蛋白可有效克服黑色素瘤对PD-1免疫疗法的耐药性。


<details>
  <summary>Details</summary>
Motivation: 解决转移性黑色素瘤对PD-1免疫疗法的先天耐药性及其分子机制不明确的问题。

Method: 构建动态概率布尔网络模型，结合转录组数据和强化学习，发现多步治疗干预策略，并通过可解释AI分析机制。

Result: 发现精确时机的4步短暂抑制LOXL2蛋白是最佳策略，可消除耐药性分子特征。

Conclusion: 提出了一种时间依赖性治疗假设，为复杂生物系统中的干预策略提供了计算框架。

Abstract: Innate resistance to anti-PD-1 immunotherapy remains a major clinical
challenge in metastatic melanoma, with the underlying molecular networks being
poorly understood. To address this, we constructed a dynamic Probabilistic
Boolean Network model using transcriptomic data from patient tumor biopsies to
elucidate the regulatory logic governing therapy response. We then employed a
reinforcement learning agent to systematically discover optimal, multi-step
therapeutic interventions and used explainable artificial intelligence to
mechanistically interpret the agent's control policy. The analysis revealed
that a precisely timed, 4-step temporary inhibition of the lysyl oxidase like 2
protein (LOXL2) was the most effective strategy. Our explainable analysis
showed that this ``hit-and-run" intervention is sufficient to erase the
molecular signature driving resistance, allowing the network to self-correct
without requiring sustained intervention. This study presents a novel,
time-dependent therapeutic hypothesis for overcoming immunotherapy resistance
and provides a powerful computational framework for identifying non-obvious
intervention protocols in complex biological systems.

</details>


<div id='q-fin.ST'></div>

# q-fin.ST [[Back]](#toc)

### [584] [A Framework for Predictive Directional Trading Based on Volatility and Causal Inference](https://arxiv.org/abs/2507.09347)
*Ivan Letteri*

Main category: q-fin.ST

TL;DR: 该研究提出了一种结合统计方法与机器学习的新框架，用于识别和利用金融市场中的预测性领先-滞后关系，并通过回测验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 旨在通过整合先进统计方法和机器学习模型，增强对股票间预测性关系的识别和利用，从而开发更有效的交易策略。

Method: 使用高斯混合模型（GMM）对股票进行聚类，结合格兰杰因果检验（GCT）、PCMCI测试和有效传递熵（ETE）构建因果推断管道，并通过动态时间规整（DTW）和KNN分类器确定最佳交易时机。

Result: 策略在测试期间实现了15.38%的总回报，显著优于买入持有策略的10.39%，且夏普比率高达2.17，胜率在某些配对中达到100%。

Conclusion: 该研究为基于波动性的因果关系识别提供了系统化方法，对金融建模和算法交易实践具有重要价值。

Abstract: Purpose: This study introduces a novel framework for identifying and
exploiting predictive lead-lag relationships in financial markets. We propose
an integrated approach that combines advanced statistical methodologies with
machine learning models to enhance the identification and exploitation of
predictive relationships between equities. Methods: We employed a Gaussian
Mixture Model (GMM) to cluster nine prominent stocks based on their mid-range
historical volatility profiles over a three-year period. From the resulting
clusters, we constructed a multi-stage causal inference pipeline, incorporating
the Granger Causality Test (GCT), a customised Peter-Clark Momentary
Conditional Independence (PCMCI) test, and Effective Transfer Entropy (ETE) to
identify robust, predictive linkages. Subsequently, Dynamic Time Warping (DTW)
and a K-Nearest Neighbours (KNN) classifier were utilised to determine the
optimal time lag for trade execution. The resulting strategy was rigorously
backtested. Results: The proposed volatility-based trading strategy, tested
from 8 June 2023 to 12 August 2023, demonstrated substantial efficacy. The
portfolio yielded a total return of 15.38%, significantly outperforming the
10.39% return of a comparative Buy-and-Hold strategy. Key performance metrics,
including a Sharpe Ratio up to 2.17 and a win rate up to 100% for certain
pairs, confirmed the strategy's viability. Conclusion: This research
contributes a systematic and robust methodology for identifying profitable
trading opportunities derived from volatility-based causal relationships. The
findings have significant implications for both academic research in financial
modelling and the practical application of algorithmic trading, offering a
structured approach to developing resilient, data-driven strategies.

</details>


<div id='physics.plasm-ph'></div>

# physics.plasm-ph [[Back]](#toc)

### [585] [Optimizing External Sources for Controlled Burning Plasma in Tokamaks with Neural Ordinary Differential Equations](https://arxiv.org/abs/2507.09431)
*Zefang Liu,Weston M. Stacey*

Main category: physics.plasm-ph

TL;DR: 提出了一种基于神经ODE的逆建模方法，用于计算托卡马克中控制燃烧等离子体所需的外部源分布。


<details>
  <summary>Details</summary>
Motivation: 实现托卡马克中燃烧等离子体的精确控制需要调节外部粒子与能量源以达到目标核心密度和温度。

Method: 使用基于神经ODE的多节点等离子体动力学模型，通过优化问题计算外部源分布（如NBI功率）。

Result: NeuralPlasmaODE框架成功将正向模拟工具转化为控制导向模型，并提供了计算外部源分布的实际方法。

Conclusion: 该方法为当前和未来聚变装置中的外部源分布计算提供了实用解决方案。

Abstract: Achieving controlled burning plasma in tokamaks requires precise regulation
of external particle and energy sources to reach and maintain target core
densities and temperatures. This work presents an inverse modeling approach
using a multinodal plasma dynamics model based on neural ordinary differential
equations (Neural ODEs). Given a desired time evolution of nodal quantities
such as deuteron density or electron temperature, we compute the external
source profiles, such as neutral beam injection (NBI) power, that drive the
plasma toward the specified behavior. The approach is implemented within the
NeuralPlasmaODE framework, which models multi-region, multi-timescale transport
and incorporates physical mechanisms including radiation, auxiliary heating,
and internodal energy exchange. By formulating the control task as an
optimization problem, we use automatic differentiation through the Neural ODE
solver to minimize the discrepancy between simulated and target trajectories.
This framework transforms the forward simulation tool into a control-oriented
model and provides a practical method for computing external source profiles in
both current and future fusion devices.

</details>


### [586] [Sensitivity Analysis of Transport and Radiation in NeuralPlasmaODE for ITER Burning Plasmas](https://arxiv.org/abs/2507.09432)
*Zefang Liu,Weston M. Stacey*

Main category: physics.plasm-ph

TL;DR: 论文通过扩展NeuralPlasmaODE模型，分析了ITER等离子体中传输和辐射机制的影响，揭示了磁场强度、安全因子和杂质含量对能量约束的主导作用。


<details>
  <summary>Details</summary>
Motivation: 理解关键物理参数如何影响燃烧等离子体行为对ITER的可靠运行至关重要。

Method: 扩展NeuralPlasmaODE模型，进行传输和辐射机制的敏感性分析，计算核心和边缘温度及密度对多种参数的归一化敏感性。

Result: 分析表明磁场强度、安全因子和杂质含量对能量约束有主导影响，同时温度依赖性传输有助于自我调节行为。

Conclusion: NeuralPlasmaODE模型在燃烧等离子体环境中的预测建模和场景优化中具有实用价值。

Abstract: Understanding how key physical parameters influence burning plasma behavior
is critical for the reliable operation of ITER. In this work, we extend
NeuralPlasmaODE, a multi-region, multi-timescale model based on neural ordinary
differential equations, to perform a sensitivity analysis of transport and
radiation mechanisms in ITER plasmas. Normalized sensitivities of core and edge
temperatures and densities are computed with respect to transport
diffusivities, electron cyclotron radiation (ECR) parameters, impurity
fractions, and ion orbit loss (IOL) timescales. The analysis focuses on
perturbations around a trained nominal model for the ITER inductive scenario.
Results highlight the dominant influence of magnetic field strength, safety
factor, and impurity content on energy confinement, while also revealing how
temperature-dependent transport contributes to self-regulating behavior. These
findings demonstrate the utility of NeuralPlasmaODE for predictive modeling and
scenario optimization in burning plasma environments.

</details>


<div id='stat.ME'></div>

# stat.ME [[Back]](#toc)

### [587] [Predictive Causal Inference via Spatio-Temporal Modeling and Penalized Empirical Likelihood](https://arxiv.org/abs/2507.08896)
*Byunghee Lee,Hye Yeon Sin,Joonsung Kang*

Main category: stat.ME

TL;DR: 提出了一种结合HMM和MTGCN的预测因果推断框架，用于处理生物医学数据中的时空复杂性。


<details>
  <summary>Details</summary>
Motivation: 传统单一模型方法存在局限性，尤其在临床领域（如癌症、痴呆症等）中难以直接观察治疗效果。

Method: 结合HMM进行空间健康状态估计和MTGCN捕捉时间结果轨迹，将时空信息不对称地处理为内生和外生变量。

Result: 通过模拟研究验证了模型在不同条件下的性能，增强了偏差校正和预测准确性。

Conclusion: 该框架通过适应生物医学数据中的时空复杂性，推动了预测因果推断的发展。

Abstract: This study introduces an integrated framework for predictive causal inference
designed to overcome limitations inherent in conventional single model
approaches. Specifically, we combine a Hidden Markov Model (HMM) for spatial
health state estimation with a Multi Task and Multi Graph Convolutional Network
(MTGCN) for capturing temporal outcome trajectories. The framework
asymmetrically treats temporal and spatial information regarding them as
endogenous variables in the outcome regression, and exogenous variables in the
propensity score model, thereby expanding the standard doubly robust treatment
effect estimation to jointly enhance bias correction and predictive accuracy.
To demonstrate its utility, we focus on clinical domains such as cancer,
dementia, and Parkinson disease, where treatment effects are challenging to
observe directly. Simulation studies are conducted to emulate latent disease
dynamics and evaluate the model performance under varying conditions. Overall,
the proposed framework advances predictive causal inference by structurally
adapting to spatiotemporal complexities common in biomedical data.

</details>


<div id='q-bio.GN'></div>

# q-bio.GN [[Back]](#toc)

### [588] [Context-Aware Regularization with Markovian Integration for Attention-Based Nucleotide Analysis](https://arxiv.org/abs/2507.09378)
*Mohammadsaleh Refahi,Mahdi Abavisani,Bahrad A. Sokhansanj,James R. Brown,Gail Rosen*

Main category: q-bio.GN

TL;DR: CARMANIA是一种自监督预训练框架，通过结合转移矩阵损失和上下文感知正则化，改进了核苷酸序列分析中的长程依赖捕捉能力。


<details>
  <summary>Details</summary>
Motivation: 当前的自回归Transformer在长序列分析中存在计算效率低和全局一致性不足的问题，CARMANIA旨在解决这些问题。

Method: CARMANIA通过转移矩阵损失（TM loss）将预测的标记转移与输入序列的n-gram统计对齐，增强模型对高阶依赖的捕捉能力。

Result: 在多种基因组任务中，CARMANIA表现优异，性能提升显著，尤其在增强子和管家基因分类任务中表现突出。

Conclusion: CARMANIA通过TM损失显著提升了模型性能，证明了其在核苷酸序列分析中的有效性。

Abstract: Transformers have revolutionized nucleotide sequence analysis, yet capturing
long-range dependencies remains challenging. Recent studies show that
autoregressive transformers often exhibit Markovian behavior by relying on
fixed-length context windows for next-token prediction. However, standard
self-attention mechanisms are computationally inefficient for long sequences
due to their quadratic complexity and do not explicitly enforce global
transition consistency.
  We introduce CARMANIA (Context-Aware Regularization with Markovian
Integration for Attention-Based Nucleotide Analysis), a self-supervised
pretraining framework that augments next-token (NT) prediction with a
transition-matrix (TM) loss. The TM loss aligns predicted token transitions
with empirically derived n-gram statistics from each input sequence,
encouraging the model to capture higher-order dependencies beyond local
context. This integration enables CARMANIA to learn organism-specific sequence
structures that reflect both evolutionary constraints and functional
organization.
  We evaluate CARMANIA across diverse genomic tasks, including regulatory
element prediction, functional gene classification, taxonomic inference,
antimicrobial resistance detection, and biosynthetic gene cluster
classification. CARMANIA outperforms the previous best long-context model by at
least 7 percent, matches state-of-the-art on shorter sequences (exceeding prior
results on 20 out of 40 tasks while running approximately 2.5 times faster),
and shows particularly strong improvements on enhancer and housekeeping gene
classification tasks, including up to a 34 percent absolute gain in Matthews
correlation coefficient (MCC) for enhancer prediction. The TM loss boosts
accuracy in 33 of 40 tasks, especially where local motifs or regulatory
patterns drive prediction.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [589] [Accelerating Drug Discovery Through Agentic AI: A Multi-Agent Approach to Laboratory Automation in the DMTA Cycle](https://arxiv.org/abs/2507.09023)
*Yao Fehlis,Charles Crain,Aidan Jensen,Michael Watson,James Juhasz,Paul Mandel,Betty Liu,Shawn Mahon,Daren Wilson,Nick Lynch-Jonely,Ben Leedom,David Fuller*

Main category: cs.SE

TL;DR: 本文介绍了一种名为Tippy的新型AI框架，通过多智能体系统自动化药物发现中的DMTA循环，显著提升了效率和决策速度。


<details>
  <summary>Details</summary>
Motivation: 传统药物发现方法难以满足现代治疗需求，需要AI驱动的自动化解决方案。

Method: Tippy采用多智能体系统，包括五个专门设计的智能体（Supervisor、Molecule、Lab、Analysis、Report）和Safety Guardrail监督，覆盖DMTA循环的各个阶段。

Result: Tippy显著提升了工作流效率、决策速度和跨学科协作，为AI辅助药物发现提供了新范式。

Conclusion: Tippy是首个生产就绪的AI智能体系统，展示了AI如何通过自动化DMTA循环改变实验室工作流程。

Abstract: The pharmaceutical industry faces unprecedented challenges in drug discovery,
with traditional approaches struggling to meet modern therapeutic development
demands. This paper introduces a novel AI framework, Tippy, that transforms
laboratory automation through specialized AI agents operating within the
Design-Make-Test-Analyze (DMTA) cycle. Our multi-agent system employs five
specialized agents - Supervisor, Molecule, Lab, Analysis, and Report, with
Safety Guardrail oversight - each designed to excel in specific phases of the
drug discovery pipeline. Tippy represents the first production-ready
implementation of specialized AI agents for automating the DMTA cycle,
providing a concrete example of how AI can transform laboratory workflows. By
leveraging autonomous AI agents that reason, plan, and collaborate, we
demonstrate how Tippy accelerates DMTA cycles while maintaining scientific
rigor essential for pharmaceutical research. The system shows significant
improvements in workflow efficiency, decision-making speed, and
cross-disciplinary coordination, offering a new paradigm for AI-assisted drug
discovery.

</details>


### [590] [Semantic Source Code Segmentation using Small and Large Language Models](https://arxiv.org/abs/2507.08992)
*Abdelhalim Dahou,Ansgar Scherp,Sebastian Kurten,Brigitte Mathiak,Madhu Chauhan*

Main category: cs.SE

TL;DR: 论文提出了一种自动化方法，用于研究R代码的分割，结合大语言模型和小语言模型，并比较了两种方法的效果。


<details>
  <summary>Details</summary>
Motivation: 随着代码库规模的增长，手动和语法分析方法在低资源语言（如R）中变得不切实际，需要自动化解决方案。

Method: 提出了两种方法：基于上下文的逐行分析和基于范围的片段确定，并实验了大语言模型和微调的小语言模型。

Result: 基于上下文的逐行分析优于基于范围的分割，小语言模型（如CodeBERT和CodeT5+）表现优于大语言模型。

Conclusion: 小语言模型在未预训练R代码的情况下，仅通过微调少量标注数据即可取得最佳效果。

Abstract: Source code segmentation, dividing code into functionally coherent segments,
is crucial for knowledge retrieval and maintenance in software development.
While enabling efficient navigation and comprehension of large codebases,
manual and syntactic analysis approaches have become impractical as
repositories grow, especially for low-resource languages like R and their
research domains (e.g., social sciences, psychology).This paper introduces an
automated, domain-specific approach for research R code segmentation using
Large and Small Language Models (LLMs/SLMs). It presents two novel approaches
and a human-annotated dataset, StatCodeSeg. We explore two distinct approaches:
line-by-line analysis with context and range-based segment determination. We
experiment with LLMs and fine-tuned SLMs. To support the generalizability of
our approaches, we also include experiments on Python code from the computer
science domain.Our results show that context-based line-by-line analysis is
superior over range-based segmentation.Using smaller language models like
CodeBERT and an encoder-only version of CodeT5+ are better than their LLM
counterparts. Most notably, these two best-performing models did not see R code
during pre-training versus the LLMs but were only fine-tuned on 4,130 lines of
manually annotated code.

</details>


### [591] [Evaluating LLMs on Sequential API Call Through Automated Test Generation](https://arxiv.org/abs/2507.09481)
*Yuheng Huang,Da Song,Zhenlan Ji,Shuai Wang,Lei Ma*

Main category: cs.SE

TL;DR: StateGen是一个自动化框架，用于生成涉及顺序API交互的多样化编码任务，填补了现有基准测试的不足。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试依赖手动收集的测试用例，无法自动检查语义正确性，且忽略了顺序API调用的复杂交互。

Method: 结合状态机约束求解、能量采样和控制流注入生成可执行程序，并通过两个LLM代理将其转化为自然语言任务描述。

Result: 构建了StateEval基准测试，包含120个验证过的测试用例，覆盖三个代表性场景。

Conclusion: StateGen能有效生成具有挑战性和现实性的API导向任务，揭示了当前LLM结合API的改进空间。

Abstract: By integrating tools from external APIs, Large Language Models (LLMs) have
expanded their promising capabilities in a diverse spectrum of complex
real-world tasks. However, testing, evaluation, and analysis of LLM tool use
remain in their early stages. Most existing benchmarks rely on manually
collected test cases, many of which cannot be automatically checked for
semantic correctness and instead depend on static methods such as string
matching. Additionally, these benchmarks often overlook the complex
interactions that occur between sequential API calls, which are common in
real-world applications. To fill the gap, in this paper, we introduce StateGen,
an automated framework designed to generate diverse coding tasks involving
sequential API interactions. StateGen combines state-machine-based API
constraint solving and validation, energy-based sampling, and control-flow
injection to generate executable programs. These programs are then translated
into human-like natural language task descriptions through a collaboration of
two LLM agents. Utilizing StateGen, we construct StateEval, a benchmark
encompassing 120 verified test cases spanning across three representative
scenarios: Session Service, Tensor Operation, and ElevenLabs MCP. Experimental
results confirm that StateGen can effectively generate challenging and
realistic API-oriented tasks, highlighting areas for improvement in current
LLMs incorporating APIs.

</details>


### [592] [SetupBench: Assessing Software Engineering Agents' Ability to Bootstrap Development Environments](https://arxiv.org/abs/2507.09063)
*Avi Arora,Jinu Jang,Roshanak Zilouchian Moghaddam*

Main category: cs.SE

TL;DR: SetupBench是一个新的基准测试，专注于评估LLM代理在裸Linux环境中完成环境初始化任务的能力，发现现有代理在多个任务类别中表现不佳。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试主要评估LLM代理在预装环境中的表现，而忽略了环境初始化的关键能力。

Method: 引入SetupBench，包含93个任务，涵盖多种语言生态系统、数据库引擎和多服务编排场景，通过自然语言问题描述和确定性成功命令进行评估。

Result: 评估显示OpenHands代理在多个任务中成功率低，特别是在仓库设置和本地数据库配置中，存在系统性失败模式。

Conclusion: SetupBench填补了现有基准测试的空白，为下一代软件开发者代理提供了严格的评估标准。

Abstract: Modern Large Language Model (LLM) agents promise end to end assistance with
real-world software tasks, yet existing benchmarks evaluate LLM agents almost
exclusively in pre-baked environments where every dependency is pre-installed.
To fill this gap, we introduce SetupBench, a 93 instance benchmark that
isolates the environment-bootstrap skill: starting from a bare Linux sandbox,
an agent must install packages, resolve dependency conflicts, initialize
databases, and configure background services. Our tasks span seven language
ecosystems, five database engines, and multi-service orchestration scenarios,
each accompanies by a natural language problem statement and a deterministic
success command. Through evaluation of OpenHands, a state-of-the-art coding
agent, we find low success rates across task categories, with particular
challenges in repository setup (38.9-57.4%) and local database configuration
(20.0-53.3%). Our analysis reveals systematic failure modes including
incomplete development tooling installation, hallucinated task constraints, and
non-persistent environment modifications that break agent-human collaboration
workflows. We identify substantial inefficiencies in agent exploration
strategies, with 38-89% of actions being unnecessary compared to optimal human
behavior. These findings highlight gaps in current agents' practical
environment-bootstrap capabilities. By targeting this critical yet
under-evaluated capability, SetupBench provides a rigorous yard-stick for the
next generation of software developer agents aiming to solve end to end
real-wold tasks.

</details>


### [593] [SPICE: An Automated SWE-Bench Labeling Pipeline for Issue Clarity, Test Coverage, and Effort Estimation](https://arxiv.org/abs/2507.09108)
*Aaditya Bhatia,Gustavo A. Oliva,Gopi Krishnan Rajbahadur,Haoxiang Zhang,Yihao Chen,Zhilong Chen,Arthur Leung,Dayi Lin,Boyuan Chen,Ahmed E. Hassan*

Main category: cs.SE

TL;DR: SPICE是一个自动化标注工具，用于高效生成软件工程数据集标签，显著降低成本。


<details>
  <summary>Details</summary>
Motivation: 高质量标注数据集对训练和评估基础模型至关重要，但人工标注成本高昂且耗时。

Method: SPICE结合上下文感知代码导航、理性驱动提示和多轮共识，生成接近专家标注的标签。

Result: SPICE将标注1000个实例的成本从10万美元降至5.10美元，并与人工标注数据高度一致。

Conclusion: SPICE为软件工程领域的大规模数据集创建提供了经济高效的解决方案。

Abstract: High-quality labeled datasets are crucial for training and evaluating
foundation models in software engineering, but creating them is often
prohibitively expensive and labor-intensive. We introduce SPICE, a scalable,
automated pipeline for labeling SWE-bench-style datasets with annotations for
issue clarity, test coverage, and effort estimation. SPICE combines
context-aware code navigation, rationale-driven prompting, and multi-pass
consensus to produce labels that closely approximate expert annotations.
SPICE's design was informed by our own experience and frustration in labeling
more than 800 instances from SWE-Gym. SPICE achieves strong agreement with
human-labeled SWE-bench Verified data while reducing the cost of labeling 1,000
instances from around $100,000 (manual annotation) to just $5.10. These results
demonstrate SPICE's potential to enable cost-effective, large-scale dataset
creation for SE-focused FMs. To support the community, we release both SPICE
tool and SPICE Bench, a new dataset of 6,802 SPICE-labeled instances curated
from 291 open-source projects in SWE-Gym (over 13x larger than SWE-bench
Verified).

</details>


### [594] [Enhancing Interpretability in Software Change Management with Chain-of-Thought Reasoning](https://arxiv.org/abs/2507.09315)
*Yongqian Sun,Weihua Kuang,Chao Shen,Xidao Wen,Tinghua Zheng,Heng Liu,Shenglin Zhang,Bo Wu,Dan Pei*

Main category: cs.SE

TL;DR: SCELM是一个端到端的自动化框架，用于高效管理软件变更，减少服务故障和经济损失。


<details>
  <summary>Details</summary>
Motivation: 现代在线服务中频繁的软件变更带来显著风险，需要一种高效的管理方法。

Method: 提出SCELM框架，实现软件变更的自动化评估和生命周期管理。

Result: SCELM显著减少了服务故障和经济损失。

Conclusion: SCELM是一个有效的软件变更管理解决方案。

Abstract: In modern online services, frequent software changes introduce significant
risks. To tackle this challenge, we propose SCELM (Software Change Evaluation
and Lifecycle Management), an end-to-end automated framework for software
change management. SCELM aims to manage software changes efficiently and
precisely, significantly reducing service failures and economic losses.

</details>


### [595] [A Serverless Architecture for Real-Time Stock Analysis using Large Language Models: An Iterative Development and Debugging Case Study](https://arxiv.org/abs/2507.09583)
*Taniv Ashraf*

Main category: cs.SE

TL;DR: 本文介绍了一种基于Google Gemini API的实时股票分析系统，通过无服务器架构实现低成本高效运行，并详细记录了调试过程和架构演进。


<details>
  <summary>Details</summary>
Motivation: 利用大型语言模型（LLM）如Gemini，为个人提供低成本、高效的金融数据分析工具。

Method: 系统结合Gemini API进行定性分析，通过GitHub Actions自动化数据处理，并使用静态前端展示结果。

Result: 最终架构实现了近乎零成本的运行，展示了个人构建AI金融工具的可行性。

Conclusion: 讨论了LLM在金融分析中的作用、调试方法的重要性，以及人机协作在软件开发中的新兴范式。

Abstract: The advent of powerful, accessible Large Language Models (LLMs) like Google's
Gemini presents new opportunities for democratizing financial data analysis.
This paper documents the design, implementation, and iterative debugging of a
novel, serverless system for real-time stock analysis. The system leverages the
Gemini API for qualitative assessment, automates data ingestion and processing
via GitHub Actions, and presents the findings through a decoupled, static
frontend. We detail the architectural evolution of the system, from initial
concepts to a robust, event-driven pipeline, highlighting the practical
challenges encountered during deployment. A significant portion of this paper
is dedicated to a case study on the debugging process, covering common software
errors, platform-specific permission issues, and rare, environment-level
platform bugs. The final architecture operates at a near-zero cost,
demonstrating a viable model for individuals to build sophisticated AI-powered
financial tools. The operational application is publicly accessible, and the
complete source code is available for review. We conclude by discussing the
role of LLMs in financial analysis, the importance of robust debugging
methodologies, and the emerging paradigm of human-AI collaboration in software
development.

</details>


### [596] [OrQstrator: An AI-Powered Framework for Advanced Quantum Circuit Optimization](https://arxiv.org/abs/2507.09682)
*Laura Baird,Armin Moin*

Main category: cs.SE

TL;DR: OrQstrator是一个基于深度强化学习的模块化框架，用于在NISQ时代优化量子电路，通过智能选择三种互补的优化器来减少电路深度和门数量。


<details>
  <summary>Details</summary>
Motivation: 在NISQ时代，量子电路优化面临噪声和硬件限制的挑战，需要一种智能、自适应的优化方法。

Method: 结合深度强化学习（DRL）的电路重写器、领域特定优化器和参数化电路实例化器，通过中央协调引擎动态选择优化策略。

Result: 系统输出优化的电路，适应硬件约束，减少门数量和深度，提高保真度。

Conclusion: OrQstrator为NISQ时代的量子电路优化提供了一种高效、智能的解决方案。

Abstract: We propose a novel approach, OrQstrator, which is a modular framework for
conducting quantum circuit optimization in the Noisy Intermediate-Scale Quantum
(NISQ) era. Our framework is powered by Deep Reinforcement Learning (DRL). Our
orchestration engine intelligently selects among three complementary circuit
optimizers: A DRL-based circuit rewriter trained to reduce depth and gate count
via learned rewrite sequences; a domain-specific optimizer that performs
efficient local gate resynthesis and numeric optimization; a parameterized
circuit instantiator that improves compilation by optimizing template circuits
during gate set translation. These modules are coordinated by a central
orchestration engine that learns coordination policies based on circuit
structure, hardware constraints, and backend-aware performance features such as
gate count, depth, and expected fidelity. The system outputs an optimized
circuit for hardware-aware transpilation and execution, leveraging techniques
from an existing state-of-the-art approach, called the NISQ Analyzer, to adapt
to backend constraints.

</details>


### [597] [Prompting for Performance: Exploring LLMs for Configuring Software](https://arxiv.org/abs/2507.09790)
*Helge Spieker,Théo Matricon,Nassim Belmecheri,Jørn Eirik Betten,Gauthier Le Bartz Lyan,Heraldo Borges,Quentin Mazouni,Dennis Gross,Arnaud Gotlieb,Mathieu Acher*

Main category: cs.SE

TL;DR: 论文探讨了大型语言模型（LLMs）在性能导向的软件配置中的潜力，通过实验评估了其在识别选项、排序配置和推荐高性能配置方面的能力，发现其具有潜力但也存在局限性。


<details>
  <summary>Details</summary>
Motivation: 软件配置选项繁多且影响性能，传统方法需要大量计算资源，研究探索LLMs是否能辅助解决这一问题。

Method: 通过实验评估多种LLMs在不同任务（如选项识别、配置排序和推荐）中的表现，涉及编译器、视频编码器等系统。

Result: LLMs在某些任务中表现与专家知识一致，但也存在幻觉或浅层推理的问题。

Conclusion: 研究为LLMs在软件配置中的系统化评估和解决方案设计提供了初步探索。

Abstract: Software systems usually provide numerous configuration options that can
affect performance metrics such as execution time, memory usage, binary size,
or bitrate. On the one hand, making informed decisions is challenging and
requires domain expertise in options and their combinations. On the other hand,
machine learning techniques can search vast configuration spaces, but with a
high computational cost, since concrete executions of numerous configurations
are required. In this exploratory study, we investigate whether large language
models (LLMs) can assist in performance-oriented software configuration through
prompts. We evaluate several LLMs on tasks including identifying relevant
options, ranking configurations, and recommending performant configurations
across various configurable systems, such as compilers, video encoders, and SAT
solvers. Our preliminary results reveal both positive abilities and notable
limitations: depending on the task and systems, LLMs can well align with expert
knowledge, whereas hallucinations or superficial reasoning can emerge in other
cases. These findings represent a first step toward systematic evaluations and
the design of LLM-based solutions to assist with software configuration.

</details>


### [598] [Turning the Tide: Repository-based Code Reflection](https://arxiv.org/abs/2507.09866)
*Wei Zhang,Jian Yang,Jiaxi Yang,Ya Wang,Zhoujun Li,Zeyu Cui,Binyuan Hui,Junyang Lin*

Main category: cs.SE

TL;DR: 论文提出LiveRepoReflection基准和RepoReflection-Instruct数据集，用于评估和改进代码大语言模型在多文件仓库环境中的代码理解和生成能力。


<details>
  <summary>Details</summary>
Motivation: 现有基准忽略代码仓库修改场景，且动态基准中存在数据污染和反思能力不足的问题。

Method: 引入LiveRepoReflection基准（1,888个测试用例，6种语言）和RepoReflection-Instruct数据集，通过两轮对话训练RepoReflectionCoder模型。

Result: 评估了40多个LLM，展示了模型在仓库代码反思任务中的性能。

Conclusion: LiveRepoReflection和RepoReflection-Instruct为代码LLM在多文件仓库环境中的能力提升提供了有效工具。

Abstract: Code large language models (LLMs) enhance programming by understanding and
generating code across languages, offering intelligent feedback, bug detection,
and code updates through reflection, improving development efficiency and
accessibility. While benchmarks (e.g. HumanEval/LiveCodeBench) evaluate code
generation and real-world relevance, previous works ignore the scenario of
modifying code in repositories. Considering challenges remaining in improving
reflection capabilities and avoiding data contamination in dynamic benchmarks,
we introduce LiveRepoReflection, a challenging benchmark for evaluating code
understanding and generation in multi-file repository contexts, featuring 1,888
rigorously filtered test cases across $6$ programming languages to ensure
diversity, correctness, and high difficulty. Further, we create
RepoReflection-Instruct, a large-scale, quality-filtered instruction-tuning
dataset derived from diverse sources, used to train RepoReflectionCoder through
a two-turn dialogue process involving code generation and error-driven repair.
The leaderboard evaluates over 40 LLMs to reflect the model performance of
repository-based code reflection.

</details>


### [599] [Breaking the Myth: Can Small Models Infer Postconditions Too?](https://arxiv.org/abs/2507.10182)
*Gehao Zhang,Zhenting Wang,Juan Zhai*

Main category: cs.SE

TL;DR: 小型微调语言模型在生成高质量后置条件时，计算成本更低，性能与大型模型相当甚至更优。


<details>
  <summary>Details</summary>
Motivation: 手动编写形式化规范繁琐且易错，大型语言模型（LLMs）虽然有效但计算成本高，研究是否可以用小型模型完成任务。

Method: 构建专用数据集，监督微调一个7B参数的代码模型，处理真实仓库依赖并保留预状态信息。

Result: 在真实Java缺陷基准测试中，小型模型在语法正确性、语义正确性和区分缺陷能力上优于或匹配大型模型。

Conclusion: 通过针对性微调，小型模型可以高效完成自动化规范生成任务，为实际应用提供了实用路径。

Abstract: Formal specifications are essential for ensuring software correctness, yet
manually writing them is tedious and error-prone. Large Language Models (LLMs)
have shown promise in generating such specifications from natural language
intents, but the giant model size and high computational demands raise a
fundamental question: Do we really need large models for this task? In this
paper, we show that a small, fine-tuned language model can achieve high-quality
postcondition generation with much lower computational costs. We construct a
specialized dataset of prompts, reasoning logs, and postconditions, then
supervise the fine-tuning of a $7$B-parameter code model. Our approach tackles
real-world repository dependencies and preserves pre-state information,
allowing for expressive and accurate specifications. We evaluate the model on a
benchmark of real-world Java bugs (Defects4J) and compare against both
proprietary giants (e.g., GPT-4o) and open-source large models. Empirical
results demonstrate that our compact model matches or outperforms significantly
larger counterparts in syntax correctness, semantic correctness, and
bug-distinguishing capability. These findings highlight that targeted
fine-tuning on a modest dataset can enable small models to achieve results
formerly seen only in massive, resource-heavy LLMs, offering a practical and
efficient path for the real-world adoption of automated specification
generation.

</details>


<div id='eess.AS'></div>

# eess.AS [[Back]](#toc)

### [600] [SemAlignVC: Enhancing zero-shot timbre conversion using semantic alignment](https://arxiv.org/abs/2507.09070)
*Shivam Mehta,Yingru Liu,Zhenyu Tang,Kainan Peng,Vimal Manohar,Shun Zhang,Mike Seltzer,Qing He,Mingbo Ma*

Main category: eess.AS

TL;DR: SemAlignVC是一种新型零样本语音转换架构，通过SemAlign方法对齐文本和音频表示，减少音色泄漏，提升转换质量。


<details>
  <summary>Details</summary>
Motivation: 解决神经编解码器和LLM语音转换中音色泄漏问题，确保说话人无关的语义编码。

Method: 使用SemAlign对齐文本和音频表示，结合自回归变换器进行高保真转换。

Result: 显著减少音色泄漏，在音色相似性、清晰度和自然度上优于基线方法。

Conclusion: SemAlignVC是一种鲁棒、隐私保护且泛化性强的语音转换解决方案。

Abstract: Zero-shot voice conversion (VC) synthesizes speech in a target speaker's
voice while preserving linguistic and paralinguistic content. However, timbre
leakage-where source speaker traits persist-remains a challenge, especially in
neural codec and LLM-based VC, where quantized representations entangle speaker
identity with content. We introduce SemAlignVC, an architecture designed to
prevent timbre leakage using SemAlign, a novel method that aligns text and
audio representations to ensure speaker-independent semantic encoding. This
disentangled representation conditions an autoregressive transformer for
high-fidelity conversion without explicit speaker embeddings. Experiments show
SemAlignVC significantly reduces timbre leakage, outperforming baselines in
speaker timbre similarity, intelligibility, and naturalness, making it a
robust, privacy-preserving, and generalizable VC solution. Audio samples can be
accessed at https://shivammehta25.github.io/SemAlignVC/

</details>


### [601] [Large Language Models and Non-Negative Matrix Factorization for Bioacoustic Signal Decomposition](https://arxiv.org/abs/2507.09161)
*Yasaman Torabi,Shahram Shirani,James P. Reilly*

Main category: eess.AS

TL;DR: 结合大型语言模型和矩阵分解的生物声学信号分析方法，用于临床诊断支持。


<details>
  <summary>Details</summary>
Motivation: 传统数值方法在生物医学信号分析中受限，大型语言模型为无结构数据提供新解读方式。

Method: 使用矩阵分解分离重叠的生物声学信号，再通过大型语言模型关联声学模式与潜在医疗状况。

Result: 方法在临床模拟环境中验证，无需标记数据或先验知识，具有高解释性和临床适用性。

Conclusion: 该方法有望整合到未来智能诊断工具中，提升临床决策支持能力。

Abstract: Large language models have shown a remarkable ability to extract meaning from
unstructured data, offering new ways to interpret biomedical signals beyond
traditional numerical methods. In this study, we present a matrix factorization
framework for bioacoustic signal analysis which is enhanced by large language
models. The focus is on separating bioacoustic signals that commonly overlap in
clinical recordings, using matrix factorization to decompose the mixture into
interpretable components. A large language model is then applied to the
separated signals to associate distinct acoustic patterns with potential
medical conditions such as cardiac rhythm disturbances or respiratory
abnormalities. Recordings were obtained from a digital stethoscope applied to a
clinical manikin to ensure a controlled and high-fidelity acquisition
environment. This hybrid approach does not require labeled data or prior
knowledge of source types, and it provides a more interpretable and accessible
framework for clinical decision support. The method demonstrates promise for
integration into future intelligent diagnostic tools.

</details>


### [602] [Can We Really Repurpose Multi-Speaker ASR Corpus for Speaker Diarization?](https://arxiv.org/abs/2507.09226)
*Shota Horiguchi,Naohiro Tawara,Takanori Ashihara,Atsushi Ando,Marc Delcroix*

Main category: eess.AS

TL;DR: 论文探讨了神经说话人日志中边界松散对性能的影响，提出通过强制对齐标准化边界以提高性能。


<details>
  <summary>Details</summary>
Motivation: 多说话人数据集训练需求大，但现有ASR数据集边界松散，影响日志性能评估和模型泛化能力。

Method: 采用强制对齐标准化边界，结合简单后处理。

Result: 标准化边界提高了日志性能和ASR性能，尤其在流式场景中。

Conclusion: 标准化边界是提升日志和ASR性能的有效方法。

Abstract: Neural speaker diarization is widely used for overlap-aware speaker
diarization, but it requires large multi-speaker datasets for training. To meet
this data requirement, large datasets are often constructed by combining
multiple corpora, including those originally designed for multi-speaker
automatic speech recognition (ASR). However, ASR datasets often feature loosely
defined segment boundaries that do not align with the stricter conventions of
diarization benchmarks. In this work, we show that such boundary looseness
significantly impacts the diarization error rate, reducing evaluation
reliability. We also reveal that models trained on data with varying boundary
precision tend to learn dataset-specific looseness, leading to poor
generalization across out-of-domain datasets. Training with standardized tight
boundaries via forced alignment improves not only diarization performance,
especially in streaming scenarios, but also ASR performance when combined with
simple post-processing.

</details>


### [603] [Controllable joint noise reduction and hearing loss compensation using a differentiable auditory model](https://arxiv.org/abs/2507.09372)
*Philippe Gonzalez,Torsten Dau,Tobias May*

Main category: eess.AS

TL;DR: 该论文提出了一种基于多任务学习的深度学习方法，同时处理噪声抑制（NR）和听力损失补偿（HLC），通过可微分听觉模型优化任务平衡。


<details>
  <summary>Details</summary>
Motivation: 听力损失补偿（HLC）缺乏真实目标，现有方法缺乏灵活性或未平衡噪声抑制（NR）和HLC的任务。

Method: 将NR和HLC作为多任务学习问题，使用可微分听觉模型训练系统，从噪声语音和听力图中预测去噪和补偿信号。

Result: 系统在客观指标上表现与单独训练的系统相当，且能在推理时调整NR和HLC的平衡。

Conclusion: 多任务学习方法有效平衡了NR和HLC，提供了灵活的任务调整能力。

Abstract: Deep learning-based hearing loss compensation (HLC) seeks to enhance speech
intelligibility and quality for hearing impaired listeners using neural
networks. One major challenge of HLC is the lack of a ground-truth target.
Recent works have used neural networks to emulate non-differentiable auditory
peripheral models in closed-loop frameworks, but this approach lacks
flexibility. Alternatively, differentiable auditory models allow direct
optimization, yet previous studies focused on individual listener profiles, or
joint noise reduction (NR) and HLC without balancing each task. This work
formulates NR and HLC as a multi-task learning problem, training a system to
simultaneously predict denoised and compensated signals from noisy speech and
audiograms using a differentiable auditory model. Results show the system
achieves similar objective metric performance to systems trained for each task
separately, while being able to adjust the balance between NR and HLC during
inference.

</details>


### [604] [The DKU System for Multi-Speaker Automatic Speech Recognition in MLC-SLM Challenge](https://arxiv.org/abs/2507.09499)
*Yuke Lin,Ming Cheng,Ze Li,Ming Li*

Main category: eess.AS

TL;DR: DKU系统在MLC-SLM挑战赛任务2中表现优异，通过集成说话人嵌入和时间边界到LLM中，显著优于官方基线。


<details>
  <summary>Details</summary>
Motivation: 解决多说话人语音识别中无Oracle说话人标签和时间边界的问题。

Method: 基于Qwen2.5的LLM，集成说话人嵌入和时间边界，并通过语言特定适配器和LoRA模块优化多语言性能。

Result: 在开发集和测试集上分别达到23.56%和18.08%的tcpWER。

Conclusion: DKU系统在多说话人语音识别任务中表现突出，显著优于基线。

Abstract: We present the DKU system for Task 2 of the MLC-SLM Challenge, which aims to
perform multi-speaker automatic speech recognition directly from raw audio
without Oracle speaker labels or time boundaries. Our approach builds upon a
diarization-aware framework integrating speaker embeddings and temporal
utterance boundaries into a Qwen2.5-based large language model (LLM). Then, we
enhance the system's multilingual performance by fine-tuning language-specific
adapters and LoRA modules within the LLM decoder. Finally, our system achieves
the tcpWER of 23.56\% and 18.08\% on the development and test sets of the
MLC-SLM dataset, substantially outperforming the official baseline.

</details>


### [605] [Enhancing Stereo Sound Event Detection with BiMamba and Pretrained PSELDnet](https://arxiv.org/abs/2507.09570)
*Wenmiao Gao,Han Yin*

Main category: eess.AS

TL;DR: 提出了一种基于预训练PSELDnet和双向Mamba序列模型的立体声SELD系统，通过替换Conformer模块为BiMamba模块，并采用非对称卷积优化音频信号处理，显著提升了性能并降低了计算成本。


<details>
  <summary>Details</summary>
Motivation: 现有基于Transformer的SELD模型计算成本高，亟需更高效的解决方案。

Method: 使用预训练的PSELDnet和双向Mamba序列模型，替换Conformer模块为BiMamba模块，并引入非对称卷积优化音频信号处理。

Result: 在DCASE2025 Task 3开发数据集上表现优于基线和原始PSELDnet，同时计算资源消耗更低。

Conclusion: BiMamba架构能有效解决SELD任务中的关键挑战，且计算效率更高。

Abstract: Pre-training methods have greatly improved the performance of sound event
localization and detection (SELD). However, existing Transformer-based models
still face high computational cost. To solve this problem, we present a stereo
SELD system using a pre-trained PSELDnet and a bidirectional Mamba sequence
model. Specifically, we replace the Conformer module with a BiMamba module. We
also use asymmetric convolutions to better capture the time and frequency
relationships in the audio signal. Test results on the DCASE2025 Task 3
development dataset show that our method performs better than both the baseline
and the original PSELDnet with a Conformer decoder. In addition, the proposed
model costs fewer computing resources than the baselines. These results show
that the BiMamba architecture is effective for solving key challenges in SELD
tasks. The source code is publicly accessible at https://github.com/
alexandergwm/DCASE2025 TASK3 Stereo PSELD Mamba.

</details>


### [606] [Low-Rank Adaptation of Deep Prior Neural Networks For Room Impulse Response Reconstruction](https://arxiv.org/abs/2507.09806)
*Mirco Pezzoli,Federico Miotello,Shoichi Koyama,Fabio Antonacci*

Main category: eess.AS

TL;DR: Deep Prior框架通过LoRA实现高效微调，解决了其在声学配置变化时需重新训练的局限性。


<details>
  <summary>Details</summary>
Motivation: 解决Deep Prior框架在新声学配置（如声源位置变化）下需重新训练的问题。

Method: 采用Low-Rank Adaptation（LoRA）技术，通过低秩分解可训练参数实现高效微调。

Result: LoRA微调在声源位置变化时表现优异，计算开销低且物理保真度高。

Conclusion: LoRA微调展示了迁移学习在声学应用中的价值，尤其适用于有限麦克风场景。

Abstract: The Deep Prior framework has emerged as a powerful generative tool which can
be used for reconstructing sound fields in an environment from few sparse
pressure measurements. It employs a neural network that is trained solely on a
limited set of available data and acts as an implicit prior which guides the
solution of the underlying optimization problem. However, a significant
limitation of the Deep Prior approach is its inability to generalize to new
acoustic configurations, such as changes in the position of a sound source. As
a consequence, the network must be retrained from scratch for every new setup,
which is both computationally intensive and time-consuming. To address this, we
investigate transfer learning in Deep Prior via Low-Rank Adaptation (LoRA),
which enables efficient fine-tuning of a pre-trained neural network by
introducing a low-rank decomposition of trainable parameters, thus allowing the
network to adapt to new measurement sets with minimal computational overhead.
We embed LoRA into a MultiResUNet-based Deep Prior model and compare its
adaptation performance against full fine-tuning of all parameters as well as
classical retraining, particularly in scenarios where only a limited number of
microphones are used. The results indicate that fine-tuning, whether done
completely or via LoRA, is especially advantageous when the source location is
the sole changing parameter, preserving high physical fidelity, and
highlighting the value of transfer learning for acoustics applications.

</details>


### [607] [Generative Audio Language Modeling with Continuous-valued Tokens and Masked Next-Token Prediction](https://arxiv.org/abs/2507.09834)
*Shu-wen Yang,Byeonggeun Kim,Kuan-Po Huang,Qingming Tang,Huy Phan,Bo-Ru Lu,Harsha Sundar,Shalini Ghosh,Hung-yi Lee,Chieh-Chi Kao,Chao Wang*

Main category: eess.AS

TL;DR: 论文提出了一种基于因果语言模型的音频生成方法，通过连续值标记的扩散模型改进现有离散方案，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 扩展自回归Transformer解码器到音频生成领域，解决连续信号处理的挑战。

Method: 采用标记级扩散模型建模连续值标记的分布，并提出掩码下一标记预测任务。

Result: 在AudioCaps上，FAD和KL散度分别提升20%和40%，且模型参数更少。

Conclusion: 该方法在音频生成中表现出色，性能接近SOTA扩散模型，同时参数更高效。

Abstract: Autoregressive next-token prediction with the Transformer decoder has become
a de facto standard in large language models (LLMs), achieving remarkable
success in Natural Language Processing (NLP) at scale. Extending this paradigm
to audio poses unique challenges due to its inherently continuous nature. We
research audio generation with a causal language model (LM) without discrete
tokens. We leverage token-wise diffusion to model the continuous distribution
of the next continuous-valued token. Our approach delivers significant
improvements over previous discrete solution, AudioGen, achieving 20% and 40%
relative gains on AudioCaps in Frechet Audio Distance (FAD) and
Kullback-Leibler (KL) divergence, respectively. Additionally, we propose a
novel masked next-token prediction task that incorporates masked prediction
into the causal LM framework. On AudioCaps, the innovation yields 41% and 33%
relative FAD improvements over AudioGen Base (285M) and AudioGen Large (1B)
models, respectively, and is on par with the state-of-the-art (SOTA) diffusion
models. Furthermore, we achieve these results with significantly fewer
parameters -- 193M for our Base and 462M for our Large models.

</details>


### [608] [ZipVoice-Dialog: Non-Autoregressive Spoken Dialogue Generation with Flow Matching](https://arxiv.org/abs/2507.09318)
*Han Zhu,Wei Kang,Liyong Guo,Zengwei Yao,Fangjun Kuang,Weiji Zhuang,Zhaoqing Li,Zhifeng Han,Dong Zhang,Xin Zhang,Xingchen Song,Long Lin,Daniel Povey*

Main category: eess.AS

TL;DR: ZipVoice-Dialog是一种基于流匹配的非自回归零样本语音对话生成模型，解决了现有自回归模型推理慢且不稳定的问题，并引入了OpenDialog数据集和评估基准。


<details>
  <summary>Details</summary>
Motivation: 现有语音对话生成模型因自回归特性导致推理慢且不稳定，且缺乏大规模开源数据集。

Method: 采用流匹配技术，引入说话人轮换嵌入、课程学习策略和立体声对话生成策略，并构建OpenDialog数据集。

Result: 在清晰度、说话人轮换准确性、说话人相似性和推理速度上表现优异。

Conclusion: ZipVoice-Dialog为语音对话生成提供了高效稳定的解决方案，并开源了相关资源。

Abstract: Generating spoken dialogue is more challenging than monologue text-to-speech
(TTS) due to the need for realistic turn-taking and distinct speaker timbres.
Existing spoken dialogue generation models, being auto-regressive, suffer from
slow and unstable inference. To overcome these limitations, we introduce
ZipVoice-Dialog, a non-autoregressive zero-shot spoken dialogue generation
model built upon flow matching. Key designs include: 1) speaker-turn embeddings
for precise speaker turn-taking; 2) a curriculum learning strategy for stable
speech-text alignment; 3) specialized strategies to enable stereo dialogue
generation. Additionally, recognizing the lack of open-source large-scale
spoken dialogue datasets, we curated OpenDialog, a 6.8k-hour spoken dialogue
dataset from in-the-wild speech data. Furthermore, we established a benchmark
to comprehensively evaluate various models. Experimental results demonstrate
that ZipVoice-Dialog achieves superior performance in intelligibility, speaker
turn-taking accuracy, speaker similarity, and inference speed. Our codes, model
checkpoints, demo samples, and the OpenDialog dataset are all publicly
available at https://github.com/k2-fsa/ZipVoice.

</details>


### [609] [Natural Language-based Assessment of L2 Oral Proficiency using LLMs](https://arxiv.org/abs/2507.10200)
*Stefano Bannò,Rao Ma,Mengjie Qian,Siyuan Tang,Kate Knill,Mark Gales*

Main category: eess.AS

TL;DR: NLA是一种基于自然语言的评估方法，利用LLMs（如Qwen 2.5 72B）解释和应用人类评估指令，在零样本设置下评估语言能力。结果显示其性能优于BERT模型，但不及专用语音LLMs。


<details>
  <summary>Details</summary>
Motivation: 探索LLMs能否像人类评估者一样解释和应用语言能力描述符，以评估第二语言能力。

Method: 使用开源LLM Qwen 2.5 72B，在零样本设置下评估S&I Corpus中的回答。

Result: NLA在文本信息下表现优异，优于BERT模型，但不及专用语音LLMs，尤其在任务不匹配时表现突出。

Conclusion: NLA具有广泛适用性、可解释性，并能推广到其他数据类型和语言。

Abstract: Natural language-based assessment (NLA) is an approach to second language
assessment that uses instructions - expressed in the form of can-do descriptors
- originally intended for human examiners, aiming to determine whether large
language models (LLMs) can interpret and apply them in ways comparable to human
assessment. In this work, we explore the use of such descriptors with an
open-source LLM, Qwen 2.5 72B, to assess responses from the publicly available
S&I Corpus in a zero-shot setting. Our results show that this approach -
relying solely on textual information - achieves competitive performance: while
it does not outperform state-of-the-art speech LLMs fine-tuned for the task, it
surpasses a BERT-based model trained specifically for this purpose. NLA proves
particularly effective in mismatched task settings, is generalisable to other
data types and languages, and offers greater interpretability, as it is
grounded in clearly explainable, widely applicable language descriptors.

</details>


### [610] [Aligning Generative Speech Enhancement with Human Preferences via Direct Preference Optimization](https://arxiv.org/abs/2507.09929)
*Haoyang Li,Nana Hou,Yuchen Hu,Jixun Yao,Sabato Marco Siniscalchi,Eng Siong Chng*

Main category: eess.AS

TL;DR: 提出了一种基于语言模型（LM）的语音增强（SE）新方法，利用直接偏好优化（DPO）提升感知质量，实验显示在多个质量指标上均有显著改进。


<details>
  <summary>Details</summary>
Motivation: 现有基于LM的SE方法可能因最大化干净语音标记的似然而与人类感知不一致，导致质量下降。本文旨在通过DPO优化感知质量。

Method: 使用UTMOS作为人类评分的代理，通过DPO引导优化，使输出更符合人类感知偏好。

Result: 在2020 Deep Noise Suppression Challenge测试集上，应用DPO的预训练LM-SE模型在多个质量指标上相对提升高达56%。

Conclusion: 这是首次将DPO应用于SE，并首次在LM-SE训练中引入代理感知反馈，为感知对齐的SE提供了新方向。

Abstract: This work investigates speech enhancement (SE) from the perspective of
language models (LMs). We propose a novel method that leverages Direct
Preference Optimization (DPO) to improve the perceptual quality of enhanced
speech. Using UTMOS, a neural MOS prediction model, as a proxy for human
ratings, our approach guides optimization toward perceptually preferred
outputs. This differs from existing LM-based SE methods that focus on
maximizing the likelihood of clean speech tokens, which may misalign with human
perception and degrade quality despite low prediction error. Experiments on the
2020 Deep Noise Suppression Challenge test sets demonstrate that applying DPO
to a pretrained LM-based SE model yields consistent improvements across various
speech quality metrics, with relative gains of up to 56%. To our knowledge,
this is the first application of DPO to SE and the first to incorporate proxy
perceptual feedback into LM-based SE training, pointing to a promising
direction for perceptually aligned SE.

</details>


<div id='cond-mat.mtrl-sci'></div>

# cond-mat.mtrl-sci [[Back]](#toc)

### [611] [Surprisingly High Redundancy in Electronic Structure Data](https://arxiv.org/abs/2507.09001)
*Sazzad Hossain,Ponkrshnan Thiagarajan,Shashank Pathrudkar,Stephanie Taylor,Abhijeet S. Gangan,Amartya S. Banerjee,Susanta Ghosh*

Main category: cond-mat.mtrl-sci

TL;DR: 研究发现电子结构数据中存在高度冗余，挑战了传统认为需要大数据集的假设，证明通过修剪数据集可显著减少数据量而不损失准确性。


<details>
  <summary>Details</summary>
Motivation: 揭示电子结构数据中的冗余性，挑战现有机器学习模型依赖大数据集的假设，探索减少数据量的方法。

Method: 采用随机修剪和最先进的覆盖修剪策略，比较其对预测准确性和模型泛化能力的影响。

Result: 随机修剪可显著减少数据量，覆盖修剪在减少100倍数据量时仍保持化学准确性，而重要性修剪在高修剪率下可能失效。

Conclusion: 电子结构数据中存在高度冗余，可通过修剪识别最小代表性数据集，显著提升效率。

Abstract: Machine Learning (ML) models for electronic structure rely on large datasets
generated through expensive Kohn-Sham Density Functional Theory simulations.
This study reveals a surprisingly high level of redundancy in such datasets
across various material systems, including molecules, simple metals, and
complex alloys. Our findings challenge the prevailing assumption that large,
exhaustive datasets are necessary for accurate ML predictions of electronic
structure. We demonstrate that even random pruning can substantially reduce
dataset size with minimal loss in predictive accuracy, while a state-of-the-art
coverage-based pruning strategy retains chemical accuracy and model
generalizability using up to 100-fold less data and reducing training time by
threefold or more. By contrast, widely used importance-based pruning methods,
which eliminate seemingly redundant data, can catastrophically fail at higher
pruning factors, possibly due to the significant reduction in data coverage.
This heretofore unexplored high degree of redundancy in electronic structure
data holds the potential to identify a minimal, essential dataset
representative of each material class.

</details>


<div id='physics.data-an'></div>

# physics.data-an [[Back]](#toc)

### [612] [Mind the Gap: Navigating Inference with Optimal Transport Maps](https://arxiv.org/abs/2507.08867)
*Malte Algren,Tobias Golling,Francesco Armando Di Bello,Christopher Pollard*

Main category: physics.data-an

TL;DR: 提出了一种基于最优传输的校准方法，解决机器学习在粒子物理中因模拟与实验数据差异导致的性能限制问题。


<details>
  <summary>Details</summary>
Motivation: 现代机器学习算法依赖高质量训练样本，但模拟与实验数据的差异限制了其有效性。

Method: 采用基于最优传输的校准方法，应用于高维模拟，并通过喷注标记验证。

Result: 校准后的高维潜在表示使下游任务中的多种量得到正确校准，提升了LHC分析中喷注信息的应用。

Conclusion: 该方法为粒子物理中的基础模型校准提供了关键步骤，并在科学领域的高维模拟校正中有广泛应用。

Abstract: Machine learning (ML) techniques have recently enabled enormous gains in
sensitivity across the sciences. In particle physics, much of this progress has
relied on excellent simulations of a wide range of physical processes. However,
due to the sophistication of modern machine learning (ML) algorithms and their
reliance on high-quality training samples, discrepancies between simulation and
experimental data can significantly limit the effectiveness of ML techniques.
In this work, we present a solution to this ``mis-specification'' problem: a
calibration approach based on optimal transport, which we apply to
high-dimensional simulations for the first time. We demonstrate the performance
of our approach through jet tagging, using a CMS-inspired dataset. A
128-dimensional internal jet representation from a powerful general-purpose
classifier is studied; after calibrating this internal ``latent''
representation, we find that a wide variety of quantities derived from it for
downstream tasks are also properly calibrated: using this calibrated
high-dimensional representation, powerful new applications of jet flavor
information can be utilized in LHC analyses. This is a key step toward allowing
properly-calibrated ``foundation models'' in particle physics. More broadly,
this calibration framework has broad applications for correcting
high-dimensional simulations across the sciences.

</details>


<div id='math.OC'></div>

# math.OC [[Back]](#toc)

### [613] [Coordinated Communication and Inventory Optimization in Multi-Retailer Supply Chains](https://arxiv.org/abs/2507.09223)
*Sagar Sudhakara,Yuchong Zhang*

Main category: math.OC

TL;DR: 论文研究了多零售商供应链中动态信息共享的优化问题，通过POMDP模型平衡通信成本与运营绩效，提出了一种近似解法PBVI/SARSOP，实验表明其优于静态共享策略。


<details>
  <summary>Details</summary>
Motivation: 传统固定信息共享协议（如始终共享或从不共享）无法灵活应对通信成本与运营绩效的权衡，因此需要动态优化信息共享策略。

Method: 采用集中式POMDP模型，结合动态规划求解最优策略，并提出近似解法PBVI/SARSOP处理计算复杂度。

Result: 实验证明动态信息共享策略显著改善了成本与服务水平的权衡，且零售商仅需共享有限数据即可实现最优性能。

Conclusion: 动态信息共享策略在多零售商供应链中具有显著优势，能有效优化信息交换频率与库存控制。

Abstract: We consider a multi-retailer supply chain where each retailer can dynamically
choose when to share information (e.g., local inventory levels or demand
observations) with other retailers, incurring a communication cost for each
sharing event. This flexible information exchange mechanism contrasts with
fixed protocols such as always sharing or never sharing. We formulate a joint
optimization of inventory control and communication strategies, aiming to
balance the trade-off between communication overhead and operational
performance (service levels, holding, and stockout costs). We adopt a common
information framework and derive a centralized Partially Observable Markov
Decision Process (POMDP) model for a supply chain coordinator. Solving this
coordinator's POMDP via dynamic programming characterizes the structure of
optimal policies, determining when retailers should communicate and how they
should adjust orders based on available information. We show that, in this
setting, retailers can often act optimally by sharing only limited summaries of
their private data, reducing communication frequency without compromising
performance. We also incorporate practical constraints on communication
frequency and propose an approximate point-based POMDP solution method
(PBVI/SARSOP) to address computational complexity. Numerical experiments on
multi-retailer inventory scenarios demonstrate that our approach significantly
improves the cost-service trade-off compared to static information sharing
policies, effectively optimizing the schedule of information exchange for
cooperative inventory control.

</details>


### [614] [Stochastic Approximation with Block Coordinate Optimal Stepsizes](https://arxiv.org/abs/2507.08963)
*Tao Jiang,Lin Xiao*

Main category: math.OC

TL;DR: 论文提出了一种基于块坐标步长的随机近似方法，通过在线估计搜索方向的二阶矩，自适应调整步长，性能与Adam相当但更节省内存和超参数。


<details>
  <summary>Details</summary>
Motivation: 研究如何通过自适应步长规则优化随机近似方法，减少对最优点的期望距离，同时降低计算资源需求。

Method: 采用块坐标步长和在线二阶矩估计，提出一种新的条件估计器，减少内存和超参数需求。

Result: 新方法性能与Adam相当，但更高效；理论证明方法几乎必然收敛到最优点的邻域。

Conclusion: 该方法在非凸非光滑条件下仍适用，收敛性依赖于二阶矩估计的偏差和方差。

Abstract: We consider stochastic approximation with block-coordinate stepsizes and
propose adaptive stepsize rules that aim to minimize the expected distance from
the next iterate to an optimal point. These stepsize rules employ online
estimates of the second moment of the search direction along each block
coordinate. The popular Adam algorithm can be interpreted as a particular
heuristic for such estimation. By leveraging a simple conditional estimator, we
derive a new method that obtains comparable performance as Adam but requires
less memory and fewer hyper-parameters. We prove that this family of methods
converges almost surely to a small neighborhood of the optimal point, and the
radius of the neighborhood depends on the bias and variance of the
second-moment estimator. Our analysis relies on a simple aiming condition that
assumes neither convexity nor smoothness, thus has broad applicability.

</details>


### [615] [On the Gradient Domination of the LQG Problem](https://arxiv.org/abs/2507.09026)
*Kasra Fallah,Leonardo F. Toso,James Anderson*

Main category: math.OC

TL;DR: 论文研究了通过策略梯度（PG）方法解决线性二次高斯（LQG）调节器问题，提出了一种新的参数化方法，证明了全局收敛性和稳定性。


<details>
  <summary>Details</summary>
Motivation: 尽管PG方法在解决线性二次调节器（LQR）问题中表现出色，但在LQG问题中的理论理解仍有限，尤其是缺乏梯度优势。本文旨在填补这一空白。

Method: 采用了一种基于历史输入和输出数据的控制器参数化方法，并通过提升论证建立了梯度优势和近似平滑性。

Result: 证明了在模型依赖和非模型依赖设置下，PG方法在LQG问题中的全局收敛性和每次迭代的稳定性。

Conclusion: 通过数值实验验证了全局收敛性，并展示了不同历史长度对收敛的影响。

Abstract: We consider solutions to the linear quadratic Gaussian (LQG) regulator
problem via policy gradient (PG) methods. Although PG methods have demonstrated
strong theoretical guarantees in solving the linear quadratic regulator (LQR)
problem, despite its nonconvex landscape, their theoretical understanding in
the LQG setting remains limited. Notably, the LQG problem lacks gradient
dominance in the classical parameterization, i.e., with a dynamic controller,
which hinders global convergence guarantees. In this work, we study PG for the
LQG problem by adopting an alternative parameterization of the set of
stabilizing controllers and employing a lifting argument. We refer to this
parameterization as a history representation of the control input as it is
parameterized by past input and output data from the previous p time-steps.
This representation enables us to establish gradient dominance and approximate
smoothness for the LQG cost. We prove global convergence and per-iteration
stability guarantees for policy gradient LQG in model-based and model-free
settings. Numerical experiments on an open-loop unstable system are provided to
support the global convergence guarantees and to illustrate convergence under
different history lengths of the history representation.

</details>


### [616] [A Method for Learning to Solve Parametric Bilevel Optimization with Coupling Constraints](https://arxiv.org/abs/2507.09050)
*James Kotary,Himanshu Sharma,Ethan King,Draguna Vrabie,Ferdinando Fioretto,Jan Drgona*

Main category: math.OC

TL;DR: 本文提出了一种学习解决双层优化问题的框架，利用现代优化问题的微分技术，展示了神经网络如何高效近似参数化双层优化。


<details>
  <summary>Details</summary>
Motivation: 双层优化问题在许多重要场景中应用广泛，但传统方法难以高效解决，尤其是在时间紧迫的情况下。

Method: 利用现代优化问题的微分技术，训练神经网络作为参数化双层优化的高效近似器。

Result: 在合成双层优化问题和控制系统协同设计问题上验证了框架的有效性。

Conclusion: 该框架为双层优化问题提供了一种高效的学习解决方案，展示了神经网络在复杂优化问题中的潜力。

Abstract: Learning to Optimize (L2O) is a subfield of machine learning (ML) in which ML
models are trained to solve parametric optimization problems. The general goal
is to learn a fast approximator of solutions to constrained optimization
problems, as a function of their defining parameters. Prior L2O methods focus
almost entirely on single-level programs, in contrast to the bilevel programs,
whose constraints are themselves expressed in terms of optimization
subproblems. Bilevel programs have numerous important use cases but are
notoriously difficult to solve, particularly under stringent time demands. This
paper proposes a framework for learning to solve a broad class of challenging
bilevel optimization problems, by leveraging modern techniques for
differentiation through optimization problems. The framework is illustrated on
an array of synthetic bilevel programs, as well as challenging control system
co-design problems, showing how neural networks can be trained as efficient
approximators of parametric bilevel optimization.

</details>


### [617] [Nesterov Finds GRAAL: Optimal and Adaptive Gradient Method for Convex Optimization](https://arxiv.org/abs/2507.09823)
*Ekaterina Borodich,Dmitry Kovalev*

Main category: math.OC

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: In this paper, we focus on the problem of minimizing a continuously
differentiable convex objective function $\min_x f(x)$. Recently, several
adaptive gradient methods, including GRAAL (Malitsky, 2020), have been
developed. These methods estimate the local curvature of the objective function
to compute stepsizes, attain the standard convergence rate $\mathcal{O}(1/k)$
of fixed-stepsize gradient descent for Lipschitz-smooth functions, and do not
require any line search procedures or hyperparameter tuning. However, a natural
question arises: is it possible to accelerate the convergence of these
algorithms to match the optimal rate $\mathcal{O}(1/k^2)$ of the accelerated
gradient descent of Nesterov (1983)? Although some attempts have been made (Li
and Lan, 2023), the capabilities of the existing accelerated algorithms to
adapt to the curvature of the objective function are highly limited.
Consequently, we provide a positive answer to this question and develop GRAAL
with Nesterov acceleration. We prove that our algorithm achieves the desired
optimal convergence rate for Lipschitz smooth functions. Moreover, in contrast
to existing methods, it does so with an arbitrary, even excessively small,
initial stepsize at the cost of a logarithmic additive term in the iteration
complexity.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [618] [THOR: Transformer Heuristics for On-Demand Retrieval](https://arxiv.org/abs/2507.09592)
*Isaac Shi,Zeyuan Li,Fan Liu,Wenli Wang,Lewei He,Yang Yang,Tianyu Shi*

Main category: cs.DB

TL;DR: THOR模块是一个将自然语言问题转化为安全SQL查询的引擎，专为企业数据库设计，支持动态元数据注入、自我修正和结果解释。


<details>
  <summary>Details</summary>
Motivation: 为非技术用户提供简单、安全的企业数据访问方式，同时确保查询的可靠性和合规性。

Method: 采用解耦架构，包括查询路由、动态元数据注入、SQL生成、自我修正循环和结果解释。

Result: 在财务、销售和运营场景中验证了可靠的即席查询和自动化报告功能。

Conclusion: THOR模块通过嵌入模式感知和容错执行，实现了零SQL复杂度的企业级数据访问。

Abstract: We introduce the THOR (Transformer Heuristics for On-Demand Retrieval)
Module, designed and implemented by eSapiens, a secure, scalable engine that
transforms natural-language questions into verified, read-only SQL analytics
for enterprise databases. The Text-to-SQL module follows a decoupled
orchestration/execution architecture: a Supervisor Agent routes queries, Schema
Retrieval dynamically injects table and column metadata, and a SQL Generation
Agent emits single-statement SELECT queries protected by a read-only guardrail.
An integrated Self-Correction & Rating loop captures empty results, execution
errors, or low-quality outputs and triggers up to five LLM-driven regeneration
attempts. Finally, a Result Interpretation Agent produces concise,
human-readable insights and hands raw rows to the Insight & Intelligence engine
for visualization or forecasting.
  Smoke tests across finance, sales, and operations scenarios demonstrate
reliable ad-hoc querying and automated periodic reporting. By embedding schema
awareness, fault-tolerant execution, and compliance guardrails, the THOR Module
empowers non-technical users to access live data with zero-SQL simplicity and
enterprise-grade safety.

</details>


### [619] [TRACER: Efficient Object Re-Identification in Networked Cameras through Adaptive Query Processing](https://arxiv.org/abs/2507.09448)
*Pramod Chunduri,Yao Lu,Joy Arulraj*

Main category: cs.DB

TL;DR: Tracer是一个新型VDBMS，通过自适应查询处理框架高效处理Re-ID查询，解决了现有系统Spactula的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有系统Spactula在大型摄像头网络中因局部历史数据导致时空过滤精度不足，且不支持自适应查询处理，无法满足高召回需求的关键视频分析应用。

Method: Tracer利用递归网络建模长期历史相关性选择最优摄像头，并采用概率自适应搜索模型加速查询。此外，提出了基于真实交通分布的多摄像头Re-ID合成基准数据集。

Result: Tracer在多样化数据集上平均性能优于现有最优系统3.9倍。

Conclusion: Tracer通过自适应框架和合成基准，显著提升了Re-ID查询的效率和召回率。

Abstract: Efficiently re-identifying and tracking objects across a network of cameras
is crucial for applications like traffic surveillance. Spatula is the
state-of-the-art video database management system (VDBMS) for processing Re-ID
queries. However, it suffers from two limitations. Its spatio-temporal
filtering scheme has limited accuracy on large camera networks due to localized
camera history. It is not suitable for critical video analytics applications
that require high recall due to a lack of support for adaptive query
processing.
  In this paper, we present Tracer, a novel VDBMS for efficiently processing
Re-ID queries using an adaptive query processing framework. Tracer selects the
optimal camera to process at each time step by training a recurrent network to
model long-term historical correlations. To accelerate queries under a high
recall constraint, Tracer incorporates a probabilistic adaptive search model
that processes camera feeds in incremental search windows and dynamically
updates the sampling probabilities using an exploration-exploitation strategy.
To address the paucity of benchmarks for the Re-ID task due to privacy
concerns, we present a novel synthetic benchmark for generating multi-camera
Re-ID datasets based on real-world traffic distribution. Our evaluation shows
that Tracer outperforms the state-of-the-art cross-camera analytics system by
3.9x on average across diverse datasets.

</details>


### [620] [HedraRAG: Coordinating LLM Generation and Database Retrieval in Heterogeneous RAG Serving](https://arxiv.org/abs/2507.09138)
*Zhengding Hu,Vibha Murthy,Zaifeng Pan,Wanlu Li,Xiaoyi Fang,Yufei Ding,Yuke Wang*

Main category: cs.DB

TL;DR: HedraRAG是一个基于图抽象的系统，通过动态图变换优化异构检索增强生成（RAG）服务的执行效率，显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 解决异构RAG服务中多阶段工作流和多样化请求模式带来的系统级挑战。

Method: 采用图抽象和动态图变换（如节点分裂、重排序、边添加等）优化执行计划，并映射到CPU-GPU混合流水线。

Result: 在多种RAG工作流中实现1.5倍至5倍的加速。

Conclusion: HedraRAG通过协调生成和检索，显著提升了服务环境的效率和性能。

Abstract: This paper addresses emerging system-level challenges in heterogeneous
retrieval-augmented generation (RAG) serving, where complex multi-stage
workflows and diverse request patterns complicate efficient execution. We
present HedraRAG, a runtime system built on a graph-based abstraction that
exposes optimization opportunities across stage-level parallelism,
intra-request similarity, and inter-request skewness. These opportunities are
realized through dynamic graph transformations, such as node splitting,
reordering, edge addition, and dependency rewiring, applied to wavefronts of
subgraphs spanning concurrent requests. The resulting execution plans are
mapped onto hybrid CPU-GPU pipelines to improve resource utilization and reduce
latency. Evaluations across a wide range of RAG workflows demonstrate speedups
exceeding 1.5x and reaching up to 5x over existing frameworks, showcasing the
effectiveness of coordinated generation and retrieval in serving environments.

</details>


<div id='stat.AP'></div>

# stat.AP [[Back]](#toc)

### [621] [History Matching under Uncertainty of Geological Scenarios with Implicit Geological Realism Control with Generative Deep Learning and Graph Convolutions](https://arxiv.org/abs/2507.10201)
*Gleb Shishaev,Vasily Demyanov,Daniel Arnold*

Main category: stat.AP

TL;DR: 提出一种基于图的变分自编码器，用于处理地质场景的不确定性，通过潜在空间和测地度量控制地质真实性。


<details>
  <summary>Details</summary>
Motivation: 传统基于格网的深度学习方法在地质建模中存在局限性，需更灵活的方法处理不同地质场景的不确定性。

Method: 采用图结构替代传统格网方法，结合变分自编码器和测地度量，通过潜在变量控制地质真实性。

Result: 在合成数据集（单通道和双通道3D地质模型）上的实验验证了方法的可行性，并通过PCA、t-SNE和TDA分析了潜在空间结构。

Conclusion: 图基变分自编码器为地质建模提供了一种灵活且可控的解决方案，能够有效处理地质不确定性。

Abstract: The graph-based variational autoencoder represents an architecture that can
handle the uncertainty of different geological scenarios, such as depositional
or structural, through the concept of a lowerdimensional latent space. The main
difference from recent studies is utilisation of a graph-based approach in
reservoir modelling instead of the more traditional lattice-based deep learning
methods. We provide a solution to implicitly control the geological realism
through the latent variables of a generative model and Geodesic metrics. Our
experiments of AHM with synthetic dataset that consists of 3D realisations of
channelised geological representations with two distinct scenarios with one and
two channels shows the viability of the approach. We offer in-depth analysis of
the latent space using tools such as PCA, t-SNE, and TDA to illustrate its
structure.

</details>


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [622] [LNN-powered Fluid Antenna Multiple Access](https://arxiv.org/abs/2507.08821)
*Pedro D. Alvim,Hugerles S. Silva,Ugo S. Dias,Osamah S. Badarneh,Felipe A. P. Figueiredo,Rausley A. A. de Souza*

Main category: eess.SP

TL;DR: 将流体天线系统中的端口选择问题建模为多标签分类任务，利用液态神经网络（LNNs）优化端口选择，并通过超参数调优提升性能。


<details>
  <summary>Details</summary>
Motivation: 解决流体天线多址接入场景下端口选择问题，优化信号干扰噪声比。

Method: 将端口选择问题建模为多标签分类任务，使用液态神经网络（LNNs）预测最优端口，并结合α-μ衰落模型进行优化。

Result: 提出的方法比现有方法具有更低的中断概率。

Conclusion: 通过液态神经网络和超参数优化，流体天线系统的端口选择性能得到显著提升。

Abstract: Fluid antenna systems represent an innovative approach in wireless
communication, recently applied in multiple access to optimize the
signal-to-interference-plus-noise ratio through port selection. This letter
frames the port selection problem as a multi-label classification task for the
first time, improving best-port selection with limited port observations. We
address this challenge by leveraging liquid neural networks (LNNs) to predict
the optimal port under emerging fluid antenna multiple access scenarios
alongside a more general $\alpha$-$\mu$ fading model. We also apply
hyperparameter optimization to refine LNN architectures for different
observation scenarios. Our approach yields lower outage probability values than
existing methods.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [623] [Balancing Semantic Relevance and Engagement in Related Video Recommendations](https://arxiv.org/abs/2507.09403)
*Amit Jaspal,Feng Zhang,Wei Chang,Sumit Kumar,Yubo Wang,Roni Mittleman,Qifan Wang,Weize Mao*

Main category: cs.IR

TL;DR: 论文提出了一种多目标检索框架，通过改进标准双塔模型，平衡语义相关性和用户参与度，解决了协同过滤推荐中语义连贯性不足和流行度偏差的问题。


<details>
  <summary>Details</summary>
Motivation: 协同过滤推荐常因共参与信号驱动导致语义连贯性差和流行度偏差，需改进。

Method: 采用多任务学习优化共参与和语义相关性，融合多模态内容特征，并通过逆倾向加权减轻流行度偏差。

Result: 语义相关性提升（主题匹配率从51%到63%），流行视频推荐减少13.8%，用户参与度提升0.04%。

Conclusion: 该方法成功实现了更好的语义连贯性、平衡的参与度和实际可扩展性。

Abstract: Related video recommendations commonly use collaborative filtering (CF)
driven by co-engagement signals, often resulting in recommendations lacking
semantic coherence and exhibiting strong popularity bias. This paper introduces
a novel multi-objective retrieval framework, enhancing standard two-tower
models to explicitly balance semantic relevance and user engagement. Our
approach uniquely combines: (a) multi-task learning (MTL) to jointly optimize
co-engagement and semantic relevance, explicitly prioritizing topical
coherence; (b) fusion of multimodal content features (textual and visual
embeddings) for richer semantic understanding; and (c) off-policy correction
(OPC) via inverse propensity weighting to effectively mitigate popularity bias.
Evaluation on industrial-scale data and a two-week live A/B test reveals our
framework's efficacy. We observed significant improvements in semantic
relevance (from 51% to 63% topic match rate), a reduction in popular item
distribution (-13.8% popular video recommendations), and a +0.04% improvement
in our topline user engagement metric. Our method successfully achieves better
semantic coherence, balanced engagement, and practical scalability for
real-world deployment.

</details>


### [624] [Overview of the TREC 2023 deep learning track](https://arxiv.org/abs/2507.08890)
*Nick Craswell,Bhaskar Mitra,Emine Yilmaz,Hossein A. Rahmani,Daniel Campos,Jimmy Lin,Ellen M. Voorhees,Ian Soboroff*

Main category: cs.IR

TL;DR: TREC Deep Learning track第五年研究，基于MS MARCO数据集，采用LLM提示方法超越往年最佳方法，合成查询评估结果与人工查询一致。


<details>
  <summary>Details</summary>
Motivation: 通过改进测试集设计和引入合成查询，提升排名的难度和改进空间，验证LLM提示方法的有效性。

Method: 重复去年设计，使用MS MARCO v2数据集，引入T5和GPT-4生成的合成查询，评估LLM提示方法。

Result: LLM提示方法优于往年最佳方法，合成查询与人工查询评估结果一致（τ=0.8487），未发现明显偏差。

Conclusion: LLM提示方法在排名任务中表现优异，合成查询可作为有效补充，未来研究可进一步探索提示方法。

Abstract: This is the fifth year of the TREC Deep Learning track. As in previous years,
we leverage the MS MARCO datasets that made hundreds of thousands of
human-annotated training labels available for both passage and document ranking
tasks. We mostly repeated last year's design, to get another matching test set,
based on the larger, cleaner, less-biased v2 passage and document set, with
passage ranking as primary and document ranking as a secondary task (using
labels inferred from passage). As we did last year, we sample from MS MARCO
queries that were completely held out, unused in corpus construction, unlike
the test queries in the first three years. This approach yields a more
difficult test with more headroom for improvement. Alongside the usual MS MARCO
(human) queries from MS MARCO, this year we generated synthetic queries using a
fine-tuned T5 model and using a GPT-4 prompt.
  The new headline result this year is that runs using Large Language Model
(LLM) prompting in some way outperformed runs that use the "nnlm" approach,
which was the best approach in the previous four years. Since this is the last
year of the track, future iterations of prompt-based ranking can happen in
other tracks. Human relevance assessments were applied to all query types, not
just human MS MARCO queries. Evaluation using synthetic queries gave similar
results to human queries, with system ordering agreement of $\tau=0.8487$.
However, human effort was needed to select a subset of the synthetic queries
that were usable. We did not see clear evidence of bias, where runs using GPT-4
were favored when evaluated using synthetic GPT-4 queries, or where runs using
T5 were favored when evaluated on synthetic T5 queries.

</details>


### [625] [DS@GT at Touché: Large Language Models for Retrieval-Augmented Debate](https://arxiv.org/abs/2507.09090)
*Anthony Miyaguchi,Conor Johnston,Aaryan Potdar*

Main category: cs.IR

TL;DR: 研究探讨了大型语言模型（LLMs）在辩论中的表现，包括其辩论能力和对辩论内容的评估能力。


<details>
  <summary>Details</summary>
Motivation: 探索LLMs在结构化辩论中的表现及其对辩论内容的评估能力。

Method: 使用六个公开可用的LLMs模型，通过检索增强的辩论和评估任务，测量质量、数量、方式和相关性四个指标。

Result: LLMs在辩论中表现良好，但回答冗长，评估一致性高。

Conclusion: LLMs在辩论任务中具有潜力，但需优化其回答的简洁性。

Abstract: Large Language Models (LLMs) demonstrate strong conversational abilities. In
this Working Paper, we study them in the context of debating in two ways: their
ability to perform in a structured debate along with a dataset of arguments to
use and their ability to evaluate utterances throughout the debate. We deploy
six leading publicly available models from three providers for the
Retrieval-Augmented Debate and Evaluation. The evaluation is performed by
measuring four key metrics: Quality, Quantity, Manner, and Relation. Throughout
this task, we found that although LLMs perform well in debates when given
related arguments, they tend to be verbose in responses yet consistent in
evaluation. The accompanying source code for this paper is located at
https://github.com/dsgt-arc/touche-2025-rad.

</details>


### [626] [GraphRunner: A Multi-Stage Framework for Efficient and Accurate Graph-Based Retrieval](https://arxiv.org/abs/2507.08945)
*Savini Kashmira,Jayanaka L. Dantanarayana,Krisztián Flautner,Lingjia Tang,Jason Mars*

Main category: cs.IR

TL;DR: GraphRunner是一个新颖的基于图的检索框架，通过规划、验证和执行三阶段显著减少LLM推理错误和幻觉，性能提升10-50%，效率更高。


<details>
  <summary>Details</summary>
Motivation: 传统RAG方法在处理结构化、互联数据集（如知识图谱）时表现不佳，现有迭代方法易受LLM推理错误和幻觉影响。

Method: GraphRunner采用三阶段（规划、验证、执行）和多跳探索，生成整体遍历计划并验证以减少错误。

Result: 在GRBench数据集上，GraphRunner性能提升10-50%，推理成本降低3.0-12.9倍，响应时间减少2.5-7.1倍。

Conclusion: GraphRunner在基于图的检索任务中更稳健高效，显著优于现有方法。

Abstract: Conventional Retrieval Augmented Generation (RAG) approaches are common in
text-based applications. However, they struggle with structured, interconnected
datasets like knowledge graphs, where understanding underlying relationships is
crucial for accurate retrieval. A common direction in graph-based retrieval
employs iterative, rule-based traversal guided by Large Language Models (LLMs).
Such existing iterative methods typically combine reasoning with single hop
traversal at each step, making them vulnerable to LLM reasoning errors and
hallucinations that ultimately hinder the retrieval of relevant information.
  To address these limitations, we propose GraphRunner, a novel graph-based
retrieval framework that operates in three distinct stages: planning,
verification, and execution. This introduces high-level traversal actions that
enable multi-hop exploration in a single step. It also generates a holistic
traversal plan, which is verified against the graph structure and pre-defined
traversal actions, reducing reasoning errors and detecting hallucinations
before execution. GraphRunner significantly reduces LLM reasoning errors and
detects hallucinations through validation. Our evaluation using the GRBench
dataset shows that GraphRunner consistently outperforms existing approaches,
achieving 10-50% performance improvements over the strongest baseline while
reducing inference cost by 3.0-12.9x and response generation time by 2.5-7.1x,
making it significantly more robust and efficient for graph-based retrieval
tasks.

</details>


### [627] [MixLoRA-DSI: Dynamically Expandable Mixture-of-LoRA Experts for Rehearsal-Free Generative Retrieval over Dynamic Corpora](https://arxiv.org/abs/2507.09924)
*Tuan-Luc Huynh,Thuy-Trang Vu,Weiqing Wang,Trung Le,Dragan Gašević,Yuan-Fang Li,Thanh-Toan Do*

Main category: cs.IR

TL;DR: MixLoRA-DSI是一种新框架，通过结合可扩展的LoRA专家混合和层间OOD驱动扩展策略，解决了生成检索中模型索引持续更新的挑战。


<details>
  <summary>Details</summary>
Motivation: 解决生成检索中模型索引持续更新时计算成本高的问题。

Method: 结合可扩展的LoRA专家混合和层间OOD驱动扩展策略，选择性引入新专家以减少参数增长。

Result: 在NQ320k和MS MARCO Passage上表现优于全模型更新基线，参数开销小且训练成本低。

Conclusion: MixLoRA-DSI是一种高效且经济的解决方案，适用于资源受限环境下的模型更新。

Abstract: Continually updating model-based indexes in generative retrieval with new
documents remains challenging, as full retraining is computationally expensive
and impractical under resource constraints. We propose MixLoRA-DSI, a novel
framework that combines an expandable mixture of Low-Rank Adaptation experts
with a layer-wise out-of-distribution (OOD)-driven expansion strategy. Instead
of allocating new experts for each new corpus, our proposed expansion strategy
enables sublinear parameter growth by selectively introducing new experts only
when significant number of OOD documents are detected. Experiments on NQ320k
and MS MARCO Passage demonstrate that MixLoRA-DSI outperforms full-model update
baselines, with minimal parameter overhead and substantially lower training
costs.

</details>


### [628] [PRISM: Fine-Grained Paper-to-Paper Retrieval with Multi-Aspect-Aware Query Optimization](https://arxiv.org/abs/2507.10057)
*Sangwoo Park,Jinheon Baek,Soyeong Jeong,Sung Ju Hwang*

Main category: cs.IR

TL;DR: PRISM是一种新的文档到文档检索方法，通过多粒度表示和匹配提升性能。


<details>
  <summary>Details</summary>
Motivation: 解决传统方法仅依赖摘要稀疏信息的问题，提出更全面的文档表示方法。

Method: 将查询和候选论文分解为多视图嵌入，并匹配其多维度相似性。

Result: PRISM性能平均提升4.3%。

Conclusion: PRISM通过多粒度表示显著提升检索效果。

Abstract: Scientific paper retrieval, particularly framed as document-to-document
retrieval, aims to identify relevant papers in response to a long-form query
paper, rather than a short query string. Previous approaches to this task have
focused on abstracts, embedding them into dense vectors as surrogates for full
documents and calculating similarity across them, although abstracts provide
only sparse and high-level summaries. To address this, we propose PRISM, a
novel document-to-document retrieval method that introduces multiple,
fine-grained representations for both the query and candidate papers. In
particular, each query paper is decomposed into multiple aspect-specific views
and individually embedded, which are then matched against candidate papers
similarity segmented to consider their multifaceted dimensions. Moreover, we
present SciFullBench, a novel benchmark in which the complete and segmented
context of full papers for both queries and candidates is available. Then,
experimental results show that PRISM improves performance by an average of 4.3%
over existing retrieval baselines.

</details>


### [629] [Identifying Offline Metrics that Predict Online Impact: A Pragmatic Strategy for Real-World Recommender Systems](https://arxiv.org/abs/2507.09566)
*Timo Wilm,Philipp Normann*

Main category: cs.IR

TL;DR: 论文提出了一种通过Pareto前沿近似方法，识别与在线影响一致的离线指标的策略，适用于多测试组在线实验，并在电商平台上验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 解决推荐系统中离线指标与在线性能预测的可靠性问题，以支持数据驱动的决策。

Method: 采用Pareto前沿近似策略，模型无关的神经网络框架，支持多测试组在线实验。

Result: 在OTTO电商平台的实验中，发现离线指标与点击率、转化率和销量显著相关。

Conclusion: 该方法为行业从业者提供了理解离线与在线指标关系的实用工具。

Abstract: A critical challenge in recommender systems is to establish reliable
relationships between offline and online metrics that predict real-world
performance. Motivated by recent advances in Pareto front approximation, we
introduce a pragmatic strategy for identifying offline metrics that align with
online impact. A key advantage of this approach is its ability to
simultaneously serve multiple test groups, each with distinct offline
performance metrics, in an online experiment controlled by a single model. The
method is model-agnostic for systems with a neural network backbone, enabling
broad applicability across architectures and domains. We validate the strategy
through a large-scale online experiment in the field of session-based
recommender systems on the OTTO e-commerce platform. The online experiment
identifies significant alignments between offline metrics and real-word
click-through rate, post-click conversion rate and units sold. Our strategy
provides industry practitioners with a valuable tool for understanding
offline-to-online metric relationships and making informed, data-driven
decisions.

</details>


<div id='physics.chem-ph'></div>

# physics.chem-ph [[Back]](#toc)

### [630] [DiffNMR: Diffusion Models for Nuclear Magnetic Resonance Spectra Elucidation](https://arxiv.org/abs/2507.08854)
*Qingsong Yang,Binglan Wu,Xuwei Liu,Bo Chen,Wei Li,Gen Long,Xin Chen,Mingjun Xiao*

Main category: physics.chem-ph

TL;DR: DiffNMR是一个基于条件离散扩散模型的新框架，用于从NMR光谱中解析分子结构，通过扩散生成过程迭代优化分子图，解决了自回归方法的误差累积问题。


<details>
  <summary>Details</summary>
Motivation: NMR光谱解析分子结构具有挑战性，传统方法难以处理复杂的光谱数据和广阔的化学空间。

Method: DiffNMR采用两阶段预训练策略，结合扩散自编码器和对比学习，并在推理阶段引入检索初始化和相似性过滤，使用RBF编码器处理化学位移。

Result: 实验表明，DiffNMR在NMR光谱解析分子结构方面表现优异，提供了高效且稳健的自动化分子分析解决方案。

Conclusion: DiffNMR为分子结构解析提供了一种创新且高效的方法，解决了传统方法的局限性。

Abstract: Nuclear Magnetic Resonance (NMR) spectroscopy is a central characterization
method for molecular structure elucidation, yet interpreting NMR spectra to
deduce molecular structures remains challenging due to the complexity of
spectral data and the vastness of the chemical space. In this work, we
introduce DiffNMR, a novel end-to-end framework that leverages a conditional
discrete diffusion model for de novo molecular structure elucidation from NMR
spectra. DiffNMR refines molecular graphs iteratively through a diffusion-based
generative process, ensuring global consistency and mitigating error
accumulation inherent in autoregressive methods. The framework integrates a
two-stage pretraining strategy that aligns spectral and molecular
representations via diffusion autoencoder (Diff-AE) and contrastive learning,
the incorporation of retrieval initialization and similarity filtering during
inference, and a specialized NMR encoder with radial basis function (RBF)
encoding for chemical shifts, preserving continuity and chemical correlation.
Experimental results demonstrate that DiffNMR achieves competitive performance
for NMR-based structure elucidation, offering an efficient and robust solution
for automated molecular analysis.

</details>


### [631] [Accurate generation of chemical reaction transition states by conditional flow matching](https://arxiv.org/abs/2507.10530)
*Ping Tuo,Jiale Chen,Ju Li*

Main category: physics.chem-ph

TL;DR: TS-GEN是一种生成模型，通过单次确定性过程直接从高斯先验生成过渡态结构，显著提高了精度和速度。


<details>
  <summary>Details</summary>
Motivation: 过渡态结构在化学反应中至关重要，但实验难以捕捉，传统计算方法成本高且耗时。

Method: TS-GEN利用条件流匹配生成模型，通过最优传输路径将噪声映射到真实过渡态结构，避免了迭代优化。

Result: TS-GEN实现了0.004 Å的RMSD和1.019 kcal/mol的平均能垒误差，87%的生成结构满足化学精度要求。

Conclusion: TS-GEN结合高精度、快速和广泛适用性，为复杂反应网络的高通量探索提供了新工具。

Abstract: Transition state (TS) structures define the critical geometries and energy
barriers underlying chemical reactivity, yet their fleeting nature renders them
experimentally elusive and drives the reliance on costly, high-throughput
density functional theory (DFT) calculations. Here, we introduce TS-GEN, a
conditional flow-matching generative model that maps samples from a simple
Gaussian prior directly to transition-state saddle-point geometries in a
single, deterministic pass. By embedding both reactant and product
conformations as conditioning information, TS-GEN learns to transport latent
noise to true TS structures via an optimal-transport path, effectively
replacing the iterative optimization common in nudged-elastic band or
string-method algorithms. TS-GEN delivers unprecedented accuracy, achieving a
root-mean-square deviation of $0.004\ \rm{\mathring{A}}$ (vs. $0.103\
\rm{\mathring{A}}$ for prior state-of-the-art) and a mean barrier-height error
of $1.019\ {\rm kcal/mol}$ (vs. $2.864\ {\rm kcal/mol}$), while requiring only
$0.06\ {\rm s}$ GPU time per inference. Over 87% of generated TSs meet
chemical-accuracy criteria ($<1.58\ {\rm kcal/mol}$ error), substantially
outpacing existing methods. TS-GEN also exhibits strong transferability to
out-of-distribution reactions from a larger database. By uniting sub-angstrom
precision, sub-second speed, and broad applicability, TS-GEN will be highly
useful for high-throughput exploration of complex reaction networks, paving the
way to the exploration of novel chemical reaction mechanisms.

</details>


<div id='physics.flu-dyn'></div>

# physics.flu-dyn [[Back]](#toc)

### [632] [Physics-Based Machine Learning Closures and Wall Models for Hypersonic Transition-Continuum Boundary Layer Predictions](https://arxiv.org/abs/2507.08986)
*Ashish S. Nair,Narendra Singh,Marco Panesi,Justin Sirignano,Jonathan F. MacArt*

Main category: physics.flu-dyn

TL;DR: 论文提出了一种基于物理约束的机器学习框架，用于改进稀薄高超声速流动的建模，解决了传统连续介质模型在非平衡状态下的局限性。


<details>
  <summary>Details</summary>
Motivation: 传统Navier-Stokes-Fourier模型在过渡-连续介质区域（Knudsen数0.1-10）无法准确预测非平衡效应，如速度滑移、温度跳跃和激波结构偏差。

Method: 采用深度学习偏微分方程模型（DPMs）嵌入控制方程中，通过伴随优化训练，并结合基于偏态高斯分布的壁面模型替代经验滑移条件。

Result: 结果表明，无迹各向异性粘度模型与偏态高斯分布壁面模型显著提高了高马赫数和高Knudsen数区域的精度。

Conclusion: 该研究为传统连续介质方法无效的流动区域提供了数据驱动且物理一致的建模策略。

Abstract: Modeling rarefied hypersonic flows remains a fundamental challenge due to the
breakdown of classical continuum assumptions in the transition-continuum
regime, where the Knudsen number ranges from approximately 0.1 to 10.
Conventional Navier-Stokes-Fourier (NSF) models with empirical slip-wall
boundary conditions fail to accurately predict nonequilibrium effects such as
velocity slip, temperature jump, and shock structure deviations. We develop a
physics-constrained machine learning framework that augments transport models
and boundary conditions to extend the applicability of continuum solvers in
nonequilibrium hypersonic regimes. We employ deep learning PDE models (DPMs)
for the viscous stress and heat flux embedded in the governing PDEs and trained
via adjoint-based optimization. We evaluate these for two-dimensional
supersonic flat-plate flows across a range of Mach and Knudsen numbers.
Additionally, we introduce a wall model based on a mixture of skewed Gaussian
approximations of the particle velocity distribution function. This wall model
replaces empirical slip conditions with physically informed, data-driven
boundary conditions for the streamwise velocity and wall temperature. Our
results show that a trace-free anisotropic viscosity model, paired with the
skewed-Gaussian distribution function wall model, achieves significantly
improved accuracy, particularly at high-Mach and high-Knudsen number regimes.
Strategies such as parallel training across multiple Knudsen numbers and
inclusion of high-Mach data during training are shown to enhance model
generalization. Increasing model complexity yields diminishing returns for
out-of-sample cases, underscoring the need to balance degrees of freedom and
overfitting. This work establishes data-driven, physics-consistent strategies
for improving hypersonic flow modeling for regimes in which conventional
continuum approaches are invalid.

</details>


### [633] [WellPINN: Accurate Well Representation for Transient Fluid Pressure Diffusion in Subsurface Reservoirs with Physics-Informed Neural Networks](https://arxiv.org/abs/2507.09330)
*Linus Walter,Qingkai Kong,Sara Hanson-Hedgecock,Víctor Vilarrasa*

Main category: physics.flu-dyn

TL;DR: WellPINN是一种结合多个顺序训练的PINN模型的工作流，用于准确表示井的流体压力，特别是在注入早期阶段。


<details>
  <summary>Details</summary>
Motivation: 现有基于PINN的研究在捕捉井附近流体压力时面临挑战，尤其是在注入初期。

Method: 提出WellPINN工作流，通过分解域为逐步缩小的子域并减小等效井半径，顺序训练叠加网络。

Result: 结果表明，该方法首次实现了整个注入期间从注入速率准确推断流体压力。

Conclusion: WellPINN显著提升了PINN在逆建模和操作场景模拟中的潜力。

Abstract: Accurate representation of wells is essential for reliable reservoir
characterization and simulation of operational scenarios in subsurface flow
models. Physics-informed neural networks (PINNs) have recently emerged as a
promising method for reservoir modeling, offering seamless integration of
monitoring data and governing physical equations. However, existing PINN-based
studies face major challenges in capturing fluid pressure near wells,
particularly during the early stage after injection begins. To address this, we
propose WellPINN, a modeling workflow that combines the outputs of multiple
sequentially trained PINN models to accurately represent wells. This workflow
iteratively approximates the radius of the equivalent well to match the actual
well dimensions by decomposing the domain into stepwise shrinking subdomains
with a simultaneously reducing equivalent well radius. Our results demonstrate
that sequential training of superimposing networks around the pumping well is
the first workflow that focuses on accurate inference of fluid pressure from
pumping rates throughout the entire injection period, significantly advancing
the potential of PINNs for inverse modeling and operational scenario
simulations. All data and code for this paper will be made openly available at
https://github.com/linuswalter/WellPINN.

</details>


<div id='astro-ph.IM'></div>

# astro-ph.IM [[Back]](#toc)

### [634] [Bridging Literature and the Universe Via A Multi-Agent Large Language Model System](https://arxiv.org/abs/2507.08958)
*Xiaowen Zhang,Zhenyu Bi,Xuan Wang,Tiziana Di Matteo,Rupert A. C. Croft*

Main category: astro-ph.IM

TL;DR: SimAgents是一个多智能体系统，旨在自动化从文献中提取宇宙学模拟参数并进行初步分析，以提高研究效率。


<details>
  <summary>Details</summary>
Motivation: 解决物理学家在复杂宇宙学模拟中从文献中提取参数和生成脚本时的耗时和易错问题。

Method: 利用专门的大型语言模型（LLM）智能体进行物理推理、模拟软件验证和工具执行，通过结构化通信协作确保参数的有效性和一致性。

Result: 在包含40多个模拟的数据集上表现出色，证明了SimAgents的有效性和加速科学研究的潜力。

Conclusion: SimAgents为宇宙学研究提供了一种高效、自动化的解决方案，有望加速科学研究的进程。

Abstract: As cosmological simulations and their associated software become increasingly
complex, physicists face the challenge of searching through vast amounts of
literature and user manuals to extract simulation parameters from dense
academic papers, each using different models and formats. Translating these
parameters into executable scripts remains a time-consuming and error-prone
process. To improve efficiency in physics research and accelerate the
cosmological simulation process, we introduce SimAgents, a multi-agent system
designed to automate both parameter configuration from the literature and
preliminary analysis for cosmology research. SimAgents is powered by
specialized LLM agents capable of physics reasoning, simulation software
validation, and tool execution. These agents collaborate through structured
communication, ensuring that extracted parameters are physically meaningful,
internally consistent, and software-compliant. We also construct a cosmological
parameter extraction evaluation dataset by collecting over 40 simulations in
published papers from Arxiv and leading journals that cover diverse simulation
types. Experiments on the dataset demonstrate a strong performance of
SimAgents, highlighting its effectiveness and potential to accelerate
scientific research for physicists. Our demonstration video is available at:
https://youtu.be/w1zLpm_CaWA. The complete system and dataset are publicly
available at https://github.com/xwzhang98/SimAgents.

</details>


<div id='cond-mat.dis-nn'></div>

# cond-mat.dis-nn [[Back]](#toc)

### [635] [Dynamical stability for dense patterns in discrete attractor neural networks](https://arxiv.org/abs/2507.10383)
*Uri Cohen,Máté Lengyel*

Main category: cond-mat.dis-nn

TL;DR: 论文提出了一种理论，用于分析具有分级神经活动和噪声的网络中离散固定点的局部稳定性，并确定了临界负载。


<details>
  <summary>Details</summary>
Motivation: 研究神经网络中离散吸引子的动态稳定性，以理解生物记忆的机制。

Method: 通过直接分析雅可比矩阵谱的总体和异常值，推导了固定点的稳定性条件。

Result: 发现所有固定点在临界负载以下都是稳定的，临界负载取决于固定点中神经活动的统计特性和单神经元激活函数。

Conclusion: 研究强调了阈值线性激活和稀疏模式的计算优势。

Abstract: Neural networks storing multiple discrete attractors are canonical models of
biological memory. Previously, the dynamical stability of such networks could
only be guaranteed under highly restrictive conditions. Here, we derive a
theory of the local stability of discrete fixed points in a broad class of
networks with graded neural activities and in the presence of noise. By
directly analyzing the bulk and outliers of the Jacobian spectrum, we show that
all fixed points are stable below a critical load that is distinct from the
classical \textit{critical capacity} and depends on the statistics of neural
activities in the fixed points as well as the single-neuron activation
function. Our analysis highlights the computational benefits of
threshold-linear activation and sparse-like patterns.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [636] [Automatic Contouring of Spinal Vertebrae on X-Ray using a Novel Sandwich U-Net Architecture](https://arxiv.org/abs/2507.09158)
*Sunil Munthumoduku Krishna Murthy,Kumar Rajamani,Srividya Tirunellai Rajamani,Yupei Li,Qiyang Sun,Bjoern W. Schuller*

Main category: eess.IV

TL;DR: 提出一种改进的U-Net结构，用于从X射线图像中准确分割胸椎，提升分割精度和效率。


<details>
  <summary>Details</summary>
Motivation: 传统手动椎骨轮廓提取耗时且易出错，自动化方法能提高效率和准确性。

Method: 采用带有双重激活函数的“三明治”U-Net结构，优化了椎骨分割性能。

Result: 相比基线U-Net模型，Dice分数提升了4.1%，实现了更可靠的椎骨轮廓提取。

Conclusion: 改进的U-Net结构在椎骨分割任务中表现出更高的准确性和效率。

Abstract: In spinal vertebral mobility disease, accurately extracting and contouring
vertebrae is essential for assessing mobility impairments and monitoring
variations during flexion-extension movements. Precise vertebral contouring
plays a crucial role in surgical planning; however, this process is
traditionally performed manually by radiologists or surgeons, making it
labour-intensive, time-consuming, and prone to human error. In particular,
mobility disease analysis requires the individual contouring of each vertebra,
which is both tedious and susceptible to inconsistencies. Automated methods
provide a more efficient alternative, enabling vertebra identification,
segmentation, and contouring with greater accuracy and reduced time
consumption. In this study, we propose a novel U-Net variation designed to
accurately segment thoracic vertebrae from anteroposterior view on X-Ray
images. Our proposed approach, incorporating a ``sandwich" U-Net structure with
dual activation functions, achieves a 4.1\% improvement in Dice score compared
to the baseline U-Net model, enhancing segmentation accuracy while ensuring
reliable vertebral contour extraction.

</details>


### [637] [PanoDiff-SR: Synthesizing Dental Panoramic Radiographs using Diffusion and Super-resolution](https://arxiv.org/abs/2507.09227)
*Sanyam Jain,Bruna Neves de Freitas,Andreas Basse-OConnor,Alexandros Iosifidis,Ruben Pauwels*

Main category: eess.IV

TL;DR: 该论文提出了一种结合扩散生成（PanoDiff）和超分辨率（SR）的方法，用于生成高质量合成牙科全景X光片（PRs）。实验结果显示合成图像与真实图像在质量上接近，临床专家区分准确率为68.5%。


<details>
  <summary>Details</summary>
Motivation: 解决医学图像数据稀缺问题，为人工智能研究和教育提供合成数据集。

Method: 结合扩散生成（生成低分辨率种子）和超分辨率（提升分辨率），使用先进的transformer模型学习局部-全局关系。

Result: 合成高分辨率图像与真实图像的Frechet inception距离为40.69，临床专家区分准确率为68.5%。

Conclusion: 该方法能生成高质量的合成医学图像，可用于研究和教育。

Abstract: There has been increasing interest in the generation of high-quality,
realistic synthetic medical images in recent years. Such synthetic datasets can
mitigate the scarcity of public datasets for artificial intelligence research,
and can also be used for educational purposes. In this paper, we propose a
combination of diffusion-based generation (PanoDiff) and Super-Resolution (SR)
for generating synthetic dental panoramic radiographs (PRs). The former
generates a low-resolution (LR) seed of a PR (256 X 128) which is then
processed by the SR model to yield a high-resolution (HR) PR of size 1024 X
512. For SR, we propose a state-of-the-art transformer that learns local-global
relationships, resulting in sharper edges and textures. Experimental results
demonstrate a Frechet inception distance score of 40.69 between 7243 real and
synthetic images (in HR). Inception scores were 2.55, 2.30, 2.90 and 2.98 for
real HR, synthetic HR, real LR and synthetic LR images, respectively. Among a
diverse group of six clinical experts, all evaluating a mixture of 100
synthetic and 100 real PRs in a time-limited observation, the average accuracy
in distinguishing real from synthetic images was 68.5% (with 50% corresponding
to random guessing).

</details>


### [638] [AI-Enhanced Pediatric Pneumonia Detection: A CNN-Based Approach Using Data Augmentation and Generative Adversarial Networks (GANs)](https://arxiv.org/abs/2507.09759)
*Abdul Manaf,Nimra Mughal*

Main category: eess.IV

TL;DR: 该研究提出了一种基于机器学习的儿科胸部肺炎分类系统，利用CNN模型和GAN生成的数据增强技术，显著提高了肺炎诊断的准确性和效率。


<details>
  <summary>Details</summary>
Motivation: 肺炎是五岁以下儿童死亡的主要原因，需要准确的胸部X光诊断。本研究旨在通过深度学习技术辅助医疗专业人员提高诊断效率和准确性。

Method: 研究使用5,863张标记的儿童胸部X光图像训练CNN模型，并通过数据增强和GAN生成合成图像解决数据不足和类别不平衡问题。

Result: 系统在结合原始、增强和GAN生成数据后表现最佳，并通过Flask应用实现实时分类。

Conclusion: 该研究表明深度学习和GAN在儿科肺炎分类中具有潜力，尤其适用于资源有限的临床环境。

Abstract: Pneumonia is a leading cause of mortality in children under five, requiring
accurate chest X-ray diagnosis. This study presents a machine learning-based
Pediatric Chest Pneumonia Classification System to assist healthcare
professionals in diagnosing pneumonia from chest X-ray images. The CNN-based
model was trained on 5,863 labeled chest X-ray images from children aged 0-5
years from the Guangzhou Women and Children's Medical Center. To address
limited data, we applied augmentation techniques (rotation, zooming, shear,
horizontal flipping) and employed GANs to generate synthetic images, addressing
class imbalance. The system achieved optimal performance using combined
original, augmented, and GAN-generated data, evaluated through accuracy and F1
score metrics. The final model was deployed via a Flask web application,
enabling real-time classification with probability estimates. Results
demonstrate the potential of deep learning and GANs in improving diagnostic
accuracy and efficiency for pediatric pneumonia classification, particularly
valuable in resource-limited clinical settings
https://github.com/AbdulManaf12/Pediatric-Chest-Pneumonia-Classification

</details>


### [639] [Multi-omic Prognosis of Alzheimer's Disease with Asymmetric Cross-Modal Cross-Attention Network](https://arxiv.org/abs/2507.08855)
*Yang Ming,Jiang Shi Zhong,Zhou Su Juan*

Main category: eess.IV

TL;DR: 本文提出了一种新型深度学习算法框架，通过融合多模态医疗数据（如PET、MRI、遗传数据和临床数据），利用不对称跨模态交叉注意力机制，显著提升了阿尔茨海默病（AD）诊断的准确性。


<details>
  <summary>Details</summary>
Motivation: 传统卷积神经网络和简单特征拼接方法在多模态数据融合中效果不佳，无法充分利用互补信息，且容易丢失关键信息。深度学习技术的发展为解决这一问题提供了新思路。

Method: 采用不对称跨模态交叉注意力机制，有效捕捉不同模态数据间的关键交互特征，并与传统单模态和多模态深度学习模型进行对比。

Result: 算法模型在测试集上达到了94.88%的准确率。

Conclusion: 不对称跨模态交叉注意力机制在多模态数据融合中表现优异，为AD诊断提供了高效工具。

Abstract: Alzheimer's Disease (AD) is an irreversible neurodegenerative disease
characterized by progressive cognitive decline as its main symptom. In the
research field of deep learning-assisted diagnosis of AD, traditional
convolutional neural networks and simple feature concatenation methods fail to
effectively utilize the complementary information between multimodal data, and
the simple feature concatenation approach is prone to cause the loss of key
information during the process of modal fusion. In recent years, the
development of deep learning technology has brought new possibilities for
solving the problem of how to effectively fuse multimodal features. This paper
proposes a novel deep learning algorithm framework to assist medical
professionals in AD diagnosis. By fusing medical multi-view information such as
brain fluorodeoxyglucose positron emission tomography (PET), magnetic resonance
imaging (MRI), genetic data, and clinical data, it can accurately detect the
presence of AD, Mild Cognitive Impairment (MCI), and Cognitively Normal (CN).
The innovation of the algorithm lies in the use of an asymmetric cross-modal
cross-attention mechanism, which can effectively capture the key information
features of the interactions between different data modal features. This paper
compares the asymmetric cross-modal cross-attention mechanism with the
traditional algorithm frameworks of unimodal and multimodal deep learning
models for AD diagnosis, and evaluates the importance of the asymmetric
cross-modal cross-attention mechanism. The algorithm model achieves an accuracy
of 94.88% on the test set.

</details>


### [640] [Interpretable Artificial Intelligence for Detecting Acute Heart Failure on Acute Chest CT Scans](https://arxiv.org/abs/2507.08952)
*Silas Nyboe Ørting,Kristina Miger,Anne Sophie Overgaard Olesen,Mikael Ploug Boesen,Michael Brun Andersen,Jens Petersen,Olav W. Nielsen,Marleen de Bruijne*

Main category: eess.IV

TL;DR: 开发了一种可解释的AI模型，用于在胸部CT中检测急性心力衰竭（AHF）的放射学特征，性能与胸科放射科医生相当。


<details>
  <summary>Details</summary>
Motivation: 由于放射科医生短缺，胸部CT扫描的解读常被延迟，而AI可以作为辅助工具提高诊断精度。

Method: 采用Boosted Trees模型，基于急性胸部CT扫描中分割的心脏和肺部结构测量数据预测AHF，并使用Shapley Additive explanations解释预测。

Result: 模型在独立测试集上的ROC曲线下面积为0.87，误分类分析显示部分错误源于初始放射学报告的不准确。

Conclusion: 开发的AI模型具有强区分性能，且预测过程透明，可支持决策。

Abstract: Introduction: Chest CT scans are increasingly used in dyspneic patients where
acute heart failure (AHF) is a key differential diagnosis. Interpretation
remains challenging and radiology reports are frequently delayed due to a
radiologist shortage, although flagging such information for emergency
physicians would have therapeutic implication. Artificial intelligence (AI) can
be a complementary tool to enhance the diagnostic precision. We aim to develop
an explainable AI model to detect radiological signs of AHF in chest CT with an
accuracy comparable to thoracic radiologists.
  Methods: A single-center, retrospective study during 2016-2021 at Copenhagen
University Hospital - Bispebjerg and Frederiksberg, Denmark. A Boosted Trees
model was trained to predict AHF based on measurements of segmented cardiac and
pulmonary structures from acute thoracic CT scans. Diagnostic labels for
training and testing were extracted from radiology reports. Structures were
segmented with TotalSegmentator. Shapley Additive explanations values were used
to explain the impact of each measurement on the final prediction.
  Results: Of the 4,672 subjects, 49% were female. The final model incorporated
twelve key features of AHF and achieved an area under the ROC of 0.87 on the
independent test set. Expert radiologist review of model misclassifications
found that 24 out of 64 (38%) false positives and 24 out of 61 (39%) false
negatives were actually correct model predictions, with the errors originating
from inaccuracies in the initial radiology reports.
  Conclusion: We developed an explainable AI model with strong discriminatory
performance, comparable to thoracic radiologists. The AI model's stepwise,
transparent predictions may support decision-making.

</details>


### [641] [VIP: Visual Information Protection through Adversarial Attacks on Vision-Language Models](https://arxiv.org/abs/2507.08982)
*Hanene F. Z. Brachemi Meftah,Wassim Hamidouche,Sid Ahmed Fezza,Olivier Déforges*

Main category: eess.IV

TL;DR: 该论文提出了一种针对视觉语言模型（VLMs）的隐私保护方法，通过选择性隐藏图像中的敏感区域（ROIs），防止模型访问隐私内容，同时保持图像其余部分的语义完整性。


<details>
  <summary>Details</summary>
Motivation: 随着视觉语言模型的广泛应用，用户隐私问题日益突出，尤其是模型可能无意中处理或暴露隐私视觉信息。

Method: 将隐私保护问题建模为对抗攻击问题，提出了一种新颖的攻击策略，选择性隐藏图像中的ROIs，避免破坏整体图像语义。

Result: 在LLaVA、Instruct-BLIP和BLIP2-T5三种先进VLMs上的实验表明，目标ROIs的检测率降低了98%，同时保持了图像全局语义的完整性。

Conclusion: 该研究为多模态模型的隐私保护提供了实用工具，推动了隐私意识的模型应用。

Abstract: Recent years have witnessed remarkable progress in developing Vision-Language
Models (VLMs) capable of processing both textual and visual inputs. These
models have demonstrated impressive performance, leading to their widespread
adoption in various applications. However, this widespread raises serious
concerns regarding user privacy, particularly when models inadvertently process
or expose private visual information. In this work, we frame the preservation
of privacy in VLMs as an adversarial attack problem. We propose a novel attack
strategy that selectively conceals information within designated Region Of
Interests (ROIs) in an image, effectively preventing VLMs from accessing
sensitive content while preserving the semantic integrity of the remaining
image. Unlike conventional adversarial attacks that often disrupt the entire
image, our method maintains high coherence in unmasked areas. Experimental
results across three state-of-the-art VLMs namely LLaVA, Instruct-BLIP, and
BLIP2-T5 demonstrate up to 98% reduction in detecting targeted ROIs, while
maintaining global image semantics intact, as confirmed by high similarity
scores between clean and adversarial outputs. We believe that this work
contributes to a more privacy conscious use of multimodal models and offers a
practical tool for further research, with the source code publicly available
at: https://github.com/hbrachemi/Vlm_defense-attack.

</details>


### [642] [prNet: Data-Driven Phase Retrieval via Stochastic Refinement](https://arxiv.org/abs/2507.09608)
*Mehmet Onurcan Kaya,Figen S. Oktem*

Main category: eess.IV

TL;DR: 提出了一种基于Langevin动力学的新型相位检索框架，通过后验采样平衡失真与感知质量。


<details>
  <summary>Details</summary>
Motivation: 传统方法过于关注像素级精度，而忽视了感知质量与失真之间的权衡。

Method: 结合随机采样、学习去噪和基于模型的更新，提出三种复杂度递增的变体，包括理论支持的Langevin推断、自适应噪声调度学习和并行重建采样。

Result: 在多个基准测试中实现了最先进的性能，兼顾保真度和感知质量。

Conclusion: 该框架通过理论驱动的采样方法，有效解决了相位检索中的感知-失真权衡问题。

Abstract: We propose a novel framework for phase retrieval that leverages Langevin
dynamics to enable efficient posterior sampling, yielding reconstructions that
explicitly balance distortion and perceptual quality. Unlike conventional
approaches that prioritize pixel-wise accuracy, our method navigates the
perception-distortion tradeoff through a principled combination of stochastic
sampling, learned denoising, and model-based updates. The framework comprises
three variants of increasing complexity, integrating theoretically grounded
Langevin inference, adaptive noise schedule learning, parallel reconstruction
sampling, and warm-start initialization from classical solvers. Extensive
experiments demonstrate that our method achieves state-of-the-art performance
across multiple benchmarks, both in terms of fidelity and perceptual quality.

</details>


### [643] [I2I-PR: Deep Iterative Refinement for Phase Retrieval using Image-to-Image Diffusion Models](https://arxiv.org/abs/2507.09609)
*Mehmet Onurcan Kaya,Figen S. Oktem*

Main category: eess.IV

TL;DR: 提出了一种基于图像到图像扩散框架的新相位检索方法，结合混合迭代技术和加速机制，显著提升了训练效率和重建质量。


<details>
  <summary>Details</summary>
Motivation: 传统相位检索算法对初始化和测量噪声敏感，扩散模型在图像重建任务中表现出色，因此探索其在相位检索中的应用。

Method: 采用混合输入输出和误差减少方法的混合迭代技术，结合加速机制生成初始估计，再通过图像到图像扩散框架迭代优化。

Result: 在训练效率和重建质量上均有显著提升，优于传统和现代方法。

Conclusion: 该方法在相位检索中表现出高效性和有效性，具有广泛应用潜力。

Abstract: Phase retrieval involves recovering a signal from intensity-only
measurements, crucial in many fields such as imaging, holography, optical
computing, crystallography, and microscopy. Although there are several
well-known phase retrieval algorithms, including classical iterative solvers,
the reconstruction performance often remains sensitive to initialization and
measurement noise. Recently, image-to-image diffusion models have gained
traction in various image reconstruction tasks, yielding significant
theoretical insights and practical breakthroughs. In this work, we introduce a
novel phase retrieval approach based on an image-to-image diffusion framework
called Inversion by Direct Iteration. Our method begins with an enhanced
initialization stage that leverages a hybrid iterative technique, combining the
Hybrid Input-Output and Error Reduction methods and incorporating a novel
acceleration mechanism to obtain a robust crude estimate. Then, it iteratively
refines this initial crude estimate using the learned image-to-image pipeline.
Our method achieves substantial improvements in both training efficiency and
reconstruction quality. Furthermore, our approach utilizes aggregation
techniques to refine quality metrics and demonstrates superior results compared
to both classical and contemporary techniques. This highlights its potential
for effective and efficient phase retrieval across various applications.

</details>


### [644] [Pre-trained Under Noise: A Framework for Robust Bone Fracture Detection in Medical Imaging](https://arxiv.org/abs/2507.09731)
*Robby Hoover,Nelly Elsayed,Zag ElSayed,Chengcheng Li*

Main category: eess.IV

TL;DR: 论文研究了预训练深度学习模型在X射线图像中分类骨骨折的鲁棒性，通过模拟不同设备质量条件测试了ResNet50、VGG16和EfficientNetv2模型，分析了噪声对模型性能的影响。


<details>
  <summary>Details</summary>
Motivation: 解决全球医疗资源不均问题，通过技术手段评估AI模型在不同条件下的表现。

Method: 使用ResNet50、VGG16和EfficientNetv2三种预训练模型，通过逐步添加噪声模拟图像质量下降，评估模型性能变化。

Result: 研究发现噪声显著影响模型性能，不同模型对噪声的鲁棒性存在差异。

Conclusion: 论文为评估AI模型在真实医疗环境中的表现提供了方法论框架，并揭示了预训练模型在不同条件下的适用性。

Abstract: Medical Imagings are considered one of the crucial diagnostic tools for
different bones-related diseases, especially bones fractures. This paper
investigates the robustness of pre-trained deep learning models for classifying
bone fractures in X-ray images and seeks to address global healthcare disparity
through the lens of technology. Three deep learning models have been tested
under varying simulated equipment quality conditions. ResNet50, VGG16 and
EfficientNetv2 are the three pre-trained architectures which are compared.
These models were used to perform bone fracture classification as images were
progressively degraded using noise. This paper specifically empirically studies
how the noise can affect the bone fractures detection and how the pre-trained
models performance can be changes due to the noise that affect the quality of
the X-ray images. This paper aims to help replicate real world challenges
experienced by medical imaging technicians across the world. Thus, this paper
establishes a methodological framework for assessing AI model degradation using
transfer learning and controlled noise augmentation. The findings provide
practical insight into how robust and generalizable different pre-trained deep
learning powered computer vision models can be when used in different contexts.

</details>


### [645] [Advanced U-Net Architectures with CNN Backbones for Automated Lung Cancer Detection and Segmentation in Chest CT Images](https://arxiv.org/abs/2507.09898)
*Alireza Golkarieha,Kiana Kiashemshakib,Sajjad Rezvani Boroujenic,Nasibeh Asadi Isakand*

Main category: eess.IV

TL;DR: 研究探讨了结合不同CNN骨干网络的U-Net架构在胸部CT图像中自动检测和分割肺癌的效果，结果显示U-Net与ResNet50结合在癌症分割中表现最佳，而U-Net与Xception结合的分类模型准确率高达99.1%。


<details>
  <summary>Details</summary>
Motivation: 解决临床环境中对准确诊断工具的需求，提升肺癌早期检测和分割的准确性。

Method: 使用CLAHE预处理832张胸部CT图像，开发了基于ResNet50、VGG16和Xception的U-Net模型进行分割，并评估了CNN分类器及混合模型（结合传统机器学习分类器）。

Result: U-Net与ResNet50在癌症分割中表现最佳（Dice: 0.9495，准确率: 0.9735），U-Net与Xception的分类模型准确率达99.1%。

Conclusion: 结合U-Net与先进CNN骨干网络的方法在肺癌CT扫描的分割和分类中表现优异，支持早期诊断和临床决策。

Abstract: This study investigates the effectiveness of U-Net architectures integrated
with various convolutional neural network (CNN) backbones for automated lung
cancer detection and segmentation in chest CT images, addressing the critical
need for accurate diagnostic tools in clinical settings. A balanced dataset of
832 chest CT images (416 cancerous and 416 non-cancerous) was preprocessed
using Contrast Limited Adaptive Histogram Equalization (CLAHE) and resized to
128x128 pixels. U-Net models were developed with three CNN backbones: ResNet50,
VGG16, and Xception, to segment lung regions. After segmentation, CNN-based
classifiers and hybrid models combining CNN feature extraction with traditional
machine learning classifiers (Support Vector Machine, Random Forest, and
Gradient Boosting) were evaluated using 5-fold cross-validation. Metrics
included accuracy, precision, recall, F1-score, Dice coefficient, and ROC-AUC.
U-Net with ResNet50 achieved the best performance for cancerous lungs (Dice:
0.9495, Accuracy: 0.9735), while U-Net with VGG16 performed best for
non-cancerous segmentation (Dice: 0.9532, Accuracy: 0.9513). For
classification, the CNN model using U-Net with Xception achieved 99.1 percent
accuracy, 99.74 percent recall, and 99.42 percent F1-score. The hybrid
CNN-SVM-Xception model achieved 96.7 percent accuracy and 97.88 percent
F1-score. Compared to prior methods, our framework consistently outperformed
existing models. In conclusion, combining U-Net with advanced CNN backbones
provides a powerful method for both segmentation and classification of lung
cancer in CT scans, supporting early diagnosis and clinical decision-making.

</details>


### [646] [Resolution Revolution: A Physics-Guided Deep Learning Framework for Spatiotemporal Temperature Reconstruction](https://arxiv.org/abs/2507.09872)
*Shengjie Liu,Lu Zhang,Siqin Wang*

Main category: eess.IV

TL;DR: 提出了一种物理引导的深度学习框架，用于整合不同分辨率的温度数据，实现高时空分辨率的重建。


<details>
  <summary>Details</summary>
Motivation: 解决地球观测中温度数据的时空分辨率矛盾，满足实际应用对高分辨率数据的需求。

Method: 结合卷积神经网络和物理模型，利用卫星数据（GOES-16和Landsat）和地球系统模型数据，通过线性放大实现温度重建。

Result: 在四个数据集上验证了框架的有效性，能够生成高分辨率温度数据。

Conclusion: 该框架为全球范围内全天候的高分辨率温度数据生成提供了新可能。

Abstract: Central to Earth observation is the trade-off between spatial and temporal
resolution. For temperature, this is especially critical because real-world
applications require high spatiotemporal resolution data. Current technology
allows for hourly temperature observations at 2 km, but only every 16 days at
100 m, a gap further exacerbated by cloud cover. Earth system models offer
continuous hourly temperature data, but at a much coarser spatial resolution
(9-31 km). Here, we present a physics-guided deep learning framework for
temperature data reconstruction that integrates these two data sources. The
proposed framework uses a convolutional neural network that incorporates the
annual temperature cycle and includes a linear term to amplify the coarse Earth
system model output into fine-scale temperature values observed from
satellites. We evaluated this framework using data from two satellites, GOES-16
(2 km, hourly) and Landsat (100 m, every 16 days), and demonstrated effective
temperature reconstruction with hold-out and in situ data across four datasets.
This physics-guided deep learning framework opens new possibilities for
generating high-resolution temperature data across spatial and temporal scales,
under all weather conditions and globally.

</details>


### [647] [IM-LUT: Interpolation Mixing Look-Up Tables for Image Super-Resolution](https://arxiv.org/abs/2507.09923)
*Sejin Park,Sangmin Lee,Kyong Hwan Jin,Seung-Won Jung*

Main category: eess.IV

TL;DR: 论文提出了一种名为IM-LUT的新框架，通过混合多种插值函数实现任意尺度超分辨率（ASISR），解决了现有方法计算成本高的问题。


<details>
  <summary>Details</summary>
Motivation: 现有基于查找表（LUT）的超分辨率方法仅适用于固定尺度，而基于隐式神经表示的ASISR方法计算和内存需求高。

Method: 提出IM-LUT框架，通过IM-Net预测插值函数的混合权重，并利用LUT替换高计算量操作，实现轻量级高效推理。

Result: 实验表明，IM-LUT在多个基准数据集上实现了图像质量与效率的优越平衡。

Conclusion: IM-LUT是一种适用于资源受限应用的高效ASISR解决方案。

Abstract: Super-resolution (SR) has been a pivotal task in image processing, aimed at
enhancing image resolution across various applications. Recently, look-up table
(LUT)-based approaches have attracted interest due to their efficiency and
performance. However, these methods are typically designed for fixed scale
factors, making them unsuitable for arbitrary-scale image SR (ASISR). Existing
ASISR techniques often employ implicit neural representations, which come with
considerable computational cost and memory demands. To address these
limitations, we propose Interpolation Mixing LUT (IM-LUT), a novel framework
that operates ASISR by learning to blend multiple interpolation functions to
maximize their representational capacity. Specifically, we introduce IM-Net, a
network trained to predict mixing weights for interpolation functions based on
local image patterns and the target scale factor. To enhance efficiency of
interpolation-based methods, IM-Net is transformed into IM-LUT, where LUTs are
employed to replace computationally expensive operations, enabling lightweight
and fast inference on CPUs while preserving reconstruction quality.
Experimental results on several benchmark datasets demonstrate that IM-LUT
consistently achieves a superior balance between image quality and efficiency
compared to existing methods, highlighting its potential as a promising
solution for resource-constrained applications.

</details>


### [648] [A Brain Tumor Segmentation Method Based on CLIP and 3D U-Net with Cross-Modal Semantic Guidance and Multi-Level Feature Fusion](https://arxiv.org/abs/2507.09966)
*Mingda Zhang*

Main category: eess.IV

TL;DR: 该研究提出了一种多级融合架构，结合像素级、特征级和语义级信息，显著提升了脑肿瘤MRI分割的精度。


<details>
  <summary>Details</summary>
Motivation: 脑肿瘤MRI分割的精确性对诊断和治疗至关重要，但现有方法因肿瘤形态异质性和复杂空间关系而受限，且未充分利用医学报告中的语义知识。

Method: 采用多级融合架构，结合3D U-Net和CLIP模型，通过3D-2D语义桥接、跨模态语义引导和基于语义的注意力机制实现信息融合。

Result: 在BraTS 2020数据集上，整体Dice系数达到0.8567，比传统3D U-Net提升4.8%，在增强肿瘤区域提升7.3%。

Conclusion: 多级融合架构显著提升了脑肿瘤分割性能，证明了语义信息在医学图像分析中的重要性。

Abstract: Precise segmentation of brain tumors from magnetic resonance imaging (MRI) is
essential for neuro-oncology diagnosis and treatment planning. Despite advances
in deep learning methods, automatic segmentation remains challenging due to
tumor morphological heterogeneity and complex three-dimensional spatial
relationships. Current techniques primarily rely on visual features extracted
from MRI sequences while underutilizing semantic knowledge embedded in medical
reports. This research presents a multi-level fusion architecture that
integrates pixel-level, feature-level, and semantic-level information,
facilitating comprehensive processing from low-level data to high-level
concepts. The semantic-level fusion pathway combines the semantic understanding
capabilities of Contrastive Language-Image Pre-training (CLIP) models with the
spatial feature extraction advantages of 3D U-Net through three mechanisms:
3D-2D semantic bridging, cross-modal semantic guidance, and semantic-based
attention mechanisms. Experimental validation on the BraTS 2020 dataset
demonstrates that the proposed model achieves an overall Dice coefficient of
0.8567, representing a 4.8% improvement compared to traditional 3D U-Net, with
a 7.3% Dice coefficient increase in the clinically important enhancing tumor
(ET) region.

</details>


### [649] [Graph-based Multi-Modal Interaction Lightweight Network for Brain Tumor Segmentation (GMLN-BTS) in Edge Iterative MRI Lesion Localization System (EdgeIMLocSys)](https://arxiv.org/abs/2507.09995)
*Guohao Huo,Ruiting Dai,Hao Tang*

Main category: eess.IV

TL;DR: 提出EdgeIMLocSys系统，结合GMLN-BTS网络和VRUM模块，通过人类反馈持续学习，实现高效、轻量化的脑肿瘤分割。


<details>
  <summary>Details</summary>
Motivation: 解决MRI扫描仪成像质量差异导致的模型泛化问题，提升脑肿瘤分割的鲁棒性和准确性。

Method: 采用GMLN-BTS网络，结合M2AE编码器和G2MCIM模块提取多模态特征，并引入VRUM模块优化分割边界。

Result: 在BraTS2017数据集上Dice得分为85.1%，参数量仅4.58M，显著优于现有轻量化方法。

Conclusion: 实现了高精度、低资源消耗的脑肿瘤分割，适合资源有限的临床环境部署。

Abstract: Brain tumor segmentation plays a critical role in clinical diagnosis and
treatment planning, yet the variability in imaging quality across different MRI
scanners presents significant challenges to model generalization. To address
this, we propose the Edge Iterative MRI Lesion Localization System
(EdgeIMLocSys), which integrates Continuous Learning from Human Feedback to
adaptively fine-tune segmentation models based on clinician feedback, thereby
enhancing robustness to scanner-specific imaging characteristics. Central to
this system is the Graph-based Multi-Modal Interaction Lightweight Network for
Brain Tumor Segmentation (GMLN-BTS), which employs a Modality-Aware Adaptive
Encoder (M2AE) to extract multi-scale semantic features efficiently, and a
Graph-based Multi-Modal Collaborative Interaction Module (G2MCIM) to model
complementary cross-modal relationships via graph structures. Additionally, we
introduce a novel Voxel Refinement UpSampling Module (VRUM) that
synergistically combines linear interpolation and multi-scale transposed
convolutions to suppress artifacts while preserving high-frequency details,
improving segmentation boundary accuracy. Our proposed GMLN-BTS model achieves
a Dice score of 85.1% on the BraTS2017 dataset with only 4.58 million
parameters, representing a 98% reduction compared to mainstream 3D Transformer
models, and significantly outperforms existing lightweight approaches. This
work demonstrates a synergistic breakthrough in achieving high-accuracy,
resource-efficient brain tumor segmentation suitable for deployment in
resource-constrained clinical environments.

</details>


### [650] [DepViT-CAD: Deployable Vision Transformer-Based Cancer Diagnosis in Histopathology](https://arxiv.org/abs/2507.10250)
*Ashkan Shakarami,Lorenzo Nicole,Rocco Cappellesso,Angelo Paolo Dei Tos,Stefano Ghidoni*

Main category: eess.IV

TL;DR: DepViT-CAD是一种基于多注意力视觉Transformer（MAViT）的可部署AI系统，用于组织病理学中的多类癌症诊断，通过大规模验证表现出高敏感性和可扩展性。


<details>
  <summary>Details</summary>
Motivation: 准确及时的癌症诊断对临床决策至关重要，现有方法需要更高效和可扩展的解决方案。

Method: 提出MAViT，一种多注意力视觉Transformer，用于捕捉多样肿瘤类型的细粒度形态模式，并在1008张全切片图像上训练。

Result: 在两个独立队列中验证，诊断敏感性分别为94.11%和92%。

Conclusion: DepViT-CAD结合先进Transformer架构和大规模验证，为AI辅助癌症诊断提供了稳健且可扩展的方法。

Abstract: Accurate and timely cancer diagnosis from histopathological slides is vital
for effective clinical decision-making. This paper introduces DepViT-CAD, a
deployable AI system for multi-class cancer diagnosis in histopathology. At its
core is MAViT, a novel Multi-Attention Vision Transformer designed to capture
fine-grained morphological patterns across diverse tumor types. MAViT was
trained on expert-annotated patches from 1008 whole-slide images, covering 11
diagnostic categories, including 10 major cancers and non-tumor tissue.
DepViT-CAD was validated on two independent cohorts: 275 WSIs from The Cancer
Genome Atlas and 50 routine clinical cases from pathology labs, achieving
diagnostic sensitivities of 94.11% and 92%, respectively. By combining
state-of-the-art transformer architecture with large-scale real-world
validation, DepViT-CAD offers a robust and scalable approach for AI-assisted
cancer diagnostics. To support transparency and reproducibility, software and
code will be made publicly available at GitHub.

</details>


<div id='math.NA'></div>

# math.NA [[Back]](#toc)

### [651] [Energy Dissipation Rate Guided Adaptive Sampling for Physics-Informed Neural Networks: Resolving Surface-Bulk Dynamics in Allen-Cahn Systems](https://arxiv.org/abs/2507.09757)
*Chunyan Li,Wenkai Yu,Qi Wang*

Main category: math.NA

TL;DR: EDRAS是一种基于能量耗散率引导的自适应采样策略，显著提升了PINN在任意域中求解热力学一致PDE的性能。


<details>
  <summary>Details</summary>
Motivation: 传统残差自适应方法在求解复杂几何和动态边界条件下的PDE时效率不足，EDRAS通过物理结构指导采样以提高精度。

Method: 利用局部能量耗散率密度作为指标，动态重采样关键点，优化PINN的训练过程。

Result: 在Allen-Cahn模型中，EDRAS将相对均方误差降低至传统方法的六分之一，且计算效率更高。

Conclusion: EDRAS为PINN提供了一种高效的物理增强方法，适用于复杂热力学过程的模拟。

Abstract: We introduce the Energy Dissipation Rate guided Adaptive Sampling (EDRAS)
strategy, a novel method that substantially enhances the performance of
Physics-Informed Neural Networks (PINNs) in solving thermodynamically
consistent partial differential equations (PDEs) over arbitrary domains. EDRAS
leverages the local energy dissipation rate density as a guiding metric to
identify and adaptively re-sample critical collocation points from both the
interior and boundary of the computational domain. This dynamical sampling
approach improves the accuracy of residual-based PINNs by aligning the training
process with the underlying physical structure of the system. In this study, we
demonstrate the effectiveness of EDRAS using the Allen-Cahn phase field model
in irregular geometries, achieving up to a sixfold reduction in the relative
mean square error compared to traditional residual-based adaptive refinement
(RAR) methods. Moreover, we compare EDRAS with other residual-based adaptive
sampling approaches and show that EDRAS is not only computationally more
efficient but also more likely to identify high-impact collocation points.
Through numerical solutions of the Allen-Cahn equation with both static
(Neumann) and dynamic boundary conditions in 2D disk- and ellipse-shaped
domains solved using PINN coupled with EDRAS, we gain significant insights into
how dynamic boundary conditions influence bulk phase evolution and
thermodynamic behavior. The proposed approach offers an effective, physically
informed enhancement to PINN frameworks for solving thermodynamically
consistent models, making PINN a robust and versatile computational tool for
investigating complex thermodynamic processes in arbitrary geometries.

</details>


### [652] [Physics-informed neural networks for high-dimensional solutions and snaking bifurcations in nonlinear lattices](https://arxiv.org/abs/2507.09782)
*Muhammad Luthfi Shahab,Fidya Almira Suheri,Rudy Kusdiantara,Hadi Susanto*

Main category: math.NA

TL;DR: 本文提出了一种基于物理信息神经网络（PINNs）的框架，用于解决非线性晶格中的关键挑战，包括解近似、分岔图构建和线性稳定性分析。


<details>
  <summary>Details</summary>
Motivation: 研究旨在利用PINNs解决非线性晶格系统中的计算难题，特别是在高维情况下提高效率和准确性。

Method: 使用Levenberg-Marquardt算法优化PINNs权重，结合随机采样策略提升效率；通过耦合PINNs与延拓方法构建分岔图；引入输出约束进行线性稳定性分析。

Result: 数值实验表明，该方法在高维情况下与传统方法相比具有更高或相当的准确性。

Conclusion: 神经网络在复杂非线性晶格系统研究中具有可扩展性和高效性的潜力。

Abstract: This paper introduces a framework based on physics-informed neural networks
(PINNs) for addressing key challenges in nonlinear lattices, including solution
approximation, bifurcation diagram construction, and linear stability analysis.
We first employ PINNs to approximate solutions of nonlinear systems arising
from lattice models, using the Levenberg-Marquardt algorithm to optimize
network weights for greater accuracy. To enhance computational efficiency in
high-dimensional settings, we integrate a stochastic sampling strategy. We then
extend the method by coupling PINNs with a continuation approach to compute
snaking bifurcation diagrams, incorporating an auxiliary equation to
effectively track successive solution branches. For linear stability analysis,
we adapt PINNs to compute eigenvectors, introducing output constraints to
enforce positivity, in line with Sturm-Liouville theory. Numerical experiments
are conducted on the discrete Allen-Cahn equation with cubic and quintic
nonlinearities in one to five spatial dimensions. The results demonstrate that
the proposed approach achieves accuracy comparable to, or better than,
traditional numerical methods, especially in high-dimensional regimes where
computational resources are a limiting factor. These findings highlight the
potential of neural networks as scalable and efficient tools for the study of
complex nonlinear lattice systems.

</details>
