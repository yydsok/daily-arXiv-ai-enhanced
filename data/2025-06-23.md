<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 41]
- [cs.CL](#cs.CL) [Total: 79]
- [cs.CV](#cs.CV) [Total: 117]
- [cs.LG](#cs.LG) [Total: 171]
- [cs.MA](#cs.MA) [Total: 6]
- [cs.MM](#cs.MM) [Total: 2]
- [cs.RO](#cs.RO) [Total: 52]
- [cs.SD](#cs.SD) [Total: 10]
- [physics.comp-ph](#physics.comp-ph) [Total: 2]
- [astro-ph.IM](#astro-ph.IM) [Total: 1]
- [q-bio.BM](#q-bio.BM) [Total: 1]
- [stat.ME](#stat.ME) [Total: 2]
- [quant-ph](#quant-ph) [Total: 5]
- [cs.IT](#cs.IT) [Total: 2]
- [cs.GT](#cs.GT) [Total: 2]
- [cs.AR](#cs.AR) [Total: 2]
- [stat.ML](#stat.ML) [Total: 10]
- [astro-ph.EP](#astro-ph.EP) [Total: 1]
- [cs.DB](#cs.DB) [Total: 1]
- [physics.flu-dyn](#physics.flu-dyn) [Total: 1]
- [cs.HC](#cs.HC) [Total: 4]
- [eess.SY](#eess.SY) [Total: 1]
- [q-fin.ST](#q-fin.ST) [Total: 1]
- [astro-ph.GA](#astro-ph.GA) [Total: 1]
- [physics.soc-ph](#physics.soc-ph) [Total: 1]
- [eess.IV](#eess.IV) [Total: 21]
- [cs.CR](#cs.CR) [Total: 6]
- [math.ST](#math.ST) [Total: 1]
- [cs.GR](#cs.GR) [Total: 6]
- [cs.OS](#cs.OS) [Total: 1]
- [physics.ins-det](#physics.ins-det) [Total: 2]
- [cond-mat.str-el](#cond-mat.str-el) [Total: 1]
- [q-bio.QM](#q-bio.QM) [Total: 1]
- [cs.SY](#cs.SY) [Total: 1]
- [eess.SP](#eess.SP) [Total: 1]
- [cs.CG](#cs.CG) [Total: 1]
- [cond-mat.quant-gas](#cond-mat.quant-gas) [Total: 1]
- [cs.ET](#cs.ET) [Total: 2]
- [cs.SI](#cs.SI) [Total: 1]
- [physics.med-ph](#physics.med-ph) [Total: 1]
- [cs.IR](#cs.IR) [Total: 4]
- [math.NA](#math.NA) [Total: 1]
- [cs.NI](#cs.NI) [Total: 1]
- [cond-mat.soft](#cond-mat.soft) [Total: 1]
- [cs.CY](#cs.CY) [Total: 4]
- [cs.NE](#cs.NE) [Total: 3]
- [hep-th](#hep-th) [Total: 1]
- [cs.DC](#cs.DC) [Total: 1]
- [cs.SE](#cs.SE) [Total: 5]
- [eess.AS](#eess.AS) [Total: 4]
- [cs.DS](#cs.DS) [Total: 1]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [LLMs Struggle to Perform Counterfactual Reasoning with Parametric Knowledge](https://arxiv.org/abs/2506.15732)
*Khurram Yamin,Gaurav Ghosal,Bryan Wilder*

Main category: cs.AI

TL;DR: 大型语言模型（LLM）在知识密集型任务中表现优异，但在新环境中整合参数化知识与新信息时存在困难，尤其是在反事实推理任务中。


<details>
  <summary>Details</summary>
Motivation: 探索LLM是否能在新环境中结合上下文知识与参数化知识，特别是在反事实推理任务中的表现。

Method: 通过合成和真实实验，研究LLM在多跳推理问题中的表现，并尝试通过微调提升其能力。

Result: LLM在反事实推理中表现不佳，倾向于依赖参数化知识，且微调可能导致参数化知识退化。

Conclusion: 当前LLM在新环境中重新利用参数化知识的能力存在显著局限性。

Abstract: Large Language Models have been shown to contain extensive world knowledge in
their parameters, enabling impressive performance on many knowledge intensive
tasks. However, when deployed in novel settings, LLMs often encounter
situations where they must integrate parametric knowledge with new or
unfamiliar information. In this work, we explore whether LLMs can combine
knowledge in-context with their parametric knowledge through the lens of
counterfactual reasoning. Through synthetic and real experiments in multi-hop
reasoning problems, we show that LLMs generally struggle with counterfactual
reasoning, often resorting to exclusively using their parametric knowledge.
Moreover, we show that simple post-hoc finetuning can struggle to instill
counterfactual reasoning ability -- often leading to degradation in stored
parametric knowledge. Ultimately, our work reveals important limitations of
current LLM's abilities to re-purpose parametric knowledge in novel settings.

</details>


### [2] [$\texttt{SPECS}$: Faster Test-Time Scaling through Speculative Drafts](https://arxiv.org/abs/2506.15733)
*Mert Cemri,Nived Rajaraman,Rishabh Tiwari,Xiaoxuan Liu,Kurt Keutzer,Ion Stoica,Kannan Ramchandran,Ahmad Beirami,Ziteng Sun*

Main category: cs.AI

TL;DR: 论文提出了一种名为SPECS的延迟感知测试时扩展方法，通过结合小模型生成候选序列和大模型评估，优化推理能力并降低延迟。


<details>
  <summary>Details</summary>
Motivation: 当前测试时扩展方法主要关注计算资源（FLOPS）优化，忽略了延迟对用户体验的影响，SPECS旨在填补这一空白。

Method: SPECS利用小模型高效生成候选序列，结合大模型和奖励模型的信号进行评估，并引入奖励引导的软验证和延迟机制。

Result: 在MATH500等数据集上，SPECS在保持或超越束搜索精度的同时，延迟降低达19.1%。

Conclusion: SPECS通过KL正则化强化学习目标收敛，有效平衡了推理能力和延迟。

Abstract: Scaling test-time compute has driven the recent advances in the reasoning
capabilities of large language models (LLMs), typically by allocating
additional computation for more thorough exploration. However, increased
compute often comes at the expense of higher user-facing latency, directly
impacting user experience. Current test-time scaling methods primarily optimize
for accuracy based on total compute resources (FLOPS), often overlooking
latency constraints. To address this gap, we propose $\texttt{SPECS}$, a
latency-aware test-time scaling method inspired by speculative decoding.
$\texttt{SPECS}$~uses a smaller, faster model to generate candidate sequences
efficiently, and evaluates these candidates using signals from both a larger
target model and a dedicated reward model. We introduce new integration
strategies, including reward-guided soft verification and a reward-based
deferral mechanism. Empirical results on MATH500, AMC23 and OlympiadBench
datasets show that $\texttt{SPECS}$~matches or surpasses beam search accuracy
while reducing latency by up to $\sim$19.1\%. Our theoretical analysis shows
that our algorithm converges to the solution of a KL-regularized reinforcement
learning objective with increasing beam width.

</details>


### [3] [The Safety Reminder: A Soft Prompt to Reactivate Delayed Safety Awareness in Vision-Language Models](https://arxiv.org/abs/2506.15734)
*Peiyuan Tang,Haojie Xin,Xiaodong Zhang,Jun Sun,Qin Xia,Zijiang Yang*

Main category: cs.AI

TL;DR: 论文提出了一种名为“安全提醒”的软提示调优方法，通过优化可学习的提示令牌来增强视觉语言模型（VLM）的安全性，防止有害内容生成。


<details>
  <summary>Details</summary>
Motivation: 随着视觉语言模型（VLMs）在代码生成和聊天机器人等实际应用中的能力增强，其安全性问题变得至关重要。VLMs因其多模态特性面临独特的漏洞，攻击者可能通过修改视觉或文本输入绕过安全防护，触发有害内容生成。

Method: 通过系统分析VLM在攻击下的行为，发现了一种称为“延迟安全意识”的现象。基于此，提出了一种名为“安全提醒”的软提示调优方法，通过优化可学习的提示令牌，定期注入文本生成过程以增强安全意识。

Result: 在三个安全基准和一个对抗攻击测试中，该方法显著降低了攻击成功率，同时保持了模型在正常任务中的性能。

Conclusion: 该方法为实际应用中部署更安全的VLMs提供了实用解决方案。

Abstract: As Vision-Language Models (VLMs) demonstrate increasing capabilities across
real-world applications such as code generation and chatbot assistance,
ensuring their safety has become paramount. Unlike traditional Large Language
Models (LLMs), VLMs face unique vulnerabilities due to their multimodal nature,
allowing adversaries to modify visual or textual inputs to bypass safety
guardrails and trigger the generation of harmful content. Through systematic
analysis of VLM behavior under attack, we identify a novel phenomenon termed
``delayed safety awareness''. Specifically, we observe that safety-aligned VLMs
may initially be compromised to produce harmful content, but eventually
recognize the associated risks and attempt to self-correct. This pattern
suggests that VLMs retain their underlying safety awareness but experience a
temporal delay in their activation. Building on this insight, we hypothesize
that VLMs' safety awareness can be proactively reactivated through carefully
designed prompts. To this end, we introduce ``The Safety Reminder'', a soft
prompt tuning approach that optimizes learnable prompt tokens, which are
periodically injected during the text generation process to enhance safety
awareness, effectively preventing harmful content generation. Additionally, our
safety reminder only activates when harmful content is detected, leaving normal
conversations unaffected and preserving the model's performance on benign
tasks. Through comprehensive evaluation across three established safety
benchmarks and one adversarial attacks, we demonstrate that our approach
significantly reduces attack success rates while maintaining model utility,
offering a practical solution for deploying safer VLMs in real-world
applications.

</details>


### [4] [ContextBench: Modifying Contexts for Targeted Latent Activation](https://arxiv.org/abs/2506.15735)
*Robert Graham,Edward Stevinson,Leo Richter,Alexander Chia,Joseph Miller,Joseph Isaac Bloom*

Main category: cs.AI

TL;DR: 论文提出了一种通过上下文修改生成针对性输入的方法，用于激活语言模型的特定行为或潜在特征，并提出了ContextBench基准和评估框架。通过改进进化提示优化（EPO）方法，结合LLM辅助和扩散模型修复，实现了在激活效果和语言流畅性上的最佳平衡。


<details>
  <summary>Details</summary>
Motivation: 识别能够触发语言模型特定行为或潜在特征的输入，对安全应用具有重要意义。

Method: 提出上下文修改方法，并构建ContextBench基准。改进进化提示优化（EPO），结合LLM辅助和扩散模型修复。

Result: 改进后的方法在激活效果和语言流畅性上达到最佳平衡，优于现有方法。

Conclusion: 上下文修改方法及其改进版本在语言模型行为激活和输入流畅性方面具有潜力，为安全应用提供了新工具。

Abstract: Identifying inputs that trigger specific behaviours or latent features in
language models could have a wide range of safety use cases. We investigate a
class of methods capable of generating targeted, linguistically fluent inputs
that activate specific latent features or elicit model behaviours. We formalise
this approach as context modification and present ContextBench -- a benchmark
with tasks assessing core method capabilities and potential safety
applications. Our evaluation framework measures both elicitation strength
(activation of latent features or behaviours) and linguistic fluency,
highlighting how current state-of-the-art methods struggle to balance these
objectives. We enhance Evolutionary Prompt Optimisation (EPO) with
LLM-assistance and diffusion model inpainting, and demonstrate that these
variants achieve state-of-the-art performance in balancing elicitation
effectiveness and fluency.

</details>


### [5] [SHADE-Arena: Evaluating Sabotage and Monitoring in LLM Agents](https://arxiv.org/abs/2506.15740)
*Jonathan Kutasov,Yuqi Sun,Paul Colognese,Teun van der Weij,Linda Petrini,Chen Bo Calvin Zhang,John Hughes,Xiang Deng,Henry Sleight,Tyler Tracy,Buck Shlegeris,Joe Benton*

Main category: cs.AI

TL;DR: 论文研究了前沿大语言模型（LLM）在执行任务时隐藏有害目标的能力，并开发了SHADE-Arena评估数据集。结果显示，最佳模型在隐藏目标完成率上表现有限（Claude 3.7 Sonnet为27%，Gemini 2.5 Pro为15%），且监控能力仍有提升空间（Gemini 2.5 Pro的AUC为0.87）。


<details>
  <summary>Details</summary>
Motivation: 评估LLM作为自主代理时隐藏有害目标的能力，以应对复杂任务中的潜在风险。

Method: 使用SHADE-Arena数据集，评估模型在完成主任务和隐藏有害任务时的表现，并测试其避免被监控的能力。

Result: 前沿模型在隐藏任务完成率上表现有限，监控能力较好但仍有不足。隐藏任务的成功依赖于不可见的草稿板。

Conclusion: 当前模型在隐藏有害目标方面仍有困难，但监控复杂任务的挑战将随着任务复杂度增加而加剧。

Abstract: As Large Language Models (LLMs) are increasingly deployed as autonomous
agents in complex and long horizon settings, it is critical to evaluate their
ability to sabotage users by pursuing hidden objectives. We study the ability
of frontier LLMs to evade monitoring and achieve harmful hidden goals while
completing a wide array of realistic tasks. We evaluate a broad range of
frontier LLMs using SHADE (Subtle Harmful Agent Detection & Evaluation)-Arena,
the first highly diverse agent evaluation dataset for sabotage and monitoring
capabilities of LLM agents. SHADE-Arena consists of complex pairs of benign
main tasks and harmful side objectives in complicated environments. Agents are
evaluated on their ability to complete the side task without appearing
suspicious to an LLM monitor. When measuring agent ability to (a) complete the
main task, (b) complete the side task, and (c) avoid detection, we find that
the best performing frontier models score 27% (Claude 3.7 Sonnet) and 15%
(Gemini 2.5 Pro) as sabotage agents when overseen by Claude 3.6 Sonnet. For
current frontier models, success on the side task relies heavily on having
access to a hidden scratchpad that is not visible to the monitor. We also use
SHADE-Arena to measure models' monitoring abilities, with the top monitor
(Gemini 2.5 Pro) achieving an AUC of 0.87 at distinguishing benign and malign
transcripts. We find that for now, models still struggle at sabotage due to
failures in long-context main task execution. However, our measurements already
demonstrate the difficulty of monitoring for subtle sabotage attempts, which we
expect to only increase in the face of more complex and longer-horizon tasks.

</details>


### [6] [OAgents: An Empirical Study of Building Effective Agents](https://arxiv.org/abs/2506.15741)
*He Zhu,Tianrui Qin,King Zhu,Heyuan Huang,Yeyi Guan,Jinxiang Xia,Yi Yao,Hanhao Li,Ningning Wang,Pai Liu,Tianhao Peng,Xin Gui,Xiaowan Li,Yuhui Liu,Yuchen Eleanor Jiang,Jun Wang,Changwang Zhang,Xiangru Tang,Ge Zhang,Jian Yang,Minghao Liu,Xitong Gao,Wangchunshu Zhou,Jiaheng Liu*

Main category: cs.AI

TL;DR: 论文指出当前Agentic AI研究缺乏标准化和科学严谨性，通过系统实证研究提出更稳健的评估协议，并开源了高性能的OAgents框架。


<details>
  <summary>Details</summary>
Motivation: 当前Agentic AI研究缺乏标准化和科学严谨性，难以公平比较不同方法的效果。

Method: 在GAIA基准和BrowseComp上系统实证研究关键组件设计选择的影响，并提出稳健评估协议。

Result: 发现缺乏标准评估协议导致结果不可复现，提出新协议并开源高性能OAgents框架。

Conclusion: OAgents框架为Agentic AI研究提供了模块化设计，推动未来研究发展。

Abstract: Recently, Agentic AI has become an increasingly popular research field.
However, we argue that current agent research practices lack standardization
and scientific rigor, making it hard to conduct fair comparisons among methods.
As a result, it is still unclear how different design choices in agent
frameworks affect effectiveness, and measuring their progress remains
challenging. In this work, we conduct a systematic empirical study on GAIA
benchmark and BrowseComp to examine the impact of popular design choices in key
agent components in a fair and rigorous manner. We find that the lack of a
standard evaluation protocol makes previous works, even open-sourced ones,
non-reproducible, with significant variance between random runs. Therefore, we
introduce a more robust evaluation protocol to stabilize comparisons. Our study
reveals which components and designs are crucial for effective agents, while
others are redundant, despite seeming logical. Based on our findings, we build
and open-source OAgents, a new foundation agent framework that achieves
state-of-the-art performance among open-source projects. OAgents offers a
modular design for various agent components, promoting future research in
Agentic AI.

</details>


### [7] [Sysformer: Safeguarding Frozen Large Language Models with Adaptive System Prompts](https://arxiv.org/abs/2506.15751)
*Kartik Sharma,Yiqiao Jin,Vineeth Rakesh,Yingtong Dou,Menghai Pan,Mahashweta Das,Srijan Kumar*

Main category: cs.AI

TL;DR: 论文提出了一种名为Sysformer的新方法，通过学习调整系统提示来增强大型语言模型（LLM）的安全性，无需微调模型参数。


<details>
  <summary>Details</summary>
Motivation: 现有方法在确保LLM安全性时依赖昂贵的微调或次优启发式技术，无法有效平衡对无害提示的响应和对有害提示的拒绝。

Method: 提出Sysformer模型，在LLM输入嵌入空间中动态调整系统提示，同时保持LLM参数不变。

Result: 实验表明，Sysformer显著提升了LLM的鲁棒性，对有害提示的拒绝率提升80%，对安全提示的合规性提升90%，并能抵御复杂的越狱攻击。

Conclusion: Sysformer为LLM提供了一种低成本的安全保障方法，并启发了未来对可变系统提示设计的进一步研究。

Abstract: As large language models (LLMs) are deployed in safety-critical settings, it
is essential to ensure that their responses comply with safety standards. Prior
research has revealed that LLMs often fail to grasp the notion of safe
behaviors, resulting in either unjustified refusals to harmless prompts or the
generation of harmful content. While substantial efforts have been made to
improve their robustness, existing defenses often rely on costly fine-tuning of
model parameters or employ suboptimal heuristic techniques. In this work, we
take a novel approach to safeguard LLMs by learning to adapt the system prompts
in instruction-tuned LLMs. While LLMs are typically pre-trained to follow a
fixed system prompt, we investigate the impact of tailoring the system prompt
to each specific user input on the safety of the responses. To this end, we
propose $\textbf{Sysformer}$, a trans$\textbf{former}$ model that updates an
initial $\textbf{sys}$tem prompt to a more robust system prompt in the LLM
input embedding space while attending to the user prompt. While keeping the LLM
parameters frozen, the Sysformer is trained to refuse to respond to a set of
harmful prompts while responding ideally to a set of safe ones. Through
extensive experiments on $5$ LLMs from different families and $2$ recent
benchmarks, we demonstrate that Sysformer can significantly enhance the
robustness of LLMs, leading to upto $80\%$ gain in the refusal rate on harmful
prompts while enhancing the compliance with the safe prompts by upto $90\%$.
Results also generalize well to sophisticated jailbreaking attacks, making LLMs
upto $100\%$ more robust against different attack strategies. We hope our
findings lead to cheaper safeguarding of LLMs and motivate future
investigations into designing variable system prompts.

</details>


### [8] [Linear-Time Primitives for Algorithm Development in Graphical Causal Inference](https://arxiv.org/abs/2506.15758)
*Marcel Wienöbst,Sebastian Weichwald,Leonard Henckel*

Main category: cs.AI

TL;DR: CIfly是一个高效的图形因果推断框架，将可达性作为核心操作，支持多种因果推理任务，并提供高性能实现。


<details>
  <summary>Details</summary>
Motivation: 许多因果推理任务可以简化为状态空间图中的可达性问题，但现有方法如道德化和潜在投影效率较低。

Method: CIfly通过动态构建状态空间图并利用规则表规范算法，实现线性时间复杂度。

Result: CIfly比传统方法更高效，支持Python和R调用，并成功应用于经典任务和新算法开发。

Conclusion: CIfly为图形因果推断提供了灵活、可扩展的框架，推动了算法开发和高效部署。

Abstract: We introduce CIfly, a framework for efficient algorithmic primitives in
graphical causal inference that isolates reachability as a reusable core
operation. It builds on the insight that many causal reasoning tasks can be
reduced to reachability in purpose-built state-space graphs that can be
constructed on the fly during traversal. We formalize a rule table schema for
specifying such algorithms and prove they run in linear time. We establish
CIfly as a more efficient alternative to the common primitives moralization and
latent projection, which we show are computationally equivalent to Boolean
matrix multiplication. Our open-source Rust implementation parses rule table
text files and runs the specified CIfly algorithms providing high-performance
execution accessible from Python and R. We demonstrate CIfly's utility by
re-implementing a range of established causal inference tasks within the
framework and by developing new algorithms for instrumental variables. These
contributions position CIfly as a flexible and scalable backbone for graphical
causal inference, guiding algorithm development and enabling easy and efficient
deployment.

</details>


### [9] [Advancing Stochastic 3-SAT Solvers by Dissipating Oversatisfied Constraints](https://arxiv.org/abs/2506.15774)
*J. Schwardt,J. C. Budich*

Main category: cs.AI

TL;DR: 提出了一种名为DOCSAT的随机局部搜索启发式算法，用于解决3-SAT问题，显著优于现有求解器，尤其是在处理极难实例时。


<details>
  <summary>Details</summary>
Motivation: 现有方法（如WalkSAT）容易陷入局部极小值，而DOCSAT通过减少过满足约束的数量来解决这一问题。

Method: DOCSAT通过消散过满足约束（DOC）来避免局部极小值，并利用统计结构优化搜索过程。

Result: 在N=15000的极难3-SAT实例上，DOCSAT表现优于WalkSAT和Kissat等算法。

Conclusion: DOCSAT为组合优化问题提供了一种避免局部极小值的新方法，具有推广到其他优化问题的潜力。

Abstract: We introduce and benchmark a stochastic local search heuristic for the
NP-complete satisfiability problem 3-SAT that drastically outperforms existing
solvers in the notoriously difficult realm of critically hard instances. Our
construction is based on the crucial observation that well established previous
approaches such as WalkSAT are prone to get stuck in local minima that are
distinguished from true solutions by a larger number of oversatisfied
combinatorial constraints. To address this issue, the proposed algorithm,
coined DOCSAT, dissipates oversatisfied constraints (DOC), i.e. reduces their
unfavorable abundance so as to render them critical. We analyze and benchmark
our algorithm on a randomly generated sample of hard but satisfiable 3-SAT
instances with varying problem sizes up to N=15000. Quite remarkably, we find
that DOCSAT outperforms both WalkSAT and other well known algorithms including
the complete solver Kissat, even when comparing its ability to solve the
hardest quintile of the sample to the average performance of its competitors.
The essence of DOCSAT may be seen as a way of harnessing statistical structure
beyond the primary cost function of a combinatorial problem to avoid or escape
local minima traps in stochastic local search, which opens avenues for
generalization to other optimization problems.

</details>


### [10] [SLR: An Automated Synthesis Framework for Scalable Logical Reasoning](https://arxiv.org/abs/2506.15787)
*Lukas Helff,Ahmad Omar,Felix Friedrich,Wolfgang Stammer,Antonia Wüst,Tim Woydt,Rupert Mitchell,Patrick Schramowski,Kristian Kersting*

Main category: cs.AI

TL;DR: SLR是一个端到端框架，用于通过可扩展的逻辑推理系统评估和训练大型语言模型（LLMs）。它自动化生成推理任务，创建SLR-Bench基准，并发现当代LLMs在逻辑推理上的不足，同时通过逻辑调优显著提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 当前LLMs在逻辑推理任务中表现不佳，且缺乏自动化、可扩展的评估和训练方法。SLR旨在填补这一空白，提供无需人工标注的解决方案。

Method: SLR通过自动化合成推理任务，包括潜在规则、验证程序和任务提示，创建SLR-Bench基准，涵盖20个难度递增的课程级别。

Result: 当代LLMs在逻辑推理上表现有限，而逻辑调优使Llama-3-8B性能翻倍，达到与Gemini-Flash-Thinking相当的水平，但计算成本更低。

Conclusion: SLR为LLMs的逻辑推理能力提供了自动化、可扩展的评估和训练环境，显著提升了模型性能。

Abstract: We introduce SLR, an end-to-end framework for systematic evaluation and
training of Large Language Models (LLMs) via Scalable Logical Reasoning. Given
a user's task specification, SLR enables scalable, automated synthesis of
inductive reasoning tasks with precisely controlled difficulty. For each task,
SLR synthesizes (i) a latent ground-truth rule, (ii) an executable validation
program used by a symbolic judge to deterministically verify model outputs, and
(iii) an instruction prompt for the reasoning task. Using SLR, we create
SLR-Bench, a benchmark comprising over 19k prompts spanning 20 curriculum
levels that progressively increase in relational, arithmetic, and recursive
complexity. Large-scale evaluation reveals that contemporary LLMs readily
produce syntactically valid rules, yet often fail at correct logical inference.
Recent reasoning LLMs do somewhat better, but incur substantial increases in
test-time compute, sometimes exceeding 15k completion tokens. Finally,
logic-tuning via SLR doubles Llama-3-8B accuracy on SLR-Bench, achieving parity
with Gemini-Flash-Thinking at a fraction of computational cost. SLR is fully
automated, requires no human annotation, ensures dataset novelty, and offers a
scalable environment for probing and advancing LLMs' reasoning capabilities.

</details>


### [11] [Deep Reinforcement Learning Xiangqi Player with Monte Carlo Tree Search](https://arxiv.org/abs/2506.15880)
*Berk Yilmaz,Junyu Hu,Jinsong Liu*

Main category: cs.AI

TL;DR: 本文提出了一种结合深度强化学习（DRL）和蒙特卡洛树搜索（MCTS）的象棋AI系统，通过策略-价值网络和MCTS模拟走棋后果，优化决策。


<details>
  <summary>Details</summary>
Motivation: 解决象棋中独特的棋盘布局、棋子移动限制和胜利条件等复杂性问题，填补该领域的空白。

Method: 结合策略-价值网络与MCTS，模拟走棋后果并优化决策。

Result: 克服了象棋的高分支因子和非对称棋子动态等挑战，提升了AI在文化策略游戏中的能力。

Conclusion: 该研究不仅推动了象棋AI的发展，还为DRL-MCTS框架在特定领域规则系统中的应用提供了借鉴。

Abstract: This paper presents a Deep Reinforcement Learning (DRL) system for Xiangqi
(Chinese Chess) that integrates neural networks with Monte Carlo Tree Search
(MCTS) to enable strategic self-play and self-improvement. Addressing the
underexplored complexity of Xiangqi, including its unique board layout, piece
movement constraints, and victory conditions, our approach combines
policy-value networks with MCTS to simulate move consequences and refine
decision-making. By overcoming challenges such as Xiangqi's high branching
factor and asymmetrical piece dynamics, our work advances AI capabilities in
culturally significant strategy games while providing insights for adapting
DRL-MCTS frameworks to domain-specific rule systems.

</details>


### [12] [Exploring Big Five Personality and AI Capability Effects in LLM-Simulated Negotiation Dialogues](https://arxiv.org/abs/2506.15928)
*Myke C. Cohen,Zhe Su,Hsien-Te Kao,Daniel Nguyen,Spencer Lynch,Maarten Sap,Svitlana Volkova*

Main category: cs.AI

TL;DR: 论文提出了一个评估框架，用于关键任务谈判场景中的AI代理系统，重点研究人格特质和AI特性如何影响谈判结果。


<details>
  <summary>Details</summary>
Motivation: 解决AI代理在多样化人类操作者和利益相关者中适应的需求，特别是在高风险的跨团队协调和军民互动中。

Method: 使用Sotopia模拟平台进行两项实验：实验1通过因果发现方法分析人格特质对价格谈判的影响；实验2评估人类-AI工作谈判中AI透明度和信任度的作用。

Result: 发现Agreeableness和Extraversion显著影响谈判结果；AI透明度和信任度对任务有效性至关重要。

Conclusion: 研究为AI代理在复杂操作中的可靠性评估提供了可重复的方法，强调了社会动态对任务成功的重要性。

Abstract: This paper presents an evaluation framework for agentic AI systems in
mission-critical negotiation contexts, addressing the need for AI agents that
can adapt to diverse human operators and stakeholders. Using Sotopia as a
simulation testbed, we present two experiments that systematically evaluated
how personality traits and AI agent characteristics influence LLM-simulated
social negotiation outcomes--a capability essential for a variety of
applications involving cross-team coordination and civil-military interactions.
Experiment 1 employs causal discovery methods to measure how personality traits
impact price bargaining negotiations, through which we found that Agreeableness
and Extraversion significantly affect believability, goal achievement, and
knowledge acquisition outcomes. Sociocognitive lexical measures extracted from
team communications detected fine-grained differences in agents' empathic
communication, moral foundations, and opinion patterns, providing actionable
insights for agentic AI systems that must operate reliably in high-stakes
operational scenarios. Experiment 2 evaluates human-AI job negotiations by
manipulating both simulated human personality and AI system characteristics,
specifically transparency, competence, adaptability, demonstrating how AI agent
trustworthiness impact mission effectiveness. These findings establish a
repeatable evaluation methodology for experimenting with AI agent reliability
across diverse operator personalities and human-agent team dynamics, directly
supporting operational requirements for reliable AI systems. Our work advances
the evaluation of agentic AI workflows by moving beyond standard performance
metrics to incorporate social dynamics essential for mission success in complex
operations.

</details>


### [13] [Bayesian Epistemology with Weighted Authority: A Formal Architecture for Truth-Promoting Autonomous Scientific Reasoning](https://arxiv.org/abs/2506.16015)
*Craig S. Wright*

Main category: cs.AI

TL;DR: BEWA是一种基于贝叶斯推理的架构，用于动态评估科学主张，结合复制分数、引用权重和时间衰减，支持图传播和作者可信度建模。


<details>
  <summary>Details</summary>
Motivation: 科学文献的爆炸性增长超出了人类和AI系统的处理能力，需要一种结构化方法来动态评估和更新科学主张的可信度。

Method: 采用贝叶斯推理、复制评分、引用权重和时间衰减机制，结合图传播和作者可信度建模，构建动态的科学主张评估网络。

Result: BEWA提供了一种可计算验证的认知网络，支持真理效用、理性信念收敛和审计弹性。

Conclusion: BEWA为机器推理系统提供了动态科学领域中的可信度评估和完整性保障的基础。

Abstract: The exponential expansion of scientific literature has surpassed the
epistemic processing capabilities of both human experts and current artificial
intelligence systems. This paper introduces Bayesian Epistemology with Weighted
Authority (BEWA), a formally structured architecture that operationalises
belief as a dynamic, probabilistically coherent function over structured
scientific claims. Each claim is contextualised, author-attributed, and
evaluated through a system of replication scores, citation weighting, and
temporal decay. Belief updates are performed via evidence-conditioned Bayesian
inference, contradiction processing, and epistemic decay mechanisms. The
architecture supports graph-based claim propagation, authorial credibility
modelling, cryptographic anchoring, and zero-knowledge audit verification. By
formalising scientific reasoning into a computationally verifiable epistemic
network, BEWA advances the foundation for machine reasoning systems that
promote truth utility, rational belief convergence, and audit-resilient
integrity across dynamic scientific domains.

</details>


### [14] [Dual-Objective Reinforcement Learning with Novel Hamilton-Jacobi-Bellman Formulations](https://arxiv.org/abs/2506.16016)
*William Sharpless,Dylan Hirsch,Sander Tonkens,Nikhil Shinde,Sylvia Herbert*

Main category: cs.AI

TL;DR: 论文提出两种新的价值函数，用于解决强化学习中的双目标约束问题，并通过改进的PPO算法（DO-HJ-PPO）实现。


<details>
  <summary>Details</summary>
Motivation: 传统强化学习中的硬约束会降低策略性能，拉格朗日方法需要复杂的奖励工程和参数调优，因此需要更高效的方法。

Method: 将Hamilton-Jacobi方程与强化学习结合，提出两种新的价值函数，分别解决Reach-Always-Avoid和Reach-Reach问题，并开发DO-HJ-PPO算法。

Result: DO-HJ-PPO在安全到达和多目标达成任务中表现优于基线方法，并产生独特的行为。

Conclusion: 该研究为约束决策提供了新视角，并通过DO-HJ-PPO算法实现了高效的双目标约束解决。

Abstract: Hard constraints in reinforcement learning (RL), whether imposed via the
reward function or the model architecture, often degrade policy performance.
Lagrangian methods offer a way to blend objectives with constraints, but often
require intricate reward engineering and parameter tuning. In this work, we
extend recent advances that connect Hamilton-Jacobi (HJ) equations with RL to
propose two novel value functions for dual-objective satisfaction. Namely, we
address: (1) the Reach-Always-Avoid problem - of achieving distinct reward and
penalty thresholds - and (2) the Reach-Reach problem - of achieving thresholds
of two distinct rewards. In contrast with temporal logic approaches, which
typically involve representing an automaton, we derive explicit, tractable
Bellman forms in this context by decomposing our problem into reach, avoid, and
reach-avoid problems, as to leverage these aforementioned recent advances. From
a mathematical perspective, the Reach-Always-Avoid and Reach-Reach problems are
complementary and fundamentally different from standard sum-of-rewards problems
and temporal logic problems, providing a new perspective on constrained
decision-making. We leverage our analysis to propose a variation of Proximal
Policy Optimization (DO-HJ-PPO), which solves these problems. Across a range of
tasks for safe-arrival and multi-target achievement, we demonstrate that
DO-HJ-PPO produces qualitatively distinct behaviors from previous approaches
and out-competes a number of baselines in various metrics.

</details>


### [15] [OSWorld-Human: Benchmarking the Efficiency of Computer-Use Agents](https://arxiv.org/abs/2506.16042)
*Reyna Abhyankar,Qi Qi,Yiying Zhang*

Main category: cs.AI

TL;DR: 生成式AI在桌面应用任务中表现高效，但现有系统因高延迟（如数十分钟）而实际不可用。研究首次分析了计算机代理的时间性能，发现规划和反思的大模型调用是延迟主因。构建了人工标注数据集OSWorld-Human，评估显示高效代理仍需1.4-2.7倍多余步骤。


<details>
  <summary>Details</summary>
Motivation: 现有生成式AI系统在桌面任务中延迟过高，难以实用，需研究其时间性能以指导未来开发。

Method: 在OSWorld基准上分析计算机代理的时间性能，构建人工标注数据集OSWorld-Human，评估16种代理的效率。

Result: 规划和反思的大模型调用是延迟主因，高效代理仍需1.4-2.7倍多余步骤。

Conclusion: 未来计算机代理开发需优化规划和反思步骤以减少延迟和冗余。

Abstract: Generative AI is being leveraged to solve a variety of computer-use tasks
involving desktop applications. State-of-the-art systems have focused solely on
improving accuracy on leading benchmarks. However, these systems are
practically unusable due to extremely high end-to-end latency (e.g., tens of
minutes) for tasks that typically take humans just a few minutes to complete.
To understand the cause behind this and to guide future developments of
computer agents, we conduct the first study on the temporal performance of
computer-use agents on OSWorld, the flagship benchmark in computer-use AI. We
find that large model calls for planning and reflection account for the
majority of the overall latency, and as an agent uses more steps to complete a
task, each successive step can take 3x longer than steps at the beginning of a
task. We then construct OSWorld-Human, a manually annotated version of the
original OSWorld dataset that contains a human-determined trajectory for each
task. We evaluate 16 agents on their efficiency using OSWorld-Human and found
that even the highest-scoring agents on OSWorld take 1.4-2.7x more steps than
necessary.

</details>


### [16] [Consistency Verification in Ontology-Based Process Models with Parameter Interdependencies](https://arxiv.org/abs/2506.16087)
*Tom Jeleniewski,Hamied Nabizada,Jonathan Reif,Felix Gehlhoff,Alexander Fay*

Main category: cs.AI

TL;DR: 本文提出了一套验证机制，用于基于本体的过程模型，支持跨上下文的数据检索和数学表达式的正确性验证。


<details>
  <summary>Details</summary>
Motivation: 制造过程中参数依赖关系的建模需要一致且语义清晰的模型，以确保数据检索和解释的正确性。

Method: 提出三种验证机制：(i) SPARQL过滤检索过程相关数据，(ii) 单位一致性检查，(iii) 数据完整性验证。

Result: 通过树脂传递模塑（RTM）的用例验证了方法的适用性。

Conclusion: 该方法支持开发机器可解释且可验证的工程模型。

Abstract: The formalization of process knowledge using ontologies enables consistent
modeling of parameter interdependencies in manufacturing. These
interdependencies are typically represented as mathematical expressions that
define relations between process parameters, supporting tasks such as
calculation, validation, and simulation. To support cross-context application
and knowledge reuse, such expressions are often defined in a generic form and
applied across multiple process contexts. This highlights the necessity of a
consistent and semantically coherent model to ensure the correctness of data
retrieval and interpretation. Consequently, dedicated mechanisms are required
to address key challenges such as selecting context-relevant data, ensuring
unit compatibility between variables and data elements, and verifying the
completeness of input data required for evaluating mathematical expressions.
This paper presents a set of verification mechanisms for a previously developed
ontology-based process model that integrates standardized process semantics,
data element definitions, and formal mathematical constructs. The approach
includes (i) SPARQL-based filtering to retrieve process-relevant data, (ii) a
unit consistency check based on expected-unit annotations and semantic
classification, and (iii) a data completeness check to validate the
evaluability of interdependencies. The applicability of the approach is
demonstrated with a use case from Resin Transfer Molding (RTM), supporting the
development of machine-interpretable and verifiable engineering models.

</details>


### [17] [Geometric Learning in Black-Box Optimization: A GNN Framework for Algorithm Performance Prediction](https://arxiv.org/abs/2506.16144)
*Ana Kostovska,Carola Doerr,Sašo Džeroski,Panče Panov,Tome Eftimov*

Main category: cs.AI

TL;DR: 该论文提出了一种基于图神经网络的方法，用于预测黑盒优化算法的性能，通过捕捉问题、算法配置和性能之间的复杂关系，相比传统表格方法提升了36.6%的MSE。


<details>
  <summary>Details</summary>
Motivation: 传统基于表格的算法性能预测方法忽略了算法配置的影响，而问题、算法配置和性能之间的关系更适合用图结构表示。

Method: 使用异构图数据结构和图神经网络，对两种模块化框架（modCMA-ES和modDE）的变体在BBOB问题上进行性能预测。

Result: 在324种modCMA-ES和576种modDE变体上测试，MSE比传统方法提升高达36.6%。

Conclusion: 几何学习在黑盒优化中具有潜力，图神经网络能有效捕捉复杂依赖关系。

Abstract: Automated algorithm performance prediction in numerical blackbox optimization
often relies on problem characterizations, such as exploratory landscape
analysis features. These features are typically used as inputs to machine
learning models and are represented in a tabular format. However, such
approaches often overlook algorithm configurations, a key factor influencing
performance. The relationships between algorithm operators, parameters, problem
characteristics, and performance outcomes form a complex structure best
represented as a graph. This work explores the use of heterogeneous graph data
structures and graph neural networks to predict the performance of optimization
algorithms by capturing the complex dependencies between problems, algorithm
configurations, and performance outcomes. We focus on two modular frameworks,
modCMA-ES and modDE, which decompose two widely used derivative-free
optimization algorithms: the covariance matrix adaptation evolution strategy
(CMA-ES) and differential evolution (DE). We evaluate 324 modCMA-ES and 576
modDE variants on 24 BBOB problems across six runtime budgets and two problem
dimensions. Achieving up to 36.6% improvement in MSE over traditional
tabular-based methods, this work highlights the potential of geometric learning
in black-box optimization.

</details>


### [18] [Large Language Models are Near-Optimal Decision-Makers with a Non-Human Learning Behavior](https://arxiv.org/abs/2506.16163)
*Hao Li,Gengrui Zhang,Petter Holme,Shuyue Hu,Zhen Wang*

Main category: cs.AI

TL;DR: 研究比较了大型语言模型（LLMs）与人类在不确定性、风险和任务转换三个维度的决策行为，发现LLMs表现优于人类，但决策机制与人类存在根本差异。


<details>
  <summary>Details</summary>
Motivation: 探索LLMs在决策行为中的表现及其与人类的差异，以评估其作为人类决策替代品的潜在风险。

Method: 使用三种心理学任务对五个领先LLMs和360名人类参与者进行测试，比较其在不确定性、风险和任务转换中的表现。

Result: LLMs在任务中表现优于人类，接近最优水平，但其决策机制与人类存在根本差异。

Conclusion: LLMs在决策能力上表现出色，但作为人类决策替代品存在风险，需进一步研究。

Abstract: Human decision-making belongs to the foundation of our society and
civilization, but we are on the verge of a future where much of it will be
delegated to artificial intelligence. The arrival of Large Language Models
(LLMs) has transformed the nature and scope of AI-supported decision-making;
however, the process by which they learn to make decisions, compared to humans,
remains poorly understood. In this study, we examined the decision-making
behavior of five leading LLMs across three core dimensions of real-world
decision-making: uncertainty, risk, and set-shifting. Using three
well-established experimental psychology tasks designed to probe these
dimensions, we benchmarked LLMs against 360 newly recruited human participants.
Across all tasks, LLMs often outperformed humans, approaching near-optimal
performance. Moreover, the processes underlying their decisions diverged
fundamentally from those of humans. On the one hand, our finding demonstrates
the ability of LLMs to manage uncertainty, calibrate risk, and adapt to
changes. On the other hand, this disparity highlights the risks of relying on
them as substitutes for human judgment, calling for further inquiry.

</details>


### [19] [Approximation Fixpoint Theory with Refined Approximation Spaces](https://arxiv.org/abs/2506.16294)
*Linde Vanbesien,Bart Bogaerts,Marc Denecker*

Main category: cs.AI

TL;DR: 本文扩展了近似不动点理论（AFT），通过引入更精细的近似空间，克服了其在某些简单例子中的局限性。


<details>
  <summary>Details</summary>
Motivation: AFT在非单调推理形式中广泛应用，但在某些简单例子中存在局限性，需要更精细的近似方法。

Method: 引入更一般的近似空间概念，研究不同近似空间之间的关系。

Result: 展示了改进的表达能力，并成功克服了AFT的局限性。

Conclusion: 扩展的AFT为更广泛的非单调推理形式提供了更灵活和强大的理论支持。

Abstract: Approximation Fixpoint Theory (AFT) is a powerful theory covering various
semantics of non-monotonic reasoning formalisms in knowledge representation
such as Logic Programming and Answer Set Programming. Many semantics of such
non-monotonic formalisms can be characterized as suitable fixpoints of a
non-monotonic operator on a suitable lattice. Instead of working on the
original lattice, AFT operates on intervals in such lattice to approximate or
construct the fixpoints of interest. While AFT has been applied successfully
across a broad range of non-monotonic reasoning formalisms, it is confronted by
its limitations in other, relatively simple, examples. In this paper, we
overcome those limitations by extending consistent AFT to deal with
approximations that are more refined than intervals. Therefore, we introduce a
more general notion of approximation spaces, showcase the improved
expressiveness and investigate relations between different approximation
spaces.

</details>


### [20] [Explainable Rule Application via Structured Prompting: A Neural-Symbolic Approach](https://arxiv.org/abs/2506.16335)
*Albert Sadowski,Jarosław A. Chudziak*

Main category: cs.AI

TL;DR: 论文提出了一种结构化提示框架，通过分解推理步骤（实体识别、属性提取和符号规则应用）结合神经与符号方法，提升LLMs在逻辑一致性任务中的表现，并在法律分析任务中显著优于基线。


<details>
  <summary>Details</summary>
Motivation: 解决LLMs在需要自然语言理解和精确逻辑推理的领域（如法律分析）中规则应用不一致、异常处理困难和解释性不足的问题。

Method: 采用结构化提示框架，将推理分解为三个可验证步骤，结合神经与符号方法，并通过形式化验证确保逻辑一致性。

Result: 在LegalBench传闻判定任务中，OpenAI o系列模型表现显著提升（o1 F1分数0.929，o3-mini 0.867），远超基线（0.714和0.74）。

Conclusion: 该混合神经符号系统为透明且一致的基于规则的推理提供了可行路径，适用于结构化法律推理任务的可解释AI应用。

Abstract: Large Language Models (LLMs) excel in complex reasoning tasks but struggle
with consistent rule application, exception handling, and explainability,
particularly in domains like legal analysis that require both natural language
understanding and precise logical inference. This paper introduces a structured
prompting framework that decomposes reasoning into three verifiable steps:
entity identification, property extraction, and symbolic rule application. By
integrating neural and symbolic approaches, our method leverages LLMs'
interpretive flexibility while ensuring logical consistency through formal
verification. The framework externalizes task definitions, enabling domain
experts to refine logical structures without altering the architecture.
Evaluated on the LegalBench hearsay determination task, our approach
significantly outperformed baselines, with OpenAI o-family models showing
substantial improvements - o1 achieving an F1 score of 0.929 and o3-mini
reaching 0.867 using structured decomposition with complementary predicates,
compared to their few-shot baselines of 0.714 and 0.74 respectively. This
hybrid neural-symbolic system offers a promising pathway for transparent and
consistent rule-based reasoning, suggesting potential for explainable AI
applications in structured legal reasoning tasks.

</details>


### [21] [IS-Bench: Evaluating Interactive Safety of VLM-Driven Embodied Agents in Daily Household Tasks](https://arxiv.org/abs/2506.16402)
*Xiaoya Lu,Zeren Chen,Xuhao Hu,Yijin Zhou,Weichen Zhang,Dongrui Liu,Lu Sheng,Jing Shao*

Main category: cs.AI

TL;DR: IS-Bench是一个多模态基准测试，用于评估VLM驱动的具身代理的交互安全性，揭示当前代理在风险感知和缓解方面的不足。


<details>
  <summary>Details</summary>
Motivation: 现有静态评估方法无法捕捉动态风险，阻碍了具身代理在现实任务中的安全部署。

Method: 提出IS-Bench，包含161个场景和388个风险，支持过程导向评估，验证风险缓解步骤的顺序。

Result: 实验显示当前代理缺乏交互安全意识，安全感知的Chain-of-Thought虽能提升性能，但可能影响任务完成。

Conclusion: IS-Bench为开发更安全的具身AI系统奠定了基础。

Abstract: Flawed planning from VLM-driven embodied agents poses significant safety
hazards, hindering their deployment in real-world household tasks. However,
existing static, non-interactive evaluation paradigms fail to adequately assess
risks within these interactive environments, since they cannot simulate dynamic
risks that emerge from an agent's actions and rely on unreliable post-hoc
evaluations that ignore unsafe intermediate steps. To bridge this critical gap,
we propose evaluating an agent's interactive safety: its ability to perceive
emergent risks and execute mitigation steps in the correct procedural order. We
thus present IS-Bench, the first multi-modal benchmark designed for interactive
safety, featuring 161 challenging scenarios with 388 unique safety risks
instantiated in a high-fidelity simulator. Crucially, it facilitates a novel
process-oriented evaluation that verifies whether risk mitigation actions are
performed before/after specific risk-prone steps. Extensive experiments on
leading VLMs, including the GPT-4o and Gemini-2.5 series, reveal that current
agents lack interactive safety awareness, and that while safety-aware
Chain-of-Thought can improve performance, it often compromises task completion.
By highlighting these critical limitations, IS-Bench provides a foundation for
developing safer and more reliable embodied AI systems.

</details>


### [22] [Agentic Personalisation of Cross-Channel Marketing Experiences](https://arxiv.org/abs/2506.16429)
*Sami Abboud,Eleanor Hanna,Olivier Jeunen,Vineesha Raheja,Schaun Wheeler*

Main category: cs.AI

TL;DR: 论文提出了一种基于序列决策框架的自动化通信编排方法，替代传统的人工营销工作，通过个性化优化提升用户参与度。


<details>
  <summary>Details</summary>
Motivation: 传统通信编排依赖人工，难以实现内容、时间、频率和文案的个性化优化。

Method: 采用差分设计估计个体处理效应，结合Thompson采样平衡探索与利用。

Result: 在多服务应用中显著提升了多种目标事件，已部署于1.5亿用户。

Conclusion: 该方法有效提升了用户参与度，并实现了规模化部署。

Abstract: Consumer applications provide ample opportunities to surface and communicate
various forms of content to users. From promotional campaigns for new features
or subscriptions, to evergreen nudges for engagement, or personalised
recommendations; across e-mails, push notifications, and in-app surfaces. The
conventional approach to orchestration for communication relies heavily on
labour-intensive manual marketer work, and inhibits effective personalisation
of content, timing, frequency, and copy-writing. We formulate this task under a
sequential decision-making framework, where we aim to optimise a modular
decision-making policy that maximises incremental engagement for any funnel
event. Our approach leverages a Difference-in-Differences design for Individual
Treatment Effect estimation, and Thompson sampling to balance the
explore-exploit trade-off. We present results from a multi-service application,
where our methodology has resulted in significant increases to a variety of
goal events across several product features, and is currently deployed across
150 million users.

</details>


### [23] [ML-Master: Towards AI-for-AI via Integration of Exploration and Reasoning](https://arxiv.org/abs/2506.16499)
*Zexi Liu,Yuzhu Cai,Xinyu Zhu,Yujie Zheng,Runkun Chen,Ying Wen,Yanfeng Wang,Weinan E,Siheng Chen*

Main category: cs.AI

TL;DR: ML-Master是一种新型AI4AI代理，通过选择性记忆机制整合探索与推理，显著提升性能。


<details>
  <summary>Details</summary>
Motivation: AI驱动的开发效率可能超越人类，但现有LLM代理无法充分利用探索经验，导致效率低下。

Method: 提出ML-Master，采用选择性记忆机制，高效整合并行解决方案的多样性与分析推理。

Result: 在MLE-Bench上，ML-Master平均奖牌率提升29.3%，且在12小时内完成，优于基线。

Conclusion: ML-Master展示了作为AI4AI强大工具的潜力。

Abstract: As AI capabilities advance toward and potentially beyond human-level
performance, a natural transition emerges where AI-driven development becomes
more efficient than human-centric approaches. A promising pathway toward this
transition lies in AI-for-AI (AI4AI), which leverages AI techniques to automate
and optimize the design, training, and deployment of AI systems themselves.
While LLM-based agents have shown the potential to realize AI4AI, they are
often unable to fully leverage the experience accumulated by agents during the
exploration of solutions in the reasoning process, leading to inefficiencies
and suboptimal performance. To address this limitation, we propose ML-Master, a
novel AI4AI agent that seamlessly integrates exploration and reasoning by
employing a selectively scoped memory mechanism. This approach allows ML-Master
to efficiently combine diverse insights from parallel solution trajectories
with analytical reasoning, guiding further exploration without overwhelming the
agent with excessive context. We evaluate ML-Master on the MLE-Bench, where it
achieves a 29.3% average medal rate, significantly surpassing existing methods,
particularly in medium-complexity tasks, while accomplishing this superior
performance within a strict 12-hour time constraint-half the 24-hour limit used
by previous baselines. These results demonstrate ML-Master's potential as a
powerful tool for advancing AI4AI.

</details>


### [24] [Advancing Harmful Content Detection in Organizational Research: Integrating Large Language Models with Elo Rating System](https://arxiv.org/abs/2506.16575)
*Mustafa Akben,Aaron Satko*

Main category: cs.AI

TL;DR: 本文提出了一种基于Elo评分的改进方法，用于提升大语言模型（LLMs）在有害内容分析中的性能，尤其在微侵犯和仇恨言论检测中表现优于传统方法。


<details>
  <summary>Details</summary>
Motivation: LLMs的内置审核系统在分析有害内容时可能拒绝执行指令或生成过于谨慎的响应，影响研究有效性。

Method: 采用Elo评分方法优化LLMs在有害内容分析中的表现。

Result: 在两个数据集中（微侵犯和仇恨言论检测），该方法在准确性、精确度和F1分数上优于传统LLM提示技术和常规机器学习模型。

Conclusion: 该方法提高了有害内容分析的可靠性，减少了误报，适用于大规模数据集，支持组织应用如工作场所骚扰检测和促进包容性环境。

Abstract: Large language models (LLMs) offer promising opportunities for organizational
research. However, their built-in moderation systems can create problems when
researchers try to analyze harmful content, often refusing to follow certain
instructions or producing overly cautious responses that undermine validity of
the results. This is particularly problematic when analyzing organizational
conflicts such as microaggressions or hate speech. This paper introduces an Elo
rating-based method that significantly improves LLM performance for harmful
content analysis In two datasets, one focused on microaggression detection and
the other on hate speech, we find that our method outperforms traditional LLM
prompting techniques and conventional machine learning models on key measures
such as accuracy, precision, and F1 scores. Advantages include better
reliability when analyzing harmful content, fewer false positives, and greater
scalability for large-scale datasets. This approach supports organizational
applications, including detecting workplace harassment, assessing toxic
communication, and fostering safer and more inclusive work environments.

</details>


### [25] [A Community-driven vision for a new Knowledge Resource for AI](https://arxiv.org/abs/2506.16596)
*Vinay K Chaudhri,Chaitan Baru,Brandon Bennett,Mehul Bhatt,Darion Cassel,Anthony G Cohn,Rina Dechter,Esra Erdem,Dave Ferrucci,Ken Forbus,Gregory Gelfond,Michael Genesereth,Andrew S. Gordon,Benjamin Grosof,Gopal Gupta,Jim Hendler,Sharat Israni,Tyler R. Josephson,Patrick Kyllonen,Yuliya Lierler,Vladimir Lifschitz,Clifton McFate,Hande K. McGinty,Leora Morgenstern,Alessandro Oltramari,Praveen Paritosh,Dan Roth,Blake Shepard,Cogan Shimzu,Denny Vrandečić,Mark Whiting,Michael Witbrock*

Main category: cs.AI

TL;DR: 论文探讨了AI领域对全面、多用途知识资源的需求，提出了社区驱动的知识基础设施愿景，并建议利用现代技术构建开放工程框架。


<details>
  <summary>Details</summary>
Motivation: 尽管已有WordNet、ConceptNet等知识资源，但AI仍缺乏可验证、通用的知识来源，导致语言模型、机器人规划和事实检测等领域存在问题。

Method: 通过AAAI研讨会收集50多位研究者的意见，结合知识表示与推理的现代进展，提出构建开放工程框架的设想。

Result: 提出了社区驱动的知识基础设施愿景，强调利用知识模块和实际应用结合的框架。

Conclusion: 需要开发开放、协作的知识基础设施，结合现代技术和社会结构，以解决AI中的知识缺口问题。

Abstract: The long-standing goal of creating a comprehensive, multi-purpose knowledge
resource, reminiscent of the 1984 Cyc project, still persists in AI. Despite
the success of knowledge resources like WordNet, ConceptNet, Wolfram|Alpha and
other commercial knowledge graphs, verifiable, general-purpose widely available
sources of knowledge remain a critical deficiency in AI infrastructure. Large
language models struggle due to knowledge gaps; robotic planning lacks
necessary world knowledge; and the detection of factually false information
relies heavily on human expertise. What kind of knowledge resource is most
needed in AI today? How can modern technology shape its development and
evaluation? A recent AAAI workshop gathered over 50 researchers to explore
these questions. This paper synthesizes our findings and outlines a
community-driven vision for a new knowledge infrastructure. In addition to
leveraging contemporary advances in knowledge representation and reasoning, one
promising idea is to build an open engineering framework to exploit knowledge
modules effectively within the context of practical applications. Such a
framework should include sets of conventions and social structures that are
adopted by contributors.

</details>


### [26] [The Role of Explanation Styles and Perceived Accuracy on Decision Making in Predictive Process Monitoring](https://arxiv.org/abs/2506.16617)
*Soobin Chae,Suhwan Lee,Hanna Hauptmann,Hajo A. Reijers,Xixi Lu*

Main category: cs.AI

TL;DR: 该研究探讨了在预测性流程监控（PPM）中，解释风格和感知AI准确性对用户决策的影响，发现两者均显著影响决策。


<details>
  <summary>Details</summary>
Motivation: 尽管深度学习模型在PPM中准确性高，但其缺乏可解释性降低了用户信任。当前XAI评估多关注功能指标，忽视了用户中心的决策影响。

Method: 通过决策实验，比较不同解释风格（特征重要性、基于规则、反事实）和感知准确性（高/低）对用户决策的影响。

Result: 感知准确性和解释风格显著影响用户的任务表现、一致性和决策信心。

Conclusion: 研究强调了在PPM中结合用户中心视角评估XAI的重要性。

Abstract: Predictive Process Monitoring (PPM) often uses deep learning models to
predict the future behavior of ongoing processes, such as predicting process
outcomes. While these models achieve high accuracy, their lack of
interpretability undermines user trust and adoption. Explainable AI (XAI) aims
to address this challenge by providing the reasoning behind the predictions.
However, current evaluations of XAI in PPM focus primarily on functional
metrics (such as fidelity), overlooking user-centered aspects such as their
effect on task performance and decision-making. This study investigates the
effects of explanation styles (feature importance, rule-based, and
counterfactual) and perceived AI accuracy (low or high) on decision-making in
PPM. We conducted a decision-making experiment, where users were presented with
the AI predictions, perceived accuracy levels, and explanations of different
styles. Users' decisions were measured both before and after receiving
explanations, allowing the assessment of objective metrics (Task Performance
and Agreement) and subjective metrics (Decision Confidence). Our findings show
that perceived accuracy and explanation style have a significant effect.

</details>


### [27] [Interpretable Low-Dimensional Modeling of Spatiotemporal Agent States for Decision Making in Football Tactics](https://arxiv.org/abs/2506.16696)
*Kenjiro Ide,Taiga Someya,Kohei Kawaguchi,Keisuke Fujii*

Main category: cs.AI

TL;DR: 研究探讨了低维、基于规则的模型是否能有效捕捉足球战术，通过可解释的状态变量和实际数据训练模型，发现球员与球的距离及空间评分是关键因素。


<details>
  <summary>Details</summary>
Motivation: 现有模型计算成本高或缺乏可解释性，且未充分考虑球员状态，因此需要一种更高效且直观的方法来分析足球战术。

Method: 定义了球持有者和潜在接球者的可解释状态变量，使用StatsBomb和SkillCorner数据训练XGBoost模型预测传球成功率。

Result: 球员与球的距离及空间评分是决定传球成功的关键因素。

Conclusion: 低维可解释模型为足球战术分析提供了实用工具，支持决策制定。

Abstract: Understanding football tactics is crucial for managers and analysts. Previous
research has proposed models based on spatial and kinematic equations, but
these are computationally expensive. Also, Reinforcement learning approaches
use player positions and velocities but lack interpretability and require large
datasets. Rule-based models align with expert knowledge but have not fully
considered all players' states. This study explores whether low-dimensional,
rule-based models using spatiotemporal data can effectively capture football
tactics. Our approach defines interpretable state variables for both the
ball-holder and potential pass receivers, based on criteria that explore
options like passing. Through discussions with a manager, we identified key
variables representing the game state. We then used StatsBomb event data and
SkillCorner tracking data from the 2023$/$24 LaLiga season to train an XGBoost
model to predict pass success. The analysis revealed that the distance between
the player and the ball, as well as the player's space score, were key factors
in determining successful passes. Our interpretable low-dimensional modeling
facilitates tactical analysis through the use of intuitive variables and
provides practical value as a tool to support decision-making in football.

</details>


### [28] [Incentivizing High-quality Participation From Federated Learning Agents](https://arxiv.org/abs/2506.16731)
*Jinlong Pang,Jiaheng Wei,Yifan Hua,Chen Qian,Yang Liu*

Main category: cs.AI

TL;DR: 提出了一种考虑数据异质性的激励感知联邦学习框架，通过Wasserstein距离量化异质性，并设计两阶段Stackelberg博弈模型确保真实参与。


<details>
  <summary>Details</summary>
Motivation: 现有联邦学习研究假设参与者自愿无私，但实际中自私的参与者可能退出或提供低质量贡献，且现有方法忽略了数据异质性导致的努力差异。

Method: 引入Wasserstein距离量化数据异质性，改进收敛上界；利用同伴预测机制设计评分函数；提出两阶段Stackelberg博弈模型。

Result: 实验证明所提机制在真实数据集上有效。

Conclusion: 该框架解决了参与者激励和数据异质性问题，加速了收敛过程。

Abstract: Federated learning (FL) provides a promising paradigm for facilitating
collaboration between multiple clients that jointly learn a global model
without directly sharing their local data. However, existing research suffers
from two caveats: 1) From the perspective of agents, voluntary and unselfish
participation is often assumed. But self-interested agents may opt out of the
system or provide low-quality contributions without proper incentives; 2) From
the mechanism designer's perspective, the aggregated models can be
unsatisfactory as the existing game-theoretical federated learning approach for
data collection ignores the potential heterogeneous effort caused by
contributed data. To alleviate above challenges, we propose an incentive-aware
framework for agent participation that considers data heterogeneity to
accelerate the convergence process. Specifically, we first introduce the notion
of Wasserstein distance to explicitly illustrate the heterogeneous effort and
reformulate the existing upper bound of convergence. To induce truthful
reporting from agents, we analyze and measure the generalization error gap of
any two agents by leveraging the peer prediction mechanism to develop score
functions. We further present a two-stage Stackelberg game model that
formalizes the process and examines the existence of equilibrium. Extensive
experiments on real-world datasets demonstrate the effectiveness of our
proposed mechanism.

</details>


### [29] [Reinforcement learning for hybrid charging stations planning and operation considering fixed and mobile chargers](https://arxiv.org/abs/2506.16764)
*Yanchen Zhu,Honghui Zou,Chufan Liu,Yuyu Luo,Yuankai Wu,Yuxuan Liang*

Main category: cs.AI

TL;DR: 本文提出了一种混合充电基础设施（固定和移动充电器）的优化规划与运营方法，通过深度强化学习和启发式调度技术，显著提升了充电设施的可用性并减少了用户不便。


<details>
  <summary>Details</summary>
Motivation: 传统固定充电站因需求动态性导致利用率不均或拥堵，移动充电器作为灵活解决方案，需与固定充电器协同优化。

Method: 结合模型预测控制（MPC）的需求预测模型，提出深度强化学习方法，辅以启发式调度技术，解决混合充电站规划与运营问题（HCSPO）。

Result: 实际案例研究表明，该方法显著提高了充电设施的可用性，减少了用户不便。

Conclusion: 混合充电基础设施的优化方法有效解决了动态需求下的充电问题，为城市充电网络提供了可行方案。

Abstract: The success of vehicle electrification, which brings significant societal and
environmental benefits, is contingent upon the availability of efficient and
adaptable charging infrastructure. Traditional fixed-location charging stations
often face issues like underutilization or congestion due to the dynamic nature
of charging demand. Mobile chargers have emerged as a flexible solution,
capable of relocating to align with these demand fluctuations. This paper
addresses the optimal planning and operation of hybrid charging
infrastructures, integrating both fixed and mobile chargers within urban road
networks. We introduce the Hybrid Charging Station Planning and Operation
(HCSPO) problem, which simultaneously optimizes the location and configuration
of fixed charging stations and schedules mobile chargers for dynamic
operations. Our approach incorporates a charging demand prediction model
grounded in Model Predictive Control (MPC) to enhance decision-making. To solve
the HCSPO problem, we propose a deep reinforcement learning method, augmented
with heuristic scheduling techniques, to effectively bridge the planning of
fixed chargers with the real-time operation of mobile chargers. Extensive case
studies using real-world urban scenarios demonstrate that our method
significantly improves the availability of charging infrastructure and reduces
user inconvenience compared to existing solutions and baselines.

</details>


### [30] [AI's Blind Spots: Geographic Knowledge and Diversity Deficit in Generated Urban Scenario](https://arxiv.org/abs/2506.16898)
*Ciro Beneduce,Massimiliano Luca,Bruno Lepri*

Main category: cs.AI

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Image generation models are revolutionizing many domains, and urban analysis
and design is no exception. While such models are widely adopted, there is a
limited literature exploring their geographic knowledge, along with the biases
they embed. In this work, we generated 150 synthetic images for each state in
the USA and related capitals using FLUX 1 and Stable Diffusion 3.5, two
state-of-the-art models for image generation. We embed each image using DINO-v2
ViT-S/14 and the Fr\'echet Inception Distances to measure the similarity
between the generated images. We found that while these models have implicitly
learned aspects of USA geography, if we prompt the models to generate an image
for "United States" instead of specific cities or states, the models exhibit a
strong representative bias toward metropolis-like areas, excluding rural states
and smaller cities. {\color{black} In addition, we found that models
systematically exhibit some entity-disambiguation issues with European-sounding
names like Frankfort or Devon.

</details>


### [31] [Real-Time Black-Box Optimization for Dynamic Discrete Environments Using Embedded Ising Machines](https://arxiv.org/abs/2506.16924)
*Tomoya Kashimata,Yohei Hamakawa,Masaya Yamasaki,Kosuke Tatsumura*

Main category: cs.AI

TL;DR: 提出了一种启发式多臂老虎机（MAB）方法，用于动态离散环境，通过扩展黑盒优化（BBO）方法，利用伊辛机有效探索动作并考虑变量间的相互作用和动态环境变化。


<details>
  <summary>Details</summary>
Motivation: 动态离散环境中的实时系统需要优化离散变量，但传统MAB算法因组合爆炸无法有效处理。

Method: 扩展BBO方法，利用伊辛机探索动作，同时考虑变量间相互作用和动态环境变化。

Result: 在移动用户的无线通信系统中验证了方法的动态适应性。

Conclusion: 该方法在动态离散环境中表现出良好的适应性和优化能力。

Abstract: Many real-time systems require the optimization of discrete variables.
Black-box optimization (BBO) algorithms and multi-armed bandit (MAB) algorithms
perform optimization by repeatedly taking actions and observing the
corresponding instant rewards without any prior knowledge. Recently, a BBO
method using an Ising machine has been proposed to find the best action that is
represented by a combination of discrete values and maximizes the instant
reward in static environments. In contrast, dynamic environments, where
real-time systems operate, necessitate MAB algorithms that maximize the average
reward over multiple trials. However, due to the enormous number of actions
resulting from the combinatorial nature of discrete optimization, conventional
MAB algorithms cannot effectively optimize dynamic, discrete environments.
Here, we show a heuristic MAB method for dynamic, discrete environments by
extending the BBO method, in which an Ising machine effectively explores the
actions while considering interactions between variables and changes in dynamic
environments. We demonstrate the dynamic adaptability of the proposed method in
a wireless communication system with moving users.

</details>


### [32] [Multimodal Fused Learning for Solving the Generalized Traveling Salesman Problem in Robotic Task Planning](https://arxiv.org/abs/2506.16931)
*Jiaqi Chen,Mingfeng Fan,Xuefeng Zhang,Jingsong Liang,Yuhong Cao,Guohua Wu,Guillaume Adrien Sartoretti*

Main category: cs.AI

TL;DR: 提出了一种多模态融合学习（MMFL）框架，通过结合图和图像表示解决广义旅行商问题（GTSP），实现高效任务规划。


<details>
  <summary>Details</summary>
Motivation: 移动机器人任务规划（如仓库检索和环境监测）需高效解决GTSP，现有方法难以兼顾准确性和效率。

Method: 引入坐标图像构建器、自适应分辨率缩放策略和多模态融合模块，整合几何与空间特征。

Result: MMFL在多种GTSP实例中显著优于现有方法，并保持实时计算效率，物理机器人测试验证了其实际效果。

Conclusion: MMFL框架为GTSP提供了一种高效且准确的解决方案，适用于实时机器人应用。

Abstract: Effective and efficient task planning is essential for mobile robots,
especially in applications like warehouse retrieval and environmental
monitoring. These tasks often involve selecting one location from each of
several target clusters, forming a Generalized Traveling Salesman Problem
(GTSP) that remains challenging to solve both accurately and efficiently. To
address this, we propose a Multimodal Fused Learning (MMFL) framework that
leverages both graph and image-based representations to capture complementary
aspects of the problem, and learns a policy capable of generating high-quality
task planning schemes in real time. Specifically, we first introduce a
coordinate-based image builder that transforms GTSP instances into spatially
informative representations. We then design an adaptive resolution scaling
strategy to enhance adaptability across different problem scales, and develop a
multimodal fusion module with dedicated bottlenecks that enables effective
integration of geometric and spatial features. Extensive experiments show that
our MMFL approach significantly outperforms state-of-the-art methods across
various GTSP instances while maintaining the computational efficiency required
for real-time robotic applications. Physical robot tests further validate its
practical effectiveness in real-world scenarios.

</details>


### [33] [Elevating Styled Mahjong Agents with Learning from Demonstration](https://arxiv.org/abs/2506.16995)
*Lingfeng Li,Yunlong Lu,Yongyi Wang,Wenxin Li*

Main category: cs.AI

TL;DR: 本文提出了一种新的基于示范学习（LfD）的算法，用于提升麻将游戏中AI代理的熟练度并保持其独特玩法风格。


<details>
  <summary>Details</summary>
Motivation: 现有离线学习和示范学习算法在麻将游戏的高随机性和分布外状态中表现不佳，需要改进。

Method: 利用现有麻将代理的游戏历史，提出了一种仅需对PPO算法进行最小修改的新型LfD算法。

Result: 实验结果表明，该方法显著提升了代理的熟练度，同时有效保留了其独特玩法风格。

Conclusion: 该算法为开发高能力且多样化的游戏AI提供了一种有效方法。

Abstract: A wide variety of bots in games enriches the gameplay experience and enhances
replayability. Recent advancements in game artificial intelligence have
predominantly focused on improving the proficiency of bots. Nevertheless,
developing highly competent bots with a wide range of distinct play styles
remains a relatively under-explored area. We select the Mahjong game
environment as a case study. The high degree of randomness inherent in the
Mahjong game and the prevalence of out-of-distribution states lead to
suboptimal performance of existing offline learning and
Learning-from-Demonstration (LfD) algorithms. In this paper, we leverage the
gameplay histories of existing Mahjong agents and put forward a novel LfD
algorithm that necessitates only minimal modifications to the Proximal Policy
Optimization algorithm. The comprehensive empirical results illustrate that our
proposed method not only significantly enhances the proficiency of the agents
but also effectively preserves their unique play styles.

</details>


### [34] [A Quantile Regression Approach for Remaining Useful Life Estimation with State Space Models](https://arxiv.org/abs/2506.17018)
*Davide Frizzo,Francesco Borsatti,Gian Antonio Susto*

Main category: cs.AI

TL;DR: 本文提出了一种基于状态空间模型（SSM）和同步分位数回归（SQR）的新方法，用于预测设备剩余使用寿命（RUL），在C-MAPSS数据集上表现优于传统序列建模技术。


<details>
  <summary>Details</summary>
Motivation: 预测性维护（PdM）在工业4.0和5.0中至关重要，通过准确预测RUL优化维护计划，减少意外故障和过早干预。

Method: 结合状态空间模型（SSM）和同步分位数回归（SQR），实现长期序列建模和多分位数估计。

Result: SSM模型在C-MAPSS数据集上表现出更高的准确性和计算效率。

Conclusion: SSM模型在高风险工业应用中具有潜力。

Abstract: Predictive Maintenance (PdM) is pivotal in Industry 4.0 and 5.0, proactively
enhancing efficiency through accurate equipment Remaining Useful Life (RUL)
prediction, thus optimizing maintenance scheduling and reducing unexpected
failures and premature interventions. This paper introduces a novel RUL
estimation approach leveraging State Space Models (SSM) for efficient long-term
sequence modeling. To handle model uncertainty, Simoultaneous Quantile
Regression (SQR) is integrated into the SSM, enabling multiple quantile
estimations. The proposed method is benchmarked against traditional sequence
modelling techniques (LSTM, Transformer, Informer) using the C-MAPSS dataset.
Results demonstrate superior accuracy and computational efficiency of SSM
models, underscoring their potential for high-stakes industrial applications.

</details>


### [35] [Dispositions and Roles of Generically Dependent Entities](https://arxiv.org/abs/2506.17085)
*Fabian Neuhaus*

Main category: cs.AI

TL;DR: 论文指出BFO 2020不支持通用依赖持续体的功能、倾向和角色，提出了两种解决方法。


<details>
  <summary>Details</summary>
Motivation: BFO 2020无法充分表示通用依赖持续体（如软件或数据集）的功能和角色，限制了其应用。

Method: 讨论了BFO 2020的限制，并提出两种解决方案：(a)使用定义类，(b)修改BFO以支持这些功能。

Result: 提出了两种可能的解决方案，以弥补BFO 2020的不足。

Conclusion: 通过定义类或修改BFO，可以更好地支持通用依赖持续体的功能、倾向和角色。

Abstract: BFO 2020 does not support functions, dispositions, and roles of generically
dependent continuants (like software or datasets). In this paper, we argue that
this is a severe limitation, which prevents, for example, the adequate
representation of the functions of computer models or the various roles of
datasets during the execution of these models. We discuss the aspects of BFO
2020 that prevent the representation of realizable entities of generically
dependent continuants. Two approaches to address the issue are presented: (a)
the use of defined classes and (b) a proposal of changes that allow BFO to
support functions, dispositions, and roles of generically dependent
continuants.

</details>


### [36] [Towards Advanced Mathematical Reasoning for LLMs via First-Order Logic Theorem Proving](https://arxiv.org/abs/2506.17104)
*Chuxue Cao,Mengze Li,Juntao Dai,Jinluan Yang,Zijian Zhao,Shengyu Zhang,Weijie Shi,Chengzhong Liu,Sirui Han,Yike Guo*

Main category: cs.AI

TL;DR: 论文提出DREAM方法，通过增强LLMs的多样性和合理性，提升其在多步FOL推理中的表现。


<details>
  <summary>Details</summary>
Motivation: LLMs在多步FOL推理中表现不佳，尤其是在复杂数学推理中，需要解决策略单一和早期错误影响整体证明的问题。

Method: DREAM结合了Axiom-Driven Strategy Diversification机制和Sub-Proposition Error Feedback，以多样化策略并纠正错误。

Result: DREAM将性能提升了0.6%至6.4%，并提供了447个数学定理的数据集用于评估。

Conclusion: DREAM为LLMs在数学推理中的多步FOL任务提供了有效解决方案，并推动了相关研究的发展。

Abstract: Large language models (LLMs) have shown promising first-order logic (FOL)
reasoning capabilities with applications in various areas. However, their
effectiveness in complex mathematical reasoning involving multi-step FOL
deductions is still under-researched. While LLMs perform competitively on
established mathematical reasoning benchmarks, they struggle with multi-step
FOL tasks, as demonstrated by Deepseek-Prover-V2-7B's low accuracy (4.2%) on
our proposed theorem proving dataset. This issue arises from the limited
exploration of diverse proof strategies and the potential for early reasoning
mistakes to undermine entire proofs. To address these issues, we propose DREAM,
a self-adaptive solution that enhances the Diversity and REAsonability of LLMs'
generation strategies. DREAM incorporates an Axiom-Driven Strategy
Diversification mechanism to promote varied strategic outcomes and a
Sub-Proposition Error Feedback to help LLMs reflect on and correct their
proofs. Our contributions include pioneering advancements in LLMs' mathematical
reasoning through FOL theorem proving, introducing a novel inference stage
solution that improves performance by 0.6% to 6.4%, and providing a curated
dataset of 447 mathematical theorems in Lean 4 format for evaluation.

</details>


### [37] [Are Bias Evaluation Methods Biased ?](https://arxiv.org/abs/2506.17111)
*Lina Berrayana,Sean Rooney,Luis Garcés-Erice,Ioana Giurgiu*

Main category: cs.AI

TL;DR: 论文研究了大型语言模型安全评估基准的鲁棒性，发现不同评估方法导致模型排名差异显著，并提出了使用建议。


<details>
  <summary>Details</summary>
Motivation: 评估大型语言模型安全性的基准在可信AI社区中至关重要，但独立基准采用不同方法和数据集，需验证其鲁棒性。

Method: 通过不同方法对代表性模型进行偏见排名，比较整体排名的相似性。

Result: 广泛使用的偏见评估方法导致模型排名差异显著。

Conclusion: 建议社区在使用此类基准时注意方法差异，并提出改进方向。

Abstract: The creation of benchmarks to evaluate the safety of Large Language Models is
one of the key activities within the trusted AI community. These benchmarks
allow models to be compared for different aspects of safety such as toxicity,
bias, harmful behavior etc. Independent benchmarks adopt different approaches
with distinct data sets and evaluation methods. We investigate how robust such
benchmarks are by using different approaches to rank a set of representative
models for bias and compare how similar are the overall rankings. We show that
different but widely used bias evaluations methods result in disparate model
rankings. We conclude with recommendations for the community in the usage of
such benchmarks.

</details>


### [38] [Mathematical Proof as a Litmus Test: Revealing Failure Modes of Advanced Large Reasoning Models](https://arxiv.org/abs/2506.17114)
*Dadi Guo,Jiayu Liu,Zhiyuan Fan,Zhitao He,Haoran Li,Yumeng Wang,Yi R.,Fung*

Main category: cs.AI

TL;DR: 论文提出利用数学证明的严谨性作为诊断工具，揭示大型推理模型在数学问题中的隐藏缺陷，并通过RFMDataset评估模型表现，发现10种细粒度错误类型。


<details>
  <summary>Details</summary>
Motivation: 当前大型推理模型在数学问题上的高准确率掩盖了其实际推理缺陷，需通过数学证明的复杂性揭示这些隐藏问题。

Method: 引入RFMDataset（200个数学证明问题），评估模型表现并分析其错误类型。

Result: 模型在数学证明中表现不佳（正确率<20%），存在逻辑不严谨、幻觉和不完整性等问题。

Conclusion: 当前模型的自我反思不足以解决逻辑困境，需加强形式化和细粒度的逻辑训练。

Abstract: Large reasoning models (e.g., R1, o3) have demonstrated remarkable
mathematical problem-solving abilities. However, the high reported accuracy of
these advanced models on popular datasets, reliance on purely numerical
evaluation and potential benchmark leakage, often masks their true reasoning
shortcomings. To address this, we propose leveraging the inherent rigor and
methodological complexity of mathematical proofs as a diagnostic tool to expose
these hidden failures. Specifically, we introduce the RFMDataset (Reveal
Failure Modes), a collection of 200 diverse mathematical proof problems, and
thoroughly evaluate advanced models' performance on it. Our in-depth analysis
of their failures uncovers 10 fine-grained error types, which shows fundamental
limitations in current large reasoning models: 1) large reasoning models
grapple profoundly with mathematical proofs, with some generating entirely
correct proofs for less than 20% of problems and failing even on basic ones; 2)
models exhibit a diverse spectrum of reasoning failures, prominently
demonstrating the lack of guarantees for the correctness and rigor of
single-step reasoning; and 3) models show hallucination and incompleteness
during the reasoning process. Our findings reveal that models' self-reflection
is insufficient to resolve the current logical dilemmas, necessitating
formalized and fine-grained logical training.

</details>


### [39] [When Can Model-Free Reinforcement Learning be Enough for Thinking?](https://arxiv.org/abs/2506.17124)
*Josiah P. Hanna,Nicholas E. Corrado*

Main category: cs.AI

TL;DR: 本文探讨了模型无关强化学习（RL）如何通过“思考”策略最大化奖励，提出了“思想马尔可夫决策过程”（MDP）理论模型，并分析了其条件与效果。


<details>
  <summary>Details</summary>
Motivation: 研究模型无关RL如何在没有直接奖励或外部状态改变的情况下，通过“思考”行为实现奖励最大化。

Method: 引入“思想MDP”理论模型，分析策略初始化的影响，并通过实验验证开源大语言模型的条件。

Result: 理论证明“思考”行为等价于策略改进步骤，实验表明开源LLM满足相关条件。

Conclusion: 提出了在多任务预训练和指定思考动作下，RL能更高效学习“思考”行为的假设。

Abstract: Recent work on large language models has demonstrated the use of model-free
reinforcement learning (RL) to train reasoning-like capabilities. The emergence
of "thinking" through model-free RL is interesting as thinking actions neither
produce reward nor change the external world state to one where the agent is
more likely to get reward. This paper seeks to build a domain-independent
understanding of when model-free RL will lead to "thinking" as a strategy for
reward maximization. To build this understanding, we first introduce a
theoretical model which we call a \textit{thought Markov decision process}
(MDP). Thought MDPs minimally extend the classical MDP model to include an
abstract notion of thought state and thought action. Using the thought MDP
model, we prove the importance of policy initialization in determining whether
or not thinking emerges and show formally that thought actions are equivalent
to the agent choosing to perform a step of policy improvement before continuing
to act. We then show that open-source LLMs satisfy the conditions that our
theory predicts are necessary for model-free RL to produce thinking-like
behavior. Finally, we hypothesize sufficient conditions that would enable
thinking to be learned outside of language generation and introduce a toy
domain where a combination of multi-task pre-training and designated thought
actions enable more data-efficient RL compared to non-thinking agents.

</details>


### [40] [Chain-of-Trust: A Progressive Trust Evaluation Framework Enabled by Generative AI](https://arxiv.org/abs/2506.17130)
*Botao Zhu,Xianbin Wang,Lei Zhang,Xuemin,Shen*

Main category: cs.AI

TL;DR: 提出了一种名为chain-of-trust的渐进式信任评估框架，通过分阶段评估减少复杂性，利用生成式AI快速分析数据，实验证明其高准确性。


<details>
  <summary>Details</summary>
Motivation: 在依赖分布式资源的复杂协作系统中，信任评估对任务完成至关重要，但由于网络动态性和信息收集延迟，全面评估信任属性具有挑战性。

Method: 框架将信任评估分为多个链式阶段，每阶段仅收集相关设备属性数据，利用生成式AI（上下文学习、少样本学习和推理能力）快速分析数据，仅信任设备进入下一阶段。

Result: 实验结果表明，该框架在信任评估中实现了高准确性。

Conclusion: chain-of-trust框架通过分阶段和生成式AI的结合，有效降低了信任评估的复杂性，提高了任务完成的效率。

Abstract: In collaborative systems with complex tasks relying on distributed resources,
trust evaluation of potential collaborators has emerged as an effective
mechanism for task completion. However, due to the network dynamics and varying
information gathering latencies, it is extremely challenging to observe and
collect all trust attributes of a collaborating device concurrently for a
comprehensive trust assessment. In this paper, a novel progressive trust
evaluation framework, namely chain-of-trust, is proposed to make better use of
misaligned device attribute data. This framework, designed for effective task
completion, divides the trust evaluation process into multiple chained stages
based on task decomposition. At each stage, based on the task completion
process, the framework only gathers the latest device attribute data relevant
to that stage, leading to reduced trust evaluation complexity and overhead. By
leveraging advanced in-context learning, few-shot learning, and reasoning
capabilities, generative AI is then employed to analyze and interpret the
collected data to produce correct evaluation results quickly. Only devices
deemed trustworthy at this stage proceed to the next round of trust evaluation.
The framework ultimately determines devices that remain trustworthy across all
stages. Experimental results demonstrate that the proposed framework achieves
high accuracy in trust evaluation.

</details>


### [41] [The MedPerturb Dataset: What Non-Content Perturbations Reveal About Human and Clinical LLM Decision Making](https://arxiv.org/abs/2506.17163)
*Abinitha Gourabathina,Yuexing Hao,Walter Gerych,Marzyeh Ghassemi*

Main category: cs.AI

TL;DR: MedPerturb数据集用于评估医疗大语言模型（LLMs）在临床输入扰动下的表现，发现LLMs对性别和语言风格更敏感，而人类专家对格式变化更敏感。


<details>
  <summary>Details</summary>
Motivation: 研究医疗LLMs在真实临床环境中的稳健性，揭示其与人类决策的差异。

Method: 构建MedPerturb数据集，包含性别、风格和格式扰动的临床案例，对比LLMs和人类专家的反应。

Result: LLMs对性别和风格扰动更敏感，人类对格式变化更敏感。

Conclusion: 需开发动态评估框架，以更真实地模拟临床环境中的决策差异。

Abstract: Clinical robustness is critical to the safe deployment of medical Large
Language Models (LLMs), but key questions remain about how LLMs and humans may
differ in response to the real-world variability typified by clinical settings.
To address this, we introduce MedPerturb, a dataset designed to systematically
evaluate medical LLMs under controlled perturbations of clinical input.
MedPerturb consists of clinical vignettes spanning a range of pathologies, each
transformed along three axes: (1) gender modifications (e.g., gender-swapping
or gender-removal); (2) style variation (e.g., uncertain phrasing or colloquial
tone); and (3) format changes (e.g., LLM-generated multi-turn conversations or
summaries). With MedPerturb, we release a dataset of 800 clinical contexts
grounded in realistic input variability, outputs from four LLMs, and three
human expert reads per clinical context. We use MedPerturb in two case studies
to reveal how shifts in gender identity cues, language style, or format reflect
diverging treatment selections between humans and LLMs. We find that LLMs are
more sensitive to gender and style perturbations while human annotators are
more sensitive to LLM-generated format perturbations such as clinical
summaries. Our results highlight the need for evaluation frameworks that go
beyond static benchmarks to assess the similarity between human clinician and
LLM decisions under the variability characteristic of clinical settings.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [42] [Veracity: An Open-Source AI Fact-Checking System](https://arxiv.org/abs/2506.15794)
*Taylor Lynn Curtis,Maximilian Puelma Touzel,William Garneau,Manon Gruaz,Mike Pinder,Li Wei Wang,Sukanya Krishna,Luda Cohen,Jean-François Godbout,Reihaneh Rabbany,Kellin Pelrine*

Main category: cs.CL

TL;DR: Veracity是一个开源AI系统，结合大语言模型和网络检索代理，提供透明的事实核查，支持多语言和直观解释。


<details>
  <summary>Details</summary>
Motivation: 应对生成式AI加剧的虚假信息威胁，提升公众媒体素养。

Method: 利用大语言模型和网络检索代理分析用户提交的声明，提供基于证据的真实性评估。

Result: 系统支持多语言、提供数值评分和交互式界面，能检测虚假信息并解释推理过程。

Conclusion: Veracity通过透明的事实核查促进媒体素养，助力构建更知情的社会。

Abstract: The proliferation of misinformation poses a significant threat to society,
exacerbated by the capabilities of generative AI. This demo paper introduces
Veracity, an open-source AI system designed to empower individuals to combat
misinformation through transparent and accessible fact-checking. Veracity
leverages the synergy between Large Language Models (LLMs) and web retrieval
agents to analyze user-submitted claims and provide grounded veracity
assessments with intuitive explanations. Key features include multilingual
support, numerical scoring of claim veracity, and an interactive interface
inspired by familiar messaging applications. This paper will showcase
Veracity's ability to not only detect misinformation but also explain its
reasoning, fostering media literacy and promoting a more informed society.

</details>


### [43] [Rethinking LLM Training through Information Geometry and Quantum Metrics](https://arxiv.org/abs/2506.15830)
*Riccardo Di Sipio*

Main category: cs.CL

TL;DR: 论文探讨了在大型语言模型（LLMs）的高维参数空间中，利用信息几何（如Fisher信息度量）优化训练的方法，并展望了量子类比的可能性。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于通过几何视角（如Fisher信息度量）更深入地理解LLM训练中的现象（如尖锐最小值、泛化性和缩放规律），并探索量子类比对优化的潜在影响。

Method: 方法包括利用信息几何（如自然梯度下降）分析高维参数空间的非欧几里得结构，并探讨量子度量（如Fubini-Study度量和量子Fisher信息）的类比。

Result: 结果表明，几何视角能够更清晰地解释LLM训练中的现象，并为优化提供理论支持。

Conclusion: 结论指出，曲率感知方法有助于深化对LLM训练的理解，并暗示量子类比可能在量子增强系统中实现高效优化。

Abstract: Optimization in large language models (LLMs) unfolds over high-dimensional
parameter spaces with non-Euclidean structure. Information geometry frames this
landscape using the Fisher information metric, enabling more principled
learning via natural gradient descent. Though often impractical, this geometric
lens clarifies phenomena such as sharp minima, generalization, and observed
scaling laws. We argue that curvature-aware approaches deepen our understanding
of LLM training. Finally, we speculate on quantum analogies based on the
Fubini-Study metric and Quantum Fisher Information, hinting at efficient
optimization in quantum-enhanced systems.

</details>


### [44] [MEM1: Learning to Synergize Memory and Reasoning for Efficient Long-Horizon Agents](https://arxiv.org/abs/2506.15841)
*Zijian Zhou,Ao Qu,Zhaoxuan Wu,Sunghwan Kim,Alok Prakash,Daniela Rus,Jinhua Zhao,Bryan Kian Hsiang Low,Paul Pu Liang*

Main category: cs.CL

TL;DR: MEM1是一个强化学习框架，通过固定内存处理多轮任务，提升性能和效率。


<details>
  <summary>Details</summary>
Motivation: 现有LLM系统依赖全上下文提示，导致内存增长、计算成本增加和性能下降。

Method: MEM1通过更新紧凑的内部状态整合记忆和推理，丢弃无关信息，并构建多轮环境训练。

Result: MEM1-7B在性能提升3.5倍的同时减少内存使用3.7倍，并能泛化到训练范围之外。

Conclusion: MEM1展示了推理驱动记忆整合的潜力，为长周期交互代理提供高效解决方案。

Abstract: Modern language agents must operate over long-horizon, multi-turn
interactions, where they retrieve external information, adapt to observations,
and answer interdependent queries. Yet, most LLM systems rely on full-context
prompting, appending all past turns regardless of their relevance. This leads
to unbounded memory growth, increased computational costs, and degraded
reasoning performance on out-of-distribution input lengths. We introduce MEM1,
an end-to-end reinforcement learning framework that enables agents to operate
with constant memory across long multi-turn tasks. At each turn, MEM1 updates a
compact shared internal state that jointly supports memory consolidation and
reasoning. This state integrates prior memory with new observations from the
environment while strategically discarding irrelevant or redundant information.
To support training in more realistic and compositional settings, we propose a
simple yet effective and scalable approach to constructing multi-turn
environments by composing existing datasets into arbitrarily complex task
sequences. Experiments across three domains, including internal retrieval QA,
open-domain web QA, and multi-turn web shopping, show that MEM1-7B improves
performance by 3.5x while reducing memory usage by 3.7x compared to
Qwen2.5-14B-Instruct on a 16-objective multi-hop QA task, and generalizes
beyond the training horizon. Our results demonstrate the promise of
reasoning-driven memory consolidation as a scalable alternative to existing
solutions for training long-horizon interactive agents, where both efficiency
and performance are optimized.

</details>


### [45] [Finance Language Model Evaluation (FLaME)](https://arxiv.org/abs/2506.15846)
*Glenn Matlin,Mika Okamoto,Huzaifa Pardawala,Yang Yang,Sudheer Chava*

Main category: cs.CL

TL;DR: 论文提出了首个金融语言模型评估套件FLaME，全面研究了23个基础语言模型在20个金融NLP任务中的表现，并开源了框架软件和数据。


<details>
  <summary>Details</summary>
Motivation: 评估语言模型在金融领域知识密集型任务中的潜力，填补现有评估框架的不足。

Method: 开发了FLaME评估套件，对23个基础语言模型和‘推理增强’语言模型在20个金融NLP任务上进行实证研究。

Result: 展示了语言模型在金融NLP任务中的潜力，纠正了对其性能的低估。

Conclusion: FLaME为金融语言模型评估提供了全面工具，证明了语言模型在金融领域的应用潜力。

Abstract: Language Models (LMs) have demonstrated impressive capabilities with core
Natural Language Processing (NLP) tasks. The effectiveness of LMs for highly
specialized knowledge-intensive tasks in finance remains difficult to assess
due to major gaps in the methodologies of existing evaluation frameworks, which
have caused an erroneous belief in a far lower bound of LMs' performance on
common Finance NLP (FinNLP) tasks. To demonstrate the potential of LMs for
these FinNLP tasks, we present the first holistic benchmarking suite for
Financial Language Model Evaluation (FLaME). We are the first research paper to
comprehensively study LMs against 'reasoning-reinforced' LMs, with an empirical
study of 23 foundation LMs over 20 core NLP tasks in finance. We open-source
our framework software along with all data and results.

</details>


### [46] [Entropy-Driven Pre-Tokenization for Byte-Pair Encoding](https://arxiv.org/abs/2506.15889)
*Yifan Hu,Frank Liang,Dachuan Zhao,Jonathan Geuter,Varshini Reddy,Craig W. Schmidt,Chris Tanner*

Main category: cs.CL

TL;DR: 论文提出两种基于熵的预分词策略，改进BPE在中文等未分词语言中的表现，显著提升分词质量。


<details>
  <summary>Details</summary>
Motivation: BPE在未分词语言（如中文）中因忽略语言边界而表现不佳，需改进。

Method: 提出两种熵引导的预分词策略：1）利用点互信息和左右熵识别连贯字符；2）利用GPT-2预测熵检测边界不确定性。

Result: 在PKU数据集上，两种方法显著提升分词精度、召回率和F1分数。

Conclusion: 熵引导的预分词策略能更好对齐语言学单位，为低资源或多语言场景提供改进方向。

Abstract: Byte-Pair Encoding (BPE) has become a widely adopted subword tokenization
method in modern language models due to its simplicity and strong empirical
performance across downstream tasks. However, applying BPE to unsegmented
languages such as Chinese presents significant challenges, as its
frequency-driven merge operation is agnostic to linguistic boundaries. To
address this, we propose two entropy-informed pre-tokenization strategies that
guide BPE segmentation using unsupervised information-theoretic cues. The first
approach uses pointwise mutual information and left/right entropy to identify
coherent character spans, while the second leverages predictive entropy derived
from a pretrained GPT-2 model to detect boundary uncertainty. We evaluate both
methods on a subset of the PKU dataset and demonstrate substantial improvements
in segmentation precision, recall, and F1 score compared to standard BPE. Our
results suggest that entropy-guided pre-tokenization not only enhances
alignment with gold-standard linguistic units but also offers a promising
direction for improving tokenization quality in low-resource and multilingual
settings.

</details>


### [47] [GeoGuess: Multimodal Reasoning based on Hierarchy of Visual Information in Street View](https://arxiv.org/abs/2506.16633)
*Fenghua Cheng,Jinxiang Wang,Sen Wang,Zi Huang,Xue Li*

Main category: cs.CL

TL;DR: 论文提出了一种名为GeoGuess的多模态推理任务，要求通过街景图像识别位置并提供详细解释，同时提出了数据集GeoExplain和方法SightSense。


<details>
  <summary>Details</summary>
Motivation: 现有的多模态推理任务在层次化视觉线索推理方面存在不足，GeoGuess旨在填补这一空白。

Method: 提出了SightSense方法，结合层次化视觉信息和外部知识进行推理和解释生成。

Result: 实验表明，SightSense在GeoGuess任务中表现优异。

Conclusion: GeoGuess为多模态推理提供了新挑战，SightSense方法展示了其在层次化推理中的有效性。

Abstract: Multimodal reasoning is a process of understanding, integrating and inferring
information across different data modalities. It has recently attracted surging
academic attention as a benchmark for Artificial Intelligence (AI). Although
there are various tasks for evaluating multimodal reasoning ability, they still
have limitations. Lack of reasoning on hierarchical visual clues at different
levels of granularity, e.g., local details and global context, is of little
discussion, despite its frequent involvement in real scenarios. To bridge the
gap, we introduce a novel and challenging task for multimodal reasoning, namely
GeoGuess. Given a street view image, the task is to identify its location and
provide a detailed explanation. A system that succeeds in GeoGuess should be
able to detect tiny visual clues, perceive the broader landscape, and associate
with vast geographic knowledge. Therefore, GeoGuess would require the ability
to reason between hierarchical visual information and geographic knowledge. In
this work, we establish a benchmark for GeoGuess by introducing a specially
curated dataset GeoExplain which consists of
panoramas-geocoordinates-explanation tuples. Additionally, we present a
multimodal and multilevel reasoning method, namely SightSense which can make
prediction and generate comprehensive explanation based on hierarchy of visual
information and external knowledge. Our analysis and experiments demonstrate
their outstanding performance in GeoGuess.

</details>


### [48] [Language Models can perform Single-Utterance Self-Correction of Perturbed Reasoning](https://arxiv.org/abs/2506.15894)
*Sam Silver,Jimin Sun,Ivan Zhang,Sara Hooker,Eddie Kim*

Main category: cs.CL

TL;DR: 研究发现大型语言模型（LLMs）具有内在的自我纠正能力，即使未经专门训练，也能在推理过程中自我修正错误。


<details>
  <summary>Details</summary>
Motivation: LLMs在数学推理中表现脆弱，对问题描述和提示策略的微小变化敏感，且易受采样误差影响。研究旨在探索其自我纠正能力。

Method: 通过实验测量模型对合成扰动的自我纠正能力，观察其在Chain of Thought（CoT）推理中的表现。

Result: 发现多种模型在单次推理中表现出稳健的自我纠正行为，包括隐式和显式修正。

Conclusion: LLMs的内在自我纠正能力可能比文献中显示的更强，表明当前推理研究可能放大了模型已有的能力。

Abstract: Large Language Models (LLMs) have demonstrated impressive mathematical
reasoning capabilities, yet their performance remains brittle to minor
variations in problem description and prompting strategy. Furthermore,
reasoning is vulnerable to sampling-induced errors which autoregressive models
must primarily address using self-correction via additionally-generated tokens.
To better understand self-correction capabilities of recent models, we conduct
experiments measuring models' ability to self-correct synthetic perturbations
introduced into their Chain of Thought (CoT) reasoning. We observe robust
single-utterance intrinsic self-correction behavior across a range of
open-weight models and datasets, ranging from subtle, implicit corrections to
explicit acknowledgments and corrections of errors. Our findings suggest that
LLMs, including those not finetuned for long CoT, may possess stronger
intrinsic self-correction capabilities than commonly shown in the literature.
The presence of this ability suggests that recent "reasoning" model work
involves amplification of traits already meaningfully present in models.

</details>


### [49] [From RAG to Agentic: Validating Islamic-Medicine Responses with LLM Agents](https://arxiv.org/abs/2506.15911)
*Mohammad Amaan Sayeed,Mohammed Talha Alam,Raza Imam,Shahab Saquib Sohail,Amir Hussain*

Main category: cs.CL

TL;DR: 论文提出Tibbe-AG评估框架，结合伊斯兰医学文本、检索增强生成和自评过滤，提升AI在文化敏感医学问答中的表现。


<details>
  <summary>Details</summary>
Motivation: 伊斯兰医学文本如《医典》和《先知医学》蕴含丰富预防和整体疗法知识，但未被现代AI充分利用，现有评估标准过于狭窄。

Method: 提出Tibbe-AG框架，结合30个精选问题、人类验证疗法，测试三种LLM（LLaMA-3、Mistral-7B、Qwen2-7B）在直接生成、检索增强生成和自评过滤下的表现。

Result: 检索提升事实准确性13%，自评提示进一步改善10%，结合传统文本与AI技术可实现可靠、文化敏感的医学问答。

Conclusion: 融合古典伊斯兰医学、检索与自评技术，能有效提升AI在文化敏感医学领域的表现。

Abstract: Centuries-old Islamic medical texts like Avicenna's Canon of Medicine and the
Prophetic Tibb-e-Nabawi encode a wealth of preventive care, nutrition, and
holistic therapies, yet remain inaccessible to many and underutilized in modern
AI systems. Existing language-model benchmarks focus narrowly on factual recall
or user preference, leaving a gap in validating culturally grounded medical
guidance at scale. We propose a unified evaluation pipeline, Tibbe-AG, that
aligns 30 carefully curated Prophetic-medicine questions with human-verified
remedies and compares three LLMs (LLaMA-3, Mistral-7B, Qwen2-7B) under three
configurations: direct generation, retrieval-augmented generation, and a
scientific self-critique filter. Each answer is then assessed by a secondary
LLM serving as an agentic judge, yielding a single 3C3H quality score.
Retrieval improves factual accuracy by 13%, while the agentic prompt adds
another 10% improvement through deeper mechanistic insight and safety
considerations. Our results demonstrate that blending classical Islamic texts
with retrieval and self-evaluation enables reliable, culturally sensitive
medical question-answering.

</details>


### [50] [Reranking-based Generation for Unbiased Perspective Summarization](https://arxiv.org/abs/2506.15925)
*Narutatsu Ri,Nicholas Deas,Kathleen McKeown*

Main category: cs.CL

TL;DR: 论文研究了如何生成无偏见的政治观点摘要，提出了可靠的评估指标，并验证了基于LLM的方法优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 现有评估框架依赖传统指标，但未验证其适用性，且改进摘要生成方法的研究尚不成熟。

Method: 通过构建测试集验证指标可靠性，比较传统指标与基于语言模型的指标，并研究基于LLM的重新排序和偏好调优方法。

Result: 基于语言模型的指标表现优于传统指标，重新排序和偏好调优方法显著提升了摘要质量。

Conclusion: 研究为政治观点摘要的可靠评估和方法开发提供了贡献。

Abstract: Generating unbiased summaries in real-world settings such as political
perspective summarization remains a crucial application of Large Language
Models (LLMs). Yet, existing evaluation frameworks rely on traditional metrics
for measuring key attributes such as coverage and faithfulness without
verifying their applicability, and efforts to develop improved summarizers are
still nascent. We address these gaps by (1) identifying reliable metrics for
measuring perspective summary quality, and (2) investigating the efficacy of
LLM-based methods beyond zero-shot inference. Namely, we build a test set for
benchmarking metric reliability using human annotations and show that
traditional metrics underperform compared to language model-based metrics,
which prove to be strong evaluators. Using these metrics, we show that
reranking-based methods yield strong results, and preference tuning with
synthetically generated and reranking-labeled data further boosts performance.
Our findings aim to contribute to the reliable evaluation and development of
perspective summarization methods.

</details>


### [51] [A Vietnamese Dataset for Text Segmentation and Multiple Choices Reading Comprehension](https://arxiv.org/abs/2506.15978)
*Toan Nguyen Hai,Ha Nguyen Viet,Truong Quan Xuan,Duc Do Minh*

Main category: cs.CL

TL;DR: 论文介绍了越南语文本分割和阅读理解数据集VSMRC，实验表明多语言模型（如mBERT）在越南语任务中表现优于单语模型。


<details>
  <summary>Details</summary>
Motivation: 越南语作为第20大语言，缺乏自然语言处理任务的资源，如文本分割和阅读理解。

Method: 从越南维基百科构建数据集，包括15,942份文本分割文档和16,347个人工验证的多选题对。

Result: mBERT在阅读理解测试集上准确率为88.01%，文本分割测试集F1得分为63.15%。

Conclusion: 多语言模型在越南语任务中表现优异，可推广至其他资源匮乏语言。

Abstract: Vietnamese, the 20th most spoken language with over 102 million native
speakers, lacks robust resources for key natural language processing tasks such
as text segmentation and machine reading comprehension (MRC). To address this
gap, we present VSMRC, the Vietnamese Text Segmentation and Multiple-Choice
Reading Comprehension Dataset. Sourced from Vietnamese Wikipedia, our dataset
includes 15,942 documents for text segmentation and 16,347 synthetic
multiple-choice question-answer pairs generated with human quality assurance,
ensuring a reliable and diverse resource. Experiments show that mBERT
consistently outperforms monolingual models on both tasks, achieving an
accuracy of 88.01% on MRC test set and an F1 score of 63.15\% on text
segmentation test set. Our analysis reveals that multilingual models excel in
NLP tasks for Vietnamese, suggesting potential applications to other
under-resourced languages. VSMRC is available at HuggingFace

</details>


### [52] [Double Entendre: Robust Audio-Based AI-Generated Lyrics Detection via Multi-View Fusion](https://arxiv.org/abs/2506.15981)
*Markus Frohmann,Gabriel Meseguer-Brocal,Markus Schedl,Elena V. Epure*

Main category: cs.CL

TL;DR: 提出了一种多模态、模块化的晚期融合方法DE-detect，结合音频中的歌词转录和语音特征，以更鲁棒地检测AI生成的音乐。


<details>
  <summary>Details</summary>
Motivation: AI音乐生成工具的快速发展对艺术家、版权持有者和服务提供商带来挑战，现有检测方法（基于音频或歌词）存在局限性。

Method: 采用多模态、模块化的晚期融合管道，结合自动转录的歌词和音频中的语音特征。

Result: DE-detect优于现有基于歌词的检测器，且对音频扰动更具鲁棒性。

Conclusion: 该方法为现实场景中检测AI生成音乐提供了有效且鲁棒的解决方案。

Abstract: The rapid advancement of AI-based music generation tools is revolutionizing
the music industry but also posing challenges to artists, copyright holders,
and providers alike. This necessitates reliable methods for detecting such
AI-generated content. However, existing detectors, relying on either audio or
lyrics, face key practical limitations: audio-based detectors fail to
generalize to new or unseen generators and are vulnerable to audio
perturbations; lyrics-based methods require cleanly formatted and accurate
lyrics, unavailable in practice. To overcome these limitations, we propose a
novel, practically grounded approach: a multimodal, modular late-fusion
pipeline that combines automatically transcribed sung lyrics and speech
features capturing lyrics-related information within the audio. By relying on
lyrical aspects directly from audio, our method enhances robustness, mitigates
susceptibility to low-level artifacts, and enables practical applicability.
Experiments show that our method, DE-detect, outperforms existing lyrics-based
detectors while also being more robust to audio perturbations. Thus, it offers
an effective, robust solution for detecting AI-generated music in real-world
scenarios. Our code is available at
https://github.com/deezer/robust-AI-lyrics-detection.

</details>


### [53] [From General to Targeted Rewards: Surpassing GPT-4 in Open-Ended Long-Context Generation](https://arxiv.org/abs/2506.16024)
*Zhihan Guo,Jiele Wu,Wenqian Cui,Yifei Zhang,Minda Hu,Yufei Wang,Irwin King*

Main category: cs.CL

TL;DR: 论文提出ProxyReward框架，通过强化学习解决长文本生成任务中缺乏高质量参考数据的问题，显著提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 当前研究主要关注长上下文理解，而开放长文本生成任务缺乏高质量参考数据，现有方法仅使用通用评估信号，限制了准确性。

Method: 提出ProxyReward框架，包括自动生成数据集和针对性奖励信号计算方法。

Result: ProxyReward在开放长文本生成任务上性能提升20%，超越GPT-4-Turbo和LLM-as-a-Judge方法。

Conclusion: ProxyReward有效提升LLMs处理复杂开放问题的能力。

Abstract: Current research on long-form context in Large Language Models (LLMs)
primarily focuses on the understanding of long-contexts, the Open-ended Long
Text Generation (Open-LTG) remains insufficiently explored. Training a
long-context generation model requires curation of gold standard reference
data, which is typically nonexistent for informative Open-LTG tasks. However,
previous methods only utilize general assessments as reward signals, which
limits accuracy. To bridge this gap, we introduce ProxyReward, an innovative
reinforcement learning (RL) based framework, which includes a dataset and a
reward signal computation method. Firstly, ProxyReward Dataset generation is
accomplished through simple prompts that enables the model to create
automatically, obviating extensive labeled data or significant manual effort.
Secondly, ProxyReward Signal offers a targeted evaluation of information
comprehensiveness and accuracy for specific questions. The experimental results
indicate that our method ProxyReward surpasses even GPT-4-Turbo. It can
significantly enhance performance by 20% on the Open-LTG task when training
widely used open-source models, while also surpassing the LLM-as-a-Judge
approach. Our work presents effective methods to enhance the ability of LLMs to
address complex open-ended questions posed by human.

</details>


### [54] [EvoLM: In Search of Lost Language Model Training Dynamics](https://arxiv.org/abs/2506.16029)
*Zhenting Qi,Fan Nie,Alexandre Alahi,James Zou,Himabindu Lakkaraju,Yilun Du,Eric Xing,Sham Kakade,Hanlin Zhang*

Main category: cs.CL

TL;DR: EvoLM是一个模型套件，用于系统分析语言模型在不同训练阶段的动态，包括预训练、持续预训练、监督微调和强化学习。通过训练100多个模型，揭示了过度预训练和后期训练的收益递减、持续预训练的重要性以及微调与强化学习的权衡。


<details>
  <summary>Details</summary>
Motivation: 现代语言模型训练分为多个阶段，下游开发者难以评估每个阶段设计选择的影响，因此需要透明和系统的分析工具。

Method: 训练了100多个1B和4B参数的模型，评估上游（语言建模）和下游（问题解决）能力，包括领域内外泛化。

Result: 发现过度预训练和后期训练收益递减，持续预训练对缓解遗忘和衔接前后阶段至关重要，微调与强化学习存在复杂权衡。

Conclusion: EvoLM为开放研究和可重复性提供了所有模型、数据集和评估流程，揭示了语言模型训练的关键见解。

Abstract: Modern language model (LM) training has been divided into multiple stages,
making it difficult for downstream developers to evaluate the impact of design
choices made at each stage. We present EvoLM, a model suite that enables
systematic and transparent analysis of LMs' training dynamics across
pre-training, continued pre-training, supervised fine-tuning, and reinforcement
learning. By training over 100 LMs with 1B and 4B parameters from scratch, we
rigorously evaluate both upstream (language modeling) and downstream
(problem-solving) reasoning capabilities, including considerations of both
in-domain and out-of-domain generalization. Key insights highlight the
diminishing returns from excessive pre-training and post-training, the
importance and practices of mitigating forgetting during domain-specific
continued pre-training, the crucial role of continued pre-training in bridging
pre-training and post-training phases, and various intricate trade-offs when
configuring supervised fine-tuning and reinforcement learning. To facilitate
open research and reproducibility, we release all pre-trained and post-trained
models, training datasets for all stages, and our entire training and
evaluation pipeline.

</details>


### [55] [Enhancing Document-Level Question Answering via Multi-Hop Retrieval-Augmented Generation with LLaMA 3](https://arxiv.org/abs/2506.16037)
*Xinyue Huang,Ziqi Lin,Fang Sun,Wenchao Zhang,Kejian Tong,Yunbo Liu*

Main category: cs.CL

TL;DR: 提出了一种基于LLaMA 3的检索增强生成（RAG）框架，用于复杂问答任务，解决了多跳推理和长文档上下文理解的挑战。


<details>
  <summary>Details</summary>
Motivation: 解决复杂问答任务中多跳推理和长文档上下文理解的难题。

Method: 结合密集检索模块、高级上下文融合和多跳推理机制，采用检索似然与生成交叉熵联合优化策略。

Result: 实验表明，该系统优于现有检索增强和生成基线，能提供更精确、上下文相关的答案。

Conclusion: 该框架在复杂问答任务中表现出色，验证了其有效性。

Abstract: This paper presents a novel Retrieval-Augmented Generation (RAG) framework
tailored for complex question answering tasks, addressing challenges in
multi-hop reasoning and contextual understanding across lengthy documents.
Built upon LLaMA 3, the framework integrates a dense retrieval module with
advanced context fusion and multi-hop reasoning mechanisms, enabling more
accurate and coherent response generation. A joint optimization strategy
combining retrieval likelihood and generation cross-entropy improves the
model's robustness and adaptability. Experimental results show that the
proposed system outperforms existing retrieval-augmented and generative
baselines, confirming its effectiveness in delivering precise, contextually
grounded answers.

</details>


### [56] [DynScaling: Efficient Verifier-free Inference Scaling via Dynamic and Integrated Sampling](https://arxiv.org/abs/2506.16043)
*Fei Wang,Xingchen Wan,Ruoxi Sun,Jiefeng Chen,Sercan Ö. Arık*

Main category: cs.CL

TL;DR: DynScaling通过集成并行-顺序采样策略和动态预算分配框架，提升大语言模型性能，无需外部验证器。


<details>
  <summary>Details</summary>
Motivation: 现有推理时间扩展方法依赖外部验证器或未优化实际计算约束，限制了其实际应用。

Method: 提出DynScaling，结合并行-顺序采样策略（构建合成推理链）和动态预算分配框架（多臂老虎机问题）。

Result: 实验显示DynScaling在任务性能和计算成本上均优于现有无验证器基线。

Conclusion: DynScaling在资源约束下有效提升LLM性能，无需外部验证器。

Abstract: Inference-time scaling has proven effective in boosting large language model
(LLM) performance through increased test-time computation. Yet, its practical
application is often hindered by reliance on external verifiers or a lack of
optimization for realistic computational constraints. We propose DynScaling,
which addresses these limitations through two primary innovations: an
integrated parallel-sequential sampling strategy and a bandit-based dynamic
budget allocation framework. The integrated sampling strategy unifies parallel
and sequential sampling by constructing synthetic sequential reasoning chains
from initially independent parallel responses, promoting diverse and coherent
reasoning trajectories. The dynamic budget allocation framework formulates the
allocation of computational resources as a multi-armed bandit problem,
adaptively distributing the inference budget across queries based on the
uncertainty of previously sampled responses, thereby maximizing computational
efficiency. By combining these components, DynScaling effectively improves LLM
performance under practical resource constraints without the need for external
verifiers. Experimental results demonstrate that DynScaling consistently
surpasses existing verifier-free inference scaling baselines in both task
performance and computational cost.

</details>


### [57] [A Hybrid DeBERTa and Gated Broad Learning System for Cyberbullying Detection in English Text](https://arxiv.org/abs/2506.16052)
*Devesh Kumar*

Main category: cs.CL

TL;DR: 提出了一种结合Transformer和GBLS的混合架构，用于高效检测网络欺凌，性能优于现有方法，并具备可解释性。


<details>
  <summary>Details</summary>
Motivation: 网络欺凌对青少年影响严重（54.4%），现有检测方法需提升性能和透明度。

Method: 结合改进的DeBERTa（含SE块和情感分析）与GBLS分类器，形成混合框架。

Result: 在四个数据集上表现优异（HateXplain 79.3%，SOSNet 95.41%，Mendeley-I 91.37%，Mendeley-II 94.67%）。

Conclusion: 框架性能优越且透明，但检测隐含偏见和讽刺内容仍有挑战，为未来改进提供方向。

Abstract: The proliferation of online communication platforms has created unprecedented
opportunities for global connectivity while simultaneously enabling harmful
behaviors such as cyberbullying, which affects approximately 54.4\% of
teenagers according to recent research. This paper presents a hybrid
architecture that combines the contextual understanding capabilities of
transformer-based models with the pattern recognition strengths of broad
learning systems for effective cyberbullying detection. This approach
integrates a modified DeBERTa model augmented with Squeeze-and-Excitation
blocks and sentiment analysis capabilities with a Gated Broad Learning System
(GBLS) classifier, creating a synergistic framework that outperforms existing
approaches across multiple benchmark datasets. The proposed ModifiedDeBERTa +
GBLS model achieved good performance on four English datasets: 79.3\% accuracy
on HateXplain, 95.41\% accuracy on SOSNet, 91.37\% accuracy on Mendeley-I, and
94.67\% accuracy on Mendeley-II. Beyond performance gains, the framework
incorporates comprehensive explainability mechanisms including token-level
attribution analysis, LIME-based local interpretations, and confidence
calibration, addressing critical transparency requirements in automated content
moderation. Ablation studies confirm the meaningful contribution of each
architectural component, while failure case analysis reveals specific
challenges in detecting implicit bias and sarcastic content, providing valuable
insights for future improvements in cyberbullying detection systems.

</details>


### [58] [Advancing Automated Speaking Assessment Leveraging Multifaceted Relevance and Grammar Information](https://arxiv.org/abs/2506.16285)
*Hao-Chien Lu,Jhen-Ke Lin,Hong-Yun Lin,Chung-Chun Wang,Berlin Chen*

Main category: cs.CL

TL;DR: 本文提出了一种混合评分模型，通过多方面的相关性模块和细粒度语法错误特征，显著提升了自动口语评估系统的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的自动口语评估系统在多方面评估中未能充分利用内容相关性，且语法分析较为肤浅。

Method: 引入多方面的相关性模块和细粒度的语法错误特征，构建混合评分模型。

Result: 实验表明，这些改进显著提升了内容相关性、语言使用及整体评估性能。

Conclusion: 使用更丰富、更细致的特征集有助于实现更全面的口语评估。

Abstract: Current automated speaking assessment (ASA) systems for use in multi-aspect
evaluations often fail to make full use of content relevance, overlooking image
or exemplar cues, and employ superficial grammar analysis that lacks detailed
error types. This paper ameliorates these deficiencies by introducing two novel
enhancements to construct a hybrid scoring model. First, a multifaceted
relevance module integrates question and the associated image content,
exemplar, and spoken response of an L2 speaker for a comprehensive assessment
of content relevance. Second, fine-grained grammar error features are derived
using advanced grammar error correction (GEC) and detailed annotation to
identify specific error categories. Experiments and ablation studies
demonstrate that these components significantly improve the evaluation of
content relevance, language use, and overall ASA performance, highlighting the
benefits of using richer, more nuanced feature sets for holistic speaking
assessment.

</details>


### [59] [Knee-Deep in C-RASP: A Transformer Depth Hierarchy](https://arxiv.org/abs/2506.16055)
*Andy Yang,Michaël Cadilhac,David Chiang*

Main category: cs.CL

TL;DR: 论文通过理论证明和实证研究，探讨了更深层的Transformer模型在表达能力上的优势，并验证了其在实际任务中的表现。


<details>
  <summary>Details</summary>
Motivation: 研究动机是明确更深层的Transformer模型在表达能力上的具体增益。

Method: 方法包括理论证明（将Transformer子类与C-RASP编程语言等价性联系）和实证研究（验证理论预测）。

Result: 结果显示更深层的Transformer模型表达能力更强，且理论预测与实际任务表现一致。

Conclusion: 结论是更深层的Transformer模型在表达能力上具有优势，且理论结果可指导实际应用。

Abstract: It has been observed that transformers with greater depth (that is, more
layers) have more capabilities, but can we establish formally which
capabilities are gained with greater depth? We answer this question with a
theoretical proof followed by an empirical study. First, we consider
transformers that round to fixed precision except inside attention. We show
that this subclass of transformers is expressively equivalent to the
programming language C-RASP and this equivalence preserves depth. Second, we
prove that deeper C-RASP programs are more expressive than shallower C-RASP
programs, implying that deeper transformers are more expressive than shallower
transformers (within the subclass mentioned above). These results are
established by studying a form of temporal logic with counting operators, which
was shown equivalent to C-RASP in previous work. Finally, we provide empirical
evidence that our theory predicts the depth required for transformers without
positional encodings to length-generalize on a family of sequential dependency
tasks.

</details>


### [60] [Self-Critique-Guided Curiosity Refinement: Enhancing Honesty and Helpfulness in Large Language Models via In-Context Learning](https://arxiv.org/abs/2506.16064)
*Duc Hieu Ho,Chenglin Fan*

Main category: cs.CL

TL;DR: 论文提出了一种自批判引导的好奇心优化提示策略，通过无需额外训练的方式提升大语言模型的诚实性和帮助性。


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型在多种任务中表现优异，但其输出的诚实性和帮助性仍存在挑战。

Method: 通过自批判和优化步骤扩展好奇心驱动提示策略，并评估了十种主流模型。

Result: 实验显示，该方法显著减少了低质量回答，提高了高质量回答比例，H²分数相对提升1.4%至4.3%。

Conclusion: 结构化自优化是一种可扩展且无需训练的策略，能有效提升大语言模型输出的可信度。

Abstract: Large language models (LLMs) have demonstrated robust capabilities across
various natural language tasks. However, producing outputs that are
consistently honest and helpful remains an open challenge. To overcome this
challenge, this paper tackles the problem through two complementary directions.
It conducts a comprehensive benchmark evaluation of ten widely used large
language models, including both proprietary and open-weight models from OpenAI,
Meta, and Google. In parallel, it proposes a novel prompting strategy,
self-critique-guided curiosity refinement prompting. The key idea behind this
strategy is enabling models to self-critique and refine their responses without
additional training. The proposed method extends the curiosity-driven prompting
strategy by incorporating two lightweight in-context steps including
self-critique step and refinement step.
  The experiment results on the HONESET dataset evaluated using the framework
$\mathrm{H}^2$ (honesty and helpfulness), which was executed with GPT-4o as a
judge of honesty and helpfulness, show consistent improvements across all
models. The approach reduces the number of poor-quality responses, increases
high-quality responses, and achieves relative gains in $\mathrm{H}^2$ scores
ranging from 1.4% to 4.3% compared to curiosity-driven prompting across
evaluated models. These results highlight the effectiveness of structured
self-refinement as a scalable and training-free strategy to improve the
trustworthiness of LLMs outputs.

</details>


### [61] [InstructTTSEval: Benchmarking Complex Natural-Language Instruction Following in Text-to-Speech Systems](https://arxiv.org/abs/2506.16381)
*Kexin Huang,Qian Tu,Liwei Fan,Chenchen Yang,Dong Zhang,Shimin Li,Zhaoye Fei,Qinyuan Cheng,Xipeng Qiu*

Main category: cs.CL

TL;DR: 论文介绍了InstructTTSEval基准，用于评估基于自然语言指令的语音合成系统在复杂风格控制上的能力，并指出当前系统仍有改进空间。


<details>
  <summary>Details</summary>
Motivation: 传统语音合成系统在控制副语言信息（如音色、情感、韵律）时灵活性不足，而现有基于指令的TTS系统缺乏高质量评估基准和自动化指标。

Method: 提出InstructTTSEval基准，包含三个任务（声学参数指定、描述性风格指令、角色扮演），涵盖中英文各1k测试用例，并利用Gemini作为自动评估工具。

Result: 评估显示当前指令驱动的TTS系统在复杂指令执行上仍有显著改进空间。

Conclusion: InstructTTSEval有望推动更强大、灵活和准确的指令驱动TTS系统的发展。

Abstract: In modern speech synthesis, paralinguistic information--such as a speaker's
vocal timbre, emotional state, and dynamic prosody--plays a critical role in
conveying nuance beyond mere semantics. Traditional Text-to-Speech (TTS)
systems rely on fixed style labels or inserting a speech prompt to control
these cues, which severely limits flexibility. Recent attempts seek to employ
natural-language instructions to modulate paralinguistic features,
substantially improving the generalization of instruction-driven TTS models.
Although many TTS systems now support customized synthesis via textual
description, their actual ability to interpret and execute complex instructions
remains largely unexplored. In addition, there is still a shortage of
high-quality benchmarks and automated evaluation metrics specifically designed
for instruction-based TTS, which hinders accurate assessment and iterative
optimization of these models. To address these limitations, we introduce
InstructTTSEval, a benchmark for measuring the capability of complex
natural-language style control. We introduce three tasks, namely
Acoustic-Parameter Specification, Descriptive-Style Directive, and Role-Play,
including English and Chinese subsets, each with 1k test cases (6k in total)
paired with reference audio. We leverage Gemini as an automatic judge to assess
their instruction-following abilities. Our evaluation of accessible
instruction-following TTS systems highlights substantial room for further
improvement. We anticipate that InstructTTSEval will drive progress toward more
powerful, flexible, and accurate instruction-following TTS.

</details>


### [62] [Cyberbullying Detection in Hinglish Text Using MURIL and Explainable AI](https://arxiv.org/abs/2506.16066)
*Devesh Kumar*

Main category: cs.CL

TL;DR: 该论文提出了一种基于MURIL架构的框架，用于检测Hinglish（印地语-英语混合）文本中的网络欺凌，并在多个数据集上优于现有多语言模型。


<details>
  <summary>Details</summary>
Motivation: 数字通信平台的增长导致全球网络欺凌事件增加，现有检测系统主要针对单语言文本，无法有效处理Hinglish混合语言内容。

Method: 使用MURIL架构，结合选择性层冻结、分类头设计和针对混合语言的预处理，提升检测性能。

Result: 在六个基准数据集上，MURIL模型表现优于RoBERTa和IndicBERT，准确率提升1.36至13.07个百分点。

Conclusion: 该框架通过属性分析和跨语言模式识别提供可解释性，未来研究方向包括上下文依赖解释和文化理解等挑战。

Abstract: The growth of digital communication platforms has led to increased
cyberbullying incidents worldwide, creating a need for automated detection
systems to protect users. The rise of code-mixed Hindi-English (Hinglish)
communication on digital platforms poses challenges for existing cyberbullying
detection systems, which were designed primarily for monolingual text. This
paper presents a framework for cyberbullying detection in Hinglish text using
the Multilingual Representations for Indian Languages (MURIL) architecture to
address limitations in current approaches. Evaluation across six benchmark
datasets -- Bohra \textit{et al.}, BullyExplain, BullySentemo, Kumar \textit{et
al.}, HASOC 2021, and Mendeley Indo-HateSpeech -- shows that the MURIL-based
approach outperforms existing multilingual models including RoBERTa and
IndicBERT, with improvements of 1.36 to 13.07 percentage points and accuracies
of 86.97\% on Bohra, 84.62\% on BullyExplain, 86.03\% on BullySentemo, 75.41\%
on Kumar datasets, 83.92\% on HASOC 2021, and 94.63\% on Mendeley dataset. The
framework includes explainability features through attribution analysis and
cross-linguistic pattern recognition. Ablation studies show that selective
layer freezing, appropriate classification head design, and specialized
preprocessing for code-mixed content improve detection performance, while
failure analysis identifies challenges including context-dependent
interpretation, cultural understanding, and cross-linguistic sarcasm detection,
providing directions for future research in multilingual cyberbullying
detection.

</details>


### [63] [Automatic Speech Recognition Biases in Newcastle English: an Error Analysis](https://arxiv.org/abs/2506.16558)
*Dana Serditova,Kevin Tang,Jochen Steffens*

Main category: cs.CL

TL;DR: 研究探讨了自动语音识别（ASR）系统在纽卡斯尔英语方言上的表现，发现其错误与方言特征直接相关，呼吁增加训练数据的方言多样性。


<details>
  <summary>Details</summary>
Motivation: ASR系统因训练数据偏向主流语言而难以处理地区方言，地区偏见尚未充分研究。

Method: 采用两阶段分析：手动错误分析识别关键错误类型；案例研究聚焦方言代词“yous”和“wor”的识别。

Result: ASR错误与方言特征直接相关，社会因素影响较小。

Conclusion: 建议增加ASR训练数据的方言多样性，并利用社会语言学分析解决地区偏见。

Abstract: Automatic Speech Recognition (ASR) systems struggle with regional dialects
due to biased training which favours mainstream varieties. While previous
research has identified racial, age, and gender biases in ASR, regional bias
remains underexamined. This study investigates ASR performance on Newcastle
English, a well-documented regional dialect known to be challenging for ASR. A
two-stage analysis was conducted: first, a manual error analysis on a subsample
identified key phonological, lexical, and morphosyntactic errors behind ASR
misrecognitions; second, a case study focused on the systematic analysis of ASR
recognition of the regional pronouns ``yous'' and ``wor''. Results show that
ASR errors directly correlate with regional dialectal features, while social
factors play a lesser role in ASR mismatches. We advocate for greater dialectal
diversity in ASR training data and highlight the value of sociolinguistic
analysis in diagnosing and addressing regional biases.

</details>


### [64] [FinCoT: Grounding Chain-of-Thought in Expert Financial Reasoning](https://arxiv.org/abs/2506.16123)
*Natapong Nitarach,Warit Sirichotedumrong,Panop Pitchayarthorn,Pittawat Taveekitworachai,Potsawee Manakul,Kunat Pipatanakul*

Main category: cs.CL

TL;DR: FinCoT是一种结合领域专家金融推理的结构化思维链提示方法，显著提升金融NLP任务性能并降低推理成本。


<details>
  <summary>Details</summary>
Motivation: 金融NLP中结构化思维链提示的研究不足，且现有方法多基于非领域专家的启发式设计。

Method: 提出FinCoT，对比标准提示、非结构化思维链提示和结构化思维链提示，并在10个金融领域的CFA式问题中评估。

Result: FinCoT将性能从63.2%提升至80.5%，并减少生成令牌数8倍。

Conclusion: 领域对齐的结构化提示提升性能、降低成本，并生成更可解释的专家对齐推理轨迹。

Abstract: This paper presents FinCoT, a structured chain-of-thought (CoT) prompting
approach that incorporates insights from domain-specific expert financial
reasoning to guide the reasoning traces of large language models. We
investigate that there are three main prompting styles in FinNLP: (1) standard
prompting--zero-shot prompting; (2) unstructured CoT--CoT prompting without an
explicit reasoning structure, such as the use of tags; and (3) structured CoT
prompting--CoT prompting with explicit instructions or examples that define
structured reasoning steps. Previously, FinNLP has primarily focused on prompt
engineering with either standard or unstructured CoT prompting. However,
structured CoT prompting has received limited attention in prior work.
Furthermore, the design of reasoning structures in structured CoT prompting is
often based on heuristics from non-domain experts. In this study, we
investigate each prompting approach in FinNLP. We evaluate the three main
prompting styles and FinCoT on CFA-style questions spanning ten financial
domains. We observe that FinCoT improves performance from 63.2% to 80.5% and
Qwen-2.5-7B-Instruct from 69.7% to 74.2%, while reducing generated tokens
eight-fold compared to structured CoT prompting. Our findings show that
domain-aligned structured prompts not only improve performance and reduce
inference costs but also yield more interpretable and expert-aligned reasoning
traces.

</details>


### [65] [Weight Factorization and Centralization for Continual Learning in Speech Recognition](https://arxiv.org/abs/2506.16574)
*Enes Yavuz Ugan,Ngoc-Quan Pham,Alexander Waibel*

Main category: cs.CL

TL;DR: 提出一种基于因子化和中心化的持续学习方法，通过低秩适配器防止灾难性遗忘。


<details>
  <summary>Details</summary>
Motivation: 现代神经网络语音识别模型需要在不重新训练的情况下吸收新数据，但无排练、多语言和语言无关的条件容易导致灾难性遗忘。

Method: 采用两阶段方法：因子化和中心化，通过低秩适配器积累知识。

Result: 实验证明中心化阶段能有效防止灾难性遗忘。

Conclusion: 该方法在多语言和语言无关条件下表现良好。

Abstract: Modern neural network based speech recognition models are required to
continually absorb new data without re-training the whole system, especially in
downstream applications using foundation models, having no access to the
original training data. Continually training the models in a rehearsal-free,
multilingual, and language agnostic condition, likely leads to catastrophic
forgetting, when a seemingly insignificant disruption to the weights can
destructively harm the quality of the models. Inspired by the ability of human
brains to learn and consolidate knowledge through the waking-sleeping cycle, we
propose a continual learning approach with two distinct phases: factorization
and centralization, learning and merging knowledge accordingly. Our experiments
on a sequence of varied code-switching datasets showed that the centralization
stage can effectively prevent catastrophic forgetting by accumulating the
knowledge in multiple scattering low-rank adapters.

</details>


### [66] [Under the Shadow of Babel: How Language Shapes Reasoning in LLMs](https://arxiv.org/abs/2506.16151)
*Chenxi Wang,Yixuan Zhang,Lang Gao,Zixiang Xu,Zirui Song,Yanbo Wang,Xiuying Chen*

Main category: cs.CL

TL;DR: 论文研究了语言结构对大型语言模型（LLMs）推理模式的影响，通过双语数据集BICAUSE验证了LLMs会内化语言特有的逻辑偏好。


<details>
  <summary>Details</summary>
Motivation: 探讨语言结构是否会影响LLMs的认知和推理模式，验证语言相对性假说在模型中的体现。

Method: 使用BICAUSE双语数据集（中英文语义对齐的正反因果样本），分析LLMs的注意力模式和推理表现。

Result: LLMs表现出与语言类型一致的注意力模式，内化了语言特有的因果词序偏好，并在跨语言推理中展现出语义对齐的抽象理解。

Conclusion: LLMs不仅模仿语言表面形式，还内化了语言塑造的推理偏见，首次通过模型内部结构分析验证了这一现象。

Abstract: Language is not only a tool for communication but also a medium for human
cognition and reasoning. If, as linguistic relativity suggests, the structure
of language shapes cognitive patterns, then large language models (LLMs)
trained on human language may also internalize the habitual logical structures
embedded in different languages. To examine this hypothesis, we introduce
BICAUSE, a structured bilingual dataset for causal reasoning, which includes
semantically aligned Chinese and English samples in both forward and reversed
causal forms. Our study reveals three key findings: (1) LLMs exhibit
typologically aligned attention patterns, focusing more on causes and
sentence-initial connectives in Chinese, while showing a more balanced
distribution in English. (2) Models internalize language-specific preferences
for causal word order and often rigidly apply them to atypical inputs, leading
to degraded performance, especially in Chinese. (3) When causal reasoning
succeeds, model representations converge toward semantically aligned
abstractions across languages, indicating a shared understanding beyond surface
form. Overall, these results suggest that LLMs not only mimic surface
linguistic forms but also internalize the reasoning biases shaped by language.
Rooted in cognitive linguistic theory, this phenomenon is for the first time
empirically verified through structural analysis of model internals.

</details>


### [67] [Streaming Non-Autoregressive Model for Accent Conversion and Pronunciation Improvement](https://arxiv.org/abs/2506.16580)
*Tuan-Nam Nguyen,Ngoc-Quan Pham,Seymanur Akti,Alexander Waibel*

Main category: cs.CL

TL;DR: 提出首个流式口音转换模型，将非母语语音转换为母语口音，同时保留说话者身份和韵律，并改进发音。


<details>
  <summary>Details</summary>
Motivation: 解决现有口音转换模型无法实时流式处理的问题，同时提升发音质量。

Method: 改进现有AC架构，采用Emformer编码器和优化的推理机制，并集成母语TTS模型生成理想训练数据。

Result: 流式AC模型性能与顶级AC模型相当，且保持稳定延迟，成为首个支持流式处理的AC系统。

Conclusion: 该模型成功实现了流式口音转换，为实时应用提供了可能。

Abstract: We propose a first streaming accent conversion (AC) model that transforms
non-native speech into a native-like accent while preserving speaker identity,
prosody and improving pronunciation. Our approach enables stream processing by
modifying a previous AC architecture with an Emformer encoder and an optimized
inference mechanism. Additionally, we integrate a native text-to-speech (TTS)
model to generate ideal ground-truth data for efficient training. Our streaming
AC model achieves comparable performance to the top AC models while maintaining
stable latency, making it the first AC system capable of streaming.

</details>


### [68] [SGIC: A Self-Guided Iterative Calibration Framework for RAG](https://arxiv.org/abs/2506.16172)
*Guanhua Chen,Yutong Yao,Lidia S. Chao,Xuebo Liu,Derek F. Wong*

Main category: cs.CL

TL;DR: 论文提出了一种名为SGIC的自引导迭代校准框架，利用不确定性评分提升LLM在多轮校准中的效能。


<details>
  <summary>Details</summary>
Motivation: 现有检索增强生成方法常忽视LLM的校准能力，而SGIC框架通过特定提示显著提升校准效果。

Method: SGIC框架通过计算不确定性评分评估文档相关性和LLM响应置信度，并迭代优化校准。同时提出一种自校准训练集构建方法。

Result: SGIC框架显著提升了闭源和开源LLM的性能。

Conclusion: SGIC框架通过不确定性评分和迭代校准，有效提升了LLM的响应准确性和信息捕获能力。

Abstract: Recent research in retrieval-augmented generation (RAG) has concentrated on
retrieving useful information from candidate documents. However, numerous
methodologies frequently neglect the calibration capabilities of large language
models (LLMs), which capitalize on their robust in-context reasoning prowess.
This work illustrates that providing LLMs with specific cues substantially
improves their calibration efficacy, especially in multi-round calibrations. We
present a new SGIC: Self-Guided Iterative Calibration Framework that employs
uncertainty scores as a tool. Initially, this framework calculates uncertainty
scores to determine both the relevance of each document to the query and the
confidence level in the responses produced by the LLMs. Subsequently, it
reevaluates these scores iteratively, amalgamating them with prior responses to
refine calibration. Furthermore, we introduce an innovative approach for
constructing an iterative self-calibration training set, which optimizes LLMs
to efficiently harness uncertainty scores for capturing critical information
and enhancing response accuracy. Our proposed framework significantly improves
performance on both closed-source and open-weight LLMs.

</details>


### [69] [LM-SPT: LM-Aligned Semantic Distillation for Speech Tokenization](https://arxiv.org/abs/2506.16738)
*Daejin Jo,Jeeyoung Yun,Byungseok Roh,Sungwoong Kim*

Main category: cs.CL

TL;DR: LM-SPT是一种新的语音标记化方法，通过语义蒸馏和架构改进，解决了语音标记序列过长的问题，并在语音转文本和文本转语音任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有语音标记化方法产生的标记序列过长，影响语音-语言建模效率，且传统降帧率方法可能破坏语义结构。

Method: 提出LM-SPT，通过语义蒸馏间接监督，从重建的语音波形中学习语义对齐的离散单元，并支持多帧率。

Result: LM-SPT在重建保真度上优于基线，其标记训练的SLM在语音转文本和文本转语音任务中表现优异。

Conclusion: LM-SPT通过语义蒸馏和架构改进，显著提升了语音标记化的效率和语义对齐能力。

Abstract: With the rapid progress of speech language models (SLMs), discrete speech
tokens have emerged as a core interface between speech and text, enabling
unified modeling across modalities. Recent speech tokenization approaches aim
to isolate semantic information from low-level acoustics to better align with
language models. In particular, previous methods use SSL teachers such as
HuBERT to extract semantic representations, which are then distilled into a
semantic quantizer to suppress acoustic redundancy as well as capture
content-related latent structures. However, they still produce speech token
sequences significantly longer than their textual counterparts, creating
challenges for efficient speech-language modeling. Reducing the frame rate is a
natural solution, but standard techniques, such as rigid average pooling across
frames, can distort or dilute the semantic structure required for effective LM
alignment. To address this, we propose LM-SPT, a speech tokenization method
that introduces a novel semantic distillation. Instead of directly matching
teacher and student features via pooling, we reconstruct speech solely from
semantic tokens and minimize the discrepancy between the encoded
representations of the original and reconstructed waveforms, obtained from a
frozen automatic speech recognition (ASR) encoder. This indirect yet
data-driven supervision enables the tokenizer to learn discrete units that are
more semantically aligned with language models. LM-SPT further incorporates
architectural improvements to the encoder and decoder for speech tokenization,
and supports multiple frame rates, including 25Hz, 12.5Hz, and 6.25Hz.
Experimental results show that LM-SPT achieves superior reconstruction fidelity
compared to baselines, and that SLMs trained with LM-SPT tokens achieve
competitive performances on speech-to-text and consistently outperform
baselines on text-to-speech tasks.

</details>


### [70] [JETHICS: Japanese Ethics Understanding Evaluation Dataset](https://arxiv.org/abs/2506.16187)
*Masashi Takeshita,Rafal Rzepka*

Main category: cs.CL

TL;DR: JETHICS是一个日本数据集，用于评估AI模型的伦理理解能力，包含78K样本，基于英语ETHICS数据集构建。实验显示，GPT-4o平均得分约0.7，最佳日本LLM约0.5，表明现有模型仍有较大改进空间。


<details>
  <summary>Details</summary>
Motivation: 构建一个日语数据集以评估AI模型的伦理理解能力，填补现有研究的空白。

Method: 采用英语ETHICS数据集的构建方法，包含四类伦理理论和常识道德类别。

Result: GPT-4o平均得分0.7，最佳日本LLM得分0.5，显示模型表现有待提升。

Conclusion: 当前LLM在伦理理解方面仍有较大改进空间，JETHICS为未来研究提供了基准。

Abstract: In this work, we propose JETHICS, a Japanese dataset for evaluating ethics
understanding of AI models. JETHICS contains 78K examples and is built by
following the construction methods of the existing English ETHICS dataset. It
includes four categories based normative theories and concepts from ethics and
political philosophy; and one representing commonsense morality. Our evaluation
experiments on non-proprietary large language models (LLMs) and on GPT-4o
reveal that even GPT-4o achieves only an average score of about 0.7, while the
best-performing Japanese LLM attains around 0.5, indicating a relatively large
room for improvement in current LLMs.

</details>


### [71] [Web(er) of Hate: A Survey on How Hate Speech Is Typed](https://arxiv.org/abs/2506.16190)
*Luna Wang,Andrew Caines,Alice Hutchings*

Main category: cs.CL

TL;DR: 论文探讨了仇恨言论数据集构建中的方法论选择及其对可靠性的影响，提倡采用反思性方法以提高透明度。


<details>
  <summary>Details</summary>
Motivation: 研究仇恨言论数据集构建中的复杂设计决策及其对数据集可靠性的影响。

Method: 通过分析多种数据集的方法论选择，结合马克斯·韦伯的理想类型概念，提出反思性方法。

Result: 揭示了数据集构建中的常见主题和实践，强调了透明度和方法论严谨的重要性。

Conclusion: 呼吁研究者在数据集构建中承认自身价值判断，以提升透明度和方法论严谨性。

Abstract: The curation of hate speech datasets involves complex design decisions that
balance competing priorities. This paper critically examines these
methodological choices in a diverse range of datasets, highlighting common
themes and practices, and their implications for dataset reliability. Drawing
on Max Weber's notion of ideal types, we argue for a reflexive approach in
dataset creation, urging researchers to acknowledge their own value judgments
during dataset construction, fostering transparency and methodological rigour.

</details>


### [72] [Comparative Analysis of Abstractive Summarization Models for Clinical Radiology Reports](https://arxiv.org/abs/2506.16247)
*Anindita Bhattacharya,Tohida Rehman,Debarshi Kumar Sanyal,Samiran Chattopadhyay*

Main category: cs.CL

TL;DR: 研究探讨了使用先进的抽象摘要模型从放射学报告的详细发现部分生成简洁印象的方法，比较了多种预训练和开源大型语言模型的性能。


<details>
  <summary>Details</summary>
Motivation: 放射学报告的发现部分通常冗长详细，而印象部分更简洁且包含关键诊断结论，研究旨在为医疗专业人员提供自动化摘要解决方案。

Method: 使用MIMIC-CXR数据集，比较了T5-base、BART-base、PEGASUS-x-base、ChatGPT-4、LLaMA-3-8B和自定义Pointer Generator Network的性能，并采用多种评估指标（如ROUGE、METEOR、BERTScore）。

Result: 研究分析了各模型在医学文本摘要中的表现，识别了它们的优势和局限性。

Conclusion: 研究结果为医疗领域需要自动化摘要的专业人员提供了有用信息。

Abstract: The findings section of a radiology report is often detailed and lengthy,
whereas the impression section is comparatively more compact and captures key
diagnostic conclusions. This research explores the use of advanced abstractive
summarization models to generate the concise impression from the findings
section of a radiology report. We have used the publicly available MIMIC-CXR
dataset. A comparative analysis is conducted on leading pre-trained and
open-source large language models, including T5-base, BART-base,
PEGASUS-x-base, ChatGPT-4, LLaMA-3-8B, and a custom Pointer Generator Network
with a coverage mechanism. To ensure a thorough assessment, multiple evaluation
metrics are employed, including ROUGE-1, ROUGE-2, ROUGE-L, METEOR, and
BERTScore. By analyzing the performance of these models, this study identifies
their respective strengths and limitations in the summarization of medical
text. The findings of this paper provide helpful information for medical
professionals who need automated summarization solutions in the healthcare
sector.

</details>


### [73] [End-to-End Speech Translation for Low-Resource Languages Using Weakly Labeled Data](https://arxiv.org/abs/2506.16251)
*Aishwarya Pothula,Bhavana Akkiraju,Srihari Bandarupalli,Charan D,Santosh Kesiraju,Anil Kumar Vuppala*

Main category: cs.CL

TL;DR: 论文探讨了如何利用弱标注数据构建低资源语言的端到端语音转文本翻译系统，并通过实验验证其性能。


<details>
  <summary>Details</summary>
Motivation: 高标注质量数据的稀缺性限制了低资源语言语音转文本翻译系统的发展，弱标注数据可能是一种解决方案。

Method: 使用先进的句子编码器通过双语文本挖掘构建语音转文本翻译数据集，并研究数据质量和数量对模型性能的影响。

Result: 实验表明，弱标注数据可以构建性能接近SONAR和SeamlessM4T等大规模多模态多语言基线的翻译系统。

Conclusion: 弱标注数据是构建低资源语言语音转文本翻译系统的可行方案。

Abstract: The scarcity of high-quality annotated data presents a significant challenge
in developing effective end-to-end speech-to-text translation (ST) systems,
particularly for low-resource languages. This paper explores the hypothesis
that weakly labeled data can be used to build ST models for low-resource
language pairs. We constructed speech-to-text translation datasets with the
help of bitext mining using state-of-the-art sentence encoders. We mined the
multilingual Shrutilipi corpus to build Shrutilipi-anuvaad, a dataset
comprising ST data for language pairs Bengali-Hindi, Malayalam-Hindi,
Odia-Hindi, and Telugu-Hindi. We created multiple versions of training data
with varying degrees of quality and quantity to investigate the effect of
quality versus quantity of weakly labeled data on ST model performance. Results
demonstrate that ST systems can be built using weakly labeled data, with
performance comparable to massive multi-modal multilingual baselines such as
SONAR and SeamlessM4T.

</details>


### [74] [PL-Guard: Benchmarking Language Model Safety for Polish](https://arxiv.org/abs/2506.16322)
*Aleksandra Krasnodębska,Karolina Seweryn,Szymon Łukasik,Wojciech Kusa*

Main category: cs.CL

TL;DR: 论文提出了一种针对波兰语的语言模型安全分类基准数据集，并测试了多种模型的性能，发现基于HerBERT的分类器表现最佳。


<details>
  <summary>Details</summary>
Motivation: 现有的大语言模型安全评估和审核工具偏向英语等高资源语言，全球多数语言未被充分研究，因此需要填补这一空白。

Method: 创建了手动标注的波兰语安全分类数据集及其对抗性扰动版本，测试了三种模型（Llama-Guard-3-8B、HerBERT-based分类器和PLLuM）的性能。

Result: 基于HerBERT的分类器在对抗条件下表现最佳。

Conclusion: 研究填补了波兰语安全分类的空白，并验证了HerBERT分类器的优越性。

Abstract: Despite increasing efforts to ensure the safety of large language models
(LLMs), most existing safety assessments and moderation tools remain heavily
biased toward English and other high-resource languages, leaving majority of
global languages underexamined. To address this gap, we introduce a manually
annotated benchmark dataset for language model safety classification in Polish.
We also create adversarially perturbed variants of these samples designed to
challenge model robustness. We conduct a series of experiments to evaluate
LLM-based and classifier-based models of varying sizes and architectures.
Specifically, we fine-tune three models: Llama-Guard-3-8B, a HerBERT-based
classifier (a Polish BERT derivative), and PLLuM, a Polish-adapted Llama-8B
model. We train these models using different combinations of annotated data and
evaluate their performance, comparing it against publicly available guard
models. Results demonstrate that the HerBERT-based classifier achieves the
highest overall performance, particularly under adversarial conditions.

</details>


### [75] [Generalizability of Media Frames: Corpus creation and analysis across countries](https://arxiv.org/abs/2506.16337)
*Agnese Daffara,Sourabh Dattawad,Sebastian Padó,Tanise Ceron*

Main category: cs.CL

TL;DR: 研究探讨了MFC框架在巴西新闻中的适用性，发现其15个框架基本适用但需微调，跨文化使用时需谨慎。


<details>
  <summary>Details</summary>
Motivation: 评估MFC框架在非美国文化背景（巴西）中的适用性，以理解框架的跨文化通用性。

Method: 引入FrameNews-PT数据集，通过多轮标注评估MFC框架在巴西新闻中的表现，并测试微调和零样本模型的性能。

Result: MFC框架基本适用，但需少量修订；部分框架使用较少，新议题需依赖通用框架。

Conclusion: 跨文化框架应用需谨慎，MFC框架需调整以适应不同文化背景。

Abstract: Frames capture aspects of an issue that are emphasized in a debate by
interlocutors and can help us understand how political language conveys
different perspectives and ultimately shapes people's opinions. The Media Frame
Corpus (MFC) is the most commonly used framework with categories and detailed
guidelines for operationalizing frames. It is, however, focused on a few
salient U.S. news issues, making it unclear how well these frames can capture
news issues in other cultural contexts. To explore this, we introduce
FrameNews-PT, a dataset of Brazilian Portuguese news articles covering
political and economic news and annotate it within the MFC framework. Through
several annotation rounds, we evaluate the extent to which MFC frames
generalize to the Brazilian debate issues. We further evaluate how fine-tuned
and zero-shot models perform on out-of-domain data. Results show that the 15
MFC frames remain broadly applicable with minor revisions of the guidelines.
However, some MFC frames are rarely used, and novel news issues are analyzed
using general 'fall-back' frames. We conclude that cross-cultural frame use
requires careful consideration.

</details>


### [76] [Analyzing the Influence of Knowledge Graph Information on Relation Extraction](https://arxiv.org/abs/2506.16343)
*Cedric Möller,Ricardo Usbeck*

Main category: cs.CL

TL;DR: 研究探讨了知识图谱信息对关系抽取模型性能的影响，发现其能显著提升性能，尤其在训练样本不平衡时。


<details>
  <summary>Details</summary>
Motivation: 假设知识图谱中实体的位置信息对关系抽取任务有重要帮助。

Method: 结合传统关系抽取方法与图感知的Neural Bellman-Ford网络，测试了监督和零样本设置。

Result: 实验表明，整合知识图谱信息显著提升了性能，尤其在样本不平衡的情况下。

Conclusion: 知识图谱信息的整合对关系抽取任务有显著帮助，尤其在多样化的数据集上表现一致。

Abstract: We examine the impact of incorporating knowledge graph information on the
performance of relation extraction models across a range of datasets. Our
hypothesis is that the positions of entities within a knowledge graph provide
important insights for relation extraction tasks. We conduct experiments on
multiple datasets, each varying in the number of relations, training examples,
and underlying knowledge graphs. Our results demonstrate that integrating
knowledge graph information significantly enhances performance, especially when
dealing with an imbalance in the number of training examples for each relation.
We evaluate the contribution of knowledge graph-based features by combining
established relation extraction methods with graph-aware Neural Bellman-Ford
networks. These features are tested in both supervised and zero-shot settings,
demonstrating consistent performance improvements across various datasets.

</details>


### [77] [DISCIE -- Discriminative Closed Information Extraction](https://arxiv.org/abs/2506.16348)
*Cedric Möller,Ricardo Usbeck*

Main category: cs.CL

TL;DR: 提出了一种新的封闭信息抽取方法，结合类型和实体特定信息，显著提升关系抽取准确性，尤其在长尾关系上表现优异。


<details>
  <summary>Details</summary>
Motivation: 解决大规模封闭信息抽取中实体和关系数量庞大时的准确性和效率问题。

Method: 采用判别式方法，整合类型和实体特定信息，使用较小模型提升效率。

Result: 性能优于现有端到端生成模型，尤其在长尾关系和大规模数据上表现突出。

Conclusion: 该方法为更准确高效的信息抽取技术提供了新方向。

Abstract: This paper introduces a novel method for closed information extraction. The
method employs a discriminative approach that incorporates type and
entity-specific information to improve relation extraction accuracy,
particularly benefiting long-tail relations. Notably, this method demonstrates
superior performance compared to state-of-the-art end-to-end generative models.
This is especially evident for the problem of large-scale closed information
extraction where we are confronted with millions of entities and hundreds of
relations. Furthermore, we emphasize the efficiency aspect by leveraging
smaller models. In particular, the integration of type-information proves
instrumental in achieving performance levels on par with or surpassing those of
a larger generative model. This advancement holds promise for more accurate and
efficient information extraction techniques.

</details>


### [78] [Can structural correspondences ground real world representational content in Large Language Models?](https://arxiv.org/abs/2506.16370)
*Iwan Williams*

Main category: cs.CL

TL;DR: 论文探讨了大型语言模型（LLMs）是否能表示现实世界内容，并提出了基于结构对应性的表示理论。


<details>
  <summary>Details</summary>
Motivation: 研究LLMs是否能够表示现实世界内容，以及这种表示的条件。

Method: 采用结构对应性理论分析LLMs的表示能力，并调查相关证据。

Result: 仅存在结构对应性不足以支持表示，但若这些对应性在任务中发挥作用，则可能支持真实世界内容的表示。

Conclusion: LLMs的文本局限性可能阻碍其参与适当的任务，但若能克服这一挑战，结构对应性可能支持其表示能力。

Abstract: Large Language Models (LLMs) such as GPT-4 produce compelling responses to a
wide range of prompts. But their representational capacities are uncertain.
Many LLMs have no direct contact with extra-linguistic reality: their inputs,
outputs and training data consist solely of text, raising the questions (1) can
LLMs represent anything and (2) if so, what? In this paper, I explore what it
would take to answer these questions according to a structural-correspondence
based account of representation, and make an initial survey of this evidence. I
argue that the mere existence of structural correspondences between LLMs and
worldly entities is insufficient to ground representation of those entities.
However, if these structural correspondences play an appropriate role - they
are exploited in a way that explains successful task performance - then they
could ground real world contents. This requires overcoming a challenge: the
text-boundedness of LLMs appears, on the face of it, to prevent them engaging
in the right sorts of tasks.

</details>


### [79] [Large Language Models in Argument Mining: A Survey](https://arxiv.org/abs/2506.16383)
*Hao Li,Viktor Schlegel,Yizheng Sun,Riza Batista-Navarro,Goran Nenadic*

Main category: cs.CL

TL;DR: 本文综述了大型语言模型（LLMs）在论点挖掘（AM）领域的最新进展，包括理论基础、数据集、任务分类及LLM技术应用，并提出了未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 随着LLMs的发展，其在论点挖掘中的应用潜力巨大，但缺乏系统性总结和未来方向指导。

Method: 通过系统综述，整理AM的理论基础、数据集、任务分类，并分析LLM技术（如提示、思维链推理）的应用。

Result: 总结了LLM在AM中的技术进展，评估了当前实践，并指出长文本推理、可解释性等挑战。

Conclusion: 提出了LLM驱动的计算论证的未来研究议程，为快速发展的领域提供战略指导。

Abstract: Argument Mining (AM), a critical subfield of Natural Language Processing
(NLP), focuses on extracting argumentative structures from text. The advent of
Large Language Models (LLMs) has profoundly transformed AM, enabling advanced
in-context learning, prompt-based generation, and robust cross-domain
adaptability. This survey systematically synthesizes recent advancements in
LLM-driven AM. We provide a concise review of foundational theories and
annotation frameworks, alongside a meticulously curated catalog of datasets. A
key contribution is our comprehensive taxonomy of AM subtasks, elucidating how
contemporary LLM techniques -- such as prompting, chain-of-thought reasoning,
and retrieval augmentation -- have reconfigured their execution. We further
detail current LLM architectures and methodologies, critically assess
evaluation practices, and delineate pivotal challenges including long-context
reasoning, interpretability, and annotation bottlenecks. Conclusively, we
highlight emerging trends and propose a forward-looking research agenda for
LLM-based computational argumentation, aiming to strategically guide
researchers in this rapidly evolving domain.

</details>


### [80] [HausaNLP at SemEval-2025 Task 11: Advancing Hausa Text-based Emotion Detection](https://arxiv.org/abs/2506.16388)
*Sani Abdullahi Sani,Salim Abubakar,Falalu Ibrahim Lawan,Abdulhamid Abubakar,Maryam Bala*

Main category: cs.CL

TL;DR: 本文提出了一种针对低资源非洲语言豪萨语的多标签情感检测方法，使用AfriBERTa模型进行微调，验证准确率达74.00%。


<details>
  <summary>Details</summary>
Motivation: 解决豪萨语这一低资源语言中的情感检测问题，填补相关研究空白。

Method: 数据预处理、分词，并使用Hugging Face Trainer API对AfriBERTa模型进行微调。

Result: 验证准确率为74.00%，F1分数为73.50%。

Conclusion: 基于Transformer的模型在低资源语言情感检测中表现有效。

Abstract: This paper presents our approach to multi-label emotion detection in Hausa, a
low-resource African language, as part of SemEval Track A. We fine-tuned
AfriBERTa, a transformer-based model pre-trained on African languages, to
classify Hausa text into six emotions: anger, disgust, fear, joy, sadness, and
surprise. Our methodology involved data preprocessing, tokenization, and model
fine-tuning using the Hugging Face Trainer API. The system achieved a
validation accuracy of 74.00%, with an F1-score of 73.50%, demonstrating the
effectiveness of transformer-based models for emotion detection in low-resource
languages.

</details>


### [81] [RiOT: Efficient Prompt Refinement with Residual Optimization Tree](https://arxiv.org/abs/2506.16389)
*Chenyi Zhou,Zhengyan Shi,Yuan Yao,Lei Liang,Huajun Chen,Qiang Zhang*

Main category: cs.CL

TL;DR: RiOT是一种新型自动提示优化框架，通过文本梯度和残差连接解决现有方法的多样性和语义漂移问题。


<details>
  <summary>Details</summary>
Motivation: 现有自动提示优化方法缺乏多样性且易导致语义漂移，限制了LLMs的潜力。

Method: RiOT通过迭代文本梯度生成多样化候选提示，利用困惑度选择最佳提示，并引入残差连接防止语义漂移。

Result: 在五个基准测试中，RiOT优于现有自动优化方法和手动提示。

Conclusion: RiOT为LLMs的提示优化提供了高效、灵活的解决方案。

Abstract: Recent advancements in large language models (LLMs) have highlighted their
potential across a variety of tasks, but their performance still heavily relies
on the design of effective prompts. Existing methods for automatic prompt
optimization face two challenges: lack of diversity, limiting the exploration
of valuable and innovative directions and semantic drift, where optimizations
for one task can degrade performance in others. To address these issues, we
propose Residual Optimization Tree (RiOT), a novel framework for automatic
prompt optimization. RiOT iteratively refines prompts through text gradients,
generating multiple semantically diverse candidates at each step, and selects
the best prompt using perplexity. Additionally, RiOT incorporates the text
residual connection to mitigate semantic drift by selectively retaining
beneficial content across optimization iterations. A tree structure efficiently
manages the optimization process, ensuring scalability and flexibility.
Extensive experiments across five benchmarks, covering commonsense,
mathematical, logical, temporal, and semantic reasoning, demonstrate that RiOT
outperforms both previous prompt optimization methods and manual prompting.

</details>


### [82] [From LLM-anation to LLM-orchestrator: Coordinating Small Models for Data Labeling](https://arxiv.org/abs/2506.16393)
*Yao Lu,Zhaiyuan Ji,Jiawei Du,Yu Shanqing,Qi Xuan,Tianyi Zhou*

Main category: cs.CL

TL;DR: 论文提出了一种多模型协作标注的新范式AutoAnnotator，解决了LLMs标注成本高和细粒度语义理解场景下准确率低的问题。


<details>
  <summary>Details</summary>
Motivation: 解决LLMs在大规模标注中成本高昂及细粒度语义理解场景下准确率低于SLMs的问题。

Method: 设计了双层框架AutoAnnotator：上层元控制器层利用LLMs选择SLMs并生成标注代码，下层任务专家层通过多模型投票进行标注，并通过持续学习策略优化SLMs。

Result: AutoAnnotator在零样本、单样本、CoT和多数投票设置下均优于现有开源/API LLMs，成本降低74.15%，准确率提升6.21%。

Conclusion: AutoAnnotator显著降低了标注成本并提高了准确率，为多模型协作标注提供了有效解决方案。

Abstract: Although the annotation paradigm based on Large Language Models (LLMs) has
made significant breakthroughs in recent years, its actual deployment still has
two core bottlenecks: first, the cost of calling commercial APIs in large-scale
annotation is very expensive; second, in scenarios that require fine-grained
semantic understanding, such as sentiment classification and toxicity
classification, the annotation accuracy of LLMs is even lower than that of
Small Language Models (SLMs) dedicated to this field. To address these
problems, we propose a new paradigm of multi-model cooperative annotation and
design a fully automatic annotation framework AutoAnnotator based on this.
Specifically, AutoAnnotator consists of two layers. The upper-level
meta-controller layer uses the generation and reasoning capabilities of LLMs to
select SLMs for annotation, automatically generate annotation code and verify
difficult samples; the lower-level task-specialist layer consists of multiple
SLMs that perform annotation through multi-model voting. In addition, we use
the difficult samples obtained by the secondary review of the meta-controller
layer as the reinforcement learning set and fine-tune the SLMs in stages
through a continual learning strategy, thereby improving the generalization of
SLMs. Extensive experiments show that AutoAnnotator outperforms existing
open-source/API LLMs in zero-shot, one-shot, CoT, and majority voting settings.
Notably, AutoAnnotator reduces the annotation cost by 74.15% compared to
directly annotating with GPT-3.5-turbo, while still improving the accuracy by
6.21%. Project page: https://github.com/Zhaiyuan-Ji/AutoAnnotator.

</details>


### [83] [OJBench: A Competition Level Code Benchmark For Large Language Models](https://arxiv.org/abs/2506.16395)
*Zhexu Wang,Yiping Liu,Yejie Wang,Wenyang He,Bofei Gao,Muxi Diao,Yanxu Chen,Kelin Fu,Flood Sung,Zhilin Yang,Tianyu Liu,Weiran Xu*

Main category: cs.CL

TL;DR: OJBench是一个新的基准测试，用于评估大语言模型在竞争级代码推理能力上的表现，结果显示即使是顶尖模型也难以应对高难度问题。


<details>
  <summary>Details</summary>
Motivation: 现有代码基准测试无法全面评估大语言模型在竞争级代码推理上的能力，因此需要更严格的测试工具。

Method: 引入OJBench，包含232个来自NOI和ICPC的编程竞赛问题，并对37种模型进行全面评估。

Result: 即使是顶尖推理导向模型（如o4-mini和Gemini-2.5-pro-exp）也难以解决高难度竞赛问题。

Conclusion: 竞争级代码推理对大语言模型仍具挑战性，OJBench为评估提供了更严格的标准。

Abstract: Recent advancements in large language models (LLMs) have demonstrated
significant progress in math and code reasoning capabilities. However, existing
code benchmark are limited in their ability to evaluate the full spectrum of
these capabilities, particularly at the competitive level. To bridge this gap,
we introduce OJBench, a novel and challenging benchmark designed to assess the
competitive-level code reasoning abilities of LLMs. OJBench comprises 232
programming competition problems from NOI and ICPC, providing a more rigorous
test of models' reasoning skills. We conducted a comprehensive evaluation using
OJBench on 37 models, including both closed-source and open-source models,
reasoning-oriented and non-reasoning-oriented models. Our results indicate that
even state-of-the-art reasoning-oriented models, such as o4-mini and
Gemini-2.5-pro-exp, struggle with highly challenging competition-level
problems. This highlights the significant challenges that models face in
competitive-level code reasoning.

</details>


### [84] [NepaliGPT: A Generative Language Model for the Nepali Language](https://arxiv.org/abs/2506.16399)
*Shushanta Pudasaini,Aman Shakya,Siddhartha Shrestha,Sahil Bhatta,Sunil Thapa,Sushmita Palikhe*

Main category: cs.CL

TL;DR: 该研究填补了尼泊尔语生成语言模型的空白，提出了NepaliGPT，并引入了Devanagari语料库和首个尼泊尔语基准数据集。


<details>
  <summary>Details</summary>
Motivation: 由于缺乏针对尼泊尔语的生成语言模型，下游任务如微调尚未探索，因此需要填补这一研究空白。

Method: 研究提出了NepaliGPT模型，并构建了Devanagari语料库和包含4,296个问答对的基准数据集。

Result: NepaliGPT在文本生成中表现优异：困惑度为26.32245，ROUGE-1得分为0.2604，因果连贯性为81.25%，因果一致性为85.41%。

Conclusion: NepaliGPT为尼泊尔语NLP领域提供了首个生成模型，为未来研究奠定了基础。

Abstract: After the release of ChatGPT, Large Language Models (LLMs) have gained huge
popularity in recent days and thousands of variants of LLMs have been released.
However, there is no generative language model for the Nepali language, due to
which other downstream tasks, including fine-tuning, have not been explored
yet. To fill this research gap in the Nepali NLP space, this research proposes
\textit{NepaliGPT}, a generative large language model tailored specifically for
the Nepali language. This research introduces an advanced corpus for the Nepali
language collected from several sources, called the Devanagari Corpus.
Likewise, the research introduces the first NepaliGPT benchmark dataset
comprised of 4,296 question-answer pairs in the Nepali language. The proposed
LLM NepaliGPT achieves the following metrics in text generation: Perplexity of
26.32245, ROUGE-1 score of 0.2604, causal coherence of 81.25\%, and causal
consistency of 85.41\%.

</details>


### [85] [When Does Divide and Conquer Work for Long Context LLM? A Noise Decomposition Framework](https://arxiv.org/abs/2506.16411)
*Zhen Xu,Shang Zhu,Jue Wang,Junlin Wang,Ben Athiwaratkun,Chi Wang,James Zou,Ce Zhang*

Main category: cs.CL

TL;DR: 论文研究了将大语言模型（LLMs）应用于长文本的挑战，提出了一个理论框架，将长上下文任务的失败模式分为三类，并分析了多代理分块方法的有效性。实验验证了理论分析，并解释了为何分块处理在某些情况下优于单次处理。


<details>
  <summary>Details</summary>
Motivation: 解决大语言模型在处理长文本时的性能问题，尤其是跨块依赖、模型噪声和聚合噪声等挑战。

Method: 提出理论框架分析长上下文任务的失败模式，并通过多代理分块方法（将长序列分为小块并聚合结果）进行实验验证。

Result: 实验证实多代理分块在检索、问答和摘要等任务中有效，且分块处理在大型输入下可能优于单次处理的先进模型。

Conclusion: 通过理论框架和实验结果，论文为处理长上下文提供了原则性方法，强调分块和聚合策略的重要性。

Abstract: We investigate the challenge of applying Large Language Models (LLMs) to long
texts. We propose a theoretical framework that distinguishes the failure modes
of long context tasks into three categories: cross-chunk dependence (task
noise), confusion that grows with context size (model noise), and the imperfect
integration of partial results (aggregator noise). Under this view, we analyze
when it is effective to use multi-agent chunking, i.e., dividing a length
sequence into smaller chunks and aggregating the processed results of each
chunk. Our experiments on tasks such as retrieval, question answering, and
summarization confirm both the theoretical analysis and the conditions that
favor multi-agent chunking. By exploring superlinear model noise growth with
input length, we also explain why, for large inputs, a weaker model configured
with chunk-based processing can surpass a more advanced model like GPT4o
applied in a single shot. Overall, we present a principled understanding
framework and our results highlight a direct pathway to handling long contexts
in LLMs with carefully managed chunking and aggregator strategies.

</details>


### [86] [REIS: A High-Performance and Energy-Efficient Retrieval System with In-Storage Processing](https://arxiv.org/abs/2506.16444)
*Kangqi Chen,Andreas Kosmas Kakolyris,Rakesh Nadig,Manos Frouzakis,Nika Mansouri Ghiasi,Yu Liang,Haiyu Mao,Jisung Park,Mohammad Sadrosadati,Onur Mutlu*

Main category: cs.CL

TL;DR: 论文提出REIS系统，通过优化存储内处理（ISP）技术，显著提升检索增强生成（RAG）中检索阶段的性能和能效。


<details>
  <summary>Details</summary>
Motivation: 解决大型语言模型（LLMs）因静态训练数据限制而无法动态获取外部知识的问题，并优化检索阶段的瓶颈。

Method: 提出REIS系统，采用数据库布局优化、ISP定制数据放置技术和轻量级Flash Translation Layer，以及利用存储系统现有计算资源的ANNS引擎。

Result: 相比服务器级系统，REIS在检索性能上平均提升13倍，能效提升55倍。

Conclusion: REIS通过定制化ISP技术，有效解决了RAG中检索阶段的瓶颈，显著提升了性能和能效。

Abstract: Large Language Models (LLMs) face an inherent challenge: their knowledge is
confined to the data that they have been trained on. To overcome this issue,
Retrieval-Augmented Generation (RAG) complements the static training-derived
knowledge of LLMs with an external knowledge repository. RAG consists of three
stages: indexing, retrieval, and generation. The retrieval stage of RAG becomes
a significant bottleneck in inference pipelines. In this stage, a user query is
mapped to an embedding vector and an Approximate Nearest Neighbor Search (ANNS)
algorithm searches for similar vectors in the database to identify relevant
items. Due to the large database sizes, ANNS incurs significant data movement
overheads between the host and the storage system. To alleviate these
overheads, prior works propose In-Storage Processing (ISP) techniques that
accelerate ANNS by performing computations inside storage. However, existing
works that leverage ISP for ANNS (i) employ algorithms that are not tailored to
ISP systems, (ii) do not accelerate data retrieval operations for data selected
by ANNS, and (iii) introduce significant hardware modifications, limiting
performance and hindering their adoption. We propose REIS, the first ISP system
tailored for RAG that addresses these limitations with three key mechanisms.
First, REIS employs a database layout that links database embedding vectors to
their associated documents, enabling efficient retrieval. Second, it enables
efficient ANNS by introducing an ISP-tailored data placement technique that
distributes embeddings across the planes of the storage system and employs a
lightweight Flash Translation Layer. Third, REIS leverages an ANNS engine that
uses the existing computational resources inside the storage system. Compared
to a server-grade system, REIS improves the performance (energy efficiency) of
retrieval by an average of 13x (55x).

</details>


### [87] [StoryWriter: A Multi-Agent Framework for Long Story Generation](https://arxiv.org/abs/2506.16445)
*Haotian Xia,Hao Peng,Yunjia Qi,Xiaozhi Wang,Bin Xu,Lei Hou,Juanzi Li*

Main category: cs.CL

TL;DR: 论文提出StoryWriter框架，通过多智能体协作解决长故事生成的连贯性和复杂性挑战，显著优于现有基线。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型在长故事生成中面临连贯性和复杂性挑战，需改进。

Method: StoryWriter框架包含三个模块：提纲生成、事件规划和动态写作，确保故事连贯。

Result: StoryWriter在质量和长度上显著优于基线，并生成了6000篇高质量长故事数据集。

Conclusion: StoryWriter框架在长故事生成中表现出色，并通过微调进一步优化性能。

Abstract: Long story generation remains a challenge for existing large language models
(LLMs), primarily due to two main factors: (1) discourse coherence, which
requires plot consistency, logical coherence, and completeness in the long-form
generation, and (2) narrative complexity, which requires an interwoven and
engaging narrative. To address these challenges, we propose StoryWriter, a
multi-agent story generation framework, which consists of three main modules:
(1) outline agent, which generates event-based outlines containing rich event
plots, character, and event-event relationships. (2) planning agent, which
further details events and plans which events should be written in each chapter
to maintain an interwoven and engaging story. (3) writing agent, which
dynamically compresses the story history based on the current event to generate
and reflect new plots, ensuring the coherence of the generated story. We
conduct both human and automated evaluation, and StoryWriter significantly
outperforms existing story generation baselines in both story quality and
length. Furthermore, we use StoryWriter to generate a dataset, which contains
about $6,000$ high-quality long stories, with an average length of $8,000$
words. We train the model Llama3.1-8B and GLM4-9B using supervised fine-tuning
on LongStory and develop StoryWriter_GLM and StoryWriter_GLM, which
demonstrates advanced performance in long story generation.

</details>


### [88] [Towards Generalizable Generic Harmful Speech Datasets for Implicit Hate Speech Detection](https://arxiv.org/abs/2506.16476)
*Saad Almohaimeed,Saleh Almohaimeed,Damla Turgut,Ladislau Bölöni*

Main category: cs.CL

TL;DR: 论文提出了一种检测隐式仇恨言论的方法，通过利用现有有害言论数据集，结合关键样本识别、重新标注和增强技术，显著提升了检测效果。


<details>
  <summary>Details</summary>
Motivation: 隐式仇恨言论是社交媒体平台的新挑战，现有研究多关注显式有害言论，而隐式仇恨言论的检测需求日益迫切。

Method: 方法包括三个关键部分：关键样本识别、重新标注和利用Llama-3 70B与GPT-4o进行数据增强。

Result: 实验结果显示，该方法在隐式仇恨检测上显著优于基线，F1分数提升了12.9分。

Conclusion: 该方法有效提升了隐式仇恨言论的检测能力，并增强了跨数据集的泛化性。

Abstract: Implicit hate speech has recently emerged as a critical challenge for social
media platforms. While much of the research has traditionally focused on
harmful speech in general, the need for generalizable techniques to detect
veiled and subtle forms of hate has become increasingly pressing. Based on
lexicon analysis, we hypothesize that implicit hate speech is already present
in publicly available harmful speech datasets but may not have been explicitly
recognized or labeled by annotators. Additionally, crowdsourced datasets are
prone to mislabeling due to the complexity of the task and often influenced by
annotators' subjective interpretations. In this paper, we propose an approach
to address the detection of implicit hate speech and enhance generalizability
across diverse datasets by leveraging existing harmful speech datasets. Our
method comprises three key components: influential sample identification,
reannotation, and augmentation using Llama-3 70B and GPT-4o. Experimental
results demonstrate the effectiveness of our approach in improving implicit
hate detection, achieving a +12.9-point F1 score improvement compared to the
baseline.

</details>


### [89] [Relic: Enhancing Reward Model Generalization for Low-Resource Indic Languages with Few-Shot Examples](https://arxiv.org/abs/2506.16502)
*Soumya Suvra Ghosal,Vaibhav Singh,Akash Ghosh,Soumyabrata Pal,Subhadip Baidya,Sriparna Saha,Dinesh Manocha*

Main category: cs.CL

TL;DR: RELIC是一种新颖的上下文学习框架，用于低资源印度语言的奖励建模，通过从高资源语言中选择上下文示例，显著提高了奖励模型的准确性。


<details>
  <summary>Details</summary>
Motivation: 现有开源多语言奖励模型主要基于高资源语言的偏好数据训练，导致对低资源印度语言的奖励信号不可靠，而收集这些语言的大规模高质量偏好数据成本过高。

Method: RELIC通过训练一个检索器，使用成对排序目标从高资源语言中选择最能区分偏好和非偏好响应的上下文示例。

Result: 在三个偏好数据集上的实验表明，RELIC显著提高了低资源印度语言的奖励模型准确性，优于现有方法，例如在Bodo语言上提升了12.81%和10.13%。

Conclusion: RELIC为解决低资源语言的奖励建模问题提供了一种高效且实用的方法。

Abstract: Reward models are essential for aligning large language models (LLMs) with
human preferences. However, most open-source multilingual reward models are
primarily trained on preference datasets in high-resource languages, resulting
in unreliable reward signals for low-resource Indic languages. Collecting
large-scale, high-quality preference data for these languages is prohibitively
expensive, making preference-based training approaches impractical. To address
this challenge, we propose RELIC, a novel in-context learning framework for
reward modeling in low-resource Indic languages. RELIC trains a retriever with
a pairwise ranking objective to select in-context examples from auxiliary
high-resource languages that most effectively highlight the distinction between
preferred and less-preferred responses. Extensive experiments on three
preference datasets- PKU-SafeRLHF, WebGPT, and HH-RLHF-using state-of-the-art
open-source reward models demonstrate that RELIC significantly improves reward
model accuracy for low-resource Indic languages, consistently outperforming
existing example selection methods. For example, on Bodo-a low-resource Indic
language-using a LLaMA-3.2-3B reward model, RELIC achieves a 12.81% and 10.13%
improvement in accuracy over zero-shot prompting and state-of-the-art example
selection method, respectively.

</details>


### [90] [Measuring (a Sufficient) World Model in LLMs: A Variance Decomposition Framework](https://arxiv.org/abs/2506.16584)
*Nadav Kunievsky,James A. Evans*

Main category: cs.CL

TL;DR: 论文提出了一种评估大语言模型（LLMs）是否具备稳健世界模型的框架，通过分解模型响应变异性来量化其语义基础行为，发现更大模型在用户意图变化上表现更好，但优势有限且不统一。


<details>
  <summary>Details</summary>
Motivation: 评估LLMs是否具备结构化世界模型，以支持高风险应用中的可靠性。

Method: 提出新评估方法，分解模型响应变异性为用户目的、表达方式和模型不稳定性三部分，量化语义基础行为。

Result: 更大模型在用户意图变化上表现更稳健，但优势有限且不统一。

Conclusion: 需超越准确性基准，采用语义诊断直接评估模型内部世界理解的结构与稳定性。

Abstract: Understanding whether large language models (LLMs) possess a world model-a
structured understanding of the world that supports generalization beyond
surface-level patterns-is central to assessing their reliability, especially in
high-stakes applications. We propose a formal framework for evaluating whether
an LLM exhibits a sufficiently robust world model, defined as producing
consistent outputs across semantically equivalent prompts while distinguishing
between prompts that express different intents. We introduce a new evaluation
approach to measure this that decomposes model response variability into three
components: variability due to user purpose, user articulation, and model
instability. An LLM with a strong world model should attribute most of the
variability in its responses to changes in foundational purpose rather than
superficial changes in articulation. This approach allows us to quantify how
much of a model's behavior is semantically grounded rather than driven by model
instability or alternative wording. We apply this framework to evaluate LLMs
across diverse domains. Our results show how larger models attribute a greater
share of output variability to changes in user purpose, indicating a more
robust world model. This improvement is not uniform, however: larger models do
not consistently outperform smaller ones across all domains, and their
advantage in robustness is often modest. These findings highlight the
importance of moving beyond accuracy-based benchmarks toward semantic
diagnostics that more directly assess the structure and stability of a model's
internal understanding of the world.

</details>


### [91] [A Scoping Review of Synthetic Data Generation for Biomedical Research and Applications](https://arxiv.org/abs/2506.16594)
*Hanshu Rao,Weisi Liu,Haohan Wang,I-Chan Huang,Zhe He,Xiaolei Huang*

Main category: cs.CL

TL;DR: 综述分析了2020-2025年间59项研究，探讨了合成数据生成在生物医学领域的应用趋势、方法及评估，指出了当前局限与挑战。


<details>
  <summary>Details</summary>
Motivation: 解决生物医学领域数据稀缺、隐私和质量问题，利用大语言模型（LLMs）推动合成数据生成的发展。

Method: 遵循PRISMA-ScR指南，系统综述了PubMed、ACM、Web of Science和Google Scholar中的59项研究，分析了数据模态、生成方法和评估方式。

Result: 发现主要数据模态为无结构文本（78.0%），生成方法以提示（72.9%）为主，评估方式多样，包括人工参与（55.9%）。

Conclusion: 当前合成数据生成在生物医学领域存在局限，需解决跨临床领域适应性、资源可及性和评估标准化等挑战。

Abstract: Synthetic data generation--mitigating data scarcity, privacy concerns, and
data quality challenges in biomedical fields--has been facilitated by rapid
advances of large language models (LLMs). This scoping review follows
PRISMA-ScR guidelines and synthesizes 59 studies, published between 2020 and
2025 and collected from PubMed, ACM, Web of Science, and Google Scholar. The
review systematically examines biomedical research and application trends in
synthetic data generation, emphasizing clinical applications, methodologies,
and evaluations. Our analysis identifies data modalities of unstructured texts
(78.0%), tabular data (13.6%), and multimodal sources (8.4%); generation
methods of prompting (72.9%), fine-tuning (22.0%) LLMs and specialized model
(5.1%); and heterogeneous evaluations of intrinsic metrics (27.1%),
human-in-the-loop assessments (55.9%), and LLM-based evaluations (13.6%). The
analysis addresses current limitations in what, where, and how health
professionals can leverage synthetic data generation for biomedical domains.
Our review also highlights challenges in adaption across clinical domains,
resource and model accessibility, and evaluation standardizations.

</details>


### [92] [Modeling Public Perceptions of Science in Media](https://arxiv.org/abs/2506.16622)
*Jiaxin Pei,Dustin Wright,Isabelle Augenstin,David Jurgens*

Main category: cs.CL

TL;DR: 论文提出了一种计算框架，用于建模公众对科学新闻的感知，并通过大规模数据集和NLP模型预测公众反应，揭示了感知与公众参与之间的直接联系。


<details>
  <summary>Details</summary>
Motivation: 科学传播者难以预测公众对科学新闻的感知和互动，因此需要一种方法来量化公众反应并优化传播策略。

Method: 开发了一个计算框架，建模12个维度的公众感知，创建了包含10,489条注释的大规模数据集，并训练NLP模型预测感知分数。

Result: 研究发现，科学新闻的消费频率是感知的主要驱动因素，而人口统计因素影响较小；感知分数与Reddit上的公众参与度显著相关。

Conclusion: 研究强调了感知建模在科学传播中的重要性，为预测公众兴趣和参与提供了新途径。

Abstract: Effectively engaging the public with science is vital for fostering trust and
understanding in our scientific community. Yet, with an ever-growing volume of
information, science communicators struggle to anticipate how audiences will
perceive and interact with scientific news. In this paper, we introduce a
computational framework that models public perception across twelve dimensions,
such as newsworthiness, importance, and surprisingness. Using this framework,
we create a large-scale science news perception dataset with 10,489 annotations
from 2,101 participants from diverse US and UK populations, providing valuable
insights into public responses to scientific information across domains. We
further develop NLP models that predict public perception scores with a strong
performance. Leveraging the dataset and model, we examine public perception of
science from two perspectives: (1) Perception as an outcome: What factors
affect the public perception of scientific information? (2) Perception as a
predictor: Can we use the estimated perceptions to predict public engagement
with science? We find that individuals' frequency of science news consumption
is the driver of perception, whereas demographic factors exert minimal
influence. More importantly, through a large-scale analysis and carefully
designed natural experiment on Reddit, we demonstrate that the estimated public
perception of scientific information has direct connections with the final
engagement pattern. Posts with more positive perception scores receive
significantly more comments and upvotes, which is consistent across different
scientific information and for the same science, but are framed differently.
Overall, this research underscores the importance of nuanced perception
modeling in science communication, offering new pathways to predict public
interest and engagement with scientific content.

</details>


### [93] [Initial Investigation of LLM-Assisted Development of Rule-Based Clinical NLP System](https://arxiv.org/abs/2506.16628)
*Jianlin Shi,Brian T. Bucher*

Main category: cs.CL

TL;DR: 论文提出了一种利用大语言模型（LLM）辅助开发基于规则的NLP系统的新方法，显著提高了开发效率和性能。


<details>
  <summary>Details</summary>
Motivation: 尽管机器学习和LLM有进展，基于规则的NLP系统因其可解释性和操作效率仍在临床中使用，但其开发和维护成本高。

Method: 在开发阶段使用LLM，专注于从临床笔记中提取相关片段和关键词，用于基于规则的命名实体识别（NER）。

Result: 实验显示，识别临床相关文本片段的召回率极高（Deepseek: 0.98, Qwen: 0.99），关键词提取准确率为1.0。

Conclusion: 该方法为NLP开发提供了新方向，显著提升了基于规则系统的开发效率和透明度。

Abstract: Despite advances in machine learning (ML) and large language models (LLMs),
rule-based natural language processing (NLP) systems remain active in clinical
settings due to their interpretability and operational efficiency. However,
their manual development and maintenance are labor-intensive, particularly in
tasks with large linguistic variability. To overcome these limitations, we
proposed a novel approach employing LLMs solely during the rule-based systems
development phase. We conducted the initial experiments focusing on the first
two steps of developing a rule-based NLP pipeline: find relevant snippets from
the clinical note; extract informative keywords from the snippets for the
rule-based named entity recognition (NER) component. Our experiments
demonstrated exceptional recall in identifying clinically relevant text
snippets (Deepseek: 0.98, Qwen: 0.99) and 1.0 in extracting key terms for NER.
This study sheds light on a promising new direction for NLP development,
enabling semi-automated or automated development of rule-based systems with
significantly faster, more cost-effective, and transparent execution compared
with deep learning model-based solutions.

</details>


### [94] [Long-Context Generalization with Sparse Attention](https://arxiv.org/abs/2506.16640)
*Pavlo Vasylenko,Marcos Treviso,André F. T. Martins*

Main category: cs.CL

TL;DR: 论文提出使用稀疏注意力机制（α-entmax）和自适应可扩展熵最大化（ASEntmax）解决传统softmax注意力在长序列任务中的问题，并通过改进位置编码进一步提升性能。


<details>
  <summary>Details</summary>
Motivation: 传统softmax注意力在长序列任务中会导致注意力分散和表示崩溃，影响对固定大小模式的精确聚焦。

Method: 引入α-entmax稀疏注意力机制和ASEntmax（带可学习温度参数），结合改进的位置编码设计。

Result: 模型在长上下文泛化任务中显著优于softmax、可扩展softmax和固定温度α-entmax基线。

Conclusion: 稀疏注意力机制和自适应温度调节能有效提升长序列任务的性能，位置编码设计对注意力方法有重要影响。

Abstract: Transformer-based architectures traditionally employ softmax to compute
attention weights, which produces dense distributions over all tokens in a
sequence. While effective in many settings, this density has been shown to be
detrimental for tasks that demand precise focus on fixed-size patterns: as
sequence length increases, non-informative tokens accumulate attention
probability mass, leading to dispersion and representational collapse. We show
in this paper that sparse attention mechanisms using $\alpha$-entmax can avoid
these issues, due to their ability to assign exact zeros to irrelevant tokens.
Furthermore, we introduce Adaptive-Scalable Entmax (ASEntmax), which endows
$\alpha$-entmax with a learnable temperature parameter, allowing the attention
distribution to interpolate between sparse (pattern-focused) and dense
(softmax-like) regimes. Finally, we show that the ability to locate and
generalize fixed-size patterns can be further improved through a careful design
of position encodings, which impacts both dense and sparse attention methods.
By integrating ASEntmax into standard transformer layers alongside proper
positional encodings, we show that our models greatly outperform softmax,
scalable softmax, and fixed-temperature $\alpha$-entmax baselines on
long-context generalization.

</details>


### [95] [Arch-Router: Aligning LLM Routing with Human Preferences](https://arxiv.org/abs/2506.16655)
*Co Tran,Salman Paracha,Adil Hafeez,Shuguang Chen*

Main category: cs.CL

TL;DR: 提出了一种基于用户偏好的LLM路由框架Arch-Router，通过匹配查询与用户定义的领域或动作类型，提升路由决策的透明性和灵活性。


<details>
  <summary>Details</summary>
Motivation: 现有LLM路由方法在评估性能和模型选择上存在局限，无法充分捕捉主观偏好。

Method: 提出Arch-Router，一个1.5B的紧凑模型，学习将查询映射到领域-动作偏好以进行路由决策，支持无缝添加新模型。

Result: 在对话数据集上实现了SOTA结果，优于顶级专有模型，且能捕捉主观评价标准。

Conclusion: Arch-Router提供了一种透明、灵活的路由方案，支持动态扩展模型池。

Abstract: With the rapid proliferation of large language models (LLMs) -- each
optimized for different strengths, style, or latency/cost profile -- routing
has become an essential technique to operationalize the use of different
models. However, existing LLM routing approaches are limited in two key ways:
they evaluate performance using benchmarks that often fail to capture human
preferences driven by subjective evaluation criteria, and they typically select
from a limited pool of models. In this work, we propose a preference-aligned
routing framework that guides model selection by matching queries to
user-defined domains (e.g., travel) or action types (e.g., image editing) --
offering a practical mechanism to encode preferences in routing decisions.
Specifically, we introduce \textbf{Arch-Router}, a compact 1.5B model that
learns to map queries to domain-action preferences for model routing decisions.
Our approach also supports seamlessly adding new models for routing without
requiring retraining or architectural modifications. Experiments on
conversational datasets demonstrate that our approach achieves state-of-the-art
(SOTA) results in matching queries with human preferences, outperforming top
proprietary models. Our approach captures subjective evaluation criteria and
makes routing decisions more transparent and flexible. Our model is available
at: \texttt{https://huggingface.co/katanemo/Arch-Router-1.5B}.

</details>


### [96] [Mechanisms vs. Outcomes: Probing for Syntax Fails to Explain Performance on Targeted Syntactic Evaluations](https://arxiv.org/abs/2506.16678)
*Ananth Agarwal,Jasper Jian,Christopher D. Manning,Shikhar Murty*

Main category: cs.CL

TL;DR: 研究发现，尽管大语言模型（LLMs）在语法处理上表现出色，但通过探测提取的语法特征无法预测其在具体语法任务中的表现。


<details>
  <summary>Details</summary>
Motivation: 探索LLMs内部语法表示机制，并验证探测方法是否能可靠预测模型的下游语法表现。

Method: 采用“机制与结果”框架，评估32个开源Transformer模型，比较探测提取的语法特征与下游语法任务的表现。

Result: 探测提取的语法特征与下游语法任务表现无显著关联，表明潜在语法表示与实际行为存在脱节。

Conclusion: 研究揭示了探测方法在预测模型语法行为上的局限性，强调了进一步研究语法表示机制的必要性。

Abstract: Large Language Models (LLMs) exhibit a robust mastery of syntax when
processing and generating text. While this suggests internalized understanding
of hierarchical syntax and dependency relations, the precise mechanism by which
they represent syntactic structure is an open area within interpretability
research. Probing provides one way to identify the mechanism of syntax being
linearly encoded in activations, however, no comprehensive study has yet
established whether a model's probing accuracy reliably predicts its downstream
syntactic performance. Adopting a "mechanisms vs. outcomes" framework, we
evaluate 32 open-weight transformer models and find that syntactic features
extracted via probing fail to predict outcomes of targeted syntax evaluations
across English linguistic phenomena. Our results highlight a substantial
disconnect between latent syntactic representations found via probing and
observable syntactic behaviors in downstream tasks.

</details>


### [97] [LegiGPT: Party Politics and Transport Policy with Large Language Model](https://arxiv.org/abs/2506.16692)
*Hyunsoo Yun,Eun Hak Lee*

Main category: cs.CL

TL;DR: LegiGPT结合LLM和XAI分析立法提案，揭示政治意识形态对交通政策的影响。


<details>
  <summary>Details</summary>
Motivation: 研究立法者政治意识形态对政策制定的影响。

Method: 使用LegiGPT框架，结合GPT-4和XAI技术，分析韩国立法数据。

Result: 保守派和进步派提案数量、选区规模等是关键因素。

Conclusion: 该方法为理解立法动态和政策发展提供了工具。

Abstract: Given the significant influence of lawmakers' political ideologies on
legislative decision-making, understanding their impact on policymaking is
critically important. We introduce a novel framework, LegiGPT, which integrates
a large language model (LLM) with explainable artificial intelligence (XAI) to
analyze transportation-related legislative proposals. LegiGPT employs a
multi-stage filtering and classification pipeline using zero-shot prompting
with GPT-4. Using legislative data from South Korea's 21st National Assembly,
we identify key factors - including sponsor characteristics, political
affiliations, and geographic variables - that significantly influence
transportation policymaking. The LLM was used to classify
transportation-related bill proposals through a stepwise filtering process
based on keywords, phrases, and contextual relevance. XAI techniques were then
applied to examine relationships between party affiliation and associated
attributes. The results reveal that the number and proportion of conservative
and progressive sponsors, along with district size and electoral population,
are critical determinants shaping legislative outcomes. These findings suggest
that both parties contributed to bipartisan legislation through different forms
of engagement, such as initiating or supporting proposals. This integrated
approach provides a valuable tool for understanding legislative dynamics and
guiding future policy development, with broader implications for infrastructure
planning and governance.

</details>


### [98] [ReasonGRM: Enhancing Generative Reward Models through Large Reasoning Models](https://arxiv.org/abs/2506.16712)
*Bin Chen,Xinzge Gao,Chuanrui Hu,Penghang Yu,Hua Zhang,Bing-Kun Bao*

Main category: cs.CL

TL;DR: ReasonGRM是一个三阶段生成奖励模型框架，通过优化推理路径和引入新评估指标，显著提升了生成奖励模型的性能。


<details>
  <summary>Details</summary>
Motivation: 生成奖励模型（GRMs）在捕捉人类偏好方面比标量奖励模型更灵活，但其推理能力不足导致推理路径不完整或过度推测，影响效果。

Method: ReasonGRM采用三阶段框架：1）Zero-RL生成简洁的推理路径；2）引入$R^\star$评分推理路径；3）通过强化学习进一步优化模型。

Result: 在三个公开基准测试中，ReasonGRM平均优于之前最佳GRMs 1.8%，最高超越GPT-4o 5.6%。

Conclusion: ReasonGRM展示了推理感知训练的有效性，并强调了高质量推理选择对可靠偏好建模的重要性。

Abstract: Generative Reward Models (GRMs) provide greater flexibility than scalar
reward models in capturing human preferences, but their effectiveness is
limited by poor reasoning capabilities. This often results in incomplete or
overly speculative reasoning paths, leading to hallucinations or missing key
information in complex tasks. We address this challenge with ReasonGRM, a
three-stage generative reward modeling framework. In the first stage, Zero-RL
is used to generate concise, outcome-directed reasoning paths that reduce the
likelihood of critical omissions. In the second stage, we introduce a novel
evaluation metric, $R^\star$, which scores reasoning paths based on their
generation likelihood. This favors paths that reach correct answers with
minimal exploration, helping to reduce hallucination-prone data during
training. In the final stage, the model is further refined through
reinforcement learning on challenging examples to enhance its preference
discrimination capabilities. Experiments on three public benchmarks show that
ReasonGRM achieves competitive or state-of-the-art performance, outperforming
previous best GRMs by 1.8\% on average and surpassing proprietary models such
as GPT-4o by up to 5.6\%. These results demonstrate the effectiveness of
reasoning-aware training and highlight the importance of high-quality rationale
selection for reliable preference modeling.

</details>


### [99] [The Role of Model Confidence on Bias Effects in Measured Uncertainties](https://arxiv.org/abs/2506.16724)
*Xinyi Liu,Weiguang Wang,Hangfeng He*

Main category: cs.CL

TL;DR: 论文研究了在大型语言模型（LLMs）中如何准确评估认知不确定性，并探讨了偏置对不确定性量化的影响。实验表明，减少提示偏置能改善GPT-4o的不确定性量化，且偏置对认知和随机不确定性的影响因模型置信度而异。


<details>
  <summary>Details</summary>
Motivation: 随着LLMs在开放任务中的广泛应用，准确评估认知不确定性（反映模型知识不足）对确保可靠结果至关重要。然而，由于随机不确定性的存在（源于多个有效答案），量化认知不确定性具有挑战性。偏置可能影响不确定性估计，但其作用尚不明确。

Method: 研究通过视觉问答（VQA）任务进行实验，分析提示偏置对GPT-4o和Qwen2-VL中认知和随机不确定性的影响，并探讨偏置与模型置信度的关系。

Result: 发现偏置在模型置信度较低时对两种不确定性的影响更大。低置信度下，偏置会导致认知不确定性被低估（即过度自信），但对随机不确定性的方向无显著影响。

Conclusion: 研究揭示了偏置对不确定性量化的复杂影响，为开发更先进的技术提供了理论支持。

Abstract: With the growing adoption of Large Language Models (LLMs) for open-ended
tasks, accurately assessing epistemic uncertainty, which reflects a model's
lack of knowledge, has become crucial to ensuring reliable outcomes. However,
quantifying epistemic uncertainty in such tasks is challenging due to the
presence of aleatoric uncertainty, which arises from multiple valid answers.
While bias can introduce noise into epistemic uncertainty estimation, it may
also reduce noise from aleatoric uncertainty. To investigate this trade-off, we
conduct experiments on Visual Question Answering (VQA) tasks and find that
mitigating prompt-introduced bias improves uncertainty quantification in
GPT-4o. Building on prior work showing that LLMs tend to copy input information
when model confidence is low, we further analyze how these prompt biases affect
measured epistemic and aleatoric uncertainty across varying bias-free
confidence levels with GPT-4o and Qwen2-VL. We find that all considered biases
induce greater changes in both uncertainties when bias-free model confidence is
lower. Moreover, lower bias-free model confidence leads to greater
underestimation of epistemic uncertainty (i.e. overconfidence) due to bias,
whereas it has no significant effect on the direction of changes in aleatoric
uncertainty estimation. These distinct effects deepen our understanding of bias
mitigation for uncertainty quantification and potentially inform the
development of more advanced techniques.

</details>


### [100] [Language-Informed Synthesis of Rational Agent Models for Grounded Theory-of-Mind Reasoning On-The-Fly](https://arxiv.org/abs/2506.16755)
*Lance Ying,Ryan Truong,Katherine M. Collins,Cedegao E. Zhang,Megan Wei,Tyler Brooke-Wilson,Tan Zhi-Xuan,Lionel Wong,Joshua B. Tenenbaum*

Main category: cs.CL

TL;DR: LIRAS框架整合语言和视觉输入，通过多模态语言模型和贝叶斯逆规划引擎，提升社交推理任务的表现。


<details>
  <summary>Details</summary>
Motivation: 现实世界中的社交推理需要多模态信息，语言在社交环境中尤为重要，尤其是在新情境中。

Method: 提出LIRAS框架，结合多模态语言模型和贝叶斯逆规划引擎，构建结构化、情境特定的代理和环境表征。

Result: 在多个社交推理任务中，LIRAS表现优于现有模型，更接近人类判断。

Conclusion: LIRAS通过整合语言和视觉输入，有效提升了社交推理的准确性和适用性。

Abstract: Drawing real world social inferences usually requires taking into account
information from multiple modalities. Language is a particularly powerful
source of information in social settings, especially in novel situations where
language can provide both abstract information about the environment dynamics
and concrete specifics about an agent that cannot be easily visually observed.
In this paper, we propose Language-Informed Rational Agent Synthesis (LIRAS), a
framework for drawing context-specific social inferences that integrate
linguistic and visual inputs. LIRAS frames multimodal social reasoning as a
process of constructing structured but situation-specific agent and environment
representations - leveraging multimodal language models to parse language and
visual inputs into unified symbolic representations, over which a Bayesian
inverse planning engine can be run to produce granular probabilistic judgments.
On a range of existing and new social reasoning tasks derived from cognitive
science experiments, we find that our model (instantiated with a comparatively
lightweight VLM) outperforms ablations and state-of-the-art models in capturing
human judgments across all domains.

</details>


### [101] [SocialSim: Towards Socialized Simulation of Emotional Support Conversation](https://arxiv.org/abs/2506.16756)
*Zhuang Chen,Yaru Cao,Guanqun Bi,Jincenzi Wu,Jinfeng Zhou,Xiyao Xiao,Si Chen,Hongning Wang,Minlie Huang*

Main category: cs.CL

TL;DR: SocialSim框架通过整合社交互动中的关键要素（社交披露和社交意识），模拟情感支持对话（ESC），并构建了高质量的合成ESC语料库SSConv。


<details>
  <summary>Details</summary>
Motivation: 现有方法忽视了ESC中的社交动态，导致模拟效果不佳。

Method: SocialSim框架通过构建全面的角色库（社交披露）和激发认知推理（社交意识）来模拟ESC。

Result: 构建的SSConv语料库质量超越众包数据，训练出的聊天机器人表现优异。

Conclusion: SocialSim为ESC合成提供了可扩展的方法，使情感关怀更易实现。

Abstract: Emotional support conversation (ESC) helps reduce people's psychological
stress and provide emotional value through interactive dialogues. Due to the
high cost of crowdsourcing a large ESC corpus, recent attempts use large
language models for dialogue augmentation. However, existing approaches largely
overlook the social dynamics inherent in ESC, leading to less effective
simulations. In this paper, we introduce SocialSim, a novel framework that
simulates ESC by integrating key aspects of social interactions: social
disclosure and social awareness. On the seeker side, we facilitate social
disclosure by constructing a comprehensive persona bank that captures diverse
and authentic help-seeking scenarios. On the supporter side, we enhance social
awareness by eliciting cognitive reasoning to generate logical and supportive
responses. Building upon SocialSim, we construct SSConv, a large-scale
synthetic ESC corpus of which quality can even surpass crowdsourced ESC data.
We further train a chatbot on SSConv and demonstrate its state-of-the-art
performance in both automatic and human evaluations. We believe SocialSim
offers a scalable way to synthesize ESC, making emotional care more accessible
and practical.

</details>


### [102] [Cross-Modal Obfuscation for Jailbreak Attacks on Large Vision-Language Models](https://arxiv.org/abs/2506.16760)
*Lei Jiang,Zixun Zhang,Zizhou Wang,Xiaobing Sun,Zhen Li,Liangli Zhen,Xiaohua Xu*

Main category: cs.CL

TL;DR: CAMO是一种新型黑盒越狱攻击框架，通过将恶意提示分解为视觉和文本片段，利用LVLMs的跨模态推理能力绕过安全机制。


<details>
  <summary>Details</summary>
Motivation: 现有越狱方法易被检测且效率低，需更隐蔽高效的攻击方式。

Method: CAMO将恶意提示分解为语义无害的视觉和文本片段，利用跨模态推理重建有害指令。

Result: CAMO在主流LVLMs上表现优异，具有高效性和跨模型迁移能力。

Conclusion: 当前安全机制存在漏洞，需开发更先进的视觉-语言系统安全解决方案。

Abstract: Large Vision-Language Models (LVLMs) demonstrate exceptional performance
across multimodal tasks, yet remain vulnerable to jailbreak attacks that bypass
built-in safety mechanisms to elicit restricted content generation. Existing
black-box jailbreak methods primarily rely on adversarial textual prompts or
image perturbations, yet these approaches are highly detectable by standard
content filtering systems and exhibit low query and computational efficiency.
In this work, we present Cross-modal Adversarial Multimodal Obfuscation (CAMO),
a novel black-box jailbreak attack framework that decomposes malicious prompts
into semantically benign visual and textual fragments. By leveraging LVLMs'
cross-modal reasoning abilities, CAMO covertly reconstructs harmful
instructions through multi-step reasoning, evading conventional detection
mechanisms. Our approach supports adjustable reasoning complexity and requires
significantly fewer queries than prior attacks, enabling both stealth and
efficiency. Comprehensive evaluations conducted on leading LVLMs validate
CAMO's effectiveness, showcasing robust performance and strong cross-model
transferability. These results underscore significant vulnerabilities in
current built-in safety mechanisms, emphasizing an urgent need for advanced,
alignment-aware security and safety solutions in vision-language systems.

</details>


### [103] [DistillNote: LLM-based clinical note summaries improve heart failure diagnosis](https://arxiv.org/abs/2506.16777)
*Heloisa Oss Boll,Antonio Oss Boll,Leticia Puttlitz Boll,Ameen Abu Hanna,Iacer Calixto*

Main category: cs.CL

TL;DR: Distillnote框架利用LLM生成临床笔记摘要，通过三种技术实现高效压缩和性能提升，并在心衰预测中表现优于原始笔记。


<details>
  <summary>Details</summary>
Motivation: 减轻临床文档负担，提升医疗信息摘要的效率和实用性。

Method: 采用一步直接摘要、结构化摘要和蒸馏摘要三种技术生成临床笔记摘要。

Result: 蒸馏摘要实现79%文本压缩，AUPRC提升18.2%，临床评估显示一步摘要更受青睐。

Conclusion: Distillnote框架在临床摘要中表现出高效性和实用性，为未来研究提供资源。

Abstract: Large language models (LLMs) offer unprecedented opportunities to generate
concise summaries of patient information and alleviate the burden of clinical
documentation that overwhelms healthcare providers. We present Distillnote, a
framework for LLM-based clinical note summarization, and generate over 64,000
admission note summaries through three techniques: (1) One-step, direct
summarization, and a divide-and-conquer approach involving (2) Structured
summarization focused on independent clinical insights, and (3) Distilled
summarization that further condenses the Structured summaries. We test how
useful are the summaries by using them to predict heart failure compared to a
model trained on the original notes. Distilled summaries achieve 79% text
compression and up to 18.2% improvement in AUPRC compared to an LLM trained on
the full notes. We also evaluate the quality of the generated summaries in an
LLM-as-judge evaluation as well as through blinded pairwise comparisons with
clinicians. Evaluations indicate that one-step summaries are favoured by
clinicians according to relevance and clinical actionability, while distilled
summaries offer optimal efficiency (avg. 6.9x compression-to-performance ratio)
and significantly reduce hallucinations. We release our summaries on PhysioNet
to encourage future research.

</details>


### [104] [MIST: Jailbreaking Black-box Large Language Models via Iterative Semantic Tuning](https://arxiv.org/abs/2506.16792)
*Muyang Zheng,Yuanzhi Yao,Changting Lin,Rui Wang,Meng Han*

Main category: cs.CL

TL;DR: 论文提出了一种名为MIST的方法，通过迭代语义调整破解黑盒大语言模型，平衡语义相似性与计算效率，实验证明其攻击成功率和迁移性优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型（LLMs）已努力与社会和道德价值观对齐，但仍易受破解攻击，而黑盒LLMs的破解因离散输入、访问限制和有限查询预算而更具挑战性。

Method: 提出MIST方法，通过迭代语义调整生成保留原语义但诱导有害内容的提示，结合顺序同义词搜索和顺序确定优化策略。

Result: 在开源和闭源模型上的实验显示，MIST在攻击成功率和迁移性上优于现有白盒和黑盒破解方法，且计算效率验证了其实际可行性。

Conclusion: MIST是一种高效的黑盒LLMs破解方法，具有实际应用潜力。

Abstract: Despite efforts to align large language models (LLMs) with societal and moral
values, these models remain susceptible to jailbreak attacks--methods designed
to elicit harmful responses. Jailbreaking black-box LLMs is considered
challenging due to the discrete nature of token inputs, restricted access to
the target LLM, and limited query budget. To address the issues above, we
propose an effective method for jailbreaking black-box large language Models
via Iterative Semantic Tuning, named MIST. MIST enables attackers to
iteratively refine prompts that preserve the original semantic intent while
inducing harmful content. Specifically, to balance semantic similarity with
computational efficiency, MIST incorporates two key strategies: sequential
synonym search, and its advanced version--order-determining optimization.
Extensive experiments across two open-source models and four closed-source
models demonstrate that MIST achieves competitive attack success rates and
attack transferability compared with other state-of-the-art white-box and
black-box jailbreak methods. Additionally, we conduct experiments on
computational efficiency to validate the practical viability of MIST.

</details>


### [105] [From Data to Knowledge: Evaluating How Efficiently Language Models Learn Facts](https://arxiv.org/abs/2506.16912)
*Daniel Christoph,Max Ploner,Patrick Haller,Alan Akbik*

Main category: cs.CL

TL;DR: 研究分析了不同架构和大小的语言模型在样本效率上的表现，发现模型在高频事实上的表现相似，但在低频事实上差异显著。


<details>
  <summary>Details</summary>
Motivation: 现实文本中信息呈长尾分布，模型需高效学习和记忆罕见信息，样本效率是关键。

Method: 通过标注训练语料中关系事实的频率，分析不同模型在不同频率事实上的表现。

Result: 大多数模型在高频事实上表现相似，但在低频事实上差异显著。

Conclusion: 研究揭示了模型架构、大小与事实学习效率之间的关系。

Abstract: Sample efficiency is a crucial property of language models with practical
implications for training efficiency. In real-world text, information follows a
long-tailed distribution. Yet, we expect models to learn and recall frequent
and infrequent facts. Sample-efficient models are better equipped to handle
this challenge of learning and retaining rare information without requiring
excessive exposure. This study analyzes multiple models of varying
architectures and sizes, all trained on the same pre-training data. By
annotating relational facts with their frequencies in the training corpus, we
examine how model performance varies with fact frequency. Our findings show
that most models perform similarly on high-frequency facts but differ notably
on low-frequency facts. This analysis provides new insights into the
relationship between model architecture, size, and factual learning efficiency.

</details>


### [106] [Language Bottleneck Models: A Framework for Interpretable Knowledge Tracing and Beyond](https://arxiv.org/abs/2506.16982)
*Antonin Berthon,Mihaela van der Schaar*

Main category: cs.CL

TL;DR: 论文提出了一种语言瓶颈模型（LBM），通过自然语言摘要解决知识追踪（KT）的透明性问题，同时保持高准确性。


<details>
  <summary>Details</summary>
Motivation: 传统知识追踪方法依赖不透明的潜在嵌入，限制了可解释性；而基于LLM的方法可能产生不准确的预测或摘要。

Method: LBM包括一个编码器LLM生成可解释的知识摘要，和一个冻结的解码器LLM基于摘要重建和预测学生回答。通过自然语言瓶颈约束所有预测信息。

Result: 在合成算术基准和Eedi数据集上，LBM的准确性媲美最先进的KT和直接LLM方法，且所需学生轨迹数据更少。

Conclusion: LBM通过语言瓶颈实现了知识追踪的高准确性和可解释性，同时优化编码器可提升摘要质量。

Abstract: Accurately assessing student knowledge is critical for effective education,
yet traditional Knowledge Tracing (KT) methods rely on opaque latent
embeddings, limiting interpretability. Even LLM-based approaches generate
direct predictions or summaries that may hallucinate without any accuracy
guarantees. We recast KT as an inverse problem: learning the minimum
natural-language summary that makes past answers explainable and future answers
predictable. Our Language Bottleneck Model (LBM) consists of an encoder LLM
that writes an interpretable knowledge summary and a frozen decoder LLM that
must reconstruct and predict student responses using only that summary text. By
constraining all predictive information to pass through a short
natural-language bottleneck, LBMs ensure that the summary contains accurate
information while remaining human-interpretable. Experiments on synthetic
arithmetic benchmarks and the large-scale Eedi dataset show that LBMs rival the
accuracy of state-of-the-art KT and direct LLM methods while requiring
orders-of-magnitude fewer student trajectories. We demonstrate that training
the encoder with group-relative policy optimization, using downstream decoding
accuracy as a reward signal, effectively improves summary quality.

</details>


### [107] [TeXpert: A Multi-Level Benchmark for Evaluating LaTeX Code Generation by LLMs](https://arxiv.org/abs/2506.16990)
*Sahil Kale,Vijaykant Nadadur*

Main category: cs.CL

TL;DR: TeXpert是一个评估LLMs生成LaTeX代码能力的基准数据集，发现LLMs在复杂任务中表现不佳，开源模型表现接近闭源模型，且格式和包错误常见。


<details>
  <summary>Details</summary>
Motivation: 当前基准缺乏对LLMs生成LaTeX代码能力的评估，研究者希望通过TeXpert填补这一空白。

Method: 引入TeXpert数据集，包含多难度级别的自然语言提示，用于生成LaTeX代码，并对开源和闭源LLMs进行深入分析。

Result: LLMs在标准基准表现优异，但在LaTeX生成中表现差，复杂度增加时准确性下降；开源模型（如DeepSeek v3）表现接近闭源模型；格式和包错误普遍。

Conclusion: LLMs在LaTeX生成任务中表现不佳，训练数据缺乏多样性可能是主要原因，开源模型表现优异。

Abstract: LaTeX's precision and flexibility in typesetting have made it the gold
standard for the preparation of scientific documentation. Large Language Models
(LLMs) present a promising opportunity for researchers to produce
publication-ready material using LaTeX with natural language instructions, yet
current benchmarks completely lack evaluation of this ability. By introducing
TeXpert, our benchmark dataset with natural language prompts for generating
LaTeX code focused on components of scientific documents across multiple
difficulty levels, we conduct an in-depth analysis of LLM performance in this
regard and identify frequent error types. Our evaluation across open and
closed-source LLMs highlights multiple key findings: LLMs excelling on standard
benchmarks perform poorly in LaTeX generation with a significant accuracy
drop-off as the complexity of tasks increases; open-source models like DeepSeek
v3 and DeepSeek Coder strongly rival closed-source counterparts in LaTeX tasks;
and formatting and package errors are unexpectedly prevalent, suggesting a lack
of diverse LaTeX examples in the training datasets of most LLMs. Our dataset,
code, and model evaluations are available at
https://github.com/knowledge-verse-ai/TeXpert.

</details>


### [108] [PersonalAI: Towards digital twins in the graph form](https://arxiv.org/abs/2506.17001)
*Mikhail Menschikov,Dmitry Evseev,Ruslan Kostoev,Ilya Perepechkin,Ilnaz Salimov,Victoria Dochkina,Petr Anokhin,Evgeny Burnaev,Nikita Semenov*

Main category: cs.CL

TL;DR: 论文提出了一种利用知识图谱作为外部记忆的方法，以增强语言模型的个性化能力，并通过实验验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 解决语言模型在个性化交互中难以保留和利用用户历史信息的问题。

Method: 采用知识图谱作为外部记忆，结合标准边和超边，扩展了AriGraph架构。

Result: 在TriviaQA、HotpotQA和DiaASQ基准测试中表现稳健，能够处理时间依赖性和矛盾信息。

Conclusion: 提出的架构能够有效构建和利用知识图谱，提升语言模型的个性化能力。

Abstract: The challenge of personalizing language models, specifically the ability to
account for a user's history during interactions, is of significant interest.
Despite recent advancements in large language models (LLMs) and Retrieval
Augmented Generation that have enhanced the factual base of LLMs, the task of
retaining extensive personal information and using it to generate personalized
responses remains pertinent. To address this, we propose utilizing external
memory in the form of knowledge graphs, which are constructed and updated by
the LLM itself. We have expanded upon ideas of AriGraph architecture and for
the first time introduced a combined graph featuring both standard edges and
two types of hyperedges. Experiments conducted on the TriviaQA, HotpotQA and
DiaASQ benchmarks indicates that this approach aids in making the process of
graph construction and knowledge extraction unified and robust. Furthermore, we
augmented the DiaASQ benchmark by incorporating parameters such as time into
dialogues and introducing contradictory statements made by the same speaker at
different times. Despite these modifications, the performance of the
question-answering system remained robust, demonstrating the proposed
architecture's ability to maintain and utilize temporal dependencies.

</details>


### [109] [LLM-Generated Feedback Supports Learning If Learners Choose to Use It](https://arxiv.org/abs/2506.17006)
*Danielle R. Thomas,Conrad Borchers,Shambhavi Bhushan,Erin Gatz,Shivang Gupta,Kenneth R. Koedinger*

Main category: cs.CL

TL;DR: 研究探讨了LLM生成反馈对学习的影响，发现其效果取决于学习者的使用倾向，部分课程显示显著学习提升。


<details>
  <summary>Details</summary>
Motivation: 探索LLM生成反馈对学习的影响，尤其是与现有反馈方法的比较。

Method: 比较三组学习者（接受LLM反馈、拒绝LLM反馈、无LLM反馈）的后测表现，使用倾向评分调整选择偏差。

Result: 两门课程显示LLM反馈显著提升学习效果（效应量0.28和0.33），学习者普遍认为反馈有帮助且不增加完成时间。

Conclusion: LLM反馈是一种低成本、可扩展的学习改进方法，尤其在已有反馈系统中效果显著。

Abstract: Large language models (LLMs) are increasingly used to generate feedback, yet
their impact on learning remains underexplored, especially compared to existing
feedback methods. This study investigates how on-demand LLM-generated
explanatory feedback influences learning in seven scenario-based tutor training
lessons. Analyzing over 2,600 lesson completions from 885 tutor learners, we
compare posttest performance among learners across three groups: learners who
received feedback generated by gpt-3.5-turbo, those who declined it, and those
without access. All groups received non-LLM corrective feedback. To address
potential selection bias-where higher-performing learners may be more inclined
to use LLM feedback-we applied propensity scoring. Learners with a higher
predicted likelihood of engaging with LLM feedback scored significantly higher
at posttest than those with lower propensity. After adjusting for this effect,
two out of seven lessons showed statistically significant learning benefits
from LLM feedback with standardized effect sizes of 0.28 and 0.33. These
moderate effects suggest that the effectiveness of LLM feedback depends on the
learners' tendency to seek support. Importantly, LLM feedback did not
significantly increase completion time, and learners overwhelmingly rated it as
helpful. These findings highlight LLM feedback's potential as a low-cost and
scalable way to improve learning on open-ended tasks, particularly in existing
systems already providing feedback without LLMs. This work contributes open
datasets, LLM prompts, and rubrics to support reproducibility.

</details>


### [110] [Instituto de Telecomunicações at IWSLT 2025: Aligning Small-Scale Speech and Language Models for Speech-to-Text Learning](https://arxiv.org/abs/2506.17019)
*Giuseppe Attanasio,Sonal Sannigrahi,Ben Peters,André F. T. Martins*

Main category: cs.CL

TL;DR: 本文介绍了IT-IST团队在IWSLT 2025共享任务中的提交，专注于语音识别、翻译和口语问答，采用小型语言模型（<2B）和高质量数据。


<details>
  <summary>Details</summary>
Motivation: 旨在通过统一语音到文本模型，结合预训练语音编码器和文本解码器，提升指令跟随语音处理任务的性能。

Method: 采用两阶段方法：模态对齐和指令微调，结合高质量CC-BY数据和合成数据补充资源。

Result: 提交了在短赛道（语音识别、翻译和口语问答）上的结果。

Conclusion: 通过小型语言模型和高质量数据，实现了高效的指令跟随语音处理。

Abstract: This paper presents the IT-IST submission to the IWSLT 2025 Shared Task on
Instruction Following Speech Processing. We submit results for the Short Track,
i.e., speech recognition, translation, and spoken question answering. Our model
is a unified speech-to-text model that integrates a pre-trained continuous
speech encoder and text decoder through a first phase of modality alignment and
a second phase of instruction fine-tuning. Crucially, we focus on using
small-scale language model backbones (< 2B) and restrict to high-quality, CC-BY
data along with synthetic data generation to supplement existing resources.

</details>


### [111] [MUCAR: Benchmarking Multilingual Cross-Modal Ambiguity Resolution for Multimodal Large Language Models](https://arxiv.org/abs/2506.17046)
*Xiaolong Wang,Zhaolu Kang,Wangyuxuan Zhai,Xinyue Lou,Yunghwei Lai,Ziyue Wang,Yawen Wang,Kaiyu Huang,Yile Wang,Peng Li,Yang Liu*

Main category: cs.CL

TL;DR: 论文介绍了MUCAR，一个用于评估多模态模糊解决能力的新基准，包含多语言和双模糊数据集，测试了19种先进模型，发现与人类表现存在显著差距。


<details>
  <summary>Details</summary>
Motivation: 现有多模态基准通常忽略语言和视觉模糊性，未能充分利用模态间的相互澄清潜力，因此需要新的评估工具。

Method: 提出MUCAR基准，包括多语言数据集和双模糊数据集，通过视觉和文本上下文的相互澄清解决模糊性。

Result: 测试19种先进模型，发现它们在多模态模糊解决任务中表现远低于人类水平。

Conclusion: 未来研究需开发更复杂的跨模态模糊理解方法，以推动多模态推理的边界。

Abstract: Multimodal Large Language Models (MLLMs) have demonstrated significant
advances across numerous vision-language tasks. Due to their strong image-text
alignment capability, MLLMs can effectively understand image-text pairs with
clear meanings. However, effectively resolving the inherent ambiguities in
natural language and visual contexts remains challenging. Existing multimodal
benchmarks typically overlook linguistic and visual ambiguities, relying mainly
on unimodal context for disambiguation and thus failing to exploit the mutual
clarification potential between modalities. To bridge this gap, we introduce
MUCAR, a novel and challenging benchmark designed explicitly for evaluating
multimodal ambiguity resolution across multilingual and cross-modal scenarios.
MUCAR includes: (1) a multilingual dataset where ambiguous textual expressions
are uniquely resolved by corresponding visual contexts, and (2) a
dual-ambiguity dataset that systematically pairs ambiguous images with
ambiguous textual contexts, with each combination carefully constructed to
yield a single, clear interpretation through mutual disambiguation. Extensive
evaluations involving 19 state-of-the-art multimodal models--encompassing both
open-source and proprietary architectures--reveal substantial gaps compared to
human-level performance, highlighting the need for future research into more
sophisticated cross-modal ambiguity comprehension methods, further pushing the
boundaries of multimodal reasoning.

</details>


### [112] [Simultaneous Translation with Offline Speech and LLM Models in CUNI Submission to IWSLT 2025](https://arxiv.org/abs/2506.17077)
*Dominik Macháček,Peter Polák*

Main category: cs.CL

TL;DR: 本文介绍了查尔斯大学在IWSLT 2025同步语音翻译任务中的提交方案，涵盖四种语言对，采用直接或级联方法，基于Whisper模型和AlignAtt策略，并通过提示和上下文优化性能。


<details>
  <summary>Details</summary>
Motivation: 提升同步语音翻译的性能，特别是在多语言对任务中，同时优化延迟和翻译质量。

Method: 使用Whisper模型和AlignAtt策略，结合提示和上下文优化，级联系统采用EuroLLM进行无界同步翻译。

Result: 相比基线，捷克语到英语提升2 BLEU点，英语到德语、汉语和日语提升13-22 BLEU点。

Conclusion: 提出的方法显著提升了翻译质量，并引入了新的语音识别延迟测量方法。

Abstract: This paper describes Charles University submission to the Simultaneous Speech
Translation Task of the IWSLT 2025. We cover all four language pairs with a
direct or cascade approach. The backbone of our systems is the offline Whisper
speech model, which we use for both translation and transcription in
simultaneous mode with the state-of-the-art simultaneous policy AlignAtt. We
further improve the performance by prompting to inject in-domain terminology,
and we accommodate context. Our cascaded systems further use EuroLLM for
unbounded simultaneous translation. Compared to the Organizers' baseline, our
systems improve by 2 BLEU points on Czech to English and 13-22 BLEU points on
English to German, Chinese and Japanese on the development sets. Additionally,
we also propose a new enhanced measure of speech recognition latency.

</details>


### [113] [Tower+: Bridging Generality and Translation Specialization in Multilingual LLMs](https://arxiv.org/abs/2506.17080)
*Ricardo Rei,Nuno M. Guerreiro,José Pombal,João Alves,Pedro Teixeirinha,Amin Farajian,André F. T. Martins*

Main category: cs.CL

TL;DR: 论文提出Tower+模型套件，通过多阶段训练方法在翻译和多语言通用任务上实现高性能，同时避免牺牲通用能力。


<details>
  <summary>Details</summary>
Motivation: 微调预训练大模型虽能提升特定任务性能，但会削弱通用能力，影响实际应用。

Method: 采用持续预训练、监督微调、偏好优化和强化学习的多阶段训练方法，并精心生成数据。

Result: Tower+模型在翻译和多语言通用任务上表现优异，小模型超越大模型，大模型在高资源语言翻译中领先。

Conclusion: 研究表明，优化特定领域（如翻译）的同时，仍可保持与前沿模型相当的通用能力。

Abstract: Fine-tuning pretrained LLMs has been shown to be an effective strategy for
reaching state-of-the-art performance on specific tasks like machine
translation. However, this process of adaptation often implies sacrificing
general-purpose capabilities, such as conversational reasoning and
instruction-following, hampering the utility of the system in real-world
applications that require a mixture of skills. In this paper, we introduce
Tower+, a suite of models designed to deliver strong performance across both
translation and multilingual general-purpose text capabilities. We achieve a
Pareto frontier between translation specialization and multilingual
general-purpose capabilities by introducing a novel training recipe that builds
on Tower (Alves et al., 2024), comprising continued pretraining, supervised
fine-tuning, preference optimization, and reinforcement learning with
verifiable rewards. At each stage of training, we carefully generate and curate
data to strengthen performance on translation as well as general-purpose tasks
involving code generation, mathematics problem solving, and general
instruction-following. We develop models at multiple scales: 2B, 9B, and 72B.
Our smaller models often outperform larger general-purpose open-weight and
proprietary LLMs (e.g., Llama 3.3 70B, GPT-4o). Our largest model delivers
best-in-class translation performance for high-resource languages and top
results in multilingual Arena Hard evaluations and in IF-MT, a benchmark we
introduce for evaluating both translation and instruction-following. Our
findings highlight that it is possible to rival frontier models in general
capabilities, while optimizing for specific business domains, such as
translation and localization.

</details>


### [114] [Chain-of-Thought Prompting Obscures Hallucination Cues in Large Language Models: An Empirical Evaluation](https://arxiv.org/abs/2506.17088)
*Jiahao Cheng,Tiancheng Su,Jia Yuan,Guoxiu He,Jiawei Liu,Xinqi Tao,Jingwen Xie,Huaxia Li*

Main category: cs.CL

TL;DR: 研究探讨了链式思维（CoT）提示对大型语言模型（LLM）幻觉检测的影响，发现CoT虽减少幻觉频率，但会削弱检测信号。


<details>
  <summary>Details</summary>
Motivation: LLM常产生幻觉内容，CoT提示可缓解这一问题，但其对幻觉检测的影响尚未充分研究。

Method: 通过实验评估CoT提示对LLM内部状态和幻觉检测方法的影响，分析幻觉分数分布、检测准确性和置信度的变化。

Result: CoT提示减少幻觉频率，但会掩盖检测信号，降低检测方法的有效性。

Conclusion: 研究揭示了使用推理时的权衡，提示需谨慎结合CoT与幻觉检测。

Abstract: Large Language Models (LLMs) often exhibit \textit{hallucinations},
generating factually incorrect or semantically irrelevant content in response
to prompts. Chain-of-Thought (CoT) prompting can mitigate hallucinations by
encouraging step-by-step reasoning, but its impact on hallucination detection
remains underexplored. To bridge this gap, we conduct a systematic empirical
evaluation. We begin with a pilot experiment, revealing that CoT reasoning
significantly affects the LLM's internal states and token probability
distributions. Building on this, we evaluate the impact of various CoT
prompting methods on mainstream hallucination detection methods across both
instruction-tuned and reasoning-oriented LLMs. Specifically, we examine three
key dimensions: changes in hallucination score distributions, variations in
detection accuracy, and shifts in detection confidence. Our findings show that
while CoT prompting helps reduce hallucination frequency, it also tends to
obscure critical signals used for detection, impairing the effectiveness of
various detection methods. Our study highlights an overlooked trade-off in the
use of reasoning. Code is publicly available at:
https://anonymous.4open.science/r/cot-hallu-detect.

</details>


### [115] [Better Language Model Inversion by Compactly Representing Next-Token Distributions](https://arxiv.org/abs/2506.17090)
*Murtaza Nazir,Matthew Finlayson,John X. Morris,Xiang Ren,Swabha Swayamdipta*

Main category: cs.CL

TL;DR: 提出了一种新的方法PILS，通过语言模型的多步生成概率恢复隐藏提示，显著提升了恢复率，并展示了在跨模型任务中的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 研究语言模型反转能力对安全和问责的影响，特别是如何从API保护的语言模型中泄露私有信息。

Method: 提出PILS方法，利用语言模型的多步生成概率的低维子空间特性，通过线性映射无损压缩概率分布，从而恢复隐藏提示。

Result: PILS方法在恢复隐藏提示上比现有方法提升2-3.5倍，恢复率从17%提高到60%，并在跨模型任务中表现出色。

Conclusion: 语言模型的下一个标记概率是反转攻击的脆弱点，PILS方法为安全和问责提供了新的研究方向。

Abstract: Language model inversion seeks to recover hidden prompts using only language
model outputs. This capability has implications for security and accountability
in language model deployments, such as leaking private information from an
API-protected language model's system message. We propose a new method --
prompt inversion from logprob sequences (PILS) -- that recovers hidden prompts
by gleaning clues from the model's next-token probabilities over the course of
multiple generation steps. Our method is enabled by a key insight: The
vector-valued outputs of a language model occupy a low-dimensional subspace.
This enables us to losslessly compress the full next-token probability
distribution over multiple generation steps using a linear map, allowing more
output information to be used for inversion. Our approach yields massive gains
over previous state-of-the-art methods for recovering hidden prompts, achieving
2--3.5 times higher exact recovery rates across test sets, in one case
increasing the recovery rate from 17% to 60%. Our method also exhibits
surprisingly good generalization behavior; for instance, an inverter trained on
16 generations steps gets 5--27 points higher prompt recovery when we increase
the number of steps to 32 at test time. Furthermore, we demonstrate strong
performance of our method on the more challenging task of recovering hidden
system messages. We also analyze the role of verbatim repetition in prompt
recovery and propose a new method for cross-family model transfer for
logit-based inverters. Our findings show that next-token probabilities are a
considerably more vulnerable attack surface for inversion attacks than
previously known.

</details>


### [116] [Cache Me If You Can: How Many KVs Do You Need for Effective Long-Context LMs?](https://arxiv.org/abs/2506.17121)
*Adithya Bhaskar,Alexander Wettig,Tianyu Gao,Yihe Dong,Danqi Chen*

Main category: cs.CL

TL;DR: 论文提出了一种统一的度量标准*KV footprint*，用于评估语言模型中KV缓存的效率，揭示了现有方法的局限性，并提出改进方法以降低内存占用。


<details>
  <summary>Details</summary>
Motivation: 随着语言模型处理长上下文任务的增加，KV缓存的内存成本显著上升，现有方法存在高内存峰值和性能下降的问题，缺乏统一的比较标准。

Method: 提出*KV footprint*作为统一度量，评估方法在保持性能的同时最小化KV缓存的能力；改进*post-fill eviction*方法以支持预填充阶段的KV丢弃，并提出PruLong优化*recency eviction*方法。

Result: 改进后的方法显著降低了KV footprint，PruLong比现有方法减少12%的内存占用，同时保持长上下文任务的性能。

Conclusion: 论文通过统一度量标准和优化方法，为长上下文推理方法的未来发展提供了清晰方向，目标是进一步最小化KV footprint。

Abstract: Language models handle increasingly long contexts for tasks such as book
summarization, but this leads to growing memory costs for the key-value (KV)
cache. Many prior works have proposed ways of discarding KVs from memory, but
their approaches are tailored to favorable settings, obscuring caveats like
high peak memory and performance degradation, and a fair comparison between
methods is difficult. In this paper, we propose the *KV footprint* as a unified
metric, which accounts for both the amount of KV entries stored and their
lifespan in memory. We evaluate methods based on the smallest footprint they
attain while preserving performance in both long-context understanding and
generation, with context lengths of up to 128K tokens. This metric reveals the
high peak memory of prior KV eviction methods. One class of methods --
*post-fill eviction* -- has a high footprint due to being incompatible with
eviction during pre-filling. We adapt these methods to be able to evict KVs
during pre-filling, achieving substantially lower KV footprints. We then turn
to *recency eviction* methods, wherein we propose PruLong, an end-to-end
optimization method for learning which attention heads need to retain the full
KV cache and which do not. PruLong saves memory while preserving long-context
performance, achieving 12% smaller KV footprint than prior methods while
retaining performance in challenging recall tasks. Our paper clarifies the
complex tangle of long-context inference methods and paves the way for future
development to minimize the KV footprint.

</details>


### [117] [CLEAR-3K: Assessing Causal Explanatory Capabilities in Language Models](https://arxiv.org/abs/2506.17180)
*Naiming Liu,Richard Baraniuk,Shashank Sonkar*

Main category: cs.CL

TL;DR: CLEAR-3K是一个包含3000个断言-推理问题的数据集，用于评估语言模型是否能判断一个陈述是否因果解释另一个陈述。研究发现，语言模型常混淆语义相似性与因果关系，且随着参数增加，模型从过度怀疑转向过度接受因果关系，但性能提升有限。


<details>
  <summary>Details</summary>
Motivation: 评估语言模型在区分语义相关性和真实因果关系方面的能力，以推动模型在因果推理上的发展。

Method: 通过CLEAR-3K数据集测试21种先进语言模型（参数从0.5B到72B），分析其对断言-推理对的判断能力。

Result: 模型常混淆语义相似性与因果关系，参数增加导致模型从过度怀疑转向过度接受，但性能仅达0.55（MCC）。

Conclusion: CLEAR-3K为语言模型的因果推理能力提供了关键基准，对需要准确评估因果关系的应用至关重要。

Abstract: We introduce CLEAR-3K, a dataset of 3,000 assertion-reasoning questions
designed to evaluate whether language models can determine if one statement
causally explains another. Each question present an assertion-reason pair and
challenge language models to distinguish between semantic relatedness and
genuine causal explanatory relationships. Through comprehensive evaluation of
21 state-of-the-art language models (ranging from 0.5B to 72B parameters), we
identify two fundamental findings. First, language models frequently confuse
semantic similarity with causality, relying on lexical and semantic overlap
instead of inferring actual causal explanatory relationships. Second, as
parameter size increases, models tend to shift from being overly skeptical
about causal relationships to being excessively permissive in accepting them.
Despite this shift, performance measured by the Matthews Correlation
Coefficient plateaus at just 0.55, even for the best-performing models.Hence,
CLEAR-3K provides a crucial benchmark for developing and evaluating genuine
causal reasoning in language models, which is an essential capability for
applications that require accurate assessment of causal relationships.

</details>


### [118] [Towards AI Search Paradigm](https://arxiv.org/abs/2506.17188)
*Yuchen Li,Hengyi Cai,Rui Kong,Xinran Chen,Jiamin Chen,Jun Yang,Haojie Zhang,Jiayi Li,Jiayi Wu,Yiqun Chen,Changle Qu,Keyi Kong,Wenwen Ye,Lixin Su,Xinyu Ma,Long Xia,Daiting Shi,Jiashu Zhao,Haoyi Xiong,Shuaiqiang Wang,Dawei Yin*

Main category: cs.CL

TL;DR: 本文提出了一种名为AI Search Paradigm的下一代搜索系统蓝图，通过四个LLM驱动的智能体协作，动态适应从简单查询到复杂推理任务的信息需求。


<details>
  <summary>Details</summary>
Motivation: 旨在开发能够模拟人类信息处理和决策的下一代搜索系统，提供可信赖、自适应且可扩展的AI搜索解决方案。

Method: 采用模块化架构，包括Master、Planner、Executor和Writer四个智能体，通过协调工作流评估查询复杂度、分解问题并执行任务。

Result: 系统能够处理从简单查询到复杂多阶段推理任务的全谱信息需求。

Conclusion: 本文为开发可信赖、自适应且可扩展的AI搜索系统提供了基础性指导。

Abstract: In this paper, we introduce the AI Search Paradigm, a comprehensive blueprint
for next-generation search systems capable of emulating human information
processing and decision-making. The paradigm employs a modular architecture of
four LLM-powered agents (Master, Planner, Executor and Writer) that dynamically
adapt to the full spectrum of information needs, from simple factual queries to
complex multi-stage reasoning tasks. These agents collaborate dynamically
through coordinated workflows to evaluate query complexity, decompose problems
into executable plans, and orchestrate tool usage, task execution, and content
synthesis. We systematically present key methodologies for realizing this
paradigm, including task planning and tool integration, execution strategies,
aligned and robust retrieval-augmented generation, and efficient LLM inference,
spanning both algorithmic techniques and infrastructure-level optimizations. By
providing an in-depth guide to these foundational components, this work aims to
inform the development of trustworthy, adaptive, and scalable AI search
systems.

</details>


### [119] [Fine-Tuning Lowers Safety and Disrupts Evaluation Consistency](https://arxiv.org/abs/2506.17209)
*Kathleen C. Fraser,Hillary Dawkins,Isar Nejadgholi,Svetlana Kiritchenko*

Main category: cs.CL

TL;DR: 研究发现微调通用大语言模型（LLM）会削弱其安全对齐功能，即使微调数据无害。安全评估结果受实验设置微小变化影响显著，需改进报告方式以支持未来研究比较。


<details>
  <summary>Details</summary>
Motivation: 微调LLM已成为常规操作，但会意外移除模型的安全对齐功能，即使数据无害。这一漏洞可能被恶意利用，亟需可靠的安全评估方法。

Method: 研究安全评估对实验设置微小变化的鲁棒性，分析LLM随机性对结果的影响。

Result: 实验显示安全评估结果对微调设置的微小变化极为敏感，结果差异显著。

Conclusion: 需改进安全评估的报告方式，确保结果可重复和可比，以应对LLM微调带来的安全隐患。

Abstract: Fine-tuning a general-purpose large language model (LLM) for a specific
domain or task has become a routine procedure for ordinary users. However,
fine-tuning is known to remove the safety alignment features of the model, even
when the fine-tuning data does not contain any harmful content. We consider
this to be a critical failure mode of LLMs due to the widespread uptake of
fine-tuning, combined with the benign nature of the "attack". Most
well-intentioned developers are likely unaware that they are deploying an LLM
with reduced safety. On the other hand, this known vulnerability can be easily
exploited by malicious actors intending to bypass safety guardrails. To make
any meaningful progress in mitigating this issue, we first need reliable and
reproducible safety evaluations. In this work, we investigate how robust a
safety benchmark is to trivial variations in the experimental procedure, and
the stochastic nature of LLMs. Our initial experiments expose surprising
variance in the results of the safety evaluation, even when seemingly
inconsequential changes are made to the fine-tuning setup. Our observations
have serious implications for how researchers in this field should report
results to enable meaningful comparisons in the future.

</details>


### [120] [LaMP-Cap: Personalized Figure Caption Generation With Multimodal Figure Profiles](https://arxiv.org/abs/2506.06561)
*Ho Yin 'Sam' Ng,Ting-Yao Hsu,Aashish Anantha Ramakrishnan,Branislav Kveton,Nedim Lipka,Franck Dernoncourt,Dongwon Lee,Tong Yu,Sungchul Kim,Ryan A. Rossi,Ting-Hao 'Kenneth' Huang*

Main category: cs.CL

TL;DR: LaMP-Cap是一个用于个性化图注生成的多模态数据集，通过结合图像和文本信息，帮助生成更接近作者风格的图注。


<details>
  <summary>Details</summary>
Motivation: 现有AI生成的图注缺乏个性化，难以匹配作者的写作风格和领域风格，需要多模态个性化支持。

Method: 提出LaMP-Cap数据集，包含目标图像及其相关图像、图注和段落，作为多模态上下文。实验使用四种LLM验证多模态信息的有效性。

Result: 实验表明，使用多模态上下文生成的图注更接近作者原版，其中图像信息比段落信息更有帮助。

Conclusion: 多模态个性化在生成图注中优于纯文本方法，LaMP-Cap为未来研究提供了新方向。

Abstract: Figure captions are crucial for helping readers understand and remember a
figure's key message. Many models have been developed to generate these
captions, helping authors compose better quality captions more easily. Yet,
authors almost always need to revise generic AI-generated captions to match
their writing style and the domain's style, highlighting the need for
personalization. Despite language models' personalization (LaMP) advances,
these technologies often focus on text-only settings and rarely address
scenarios where both inputs and profiles are multimodal. This paper introduces
LaMP-Cap, a dataset for personalized figure caption generation with multimodal
figure profiles. For each target figure, LaMP-Cap provides not only the needed
inputs, such as figure images, but also up to three other figures from the same
document--each with its image, caption, and figure-mentioning paragraphs--as a
profile to characterize the context. Experiments with four LLMs show that using
profile information consistently helps generate captions closer to the original
author-written ones. Ablation studies reveal that images in the profile are
more helpful than figure-mentioning paragraphs, highlighting the advantage of
using multimodal profiles over text-only ones.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [121] [A Strong View-Free Baseline Approach for Single-View Image Guided Point Cloud Completion](https://arxiv.org/abs/2506.15747)
*Fangzhou Lin,Zilin Dai,Rigved Sanku,Songlin Hou,Kazunori D Yamada,Haichong K. Zhang,Ziming Zhang*

Main category: cs.CV

TL;DR: 提出了一种基于注意力机制的多分支编码器-解码器网络，仅需部分点云输入即可完成点云补全任务，性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 探究单视角图像引导点云补全任务中图像引导的必要性，并提出一种无需图像输入的高效方法。

Method: 采用注意力驱动的多分支编码器-解码器网络，结合层次化自融合机制（交叉注意力和自注意力层）整合多流信息。

Result: 在ShapeNet-ViPC数据集上表现优于现有方法。

Conclusion: 该方法为多模态学习在点云补全中的应用提供了新视角。

Abstract: The single-view image guided point cloud completion (SVIPC) task aims to
reconstruct a complete point cloud from a partial input with the help of a
single-view image. While previous works have demonstrated the effectiveness of
this multimodal approach, the fundamental necessity of image guidance remains
largely unexamined. To explore this, we propose a strong baseline approach for
SVIPC based on an attention-based multi-branch encoder-decoder network that
only takes partial point clouds as input, view-free. Our hierarchical
self-fusion mechanism, driven by cross-attention and self-attention layers,
effectively integrates information across multiple streams, enriching feature
representations and strengthening the networks ability to capture geometric
structures. Extensive experiments and ablation studies on the ShapeNet-ViPC
dataset demonstrate that our view-free framework performs superiorly to
state-of-the-art SVIPC methods. We hope our findings provide new insights into
the development of multimodal learning in SVIPC. Our demo code will be
available at https://github.com/Zhang-VISLab.

</details>


### [122] [VLMInferSlow: Evaluating the Efficiency Robustness of Large Vision-Language Models as a Service](https://arxiv.org/abs/2506.15755)
*Xiasi Wang,Tianliang Yao,Simin Chen,Runqi Wang,Lei YE,Kuofeng Gao,Yi Huang,Yuan Yao*

Main category: cs.CV

TL;DR: 提出了一种名为VLMInferSlow的新方法，用于在现实黑盒设置中评估视觉语言模型（VLM）的效率鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注VLM的准确性，而效率问题未充分探索，且现有评估方法在现实场景中不实用。

Method: VLMInferSlow结合了细粒度效率建模和零阶优化，以搜索对抗样本。

Result: 实验显示，VLMInferSlow生成的对抗图像可增加计算成本高达128.47%。

Conclusion: 该研究旨在提高社区对VLM效率鲁棒性的关注。

Abstract: Vision-Language Models (VLMs) have demonstrated great potential in real-world
applications. While existing research primarily focuses on improving their
accuracy, the efficiency remains underexplored. Given the real-time demands of
many applications and the high inference overhead of VLMs, efficiency
robustness is a critical issue. However, previous studies evaluate efficiency
robustness under unrealistic assumptions, requiring access to the model
architecture and parameters -- an impractical scenario in ML-as-a-service
settings, where VLMs are deployed via inference APIs. To address this gap, we
propose VLMInferSlow, a novel approach for evaluating VLM efficiency robustness
in a realistic black-box setting. VLMInferSlow incorporates fine-grained
efficiency modeling tailored to VLM inference and leverages zero-order
optimization to search for adversarial examples. Experimental results show that
VLMInferSlow generates adversarial images with imperceptible perturbations,
increasing the computational cost by up to 128.47%. We hope this research
raises the community's awareness about the efficiency robustness of VLMs.

</details>


### [123] [Weakly-supervised VLM-guided Partial Contrastive Learning for Visual Language Navigation](https://arxiv.org/abs/2506.15757)
*Ruoyu Wang,Tong Yu,Junda Wu,Yao Liu,Julian McAuley,Lina Yao*

Main category: cs.CV

TL;DR: 提出了一种名为WPCL的弱监督部分对比学习方法，解决了VLN任务中动态视角和预训练模型未微调的问题，提升了导航性能且计算高效。


<details>
  <summary>Details</summary>
Motivation: 现有VLN方法依赖预训练模型，但动态视角和未微调限制了性能，且微调计算成本高。

Method: 提出WPCL方法，通过弱监督部分对比学习，将预训练VLM知识融入感知过程，无需微调。

Result: 实验表明，WPCL在多个基准测试中优于基线方法，验证了其有效性、鲁棒性和泛化性。

Conclusion: WPCL在提升VLN任务性能的同时，保持了计算效率，为动态视角和预训练模型问题提供了有效解决方案。

Abstract: Visual Language Navigation (VLN) is a fundamental task within the field of
Embodied AI, focusing on the ability of agents to navigate complex environments
based on natural language instructions. Despite the progress made by existing
methods, these methods often present some common challenges. First, they rely
on pre-trained backbone models for visual perception, which struggle with the
dynamic viewpoints in VLN scenarios. Second, the performance is limited when
using pre-trained LLMs or VLMs without fine-tuning, due to the absence of VLN
domain knowledge. Third, while fine-tuning LLMs and VLMs can improve results,
their computational costs are higher than those without fine-tuning. To address
these limitations, we propose Weakly-supervised Partial Contrastive Learning
(WPCL), a method that enhances an agent's ability to identify objects from
dynamic viewpoints in VLN scenarios by effectively integrating pre-trained VLM
knowledge into the perception process, without requiring VLM fine-tuning. Our
method enhances the agent's ability to interpret and respond to environmental
cues while ensuring computational efficiency. Experimental results have shown
that our method outperforms the baseline methods on multiple benchmarks, which
validate the effectiveness, robustness and generalizability of our method.

</details>


### [124] [Fine-grained Image Retrieval via Dual-Vision Adaptation](https://arxiv.org/abs/2506.16273)
*Xin Jiang,Meiqi Cao,Hao Tang,Fei Shen,Zechao Li*

Main category: cs.CV

TL;DR: 提出了一种双视觉适应（DVA）方法，通过样本和特征协同适应解决细粒度图像检索中的过拟合问题，同时保持预训练知识。


<details>
  <summary>Details</summary>
Motivation: 现有细粒度图像检索方法容易过拟合训练数据，忽视预训练知识，导致泛化能力下降。

Method: 设计了对象感知适应和上下文适应，前者调整输入样本，后者引入少量参数进行特征适应，同时通过知识蒸馏传递判别知识。

Result: DVA在三个分布内和三个分布外细粒度数据集上表现优异，且参数较少。

Conclusion: DVA通过协同适应和知识蒸馏，有效提升了细粒度图像检索的泛化能力和效率。

Abstract: Fine-Grained Image Retrieval~(FGIR) faces challenges in learning
discriminative visual representations to retrieve images with similar
fine-grained features. Current leading FGIR solutions typically follow two
regimes: enforce pairwise similarity constraints in the semantic embedding
space, or incorporate a localization sub-network to fine-tune the entire model.
However, such two regimes tend to overfit the training data while forgetting
the knowledge gained from large-scale pre-training, thus reducing their
generalization ability. In this paper, we propose a Dual-Vision Adaptation
(DVA) approach for FGIR, which guides the frozen pre-trained model to perform
FGIR through collaborative sample and feature adaptation. Specifically, we
design Object-Perceptual Adaptation, which modifies input samples to help the
pre-trained model perceive critical objects and elements within objects that
are helpful for category prediction. Meanwhile, we propose In-Context
Adaptation, which introduces a small set of parameters for feature adaptation
without modifying the pre-trained parameters. This makes the FGIR task using
these adjusted features closer to the task solved during the pre-training.
Additionally, to balance retrieval efficiency and performance, we propose
Discrimination Perception Transfer to transfer the discriminative knowledge in
the object-perceptual adaptation to the image encoder using the knowledge
distillation mechanism. Extensive experiments show that DVA has fewer learnable
parameters and performs well on three in-distribution and three
out-of-distribution fine-grained datasets.

</details>


### [125] [Implicit 3D scene reconstruction using deep learning towards efficient collision understanding in autonomous driving](https://arxiv.org/abs/2506.15806)
*Akarshani Ramanayake,Nihal Kodikara*

Main category: cs.CV

TL;DR: 论文提出了一种基于学习的方法，利用LiDAR数据和深度神经网络构建静态符号距离函数（SDF）地图，以解决密集交通环境中3D场景重建的边界精度问题。


<details>
  <summary>Details</summary>
Motivation: 在密集的城市交通环境中，现有技术难以精确导航，而3D场景重建的高边界精度问题尚未完全解决。符号距离函数（SDF）因其高效的存储特性成为潜在解决方案。

Method: 采用LiDAR数据和深度神经网络，开发了一种学习型的3D场景重建方法，构建静态SDF地图，以替代传统的多边形表示。

Result: 初步结果表明，该方法能显著提升碰撞检测性能，尤其在拥挤和动态环境中。

Conclusion: 该研究填补了现有文献的空白，展示了SDF在3D障碍物形状重建中的潜力，特别是在边界细节方面。

Abstract: In crowded urban environments where traffic is dense, current technologies
struggle to oversee tight navigation, but surface-level understanding allows
autonomous vehicles to safely assess proximity to surrounding obstacles. 3D or
2D scene mapping of the surrounding objects is an essential task in addressing
the above problem. Despite its importance in dense vehicle traffic conditions,
3D scene reconstruction of object shapes with higher boundary level accuracy is
not yet entirely considered in current literature. The sign distance function
represents any shape through parameters that calculate the distance from any
point in space to the closest obstacle surface, making it more efficient in
terms of storage. In recent studies, researchers have started to formulate
problems with Implicit 3D reconstruction methods in the autonomous driving
domain, highlighting the possibility of using sign distance function to map
obstacles effectively. This research addresses this gap by developing a
learning-based 3D scene reconstruction methodology that leverages LiDAR data
and a deep neural network to build a the static Signed Distance Function (SDF)
maps. Unlike traditional polygonal representations, this approach has the
potential to map 3D obstacle shapes with more boundary-level details. Our
preliminary results demonstrate that this method would significantly enhance
collision detection performance, particularly in congested and dynamic
environments.

</details>


### [126] [ADAM-Dehaze: Adaptive Density-Aware Multi-Stage Dehazing for Improved Object Detection in Foggy Conditions](https://arxiv.org/abs/2506.15837)
*Fatmah AlHindaassi,Mohammed Talha Alam,Fakhri Karray*

Main category: cs.CV

TL;DR: ADAM-Dehaze是一种自适应、密度感知的去雾框架，通过动态路由和自适应损失优化图像恢复和目标检测，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 雾等恶劣天气条件严重影响了视觉信息，对自动驾驶和监控系统等安全关键应用构成挑战。

Method: 使用轻量级雾密度估计网络（HDEN）分类雾强度，动态路由图像至三个CORUN分支之一，并结合自适应损失平衡物理模型一致性和感知保真度。

Result: 在Cityscapes和RTTS基准测试中，PSNR提升2.1 dB，FADE降低30%，目标检测mAP提高13点，推理时间减少20%。

Conclusion: ADAM-Dehaze证明了雾强度特定处理和与下游视觉任务无缝集成的重要性。

Abstract: Adverse weather conditions, particularly fog, pose a significant challenge to
autonomous vehicles, surveillance systems, and other safety-critical
applications by severely degrading visual information. We introduce
ADAM-Dehaze, an adaptive, density-aware dehazing framework that jointly
optimizes image restoration and object detection under varying fog intensities.
A lightweight Haze Density Estimation Network (HDEN) classifies each input as
light, medium, or heavy fog. Based on this score, the system dynamically routes
the image through one of three CORUN branches: Light, Medium, or Complex, each
tailored to its haze regime. A novel adaptive loss balances physical-model
coherence and perceptual fidelity, ensuring both accurate defogging and
preservation of fine details. On Cityscapes and the real-world RTTS benchmark,
ADAM-Dehaze improves PSNR by up to 2.1 dB, reduces FADE by 30 percent, and
increases object detection mAP by up to 13 points, while cutting inference time
by 20 percent. These results highlight the importance of intensity-specific
processing and seamless integration with downstream vision tasks. Code
available at: https://github.com/talha-alam/ADAM-Dehaze.

</details>


### [127] [Class Agnostic Instance-level Descriptor for Visual Instance Search](https://arxiv.org/abs/2506.16745)
*Qi-Ying Sun,Wan-Lei Zhao,Yi-Bo Miao,Chong-Wah Ngo*

Main category: cs.CV

TL;DR: 论文提出了一种基于自监督ViT的分层特征子集检测方法，用于解决视觉实例搜索中实例级特征表示不足的问题，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有深度特征在视觉实例搜索中表现不佳，尤其是对未知物体类别的检测效果差，需要一种更有效的实例级特征表示方法。

Method: 利用自监督ViT输出的特征集，通过分层方式检测紧凑特征子集，形成层次结构，非叶节点和叶节点对应不同语义尺度的实例区域。

Result: 在三个实例搜索基准测试中，该方法显著优于现有技术，能够有效处理已知和未知物体类别。

Conclusion: 分层分解方法解决了物体嵌入和遮挡问题，提供了一种全面的实例级描述符，适用于实际场景。

Abstract: Despite the great success of the deep features in content-based image
retrieval, the visual instance search remains challenging due to the lack of
effective instance level feature representation. Supervised or weakly
supervised object detection methods are not among the options due to their poor
performance on the unknown object categories. In this paper, based on the
feature set output from self-supervised ViT, the instance level region
discovery is modeled as detecting the compact feature subsets in a hierarchical
fashion. The hierarchical decomposition results in a hierarchy of feature
subsets. The non-leaf nodes and leaf nodes on the hierarchy correspond to the
various instance regions in an image of different semantic scales. The
hierarchical decomposition well addresses the problem of object embedding and
occlusions, which are widely observed in the real scenarios. The features
derived from the nodes on the hierarchy make up a comprehensive representation
for the latent instances in the image. Our instance-level descriptor remains
effective on both the known and unknown object categories. Empirical studies on
three instance search benchmarks show that it outperforms state-of-the-art
methods considerably.

</details>


### [128] [EchoShot: Multi-Shot Portrait Video Generation](https://arxiv.org/abs/2506.15838)
*Jiahao Wang,Hualian Sheng,Sijia Cai,Weizhan Zhang,Caixia Yan,Yachuang Feng,Bing Deng,Jieping Ye*

Main category: cs.CV

TL;DR: EchoShot是一个基于视频扩散模型的多镜头肖像定制框架，通过创新的位置嵌入机制和高质量数据集PortraitGala，实现了身份一致性和内容可控性。


<details>
  <summary>Details</summary>
Motivation: 现实应用需要多镜头生成且保持身份一致性和内容可控性，而现有方法局限于单镜头生成。

Method: 提出shot-aware位置嵌入机制，构建PortraitGala数据集，支持基于参考图像的个性化生成和长视频合成。

Result: EchoShot在多镜头肖像视频生成中表现出卓越的身份一致性和属性级可控性。

Conclusion: EchoShot为通用多镜头视频建模提供了潜在的基础范式。

Abstract: Video diffusion models substantially boost the productivity of artistic
workflows with high-quality portrait video generative capacity. However,
prevailing pipelines are primarily constrained to single-shot creation, while
real-world applications urge for multiple shots with identity consistency and
flexible content controllability. In this work, we propose EchoShot, a native
and scalable multi-shot framework for portrait customization built upon a
foundation video diffusion model. To start with, we propose shot-aware position
embedding mechanisms within video diffusion transformer architecture to model
inter-shot variations and establish intricate correspondence between multi-shot
visual content and their textual descriptions. This simple yet effective design
enables direct training on multi-shot video data without introducing additional
computational overhead. To facilitate model training within multi-shot
scenario, we construct PortraitGala, a large-scale and high-fidelity
human-centric video dataset featuring cross-shot identity consistency and
fine-grained captions such as facial attributes, outfits, and dynamic motions.
To further enhance applicability, we extend EchoShot to perform reference
image-based personalized multi-shot generation and long video synthesis with
infinite shot counts. Extensive evaluations demonstrate that EchoShot achieves
superior identity consistency as well as attribute-level controllability in
multi-shot portrait video generation. Notably, the proposed framework
demonstrates potential as a foundational paradigm for general multi-shot video
modeling.

</details>


### [129] [TextBraTS: Text-Guided Volumetric Brain Tumor Segmentation with Innovative Dataset Development and Fusion Module Exploration](https://arxiv.org/abs/2506.16784)
*Xiaoyu Shi,Rahul Kumar Jain,Yinhao Li,Ruibo Hou,Jingliang Cheng,Jie Bai,Guohua Zhao,Lanfen Lin,Rui Xu,Yen-wei Chen*

Main category: cs.CV

TL;DR: 论文介绍了首个公开的多模态数据集TextBraTS，结合MRI图像与文本注释，并提出了一种基于文本引导的医学图像分割方法，显著提高了脑肿瘤分割的准确性。


<details>
  <summary>Details</summary>
Motivation: 现有脑肿瘤分析领域缺乏结合影像与文本注释的综合数据集，限制了多模态方法的研究。

Method: 提出TextBraTS数据集及一种基于序列交叉注意力的文本引导体积医学图像分割框架。

Result: 实验表明，该方法显著提高了脑肿瘤分割的准确性。

Conclusion: TextBraTS数据集和提出的方法为多模态医学图像分割提供了有价值的资源和技术。

Abstract: Deep learning has demonstrated remarkable success in medical image
segmentation and computer-aided diagnosis. In particular, numerous advanced
methods have achieved state-of-the-art performance in brain tumor segmentation
from MRI scans. While recent studies in other medical imaging domains have
revealed that integrating textual reports with visual data can enhance
segmentation accuracy, the field of brain tumor analysis lacks a comprehensive
dataset that combines radiological images with corresponding textual
annotations. This limitation has hindered the exploration of multimodal
approaches that leverage both imaging and textual data.
  To bridge this critical gap, we introduce the TextBraTS dataset, the first
publicly available volume-level multimodal dataset that contains paired MRI
volumes and rich textual annotations, derived from the widely adopted BraTS2020
benchmark. Building upon this novel dataset, we propose a novel baseline
framework and sequential cross-attention method for text-guided volumetric
medical image segmentation. Through extensive experiments with various
text-image fusion strategies and templated text formulations, our approach
demonstrates significant improvements in brain tumor segmentation accuracy,
offering valuable insights into effective multimodal integration techniques.
  Our dataset, implementation code, and pre-trained models are publicly
available at https://github.com/Jupitern52/TextBraTS.

</details>


### [130] [Assessing the impact of Binarization for Writer Identification in Greek Papyrus](https://arxiv.org/abs/2506.15852)
*Dominic Akt,Marco Peer,Florian Kleber*

Main category: cs.CV

TL;DR: 本文研究了希腊纸草文献的作者识别任务，重点比较了传统二值化方法与深度学习模型的效果，并评估了数据增强和二值化质量对作者识别性能的影响。


<details>
  <summary>Details</summary>
Motivation: 希腊纸草文献的背景通常不均匀、碎片化且变色，传统二值化方法难以处理，因此需要探索深度学习模型的效果。

Method: 比较了传统二值化方法与深度学习模型，并应用了自定义数据增强技术和不同模型选择标准，随后在DIBCO 2019数据集上评估性能。

Result: 数据增强对深度学习方法有显著影响，二值化效果与下游作者识别性能密切相关。

Conclusion: 深度学习结合数据增强在希腊纸草文献的二值化和作者识别任务中表现优越。

Abstract: This paper tackles the task of writer identification for Greek papyri. A
common preprocessing step in writer identification pipelines is image
binarization, which prevents the model from learning background features. This
is challenging in historical documents, in our case Greek papyri, as background
is often non-uniform, fragmented, and discolored with visible fiber structures.
We compare traditional binarization methods to state-of-the-art Deep Learning
(DL) models, evaluating the impact of binarization quality on subsequent writer
identification performance. DL models are trained with and without a custom
data augmentation technique, as well as different model selection criteria are
applied. The performance of these binarization methods, is then systematically
evaluated on the DIBCO 2019 dataset. The impact of binarization on writer
identification is subsequently evaluated using a state-of-the-art approach for
writer identification. The results of this analysis highlight the influence of
data augmentation for DL methods. Furthermore, findings indicate a strong
correlation between binarization effectiveness on papyri documents of DIBCO
2019 and downstream writer identification performance.

</details>


### [131] [Privacy-Preserving in Connected and Autonomous Vehicles Through Vision to Text Transformation](https://arxiv.org/abs/2506.15854)
*Abdolazim Rezaei,Mehdi Sookhak,Ahmad Patooghy*

Main category: cs.CV

TL;DR: 论文提出了一种基于反馈强化学习和视觉语言模型的新框架，将图像转换为语义等效的文本描述，以保护AI摄像头捕获的隐私敏感信息。


<details>
  <summary>Details</summary>
Motivation: 路边AI摄像头捕获的图像可能被滥用，传统隐私保护技术（如模糊处理）无法完全防止个体追踪。

Method: 采用分层强化学习策略，结合视觉语言模型，将图像转换为语义等效的文本描述。

Result: 相比现有方法，隐私保护和文本质量显著提升，独特词数增加约77%，细节密度提高约50%。

Conclusion: 该框架在保留场景信息的同时有效保护隐私，为CAV系统中的隐私问题提供了创新解决方案。

Abstract: Connected and Autonomous Vehicles (CAVs) rely on a range of devices that
often process privacy-sensitive data. Among these, roadside units play a
critical role particularly through the use of AI-equipped (AIE) cameras for
applications such as violation detection. However, the privacy risks associated
with captured imagery remain a major concern, as such data can be misused for
identity theft, profiling, or unauthorized commercial purposes. While
traditional techniques such as face blurring and obfuscation have been applied
to mitigate privacy risks, individual privacy remains at risk, as individuals
can still be tracked using other features such as their clothing. This paper
introduces a novel privacy-preserving framework that leverages feedback-based
reinforcement learning (RL) and vision-language models (VLMs) to protect
sensitive visual information captured by AIE cameras. The main idea is to
convert images into semantically equivalent textual descriptions, ensuring that
scene-relevant information is retained while visual privacy is preserved. A
hierarchical RL strategy is employed to iteratively refine the generated text,
enhancing both semantic accuracy and privacy. Evaluation results demonstrate
significant improvements in both privacy protection and textual quality, with
the Unique Word Count increasing by approximately 77\% and Detail Density by
around 50\% compared to existing approaches.

</details>


### [132] [Visual symbolic mechanisms: Emergent symbol processing in vision language models](https://arxiv.org/abs/2506.15871)
*Rim Assouel,Declan Campbell,Taylor Webb*

Main category: cs.CV

TL;DR: 研究发现视觉语言模型（VLMs）通过内容无关的空间索引机制解决特征绑定问题，并揭示了绑定错误的根源。


<details>
  <summary>Details</summary>
Motivation: 探讨视觉语言模型（VLMs）是否采用类似语言模型的符号化机制解决特征绑定问题，以解释其在此类任务中的持续失败。

Method: 识别VLMs中支持特征绑定的内容无关空间索引机制，并分析绑定错误与这些机制的关系。

Result: 发现VLMs通过符号化机制实现特征绑定，绑定错误源于这些机制的失效。

Conclusion: 研究揭示了VLMs中符号化处理的机制，为解决其绑定问题提供了潜在方向。

Abstract: To accurately process a visual scene, observers must bind features together
to represent individual objects. This capacity is necessary, for instance, to
distinguish an image containing a red square and a blue circle from an image
containing a blue square and a red circle. Recent work has found that language
models solve this 'binding problem' via a set of symbol-like,
content-independent indices, but it is unclear whether similar mechanisms are
employed by vision language models (VLMs). This question is especially
relevant, given the persistent failures of VLMs on tasks that require binding.
Here, we identify a set of emergent symbolic mechanisms that support binding in
VLMs via a content-independent, spatial indexing scheme. Moreover, we find that
binding errors can be traced directly to failures in these mechanisms. Taken
together, these results shed light on the mechanisms that support symbol-like
processing in VLMs, and suggest possible avenues for addressing the persistent
binding failures exhibited by these models.

</details>


### [133] [Pediatric Pancreas Segmentation from MRI Scans with Deep Learning](https://arxiv.org/abs/2506.15908)
*Elif Keles,Merve Yazol,Gorkem Durak,Ziliang Hong,Halil Ertugrul Aktas,Zheyuan Zhang,Linkai Peng,Onkar Susladkar,Necati Guzelyel,Oznur Leman Boyunaga,Cemal Yazici,Mark Lowe,Aliye Uc,Ulas Bagci*

Main category: cs.CV

TL;DR: PanSegNet是一种深度学习算法，用于儿童胰腺MRI分割，在健康儿童和急慢性胰腺炎患者中表现优异。


<details>
  <summary>Details</summary>
Motivation: 评估和验证PanSegNet在儿童胰腺MRI分割中的性能，填补这一领域的空白。

Method: 回顾性收集84例MRI扫描，由放射科医生手动分割胰腺，并通过DSC和HD95指标评估PanSegNet的性能。

Result: PanSegNet在健康儿童中DSC为88%，急慢性胰腺炎患者中为81%和80%，表现出高可靠性和临床实用性。

Conclusion: PanSegNet是首个经过验证的胰腺MRI分割深度学习工具，性能达到专家水平，并公开数据集和算法促进研究。

Abstract: Objective: Our study aimed to evaluate and validate PanSegNet, a deep
learning (DL) algorithm for pediatric pancreas segmentation on MRI in children
with acute pancreatitis (AP), chronic pancreatitis (CP), and healthy controls.
Methods: With IRB approval, we retrospectively collected 84 MRI scans (1.5T/3T
Siemens Aera/Verio) from children aged 2-19 years at Gazi University
(2015-2024). The dataset includes healthy children as well as patients
diagnosed with AP or CP based on clinical criteria. Pediatric and general
radiologists manually segmented the pancreas, then confirmed by a senior
pediatric radiologist. PanSegNet-generated segmentations were assessed using
Dice Similarity Coefficient (DSC) and 95th percentile Hausdorff distance
(HD95). Cohen's kappa measured observer agreement. Results: Pancreas MRI T2W
scans were obtained from 42 children with AP/CP (mean age: 11.73 +/- 3.9 years)
and 42 healthy children (mean age: 11.19 +/- 4.88 years). PanSegNet achieved
DSC scores of 88% (controls), 81% (AP), and 80% (CP), with HD95 values of 3.98
mm (controls), 9.85 mm (AP), and 15.67 mm (CP). Inter-observer kappa was 0.86
(controls), 0.82 (pancreatitis), and intra-observer agreement reached 0.88 and
0.81. Strong agreement was observed between automated and manual volumes (R^2 =
0.85 in controls, 0.77 in diseased), demonstrating clinical reliability.
Conclusion: PanSegNet represents the first validated deep learning solution for
pancreatic MRI segmentation, achieving expert-level performance across healthy
and diseased states. This tool, algorithm, along with our annotated dataset,
are freely available on GitHub and OSF, advancing accessible, radiation-free
pediatric pancreatic imaging and fostering collaborative research in this
underserved domain.

</details>


### [134] [MoiréXNet: Adaptive Multi-Scale Demoiréing with Linear Attention Test-Time Training and Truncated Flow Matching Prior](https://arxiv.org/abs/2506.15929)
*Liangyan Li,Yimo Ning,Kevin Le,Wei Dong,Yunzhe Li,Jun Chen,Xiaohong Liu*

Main category: cs.CV

TL;DR: 提出了一种结合MAP估计与深度学习的图像和视频去摩尔纹框架，解决了非线性退化问题。


<details>
  <summary>Details</summary>
Motivation: 传统方法无法完全去除摩尔纹或导致结果过于平滑，生成模型在非线性退化中表现不佳。

Method: 结合监督学习模型（带线性注意力TTT模块）和TFMP先验，优化非线性映射和细节恢复。

Result: 框架结合了线性注意力的高效性和生成模型的细化能力，提升了恢复性能。

Conclusion: 混合MAP框架有效解决了去摩尔纹问题，恢复了高频细节并抑制了伪影。

Abstract: This paper introduces a novel framework for image and video demoir\'eing by
integrating Maximum A Posteriori (MAP) estimation with advanced deep learning
techniques. Demoir\'eing addresses inherently nonlinear degradation processes,
which pose significant challenges for existing methods.
  Traditional supervised learning approaches either fail to remove moir\'e
patterns completely or produce overly smooth results. This stems from
constrained model capacity and scarce training data, which inadequately
represent the clean image distribution and hinder accurate reconstruction of
ground-truth images. While generative models excel in image restoration for
linear degradations, they struggle with nonlinear cases such as demoir\'eing
and often introduce artifacts.
  To address these limitations, we propose a hybrid MAP-based framework that
integrates two complementary components. The first is a supervised learning
model enhanced with efficient linear attention Test-Time Training (TTT)
modules, which directly learn nonlinear mappings for RAW-to-sRGB demoir\'eing.
The second is a Truncated Flow Matching Prior (TFMP) that further refines the
outputs by aligning them with the clean image distribution, effectively
restoring high-frequency details and suppressing artifacts. These two
components combine the computational efficiency of linear attention with the
refinement abilities of generative models, resulting in improved restoration
performance.

</details>


### [135] [Beyond Audio and Pose: A General-Purpose Framework for Video Synchronization](https://arxiv.org/abs/2506.15937)
*Yosub Shin,Igor Molybog*

Main category: cs.CV

TL;DR: VideoSync是一个独立于特定特征提取方法的视频同步框架，适用于多种内容类型，并通过新数据集和严格评估框架证明了其优越性。


<details>
  <summary>Details</summary>
Motivation: 现有视频同步方法依赖音频或特定视觉事件，适用性受限；现有基准缺乏通用性和可重复性。

Method: 提出VideoSync框架，不依赖特定特征提取方法，使用新数据集评估，并纠正现有方法的偏差。

Result: VideoSync在公平条件下优于现有方法（如SeSyn-Net），CNN模型在同步偏移预测中表现最佳。

Conclusion: VideoSync突破了领域限制，更具通用性和鲁棒性，适用于实际应用。

Abstract: Video synchronization-aligning multiple video streams capturing the same
event from different angles-is crucial for applications such as reality TV show
production, sports analysis, surveillance, and autonomous systems. Prior work
has heavily relied on audio cues or specific visual events, limiting
applicability in diverse settings where such signals may be unreliable or
absent. Additionally, existing benchmarks for video synchronization lack
generality and reproducibility, restricting progress in the field. In this
work, we introduce VideoSync, a video synchronization framework that operates
independently of specific feature extraction methods, such as human pose
estimation, enabling broader applicability across different content types. We
evaluate our system on newly composed datasets covering single-human,
multi-human, and non-human scenarios, providing both the methodology and code
for dataset creation to establish reproducible benchmarks. Our analysis reveals
biases in prior SOTA work, particularly in SeSyn-Net's preprocessing pipeline,
leading to inflated performance claims. We correct these biases and propose a
more rigorous evaluation framework, demonstrating that VideoSync outperforms
existing approaches, including SeSyn-Net, under fair experimental conditions.
Additionally, we explore various synchronization offset prediction methods,
identifying a convolutional neural network (CNN)-based model as the most
effective. Our findings advance video synchronization beyond domain-specific
constraints, making it more generalizable and robust for real-world
applications.

</details>


### [136] [Polyline Path Masked Attention for Vision Transformer](https://arxiv.org/abs/2506.15940)
*Zhongchen Zhao,Chaodong Xiao,Hui Lin,Qi Xie,Lei Zhang,Deyu Meng*

Main category: cs.CV

TL;DR: 论文提出了一种名为PPMA的新方法，结合了ViTs的自注意力机制和Mamba2的结构化掩码，通过2D折线路径扫描策略优化掩码，提升了图像任务的性能。


<details>
  <summary>Details</summary>
Motivation: 解决深度学习框架中全局依赖建模和空间位置建模的核心问题，结合ViTs和Mamba2的优势。

Method: 改进Mamba2的结构化掩码为2D折线路径掩码，并将其嵌入ViTs的自注意力机制中。

Result: 在图像分类、目标检测和分割任务中表现优异，例如在ADE20K上PPMA-T/S/B模型分别提升0.7%/1.3%/0.3%的mIoU。

Conclusion: PPMA成功整合了两种架构的优势，显著提升了性能，代码已开源。

Abstract: Global dependency modeling and spatial position modeling are two core issues
of the foundational architecture design in current deep learning frameworks.
Recently, Vision Transformers (ViTs) have achieved remarkable success in
computer vision, leveraging the powerful global dependency modeling capability
of the self-attention mechanism. Furthermore, Mamba2 has demonstrated its
significant potential in natural language processing tasks by explicitly
modeling the spatial adjacency prior through the structured mask. In this
paper, we propose Polyline Path Masked Attention (PPMA) that integrates the
self-attention mechanism of ViTs with an enhanced structured mask of Mamba2,
harnessing the complementary strengths of both architectures. Specifically, we
first ameliorate the traditional structured mask of Mamba2 by introducing a 2D
polyline path scanning strategy and derive its corresponding structured mask,
polyline path mask, which better preserves the adjacency relationships among
image tokens. Notably, we conduct a thorough theoretical analysis on the
structural characteristics of the proposed polyline path mask and design an
efficient algorithm for the computation of the polyline path mask. Next, we
embed the polyline path mask into the self-attention mechanism of ViTs,
enabling explicit modeling of spatial adjacency prior. Extensive experiments on
standard benchmarks, including image classification, object detection, and
segmentation, demonstrate that our model outperforms previous state-of-the-art
approaches based on both state-space models and Transformers. For example, our
proposed PPMA-T/S/B models achieve 48.7%/51.1%/52.3% mIoU on the ADE20K
semantic segmentation task, surpassing RMT-T/S/B by 0.7%/1.3%/0.3%,
respectively. Code is available at https://github.com/zhongchenzhao/PPMA.

</details>


### [137] [Heterogeneous-Modal Unsupervised Domain Adaptation via Latent Space Bridging](https://arxiv.org/abs/2506.15971)
*Jiawen Yang,Shuhao Chen,Yucong Duan,Ke Tang,Yu Zhang*

Main category: cs.CV

TL;DR: 提出了一种新的异构模态无监督域适应（HMUDA）设置，通过桥接域实现不同模态间的知识迁移，并设计了LSB框架用于语义分割任务。


<details>
  <summary>Details</summary>
Motivation: 解决源域和目标域属于完全不同模态时无监督域适应方法的局限性。

Method: 提出LSB框架，采用双分支架构，结合特征一致性损失和域对齐损失。

Result: 在六个基准数据集上实现了最先进的性能。

Conclusion: LSB框架有效解决了异构模态间的域适应问题。

Abstract: Unsupervised domain adaptation (UDA) methods effectively bridge domain gaps
but become struggled when the source and target domains belong to entirely
distinct modalities. To address this limitation, we propose a novel setting
called Heterogeneous-Modal Unsupervised Domain Adaptation (HMUDA), which
enables knowledge transfer between completely different modalities by
leveraging a bridge domain containing unlabeled samples from both modalities.
To learn under the HMUDA setting, we propose Latent Space Bridging (LSB), a
specialized framework designed for the semantic segmentation task.
Specifically, LSB utilizes a dual-branch architecture, incorporating a feature
consistency loss to align representations across modalities and a domain
alignment loss to reduce discrepancies between class centroids across domains.
Extensive experiments conducted on six benchmark datasets demonstrate that LSB
achieves state-of-the-art performance.

</details>


### [138] [LBMamba: Locally Bi-directional Mamba](https://arxiv.org/abs/2506.15976)
*Jingwei Zhang,Xi Han,Hong Qin,Mahdi S. Hosseini,Dimitris Samaras*

Main category: cs.CV

TL;DR: LBMamba通过嵌入轻量级局部反向扫描到前向选择性扫描中，避免了传统双向扫描的计算负担，构建了LBVim视觉骨干网络，提升了性能与效率的平衡。


<details>
  <summary>Details</summary>
Motivation: 解决Mamba模型因单向性导致的信息缺失问题，同时避免传统双向扫描带来的计算效率损失。

Method: 提出LBMamba块，嵌入局部反向扫描于前向扫描中，构建LBVim网络，交替扫描方向以恢复全局感受野。

Result: 在多个数据集上，LBVim在相同计算量下性能显著提升，病理图像分类任务中AUC、F1和准确率均有提高。

Conclusion: LBMamba和LBVim有效平衡了性能与效率，为计算机视觉任务提供了高效解决方案。

Abstract: Mamba, a State Space Model (SSM) that accelerates training by recasting
recurrence as a parallel selective scan, has recently emerged as a
linearly-scaling, efficient alternative to self-attention. Because of its
unidirectional nature, each state in Mamba only has information of its previous
states and is blind to states after. Current Mamba-based computer-vision
methods typically overcome this limitation by augmenting Mamba's global forward
scan with a global backward scan, forming a bi-directional scan that restores a
full receptive field. However, this operation doubles the computational load,
eroding much of the efficiency advantage that originally Mamba have. To
eliminate this extra scans, we introduce LBMamba, a locally bi-directional SSM
block that embeds a lightweight locally backward scan inside the forward
selective scan and executes it entirely in per-thread registers. Building on
LBMamba, we present LBVim, a scalable vision backbone that alternates scan
directions every two layers to recover a global receptive field without extra
backward sweeps. We validate the versatility of our approach on both natural
images and whole slide images (WSIs). We show that our LBVim constantly offers
a superior performance-throughput trade-off. That is under the same throughput,
LBVim achieves 0.8% to 1.6% higher top-1 accuracy on the ImageNet-1K
classification dataset, 0.6% to 2.7% higher mIoU on the ADE20K semantic
segmentation dataset, 0.9% higher APb and 1.1% higher APm on the COCO detection
dataset. We also integrate LBMamba into the SOTA pathology multiple instance
learning (MIL) approach, MambaMIL, which uses single directional scan.
Experiments on 3 public WSI classification datasets for show that our method
achieves a relative improvement of up to 3.06% better AUC, 3.39% better F1,
1.67% better accuracy.

</details>


### [139] [Towards Classifying Histopathological Microscope Images as Time Series Data](https://arxiv.org/abs/2506.15977)
*Sungrae Hong,Hyeongmin Park,Youngsin Ko,Sol Lee,Bryan Wong,Mun Yong Yi*

Main category: cs.CV

TL;DR: 提出一种新方法，将显微镜图像作为时间序列数据分类，解决手动采集和弱标签问题，利用动态时间规整和注意力池化提升性能。


<details>
  <summary>Details</summary>
Motivation: 显微镜病理图像对癌症诊断至关重要，但深度学习领域对其关注不足，需解决其手动采集和弱标签的挑战。

Method: 将图像序列拟合为固定长度目标，采用动态时间规整（DTW）和注意力池化方法进行分类。

Result: 方法在性能上优于多种基线，并通过消融实验验证各组件贡献，达到稳定可靠的结果。

Conclusion: 该方法不仅将显微镜图像纳入分析，还将其性能提升至可信水平，为医学图像分析做出贡献。

Abstract: As the frontline data for cancer diagnosis, microscopic pathology images are
fundamental for providing patients with rapid and accurate treatment. However,
despite their practical value, the deep learning community has largely
overlooked their usage. This paper proposes a novel approach to classifying
microscopy images as time series data, addressing the unique challenges posed
by their manual acquisition and weakly labeled nature. The proposed method fits
image sequences of varying lengths to a fixed-length target by leveraging
Dynamic Time-series Warping (DTW). Attention-based pooling is employed to
predict the class of the case simultaneously. We demonstrate the effectiveness
of our approach by comparing performance with various baselines and showcasing
the benefits of using various inference strategies in achieving stable and
reliable results. Ablation studies further validate the contribution of each
component. Our approach contributes to medical image analysis by not only
embracing microscopic images but also lifting them to a trustworthy level of
performance.

</details>


### [140] [Advanced Sign Language Video Generation with Compressed and Quantized Multi-Condition Tokenization](https://arxiv.org/abs/2506.15980)
*Cong Wang,Zexuan Deng,Zhiwei Jiang,Fei Shen,Yafeng Yin,Shiwei Gan,Zifeng Cheng,Shiping Ge,Qing Gu*

Main category: cs.CV

TL;DR: SignViP是一种新的手语视频生成框架，通过多细粒度条件提升生成质量，采用离散标记化和扩散模型实现高效生成。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖单一粗粒度条件（如骨架序列）作为中间媒介，限制了生成视频的自然性和表现力。

Method: SignViP包含三个核心组件：1）联合训练的视频扩散模型和多条件编码器；2）FSQ自动编码器压缩嵌入为离散标记；3）多条件标记翻译器将文本转换为离散标记。

Result: 实验表明SignViP在视频质量、时间一致性和语义保真度方面达到最优性能。

Conclusion: SignViP通过多细粒度条件和离散标记化显著提升了手语视频生成的质量和表现力。

Abstract: Sign Language Video Generation (SLVG) seeks to generate identity-preserving
sign language videos from spoken language texts. Existing methods primarily
rely on the single coarse condition (\eg, skeleton sequences) as the
intermediary to bridge the translation model and the video generation model,
which limits both the naturalness and expressiveness of the generated videos.
To overcome these limitations, we propose SignViP, a novel SLVG framework that
incorporates multiple fine-grained conditions for improved generation fidelity.
Rather than directly translating error-prone high-dimensional conditions,
SignViP adopts a discrete tokenization paradigm to integrate and represent
fine-grained conditions (\ie, fine-grained poses and 3D hands). SignViP
contains three core components. (1) Sign Video Diffusion Model is jointly
trained with a multi-condition encoder to learn continuous embeddings that
encapsulate fine-grained motion and appearance. (2) Finite Scalar Quantization
(FSQ) Autoencoder is further trained to compress and quantize these embeddings
into discrete tokens for compact representation of the conditions. (3)
Multi-Condition Token Translator is trained to translate spoken language text
to discrete multi-condition tokens. During inference, Multi-Condition Token
Translator first translates the spoken language text into discrete
multi-condition tokens. These tokens are then decoded to continuous embeddings
by FSQ Autoencoder, which are subsequently injected into Sign Video Diffusion
Model to guide video generation. Experimental results show that SignViP
achieves state-of-the-art performance across metrics, including video quality,
temporal coherence, and semantic fidelity. The code is available at
https://github.com/umnooob/signvip/.

</details>


### [141] [Adversarial Attacks and Detection in Visual Place Recognition for Safer Robot Navigation](https://arxiv.org/abs/2506.15988)
*Connor Malone,Owen Claxton,Iman Shames,Michael Milford*

Main category: cs.CV

TL;DR: 论文分析了视觉地点识别（VPR）系统在对抗攻击下的脆弱性，提出了一种结合对抗攻击检测器（AAD）和主动导航决策的系统框架，显著提升了定位性能。


<details>
  <summary>Details</summary>
Motivation: VPR系统在对抗攻击下缺乏防御能力，可能导致机器人导航灾难性后果，因此需要研究其脆弱性并提出解决方案。

Method: 分析了四种常见对抗攻击和四种VPR特有攻击对定位性能的影响，提出了一种结合AAD和主动导航决策的实验框架。

Result: 实验显示，加入AAD能显著改善性能，例如沿轨定位误差减少约50%，检测准确率为75%真阳性率和25%假阳性率。

Conclusion: 研究表明AAD在真实系统中对可信导航至关重要，并为系统设计提供了定量要求。

Abstract: Stand-alone Visual Place Recognition (VPR) systems have little defence
against a well-designed adversarial attack, which can lead to disastrous
consequences when deployed for robot navigation. This paper extensively
analyzes the effect of four adversarial attacks common in other perception
tasks and four novel VPR-specific attacks on VPR localization performance. We
then propose how to close the loop between VPR, an Adversarial Attack Detector
(AAD), and active navigation decisions by demonstrating the performance benefit
of simulated AADs in a novel experiment paradigm -- which we detail for the
robotics community to use as a system framework. In the proposed experiment
paradigm, we see the addition of AADs across a range of detection accuracies
can improve performance over baseline; demonstrating a significant improvement
-- such as a ~50% reduction in the mean along-track localization error -- can
be achieved with True Positive and False Positive detection rates of only 75%
and up to 25% respectively. We examine a variety of metrics including:
Along-Track Error, Percentage of Time Attacked, Percentage of Time in an
`Unsafe' State, and Longest Continuous Time Under Attack. Expanding further on
these results, we provide the first investigation into the efficacy of the Fast
Gradient Sign Method (FGSM) adversarial attack for VPR. The analysis in this
work highlights the need for AADs in real-world systems for trustworthy
navigation, and informs quantitative requirements for system design.

</details>


### [142] [DIGMAPPER: A Modular System for Automated Geologic Map Digitization](https://arxiv.org/abs/2506.16006)
*Weiwei Duan,Michael P. Gerlek,Steven N. Minton,Craig A. Knoblock,Fandel Lin,Theresa Chen,Leeje Jang,Sofia Kirsanova,Zekun Li,Yijun Lin,Yao-Yi Chiang*

Main category: cs.CV

TL;DR: DIGMAPPER是一个模块化、可扩展的系统，用于自动化地质地图的数字化，结合深度学习模型和创新技术，显著提高了效率。


<details>
  <summary>Details</summary>
Motivation: 地质地图的数字化是劳动密集型任务，但其中包含的关键地质信息对可再生能源、电动汽车和国家安全至关重要。

Method: 系统采用Docker化、工作流协调的架构，结合深度学习模型（如布局分析、特征提取和地理配准），并利用上下文学习、合成数据生成和基于Transformer的模型。

Result: 在DARPA-USGS数据集上的评估显示，系统在多类特征提取和地理配准方面表现高精度，已在美国地质调查局部署。

Conclusion: DIGMAPPER显著加速了地质空间数据集的创建，支持国家规模的矿产评估和更广泛的地球科学应用。

Abstract: Historical geologic maps contain rich geospatial information, such as rock
units, faults, folds, and bedding planes, that is critical for assessing
mineral resources essential to renewable energy, electric vehicles, and
national security. However, digitizing maps remains a labor-intensive and
time-consuming task. We present DIGMAPPER, a modular, scalable system developed
in collaboration with the United States Geological Survey (USGS) to automate
the digitization of geologic maps. DIGMAPPER features a fully dockerized,
workflow-orchestrated architecture that integrates state-of-the-art deep
learning models for map layout analysis, feature extraction, and
georeferencing. To overcome challenges such as limited training data and
complex visual content, our system employs innovative techniques, including
in-context learning with large language models, synthetic data generation, and
transformer-based models. Evaluations on over 100 annotated maps from the
DARPA-USGS dataset demonstrate high accuracy across polygon, line, and point
feature extraction, and reliable georeferencing performance. Deployed at USGS,
DIGMAPPER significantly accelerates the creation of analysis-ready geospatial
datasets, supporting national-scale critical mineral assessments and broader
geoscientific applications.

</details>


### [143] [EndoMUST: Monocular Depth Estimation for Robotic Endoscopy via End-to-end Multi-step Self-supervised Training](https://arxiv.org/abs/2506.16017)
*Liangjing Shao,Linxin Bai,Chenkang Du,Xinrong Chen*

Main category: cs.CV

TL;DR: 提出了一种多步微调框架，用于内窥镜场景中的自监督深度估计，解决了光照变化和稀疏纹理问题，并在SCARED和Hamlyn数据集上实现了最佳性能。


<details>
  <summary>Details</summary>
Motivation: 解决内窥镜场景中光照变化和稀疏纹理对深度估计的干扰问题。

Method: 采用多步高效微调框架，分三步训练：光流注册、多尺度图像分解和多变换对齐。

Result: 在SCARED和Hamlyn数据集上，误差降低了4%~10%，达到最佳性能。

Conclusion: 该方法通过分步训练和参数高效微调，显著提升了内窥镜场景中的深度估计效果。

Abstract: Monocular depth estimation and ego-motion estimation are significant tasks
for scene perception and navigation in stable, accurate and efficient
robot-assisted endoscopy. To tackle lighting variations and sparse textures in
endoscopic scenes, multiple techniques including optical flow, appearance flow
and intrinsic image decomposition have been introduced into the existing
methods. However, the effective training strategy for multiple modules are
still critical to deal with both illumination issues and information
interference for self-supervised depth estimation in endoscopy. Therefore, a
novel framework with multistep efficient finetuning is proposed in this work.
In each epoch of end-to-end training, the process is divided into three steps,
including optical flow registration, multiscale image decomposition and
multiple transformation alignments. At each step, only the related networks are
trained without interference of irrelevant information. Based on
parameter-efficient finetuning on the foundation model, the proposed method
achieves state-of-the-art performance on self-supervised depth estimation on
SCARED dataset and zero-shot depth estimation on Hamlyn dataset, with
4\%$\sim$10\% lower error. The evaluation code of this work has been published
on https://github.com/BaymaxShao/EndoMUST.

</details>


### [144] [PAROAttention: Pattern-Aware ReOrdering for Efficient Sparse and Quantized Attention in Visual Generation Models](https://arxiv.org/abs/2506.16054)
*Tianchen Zhao,Ke Hong,Xinhao Yang,Xuefeng Xiao,Huixia Li,Feng Ling,Ruiqi Xie,Siqi Chen,Hongyu Zhu,Yichong Zhang,Yu Wang*

Main category: cs.CV

TL;DR: 论文提出了一种名为PARO的技术，通过重新组织注意力模式来降低视觉生成中的计算和内存成本，同时保持生成质量。


<details>
  <summary>Details</summary>
Motivation: 视觉生成中注意力机制的二次复杂度导致高计算和内存成本，尤其是在高分辨率图像或多帧视频生成中。现有稀疏化和量化技术面临低密度和低比特宽度的挑战。

Method: 提出Pattern-Aware token ReOrdering (PARO)技术，将多样化的注意力模式统一为硬件友好的块状模式，简化稀疏化和量化。

Result: PAROAttention在低密度（20%-30%）和低比特宽度（INT8/INT4）下实现无损生成，速度提升1.9x至2.7x。

Conclusion: PARO技术有效解决了视觉生成中的计算和内存问题，同时保持生成质量。

Abstract: In visual generation, the quadratic complexity of attention mechanisms
results in high memory and computational costs, especially for longer token
sequences required in high-resolution image or multi-frame video generation. To
address this, prior research has explored techniques such as sparsification and
quantization. However, these techniques face significant challenges under low
density and reduced bitwidths. Through systematic analysis, we identify that
the core difficulty stems from the dispersed and irregular characteristics of
visual attention patterns. Therefore, instead of introducing specialized
sparsification and quantization design to accommodate such patterns, we propose
an alternative strategy: *reorganizing* the attention pattern to alleviate the
challenges. Inspired by the local aggregation nature of visual feature
extraction, we design a novel **Pattern-Aware token ReOrdering (PARO)**
technique, which unifies the diverse attention patterns into a
hardware-friendly block-wise pattern. This unification substantially simplifies
and enhances both sparsification and quantization. We evaluate the
performance-efficiency trade-offs of various design choices and finalize a
methodology tailored for the unified pattern. Our approach, **PAROAttention**,
achieves video and image generation with lossless metrics, and nearly identical
results from full-precision (FP) baselines, while operating at notably lower
density (~20%-30%) and bitwidth (**INT8/INT4**), achieving a **1.9x** to
**2.7x** end-to-end latency speedup.

</details>


### [145] [Stepping Out of Similar Semantic Space for Open-Vocabulary Segmentation](https://arxiv.org/abs/2506.16058)
*Yong Liu,SongLi Wu,Sule Bai,Jiahao Wang,Yitong Wang,Yansong Tang*

Main category: cs.CV

TL;DR: 论文提出了一种新基准OpenBench，用于更准确评估开放词汇分割模型的性能，并提出了OVSNet方法，通过特征融合和训练空间扩展提升效果。


<details>
  <summary>Details</summary>
Motivation: 现有测试集在衡量模型对开放词汇概念的理解能力上存在局限，因其语义空间与训练空间相似。

Method: 提出了OVSNet方法，通过异构特征融合和训练空间的无成本扩展提升分割性能。

Result: OVSNet在现有数据集和新基准OpenBench上均取得最优结果。

Conclusion: OpenBench和OVSNet的有效性通过分析得到验证，为开放词汇分割提供了新方向。

Abstract: Open-vocabulary segmentation aims to achieve segmentation of arbitrary
categories given unlimited text inputs as guidance. To achieve this, recent
works have focused on developing various technical routes to exploit the
potential of large-scale pre-trained vision-language models and have made
significant progress on existing benchmarks. However, we find that existing
test sets are limited in measuring the models' comprehension of
``open-vocabulary" concepts, as their semantic space closely resembles the
training space, even with many overlapping categories. To this end, we present
a new benchmark named OpenBench that differs significantly from the training
semantics. It is designed to better assess the model's ability to understand
and segment a wide range of real-world concepts. When testing existing methods
on OpenBench, we find that their performance diverges from the conclusions
drawn on existing test sets. In addition, we propose a method named OVSNet to
improve the segmentation performance for diverse and open scenarios. Through
elaborate fusion of heterogeneous features and cost-free expansion of the
training space, OVSNet achieves state-of-the-art results on both existing
datasets and our proposed OpenBench. Corresponding analysis demonstrate the
soundness and effectiveness of our proposed benchmark and method.

</details>


### [146] [STAR-Pose: Efficient Low-Resolution Video Human Pose Estimation via Spatial-Temporal Adaptive Super-Resolution](https://arxiv.org/abs/2506.16061)
*Yucheng Jin,Jinyan Chen,Ziyue He,Baojun Han,Furan An*

Main category: cs.CV

TL;DR: STAR-Pose是一个针对低分辨率视频中人体姿态估计的时空自适应超分辨率框架，通过改进的Transformer和自适应融合模块提升性能。


<details>
  <summary>Details</summary>
Motivation: 低分辨率视频中的人体姿态估计存在挑战，传统方法计算成本高或依赖高质量输入，难以在资源受限环境中部署。

Method: 提出STAR-Pose框架，结合时空Transformer和CNN分支，采用LeakyReLU改进的线性注意力机制和自适应融合模块，设计姿态感知的复合损失函数。

Result: 在多个主流数据集上表现优异，极低分辨率下mAP提升5.2%，推理速度比级联方法快2.8x至4.4x。

Conclusion: STAR-Pose在低分辨率视频中高效且准确地实现了人体姿态估计，为资源受限环境提供了实用解决方案。

Abstract: Human pose estimation in low-resolution videos presents a fundamental
challenge in computer vision. Conventional methods either assume high-quality
inputs or employ computationally expensive cascaded processing, which limits
their deployment in resource-constrained environments. We propose STAR-Pose, a
spatial-temporal adaptive super-resolution framework specifically designed for
video-based human pose estimation. Our method features a novel spatial-temporal
Transformer with LeakyReLU-modified linear attention, which efficiently
captures long-range temporal dependencies. Moreover, it is complemented by an
adaptive fusion module that integrates parallel CNN branch for local texture
enhancement. We also design a pose-aware compound loss to achieve task-oriented
super-resolution. This loss guides the network to reconstruct structural
features that are most beneficial for keypoint localization, rather than
optimizing purely for visual quality. Extensive experiments on several
mainstream video HPE datasets demonstrate that STAR-Pose outperforms existing
approaches. It achieves up to 5.2% mAP improvement under extremely
low-resolution (64x48) conditions while delivering 2.8x to 4.4x faster
inference than cascaded approaches.

</details>


### [147] [TD3Net: A Temporal Densely Connected Multi-Dilated Convolutional Network for Lipreading](https://arxiv.org/abs/2506.16073)
*Byung Hoon Lee,Wooseok Shin,Sung Won Han*

Main category: cs.CV

TL;DR: 论文提出TD3Net，一种结合密集跳跃连接和多扩张时间卷积的后端架构，用于解决唇读任务中时间连续性信息丢失的问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法在建模复杂时间表示时，由于感受野的盲点可能导致唇部运动连续性信息丢失，限制了性能。

Method: 提出TD3Net，结合密集跳跃连接和多扩张时间卷积，覆盖广泛且密集的感受野，避免盲点。

Result: 在LRW和LRW-1000数据集上，TD3Net性能与现有方法相当，但参数和计算量更少。

Conclusion: TD3Net有效利用多样时间特征并保持时间连续性，在唇读系统中具有显著优势。

Abstract: The word-level lipreading approach typically employs a two-stage framework
with separate frontend and backend architectures to model dynamic lip
movements. Each component has been extensively studied, and in the backend
architecture, temporal convolutional networks (TCNs) have been widely adopted
in state-of-the-art methods. Recently, dense skip connections have been
introduced in TCNs to mitigate the limited density of the receptive field,
thereby improving the modeling of complex temporal representations. However,
their performance remains constrained owing to potential information loss
regarding the continuous nature of lip movements, caused by blind spots in the
receptive field. To address this limitation, we propose TD3Net, a temporal
densely connected multi-dilated convolutional network that combines dense skip
connections and multi-dilated temporal convolutions as the backend
architecture. TD3Net covers a wide and dense receptive field without blind
spots by applying different dilation factors to skip-connected features.
Experimental results on a word-level lipreading task using two large publicly
available datasets, Lip Reading in the Wild (LRW) and LRW-1000, indicate that
the proposed method achieves performance comparable to state-of-the-art
methods. It achieved higher accuracy with fewer parameters and lower
floating-point operations compared to existing TCN-based backend architectures.
Moreover, visualization results suggest that our approach effectively utilizes
diverse temporal features while preserving temporal continuity, presenting
notable advantages in lipreading systems. The code is available at our GitHub
repository:
https://github.com/Leebh-kor/TD3Net-A-Temporal-Densely-Connected-Multi-dilated-Convolutional-Network-for-Lipreading

</details>


### [148] [PR-DETR: Injecting Position and Relation Prior for Dense Video Captioning](https://arxiv.org/abs/2506.16082)
*Yizhe Li,Sanping Zhou,Zheng Qin,Le Wang*

Main category: cs.CV

TL;DR: PR-DETR是一种新的密集视频描述框架，通过显式位置和关系先验提升事件定位和描述生成性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于Transformer的方法隐式学习事件位置和语义，需要大量训练数据且性能受限。

Method: PR-DETR引入位置锚定查询和事件关系编码器，分别提供位置先验和关系先验。

Result: 在ActivityNet Captions和YouCook2数据集上表现优异。

Conclusion: 显式位置和关系先验显著提升密集视频描述任务的性能。

Abstract: Dense video captioning is a challenging task that aims to localize and
caption multiple events in an untrimmed video. Recent studies mainly follow the
transformer-based architecture to jointly perform the two sub-tasks, i.e.,
event localization and caption generation, in an end-to-end manner. Based on
the general philosophy of detection transformer, these methods implicitly learn
the event locations and event semantics, which requires a large amount of
training data and limits the model's performance in practice. In this paper, we
propose a novel dense video captioning framework, named PR-DETR, which injects
the explicit position and relation prior into the detection transformer to
improve the localization accuracy and caption quality, simultaneously. On the
one hand, we first generate a set of position-anchored queries to provide the
scene-specific position and semantic information about potential events as
position prior, which serves as the initial event search regions to eliminate
the implausible event proposals. On the other hand, we further design an event
relation encoder to explicitly calculate the relationship between event
boundaries as relation prior to guide the event interaction to improve the
semantic coherence of the captions. Extensive ablation studies are conducted to
verify the effectiveness of the position and relation prior. Experimental
results also show the competitive performance of our method on ActivityNet
Captions and YouCook2 datasets.

</details>


### [149] [AutoV: Learning to Retrieve Visual Prompt for Large Vision-Language Models](https://arxiv.org/abs/2506.16112)
*Yuan Zhang,Chun-Kai Fan,Tao Huang,Ming Lu,Sicheng Yu,Junwen Pan,Kuan Cheng,Qi She,Shanghang Zhang*

Main category: cs.CV

TL;DR: AutoV是一种自动选择最优视觉提示的方法，通过训练模型从多个候选提示中选择最佳选项，提升大型视觉语言模型的性能。


<details>
  <summary>Details</summary>
Motivation: 手动设计有效的视觉提示耗时且效果有限，因此需要一种自动化的方法来优化视觉提示选择。

Method: 开发了自动数据收集和标注流程，利用预训练模型评估不同视觉提示，并通过排名信号训练AutoV。

Result: AutoV显著提升了多种LVLM的性能，例如LLaVA-OV和Qwen2.5-VL在多个任务中表现更优。

Conclusion: AutoV作为一种自动视觉提示方法，具有潜力成为LVLM的优化工具。

Abstract: Inspired by text prompts in large language models (LLMs), visual prompts have
been explored to enhance the reasoning capabilities of large vision-language
models (LVLMs). Current methods design heuristic visual prompts, such as
overlaying a text-query-guided attention heatmap on the original input image.
However, designing effective prompts manually is challenging and
time-consuming, and it often fails to explore the benefits of different visual
prompts, leading to sub-optimal performance. To this end, we propose
\textbf{AutoV} that learns to automatically select the optimal visual prompt
from various candidates based on given textual queries and the input image. To
train AutoV, we developed an automatic data collection and labeling pipeline
that evaluates various visual prompts with a pre-trained LVLM. We input a set
of visual prompts into the LVLM and rank them according to the prediction
losses generated by the model. Using the ranking as a supervision signal, we
train AutoV to automatically choose the optimal visual prompt from various
visual prompts for LVLMs. Experimental results indicate that AutoV enhances the
performance of various LVLMs across multiple popular image understanding tasks.
For instance, LLaVA-OV with AutoV achieves $\textbf{1.7}\%$ accuracy gain on
LLaVA$^{\text{Wild}}$, and AutoV boosts Qwen2.5-VL by $\textbf{1.9}\%$ on MMMU,
highlighting its potential as an optimal visual prompting method for LVLMs.

</details>


### [150] [FastInit: Fast Noise Initialization for Temporally Consistent Video Generation](https://arxiv.org/abs/2506.16119)
*Chengyu Bai,Yuming Li,Zhongyu Zhao,Jintao Chen,Peidong Jia,Qi She,Ming Lu,Shanghang Zhang*

Main category: cs.CV

TL;DR: FastInit提出了一种快速噪声初始化方法，通过单次前向传播生成高质量视频噪声，显著提升视频生成效率和时间一致性。


<details>
  <summary>Details</summary>
Motivation: 现有方法（如FreeInit）通过迭代优化噪声提高时间一致性，但计算成本高，FastInit旨在解决这一问题。

Method: FastInit训练一个视频噪声预测网络（VNPNet），输入随机噪声和文本提示，单次生成优化后的噪声。

Result: 实验表明，FastInit显著提升视频生成质量和时间一致性，且计算成本低。

Conclusion: FastInit为视频生成提供了一种高效实用的解决方案，代码和数据集将公开。

Abstract: Video generation has made significant strides with the development of
diffusion models; however, achieving high temporal consistency remains a
challenging task. Recently, FreeInit identified a training-inference gap and
introduced a method to iteratively refine the initial noise during inference.
However, iterative refinement significantly increases the computational cost
associated with video generation. In this paper, we introduce FastInit, a fast
noise initialization method that eliminates the need for iterative refinement.
FastInit learns a Video Noise Prediction Network (VNPNet) that takes random
noise and a text prompt as input, generating refined noise in a single forward
pass. Therefore, FastInit greatly enhances the efficiency of video generation
while achieving high temporal consistency across frames. To train the VNPNet,
we create a large-scale dataset consisting of pairs of text prompts, random
noise, and refined noise. Extensive experiments with various text-to-video
models show that our method consistently improves the quality and temporal
consistency of the generated videos. FastInit not only provides a substantial
improvement in video generation but also offers a practical solution that can
be applied directly during inference. The code and dataset will be released.

</details>


### [151] [Neurosymbolic Object-Centric Learning with Distant Supervision](https://arxiv.org/abs/2506.16129)
*Stefano Colamonaco,David Debot,Giuseppe Marra*

Main category: cs.CV

TL;DR: 论文提出了一种神经符号方法DeepObjectLog，直接从原始非结构化感知数据中学习对象中心表示，仅需远距离监督。该方法结合感知模块和符号推理层，显著优于现有基线。


<details>
  <summary>Details</summary>
Motivation: 现有系统依赖对象级监督或预定义的对象分解，限制了泛化能力。本文旨在直接从原始数据学习对象表示，减少监督需求。

Method: 提出DeepObjectLog模型，整合感知模块（提取对象表示）和基于概率逻辑编程的符号推理层，通过概率逻辑推理引导对象发现。

Result: 在多种泛化场景（如未见对象组合、任务和对象数量）中，模型表现优于神经和神经符号基线。

Conclusion: DeepObjectLog展示了直接从原始数据学习对象中心表示的潜力，为神经符号推理提供了新方向。

Abstract: Relational learning enables models to generalize across structured domains by
reasoning over objects and their interactions. While recent advances in
neurosymbolic reasoning and object-centric learning bring us closer to this
goal, existing systems rely either on object-level supervision or on a
predefined decomposition of the input into objects. In this work, we propose a
neurosymbolic formulation for learning object-centric representations directly
from raw unstructured perceptual data and using only distant supervision. We
instantiate this approach in DeepObjectLog, a neurosymbolic model that
integrates a perceptual module, which extracts relevant object representations,
with a symbolic reasoning layer based on probabilistic logic programming. By
enabling sound probabilistic logical inference, the symbolic component
introduces a novel learning signal that further guides the discovery of
meaningful objects in the input. We evaluate our model across a diverse range
of generalization settings, including unseen object compositions, unseen tasks,
and unseen number of objects. Experimental results show that our method
outperforms neural and neurosymbolic baselines across the tested settings.

</details>


### [152] [GRPO-CARE: Consistency-Aware Reinforcement Learning for Multimodal Reasoning](https://arxiv.org/abs/2506.16141)
*Yi Chen,Yuying Ge,Rui Wang,Yixiao Ge,Junhao Cheng,Ying Shan,Xihui Liu*

Main category: cs.CV

TL;DR: 论文提出GRPO-CARE框架，通过双重奖励机制提升多模态大语言模型的推理一致性和答案准确性，并在SEED-Bench-R1基准上验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有强化学习方法（如GRPO）在多模态大语言模型（MLLMs）中的应用缺乏严格评估，且存在推理步骤与答案逻辑一致性不足的问题。

Method: 提出GRPO-CARE框架，包含基础答案正确性奖励和自适应一致性奖励，通过参考模型和同伴比较优化推理路径。

Result: GRPO-CARE在SEED-Bench-R1上表现优于标准GRPO，性能提升6.7%，一致性提高24.5%，并展示出强迁移能力。

Conclusion: GRPO-CARE为MLLMs提供了一种可泛化的后训练框架，推动了更可解释和鲁棒的模型发展。

Abstract: Recent reinforcement learning approaches, such as outcome-supervised GRPO,
have advanced Chain-of-Thought reasoning in large language models (LLMs), yet
their adaptation to multimodal LLMs (MLLMs) is unexplored. To address the lack
of rigorous evaluation for MLLM post-training methods, we introduce
SEED-Bench-R1, a benchmark with complex real-world videos requiring balanced
perception and reasoning. It offers a large training set and evaluates
generalization across three escalating challenges: in-distribution,
cross-environment, and cross-environment-task scenarios. Using SEED-Bench-R1,
we find that standard GRPO, while improving answer accuracy, often reduces
logical coherence between reasoning steps and answers, with only a 57.9%
consistency rate. This stems from reward signals focusing solely on final
answers, encouraging shortcuts, and strict KL penalties limiting exploration.To
address this, we propose GRPO-CARE, a consistency-aware RL framework optimizing
both answer correctness and reasoning coherence without explicit supervision.
GRPO-CARE introduces a two-tiered reward: (1) a base reward for answer
correctness, and (2) an adaptive consistency bonus, computed by comparing the
model's reasoning-to-answer likelihood (via a slowly-evolving reference model)
against group peers.This dual mechanism amplifies rewards for reasoning paths
that are both correct and logically consistent. Replacing KL penalties with
this adaptive bonus, GRPO-CARE outperforms standard GRPO on SEED-Bench-R1,
achieving a 6.7% performance gain on the hardest evaluation level and a 24.5%
improvement in consistency. It also shows strong transferability, improving
model performance across diverse video understanding benchmarks. Our work
contributes a systematically designed benchmark and a generalizable
post-training framework, advancing the development of more interpretable and
robust MLLMs.

</details>


### [153] [MBA: Multimodal Bidirectional Attack for Referring Expression Segmentation Models](https://arxiv.org/abs/2506.16157)
*Xingbai Chen,Tingchao Fu,Renyang Liu,Wei Zhou,Chao Yi*

Main category: cs.CV

TL;DR: 本文提出了一种针对Referring Expression Segmentation（RES）模型的多模态双向攻击方法，通过联合优化图像和文本模态，生成具有跨文本迁移性的对抗样本。


<details>
  <summary>Details</summary>
Motivation: 尽管RES模型在自然语言描述下的对象分割中表现优异，但其对抗鲁棒性尚未被充分研究。现有攻击方法在RES上效果不佳，且实际场景中用户会使用多样化的文本输入，因此需要一种能适应多模态结构的攻击策略。

Method: 提出Multimodal Bidirectional Attack，通过可学习的代理文本嵌入扰动和视觉对齐优化，联合优化图像和文本模态，生成对抗样本。

Result: 在多个RES模型和基准数据集上的实验表明，该方法优于现有方法，生成的对抗样本具有更强的跨文本迁移性。

Conclusion: 该方法有效提升了对抗样本在RES模型中的攻击效果，为多模态模型的鲁棒性研究提供了新思路。

Abstract: Referring Expression Segmentation (RES) enables precise object segmentation
in images based on natural language descriptions, offering high flexibility and
broad applicability in real-world vision tasks. Despite its impressive
performance, the robustness of RES models against adversarial examples remains
largely unexplored. While prior adversarial attack methods have explored
adversarial robustness on conventional segmentation models, they perform poorly
when directly applied to RES, failing to expose vulnerabilities in its
multimodal structure. Moreover, in practical open-world scenarios, users
typically issue multiple, diverse referring expressions to interact with the
same image, highlighting the need for adversarial examples that generalize
across varied textual inputs. To address these multimodal challenges, we
propose a novel adversarial attack strategy termed \textbf{Multimodal
Bidirectional Attack}, tailored for RES models. Our method introduces learnable
proxy textual embedding perturbation and jointly performs visual-aligned
optimization on the image modality and textual-adversarial optimization on the
textual modality during attack generation. This dual optimization framework
encourages adversarial images to actively adapt to more challenging text
embedding during optimization, thereby enhancing their cross-text
transferability, which refers to the ability of adversarial examples to remain
effective under a variety of unseen or semantically diverse textual inputs.
Extensive experiments conducted on multiple RES models and benchmark datasets
demonstrate the superior effectiveness of our method compared to existing
methods.

</details>


### [154] [Co-Speech Gesture and Facial Expression Generation for Non-Photorealistic 3D Characters](https://arxiv.org/abs/2506.16159)
*Taisei Omine,Naoyuki Kawabata,Fuminori Homma*

Main category: cs.CV

TL;DR: 研究提出了一种为非写实角色（如动漫角色）设计情感表达的方法，通过漫画提取表情数据和对话语义手势，显著优于现有研究。


<details>
  <summary>Details</summary>
Motivation: 现有研究多关注写实虚拟形象，缺乏对非写实角色情感表达的支持。

Method: 利用漫画提取表情数据并结合对话语义手势，设计非写实角色的情感表达方法。

Result: 用户研究表明，该方法在多个方面显著优于现有研究。

Conclusion: 该方法为非写实角色的情感表达提供了有效解决方案。

Abstract: With the advancement of conversational AI, research on bodily expressions,
including gestures and facial expressions, has also progressed. However, many
existing studies focus on photorealistic avatars, making them unsuitable for
non-photorealistic characters, such as those found in anime. This study
proposes methods for expressing emotions, including exaggerated expressions
unique to non-photorealistic characters, by utilizing expression data extracted
from comics and dialogue-specific semantic gestures. A user study demonstrated
significant improvements across multiple aspects when compared to existing
research.

</details>


### [155] [Align the GAP: Prior-based Unified Multi-Task Remote Physiological Measurement Framework For Domain Generalization and Personalization](https://arxiv.org/abs/2506.16160)
*Jiyao Wang,Xiao Yang,Hao Lu,Dengbo He,Kaishun Wu*

Main category: cs.CV

TL;DR: 提出了一种统一框架GAP，结合多源语义域泛化（MSSDG）和测试时个性化适应（TTPA），通过先验知识提升远程生理测量的泛化性和个性化能力。


<details>
  <summary>Details</summary>
Motivation: 多任务远程生理测量中，部分标注和环境噪声影响任务准确性，且泛化与个性化方法之间存在显著差距。

Method: 将面部视频信息分解为不变语义、个体偏差和噪声，利用先验知识设计多模块框架，分别处理泛化和个性化需求。

Result: 在六个公开数据集和新引入的真实驾驶数据集上验证了框架的有效性。

Conclusion: GAP框架能同时解决MSSDG和TTPA问题，代码和新数据集将公开。

Abstract: Multi-source synsemantic domain generalization (MSSDG) for multi-task remote
physiological measurement seeks to enhance the generalizability of these
metrics and attracts increasing attention. However, challenges like partial
labeling and environmental noise may disrupt task-specific accuracy. Meanwhile,
given that real-time adaptation is necessary for personalized products, the
test-time personalized adaptation (TTPA) after MSSDG is also worth exploring,
while the gap between previous generalization and personalization methods is
significant and hard to fuse. Thus, we proposed a unified framework for
MSSD\textbf{G} and TTP\textbf{A} employing \textbf{P}riors (\textbf{GAP}) in
biometrics and remote photoplethysmography (rPPG). We first disentangled
information from face videos into invariant semantics, individual bias, and
noise. Then, multiple modules incorporating priors and our observations were
applied in different stages and for different facial information. Then, based
on the different principles of achieving generalization and personalization,
our framework could simultaneously address MSSDG and TTPA under multi-task
remote physiological estimation with minimal adjustments. We expanded the MSSDG
benchmark to the TTPA protocol on six publicly available datasets and
introduced a new real-world driving dataset with complete labeling. Extensive
experiments that validated our approach, and the codes along with the new
dataset will be released.

</details>


### [156] [Integrating Generative Adversarial Networks and Convolutional Neural Networks for Enhanced Traffic Accidents Detection and Analysis](https://arxiv.org/abs/2506.16186)
*Zhenghao Xi,Xiang Liu,Yaqi Liu,Yitong Cai,Yangyu Zheng*

Main category: cs.CV

TL;DR: 该研究利用深度学习技术（GANs和CNN）解决交通事故检测中的数据不足问题，提出了一种高效框架，准确率达94%-95%。


<details>
  <summary>Details</summary>
Motivation: 全球交通事故数量上升，需要智能、高效的自动化事故检测系统以挽救生命。

Method: 结合GANs生成数据和CNN训练模型，使用CNN、FTCNN和VIT三种模型进行事故检测。

Result: FTCNN和VIT模型准确率分别为94%和95%，CNN为88%，验证了框架的高效性。

Conclusion: 该框架适用于实时交通监控和智能城市系统，为未来智能监控奠定了基础。

Abstract: Accident detection using Closed Circuit Television (CCTV) footage is one of
the most imperative features for enhancing transport safety and efficient
traffic control. To this end, this research addresses the issues of supervised
monitoring and data deficiency in accident detection systems by adapting
excellent deep learning technologies. The motivation arises from rising
statistics in the number of car accidents worldwide; this calls for innovation
and the establishment of a smart, efficient and automated way of identifying
accidents and calling for help to save lives. Addressing the problem of the
scarcity of data, the presented framework joins Generative Adversarial Networks
(GANs) for synthesizing data and Convolutional Neural Networks (CNN) for model
training. Video frames for accidents and non-accidents are collected from
YouTube videos, and we perform resizing, image enhancement and image
normalisation pixel range adjustments. Three models are used: CNN, Fine-tuned
Convolutional Neural Network (FTCNN) and Vision Transformer (VIT) worked best
for detecting accidents from CCTV, obtaining an accuracy rate of 94% and 95%,
while the CNN model obtained 88%. Such results show that the proposed framework
suits traffic safety applications due to its high real-time accident detection
capabilities and broad-scale applicability. This work lays the foundation for
intelligent surveillance systems in the future for real-time traffic
monitoring, smart city framework, and integration of intelligent surveillance
systems into emergency management systems.

</details>


### [157] [VideoGAN-based Trajectory Proposal for Automated Vehicles](https://arxiv.org/abs/2506.16209)
*Annajoyce Mariani,Kira Maag,Hanno Gottschalk*

Main category: cs.CV

TL;DR: 使用生成对抗网络（GAN）从鸟瞰视角（BEV）视频中生成统计准确的交通轨迹，以捕捉复杂多模态分布。


<details>
  <summary>Details</summary>
Motivation: 现有方法难以有效捕捉未来轨迹的复杂多模态分布，需要更高效的解决方案。

Method: 提出基于低分辨率BEV占用网格视频的GAN训练流程，通过单帧目标检测和帧间匹配提取轨迹数据。

Result: 在100 GPU小时内完成训练，推理时间低于20毫秒，生成的轨迹在空间和动态参数上与真实数据对齐。

Conclusion: GAN能高效生成物理真实的交通轨迹，适用于自动驾驶场景。

Abstract: Being able to generate realistic trajectory options is at the core of
increasing the degree of automation of road vehicles. While model-driven,
rule-based, and classical learning-based methods are widely used to tackle
these tasks at present, they can struggle to effectively capture the complex,
multimodal distributions of future trajectories. In this paper we investigate
whether a generative adversarial network (GAN) trained on videos of bird's-eye
view (BEV) traffic scenarios can generate statistically accurate trajectories
that correctly capture spatial relationships between the agents. To this end,
we propose a pipeline that uses low-resolution BEV occupancy grid videos as
training data for a video generative model. From the generated videos of
traffic scenarios we extract abstract trajectory data using single-frame object
detection and frame-to-frame object matching. We particularly choose a GAN
architecture for the fast training and inference times with respect to
diffusion models. We obtain our best results within 100 GPU hours of training,
with inference times under 20\,ms. We demonstrate the physical realism of the
proposed trajectories in terms of distribution alignment of spatial and dynamic
parameters with respect to the ground truth videos from the Waymo Open Motion
Dataset.

</details>


### [158] [FOCoOp: Enhancing Out-of-Distribution Robustness in Federated Prompt Learning for Vision-Language Models](https://arxiv.org/abs/2506.16218)
*Xinting Liao,Weiming Liu,Jiaming Qian,Pengyang Zhou,Jiahe Xu,Wenjie Wang,Chaochao Chen,Xiaolin Zheng,Tat-Seng Chua*

Main category: cs.CV

TL;DR: FOCoOp框架通过全局、局部和OOD提示优化联邦提示学习，提升分布外数据的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有联邦提示学习方法在性能和鲁棒性之间存在权衡，尤其在分布外数据上表现不佳，限制了实际应用。

Method: 提出FOCoOp框架，利用三类提示（全局、局部、OOD）实现类别和分布级别的分离，并通过双层分布鲁棒优化适应OOD变化。

Result: 实验表明FOCoOp能有效捕捉异构分布并增强对OOD变化的鲁棒性。

Conclusion: FOCoOp填补了联邦提示学习在分布外数据上的不足，提升了模型的可靠性和适应性。

Abstract: Federated prompt learning (FPL) for vision-language models is a powerful
approach to collaboratively adapt models across distributed clients while
preserving data privacy. However, existing FPL approaches suffer from a
trade-off between performance and robustness, particularly in
out-of-distribution (OOD) shifts, limiting their reliability in real-world
scenarios. The inherent in-distribution (ID) data heterogeneity among different
clients makes it more challenging to maintain this trade-off. To fill this gap,
we introduce a Federated OOD-aware Context Optimization (FOCoOp) framework,
which captures diverse distributions among clients using ID global prompts,
local prompts, and OOD prompts. Specifically, FOCoOp leverages three sets of
prompts to create both class-level and distribution-level separations, which
adapt to OOD shifts through bi-level distributionally robust optimization.
Additionally, FOCoOp improves the discrimination consistency among clients,
i.e., calibrating global prompts, seemingly OOD prompts, and OOD prompts by
semi-unbalanced optimal transport. The extensive experiments on real-world
datasets demonstrate that FOCoOp effectively captures decentralized
heterogeneous distributions and enhances robustness of different OOD shifts.
The project is available at GitHub.

</details>


### [159] [R3eVision: A Survey on Robust Rendering, Restoration, and Enhancement for 3D Low-Level Vision](https://arxiv.org/abs/2506.16262)
*Weeyoung Kwon,Jeahun Sung,Minkyu Jeon,Chanho Eom,Jihyong Oh*

Main category: cs.CV

TL;DR: 该论文综述了3D低层视觉（3D LLV）领域，探讨了如何在退化条件下实现高保真3D重建，并提出了相关挑战和方法。


<details>
  <summary>Details</summary>
Motivation: 现有神经渲染方法（如NeRF和3DGS）依赖高质量输入，限制了其在真实世界退化条件下的鲁棒性，因此需要研究3D LLV以解决这一问题。

Method: 通过形式化退化感知渲染问题，分类整合低层视觉任务的神经渲染方法，并讨论时空一致性和病态优化等关键挑战。

Result: 综述了代表性方法、数据集和评估协议，展示了3D LLV在退化条件下实现高保真3D重建的潜力。

Conclusion: 3D LLV是真实环境中鲁棒3D内容生成和场景重建的基础方向，具有广泛的应用前景。

Abstract: Neural rendering methods such as Neural Radiance Fields (NeRF) and 3D
Gaussian Splatting (3DGS) have achieved significant progress in photorealistic
3D scene reconstruction and novel view synthesis. However, most existing models
assume clean and high-resolution (HR) multi-view inputs, which limits their
robustness under real-world degradations such as noise, blur, low-resolution
(LR), and weather-induced artifacts. To address these limitations, the emerging
field of 3D Low-Level Vision (3D LLV) extends classical 2D Low-Level Vision
tasks including super-resolution (SR), deblurring, weather degradation removal,
restoration, and enhancement into the 3D spatial domain. This survey, referred
to as R\textsuperscript{3}eVision, provides a comprehensive overview of robust
rendering, restoration, and enhancement for 3D LLV by formalizing the
degradation-aware rendering problem and identifying key challenges related to
spatio-temporal consistency and ill-posed optimization. Recent methods that
integrate LLV into neural rendering frameworks are categorized to illustrate
how they enable high-fidelity 3D reconstruction under adverse conditions.
Application domains such as autonomous driving, AR/VR, and robotics are also
discussed, where reliable 3D perception from degraded inputs is critical. By
reviewing representative methods, datasets, and evaluation protocols, this work
positions 3D LLV as a fundamental direction for robust 3D content generation
and scene-level reconstruction in real-world environments.

</details>


### [160] [Dense 3D Displacement Estimation for Landslide Monitoring via Fusion of TLS Point Clouds and Embedded RGB Images](https://arxiv.org/abs/2506.16265)
*Zhaoyi Wang,Jemil Avers Butt,Shengyu Huang,Tomislav Medic,Andreas Wieser*

Main category: cs.CV

TL;DR: 提出了一种基于点云和RGB图像的分层分区方法，用于估计密集3D位移场，提高了滑坡监测的空间覆盖率和准确性。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖单一几何或辐射信息，导致稀疏或非3D位移估计，无法满足滑坡监测需求。

Method: 融合3D点云和RGB图像，通过几何一致性检查和刚性变换估计，实现密集3D位移场估计。

Result: 在两个真实滑坡数据集上，空间覆盖率达79%和97%，位移幅度偏差低于扫描分辨率，优于现有方法F2S3。

Conclusion: 该方法为滑坡监测提供了实用且适应性强的解决方案，并可扩展至其他点云和监测任务。

Abstract: Landslide monitoring is essential for understanding geohazards and mitigating
associated risks. However, existing point cloud-based methods typically rely on
either geometric or radiometric information and often yield sparse or non-3D
displacement estimates. In this paper, we propose a hierarchical
partition-based coarse-to-fine approach that fuses 3D point clouds and
co-registered RGB images to estimate dense 3D displacement vector fields. We
construct patch-level matches using both 3D geometry and 2D image features.
These matches are refined via geometric consistency checks, followed by rigid
transformation estimation per match. Experimental results on two real-world
landslide datasets demonstrate that our method produces 3D displacement
estimates with high spatial coverage (79% and 97%) and high accuracy.
Deviations in displacement magnitude with respect to external measurements
(total station or GNSS observations) are 0.15 m and 0.25 m on the two datasets,
respectively, and only 0.07 m and 0.20 m compared to manually derived
references. These values are below the average scan resolutions (0.08 m and
0.30 m). Our method outperforms the state-of-the-art method F2S3 in spatial
coverage while maintaining comparable accuracy. Our approach offers a practical
and adaptable solution for TLS-based landslide monitoring and is extensible to
other types of point clouds and monitoring tasks. Our example data and source
code are publicly available at https://github.com/zhaoyiww/fusion4landslide.

</details>


### [161] [SycnMapV2: Robust and Adaptive Unsupervised Segmentation](https://arxiv.org/abs/2506.16297)
*Heng Zhang,Zikang Wan,Danilo Vasconcellos Vargas*

Main category: cs.CV

TL;DR: SyncMapV2是一种无监督分割算法，具有卓越的鲁棒性和在线适应能力，无需重新初始化即可处理各种输入。


<details>
  <summary>Details</summary>
Motivation: 人类视觉在无监督分割中表现优异，而现有AI算法在噪声条件下性能下降明显，SyncMapV2旨在解决这一问题。

Method: 基于自组织动力学方程和随机网络概念，无需鲁棒训练或监督。

Result: 在数字噪声、天气和模糊等条件下，性能下降显著低于现有方法（如mIoU仅下降0.01%）。

Conclusion: SyncMapV2为未来鲁棒且自适应的智能算法提供了新方向。

Abstract: Human vision excels at segmenting visual cues without the need for explicit
training, and it remains remarkably robust even as noise severity increases. In
contrast, existing AI algorithms struggle to maintain accuracy under similar
conditions. Here, we present SyncMapV2, the first to solve unsupervised
segmentation with state-of-the-art robustness. SyncMapV2 exhibits a minimal
drop in mIoU, only 0.01%, under digital corruption, compared to a 23.8% drop
observed in SOTA methods.This superior performance extends across various types
of corruption: noise (7.3% vs. 37.7%), weather (7.5% vs. 33.8%), and blur (7.0%
vs. 29.5%). Notably, SyncMapV2 accomplishes this without any robust training,
supervision, or loss functions. It is based on a learning paradigm that uses
self-organizing dynamical equations combined with concepts from random
networks. Moreover,unlike conventional methods that require re-initialization
for each new input, SyncMapV2 adapts online, mimicking the continuous
adaptability of human vision. Thus, we go beyond the accurate and robust
results, and present the first algorithm that can do all the above online,
adapting to input rather than re-initializing. In adaptability tests, SyncMapV2
demonstrates near-zero performance degradation, which motivates and fosters a
new generation of robust and adaptive intelligence in the near future.

</details>


### [162] [Learning Multi-scale Spatial-frequency Features for Image Denoising](https://arxiv.org/abs/2506.16307)
*Xu Zhao,Chen Zhao,Xiantao Hu,Hongliang Zhang,Ying Tai,Jian Yang*

Main category: cs.CV

TL;DR: 提出了一种新型多尺度自适应双域网络（MADNet），用于图像去噪，通过图像金字塔输入和自适应空间频率学习单元（ASFU）提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖固定单输入单输出的Unet架构，忽视了像素级多尺度表示，且对高、低频噪声特性处理不足。

Method: 使用图像金字塔输入，设计ASFU单元通过可学习掩码分离高低频信息，并在跳跃连接中引入全局特征融合块。

Result: 在合成和真实噪声图像数据集上的实验验证了MADNet优于当前最先进的去噪方法。

Conclusion: MADNet通过多尺度自适应双域设计，显著提升了图像去噪效果。

Abstract: Recent advancements in multi-scale architectures have demonstrated
exceptional performance in image denoising tasks. However, existing
architectures mainly depends on a fixed single-input single-output Unet
architecture, ignoring the multi-scale representations of pixel level. In
addition, previous methods treat the frequency domain uniformly, ignoring the
different characteristics of high-frequency and low-frequency noise. In this
paper, we propose a novel multi-scale adaptive dual-domain network (MADNet) for
image denoising. We use image pyramid inputs to restore noise-free results from
low-resolution images. In order to realize the interaction of high-frequency
and low-frequency information, we design an adaptive spatial-frequency learning
unit (ASFU), where a learnable mask is used to separate the information into
high-frequency and low-frequency components. In the skip connections, we design
a global feature fusion block to enhance the features at different scales.
Extensive experiments on both synthetic and real noisy image datasets verify
the effectiveness of MADNet compared with current state-of-the-art denoising
approaches.

</details>


### [163] [Segment Anything for Satellite Imagery: A Strong Baseline and a Regional Dataset for Automatic Field Delineation](https://arxiv.org/abs/2506.16318)
*Carmelo Scribano,Elena Govi,Paolo bertellini,Simone Parisi,Giorgia Franchini,Marko Bertogna*

Main category: cs.CV

TL;DR: 提出了一种基于SAM模型的农田边界自动提取方法，并公开了新的区域数据集ERAS。


<details>
  <summary>Details</summary>
Motivation: 高效农业需要精确的农田边界映射，自动提取可避免昂贵的地面调查。

Method: 基于Segment Anything Model（SAM）的管道，引入微调策略，并补充了新的区域数据集ERAS。

Result: 实验评估了分割准确性和泛化能力，方法为自动化农田边界提取提供了稳健基线。

Conclusion: 该方法有效，ERAS数据集已公开。

Abstract: Accurate mapping of agricultural field boundaries is essential for the
efficient operation of agriculture. Automatic extraction from high-resolution
satellite imagery, supported by computer vision techniques, can avoid costly
ground surveys. In this paper, we present a pipeline for field delineation
based on the Segment Anything Model (SAM), introducing a fine-tuning strategy
to adapt SAM to this task. In addition to using published datasets, we describe
a method for acquiring a complementary regional dataset that covers areas
beyond current sources. Extensive experiments assess segmentation accuracy and
evaluate the generalization capabilities. Our approach provides a robust
baseline for automated field delineation. The new regional dataset, known as
ERAS, is now publicly available.

</details>


### [164] [RealDriveSim: A Realistic Multi-Modal Multi-Task Synthetic Dataset for Autonomous Driving](https://arxiv.org/abs/2506.16319)
*Arpit Jadon,Haoran Wang,Phillip Thomas,Michael Stanley,S. Nathaniel Cibik,Rachel Laurat,Omar Maher,Lukas Hoyer,Ozan Unal,Dengxin Dai*

Main category: cs.CV

TL;DR: RealDriveSim是一个多模态合成数据集，用于自动驾驶，支持2D计算机视觉和LiDAR应用，提供64类精细标注，性能优于现有合成基准。


<details>
  <summary>Details</summary>
Motivation: 大规模数据标注成本高，现有合成数据集在范围、真实性和任务适用性上有限，需更高效、通用的解决方案。

Method: 开发RealDriveSim，一个真实感强的多模态合成数据集，支持多种应用和领域，并提供精细标注。

Result: 在广泛的应用和领域中评估，性能优于现有合成基准。

Conclusion: RealDriveSim为自动驾驶提供了一种高效、低成本的数据解决方案，公开可用。

Abstract: As perception models continue to develop, the need for large-scale datasets
increases. However, data annotation remains far too expensive to effectively
scale and meet the demand. Synthetic datasets provide a solution to boost model
performance with substantially reduced costs. However, current synthetic
datasets remain limited in their scope, realism, and are designed for specific
tasks and applications. In this work, we present RealDriveSim, a realistic
multi-modal synthetic dataset for autonomous driving that not only supports
popular 2D computer vision applications but also their LiDAR counterparts,
providing fine-grained annotations for up to 64 classes. We extensively
evaluate our dataset for a wide range of applications and domains,
demonstrating state-of-the-art results compared to existing synthetic
benchmarks. The dataset is publicly available at
https://realdrivesim.github.io/.

</details>


### [165] [Reliable Few-shot Learning under Dual Noises](https://arxiv.org/abs/2506.16330)
*Ji Zhang,Jingkuan Song,Lianli Gao,Nicu Sebe,Heng Tao Shen*

Main category: cs.CV

TL;DR: DETA++提出了一种去噪任务适应方法，通过对比相关性聚合和噪声熵最大化损失，提升小样本学习在开放世界中的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有方法在开放世界中可能因支持样本和查询样本中的分布内外噪声而失效，DETA++旨在解决这一问题。

Method: 使用对比相关性聚合模块计算样本权重，提出干净原型损失和噪声熵最大化损失，并结合内存库和局部最近质心分类器。

Result: 实验证明DETA++在噪声环境下具有有效性和灵活性。

Conclusion: DETA++通过去噪和原型修正，显著提升了小样本学习的鲁棒性和预测可靠性。

Abstract: Recent advances in model pre-training give rise to task adaptation-based
few-shot learning (FSL), where the goal is to adapt a pre-trained task-agnostic
model for capturing task-specific knowledge with a few-labeled support samples
of the target task.Nevertheless, existing approaches may still fail in the open
world due to the inevitable in-distribution (ID) and out-of-distribution (OOD)
noise from both support and query samples of the target task. With limited
support samples available, i) the adverse effect of the dual noises can be
severely amplified during task adaptation, and ii) the adapted model can
produce unreliable predictions on query samples in the presence of the dual
noises. In this work, we propose DEnoised Task Adaptation (DETA++) for reliable
FSL. DETA++ uses a Contrastive Relevance Aggregation (CoRA) module to calculate
image and region weights for support samples, based on which a clean prototype
loss and a noise entropy maximization loss are proposed to achieve noise-robust
task adaptation. Additionally,DETA++ employs a memory bank to store and refine
clean regions for each inner-task class, based on which a Local Nearest
Centroid Classifier (LocalNCC) is devised to yield noise-robust predictions on
query samples. Moreover, DETA++ utilizes an Intra-class Region Swapping
(IntraSwap) strategy to rectify ID class prototypes during task adaptation,
enhancing the model's robustness to the dual noises. Extensive experiments
demonstrate the effectiveness and flexibility of DETA++.

</details>


### [166] [Transparency Techniques for Neural Networks trained on Writer Identification and Writer Verification](https://arxiv.org/abs/2506.16331)
*Viktoria Pundy,Marco Peer,Florian Kleber*

Main category: cs.CV

TL;DR: 论文研究了神经网络在笔迹识别和验证中的透明度问题，首次应用了两种透明度技术，并评估了其效果。


<details>
  <summary>Details</summary>
Motivation: 提高神经网络在笔迹识别和验证中的透明度和可靠性，支持法医专家分析手写文本的相似性。

Method: 应用两种透明度技术：像素级显著性图和点特异性显著性图，并使用删除和插入评分指标进行评估。

Result: 像素级显著性图优于点特异性显著性图，适合支持法医专家工作。

Conclusion: 像素级显著性图在笔迹识别和验证中表现更优，有助于提升透明度和专家支持。

Abstract: Neural Networks are the state of the art for many tasks in the computer
vision domain, including Writer Identification (WI) and Writer Verification
(WV). The transparency of these "black box" systems is important for
improvements of performance and reliability. For this work, two transparency
techniques are applied to neural networks trained on WI and WV for the first
time in this domain. The first technique provides pixel-level saliency maps,
while the point-specific saliency maps of the second technique provide
information on similarities between two images. The transparency techniques are
evaluated using deletion and insertion score metrics. The goal is to support
forensic experts with information on similarities in handwritten text and to
explore the characteristics selected by a neural network for the identification
process. For the qualitative evaluation, the highlights of the maps are
compared to the areas forensic experts consider during the identification
process. The evaluation results show that the pixel-wise saliency maps
outperform the point-specific saliency maps and are suitable for the support of
forensic experts.

</details>


### [167] [MambaHash: Visual State Space Deep Hashing Model for Large-Scale Image Retrieval](https://arxiv.org/abs/2506.16353)
*Chao He,Hongxi Wei*

Main category: cs.CV

TL;DR: 论文提出了一种基于视觉状态空间的哈希模型MambaHash，用于大规模图像检索，通过分组Mamba操作和通道交互注意力模块提升性能。


<details>
  <summary>Details</summary>
Motivation: 探索Mamba在大规模图像检索任务中的适用性，并提出一种高效的哈希方法。

Method: 提出分阶段的主干网络，引入分组Mamba操作建模局部和全局信息，使用通道交互注意力模块增强跨通道通信，设计自适应特征增强模块提升特征多样性。

Result: 在CIFAR-10、NUS-WIDE和IMAGENET数据集上实验表明，MambaHash在效率和性能上优于现有深度哈希方法。

Conclusion: MambaHash是一种高效且性能优越的大规模图像检索方法。

Abstract: Deep image hashing aims to enable effective large-scale image retrieval by
mapping the input images into simple binary hash codes through deep neural
networks. More recently, Vision Mamba with linear time complexity has attracted
extensive attention from researchers by achieving outstanding performance on
various computer tasks. Nevertheless, the suitability of Mamba for large-scale
image retrieval tasks still needs to be explored. Towards this end, we propose
a visual state space hashing model, called MambaHash. Concretely, we propose a
backbone network with stage-wise architecture, in which grouped Mamba operation
is introduced to model local and global information by utilizing Mamba to
perform multi-directional scanning along different groups of the channel.
Subsequently, the proposed channel interaction attention module is used to
enhance information communication across channels. Finally, we meticulously
design an adaptive feature enhancement module to increase feature diversity and
enhance the visual representation capability of the model. We have conducted
comprehensive experiments on three widely used datasets: CIFAR-10, NUS-WIDE and
IMAGENET. The experimental results demonstrate that compared with the
state-of-the-art deep hashing methods, our proposed MambaHash has well
efficiency and superior performance to effectively accomplish large-scale image
retrieval tasks. Source code is available
https://github.com/shuaichaochao/MambaHash.git

</details>


### [168] [Prompt-based Dynamic Token Pruning to Guide Transformer Attention in Efficient Segmentation](https://arxiv.org/abs/2506.16369)
*Pallabi Dutta,Anubhab Maity,Sushmita Mitra*

Main category: cs.CV

TL;DR: 提出了一种自适应提示引导的剪枝方法，减少ViTs在医学图像分割中的计算负担，同时保持准确性。


<details>
  <summary>Details</summary>
Motivation: Vision Transformers（ViTs）在处理大量标记时计算需求高，限制了其在医学图像分析中的实际应用。

Method: 通过提示引导的空间先验对标记进行相关性排序，剪枝低相关性标记，仅保留相关标记进行后续处理。

Result: 实验结果显示标记减少35-55%，计算成本降低，同时保持分割准确性。

Conclusion: 该方法提高了计算效率，适用于资源受限环境，促进实时诊断。

Abstract: The high computational demands of Vision Transformers (ViTs), in processing a
huge number of tokens, often constrain their practical application in analyzing
medical images. This research proposes an adaptive prompt-guided pruning method
to selectively reduce the processing of irrelevant tokens in the segmentation
pipeline. The prompt-based spatial prior helps to rank the tokens according to
their relevance. Tokens with low-relevance scores are down-weighted, ensuring
that only the relevant ones are propagated for processing across subsequent
stages. This data-driven pruning strategy facilitates end-to-end training,
maintains gradient flow, and improves segmentation accuracy by focusing
computational resources on essential regions. The proposed framework is
integrated with several state-of-the-art models to facilitate the elimination
of irrelevant tokens; thereby, enhancing computational efficiency while
preserving segmentation accuracy. The experimental results show a reduction of
$\sim$ 35-55\% tokens; thus reducing the computational costs relative to the
baselines. Cost-effective medical image processing, using our framework,
facilitates real-time diagnosis by expanding its applicability in
resource-constrained environments.

</details>


### [169] [AGC-Drive: A Large-Scale Dataset for Real-World Aerial-Ground Collaboration in Driving Scenarios](https://arxiv.org/abs/2506.16371)
*Yunhao Hou,Bochao Zou,Min Zhang,Ran Chen,Shangdong Yang,Yanmei Zhang,Junbao Zhuo,Siheng Chen,Jiansheng Chen,Huimin Ma*

Main category: cs.CV

TL;DR: 该论文介绍了AGC-Drive，首个大规模真实世界数据集，用于空中-地面协同3D感知，填补了无人机视角在协作感知中的空白。


<details>
  <summary>Details</summary>
Motivation: 现有研究多关注车辆间或车辆与基础设施的协作感知，而无人机提供的动态俯视视角被忽视，主要原因是缺乏高质量数据集。

Method: 通过两辆配备多摄像头和LiDAR的车辆及一架无人机收集数据，构建包含12万LiDAR帧和44万图像的数据集，覆盖14种驾驶场景。

Result: 数据集包含400个场景，13类物体的3D标注，并提供了两种3D感知任务的基准测试。

Conclusion: AGC-Drive填补了空中-地面协作感知的数据空白，并提供了开源工具支持进一步研究。

Abstract: By sharing information across multiple agents, collaborative perception helps
autonomous vehicles mitigate occlusions and improve overall perception
accuracy. While most previous work focus on vehicle-to-vehicle and
vehicle-to-infrastructure collaboration, with limited attention to aerial
perspectives provided by UAVs, which uniquely offer dynamic, top-down views to
alleviate occlusions and monitor large-scale interactive environments. A major
reason for this is the lack of high-quality datasets for aerial-ground
collaborative scenarios. To bridge this gap, we present AGC-Drive, the first
large-scale real-world dataset for Aerial-Ground Cooperative 3D perception. The
data collection platform consists of two vehicles, each equipped with five
cameras and one LiDAR sensor, and one UAV carrying a forward-facing camera and
a LiDAR sensor, enabling comprehensive multi-view and multi-agent perception.
Consisting of approximately 120K LiDAR frames and 440K images, the dataset
covers 14 diverse real-world driving scenarios, including urban roundabouts,
highway tunnels, and on/off ramps. Notably, 19.5% of the data comprises dynamic
interaction events, including vehicle cut-ins, cut-outs, and frequent lane
changes. AGC-Drive contains 400 scenes, each with approximately 100 frames and
fully annotated 3D bounding boxes covering 13 object categories. We provide
benchmarks for two 3D perception tasks: vehicle-to-vehicle collaborative
perception and vehicle-to-UAV collaborative perception. Additionally, we
release an open-source toolkit, including spatiotemporal alignment verification
tools, multi-agent visualization systems, and collaborative annotation
utilities. The dataset and code are available at
https://github.com/PercepX/AGC-Drive.

</details>


### [170] [CLIP-MG: Guiding Semantic Attention with Skeletal Pose Features and RGB Data for Micro-Gesture Recognition on the iMiGUE Dataset](https://arxiv.org/abs/2506.16385)
*Santosh Patapati,Trisanth Srinivasan,Amith Adiraju*

Main category: cs.CV

TL;DR: 提出了一种基于CLIP的微手势识别模型CLIP-MG，结合姿态信息和多模态融合，在iMiGUE数据集上达到61.82%的Top-1准确率。


<details>
  <summary>Details</summary>
Motivation: 微手势因其细微性和低幅度难以识别，现有方法效果有限。

Method: 通过姿态引导的语义查询生成和门控多模态融合机制，将姿态信息整合到CLIP模型中。

Result: Top-1准确率为61.82%。

Conclusion: CLIP-MG展示了潜力，但微手势识别仍具挑战性。

Abstract: Micro-gesture recognition is a challenging task in affective computing due to
the subtle, involuntary nature of the gestures and their low movement
amplitude. In this paper, we introduce a Pose-Guided Semantics-Aware CLIP-based
architecture, or CLIP for Micro-Gesture recognition (CLIP-MG), a modified CLIP
model tailored for micro-gesture classification on the iMiGUE dataset. CLIP-MG
integrates human pose (skeleton) information into the CLIP-based recognition
pipeline through pose-guided semantic query generation and a gated multi-modal
fusion mechanism. The proposed model achieves a Top-1 accuracy of 61.82%. These
results demonstrate both the potential of our approach and the remaining
difficulty in fully adapting vision-language models like CLIP for micro-gesture
recognition.

</details>


### [171] [HyperPath: Knowledge-Guided Hyperbolic Semantic Hierarchy Modeling for WSI Analysis](https://arxiv.org/abs/2506.16398)
*Peixiang Huang,Yanyan Huang,Weiqin Zhao,Junjun He,Lequan Yu*

Main category: cs.CV

TL;DR: 论文提出了一种名为HyperPath的新方法，利用双曲空间建模WSI的语义层次结构，结合文本描述知识提升分类性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要依赖欧几里得嵌入，难以充分捕捉WSI的语义层次结构，因此需要一种更有效的方法。

Method: 通过双曲空间整合视觉和文本特征，设计角度模态对齐损失和语义层次一致性损失，利用测地距离进行分类。

Result: 实验表明，HyperPath在多个任务上优于现有方法，验证了双曲嵌入在WSI分析中的潜力。

Conclusion: 双曲空间能有效建模WSI的语义层次，提升分类性能，为WSI分析提供了新的几何感知方法。

Abstract: Pathology is essential for cancer diagnosis, with multiple instance learning
(MIL) widely used for whole slide image (WSI) analysis. WSIs exhibit a natural
hierarchy -- patches, regions, and slides -- with distinct semantic
associations. While some methods attempt to leverage this hierarchy for
improved representation, they predominantly rely on Euclidean embeddings, which
struggle to fully capture semantic hierarchies. To address this limitation, we
propose HyperPath, a novel method that integrates knowledge from textual
descriptions to guide the modeling of semantic hierarchies of WSIs in
hyperbolic space, thereby enhancing WSI classification. Our approach adapts
both visual and textual features extracted by pathology vision-language
foundation models to the hyperbolic space. We design an Angular Modality
Alignment Loss to ensure robust cross-modal alignment, while a Semantic
Hierarchy Consistency Loss further refines feature hierarchies through
entailment and contradiction relationships and thus enhance semantic coherence.
The classification is performed with geodesic distance, which measures the
similarity between entities in the hyperbolic semantic hierarchy. This
eliminates the need for linear classifiers and enables a geometry-aware
approach to WSI analysis. Extensive experiments show that our method achieves
superior performance across tasks compared to existing methods, highlighting
the potential of hyperbolic embeddings for WSI analysis.

</details>


### [172] [Robustness Evaluation of OCR-based Visual Document Understanding under Multi-Modal Adversarial Attacks](https://arxiv.org/abs/2506.16407)
*Dong Nguyen Tien,Dung D. Le*

Main category: cs.CV

TL;DR: 论文提出了首个统一框架，用于生成和评估基于OCR的视觉文档理解（VDU）模型的多模态对抗攻击，涵盖六种基于梯度的布局攻击场景。


<details>
  <summary>Details</summary>
Motivation: 现有VDU系统在对抗扰动下的鲁棒性研究不足，需要探索多模态攻击的影响。

Method: 采用梯度方法生成对抗攻击，包括OCR边界框、像素和文本的操纵，约束布局扰动预算以保持合理性。

Result: 实验表明，行级攻击和复合扰动（BBox + Pixel + Text）导致性能下降最严重，PGD-based BBox扰动优于随机基线。

Conclusion: 研究验证了布局预算、文本修改和对抗迁移性的重要性，为VDU系统的鲁棒性提供了新见解。

Abstract: Visual Document Understanding (VDU) systems have achieved strong performance
in information extraction by integrating textual, layout, and visual signals.
However, their robustness under realistic adversarial perturbations remains
insufficiently explored. We introduce the first unified framework for
generating and evaluating multi-modal adversarial attacks on OCR-based VDU
models. Our method covers six gradient-based layout attack scenarios,
incorporating manipulations of OCR bounding boxes, pixels, and texts across
both word and line granularities, with constraints on layout perturbation
budget (e.g., IoU >= 0.6) to preserve plausibility.
  Experimental results across four datasets (FUNSD, CORD, SROIE, DocVQA) and
six model families demonstrate that line-level attacks and compound
perturbations (BBox + Pixel + Text) yield the most severe performance
degradation. Projected Gradient Descent (PGD)-based BBox perturbations
outperform random-shift baselines in all investigated models. Ablation studies
further validate the impact of layout budget, text modification, and
adversarial transferability.

</details>


### [173] [Efficient Transformations in Deep Learning Convolutional Neural Networks](https://arxiv.org/abs/2506.16418)
*Berk Yilmaz,Daniel Fidel Harvey,Prajit Dhuri*

Main category: cs.CV

TL;DR: 研究探讨了在ResNet50中集成FFT、WHT和DCT信号处理变换对图像分类的影响，发现WHT显著降低能耗并提高准确率。


<details>
  <summary>Details</summary>
Motivation: 评估信号处理变换在CNN中的计算效率、能耗和分类准确率之间的权衡。

Method: 在ResNet50中集成FFT、WHT和DCT，使用CIFAR-100数据集进行实验。

Result: WHT在早期卷积层中应用时准确率提升至74%，能耗降至39 kJ；在更多层应用时准确率达79%。

Conclusion: WHT是一种高效且有效的能耗优化方法，适用于能耗受限的CNN应用。

Abstract: This study investigates the integration of signal processing transformations
-- Fast Fourier Transform (FFT), Walsh-Hadamard Transform (WHT), and Discrete
Cosine Transform (DCT) -- within the ResNet50 convolutional neural network
(CNN) model for image classification. The primary objective is to assess the
trade-offs between computational efficiency, energy consumption, and
classification accuracy during training and inference. Using the CIFAR-100
dataset (100 classes, 60,000 images), experiments demonstrated that
incorporating WHT significantly reduced energy consumption while improving
accuracy. Specifically, a baseline ResNet50 model achieved a testing accuracy
of 66%, consuming an average of 25,606 kJ per model. In contrast, a modified
ResNet50 incorporating WHT in the early convolutional layers achieved 74%
accuracy, and an enhanced version with WHT applied to both early and late
layers achieved 79% accuracy, with an average energy consumption of only 39 kJ
per model. These results demonstrate the potential of WHT as a highly efficient
and effective approach for energy-constrained CNN applications.

</details>


### [174] [Structured Semantic 3D Reconstruction (S23DR) Challenge 2025 -- Winning solution](https://arxiv.org/abs/2506.16421)
*Jan Skvrna,Lukas Neumann*

Main category: cs.CV

TL;DR: 本文介绍了S23DR Challenge 2025的获胜方案，通过3D深度学习直接从稀疏点云和语义分割预测房屋的3D屋顶线框。


<details>
  <summary>Details</summary>
Motivation: 解决从稀疏点云和语义分割中预测房屋3D屋顶线框的挑战。

Method: 采用两阶段3D深度学习：1) 从COLMAP点云中识别顶点候选；2) 使用两个PointNet-like模型分别精炼顶点和预测边。

Result: 在私有排行榜上获得0.43的混合结构分数(HSS)。

Conclusion: 该方法在3D屋顶线框预测任务中表现出色，赢得了比赛。

Abstract: This paper presents the winning solution for the S23DR Challenge 2025, which
involves predicting a house's 3D roof wireframe from a sparse point cloud and
semantic segmentations. Our method operates directly in 3D, first identifying
vertex candidates from the COLMAP point cloud using Gestalt segmentations. We
then employ two PointNet-like models: one to refine and classify these
candidates by analyzing local cubic patches, and a second to predict edges by
processing the cylindrical regions connecting vertex pairs. This two-stage, 3D
deep learning approach achieved a winning Hybrid Structure Score (HSS) of 0.43
on the private leaderboard.

</details>


### [175] [How Far Can Off-the-Shelf Multimodal Large Language Models Go in Online Episodic Memory Question Answering?](https://arxiv.org/abs/2506.16450)
*Giuseppe Lando,Rosario Forte,Giovanni Maria Farinella,Antonino Furnari*

Main category: cs.CV

TL;DR: 研究探讨现成多模态大语言模型（MLLMs）能否无需额外训练处理在线情景记忆视频问答（OEM-VQA），通过轻量级文本记忆实现高效存储与推理。


<details>
  <summary>Details</summary>
Motivation: 探索现成MLLMs在OEM-VQA任务中的潜力，避免额外训练成本，同时实现高效存储。

Method: 将流式第一人称视频转换为轻量级文本记忆（每分钟仅3.6 kB），通过MLLM描述模块和LLM推理模块回答问题。

Result: 在QAEgo4D-Closed基准上达到56.0%准确率，存储效率比现有系统高10^4/10^5倍。

Conclusion: 现成MLLMs可高效处理OEM-VQA，未来研究可优化组件设计以进一步提升性能。

Abstract: We investigate whether off-the-shelf Multimodal Large Language Models (MLLMs)
can tackle Online Episodic-Memory Video Question Answering (OEM-VQA) without
additional training. Our pipeline converts a streaming egocentric video into a
lightweight textual memory, only a few kilobytes per minute, via an MLLM
descriptor module, and answers multiple-choice questions by querying this
memory with an LLM reasoner module. On the QAEgo4D-Closed benchmark, our best
configuration attains 56.0% accuracy with 3.6 kB per minute storage, matching
the performance of dedicated state-of-the-art systems while being 10**4/10**5
times more memory-efficient. Extensive ablations provides insights into the
role of each component and design choice, and highlight directions of
improvement for future research.

</details>


### [176] [Spotting tell-tale visual artifacts in face swapping videos: strengths and pitfalls of CNN detectors](https://arxiv.org/abs/2506.16497)
*Riccardo Ziglio,Cecilia Pasquini,Silvio Ranise*

Main category: cs.CV

TL;DR: 论文研究了基于CNN的模型在视频换脸检测中的表现，发现其在同一数据源下表现优异，但在跨数据集时难以捕捉遮挡相关线索。


<details>
  <summary>Details</summary>
Motivation: 视频换脸技术的进步带来了远程视频通信中的安全威胁，需要有效检测方法。

Method: 通过基准测试CNN模型在两个数据集（包括新收集的数据集）上的表现，分析其对不同采集源和换脸算法的泛化能力。

Result: CNN模型在同一数据源下表现优异，但在跨数据集时难以捕捉遮挡相关线索。

Conclusion: 需要开发专门的检测策略以应对遮挡相关的视觉伪影。

Abstract: Face swapping manipulations in video streams represents an increasing threat
in remote video communications, due to advances
  in automated and real-time tools. Recent literature proposes to characterize
and exploit visual artifacts introduced in video frames
  by swapping algorithms when dealing with challenging physical scenes, such as
face occlusions. This paper investigates the
  effectiveness of this approach by benchmarking CNN-based data-driven models
on two data corpora (including a newly collected
  one) and analyzing generalization capabilities with respect to different
acquisition sources and swapping algorithms. The results
  confirm excellent performance of general-purpose CNN architectures when
operating within the same data source, but a significant
  difficulty in robustly characterizing occlusion-based visual cues across
datasets. This highlights the need for specialized detection
  strategies to deal with such artifacts.

</details>


### [177] [Hunyuan3D 2.5: Towards High-Fidelity 3D Assets Generation with Ultimate Details](https://arxiv.org/abs/2506.16504)
*Zeqiang Lai,Yunfei Zhao,Haolin Liu,Zibo Zhao,Qingxiang Lin,Huiwen Shi,Xianghui Yang,Mingxin Yang,Shuhui Yang,Yifei Feng,Sheng Zhang,Xin Huang,Di Luo,Fan Yang,Fang Yang,Lifu Wang,Sicong Liu,Yixuan Tang,Yulin Cai,Zebin He,Tian Liu,Yuhong Liu,Jie Jiang,Linus,Jingwei Huang,Chunchao Guo*

Main category: cs.CV

TL;DR: Hunyuan3D 2.5是一款强大的3D扩散模型套件，通过两阶段流程显著提升了形状和纹理生成的质量。


<details>
  <summary>Details</summary>
Motivation: 旨在生成高保真且细节丰富的3D资产，缩小生成与手工制作3D形状之间的差距。

Method: 采用两阶段流程，引入新的形状基础模型LATTICE（10B参数）和基于物理渲染（PBR）的多视图架构。

Result: 在形状和纹理生成方面显著优于先前方法，生成结果更接近手工制作的3D形状。

Conclusion: Hunyuan3D 2.5在3D资产生成领域取得了重要进展，具有更高的保真度和细节表现。

Abstract: In this report, we present Hunyuan3D 2.5, a robust suite of 3D diffusion
models aimed at generating high-fidelity and detailed textured 3D assets.
Hunyuan3D 2.5 follows two-stages pipeline of its previous version Hunyuan3D
2.0, while demonstrating substantial advancements in both shape and texture
generation. In terms of shape generation, we introduce a new shape foundation
model -- LATTICE, which is trained with scaled high-quality datasets,
model-size, and compute. Our largest model reaches 10B parameters and generates
sharp and detailed 3D shape with precise image-3D following while keeping mesh
surface clean and smooth, significantly closing the gap between generated and
handcrafted 3D shapes. In terms of texture generation, it is upgraded with
phyiscal-based rendering (PBR) via a novel multi-view architecture extended
from Hunyuan3D 2.0 Paint model. Our extensive evaluation shows that Hunyuan3D
2.5 significantly outperforms previous methods in both shape and end-to-end
texture generation.

</details>


### [178] [How Hard Is Snow? A Paired Domain Adaptation Dataset for Clear and Snowy Weather: CADC+](https://arxiv.org/abs/2506.16531)
*Mei Qi Tang,Sean Sedwards,Chengjie Huang,Krzysztof Czarnecki*

Main category: cs.CV

TL;DR: CADC+是首个针对冬季自动驾驶的配对天气域适应数据集，扩展了CADC数据集，用于评估降雪对3D物体检测性能的影响。


<details>
  <summary>Details</summary>
Motivation: 当前数据集在降雪和晴朗天气条件下的标记数据不足，或依赖去雪方法生成合成数据，导致评估不准确。

Method: 通过在同一道路和时段采集晴朗天气数据，与CADC的降雪序列配对，创建CADC+数据集以减少无关域偏移。

Result: 初步结果显示，降雪同时引入随机和认知不确定性，既作为噪声也作为独特数据域。

Conclusion: CADC+为研究降雪对3D物体检测的影响提供了更准确的数据基础。

Abstract: The impact of snowfall on 3D object detection performance remains
underexplored. Conducting such an evaluation requires a dataset with sufficient
labelled data from both weather conditions, ideally captured in the same
driving environment. Current driving datasets with LiDAR point clouds either do
not provide enough labelled data in both snowy and clear weather conditions, or
rely on de-snowing methods to generate synthetic clear weather. Synthetic data
often lacks realism and introduces an additional domain shift that confounds
accurate evaluations. To address these challenges, we present CADC+, the first
paired weather domain adaptation dataset for autonomous driving in winter
conditions. CADC+ extends the Canadian Adverse Driving Conditions dataset
(CADC) using clear weather data that was recorded on the same roads and in the
same period as CADC. To create CADC+, we pair each CADC sequence with a clear
weather sequence that matches the snowy sequence as closely as possible. CADC+
thus minimizes the domain shift resulting from factors unrelated to the
presence of snow. We also present some preliminary results using CADC+ to
evaluate the effect of snow on 3D object detection performance. We observe that
snow introduces a combination of aleatoric and epistemic uncertainties, acting
as both noise and a distinct data domain.

</details>


### [179] [From Semantic To Instance: A Semi-Self-Supervised Learning Approach](https://arxiv.org/abs/2506.16563)
*Keyhan Najafian,Farhad Maleki,Lingling Jin,Ian Stavness*

Main category: cs.CV

TL;DR: 提出了一种半自监督学习方法GLMask，用于实例分割，减少了对大量手动标注的依赖，并在农业和通用数据集上表现优异。


<details>
  <summary>Details</summary>
Motivation: 实例分割在植物健康监测等应用中至关重要，但大规模像素级标注数据集的创建成本高，限制了深度学习的使用。

Method: 设计了GLMask图像-掩码表示，专注于形状、纹理和模式，减少对颜色特征的依赖，并通过语义分割生成实例级分割。

Result: 在小麦头实例分割任务中达到98.5% mAP@50，在COCO数据集上性能提升12.6%。

Conclusion: 该方法不仅适用于精准农业，还可推广到其他具有类似数据特征的领域。

Abstract: Instance segmentation is essential for applications such as automated
monitoring of plant health, growth, and yield. However, extensive effort is
required to create large-scale datasets with pixel-level annotations of each
object instance for developing instance segmentation models that restrict the
use of deep learning in these areas. This challenge is more significant in
images with densely packed, self-occluded objects, which are common in
agriculture. To address this challenge, we propose a semi-self-supervised
learning approach that requires minimal manual annotation to develop a
high-performing instance segmentation model. We design GLMask, an image-mask
representation for the model to focus on shape, texture, and pattern while
minimizing its dependence on color features. We develop a pipeline to generate
semantic segmentation and then transform it into instance-level segmentation.
The proposed approach substantially outperforms the conventional instance
segmentation models, establishing a state-of-the-art wheat head instance
segmentation model with mAP@50 of 98.5%. Additionally, we assessed the proposed
methodology on the general-purpose Microsoft COCO dataset, achieving a
significant performance improvement of over 12.6% mAP@50. This highlights that
the utility of our proposed approach extends beyond precision agriculture and
applies to other domains, specifically those with similar data characteristics.

</details>


### [180] [SafeTriage: Facial Video De-identification for Privacy-Preserving Stroke Triage](https://arxiv.org/abs/2506.16578)
*Tongan Cai,Haomiao Ni,Wenchao Ma,Yuan Xue,Qian Ma,Rachel Leicht,Kelvin Wong,John Volpi,Stephen T. C. Wong,James Z. Wang,Sharon X. Huang*

Main category: cs.CV

TL;DR: SafeTriage是一种新方法，通过将真实患者面部视频的运动特征映射到合成身份上，解决AI模型依赖真实患者数据带来的隐私和伦理问题。


<details>
  <summary>Details</summary>
Motivation: 当前AI模型依赖真实患者数据进行中风识别，但存在隐私和伦理挑战。

Method: 使用预训练的视频运动转移模型和条件生成模型，保留诊断相关的面部动态同时隐藏患者身份。

Result: 合成视频有效保留了中风相关的面部模式，同时提供强大的隐私保护。

Conclusion: SafeTriage为神经疾病的数据共享和AI临床分析提供了安全且伦理的基础。

Abstract: Effective stroke triage in emergency settings often relies on clinicians'
ability to identify subtle abnormalities in facial muscle coordination. While
recent AI models have shown promise in detecting such patterns from patient
facial videos, their reliance on real patient data raises significant ethical
and privacy challenges -- especially when training robust and generalizable
models across institutions. To address these concerns, we propose SafeTriage, a
novel method designed to de-identify patient facial videos while preserving
essential motion cues crucial for stroke diagnosis. SafeTriage leverages a
pretrained video motion transfer (VMT) model to map the motion characteristics
of real patient faces onto synthetic identities. This approach retains
diagnostically relevant facial dynamics without revealing the patients'
identities. To mitigate the distribution shift between normal population
pre-training videos and patient population test videos, we introduce a
conditional generative model for visual prompt tuning, which adapts the input
space of the VMT model to ensure accurate motion transfer without needing to
fine-tune the VMT model backbone. Comprehensive evaluation, including
quantitative metrics and clinical expert assessments, demonstrates that
SafeTriage-produced synthetic videos effectively preserve stroke-relevant
facial patterns, enabling reliable AI-based triage. Our evaluations also show
that SafeTriage provides robust privacy protection while maintaining diagnostic
accuracy, offering a secure and ethically sound foundation for data sharing and
AI-driven clinical analysis in neurological disorders.

</details>


### [181] [Spatially-Aware Evaluation of Segmentation Uncertainty](https://arxiv.org/abs/2506.16589)
*Tal Zeevi,Eléonore V. Lieffrig,Lawrence H. Staib,John A. Onofrey*

Main category: cs.CV

TL;DR: 提出三种空间感知指标，结合结构和边界信息，改进医学图像分割中的不确定性评估。


<details>
  <summary>Details</summary>
Motivation: 现有不确定性评估指标忽略空间上下文和解剖结构，无法区分不同模式的不确定性。

Method: 提出三种结合结构和边界信息的空间感知指标，并在前列腺分区分割数据上进行验证。

Result: 新指标与临床重要因素更一致，能更好区分有意义和虚假的不确定性模式。

Conclusion: 空间感知指标显著提升不确定性评估的临床相关性。

Abstract: Uncertainty maps highlight unreliable regions in segmentation predictions.
However, most uncertainty evaluation metrics treat voxels independently,
ignoring spatial context and anatomical structure. As a result, they may assign
identical scores to qualitatively distinct patterns (e.g., scattered vs.
boundary-aligned uncertainty). We propose three spatially aware metrics that
incorporate structural and boundary information and conduct a thorough
validation on medical imaging data from the prostate zonal segmentation
challenge within the Medical Segmentation Decathlon. Our results demonstrate
improved alignment with clinically important factors and better discrimination
between meaningful and spurious uncertainty patterns.

</details>


### [182] [MetaQAP -- A Meta-Learning Approach for Quality-Aware Pretraining in Image Quality Assessment](https://arxiv.org/abs/2506.16601)
*Muhammad Azeem Aslam,Muhammad Hamza,Nisar Ahmed,Gulshan Saleem,Zhu Shuangtong,Hu Hongfei,Xu Wei,Saba Aslam,Wang Jun*

Main category: cs.CV

TL;DR: MetaQAP是一种新型无参考图像质量评估模型，通过质量感知预训练和元学习解决IQA挑战，性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 图像质量评估（IQA）因人类感知的主观性和真实图像失真的复杂性而具有挑战性。

Method: MetaQAP结合质量感知预训练CNN、质量感知损失函数和元学习集成模型。

Result: 在LiveCD、KonIQ-10K和BIQ2021数据集上表现优异，PLCC和SROCC分数显著高于现有方法。

Conclusion: MetaQAP为IQA提供了鲁棒且通用的框架，推动了无参考IQA领域的发展。

Abstract: Image Quality Assessment (IQA) is a critical task in a wide range of
applications but remains challenging due to the subjective nature of human
perception and the complexity of real-world image distortions. This study
proposes MetaQAP, a novel no-reference IQA model designed to address these
challenges by leveraging quality-aware pre-training and meta-learning. The
model performs three key contributions: pre-training Convolutional Neural
Networks (CNNs) on a quality-aware dataset, implementing a quality-aware loss
function to optimize predictions, and integrating a meta-learner to form an
ensemble model that effectively combines predictions from multiple base models.
Experimental evaluations were conducted on three benchmark datasets: LiveCD,
KonIQ-10K, and BIQ2021. The proposed MetaQAP model achieved exceptional
performance with Pearson Linear Correlation Coefficient (PLCC) and Spearman
Rank Order Correlation Coefficient (SROCC) scores of 0.9885/0.9812 on LiveCD,
0.9702/0.9658 on KonIQ-10K, and 0.884/0.8765 on BIQ2021, outperforming existing
IQA methods. Cross-dataset evaluations further demonstrated the
generalizability of the model, with PLCC and SROCC scores ranging from 0.6721
to 0.8023 and 0.6515 to 0.7805, respectively, across diverse datasets. The
ablation study confirmed the significance of each model component, revealing
substantial performance degradation when critical elements such as the
meta-learner or quality-aware loss function were omitted. MetaQAP not only
addresses the complexities of authentic distortions but also establishes a
robust and generalizable framework for practical IQA applications. By advancing
the state-of-the-art in no-reference IQA, this research provides valuable
insights and methodologies for future improvements and extensions in the field.

</details>


### [183] [Leveraging CNN and IoT for Effective E-Waste Management](https://arxiv.org/abs/2506.16647)
*Ajesh Thangaraj Nadar,Gabriel Nixon Raj,Soham Chandane,Sushant Bhat*

Main category: cs.CV

TL;DR: 本文提出了一种结合物联网和轻量级CNN分类的系统，用于电子废物的智能分类与处理。


<details>
  <summary>Details</summary>
Motivation: 电子废物（e-waste）的快速增长及其不当处理对环境和健康构成严重威胁。

Method: 通过集成摄像头系统和数字称重设备，利用轻量级CNN分类管道，基于视觉和重量属性自动分类电子废物。

Result: 系统能够实时检测电路板、传感器等电子废物组件，提升智能回收流程和废物处理效率。

Conclusion: 该IoT和CNN结合的系统为电子废物管理提供了高效、自动化的解决方案。

Abstract: The increasing proliferation of electronic devices in the modern era has led
to a significant surge in electronic waste (e-waste). Improper disposal and
insufficient recycling of e-waste pose serious environmental and health risks.
This paper proposes an IoT-enabled system combined with a lightweight CNN-based
classification pipeline to enhance the identification, categorization, and
routing of e-waste materials. By integrating a camera system and a digital
weighing scale, the framework automates the classification of electronic items
based on visual and weight-based attributes. The system demonstrates how
real-time detection of e-waste components such as circuit boards, sensors, and
wires can facilitate smart recycling workflows and improve overall waste
processing efficiency.

</details>


### [184] [A Comparative Analysis of Principal Component Analysis (PCA) and Singular Value Decomposition (SVD) as Dimensionality Reduction Techniques](https://arxiv.org/abs/2506.16663)
*Michael Gyimadu,Gregory Bell*

Main category: cs.CV

TL;DR: 本文对PCA和SVD两种线性降维技术进行了理论比较，评估了它们的可解释性、数值稳定性和适用性，并提出了选择指南。


<details>
  <summary>Details</summary>
Motivation: 高维图像数据通常需要降维处理，但缺乏对PCA和SVD的理论比较和选择指导。

Method: 从基本原理推导PCA和SVD算法，评估其特性，并结合文献提出选择建议。

Result: 提供了基于理论分析的PCA和SVD选择指南，无需依赖经验基准测试。

Conclusion: 总结了两种方法的优缺点，并指出了未来实验研究的方向。

Abstract: High-dimensional image data often require dimensionality reduction before
further analysis. This paper provides a purely analytical comparison of two
linear techniques-Principal Component Analysis (PCA) and Singular Value
Decomposition (SVD). After the derivation of each algorithm from first
principles, we assess their interpretability, numerical stability, and
suitability for differing matrix shapes. building on classical and recent
numerical literature, We synthesize rule-of-thumb guidelines for choosing one
out of the two algorithms without empirical benchmarking, building on classical
and recent numerical literature. Limitations and directions for future
experimental work are outlined at the end.

</details>


### [185] [Extracting Multimodal Learngene in CLIP: Unveiling the Multimodal Generalizable Knowledge](https://arxiv.org/abs/2506.16673)
*Ruiming Chen,Junming Yang,Shiyu Xia,Xu Yang,Jing Wang,Xin Geng*

Main category: cs.CV

TL;DR: MM-LG提出了一种从CLIP中提取多模态通用知识的新框架，通过多模态和单模态块的加权和提取知识，初始化不同规模的模型，显著提升了性能并降低了计算成本。


<details>
  <summary>Details</summary>
Motivation: CLIP的多模态通用知识对下游任务很重要，但大规模预训练的计算开销大。现有Learngene方法无法处理多模态场景中的通用知识。

Method: 提出MM-LG框架，通过多模态和单模态块提取通用知识，并以加权和方式初始化不同规模和模态的模型。

Result: MM-LG在多个数据集上性能优于现有方法（如Oxford-IIIT PET提升3.1%，Flickr30k提升4.13%），且仅需25%参数存储和减少2.8倍预训练成本。

Conclusion: MM-LG是一种高效的多模态通用知识提取框架，适用于多样化下游任务的高效部署。

Abstract: CLIP (Contrastive Language-Image Pre-training) has attracted widespread
attention for its multimodal generalizable knowledge, which is significant for
downstream tasks. However, the computational overhead of a large number of
parameters and large-scale pre-training poses challenges of pre-training a
different scale of CLIP. Learngene extracts the generalizable components termed
as learngene from an ancestry model and initializes diverse descendant models
with it. Previous Learngene paradigms fail to handle the generalizable
knowledge in multimodal scenarios. In this paper, we put forward the idea of
utilizing a multimodal block to extract the multimodal generalizable knowledge,
which inspires us to propose MM-LG (Multimodal Learngene), a novel framework
designed to extract and leverage generalizable components from CLIP.
Specifically, we first establish multimodal and unimodal blocks to extract the
multimodal and unimodal generalizable knowledge in a weighted-sum manner.
Subsequently, we employ these components to numerically initialize descendant
models of varying scales and modalities. Extensive experiments demonstrate
MM-LG's effectiveness, which achieves performance gains over existing learngene
approaches (e.g.,+3.1% on Oxford-IIIT PET and +4.13% on Flickr30k) and
comparable or superior results to the pre-training and fine-tuning paradigm
(e.g.,+1.9% on Oxford-IIIT PET and +3.65% on Flickr30k). Notably, MM-LG
requires only around 25% of the parameter storage while reducing around 2.8
times pre-training costs for diverse model scales compared to the pre-training
and fine-tuning paradigm, making it particularly suitable for efficient
deployment across diverse downstream tasks.

</details>


### [186] [AnyTraverse: An off-road traversability framework with VLM and human operator in the loop](https://arxiv.org/abs/2506.16826)
*Sattwik Sahu,Agamdeep Singh,Karthik Nambiar,Srikanth Saripalli,P. B. Sujit*

Main category: cs.CV

TL;DR: AnyTraverse是一个结合自然语言提示和人工辅助的框架，用于分割不同机器人车辆的可导航区域，减少主动监督负担并适应多变环境。


<details>
  <summary>Details</summary>
Motivation: 当前框架在非结构化环境中表现不佳，且无法适应不同机器人类型，因此需要一种更灵活、适应性强的解决方案。

Method: 结合自然语言提示和人工辅助，仅在遇到未知场景或类别时调用操作员，采用零样本学习方法避免大量数据收集或重新训练。

Result: 在多个数据集和机器人平台上验证，性能优于GA-NAV和Off-seg，提供了一种车辆无关的解决方案。

Conclusion: AnyTraverse通过平衡自动化和人工监督，为越野可通行性提供了一种高效且适应性强的框架。

Abstract: Off-road traversability segmentation enables autonomous navigation with
applications in search-and-rescue, military operations, wildlife exploration,
and agriculture. Current frameworks struggle due to significant variations in
unstructured environments and uncertain scene changes, and are not adaptive to
be used for different robot types. We present AnyTraverse, a framework
combining natural language-based prompts with human-operator assistance to
determine navigable regions for diverse robotic vehicles. The system segments
scenes for a given set of prompts and calls the operator only when encountering
previously unexplored scenery or unknown class not part of the prompt in its
region-of-interest, thus reducing active supervision load while adapting to
varying outdoor scenes. Our zero-shot learning approach eliminates the need for
extensive data collection or retraining. Our experimental validation includes
testing on RELLIS-3D, Freiburg Forest, and RUGD datasets and demonstrate
real-world deployment on multiple robot platforms. The results show that
AnyTraverse performs better than GA-NAV and Off-seg while offering a
vehicle-agnostic approach to off-road traversability that balances automation
with targeted human supervision.

</details>


### [187] [How to Train your Text-to-Image Model: Evaluating Design Choices for Synthetic Training Captions](https://arxiv.org/abs/2506.16679)
*Manuel Brack,Sudeep Katakol,Felix Friedrich,Patrick Schramowski,Hareesh Ravi,Kristian Kersting,Ajinkya Kale*

Main category: cs.CV

TL;DR: 研究了合成标注策略对文本到图像模型性能的影响，发现密集高质量标注提升文本对齐但可能牺牲美学和多样性，而随机长度标注则能平衡美学与对齐。


<details>
  <summary>Details</summary>
Motivation: 探索合成标注策略对文本到图像模型性能的影响，填补现有文献的空白。

Method: 系统研究不同合成标注策略，并通过实验验证其对模型性能的影响。

Result: 密集高质量标注提升文本对齐但可能牺牲美学和多样性；随机长度标注能平衡美学与对齐；不同标注分布会显著影响模型输出偏差。

Conclusion: 标注设计对模型性能至关重要，研究结果为文本到图像生成提供了更有效的训练数据策略。

Abstract: Training data is at the core of any successful text-to-image models. The
quality and descriptiveness of image text are crucial to a model's performance.
Given the noisiness and inconsistency in web-scraped datasets, recent works
shifted towards synthetic training captions. While this setup is generally
believed to produce more capable models, current literature does not provide
any insights into its design choices. This study closes this gap by
systematically investigating how different synthetic captioning strategies
impact the downstream performance of text-to-image models. Our experiments
demonstrate that dense, high-quality captions enhance text alignment but may
introduce trade-offs in output aesthetics and diversity. Conversely, captions
of randomized lengths yield balanced improvements across aesthetics and
alignment without compromising sample diversity. We also demonstrate that
varying caption distributions introduce significant shifts in the output bias
of a trained model. Our findings underscore the importance of caption design in
achieving optimal model performance and provide practical insights for more
effective training data strategies in text-to-image generation.

</details>


### [188] [Camera Calibration via Circular Patterns: A Comprehensive Framework with Measurement Uncertainty and Unbiased Projection Model](https://arxiv.org/abs/2506.16842)
*Chaehyeon Song,Dongjae Lee,Jongwoo Lim,Ayoung Kim*

Main category: cs.CV

TL;DR: 提出了一种无偏的圆形图案投影模型，并引入不确定性以提高校准的鲁棒性和准确性。


<details>
  <summary>Details</summary>
Motivation: 现有圆形图案的投影模型在镜头畸变下存在偏差，导致性能不佳。

Method: 通过建模二维形状边界点为马尔可夫随机场，结合格林定理传播形状分布到质心不确定性。

Result: 新框架显著提高了校准的准确性和鲁棒性。

Conclusion: 该方法优于传统棋盘格校准，并提供了完整的源代码和演示视频。

Abstract: Camera calibration using planar targets has been widely favored, and two
types of control points have been mainly considered as measurements: the
corners of the checkerboard and the centroid of circles. Since a centroid is
derived from numerous pixels, the circular pattern provides more precise
measurements than the checkerboard. However, the existing projection model of
circle centroids is biased under lens distortion, resulting in low performance.
To surmount this limitation, we propose an unbiased projection model of the
circular pattern and demonstrate its superior accuracy compared to the
checkerboard. Complementing this, we introduce uncertainty into circular
patterns to enhance calibration robustness and completeness. Defining centroid
uncertainty improves the performance of calibration components, including
pattern detection, optimization, and evaluation metrics. We also provide
guidelines for performing good camera calibration based on the evaluation
metric. The core concept of this approach is to model the boundary points of a
two-dimensional shape as a Markov random field, considering its connectivity.
The shape distribution is propagated to the centroid uncertainty through an
appropriate shape representation based on the Green theorem. Consequently, the
resulting framework achieves marked gains in calibration accuracy and
robustness. The complete source code and demonstration video are available at
https://github.com/chaehyeonsong/discocal.

</details>


### [189] [DepthVanish: Optimizing Adversarial Interval Structures for Stereo-Depth-Invisible Patches](https://arxiv.org/abs/2506.16690)
*Yun Xing,Yue Cao,Nhat Chung,Jie Zhang,Ivor Tsang,Ming-Ming Cheng,Yang Liu,Lei Ma,Qing Guo*

Main category: cs.CV

TL;DR: 论文提出了一种新型的条纹结构对抗补丁，显著提升了立体深度估计攻击的物理世界效果，并成功攻击了商业RGB-D相机。


<details>
  <summary>Details</summary>
Motivation: 立体深度估计在自动驾驶和机器人中至关重要，但现有对抗攻击方法在物理世界中效果不佳，限制了其实际应用。

Method: 通过引入条纹结构（即在重复纹理间加入规则间隔），联合优化结构和纹理元素，生成对抗补丁。

Result: 生成的补丁能有效攻击RAFT-Stereo和STTR等先进方法，并在真实环境中成功攻击Intel RealSense相机。

Conclusion: 条纹结构显著提升了对抗补丁的实用性，为立体系统的安全评估提供了新工具。

Abstract: Stereo Depth estimation is a critical task in autonomous driving and
robotics, where inaccuracies (such as misidentifying nearby objects as distant)
can lead to dangerous situations. Adversarial attacks against stereo depth
estimation can help reveal vulnerabilities before deployment. Previous work has
shown that repeating optimized textures can effectively mislead stereo depth
estimation in digital settings. However, our research reveals that these
naively repeated texture structures perform poorly in physical-world
implementations, i.e., when deployed as patches, limiting their practical
utility for testing stereo depth estimation systems. In this work, for the
first time, we discover that introducing regular intervals between repeated
textures, creating a striped structure, significantly enhances the patch attack
effectiveness. Through extensive experimentation, we analyze how variations of
this novel structure influence the performance. Based on these insights, we
develop a novel stereo depth attack that jointly optimizes both the striped
structure and texture elements. Our generated adversarial patches can be
inserted into any scenes and successfully attack state-of-the-art stereo depth
estimation methods, i.e., RAFT-Stereo and STTR. Most critically, our patch can
also attack commercial RGB-D cameras (Intel RealSense) in real-world
conditions, demonstrating their practical relevance for security assessment of
stereo systems.

</details>


### [190] [LaVi: Efficient Large Vision-Language Models via Internal Feature Modulation](https://arxiv.org/abs/2506.16691)
*Tongtian Yue,Longteng Guo,Yepeng Tang,Zijia Zhao,Xinxin Zhu,Hua Huang,Jing Liu*

Main category: cs.CV

TL;DR: LaVi提出了一种新型的大视觉语言模型，通过内部特征调制实现高效视觉语言融合，显著提升了计算效率和性能。


<details>
  <summary>Details</summary>
Motivation: 现有的大视觉语言模型在视觉语言融合上效率低下，影响了可扩展性和效率。

Method: LaVi通过轻量级自适应变换，将视觉条件化的增量注入层归一化的仿射参数中，避免了长上下文扩展。

Result: LaVi在15个图像和视频基准测试中表现优异，计算成本大幅降低（FLOPs减少94.0%，推理速度提升3.1倍，内存使用减半）。

Conclusion: LaVi是一种可扩展且实用的实时多模态推理解决方案。

Abstract: Despite the impressive advancements of Large Vision-Language Models (LVLMs),
existing approaches suffer from a fundamental bottleneck: inefficient
visual-language integration. Current methods either disrupt the model's
inherent structure or introduce severe long-context computational burden,
severely limiting scalability and efficiency. In this paper, we rethink
multimodal integration and present LaVi, a novel LVLM that enables seamless and
efficient vision-language fusion through internal feature modulation within the
Large Language Models (LLMs). Unlike dominant LVLMs that rely on visual token
concatenation, LaVi bypasses long-context expansion by introducing a
lightweight and adaptive transformation, which incorporates visual context by
injecting token-wise vision-conditioned deltas into the affine parameters of
layer normalization. This mechanism directly modulates linguistic hidden states
based on visual input, ensuring precise vision-language alignment while
preserving the LLM's linguistic priors and drastically reducing computational
costs. Extensive evaluations across 15 image and video benchmarks demonstrate
that LaVi not only achieves state-of-the-art multimodal performance but also
dramatically enhances efficiency. Compared to LLaVA-OV-7B, LaVi reduces FLOPs
by 94.0%, improves inference speed by 3.1 times, and cuts memory usage in half
- establishing LaVi as a scalable and practical solution for real-time
multimodal reasoning. The code and models will be released soon.

</details>


### [191] [Language-driven Description Generation and Common Sense Reasoning for Video Action Recognition](https://arxiv.org/abs/2506.16701)
*Xiaodan Hu,Chuhang Zou,Suchen Wang,Jaechul Kim,Narendra Ahuja*

Main category: cs.CV

TL;DR: 论文提出了一种结合语言驱动常识先验的框架，用于识别单目视角下遮挡严重的视频动作序列，包括视频上下文总结、描述生成和多模态活动识别模块。


<details>
  <summary>Details</summary>
Motivation: 现有视频动作识别方法未充分利用语言模型中的常识先验（如场景上下文），而这些先验对人类理解物体、人-物交互和活动至关重要。

Method: 1. 视频上下文总结组件生成候选对象、活动及其交互；2. 描述生成模块通过辅助提示和常识推理描述场景并推断后续活动；3. 多模态活动识别头结合视觉和文本线索识别动作。

Result: 在Action Genome和Charades数据集上验证了方法的有效性。

Conclusion: 通过引入语言驱动的常识先验，显著提升了遮挡严重视频动作序列的识别性能。

Abstract: Recent video action recognition methods have shown excellent performance by
adapting large-scale pre-trained language-image models to the video domain.
However, language models contain rich common sense priors - the scene contexts
that humans use to constitute an understanding of objects, human-object
interactions, and activities - that have not been fully exploited. In this
paper, we introduce a framework incorporating language-driven common sense
priors to identify cluttered video action sequences from monocular views that
are often heavily occluded. We propose: (1) A video context summary component
that generates candidate objects, activities, and the interactions between
objects and activities; (2) A description generation module that describes the
current scene given the context and infers subsequent activities, through
auxiliary prompts and common sense reasoning; (3) A multi-modal activity
recognition head that combines visual and textual cues to recognize video
actions. We demonstrate the effectiveness of our approach on the challenging
Action Genome and Charades datasets.

</details>


### [192] [Few-Shot Generalized Category Discovery With Retrieval-Guided Decision Boundary Enhancement](https://arxiv.org/abs/2506.16728)
*Yunhan Ren,Feng Luo,Siyu Huang*

Main category: cs.CV

TL;DR: 论文提出了一种Few-shot Generalized Category Discovery (FSGCD)任务，并设计了一个基于决策边界增强和亲和力检索的框架，以在已知信息稀缺的情况下提升GCD任务的性能。


<details>
  <summary>Details</summary>
Motivation: 现有GCD模型在有限标注样本和少量已知类别下的性能尚未充分研究，因此提出了FSGCD任务。

Method: 采用决策边界预训练模块和两阶段检索引导的决策边界优化策略，通过亲和力检索伪标注样本增强已知边界。

Result: 在六个公开GCD基准测试中，该方法优于现有方法。

Conclusion: 提出的框架在FSGCD任务中表现出色，代码已开源。

Abstract: While existing Generalized Category Discovery (GCD) models have achieved
significant success, their performance with limited labeled samples and a small
number of known categories remains largely unexplored. In this work, we
introduce the task of Few-shot Generalized Category Discovery (FSGCD), aiming
to achieve competitive performance in GCD tasks under conditions of known
information scarcity. To tackle this challenge, we propose a decision boundary
enhancement framework with affinity-based retrieval. Our framework is designed
to learn the decision boundaries of known categories and transfer these
boundaries to unknown categories. First, we use a decision boundary
pre-training module to mitigate the overfitting of pre-trained information on
known category boundaries and improve the learning of these decision boundaries
using labeled samples. Second, we implement a two-stage retrieval-guided
decision boundary optimization strategy. Specifically, this strategy further
enhances the severely limited known boundaries by using affinity-retrieved
pseudo-labeled samples. Then, these refined boundaries are applied to unknown
clusters via guidance from affinity-based feature retrieval. Experimental
results demonstrate that our proposed method outperforms existing methods on
six public GCD benchmarks under the FSGCD setting. The codes are available at:
https://github.com/Ryh1218/FSGCD

</details>


### [193] [RGBTrack: Fast, Robust Depth-Free 6D Pose Estimation and Tracking](https://arxiv.org/abs/2506.17119)
*Teng Guo,Jingjin Yu*

Main category: cs.CV

TL;DR: RGBTrack是一种仅依赖RGB数据的实时6D姿态估计与跟踪框架，无需深度输入，通过结合二进制搜索策略和渲染比较机制，实现高效深度推断和姿态假设生成。


<details>
  <summary>Details</summary>
Motivation: 解决动态场景中（如快速运动和遮挡）的精确物体姿态跟踪问题，同时避免对深度数据的依赖。

Method: 基于FoundationPose架构，结合二进制搜索策略、渲染比较机制、2D物体跟踪（XMem）、卡尔曼滤波和状态机，实现动态场景下的稳定跟踪。

Result: 在基准数据集上表现出竞争性精度和实时性能，适用于机器人、增强现实和计算机视觉等领域。

Conclusion: RGBTrack是一种无需深度输入的实用解决方案，具有广泛的应用潜力。

Abstract: We introduce a robust framework, RGBTrack, for real-time 6D pose estimation
and tracking that operates solely on RGB data, thereby eliminating the need for
depth input for such dynamic and precise object pose tracking tasks. Building
on the FoundationPose architecture, we devise a novel binary search strategy
combined with a render-and-compare mechanism to efficiently infer depth and
generate robust pose hypotheses from true-scale CAD models. To maintain stable
tracking in dynamic scenarios, including rapid movements and occlusions,
RGBTrack integrates state-of-the-art 2D object tracking (XMem) with a Kalman
filter and a state machine for proactive object pose recovery. In addition,
RGBTrack's scale recovery module dynamically adapts CAD models of unknown scale
using an initial depth estimate, enabling seamless integration with modern
generative reconstruction techniques. Extensive evaluations on benchmark
datasets demonstrate that RGBTrack's novel depth-free approach achieves
competitive accuracy and real-time performance, making it a promising practical
solution candidate for application areas including robotics, augmented reality,
and computer vision.
  The source code for our implementation will be made publicly available at
https://github.com/GreatenAnoymous/RGBTrack.git.

</details>


### [194] [TeSG: Textual Semantic Guidance for Infrared and Visible Image Fusion](https://arxiv.org/abs/2506.16730)
*Mingrui Zhu,Xiru Chen,Xin Wei,Nannan Wang,Xinbo Gao*

Main category: cs.CV

TL;DR: 提出了一种基于文本语义引导的红外与可见光图像融合方法（TeSG），通过多级语义信息优化融合过程，提升下游任务性能。


<details>
  <summary>Details</summary>
Motivation: 现有文本引导的红外与可见光图像融合方法对文本语义信息的利用不足，需更有效整合语义信息以优化融合结果。

Method: TeSG包含语义信息生成器（SIG）、掩码引导交叉注意力（MGCA）模块和文本驱动注意力融合（TDAF）模块，分层次利用文本语义指导融合。

Result: 实验表明，TeSG在下游任务（如检测和分割）中性能优于现有方法。

Conclusion: TeSG通过多级文本语义引导，有效提升了红外与可见光图像融合的质量和实用性。

Abstract: Infrared and visible image fusion (IVF) aims to combine complementary
information from both image modalities, producing more informative and
comprehensive outputs. Recently, text-guided IVF has shown great potential due
to its flexibility and versatility. However, the effective integration and
utilization of textual semantic information remains insufficiently studied. To
tackle these challenges, we introduce textual semantics at two levels: the mask
semantic level and the text semantic level, both derived from textual
descriptions extracted by large Vision-Language Models (VLMs). Building on
this, we propose Textual Semantic Guidance for infrared and visible image
fusion, termed TeSG, which guides the image synthesis process in a way that is
optimized for downstream tasks such as detection and segmentation.
Specifically, TeSG consists of three core components: a Semantic Information
Generator (SIG), a Mask-Guided Cross-Attention (MGCA) module, and a Text-Driven
Attentional Fusion (TDAF) module. The SIG generates mask and text semantics
based on textual descriptions. The MGCA module performs initial attention-based
fusion of visual features from both infrared and visible images, guided by mask
semantics. Finally, the TDAF module refines the fusion process with gated
attention driven by text semantics. Extensive experiments demonstrate the
competitiveness of our approach, particularly in terms of performance on
downstream tasks, compared to existing state-of-the-art methods.

</details>


### [195] [Part$^{2}$GS: Part-aware Modeling of Articulated Objects using 3D Gaussian Splatting](https://arxiv.org/abs/2506.17212)
*Tianjiao Yu,Vedant Shah,Muntasir Wahed,Ying Shen,Kiet A. Nguyen,Ismini Lourentzou*

Main category: cs.CV

TL;DR: Part$^{2}$GS是一种新型框架，用于建模多部分物体的高保真几何和物理一致性的数字孪生。


<details>
  <summary>Details</summary>
Motivation: 现实世界中的关节物体建模仍是一个挑战，现有方法难以实现高保真几何和物理一致的运动。

Method: 采用部分感知的3D高斯表示，结合物理约束（如接触强制、速度一致性和矢量场对齐）和排斥点场以防止碰撞。

Result: 在合成和真实数据集上，Part$^{2}$GS在可移动部分的Chamfer Distance上比现有方法提升高达10倍。

Conclusion: Part$^{2}$GS在关节物体建模中实现了高保真几何和物理一致性，显著优于现有方法。

Abstract: Articulated objects are common in the real world, yet modeling their
structure and motion remains a challenging task for 3D reconstruction methods.
In this work, we introduce Part$^{2}$GS, a novel framework for modeling
articulated digital twins of multi-part objects with high-fidelity geometry and
physically consistent articulation. Part$^{2}$GS leverages a part-aware 3D
Gaussian representation that encodes articulated components with learnable
attributes, enabling structured, disentangled transformations that preserve
high-fidelity geometry. To ensure physically consistent motion, we propose a
motion-aware canonical representation guided by physics-based constraints,
including contact enforcement, velocity consistency, and vector-field
alignment. Furthermore, we introduce a field of repel points to prevent part
collisions and maintain stable articulation paths, significantly improving
motion coherence over baselines. Extensive evaluations on both synthetic and
real-world datasets show that Part$^{2}$GS consistently outperforms
state-of-the-art methods by up to 10$\times$ in Chamfer Distance for movable
parts.

</details>


### [196] [3DeepRep: 3D Deep Low-rank Tensor Representation for Hyperspectral Image Inpainting](https://arxiv.org/abs/2506.16735)
*Yunshan Li,Wenwu Gong,Qianqian Wang,Chao Wang,Lili Yang*

Main category: cs.CV

TL;DR: 提出了一种新型3方向深度低秩张量表示模型（3DeepRep），通过在所有三个HSI张量模式上执行深度非线性变换，显著提升了高光谱图像修复性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法通常仅限制变换在光谱模式上，忽略了其他张量模式的低秩特性。

Method: 提出3DeepRep模型，通过3方向TNN正则化在潜在空间中最小化模式i正面切片的核范数，并通过可学习聚合模块融合结果。

Result: 在真实HSI数据集上的实验表明，该方法在定性和定量上均优于现有技术。

Conclusion: 3DeepRep模型通过多方向深度变换和低秩约束，显著提升了高光谱图像修复效果。

Abstract: Recent approaches based on transform-based tensor nuclear norm (TNN) have
demonstrated notable effectiveness in hyperspectral image (HSI) inpainting by
leveraging low-rank structures in latent representations. Recent developments
incorporate deep transforms to improve low-rank tensor representation; however,
existing approaches typically restrict the transform to the spectral mode,
neglecting low-rank properties along other tensor modes. In this paper, we
propose a novel 3-directional deep low-rank tensor representation (3DeepRep)
model, which performs deep nonlinear transforms along all three modes of the
HSI tensor. To enforce low-rankness, the model minimizes the nuclear norms of
mode-i frontal slices in the corresponding latent space for each direction
(i=1,2,3), forming a 3-directional TNN regularization. The outputs from the
three directional branches are subsequently fused via a learnable aggregation
module to produce the final result. An efficient gradient-based optimization
algorithm is developed to solve the model in a self-supervised manner.
Extensive experiments on real-world HSI datasets demonstrate that the proposed
method achieves superior inpainting performance compared to existing
state-of-the-art techniques, both qualitatively and quantitatively.

</details>


### [197] [Long-term Traffic Simulation with Interleaved Autoregressive Motion and Scenario Generation](https://arxiv.org/abs/2506.17213)
*Xiuyu Yang,Shuhan Tan,Philipp Krähenbühl*

Main category: cs.CV

TL;DR: InfGen是一个统一的下一代预测模型，用于交替进行闭环运动模拟和场景生成，显著提升了长期交通模拟的稳定性。


<details>
  <summary>Details</summary>
Motivation: 现有模型和基准主要关注场景中初始代理的闭环运动模拟，无法满足长期模拟需求，因为代理会随着车辆进入新区域而动态变化。

Method: 提出InfGen模型，通过交替进行闭环运动模拟和场景生成，实现长期稳定的交通模拟。

Result: InfGen在短期（9秒）交通模拟中达到最先进水平，在长期（30秒）模拟中显著优于其他方法。

Conclusion: InfGen为长期交通模拟提供了有效的解决方案，代码和模型将公开。

Abstract: An ideal traffic simulator replicates the realistic long-term point-to-point
trip that a self-driving system experiences during deployment. Prior models and
benchmarks focus on closed-loop motion simulation for initial agents in a
scene. This is problematic for long-term simulation. Agents enter and exit the
scene as the ego vehicle enters new regions. We propose InfGen, a unified
next-token prediction model that performs interleaved closed-loop motion
simulation and scene generation. InfGen automatically switches between
closed-loop motion simulation and scene generation mode. It enables stable
long-term rollout simulation. InfGen performs at the state-of-the-art in
short-term (9s) traffic simulation, and significantly outperforms all other
methods in long-term (30s) simulation. The code and model of InfGen will be
released at https://orangesodahub.github.io/InfGen

</details>


### [198] [Cross-modal Offset-guided Dynamic Alignment and Fusion for Weakly Aligned UAV Object Detection](https://arxiv.org/abs/2506.16737)
*Liu Zongzhen,Luo Hui,Wang Zhixing,Wei Yuxing,Zuo Haorui,Zhang Jianlin*

Main category: cs.CV

TL;DR: 论文提出CoDAF框架，通过联合解决弱对齐问题，提升无人机多模态目标检测的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 无人机多模态目标检测中，RGB和红外图像的空间错位导致语义不一致和模态冲突，现有方法单独处理这些问题效果有限。

Method: 提出CoDAF框架，包含OSA模块（基于偏移的语义对齐）和DAFM模块（动态注意力引导的融合），联合处理对齐与融合。

Result: 在DroneVehicle数据集上，CoDAF达到78.6%的mAP。

Conclusion: CoDAF通过统一框架有效解决了弱对齐问题，提升了无人机目标检测的性能。

Abstract: Unmanned aerial vehicle (UAV) object detection plays a vital role in
applications such as environmental monitoring and urban security. To improve
robustness, recent studies have explored multimodal detection by fusing visible
(RGB) and infrared (IR) imagery. However, due to UAV platform motion and
asynchronous imaging, spatial misalignment frequently occurs between
modalities, leading to weak alignment. This introduces two major challenges:
semantic inconsistency at corresponding spatial locations and modality conflict
during feature fusion. Existing methods often address these issues in
isolation, limiting their effectiveness. In this paper, we propose Cross-modal
Offset-guided Dynamic Alignment and Fusion (CoDAF), a unified framework that
jointly tackles both challenges in weakly aligned UAV-based object detection.
CoDAF comprises two novel modules: the Offset-guided Semantic Alignment (OSA),
which estimates attention-based spatial offsets and uses deformable convolution
guided by a shared semantic space to align features more precisely; and the
Dynamic Attention-guided Fusion Module (DAFM), which adaptively balances
modality contributions through gating and refines fused features via
spatial-channel dual attention. By integrating alignment and fusion in a
unified design, CoDAF enables robust UAV object detection. Experiments on
standard benchmarks validate the effectiveness of our approach, with CoDAF
achieving a mAP of 78.6% on the DroneVehicle dataset.

</details>


### [199] [Uncertainty-Aware Variational Information Pursuit for Interpretable Medical Image Analysis](https://arxiv.org/abs/2506.16742)
*Md Nahiduzzaman,Ruwan Tennakoon,Steven Korevaar,Zongyuan Ge,Alireza Bab-Hadiashar*

Main category: cs.CV

TL;DR: 论文提出了一种不确定性感知的V-IP框架（UAV-IP），在医疗影像中提升模型解释性和准确性。


<details>
  <summary>Details</summary>
Motivation: 现有V-IP方法忽略查询-答案生成中的实例级不确定性，影响模型可靠性和临床决策。

Method: 引入UAV-IP框架，集成不确定性量化，评估于四个医疗影像数据集。

Result: AUC平均提升3.2%，解释更简洁20%，且信息量不损失。

Conclusion: 不确定性感知对可解释模型在医疗决策中的稳健性和可靠性至关重要。

Abstract: In medical imaging, AI decision-support systems must balance accuracy and
interpretability to build user trust and support effective clinical
decision-making. Recently, Variational Information Pursuit (V-IP) and its
variants have emerged as interpretable-by-design modeling techniques, aiming to
explain AI decisions in terms of human-understandable, clinically relevant
concepts. However, existing V-IP methods overlook instance-level uncertainties
in query-answer generation, which can arise from model limitations (epistemic
uncertainty) or variability in expert responses (aleatoric uncertainty).
  This paper introduces Uncertainty-Aware V-IP (UAV-IP), a novel framework that
integrates uncertainty quantification into the V-IP process. We evaluate UAV-IP
across four medical imaging datasets, PH2, Derm7pt, BrEaST, and SkinCon,
demonstrating an average AUC improvement of approximately 3.2% while generating
20% more concise explanations compared to baseline V-IP, without sacrificing
informativeness. These findings highlight the importance of uncertainty-aware
reasoning in interpretable by design models for robust and reliable medical
decision-making.

</details>


### [200] [Noise-Informed Diffusion-Generated Image Detection with Anomaly Attention](https://arxiv.org/abs/2506.16743)
*Weinan Guan,Wei Wang,Bo Peng,Ziwen He,Jing Dong,Haonan Cheng*

Main category: cs.CV

TL;DR: 论文提出了一种基于噪声感知自注意力模块（NASA）的检测方法，用于识别扩散模型生成的图像，并在未见过的生成方法上取得了最佳性能。


<details>
  <summary>Details</summary>
Motivation: 随着扩散模型生成图像质量的提升，信息安全的担忧增加，需要一种能泛化到未见过扩散模型的检测方法。

Method: 通过分析图像噪声模式，引入NASA模块并结合Swin Transformer，提出NASA-Swin架构，并采用跨模态融合和通道掩码策略。

Result: 实验表明，该方法在检测扩散生成图像方面表现优异，尤其在未见过生成方法时达到SOTA性能。

Conclusion: NASA-Swin通过噪声分析和跨模态学习，有效提升了扩散生成图像的检测能力。

Abstract: With the rapid development of image generation technologies, especially the
advancement of Diffusion Models, the quality of synthesized images has
significantly improved, raising concerns among researchers about information
security. To mitigate the malicious abuse of diffusion models,
diffusion-generated image detection has proven to be an effective
countermeasure.However, a key challenge for forgery detection is generalising
to diffusion models not seen during training. In this paper, we address this
problem by focusing on image noise. We observe that images from different
diffusion models share similar noise patterns, distinct from genuine images.
Building upon this insight, we introduce a novel Noise-Aware Self-Attention
(NASA) module that focuses on noise regions to capture anomalous patterns. To
implement a SOTA detection model, we incorporate NASA into Swin Transformer,
forming an novel detection architecture NASA-Swin. Additionally, we employ a
cross-modality fusion embedding to combine RGB and noise images, along with a
channel mask strategy to enhance feature learning from both modalities.
Extensive experiments demonstrate the effectiveness of our approach in
enhancing detection capabilities for diffusion-generated images. When
encountering unseen generation methods, our approach achieves the
state-of-the-art performance.Our code is available at
https://github.com/WeinanGuan/NASA-Swin.

</details>


### [201] [Infrared and Visible Image Fusion Based on Implicit Neural Representations](https://arxiv.org/abs/2506.16773)
*Shuchen Sun,Ligen Shi,Chang Liu,Lina Wu,Jun Qiu*

Main category: cs.CV

TL;DR: 本文提出了一种基于隐式神经表示（INR）的红外与可见光图像融合方法INRFuse，通过神经网络参数化连续函数，自适应融合多模态信息，支持多分辨率图像融合与超分辨率重建，无需训练数据集，效果优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 红外与可见光图像融合旨在结合两种模态的优势，生成信息丰富且满足视觉或计算需求的图像。传统方法依赖离散像素或显式特征，限制了融合效果。

Method: 利用归一化空间坐标作为输入，通过多层感知机自适应融合红外与可见光图像的特征，设计多损失函数联合优化融合图像与原图的相似性。

Result: 实验表明，INRFuse在主观视觉质量和客观评价指标上均优于现有方法，融合图像结构清晰、细节自然、信息丰富。

Conclusion: INRFuse通过隐式神经表示突破了传统方法的限制，实现了高效的多模态图像融合，支持多分辨率融合与超分辨率重建。

Abstract: Infrared and visible light image fusion aims to combine the strengths of both
modalities to generate images that are rich in information and fulfill visual
or computational requirements. This paper proposes an image fusion method based
on Implicit Neural Representations (INR), referred to as INRFuse. This method
parameterizes a continuous function through a neural network to implicitly
represent the multimodal information of the image, breaking through the
traditional reliance on discrete pixels or explicit features. The normalized
spatial coordinates of the infrared and visible light images serve as inputs,
and multi-layer perceptrons is utilized to adaptively fuse the features of both
modalities, resulting in the output of the fused image. By designing multiple
loss functions, the method jointly optimizes the similarity between the fused
image and the original images, effectively preserving the thermal radiation
information of the infrared image while maintaining the texture details of the
visible light image. Furthermore, the resolution-independent characteristic of
INR allows for the direct fusion of images with varying resolutions and
achieves super-resolution reconstruction through high-density coordinate
queries. Experimental results indicate that INRFuse outperforms existing
methods in both subjective visual quality and objective evaluation metrics,
producing fused images with clear structures, natural details, and rich
information without the necessity for a training dataset.

</details>


### [202] [PQCAD-DM: Progressive Quantization and Calibration-Assisted Distillation for Extremely Efficient Diffusion Model](https://arxiv.org/abs/2506.16776)
*Beomseok Ko,Hyeryung Jang*

Main category: cs.CV

TL;DR: PQCAD-DM是一种结合渐进量化（PQ）和校准辅助蒸馏（CAD）的混合压缩框架，用于解决扩散模型的计算和资源消耗问题，同时保持生成质量。


<details>
  <summary>Details</summary>
Motivation: 扩散模型在图像生成中表现出色，但依赖迭代马尔可夫链过程导致计算和资源消耗大，且误差累积限制了压缩技术的效果。

Method: PQ采用两阶段量化，通过动量机制自适应调整位宽；CAD利用全精度校准数据集在蒸馏过程中提升量化模型的性能。

Result: PQCAD-DM在计算效率和生成质量之间取得平衡，推理时间减半，性能仍具竞争力。

Conclusion: PQCAD-DM在多种数据集上验证了其优越的生成能力和效率，优于固定位量化方法。

Abstract: Diffusion models excel in image generation but are computational and
resource-intensive due to their reliance on iterative Markov chain processes,
leading to error accumulation and limiting the effectiveness of naive
compression techniques. In this paper, we propose PQCAD-DM, a novel hybrid
compression framework combining Progressive Quantization (PQ) and
Calibration-Assisted Distillation (CAD) to address these challenges. PQ employs
a two-stage quantization with adaptive bit-width transitions guided by a
momentum-based mechanism, reducing excessive weight perturbations in
low-precision. CAD leverages full-precision calibration datasets during
distillation, enabling the student to match full-precision performance even
with a quantized teacher. As a result, PQCAD-DM achieves a balance between
computational efficiency and generative quality, halving inference time while
maintaining competitive performance. Extensive experiments validate PQCAD-DM's
superior generative capabilities and efficiency across diverse datasets,
outperforming fixed-bit quantization methods.

</details>


### [203] [RealSR-R1: Reinforcement Learning for Real-World Image Super-Resolution with Vision-Language Chain-of-Thought](https://arxiv.org/abs/2506.16796)
*Junbo Qiao,Miaomiao Cai,Wei Li,Yutong Liu,Xudong Huang,Gaoqi He,Jiao Xie,Jie Hu,Xinghao Chen,Shaohui Lin*

Main category: cs.CV

TL;DR: 论文提出RealSR-R1方法，结合视觉与语言推理（VLCoT框架）和GRPO优化，提升真实世界图像超分辨率的细节恢复和内容理解能力。


<details>
  <summary>Details</summary>
Motivation: 现有方法在真实世界图像超分辨率任务中对退化图像内容理解不足，导致重建结果低保真且不自然。

Method: 提出VLCoT框架模拟人类处理退化图像的过程，结合GRPO优化，设计四种奖励函数（格式、退化、理解和生成奖励）以提升性能。

Result: 实验表明RealSR-R1能生成更真实的细节，尤其在语义丰富或严重退化的场景中表现优异。

Conclusion: RealSR-R1通过结合视觉语言推理和GRPO优化，显著提升了真实世界图像超分辨率的性能。

Abstract: Real-World Image Super-Resolution is one of the most challenging task in
image restoration. However, existing methods struggle with an accurate
understanding of degraded image content, leading to reconstructed results that
are both low-fidelity and unnatural. We present RealSR-R1 in this work, which
empowers the RealSR models with understanding and reasoning capabilities.
Inspired by the success of Chain of Thought (CoT) in large language models
(LLMs), we simulate the human process of handling degraded images and propose
the VLCoT framework, which integrates vision and language reasoning. The
framework aims to precisely restore image details by progressively generating
more comprehensive text and higher-resolution images. To overcome the challenge
of traditional supervised learning CoT failing to generalize to real-world
scenarios, we introduce, for the first time, Group Relative Policy Optimization
(GRPO) into the Real-World Image Super-Resolution task. We propose VLCoT-GRPO
as a solution, which designs four reward functions: (1) Format reward, used to
standardize the CoT process; (2) Degradation reward, to incentivize accurate
degradation estimation; (3) Understanding reward, to ensure the accuracy of the
generated content; and (4) Generation reward, where we propose using a visual
expert model to evaluate the quality of generated images, encouraging the model
to generate more realistic images. Extensive experiments demonstrate that our
proposed RealSR-R1 can generate realistic details and accurately understand
image content, particularly in semantically rich scenes or images with severe
degradation.

</details>


### [204] [Seeing What Matters: Generalizable AI-generated Video Detection with Forensic-Oriented Augmentation](https://arxiv.org/abs/2506.16802)
*Riccardo Corvi,Davide Cozzolino,Ekta Prashnani,Shalini De Mello,Koki Nagano,Luisa Verdoliva*

Main category: cs.CV

TL;DR: 论文提出了一种新的视频伪造检测方法，通过关注生成模型引入的低级伪影而非高级语义缺陷，提高了检测器的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有视频伪造检测器泛化能力差，限制了实际应用。

Method: 研究不同生成架构，识别共享特征；提出基于小波分解的数据增强策略，引导模型关注更相关的伪造线索。

Result: 新方法显著提高了检测器的准确率，即使对最新生成模型（如NOVA和FLUX）也表现优异。

Conclusion: 该方法简单有效，无需复杂算法或大数据集，显著提升了AI生成视频检测的泛化能力。

Abstract: Synthetic video generation is progressing very rapidly. The latest models can
produce very realistic high-resolution videos that are virtually
indistinguishable from real ones. Although several video forensic detectors
have been recently proposed, they often exhibit poor generalization, which
limits their applicability in a real-world scenario. Our key insight to
overcome this issue is to guide the detector towards seeing what really
matters. In fact, a well-designed forensic classifier should focus on
identifying intrinsic low-level artifacts introduced by a generative
architecture rather than relying on high-level semantic flaws that characterize
a specific model. In this work, first, we study different generative
architectures, searching and identifying discriminative features that are
unbiased, robust to impairments, and shared across models. Then, we introduce a
novel forensic-oriented data augmentation strategy based on the wavelet
decomposition and replace specific frequency-related bands to drive the model
to exploit more relevant forensic cues. Our novel training paradigm improves
the generalizability of AI-generated video detectors, without the need for
complex algorithms and large datasets that include multiple synthetic
generators. To evaluate our approach, we train the detector using data from a
single generative model and test it against videos produced by a wide range of
other models. Despite its simplicity, our method achieves a significant
accuracy improvement over state-of-the-art detectors and obtains excellent
results even on very recent generative models, such as NOVA and FLUX. Code and
data will be made publicly available.

</details>


### [205] [Co-VisiON: Co-Visibility ReasONing on Sparse Image Sets of Indoor Scenes](https://arxiv.org/abs/2506.16805)
*Chao Chen,Nobel Dang,Juexiao Zhang,Wenkai Sun,Pengfei Zheng,Xuhang He,Yimeng Ye,Taarun Srinivas,Chen Feng*

Main category: cs.CV

TL;DR: 论文提出了Co-VisiON基准，评估稀疏图像集的共视性推理能力，发现现有视觉模型表现不佳，需高层次多视角推理。


<details>
  <summary>Details</summary>
Motivation: 人类在稀疏图像中识别共视性的能力是3D视觉和机器人感知的基础，但现有视觉模型是否达到人类水平尚不明确。

Method: 引入Co-VisiON基准，评估1000多个室内场景的稀疏图像共视性推理，并提出多视角基线模型Covis。

Result: 专有视觉语言模型表现最佳，但所有模型远落后于人类，Covis在纯视觉模型中表现最优。

Conclusion: 需高层次多视角推理提升稀疏环境下的共视性分析，基准和发现将推动视觉模型发展。

Abstract: Humans exhibit a remarkable ability to recognize co-visibility-the
overlapping regions visible in multiple images-even when these images are
sparsely distributed across a complex scene. This capability is foundational in
3D vision and robotic perception. Despite significant progress in vision
learning, it remains unclear whether current vision models have reached
human-level proficiency in co-visibility analysis. In this work, we introduce
the Co-Visibility reasONing (Co-VisiON) benchmark, designed to directly
evaluate co-visibility reasoning on sparse image sets across over 1000 indoor
scenarios. Our experiments reveal that while co-visibility is typically treated
as a low-level feature matching task, it poses a significant challenge for
existing vision models under sparse conditions. Notably, a proprietary
vision-language model outperforms all purely vision-based approaches, with all
models lagging substantially behind human performance. This gap underscores the
need for more than basic pairwise vision processing-it calls for a
comprehensive spatial understanding through high-level reasoning across
multiple views. Inspired by human visual cognition, we propose a novel
multi-view baseline, Covis, which achieves top performance among pure vision
models and narrows the gap to the proprietary VLM. We hope our benchmark and
findings will spur further advancements in developing vision models capable of
robust, high-level reasoning in challenging, sparse environments. Our dataset
and source code can be found at: https://ai4ce.github.io/CoVISION

</details>


### [206] [FOCUS: Unified Vision-Language Modeling for Interactive Editing Driven by Referential Segmentation](https://arxiv.org/abs/2506.16806)
*Fan Yang,Yousong Zhu,Xin Li,Yufei Zhan,Hongyin Zhao,Shurong Zheng,Yaowei Wang,Ming Tang,Jinqiao Wang*

Main category: cs.CV

TL;DR: FOCUS是一个统一的LVLM模型，整合了分割感知和可控对象生成，通过端到端框架实现视觉感知与生成的联合优化。


<details>
  <summary>Details</summary>
Motivation: 当前LVLMs将视觉理解与编辑分离，依赖多个独立模型，FOCUS旨在填补这一空白。

Method: 采用双分支视觉编码器捕获全局语义和空间细节，结合MoVQGAN视觉标记器和多阶段训练管道。

Result: 在多项任务中表现优异，包括多模态理解、参考分割和可控图像生成。

Conclusion: FOCUS通过联合优化视觉感知与生成能力，显著提升了性能。

Abstract: Recent Large Vision Language Models (LVLMs) demonstrate promising
capabilities in unifying visual understanding and generative modeling, enabling
both accurate content understanding and flexible editing. However, current
approaches treat "what to see" and "how to edit" separately: they either
perform isolated object segmentation or utilize segmentation masks merely as
conditional prompts for local edit generation tasks, often relying on multiple
disjointed models. To bridge these gaps, we introduce FOCUS, a unified LVLM
that integrates segmentation-aware perception and controllable object-centric
generation within an end-to-end framework. FOCUS employs a dual-branch visual
encoder to simultaneously capture global semantic context and fine-grained
spatial details. In addition, we leverage a MoVQGAN-based visual tokenizer to
produce discrete visual tokens that enhance generation quality. To enable
accurate and controllable image editing, we propose a progressive multi-stage
training pipeline, where segmentation masks are jointly optimized and used as
spatial condition prompts to guide the diffusion decoder. This strategy aligns
visual encoding, segmentation, and generation modules, effectively bridging
segmentation-aware perception with fine-grained visual synthesis. Extensive
experiments across three core tasks, including multimodal understanding,
referring segmentation accuracy, and controllable image generation, demonstrate
that FOCUS achieves strong performance by jointly optimizing visual perception
and generative capabilities.

</details>


### [207] [Loupe: A Generalizable and Adaptive Framework for Image Forgery Detection](https://arxiv.org/abs/2506.16819)
*Yuchu Jiang,Jiaming Chu,Jian Zhao,Xin Zhang,Xu Yang,Lei Jin,Chi Zhang,Xuelong Li*

Main category: cs.CV

TL;DR: Loupe是一个轻量级框架，用于联合深度伪造检测和定位，通过集成补丁感知分类器和分割模块，实现全局分类和细粒度掩码预测，并在测试时引入伪标签机制提升鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 生成模型的普及引发了视觉内容伪造的严重问题，现有方法在泛化性和架构复杂性上存在局限。

Method: Loupe结合补丁感知分类器和条件查询的分割模块，同时支持全局分类和掩码预测，并引入伪标签测试时适应机制。

Result: 在DDL数据集上表现优异，IJCAI 2025挑战赛中得分0.846，验证了补丁级融合和条件查询设计的有效性。

Conclusion: Loupe在分类精度和空间定位上表现优异，适用于多样伪造模式。

Abstract: The proliferation of generative models has raised serious concerns about
visual content forgery. Existing deepfake detection methods primarily target
either image-level classification or pixel-wise localization. While some
achieve high accuracy, they often suffer from limited generalization across
manipulation types or rely on complex architectures. In this paper, we propose
Loupe, a lightweight yet effective framework for joint deepfake detection and
localization. Loupe integrates a patch-aware classifier and a segmentation
module with conditional queries, allowing simultaneous global authenticity
classification and fine-grained mask prediction. To enhance robustness against
distribution shifts of test set, Loupe introduces a pseudo-label-guided
test-time adaptation mechanism by leveraging patch-level predictions to
supervise the segmentation head. Extensive experiments on the DDL dataset
demonstrate that Loupe achieves state-of-the-art performance, securing the
first place in the IJCAI 2025 Deepfake Detection and Localization Challenge
with an overall score of 0.846. Our results validate the effectiveness of the
proposed patch-level fusion and conditional query design in improving both
classification accuracy and spatial localization under diverse forgery
patterns. The code is available at https://github.com/Kamichanw/Loupe.

</details>


### [208] [Self-supervised Feature Extraction for Enhanced Ball Detection on Soccer Robots](https://arxiv.org/abs/2506.16821)
*Can Lin,Daniele Affinita,Marco E. P. Zimmatore,Daniele Nardi,Domenico D. Bloisi,Vincenzo Suriani*

Main category: cs.CV

TL;DR: 提出了一种自监督学习框架，用于提升足球机器人在动态环境中的球检测性能，无需大量手动标注。


<details>
  <summary>Details</summary>
Motivation: 传统监督方法需要大量手动标注，成本高且耗时。

Method: 利用预训练模型生成伪标签，结合自监督任务（如着色、边缘检测和三元组损失）学习鲁棒特征，并引入MAML策略快速适应新场景。

Result: 实验表明，该方法在准确性、F1分数和IoU上优于基线模型，且收敛更快。

Conclusion: 提出的自监督框架有效提升了球检测性能，适用于动态环境。

Abstract: Robust and accurate ball detection is a critical component for autonomous
humanoid soccer robots, particularly in dynamic and challenging environments
such as RoboCup outdoor fields. However, traditional supervised approaches
require extensive manual annotation, which is costly and time-intensive. To
overcome this problem, we present a self-supervised learning framework for
domain-adaptive feature extraction to enhance ball detection performance. The
proposed approach leverages a general-purpose pretrained model to generate
pseudo-labels, which are then used in a suite of self-supervised pretext tasks
-- including colorization, edge detection, and triplet loss -- to learn robust
visual features without relying on manual annotations. Additionally, a
model-agnostic meta-learning (MAML) strategy is incorporated to ensure rapid
adaptation to new deployment scenarios with minimal supervision. A new dataset
comprising 10,000 labeled images from outdoor RoboCup SPL matches is
introduced, used to validate the method, and made available to the community.
Experimental results demonstrate that the proposed pipeline outperforms
baseline models in terms of accuracy, F1 score, and IoU, while also exhibiting
faster convergence.

</details>


### [209] [Controllable and Expressive One-Shot Video Head Swapping](https://arxiv.org/abs/2506.16852)
*Chaonan Ji,Jinwei Qi,Peng Zhang,Bang Zhang,Liefeng Bo*

Main category: cs.CV

TL;DR: 提出了一种基于扩散的多条件可控视频头部替换框架，支持静态图像头部动态移植到视频中，并允许调整表情和动作。


<details>
  <summary>Details</summary>
Motivation: 现有方法局限于局部面部替换或难以处理复杂背景和发型多样性，且不支持替换后表情修改。

Method: 采用统一潜在扩散范式，包括身份保留上下文融合和表情感知地标重定向与编辑。

Result: 实验显示方法在背景无缝融合、身份保留和表情转移方面表现优异。

Conclusion: 该方法在视频头部替换中实现了高质量的身份保留和表情控制。

Abstract: In this paper, we propose a novel diffusion-based multi-condition
controllable framework for video head swapping, which seamlessly transplant a
human head from a static image into a dynamic video, while preserving the
original body and background of target video, and further allowing to tweak
head expressions and movements during swapping as needed. Existing
face-swapping methods mainly focus on localized facial replacement neglecting
holistic head morphology, while head-swapping approaches struggling with
hairstyle diversity and complex backgrounds, and none of these methods allow
users to modify the transplanted head expressions after swapping. To tackle
these challenges, our method incorporates several innovative strategies through
a unified latent diffusion paradigm. 1) Identity-preserving context fusion: We
propose a shape-agnostic mask strategy to explicitly disentangle foreground
head identity features from background/body contexts, combining hair
enhancement strategy to achieve robust holistic head identity preservation
across diverse hair types and complex backgrounds. 2) Expression-aware landmark
retargeting and editing: We propose a disentangled 3DMM-driven retargeting
module that decouples identity, expression, and head poses, minimizing the
impact of original expressions in input images and supporting expression
editing. While a scale-aware retargeting strategy is further employed to
minimize cross-identity expression distortion for higher transfer precision.
Experimental results demonstrate that our method excels in seamless background
integration while preserving the identity of the source portrait, as well as
showcasing superior expression transfer capabilities applicable to both real
and virtual characters.

</details>


### [210] [ParkFormer: A Transformer-Based Parking Policy with Goal Embedding and Pedestrian-Aware Control](https://arxiv.org/abs/2506.16856)
*Jun Fu,Bin Tian,Haonan Chen,Shi Meng,Tingting Yao*

Main category: cs.CV

TL;DR: 提出了一种基于Transformer的端到端自动驾驶停车框架，通过专家演示学习，结合多模态输入和高精度控制，在模拟环境中验证了高成功率和低误差。


<details>
  <summary>Details</summary>
Motivation: 传统基于规则的停车系统难以应对环境不确定性和动态场景，而人类驾驶员能直觉停车，因此提出学习专家演示的方法。

Method: 使用Transformer框架，输入包括环视摄像头图像、目标点表示、车辆运动和行人轨迹，输出离散控制序列；引入交叉注意力模块和GRU行人预测器。

Result: 在CARLA模拟器中验证，成功率达96.57%，平均位置误差0.21米，方向误差0.41度。

Conclusion: 提出的方法在停车任务中表现优异，关键模块如行人预测和目标点注意力融合效果显著，代码和数据集将开源。

Abstract: Autonomous parking plays a vital role in intelligent vehicle systems,
particularly in constrained urban environments where high-precision control is
required. While traditional rule-based parking systems struggle with
environmental uncertainties and lack adaptability in crowded or dynamic scenes,
human drivers demonstrate the ability to park intuitively without explicit
modeling. Inspired by this observation, we propose a Transformer-based
end-to-end framework for autonomous parking that learns from expert
demonstrations. The network takes as input surround-view camera images,
goal-point representations, ego vehicle motion, and pedestrian trajectories. It
outputs discrete control sequences including throttle, braking, steering, and
gear selection. A novel cross-attention module integrates BEV features with
target points, and a GRU-based pedestrian predictor enhances safety by modeling
dynamic obstacles. We validate our method on the CARLA 0.9.14 simulator in both
vertical and parallel parking scenarios. Experiments show our model achieves a
high success rate of 96.57\%, with average positional and orientation errors of
0.21 meters and 0.41 degrees, respectively. The ablation studies further
demonstrate the effectiveness of key modules such as pedestrian prediction and
goal-point attention fusion. The code and dataset will be released at:
https://github.com/little-snail-f/ParkFormer.

</details>


### [211] [With Limited Data for Multimodal Alignment, Let the STRUCTURE Guide You](https://arxiv.org/abs/2506.16895)
*Fabian Gröger,Shuo Wen,Huyen Le,Maria Brbić*

Main category: cs.CV

TL;DR: 该论文提出了一种在有限配对数据下构建多模态模型的方法，通过对齐预训练的单模态基础模型，仅需少量数据即可实现高质量对齐。


<details>
  <summary>Details</summary>
Motivation: 现有多模态模型通常依赖大量配对数据，但在许多领域中获取这些数据成本高昂或不可行。

Method: 引入STRUCTURE正则化技术，保持单模态编码器潜在空间的邻域几何结构，并优化对齐层选择。

Result: 在24个零样本图像分类和检索基准测试中，分类任务平均提升51.6%，检索任务提升91.8%。

Conclusion: 该方法为资源受限领域提供了一种高效的多模态学习框架。

Abstract: Multimodal models have demonstrated powerful capabilities in complex tasks
requiring multimodal alignment including zero-shot classification and
cross-modal retrieval. However, existing models typically rely on millions of
paired multimodal samples, which are prohibitively expensive or infeasible to
obtain in many domains. In this work, we explore the feasibility of building
multimodal models with limited amount of paired data by aligning pretrained
unimodal foundation models. We show that high-quality alignment is possible
with as few as tens of thousands of paired samples$\unicode{x2013}$less than
$1\%$ of the data typically used in the field. To achieve this, we introduce
STRUCTURE, an effective regularization technique that preserves the
neighborhood geometry of the latent space of unimodal encoders. Additionally,
we show that aligning last layers is often suboptimal and demonstrate the
benefits of aligning the layers with the highest representational similarity
across modalities. These two components can be readily incorporated into
existing alignment methods, yielding substantial gains across 24 zero-shot
image classification and retrieval benchmarks, with average relative
improvement of $51.6\%$ in classification and $91.8\%$ in retrieval tasks. Our
results highlight the effectiveness and broad applicability of our framework
for limited-sample multimodal learning and offer a promising path forward for
resource-constrained domains.

</details>


### [212] [LunarLoc: Segment-Based Global Localization on the Moon](https://arxiv.org/abs/2506.16940)
*Annika Thomas,Robaire Galliath,Aleksander Garbuz,Luke Anger,Cormac O'Neill,Trevor Johst,Dami Thomas,George Lordos,Jonathan P. How*

Main category: cs.CV

TL;DR: 论文提出了一种名为LunarLoc的方法，用于月球表面的全局定位，解决了视觉惯性里程计（VIO）在长距离任务中累积漂移的问题。该方法利用实例分割提取地标，并通过图论数据关联实现高精度定位。


<details>
  <summary>Details</summary>
Motivation: 由于月球缺乏地球导航基础设施（如GPS），传统方法如VIO在长距离任务中会累积漂移，影响自主任务的精确性。NASA的Artemis计划需要高精度的自主操作，如挖掘和运输。

Method: LunarLoc利用实例分割从立体图像中提取地标，构建地形图，并通过图论数据关联与参考地图对齐，实现全局定位。

Result: 实验表明，LunarLoc在多会话全局定位中达到亚厘米级精度，显著优于现有方法。

Conclusion: LunarLoc为月球全局定位提供了一种高精度解决方案，并公开了数据集以促进进一步研究。

Abstract: Global localization is necessary for autonomous operations on the lunar
surface where traditional Earth-based navigation infrastructure, such as GPS,
is unavailable. As NASA advances toward sustained lunar presence under the
Artemis program, autonomous operations will be an essential component of tasks
such as robotic exploration and infrastructure deployment. Tasks such as
excavation and transport of regolith require precise pose estimation, but
proposed approaches such as visual-inertial odometry (VIO) accumulate odometry
drift over long traverses. Precise pose estimation is particularly important
for upcoming missions such as the ISRU Pilot Excavator (IPEx) that rely on
autonomous agents to operate over extended timescales and varied terrain. To
help overcome odometry drift over long traverses, we propose LunarLoc, an
approach to global localization that leverages instance segmentation for
zero-shot extraction of boulder landmarks from onboard stereo imagery. Segment
detections are used to construct a graph-based representation of the terrain,
which is then aligned with a reference map of the environment captured during a
previous session using graph-theoretic data association. This method enables
accurate and drift-free global localization in visually ambiguous settings.
LunarLoc achieves sub-cm level accuracy in multi-session global localization
experiments, significantly outperforming the state of the art in lunar global
localization. To encourage the development of further methods for global
localization on the Moon, we release our datasets publicly with a playback
module: https://github.com/mit-acl/lunarloc-data.

</details>


### [213] [LAION-C: An Out-of-Distribution Benchmark for Web-Scale Vision Models](https://arxiv.org/abs/2506.16950)
*Fanfei Li,Thomas Klein,Wieland Brendel,Robert Geirhos,Roland S. Zimmermann*

Main category: cs.CV

TL;DR: 论文提出LAION-C作为ImageNet-C的替代基准，用于评估现代模型在OOD鲁棒性上的表现，发现LAION-C对当前模型构成挑战，甚至部分模型性能接近或超过人类。


<details>
  <summary>Details</summary>
Motivation: 现有OOD基准（如ImageNet-C）已无法有效评估现代模型在web-scale数据集上的鲁棒性，因为训练数据已包含常见失真。

Method: 引入LAION-C，包含六种新型失真类型，确保其OOD特性；评估了包括MLLMs在内的先进模型，并与人类鲁棒性数据对比。

Result: LAION-C对当代模型构成显著挑战，部分模型性能接近或超过人类。

Conclusion: LAION-C为OOD鲁棒性评估提供了更有效的基准，揭示了模型性能的新范式：部分模型已超越人类。

Abstract: Out-of-distribution (OOD) robustness is a desired property of computer vision
models. Improving model robustness requires high-quality signals from
robustness benchmarks to quantify progress. While various benchmark datasets
such as ImageNet-C were proposed in the ImageNet era, most ImageNet-C
corruption types are no longer OOD relative to today's large, web-scraped
datasets, which already contain common corruptions such as blur or JPEG
compression artifacts. Consequently, these benchmarks are no longer well-suited
for evaluating OOD robustness in the era of web-scale datasets. Indeed, recent
models show saturating scores on ImageNet-era OOD benchmarks, indicating that
it is unclear whether models trained on web-scale datasets truly become better
at OOD generalization or whether they have simply been exposed to the test
distortions during training. To address this, we introduce LAION-C as a
benchmark alternative for ImageNet-C. LAION-C consists of six novel distortion
types specifically designed to be OOD, even for web-scale datasets such as
LAION. In a comprehensive evaluation of state-of-the-art models, we find that
the LAION-C dataset poses significant challenges to contemporary models,
including MLLMs such as Gemini and GPT-4o. We additionally conducted a
psychophysical experiment to evaluate the difficulty of our corruptions for
human observers, enabling a comparison of models to lab-quality human
robustness data. We observe a paradigm shift in OOD generalization: from humans
outperforming models, to the best models now matching or outperforming the best
human observers.

</details>


### [214] [Visual-Instructed Degradation Diffusion for All-in-One Image Restoration](https://arxiv.org/abs/2506.16960)
*Wenyang Luo,Haina Qin,Zewen Chen,Libin Wang,Dandan Zheng,Yuming Li,Yufan Liu,Bing Li,Weiming Hu*

Main category: cs.CV

TL;DR: Defusion提出了一种基于视觉指令引导的全能图像修复框架，通过扩散模型直接在退化空间中操作，显著提升了泛化能力和稳定性。


<details>
  <summary>Details</summary>
Motivation: 现有图像修复方法通常针对单一退化类型设计模型，难以应对真实场景中混合或未知的退化问题。

Method: Defusion通过构建与视觉退化模式对齐的视觉指令，指导扩散模型在退化空间中直接操作，实现图像修复。

Result: 实验表明，Defusion在多种图像修复任务中优于现有方法，包括复杂和真实世界的退化情况。

Conclusion: Defusion提供了一种通用且高效的图像修复解决方案，适用于多样化的退化场景。

Abstract: Image restoration tasks like deblurring, denoising, and dehazing usually need
distinct models for each degradation type, restricting their generalization in
real-world scenarios with mixed or unknown degradations. In this work, we
propose \textbf{Defusion}, a novel all-in-one image restoration framework that
utilizes visual instruction-guided degradation diffusion. Unlike existing
methods that rely on task-specific models or ambiguous text-based priors,
Defusion constructs explicit \textbf{visual instructions} that align with the
visual degradation patterns. These instructions are grounded by applying
degradations to standardized visual elements, capturing intrinsic degradation
features while agnostic to image semantics. Defusion then uses these visual
instructions to guide a diffusion-based model that operates directly in the
degradation space, where it reconstructs high-quality images by denoising the
degradation effects with enhanced stability and generalizability. Comprehensive
experiments demonstrate that Defusion outperforms state-of-the-art methods
across diverse image restoration tasks, including complex and real-world
degradations.

</details>


### [215] [Reversing Flow for Image Restoration](https://arxiv.org/abs/2506.16961)
*Haina Qin,Wenyang Luo,Libin Wang,Dandan Zheng,Jingdong Chen,Ming Yang,Bing Li,Weiming Hu*

Main category: cs.CV

TL;DR: ResFlow是一种基于连续归一化流的图像恢复框架，将退化过程建模为确定性路径，显著提升了性能和速度。


<details>
  <summary>Details</summary>
Motivation: 现有生成模型将退化过程视为随机变换，导致效率低下和复杂性增加。

Method: ResFlow通过辅助过程增强退化过程，采用熵保持流路径并匹配速度场学习增强的退化流。

Result: ResFlow在四个采样步骤内完成任务，在多个图像恢复基准上达到最先进水平。

Conclusion: ResFlow为实际应用提供了高效实用的图像恢复解决方案。

Abstract: Image restoration aims to recover high-quality (HQ) images from degraded
low-quality (LQ) ones by reversing the effects of degradation. Existing
generative models for image restoration, including diffusion and score-based
models, often treat the degradation process as a stochastic transformation,
which introduces inefficiency and complexity. In this work, we propose ResFlow,
a novel image restoration framework that models the degradation process as a
deterministic path using continuous normalizing flows. ResFlow augments the
degradation process with an auxiliary process that disambiguates the
uncertainty in HQ prediction to enable reversible modeling of the degradation
process. ResFlow adopts entropy-preserving flow paths and learns the augmented
degradation flow by matching the velocity field. ResFlow significantly improves
the performance and speed of image restoration, completing the task in fewer
than four sampling steps. Extensive experiments demonstrate that ResFlow
achieves state-of-the-art results across various image restoration benchmarks,
offering a practical and efficient solution for real-world applications.

</details>


### [216] [Enhancing Step-by-Step and Verifiable Medical Reasoning in MLLMs](https://arxiv.org/abs/2506.16962)
*Haoran Sun,Yankai Jiang,Wenjie Lou,Yujie Zhang,Wenjie Li,Lilong Wang,Mianxin Liu,Lei Liu,Xiaosong Wang*

Main category: cs.CV

TL;DR: 提出了一种名为MICS的新方法，用于生成医学领域的链式思维（CoT）训练数据，并通过MICS-Score评估推理路径质量，最终构建了多任务医学推理数据集MMRP和医学MLLM模型Chiron-o1。


<details>
  <summary>Details</summary>
Motivation: 医学领域的多模态大语言模型（MLLMs）推理能力尚处于早期阶段，缺乏有效的推理路径搜索和评估框架。

Method: 提出MICS方法，通过导师模型初始化推理路径，实习生模型继续推理，并根据MICS-Score选择最优路径，构建MMRP数据集和Chiron-o1模型。

Result: Chiron-o1在多个医学视觉问答和推理基准测试中达到最优性能。

Conclusion: MICS方法有效提升了医学MLLMs的推理能力，Chiron-o1展示了强大的视觉问答和泛化推理能力。

Abstract: Multimodal large language models (MLLMs) have begun to demonstrate robust
reasoning capabilities on general tasks, yet their application in the medical
domain remains in its early stages. Constructing chain-of-thought (CoT)
training data is essential for bolstering the reasoning abilities of medical
MLLMs. However, existing approaches exhibit a deficiency in offering a
comprehensive framework for searching and evaluating effective reasoning paths
towards critical diagnosis. To address this challenge, we propose Mentor-Intern
Collaborative Search (MICS), a novel reasoning-path searching scheme to
generate rigorous and effective medical CoT data. MICS first leverages mentor
models to initialize the reasoning, one step at a time, then prompts each
intern model to continue the thinking along those initiated paths, and finally
selects the optimal reasoning path according to the overall reasoning
performance of multiple intern models. The reasoning performance is determined
by an MICS-Score, which assesses the quality of generated reasoning paths.
Eventually, we construct MMRP, a multi-task medical reasoning dataset with
ranked difficulty, and Chiron-o1, a new medical MLLM devised via a curriculum
learning strategy, with robust visual question-answering and generalizable
reasoning capabilities. Extensive experiments demonstrate that Chiron-o1,
trained on our CoT dataset constructed using MICS, achieves state-of-the-art
performance across a list of medical visual question answering and reasoning
benchmarks. Codes are available at GitHub - manglu097/Chiron-o1: Enhancing
Step-by-Step and Verifiable Medical Reasoning in MLLMs

</details>


### [217] [ForestFormer3D: A Unified Framework for End-to-End Segmentation of Forest LiDAR 3D Point Clouds](https://arxiv.org/abs/2506.16991)
*Binbin Xiang,Maciej Wielgosz,Stefano Puliti,Kamil Král,Martin Krůček,Azim Missarov,Rasmus Astrup*

Main category: cs.CV

TL;DR: ForestFormer3D是一个端到端的框架，用于森林LiDAR点云的个体树和语义分割，通过新组件实现高性能。


<details>
  <summary>Details</summary>
Motivation: 当前方法难以应对自然森林环境的复杂性和多样性，需要更精确的分割工具。

Method: 结合ISA引导的查询点选择、基于分数的块合并策略和一对多关联机制。

Result: 在新数据集FOR-instanceV2上达到最优性能，并在未见过的测试集上表现鲁棒。

Conclusion: ForestFormer3D为森林管理和生态研究提供了高效工具，代码和数据集将公开。

Abstract: The segmentation of forest LiDAR 3D point clouds, including both individual
tree and semantic segmentation, is fundamental for advancing forest management
and ecological research. However, current approaches often struggle with the
complexity and variability of natural forest environments. We present
ForestFormer3D, a new unified and end-to-end framework designed for precise
individual tree and semantic segmentation. ForestFormer3D incorporates
ISA-guided query point selection, a score-based block merging strategy during
inference, and a one-to-many association mechanism for effective training. By
combining these new components, our model achieves state-of-the-art performance
for individual tree segmentation on the newly introduced FOR-instanceV2
dataset, which spans diverse forest types and regions. Additionally,
ForestFormer3D generalizes well to unseen test sets (Wytham woods and LAUTx),
showcasing its robustness across different forest conditions and sensor
modalities. The FOR-instanceV2 dataset and the ForestFormer3D code will be
released soon.

</details>


### [218] [Prmpt2Adpt: Prompt-Based Zero-Shot Domain Adaptation for Resource-Constrained Environments](https://arxiv.org/abs/2506.16994)
*Yasir Ali Farrukh,Syed Wali,Irfan Khan,Nathaniel D. Bastian*

Main category: cs.CV

TL;DR: Prmpt2Adpt是一个轻量级、高效的零样本域适应框架，通过基于提示的特征对齐和教师-学生范式实现快速适应，适用于资源受限环境。


<details>
  <summary>Details</summary>
Motivation: 解决现有提示驱动UDA方法依赖大型模型和完整源域数据的问题，提升在资源受限环境（如无人机）中的适用性。

Method: 使用蒸馏和微调的CLIP模型作为Faster R-CNN教师的冻结主干，通过提示驱动的实例归一化（PIN）对齐少量源特征到目标域语义，生成高质量伪标签指导紧凑学生模型适应。

Result: 在MDS-A数据集上表现优于现有方法，适应速度快7倍，推理速度快5倍，且仅需少量源图像。

Conclusion: Prmpt2Adpt是实时低资源域适应的实用且可扩展解决方案。

Abstract: Unsupervised Domain Adaptation (UDA) is a critical challenge in real-world
vision systems, especially in resource-constrained environments like drones,
where memory and computation are limited. Existing prompt-driven UDA methods
typically rely on large vision-language models and require full access to
source-domain data during adaptation, limiting their applicability. In this
work, we propose Prmpt2Adpt, a lightweight and efficient zero-shot domain
adaptation framework built around a teacher-student paradigm guided by
prompt-based feature alignment. At the core of our method is a distilled and
fine-tuned CLIP model, used as the frozen backbone of a Faster R-CNN teacher. A
small set of low-level source features is aligned to the target domain
semantics-specified only through a natural language prompt-via Prompt-driven
Instance Normalization (PIN). These semantically steered features are used to
briefly fine-tune the detection head of the teacher model. The adapted teacher
then generates high-quality pseudo-labels, which guide the on-the-fly
adaptation of a compact student model. Experiments on the MDS-A dataset
demonstrate that Prmpt2Adpt achieves competitive detection performance compared
to state-of-the-art methods, while delivering up to 7x faster adaptation and 5x
faster inference speed using few source images-making it a practical and
scalable solution for real-time adaptation in low-resource domains.

</details>


### [219] [A Synthetic Benchmark for Collaborative 3D Semantic Occupancy Prediction in V2X Autonomous Driving](https://arxiv.org/abs/2506.17004)
*Hanlin Wu,Pengfei Lin,Ehsan Javanmardi,Naren Bao,Bo Qian,Hao Si,Manabu Tsukada*

Main category: cs.CV

TL;DR: 论文提出了一种协作式3D语义占用预测方法，通过多车协作解决单车的感知限制，并在CARLA中生成密集标注的数据集和基准测试。


<details>
  <summary>Details</summary>
Motivation: 单车感知受限于遮挡、传感器范围和视角狭窄，协作感知可以交换互补信息，提升感知的完整性和准确性。

Method: 通过CARLA重放现有协作感知数据集，生成高分辨率语义体素标注；建立不同预测范围的基准测试；提出基于空间对齐和注意力聚合的基线模型。

Result: 基线模型在扩展预测范围时表现优于单机模型，增益随范围扩大而增加。

Conclusion: 协作感知能有效提升3D语义占用预测的完整性和准确性，尤其在长距离预测中效果显著。

Abstract: 3D semantic occupancy prediction is an emerging perception paradigm in
autonomous driving, providing a voxel-level representation of both geometric
details and semantic categories. However, the perception capability of a single
vehicle is inherently constrained by occlusion, restricted sensor range, and
narrow viewpoints. To address these limitations, collaborative perception
enables the exchange of complementary information, thereby enhancing the
completeness and accuracy. In the absence of a dedicated dataset for
collaborative 3D semantic occupancy prediction, we augment an existing
collaborative perception dataset by replaying it in CARLA with a
high-resolution semantic voxel sensor to provide dense and comprehensive
occupancy annotations. In addition, we establish benchmarks with varying
prediction ranges designed to systematically assess the impact of spatial
extent on collaborative prediction. We further develop a baseline model that
performs inter-agent feature fusion via spatial alignment and attention
aggregation. Experimental results demonstrate that our baseline model
consistently outperforms single-agent models, with increasing gains observed as
the prediction range expands.

</details>


### [220] [Unsupervised Image Super-Resolution Reconstruction Based on Real-World Degradation Patterns](https://arxiv.org/abs/2506.17027)
*Yiyang Tie,Hong Zhu,Yunyun Luo,Jing Shi*

Main category: cs.CV

TL;DR: 论文提出TripleGAN框架，通过两个GAN组件分别处理模糊特性和退化模式，以从真实低分辨率图像中学习退化模式并生成对齐数据集。


<details>
  <summary>Details</summary>
Motivation: 真实世界超分辨率重建依赖反映真实退化模式的数据集，但仅从低分辨率图像提取退化模式或仅依赖域转换方法均无法准确模拟真实退化。

Method: 提出TripleGAN框架，包含FirstGAN（缩小模糊域差距）、SecondGAN（学习目标域模糊特性和退化模式）和ThirdGAN（重建真实低分辨率图像）。

Result: 在RealSR和DRealSR数据集上，该方法在定量指标上表现优越，重建结果清晰且无过度平滑伪影。

Conclusion: TripleGAN能有效学习真实退化模式并生成对齐数据集，从而提升超分辨率重建性能。

Abstract: The training of real-world super-resolution reconstruction models heavily
relies on datasets that reflect real-world degradation patterns. Extracting and
modeling degradation patterns for super-resolution reconstruction using only
real-world low-resolution (LR) images remains a challenging task. When
synthesizing datasets to simulate real-world degradation, relying solely on
degradation extraction methods fails to capture both blur and diverse noise
characteristics across varying LR distributions, as well as more implicit
degradations such as color gamut shifts. Conversely, domain translation alone
cannot accurately approximate real-world blur characteristics due to the
significant degradation domain gap between synthetic and real data. To address
these challenges, we propose a novel TripleGAN framework comprising two
strategically designed components: The FirstGAN primarily focuses on narrowing
the domain gap in blur characteristics, while the SecondGAN performs
domain-specific translation to approximate target-domain blur properties and
learn additional degradation patterns. The ThirdGAN is trained on pseudo-real
data generated by the FirstGAN and SecondGAN to reconstruct real-world LR
images. Extensive experiments on the RealSR and DRealSR datasets demonstrate
that our method exhibits clear advantages in quantitative metrics while
maintaining sharp reconstructions without over-smoothing artifacts. The
proposed framework effectively learns real-world degradation patterns from LR
observations and synthesizes aligned datasets with corresponding degradation
characteristics, thereby enabling the trained network to achieve superior
performance in reconstructing high-quality SR images from real-world LR inputs.

</details>


### [221] [Stretching Beyond the Obvious: A Gradient-Free Framework to Unveil the Hidden Landscape of Visual Invariance](https://arxiv.org/abs/2506.17040)
*Lorenzo Tausani,Paolo Muratore,Morgan B. Talbot,Giacomo Amerio,Gabriel Kreiman,Davide Zoccolan*

Main category: cs.CV

TL;DR: 论文提出了一种名为Stretch-and-Squeeze（SnS）的方法，用于系统分析视觉单元的不变性及其对抗扰动的脆弱性。


<details>
  <summary>Details</summary>
Motivation: 理解图像如何转化为支持识别的表征，并揭示视觉单元在变换下的不变性，这对视觉泛化至关重要。

Method: SnS通过双目标优化问题，探索图像扰动以保持单元激活或抑制单元激活，从而分析不变性和对抗敏感性。

Result: 在卷积神经网络（CNN）中，SnS揭示了像素空间变化与单元响应的关系，并发现鲁棒网络生成的图像对人类更具可识别性。

Conclusion: SnS为理解视觉系统的不变性提供了新工具，并支持鲁棒CNN作为视觉系统模型的更高保真度。

Abstract: Uncovering which features' combinations high-level visual units encode is
critical to understand how images are transformed into representations that
support recognition. While existing feature visualization approaches typically
infer a unit's most exciting images, this is insufficient to reveal the
manifold of transformations under which responses remain invariant, which is
key to generalization in vision. Here we introduce Stretch-and-Squeeze (SnS),
an unbiased, model-agnostic, and gradient-free framework to systematically
characterize a unit's invariance landscape and its vulnerability to adversarial
perturbations in both biological and artificial visual systems. SnS frames
these transformations as bi-objective optimization problems. To probe
invariance, SnS seeks image perturbations that maximally alter the
representation of a reference stimulus in a given processing stage while
preserving unit activation. To probe adversarial sensitivity, SnS seeks
perturbations that minimally alter the stimulus while suppressing unit
activation. Applied to convolutional neural networks (CNNs), SnS revealed image
variations that were further from a reference image in pixel-space than those
produced by affine transformations, while more strongly preserving the target
unit's response. The discovered invariant images differed dramatically
depending on the choice of image representation used for optimization:
pixel-level changes primarily affected luminance and contrast, while stretching
mid- and late-layer CNN representations altered texture and pose respectively.
Notably, the invariant images from robust networks were more recognizable by
human subjects than those from standard networks, supporting the higher
fidelity of robust CNNs as models of the visual system.

</details>


### [222] [Relaxed syntax modeling in Transformers for future-proof license plate recognition](https://arxiv.org/abs/2506.17051)
*Florent Meyer,Laurent Guichard,Denis Coquenet,Guillaume Gravier,Yann Soullard,Bertrand Coüasnon*

Main category: cs.CV

TL;DR: 论文提出了一种名为SaLT的语法无关Transformer模型，用于解决车牌识别系统中因语法变化导致的性能下降问题。


<details>
  <summary>Details</summary>
Motivation: 现有的基于Transformer的车牌识别系统在面对新语法车牌时性能显著下降，无法适应生产环境的需求。

Method: 通过分析Transformer编码器-解码器中位置和上下文信息的流动，识别了其对过去语法的过度依赖问题，并设计了架构上的改进（SaLT）。

Result: 实验表明，SaLT在旧语法车牌上达到最高准确率，并在新语法车牌上几乎保持性能。

Conclusion: SaLT通过语法无关建模提升了车牌识别的鲁棒性，适用于动态变化的实际场景。

Abstract: Effective license plate recognition systems are required to be resilient to
constant change, as new license plates are released into traffic daily. While
Transformer-based networks excel in their recognition at first sight, we
observe significant performance drop over time which proves them unsuitable for
tense production environments. Indeed, such systems obtain state-of-the-art
results on plates whose syntax is seen during training. Yet, we show they
perform similarly to random guessing on future plates where legible characters
are wrongly recognized due to a shift in their syntax. After highlighting the
flows of positional and contextual information in Transformer encoder-decoders,
we identify several causes for their over-reliance on past syntax. Following,
we devise architectural cut-offs and replacements which we integrate into SaLT,
an attempt at a Syntax-Less Transformer for syntax-agnostic modeling of license
plate representations. Experiments on both real and synthetic datasets show
that our approach reaches top accuracy on past syntax and most importantly
nearly maintains performance on future license plates. We further demonstrate
the robustness of our architecture enhancements by way of various ablations.

</details>


### [223] [Assembler: Scalable 3D Part Assembly via Anchor Point Diffusion](https://arxiv.org/abs/2506.17074)
*Wang Zhao,Yan-Pei Cao,Jiale Xu,Yuejiang Dong,Ying Shan*

Main category: cs.CV

TL;DR: Assembler是一个可扩展且通用的3D零件组装框架，通过扩散模型和稀疏锚点云表示，实现了高质量、多样化的零件组装。


<details>
  <summary>Details</summary>
Motivation: 解决现有方法在多样化、真实世界对象组装中的局限性，如对称性、重复零件和多有效组装带来的模糊性。

Method: 采用生成式问题框架，利用扩散模型采样配置，引入稀疏锚点云表示，构建大规模数据集。

Result: 在PartNet上达到最先进性能，首次实现复杂真实世界对象的高质量组装。

Conclusion: Assembler展示了在交互式和组合式设计中的潜力，支持高分辨率、可编辑对象的生成。

Abstract: We present Assembler, a scalable and generalizable framework for 3D part
assembly that reconstructs complete objects from input part meshes and a
reference image. Unlike prior approaches that mostly rely on deterministic part
pose prediction and category-specific training, Assembler is designed to handle
diverse, in-the-wild objects with varying part counts, geometries, and
structures. It addresses the core challenges of scaling to general 3D part
assembly through innovations in task formulation, representation, and data.
First, Assembler casts part assembly as a generative problem and employs
diffusion models to sample plausible configurations, effectively capturing
ambiguities arising from symmetry, repeated parts, and multiple valid
assemblies. Second, we introduce a novel shape-centric representation based on
sparse anchor point clouds, enabling scalable generation in Euclidean space
rather than SE(3) pose prediction. Third, we construct a large-scale dataset of
over 320K diverse part-object assemblies using a synthesis and filtering
pipeline built on existing 3D shape repositories. Assembler achieves
state-of-the-art performance on PartNet and is the first to demonstrate
high-quality assembly for complex, real-world objects. Based on Assembler, we
further introduce an interesting part-aware 3D modeling system that generates
high-resolution, editable objects from images, demonstrating potential for
interactive and compositional design. Project page:
https://assembler3d.github.io

</details>


### [224] [Acquiring and Accumulating Knowledge from Diverse Datasets for Multi-label Driving Scene Classification](https://arxiv.org/abs/2506.17101)
*Ke Li,Chenyu Zhang,Yuxin Ding,Xianbiao Hu,Ruwen Qin*

Main category: cs.CV

TL;DR: 论文提出了一种结合知识获取与积累（KAA）和基于一致性的主动学习（CAL）的新系统，用于解决驾驶场景识别的多标签分类问题，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 驾驶场景识别为自动驾驶车辆提供上下文感知能力，但多标签分类面临数据集标注不平衡和任务学习平衡的挑战。

Method: 通过KAA从单标签数据集获取知识，再通过CAL解决知识分布差异问题。

Result: 在DSI数据集上性能提升56.1%，KAA贡献31.3%，CAL贡献24.8%。在BDD100K和HSD数据集上表现优于SOTA模型，且数据使用量减少85%。

Conclusion: KAA-CAL系统有效解决了多标签分类的挑战，显著提升了驾驶场景识别的性能。

Abstract: Driving scene identification, which assigns multiple non-exclusive class
labels to a scene, provides the contextual awareness necessary for enhancing
autonomous vehicles' ability to understand, reason about, and interact with the
complex driving environment. As a multi-label classification problem, it is
better tackled via multitasking learning. However, directly training a
multi-label classification model for driving scene identification through
multitask learning presents two main challenges: acquiring a balanced,
comprehensively annotated multi-label dataset and balancing learning across
different tasks. This paper introduces a novel learning system that synergizes
knowledge acquisition and accumulation (KAA) with consistency-based active
learning (CAL) to address those challenges. KAA acquires and accumulates
knowledge about scene identification from various single-label datasets via
monotask learning. Subsequently, CAL effectively resolves the knowledge gap
caused by the discrepancy between the marginal distributions of individual
attributes and their joint distribution. An ablation study on our Driving Scene
Identification (DSI) dataset demonstrates a 56.1% performance increase over the
baseline model pretrained on ImageNet. Of this, KAA accounts for 31.3% of the
gain, and CAL contributes 24.8%. Moreover, KAA-CAL stands out as the best
performer when compared to state-of-the-art (SOTA) multi-label models on two
public datasets, BDD100K and HSD, achieving this while using 85% less data. The
DSI dataset and the implementation code for KAA-CAL are available at
https://github.com/KELISBU/KAA-CAL .

</details>


### [225] [MEXA: Towards General Multimodal Reasoning with Dynamic Multi-Expert Aggregation](https://arxiv.org/abs/2506.17113)
*Shoubin Yu,Yue Zhang,Ziyang Wang,Jaehong Yoon,Mohit Bansal*

Main category: cs.CV

TL;DR: MEXA是一个无需训练的框架，通过动态选择专家模型并结合大型推理模型（LRM）实现跨模态任务的高效推理。


<details>
  <summary>Details</summary>
Motivation: 解决多模态输入多样性和任务复杂性带来的统一框架构建挑战。

Method: 动态选择专家模型，生成可解释的文本推理输出，并通过LRM聚合推理。

Result: 在多种多模态基准测试中表现优于基线方法。

Conclusion: MEXA展示了专家驱动选择和聚合在多模态推理任务中的有效性和广泛适用性。

Abstract: Combining pre-trained expert models offers substantial potential for scalable
multimodal reasoning, but building a unified framework remains challenging due
to the increasing diversity of input modalities and task complexity. For
instance, medical diagnosis requires precise reasoning over structured clinical
tables, while financial forecasting depends on interpreting plot-based data to
make informed predictions. To tackle this challenge, we introduce MEXA, a
training-free framework that performs modality- and task-aware aggregation of
multiple expert models to enable effective multimodal reasoning across diverse
and distinct domains. MEXA dynamically selects expert models based on the input
modality and the task-specific reasoning demands (i.e., skills). Each expert
model, specialized in a modality task pair, generates interpretable textual
reasoning outputs. MEXA then aggregates and reasons over these outputs using a
Large Reasoning Model (LRM) to produce the final answer. This modular design
allows flexible and transparent multimodal reasoning across diverse domains
without additional training overhead. We extensively evaluate our approach on
diverse multimodal benchmarks, including Video Reasoning, Audio Reasoning, 3D
Understanding, and Medical QA. MEXA consistently delivers performance
improvements over strong multimodal baselines, highlighting the effectiveness
and broad applicability of our expert-driven selection and aggregation in
diverse multimodal reasoning tasks.

</details>


### [226] [Dynamic Watermark Generation for Digital Images using Perimeter Gated SPAD Imager PUFs](https://arxiv.org/abs/2506.17134)
*Md Sakibur Sajal,Marc Dandin*

Main category: cs.CV

TL;DR: 提出了一种基于pgSPAD成像器的数字水印技术，利用DSNU实现源识别和篡改检测。


<details>
  <summary>Details</summary>
Motivation: 探索SPAD成像器在数字水印中的应用，填补现有研究空白。

Method: 利用三块64x64 pgSPAD芯片的DSNU，模拟水印并测试标准图像。

Result: 实现了源识别和篡改检测，具有可控的灵敏度-鲁棒性权衡。

Conclusion: pgSPAD成像器可用于动态水印技术，具有实际应用潜力。

Abstract: Digital image watermarks as a security feature can be derived from the
imager's physically unclonable functions (PUFs) by utilizing the manufacturing
variations, i.e., the dark signal non-uniformity (DSNU). While a few
demonstrations focused on the CMOS image sensors (CIS) and active pixel sensors
(APS), single photon avalanche diode (SPAD) imagers have never been
investigated for this purpose. In this work, we have proposed a novel
watermarking technique using perimeter gated SPAD (pgSPAD) imagers. We utilized
the DSNU of three 64 x 64 pgSPAD imager chips, fabricated in a 0.35 {\mu}m
standard CMOS process and analyzed the simulated watermarks for standard test
images from publicly available database. Our observation shows that both source
identification and tamper detection can be achieved using the proposed
source-scene-specific dynamic watermarks with a controllable
sensitivity-robustness trade-off.

</details>


### [227] [Semi-Supervised Multi-Modal Medical Image Segmentation for Complex Situations](https://arxiv.org/abs/2506.17136)
*Dongdong Meng,Sheng Li,Hao Wu,Guoping Wang,Xueqing Yan*

Main category: cs.CV

TL;DR: 提出了一种新的半监督多模态医学图像分割方法，通过多阶段融合和对比互学习提升性能。


<details>
  <summary>Details</summary>
Motivation: 解决半监督条件下多模态数据利用不足的问题，提升复杂背景下的分割准确性。

Method: 采用多阶段多模态融合与增强策略，结合对比互学习约束预测一致性。

Result: 在两个多模态数据集上验证了方法的优越性能和鲁棒性。

Conclusion: 该方法在复杂场景下具有解决医学图像分割任务的潜力。

Abstract: Semi-supervised learning addresses the issue of limited annotations in
medical images effectively, but its performance is often inadequate for complex
backgrounds and challenging tasks. Multi-modal fusion methods can significantly
improve the accuracy of medical image segmentation by providing complementary
information. However, they face challenges in achieving significant
improvements under semi-supervised conditions due to the challenge of
effectively leveraging unlabeled data. There is a significant need to create an
effective and reliable multi-modal learning strategy for leveraging unlabeled
data in semi-supervised segmentation. To address these issues, we propose a
novel semi-supervised multi-modal medical image segmentation approach, which
leverages complementary multi-modal information to enhance performance with
limited labeled data. Our approach employs a multi-stage multi-modal fusion and
enhancement strategy to fully utilize complementary multi-modal information,
while reducing feature discrepancies and enhancing feature sharing and
alignment. Furthermore, we effectively introduce contrastive mutual learning to
constrain prediction consistency across modalities, thereby facilitating the
robustness of segmentation results in semi-supervised tasks. Experimental
results on two multi-modal datasets demonstrate the superior performance and
robustness of the proposed framework, establishing its valuable potential for
solving medical image segmentation tasks in complex scenarios.

</details>


### [228] [On the Theory of Conditional Feature Alignment for Unsupervised Domain-Adaptive Counting](https://arxiv.org/abs/2506.17137)
*Zhuonan Liang,Dongnan Liu,Jianan Fan,Yaxuan Song,Qiang Qu,Yu Yao,Peng Fu,Weidong Cai*

Main category: cs.CV

TL;DR: 论文提出了一种条件特征对齐的理论框架，通过条件化处理密度变化，提升跨域物体计数的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 物体计数模型在跨域部署时因密度变化导致性能下降，传统域适应方法无法处理任务相关的密度变化。

Method: 提出条件特征对齐框架，通过条件化分割域并测量条件差异，推导联合误差边界，证明条件对齐优于无条件对齐。

Result: 实验表明，该方法在多个密度分布不同的计数数据集上优于现有无监督域适应方法。

Conclusion: 条件特征对齐能有效保留任务相关信息，提升跨域计数的泛化性能。

Abstract: Object counting models suffer when deployed across domains with differing
density variety, since density shifts are inherently task-relevant and violate
standard domain adaptation assumptions. To address this, we propose a
theoretical framework of conditional feature alignment. We first formalize the
notion of conditional divergence by partitioning each domain into subsets
(e.g., object vs. background) and measuring divergences per condition. We then
derive a joint error bound showing that, under discrete label spaces treated as
condition sets, aligning distributions conditionally leads to tighter bounds on
the combined source-target decision error than unconditional alignment. These
insights motivate a general conditional adaptation principle: by preserving
task-relevant variations while filtering out nuisance shifts, one can achieve
superior cross-domain generalization for counting. We provide both defining
conditional divergence then proving its benefit in lowering joint error and a
practical adaptation strategy that preserves task-relevant information in
unsupervised domain-adaptive counting. We demonstrate the effectiveness of our
approach through extensive experiments on multiple counting datasets with
varying density distributions. The results show that our method outperforms
existing unsupervised domain adaptation methods, empirically validating the
theoretical insights on conditional feature alignment.

</details>


### [229] [Do We Need Large VLMs for Spotting Soccer Actions?](https://arxiv.org/abs/2506.17144)
*Ritabrata Chakraborty,Rajatsubhra Chakraborty,Avijit Dasgupta,Sandeep Chaurasia*

Main category: cs.CV

TL;DR: 论文提出了一种基于文本的轻量级方法，利用大型语言模型（LLMs）替代传统的视觉语言模型（VLMs），通过专家评论来识别足球比赛中的关键动作。


<details>
  <summary>Details</summary>
Motivation: 传统视频任务依赖视觉输入，计算成本高。本文希望通过文本数据（如专家评论）提供丰富的上下文信息，实现轻量级且可扩展的动作识别。

Method: 使用SoccerNet Echoes数据集，通过三个LLMs分别评估评论窗口，识别进球、黄牌等动作并生成时间戳。

Result: 实验表明，这种语言中心方法能有效检测关键比赛事件，且无需训练。

Conclusion: 该方法为动作识别提供了一种轻量级、无需训练的替代方案，优于传统视频方法。

Abstract: Traditional video-based tasks like soccer action spotting rely heavily on
visual inputs, often requiring complex and computationally expensive models to
process dense video data. In this work, we propose a shift from this
video-centric approach to a text-based task, making it lightweight and scalable
by utilizing Large Language Models (LLMs) instead of Vision-Language Models
(VLMs). We posit that expert commentary, which provides rich, fine-grained
descriptions and contextual cues such as excitement and tactical insights,
contains enough information to reliably spot key actions in a match. To
demonstrate this, we use the SoccerNet Echoes dataset, which provides
timestamped commentary, and employ a system of three LLMs acting as judges
specializing in outcome, excitement, and tactics. Each LLM evaluates sliding
windows of commentary to identify actions like goals, cards, and substitutions,
generating accurate timestamps for these events. Our experiments show that this
language-centric approach performs effectively in detecting critical match
events, providing a lightweight and training-free alternative to traditional
video-based methods for action spotting.

</details>


### [230] [Co-Seg++: Mutual Prompt-Guided Collaborative Learning for Versatile Medical Segmentation](https://arxiv.org/abs/2506.17159)
*Qing Xu,Yuxiang Luo,Wenting Duan,Zhen Chen*

Main category: cs.CV

TL;DR: 提出了一种名为Co-Seg++的框架，用于医学图像的多任务分割，通过联合语义和实例分割任务提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有研究通常将分割任务孤立处理，忽略了任务间的相互依赖性，导致分割性能不佳。

Method: 设计了STP-Encoder捕获空间和时间关系，以及MTC-Decoder通过跨任务指导增强一致性。

Result: 在多个数据集上表现优于现有方法，适用于牙齿结构、组织病理学和细胞核实例的分割。

Conclusion: Co-Seg++通过任务协同显著提升了医学图像分割的准确性和理解能力。

Abstract: Medical image analysis is critical yet challenged by the need of jointly
segmenting organs or tissues, and numerous instances for anatomical structures
and tumor microenvironment analysis. Existing studies typically formulated
different segmentation tasks in isolation, which overlooks the fundamental
interdependencies between these tasks, leading to suboptimal segmentation
performance and insufficient medical image understanding. To address this
issue, we propose a Co-Seg++ framework for versatile medical segmentation.
Specifically, we introduce a novel co-segmentation paradigm, allowing semantic
and instance segmentation tasks to mutually enhance each other. We first devise
a spatio-temporal prompt encoder (STP-Encoder) to capture long-range spatial
and temporal relationships between segmentation regions and image embeddings as
prior spatial constraints. Moreover, we devise a multi-task collaborative
decoder (MTC-Decoder) that leverages cross-guidance to strengthen the
contextual consistency of both tasks, jointly computing semantic and instance
segmentation masks. Extensive experiments on diverse CT and histopathology
datasets demonstrate that the proposed Co-Seg++ outperforms state-of-the-arts
in the semantic, instance, and panoptic segmentation of dental anatomical
structures, histopathology tissues, and nuclei instances. The source code is
available at https://github.com/xq141839/Co-Seg-Plus.

</details>


### [231] [YASMOT: Yet another stereo image multi-object tracker](https://arxiv.org/abs/2506.17186)
*Ketil Malde*

Main category: cs.CV

TL;DR: yasmot是一个轻量级、灵活的对象跟踪器，用于处理来自流行对象检测器的输出，支持单目或立体相机配置，并能生成检测器集合的共识检测。


<details>
  <summary>Details</summary>
Motivation: 在图像时间序列中，跟踪对象并保持其身份可以提高检测性能，并为下游任务（如行为分类、预测和丰度估计）提供必要支持。

Method: yasmot处理对象检测器的输出，支持单目或立体相机配置，并生成检测器集合的共识检测。

Result: yasmot能够有效跟踪对象并生成共识检测。

Conclusion: yasmot是一个适用于多种配置的轻量级对象跟踪器，能提升对象检测性能并支持下游任务。

Abstract: There now exists many popular object detectors based on deep learning that
can analyze images and extract locations and class labels for occurrences of
objects. For image time series (i.e., video or sequences of stills), tracking
objects over time and preserving object identity can help to improve object
detection performance, and is necessary for many downstream tasks, including
classifying and predicting behaviors, and estimating total abundances. Here we
present yasmot, a lightweight and flexible object tracker that can process the
output from popular object detectors and track objects over time from either
monoscopic or stereoscopic camera configurations. In addition, it includes
functionality to generate consensus detections from ensembles of object
detectors.

</details>


### [232] [Facial Landmark Visualization and Emotion Recognition Through Neural Networks](https://arxiv.org/abs/2506.17191)
*Israel Juárez-Jiménez,Tiffany Guadalupe Martínez Paredes,Jesús García-Ramírez,Eric Ramos Aguilar*

Main category: cs.CV

TL;DR: 论文提出了一种面部标志箱线图的可视化技术，用于识别数据集中的异常值，并比较了两种面部标志特征。结果显示神经网络优于随机森林分类器。


<details>
  <summary>Details</summary>
Motivation: 面部图像的情感识别是人机交互中的重要任务，但现有研究缺乏深入的数据集分析，且面部标志的可视化提取有意义信息具有挑战性。

Method: 提出面部标志箱线图技术，比较两种面部标志特征：绝对位置和从中性表情到情绪峰值时的位移。

Result: 神经网络在性能上优于随机森林分类器。

Conclusion: 面部标志箱线图技术有助于数据集分析，神经网络在情感识别任务中表现更优。

Abstract: Emotion recognition from facial images is a crucial task in human-computer
interaction, enabling machines to learn human emotions through facial
expressions. Previous studies have shown that facial images can be used to
train deep learning models; however, most of these studies do not include a
through dataset analysis. Visualizing facial landmarks can be challenging when
extracting meaningful dataset insights; to address this issue, we propose
facial landmark box plots, a visualization technique designed to identify
outliers in facial datasets. Additionally, we compare two sets of facial
landmark features: (i) the landmarks' absolute positions and (ii) their
displacements from a neutral expression to the peak of an emotional expression.
Our results indicate that a neural network achieves better performance than a
random forest classifier.

</details>


### [233] [Hunyuan-GameCraft: High-dynamic Interactive Game Video Generation with Hybrid History Condition](https://arxiv.org/abs/2506.17201)
*Jiaqi Li,Junshu Tang,Zhiyong Xu,Longhuang Wu,Yuan Zhou,Shuai Shao,Tianbao Yu,Zhiguo Cao,Qinglin Lu*

Main category: cs.CV

TL;DR: Hunyuan-GameCraft是一个用于游戏环境中高动态交互视频生成的新框架，通过统一输入、混合历史条件训练和模型蒸馏，解决了现有方法在动态性、通用性、长期一致性和效率上的限制。


<details>
  <summary>Details</summary>
Motivation: 当前基于扩散和可控视频生成的方法在动态性、通用性、长期一致性和效率上存在不足，限制了多样化游戏视频的生成能力。

Method: 提出Hunyuan-GameCraft框架，统一键盘和鼠标输入为共享相机表示空间，采用混合历史条件训练策略，并通过模型蒸馏提升推理效率。

Result: 在超过100款AAA游戏的百万级游戏录像数据集上训练，实验表明Hunyuan-GameCraft在真实感和可玩性上显著优于现有模型。

Conclusion: Hunyuan-GameCraft通过创新的框架和训练策略，显著提升了交互游戏视频生成的动态性、真实感和实时性。

Abstract: Recent advances in diffusion-based and controllable video generation have
enabled high-quality and temporally coherent video synthesis, laying the
groundwork for immersive interactive gaming experiences. However, current
methods face limitations in dynamics, generality, long-term consistency, and
efficiency, which limit the ability to create various gameplay videos. To
address these gaps, we introduce Hunyuan-GameCraft, a novel framework for
high-dynamic interactive video generation in game environments. To achieve
fine-grained action control, we unify standard keyboard and mouse inputs into a
shared camera representation space, facilitating smooth interpolation between
various camera and movement operations. Then we propose a hybrid
history-conditioned training strategy that extends video sequences
autoregressively while preserving game scene information. Additionally, to
enhance inference efficiency and playability, we achieve model distillation to
reduce computational overhead while maintaining consistency across long
temporal sequences, making it suitable for real-time deployment in complex
interactive environments. The model is trained on a large-scale dataset
comprising over one million gameplay recordings across over 100 AAA games,
ensuring broad coverage and diversity, then fine-tuned on a carefully annotated
synthetic dataset to enhance precision and control. The curated game scene data
significantly improves the visual fidelity, realism and action controllability.
Extensive experiments demonstrate that Hunyuan-GameCraft significantly
outperforms existing models, advancing the realism and playability of
interactive game video generation.

</details>


### [234] [UniFork: Exploring Modality Alignment for Unified Multimodal Understanding and Generation](https://arxiv.org/abs/2506.17202)
*Teng Li,Quanfeng Lu,Lirui Zhao,Hao Li,Xizhou Zhu,Yu Qiao,Jun Zhang,Wenqi Shao*

Main category: cs.CV

TL;DR: UniFork是一种新型Y形架构，通过共享浅层和任务特定分支的深层设计，解决了统一图像理解与生成模型中模态对齐的冲突，性能优于传统共享Transformer架构。


<details>
  <summary>Details</summary>
Motivation: 统一图像理解与生成模型的架构设计存在挑战，任务间模态对齐模式不同导致性能妥协。

Method: 分析任务特定模型和统一模型的模态对齐行为，提出UniFork架构，共享浅层并采用任务特定分支。

Result: UniFork性能优于传统共享Transformer架构，与任务特定模型相当或更好。

Conclusion: UniFork通过平衡共享学习和任务专业化，有效解决了统一模型中的任务冲突问题。

Abstract: Unified image understanding and generation has emerged as a promising
paradigm in multimodal artificial intelligence. Despite recent progress, the
optimal architectural design for such unified models remains an open challenge.
In this work, we start by analyzing the modality alignment behaviors of
task-specific expert models for understanding and generation, as well as
current unified models. Our analysis reveals a crucial observation:
understanding tasks benefit from a progressively increasing modality alignment
across network depth, which helps build up semantic information for better
comprehension; In contrast, generation tasks follow a different trend: modality
alignment increases in the early layers but decreases in the deep layers to
recover spatial details. These divergent alignment patterns create a
fundamental conflict in fully shared Transformer backbones, where a uniform
representational flow often leads to performance compromises across two tasks.
Motivated by this finding, we introduce UniFork, a novel Y-shaped architecture
that shares the shallow layers for cross-task representation learning, while
employing task-specific branches in deeper layers to avoid task interference.
This design effectively balances shared learning and task specialization.
Through extensive ablation experiments, we demonstrate that Unifork
consistently outperforms conventional fully shared Transformer architectures,
and achieves performance on par with or better than task-specific models.

</details>


### [235] [Machine Mental Imagery: Empower Multimodal Reasoning with Latent Visual Tokens](https://arxiv.org/abs/2506.17218)
*Zeyuan Yang,Xueyang Yu,Delin Chen,Maohao Shen,Chuang Gan*

Main category: cs.CV

TL;DR: 论文提出Mirage框架，通过隐式视觉标记增强视觉语言模型（VLM）的解码能力，避免显式图像生成，从而提升多模态推理性能。


<details>
  <summary>Details</summary>
Motivation: 现有VLM在需要视觉想象的任务中表现受限，因为它们只能通过文本解码表达视觉推理。

Method: 提出Mirage框架，通过隐式视觉标记和文本交替解码，结合蒸馏和强化学习优化多模态推理。

Result: 实验表明，Mirage在不生成显式图像的情况下显著提升了多模态推理能力。

Conclusion: Mirage为VLM提供了一种更高效的视觉推理方式，避免了显式图像生成的负担。

Abstract: Vision-language models (VLMs) excel at multimodal understanding, yet their
text-only decoding forces them to verbalize visual reasoning, limiting
performance on tasks that demand visual imagination. Recent attempts train VLMs
to render explicit images, but the heavy image-generation pre-training often
hinders the reasoning ability. Inspired by the way humans reason with mental
imagery-the internal construction and manipulation of visual cues-we
investigate whether VLMs can reason through interleaved multimodal trajectories
without producing explicit images. To this end, we present a Machine Mental
Imagery framework, dubbed as Mirage, which augments VLM decoding with latent
visual tokens alongside ordinary text. Concretely, whenever the model chooses
to ``think visually'', it recasts its hidden states as next tokens, thereby
continuing a multimodal trajectory without generating pixel-level images. Begin
by supervising the latent tokens through distillation from ground-truth image
embeddings, we then switch to text-only supervision to make the latent
trajectory align tightly with the task objective. A subsequent reinforcement
learning stage further enhances the multimodal reasoning capability.
Experiments on diverse benchmarks demonstrate that Mirage unlocks stronger
multimodal reasoning without explicit image generation.

</details>


### [236] [Emergent Temporal Correspondences from Video Diffusion Transformers](https://arxiv.org/abs/2506.17220)
*Jisu Nam,Soowon Son,Dahyun Chung,Jiyoung Kim,Siyoon Jin,Junhwa Hur,Seungryong Kim*

Main category: cs.CV

TL;DR: DiffTrack是一个定量分析框架，用于研究视频扩散模型（DiTs）如何建立时间对应关系，揭示了特定层在时间匹配中的关键作用，并展示了在零样本点跟踪和视频生成中的实际应用。


<details>
  <summary>Details</summary>
Motivation: 研究视频扩散模型（DiTs）内部如何建立和表示时间对应关系，填补了这一领域的空白。

Method: 构建带有伪真实跟踪注释的数据集，提出新评估指标，系统分析DiTs的3D注意力机制中各组件的作用。

Result: 发现特定层的查询-键相似性在时间匹配中起关键作用，且在去噪过程中逐渐显著；DiffTrack在零样本点跟踪中表现优异，并改进了视频生成的时间一致性。

Conclusion: DiffTrack为理解视频DiTs的内部机制提供了重要见解，并为未来研究和应用奠定了基础。

Abstract: Recent advancements in video diffusion models based on Diffusion Transformers
(DiTs) have achieved remarkable success in generating temporally coherent
videos. Yet, a fundamental question persists: how do these models internally
establish and represent temporal correspondences across frames? We introduce
DiffTrack, the first quantitative analysis framework designed to answer this
question. DiffTrack constructs a dataset of prompt-generated video with pseudo
ground-truth tracking annotations and proposes novel evaluation metrics to
systematically analyze how each component within the full 3D attention
mechanism of DiTs (e.g., representations, layers, and timesteps) contributes to
establishing temporal correspondences. Our analysis reveals that query-key
similarities in specific, but not all, layers play a critical role in temporal
matching, and that this matching becomes increasingly prominent during the
denoising process. We demonstrate practical applications of DiffTrack in
zero-shot point tracking, where it achieves state-of-the-art performance
compared to existing vision foundation and self-supervised video models.
Further, we extend our findings to motion-enhanced video generation with a
novel guidance method that improves temporal consistency of generated videos
without additional training. We believe our work offers crucial insights into
the inner workings of video DiTs and establishes a foundation for further
research and applications leveraging their temporal understanding.

</details>


### [237] [VLN-R1: Vision-Language Navigation via Reinforcement Fine-Tuning](https://arxiv.org/abs/2506.17221)
*Zhangyang Qi,Zhixiong Zhang,Yizhou Yu,Jiaqi Wang,Hengshuang Zhao*

Main category: cs.CV

TL;DR: VLN-R1是一个端到端框架，利用大型视觉语言模型（LVLM）将第一人称视频流直接转换为连续导航动作，采用GRPO训练方法，并在VLN-CE基准测试中表现优异。


<details>
  <summary>Details</summary>
Motivation: 当前基于语言模型的导航系统局限于离散拓扑图，限制了路径规划的灵活性。VLN-R1旨在通过连续动作和LVLM提升导航能力。

Method: 提出VLN-Ego数据集和长短期记忆采样方法，采用两阶段训练：监督微调（SFT）和强化微调（RFT）结合时间衰减奖励（TDR）机制。

Result: VLN-R1在VLN-CE基准测试中表现优异，证明LVLM可通过数据高效、奖励驱动的后训练提升导航任务。

Conclusion: VLN-R1展示了LVLM在具身导航中的潜力，并通过任务特定推理增强性能。

Abstract: Vision-Language Navigation (VLN) is a core challenge in embodied AI,
requiring agents to navigate real-world environments using natural language
instructions. Current language model-based navigation systems operate on
discrete topological graphs, limiting path planning to predefined node
connections. We propose VLN-R1, an end-to-end framework that leverages Large
Vision-Language Models (LVLM) to directly translate egocentric video streams
into continuous navigation actions, adopting GRPO-based training inspired by
DeepSeek-R1. To enable effective training, we first construct the VLN-Ego
dataset using a 3D simulator, Habitat, and propose Long-Short Memory Sampling
to balance historical and current observations. While large language models can
supervise complete textual instructions, they lack fine-grained action-level
control. Our framework employs a two-stage training approach: a) Supervised
fine-tuning (SFT) to align the model's action sequence text predictions with
expert demonstrations, followed by b) Reinforcement fine-tuning (RFT) enhanced
with a Time-Decayed Reward (TDR) mechanism that strategically weights
multi-step future actions. Experimental results show VLN-R1 achieves strong
performance on VLN-CE benchmark. VLN-R1 proves LVLMs can drive embodied
navigation and enhance task-specific reasoning through data-efficient,
reward-driven post-training.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [238] [Ignition Phase : Standard Training for Fast Adversarial Robustness](https://arxiv.org/abs/2506.15685)
*Wang Yu-Hang,Liu ying,Fang liang,Wang Xuelin,Junkang Guo,Shiwei Li,Lei Gao,Jian Liu,Wenfei Yin*

Main category: cs.LG

TL;DR: AET框架通过在传统对抗训练前加入ERM阶段，优化特征表示，提升鲁棒性和效率。


<details>
  <summary>Details</summary>
Motivation: 现有对抗训练方法忽视基础特征表示，专注于攻击生成，导致效率不足。

Method: 引入AET框架，先进行ERM阶段优化特征，再进行对抗训练。

Result: AET在多个数据集和架构中表现优异，鲁棒性提升，训练成本降低8-25%。

Conclusion: 特征预条件化对高效鲁棒防御至关重要，AET为对抗训练提供了新思路。

Abstract: Adversarial Training (AT) is a cornerstone defense, but many variants
overlook foundational feature representations by primarily focusing on stronger
attack generation. We introduce Adversarial Evolution Training (AET), a simple
yet powerful framework that strategically prepends an Empirical Risk
Minimization (ERM) phase to conventional AT. We hypothesize this initial ERM
phase cultivates a favorable feature manifold, enabling more efficient and
effective robustness acquisition. Empirically, AET achieves comparable or
superior robustness more rapidly, improves clean accuracy, and cuts training
costs by 8-25\%. Its effectiveness is shown across multiple datasets,
architectures, and when augmenting established AT methods. Our findings
underscore the impact of feature pre-conditioning via standard training for
developing more efficient, principled robust defenses. Code is available in the
supplementary material.

</details>


### [239] [Learning from M-Tuple Dominant Positive and Unlabeled Data](https://arxiv.org/abs/2506.15686)
*Jiahe Qin,Junpeng Li,Changchun Hua,Yana Yang*

Main category: cs.LG

TL;DR: 本文提出了一种广义学习框架MDPU，用于解决标签比例学习（LLP）中实例比例信息不精确的问题，通过数学建模和风险校正方法提升分类性能。


<details>
  <summary>Details</summary>
Motivation: 在LLP中，精确的实例比例信息难以获取，限制了实际应用效果。本文旨在提出一种更贴近实际场景的方法，利用实例比例约束提升分类性能。

Method: 首先对任意大小的元组内实例分布进行数学建模，确保正例数不少于负例数；然后基于经验风险最小化（ERM）推导出满足风险一致性的无偏风险估计器；最后引入风险校正方法解决过拟合问题。

Result: 理论分析证明了无偏风险估计器的泛化误差边界一致性，实验验证了MDPU框架在多个数据集上的有效性。

Conclusion: MDPU框架通过数学建模和风险校正，显著提升了LLP在实际应用中的性能，为相关领域提供了新的解决方案。

Abstract: Label Proportion Learning (LLP) addresses the classification problem where
multiple instances are grouped into bags and each bag contains information
about the proportion of each class. However, in practical applications,
obtaining precise supervisory information regarding the proportion of instances
in a specific class is challenging. To better align with real-world application
scenarios and effectively leverage the proportional constraints of instances
within tuples, this paper proposes a generalized learning framework
\emph{MDPU}. Specifically, we first mathematically model the distribution of
instances within tuples of arbitrary size, under the constraint that the number
of positive instances is no less than that of negative instances. Then we
derive an unbiased risk estimator that satisfies risk consistency based on the
empirical risk minimization (ERM) method. To mitigate the inevitable
overfitting issue during training, a risk correction method is introduced,
leading to the development of a corrected risk estimator. The generalization
error bounds of the unbiased risk estimator theoretically demonstrate the
consistency of the proposed method. Extensive experiments on multiple datasets
and comparisons with other relevant baseline methods comprehensively validate
the effectiveness of the proposed learning framework.

</details>


### [240] [S$^2$GPT-PINNs: Sparse and Small models for PDEs](https://arxiv.org/abs/2506.15687)
*Yajie Ji,Yanlai Chen,Shawn Koohy*

Main category: cs.LG

TL;DR: S$^2$GPT-PINN是一种稀疏且小型的模型，用于求解参数化偏微分方程（PDEs），其特点是架构紧凑且计算需求低。


<details>
  <summary>Details</summary>
Motivation: 针对特定领域的PDEs，设计一种高效且参数少的模型，以解决传统PINNs参数过多的问题。

Method: 通过数学严谨的贪婪算法利用少量高质量数据，结合知识蒸馏和物理信息损失的下采样，实现高效求解。

Result: S$^2$GPT-PINN的参数数量比PINNs少多个数量级，同时保持极高效率。

Conclusion: S$^2$GPT-PINN为特定领域PDEs提供了一种高效且资源友好的解决方案。

Abstract: We propose S$^2$GPT-PINN, a sparse and small model for solving parametric
partial differential equations (PDEs). Similar to Small Language Models (SLMs),
S$^2$GPT-PINN is tailored to domain-specific (families of) PDEs and
characterized by its compact architecture and minimal computational power.
Leveraging a small amount of extremely high quality data via a mathematically
rigorous greedy algorithm that is enabled by the large full-order models,
S$^2$GPT-PINN relies on orders of magnitude less parameters than PINNs to
achieve extremely high efficiency via two levels of customizations. The first
is knowledge distillation via task-specific activation functions that are
transferred from Pre-Trained PINNs. The second is a judicious down-sampling
when calculating the physics-informed loss of the network compressing the
number of data sites by orders of magnitude to the size of the small model.

</details>


### [241] [Cellular Traffic Prediction via Deep State Space Models with Attention Mechanism](https://arxiv.org/abs/2506.15688)
*Hui Ma,Kai Yang,Man-On Pun*

Main category: cs.LG

TL;DR: 提出了一种端到端框架，结合卷积神经网络和注意力机制捕捉空间动态，卡尔曼滤波建模时间动态，利用辅助信息提升蜂窝流量预测精度。


<details>
  <summary>Details</summary>
Motivation: 蜂窝流量高度动态且受多种外部因素影响，导致预测精度下降，需改进现有方法。

Method: 使用卷积神经网络和注意力机制捕捉空间动态，卡尔曼滤波建模时间动态，并利用辅助信息（如社交活动）。

Result: 在三个真实数据集上实验，模型在预测精度上优于现有机器学习技术。

Conclusion: 提出的框架能有效提升蜂窝流量预测精度，优于现有方法。

Abstract: Cellular traffic prediction is of great importance for operators to manage
network resources and make decisions. Traffic is highly dynamic and influenced
by many exogenous factors, which would lead to the degradation of traffic
prediction accuracy. This paper proposes an end-to-end framework with two
variants to explicitly characterize the spatiotemporal patterns of cellular
traffic among neighboring cells. It uses convolutional neural networks with an
attention mechanism to capture the spatial dynamics and Kalman filter for
temporal modelling. Besides, we can fully exploit the auxiliary information
such as social activities to improve prediction performance. We conduct
extensive experiments on three real-world datasets. The results show that our
proposed models outperform the state-of-the-art machine learning techniques in
terms of prediction accuracy.

</details>


### [242] [BASE-Q: Bias and Asymmetric Scaling Enhanced Rotational Quantization for Large Language Models](https://arxiv.org/abs/2506.15689)
*Liulu He,Shenli Zhen,Karwei Sun,Yijiang Liu,Yufei Zhao,Chongkang Tan,Huanrui Yang,Yuan Du,Li Du*

Main category: cs.LG

TL;DR: 论文提出了一种名为BASE-Q的新方法，通过结合偏置校正和非对称缩放，有效减少量化误差，并支持分块优化，避免了内存密集型全模型反向传播。


<details>
  <summary>Details</summary>
Motivation: 当前旋转量化方法存在两个问题：(i) 旋转未能对齐通道均值，导致量化边界变宽和舍入误差增加；(ii) 旋转使激活分布更接近高斯分布，增加了截断误差的能量损失。

Method: 提出BASE-Q方法，结合偏置校正和非对称缩放，减少舍入和截断误差，并支持分块优化。

Result: 实验表明，BASE-Q在多种LLM和基准测试中表现优异，将精度差距缩小了50.5%、42.9%和29.2%（相比QuaRot、SpinQuant和OSTQuant）。

Conclusion: BASE-Q是一种简单而强大的方法，显著提升了量化性能，同时降低了内存需求。

Abstract: Rotations have become essential to state-of-the-art quantization pipelines
for large language models (LLMs) by effectively smoothing outliers in weights
and activations. However, further optimizing the rotation parameters offers
only limited performance gains and introduces significant training overhead:
due to rotation parameter sharing, full-model must be loaded simultaneously to
enable backpropagation, resulting in substantial memory consumption and limited
practical utility. In this work, we identify two fundamental limitations of
current rotational quantization methods: (i) rotation fails to align channel
means, resulting in wider quantization bounds and increased rounding errors;
and (ii) rotation makes the activation distribution more Gaussian-like,
increasing energy loss caused by clipping errors. To address these issues, we
introduce \textbf{BASE-Q}, a simple yet powerful approach that combines bias
correction and asymmetric scaling to effectively reduce rounding and clipping
errors. Furthermore, BASE-Q enables blockwise optimization, eliminating the
need for memory-intensive full-model backpropagation. Extensive experiments on
various LLMs and benchmarks demonstrate the effectiveness of BASE-Q, narrowing
the accuracy gap to full-precision models by 50.5\%, 42.9\%, and 29.2\%
compared to QuaRot, SpinQuant, and OSTQuant, respectively. The code will be
released soon.

</details>


### [243] [LLM Web Dynamics: Tracing Model Collapse in a Network of LLMs](https://arxiv.org/abs/2506.15690)
*Tianyu Wang,Lingyou Pang,Akira Horiguchi,Carey E. Priebe*

Main category: cs.LG

TL;DR: 论文提出LLM Web Dynamics (LWD)框架，研究网络级模型崩溃问题，通过模拟互联网和RAG数据库分析模型输出收敛模式，并提供理论保证。


<details>
  <summary>Details</summary>
Motivation: 尽管合成数据在LLM训练中提高了效率，但模型崩溃的潜在威胁尚未充分研究，现有研究多限于单模型或统计代理。

Method: 引入LWD框架，模拟互联网环境并使用RAG数据库，分析模型输出收敛模式，并通过高斯混合模型类比提供理论保证。

Result: 揭示了模型在网络级环境下的收敛行为，并提供了理论支持。

Conclusion: LWD框架为研究模型崩溃提供了新视角，有助于理解网络级LLM的动态行为。

Abstract: The increasing use of synthetic data from the public Internet has enhanced
data usage efficiency in large language model (LLM) training. However, the
potential threat of model collapse remains insufficiently explored. Existing
studies primarily examine model collapse in a single model setting or rely
solely on statistical surrogates. In this work, we introduce LLM Web Dynamics
(LWD), an efficient framework for investigating model collapse at the network
level. By simulating the Internet with a retrieval-augmented generation (RAG)
database, we analyze the convergence pattern of model outputs. Furthermore, we
provide theoretical guarantees for this convergence by drawing an analogy to
interacting Gaussian Mixture Models.

</details>


### [244] [What Do Latent Action Models Actually Learn?](https://arxiv.org/abs/2506.15691)
*Chuheng Zhang,Tim Pearce,Pushi Zhang,Kaixin Wang,Xiaoyu Chen,Wei Shen,Li Zhao,Jiang Bian*

Main category: cs.LG

TL;DR: 论文通过线性模型分析潜在动作模型（LAMs）是否能捕捉动作相关变化而非噪声，揭示了LAM与PCA的联系，并提出了数据增强等策略。


<details>
  <summary>Details</summary>
Motivation: 研究LAMs是否能区分动作相关变化与噪声，以提升模型对可控变化的捕捉能力。

Method: 提出一个可分析的线性模型，探讨LAM学习过程，并通过数值模拟验证。

Result: 揭示了LAM与PCA的联系，提出了数据增强、清理和辅助动作预测等策略。

Conclusion: LAMs需结合策略以区分动作与噪声，线性模型为研究提供了理论基础。

Abstract: Latent action models (LAMs) aim to learn action-relevant changes from
unlabeled videos by compressing changes between frames as latents. However,
differences between video frames can be caused by controllable changes as well
as exogenous noise, leading to an important concern -- do latents capture the
changes caused by actions or irrelevant noise? This paper studies this issue
analytically, presenting a linear model that encapsulates the essence of LAM
learning, while being tractable.This provides several insights, including
connections between LAM and principal component analysis (PCA), desiderata of
the data-generating policy, and justification of strategies to encourage
learning controllable changes using data augmentation, data cleaning, and
auxiliary action-prediction. We also provide illustrative results based on
numerical simulation, shedding light on the specific structure of observations,
actions, and noise in data that influence LAM learning.

</details>


### [245] [The Hidden Cost of an Image: Quantifying the Energy Consumption of AI Image Generation](https://arxiv.org/abs/2506.17016)
*Giulia Bertazzini,Chiara Albisani,Daniele Baracchi,Dasara Shullani,Roberto Verdecchia*

Main category: cs.LG

TL;DR: 该研究通过实验评估了17种AI图像生成模型的能耗，发现能耗差异显著（最高达46倍），并探讨了分辨率、模型类型、量化等因素对能耗的影响。


<details>
  <summary>Details</summary>
Motivation: 随着AI图像生成的普及及其对环境资源需求的增加，研究旨在揭示生成每张图像背后的环境影响。

Method: 实验比较了17种先进图像生成模型，考虑了模型量化、图像分辨率、提示长度等因素对能耗的影响，并结合图像质量指标分析能耗与质量的权衡。

Result: 结果显示，模型能耗差异显著，分辨率影响不一致（1.3x至4.7x），U-Net模型能耗低于Transformer模型，量化反而降低能效。图像质量提升不一定伴随能耗增加。

Conclusion: 研究表明，AI图像生成的能耗与模型类型、分辨率等因素密切相关，但高质量图像不一定需要高能耗，为未来优化提供了方向。

Abstract: With the growing adoption of AI image generation, in conjunction with the
ever-increasing environmental resources demanded by AI, we are urged to answer
a fundamental question: What is the environmental impact hidden behind each
image we generate? In this research, we present a comprehensive empirical
experiment designed to assess the energy consumption of AI image generation.
Our experiment compares 17 state-of-the-art image generation models by
considering multiple factors that could affect their energy consumption, such
as model quantization, image resolution, and prompt length. Additionally, we
consider established image quality metrics to study potential trade-offs
between energy consumption and generated image quality. Results show that image
generation models vary drastically in terms of the energy they consume, with up
to a 46x difference. Image resolution affects energy consumption
inconsistently, ranging from a 1.3x to 4.7x increase when doubling resolution.
U-Net-based models tend to consume less than Transformer-based one. Model
quantization instead results to deteriorate the energy efficiency of most
models, while prompt length and content have no statistically significant
impact. Improving image quality does not always come at the cost of a higher
energy consumption, with some of the models producing the highest quality
images also being among the most energy efficient ones.

</details>


### [246] [MLE-STAR: Machine Learning Engineering Agent via Search and Targeted Refinement](https://arxiv.org/abs/2506.15692)
*Jaehyun Nam,Jinsung Yoon,Jiefeng Chen,Jinwoo Shin,Sercan Ö. Arık,Tomas Pfister*

Main category: cs.LG

TL;DR: MLE-STAR是一种基于LLM的MLE代理新方法，通过结合外部知识和迭代细化策略，显著提升了模型选择和组件探索能力。


<details>
  <summary>Details</summary>
Motivation: 现有基于LLM的MLE代理依赖固有知识且探索策略粗糙，限制了任务特定模型选择和深度组件探索的能力。

Method: MLE-STAR利用搜索引擎检索外部知识形成初始方案，并通过针对特定ML组件的迭代探索策略进行细化，辅以消融研究指导。

Result: MLE-STAR在MLE-bench的Kaggle竞赛中44%的案例中获奖，显著优于其他方法。

Conclusion: MLE-STAR通过结合外部知识和精细化探索策略，显著提升了MLE代理的性能。

Abstract: Agents based on large language models (LLMs) for machine learning engineering
(MLE) can automatically implement ML models via code generation. However,
existing approaches to build such agents often rely heavily on inherent LLM
knowledge and employ coarse exploration strategies that modify the entire code
structure at once. This limits their ability to select effective task-specific
models and perform deep exploration within specific components, such as
experimenting extensively with feature engineering options. To overcome these,
we propose MLE-STAR, a novel approach to build MLE agents. MLE-STAR first
leverages external knowledge by using a search engine to retrieve effective
models from the web, forming an initial solution, then iteratively refines it
by exploring various strategies targeting specific ML components. This
exploration is guided by ablation studies analyzing the impact of individual
code blocks. Furthermore, we introduce a novel ensembling method using an
effective strategy suggested by MLE-STAR. Our experimental results show that
MLE-STAR achieves medals in 44% of the Kaggle competitions on the MLE-bench,
significantly outperforming the best alternative.

</details>


### [247] [Verifiable Safety Q-Filters via Hamilton-Jacobi Reachability and Multiplicative Q-Networks](https://arxiv.org/abs/2506.15693)
*Jiaxing Li,Hanjiang Hu,Yujie Yang,Changliu Liu*

Main category: cs.LG

TL;DR: 论文提出了一种基于Hamilton-Jacobi可达性分析的可验证无模型安全过滤器，解决了学习型安全过滤器缺乏形式化安全保证的问题。


<details>
  <summary>Details</summary>
Motivation: 学习型安全过滤器虽优于传统方法（如手工CBFs），但缺乏形式化安全保证。

Method: 1) 扩展Q值函数的可验证自一致性属性；2) 提出乘法Q网络结构以减少零子级集收缩问题；3) 开发验证管道，确保自一致性属性的正确验证。

Result: 该方法在四个标准安全控制基准测试中成功生成了形式化验证的无模型安全证书。

Conclusion: 该研究为学习型安全过滤器提供了形式化安全保证，解决了现有方法的局限性。

Abstract: Recent learning-based safety filters have outperformed conventional methods,
such as hand-crafted Control Barrier Functions (CBFs), by effectively adapting
to complex constraints. However, these learning-based approaches lack formal
safety guarantees. In this work, we introduce a verifiable model-free safety
filter based on Hamilton-Jacobi reachability analysis. Our primary
contributions include: 1) extending verifiable self-consistency properties for
Q value functions, 2) proposing a multiplicative Q-network structure to
mitigate zero-sublevel-set shrinkage issues, and 3) developing a verification
pipeline capable of soundly verifying these self-consistency properties. Our
proposed approach successfully synthesizes formally verified, model-free safety
certificates across four standard safe-control benchmarks.

</details>


### [248] [Development of a Multiprocessing Interface Genetic Algorithm for Optimising a Multilayer Perceptron for Disease Prediction](https://arxiv.org/abs/2506.15694)
*Iliyas Ibrahim Iliyas,Souley Boukari,Abdulsalam Yau Gital*

Main category: cs.LG

TL;DR: 该研究提出了一种结合非线性特征提取、分类和高效优化的框架，通过核主成分分析降维、多层感知机预测疾病状态，并利用改进的多处理遗传算法优化超参数，在多个数据集上取得了优于其他方法的准确率。


<details>
  <summary>Details</summary>
Motivation: 旨在通过集成非线性特征提取和高效优化方法，提升疾病预测的准确性和效率。

Method: 1. 使用核主成分分析降维；2. 多层感知机预测疾病状态；3. 改进的多处理遗传算法优化超参数。

Result: 在乳腺癌、帕金森病和慢性肾病数据集上分别达到99.12%、94.87%和100%的准确率，优于其他优化方法。

Conclusion: 该框架显著提升了疾病预测的准确性和效率，尤其在并行优化方面表现出色。

Abstract: This study introduces a framework that integrates nonlinear feature
extraction, classification, and efficient optimization. First, kernel principal
component analysis with a radial basis function kernel reduces dimensionality
while preserving 95% of the variance. Second, a multilayer perceptron (MLP)
learns to predict disease status. Finally, a modified multiprocessing genetic
algorithm (MIGA) optimizes MLP hyperparameters in parallel over ten
generations. We evaluated this approach on three datasets: the Wisconsin
Diagnostic Breast Cancer dataset, the Parkinson's Telemonitoring dataset, and
the chronic kidney disease dataset. The MLP tuned by the MIGA achieved the best
accuracy of 99.12% for breast cancer, 94.87% for Parkinson's disease, and 100%
for chronic kidney disease. These results outperform those of other methods,
such as grid search, random search, and Bayesian optimization. Compared with a
standard genetic algorithm, kernel PCA revealed nonlinear relationships that
improved classification, and the MIGA's parallel fitness evaluations reduced
the tuning time by approximately 60%. The genetic algorithm incurs high
computational cost from sequential fitness evaluations, but our multiprocessing
interface GA (MIGA) parallelizes this step, slashing the tuning time and
steering the MLP toward the best accuracy score of 99.12%, 94.87%, and 100% for
breast cancer, Parkinson's disease, and CKD, respectively.

</details>


### [249] [Early Attentive Sparsification Accelerates Neural Speech Transcription](https://arxiv.org/abs/2506.15912)
*Zifei Xu,Sayeh Sharify,Hesham Mostafa,Tristan Webb,Wanzin Yazar,Xin Wang*

Main category: cs.LG

TL;DR: 通过时间域信号稀疏化加速神经语音转录，利用Transformer音频编码器的自注意力机制可解释性，在Whisper模型上进行架构搜索，实现1.6倍运行时加速。


<details>
  <summary>Details</summary>
Motivation: 语音音频信号高度可压缩，因此希望通过早期稀疏化加速神经语音转录。

Method: 在Whisper模型上进行系统架构搜索，探索稀疏化阶段（特定编码层）和压缩比（稀疏度）的联合空间。

Result: 最佳方案在1%准确率下降内，选择早期编码阶段将隐藏状态稀疏化至40-60%，实现1.6倍运行时加速。

Conclusion: 早期稀疏化可显著加速语音转录，且无需微调。

Abstract: Transformer-based neural speech processing has achieved state-of-the-art
performance. Since speech audio signals are known to be highly compressible,
here we seek to accelerate neural speech transcription by time-domain signal
sparsification early in the neural encoding stage, taking advantage of the
interpretability of the self-attention mechanism in transformer audio encoders.
With the Whisper family of models, we perform a systematic architecture search
over the joint space of sparsification stage (a certain encoder layer) and
compression ratio (sparsity). We found that the best resulting solutions under
1% accuracy degradation choose to sparsify the hidden state to 40-60% sparsity
at an early encoding stage, and thereby achieve up to 1.6x runtime acceleration
in English speech transcription tasks on Nvidia GPUs without any fine-tuning.

</details>


### [250] [SimuGen: Multi-modal Agentic Framework for Constructing Block Diagram-Based Simulation Models](https://arxiv.org/abs/2506.15695)
*Xinxing Ren,Qianbo Zang,Zekun Guo*

Main category: cs.LG

TL;DR: 论文提出SimuGen框架，通过多模态代理方法解决LLM在生成Simulink模型时的不足，结合视觉图表和领域知识生成可靠代码。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLM）在数学推理和代码生成方面表现优异，但在Simulink模型生成领域表现不佳，主要因缺乏相关预训练数据。

Method: 提出SimuGen框架，通过协调多个专业代理（如调查员、代码生成器等）并结合领域知识库，实现多模态Simulink代码生成。

Result: SimuGen能够生成准确、可解释且可复现的Simulink模拟代码。

Conclusion: SimuGen通过多模态协作设计，有效解决了LLM在Simulink模型生成中的局限性。

Abstract: Recent advances in large language models (LLMs) have shown impressive
performance in mathematical reasoning and code generation. However, LLMs still
struggle in the simulation domain, particularly in generating Simulink models,
which are essential tools in engineering and scientific research. Our
preliminary experiments indicate that LLM agents often fail to produce reliable
and complete Simulink simulation code from text-only inputs, likely due to the
lack of Simulink-specific data in their pretraining. To address this challenge,
we propose SimuGen, a multimodal agent-based framework that automatically
generates accurate Simulink simulation code by leveraging both the visual
Simulink diagram and domain knowledge. SimuGen coordinates several specialized
agents, including an investigator, unit test reviewer, code generator,
executor, debug locator, and report writer, supported by a domain-specific
knowledge base. This collaborative and modular design enables interpretable,
robust, and reproducible Simulink simulation generation. Our source code is
publicly available at https://github.com/renxinxing123/SimuGen_beta.

</details>


### [251] [CoC: Chain-of-Cancer based on Cross-Modal Autoregressive Traction for Survival Prediction](https://arxiv.org/abs/2506.15696)
*Haipeng Zhou,Sicheng Yang,Sihan Yang,Jing Qin,Lei Chen,Lei Zhu*

Main category: cs.LG

TL;DR: 论文提出了一种名为Chain-of-Cancer（CoC）的多模态框架，结合临床数据和语言描述，用于癌症患者的生存预测。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要依赖病理学和基因组数据，忽略了表观遗传变化（如甲基化数据）和语言描述的作用。

Method: 提出CoC框架，通过intra-learning和inter-learning结合临床数据和语言提示，使用Autoregressive Mutual Traction模块实现多模态协同表示。

Result: 在五个公开癌症数据集上验证了方法的有效性，取得了最先进的结果。

Conclusion: CoC框架通过多模态联合学习，显著提升了生存预测性能。

Abstract: Survival prediction aims to evaluate the risk level of cancer patients.
Existing methods primarily rely on pathology and genomics data, either
individually or in combination. From the perspective of cancer pathogenesis,
epigenetic changes, such as methylation data, could also be crucial for this
task. Furthermore, no previous endeavors have utilized textual descriptions to
guide the prediction. To this end, we are the first to explore the use of four
modalities, including three clinical modalities and language, for conducting
survival prediction. In detail, we are motivated by the Chain-of-Thought (CoT)
to propose the Chain-of-Cancer (CoC) framework, focusing on intra-learning and
inter-learning. We encode the clinical data as the raw features, which remain
domain-specific knowledge for intra-learning. In terms of inter-learning, we
use language to prompt the raw features and introduce an Autoregressive Mutual
Traction module for synergistic representation. This tailored framework
facilitates joint learning among multiple modalities. Our approach is evaluated
across five public cancer datasets, and extensive experiments validate the
effectiveness of our methods and proposed designs, leading to producing \sota
results. Codes will be released.

</details>


### [252] [Global Context-aware Representation Learning for Spatially Resolved Transcriptomics](https://arxiv.org/abs/2506.15698)
*Yunhak Oh,Junseok Lee,Yeongmin Kim,Sangwoo Seo,Namkyeong Lee,Chanyoung Park*

Main category: cs.LG

TL;DR: Spotscape提出了一种新框架，通过Similarity Telescope模块和相似性缩放策略，解决了现有图方法在空间域边界附近表现不佳的问题。


<details>
  <summary>Details</summary>
Motivation: 现有图方法在空间域边界附近表现不佳，因为它们过于依赖相邻点而忽略了全局关系。

Method: 引入Similarity Telescope模块捕捉全局关系，并提出相似性缩放策略以调节点间距离。

Result: Spotscape在单切片和多切片任务中表现优异。

Conclusion: Spotscape为空间转录组学提供了更有效的分析工具。

Abstract: Spatially Resolved Transcriptomics (SRT) is a cutting-edge technique that
captures the spatial context of cells within tissues, enabling the study of
complex biological networks. Recent graph-based methods leverage both gene
expression and spatial information to identify relevant spatial domains.
However, these approaches fall short in obtaining meaningful spot
representations, especially for spots near spatial domain boundaries, as they
heavily emphasize adjacent spots that have minimal feature differences from an
anchor node. To address this, we propose Spotscape, a novel framework that
introduces the Similarity Telescope module to capture global relationships
between multiple spots. Additionally, we propose a similarity scaling strategy
to regulate the distances between intra- and inter-slice spots, facilitating
effective multi-slice integration. Extensive experiments demonstrate the
superiority of Spotscape in various downstream tasks, including single-slice
and multi-slice scenarios. Our code is available at the following link: https:
//github.com/yunhak0/Spotscape.

</details>


### [253] [BLUR: A Benchmark for LLM Unlearning Robust to Forget-Retain Overlap](https://arxiv.org/abs/2506.15699)
*Shengyuan Hu,Neil Kale,Pratiksha Thaker,Yiwei Fu,Steven Wu,Virginia Smith*

Main category: cs.LG

TL;DR: 论文提出了BLUR基准，用于更真实地评估LLM的遗忘方法，发现现有方法在BLUR上表现不佳，强调了鲁棒评估的重要性。


<details>
  <summary>Details</summary>
Motivation: 现有LLM遗忘基准的遗忘和保留集差异过大，导致对遗忘方法效果的评估不准确，可能在实际部署中暴露未遗忘的知识。

Method: 提出了BLUR基准，扩展了评估任务，结合了遗忘/保留查询，并提供了不同难度的重新学习数据集。

Result: 现有方法在BLUR上表现显著下降，简单方法平均优于新方法。

Conclusion: BLUR基准揭示了现有方法的不足，强调了鲁棒评估的重要性，并指出了未来研究方向。

Abstract: Machine unlearning has the potential to improve the safety of large language
models (LLMs) by removing sensitive or harmful information post hoc. A key
challenge in unlearning involves balancing between forget quality (effectively
unlearning undesirable information) and retain quality (maintaining good
performance on other, general tasks). Unfortunately, as we show, current LLM
unlearning benchmarks contain highly disparate forget and retain sets --
painting a false picture of the effectiveness of LLM unlearning methods. This
can be particularly problematic because it opens the door for benign
perturbations, such as relearning attacks, to easily reveal supposedly
unlearned knowledge once models are deployed. To address this, we present
$\texttt{BLUR}$: a benchmark for LLM unlearning that provides more realistic
scenarios of forget-retain overlap. $\texttt{BLUR}$ significantly expands on
existing unlearning benchmarks by providing extended evaluation tasks, combined
forget/retain queries, and relearning datasets of varying degrees of
difficulty. Despite the benign nature of the queries considered, we find that
the performance of existing methods drops significantly when evaluated on
$\texttt{BLUR}$, with simple approaches performing better on average than more
recent methods. These results highlight the importance of robust evaluation and
suggest several important directions of future study. Our benchmark is publicly
available at: https://huggingface.co/datasets/forgelab/BLUR

</details>


### [254] [Contraction Actor-Critic: Contraction Metric-Guided Reinforcement Learning for Robust Path Tracking](https://arxiv.org/abs/2506.15700)
*Minjae Cho,Hiroyasu Tsukamoto,Huy Trong Tran*

Main category: cs.LG

TL;DR: 论文提出了一种将控制收缩度量（CCMs）与强化学习（RL）结合的方法，以解决CCMs在动态模型已知性和最优性方面的限制。


<details>
  <summary>Details</summary>
Motivation: CCMs虽然能保证闭环系统的增量稳定性，但缺乏轨迹最优性且对动态模型要求高，限制了其在复杂系统中的应用。

Method: 提出收缩演员-评论家（CAC）算法，通过强化学习自动学习收缩度量生成器和最优跟踪策略。

Result: CAC在模拟和真实机器人实验中表现优于基线方法，证明了其有效性。

Conclusion: 结合收缩理论与强化学习，CAC在自动化环境中提升了CCMs的能力，同时实现了长期最优性。

Abstract: Control contraction metrics (CCMs) provide a framework to co-synthesize a
controller and a corresponding contraction metric -- a positive-definite
Riemannian metric under which a closed-loop system is guaranteed to be
incrementally exponentially stable. However, the synthesized controller only
ensures that all the trajectories of the system converge to one single
trajectory and, as such, does not impose any notion of optimality across an
entire trajectory. Furthermore, constructing CCMs requires a known dynamics
model and non-trivial effort in solving an infinite-dimensional convex
feasibility problem, which limits its scalability to complex systems featuring
high dimensionality with uncertainty. To address these issues, we propose to
integrate CCMs into reinforcement learning (RL), where CCMs provide
dynamics-informed feedback for learning control policies that minimize
cumulative tracking error under unknown dynamics. We show that our algorithm,
called contraction actor-critic (CAC), formally enhances the capability of CCMs
to provide a set of contracting policies with the long-term optimality of RL in
a fully automated setting. Given a pre-trained dynamics model, CAC
simultaneously learns a contraction metric generator (CMG) -- which generates a
contraction metric -- and uses an actor-critic algorithm to learn an optimal
tracking policy guided by that metric. We demonstrate the effectiveness of our
algorithm relative to established baselines through extensive empirical
studies, including simulated and real-world robot experiments, and provide a
theoretical rationale for incorporating contraction theory into RL.

</details>


### [255] [Optimizing Multilingual Text-To-Speech with Accents & Emotions](https://arxiv.org/abs/2506.16310)
*Pranav Pawar,Akshansh Dwivedi,Jenish Boricha,Himanshu Gohil,Aditya Dubey*

Main category: cs.LG

TL;DR: 本文提出了一种新的TTS架构，结合口音和情感建模，特别针对印地语和印度英语，显著提升了口音准确性和情感识别能力。


<details>
  <summary>Details</summary>
Motivation: 当前TTS系统在多语言环境（尤其是印度语言）中难以正确合成口音和情感，本文旨在解决这一问题。

Method: 扩展Parler-TTS模型，引入语言特定音素对齐混合编码器-解码器架构、文化敏感情感嵌入层和动态口音代码切换。

Result: 口音准确性提升23.7%（WER从15.4%降至11.8%），情感识别准确率达85.3%，用户评价MOS为4.2/5。

Conclusion: 该系统通过可扩展的口音-情感解耦，为跨语言合成提供了可行性，适用于南亚教育科技和辅助软件。

Abstract: State-of-the-art text-to-speech (TTS) systems realize high naturalness in
monolingual environments, synthesizing speech with correct multilingual accents
(especially for Indic languages) and context-relevant emotions still poses
difficulty owing to cultural nuance discrepancies in current frameworks. This
paper introduces a new TTS architecture integrating accent along with
preserving transliteration with multi-scale emotion modelling, in particularly
tuned for Hindi and Indian English accent. Our approach extends the Parler-TTS
model by integrating A language-specific phoneme alignment hybrid
encoder-decoder architecture, and culture-sensitive emotion embedding layers
trained on native speaker corpora, as well as incorporating a dynamic accent
code switching with residual vector quantization. Quantitative tests
demonstrate 23.7% improvement in accent accuracy (Word Error Rate reduction
from 15.4% to 11.8%) and 85.3% emotion recognition accuracy from native
listeners, surpassing METTS and VECL-TTS baselines. The novelty of the system
is that it can mix code in real time - generating statements such as "Namaste,
let's talk about <Hindi phrase>" with uninterrupted accent shifts while
preserving emotional consistency. Subjective evaluation with 200 users reported
a mean opinion score (MOS) of 4.2/5 for cultural correctness, much better than
existing multilingual systems (p<0.01). This research makes cross-lingual
synthesis more feasible by showcasing scalable accent-emotion disentanglement,
with direct application in South Asian EdTech and accessibility software.

</details>


### [256] [Compiler-R1: Towards Agentic Compiler Auto-tuning with Reinforcement Learning](https://arxiv.org/abs/2506.15701)
*Haolin Pan,Hongyu Lin,Haoran Luo,Yang Liu,Kaichun Yao,Libo Zhang,Mingjie Xing,Yanjun Wu*

Main category: cs.LG

TL;DR: Compiler-R1是一个基于强化学习的框架，通过增强LLM能力优化编译器自动调优，解决了高质量推理数据集缺失和编译环境交互有限的问题。


<details>
  <summary>Details</summary>
Motivation: 当前编译器自动调优面临高质量推理数据集缺失和与编译环境交互有限的挑战。

Method: 引入Compiler-R1框架，包含高质量推理数据集和两阶段端到端强化学习训练流程，通过结果导向奖励实现高效探索和学习。

Result: 在七个数据集上平均减少8.46%的IR指令数，优于opt -Oz。

Conclusion: Compiler-R1展示了RL训练的LLM在编译器优化中的强大潜力。

Abstract: Compiler auto-tuning optimizes pass sequences to improve performance metrics
such as Intermediate Representation (IR) instruction count. Although recent
advances leveraging Large Language Models (LLMs) have shown promise in
automating compiler tuning, two significant challenges still remain: the
absence of high-quality reasoning datasets for agents training, and limited
effective interactions with the compilation environment. In this work, we
introduce Compiler-R1, the first reinforcement learning (RL)-driven framework
specifically augmenting LLM capabilities for compiler auto-tuning. Compiler-R1
features a curated, high-quality reasoning dataset and a novel two-stage
end-to-end RL training pipeline, enabling efficient environment exploration and
learning through an outcome-based reward. Extensive experiments across seven
datasets demonstrate Compiler-R1 achieving an average 8.46% IR instruction
count reduction compared to opt -Oz, showcasing the strong potential of
RL-trained LLMs for compiler optimization. Our code and datasets are publicly
available at https://github.com/Panhaolin2001/Compiler-R1.

</details>


### [257] [Minifinetuning: Low-Data Generation Domain Adaptation through Corrective Self-Distillation](https://arxiv.org/abs/2506.15702)
*Peter Belcak,Greg Heinrich,Jan Kautz,Pavlo Molchanov*

Main category: cs.LG

TL;DR: 论文提出了一种名为minifinetuning（MFT）的方法，用于在低数据环境下减少语言模型微调时的过拟合和泛化性能下降问题，效果显著优于标准微调方法。


<details>
  <summary>Details</summary>
Motivation: 语言模型在新领域的微调通常会导致其泛化性能下降，尤其是在数据有限的情况下。

Method: MFT通过样本级的自我蒸馏技术，在不依赖预训练数据的情况下，有效减少过拟合，并在低至500样本的数据量下仍保持鲁棒性。

Result: MFT在多种模型和领域中表现出2-10倍优于标准微调的性能，且能与参数高效微调方法结合使用。

Conclusion: MFT是一种高效、鲁棒的语言模型微调方法，特别适用于数据稀缺的场景。

Abstract: Finetuning language models for a new domain inevitably leads to the
deterioration of their general performance. This becomes more pronounced the
more limited the finetuning data resource.
  We introduce minifinetuning (MFT), a method for language model domain
adaptation that considerably reduces the effects of overfitting-induced
degeneralization in low-data settings and which does so in the absence of any
pre-training data for replay. MFT demonstrates 2-10x more favourable
specialization-to-degeneralization ratios than standard finetuning across a
wide range of models and domains and exhibits an intrinsic robustness to
overfitting when data in the new domain is scarce and down to as little as 500
samples.
  Employing corrective self-distillation that is individualized on the sample
level, MFT outperforms parameter-efficient finetuning methods, demonstrates
replay-like degeneralization mitigation properties, and is composable with
either for a combined effect.

</details>


### [258] [Federated Incomplete Multi-view Clustering with Globally Fused Graph Guidance](https://arxiv.org/abs/2506.15703)
*Guoqing Chao,Zhenghao Zhang,Lei Meng,Jie Wen,Dianhui Chu*

Main category: cs.LG

TL;DR: 提出了一种新的联邦不完全多视图聚类方法FIMCFG，通过全局融合图指导解决现有方法在特征提取和缺失数据问题上的不足。


<details>
  <summary>Details</summary>
Motivation: 现有联邦多视图聚类方法仅使用全局伪标签指导聚类，未能充分利用全局信息，且对缺失数据问题研究较少。

Method: 设计了双头图卷积编码器提取全局和视图特定特征，通过融合图指导特征融合和伪标签监督聚类。

Result: 实验证明FIMCFG在效果和性能上优于现有方法。

Conclusion: FIMCFG有效解决了联邦多视图聚类中的全局信息利用和缺失数据问题，具有优越性。

Abstract: Federated multi-view clustering has been proposed to mine the valuable
information within multi-view data distributed across different devices and has
achieved impressive results while preserving the privacy. Despite great
progress, most federated multi-view clustering methods only used global
pseudo-labels to guide the downstream clustering process and failed to exploit
the global information when extracting features. In addition, missing data
problem in federated multi-view clustering task is less explored. To address
these problems, we propose a novel Federated Incomplete Multi-view Clustering
method with globally Fused Graph guidance (FIMCFG). Specifically, we designed a
dual-head graph convolutional encoder at each client to extract two kinds of
underlying features containing global and view-specific information.
Subsequently, under the guidance of the fused graph, the two underlying
features are fused into high-level features, based on which clustering is
conducted under the supervision of pseudo-labeling. Finally, the high-level
features are uploaded to the server to refine the graph fusion and
pseudo-labeling computation. Extensive experimental results demonstrate the
effectiveness and superiority of FIMCFG. Our code is publicly available at
https://github.com/PaddiHunter/FIMCFG.

</details>


### [259] [Learn from the Past: Fast Sparse Indexing for Large Language Model Decoding](https://arxiv.org/abs/2506.15704)
*Feiyu Yao,Qian Wang*

Main category: cs.LG

TL;DR: LFPS是一种基于历史注意力模式动态构建稀疏索引的加速方法，显著提升了长上下文LLM的解码效率。


<details>
  <summary>Details</summary>
Motivation: 随着LLM支持更长的上下文，KV缓存的存储需求成为GPU内存和PCIe带宽的瓶颈，稀疏注意力机制虽能缓解但索引计算开销大。

Method: LFPS通过捕捉历史解码中的垂直和斜线模式，动态预测当前步骤的Top-k索引，并采用位置扩展策略。

Result: 在LongBench-RULER基准测试中，LFPS比全注意力快22.8倍，比精确Top-k检索快9.6倍，且保持生成准确性。

Conclusion: LFPS为长上下文LLM推理提供了一种高效且实用的解码优化方案。

Abstract: As large language models (LLMs) continue to support increasingly longer
contexts, the memory demand for key-value (KV) caches during decoding grows
rapidly, becoming a critical bottleneck in both GPU memory capacity and PCIe
bandwidth. Sparse attention mechanisms alleviate this issue by computing
attention weights only for selected key-value pairs. However, their indexing
computation typically requires traversing all key vectors, resulting in
significant computational and data transfer overhead. To reduce the cost of
index retrieval, existing methods often treat each decoding step as an
independent process, failing to exploit the temporal correlations embedded in
historical decoding information. To this end, we propose LFPS(Learn From the
Past for Sparse Indexing), an acceleration method that dynamically constructs
sparse indexing candidates based on historical attention patterns. LFPS
captures two prevalent trends in decoder attention -vertical patterns
(attending to fixed positions) and slash patterns (attending to relative
positions) -and incorporates a positional expansion strategy to effectively
predict the Top-k indices for the current step. We validate LFPS on challenging
long-context benchmarks such as LongBench-RULER, using Llama-3.1-8B-Instruct as
the base model. Experimental results show that LFPS achieves up to 22.8$\times$
speedup over full attention and 9.6$\times$ speedup over exact Top-k retrieval
on an RTX 4090 GPU and a single CPU core of a Xeon Gold 6430, respectively,
while preserving generation accuracy. These results demonstrate that LFPS
offers a practical and efficient solution for decoding optimization in
long-context LLM inference.

</details>


### [260] [Generalisation Bounds of Zero-Shot Economic Forecasting using Time Series Foundation Models](https://arxiv.org/abs/2506.15705)
*Jittarin Jetwiriyanon,Teo Susnjak,Surangika Ranathunga*

Main category: cs.LG

TL;DR: 研究探讨了时间序列基础模型（TSFMs）在宏观经济指标零样本预测中的能力，发现其在稳定经济条件下表现优异，但在快速冲击期性能下降。


<details>
  <summary>Details</summary>
Motivation: 传统经济计量模型需要大量训练数据和定制化，而TSFMs可能提供一种无需微调的高效替代方案。

Method: 在数据稀缺和结构断裂条件下，对三种先进TSFM（Chronos、TimeGPT和Moirai）进行严格回测。

Result: TSFMs能够内化经济动态、适应制度变化并提供良好的不确定性估计，性能与多变量模型相当。

Conclusion: TSFMs在稳定经济条件下表现优异，但在快速冲击期需谨慎使用，为零样本部署提供了实践指导。

Abstract: This study investigates zero-shot forecasting capabilities of Time Series
Foundation Models (TSFMs) for macroeconomic indicators. We apply TSFMs to
forecasting economic indicators under univariate conditions, bypassing the need
for train bespoke econometric models using and extensive training datasets. Our
experiments were conducted on a case study dataset, without additional
customisation. We rigorously back-tested three state-of-the-art TSFMs (Chronos,
TimeGPT and Moirai) under data-scarce conditions and structural breaks. Our
results demonstrate that appropriately engineered TSFMs can internalise rich
economic dynamics, accommodate regime shifts, and deliver well-behaved
uncertainty estimates out of the box, while matching state-of-the-art
multivariate models on this domain. Our findings suggest that, without any
fine-tuning, TSFMs can match or exceed classical models during stable economic
conditions. However, they are vulnerable to degradation in performances during
periods of rapid shocks. The findings offer guidance to practitioners on when
zero-shot deployments are viable for macroeconomic monitoring and strategic
planning.

</details>


### [261] [MDPO: Multi-Granularity Direct Preference Optimization for Mathematical Reasoning](https://arxiv.org/abs/2506.15706)
*Yunze Lin*

Main category: cs.LG

TL;DR: 论文提出了一种多粒度直接偏好优化（MDPO）方法，通过三个粒度优化LLMs的数学推理能力，实验表明其在GSM8K和MATH数据集上优于DPO及其变体。


<details>
  <summary>Details</summary>
Motivation: 数学推理对LLMs是一个挑战，现有方法如DPO在长链推理中效果有限，无法有效抑制错误输出。

Method: 提出MDPO方法，在Solution2Solution、Inference2Inference和Step2Step三个粒度上优化LLMs的数学推理能力，并统一训练目标与生成指标对齐。

Result: 在Qwen2和Llama3模型上，GSM8K和MATH数据集的性能分别提升1.7%/0.9%和2.3%/1.2%。

Conclusion: MDPO方法有效提升了LLMs的数学推理能力，且提供了无需人工标注的训练数据构建流程。

Abstract: Mathematical reasoning presents a significant challenge for Large Language
Models (LLMs) as it requires ensuring the correctness of each reasoning step.
Researchers have been strengthening the mathematical reasoning abilities of
LLMs through supervised fine-tuning, but due to the inability to suppress
incorrect outputs, illusions can easily arise. Recently, Direct Preference
Optimization (DPO) has been widely adopted for aligning human intent by using
preference data to prevent LLMs from generating incorrect outputs. However, it
has shown limited benefits in long-chain mathematical reasoning, mainly because
DPO struggles to effectively capture the differences between accepted and
rejected answers from preferences in long-chain data. The inconsistency between
DPO training and LLMs' generation metrics also affects the effectiveness of
suppressing incorrect outputs. We propose the Multi-Granularity Direct
Preference Optimization (MDPO) method, optimizing the mathematical reasoning of
LLMs at three granularities: Solution2Solution, Inference2Inference, and
Step2Step. Solution2Solution focuses on the correctness of entire long-chain
reasoning; Inference2Inference concentrates on logical reasoning between steps;
Step2Step corrects computational errors in steps, enhancing the computational
capabilities of LLMs. Additionally, we unify the training objectives of the
three granularities to align with the generation metrics. We conducted
experiments on the open-source models Qwen2 and Llama3, achieving improvements
of 1.7% and 0.9% on the GSM8K dataset, and 2.3% and 1.2% on the MATH dataset,
outperforming DPO and other DPO variant methods. Furthermore, we also provide a
pipeline for constructing MDPO training data that is simple and does not
require manual annotation costs.

</details>


### [262] [Every Rollout Counts: Optimal Resource Allocation for Efficient Test-Time Scaling](https://arxiv.org/abs/2506.15707)
*Xinglin Wang,Yiwei Li,Shaoxiong Feng,Peiwen Yuan,Yueqi Zhang,Jiayi Shi,Chuyi Tan,Boyuan Pan,Yao Hu,Kan Li*

Main category: cs.LG

TL;DR: TTS通过搜索优化LLM推理性能，但现有方法因资源分配不均效率低下。DORA提出方向级资源分配，实验证明其优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 探索如何更高效地分配固定推理预算以提升LLM性能。

Method: 将测试时搜索建模为资源分配问题，提出DORA方法，优化方向级资源分配。

Result: DORA在MATH500等数学推理基准上表现最优，计算成本相近。

Conclusion: DORA为LLM测试时优化提供了更高效的方法，推动了TTS领域发展。

Abstract: Test-Time Scaling (TTS) improves the performance of Large Language Models
(LLMs) by using additional inference-time computation to explore multiple
reasoning paths through search. Yet how to allocate a fixed rollout budget most
effectively during search remains underexplored, often resulting in inefficient
use of compute at test time. To bridge this gap, we formulate test-time search
as a resource allocation problem and derive the optimal allocation strategy
that maximizes the probability of obtaining a correct solution under a fixed
rollout budget. Within this formulation, we reveal a core limitation of
existing search methods: solution-level allocation tends to favor reasoning
directions with more candidates, leading to theoretically suboptimal and
inefficient use of compute. To address this, we propose Direction-Oriented
Resource Allocation (DORA), a provably optimal method that mitigates this bias
by decoupling direction quality from candidate count and allocating resources
at the direction level. To demonstrate DORA's effectiveness, we conduct
extensive experiments on challenging mathematical reasoning benchmarks
including MATH500, AIME2024, and AIME2025. The empirical results show that DORA
consistently outperforms strong baselines with comparable computational cost,
achieving state-of-the-art accuracy. We hope our findings contribute to a
broader understanding of optimal TTS for LLMs.

</details>


### [263] [Refined Causal Graph Structure Learning via Curvature for Brain Disease Classification](https://arxiv.org/abs/2506.15708)
*Falih Gozi Febrinanto,Adonia Simango,Chengpei Xu,Jingjing Zhou,Jiangang Ma,Sonika Tyagi,Feng Xia*

Main category: cs.LG

TL;DR: 论文提出了一种名为CGB的新框架，通过因果发现方法、传递熵和几何曲率策略建模脑网络，显著提升了脑疾病分类性能。


<details>
  <summary>Details</summary>
Motivation: 现有图神经网络（GNNs）未充分考虑脑区间的因果关系，而这对信号间的因果交互至关重要。

Method: CGB结合因果发现方法、传递熵和几何曲率策略，建模脑网络并进行图重连。

Result: 实验表明，CGB在脑疾病分类任务中的F1分数优于现有方法。

Conclusion: CGB通过建模因果关系和优化图结构，显著提升了脑疾病分类性能。

Abstract: Graph neural networks (GNNs) have been developed to model the relationship
between regions of interest (ROIs) in brains and have shown significant
improvement in detecting brain diseases. However, most of these frameworks do
not consider the intrinsic relationship of causality factor between brain ROIs,
which is arguably more essential to observe cause and effect interaction
between signals rather than typical correlation values. We propose a novel
framework called CGB (Causal Graphs for Brains) for brain disease
classification/detection, which models refined brain networks based on the
causal discovery method, transfer entropy, and geometric curvature strategy.
CGB unveils causal relationships between ROIs that bring vital information to
enhance brain disease classification performance. Furthermore, CGB also
performs a graph rewiring through a geometric curvature strategy to refine the
generated causal graph to become more expressive and reduce potential
information bottlenecks when GNNs model it. Our extensive experiments show that
CGB outperforms state-of-the-art methods in classification tasks on brain
disease datasets, as measured by average F1 scores.

</details>


### [264] [Studying and Improving Graph Neural Network-based Motif Estimation](https://arxiv.org/abs/2506.15709)
*Pedro C. Vieira,Miguel E. P. Silva,Pedro Manuel Pinto Ribeiro*

Main category: cs.LG

TL;DR: 该论文提出了一种直接估计网络模体显著性分布（SP）的方法，避免了传统子图频率估计的限制，并通过多目标回归优化了可解释性、稳定性和扩展性。


<details>
  <summary>Details</summary>
Motivation: 现有图神经网络（GNNs）在网络模体SP预测方面缺乏基准研究，传统子图频率估计方法存在局限性。

Method: 将SP估计重新定义为独立于子图频率的任务，采用多目标回归方法，优化可解释性和扩展性。

Result: 实验表明，1-WL受限模型难以精确估计SP，但能通过比较预测SP与合成生成器的SP来近似网络生成过程。

Conclusion: 直接SP估计有助于突破子图计数在模体估计中的理论限制，为GNNs在模体分析中的应用提供了新方向。

Abstract: Graph Neural Networks (GNNs) are a predominant method for graph
representation learning. However, beyond subgraph frequency estimation, their
application to network motif significance-profile (SP) prediction remains
under-explored, with no established benchmarks in the literature. We propose to
address this problem, framing SP estimation as a task independent of subgraph
frequency estimation. Our approach shifts from frequency counting to direct SP
estimation and modulates the problem as multitarget regression. The
reformulation is optimised for interpretability, stability and scalability on
large graphs. We validate our method using a large synthetic dataset and
further test it on real-world graphs. Our experiments reveal that 1-WL limited
models struggle to make precise estimations of SPs. However, they can
generalise to approximate the graph generation processes of networks by
comparing their predicted SP with the ones originating from synthetic
generators. This first study on GNN-based motif estimation also hints at how
using direct SP estimation can help go past the theoretical limitations that
motif estimation faces when performed through subgraph counting.

</details>


### [265] [RAST: Reasoning Activation in LLMs via Small-model Transfer](https://arxiv.org/abs/2506.15710)
*Siru Ouyang,Xinyu Zhu,Zilin Xiao,Minhao Jiang,Yu Meng,Jiawei Han*

Main category: cs.LG

TL;DR: 论文提出RAST方法，通过从小型RL训练模型转移概率调整到大型模型，提升推理能力，同时降低计算成本。


<details>
  <summary>Details</summary>
Motivation: 尽管强化学习（RL）能提升大语言模型（LLMs）的推理能力，但其大规模应用资源消耗巨大。研究发现RL并未赋予模型新知识，而是调整输出分布，因此假设这种调整与模型规模无关。

Method: 提出RAST方法，通过分析RL诱导的输出分布，将小型RL训练模型的概率调整注入大型模型。

Result: 实验表明，RAST显著提升基础模型的推理能力，且GPU内存需求更低，有时性能甚至优于直接RL训练的模型。

Conclusion: RAST揭示了RL驱动推理的本质，并提供了无需高计算成本即可扩展其优势的实用策略。

Abstract: Reinforcement learning (RL) has become a powerful approach for improving the
reasoning capabilities of large language models (LLMs), as evidenced by recent
successes such as OpenAI's o1 and Deepseek-R1. However, applying RL at scale
remains intimidatingly resource-intensive, requiring multiple model copies and
extensive GPU workloads. On the other hand, while being powerful, recent
studies suggest that RL does not fundamentally endow models with new knowledge;
rather, it primarily reshapes the model's output distribution to activate
reasoning capabilities latent in the base model. Building on this insight, we
hypothesize that the changes in output probabilities induced by RL are largely
model-size invariant, opening the door to a more efficient paradigm: training a
small model with RL and transferring its induced probability shifts to larger
base models. To verify our hypothesis, we conduct a token-level analysis of
decoding trajectories and find high alignment in RL-induced output
distributions across model scales, validating our hypothesis. Motivated by
this, we propose RAST, a simple yet effective method that transfers reasoning
behaviors by injecting RL-induced probability adjustments from a small
RL-trained model into larger models. Experiments across multiple mathematical
reasoning benchmarks show that RAST substantially and consistently enhances the
reasoning capabilities of base models while requiring significantly lower GPU
memory than direct RL training, sometimes even yielding better performance than
the RL-trained counterparts. Our findings offer new insights into the nature of
RL-driven reasoning and practical strategies for scaling its benefits without
incurring its full computational cost. The project page of RAST is available at
https://ozyyshr.github.io/RAST/.

</details>


### [266] [Shadow defense against gradient inversion attack in federated learning](https://arxiv.org/abs/2506.15711)
*Le Jiang,Liyan Ma,Guang Yang*

Main category: cs.LG

TL;DR: 该论文提出了一种针对联邦学习中梯度反转攻击的防御框架，通过影子模型和针对性噪声注入保护敏感信息，同时最小化对模型性能的影响。


<details>
  <summary>Details</summary>
Motivation: 联邦学习在隐私保护分布式训练中具有潜力，但梯度反转攻击可能导致隐私泄露，现有防御方法缺乏针对性。

Method: 利用影子模型识别敏感区域，并进行针对性噪声注入。

Result: 在ChestXRay和EyePACS数据集上，PSNR和SSIM指标显著优于无防御情况，且模型性能影响小于1%。

Conclusion: 该框架在多种医学图像上验证了其泛化能力，并能稳定提升防御效果。

Abstract: Federated learning (FL) has emerged as a transformative framework for
privacy-preserving distributed training, allowing clients to collaboratively
train a global model without sharing their local data. This is especially
crucial in sensitive fields like healthcare, where protecting patient data is
paramount. However, privacy leakage remains a critical challenge, as the
communication of model updates can be exploited by potential adversaries.
Gradient inversion attacks (GIAs), for instance, allow adversaries to
approximate the gradients used for training and reconstruct training images,
thus stealing patient privacy. Existing defense mechanisms obscure gradients,
yet lack a nuanced understanding of which gradients or types of image
information are most vulnerable to such attacks. These indiscriminate
calibrated perturbations result in either excessive privacy protection
degrading model accuracy, or insufficient one failing to safeguard sensitive
information. Therefore, we introduce a framework that addresses these
challenges by leveraging a shadow model with interpretability for identifying
sensitive areas. This enables a more targeted and sample-specific noise
injection. Specially, our defensive strategy achieves discrepancies of 3.73 in
PSNR and 0.2 in SSIM compared to the circumstance without defense on the
ChestXRay dataset, and 2.78 in PSNR and 0.166 in the EyePACS dataset. Moreover,
it minimizes adverse effects on model performance, with less than 1\% F1
reduction compared to SOTA methods. Our extensive experiments, conducted across
diverse types of medical images, validate the generalization of the proposed
framework. The stable defense improvements for FedAvg are consistently over
1.5\% times in LPIPS and SSIM. It also offers a universal defense against
various GIA types, especially for these sensitive areas in images.

</details>


### [267] [BatteryBERT for Realistic Battery Fault Detection Using Point-Masked Signal Modeling](https://arxiv.org/abs/2506.15712)
*Songqi Zhou,Ruixue Liu,Yixing Wang,Jia Lu,Benben Jiang*

Main category: cs.LG

TL;DR: 论文提出了一种基于BERT风格预训练的新框架，用于锂离子电池故障检测，通过定制的时间序列表示模块和点级掩码信号建模任务，显著提升了分类准确性和表示质量。


<details>
  <summary>Details</summary>
Motivation: 锂离子电池的精确故障检测对电动汽车和储能系统的安全可靠运行至关重要，但现有方法难以捕捉复杂的时间依赖性且无法充分利用未标记数据。

Method: 提出了一种新框架，扩展了标准BERT架构，包括定制的时间序列到令牌表示模块和点级掩码信号建模（point-MSM）预训练任务，支持自监督学习。

Result: 在大规模真实数据集上的实验表明，该框架显著提升了表示质量和分类准确性，AUROC达到0.945，优于现有方法。

Conclusion: 验证了BERT风格预训练在时间序列故障检测中的有效性。

Abstract: Accurate fault detection in lithium-ion batteries is essential for the safe
and reliable operation of electric vehicles and energy storage systems.
However, existing methods often struggle to capture complex temporal
dependencies and cannot fully leverage abundant unlabeled data. Although large
language models (LLMs) exhibit strong representation capabilities, their
architectures are not directly suited to the numerical time-series data common
in industrial settings. To address these challenges, we propose a novel
framework that adapts BERT-style pretraining for battery fault detection by
extending the standard BERT architecture with a customized time-series-to-token
representation module and a point-level Masked Signal Modeling (point-MSM)
pretraining task tailored to battery applications. This approach enables
self-supervised learning on sequential current, voltage, and other
charge-discharge cycle data, yielding distributionally robust, context-aware
temporal embeddings. We then concatenate these embeddings with battery metadata
and feed them into a downstream classifier for accurate fault classification.
Experimental results on a large-scale real-world dataset show that models
initialized with our pretrained parameters significantly improve both
representation quality and classification accuracy, achieving an AUROC of 0.945
and substantially outperforming existing approaches. These findings validate
the effectiveness of BERT-style pretraining for time-series fault detection.

</details>


### [268] [An application of machine learning to the motion response prediction of floating assets](https://arxiv.org/abs/2506.15713)
*Michael T. M. B. Morris-Thomas,Marius Martens*

Main category: cs.LG

TL;DR: 本文提出了一种监督机器学习方法，用于预测浮动海上资产在随机海洋条件下的非线性运动响应，显著优于传统频域方法。


<details>
  <summary>Details</summary>
Motivation: 实时预测浮动海上资产在随机海洋条件下的行为是海上工程中的重大挑战，传统方法在极端海况和非线性响应中表现不佳。

Method: 采用多元回归的监督机器学习方法，结合梯度提升集成方法和自定义被动迎风求解器，训练了约100万样本和100个特征。

Result: 模型对关键系泊参数的平均预测误差小于5%，船舶航向精度在2.5度以内，显著优于传统频域方法。

Conclusion: 该框架已成功应用于实际设施，证明了其在实时船舶监测和海上操作决策中的有效性。

Abstract: The real-time prediction of floating offshore asset behavior under stochastic
metocean conditions remains a significant challenge in offshore engineering.
While traditional empirical and frequency-domain methods work well in benign
conditions, they struggle with both extreme sea states and nonlinear responses.
This study presents a supervised machine learning approach using multivariate
regression to predict the nonlinear motion response of a turret-moored vessel
in 400 m water depth. We developed a machine learning workflow combining a
gradient-boosted ensemble method with a custom passive weathervaning solver,
trained on approximately $10^6$ samples spanning 100 features. The model
achieved mean prediction errors of less than 5% for critical mooring parameters
and vessel heading accuracy to within 2.5 degrees across diverse metocean
conditions, significantly outperforming traditional frequency-domain methods.
The framework has been successfully deployed on an operational facility,
demonstrating its efficacy for real-time vessel monitoring and operational
decision-making in offshore environments.

</details>


### [269] [Adaptive Two Sided Laplace Transforms: A Learnable, Interpretable, and Scalable Replacement for Self-Attention](https://arxiv.org/abs/2506.15714)
*Andrew Kiruluta*

Main category: cs.LG

TL;DR: 提出了一种可学习的两侧短时拉普拉斯变换（STLT）机制，替代传统自注意力，实现动态调整衰减率和频率响应，支持超长序列建模。


<details>
  <summary>Details</summary>
Motivation: 传统自注意力在长序列建模中存在计算瓶颈，需要更高效且可解释的替代方案。

Method: 引入可训练参数的拉普拉斯节点，结合快速递归卷积和FFT计算，动态调整节点数量。

Result: 在语言建模、机器翻译和长文档问答任务中表现优异，支持超过100k token的上下文长度。

Conclusion: STLT结合了可解释性、扩展性和鲁棒性，为超长序列建模提供了新途径。

Abstract: We propose an innovative, learnable two-sided short-time Laplace transform
(STLT) mechanism to supplant the traditional self attention in
transformer-based LLMs. Our STLT introduces trainable parameters for each
Laplace node, enabling end-to-end learning of decay rates , oscillatory
frequencies, and window bandwidth T. This flexibility allows the model to
dynamically adapt token relevance half lives and frequency responses during
training. By selecting S learnable nodes and leveraging fast recursive
convolution, we achieve an effective complexity of in time and memory. We
further incorporate an efficient FFT-based computation of the relevance matrix
and an adaptive node allocation mechanism to dynamically adjust the number of
active Laplace nodes. Empirical results on language modeling (WikiText\-103,
Project Gutenberg), machine translation (WMT'14 En\-De), and long document
question answering (NarrativeQA) demonstrate that our learnable STLT achieves
perplexities and scores on par with or better than existing efficient
transformers while naturally extending to context lengths exceeding 100k tokens
or more limited only by available hardware. Ablation studies confirm the
importance of learnable parameters and adaptive node allocation. The proposed
approach combines interpretability, through explicit decay and frequency
parameters, with scalability and robustness, offering a pathway towards
ultra-long-sequence language modeling without the computational bottleneck of
self-attention.

</details>


### [270] [NeuronSeek: On Stability and Expressivity of Task-driven Neurons](https://arxiv.org/abs/2506.15715)
*Hanyu Pei,Jing-Xiao Liao,Qibin Zhao,Ting Gao,Shijun Zhang,Xiaoge Zhang,Feng-Lei Fan*

Main category: cs.LG

TL;DR: 论文提出了一种基于张量分解（TD）的任务驱动神经元优化方法（NeuronSeek-TD），替代了符号回归（SR），提升了稳定性和收敛速度，并提供了理论保证。


<details>
  <summary>Details</summary>
Motivation: 受人类大脑中不同神经元处理不同任务的启发，探索通过优化神经元设计来提升网络性能。

Method: 使用张量分解（TD）替代符号回归（SR）来发现最优神经元结构，并通过修改聚合函数增强网络表达能力。

Result: 实验表明，NeuronSeek-TD在稳定性和性能上优于现有方法，并在多个基准测试中表现优异。

Conclusion: NeuronSeek-TD为任务驱动神经元设计提供了更稳定、高效的解决方案，并具有理论支持。

Abstract: Drawing inspiration from our human brain that designs different neurons for
different tasks, recent advances in deep learning have explored modifying a
network's neurons to develop so-called task-driven neurons. Prototyping
task-driven neurons (referred to as NeuronSeek) employs symbolic regression
(SR) to discover the optimal neuron formulation and construct a network from
these optimized neurons. Along this direction, this work replaces symbolic
regression with tensor decomposition (TD) to discover optimal neuronal
formulations, offering enhanced stability and faster convergence. Furthermore,
we establish theoretical guarantees that modifying the aggregation functions
with common activation functions can empower a network with a fixed number of
parameters to approximate any continuous function with an arbitrarily small
error, providing a rigorous mathematical foundation for the NeuronSeek
framework. Extensive empirical evaluations demonstrate that our NeuronSeek-TD
framework not only achieves superior stability, but also is competitive
relative to the state-of-the-art models across diverse benchmarks. The code is
available at https://github.com/HanyuPei22/NeuronSeek.

</details>


### [271] [Alternates, Assemble! Selecting Optimal Alternates for Citizens' Assemblies](https://arxiv.org/abs/2506.15716)
*Angelos Assos,Carmel Baharav,Bailey Flanigan,Ariel Procaccia*

Main category: cs.LG

TL;DR: 论文提出了一种优化框架，通过历史数据估计参与者退出概率，并选择替补以最小化预期代表性偏差，显著改善了公民议会的代表性。


<details>
  <summary>Details</summary>
Motivation: 公民议会中随机选出的参与者常因退出导致代表性失衡，现有方法未考虑替补选择问题。

Method: 利用学习理论工具估计退出概率，优化替补选择以减少预期代表性偏差。

Result: 理论保证包括样本复杂性和退出概率误估时的损失界限，实证显示方法显著提升代表性且减少替补需求。

Conclusion: 提出的算法框架有效解决了公民议会中的代表性失衡问题，具有理论和实践优势。

Abstract: An increasingly influential form of deliberative democracy centers on
citizens' assemblies, where randomly selected people discuss policy questions.
The legitimacy of these panels hinges on their representation of the broader
population, but panelists often drop out, leading to an unbalanced composition.
Although participant attrition is mitigated in practice by alternates, their
selection is not taken into account by existing methods. To address this gap,
we introduce an optimization framework for alternate selection. Our algorithmic
approach, which leverages learning-theoretic machinery, estimates dropout
probabilities using historical data and selects alternates to minimize expected
misrepresentation. We establish theoretical guarantees for our approach,
including worst-case bounds on sample complexity (with implications for
computational efficiency) and on loss when panelists' probabilities of dropping
out are mis-estimated. Empirical evaluation using real-world data demonstrates
that, compared to the status quo, our method significantly improves
representation while requiring fewer alternates.

</details>


### [272] [daDPO: Distribution-Aware DPO for Distilling Conversational Abilities](https://arxiv.org/abs/2506.15717)
*Zhengze Zhang,Shiqi Wang,Yiqun Shen,Simin Guo,Dahua Lin,Xiaoliang Wang,Nguyen Cam-Tu,Fei Tan*

Main category: cs.LG

TL;DR: 本文提出了一种名为daDPO（Distribution-Aware DPO）的新方法，通过结合偏好优化和基于分布的蒸馏，显著提升了小型语言模型的对话能力。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）在资源受限环境中表现下降，现有知识蒸馏方法仅关注教师模型的输出而忽略其分布信息。

Method: 提出daDPO方法，统一偏好优化和基于分布的蒸馏，并进行理论和实证验证。

Result: daDPO在修剪模型和小型LLM上表现优于现有方法，例如修剪后的Vicuna1.5-7B接近教师性能，Qwen2.5-1.5B甚至偶尔超越7B教师模型。

Conclusion: daDPO是一种有效的方法，能够显著提升小型语言模型的性能，填补了现有方法的不足。

Abstract: Large language models (LLMs) have demonstrated exceptional performance across
various applications, but their conversational abilities decline sharply as
model size decreases, presenting a barrier to their deployment in
resource-constrained environments. Knowledge distillation with Direct
Preference Optimization (dDPO) has emerged as a promising approach to enhancing
the conversational abilities of smaller models using a larger teacher model.
However, current methods primarily focus on 'black-box' KD, which only uses the
teacher's responses, overlooking the output distribution offered by the
teacher. This paper addresses this gap by introducing daDPO (Distribution-Aware
DPO), a unified method for preference optimization and distribution-based
distillation. We provide rigorous theoretical analysis and empirical
validation, showing that daDPO outperforms existing methods in restoring
performance for pruned models and enhancing smaller LLM models. Notably, in
in-domain evaluation, our method enables a 20% pruned Vicuna1.5-7B to achieve
near-teacher performance (-7.3% preference rate compared to that of dDPO's
-31%), and allows Qwen2.5-1.5B to occasionally outperform its 7B teacher model
(14.0% win rate).

</details>


### [273] [BuildingBRep-11K: Precise Multi-Storey B-Rep Building Solids with Rich Layout Metadata](https://arxiv.org/abs/2506.15718)
*Yu Guo,Hongji Fang,Tianyu Fang,Zhe Cui*

Main category: cs.LG

TL;DR: 论文介绍了BuildingBRep-11K数据集，包含11,978个多楼层建筑模型，用于训练AI模型进行建筑几何和拓扑分析。通过两个轻量级PointNet基线实验验证了数据集的可学习性。


<details>
  <summary>Details</summary>
Motivation: 随着人工智能的发展，建筑尺度3D对象的自动生成成为研究热点，但现有模型需要大规模、干净且标注丰富的数据集。BuildingBRep-11K旨在填补这一空白。

Method: 数据集通过形状语法驱动的流程生成，包含几何精确的B-rep实体和快速加载的元数据文件。生成器结合空间尺度、日光优化和室内布局约束，并通过多阶段过滤器确保符合建筑标准。

Result: 两个PointNet基线实验显示：(i)多属性回归在楼层数、房间数和平均房间面积上表现良好；(ii)缺陷检测的准确率为54%，召回率为82%。

Conclusion: BuildingBRep-11K是一个可学习但非简单的数据集，适用于几何回归和拓扑质量评估。

Abstract: With the rise of artificial intelligence, the automatic generation of
building-scale 3-D objects has become an active research topic, yet training
such models still demands large, clean and richly annotated datasets. We
introduce BuildingBRep-11K, a collection of 11 978 multi-storey (2-10 floors)
buildings (about 10 GB) produced by a shape-grammar-driven pipeline that
encodes established building-design principles. Every sample consists of a
geometrically exact B-rep solid-covering floors, walls, slabs and rule-based
openings-together with a fast-loading .npy metadata file that records detailed
per-floor parameters. The generator incorporates constraints on spatial scale,
daylight optimisation and interior layout, and the resulting objects pass
multi-stage filters that remove Boolean failures, undersized rooms and extreme
aspect ratios, ensuring compliance with architectural standards. To verify the
dataset's learnability we trained two lightweight PointNet baselines. (i)
Multi-attribute regression. A single encoder predicts storey count, total
rooms, per-storey vector and mean room area from a 4 000-point cloud. On 100
unseen buildings it attains 0.37-storey MAE (87 \% within $\pm1$), 5.7-room
MAE, and 3.2 m$^2$ MAE on mean area. (ii) Defect detection. With the same
backbone we classify GOOD versus DEFECT; on a balanced 100-model set the
network reaches 54 \% accuracy, recalling 82 \% of true defects at 53 \%
precision (41 TP, 9 FN, 37 FP, 13 TN). These pilots show that BuildingBRep-11K
is learnable yet non-trivial for both geometric regression and topological
quality assessment

</details>


### [274] [Data-Driven Heat Pump Management: Combining Machine Learning with Anomaly Detection for Residential Hot Water Systems](https://arxiv.org/abs/2506.15719)
*Manal Rahal,Bestoun S. Ahmed,Roger Renstrom,Robert Stener,Albrecht Wurtz*

Main category: cs.LG

TL;DR: 论文提出了一种结合机器学习和异常检测的新方法，用于优化家用热泵热水需求预测和控制策略，实验验证了其优越性能。


<details>
  <summary>Details</summary>
Motivation: 传统热泵热水系统的效率受限于基于阈值的控制方法，机器学习在热泵应用中虽成功但家用热水需求预测优化研究不足。

Method: 结合预测性机器学习和隔离森林（iForest）方法，采用多步特征选择和三种ML模型（LightGBM、LSTM、Bi-LSTM）进行优化。

Result: LightGBM表现最佳，RMSE提升9.37%，iForest异常检测F1-score达0.87，误报率仅5.2%。

Conclusion: 该方法适用于实际热泵部署，具有强泛化能力。

Abstract: Heat pumps (HPs) have emerged as a cost-effective and clean technology for
sustainable energy systems, but their efficiency in producing hot water remains
restricted by conventional threshold-based control methods. Although machine
learning (ML) has been successfully implemented for various HP applications,
optimization of household hot water demand forecasting remains understudied.
This paper addresses this problem by introducing a novel approach that combines
predictive ML with anomaly detection to create adaptive hot water production
strategies based on household-specific consumption patterns. Our key
contributions include: (1) a composite approach combining ML and isolation
forest (iForest) to forecast household demand for hot water and steer
responsive HP operations; (2) multi-step feature selection with advanced
time-series analysis to capture complex usage patterns; (3) application and
tuning of three ML models: Light Gradient Boosting Machine (LightGBM), Long
Short-Term Memory (LSTM), and Bi-directional LSTM with the self-attention
mechanism on data from different types of real HP installations; and (4)
experimental validation on six real household installations. Our experiments
show that the best-performing model LightGBM achieves superior performance,
with RMSE improvements of up to 9.37\% compared to LSTM variants with $R^2$
values between 0.748-0.983. For anomaly detection, our iForest implementation
achieved an F1-score of 0.87 with a false alarm rate of only 5.2\%,
demonstrating strong generalization capabilities across different household
types and consumption patterns, making it suitable for real-world HP
deployments.

</details>


### [275] [Tripartite Weight-Space Ensemble for Few-Shot Class-Incremental Learning](https://arxiv.org/abs/2506.15720)
*Juntae Lee,Munawar Hayat,Sungrack Yun*

Main category: cs.LG

TL;DR: 论文提出了一种新的少样本类增量学习方法（Tri-WE），通过权重空间的三元集成和知识蒸馏正则化，解决了灾难性遗忘和过拟合问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法固定特征提取器限制了模型对新类的适应性，导致灾难性遗忘和过拟合。

Method: 提出Tri-WE方法，在权重空间中对基础模型、前一个模型和当前模型进行集成，并结合知识蒸馏正则化。

Result: 在miniImageNet、CUB200和CIFAR100数据集上取得了最先进的结果。

Conclusion: Tri-WE方法有效解决了少样本类增量学习中的关键问题，提升了模型性能。

Abstract: Few-shot class incremental learning (FSCIL) enables the continual learning of
new concepts with only a few training examples. In FSCIL, the model undergoes
substantial updates, making it prone to forgetting previous concepts and
overfitting to the limited new examples. Most recent trend is typically to
disentangle the learning of the representation from the classification head of
the model. A well-generalized feature extractor on the base classes (many
examples and many classes) is learned, and then fixed during incremental
learning. Arguing that the fixed feature extractor restricts the model's
adaptability to new classes, we introduce a novel FSCIL method to effectively
address catastrophic forgetting and overfitting issues. Our method enables to
seamlessly update the entire model with a few examples. We mainly propose a
tripartite weight-space ensemble (Tri-WE). Tri-WE interpolates the base,
immediately previous, and current models in weight-space, especially for the
classification heads of the models. Then, it collaboratively maintains
knowledge from the base and previous models. In addition, we recognize the
challenges of distilling generalized representations from the previous model
from scarce data. Hence, we suggest a regularization loss term using amplified
data knowledge distillation. Simply intermixing the few-shot data, we can
produce richer data enabling the distillation of critical knowledge from the
previous model. Consequently, we attain state-of-the-art results on the
miniImageNet, CUB200, and CIFAR100 datasets.

</details>


### [276] [Bohdi: Heterogeneous LLM Fusion with Automatic Data Exploration](https://arxiv.org/abs/2506.15721)
*Junqi Gao,Zhichang Guo,Dazhi Zhang,Dong Li,Runze Liu,Pengfei Li,Kai Tian,Biqing Qi*

Main category: cs.LG

TL;DR: Bohdi是一个仅使用合成数据的异构大型语言模型（LLM）融合框架，通过层次化多臂老虎机问题动态调整知识域的数据采样比例，显著提升了目标LLM的性能和数据效率。


<details>
  <summary>Details</summary>
Motivation: 现有异构LLM融合方法依赖有限领域的真实数据，且数据分配比例固定，导致目标LLM无法全面获取跨领域知识且能力不平衡。

Method: Bohdi采用层次化知识树结构，通过多模型协作自动探索领域并生成数据，利用DynaBranches机制和Introspection-Rebirth机制动态调整采样比例。

Result: 实验表明，Bohdi在多个目标LLM上显著优于现有基线，数据效率更高，且几乎消除了能力不平衡问题。

Conclusion: Bohdi通过合成数据和动态调整机制，有效解决了异构LLM融合中的知识获取和能力平衡问题。

Abstract: Heterogeneous Large Language Model (LLM) fusion integrates the strengths of
multiple source LLMs with different architectures into a target LLM with low
computational overhead. While promising, existing methods suffer from two major
limitations: 1) reliance on real data from limited domain for knowledge fusion,
preventing the target LLM from fully acquiring knowledge across diverse
domains, and 2) fixed data allocation proportions across domains, failing to
dynamically adjust according to the target LLM's varying capabilities across
domains, leading to a capability imbalance. To overcome these limitations, we
propose Bohdi, a synthetic-data-only heterogeneous LLM fusion framework.
Through the organization of knowledge domains into a hierarchical tree
structure, Bohdi enables automatic domain exploration and multi-domain data
generation through multi-model collaboration, thereby comprehensively
extracting knowledge from source LLMs. By formalizing domain expansion and data
sampling proportion allocation on the knowledge tree as a Hierarchical
Multi-Armed Bandit problem, Bohdi leverages the designed DynaBranches mechanism
to adaptively adjust sampling proportions based on the target LLM's performance
feedback across domains. Integrated with our proposed Introspection-Rebirth
(IR) mechanism, DynaBranches dynamically tracks capability shifts during target
LLM's updates via Sliding Window Binomial Likelihood Ratio Testing (SWBLRT),
further enhancing its online adaptation capability. Comparative experimental
results on a comprehensive suite of benchmarks demonstrate that Bohdi
significantly outperforms existing baselines on multiple target LLMs, exhibits
higher data efficiency, and virtually eliminates the imbalance in the target
LLM's capabilities. Our code is available at
https://github.com/gjq100/Bohdi.git.

</details>


### [277] [UniMate: A Unified Model for Mechanical Metamaterial Generation, Property Prediction, and Condition Confirmation](https://arxiv.org/abs/2506.15722)
*Wangzhi Zhan,Jianpeng Chen,Dongqi Fu,Dawei Zhou*

Main category: cs.LG

TL;DR: 论文提出了一种名为UNIMATE的统一模型，用于同时处理机械超材料设计中的3D拓扑、密度条件和机械性能三种模态，填补了现有研究的空白。


<details>
  <summary>Details</summary>
Motivation: 现有研究通常仅考虑机械超材料设计中的两种模态，而实际应用需要同时处理三种模态，因此需要一种统一的模型来解决这一问题。

Method: UNIMATE包含模态对齐模块和协同扩散生成模块，能够同时处理三种模态。

Result: 实验表明，UNIMATE在拓扑生成、性能预测和条件确认任务中分别优于基线模型80.2%、5.1%和50.2%。

Conclusion: UNIMATE是一种有效的统一模型，填补了现有研究的空白，并在多个任务中表现出色。

Abstract: Metamaterials are artificial materials that are designed to meet unseen
properties in nature, such as ultra-stiffness and negative materials indices.
In mechanical metamaterial design, three key modalities are typically involved,
i.e., 3D topology, density condition, and mechanical property. Real-world
complex application scenarios place the demanding requirements on machine
learning models to consider all three modalities together. However, a
comprehensive literature review indicates that most existing works only
consider two modalities, e.g., predicting mechanical properties given the 3D
topology or generating 3D topology given the required properties. Therefore,
there is still a significant gap for the state-of-the-art machine learning
models capturing the whole. Hence, we propose a unified model named UNIMATE,
which consists of a modality alignment module and a synergetic diffusion
generation module. Experiments indicate that UNIMATE outperforms the other
baseline models in topology generation task, property prediction task, and
condition confirmation task by up to 80.2%, 5.1%, and 50.2%, respectively. We
opensource our proposed UNIMATE model and corresponding results at
https://github.com/wzhan24/UniMate.

</details>


### [278] [MadaKV: Adaptive Modality-Perception KV Cache Eviction for Efficient Multimodal Long-Context Inference](https://arxiv.org/abs/2506.15724)
*Kunxi Li,Zhonghua Jiang,Zhouzhou Shen,Zhaode Wang,Chengfei Lv,Shengyu Zhang,Fan Wu,Fei Wu*

Main category: cs.LG

TL;DR: MadaKV是一种模态自适应的KV缓存淘汰策略，旨在提升多模态大语言模型在长上下文推理中的效率。


<details>
  <summary>Details</summary>
Motivation: 传统KV缓存淘汰方法在单模态场景下设计，无法捕捉多模态场景中模态特异性信息，导致性能不佳。

Method: MadaKV通过模态偏好适应和分层压缩补偿两个关键组件，动态感知注意力头中的模态信息并自适应保留关键令牌。

Result: 实验表明，MadaKV显著减少了KV缓存内存占用和模型推理延迟（提升1.3至1.5倍），同时保持高准确性。

Conclusion: MadaKV在多模态长上下文任务中表现优于现有KV缓存淘汰方法。

Abstract: This paper introduces MadaKV, a modality-adaptive key-value (KV) cache
eviction strategy designed to enhance the efficiency of multimodal large
language models (MLLMs) in long-context inference. In multimodal scenarios,
attention heads exhibit varying preferences for different modalities, resulting
in significant disparities in modality importance across attention heads.
Traditional KV cache eviction methods, which are tailored for unimodal
settings, fail to capture modality-specific information, thereby yielding
suboptimal performance. MadaKV addresses these challenges through two key
components: modality preference adaptation and hierarchical compression
compensation. By dynamically sensing modality information within attention
heads and adaptively retaining critical tokens, MadaKV achieves substantial
reductions in KV cache memory footprint and model inference decoding latency
(1.3 to 1.5 times improvement) while maintaining high accuracy across various
multimodal long-context tasks. Extensive experiments on representative MLLMs
and the MileBench benchmark demonstrate the effectiveness of MadaKV compared to
existing KV cache eviction methods.

</details>


### [279] [Graph Diffusion that can Insert and Delete](https://arxiv.org/abs/2506.15725)
*Matteo Ninniri,Marco Podda,Davide Bacciu*

Main category: cs.LG

TL;DR: 论文提出了一种基于离散去噪扩散概率模型（DDPMs）的图生成模型GrIDDD，通过动态调整节点数量解决了现有方法无法适应图大小变化的问题。


<details>
  <summary>Details</summary>
Motivation: 现有图扩散模型在分子生成中无法适应图大小的变化，限制了其在条件生成场景（如属性驱动分子设计）中的有效性。

Method: 重新定义了噪声和去噪过程，支持节点的单调插入和删除，动态调整化学图的大小。

Result: GrIDDD在分子属性目标上表现优于现有图扩散模型，并在分子优化中展现出竞争力。

Conclusion: 该工作为基于图扩散的大小自适应分子生成提供了新思路。

Abstract: Generative models of graphs based on discrete Denoising Diffusion
Probabilistic Models (DDPMs) offer a principled approach to molecular
generation by systematically removing structural noise through iterative atom
and bond adjustments. However, existing formulations are fundamentally limited
by their inability to adapt the graph size (that is, the number of atoms)
during the diffusion process, severely restricting their effectiveness in
conditional generation scenarios such as property-driven molecular design,
where the targeted property often correlates with the molecular size. In this
paper, we reformulate the noising and denoising processes to support monotonic
insertion and deletion of nodes. The resulting model, which we call GrIDDD,
dynamically grows or shrinks the chemical graph during generation. GrIDDD
matches or exceeds the performance of existing graph diffusion models on
molecular property targeting despite being trained on a more difficult problem.
Furthermore, when applied to molecular optimization, GrIDDD exhibits
competitive performance compared to specialized optimization models. This work
paves the way for size-adaptive molecular generation with graph diffusion.

</details>


### [280] [Descriptor-based Foundation Models for Molecular Property Prediction](https://arxiv.org/abs/2506.15792)
*Jackson Burns,Akshat Zalte,William Green*

Main category: cs.LG

TL;DR: CheMeleon是一种基于分子描述符预训练的分子基础模型，通过无噪声数据学习分子表示，在多个基准测试中表现优异，但在区分活性悬崖方面仍有不足。


<details>
  <summary>Details</summary>
Motivation: 快速准确地预测分子性质对科学进步至关重要，传统方法依赖噪声数据或偏置模拟，而CheMeleon利用低噪声分子描述符提供更优解决方案。

Method: CheMeleon基于Mordred包的分子描述符预训练，使用定向消息传递神经网络（D-MPNN）在无噪声环境中预测描述符。

Result: 在Polaris和MoleculeACE的58个基准测试中，CheMeleon分别以79%和97%的胜率优于基线模型，但难以区分活性悬崖。

Conclusion: 描述符预训练为分子性质预测提供了可扩展且高效的途径，未来可进一步探索描述符集和无标签数据集。

Abstract: Fast and accurate prediction of molecular properties with machine learning is
pivotal to scientific advancements across myriad domains. Foundation models in
particular have proven especially effective, enabling accurate training on
small, real-world datasets. This study introduces CheMeleon, a novel molecular
foundation model pre-trained on deterministic molecular descriptors from the
Mordred package, leveraging a Directed Message-Passing Neural Network to
predict these descriptors in a noise-free setting. Unlike conventional
approaches relying on noisy experimental data or biased quantum mechanical
simulations, CheMeleon uses low-noise molecular descriptors to learn rich
molecular representations. Evaluated on 58 benchmark datasets from Polaris and
MoleculeACE, CheMeleon achieves a win rate of 79% on Polaris tasks,
outperforming baselines like Random Forest (46%), fastprop (39%), and Chemprop
(36%), and a 97% win rate on MoleculeACE assays, surpassing Random Forest (63%)
and other foundation models. However, it struggles to distinguish activity
cliffs like many of the tested models. The t-SNE projection of CheMeleon's
learned representations demonstrates effective separation of chemical series,
highlighting its ability to capture structural nuances. These results
underscore the potential of descriptor-based pre-training for scalable and
effective molecular property prediction, opening avenues for further
exploration of descriptor sets and unlabeled datasets.

</details>


### [281] [DeepJ: Graph Convolutional Transformers with Differentiable Pooling for Patient Trajectory Modeling](https://arxiv.org/abs/2506.15809)
*Deyi Li,Zijun Yao,Muxuan Liang,Mei Liu*

Main category: cs.LG

TL;DR: 论文提出了一种名为DeepJ的图卷积变换器模型，用于捕捉医疗事件在纵向记录中的动态交互，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有基于图的方法在处理电子健康记录（EHR）数据时，无法有效建模跨记录的医疗事件交互及其时间依赖性。

Method: DeepJ结合了图卷积和变换器技术，并采用可微分图池化方法，以捕捉记录内和记录间的医疗事件交互。

Result: DeepJ在预测患者结果时显著优于五种基线模型，同时提高了可解释性。

Conclusion: DeepJ为患者风险分层提供了更有效的工具，能够识别与患者结果相关的关键事件群。

Abstract: In recent years, graph learning has gained significant interest for modeling
complex interactions among medical events in structured Electronic Health
Record (EHR) data. However, existing graph-based approaches often work in a
static manner, either restricting interactions within individual encounters or
collapsing all historical encounters into a single snapshot. As a result, when
it is necessary to identify meaningful groups of medical events spanning
longitudinal encounters, existing methods are inadequate in modeling
interactions cross encounters while accounting for temporal dependencies. To
address this limitation, we introduce Deep Patient Journey (DeepJ), a novel
graph convolutional transformer model with differentiable graph pooling to
effectively capture intra-encounter and inter-encounter medical event
interactions. DeepJ can identify groups of temporally and functionally related
medical events, offering valuable insights into key event clusters pertinent to
patient outcome prediction. DeepJ significantly outperformed five
state-of-the-art baseline models while enhancing interpretability,
demonstrating its potential for improved patient risk stratification.

</details>


### [282] [Optimizing Bidding Strategies in First-Price Auctions in Binary Feedback Setting with Predictions](https://arxiv.org/abs/2506.15817)
*Jason Tandiary*

Main category: cs.LG

TL;DR: 本文研究二进制反馈下的Vickrey首价拍卖，提出一种新算法，利用机器学习提升BROAD-OMD算法的遗憾边界。


<details>
  <summary>Details</summary>
Motivation: 首价拍卖的日益重要及机器学习模型的预测能力推动了研究。

Method: 在BROAD-OMD框架内提出新算法，利用最高竞争出价的预测。

Result: 算法在准确预测下实现零遗憾，并在特定条件下建立O(T^(3/4)*Vt^(1/4))的遗憾边界。

Conclusion: 新算法在首价拍卖中显著提升了性能，尤其在预测准确时表现优异。

Abstract: This paper studies Vickrey first-price auctions under binary feedback.
Leveraging the enhanced performance of machine learning algorithms, the new
algorithm uses past information to improve the regret bounds of the BROAD-OMD
algorithm. Motivated by the growing relevance of first-price auctions and the
predictive capabilities of machine learning models, this paper proposes a new
algorithm within the BROAD-OMD framework (Hu et al., 2025) that leverages
predictions of the highest competing bid. This paper's main contribution is an
algorithm that achieves zero regret under accurate predictions. Additionally, a
bounded regret bound of O(T^(3/4) * Vt^(1/4)) is established under certain
normality conditions.

</details>


### [283] [AI-based modular warning machine for risk identification in proximity healthcare](https://arxiv.org/abs/2506.15823)
*Chiara Razzetta,Shahryar Noei,Federico Barbarossa,Edoardo Spairani,Monica Roascio,Elisa Barbi,Giulia Ciacci,Sara Sommariva,Sabrina Guastavino,Michele Piana,Matteo Lenge,Gabriele Arnulfo,Giovanni Magenes,Elvira Maranesi,Giulio Amabili,Anna Maria Massone,Federico Benvenuto,Giuseppe Jurman,Diego Sona,Cristina Campi*

Main category: cs.LG

TL;DR: DHEAL-COM项目开发数字健康解决方案，利用机器学习分析多模态数据，提出自动化流程。


<details>
  <summary>Details</summary>
Motivation: 为社区医疗开发数字健康解决方案，提升数据分析和预测能力。

Method: 结合无监督和监督学习方法的自动化流程，用于数据分析和特征识别。

Result: 提出了一种能够处理多模态数据并提供预测结果的自动化流程。

Conclusion: 该研究为社区医疗的数字健康解决方案提供了高效的数据分析工具。

Abstract: "DHEAL-COM - Digital Health Solutions in Community Medicine" is a research
and technology project funded by the Italian Department of Health for the
development of digital solutions of interest in proximity healthcare. The
activity within the DHEAL-COM framework allows scientists to gather a notable
amount of multi-modal data whose interpretation can be performed by means of
machine learning algorithms. The present study illustrates a general automated
pipeline made of numerous unsupervised and supervised methods that can ingest
such data, provide predictive results, and facilitate model interpretations via
feature identification.

</details>


### [284] [Heterogeneous Federated Reinforcement Learning Using Wasserstein Barycenters](https://arxiv.org/abs/2506.15825)
*Luiz Pereira,M. Hadi Amini*

Main category: cs.LG

TL;DR: 提出了一种基于Wasserstein重心的新算法FedWB，用于分布式架构下的全局DNN训练，并扩展应用于异构联邦强化学习（HFRL）。


<details>
  <summary>Details</summary>
Motivation: 解决分布式环境下模型融合的挑战，并扩展到异构联邦强化学习场景。

Method: 将数据集分发给多个代理训练本地DNN，通过Wasserstein重心聚合权重；应用于HFRL时，训练本地DQN并通过全局聚合泛化模型。

Result: 在CartPole问题中验证了FedWB和HFRL算法的有效性，生成了适用于所有环境的全局DQN。

Conclusion: FedWB算法在分布式训练和异构联邦强化学习中表现出色，具有广泛的应用潜力。

Abstract: In this paper, we first propose a novel algorithm for model fusion that
leverages Wasserstein barycenters in training a global Deep Neural Network
(DNN) in a distributed architecture. To this end, we divide the dataset into
equal parts that are fed to "agents" who have identical deep neural networks
and train only over the dataset fed to them (known as the local dataset). After
some training iterations, we perform an aggregation step where we combine the
weight parameters of all neural networks using Wasserstein barycenters. These
steps form the proposed algorithm referred to as FedWB. Moreover, we leverage
the processes created in the first part of the paper to develop an algorithm to
tackle Heterogeneous Federated Reinforcement Learning (HFRL). Our test
experiment is the CartPole toy problem, where we vary the lengths of the poles
to create heterogeneous environments. We train a deep Q-Network (DQN) in each
environment to learn to control each cart, while occasionally performing a
global aggregation step to generalize the local models; the end outcome is a
global DQN that functions across all environments.

</details>


### [285] [In-field Calibration of Low-Cost Sensors through XGBoost $\&$ Aggregate Sensor Data](https://arxiv.org/abs/2506.15840)
*Kevin Yin,Julia Gersey,Pei Zhang*

Main category: cs.LG

TL;DR: 提出了一种基于XGBoost集成学习的现场传感器校准模型，用于提高低成本传感器的准确性。


<details>
  <summary>Details</summary>
Motivation: 由于高精度传感器成本高昂，限制了部署范围，而低成本传感器易受环境因素影响导致漂移，因此需要一种校准方法。

Method: 使用XGBoost集成学习整合邻近传感器的数据，减少对单个传感器精度的依赖。

Result: 该方法提高了传感器数据的泛化能力，适用于不同地点。

Conclusion: 提出的模型能有效校准低成本传感器，提升空气质量监测的覆盖范围和准确性。

Abstract: Effective large-scale air quality monitoring necessitates distributed sensing
due to the pervasive and harmful nature of particulate matter (PM),
particularly in urban environments. However, precision comes at a cost: highly
accurate sensors are expensive, limiting the spatial deployments and thus their
coverage. As a result, low-cost sensors have become popular, though they are
prone to drift caused by environmental sensitivity and manufacturing
variability. This paper presents a model for in-field sensor calibration using
XGBoost ensemble learning to consolidate data from neighboring sensors. This
approach reduces dependence on the presumed accuracy of individual sensors and
improves generalization across different locations.

</details>


### [286] [Uncertainty Estimation by Human Perception versus Neural Models](https://arxiv.org/abs/2506.15850)
*Pedro Mendes,Paolo Romano,David Garlan*

Main category: cs.LG

TL;DR: 现代神经网络预测准确但校准不佳，本研究比较人类感知不确定性与神经网络估计的不确定性，发现当前方法与人类直觉弱相关，但通过引入人类软标签可改善校准。


<details>
  <summary>Details</summary>
Motivation: 神经网络常产生过度自信的预测，缺乏可靠的不确定性估计，本研究旨在探索人类感知不确定性与模型预测不确定性的关系。

Method: 使用三个视觉基准数据集，结合人类标注的分歧和众包置信度，评估模型预测不确定性与人类感知不确定性的相关性。

Result: 当前方法与人类直觉相关性较弱，但引入人类软标签可提升校准性而不影响准确性。

Conclusion: 模型与人类不确定性存在差距，利用人类洞察可推动更可信AI的发展。

Abstract: Modern neural networks (NNs) often achieve high predictive accuracy but
remain poorly calibrated, producing overconfident predictions even when wrong.
This miscalibration poses serious challenges in applications where reliable
uncertainty estimates are critical. In this work, we investigate how human
perceptual uncertainty compares to uncertainty estimated by NNs. Using three
vision benchmarks annotated with both human disagreement and crowdsourced
confidence, we assess the correlation between model-predicted uncertainty and
human-perceived uncertainty. Our results show that current methods only weakly
align with human intuition, with correlations varying significantly across
tasks and uncertainty metrics. Notably, we find that incorporating
human-derived soft labels into the training process can improve calibration
without compromising accuracy. These findings reveal a persistent gap between
model and human uncertainty and highlight the potential of leveraging human
insights to guide the development of more trustworthy AI systems.

</details>


### [287] [Improving Rectified Flow with Boundary Conditions](https://arxiv.org/abs/2506.15864)
*Xixi Hu,Runlong Liao,Keyang Xu,Bo Liu,Yeqing Li,Eugene Ie,Hongliang Fei,Qiang Liu*

Main category: cs.LG

TL;DR: Boundary-enforced Rectified Flow Model (Boundary RF Model)通过强制边界条件改进Rectified Flow，显著提升生成模型性能。


<details>
  <summary>Details</summary>
Motivation: 直接建模速度场时，未约束的神经网络可能无法满足边界条件，导致速度场估计不准确，尤其在边界附近误差放大。

Method: 提出Boundary RF Model，通过最小代码修改强制边界条件。

Result: 在ImageNet上，ODE采样FID提升8.01%，SDE采样FID提升8.98%。

Conclusion: Boundary RF Model有效解决了边界条件问题，显著提升了生成模型性能。

Abstract: Rectified Flow offers a simple and effective approach to high-quality
generative modeling by learning a velocity field. However, we identify a
limitation in directly modeling the velocity with an unconstrained neural
network: the learned velocity often fails to satisfy certain boundary
conditions, leading to inaccurate velocity field estimations that deviate from
the desired ODE. This issue is particularly critical during stochastic sampling
at inference, as the score function's errors are amplified near the boundary.
To mitigate this, we propose a Boundary-enforced Rectified Flow Model (Boundary
RF Model), in which we enforce boundary conditions with a minimal code
modification. Boundary RF Model improves performance over vanilla RF model,
demonstrating 8.01% improvement in FID score on ImageNet using ODE sampling and
8.98% improvement using SDE sampling.

</details>


### [288] [Hidden Breakthroughs in Language Model Training](https://arxiv.org/abs/2506.15872)
*Sara Kangaslahti,Elan Rosenfeld,Naomi Saphra*

Main category: cs.LG

TL;DR: 论文提出POLCA方法，通过分解损失变化来识别训练中的隐藏突破点，帮助理解模型学习动态。


<details>
  <summary>Details</summary>
Motivation: 研究训练中损失曲线的不连续性，以揭示模型学习的潜在突破点，但传统标量损失指标掩盖了这些变化。

Method: 引入POLCA方法，分解低秩训练子空间中损失的变化，识别样本聚类以揭示隐藏的突破点。

Result: 在合成算术和自然语言任务中验证，POLCA成功识别出可解释的模型能力突破点。

Conclusion: POLCA为无监督可解释性提供了新工具，揭示了训练中的隐藏阶段转变。

Abstract: Loss curves are smooth during most of model training, so visible
discontinuities stand out as possible conceptual breakthroughs. Studying these
breakthroughs enables a deeper understanding of learning dynamics, but only
when they are properly identified. This paper argues that similar breakthroughs
occur frequently throughout training but they are obscured by a loss metric
that collapses all variation into a single scalar. To find these hidden
transitions, we introduce POLCA, a method for decomposing changes in loss along
arbitrary bases of the low-rank training subspace. We use our method to
identify clusters of samples that share similar changes in loss during
training, disaggregating the overall loss into that of smaller groups of
conceptually similar data. We validate our method on synthetic arithmetic and
natural language tasks, showing that POLCA recovers clusters that represent
interpretable breakthroughs in the model's capabilities. We demonstrate the
promise of these hidden phase transitions as a tool for unsupervised
interpretability.

</details>


### [289] [Job Market Cheat Codes: Prototyping Salary Prediction and Job Grouping with Synthetic Job Listings](https://arxiv.org/abs/2506.15879)
*Abdel Rahman Alsheyab,Mohammad Alkhasawneh,Nidal Shahin*

Main category: cs.LG

TL;DR: 本文提出了一种基于合成数据集的机器学习方法，用于分析职位趋势、预测薪资并聚类相似职位。


<details>
  <summary>Details</summary>
Motivation: 研究旨在揭示影响就业市场动态的关键特征，为求职者、雇主和研究人员提供有价值的见解。

Method: 采用回归、分类、聚类和自然语言处理（NLP）技术进行文本特征提取和表示，并进行了探索性数据分析。

Result: 分析揭示了影响薪资和职位角色的重要因素，并识别出基于数据的独特职位聚类。

Conclusion: 尽管结果基于合成数据，但该方法为就业市场分析提供了一个可转移的框架。

Abstract: This paper presents a machine learning methodology prototype using a large
synthetic dataset of job listings to identify trends, predict salaries, and
group similar job roles. Employing techniques such as regression,
classification, clustering, and natural language processing (NLP) for
text-based feature extraction and representation, this study aims to uncover
the key features influencing job market dynamics and provide valuable insights
for job seekers, employers, and researchers. Exploratory data analysis was
conducted to understand the dataset's characteristics. Subsequently, regression
models were developed to predict salaries, classification models to predict job
titles, and clustering techniques were applied to group similar jobs. The
analyses revealed significant factors influencing salary and job roles, and
identified distinct job clusters based on the provided data. While the results
are based on synthetic data and not intended for real-world deployment, the
methodology demonstrates a transferable framework for job market analysis.

</details>


### [290] [T-SHRED: Symbolic Regression for Regularization and Model Discovery with Transformer Shallow Recurrent Decoders](https://arxiv.org/abs/2506.15881)
*Alexey Yermakov,David Zoro,Mars Liyao Gao,J. Nathan Kutz*

Main category: cs.LG

TL;DR: SHRED是一种轻量级模型，用于稀疏传感器数据的系统识别和预测。改进版T-SHRED引入Transformer和SINDy注意力机制，提升了预测性能和模型可解释性。


<details>
  <summary>Details</summary>
Motivation: 提升SHRED模型在稀疏传感器数据上的预测性能，并通过符号回归增强模型的可解释性。

Method: 使用Transformer替代RNN进行时间编码，并引入SINDy注意力机制进行符号回归。

Result: T-SHRED在多种动态系统上表现优异，能够准确预测未来状态并提供可解释的符号模型。

Conclusion: 改进后的T-SHRED在性能和可解释性上均优于原版SHRED，适用于不同数据规模的动态系统。

Abstract: SHallow REcurrent Decoders (SHRED) are effective for system identification
and forecasting from sparse sensor measurements. Such models are light-weight
and computationally efficient, allowing them to be trained on consumer laptops.
SHRED-based models rely on Recurrent Neural Networks (RNNs) and a simple
Multi-Layer Perceptron (MLP) for the temporal encoding and spatial decoding
respectively. Despite the relatively simple structure of SHRED, they are able
to predict chaotic dynamical systems on different physical, spatial, and
temporal scales directly from a sparse set of sensor measurements. In this
work, we improve SHRED by leveraging transformers (T-SHRED) for the temporal
encoding which improves performance on next-step state prediction on large
datasets. We also introduce a sparse identification of nonlinear dynamics
(SINDy) attention mechanism into T-SHRED to perform symbolic regression
directly on the latent space as part of the model regularization architecture.
Symbolic regression improves model interpretability by learning and
regularizing the dynamics of the latent space during training. We analyze the
performance of T-SHRED on three different dynamical systems ranging from
low-data to high-data regimes. We observe that SINDy attention T-SHRED
accurately predicts future frames based on an interpretable symbolic model
across all tested datasets.

</details>


### [291] [Fractional Reasoning via Latent Steering Vectors Improves Inference Time Compute](https://arxiv.org/abs/2506.15882)
*Sheng Liu,Tianlang Chen,Pan Lu,Haotian Ye,Yizheng Chen,Lei Xing,James Zou*

Main category: cs.LG

TL;DR: Fractional Reasoning是一种无需训练、模型无关的框架，通过动态调整推理强度提升LLM性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法（如Best-of-N、多数投票）对所有输入采用统一推理强度，忽略了问题复杂度的差异。

Method: 提取与深度推理相关的潜在导向向量，并通过可调缩放因子重新应用，动态调整推理强度。

Result: 在GSM8K、MATH500和GPQA等任务中，Fractional Reasoning显著提升了性能。

Conclusion: Fractional Reasoning为测试时计算提供了灵活且高效的解决方案。

Abstract: Test-time compute has emerged as a powerful paradigm for improving the
performance of large language models (LLMs), where generating multiple outputs
or refining individual chains can significantly boost answer accuracy. However,
existing methods like Best-of-N, majority voting, and self-reflection typically
apply reasoning in a uniform way across inputs, overlooking the fact that
different problems may require different levels of reasoning depth. In this
work, we propose Fractional Reasoning, a training-free and model-agnostic
framework that enables continuous control over reasoning intensity at inference
time, going beyond the limitations of fixed instructional prompts. Our method
operates by extracting the latent steering vector associated with deeper
reasoning and reapplying it with a tunable scaling factor, allowing the model
to tailor its reasoning process to the complexity of each input. This supports
two key modes of test-time scaling: (1) improving output quality in
breadth-based strategies (e.g., Best-of-N, majority voting), and (2) enhancing
the correctness of individual reasoning chains in depth-based strategies (e.g.,
self-reflection). Experiments on GSM8K, MATH500, and GPQA demonstrate that
Fractional Reasoning consistently improves performance across diverse reasoning
tasks and models.

</details>


### [292] [Formal Models of Active Learning from Contrastive Examples](https://arxiv.org/abs/2506.15893)
*Farnam Mansouri,Hans U. Simon,Adish Singla,Yuxin Chen,Sandra Zilles*

Main category: cs.LG

TL;DR: 论文提出了一种理论框架，研究对比训练样本对主动学习算法的影响，重点关注样本复杂性和对比样本选择的关系。


<details>
  <summary>Details</summary>
Motivation: 通过对比样本（仅微小差异但标签不同的实例）帮助解释标签差异，提升机器学习效果。

Method: 提出理论框架，分析对比样本对主动学习的影响，以几何概念类和布尔函数类为例。

Result: 揭示了对比样本学习与经典自导向学习模型之间的联系。

Conclusion: 对比样本的选择显著影响学习样本复杂性，且与自导向学习模型存在关联。

Abstract: Machine learning can greatly benefit from providing learning algorithms with
pairs of contrastive training examples -- typically pairs of instances that
differ only slightly, yet have different class labels. Intuitively, the
difference in the instances helps explain the difference in the class labels.
This paper proposes a theoretical framework in which the effect of various
types of contrastive examples on active learners is studied formally. The focus
is on the sample complexity of learning concept classes and how it is
influenced by the choice of contrastive examples. We illustrate our results
with geometric concept classes and classes of Boolean functions. Interestingly,
we reveal a connection between learning from contrastive examples and the
classical model of self-directed learning.

</details>


### [293] [KG-FGNN: Knowledge-guided GNN Foundation Model for Fertilisation-oriented Soil GHG Flux Prediction](https://arxiv.org/abs/2506.15896)
*Yu Zhang,Gaoshan Bi,Simon Jeffery,Max Davis,Yang Li,Qing Xue,Po Yang*

Main category: cs.LG

TL;DR: 该研究提出了一种知识引导的图神经网络框架，用于精准预测土壤温室气体通量，解决了农业数据稀缺问题。


<details>
  <summary>Details</summary>
Motivation: 精准预测土壤温室气体通量对评估环境影响和促进可持续农业至关重要，但农业数据稀缺限制了机器学习应用。

Method: 结合农业过程模型和图神经网络技术，利用自动编码器提取关键特征，并通过多目标多图神经网络整合特征相关性。

Result: 实验表明，该方法在预测施肥导向的土壤温室气体通量方面具有更高的准确性和稳定性。

Conclusion: 该框架为精准农业和温室气体减排提供了有效工具。

Abstract: Precision soil greenhouse gas (GHG) flux prediction is essential in
agricultural systems for assessing environmental impacts, developing emission
mitigation strategies and promoting sustainable agriculture. Due to the lack of
advanced sensor and network technologies on majority of farms, there are
challenges in obtaining comprehensive and diverse agricultural data. As a
result, the scarcity of agricultural data seriously obstructs the application
of machine learning approaches in precision soil GHG flux prediction. This
research proposes a knowledge-guided graph neural network framework that
addresses the above challenges by integrating knowledge embedded in an
agricultural process-based model and graph neural network techniques.
Specifically, we utilise the agricultural process-based model to simulate and
generate multi-dimensional agricultural datasets for 47 countries that cover a
wide range of agricultural variables. To extract key agricultural features and
integrate correlations among agricultural features in the prediction process,
we propose a machine learning framework that integrates the autoencoder and
multi-target multi-graph based graph neural networks, which utilises the
autoencoder to selectively extract significant agricultural features from the
agricultural process-based model simulation data and the graph neural network
to integrate correlations among agricultural features for accurately predict
fertilisation-oriented soil GHG fluxes. Comprehensive experiments were
conducted with both the agricultural simulation dataset and real-world
agricultural dataset to evaluate the proposed approach in comparison with
well-known baseline and state-of-the-art regression methods. The results
demonstrate that our proposed approach provides superior accuracy and stability
in fertilisation-oriented soil GHG prediction.

</details>


### [294] [TrajDiff: Diffusion Bridge Network with Semantic Alignment for Trajectory Similarity Computation](https://arxiv.org/abs/2506.15898)
*Xiao Zhang,Xingyu Zhao,Hong Xia,Yuan Cao,Guiyuan Jiang,Junyu Dong,Yanwei Yu*

Main category: cs.LG

TL;DR: 论文提出了一种名为TrajDiff的新框架，用于解决轨迹相似性计算中的语义差距、噪声问题和全局排序信息利用不足的挑战。


<details>
  <summary>Details</summary>
Motivation: 随着定位技术的普及，轨迹数据量激增，但现有学习方法存在语义差距、噪声干扰和全局排序信息利用不足的问题。

Method: TrajDiff框架包含语义对齐模块、基于DDBM的噪声鲁棒预训练和全局排序感知正则化。

Result: 在三个公开数据集上的实验表明，TrajDiff显著优于现有方法，平均HR@1提升33.38%。

Conclusion: TrajDiff通过多模块协同，有效解决了轨迹相似性计算中的关键挑战，性能显著提升。

Abstract: With the proliferation of location-tracking technologies, massive volumes of
trajectory data are continuously being collected. As a fundamental task in
trajectory data mining, trajectory similarity computation plays a critical role
in a wide range of real-world applications. However, existing learning-based
methods face three challenges: First, they ignore the semantic gap between GPS
and grid features in trajectories, making it difficult to obtain meaningful
trajectory embeddings. Second, the noise inherent in the trajectories, as well
as the noise introduced during grid discretization, obscures the true motion
patterns of the trajectories. Third, existing methods focus solely on
point-wise and pair-wise losses, without utilizing the global ranking
information obtained by sorting all trajectories according to their similarity
to a given trajectory. To address the aforementioned challenges, we propose a
novel trajectory similarity computation framework, named TrajDiff.
Specifically, the semantic alignment module relies on cross-attention and an
attention score mask mechanism with adaptive fusion, effectively eliminating
semantic discrepancies between data at two scales and generating a unified
representation. Additionally, the DDBM-based Noise-robust Pre-Training
introduces the transfer patterns between any two trajectories into the model
training process, enhancing the model's noise robustness. Finally, the overall
ranking-aware regularization shifts the model's focus from a local to a global
perspective, enabling it to capture the holistic ordering information among
trajectories. Extensive experiments on three publicly available datasets show
that TrajDiff consistently outperforms state-of-the-art baselines. In
particular, it achieves an average HR@1 gain of 33.38% across all three
evaluation metrics and datasets.

</details>


### [295] [Clinically Interpretable Mortality Prediction for ICU Patients with Diabetes and Atrial Fibrillation: A Machine Learning Approach](https://arxiv.org/abs/2506.15901)
*Li Sun,Shuheng Chen,Yong Si,Junyi Fan,Maryam Pishgar,Elham Pishgar,Kamiar Alaei,Greg Placencia*

Main category: cs.LG

TL;DR: 开发了一种可解释的机器学习模型，用于预测ICU中同时患有糖尿病和房颤患者的28天死亡率，逻辑回归表现最佳。


<details>
  <summary>Details</summary>
Motivation: 糖尿病和房颤患者在ICU中死亡率较高，但针对这一高风险群体的预测模型有限。

Method: 使用MIMIC-IV数据库中的1,535名患者数据，通过特征选择和多种机器学习模型训练，评估模型性能。

Result: 逻辑回归表现最佳（AUROC: 0.825），关键预测因素包括RAS、年龄、胆红素和拔管。

Conclusion: 该模型为糖尿病和房颤患者的早期ICU分诊提供了准确的风险预测和临床见解。

Abstract: Background: Patients with both diabetes mellitus (DM) and atrial fibrillation
(AF) face elevated mortality in intensive care units (ICUs), yet models
targeting this high-risk group remain limited.
  Objective: To develop an interpretable machine learning (ML) model predicting
28-day mortality in ICU patients with concurrent DM and AF using early-phase
clinical data.
  Methods: A retrospective cohort of 1,535 adult ICU patients with DM and AF
was extracted from the MIMIC-IV database. Data preprocessing involved
median/mode imputation, z-score normalization, and early temporal feature
engineering. A two-step feature selection pipeline-univariate filtering (ANOVA
F-test) and Random Forest-based multivariate ranking-yielded 19 interpretable
features. Seven ML models were trained with stratified 5-fold cross-validation
and SMOTE oversampling. Interpretability was assessed via ablation and
Accumulated Local Effects (ALE) analysis.
  Results: Logistic regression achieved the best performance (AUROC: 0.825; 95%
CI: 0.779-0.867), surpassing more complex models. Key predictors included RAS,
age, bilirubin, and extubation. ALE plots showed intuitive, non-linear effects
such as age-related risk acceleration and bilirubin thresholds.
  Conclusion: This interpretable ML model offers accurate risk prediction and
clinical insights for early ICU triage in patients with DM and AF.

</details>


### [296] [VectorEdits: A Dataset and Benchmark for Instruction-Based Editing of Vector Graphics](https://arxiv.org/abs/2506.15903)
*Josef Kuchař,Marek Kadlčík,Michal Spiegel,Michal Štefánik*

Main category: cs.LG

TL;DR: 论文介绍了一个大规模数据集，用于指令引导的矢量图像编辑，包含27万对SVG图像和自然语言编辑指令，旨在训练和评估基于文本命令修改矢量图形的模型。


<details>
  <summary>Details</summary>
Motivation: 当前方法在基于自然语言指令编辑矢量图形时表现不佳，因此需要构建一个大规模数据集以推动相关研究。

Method: 通过CLIP相似性配对图像，并利用视觉语言模型生成指令，构建数据集。

Result: 初步实验显示，现有大型语言模型难以生成准确且有效的编辑结果。

Conclusion: 公开数据集以促进自然语言驱动的矢量图形生成和编辑研究。

Abstract: We introduce a large-scale dataset for instruction-guided vector image
editing, consisting of over 270,000 pairs of SVG images paired with natural
language edit instructions. Our dataset enables training and evaluation of
models that modify vector graphics based on textual commands. We describe the
data collection process, including image pairing via CLIP similarity and
instruction generation with vision-language models. Initial experiments with
state-of-the-art large language models reveal that current methods struggle to
produce accurate and valid edits, underscoring the challenge of this task. To
foster research in natural language-driven vector graphic generation and
editing, we make our resources created within this work publicly available.

</details>


### [297] [Pieceformer: Similarity-Driven Knowledge Transfer via Scalable Graph Transformer in VLSI](https://arxiv.org/abs/2506.15907)
*Hang Yang,Yusheng Hu,Yong Liu,Cong,Hao*

Main category: cs.LG

TL;DR: Pieceformer是一个可扩展的自监督图相似性评估框架，结合了混合消息传递和图变换器编码器，显著提高了VLSI设计中的知识转移效率。


<details>
  <summary>Details</summary>
Motivation: 在VLSI设计中，准确的图相似性评估对知识转移至关重要，可以减少工程工作量和周转时间。

Method: 提出Pieceformer框架，采用混合消息传递和图变换器编码器，并引入线性变换器主干和分区训练管道以提高可扩展性。

Result: 在合成和真实CircuitNet数据集上，Pieceformer将平均绝对误差降低24.9%，并正确聚类所有真实设计组。

Conclusion: Pieceformer验证了其在现代VLSI系统中可扩展、无偏设计重用的有效性，并在分区任务中实现了高达89%的运行时减少。

Abstract: Accurate graph similarity is critical for knowledge transfer in VLSI design,
enabling the reuse of prior solutions to reduce engineering effort and
turnaround time. We propose Pieceformer, a scalable, self-supervised similarity
assessment framework, equipped with a hybrid message-passing and graph
transformer encoder. To address transformer scalability, we incorporate a
linear transformer backbone and introduce a partitioned training pipeline for
efficient memory and parallelism management. Evaluations on synthetic and
real-world CircuitNet datasets show that Pieceformer reduces mean absolute
error (MAE) by 24.9% over the baseline and is the only method to correctly
cluster all real-world design groups. We further demonstrate the practical
usage of our model through a case study on a partitioning task, achieving up to
89% runtime reduction. These results validate the framework's effectiveness for
scalable, unbiased design reuse in modern VLSI systems.

</details>


### [298] [PNCS:Power-Norm Cosine Similarity for Diverse Client Selection in Federated Learning](https://arxiv.org/abs/2506.15923)
*Liangyan Li,Yangyi Liu,Yimo Ning,Stefano Rini,Jun Chen*

Main category: cs.LG

TL;DR: 提出了一种基于Power-Norm Cosine Similarity（PNCS）的联邦学习框架，通过捕捉高阶梯度矩解决非独立同分布数据问题，提升收敛速度和准确性。


<details>
  <summary>Details</summary>
Motivation: 现有联邦学习方法未充分考虑客户端间梯度相关性，尤其在数据异构场景下表现不佳。

Method: 利用PNCS改进客户端选择，并通过选择历史队列确保多样性。

Result: 在VGG16模型上的实验表明，该方法在多种数据分区下均优于现有技术。

Conclusion: PNCS框架有效解决了数据异构性问题，提升了联邦学习的性能。

Abstract: Federated Learning (FL) has emerged as a powerful paradigm for leveraging
diverse datasets from multiple sources while preserving data privacy by
avoiding centralized storage. However, many existing approaches fail to account
for the intricate gradient correlations between remote clients, a limitation
that becomes especially problematic in data heterogeneity scenarios. In this
work, we propose a novel FL framework utilizing Power-Norm Cosine Similarity
(PNCS) to improve client selection for model aggregation. By capturing
higher-order gradient moments, PNCS addresses non-IID data challenges,
enhancing convergence speed and accuracy. Additionally, we introduce a simple
algorithm ensuring diverse client selection through a selection history queue.
Experiments with a VGG16 model across varied data partitions demonstrate
consistent improvements over state-of-the-art methods.

</details>


### [299] [Competing Bandits in Matching Markets via Super Stability](https://arxiv.org/abs/2506.15926)
*Soumya Basu*

Main category: cs.LG

TL;DR: 研究双边奖励不确定性的匹配市场中的强盗学习，扩展了以往单边不确定性的研究。通过扩展Gale-Shapley算法，实现了对数级的最差稳定遗憾。


<details>
  <summary>Details</summary>
Motivation: 填补双边奖励不确定性在匹配市场中的研究空白，探索更高效的稳定匹配算法。

Method: 利用扩展Gale-Shapley算法，设计集中式和分散式算法，分析其遗憾性能。

Result: 集中式算法实现了对数级的最差稳定遗憾，分散式算法仅增加常数遗憾。

Conclusion: 扩展Gale-Shapley算法在双边不确定性下表现优越，为稳定匹配的复杂性提供了新见解。

Abstract: We study bandit learning in matching markets with two-sided reward
uncertainty, extending prior research primarily focused on single-sided
uncertainty. Leveraging the concept of `super-stability' from Irving (1994), we
demonstrate the advantage of the Extended Gale-Shapley (GS) algorithm over the
standard GS algorithm in achieving true stable matchings under incomplete
information. By employing the Extended GS algorithm, our centralized algorithm
attains a logarithmic pessimal stable regret dependent on an instance-dependent
admissible gap parameter. This algorithm is further adapted to a decentralized
setting with a constant regret increase. Finally, we establish a novel
centralized instance-dependent lower bound for binary stable regret,
elucidating the roles of the admissible gap and super-stable matching in
characterizing the complexity of stable matching with bandit feedback.

</details>


### [300] [IsoNet: Causal Analysis of Multimodal Transformers for Neuromuscular Gesture Classification](https://arxiv.org/abs/2506.16744)
*Eion Tyacke,Kunal Gupta,Jay Patel,Rui Li*

Main category: cs.LG

TL;DR: 论文研究了多模态融合策略在手势解码中的效果，提出了一种基于注意力的分层Transformer方法，显著提升了分类精度。


<details>
  <summary>Details</summary>
Motivation: 手势解码是神经科学和辅助技术（如假肢）的关键瓶颈，传统方法依赖单一生物信号模态，而多模态融合能利用互补信息。

Method: 比较了线性与基于注意力的融合策略，测试了三种架构：多模态MLP、多模态Transformer和分层Transformer，并在公开数据集上评估。

Result: 分层Transformer结合注意力融合在两种数据集上表现最佳，精度提升显著。交叉模态交互贡献了约30%的决策信号。

Conclusion: 研究揭示了多模态融合的优势和机制，为神经机器人系统的传感器设计提供了指导。

Abstract: Hand gestures are a primary output of the human motor system, yet the
decoding of their neuromuscular signatures remains a bottleneck for basic
neuroscience and assistive technologies such as prosthetics. Traditional
human-machine interface pipelines rely on a single biosignal modality, but
multimodal fusion can exploit complementary information from sensors. We
systematically compare linear and attention-based fusion strategies across
three architectures: a Multimodal MLP, a Multimodal Transformer, and a
Hierarchical Transformer, evaluating performance on scenarios with unimodal and
multimodal inputs. Experiments use two publicly available datasets: NinaPro DB2
(sEMG and accelerometer) and HD-sEMG 65-Gesture (high-density sEMG and force).
Across both datasets, the Hierarchical Transformer with attention-based fusion
consistently achieved the highest accuracy, surpassing the multimodal and best
single-modality linear-fusion MLP baseline by over 10% on NinaPro DB2 and 3.7%
on HD-sEMG. To investigate how modalities interact, we introduce an Isolation
Network that selectively silences unimodal or cross-modal attention pathways,
quantifying each group of token interactions' contribution to downstream
decisions. Ablations reveal that cross-modal interactions contribute
approximately 30% of the decision signal across transformer layers,
highlighting the importance of attention-driven fusion in harnessing
complementary modality information. Together, these findings reveal when and
how multimodal fusion would enhance biosignal classification and also provides
mechanistic insights of human muscle activities. The study would be beneficial
in the design of sensor arrays for neurorobotic systems.

</details>


### [301] [CORAL: Disentangling Latent Representations in Long-Tailed Diffusion](https://arxiv.org/abs/2506.15933)
*Esther Rodriguez,Monica Welfert,Samuel McDowell,Nathan Stromberg,Julian Antolin Camarena,Lalitha Sankar*

Main category: cs.LG

TL;DR: 论文研究了扩散模型在长尾数据分布下的表现，提出了一种对比潜在对齐框架（CORAL）以改善尾部类别的生成质量。


<details>
  <summary>Details</summary>
Motivation: 现实世界中的多类数据通常呈现长尾分布，标准扩散模型在此类数据上表现不佳，尾部类别的生成样本多样性和质量较低。

Method: 通过分析扩散模型在长尾数据集上的行为，发现尾部类别的潜在表示与头部类别重叠严重。提出CORAL框架，利用监督对比损失分离潜在类别表示。

Result: 实验表明，CORAL显著提高了尾部类别生成样本的多样性和视觉质量。

Conclusion: CORAL通过对比潜在对齐有效解决了长尾分布下扩散模型的性能问题。

Abstract: Diffusion models have achieved impressive performance in generating
high-quality and diverse synthetic data. However, their success typically
assumes a class-balanced training distribution. In real-world settings,
multi-class data often follow a long-tailed distribution, where standard
diffusion models struggle -- producing low-diversity and lower-quality samples
for tail classes. While this degradation is well-documented, its underlying
cause remains poorly understood. In this work, we investigate the behavior of
diffusion models trained on long-tailed datasets and identify a key issue: the
latent representations (from the bottleneck layer of the U-Net) for tail class
subspaces exhibit significant overlap with those of head classes, leading to
feature borrowing and poor generation quality. Importantly, we show that this
is not merely due to limited data per class, but that the relative class
imbalance significantly contributes to this phenomenon. To address this, we
propose COntrastive Regularization for Aligning Latents (CORAL), a contrastive
latent alignment framework that leverages supervised contrastive losses to
encourage well-separated latent class representations. Experiments demonstrate
that CORAL significantly improves both the diversity and visual quality of
samples generated for tail classes relative to state-of-the-art methods.

</details>


### [302] [Off-Policy Actor-Critic for Adversarial Observation Robustness: Virtual Alternative Training via Symmetric Policy Evaluation](https://arxiv.org/abs/2506.16753)
*Kosuke Nakanishi,Akihiro Kubo,Yuji Yasui,Shin Ishii*

Main category: cs.LG

TL;DR: 提出了一种新的离策略方法，通过将对抗学习重新表述为软约束优化问题，消除了额外环境交互的需求。


<details>
  <summary>Details</summary>
Motivation: 现有方法在处理长期最坏情况时效率低下，且阻碍了离策略方法的发展。

Method: 将对抗学习重新表述为软约束优化问题，利用策略评估的对称性。

Result: 理论支持了方法的有效性，并提供了开源实现。

Conclusion: 新方法解决了现有方法的效率问题，为对抗强化学习提供了更优的解决方案。

Abstract: Recently, robust reinforcement learning (RL) methods designed to handle
adversarial input observations have received significant attention, motivated
by RL's inherent vulnerabilities. While existing approaches have demonstrated
reasonable success, addressing worst-case scenarios over long time horizons
requires both minimizing the agent's cumulative rewards for adversaries and
training agents to counteract them through alternating learning. However, this
process introduces mutual dependencies between the agent and the adversary,
making interactions with the environment inefficient and hindering the
development of off-policy methods. In this work, we propose a novel off-policy
method that eliminates the need for additional environmental interactions by
reformulating adversarial learning as a soft-constrained optimization problem.
Our approach is theoretically supported by the symmetric property of policy
evaluation between the agent and the adversary. The implementation is available
at https://github.com/nakanakakosuke/VALT_SAC.

</details>


### [303] [On the optimal regret of collaborative personalized linear bandits](https://arxiv.org/abs/2506.15943)
*Bruce Huang,Ruida Zhou,Lin F. Yang,Suhas Diggavi*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Stochastic linear bandits are a fundamental model for sequential decision
making, where an agent selects a vector-valued action and receives a noisy
reward with expected value given by an unknown linear function. Although well
studied in the single-agent setting, many real-world scenarios involve multiple
agents solving heterogeneous bandit problems, each with a different unknown
parameter. Applying single agent algorithms independently ignores cross-agent
similarity and learning opportunities. This paper investigates the optimal
regret achievable in collaborative personalized linear bandits. We provide an
information-theoretic lower bound that characterizes how the number of agents,
the interaction rounds, and the degree of heterogeneity jointly affect regret.
We then propose a new two-stage collaborative algorithm that achieves the
optimal regret. Our analysis models heterogeneity via a hierarchical Bayesian
framework and introduces a novel information-theoretic technique for bounding
regret. Our results offer a complete characterization of when and how
collaboration helps with a optimal regret bound $\tilde{O}(d\sqrt{mn})$,
$\tilde{O}(dm^{1-\gamma}\sqrt{n})$, $\tilde{O}(dm\sqrt{n})$ for the number of
rounds $n$ in the range of $(0, \frac{d}{m \sigma^2})$, $[\frac{d}{m^{2\gamma}
\sigma^2}, \frac{d}{\sigma^2}]$ and $(\frac{d}{\sigma^2}, \infty)$
respectively, where $\sigma$ measures the level of heterogeneity, $m$ is the
number of agents, and $\gamma\in[0, 1/2]$ is an absolute constant. In contrast,
agents without collaboration achieve a regret bound $O(dm\sqrt{n})$ at best.

</details>


### [304] [One Period to Rule Them All: Identifying Critical Learning Periods in Deep Networks](https://arxiv.org/abs/2506.15954)
*Vinicius Yuiti Fukase,Heitor Gama,Barbara Bueno,Lucas Libanio,Anna Helena Reali Costa,Artur Jordao*

Main category: cs.LG

TL;DR: 本文提出了一种系统方法，用于识别深度神经网络训练中的关键学习期，通过减少计算成本和资源消耗，显著提升训练效率和可持续性。


<details>
  <summary>Details</summary>
Motivation: 现有研究证实了关键学习期的存在，但缺乏对其具体发生时间的精确识别。本文旨在填补这一空白。

Method: 利用泛化预测机制识别关键期，停止资源密集型训练方法，结合数据剪枝等技术降低计算成本。

Result: 实验表明，该方法将流行架构的训练时间减少59.67%，CO$_2$排放减少59.47%，成本降低60%，且不影响性能。

Conclusion: 该方法为深度学习提供了更高效、可持续的训练框架，特别适用于资源受限环境。

Abstract: Critical Learning Periods comprehend an important phenomenon involving deep
learning, where early epochs play a decisive role in the success of many
training recipes, such as data augmentation. Existing works confirm the
existence of this phenomenon and provide useful insights. However, the
literature lacks efforts to precisely identify when critical periods occur. In
this work, we fill this gap by introducing a systematic approach for
identifying critical periods during the training of deep neural networks,
focusing on eliminating computationally intensive regularization techniques and
effectively applying mechanisms for reducing computational costs, such as data
pruning. Our method leverages generalization prediction mechanisms to pinpoint
critical phases where training recipes yield maximum benefits to the predictive
ability of models. By halting resource-intensive recipes beyond these periods,
we significantly accelerate the learning phase and achieve reductions in
training time, energy consumption, and CO$_2$ emissions. Experiments on
standard architectures and benchmarks confirm the effectiveness of our method.
Specifically, we achieve significant milestones by reducing the training time
of popular architectures by up to 59.67%, leading to a 59.47% decrease in
CO$_2$ emissions and a 60% reduction in financial costs, without compromising
performance. Our work enhances understanding of training dynamics and paves the
way for more sustainable and efficient deep learning practices, particularly in
resource-constrained environments. In the era of the race for foundation
models, we believe our method emerges as a valuable framework. The repository
is available at https://github.com/baunilhamarga/critical-periods

</details>


### [305] [On the Theoretical Understanding of Identifiable Sparse Autoencoders and Beyond](https://arxiv.org/abs/2506.15963)
*Jingyi Cui,Qi Zhang,Yifei Wang,Yisen Wang*

Main category: cs.LG

TL;DR: 论文提出了稀疏自编码器（SAEs）在恢复单义特征时的必要和充分条件，并提出了一种加权策略以提高可识别性。


<details>
  <summary>Details</summary>
Motivation: 研究SAEs在何种条件下能够完全从叠加的多义特征中恢复真实的单义特征。

Method: 通过理论分析提出可识别SAEs的条件，并提出加权策略缩小损失函数差距。

Result: 实验验证了理论发现，加权SAE显著提高了特征的单义性和可解释性。

Conclusion: 论文为SAEs的可识别性提供了理论支持，并通过加权策略改进了性能。

Abstract: Sparse autoencoders (SAEs) have emerged as a powerful tool for interpreting
features learned by large language models (LLMs). It aims to recover complex
superposed polysemantic features into interpretable monosemantic ones through
feature reconstruction via sparsely activated neural networks. Despite the wide
applications of SAEs, it remains unclear under what conditions an SAE can fully
recover the ground truth monosemantic features from the superposed polysemantic
ones. In this paper, through theoretical analysis, we for the first time
propose the necessary and sufficient conditions for identifiable SAEs (SAEs
that learn unique and ground truth monosemantic features), including 1) extreme
sparsity of the ground truth feature, 2) sparse activation of SAEs, and 3)
enough hidden dimensions of SAEs. Moreover, when the identifiable conditions
are not fully met, we propose a reweighting strategy to improve the
identifiability. Specifically, following the theoretically suggested weight
selection principle, we prove that the gap between the loss functions of SAE
reconstruction and monosemantic feature reconstruction can be narrowed, so that
the reweighted SAEs have better reconstruction of the ground truth monosemantic
features than the uniformly weighted ones. In experiments, we validate our
theoretical findings and show that our weighted SAE significantly improves
feature monosemanticity and interpretability.

</details>


### [306] [LazyEviction: Lagged KV Eviction with Attention Pattern Observation for Efficient Long Reasoning](https://arxiv.org/abs/2506.15969)
*Haoyue Zhang,Hualei Zhang,Xiaosong Ma,Jie Zhang,Song Guo*

Main category: cs.LG

TL;DR: 论文提出LazyEviction框架，通过延迟KV缓存淘汰来减少内存开销，同时保持推理性能。


<details>
  <summary>Details</summary>
Motivation: 现有KV缓存压缩方法在长推理任务中表现不佳，未能捕捉到Token Importance Recurrence现象。

Method: 提出LazyEviction框架，包含Recurrence Interval Tracking和Maximum Recurrence Interval-Centric Eviction Policy。

Result: 实验显示LazyEviction将KV缓存大小减少50%，同时在数学推理任务中保持准确性。

Conclusion: 保留周期性关键Token对多步推理任务的知识连续性至关重要。

Abstract: Large Language Models (LLMs) exhibit enhanced reasoning capabilities by
employing Chain-of-Thought (CoT). However, the extended reasoning sequences
introduce significant GPU memory overhead due to increased key-value (KV) cache
size, particularly in tasks requiring long reasoning sequences, such as
mathematics and programming. Existing KV cache compression methods mitigate
memory bottlenecks but struggle in long reasoning tasks. In this paper, we
analyze attention patterns in reasoning tasks and reveal a Token Importance
Recurrence phenomenon: a large proportion of tokens receive renewed attention
after multiple decoding steps, which is failed to capture by existing works and
may lead to unpredictable eviction on such periodically critical tokens. To
address this, we propose LazyEviction, a lagged KV eviction framework designed
to maintain reasoning performance while reducing KV memory. LazyEviction is an
Observation Window-based Lagged Eviction Mechanism retaining latent recurring
tokens by performing lagged evictions across decoding steps, which contains two
key components: (1) Recurrence Interval Tracking for capturing temporal
variations in token importance, and (2) an Maximum Recurrence Interval-Centric
Eviction Policy that prioritizes eviction based on tokens' recurrence patterns.
Extensive experiments demonstrate that LazyEviction reduces KV cache size by
50% while maintaining comparable accuracy on mathematics reasoning datasets,
outperforming state-of-the-art methods. Our findings highlight the importance
of preserving recurring tokens, which are critical for maintaining knowledge
continuity in multi-step reasoning tasks.

</details>


### [307] [AutoHFormer: Efficient Hierarchical Autoregressive Transformer for Time Series Prediction](https://arxiv.org/abs/2506.16001)
*Qianru Zhang,Honggang Wen,Ming Li,Dong Huang,Siu-Ming Yiu,Christian S. Jensen,Pietro Liò*

Main category: cs.LG

TL;DR: AutoHFormer是一种分层自回归Transformer，通过分层时间建模、动态窗口注意力和自适应时间编码，解决了时间序列预测中的三个关键挑战，实现了高效且精确的预测。


<details>
  <summary>Details</summary>
Motivation: 解决时间序列预测中严格时间因果关系、次二次复杂度和多尺度模式识别的竞争需求。

Method: 采用分层时间建模、动态窗口注意力和自适应时间编码，结合并行处理和顺序细化。

Result: 在PEMS08数据集上，训练速度提升10.76倍，内存减少6.06倍，同时在96-720步预测范围内保持准确性。

Conclusion: AutoHFormer为高效且精确的时间序列建模设立了新基准。

Abstract: Time series forecasting requires architectures that simultaneously achieve
three competing objectives: (1) strict temporal causality for reliable
predictions, (2) sub-quadratic complexity for practical scalability, and (3)
multi-scale pattern recognition for accurate long-horizon forecasting. We
introduce AutoHFormer, a hierarchical autoregressive transformer that addresses
these challenges through three key innovations: 1) Hierarchical Temporal
Modeling: Our architecture decomposes predictions into segment-level blocks
processed in parallel, followed by intra-segment sequential refinement. This
dual-scale approach maintains temporal coherence while enabling efficient
computation. 2) Dynamic Windowed Attention: The attention mechanism employs
learnable causal windows with exponential decay, reducing complexity while
preserving precise temporal relationships. This design avoids both the
anti-causal violations of standard transformers and the sequential bottlenecks
of RNN hybrids. 3) Adaptive Temporal Encoding: a novel position encoding system
is adopted to capture time patterns at multiple scales. It combines fixed
oscillating patterns for short-term variations with learnable decay rates for
long-term trends. Comprehensive experiments demonstrate that AutoHFormer 10.76X
faster training and 6.06X memory reduction compared to PatchTST on PEMS08,
while maintaining consistent accuracy across 96-720 step horizons in most of
cases. These breakthroughs establish new benchmarks for efficient and precise
time series modeling. Implementations of our method and all baselines in
hierarchical autoregressive mechanism are available at
https://github.com/lizzyhku/Autotime.

</details>


### [308] [Bridging Brain with Foundation Models through Self-Supervised Learning](https://arxiv.org/abs/2506.16009)
*Hamdi Altaheri,Fakhri Karray,Md. Milon Islam,S M Taslim Uddin Raju,Amir-Hossein Karimi*

Main category: cs.LG

TL;DR: 综述探讨了自监督学习（SSL）如何通过基础模型（FMs）革新脑信号分析，解决了传统监督学习在标记数据稀缺时的局限性，并展望了未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 传统监督学习在脑信号分析中受限于标记数据的稀缺性，而自监督学习（SSL）能够从未标记数据中学习有意义的表示，为脑信号分析提供了新机遇。

Method: 系统回顾了SSL技术在脑信号分析中的应用，包括关键SSL技术、脑专用基础模型的开发、下游任务适配以及多模态SSL框架中的脑信号整合。

Result: 综述总结了常用评估指标和基准数据集，支持比较分析，并展示了SSL在脑信号分析中的潜力。

Conclusion: 该领域仍面临挑战，但SSL驱动的通用脑基础模型开发前景广阔，为研究者提供了结构化理解和未来研究路线图。

Abstract: Foundation models (FMs), powered by self-supervised learning (SSL), have
redefined the capabilities of artificial intelligence, demonstrating
exceptional performance in domains like natural language processing and
computer vision. These advances present a transformative opportunity for brain
signal analysis. Unlike traditional supervised learning, which is limited by
the scarcity of labeled neural data, SSL offers a promising solution by
enabling models to learn meaningful representations from unlabeled data. This
is particularly valuable in addressing the unique challenges of brain signals,
including high noise levels, inter-subject variability, and low signal-to-noise
ratios. This survey systematically reviews the emerging field of bridging brain
signals with foundation models through the innovative application of SSL. It
explores key SSL techniques, the development of brain-specific foundation
models, their adaptation to downstream tasks, and the integration of brain
signals with other modalities in multimodal SSL frameworks. The review also
covers commonly used evaluation metrics and benchmark datasets that support
comparative analysis. Finally, it highlights key challenges and outlines future
research directions. This work aims to provide researchers with a structured
understanding of this rapidly evolving field and a roadmap for developing
generalizable brain foundation models powered by self-supervision.

</details>


### [309] [VRAIL: Vectorized Reward-based Attribution for Interpretable Learning](https://arxiv.org/abs/2506.16014)
*Jina Kim,Youjin Jang,Jeongjin Han*

Main category: cs.LG

TL;DR: VRAIL是一个基于向量化奖励的双层框架，用于强化学习，通过学习可解释的权重表示提升训练稳定性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 提出一种方法，既能提升强化学习的性能，又能提供可解释的行为分析。

Method: 分为深度学习阶段和强化学习阶段，通过线性或二次模型估计价值函数，并利用奖励转换进行学习。

Result: 在Taxi-v3环境中，VRAIL比标准DQN表现更好，训练更稳定且能发现语义上有意义的子目标。

Conclusion: VRAIL是一个通用且模型无关的框架，能同时提升学习效果和可解释性。

Abstract: We propose VRAIL (Vectorized Reward-based Attribution for Interpretable
Learning), a bi-level framework for value-based reinforcement learning (RL)
that learns interpretable weight representations from state features. VRAIL
consists of two stages: a deep learning (DL) stage that fits an estimated value
function using state features, and an RL stage that uses this to shape learning
via potential-based reward transformations. The estimator is modeled in either
linear or quadratic form, allowing attribution of importance to individual
features and their interactions. Empirical results on the Taxi-v3 environment
demonstrate that VRAIL improves training stability and convergence compared to
standard DQN, without requiring environment modifications. Further analysis
shows that VRAIL uncovers semantically meaningful subgoals, such as passenger
possession, highlighting its ability to produce human-interpretable behavior.
Our findings suggest that VRAIL serves as a general, model-agnostic framework
for reward shaping that enhances both learning and interpretability.

</details>


### [310] [A Scalable Factorization Approach for High-Order Structured Tensor Recovery](https://arxiv.org/abs/2506.16032)
*Zhen Qin,Michael B. Wakin,Zhihui Zhu*

Main category: cs.LG

TL;DR: 论文提出了一种统一的框架，通过Riemannian梯度下降（RGD）优化张量分解问题，证明了在适当初始化下RGD能以线性速率收敛到真实张量。


<details>
  <summary>Details</summary>
Motivation: 张量分解能显著减少参数数量，但由于非凸性，收敛分析和恢复保证存在挑战。本文旨在解决这一问题。

Method: 利用张量分解的规范形式，应用Riemannian梯度下降（RGD）在Stiefel流形上优化正交因子。

Result: 在损失函数的温和条件下，证明了RGD能以线性速率收敛到真实张量，且初始化和收敛速率与张量阶数多项式相关。

Conclusion: 该方法改进了现有Tucker和张量列格式的结果，为张量分解问题提供了更高效的解决方案。

Abstract: Tensor decompositions, which represent an $N$-order tensor using
approximately $N$ factors of much smaller dimensions, can significantly reduce
the number of parameters. This is particularly beneficial for high-order
tensors, as the number of entries in a tensor grows exponentially with the
order. Consequently, they are widely used in signal recovery and data analysis
across domains such as signal processing, machine learning, and quantum
physics. A computationally and memory-efficient approach to these problems is
to optimize directly over the factors using local search algorithms such as
gradient descent, a strategy known as the factorization approach in matrix and
tensor optimization. However, the resulting optimization problems are highly
nonconvex due to the multiplicative interactions between factors, posing
significant challenges for convergence analysis and recovery guarantees.
  In this paper, we present a unified framework for the factorization approach
to solving various tensor decomposition problems. Specifically, by leveraging
the canonical form of tensor decompositions--where most factors are constrained
to be orthonormal to mitigate scaling ambiguity--we apply Riemannian gradient
descent (RGD) to optimize these orthonormal factors on the Stiefel manifold.
Under a mild condition on the loss function, we establish a Riemannian
regularity condition for the factorized objective and prove that RGD converges
to the ground-truth tensor at a linear rate when properly initialized. Notably,
both the initialization requirement and the convergence rate scale polynomially
rather than exponentially with $N$, improving upon existing results for Tucker
and tensor-train format tensors.

</details>


### [311] [Vision-Guided Chunking Is All You Need: Enhancing RAG with Multimodal Document Understanding](https://arxiv.org/abs/2506.16035)
*Vishesh Tripathi,Tanmay Odapally,Indraneel Das,Uday Allu,Biddwan Ahmed*

Main category: cs.LG

TL;DR: 提出了一种基于大型多模态模型（LMMs）的多模态文档分块方法，用于改进PDF文档的处理，解决了传统文本分块方法在复杂文档结构、多页表格和跨页上下文依赖中的不足。


<details>
  <summary>Details</summary>
Motivation: 传统文本分块方法在处理复杂文档结构（如多页表格、嵌入式图表和跨页上下文依赖）时效果不佳，影响了检索增强生成（RAG）系统的性能。

Method: 采用多模态模型（LMMs）分批次处理PDF文档，保持语义连贯性和结构完整性，支持跨批次上下文保留，以准确处理多页表格和嵌入式视觉元素。

Result: 在精选的PDF文档数据集上评估，新方法在分块质量和下游RAG性能上优于传统方法，定性分析显示其能更好地保持文档结构和语义连贯性。

Conclusion: 基于视觉指导的多模态分块方法显著提升了RAG系统的准确性，尤其在处理复杂文档结构时表现优异。

Abstract: Retrieval-Augmented Generation (RAG) systems have revolutionized information
retrieval and question answering, but traditional text-based chunking methods
struggle with complex document structures, multi-page tables, embedded figures,
and contextual dependencies across page boundaries. We present a novel
multimodal document chunking approach that leverages Large Multimodal Models
(LMMs) to process PDF documents in batches while maintaining semantic coherence
and structural integrity. Our method processes documents in configurable page
batches with cross-batch context preservation, enabling accurate handling of
tables spanning multiple pages, embedded visual elements, and procedural
content. We evaluate our approach on a curated dataset of PDF documents with
manually crafted queries, demonstrating improvements in chunk quality and
downstream RAG performance. Our vision-guided approach achieves better accuracy
compared to traditional vanilla RAG systems, with qualitative analysis showing
superior preservation of document structure and semantic coherence.

</details>


### [312] [From Data to Decision: Data-Centric Infrastructure for Reproducible ML in Collaborative eScience](https://arxiv.org/abs/2506.16051)
*Zhiwei Li,Carl Kesselman,Tran Huy Nguyen,Benjamin Yixing Xu,Kyle Bolo,Kimberley Yu*

Main category: cs.LG

TL;DR: 论文提出了一种数据为中心的生命周期感知可复现性框架，通过六个结构化工件（数据集、特征、工作流、执行、资产和受控词汇）来解决机器学习中的可复现性挑战。


<details>
  <summary>Details</summary>
Motivation: 机器学习（ML）中的可复现性是一个核心挑战，尤其是在协作式电子科学项目中，团队需要迭代数据、特征和模型。当前的ML工作流程通常是动态但碎片化的，依赖非正式的数据共享、临时脚本和松散连接的工具，这阻碍了透明度、可复现性和实验的适应性。

Method: 论文引入了一个数据为中心的生命周期感知可复现性框架，围绕六个结构化工件（Dataset、Feature、Workflow、Execution、Asset和Controlled Vocabulary）展开，这些工件形式化了数据、代码和决策之间的关系。

Result: 通过一个临床ML用例（青光眼检测）展示了该框架如何支持迭代探索、提高可复现性，并在ML生命周期中保留协作决策的溯源。

Conclusion: 该框架通过结构化工件实现了ML实验的版本控制、可解释性和可追溯性，从而提升了协作项目的透明度和可复现性。

Abstract: Reproducibility remains a central challenge in machine learning (ML),
especially in collaborative eScience projects where teams iterate over data,
features, and models. Current ML workflows are often dynamic yet fragmented,
relying on informal data sharing, ad hoc scripts, and loosely connected tools.
This fragmentation impedes transparency, reproducibility, and the adaptability
of experiments over time. This paper introduces a data-centric framework for
lifecycle-aware reproducibility, centered around six structured artifacts:
Dataset, Feature, Workflow, Execution, Asset, and Controlled Vocabulary. These
artifacts formalize the relationships between data, code, and decisions,
enabling ML experiments to be versioned, interpretable, and traceable over
time. The approach is demonstrated through a clinical ML use case of glaucoma
detection, illustrating how the system supports iterative exploration, improves
reproducibility, and preserves the provenance of collaborative decisions across
the ML lifecycle.

</details>


### [313] [CRIA: A Cross-View Interaction and Instance-Adapted Pre-training Framework for Generalizable EEG Representations](https://arxiv.org/abs/2506.16056)
*Puchun Liu,C. L. Philip Chen,Yubin He,Tong Zhang*

Main category: cs.LG

TL;DR: CRIA是一个自适应框架，通过多视图融合和注意力机制提升EEG表示学习的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有预训练方法仅依赖单一视图，无法捕捉多视图间的复杂交互，限制了表示的表达能力和泛化性。

Method: 提出CRIA框架，利用变长和变通道编码统一EEG表示，通过跨注意力机制融合时域、频域和空域特征，并结合信息瓶颈原则的注意力掩码策略。

Result: 在Temple University EEG和CHB-MIT数据集上，CRIA在多类事件分类（57.02%）和异常检测（80.03%）中表现优于现有方法。

Conclusion: CRIA通过多视图融合和注意力机制显著提升了EEG表示学习的泛化能力和性能。

Abstract: The difficulty of extracting deep features from EEG data and effectively
integrating information from multiple views presents significant challenges for
developing a generalizable pretraining framework for EEG representation
learning. However, most existing pre-training methods rely solely on the
contextual semantics of a single view, failing to capture the complex and
synergistic interactions among different perspectives, limiting the
expressiveness and generalization of learned representations. To address these
issues, this paper proposes CRIA, an adaptive framework that utilizes
variable-length and variable-channel coding to achieve a unified representation
of EEG data across different datasets. In this work, we define cross-view
information as the integrated representation that emerges from the interaction
among temporal, spectral, and spatial views of EEG signals. The model employs a
cross-attention mechanism to fuse temporal, spectral, and spatial features
effectively, and combines an attention matrix masking strategy based on the
information bottleneck principle with a novel viewpoint masking pre-training
scheme. Experimental results on the Temple University EEG corpus and the
CHB-MIT dataset show that CRIA outperforms existing methods with the same
pre-training conditions, achieving a balanced accuracy of 57.02% for
multi-class event classification and 80.03% for anomaly detection, highlighting
its strong generalization ability.

</details>


### [314] [Floating-Point Neural Networks Are Provably Robust Universal Approximators](https://arxiv.org/abs/2506.16065)
*Geonho Hwang,Wonyeol Lee,Yeachan Park,Sejun Park,Feras Saad*

Main category: cs.LG

TL;DR: 本文提出了首个针对浮点神经网络的区间通用逼近定理（IUA），证明了其在浮点计算环境下仍能完美逼近目标函数的直接图像映射。


<details>
  <summary>Details</summary>
Motivation: 研究浮点神经网络是否仍能保持区间通用逼近性质，以弥补经典理论中无限精度实数假设的不足。

Method: 通过理论分析，证明了浮点神经网络在有限精度下仍能实现区间通用逼近。

Result: 浮点神经网络在有限精度下仍具有强大的表达能力，且存在可证明鲁棒的浮点神经网络。

Conclusion: 浮点神经网络的IUA定理揭示了其与实数模型在计算上的本质差异，并带来了计算完备性等意外推论。

Abstract: The classical universal approximation (UA) theorem for neural networks
establishes mild conditions under which a feedforward neural network can
approximate a continuous function $f$ with arbitrary accuracy. A recent result
shows that neural networks also enjoy a more general interval universal
approximation (IUA) theorem, in the sense that the abstract interpretation
semantics of the network using the interval domain can approximate the direct
image map of $f$ (i.e., the result of applying $f$ to a set of inputs) with
arbitrary accuracy. These theorems, however, rest on the unrealistic assumption
that the neural network computes over infinitely precise real numbers, whereas
their software implementations in practice compute over finite-precision
floating-point numbers. An open question is whether the IUA theorem still holds
in the floating-point setting.
  This paper introduces the first IUA theorem for floating-point neural
networks that proves their remarkable ability to perfectly capture the direct
image map of any rounded target function $f$, showing no limits exist on their
expressiveness. Our IUA theorem in the floating-point setting exhibits material
differences from the real-valued setting, which reflects the fundamental
distinctions between these two computational models. This theorem also implies
surprising corollaries, which include (i) the existence of provably robust
floating-point neural networks; and (ii) the computational completeness of the
class of straight-line programs that use only floating-point additions and
multiplications for the class of all floating-point programs that halt.

</details>


### [315] [A Lightweight RL-Driven Deep Unfolding Network for Robust WMMSE Precoding in Massive MU-MIMO-OFDM Systems](https://arxiv.org/abs/2506.16072)
*Kexuan Wang,An Liu*

Main category: cs.LG

TL;DR: 论文提出了一种基于强化学习的深度展开网络（RLDDU-Net），用于解决WMMSE预编码在信道状态信息不完美和高计算复杂度下的问题，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: WMMSE预编码在大规模MU-MIMO-OFDM系统中因假设完美CSI和高计算复杂度而难以实际部署。

Method: 开发了宽频随机WMMSE（SWMMSE）算法，并基于此提出RLDDU-Net，将SWMMSE迭代映射到网络层，结合近似技术和稀疏性加速收敛。

Result: 仿真显示RLDDU-Net在不完美CSI下优于现有基线，具有更高的EWSR性能和计算效率。

Conclusion: RLDDU-Net通过深度展开和强化学习有效解决了WMMSE的局限性，为实际部署提供了高效解决方案。

Abstract: Weighted Minimum Mean Square Error (WMMSE) precoding is widely recognized for
its near-optimal weighted sum rate performance. However, its practical
deployment in massive multi-user (MU) multiple-input multiple-output (MIMO)
orthogonal frequency-division multiplexing (OFDM) systems is hindered by the
assumption of perfect channel state information (CSI) and high computational
complexity. To address these issues, we first develop a wideband stochastic
WMMSE (SWMMSE) algorithm that iteratively maximizes the ergodic weighted
sum-rate (EWSR) under imperfect CSI. Building on this, we propose a lightweight
reinforcement learning (RL)-driven deep unfolding (DU) network (RLDDU-Net),
where each SWMMSE iteration is mapped to a network layer. Specifically, its DU
module integrates approximation techniques and leverages beam-domain sparsity
as well as frequency-domain subcarrier correlation, significantly accelerating
convergence and reducing computational overhead. Furthermore, the RL module
adaptively adjusts the network depth and generates compensation matrices to
mitigate approximation errors. Simulation results under imperfect CSI
demonstrate that RLDDU-Net outperforms existing baselines in EWSR performance
while offering superior computational and convergence efficiency.

</details>


### [316] [Joint User Priority and Power Scheduling for QoS-Aware WMMSE Precoding: A Constrained-Actor Attentive-Critic Approach](https://arxiv.org/abs/2506.16074)
*Kexuan Wang,An Liu*

Main category: cs.LG

TL;DR: 提出了一种基于约束强化学习的动态优先级和功率分配算法（CAAC），用于优化6G网络中的WMMSE预编码，提升能效和QoS满意度。


<details>
  <summary>Details</summary>
Motivation: 传统WMMSE预编码因固定优先级和功率分配，无法适应动态用户需求和信道变化，需更灵活的解决方案。

Method: 结合约束随机逐次凸逼近（CSSCA）和注意力增强Q网络，动态优化用户优先级和功率分配。

Result: 仿真显示CAAC在能效和QoS满意度上优于基线方法。

Conclusion: CAAC为6G网络中的动态资源分配提供了高效且灵活的解决方案。

Abstract: 6G wireless networks are expected to support diverse quality-of-service (QoS)
demands while maintaining high energy efficiency. Weighted Minimum Mean Square
Error (WMMSE) precoding with fixed user priorities and transmit power is widely
recognized for enhancing overall system performance but lacks flexibility to
adapt to user-specific QoS requirements and time-varying channel conditions. To
address this, we propose a novel constrained reinforcement learning (CRL)
algorithm, Constrained-Actor Attentive-Critic (CAAC), which uses a policy
network to dynamically allocate user priorities and power for WMMSE precoding.
Specifically, CAAC integrates a Constrained Stochastic Successive Convex
Approximation (CSSCA) method to optimize the policy, enabling more effective
handling of energy efficiency goals and satisfaction of stochastic non-convex
QoS constraints compared to traditional and existing CRL methods. Moreover,
CAAC employs lightweight attention-enhanced Q-networks to evaluate policy
updates without prior environment model knowledge. The network architecture not
only enhances representational capacity but also boosts learning efficiency.
Simulation results show that CAAC outperforms baselines in both energy
efficiency and QoS satisfaction.

</details>


### [317] [Probing the Robustness of Large Language Models Safety to Latent Perturbations](https://arxiv.org/abs/2506.16078)
*Tianle Gu,Kexin Huang,Zongqi Wang,Yixu Wang,Jie Li,Yuanqi Yao,Yang Yao,Yujiu Yang,Yan Teng,Yingchun Wang*

Main category: cs.LG

TL;DR: 论文探讨了现有安全对齐方法的浅层性，提出了一种探测方法（ASA）和新的训练策略（LAPT）以提高对齐鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有安全对齐方法仅关注表面拒绝行为，未充分改变内部表示，导致微小潜在偏移仍可能触发不安全响应。

Method: 引入负对数似然探测方法量化潜在空间敏感性，提出ASA攻击；设计LAPT训练策略，注入受控扰动以增强鲁棒性。

Result: 实验表明LAPT能提升对齐鲁棒性且不影响模型通用能力。

Conclusion: 当前对齐范式存在根本缺陷，需转向表示级训练策略。

Abstract: Safety alignment is a key requirement for building reliable Artificial
General Intelligence. Despite significant advances in safety alignment, we
observe that minor latent shifts can still trigger unsafe responses in aligned
models. We argue that this stems from the shallow nature of existing alignment
methods, which focus on surface-level refusal behaviors without sufficiently
altering internal representations. Consequently, small shifts in hidden
activations can re-trigger harmful behaviors embedded in the latent space. To
explore the robustness of safety alignment to latent perturbations, we
introduce a probing method that measures the Negative Log-Likelihood of the
original response generated by the model. This probe quantifies local
sensitivity in the latent space, serving as a diagnostic tool for identifying
vulnerable directions. Based on this signal, we construct effective jailbreak
trajectories, giving rise to the Activation Steering Attack (ASA). More
importantly, these insights offer a principled foundation for improving
alignment robustness. To this end, we introduce Layer-wise Adversarial Patch
Training~(LAPT), a fine-tuning strategy that inject controlled perturbations
into hidden representations during training. Experimental results highlight
that LAPT strengthen alignment robustness without compromising general
capabilities. Our findings reveal fundamental flaws in current alignment
paradigms and call for representation-level training strategies that move
beyond surface-level behavior supervision. Codes and results are available at
https://github.com/Carol-gutianle/LatentSafety.

</details>


### [318] [A Brain-to-Population Graph Learning Framework for Diagnosing Brain Disorders](https://arxiv.org/abs/2506.16096)
*Qianqian Liao,Wuque Cai,Hongze Sun,Dongze Liu,Duo Chen,Dezhong Yao,Daqing Guo*

Main category: cs.LG

TL;DR: 提出了一种名为B2P-GL的两阶段框架，结合脑区语义相似性和基于条件的群体图建模，用于脑部疾病诊断。


<details>
  <summary>Details</summary>
Motivation: 现有基于图的方法依赖预定义脑图谱，但忽略了图谱中的丰富信息及站点和表型变异的混杂效应。

Method: 第一阶段利用GPT-4知识丰富脑图谱表示，通过自适应节点重分配图注意力网络优化脑图；第二阶段结合表型数据构建群体图并融合特征以提升诊断性能。

Result: 在ABIDE I、ADHD-200和Rest-meta-MDD数据集上，B2P-GL在预测准确性和可解释性上优于现有方法。

Conclusion: B2P-GL为脑部疾病诊断提供了可靠且个性化的方法，推动了临床应用的进展。

Abstract: Recent developed graph-based methods for diagnosing brain disorders using
functional connectivity highly rely on predefined brain atlases, but overlook
the rich information embedded within atlases and the confounding effects of
site and phenotype variability. To address these challenges, we propose a
two-stage Brain-to-Population Graph Learning (B2P-GL) framework that integrates
the semantic similarity of brain regions and condition-based population graph
modeling. In the first stage, termed brain representation learning, we leverage
brain atlas knowledge from GPT-4 to enrich the graph representation and refine
the brain graph through an adaptive node reassignment graph attention network.
In the second stage, termed population disorder diagnosis, phenotypic data is
incorporated into population graph construction and feature fusion to mitigate
confounding effects and enhance diagnosis performance. Experiments on the ABIDE
I, ADHD-200, and Rest-meta-MDD datasets show that B2P-GL outperforms
state-of-the-art methods in prediction accuracy while enhancing
interpretability. Overall, our proposed framework offers a reliable and
personalized approach to brain disorder diagnosis, advancing clinical
applicability.

</details>


### [319] [Mitigating Over-Squashing in Graph Neural Networks by Spectrum-Preserving Sparsification](https://arxiv.org/abs/2506.16110)
*Langzhang Liang,Fanchen Bu,Zixing Song,Zenglin Xu,Shirui Pan,Kijung Shin*

Main category: cs.LG

TL;DR: 提出了一种基于谱保留图稀疏化的新图重连方法，以缓解图神经网络中的过挤压问题，同时保持图的稀疏性和谱特性。


<details>
  <summary>Details</summary>
Motivation: 图神经网络在远距离节点间信息传递时存在过挤压问题，现有图重连方法常忽略原始图的谱特性或引入计算开销。

Method: 利用谱保留图稀疏化技术，生成具有增强连通性但保持稀疏性和原始图谱的图。

Result: 实验证明该方法在分类准确性和拉普拉斯谱保留方面优于基线方法。

Conclusion: 该方法有效平衡了结构瓶颈减少和图特性保留，为图重连提供了新思路。

Abstract: The message-passing paradigm of Graph Neural Networks often struggles with
exchanging information across distant nodes typically due to structural
bottlenecks in certain graph regions, a limitation known as
\textit{over-squashing}. To reduce such bottlenecks, \textit{graph rewiring},
which modifies graph topology, has been widely used. However, existing graph
rewiring techniques often overlook the need to preserve critical properties of
the original graph, e.g., \textit{spectral properties}. Moreover, many
approaches rely on increasing edge count to improve connectivity, which
introduces significant computational overhead and exacerbates the risk of
over-smoothing. In this paper, we propose a novel graph rewiring method that
leverages \textit{spectrum-preserving} graph \textit{sparsification}, for
mitigating over-squashing. Our method generates graphs with enhanced
connectivity while maintaining sparsity and largely preserving the original
graph spectrum, effectively balancing structural bottleneck reduction and graph
property preservation. Experimental results validate the effectiveness of our
approach, demonstrating its superiority over strong baseline methods in
classification accuracy and retention of the Laplacian spectrum.

</details>


### [320] [From Teacher to Student: Tracking Memorization Through Model Distillation](https://arxiv.org/abs/2506.16170)
*Simardeep Singh*

Main category: cs.LG

TL;DR: 研究表明，知识蒸馏（KD）能显著降低大型语言模型（LLMs）对任务数据的记忆风险，同时减少计算成本和模型大小。


<details>
  <summary>Details</summary>
Motivation: 探讨知识蒸馏如何影响模型对任务数据的记忆，以解决隐私和安全问题。

Method: 通过将大型教师模型蒸馏为小型学生模型，研究不同KD方法对记忆的影响。

Result: 蒸馏显著降低了记忆风险，同时减少了计算成本和模型大小。

Conclusion: 知识蒸馏是一种有效的降低记忆风险的方法，同时保持模型效率。

Abstract: Large language models (LLMs) are known to memorize parts of their training
data, raising important concerns around privacy and security. While previous
research has focused on studying memorization in pre-trained models, much less
is known about how knowledge distillation (KD) affects memorization.In this
study, we explore how different KD methods influence the memorization of
fine-tuned task data when a large teacher model is distilled into smaller
student variants.This study demonstrates that distilling a larger teacher
model, fine-tuned on a dataset, into a smaller variant not only lowers
computational costs and model size but also significantly reduces the
memorization risks compared to standard fine-tuning approaches.

</details>


### [321] [Hallucination Level of Artificial Intelligence Whisperer: Case Speech Recognizing Pantterinousut Rap Song](https://arxiv.org/abs/2506.16174)
*Ismo Horppu,Frederick Ayala,Erlin Gulbenkoglu*

Main category: cs.LG

TL;DR: 论文探讨了AI在翻译芬兰说唱歌曲文本中的表现，比较了Faster Whisperer算法和YouTube的语音转文字功能，并以芬兰说唱歌词为参考标准。


<details>
  <summary>Details</summary>
Motivation: 芬兰语复杂且艺术家使用时更难以理解，因此测试AI在翻译芬兰说唱歌曲中的表现具有挑战性和趣味性。

Method: 使用Faster Whisperer算法和YouTube语音转文字功能翻译芬兰说唱歌曲，并与原始歌词对比。

Result: 通过错误函数测量AI的幻觉水平和误听情况。

Conclusion: 实验展示了AI在复杂语言和艺术表达中的翻译挑战。

Abstract: All languages are peculiar. Some of them are considered more challenging to
understand than others. The Finnish Language is known to be a complex language.
Also, when languages are used by artists, the pronunciation and meaning might
be more tricky to understand. Therefore, we are putting AI to a fun, yet
challenging trial: translating a Finnish rap song to text. We will compare the
Faster Whisperer algorithm and YouTube's internal speech-to-text functionality.
The reference truth will be Finnish rap lyrics, which the main author's little
brother, Mc Timo, has written. Transcribing the lyrics will be challenging
because the artist raps over synth music player by Syntikka Janne. The
hallucination level and mishearing of AI speech-to-text extractions will be
measured by comparing errors made against the original Finnish lyrics. The
error function is informal but still works for our case.

</details>


### [322] [Efficient and Privacy-Preserving Soft Prompt Transfer for LLMs](https://arxiv.org/abs/2506.16196)
*Xun Wang,Jing Xu,Franziska Boenisch,Michael Backes,Christopher A. Choquette-Choo,Adam Dziedzic*

Main category: cs.LG

TL;DR: POST框架通过在小模型上私有调优软提示并转移到大型LLM，解决了计算成本和隐私问题。


<details>
  <summary>Details</summary>
Motivation: 软提示与特定LLM紧密耦合，导致计算成本高和隐私泄露风险。

Method: POST利用知识蒸馏从小模型调优软提示，可选差分隐私保护，再通过公共数据集转移回大模型。

Result: 实验表明POST降低了计算成本，保护隐私，并有效转移高实用性软提示。

Conclusion: POST为软提示调优提供了一种高效、隐私安全的解决方案。

Abstract: Prompting has become a dominant paradigm for adapting large language models
(LLMs). While discrete (textual) prompts are widely used for their
interpretability, soft (parameter) prompts have recently gained traction in
APIs. This is because they can encode information from more training samples
while minimizing the user's token usage, leaving more space in the context
window for task-specific input. However, soft prompts are tightly coupled to
the LLM they are tuned on, limiting their generalization to other LLMs. This
constraint is particularly problematic for efficiency and privacy: (1) tuning
prompts on each LLM incurs high computational costs, especially as LLMs
continue to grow in size. Additionally, (2) when the LLM is hosted externally,
soft prompt tuning often requires sharing private data with the LLM provider.
For instance, this is the case with the NVIDIA NeMo API. To address these
issues, we propose POST (Privacy Of Soft prompt Transfer), a framework that
enables private tuning of soft prompts on a small model and subsequently
transfers these prompts to a larger LLM. POST uses knowledge distillation to
derive a small model directly from the large LLM to improve prompt
transferability, tunes the soft prompt locally, optionally with differential
privacy guarantees, and transfers it back to the larger LLM using a small
public dataset. Our experiments show that POST reduces computational costs,
preserves privacy, and effectively transfers high-utility soft prompts.

</details>


### [323] [From Pixels to CSI: Distilling Latent Dynamics For Efficient Wireless Resource Management](https://arxiv.org/abs/2506.16216)
*Charbel Bou Chaaya,Abanoub M. Girgis,Mehdi Bennis*

Main category: cs.LG

TL;DR: 提出一种联合建模控制动态和无线传播环境的机器学习方法，通过耦合JEPA网络和强化学习优化无线电资源管理，减少50%以上传输功率。


<details>
  <summary>Details</summary>
Motivation: 优化远程控制器与设备间的无线电资源管理，同时不影响控制任务性能。

Method: 使用两个耦合的JEPA网络分别建模控制动态和无线传播环境，结合强化学习训练控制策略和功率预测器。

Result: 在合成多模态数据上，传输功率减少50%以上，控制性能与基线方法相当。

Conclusion: 该方法有效优化无线电资源使用，显著降低功耗且不影响控制性能。

Abstract: In this work, we aim to optimize the radio resource management of a
communication system between a remote controller and its device, whose state is
represented through image frames, without compromising the performance of the
control task. We propose a novel machine learning (ML) technique to jointly
model and predict the dynamics of the control system as well as the wireless
propagation environment in latent space. Our method leverages two coupled
joint-embedding predictive architectures (JEPAs): a control JEPA models the
control dynamics and guides the predictions of a wireless JEPA, which captures
the dynamics of the device's channel state information (CSI) through
cross-modal conditioning. We then train a deep reinforcement learning (RL)
algorithm to derive a control policy from latent control dynamics and a power
predictor to estimate scheduling intervals with favorable channel conditions
based on latent CSI representations. As such, the controller minimizes the
usage of radio resources by utilizing the coupled JEPA networks to imagine the
device's trajectory in latent space. We present simulation results on synthetic
multimodal data and show that our proposed approach reduces transmit power by
over 50% while maintaining control performance comparable to baseline methods
that do not account for wireless optimization.

</details>


### [324] [Think Global, Act Local: Bayesian Causal Discovery with Language Models in Sequential Data](https://arxiv.org/abs/2506.16234)
*Prakhar Verma,David Arbour,Sunav Choudhary,Harshita Chopra,Arno Solin,Atanu R. Sinha*

Main category: cs.LG

TL;DR: BLANCE是一个混合贝叶斯框架，通过自适应整合序列批次数据和语言模型（LM）生成的噪声专家知识，填补了因果发现中数据批次到达和专家知识稀缺的缺口。


<details>
  <summary>Details</summary>
Motivation: 解决因果发现中数据批次到达和专家知识稀缺的问题，同时应对LM的幻觉、不一致性和偏见。

Method: 提出BLANCE框架，将DAG转换为PAG以处理模糊性，并采用序列优化方案自适应查询最有信息的边。

Result: BLANCE在结构准确性上优于先前工作，并扩展到贝叶斯参数估计，对LM噪声表现出鲁棒性。

Conclusion: BLANCE通过结合LM知识和观测数据，提供了一种鲁棒的因果发现方法。

Abstract: Causal discovery from observational data typically assumes full access to
data and availability of domain experts. In practice, data often arrive in
batches, and expert knowledge is scarce. Language Models (LMs) offer a
surrogate but come with their own issues-hallucinations, inconsistencies, and
bias. We present BLANCE (Bayesian LM-Augmented Causal Estimation)-a hybrid
Bayesian framework that bridges these gaps by adaptively integrating sequential
batch data with LM-derived noisy, expert knowledge while accounting for both
data-induced and LM-induced biases. Our proposed representation shift from
Directed Acyclic Graph (DAG) to Partial Ancestral Graph (PAG) accommodates
ambiguities within a coherent Bayesian framework, allowing grounding the global
LM knowledge in local observational data. To guide LM interaction, we use a
sequential optimization scheme that adaptively queries the most informative
edges. Across varied datasets, BLANCE outperforms prior work in structural
accuracy and extends to Bayesian parameter estimation, showing robustness to LM
noise.

</details>


### [325] [Active MRI Acquisition with Diffusion Guided Bayesian Experimental Design](https://arxiv.org/abs/2506.16237)
*Jacopo Iollo,Geoffroy Oudoumanessah,Carole Lartizien,Michel Dojat,Florence Forbes*

Main category: cs.LG

TL;DR: 论文提出了一种基于贝叶斯实验设计的自适应MRI采样方法，以在加速采集的同时保持图像质量。


<details>
  <summary>Details</summary>
Motivation: 在临床MRI中，如何在加速采集时间的同时不显著降低图像质量是一个关键挑战。

Method: 采用顺序贝叶斯实验设计（BED），通过梯度优化选择信息量最大的采样点，并结合扩散生成模型处理高维图像。

Result: 该方法在多种MRI采集任务中表现出高效性和多功能性。

Conclusion: 提出的方法不仅能优化标准图像重建，还能适应其他图像分析任务。

Abstract: A key challenge in maximizing the benefits of Magnetic Resonance Imaging
(MRI) in clinical settings is to accelerate acquisition times without
significantly degrading image quality. This objective requires a balance
between under-sampling the raw k-space measurements for faster acquisitions and
gathering sufficient raw information for high-fidelity image reconstruction and
analysis tasks. To achieve this balance, we propose to use sequential Bayesian
experimental design (BED) to provide an adaptive and task-dependent selection
of the most informative measurements. Measurements are sequentially augmented
with new samples selected to maximize information gain on a posterior
distribution over target images. Selection is performed via a gradient-based
optimization of a design parameter that defines a subsampling pattern. In this
work, we introduce a new active BED procedure that leverages diffusion-based
generative models to handle the high dimensionality of the images and employs
stochastic optimization to select among a variety of patterns while meeting the
acquisition process constraints and budget. So doing, we show how our setting
can optimize, not only standard image reconstruction, but also any associated
image analysis task. The versatility and performance of our approach are
demonstrated on several MRI acquisitions.

</details>


### [326] [Synthetic ALS-EEG Data Augmentation for ALS Diagnosis Using Conditional WGAN with Weight Clipping](https://arxiv.org/abs/2506.16243)
*Abdulvahap Mutlu,Şengül Doğan,Türker Tuncer*

Main category: cs.LG

TL;DR: 利用条件Wasserstein生成对抗网络（CWGAN）生成ALS患者的合成EEG信号，以解决数据稀缺和类别不平衡问题。


<details>
  <summary>Details</summary>
Motivation: ALS是一种罕见神经退行性疾病，高质量EEG数据稀缺且类别不平衡，导致机器学习分类器训练困难。

Method: 使用CWGAN在私人EEG数据集上训练，生成合成ALS EEG信号，并进行预处理和归一化。

Result: 生成的合成信号与真实ALS EEG模式相似，训练稳定，可用于增强分类器训练。

Conclusion: 该方法有助于缓解数据稀缺和类别不平衡，提升ALS检测准确性，并促进数据共享。

Abstract: Amyotrophic Lateral Sclerosis (ALS) is a rare neurodegenerative disease, and
high-quality EEG data from ALS patients are scarce. This data scarcity, coupled
with severe class imbalance between ALS and healthy control recordings, poses a
challenge for training reliable machine learning classifiers. In this work, we
address these issues by generating synthetic EEG signals for ALS patients using
a Conditional Wasserstein Generative Adversarial Network (CWGAN). We train
CWGAN on a private EEG dataset (ALS vs. non-ALS) to learn the distribution of
ALS EEG signals and produce realistic synthetic samples. We preprocess and
normalize EEG recordings, and train a CWGAN model to generate synthetic ALS
signals. The CWGAN architecture and training routine are detailed, with key
hyperparameters chosen for stable training. Qualitative evaluation of generated
signals shows that they closely mimic real ALS EEG patterns. The CWGAN training
converged with generator and discriminator loss curves stabilizing, indicating
successful learning. The synthetic EEG signals appear realistic and have
potential use as augmented data for training classifiers, helping to mitigate
class imbalance and improve ALS detection accuracy. We discuss how this
approach can facilitate data sharing and enhance diagnostic models.

</details>


### [327] [Optimal Online Bookmaking for Any Number of Outcomes](https://arxiv.org/abs/2506.16253)
*Hadar Tal,Oron Sabag*

Main category: cs.LG

TL;DR: 研究在线博彩问题，展示庄家如何动态调整赔率以最大化利润并减少潜在损失，最优损失为多项式最大根，提出高效算法计算最优策略。


<details>
  <summary>Details</summary>
Motivation: 探讨庄家在动态调整赔率时的最优策略，以平衡公平性和财务风险。

Method: 通过多项式根和Bellman-Pareto前沿的显式表征，开发高效算法计算最优博彩策略。

Result: 最优损失为多项式最大根，算法在面对最优或次优赌徒时均能实现最优损失。

Conclusion: 庄家可实现公平性并避免财务风险，算法揭示了博彩后悔与Hermite多项式的关系。

Abstract: We study the Online Bookmaking problem, where a bookmaker dynamically updates
betting odds on the possible outcomes of an event. In each betting round, the
bookmaker can adjust the odds based on the cumulative betting behavior of
gamblers, aiming to maximize profit while mitigating potential loss. We show
that for any event and any number of betting rounds, in a worst-case setting
over all possible gamblers and outcome realizations, the bookmaker's optimal
loss is the largest root of a simple polynomial. Our solution shows that
bookmakers can be as fair as desired while avoiding financial risk, and the
explicit characterization reveals an intriguing relation between the
bookmaker's regret and Hermite polynomials. We develop an efficient algorithm
that computes the optimal bookmaking strategy: when facing an optimal gambler,
the algorithm achieves the optimal loss, and in rounds where the gambler is
suboptimal, it reduces the achieved loss to the optimal opportunistic loss, a
notion that is related to subgame perfect Nash equilibrium. The key technical
contribution to achieve these results is an explicit characterization of the
Bellman-Pareto frontier, which unifies the dynamic programming updates for
Bellman's value function with the multi-criteria optimization framework of the
Pareto frontier in the context of vector repeated games.

</details>


### [328] [Next-Token Prediction Should be Ambiguity-Sensitive: A Meta-Learning Perspective](https://arxiv.org/abs/2506.16288)
*Leo Gagnon,Eric Elmoznino,Sarthak Mittal,Tom Marty,Tejas Kasetty,Dhanya Sridhar,Guillaume Lajoie*

Main category: cs.LG

TL;DR: 论文探讨自回归基础模型在快速适应能力上的局限性，提出高模糊性预测的计算需求问题，并引入MetaHMM基准测试和蒙特卡洛预测方法。


<details>
  <summary>Details</summary>
Motivation: 自回归模型的预测能力受限于高模糊性场景下的计算复杂性，认知科学的启发表明需要更优的策略。

Method: 提出MetaHMM合成序列元学习基准，并设计蒙特卡洛预测方法，将任务推断与令牌预测解耦。

Result: 实验显示Transformer在高模糊性预测中表现不佳，但蒙特卡洛方法在模糊场景中显著提升性能。

Conclusion: 通过改进容量分配和推理可扩展性，蒙特卡洛方法在高模糊性预测中具有潜力，但仍需进一步研究。

Abstract: The rapid adaptation ability of auto-regressive foundation models is often
attributed to the diversity of their pre-training data. This is because, from a
Bayesian standpoint, minimizing prediction error in such settings requires
integrating over all plausible latent hypotheses consistent with observations.
While this behavior is desirable in principle, it often proves too ambitious in
practice: under high ambiguity, the number of plausible latent alternatives
makes Bayes-optimal prediction computationally intractable. Cognitive science
has long recognized this limitation, suggesting that under such conditions,
heuristics or information-seeking strategies are preferable to exhaustive
inference. Translating this insight to next-token prediction, we hypothesize
that low- and high-ambiguity predictions pose different computational demands,
making ambiguity-agnostic next-token prediction a detrimental inductive bias.
To test this, we introduce MetaHMM, a synthetic sequence meta-learning
benchmark with rich compositional structure and a tractable Bayesian oracle. We
show that Transformers indeed struggle with high-ambiguity predictions across
model sizes. Motivated by cognitive theories, we propose a method to convert
pre-trained models into Monte Carlo predictors that decouple task inference
from token prediction. Preliminary results show substantial gains in ambiguous
contexts through improved capacity allocation and test-time scalable inference,
though challenges remain.

</details>


### [329] [Improved Exploration in GFlownets via Enhanced Epistemic Neural Networks](https://arxiv.org/abs/2506.16313)
*Sajan Muhammad,Salem Lahlou*

Main category: cs.LG

TL;DR: 论文提出了一种结合ENN和GFlowNets的新算法ENN-GFN-Enhanced，以改进轨迹探索和不确定性量化。


<details>
  <summary>Details</summary>
Motivation: GFlowNets中高效识别训练轨迹的问题尚未解决，需在未充分学习的奖励分布区域优先探索。

Method: 将ENN与GFlowNets结合，提升联合预测能力和不确定性量化。

Result: 在网格环境和结构化序列生成中，ENN-GFN-Enhanced表现优于基线方法。

Conclusion: ENN-GFN-Enhanced能有效提升探索效率和轨迹优化。

Abstract: Efficiently identifying the right trajectories for training remains an open
problem in GFlowNets. To address this, it is essential to prioritize
exploration in regions of the state space where the reward distribution has not
been sufficiently learned. This calls for uncertainty-driven exploration, in
other words, the agent should be aware of what it does not know. This attribute
can be measured by joint predictions, which are particularly important for
combinatorial and sequential decision problems. In this research, we integrate
epistemic neural networks (ENN) with the conventional architecture of GFlowNets
to enable more efficient joint predictions and better uncertainty
quantification, thereby improving exploration and the identification of optimal
trajectories. Our proposed algorithm, ENN-GFN-Enhanced, is compared to the
baseline method in GFlownets and evaluated in grid environments and structured
sequence generation in various settings, demonstrating both its efficacy and
efficiency.

</details>


### [330] [Signatures to help interpretability of anomalies](https://arxiv.org/abs/2506.16314)
*Emmanuel Gangler,Emille E. O. Ishida,Matwey V. Kornilov,Vladimir Korolev,Anastasia Lavrukhina,Konstantin Malanchev,Maria V. Pruzhinskaya,Etienne Russeil,Timofey Semenikhin,Sreevarsha Sreejith,Alina A. Volnova*

Main category: cs.LG

TL;DR: 论文提出了一种称为“异常签名”的方法，旨在通过突出显示影响决策的特征来提高异常检测的可解释性。


<details>
  <summary>Details</summary>
Motivation: 机器学习常被视为黑箱，难以理解其输出（如决策或分数），天文学家需要独立分析数据以理解异常事件的原因。

Method: 引入了“异常签名”的概念，通过突出显示影响异常检测决策的特征来帮助解释异常。

Result: 该方法有助于提高异常检测的可解释性。

Conclusion: “异常签名”是一种有效的方法，能够帮助用户理解异常检测的结果。

Abstract: Machine learning is often viewed as a black box when it comes to
understanding its output, be it a decision or a score. Automatic anomaly
detection is no exception to this rule, and quite often the astronomer is left
to independently analyze the data in order to understand why a given event is
tagged as an anomaly. We introduce here idea of anomaly signature, whose aim is
to help the interpretability of anomalies by highlighting which features
contributed to the decision.

</details>


### [331] [Bayesian Optimization over Bounded Domains with the Beta Product Kernel](https://arxiv.org/abs/2506.16316)
*Huy Hoang Nguyen,Han Zhou,Matthew B. Blaschko,Aleksei Tiulpin*

Main category: cs.LG

TL;DR: 论文提出了一种新的Beta核函数，用于在贝叶斯优化中处理有界域问题，相比传统核函数（如Matérn和RBF）表现更优。


<details>
  <summary>Details</summary>
Motivation: 传统核函数（如Matérn和RBF）未考虑函数定义域的有界性，限制了其适用性。

Method: 引入基于Beta分布密度函数的非平稳Beta核函数，通过其谱特性分析支持其指数特征衰减假设。

Result: 实验表明Beta核在优化问题中表现稳健，尤其在最优解位于超立方体边界或顶点时，优于多种传统核函数。

Conclusion: Beta核为有界域函数优化提供了更有效的工具，在合成函数优化及模型压缩任务中表现优异。

Abstract: Bayesian optimization with Gaussian processes (GP) is commonly used to
optimize black-box functions. The Mat\'ern and the Radial Basis Function (RBF)
covariance functions are used frequently, but they do not make any assumptions
about the domain of the function, which may limit their applicability in
bounded domains. To address the limitation, we introduce the Beta kernel, a
non-stationary kernel induced by a product of Beta distribution density
functions. Such a formulation allows our kernel to naturally model functions on
bounded domains. We present statistical evidence supporting the hypothesis that
the kernel exhibits an exponential eigendecay rate, based on empirical analyses
of its spectral properties across different settings. Our experimental results
demonstrate the robustness of the Beta kernel in modeling functions with optima
located near the faces or vertices of the unit hypercube. The experiments show
that our kernel consistently outperforms a wide range of kernels, including the
well-known Mat\'ern and RBF, in different problems, including synthetic
function optimization and the compression of vision and language models.

</details>


### [332] [Watermarking Autoregressive Image Generation](https://arxiv.org/abs/2506.16349)
*Nikola Jovanović,Ismail Labiad,Tomáš Souček,Martin Vechev,Pierre Fernandez*

Main category: cs.LG

TL;DR: 本文提出了一种在自回归图像生成模型中实现令牌级水印的方法，解决了重新标记化导致水印丢失的问题，并通过实验验证了其可靠性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 生成模型输出的水印技术用于追踪来源，但现有方法未能在自回归图像生成模型中实现令牌级水印。

Method: 通过调整语言模型水印技术，引入自定义的标记化-去标记化微调程序和水印同步层，解决反向循环一致性问题。

Result: 实验表明，该方法能够可靠且鲁棒地检测水印，并提供理论支持的p值。

Conclusion: 本文首次实现了自回归图像生成模型的令牌级水印，解决了重新标记化带来的挑战，为水印技术提供了新思路。

Abstract: Watermarking the outputs of generative models has emerged as a promising
approach for tracking their provenance. Despite significant interest in
autoregressive image generation models and their potential for misuse, no prior
work has attempted to watermark their outputs at the token level. In this work,
we present the first such approach by adapting language model watermarking
techniques to this setting. We identify a key challenge: the lack of reverse
cycle-consistency (RCC), wherein re-tokenizing generated image tokens
significantly alters the token sequence, effectively erasing the watermark. To
address this and to make our method robust to common image transformations,
neural compression, and removal attacks, we introduce (i) a custom
tokenizer-detokenizer finetuning procedure that improves RCC, and (ii) a
complementary watermark synchronization layer. As our experiments demonstrate,
our approach enables reliable and robust watermark detection with theoretically
grounded p-values.

</details>


### [333] [Data-Driven Policy Mapping for Safe RL-based Energy Management Systems](https://arxiv.org/abs/2506.16352)
*Theo Zangato,Aomar Osmani,Pegah Alizadeh*

Main category: cs.LG

TL;DR: 提出了一种基于强化学习的三步建筑能源管理系统（BEMS），结合聚类、预测和约束策略学习，解决可扩展性、适应性和安全性问题。


<details>
  <summary>Details</summary>
Motivation: 全球能源需求增长和可再生能源集成复杂性增加，使建筑成为可持续能源管理的核心。

Method: 1. 聚类非可转移负载模式以识别共同消费模式；2. 集成LSTM预测模块；3. 使用领域知识约束策略学习。

Result: 在真实数据上测试，运行成本降低15%，环境性能稳定，并能快速优化新建筑。

Conclusion: 该框架提供了可扩展、稳健且经济高效的建筑能源管理解决方案。

Abstract: Increasing global energy demand and renewable integration complexity have
placed buildings at the center of sustainable energy management. We present a
three-step reinforcement learning(RL)-based Building Energy Management System
(BEMS) that combines clustering, forecasting, and constrained policy learning
to address scalability, adaptability, and safety challenges. First, we cluster
non-shiftable load profiles to identify common consumption patterns, enabling
policy generalization and transfer without retraining for each new building.
Next, we integrate an LSTM based forecasting module to anticipate future
states, improving the RL agents' responsiveness to dynamic conditions. Lastly,
domain-informed action masking ensures safe exploration and operation,
preventing harmful decisions. Evaluated on real-world data, our approach
reduces operating costs by up to 15% for certain building types, maintains
stable environmental performance, and quickly classifies and optimizes new
buildings with limited data. It also adapts to stochastic tariff changes
without retraining. Overall, this framework delivers scalable, robust, and
cost-effective building energy management.

</details>


### [334] [Classification of Cattle Behavior and Detection of Heat (Estrus) using Sensor Data](https://arxiv.org/abs/2506.16380)
*Druva Dhakshinamoorthy,Avikshit Jha,Sabyasachi Majumdar,Devdulal Ghosh,Ranjita Chakraborty,Hena Ray*

Main category: cs.LG

TL;DR: 论文提出了一种基于传感器数据和机器学习的牛行为监测与发情期检测系统，通过低成本蓝牙颈圈和多种机器学习模型实现了高准确率。


<details>
  <summary>Details</summary>
Motivation: 旨在为精准畜牧业提供一种低成本、可扩展的解决方案，特别是在资源有限的环境中。

Method: 设计并部署了配备加速度计和陀螺仪的低成本蓝牙颈圈，采集实时数据；使用同步CCTV标注行为数据；评估了SVM、RF、CNN等模型的行为分类能力，并采用LSTM模型进行发情检测。

Result: 行为分类准确率超过93%，发情检测准确率达96%。

Conclusion: 该系统为精准畜牧业提供了一种高效且可扩展的解决方案，适用于资源受限环境。

Abstract: This paper presents a novel system for monitoring cattle behavior and
detecting estrus (heat) periods using sensor data and machine learning. We
designed and deployed a low-cost Bluetooth-based neck collar equipped with
accelerometer and gyroscope sensors to capture real-time behavioral data from
real cows, which was synced to the cloud. A labeled dataset was created using
synchronized CCTV footage to annotate behaviors such as feeding, rumination,
lying, and others. We evaluated multiple machine learning models -- Support
Vector Machines (SVM), Random Forests (RF), and Convolutional Neural Networks
(CNN) -- for behavior classification. Additionally, we implemented a Long
Short-Term Memory (LSTM) model for estrus detection using behavioral patterns
and anomaly detection. Our system achieved over 93% behavior classification
accuracy and 96% estrus detection accuracy on a limited test set. The approach
offers a scalable and accessible solution for precision livestock monitoring,
especially in resource-constrained environments.

</details>


### [335] [State-Space Kolmogorov Arnold Networks for Interpretable Nonlinear System Identification](https://arxiv.org/abs/2506.16392)
*Gonçalo Granjal Cruz,Balazs Renczes,Mark C Runacres,Jan Decuyper*

Main category: cs.LG

TL;DR: 论文提出SS-KAN模型，通过结合Kolmogorov-Arnold网络与状态空间框架，提升非线性系统辨识的可解释性，但牺牲了部分准确性。


<details>
  <summary>Details</summary>
Motivation: 黑箱系统辨识模型缺乏对系统动态的可解释性，需要一种平衡准确性和可解释性的方法。

Method: 提出SS-KAN模型，利用稀疏正则化和单变量函数可视化，增强可解释性。

Result: 在Silverbox和Wiener-Hammerstein基准测试中，SS-KAN展示了更高的可解释性，但准确性略低于黑箱模型。

Conclusion: SS-KAN是一种有前景的折中方案，适用于需要可解释性的非线性系统辨识。

Abstract: While accurate, black-box system identification models lack interpretability
of the underlying system dynamics. This paper proposes State-Space
Kolmogorov-Arnold Networks (SS-KAN) to address this challenge by integrating
Kolmogorov-Arnold Networks within a state-space framework. The proposed model
is validated on two benchmark systems: the Silverbox and the Wiener-Hammerstein
benchmarks. Results show that SS-KAN provides enhanced interpretability due to
sparsity-promoting regularization and the direct visualization of its learned
univariate functions, which reveal system nonlinearities at the cost of
accuracy when compared to state-of-the-art black-box models, highlighting
SS-KAN as a promising approach for interpretable nonlinear system
identification, balancing accuracy and interpretability of nonlinear system
dynamics.

</details>


### [336] [GoalLadder: Incremental Goal Discovery with Vision-Language Models](https://arxiv.org/abs/2506.16396)
*Alexey Zakharov,Shimon Whiteson*

Main category: cs.LG

TL;DR: GoalLadder利用视觉语言模型（VLM）从单条语言指令中训练RL代理，通过逐步发现任务进展状态并减少噪声反馈的影响，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 自然语言可以简洁地指定RL任务，但现有方法依赖非视觉表示或需要大量反馈，且生成的奖励函数噪声大。

Method: GoalLadder通过查询VLM识别任务进展状态，并使用ELO评级系统排序目标状态，减少噪声反馈的影响。

Result: 在经典控制和机器人操作环境中，GoalLadder平均最终成功率约95%，远优于竞争对手的45%。

Conclusion: GoalLadder通过减少噪声反馈和利用无标签视觉数据，显著提升了RL代理的性能。

Abstract: Natural language can offer a concise and human-interpretable means of
specifying reinforcement learning (RL) tasks. The ability to extract rewards
from a language instruction can enable the development of robotic systems that
can learn from human guidance; however, it remains a challenging problem,
especially in visual environments. Existing approaches that employ large,
pretrained language models either rely on non-visual environment
representations, require prohibitively large amounts of feedback, or generate
noisy, ill-shaped reward functions. In this paper, we propose a novel method,
$\textbf{GoalLadder}$, that leverages vision-language models (VLMs) to train RL
agents from a single language instruction in visual environments. GoalLadder
works by incrementally discovering states that bring the agent closer to
completing a task specified in natural language. To do so, it queries a VLM to
identify states that represent an improvement in agent's task progress and to
rank them using pairwise comparisons. Unlike prior work, GoalLadder does not
trust VLM's feedback completely; instead, it uses it to rank potential goal
states using an ELO-based rating system, thus reducing the detrimental effects
of noisy VLM feedback. Over the course of training, the agent is tasked with
minimising the distance to the top-ranked goal in a learned embedding space,
which is trained on unlabelled visual data. This key feature allows us to
bypass the need for abundant and accurate feedback typically required to train
a well-shaped reward function. We demonstrate that GoalLadder outperforms
existing related methods on classic control and robotic manipulation
environments with the average final success rate of $\sim$95% compared to only
$\sim$45% of the best competitor.

</details>


### [337] [Generating Directed Graphs with Dual Attention and Asymmetric Encoding](https://arxiv.org/abs/2506.16404)
*Alba Carballo-Castro,Manuel Madeira,Yiming Qin,Dorina Thanou,Pascal Frossard*

Main category: cs.LG

TL;DR: 论文提出了一种名为Directo的生成模型，专注于有向图生成，解决了现有方法在建模方向性和缺乏标准化基准的问题。


<details>
  <summary>Details</summary>
Motivation: 有向图在多个领域（如生物学、交通、社交网络）中具有重要应用，但现有生成方法未能充分探索方向性建模，且缺乏统一评估标准。

Method: Directo基于离散流匹配框架，结合了针对不对称关系的编码、双注意力机制（捕捉入度和出度依赖）和离散生成框架。

Result: 实验表明，Directo在多种数据集上表现优异，甚至能与特定类别的专用模型（如有向无环图）竞争。

Conclusion: Directo为有向图生成提供了有效且通用的方法，为未来研究奠定了基础。

Abstract: Directed graphs naturally model systems with asymmetric, ordered
relationships, essential to applications in biology, transportation, social
networks, and visual understanding. Generating such graphs enables tasks such
as simulation, data augmentation and novel instance discovery; however,
directed graph generation remains underexplored. We identify two key factors
limiting progress in this direction: first, modeling edge directionality
introduces a substantially larger dependency space, making the underlying
distribution harder to learn; second, the absence of standardized benchmarks
hinders rigorous evaluation. Addressing the former requires more expressive
models that are sensitive to directional topologies. We propose Directo, the
first generative model for directed graphs built upon the discrete flow
matching framework. Our approach combines: (i) principled positional encodings
tailored to asymmetric pairwise relations, (ii) a dual-attention mechanism
capturing both incoming and outgoing dependencies, and (iii) a robust, discrete
generative framework. To support evaluation, we introduce a benchmark suite
covering synthetic and real-world datasets. It shows that our method performs
strongly across diverse settings and even competes with specialized models for
particular classes, such as directed acyclic graphs. Our results highlight the
effectiveness and generality of our approach, establishing a solid foundation
for future research in directed graph generation.

</details>


### [338] [Drag-and-Drop LLMs: Zero-Shot Prompt-to-Weights](https://arxiv.org/abs/2506.16406)
*Zhiyuan Liang,Dongwen Tang,Yuhao Zhou,Xuanlei Zhao,Mingjia Shi,Wangbo Zhao,Zekai Li,Peihao Wang,Konstantin Schürholt,Damian Borth,Michael M. Bronstein,Yang You,Zhangyang Wang,Kai Wang*

Main category: cs.LG

TL;DR: DnD是一种无需逐任务训练的PEFT方法，通过提示生成LoRA权重更新，显著降低计算开销并提升性能。


<details>
  <summary>Details</summary>
Motivation: 减少为每个下游数据集单独优化LLM的成本，提高效率。

Method: 使用提示条件参数生成器，通过轻量级文本编码器和超卷积解码器生成LoRA矩阵。

Result: 计算开销降低12,000倍，性能提升30%，并具有跨领域泛化能力。

Conclusion: 提示条件参数生成是梯度适应之外的有效替代方案。

Abstract: Modern Parameter-Efficient Fine-Tuning (PEFT) methods such as low-rank
adaptation (LoRA) reduce the cost of customizing large language models (LLMs),
yet still require a separate optimization run for every downstream dataset. We
introduce \textbf{Drag-and-Drop LLMs (\textit{DnD})}, a prompt-conditioned
parameter generator that eliminates per-task training by mapping a handful of
unlabeled task prompts directly to LoRA weight updates. A lightweight text
encoder distills each prompt batch into condition embeddings, which are then
transformed by a cascaded hyper-convolutional decoder into the full set of LoRA
matrices. Once trained in a diverse collection of prompt-checkpoint pairs, DnD
produces task-specific parameters in seconds, yielding i) up to
\textbf{12,000$\times$} lower overhead than full fine-tuning, ii) average gains
up to \textbf{30\%} in performance over the strongest training LoRAs on unseen
common-sense reasoning, math, coding, and multimodal benchmarks, and iii)
robust cross-domain generalization despite never seeing the target data or
labels. Our results demonstrate that prompt-conditioned parameter generation is
a viable alternative to gradient-based adaptation for rapidly specializing
LLMs. Our project is available at
\href{https://jerryliang24.github.io/DnD}{https://jerryliang24.github.io/DnD}.

</details>


### [339] [Optimizing MoE Routers: Design, Implementation, and Evaluation in Transformer Models](https://arxiv.org/abs/2506.16419)
*Daniel Fidel Harvey,George Weale,Berk Yilmaz*

Main category: cs.LG

TL;DR: 论文研究了MoE架构中的路由器设计，比较了六种路由器变体，发现不同路由器在速度、表达能力和稀疏路由方面各有优劣，最终在Qwen1.5-MoE模型中成功应用并优化了路由器。


<details>
  <summary>Details</summary>
Motivation: MoE架构的性能依赖于路由器模块，但不良路由会导致负载不均衡和精度下降，因此需要设计和实现更好的路由器架构。

Method: 设计了六种路由器变体（Linear、Attention、MLP、Hybrid、Hash和新的MLP-Hadamard），并在BERT和Qwen1.5-MoE模型上评估其参数效率、推理延迟、路由熵和专家利用率。

Result: 不同路由器表现出不同的权衡：Linear路由器速度快，MLP和Attention路由器表达能力更强，MLP-Hadamard路由器在结构化稀疏路由方面表现独特。

Conclusion: 研究为MoE路由器设计提供了比较分析，并为大规模模型部署中的路由器性能优化提供了见解。

Abstract: Mixture of Experts (MoE) architectures increase large language model
scalability, yet their performance depends on the router module that moves
tokens to specialized experts. Bad routing can load imbalance and reduced
accuracy. This project designed and implemented different router architectures
within Transformer models to fix these limitations. We experimented with six
distinct router variants Linear, Attention, Multi-Layer Perceptron (MLP),
Hybrid, Hash, and our new MLP-Hadamard. We characterized these routers using
BERT and the Qwen1.5-MoE model, looking at parameter efficiency, inference
latency, routing entropy, and expert utilization patterns. Our evaluations
showed distinct trade-offs: Linear routers offer speed, while MLP and Attention
routers provide greater expressiveness. The MLP-Hadamard router shows a unique
capability for structured, sparse routing. We successfully replaced and
fine-tuned custom routers within the complex, quantized Qwen1.5-MoE model. This
work provides a comparative analysis of MoE router designs and offers insights
into optimizing their performance for efficient and effective large-scale model
deployment.

</details>


### [340] [EFormer: An Effective Edge-based Transformer for Vehicle Routing Problems](https://arxiv.org/abs/2506.16428)
*Dian Meng,Zhiguang Cao,Yaoxin Wu,Yaqing Hou,Hongwei Ge,Qiang Zhang*

Main category: cs.LG

TL;DR: EFormer是一种基于边的Transformer模型，用于解决车辆路径问题（VRP），通过边输入和并行编码策略，显著优于现有基线。


<details>
  <summary>Details</summary>
Motivation: 现有神经启发式方法主要依赖节点坐标输入，而在实际场景中，基于边的距离等成本指标更为相关，因此需要改进。

Method: EFormer采用边作为唯一输入，通过预编码模块和并行编码策略（图编码器和节点编码器）处理边信息，解码阶段使用并行上下文嵌入和多查询集成。

Result: 在TSP和CVRP上，EFormer在合成数据集和真实数据集（如TSPLib和CVRPLib）上均表现优异，验证了其设计的有效性。

Conclusion: EFormer的核心设计在解决VRP问题上具有显著优势，尤其是在处理边信息时表现突出。

Abstract: Recent neural heuristics for the Vehicle Routing Problem (VRP) primarily rely
on node coordinates as input, which may be less effective in practical
scenarios where real cost metrics-such as edge-based distances-are more
relevant. To address this limitation, we introduce EFormer, an Edge-based
Transformer model that uses edge as the sole input for VRPs. Our approach
employs a precoder module with a mixed-score attention mechanism to convert
edge information into temporary node embeddings. We also present a parallel
encoding strategy characterized by a graph encoder and a node encoder, each
responsible for processing graph and node embeddings in distinct feature
spaces, respectively. This design yields a more comprehensive representation of
the global relationships among edges. In the decoding phase, parallel context
embedding and multi-query integration are used to compute separate attention
mechanisms over the two encoded embeddings, facilitating efficient path
construction. We train EFormer using reinforcement learning in an
autoregressive manner. Extensive experiments on the Traveling Salesman Problem
(TSP) and Capacitated Vehicle Routing Problem (CVRP) reveal that EFormer
outperforms established baselines on synthetic datasets, including large-scale
and diverse distributions. Moreover, EFormer demonstrates strong generalization
on real-world instances from TSPLib and CVRPLib. These findings confirm the
effectiveness of EFormer's core design in solving VRPs.

</details>


### [341] [An efficient neuromorphic approach for collision avoidance combining Stack-CNN with event cameras](https://arxiv.org/abs/2506.16436)
*Antonio Giulio Coretti,Mattia Varile,Mario Edoardo Bertaina*

Main category: cs.LG

TL;DR: 论文提出了一种基于事件相机的碰撞避免系统，用于空间碎片监测，采用Stack-CNN算法提升信噪比。


<details>
  <summary>Details</summary>
Motivation: 空间碎片对航天器构成严重威胁，需要开发有效的监测和避免技术。

Method: 利用事件相机和Stack-CNN算法实时分析数据，检测微弱移动物体。

Result: 地面测试显示算法能显著提升信噪比，适用于空间成像和SSA/STM。

Conclusion: 该系统为空间交通管理和态势感知提供了有前景的解决方案。

Abstract: Space debris poses a significant threat, driving research into active and
passive mitigation strategies. This work presents an innovative collision
avoidance system utilizing event-based cameras - a novel imaging technology
well-suited for Space Situational Awareness (SSA) and Space Traffic Management
(STM). The system, employing a Stack-CNN algorithm (previously used for meteor
detection), analyzes real-time event-based camera data to detect faint moving
objects. Testing on terrestrial data demonstrates the algorithm's ability to
enhance signal-to-noise ratio, offering a promising approach for on-board space
imaging and improving STM/SSA operations.

</details>


### [342] [Leveraging Influence Functions for Resampling Data in Physics-Informed Neural Networks](https://arxiv.org/abs/2506.16443)
*Jonas R. Naujoks,Aleksander Krasowski,Moritz Weckbecker,Galip Ümit Yolcu,Thomas Wiegand,Sebastian Lapuschkin,Wojciech Samek,René P. Klausen*

Main category: cs.LG

TL;DR: PINNs结合XAI中的影响函数进行数据采样，提升训练效果。


<details>
  <summary>Details</summary>
Motivation: 探索如何利用XAI中的影响函数改进PINNs的数据采样，以提高预测精度。

Method: 采用影响函数对训练数据进行针对性重采样。

Result: 基于数据归因方法的采样策略能提升PINNs的预测准确性。

Conclusion: 影响函数在PINN训练中具有实际应用价值。

Abstract: Physics-informed neural networks (PINNs) offer a powerful approach to solving
partial differential equations (PDEs), which are ubiquitous in the quantitative
sciences. Applied to both forward and inverse problems across various
scientific domains, PINNs have recently emerged as a valuable tool in the field
of scientific machine learning. A key aspect of their training is that the data
-- spatio-temporal points sampled from the PDE's input domain -- are readily
available. Influence functions, a tool from the field of explainable AI (XAI),
approximate the effect of individual training points on the model, enhancing
interpretability. In the present work, we explore the application of influence
function-based sampling approaches for the training data. Our results indicate
that such targeted resampling based on data attribution methods has the
potential to enhance prediction accuracy in physics-informed neural networks,
demonstrating a practical application of an XAI method in PINN training.

</details>


### [343] [Consumer-friendly EEG-based Emotion Recognition System: A Multi-scale Convolutional Neural Network Approach](https://arxiv.org/abs/2506.16448)
*Tri Duc Ly,Gia H. Ngo*

Main category: cs.LG

TL;DR: 提出了一种基于多尺度卷积神经网络的EEG情绪识别方法，优于现有技术。


<details>
  <summary>Details</summary>
Motivation: 开发一种适用于真实场景的深度学习模型，用于EEG情绪识别。

Method: 使用多尺度卷积神经网络，结合多种比例系数的特征提取核和新型核（从大脑四个区域学习关键信息）。

Result: 模型在预测效价、唤醒度和支配度方面优于TSception模型。

Conclusion: 多尺度卷积神经网络在EEG情绪识别中表现优异。

Abstract: EEG is a non-invasive, safe, and low-risk method to record
electrophysiological signals inside the brain. Especially with recent
technology developments like dry electrodes, consumer-grade EEG devices, and
rapid advances in machine learning, EEG is commonly used as a resource for
automatic emotion recognition. With the aim to develop a deep learning model
that can perform EEG-based emotion recognition in a real-life context, we
propose a novel approach to utilize multi-scale convolutional neural networks
to accomplish such tasks. By implementing feature extraction kernels with many
ratio coefficients as well as a new type of kernel that learns key information
from four separate areas of the brain, our model consistently outperforms the
state-of-the-art TSception model in predicting valence, arousal, and dominance
scores across many performance evaluation metrics.

</details>


### [344] [Joint Tensor-Train Parameterization for Efficient and Expressive Low-Rank Adaptation](https://arxiv.org/abs/2506.16456)
*Jun Qi,Chen-Yu Liu,Sabato Marco Siniscalchi,Chao-Han Huck Yang,Min-Hsiu Hsieh*

Main category: cs.LG

TL;DR: TensorGuide提出了一种新的张量训练引导的适应框架，通过联合TT结构生成两个相关的低秩LoRA矩阵，显著提升了表达性、泛化能力和参数效率。


<details>
  <summary>Details</summary>
Motivation: 标准LoRA独立优化低秩矩阵，限制了其表达性和泛化能力，而传统的TT分解方法未能显著改进参数效率或性能。

Method: TensorGuide通过统一的TT结构生成两个相关的低秩LoRA矩阵，利用受控高斯噪声驱动，提供结构化低秩适应。

Result: 实验表明，TensorGuide在量子点分类和GPT-2微调任务中优于标准LoRA和TT-LoRA，实现了更高的准确性和可扩展性。

Conclusion: TensorGuide通过联合TT表示显著提升了LoRA的性能，且未增加可训练参数数量。

Abstract: Low-Rank Adaptation (LoRA) is widely recognized for its parameter-efficient
fine-tuning of large-scale neural models. However, standard LoRA independently
optimizes low-rank matrices, which inherently limits its expressivity and
generalization capabilities. While classical tensor-train (TT) decomposition
can be separately employed on individual LoRA matrices, this work demonstrates
that the classical TT-based approach neither significantly improves parameter
efficiency nor achieves substantial performance gains. This paper proposes
TensorGuide, a novel tensor-train-guided adaptation framework to overcome these
limitations. TensorGuide generates two correlated low-rank LoRA matrices
through a unified TT structure driven by controlled Gaussian noise. The
resulting joint TT representation inherently provides structured, low-rank
adaptations, significantly enhancing expressivity, generalization, and
parameter efficiency without increasing the number of trainable parameters.
Theoretically, we justify these improvements through neural tangent kernel
analyses, demonstrating superior optimization dynamics and enhanced
generalization. Extensive experiments on quantum dot classification and GPT-2
fine-tuning benchmarks demonstrate that TensorGuide-based LoRA consistently
outperforms standard LoRA and TT-LoRA, achieving improved accuracy and
scalability with fewer parameters.

</details>


### [345] [Latent Concept Disentanglement in Transformer-based Language Models](https://arxiv.org/abs/2506.16975)
*Guan Zhe Hong,Bhavya Vasudeva,Vatsal Sharan,Cyrus Rashtchian,Prabhakar Raghavan,Rina Panigrahy*

Main category: cs.LG

TL;DR: 论文探讨了大型语言模型（LLMs）在上下文学习（ICL）中是否能够理解和表示潜在概念，并通过实验验证了模型在离散和连续潜在概念任务中的表现。


<details>
  <summary>Details</summary>
Motivation: 研究动机是探究LLMs在ICL中是否真正理解潜在概念，还是仅通过捷径解决问题。

Method: 方法包括分析模型在2跳推理任务（离散潜在概念）和连续潜在概念任务中的表现，观察其表示空间的几何结构。

Result: 结果显示模型能成功识别离散潜在概念并逐步组合，同时在连续潜在概念任务中发现了低维表示空间。

Conclusion: 结论表明ICL中存在高度局部化的结构，能够解耦潜在概念，深化了对LLMs表示能力的理解。

Abstract: When large language models (LLMs) use in-context learning (ICL) to solve a
new task, they seem to grasp not only the goal of the task but also core,
latent concepts in the demonstration examples. This begs the question of
whether transformers represent latent structures as part of their computation
or whether they take shortcuts to solve the problem. Prior mechanistic work on
ICL does not address this question because it does not sufficiently examine the
relationship between the learned representation and the latent concept, and the
considered problem settings often involve only single-step reasoning. In this
work, we examine how transformers disentangle and use latent concepts. We show
that in 2-hop reasoning tasks with a latent, discrete concept, the model
successfully identifies the latent concept and does step-by-step concept
composition. In tasks parameterized by a continuous latent concept, we find
low-dimensional subspaces in the representation space where the geometry mimics
the underlying parameterization. Together, these results refine our
understanding of ICL and the representation of transformers, and they provide
evidence for highly localized structures in the model that disentangle latent
concepts in ICL tasks.

</details>


### [346] [Black-Box Privacy Attacks on Shared Representations in Multitask Learning](https://arxiv.org/abs/2506.16460)
*John Abascal,Nicolás Berrios,Alina Oprea,Jonathan Ullman,Adam Smith,Matthew Jagielski*

Main category: cs.LG

TL;DR: 该论文研究了多任务学习（MTL）中共享表示可能泄露敏感信息的问题，提出了一种黑盒任务推断攻击模型，并通过实验和理论分析验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 多任务学习通过共享表示提高任务性能，但共享表示可能泄露任务训练信息，作者旨在探索这种信息泄露的可能性及其影响。

Method: 提出了一种黑盒任务推断攻击模型，利用任务样本的嵌入向量推断任务是否参与训练，无需影子模型或标记数据。

Result: 实验证明，攻击者在仅访问任务样本的情况下，能成功推断任务是否参与训练，且理论分析支持这一结论。

Conclusion: 共享表示在多任务学习中存在信息泄露风险，需进一步研究防御措施以保护隐私。

Abstract: Multitask learning (MTL) has emerged as a powerful paradigm that leverages
similarities among multiple learning tasks, each with insufficient samples to
train a standalone model, to solve them simultaneously while minimizing data
sharing across users and organizations. MTL typically accomplishes this goal by
learning a shared representation that captures common structure among the tasks
by embedding data from all tasks into a common feature space. Despite being
designed to be the smallest unit of shared information necessary to effectively
learn patterns across multiple tasks, these shared representations can
inadvertently leak sensitive information about the particular tasks they were
trained on.
  In this work, we investigate what information is revealed by the shared
representations through the lens of inference attacks. Towards this, we propose
a novel, black-box task-inference threat model where the adversary, given the
embedding vectors produced by querying the shared representation on samples
from a particular task, aims to determine whether that task was present when
training the shared representation. We develop efficient, purely black-box
attacks on machine learning models that exploit the dependencies between
embeddings from the same task without requiring shadow models or labeled
reference data. We evaluate our attacks across vision and language domains for
multiple use cases of MTL and demonstrate that even with access only to fresh
task samples rather than training data, a black-box adversary can successfully
infer a task's inclusion in training. To complement our experiments, we provide
theoretical analysis of a simplified learning setting and show a strict
separation between adversaries with training samples and fresh samples from the
target task's distribution.

</details>


### [347] [From Concepts to Components: Concept-Agnostic Attention Module Discovery in Transformers](https://arxiv.org/abs/2506.17052)
*Jingtong Su,Julia Kempe,Karen Ullrich*

Main category: cs.LG

TL;DR: 论文提出SAMD和SAMI方法，用于解释和干预Transformer模型的注意力机制，支持复杂概念分析，并在多个任务中验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 提升Transformer模型的解释性和行为控制能力，填补现有方法在注意力机制和复杂概念分析上的不足。

Method: 提出SAMD方法，将复杂概念映射到注意力头；SAMI方法通过标量参数调整注意力模块效果。

Result: SAMD能稳定定位概念模块；SAMI在安全性和推理任务中显著提升或抑制模型表现。

Conclusion: SAMD和SAMI为Transformer解释和干预提供了通用方法，适用于语言和视觉任务。

Abstract: Transformers have achieved state-of-the-art performance across language and
vision tasks. This success drives the imperative to interpret their internal
mechanisms with the dual goals of enhancing performance and improving
behavioral control. Attribution methods help advance interpretability by
assigning model outputs associated with a target concept to specific model
components. Current attribution research primarily studies multi-layer
perceptron neurons and addresses relatively simple concepts such as factual
associations (e.g., Paris is located in France). This focus tends to overlook
the impact of the attention mechanism and lacks a unified approach for
analyzing more complex concepts. To fill these gaps, we introduce Scalable
Attention Module Discovery (SAMD), a concept-agnostic method for mapping
arbitrary, complex concepts to specific attention heads of general transformer
models. We accomplish this by representing each concept as a vector,
calculating its cosine similarity with each attention head, and selecting the
TopK-scoring heads to construct the concept-associated attention module. We
then propose Scalar Attention Module Intervention (SAMI), a simple strategy to
diminish or amplify the effects of a concept by adjusting the attention module
using only a single scalar parameter. Empirically, we demonstrate SAMD on
concepts of varying complexity, and visualize the locations of their
corresponding modules. Our results demonstrate that module locations remain
stable before and after LLM post-training, and confirm prior work on the
mechanics of LLM multilingualism. Through SAMI, we facilitate jailbreaking on
HarmBench (+72.7%) by diminishing "safety" and improve performance on the GSM8K
benchmark (+1.6%) by amplifying "reasoning". Lastly, we highlight the
domain-agnostic nature of our approach by suppressing the image classification
accuracy of vision transformers on ImageNet.

</details>


### [348] [Progressive Inference-Time Annealing of Diffusion Models for Sampling from Boltzmann Densities](https://arxiv.org/abs/2506.16471)
*Tara Akhound-Sadegh,Jungyoon Lee,Avishek Joey Bose,Valentin De Bortoli,Arnaud Doucet,Michael M. Bronstein,Dominique Beaini,Siamak Ravanbakhsh,Kirill Neklyudov,Alexander Tong*

Main category: cs.LG

TL;DR: 提出了一种名为PITA的新框架，结合退火和扩散平滑技术，首次实现了N体粒子系统、Alanine Dipeptide和三肽的平衡采样。


<details>
  <summary>Details</summary>
Motivation: 高效采样未归一化概率密度是核心挑战，现有扩散采样器无法处理简单分子系统。

Method: 结合Boltzmann分布的退火和扩散平滑，训练从高到低温度的扩散模型序列，利用Feynman-Kac PDE和序贯蒙特卡洛进行推理时间退火。

Result: 首次实现N体粒子系统、Alanine Dipeptide和三肽的平衡采样，显著减少能量函数评估次数。

Conclusion: PITA框架为扩散采样器提供了新思路，解决了分子系统采样难题。

Abstract: Sampling efficiently from a target unnormalized probability density remains a
core challenge, with relevance across countless high-impact scientific
applications. A promising approach towards this challenge is the design of
amortized samplers that borrow key ideas, such as probability path design, from
state-of-the-art generative diffusion models. However, all existing
diffusion-based samplers remain unable to draw samples from distributions at
the scale of even simple molecular systems. In this paper, we propose
Progressive Inference-Time Annealing (PITA), a novel framework to learn
diffusion-based samplers that combines two complementary interpolation
techniques: I.) Annealing of the Boltzmann distribution and II.) Diffusion
smoothing. PITA trains a sequence of diffusion models from high to low
temperatures by sequentially training each model at progressively higher
temperatures, leveraging engineered easy access to samples of the
temperature-annealed target density. In the subsequent step, PITA enables
simulating the trained diffusion model to procure training samples at a lower
temperature for the next diffusion model through inference-time annealing using
a novel Feynman-Kac PDE combined with Sequential Monte Carlo. Empirically, PITA
enables, for the first time, equilibrium sampling of N-body particle systems,
Alanine Dipeptide, and tripeptides in Cartesian coordinates with dramatically
lower energy function evaluations. Code available at:
https://github.com/taraak/pita

</details>


### [349] [Manifold Learning for Personalized and Label-Free Detection of Cardiac Arrhythmias](https://arxiv.org/abs/2506.16494)
*Amir Reza Vazifeh,Jason W. Fleischer*

Main category: cs.LG

TL;DR: 非线性降维（NLDR）方法在无需训练或先验信息的情况下，能有效识别心电图（ECG）中的医学相关特征，并在心律失常分类中表现出高准确性。


<details>
  <summary>Details</summary>
Motivation: 手动ECG分析耗时且易出错，传统机器学习方法因信号多样性和数据偏差难以泛化，而无监督方法常忽略临床相关的小变化。

Method: 采用非线性降维技术（如t-SNE和UMAP）分析MIT-BIH数据集中的MLII和V1导联信号。

Result: 在混合人群中区分个体记录的准确率≥90%，对心律失常分类的中位准确率为98.96%，中位F1分数为91.02%。

Conclusion: NLDR在心脏监测和个性化医疗中具有广阔应用前景，适用于单导联和12导联ECG。

Abstract: Electrocardiograms (ECGs) provide direct, non-invasive measurements of heart
activity and are well-established tools for detecting and monitoring
cardiovascular disease. However, manual ECG analysis can be time-consuming and
prone to errors. Machine learning has emerged as a promising approach for
automated heartbeat recognition and classification, but substantial variations
in ECG signals make it challenging to develop generalizable models. ECG signals
can vary widely across individuals and leads, while datasets often follow
different labeling standards and may be biased, all of which greatly hinder
supervised methods. Conventional unsupervised methods, e.g. principal component
analysis, prioritize large (and often obvious) variances in the data and
typically overlook subtle yet clinically relevant patterns. If labels are
missing and/or variations are significant but small, both approaches fail.
Here, we show that nonlinear dimensionality reduction (NLDR) can accommodate
these issues and identify medically relevant features in ECG signals, with no
need for training or prior information. Using the MLII and V1 leads of the
MIT-BIH dataset, we demonstrate that t-distributed stochastic neighbor
embedding and uniform manifold approximation and projection can discriminate
individual recordings in mixed populations with >= 90% accuracy and distinguish
different arrhythmias in individual patients with a median accuracy of 98.96%
and a median F1-score of 91.02%. The results show that NLDR holds much promise
for cardiac monitoring, including the limiting cases of single-lead ECG and the
current 12-lead standard of care, and for personalized health care beyond
cardiology.

</details>


### [350] [SparseLoRA: Accelerating LLM Fine-Tuning with Contextual Sparsity](https://arxiv.org/abs/2506.16500)
*Samir Khaki,Xiuyu Li,Junxian Guo,Ligeng Zhu,Chenfeng Xu,Konstantinos N. Plataniotis,Amir Yazdanbakhsh,Kurt Keutzer,Song Han,Zhijian Liu*

Main category: cs.LG

TL;DR: SparseLoRA是一种通过上下文稀疏性加速LLM微调的方法，减少计算成本并保持准确性。


<details>
  <summary>Details</summary>
Motivation: 现有参数高效微调方法（如QLoRA和DoRA）虽减少可训练参数和内存使用，但未降低计算成本，甚至可能减慢微调速度。

Method: 提出轻量级、无需训练的SVD稀疏性估计器，动态选择稀疏权重子集进行损失和梯度计算，并系统分析层、标记和训练步骤的敏感性。

Result: 实验显示，SparseLoRA将计算成本降低至2.2倍，实测加速达1.6倍，同时在多种下游任务中保持准确性。

Conclusion: SparseLoRA有效加速LLM微调，降低计算成本，且不影响模型性能。

Abstract: Fine-tuning LLMs is both computationally and memory-intensive. While
parameter-efficient fine-tuning methods, such as QLoRA and DoRA, reduce the
number of trainable parameters and lower memory usage, they do not decrease
computational cost. In some cases, they may even slow down fine-tuning. In this
paper, we introduce SparseLoRA, a method that accelerates LLM fine-tuning
through contextual sparsity. We propose a lightweight, training-free SVD
sparsity estimator that dynamically selects a sparse subset of weights for loss
and gradient computation. Also, we systematically analyze and address
sensitivity across layers, tokens, and training steps. Our experimental results
show that SparseLoRA reduces computational cost by up to 2.2 times and a
measured speedup of up to 1.6 times while maintaining accuracy across various
downstream tasks, including commonsense and arithmetic reasoning, code
generation, and instruction following.

</details>


### [351] [Subspace-Boosted Model Merging](https://arxiv.org/abs/2506.16506)
*Ronald Skorobogat,Karsten Roth,Mariana-Iuliana Georgescu,Zeynep Akata*

Main category: cs.LG

TL;DR: 论文提出Subspace Boosting方法，通过维持任务向量空间的秩来提升模型合并效果，并在视觉基准测试中显著提高性能。


<details>
  <summary>Details</summary>
Motivation: 随着合并专家模型数量的增加，性能提升逐渐减少，任务向量空间出现秩崩溃问题。

Method: 提出Subspace Boosting方法，基于奇异值分解的任务向量空间维持秩，并引入高阶广义奇异值分解量化任务相似性。

Result: Subspace Boosting在合并多达20个专家模型时，性能提升超过10%。

Conclusion: Subspace Boosting有效解决了模型合并中的秩崩溃问题，并提供了任务相似性的新视角。

Abstract: Model merging enables the combination of multiple specialized expert models
into a single model capable of performing multiple tasks. However, the benefits
of merging an increasing amount of specialized experts generally lead to
diminishing returns and reduced overall performance gains. In this work, we
offer an explanation and analysis from a task arithmetic perspective; revealing
that as the merging process (across numerous existing merging methods)
continues for more and more experts, the associated task vector space
experiences rank collapse. To mitigate this issue, we introduce Subspace
Boosting, which operates on the singular value decomposed task vector space and
maintains task vector ranks. Subspace Boosting raises merging efficacy for up
to 20 expert models by large margins of more than 10% when evaluated on vision
benchmarks. Moreover, we propose employing Higher-Order Generalized Singular
Value Decomposition to further quantify task similarity, offering a new
interpretable perspective on model merging.

</details>


### [352] [Robust Reward Modeling via Causal Rubrics](https://arxiv.org/abs/2506.16507)
*Pragya Srivastava,Harman Singh,Rahul Madhavan,Gandharv Patil,Sravanti Addepalli,Arun Suggala,Rengarajan Aravamudhan,Soumya Sharma,Anirban Laha,Aravindan Raghuveer,Karthikeyan Shanmugam,Doina Precup*

Main category: cs.LG

TL;DR: 论文提出Crome框架，通过因果增强和中性增强解决奖励模型中的奖励黑客问题，显著提升了模型性能。


<details>
  <summary>Details</summary>
Motivation: 奖励模型（RMs）在训练中容易受到奖励黑客的影响，即模型会错误地将表面或虚假属性（如回答长度或格式）与质量（如事实性、相关性）关联起来。

Method: Crome框架通过因果增强（针对因果属性）和中性增强（针对虚假属性）进行训练，无需了解虚假因素，仅通过干预因果属性生成增强数据。

Result: Crome在RewardBench上平均准确率提升5.4%，特定类别提升高达13.2%和7.2%，并在多个基准测试中表现稳健。

Conclusion: Crome通过因果建模有效缓解奖励黑客问题，显著提升了奖励模型的鲁棒性和性能。

Abstract: Reward models (RMs) are fundamental to aligning Large Language Models (LLMs)
via human feedback, yet they often suffer from reward hacking. They tend to
latch on to superficial or spurious attributes, such as response length or
formatting, mistaking these cues learned from correlations in training data for
the true causal drivers of quality (e.g., factuality, relevance). This occurs
because standard training objectives struggle to disentangle these factors,
leading to brittle RMs and misaligned policies. We introduce Crome (Causally
Robust Reward Modeling), a novel framework grounded in an explicit causal model
designed to mitigate reward hacking. Crome employs the following synthetic
targeted augmentations during training: (1) Causal Augmentations, which are
pairs that differ along specific causal attributes, to enforce sensitivity
along each causal attribute individually, and (2) Neutral Augmentations, which
are tie-label pairs varying primarily in spurious attributes, to enforce
invariance along spurious attributes. Notably, our augmentations are produced
without any knowledge of spurious factors, via answer interventions only along
causal rubrics, that are identified by querying an oracle LLM. Empirically,
Crome significantly outperforms standard baselines on RewardBench, improving
average accuracy by up to 5.4% and achieving gains of up to 13.2% and 7.2% in
specific categories. The robustness of Crome is further testified by the
consistent gains obtained in a Best-of-N inference setting across increasing N,
across various benchmarks, including the popular RewardBench (covering chat,
chat-hard, safety, and reasoning tasks), the safety-focused WildGuardTest, and
the reasoning-specific GSM8k.

</details>


### [353] [Aligning ASR Evaluation with Human and LLM Judgments: Intelligibility Metrics Using Phonetic, Semantic, and NLI Approaches](https://arxiv.org/abs/2506.16528)
*Bornali Phukon,Xiuwen Zheng,Mark Hasegawa-Johnson*

Main category: cs.LG

TL;DR: 论文提出了一种新的ASR评估指标，结合NLI分数、语义相似性和语音相似性，以更好地捕捉语音可懂度，尤其在发音障碍语音中表现优于传统指标。


<details>
  <summary>Details</summary>
Motivation: 传统ASR指标（如WER和CER）无法有效衡量语音可懂度，尤其在发音障碍语音中，语义对齐比精确单词匹配更重要。

Method: 提出了一种新指标，整合了自然语言推理（NLI）分数、语义相似性和语音相似性。

Result: 新指标在Speech Accessibility Project数据上与人类判断的相关系数达0.890，优于传统方法。

Conclusion: 强调了在ASR评估中优先考虑可懂度而非基于错误的方法的重要性。

Abstract: Traditional ASR metrics like WER and CER fail to capture intelligibility,
especially for dysarthric and dysphonic speech, where semantic alignment
matters more than exact word matches. ASR systems struggle with these speech
types, often producing errors like phoneme repetitions and imprecise
consonants, yet the meaning remains clear to human listeners. We identify two
key challenges: (1) Existing metrics do not adequately reflect intelligibility,
and (2) while LLMs can refine ASR output, their effectiveness in correcting ASR
transcripts of dysarthric speech remains underexplored. To address this, we
propose a novel metric integrating Natural Language Inference (NLI) scores,
semantic similarity, and phonetic similarity. Our ASR evaluation metric
achieves a 0.890 correlation with human judgments on Speech Accessibility
Project data, surpassing traditional methods and emphasizing the need to
prioritize intelligibility over error-based measures.

</details>


### [354] [Mr. Snuffleupagus at SemEval-2025 Task 4: Unlearning Factual Knowledge from LLMs Using Adaptive RMU](https://arxiv.org/abs/2506.16548)
*Arjun Dosajh,Mihika Sanghi*

Main category: cs.LG

TL;DR: 论文提出了一种名为自适应表示误导遗忘（RMU）的技术，用于从大语言模型中遗忘敏感信息，解决了隐私和版权问题。


<details>
  <summary>Details</summary>
Motivation: 大语言模型（LLMs）容易记忆训练数据，引发隐私、版权和安全问题，尤其是涉及个人身份信息（PII）时。

Method: 采用自适应表示误导遗忘（RMU）技术，分析不同解码层对敏感信息移除的效果。

Result: 该技术在1B和7B参数模型的官方排行榜上均排名第4。

Conclusion: RMU技术为LLMs中的敏感信息遗忘提供了有效解决方案。

Abstract: Large Language Models (LLMs) have demonstrated remarkable capabilities in
natural language understanding and generation. However, their tendency to
memorize training data raises concerns regarding privacy, copyright compliance,
and security, particularly in cases involving Personally Identifiable
Information (PII). Effective machine unlearning techniques are essential to
mitigate these risks, yet existing methods remain underdeveloped for LLMs due
to their open-ended output space. In this work, we apply the Adaptive
Representation Misdirection Unlearning (RMU) technique to unlearn sensitive
information from LLMs. Through extensive experiments, we analyze the effects of
unlearning across different decoder layers to determine the most effective
regions for sensitive information removal. Our technique ranked 4th on the
official leaderboard of both 1B parameter and 7B parameter models.

</details>


### [355] [A Free Probabilistic Framework for Analyzing the Transformer-based Language Models](https://arxiv.org/abs/2506.16550)
*Swagatam Das*

Main category: cs.LG

TL;DR: 该论文提出了一种基于自由概率理论的算子理论框架，用于分析基于Transformer的语言模型，揭示了其谱动力学系统，并提供了对归纳偏差、泛化行为和熵动态的见解。


<details>
  <summary>Details</summary>
Motivation: 通过将Transformer模型中的嵌入和注意力机制表示为自伴算子，研究其非交换卷积特性，以更深入地理解模型的架构和行为。

Method: 使用自由概率理论，将token嵌入和注意力机制建模为自伴算子，并将注意力重新解释为非交换卷积，分析层间表示的传播。

Result: 揭示了Transformer层的谱迹随深度可预测地演化，并基于自由熵推导了泛化边界。

Conclusion: 该框架为大型语言模型的信息流和结构复杂性提供了理论分析工具，连接了神经网络架构与非交换调和分析。

Abstract: We outline an operator-theoretic framework for analyzing transformer-based
language models using the tools of free probability theory. By representing
token embeddings and attention mechanisms as self-adjoint operators in a racial
probability space, we reinterpret attention as a non-commutative convolution
and view the layer-wise propagation of representations as an evolution governed
by free additive convolution. This formalism reveals a spectral dynamical
system underpinning deep transformer stacks and offers insight into their
inductive biases, generalization behavior, and entropy dynamics. We derive a
generalization bound based on free entropy and demonstrate that the spectral
trace of transformer layers evolves predictably with depth. Our approach
bridges neural architecture with non-commutative harmonic analysis, enabling
principled analysis of information flow and structural complexity in large
language models

</details>


### [356] [One Sample is Enough to Make Conformal Prediction Robust](https://arxiv.org/abs/2506.16553)
*Soroush H. Zargarbashi,Mohammad Sadegh Akhondzadeh,Aleksandar Bojchevski*

Main category: cs.LG

TL;DR: 本文提出了一种单样本鲁棒共形预测（RCP1）方法，通过单次随机扰动输入实现鲁棒性，相比现有方法减少了计算开销。


<details>
  <summary>Details</summary>
Motivation: 现有基于平滑的鲁棒共形预测方法需要多次模型前向传递，计算成本高。

Method: 利用任何二元证书，提出RCP1方法，通过单次随机扰动输入实现鲁棒性。

Result: RCP1生成的鲁棒集合平均尺寸更小，优于现有方法。

Conclusion: 该方法适用于分类和回归任务，并可扩展至基于平滑的鲁棒共形风险控制。

Abstract: Given any model, conformal prediction (CP) returns prediction sets guaranteed
to include the true label with high adjustable probability. Robust CP (RCP)
extends this to inputs with worst-case noise. A well-established approach is to
use randomized smoothing for RCP since it is applicable to any black-box model
and provides smaller sets compared to deterministic methods. However, current
smoothing-based RCP requires many model forward passes per each input which is
computationally expensive. We show that conformal prediction attains some
robustness even with a forward pass on a single randomly perturbed input. Using
any binary certificate we propose a single sample robust CP (RCP1). Our
approach returns robust sets with smaller average set size compared to SOTA
methods which use many (e.g. around 100) passes per input. Our key insight is
to certify the conformal prediction procedure itself rather than individual
scores. Our approach is agnostic to the setup (classification and regression).
We further extend our approach to smoothing-based robust conformal risk
control.

</details>


### [357] [Energy-Based Transfer for Reinforcement Learning](https://arxiv.org/abs/2506.16590)
*Zeyun Deng,Jasorsi Ghosh,Fiona Xie,Yuzhe Lu,Katia Sycara,Joseph Campbell*

Main category: cs.LG

TL;DR: 提出了一种基于能量的迁移学习方法，通过选择性指导提升强化学习在多任务或持续学习中的样本效率。


<details>
  <summary>Details</summary>
Motivation: 强化学习算法样本效率低，迁移知识时可能因任务差异导致指导效果不佳。

Method: 使用基于能量的迁移学习方法，结合分布外检测选择性指导，确保教师策略仅在训练分布内状态干预。

Result: 理论证明能量分数反映教师的状态访问密度，实验显示在单任务和多任务中均提升了样本效率和性能。

Conclusion: 该方法有效解决了迁移学习中指导偏差问题，显著提升了强化学习的样本效率和性能。

Abstract: Reinforcement learning algorithms often suffer from poor sample efficiency,
making them challenging to apply in multi-task or continual learning settings.
Efficiency can be improved by transferring knowledge from a previously trained
teacher policy to guide exploration in new but related tasks. However, if the
new task sufficiently differs from the teacher's training task, the transferred
guidance may be sub-optimal and bias exploration toward low-reward behaviors.
We propose an energy-based transfer learning method that uses
out-of-distribution detection to selectively issue guidance, enabling the
teacher to intervene only in states within its training distribution. We
theoretically show that energy scores reflect the teacher's state-visitation
density and empirically demonstrate improved sample efficiency and performance
across both single-task and multi-task settings.

</details>


### [358] [FLAME: Towards Federated Fine-Tuning Large Language Models Through Adaptive SMoE](https://arxiv.org/abs/2506.16600)
*Khiem Le,Tuan Tran,Ting Hua,Nitesh V. Chawla*

Main category: cs.LG

TL;DR: FLAME是一种基于稀疏混合专家架构的联邦学习框架，通过动态激活专家数量实现客户端适应性，解决了现有方法因压缩导致性能下降的问题。


<details>
  <summary>Details</summary>
Motivation: 现有资源自适应的LoRA联邦微调方法因压缩全局LoRA矩阵导致信息丢失，性能不佳。FLAME旨在通过保留完整矩阵并动态调整专家数量来解决这一问题。

Method: FLAME采用稀疏混合专家（SMoE）架构，保留完整全局LoRA矩阵，通过激活不同数量的专家实现客户端适应性，并引入轻量级重缩放机制和激活感知聚合方案。

Result: 实验表明，FLAME在不同计算环境下均优于现有方法，提供了稳健且高效的资源自适应联邦学习解决方案。

Conclusion: FLAME通过SMoE架构和优化机制，显著提升了资源自适应联邦学习的性能，解决了信息丢失和专家训练不平衡的问题。

Abstract: Existing resource-adaptive LoRA federated fine-tuning methods enable clients
to fine-tune models using compressed versions of global LoRA matrices, in order
to accommodate various compute resources across clients. This compression
requirement will lead to suboptimal performance due to information loss. To
address this, we propose FLAME, a novel federated learning framework based on
the Sparse Mixture-of-Experts (SMoE) architecture. Unlike prior approaches,
FLAME retains full (uncompressed) global LoRA matrices and achieves client-side
adaptability by varying the number of activated experts per client. However,
incorporating SMoE into federated learning introduces unique challenges,
specifically, the mismatch in output magnitude from partial expert activation
and the imbalance in expert training quality across clients. FLAME tackles
these challenges through a lightweight rescaling mechanism and an
activation-aware aggregation scheme. Empirical results across diverse
computational settings demonstrate that FLAME consistently outperforms existing
methods, providing a robust and effective solution for resource-adaptive
federated learning.

</details>


### [359] [SlepNet: Spectral Subgraph Representation Learning for Neural Dynamics](https://arxiv.org/abs/2506.16602)
*Siddharth Viswanath,Rahul Singh,Yanlei Zhang,J. Adam Noah,Joy Hirsch,Smita Krishnaswamy*

Main category: cs.LG

TL;DR: SlepNet是一种新型图卷积网络架构，利用Slepian基而非图傅里叶谐波，专注于子图的信号能量集中，优于传统GNN和图信号处理方法。


<details>
  <summary>Details</summary>
Motivation: 图神经网络在表示图信号模式上存在局限性，特别是在神经科学等领域中，信号的高维和局部化特征难以捕捉。

Method: 提出SlepNet，使用Slepian基自动学习相关子图，集中信号能量，生成高分辨率表示。

Result: 在多个fMRI和交通动态数据集上，SlepNet表现优于基线方法，并能区分相似模式。

Conclusion: SlepNet在时空数据的预测和表示学习上具有潜力，支持下游任务。

Abstract: Graph neural networks have been useful in machine learning on
graph-structured data, particularly for node classification and some types of
graph classification tasks. However, they have had limited use in representing
patterning of signals over graphs. Patterning of signals over graphs and in
subgraphs carries important information in many domains including neuroscience.
Neural signals are spatiotemporally patterned, high dimensional and difficult
to decode. Graph signal processing and associated GCN models utilize the graph
Fourier transform and are unable to efficiently represent spatially or
spectrally localized signal patterning on graphs. Wavelet transforms have shown
promise here, but offer non-canonical representations and cannot be tightly
confined to subgraphs. Here we propose SlepNet, a novel GCN architecture that
uses Slepian bases rather than graph Fourier harmonics. In SlepNet, the Slepian
harmonics optimally concentrate signal energy on specifically relevant
subgraphs that are automatically learned with a mask. Thus, they can produce
canonical and highly resolved representations of neural activity, focusing
energy of harmonics on areas of the brain which are activated. We evaluated
SlepNet across three fMRI datasets, spanning cognitive and visual tasks, and
two traffic dynamics datasets, comparing its performance against conventional
GNNs and graph signal processing constructs. SlepNet outperforms the baselines
in all datasets. Moreover, the extracted representations of signal patterns
from SlepNet offers more resolution in distinguishing between similar patterns,
and thus represent brain signaling transients as informative trajectories. Here
we have shown that these extracted trajectory representations can be used for
other downstream untrained tasks. Thus we establish that SlepNet is useful both
for prediction and representation learning in spatiotemporal data.

</details>


### [360] [Distribution Parameter Actor-Critic: Shifting the Agent-Environment Boundary for Diverse Action Spaces](https://arxiv.org/abs/2506.16608)
*Jiamin He,A. Rupam Mahmood,Martha White*

Main category: cs.LG

TL;DR: 提出了一种新的强化学习框架，将分布参数作为动作，重新定义了智能体与环境的边界，使动作空间连续化。提出了DPPG梯度估计器和DPAC算法，实验表明其性能优于TD3。


<details>
  <summary>Details</summary>
Motivation: 传统强化学习中，动作空间的离散性或混合性可能导致学习效率低下。通过将分布参数作为动作，可以统一处理不同类型的动作空间，提高学习效率。

Method: 提出了Distribution Parameter Policy Gradient (DPPG)梯度估计器，降低了方差；设计了Interpolated Critic Learning (ICL)策略优化学习；基于TD3提出了DPAC算法。

Result: DPAC在MuJoCo连续控制任务中表现优于TD3，并在离散化动作空间中具有竞争力。

Conclusion: 通过重新参数化动作空间，DPAC框架在连续和离散动作空间中均表现出色，为强化学习提供了新的思路。

Abstract: We introduce a novel reinforcement learning (RL) framework that treats
distribution parameters as actions, redefining the boundary between agent and
environment. This reparameterization makes the new action space continuous,
regardless of the original action type (discrete, continuous, mixed, etc.).
Under this new parameterization, we develop a generalized deterministic policy
gradient estimator, Distribution Parameter Policy Gradient (DPPG), which has
lower variance than the gradient in the original action space. Although
learning the critic over distribution parameters poses new challenges, we
introduce interpolated critic learning (ICL), a simple yet effective strategy
to enhance learning, supported by insights from bandit settings. Building on
TD3, a strong baseline for continuous control, we propose a practical
DPPG-based actor-critic algorithm, Distribution Parameter Actor-Critic (DPAC).
Empirically, DPAC outperforms TD3 in MuJoCo continuous control tasks from
OpenAI Gym and DeepMind Control Suite, and demonstrates competitive performance
on the same environments with discretized action spaces.

</details>


### [361] [Learning Causally Predictable Outcomes from Psychiatric Longitudinal Data](https://arxiv.org/abs/2506.16629)
*Eric V. Strobl*

Main category: cs.LG

TL;DR: 论文提出DEBIAS算法，通过优化结果定义以最大化因果可识别性，解决精神病学纵向数据中的混杂问题。


<details>
  <summary>Details</summary>
Motivation: 精神病学中症状异质性和潜在混杂因素常削弱传统估计方法的效果，现有方法假设结果变量固定且通过观察协变量调整混杂，但实践中无混杂假设可能不成立。

Method: DEBIAS算法学习非负、临床可解释的权重以聚合结果，最大化持久治疗效果，并利用精神病纵向数据中先前治疗的时滞直接效应最小化观察和潜在混杂。

Result: DEBIAS在抑郁症和精神分裂症的综合实验中，始终优于现有方法，恢复临床可解释复合结果的因果效应。

Conclusion: DEBIAS通过优化结果定义和最小化混杂，显著提升了因果推断的准确性，为精神病学纵向数据提供了实用工具。

Abstract: Causal inference in longitudinal biomedical data remains a central challenge,
especially in psychiatry, where symptom heterogeneity and latent confounding
frequently undermine classical estimators. Most existing methods for treatment
effect estimation presuppose a fixed outcome variable and address confounding
through observed covariate adjustment. However, the assumption of
unconfoundedness may not hold for a fixed outcome in practice. To address this
foundational limitation, we directly optimize the outcome definition to
maximize causal identifiability. Our DEBIAS (Durable Effects with
Backdoor-Invariant Aggregated Symptoms) algorithm learns non-negative,
clinically interpretable weights for outcome aggregation, maximizing durable
treatment effects and empirically minimizing both observed and latent
confounding by leveraging the time-limited direct effects of prior treatments
in psychiatric longitudinal data. The algorithm also furnishes an empirically
verifiable test for outcome unconfoundedness. DEBIAS consistently outperforms
state-of-the-art methods in recovering causal effects for clinically
interpretable composite outcomes across comprehensive experiments in depression
and schizophrenia.

</details>


### [362] [Semantic Outlier Removal with Embedding Models and LLMs](https://arxiv.org/abs/2506.16644)
*Eren Akbiyik,João Almeida,Rik Melis,Ritu Sriram,Viviana Petrescu,Vilhjálmur Vilhjálmsson*

Main category: cs.LG

TL;DR: SORE是一种基于多语言句子嵌入和近似最近邻搜索的语义离群值去除方法，高效且透明，能在多语言环境中高精度去除无关内容。


<details>
  <summary>Details</summary>
Motivation: 传统方法如HTML模板提取或关键词过滤在多语言和上下文敏感场景中表现不佳，而大型语言模型（LLM）虽然效果好但计算成本高。

Method: SORE通过元数据嵌入识别核心内容，并利用近似最近邻搜索标记与预定义离群组匹配或显著偏离核心的文本段。

Result: 实验表明，SORE在HTML数据集上优于结构方法，并在多语言场景中实现高精度。

Conclusion: SORE以低成本实现接近LLM的提取精度，已在生产环境中部署，支持多语言处理，并开源了实现和评估数据集。

Abstract: Modern text processing pipelines demand robust methods to remove extraneous
content while preserving a document's core message. Traditional approaches such
as HTML boilerplate extraction or keyword filters often fail in multilingual
settings and struggle with context-sensitive nuances, whereas Large Language
Models (LLMs) offer improved quality at high computational cost. We introduce
SORE (Semantic Outlier Removal), a cost-effective, transparent method that
leverages multilingual sentence embeddings and approximate nearest-neighbor
search to identify and excise unwanted text segments. By first identifying core
content via metadata embedding and then flagging segments that either closely
match predefined outlier groups or deviate significantly from the core, SORE
achieves near-LLM extraction precision at a fraction of the cost. Experiments
on HTML datasets demonstrate that SORE outperforms structural methods and yield
high precision in diverse scenarios. Our system is currently deployed in
production, processing millions of documents daily across multiple languages
while maintaining both efficiency and accuracy. To facilitate reproducibility
and further research, we release our implementation and evaluation datasets.

</details>


### [363] [A Distributional-Lifting Theorem for PAC Learning](https://arxiv.org/abs/2506.16651)
*Guy Blanc,Jane Lange,Carmen Strassle,Li-Yang Tan*

Main category: cs.LG

TL;DR: 论文提出了一种分布提升定理，将有限分布族的PAC学习器升级为适用于任何分布的学习器，效率与目标分布的复杂性相关。


<details>
  <summary>Details</summary>
Motivation: 解决分布特定学习算法的局限性，扩展其适用范围。

Method: 提出分布提升定理，避免直接学习目标分布，适用于标准PAC模型。

Result: 新方法更高效、通用，且保留了学习器的噪声容忍性。

Conclusion: 新方法在标准PAC模型中更优，适用于所有基础分布族。

Abstract: The apparent difficulty of efficient distribution-free PAC learning has led
to a large body of work on distribution-specific learning. Distributional
assumptions facilitate the design of efficient algorithms but also limit their
reach and relevance. Towards addressing this, we prove a distributional-lifting
theorem: This upgrades a learner that succeeds with respect to a limited
distribution family $\mathcal{D}$ to one that succeeds with respect to any
distribution $D^\star$, with an efficiency overhead that scales with the
complexity of expressing $D^\star$ as a mixture of distributions in
$\mathcal{D}$.
  Recent work of Blanc, Lange, Malik, and Tan considered the special case of
lifting uniform-distribution learners and designed a lifter that uses a
conditional sample oracle for $D^\star$, a strong form of access not afforded
by the standard PAC model. Their approach, which draws on ideas from
semi-supervised learning, first learns $D^\star$ and then uses this information
to lift.
  We show that their approach is information-theoretically intractable with
access only to random examples, thereby giving formal justification for their
use of the conditional sample oracle. We then take a different approach that
sidesteps the need to learn $D^\star$, yielding a lifter that works in the
standard PAC model and enjoys additional advantages: it works for all base
distribution families, preserves the noise tolerance of learners, has better
sample complexity, and is simpler.

</details>


### [364] [Relational Deep Learning: Challenges, Foundations and Next-Generation Architectures](https://arxiv.org/abs/2506.16654)
*Vijay Prakash Dwivedi,Charilaos Kanatsoulis,Shenyang Huang,Jure Leskovec*

Main category: cs.LG

TL;DR: 本文综述了关系深度学习（RDL），将关系数据库表示为关系实体图，并探讨了其关键特性、挑战及最新进展。


<details>
  <summary>Details</summary>
Motivation: 关系数据库的多表数据可以通过关系实体图进行端到端表示学习，避免了传统特征工程的复杂性。

Method: 介绍了关系实体图的表示方法，并回顾了用于评估RDL模型的公共基准数据集，讨论了GNN在RDL中的应用。

Result: 总结了RDL的关键挑战（如大规模多表集成、时序动态建模）及针对关系实体图的神经网络方法。

Conclusion: RDL有望统一图机器学习的多个子领域，推动关系数据处理的基础模型设计。

Abstract: Graph machine learning has led to a significant increase in the capabilities
of models that learn on arbitrary graph-structured data and has been applied to
molecules, social networks, recommendation systems, and transportation, among
other domains. Data in multi-tabular relational databases can also be
constructed as 'relational entity graphs' for Relational Deep Learning (RDL) -
a new blueprint that enables end-to-end representation learning without
traditional feature engineering. Compared to arbitrary graph-structured data,
relational entity graphs have key properties: (i) their structure is defined by
primary-foreign key relationships between entities in different tables, (ii)
the structural connectivity is a function of the relational schema defining a
database, and (iii) the graph connectivity is temporal and heterogeneous in
nature. In this paper, we provide a comprehensive review of RDL by first
introducing the representation of relational databases as relational entity
graphs, and then reviewing public benchmark datasets that have been used to
develop and evaluate recent GNN-based RDL models. We discuss key challenges
including large-scale multi-table integration and the complexities of modeling
temporal dynamics and heterogeneous data, while also surveying foundational
neural network methods and recent architectural advances specialized for
relational entity graphs. Finally, we explore opportunities to unify these
distinct modeling challenges, highlighting how RDL converges multiple
sub-fields in graph machine learning towards the design of foundation models
that can transform the processing of relational data.

</details>


### [365] [Mesh-Informed Neural Operator : A Transformer Generative Approach](https://arxiv.org/abs/2506.16656)
*Yaozhong Shi,Zachary E. Ross,Domniki Asimaki,Kamyar Azizzadenesheli*

Main category: cs.LG

TL;DR: 论文提出了一种名为MINO的网格感知神经算子，解决了现有功能生成模型依赖FNO的限制，扩展了应用范围。


<details>
  <summary>Details</summary>
Motivation: 现有功能生成模型依赖FNO，限制了其在非规则网格和矩形域的应用，亟需一种更通用的方法。

Method: 通过图神经算子和交叉注意力机制，提出MINO作为功能生成模型的通用框架。

Result: MINO显著扩展了功能生成模型的应用范围，并提供了统一的视角。

Conclusion: MINO填补了功能生成模型领域的空白，并提供了标准化评估指标。

Abstract: Generative models in function spaces, situated at the intersection of
generative modeling and operator learning, are attracting increasing attention
due to their immense potential in diverse scientific and engineering
applications. While functional generative models are theoretically domain- and
discretization-agnostic, current implementations heavily rely on the Fourier
Neural Operator (FNO), limiting their applicability to regular grids and
rectangular domains. To overcome these critical limitations, we introduce the
Mesh-Informed Neural Operator (MINO). By leveraging graph neural operators and
cross-attention mechanisms, MINO offers a principled, domain- and
discretization-agnostic backbone for generative modeling in function spaces.
This advancement significantly expands the scope of such models to more diverse
applications in generative, inverse, and regression tasks. Furthermore, MINO
provides a unified perspective on integrating neural operators with general
advanced deep learning architectures. Finally, we introduce a suite of
standardized evaluation metrics that enable objective comparison of functional
generative models, addressing another critical gap in the field.

</details>


### [366] [A Minimalist Optimizer Design for LLM Pretraining](https://arxiv.org/abs/2506.16659)
*Athanasios Glentis,Jiaxiang Li,Andi Han,Mingyi Hong*

Main category: cs.LG

TL;DR: SCALE是一种新的优化器，结合列归一化SGD和最后一层动量，显著减少内存使用，同时保持或超越Adam的性能。


<details>
  <summary>Details</summary>
Motivation: 研究在LLM预训练中，优化器状态的最小需求，以降低内存消耗。

Method: 采用列归一化梯度和仅对输出层添加一阶动量，提出SCALE优化器。

Result: SCALE在LLaMA模型上表现优于Adam和其他内存高效优化器，内存使用仅为35-45%。

Conclusion: SCALE是内存受限下大规模预训练的高效选择，并为优化器设计提供了简约基线。

Abstract: Training large language models (LLMs) typically relies on adaptive optimizers
such as Adam, which require significant memory to maintain first- and
second-moment matrices, known as optimizer states. While recent works such as
GaLore, Fira, and APOLLO have proposed state-compressed variants to reduce
memory consumption, a fundamental question remains: What is the minimal amount
of optimizer state that is truly necessary to retain state-of-the-art
performance in LLM pretraining? In this work, we systematically investigate
this question using a bottom-up approach. We find that two memory- and
compute-efficient optimization techniques are particularly effective: (1)
column-wise gradient normalization significantly boosts the performance of
plain SGD without requiring momentum; and (2) adding first-order momentum only
to the output layer - where gradient variance is highest - yields performance
competitive with fully adaptive methods such as Muon. Based on these insights,
we propose SCALE (Stochastic Column-normalized Last-layer Momentum), a new
optimizer that combines column-normalized SGD with last-layer momentum, where
column normalization refers to normalizing the gradient along the output
dimension. Across multiple LLaMA models (60M-1B), SCALE matches or exceeds the
performance of Adam while using only 35-45% of the total memory. It also
consistently outperforms memory-efficient optimizers such as GaLore, Fira, and
APOLLO, making it a strong candidate for large-scale pretraining under memory
constraints. For the LLaMA 7B model, SCALE outperforms the state-of-the-art
method APOLLO in terms of both perplexity and memory consumption. In addition,
our method serves as a minimalist baseline for more sophisticated optimizer
design.

</details>


### [367] [Private Training & Data Generation by Clustering Embeddings](https://arxiv.org/abs/2506.16661)
*Felix Zhou,Samson Zhou,Vahab Mirrokni,Alessandro Epasto,Vincent Cohen-Addad*

Main category: cs.LG

TL;DR: 论文提出了一种基于差分隐私（DP）的合成图像嵌入生成方法，通过高斯混合模型（GMM）在嵌入空间中进行DP聚类，实现了高分类精度和隐私保护。


<details>
  <summary>Details</summary>
Motivation: 深度神经网络在训练过程中可能泄露敏感数据，差分隐私（DP）提供了一种保护机制，但需要高效且可扩展的方法来生成合成数据。

Method: 使用DP聚类在高斯混合模型（GMM）中拟合嵌入空间，生成合成图像嵌入，并通过简单的两层神经网络进行分类。

Result: 实验表明，该方法在标准数据集上实现了最先进的分类精度，并能生成逼真的合成图像。

Conclusion: 该方法具有通用性和可扩展性，适用于不同任务，同时保护了数据隐私。

Abstract: Deep neural networks often use large, high-quality datasets to achieve high
performance on many machine learning tasks. When training involves potentially
sensitive data, this process can raise privacy concerns, as large models have
been shown to unintentionally memorize and reveal sensitive information,
including reconstructing entire training samples. Differential privacy (DP)
provides a robust framework for protecting individual data and in particular, a
new approach to privately training deep neural networks is to approximate the
input dataset with a privately generated synthetic dataset, before any
subsequent training algorithm. We introduce a novel principled method for DP
synthetic image embedding generation, based on fitting a Gaussian Mixture Model
(GMM) in an appropriate embedding space using DP clustering. Our method
provably learns a GMM under separation conditions. Empirically, a simple
two-layer neural network trained on synthetically generated embeddings achieves
state-of-the-art (SOTA) classification accuracy on standard benchmark datasets.
Additionally, we demonstrate that our method can generate realistic synthetic
images that achieve downstream classification accuracy comparable to SOTA
methods. Our method is quite general, as the encoder and decoder modules can be
freely substituted to suit different tasks. It is also highly scalable,
consisting only of subroutines that scale linearly with the number of samples
and/or can be implemented efficiently in distributed systems.

</details>


### [368] [Fast and Stable Diffusion Planning through Variational Adaptive Weighting](https://arxiv.org/abs/2506.16688)
*Zhiying Qiu,Tao Lin*

Main category: cs.LG

TL;DR: 本文提出了一种基于变分最优不确定性感知的权重函数，并通过多项式近似方法优化扩散模型在离线强化学习中的训练效率。


<details>
  <summary>Details</summary>
Motivation: 扩散模型在离线强化学习中表现良好，但存在训练成本高、收敛慢的问题，尤其是使用基于Transformer的去噪主干时。现有权重函数依赖神经网络近似器，在早期训练阶段效果不佳。

Method: 提出了一种变分最优不确定性感知权重函数，并引入闭式多项式近似方法在线估计。将其集成到扩散规划流程中。

Result: 在Maze2D和Kitchen任务上的实验表明，该方法性能优异，训练步骤减少高达10倍。

Conclusion: 该方法显著提升了扩散模型在离线强化学习中的训练效率和稳定性。

Abstract: Diffusion models have recently shown promise in offline RL. However, these
methods often suffer from high training costs and slow convergence,
particularly when using transformer-based denoising backbones. While several
optimization strategies have been proposed -- such as modified noise schedules,
auxiliary prediction targets, and adaptive loss weighting -- challenges remain
in achieving stable and efficient training. In particular, existing loss
weighting functions typically rely on neural network approximators, which can
be ineffective in early training phases due to limited generalization capacity
of MLPs when exposed to sparse feedback in the early training stages. In this
work, we derive a variationally optimal uncertainty-aware weighting function
and introduce a closed-form polynomial approximation method for its online
estimation under the flow-based generative modeling framework. We integrate our
method into a diffusion planning pipeline and evaluate it on standard offline
RL benchmarks. Experimental results on Maze2D and Kitchen tasks show that our
method achieves competitive performance with up to 10 times fewer training
steps, highlighting its practical effectiveness.

</details>


### [369] [SIDE: Semantic ID Embedding for effective learning from sequences](https://arxiv.org/abs/2506.16698)
*Dinesh Ramasamy,Shakti Kumar,Chris Cadonic,Jiaxin Yang,Sohini Roychowdhury,Esam Abdel Rhman,Srihari Reddy*

Main category: cs.LG

TL;DR: 论文提出了一种基于向量量化（VQ）的新方法，通过生成紧凑的语义ID（SID）替代传统嵌入，解决了实时推荐系统中存储和推理成本高的问题。


<details>
  <summary>Details</summary>
Motivation: 工业广告推荐系统在处理大规模用户历史数据时，传统嵌入方法在存储和推理成本上存在挑战。

Method: 提出三种创新：多任务VQ-VAE框架（VQ融合）、参数自由的SID转换技术（SIDE）和离散PCA（DPCA）量化方法。

Result: 在大规模工业广告推荐系统中，实现了2.4倍的归一化熵增益提升和3倍的数据占用减少。

Conclusion: 该方法显著提升了推荐系统的效率和性能，为大规模实时推荐提供了可行解决方案。

Abstract: Sequence-based recommendations models are driving the state-of-the-art for
industrial ad-recommendation systems. Such systems typically deal with user
histories or sequence lengths ranging in the order of O(10^3) to O(10^4)
events. While adding embeddings at this scale is manageable in pre-trained
models, incorporating them into real-time prediction models is challenging due
to both storage and inference costs. To address this scaling challenge, we
propose a novel approach that leverages vector quantization (VQ) to inject a
compact Semantic ID (SID) as input to the recommendation models instead of a
collection of embeddings. Our method builds on recent works of SIDs by
introducing three key innovations: (i) a multi-task VQ-VAE framework, called VQ
fusion that fuses multiple content embeddings and categorical predictions into
a single Semantic ID; (ii) a parameter-free, highly granular SID-to-embedding
conversion technique, called SIDE, that is validated with two content embedding
collections, thereby eliminating the need for a large parameterized lookup
table; and (iii) a novel quantization method called Discrete-PCA (DPCA) which
generalizes and enhances residual quantization techniques. The proposed
enhancements when applied to a large-scale industrial ads-recommendation system
achieves 2.4X improvement in normalized entropy (NE) gain and 3X reduction in
data footprint compared to traditional SID methods.

</details>


### [370] [How Many Domains Suffice for Domain Generalization? A Tight Characterization via the Domain Shattering Dimension](https://arxiv.org/abs/2506.16704)
*Cynthia Dwork,Lunjia Hu,Han Shao*

Main category: cs.LG

TL;DR: 论文研究了领域泛化的基本问题，提出了一个新的组合度量“域破碎维度”，并证明了其与经典VC维度的紧密关系。


<details>
  <summary>Details</summary>
Motivation: 探讨在给定领域族中，需要从多少随机采样的领域收集数据才能学习一个在所有领域（包括未见过的）上表现良好的模型。

Method: 在PAC框架下建模问题，引入域破碎维度作为新的组合度量。

Result: 域破碎维度刻画了领域样本复杂度，并与VC维度建立了定量关系。

Conclusion: 证明在标准PAC设置下可学习的假设类在该设置下也可学习。

Abstract: We study a fundamental question of domain generalization: given a family of
domains (i.e., data distributions), how many randomly sampled domains do we
need to collect data from in order to learn a model that performs reasonably
well on every seen and unseen domain in the family? We model this problem in
the PAC framework and introduce a new combinatorial measure, which we call the
domain shattering dimension. We show that this dimension characterizes the
domain sample complexity. Furthermore, we establish a tight quantitative
relationship between the domain shattering dimension and the classic VC
dimension, demonstrating that every hypothesis class that is learnable in the
standard PAC setting is also learnable in our setting.

</details>


### [371] [TriCon-SF: A Triple-Shuffle and Contribution-Aware Serial Federated Learning Framework for Heterogeneous Healthcare Data](https://arxiv.org/abs/2506.16723)
*Yuping Yan,Yizhi Wang,Yuanshuai Li,Yaochu Jin*

Main category: cs.LG

TL;DR: TriCon-SF是一种新型的串行联邦学习框架，通过三重随机化和贡献感知机制解决数据异构性和隐私安全问题。


<details>
  <summary>Details</summary>
Motivation: 在跨机构联邦学习中，直接模型传输可能违反隐私法规并易受攻击，同时需应对半诚实或恶意客户端的挑战。

Method: TriCon-SF引入模型层、数据段和训练序列的三重随机化，并结合Shapley值动态评估客户端贡献。

Result: 在非独立同分布的医疗数据集上，TriCon-SF在准确性和通信效率上优于标准方法，且能抵御隐私攻击。

Conclusion: TriCon-SF通过增强隐私保护和系统问责制，为隐私敏感领域的联邦学习提供了有效解决方案。

Abstract: Serial pipeline training is an efficient paradigm for handling data
heterogeneity in cross-silo federated learning with low communication overhead.
However, even without centralized aggregation, direct transfer of models
between clients can violate privacy regulations and remain susceptible to
gradient leakage and linkage attacks. Additionally, ensuring resilience against
semi-honest or malicious clients who may manipulate or misuse received models
remains a grand challenge, particularly in privacy-sensitive domains such as
healthcare. To address these challenges, we propose TriCon-SF, a novel serial
federated learning framework that integrates triple shuffling and contribution
awareness. TriCon-SF introduces three levels of randomization by shuffling
model layers, data segments, and training sequences to break deterministic
learning patterns and disrupt potential attack vectors, thereby enhancing
privacy and robustness. In parallel, it leverages Shapley value methods to
dynamically evaluate client contributions during training, enabling the
detection of dishonest behavior and enhancing system accountability. Extensive
experiments on non-IID healthcare datasets demonstrate that TriCon-SF
outperforms standard serial and parallel federated learning in both accuracy
and communication efficiency. Security analysis further supports its resilience
against client-side privacy attacks.

</details>


### [372] [On Training-Test (Mis)alignment in Unsupervised Combinatorial Optimization: Observation, Empirical Exploration, and Analysis](https://arxiv.org/abs/2506.16732)
*Fanchen Bu,Kijung Shin*

Main category: cs.LG

TL;DR: 论文探讨了无监督组合优化（UCO）中训练与测试的不对齐问题，提出在训练中加入可微分的去随机化方法以改善对齐性，但同时也带来了训练挑战。


<details>
  <summary>Details</summary>
Motivation: 现有UCO方法中训练与测试阶段存在不对齐问题，导致训练损失低并不一定带来更好的去随机化性能。

Method: 在训练阶段引入可微分的去随机化方法，以更好地对齐训练与测试。

Result: 初步实验表明该方法改善了训练与测试的对齐性，但也增加了训练难度。

Conclusion: 研究提出了改进UCO训练与测试对齐的新思路，但需进一步解决训练中的挑战。

Abstract: In unsupervised combinatorial optimization (UCO), during training, one aims
to have continuous decisions that are promising in a probabilistic sense for
each training instance, which enables end-to-end training on initially discrete
and non-differentiable problems. At the test time, for each test instance,
starting from continuous decisions, derandomization is typically applied to
obtain the final deterministic decisions. Researchers have developed more and
more powerful test-time derandomization schemes to enhance the empirical
performance and the theoretical guarantee of UCO methods. However, we notice a
misalignment between training and testing in the existing UCO methods.
Consequently, lower training losses do not necessarily entail better
post-derandomization performance, even for the training instances without any
data distribution shift. Empirically, we indeed observe such undesirable cases.
We explore a preliminary idea to better align training and testing in UCO by
including a differentiable version of derandomization into training. Our
empirical exploration shows that such an idea indeed improves training-test
alignment, but also introduces nontrivial challenges into training.

</details>


### [373] [Optimism Without Regularization: Constant Regret in Zero-Sum Games](https://arxiv.org/abs/2506.16736)
*John Lazarsfeld,Georgios Piliouras,Ryann Sim,Stratis Skoulakis*

Main category: cs.LG

TL;DR: 本文研究了乐观虚构博弈在两人零和游戏中的学习性能，首次证明无需正则化也能实现最优后悔率。


<details>
  <summary>Details</summary>
Motivation: 探讨乐观虚构博弈在无正则化情况下是否仍能实现低后悔率，填补理论空白。

Method: 通过双空间几何视角分析乐观虚构博弈，证明能量函数有界。

Result: 乐观虚构博弈在双策略游戏中仅产生常数后悔，而交替虚构博弈后悔下界为Ω(√T)。

Conclusion: 乐观性在无正则化下能实现低后悔，而交替性则无法。

Abstract: This paper studies the optimistic variant of Fictitious Play for learning in
two-player zero-sum games. While it is known that Optimistic FTRL -- a
regularized algorithm with a bounded stepsize parameter -- obtains constant
regret in this setting, we show for the first time that similar, optimal rates
are also achievable without regularization: we prove for two-strategy games
that Optimistic Fictitious Play (using any tiebreaking rule) obtains only
constant regret, providing surprising new evidence on the ability of
non-no-regret algorithms for fast learning in games. Our proof technique
leverages a geometric view of Optimistic Fictitious Play in the dual space of
payoff vectors, where we show a certain energy function of the iterates remains
bounded over time. Additionally, we also prove a regret lower bound of
$\Omega(\sqrt{T})$ for Alternating Fictitious Play. In the unregularized
regime, this separates the ability of optimism and alternation in achieving
$o(\sqrt{T})$ regret.

</details>


### [374] [Metapath-based Hyperbolic Contrastive Learning for Heterogeneous Graph Embedding](https://arxiv.org/abs/2506.16754)
*Jongmin Park,Seunghoon Han,Won-Yong Shin,Sungsu Lim*

Main category: cs.LG

TL;DR: 论文提出了一种基于多双曲空间的对比学习框架（MHCL），用于捕捉异质图中多样化的幂律结构，通过优化对比学习提升元路径嵌入的区分性。


<details>
  <summary>Details</summary>
Motivation: 异质图具有多样化的幂律结构，但现有双曲异质图嵌入模型仅依赖单一双曲空间，无法有效捕捉这些结构。

Method: 提出MHCL框架，利用多双曲空间分别描述不同元路径的复杂结构分布，并通过对比学习优化元路径嵌入的区分性。

Result: 实验表明MHCL在多种图机器学习任务中优于现有基线，能有效捕捉异质图的复杂结构。

Conclusion: MHCL通过多双曲空间和对比学习，显著提升了异质图嵌入的性能。

Abstract: The hyperbolic space, characterized by a constant negative curvature and
exponentially expanding space, aligns well with the structural properties of
heterogeneous graphs. However, although heterogeneous graphs inherently possess
diverse power-law structures, most hyperbolic heterogeneous graph embedding
models rely on a single hyperbolic space. This approach may fail to effectively
capture the diverse power-law structures within heterogeneous graphs. To
address this limitation, we propose a Metapath-based Hyperbolic Contrastive
Learning framework (MHCL), which uses multiple hyperbolic spaces to capture
diverse complex structures within heterogeneous graphs. Specifically, by
learning each hyperbolic space to describe the distribution of complex
structures corresponding to each metapath, it is possible to capture semantic
information effectively. Since metapath embeddings represent distinct semantic
information, preserving their discriminability is important when aggregating
them to obtain node representations. Therefore, we use a contrastive learning
approach to optimize MHCL and improve the discriminability of metapath
embeddings. In particular, our contrastive learning method minimizes the
distance between embeddings of the same metapath and maximizes the distance
between those of different metapaths in hyperbolic space, thereby improving the
separability of metapath embeddings with distinct semantic information. We
conduct comprehensive experiments to evaluate the effectiveness of MHCL. The
experimental results demonstrate that MHCL outperforms state-of-the-art
baselines in various graph machine learning tasks, effectively capturing the
complex structures of heterogeneous graphs.

</details>


### [375] [What Is the Point of Equality in Machine Learning Fairness? Beyond Equality of Opportunity](https://arxiv.org/abs/2506.16782)
*Youjin Kong*

Main category: cs.LG

TL;DR: 论文探讨了机器学习公平性的伦理基础，指出仅关注分配平等是不完整的，提出结合分配平等和关系平等的多元平等框架。


<details>
  <summary>Details</summary>
Motivation: 现有公平性研究主要基于分配平等，但忽视了结构性不平等和关系平等的重要性，需要更全面的伦理基础。

Method: 结合批判性社会和政治哲学，提出多元平等框架，整合分配平等和关系平等。

Result: 框架能更全面地解决机器学习系统中的分配性和表征性伤害。

Conclusion: 多元平等框架为机器学习公平性提供了更全面的伦理基础，并提出了实际应用路径。

Abstract: Fairness in machine learning (ML) has become a rapidly growing area of
research. But why, in the first place, is unfairness in ML morally wrong? And
why should we care about improving fairness? Most fair-ML research implicitly
appeals to distributive equality: the idea that desirable goods and benefits,
such as opportunities (e.g., Barocas et al., 2023), should be equally
distributed across society. Unfair ML models, then, are seen as wrong because
they unequally distribute such benefits. This paper argues that this exclusive
focus on distributive equality offers an incomplete and potentially misleading
ethical foundation. Grounding ML fairness in egalitarianism -- the view that
equality is a fundamental moral and social ideal -- requires challenging
structural inequality: systematic, institutional, and durable arrangements that
privilege some groups while disadvantaging others. Structural inequality
manifests through ML systems in two primary forms: allocative harms (e.g.,
economic loss) and representational harms (e.g., stereotypes, erasure). While
distributive equality helps address allocative harms, it fails to explain why
representational harms are wrong -- why it is wrong for ML systems to reinforce
social hierarchies that stratify people into superior and inferior groups --
and why ML systems should aim to foster a society where people relate as equals
(i.e., relational equality). To address these limitations, the paper proposes a
multifaceted egalitarian framework for ML fairness that integrates both
distributive and relational equality. Drawing on critical social and political
philosophy, this framework offers a more comprehensive ethical foundation for
tackling the full spectrum of harms perpetuated by ML systems. The paper also
outlines practical pathways for implementing the framework across the ML
pipeline.

</details>


### [376] [Revisiting LoRA through the Lens of Parameter Redundancy: Spectral Encoding Helps](https://arxiv.org/abs/2506.16787)
*Jiashun Cheng,Aochuan Chen,Nuo Chen,Ziqi Gao,Yuhan Li,Jia Li,Fugee Tsung*

Main category: cs.LG

TL;DR: SeLoRA通过谱编码重新参数化LoRA，减少冗余参数，提升效率与性能。


<details>
  <summary>Details</summary>
Motivation: LoRA在微调大型基础模型时存在参数冗余问题，限制了其容量和效率。

Method: 提出SeLoRA，利用谱基的鲁棒表达能力，从稀疏谱子空间重新参数化LoRA。

Result: SeLoRA以更少参数实现更高效率，并在多项下游任务中超越基线。

Conclusion: SeLoRA是一个可扩展的即插即用框架，显著提升LoRA性能。

Abstract: Low-Rank Adaptation (LoRA) has emerged as a prominent technique for
fine-tuning large foundation models. Despite its successes, the substantial
parameter redundancy, which limits the capacity and efficiency of LoRA, has
been recognized as a bottleneck. In this work, we systematically investigate
the impact of redundancy in fine-tuning LoRA and reveal that reducing density
redundancy does not degrade expressiveness. Based on this insight, we introduce
\underline{S}pectral-\underline{e}ncoding \underline{L}ow-\underline{R}ank
\underline{A}daptation (SeLoRA), which harnesses the robust expressiveness of
spectral bases to re-parameterize LoRA from a sparse spectral subspace.
Designed with simplicity, SeLoRA enables seamless integration with various LoRA
variants for performance boosting, serving as a scalable plug-and-play
framework. Extensive experiments substantiate that SeLoRA achieves greater
efficiency with fewer parameters, delivering superior performance enhancements
over strong baselines on various downstream tasks, including commonsense
reasoning, math reasoning, and code generation.

</details>


### [377] [Exploring and Improving Initialization for Deep Graph Neural Networks: A Signal Propagation Perspective](https://arxiv.org/abs/2506.16790)
*Senmiao Wang,Yupeng Chen,Yushun Zhang,Ruoyu Sun,Tian Ding*

Main category: cs.LG

TL;DR: 论文提出了一种名为SPoGInit的初始化方法，通过优化信号传播（SP）的三个关键指标（前向传播、反向传播和图嵌入变化），解决了GNN深度增加时性能下降的问题。


<details>
  <summary>Details</summary>
Motivation: GNN在深度增加时性能下降，现有初始化方法无法同时控制信号传播的三个关键指标。

Method: 提出SPoGInit方法，通过搜索优化三个指标的权重初始化方差来增强信号传播。

Result: 实验表明，SPoGInit在多种任务和架构上优于常用初始化方法，并能在GNN加深时提升性能。

Conclusion: SPoGInit有效解决了GNN深度相关挑战，验证了信号传播分析框架的有效性。

Abstract: Graph Neural Networks (GNNs) often suffer from performance degradation as the
network depth increases. This paper addresses this issue by introducing
initialization methods that enhance signal propagation (SP) within GNNs. We
propose three key metrics for effective SP in GNNs: forward propagation,
backward propagation, and graph embedding variation (GEV). While the first two
metrics derive from classical SP theory, the third is specifically designed for
GNNs. We theoretically demonstrate that a broad range of commonly used
initialization methods for GNNs, which exhibit performance degradation with
increasing depth, fail to control these three metrics simultaneously. To deal
with this limitation, a direct exploitation of the SP analysis--searching for
weight initialization variances that optimize the three metrics--is shown to
significantly enhance the SP in deep GCNs. This approach is called Signal
Propagation on Graph-guided Initialization (SPoGInit). Our experiments
demonstrate that SPoGInit outperforms commonly used initialization methods on
various tasks and architectures. Notably, SPoGInit enables performance
improvements as GNNs deepen, which represents a significant advancement in
addressing depth-related challenges and highlights the validity and
effectiveness of the SP analysis framework.

</details>


### [378] [TabArena: A Living Benchmark for Machine Learning on Tabular Data](https://arxiv.org/abs/2506.16791)
*Nick Erickson,Lennart Purucker,Andrej Tschalzev,David Holzmüller,Prateek Mutalik Desai,and David Salinas,Frank Hutter*

Main category: cs.LG

TL;DR: TabArena是一个持续维护的表格数据基准测试系统，解决了现有基准测试静态化的问题，并提供了公开排行榜、可复现代码和维护协议。


<details>
  <summary>Details</summary>
Motivation: 随着深度学习和基础模型在表格数据中的普及，标准化和可靠的基准测试需求增加，但现有基准测试存在静态化问题。

Method: 通过手动整理代表性数据集和模型，进行大规模基准测试，并组建维护团队，推出TabArena系统。

Result: 结果显示验证方法和超参数配置集成对模型性能有显著影响，梯度提升树仍是强竞争者，深度学习方法在时间充足时表现优异，基础模型在小数据集上表现突出。

Conclusion: TabArena通过公开排行榜和维护协议，推动了表格机器学习的发展，并展示了模型集成的优势。

Abstract: With the growing popularity of deep learning and foundation models for
tabular data, the need for standardized and reliable benchmarks is higher than
ever. However, current benchmarks are static. Their design is not updated even
if flaws are discovered, model versions are updated, or new models are
released. To address this, we introduce TabArena, the first continuously
maintained living tabular benchmarking system. To launch TabArena, we manually
curate a representative collection of datasets and well-implemented models,
conduct a large-scale benchmarking study to initialize a public leaderboard,
and assemble a team of experienced maintainers. Our results highlight the
influence of validation method and ensembling of hyperparameter configurations
to benchmark models at their full potential. While gradient-boosted trees are
still strong contenders on practical tabular datasets, we observe that deep
learning methods have caught up under larger time budgets with ensembling. At
the same time, foundation models excel on smaller datasets. Finally, we show
that ensembles across models advance the state-of-the-art in tabular machine
learning and investigate the contributions of individual models. We launch
TabArena with a public leaderboard, reproducible code, and maintenance
protocols to create a living benchmark available at https://tabarena.ai.

</details>


### [379] [Robust Group Anomaly Detection for Quasi-Periodic Network Time Series](https://arxiv.org/abs/2506.16815)
*Kai Yang,Shaoyu Dou,Pan Luo,Xin Wang,H. Vincent Poor*

Main category: cs.LG

TL;DR: 提出seq2GMM框架，用于识别网络时间序列数据库中的异常时间序列，并通过优化算法高效训练模型，性能优于现有技术。


<details>
  <summary>Details</summary>
Motivation: 解决多变量时间序列中因同步误差导致的周期变化问题，识别异常行为并提供可解释性。

Method: 采用序列到高斯混合模型（seq2GMM）框架，结合基于代理的优化算法进行训练。

Result: 在多个公共数据集上表现优异，显著优于现有异常检测技术。

Conclusion: seq2GMM框架有效识别异常时间序列，理论分析和数值结果验证了其收敛性和性能。

Abstract: Many real-world multivariate time series are collected from a network of
physical objects embedded with software, electronics, and sensors. The
quasi-periodic signals generated by these objects often follow a similar
repetitive and periodic pattern, but have variations in the period, and come in
different lengths caused by timing (synchronization) errors. Given a multitude
of such quasi-periodic time series, can we build machine learning models to
identify those time series that behave differently from the majority of the
observations? In addition, can the models help human experts to understand how
the decision was made? We propose a sequence to Gaussian Mixture Model
(seq2GMM) framework. The overarching goal of this framework is to identify
unusual and interesting time series within a network time series database. We
further develop a surrogate-based optimization algorithm that can efficiently
train the seq2GMM model. Seq2GMM exhibits strong empirical performance on a
plurality of public benchmark datasets, outperforming state-of-the-art anomaly
detection techniques by a significant margin. We also theoretically analyze the
convergence property of the proposed training algorithm and provide numerical
results to substantiate our theoretical claims.

</details>


### [380] [Predicting New Research Directions in Materials Science using Large Language Models and Concept Graphs](https://arxiv.org/abs/2506.16824)
*Thomas Marwitz,Alexander Colsmann,Ben Breitung,Christoph Brabec,Christoph Kirchlechner,Eva Blasco,Gabriel Cadilha Marques,Horst Hahn,Michael Hirtz,Pavel A. Levkin,Yolita M. Eggeler,Tobias Schlöder,Pascal Friederich*

Main category: cs.LG

TL;DR: 利用大语言模型（LLMs）从材料科学摘要中提取主要概念和语义信息，构建概念图并提出新的研究方向。


<details>
  <summary>Details</summary>
Motivation: 由于研究文献数量激增，科学家难以全面阅读，需自动化工具辅助发现潜在研究联系。

Method: 使用LLMs提取概念并构建概念图，训练机器学习模型预测新兴概念组合。

Result: 模型预测性能提升，专家访谈验证其能激发创新研究思路。

Conclusion: LLMs能有效辅助科学家发现未探索的研究方向，提升研究效率。

Abstract: Due to an exponential increase in published research articles, it is
impossible for individual scientists to read all publications, even within
their own research field. In this work, we investigate the use of large
language models (LLMs) for the purpose of extracting the main concepts and
semantic information from scientific abstracts in the domain of materials
science to find links that were not noticed by humans and thus to suggest
inspiring near/mid-term future research directions. We show that LLMs can
extract concepts more efficiently than automated keyword extraction methods to
build a concept graph as an abstraction of the scientific literature. A machine
learning model is trained to predict emerging combinations of concepts, i.e.
new research ideas, based on historical data. We demonstrate that integrating
semantic concept information leads to an increased prediction performance. The
applicability of our model is demonstrated in qualitative interviews with
domain experts based on individualized model suggestions. We show that the
model can inspire materials scientists in their creative thinking process by
predicting innovative combinations of topics that have not yet been
investigated.

</details>


### [381] [FedFitTech: A Baseline in Federated Learning for Fitness Tracking](https://arxiv.org/abs/2506.16840)
*Zeyneddin Oz,Shreyas Korde,Marius Bock,Kristof Van Laerhoven*

Main category: cs.LG

TL;DR: 本文介绍了FedFitTech基线，一种基于联邦学习（FL）的解决方案，用于解决可穿戴健身设备中的数据隐私和通信效率问题。


<details>
  <summary>Details</summary>
Motivation: 传统集中式学习在健身技术（FitTech）中存在隐私、监管和通信效率问题，而联邦学习（FL）提供了一种分散式训练方法。

Method: 提出FedFitTech基线，基于Flower框架，支持分散式模型训练，并引入客户端早期停止策略。

Result: 实验显示，该系统减少了13%的冗余通信，同时识别性能仅下降1%。

Conclusion: FedFitTech为FitTech领域的研究和开发提供了开源基础，支持隐私保护和高效通信。

Abstract: Rapid evolution of sensors and resource-efficient machine learning models
have spurred the widespread adoption of wearable fitness tracking devices.
Equipped with inertial sensors, such devices can continuously capture physical
movements for fitness technology (FitTech), enabling applications from sports
optimization to preventive healthcare. Traditional centralized learning
approaches to detect fitness activities struggle with privacy concerns,
regulatory constraints, and communication inefficiencies. In contrast,
Federated Learning (FL) enables a decentralized model training by communicating
model updates rather than private wearable sensor data. Applying FL to FitTech
presents unique challenges, such as data imbalance, lack of labelled data,
heterogeneous user activity patterns, and trade-offs between personalization
and generalization. To simplify research on FitTech in FL, we present the
FedFitTech baseline, under the Flower framework, which is publicly available
and widely used by both industry and academic researchers. Additionally, to
illustrate its usage, this paper presents a case study that implements a system
based on the FedFitTech baseline, incorporating a client-side early stopping
strategy and comparing the results. For instance, this system allows wearable
devices to optimize the trade-off between capturing common fitness activity
patterns and preserving individuals' nuances, thereby enhancing both the
scalability and efficiency of privacy-aware fitness tracking applications.
Results show that this reduces overall redundant communications by 13 percent,
while maintaining the overall recognition performance at a negligible
recognition cost by 1 percent. Thus, FedFitTech baseline creates a foundation
for a wide range of new research and development opportunities in FitTech, and
it is available as open-source at:
https://github.com/adap/flower/tree/main/baselines/fedfittech

</details>


### [382] [Bandwidth Selectors on Semiparametric Bayesian Networks](https://arxiv.org/abs/2506.16844)
*Victor Alejandre,Concha Bielza,Pedro Larrañaga*

Main category: cs.LG

TL;DR: 论文研究了半参数贝叶斯网络（SPBNs）中带宽选择器对性能的影响，提出并评估了交叉验证和插件选择器，证明其优于传统的正态规则。


<details>
  <summary>Details</summary>
Motivation: 现实数据常偏离正态分布，传统正态规则可能导致次优密度估计和预测性能下降，因此需要更有效的带宽选择方法。

Method: 提出并应用交叉验证和插件选择器，扩展PyBNesian工具包，进行实验分析。

Result: 交叉验证和插件选择器在信息利用和性能上优于正态规则，尤其在高样本量场景中表现更佳。

Conclusion: 研究为SPBNs提供了更优的带宽选择方法，提升了其学习能力和适用性。

Abstract: Semiparametric Bayesian networks (SPBNs) integrate parametric and
non-parametric probabilistic models, offering flexibility in learning complex
data distributions from samples. In particular, kernel density estimators
(KDEs) are employed for the non-parametric component. Under the assumption of
data normality, the normal rule is used to learn the bandwidth matrix for the
KDEs in SPBNs. This matrix is the key hyperparameter that controls the
trade-off between bias and variance. However, real-world data often deviates
from normality, potentially leading to suboptimal density estimation and
reduced predictive performance. This paper first establishes the theoretical
framework for the application of state-of-the-art bandwidth selectors and
subsequently evaluates their impact on SPBN performance. We explore the
approaches of cross-validation and plug-in selectors, assessing their
effectiveness in enhancing the learning capability and applicability of SPBNs.
To support this investigation, we have extended the open-source package
PyBNesian for SPBNs with the additional bandwidth selection techniques and
conducted extensive experimental analyses. Our results demonstrate that the
proposed bandwidth selectors leverage increasing information more effectively
than the normal rule, which, despite its robustness, stagnates with more data.
In particular, unbiased cross-validation generally outperforms the normal rule,
highlighting its advantage in high sample size scenarios.

</details>


### [383] [Soft decision trees for survival analysis](https://arxiv.org/abs/2506.16846)
*Antonio Consoloa,Edoardo Amaldi,Emilio Carrizosa*

Main category: cs.LG

TL;DR: 提出了一种新的软生存树模型（SST），通过非线性优化训练，具有灵活性和可解释性，在15个数据集上表现优于基准模型。


<details>
  <summary>Details</summary>
Motivation: 决策树在生存分析中因其可解释性和建模复杂关系的能力而受欢迎，但传统方法多为启发式，本研究旨在通过全局优化提升性能。

Method: SST采用软分割规则，通过非线性优化训练，支持多种生存函数（参数、半参数、非参数），并利用分解算法优化。

Result: 在15个数据集上，SST在判别和校准指标上优于三种基准生存树模型。

Conclusion: SST结合了灵活性和可解释性，可扩展至群体公平性，是一种有效的生存分析方法。

Abstract: Decision trees are popular in survival analysis for their interpretability
and ability to model complex relationships. Survival trees, which predict the
timing of singular events using censored historical data, are typically built
through heuristic approaches. Recently, there has been growing interest in
globally optimized trees, where the overall tree is trained by minimizing the
error function over all its parameters. We propose a new soft survival tree
model (SST), with a soft splitting rule at each branch node, trained via a
nonlinear optimization formulation amenable to decomposition. Since SSTs
provide for every input vector a specific survival function associated to a
single leaf node, they satisfy the conditional computation property and inherit
the related benefits. SST and the training formulation combine flexibility with
interpretability: any smooth survival function (parametric, semiparametric, or
nonparametric) estimated through maximum likelihood can be used, and each leaf
node of an SST yields a cluster of distinct survival functions which are
associated to the data points routed to it. Numerical experiments on 15
well-known datasets show that SSTs, with parametric and spline-based
semiparametric survival functions, trained using an adaptation of the
node-based decomposition algorithm proposed by Consolo et al. (2024) for soft
regression trees, outperform three benchmark survival trees in terms of four
widely-used discrimination and calibration measures. SSTs can also be extended
to consider group fairness.

</details>


### [384] [Reward-Agnostic Prompt Optimization for Text-to-Image Diffusion Models](https://arxiv.org/abs/2506.16853)
*Semin Kim,Yeonwoo Cha,Jaehoon Yoo,Seunghoon Hong*

Main category: cs.LG

TL;DR: RATTPO是一种通用的测试时提示优化方法，适用于多种奖励场景，无需修改即可提升文本到图像模型的提示效果。


<details>
  <summary>Details</summary>
Motivation: 现有自动化提示工程方法针对特定奖励配置，在新场景中表现不佳，需一种更通用的解决方案。

Method: RATTPO通过迭代搜索优化提示，利用LLM查询和奖励感知反馈信号（“提示”）作为上下文。

Result: RATTPO在多种奖励设置下表现优异，搜索效率高，性能接近需微调的学习基线。

Conclusion: RATTPO是一种高效、通用的提示优化方法，适用于多样化奖励场景。

Abstract: We investigate a general approach for improving user prompts in text-to-image
(T2I) diffusion models by finding prompts that maximize a reward function
specified at test-time. Although diverse reward models are used for evaluating
image generation, existing automated prompt engineering methods typically
target specific reward configurations. Consequently, these specialized designs
exhibit suboptimal performance when applied to new prompt engineering scenarios
involving different reward models. To address this limitation, we introduce
RATTPO (Reward-Agnostic Test-Time Prompt Optimization), a flexible test-time
optimization method applicable across various reward scenarios without
modification. RATTPO iteratively searches for optimized prompts by querying
large language models (LLMs) \textit{without} requiring reward-specific task
descriptions. Instead, it uses the optimization trajectory and a novel
reward-aware feedback signal (termed a "hint") as context. Empirical results
demonstrate the versatility of RATTPO, effectively enhancing user prompts
across diverse reward setups that assess various generation aspects, such as
aesthetics, general human preference, or spatial relationships between objects.
RATTPO surpasses other test-time search baselines in search efficiency, using
up to 3.5 times less inference budget, and, given sufficient inference budget,
achieves performance comparable to learning-based baselines that require
reward-specific fine-tuning. The code is available at
https://github.com/seminkim/RATTPO.

</details>


### [385] [Anomaly Detection in Event-triggered Traffic Time Series via Similarity Learning](https://arxiv.org/abs/2506.16855)
*Shaoyu Dou,Kai Yang,Yang Jiao,Chengbo Qiu,Kui Ren*

Main category: cs.LG

TL;DR: 提出了一种无监督学习框架，用于学习事件触发时间序列之间的相似性，结合了分层多分辨率序列自编码器和GMM，性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 由于事件触发时间序列的复杂动态性，现有相似性度量方法在安全相关任务中表现不佳，需要一种更有效的无监督学习框架。

Method: 结合分层多分辨率序列自编码器和Gaussian Mixture Model (GMM)，学习时间序列的低维表示。

Result: 通过实验验证，该方法在性能上显著优于现有技术。

Conclusion: 该框架为事件触发时间序列的相似性建模提供了系统化方法，具有实际应用潜力。

Abstract: Time series analysis has achieved great success in cyber security such as
intrusion detection and device identification. Learning similarities among
multiple time series is a crucial problem since it serves as the foundation for
downstream analysis. Due to the complex temporal dynamics of the
event-triggered time series, it often remains unclear which similarity metric
is appropriate for security-related tasks, such as anomaly detection and
clustering. The overarching goal of this paper is to develop an unsupervised
learning framework that is capable of learning similarities among a set of
event-triggered time series. From the machine learning vantage point, the
proposed framework harnesses the power of both hierarchical multi-resolution
sequential autoencoders and the Gaussian Mixture Model (GMM) to effectively
learn the low-dimensional representations from the time series. Finally, the
obtained similarity measure can be easily visualized for the explanation. The
proposed framework aspires to offer a stepping stone that gives rise to a
systematic approach to model and learn similarities among a multitude of
event-triggered time series. Through extensive qualitative and quantitative
experiments, it is revealed that the proposed method outperforms
state-of-the-art methods considerably.

</details>


### [386] [Optimal Depth of Neural Networks](https://arxiv.org/abs/2506.16862)
*Qian Qi*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Determining the optimal depth of a neural network is a fundamental yet
challenging problem, typically resolved through resource-intensive
experimentation. This paper introduces a formal theoretical framework to
address this question by recasting the forward pass of a deep network,
specifically a Residual Network (ResNet), as an optimal stopping problem. We
model the layer-by-layer evolution of hidden representations as a sequential
decision process where, at each layer, a choice is made between halting
computation to make a prediction or continuing to a deeper layer for a
potentially more refined representation. This formulation captures the
intrinsic trade-off between accuracy and computational cost. Our primary
theoretical contribution is a proof that, under a plausible condition of
diminishing returns on the residual functions, the expected optimal stopping
depth is provably finite, even in an infinite-horizon setting. We leverage this
insight to propose a novel and practical regularization term, $\mathcal{L}_{\rm
depth}$, that encourages the network to learn representations amenable to
efficient, early exiting. We demonstrate the generality of our framework by
extending it to the Transformer architecture and exploring its connection to
continuous-depth models via free-boundary problems. Empirical validation on
ImageNet confirms that our regularizer successfully induces the theoretically
predicted behavior, leading to significant gains in computational efficiency
without compromising, and in some cases improving, final model accuracy.

</details>


### [387] [The Importance of Being Lazy: Scaling Limits of Continual Learning](https://arxiv.org/abs/2506.16884)
*Jacopo Graldi,Alessandro Breccia,Giulia Lanzillotta,Thomas Hofmann,Lorenzo Noci*

Main category: cs.LG

TL;DR: 本文研究了模型规模和特征学习程度对持续学习的影响，揭示了模型宽度增加仅在减少特征学习时有益，并通过理论分析扩展了对灾难性遗忘的理解。


<details>
  <summary>Details</summary>
Motivation: 尽管已有研究，神经网络在非平稳环境中的学习仍面临挑战，对灾难性遗忘的理解尚不完整。本文旨在系统研究模型规模和特征学习对持续学习的影响。

Method: 通过区分懒惰和丰富训练机制，结合动态平均场理论，分析了无限宽度模型在特征学习机制下的动态特性。

Result: 研究发现，高特征学习仅对高度相似任务有益，且存在一个由任务相似性调制的过渡点，模型从低遗忘的懒惰机制进入高遗忘的丰富机制。

Conclusion: 神经网络在特征学习的临界水平下表现最佳，该水平取决于任务非平稳性，且跨模型规模可迁移。本文为持续学习中规模和特征学习的作用提供了统一视角。

Abstract: Despite recent efforts, neural networks still struggle to learn in
non-stationary environments, and our understanding of catastrophic forgetting
(CF) is far from complete. In this work, we perform a systematic study on the
impact of model scale and the degree of feature learning in continual learning.
We reconcile existing contradictory observations on scale in the literature, by
differentiating between lazy and rich training regimes through a variable
parameterization of the architecture. We show that increasing model width is
only beneficial when it reduces the amount of feature learning, yielding more
laziness. Using the framework of dynamical mean field theory, we then study the
infinite width dynamics of the model in the feature learning regime and
characterize CF, extending prior theoretical results limited to the lazy
regime. We study the intricate relationship between feature learning, task
non-stationarity, and forgetting, finding that high feature learning is only
beneficial with highly similar tasks. We identify a transition modulated by
task similarity where the model exits an effectively lazy regime with low
forgetting to enter a rich regime with significant forgetting. Finally, our
findings reveal that neural networks achieve optimal performance at a critical
level of feature learning, which depends on task non-stationarity and transfers
across model scales. This work provides a unified perspective on the role of
scale and feature learning in continual learning.

</details>


### [388] [From Lab to Factory: Pitfalls and Guidelines for Self-/Unsupervised Defect Detection on Low-Quality Industrial Images](https://arxiv.org/abs/2506.16890)
*Sebastian Hönel,Jonas Nordqvist*

Main category: cs.LG

TL;DR: 论文探讨了在工业产品中检测和定位质量问题的无监督方法，分析了现有方法的不足，并提出改进框架。


<details>
  <summary>Details</summary>
Motivation: 传统手动检测成本高且易出错，机器学习有潜力替代，但现有方法在低质量数据和实际场景中表现不佳。

Method: 评估两种先进模型，识别生产数据中的质量问题，并提出改进框架。

Result: 模型能识别表面异常，但需注意鲁棒性和不变性问题。

Conclusion: 提供实践指南，改进似然方法的常见缺陷，并提出更适合实际场景的风险评估框架。

Abstract: The detection and localization of quality-related problems in industrially
mass-produced products has historically relied on manual inspection, which is
costly and error-prone. Machine learning has the potential to replace manual
handling. As such, the desire is to facilitate an unsupervised (or
self-supervised) approach, as it is often impossible to specify all conceivable
defects ahead of time. A plethora of prior works have demonstrated the aptitude
of common reconstruction-, embedding-, and synthesis-based methods in
laboratory settings. However, in practice, we observe that most methods do not
handle low data quality well or exude low robustness in unfavorable, but
typical real-world settings. For practitioners it may be very difficult to
identify the actual underlying problem when such methods underperform. Worse,
often-reported metrics (e.g., AUROC) are rarely suitable in practice and may
give misleading results. In our setting, we attempt to identify subtle
anomalies on the surface of blasted forged metal parts, using rather
low-quality RGB imagery only, which is a common industrial setting. We
specifically evaluate two types of state-of-the-art models that allow us to
identify and improve quality issues in production data, without having to
obtain new data. Our contribution is to provide guardrails for practitioners
that allow them to identify problems related to, e.g., (lack of) robustness or
invariance, in either the chosen model or the data reliably in similar
scenarios. Furthermore, we exemplify common pitfalls in and shortcomings of
likelihood-based approaches and outline a framework for proper empirical risk
estimation that is more suitable for real-world scenarios.

</details>


### [389] [A deep learning and machine learning approach to predict neonatal death in the context of São Paulo](https://arxiv.org/abs/2506.16929)
*Mohon Raihan,Plabon Kumar Saha,Rajan Das Gupta,A Z M Tahmidul Kabir,Afia Anjum Tamanna,Md. Harun-Ur-Rashid,Adnan Bin Abdus Salam,Md Tanvir Anjum,A Z M Ahteshamul Kabir*

Main category: cs.LG

TL;DR: 使用机器学习和深度学习技术预测新生儿死亡风险，LSTM模型准确率最高（99%）。


<details>
  <summary>Details</summary>
Motivation: 新生儿死亡率高，早期预测可降低风险。

Method: 使用逻辑回归、KNN、随机森林、XGBoost、CNN和LSTM等技术分析140万新生儿数据。

Result: XGBoost和随机森林准确率94%，LSTM达99%。

Conclusion: LSTM是最适合预测新生儿风险的模型。

Abstract: Neonatal death is still a concerning reality for underdeveloped and even some
developed countries. Worldwide data indicate that 26.693 babies out of 1,000
births die, according to Macro Trades. To reduce this number, early prediction
of endangered babies is crucial. Such prediction enables the opportunity to
take ample care of the child and mother so that early child death can be
avoided. In this context, machine learning was used to determine whether a
newborn baby is at risk. To train the predictive model, historical data of 1.4
million newborns was used. Machine learning and deep learning techniques such
as logical regression, K-nearest neighbor, random forest classifier, extreme
gradient boosting (XGBoost), convolutional neural network, and long short-term
memory (LSTM) were implemented using the dataset to identify the most accurate
model for predicting neonatal mortality. Among the machine learning algorithms,
XGBoost and random forest classifier achieved the best accuracy with 94%, while
among the deep learning models, LSTM delivered the highest accuracy with 99%.
Therefore, using LSTM appears to be the most suitable approach to predict
whether precautionary measures for a child are necessary.

</details>


### [390] [RocketStack: A level-aware deep recursive ensemble learning framework with exploratory feature fusion and model pruning dynamics](https://arxiv.org/abs/2506.16965)
*Çağatay Demirel*

Main category: cs.LG

TL;DR: RocketStack是一种递归集成框架，通过逐层修剪弱学习器和周期性特征压缩，实现了深度堆叠，显著提升了性能并降低了计算负担。


<details>
  <summary>Details</summary>
Motivation: 当前深度堆叠集成学习因模型复杂性、特征冗余和计算负担而受限，RocketStack旨在解决这些问题。

Method: 引入RocketStack框架，逐层修剪弱学习器，添加高斯噪声防止过早饱和，并探索多种特征压缩方法（如注意力选择、SFE过滤器和自动编码器）。

Result: 在33个数据集上测试，深度堆叠显著提升了准确率，最高达到97.08%（二分类）和98.60%（多分类），同时减少了运行时间和特征维度。

Conclusion: RocketStack通过修剪和压缩实现了深度递归集成，证明了轻度随机化和周期性压缩的有效性。

Abstract: Ensemble learning remains a cornerstone of machine learning, with stacking
used to integrate predictions from multiple base learners through a meta-model.
However, deep stacking remains rare, as most designs prioritize horizontal
diversity over recursive depth due to model complexity, feature redundancy, and
computational burden. To address these challenges, RocketStack, a level-aware
recursive ensemble framework, is introduced and explored up to ten stacking
levels, extending beyond prior architectures. The framework incrementally
prunes weaker learners at each level, enabling deeper stacking without
excessive complexity. To mitigate early performance saturation, mild Gaussian
noise is added to out-of-fold (OOF) scores before pruning, and compared against
strict OOF pruning. Further both per-level and periodic feature compressions
are explored using attention-based selection, Simple, Fast, Efficient (SFE)
filter, and autoencoders. Across 33 datasets (23 binary, 10 multi-class),
linear-trend tests confirmed rising accuracy with depth in most variants, and
the top performing meta-model at each level increasingly outperformed the
strongest standalone ensemble. In the binary subset, periodic SFE with mild
OOF-score randomization reached 97.08% at level 10, 5.14% above the
strict-pruning configuration and cut runtime by 10.5% relative to no
compression. In the multi-class subset, periodic attention selection reached
98.60% at level 10, exceeding the strongest baseline by 6.11%, while reducing
runtime by 56.1% and feature dimensionality by 74% compared to no compression.
These findings highlight mild randomization as an effective regularizer and
periodic compression as a stabilizer. Echoing the design of multistage rockets
in aerospace (prune, compress, propel) RocketStack achieves deep recursive
ensembling with tractable complexity.

</details>


### [391] [Robust Reinforcement Learning for Discrete Compositional Generation via General Soft Operators](https://arxiv.org/abs/2506.17007)
*Marco Jiralerspong,Esther Derman,Danilo Vucetic,Nikolay Malkin,Bilun Sun,Tianyu Zhang,Pierre-Luc Bacon,Gauthier Gidel*

Main category: cs.LG

TL;DR: 论文提出了一种鲁棒强化学习方法，通过统一操作符解决代理奖励函数的不确定性，从而在大型搜索空间中生成更高质量和多样化的候选对象。


<details>
  <summary>Details</summary>
Motivation: 科学发现中，从大量组合对象中筛选出有潜力的候选对象是一个主要瓶颈。现有方法依赖专家知识或强化学习，但代理奖励函数的不确定性导致结果不理想。

Method: 采用鲁棒强化学习方法，引入一个统一操作符，针对代理奖励函数的不确定性进行优化，生成更集中的采样分布。

Result: 新算法在合成和真实任务中均能识别出更高质量且多样化的候选对象。

Conclusion: 该研究为离散组合生成任务提供了新的灵活视角，并通过开源代码实现。

Abstract: A major bottleneck in scientific discovery involves narrowing a large
combinatorial set of objects, such as proteins or molecules, to a small set of
promising candidates. While this process largely relies on expert knowledge,
recent methods leverage reinforcement learning (RL) to enhance this filtering.
They achieve this by estimating proxy reward functions from available datasets
and using regularization to generate more diverse candidates. These reward
functions are inherently uncertain, raising a particularly salient challenge
for scientific discovery. In this work, we show that existing methods, often
framed as sampling proportional to a reward function, are inadequate and yield
suboptimal candidates, especially in large search spaces. To remedy this issue,
we take a robust RL approach and introduce a unified operator that seeks
robustness to the uncertainty of the proxy reward function. This general
operator targets peakier sampling distributions while encompassing known soft
RL operators. It also leads us to a novel algorithm that identifies
higher-quality, diverse candidates in both synthetic and real-world tasks.
Ultimately, our work offers a new, flexible perspective on discrete
compositional generation tasks. Code: https://github.com/marcojira/tgm.

</details>


### [392] [Scalable and Reliable Multi-agent Reinforcement Learning for Traffic Assignment](https://arxiv.org/abs/2506.17029)
*Leizhen Wang,Peibo Duan,Cheng Lyu,Zewen Wang,Zhiqiang He,Nan Zheng,Zhenliang Ma*

Main category: cs.LG

TL;DR: 论文提出了一种新的多智能体强化学习框架MARL-OD-DA，用于解决大规模交通分配问题，通过将智能体定义为OD对路由器而非个体旅行者，显著提升了可扩展性和可靠性。


<details>
  <summary>Details</summary>
Motivation: 随着大都市的发展和旅行需求的增加，传统交通分配方法难以满足需求，而现有的多智能体强化学习方法在可扩展性和可靠性方面存在不足。

Method: MARL-OD-DA框架将智能体定义为OD对路由器，设计了基于Dirichlet的动作空间和局部相对差距的奖励函数，以提升可靠性和收敛效率。

Result: 实验表明，MARL-OD-DA在中等规模网络中表现优异，SiouxFalls网络中相对差距比传统方法低94.99%。

Conclusion: MARL-OD-DA为解决大规模交通分配问题提供了高效且可靠的新方法。

Abstract: The evolution of metropolitan cities and the increase in travel demands
impose stringent requirements on traffic assignment methods. Multi-agent
reinforcement learning (MARL) approaches outperform traditional methods in
modeling adaptive routing behavior without requiring explicit system dynamics,
which is beneficial for real-world deployment. However, MARL frameworks face
challenges in scalability and reliability when managing extensive networks with
substantial travel demand, which limiting their practical applicability in
solving large-scale traffic assignment problems. To address these challenges,
this study introduces MARL-OD-DA, a new MARL framework for the traffic
assignment problem, which redefines agents as origin-destination (OD) pair
routers rather than individual travelers, significantly enhancing scalability.
Additionally, a Dirichlet-based action space with action pruning and a reward
function based on the local relative gap are designed to enhance solution
reliability and improve convergence efficiency. Experiments demonstrate that
the proposed MARL framework effectively handles medium-sized networks with
extensive and varied city-level OD demand, surpassing existing MARL methods.
When implemented in the SiouxFalls network, MARL-OD-DA achieves better
assignment solutions in 10 steps, with a relative gap that is 94.99% lower than
that of conventional methods.

</details>


### [393] [Critical Appraisal of Fairness Metrics in Clinical Predictive AI](https://arxiv.org/abs/2506.17035)
*João Matos,Ben Van Calster,Leo Anthony Celi,Paula Dhiman,Judy Wawira Gichoya,Richard D. Riley,Chris Russell,Sara Khalid,Gary S. Collins*

Main category: cs.LG

TL;DR: 该论文探讨了临床预测AI中的公平性度量问题，发现现有度量标准碎片化且缺乏临床验证，未来需优先发展具有临床意义的度量。


<details>
  <summary>Details</summary>
Motivation: 预测AI可改善临床实践和患者结果，但若公平性问题未解决，可能加剧偏见。目前公平性定义尚不明确。

Method: 通过范围综述，筛选820篇记录，纳入41项研究，提取62种公平性度量，并按性能依赖性、模型输出级别和基础性能度量分类。

Result: 发现公平性度量碎片化，临床验证有限，过度依赖阈值依赖性度量。仅18种度量专为医疗设计，其中仅1种涉及临床效用。

Conclusion: 公平性定义和量化存在概念挑战，未来需关注不确定性量化、交叉性和实际应用性，优先发展临床相关度量。

Abstract: Predictive artificial intelligence (AI) offers an opportunity to improve
clinical practice and patient outcomes, but risks perpetuating biases if
fairness is inadequately addressed. However, the definition of "fairness"
remains unclear. We conducted a scoping review to identify and critically
appraise fairness metrics for clinical predictive AI. We defined a "fairness
metric" as a measure quantifying whether a model discriminates (societally)
against individuals or groups defined by sensitive attributes. We searched five
databases (2014-2024), screening 820 records, to include 41 studies, and
extracted 62 fairness metrics. Metrics were classified by
performance-dependency, model output level, and base performance metric,
revealing a fragmented landscape with limited clinical validation and
overreliance on threshold-dependent measures. Eighteen metrics were explicitly
developed for healthcare, including only one clinical utility metric. Our
findings highlight conceptual challenges in defining and quantifying fairness
and identify gaps in uncertainty quantification, intersectionality, and
real-world applicability. Future work should prioritise clinically meaningful
metrics.

</details>


### [394] [LSCD: Lomb-Scargle Conditioned Diffusion for Time series Imputation](https://arxiv.org/abs/2506.17039)
*Elizabeth Fons,Alejandro Sztrajman,Yousef El-Laham,Luciana Ferrer,Svitlana Vyetrenko,Manuela Veloso*

Main category: cs.LG

TL;DR: 论文提出了一种可微分的Lomb-Scargle层，用于计算不规则采样数据的功率谱，并将其集成到基于分数的扩散模型（LSCD）中，以更准确地恢复缺失数据。


<details>
  <summary>Details</summary>
Motivation: 解决不规则采样或缺失数据时间序列在机器学习中的挑战，避免传统FFT方法因均匀采样假设而导致的频谱失真。

Method: 引入可微分的Lomb-Scargle层计算功率谱，结合基于分数的扩散模型（LSCD）进行时间序列插补。

Result: 在合成和真实数据集上，LSCD比纯时域基线更准确地恢复缺失数据，并提供一致的频率估计。

Conclusion: 该方法可轻松集成到学习框架中，为涉及不完整或不规则数据的机器学习方法提供频谱指导。

Abstract: Time series with missing or irregularly sampled data are a persistent
challenge in machine learning. Many methods operate on the frequency-domain,
relying on the Fast Fourier Transform (FFT) which assumes uniform sampling,
therefore requiring prior interpolation that can distort the spectra. To
address this limitation, we introduce a differentiable Lomb--Scargle layer that
enables a reliable computation of the power spectrum of irregularly sampled
data. We integrate this layer into a novel score-based diffusion model (LSCD)
for time series imputation conditioned on the entire signal spectrum.
Experiments on synthetic and real-world benchmarks demonstrate that our method
recovers missing data more accurately than purely time-domain baselines, while
simultaneously producing consistent frequency estimates. Crucially, our method
can be easily integrated into learning frameworks, enabling broader adoption of
spectral guidance in machine learning approaches involving incomplete or
irregular data.

</details>


### [395] [MAWIFlow Benchmark: Realistic Flow-Based Evaluation for Network Intrusion Detection](https://arxiv.org/abs/2506.17041)
*Joshua Schraven,Alexander Windmann,Oliver Niggemann*

Main category: cs.LG

TL;DR: MAWIFlow是一个基于MAWILAB v1.1数据集的流量基准，用于评估异常检测方法。通过可重复的预处理流程生成符合CICFlowMeter格式的流量数据，并比较传统机器学习与深度学习模型的性能。结果表明，CNN-BiLSTM模型在时间变化数据上表现更好。


<details>
  <summary>Details</summary>
Motivation: 现有网络入侵检测基准数据集多依赖合成流量，无法反映实际环境中的统计变异和时间漂移。

Method: 提出MAWIFlow基准，通过预处理流程将原始数据转换为流量表示，并比较传统机器学习（如决策树、随机森林）与CNN-BiLSTM模型的性能。

Result: 树基分类器在静态数据上表现良好，但在时间变化数据上性能显著下降；CNN-BiLSTM模型表现更稳定，泛化能力更强。

Conclusion: 合成基准和静态模型存在局限性，应采用具有明确时间结构的真实数据集。所有数据和代码公开以促进透明性和可重复性。

Abstract: Benchmark datasets for network intrusion detection commonly rely on
synthetically generated traffic, which fails to reflect the statistical
variability and temporal drift encountered in operational environments. This
paper introduces MAWIFlow, a flow-based benchmark derived from the MAWILAB v1.1
dataset, designed to enable realistic and reproducible evaluation of anomaly
detection methods. A reproducible preprocessing pipeline is presented that
transforms raw packet captures into flow representations conforming to the
CICFlowMeter format, while preserving MAWILab's original anomaly labels. The
resulting datasets comprise temporally distinct samples from January 2011,
2016, and 2021, drawn from trans-Pacific backbone traffic.
  To establish reference baselines, traditional machine learning methods,
including Decision Trees, Random Forests, XGBoost, and Logistic Regression, are
compared to a deep learning model based on a CNN-BiLSTM architecture. Empirical
results demonstrate that tree-based classifiers perform well on temporally
static data but experience significant performance degradation over time. In
contrast, the CNN-BiLSTM model maintains better performance, thus showing
improved generalization. These findings underscore the limitations of synthetic
benchmarks and static models, and motivate the adoption of realistic datasets
with explicit temporal structure. All datasets, pipeline code, and model
implementations are made publicly available to foster transparency and
reproducibility.

</details>


### [396] [Navigating the Deep: Signature Extraction on Deep Neural Networks](https://arxiv.org/abs/2506.17047)
*Haolin Liu,Adrien Siproudhis,Samuel Experton,Peter Lorenz,Christina Boura,Thomas Peyrin*

Main category: cs.LG

TL;DR: 本文改进了神经网络模型提取中的签名提取方法，解决了Carlini等人方法的局限性，显著提升了提取深度和准确性。


<details>
  <summary>Details</summary>
Motivation: 现有签名提取方法（如Carlini等人）存在局限性，无法有效提取深层网络参数，亟需改进。

Method: 通过系统性分析Carlini方法的不足（如秩不足和噪声传播），提出高效算法解决方案。

Result: 实验验证表明，新方法能提取更深层网络（如CIFAR-10数据集的8层网络），准确率超过95%。

Conclusion: 新方法为攻击更复杂神经网络架构提供了关键技术支持。

Abstract: Neural network model extraction has emerged in recent years as an important
security concern, as adversaries attempt to recover a network's parameters via
black-box queries. A key step in this process is signature extraction, which
aims to recover the absolute values of the network's weights layer by layer.
Prior work, notably by Carlini et al. (2020), introduced a technique inspired
by differential cryptanalysis to extract neural network parameters. However,
their method suffers from several limitations that restrict its applicability
to networks with a few layers only. Later works focused on improving sign
extraction, but largely relied on the assumption that signature extraction
itself was feasible.
  In this work, we revisit and refine the signature extraction process by
systematically identifying and addressing for the first time critical
limitations of Carlini et al.'s signature extraction method. These limitations
include rank deficiency and noise propagation from deeper layers. To overcome
these challenges, we propose efficient algorithmic solutions for each of the
identified issues, greatly improving the efficiency of signature extraction.
Our approach permits the extraction of much deeper networks than was previously
possible. We validate our method through extensive experiments on ReLU-based
neural networks, demonstrating significant improvements in extraction depth and
accuracy. For instance, our extracted network matches the target network on at
least 95% of the input space for each of the eight layers of a neural network
trained on the CIFAR-10 dataset, while previous works could barely extract the
first three layers. Our results represent a crucial step toward practical
attacks on larger and more complex neural network architectures.

</details>


### [397] [Flow-Based Non-stationary Temporal Regime Causal Structure Learning](https://arxiv.org/abs/2506.17065)
*Abdellah Rahmani,Pascal Frossard*

Main category: cs.LG

TL;DR: FANTOM是一个用于非平稳、非高斯和异方差噪声的多变量时间序列因果发现框架，能够同时推断多个因果结构的区域及其边界。


<details>
  <summary>Details</summary>
Motivation: 多变量时间序列中的因果关系分析在金融和神经科学等领域至关重要，但现有方法无法处理非平稳性和复杂噪声分布。

Method: FANTOM采用贝叶斯期望最大化算法，最大化数据对数似然的证据下界，同时推断区域数量、边界及其因果图。

Result: 理论证明FANTOM在平稳和非平稳设置下均可识别，实验表明其在合成和真实数据上优于现有方法。

Conclusion: FANTOM为复杂时间序列因果发现提供了有效解决方案，解决了非平稳性和噪声分布的挑战。

Abstract: Understanding causal relationships in multivariate time series is crucial in
many scenarios, such as those dealing with financial or neurological data. Many
such time series exhibit multiple regimes, i.e., consecutive temporal segments
with a priori unknown boundaries, with each regime having its own causal
structure. Inferring causal dependencies and regime shifts is critical for
analyzing the underlying processes. However, causal structure learning in this
setting is challenging due to (1) non stationarity, i.e., each regime can have
its own causal graph and mixing function, and (2) complex noise distributions,
which may be non Gaussian or heteroscedastic. Existing causal discovery
approaches cannot address these challenges, since generally assume stationarity
or Gaussian noise with constant variance. Hence, we introduce FANTOM, a unified
framework for causal discovery that handles non stationary processes along with
non Gaussian and heteroscedastic noises. FANTOM simultaneously infers the
number of regimes and their corresponding indices and learns each regime's
Directed Acyclic Graph. It uses a Bayesian Expectation Maximization algorithm
that maximizes the evidence lower bound of the data log likelihood. On the
theoretical side, we prove, under mild assumptions, that temporal
heteroscedastic causal models, introduced in FANTOM's formulation, are
identifiable in both stationary and non stationary settings. In addition,
extensive experiments on synthetic and real data show that FANTOM outperforms
existing methods.

</details>


### [398] [Identifiability of Deep Polynomial Neural Networks](https://arxiv.org/abs/2506.17093)
*Konstantin Usevich,Clara Dérand,Ricardo Borsoi,Marianne Clausel*

Main category: cs.LG

TL;DR: 本文全面分析了深度多项式神经网络（PNNs）的可识别性，揭示了激活度与层宽度之间的复杂关系，并解决了关于PNN神经变种预期维度的开放猜想。


<details>
  <summary>Details</summary>
Motivation: 多项式神经网络（PNNs）具有丰富的代数和几何结构，但其可识别性（确保可解释性的关键属性）尚未被充分理解。

Method: 通过将深度PNNs与低秩张量分解及Kruskal型唯一性定理联系起来，提出了构造性证明。

Result: 结果表明，在温和条件下，层宽度非递增的架构通常是可识别的，而解码器宽度增长不过快的编码器-解码器网络也是可识别的。

Conclusion: 研究不仅提供了由架构决定的通用条件，还给出了依赖于网络参数的有效条件，并解决了PNN神经变种预期维度的开放猜想。

Abstract: Polynomial Neural Networks (PNNs) possess a rich algebraic and geometric
structure. However, their identifiability -- a key property for ensuring
interpretability -- remains poorly understood. In this work, we present a
comprehensive analysis of the identifiability of deep PNNs, including
architectures with and without bias terms. Our results reveal an intricate
interplay between activation degrees and layer widths in achieving
identifiability. As special cases, we show that architectures with
non-increasing layer widths are generically identifiable under mild conditions,
while encoder-decoder networks are identifiable when the decoder widths do not
grow too rapidly. Our proofs are constructive and center on a connection
between deep PNNs and low-rank tensor decompositions, and Kruskal-type
uniqueness theorems. This yields both generic conditions determined by the
architecture, and effective conditions that depend on the network's parameters.
We also settle an open conjecture on the expected dimension of PNN's
neurovarieties, and provide new bounds on the activation degrees required for
it to reach its maximum.

</details>


### [399] [TransDreamerV3: Implanting Transformer In DreamerV3](https://arxiv.org/abs/2506.17103)
*Shruti Sadanand Dongare,Amun Kharel,Jonathan Samuel,Xiaona Zhou*

Main category: cs.LG

TL;DR: TransDreamerV3通过集成Transformer编码器改进DreamerV3，提升复杂环境中的记忆与决策能力，在Atari-Freeway和Crafter任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: 增强DreamerV3的记忆与决策能力，以应对复杂环境。

Method: 集成Transformer编码器到DreamerV3架构中。

Result: 在Atari-Freeway和Crafter任务中性能提升，但在Minecraft任务中存在不足。

Conclusion: TransDreamerV3展示了基于世界模型的强化学习进展，尤其通过Transformer架构。

Abstract: This paper introduces TransDreamerV3, a reinforcement learning model that
enhances the DreamerV3 architecture by integrating a transformer encoder. The
model is designed to improve memory and decision-making capabilities in complex
environments. We conducted experiments on Atari-Boxing, Atari-Freeway,
Atari-Pong, and Crafter tasks, where TransDreamerV3 demonstrated improved
performance over DreamerV3, particularly in the Atari-Freeway and Crafter
tasks. While issues in the Minecraft task and limited training across all tasks
were noted, TransDreamerV3 displays advancement in world model-based
reinforcement learning, leveraging transformer architectures.

</details>


### [400] [Rapid and Continuous Trust Evaluation for Effective Task Collaboration Through Siamese Model](https://arxiv.org/abs/2506.17128)
*Botao Zhu,Xianbin Wang*

Main category: cs.LG

TL;DR: 论文提出了一种基于Siamese模型的快速连续信任评估框架（SRCTE），用于协作系统中动态评估合作者的可信度。


<details>
  <summary>Details</summary>
Motivation: 协作系统中动态变化的资源和复杂环境使得快速连续评估合作者可信度成为挑战。

Method: 使用属性控制流图（ACFG）表示合作者的资源属性和历史数据，通过Siamese模型学习ACFG的语义并生成嵌入，计算相似度以评估信任值。

Result: 实验表明SRCTE能快速收敛且异常信任检测率高。

Conclusion: SRCTE框架有效解决了协作系统中动态信任评估的挑战。

Abstract: Trust is emerging as an effective tool to ensure the successful completion of
collaborative tasks within collaborative systems. However, rapidly and
continuously evaluating the trustworthiness of collaborators during task
execution is a significant challenge due to distributed devices, complex
operational environments, and dynamically changing resources. To tackle this
challenge, this paper proposes a Siamese-enabled rapid and continuous trust
evaluation framework (SRCTE) to facilitate effective task collaboration. First,
the communication and computing resource attributes of the collaborator in a
trusted state, along with historical collaboration data, are collected and
represented using an attributed control flow graph (ACFG) that captures
trust-related semantic information and serves as a reference for comparison
with data collected during task execution. At each time slot of task execution,
the collaborator's communication and computing resource attributes, as well as
task completion effectiveness, are collected in real time and represented with
an ACFG to convey their trust-related semantic information. A Siamese model,
consisting of two shared-parameter Structure2vec networks, is then employed to
learn the deep semantics of each pair of ACFGs and generate their embeddings.
Finally, the similarity between the embeddings of each pair of ACFGs is
calculated to determine the collaborator's trust value at each time slot. A
real system is built using two Dell EMC 5200 servers and a Google Pixel 8 to
test the effectiveness of the proposed SRCTE framework. Experimental results
demonstrate that SRCTE converges rapidly with only a small amount of data and
achieves a high anomaly trust detection rate compared to the baseline
algorithm.

</details>


### [401] [Consistent Sampling and Simulation: Molecular Dynamics with Energy-Based Diffusion Models](https://arxiv.org/abs/2506.17139)
*Michael Plainer,Hao Wu,Leon Klein,Stephan Günnemann,Frank Noé*

Main category: cs.LG

TL;DR: 扩散模型在生物化学等领域表现优异，但用于分子动力学模拟时存在不一致性。作者提出一种基于能量的扩散模型，通过Fokker-Planck正则化提升一致性，并在多个系统中验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 扩散模型在生成平衡分子构象和相关力方面表现出色，但在模拟中与经典扩散推断结果不一致，尤其是在小时间步长下不符合Fokker-Planck方程。

Method: 提出一种基于能量的扩散模型，引入Fokker-Planck正则化项以确保一致性。

Result: 在玩具系统、丙氨酸二肽等实验中验证了模型的有效性，并开发了一种支持模拟的先进二肽Boltzmann模拟器。

Conclusion: 通过Fokker-Planck正则化，扩散模型在模拟中的一致性得到显著提升，为高效采样提供了新方法。

Abstract: Diffusion models have recently gained significant attention due to their
effectiveness in various scientific domains, including biochemistry. When
trained on equilibrium molecular distributions, diffusion models provide both:
a generative procedure to sample equilibrium conformations and associated
forces derived from the model's scores. However, using the forces for
coarse-grained molecular dynamics simulations uncovers inconsistencies in the
samples generated via classical diffusion inference and simulation, despite
both originating from the same model. Particularly at the small diffusion
timesteps required for simulations, diffusion models fail to satisfy the
Fokker-Planck equation, which governs how the score should evolve over time. We
interpret this deviation as an indication of the observed inconsistencies and
propose an energy-based diffusion model with a Fokker-Planck-derived
regularization term enforcing consistency. We demonstrate the effectiveness of
our approach on toy systems, alanine dipeptide, and introduce a
state-of-the-art transferable Boltzmann emulator for dipeptides that supports
simulation and demonstrates enhanced consistency and efficient sampling.

</details>


### [402] [Sparse-Reg: Improving Sample Complexity in Offline Reinforcement Learning using Sparsity](https://arxiv.org/abs/2506.17155)
*Samin Yeasar Arnob,Scott Fujimoto,Doina Precup*

Main category: cs.LG

TL;DR: 论文研究了小数据集在离线强化学习中的应用，提出了一种基于稀疏性的正则化方法Sparse-Reg，以解决过拟合问题。


<details>
  <summary>Details</summary>
Motivation: 许多离线强化学习应用依赖于小数据集，但现有算法在小数据集上容易过拟合，导致性能下降。

Method: 引入Sparse-Reg正则化技术，通过稀疏性减少过拟合。

Result: Sparse-Reg在有限数据环境下表现优异，优于现有基线方法。

Conclusion: Sparse-Reg能有效解决小数据集中的过拟合问题，提升离线强化学习性能。

Abstract: In this paper, we investigate the use of small datasets in the context of
offline reinforcement learning (RL). While many common offline RL benchmarks
employ datasets with over a million data points, many offline RL applications
rely on considerably smaller datasets. We show that offline RL algorithms can
overfit on small datasets, resulting in poor performance. To address this
challenge, we introduce "Sparse-Reg": a regularization technique based on
sparsity to mitigate overfitting in offline reinforcement learning, enabling
effective learning in limited data settings and outperforming state-of-the-art
baselines in continuous control.

</details>


### [403] [Deep generative models as the probability transformation functions](https://arxiv.org/abs/2506.17171)
*Vitalii Bondar,Vira Babenko,Roman Trembovetskyi,Yurii Korobeinyk,Viktoriya Dzyuba*

Main category: cs.LG

TL;DR: 论文提出了一种统一的理论视角，将深度生成模型视为概率变换函数，揭示了不同生成模型之间的共性。


<details>
  <summary>Details</summary>
Motivation: 尽管不同类型的生成模型在架构和训练方法上存在明显差异，但本文旨在揭示它们本质上都是通过变换简单预定义分布来生成复杂目标数据分布。

Method: 通过理论分析，将自编码器、自回归模型、生成对抗网络、标准化流、扩散模型和流匹配等模型统一为概率变换函数。

Result: 这种统一视角促进了不同模型架构之间方法改进的迁移，并为开发通用理论方法奠定了基础。

Conclusion: 这一理论框架有望推动更高效、更有效的生成建模技术的发展。

Abstract: This paper introduces a unified theoretical perspective that views deep
generative models as probability transformation functions. Despite the apparent
differences in architecture and training methodologies among various types of
generative models - autoencoders, autoregressive models, generative adversarial
networks, normalizing flows, diffusion models, and flow matching - we
demonstrate that they all fundamentally operate by transforming simple
predefined distributions into complex target data distributions. This unifying
perspective facilitates the transfer of methodological improvements between
model architectures and provides a foundation for developing universal
theoretical approaches, potentially leading to more efficient and effective
generative modeling techniques.

</details>


### [404] [Variational Learning of Disentangled Representations](https://arxiv.org/abs/2506.17182)
*Yuli Slavutsky,Ozgur Beker,David Blei,Bianca Dumitrascu*

Main category: cs.LG

TL;DR: DISCoVeR是一种新的变分框架，用于在多条件设置中学习解耦表示，通过双潜在架构和最大-最小目标实现共享和特定因素的分离。


<details>
  <summary>Details</summary>
Motivation: 在生物医学数据分析等领域，解耦表示对于泛化到新条件至关重要，但现有方法存在潜在表示泄漏问题。

Method: DISCoVeR采用双潜在架构、并行重建和最大-最小目标，无需手工先验即可实现干净分离。

Result: DISCoVeR在合成数据集、自然图像和单细胞RNA-seq数据上实现了更好的解耦效果。

Conclusion: DISCoVeR为多条件设置中的解耦表示学习提供了一种原则性方法。

Abstract: Disentangled representations enable models to separate factors of variation
that are shared across experimental conditions from those that are
condition-specific. This separation is essential in domains such as biomedical
data analysis, where generalization to new treatments, patients, or species
depends on isolating stable biological signals from context-dependent effects.
While extensions of the variational autoencoder (VAE) framework have been
proposed to address this problem, they frequently suffer from leakage between
latent representations, limiting their ability to generalize to unseen
conditions. Here, we introduce DISCoVeR, a new variational framework that
explicitly separates condition-invariant and condition-specific factors.
DISCoVeR integrates three key components: (i) a dual-latent architecture that
models shared and specific factors separately; (ii) two parallel
reconstructions that ensure both representations remain informative; and (iii)
a novel max-min objective that encourages clean separation without relying on
handcrafted priors, while making only minimal assumptions. Theoretically, we
show that this objective maximizes data likelihood while promoting
disentanglement, and that it admits a unique equilibrium. Empirically, we
demonstrate that DISCoVeR achieves improved disentanglement on synthetic
datasets, natural images, and single-cell RNA-seq data. Together, these results
establish DISCoVeR as a principled approach for learning disentangled
representations in multi-condition settings.

</details>


### [405] [Optimal Implicit Bias in Linear Regression](https://arxiv.org/abs/2506.17187)
*Kanumuri Nithin Varma,Babak Hassibi*

Main category: cs.LG

TL;DR: 本文研究了过参数化学习问题中优化算法的隐式偏差对泛化性能的影响，通过凸函数最小化分析，找到了最优隐式偏差及其条件。


<details>
  <summary>Details</summary>
Motivation: 在过参数化学习中，训练损失有无数全局最优解，但泛化性能各异。本文旨在确定哪种隐式偏差能带来最佳泛化性能。

Method: 采用凸函数/势能最小化的渐近分析，针对非各向同性高斯数据的过参数化线性回归，研究插值器的泛化性能。

Result: 获得了泛化误差的紧下界，并找到了在某些条件下（涉及高斯卷积与先验分布的对数凹性）达到该下界的最优凸隐式偏差。

Conclusion: 本文确定了过参数化线性回归中最佳泛化性能的隐式偏差，并提供了相关理论条件。

Abstract: Most modern learning problems are over-parameterized, where the number of
learnable parameters is much greater than the number of training data points.
In this over-parameterized regime, the training loss typically has infinitely
many global optima that completely interpolate the data with varying
generalization performance. The particular global optimum we converge to
depends on the implicit bias of the optimization algorithm. The question we
address in this paper is, ``What is the implicit bias that leads to the best
generalization performance?". To find the optimal implicit bias, we provide a
precise asymptotic analysis of the generalization performance of interpolators
obtained from the minimization of convex functions/potentials for
over-parameterized linear regression with non-isotropic Gaussian data. In
particular, we obtain a tight lower bound on the best generalization error
possible among this class of interpolators in terms of the
over-parameterization ratio, the variance of the noise in the labels, the
eigenspectrum of the data covariance, and the underlying distribution of the
parameter to be estimated. Finally, we find the optimal convex implicit bias
that achieves this lower bound under certain sufficient conditions involving
the log-concavity of the distribution of a Gaussian convolved with the prior of
the true underlying parameter.

</details>


### [406] [Network Sparsity Unlocks the Scaling Potential of Deep Reinforcement Learning](https://arxiv.org/abs/2506.17204)
*Guozheng Ma,Lu Li,Zilin Wang,Li Shen,Pierre-Luc Bacon,Dacheng Tao*

Main category: cs.LG

TL;DR: 通过静态网络稀疏化（如一次性随机剪枝）提升深度强化学习的扩展性，优于密集网络。


<details>
  <summary>Details</summary>
Motivation: 深度强化学习模型在扩展时易出现网络训练问题，现有方法复杂且效果有限。

Method: 采用一次性随机剪枝，训练前随机移除部分网络权重。

Result: 稀疏网络在参数效率和优化稳定性上优于密集网络，适用于多种场景。

Conclusion: 网络稀疏化是提升深度强化学习扩展性的简单有效方法。

Abstract: Effectively scaling up deep reinforcement learning models has proven
notoriously difficult due to network pathologies during training, motivating
various targeted interventions such as periodic reset and architectural
advances such as layer normalization. Instead of pursuing more complex
modifications, we show that introducing static network sparsity alone can
unlock further scaling potential beyond their dense counterparts with
state-of-the-art architectures. This is achieved through simple one-shot random
pruning, where a predetermined percentage of network weights are randomly
removed once before training. Our analysis reveals that, in contrast to naively
scaling up dense DRL networks, such sparse networks achieve both higher
parameter efficiency for network expressivity and stronger resistance to
optimization challenges like plasticity loss and gradient interference. We
further extend our evaluation to visual and streaming RL scenarios,
demonstrating the consistent benefits of network sparsity.

</details>


### [407] [BREAD: Branched Rollouts from Expert Anchors Bridge SFT & RL for Reasoning](https://arxiv.org/abs/2506.17211)
*Xuechen Zhang,Zijian Huang,Yingcong Li,Chenshun Ni,Jiasi Chen,Samet Oymak*

Main category: cs.LG

TL;DR: 论文提出BREAD方法，通过结合部分专家指导和分支展开，解决了小语言模型在SFT + RL范式中的学习局限性，显著提升了推理能力。


<details>
  <summary>Details</summary>
Motivation: 小语言模型（SLMs）在复杂推理任务中表现不佳，尤其是在高质量训练数据稀缺或难以学习的情况下。传统的SFT + RL方法存在局限性，如专家轨迹难以表达或模型初始化成功率极低。

Method: 提出BREAD方法，通过部分专家指导和分支展开统一SFT和RL阶段，自适应插入专家提示以完成推理路径，确保每次更新至少包含一条成功轨迹。

Result: BREAD仅需不到40%的真实轨迹，性能优于标准GRPO，训练速度提升约3倍，并能解决SFT + RL无法解决的问题。

Conclusion: BREAD通过分支展开和专家指导显著提升了小语言模型的推理能力，为SLM训练提供了新思路。

Abstract: Small language models (SLMs) struggle to learn complex reasoning behaviors,
especially when high-quality traces are scarce or difficult to learn from. The
standard training approach combines a supervised fine-tuning (SFT) stage, often
to distill capabilities of a larger model, followed by a reinforcement learning
(RL)stage such as Group Relative Policy Optimization (GRPO). In this paper, we
investigate the fundamental limitations of this SFT + RL paradigm and propose
methods to overcome them. Under a suitable theoretical model, we demonstrate
that the SFT + RL strategy can fail completely when (1) the expert's traces are
too difficult for the small model to express, or (2) the small model's
initialization has exponentially small likelihood of success. To address these,
we introduce BREAD: a GRPO variant that unifies the SFT and RL stages via
partial expert guidance and branched rollouts. When self-generated traces fail,
BREAD adaptively inserts short expert prefixes/hints, allowing the small model
to complete the rest of the reasoning path, and ensuring that each update
includes at least one successful trace. This mechanism both densifies the
reward signal and induces a natural learning curriculum. BREAD requires fewer
than 40% of ground-truth traces, consistently outperforming standard GRPO while
speeding up the training by about 3 times. Importantly, we demonstrate that
BREAD helps the model solve problems that are otherwise unsolvable by the SFT +
RL strategy, highlighting how branched rollouts and expert guidance can
substantially boost SLM reasoning.

</details>


### [408] [No Free Lunch: Rethinking Internal Feedback for LLM Reasoning](https://arxiv.org/abs/2506.17219)
*Yanzhi Zhang,Zhaoxi Zhang,Haoxiang Guan,Yilin Cheng,Yitong Duan,Chen Wang,Yue Wang,Shuxin Zheng,Jiyan He*

Main category: cs.LG

TL;DR: 论文提出了一种基于内部反馈的强化学习方法（RLIF），用于提升大语言模型的推理能力，无需外部监督。实验表明RLIF在训练初期能显著提升性能，但随着训练深入，性能会下降甚至低于基线。此外，RLIF对指令微调后的模型效果有限。


<details>
  <summary>Details</summary>
Motivation: 现有强化学习方法（如RLHF和RLVR）依赖外部监督，成本高。RLIF旨在通过内部反馈信号（如熵和自确定性）减少对外部监督的依赖。

Method: 提出RLIF方法，利用无监督的代理奖励（如token级熵、轨迹级熵和自确定性）作为内部反馈信号。理论分析表明这些目标部分等价，并通过实验验证其效果。

Result: RLIF在训练初期能显著提升推理性能，甚至超越RLVR，但随着训练深入，性能会下降。对指令微调后的模型效果有限。

Conclusion: RLIF在训练初期有效，但长期效果有限，尤其是对指令微调后的模型。研究为内部反馈的应用提供了实践指导。

Abstract: Reinforcement learning has emerged as a powerful paradigm for post-training
large language models (LLMs) to improve reasoning. Approaches like
Reinforcement Learning from Human Feedback (RLHF) and Reinforcement Learning
with Verifiable Rewards (RLVR) have shown strong results, but they require
extensive external supervision. We investigate an alternative class of methods,
Reinforcement Learning from Internal Feedback (RLIF), which relies solely on
intrinsic model-derived signals instead of external rewards. In particular, we
leverage unsupervised reward proxies such as token-level entropy,
trajectory-level entropy, and self-certainty. Our theoretical analysis shows
these internal objectives are partially equivalent, and we empirically evaluate
various RLIF strategies on challenging math reasoning benchmarks. Experimental
results demonstrate that RLIF can boost the reasoning performance of base LLMs
at the beginning phase of the training, matching or surpassing RLVR techniques
on these tasks. However, when training progresses, performance degrades even
below the model before training. Moreover, we find that RLIF yields little
improvement for instruction-tuned models, indicating diminishing returns of
intrinsic feedback once an LLM is already instruction-tuned. We further analyze
this limitation by mixing model weights and explain the reason of RLIF's
training behaviors, providing practical guidelines for integrating internal
feedback signals into LLM training. We hope our analysis of internal feedback
will inform more principled and effective strategies for LLM post-training.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [409] [RecBayes: Recurrent Bayesian Ad Hoc Teamwork in Large Partially Observable Domains](https://arxiv.org/abs/2506.15756)
*João G. Ribeiro,Yaniv Oren,Alberto Sardinha,Matthijs Spaan,Francisco S. Melo*

Main category: cs.MA

TL;DR: RecBayes是一种新颖的方法，用于部分可观测环境下的临时团队协作，无需依赖环境状态或队友动作。


<details>
  <summary>Details</summary>
Motivation: 解决现有方法在部分可观测环境中需要完全可观测状态或队友动作的限制，以及无法处理大规模环境的问题。

Method: 使用基于过去经验的循环贝叶斯分类器，仅通过观测识别已知团队和任务。

Result: 在扩展到1M状态和2^125观测的大规模环境中，RecBayes能有效识别团队和任务，并协助完成任务。

Conclusion: RecBayes在无需环境状态或队友动作的情况下，能够处理大规模部分可观测环境，表现优于现有方法。

Abstract: This paper proposes RecBayes, a novel approach for ad hoc teamwork under
partial observability, a setting where agents are deployed on-the-fly to
environments where pre-existing teams operate, that never requires, at any
stage, access to the states of the environment or the actions of its teammates.
We show that by relying on a recurrent Bayesian classifier trained using past
experiences, an ad hoc agent is effectively able to identify known teams and
tasks being performed from observations alone. Unlike recent approaches such as
PO-GPL (Gu et al., 2021) and FEAT (Rahman et al., 2023), that require at some
stage fully observable states of the environment, actions of teammates, or
both, or approaches such as ATPO (Ribeiro et al., 2023) that require the
environments to be small enough to be tabularly modelled (Ribeiro et al.,
2023), in their work up to 4.8K states and 1.7K observations, we show RecBayes
is both able to handle arbitrarily large spaces while never relying on either
states and teammates' actions. Our results in benchmark domains from the
multi-agent systems literature, adapted for partial observability and scaled up
to 1M states and 2^125 observations, show that RecBayes is effective at
identifying known teams and tasks being performed from partial observations
alone, and as a result, is able to assist the teams in solving the tasks
effectively.

</details>


### [410] [Learning to Coordinate Under Threshold Rewards: A Cooperative Multi-Agent Bandit Framework](https://arxiv.org/abs/2506.15856)
*Michael Ledford,William Regli*

Main category: cs.MA

TL;DR: 论文提出了一种去中心化算法T-Coop-UCB，用于解决多智能体在未知阈值激活奖励和存在诱饵臂情况下的协作学习问题。


<details>
  <summary>Details</summary>
Motivation: 多智能体系统在不确定性下需要协调行动，但现有多臂老虎机框架假设奖励可单独获得，而实际中奖励可能需阈值激活且存在诱饵臂。

Method: 提出了T-Coop-UCB算法，通过去中心化方式联合学习激活阈值和奖励分布，避免无效协作。

Result: 实验表明T-Coop-UCB在累积奖励、遗憾和协调指标上优于基线方法，接近Oracle性能。

Conclusion: 联合阈值学习和诱饵避免对复杂多智能体协作至关重要。

Abstract: Cooperative multi-agent systems often face tasks that require coordinated
actions under uncertainty. While multi-armed bandit (MAB) problems provide a
powerful framework for decentralized learning, most prior work assumes
individually attainable rewards. We address the challenging setting where
rewards are threshold-activated: an arm yields a payoff only when a minimum
number of agents pull it simultaneously, with this threshold unknown in
advance. Complicating matters further, some arms are decoys - requiring
coordination to activate but yielding no reward - introducing a new challenge
of wasted joint exploration. We introduce Threshold-Coop-UCB (T-Coop-UCB), a
decentralized algorithm that enables agents to jointly learn activation
thresholds and reward distributions, forming effective coalitions without
centralized control. Empirical results show that T-Coop-UCB consistently
outperforms baseline methods in cumulative reward, regret, and coordination
metrics, achieving near-Oracle performance. Our findings underscore the
importance of joint threshold learning and decoy avoidance for scalable,
decentralized cooperation in complex multi-agent

</details>


### [411] [Coordination of Electrical and Heating Resources by Self-Interested Agents](https://arxiv.org/abs/2506.16277)
*Rico Schrage,Jari Radler,Astrid Nieße*

Main category: cs.MA

TL;DR: 提出了一种分布式混合算法，用于协调分布式能源资源的多能源调度优化问题，同时优化私人和集体目标。


<details>
  <summary>Details</summary>
Motivation: 随着分布式能源资源和部门耦合的增加，需要一种方法来协调分散的能源资源，同时优化电力和热力输出。

Method: 基于八卦启发式和局部搜索的分布式混合算法，同时优化多个能源领域的私人和集体目标。

Result: 算法找到了全局接近最优的解决方案，同时保护了利益相关者的经济目标和设备的技术特性。

Conclusion: 该算法在纯电力和燃气技术测试案例中表现良好，证明了其有效性。

Abstract: With the rise of distributed energy resources and sector coupling,
distributed optimization can be a sensible approach to coordinate decentralized
energy resources. Further, district heating, heat pumps, cogeneration, and
sharing concepts like local energy communities introduce the potential to
optimize heating and electricity output simultaneously. To solve this issue, we
tackle the distributed multi-energy scheduling optimization problem, which
describes the optimization of distributed energy generators over multiple time
steps to reach a specific target schedule. This work describes a novel
distributed hybrid algorithm as a solution approach. This approach is based on
the heuristics of gossiping and local search and can simultaneously optimize
the private objective of the participants and the collective objective,
considering multiple energy sectors. We show that the algorithm finds globally
near-optimal solutions while protecting the stakeholders' economic goals and
the plants' technical properties. Two test cases representing pure electrical
and gas-based technologies are evaluated.

</details>


### [412] [Towards Emergency Scenarios: An Integrated Decision-making Framework of Multi-lane Platoon Reorganization](https://arxiv.org/abs/2506.16311)
*Aijing Kong,Chengkai Xu,Xian Wu,Xinbo Chen,Peng Hang*

Main category: cs.MA

TL;DR: 提出了一种基于强化学习和联盟博弈的车辆编队重组决策框架，显著降低碰撞率并提高重组效率。


<details>
  <summary>Details</summary>
Motivation: 提升车辆编队在紧急场景下的响应能力。

Method: 结合强化学习的编队分布模型、联盟博弈的车辆协同决策模型及基于图论的编队分布指数（PDI）。

Result: 在随机交通流的高风险场景中验证，显著降低碰撞率、提高驾驶效率，并减少重组时间。

Conclusion: 所提框架有效提升编队重组效率与安全性。

Abstract: To enhance the ability for vehicle platoons to respond to emergency
scenarios, a platoon distribution reorganization decision-making framework is
proposed. This framework contains platoon distribution layer, vehicle
cooperative decision-making layer and vehicle planning and control layer.
Firstly, a reinforcement-learning-based platoon distribution model is
presented, where a risk potential field is established to quantitatively assess
driving risks, and a reward function tailored to the platoon reorganization
process is constructed. Then, a coalition-game-based vehicle cooperative
decision-making model is put forward, modeling the cooperative relationships
among vehicles through dividing coalitions and generating the optimal decision
results for each vehicle. Additionally, a novel graph-theory-based Platoon
Disposition Index (PDI) is incorporated into the game reward function to
measure the platoon's distribution state during the reorganization process, in
order to accelerating the reorganization process. Finally, the validation of
the proposed framework is conducted in two high-risk scenarios under random
traffic flows. The results show that, compared to the baseline models, the
proposed method can significantly reduce the collision rate and improve driving
efficiency. Moreover, the model with PDI can significantly decrease the platoon
formation reorganization time and improve the reorganization efficiency.

</details>


### [413] [Generalizable Agent Modeling for Agent Collaboration-Competition Adaptation with Multi-Retrieval and Dynamic Generation](https://arxiv.org/abs/2506.16718)
*Chenxu Wang,Yonggang Jin,Cheng Hu,Youpeng Zhao,Zipeng Dai,Jian Zhao,Shiyu Huang,Liuyu Xiang,Junge Zhang,Zhaofeng He*

Main category: cs.MA

TL;DR: 论文提出了一种新的多智能体协作-竞争适应（ACCA）框架，并引入了一种多检索与动态生成（MRDG）方法，用于在未知队友和对手的环境中提升智能体的适应能力。


<details>
  <summary>Details</summary>
Motivation: 解决单一智能体在新多智能体系统中的适应挑战，尤其是在与未知队友和对手交互时的复杂性问题。

Method: 提出MRDG方法，利用行为轨迹建模队友和对手，结合位置编码器和超网络模块提升适应性，并通过视角对齐模块协调观测视角。

Result: 在SMAC、Overcooked-AI和Melting Pot等基准测试中，MRDG显著优于现有基线方法。

Conclusion: MRDG方法在多智能体协作与竞争中表现出强大的适应性和泛化能力。

Abstract: Adapting a single agent to a new multi-agent system brings challenges,
necessitating adjustments across various tasks, environments, and interactions
with unknown teammates and opponents. Addressing this challenge is highly
complex, and researchers have proposed two simplified scenarios, Multi-agent
reinforcement learning for zero-shot learning and Ad-Hoc Teamwork. Building on
these foundations, we propose a more comprehensive setting, Agent
Collaborative-Competitive Adaptation (ACCA), which evaluates an agent to
generalize across diverse scenarios, tasks, and interactions with both
unfamiliar opponents and teammates. In ACCA, agents adjust to task and
environmental changes, collaborate with unseen teammates, and compete against
unknown opponents. We introduce a new modeling approach, Multi-Retrieval and
Dynamic Generation (MRDG), that effectively models both teammates and opponents
using their behavioral trajectories. This method incorporates a positional
encoder for varying team sizes and a hypernetwork module to boost agents'
learning and adaptive capabilities. Additionally, a viewpoint alignment module
harmonizes the observational perspectives of retrieved teammates and opponents
with the learning agent. Extensive tests in benchmark scenarios like SMAC,
Overcooked-AI, and Melting Pot show that MRDG significantly improves robust
collaboration and competition with unseen teammates and opponents, surpassing
established baselines. Our code is available at:
https://github.com/vcis-wangchenxu/MRDG.git

</details>


### [414] [Engineering Resilience: An Energy-Based Approach to Sustainable Behavioural Interventions](https://arxiv.org/abs/2506.16836)
*Arpitha Srivathsa Malavalli,Karthik Sama,Janvi Chhabra,Pooja Bassin,Srinath Srinivasa*

Main category: cs.MA

TL;DR: 论文提出了一种基于自然启发的低能量状态假设的干预设计方法，旨在提高行为干预的可持续性和韧性。


<details>
  <summary>Details</summary>
Motivation: 解决复杂社会挑战（如公共健康、职场诚信、环保行为）需要有效的行为干预，但现有研究较少关注干预后状态的可持续性。

Method: 通过将低能量状态假设（即低能量状态更具韧性）作为正则化机制嵌入干预优化，设计可持续的干预方案，并通过基于代理的模拟验证。

Result: 模拟结果表明，基于能量假设的干预设计能有效诱导韧性行为状态，确保行为改变在扰动下仍能维持。

Conclusion: 能量视角为干预设计提供了更全面的框架，有助于实现可持续的行为改变。

Abstract: Addressing complex societal challenges, such as improving public health,
fostering honesty in workplaces, or encouraging eco-friendly behaviour requires
effective nudges to influence human behaviour at scale. Intervention science
seeks to design such nudges within complex societal systems. While
interventions primarily aim to shift the system toward a desired state, less
attention is given to the sustainability of that state, which we define in
terms of resilience: the system's ability to retain the desired state even
under perturbations. In this work, we offer a more holistic perspective to
intervention design by incorporating a nature-inspired postulate i.e., lower
energy states tend to exhibit greater resilience, as a regularization mechanism
within intervention optimization to ensure that the resulting state is also
sustainable. Using a simple agent-based simulation where commuters are nudged
to choose eco-friendly options (e.g., cycles) over individually attractive but
less eco-friendly ones (e.g., cars), we demonstrate how embedding lower energy
postulate into intervention design induces resilience. The system energy is
defined in terms of motivators that drive its agent's behaviour. By inherently
ensuring that agents are not pushed into actions that contradict their
motivators, the energy-based approach helps design effective interventions that
contribute to resilient behavioural states.

</details>


<div id='cs.MM'></div>

# cs.MM [[Back]](#toc)

### [415] [ViFusion: In-Network Tensor Fusion for Scalable Video Feature Indexing](https://arxiv.org/abs/2506.16258)
*Yisu Wang,Yixiang Zhu,Xinjiao Li,Yulong Zhang,Ruilong Wu,Dirk Kutscher*

Main category: cs.MM

TL;DR: ViFusion是一个通信感知的张量融合框架，通过合并小特征张量提升分布式视频索引效率，在数据中心环境中显著提高吞吐量。


<details>
  <summary>Details</summary>
Motivation: 大规模视频特征索引依赖于高效数据传输，现有方法在网络拥塞时性能下降，需要解决异构硬件和复杂网络拓扑的挑战。

Method: ViFusion结合网络内计算模块和专用张量融合机制，将小特征张量合并为更易管理的单元。

Result: ViFusion将视频检索系统的吞吐量提升8-22倍，同时保持与现有系统相同的延迟水平。

Conclusion: ViFusion通过优化张量融合和网络内计算，显著提升了分布式视频索引的效率。

Abstract: Large-scale video feature indexing in datacenters is critically dependent on
efficient data transfer. Although in-network computation has emerged as a
compelling strategy for accelerating feature extraction and reducing overhead
in distributed multimedia systems, harnessing advanced networking resources at
both the switch and host levels remains a formidable challenge. These
difficulties are compounded by heterogeneous hardware, diverse application
requirements, and complex multipath topologies. Existing methods focus
primarily on optimizing inference for large neural network models using
specialized collective communication libraries, which often face performance
degradation in network congestion scenarios.
  To overcome these limitations, we present ViFusion, a communication aware
tensor fusion framework that streamlines distributed video indexing by merging
numerous small feature tensors into consolidated and more manageable units. By
integrating an in-network computation module and a dedicated tensor fusion
mechanism within datacenter environments, ViFusion substantially improves the
efficiency of video feature indexing workflows. The deployment results show
that ViFusion improves the throughput of the video retrieval system by 8--22
times with the same level of latency as state-of-the-art systems.

</details>


### [416] [DT-UFC: Universal Large Model Feature Coding via Peaky-to-Balanced Distribution Transformation](https://arxiv.org/abs/2506.16495)
*Changsheng Gao,Zijie Liu,Li Li,Dong Liu,Xiaoyan Sun,Weisi Lin*

Main category: cs.MM

TL;DR: 本文首次系统研究了大型模型的通用特征编码，提出了一种非均匀、数据驱动的分布变换方法，显著提升了压缩效率和跨模型泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有研究多针对特定任务或模型，缺乏对跨大型模型的通用特征编码解决方案。

Method: 提出了一种学习的峰值到平衡分布变换方法，将不同模型的特征分布对齐到一个共同的目标空间。

Result: 在LLaMA3、DINOv2和SD3等模型上验证了方法的有效性，压缩效率和跨模型泛化能力显著提升。

Conclusion: 该方法为大型模型的通用特征编码提供了有效解决方案，未来将开源代码以促进研究。

Abstract: Like image coding in visual data transmission, feature coding is essential
for the distributed deployment of large models by significantly reducing
transmission and storage overhead. However, prior studies have mostly targeted
task- or model-specific scenarios, leaving the challenge of universal feature
coding across diverse large models largely unaddressed. In this paper, we
present the first systematic study on universal feature coding for large
models. The key challenge lies in the inherently diverse and distributionally
incompatible nature of features extracted from different models. For example,
features from DINOv2 exhibit highly peaky, concentrated distributions, while
those from Stable Diffusion 3 (SD3) are more dispersed and uniform. This
distributional heterogeneity severely hampers both compression efficiency and
cross-model generalization. To address this, we propose a learned
peaky-to-balanced distribution transformation, which reshapes highly skewed
feature distributions into a common, balanced target space. This transformation
is non-uniform, data-driven, and plug-and-play, enabling effective alignment of
heterogeneous distributions without modifying downstream codecs. With this
alignment, a universal codec trained on the balanced target distribution can
effectively generalize to features from different models and tasks. We validate
our approach on three representative large models-LLaMA3, DINOv2, and
SD3-across multiple tasks and modalities. Extensive experiments show that our
method achieves notable improvements in both compression efficiency and
cross-model generalization over task-specific baselines. All source code will
be released for future research.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [417] [Robust control for multi-legged elongate robots in noisy environments](https://arxiv.org/abs/2506.15788)
*Baxi Chong,Juntao He,Daniel Irvine,Tianyu Wang,Esteban Flores,Daniel Soto,Jianfeng Lin,Zhaochen Xu,Vincent R Nienhusser,Grigoriy Blekherman,Daniel I. Goldman*

Main category: cs.RO

TL;DR: 提出了一种基于机械智能（MI）和计算智能（CI）的新型多足机器人控制框架，通过模拟通信理论中的冗余和反馈机制，实现了在复杂地形中的鲁棒运动。


<details>
  <summary>Details</summary>
Motivation: 当前多足机器人依赖高带宽传感和计算，且训练过程复杂，缺乏通用性。研究旨在开发一种更简单、通用的控制方法。

Method: 将每条腿与地面的接触视为基本主动接触（bac），模拟通信中的冗余和反馈机制（FEC和ARQ），结合MI和CI实现鲁棒控制。

Result: 在复杂地形中实现了高效可靠的运动（每周期约半个身长），地形噪声高达机器人高度的两倍。

Conclusion: 该框架为多足机器人控制提供了系统性方法，为极端环境下的敏捷、鲁棒机器人系统奠定了基础。

Abstract: Modern two and four legged robots exhibit impressive mobility on complex
terrain, largely attributed to advancement in learning algorithms. However,
these systems often rely on high-bandwidth sensing and onboard computation to
perceive/respond to terrain uncertainties. Further, current locomotion
strategies typically require extensive robot-specific training, limiting their
generalizability across platforms. Building on our prior research connecting
robot-environment interaction and communication theory, we develop a new
paradigm to construct robust and simply controlled multi-legged elongate robots
(MERs) capable of operating effectively in cluttered, unstructured
environments. In this framework, each leg-ground contact is thought of as a
basic active contact (bac), akin to bits in signal transmission. Reliable
locomotion can be achieved in open-loop on "noisy" landscapes via sufficient
redundancy in bacs. In such situations, robustness is achieved through passive
mechanical responses. We term such processes as those displaying mechanical
intelligence (MI) and analogize these processes to forward error correction
(FEC) in signal transmission. To augment MI, we develop feedback control
schemes, which we refer to as computational intelligence (CI) and such
processes analogize automatic repeat request (ARQ) in signal transmission.
Integration of these analogies between locomotion and communication theory
allow analysis, design, and prediction of embodied intelligence control schemes
(integrating MI and CI) in MERs, showing effective and reliable performance
(approximately half body lengths per cycle) on complex landscapes with terrain
"noise" over twice the robot's height. Our work provides a foundation for
systematic development of MER control, paving the way for terrain-agnostic,
agile, and resilient robotic systems capable of operating in extreme
environments.

</details>


### [418] [Steering Your Diffusion Policy with Latent Space Reinforcement Learning](https://arxiv.org/abs/2506.15799)
*Andrew Wagenmaker,Mitsuhiko Nakamoto,Yunchu Zhang,Seohong Park,Waleed Yagoub,Anusha Nagabandi,Abhishek Gupta,Sergey Levine*

Main category: cs.RO

TL;DR: 论文提出了一种名为DSRL的方法，通过强化学习在行为克隆（BC）策略的潜在噪声空间中进行调整，实现了高效的自主策略改进。


<details>
  <summary>Details</summary>
Motivation: 在开放世界场景中，行为克隆策略初始表现不佳时，通常需要额外的人类演示来改进，成本高昂。强化学习虽能自主改进策略，但样本效率低。DSRL旨在解决这一问题。

Method: 提出DSRL方法，通过在扩散策略（一种先进的BC方法）的潜在噪声空间运行强化学习，调整BC策略。

Result: DSRL样本效率高，仅需对BC策略的黑盒访问，能有效实现自主策略改进，且无需修改基础策略权重。

Conclusion: DSRL在模拟和真实机器人任务中表现出高效性和有效性，适用于预训练通用策略的调整。

Abstract: Robotic control policies learned from human demonstrations have achieved
impressive results in many real-world applications. However, in scenarios where
initial performance is not satisfactory, as is often the case in novel
open-world settings, such behavioral cloning (BC)-learned policies typically
require collecting additional human demonstrations to further improve their
behavior -- an expensive and time-consuming process. In contrast, reinforcement
learning (RL) holds the promise of enabling autonomous online policy
improvement, but often falls short of achieving this due to the large number of
samples it typically requires. In this work we take steps towards enabling fast
autonomous adaptation of BC-trained policies via efficient real-world RL.
Focusing in particular on diffusion policies -- a state-of-the-art BC
methodology -- we propose diffusion steering via reinforcement learning (DSRL):
adapting the BC policy by running RL over its latent-noise space. We show that
DSRL is highly sample efficient, requires only black-box access to the BC
policy, and enables effective real-world autonomous policy improvement.
Furthermore, DSRL avoids many of the challenges associated with finetuning
diffusion policies, obviating the need to modify the weights of the base policy
at all. We demonstrate DSRL on simulated benchmarks, real-world robotic tasks,
and for adapting pretrained generalist policies, illustrating its sample
efficiency and effective performance at real-world policy improvement.

</details>


### [419] [Context Matters! Relaxing Goals with LLMs for Feasible 3D Scene Planning](https://arxiv.org/abs/2506.15828)
*Emanuele Musumeci,Michele Brienza,Francesco Argenziano,Vincenzo Suriani,Daniele Nardi,Domenico D. Bloisi*

Main category: cs.RO

TL;DR: 论文提出了一种结合经典规划与大型语言模型（LLMs）的方法，以解决机器人任务规划中感知受限和动作可行性问题。


<details>
  <summary>Details</summary>
Motivation: 经典规划方法在真实场景中因感知受限和动作可行性问题表现不佳，而LLMs虽能利用常识推理但生成的计划可能不可行或不安全。

Method: 通过分层规划框架，结合LLMs的常识推理能力，逐步放松任务目标以实现适应性规划。

Result: 方法在3D场景图中表现出高效的任务执行能力，并在复杂场景中优于基准方法。

Conclusion: 该方法通过整合经典规划与LLMs，显著提升了机器人任务规划的适应性和可行性。

Abstract: Classical planning in AI and Robotics addresses complex tasks by shifting
from imperative to declarative approaches (e.g., PDDL). However, these methods
often fail in real scenarios due to limited robot perception and the need to
ground perceptions to planning predicates. This often results in heavily
hard-coded behaviors that struggle to adapt, even with scenarios where goals
can be achieved through relaxed planning. Meanwhile, Large Language Models
(LLMs) lead to planning systems that leverage commonsense reasoning but often
at the cost of generating unfeasible and/or unsafe plans. To address these
limitations, we present an approach integrating classical planning with LLMs,
leveraging their ability to extract commonsense knowledge and ground actions.
We propose a hierarchical formulation that enables robots to make unfeasible
tasks tractable by defining functionally equivalent goals through gradual
relaxation. This mechanism supports partial achievement of the intended
objective, suited to the agent's specific context. Our method demonstrates its
ability to adapt and execute tasks effectively within environments modeled
using 3D Scene Graphs through comprehensive qualitative and quantitative
evaluations. We also show how this method succeeds in complex scenarios where
other benchmark methods are more likely to fail. Code, dataset, and additional
material are released to the community.

</details>


### [420] [SafeMimic: Towards Safe and Autonomous Human-to-Robot Imitation for Mobile Manipulation](https://arxiv.org/abs/2506.15847)
*Arpit Bahety,Arnav Balaji,Ben Abbatematteo,Roberto Martín-Martín*

Main category: cs.RO

TL;DR: SafeMimic框架通过单次人类视频演示，安全自主地学习移动操作技能，包括解析视频、适应机器人形态和验证安全性。


<details>
  <summary>Details</summary>
Motivation: 让机器人通过观察人类演示学习新任务，减少对人类监控的依赖，并确保学习过程安全自主。

Method: 解析视频为语义和动作段，转换为第一人称视角，采样候选动作并验证安全性，回溯调整轨迹和抓取模式。

Result: 实验显示，SafeMimic能安全高效地从单次演示学习多步任务，优于现有基线方法。

Conclusion: SafeMimic为机器人学习复杂任务提供了一种安全自主的解决方案，减少了未来探索的需求。

Abstract: For robots to become efficient helpers in the home, they must learn to
perform new mobile manipulation tasks simply by watching humans perform them.
Learning from a single video demonstration from a human is challenging as the
robot needs to first extract from the demo what needs to be done and how,
translate the strategy from a third to a first-person perspective, and then
adapt it to be successful with its own morphology. Furthermore, to mitigate the
dependency on costly human monitoring, this learning process should be
performed in a safe and autonomous manner. We present SafeMimic, a framework to
learn new mobile manipulation skills safely and autonomously from a single
third-person human video. Given an initial human video demonstration of a
multi-step mobile manipulation task, SafeMimic first parses the video into
segments, inferring both the semantic changes caused and the motions the human
executed to achieve them and translating them to an egocentric reference. Then,
it adapts the behavior to the robot's own morphology by sampling candidate
actions around the human ones, and verifying them for safety before execution
in a receding horizon fashion using an ensemble of safety Q-functions trained
in simulation. When safe forward progression is not possible, SafeMimic
backtracks to previous states and attempts a different sequence of actions,
adapting both the trajectory and the grasping modes when required for its
morphology. As a result, SafeMimic yields a strategy that succeeds in the
demonstrated behavior and learns task-specific actions that reduce exploration
in future attempts. Our experiments show that our method allows robots to
safely and efficiently learn multi-step mobile manipulation behaviors from a
single human demonstration, from different users, and in different
environments, with improvements over state-of-the-art baselines across seven
tasks

</details>


### [421] [PRISM-Loc: a Lightweight Long-range LiDAR Localization in Urban Environments with Topological Maps](https://arxiv.org/abs/2506.15849)
*Kirill Muravyev,Vasily Yuryev,Oleg Bulichev,Dmitry Yudin,Konstantin Yakovlev*

Main category: cs.RO

TL;DR: PRISM-Loc是一种基于拓扑地图的定位方法，用于大型环境中的实时定位，结合全局地点识别和局部位姿估计，性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 在长距离路径中，使用密集全局激光雷达地图进行实时定位可能困难且占用内存大，因此需要拓扑地图解决方案。

Method: 提出PRISM-Loc，采用双重定位流程：全局地点识别和局部位姿估计，后者基于2D特征和点优化算法。

Result: 在3公里路线的ITLP-Campus数据集上测试，PRISM-Loc在质量和计算效率上均优于现有方法。

Conclusion: PRISM-Loc是一种高效且性能优越的拓扑地图定位方法，适用于大型环境。

Abstract: Localization in the environment is one of the crucial tasks of navigation of
a mobile robot or a self-driving vehicle. For long-range routes, performing
localization within a dense global lidar map in real time may be difficult, and
the creation of such a map may require much memory. To this end, leveraging
topological maps may be useful. In this work, we propose PRISM-Loc -- a
topological map-based approach for localization in large environments. The
proposed approach leverages a twofold localization pipeline, which consists of
global place recognition and estimation of the local pose inside the found
location. For local pose estimation, we introduce an original lidar scan
matching algorithm, which is based on 2D features and point-based optimization.
We evaluate the proposed method on the ITLP-Campus dataset on a 3 km route, and
compare it against the state-of-the-art metric map-based and place
recognition-based competitors. The results of the experiments show that the
proposed method outperforms its competitors both quality-wise and
computationally-wise.

</details>


### [422] [Semantic and Feature Guided Uncertainty Quantification of Visual Localization for Autonomous Vehicles](https://arxiv.org/abs/2506.15851)
*Qiyuan Wu,Mark Campbell*

Main category: cs.RO

TL;DR: 本文提出了一种用于自动驾驶视觉定位的轻量级传感器误差模型，通过图像特征和语义信息预测二维误差分布，并验证了其在高斯混合模型中的优越性。


<details>
  <summary>Details</summary>
Motivation: 传感器测量与深度学习网络的不确定性量化对自动驾驶等安全关键应用至关重要。

Method: 使用轻量级传感器误差模型，结合图像特征和语义信息预测误差分布，并通过高斯混合模型改进不确定性预测。

Result: 在Ithaca365数据集上验证了方法的准确性，尤其在恶劣天气和光照条件下，高斯混合模型优于高斯分布。

Conclusion: 该方法能有效捕捉未标注的上下文因素，为自动驾驶视觉定位提供更准确的不确定性量化。

Abstract: The uncertainty quantification of sensor measurements coupled with deep
learning networks is crucial for many robotics systems, especially for
safety-critical applications such as self-driving cars. This paper develops an
uncertainty quantification approach in the context of visual localization for
autonomous driving, where locations are selected based on images. Key to our
approach is to learn the measurement uncertainty using light-weight sensor
error model, which maps both image feature and semantic information to
2-dimensional error distribution. Our approach enables uncertainty estimation
conditioned on the specific context of the matched image pair, implicitly
capturing other critical, unannotated factors (e.g., city vs highway, dynamic
vs static scenes, winter vs summer) in a latent manner. We demonstrate the
accuracy of our uncertainty prediction framework using the Ithaca365 dataset,
which includes variations in lighting and weather (sunny, night, snowy). Both
the uncertainty quantification of the sensor+network is evaluated, along with
Bayesian localization filters using unique sensor gating method. Results show
that the measurement error does not follow a Gaussian distribution with poor
weather and lighting conditions, and is better predicted by our Gaussian
Mixture model.

</details>


### [423] [Improving Robotic Manipulation: Techniques for Object Pose Estimation, Accommodating Positional Uncertainty, and Disassembly Tasks from Examples](https://arxiv.org/abs/2506.15865)
*Viral Rasik Galaiya*

Main category: cs.RO

TL;DR: 论文探讨了在非结构化环境中利用触觉传感和强化学习改进机器人抓取和移动物体的能力。


<details>
  <summary>Details</summary>
Motivation: 在非结构化环境中，机器人需要更强的环境感知能力以适应不确定性。相机存在遮挡和视野限制，因此转向触觉传感。

Method: 使用触觉传感确定物体姿态，结合强化学习减少抓取尝试次数，并通过人类示例缩短训练时间。

Result: 触觉传感和强化学习的结合提高了机器人抓取和移动物体的效率和准确性。

Conclusion: 触觉传感和强化学习是提升机器人在复杂环境中操作能力的有效方法。

Abstract: To use robots in more unstructured environments, we have to accommodate for
more complexities. Robotic systems need more awareness of the environment to
adapt to uncertainty and variability. Although cameras have been predominantly
used in robotic tasks, the limitations that come with them, such as occlusion,
visibility and breadth of information, have diverted some focus to tactile
sensing. In this thesis, we explore the use of tactile sensing to determine the
pose of the object using the temporal features. We then use reinforcement
learning with tactile collisions to reduce the number of attempts required to
grasp an object resulting from positional uncertainty from camera estimates.
Finally, we use information provided by these tactile sensors to a
reinforcement learning agent to determine the trajectory to take to remove an
object from a restricted passage while reducing training time by pertaining
from human examples.

</details>


### [424] [CooperRisk: A Driving Risk Quantification Pipeline with Multi-Agent Cooperative Perception and Prediction](https://arxiv.org/abs/2506.15868)
*Mingyue Lei,Zewei Zhou,Hongchen Li,Jia Hu,Jiaqi Ma*

Main category: cs.RO

TL;DR: 论文提出了一种基于V2X的风险量化框架CooperRisk，通过多智能体感知信息融合和未来时间戳的风险量化，解决了复杂场景下风险解释性和多智能体交互的问题。


<details>
  <summary>Details</summary>
Motivation: 单车辆系统在复杂密集场景中的感知范围和遮挡限制了风险量化能力，V2X技术虽能共享感知信息，但如何确保风险解释性和多智能体交互理解仍待解决。

Method: 设计了基于Transformer的风险导向预测模型，融合多模态和多智能体信息，确保场景一致的未来行为预测，避免冲突预测导致的风险量化保守。

Result: 在V2XPnP数据集上实验表明，CooperRisk在风险量化方面表现优异，冲突率降低了44.35%。

Conclusion: CooperRisk通过多智能体信息融合和风险导向预测，显著提升了风险量化的准确性和解释性，为自动驾驶安全提供了有效支持。

Abstract: Risk quantification is a critical component of safe autonomous driving,
however, constrained by the limited perception range and occlusion of
single-vehicle systems in complex and dense scenarios. Vehicle-to-everything
(V2X) paradigm has been a promising solution to sharing complementary
perception information, nevertheless, how to ensure the risk interpretability
while understanding multi-agent interaction with V2X remains an open question.
In this paper, we introduce the first V2X-enabled risk quantification pipeline,
CooperRisk, to fuse perception information from multiple agents and quantify
the scenario driving risk in future multiple timestamps. The risk is
represented as a scenario risk map to ensure interpretability based on risk
severity and exposure, and the multi-agent interaction is captured by the
learning-based cooperative prediction model. We carefully design a
risk-oriented transformer-based prediction model with multi-modality and
multi-agent considerations. It aims to ensure scene-consistent future behaviors
of multiple agents and avoid conflicting predictions that could lead to overly
conservative risk quantification and cause the ego vehicle to become overly
hesitant to drive. Then, the temporal risk maps could serve to guide a model
predictive control planner. We evaluate the CooperRisk pipeline in a real-world
V2X dataset V2XPnP, and the experiments demonstrate its superior performance in
risk quantification, showing a 44.35% decrease in conflict rate between the ego
vehicle and background traffic participants.

</details>


### [425] [A Small-Scale Robot for Autonomous Driving: Design, Challenges, and Best Practices](https://arxiv.org/abs/2506.15870)
*Hossein Maghsoumi,Yaser Fallah*

Main category: cs.RO

TL;DR: 本文探讨了六分之一比例的小型自动驾驶车辆平台的设计与开发，旨在填补该领域的研究空白，提升其可靠性和性能。


<details>
  <summary>Details</summary>
Motivation: 小型自动驾驶车辆平台在开发和测试高级驾驶系统中具有成本效益，但特定配置的研究不足，限制了其潜力。

Method: 提供了六分之一比例平台的设计、硬件与软件集成概述，并讨论了开发中的常见挑战及解决方法。

Result: 提出了提升小型车辆平台可靠性和性能的指南，并分享了相关见解。

Conclusion: 通过分享这些经验，旨在扩展小型车辆在自动驾驶算法测试中的实用性，并鼓励进一步研究。

Abstract: Small-scale autonomous vehicle platforms provide a cost-effective environment
for developing and testing advanced driving systems. However, specific
configurations within this scale are underrepresented, limiting full awareness
of their potential. This paper focuses on a one-sixth-scale setup, offering a
high-level overview of its design, hardware and software integration, and
typical challenges encountered during development. We discuss methods for
addressing mechanical and electronic issues common to this scale and propose
guidelines for improving reliability and performance. By sharing these
insights, we aim to expand the utility of small-scale vehicles for testing
autonomous driving algorithms and to encourage further research in this domain.

</details>


### [426] [Goal-conditioned Hierarchical Reinforcement Learning for Sample-efficient and Safe Autonomous Driving at Intersections](https://arxiv.org/abs/2506.16336)
*Yiou Huang*

Main category: cs.RO

TL;DR: 提出了一种基于目标条件碰撞预测（GCCP）的分层强化学习（HRL）框架，用于自动驾驶任务，提高了样本效率和安全性。


<details>
  <summary>Details</summary>
Motivation: 传统强化学习在复杂自动驾驶场景中样本效率低且安全性不足。

Method: 采用分层结构，高层决策选择安全子目标，低层运动规划执行；GCCP模块预测碰撞风险。

Result: 实验表明，该方法收敛更快，安全性更高。

Conclusion: 分层结构和GCCP模块显著提升了自动驾驶任务的样本效率和安全性。

Abstract: Reinforcement learning (RL) exhibits remarkable potential in addressing
autonomous driving tasks. However, it is difficult to train a sample-efficient
and safe policy in complex scenarios. In this article, we propose a novel
hierarchical reinforcement learning (HRL) framework with a goal-conditioned
collision prediction (GCCP) module. In the hierarchical structure, the GCCP
module predicts collision risks according to different potential subgoals of
the ego vehicle. A high-level decision-maker choose the best safe subgoal. A
low-level motion-planner interacts with the environment according to the
subgoal. Compared to traditional RL methods, our algorithm is more
sample-efficient, since its hierarchical structure allows reusing the policies
of subgoals across similar tasks for various navigation scenarios. In
additional, the GCCP module's ability to predict both the ego vehicle's and
surrounding vehicles' future actions according to different subgoals, ensures
the safety of the ego vehicle throughout the decision-making process.
Experimental results demonstrate that the proposed method converges to an
optimal policy faster and achieves higher safety than traditional RL methods.

</details>


### [427] [Challenges and Research Directions from the Operational Use of a Machine Learning Damage Assessment System via Small Uncrewed Aerial Systems at Hurricanes Debby and Helene](https://arxiv.org/abs/2506.15890)
*Thomas Manzini,Priyankari Perali,Robin R. Murphy,David Merrick*

Main category: cs.RO

TL;DR: 论文分析了基于小型无人机的机器学习在飓风灾害评估中的四大挑战，并提出了未来研究的三个方向。


<details>
  <summary>Details</summary>
Motivation: 研究旨在解决实际部署中机器学习系统在灾害评估中遇到的问题，以提高其操作效率。

Method: 通过分析飓风Debby和Helene的数据集，以及对比有人机影像，识别挑战并提出改进建议。

Result: 发现四大挑战（影像分辨率差异、空间对齐问题、无线连接依赖、数据格式问题），并提出了三项研究建议。

Conclusion: 未来研究应关注提高机器学习模型的适应性和减少对无线连接的依赖，以优化灾害响应效果。

Abstract: This paper details four principal challenges encountered with machine
learning (ML) damage assessment using small uncrewed aerial systems (sUAS) at
Hurricanes Debby and Helene that prevented, degraded, or delayed the delivery
of data products during operations and suggests three research directions for
future real-world deployments. The presence of these challenges is not
surprising given that a review of the literature considering both datasets and
proposed ML models suggests this is the first sUAS-based ML system for disaster
damage assessment actually deployed as a part of real-world operations. The
sUAS-based ML system was applied by the State of Florida to Hurricanes Helene
(2 orthomosaics, 3.0 gigapixels collected over 2 sorties by a Wintra WingtraOne
sUAS) and Debby (1 orthomosaic, 0.59 gigapixels collected via 1 sortie by a
Wintra WingtraOne sUAS) in Florida. The same model was applied to crewed aerial
imagery of inland flood damage resulting from post-tropical remnants of
Hurricane Debby in Pennsylvania (436 orthophotos, 136.5 gigapixels), providing
further insights into the advantages and limitations of sUAS for disaster
response. The four challenges (variationin spatial resolution of input imagery,
spatial misalignment between imagery and geospatial data, wireless
connectivity, and data product format) lead to three recommendations that
specify research needed to improve ML model capabilities to accommodate the
wide variation of potential spatial resolutions used in practice, handle
spatial misalignment, and minimize the dependency on wireless connectivity.
These recommendations are expected to improve the effective operational use of
sUAS and sUAS-based ML damage assessment systems for disaster response.

</details>


### [428] [eCAV: An Edge-Assisted Evaluation Platform for Connected Autonomous Vehicles](https://arxiv.org/abs/2506.16535)
*Tyler Landle,Jordan Rapp,Dean Blank,Chandramouli Amarnath,Abhijit Chatterjee,Alex Daglis,Umakishore Ramachandran*

Main category: cs.RO

TL;DR: 论文提出eCAV平台，用于高效、模块化和可扩展的评估，以提升自动驾驶车辆的安全性和性能，支持256辆无感知车辆或64辆有感知车辆的模拟。


<details>
  <summary>Details</summary>
Motivation: 随着自动驾驶车辆接近广泛应用，通过碰撞避免和减少附带损害提升道路安全变得至关重要。V2X技术（如V2V、V2I、V2C）被提出以实现这一目标，但缺乏有效的评估框架。

Method: 提出eCAV平台，支持大规模自动驾驶车辆模拟，包括无感知和有感知模式，比现有技术（如OpenCDA）更高效。

Result: eCAV可模拟256辆无感知车辆或64辆有感知车辆，性能优于现有技术（8倍和4倍车辆数量，1.5倍速度）。

Conclusion: eCAV为自动驾驶车辆的安全性和性能评估提供了高效、可扩展的解决方案，填补了现有技术的不足。

Abstract: As autonomous vehicles edge closer to widespread adoption, enhancing road
safety through collision avoidance and minimization of collateral damage
becomes imperative. Vehicle-to-everything (V2X) technologies, which include
vehicle-to-vehicle (V2V), vehicle-to-infrastructure (V2I), and vehicle-to-cloud
(V2C), are being proposed as mechanisms to achieve this safety improvement.
  Simulation-based testing is crucial for early-stage evaluation of Connected
Autonomous Vehicle (CAV) control systems, offering a safer and more
cost-effective alternative to real-world tests. However, simulating large 3D
environments with many complex single- and multi-vehicle sensors and
controllers is computationally intensive. There is currently no evaluation
framework that can effectively evaluate realistic scenarios involving large
numbers of autonomous vehicles.
  We propose eCAV -- an efficient, modular, and scalable evaluation platform to
facilitate both functional validation of algorithmic approaches to increasing
road safety, as well as performance prediction of algorithms of various V2X
technologies, including a futuristic Vehicle-to-Edge control plane and
correspondingly designed control algorithms. eCAV can model up to 256 vehicles
running individual control algorithms without perception enabled, which is
$8\times$ more vehicles than what is possible with state-of-the-art
alternatives. %faster than state-of-the-art alternatives that can simulate
$8\times$ fewer vehicles. With perception enabled, eCAV simulates up to 64
vehicles with a step time under 800ms, which is $4\times$ more and $1.5\times$
faster than the state-of-the-art OpenCDA framework.

</details>


### [429] [Advancing Autonomous Racing: A Comprehensive Survey of the RoboRacer (F1TENTH) Platform](https://arxiv.org/abs/2506.15899)
*Israel Charles,Hossein Maghsoumi,Yaser Fallah*

Main category: cs.RO

TL;DR: 本文综述了RoboRacer（F1TENTH）平台，分析了其硬件和软件架构、研究应用及教育价值，强调了其在自动驾驶研究中的重要性。


<details>
  <summary>Details</summary>
Motivation: RoboRacer平台为自动驾驶研究提供了可扩展、经济高效且社区驱动的实验环境，本文旨在全面评估其贡献和潜力。

Method: 通过分析平台的模块化架构、Sim2Real技术、仿真环境集成、标准化数据集及算法进展，结合竞赛和合作研究成果进行综述。

Result: 研究发现RoboRacer是加速创新、连接理论与实际部署的通用框架，推动了自动驾驶赛车和机器人技术的发展。

Conclusion: RoboRacer平台在自动驾驶研究和教育中具有重要价值，是理论与现实应用之间的桥梁。

Abstract: The RoboRacer (F1TENTH) platform has emerged as a leading testbed for
advancing autonomous driving research, offering a scalable, cost-effective, and
community-driven environment for experimentation. This paper presents a
comprehensive survey of the platform, analyzing its modular hardware and
software architecture, diverse research applications, and role in autonomous
systems education. We examine critical aspects such as bridging the
simulation-to-reality (Sim2Real) gap, integration with simulation environments,
and the availability of standardized datasets and benchmarks. Furthermore, the
survey highlights advancements in perception, planning, and control algorithms,
as well as insights from global competitions and collaborative research
efforts. By consolidating these contributions, this study positions RoboRacer
as a versatile framework for accelerating innovation and bridging the gap
between theoretical research and real-world deployment. The findings underscore
the platform's significance in driving forward developments in autonomous
racing and robotics.

</details>


### [430] [Learning from Planned Data to Improve Robotic Pick-and-Place Planning Efficiency](https://arxiv.org/abs/2506.15920)
*Liang Qin,Weiwei Wan,Jun Takahashi,Ryo Negishi,Masaki Matsushita,Kensuke Harada*

Main category: cs.RO

TL;DR: 提出了一种基于能量模型（EBM）的学习方法，用于加速机器人抓取-放置任务中的共享抓取预测。


<details>
  <summary>Details</summary>
Motivation: 传统分析方法在解决共享抓取时需单独评估候选抓取，计算开销大。

Method: 引入能量模型（EBM），通过结合初始和目标物体姿态的可行抓取能量来预测共享抓取。

Result: 实验表明，该方法提升了抓取选择性能，数据效率更高，且能泛化到未见过的抓取和类似形状物体。

Conclusion: 该方法显著减少了搜索空间，提高了机器人抓取-放置任务的效率。

Abstract: This work proposes a learning method to accelerate robotic pick-and-place
planning by predicting shared grasps. Shared grasps are defined as grasp poses
feasible to both the initial and goal object configurations in a pick-and-place
task. Traditional analytical methods for solving shared grasps evaluate grasp
candidates separately, leading to substantial computational overhead as the
candidate set grows. To overcome the limitation, we introduce an Energy-Based
Model (EBM) that predicts shared grasps by combining the energies of feasible
grasps at both object poses. This formulation enables early identification of
promising candidates and significantly reduces the search space. Experiments
show that our method improves grasp selection performance, offers higher data
efficiency, and generalizes well to unseen grasps and similarly shaped objects.

</details>


### [431] [Experimental Setup and Software Pipeline to Evaluate Optimization based Autonomous Multi-Robot Search Algorithms](https://arxiv.org/abs/2506.16710)
*Aditya Bhatt,Mary Katherine Corra,Franklin Merlo,Prajit KrisshnaKumar,Souma Chowdhury*

Main category: cs.RO

TL;DR: 该论文提出了一种新的实验室规模物理设置和开源软件管道，用于评估和基准测试多机器人搜索算法，填补了算法仅在仿真中测试的空白。


<details>
  <summary>Details</summary>
Motivation: 多机器人信号源定位在搜索救援和危险定位中有广泛应用，但现有算法多仅在仿真中测试，缺乏实际物理环境中的性能对比。

Method: 使用声源和小型地面机器人（e-pucks）在标准运动捕捉环境中构建物理设置，设计开源软件管道支持多机器人搜索算法的分布式实现。

Result: 通过评估两种先进的多机器人搜索算法（群优化和批量贝叶斯优化）以及随机游走基线，展示了该设置的实用性。

Conclusion: 该物理设置和软件管道易于复制和使用，为多机器人搜索算法的实际性能评估提供了有效工具。

Abstract: Signal source localization has been a problem of interest in the multi-robot
systems domain given its applications in search \& rescue and hazard
localization in various industrial and outdoor settings. A variety of
multi-robot search algorithms exist that usually formulate and solve the
associated autonomous motion planning problem as a heuristic model-free or
belief model-based optimization process. Most of these algorithms however
remains tested only in simulation, thereby losing the opportunity to generate
knowledge about how such algorithms would compare/contrast in a real physical
setting in terms of search performance and real-time computing performance. To
address this gap, this paper presents a new lab-scale physical setup and
associated open-source software pipeline to evaluate and benchmark multi-robot
search algorithms. The presented physical setup innovatively uses an acoustic
source (that is safe and inexpensive) and small ground robots (e-pucks)
operating in a standard motion-capture environment. This setup can be easily
recreated and used by most robotics researchers. The acoustic source also
presents interesting uncertainty in terms of its noise-to-signal ratio, which
is useful to assess sim-to-real gaps. The overall software pipeline is designed
to readily interface with any multi-robot search algorithm with minimal effort
and is executable in parallel asynchronous form. This pipeline includes a
framework for distributed implementation of multi-robot or swarm search
algorithms, integrated with a ROS (Robotics Operating System)-based software
stack for motion capture supported localization. The utility of this novel
setup is demonstrated by using it to evaluate two state-of-the-art multi-robot
search algorithms, based on swarm optimization and batch-Bayesian Optimization
(called Bayes-Swarm), as well as a random walk baseline.

</details>


### [432] [Single-Microphone-Based Sound Source Localization for Mobile Robots in Reverberant Environments](https://arxiv.org/abs/2506.16173)
*Jiang Wang,Runwu Shi,Benjamin Yen,He Kong,Kazuhiro Nakadai*

Main category: cs.RO

TL;DR: 提出了一种基于单麦克风的在线声源定位方法，适用于移动机器人在混响环境中实时定位声源。


<details>
  <summary>Details</summary>
Motivation: 现有声源定位方法通常需要至少两个麦克风，限制了其应用范围。本文旨在解决这一问题。

Method: 开发了一个轻量级神经网络模型（43k参数），通过提取混响信号的时间信息实时估计距离，并结合扩展卡尔曼滤波器实现在线定位。

Result: 实验证明该方法有效，填补了单麦克风移动机器人声源定位的空白。

Conclusion: 该方法具有实际应用价值，代码已开源。

Abstract: Accurately estimating sound source positions is crucial for robot audition.
However, existing sound source localization methods typically rely on a
microphone array with at least two spatially preconfigured microphones. This
requirement hinders the applicability of microphone-based robot audition
systems and technologies. To alleviate these challenges, we propose an online
sound source localization method that uses a single microphone mounted on a
mobile robot in reverberant environments. Specifically, we develop a
lightweight neural network model with only 43k parameters to perform real-time
distance estimation by extracting temporal information from reverberant
signals. The estimated distances are then processed using an extended Kalman
filter to achieve online sound source localization. To the best of our
knowledge, this is the first work to achieve online sound source localization
using a single microphone on a moving robot, a gap that we aim to fill in this
work. Extensive experiments demonstrate the effectiveness and merits of our
approach. To benefit the broader research community, we have open-sourced our
code at https://github.com/JiangWAV/single-mic-SSL.

</details>


### [433] [KARL: Kalman-Filter Assisted Reinforcement Learner for Dynamic Object Tracking and Grasping](https://arxiv.org/abs/2506.15945)
*Kowndinya Boyalakuntla,Abdeslam Boularias,Jingjin Yu*

Main category: cs.RO

TL;DR: KARL是一种结合卡尔曼滤波和强化学习的动态物体跟踪与抓取方法，显著提升了眼在手上系统的性能。


<details>
  <summary>Details</summary>
Motivation: 解决动态物体跟踪和抓取在复杂环境中的挑战，提升系统的鲁棒性和性能。

Method: 采用六阶段强化学习课程、卡尔曼滤波层和失败恢复机制。

Result: 在仿真和实际实验中表现优于现有方法，抓取成功率和执行速度更高。

Conclusion: KARL通过结合卡尔曼滤波和强化学习，显著提升了动态物体抓取的性能。

Abstract: We present Kalman-filter Assisted Reinforcement Learner (KARL) for dynamic
object tracking and grasping over eye-on-hand (EoH) systems, significantly
expanding such systems capabilities in challenging, realistic environments. In
comparison to the previous state-of-the-art, KARL (1) incorporates a novel
six-stage RL curriculum that doubles the system's motion range, thereby greatly
enhancing the system's grasping performance, (2) integrates a robust Kalman
filter layer between the perception and reinforcement learning (RL) control
modules, enabling the system to maintain an uncertain but continuous 6D pose
estimate even when the target object temporarily exits the camera's
field-of-view or undergoes rapid, unpredictable motion, and (3) introduces
mechanisms to allow retries to gracefully recover from unavoidable policy
execution failures. Extensive evaluations conducted in both simulation and
real-world experiments qualitatively and quantitatively corroborate KARL's
advantage over earlier systems, achieving higher grasp success rates and faster
robot execution speed. Source code and supplementary materials for KARL will be
made available at: https://github.com/arc-l/karl.

</details>


### [434] [A Scalable Post-Processing Pipeline for Large-Scale Free-Space Multi-Agent Path Planning with PiBT](https://arxiv.org/abs/2506.16748)
*Arjo Chakravarty,Michael X. Grey,M. A. Viraj J. Muthugala,Mohan Rajesh Elara*

Main category: cs.RO

TL;DR: 提出了一种结合优先级继承回溯（PiBT）和安全感知路径平滑的混合规划框架，用于大规模自由空间多智能体路径规划。


<details>
  <summary>Details</summary>
Motivation: 现有方法在大规模多智能体路径规划中难以兼顾最优性和可扩展性，或依赖网格假设而无法适应连续空间。

Method: 扩展PiBT到8连通网格，结合基于字符串拉动的路径平滑方法，并通过局部交互感知和SIPP回退碰撞解决步骤确保安全性。

Result: 该方法可扩展到500个智能体，在运行时和路径质量上优于现有方法，在稀疏领域生成接近最优的轨迹。

Conclusion: 该框架为机器人系统在非网格约束下的实时多智能体导航提供了可扩展的解决方案。

Abstract: Free-space multi-agent path planning remains challenging at large scales.
Most existing methods either offer optimality guarantees but do not scale
beyond a few dozen agents, or rely on grid-world assumptions that do not
generalize well to continuous space. In this work, we propose a hybrid,
rule-based planning framework that combines Priority Inheritance with
Backtracking (PiBT) with a novel safety-aware path smoothing method. Our
approach extends PiBT to 8-connected grids and selectively applies
string-pulling based smoothing while preserving collision safety through local
interaction awareness and a fallback collision resolution step based on Safe
Interval Path Planning (SIPP). This design allows us to reduce overall path
lengths while maintaining real-time performance. We demonstrate that our method
can scale to over 500 agents in large free-space environments, outperforming
existing any-angle and optimal methods in terms of runtime, while producing
near-optimal trajectories in sparse domains. Our results suggest this framework
is a promising building block for scalable, real-time multi-agent navigation in
robotics systems operating beyond grid constraints.

</details>


### [435] [ViTacFormer: Learning Cross-Modal Representation for Visuo-Tactile Dexterous Manipulation](https://arxiv.org/abs/2506.15953)
*Liang Heng,Haoran Geng,Kaifeng Zhang,Pieter Abbeel,Jitendra Malik*

Main category: cs.RO

TL;DR: ViTacFormer是一种结合视觉和触觉的表示学习方法，通过跨模态融合和预测提升机器人灵巧操作的精度和鲁棒性，在真实任务中表现优于现有技术。


<details>
  <summary>Details</summary>
Motivation: 灵巧操作是机器人系统与物理世界交互的关键能力，视觉方法虽进步快，但触觉传感在精细控制中仍不可或缺。

Method: ViTacFormer采用跨注意力编码器融合视觉与触觉信号，并结合自回归触觉预测头，通过渐进式课程学习优化表示。

Result: 在真实任务中，该方法成功率比现有技术高50%，首次实现长序列灵巧操作任务，持续运行2.5分钟。

Conclusion: ViTacFormer通过跨模态表示学习显著提升机器人灵巧操作的性能，为复杂任务提供了新解决方案。

Abstract: Dexterous manipulation is a cornerstone capability for robotic systems aiming
to interact with the physical world in a human-like manner. Although
vision-based methods have advanced rapidly, tactile sensing remains crucial for
fine-grained control, particularly in unstructured or visually occluded
settings. We present ViTacFormer, a representation-learning approach that
couples a cross-attention encoder to fuse high-resolution vision and touch with
an autoregressive tactile prediction head that anticipates future contact
signals. Building on this architecture, we devise an easy-to-challenging
curriculum that steadily refines the visual-tactile latent space, boosting both
accuracy and robustness. The learned cross-modal representation drives
imitation learning for multi-fingered hands, enabling precise and adaptive
manipulation. Across a suite of challenging real-world benchmarks, our method
achieves approximately 50% higher success rates than prior state-of-the-art
systems. To our knowledge, it is also the first to autonomously complete
long-horizon dexterous manipulation tasks that demand highly precise control
with an anthropomorphic hand, successfully executing up to 11 sequential stages
and sustaining continuous operation for 2.5 minutes.

</details>


### [436] [A Low-Cost Portable Lidar-based Mobile Mapping System on an Android Smartphone](https://arxiv.org/abs/2506.15983)
*Jianzhu Huai,Yuxin Shao,Yujia Zhang,Alper Yilmaz*

Main category: cs.RO

TL;DR: 论文提出了一种低成本、便携的移动测绘系统，结合激光雷达、Android智能手机和RTK-GNSS模块，总成本低于2000美元，重量约1千克。


<details>
  <summary>Details</summary>
Motivation: 当前移动测绘解决方案（如Leica BLK2Go和配备激光雷达的智能手机）成本高或性能有限，需要更经济高效的替代方案。

Method: 系统集成了激光雷达、Android智能手机和RTK-GNSS模块，利用NDK开发激光雷达惯性里程计，并记录多传感器数据。

Result: 系统在2000美元预算内实现了良好的测绘性能，设计及软件已开源。

Conclusion: 该系统为低成本便携测绘提供了可行方案，并通过开源促进社区发展。

Abstract: The rapid advancement of the metaverse, digital twins, and robotics
underscores the demand for low-cost, portable mapping systems for reality
capture. Current mobile solutions, such as the Leica BLK2Go and lidar-equipped
smartphones, either come at a high cost or are limited in range and accuracy.
Leveraging the proliferation and technological evolution of mobile devices
alongside recent advancements in lidar technology, we introduce a novel,
low-cost, portable mobile mapping system. Our system integrates a lidar unit,
an Android smartphone, and an RTK-GNSS stick. Running on the Android platform,
it features lidar-inertial odometry built with the NDK, and logs data from the
lidar, wide-angle camera, IMU, and GNSS. With a total bill of materials (BOM)
cost under 2,000 USD and a weight of about 1 kilogram, the system achieves a
good balance between affordability and portability. We detail the system
design, multisensor calibration, synchronization, and evaluate its performance
for tracking and mapping. To further contribute to the community, the system's
design and software are made open source at:
https://github.com/OSUPCVLab/marslogger_android/releases/tag/v2.1

</details>


### [437] [DualTHOR: A Dual-Arm Humanoid Simulation Platform for Contingency-Aware Planning](https://arxiv.org/abs/2506.16012)
*Boyu Li,Siyuan He,Hang Xu,Haoqi Yuan,Yu Zang,Liwei Hu,Junpeng Yue,Zhenxiong Jiang,Pengbo Hu,Börje F. Karlsson,Yehui Tang,Zongqing Lu*

Main category: cs.RO

TL;DR: 论文提出了一种基于物理的模拟平台DualTHOR，用于复杂双臂人形机器人，旨在解决现有平台在真实世界机器人转移性上的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有模拟平台依赖简化的机器人形态并忽略低级执行的随机性，限制了其在真实世界机器人中的适用性。

Method: 基于扩展版AI2-THOR构建DualTHOR，包含真实机器人资产、双臂协作任务套件及人形机器人逆运动学求解器，并引入基于物理的低级执行故障机制。

Result: 评估显示当前视觉语言模型在双臂协调和真实环境中的鲁棒性表现不佳，凸显了该模拟器的重要性。

Conclusion: DualTHOR为开发更强大的视觉语言模型提供了更全面的评估环境，有助于提升其在真实场景中的表现。

Abstract: Developing embodied agents capable of performing complex interactive tasks in
real-world scenarios remains a fundamental challenge in embodied AI. Although
recent advances in simulation platforms have greatly enhanced task diversity to
train embodied Vision Language Models (VLMs), most platforms rely on simplified
robot morphologies and bypass the stochastic nature of low-level execution,
which limits their transferability to real-world robots. To address these
issues, we present a physics-based simulation platform DualTHOR for complex
dual-arm humanoid robots, built upon an extended version of AI2-THOR. Our
simulator includes real-world robot assets, a task suite for dual-arm
collaboration, and inverse kinematics solvers for humanoid robots. We also
introduce a contingency mechanism that incorporates potential failures through
physics-based low-level execution, bridging the gap to real-world scenarios.
Our simulator enables a more comprehensive evaluation of the robustness and
generalization of VLMs in household environments. Extensive evaluations reveal
that current VLMs struggle with dual-arm coordination and exhibit limited
robustness in realistic environments with contingencies, highlighting the
importance of using our simulator to develop more capable VLMs for embodied
tasks. The code is available at https://github.com/ds199895/DualTHOR.git.

</details>


### [438] [Noise Fusion-based Distillation Learning for Anomaly Detection in Complex Industrial Environments](https://arxiv.org/abs/2506.16050)
*Jiawen Yu,Jieji Ren,Yang Chang,Qiaojun Yu,Xuan Tong,Boyang Wang,Yan Song,You Li,Xinji Mai,Wenqiang Zhang*

Main category: cs.RO

TL;DR: 提出了一种基于异构教师网络（HetNet）的新方法，用于复杂工业环境中的异常检测与定位，性能提升显著。


<details>
  <summary>Details</summary>
Motivation: 现有方法在复杂工业环境中检测工件缺陷存在挑战，需解决多变视角、姿态和光照问题。

Method: 采用异构教师网络（HetNet）、自适应局部-全局特征融合模块和局部多元高斯噪声生成模块。

Result: 在主流基准测试中表现优异，MSC-AD指标提升约10%，并在其他数据集上达到最优。

Conclusion: HetNet能有效应对环境波动，提升工业异常检测系统的可靠性，适用于实时检测。

Abstract: Anomaly detection and localization in automated industrial manufacturing can
significantly enhance production efficiency and product quality. Existing
methods are capable of detecting surface defects in pre-defined or controlled
imaging environments. However, accurately detecting workpiece defects in
complex and unstructured industrial environments with varying views, poses and
illumination remains challenging. We propose a novel anomaly detection and
localization method specifically designed to handle inputs with perturbative
patterns. Our approach introduces a new framework based on a collaborative
distillation heterogeneous teacher network (HetNet), an adaptive local-global
feature fusion module, and a local multivariate Gaussian noise generation
module. HetNet can learn to model the complex feature distribution of normal
patterns using limited information about local disruptive changes. We conducted
extensive experiments on mainstream benchmarks. HetNet demonstrates superior
performance with approximately 10% improvement across all evaluation metrics on
MSC-AD under industrial conditions, while achieving state-of-the-art results on
other datasets, validating its resilience to environmental fluctuations and its
capability to enhance the reliability of industrial anomaly detection systems
across diverse scenarios. Tests in real-world environments further confirm that
HetNet can be effectively integrated into production lines to achieve robust
and real-time anomaly detection. Codes, images and videos are published on the
project website at: https://zihuatanejoyu.github.io/HetNet/

</details>


### [439] [Investigating Lagrangian Neural Networks for Infinite Horizon Planning in Quadrupedal Locomotion](https://arxiv.org/abs/2506.16079)
*Prakrut Kotecha,Aditya Shirwatkar,Shishir Kolathaya*

Main category: cs.RO

TL;DR: LNNs利用归纳偏置学习系统动力学，优于传统方法，在四足机器人中实现高效、准确的预测与控制。


<details>
  <summary>Details</summary>
Motivation: 传统动力学模型在长期预测中误差累积，而LNNs能保持物理规律，适用于可持续运动。

Method: 评估四种LNNs动力学模型：全阶正向动力学、对角化质量矩阵、全阶逆向动力学与降阶模型。

Result: LNNs在样本效率（10倍）和预测精度（2-10倍）上优于基线方法，计算复杂度降低且保持可解释性。

Conclusion: LNNs能有效捕捉四足机器人动力学结构，提升运动规划与控制性能，适合实时部署。

Abstract: Lagrangian Neural Networks (LNNs) present a principled and interpretable
framework for learning the system dynamics by utilizing inductive biases. While
traditional dynamics models struggle with compounding errors over long
horizons, LNNs intrinsically preserve the physical laws governing any system,
enabling accurate and stable predictions essential for sustainable locomotion.
This work evaluates LNNs for infinite horizon planning in quadrupedal robots
through four dynamics models: (1) full-order forward dynamics (FD) training and
inference, (2) diagonalized representation of Mass Matrix in full order FD, (3)
full-order inverse dynamics (ID) training with FD inference, (4) reduced-order
modeling via torso centre-of-mass (CoM) dynamics. Experiments demonstrate that
LNNs bring improvements in sample efficiency (10x) and superior prediction
accuracy (up to 2-10x) compared to baseline methods. Notably, the
diagonalization approach of LNNs reduces computational complexity while
retaining some interpretability, enabling real-time receding horizon control.
These findings highlight the advantages of LNNs in capturing the underlying
structure of system dynamics in quadrupeds, leading to improved performance and
efficiency in locomotion planning and control. Additionally, our approach
achieves a higher control frequency than previous LNN methods, demonstrating
its potential for real-world deployment on quadrupeds.

</details>


### [440] [From Theory to Practice: Identifying the Optimal Approach for Offset Point Tracking in the Context of Agricultural Robotics](https://arxiv.org/abs/2506.16143)
*Stephane Ngnepiepaye Wembe,Vincent Rousseau,Johann Laconte,Roland Lenain*

Main category: cs.RO

TL;DR: 本文提出了一种针对农业机器人工具的预测控制策略，通过预测工具的运动来改善跟踪性能，解决了传统控制方法忽视工具实际工作点的问题。


<details>
  <summary>Details</summary>
Motivation: 现代农业面临劳动力短缺和环境压力，农业机器人成为解决方案。然而，现有控制策略多关注机器人本体，忽视工具的实际工作点，影响任务效果。

Method: 提出一种预测控制策略，专注于工具参考点，通过预测工具运动来优化跟踪性能，避免转弯时的过度偏移。

Result: 该方法显著提高了工具在非直线作物行中的跟踪性能，减少了转弯时的过度偏移。

Conclusion: 通过专注于工具参考点的预测控制策略，可以有效提升农业机器人在复杂环境中的操作精度。

Abstract: Modern agriculture faces escalating challenges: increasing demand for food,
labor shortages, and the urgent need to reduce environmental impact.
Agricultural robotics has emerged as a promising response to these pressures,
enabling the automation of precise and suitable field operations. In
particular, robots equipped with implements for tasks such as weeding or sowing
must interact delicately and accurately with the crops and soil. Unlike robots
in other domains, these agricultural platforms typically use rigidly mounted
implements, where the implement's position is more critical than the robot's
center in determining task success. Yet, most control strategies in the
literature focus on the vehicle body, often neglecting the acctual working
point of the system. This is particularly important when considering new
agriculture practices where crops row are not necessary straights. This paper
presents a predictive control strategy targeting the implement's reference
point. The method improves tracking performance by anticipating the motion of
the implement, which, due to its offset from the vehicle's center of rotation,
is prone to overshooting during turns if not properly accounted for.

</details>


### [441] [FlowRAM: Grounding Flow Matching Policy with Region-Aware Mamba Framework for Robotic Manipulation](https://arxiv.org/abs/2506.16201)
*Sen Wang,Le Wang,Sanping Zhou,Jingyi Tian,Jiayi Li,Haowen Sun,Wei Tang*

Main category: cs.RO

TL;DR: FlowRAM提出了一种基于生成模型的新框架，通过动态半径调度和条件流匹配，显著提升了高精度机器人操作的效率和性能。


<details>
  <summary>Details</summary>
Motivation: 当前基于扩散的策略学习方法在推理时计算效率低，且未充分利用生成模型在3D环境中的信息探索潜力。

Method: FlowRAM采用动态半径调度实现自适应感知，结合状态空间模型整合多模态信息，并使用条件流匹配学习动作位姿。

Result: 在RLBench基准测试中，FlowRAM平均成功率提升12.0%，推理速度显著提高（少于4步）。

Conclusion: FlowRAM在高精度任务中表现出色，为机器人操作提供了高效且性能优越的解决方案。

Abstract: Robotic manipulation in high-precision tasks is essential for numerous
industrial and real-world applications where accuracy and speed are required.
Yet current diffusion-based policy learning methods generally suffer from low
computational efficiency due to the iterative denoising process during
inference. Moreover, these methods do not fully explore the potential of
generative models for enhancing information exploration in 3D environments. In
response, we propose FlowRAM, a novel framework that leverages generative
models to achieve region-aware perception, enabling efficient multimodal
information processing. Specifically, we devise a Dynamic Radius Schedule,
which allows adaptive perception, facilitating transitions from global scene
comprehension to fine-grained geometric details. Furthermore, we integrate
state space models to integrate multimodal information, while preserving linear
computational complexity. In addition, we employ conditional flow matching to
learn action poses by regressing deterministic vector fields, simplifying the
learning process while maintaining performance. We verify the effectiveness of
the FlowRAM in the RLBench, an established manipulation benchmark, and achieve
state-of-the-art performance. The results demonstrate that FlowRAM achieves a
remarkable improvement, particularly in high-precision tasks, where it
outperforms previous methods by 12.0% in average success rate. Additionally,
FlowRAM is able to generate physically plausible actions for a variety of
real-world tasks in less than 4 time steps, significantly increasing inference
speed.

</details>


### [442] [ControlVLA: Few-shot Object-centric Adaptation for Pre-trained Vision-Language-Action Models](https://arxiv.org/abs/2506.16211)
*Puhao Li,Yingying Wu,Ziheng Xi,Wanlin Li,Yuzhe Huang,Zhiyuan Zhang,Yinghan Chen,Jianan Wang,Song-Chun Zhu,Tengyu Liu,Siyuan Huang*

Main category: cs.RO

TL;DR: ControlVLA是一种新型框架，通过结合预训练的VLA模型和对象中心表示，实现了在少量演示下的高效微调，显著提升了机器人操作的性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法在少量演示下表现不佳，且难以适应特定任务。ControlVLA旨在解决这些问题，提供更高效和可扩展的解决方案。

Method: 采用ControlNet风格的架构，通过零初始化投影层，逐步调整预训练的策略，引入对象中心条件。

Result: 在6项任务中，仅需10-20次演示即达到76.7%的成功率，优于传统方法。

Conclusion: ControlVLA在少量数据下表现出色，具有扩展性和鲁棒性，适用于复杂任务和新场景。

Abstract: Learning real-world robotic manipulation is challenging, particularly when
limited demonstrations are available. Existing methods for few-shot
manipulation often rely on simulation-augmented data or pre-built modules like
grasping and pose estimation, which struggle with sim-to-real gaps and lack
extensibility. While large-scale imitation pre-training shows promise, adapting
these general-purpose policies to specific tasks in data-scarce settings
remains unexplored. To achieve this, we propose ControlVLA, a novel framework
that bridges pre-trained VLA models with object-centric representations via a
ControlNet-style architecture for efficient fine-tuning. Specifically, to
introduce object-centric conditions without overwriting prior knowledge,
ControlVLA zero-initializes a set of projection layers, allowing them to
gradually adapt the pre-trained manipulation policies. In real-world
experiments across 6 diverse tasks, including pouring cubes and folding
clothes, our method achieves a 76.7% success rate while requiring only 10-20
demonstrations -- a significant improvement over traditional approaches that
require more than 100 demonstrations to achieve comparable success. Additional
experiments highlight ControlVLA's extensibility to long-horizon tasks and
robustness to unseen objects and backgrounds.

</details>


### [443] [Probabilistic Collision Risk Estimation for Pedestrian Navigation](https://arxiv.org/abs/2506.16219)
*Amine Tourki,Paul Prevel,Nils Einecke,Tim Puphal,Alexandre Alahi*

Main category: cs.RO

TL;DR: 将用于自动驾驶的风险模型技术应用于视觉障碍辅助设备，实验显示其警告准确率（67%）优于距离和时间接触测量（51%）。


<details>
  <summary>Details</summary>
Motivation: 智能设备在视觉障碍辅助领域落后于智能驾驶辅助系统的发展，因此探索将自动驾驶中的风险模型技术应用于视觉障碍辅助设备。

Method: 将风险模型技术整合到视觉障碍辅助设备中，通过计算物体轨迹的概率碰撞风险来提供警告。

Result: 风险模型的警告准确率为67%，而距离和时间接触测量的准确率仅为51%。

Conclusion: 风险模型在视觉障碍辅助设备中表现优于传统方法，具有实际应用潜力。

Abstract: Intelligent devices for supporting persons with vision impairment are
becoming more widespread, but they are lacking behind the advancements in
intelligent driver assistant system. To make a first step forward, this work
discusses the integration of the risk model technology, previously used in
autonomous driving and advanced driver assistance systems, into an assistance
device for persons with vision impairment. The risk model computes a
probabilistic collision risk given object trajectories which has previously
been shown to give better indications of an object's collision potential
compared to distance or time-to-contact measures in vehicle scenarios. In this
work, we show that the risk model is also superior in warning persons with
vision impairment about dangerous objects. Our experiments demonstrate that the
warning accuracy of the risk model is 67% while both distance and
time-to-contact measures reach only 51% accuracy for real-world data.

</details>


### [444] [CapsDT: Diffusion-Transformer for Capsule Robot Manipulation](https://arxiv.org/abs/2506.16263)
*Xiting He,Mingwu Su,Xinqi Jiang,Long Bai,Jiewen Lai,Hongliang Ren*

Main category: cs.RO

TL;DR: CapsDT是一种基于扩散变换器的模型，用于胶囊机器人在胃部的操作，通过处理视觉和文本输入生成控制信号，提升内窥镜任务的效率。


<details>
  <summary>Details</summary>
Motivation: 探索VLA模型在内窥镜胶囊机器人中的应用，以提升人机交互的直观性和效率，改善诊断和治疗效果。

Method: 设计CapsDT模型，结合视觉和文本输入生成机器人控制信号，并开发胶囊内窥镜机器人系统。

Result: CapsDT在多种内窥镜任务中表现优异，真实模拟操作成功率为26.25%。

Conclusion: CapsDT作为视觉-语言通用模型，在内窥镜任务中展现出先进性能，具有实际应用潜力。

Abstract: Vision-Language-Action (VLA) models have emerged as a prominent research
area, showcasing significant potential across a variety of applications.
However, their performance in endoscopy robotics, particularly endoscopy
capsule robots that perform actions within the digestive system, remains
unexplored. The integration of VLA models into endoscopy robots allows more
intuitive and efficient interactions between human operators and medical
devices, improving both diagnostic accuracy and treatment outcomes. In this
work, we design CapsDT, a Diffusion Transformer model for capsule robot
manipulation in the stomach. By processing interleaved visual inputs, and
textual instructions, CapsDT can infer corresponding robotic control signals to
facilitate endoscopy tasks. In addition, we developed a capsule endoscopy robot
system, a capsule robot controlled by a robotic arm-held magnet, addressing
different levels of four endoscopy tasks and creating corresponding capsule
robot datasets within the stomach simulator. Comprehensive evaluations on
various robotic tasks indicate that CapsDT can serve as a robust
vision-language generalist, achieving state-of-the-art performance in various
levels of endoscopy tasks while achieving a 26.25% success rate in real-world
simulation manipulation.

</details>


### [445] [M-Predictive Spliner: Enabling Spatiotemporal Multi-Opponent Overtaking for Autonomous Racing](https://arxiv.org/abs/2506.16301)
*Nadine Imholz,Maurice Brunner,Nicolas Baumann,Edoardo Ghignone,Michele Magno*

Main category: cs.RO

TL;DR: 本文提出了一种考虑对手未来意图的多对手赛车决策方法，通过KF跟踪器和GPR预测对手轨迹，实验验证了其高效性和安全性。


<details>
  <summary>Details</summary>
Motivation: 多对手赛车决策在机器人极限操作下具有挑战性，现有方法未能充分利用时空信息或仅限于单对手场景。

Method: 采用KF多对手跟踪器进行对手重识别，结合GPR预测对手轨迹，计算超车策略。

Result: 在1:10比例赛车实验中，超车成功率达91.65%，安全性比SotA提升10.13%。

Conclusion: 该方法在高性能自主赛车中具有潜力。

Abstract: Unrestricted multi-agent racing presents a significant research challenge,
requiring decision-making at the limits of a robot's operational capabilities.
While previous approaches have either ignored spatiotemporal information in the
decision-making process or been restricted to single-opponent scenarios, this
work enables arbitrary multi-opponent head-to-head racing while considering the
opponents' future intent. The proposed method employs a KF-based multi-opponent
tracker to effectively perform opponent ReID by associating them across
observations. Simultaneously, spatial and velocity GPR is performed on all
observed opponent trajectories, providing predictive information to compute the
overtaking maneuvers. This approach has been experimentally validated on a
physical 1:10 scale autonomous racing car, achieving an overtaking success rate
of up to 91.65% and demonstrating an average 10.13%-point improvement in safety
at the same speed as the previous SotA. These results highlight its potential
for high-performance autonomous racing.

</details>


### [446] [Comparison between External and Internal Single Stage Planetary gearbox actuators for legged robots](https://arxiv.org/abs/2506.16356)
*Aman Singh,Deepak Kapa,Prasham Chedda,Shishir N. Y. Kolathaya*

Main category: cs.RO

TL;DR: 本文提出了一种设计框架，用于优化选择执行器参数，比较了两种行星齿轮箱架构（ISSPG和ESSPG），并验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 目前缺乏对ISSPG和ESSPG两种行星齿轮箱架构的客观比较，且现有设计多依赖启发式方法而非系统优化。

Method: 提出了一种基于性能需求和电机规格的设计框架，生成并分析两种架构的优化齿轮箱设计。

Result: 对于T-motor U12，ISSPG在5:1至7:1的齿轮比范围内更优，而ESSPG在7:1至11:1范围内更优。优化设计的执行器质量与预测一致。

Conclusion: 提出的设计框架有效，能够为特定需求选择最优的齿轮箱架构。

Abstract: Legged robots, such as quadrupeds and humanoids, require high-performance
actuators for efficient locomotion. Quasi-Direct-Drive (QDD) actuators with
single-stage planetary gearboxes offer low inertia, high efficiency, and
transparency. Among planetary gearbox architectures, Internal (ISSPG) and
External Single-Stage Planetary Gearbox (ESSPG) are the two predominant
designs. While ISSPG is often preferred for its compactness and high torque
density at certain gear ratios, no objective comparison between the two
architectures exists. Additionally, existing designs rely on heuristics rather
than systematic optimization. This paper presents a design framework for
optimally selecting actuator parameters based on given performance requirements
and motor specifications. Using this framework, we generate and analyze various
optimized gearbox designs for both architectures. Our results demonstrate that
for the T-motor U12, ISSPG is the superior choice within the lower gear ratio
range of 5:1 to 7:1, offering a lighter design. However, for gear ratios
exceeding 7:1, ISSPG becomes infeasible, making ESSPG the better option in the
7:1 to 11:1 range. To validate our approach, we designed and optimized two
actuators for manufacturing: an ISSPG with a 6.0:1 gear ratio and an ESSPG with
a 7.2:1 gear ratio. Their respective masses closely align with our optimization
model predictions, confirming the effectiveness of our methodology.

</details>


### [447] [CSC-MPPI: A Novel Constrained MPPI Framework with DBSCAN for Reliable Obstacle Avoidance](https://arxiv.org/abs/2506.16386)
*Leesai Park,Keunwoo Jang,Sanghyun Kim*

Main category: cs.RO

TL;DR: CSC-MPPI是一种改进的MPPI方法，通过结合原始-对偶梯度法和DBSCAN聚类，在轨迹优化中严格满足约束条件，提升性能。


<details>
  <summary>Details</summary>
Motivation: 传统MPPI在约束满足和轨迹优化上表现不佳，CSC-MPPI旨在解决这些问题。

Method: 结合原始-对偶梯度法确保约束满足，使用DBSCAN聚类选择代表性控制输入。

Result: 实验表明CSC-MPPI在避障和效率上优于传统MPPI。

Conclusion: CSC-MPPI显著提升了约束满足和轨迹优化的性能。

Abstract: This paper proposes Constrained Sampling Cluster Model Predictive Path
Integral (CSC-MPPI), a novel constrained formulation of MPPI designed to
enhance trajectory optimization while enforcing strict constraints on system
states and control inputs. Traditional MPPI, which relies on a probabilistic
sampling process, often struggles with constraint satisfaction and generates
suboptimal trajectories due to the weighted averaging of sampled trajectories.
To address these limitations, the proposed framework integrates a primal-dual
gradient-based approach and Density-Based Spatial Clustering of Applications
with Noise (DBSCAN) to steer sampled input trajectories into feasible regions
while mitigating risks associated with weighted averaging. First, to ensure
that sampled trajectories remain within the feasible region, the primal-dual
gradient method is applied to iteratively shift sampled inputs while enforcing
state and control constraints. Then, DBSCAN groups the sampled trajectories,
enabling the selection of representative control inputs within each cluster.
Finally, among the representative control inputs, the one with the lowest cost
is chosen as the optimal action. As a result, CSC-MPPI guarantees constraint
satisfaction, improves trajectory selection, and enhances robustness in complex
environments. Simulation and real-world experiments demonstrate that CSC-MPPI
outperforms traditional MPPI in obstacle avoidance, achieving improved
reliability and efficiency. The experimental videos are available at
https://cscmppi.github.io

</details>


### [448] [Full-Pose Tracking via Robust Control for Over-Actuated Multirotors](https://arxiv.org/abs/2506.16427)
*Mohamad Hachem,Clément Roos,Thierry Miquel,Murat Bronz*

Main category: cs.RO

TL;DR: 提出了一种用于过驱动多旋翼的级联控制架构，结合INDI和结构化H∞控制，通过几何引导控制分配实现精确跟踪。


<details>
  <summary>Details</summary>
Motivation: 扩展INDI和结构化H∞控制的应用范围，解决过驱动多旋翼的姿态和位置跟踪问题。

Method: 采用加权最小二乘几何引导控制分配方法，并将其建模为二次优化问题。

Result: 数值模拟验证了方法的有效性，展示了其在多样化任务中的适应性和实际应用潜力。

Conclusion: 该方法成功解决了不可行姿态参考和抗干扰鲁棒性等关键挑战，适用于实际飞行应用。

Abstract: This paper presents a robust cascaded control architecture for over-actuated
multirotors. It extends the Incremental Nonlinear Dynamic Inversion (INDI)
control combined with structured H_inf control, initially proposed for
under-actuated multirotors, to a broader range of multirotor configurations. To
achieve precise and robust attitude and position tracking, we employ a weighted
least-squares geometric guidance control allocation method, formulated as a
quadratic optimization problem, enabling full-pose tracking. The proposed
approach effectively addresses key challenges, such as preventing infeasible
pose references and enhancing robustness against disturbances, as well as
considering multirotor's actual physical limitations. Numerical simulations
with an over-actuated hexacopter validate the method's effectiveness,
demonstrating its adaptability to diverse mission scenarios and its potential
for real-world aerial applications.

</details>


### [449] [Human2LocoMan: Learning Versatile Quadrupedal Manipulation with Human Pretraining](https://arxiv.org/abs/2506.16475)
*Yaru Niu,Yunzhe Zhang,Mingyang Yu,Changyi Lin,Chenhao Li,Yikai Wang,Yuxiang Yang,Wenhao Yu,Tingnan Zhang,Bingqing Chen,Jonathan Francis,Zhenzhen Li,Jie Tan,Ding Zhao*

Main category: cs.RO

TL;DR: 提出了一种跨体现模仿学习系统，用于四足机器人的多功能操作，通过结合人类和机器人数据，显著提升了任务成功率。


<details>
  <summary>Details</summary>
Motivation: 解决四足机器人多功能操作的自主性和可扩展性问题。

Method: 开发了统一的遥操作和数据收集流程，提出模块化架构支持跨体现数据的联合训练和预训练。

Result: 在六项真实任务中，平均成功率提升41.9%，OOD设置下提升79.7%；预训练人类数据贡献显著。

Conclusion: 系统通过结合人类和机器人数据，显著提升了四足机器人的操作能力，且开源了代码和数据。

Abstract: Quadrupedal robots have demonstrated impressive locomotion capabilities in
complex environments, but equipping them with autonomous versatile manipulation
skills in a scalable way remains a significant challenge. In this work, we
introduce a cross-embodiment imitation learning system for quadrupedal
manipulation, leveraging data collected from both humans and LocoMan, a
quadruped equipped with multiple manipulation modes. Specifically, we develop a
teleoperation and data collection pipeline, which unifies and modularizes the
observation and action spaces of the human and the robot. To effectively
leverage the collected data, we propose an efficient modularized architecture
that supports co-training and pretraining on structured modality-aligned data
across different embodiments. Additionally, we construct the first manipulation
dataset for the LocoMan robot, covering various household tasks in both
unimanual and bimanual modes, supplemented by a corresponding human dataset. We
validate our system on six real-world manipulation tasks, where it achieves an
average success rate improvement of 41.9% overall and 79.7% under
out-of-distribution (OOD) settings compared to the baseline. Pretraining with
human data contributes a 38.6% success rate improvement overall and 82.7% under
OOD settings, enabling consistently better performance with only half the
amount of robot data. Our code, hardware, and data are open-sourced at:
https://human2bots.github.io.

</details>


### [450] [Grounding Language Models with Semantic Digital Twins for Robotic Planning](https://arxiv.org/abs/2506.16493)
*Mehreen Naeem,Andrew Melnik,Michael Beetz*

Main category: cs.RO

TL;DR: 提出了一种结合语义数字孪生（SDTs）与大语言模型（LLMs）的框架，用于动态环境中机器人任务的适应性执行。


<details>
  <summary>Details</summary>
Motivation: 解决机器人在动态环境中执行任务时的高层推理与语义环境理解的结合问题。

Method: 将自然语言指令分解为结构化动作三元组，并通过SDT提供的环境数据进行语义接地，利用LLM生成恢复策略。

Result: 在ALFRED基准测试中表现稳健，能可靠完成任务。

Conclusion: 该框架成功结合了高层推理与语义理解，适应不确定性和失败情况。

Abstract: We introduce a novel framework that integrates Semantic Digital Twins (SDTs)
with Large Language Models (LLMs) to enable adaptive and goal-driven robotic
task execution in dynamic environments. The system decomposes natural language
instructions into structured action triplets, which are grounded in contextual
environmental data provided by the SDT. This semantic grounding allows the
robot to interpret object affordances and interaction rules, enabling action
planning and real-time adaptability. In case of execution failures, the LLM
utilizes error feedback and SDT insights to generate recovery strategies and
iteratively revise the action plan. We evaluate our approach using tasks from
the ALFRED benchmark, demonstrating robust performance across various household
scenarios. The proposed framework effectively combines high-level reasoning
with semantic environment understanding, achieving reliable task completion in
the face of uncertainty and failure.

</details>


### [451] [Agile, Autonomous Spacecraft Constellations with Disruption Tolerant Networking to Monitor Precipitation and Urban Floods](https://arxiv.org/abs/2506.16537)
*Sreeja Roy-Singh,Alan P. Li,Vinay Ravindra,Roderick Lammers,Marc Sanchez Net*

Main category: cs.RO

TL;DR: 论文提出了一种结合轨道力学、姿态控制和卫星间通信的算法框架，用于调度敏捷小型卫星星座的时变重定向，显著提升对瞬态或演化现象的响应能力。


<details>
  <summary>Details</summary>
Motivation: 利用商业技术支持的全向小型航天器，结合改进的机载处理和卫星间通信，提升对瞬态或演化现象（如降水与城市洪水）的观测能力。

Method: 开发了基于地面和机载的算法框架，结合轨道力学、姿态控制、卫星间通信、智能预测与规划，动态调度卫星星座的重定向。

Result: 在24颗卫星的星座中，信息交换延迟低（平均在可用时间的1/3内），机载调度器比地面实现多观测7%的洪水强度，且比非敏捷星座性能提升约98%。

Conclusion: 该框架通过智能调度和卫星间协作，显著提升了星座对动态现象的观测效率和响应能力。

Abstract: Fully re-orientable small spacecraft are now supported by commercial
technologies, allowing them to point their instruments in any direction and
capture images, with short notice. When combined with improved onboard
processing, and implemented on a constellation of inter-communicable
satellites, this intelligent agility can significantly increase responsiveness
to transient or evolving phenomena. We demonstrate a ground-based and onboard
algorithmic framework that combines orbital mechanics, attitude control,
inter-satellite communication, intelligent prediction and planning to schedule
the time-varying, re-orientation of agile, small satellites in a constellation.
Planner intelligence is improved by updating the predictive value of future
space-time observations based on shared observations of evolving episodic
precipitation and urban flood forecasts. Reliable inter-satellite communication
within a fast, dynamic constellation topology is modeled in the physical,
access control and network layer. We apply the framework on a representative
24-satellite constellation observing 5 global regions. Results show
appropriately low latency in information exchange (average within 1/3rd
available time for implicit consensus), enabling the onboard scheduler to
observe ~7% more flood magnitude than a ground-based implementation. Both
onboard and offline versions performed ~98% better than constellations without
agility.

</details>


### [452] [BIDA: A Bi-level Interaction Decision-making Algorithm for Autonomous Vehicles in Dynamic Traffic Scenarios](https://arxiv.org/abs/2506.16546)
*Liyang Yu,Tianyi Wang,Junfeng Jiao,Fengwu Shan,Hongqing Chu,Bingzhao Gao*

Main category: cs.RO

TL;DR: 论文提出了一种双层交互决策算法（BIDA），结合交互式蒙特卡洛树搜索（MCTS）和深度强化学习（DRL），以提升自动驾驶车辆在动态交通场景中的交互合理性、效率和安全性。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶车辆在复杂交通环境中需实时与人类参与者互动，但人类行为的不可预测性带来了挑战，尤其是在多车道高速和无信号T型路口等动态场景中。

Method: 采用三种DRL算法构建可靠的价值网络和策略网络，指导交互式MCTS的在线推理过程，并通过动态轨迹规划器和轨迹跟踪控制器在CARLA中实现平滑执行。

Result: 实验表明，BIDA不仅提升了交互推理能力并降低了计算成本，还在不同交通条件下表现出优于其他基准的安全性和效率。

Conclusion: BIDA算法在动态交通场景中显著提升了自动驾驶车辆的交互合理性、效率和安全性。

Abstract: In complex real-world traffic environments, autonomous vehicles (AVs) need to
interact with other traffic participants while making real-time and
safety-critical decisions accordingly. The unpredictability of human behaviors
poses significant challenges, particularly in dynamic scenarios, such as
multi-lane highways and unsignalized T-intersections. To address this gap, we
design a bi-level interaction decision-making algorithm (BIDA) that integrates
interactive Monte Carlo tree search (MCTS) with deep reinforcement learning
(DRL), aiming to enhance interaction rationality, efficiency and safety of AVs
in dynamic key traffic scenarios. Specifically, we adopt three types of DRL
algorithms to construct a reliable value network and policy network, which
guide the online deduction process of interactive MCTS by assisting in value
update and node selection. Then, a dynamic trajectory planner and a trajectory
tracking controller are designed and implemented in CARLA to ensure smooth
execution of planned maneuvers. Experimental evaluations demonstrate that our
BIDA not only enhances interactive deduction and reduces computational costs,
but also outperforms other latest benchmarks, which exhibits superior safety,
efficiency and interaction rationality under varying traffic conditions.

</details>


### [453] [An Optimization-Augmented Control Framework for Single and Coordinated Multi-Arm Robotic Manipulation](https://arxiv.org/abs/2506.16555)
*Melih Özcan,Ozgur S. Oguz*

Main category: cs.RO

TL;DR: 提出了一种结合力控制和优化运动规划的多模态控制框架，用于复杂机器人操作任务。


<details>
  <summary>Details</summary>
Motivation: 力控制在高频适应和柔顺交互中表现优异，但在远距离运动和稳定方向上受限；优化运动规划擅长生成无碰撞轨迹，但在动态交互中表现不足。

Method: 将任务分解为子任务，动态分配三种控制模式：纯优化、纯力控制或混合控制。

Result: 在单臂、双臂和多臂操作任务中展示了方法的鲁棒性和精确性。

Conclusion: 该框架能有效处理自由空间运动和接触丰富的操作任务。

Abstract: Robotic manipulation demands precise control over both contact forces and
motion trajectories. While force control is essential for achieving compliant
interaction and high-frequency adaptation, it is limited to operations in close
proximity to the manipulated object and often fails to maintain stable
orientation during extended motion sequences. Conversely, optimization-based
motion planning excels in generating collision-free trajectories over the
robot's configuration space but struggles with dynamic interactions where
contact forces play a crucial role. To address these limitations, we propose a
multi-modal control framework that combines force control and
optimization-augmented motion planning to tackle complex robotic manipulation
tasks in a sequential manner, enabling seamless switching between control modes
based on task requirements. Our approach decomposes complex tasks into
subtasks, each dynamically assigned to one of three control modes: Pure
optimization for global motion planning, pure force control for precise
interaction, or hybrid control for tasks requiring simultaneous trajectory
tracking and force regulation. This framework is particularly advantageous for
bimanual and multi-arm manipulation, where synchronous motion and coordination
among arms are essential while considering both the manipulated object and
environmental constraints. We demonstrate the versatility of our method through
a range of long-horizon manipulation tasks, including single-arm, bimanual, and
multi-arm applications, highlighting its ability to handle both free-space
motion and contact-rich manipulation with robustness and precision.

</details>


### [454] [Reimagination with Test-time Observation Interventions: Distractor-Robust World Model Predictions for Visual Model Predictive Control](https://arxiv.org/abs/2506.16565)
*Yuxin Chen,Jianglan Wei,Chenfeng Xu,Boyi Li,Masayoshi Tomizuka,Andrea Bajcsy,Ran Tian*

Main category: cs.RO

TL;DR: 论文提出ReOI方法，通过检测并移除视觉干扰物，提升世界模型在开放场景中的预测可靠性。


<details>
  <summary>Details</summary>
Motivation: 世界模型在遇到训练中未见的视觉干扰物时表现脆弱，影响机器人规划和动作验证。

Method: 提出ReOI方法，包括检测干扰物、修改观测、重新预测并恢复干扰物。

Result: ReOI显著提升任务成功率，在新型干扰物下表现优于未干预方法。

Conclusion: ReOI是一种简单有效的测试时策略，能增强世界模型在开放世界中的鲁棒性。

Abstract: World models enable robots to "imagine" future observations given current
observations and planned actions, and have been increasingly adopted as
generalized dynamics models to facilitate robot learning. Despite their
promise, these models remain brittle when encountering novel visual distractors
such as objects and background elements rarely seen during training.
Specifically, novel distractors can corrupt action outcome predictions, causing
downstream failures when robots rely on the world model imaginations for
planning or action verification. In this work, we propose Reimagination with
Observation Intervention (ReOI), a simple yet effective test-time strategy that
enables world models to predict more reliable action outcomes in open-world
scenarios where novel and unanticipated visual distractors are inevitable.
Given the current robot observation, ReOI first detects visual distractors by
identifying which elements of the scene degrade in physically implausible ways
during world model prediction. Then, it modifies the current observation to
remove these distractors and bring the observation closer to the training
distribution. Finally, ReOI "reimagines" future outcomes with the modified
observation and reintroduces the distractors post-hoc to preserve visual
consistency for downstream planning and verification. We validate our approach
on a suite of robotic manipulation tasks in the context of action verification,
where the verifier needs to select desired action plans based on predictions
from a world model. Our results show that ReOI is robust to both
in-distribution and out-of-distribution visual distractors. Notably, it
improves task success rates by up to 3x in the presence of novel distractors,
significantly outperforming action verification that relies on world model
predictions without imagination interventions.

</details>


### [455] [DRIVE Through the Unpredictability:From a Protocol Investigating Slip to a Metric Estimating Command Uncertainty](https://arxiv.org/abs/2506.16593)
*Nicolas Samson,William Larrivée-Hardy,William Dubois,Élie Roy-Brouard,Edith Brotherton,Dominic Baril,Julien Lépine,François Pomerleau*

Main category: cs.RO

TL;DR: 论文提出使用DRIVE协议标准化数据收集，以识别和表征UGV的滑移状态空间，并通过实验验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 越野自主导航依赖于运动模型的准确性，但运动模型对地形与UGV交互的预测能力有限，需要标准化数据收集方法。

Method: 使用DRIVE协议收集数据，验证了两种平台在六种地形上的数据集，评估协议对速度命令空间的探索能力，并提出不可预测性度量。

Result: 实验验证了DRIVE协议的有效性，提出了速度命令空间与稳态滑移之间的传递函数，并分享了大型UGV系统识别的经验。

Conclusion: DRIVE协议为UGV系统识别和风险评估提供了标准化方法，有助于社区研究和部署。

Abstract: Off-road autonomous navigation is a challenging task as it is mainly
dependent on the accuracy of the motion model. Motion model performances are
limited by their ability to predict the interaction between the terrain and the
UGV, which an onboard sensor can not directly measure. In this work, we propose
using the DRIVE protocol to standardize the collection of data for system
identification and characterization of the slip state space. We validated this
protocol by acquiring a dataset with two platforms (from 75 kg to 470 kg) on
six terrains (i.e., asphalt, grass, gravel, ice, mud, sand) for a total of 4.9
hours and 14.7 km. Using this data, we evaluate the DRIVE protocol's ability to
explore the velocity command space and identify the reachable velocities for
terrain-robot interactions. We investigated the transfer function between the
command velocity space and the resulting steady-state slip for an SSMR. An
unpredictability metric is proposed to estimate command uncertainty and help
assess risk likelihood and severity in deployment. Finally, we share our
lessons learned on running system identification on large UGV to help the
community.

</details>


### [456] [History-Augmented Vision-Language Models for Frontier-Based Zero-Shot Object Navigation](https://arxiv.org/abs/2506.16623)
*Mobin Habibpour,Fatemeh Afghah*

Main category: cs.RO

TL;DR: 论文提出了一种基于动态历史感知提示的零样本目标导航框架，通过深度整合视觉语言模型（VLM）推理，提升机器人在未知环境中寻找目标的能力。


<details>
  <summary>Details</summary>
Motivation: 当前目标导航方法对视觉语言模型（VLM）的使用较浅，仅用于对象-场景相似性检查，缺乏深度推理，导致上下文理解不足和重复导航行为。

Method: 采用动态、历史感知提示策略，为VLM提供动作历史上下文，生成语义导航评分，并引入VLM辅助的路径点生成机制。

Result: 在HM3D数据集上测试，成功率为46%，路径长度加权成功率为24.8%，与现有零样本方法相当。

Conclusion: 历史增强的VLM提示策略显著提升了导航的鲁棒性和上下文感知能力。

Abstract: Object Goal Navigation (ObjectNav) challenges robots to find objects in
unseen environments, demanding sophisticated reasoning. While Vision-Language
Models (VLMs) show potential, current ObjectNav methods often employ them
superficially, primarily using vision-language embeddings for object-scene
similarity checks rather than leveraging deeper reasoning. This limits
contextual understanding and leads to practical issues like repetitive
navigation behaviors. This paper introduces a novel zero-shot ObjectNav
framework that pioneers the use of dynamic, history-aware prompting to more
deeply integrate VLM reasoning into frontier-based exploration. Our core
innovation lies in providing the VLM with action history context, enabling it
to generate semantic guidance scores for navigation actions while actively
avoiding decision loops. We also introduce a VLM-assisted waypoint generation
mechanism for refining the final approach to detected objects. Evaluated on the
HM3D dataset within Habitat, our approach achieves a 46% Success Rate (SR) and
24.8% Success weighted by Path Length (SPL). These results are comparable to
state-of-the-art zero-shot methods, demonstrating the significant potential of
our history-augmented VLM prompting strategy for more robust and context-aware
robotic navigation.

</details>


### [457] [See What I Mean? Expressiveness and Clarity in Robot Display Design](https://arxiv.org/abs/2506.16643)
*Matthew Ebisu,Hang Yu,Reuben Aronson,Elaine Short*

Main category: cs.RO

TL;DR: 研究探讨了非语言视觉符号在人与机器人协作中的作用，发现动画显示能提升信任感，而静态图标更易理解。


<details>
  <summary>Details</summary>
Motivation: 研究动机是探索不同类型的非语言提示如何影响动态环境中人与机器人协作的任务表现。

Method: 设计了一个协作导航任务，参与者与机器人通过部分地图信息沟通，比较了动画与静态显示的效果。

Result: 动画显示提升信任和满意度，静态图标更易理解，静态眼睛显示的任务完成率最高。

Conclusion: 动画能增强信任，但结合熟悉的静态图标可以优化人机沟通。

Abstract: Nonverbal visual symbols and displays play an important role in communication
when humans and robots work collaboratively. However, few studies have
investigated how different types of non-verbal cues affect objective task
performance, especially in a dynamic environment that requires real time
decision-making. In this work, we designed a collaborative navigation task
where the user and the robot only had partial information about the map on each
end and thus the users were forced to communicate with a robot to complete the
task. We conducted our study in a public space and recruited 37 participants
who randomly passed by our setup. Each participant collaborated with a robot
utilizing either animated anthropomorphic eyes and animated icons, or static
anthropomorphic eyes and static icons. We found that participants that
interacted with a robot with animated displays reported the greatest level of
trust and satisfaction; that participants interpreted static icons the best;
and that participants with a robot with static eyes had the highest completion
success. These results suggest that while animation can foster trust with
robots, human-robot communication can be optimized by the addition of familiar
static icons that may be easier for users to interpret. We published our code,
designed symbols, and collected results online at:
https://github.com/mattufts/huamn_Cozmo_interaction.

</details>


### [458] [CodeDiffuser: Attention-Enhanced Diffusion Policy via VLM-Generated Code for Instruction Ambiguity](https://arxiv.org/abs/2506.16652)
*Guang Yin,Yitong Li,Yixuan Wang,Dale McConachie,Paarth Shah,Kunimatsu Hashimoto,Huan Zhang,Katherine Liu,Yunzhu Li*

Main category: cs.RO

TL;DR: 论文提出了一种新型机器人操作框架，通过Vision-Language Model（VLM）解析自然语言指令中的模糊性，生成可执行代码，并结合感知模块生成3D注意力图，有效解决指令歧义问题。


<details>
  <summary>Details</summary>
Motivation: 自然语言指令在机器人操作任务中常存在模糊性，现有端到端模型因缺乏模块化和可解释性导致性能不佳。

Method: 采用VLM解析指令并生成可执行代码，结合感知模块生成3D注意力图以解决指令歧义。

Result: 实验表明该方法在语言模糊性、接触密集操作和多物体交互任务中表现优异。

Conclusion: 该框架通过模块化和可解释性设计，显著提升了机器人操作任务的性能。

Abstract: Natural language instructions for robotic manipulation tasks often exhibit
ambiguity and vagueness. For instance, the instruction "Hang a mug on the mug
tree" may involve multiple valid actions if there are several mugs and branches
to choose from. Existing language-conditioned policies typically rely on
end-to-end models that jointly handle high-level semantic understanding and
low-level action generation, which can result in suboptimal performance due to
their lack of modularity and interpretability. To address these challenges, we
introduce a novel robotic manipulation framework that can accomplish tasks
specified by potentially ambiguous natural language. This framework employs a
Vision-Language Model (VLM) to interpret abstract concepts in natural language
instructions and generates task-specific code - an interpretable and executable
intermediate representation. The generated code interfaces with the perception
module to produce 3D attention maps that highlight task-relevant regions by
integrating spatial and semantic information, effectively resolving ambiguities
in instructions. Through extensive experiments, we identify key limitations of
current imitation learning methods, such as poor adaptation to language and
environmental variations. We show that our approach excels across challenging
manipulation tasks involving language ambiguity, contact-rich manipulation, and
multi-object interactions.

</details>


### [459] [Compliant Residual DAgger: Improving Real-World Contact-Rich Manipulation with Human Corrections](https://arxiv.org/abs/2506.16685)
*Xiaomeng Xu,Yifan Hou,Zeyi Liu,Shuran Song*

Main category: cs.RO

TL;DR: 论文提出CR-DAgger方法，通过合规干预接口和残差策略改进DAgger在接触密集任务中的表现，显著提升成功率。


<details>
  <summary>Details</summary>
Motivation: 解决DAgger在真实世界接触密集任务中收集有效人类校正数据及高效更新策略的挑战。

Method: 引入CR-DAgger，包含合规干预接口和残差策略，结合力反馈与控制。

Result: 在书籍翻页和皮带装配任务中，成功率提升50%以上，优于从头训练和微调方法。

Conclusion: CR-DAgger为真实世界机器人学习任务提供了有效的DAgger实现指导。

Abstract: We address key challenges in Dataset Aggregation (DAgger) for real-world
contact-rich manipulation: how to collect informative human correction data and
how to effectively update policies with this new data. We introduce Compliant
Residual DAgger (CR-DAgger), which contains two novel components: 1) a
Compliant Intervention Interface that leverages compliance control, allowing
humans to provide gentle, accurate delta action corrections without
interrupting the ongoing robot policy execution; and 2) a Compliant Residual
Policy formulation that learns from human corrections while incorporating force
feedback and force control. Our system significantly enhances performance on
precise contact-rich manipulation tasks using minimal correction data,
improving base policy success rates by over 50\% on two challenging tasks (book
flipping and belt assembly) while outperforming both retraining-from-scratch
and finetuning approaches. Through extensive real-world experiments, we provide
practical guidance for implementing effective DAgger in real-world robot
learning tasks. Result videos are available at:
https://compliant-residual-dagger.github.io/

</details>


### [460] [VLM-Empowered Multi-Mode System for Efficient and Safe Planetary Navigation](https://arxiv.org/abs/2506.16703)
*Sinuo Cheng,Ruyi Zhou,Wenhao Feng,Huaiguang Yang,Haibo Gao,Zongquan Deng,Liang Ding*

Main category: cs.RO

TL;DR: 提出了一种基于视觉语言模型（VLM）的多模式行星漫游车导航系统，通过地形复杂度分类切换导航模式，提升效率和安全性。


<details>
  <summary>Details</summary>
Motivation: 行星探索环境复杂多样，需要更灵活适应的导航策略。

Method: 利用VLM解析场景信息，根据地形复杂度分类切换导航模式，结合感知、建图和规划模块。

Result: 在模拟环境中，多模式系统比单模式保守方法效率提升79.5%，同时保持避障能力。

Conclusion: 多模式导航系统显著提升了行星漫游车的效率和安全性。

Abstract: The increasingly complex and diverse planetary exploration environment
requires more adaptable and flexible rover navigation strategy. In this study,
we propose a VLM-empowered multi-mode system to achieve efficient while safe
autonomous navigation for planetary rovers. Vision-Language Model (VLM) is used
to parse scene information by image inputs to achieve a human-level
understanding of terrain complexity. Based on the complexity classification,
the system switches to the most suitable navigation mode, composing of
perception, mapping and planning modules designed for different terrain types,
to traverse the terrain ahead before reaching the next waypoint. By integrating
the local navigation system with a map server and a global waypoint generation
module, the rover is equipped to handle long-distance navigation tasks in
complex scenarios. The navigation system is evaluated in various simulation
environments. Compared to the single-mode conservative navigation method, our
multi-mode system is able to bootstrap the time and energy efficiency in a
long-distance traversal with varied type of obstacles, enhancing efficiency by
79.5%, while maintaining its avoidance capabilities against terrain hazards to
guarantee rover safety. More system information is shown at
https://chengsn1234.github.io/multi-mode-planetary-navigation/.

</details>


### [461] [DRARL: Disengagement-Reason-Augmented Reinforcement Learning for Efficient Improvement of Autonomous Driving Policy](https://arxiv.org/abs/2506.16720)
*Weitao Zhou,Bo Zhang,Zhong Cao,Xiang Li,Qian Cheng,Chunyang Liu,Yaqin Zhang,Diange Yang*

Main category: cs.RO

TL;DR: 论文提出了一种基于脱管原因增强的强化学习方法（DRARL），通过识别脱管原因来优化自动驾驶策略，避免无效数据干扰，并提升策略性能。


<details>
  <summary>Details</summary>
Motivation: 随着自动驾驶车辆在开放道路上的增多，脱管事件频发，但现有数据驱动方法因脱管数据稀缺且部分无效而效果受限。

Method: 提出DRARL方法，利用OOD状态估计模型识别脱管原因，仅对策略相关脱管进行增强训练。

Result: 实验证明，该方法能准确识别策略相关脱管原因，并通过增强训练提升策略性能，同时避免策略过于保守。

Conclusion: DRARL为利用脱管数据优化自动驾驶策略提供了高效方法。

Abstract: With the increasing presence of automated vehicles on open roads under driver
supervision, disengagement cases are becoming more prevalent. While some
data-driven planning systems attempt to directly utilize these disengagement
cases for policy improvement, the inherent scarcity of disengagement data
(often occurring as a single instances) restricts training effectiveness.
Furthermore, some disengagement data should be excluded since the disengagement
may not always come from the failure of driving policies, e.g. the driver may
casually intervene for a while. To this end, this work proposes
disengagement-reason-augmented reinforcement learning (DRARL), which enhances
driving policy improvement process according to the reason of disengagement
cases. Specifically, the reason of disengagement is identified by a
out-of-distribution (OOD) state estimation model. When the reason doesn't
exist, the case will be identified as a casual disengagement case, which
doesn't require additional policy adjustment. Otherwise, the policy can be
updated under a reason-augmented imagination environment, improving the policy
performance of disengagement cases with similar reasons. The method is
evaluated using real-world disengagement cases collected by autonomous driving
robotaxi. Experimental results demonstrate that the method accurately
identifies policy-related disengagement reasons, allowing the agent to handle
both original and semantically similar cases through reason-augmented training.
Furthermore, the approach prevents the agent from becoming overly conservative
after policy adjustments. Overall, this work provides an efficient way to
improve driving policy performance with disengagement cases.

</details>


### [462] [Learning Dexterous Object Handover](https://arxiv.org/abs/2506.16822)
*Daniel Frau-Alfaro,Julio Castaño-Amoros,Santiago Puente,Pablo Gil,Roberto Calandra*

Main category: cs.RO

TL;DR: 该论文提出了一种基于强化学习的多指手间物体交接方法，通过双四元数奖励函数优化旋转距离，实验证明其对新物体和干扰具有鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 在协作环境中（如家庭），机器人需要安全高效地进行物体交接，这是日常人机交互的重要技能。

Method: 使用强化学习（RL）和基于双四元数的新奖励函数，最小化旋转距离，优于欧拉角和旋转矩阵表示。

Result: 训练策略在未见过物体和干扰情况下表现优异，最佳场景成功率达94%，干扰下性能仅下降13.8%。

Conclusion: 该方法在物体交接任务中表现出高效性和鲁棒性，适用于现实场景。

Abstract: Object handover is an important skill that we use daily when interacting with
other humans. To deploy robots in collaborative setting, like houses, being
able to receive and handing over objects safely and efficiently becomes a
crucial skill. In this work, we demonstrate the use of Reinforcement Learning
(RL) for dexterous object handover between two multi-finger hands. Key to this
task is the use of a novel reward function based on dual quaternions to
minimize the rotation distance, which outperforms other rotation
representations such as Euler and rotation matrices. The robustness of the
trained policy is experimentally evaluated by testing w.r.t. objects that are
not included in the training distribution, and perturbations during the
handover process. The results demonstrate that the trained policy successfully
perform this task, achieving a total success rate of 94% in the best-case
scenario after 100 experiments, thereby showing the robustness of our policy
with novel objects. In addition, the best-case performance of the policy
decreases by only 13.8% when the other robot moves during the handover, proving
that our policy is also robust to this type of perturbation, which is common in
real-world object handovers.

</details>


### [463] [Orbital Collision: An Indigenously Developed Web-based Space Situational Awareness Platform](https://arxiv.org/abs/2506.16892)
*Partha Chowdhury,Harsha M,Ayush Gupta,Sanat K Biswas*

Main category: cs.RO

TL;DR: OrCo平台通过TLE数据预测空间物体碰撞概率，提升空间态势感知能力。


<details>
  <summary>Details</summary>
Motivation: 地球轨道环境日益拥挤，空间碎片和失效卫星增加了碰撞风险，需提升空间态势感知。

Method: 使用多种方法传播轨道不确定性并计算碰撞概率。

Result: 通过准确性和效率评估验证了平台性能。

Conclusion: OrCo平台能有效追踪空间物体，保障卫星在拥挤空间中的安全。

Abstract: This work presents an indigenous web based platform Orbital Collision (OrCo),
created by the Space Systems Laboratory at IIIT Delhi, to enhance Space
Situational Awareness (SSA) by predicting collision probabilities of space
objects using Two Line Elements (TLE) data. The work highlights the growing
challenges of congestion in the Earth's orbital environment, mainly due to
space debris and defunct satellites, which increase collision risks. It employs
several methods for propagating orbital uncertainty and calculating the
collision probability. The performance of the platform is evaluated through
accuracy assessments and efficiency metrics, in order to improve the tracking
of space objects and ensure the safety of the satellite in congested space.

</details>


### [464] [SDDiff: Boost Radar Perception via Spatial-Doppler Diffusion](https://arxiv.org/abs/2506.16936)
*Shengpeng Wang,Xin Luo,Yulong Xie,Wei Wang*

Main category: cs.RO

TL;DR: 论文提出了一种名为SDDiff的模型，首次将点云提取（PCE）和自车速度估计（EVE）任务结合，通过空间-多普勒扩散模型实现高精度和密集生成。


<details>
  <summary>Details</summary>
Motivation: 现有研究通常独立处理PCE和EVE任务，忽略了雷达空间和多普勒域特征的相互作用，可能引入额外偏差。论文发现3D点与自车速度之间存在潜在关联，可为两项任务提供互补优势。

Method: 设计了SDDiff模型，改进传统潜在扩散过程：1）引入包含空间占用和多普勒特征的表示；2）设计基于雷达先验的方向性扩散以优化采样；3）提出迭代多普勒细化以增强对密度变化和鬼影效应的适应性。

Result: SDDiff在EVE准确率上显著优于现有基线59%，生成密度提升4倍，同时提高了PCE的有效性和可靠性。

Conclusion: SDDiff首次实现了PCE和EVE的联合优化，通过空间-多普勒扩散模型显著提升了雷达感知性能。

Abstract: Point cloud extraction (PCE) and ego velocity estimation (EVE) are key
capabilities gaining attention in 3D radar perception. However, existing work
typically treats these two tasks independently, which may neglect the interplay
between radar's spatial and Doppler domain features, potentially introducing
additional bias. In this paper, we observe an underlying correlation between 3D
points and ego velocity, which offers reciprocal benefits for PCE and EVE. To
fully unlock such inspiring potential, we take the first step to design a
Spatial-Doppler Diffusion (SDDiff) model for simultaneously dense PCE and
accurate EVE. To seamlessly tailor it to radar perception, SDDiff improves the
conventional latent diffusion process in three major aspects. First, we
introduce a representation that embodies both spatial occupancy and Doppler
features. Second, we design a directional diffusion with radar priors to
streamline the sampling. Third, we propose Iterative Doppler Refinement to
enhance the model's adaptability to density variations and ghosting effects.
Extensive evaluations show that SDDiff significantly outperforms
state-of-the-art baselines by achieving 59% higher in EVE accuracy, 4X greater
in valid generation density while boosting PCE effectiveness and reliability.

</details>


### [465] [Learning Accurate Whole-body Throwing with High-frequency Residual Policy and Pullback Tube Acceleration](https://arxiv.org/abs/2506.16986)
*Yuntao Ma,Yang Liu,Kaixian Qu,Marco Hutter*

Main category: cs.RO

TL;DR: 论文提出了一种结合学习和模型控制的方法，用于实现腿式移动机械臂的抓握投掷，展示了较高的投掷精度和成功率。


<details>
  <summary>Details</summary>
Motivation: 投掷是机器人超越手臂范围操作物体的基本技能，但实现高精度动态投掷具有挑战性。

Method: 框架包括末端执行器的名义跟踪策略、高频残差策略提升跟踪精度，以及基于优化的模块改进加速度控制。

Result: 投掷6米远目标时平均落点误差0.28米；与大学生对比实验中，系统成功率56.8%，人类仅15.2%。

Conclusion: 该工作为动态全身操作提供了硬件上的量化精度验证，推动了相关领域进展。

Abstract: Throwing is a fundamental skill that enables robots to manipulate objects in
ways that extend beyond the reach of their arms. We present a control framework
that combines learning and model-based control for prehensile whole-body
throwing with legged mobile manipulators. Our framework consists of three
components: a nominal tracking policy for the end-effector, a high-frequency
residual policy to enhance tracking accuracy, and an optimization-based module
to improve end-effector acceleration control. The proposed controller achieved
the average of 0.28 m landing error when throwing at targets located 6 m away.
Furthermore, in a comparative study with university students, the system
achieved a velocity tracking error of 0.398 m/s and a success rate of 56.8%,
hitting small targets randomly placed at distances of 3-5 m while throwing at a
specified speed of 6 m/s. In contrast, humans have a success rate of only
15.2%. This work provides an early demonstration of prehensile throwing with
quantified accuracy on hardware, contributing to progress in dynamic whole-body
manipulation.

</details>


### [466] [Monocular One-Shot Metric-Depth Alignment for RGB-Based Robot Grasping](https://arxiv.org/abs/2506.17110)
*Teng Guo,Baichuan Huang,Jingjin Yu*

Main category: cs.RO

TL;DR: MOMA框架通过单张RGB图像恢复度量深度，支持透明物体处理，并在实际机器人任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: 解决现有6D姿态估计依赖昂贵深度传感器且无法处理透明物体的问题，同时改进单目深度估计模型的度量深度恢复能力。

Method: 提出MOMA框架，通过相机标定中的尺度-旋转-偏移对齐，结合稀疏真实深度点指导，实现无需额外数据或模型重训练的准确深度估计。

Result: MOMA在透明物体上表现良好，并在实际机器人抓取和分拣任务中取得高成功率。

Conclusion: MOMA是一种高效的单次适应方法，能够泛化到透明物体和多样化任务中。

Abstract: Accurate 6D object pose estimation is a prerequisite for successfully
completing robotic prehensile and non-prehensile manipulation tasks. At
present, 6D pose estimation for robotic manipulation generally relies on depth
sensors based on, e.g., structured light, time-of-flight, and stereo-vision,
which can be expensive, produce noisy output (as compared with RGB cameras),
and fail to handle transparent objects. On the other hand, state-of-the-art
monocular depth estimation models (MDEMs) provide only affine-invariant depths
up to an unknown scale and shift. Metric MDEMs achieve some successful
zero-shot results on public datasets, but fail to generalize. We propose a
novel framework, Monocular One-shot Metric-depth Alignment (MOMA), to recover
metric depth from a single RGB image, through a one-shot adaptation building on
MDEM techniques. MOMA performs scale-rotation-shift alignments during camera
calibration, guided by sparse ground-truth depth points, enabling accurate
depth estimation without additional data collection or model retraining on the
testing setup. MOMA supports fine-tuning the MDEM on transparent objects,
demonstrating strong generalization capabilities. Real-world experiments on
tabletop 2-finger grasping and suction-based bin-picking applications show MOMA
achieves high success rates in diverse tasks, confirming its effectiveness.

</details>


### [467] [Judo: A User-Friendly Open-Source Package for Sampling-Based Model Predictive Control](https://arxiv.org/abs/2506.17184)
*Albert H. Li,Brandon Hung,Aaron D. Ames,Jiuguang Wang,Simon Le Cleac'h,Preston Culbertson*

Main category: cs.RO

TL;DR: Judo是一个用于快速原型设计和评估采样基于MPC算法的软件包，支持异步执行和交互式GUI调优。


<details>
  <summary>Details</summary>
Motivation: 机器人社区需要通用工具来支持采样基于模型预测控制的开发、评估和部署。

Method: Judo提供了常见采样基于MPC算法的实现、标准化基准任务，以及简单可扩展的接口和异步执行功能。

Result: Judo在消费级和服务器级硬件上实现了实时性能，并通过MuJoCo物理后端支持。

Conclusion: Judo为采样基于MPC控制器的开发提供了高效、易用的工具，促进了机器人应用的进展。

Abstract: Recent advancements in parallel simulation and successful robotic
applications are spurring a resurgence in sampling-based model predictive
control. To build on this progress, however, the robotics community needs
common tooling for prototyping, evaluating, and deploying sampling-based
controllers. We introduce Judo, a software package designed to address this
need. To facilitate rapid prototyping and evaluation, Judo provides robust
implementations of common sampling-based MPC algorithms and standardized
benchmark tasks. It further emphasizes usability with simple but extensible
interfaces for controller and task definitions, asynchronous execution for
straightforward simulation-to-hardware transfer, and a highly customizable
interactive GUI for tuning controllers interactively. While written in Python,
the software leverages MuJoCo as its physics backend to achieve real-time
performance, which we validate across both consumer and server-grade hardware.
Code at https://github.com/bdaiinstitute/judo.

</details>


### [468] [Dex1B: Learning with 1B Demonstrations for Dexterous Manipulation](https://arxiv.org/abs/2506.17198)
*Jianglong Ye,Keyi Wang,Chengjing Yuan,Ruihan Yang,Yiquan Li,Jiyue Zhu,Yuzhe Qin,Xueyan Zou,Xiaolong Wang*

Main category: cs.RO

TL;DR: 论文介绍了Dex1B数据集，通过生成模型创建了大规模、多样且高质量的手部操作演示数据，并在仿真和真实实验中验证了其优越性。


<details>
  <summary>Details</summary>
Motivation: 解决大规模手部操作演示数据生成的挑战，提供多样且物理可行的数据。

Method: 提出一种结合几何约束和多样性条件的生成模型，构建包含10亿演示的Dex1B数据集。

Result: 在仿真和真实实验中显著优于现有方法，验证了模型的有效性和鲁棒性。

Conclusion: Dex1B数据集和生成模型为手部操作任务提供了高效且可靠的解决方案。

Abstract: Generating large-scale demonstrations for dexterous hand manipulation remains
challenging, and several approaches have been proposed in recent years to
address this. Among them, generative models have emerged as a promising
paradigm, enabling the efficient creation of diverse and physically plausible
demonstrations. In this paper, we introduce Dex1B, a large-scale, diverse, and
high-quality demonstration dataset produced with generative models. The dataset
contains one billion demonstrations for two fundamental tasks: grasping and
articulation. To construct it, we propose a generative model that integrates
geometric constraints to improve feasibility and applies additional conditions
to enhance diversity. We validate the model on both established and newly
introduced simulation benchmarks, where it significantly outperforms prior
state-of-the-art methods. Furthermore, we demonstrate its effectiveness and
robustness through real-world robot experiments. Our project page is at
https://jianglongye.com/dex1b

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [469] [Explainable speech emotion recognition through attentive pooling: insights from attention-based temporal localization](https://arxiv.org/abs/2506.15754)
*Tahitoa Leygue,Astrid Sabourin,Christian Bolzmacher,Sylvain Bouchigny,Margarita Anastassova,Quoc-Cuong Pham*

Main category: cs.SD

TL;DR: 论文系统评估了语音情感识别中的池化策略，提出了一种多查询多头注意力统计池化方法，性能优于平均池化，并揭示了情感信息的局部化模式。


<details>
  <summary>Details</summary>
Motivation: 当前语音情感识别中的高级池化方法研究不足，希望通过系统评估池化策略提升性能。

Method: 采用多查询多头注意力统计池化方法，并分析注意力分布和情感信息的局部化特征。

Result: 新池化方法比平均池化提升了3.5个百分点的宏F1值，且发现15%的帧捕获了80%的情感信息。

Conclusion: 注意力池化不仅是一种高性能的语音情感识别机制，还具有生物可解释性。

Abstract: State-of-the-art transformer models for Speech Emotion Recognition (SER) rely
on temporal feature aggregation, yet advanced pooling methods remain
underexplored. We systematically benchmark pooling strategies, including
Multi-Query Multi-Head Attentive Statistics Pooling, which achieves a 3.5
percentage point macro F1 gain over average pooling. Attention analysis shows
15 percent of frames capture 80 percent of emotion cues, revealing a localized
pattern of emotional information. Analysis of high-attention frames reveals
that non-linguistic vocalizations and hyperarticulated phonemes are
disproportionately prioritized during pooling, mirroring human perceptual
strategies. Our findings position attentive pooling as both a performant SER
mechanism and a biologically plausible tool for explainable emotion
localization. On Interspeech 2025 Speech Emotion Recognition in Naturalistic
Conditions Challenge, our approach obtained a macro F1 score of 0.3649.

</details>


### [470] [Sonic4D: Spatial Audio Generation for Immersive 4D Scene Exploration](https://arxiv.org/abs/2506.15759)
*Siyi Xie,Hanxin Zhu,Tianyu He,Xin Li,Zhibo Chen*

Main category: cs.SD

TL;DR: Sonic4D框架通过生成与4D场景对齐的空间音频，解决了现有方法在沉浸式视听体验中的不足。


<details>
  <summary>Details</summary>
Motivation: 现有4D生成方法虽能合成逼真动态3D场景，但忽略了空间音频的生成，限制了沉浸式体验。

Method: 分三阶段：1) 用预训练模型从单目视频生成4D场景和单声道音频；2) 通过视觉定位策略估计声源3D坐标；3) 基于物理模拟合成空间音频。

Result: 实验表明，Sonic4D能无训练生成与4D场景一致的空间音频，显著提升沉浸感。

Conclusion: Sonic4D填补了4D生成中空间音频的空白，为沉浸式体验提供了完整解决方案。

Abstract: Recent advancements in 4D generation have demonstrated its remarkable
capability in synthesizing photorealistic renderings of dynamic 3D scenes.
However, despite achieving impressive visual performance, almost all existing
methods overlook the generation of spatial audio aligned with the corresponding
4D scenes, posing a significant limitation to truly immersive audiovisual
experiences. To mitigate this issue, we propose Sonic4D, a novel framework that
enables spatial audio generation for immersive exploration of 4D scenes.
Specifically, our method is composed of three stages: 1) To capture both the
dynamic visual content and raw auditory information from a monocular video, we
first employ pre-trained expert models to generate the 4D scene and its
corresponding monaural audio. 2) Subsequently, to transform the monaural audio
into spatial audio, we localize and track the sound sources within the 4D
scene, where their 3D spatial coordinates at different timestamps are estimated
via a pixel-level visual grounding strategy. 3) Based on the estimated sound
source locations, we further synthesize plausible spatial audio that varies
across different viewpoints and timestamps using physics-based simulation.
Extensive experiments have demonstrated that our proposed method generates
realistic spatial audio consistent with the synthesized 4D scene in a
training-free manner, significantly enhancing the immersive experience for
users. Generated audio and video examples are available at
https://x-drunker.github.io/Sonic4D-project-page.

</details>


### [471] [VS-Singer: Vision-Guided Stereo Singing Voice Synthesis with Consistency Schrödinger Bridge](https://arxiv.org/abs/2506.16020)
*Zijing Zhao,Kai Wang,Hao Huang,Ying Hu,Liang He,Jichen Yang*

Main category: cs.SD

TL;DR: VS-Singer是一个视觉引导模型，通过场景图像生成带有房间混响的立体声歌唱声音，结合空间特征和一致性Schrödinger桥实现一步生成。


<details>
  <summary>Details</summary>
Motivation: 探索利用图像空间线索生成带有房间混响的立体声歌唱声音的潜在优势。

Method: VS-Singer包含三个模块：模态交互网络整合空间特征到文本编码，一致性Schrödinger桥解码器实现一步生成，SFE模块提升视听匹配一致性。

Result: 实验证明VS-Singer能有效生成与场景视角一致的立体声歌唱声音。

Conclusion: 该研究首次将立体声歌唱合成与视觉声学匹配结合在一个统一框架中。

Abstract: To explore the potential advantages of utilizing spatial cues from images for
generating stereo singing voices with room reverberation, we introduce
VS-Singer, a vision-guided model designed to produce stereo singing voices with
room reverberation from scene images. VS-Singer comprises three modules:
firstly, a modal interaction network integrates spatial features into text
encoding to create a linguistic representation enriched with spatial
information. Secondly, the decoder employs a consistency Schr\"odinger bridge
to facilitate one-step sample generation. Moreover, we utilize the SFE module
to improve the consistency of audio-visual matching. To our knowledge, this
study is the first to combine stereo singing voice synthesis with visual
acoustic matching within a unified framework. Experimental results demonstrate
that VS-Singer can effectively generate stereo singing voices that align with
the scene perspective in a single step.

</details>


### [472] [Improved Intelligibility of Dysarthric Speech using Conditional Flow Matching](https://arxiv.org/abs/2506.16127)
*Shoutrik Das,Nishant Singh,Arjun Gangwar,S Umesh*

Main category: cs.SD

TL;DR: 本文研究了自监督学习特征及其量化表示在改善构音障碍语音转换中的效果，提出了一种基于条件流匹配和扩散变换器的非自回归方法。


<details>
  <summary>Details</summary>
Motivation: 构音障碍严重影响语音清晰度，需开发有效的语音转换技术以改善患者交流能力。

Method: 采用自监督学习特征（如WavLM）和量化表示，结合条件流匹配与扩散变换器，实现非自回归的语音生成。

Result: 离散声学单元显著提升语音清晰度，且收敛速度快于传统梅尔频谱方法。

Conclusion: 自监督学习特征和量化表示在构音障碍语音转换中具有潜力，非自回归方法效果显著。

Abstract: Dysarthria is a neurological disorder that significantly impairs speech
intelligibility, often rendering affected individuals unable to communicate
effectively. This necessitates the development of robust dysarthric-to-regular
speech conversion techniques. In this work, we investigate the utility and
limitations of self-supervised learning (SSL) features and their quantized
representations as an alternative to mel-spectrograms for speech generation.
Additionally, we explore methods to mitigate speaker variability by generating
clean speech in a single-speaker voice using features extracted from WavLM. To
this end, we propose a fully non-autoregressive approach that leverages
Conditional Flow Matching (CFM) with Diffusion Transformers to learn a direct
mapping from dysarthric to clean speech. Our findings highlight the
effectiveness of discrete acoustic units in improving intelligibility while
achieving faster convergence compared to traditional mel-spectrogram-based
approaches.

</details>


### [473] [AeroGPT: Leveraging Large-Scale Audio Model for Aero-Engine Bearing Fault Diagnosis](https://arxiv.org/abs/2506.16225)
*Jiale Liu,Dandan Peng,Huan Wang,Chenyu Liu,Yan-Fu Li,Min Xie*

Main category: cs.SD

TL;DR: AeroGPT是一个基于大规模音频模型的框架，通过振动信号对齐和生成式故障分类，直接输出可解释的故障标签，显著提升了航空发动机轴承故障诊断的准确性和实用性。


<details>
  <summary>Details</summary>
Motivation: 航空发动机故障诊断需要高精度和可操作性，现有深度学习方法需要后处理且未充分利用大规模音频模型潜力。

Method: 提出AeroGPT框架，结合振动信号对齐（VSA）和生成式故障分类（GFC），直接输出可解释的故障标签。

Result: 在两个数据集上分别达到98.94%和100%的分类准确率，优于传统深度学习方法。

Conclusion: AeroGPT展示了大规模模型在故障诊断中的潜力，提升了工业适用性。

Abstract: Aerospace engines, as critical components in aviation and aerospace
industries, require continuous and accurate fault diagnosis to ensure
operational safety and prevent catastrophic failures. While deep learning
techniques have been extensively studied in this context, they output logits or
confidence scores, necessitating post-processing to derive actionable insights.
Furthermore, the potential of large-scale audio models in this domain remains
largely untapped. To address these limitations, this paper proposes AeroGPT, a
novel framework that transfers knowledge from general audio domain to
aero-engine bearing fault diagnosis. AeroGPT is a framework based on
large-scale audio model that incorporates Vibration Signal Alignment (VSA) to
adapt general audio knowledge to domain-specific vibration patterns, and
combines Generative Fault Classification (GFC) to directly output interpretable
fault labels. This approach eliminates the need for post-processing of fault
labels, supports interactive, interpretable, and actionable fault diagnosis,
thereby greatly enhancing industrial applicability. Through comprehensive
experimental validation on two aero-engine bearing datasets, AeroGPT achieved
exceptional performance with 98.94% accuracy on the DIRG dataset and perfect
100% classification on the HIT bearing dataset, surpassing traditional deep
learning approaches. Additional Qualitative analysis validates the
effectiveness of our approach and highlights the potential of large-scale
models to revolutionize fault diagnosis.

</details>


### [474] [Towards Bitrate-Efficient and Noise-Robust Speech Coding with Variable Bitrate RVQ](https://arxiv.org/abs/2506.16538)
*Yunkee Chae,Kyogu Lee*

Main category: cs.SD

TL;DR: 本文提出了一种可变比特率残差向量量化（VRVQ）框架，用于噪声鲁棒的语音编码，动态调整每帧的比特率以优化率失真权衡。


<details>
  <summary>Details</summary>
Motivation: 传统残差向量量化（RVQ）在噪声环境下效率下降，标准编解码器均匀分配比特率，浪费在无助于清晰度的噪声成分上。

Method: 引入VRVQ框架，动态调整比特率，优先处理关键语音成分并抑制残余噪声，同时集成特征降噪器以提高噪声鲁棒性。

Result: 实验结果表明，VRVQ在噪声条件下优于传统方法，提高了压缩效率和感知质量。

Conclusion: VRVQ框架在噪声环境中显著提升了语音编码的性能，为实际应用提供了更优的解决方案。

Abstract: Residual Vector Quantization (RVQ) has become a dominant approach in neural
speech and audio coding, providing high-fidelity compression. However, speech
coding presents additional challenges due to real-world noise, which degrades
compression efficiency. Standard codecs allocate bits uniformly, wasting
bitrate on noise components that do not contribute to intelligibility. This
paper introduces a Variable Bitrate RVQ (VRVQ) framework for noise-robust
speech coding, dynamically adjusting bitrate per frame to optimize
rate-distortion trade-offs. Unlike constant bitrate (CBR) RVQ, our method
prioritizes critical speech components while suppressing residual noise.
Additionally, we integrate a feature denoiser to further improve noise
robustness. Experimental results show that VRVQ improves rate-distortion
trade-offs over conventional methods, achieving better compression efficiency
and perceptual quality in noisy conditions. Samples are available at our
project page: https://yoongi43.github.io/noise_robust_vrvq/.

</details>


### [475] [Learning Magnitude Distribution of Sound Fields via Conditioned Autoencoder](https://arxiv.org/abs/2506.16729)
*Shoichi Koyama,Kenji Ishizuka*

Main category: cs.SD

TL;DR: 提出了一种基于学习的方法，通过稀疏空间测量估计声场幅度分布，适用于相位测量不可靠或无访问权限的场景。


<details>
  <summary>Details</summary>
Motivation: 在相位测量不可靠或无访问权限时，估计声学传递函数（ATF）幅度分布对空间音频应用具有重要意义。

Method: 采用基于神经网络的方法，网络架构的关键在于输入输出层受源和接收器位置及频率条件约束，并包含潜在变量聚合模块。

Result: 数值模拟结果表明，该方法能用少量接收器准确估计ATF幅度。

Conclusion: 该方法为稀疏测量下的声场幅度估计提供了有效解决方案。

Abstract: A learning-based method for estimating the magnitude distribution of sound
fields from spatially sparse measurements is proposed. Estimating the magnitude
distribution of acoustic transfer function (ATF) is useful when phase
measurements are unreliable or inaccessible and has a wide range of
applications related to spatial audio. We propose a neural-network-based method
for the ATF magnitude estimation. The key feature of our network architecture
is the input and output layers conditioned on source and receiver positions and
frequency and the aggregation module of latent variables, which can be
interpreted as an autoencoder-based extension of the basis expansion of the
sound field. Numerical simulation results indicated that the ATF magnitude is
accurately estimated with a small number of receivers by our proposed method.

</details>


### [476] [Hybrid-Sep: Language-queried audio source separation via pre-trained Model Fusion and Adversarial Diffusion Training](https://arxiv.org/abs/2506.16833)
*Jianyuan Feng,Guangzheng Li,Yangfei Xu*

Main category: cs.SD

TL;DR: HybridSep结合自监督学习和对比语言-音频预训练，提出两阶段框架和对抗一致性训练，显著提升语言查询音频分离性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法在复杂听觉特征与语言上下文对齐及分离精度上存在不足，且未充分探索自监督学习和跨模态预训练的潜力。

Method: 提出HybridSep框架，结合SSL音频表征和CLAP语义嵌入，引入对抗一致性训练（ACT）优化策略。

Result: 实验显示HybridSep在多个指标上优于现有基准（如AudioSep、FlowSep），为LASS任务设新标杆。

Conclusion: HybridSep通过整合SSL和CLAP，结合ACT策略，显著提升语言查询音频分离的性能和精度。

Abstract: Language-queried Audio Separation (LASS) employs linguistic queries to
isolate target sounds based on semantic descriptions. However, existing methods
face challenges in aligning complex auditory features with linguistic context
while preserving separation precision. Current research efforts focus primarily
on text description augmentation and architectural innovations, yet the
potential of integrating pre-trained self-supervised learning (SSL) audio
models and Contrastive Language-Audio Pretraining (CLAP) frameworks, capable of
extracting cross-modal audio-text relationships, remains underexplored. To
address this, we present HybridSep, a two-stage LASS framework that synergizes
SSL-based acoustic representations with CLAP-derived semantic embeddings. Our
framework introduces Adversarial Consistent Training (ACT), a novel
optimization strategy that treats diffusion as an auxiliary regularization loss
while integrating adversarial training to enhance separation fidelity.
Experiments demonstrate that HybridSep achieves significant performance
improvements over state-of-the-art baselines (e.g., AudioSep, FlowSep) across
multiple metrics, establishing new benchmarks for LASS tasks.

</details>


### [477] [ITO-Master: Inference-Time Optimization for Audio Effects Modeling of Music Mastering Processors](https://arxiv.org/abs/2506.16889)
*Junghyun Koo,Marco A. Martinez-Ramirez,Wei-Hsiang Liao,Giorgio Fabbro,Michele Mancusi,Yuki Mitsufuji*

Main category: cs.SD

TL;DR: 论文提出ITO-Master框架，通过推理时优化（ITO）实现更精细的音乐母带风格迁移控制。


<details>
  <summary>Details</summary>
Motivation: 现有方法基于固定处理，限制了用户根据艺术意图微调结果的能力。

Method: 结合黑盒和白盒方法建模母带处理器，通过优化参考嵌入实现动态调整。

Result: ITO提升了不同风格的母带性能，增强了风格相似性和适应性。

Conclusion: ITO-Master为用户提供了可控的母带风格迁移解决方案。

Abstract: Music mastering style transfer aims to model and apply the mastering
characteristics of a reference track to a target track, simulating the
professional mastering process. However, existing methods apply fixed
processing based on a reference track, limiting users' ability to fine-tune the
results to match their artistic intent. In this paper, we introduce the
ITO-Master framework, a reference-based mastering style transfer system that
integrates Inference-Time Optimization (ITO) to enable finer user control over
the mastering process. By optimizing the reference embedding during inference,
our approach allows users to refine the output dynamically, making micro-level
adjustments to achieve more precise mastering results. We explore both
black-box and white-box methods for modeling mastering processors and
demonstrate that ITO improves mastering performance across different styles.
Through objective evaluation, subjective listening tests, and qualitative
analysis using text-based conditioning with CLAP embeddings, we validate that
ITO enhances mastering style similarity while offering increased adaptability.
Our framework provides an effective and user-controllable solution for
mastering style transfer, allowing users to refine their results beyond the
initial style transfer.

</details>


### [478] [Universal Music Representations? Evaluating Foundation Models on World Music Corpora](https://arxiv.org/abs/2506.17055)
*Charilaos Papaioannou,Emmanouil Benetos,Alexandros Potamianos*

Main category: cs.SD

TL;DR: 该论文评估了五种音频基础模型在六种不同音乐传统中的跨文化泛化能力，发现较大模型在非西方音乐中表现更好，但文化差异会影响结果。通过多种方法，论文在多数数据集上取得了最佳性能。


<details>
  <summary>Details</summary>
Motivation: 研究基础模型是否能够泛化到多样化的音乐传统中，填补跨文化音乐信息检索的空白。

Method: 采用三种方法：探测模型固有表示、针对性监督微调1-2层、多标签少样本学习。

Result: 较大模型在非西方音乐中表现更好，但文化差异导致性能下降。论文在五个数据集上取得最佳性能。

Conclusion: 基础模型已具备大量音乐知识，但跨文化泛化仍有提升空间。论文为未来研究提供了评估框架和基准。

Abstract: Foundation models have revolutionized music information retrieval, but
questions remain about their ability to generalize across diverse musical
traditions. This paper presents a comprehensive evaluation of five
state-of-the-art audio foundation models across six musical corpora spanning
Western popular, Greek, Turkish, and Indian classical traditions. We employ
three complementary methodologies to investigate these models' cross-cultural
capabilities: probing to assess inherent representations, targeted supervised
fine-tuning of 1-2 layers, and multi-label few-shot learning for low-resource
scenarios. Our analysis shows varying cross-cultural generalization, with
larger models typically outperforming on non-Western music, though results
decline for culturally distant traditions. Notably, our approaches achieve
state-of-the-art performance on five out of six evaluated datasets,
demonstrating the effectiveness of foundation models for world music
understanding. We also find that our targeted fine-tuning approach does not
consistently outperform probing across all settings, suggesting foundation
models already encode substantial musical knowledge. Our evaluation framework
and benchmarking results contribute to understanding how far current models are
from achieving universal music representations while establishing metrics for
future progress.

</details>


<div id='physics.comp-ph'></div>

# physics.comp-ph [[Back]](#toc)

### [479] [Optimal Navigation in Microfluidics via the Optimization of a Discrete Loss](https://arxiv.org/abs/2506.15902)
*Petr Karnakov,Lucas Amoudruz,Petros Koumoutsakos*

Main category: physics.comp-ph

TL;DR: 论文提出了一种名为ODIL的闭环控制方法，用于优化微观设备在流体环境中的路径规划与控制，相比强化学习更高效且稳健。


<details>
  <summary>Details</summary>
Motivation: 微观设备在流体环境中的路径规划与控制对靶向药物输送和环境监测等应用至关重要，但微设备与流体的复杂交互增加了任务难度。

Method: 采用ODIL方法，通过优化离散损失函数（结合动力学和路径目标）实现闭环控制。

Result: ODIL比强化学习更稳健，速度快三个数量级，且在复杂流环境的高维动作/状态空间中表现优异。

Conclusion: ODIL是一种高效且强大的工具，适用于复杂流体环境中的微观设备导航。

Abstract: Optimal path planning and control of microscopic devices navigating in fluid
environments is essential for applications ranging from targeted drug delivery
to environmental monitoring. These tasks are challenging due to the complexity
of microdevice-flow interactions. We introduce a closed-loop control method
that optimizes a discrete loss (ODIL) in terms of dynamics and path objectives.
In comparison with reinforcement learning, ODIL is more robust, up to three
orders faster, and excels in high-dimensional action/state spaces, making it a
powerful tool for navigating complex flow environments.

</details>


### [480] [A Neural Operator based Hybrid Microscale Model for Multiscale Simulation of Rate-Dependent Materials](https://arxiv.org/abs/2506.16918)
*Dhananjeyan Jeyaraj,Hamidreza Eivazi,Jendrik-Alexander Tröger,Stefan Wittek,Stefan Hartmann,Andreas Rausch*

Main category: physics.comp-ph

TL;DR: 该论文提出了一种结合深度学习和多尺度建模的方法，通过神经算子预测微观物理行为，显著加速了计算均质化过程。


<details>
  <summary>Details</summary>
Motivation: 理解微观结构对宏观行为的影响需要高效的多尺度建模方法，传统方法计算成本高。

Method: 采用神经算子预测微观物理行为，结合数据驱动和物理基础方法，应用于粘弹性材料的时变固体力学问题。

Result: 方法计算效率高（约快100倍），均质化应力误差低于6%。

Conclusion: 该混合方法在计算效率和准确性上表现优异，适用于多种材料和空间离散化问题。

Abstract: The behavior of materials is influenced by a wide range of phenomena
occurring across various time and length scales. To better understand the
impact of microstructure on macroscopic response, multiscale modeling
strategies are essential. Numerical methods, such as the $\text{FE}^2$
approach, account for micro-macro interactions to predict the global response
in a concurrent manner. However, these methods are computationally intensive
due to the repeated evaluations of the microscale. This challenge has led to
the integration of deep learning techniques into computational homogenization
frameworks to accelerate multiscale simulations. In this work, we employ neural
operators to predict the microscale physics, resulting in a hybrid model that
combines data-driven and physics-based approaches. This allows for
physics-guided learning and provides flexibility for different materials and
spatial discretizations. We apply this method to time-dependent solid mechanics
problems involving viscoelastic material behavior, where the state is
represented by internal variables only at the microscale. The constitutive
relations of the microscale are incorporated into the model architecture and
the internal variables are computed based on established physical principles.
The results for homogenized stresses ($<6\%$ error) show that the approach is
computationally efficient ($\sim 100 \times$ faster).

</details>


<div id='astro-ph.IM'></div>

# astro-ph.IM [[Back]](#toc)

### [481] [Category-based Galaxy Image Generation via Diffusion Models](https://arxiv.org/abs/2506.16255)
*Xingzhong Fan,Hongming Tang,Yue Zeng,M. B. N. Kouwenhoven,Guangquan Zeng*

Main category: astro-ph.IM

TL;DR: GalCatDiff是一个基于扩散模型的天文学框架，结合了星系图像特征和天体物理属性，通过增强的U-Net和Astro-RAB块实现高质量的星系生成。


<details>
  <summary>Details</summary>
Motivation: 传统星系生成方法依赖物理假设和参数调优，而数据驱动的生成模型通过学习观测数据提供替代方案。扩散模型在质量和多样性上优于VAEs和GANs。

Method: GalCatDiff采用增强的U-Net和Astro-RAB块，动态结合注意力机制与卷积操作，并使用类别嵌入实现类别特异性生成。

Result: 实验表明，GalCatDiff在样本颜色和大小分布一致性上显著优于现有方法，生成的星系视觉逼真且物理一致。

Conclusion: GalCatDiff提升了星系模拟的可靠性，并可能作为数据增强工具支持未来星系分类算法开发。

Abstract: Conventional galaxy generation methods rely on semi-analytical models and
hydrodynamic simulations, which are highly dependent on physical assumptions
and parameter tuning. In contrast, data-driven generative models do not have
explicit physical parameters pre-determined, and instead learn them efficiently
from observational data, making them alternative solutions to galaxy
generation. Among these, diffusion models outperform Variational Autoencoders
(VAEs) and Generative Adversarial Networks (GANs) in quality and diversity.
Leveraging physical prior knowledge to these models can further enhance their
capabilities. In this work, we present GalCatDiff, the first framework in
astronomy to leverage both galaxy image features and astrophysical properties
in the network design of diffusion models. GalCatDiff incorporates an enhanced
U-Net and a novel block entitled Astro-RAB (Residual Attention Block), which
dynamically combines attention mechanisms with convolution operations to ensure
global consistency and local feature fidelity. Moreover, GalCatDiff uses
category embeddings for class-specific galaxy generation, avoiding the high
computational costs of training separate models for each category. Our
experimental results demonstrate that GalCatDiff significantly outperforms
existing methods in terms of the consistency of sample color and size
distributions, and the generated galaxies are both visually realistic and
physically consistent. This framework will enhance the reliability of galaxy
simulations and can potentially serve as a data augmentor to support future
galaxy classification algorithm development.

</details>


<div id='q-bio.BM'></div>

# q-bio.BM [[Back]](#toc)

### [482] [Generative Modeling of Full-Atom Protein Conformations using Latent Diffusion on Graph Embeddings](https://arxiv.org/abs/2506.17064)
*Aditya Sengar,Ali Hariri,Daniel Probst,Patrick Barth,Pierre Vandergheynst*

Main category: q-bio.BM

TL;DR: LD-FPG框架通过潜在扩散模型生成全原子蛋白质结构，利用ChebNet和三种池化策略，从分子动力学轨迹中重建高保真度的构象集合。


<details>
  <summary>Details</summary>
Motivation: 生成动态蛋白质（如GPCRs）的多样化全原子构象集合对理解其功能至关重要，但现有模型常简化原子细节或忽略构象多样性。

Method: 使用ChebNet获取蛋白质构象的低维潜在嵌入，通过三种池化策略（盲、顺序和残基）处理，训练扩散模型生成新样本，解码器映射回笛卡尔坐标。

Result: 在D2R-MD数据集上，顺序和残基池化策略能高保真重建参考集合（全原子lDDT约0.7，C-alpha-lDDT约0.8），并恢复二面角分布（Jensen-Shannon散度<0.03）。

Conclusion: LD-FPG为大型蛋白质的系统特异性全原子集合生成提供了实用工具，对复杂动态靶点的结构治疗设计具有潜力。

Abstract: Generating diverse, all-atom conformational ensembles of dynamic proteins
such as G-protein-coupled receptors (GPCRs) is critical for understanding their
function, yet most generative models simplify atomic detail or ignore
conformational diversity altogether. We present latent diffusion for full
protein generation (LD-FPG), a framework that constructs complete all-atom
protein structures, including every side-chain heavy atom, directly from
molecular dynamics (MD) trajectories. LD-FPG employs a Chebyshev graph neural
network (ChebNet) to obtain low-dimensional latent embeddings of protein
conformations, which are processed using three pooling strategies: blind,
sequential and residue-based. A diffusion model trained on these latent
representations generates new samples that a decoder, optionally regularized by
dihedral-angle losses, maps back to Cartesian coordinates. Using D2R-MD, a
2-microsecond MD trajectory (12 000 frames) of the human dopamine D2 receptor
in a membrane environment, the sequential and residue-based pooling strategy
reproduces the reference ensemble with high structural fidelity (all-atom lDDT
of approximately 0.7; C-alpha-lDDT of approximately 0.8) and recovers backbone
and side-chain dihedral-angle distributions with a Jensen-Shannon divergence of
less than 0.03 compared to the MD data. LD-FPG thereby offers a practical route
to system-specific, all-atom ensemble generation for large proteins, providing
a promising tool for structure-based therapeutic design on complex, dynamic
targets. The D2R-MD dataset and our implementation are freely available to
facilitate further research.

</details>


<div id='stat.ME'></div>

# stat.ME [[Back]](#toc)

### [483] [TRUST: Transparent, Robust and Ultra-Sparse Trees](https://arxiv.org/abs/2506.15791)
*Albert Dorador*

Main category: stat.ME

TL;DR: TRUST是一种新型回归树模型，结合了随机森林的准确性和浅层决策树的可解释性，并通过大语言模型生成用户友好的解释。


<details>
  <summary>Details</summary>
Motivation: 尽管分段常数回归树具有可解释性，但其预测准确性通常不如黑盒模型（如随机森林）。TRUST旨在填补这一差距。

Method: TRUST结合了随机森林的准确性和浅层决策树的可解释性，利用大语言模型生成解释。

Result: TRUST在合成和真实数据集上表现优于其他可解释模型（如CART、Lasso），并匹配随机森林的准确性。

Conclusion: TRUST在准确性和可解释性上均优于现有模型，是一种高效且透明的回归树模型。

Abstract: Piecewise-constant regression trees remain popular for their
interpretability, yet often lag behind black-box models like Random Forest in
predictive accuracy. In this work, we introduce TRUST (Transparent, Robust, and
Ultra-Sparse Trees), a novel regression tree model that combines the accuracy
of Random Forests with the interpretability of shallow decision trees and
sparse linear models. TRUST further enhances transparency by leveraging Large
Language Models to generate tailored, user-friendly explanations. Extensive
validation on synthetic and real-world benchmark datasets demonstrates that
TRUST consistently outperforms other interpretable models -- including CART,
Lasso, and Node Harvest -- in predictive accuracy, while matching the accuracy
of Random Forest and offering substantial gains in both accuracy and
interpretability over M5', a well-established model that is conceptually
related.

</details>


### [484] [Bayesian Joint Model of Multi-Sensor and Failure Event Data for Multi-Mode Failure Prediction](https://arxiv.org/abs/2506.17036)
*Sina Aghaee Dabaghan Fard,Minhee Kim,Akash Deep,Jaesung Lee*

Main category: stat.ME

TL;DR: 提出了一种统一方法，联合建模多传感器时间序列数据和多模式失效时间，结合Cox比例风险模型、卷积多输出高斯过程和多项失效模式分布，通过分层贝叶斯框架实现准确预测和不确定性量化。


<details>
  <summary>Details</summary>
Motivation: 现有模型独立处理失效模式和剩余使用寿命预测，忽略了任务间的内在联系，且缺乏统计严谨性。

Method: 结合Cox比例风险模型、卷积多输出高斯过程和多项失效模式分布，采用分层贝叶斯框架和变分贝叶斯后验推断。

Result: 通过数值和案例研究（喷气发动机数据集）验证了模型的优越性。

Conclusion: 提出的方法能有效联合建模多传感器数据和失效时间，实现准确预测和不确定性量化。

Abstract: Modern industrial systems are often subject to multiple failure modes, and
their conditions are monitored by multiple sensors, generating multiple
time-series signals. Additionally, time-to-failure data are commonly available.
Accurately predicting a system's remaining useful life (RUL) requires
effectively leveraging multi-sensor time-series data alongside multi-mode
failure event data. In most existing models, failure modes and RUL prediction
are performed independently, ignoring the inherent relationship between these
two tasks. Some models integrate multiple failure modes and event prediction
using black-box machine learning approaches, which lack statistical rigor and
cannot characterize the inherent uncertainty in the model and data. This paper
introduces a unified approach to jointly model the multi-sensor time-series
data and failure time concerning multiple failure modes. This proposed model
integrate a Cox proportional hazards model, a Convolved Multi-output Gaussian
Process, and multinomial failure mode distributions in a hierarchical Bayesian
framework with corresponding priors, enabling accurate prediction with robust
uncertainty quantification. Posterior distributions are effectively obtained by
Variational Bayes, and prediction is performed with Monte Carlo sampling. The
advantages of the proposed model is validated through extensive numerical and
case studies with jet-engine dataset.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [485] [Quantum Fisher-Preconditioned Reinforcement Learning: From Single-Qubit Control to Rayleigh-Fading Link Adaptation](https://arxiv.org/abs/2506.15753)
*Oluwaseyi Giwa,Muhammad Ahmed Mohsin,Muhammad Ali Jamshed*

Main category: quant-ph

TL;DR: QPPG是一种基于自然梯度的算法，通过量子Fisher信息实现策略更新的白化，在噪声环境下稳定学习。


<details>
  <summary>Details</summary>
Motivation: 桥接经典与量子几何，提升在噪声环境中的学习稳定性。

Method: 使用全逆量子Fisher信息与Tikhonov正则化进行策略更新。

Result: 在经典和量子环境中，QPPG比REINFORCE快4倍收敛，并在不确定性下保持1 dB增益。

Conclusion: QPPG展示了基于全QFI预处理的优势，适用于可扩展的量子强化学习。

Abstract: In this letter, we propose Quantum-Preconditioned Policy Gradient (QPPG), a
natural gradient-based algorithm for link adaptation that whitens policy
updates using the full inverse quantum Fisher information with Tikhonov
regularization. QPPG bridges classical and quantum geometry, achieving stable
learning even under noise. Evaluated on classical and quantum environments,
including noisy single-qubit Gym tasks and Rayleigh-fading channels, QPPG
converges 4 times faster than REINFORCE and sustains a 1 dB gain under
uncertainty. It reaches a 90 percent return in one hundred episodes with high
noise robustness, showcasing the advantages of full QFI-based preconditioning
for scalable quantum reinforcement learning.

</details>


### [486] [Compilation, Optimization, Error Mitigation, and Machine Learning in Quantum Algorithms](https://arxiv.org/abs/2506.15760)
*Shuangbao Paul Wang,Jianzhou Mao,Eric Sakk*

Main category: quant-ph

TL;DR: 论文讨论了量子算法的编译、优化和错误缓解，提出了近似量子傅里叶变换（AQFT）以优化电路执行。


<details>
  <summary>Details</summary>
Motivation: 量子算法在混合平台（QPU与CPU/GPU）上运行，结合高性能计算能力与量子加速，但需要优化和错误缓解以实现实际应用。

Method: 提出近似量子傅里叶变换（AQFT）以优化量子算法，提升电路执行效率。

Result: AQFT在量子傅里叶变换的指数加速基础上进一步优化了电路执行。

Conclusion: AQFT为量子算法的实际执行提供了有效的优化方法。

Abstract: This paper discusses the compilation, optimization, and error mitigation of
quantum algorithms, essential steps to execute real-world quantum algorithms.
Quantum algorithms running on a hybrid platform with QPU and CPU/GPU take
advantage of existing high-performance computing power with quantum-enabled
exponential speedups. The proposed approximate quantum Fourier transform (AQFT)
for quantum algorithm optimization improves the circuit execution on top of an
exponential speed-ups the quantum Fourier transform has provided.

</details>


### [487] [Superconducting Qubit Readout Using Next-Generation Reservoir Computing](https://arxiv.org/abs/2506.15771)
*Robert Kent,Benjamin Lienhard,Gregory Lafyatis,Daniel J. Gauthier*

Main category: quant-ph

TL;DR: 论文提出了一种基于下一代储层计算的机器学习方法，用于提高量子比特状态识别的准确性和可扩展性，相比传统方法和神经网络方法，具有更低的计算复杂度和更高的效率。


<details>
  <summary>Details</summary>
Motivation: 超导量子比特的读出是量子处理器的一个瓶颈，传统方法和神经网络方法存在计算成本高、延迟大和可扩展性差的问题。

Method: 采用储层计算方法，通过构建测量信号的多项式特征并将其映射到对应的量子比特状态，避免了神经网络中昂贵的非线性激活函数。

Result: 相比传统方法，单量子比特和五量子比特数据集的错误率分别降低了50%和11%，串扰减少了2.5倍；相比机器学习方法，计算量显著减少。

Conclusion: 储层计算方法能够在保持高保真度的同时，提升量子比特状态识别的可扩展性和实时性，适用于未来量子处理器。

Abstract: Quantum processors require rapid and high-fidelity simultaneous measurements
of many qubits. While superconducting qubits are among the leading modalities
toward a useful quantum processor, their readout remains a bottleneck.
Traditional approaches to processing measurement data often struggle to account
for crosstalk present in frequency-multiplexed readout, the preferred method to
reduce the resource overhead. Recent approaches to address this challenge use
neural networks to improve the state-discrimination fidelity. However, they are
computationally expensive to train and evaluate, resulting in increased latency
and poor scalability as the number of qubits increases. We present an
alternative machine learning approach based on next-generation reservoir
computing that constructs polynomial features from the measurement signals and
maps them to the corresponding qubit states. This method is highly
parallelizable, avoids the costly nonlinear activation functions common in
neural networks, and supports real-time training, enabling fast evaluation,
adaptability, and scalability. Despite its lower computational complexity, our
reservoir approach is able to maintain high qubit-state-discrimination
fidelity. Relative to traditional methods, our approach achieves error
reductions of up to 50% and 11% on single- and five-qubit datasets,
respectively, and delivers up to 2.5x crosstalk reduction on the five-qubit
dataset. Compared with recent machine-learning methods, evaluating our model
requires 100x fewer multiplications for single-qubit and 2.5x fewer for
five-qubit models. This work demonstrates that reservoir computing can enhance
qubit-state discrimination while maintaining scalability for future quantum
processors.

</details>


### [488] [Feedback-driven recurrent quantum neural network universality](https://arxiv.org/abs/2506.16332)
*Lukas Gonon,Rodrigo Martínez-Peña,Juan-Pablo Ortega*

Main category: quant-ph

TL;DR: 论文提出了一种基于反馈的量子储层计算架构，解决了现有方法在实时处理和计算开销上的局限性，并提供了理论保证。


<details>
  <summary>Details</summary>
Motivation: 量子储层计算在处理时序数据时具有潜力，但现有方法（如重启和回放协议）存在实时处理和计算开销的问题。反馈协议虽具优势，但其理论基础尚未完善。

Method: 提出了一种递归量子神经网络架构，将现有前馈模型扩展到动态反馈驱动的储层设置，并提供了变分递归量子神经网络的理论保证。

Result: 证明了该模型具有线性读出器的普适性，同时具备强大的计算能力和实验可行性。

Conclusion: 研究为具有实时处理能力的量子储层计算提供了实用且理论扎实的基础。

Abstract: Quantum reservoir computing uses the dynamics of quantum systems to process
temporal data, making it particularly well-suited for learning with noisy
intermediate-scale quantum devices. Early experimental proposals, such as the
restarting and rewinding protocols, relied on repeating previous steps of the
quantum map to avoid backaction. However, this approach compromises real-time
processing and increases computational overhead. Recent developments have
introduced alternative protocols that address these limitations. These include
online, mid-circuit measurement, and feedback techniques, which enable
real-time computation while preserving the input history. Among these, the
feedback protocol stands out for its ability to process temporal information
with comparatively fewer components. Despite this potential advantage, the
theoretical foundations of feedback-based quantum reservoir computing remain
underdeveloped, particularly with regard to the universality and the
approximation capabilities of this approach. This paper addresses this issue by
presenting a recurrent quantum neural network architecture that extends a class
of existing feedforward models to a dynamic, feedback-driven reservoir setting.
We provide theoretical guarantees for variational recurrent quantum neural
networks, including approximation bounds and universality results. Notably, our
analysis demonstrates that the model is universal with linear readouts, making
it both powerful and experimentally accessible. These results pave the way for
practical and theoretically grounded quantum reservoir computing with real-time
processing capabilities.

</details>


### [489] [Enhancing Expressivity of Quantum Neural Networks Based on the SWAP test](https://arxiv.org/abs/2506.16938)
*Sebastian Nagies,Emiliano Tolotti,Davide Pastorello,Enrico Blanzieri*

Main category: quant-ph

TL;DR: 论文分析了基于SWAP测试电路的量子神经网络（QNN）与经典两层前馈网络的数学等价性，发现其表达能力有限，并提出改进方案以增强其学习能力。


<details>
  <summary>Details</summary>
Motivation: 研究量子神经网络与经典模型的联系，以推动量子机器学习的发展。

Method: 分析基于SWAP测试电路的QNN，提出改进方案使用广义SWAP测试电路。

Result: 改进后的架构能学习更复杂的任务（如奇偶校验函数），而原始架构在二维以上无法实现。

Conclusion: 通过经典任务分析增强QNN表达能力，展示了SWAP测试架构的广泛潜力。

Abstract: Parameterized quantum circuits represent promising architectures for machine
learning applications, yet many lack clear connections to classical models,
potentially limiting their ability to translate the wide success of classical
neural networks to the quantum realm. We examine a specific type of quantum
neural network (QNN) built exclusively from SWAP test circuits, and discuss its
mathematical equivalence to a classical two-layer feedforward network with
quadratic activation functions under amplitude encoding. Our analysis across
classical real-world and synthetic datasets reveals that while this
architecture can successfully learn many practical tasks, it exhibits
fundamental expressivity limitations due to violating the universal
approximation theorem, particularly failing on harder problems like the parity
check function. To address this limitation, we introduce a circuit modification
using generalized SWAP test circuits that effectively implements classical
neural networks with product layers. This enhancement enables successful
learning of parity check functions in arbitrary dimensions which we
analytically argue to be impossible for the original architecture beyond two
dimensions regardless of network size. Our results establish a framework for
enhancing QNN expressivity through classical task analysis and demonstrate that
our SWAP test-based architecture offers broad representational capacity,
suggesting potential promise also for quantum learning tasks.

</details>


<div id='cs.IT'></div>

# cs.IT [[Back]](#toc)

### [490] [Code Rate Optimization via Neural Polar Decoders](https://arxiv.org/abs/2506.15836)
*Ziv Aharoni,Bashar Huleihel,Henry D Pfister,Haim H Permuter*

Main category: cs.IT

TL;DR: 该论文提出了一种通过神经极化解码器（NPDs）优化通信码率的方法，适用于未知信道模型的场景。


<details>
  <summary>Details</summary>
Motivation: 在信道模型未知的情况下，如何优化码率并实现实用的编码方案是研究的核心动机。

Method: 采用两阶段方法：训练阶段通过NPDs估计互信息并优化输入分布参数；推理阶段利用优化模型构建极化码，结合HY方案和列表解码提升性能。

Result: 实验表明，在非均匀输入分布的信道中，该方法显著提升了互信息和误码率性能，适用于1024以下的块长度。

Conclusion: 该方法为实际通信系统提供了一种可扩展的解决方案，连接了理论容量估计与实际编码性能。

Abstract: This paper proposes a method to optimize communication code rates via the
application of neural polar decoders (NPDs). Employing this approach enables
simultaneous optimization of code rates over input distributions while
providing a practical coding scheme within the framework of polar codes. The
proposed approach is designed for scenarios where the channel model is unknown,
treating the channel as a black box that produces output samples from input
samples. We employ polar codes to achieve our objectives, using NPDs to
estimate mutual information (MI) between the channel inputs and outputs, and
optimize a parametric model of the input distribution. The methodology involves
a two-phase process: a training phase and an inference phase. In the training
phase, two steps are repeated interchangeably. First, the estimation step
estimates the MI of the channel inputs and outputs via NPDs. Second, the
improvement step optimizes the input distribution parameters to maximize the MI
estimate obtained by the NPDs. In the inference phase, the optimized model is
used to construct polar codes. This involves incorporating the Honda-Yamamoto
(HY) scheme to accommodate the optimized input distributions and list decoding
to enhance decoding performance. Experimental results on memoryless and
finite-state channels (FSCs) demonstrate the effectiveness of our approach,
particularly in cases where the channel's capacity-achieving input distribution
is non-uniform. For these cases, we show significant improvements in MI and bit
error rates (BERs) over those achieved by uniform and independent and
identically distributed (i.i.d.) input distributions, validating our method for
block lengths up to 1024. This scalable approach has potential applications in
real-world communication systems, bridging theoretical capacity estimation and
practical coding performance.

</details>


### [491] [Neural Polar Decoders for DNA Data Storage](https://arxiv.org/abs/2506.17076)
*Ziv Aharoni,Henry D. Pfister*

Main category: cs.IT

TL;DR: 提出了一种基于神经极化解码器（NPDs）的低复杂度解码方法，用于处理DNA数据存储中的同步错误（插入、删除、替换）。该方法复杂度低，无需显式信道模型，且在性能和参数效率上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: DNA数据存储中的同步错误（如插入、删除）是主要挑战，现有最大似然解码器计算成本高，需高效解决方案。

Method: 使用神经极化解码器（NPDs），复杂度为O(AN log N)，仅需样本访问信道，无需显式模型，并能估计互信息以优化输入分布和编码设计。

Result: 在合成删除和IDS信道上，NPDs性能接近最优，复杂度显著低于基于网格的解码器，并在真实DNA存储场景中表现优异。

Conclusion: NPDs在DNA数据存储系统中展现出高效、鲁棒的解码潜力，性能优于现有方法且参数更少。

Abstract: Synchronization errors, such as insertions and deletions, present a
fundamental challenge in DNA-based data storage systems, arising from both
synthesis and sequencing noise. These channels are often modeled as
insertion-deletion-substitution (IDS) channels, for which designing
maximum-likelihood decoders is computationally expensive. In this work, we
propose a data-driven approach based on neural polar decoders (NPDs) to design
low-complexity decoders for channels with synchronization errors. The proposed
architecture enables decoding over IDS channels with reduced complexity $O(AN
log N )$, where $A$ is a tunable parameter independent of the channel. NPDs
require only sample access to the channel and can be trained without an
explicit channel model. Additionally, NPDs provide mutual information (MI)
estimates that can be used to optimize input distributions and code design. We
demonstrate the effectiveness of NPDs on both synthetic deletion and IDS
channels. For deletion channels, we show that NPDs achieve near-optimal
decoding performance and accurate MI estimation, with significantly lower
complexity than trellis-based decoders. We also provide numerical estimates of
the channel capacity for the deletion channel. We extend our evaluation to
realistic DNA storage settings, including channels with multiple noisy reads
and real-world Nanopore sequencing data. Our results show that NPDs match or
surpass the performance of existing methods while using significantly fewer
parameters than the state-of-the-art. These findings highlight the promise of
NPDs for robust and efficient decoding in DNA data storage systems.

</details>


<div id='cs.GT'></div>

# cs.GT [[Back]](#toc)

### [492] [Fair Contracts in Principal-Agent Games with Heterogeneous Types](https://arxiv.org/abs/2506.15887)
*Jakub Tłuczek,Victor Villin,Christos Dimitrakakis*

Main category: cs.GT

TL;DR: 论文提出了一种基于重复委托-代理游戏的框架，通过自适应合同实现多智能体系统中的公平性，同时保持效率。


<details>
  <summary>Details</summary>
Motivation: 多智能体系统中隐藏的异质性导致财富分配不均，即使规则相同。现实中的例子激发了研究公平性的需求。

Method: 采用重复委托-代理游戏框架，委托人通过学习提供自适应合同，利用简单的线性合同结构。

Result: 公平性委托人能学习到同质线性合同，在序列社会困境中均衡结果，且不牺牲效率。

Conclusion: 研究表明，公平性和系统稳定性可以兼顾，同时保持整体性能。

Abstract: Fairness is desirable yet challenging to achieve within multi-agent systems,
especially when agents differ in latent traits that affect their abilities.
This hidden heterogeneity often leads to unequal distributions of wealth, even
when agents operate under the same rules. Motivated by real-world examples, we
propose a framework based on repeated principal-agent games, where a principal,
who also can be seen as a player of the game, learns to offer adaptive
contracts to agents. By leveraging a simple yet powerful contract structure, we
show that a fairness-aware principal can learn homogeneous linear contracts
that equalize outcomes across agents in a sequential social dilemma.
Importantly, this fairness does not come at the cost of efficiency: our results
demonstrate that it is possible to promote equity and stability in the system
while preserving overall performance.

</details>


### [493] [Solving Zero-Sum Convex Markov Games](https://arxiv.org/abs/2506.16120)
*Fivos Kalogiannis,Emmanouil-Vasileios Vlatakis-Gkaragkounis,Ian Gemp,Georgios Piliouras*

Main category: cs.GT

TL;DR: 本文首次证明了在双人零和凸马尔可夫博弈中，独立策略梯度方法能全局收敛到纳什均衡。通过非凸正则化将问题转化为NC-pPL目标，并提供了随机嵌套和交替梯度下降-上升方法的全局收敛保证。


<details>
  <summary>Details</summary>
Motivation: 凸马尔可夫博弈扩展了马尔可夫决策过程到多智能体设置，但即使是最小-最大情况也存在非凸性、缺乏贝尔曼一致性和无限视野复杂性等挑战。

Method: 采用两步法：1）利用隐藏凸-隐藏凹函数性质，通过非凸正则化将问题转化为NC-pPL目标；2）在NC-pPL和双面pPL条件下，研究一般约束最小-最大问题。

Result: 证明了独立策略梯度方法能稳定迭代并收敛到均衡，同时为随机嵌套和交替梯度下降-上升方法提供了全局收敛保证。

Conclusion: 本文为凸马尔可夫博弈中的策略梯度方法提供了理论支持，扩展了其在多智能体博弈中的应用潜力。

Abstract: We contribute the first provable guarantees of global convergence to Nash
equilibria (NE) in two-player zero-sum convex Markov games (cMGs) by using
independent policy gradient methods. Convex Markov games, recently defined by
Gemp et al. (2024), extend Markov decision processes to multi-agent settings
with preferences that are convex over occupancy measures, offering a broad
framework for modeling generic strategic interactions. However, even the
fundamental min-max case of cMGs presents significant challenges, including
inherent nonconvexity, the absence of Bellman consistency, and the complexity
of the infinite horizon.
  We follow a two-step approach. First, leveraging properties of
hidden-convex--hidden-concave functions, we show that a simple nonconvex
regularization transforms the min-max optimization problem into a
nonconvex-proximal Polyak-Lojasiewicz (NC-pPL) objective. Crucially, this
regularization can stabilize the iterates of independent policy gradient
methods and ultimately lead them to converge to equilibria. Second, building on
this reduction, we address the general constrained min-max problems under
NC-pPL and two-sided pPL conditions, providing the first global convergence
guarantees for stochastic nested and alternating gradient descent-ascent
methods, which we believe may be of independent interest.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [494] [DeepRTL2: A Versatile Model for RTL-Related Tasks](https://arxiv.org/abs/2506.15697)
*Yi Liu,Hongji Zhang,Yunhao Zhou,Zhengyuan Shi,Changran Xu,Qiang Xu*

Main category: cs.AR

TL;DR: DeepRTL2是一个多功能LLM家族，首次统一了EDA中与RTL相关的生成和嵌入任务，填补了嵌入任务的研究空白，并在实验中表现出色。


<details>
  <summary>Details</summary>
Motivation: 尽管LLM在EDA中的生成任务（如RTL代码生成）已取得进展，但嵌入任务（如代码搜索、功能等价性检查）被忽视，而这些任务对硬件设计流程至关重要。

Method: 提出DeepRTL2模型，同时处理生成和嵌入任务，为EDA提供全面解决方案。

Result: 实验表明，DeepRTL2在所有评估任务中均达到最先进性能。

Conclusion: DeepRTL2填补了EDA中嵌入任务的空白，为硬件设计提供了高效、统一的解决方案。

Abstract: The integration of large language models (LLMs) into electronic design
automation (EDA) has significantly advanced the field, offering transformative
benefits, particularly in register transfer level (RTL) code generation and
understanding. While previous studies have demonstrated the efficacy of
fine-tuning LLMs for these generation-based tasks, embedding-based tasks, which
are equally critical to EDA workflows, have been largely overlooked. These
tasks, including natural language code search, RTL code functionality
equivalence checking, and performance prediction, are essential for
accelerating and optimizing the hardware design process. To address this gap,
we present DeepRTL2, a family of versatile LLMs that unifies both generation-
and embedding-based tasks related to RTL. By simultaneously tackling a broad
range of tasks, DeepRTL2 represents the first model to provide a comprehensive
solution to the diverse challenges in EDA. Through extensive experiments, we
show that DeepRTL2 achieves state-of-the-art performance across all evaluated
tasks.

</details>


### [495] [RCNet: $ΔΣ$ IADCs as Recurrent AutoEncoders](https://arxiv.org/abs/2506.16903)
*Arnaud Verdant,William Guicquero,Jérôme Chossat*

Main category: cs.AR

TL;DR: 提出了一种用于Delta-Sigma ADC的深度学习模型RCNet，结合RNN和硬件设计约束优化SNR与面积。


<details>
  <summary>Details</summary>
Motivation: 探索利用深度学习模型优化Delta-Sigma ADC的设计，特别是针对硬件约束条件下的性能与面积权衡。

Method: 使用RNN描述调制器和滤波器，结合高优化器和定制损失函数，考虑量化权重、信号饱和等硬件约束。

Result: 在DC转换中，RCNet成功优化了SNR（ENOB >13bit）与面积（<14pF电容）的权衡，且最佳架构不依赖高阶调制器。

Conclusion: RCNet为Delta-Sigma ADC设计提供了新的自由度，展示了在硬件约束下优化性能的潜力。

Abstract: This paper proposes a deep learning model (RCNet) for Delta-Sigma
($\Delta\Sigma$) ADCs. Recurrent Neural Networks (RNNs) allow to describe both
modulators and filters. This analogy is applied to Incremental ADCs (IADC).
High-end optimizers combined with full-custom losses are used to define
additional hardware design constraints: quantized weights, signal saturation,
temporal noise injection, devices area. Focusing on DC conversion, our early
results demonstrate that $SNR$ defined as an Effective Number Of Bits (ENOB)
can be optimized under a certain hardware mapping complexity. The proposed
RCNet succeeded to provide design tradeoffs in terms of $SNR$ ($>$13bit) versus
area constraints ($<$14pF total capacitor) at a given $OSR$ (80 samples).
Interestingly, it appears that the best RCNet architectures do not necessarily
rely on high-order modulators, leveraging additional topology exploration
degrees of freedom.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [496] [CP$^2$: Leveraging Geometry for Conformal Prediction via Canonicalization](https://arxiv.org/abs/2506.16189)
*Putri A. van der Linden,Alexander Timans,Erik J. Bekkers*

Main category: stat.ML

TL;DR: 该论文研究了在几何数据变换（如旋转或翻转）下的一致性预测（CP）问题，提出通过整合几何信息（如几何姿态）来恢复CP的保证，确保在几何变换下的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 传统CP在数据分布变化时性能下降，无法提供有效的后验不确定性量化和覆盖保证，因此需要一种方法在几何变换下保持CP的实用性。

Method: 结合几何信息（如姿态规范化）到CP过程中，利用姿态规范化作为信息提取器，评估其在离散和连续变换下的表现。

Result: 实验表明，整合几何信息的CP方法能够有效应对几何变换，同时适用于黑盒预测器。

Conclusion: 通过整合几何信息，CP能够在几何变换下保持鲁棒性和广泛适用性。

Abstract: We study the problem of conformal prediction (CP) under geometric data
shifts, where data samples are susceptible to transformations such as rotations
or flips. While CP endows prediction models with post-hoc uncertainty
quantification and formal coverage guarantees, their practicality breaks under
distribution shifts that deteriorate model performance. To address this issue,
we propose integrating geometric information--such as geometric pose--into the
conformal procedure to reinstate its guarantees and ensure robustness under
geometric shifts. In particular, we explore recent advancements on pose
canonicalization as a suitable information extractor for this purpose.
Evaluating the combined approach across discrete and continuous shifts and
against equivariant and augmentation-based baselines, we find that integrating
geometric information with CP yields a principled way to address geometric
shifts while maintaining broad applicability to black-box predictors.

</details>


### [497] [Latent Noise Injection for Private and Statistically Aligned Synthetic Data Generation](https://arxiv.org/abs/2506.16636)
*Rex Shen,Lu Tian*

Main category: stat.ML

TL;DR: 提出了一种基于掩码自回归流（MAF）的潜在噪声注入方法，用于生成高维合成数据，解决了传统生成模型收敛慢的问题，同时满足差分隐私要求。


<details>
  <summary>Details</summary>
Motivation: 传统生成模型在高维数据中收敛慢，且难以平衡隐私与效用。

Method: 通过潜在噪声注入扰动数据点，保持观测与合成数据的一一对应，结合元分析框架恢复经典效率。

Result: 方法在统计对齐和隐私保护方面表现优异，对抗成员推理攻击具有鲁棒性。

Conclusion: 潜在噪声注入是隐私敏感领域合成数据共享的有效替代方案。

Abstract: Synthetic Data Generation has become essential for scalable,
privacy-preserving statistical analysis. While standard approaches based on
generative models, such as Normalizing Flows, have been widely used, they often
suffer from slow convergence in high-dimensional settings, frequently
converging more slowly than the canonical $1/\sqrt{n}$ rate when approximating
the true data distribution.
  To overcome these limitations, we propose a Latent Noise Injection method
using Masked Autoregressive Flows (MAF). Instead of directly sampling from the
trained model, our method perturbs each data point in the latent space and maps
it back to the data domain. This construction preserves a one to one
correspondence between observed and synthetic data, enabling synthetic outputs
that closely reflect the underlying distribution, particularly in challenging
high-dimensional regimes where traditional sampling struggles.
  Our procedure satisfies local $(\epsilon, \delta)$-differential privacy and
introduces a single perturbation parameter to control the privacy-utility
trade-off. Although estimators based on individual synthetic datasets may
converge slowly, we show both theoretically and empirically that aggregating
across $K$ studies in a meta analysis framework restores classical efficiency
and yields consistent, reliable inference. We demonstrate that with a
well-calibrated perturbation parameter, Latent Noise Injection achieves strong
statistical alignment with the original data and robustness against membership
inference attacks. These results position our method as a compelling
alternative to conventional flow-based sampling for synthetic data sharing in
decentralized and privacy-sensitive domains, such as biomedical research.

</details>


### [498] [Sampling conditioned diffusions via Pathspace Projected Monte Carlo](https://arxiv.org/abs/2506.15743)
*Tobias Grafke*

Main category: stat.ML

TL;DR: 提出一种算法，用于采样满足一般约束条件的随机微分方程，包括积分约束、端点约束和随机积分约束。


<details>
  <summary>Details</summary>
Motivation: 解决在复杂约束条件下采样随机微分方程的挑战。

Method: 采用路径空间Metropolis调整流形采样方案，采样满足约束条件的随机路径。

Result: 算法在多种场景中有效，如动态凝聚相变、固定Levy随机面积的随机行走、高振幅波的非线性随机波方程等。

Conclusion: 该算法在复杂约束条件下采样随机微分方程具有广泛适用性和有效性。

Abstract: We present an algorithm to sample stochastic differential equations
conditioned on rather general constraints, including integral constraints,
endpoint constraints, and stochastic integral constraints. The algorithm is a
pathspace Metropolis-adjusted manifold sampling scheme, which samples
stochastic paths on the submanifold of realizations that adhere to the
conditioning constraint. We demonstrate the effectiveness of the algorithm by
sampling a dynamical condensation phase transition, conditioning a random walk
on a fixed Levy stochastic area, conditioning a stochastic nonlinear wave
equation on high amplitude waves, and sampling a stochastic partial
differential equation model of turbulent pipe flow conditioned on
relaminarization events.

</details>


### [499] [From Local Interactions to Global Operators: Scalable Gaussian Process Operator for Physical Systems](https://arxiv.org/abs/2506.15906)
*Sawan Kumar,Tapas Tripura,Rajdip Nayek,Souvik Chakraborty*

Main category: stat.ML

TL;DR: 提出了一种可扩展的高斯过程算子（GPO）方法，通过稀疏性、局部性和结构化核设计解决高维数据问题，并在多种非线性PDE中验证了其高精度。


<details>
  <summary>Details</summary>
Motivation: 解决概率神经算子（如GPO）在高维、数据密集型场景下的可扩展性问题。

Method: 利用空间域的最近邻局部核近似、参数空间的稀疏核近似和结构化Kronecker分解，结合算子感知的核结构和任务驱动的均值函数。

Result: 在Navier-Stokes、波动平流、Darcy流和Burgers方程等非线性PDE中表现出高精度。

Conclusion: 该方法在可扩展性和准确性之间取得了平衡，为复杂物理系统中的不确定性建模提供了有力工具。

Abstract: Operator learning offers a powerful paradigm for solving parametric partial
differential equations (PDEs), but scaling probabilistic neural operators such
as the recently proposed Gaussian Processes Operators (GPOs) to
high-dimensional, data-intensive regimes remains a significant challenge. In
this work, we introduce a novel, scalable GPO, which capitalizes on sparsity,
locality, and structural information through judicious kernel design.
Addressing the fundamental limitation of cubic computational complexity, our
method leverages nearest-neighbor-based local kernel approximations in the
spatial domain, sparse kernel approximation in the parameter space, and
structured Kronecker factorizations to enable tractable inference on
large-scale datasets and high-dimensional input. While local approximations
often introduce accuracy trade-offs due to limited kernel interactions, we
overcome this by embedding operator-aware kernel structures and employing
expressive, task-informed mean functions derived from neural operator
architectures. Through extensive evaluations on a broad class of nonlinear PDEs
- including Navier-Stokes, wave advection, Darcy flow, and Burgers' equations -
we demonstrate that our framework consistently achieves high accuracy across
varying discretization scales. These results underscore the potential of our
approach to bridge the gap between scalability and fidelity in GPO, offering a
compelling foundation for uncertainty-aware modeling in complex physical
systems.

</details>


### [500] [Diffusion-Based Hypothesis Testing and Change-Point Detection](https://arxiv.org/abs/2506.16089)
*Sean Moushegian,Taposh Banerjee,Vahid Tarokh*

Main category: stat.ML

TL;DR: 论文提出了一种基于扩散的假设检验和变点检测方法，通过优化权重矩阵提升性能，并进行了理论分析和数值模拟验证。


<details>
  <summary>Details</summary>
Motivation: 现有的基于得分函数的方法在假设检验和变点检测中性能不如基于似然的方法，因此需要改进。

Method: 将得分函数通过矩阵变换推广为扩散散度，并优化权重矩阵。

Result: 理论分析和数值模拟表明，扩散基算法在特定场景下能实现最优性能。

Conclusion: 扩散基算法在假设检验和变点检测中具有优势，优化权重矩阵可进一步提升性能。

Abstract: Score-based methods have recently seen increasing popularity in modeling and
generation. Methods have been constructed to perform hypothesis testing and
change-point detection with score functions, but these methods are in general
not as powerful as their likelihood-based peers. Recent works consider
generalizing the score-based Fisher divergence into a diffusion-divergence by
transforming score functions via multiplication with a matrix-valued function
or a weight matrix. In this paper, we extend the score-based hypothesis test
and change-point detection stopping rule into their diffusion-based analogs.
Additionally, we theoretically quantify the performance of these
diffusion-based algorithms and study scenarios where optimal performance is
achievable. We propose a method of numerically optimizing the weight matrix and
present numerical simulations to illustrate the advantages of diffusion-based
algorithms.

</details>


### [501] [Random feature approximation for general spectral methods](https://arxiv.org/abs/2506.16283)
*Mike Nguyen,Nicole Mücke*

Main category: stat.ML

TL;DR: 该论文研究了随机特征方法在核方法中的泛化性质，扩展了Tikhonov正则化的结果，涵盖了梯度下降等隐式方法，并通过NTK框架分析了神经网络的理论性质。


<details>
  <summary>Details</summary>
Motivation: 随机特征近似是大规模学习算法中广泛使用的技术，但其泛化性质的理论分析尚不完善，尤其是对隐式正则化方法的扩展。

Method: 通过谱正则化技术框架，分析了包括梯度下降和加速算法在内的多种方法，并结合NTK理论分析神经网络。

Result: 在适当的源条件下，获得了最优学习率，即使对于不在再生核希尔伯特空间中的类别。

Conclusion: 该研究改进了先前特定核算法的结果，为随机特征方法和神经网络的理论分析提供了更全面的框架。

Abstract: Random feature approximation is arguably one of the most widely used
techniques for kernel methods in large-scale learning algorithms. In this work,
we analyze the generalization properties of random feature methods, extending
previous results for Tikhonov regularization to a broad class of spectral
regularization techniques. This includes not only explicit methods but also
implicit schemes such as gradient descent and accelerated algorithms like the
Heavy-Ball and Nesterov method. Through this framework, we enable a theoretical
analysis of neural networks and neural operators through the lens of the Neural
Tangent Kernel (NTK) approach trained via gradient descent. For our estimators
we obtain optimal learning rates over regularity classes (even for classes that
are not included in the reproducing kernel Hilbert space), which are defined
through appropriate source conditions. This improves or completes previous
results obtained in related settings for specific kernel algorithms.

</details>


### [502] [The Condition Number as a Scale-Invariant Proxy for Information Encoding in Neural Units](https://arxiv.org/abs/2506.16289)
*Oswaldo Ludwig*

Main category: stat.ML

TL;DR: 论文探讨了神经网络权重张量的条件数与信息编码效率的关系，提出高条件数可能表示选择性信息放大与压缩，并通过线性单元和高斯输入验证。


<details>
  <summary>Details</summary>
Motivation: 研究条件数对信息编码的影响，为高效编码策略提供理论支持。

Method: 通过分析条件数与对数体积缩放因子的关系，结合输出熵和几何特性，验证信息传递效率。

Result: 高条件数对应减少的信息传递，表明高效编码策略；选择性微调方法可缓解跨模态适应中的灾难性遗忘。

Conclusion: 条件数可作为信息编码效率的指标，选择性微调方法为缓解灾难性遗忘提供新思路。

Abstract: This paper explores the relationship between the condition number of a neural
network's weight tensor and the extent of information encoded by the associated
processing unit, viewed through the lens of information theory. We argue that a
high condition number, though not sufficient for effective knowledge encoding,
may indicate that the unit has learned to selectively amplify and compress
information. We formalize this intuition, particularly for linear units with
Gaussian inputs, linking the condition number and the transformation's
log-volume scaling factor to the characteristics of the output entropy and the
geometric properties of the learned transformation. Our analysis demonstrates
that for a fixed weight norm, a concentrated distribution of singular values
(high condition number) corresponds to reduced overall information transfer,
indicating a specialized and efficient encoding strategy. Furthermore, we
present a practical case study where these principles are applied to guide
selective fine-tuning of a multimodal Large Language Model, aiming to mitigate
catastrophic forgetting during cross-modal adaptation. Unlike many existing
catastrophic forgetting mitigation methods that rely on access to pre-training
statistics, which are often unavailable, our selective fine-tuning approach
offers a way to bypass this common requirement.

</details>


### [503] [Identifying Heterogeneity in Distributed Learning](https://arxiv.org/abs/2506.16394)
*Zelin Xiao,Jia Gu,Song Xi Chen*

Main category: stat.ML

TL;DR: 论文研究了在分布式M估计中识别异质参数分量的方法，提出了两种测试：基于重归一化Wald检验的方法和极端对比测试（ECT）。Wald检验适用于异质性密集且数据块数K较小时，而ECT适用于异质性稀疏且K较大时。两种方法结合可提升鲁棒性。实验验证了方法的有效性和性能。


<details>
  <summary>Details</summary>
Motivation: 在分布式M估计中，识别异质参数分量是一个重要问题，但现有方法在数据块数K较大或异质性稀疏时表现不佳。本文旨在提出更高效且适应性强的方法。

Method: 提出了两种方法：1）基于重归一化Wald检验的方法，适用于异质性密集且K较小；2）极端对比测试（ECT），适用于异质性稀疏且K较大。通过样本分割避免偏差积累，并结合两种方法提升鲁棒性。

Result: 实验表明，Wald检验在异质性密集时表现良好，而ECT在异质性稀疏且K较大时更优。结合方法在不同稀疏度下均表现出稳健性能。

Conclusion: 本文提出的方法在分布式M估计中有效识别异质参数分量，尤其在数据块数K较大或异质性稀疏时表现突出，具有实际应用价值。

Abstract: We study methods for identifying heterogeneous parameter components in
distributed M-estimation with minimal data transmission. One is based on a
re-normalized Wald test, which is shown to be consistent as long as the number
of distributed data blocks $K$ is of a smaller order of the minimum block
sample size {and the level of heterogeneity is dense}. The second one is an
extreme contrast test (ECT) based on the difference between the largest and
smallest component-wise estimated parameters among data blocks. By introducing
a sample splitting procedure, the ECT can avoid the bias accumulation arising
from the M-estimation procedures, and exhibits consistency for $K$ being much
larger than the sample size while the heterogeneity is sparse. The ECT
procedure is easy to operate and communication-efficient. A combination of the
Wald and the extreme contrast tests is formulated to attain more robust power
under varying levels of sparsity of the heterogeneity. We also conduct
intensive numerical experiments to compare the family-wise error rate (FWER)
and the power of the proposed methods. Additionally, we conduct a case study to
present the implementation and validity of the proposed methods.

</details>


### [504] [On Continuous Monitoring of Risk Violations under Unknown Shift](https://arxiv.org/abs/2506.16416)
*Alexander Timans,Rajeev Verma,Eric Nalisnick,Christian A. Naesseth*

Main category: stat.ML

TL;DR: 提出了一种实时监控机器学习系统风险违规的通用框架，利用顺序假设检验检测风险边界违规，同时控制误报率。


<details>
  <summary>Details</summary>
Motivation: 现实中的机器学习系统面临动态且不可预测的分布变化，传统风险控制框架无法持续监控可靠性。

Method: 基于'测试下注'范式，提出顺序假设检验方法，检测模型决策机制的风险违规，最小化假设需求。

Result: 在异常检测和集合预测任务中展示了方法的有效性，适用于多种分布变化。

Conclusion: 该框架广泛适用，能实时监控风险，确保系统可靠性。

Abstract: Machine learning systems deployed in the real world must operate under
dynamic and often unpredictable distribution shifts. This challenges the
validity of statistical safety assurances on the system's risk established
beforehand. Common risk control frameworks rely on fixed assumptions and lack
mechanisms to continuously monitor deployment reliability. In this work, we
propose a general framework for the real-time monitoring of risk violations in
evolving data streams. Leveraging the 'testing by betting' paradigm, we propose
a sequential hypothesis testing procedure to detect violations of bounded risks
associated with the model's decision-making mechanism, while ensuring control
on the false alarm rate. Our method operates under minimal assumptions on the
nature of encountered shifts, rendering it broadly applicable. We illustrate
the effectiveness of our approach by monitoring risks in outlier detection and
set prediction under a variety of shifts.

</details>


### [505] [Schrödinger Bridge Matching for Tree-Structured Costs and Entropic Wasserstein Barycentres](https://arxiv.org/abs/2506.17197)
*Samuel Howard,Peter Potaptchik,George Deligiannidis*

Main category: stat.ML

TL;DR: 本文提出了一种扩展的IMF方法，用于解决树结构SB问题，继承了IMF优于IPF的特性，并在Wasserstein重心计算中推广了流式熵OT求解器。


<details>
  <summary>Details</summary>
Motivation: 传统方法如IPF在解决SB问题时存在局限性，而IMF方法具有更优特性。本文旨在将IMF扩展到树结构SB问题，以解决多边际最优传输问题。

Method: 通过扩展IMF方法，提出了一种解决树结构SB问题的算法，特别适用于Wasserstein重心计算。

Result: 新算法在树结构SB问题中表现出色，继承了IMF的优越性，并成功推广到流式熵OT求解器。

Conclusion: 本文提出的扩展IMF方法为树结构SB问题提供了高效解决方案，并在Wasserstein重心计算中展示了其潜力。

Abstract: Recent advances in flow-based generative modelling have provided scalable
methods for computing the Schr\"odinger Bridge (SB) between distributions, a
dynamic form of entropy-regularised Optimal Transport (OT) for the quadratic
cost. The successful Iterative Markovian Fitting (IMF) procedure solves the SB
problem via sequential bridge-matching steps, presenting an elegant and
practical approach with many favourable properties over the more traditional
Iterative Proportional Fitting (IPF) procedure. Beyond the standard setting,
optimal transport can be generalised to the multi-marginal case in which the
objective is to minimise a cost defined over several marginal distributions. Of
particular importance are costs defined over a tree structure, from which
Wasserstein barycentres can be recovered as a special case. In this work, we
extend the IMF procedure to solve for the tree-structured SB problem. Our
resulting algorithm inherits the many advantages of IMF over IPF approaches in
the tree-based setting. In the specific case of Wasserstein barycentres, our
approach can be viewed as extending fixed-point approaches for barycentre
computation to the case of flow-based entropic OT solvers.

</details>


<div id='astro-ph.EP'></div>

# astro-ph.EP [[Back]](#toc)

### [506] [Exoplanet Classification through Vision Transformers with Temporal Image Analysis](https://arxiv.org/abs/2506.16597)
*Anupma Choudhary,Sohith Bandari,B. S. Kushvah,C. Swastik*

Main category: astro-ph.EP

TL;DR: 该研究提出了一种利用Gramian Angular Fields和Recurrence Plots转换开普勒任务光曲线数据，并通过Vision Transformer模型高效分类系外行星的方法。结果显示Recurrence Plots表现更优，模型在召回率和精确率上表现突出，但数据不平衡和规模限制仍需改进。


<details>
  <summary>Details</summary>
Motivation: 传统系外行星分类方法资源消耗大，需高效机器学习技术提升效率。

Method: 将开普勒任务光曲线数据转换为GAFs和RPs，输入Vision Transformer模型，并通过5折交叉验证评估性能。

Result: RPs优于GAFs，ViT模型召回率89.46%，精确率85.09%，但数据不平衡问题仍存在。

Conclusion: 研究强调需进一步优化模型架构以提升自动化、性能和泛化能力。

Abstract: The classification of exoplanets has been a longstanding challenge in
astronomy, requiring significant computational and observational resources.
Traditional methods demand substantial effort, time, and cost, highlighting the
need for advanced machine learning techniques to enhance classification
efficiency. In this study, we propose a methodology that transforms raw light
curve data from NASA's Kepler mission into Gramian Angular Fields (GAFs) and
Recurrence Plots (RPs) using the Gramian Angular Difference Field and
recurrence plot techniques. These transformed images serve as inputs to the
Vision Transformer (ViT) model, leveraging its ability to capture intricate
temporal dependencies. We assess the performance of the model through recall,
precision, and F1 score metrics, using a 5-fold cross-validation approach to
obtain a robust estimate of the model's performance and reduce evaluation bias.
Our comparative analysis reveals that RPs outperform GAFs, with the ViT model
achieving an 89.46$\%$ recall and an 85.09$\%$ precision rate, demonstrating
its significant capability in accurately identifying exoplanetary transits.
Despite using under-sampling techniques to address class imbalance, dataset
size reduction remains a limitation. This study underscores the importance of
further research into optimizing model architectures to enhance automation,
performance, and generalization of the model.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [507] [Data-Agnostic Cardinality Learning from Imperfect Workloads](https://arxiv.org/abs/2506.16007)
*Peizhi Wu,Rong Kang,Tieying Zhang,Jianjun Chen,Ryan Marcus,Zachary G. Ives*

Main category: cs.DB

TL;DR: GRASP是一种数据无关的基数估计系统，能在真实世界的不完整和不平衡查询工作负载下高效工作。


<details>
  <summary>Details</summary>
Motivation: 传统基数估计方法依赖数据统计，但受限于组织政策无法全局访问数据。现有查询驱动模型需要数据或摘要支持，且假设训练工作负载完整平衡，这在现实中不成立。

Method: GRASP采用组合设计，支持未见过的连接模板，并引入新的每表基数估计模型和学习的计数草图模型。

Result: 在三个数据库实例中，GRASP在不完美工作负载下优于现有查询驱动模型，甚至在CEB-IMDb-full基准测试中接近或超越传统方法。

Conclusion: GRASP在无数据访问和仅使用10%连接模板的情况下，实现了高精度和低延迟的基数估计。

Abstract: Cardinality estimation (CardEst) is a critical aspect of query optimization.
Traditionally, it leverages statistics built directly over the data. However,
organizational policies (e.g., regulatory compliance) may restrict global data
access. Fortunately, query-driven cardinality estimation can learn CardEst
models using query workloads. However, existing query-driven models often
require access to data or summaries for best performance, and they assume
perfect training workloads with complete and balanced join templates (or join
graphs). Such assumptions rarely hold in real-world scenarios, in which join
templates are incomplete and imbalanced. We present GRASP, a data-agnostic
cardinality learning system designed to work under these real-world
constraints. GRASP's compositional design generalizes to unseen join templates
and is robust to join template imbalance. It also introduces a new per-table
CardEst model that handles value distribution shifts for range predicates, and
a novel learned count sketch model that captures join correlations across base
relations. Across three database instances, we demonstrate that GRASP
consistently outperforms existing query-driven models on imperfect workloads,
both in terms of estimation accuracy and query latency. Remarkably, GRASP
achieves performance comparable to, or even surpassing, traditional approaches
built over the underlying data on the complex CEB-IMDb-full benchmark --
despite operating without any data access and using only 10% of all possible
join templates.

</details>


<div id='physics.flu-dyn'></div>

# physics.flu-dyn [[Back]](#toc)

### [508] [Contactless Precision Steering of Particles in a Fluid inside a Cube with Rotating Walls](https://arxiv.org/abs/2506.15958)
*Lucas Amoudruz,Petr Karnakov,Petros Koumoutsakos*

Main category: physics.flu-dyn

TL;DR: 提出了一种新型控制算法，通过旋转盘生成流场，实现多颗粒的精确操控，适用于生物医学应用。


<details>
  <summary>Details</summary>
Motivation: 无接触操控小物体在生物医学和化学应用中至关重要，但现有方法难以同时操控多个颗粒。

Method: 采用基于ODIL框架的反馈控制策略，结合流体动力学方程和路径目标，生成流场操控颗粒。

Result: 实验证明该方法能同时将两个颗粒精确运输到预定位置。

Conclusion: 该算法为生物医学应用中的无接触颗粒操控提供了新方案。

Abstract: Contactless manipulation of small objects is essential for biomedical and
chemical applications, such as cell analysis, assisted fertilisation, and
precision chemistry. Established methods, including optical, acoustic, and
magnetic tweezers, are now complemented by flow control techniques that use
flow-induced motion to enable precise and versatile manipulation. However,
trapping multiple particles in fluid remains a challenge. This study introduces
a novel control algorithm capable of steering multiple particles in flow. The
system uses rotating disks to generate flow fields that transport particles to
precise locations. Disk rotations are governed by a feedback control policy
based on the Optimising a Discrete Loss (ODIL) framework, which combines fluid
dynamics equations with path objectives into a single loss function. Our
experiments, conducted in both simulations and with the physical device,
demonstrate the capability of the approach to transport two beads
simultaneously to predefined locations, advancing robust contactless particle
manipulation for biomedical applications.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [509] [Human-Centered Shared Autonomy for Motor Planning, Learning, and Control Applications](https://arxiv.org/abs/2506.16044)
*MH Farhadi,Ali Rabiee,Sima Ghafoori,Anna Cetera,Wei Xu,Reza Abiri*

Main category: cs.HC

TL;DR: 本文综述了以人为中心的共享自主AI框架，重点研究基于上肢生物信号的机器接口及其在医疗中的应用，探讨了人机协作的理论与实践。


<details>
  <summary>Details</summary>
Motivation: 在医疗领域，完全独立的机器决策可能不适合，因为人类意图至关重要。因此，需要开发人机协作的共享自主AI框架。

Method: 通过综述和分析，探讨了基于生物信号的共享自主AI框架，包括运动规划、学习与控制，以及脑机接口、康复和辅助机器人等应用。

Result: 提出了自适应共享自主AI作为高性能人机协作范式，并指出了关键挑战和未来方向。

Conclusion: 本文旨在将神经科学与机器人技术结合，构建更直观、有效且伦理的人机协作框架。

Abstract: With recent advancements in AI and computational tools, intelligent paradigms
have emerged to enhance fields like shared autonomy and human-machine teaming
in healthcare. Advanced AI algorithms (e.g., reinforcement learning) can
autonomously make decisions to achieve planning and motion goals. However, in
healthcare, where human intent is crucial, fully independent machine decisions
may not be ideal. This chapter presents a comprehensive review of
human-centered shared autonomy AI frameworks, focusing on upper limb
biosignal-based machine interfaces and associated motor control systems,
including computer cursors, robotic arms, and planar platforms. We examine
motor planning, learning (rehabilitation), and control, covering conceptual
foundations of human-machine teaming in reach-and-grasp tasks and analyzing
both theoretical and practical implementations. Each section explores how human
and machine inputs can be blended for shared autonomy in healthcare
applications. Topics include human factors, biosignal processing for intent
detection, shared autonomy in brain-computer interfaces (BCI), rehabilitation,
assistive robotics, and Large Language Models (LLMs) as the next frontier. We
propose adaptive shared autonomy AI as a high-performance paradigm for
collaborative human-AI systems, identify key implementation challenges, and
outline future directions, particularly regarding AI reasoning agents. This
analysis aims to bridge neuroscientific insights with robotics to create more
intuitive, effective, and ethical human-machine teaming frameworks.

</details>


### [510] [PPTP: Performance-Guided Physiological Signal-Based Trust Prediction in Human-Robot Collaboration](https://arxiv.org/abs/2506.16677)
*Hao Guo,Wei Fan,Shaohui Liu,Feng Jiang,Chunzhi Yi*

Main category: cs.HC

TL;DR: 论文提出了一种基于性能引导的多模态生理信号信任预测框架（PPTP），用于建筑场景中的人机协作信任评估，显著提升了分类性能。


<details>
  <summary>Details</summary>
Motivation: 在建筑场景中，人机协作的信任预测对安全和效率至关重要，但现有方法难以准确评估信任水平。

Method: 设计了三种难度的人机协作场景，结合多模态生理信号（ECG、GSR、EMG）和协作性能评估，提出PPTP框架。

Result: 模型在三级别信任分类中准确率达81%，七级别分类中达74.3%，优于基线方法6.7%。

Conclusion: PPTP框架通过性能引导的生理信号处理，显著提升了信任预测的准确性，为相关研究提供了新思路。

Abstract: Trust prediction is a key issue in human-robot collaboration, especially in
construction scenarios where maintaining appropriate trust calibration is
critical for safety and efficiency. This paper introduces the
Performance-guided Physiological signal-based Trust Prediction (PPTP), a novel
framework designed to improve trust assessment. We designed a human-robot
construction scenario with three difficulty levels to induce different trust
states. Our approach integrates synchronized multimodal physiological signals
(ECG, GSR, and EMG) with collaboration performance evaluation to predict human
trust levels. Individual physiological signals are processed using
collaboration performance information as guiding cues, leveraging the
standardized nature of collaboration performance to compensate for individual
variations in physiological responses. Extensive experiments demonstrate the
efficacy of our cross-modality fusion method in significantly improving trust
classification performance. Our model achieves over 81% accuracy in three-level
trust classification, outperforming the best baseline method by 6.7%, and
notably reaches 74.3% accuracy in high-resolution seven-level classification,
which is a first in trust prediction research. Ablation experiments further
validate the superiority of physiological signal processing guided by
collaboration performance assessment.

</details>


### [511] [Do We Talk to Robots Like Therapists, and Do They Respond Accordingly? Language Alignment in AI Emotional Support](https://arxiv.org/abs/2506.16473)
*Sophie Chiang,Guy Laban,Hatice Gunes*

Main category: cs.HC

TL;DR: 研究比较了机器人对话与传统治疗对话的相似性，发现90.88%的机器人对话主题可映射到人类治疗对话中，且语义重叠显著。


<details>
  <summary>Details</summary>
Motivation: 探讨机器人情感支持对话与传统治疗对话的相似性，以评估其在心理健康干预中的潜力。

Method: 分析两个数据集（人类治疗对话和机器人对话），使用句子嵌入和K-means聚类评估主题对齐，并通过距离方法验证。

Result: 机器人对话主题与人类治疗对话高度匹配，语义分析显示响应内容也有显著重叠。

Conclusion: 机器人支持对话与传统治疗对话有显著相似性，可能成为心理健康干预的补充工具。

Abstract: As conversational agents increasingly engage in emotionally supportive
dialogue, it is important to understand how closely their interactions resemble
those in traditional therapy settings. This study investigates whether the
concerns shared with a robot align with those shared in human-to-human (H2H)
therapy sessions, and whether robot responses semantically mirror those of
human therapists. We analyzed two datasets: one of interactions between users
and professional therapists (Hugging Face's NLP Mental Health Conversations),
and another involving supportive conversations with a social robot (QTrobot
from LuxAI) powered by a large language model (LLM, GPT-3.5). Using sentence
embeddings and K-means clustering, we assessed cross-agent thematic alignment
by applying a distance-based cluster-fitting method that evaluates whether
responses from one agent type map to clusters derived from the other, and
validated it using Euclidean distances. Results showed that 90.88% of robot
conversation disclosures could be mapped to clusters from the human therapy
dataset, suggesting shared topical structure. For matched clusters, we compared
the subjects as well as therapist and robot responses using Transformer,
Word2Vec, and BERT embeddings, revealing strong semantic overlap in subjects'
disclosures in both datasets, as well as in the responses given to similar
human disclosure themes across agent types (robot vs. human therapist). These
findings highlight both the parallels and boundaries of robot-led support
conversations and their potential for augmenting mental health interventions.

</details>


### [512] [On using AI for EEG-based BCI applications: problems, current challenges and future trends](https://arxiv.org/abs/2506.16168)
*Thomas Barbera,Jacopo Burger,Alessandro D'Amelio,Simone Zini,Simone Bianco,Raffaella Lanzarotti,Paolo Napoletano,Giuseppe Boccignone,Jose Luis Contreras-Vidal*

Main category: cs.HC

TL;DR: 论文探讨了AI在解码脑电图（EEG）信号中的应用，旨在开发实用的脑机接口（BCI），并分析了当前的技术挑战和未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 利用AI技术解码EEG信号，推动脑机接口（如脑到语音、脑到图像等）的实际应用，超越传统用途。

Method: 通过因果视角分析基本范式，探讨AI模型在EEG-BCI中的挑战，并提出未来研究方向。

Result: 揭示了当前技术的局限性，并提出了克服技术、方法和伦理障碍的潜在途径。

Conclusion: 为开发实用且高效的EEG-BCI解决方案提供了清晰的路线图。

Abstract: Imagine unlocking the power of the mind to communicate, create, and even
interact with the world around us. Recent breakthroughs in Artificial
Intelligence (AI), especially in how machines "see" and "understand" language,
are now fueling exciting progress in decoding brain signals from scalp
electroencephalography (EEG). Prima facie, this opens the door to revolutionary
brain-computer interfaces (BCIs) designed for real life, moving beyond
traditional uses to envision Brain-to-Speech, Brain-to-Image, and even a
Brain-to-Internet of Things (BCIoT).
  However, the journey is not as straightforward as it was for Computer Vision
(CV) and Natural Language Processing (NLP). Applying AI to real-world EEG-based
BCIs, particularly in building powerful foundational models, presents unique
and intricate hurdles that could affect their reliability.
  Here, we unfold a guided exploration of this dynamic and rapidly evolving
research area. Rather than barely outlining a map of current endeavors and
results, the goal is to provide a principled navigation of this hot and
cutting-edge research landscape. We consider the basic paradigms that emerge
from a causal perspective and the attendant challenges presented to AI-based
models. Looking ahead, we then discuss promising research avenues that could
overcome today's technological, methodological, and ethical limitations. Our
aim is to lay out a clear roadmap for creating truly practical and effective
EEG-based BCI solutions that can thrive in everyday environments.

</details>


<div id='eess.SY'></div>

# eess.SY [[Back]](#toc)

### [513] [Vision-Based Multirotor Control for Spherical Target Tracking: A Bearing-Angle Approach](https://arxiv.org/abs/2506.16870)
*Marcelo Jacinto,Rita Cunha*

Main category: eess.SY

TL;DR: 设计了一种基于视觉伺服的控制器，用于多旋翼飞行器跟踪未知半径的移动球形目标。


<details>
  <summary>Details</summary>
Motivation: 解决多旋翼飞行器在未知半径球形目标跟踪中的控制问题。

Method: 将相机传感器的两个方位测量转换为方位-角度对，并利用新坐标系设计自适应非线性控制算法。

Result: 仿真结果表明所提控制算法性能良好。

Conclusion: 通过新坐标系和自适应控制算法，成功实现了对移动球形目标的跟踪。

Abstract: This work addresses the problem of designing a visual servo controller for a
multirotor vehicle, with the end goal of tracking a moving spherical target
with unknown radius. To address this problem, we first transform two bearing
measurements provided by a camera sensor into a bearing-angle pair. We then use
this information to derive the system's dynamics in a new set of coordinates,
where the angle measurement is used to quantify a relative distance to the
target. Building on this system representation, we design an adaptive nonlinear
control algorithm that takes advantage of the properties of the new system
geometry and assumes that the target follows a constant acceleration model.
Simulation results illustrate the performance of the proposed control
algorithm.

</details>


<div id='q-fin.ST'></div>

# q-fin.ST [[Back]](#toc)

### [514] [Modern approaches to building effective interpretable models of the property market using machine learning](https://arxiv.org/abs/2506.15723)
*Irina G. Tanashkina,Alexey S. Tanashkin,Alexander S. Maksimchuik,Anna Yu. Poshivailo*

Main category: q-fin.ST

TL;DR: 本文回顾了利用机器学习构建房地产市场价格可解释模型的现代方法，以俄罗斯滨海边疆区的大规模房地产估价为基础。研究者通过解决数据噪声与理想数据差异等问题，展示了如何构建有效模型。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于解决房地产市场中数据噪声与理想数据差异带来的建模困难，同时满足可解释性要求。

Method: 方法包括数据收集、异常值识别、模式分析、价格因素选择、模型构建及效率评估。针对土地和公寓分别采用线性回归结合地统计插值法和RuleFit方法。

Result: 结果表明，尽管可解释性要求严格，仍能构建有效的房地产市场模型。

Conclusion: 结论是，即使在可解释性限制下，也能通过合适方法构建实用的房地产市场模型。

Abstract: In this article, we review modern approaches to building interpretable models
of property markets using machine learning on the base of mass valuation of
property in the Primorye region, Russia. The researcher, lacking expertise in
this topic, encounters numerous difficulties in the effort to build a good
model. The main source of this is the huge difference between noisy real market
data and ideal data which is very common in all types of tutorials on machine
learning. This paper covers all stages of modeling: the collection of initial
data, identification of outliers, the search and analysis of patterns in data,
the formation and final choice of price factors, the building of the model, and
the evaluation of its efficiency. For each stage, we highlight potential issues
and describe sound methods for overcoming emerging difficulties on actual
examples. We show that the combination of classical linear regression with
interpolation methods of geostatistics allows to build an effective model for
land parcels. For flats, when many objects are attributed to one spatial point
the application of geostatistical methods is difficult. Therefore we suggest
linear regression with automatic generation and selection of additional rules
on the base of decision trees, so called the RuleFit method. Thus we show, that
despite the strong restriction as the requirement of interpretability which is
important in practical aspects, for example, legal matters, it is still
possible to build effective models of real property markets.

</details>


<div id='astro-ph.GA'></div>

# astro-ph.GA [[Back]](#toc)

### [515] [Can AI Dream of Unseen Galaxies? Conditional Diffusion Model for Galaxy Morphology Augmentation](https://arxiv.org/abs/2506.16233)
*Chenrui Ma,Zechang Sun,Tao Jing,Zheng Cai,Yuan-Sen Ting,Song Huang,Mingyu Li*

Main category: astro-ph.GA

TL;DR: 提出了一种条件扩散模型，用于合成高质量星系图像以增强机器学习训练数据，显著提升了稀有天体检测的性能。


<details>
  <summary>Details</summary>
Motivation: 解决天文观测中因标记数据有限（尤其是稀有天体）导致的机器学习模型泛化能力不足的问题。

Method: 利用Galaxy Zoo 2数据集，开发条件扩散模型生成符合形态特征的星系图像，并用于数据增强。

Result: 合成图像提升了形态分类性能（关键指标提高30%），稀有天体检测数量翻倍（从352增至872）。

Conclusion: 生成模型能有效填补标记数据不足的空白，为未来天体物理基础模型开发提供启示。

Abstract: Observational astronomy relies on visual feature identification to detect
critical astrophysical phenomena. While machine learning (ML) increasingly
automates this process, models often struggle with generalization in
large-scale surveys due to the limited representativeness of labeled datasets
-- whether from simulations or human annotation -- a challenge pronounced for
rare yet scientifically valuable objects. To address this, we propose a
conditional diffusion model to synthesize realistic galaxy images for
augmenting ML training data. Leveraging the Galaxy Zoo 2 dataset which contains
visual feature -- galaxy image pairs from volunteer annotation, we demonstrate
that our model generates diverse, high-fidelity galaxy images closely adhere to
the specified morphological feature conditions. Moreover, this model enables
generative extrapolation to project well-annotated data into unseen domains and
advancing rare object detection. Integrating synthesized images into ML
pipelines improves performance in standard morphology classification, boosting
completeness and purity by up to 30\% across key metrics. For rare object
detection, using early-type galaxies with prominent dust lane features (
$\sim$0.1\% in GZ2 dataset) as a test case, our approach doubled the number of
detected instances from 352 to 872, compared to previous studies based on
visual inspection. This study highlights the power of generative models to
bridge gaps between scarce labeled data and the vast, uncharted parameter space
of observational astronomy and sheds insight for future astrophysical
foundation model developments. Our project homepage is available at
https://galaxysd-webpage.streamlit.app/.

</details>


<div id='physics.soc-ph'></div>

# physics.soc-ph [[Back]](#toc)

### [516] [Autocratic strategies in Cournot oligopoly game](https://arxiv.org/abs/2506.16038)
*Masahiko Ueda,Shoma Yagi,Genki Ichinose*

Main category: physics.soc-ph

TL;DR: 论文研究了重复古诺寡头博弈中的零行列式策略，证明其存在性并展示了其对市场合谋的影响。


<details>
  <summary>Details</summary>
Motivation: 探讨零行列式策略在重复古诺寡头博弈中的存在性及其对市场合谋的潜在负面影响。

Method: 理论证明零行列式策略的存在性，并通过数值模拟分析其对适应学习玩家的影响。

Result: 发现平均不可战胜的零行列式策略能促进单边合谋，但对双边适应学习玩家无效。

Conclusion: 零行列式策略在寡头市场中可能带来负面效应，需谨慎应用。

Abstract: An oligopoly is a market in which the price of a goods is controlled by a few
firms. Cournot introduced the simplest game-theoretic model of oligopoly, where
profit-maximizing behavior of each firm results in market failure. Furthermore,
when the Cournot oligopoly game is infinitely repeated, firms can tacitly
collude to monopolize the market. Such tacit collusion is realized by the same
mechanism as direct reciprocity in the repeated prisoner's dilemma game, where
mutual cooperation can be realized whereas defection is favorable for both
prisoners in one-shot game. Recently, in the repeated prisoner's dilemma game,
a class of strategies called zero-determinant strategies attracts much
attention in the context of direct reciprocity. Zero-determinant strategies are
autocratic strategies which unilaterally control payoffs of players. There were
many attempts to find zero-determinant strategies in other games and to extend
them so as to apply them to broader situations. In this paper, first, we show
that zero-determinant strategies exist even in the repeated Cournot oligopoly
game. Especially, we prove that an averagely unbeatable zero-determinant
strategy exists, which is guaranteed to obtain the average payoff of the
opponents. Second, we numerically show that the averagely unbeatable
zero-determinant strategy can be used to promote collusion when it is used
against an adaptively learning player, whereas it cannot promote collusion when
it is used against two adaptively learning players. Our findings elucidate some
negative impact of zero-determinant strategies in oligopoly market.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [517] [MoNetV2: Enhanced Motion Network for Freehand 3D Ultrasound Reconstruction](https://arxiv.org/abs/2506.15835)
*Mingyuan Luo,Xin Yang,Zhongnuo Yan,Yan Cao,Yuanji Zhang,Xindi Hu,Jin Wang,Haoxuan Ding,Wei Han,Litao Sun,Dong Ni*

Main category: eess.IV

TL;DR: MoNetV2通过融合图像与运动信息、多级一致性约束及多模态自监督策略，显著提升了自由手三维超声重建的精度和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 自由手三维超声重建在复杂运动轨迹下存在累积漂移和精度不足的问题，需改进。

Method: 提出MoNetV2，包括传感器辅助的多分支结构、在线多级一致性约束及多模态自监督策略。

Result: 在三个大型数据集上，MoNetV2在重建质量和泛化性能上优于现有方法。

Conclusion: MoNetV2有效解决了自由手三维超声重建的挑战，具有临床应用潜力。

Abstract: Three-dimensional (3D) ultrasound (US) aims to provide sonographers with the
spatial relationships of anatomical structures, playing a crucial role in
clinical diagnosis. Recently, deep-learning-based freehand 3D US has made
significant advancements. It reconstructs volumes by estimating transformations
between images without external tracking. However, image-only reconstruction
poses difficulties in reducing cumulative drift and further improving
reconstruction accuracy, particularly in scenarios involving complex motion
trajectories. In this context, we propose an enhanced motion network (MoNetV2)
to enhance the accuracy and generalizability of reconstruction under diverse
scanning velocities and tactics. First, we propose a sensor-based temporal and
multi-branch structure that fuses image and motion information from a velocity
perspective to improve image-only reconstruction accuracy. Second, we devise an
online multi-level consistency constraint that exploits the inherent
consistency of scans to handle various scanning velocities and tactics. This
constraint exploits both scan-level velocity consistency, path-level appearance
consistency, and patch-level motion consistency to supervise inter-frame
transformation estimation. Third, we distill an online multi-modal
self-supervised strategy that leverages the correlation between network
estimation and motion information to further reduce cumulative errors.
Extensive experiments clearly demonstrate that MoNetV2 surpasses existing
methods in both reconstruction quality and generalizability performance across
three large datasets.

</details>


### [518] [Cross-Modality Learning for Predicting IHC Biomarkers from H&E-Stained Whole-Slide Images](https://arxiv.org/abs/2506.15853)
*Amit Das,Naofumi Tomita,Kyle J. Syme,Weijie Ma,Paige O'Connor,Kristin N. Corbett,Bing Ren,Xiaoying Liu,Saeed Hassanpour*

Main category: eess.IV

TL;DR: HistoStainAlign是一种深度学习框架，直接从H&E全切片图像预测IHC染色模式，无需标注或组织配准，提高了工作效率。


<details>
  <summary>Details</summary>
Motivation: IHC染色成本高、耗时长且资源密集，而H&E染色是病理分析的基础。研究旨在通过计算模型预测IHC染色模式，减少对IHC的依赖。

Method: 提出HistoStainAlign框架，通过对比训练策略整合H&E和IHC嵌入，学习形态和分子特征的联合表示。

Result: 在胃肠道和肺组织WSIs上评估，对P53、PD-L1和Ki-67三种IHC染色的加权F1分数分别为0.735、0.830和0.723。

Conclusion: HistoStainAlign展示了作为预筛查工具的潜力，可优化IHC染色工作流程。

Abstract: Hematoxylin and Eosin (H&E) staining is a cornerstone of pathological
analysis, offering reliable visualization of cellular morphology and tissue
architecture for cancer diagnosis, subtyping, and grading. Immunohistochemistry
(IHC) staining provides molecular insights by detecting specific proteins
within tissues, enhancing diagnostic accuracy, and improving treatment
planning. However, IHC staining is costly, time-consuming, and
resource-intensive, requiring specialized expertise. To address these
limitations, this study proposes HistoStainAlign, a novel deep learning
framework that predicts IHC staining patterns directly from H&E whole-slide
images (WSIs) by learning joint representations of morphological and molecular
features. The framework integrates paired H&E and IHC embeddings through a
contrastive training strategy, capturing complementary features across staining
modalities without patch-level annotations or tissue registration. The model
was evaluated on gastrointestinal and lung tissue WSIs with three commonly used
IHC stains: P53, PD-L1, and Ki-67. HistoStainAlign achieved weighted F1 scores
of 0.735 [95% Confidence Interval (CI): 0.670-0.799], 0.830 [95% CI:
0.772-0.886], and 0.723 [95% CI: 0.607-0.836], respectively for these three IHC
stains. Embedding analyses demonstrated the robustness of the contrastive
alignment in capturing meaningful cross-stain relationships. Comparisons with a
baseline model further highlight the advantage of incorporating contrastive
learning for improved stain pattern prediction. This study demonstrates the
potential of computational approaches to serve as a pre-screening tool, helping
prioritize cases for IHC staining and improving workflow efficiency.

</details>


### [519] [InfiniPot-V: Memory-Constrained KV Cache Compression for Streaming Video Understanding](https://arxiv.org/abs/2506.15745)
*Minsoo Kim,Kyuhong Shim,Jungwook Choi,Simyung Chang*

Main category: eess.IV

TL;DR: InfiniPot-V是一种无需训练、与查询无关的框架，通过动态压缩KV缓存，实现固定内存限制下的流式视频理解。


<details>
  <summary>Details</summary>
Motivation: 解决多模态大语言模型在流式视频理解中KV缓存线性增长导致内存溢出的问题。

Method: 通过Temporal-axis Redundancy (TaR)和Value-Norm (VaN)动态压缩缓存，移除冗余并保留语义重要令牌。

Result: 在多个基准测试中，峰值GPU内存降低94%，保持实时生成，且精度不降。

Conclusion: InfiniPot-V为设备端流式视频助手提供了高效解决方案。

Abstract: Modern multimodal large language models (MLLMs) can reason over hour-long
video, yet their key-value (KV) cache grows linearly with time--quickly
exceeding the fixed memory of phones, AR glasses, and edge robots. Prior
compression schemes either assume the whole video and user query are available
offline or must first build the full cache, so memory still scales with stream
length. InfiniPot-V is the first training-free, query-agnostic framework that
enforces a hard, length-independent memory cap for streaming video
understanding. During video encoding it monitors the cache and, once a user-set
threshold is reached, runs a lightweight compression pass that (i) removes
temporally redundant tokens via Temporal-axis Redundancy (TaR) metric and (ii)
keeps semantically significant tokens via Value-Norm (VaN) ranking. Across four
open-source MLLMs and four long-video and two streaming-video benchmarks,
InfiniPot-V cuts peak GPU memory by up to 94%, sustains real-time generation,
and matches or surpasses full-cache accuracy--even in multi-turn dialogues. By
dissolving the KV cache bottleneck without retraining or query knowledge,
InfiniPot-V closes the gap for on-device streaming video assistants.

</details>


### [520] [CF-Seg: Counterfactuals meet Segmentation](https://arxiv.org/abs/2506.16213)
*Raghav Mehta,Fabio De Sousa Ribeiro,Tian Xia,Melanie Roschewitz,Ainkaran Santhirasekaram,Dominic C. Marshall,Ben Glocker*

Main category: eess.IV

TL;DR: 通过生成反事实图像模拟无疾病状态下的解剖结构，提升医学图像分割准确性。


<details>
  <summary>Details</summary>
Motivation: 疾病会改变健康组织的表现，增加分割难度，可能导致误诊。

Method: 生成反事实图像模拟无疾病状态，用于分割目标结构，无需修改分割模型。

Result: 在两个真实临床胸部X光数据集上，反事实图像显著改善了分割效果。

Conclusion: 反事实图像有助于提升解剖结构分割，支持临床决策。

Abstract: Segmenting anatomical structures in medical images plays an important role in
the quantitative assessment of various diseases. However, accurate segmentation
becomes significantly more challenging in the presence of disease. Disease
patterns can alter the appearance of surrounding healthy tissues, introduce
ambiguous boundaries, or even obscure critical anatomical structures. As such,
segmentation models trained on real-world datasets may struggle to provide good
anatomical segmentation, leading to potential misdiagnosis. In this paper, we
generate counterfactual (CF) images to simulate how the same anatomy would
appear in the absence of disease without altering the underlying structure. We
then use these CF images to segment structures of interest, without requiring
any changes to the underlying segmentation model. Our experiments on two
real-world clinical chest X-ray datasets show that the use of counterfactual
images improves anatomical segmentation, thereby aiding downstream clinical
decision-making.

</details>


### [521] [Pixel-wise Modulated Dice Loss for Medical Image Segmentation](https://arxiv.org/abs/2506.15744)
*Seyed Mohsen Hosseini*

Main category: eess.IV

TL;DR: 论文提出了一种改进的Dice损失函数（PM Dice损失），通过像素级调制项同时解决医学图像分割中的类别不平衡和难度不平衡问题，计算成本低且效果优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 医学图像分割任务中存在类别不平衡和难度不平衡问题，传统方法（如交叉熵损失或修改的Dice损失）计算成本高且效果有限。

Method: 提出一种简单的Dice损失修改方法（PM Dice损失），通过像素级调制项在保持Dice损失对类别不平衡处理优势的同时，解决难度不平衡问题。

Result: 在三个常用医学分割任务上的实验表明，PM Dice损失优于其他针对难度不平衡问题的方法。

Conclusion: PM Dice损失是一种高效且有效的解决方案，能够同时处理类别和难度不平衡问题。

Abstract: Class imbalance and the difficulty imbalance are the two types of data
imbalance that affect the performance of neural networks in medical
segmentation tasks. In class imbalance the loss is dominated by the majority
classes and in difficulty imbalance the loss is dominated by easy to classify
pixels. This leads to an ineffective training. Dice loss, which is based on a
geometrical metric, is very effective in addressing the class imbalance
compared to the cross entropy (CE) loss, which is adopted directly from
classification tasks. To address the difficulty imbalance, the common approach
is employing a re-weighted CE loss or a modified Dice loss to focus the
training on difficult to classify areas. The existing modification methods are
computationally costly and with limited success. In this study we propose a
simple modification to the Dice loss with minimal computational cost. With a
pixel level modulating term, we take advantage of the effectiveness of Dice
loss in handling the class imbalance to also handle the difficulty imbalance.
Results on three commonly used medical segmentation tasks show that the
proposed Pixel-wise Modulated Dice loss (PM Dice loss) outperforms other
methods, which are designed to tackle the difficulty imbalance problem.

</details>


### [522] [Diffusion-based Counterfactual Augmentation: Towards Robust and Interpretable Knee Osteoarthritis Grading](https://arxiv.org/abs/2506.15748)
*Zhe Wang,Yuhua Ru,Aladine Chetouani,Tina Shiang,Fang Chen,Fabian Bauer,Liping Zhang,Didier Hans,Rachid Jennane,William Ewing Palmer,Mohamed Jarraya,Yung Hsin Chen*

Main category: eess.IV

TL;DR: 论文提出了一种基于扩散模型的反事实增强框架（DCA），用于提升膝关节骨关节炎（KOA）自动分级的鲁棒性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 解决KOA自动分级中观察者间差异大和深度学习模型在关键决策边界附近鲁棒性不足的问题。

Method: 使用随机微分方程（SDE）在扩散模型的潜在空间中生成目标反事实示例，结合自校正学习策略优化分类器。

Result: 在OAI和MOST数据集上显著提高了分类准确性，且潜在空间拓扑与KOA临床进展知识一致。

Conclusion: DCA框架将模型不确定性转化为鲁棒训练信号，为开发更准确可靠的自动诊断系统提供了新途径。

Abstract: Automated grading of Knee Osteoarthritis (KOA) from radiographs is challenged
by significant inter-observer variability and the limited robustness of deep
learning models, particularly near critical decision boundaries. To address
these limitations, this paper proposes a novel framework, Diffusion-based
Counterfactual Augmentation (DCA), which enhances model robustness and
interpretability by generating targeted counterfactual examples. The method
navigates the latent space of a diffusion model using a Stochastic Differential
Equation (SDE), governed by balancing a classifier-informed boundary drive with
a manifold constraint. The resulting counterfactuals are then used within a
self-corrective learning strategy to improve the classifier by focusing on its
specific areas of uncertainty. Extensive experiments on the public
Osteoarthritis Initiative (OAI) and Multicenter Osteoarthritis Study (MOST)
datasets demonstrate that this approach significantly improves classification
accuracy across multiple model architectures. Furthermore, the method provides
interpretability by visualizing minimal pathological changes and revealing that
the learned latent space topology aligns with clinical knowledge of KOA
progression. The DCA framework effectively converts model uncertainty into a
robust training signal, offering a promising pathway to developing more
accurate and trustworthy automated diagnostic systems. Our code is available at
https://github.com/ZWang78/DCA.

</details>


### [523] [Fast Training-free Perceptual Image Compression](https://arxiv.org/abs/2506.16102)
*Ziran Zhu,Tongda Xu,Minye Huang,Dailan He,Xingtong Ge,Xinjie Zhang,Ling Li,Yan Wang*

Main category: eess.IV

TL;DR: 提出一种无需训练的算法，显著提升现有图像编解码器的感知质量，并针对不同解码时间预算优化实现。


<details>
  <summary>Details</summary>
Motivation: 现有无需训练的感知图像编解码器依赖扩散反转或样本通信，解码时间过长（1分钟至不可行）。

Method: 提出理论保证的算法，针对不同解码时间预算（≈0.1s、0.1-10s、≥10s）优化实现。

Result: 解码时间从1分钟降至0.1-10s，感知质量相当；适用于非可微分编解码器（如VTM）并提升现有感知编解码器（如MS-ILLM）。

Conclusion: 该方法在快速解码下显著提升感知质量，FID优于现有条件生成模型编解码器（如HiFiC和MS-ILLM）。

Abstract: Training-free perceptual image codec adopt pre-trained unconditional
generative model during decoding to avoid training new conditional generative
model. However, they heavily rely on diffusion inversion or sample
communication, which take 1 min to intractable amount of time to decode a
single image. In this paper, we propose a training-free algorithm that improves
the perceptual quality of any existing codec with theoretical guarantee. We
further propose different implementations for optimal perceptual quality when
decoding time budget is $\approx 0.1$s, $0.1-10$s and $\ge 10$s. Our approach:
1). improves the decoding time of training-free codec from 1 min to $0.1-10$s
with comparable perceptual quality. 2). can be applied to non-differentiable
codec such as VTM. 3). can be used to improve previous perceptual codecs, such
as MS-ILLM. 4). can easily achieve perception-distortion trade-off.
Empirically, we show that our approach successfully improves the perceptual
quality of ELIC, VTM and MS-ILLM with fast decoding. Our approach achieves
comparable FID to previous training-free codec with significantly less decoding
time. And our approach still outperforms previous conditional generative model
based codecs such as HiFiC and MS-ILLM in terms of FID. The source code is
provided in the supplementary material.

</details>


### [524] [Enhanced Dermatology Image Quality Assessment via Cross-Domain Training](https://arxiv.org/abs/2506.16116)
*Ignacio Hernández Montilla,Alfonso Medela,Paola Pasquali,Andy Aguilar,Taig Mac Carthy,Gerardo Fernández,Antonio Martorell,Enrique Onieva*

Main category: eess.IV

TL;DR: 论文提出跨域训练图像质量评估（IQA）模型，结合皮肤病学和非皮肤病学数据集，以解决远程皮肤病学中图像质量差的问题。


<details>
  <summary>Details</summary>
Motivation: 远程皮肤病学中图像质量差影响诊断效果，但现有皮肤病学IQA研究不足且未充分利用非皮肤病学IQA的最新进展。

Method: 创建新的皮肤病学IQA数据库（Legit.Health-DIQA-Artificial），结合跨域训练方法，利用更大规模的图像数据集。

Result: 跨域训练在多个领域表现最优，克服了皮肤病学IQA数据规模小的限制，提升了图像质量管理能力。

Conclusion: 跨域训练方法显著提升了远程皮肤病学中的图像质量评估效果，为未来研究提供了新方向。

Abstract: Teledermatology has become a widely accepted communication method in daily
clinical practice, enabling remote care while showing strong agreement with
in-person visits. Poor image quality remains an unsolved problem in
teledermatology and is a major concern to practitioners, as bad-quality images
reduce the usefulness of the remote consultation process. However, research on
Image Quality Assessment (IQA) in dermatology is sparse, and does not leverage
the latest advances in non-dermatology IQA, such as using larger image
databases with ratings from large groups of human observers. In this work, we
propose cross-domain training of IQA models, combining dermatology and
non-dermatology IQA datasets. For this purpose, we created a novel dermatology
IQA database, Legit.Health-DIQA-Artificial, using dermatology images from
several sources and having them annotated by a group of human observers. We
demonstrate that cross-domain training yields optimal performance across
domains and overcomes one of the biggest limitations in dermatology IQA, which
is the small scale of data, and leads to models trained on a larger pool of
image distortions, resulting in a better management of image quality in the
teledermatology process.

</details>


### [525] [From Coarse to Continuous: Progressive Refinement Implicit Neural Representation for Motion-Robust Anisotropic MRI Reconstruction](https://arxiv.org/abs/2506.16210)
*Zhenxuan Zhang,Lipei Zhang,Yanqi Cheng,Zi Wang,Fanwen Wang,Haosen Zhang,Yue Yang,Yinzhe Wu,Jiahao Huang,Angelica I Aviles-Rivero,Zhifan Gao,Guang Yang,Peter J. Lally*

Main category: eess.IV

TL;DR: 提出了一种渐进式细化隐式神经表示（PR-INR）框架，用于运动鲁棒MRI中的切片到体积重建，解决了局部细节丢失、全局结构混叠和体积各向异性问题。


<details>
  <summary>Details</summary>
Motivation: 运动鲁棒MRI中切片到体积重建面临局部细节丢失、全局结构混叠和体积各向异性的挑战，需要一种统一的方法来纠正运动伪影并恢复结构。

Method: PR-INR框架结合运动感知扩散模块生成粗略体积重建，隐式细节恢复模块进行残差细化，体素连续感知表示模块实现高精度细节恢复。

Result: 在多种运动条件、欠采样率和切片分辨率下，PR-INR在定量重建指标和视觉质量上均优于现有方法，并展示了跨领域的泛化能力。

Conclusion: PR-INR是一种高效且鲁棒的框架，适用于运动鲁棒MRI中的切片到体积重建，具有广泛的应用潜力。

Abstract: In motion-robust magnetic resonance imaging (MRI), slice-to-volume
reconstruction is critical for recovering anatomically consistent 3D brain
volumes from 2D slices, especially under accelerated acquisitions or patient
motion. However, this task remains challenging due to hierarchical structural
disruptions. It includes local detail loss from k-space undersampling, global
structural aliasing caused by motion, and volumetric anisotropy. Therefore, we
propose a progressive refinement implicit neural representation (PR-INR)
framework. Our PR-INR unifies motion correction, structural refinement, and
volumetric synthesis within a geometry-aware coordinate space. Specifically, a
motion-aware diffusion module is first employed to generate coarse volumetric
reconstructions that suppress motion artifacts and preserve global anatomical
structures. Then, we introduce an implicit detail restoration module that
performs residual refinement by aligning spatial coordinates with visual
features. It corrects local structures and enhances boundary precision.
Further, a voxel continuous-aware representation module represents the image as
a continuous function over 3D coordinates. It enables accurate inter-slice
completion and high-frequency detail recovery. We evaluate PR-INR on five
public MRI datasets under various motion conditions (3% and 5% displacement),
undersampling rates (4x and 8x) and slice resolutions (scale = 5). Experimental
results demonstrate that PR-INR outperforms state-of-the-art methods in both
quantitative reconstruction metrics and visual quality. It further shows
generalization and robustness across diverse unseen domains.

</details>


### [526] [AGE-US: automated gestational age estimation based on fetal ultrasound images](https://arxiv.org/abs/2506.16256)
*César Díaz-Parga,Marta Nuñez-Garcia,Maria J. Carreira,Gabriel Bernardino,Nicolás Vila-Blanco*

Main category: eess.IV

TL;DR: 提出了一种基于深度学习的自动化孕龄计算方法，通过新颖的分割架构和距离图克服数据限制，性能媲美现有模型且复杂度更低。


<details>
  <summary>Details</summary>
Motivation: 传统孕龄估计方法（如末次月经）在某些情况下难以获取，而超声方法依赖人工测量且存在变异性。

Method: 采用可解释的深度学习模型，结合新颖的分割架构和距离图，减少对标注数据的依赖。

Result: 模型性能与现有最佳模型相当，复杂度更低，尤其适用于资源有限和标注数据稀缺的环境。

Conclusion: 该方法为孕龄估计提供了高效且可靠的解决方案，距离图特别适用于股骨端点估计。

Abstract: Being born small carries significant health risks, including increased
neonatal mortality and a higher likelihood of future cardiac diseases. Accurate
estimation of gestational age is critical for monitoring fetal growth, but
traditional methods, such as estimation based on the last menstrual period, are
in some situations difficult to obtain. While ultrasound-based approaches offer
greater reliability, they rely on manual measurements that introduce
variability. This study presents an interpretable deep learning-based method
for automated gestational age calculation, leveraging a novel segmentation
architecture and distance maps to overcome dataset limitations and the scarcity
of segmentation masks. Our approach achieves performance comparable to
state-of-the-art models while reducing complexity, making it particularly
suitable for resource-constrained settings and with limited annotated data.
Furthermore, our results demonstrate that the use of distance maps is
particularly suitable for estimating femur endpoints.

</details>


### [527] [VesselSDF: Distance Field Priors for Vascular Network Reconstruction](https://arxiv.org/abs/2506.16556)
*Salvatore Esposito,Daniel Rebain,Arno Onken,Changjian Li,Oisin Mac Aodha*

Main category: eess.IV

TL;DR: VesselSDF提出了一种基于符号距离场（SDF）的新方法，用于从稀疏CT扫描切片中准确分割血管网络，解决了现有深度学习方法在结构连续性和几何保真度上的不足。


<details>
  <summary>Details</summary>
Motivation: 血管网络的准确分割在医学成像中具有挑战性，尤其是由于血管的细长、分支特性以及扫描平面的稀疏性。现有基于二值体素分类的深度学习方法难以保持结构连续性和几何保真度。

Method: VesselSDF将血管分割问题重新定义为连续的SDF回归问题，通过自适应高斯正则化器消除SDF的常见伪影，同时保留血管表面的精确几何形状。

Result: 实验结果表明，VesselSDF显著优于现有方法，能够更好地保持血管的几何形状和连通性。

Conclusion: VesselSDF为临床环境中的血管分析提供了更可靠的工具。

Abstract: Accurate segmentation of vascular networks from sparse CT scan slices remains
a significant challenge in medical imaging, particularly due to the thin,
branching nature of vessels and the inherent sparsity between imaging planes.
Existing deep learning approaches, based on binary voxel classification, often
struggle with structural continuity and geometric fidelity. To address this
challenge, we present VesselSDF, a novel framework that leverages signed
distance fields (SDFs) for robust vessel reconstruction. Our method
reformulates vessel segmentation as a continuous SDF regression problem, where
each point in the volume is represented by its signed distance to the nearest
vessel surface. This continuous representation inherently captures the smooth,
tubular geometry of blood vessels and their branching patterns. We obtain
accurate vessel reconstructions while eliminating common SDF artifacts such as
floating segments, thanks to our adaptive Gaussian regularizer which ensures
smoothness in regions far from vessel surfaces while producing precise geometry
near the surface boundaries. Our experimental results demonstrate that
VesselSDF significantly outperforms existing methods and preserves vessel
geometry and connectivity, enabling more reliable vascular analysis in clinical
settings.

</details>


### [528] [DiffO: Single-step Diffusion for Image Compression at Ultra-Low Bitrates](https://arxiv.org/abs/2506.16572)
*Chanung Park,Joo Chan Lee,Jong Hwan Ko*

Main category: eess.IV

TL;DR: DiffO是一种单步扩散模型，用于图像压缩，提供高感知质量和快速解码，适用于极低比特率场景。


<details>
  <summary>Details</summary>
Motivation: 现有图像压缩方法在极低比特率下质量下降严重，而基于扩散的模型虽有所改进，但仍存在感知质量有限和解码延迟高的问题。

Method: DiffO结合了VQ残差训练和速率自适应噪声调制，分别捕捉全局几何和高频细节，并动态调整去噪强度以匹配目标比特率。

Result: 实验表明，DiffO在压缩性能上超越现有方法，解码速度提升约50倍。

Conclusion: DiffO显著提升了生成编解码器的实用性，适用于极低比特率场景。

Abstract: Although image compression is fundamental to visual data processing and has
inspired numerous standard and learned codecs, these methods still suffer
severe quality degradation at extremely low bits per pixel. While recent
diffusion based models provided enhanced generative performance at low
bitrates, they still yields limited perceptual quality and prohibitive decoding
latency due to multiple denoising steps. In this paper, we propose the first
single step diffusion model for image compression (DiffO) that delivers high
perceptual quality and fast decoding at ultra low bitrates. DiffO achieves
these goals by coupling two key innovations: (i) VQ Residual training, which
factorizes a structural base code and a learned residual in latent space,
capturing both global geometry and high frequency details; and (ii) rate
adaptive noise modulation, which tunes denoising strength on the fly to match
the desired bitrate. Extensive experiments show that DiffO surpasses state of
the art compression performance while improving decoding speed by about 50x
compared to prior diffusion-based methods, greatly improving the practicality
of generative codecs. The code will be available at
https://github.com/Freemasti/DiffO.

</details>


### [529] [Hybrid Attention Network for Accurate Breast Tumor Segmentation in Ultrasound Images](https://arxiv.org/abs/2506.16592)
*Muhammad Azeem Aslam,Asim Naveed,Nisar Ahmed*

Main category: eess.IV

TL;DR: 提出了一种基于混合注意力的网络，用于乳腺癌超声图像中的病灶分割，结合了预训练的DenseNet121和多分支注意力增强解码器，显著提升了分割性能。


<details>
  <summary>Details</summary>
Motivation: 乳腺癌超声图像分割面临噪声、病灶尺度变化和模糊边界等挑战，需要更精确的自动化方法。

Method: 采用预训练的DenseNet121作为编码器，结合多分支注意力增强解码器，引入全局空间注意力、位置编码和缩放点积注意力，并嵌入空间特征增强块优化特征。

Result: 在公开数据集上表现优于现有方法，提升了分割准确性和鲁棒性。

Conclusion: 该方法有望辅助放射科医生实现早期和准确的乳腺癌诊断。

Abstract: Breast ultrasound imaging is a valuable tool for early breast cancer
detection, but automated tumor segmentation is challenging due to inherent
noise, variations in scale of lesions, and fuzzy boundaries. To address these
challenges, we propose a novel hybrid attention-based network for lesion
segmentation. Our proposed architecture integrates a pre-trained DenseNet121 in
the encoder part for robust feature extraction with a multi-branch
attention-enhanced decoder tailored for breast ultrasound images. The
bottleneck incorporates Global Spatial Attention (GSA), Position Encoding (PE),
and Scaled Dot-Product Attention (SDPA) to learn global context, spatial
relationships, and relative positional features. The Spatial Feature
Enhancement Block (SFEB) is embedded at skip connections to refine and enhance
spatial features, enabling the network to focus more effectively on tumor
regions. A hybrid loss function combining Binary Cross-Entropy (BCE) and
Jaccard Index loss optimizes both pixel-level accuracy and region-level overlap
metrics, enhancing robustness to class imbalance and irregular tumor shapes.
Experiments on public datasets demonstrate that our method outperforms existing
approaches, highlighting its potential to assist radiologists in early and
accurate breast cancer diagnosis.

</details>


### [530] [Overfitting in Histopathology Model Training: The Need for Customized Architectures](https://arxiv.org/abs/2506.16631)
*Saghir Alfasly,Ghazal Alabtah,H. R. Tizhoosh*

Main category: eess.IV

TL;DR: 研究发现，在组织病理学图像分析中，直接采用自然图像分析的大规模模型会导致过拟合和性能不佳，需定制化架构。


<details>
  <summary>Details</summary>
Motivation: 解决深度学习模型在组织病理学图像分析中的过拟合问题，并探索更适合的模型架构。

Method: 通过实验比较不同架构（如ResNet和ViT），验证模型容量增加不一定提升性能，并提出领域特定的简化架构。

Result: 在Oesophageal Adenocarcinomas数据集上，领域特定架构表现更优且减少过拟合。

Conclusion: 组织病理学图像分析需定制化模型架构，尤其在数据有限时，简单架构可能更有效。

Abstract: This study investigates the critical problem of overfitting in deep learning
models applied to histopathology image analysis. We show that simply adopting
and fine-tuning large-scale models designed for natural image analysis often
leads to suboptimal performance and significant overfitting when applied to
histopathology tasks. Through extensive experiments with various model
architectures, including ResNet variants and Vision Transformers (ViT), we show
that increasing model capacity does not necessarily improve performance on
histopathology datasets. Our findings emphasize the need for customized
architectures specifically designed for histopathology image analysis,
particularly when working with limited datasets. Using Oesophageal
Adenocarcinomas public dataset, we demonstrate that simpler, domain-specific
architectures can achieve comparable or better performance while minimizing
overfitting.

</details>


### [531] [A Prior-Guided Joint Diffusion Model in Projection Domain for PET Tracer Conversion](https://arxiv.org/abs/2506.16733)
*Fang Chen,Weifeng Zhang,Xingyu Ai,BingXuan Li,An Li,Qiegen Liu*

Main category: eess.IV

TL;DR: 该研究提出了一种先验引导的联合扩散模型（PJDM），用于在投影域中将18F-FDG PET图像转换为18F-DOPA PET图像，以提高图像质量和合成效果。


<details>
  <summary>Details</summary>
Motivation: 18F-DOPA在神经内分泌肿瘤和神经系统疾病中具有更高的特异性，但其复杂的合成和临床应用限制阻碍了其广泛使用。通过直接在投影域建模，可以减少图像重建过程中引入的误差。

Method: 研究训练了一个粗估计模型和一个先验细化模型，并使用高阶混合采样器生成初始合成18F-DOPA PET正弦图，通过迭代细化过程提升结果。

Result: 实验结果表明，PJDM显著提高了正弦图质量和合成效果。

Conclusion: PJDM为18F-DOPA PET图像的合成提供了一种有效方法，克服了18F-DOPA的实际应用限制。

Abstract: Positron emission tomography (PET) is widely used to assess metabolic
activity, but its application is limited by the availability of radiotracers.
18F-labeled fluorodeoxyglucose (18F-FDG) is the most commonly used tracer but
shows limited effectiveness for certain tumors. In contrast,
6-18F-fluoro-3,4-dihydroxy-L-phenylalanine (18F-DOPA) offers higher specificity
for neuroendocrine tumors and neurological disorders. However, its complex
synthesis and limitations in transportation and clinical use hinder widespread
adoption. During PET imaging, the sinogram represents a form of raw data
acquired by the scanner. Therefore, modeling in projection domain enables more
direct utilization of the original information, potentially reducing the
accumulation of errors introduced during the image reconstruction process.
Inspired by these factors, this study proposes a prior-guided joint diffusion
model (PJDM) for transforming 18F-FDG PET images into 18F-DOPA PET images in
projection domain. Specifically, a coarse estimation model and a prior
refinement model are trained independently. During inference, an initial
synthetic 18F-DOPA PET sinogram is generated using a higher-order hybrid
sampler. This sinogram is then degraded and serves as an additional condition
to guide the iterative refinement process using learned prior. Experimental
results demonstrated that PJDM effectively improved both sinogram quality and
synthetic outcomes. The code is available at: https://github.com/yqx7150/PJDM.

</details>


### [532] [Temperature calibration of surface emissivities with an improved thermal image enhancement network](https://arxiv.org/abs/2506.16803)
*Ning Chu,Siya Zheng,Shanqing Zhang,Li Li,Caifang Cai,Ali Mohammad-Djafari,Feng Zhao,Yuanbo Song*

Main category: eess.IV

TL;DR: 提出了一种物理引导的神经网络框架，通过对称跳跃CNN和发射率感知注意力模块联合优化温度校正和图像增强，解决了红外热成像中材料发射率变化导致的温度精度问题。


<details>
  <summary>Details</summary>
Motivation: 红外热成像因材料发射率变化导致温度精度问题，现有方法常忽略辐射校准和图像退化的联合优化。

Method: 采用对称跳跃CNN架构和发射率感知注意力模块，通过双约束损失函数（均值-方差对齐和KL散度直方图匹配）动态融合热辐射特征和空间背景。

Result: 在工业鼓风机系统验证中，模型有效抑制发射率伪影并恢复结构细节，实现了不同工业条件下的准确校准。

Conclusion: 该方法通过联合优化温度校正和图像增强，显著提升了红外热成像的温度精度和图像质量。

Abstract: Infrared thermography faces persistent challenges in temperature accuracy due
to material emissivity variations, where existing methods often neglect the
joint optimization of radiometric calibration and image degradation. This study
introduces a physically guided neural framework that unifies temperature
correction and image enhancement through a symmetric skip-CNN architecture and
an emissivity-aware attention module. The pre-processing stage segments the
ROIs of the image and and initially corrected the firing rate. A novel
dual-constrained loss function strengthens the statistical consistency between
the target and reference regions through mean-variance alignment and histogram
matching based on Kullback-Leibler dispersion. The method works by dynamically
fusing thermal radiation features and spatial context, and the model suppresses
emissivity artifacts while recovering structural details. After validating the
industrial blower system under different conditions, the improved network
realizes the dynamic fusion of thermal radiation characteristics and spatial
background, with accurate calibration results in various industrial conditions.

</details>


### [533] [PET Tracer Separation Using Conditional Diffusion Transformer with Multi-latent Space Learning](https://arxiv.org/abs/2506.16934)
*Bin Huang,Feihong Xu,Xinchong Shi,Shan Huang,Binxuan Li,Fei Li,Qiegen Liu*

Main category: eess.IV

TL;DR: 提出了一种多潜在空间引导的纹理条件扩散变换模型（MS-CDT），用于PET成像中的示踪剂分离，首次结合纹理条件和多潜在空间技术。


<details>
  <summary>Details</summary>
Motivation: 多示踪剂PET成像能提供更全面的生理和病理信息，但由于不同示踪剂的光子对能量相同，信号难以区分。

Method: 结合扩散和变换器架构，引入纹理掩码作为条件输入，利用多潜在空间先验捕捉多层次特征。

Result: 在脑部和胸部3D PET数据集上，MS-CDT在图像质量和临床信息保留方面表现优异。

Conclusion: MS-CDT通过纹理条件和多潜在空间技术，实现了更准确和鲁棒的示踪剂分离。

Abstract: In clinical practice, single-radiotracer positron emission tomography (PET)
is commonly used for imaging. Although multi-tracer PET imaging can provide
supplementary information of radiotracers that are sensitive to physiological
function changes, enabling a more comprehensive characterization of
physiological and pathological states, the gamma-photon pairs generated by
positron annihilation reactions of different tracers in PET imaging have the
same energy, making it difficult to distinguish the tracer signals. In this
study, a multi-latent space guided texture conditional diffusion transformer
model (MS-CDT) is proposed for PET tracer separation. To the best of our
knowledge, this is the first attempt to use texture condition and multi-latent
space for tracer separation in PET imaging. The proposed model integrates
diffusion and transformer architectures into a unified optimization framework,
with the novel addition of texture masks as conditional inputs to enhance image
details. By leveraging multi-latent space prior derived from different tracers,
the model captures multi-level feature representations, aiming to balance
computational efficiency and detail preservation. The texture masks, serving as
conditional guidance, help the model focus on salient structural patterns,
thereby improving the extraction and utilization of fine-grained image
textures. When combined with the diffusion transformer backbone, this
conditioning mechanism contributes to more accurate and robust tracer
separation. To evaluate its effectiveness, the proposed MS-CDT is compared with
several advanced methods on two types of 3D PET datasets: brain and chest
scans. Experimental results indicate that MS-CDT achieved competitive
performance in terms of image quality and preservation of clinically relevant
information. Code is available at: https://github.com/yqx7150/MS-CDT.

</details>


### [534] [Robust Training with Data Augmentation for Medical Imaging Classification](https://arxiv.org/abs/2506.17133)
*Josué Martínez-Martínez,Olivia Brown,Mostafa Karami,Sheida Nabavi*

Main category: eess.IV

TL;DR: 提出了一种名为RTDA的鲁棒训练算法，用于增强医学图像分类模型对抗对抗性攻击和分布偏移的能力。


<details>
  <summary>Details</summary>
Motivation: 深度学习模型在医学图像诊断中易受对抗性攻击和分布偏移影响，影响诊断可靠性。

Method: 采用数据增强的鲁棒训练算法（RTDA），并与六种基线技术（包括对抗训练和数据增强）进行对比。

Result: RTDA在对抗性攻击和分布偏移下表现出更高的鲁棒性和泛化性能，同时保持高准确率。

Conclusion: RTDA是一种有效的解决方案，可提升医学图像分类模型的鲁棒性和可靠性。

Abstract: Deep neural networks are increasingly being used to detect and diagnose
medical conditions using medical imaging. Despite their utility, these models
are highly vulnerable to adversarial attacks and distribution shifts, which can
affect diagnostic reliability and undermine trust among healthcare
professionals. In this study, we propose a robust training algorithm with data
augmentation (RTDA) to mitigate these vulnerabilities in medical image
classification. We benchmark classifier robustness against adversarial
perturbations and natural variations of RTDA and six competing baseline
techniques, including adversarial training and data augmentation approaches in
isolation and combination, using experimental data sets with three different
imaging technologies (mammograms, X-rays, and ultrasound). We demonstrate that
RTDA achieves superior robustness against adversarial attacks and improved
generalization performance in the presence of distribution shift in each image
classification task while maintaining high clean accuracy.

</details>


### [535] [MeDi: Metadata-Guided Diffusion Models for Mitigating Biases in Tumor Classification](https://arxiv.org/abs/2506.17140)
*David Jacob Drexlin,Jonas Dippel,Julius Hense,Niklas Prenißl,Grégoire Montavon,Frederick Klauschen,Klaus-Robert Müller*

Main category: eess.IV

TL;DR: 提出了一种基于元数据引导的生成扩散模型（MeDi），用于增强代表性不足的子群体数据，减少下游模型的偏见。


<details>
  <summary>Details</summary>
Motivation: 深度学习模型在组织学预测任务中表现优异，但对不同条件（如染色、扫描仪、医院等）的鲁棒性不足，容易产生偏见。

Method: 提出MeDi框架，通过生成合成数据平衡训练数据，减少偏见。

Result: 实验表明MeDi能生成高质量图像，提升下游分类器性能。

Conclusion: MeDi为利用生成模型减少数据偏见提供了概念验证。

Abstract: Deep learning models have made significant advances in histological
prediction tasks in recent years. However, for adaptation in clinical practice,
their lack of robustness to varying conditions such as staining, scanner,
hospital, and demographics is still a limiting factor: if trained on
overrepresented subpopulations, models regularly struggle with less frequent
patterns, leading to shortcut learning and biased predictions. Large-scale
foundation models have not fully eliminated this issue. Therefore, we propose a
novel approach explicitly modeling such metadata into a Metadata-guided
generative Diffusion model framework (MeDi). MeDi allows for a targeted
augmentation of underrepresented subpopulations with synthetic data, which
balances limited training data and mitigates biases in downstream models. We
experimentally show that MeDi generates high-quality histopathology images for
unseen subpopulations in TCGA, boosts the overall fidelity of the generated
images, and enables improvements in performance for downstream classifiers on
datasets with subpopulation shifts. Our work is a proof-of-concept towards
better mitigating data biases with generative models.

</details>


### [536] [Proportional Sensitivity in Generative Adversarial Network (GAN)-Augmented Brain Tumor Classification Using Convolutional Neural Network](https://arxiv.org/abs/2506.17165)
*Mahin Montasir Afif,Abdullah Al Noman,K. M. Tahsin Kabir,Md. Mortuza Ahmmed,Md. Mostafizur Rahman,Mufti Mahmud,Md. Ashraful Babu*

Main category: eess.IV

TL;DR: 研究探讨了GAN生成的脑肿瘤MRI图像与真实图像的不同比例对CNN分类性能的影响，发现少量GAN数据能显著提升性能，但过多会降低效果。


<details>
  <summary>Details</summary>
Motivation: 解决医学影像数据稀缺问题，探索GAN生成数据在增强数据集中的作用。

Method: 使用DCGAN生成合成图像，与真实图像按不同比例混合训练CNN，并在真实测试集上评估性能。

Result: 少量GAN数据（如100张）能显著提升模型性能（准确率95.2%），但过多会降低效果。

Conclusion: GAN可用于增强有限数据集，但需控制合成数据比例以避免性能下降。

Abstract: Generative Adversarial Networks (GAN) have shown potential in expanding
limited medical imaging datasets. This study explores how different ratios of
GAN-generated and real brain tumor MRI images impact the performance of a CNN
in classifying healthy vs. tumorous scans. A DCGAN was used to create synthetic
images which were mixed with real ones at various ratios to train a custom CNN.
The CNN was then evaluated on a separate real-world test set. Our results
indicate that the model maintains high sensitivity and precision in tumor
classification, even when trained predominantly on synthetic data. When only a
small portion of GAN data was added, such as 900 real images and 100 GAN
images, the model achieved excellent performance, with test accuracy reaching
95.2%, and precision, recall, and F1-score all exceeding 95%. However, as the
proportion of GAN images increased further, performance gradually declined.
This study suggests that while GANs are useful for augmenting limited datasets
especially when real data is scarce, too much synthetic data can introduce
artifacts that affect the model's ability to generalize to real world cases.

</details>


### [537] [Implicit neural representations for accurate estimation of the standard model of white matter](https://arxiv.org/abs/2506.15762)
*Tom Hendriks,Gerrit Arends,Edwin Versteeg,Anna Vilanova,Maxime Chamberland,Chantal M. W. Tax*

Main category: eess.IV

TL;DR: 该论文提出了一种基于隐式神经表示（INRs）的新框架，用于提高扩散磁共振成像（dMRI）中白质标准模型参数的估计精度，尤其在低信噪比条件下表现优异。


<details>
  <summary>Details</summary>
Motivation: 白质标准模型（SM）在估计dMRI信号时面临高维参数和噪声干扰的挑战，需要复杂的采集协议。现有方法难以准确估计参数。

Method: 采用隐式神经表示（INRs），通过输入坐标的正弦编码实现空间正则化，无需标记训练数据，支持快速推断和噪声鲁棒性。

Result: INR方法在合成和体内数据上均表现出更高的参数估计精度，支持空间上采样和连续表示，优于多项式、神经网络和非线性最小二乘法。

Conclusion: INRs为dMRI数据分析提供了高效、灵活的工具，适用于多种模型和噪声条件，具有广泛的应用潜力。

Abstract: Diffusion magnetic resonance imaging (dMRI) enables non-invasive
investigation of tissue microstructure. The Standard Model (SM) of white matter
aims to disentangle dMRI signal contributions from intra- and extra-axonal
water compartments. However, due to the model its high-dimensional nature,
extensive acquisition protocols with multiple b-values and diffusion tensor
shapes are typically required to mitigate parameter degeneracies. Even then,
accurate estimation remains challenging due to noise. This work introduces a
novel estimation framework based on implicit neural representations (INRs),
which incorporate spatial regularization through the sinusoidal encoding of the
input coordinates. The INR method is evaluated on both synthetic and in vivo
datasets and compared to parameter estimates using cubic polynomials,
supervised neural networks, and nonlinear least squares. Results demonstrate
superior accuracy of the INR method in estimating SM parameters, particularly
in low signal-to-noise conditions. Additionally, spatial upsampling of the INR
can represent the underlying dataset anatomically plausibly in a continuous
way, which is unattainable with linear or cubic interpolation. The INR is fully
unsupervised, eliminating the need for labeled training data. It achieves fast
inference ($\sim$6 minutes), is robust to both Gaussian and Rician noise,
supports joint estimation of SM kernel parameters and the fiber orientation
distribution function with spherical harmonics orders up to at least 8 and
non-negativity constraints, and accommodates spatially varying acquisition
protocols caused by magnetic gradient non-uniformities. The combination of
these properties along with the possibility to easily adapt the framework to
other dMRI models, positions INRs as a potentially important tool for analyzing
and interpreting diffusion MRI data.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [538] [Multi-use LLM Watermarking and the False Detection Problem](https://arxiv.org/abs/2506.15975)
*Zihao Fu,Chris Russell*

Main category: cs.CR

TL;DR: 论文提出双水印技术，结合检测和用户识别水印，减少误检并保持高检测精度。


<details>
  <summary>Details</summary>
Motivation: 解决传统水印技术在用户容量增长时误检率上升的问题。

Method: 通过理论分析提出双水印技术，联合编码检测和识别水印。

Result: 实验验证了理论分析，双水印显著降低误检率并保持高检测精度。

Conclusion: 双水印技术有效解决了传统水印的误检问题，具有实际应用价值。

Abstract: Digital watermarking is a promising solution for mitigating some of the risks
arising from the misuse of automatically generated text. These approaches
either embed non-specific watermarks to allow for the detection of any text
generated by a particular sampler, or embed specific keys that allow the
identification of the LLM user. However, simultaneously using the same
embedding for both detection and user identification leads to a false detection
problem, whereby, as user capacity grows, unwatermarked text is increasingly
likely to be falsely detected as watermarked. Through theoretical analysis, we
identify the underlying causes of this phenomenon. Building on these insights,
we propose Dual Watermarking which jointly encodes detection and identification
watermarks into generated text, significantly reducing false positives while
maintaining high detection accuracy. Our experimental results validate our
theoretical findings and demonstrate the effectiveness of our approach.

</details>


### [539] [Probe before You Talk: Towards Black-box Defense against Backdoor Unalignment for Large Language Models](https://arxiv.org/abs/2506.16447)
*Biao Yi,Tiansheng Huang,Sishuo Chen,Tong Li,Zheli Liu,Zhixuan Chu,Yiming Li*

Main category: cs.CR

TL;DR: BEAT是一种黑盒防御方法，通过检测推理过程中的触发样本来解除大型语言模型（LLM）的后门攻击。


<details>
  <summary>Details</summary>
Motivation: 后门攻击通过隐藏触发器破坏LLM的安全性，且攻击目标具有样本依赖性，威胁LLMaaS的实际应用。

Method: BEAT利用探针串联效应，通过测量输入与探针串联前后输出分布的变化来识别触发样本。

Result: 实验验证了BEAT在多种后门攻击和LLM（包括GPT-3.5-turbo）上的有效性和高效性。

Conclusion: BEAT不仅能防御后门攻击，还能有效应对越狱攻击，因其可视为'自然后门'。

Abstract: Backdoor unalignment attacks against Large Language Models (LLMs) enable the
stealthy compromise of safety alignment using a hidden trigger while evading
normal safety auditing. These attacks pose significant threats to the
applications of LLMs in the real-world Large Language Model as a Service
(LLMaaS) setting, where the deployed model is a fully black-box system that can
only interact through text. Furthermore, the sample-dependent nature of the
attack target exacerbates the threat. Instead of outputting a fixed label, the
backdoored LLM follows the semantics of any malicious command with the hidden
trigger, significantly expanding the target space. In this paper, we introduce
BEAT, a black-box defense that detects triggered samples during inference to
deactivate the backdoor. It is motivated by an intriguing observation (dubbed
the probe concatenate effect), where concatenated triggered samples
significantly reduce the refusal rate of the backdoored LLM towards a malicious
probe, while non-triggered samples have little effect. Specifically, BEAT
identifies whether an input is triggered by measuring the degree of distortion
in the output distribution of the probe before and after concatenation with the
input. Our method addresses the challenges of sample-dependent targets from an
opposite perspective. It captures the impact of the trigger on the refusal
signal (which is sample-independent) instead of sample-specific successful
attack behaviors. It overcomes black-box access limitations by using multiple
sampling to approximate the output distribution. Extensive experiments are
conducted on various backdoor attacks and LLMs (including the closed-source
GPT-3.5-turbo), verifying the effectiveness and efficiency of our defense.
Besides, we also preliminarily verify that BEAT can effectively defend against
popular jailbreak attacks, as they can be regarded as 'natural backdoors'.

</details>


### [540] [PRISON: Unmasking the Criminal Potential of Large Language Models](https://arxiv.org/abs/2506.16150)
*Xinyi Wu,Geng Hong,Pei Chen,Yueyue Chen,Xudong Pan,Min Yang*

Main category: cs.CR

TL;DR: 论文提出PRISON框架，评估大语言模型在五种犯罪维度上的潜力，发现其易产生犯罪倾向且反犯罪能力不足，呼吁加强安全机制。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型的发展，其在复杂社会背景下的不当行为引发担忧，现有研究缺乏对其犯罪能力的系统性评估。

Method: 提出PRISON框架，通过角色扮演评估模型在五种犯罪维度上的潜力及反犯罪能力。

Result: 先进模型常表现出犯罪倾向，反犯罪识别准确率仅41%，显示其行为与识别能力不匹配。

Conclusion: 需加强对抗鲁棒性、行为对齐和安全机制，以确保大语言模型的广泛部署安全。

Abstract: As large language models (LLMs) advance, concerns about their misconduct in
complex social contexts intensify. Existing research overlooked the systematic
understanding and assessment of their criminal capability in realistic
interactions. We propose a unified framework PRISON, to quantify LLMs' criminal
potential across five dimensions: False Statements, Frame-Up, Psychological
Manipulation, Emotional Disguise, and Moral Disengagement. Using structured
crime scenarios adapted from classic films, we evaluate both criminal potential
and anti-crime ability of LLMs via role-play. Results show that
state-of-the-art LLMs frequently exhibit emergent criminal tendencies, such as
proposing misleading statements or evasion tactics, even without explicit
instructions. Moreover, when placed in a detective role, models recognize
deceptive behavior with only 41% accuracy on average, revealing a striking
mismatch between conducting and detecting criminal behavior. These findings
underscore the urgent need for adversarial robustness, behavioral alignment,
and safety mechanisms before broader LLM deployment.

</details>


### [541] [Towards Effective Complementary Security Analysis using Large Language Models](https://arxiv.org/abs/2506.16899)
*Jonas Wagner,Simon Müller,Christian Näther,Jan-Philipp Steghöfer,Andreas Both*

Main category: cs.CR

TL;DR: 使用大型语言模型（LLMs）改进静态应用安全测试（SAST）工具的假阳性（FP）检测，结合高级提示技术显著提升效果。


<details>
  <summary>Details</summary>
Motivation: SAST工具生成的报告存在大量假阳性，降低了安全分析的效率，需要自动化方法改进评估。

Method: 利用LLMs（如Chain-of-Thought和Self-Consistency）分析SAST报告，测试数据集来自OWASP Benchmark和真实项目。

Result: LLMs在OWASP Benchmark中检测到62.5%的FP，结合多模型可达78.9%；在真实项目中检测到33.85%的FP，结合模型可达38.46%。

Conclusion: LLMs能有效补充传统SAST工具，提高自动化水平并减少假警报处理资源。

Abstract: A key challenge in security analysis is the manual evaluation of potential
security weaknesses generated by static application security testing (SAST)
tools. Numerous false positives (FPs) in these reports reduce the effectiveness
of security analysis. We propose using Large Language Models (LLMs) to improve
the assessment of SAST findings. We investigate the ability of LLMs to reduce
FPs while trying to maintain a perfect true positive rate, using datasets
extracted from the OWASP Benchmark (v1.2) and a real-world software project.
Our results indicate that advanced prompting techniques, such as
Chain-of-Thought and Self-Consistency, substantially improve FP detection.
Notably, some LLMs identified approximately 62.5% of FPs in the OWASP Benchmark
dataset without missing genuine weaknesses. Combining detections from different
LLMs would increase this FP detection to approximately 78.9%. Additionally, we
demonstrate our approach's generalizability using a real-world dataset covering
five SAST tools, three programming languages, and infrastructure files. The
best LLM detected 33.85% of all FPs without missing genuine weaknesses, while
combining detections from different LLMs would increase this detection to
38.46%. Our findings highlight the potential of LLMs to complement traditional
SAST tools, enhancing automation and reducing resources spent addressing false
alarms.

</details>


### [542] [Malware Classification Leveraging NLP & Machine Learning for Enhanced Accuracy](https://arxiv.org/abs/2506.16224)
*Bishwajit Prasad Gond,Rajneekant,Pushkar Kishore,Durga Prasad Mohapatra*

Main category: cs.CR

TL;DR: 该论文研究了基于NLP的n-gram分析和机器学习技术在恶意软件分类中的应用，通过提取和分析文本特征，显著提高了分类准确性。


<details>
  <summary>Details</summary>
Motivation: 探索如何利用NLP技术从恶意软件样本中提取文本特征，以改进恶意软件分类的准确性和粒度。

Method: 采用n-gram分析提取特征，结合混合特征选择技术降低维度，并使用多种机器学习算法进行分类。

Result: 在真实恶意软件样本上测试，分类准确率达到99.02%，特征维度降至原始特征的1.6%。

Conclusion: 基于NLP的n-gram分析结合混合特征选择技术，能显著提升恶意软件分类的准确性和效率。

Abstract: This paper investigates the application of natural language processing
(NLP)-based n-gram analysis and machine learning techniques to enhance malware
classification. We explore how NLP can be used to extract and analyze textual
features from malware samples through n-grams, contiguous string or API call
sequences. This approach effectively captures distinctive linguistic patterns
among malware and benign families, enabling finer-grained classification. We
delve into n-gram size selection, feature representation, and classification
algorithms. While evaluating our proposed method on real-world malware samples,
we observe significantly improved accuracy compared to the traditional methods.
By implementing our n-gram approach, we achieved an accuracy of 99.02% across
various machine learning algorithms by using hybrid feature selection technique
to address high dimensionality. Hybrid feature selection technique reduces the
feature set to only 1.6% of the original features.

</details>


### [543] [The Hitchhiker's Guide to Efficient, End-to-End, and Tight DP Auditing](https://arxiv.org/abs/2506.16666)
*Meenatchi Sundaram Muthu Selva Annamalai,Borja Balle,Jamie Hayes,Georgios Kaissis,Emiliano De Cristofaro*

Main category: cs.CR

TL;DR: 本文系统化研究了差分隐私（DP）审计技术，总结了当前研究的关键见解和开放挑战。


<details>
  <summary>Details</summary>
Motivation: 旨在为DP审计领域提供一个全面的框架，明确审计应满足的三大目标：效率、端到端性和紧密度。

Method: 通过系统化分析现有DP审计技术的操作模式（包括威胁模型、攻击方式和评估函数），识别了以往研究忽略的关键细节。

Result: 提出了一个可重复使用的系统化方法，用于评估领域进展并识别未来研究方向。

Conclusion: 本文为社区提供了一个评估DP审计技术进展和未来研究方向的框架。

Abstract: This paper systematizes research on auditing Differential Privacy (DP)
techniques, aiming to identify key insights into the current state of the art
and open challenges. First, we introduce a comprehensive framework for
reviewing work in the field and establish three cross-contextual desiderata
that DP audits should target--namely, efficiency, end-to-end-ness, and
tightness. Then, we systematize the modes of operation of state-of-the-art DP
auditing techniques, including threat models, attacks, and evaluation
functions. This allows us to highlight key details overlooked by prior work,
analyze the limiting factors to achieving the three desiderata, and identify
open research problems. Overall, our work provides a reusable and systematic
methodology geared to assess progress in the field and identify friction points
and future directions for our community to focus on.

</details>


<div id='math.ST'></div>

# math.ST [[Back]](#toc)

### [544] [Multi-Armed Bandits With Machine Learning-Generated Surrogate Rewards](https://arxiv.org/abs/2506.16658)
*Wenlong Ji,Yihan Pan,Ruihao Zhu,Lihua Lei*

Main category: math.ST

TL;DR: 论文提出了一种新的多臂老虎机（MAB）框架，利用预训练的机器学习模型将辅助数据转化为替代奖励，并设计了MLA-UCB算法以解决替代奖励的偏差问题。


<details>
  <summary>Details</summary>
Motivation: 传统MAB算法仅依赖在线数据，而实际场景中丰富的辅助数据未被充分利用。论文旨在利用这些数据提升决策效率。

Method: 提出MLA-UCB算法，结合预训练的ML模型生成的替代奖励，无需先验协方差矩阵知识，适用于任意预测模型和辅助数据形式。

Result: 在预测与真实奖励联合高斯分布且相关性非零时，MLA-UCB能显著降低累积遗憾，即使替代奖励均值与真实均值完全不一致。

Conclusion: MLA-UCB在数值实验中表现优于标准UCB，尤其在离线数据量和相关性适中时仍能显著提升效率。

Abstract: Multi-armed bandit (MAB) is a widely adopted framework for sequential
decision-making under uncertainty. Traditional bandit algorithms rely solely on
online data, which tends to be scarce as it must be gathered during the online
phase when the arms are actively pulled. However, in many practical settings,
rich auxiliary data, such as covariates of past users, is available prior to
deploying any arms. We introduce a new setting for MAB where pre-trained
machine learning (ML) models are applied to convert side information and
historical data into \emph{surrogate rewards}. A prominent feature of this
setting is that the surrogate rewards may exhibit substantial bias, as true
reward data is typically unavailable in the offline phase, forcing ML
predictions to heavily rely on extrapolation. To address the issue, we propose
the Machine Learning-Assisted Upper Confidence Bound (MLA-UCB) algorithm, which
can be applied to any reward prediction model and any form of auxiliary data.
When the predicted and true rewards are jointly Gaussian, it provably improves
the cumulative regret, provided that the correlation is non-zero -- even in
cases where the mean surrogate reward completely misaligns with the true mean
rewards. Notably, our method requires no prior knowledge of the covariance
matrix between true and surrogate rewards. We compare MLA-UCB with the standard
UCB on a range of numerical studies and show a sizable efficiency gain even
when the size of the offline data and the correlation between predicted and
true rewards are moderate.

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [545] [Graphics4Science: Computer Graphics for Scientific Impacts](https://arxiv.org/abs/2506.15786)
*Peter Yichen Chen,Minghao Guo,Hanspeter Pfister,Ming Lin,William Freeman,Qixing Huang,Han-Wei Shen,Wojciech Matusik*

Main category: cs.GR

TL;DR: 该课程探讨计算机图形学与科学的深层关系，强调其作为科学建模语言的潜力，并鼓励图形学社区参与解决科学问题。


<details>
  <summary>Details</summary>
Motivation: 计算机图形学在科学领域有广泛应用，但两领域间的词汇差距限制了合作。课程旨在弥合这一差距，推动图形学在科学发现中的作用。

Method: 课程通过几何推理和物理建模等核心方法，为数据稀缺的科学问题提供归纳偏置。

Result: 课程展示了图形学在科学中的成就和贡献，并提出了开放性问题，鼓励社区参与。

Conclusion: Graphics4Science旨在将图形学重新定义为科学的建模语言，推动两领域的合作与创新。

Abstract: Computer graphics, often associated with films, games, and visual effects,
has long been a powerful tool for addressing scientific challenges--from its
origins in 3D visualization for medical imaging to its role in modern
computational modeling and simulation. This course explores the deep and
evolving relationship between computer graphics and science, highlighting past
achievements, ongoing contributions, and open questions that remain. We show
how core methods, such as geometric reasoning and physical modeling, provide
inductive biases that help address challenges in both fields, especially in
data-scarce settings. To that end, we aim to reframe graphics as a modeling
language for science by bridging vocabulary gaps between the two communities.
Designed for both newcomers and experts, Graphics4Science invites the graphics
community to engage with science, tackle high-impact problems where graphics
expertise can make a difference, and contribute to the future of scientific
discovery. Additional details are available on the course website:
https://graphics4science.github.io

</details>


### [546] [VEIGAR: View-consistent Explicit Inpainting and Geometry Alignment for 3D object Removal](https://arxiv.org/abs/2506.15821)
*Pham Khai Nguyen Do,Bao Nguyen Tran,Nam Nguyen,Duc Dung Nguyen*

Main category: cs.GR

TL;DR: VEIGAR是一种高效的新视角合成框架，无需初始3D重建阶段，通过轻量级基础模型和尺度不变深度损失监督策略，显著提升了重建质量和跨视图一致性，同时大幅减少训练时间。


<details>
  <summary>Details</summary>
Motivation: 现有方法通常依赖初始3D重建阶段，计算成本高且重建质量不佳，因此需要一种更高效且性能优越的解决方案。

Method: VEIGAR采用轻量级基础模型在像素空间显式对齐先验，并引入基于尺度不变深度损失的新型监督策略，避免传统单目深度正则化中的尺度调整操作。

Result: 实验表明，VEIGAR在重建质量和跨视图一致性上达到新标杆，训练时间减少三倍。

Conclusion: VEIGAR在效率和效果上实现了优越的平衡，为NVS和3D生成任务提供了更高效的解决方案。

Abstract: Recent advances in Novel View Synthesis (NVS) and 3D generation have
significantly improved editing tasks, with a primary emphasis on maintaining
cross-view consistency throughout the generative process. Contemporary methods
typically address this challenge using a dual-strategy framework: performing
consistent 2D inpainting across all views guided by embedded priors either
explicitly in pixel space or implicitly in latent space; and conducting 3D
reconstruction with additional consistency guidance. Previous strategies, in
particular, often require an initial 3D reconstruction phase to establish
geometric structure, introducing considerable computational overhead. Even with
the added cost, the resulting reconstruction quality often remains suboptimal.
In this paper, we present VEIGAR, a computationally efficient framework that
outperforms existing methods without relying on an initial reconstruction
phase. VEIGAR leverages a lightweight foundation model to reliably align priors
explicitly in the pixel space. In addition, we introduce a novel supervision
strategy based on scale-invariant depth loss, which removes the need for
traditional scale-and-shift operations in monocular depth regularization.
Through extensive experimentation, VEIGAR establishes a new state-of-the-art
benchmark in reconstruction quality and cross-view consistency, while achieving
a threefold reduction in training time compared to the fastest existing method,
highlighting its superior balance of efficiency and effectiveness.

</details>


### [547] [GratNet: A Photorealistic Neural Shader for Diffractive Surfaces](https://arxiv.org/abs/2506.15815)
*Narayan Kandel,Daljit Singh J. S. Dhillon*

Main category: cs.GR

TL;DR: 本文提出了一种基于多层感知机（MLP）的数据驱动方法，用于高效准确地渲染衍射表面，显著降低了数据存储需求。


<details>
  <summary>Details</summary>
Motivation: 当前结构着色模型依赖密集的预处理数据，缺乏对隐式神经表示的全面研究。

Method: 采用MLP方法，从数据压缩角度设计训练和建模方法，避免过拟合并具有鲁棒的重采样行为。

Result: 通过PSNR、SSIM和FLIP评估，实现了高质量重建，内存占用减少两个数量级。

Conclusion: 该方法在保持主观相似结果的同时，显著提升了性能，适用于衍射表面渲染。

Abstract: Structural coloration is commonly modeled using wave optics for reliable and
photorealistic rendering of natural, quasi-periodic and complex nanostructures.
Such models often rely on dense, preliminary or preprocessed data to accurately
capture the nuanced variations in diffractive surface reflectances. This heavy
data dependency warrants implicit neural representation which has not been
addressed comprehensively in the current literature. In this paper, we present
a multi-layer perceptron (MLP) based method for data-driven rendering of
diffractive surfaces with high accuracy and efficiency. We primarily approach
this problem from a data compression perspective to devise a nuanced training
and modeling method which is attuned to the domain and range characteristics of
diffractive reflectance datasets. Importantly, our approach avoids over-fitting
and has robust resampling behavior. Using Peak-Signal-to-Noise (PSNR),
Structural Similarity Index Measure (SSIM) and a flipping difference evaluator
(FLIP) as evaluation metrics, we demonstrate the high-quality reconstruction of
the ground-truth. In comparison to a recent state-of-the-art offline,
wave-optical, forward modeling approach, our method reproduces subjectively
similar results with significant performance gains. We reduce the memory
footprint of the raw datasets by two orders of magnitude in general. Lastly, we
depict the working of our method with actual surface renderings.

</details>


### [548] [FlatCAD: Fast Curvature Regularization of Neural SDFs for CAD Models](https://arxiv.org/abs/2506.16627)
*Haotian Yin,Aleksander Plocharski,Michal Jan Wlodarczyk,Mikolaj Kida,Przemyslaw Musialski*

Main category: cs.GR

TL;DR: 论文提出了一种新的曲率代理方法，用于优化神经符号距离场（SDF）的几何学习，避免了昂贵的Hessian计算和二阶自动微分，显著降低了内存和运行时间。


<details>
  <summary>Details</summary>
Motivation: 当前神经SDF在几何学习中需要依赖高斯曲率惩罚，这需要完整的Hessian评估和二阶自动微分，导致高昂的内存和运行时成本。

Method: 提出了两种曲率代理方法：(i)有限差分代理，用四个前向SDF评估和一个一阶梯度替代Hessian条目；(ii)自动微分代理，通过Hessian-向量积计算混合导数，避免显式Hessian组装。

Result: 在ABC基准测试中，代理方法与基于Hessian的基线相比，重建保真度相当或更高，同时将GPU内存使用和运行时间减少了一半。

Conclusion: 该方法是一种即插即用且框架无关的解决方案，为工程级形状重建提供了可扩展的、曲率感知的SDF学习路径。

Abstract: Neural signed-distance fields (SDFs) have become a versatile backbone for
geometric learning, yet enforcing developable, CAD-style behavior still hinges
on Gaussian curvature penalties that require full Hessian evaluation and
second-order automatic differentiation, both of which are costly in memory and
runtime. We present a curvature proxy that regularizes only the mixed
second-order term (Weingarten term), allowing the two principal curvatures to
adapt freely to data while suppressing unwanted warp. Two complementary
instantiations realize this idea: (i) a finite-difference proxy that replaces
each Hessian entry with four forward SDF evaluations and a single first-order
gradient, and (ii) an autodiff proxy that computes the same mixed derivative
via one Hessian-vector product, sidestepping explicit full Hessian assembly and
remaining faster in practice. Both variants converge to the exact mixed second
derivative, thus preserving the intended geometric bias without incurring full
second-order graphs. On the ABC benchmarks, the proxies match or exceed the
reconstruction fidelity of Hessian-based baselines while reducing GPU memory
use and wall-clock time by a factor of two. Because the method is drop-in and
framework-agnostic, it opens a practical path toward scalable, curvature-aware
SDF learning for engineering-grade shape reconstruction.

</details>


### [549] [Beyond Blur: A Fluid Perspective on Generative Diffusion Models](https://arxiv.org/abs/2506.16827)
*Grzegorz Gruszczynski,Michal Jan Wlodarczyk,Jakub J Meixner,Przemyslaw Musialski*

Main category: cs.GR

TL;DR: 提出了一种基于PDE的图像生成方法，通过物理驱动的腐蚀过程，结合对流-扩散和噪声，利用GPU加速求解器实现高效生成。


<details>
  <summary>Details</summary>
Motivation: 结合流体动力学和无量纲PDE理论，为基于扩散的图像生成提供新的物理视角。

Method: 使用GPU加速的Lattice Boltzmann求解器实现PDE数值解，生成随机速度场以模拟湍流，神经网络学习反转腐蚀过程。

Result: 对流提高了生成图像的多样性和质量，同时保持色彩一致性，框架可泛化现有PDE方法。

Conclusion: 该工作为物理驱动的图像生成提供了新思路，结合了流体动力学与深度学习。

Abstract: We propose a novel PDE-driven corruption process for generative image
synthesis based on advection-diffusion processes which generalizes existing
PDE-based approaches. Our forward pass formulates image corruption via a
physically motivated PDE that couples directional advection with isotropic
diffusion and Gaussian noise, controlled by dimensionless numbers (Peclet,
Fourier). We implement this PDE numerically through a GPU-accelerated custom
Lattice Boltzmann solver for fast evaluation. To induce realistic turbulence,
we generate stochastic velocity fields that introduce coherent motion and
capture multi-scale mixing. In the generative process, a neural network learns
to reverse the advection-diffusion operator thus constituting a novel
generative model. We discuss how previous methods emerge as specific cases of
our operator, demonstrating that our framework generalizes prior PDE-based
corruption techniques. We illustrate how advection improves the diversity and
quality of the generated images while keeping the overall color palette
unaffected. This work bridges fluid dynamics, dimensionless PDE theory, and
deep generative modeling, offering a fresh perspective on physically informed
image corruption processes for diffusion-based synthesis.

</details>


### [550] [DreamCube: 3D Panorama Generation via Multi-plane Synchronization](https://arxiv.org/abs/2506.17206)
*Yukun Huang,Yanning Zhou,Jianan Wang,Kaiyi Huang,Xihui Liu*

Main category: cs.GR

TL;DR: 论文提出了一种通过多平面同步扩展2D基础模型能力的方法，并设计了DreamCube模型，用于3D全景生成，实现了多样外观和准确几何。


<details>
  <summary>Details</summary>
Motivation: 解决现有方法因3D全景与2D单视图不兼容而受限的问题，利用2D基础模型的丰富先验知识。

Method: 应用多平面同步技术扩展2D基础模型能力，并设计多平面RGB-D扩散模型DreamCube。

Result: 实验证明该方法在全景图像生成、深度估计和3D场景生成中有效。

Conclusion: 通过多平面同步和DreamCube模型，成功将2D基础模型能力扩展到全景领域。

Abstract: 3D panorama synthesis is a promising yet challenging task that demands
high-quality and diverse visual appearance and geometry of the generated
omnidirectional content. Existing methods leverage rich image priors from
pre-trained 2D foundation models to circumvent the scarcity of 3D panoramic
data, but the incompatibility between 3D panoramas and 2D single views limits
their effectiveness. In this work, we demonstrate that by applying multi-plane
synchronization to the operators from 2D foundation models, their capabilities
can be seamlessly extended to the omnidirectional domain. Based on this design,
we further introduce DreamCube, a multi-plane RGB-D diffusion model for 3D
panorama generation, which maximizes the reuse of 2D foundation model priors to
achieve diverse appearances and accurate geometry while maintaining multi-view
consistency. Extensive experiments demonstrate the effectiveness of our
approach in panoramic image generation, panoramic depth estimation, and 3D
scene generation.

</details>


<div id='cs.OS'></div>

# cs.OS [[Back]](#toc)

### [551] [ROS 2 Agnocast: Supporting Unsized Message Types for True Zero-Copy Publish/Subscribe IPC](https://arxiv.org/abs/2506.16882)
*Takahiro Ishikawa-Aso,Shinpei Kato*

Main category: cs.OS

TL;DR: Agnocast是一个适用于ROS 2 C++的零拷贝IPC框架，满足所有关键需求，显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 大规模ROS 2系统（如自动驾驶平台）需要高效零拷贝通信，但现有解决方案无法完全满足关键需求。

Method: 提出Agnocast框架，支持所有ROS 2消息类型，最小化代码修改，并选择性实现零拷贝通信。

Result: Agnocast在Autoware点云预处理中，平均响应时间提升16%，最坏情况响应时间提升25%。

Conclusion: Agnocast是一个实用且高效的零拷贝解决方案，适用于ROS 2系统。

Abstract: Robot applications, comprising independent components that mutually
publish/subscribe messages, are built on inter-process communication (IPC)
middleware such as Robot Operating System 2 (ROS 2). In large-scale ROS 2
systems like autonomous driving platforms, true zero-copy communication --
eliminating serialization and deserialization -- is crucial for efficiency and
real-time performance. However, existing true zero-copy middleware solutions
lack widespread adoption as they fail to meet three essential requirements: 1)
Support for all ROS 2 message types including unsized ones; 2) Minimal
modifications to existing application code; 3) Selective implementation of
zero-copy communication between specific nodes while maintaining conventional
communication mechanisms for other inter-node communications including
inter-host node communications. This first requirement is critical, as
production-grade ROS 2 projects like Autoware rely heavily on unsized message
types throughout their codebase to handle diverse use cases (e.g., various
sensors), and depend on the broader ROS 2 ecosystem, where unsized message
types are pervasive in libraries. The remaining requirements facilitate
seamless integration with existing projects. While IceOryx middleware, a
practical true zero-copy solution, meets all but the first requirement, other
studies achieving the first requirement fail to satisfy the remaining criteria.
This paper presents Agnocast, a true zero-copy IPC framework applicable to ROS
2 C++ on Linux that fulfills all these requirements. Our evaluation
demonstrates that Agnocast maintains constant IPC overhead regardless of
message size, even for unsized message types. In Autoware PointCloud
Preprocessing, Agnocast achieves a 16% improvement in average response time and
a 25% improvement in worst-case response time.

</details>


<div id='physics.ins-det'></div>

# physics.ins-det [[Back]](#toc)

### [552] [Bias Variation Compensation in Perimeter-Gated SPAD TRNGs](https://arxiv.org/abs/2506.15888)
*Md Sakibur Sajal,Hunter Guthrie,Marc Dandin*

Main category: physics.ins-det

TL;DR: 论文提出了一种基于64x64阵列的pgSPADs熵源，通过调节门电压降低偏置变化（BV），并结合Von Neumann算法去偏，生成高质量随机数。


<details>
  <summary>Details</summary>
Motivation: 现有硬件友好的去偏算法无法适应宽范围的偏置变化（BV），需要一种新的熵源和补偿技术。

Method: 使用64x64阵列的pgSPADs作为熵源，通过调节门电压控制BV，并应用Von Neumann算法去偏。

Result: 在室温下实现了小于1%的BV，去偏后的比特通过了NIST的全部16项统计测试。

Conclusion: 该方法有效降低了BV，生成了高质量的随机数，适用于硬件实现。

Abstract: Random number generators that utilize arrays of entropy source elements
suffer from bias variation (BV). Despite the availability of efficient
debiasing algorithms, optimized implementations of hardware friendly options
depend on the bit bias in the raw bit streams and cannot accommodate a wide BV.
In this work, we present a 64 x 64 array of perimeter gated single photon
avalanche diodes (pgSPADs), fabricated in a 0.35 {\mu}m standard CMOS
technology, as a source of entropy to generate random binary strings with a BV
compensation technique. By applying proper gate voltages based on the devices'
native dark count rates, we demonstrate less than 1% BV for a raw-bit
generation rate of 2 kHz/pixel at room temperature. The raw bits were debiased
using the classical iterative Von Neumann's algorithm and the debiased bits
were found to pass all of the 16 tests from NIST's Statistical Test Suite.

</details>


### [553] [Improvement of Nuclide Detection through Graph Spectroscopic Analysis Framework and its Application to Nuclear Facility Upset Detection](https://arxiv.org/abs/2506.16522)
*Pedro Rodríguez Fernández,Christian Svinth,Alex Hagen*

Main category: physics.ins-det

TL;DR: 提出了一种利用辐射量子到达时间和注意力机制神经网络提高放射性核素检测灵敏度的方法，相比传统光谱方法提升2倍。


<details>
  <summary>Details</summary>
Motivation: 传统光谱方法在放射性核素检测中存在灵敏度不足的问题，尤其是在复杂衰变链的情况下。

Method: 使用带有注意力机制的神经网络，通过调整检测阈值（基于时间事件分布和局部光谱特征）来优化检测概率。

Result: 在核设施释放铯的检测中，新方法比传统方法灵敏度提高2倍。

Conclusion: 该方法具有广泛适用性，可扩展到其他检测事件数据（如脉冲质量、探测器位置等），尤其适用于复杂衰变链的放射性核素。

Abstract: We present a method to improve the detection limit for radionuclides using
spectroscopic radiation detectors and the arrival time of each detected
radiation quantum. We enable this method using a neural network with an
attention mechanism. We illustrate the method on the detection of Cesium
release from a nuclear facility during an upset, and our method shows $2\times$
improvement over the traditional spectroscopic method. We hypothesize that our
method achieves this performance increase by modulating its detection
probability by the overall rate of probable detections, specifically by
adapting detection thresholds based on temporal event distributions and local
spectral features, and show evidence to this effect. We believe this method is
applicable broadly and may be more successful for radionuclides with more
complicated decay chains than Cesium; we also note that our method can
generalize beyond the addition of arrival time and could integrate other data
about each detection event, such as pulse quality, location in detector, or
even combining the energy and time from detections in different detectors.

</details>


<div id='cond-mat.str-el'></div>

# cond-mat.str-el [[Back]](#toc)

### [554] [Simulating Correlated Electrons with Symmetry-Enforced Normalizing Flows](https://arxiv.org/abs/2506.17015)
*Dominic Schuh,Janik Kreit,Evan Berkowitz,Lena Funcke,Thomas Luu,Kim A. Nicoli,Marcel Rodekamp*

Main category: cond-mat.str-el

TL;DR: 利用归一化流准确学习费米子Hubbard模型的玻尔兹曼分布，解决了传统方法的遍历性问题，并显著提升速度。


<details>
  <summary>Details</summary>
Motivation: 传统方法如混合蒙特卡洛在时间连续极限附近存在遍历性问题，导致估计偏差。

Method: 采用对称感知架构和独立同分布采样。

Result: 解决了遍历性问题，并显著提升了计算速度。

Conclusion: 归一化流在费米子Hubbard模型中表现出色，优于传统方法。

Abstract: We present the first proof of principle that normalizing flows can accurately
learn the Boltzmann distribution of the fermionic Hubbard model - a key
framework for describing the electronic structure of graphene and related
materials. State-of-the-art methods like Hybrid Monte Carlo often suffer from
ergodicity issues near the time-continuum limit, leading to biased estimates.
Leveraging symmetry-aware architectures as well as independent and identically
distributed sampling, our approach resolves these issues and achieves
significant speed-ups over traditional methods.

</details>


<div id='q-bio.QM'></div>

# q-bio.QM [[Back]](#toc)

### [555] [Smartphone-integrated RPA-CRISPR-Cas12a Detection System with Microneedle Sampling for Point-of-Care Diagnosis of Potato Late Blight in Early Stage](https://arxiv.org/abs/2506.15728)
*Jiangnan Zhao,Hanbo Xu,Cifu Xu,Wenlong Yin,Laixin Luo,Gang Liu,Yan Wang*

Main category: q-bio.QM

TL;DR: 开发了一种基于RPA-CRISPR的便携式诊断系统，结合智能手机进行荧光图像分析，用于马铃薯晚疫病的早期检测。


<details>
  <summary>Details</summary>
Motivation: 传统PCR和LAMP方法依赖昂贵设备和复杂操作，不适用于田间即时诊断。

Method: 使用PVA微针贴片快速提取样本，建立RPA-CRISPR-Cas12a等温检测系统，结合智能手机分析。

Result: 系统检测限为2 pg/uL，灵敏度与实验室设备相当，接种后第3天和第4天分别达到80%和100%检出率。

Conclusion: 该系统为田间植物病害早期检测提供了高效、便携的解决方案。

Abstract: Potato late blight, caused by the oomycete pathogen Phytophthora infestans,
is one of the most devastating diseases affecting potato crops in the history.
Although conventional detection methods of plant diseases such as PCR and LAMP
are highly sensitive and specific, they rely on bulky and expensive laboratory
equipment and involve complex operations, making them impracticable for
point-of care diagnosis in the field. Here in this study, we report a portable
RPA-CRISPR based diagnosis system for plant disease, integrating smartphone for
acquisition and analysis of fluorescent images. A polyvinyl alcohol (PVA)
microneedle patch was employed for sample extraction on the plant leaves within
one minute, the DNA extraction efficiency achieved 56 ug/mg, which is
approximately 3 times to the traditional CTAB methods (18 ug/mg). The system of
RPA-CRISPR-Cas12a isothermal assay was established to specifically target P.
infestans with no cross-reactivity observed against closely-related species (P.
sojae, P. capsici). The system demonstrated a detection limit of 2 pg/uL for P.
infestans genomic DNA, offering sensitivity comparable to that of benchtop
laboratory equipment. The system demonstrates the early-stage diagnosis
capability by achieving a approximately 80% and 100% detection rate on the
third and fourth day post-inoculation respectively, before visible symptoms
observed on the leaves. The smartphone-based "sample-to-result" system
decouples the limitations of traditional methods that rely heavily on
specialized equipment, offering a promising way for early-stage plant disease
detection and control in the field.

</details>


<div id='cs.SY'></div>

# cs.SY [[Back]](#toc)

### [556] [Formal Control for Uncertain Systems via Contract-Based Probabilistic Surrogates (Extended Version)](https://arxiv.org/abs/2506.16971)
*Oliver Schön,Sofie Haesaert,Sadegh Soudjani*

Main category: cs.SY

TL;DR: 提出了一种基于抽象的技术，通过消除直接计算误差界限的需求，显著提升了概率模拟关系和随机系统代理模型的扩展性和实用性。


<details>
  <summary>Details</summary>
Motivation: 精确系统表示的识别需求不仅难以满足，还限制了形式化方法的扩展性，导致模型过于复杂，难以有效决策。

Method: 聚焦于概率模拟关系和随机系统的代理模型，提出了一种抽象技术，避免直接计算误差界限。

Result: 该方法在复杂高维车辆交叉口案例中验证了其扩展性与保守性之间的良好平衡。

Conclusion: 该技术有效提升了形式化方法在不确定性和复杂非线性交互中的实用性。

Abstract: The requirement for identifying accurate system representations has not only
been a challenge to fulfill, but it has compromised the scalability of formal
methods, as the resulting models are often too complex for effective decision
making with formal correctness and performance guarantees. Focusing on
probabilistic simulation relations and surrogate models of stochastic systems,
we propose an approach that significantly enhances the scalability and
practical applicability of such simulation relations by eliminating the need to
compute error bounds directly. As a result, we provide an abstraction-based
technique that scales effectively to higher dimensions while addressing complex
nonlinear agent-environment interactions with infinite-horizon temporal logic
guarantees amidst uncertainty. Our approach trades scalability for conservatism
favorably, as demonstrated on a complex high-dimensional vehicle intersection
case study.

</details>


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [557] [Empowering Near-Field Communications in Low-Altitude Economy with LLM: Fundamentals, Potentials, Solutions, and Future Directions](https://arxiv.org/abs/2506.17067)
*Zhuo Xu,Tianyue Zheng,Linglong Dai*

Main category: eess.SP

TL;DR: 论文探讨了低空经济（LAE）与超大规模MIMO（XL-MIMO）系统的近场通信结合，提出利用大语言模型（LLM）解决近场通信中的挑战。


<details>
  <summary>Details</summary>
Motivation: 低空经济（LAE）与近场通信结合可提升频谱效率，但面临信号处理复杂性和用户区分等挑战，LLM为解决这些问题提供了新思路。

Method: 论文提出了一种基于LLM的方案，用于区分远近场用户并设计多用户预编码矩阵。

Result: 通过案例研究展示了LLM在解决近场通信挑战中的潜力。

Conclusion: 论文总结了LLM在LAE近场通信中的应用前景，并提出了未来研究方向。

Abstract: The low-altitude economy (LAE) is gaining significant attention from academia
and industry. Fortunately, LAE naturally aligns with near-field communications
in extremely large-scale MIMO (XL-MIMO) systems. By leveraging near-field
beamfocusing, LAE can precisely direct beam energy to unmanned aerial vehicles,
while the additional distance dimension boosts overall spectrum efficiency.
However, near-field communications in LAE still face several challenges, such
as the increase in signal processing complexity and the necessity of
distinguishing between far and near-field users. Inspired by the large language
models (LLM) with powerful ability to handle complex problems, we apply LLM to
solve challenges of near-field communications in LAE. The objective of this
article is to provide a comprehensive analysis and discussion on LLM-empowered
near-field communications in LAE. Specifically, we first introduce fundamentals
of LLM and near-field communications, including the key advantages of LLM and
key characteristics of near-field communications. Then, we reveal the
opportunities and challenges of near-field communications in LAE. To address
these challenges, we present a LLM-based scheme for near-field communications
in LAE, and provide a case study which jointly distinguishes far and near-field
users and designs multi-user precoding matrix. Finally, we outline and
highlight several future research directions and open issues.

</details>


<div id='cs.CG'></div>

# cs.CG [[Back]](#toc)

### [558] [Wavelet-based Global Orientation and Surface Reconstruction for Point Clouds](https://arxiv.org/abs/2506.16299)
*Yueji Ma,Yanzun Meng,Dong Xiao,Zuoqiang Shi,Bin Wang*

Main category: cs.CG

TL;DR: 提出了一种基于小波的方法，用于处理无定向点云的重建问题，通过改进核函数和小波基函数的性质，实现了高效且稳定的重建性能。


<details>
  <summary>Details</summary>
Motivation: 传统小波重建方法仅适用于定向点云，而现有改进方法（如iWSR）在稀疏点云上表现不佳。本文旨在解决这些问题。

Method: 利用小波基函数的紧支撑性和正交性，通过改进核函数平滑表面不连续性，并利用卷积核函数性质加速计算。提出了一种构建无散度函数场的方法以增强稳定性和效果。

Result: 实验表明，该方法在稀疏点云的定向和重建任务中达到最优性能，并在CPU上实现高效运行。

Conclusion: 该方法通过优化小波基函数和核函数的设计，显著提升了无定向点云重建的效果和效率。

Abstract: Unoriented surface reconstruction is an important task in computer graphics
and has extensive applications. Based on the compact support of wavelet and
orthogonality properties, classic wavelet surface reconstruction achieves good
and fast reconstruction. However, this method can only handle oriented points.
Despite some improved attempts for unoriented points, such as iWSR, these
methods perform poorly on sparse point clouds. To address these shortcomings,
we propose a wavelet-based method to represent the mollified indicator function
and complete both the orientation and surface reconstruction tasks. We use the
modifying kernel function to smoothen out discontinuities on the surface,
aligning with the continuity of the wavelet basis function. During the
calculation of coefficient, we fully utilize the properties of the
convolutional kernel function to shift the modifying computation onto wavelet
basis to accelerate. In addition, we propose a novel method for constructing
the divergence-free function field and using them to construct the additional
homogeneous constraints to improve the effectiveness and stability. Extensive
experiments demonstrate that our method achieves state-of-the-art performance
in both orientation and reconstruction for sparse models. We align the matrix
construction with the compact support property of wavelet basis functions to
further accelerate our method, resulting in efficient performance on CPU. Our
source codes will be released on GitHub.

</details>


<div id='cond-mat.quant-gas'></div>

# cond-mat.quant-gas [[Back]](#toc)

### [559] [Single-shot thermometry of simulated Bose--Einstein condensates using artificial intelligence](https://arxiv.org/abs/2506.16925)
*Jack Griffiths,Steven A. Wrathmall,Simon A. Gardiner*

Main category: cond-mat.quant-gas

TL;DR: 使用卷积神经网络从单次原位密度图像中快速无损估计超冷玻色气体的化学势和温度，展示了零样本泛化能力。


<details>
  <summary>Details</summary>
Motivation: 传统测量技术具有破坏性且存在实验不确定性，需要一种快速、无损的测量方法。

Method: 训练卷积神经网络，基于准二维‘薄饼’凝聚体的密度图像，实现参数提取。

Result: 模型在未训练的环形陷阱几何和动态热化过程中仍能准确预测，误差仅为几纳开尔文。

Conclusion: 监督学习可突破传统超冷原子测温的限制，有望实现量子气体实验的实时分析。

Abstract: Precise determination of thermodynamic parameters in ultracold Bose gases
remains challenging due to the destructive nature of conventional measurement
techniques and inherent experimental uncertainties. We demonstrate an
artificial intelligence approach for rapid, non-destructive estimation of the
chemical potential and temperature from single-shot, in situ imaged density
profiles of finite-temperature Bose gases. Our convolutional neural network is
trained exclusively on quasi-2D `pancake' condensates in harmonic trap
configurations. It achieves parameter extraction within fractions of a second.
The model also demonstrates zero-shot generalisation across both trap geometry
and thermalisation dynamics, successfully estimating thermodynamic parameters
for toroidally trapped condensates with errors of only a few nanokelvin despite
no prior exposure to such geometries during training, and maintaining
predictive accuracy during dynamic thermalisation processes after a relatively
brief evolution without explicit training on non-equilibrium states. These
results suggest that supervised learning can overcome traditional limitations
in ultracold atom thermometry, with extension to broader geometric
configurations, temperature ranges, and additional parameters potentially
enabling comprehensive real-time analysis of quantum gas experiments. Such
capabilities could significantly streamline experimental workflows whilst
improving measurement precision across a range of quantum fluid systems.

</details>


<div id='cs.ET'></div>

# cs.ET [[Back]](#toc)

### [560] [Quantum Artificial Intelligence for Secure Autonomous Vehicle Navigation: An Architectural Proposal](https://arxiv.org/abs/2506.16000)
*Hemanth Kannamarlapudi,Sowmya Chintalapudi*

Main category: cs.ET

TL;DR: 提出了一种基于量子人工智能的新型架构，用于自动驾驶车辆的导航决策和通信，包括量子神经网络、量子强化学习和后量子加密协议。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶车辆的导航需要处理大量数据并做出安全决策，现有方法在性能和安全性上存在挑战。

Method: 使用量子神经网络进行多模态传感器融合，Nav-Q模块通过量子强化学习优化导航策略，后量子加密协议确保通信安全。

Result: 该框架提供了量子性能的未来安全解决方案，解决了自动驾驶导航中的关键问题。

Conclusion: 提出的方法为自动驾驶车辆导航提供了高性能和安全性，具有潜在的实际应用价值。

Abstract: Navigation is a very crucial aspect of autonomous vehicle ecosystem which
heavily relies on collecting and processing large amounts of data in various
states and taking a confident and safe decision to define the next vehicle
maneuver. In this paper, we propose a novel architecture based on Quantum
Artificial Intelligence by enabling quantum and AI at various levels of
navigation decision making and communication process in Autonomous vehicles :
Quantum Neural Networks for multimodal sensor fusion, Nav-Q for Quantum
reinforcement learning for navigation policy optimization and finally
post-quantum cryptographic protocols for secure communication. Quantum neural
networks uses quantum amplitude encoding to fuse data from various sensors like
LiDAR, radar, camera, GPS and weather etc., This approach gives a unified
quantum state representation between heterogeneous sensor modalities. Nav-Q
module processes the fused quantum states through variational quantum circuits
to learn optimal navigation policies under swift dynamic and complex
conditions. Finally, post quantum cryptographic protocols are used to secure
communication channels for both within vehicle communication and V2X (Vehicle
to Everything) communications and thus secures the autonomous vehicle
communication from both classical and quantum security threats. Thus, the
proposed framework addresses fundamental challenges in autonomous vehicles
navigation by providing quantum performance and future proof security. Index
Terms Quantum Computing, Autonomous Vehicles, Sensor Fusion

</details>


### [561] [Artificial Intelligence for Atmospheric Sciences: A Research Roadmap](https://arxiv.org/abs/2506.16281)
*Martha Arbayani Zaidan,Naser Hossein Motlagh,Petteri Nurmi,Tareq Hussein,Markku Kulmala,Tuukka Petäjä,Sasu Tarkoma*

Main category: cs.ET

TL;DR: 本文综述了人工智能（AI）在大气科学中的变革潜力，探讨了AI整合中的关键挑战，并提出了研究路线图。


<details>
  <summary>Details</summary>
Motivation: 大气科学对理解环境现象至关重要，而AI等技术的进步为研究提供了新工具。本文旨在弥合大气科学与计算机科学的交叉领域。

Method: 通过跨学科综述，分析AI在大气科学中的应用及挑战。

Result: 提出了解决当前和新兴挑战的研究路线图。

Conclusion: AI在大气科学中具有巨大潜力，但需克服数据与基础设施等挑战。

Abstract: Atmospheric sciences are crucial for understanding environmental phenomena
ranging from air quality to extreme weather events, and climate change. Recent
breakthroughs in sensing, communication, computing, and Artificial Intelligence
(AI) have significantly advanced atmospheric sciences, enabling the generation
of vast amounts of data through long-term Earth observations and providing
powerful tools for analyzing atmospheric phenomena and predicting natural
disasters. This paper contributes a critical interdisciplinary overview that
bridges the fields of atmospheric science and computer science, highlighting
the transformative potential of AI in atmospheric research. We identify key
challenges associated with integrating AI into atmospheric research, including
issues related to big data and infrastructure, and provide a detailed research
roadmap that addresses both current and emerging challenges.

</details>


<div id='cs.SI'></div>

# cs.SI [[Back]](#toc)

### [562] [Unpacking Generative AI in Education: Computational Modeling of Teacher and Student Perspectives in Social Media Discourse](https://arxiv.org/abs/2506.16412)
*Paulina DeVito,Akhil Vallala,Sean Mcmahon,Yaroslav Hinda,Benjamin Thaw,Hanqi Zhuang,Hari Kalva*

Main category: cs.SI

TL;DR: 研究分析了生成式AI在教育中的影响，通过社交媒体数据（1,199篇Reddit帖子和13,959条评论）进行情感分析、主题建模和作者分类，提出并验证了一种基于LLM的模块化框架，其性能优于传统NLP模型。


<details>
  <summary>Details</summary>
Motivation: 了解学生和教育工作者对生成式AI在教育中的看法，以应对快速发展的技术带来的挑战和机遇。

Method: 使用情感分析、主题建模和作者分类，提出并验证基于GPT-4o的模块化框架，对比传统NLP模型。

Result: GPT-4o框架在情感分析中达到90.6%准确率，提取出12个潜在主题。学生和教师对生成式AI的态度存在差异：学生担忧作弊误判，教师关注职业安全和学术诚信。

Conclusion: 研究呼吁更明确的政策、透明的技术整合和支持机制，同时展示了LLM框架在分析在线社区话语中的潜力。

Abstract: Generative AI (GAI) technologies are quickly reshaping the educational
landscape. As adoption accelerates, understanding how students and educators
perceive these tools is essential. This study presents one of the most
comprehensive analyses to date of stakeholder discourse dynamics on GAI in
education using social media data. Our dataset includes 1,199 Reddit posts and
13,959 corresponding top-level comments. We apply sentiment analysis, topic
modeling, and author classification. To support this, we propose and validate a
modular framework that leverages prompt-based large language models (LLMs) for
analysis of online social discourse, and we evaluate this framework against
classical natural language processing (NLP) models. Our GPT-4o pipeline
consistently outperforms prior approaches across all tasks. For example, it
achieved 90.6% accuracy in sentiment analysis against gold-standard human
annotations. Topic extraction uncovered 12 latent topics in the public
discourse with varying sentiment and author distributions. Teachers and
students convey optimism about GAI's potential for personalized learning and
productivity in higher education. However, key differences emerged: students
often voice distress over false accusations of cheating by AI detectors, while
teachers generally express concern about job security, academic integrity, and
institutional pressures to adopt GAI tools. These contrasting perspectives
highlight the tension between innovation and oversight in GAI-enabled learning
environments. Our findings suggest a need for clearer institutional policies,
more transparent GAI integration practices, and support mechanisms for both
educators and students. More broadly, this study demonstrates the potential of
LLM-based frameworks for modeling stakeholder discourse within online
communities.

</details>


<div id='physics.med-ph'></div>

# physics.med-ph [[Back]](#toc)

### [563] [Unsupervised deep learning model for fast energy layer pre-selection of delivery-efficient proton arc therapy plan optimization of nasopharyngeal carcinoma](https://arxiv.org/abs/2506.15803)
*Bohan Yang,Gang Liu,Rirao Dao,Yujia Qian,Ke Shi,Anke Tang,Yong Luo,Jingnan Liu*

Main category: physics.med-ph

TL;DR: 该研究提出了一种基于无监督深度学习的框架SPArcdl，用于快速预选质子弧治疗（PAT）中的能量层序列，以优化治疗计划质量和交付效率。


<details>
  <summary>Details</summary>
Motivation: 质子弧治疗（PAT）在放射治疗中具有优势，但能量层序列优化计算复杂，因此需要一种高效的方法来减少能量切换时间并保持高质量的治疗计划。

Method: 研究提出了一种新的数据表示方法（spot-count representation），并通过UNet架构SPArcdl优化目标函数（靶区覆盖、OAR暴露和能量切换时间）。模型在54例鼻咽癌病例上进行了评估。

Result: SPArcdl显著提升了计划质量和交付效率，包括提高一致性指数、降低均匀性指数、缩短能量切换时间，并减少脑干平均剂量。

Conclusion: SPArcdl是一种快速有效的工具，能够通过预选能量层生成高质量的PAT计划，同时减少交付时间。

Abstract: Objective. Proton arc therapy (PAT) is an emerging and promising modality in
radiotherapy, offering several advantages over conventional intensitymodulated
proton therapy (IMPT). However, identifying the optimal energy layer (EL)
sequence remains computationally intensive due to the large number of possible
energy layer transitions. This study proposes an unsupervised deep learning
framework for fast and effective EL pre-selection, aiming to minimize energy
layer switch time while preserving high plan quality. Approach. We introduce a
novel data representation method, spot-count representation, which encodes the
number of proton spots intersecting the target and organs at risk (OARs) in a
matrix structured by sorted gantry angles and energy layers. This
representation is the input of a UNet-based architecture, SPArcdl, which is
trained to optimize a tri-objective function: maximizing target coverage,
minimizing OAR exposure, and reducing energy switching time. The model is
evaluated on 54 nasopharyngeal cancer cases, and its performance is benchmarked
against plans generated by SPArcparticle swarm. Main results. SPArcdl produces
EL pre-selection that significantly improves both plan quality and delivery
efficiency. Compared to SPArc particle swarm, it enhances the conformity index
by 0.16 (p < 0.01), reduces the homogeneity index by 0.71 (p < 0.01), shortens
the energy switching time by 38.4% (p < 0.01), and lowers the mean dose to
brainstem by 0.21 (p < 0.01). The results unintentionally reveal employing
unchanged ELS is more time-wise efficient than descended ELS. SPArcdl's
inference time is within 1 second. Significance. SPArcdl is a fast and
effective tool for generating high-quality PAT plans by strategically
pre-selecting energy layers to reduce delivery time while maintaining excellent
dosimetric performance.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [564] [MoR: Better Handling Diverse Queries with a Mixture of Sparse, Dense, and Human Retrievers](https://arxiv.org/abs/2506.15862)
*Jushaan Singh Kalra,Xinran Zhao,To Eun Kim,Fengyu Cai,Fernando Diaz,Tongshuang Wu*

Main category: cs.IR

TL;DR: 论文提出了一种动态选择和集成多种检索器的方法，称为“混合检索器”，通过零样本加权组合提升检索增强生成的性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法通常固定使用单一检索器，无法适应多样化的信息需求，因此需要动态选择和集成多种检索器。

Method: 提出混合检索器框架，零样本加权组合异构检索器（如BM25和密集检索器）。

Result: 混合检索器性能优于单一检索器和更大的模型（+10.8%和+3.9%），并能有效整合非专家人类信息源（性能提升58.9%）。

Conclusion: 混合检索器框架高效且灵活，能够动态适应不同查询需求，显著提升检索增强生成的效果。

Abstract: Retrieval-augmented Generation (RAG) is powerful, but its effectiveness
hinges on which retrievers we use and how. Different retrievers offer distinct,
often complementary signals: BM25 captures lexical matches; dense retrievers,
semantic similarity. Yet in practice, we typically fix a single retriever based
on heuristics, which fails to generalize across diverse information needs. Can
we dynamically select and integrate multiple retrievers for each individual
query, without the need for manual selection? In our work, we validate this
intuition with quantitative analysis and introduce mixture of retrievers: a
zero-shot, weighted combination of heterogeneous retrievers. Extensive
experiments show that such mixtures are effective and efficient: Despite
totaling just 0.8B parameters, this mixture outperforms every individual
retriever and even larger 7B models by +10.8% and +3.9% on average,
respectively. Further analysis also shows that this mixture framework can help
incorporate specialized non-oracle human information sources as retrievers to
achieve good collaboration, with a 58.9% relative performance improvement over
simulated humans alone.

</details>


### [565] [Revela: Dense Retriever Learning via Language Modeling](https://arxiv.org/abs/2506.16552)
*Fengyu Cai,Tong Chen,Xinran Zhao,Sihao Chen,Hongming Zhang,Sherry Tongshuang Wu,Iryna Gurevych,Heinz Koeppl*

Main category: cs.IR

TL;DR: Revela是一种通过语言建模自监督学习训练密集检索器的统一框架，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 在专业领域（如代码）中，标注查询-文档对成本高且难以获取，因此需要自监督学习训练检索器。

Method: Revela通过语言建模目标建模文档间的语义依赖，利用跨文档注意力机制优化检索器。

Result: 在BEIR和CoIR基准测试中，Revela在NDCG@10上分别相对提升18.3%和14.4%。

Conclusion: Revela展示了自监督检索器学习的有效性和可扩展性。

Abstract: Dense retrievers play a vital role in accessing external and specialized
knowledge to augment language models (LMs). Training dense retrievers typically
requires annotated query-document pairs, which are costly and hard to obtain in
specialized domains such as code-motivating growing interest in self-supervised
retriever learning. Since LMs are trained to capture token-level dependencies
through a self-supervised learning objective (i.e., next-token prediction), we
can analogously cast retrieval as learning dependencies among chunks of tokens.
This analogy naturally leads to the question: How can we adapt self-supervised
learning objectives in the spirit of language modeling to train retrievers?
  To answer this question, we introduce Revela, a unified and scalable training
framework for self-supervised retriever learning via language modeling. Revela
models semantic dependencies among documents by conditioning next-token
prediction on both local and cross-document context through an in-batch
attention mechanism. This attention is weighted by retriever-computed
similarity scores, enabling the retriever to be optimized as part of language
modeling. We evaluate Revela on both general-domain (BEIR) and domain-specific
(CoIR) benchmarks across various retriever backbones. At a comparable parameter
scale, Revela outperforms the previous best method with absolute improvements
of 5.2 % (18.3 % relative) and 5.6 % (14.4 % relative) on NDCG@10,
respectively, underscoring its effectiveness. Performance increases with model
size, highlighting both the scalability of our approach and its promise for
self-supervised retriever learning.

</details>


### [566] [GFlowGR: Fine-tuning Generative Recommendation Frameworks with Generative Flow Networks](https://arxiv.org/abs/2506.16114)
*Yejing Wang,Shengyu Zhou,Jinyu Lu,Qidong Liu,Xinhang Li,Wenlin Zhang,Feng Li,Pengjie Wang,Jian Xu,Bo Zheng,Xiangyu Zhao*

Main category: cs.IR

TL;DR: 该论文提出了一种基于GFlowNets的生成推荐框架（GFlowGR），通过多步生成任务解决现有方法中的曝光偏差问题。


<details>
  <summary>Details</summary>
Motivation: 现有生成推荐框架中的微调步骤未充分探索，且忽视了未观测到的正样本（曝光偏差问题）。

Method: 将生成推荐视为多步生成任务，构建GFlowNets框架，结合传统推荐系统的协同知识，设计自适应轨迹采样器和综合奖励模型。

Result: 在两个真实数据集和两种不同生成推荐骨干上的实验表明，GFlowGR具有高效性和鲁棒性。

Conclusion: GFlowGR通过GFlowNets的多样性生成特性，有效缓解了曝光偏差问题，为生成推荐提供了新思路。

Abstract: Generative recommendations (GR), which usually include item tokenizers and
generative Large Language Models (LLMs), have demonstrated remarkable success
across a wide range of scenarios. The majority of existing research efforts
primarily concentrate on developing powerful item tokenizers or advancing LLM
decoding strategies to attain superior performance. However, the critical
fine-tuning step in GR frameworks, which is essential for adapting LLMs to
recommendation data, remains largely unexplored. Current approaches
predominantly rely on either the next-token prediction loss of supervised
fine-tuning (SFT) or recommendationspecific direct preference optimization
(DPO) strategies. Both methods ignore the exploration of possible positive
unobserved samples, which is commonly referred to as the exposure bias problem.
To mitigate this problem, this paper treats the GR as a multi-step generation
task and constructs a GFlowNets-based fine-tuning framework (GFlowGR). The
proposed framework integrates collaborative knowledge from traditional
recommender systems to create an adaptive trajectory sampler and a
comprehensive reward model. Leveraging the diverse generation property of
GFlowNets, along with sampling and heuristic weighting techniques, GFlowGR
emerges as a promising approach to mitigate the exposure bias problem.
Extensive empirical results on two real-world datasets and with two different
GR backbones highlight the effectiveness and robustness of GFlowGR.

</details>


### [567] [A Simple Contrastive Framework Of Item Tokenization For Generative Recommendation](https://arxiv.org/abs/2506.16683)
*Penglong Zhai,Yifang Yuan,Fanyi Di,Jie Li,Yue Liu,Chen Li,Jie Huang,Sicong Wang,Yao Xu,Xin Li*

Main category: cs.IR

TL;DR: 论文提出了一种基于对比学习的无监督深度量化方法SimCIT，用于解决生成式检索推荐中的冗余和大规模标记空间问题。


<details>
  <summary>Details</summary>
Motivation: 生成式检索推荐在大规模系统中面临冗余和标记空间庞大的问题，现有方法如RQ-VAE的重建策略与任务目标存在冲突，且多模态信息的整合仍具挑战性。

Method: 提出SimCIT框架，通过可学习的残差量化模块结合多模态知识对齐和语义标记化，利用对比学习实现优化。

Result: 在公开数据集和工业数据集上的实验验证了SimCIT在基于LLM的生成推荐中的有效性。

Conclusion: SimCIT通过对比学习解决了现有方法的局限性，为生成式推荐提供了更高效的解决方案。

Abstract: Generative retrieval-based recommendation has emerged as a promising paradigm
aiming at directly generating the identifiers of the target candidates.
However, in large-scale recommendation systems, this approach becomes
increasingly cumbersome due to the redundancy and sheer scale of the token
space. To overcome these limitations, recent research has explored the use of
semantic tokens as an alternative to ID tokens, which typically leveraged
reconstruction-based strategies, like RQ-VAE, to quantize content embeddings
and significantly reduce the embedding size. However, reconstructive
quantization aims for the precise reconstruction of each item embedding
independently, which conflicts with the goal of generative retrieval tasks
focusing more on differentiating among items. Moreover, multi-modal side
information of items, such as descriptive text and images, geographical
knowledge in location-based recommendation services, has been shown to be
effective in improving recommendations by providing richer contexts for
interactions. Nevertheless, effectively integrating such complementary
knowledge into existing generative recommendation frameworks remains
challenging. To overcome these challenges, we propose a novel unsupervised deep
quantization exclusively based on contrastive learning, named SimCIT (a Simple
Contrastive Item Tokenization framework). Specifically, different from existing
reconstruction-based strategies, SimCIT propose to use a learnable residual
quantization module to align with the signals from different modalities of the
items, which combines multi-modal knowledge alignment and semantic tokenization
in a mutually beneficial contrastive learning framework. Extensive experiments
across public datasets and a large-scale industrial dataset from various
domains demonstrate SimCIT's effectiveness in LLM-based generative
recommendation.

</details>


<div id='math.NA'></div>

# math.NA [[Back]](#toc)

### [568] [Convergent Methods for Koopman Operators on Reproducing Kernel Hilbert Spaces](https://arxiv.org/abs/2506.15782)
*Nicolas Boullé,Matthew J. Colbrook,Gustav Conradie*

Main category: math.NA

TL;DR: 该论文提出了一种基于RKHS的数据驱动算法，用于计算Koopman和Perron--Frobenius算子的谱性质，具有收敛性和高效性，适用于高维数据。


<details>
  <summary>Details</summary>
Motivation: 研究Koopman算子在RKHS上的应用，以解决传统L^2空间中的计算限制，如点预测误差控制、谱性质改进和高维数据处理的效率问题。

Method: 引入基于RKHS的通用数据驱动算法，计算算子的谱和伪谱，利用核函数结构避免大数据限制，并通过Solvability Complexity Index证明算法最优性。

Result: 算法在多个高维实际数据集（如湍流、分子动力学、海冰和海面高度）中表现出高效性和准确性，并公开了软件包SpecRKHS。

Conclusion: 提出的算法在RKHS上实现了Koopman算子的高效谱分析，解决了传统方法的局限性，适用于复杂动态系统的研究。

Abstract: Data-driven spectral analysis of Koopman operators is a powerful tool for
understanding numerous real-world dynamical systems, from neuronal activity to
variations in sea surface temperature. The Koopman operator acts on a function
space and is most commonly studied on the space of square-integrable functions.
However, defining it on a suitable reproducing kernel Hilbert space (RKHS)
offers numerous practical advantages, including pointwise predictions with
error bounds, improved spectral properties that facilitate computations, and
more efficient algorithms, particularly in high dimensions. We introduce the
first general, provably convergent, data-driven algorithms for computing
spectral properties of Koopman and Perron--Frobenius operators on RKHSs. These
methods efficiently compute spectra and pseudospectra with error control and
spectral measures while exploiting the RKHS structure to avoid the large-data
limits required in the $L^2$ settings. The function space is determined by a
user-specified kernel, eliminating the need for quadrature-based sampling as in
$L^2$ and enabling greater flexibility with finite, externally provided
datasets. Using the Solvability Complexity Index hierarchy, we construct
adversarial dynamical systems for these problems to show that no algorithm can
succeed in fewer limits, thereby proving the optimality of our algorithms.
Notably, this impossibility extends to randomized algorithms and datasets. We
demonstrate the effectiveness of our algorithms on challenging,
high-dimensional datasets arising from real-world measurements and
high-fidelity numerical simulations, including turbulent channel flow,
molecular dynamics of a binding protein, Antarctic sea ice concentration, and
Northern Hemisphere sea surface height. The algorithms are publicly available
in the software package $\texttt{SpecRKHS}$.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [569] [Client Selection Strategies for Federated Semantic Communications in Heterogeneous IoT Networks](https://arxiv.org/abs/2506.17063)
*Samer Lahoud,Kinda Khawam*

Main category: cs.NI

TL;DR: 本文提出了一种新颖的联邦语义通信框架，用于异构物联网设备间协作训练带宽高效模型，显著降低通信开销并保持重建质量。


<details>
  <summary>Details</summary>
Motivation: 物联网设备激增对带宽受限的无线网络提出高效数据传输和隐私保护的挑战。

Method: 采用联邦语义通信框架，结合三种客户端选择策略和语义瓶颈的端到端架构，以及基于损失的聚合机制。

Result: 实验表明，功利选择策略重建质量最高，而比例公平策略在减少参与不平等和提高计算效率方面表现优异。

Conclusion: 联邦语义通信能在异构物联网部署中平衡重建质量、资源效率和公平性，为可持续且隐私保护的边缘智能应用铺平道路。

Abstract: The exponential growth of IoT devices presents critical challenges in
bandwidth-constrained wireless networks, particularly regarding efficient data
transmission and privacy preservation. This paper presents a novel federated
semantic communication (SC) framework that enables collaborative training of
bandwidth-efficient models for image reconstruction across heterogeneous IoT
devices. By leveraging SC principles to transmit only semantic features, our
approach dramatically reduces communication overhead while preserving
reconstruction quality. We address the fundamental challenge of client
selection in federated learning environments where devices exhibit significant
disparities in dataset sizes and data distributions. Our framework implements
three distinct client selection strategies that explore different trade-offs
between system performance and fairness in resource allocation. The system
employs an end-to-end SC architecture with semantic bottlenecks, coupled with a
loss-based aggregation mechanism that naturally adapts to client heterogeneity.
Experimental evaluation on image data demonstrates that while Utilitarian
selection achieves the highest reconstruction quality, Proportional Fairness
maintains competitive performance while significantly reducing participation
inequality and improving computational efficiency. These results establish that
federated SC can successfully balance reconstruction quality, resource
efficiency, and fairness in heterogeneous IoT deployments, paving the way for
sustainable and privacy-preserving edge intelligence applications.

</details>


<div id='cond-mat.soft'></div>

# cond-mat.soft [[Back]](#toc)

### [570] [Learning to flock in open space by avoiding collisions and staying together](https://arxiv.org/abs/2506.15587)
*Martino Brambati,Antonio Celani,Marco Gherardi,Francesco Ginelli*

Main category: cond-mat.soft

TL;DR: 研究通过多智能体强化学习框架，在开放无界空间中探索凝聚性群体行为的涌现。智能体通过学习平衡对齐和吸引相互作用，实现类似Vicsek模型的动力学行为。


<details>
  <summary>Details</summary>
Motivation: 探索在开放无界空间中，智能体如何通过局部信息交互，自发形成凝聚性群体行为，并避免碰撞。

Method: 使用多智能体强化学习框架，智能体整合最近邻的位置和方向信息，优化局部成本函数以平衡对齐和吸引相互作用。

Result: 实现了高极序的凝聚性群体运动，最优策略在近距离以强对齐为主，远距离则灵活结合对齐和吸引。

Conclusion: 群体行为可能是智能体为适应生物需求（如保持聚集和避免碰撞）的自适应响应，与椋鸟群体的实验观察一致。

Abstract: We investigate the emergence of cohesive flocking in open, boundless space
using a multi-agent reinforcement learning framework. Agents integrate
positional and orientational information from their closest topological
neighbours and learn to balance alignment and attractive interactions by
optimizing a local cost function that penalizes both excessive separation and
close-range crowding. The resulting Vicsek-like dynamics is robust to
algorithmic implementation details and yields cohesive collective motion with
high polar order. The optimal policy is dominated by strong aligning
interactions when agents are sufficiently close to their neighbours, and a
flexible combination of alignment and attraction at larger separations. We
further characterize the internal structure and dynamics of the resulting
groups using liquid-state metrics and neighbour exchange rates, finding
qualitative agreement with empirical observations in starling flocks. These
results suggest that flocking may emerge in groups of moving agents as an
adaptive response to the biological imperatives of staying together while
avoiding collisions.

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [571] [From Prompts to Constructs: A Dual-Validity Framework for LLM Research in Psychology](https://arxiv.org/abs/2506.16697)
*Zhicheng Lin*

Main category: cs.CY

TL;DR: 论文探讨了在心理学研究中应用大型语言模型（LLMs）时可能产生的测量假象问题，提出了一个双效度框架，强调需要结合可靠测量原则和因果推断标准。


<details>
  <summary>Details</summary>
Motivation: 当前心理学研究中，LLMs被广泛用作工具或模型，但直接应用人类测量工具可能导致矛盾结果，甚至产生虚假现象。

Method: 提出了一个双效度框架，根据科学目标的不同，明确所需的证据强度，并区分不同研究目的（如测量、模拟、建模）的验证策略。

Result: 当前实践往往未能满足这些要求，常将统计模式匹配误认为心理现象的证据。

Conclusion: 未来需开发心理构念的计算模拟，并建立清晰、可扩展的证据标准，而非盲目应用人类测量工具。

Abstract: Large language models (LLMs) are rapidly being adopted across psychology,
serving as research tools, experimental subjects, human simulators, and
computational models of cognition. However, the application of human
measurement tools to these systems can produce contradictory results, raising
concerns that many findings are measurement phantoms--statistical artifacts
rather than genuine psychological phenomena. In this Perspective, we argue that
building a robust science of AI psychology requires integrating two of our
field's foundational pillars: the principles of reliable measurement and the
standards for sound causal inference. We present a dual-validity framework to
guide this integration, which clarifies how the evidence needed to support a
claim scales with its scientific ambition. Using an LLM to classify text may
require only basic accuracy checks, whereas claiming it can simulate anxiety
demands a far more rigorous validation process. Current practice systematically
fails to meet these requirements, often treating statistical pattern matching
as evidence of psychological phenomena. The same model output--endorsing "I am
anxious"--requires different validation strategies depending on whether
researchers claim to measure, characterize, simulate, or model psychological
constructs. Moving forward requires developing computational analogues of
psychological constructs and establishing clear, scalable standards of evidence
rather than the uncritical application of human measurement tools.

</details>


### [572] [Large Language Models as Psychological Simulators: A Methodological Guide](https://arxiv.org/abs/2506.16702)
*Zhicheng Lin*

Main category: cs.CY

TL;DR: 本文提供了一个框架，指导如何将大语言模型（LLMs）用作心理学模拟器，涵盖角色模拟和认知建模两大应用，并探讨了相关挑战和伦理问题。


<details>
  <summary>Details</summary>
Motivation: 缺乏关于如何利用LLMs进行心理学和行为研究的方法论指导，本文旨在填补这一空白。

Method: 提出两种主要应用方法：1）模拟角色和人物以探索多样情境；2）作为计算模型研究认知过程。包括验证策略、因果干预方法等。

Result: 整合了关于LLMs性能的实证证据（如系统性偏见、文化局限性等），帮助研究者应对挑战并利用其独特能力。

Conclusion: 强调需透明化模型能力和限制，为心理学研究提供实用框架。

Abstract: Large language models (LLMs) offer emerging opportunities for psychological
and behavioral research, but methodological guidance is lacking. This article
provides a framework for using LLMs as psychological simulators across two
primary applications: simulating roles and personas to explore diverse
contexts, and serving as computational models to investigate cognitive
processes. For simulation, we present methods for developing psychologically
grounded personas that move beyond demographic categories, with strategies for
validation against human data and use cases ranging from studying inaccessible
populations to prototyping research instruments. For cognitive modeling, we
synthesize emerging approaches for probing internal representations,
methodological advances in causal interventions, and strategies for relating
model behavior to human cognition. We address overarching challenges including
prompt sensitivity, temporal limitations from training data cutoffs, and
ethical considerations that extend beyond traditional human subjects review.
Throughout, we emphasize the need for transparency about model capabilities and
constraints. Together, this framework integrates emerging empirical evidence
about LLM performance--including systematic biases, cultural limitations, and
prompt brittleness--to help researchers wrangle these challenges and leverage
the unique capabilities of LLMs in psychological research.

</details>


### [573] [TrajSceneLLM: A Multimodal Perspective on Semantic GPS Trajectory Analysis](https://arxiv.org/abs/2506.16401)
*Chunhou Ji,Qiumeng Li*

Main category: cs.CY

TL;DR: TrajSceneLLM提出了一种多模态框架，结合地图图像和LLM生成的文本描述，增强GPS轨迹的语义理解，显著提升了旅行模式识别的性能。


<details>
  <summary>Details</summary>
Motivation: 传统方法难以提取GPS轨迹的深层语义表示和结合地图上下文信息，需要更高效的方法。

Method: 通过整合地图图像和LLM生成的文本描述，生成多模态嵌入，并结合简单MLP分类器。

Result: 实验表明，该方法在旅行模式识别任务中性能显著提升，减少了对手工特征的依赖。

Conclusion: TrajSceneLLM展示了在空间人工智能中捕捉深层时空依赖的潜力，为下游应用提供了新方向。

Abstract: GPS trajectory data reveals valuable patterns of human mobility and urban
dynamics, supporting a variety of spatial applications. However, traditional
methods often struggle to extract deep semantic representations and incorporate
contextual map information. We propose TrajSceneLLM, a multimodal perspective
for enhancing semantic understanding of GPS trajectories. The framework
integrates visualized map images (encoding spatial context) and textual
descriptions generated through LLM reasoning (capturing temporal sequences and
movement dynamics). Separate embeddings are generated for each modality and
then concatenated to produce trajectory scene embeddings with rich semantic
content which are further paired with a simple MLP classifier. We validate the
proposed framework on Travel Mode Identification (TMI), a critical task for
analyzing travel choices and understanding mobility behavior. Our experiments
show that these embeddings achieve significant performance improvement,
highlighting the advantage of our LLM-driven method in capturing deep
spatio-temporal dependencies and reducing reliance on handcrafted features.
This semantic enhancement promises significant potential for diverse downstream
applications and future research in geospatial artificial intelligence. The
source code and dataset are publicly available at:
https://github.com/februarysea/TrajSceneLLM.

</details>


### [574] [LLM-Based Bot Broadens the Range of Arguments in Online Discussions, Even When Transparently Disclosed as AI](https://arxiv.org/abs/2506.17073)
*Valeria Vuk,Cristina Sarasua,Fabrizio Gilardi*

Main category: cs.CY

TL;DR: 研究探讨了基于LLM的机器人是否能扩大在线政治讨论的视角范围，通过实验证明其能显著增加讨论的多样性，且AI身份披露不影响效果。


<details>
  <summary>Details</summary>
Motivation: 在线政治讨论常因自我选择和同质化交流导致观点单一，影响民主参与的广泛性。

Method: 通过两项预注册随机实验，使用LLM机器人监控讨论、识别并引入缺失观点。

Result: 机器人显著扩大了讨论的视角范围，且AI身份披露未显著影响效果。

Conclusion: LLM为基础的调节工具可积极影响在线政治讨论。

Abstract: A wide range of participation is essential for democracy, as it helps prevent
the dominance of extreme views, erosion of legitimacy, and political
polarization. However, engagement in online political discussions often
features a limited spectrum of views due to high levels of self-selection and
the tendency of online platforms to facilitate exchanges primarily among
like-minded individuals. This study examines whether an LLM-based bot can widen
the scope of perspectives expressed by participants in online discussions
through two pre-registered randomized experiments conducted in a chatroom. We
evaluate the impact of a bot that actively monitors discussions, identifies
missing arguments, and introduces them into the conversation. The results
indicate that our bot significantly expands the range of arguments, as measured
by both objective and subjective metrics. Furthermore, disclosure of the bot as
AI does not significantly alter these effects. These findings suggest that
LLM-based moderation tools can positively influence online political discourse.

</details>


<div id='cs.NE'></div>

# cs.NE [[Back]](#toc)

### [575] [A Study of Hybrid and Evolutionary Metaheuristics for Single Hidden Layer Feedforward Neural Network Architecture](https://arxiv.org/abs/2506.15737)
*Gautam Siddharth Kashyap,Md Tabrez Nafis,Samar Wazir*

Main category: cs.NE

TL;DR: 论文提出了一种混合PSO-SGD方法，用于替代传统的SGD训练ANN，显著降低了训练误差。


<details>
  <summary>Details</summary>
Motivation: SGD训练ANN存在计算成本高和易陷入局部最优的问题，因此研究PSO和GA等元启发式优化器作为替代方案。

Method: 开发了一种混合PSO-SGD策略，结合了PSO的全局搜索能力和SGD的局部搜索效率。

Result: 混合PSO-SGD方法将训练MSE中值降低了90-95%，优于传统GA和PSO。RMHC也显著降低了MSE，而RS表现较差。

Conclusion: 混合和进化方法显著提升了训练效率和准确性，支持了Building Block Hypothesis的有效性。

Abstract: Training Artificial Neural Networks (ANNs) with Stochastic Gradient Descent
(SGD) frequently encounters difficulties, including substantial computing
expense and the risk of converging to local optima, attributable to its
dependence on partial weight gradients. Therefore, this work investigates
Particle Swarm Optimization (PSO) and Genetic Algorithms (GAs) - two
population-based Metaheuristic Optimizers (MHOs) - as alternatives to SGD to
mitigate these constraints. A hybrid PSO-SGD strategy is developed to improve
local search efficiency. The findings indicate that the hybrid PSO-SGD
technique decreases the median training MSE by 90 to 95 percent relative to
conventional GA and PSO across various network sizes (e.g., from around 0.02 to
approximately 0.001 in the Sphere function). RMHC attains substantial
enhancements, reducing MSE by roughly 85 to 90 percent compared to GA.
Simultaneously, RS consistently exhibits errors exceeding 0.3, signifying
subpar performance. These findings underscore that hybrid and evolutionary
procedures significantly improve training efficiency and accuracy compared to
conventional optimization methods and imply that the Building Block Hypothesis
(BBH) may still be valid, indicating that advantageous weight structures are
retained during evolutionary search.

</details>


### [576] [Robust Dynamic Material Handling via Adaptive Constrained Evolutionary Reinforcement Learning](https://arxiv.org/abs/2506.16795)
*Chengpeng Hu,Ziming Wang,Bo Yuan,Jialin Liu,Chengqi Zhang,Xin Yao*

Main category: cs.NE

TL;DR: 本文提出了一种自适应约束进化强化学习（ACERL）方法，用于动态物料搬运（DMH）问题，通过多样化的探索和自适应选择训练实例，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 动态物料搬运问题中，实时分配任务以满足约束（如任务延迟）并最小化完成时间和延迟具有挑战性，且奖励稀疏。历史任务记录可用于训练决策策略，但需高效利用计算资源和历史数据。

Method: 提出ACERL方法，维护一组演员进行多样化探索，处理稀疏奖励和约束违反，并自适应选择最有益的训练实例。

Result: 在8个训练和8个未见测试实例上，ACERL表现优于现有算法，且能完全满足约束。在40个噪声实例上的实验进一步验证了其鲁棒性。

Conclusion: ACERL通过协调多样化探索和自适应训练实例选择，有效解决了DMH问题，并在性能和鲁棒性上表现优异。

Abstract: Dynamic material handling (DMH) involves the assignment of dynamically
arriving material transporting tasks to suitable vehicles in real time for
minimising makespan and tardiness. In real-world scenarios, historical task
records are usually available, which enables the training of a decision policy
on multiple instances consisting of historical records. Recently, reinforcement
learning has been applied to solve DMH. Due to the occurrence of dynamic events
such as new tasks, adaptability is highly required. Solving DMH is challenging
since constraints including task delay should be satisfied. A feedback is
received only when all tasks are served, which leads to sparse reward. Besides,
making the best use of limited computational resources and historical records
for training a robust policy is crucial. The time allocated to different
problem instances would highly impact the learning process. To tackle those
challenges, this paper proposes a novel adaptive constrained evolutionary
reinforcement learning (ACERL) approach, which maintains a population of actors
for diverse exploration. ACERL accesses each actor for tackling sparse rewards
and constraint violation to restrict the behaviour of the policy. Moreover,
ACERL adaptively selects the most beneficial training instances for improving
the policy. Extensive experiments on eight training and eight unseen test
instances demonstrate the outstanding performance of ACERL compared with
several state-of-the-art algorithms. Policies trained by ACERL can schedule the
vehicles while fully satisfying the constraints. Additional experiments on 40
unseen noised instances show the robust performance of ACERL. Cross-validation
further presents the overall effectiveness of ACREL. Besides, a rigorous
ablation study highlights the coordination and benefits of each ingredient of
ACERL.

</details>


### [577] [Continual Learning with Columnar Spiking Neural Networks](https://arxiv.org/abs/2506.17169)
*Denis Larionov,Nikolay Bazenkov,Mikhail Kiselev*

Main category: cs.NE

TL;DR: 研究探讨了柱状组织的脉冲神经网络（SNN）在持续学习和灾难性遗忘中的表现，通过CoLaNET展示了微柱在新任务中的高效适应能力。


<details>
  <summary>Details</summary>
Motivation: 解决持续学习中的灾难性遗忘问题，探索SNN的稳定性和可塑性平衡。

Method: 使用CoLaNET（柱状分层网络），分析微柱在新任务中的适应效率及超参数对稳定性和可塑性的影响。

Result: 最优配置在十个MNIST任务中保持92%的准确率，首次任务仅4%性能下降。

Conclusion: CoLaNET在平衡稳定性和可塑性方面表现优异，适用于持续学习任务。

Abstract: This study investigates columnar-organized spiking neural networks (SNNs) for
continual learning and catastrophic forgetting. Using CoLaNET (Columnar Layered
Network), we show that microcolumns adapt most efficiently to new tasks when
they lack shared structure with prior learning. We demonstrate how CoLaNET
hyperparameters govern the trade-off between retaining old knowledge
(stability) and acquiring new information (plasticity). Our optimal
configuration learns ten sequential MNIST tasks effectively, maintaining 92%
accuracy on each. It shows low forgetting, with only 4% performance degradation
on the first task after training on nine subsequent tasks.

</details>


<div id='hep-th'></div>

# hep-th [[Back]](#toc)

### [578] [Approximate Ricci-flat Metrics for Calabi-Yau Manifolds](https://arxiv.org/abs/2506.15766)
*Seung-Joo Lee,Andre Lukas*

Main category: hep-th

TL;DR: 提出了一种通过机器学习技术计算Ricci-flat Kähler势的方法，并将其拟合到Donaldson的Ansatz中，应用于Dwork家族和双三次CY超曲面。


<details>
  <summary>Details</summary>
Motivation: 研究如何在Calabi-Yau流形上确定解析Kähler势及其近似Ricci-flat Kähler度量。

Method: 利用机器学习技术数值计算Ricci-flat Kähler势，并将结果拟合到Donaldson的Ansatz中。

Result: 在Dwork家族和双三次CY超曲面中，得到了简单的解析表达式，且Kähler势仅依赖于复结构参数的模。

Conclusion: 该方法成功生成了近似Ricci-flat Kähler势的解析表达式，揭示了其与复结构参数模的依赖关系。

Abstract: We outline a method to determine analytic K\"ahler potentials with associated
approximately Ricci-flat K\"ahler metrics on Calabi-Yau manifolds. Key
ingredients are numerically calculating Ricci-flat K\"ahler potentials via
machine learning techniques and fitting the numerical results to Donaldson's
Ansatz. We apply this method to the Dwork family of quintic hypersurfaces in
$\mathbb{P}^4$ and an analogous one-parameter family of bi-cubic CY
hypersurfaces in $\mathbb{P}^2\times\mathbb{P}^2$. In each case, a relatively
simple analytic expression is obtained for the approximately Ricci-flat
K\"ahler potentials, including the explicit dependence on the complex structure
parameter. We find that these K\"ahler potentials only depend on the modulus of
the complex structure parameter.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [579] [TrainVerify: Equivalence-Based Verification for Distributed LLM Training](https://arxiv.org/abs/2506.15961)
*Yunchi Lu,Youshan Miao,Cheng Tan,Peng Huang,Yi Zhu,Xian Zhang,Fan Yang*

Main category: cs.DC

TL;DR: TrainVerify是一个用于验证大规模语言模型分布式训练的系统，确保并行执行计划与逻辑规范一致，解决了传统验证方法难以应对的规模问题。


<details>
  <summary>Details</summary>
Motivation: 大规模语言模型的分布式训练成本高昂且缺乏验证，容易产生错误并浪费资源。

Method: TrainVerify通过形状缩减技术和分阶段并行验证算法，显著降低验证复杂度，同时保持形式正确性。

Result: 成功验证了包括Llama3（405B）和DeepSeek-V3（671B）在内的前沿大规模语言模型的训练计划。

Conclusion: TrainVerify为大规模语言模型的分布式训练提供了高效且可靠的验证方法。

Abstract: Training large language models (LLMs) at scale requires parallel execution
across thousands of devices, incurring enormous computational costs. Yet, these
costly distributed trainings are rarely verified, leaving them prone to silent
errors and potentially wasting millions of GPU hours. We introduce TrainVerify,
a system for verifiable distributed training of LLMs. Given a deep learning
model's logical specification as the ground truth, TrainVerify formally
verifies that a distributed parallel execution plan is mathematically
equivalent to it. Direct verification is notoriously difficult due to the sheer
scale of LLMs which often involves billions of variables and highly intricate
computation graphs. Therefore, TrainVerify introduces shape-reduction
techniques and a stage-wise parallel verification algorithm that significantly
reduces complexity while preserving formal correctness. TrainVerify scales to
frontier LLMs, including the successful verification of the Llama3 (405B) and
DeepSeek-V3 (671B) training plans.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [580] [SemAgent: A Semantics Aware Program Repair Agent](https://arxiv.org/abs/2506.16650)
*Anvith Pabba,Alex Mathai,Anindya Chakraborty,Baishakhi Ray*

Main category: cs.SE

TL;DR: 论文提出SemAgent，一种基于工作流的方法，通过结合问题、代码和执行语义生成更完整的补丁，显著提高了自动程序修复（APR）的效果。


<details>
  <summary>Details</summary>
Motivation: 现有系统在解决代码问题时倾向于局部修复，缺乏对问题、代码和执行语义的深入理解，导致补丁过拟合。

Method: SemAgent通过四步流程：（a）利用执行语义获取上下文，（b）通过抽象理解问题语义，（c）在抽象上下文中隔离代码语义，（d）采用两阶段架构（修复和审查）生成补丁。

Result: 在SWEBench-Lite基准测试中，解决率达到44.66%，优于其他工作流方法，且对多行推理和边缘情况处理表现突出。

Conclusion: 结合语义理解的APR流程能生成更鲁棒和语义一致的修复。

Abstract: Large Language Models (LLMs) have shown impressive capabilities in downstream
software engineering tasks such as Automated Program Repair (APR). In
particular, there has been a lot of research on repository-level
issue-resolution benchmarks such as SWE-Bench. Although there has been
significant progress on this topic, we notice that in the process of solving
such issues, existing agentic systems tend to hyper-localize on immediately
suspicious lines of code and fix them in isolation, without a deeper
understanding of the issue semantics, code semantics, or execution semantics.
Consequently, many existing systems generate patches that overfit to the user
issue, even when a more general fix is preferable. To address this limitation,
we introduce SemAgent, a novel workflow-based procedure that leverages issue,
code, and execution semantics to generate patches that are complete -
identifying and fixing all lines relevant to the issue. We achieve this through
a novel pipeline that (a) leverages execution semantics to retrieve relevant
context, (b) comprehends issue-semantics via generalized abstraction, (c)
isolates code-semantics within the context of this abstraction, and (d)
leverages this understanding in a two-stage architecture: a repair stage that
proposes fine-grained fixes, followed by a reviewer stage that filters relevant
fixes based on the inferred issue-semantics. Our evaluations show that our
methodology achieves a solve rate of 44.66% on the SWEBench-Lite benchmark
beating all other workflow-based approaches, and an absolute improvement of
7.66% compared to our baseline, which lacks such deep semantic understanding.
We note that our approach performs particularly well on issues requiring
multi-line reasoning (and editing) and edge-case handling, suggesting that
incorporating issue and code semantics into APR pipelines can lead to robust
and semantically consistent repairs.

</details>


### [581] [cAST: Enhancing Code Retrieval-Augmented Generation with Structural Chunking via Abstract Syntax Tree](https://arxiv.org/abs/2506.15655)
*Yilin Zhang,Xinran Zhao,Zora Zhiruo Wang,Chenyang Yang,Jiayi Wei,Tongshuang Wu*

Main category: cs.SE

TL;DR: 论文提出了一种基于抽象语法树（AST）的代码分块方法（\ourwork），以解决现有行基分块方法破坏语义结构的问题，显著提升了代码生成任务的表现。


<details>
  <summary>Details</summary>
Motivation: 现有基于行的分块方法在代码检索增强生成（RAG）中常破坏语义结构（如拆分函数或合并无关代码），影响生成质量，因此需要一种结构感知的分块方法。

Method: 提出了一种基于AST的分块方法，递归地将大型AST节点分解为小块，并在大小限制内合并兄弟节点，生成语义连贯的自包含单元。

Result: 该方法在多种代码生成任务中表现优异，如在RepoEval检索中Recall@5提升4.3点，在SWE-bench生成中Pass@1提升2.67点。

Conclusion: 结构感知的分块方法对扩展检索增强的代码智能至关重要。

Abstract: Retrieval-Augmented Generation (RAG) has become essential for large-scale
code generation, grounding predictions in external code corpora to improve
actuality. However, a critical yet underexplored aspect of RAG pipelines is
chunking -- the process of dividing documents into retrievable units. Existing
line-based chunking heuristics often break semantic structures, splitting
functions or merging unrelated code, which can degrade generation quality. We
propose chunking via Abstract Syntax Trees (\ourwork), a structure-aware method
that recursively breaks large AST nodes into smaller chunks and merges sibling
nodes while respecting size limits. This approach generates self-contained,
semantically coherent units across programming languages and tasks, improving
performance on diverse code generation tasks, e.g., boosting Recall@5 by 4.3
points on RepoEval retrieval and Pass@1 by 2.67 points on SWE-bench generation.
Our work highlights the importance of structure-aware chunking for scaling
retrieval-enhanced code intelligence.

</details>


### [582] [Dissecting the SWE-Bench Leaderboards: Profiling Submitters and Architectures of LLM- and Agent-Based Repair Systems](https://arxiv.org/abs/2506.17208)
*Matias Martinez,Xavier Franch*

Main category: cs.SE

TL;DR: 本文对SWE-Bench Lite和Verified排行榜上的所有提交进行了首次全面研究，分析了67种独特方法，揭示了专有LLM（如Claude 3.5/3.7）的主导地位、代理与非代理设计的存在，以及从个人开发者到大型科技公司的贡献者群体。


<details>
  <summary>Details</summary>
Motivation: 由于SWE-Bench提交过程缺乏详细文档，许多解决方案的架构设计和来源不明确，因此需要对其进行全面研究以揭示现状。

Method: 分析了SWE-Bench Lite（68项）和Verified（79项）排行榜上的所有提交，从提交者类型、产品可用性、LLM使用和系统架构等维度研究了67种独特方法。

Result: 研究发现专有LLM（尤其是Claude 3.5/3.7）占主导地位，存在代理和非代理设计，贡献者群体包括个人开发者和大型科技公司。

Conclusion: 本研究为SWE-Bench的现状提供了全面视角，揭示了当前APR领域的技术趋势和参与者多样性。

Abstract: The rapid progress in Automated Program Repair (APR) has been driven by
advances in AI, particularly large language models (LLMs) and agent-based
systems. SWE-Bench is a recent benchmark designed to evaluate LLM-based repair
systems using real issues and pull requests mined from 12 popular open-source
Python repositories. Its public leaderboards, SWE-Bench Lite and SWE-Bench
Verified, have become central platforms for tracking progress and comparing
solutions. However, because the submission process does not require detailed
documentation, the architectural design and origin of many solutions remain
unclear. In this paper, we present the first comprehensive study of all
submissions to the SWE-Bench Lite (68 entries) and Verified (79 entries)
leaderboards, analyzing 67 unique approaches across dimensions such as
submitter type, product availability, LLM usage, and system architecture. Our
findings reveal the dominance of proprietary LLMs (especially Claude 3.5/3.7),
the presence of both agentic and non-agentic designs, and a contributor base
spanning from individual developers to large tech companies.

</details>


### [583] [AI-Driven Tools in Modern Software Quality Assurance: An Assessment of Benefits, Challenges, and Future Directions](https://arxiv.org/abs/2506.16586)
*Ihor Pysmennyi,Roman Kyslyi,Kyrylo Kleshch*

Main category: cs.SE

TL;DR: 研究探讨了将AI工具整合到现代分布式软件质量保证（QA）中的潜力与挑战，通过实验验证了其有效性，但也指出了语义覆盖、黑盒特性等实际应用中的问题。


<details>
  <summary>Details</summary>
Motivation: 传统QA方法难以应对现代软件的复杂性、规模和快速迭代，资源有限导致质量成本高昂，因此研究探索AI工具的整合。

Method: 综合分析了AI工具在验证和验证过程中的应用，包括测试生成、优化等，并通过企业应用的端到端回归实验验证。

Result: 实验结果显示生成测试用例的稳定性较高（仅8.3%的波动），但AI工具存在语义覆盖不足、黑盒特性等问题。

Conclusion: AI在QA中具有变革潜力，但需战略性地实施，并开发验证方法以克服现有局限性。

Abstract: Traditional quality assurance (QA) methods face significant challenges in
addressing the complexity, scale, and rapid iteration cycles of modern software
systems and are strained by limited resources available, leading to substantial
costs associated with poor quality. The object of this research is the Quality
Assurance processes for modern distributed software applications. The subject
of the research is the assessment of the benefits, challenges, and prospects of
integrating modern AI-oriented tools into quality assurance processes. We
performed comprehensive analysis of implications on both verification and
validation processes covering exploratory test analyses, equivalence
partitioning and boundary analyses, metamorphic testing, finding
inconsistencies in acceptance criteria (AC), static analyses, test case
generation, unit test generation, test suit optimization and assessment, end to
end scenario execution. End to end regression of sample enterprise application
utilizing AI-agents over generated test scenarios was implemented as a proof of
concept highlighting practical use of the study. The results, with only 8.3%
flaky executions of generated test cases, indicate significant potential for
the proposed approaches. However, the study also identified substantial
challenges for practical adoption concerning generation of semantically
identical coverage, "black box" nature and lack of explainability from
state-of-the-art Large Language Models (LLMs), the tendency to correct mutated
test cases to match expected results, underscoring the necessity for thorough
verification of both generated artifacts and test execution results. The
research demonstrates AI's transformative potential for QA but highlights the
importance of a strategic approach to implementing these technologies,
considering the identified limitations and the need for developing appropriate
verification methodologies.

</details>


### [584] [LLMs in Coding and their Impact on the Commercial Software Engineering Landscape](https://arxiv.org/abs/2506.16653)
*Vladislav Belozerov,Peter J Barclay,Askhan Sami*

Main category: cs.SE

TL;DR: 论文讨论了大型语言模型编码工具在软件工程中的主流化及其带来的隐私泄露、安全漏洞和模型迎合性等新风险，提出了企业应采取的措施以确保安全性和准确性。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型编码工具的普及，其在提升开发效率的同时也带来了隐私泄露、安全漏洞和模型迎合性（sycophancy）等新问题，亟需解决。

Method: 论文建议企业对每行AI生成的代码进行标记和审查，将提示和输出限制在私有或本地部署中，遵守新兴安全法规，并增加测试以检测迎合性回答。

Result: 研究发现10%的真实提示会泄露隐私数据，42%的生成代码片段隐藏安全漏洞，模型还可能表现出迎合性行为。

Conclusion: 企业需通过标记审查、私有部署、遵守法规和增加测试等措施，在提升开发速度的同时确保安全性和准确性。

Abstract: Large-language-model coding tools are now mainstream in software engineering.
But as these same tools move human effort up the development stack, they
present fresh dangers: 10% of real prompts leak private data, 42% of generated
snippets hide security flaws, and the models can even ``agree'' with wrong
ideas, a trait called sycophancy. We argue that firms must tag and review every
AI-generated line of code, keep prompts and outputs inside private or
on-premises deployments, obey emerging safety regulations, and add tests that
catch sycophantic answers -- so they can gain speed without losing security and
accuracy.

</details>


<div id='eess.AS'></div>

# eess.AS [[Back]](#toc)

### [585] [Spatio-spectral diarization of meetings by combining TDOA-based segmentation and speaker embedding-based clustering](https://arxiv.org/abs/2506.16228)
*Tobias Cord-Landwehr,Tobias Gburrek,Marc Deegen,Reinhold Haeb-Umbach*

Main category: eess.AS

TL;DR: 提出了一种结合模型与数据驱动的空间-频谱说话人日志系统，无需多通道训练数据或麦克风信息，适用于紧凑和分布式麦克风。


<details>
  <summary>Details</summary>
Motivation: 解决现有单通道方法在重叠语音和说话人移动时的性能不足问题。

Method: 结合TDOA分割和嵌入聚类，无需多通道训练数据或麦克风配置信息。

Result: 在紧凑和分布式麦克风场景下显著优于单通道方法，并能正确跟踪移动说话人。

Conclusion: 该方法在灵活性和性能上优于现有技术，适用于多种麦克风配置。

Abstract: We propose a spatio-spectral, combined model-based and data-driven
diarization pipeline consisting of TDOA-based segmentation followed by
embedding-based clustering. The proposed system requires neither access to
multi-channel training data nor prior knowledge about the number or placement
of microphones. It works for both a compact microphone array and distributed
microphones, with minor adjustments. Due to its superior handling of
overlapping speech during segmentation, the proposed pipeline significantly
outperforms the single-channel pyannote approach, both in a scenario with a
compact microphone array and in a setup with distributed microphones.
Additionally, we show that, unlike fully spatial diarization pipelines, the
proposed system can correctly track speakers when they change positions.

</details>


### [586] [EDNet: A Distortion-Agnostic Speech Enhancement Framework with Gating Mamba Mechanism and Phase Shift-Invariant Training](https://arxiv.org/abs/2506.16231)
*Doyeop Kwak,Youngjoon Jang,Seongyu Kim,Joon Son Chung*

Main category: eess.AS

TL;DR: EDNet是一种通用的语音增强框架，通过自适应结合掩蔽和映射方法，以及相位不变训练策略，有效处理多种失真类型。


<details>
  <summary>Details</summary>
Motivation: 现实环境中的语音信号常受多种失真影响，传统方法在特定场景外效果有限，需要一种更通用的解决方案。

Method: EDNet包含Gating Mamba模块（自适应选择掩蔽或重建）和Phase Shift-Invariant Training（提升相位估计）。

Result: 实验表明，EDNet在去噪、去混响、带宽扩展等任务中表现优异，适应性强。

Conclusion: EDNet是一种灵活且适应性强的语音增强框架，适用于多种失真场景。

Abstract: Speech signals in real-world environments are frequently affected by various
distortions such as additive noise, reverberation, and bandwidth limitation,
which may appear individually or in combination. Traditional speech enhancement
methods typically rely on either masking, which focuses on suppressing
non-speech components while preserving observable structure, or mapping, which
seeks to recover clean speech through direct transformation of the input. Each
approach offers strengths in specific scenarios but may be less effective
outside its target conditions. We propose the Erase and Draw Network (EDNet), a
distortion-agnostic speech enhancement framework designed to handle a broad
range of distortion types without prior assumptions about task or input
characteristics. EDNet consists of two main components: (1) the Gating Mamba
(GM) module, which adaptively combines masking and mapping through a learnable
gating mechanism that selects between suppression (Erase) and reconstruction
(Draw) based on local signal features, and (2) Phase Shift-Invariant Training
(PSIT), a shift tolerant supervision strategy that improves phase estimation by
enabling dynamic alignment during training while remaining compatible with
standard loss functions. Experimental results on denoising, dereverberation,
bandwidth extension, and multi distortion enhancement tasks show that EDNet
consistently achieves strong performance across conditions, demonstrating its
architectural flexibility and adaptability to diverse task settings.

</details>


### [587] [State-Space Models in Efficient Whispered and Multi-dialect Speech Recognition](https://arxiv.org/abs/2506.16969)
*Aref Farhadipour,Homayoon Beigi,Volker Dellwo,Hadi Veisi*

Main category: eess.AS

TL;DR: 提出了一种基于Mamba的状态空间模型和四种自监督模型（Wav2Vec2、WavLM、HuBERT、Whisper）的方法，用于解决耳语语音和方言多样性的双重挑战，并在wTIMIT和CHAINS数据集上取得了最佳性能。


<details>
  <summary>Details</summary>
Motivation: 耳语语音识别在方言变化下对传统语音识别系统提出了显著挑战，需要高效且低资源消耗的解决方案。

Method: 使用Mamba状态空间模型和四种自监督模型（Wav2Vec2、WavLM、HuBERT、Whisper），结合新加坡、美国和爱尔兰方言的耳语和正常语音数据进行训练。

Result: 在wTIMIT和CHAINS数据集上取得了最佳性能，证明了Mamba模型在低耳语数据量下高效处理耳语和正常语音识别的能力。

Conclusion: 提出的方法有效解决了耳语语音和方言多样性的挑战，代码已开源。

Abstract: Whispered speech recognition presents significant challenges for conventional
automatic speech recognition systems, particularly when combined with dialect
variation. However, utilizing an efficient method to solve this problem using a
low-range dataset and processing load is beneficial. This paper proposes a
solution using a Mamba-based state-space model and four fine-tuned
self-supervised models consisting of Wav2Vec2, WavLM, HuBERT, and Whisper to
address the dual challenges of whispered speech and dialect diversity. Based on
our knowledge, this represents the best performance reported on the wTIMIT and
CHAINS datasets for whispered speech recognition. We trained the models using
whispered and normal speech data across Singaporean, US, and Irish dialects.
The findings demonstrated that utilizing the proposed Mamba-based model could
work as a highly efficient model trained with low amounts of whispered data to
simultaneously work on whispered and normal speech recognition. The code for
this work is freely available.

</details>


### [588] [RapFlow-TTS: Rapid and High-Fidelity Text-to-Speech with Improved Consistency Flow Matching](https://arxiv.org/abs/2506.16741)
*Hyun Joon Park,Jeongmin Liu,Jin Sob Kim,Jeong Yeol Yang,Sung Won Han,Eunwoo Song*

Main category: eess.AS

TL;DR: RapFlow-TTS是一种快速高保真的TTS声学模型，通过流匹配（FM）训练中的速度一致性约束，减少生成步骤而不牺牲质量。


<details>
  <summary>Details</summary>
Motivation: 传统基于ODE的TTS生成需要大量步骤，导致质量与推理速度的权衡。RapFlow-TTS旨在解决这一问题。

Method: 利用FM训练中的速度一致性约束，结合时间间隔调度和对抗学习技术，优化少步合成质量。

Result: 实验显示，RapFlow-TTS在5倍和10倍减少合成步骤的情况下，仍能实现高保真语音合成。

Conclusion: RapFlow-TTS通过速度一致性约束和优化技术，显著提升了TTS的推理效率和质量。

Abstract: We introduce RapFlow-TTS, a rapid and high-fidelity TTS acoustic model that
leverages velocity consistency constraints in flow matching (FM) training.
Although ordinary differential equation (ODE)-based TTS generation achieves
natural-quality speech, it typically requires a large number of generation
steps, resulting in a trade-off between quality and inference speed. To address
this challenge, RapFlow-TTS enforces consistency in the velocity field along
the FM-straightened ODE trajectory, enabling consistent synthetic quality with
fewer generation steps. Additionally, we introduce techniques such as time
interval scheduling and adversarial learning to further enhance the quality of
the few-step synthesis. Experimental results show that RapFlow-TTS achieves
high-fidelity speech synthesis with a 5- and 10-fold reduction in synthesis
steps than the conventional FM- and score-based approaches, respectively.

</details>


<div id='cs.DS'></div>

# cs.DS [[Back]](#toc)

### [589] [Linearithmic Clean-up for Vector-Symbolic Key-Value Memory with Kroneker Rotation Products](https://arxiv.org/abs/2506.15793)
*Ruipeng Liu,Qinru Qiu,Simon Khan,Garrett E. Katz*

Main category: cs.DS

TL;DR: 提出了一种基于Kroneker乘积的旋转矩阵的VSA清理步骤新方法，显著降低了时间和空间复杂度。


<details>
  <summary>Details</summary>
Motivation: 当前VSA的清理步骤计算复杂度高，成为性能瓶颈。

Method: 使用Kroneker乘积的旋转矩阵表示codebook，实现高效清理。

Result: 清理时间复杂度和空间复杂度显著降低，实验验证了方法的可扩展性。

Conclusion: 新方法在保持内存容量的同时，大幅提升了VSA的清理效率。

Abstract: A computational bottleneck in current Vector-Symbolic Architectures (VSAs) is
the ``clean-up'' step, which decodes the noisy vectors retrieved from the
architecture. Clean-up typically compares noisy vectors against a ``codebook''
of prototype vectors, incurring computational complexity that is quadratic or
similar. We present a new codebook representation that supports efficient
clean-up, based on Kroneker products of rotation-like matrices. The resulting
clean-up time complexity is linearithmic, i.e. $\mathcal{O}(N\,\text{log}\,N)$,
where $N$ is the vector dimension and also the number of vectors in the
codebook. Clean-up space complexity is $\mathcal{O}(N)$. Furthermore, the
codebook is not stored explicitly in computer memory: It can be represented in
$\mathcal{O}(\text{log}\,N)$ space, and individual vectors in the codebook can
be materialized in $\mathcal{O}(N)$ time and space. At the same time,
asymptotic memory capacity remains comparable to standard approaches. Computer
experiments confirm these results, demonstrating several orders of magnitude
more scalability than baseline VSA techniques.

</details>
