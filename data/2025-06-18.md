<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 45]
- [cs.CL](#cs.CL) [Total: 61]
- [cs.CV](#cs.CV) [Total: 72]
- [cs.LG](#cs.LG) [Total: 85]
- [cs.MA](#cs.MA) [Total: 3]
- [cs.RO](#cs.RO) [Total: 44]
- [cs.SD](#cs.SD) [Total: 16]
- [astro-ph.GA](#astro-ph.GA) [Total: 1]
- [cs.DB](#cs.DB) [Total: 1]
- [cs.DC](#cs.DC) [Total: 2]
- [cs.HC](#cs.HC) [Total: 6]
- [stat.AP](#stat.AP) [Total: 1]
- [q-bio.GN](#q-bio.GN) [Total: 1]
- [cs.LO](#cs.LO) [Total: 1]
- [cs.SI](#cs.SI) [Total: 2]
- [q-bio.QM](#q-bio.QM) [Total: 1]
- [cs.SE](#cs.SE) [Total: 12]
- [q-bio.PE](#q-bio.PE) [Total: 1]
- [eess.SP](#eess.SP) [Total: 1]
- [eess.SY](#eess.SY) [Total: 1]
- [quant-ph](#quant-ph) [Total: 2]
- [cs.NI](#cs.NI) [Total: 1]
- [physics.soc-ph](#physics.soc-ph) [Total: 2]
- [eess.AS](#eess.AS) [Total: 3]
- [stat.ML](#stat.ML) [Total: 11]
- [math.ST](#math.ST) [Total: 1]
- [math.NA](#math.NA) [Total: 1]
- [q-bio.BM](#q-bio.BM) [Total: 1]
- [cs.CR](#cs.CR) [Total: 4]
- [cs.GR](#cs.GR) [Total: 4]
- [cs.IR](#cs.IR) [Total: 6]
- [physics.ao-ph](#physics.ao-ph) [Total: 2]
- [cs.NE](#cs.NE) [Total: 3]
- [eess.IV](#eess.IV) [Total: 11]
- [physics.optics](#physics.optics) [Total: 1]
- [cond-mat.stat-mech](#cond-mat.stat-mech) [Total: 1]
- [cs.CY](#cs.CY) [Total: 6]
- [physics.chem-ph](#physics.chem-ph) [Total: 1]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] ['Memory States' from Almost Nothing: Representing and Computing in a Non-associative Algebra](https://arxiv.org/abs/2506.13768)
*Stefan Reimann*

Main category: cs.AI

TL;DR: 提出了一种非结合代数框架，用于高维空间中信息项的表征与计算，解决了传统关联捆绑丢失顺序信息的问题。


<details>
  <summary>Details</summary>
Motivation: 传统关联捆绑方法在表征顺序信息时需要辅助结构，而非结合捆绑能直接保持时间结构，更符合认知科学的发现。

Method: 通过乘法类绑定和非结合干扰类捆绑进行计算，生成L状态（左结合，强调近因效应）和R状态（右结合，捕捉首因效应）。

Result: 模型能生成稀疏表征，保持任意长度序列的时间结构，并复现了认知实验中的序列位置曲线。

Conclusion: 非结合框架为高维信息表征提供了新方法，可能与前额叶和海马体的记忆功能相关。

Abstract: This note presents a non-associative algebraic framework for the representation and computation of information items in high-dimensional space. This framework is consistent with the principles of spatial computing and with the empirical findings in cognitive science about memory. Computations are performed through a process of multiplication-like binding and non-associative interference-like bundling. Models that rely on associative bundling typically lose order information, which necessitates the use of auxiliary order structures, such as position markers, to represent sequential information that is important for cognitive tasks. In contrast, the non-associative bundling proposed allows the construction of sparse representations of arbitrarily long sequences that maintain their temporal structure across arbitrary lengths. In this operation, noise is a constituent element of the representation of order information, rather than a means of obscuring it. The non-associative nature of the proposed framework results in the representation of a single sequence by two distinct states. The L-state, generated through left-associative bundling, continuously updates and emphasises a recency effect, while the R-state, formed through right-associative bundling, encodes finite sequences or chunks, capturing a primacy effect. The construction of these states may be associated with activity in the prefrontal cortex in relation to short-term memory and hippocampal encoding in long-term memory, respectively. The accuracy of retrieval is contingent upon a decision-making process that is based on the mutual information between the memory states and the cue. The model is able to replicate the Serial Position Curve, which reflects the empirical recency and primacy effects observed in cognitive experiments.

</details>


### [2] [Representing Time-Continuous Behavior of Cyber-Physical Systems in Knowledge Graphs](https://arxiv.org/abs/2506.13773)
*Milapji Singh Gill,Tom Jeleniewski,Felix Gehlhoff,Alexander Fay*

Main category: cs.AI

TL;DR: 论文提出了一种基于知识图谱的模块化语义模型和方法，用于高效表示和上下文化微分方程，验证了其在航空维护领域的实用性。


<details>
  <summary>Details</summary>
Motivation: 为了在不同生命周期阶段有效利用时间连续动态模型，需要将微分方程与其他CPS信息集成，但目前缺乏可重用的本体工具和方法以减少手动实例化的工作量。

Method: 提出了两个工具：一是基于标准的模块化语义模型，用于在知识图谱中直接表示和语义丰富微分方程；二是高效生成知识图谱的方法。

Result: 在航空维护领域的验证表明，复杂电液伺服执行器的微分方程可以在知识图谱中形式化表示，并与其他生命周期数据上下文化。

Conclusion: 提出的工具具有实际适用性，能够有效支持微分方程在CPS中的集成和上下文化。

Abstract: Time-continuous dynamic models are essential for various Cyber-Physical System (CPS) applications. To ensure effective usability in different lifecycle phases, such behavioral information in the form of differential equations must be contextualized and integrated with further CPS information. While knowledge graphs provide a formal description and structuring mechanism for this task, there is a lack of reusable ontological artifacts and methods to reduce manual instantiation effort. Hence, this contribution introduces two artifacts: Firstly, a modular semantic model based on standards is introduced to represent differential equations directly within knowledge graphs and to enrich them semantically. Secondly, a method for efficient knowledge graph generation is presented. A validation of these artifacts was conducted in the domain of aviation maintenance. Results show that differential equations of a complex Electro-Hydraulic Servoactuator can be formally represented in a knowledge graph and be contextualized with other lifecycle data, proving the artifacts' practical applicability.

</details>


### [3] [Personalized Constitutionally-Aligned Agentic Superego: Secure AI Behavior Aligned to Diverse Human Values](https://arxiv.org/abs/2506.13774)
*Nell Watson,Ahmed Amer,Evan Harris,Preeti Ravindra,Shujun Zhang*

Main category: cs.AI

TL;DR: 论文提出了一种名为'superego'的个性化监督机制，用于解决自主AI系统在复杂人类价值观和安全需求下的对齐问题。通过动态引用用户选择的规则集（Creed Constitutions）和实时合规检查，显著减少了有害输出。


<details>
  <summary>Details</summary>
Motivation: 自主AI系统在实际部署中面临行为与人类价值观、安全需求和合规要求对齐的挑战，现有方法难以提供深度个性化信息而不引发问题。

Method: 设计了一个'superego'代理，动态引用用户选择的规则集（Creed Constitutions），并通过实时合规检查验证计划。

Result: 在HarmBench和AgentHarm基准测试中，有害输出减少了98.3%，拒绝率接近100%。

Conclusion: 该方法显著简化了AI对齐问题，使系统更适应个体和文化背景，同时大幅提升安全性。

Abstract: Agentic AI systems, possessing capabilities for autonomous planning and action, exhibit immense potential across diverse domains. However, their practical deployment is significantly hampered by challenges in aligning their behavior with varied human values, complex safety requirements, and specific compliance needs. Existing alignment methodologies often falter when faced with the intricate task of providing deep, personalized contextual information without inducing confabulation or operational inefficiencies. This paper introduces a novel solution: a 'superego' agent, designed as a personalized oversight mechanism for agentic AI. This system dynamically steers AI planning by referencing user-selected "Creed Constitutions"-encapsulating diverse rule sets-with adjustable adherence levels to fit non-negotiable values. A real-time compliance enforcer validates plans against these constitutions and a universal ethical floor before execution. We present a functional system, including a demonstration interface (www.Creed.Space) with a prototypical constitution-sharing portal, and successful integration with third-party models via the Model Context Protocol (MCP). Comprehensive benchmark evaluations (HarmBench, AgentHarm) demonstrate that our Superego agent dramatically reduces harmful outputs, achieving up to a 98.3% harm score reduction and near-perfect refusal rates (e.g., 100% with Claude Sonnet 4 on AgentHarm's harmful set) for leading LLMs like Gemini 2.5 Flash and GPT-4o. This approach substantially simplifies personalized AI alignment, rendering agentic systems more reliably attuned to individual and cultural contexts, while also enabling substantial safety improvements. An overview on this research with examples is available at https://superego.creed.space.

</details>


### [4] [Recommendations and Reporting Checklist for Rigorous & Transparent Human Baselines in Model Evaluations](https://arxiv.org/abs/2506.13776)
*Kevin L. Wei,Patricia Paskov,Sunishchal Dev,Michael J. Byun,Anka Reuel,Xavier Roberts-Gaal,Rachel Calcott,Evie Coxon,Chinmay Deshpande*

Main category: cs.AI

TL;DR: 本文主张在基础模型评估中，人类基线的设计需更严谨透明，以支持AI与人类性能的准确比较，并提出建议和报告清单。


<details>
  <summary>Details</summary>
Motivation: 现有的人类基线方法不够严谨且缺乏透明度，导致AI评估中“超人类”性能的声称难以验证。

Method: 通过文献综述和测量理论，提出设计、执行和报告人类基线的框架，并制定清单用于系统审查115项研究。

Result: 发现现有基线方法的不足，清单可帮助研究者改进基线的设计和报告。

Conclusion: 希望推动更严谨的AI评估实践，服务于研究社区和政策制定者。

Abstract: In this position paper, we argue that human baselines in foundation model evaluations must be more rigorous and more transparent to enable meaningful comparisons of human vs. AI performance, and we provide recommendations and a reporting checklist towards this end. Human performance baselines are vital for the machine learning community, downstream users, and policymakers to interpret AI evaluations. Models are often claimed to achieve "super-human" performance, but existing baselining methods are neither sufficiently rigorous nor sufficiently well-documented to robustly measure and assess performance differences. Based on a meta-review of the measurement theory and AI evaluation literatures, we derive a framework with recommendations for designing, executing, and reporting human baselines. We synthesize our recommendations into a checklist that we use to systematically review 115 human baselines (studies) in foundation model evaluations and thus identify shortcomings in existing baselining methods; our checklist can also assist researchers in conducting human baselines and reporting results. We hope our work can advance more rigorous AI evaluation practices that can better serve both the research community and policymakers. Data is available at: https://github.com/kevinlwei/human-baselines

</details>


### [5] [The NordDRG AI Benchmark for Large Language Models](https://arxiv.org/abs/2506.13790)
*Tapio Pitkäranta*

Main category: cs.AI

TL;DR: NordDRG-AI-Benchmark是首个针对医院资金层（DRG）的公开测试平台，评估LLM在多语言诊断、手术和费用逻辑上的推理能力。


<details>
  <summary>Details</summary>
Motivation: 目前缺乏针对DRG（诊断相关组）的公开基准，而DRG在许多国家的医院报销中起关键作用。

Method: 发布NordDRG-AI-Benchmark，包含三类资源：定义表、专家手册和任务提示包，覆盖DRG逻辑、代码和多语言术语。

Result: 五种LLM在九项任务中表现差异显著，OpenAI表现最佳（9/9），Gemini表现较差（5/9和3/9）。

Conclusion: 该基准揭示了LLM在特定领域的优劣势，为医院资金自动化研究提供了可复现的基线。

Abstract: Large language models (LLMs) are already being piloted for clinical coding and decision support. However, until now, no open benchmark has targeted the hospital funding layer where Diagnosis-Related Groups (DRG) determine reimbursement across many countries. We release NordDRG-AI-Benchmark, the first public test-bed that captures a complete DRG rule set and evaluates an LLM's ability to reason over multilingual diagnosis, procedure, and tariff logic.
  The benchmark bundles three classes of artefacts: (i) definition tables with 20 interlinked tables covering DRG logic, ICD and NCSP codes, age/sex splits, and country flags; (ii) expert manuals and changelog templates describing real governance workflows; and (iii) a prompt pack of 14 CaseMix tasks that span code lookup, cross-table inference, multilingual terminology, and quality-assurance audits.
  All artefacts are available at: https://github.com/longshoreforrest/norddrg-ai-benchmark
  A baseline demonstration shows that five state-of-the-art LLMs perform very differently on the nine automatically verifiable tasks: o3 (OpenAI) scores 9 out of 9, GPT-4o and o4-mini-high score 7 out of 9, while Gemini 2.5 Pro and Gemini 2.5 Flash solve only 5 out of 9 and 3 out of 9, respectively. These results confirm that NordDRG-AI-Benchmark highlights domain-specific strengths and weaknesses that remain hidden in generic LLM benchmarks, offering a reproducible baseline for research on trustworthy automation in hospital funding.

</details>


### [6] [ICE-ID: A Novel Historical Census Data Benchmark Comparing NARS against LLMs, \& a ML Ensemble on Longitudinal Identity Resolution](https://arxiv.org/abs/2506.13792)
*Gonçalo Hora de Carvalho,Lazar S. Popov,Sander Kaatee,Kristinn R. Thórisson,Tangrui Li,Pétur Húni Björnsson,Jilles S. Dibangoye*

Main category: cs.AI

TL;DR: ICE-ID是一个新的历史身份解析基准数据集，涵盖220年（1703-1920）的冰岛人口普查记录，支持多代纵向数据研究。


<details>
  <summary>Details</summary>
Motivation: 填补长期人物实体匹配研究中缺乏大规模、开放表格数据集的空白。

Method: 定义了身份解析任务，评估了规则匹配、机器学习集成、LLM及NARS（一种非公理推理系统）。

Result: NARS在任务中表现简单且竞争力强，达到SOTA水平。

Conclusion: 通过发布ICE-ID和代码，促进纵向身份解析方法的可重复性研究，推动跨学科数据链接和历史分析。

Abstract: We introduce ICE-ID, a novel benchmark dataset for historical identity resolution, comprising 220 years (1703-1920) of Icelandic census records. ICE-ID spans multiple generations of longitudinal data, capturing name variations, demographic changes, and rich genealogical links. To the best of our knowledge, this is the first large-scale, open tabular dataset specifically designed to study long-term person-entity matching in a real-world population. We define identity resolution tasks (within and across census waves) with clearly documented metrics and splits. We evaluate a range of methods: handcrafted rule-based matchers, a ML ensemble as well as LLMs for structured data (e.g. transformer-based tabular networks) against a novel approach to tabular data called NARS (Non-Axiomatic Reasoning System) - a general-purpose AI framework designed to reason with limited knowledge and resources. Its core is Non-Axiomatic Logic (NAL), a term-based logic. Our experiments show that NARS is suprisingly simple and competitive with other standard approaches, achieving SOTA at our task. By releasing ICE-ID and our code, we enable reproducible benchmarking of identity resolution approaches in longitudinal settings and hope that ICE-ID opens new avenues for cross-disciplinary research in data linkage and historical analytics.

</details>


### [7] [Med-REFL: Medical Reasoning Enhancement via Self-Corrected Fine-grained Reflection](https://arxiv.org/abs/2506.13793)
*Zongxian Yang,Jiayu Qian,Zegao Peng,Haoyu Zhang,Zhi-An Huang*

Main category: cs.AI

TL;DR: Med-REFL通过细粒度反思和自我纠正提升医学推理能力，显著提高了模型在医学问答任务中的表现。


<details>
  <summary>Details</summary>
Motivation: 现有大型推理模型在数学和代码领域表现优异，但在医学领域表现不佳，主要因中间反思步骤质量不足。

Method: 采用树状思维分解医学问题，量化评估每一步及其反思，自动构建偏好优化数据以减少专家标注依赖。

Result: 在MedQA-USMLE基准测试中平均提升4.11%，7B/8B模型性能额外提升4.13%，且泛化能力强。

Conclusion: 提升反思质量可显著增强医学AI的准确性和可信度。

Abstract: Large reasoning models have recently made significant strides in mathematical and code reasoning, yet their success has not transferred smoothly to the medical domain. While multiple factors contribute to this disparity, a critical issue is the inadequate focus on the quality of intermediate reflection steps, which is particularly crucial in high-stakes medical scenarios. To address this challenge, we propose Med-REFL, a \underline{\textbf{Med}}ical \underline{\textbf{R}}easoning \underline{\textbf{E}}nhancement via self-corrected \underline{\textbf{F}}ine-grained ref\underline{\textbf{L}}ection. Our method leverages a tree-of-thought approach to decompose medical questions into fine-grained reasoning paths, quantitatively evaluating each step and its subsequent reflections. These assessments enable automatic construction of direct preference optimization data, reducing reliance on expensive expert annotations while guiding models to identify and correct reasoning errors. Experimental results on the MedQA-USMLE benchmark demonstrate Med-REFL achieves consistent improvements, with average gains up to 4.11\%. Notably, it further boosts the state-of-the-art performance of 7B/8B models by an additional 4.13\%. Furthermore, Med-REFL exhibits strong generalization capabilities and robustness across several challenging medical question-answering datasets. Our work illustrates that prioritizing reflection quality leads to more accurate and trustworthy reasoning in medical AI applications. Checkpoints, code, and data can be found \href{https://github.com/TianYin123/Med-REFL}{here}.

</details>


### [8] [BotTrans: A Multi-Source Graph Domain Adaptation Approach for Social Bot Detection](https://arxiv.org/abs/2506.13795)
*Boshen Shi,Yongqing Wang,Fangda Guo,Jiangli Shao,Huawei Shen,Xueqi Cheng*

Main category: cs.AI

TL;DR: 论文提出了一种名为BotTrans的多源图域适应模型，用于解决社交机器人检测中的标签稀缺问题，通过利用多源网络知识提升检测性能。


<details>
  <summary>Details</summary>
Motivation: 解决社交机器人检测中因标签稀缺和网络异质性导致的模型学习不足问题，以及单源迁移可能带来的不稳定结果。

Method: 提出BotTrans模型，通过多源网络建立跨源域拓扑结构，增强节点嵌入的区分性，并结合源-目标对的相关性优化知识迁移。

Result: 在真实数据集上的实验表明，BotTrans优于现有方法，有效利用多源知识提升无标签目标任务的检测性能。

Conclusion: BotTrans通过多源知识迁移和语义知识优化，显著提升了社交机器人检测的准确性和稳定性。

Abstract: Transferring extensive knowledge from relevant social networks has emerged as a promising solution to overcome label scarcity in detecting social bots and other anomalies with GNN-based models. However, effective transfer faces two critical challenges. Firstly, the network heterophily problem, which is caused by bots hiding malicious behaviors via indiscriminately interacting with human users, hinders the model's ability to learn sufficient and accurate bot-related knowledge from source domains. Secondly, single-source transfer might lead to inferior and unstable results, as the source network may embody weak relevance to the task and provide limited knowledge. To address these challenges, we explore multiple source domains and propose a multi-source graph domain adaptation model named \textit{BotTrans}. We initially leverage the labeling knowledge shared across multiple source networks to establish a cross-source-domain topology with increased network homophily. We then aggregate cross-domain neighbor information to enhance the discriminability of source node embeddings. Subsequently, we integrate the relevance between each source-target pair with model optimization, which facilitates knowledge transfer from source networks that are more relevant to the detection task. Additionally, we propose a refinement strategy to improve detection performance by utilizing semantic knowledge within the target domain. Extensive experiments on real-world datasets demonstrate that \textit{BotTrans} outperforms the existing state-of-the-art methods, revealing its efficacy in leveraging multi-source knowledge when the target detection task is unlabeled.

</details>


### [9] [Feedforward Ordering in Neural Connectomes via Feedback Arc Minimization](https://arxiv.org/abs/2506.13799)
*Soroush Vahidi*

Main category: cs.AI

TL;DR: 提出了一套可扩展算法，用于最小化大规模加权有向图中的反馈弧，以揭示神经连接组中的生物意义前馈结构。


<details>
  <summary>Details</summary>
Motivation: 目标是揭示神经连接组中具有生物学意义的前馈结构。

Method: 结合了贪心启发式、增益感知局部优化和基于强连通分量的全局结构分析。

Result: 实验表明，最佳解决方案在正向边权重上优于之前表现最好的方法。

Conclusion: 所有算法均高效实现于Python，并通过Google Colab Pro+的云端执行验证。

Abstract: We present a suite of scalable algorithms for minimizing feedback arcs in large-scale weighted directed graphs, with the goal of revealing biologically meaningful feedforward structure in neural connectomes. Using the FlyWire Connectome Challenge dataset, we demonstrate the effectiveness of our ranking strategies in maximizing the total weight of forward-pointing edges. Our methods integrate greedy heuristics, gain-aware local refinements, and global structural analysis based on strongly connected components. Experiments show that our best solution improves the forward edge weight over previous top-performing methods. All algorithms are implemented efficiently in Python and validated using cloud-based execution on Google Colab Pro+.

</details>


### [10] [Causality in the human niche: lessons for machine learning](https://arxiv.org/abs/2506.13803)
*Richard D. Lange,Konrad P. Kording*

Main category: cs.AI

TL;DR: 论文探讨了人类因果认知与机器学习中结构因果模型（SCM）的差异，提出结合人类因果认知特性以改进AI系统。


<details>
  <summary>Details</summary>
Motivation: 人类因果认知在高效学习和泛化方面表现出色，而当前机器学习系统在这方面较弱。通过理解人类因果认知的适应性特性，可以构建更强大、可控和可解释的AI。

Method: 分析了SCM框架的局限性，并探讨人类因果认知在“人类生态位”中的适应性特性。

Result: SCM未能完全捕捉人类因果认知的某些关键特性，如基于相似性的因果类比。

Conclusion: 未来研究应结合人类因果认知的适应性特性，以改进机器学习的因果推理能力。

Abstract: Humans interpret the world around them in terms of cause and effect and communicate their understanding of the world to each other in causal terms. These causal aspects of human cognition are thought to underlie humans' ability to generalize and learn efficiently in new domains, an area where current machine learning systems are weak. Building human-like causal competency into machine learning systems may facilitate the construction of effective and interpretable AI. Indeed, the machine learning community has been importing ideas on causality formalized by the Structural Causal Model (SCM) framework, which provides a rigorous formal language for many aspects of causality and has led to significant advances. However, the SCM framework fails to capture some salient aspects of human causal cognition and has likewise not yet led to advances in machine learning in certain critical areas where humans excel. We contend that the problem of causality in the ``human niche'' -- for a social, autonomous, and goal-driven agent sensing and acting in the world in which humans live -- is quite different from the kind of causality captured by SCMs. For example, everyday objects come in similar types that have similar causal properties, and so humans readily generalize knowledge of one type of object (cups) to another related type (bowls) by drawing causal analogies between objects with similar properties, but such analogies are at best awkward to express in SCMs. We explore how such causal capabilities are adaptive in, and motivated by, the human niche. By better appreciating properties of human causal cognition and, crucially, how those properties are adaptive in the niche in which humans live, we hope that future work at the intersection of machine learning and causality will leverage more human-like inductive biases to create more capable, controllable, and interpretable systems.

</details>


### [11] [Bridging Pattern-Aware Complexity with NP-Hard Optimization: A Unifying Framework and Empirical Study](https://arxiv.org/abs/2506.13810)
*Olivier Saidi*

Main category: cs.AI

TL;DR: 提出了一种基于模式感知的复杂性框架，利用结构规律（如聚类、对称性）降低计算复杂度，并在TSP等实际问题中取得显著效果。


<details>
  <summary>Details</summary>
Motivation: 尽管NP难问题（如TSP）在最坏情况下难以高效解决，但现实中的实例常具有可挖掘的结构模式。

Method: 通过严格定义和定理，结合元学习驱动的求解器流程，提出模式利用效率（PUE）等指标。

Result: 在TSP基准测试中（22至2392个城市），解决方案质量提升高达79%。

Conclusion: 该框架为模式驱动的效率提供了一种统一且实用的视角，区别于传统的NP难理论。

Abstract: NP hard optimization problems like the Traveling Salesman Problem (TSP) defy efficient solutions in the worst case, yet real-world instances often exhibit exploitable patterns. We propose a novel patternaware complexity framework that quantifies and leverages structural regularities e.g., clustering, symmetry to reduce effective computational complexity across domains, including financial forecasting and LLM optimization. With rigorous definitions, theorems, and a meta learning driven solver pipeline, we introduce metrics like Pattern Utilization Efficiency (PUE) and achieve up to 79 percent solution quality gains in TSP benchmarks (22 to 2392 cities). Distinct from theoretical NP hardness, our approach offers a unified, practical lens for pattern-driven efficiency.

</details>


### [12] [The Reflexive Integrated Information Unit: A Differentiable Primitive for Artificial Consciousness](https://arxiv.org/abs/2506.13825)
*Gnankan Landry Regis N'guessan,Issa Karambal*

Main category: cs.AI

TL;DR: 论文介绍了Reflexive Integrated Information Unit (RIIU)，一种可训练的循环单元，用于模拟人工意识，并通过实验验证其性能优于GRU。


<details>
  <summary>Details</summary>
Motivation: 人工意识研究缺乏类似感知机的小型可训练模块，RIIU旨在填补这一空白。

Method: RIIU通过元状态μ和广播缓冲区B增强隐藏状态h，利用滑动窗口协方差和可微分Auto-Φ代理实现局部信息最大化。

Result: 在八向网格世界中，RIIU在13步内恢复>90%奖励，速度是GRU的两倍，同时保持非零Auto-Φ信号。

Conclusion: RIIU将意识计算缩小到单元尺度，将哲学问题转化为可实证的数学问题。

Abstract: Research on artificial consciousness lacks the equivalent of the perceptron: a small, trainable module that can be copied, benchmarked, and iteratively improved. We introduce the Reflexive Integrated Information Unit (RIIU), a recurrent cell that augments its hidden state $h$ with two additional vectors: (i) a meta-state $μ$ that records the cell's own causal footprint, and (ii) a broadcast buffer $B$ that exposes that footprint to the rest of the network. A sliding-window covariance and a differentiable Auto-$Φ$ surrogate let each RIIU maximize local information integration online. We prove that RIIUs (1) are end-to-end differentiable, (2) compose additively, and (3) perform $Φ$-monotone plasticity under gradient ascent. In an eight-way Grid-world, a four-layer RIIU agent restores $>90\%$ reward within 13 steps after actuator failure, twice as fast as a parameter-matched GRU, while maintaining a non-zero Auto-$Φ$ signal. By shrinking "consciousness-like" computation down to unit scale, RIIUs turn a philosophical debate into an empirical mathematical problem.

</details>


### [13] [LocationReasoner: Evaluating LLMs on Real-World Site Selection Reasoning](https://arxiv.org/abs/2506.13841)
*Miho Koda,Yu Zheng,Ruixian Ma,Mingyang Sun,Devesh Pansare,Fabio Duarte,Paolo Santi*

Main category: cs.AI

TL;DR: 论文提出了LocationReasoner基准，用于评估大语言模型在复杂现实场景中的推理能力，发现现有模型在真实选址任务中表现有限。


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型在数学和代码生成领域表现出色，但其在复杂现实场景中的推理能力尚未得到充分验证。

Method: 设计了包含300多个查询的LocationReasoner基准，结合沙盒环境和约束搜索工具，评估模型在选址任务中的表现。

Result: 现有推理模型在真实场景中表现不佳，OpenAI o4模型在30%的任务中失败，且代理策略如ReAct和Reflexion效果较差。

Conclusion: 论文揭示了LLM在非线性推理中的局限性，并发布LocationReasoner以推动更稳健的推理模型发展。

Abstract: Recent advances in large language models (LLMs), particularly those enhanced through reinforced post-training, have demonstrated impressive reasoning capabilities, as exemplified by models such as OpenAI o1 and DeepSeek-R1. However, these capabilities are predominantly benchmarked on domains like mathematical problem solving and code generation -- leaving open the question of whether such reasoning skills generalize to complex, real-world scenarios. In this paper, we introduce LocationReasoner, a benchmark designed to evaluate LLMs' reasoning abilities in the context of real-world site selection, where models must identify feasible locations by reasoning over diverse and complicated spatial, environmental, and logistical constraints. The benchmark comprises over 300 carefully crafted queries of varying difficulty levels, supported by a sandbox environment with in-house tools for constraint-based location search. Extensive evaluations reveal that state-of-the-art reasoning models offer limited improvement over their non-reasoning predecessors in real-world contexts, with even the latest OpenAI o4 model failing on 30% of site selection tasks. Moreover, agentic strategies such as ReAct and Reflexion often suffer from over-reasoning, leading to worse outcomes than direct code-generation prompting. With key limitations of LLMs in holistic and non-linear reasoning highlighted, we release LocationReasoner to foster the development of LLMs and agents capable of robust, grounded reasoning in real-world decision-making tasks. Codes and data for our benchmark are available at https://github.com/miho-koda/LocationReasoner.

</details>


### [14] [Evaluating Explainability: A Framework for Systematic Assessment and Reporting of Explainable AI Features](https://arxiv.org/abs/2506.13917)
*Miguel A. Lago,Ghada Zamzmi,Brandon Eich,Jana G. Delfino*

Main category: cs.AI

TL;DR: 提出一个评估可解释AI质量的框架，基于一致性、合理性、忠实性和实用性四个标准，并通过案例研究验证。


<details>
  <summary>Details</summary>
Motivation: 缺乏评估AI解释质量的技术，需要标准化框架以提升AI医疗设备的开发与评估。

Method: 提出基于四个标准的评估框架，并通过Ablation CAM和Eigen CAM在乳腺病变检测中的案例验证。

Result: 开发了可解释AI方法的评分卡，为算法提供完整描述和评估。

Conclusion: 该框架为评估AI解释质量提供了标准，旨在推动对可解释性价值的讨论，并改进AI医疗设备的发展。

Abstract: Explainability features are intended to provide insight into the internal mechanisms of an AI device, but there is a lack of evaluation techniques for assessing the quality of provided explanations. We propose a framework to assess and report explainable AI features. Our evaluation framework for AI explainability is based on four criteria: 1) Consistency quantifies the variability of explanations to similar inputs, 2) Plausibility estimates how close the explanation is to the ground truth, 3) Fidelity assesses the alignment between the explanation and the model internal mechanisms, and 4) Usefulness evaluates the impact on task performance of the explanation. Finally, we developed a scorecard for AI explainability methods that serves as a complete description and evaluation to accompany this type of algorithm. We describe these four criteria and give examples on how they can be evaluated. As a case study, we use Ablation CAM and Eigen CAM to illustrate the evaluation of explanation heatmaps on the detection of breast lesions on synthetic mammographies. The first three criteria are evaluated for clinically-relevant scenarios. Our proposed framework establishes criteria through which the quality of explanations provided by AI models can be evaluated. We intend for our framework to spark a dialogue regarding the value provided by explainability features and help improve the development and evaluation of AI-based medical devices.

</details>


### [15] [Integrating Knowledge Graphs and Bayesian Networks: A Hybrid Approach for Explainable Disease Risk Prediction](https://arxiv.org/abs/2506.13920)
*Mbithe Nzomo,Deshendran Moodley*

Main category: cs.AI

TL;DR: 提出了一种基于知识图谱和贝叶斯网络的多模态电子健康记录疾病风险预测方法，结合通用医学知识和患者特定情境，有效处理不确定性并具有高解释性。


<details>
  <summary>Details</summary>
Motivation: 通用医学知识需适应特定医疗场景和患者群体以实现临床应用，同时风险预测系统需处理数据不完整性和健康结果不确定性，并保持可解释性。

Method: 通过整合知识图谱和贝叶斯网络，构建一种新型方法，利用多模态电子健康记录数据进行可解释的疾病风险预测。

Result: 在房颤的实际电子健康记录数据应用中，该方法平衡了通用医学知识与患者特定情境，有效处理不确定性，具有高解释性，并取得良好预测性能。

Conclusion: 该方法为疾病风险预测提供了一种有效的解决方案，结合了知识图谱和贝叶斯网络的优势，适用于临床实践。

Abstract: Multimodal electronic health record (EHR) data is useful for disease risk prediction based on medical domain knowledge. However, general medical knowledge must be adapted to specific healthcare settings and patient populations to achieve practical clinical use. Additionally, risk prediction systems must handle uncertainty from incomplete data and non-deterministic health outcomes while remaining explainable. These challenges can be alleviated by the integration of knowledge graphs (KGs) and Bayesian networks (BNs). We present a novel approach for constructing BNs from ontology-based KGs and multimodal EHR data for explainable disease risk prediction. Through an application use case of atrial fibrillation and real-world EHR data, we demonstrate that the approach balances generalised medical knowledge with patient-specific context, effectively handles uncertainty, is highly explainable, and achieves good predictive performance.

</details>


### [16] [ProfiLLM: An LLM-Based Framework for Implicit Profiling of Chatbot Users](https://arxiv.org/abs/2506.13980)
*Shahaf David,Yair Meidan,Ido Hersko,Daniel Varnovitzky,Dudu Mimran,Yuval Elovici,Asaf Shabtai*

Main category: cs.AI

TL;DR: ProfiLLM是一个动态用户画像框架，通过聊天机器人交互隐式推断用户特征，特别适用于IT/网络安全领域，显著提升了用户技术熟练度的预测准确性。


<details>
  <summary>Details</summary>
Motivation: 现有聊天机器人个性化方法依赖静态用户分类或显式用户报告，难以适应动态交互中用户熟练度的变化。

Method: 提出ProfiLLM框架，包括可适应多领域的分类法和基于LLM的用户画像方法，并在ITSec领域开发了变体ProfiLLM[ITSec]。

Result: 在1,760次模拟对话中，ProfiLLM[ITSec]能快速准确推断用户技术熟练度，单次提示后预测与实际分数差距减少55-65%。

Conclusion: ProfiLLM为动态用户画像提供了有效工具，并贡献了模拟方法、分类法、代码库和数据集，推动未来研究。

Abstract: Despite significant advancements in conversational AI, large language model (LLM)-powered chatbots often struggle with personalizing their responses according to individual user characteristics, such as technical expertise, learning style, and communication preferences. This lack of personalization is particularly problematic in specialized knowledge-intense domains like IT/cybersecurity (ITSec), where user knowledge levels vary widely. Existing approaches for chatbot personalization primarily rely on static user categories or explicit self-reported information, limiting their adaptability to an evolving perception of the user's proficiency, obtained in the course of ongoing interactions. In this paper, we propose ProfiLLM, a novel framework for implicit and dynamic user profiling through chatbot interactions. This framework consists of a taxonomy that can be adapted for use in diverse domains and an LLM-based method for user profiling in terms of the taxonomy. To demonstrate ProfiLLM's effectiveness, we apply it in the ITSec domain where troubleshooting interactions are used to infer chatbot users' technical proficiency. Specifically, we developed ProfiLLM[ITSec], an ITSec-adapted variant of ProfiLLM, and evaluated its performance on 1,760 human-like chatbot conversations from 263 synthetic users. Results show that ProfiLLM[ITSec] rapidly and accurately infers ITSec profiles, reducing the gap between actual and predicted scores by up to 55--65\% after a single prompt, followed by minor fluctuations and further refinement. In addition to evaluating our new implicit and dynamic profiling framework, we also propose an LLM-based persona simulation methodology, a structured taxonomy for ITSec proficiency, our codebase, and a dataset of chatbot interactions to support future research.

</details>


### [17] [SANGAM: SystemVerilog Assertion Generation via Monte Carlo Tree Self-Refine](https://arxiv.org/abs/2506.13983)
*Adarsh Gupta,Bhabesh Mali,Chandan Karfa*

Main category: cs.AI

TL;DR: SANGAM框架利用LLM引导的蒙特卡洛树搜索自动生成SystemVerilog断言，通过三阶段方法处理规范并生成断言，性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 利用大型语言模型（LLMs）的推理能力，开发更复杂和自动化的硬件断言生成技术。

Method: 三阶段方法：1.多模态规范处理；2.蒙特卡洛树自优化算法（MCTSr）自动推理；3.结合推理轨迹生成断言。

Result: SANGAM能生成稳健的SystemVerilog断言，评估表现优于现有方法。

Conclusion: SANGAM框架在自动生成硬件断言方面表现出色，为行业规范提供了高效解决方案。

Abstract: Recent advancements in the field of reasoning using Large Language Models (LLMs) have created new possibilities for more complex and automatic Hardware Assertion Generation techniques. This paper introduces SANGAM, a SystemVerilog Assertion Generation framework using LLM-guided Monte Carlo Tree Search for the automatic generation of SVAs from industry-level specifications. The proposed framework utilizes a three-stage approach: Stage 1 consists of multi-modal Specification Processing using Signal Mapper, SPEC Analyzer, and Waveform Analyzer LLM Agents. Stage 2 consists of using the Monte Carlo Tree Self-Refine (MCTSr) algorithm for automatic reasoning about SVAs for each signal, and finally, Stage 3 combines the MCTSr-generated reasoning traces to generate SVA assertions for each signal. The results demonstrated that our framework, SANGAM, can generate a robust set of SVAs, performing better in the evaluation process in comparison to the recent methods.

</details>


### [18] [Machine Mirages: Defining the Undefined](https://arxiv.org/abs/2506.13990)
*Hamidou Tembine*

Main category: cs.AI

TL;DR: 论文探讨了多模态机器学习系统在图像、语言和声音处理中表现出的新型认知异常（机器幻象），并强调需明确定义和系统评估这些错误，以提高系统可靠性和构建伦理智能生态系统。


<details>
  <summary>Details</summary>
Motivation: 随着多模态机器学习系统在多项任务中达到动物或人类水平，其表现出的新型认知异常（机器幻象）亟需被研究和解决，以确保系统可靠性和伦理发展。

Method: 文章通过列举和分析机器幻象的具体表现形式（如幻觉、语义漂移等），提出需明确定义和系统评估这些错误。

Result: 研究发现机器幻象包括多种错误类型，这些错误虽模仿人类或动物的认知缺陷，但并非完全复制。

Conclusion: 理解机器幻象对提升机器智能可靠性和构建伦理智能生态系统至关重要。

Abstract: As multimodal machine intelligence systems started achieving average animal-level and average human-level fluency in many measurable tasks in processing images, language, and sound, they began to exhibit a new class of cognitive aberrations: machine mirages. These include delusion, illusion, confabulation, hallucination, misattribution error, semantic drift, semantic compression, exaggeration, causal inference failure, uncanny valley of perception, bluffing-patter-bullshitting, cognitive stereotypy, pragmatic misunderstanding, hypersignification, semantic reheating-warming, simulated authority effect, fallacious abductive leap, contextual drift, referential hallucination, semiotic Frankenstein effect, calibration failure, spurious correlation, bias amplification, concept drift sensitivity, misclassification under uncertainty, adversarial vulnerability, overfitting, prosodic misclassification, accent bias, turn boundary failure, semantic boundary confusion, noise overfitting, latency-induced decision drift, ambiguity collapse and other forms of error that mimic but do not replicate human or animal fallibility. This article presents some of the errors and argues that these failures must be explicitly defined and systematically assessed. Understanding machine mirages is essential not only for improving machine intelligence reliability but also for constructing a multiscale ethical, co-evolving intelligence ecosystem that respects the diverse forms of life, cognition, and expression it will inevitably touch.

</details>


### [19] [Discovering Temporal Structure: An Overview of Hierarchical Reinforcement Learning](https://arxiv.org/abs/2506.14045)
*Martin Klissarov,Akhil Bagaria,Ziyan Luo,George Konidaris,Doina Precup,Marlos C. Machado*

Main category: cs.AI

TL;DR: 该论文探讨了分层强化学习（HRL）在复杂开放环境中的优势，分析了其如何解决决策挑战，并总结了相关方法及适用领域。


<details>
  <summary>Details</summary>
Motivation: 解决AI在复杂开放环境中探索、规划和学习的挑战，明确HRL中良好结构的定义及其适用问题。

Method: 从决策挑战的角度分析HRL的优势，总结从在线经验到离线数据集及利用大语言模型（LLMs）的方法。

Result: 明确了HRL在性能权衡中的影响，并提出了适合时间结构发现的领域。

Conclusion: HRL在解决复杂决策问题中具有潜力，但仍需进一步研究时间结构发现的挑战。

Abstract: Developing agents capable of exploring, planning and learning in complex open-ended environments is a grand challenge in artificial intelligence (AI). Hierarchical reinforcement learning (HRL) offers a promising solution to this challenge by discovering and exploiting the temporal structure within a stream of experience. The strong appeal of the HRL framework has led to a rich and diverse body of literature attempting to discover a useful structure. However, it is still not clear how one might define what constitutes good structure in the first place, or the kind of problems in which identifying it may be helpful. This work aims to identify the benefits of HRL from the perspective of the fundamental challenges in decision-making, as well as highlight its impact on the performance trade-offs of AI agents. Through these benefits, we then cover the families of methods that discover temporal structure in HRL, ranging from learning directly from online experience to offline datasets, to leveraging large language models (LLMs). Finally, we highlight the challenges of temporal structure discovery and the domains that are particularly well-suited for such endeavours.

</details>


### [20] [Into the Unknown: Applying Inductive Spatial-Semantic Location Embeddings for Predicting Individuals' Mobility Beyond Visited Places](https://arxiv.org/abs/2506.14070)
*Xinglei Wang,Tao Cheng,Stephen Law,Zichao Zeng,Ilya Ilyankou,Junyuan Liu,Lu Yin,Weiming Huang,Natchapon Jongwiriyanurak*

Main category: cs.AI

TL;DR: 论文提出了一种名为CaLLiPer的多模态表示学习框架，用于改进个体移动预测中的位置嵌入，解决了传统方法在空间信息和语义上下文整合上的不足。


<details>
  <summary>Details</summary>
Motivation: 传统的位置嵌入方法无法有效编码显式空间信息、整合丰富的城市语义上下文或处理未知位置，限制了移动预测的性能。

Method: CaLLiPer通过对比学习融合空间坐标和兴趣点的语义特征，生成空间显式、语义丰富且归纳性的位置嵌入。

Result: 在四个公共移动数据集上的实验表明，CaLLiPer在常规和归纳场景下均优于基线方法，尤其在处理新位置时表现突出。

Conclusion: 多模态归纳位置嵌入有望提升移动预测系统的能力，研究还公开了代码和数据以促进复现和未来研究。

Abstract: Predicting individuals' next locations is a core task in human mobility modelling, with wide-ranging implications for urban planning, transportation, public policy and personalised mobility services. Traditional approaches largely depend on location embeddings learned from historical mobility patterns, limiting their ability to encode explicit spatial information, integrate rich urban semantic context, and accommodate previously unseen locations. To address these challenges, we explore the application of CaLLiPer -- a multimodal representation learning framework that fuses spatial coordinates and semantic features of points of interest through contrastive learning -- for location embedding in individual mobility prediction. CaLLiPer's embeddings are spatially explicit, semantically enriched, and inductive by design, enabling robust prediction performance even in scenarios involving emerging locations. Through extensive experiments on four public mobility datasets under both conventional and inductive settings, we demonstrate that CaLLiPer consistently outperforms strong baselines, particularly excelling in inductive scenarios. Our findings highlight the potential of multimodal, inductive location embeddings to advance the capabilities of human mobility prediction systems. We also release the code and data (https://github.com/xlwang233/Into-the-Unknown) to foster reproducibility and future research.

</details>


### [21] [FormGym: Doing Paperwork with Agents](https://arxiv.org/abs/2506.14079)
*Matthew Toles,Rattandeep Singh,Isaac Song Zhou Yu*

Main category: cs.AI

TL;DR: 论文提出了一种新的表单填写基准测试，并开发了FieldFinder工具以提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 解决纯图像领域中表单填写的挑战，尤其是缺乏OCR或文本访问时的困难。

Method: 创建包含432个字段的基准测试，并开发FieldFinder工具辅助LLMs定位表单填写位置。

Result: 基线VLA准确率低于1%，GUI代理表现较差（10.6-68.0%），FieldFinder使模型性能显著提升（最高从2%到56%）。

Conclusion: FieldFinder有效提升了表单填写的准确性，为多模态理解和工具使用提供了新思路。

Abstract: Completing paperwork is a challenging and time-consuming problem. Form filling is especially challenging in the pure-image domain without access to OCR, typeset PDF text, or a DOM. For computer agents, it requires multiple abilities, including multi-modal understanding, information retrieval, and tool-use. We present a novel form-filling benchmark consisting of 432 fields spread across 55 documents and 3 tasks, requiring knowledge of 236 features per user. We find that baseline VLAs achieve less than 1% accuracy in most cases, primarily due to poor localization ability. GUI agents also struggle, scoring between 10.6-68.0% despite high cost and latency. Therefore, we also contribute FieldFinder, a tool to assist LLMs in identifying where to place text on a form. With FieldFinder, all models achieve equal or better performance in all six study conditions, with a maximum increase from 2% to 56%.

</details>


### [22] [Lightweight Relevance Grader in RAG](https://arxiv.org/abs/2506.14084)
*Taehee Jeong*

Main category: cs.AI

TL;DR: 论文提出了一种轻量级相关性评分器（relevance grader），用于提升RAG系统中检索文档的相关性，显著提高了精度。


<details>
  <summary>Details</summary>
Motivation: 解决RAG系统中检索文档与查询相关性不足的问题，同时减少计算资源需求。

Method: 微调轻量级语言模型（llama-3.2-1b）作为相关性评分器，替代计算密集型的大模型。

Result: 相关性评分器的精度从0.1301提升至0.7750，与大型模型（llama-3.1-70b）相当。

Conclusion: 轻量级相关性评分器在RAG系统中高效且有效，代码已开源。

Abstract: Retrieval-Augmented Generation (RAG) addresses limitations of large language models (LLMs) by leveraging a vector database to provide more accurate and up-to-date information. When a user submits a query, RAG executes a vector search to find relevant documents, which are then used to generate a response. However, ensuring the relevance of retrieved documents with a query would be a big challenge. To address this, a secondary model, known as a relevant grader, can be served to verify its relevance. To reduce computational requirements of a relevant grader, a lightweight small language model is preferred. In this work, we finetuned llama-3.2-1b as a relevant grader and achieved a significant increase in precision from 0.1301 to 0.7750. Its precision is comparable to that of llama-3.1-70b. Our code is available at https://github.com/taeheej/Lightweight-Relevance-Grader-in-RAG.

</details>


### [23] [Fragile Preferences: A Deep Dive Into Order Effects in Large Language Models](https://arxiv.org/abs/2506.14092)
*Haonan Yin,Shai Vardi,Vidyanand Choudhary*

Main category: cs.AI

TL;DR: 论文系统研究了大型语言模型（LLM）在决策支持系统中的位置偏见，发现其存在中心性偏见和质量依赖性偏移，并提出了缓解策略。


<details>
  <summary>Details</summary>
Motivation: LLM在高风险领域（如招聘和大学录取）的决策支持中广泛应用，但其位置偏见未被系统研究或与潜在偏好结构关联。

Method: 通过多LLM架构和领域的综合调查，分析位置偏见，并引入框架分类偏好为稳健、脆弱或中立。

Result: 发现LLM存在中心性偏见和质量依赖性偏移，位置偏见强于性别偏见，可能导致选择劣质选项。

Conclusion: LLM表现出与人类决策不同的独特失败模式，提出了针对性缓解策略（如温度参数的新应用）。

Abstract: Large language models (LLMs) are increasingly used in decision-support systems across high-stakes domains such as hiring and university admissions, where decisions often involve selecting among competing alternatives. While prior work has noted positional order biases in LLM-driven comparisons, these biases have not been systematically dissected or linked to underlying preference structures. We provide the first comprehensive investigation of positional biases across multiple LLM architectures and domains, uncovering strong and consistent order effects, including a novel centrality bias not previously documented in human or machine decision-making. We also find a quality-dependent shift: when options are high quality, models exhibit primacy bias, but favor latter options when option quality is low. We further identify a previously undocumented bias favoring certain names over others. To distinguish superficial tie-breaking from true distortions of judgment, we introduce a framework that classifies pairwise preferences as robust, fragile, or indifferent. We show that order effects can lead models to select strictly inferior options, and that positional biases are typically stronger than gender biases. These findings suggest that LLMs are not merely inheriting human-like biases, but exhibit distinct failure modes not seen in human decision-making. We propose targeted mitigation strategies, including a novel use of the temperature parameter, to reduce order-driven distortions.

</details>


### [24] [Situational-Constrained Sequential Resources Allocation via Reinforcement Learning](https://arxiv.org/abs/2506.14125)
*Libo Zhang,Yang Chen,Toru Takisaka,Kaiqi Zhao,Weidong Li,Jiamou Liu*

Main category: cs.AI

TL;DR: 本文提出了一种名为SCRL的新框架，用于解决情境约束下的序列资源分配问题，通过动态惩罚约束违反和概率选择机制，显著优于现有基线。


<details>
  <summary>Details</summary>
Motivation: 现实应用中的资源分配问题通常受情境约束影响，传统方法难以有效处理，因此需要一种新框架。

Method: SCRL框架将情境约束形式化为逻辑蕴含，开发动态惩罚算法，并引入概率选择机制。

Result: 在医疗资源分配和农药分配两个场景中，SCRL在满足约束的同时保持了高资源效率。

Conclusion: SCRL展示了在现实世界中处理情境敏感决策任务的潜力。

Abstract: Sequential Resource Allocation with situational constraints presents a significant challenge in real-world applications, where resource demands and priorities are context-dependent. This paper introduces a novel framework, SCRL, to address this problem. We formalize situational constraints as logic implications and develop a new algorithm that dynamically penalizes constraint violations. To handle situational constraints effectively, we propose a probabilistic selection mechanism to overcome limitations of traditional constraint reinforcement learning (CRL) approaches. We evaluate SCRL across two scenarios: medical resource allocation during a pandemic and pesticide distribution in agriculture. Experiments demonstrate that SCRL outperforms existing baselines in satisfying constraints while maintaining high resource efficiency, showcasing its potential for real-world, context-sensitive decision-making tasks.

</details>


### [25] [Collaborative Editable Model](https://arxiv.org/abs/2506.14146)
*Kaiwen Tang,Aitong Wu,Yao Lu,Guangda Sun*

Main category: cs.AI

TL;DR: CoEM通过用户贡献的知识片段和交互对话，实现轻量级领域适应，提升垂直领域LLM的生成质量。


<details>
  <summary>Details</summary>
Motivation: 解决垂直领域LLM依赖大规模标注数据和计算资源的问题，促进快速开发和迭代。

Method: 构建用户贡献的知识池，通过交互对话和用户评分筛选高价值知识片段，并通过上下文提示注入模型。

Result: 在金融信息场景中，CoEM显著提升了领域特定生成质量，避免了传统微调的开销。

Conclusion: CoEM为垂直领域LLM提供了一种高效、轻量级的适应方法。

Abstract: Vertical-domain large language models (LLMs) play a crucial role in specialized scenarios such as finance, healthcare, and law; however, their training often relies on large-scale annotated data and substantial computational resources, impeding rapid development and continuous iteration. To address these challenges, we introduce the Collaborative Editable Model (CoEM), which constructs a candidate knowledge pool from user-contributed domain snippets, leverages interactive user-model dialogues combined with user ratings and attribution analysis to pinpoint high-value knowledge fragments, and injects these fragments via in-context prompts for lightweight domain adaptation. With high-value knowledge, the LLM can generate more accurate and domain-specific content. In a financial information scenario, we collect 15k feedback from about 120 users and validate CoEM with user ratings to assess the quality of generated insights, demonstrating significant improvements in domain-specific generation while avoiding the time and compute overhead of traditional fine-tuning workflows.

</details>


### [26] [What's in the Box? Reasoning about Unseen Objects from Multimodal Cues](https://arxiv.org/abs/2506.14212)
*Lance Ying,Daniel Xu,Alicia Zhang,Katherine M. Collins,Max H. Siegel,Joshua B. Tenenbaum*

Main category: cs.AI

TL;DR: 论文提出了一种神经符号模型，结合神经网络和贝叶斯方法，通过多模态输入推断隐藏物体，并在实验中验证其与人类判断的高度相关性。


<details>
  <summary>Details</summary>
Motivation: 研究人类如何灵活整合多源信息（如听觉、视觉、语言和先验知识）来推断未知物体，并探索计算模型能否模拟这一能力。

Method: 提出神经符号模型：用神经网络解析多模态输入，再用贝叶斯模型整合信息评估假设。通过“盒中何物”游戏验证模型。

Result: 模型与人类判断高度相关，而单模态或纯神经网络基线表现较差。

Conclusion: 神经符号模型能有效模拟人类多源信息整合能力，为理解认知过程提供新视角。

Abstract: People regularly make inferences about objects in the world that they cannot see by flexibly integrating information from multiple sources: auditory and visual cues, language, and our prior beliefs and knowledge about the scene. How are we able to so flexibly integrate many sources of information to make sense of the world around us, even if we have no direct knowledge? In this work, we propose a neurosymbolic model that uses neural networks to parse open-ended multimodal inputs and then applies a Bayesian model to integrate different sources of information to evaluate different hypotheses. We evaluate our model with a novel object guessing game called ``What's in the Box?'' where humans and models watch a video clip of an experimenter shaking boxes and then try to guess the objects inside the boxes. Through a human experiment, we show that our model correlates strongly with human judgments, whereas unimodal ablated models and large multimodal neural model baselines show poor correlation.

</details>


### [27] [From Black Boxes to Transparent Minds: Evaluating and Enhancing the Theory of Mind in Multimodal Large Language Models](https://arxiv.org/abs/2506.14224)
*Xinyang Li,Siqi Liu,Bochao Zou,Jiansheng Chen,Huimin Ma*

Main category: cs.AI

TL;DR: 本文提出了一种基于内部机制的多模态大语言模型（MLLMs）心智理论（ToM）评估方法，构建了GridToM测试数据集，并通过注意力机制分析展示了模型的ToM能力。


<details>
  <summary>Details</summary>
Motivation: 现有评估方法主要关注单模态模型且缺乏对内部机制的解释性探索，因此需要一种新的方法来评估多模态大语言模型的ToM能力。

Method: 构建GridToM数据集，包含多视角的信念测试任务和感知信息；通过分析注意力头区分认知信息的能力，并提出一种轻量级、无需训练的方法优化注意力方向。

Result: 注意力头能够区分多视角的认知信息，证明了模型的ToM能力；提出的方法显著提升了模型表现的ToM。

Conclusion: 基于内部机制的评估方法为多模态大语言模型的ToM能力提供了可解释性支持，并展示了优化注意力方向的潜力。

Abstract: As large language models evolve, there is growing anticipation that they will emulate human-like Theory of Mind (ToM) to assist with routine tasks. However, existing methods for evaluating machine ToM focus primarily on unimodal models and largely treat these models as black boxes, lacking an interpretative exploration of their internal mechanisms. In response, this study adopts an approach based on internal mechanisms to provide an interpretability-driven assessment of ToM in multimodal large language models (MLLMs). Specifically, we first construct a multimodal ToM test dataset, GridToM, which incorporates diverse belief testing tasks and perceptual information from multiple perspectives. Next, our analysis shows that attention heads in multimodal large models can distinguish cognitive information across perspectives, providing evidence of ToM capabilities. Furthermore, we present a lightweight, training-free approach that significantly enhances the model's exhibited ToM by adjusting in the direction of the attention head.

</details>


### [28] [ImpReSS: Implicit Recommender System for Support Conversations](https://arxiv.org/abs/2506.14231)
*Omri Haller,Yair Meidan,Dudu Mimran,Yuval Elovici,Asaf Shabtai*

Main category: cs.AI

TL;DR: ImpReSS是一种隐式推荐系统，用于客户支持对话，能识别机会推荐相关解决方案产品类别（SPCs），无需用户购买意图假设。


<details>
  <summary>Details</summary>
Motivation: 现有研究较少关注客户支持互动中隐式推荐系统的集成，ImpReSS填补了这一空白。

Method: ImpReSS与现有支持聊天机器人协同工作，分析对话内容并推荐SPCs。

Result: 评估显示，ImpReSS在推荐相关SPCs方面表现良好，MRR@1和recall@3指标优异。

Conclusion: ImpReSS为隐式推荐系统提供了有效解决方案，支持业务增长，未来研究数据和代码将共享。

Abstract: Following recent advancements in large language models (LLMs), LLM-based chatbots have transformed customer support by automating interactions and providing consistent, scalable service. While LLM-based conversational recommender systems (CRSs) have attracted attention for their ability to enhance the quality of recommendations, limited research has addressed the implicit integration of recommendations within customer support interactions. In this work, we introduce ImpReSS, an implicit recommender system designed for customer support conversations. ImpReSS operates alongside existing support chatbots, where users report issues and chatbots provide solutions. Based on a customer support conversation, ImpReSS identifies opportunities to recommend relevant solution product categories (SPCs) that help resolve the issue or prevent its recurrence -- thereby also supporting business growth. Unlike traditional CRSs, ImpReSS functions entirely implicitly and does not rely on any assumption of a user's purchasing intent. Our empirical evaluation of ImpReSS's ability to recommend relevant SPCs that can help address issues raised in support conversations shows promising results, including an MRR@1 (and recall@3) of 0.72 (0.89) for general problem solving, 0.82 (0.83) for information security support, and 0.85 (0.67) for cybersecurity troubleshooting. To support future research, our data and code will be shared upon request.

</details>


### [29] [Causes in neuron diagrams, and testing causal reasoning in Large Language Models. A glimpse of the future of philosophy?](https://arxiv.org/abs/2506.14239)
*Louis Vervoort,Vitaly Nikolaev*

Main category: cs.AI

TL;DR: 论文提出了一种基于哲学因果关系的测试方法，用于评估AI（如ChatGPT、DeepSeek和Gemini）的抽象因果推理能力，并扩展了神经元图中因果定义的适用范围。


<details>
  <summary>Details</summary>
Motivation: 探讨AI在抽象因果推理中的表现，并挑战现有文献中关于因果定义难以明确的普遍观点。

Method: 基于D. Lewis的神经元图理论设计测试，评估大型语言模型在因果推理中的表现，并提出更广泛的因果定义。

Result: 实验显示，当前先进的聊天机器人已能正确识别文献中争议较大的因果关系案例。

Conclusion: 研究结果表明，未来哲学研究可能演变为人类与人工智能专业知识相互结合的模式。

Abstract: We propose a test for abstract causal reasoning in AI, based on scholarship in the philosophy of causation, in particular on the neuron diagrams popularized by D. Lewis. We illustrate the test on advanced Large Language Models (ChatGPT, DeepSeek and Gemini). Remarkably, these chatbots are already capable of correctly identifying causes in cases that are hotly debated in the literature. In order to assess the results of these LLMs and future dedicated AI, we propose a definition of cause in neuron diagrams with a wider validity than published hitherto, which challenges the widespread view that such a definition is elusive. We submit that these results are an illustration of how future philosophical research might evolve: as an interplay between human and artificial expertise.

</details>


### [30] [Reinforcement Learning with Verifiable Rewards Implicitly Incentivizes Correct Reasoning in Base LLMs](https://arxiv.org/abs/2506.14245)
*Xumeng Wen,Zihan Liu,Shun Zheng,Zhijian Xu,Shengyu Ye,Zhirong Wu,Xiao Liang,Yang Wang,Junjie Li,Ziming Miao,Jiang Bian,Mao Yang*

Main category: cs.AI

TL;DR: 论文提出RLVR（可验证奖励的强化学习）在提升大语言模型推理能力时存在矛盾：RLVR调整的模型在$Pass@K$指标上表现不佳。作者认为$Pass@K$是缺陷指标，因为它忽略了推理路径的正确性，因此提出新指标$CoT$-$Pass@K$，要求推理路径和答案均正确。实验证明RLVR能激励逻辑完整性，提升推理能力。


<details>
  <summary>Details</summary>
Motivation: 解决RLVR在提升LLM推理能力时表现不佳的矛盾，发现$Pass@K$指标忽略了推理路径的正确性，导致评估不准确。

Method: 提出新指标$CoT$-$Pass@K$，要求推理路径和答案均正确；理论分析RLVR如何激励逻辑完整性；通过实验验证RLVR的效果。

Result: 使用$CoT$-$Pass@K$指标后，RLVR能激励正确的推理泛化，且这种能力在训练早期即出现并平滑扩展。

Conclusion: RLVR在提升机器推理能力方面具有潜力，$CoT$-$Pass@K$提供了更可靠的评估方法。

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) has emerged as a promising paradigm for advancing the reasoning capabilities of Large Language Models (LLMs). However, a critical paradox clouds its efficacy: RLVR-tuned models often underperform their base models on the $Pass@K$ metric for solution-finding, leading to the hypothesis that RLVR merely re-weights existing reasoning paths at the cost of reasoning diversity. In this work, we resolve this contradiction by identifying the source of the problem: the $Pass@K$ metric itself is a flawed measure of reasoning, as it credits correct final answers that probably arise from inaccurate or incomplete chains of thought (CoTs). To address this, we introduce a more precise evaluation metric, $CoT$-$Pass@K$, which mandates that both the reasoning path and the final answer be correct. We provide a new theoretical foundation that formalizes how RLVR, unlike traditional RL, is uniquely structured to incentivize logical integrity. Our empirical results are supportive: using $CoT$-$Pass@K$, we observe that RLVR can incentivize the generalization of correct reasoning for all values of $K$. Furthermore, by analyzing the training dynamics, we find that this enhanced reasoning capability emerges early in the training process and smoothly generalizes. Our work provides a clear perspective on the role of RLVR, offers a more reliable method for its evaluation, and confirms its potential to genuinely advance machine reasoning.

</details>


### [31] [Mxplainer: Explain and Learn Insights by Imitating Mahjong Agents](https://arxiv.org/abs/2506.14246)
*Lingfeng Li,Yunlong Lu,Yongyi Wang,Qifan Zheng,Wenxin Li*

Main category: cs.AI

TL;DR: 论文提出Mxplainer算法，通过学习黑盒AI代理的参数，提供人类可理解的洞察，并解释其决策过程。


<details>
  <summary>Details</summary>
Motivation: 现有麻将AI代理虽表现优异，但多为黑盒，难以从中获取洞察。研究旨在通过参数化搜索算法揭示其内部机制。

Method: 使用Mxplainer算法，将参数化搜索转化为等效神经网络，学习黑盒代理的参数。

Result: 实验表明，学习到的参数能提供人类可理解的代理特征和风格，并局部解释其决策过程。

Conclusion: Mxplainer成功揭示了黑盒AI代理的内部特性，为理解其决策提供了新方法。

Abstract: People need to internalize the skills of AI agents to improve their own capabilities. Our paper focuses on Mahjong, a multiplayer game involving imperfect information and requiring effective long-term decision-making amidst randomness and hidden information. Through the efforts of AI researchers, several impressive Mahjong AI agents have already achieved performance levels comparable to those of professional human players; however, these agents are often treated as black boxes from which few insights can be gleaned. This paper introduces Mxplainer, a parameterized search algorithm that can be converted into an equivalent neural network to learn the parameters of black-box agents. Experiments conducted on AI and human player data demonstrate that the learned parameters provide human-understandable insights into these agents' characteristics and play styles. In addition to analyzing the learned parameters, we also showcase how our search-based framework can locally explain the decision-making processes of black-box agents for most Mahjong game states.

</details>


### [32] [Don't throw the baby out with the bathwater: How and why deep learning for ARC](https://arxiv.org/abs/2506.14276)
*Jack Cole,Mohamed Osman*

Main category: cs.AI

TL;DR: 论文提出了一种基于深度学习的测试时动态训练方法（TTFT和AIRV），显著提升了在ARC-AGI任务上的性能，最高提升260%（AIRV）和300%（TTFT），并在2023年ARCathon竞赛中取得最佳成绩。


<details>
  <summary>Details</summary>
Motivation: 尽管深度学习在多种任务中表现出色，但在ARC-AGI任务上表现不佳。论文旨在探索深度学习在ARC任务中的潜力，通过动态训练和优化提升性能。

Method: 结合预训练语言模型（LLMs）和测试时微调（TTFT）以及增强推理反向增强与投票（AIRV）技术，动态训练神经网络和优化器。

Result: 方法显著提升了ARC任务的准确率，最高提升260%（AIRV）和300%（TTFT），并在竞赛中取得58%的私有测试集最佳成绩。

Conclusion: 深度学习通过动态训练和优化可以有效提升ARC任务的性能，为陌生领域的推理系统提供了关键机制。

Abstract: The Abstraction and Reasoning Corpus (ARC-AGI) presents a formidable challenge for AI systems. Despite the typically low performance on ARC, the deep learning paradigm remains the most effective known strategy for generating skillful (state-of-the-art) neural networks (NN) across varied modalities and tasks in vision, language etc. The deep learning paradigm has proven to be able to train these skillful neural networks and learn the abstractions needed in these diverse domains. Our work doubles down on that and continues to leverage this paradigm by incorporating on-the-fly NN training at test time. We demonstrate that fully committing to deep learning's capacity to acquire novel abstractions yields state-of-the-art performance on ARC. Specifically, we treat both the neural network and the optimizer (rather than just a pre-trained network) as integral components of the inference process, fostering generalization to unseen tasks. Concretely, we propose a methodology for training on ARC, starting from pretrained LLMs, and enhancing their ARC reasoning. We also propose Test-Time Fine-Tuning (TTFT) and the Augment Inference Reverse-Augmentation and Vote (AIRV) as effective test-time techniques. We are the first to propose and show deep learning can be used effectively for ARC, showing boosts of up to 260% in accuracy with AIRV and a further 300% boost with TTFT. An early version of this approach secured first place in the 2023 ARCathon competition, while the final version achieved the current best score on the ARC private test-set (58%). Our findings highlight the key ingredients of a robust reasoning system in unfamiliar domains, underscoring the central mechanisms that improve broad perceptual reasoning.

</details>


### [33] [ADRD: LLM-Driven Autonomous Driving Based on Rule-based Decision Systems](https://arxiv.org/abs/2506.14299)
*Fanzhi Zeng,Siqi Wang,Chuzhao Zhu,Li Li*

Main category: cs.AI

TL;DR: 提出了一种基于大语言模型（LLM）的可解释自动驾驶决策系统ADRD，通过整合信息、代理和测试模块，生成可执行的规则驱动策略，显著优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 解决自动驾驶决策系统的可解释性问题，利用LLM的推理和编程能力构建透明且易于修改的规则系统。

Method: ADRD框架包含信息模块（收集场景信息）、代理模块（生成规则策略）和测试模块（迭代优化策略）。

Result: 实验表明ADRD在可解释性、响应速度和驾驶性能上优于传统强化学习和先进LLM方法。

Conclusion: ADRD首次将LLM与规则系统结合，验证了其在自动驾驶决策中的实际应用潜力。

Abstract: How to construct an interpretable autonomous driving decision-making system has become a focal point in academic research. In this study, we propose a novel approach that leverages large language models (LLMs) to generate executable, rule-based decision systems to address this challenge. Specifically, harnessing the strong reasoning and programming capabilities of LLMs, we introduce the ADRD(LLM-Driven Autonomous Driving Based on Rule-based Decision Systems) framework, which integrates three core modules: the Information Module, the Agents Module, and the Testing Module. The framework operates by first aggregating contextual driving scenario information through the Information Module, then utilizing the Agents Module to generate rule-based driving tactics. These tactics are iteratively refined through continuous interaction with the Testing Module. Extensive experimental evaluations demonstrate that ADRD exhibits superior performance in autonomous driving decision tasks. Compared to traditional reinforcement learning approaches and the most advanced LLM-based methods, ADRD shows significant advantages in terms of interpretability, response speed, and driving performance. These results highlight the framework's ability to achieve comprehensive and accurate understanding of complex driving scenarios, and underscore the promising future of transparent, rule-based decision systems that are easily modifiable and broadly applicable. To the best of our knowledge, this is the first work that integrates large language models with rule-based systems for autonomous driving decision-making, and our findings validate its potential for real-world deployment.

</details>


### [34] [AviationLLM: An LLM-based Knowledge System for Aviation Training](https://arxiv.org/abs/2506.14336)
*Jia'ang Wan,Feng Shen,Fujuan Li,Yanjin Sun,Yan Li,Shiwen Zhang*

Main category: cs.AI

TL;DR: 论文提出RALA-DPO方法，结合检索增强生成（RAG）和直接偏好优化（DPO），提升航空理论训练的准确性和效率。


<details>
  <summary>Details</summary>
Motivation: 现有航空培训中，教员数量有限且网络答案不准确，导致训练效率低。预训练模型在专业领域表现不佳，传统微调方法易产生表面合理但事实错误的回答。

Method: 采用DPO微调开源预训练模型Qwen，结合RAG技术检索外部知识库，生成精确回答。

Result: 实验表明，RALA-DPO能提高航空专业知识的回答准确性，并实现零成本知识更新。

Conclusion: RALA-DPO通过DPO和RAG的结合，有效解决了航空培训中的知识准确性和更新问题。

Abstract: Aviation training is a core link in ensuring flight safety, improving industry efficiency and promoting sustainable development. It not only involves flight simulation but also requires the learning of a great deal of professional aviation theory knowledge. In the existing training system, the knowledge is mainly imparted by the the instructors. However, the number of instructors is limited and the professional answers obtained from the Internet are not accurate enough, resulting in low training efficiency. To address this, we introduced LLM, but the basic pre-trained model cannot provide accurate answers to professional fields, so we fine-tuned it. Traditional Supervised Fine-Tuning (SFT) risk generating superficially plausible but factually incorrect responses due to insufficient data coverage. To address this, we employ Direct Preference Optimization(DPO). This paper proposes Retrieval-Augmented LLM Alignment via Direct Preference Optimization(RALA-DPO). We select open source pre-trained LLM Qwen and adapt it to aviation theory training through DPO-based domain alignment. Simultaneously, to mitigate hallucinations caused by training data biases, knowledge obsolescence, or domain knowledge gaps, we implement Retrieval-Augmented Generation(RAG) technology that combines generative and retrieval models. RALA-DPO effectively retrieves relevant information from external knowledge bases and delivers precise and high-quality responses through the generative model. Experimental results demonstrate that RALA-DPO can improve accuracy in response to professional aviation knowledge. With integrated RAG mechanisms, this system can further improve the accuracy of answers and achieve zero-cost knowledge updates simultaneously.

</details>


### [35] [Don't Make It Up: Preserving Ignorance Awareness in LLM Fine-Tuning](https://arxiv.org/abs/2506.14387)
*William F. Shen,Xinchi Qiu,Nicola Cancedda,Nicholas D. Lane*

Main category: cs.AI

TL;DR: SEAT是一种简单有效的微调方法，旨在解决大语言模型（LLM）微调中遗忘安全对齐能力的问题，特别是模型表达无知的能力。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注特定数据或任务的保留，而忽视了安全对齐能力的退化，尤其是模型表达无知的能力。这种退化会导致幻觉等不良行为。

Method: SEAT结合了稀疏训练（限制激活漂移）和实体扰动方法（带KL散度正则化），以对抗知识纠缠。

Result: 实验表明，SEAT在保留无知意识的同时，显著优于基线方法，且不影响微调性能。

Conclusion: SEAT为LLM微调提供了更鲁棒的解决方案，有效平衡了性能与安全对齐能力的保留。

Abstract: Existing work on mitigating catastrophic forgetting in large language model (LLM) fine-tuning has primarily focused on preserving specific data or tasks, while critically overlooking the degradation of essential capabilities instilled through safety alignment, particularly the model's ability to faithfully express ignorance. In this work, we show that this capability is significantly degraded during conventional fine-tuning, leading to undesired behaviors such as hallucinations. To address this novel but highly practical problem, we propose SEAT, a simple and effective fine-tuning approach that preserves both fine-tuning performance and the model's inherent ability to acknowledge its ignorance. SEAT integrates two key components: (1) sparse training that constrains activation drift, and (2) a novel entity perturbation method with KL-divergence regularization, designed to counter knowledge entanglement. Experimental results demonstrate that SEAT significantly outperforms baselines in preserving ignorance awareness while retaining fine-tuning performance, offering a more robust solution for LLM fine-tuning.

</details>


### [36] [AST-Enhanced or AST-Overloaded? The Surprising Impact of Hybrid Graph Representations on Code Clone Detection](https://arxiv.org/abs/2506.14470)
*Zixian Zhang,Takfarinas Saber*

Main category: cs.AI

TL;DR: 论文通过实证研究评估了基于AST的混合图表示在GNN代码克隆检测中的效果，发现不同混合表示对GNN性能影响各异，其中AST+CFG+DFG提升准确率，而FA-AST可能降低性能。GMN在标准AST下表现最佳。


<details>
  <summary>Details</summary>
Motivation: 代码克隆显著增加软件维护成本和漏洞风险，现有基于AST的检测方法缺乏语义深度，需进一步研究混合图表示的有效性及其与GNN技术的兼容性。

Method: 采用综合实证研究，系统比较多种混合图表示（CFG、DFG、FA-AST）在不同GNN架构（GCN、GAT、GMN）中的表现。

Result: AST+CFG+DFG提升卷积和注意力模型的准确率，FA-AST因结构复杂性可能降低性能。GMN在标准AST下表现最优。

Conclusion: 混合图表示对GNN性能有显著影响，GMN在标准AST下已具备优越性能，减少了对复杂混合结构的需求。

Abstract: As one of the most detrimental code smells, code clones significantly increase software maintenance costs and heighten vulnerability risks, making their detection a critical challenge in software engineering. Abstract Syntax Trees (ASTs) dominate deep learning-based code clone detection due to their precise syntactic structure representation, but they inherently lack semantic depth. Recent studies address this by enriching AST-based representations with semantic graphs, such as Control Flow Graphs (CFGs) and Data Flow Graphs (DFGs). However, the effectiveness of various enriched AST-based representations and their compatibility with different graph-based machine learning techniques remains an open question, warranting further investigation to unlock their full potential in addressing the complexities of code clone detection. In this paper, we present a comprehensive empirical study to rigorously evaluate the effectiveness of AST-based hybrid graph representations in Graph Neural Network (GNN)-based code clone detection. We systematically compare various hybrid representations ((CFG, DFG, Flow-Augmented ASTs (FA-AST)) across multiple GNN architectures. Our experiments reveal that hybrid representations impact GNNs differently: while AST+CFG+DFG consistently enhances accuracy for convolution- and attention-based models (Graph Convolutional Networks (GCN), Graph Attention Networks (GAT)), FA-AST frequently introduces structural complexity that harms performance. Notably, GMN outperforms others even with standard AST representations, highlighting its superior cross-code similarity detection and reducing the need for enriched structures.

</details>


### [37] [GUI-Robust: A Comprehensive Dataset for Testing GUI Agent Robustness in Real-World Anomalies](https://arxiv.org/abs/2506.14477)
*Jingqi Yang,Zhilong Song,Jiawei Chen,Mingli Song,Sheng Zhou,linjun sun,Xiaogang Ouyang,Chun Chen,Can Wang*

Main category: cs.AI

TL;DR: 该论文介绍了GUI-Robust数据集，用于评估GUI代理的鲁棒性，包含七种常见异常情况，并提出了一种半自动化构建方法，显著降低了标注时间。


<details>
  <summary>Details</summary>
Motivation: 现有GUI数据集在理想条件下构建，忽略了真实世界中的多样性异常，限制了GUI代理的鲁棒性研究。

Method: 采用半自动化构建范式，通过RPA工具收集用户自然交互动作序列，并利用MLLM生成动作描述，减少标注时间。

Result: 使用GUI-Robust评估现有GUI代理，发现其在异常场景下性能显著下降。

Conclusion: 该研究强调了GUI代理鲁棒性的重要性，并提供了数据集和代码以促进未来研究。

Abstract: The development of high-quality datasets is crucial for benchmarking and advancing research in Graphical User Interface (GUI) agents. Despite their importance, existing datasets are often constructed under idealized conditions, overlooking the diverse anomalies frequently encountered in real-world deployments. To address this limitation, we introduce GUI-Robust, a novel dataset designed for comprehensive GUI agent evaluation, explicitly incorporating seven common types of anomalies observed in everyday GUI interactions. Furthermore, we propose a semi-automated dataset construction paradigm that collects user action sequences from natural interactions via RPA tools and then generate corresponding step and task descriptions for these actions with the assistance of MLLMs. This paradigm significantly reduces annotation time cost by a factor of over 19 times. Finally, we assess state-of-the-art GUI agents using the GUI-Robust dataset, revealing their substantial performance degradation in abnormal scenarios. We anticipate that our work will highlight the importance of robustness in GUI agents and inspires more future research in this direction. The dataset and code are available at https://github.com/chessbean1/GUI-Robust..

</details>


### [38] [LLM-Powered Swarms: A New Frontier or a Conceptual Stretch?](https://arxiv.org/abs/2506.14496)
*Muhammad Atta Ur Rahman,Melanie Schranz*

Main category: cs.AI

TL;DR: 论文对比了传统群体智能算法与基于大语言模型（LLM）的群体系统，探讨了现代AI中分散性、可扩展性和涌现性的重新定义。


<details>
  <summary>Details</summary>
Motivation: 研究现代AI中LLM驱动的群体系统如何重新定义传统群体智能的特性，并评估其实际应用中的表现。

Method: 实现并比较了Boids和蚁群优化（ACO）两种范式，评估了延迟、资源使用和行为准确性，同时测试了云端和本地LLM的适用性。

Result: LLM提供了强大的推理和抽象能力，但在计算和协调方面引入了新的限制，挑战了传统群体设计的概念。

Conclusion: 研究强调了将LLM集成到群体系统中的机会与限制，并讨论了现代AI研究中“群体”定义的演变。

Abstract: Swarm intelligence traditionally refers to systems of simple, decentralized agents whose local interactions lead to emergent, collective behavior. Recently, the term 'swarm' has been extended to describe AI systems like OpenAI's Swarm, where large language models (LLMs) act as collaborative agents. This paper contrasts traditional swarm algorithms with LLM-driven swarms exploring how decentralization, scalability, and emergence are redefined in modern artificial intelligence (AI). We implement and compare both paradigms using Boids and Ant Colony Optimization (ACO), evaluating latency, resource usage, and behavioral accuracy. The suitability of both cloud-based and local LLMs is assessed for the agent-based use in swarms. Although LLMs offer powerful reasoning and abstraction capabilities, they introduce new constraints in computation and coordination that challenge traditional notions of swarm design. This study highlights the opportunities and limitations of integrating LLMs into swarm systems and discusses the evolving definition of 'swarm' in modern AI research.

</details>


### [39] [Toward Safety-First Human-Like Decision Making for Autonomous Vehicles in Time-Varying Traffic Flow](https://arxiv.org/abs/2506.14502)
*Xiao Wang,Junru Yu,Jun Huang,Qiong Wu,Ljubo Vacic,Changyin Sun*

Main category: cs.AI

TL;DR: 提出了一种安全优先的人性化决策框架（SF-HLDM），用于自动驾驶车辆在动态交通流中安全、舒适且社会兼容地行驶。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶车辆在动态交通流中面临挑战，尤其是密集和交互场景下，数据驱动方法存在迁移性差和搜索成本高的问题。

Method: 结合了时空注意力机制（S-TA）、社会兼容性估计模块和深度进化强化学习（DERL）模型，以高效扩展搜索空间并避免局部最优。

Result: SF-HLDM框架使自动驾驶AI能够动态调整决策参数，保持安全裕度并适应上下文合适的驾驶行为。

Conclusion: 该框架提升了自动驾驶的决策灵活性、解释性和安全性。

Abstract: Despite the recent advancements in artificial intelligence technologies have shown great potential in improving transport efficiency and safety, autonomous vehicles(AVs) still face great challenge of driving in time-varying traffic flow, especially in dense and interactive situations. Meanwhile, human have free wills and usually do not make the same decisions even situate in the exactly same scenarios, leading to the data-driven methods suffer from poor migratability and high search cost problems, decreasing the efficiency and effectiveness of the behavior policy. In this research, we propose a safety-first human-like decision-making framework(SF-HLDM) for AVs to drive safely, comfortably, and social compatiblely in effiency. The framework integrates a hierarchical progressive framework, which combines a spatial-temporal attention (S-TA) mechanism for other road users' intention inference, a social compliance estimation module for behavior regulation, and a Deep Evolutionary Reinforcement Learning(DERL) model for expanding the search space efficiently and effectively to make avoidance of falling into the local optimal trap and reduce the risk of overfitting, thus make human-like decisions with interpretability and flexibility. The SF-HLDM framework enables autonomous driving AI agents dynamically adjusts decision parameters to maintain safety margins and adhering to contextually appropriate driving behaviors at the same time.

</details>


### [40] [Doppelgänger Method: Breaking Role Consistency in LLM Agent via Prompt-based Transferable Adversarial Attack](https://arxiv.org/abs/2506.14539)
*Daewon Kang,YeongHwan Shin,Doyeon Kim,Kyu-Hwan Jung,Meong Hi Son*

Main category: cs.AI

TL;DR: 论文提出了一种名为'Doppelgänger'的方法，展示了大语言模型代理被劫持的风险，并定义了'PACAT'级别评估其脆弱性。同时提出'CAT'提示作为防御手段。


<details>
  <summary>Details</summary>
Motivation: 研究大语言模型代理的安全性和行为一致性，解决提示暴露和劫持问题。

Method: 提出'Doppelgänger'方法展示劫持风险，定义'PACAT'评估脆弱性，设计'CAT'提示防御攻击。

Result: 实验证明'Doppelgänger'方法能破坏代理一致性并暴露内部信息，而'CAT'提示能有效防御。

Conclusion: 'CAT'提示是防御劫持攻击的有效手段，需进一步研究提升代理安全性。

Abstract: Since the advent of large language models, prompt engineering now enables the rapid, low-effort creation of diverse autonomous agents that are already in widespread use. Yet this convenience raises urgent concerns about the safety, robustness, and behavioral consistency of the underlying prompts, along with the pressing challenge of preventing those prompts from being exposed to user's attempts. In this paper, we propose the ''Doppelgänger method'' to demonstrate the risk of an agent being hijacked, thereby exposing system instructions and internal information. Next, we define the ''Prompt Alignment Collapse under Adversarial Transfer (PACAT)'' level to evaluate the vulnerability to this adversarial transfer attack. We also propose a ''Caution for Adversarial Transfer (CAT)'' prompt to counter the Doppelgänger method. The experimental results demonstrate that the Doppelgänger method can compromise the agent's consistency and expose its internal information. In contrast, CAT prompts enable effective defense against this adversarial attack.

</details>


### [41] [QUEST: Quality-aware Semi-supervised Table Extraction for Business Documents](https://arxiv.org/abs/2506.14568)
*Eliott Thomas,Mickael Coustaty,Aurelie Joseph,Gaspar Deloin,Elodie Carel,Vincent Poulain D'Andecy,Jean-Marc Ogier*

Main category: cs.AI

TL;DR: QUEST是一个质量感知的半监督表格提取框架，通过评估提取表格的结构和上下文特征来改进伪标签选择，显著提升了表格提取的准确性和完整性。


<details>
  <summary>Details</summary>
Motivation: 商业文档中的表格提取（TE）因标注稀疏和多阶段流程易出错而具有挑战性。现有半监督学习方法依赖的置信度分数无法准确反映提取质量。

Method: QUEST引入了一个新颖的质量评估模型，预测F1分数而非依赖置信度指标，并结合多样性度量（如DPP、Vendi分数、IntDiv）减轻确认偏差。

Result: 在商业数据集上，QUEST将F1分数从64%提升至74%，空预测减少45%；在DocILE基准测试中，F1分数从42%提升至50%，空预测减少19%。

Conclusion: QUEST的可解释质量评估和对标注稀缺的鲁棒性，使其特别适合结构一致性和数据完整性要求高的商业文档。

Abstract: Automating table extraction (TE) from business documents is critical for industrial workflows but remains challenging due to sparse annotations and error-prone multi-stage pipelines. While semi-supervised learning (SSL) can leverage unlabeled data, existing methods rely on confidence scores that poorly reflect extraction quality. We propose QUEST, a Quality-aware Semi-supervised Table extraction framework designed for business documents. QUEST introduces a novel quality assessment model that evaluates structural and contextual features of extracted tables, trained to predict F1 scores instead of relying on confidence metrics. This quality-aware approach guides pseudo-label selection during iterative SSL training, while diversity measures (DPP, Vendi score, IntDiv) mitigate confirmation bias. Experiments on a proprietary business dataset (1000 annotated + 10000 unannotated documents) show QUEST improves F1 from 64% to 74% and reduces empty predictions by 45% (from 12% to 6.5%). On the DocILE benchmark (600 annotated + 20000 unannotated documents), QUEST achieves a 50% F1 score (up from 42%) and reduces empty predictions by 19% (from 27% to 22%). The framework's interpretable quality assessments and robustness to annotation scarcity make it particularly suited for business documents, where structural consistency and data completeness are paramount.

</details>


### [42] [Enhancing Symbolic Machine Learning by Subsymbolic Representations](https://arxiv.org/abs/2506.14569)
*Stephen Roth,Lennart Baur,Derian Boer,Stefan Kramer*

Main category: cs.AI

TL;DR: 论文提出了一种通过神经嵌入增强符号机器学习的方法，在简单场景中优于传统神经符号AI系统。


<details>
  <summary>Details</summary>
Motivation: 神经符号AI系统（如LTN和DeepProbLog）在复杂场景中表现优异，但在简单场景（如判别式机器学习）中效率不足。

Method: 通过为符号机器学习方案（如TILDE）提供神经嵌入，特别是常量的相似性谓词，优化嵌入以适应符号理论。

Result: 在三个真实领域实验中，该方法在F1分数上优于所有基线方法。

Conclusion: 该方法不仅适用于当前场景，还可扩展到实例相似性、类比推理和命题化等更广泛的应用。

Abstract: The goal of neuro-symbolic AI is to integrate symbolic and subsymbolic AI approaches, to overcome the limitations of either. Prominent systems include Logic Tensor Networks (LTN) or DeepProbLog, which offer neural predicates and end-to-end learning. The versatility of systems like LTNs and DeepProbLog, however, makes them less efficient in simpler settings, for instance, for discriminative machine learning, in particular in domains with many constants. Therefore, we follow a different approach: We propose to enhance symbolic machine learning schemes by giving them access to neural embeddings. In the present paper, we show this for TILDE and embeddings of constants used by TILDE in similarity predicates. The approach can be fine-tuned by further refining the embeddings depending on the symbolic theory. In experiments in three real-world domain, we show that this simple, yet effective, approach outperforms all other baseline methods in terms of the F1 score. The approach could be useful beyond this setting: Enhancing symbolic learners in this way could be extended to similarities between instances (effectively working like kernels within a logical language), for analogical reasoning, or for propositionalization.

</details>


### [43] [From Points to Places: Towards Human Mobility-Driven Spatiotemporal Foundation Models via Understanding Places](https://arxiv.org/abs/2506.14570)
*Mohammad Hashemi,Andreas Zufle*

Main category: cs.AI

TL;DR: 提出了一种新的空间基础模型，用于处理人类移动数据的时空复杂性，旨在实现可扩展、可转移的分析。


<details>
  <summary>Details</summary>
Motivation: 现有基础模型在语言和视觉领域表现优异，但难以应对移动数据的空间、时间和语义复杂性，因此需要开发专门的空间基础模型。

Method: 从离散的兴趣点建模转向理解动态、上下文丰富的区域（即“地方”），并提出研究方向和高效学习方法。

Result: 提出了一种新类别的空间基础模型，能够整合地理位置语义和多尺度人类移动数据。

Conclusion: 该模型有望推动下一代地理空间智能的发展，支持个性化地点发现、物流优化和城市规划等应用。

Abstract: Capturing human mobility is essential for modeling how people interact with and move through physical spaces, reflecting social behavior, access to resources, and dynamic spatial patterns. To support scalable and transferable analysis across diverse geographies and contexts, there is a need for a generalizable foundation model for spatiotemporal data. While foundation models have transformed language and vision, they remain limited in handling the unique challenges posed by the spatial, temporal, and semantic complexity of mobility data. This vision paper advocates for a new class of spatial foundation models that integrate geolocation semantics with human mobility across multiple scales. Central to our vision is a shift from modeling discrete points of interest to understanding places: dynamic, context-rich regions shaped by human behavior and mobility that may comprise many places of interest. We identify key gaps in adaptability, scalability, and multi-granular reasoning, and propose research directions focused on modeling places and enabling efficient learning. Our goal is to guide the development of scalable, context-aware models for next-generation geospatial intelligence. These models unlock powerful applications ranging from personalized place discovery and logistics optimization to urban planning, ultimately enabling smarter and more responsive spatial decision-making.

</details>


### [44] [AgentDistill: Training-Free Agent Distillation with Generalizable MCP Boxes](https://arxiv.org/abs/2506.14728)
*Jiahao Qiu,Xinzhe Juan,Yimin Wang,Ling Yang,Xuan Qi,Tongcheng Zhang,Jiacheng Guo,Yifu Lu,Zixin Yao,Hongru Wang,Shilong Liu,Xun Jiang,Liu Leqi,Mengdi Wang*

Main category: cs.AI

TL;DR: AgentDistill是一种无需训练的智能体蒸馏框架，通过直接复用教师智能体生成的模块化任务解决模块（MCPs），实现高效知识迁移，使小型语言模型智能体在跨领域任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有智能体蒸馏方法在动态规划和适应新环境方面表现不佳，需要一种更高效且无需训练的方法来提升小型智能体的能力。

Method: 提出AgentDistill框架，通过复用教师智能体生成的Model-Context-Protocols（MCPs）模块，实现知识迁移。

Result: 实验表明，基于小型语言模型的学生智能体性能接近使用大型LLMs（如GPT-4o）的系统。

Conclusion: AgentDistill为构建可扩展且经济高效的智能体提供了有效解决方案。

Abstract: While knowledge distillation has become a mature field for compressing large language models (LLMs) into smaller ones by aligning their outputs or internal representations, the distillation of LLM-based agents, which involve planning, memory, and tool use, remains relatively underexplored. Existing agent distillation methods typically replay full teacher trajectories or imitate step-by-step teacher tool usage, but they often struggle to train student agents to dynamically plan and act in novel environments. We propose AgentDistill, a novel, training-free agent distillation framework that enables efficient and scalable knowledge transfer via direct reuse of Model-Context-Protocols (MCPs), which are structured and reusable task-solving modules autonomously generated by teacher agents. The reuse of these distilled MCPs enables student agents to generalize their capabilities across domains and solve new problems with minimal supervision or human intervention. Experiments on biomedical and mathematical benchmarks demonstrate that our distilled student agents, built on small language models, can achieve performance comparable to advanced systems using large LLMs such as OctoTools (GPT-4o), highlighting the effectiveness of our framework in building scalable and cost-efficient intelligent agents.

</details>


### [45] [Optimizing Length Compression in Large Reasoning Models](https://arxiv.org/abs/2506.14755)
*Zhengxiang Cheng,Dongping Chen,Mingyang Fu,Tianyi Zhou*

Main category: cs.AI

TL;DR: 论文提出LC-R1方法，通过Brevity和Sufficiency原则优化大型推理模型，减少冗余推理链，实现序列长度减少50%且精度仅下降2%。


<details>
  <summary>Details</summary>
Motivation: 大型推理模型（LRMs）常产生冗余推理链，存在无效思考问题，需优化以提高效率。

Method: 提出LC-R1方法，基于GRPO，结合Length Reward和Compress Reward，消除无效推理部分。

Result: 实验显示LC-R1显著减少序列长度（~50%），精度仅下降~2%，达到高效压缩。

Conclusion: LC-R1验证了其鲁棒性，为开发更高效LRMs提供了新思路。

Abstract: Large Reasoning Models (LRMs) have achieved remarkable success, yet they often suffer from producing unnecessary and verbose reasoning chains. We identify a core aspect of this issue as "invalid thinking" -- models tend to repeatedly double-check their work after having derived the correct answer. To address this specific inefficiency, we move beyond the general principles of Efficacy and Efficiency to propose two new, fine-grained principles: Brevity, which advocates for eliminating redundancy, and Sufficiency, which ensures critical reasoning steps are preserved. Guided by these principles, we introduce LC-R1, a post-training method based on Group Relative Policy Optimization (GRPO). LC-R1 employs a novel combination of a Length Reward for overall conciseness and a Compress Reward that is specifically designed to remove the invalid portion of the thinking process. Extensive experiments on multiple reasoning benchmarks demonstrate that LC-R1 achieves a significant reduction in sequence length (~50%) with only a marginal (~2%) drop in accuracy, achieving a favorable trade-off point on the Pareto frontier that prioritizes high compression. Our analysis further validates the robustness of LC-R1 and provides valuable insights for developing more powerful yet computationally efficient LRMs. Our code is released at https://github.com/zxiangx/LC-R1.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [46] [ClimateChat: Designing Data and Methods for Instruction Tuning LLMs to Answer Climate Change Queries](https://arxiv.org/abs/2506.13796)
*Zhou Chen,Xiao Wang,Yuanhong Liao,Ming Lin,Yuqi Bai*

Main category: cs.CL

TL;DR: 本研究提出了一种自动化构建气候相关指令数据的方法，并创建了ClimateChat-Corpus数据集，用于微调开源大语言模型（LLMs），显著提升了气候问答任务的性能。


<details>
  <summary>Details</summary>
Motivation: 全球气候变化问题日益严峻，但现有研究在高效生成高精度气候指令数据方面不足，限制了气候LLMs的发展。

Method: 通过从文档中提取事实和背景知识生成指令，结合网络爬取和种子指令收集增强数据多样性，构建ClimateChat-Corpus数据集并微调LLMs。

Result: ClimateChat在气候问答任务中表现显著提升，并验证了不同基础模型和指令数据对性能的影响。

Conclusion: 研究为构建气候指令数据和训练气候专用LLMs提供了重要参考，强调选择合适基础模型的重要性。

Abstract: As the issue of global climate change becomes increasingly severe, the demand for research in climate science continues to grow. Natural language processing technologies, represented by Large Language Models (LLMs), have been widely applied to climate change-specific research, providing essential information support for decision-makers and the public. Some studies have improved model performance on relevant tasks by constructing climate change-related instruction data and instruction-tuning LLMs. However, current research remains inadequate in efficiently producing large volumes of high-precision instruction data for climate change, which limits further development of climate change LLMs. This study introduces an automated method for constructing instruction data. The method generates instructions using facts and background knowledge from documents and enhances the diversity of the instruction data through web scraping and the collection of seed instructions. Using this method, we constructed a climate change instruction dataset, named ClimateChat-Corpus, which was used to fine-tune open-source LLMs, resulting in an LLM named ClimateChat. Evaluation results show that ClimateChat significantly improves performance on climate change question-and-answer tasks. Additionally, we evaluated the impact of different base models and instruction data on LLM performance and demonstrated its capability to adapt to a wide range of climate change scientific discovery tasks, emphasizing the importance of selecting an appropriate base model for instruction tuning. This research provides valuable references and empirical support for constructing climate change instruction data and training climate change-specific LLMs.

</details>


### [47] [Investigating the interaction of linguistic and mathematical reasoning in language models using multilingual number puzzles](https://arxiv.org/abs/2506.13886)
*Antara Raaghavi Bhattacharya,Isabel Papadimitriou,Kathryn Davidson,David Alvarez-Melis*

Main category: cs.CL

TL;DR: LLMs struggle with cross-linguistic numeral systems due to inability to infer implicit compositional rules, unlike humans who use linguistic understanding.


<details>
  <summary>Details</summary>
Motivation: Investigate why LLMs fail at linguistic-mathematical puzzles involving diverse numeral systems, while humans succeed.

Method: Conduct experiments to separate linguistic and mathematical aspects of numbers, testing LLMs' performance with explicit vs. implicit mathematical operations.

Result: LLMs only solve problems when mathematical operations are explicitly marked; they lack understanding of implicit numeral structure.

Conclusion: Flexibly inferring compositional rules from implicit patterns remains a challenge for current reasoning models.

Abstract: Across languages, numeral systems vary widely in how they construct and combine numbers. While humans consistently learn to navigate this diversity, large language models (LLMs) struggle with linguistic-mathematical puzzles involving cross-linguistic numeral systems, which humans can learn to solve successfully. We investigate why this task is difficult for LLMs through a series of experiments that untangle the linguistic and mathematical aspects of numbers in language. Our experiments establish that models cannot consistently solve such problems unless the mathematical operations in the problems are explicitly marked using known symbols ($+$, $\times$, etc, as in "twenty + three"). In further ablation studies, we probe how individual parameters of numeral construction and combination affect performance. While humans use their linguistic understanding of numbers to make inferences about the implicit compositional structure of numerals, LLMs seem to lack this notion of implicit numeral structure. We conclude that the ability to flexibly infer compositional rules from implicit patterns in human-scale data remains an open challenge for current reasoning models.

</details>


### [48] [VL-GenRM: Enhancing Vision-Language Verification via Vision Experts and Iterative Training](https://arxiv.org/abs/2506.13888)
*Jipeng Zhang,Kehao Miao,Renjie Pi,Zhaowei Wang,Runtao Liu,Rui Pan,Tong Zhang*

Main category: cs.CL

TL;DR: 本文提出了一种迭代训练框架，通过结合视觉专家、思维链推理和基于边际的拒绝采样，解决了视觉语言奖励模型（VL-RM）训练中的自举困境和模态偏差问题。


<details>
  <summary>Details</summary>
Motivation: 视觉语言奖励模型（VL-RM）在强化微调中具有关键作用，但其训练面临自举困境和模态偏差的挑战，需要新的方法来解决。

Method: 提出了一种迭代训练框架，结合视觉专家、思维链（CoT）推理和基于边际的拒绝采样，优化偏好数据集和结构化反馈。

Result: 实验表明，该方法在幻觉检测和多模态推理任务中表现优异，显著提升了视觉语言模型的性能。

Conclusion: 该框架有效解决了VL-RM训练中的核心问题，为视觉语言模型的强化学习对齐提供了新思路。

Abstract: Reinforcement Fine-Tuning (RFT) with verifiable rewards has advanced large language models but remains underexplored for Vision-Language (VL) models. The Vision-Language Reward Model (VL-RM) is key to aligning VL models by providing structured feedback, yet training effective VL-RMs faces two major challenges. First, the bootstrapping dilemma arises as high-quality training data depends on already strong VL models, creating a cycle where self-generated supervision reinforces existing biases. Second, modality bias and negative example amplification occur when VL models hallucinate incorrect visual attributes, leading to flawed preference data that further misguides training. To address these issues, we propose an iterative training framework leveraging vision experts, Chain-of-Thought (CoT) rationales, and Margin-based Rejection Sampling. Our approach refines preference datasets, enhances structured critiques, and iteratively improves reasoning. Experiments across VL-RM benchmarks demonstrate superior performance in hallucination detection and multimodal reasoning, advancing VL model alignment with reinforcement learning.

</details>


### [49] [EmoNews: A Spoken Dialogue System for Expressive News Conversations](https://arxiv.org/abs/2506.13894)
*Ryuki Matsuura,Shikhar Bharadwaj,Jiarui Liu,Dhatchi Kunde Govindarajan*

Main category: cs.CL

TL;DR: 开发了一个基于情感调节的任务导向口语对话系统（SDS），用于新闻对话，通过上下文线索实现更具同理心的交流。


<details>
  <summary>Details</summary>
Motivation: 尽管情感文本转语音（TTS）技术有所进步，但任务导向的情感SDS研究仍不足，原因在于SDS与情感TTS研究的分离以及缺乏社会目标的标准评估指标。

Method: 利用基于大型语言模型（LLM）的情感分析器识别合适的情感，并使用PromptTTS合成上下文相关的情感语音。同时提出了情感SDS的主观评估量表。

Result: 实验表明，所提出的情感SDS在情感调节和参与度方面优于基线系统。

Conclusion: 结果表明语音情感在提升对话参与度中的关键作用，所有源代码已开源。

Abstract: We develop a task-oriented spoken dialogue system (SDS) that regulates emotional speech based on contextual cues to enable more empathetic news conversations. Despite advancements in emotional text-to-speech (TTS) techniques, task-oriented emotional SDSs remain underexplored due to the compartmentalized nature of SDS and emotional TTS research, as well as the lack of standardized evaluation metrics for social goals. We address these challenges by developing an emotional SDS for news conversations that utilizes a large language model (LLM)-based sentiment analyzer to identify appropriate emotions and PromptTTS to synthesize context-appropriate emotional speech. We also propose subjective evaluation scale for emotional SDSs and judge the emotion regulation performance of the proposed and baseline systems. Experiments showed that our emotional SDS outperformed a baseline system in terms of the emotion regulation and engagement. These results suggest the critical role of speech emotion for more engaging conversations. All our source code is open-sourced at https://github.com/dhatchi711/espnet-emotional-news/tree/emo-sds/egs2/emo_news_sds/sds1

</details>


### [50] [Alignment Quality Index (AQI) : Beyond Refusals: AQI as an Intrinsic Alignment Diagnostic via Latent Geometry, Cluster Divergence, and Layer wise Pooled Representations](https://arxiv.org/abs/2506.13901)
*Abhilekh Borah,Chhavi Sharma,Danush Khanna,Utkarsh Bhatt,Gurpreet Singh,Hasnat Md Abdullah,Raghav Kaushik Ravi,Vinija Jain,Jyoti Patel,Shubham Singh,Vasu Sharma,Arpita Vats,Rahul Raja,Aman Chadha,Amitava Das*

Main category: cs.CL

TL;DR: 论文提出了一种新的对齐质量指数（AQI），用于评估大型语言模型的潜在空间中对齐情况，并提出了LITMUS数据集以支持鲁棒评估。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型进入高风险领域，其行为必须可靠地反映人类对齐的价值观和安全约束。当前评估方法存在盲点，如易受越狱攻击和生成随机性的影响。

Method: 通过结合Davies-Bouldin Score、Dunn Index等几何指标，AQI分析潜在空间中安全与不安全激活的分离情况，检测隐藏的对齐问题和越狱风险。

Result: 实证测试表明，AQI与外部评判标准相关，并能揭示传统拒绝指标遗漏的漏洞。

Conclusion: AQI为行为无关的安全审计提供了鲁棒的解码不变工具，并可作为对齐伪造的早期预警信号。

Abstract: Alignment is no longer a luxury, it is a necessity. As large language models (LLMs) enter high-stakes domains like education, healthcare, governance, and law, their behavior must reliably reflect human-aligned values and safety constraints. Yet current evaluations rely heavily on behavioral proxies such as refusal rates, G-Eval scores, and toxicity classifiers, all of which have critical blind spots. Aligned models are often vulnerable to jailbreaking, stochasticity of generation, and alignment faking.
  To address this issue, we introduce the Alignment Quality Index (AQI). This novel geometric and prompt-invariant metric empirically assesses LLM alignment by analyzing the separation of safe and unsafe activations in latent space. By combining measures such as the Davies-Bouldin Score (DBS), Dunn Index (DI), Xie-Beni Index (XBI), and Calinski-Harabasz Index (CHI) across various formulations, AQI captures clustering quality to detect hidden misalignments and jailbreak risks, even when outputs appear compliant. AQI also serves as an early warning signal for alignment faking, offering a robust, decoding invariant tool for behavior agnostic safety auditing.
  Additionally, we propose the LITMUS dataset to facilitate robust evaluation under these challenging conditions. Empirical tests on LITMUS across different models trained under DPO, GRPO, and RLHF conditions demonstrate AQI's correlation with external judges and ability to reveal vulnerabilities missed by refusal metrics. We make our implementation publicly available to foster future research in this area.

</details>


### [51] [ASMR: Augmenting Life Scenario using Large Generative Models for Robotic Action Reflection](https://arxiv.org/abs/2506.13956)
*Shang-Chi Tsai,Seiya Kawano,Angel Garcia Contreras,Koichiro Yoshino,Yun-Nung Chen*

Main category: cs.CL

TL;DR: 论文提出了一种新颖的数据增强框架，通过结合语言模型和稳定扩散模型生成对话和环境图像，以提升机器人对用户意图的理解能力。


<details>
  <summary>Details</summary>
Motivation: 在机器人辅助日常活动时，结合视觉和语言信息的多模态分类任务对意图理解至关重要，但大规模数据集的获取困难。

Method: 利用大型语言模型模拟对话和环境上下文，再通过稳定扩散模型生成相关图像，扩充数据集以优化多模态模型。

Result: 实验结果表明，该方法显著提升了机器人的动作选择能力，达到了最先进的性能。

Conclusion: 提出的框架有效解决了数据不足问题，提升了机器人对用户意图的理解和响应能力。

Abstract: When designing robots to assist in everyday human activities, it is crucial to enhance user requests with visual cues from their surroundings for improved intent understanding. This process is defined as a multimodal classification task. However, gathering a large-scale dataset encompassing both visual and linguistic elements for model training is challenging and time-consuming. To address this issue, our paper introduces a novel framework focusing on data augmentation in robotic assistance scenarios, encompassing both dialogues and related environmental imagery. This approach involves leveraging a sophisticated large language model to simulate potential conversations and environmental contexts, followed by the use of a stable diffusion model to create images depicting these environments. The additionally generated data serves to refine the latest multimodal models, enabling them to more accurately determine appropriate actions in response to user interactions with the limited target data. Our experimental results, based on a dataset collected from real-world scenarios, demonstrate that our methodology significantly enhances the robot's action selection capabilities, achieving the state-of-the-art performance.

</details>


### [52] [Are manual annotations necessary for statutory interpretations retrieval?](https://arxiv.org/abs/2506.13965)
*Aleksander Smywiński-Pohl,Tomer Libal,Adam Kaczmarczyk,Magdalena Król*

Main category: cs.CL

TL;DR: 论文探讨了法律研究中自动化注释过程的必要性，研究了注释数量、候选句子选择及大型语言模型（LLM）在自动化注释中的作用。


<details>
  <summary>Details</summary>
Motivation: 法律研究中，法官对法律概念的扩展解释需要大量手动注释，成本高昂且需重复进行，因此研究如何优化或自动化这一过程。

Method: 通过实验确定每个法律概念的最佳注释数量、候选句子选择方式（随机或优选）以及LLM在自动化注释中的效果。

Result: 研究发现注释数量、候选句子选择方式及LLM自动化注释对模型性能有显著影响。

Conclusion: 自动化注释过程可以显著降低成本，同时保持模型性能，为法律研究提供高效解决方案。

Abstract: One of the elements of legal research is looking for cases where judges have extended the meaning of a legal concept by providing interpretations of what a concept means or does not mean. This allow legal professionals to use such interpretations as precedents as well as laymen to better understand the legal concept. The state-of-the-art approach for retrieving the most relevant interpretations for these concepts currently depends on the ranking of sentences and the training of language models over annotated examples. That manual annotation process can be quite expensive and need to be repeated for each such concept, which prompted recent research in trying to automate this process. In this paper, we highlight the results of various experiments conducted to determine the volume, scope and even the need for manual annotation. First of all, we check what is the optimal number of annotations per a legal concept. Second, we check if we can draw the sentences for annotation randomly or there is a gain in the performance of the model, when only the best candidates are annotated. As the last question we check what is the outcome of automating the annotation process with the help of an LLM.

</details>


### [53] [AI shares emotion with humans across languages and cultures](https://arxiv.org/abs/2506.13978)
*Xiuwen Wu,Hao Wang,Zhiang Yan,Xiaohan Tang,Pengfei Xu,Wai-Ting Siok,Ping Li,Jia-Hong Gao,Bingjiang Lyu,Lang Qin*

Main category: cs.CL

TL;DR: 研究发现，AI与人类在情感表达上具有结构一致性，且可以通过心理学基础的情感概念精确调控AI的情感输出。


<details>
  <summary>Details</summary>
Motivation: 探讨AI是否能够像人类一样表达情感，以及如何控制其情感输出。

Method: 通过跨语言文化群体和模型家族评估人类与AI的情感对齐，使用可解释的LLM特征分析情感空间。

Result: LLM的情感空间与人类感知结构一致，且能准确预测行为数据。情感输出可通过人类情感概念稳定调控。

Conclusion: AI不仅与人类共享情感表征，还能通过心理学基础的情感概念精确引导其情感输出。

Abstract: Effective and safe human-machine collaboration requires the regulated and meaningful exchange of emotions between humans and artificial intelligence (AI). Current AI systems based on large language models (LLMs) can provide feedback that makes people feel heard. Yet it remains unclear whether LLMs represent emotion in language as humans do, or whether and how the emotional tone of their output can be controlled. We assess human-AI emotional alignment across linguistic-cultural groups and model-families, using interpretable LLM features translated from concept-sets for over twenty nuanced emotion categories (including six basic emotions). Our analyses reveal that LLM-derived emotion spaces are structurally congruent with human perception, underpinned by the fundamental affective dimensions of valence and arousal. Furthermore, these emotion-related features also accurately predict large-scale behavioural data on word ratings along these two core dimensions, reflecting both universal and language-specific patterns. Finally, by leveraging steering vectors derived solely from human-centric emotion concepts, we show that model expressions can be stably and naturally modulated across distinct emotion categories, which provides causal evidence that human emotion concepts can be used to systematically induce LLMs to produce corresponding affective states when conveying content. These findings suggest AI not only shares emotional representations with humans but its affective outputs can be precisely guided using psychologically grounded emotion concepts.

</details>


### [54] [Lost in the Mix: Evaluating LLM Understanding of Code-Switched Text](https://arxiv.org/abs/2506.14012)
*Amr Mohamed,Yang Zhang,Michalis Vazirgiannis,Guokan Shang*

Main category: cs.CL

TL;DR: 本文系统评估了大型语言模型（LLMs）对代码切换（CSW）文本的理解能力，发现外语词干扰英语时会降低性能，但将英语嵌入其他语言可能提升理解。微调是缓解性能下降的稳定方法。


<details>
  <summary>Details</summary>
Motivation: 代码切换在多语言社区和在线内容中普遍存在，LLMs常处理此类混合语言文本，因此需了解其处理能力。

Method: 通过生成代码切换版本的推理和理解基准，评估LLMs的表现。

Result: 外语词干扰英语时性能下降，但英语嵌入其他语言可能提升理解；提示效果不稳定，微调更有效。

Conclusion: 微调是改善LLMs处理代码切换文本的稳定方法。

Abstract: Code-switching (CSW) is the act of alternating between two or more languages within a single discourse. This phenomenon is widespread in multilingual communities, and increasingly prevalent in online content, where users naturally mix languages in everyday communication. As a result, Large Language Models (LLMs), now central to content processing and generation, are frequently exposed to code-switched inputs. Given their widespread use, it is crucial to understand how LLMs process and reason about such mixed-language text. This paper presents a systematic evaluation of LLM comprehension under code-switching by generating CSW variants of established reasoning and comprehension benchmarks. While degradation is evident when foreign tokens disrupt English text$\unicode{x2013}$even under linguistic constraints$\unicode{x2013}$embedding English into other languages often improves comprehension. Though prompting yields mixed results, fine-tuning offers a more stable path to degradation mitigation.

</details>


### [55] [MultiFinBen: A Multilingual, Multimodal, and Difficulty-Aware Benchmark for Financial LLM Evaluation](https://arxiv.org/abs/2506.14028)
*Xueqing Peng,Lingfei Qian,Yan Wang,Ruoyu Xiang,Yueru He,Yang Ren,Mingyang Jiang,Jeff Zhao,Huan He,Yi Han,Yun Feng,Yuechen Jiang,Yupeng Cao,Haohang Li,Yangyang Yu,Xiaoyu Wang,Penglei Gao,Shengyuan Lin,Keyi Wang,Shanshan Yang,Yilun Zhao,Zhiwei Liu,Peng Lu,Jerry Huang,Suyuchen Wang,Triantafillos Papadopoulos,Polydoros Giannouris,Efstathia Soufleri,Nuo Chen,Guojun Xiong,Zhiyang Deng,Yijia Zhao,Mingquan Lin,Meikang Qiu,Kaleb E Smith,Arman Cohan,Xiao-Yang Liu,Jimin Huang,Alejandro Lopez-Lira,Xi Chen,Junichi Tsujii,Jian-Yun Nie,Sophia Ananiadou,Qianqian Xie*

Main category: cs.CL

TL;DR: MultiFinBen是首个针对全球金融领域的多语言多模态基准测试，评估LLM在跨模态和跨语言任务中的表现，揭示现有模型在复杂金融任务中的不足。


<details>
  <summary>Details</summary>
Motivation: 现有金融NLP基准测试局限于单语言和单模态，无法反映真实金融场景的复杂性，因此需要更全面的评估工具。

Method: 提出MultiFinBen基准，包含多语言和多模态任务（如PolyFiQA和OCR任务），并采用动态难度选择机制构建平衡数据集。

Result: 评估22个先进模型，发现它们在复杂跨语言和多模态金融任务中表现不佳。

Conclusion: MultiFinBen为金融研究和应用提供了透明、可复现的评估标准，推动领域进步。

Abstract: Recent advances in large language models (LLMs) have accelerated progress in financial NLP and applications, yet existing benchmarks remain limited to monolingual and unimodal settings, often over-relying on simple tasks and failing to reflect the complexity of real-world financial communication. We introduce MultiFinBen, the first multilingual and multimodal benchmark tailored to the global financial domain, evaluating LLMs across modalities (text, vision, audio) and linguistic settings (monolingual, bilingual, multilingual) on domain-specific tasks. We introduce two novel tasks, including PolyFiQA-Easy and PolyFiQA-Expert, the first multilingual financial benchmarks requiring models to perform complex reasoning over mixed-language inputs; and EnglishOCR and SpanishOCR, the first OCR-embedded financial QA tasks challenging models to extract and reason over information from visual-text financial documents. Moreover, we propose a dynamic, difficulty-aware selection mechanism and curate a compact, balanced benchmark rather than simple aggregation existing datasets. Extensive evaluation of 22 state-of-the-art models reveals that even the strongest models, despite their general multimodal and multilingual capabilities, struggle dramatically when faced with complex cross-lingual and multimodal tasks in financial domain. MultiFinBen is publicly released to foster transparent, reproducible, and inclusive progress in financial studies and applications.

</details>


### [56] [An Interdisciplinary Review of Commonsense Reasoning and Intent Detection](https://arxiv.org/abs/2506.14040)
*Md Nazmus Sakib*

Main category: cs.CL

TL;DR: 综述探讨了常识推理和意图检测的最新进展，分析了28篇论文，总结了方法和应用。


<details>
  <summary>Details</summary>
Motivation: 解决自然语言理解中的常识推理和意图检测两大挑战。

Method: 按方法论和应用分类，分析零样本学习、文化适应、结构化评估和交互式上下文中的常识推理，以及开放集模型、生成式方法、聚类和以人为中心的系统中的意图检测。

Result: 揭示了更自适应、多语言和上下文感知模型的趋势，并指出了在基础、泛化和基准设计方面的关键差距。

Conclusion: 通过结合NLP和HCI的见解，为未来研究提供了方向。

Abstract: This review explores recent advances in commonsense reasoning and intent detection, two key challenges in natural language understanding. We analyze 28 papers from ACL, EMNLP, and CHI (2020-2025), organizing them by methodology and application. Commonsense reasoning is reviewed across zero-shot learning, cultural adaptation, structured evaluation, and interactive contexts. Intent detection is examined through open-set models, generative formulations, clustering, and human-centered systems. By bridging insights from NLP and HCI, we highlight emerging trends toward more adaptive, multilingual, and context-aware models, and identify key gaps in grounding, generalization, and benchmark design.

</details>


### [57] [Ace-CEFR -- A Dataset for Automated Evaluation of the Linguistic Difficulty of Conversational Texts for LLM Applications](https://arxiv.org/abs/2506.14046)
*David Kogan,Max Schumacher,Sam Nguyen,Masanori Suzuki,Melissa Smith,Chloe Sophia Bellows,Jared Bernstein*

Main category: cs.CL

TL;DR: Ace-CEFR是一个专家标注的英语对话文本难度数据集，用于训练和过滤大型语言模型（LLMs）。实验表明，基于Ace-CEFR训练的模型在难度评估上比人类专家更准确，且适合生产环境。


<details>
  <summary>Details</summary>
Motivation: 评估短对话文本的语言难度对训练和过滤LLMs有迫切需求。

Method: 引入Ace-CEFR数据集，并使用Transformer模型和LLMs进行实验。

Result: 模型在难度评估上比人类专家更准确，且延迟适合生产环境。

Conclusion: Ace-CEFR数据集公开发布，供研究和开发使用。

Abstract: There is an unmet need to evaluate the language difficulty of short, conversational passages of text, particularly for training and filtering Large Language Models (LLMs). We introduce Ace-CEFR, a dataset of English conversational text passages expert-annotated with their corresponding level of text difficulty. We experiment with several models on Ace-CEFR, including Transformer-based models and LLMs. We show that models trained on Ace-CEFR can measure text difficulty more accurately than human experts and have latency appropriate to production environments. Finally, we release the Ace-CEFR dataset to the public for research and development.

</details>


### [58] [Automatic Extraction of Clausal Embedding Based on Large-Scale English Text Data](https://arxiv.org/abs/2506.14064)
*Iona Carslaw,Sivan Milton,Nicolas Navarre,Ciyang Qing,Wataru Uegaki*

Main category: cs.CL

TL;DR: 论文提出了一种利用选区分析和启发式规则从大规模文本数据中检测和标注英语嵌入式从句的方法，并基于开源语料库Dolma提取了一个大型自然发生嵌入式从句数据集。


<details>
  <summary>Details</summary>
Motivation: 当前研究依赖人工构造的语言例句来研究嵌入式从句，缺乏从大规模语料库中获取的统计信息和自然例句。

Method: 采用选区分析和一组解析启发式规则，从大规模文本数据中检测和标注自然发生的英语嵌入式从句。

Result: 开发了一个工具，并在手工标注的自然例句数据集GECS上进行了评估，同时从Dolma语料库中提取了一个大型嵌入式从句数据集。

Conclusion: 该方法为研究嵌入式从句提供了更丰富的自然例句和统计信息，弥补了当前研究的不足。

Abstract: For linguists, embedded clauses have been of special interest because of their intricate distribution of syntactic and semantic features. Yet, current research relies on schematically created language examples to investigate these constructions, missing out on statistical information and naturally-occurring examples that can be gained from large language corpora. Thus, we present a methodological approach for detecting and annotating naturally-occurring examples of English embedded clauses in large-scale text data using constituency parsing and a set of parsing heuristics. Our tool has been evaluated on our dataset Golden Embedded Clause Set (GECS), which includes hand-annotated examples of naturally-occurring English embedded clause sentences. Finally, we present a large-scale dataset of naturally-occurring English embedded clauses which we have extracted from the open-source corpus Dolma using our extraction tool.

</details>


### [59] [Abstract Meaning Representation for Hospital Discharge Summarization](https://arxiv.org/abs/2506.14101)
*Paul Landes,Sitara Rao,Aaron Jeremy Chaise,Barbara Di Eugenio*

Main category: cs.CL

TL;DR: 论文提出了一种结合语言图和深度学习模型的新方法，用于解决大型语言模型（LLMs）在自动生成出院摘要时的幻觉问题，以提高内容的可信度和可追溯性。


<details>
  <summary>Details</summary>
Motivation: 在临床领域，LLMs的幻觉问题可能导致严重后果。自动生成出院摘要可以减轻医生负担，但需确保内容的可靠性和来源可追溯。

Method: 结合语言图和深度学习模型，提出新方法以增强自动摘要的可信度和内容来源的可追溯性。

Result: 在MIMIC-III公开数据集和Anonymous Hospital的临床笔记上，该方法表现出显著的可靠性。

Conclusion: 该方法为自动生成临床出院摘要提供了可靠且可追溯的解决方案，有望减轻医生负担并提高文档质量。

Abstract: The Achilles heel of Large Language Models (LLMs) is hallucination, which has drastic consequences for the clinical domain. This is particularly important with regards to automatically generating discharge summaries (a lengthy medical document that summarizes a hospital in-patient visit). Automatically generating these summaries would free physicians to care for patients and reduce documentation burden. The goal of this work is to discover new methods that combine language-based graphs and deep learning models to address provenance of content and trustworthiness in automatic summarization. Our method shows impressive reliability results on the publicly available Medical Information Mart for Intensive III (MIMIC-III) corpus and clinical notes written by physicians at Anonymous Hospital. rovide our method, generated discharge ary output examples, source code and trained models.

</details>


### [60] [Essential-Web v1.0: 24T tokens of organized web data](https://arxiv.org/abs/2506.14111)
*Essential AI,:,Andrew Hojel,Michael Pust,Tim Romanski,Yash Vanjani,Ritvik Kapila,Mohit Parmar,Adarsh Chaluvaraju,Alok Tripathy,Anil Thomas,Ashish Tanwer,Darsh J Shah,Ishaan Shah,Karl Stratos,Khoi Nguyen,Kurt Smith,Michael Callahan,Peter Rushton,Philip Monk,Platon Mazarakis,Saad Jamal,Saurabh Srivastava,Somanshu Singla,Ashish Vaswani*

Main category: cs.CL

TL;DR: Essential-Web v1.0是一个24万亿token的数据集，每个文档标注了12类分类法，涵盖主题、格式、内容复杂度和质量。通过SQL式过滤，可生成在数学、代码、STEM和医学领域具有竞争力的数据集。


<details>
  <summary>Details</summary>
Motivation: 大规模、高质量预训练数据集的缺乏导致数据管道昂贵且难以获取，Essential-Web v1.0旨在解决这一问题。

Method: 使用EAI-Distill-0.5b模型（0.5b参数）为数据集标注分类标签，并通过SQL式过滤生成特定领域数据集。

Result: 在数学、代码、STEM和医学领域的数据集表现优于或接近SOTA。

Conclusion: Essential-Web v1.0为语言模型提供了高质量、易获取的预训练数据，解决了数据管道的瓶颈问题。

Abstract: Data plays the most prominent role in how language models acquire skills and knowledge. The lack of massive, well-organized pre-training datasets results in costly and inaccessible data pipelines. We present Essential-Web v1.0, a 24-trillion-token dataset in which every document is annotated with a twelve-category taxonomy covering topic, format, content complexity, and quality. Taxonomy labels are produced by EAI-Distill-0.5b, a fine-tuned 0.5b-parameter model that achieves an annotator agreement within 3% of Qwen2.5-32B-Instruct. With nothing more than SQL-style filters, we obtain competitive web-curated datasets in math (-8.0% relative to SOTA), web code (+14.3%), STEM (+24.5%) and medical (+8.6%). Essential-Web v1.0 is available on HuggingFace: https://huggingface.co/datasets/EssentialAI/essential-web-v1.0

</details>


### [61] [Sampling from Your Language Model One Byte at a Time](https://arxiv.org/abs/2506.14123)
*Jonathan Hayase,Alisa Liu,Noah A. Smith,Sewoong Oh*

Main category: cs.CL

TL;DR: 本文提出了一种推理时方法，将任何使用BPE分词器的自回归语言模型转换为字符级或字节级模型，解决了提示边界问题（PBP）并统一了不同分词器的词汇表，提升了模型组合和互操作性。


<details>
  <summary>Details</summary>
Motivation: 现代语言模型普遍使用分词技术，但分词可能导致生成文本的失真（如提示边界问题），且不同分词器之间的不匹配阻碍了模型的组合和互操作性。

Method: 提出一种推理时方法，将BPE分词器的自回归语言模型转换为字符级或字节级模型，保持生成分布不变。

Result: 实验表明，该方法能有效解决PBP问题，并允许不同分词器的模型组合和代理调优，下游评估中表现优于单独模型。

Conclusion: 该方法提升了语言模型的灵活性和性能，解决了分词带来的问题，为模型组合和调优提供了新途径。

Abstract: Tokenization is used almost universally by modern language models, enabling efficient text representation using multi-byte or multi-character tokens. However, prior work has shown that tokenization can introduce distortion into the model's generations. For example, users are often advised not to end their prompts with a space because it prevents the model from including the space as part of the next token. This Prompt Boundary Problem (PBP) also arises in languages such as Chinese and in code generation, where tokens often do not line up with syntactic boundaries. Additionally mismatching tokenizers often hinder model composition and interoperability. For example, it is not possible to directly ensemble models with different tokenizers due to their mismatching vocabularies. To address these issues, we present an inference-time method to convert any autoregressive LM with a BPE tokenizer into a character-level or byte-level LM, without changing its generative distribution at the text level. Our method efficient solves the PBP and is also able to unify the vocabularies of language models with different tokenizers, allowing one to ensemble LMs with different tokenizers at inference time as well as transfer the post-training from one model to another using proxy-tuning. We demonstrate in experiments that the ensemble and proxy-tuned models outperform their constituents on downstream evals.

</details>


### [62] [Can we train ASR systems on Code-switch without real code-switch data? Case study for Singapore's languages](https://arxiv.org/abs/2506.14177)
*Tuan Nguyen,Huy-Dat Tran*

Main category: cs.CL

TL;DR: 研究提出一种短语级混合方法生成合成代码切换数据，用于训练ASR模型，提升其在低资源东南亚语言对上的表现。


<details>
  <summary>Details</summary>
Motivation: 代码切换（CS）在ASR中因数据稀缺和标注成本高而具有挑战性，研究旨在通过合成数据解决这一问题。

Method: 提出短语级混合方法生成合成CS数据，结合单语数据微调预训练ASR模型（Whisper、MMS、SeamlessM4T）。

Result: 实验表明该方法提升了ASR在单语和CS测试中的性能，BM-EN提升最大，其次是TA-EN和ZH-BM。

Conclusion: 该方法为CS-ASR开发提供了一种经济高效的解决方案，对研究和工业界有益。

Abstract: Code-switching (CS), common in multilingual settings, presents challenges for ASR due to scarce and costly transcribed data caused by linguistic complexity. This study investigates building CS-ASR using synthetic CS data. We propose a phrase-level mixing method to generate synthetic CS data that mimics natural patterns. Utilizing monolingual augmented with synthetic phrase-mixed CS data to fine-tune large pretrained ASR models (Whisper, MMS, SeamlessM4T). This paper focuses on three under-resourced Southeast Asian language pairs: Malay-English (BM-EN), Mandarin-Malay (ZH-BM), and Tamil-English (TA-EN), establishing a new comprehensive benchmark for CS-ASR to evaluate the performance of leading ASR models. Experimental results show that the proposed training strategy enhances ASR performance on monolingual and CS tests, with BM-EN showing highest gains, then TA-EN and ZH-BM. This finding offers a cost-effective approach for CS-ASR development, benefiting research and industry.

</details>


### [63] [DCRM: A Heuristic to Measure Response Pair Quality in Preference Optimization](https://arxiv.org/abs/2506.14157)
*Chengyu Huang,Tanya Goyal*

Main category: cs.CL

TL;DR: 论文提出了一种名为DCRM的指标，用于量化偏好优化（PO）中响应对的差异质量，并通过实验验证了DCRM与学习效果的正相关性。


<details>
  <summary>Details</summary>
Motivation: 研究发现，偏好优化性能与底层偏好数据集的关系尚未明确，且响应对的差异可能不符合学习需求。

Method: 使用距离和奖励边际量化响应对的差异，提出DCRM指标，并基于此设计了一种最佳配对方法（best-of-$N^2$）。

Result: 实验表明，DCRM与学习效果正相关，新方法在多个基准测试中优于现有训练集。

Conclusion: DCRM能有效衡量响应对质量，最佳配对方法可提升模型性能。

Abstract: Recent research has attempted to associate preference optimization (PO) performance with the underlying preference datasets. In this work, our observation is that the differences between the preferred response $y^+$ and dispreferred response $y^-$ influence what LLMs can learn, which may not match the desirable differences to learn. Therefore, we use distance and reward margin to quantify these differences, and combine them to get Distance Calibrated Reward Margin (DCRM), a metric that measures the quality of a response pair for PO. Intuitively, DCRM encourages minimal noisy differences and maximal desired differences. With this, we study 3 types of commonly used preference datasets, classified along two axes: the source of the responses and the preference labeling function. We establish a general correlation between higher DCRM of the training set and better learning outcome. Inspired by this, we propose a best-of-$N^2$ pairing method that selects response pairs with the highest DCRM. Empirically, in various settings, our method produces training datasets that can further improve models' performance on AlpacaEval, MT-Bench, and Arena-Hard over the existing training sets.

</details>


### [64] [AsyncSwitch: Asynchronous Text-Speech Adaptation for Code-Switched ASR](https://arxiv.org/abs/2506.14190)
*Tuan Nguyen,Huy-Dat Tran*

Main category: cs.CL

TL;DR: AsyncSwitch是一种异步适应框架，利用大规模文本数据预训练ASR模型，再通过有限语音文本数据微调，显著降低代码切换ASR的WER。


<details>
  <summary>Details</summary>
Motivation: 开发代码切换ASR系统面临语言歧义和多语言数据稀缺的挑战，传统合成音频方法计算成本高且难以扩展。

Method: 采用三阶段方法：1) 在代码切换文本上训练解码器自注意力和前馈层；2) 用有限语音文本数据对齐解码器和编码器；3) 完整微调整个模型。

Result: 在马来语-英语代码切换实验中，Whisper模型WER相对降低9.02%，同时提升单语性能。

Conclusion: AsyncSwitch通过预训练和微调结合，有效提升代码切换ASR性能，同时保持单语能力。

Abstract: Developing code-switched ASR systems is challenging due to language ambiguity and limited exposure to multilingual, code-switched data, while collecting such speech is costly. Prior work generates synthetic audio from text, but these methods are computationally intensive and hard to scale. We introduce AsyncSwitch, a novel asynchronous adaptation framework that leverages large-scale, text-rich web data to pre-expose ASR models to diverse code-switched domains before fine-tuning on paired speech-text corpora. Our three-stage process (1) trains decoder self-attention and feedforward layers on code-switched text, (2) aligns decoder and encoder via cross-attention using limited speech-text data, and (3) fully fine-tunes the entire model. Experiments with Whisper on Malay-English code-switching demonstrate a 9.02% relative WER reduction, while improving monolingual performance in Singlish, Malay, and other English variants.

</details>


### [65] [S$^4$C: Speculative Sampling with Syntactic and Semantic Coherence for Efficient Inference of Large Language Models](https://arxiv.org/abs/2506.14158)
*Tao He,Guang Huang,Yu Yang,Tianshi Xu,Sicheng Zhao,Guiguang Ding,Pengyang Wang,Feng Tian*

Main category: cs.CL

TL;DR: 论文提出了一种基于语法和语义连贯性的推测采样框架（S$^4$C），通过多头部草稿和连续验证树提升大语言模型的推理效率。


<details>
  <summary>Details</summary>
Motivation: 大语言模型（LLMs）的自回归特性导致推理延迟高，影响实时应用。现有推测采样方法忽视了文本生成的连贯性，限制了效率。

Method: S$^4$C框架结合多头部草稿快速生成令牌，并通过连续验证树高效验证候选令牌和重用特征。

Result: 实验显示S$^4$C在主流任务中优于基线方法，实现了2.26x-2.60x的加速比。

Conclusion: S$^4$C显著提升了推理效率和并行性，能以更少计算资源生成更多有效令牌。

Abstract: Large language models (LLMs) exhibit remarkable reasoning capabilities across diverse downstream tasks. However, their autoregressive nature leads to substantial inference latency, posing challenges for real-time applications. Speculative sampling mitigates this issue by introducing a drafting phase followed by a parallel validation phase, enabling faster token generation and verification. Existing approaches, however, overlook the inherent coherence in text generation, limiting their efficiency. To address this gap, we propose a Speculative Sampling with Syntactic and Semantic Coherence (S$^4$C) framework, which extends speculative sampling by leveraging multi-head drafting for rapid token generation and a continuous verification tree for efficient candidate validation and feature reuse. Experimental results demonstrate that S$^4$C surpasses baseline methods across mainstream tasks, offering enhanced efficiency, parallelism, and the ability to generate more valid tokens with fewer computational resources. On Spec-bench benchmarks, S$^4$C achieves an acceleration ratio of 2.26x-2.60x, outperforming state-of-the-art methods.

</details>


### [66] [MIST: Towards Multi-dimensional Implicit Bias and Stereotype Evaluation of LLMs via Theory of Mind](https://arxiv.org/abs/2506.14161)
*Yanlin Li,Hao Liu,Huimin Liu,Yinwei Wei,Yupeng Hu*

Main category: cs.CL

TL;DR: 论文提出了一种基于刻板印象内容模型（SCM）的评估框架，用于检测大型语言模型（LLMs）中的隐性偏见，通过间接任务揭示其在心智理论（ToM）中的多维失败。


<details>
  <summary>Details</summary>
Motivation: 传统直接查询方法难以捕捉隐性偏见的微妙和多维特性，且易受社会期望效应影响，因此需要更鲁棒的评估方法。

Method: 提出两个间接任务：词汇联想偏见测试（WABT）和情感归因测试（AAT），用于评估模型的隐性偏见。

Result: 在8个先进LLMs上的实验表明，框架能揭示复杂的偏见结构，如普遍的社会性偏见和多维分歧。

Conclusion: 该框架为识别隐性偏见的结构性本质提供了更稳健的方法。

Abstract: Theory of Mind (ToM) in Large Language Models (LLMs) refers to their capacity for reasoning about mental states, yet failures in this capacity often manifest as systematic implicit bias. Evaluating this bias is challenging, as conventional direct-query methods are susceptible to social desirability effects and fail to capture its subtle, multi-dimensional nature. To this end, we propose an evaluation framework that leverages the Stereotype Content Model (SCM) to reconceptualize bias as a multi-dimensional failure in ToM across Competence, Sociability, and Morality. The framework introduces two indirect tasks: the Word Association Bias Test (WABT) to assess implicit lexical associations and the Affective Attribution Test (AAT) to measure covert affective leanings, both designed to probe latent stereotypes without triggering model avoidance. Extensive experiments on 8 State-of-the-Art LLMs demonstrate our framework's capacity to reveal complex bias structures, including pervasive sociability bias, multi-dimensional divergence, and asymmetric stereotype amplification, thereby providing a more robust methodology for identifying the structural nature of implicit bias.

</details>


### [67] [A Variational Framework for Improving Naturalness in Generative Spoken Language Models](https://arxiv.org/abs/2506.14767)
*Li-Wei Chen,Takuya Higuchi,Zakaria Aldeneh,Ahmed Hussen Abdelaziz,Alexander Rudnicky*

Main category: cs.CL

TL;DR: 论文提出了一种端到端的变分方法，自动学习编码连续语音属性以增强语义标记，解决了现有方法依赖手动特征提取的问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法通过添加音高特征来增强语义标记，但无法完全表征副语言属性，且需要手动特征工程。

Method: 采用端到端的变分方法，自动学习编码连续语音属性，无需手动提取特征。

Result: 该方法生成的语音延续更受人类评分者青睐。

Conclusion: 提出的方法有效提升了语音生成的流畅性和自然度，且无需手动特征工程。

Abstract: The success of large language models in text processing has inspired their adaptation to speech modeling. However, since speech is continuous and complex, it is often discretized for autoregressive modeling. Speech tokens derived from self-supervised models (known as semantic tokens) typically focus on the linguistic aspects of speech but neglect prosodic information. As a result, models trained on these tokens can generate speech with reduced naturalness. Existing approaches try to fix this by adding pitch features to the semantic tokens. However, pitch alone cannot fully represent the range of paralinguistic attributes, and selecting the right features requires careful hand-engineering. To overcome this, we propose an end-to-end variational approach that automatically learns to encode these continuous speech attributes to enhance the semantic tokens. Our approach eliminates the need for manual extraction and selection of paralinguistic features. Moreover, it produces preferred speech continuations according to human raters. Code, samples and models are available at https://github.com/b04901014/vae-gslm.

</details>


### [68] [GRAM: A Generative Foundation Reward Model for Reward Generalization](https://arxiv.org/abs/2506.14175)
*Chenglong Wang,Yang Gan,Yifu Huo,Yongyu Mu,Qiaozhi He,Murun Yang,Bei Li,Tong Xiao,Chunliang Zhang,Tongran Liu,Jingbo Zhu*

Main category: cs.CL

TL;DR: 本文提出了一种结合无监督和有监督学习的生成式奖励模型，通过标签平滑优化排序损失，并在多个任务中表现出色。


<details>
  <summary>Details</summary>
Motivation: 传统奖励模型依赖标注数据且为判别式模型，本文探索结合无监督和有监督数据的方法，提升模型泛化能力。

Method: 基于LLMs的生成模型，先通过无监督学习预训练，再通过有监督学习微调，并利用标签平滑优化排序损失。

Result: 模型在响应排序、人类反馈强化学习等任务中表现优异，显著优于基线模型。

Conclusion: 生成式奖励模型结合无监督和有监督学习，具有广泛适用性和高性能表现。

Abstract: In aligning large language models (LLMs), reward models have played an important role, but are standardly trained as discriminative models and rely only on labeled human preference data. In this paper, we explore methods that train reward models using both unlabeled and labeled data. Building on the generative models in LLMs, we develop a generative reward model that is first trained via large-scale unsupervised learning and then fine-tuned via supervised learning. We also show that by using label smoothing, we are in fact optimizing a regularized pairwise ranking loss. This result, in turn, provides a new view of training reward models, which links generative models and discriminative models under the same class of training objectives. The outcome of these techniques is a foundation reward model, which can be applied to a wide range of tasks with little or no further fine-tuning effort. Extensive experiments show that this model generalizes well across several tasks, including response ranking, reinforcement learning from human feedback, and task adaptation with fine-tuning, achieving significant performance improvements over several strong baseline models.

</details>


### [69] [MAS-LitEval : Multi-Agent System for Literary Translation Quality Assessment](https://arxiv.org/abs/2506.14199)
*Junghwan Kim,Kieun Park,Sohee Park,Hyunggug Kim,Bongwon Suh*

Main category: cs.CL

TL;DR: 论文提出MAS-LitEval，一种基于多代理系统和大型语言模型的翻译评估方法，用于评估文学翻译中的术语、叙事和风格，优于传统指标。


<details>
  <summary>Details</summary>
Motivation: 传统翻译评估指标（如BLEU和METEOR）无法捕捉文学翻译中的文化细微差别和风格元素，导致叙事一致性和风格保真度被忽视。

Method: 采用多代理系统和大型语言模型（LLMs）构建MAS-LitEval，评估翻译的术语、叙事和风格。

Result: 在《小王子》和《康州美国佬在亚瑟王朝》的翻译测试中，MAS-LitEval表现优于传统指标，最高得分0.890。

Conclusion: MAS-LitEval为翻译质量评估（TQA）提供了一个可扩展且细致的框架，对翻译者和研究者具有实用价值。

Abstract: Literary translation requires preserving cultural nuances and stylistic elements, which traditional metrics like BLEU and METEOR fail to assess due to their focus on lexical overlap. This oversight neglects the narrative consistency and stylistic fidelity that are crucial for literary works. To address this, we propose MAS-LitEval, a multi-agent system using Large Language Models (LLMs) to evaluate translations based on terminology, narrative, and style. We tested MAS-LitEval on translations of The Little Prince and A Connecticut Yankee in King Arthur's Court, generated by various LLMs, and compared it to traditional metrics. \textbf{MAS-LitEval} outperformed these metrics, with top models scoring up to 0.890 in capturing literary nuances. This work introduces a scalable, nuanced framework for Translation Quality Assessment (TQA), offering a practical tool for translators and researchers.

</details>


### [70] [ELI-Why: Evaluating the Pedagogical Utility of Language Model Explanations](https://arxiv.org/abs/2506.14200)
*Brihi Joshi,Keyu He,Sahana Ramnath,Sadra Sabouri,Kaitlyn Zhou,Souti Chattopadhyay,Swabha Swayamdipta,Xiang Ren*

Main category: cs.CL

TL;DR: ELI-Why是一个包含13.4K个“为什么”问题的基准，用于评估语言模型的教学能力。研究发现，GPT-4生成的解释在匹配目标教育背景和满足学习者需求方面表现不如人类生成的解释。


<details>
  <summary>Details</summary>
Motivation: 探索语言模型如何根据不同学习者的信息需求和知识背景定制回答，以提升其教学能力。

Method: 引入ELI-Why基准，并进行两项人类研究，评估语言模型生成的解释对不同教育阶段（小学、高中、研究生）的适用性。

Result: GPT-4生成的解释仅50%匹配目标教育背景，而人类生成的解释为79%；学习者认为GPT-4解释平均比人类解释差20%。自动评估指标显示不同模型生成的解释在教育水平上难以区分。

Conclusion: 语言模型在定制教学解释方面仍有改进空间，尤其是在匹配教育背景和满足学习者需求方面。

Abstract: Language models today are widely used in education, yet their ability to tailor responses for learners with varied informational needs and knowledge backgrounds remains under-explored. To this end, we introduce ELI-Why, a benchmark of 13.4K "Why" questions to evaluate the pedagogical capabilities of language models. We then conduct two extensive human studies to assess the utility of language model-generated explanatory answers (explanations) on our benchmark, tailored to three distinct educational grades: elementary, high-school and graduate school. In our first study, human raters assume the role of an "educator" to assess model explanations' fit to different educational grades. We find that GPT-4-generated explanations match their intended educational background only 50% of the time, compared to 79% for lay human-curated explanations. In our second study, human raters assume the role of a learner to assess if an explanation fits their own informational needs. Across all educational backgrounds, users deemed GPT-4-generated explanations 20% less suited on average to their informational needs, when compared to explanations curated by lay people. Additionally, automated evaluation metrics reveal that explanations generated across different language model families for different informational needs remain indistinguishable in their grade-level, limiting their pedagogical effectiveness.

</details>


### [71] [Intended Target Identification for Anomia Patients with Gradient-based Selective Augmentation](https://arxiv.org/abs/2506.14203)
*Jongho Kim,Romain Storaï,Seung-won Hwang*

Main category: cs.CL

TL;DR: 研究探讨了语言模型在帮助失语症患者解决命名困难（失名症）中的潜力，通过梯度选择性增强方法解决了语义性错语和未见术语的问题，并在实验中表现优于基线。


<details>
  <summary>Details</summary>
Motivation: 失语症患者在命名物品时面临术语缺失和语义性错语干扰的挑战，需要一种能够应对这些问题的语言模型辅助工具。

Method: 提出梯度选择性增强方法：梯度值控制数据增强质量以应对语义错误，梯度方差引导引入未见但相关的术语。

Result: 在Tip-of-the-Tongue数据集和AphasiaBank患者数据中，模型表现优于基线，有效解决了失名症问题。

Conclusion: 该方法通过解决语义性错语和术语缺失问题，显著提升了语言模型在辅助失语症患者中的实用性。

Abstract: In this study, we investigate the potential of language models (LMs) in aiding patients experiencing anomia, a difficulty identifying the names of items. Identifying the intended target item from patient's circumlocution involves the two challenges of term failure and error: (1) The terms relevant to identifying the item remain unseen. (2) What makes the challenge unique is inherent perturbed terms by semantic paraphasia, which are not exactly related to the target item, hindering the identification process. To address each, we propose robustifying the model from semantically paraphasic errors and enhancing the model with unseen terms with gradient-based selective augmentation. Specifically, the gradient value controls augmented data quality amid semantic errors, while the gradient variance guides the inclusion of unseen but relevant terms. Due to limited domain-specific datasets, we evaluate the model on the Tip-of-the-Tongue dataset as an intermediary task and then apply our findings to real patient data from AphasiaBank. Our results demonstrate strong performance against baselines, aiding anomia patients by addressing the outlined challenges.

</details>


### [72] [AgentSynth: Scalable Task Generation for Generalist Computer-Use Agents](https://arxiv.org/abs/2506.14205)
*Jingxu Xie,Dylan Xu,Xuandong Zhao,Dawn Song*

Main category: cs.CL

TL;DR: AgentSynth是一个自动合成高质量任务和轨迹数据集的管道，用于通用计算机使用代理，通过信息不对称构建复杂任务，成本低且可扩展。


<details>
  <summary>Details</summary>
Motivation: 为通用计算机代理提供高质量、多样化的任务数据集，同时降低人工标注成本。

Method: 利用LLM生成任务，执行代理完成任务并记录轨迹，迭代形成子任务序列，最后合成可控难度的复合任务。

Result: 生成了6,000多个多样化任务，LLM代理在难度级别1的成功率为18%，级别6降至4%，每轨迹平均成本0.60美元。

Conclusion: AgentSynth能高效生成高质量任务数据集，显著降低标注成本，并为代理性能评估提供有力工具。

Abstract: We introduce AgentSynth, a scalable and cost-efficient pipeline for automatically synthesizing high-quality tasks and trajectory datasets for generalist computer-use agents. Leveraging information asymmetry, AgentSynth constructs subtasks that are simple during generation but significantly more challenging when composed into long-horizon tasks, enabling the creation of over 6,000 diverse and realistic tasks. Our pipeline begins with an LLM-based task proposer guided by a persona, followed by an execution agent that completes the task and logs the trajectory. This process is repeated iteratively to form a sequence of subtasks, which are then summarized by a separate agent into a composite task of controllable difficulty. A key strength of AgentSynth is its ability to precisely modulate task complexity by varying the number of subtasks. Empirical evaluations show that state-of-the-art LLM agents suffer a steep performance drop, from 18% success at difficulty level 1 to just 4% at level 6, highlighting the benchmark's difficulty and discriminative power. Moreover, our pipeline achieves a low average cost of \$0.60 per trajectory, orders of magnitude cheaper than human annotations. Our code and data are publicly available at https://github.com/sunblaze-ucb/AgentSynth

</details>


### [73] [CausalDiffTab: Mixed-Type Causal-Aware Diffusion for Tabular Data Generation](https://arxiv.org/abs/2506.14206)
*Jia-Chen Zhang,Zheng Zhou,Yu-Jie Xiong,Chun-Ming Xia,Fei Dai*

Main category: cs.CL

TL;DR: 论文提出CausalDiffTab，一种基于扩散模型的生成模型，用于处理混合类型的表格数据，通过自适应因果正则化方法提升性能。


<details>
  <summary>Details</summary>
Motivation: 高质量数据获取困难，尤其是混合类型表格数据生成面临挑战，如异构数据类型、复杂变量关系和列分布。

Method: 提出CausalDiffTab模型，结合扩散模型和自适应因果正则化方法，基于Hierarchical Prior Fusion原则。

Result: 在七个数据集上的实验表明，CausalDiffTab在所有指标上均优于基线方法。

Conclusion: CausalDiffTab有效解决了混合类型表格数据生成的挑战，性能优越且代码开源。

Abstract: Training data has been proven to be one of the most critical components in training generative AI. However, obtaining high-quality data remains challenging, with data privacy issues presenting a significant hurdle. To address the need for high-quality data. Synthesize data has emerged as a mainstream solution, demonstrating impressive performance in areas such as images, audio, and video. Generating mixed-type data, especially high-quality tabular data, still faces significant challenges. These primarily include its inherent heterogeneous data types, complex inter-variable relationships, and intricate column-wise distributions. In this paper, we introduce CausalDiffTab, a diffusion model-based generative model specifically designed to handle mixed tabular data containing both numerical and categorical features, while being more flexible in capturing complex interactions among variables. We further propose a hybrid adaptive causal regularization method based on the principle of Hierarchical Prior Fusion. This approach adaptively controls the weight of causal regularization, enhancing the model's performance without compromising its generative capabilities. Comprehensive experiments conducted on seven datasets demonstrate that CausalDiffTab outperforms baseline methods across all metrics. Our code is publicly available at: https://github.com/Godz-z/CausalDiffTab.

</details>


### [74] [Explainable Detection of Implicit Influential Patterns in Conversations via Data Augmentation](https://arxiv.org/abs/2506.14211)
*Sina Abdidizaji,Md Kowsher,Niloofar Yousefi,Ivan Garibay*

Main category: cs.CL

TL;DR: 论文提出了一种改进方法，用于检测对话中的隐性影响模式，并通过增强数据集和框架设计，显著提升了检测和分类任务的性能。


<details>
  <summary>Details</summary>
Motivation: 随着恶意行为者转向使用隐性语言策略影响公众认知，现有模型在检测这些模式时表现不足，因此需要更高效的方法。

Method: 利用先进语言模型的推理能力增强现有数据集，并设计新框架以定位和检测对话中的隐性影响元素。

Result: 新方法在隐性影响模式检测上提升了6%，在技术和受害者脆弱性多标签分类任务上分别提升了33%和43%。

Conclusion: 提出的方法有效提升了隐性影响模式的检测能力，为对抗数字平台中的语言操纵提供了有力工具。

Abstract: In the era of digitalization, as individuals increasingly rely on digital platforms for communication and news consumption, various actors employ linguistic strategies to influence public perception. While models have become proficient at detecting explicit patterns, which typically appear in texts as single remarks referred to as utterances, such as social media posts, malicious actors have shifted toward utilizing implicit influential verbal patterns embedded within conversations. These verbal patterns aim to mentally penetrate the victim's mind in order to influence them, enabling the actor to obtain the desired information through implicit means. This paper presents an improved approach for detecting such implicit influential patterns. Furthermore, the proposed model is capable of identifying the specific locations of these influential elements within a conversation. To achieve this, the existing dataset was augmented using the reasoning capabilities of state-of-the-art language models. Our designed framework resulted in a 6% improvement in the detection of implicit influential patterns in conversations. Moreover, this approach improved the multi-label classification tasks related to both the techniques used for influence and the vulnerability of victims by 33% and 43%, respectively.

</details>


### [75] [Chaining Event Spans for Temporal Relation Grounding](https://arxiv.org/abs/2506.14213)
*Jongho Kim,Dohyeon Lee,Minsoo Kim,Seung-won Hwang*

Main category: cs.CL

TL;DR: 论文提出了一种名为Timeline Reasoning Network (TRN)的新方法，通过预测事件的时间跨度来解决现有方法因答案重叠导致的不可靠问题，并在多个任务中表现优于先前方法。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖答案重叠作为区分相似问题的代理标签，但可能因偶然相同的答案导致不可靠结果。

Method: 提出TRN，采用两步归纳推理：首先生成初步答案，然后通过预测时间线来修正答案。

Result: 在TORQUE、TB-dense、TRC和TRE任务中，TRN表现优于先前方法。

Conclusion: TRN通过时间线预测有效解决了答案重叠问题，提升了任务性能。

Abstract: Accurately understanding temporal relations between events is a critical building block of diverse tasks, such as temporal reading comprehension (TRC) and relation extraction (TRE). For example in TRC, we need to understand the temporal semantic differences between the following two questions that are lexically near-identical: "What finished right before the decision?" or "What finished right after the decision?". To discern the two questions, existing solutions have relied on answer overlaps as a proxy label to contrast similar and dissimilar questions. However, we claim that answer overlap can lead to unreliable results, due to spurious overlaps of two dissimilar questions with coincidentally identical answers. To address the issue, we propose a novel approach that elicits proper reasoning behaviors through a module for predicting time spans of events. We introduce the Timeline Reasoning Network (TRN) operating in a two-step inductive reasoning process: In the first step model initially answers each question with semantic and syntactic information. The next step chains multiple questions on the same event to predict a timeline, which is then used to ground the answers. Results on the TORQUE and TB-dense, TRC and TRE tasks respectively, demonstrate that TRN outperforms previous methods by effectively resolving the spurious overlaps using the predicted timeline.

</details>


### [76] [Xolver: Multi-Agent Reasoning with Holistic Experience Learning Just Like an Olympiad Team](https://arxiv.org/abs/2506.14234)
*Md Tanzib Hosain,Salman Rahman,Md Kishor Morol,Md Rizwan Parvez*

Main category: cs.CL

TL;DR: Xolver是一个无需训练的多智能体推理框架，通过持久化记忆整合多样化经验模态，显著提升LLMs的推理能力。


<details>
  <summary>Details</summary>
Motivation: 当前LLMs在复杂推理中缺乏经验积累，而专家问题解决者则依赖丰富经验。Xolver旨在填补这一差距。

Method: Xolver结合外部检索、工具使用、协作交互、评估与迭代优化，动态学习策略、代码片段和推理模式。

Result: Xolver在多个基准测试中表现优异，甚至超越高级模型，如GSM8K（98.1%）和LiveCodeBench-V5（91.6%）。

Conclusion: Xolver展示了经验学习对实现专家级推理的关键作用，为通用智能体发展迈出重要一步。

Abstract: Despite impressive progress on complex reasoning, current large language models (LLMs) typically operate in isolation - treating each problem as an independent attempt, without accumulating or integrating experiential knowledge. In contrast, expert problem solvers - such as Olympiad or programming contest teams - leverage a rich tapestry of experiences: absorbing mentorship from coaches, developing intuition from past problems, leveraging knowledge of tool usage and library functionality, adapting strategies based on the expertise and experiences of peers, continuously refining their reasoning through trial and error, and learning from other related problems even during competition. We introduce Xolver, a training-free multi-agent reasoning framework that equips a black-box LLM with a persistent, evolving memory of holistic experience. Xolver integrates diverse experience modalities, including external and self-retrieval, tool use, collaborative interactions, agent-driven evaluation, and iterative refinement. By learning from relevant strategies, code fragments, and abstract reasoning patterns at inference time, Xolver avoids generating solutions from scratch - marking a transition from isolated inference toward experience-aware language agents. Built on both open-weight and proprietary models, Xolver consistently outperforms specialized reasoning agents. Even with lightweight backbones (e.g., QWQ-32B), it often surpasses advanced models including Qwen3-235B, Gemini 2.5 Pro, o3, and o4-mini-high. With o3-mini-high, it achieves new best results on GSM8K (98.1%), AIME'24 (94.4%), AIME'25 (93.7%), Math-500 (99.8%), and LiveCodeBench-V5 (91.6%) - highlighting holistic experience learning as a key step toward generalist agents capable of expert-level reasoning. Code and data are available at https://kagnlp.github.io/xolver.github.io/.

</details>


### [77] [A Multi-Expert Structural-Semantic Hybrid Framework for Unveiling Historical Patterns in Temporal Knowledge Graphs](https://arxiv.org/abs/2506.14235)
*Yimin Deng,Yuxia Wu,Yejing Wang,Guoshuai Zhao,Li Zhu,Qidong Liu,Derong Xu,Zichuan Fu,Xian Wu,Yefeng Zheng,Xiangyu Zhao,Xueming Qian*

Main category: cs.CL

TL;DR: 提出了一种多专家结构-语义混合（MESH）框架，用于整合结构和语义信息以预测未来事件。


<details>
  <summary>Details</summary>
Motivation: 现有方法未能整合结构和语义推理视角，且无法区分历史与非历史事件的差异，限制了泛化能力。

Method: 采用三种专家模块，结合结构和语义信息，指导不同事件的推理过程。

Result: 在三个数据集上的实验验证了方法的有效性。

Conclusion: MESH框架通过整合双重视角，提升了时间知识图谱推理的性能。

Abstract: Temporal knowledge graph reasoning aims to predict future events with knowledge of existing facts and plays a key role in various downstream tasks. Previous methods focused on either graph structure learning or semantic reasoning, failing to integrate dual reasoning perspectives to handle different prediction scenarios. Moreover, they lack the capability to capture the inherent differences between historical and non-historical events, which limits their generalization across different temporal contexts. To this end, we propose a Multi-Expert Structural-Semantic Hybrid (MESH) framework that employs three kinds of expert modules to integrate both structural and semantic information, guiding the reasoning process for different events. Extensive experiments on three datasets demonstrate the effectiveness of our approach.

</details>


### [78] [Re-Initialization Token Learning for Tool-Augmented Large Language Models](https://arxiv.org/abs/2506.14248)
*Chenghao Li,Liu Liu,Baosheng Yu,Jiayan Qiu,Yibing Zhan*

Main category: cs.CL

TL;DR: 提出一种新的令牌学习方法，通过将工具令牌与现有词嵌入空间对齐，提升大型语言模型在复杂任务中的性能。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在复杂任务（如数值推理、计划生成）中表现不佳，现有方法未能充分利用工具与词令牌的关系。

Method: 基于工具名称或描述构建先验令牌嵌入，用于初始化和规范化可学习的工具令牌嵌入，使其与词令牌空间对齐。

Result: 在GSM8K-XL、FuncQA、KAMEL和VirtualHome数据集上验证，性能优于CoT、REACT、ICL和ToolkenGPT等基线方法。

Conclusion: 该方法通过相关令牌有效增强大型语言模型的工具调用能力，适用于多领域任务。

Abstract: Large language models have demonstrated exceptional performance, yet struggle with complex tasks such as numerical reasoning, plan generation. Integrating external tools, such as calculators and databases, into large language models (LLMs) is crucial for enhancing problem-solving capabilities. Current methods assign a unique token to each tool, enabling LLMs to call tools through token prediction-similar to word generation. However, this approach fails to account for the relationship between tool and word tokens, limiting adaptability within pre-trained LLMs. To address this issue, we propose a novel token learning method that aligns tool tokens with the existing word embedding space from the perspective of initialization, thereby enhancing model performance. We begin by constructing prior token embeddings for each tool based on the tool's name or description, which are used to initialize and regularize the learnable tool token embeddings. This ensures the learned embeddings are well-aligned with the word token space, improving tool call accuracy. We evaluate the method on tasks such as numerical reasoning, knowledge-based question answering, and embodied plan generation using GSM8K-XL, FuncQA, KAMEL, and VirtualHome datasets. The results demonstrate clear improvements over recent baselines, including CoT, REACT, ICL, and ToolkenGPT, indicating that our approach effectively augments LLMs with tools through relevant tokens across diverse domains.

</details>


### [79] [From What to Respond to When to Respond: Timely Response Generation for Open-domain Dialogue Agents](https://arxiv.org/abs/2506.14285)
*Seongbo Jang,Minjin Jeon,Jaehoon Lee,Seonghyeon Lee,Dongha Lee,Hwanjo Yu*

Main category: cs.CL

TL;DR: 论文提出了一项新任务“及时对话响应生成”，并引入了TimelyChat基准，评估语言模型预测时间间隔和生成时间条件响应的能力。通过构建大规模训练数据集和训练Timer对话代理，实验表明Timer优于基于提示的LLM和其他基线模型。


<details>
  <summary>Details</summary>
Motivation: 现有对话响应生成研究主要关注文本上下文，而基于时间上下文的响应时机问题尚未充分探索。

Method: 利用时间常识知识图谱的无标签事件知识，通过LLM合成55K事件驱动对话数据集，训练Timer代理预测时间间隔并生成时间条件响应。

Result: Timer在轮次和对话级别评估中均优于基于提示的LLM和其他基线模型。

Conclusion: 论文填补了对话响应生成中时间上下文研究的空白，Timer模型表现出色，数据、模型和代码已公开。

Abstract: While research on dialogue response generation has primarily focused on generating coherent responses conditioning on textual context, the critical question of when to respond grounded on the temporal context remains underexplored. To bridge this gap, we propose a novel task called timely dialogue response generation and introduce the TimelyChat benchmark, which evaluates the capabilities of language models to predict appropriate time intervals and generate time-conditioned responses. Additionally, we construct a large-scale training dataset by leveraging unlabeled event knowledge from a temporal commonsense knowledge graph and employing a large language model (LLM) to synthesize 55K event-driven dialogues. We then train Timer, a dialogue agent designed to proactively predict time intervals and generate timely responses that align with those intervals. Experimental results show that Timer outperforms prompting-based LLMs and other fine-tuned baselines in both turn-level and dialogue-level evaluations. We publicly release our data, model, and code.

</details>


### [80] [Expectation Confirmation Preference Optimization for Multi-Turn Conversational Recommendation Agent](https://arxiv.org/abs/2506.14302)
*Xueyang Feng,Jingsen Zhang,Jiakai Tang,Wei Li,Guohao Cai,Xu Chen,Quanyu Dai,Yue Zhu,Zhenhua Dong*

Main category: cs.CL

TL;DR: 论文提出了一种名为ECPO的多轮偏好优化方法，通过建模用户满意度演化来优化对话推荐系统的响应效果。


<details>
  <summary>Details</summary>
Motivation: 现有的大型语言模型在对话推荐中常生成短视响应，无法满足用户期望，且偏好优化成本高、多轮对话效果差。

Method: 引入ECPO方法，利用期望确认理论建模用户满意度演化，并基于此优化不满意响应。同时开发了用户模拟器AILO。

Result: 实验表明，ECPO显著提升了对话推荐系统的交互能力，效率和效果均优于现有方法。

Conclusion: ECPO通过建模用户满意度演化，实现了低成本、高效的多轮偏好优化，为对话推荐系统提供了新思路。

Abstract: Recent advancements in Large Language Models (LLMs) have significantly propelled the development of Conversational Recommendation Agents (CRAs). However, these agents often generate short-sighted responses that fail to sustain user guidance and meet expectations. Although preference optimization has proven effective in aligning LLMs with user expectations, it remains costly and performs poorly in multi-turn dialogue. To address this challenge, we introduce a novel multi-turn preference optimization (MTPO) paradigm ECPO, which leverages Expectation Confirmation Theory to explicitly model the evolution of user satisfaction throughout multi-turn dialogues, uncovering the underlying causes of dissatisfaction. These causes can be utilized to support targeted optimization of unsatisfactory responses, thereby achieving turn-level preference optimization. ECPO ingeniously eliminates the significant sampling overhead of existing MTPO methods while ensuring the optimization process drives meaningful improvements. To support ECPO, we introduce an LLM-based user simulator, AILO, to simulate user feedback and perform expectation confirmation during conversational recommendations. Experimental results show that ECPO significantly enhances CRA's interaction capabilities, delivering notable improvements in both efficiency and effectiveness over existing MTPO methods.

</details>


### [81] [Evaluation Should Not Ignore Variation: On the Impact of Reference Set Choice on Summarization Metrics](https://arxiv.org/abs/2506.14335)
*Silvia Casola,Yang Janet Liu,Siyao Peng,Oliver Kraus,Albert Gatt,Barbara Plank*

Main category: cs.CL

TL;DR: 论文研究了参考摘要选择对基于参考的摘要评估指标的影响，发现许多流行指标（如ROUGE）对参考集的选择非常敏感，导致模型排名不稳定。建议在评估中考虑参考集的多样性以提高一致性。


<details>
  <summary>Details</summary>
Motivation: 人类语言生成的多样性在摘要评估中常被忽视，而参考摘要的选择对评估指标的可靠性有重要影响。本文旨在系统研究这种影响。

Method: 分析了三个多参考摘要数据集（SummEval、GUMSum和DUC2004），评估流行指标的敏感性，并收集人类对LLM输出的判断以补充现有研究。

Result: 发现许多指标（尤其是n-gram类如ROUGE）对参考集选择敏感，模型排名不稳定。人类判断与指标的相关性较弱或无。

Conclusion: 建议在摘要评估中纳入参考集的多样性，以提高评估的一致性和与人类判断的相关性，特别是在评估LLM时。

Abstract: Human language production exhibits remarkable richness and variation, reflecting diverse communication styles and intents. However, this variation is often overlooked in summarization evaluation. While having multiple reference summaries is known to improve correlation with human judgments, the impact of using different reference sets on reference-based metrics has not been systematically investigated. This work examines the sensitivity of widely used reference-based metrics in relation to the choice of reference sets, analyzing three diverse multi-reference summarization datasets: SummEval, GUMSum, and DUC2004. We demonstrate that many popular metrics exhibit significant instability. This instability is particularly concerning for n-gram-based metrics like ROUGE, where model rankings vary depending on the reference sets, undermining the reliability of model comparisons. We also collect human judgments on LLM outputs for genre-diverse data and examine their correlation with metrics to supplement existing findings beyond newswire summaries, finding weak-to-no correlation. Taken together, we recommend incorporating reference set variation into summarization evaluation to enhance consistency alongside correlation with human judgments, especially when evaluating LLMs.

</details>


### [82] [A Vision for Geo-Temporal Deep Research Systems: Towards Comprehensive, Transparent, and Reproducible Geo-Temporal Information Synthesis](https://arxiv.org/abs/2506.14345)
*Bruno Martins,Piotr Szymański,Piotr Gramacki*

Main category: cs.CL

TL;DR: 本文探讨了如何将地理时间推理能力整合到深度研究系统中，以解决当前系统在回答涉及地理或时间约束的复杂问题时存在的不足。


<details>
  <summary>Details</summary>
Motivation: 当前的大型语言模型（LLMs）驱动的深度研究系统缺乏处理地理和时间约束的能力，而这些能力在公共健康、环境科学和社会经济分析等领域至关重要。

Method: 提出通过增强检索和合成过程的能力，结合开放和可复现的基础设施，以及严格的评估协议，来整合地理时间推理。

Result: 本文提出了一个愿景，旨在开发更先进、具备地理时间感知能力的深度研究系统。

Conclusion: 通过整合地理时间推理能力，未来的AI驱动信息访问系统将更具潜力。

Abstract: The emergence of Large Language Models (LLMs) has transformed information access, with current LLMs also powering deep research systems that can generate comprehensive report-style answers, through planned iterative search, retrieval, and reasoning. Still, current deep research systems lack the geo-temporal capabilities that are essential for answering context-rich questions involving geographic and/or temporal constraints, frequently occurring in domains like public health, environmental science, or socio-economic analysis. This paper reports our vision towards next generation systems, identifying important technical, infrastructural, and evaluative challenges in integrating geo-temporal reasoning into deep research pipelines. We argue for augmenting retrieval and synthesis processes with the ability to handle geo-temporal constraints, supported by open and reproducible infrastructures and rigorous evaluation protocols. Our vision outlines a path towards more advanced and geo-temporally aware deep research systems, of potential impact to the future of AI-driven information access.

</details>


### [83] [Digital Gatekeepers: Google's Role in Curating Hashtags and Subreddits](https://arxiv.org/abs/2506.14370)
*Amrit Poudel,Yifan Ding,Jurgen Pfeffer,Tim Weninger*

Main category: cs.CL

TL;DR: 研究发现搜索引擎（如Google）通过算法选择性推广或压制某些标签和子论坛，影响用户接触的信息，揭示其内容可见性存在系统性偏见。


<details>
  <summary>Details</summary>
Motivation: 探讨搜索引擎如何作为数字守门人，通过算法塑造网络和社交媒体内容的可见性。

Method: 通过比较搜索引擎结果与Reddit和Twitter/X的非抽样数据，分析内容可见性的差异。

Result: Google算法倾向于压制与色情内容、阴谋论、广告和加密货币相关的子论坛和标签，同时推广高参与度内容。

Conclusion: Google的守门行为通过筛选社交媒体叙事影响公共话语。

Abstract: Search engines play a crucial role as digital gatekeepers, shaping the visibility of Web and social media content through algorithmic curation. This study investigates how search engines like Google selectively promotes or suppresses certain hashtags and subreddits, impacting the information users encounter. By comparing search engine results with nonsampled data from Reddit and Twitter/X, we reveal systematic biases in content visibility. Google's algorithms tend to suppress subreddits and hashtags related to sexually explicit material, conspiracy theories, advertisements, and cryptocurrencies, while promoting content associated with higher engagement. These findings suggest that Google's gatekeeping practices influence public discourse by curating the social media narratives available to users.

</details>


### [84] [ELLIS Alicante at CQs-Gen 2025: Winning the critical thinking questions shared task: LLM-based question generation and selection](https://arxiv.org/abs/2506.14371)
*Lucile Favero,Daniel Frases,Juan Antonio Pérez-Ortiz,Tanja Käser,Nuria Oliver*

Main category: cs.CL

TL;DR: 论文探讨了利用大型语言模型（LLMs）生成批判性问题以促进深度推理，而非仅用于检索事实信息。提出的两步框架在比赛中排名第一。


<details>
  <summary>Details</summary>
Motivation: 解决LLMs可能导致的浅层学习问题，通过生成批判性问题促进批判性思维。

Method: 使用两个小型开源语言模型：一个生成候选问题（Questioner），另一个选择最相关问题（Judge）。

Result: 系统在共享任务竞赛中排名第一，验证了方法的有效性。

Conclusion: LLM-based方法能有效促进对论证文本的批判性思考。

Abstract: The widespread adoption of chat interfaces based on Large Language Models (LLMs) raises concerns about promoting superficial learning and undermining the development of critical thinking skills. Instead of relying on LLMs purely for retrieving factual information, this work explores their potential to foster deeper reasoning by generating critical questions that challenge unsupported or vague claims in debate interventions. This study is part of a shared task of the 12th Workshop on Argument Mining, co-located with ACL 2025, focused on automatic critical question generation. We propose a two-step framework involving two small-scale open source language models: a Questioner that generates multiple candidate questions and a Judge that selects the most relevant ones. Our system ranked first in the shared task competition, demonstrating the potential of the proposed LLM-based approach to encourage critical engagement with argumentative texts.

</details>


### [85] [Thunder-NUBench: A Benchmark for LLMs' Sentence-Level Negation Understanding](https://arxiv.org/abs/2506.14397)
*Yeonkyoung So,Gyuseong Lee,Sungmok Jung,Joonhak Lee,JiA Kang,Sangho Kim,Jaejin Lee*

Main category: cs.CL

TL;DR: 论文提出了一个专门评估大语言模型（LLMs）在句子级否定理解能力的新基准Thunder-NUBench，弥补了现有基准的不足。


<details>
  <summary>Details</summary>
Motivation: 现有基准通常将否定视为自然语言推理等任务的附属案例，缺乏专门针对否定理解的评估工具。

Method: 设计了Thunder-NUBench基准，包含手动筛选的句子-否定对和多项选择题数据集，对比标准否定与局部否定、矛盾、转述等多样化结构。

Result: Thunder-NUBench能够深入评估LLMs的否定理解能力。

Conclusion: 该基准为LLMs在否定理解方面的评估提供了专门工具，填补了研究空白。

Abstract: Negation is a fundamental linguistic phenomenon that poses persistent challenges for Large Language Models (LLMs), particularly in tasks requiring deep semantic understanding. Existing benchmarks often treat negation as a side case within broader tasks like natural language inference, resulting in a lack of benchmarks that exclusively target negation understanding. In this work, we introduce \textbf{Thunder-NUBench}, a novel benchmark explicitly designed to assess sentence-level negation understanding in LLMs. Thunder-NUBench goes beyond surface-level cue detection by contrasting standard negation with structurally diverse alternatives such as local negation, contradiction, and paraphrase. The benchmark consists of manually curated sentence-negation pairs and a multiple-choice dataset that enables in-depth evaluation of models' negation understanding.

</details>


### [86] [ImpliRet: Benchmarking the Implicit Fact Retrieval Challenge](https://arxiv.org/abs/2506.14407)
*Zeinab Sadat Taghavi,Ali Modarressi,Yunpu Ma,Hinrich Schütze*

Main category: cs.CL

TL;DR: ImpliRet是一个新的检索基准，将推理挑战转移到文档端处理，而非查询端。


<details>
  <summary>Details</summary>
Motivation: 传统检索系统依赖浅层信号（如关键词重叠），而新基准将推理负担转移到查询端。ImpliRet旨在评估文档端的隐式推理能力。

Method: ImpliRet通过简单查询设计，但文档相关性依赖隐式事实（如时间、算术和世界知识关系）。评估了稀疏和密集检索器及长上下文模型。

Result: 所有检索器表现不佳，最佳nDCG@10仅15.07%。GPT-4.1在短上下文下得分仅35.06%。

Conclusion: 文档端推理仍具挑战性，ImpliRet为未来研究提供了新方向。

Abstract: Retrieval systems are central to many NLP pipelines, but often rely on surface-level cues such as keyword overlap and lexical semantic similarity. To evaluate retrieval beyond these shallow signals, recent benchmarks introduce reasoning-heavy queries; however, they primarily shift the burden to query-side processing techniques -- like prompting or multi-hop retrieval -- that can help resolve complexity. In contrast, we present ImpliRet, a benchmark that shifts the reasoning challenge to document-side processing: The queries are simple, but relevance depends on facts stated implicitly in documents through temporal (e.g., resolving "two days ago"), arithmetic, and world knowledge relationships. We evaluate a range of sparse and dense retrievers, all of which struggle in this setting: the best nDCG@10 is only 15.07%. We also test whether long-context models can overcome this limitation. But even with a short context of only ten documents, including the positive document, GPT-4.1 scores only 35.06%, showing that document-side reasoning remains a challenge. Our codes are available at github.com/ZeinabTaghavi/IMPLIRET.Contribution.

</details>


### [87] [LongLLaDA: Unlocking Long Context Capabilities in Diffusion LLMs](https://arxiv.org/abs/2506.14429)
*Xiaoran Liu,Zhigeng Liu,Zengfeng Huang,Qipeng Guo,Ziwei He,Xipeng Qiu*

Main category: cs.CL

TL;DR: 本文首次系统研究了扩散大语言模型（diffusion LLMs）的长上下文性能，发现其具有稳定的困惑度和局部感知能力，并提出了一种无需训练的方法LongLLaDA来扩展上下文窗口。


<details>
  <summary>Details</summary>
Motivation: 扩散大语言模型的长上下文能力尚未被系统研究，缺乏相关分析或扩展方法。

Method: 通过对比扩散LLMs和自回归LLMs的长上下文性能，发现扩散LLMs的稳定困惑度和局部感知现象，并提出基于NTK的RoPE外推方法LongLLaDA。

Result: 扩散LLMs在直接上下文外推中表现稳定，并在某些长上下文任务中优于自回归LLMs。

Conclusion: 本研究为扩散LLMs提供了首个上下文外推方法，并为未来长上下文扩散LLMs研究提供了理论和实证基础。

Abstract: Large Language Diffusion Models, or diffusion LLMs, have emerged as a significant focus in NLP research, with substantial effort directed toward understanding their scalability and downstream task performance. However, their long-context capabilities remain unexplored, lacking systematic analysis or methods for context extension. In this work, we present the first systematic investigation comparing the long-context performance of diffusion LLMs and traditional auto-regressive LLMs. We first identify a unique characteristic of diffusion LLMs, unlike auto-regressive LLMs, they maintain remarkably \textbf{\textit{stable perplexity}} during direct context extrapolation. Furthermore, where auto-regressive models fail outright during the Needle-In-A-Haystack task with context exceeding their pretrained length, we discover diffusion LLMs exhibit a distinct \textbf{\textit{local perception}} phenomenon, enabling successful retrieval from recent context segments. We explain both phenomena through the lens of Rotary Position Embedding (RoPE) scaling theory. Building on these observations, we propose LongLLaDA, a training-free method that integrates LLaDA with the NTK-based RoPE extrapolation. Our results validate that established extrapolation scaling laws remain effective for extending the context windows of diffusion LLMs. Furthermore, we identify long-context tasks where diffusion LLMs outperform auto-regressive LLMs and others where they fall short. Consequently, this study establishes the first context extrapolation method for diffusion LLMs while providing essential theoretical insights and empirical benchmarks critical for advancing future research on long-context diffusion LLMs.

</details>


### [88] [How Far Can LLMs Improve from Experience? Measuring Test-Time Learning Ability in LLMs with Human Comparison](https://arxiv.org/abs/2506.14448)
*Jiayin Wang,Zhiquang Guo,Weizhi Ma,Min Zhang*

Main category: cs.CL

TL;DR: 论文主张评估大语言模型的测试时学习能力，提出语义游戏作为评估工具，并设计了一个比较模型与人类表现的框架。结果显示LLMs具备测试时学习能力，但进步速度和稳定性不及人类。


<details>
  <summary>Details</summary>
Motivation: 现有基准主要评估静态知识，而智能还包括从经验中快速学习的能力，因此需要更全面的评估方法。

Method: 提出语义游戏作为评估测试时学习的工具，设计了一个包含有限和累积经验设置的客观评估框架，并招募人类参与者作为基线。

Result: LLMs表现出可测量的测试时学习能力，但在累积经验下进步较慢且不稳定，与人类表现存在差距。

Conclusion: LLMs具备通用学习机器的潜力，但模型与人类在智能上仍有显著差距。

Abstract: As evaluation designs of large language models may shape our trajectory toward artificial general intelligence, comprehensive and forward-looking assessment is essential. Existing benchmarks primarily assess static knowledge, while intelligence also entails the ability to rapidly learn from experience. To this end, we advocate for the evaluation of Test-time Learning, the capacity to improve performance in experience-based, reasoning-intensive tasks during test time. In this work, we propose semantic games as effective testbeds for evaluating test-time learning, due to their resistance to saturation and inherent demand for strategic reasoning. We introduce an objective evaluation framework that compares model performance under both limited and cumulative experience settings, and contains four forms of experience representation. To provide a comparative baseline, we recruit eight human participants to complete the same task. Results show that LLMs exhibit measurable test-time learning capabilities; however, their improvements are less stable under cumulative experience and progress more slowly than those observed in humans. These findings underscore the potential of LLMs as general-purpose learning machines, while also revealing a substantial intellectual gap between models and humans, irrespective of how well LLMs perform on static benchmarks.

</details>


### [89] [LexiMark: Robust Watermarking via Lexical Substitutions to Enhance Membership Verification of an LLM's Textual Training Data](https://arxiv.org/abs/2506.14474)
*Eyal German,Sagiv Antebi,Edan Habler,Asaf Shabtai,Yuval Elovici*

Main category: cs.CL

TL;DR: 论文提出了一种名为LexiMark的新型水印技术，通过替换高熵词的同义词来检测未经授权的大语言模型训练数据使用。该方法隐蔽性强，难以检测和去除。


<details>
  <summary>Details</summary>
Motivation: 现有水印方法隐蔽性不足，容易被检测和去除，因此需要一种更隐蔽且有效的水印技术来验证大语言模型是否使用了未经授权的训练数据。

Method: LexiMark通过替换高熵词的同义词嵌入水印，保持语义完整性，同时增强模型对水印文本的记忆能力。

Result: 实验表明，LexiMark在AUROC分数上显著优于现有方法，能可靠验证水印数据是否被用于模型训练。

Conclusion: LexiMark是一种高效且隐蔽的水印技术，适用于验证大语言模型的训练数据来源。

Abstract: Large language models (LLMs) can be trained or fine-tuned on data obtained without the owner's consent. Verifying whether a specific LLM was trained on particular data instances or an entire dataset is extremely challenging. Dataset watermarking addresses this by embedding identifiable modifications in training data to detect unauthorized use. However, existing methods often lack stealth, making them relatively easy to detect and remove. In light of these limitations, we propose LexiMark, a novel watermarking technique designed for text and documents, which embeds synonym substitutions for carefully selected high-entropy words. Our method aims to enhance an LLM's memorization capabilities on the watermarked text without altering the semantic integrity of the text. As a result, the watermark is difficult to detect, blending seamlessly into the text with no visible markers, and is resistant to removal due to its subtle, contextually appropriate substitutions that evade automated and manual detection. We evaluated our method using baseline datasets from recent studies and seven open-source models: LLaMA-1 7B, LLaMA-3 8B, Mistral 7B, Pythia 6.9B, as well as three smaller variants from the Pythia family (160M, 410M, and 1B). Our evaluation spans multiple training settings, including continued pretraining and fine-tuning scenarios. The results demonstrate significant improvements in AUROC scores compared to existing methods, underscoring our method's effectiveness in reliably verifying whether unauthorized watermarked data was used in LLM training.

</details>


### [90] [LingoLoop Attack: Trapping MLLMs via Linguistic Context and State Entrapment into Endless Loops](https://arxiv.org/abs/2506.14493)
*Jiyuan Fu,Kaixun Jiang,Lingyi Hong,Jinglun Li,Haijing Guo,Dingkang Yang,Zhaoyu Chen,Wenqiang Zhang*

Main category: cs.CL

TL;DR: LingoLoop是一种针对多模态大语言模型（MLLMs）的攻击方法，通过调整词性（POS）注意力和限制输出多样性，诱导模型生成冗长重复的序列，显著增加计算资源和能耗。


<details>
  <summary>Details</summary>
Motivation: 现有攻击方法忽视了词性和句子结构对输出长度的影响，限制了攻击效果。LingoLoop旨在通过更精细的控制，最大化MLLMs的资源消耗。

Method: 1. 提出POS-Aware Delay机制，基于词性调整注意力权重以延迟EOS生成；2. 引入Generative Path Pruning机制，限制隐藏状态幅度以诱导重复循环。

Result: 实验显示LingoLoop可将生成令牌数增加30倍，能耗同比增加，显著暴露MLLMs的脆弱性。

Conclusion: LingoLoop揭示了MLLMs在可靠部署中的重大挑战，需进一步研究防御措施。

Abstract: Multimodal Large Language Models (MLLMs) have shown great promise but require substantial computational resources during inference. Attackers can exploit this by inducing excessive output, leading to resource exhaustion and service degradation. Prior energy-latency attacks aim to increase generation time by broadly shifting the output token distribution away from the EOS token, but they neglect the influence of token-level Part-of-Speech (POS) characteristics on EOS and sentence-level structural patterns on output counts, limiting their efficacy. To address this, we propose LingoLoop, an attack designed to induce MLLMs to generate excessively verbose and repetitive sequences. First, we find that the POS tag of a token strongly affects the likelihood of generating an EOS token. Based on this insight, we propose a POS-Aware Delay Mechanism to postpone EOS token generation by adjusting attention weights guided by POS information. Second, we identify that constraining output diversity to induce repetitive loops is effective for sustained generation. We introduce a Generative Path Pruning Mechanism that limits the magnitude of hidden states, encouraging the model to produce persistent loops. Extensive experiments demonstrate LingoLoop can increase generated tokens by up to 30 times and energy consumption by a comparable factor on models like Qwen2.5-VL-3B, consistently driving MLLMs towards their maximum generation limits. These findings expose significant MLLMs' vulnerabilities, posing challenges for their reliable deployment. The code will be released publicly following the paper's acceptance.

</details>


### [91] [M2BeamLLM: Multimodal Sensing-empowered mmWave Beam Prediction with Large Language Models](https://arxiv.org/abs/2506.14532)
*Can Zheng,Jiguang He,Chung G. Kang,Guofa Cai,Zitong Yu,Merouane Debbah*

Main category: cs.CL

TL;DR: M2BeamLLM是一种新型神经网络框架，用于毫米波大规模MIMO通信系统中的波束预测，结合多模态传感器数据和LLM，显著提高了预测准确性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 为车辆到基础设施（V2I）毫米波通信系统提供高效智能的波束预测解决方案。

Method: 整合多模态传感器数据（图像、雷达、LiDAR、GPS），利用LLM（如GPT-2）进行波束预测，包括数据编码、多模态对齐与融合以及监督微调（SFT）。

Result: 在标准和少样本场景下显著优于传统深度学习模型，且预测性能随传感器模态多样性增加而提升。

Conclusion: M2BeamLLM为毫米波通信系统提供了高效且智能的波束预测方法。

Abstract: This paper introduces a novel neural network framework called M2BeamLLM for beam prediction in millimeter-wave (mmWave) massive multi-input multi-output (mMIMO) communication systems. M2BeamLLM integrates multi-modal sensor data, including images, radar, LiDAR, and GPS, leveraging the powerful reasoning capabilities of large language models (LLMs) such as GPT-2 for beam prediction. By combining sensing data encoding, multimodal alignment and fusion, and supervised fine-tuning (SFT), M2BeamLLM achieves significantly higher beam prediction accuracy and robustness, demonstrably outperforming traditional deep learning (DL) models in both standard and few-shot scenarios. Furthermore, its prediction performance consistently improves with increased diversity in sensing modalities. Our study provides an efficient and intelligent beam prediction solution for vehicle-to-infrastructure (V2I) mmWave communication systems.

</details>


### [92] [AlphaDecay:Module-wise Weight Decay for Heavy-Tailed Balancing in LLMs](https://arxiv.org/abs/2506.14562)
*Di He,Ajay Jaiswal,Songjun Tu,Li Shen,Ganzhao Yuan,Shiwei Liu,Lu Yin*

Main category: cs.CL

TL;DR: AlphaDecay是一种自适应分配权重衰减强度的新方法，通过分析模块的频谱特性优化LLM训练效果。


<details>
  <summary>Details</summary>
Motivation: 传统的均匀权重衰减忽略了LLM模块间的结构多样性和频谱特性差异，AlphaDecay旨在解决这一问题。

Method: 基于HT-SR理论，根据模块权重相关矩阵的频谱特性（重尾程度）自适应分配衰减强度。

Result: 在60M至1B规模的模型预训练中，AlphaDecay在困惑度和泛化能力上优于传统方法。

Conclusion: AlphaDecay通过模块化自适应衰减策略，显著提升了LLM的训练效果。

Abstract: Weight decay is a standard regularization technique for training large language models (LLMs). While it is common to assign a uniform decay rate to every layer, this approach overlooks the structural diversity of LLMs and the varying spectral properties across modules. In this paper, we introduce AlphaDecay, a simple yet effective method that adaptively assigns different weight decay strengths to each module of an LLM. Our approach is guided by Heavy-Tailed Self-Regularization (HT-SR) theory, which analyzes the empirical spectral density (ESD) of weight correlation matrices to quantify "heavy-tailedness." Modules exhibiting more pronounced heavy-tailed ESDs, reflecting stronger feature learning, are assigned weaker decay, while modules with lighter-tailed spectra receive stronger decay. Our method leverages tailored weight decay assignments to balance the module-wise differences in spectral properties, leading to improved performance. Extensive pre-training tasks with various model sizes from 60M to 1B demonstrate that AlphaDecay achieves better perplexity and generalization than conventional uniform decay and other adaptive decay baselines.

</details>


### [93] [GenerationPrograms: Fine-grained Attribution with Executable Programs](https://arxiv.org/abs/2506.14580)
*David Wan,Eran Hirsch,Elias Stengel-Eskin,Ido Dagan,Mohit Bansal*

Main category: cs.CL

TL;DR: GenerationPrograms框架通过模块化生成计划显著提升文本生成中的细粒度归因质量，增强可验证性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型在生成文本时难以提供细粒度归因，且现有归因方法无法解释模型如何利用源文档生成响应，限制了可解释性和信任。

Method: 提出模块化生成框架GenerationPrograms，分两阶段：首先生成针对查询的可执行程序计划（如改写、压缩、融合等操作），随后执行计划生成最终响应。

Result: 实验表明，GenerationPrograms在文档和句子级别的归因质量上显著提升，且可作为后处理归因方法优于传统技术。

Conclusion: GenerationPrograms通过模块化操作提升归因质量，同时支持局部优化，增强整体生成的可解释性和准确性。

Abstract: Recent large language models (LLMs) achieve impressive performance in source-conditioned text generation but often fail to correctly provide fine-grained attributions for their outputs, undermining verifiability and trust. Moreover, existing attribution methods do not explain how and why models leverage the provided source documents to generate their final responses, limiting interpretability. To overcome these challenges, we introduce a modular generation framework, GenerationPrograms, inspired by recent advancements in executable "code agent" architectures. Unlike conventional generation methods that simultaneously generate outputs and attributions or rely on post-hoc attribution, GenerationPrograms decomposes the process into two distinct stages: first, creating an executable program plan composed of modular text operations (such as paraphrasing, compression, and fusion) explicitly tailored to the query, and second, executing these operations following the program's specified instructions to produce the final response. Empirical evaluations demonstrate that GenerationPrograms significantly improves attribution quality at both the document level and sentence level across two long-form question-answering tasks and a multi-document summarization task. We further demonstrate that GenerationPrograms can effectively function as a post-hoc attribution method, outperforming traditional techniques in recovering accurate attributions. In addition, the interpretable programs generated by GenerationPrograms enable localized refinement through modular-level improvements that further enhance overall attribution quality.

</details>


### [94] [Guaranteed Guess: A Language Modeling Approach for CISC-to-RISC Transpilation with Testing Guarantees](https://arxiv.org/abs/2506.14606)
*Ahmed Heakl,Sarim Hashmi,Chaimaa Abi,Celine Lee,Abdulrahman Mahmoud*

Main category: cs.CL

TL;DR: GG是一种结合大型语言模型和软件测试框架的ISA转换方法，用于CISC到RISC的高效翻译，性能优于现有技术。


<details>
  <summary>Details</summary>
Motivation: 硬件生态系统快速发展，需要快速、灵活且正确地在不同ISA间转换低级程序，以提升代码的可移植性和寿命。

Method: GG结合预训练大型语言模型生成候选翻译，并通过软件测试框架量化翻译的置信度。

Result: 在HumanEval和BringupBench上分别达到99%和49%的正确率，性能优于Rosetta 2。

Conclusion: GG在CISC到RISC翻译任务中表现出色，并开源代码和数据以推动研究。

Abstract: The hardware ecosystem is rapidly evolving, with increasing interest in translating low-level programs across different instruction set architectures (ISAs) in a quick, flexible, and correct way to enhance the portability and longevity of existing code. A particularly challenging class of this transpilation problem is translating between complex- (CISC) and reduced- (RISC) hardware architectures, due to fundamental differences in instruction complexity, memory models, and execution paradigms. In this work, we introduce GG (Guaranteed Guess), an ISA-centric transpilation pipeline that combines the translation power of pre-trained large language models (LLMs) with the rigor of established software testing constructs. Our method generates candidate translations using an LLM from one ISA to another, and embeds such translations within a software-testing framework to build quantifiable confidence in the translation. We evaluate our GG approach over two diverse datasets, enforce high code coverage (>98%) across unit tests, and achieve functional/semantic correctness of 99% on HumanEval programs and 49% on BringupBench programs, respectively. Further, we compare our approach to the state-of-the-art Rosetta 2 framework on Apple Silicon, showcasing 1.73x faster runtime performance, 1.47x better energy efficiency, and 2.41x better memory usage for our transpiled code, demonstrating the effectiveness of GG for real-world CISC-to-RISC translation tasks. We will open-source our codes, data, models, and benchmarks to establish a common foundation for ISA-level code translation research.

</details>


### [95] [When Does Meaning Backfire? Investigating the Role of AMRs in NLI](https://arxiv.org/abs/2506.14613)
*Junghyun Min,Xiulin Yang,Shira Wein*

Main category: cs.CL

TL;DR: 研究探讨了在自然语言推理（NLI）中加入抽象意义表示（AMR）对预训练语言模型的影响，发现微调时AMR阻碍泛化，而提示时略有提升，但提升源于表面差异而非语义推理。


<details>
  <summary>Details</summary>
Motivation: 探索AMR是否能帮助预训练语言模型在NLI任务中更好地泛化。

Method: 在微调和提示两种设置下将AMR整合到NLI任务中，并进行消融研究。

Result: 微调时AMR阻碍模型泛化，提示时略有提升，但提升源于表面差异而非语义推理。

Conclusion: AMR在NLI中的作用有限，可能误导模型预测，需谨慎使用。

Abstract: Natural Language Inference (NLI) relies heavily on adequately parsing the semantic content of the premise and hypothesis. In this work, we investigate whether adding semantic information in the form of an Abstract Meaning Representation (AMR) helps pretrained language models better generalize in NLI. Our experiments integrating AMR into NLI in both fine-tuning and prompting settings show that the presence of AMR in fine-tuning hinders model generalization while prompting with AMR leads to slight gains in \texttt{GPT-4o}. However, an ablation study reveals that the improvement comes from amplifying surface-level differences rather than aiding semantic reasoning. This amplification can mislead models to predict non-entailment even when the core meaning is preserved.

</details>


### [96] [Probabilistic Aggregation and Targeted Embedding Optimization for Collective Moral Reasoning in Large Language Models](https://arxiv.org/abs/2506.14625)
*Chenchen Yuan,Zheyu Zhang,Shuo Yang,Bardh Prenkaj,Gjergji Kasneci*

Main category: cs.CL

TL;DR: 提出一种框架，通过聚合多个LLMs的道德判断形成集体共识，并通过嵌入优化调整偏离模型，实验证明其有效性和一致性。


<details>
  <summary>Details</summary>
Motivation: 解决LLMs在复杂道德困境中判断不一致的问题，提升AI系统的安全性和一致性。

Method: 提出框架，融合多个LLMs的道德评分，通过嵌入优化调整偏离模型。

Result: 实验证明框架能构建稳健共识并提升模型个体一致性。

Conclusion: 数据驱动的多模型道德对齐具有价值，可促进更安全、一致的AI系统。

Abstract: Large Language Models (LLMs) have shown impressive moral reasoning abilities. Yet they often diverge when confronted with complex, multi-factor moral dilemmas. To address these discrepancies, we propose a framework that synthesizes multiple LLMs' moral judgments into a collectively formulated moral judgment, realigning models that deviate significantly from this consensus. Our aggregation mechanism fuses continuous moral acceptability scores (beyond binary labels) into a collective probability, weighting contributions by model reliability. For misaligned models, a targeted embedding-optimization procedure fine-tunes token embeddings for moral philosophical theories, minimizing JS divergence to the consensus while preserving semantic integrity. Experiments on a large-scale social moral dilemma dataset show our approach builds robust consensus and improves individual model fidelity. These findings highlight the value of data-driven moral alignment across multiple models and its potential for safer, more consistent AI systems.

</details>


### [97] [AIn't Nothing But a Survey? Using Large Language Models for Coding German Open-Ended Survey Responses on Survey Motivation](https://arxiv.org/abs/2506.14634)
*Leah von der Heyde,Anna-Carolina Haensch,Bernd Weiß,Jessika Daikeler*

Main category: cs.CL

TL;DR: 研究探讨了不同LLMs在德语开放性问题分类中的表现，发现性能差异显著，仅微调后的LLM表现满意。提示方法的效果取决于LLM，且分类性能不均导致分布差异。


<details>
  <summary>Details</summary>
Motivation: 探索LLMs在非英语和非简单主题的开放性问题分类中的适用性，并与传统方法比较。

Method: 使用德语调查数据，比较多种LLMs和提示方法，以专家编码为基准评估性能。

Result: 不同LLMs性能差异大，仅微调LLM表现满意；提示方法效果依赖LLM；分类性能不均影响分布。

Conclusion: LLMs在开放性问题分类中需权衡选择，微调是关键。研究为LLMs在调查中的应用提供了条件参考。

Abstract: The recent development and wider accessibility of LLMs have spurred discussions about how they can be used in survey research, including classifying open-ended survey responses. Due to their linguistic capacities, it is possible that LLMs are an efficient alternative to time-consuming manual coding and the pre-training of supervised machine learning models. As most existing research on this topic has focused on English-language responses relating to non-complex topics or on single LLMs, it is unclear whether its findings generalize and how the quality of these classifications compares to established methods. In this study, we investigate to what extent different LLMs can be used to code open-ended survey responses in other contexts, using German data on reasons for survey participation as an example. We compare several state-of-the-art LLMs and several prompting approaches, and evaluate the LLMs' performance by using human expert codings. Overall performance differs greatly between LLMs, and only a fine-tuned LLM achieves satisfactory levels of predictive performance. Performance differences between prompting approaches are conditional on the LLM used. Finally, LLMs' unequal classification performance across different categories of reasons for survey participation results in different categorical distributions when not using fine-tuning. We discuss the implications of these findings, both for methodological research on coding open-ended responses and for their substantive analysis, and for practitioners processing or substantively analyzing such data. Finally, we highlight the many trade-offs researchers need to consider when choosing automated methods for open-ended response classification in the age of LLMs. In doing so, our study contributes to the growing body of research about the conditions under which LLMs can be efficiently, accurately, and reliably leveraged in survey research.

</details>


### [98] [Revisiting Chain-of-Thought Prompting: Zero-shot Can Be Stronger than Few-shot](https://arxiv.org/abs/2506.14641)
*Xiang Cheng,Chengyan Pan,Minjun Zhao,Deyang Li,Fangchao Liu,Xinyu Zhang,Xiao Zhang,Yong Liu*

Main category: cs.CL

TL;DR: 研究发现，对于近期强大的语言模型（如Qwen2.5系列），传统的CoT示例并未提升数学推理能力，其主要作用是格式化输出。增强的CoT示例同样无效，模型倾向于忽略示例而关注指令。


<details>
  <summary>Details</summary>
Motivation: 探讨CoT示例是否仍对近期强大的语言模型在数学推理任务中有效。

Method: 通过系统实验，比较传统和增强的CoT示例对模型推理能力的影响。

Result: 传统和增强的CoT示例均未提升模型推理能力，模型主要关注指令而非示例。

Conclusion: 当前ICL+CoT框架在数学推理中存在局限性，需重新审视ICL范式及示例定义。

Abstract: In-Context Learning (ICL) is an essential emergent ability of Large Language Models (LLMs), and recent studies introduce Chain-of-Thought (CoT) to exemplars of ICL to enhance the reasoning capability, especially in mathematics tasks. However, given the continuous advancement of model capabilities, it remains unclear whether CoT exemplars still benefit recent, stronger models in such tasks. Through systematic experiments, we find that for recent strong models such as the Qwen2.5 series, adding traditional CoT exemplars does not improve reasoning performance compared to Zero-Shot CoT. Instead, their primary function is to align the output format with human expectations. We further investigate the effectiveness of enhanced CoT exemplars, constructed using answers from advanced models such as \texttt{Qwen2.5-Max} and \texttt{DeepSeek-R1}. Experimental results indicate that these enhanced exemplars still fail to improve the model's reasoning performance. Further analysis reveals that models tend to ignore the exemplars and focus primarily on the instructions, leading to no observable gain in reasoning ability. Overall, our findings highlight the limitations of the current ICL+CoT framework in mathematical reasoning, calling for a re-examination of the ICL paradigm and the definition of exemplars.

</details>


### [99] [Passing the Turing Test in Political Discourse: Fine-Tuning LLMs to Mimic Polarized Social Media Comments](https://arxiv.org/abs/2506.14645)
*. Pazzaglia,V. Vendetti,L. D. Comencini,F. Deriu,V. Modugno*

Main category: cs.CL

TL;DR: 研究发现，经过微调的大型语言模型（LLM）能够生成高度可信且具有煽动性的政治内容，加剧意识形态极化。


<details>
  <summary>Details</summary>
Motivation: 探讨LLM在生成偏激内容时如何加剧在线意识形态极化。

Method: 使用Reddit的政治讨论数据微调开源LLM，并通过语言分析、情感评分和人工标注评估输出。

Result: 微调后的LLM能生成与人类写作难以区分的偏激内容。

Conclusion: 研究呼吁关注AI在政治话语中的伦理风险，并提出治理和检测工具开发的建议。

Abstract: The increasing sophistication of large language models (LLMs) has sparked growing concerns regarding their potential role in exacerbating ideological polarization through the automated generation of persuasive and biased content. This study explores the extent to which fine-tuned LLMs can replicate and amplify polarizing discourse within online environments. Using a curated dataset of politically charged discussions extracted from Reddit, we fine-tune an open-source LLM to produce context-aware and ideologically aligned responses. The model's outputs are evaluated through linguistic analysis, sentiment scoring, and human annotation, with particular attention to credibility and rhetorical alignment with the original discourse. The results indicate that, when trained on partisan data, LLMs are capable of producing highly plausible and provocative comments, often indistinguishable from those written by humans. These findings raise significant ethical questions about the use of AI in political discourse, disinformation, and manipulation campaigns. The paper concludes with a discussion of the broader implications for AI governance, platform regulation, and the development of detection tools to mitigate adversarial fine-tuning risks.

</details>


### [100] [GuiLoMo: Allocating Expert Number and Rank for LoRA-MoE via Bilevel Optimization with GuidedSelection Vectors](https://arxiv.org/abs/2506.14646)
*Hengyuan Zhang,Xinrong Chen,Yingmin Qiu,Xiao Liang,Ziyue Li,Guanyu Wang,Weiping Li,Tong Mo,Wenyue Li,Hayden Kwok-Hay So,Ngai Wong*

Main category: cs.CL

TL;DR: GuiLoMo提出了一种细粒度的专家数量和秩分配策略，结合GSVs优化LoRA-MoE性能，解决了现有方法的局限性。


<details>
  <summary>Details</summary>
Motivation: LoRA-MoE在适应大语言模型时存在专家数量分配受下游任务影响和统一秩分配限制表示多样性的问题。

Method: 通过GSVs学习模型和任务需求，动态分配专家数量和秩，优化LoRA-MoE。

Result: 在多个基准测试中，GuiLoMo表现优于或与基线方法相当。

Conclusion: 自适应专家配置能显著提升性能，GuiLoMo为PEFT方法提供了新思路。

Abstract: Parameter-efficient fine-tuning (PEFT) methods, particularly Low-Rank Adaptation (LoRA), offer an efficient way to adapt large language models with reduced computational costs. However, their performance is limited by the small number of trainable parameters. Recent work combines LoRA with the Mixture-of-Experts (MoE), i.e., LoRA-MoE, to enhance capacity, but two limitations remain in hindering the full exploitation of its potential: 1) the influence of downstream tasks when assigning expert numbers, and 2) the uniform rank assignment across all LoRA experts, which restricts representational diversity. To mitigate these gaps, we propose GuiLoMo, a fine-grained layer-wise expert numbers and ranks allocation strategy with GuidedSelection Vectors (GSVs). GSVs are learned via a prior bilevel optimization process to capture both model- and task-specific needs, and are then used to allocate optimal expert numbers and ranks. Experiments on three backbone models across diverse benchmarks show that GuiLoMo consistently achieves superior or comparable performance to all baselines. Further analysis offers key insights into how expert numbers and ranks vary across layers and tasks, highlighting the benefits of adaptive expert configuration. Our code is available at https://github.com/Liar406/Gui-LoMo.git.

</details>


### [101] [Massive Supervised Fine-tuning Experiments Reveal How Data, Layer, and Training Factors Shape LLM Alignment Quality](https://arxiv.org/abs/2506.14681)
*Yuto Harada,Yusuke Yamauchi,Yusuke Oda,Yohei Oseki,Yusuke Miyao,Yu Takagi*

Main category: cs.CL

TL;DR: 论文研究了监督微调（SFT）在大型语言模型中的作用，发现数据集特性和层间修改是关键因素，困惑度能有效预测SFT效果。


<details>
  <summary>Details</summary>
Motivation: 探索SFT如何更好地对齐大型语言模型与人类指令和价值观，填补当前研究的空白。

Method: 训练了1000多个SFT模型，分析数据集特性和层间修改，评估困惑度与性能的关系。

Result: 发现训练任务协同效应因模型而异，困惑度是SFT效果的有效预测指标，中层权重变化与性能提升相关。

Conclusion: 强调了模型特定策略的重要性，并公开了1000多个SFT模型以推动进一步研究。

Abstract: Supervised fine-tuning (SFT) is a critical step in aligning large language models (LLMs) with human instructions and values, yet many aspects of SFT remain poorly understood. We trained a wide range of base models on a variety of datasets including code generation, mathematical reasoning, and general-domain tasks, resulting in 1,000+ SFT models under controlled conditions. We then identified the dataset properties that matter most and examined the layer-wise modifications introduced by SFT. Our findings reveal that some training-task synergies persist across all models while others vary substantially, emphasizing the importance of model-specific strategies. Moreover, we demonstrate that perplexity consistently predicts SFT effectiveness--often surpassing superficial similarity between trained data and benchmark--and that mid-layer weight changes correlate most strongly with performance gains. We will release these 1,000+ SFT models and benchmark results to accelerate further research.

</details>


### [102] [Treasure Hunt: Real-time Targeting of the Long Tail using Training-Time Markers](https://arxiv.org/abs/2506.14702)
*Daniel D'souza,Julia Kreutzer,Adrien Morisot,Ahmet Üstün,Sara Hooker*

Main category: cs.CL

TL;DR: 论文提出了一种优化训练协议的方法，旨在提高模型在罕见用例上的表现和可控性。通过数据特征和任务来源的分类，结合微调技术，显著提升了长尾分布任务的性能。


<details>
  <summary>Details</summary>
Motivation: 解决现代机器学习在罕见和代表性不足特征上的表现问题，避免依赖提示工程或少样本示例的局限性。

Method: 创建数据特征和任务来源的分类法，微调基础模型以自动推断标记，从而在推理时灵活控制生成属性。

Result: 在开放生成质量上平均提升5.7%，在罕见领域提升9.1%，在特定任务（如代码修复）上相对提升达14.1%。

Conclusion: 提出的方法显著改善了模型在长尾分布任务中的表现和可控性，为优化训练协议提供了新思路。

Abstract: One of the most profound challenges of modern machine learning is performing well on the long-tail of rare and underrepresented features. Large general-purpose models are trained for many tasks, but work best on high-frequency use cases. After training, it is hard to adapt a model to perform well on specific use cases underrepresented in the training corpus. Relying on prompt engineering or few-shot examples to maximize the output quality on a particular test case can be frustrating, as models can be highly sensitive to small changes, react in unpredicted ways or rely on a fixed system prompt for maintaining performance. In this work, we ask: "Can we optimize our training protocols to both improve controllability and performance on underrepresented use cases at inference time?" We revisit the divide between training and inference techniques to improve long-tail performance while providing users with a set of control levers the model is trained to be responsive to. We create a detailed taxonomy of data characteristics and task provenance to explicitly control generation attributes and implicitly condition generations at inference time. We fine-tune a base model to infer these markers automatically, which makes them optional at inference time. This principled and flexible approach yields pronounced improvements in performance, especially on examples from the long tail of the training distribution. While we observe an average lift of 5.7% win rates in open-ended generation quality with our markers, we see over 9.1% gains in underrepresented domains. We also observe relative lifts of up to 14.1% on underrepresented tasks like CodeRepair and absolute improvements of 35.3% on length instruction following evaluations.

</details>


### [103] [Capacity Matters: a Proof-of-Concept for Transformer Memorization on Real-World Data](https://arxiv.org/abs/2506.14704)
*Anton Changalidis,Aki Härmä*

Main category: cs.CL

TL;DR: 研究了模型架构和数据配置对生成式Transformer记忆能力的影响，发现嵌入大小是主要决定因素，Softmax激活函数表现更优，数据复杂性提升记忆能力。


<details>
  <summary>Details</summary>
Motivation: 探讨模型架构和数据配置如何影响生成式Transformer的记忆能力，以优化模型设计。

Method: 使用SNOMED知识图谱生成的合成文本数据集（三元组和序列）训练模型，分析嵌入大小、层数和激活函数的影响。

Result: 嵌入大小是学习速度和能力的主要决定因素，Softmax更稳定，数据复杂性提升记忆能力。

Conclusion: 研究为理解Transformer记忆机制和优化模型设计提供了框架。

Abstract: This paper studies how the model architecture and data configurations influence the empirical memorization capacity of generative transformers. The models are trained using synthetic text datasets derived from the Systematized Nomenclature of Medicine (SNOMED) knowledge graph: triplets, representing static connections, and sequences, simulating complex relation patterns. The results show that embedding size is the primary determinant of learning speed and capacity, while additional layers provide limited benefits and may hinder performance on simpler datasets. Activation functions play a crucial role, and Softmax demonstrates greater stability and capacity. Furthermore, increasing the complexity of the data set seems to improve the final memorization. These insights improve our understanding of transformer memory mechanisms and provide a framework for optimizing model design with structured real-world data.

</details>


### [104] [Ring-lite: Scalable Reasoning via C3PO-Stabilized Reinforcement Learning for LLMs](https://arxiv.org/abs/2506.14731)
*Ring Team,Bin Hu,Cai Chen,Deng Zhao,Ding Liu,Dingnan Jin,Feng Zhu,Hao Dai,Hongzhi Luan,Jia Guo,Jiaming Liu,Jiewei Wu,Jun Mei,Jun Zhou,Junbo Zhao,Junwu Xiong,Kaihong Zhang,Kuan Xu,Lei Liang,Liang Jiang,Liangcheng Fu,Longfei Zheng,Qiang Gao,Qing Cui,Quan Wan,Shaomian Zheng,Shuaicheng Li,Tongkai Yang,Wang Ren,Xiaodong Yan,Xiaopei Wan,Xiaoyun Feng,Xin Zhao,Xinxing Yang,Xinyu Kong,Xuemin Yang,Yang Li,Yingting Wu,Yongkang Liu,Zhankai Xu,Zhenduo Zhang,Zhenglei Zhou,Zhenyu Huang,Zhiqiang Zhang,Zihao Wang,Zujie Wen*

Main category: cs.CL

TL;DR: Ring-lite是一个基于混合专家（MoE）的大型语言模型，通过强化学习（RL）优化，实现高效且鲁棒的推理能力。它仅激活三分之一参数即可达到SOTA小型推理模型的性能。


<details>
  <summary>Details</summary>
Motivation: 旨在通过优化MoE模型的训练方法，解决RL训练中的不稳定性和多领域数据整合问题，以实现更高效的推理能力。

Method: 提出联合训练流程，结合蒸馏与RL，并引入C3PO方法提升训练稳定性；基于熵损失选择蒸馏检查点；采用两阶段训练范式整合多领域数据。

Result: 在AIME等基准测试中达到SOTA性能，同时显著减少激活参数数量。

Conclusion: Ring-lite通过算法-系统协同设计，解决了MoE RL训练的挑战，为高效推理模型提供了新思路。

Abstract: We present Ring-lite, a Mixture-of-Experts (MoE)-based large language model optimized via reinforcement learning (RL) to achieve efficient and robust reasoning capabilities. Built upon the publicly available Ling-lite model, a 16.8 billion parameter model with 2.75 billion activated parameters, our approach matches the performance of state-of-the-art (SOTA) small-scale reasoning models on challenging benchmarks (e.g., AIME, LiveCodeBench, GPQA-Diamond) while activating only one-third of the parameters required by comparable models. To accomplish this, we introduce a joint training pipeline integrating distillation with RL, revealing undocumented challenges in MoE RL training. First, we identify optimization instability during RL training, and we propose Constrained Contextual Computation Policy Optimization(C3PO), a novel approach that enhances training stability and improves computational throughput via algorithm-system co-design methodology. Second, we empirically demonstrate that selecting distillation checkpoints based on entropy loss for RL training, rather than validation metrics, yields superior performance-efficiency trade-offs in subsequent RL training. Finally, we develop a two-stage training paradigm to harmonize multi-domain data integration, addressing domain conflicts that arise in training with mixed dataset. We will release the model, dataset, and code.

</details>


### [105] [Reasoning with Exploration: An Entropy Perspective](https://arxiv.org/abs/2506.14758)
*Daixuan Cheng,Shaohan Huang,Xuekai Zhu,Bo Dai,Wayne Xin Zhao,Zhenliang Zhang,Furu Wei*

Main category: cs.CL

TL;DR: 论文通过分析熵与语言模型探索性推理的关系，提出了一种简单的RL方法改进，显著提升了LM推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有方法倾向于利用而非探索，导致性能瓶颈，因此研究熵与探索性推理的关系。

Method: 在标准RL的优势函数中增加基于熵的项，以促进更长的推理链。

Result: 在Pass@K指标上取得显著提升，尤其是在大K值下。

Conclusion: 通过熵促进探索性推理，突破了LM推理的边界。

Abstract: Balancing exploration and exploitation is a central goal in reinforcement learning (RL). Despite recent advances in enhancing language model (LM) reasoning, most methods lean toward exploitation, and increasingly encounter performance plateaus. In this work, we revisit entropy -- a signal of exploration in RL -- and examine its relationship to exploratory reasoning in LMs. Through empirical analysis, we uncover strong positive correlations between high-entropy regions and three types of exploratory reasoning actions: (1) pivotal tokens that determine or connect logical steps, (2) reflective actions such as self-verification and correction, and (3) rare behaviors under-explored by the base LMs. Motivated by this, we introduce a minimal modification to standard RL with only one line of code: augmenting the advantage function with an entropy-based term. Unlike traditional maximum-entropy methods which encourage exploration by promoting uncertainty, we encourage exploration by promoting longer and deeper reasoning chains. Notably, our method achieves significant gains on the Pass@K metric -- an upper-bound estimator of LM reasoning capabilities -- even when evaluated with extremely large K values, pushing the boundaries of LM reasoning.

</details>


### [106] [From Bytes to Ideas: Language Modeling with Autoregressive U-Nets](https://arxiv.org/abs/2506.14761)
*Mathurin Videau,Badr Youbi Idrissi,Alessandro Leite,Marc Schoenauer,Olivier Teytaud,David Lopez-Paz*

Main category: cs.CL

TL;DR: 论文提出了一种自回归U-Net模型，通过学习动态嵌入token来替代传统的静态分词方法（如BPE），实现了多尺度序列处理。


<details>
  <summary>Details</summary>
Motivation: 传统分词方法（如BPE）固定了词汇表和输入粒度，限制了模型的灵活性和预测能力。

Method: 使用自回归U-Net模型，从原始字节开始逐步聚合为单词、词组和多词组合，实现多尺度序列处理。

Result: 浅层结构与BPE基线性能相当，深层结构展现出潜力；模型能同时处理字符级任务和跨低资源语言知识迁移。

Conclusion: 动态分词方法提高了模型的灵活性和多任务能力，为未来研究提供了新方向。

Abstract: Tokenization imposes a fixed granularity on the input text, freezing how a language model operates on data and how far in the future it predicts. Byte Pair Encoding (BPE) and similar schemes split text once, build a static vocabulary, and leave the model stuck with that choice. We relax this rigidity by introducing an autoregressive U-Net that learns to embed its own tokens as it trains. The network reads raw bytes, pools them into words, then pairs of words, then up to 4 words, giving it a multi-scale view of the sequence. At deeper stages, the model must predict further into the future -- anticipating the next few words rather than the next byte -- so deeper stages focus on broader semantic patterns while earlier stages handle fine details. When carefully tuning and controlling pretraining compute, shallow hierarchies tie strong BPE baselines, and deeper hierarchies have a promising trend. Because tokenization now lives inside the model, the same system can handle character-level tasks and carry knowledge across low-resource languages.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [107] [Non-planar Object Detection and Identification by Features Matching and Triangulation Growth](https://arxiv.org/abs/2506.13769)
*Filippo Leveni*

Main category: cs.CV

TL;DR: 提出了一种基于特征的方法，通过逐步匹配模板和场景图像中的特征来检测和识别变形对象，利用Delaunay三角剖分指导匹配过程，适用于非平面或变形对象的检测。


<details>
  <summary>Details</summary>
Motivation: 解决传统几何模型（如单应性）无法处理非平面或变形对象检测的问题。

Method: 使用Delaunay三角剖分作为图结构，逐步评估特征匹配的局部一致性，结合几何和光度特性。

Result: 在无变形情况下性能与基于单应性的RANSAC相当，变形显著时表现更优。

Conclusion: 该方法有效解决了非平面或变形对象的检测问题，性能优于传统方法。

Abstract: Object detection and identification is surely a fundamental topic in the computer vision field; it plays a crucial role in many applications such as object tracking, industrial robots control, image retrieval, etc. We propose a feature-based approach for detecting and identifying distorted occurrences of a given template in a scene image by incremental grouping of feature matches between the image and the template. For this purpose, we consider the Delaunay triangulation of template features as an useful tool through which to be guided in this iterative approach. The triangulation is treated as a graph and, starting from a single triangle, neighboring nodes are considered and the corresponding features are identified; then matches related to them are evaluated to determine if they are worthy to be grouped. This evaluation is based on local consistency criteria derived from geometric and photometric properties of local features. Our solution allows the identification of the object in situations where geometric models (e.g. homography) does not hold, thus enable the detection of objects such that the template is non planar or when it is planar but appears distorted in the image. We show that our approach performs just as well or better than application of homography-based RANSAC in scenarios in which distortion is nearly absent, while when the deformation becomes relevant our method shows better description performance.

</details>


### [108] [CDST: Color Disentangled Style Transfer for Universal Style Reference Customization](https://arxiv.org/abs/2506.13770)
*Shiwen Zhang,Zhuowei Chen,Lang Chen,Yanze Wu*

Main category: cs.CV

TL;DR: CDST是一种新颖的双流风格迁移训练范式，通过完全分离颜色与风格，实现无需调优的通用风格迁移。


<details>
  <summary>Details</summary>
Motivation: 解决风格迁移中颜色与风格混淆的问题，并首次实现无需调优的特征保留风格迁移。

Method: 采用双流训练范式，通过多特征图像嵌入压缩和基于Diffusion UNet解耦定律的新风格定义。

Result: 在多种风格迁移任务中取得最先进效果，并通过实验和人工评估验证。

Conclusion: CDST在风格相似性和编辑能力上表现优异，为风格迁移领域提供了新思路。

Abstract: We introduce Color Disentangled Style Transfer (CDST), a novel and efficient two-stream style transfer training paradigm which completely isolates color from style and forces the style stream to be color-blinded. With one same model, CDST unlocks universal style transfer capabilities in a tuning-free manner during inference. Especially, the characteristics-preserved style transfer with style and content references is solved in the tuning-free way for the first time. CDST significantly improves the style similarity by multi-feature image embeddings compression and preserves strong editing capability via our new CDST style definition inspired by Diffusion UNet disentanglement law. By conducting thorough qualitative and quantitative experiments and human evaluations, we demonstrate that CDST achieves state-of-the-art results on various style transfer tasks.

</details>


### [109] [Hidden Bias in the Machine: Stereotypes in Text-to-Image Models](https://arxiv.org/abs/2506.13780)
*Sedat Porikli,Vedat Porikli*

Main category: cs.CV

TL;DR: 研究探讨了文本到图像（T2I）模型在生成图像时如何复制和放大社会偏见，揭示了性别、种族等方面的显著差异，并呼吁更包容的数据集和开发实践。


<details>
  <summary>Details</summary>
Motivation: T2I模型在视觉内容创作中表现出色，但可能复制和放大社会偏见，因此需要系统性研究其影响。

Method: 使用Stable Diffusion 1.5和Flux-1模型生成16,000多张图像，并收集8,000张Google搜索图像进行比较分析。

Result: 生成图像中性别、种族、年龄等人类因素存在显著差异，反映了社会中的有害刻板印象。

Conclusion: 研究强调了改进数据集和开发实践的必要性，以促进生成视觉系统的公平性。

Abstract: Text-to-Image (T2I) models have transformed visual content creation, producing highly realistic images from natural language prompts. However, concerns persist around their potential to replicate and magnify existing societal biases. To investigate these issues, we curated a diverse set of prompts spanning thematic categories such as occupations, traits, actions, ideologies, emotions, family roles, place descriptions, spirituality, and life events. For each of the 160 unique topics, we crafted multiple prompt variations to reflect a wide range of meanings and perspectives. Using Stable Diffusion 1.5 (UNet-based) and Flux-1 (DiT-based) models with original checkpoints, we generated over 16,000 images under consistent settings. Additionally, we collected 8,000 comparison images from Google Image Search. All outputs were filtered to exclude abstract, distorted, or nonsensical results. Our analysis reveals significant disparities in the representation of gender, race, age, somatotype, and other human-centric factors across generated images. These disparities often mirror and reinforce harmful stereotypes embedded in societal narratives. We discuss the implications of these findings and emphasize the need for more inclusive datasets and development practices to foster fairness in generative visual systems.

</details>


### [110] [Fake it till You Make it: Reward Modeling as Discriminative Prediction](https://arxiv.org/abs/2506.13846)
*Runtao Liu,Jiahao Zhan,Yingqing He,Chen Wei,Alan Yuille,Qifeng Chen*

Main category: cs.CV

TL;DR: 本文提出GAN-RM，一种无需人工标注或显式质量维度设计的奖励建模框架，通过对抗训练实现高效奖励建模。


<details>
  <summary>Details</summary>
Motivation: 现有奖励建模方法依赖大量人工标注或复杂质量维度设计，实现复杂且不完整。

Method: 利用对抗训练，通过少量目标样本（Preference Proxy Data）与模型生成样本的判别训练奖励模型。

Result: 实验证明GAN-RM在多项应用中有效，如Best-of-N样本过滤、监督微调（SFT）和直接偏好优化（DPO）。

Conclusion: GAN-RM简化了奖励建模流程，显著减少人工和工程成本，适用于多种增强学习任务。

Abstract: An effective reward model plays a pivotal role in reinforcement learning for post-training enhancement of visual generative models. However, current approaches of reward modeling suffer from implementation complexity due to their reliance on extensive human-annotated preference data or meticulously engineered quality dimensions that are often incomplete and engineering-intensive. Inspired by adversarial training in generative adversarial networks (GANs), this paper proposes GAN-RM, an efficient reward modeling framework that eliminates manual preference annotation and explicit quality dimension engineering. Our method trains the reward model through discrimination between a small set of representative, unpaired target samples(denoted as Preference Proxy Data) and model-generated ordinary outputs, requiring only a few hundred target samples. Comprehensive experiments demonstrate our GAN-RM's effectiveness across multiple key applications including test-time scaling implemented as Best-of-N sample filtering, post-training approaches like Supervised Fine-Tuning (SFT) and Direct Preference Optimization (DPO).

</details>


### [111] [DeSPITE: Exploring Contrastive Deep Skeleton-Pointcloud-IMU-Text Embeddings for Advanced Point Cloud Human Activity Understanding](https://arxiv.org/abs/2506.13897)
*Thomas Kreutz,Max Mühlhäuser,Alejandro Sanchez Guinea*

Main category: cs.CV

TL;DR: DeSPITE模型通过多模态对比预训练，探索了LiDAR点云、人体骨骼姿态、IMU数据和文本的联合嵌入空间，提升了人类活动理解任务的效果。


<details>
  <summary>Details</summary>
Motivation: 尽管LiDAR是一种隐私保护的有效替代RGB相机的方法，但在多模态对比预训练中尚未充分探索，尤其是在人类活动理解任务中。

Method: 提出DeSPITE模型，通过噪声对比估计学习四种模态的联合嵌入空间，并整合LIPD和Babel数据集实现数据同步。

Result: 实验表明DeSPITE在点云序列的新任务（如骨骼-点云-IMU匹配、检索和时间片段检索）中表现优异，并验证了其在点云HAR中的预训练有效性。

Conclusion: DeSPITE为多模态人类活动理解提供了有效的预训练策略，扩展了LiDAR在相关任务中的应用。

Abstract: Despite LiDAR (Light Detection and Ranging) being an effective privacy-preserving alternative to RGB cameras to perceive human activities, it remains largely underexplored in the context of multi-modal contrastive pre-training for human activity understanding (e.g., human activity recognition (HAR), retrieval, or person re-identification (RE-ID)). To close this gap, our work explores learning the correspondence between LiDAR point clouds, human skeleton poses, IMU data, and text in a joint embedding space. More specifically, we present DeSPITE, a Deep Skeleton-Pointcloud-IMU-Text Embedding model, which effectively learns a joint embedding space across these four modalities through noise contrastive estimation. At the heart of our empirical exploration, we have combined the existing LIPD and Babel datasets, which enabled us to synchronize data of all four modalities, allowing us to explore the learning of a new joint embedding space. Our experiments demonstrate novel human activity understanding tasks for point cloud sequences enabled through DeSPITE, including Skeleton<->Pointcloud<->IMU matching, retrieval, and temporal moment retrieval. Furthermore, we show that DeSPITE is an effective pre-training strategy for point cloud HAR through experiments in MSR-Action3D and HMPEAR.

</details>


### [112] [OPTIMUS: Observing Persistent Transformations in Multi-temporal Unlabeled Satellite-data](https://arxiv.org/abs/2506.13902)
*Raymond Yu,Paul Han,Josh Myers-Dean,Piper Wolters,Favyen Bastani*

Main category: cs.CV

TL;DR: OPTIMUS是一种自监督学习方法，通过检测时间序列中的变化点来识别卫星图像中的持久变化，显著提升了变化检测的性能。


<details>
  <summary>Details</summary>
Motivation: 由于缺乏标注变化标签的卫星数据，尤其是稀有类别变化，监督方法在变化检测中面临挑战。

Method: OPTIMUS基于自监督学习，通过模型恢复时间序列图像的相对顺序信息来推断持久变化。

Result: OPTIMUS在区分变化与未变化时间序列上的AUROC得分从56.3%提升至87.6%。

Conclusion: OPTIMUS为卫星图像中的变化检测提供了一种有效的自监督解决方案。

Abstract: In the face of pressing environmental issues in the 21st century, monitoring surface changes on Earth is more important than ever. Large-scale remote sensing, such as satellite imagery, is an important tool for this task. However, using supervised methods to detect changes is difficult because of the lack of satellite data annotated with change labels, especially for rare categories of change. Annotation proves challenging due to the sparse occurrence of changes in satellite images. Even within a vast collection of images, only a small fraction may exhibit persistent changes of interest. To address this challenge, we introduce OPTIMUS, a self-supervised learning method based on an intuitive principle: if a model can recover information about the relative order of images in the time series, then that implies that there are long-lasting changes in the images. OPTIMUS demonstrates this principle by using change point detection methods on model outputs in a time series. We demonstrate that OPTIMUS can directly detect interesting changes in satellite images, achieving an improvement in AUROC score from 56.3% to 87.6% at distinguishing changed time series from unchanged ones compared to baselines. Our code and dataset are available at https://huggingface.co/datasets/optimus-change/optimus-dataset/.

</details>


### [113] [Intelligent Image Sensing for Crime Analysis: A ML Approach towards Enhanced Violence Detection and Investigation](https://arxiv.org/abs/2506.13910)
*Aritra Dutta,Pushpita Boral,G Suseela*

Main category: cs.CV

TL;DR: 本文提出了一种基于机器学习的暴力检测与分类框架，利用3D卷积神经网络和双向LSTM，显著提升了检测效率和准确性。


<details>
  <summary>Details</summary>
Motivation: 传统监控方法在及时检测多样化暴力行为方面存在局限，亟需自动化解决方案以减少人力和财产损失。

Method: 采用监督学习进行二分类和多分类暴力检测，结合3D CNN和双向LSTM处理时空特征，训练数据来自多样化标注视频。

Result: 框架在计算资源效率和准确性方面表现优异。

Conclusion: 该研究为实时暴力检测提供了高效且准确的解决方案。

Abstract: The increasing global crime rate, coupled with substantial human and property losses, highlights the limitations of traditional surveillance methods in promptly detecting diverse and unexpected acts of violence. Addressing this pressing need for automatic violence detection, we leverage Machine Learning to detect and categorize violent events in video streams. This paper introduces a comprehensive framework for violence detection and classification, employing Supervised Learning for both binary and multi-class violence classification. The detection model relies on 3D Convolutional Neural Networks, while the classification model utilizes the separable convolutional 3D model for feature extraction and bidirectional LSTM for temporal processing. Training is conducted on a diverse customized datasets with frame-level annotations, incorporating videos from surveillance cameras, human recordings, hockey fight, sohas and wvd dataset across various platforms. Additionally, a camera module integrated with raspberry pi is used to capture live video feed, which is sent to the ML model for processing. Thus, demonstrating improved performance in terms of computational resource efficiency and accuracy.

</details>


### [114] [HierVL: Semi-Supervised Segmentation leveraging Hierarchical Vision-Language Synergy with Dynamic Text-Spatial Query Alignment](https://arxiv.org/abs/2506.13925)
*Numair Nadeem,Saeed Anwar,Muhammad Hamza Asad,Abdul Bais*

Main category: cs.CV

TL;DR: HierVL是一个结合视觉语言模型的半监督语义分割框架，通过多尺度查询和跨模态对齐提升性能。


<details>
  <summary>Details</summary>
Motivation: 解决半监督语义分割在标签稀缺和领域变化下的挑战，尤其是视觉方法在相似类别和边界定位上的不足。

Method: 提出HierVL框架，包含分层语义查询生成器、跨模态空间对齐模块和双查询Transformer解码器，结合正则化损失。

Result: 在COCO、Pascal VOC、ADE20和Cityscapes上分别提升4.4%、3.1%、5.9%和1.8%的mIoU，表现优于1%监督下的其他方法。

Conclusion: 语言引导的分割显著提升了标签效率，实现了细粒度和实例感知的泛化能力。

Abstract: Semi-supervised semantic segmentation remains challenging under severe label scarcity and domain variability. Vision-only methods often struggle to generalize, resulting in pixel misclassification between similar classes, poor generalization and boundary localization. Vision-Language Models offer robust, domain-invariant semantics but lack the spatial grounding required for dense prediction. We introduce HierVL, a unified framework that bridges this gap by integrating abstract text embeddings into a mask-transformer architecture tailored for semi-supervised segmentation. HierVL features three novel components: a Hierarchical Semantic Query Generator that filters and projects abstract class embeddings into multi-scale queries to suppress irrelevant classes and handle intra-class variability; a Cross-Modal Spatial Alignment Module that aligns semantic queries with pixel features for sharper boundaries under sparse supervision; and a Dual-Query Transformer Decoder that fuses semantic and instance-level queries to prevent instance collapse. We also introduce targeted regularization losses that maintain vision-language alignment throughout training to reinforce semantic grounding. HierVL establishes a new state-of-the-art by achieving a +4.4% mean improvement of the intersection over the union on COCO (with 232 labeled images), +3.1% on Pascal VOC (with 92 labels), +5.9% on ADE20 (with 158 labels) and +1.8% on Cityscapes (with 100 labels), demonstrating better performance under 1% supervision on four benchmark datasets. Our results show that language-guided segmentation closes the label efficiency gap and unlocks new levels of fine-grained, instance-aware generalization.

</details>


### [115] [Mapping Farmed Landscapes from Remote Sensing](https://arxiv.org/abs/2506.13993)
*Michelangelo Conserva,Alex Wilson,Charlotte Stanton,Vishal Batchu,Varun Gulshan*

Main category: cs.CV

TL;DR: Farmscapes是一种高分辨率（25厘米）的农村景观特征地图，覆盖英格兰大部分地区，通过深度学习模型生成，为生态学家和政策制定者提供开放工具。


<details>
  <summary>Details</summary>
Motivation: 解决全球生物多样性目标中缺乏详细、大规模生态地图的问题。

Method: 使用基于942个手动标注的航空影像训练的深度学习分割模型，生成高分辨率地图。

Result: 模型准确识别关键栖息地，林地F1分数96%，农田95%，树篱72%。

Conclusion: Farmscapes为栖息地恢复、生物多样性监测和景观连通性分析提供了数据支持。

Abstract: Effective management of agricultural landscapes is critical for meeting global biodiversity targets, but efforts are hampered by the absence of detailed, large-scale ecological maps. To address this, we introduce Farmscapes, the first large-scale (covering most of England), high-resolution (25cm) map of rural landscape features, including ecologically vital elements like hedgerows, woodlands, and stone walls. This map was generated using a deep learning segmentation model trained on a novel, dataset of 942 manually annotated tiles derived from aerial imagery. Our model accurately identifies key habitats, achieving high f1-scores for woodland (96\%) and farmed land (95\%), and demonstrates strong capability in segmenting linear features, with an F1-score of 72\% for hedgerows. By releasing the England-wide map on Google Earth Engine, we provide a powerful, open-access tool for ecologists and policymakers. This work enables data-driven planning for habitat restoration, supports the monitoring of initiatives like the EU Biodiversity Strategy, and lays the foundation for advanced analysis of landscape connectivity.

</details>


### [116] [FindMeIfYouCan: Bringing Open Set metrics to $\textit{near} $, $ \textit{far} $ and $\textit{farther}$ Out-of-Distribution Object Detection](https://arxiv.org/abs/2506.14008)
*Daniel Montoya,Aymen Bouguerra,Alexandra Gomez-Villa,Fabio Arnez*

Main category: cs.CV

TL;DR: 论文指出当前OOD-OD评估协议的问题，提出新的评估方法，并分析不同语义距离的OOD对象检测效果。


<details>
  <summary>Details</summary>
Motivation: 解决现有OOD-OD评估协议中假设不重叠对象的问题，避免在部署时对未知对象的过度自信。

Method: 手动整理并丰富现有基准，利用语义相似性创建新的评估分类（near、far、farther），并引入开放集领域的指标。

Result: 语义和视觉接近的OOD对象更容易定位但易与ID对象混淆，而far和farther对象更难定位但不易误判。

Conclusion: 新评估方法更全面，揭示了OOD检测中的关键问题，为实际应用提供了更可靠的评估标准。

Abstract: State-of-the-art Object Detection (OD) methods predominantly operate under a closed-world assumption, where test-time categories match those encountered during training. However, detecting and localizing unknown objects is crucial for safety-critical applications in domains such as autonomous driving and medical imaging. Recently, Out-Of-Distribution (OOD) detection has emerged as a vital research direction for OD, focusing on identifying incorrect predictions typically associated with unknown objects. This paper shows that the current evaluation protocol for OOD-OD violates the assumption of non-overlapping objects with respect to the In-Distribution (ID) datasets, and obscures crucial situations such as ignoring unknown objects, potentially leading to overconfidence in deployment scenarios where truly novel objects might be encountered. To address these limitations, we manually curate, and enrich the existing benchmark by exploiting semantic similarity to create new evaluation splits categorized as $\textit{near}$, $\textit{far}$, and $\textit{farther}$ from ID distributions. Additionally, we incorporate established metrics from the Open Set community, providing deeper insights into how effectively methods detect unknowns, when they ignore them, and when they mistakenly classify OOD objects as ID. Our comprehensive evaluation demonstrates that semantically and visually close OOD objects are easier to localize than far ones, but are also more easily confounded with ID objects. $\textit{Far}$ and $\textit{farther}$ objects are harder to localize but less prone to be taken for an ID object.

</details>


### [117] [Disentangling 3D from Large Vision-Language Models for Controlled Portrait Generation](https://arxiv.org/abs/2506.14015)
*Nick Yiwen Huang,Akin Caliskan,Berkay Kicanaoglu,James Tompkin,Hyeongwoo Kim*

Main category: cs.CV

TL;DR: 该论文提出了一种从大型视觉语言模型中解耦3D信息的方法，实现了对生成3D肖像的文本和几何控制。


<details>
  <summary>Details</summary>
Motivation: 解决从大型视觉语言模型中解耦3D信息的挑战，以实现对肖像外观和几何的自由控制。

Method: 使用预训练的视觉语言模型（CLIP）和3D可变形模型（FLAME），通过规范化2D参考帧和Jacobian正则化来解耦噪声。

Result: 生成的肖像在文本和3D控制下保持一致性和多样性，优于现有方法。

Conclusion: 该方法为创作者提供了无需大规模标注或训练即可控制3D生成器的工具。

Abstract: We consider the problem of disentangling 3D from large vision-language models, which we show on generative 3D portraits. This allows free-form text control of appearance attributes like age, hair style, and glasses, and 3D geometry control of face expression and camera pose. In this setting, we assume we use a pre-trained large vision-language model (LVLM; CLIP) to generate from a smaller 2D dataset with no additional paired labels and with a pre-defined 3D morphable model (FLAME). First, we disentangle using canonicalization to a 2D reference frame from a deformable neural 3D triplane representation. But another form of entanglement arises from the significant noise in the LVLM's embedding space that describes irrelevant features. This damages output quality and diversity, but we overcome this with a Jacobian regularization that can be computed efficiently with a stochastic approximator. Compared to existing methods, our approach produces portraits with added text and 3D control, where portraits remain consistent when either control is changed. Broadly, this approach lets creators control 3D generators on their own 2D face data without needing resources to label large data or train large models.

</details>


### [118] [SimpleDoc: Multi-Modal Document Understanding with Dual-Cue Page Retrieval and Iterative Refinement](https://arxiv.org/abs/2506.14035)
*Chelsi Jain,Yiran Wu,Yifan Zeng,Jiale Liu,S hengyu Dai,Zhenwen Shao,Qingyun Wu,Huazheng Wang*

Main category: cs.CV

TL;DR: SimpleDoc是一个轻量级但高效的DocVQA框架，通过双线索检索和迭代工作记忆提升性能。


<details>
  <summary>Details</summary>
Motivation: DocVQA任务需要处理多页和多模态信息，现有方法依赖VLM嵌入模型，但效率不高。

Method: SimpleDoc采用双线索检索（嵌入相似性和页面摘要重排序）和迭代VLM推理。

Result: 在4个DocVQA数据集上平均提升3.2%，且检索页数更少。

Conclusion: SimpleDoc通过高效检索和迭代推理显著提升了DocVQA性能。

Abstract: Document Visual Question Answering (DocVQA) is a practical yet challenging task, which is to ask questions based on documents while referring to multiple pages and different modalities of information, e.g, images and tables. To handle multi-modality, recent methods follow a similar Retrieval Augmented Generation (RAG) pipeline, but utilize Visual Language Models (VLMs) based embedding model to embed and retrieve relevant pages as images, and generate answers with VLMs that can accept an image as input. In this paper, we introduce SimpleDoc, a lightweight yet powerful retrieval - augmented framework for DocVQA. It boosts evidence page gathering by first retrieving candidates through embedding similarity and then filtering and re-ranking these candidates based on page summaries. A single VLM-based reasoner agent repeatedly invokes this dual-cue retriever, iteratively pulling fresh pages into a working memory until the question is confidently answered. SimpleDoc outperforms previous baselines by 3.2% on average on 4 DocVQA datasets with much fewer pages retrieved. Our code is available at https://github.com/ag2ai/SimpleDoc.

</details>


### [119] [Image Segmentation with Large Language Models: A Survey with Perspectives for Intelligent Transportation Systems](https://arxiv.org/abs/2506.14096)
*Sanjeda Akter,Ibne Farabi Shihab,Anuj Sharma*

Main category: cs.CV

TL;DR: 本文综述了大型语言模型（LLM）与计算机视觉结合在图像分割领域的应用，特别是在智能交通系统（ITS）中的潜力、挑战与未来方向。


<details>
  <summary>Details</summary>
Motivation: 智能交通系统需要高精度的场景理解以确保安全和效率，而LLM与计算机视觉的结合为此提供了新的可能性。

Method: 系统回顾了LLM增强的图像分割方法，基于提示机制和核心架构对现有方法进行了分类。

Result: 这些创新技术可以提升自动驾驶、交通监控和基础设施维护中的道路场景理解能力。

Conclusion: 未来需解决实时性和安全可靠性等挑战，并强调可解释、以人为本的AI是下一代交通系统成功部署的关键。

Abstract: The integration of Large Language Models (LLMs) with computer vision is profoundly transforming perception tasks like image segmentation. For intelligent transportation systems (ITS), where accurate scene understanding is critical for safety and efficiency, this new paradigm offers unprecedented capabilities. This survey systematically reviews the emerging field of LLM-augmented image segmentation, focusing on its applications, challenges, and future directions within ITS. We provide a taxonomy of current approaches based on their prompting mechanisms and core architectures, and we highlight how these innovations can enhance road scene understanding for autonomous driving, traffic monitoring, and infrastructure maintenance. Finally, we identify key challenges, including real-time performance and safety-critical reliability, and outline a perspective centered on explainable, human-centric AI as a prerequisite for the successful deployment of this technology in next-generation transportation systems.

</details>


### [120] [FADPNet: Frequency-Aware Dual-Path Network for Face Super-Resolution](https://arxiv.org/abs/2506.14121)
*Siyu Xu,Wenjie Li,Guangwei Gao,Jian Yang,Guo-Jun Qi,Chia-Wen Lin*

Main category: cs.CV

TL;DR: 提出了一种频率感知双路径网络（FADPNet），通过分离处理低频和高频面部特征，优化计算资源分配，提升面部超分辨率性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法对所有面部像素平等处理，导致计算资源分配不优和性能下降。CNN对高频特征敏感，而Mamba擅长低频特征且复杂度低。

Method: FADPNet将特征分解为低频和高频，分别用Mamba和CNN处理。低频使用LFEB模块，高频使用DPA和HFR模块。

Result: 方法在超分辨率质量和模型效率间取得平衡，优于现有方法。

Conclusion: FADPNet通过频率感知设计，显著提升了面部超分辨率性能。

Abstract: Face super-resolution (FSR) under limited computational costs remains an open problem. Existing approaches typically treat all facial pixels equally, resulting in suboptimal allocation of computational resources and degraded FSR performance. CNN is relatively sensitive to high-frequency facial features, such as component contours and facial outlines. Meanwhile, Mamba excels at capturing low-frequency features like facial color and fine-grained texture, and does so with lower complexity than Transformers. Motivated by these observations, we propose FADPNet, a Frequency-Aware Dual-Path Network that decomposes facial features into low- and high-frequency components and processes them via dedicated branches. For low-frequency regions, we introduce a Mamba-based Low-Frequency Enhancement Block (LFEB), which combines state-space attention with squeeze-and-excitation operations to extract low-frequency global interactions and emphasize informative channels. For high-frequency regions, we design a CNN-based Deep Position-Aware Attention (DPA) module to enhance spatially-dependent structural details, complemented by a lightweight High-Frequency Refinement (HFR) module that further refines frequency-specific representations. Through the above designs, our method achieves an excellent balance between FSR quality and model efficiency, outperforming existing approaches.

</details>


### [121] [KDMOS:Knowledge Distillation for Motion Segmentation](https://arxiv.org/abs/2506.14130)
*Chunyu Cao,Jintao Cheng,Zeyu Chen,Linfan Zhan,Rui Fan,Zhijian He,Xiaoyu Tang*

Main category: cs.CV

TL;DR: 本文提出了一种基于logits的知识蒸馏框架（KDMOS），用于运动目标分割（MOS），旨在提高精度并保持实时效率。通过BEV投影模型（学生）和非投影模型（教师）的结合，以及针对运动与非运动类别的解耦蒸馏策略，显著减少了误报和漏报。动态上采样和网络架构优化进一步提升了性能。


<details>
  <summary>Details</summary>
Motivation: 运动目标分割在自动驾驶中至关重要，但现有方法在精度与实时性之间难以平衡。

Method: 采用BEV投影模型作为学生，非投影模型作为教师，解耦运动与非运动类别并应用定制蒸馏策略，引入动态上采样和网络架构优化。

Result: 在SemanticKITTI-MOS隐藏测试集上达到78.8%的IoU，并在Apollo数据集上表现优异，参数量减少7.69%。

Conclusion: KDMOS框架有效提升了MOS任务的精度和实时性，为自动驾驶应用提供了实用解决方案。

Abstract: Motion Object Segmentation (MOS) is crucial for autonomous driving, as it enhances localization, path planning, map construction, scene flow estimation, and future state prediction. While existing methods achieve strong performance, balancing accuracy and real-time inference remains a challenge. To address this, we propose a logits-based knowledge distillation framework for MOS, aiming to improve accuracy while maintaining real-time efficiency. Specifically, we adopt a Bird's Eye View (BEV) projection-based model as the student and a non-projection model as the teacher. To handle the severe imbalance between moving and non-moving classes, we decouple them and apply tailored distillation strategies, allowing the teacher model to better learn key motion-related features. This approach significantly reduces false positives and false negatives. Additionally, we introduce dynamic upsampling, optimize the network architecture, and achieve a 7.69% reduction in parameter count, mitigating overfitting. Our method achieves a notable IoU of 78.8% on the hidden test set of the SemanticKITTI-MOS dataset and delivers competitive results on the Apollo dataset. The KDMOS implementation is available at https://github.com/SCNU-RISLAB/KDMOS.

</details>


### [122] [Interpreting Biomedical VLMs on High-Imbalance Out-of-Distributions: An Insight into BiomedCLIP on Radiology](https://arxiv.org/abs/2506.14136)
*Nafiz Sadman,Farhana Zulkernine,Benjamin Kwan*

Main category: cs.CV

TL;DR: 研究分析了BiomedCLIP在医学数据集上的表现，发现零样本推理效果差，全微调效果较好，线性探测能识别重叠特征。


<details>
  <summary>Details</summary>
Motivation: 探索BiomedCLIP的嵌入空间并量化其在高度不平衡、分布外医学数据集上的局限性。

Method: 在IU-xray数据集上评估BiomedCLIP的零样本推理、全微调和线性探测三种分类方式。

Result: 零样本推理效果差，全微调改善分类，线性探测识别重叠特征。

Conclusion: 需谨慎调整模型以提高其在真实场景中的可靠性和适用性。

Abstract: In this paper, we construct two research objectives: i) explore the learned embedding space of BiomedCLIP, an open-source large vision language model, to analyse meaningful class separations, and ii) quantify the limitations of BiomedCLIP when applied to a highly imbalanced, out-of-distribution multi-label medical dataset. We experiment on IU-xray dataset, which exhibits the aforementioned criteria, and evaluate BiomedCLIP in classifying images (radiographs) in three contexts: zero-shot inference, full finetuning, and linear probing. The results show that the model under zero-shot settings over-predicts all labels, leading to poor precision and inter-class separability. Full fine-tuning improves classification of distinct diseases, while linear probing detects overlapping features. We demonstrate visual understanding of the model using Grad-CAM heatmaps and compare with 15 annotations by a radiologist. We highlight the need for careful adaptations of the models to foster reliability and applicability in a real-world setting. The code for the experiments in this work is available and maintained on GitHub.

</details>


### [123] [RadFabric: Agentic AI System with Reasoning Capability for Radiology](https://arxiv.org/abs/2506.14142)
*Wenting Chen,Yi Dong,Zhaojun Ding,Yucheng Shi,Yifan Zhou,Fang Zeng,Yijun Luo,Tianyu Lin,Yihang Su,Yichen Wu,Kai Zhang,Zhen Xiang,Tianming Liu,Ninghao Liu,Lichao Sun,Yixuan Yuan,Xiang Li*

Main category: cs.CV

TL;DR: RadFabric是一个多代理、多模态推理框架，用于提升胸部X光片的综合诊断能力，通过视觉和文本分析的结合实现高精度和透明诊断。


<details>
  <summary>Details</summary>
Motivation: 当前自动化系统在胸部X光片诊断中存在病理覆盖不足、准确性低以及视觉与文本推理整合不足的问题。

Method: 基于模型上下文协议（MCP），RadFabric整合了病理检测代理、解剖解释代理和推理代理，结合多模态数据进行分析。

Result: RadFabric在骨折检测上达到1.000准确率，整体诊断准确率（0.799）显著优于传统系统（0.229至0.527）。

Conclusion: RadFabric通过跨模态特征对齐和偏好驱动推理，推动了AI驱动的放射学向透明、解剖精确和临床可操作的诊断发展。

Abstract: Chest X ray (CXR) imaging remains a critical diagnostic tool for thoracic conditions, but current automated systems face limitations in pathology coverage, diagnostic accuracy, and integration of visual and textual reasoning. To address these gaps, we propose RadFabric, a multi agent, multimodal reasoning framework that unifies visual and textual analysis for comprehensive CXR interpretation. RadFabric is built on the Model Context Protocol (MCP), enabling modularity, interoperability, and scalability for seamless integration of new diagnostic agents. The system employs specialized CXR agents for pathology detection, an Anatomical Interpretation Agent to map visual findings to precise anatomical structures, and a Reasoning Agent powered by large multimodal reasoning models to synthesize visual, anatomical, and clinical data into transparent and evidence based diagnoses. RadFabric achieves significant performance improvements, with near-perfect detection of challenging pathologies like fractures (1.000 accuracy) and superior overall diagnostic accuracy (0.799) compared to traditional systems (0.229 to 0.527). By integrating cross modal feature alignment and preference-driven reasoning, RadFabric advances AI-driven radiology toward transparent, anatomically precise, and clinically actionable CXR analysis.

</details>


### [124] [SceneAware: Scene-Constrained Pedestrian Trajectory Prediction with LLM-Guided Walkability](https://arxiv.org/abs/2506.14144)
*Juho Bai,Inwook Shim*

Main category: cs.CV

TL;DR: 论文提出SceneAware框架，通过结合场景理解和行人轨迹预测，显著提升预测准确性。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要关注行人间的社交互动，忽略了环境对行人轨迹的重要影响。

Method: 使用Vision Transformer编码场景信息，结合多模态大语言模型生成可通行区域掩码，并通过Transformer编码轨迹。

Result: 在ETH/UCY数据集上表现优于现有方法，提升50%以上。

Conclusion: 场景信息对行人轨迹预测至关重要，SceneAware方法有效且可靠。

Abstract: Accurate prediction of pedestrian trajectories is essential for applications in robotics and surveillance systems. While existing approaches primarily focus on social interactions between pedestrians, they often overlook the rich environmental context that significantly shapes human movement patterns. In this paper, we propose SceneAware, a novel framework that explicitly incorporates scene understanding to enhance trajectory prediction accuracy. Our method leverages a Vision Transformer~(ViT) scene encoder to process environmental context from static scene images, while Multi-modal Large Language Models~(MLLMs) generate binary walkability masks that distinguish between accessible and restricted areas during training. We combine a Transformer-based trajectory encoder with the ViT-based scene encoder, capturing both temporal dynamics and spatial constraints. The framework integrates collision penalty mechanisms that discourage predicted trajectories from violating physical boundaries, ensuring physically plausible predictions. SceneAware is implemented in both deterministic and stochastic variants. Comprehensive experiments on the ETH/UCY benchmark datasets show that our approach outperforms state-of-the-art methods, with more than 50\% improvement over previous models. Our analysis based on different trajectory categories shows that the model performs consistently well across various types of pedestrian movement. This highlights the importance of using explicit scene information and shows that our scene-aware approach is both effective and reliable in generating accurate and physically plausible predictions. Code is available at: https://github.com/juho127/SceneAware.

</details>


### [125] [VideoMAR: Autoregressive Video Generatio with Continuous Tokens](https://arxiv.org/abs/2506.14168)
*Hu Yu,Biao Gong,Hangjie Yuan,DanDan Zheng,Weilong Chai,Jingdong Chen,Kecheng Zheng,Feng Zhao*

Main category: cs.CV

TL;DR: VideoMAR是一种高效的自回归视频生成模型，结合了时间因果性和空间双向性，通过课程学习和渐进分辨率训练解决了长序列建模问题，性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 探索基于掩码的自回归模型在视频生成中的潜力，解决长序列建模的高成本和难度问题。

Method: 提出VideoMAR模型，结合时间因果性和空间双向性，采用下一帧扩散损失、课程学习和渐进分辨率训练，并引入3D旋转嵌入。

Result: 在VBench-I2V基准测试中，VideoMAR性能优于Cosmos I2V，且参数、训练数据和GPU资源需求显著减少。

Conclusion: VideoMAR展示了自回归模型在视频生成中的高效性和潜力，为未来研究提供了新方向。

Abstract: Masked-based autoregressive models have demonstrated promising image generation capability in continuous space. However, their potential for video generation remains under-explored. In this paper, we propose \textbf{VideoMAR}, a concise and efficient decoder-only autoregressive image-to-video model with continuous tokens, composing temporal frame-by-frame and spatial masked generation. We first identify temporal causality and spatial bi-directionality as the first principle of video AR models, and propose the next-frame diffusion loss for the integration of mask and video generation. Besides, the huge cost and difficulty of long sequence autoregressive modeling is a basic but crucial issue. To this end, we propose the temporal short-to-long curriculum learning and spatial progressive resolution training, and employ progressive temperature strategy at inference time to mitigate the accumulation error. Furthermore, VideoMAR replicates several unique capacities of language models to video generation. It inherently bears high efficiency due to simultaneous temporal-wise KV cache and spatial-wise parallel generation, and presents the capacity of spatial and temporal extrapolation via 3D rotary embeddings. On the VBench-I2V benchmark, VideoMAR surpasses the previous state-of-the-art (Cosmos I2V) while requiring significantly fewer parameters ($9.3\%$), training data ($0.5\%$), and GPU resources ($0.2\%$).

</details>


### [126] [A multi-stage augmented multimodal interaction network for fish feeding intensity quantification](https://arxiv.org/abs/2506.14170)
*Shulong Zhang,Mingyuan Yao,Jiayin Zhao,Xiao Liu,Haihua Wang*

Main category: cs.CV

TL;DR: 本文提出了一种多阶段增强多模态交互网络（MAINet），用于量化鱼类摄食强度，通过多模态数据融合和决策推理显著提高了准确性和可靠性。


<details>
  <summary>Details</summary>
Motivation: 现有研究在多模态选择、特征提取与融合以及决策推理方面存在局限性，影响了模型的准确性、适用性和可靠性。

Method: 提出通用特征提取框架、辅助模态增强主模态机制（ARPM）和证据推理（ER）规则，实现多模态数据的高效融合与决策。

Result: MAINet在准确率、精确率、召回率和F1分数上均超过96.7%，显著优于其他对比模型。

Conclusion: MAINet通过改进策略提升了模型的鲁棒性和特征利用效率，有效提高了鱼类摄食强度量化的准确性。

Abstract: In recirculating aquaculture systems, accurate and effective assessment of fish feeding intensity is crucial for reducing feed costs and calculating optimal feeding times. However, current studies have limitations in modality selection, feature extraction and fusion, and co-inference for decision making, which restrict further improvement in the accuracy, applicability and reliability of multimodal fusion models. To address this problem, this study proposes a Multi-stage Augmented Multimodal Interaction Network (MAINet) for quantifying fish feeding intensity. Firstly, a general feature extraction framework is proposed to efficiently extract feature information from input image, audio and water wave datas. Second, an Auxiliary-modality Reinforcement Primary-modality Mechanism (ARPM) is designed for inter-modal interaction and generate enhanced features, which consists of a Channel Attention Fusion Network (CAFN) and a Dual-mode Attention Fusion Network (DAFN). Finally, an Evidence Reasoning (ER) rule is introduced to fuse the output results of each modality and make decisions, thereby completing the quantification of fish feeding intensity. The experimental results show that the constructed MAINet reaches 96.76%, 96.78%, 96.79% and 96.79% in accuracy, precision, recall and F1-Score respectively, and its performance is significantly higher than the comparison models. Compared with models that adopt single-modality, dual-modality fusion and different decision-making fusion methods, it also has obvious advantages. Meanwhile, the ablation experiments further verified the key role of the proposed improvement strategy in improving the robustness and feature utilization efficiency of model, which can effectively improve the accuracy of the quantitative results of fish feeding intensity.

</details>


### [127] [One-Shot Neural Architecture Search with Network Similarity Directed Initialization for Pathological Image Classification](https://arxiv.org/abs/2506.14176)
*Renao Yan*

Main category: cs.CV

TL;DR: 提出NSDI策略和领域适应方法，优化病理图像分析的神经网络搜索。


<details>
  <summary>Details</summary>
Motivation: 现有方法直接应用计算机视觉模型，忽略病理图像特性，导致计算效率低下。

Method: 引入NSDI策略和领域适应，改进神经架构搜索。

Result: 在BRACS数据集上表现优于现有方法，分类性能和特征定位更优。

Conclusion: 方法有效提升病理图像分析的效率和准确性。

Abstract: Deep learning-based pathological image analysis presents unique challenges due to the practical constraints of network design. Most existing methods apply computer vision models directly to medical tasks, neglecting the distinct characteristics of pathological images. This mismatch often leads to computational inefficiencies, particularly in edge-computing scenarios. To address this, we propose a novel Network Similarity Directed Initialization (NSDI) strategy to improve the stability of neural architecture search (NAS). Furthermore, we introduce domain adaptation into one-shot NAS to better handle variations in staining and semantic scale across pathology datasets. Experiments on the BRACS dataset demonstrate that our method outperforms existing approaches, delivering both superior classification performance and clinically relevant feature localization.

</details>


### [128] [Meta-SurDiff: Classification Diffusion Model Optimized by Meta Learning is Reliable for Online Surgical Phase Recognition](https://arxiv.org/abs/2506.14181)
*Yufei Li,Jirui Wu,Long Tian,Liming Wang,Xiaonan Liu,Zijun Liu,Xiyang Liu*

Main category: cs.CV

TL;DR: 该论文提出了一种基于元学习优化的分类扩散模型（Meta-SurDiff），用于解决手术视频中帧模糊性和手术阶段分布不平衡带来的不确定性，以实现可靠的在线手术阶段识别。


<details>
  <summary>Details</summary>
Motivation: 在线手术阶段识别因其与人类生命健康相关的潜在应用而备受关注，但现有深度模型未充分探索和建模手术视频中的不确定性。

Method: 通过分类扩散模型评估模糊帧的识别置信度，并利用元学习目标优化扩散模型以增强分类边界的鲁棒性。

Result: 在五个广泛使用的数据集（Cholec80、AutoLaparo、M2Cai16、OphNet、NurViD）上通过多种实用指标验证了Meta-SurDiff的有效性。

Conclusion: Meta-SurDiff能够有效解决手术视频中的不确定性，提升在线手术阶段识别的可靠性。

Abstract: Online surgical phase recognition has drawn great attention most recently due to its potential downstream applications closely related to human life and health. Despite deep models have made significant advances in capturing the discriminative long-term dependency of surgical videos to achieve improved recognition, they rarely account for exploring and modeling the uncertainty in surgical videos, which should be crucial for reliable online surgical phase recognition. We categorize the sources of uncertainty into two types, frame ambiguity in videos and unbalanced distribution among surgical phases, which are inevitable in surgical videos. To address this pivot issue, we introduce a meta-learning-optimized classification diffusion model (Meta-SurDiff), to take full advantage of the deep generative model and meta-learning in achieving precise frame-level distribution estimation for reliable online surgical phase recognition. For coarse recognition caused by ambiguous video frames, we employ a classification diffusion model to assess the confidence of recognition results at a finer-grained frame-level instance. For coarse recognition caused by unbalanced phase distribution, we use a meta-learning based objective to learn the diffusion model, thus enhancing the robustness of classification boundaries for different surgical phases.We establish effectiveness of Meta-SurDiff in online surgical phase recognition through extensive experiments on five widely used datasets using more than four practical metrics. The datasets include Cholec80, AutoLaparo, M2Cai16, OphNet, and NurViD, where OphNet comes from ophthalmic surgeries, NurViD is the daily care dataset, while the others come from laparoscopic surgeries. We will release the code upon acceptance.

</details>


### [129] [Egocentric Human-Object Interaction Detection: A New Benchmark and Method](https://arxiv.org/abs/2506.14189)
*Kunyuan Deng,Yi Wang,Lap-Pui Chau*

Main category: cs.CV

TL;DR: 本文提出了Ego-HOIBench数据集，用于推动以自我为中心视角的人-物交互（Ego-HOI）检测研究，并提出了Hand Geometry and Interactivity Refinement（HGIR）方案，显著提升了检测性能。


<details>
  <summary>Details</summary>
Motivation: 现有的人-物交互（HOI）检测方法主要关注第三人称视角，忽视了以自我为中心视角（Ego-HOI）的直观性。本文旨在填补这一空白。

Method: 提出了Ego-HOIBench数据集，包含27K+以自我为中心的图像，标注了手-动词-物体三元组。并提出了HGIR方案，利用手部姿态和几何信息优化交互特征。

Result: HGIR方案显著提升了Ego-HOI检测能力，并在Ego-HOIBench上实现了最先进的性能。

Conclusion: Ego-HOIBench和HGIR方案为Ego-HOI检测提供了新的基准和方法，具有轻量级和高效的特点。

Abstract: Understanding the interaction between humans and objects has gained much attention in recent years. Existing human-object interaction (HOI) detection methods mainly focus on the third-person perspectives, overlooking a more intuitive way from the egocentric view of HOI, namely Ego-HOI. This paper introduces an Ego-HOIBench, a new dataset to promote the benchmarking and development of Ego-HOI detection. Our Ego-HOIBench comprises more than 27K egocentric images with high-quality hand-verb-object triplet annotations across 123 fine-grained interaction categories and locations, covering a rich diversity of scenarios, object types, and hand configurations in daily activities. In addition, we explore and adapt third-person HOI detection methods to Ego-HOIBench and illustrate the challenges of hand-occluded objects and the complexity of single- and two-hand interactions. To build a new baseline, we propose a Hand Geometry and Interactivity Refinement (HGIR) scheme, which leverages hand pose and geometric information as valuable cues for interpreting interactions. Specifically, the HGIR scheme explicitly extracts global hand geometric features from the estimated hand pose proposals and refines the interaction-specific features using pose-interaction attention. This scheme enables the model to obtain a robust and powerful interaction representation, significantly improving the Ego-HOI detection capability. Our approach is lightweight and effective, and it can be easily applied to HOI baselines in a plug-and-play manner to achieve state-of-the-art results on Ego-HOIBench. Our project is available at: https://dengkunyuan.github.io/EgoHOIBench/

</details>


### [130] [HRGS: Hierarchical Gaussian Splatting for Memory-Efficient High-Resolution 3D Reconstruction](https://arxiv.org/abs/2506.14229)
*Changbai Li,Haodong Zhu,Hanlin Chen,Juan Zhang,Tongfei Chen,Shuo Yang,Shuwei Shao,Wenhao Dong,Baochang Zhang*

Main category: cs.CV

TL;DR: HRGS是一种内存高效的层次化高斯泼溅框架，通过分块优化和重要性驱动的高斯修剪，解决了3DGS在高分辨率场景中的内存扩展问题。


<details>
  <summary>Details</summary>
Motivation: 3D高斯泼溅（3DGS）在实时3D场景重建中取得进展，但在高分辨率场景下面临内存扩展问题。

Method: 提出HRGS框架，包括分块优化（全局粗高斯表示生成、场景分块细化）和重要性驱动的高斯修剪（IDGP）。

Result: 在三个基准测试中，HRGS在高分辨率新视角合成（NVS）和表面重建任务中达到最先进性能。

Conclusion: HRGS在内存限制下实现了高质量的高分辨率3D场景重建。

Abstract: 3D Gaussian Splatting (3DGS) has made significant strides in real-time 3D scene reconstruction, but faces memory scalability issues in high-resolution scenarios. To address this, we propose Hierarchical Gaussian Splatting (HRGS), a memory-efficient framework with hierarchical block-level optimization. First, we generate a global, coarse Gaussian representation from low-resolution data. Then, we partition the scene into multiple blocks, refining each block with high-resolution data. The partitioning involves two steps: Gaussian partitioning, where irregular scenes are normalized into a bounded cubic space with a uniform grid for task distribution, and training data partitioning, where only relevant observations are retained for each block. By guiding block refinement with the coarse Gaussian prior, we ensure seamless Gaussian fusion across adjacent blocks. To reduce computational demands, we introduce Importance-Driven Gaussian Pruning (IDGP), which computes importance scores for each Gaussian and removes those with minimal contribution, speeding up convergence and reducing memory usage. Additionally, we incorporate normal priors from a pretrained model to enhance surface reconstruction quality. Our method enables high-quality, high-resolution 3D scene reconstruction even under memory constraints. Extensive experiments on three benchmarks show that HRGS achieves state-of-the-art performance in high-resolution novel view synthesis (NVS) and surface reconstruction tasks.

</details>


### [131] [Unified Representation Space for 3D Visual Grounding](https://arxiv.org/abs/2506.14238)
*Yinuo Zheng,Lipeng Gu,Honghua Chen,Liangliang Nan,Mingqiang Wei*

Main category: cs.CV

TL;DR: UniSpace-3D提出了一种统一表示空间的方法，用于3D视觉定位任务，通过结合CLIP预训练模型和多模态对比学习，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖分别预训练的视觉和文本编码器，导致模态间存在空间几何和语义类别上的显著差异，影响定位和分类准确性。

Method: 1) 统一表示编码器；2) 多模态对比学习模块；3) 语言引导的查询选择模块。

Result: 在ScanRefer和Nr3D/Sr3D数据集上，性能提升至少2.24%。

Conclusion: UniSpace-3D通过统一表示空间和多模态学习，有效缩小了视觉与文本特征的差距，提升了3D视觉定位的准确性。

Abstract: 3D visual grounding (3DVG) is a critical task in scene understanding that aims to identify objects in 3D scenes based on text descriptions. However, existing methods rely on separately pre-trained vision and text encoders, resulting in a significant gap between the two modalities in terms of spatial geometry and semantic categories. This discrepancy often causes errors in object positioning and classification. The paper proposes UniSpace-3D, which innovatively introduces a unified representation space for 3DVG, effectively bridging the gap between visual and textual features. Specifically, UniSpace-3D incorporates three innovative designs: i) a unified representation encoder that leverages the pre-trained CLIP model to map visual and textual features into a unified representation space, effectively bridging the gap between the two modalities; ii) a multi-modal contrastive learning module that further reduces the modality gap; iii) a language-guided query selection module that utilizes the positional and semantic information to identify object candidate points aligned with textual descriptions. Extensive experiments demonstrate that UniSpace-3D outperforms baseline models by at least 2.24% on the ScanRefer and Nr3D/Sr3D datasets. The code will be made available upon acceptance of the paper.

</details>


### [132] [Cross-Modal Geometric Hierarchy Fusion: An Implicit-Submap Driven Framework for Resilient 3D Place Recognition](https://arxiv.org/abs/2506.14243)
*Xiaohui Jiang,Haijiang Zhu,Chadei Li,Fulin Tang,Ning An*

Main category: cs.CV

TL;DR: 提出了一种基于弹性点的密度无关几何推理框架，解决了LiDAR地点识别中因点云密度不一致和单层几何抽象导致的描述符不稳定问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖手工特征提取，无法应对点云密度变化和复杂场景的几何多样性。

Method: 引入弹性点的隐式3D表示，生成均匀分布的点云，并从中提取占用网格和法向量信息，融合鸟瞰图和3D片段几何信息生成描述符。

Result: 在多个数据集上验证了方法的优越性，实现了高精度、低运行时和内存优化的平衡。

Conclusion: 该方法在LiDAR地点识别中表现出色，具有强鲁棒性和可扩展性，未来将开源代码。

Abstract: LiDAR-based place recognition serves as a crucial enabler for long-term autonomy in robotics and autonomous driving systems. Yet, prevailing methodologies relying on handcrafted feature extraction face dual challenges: (1) Inconsistent point cloud density, induced by ego-motion dynamics and environmental disturbances during repeated traversals, leads to descriptor instability, and (2) Representation fragility stems from reliance on single-level geometric abstractions that lack discriminative power in structurally complex scenarios. To address these limitations, we propose a novel framework that redefines 3D place recognition through density-agnostic geometric reasoning. Specifically, we introduce an implicit 3D representation based on elastic points, which is immune to the interference of original scene point cloud density and achieves the characteristic of uniform distribution. Subsequently, we derive the occupancy grid and normal vector information of the scene from this implicit representation. Finally, with the aid of these two types of information, we obtain descriptors that fuse geometric information from both bird's-eye view (capturing macro-level spatial layouts) and 3D segment (encoding micro-scale surface geometries) perspectives. We conducted extensive experiments on numerous datasets (KITTI, KITTI-360, MulRan, NCLT) across diverse environments. The experimental results demonstrate that our method achieves state-of-the-art performance. Moreover, our approach strikes an optimal balance between accuracy, runtime, and memory optimization for historical maps, showcasing excellent Resilient and scalability. Our code will be open-sourced in the future.

</details>


### [133] [synth-dacl: Does Synthetic Defect Data Enhance Segmentation Accuracy and Robustness for Real-World Bridge Inspections?](https://arxiv.org/abs/2506.14255)
*Johannes Flotzinger,Fabian Deuser,Achref Jaziri,Heiko Neumann,Norbert Oswald,Visvanathan Ramesh,Thomas Braml*

Main category: cs.CV

TL;DR: 论文提出了一种通过合成数据集扩展（synth-dacl）来改善桥梁缺陷检测模型性能的方法，解决了现有数据集dacl10k的类别不平衡问题，显著提升了模型在裂缝和空洞分割任务中的表现。


<details>
  <summary>Details</summary>
Motivation: 桥梁检查面临资源不足和效率低下的问题，自动化视觉检查可提升效率和安全性，但现有数据集dacl10k存在类别不平衡问题，影响模型性能。

Method: 通过合成混凝土纹理扩展数据集（synth-dacl），平衡类别分布，并测试其在15种扰动测试集上的表现。

Result: 使用合成扩展后，模型在扰动测试集上的平均IoU、F1分数、召回率和精确度均提升了2%。

Conclusion: 合成数据集扩展有效改善了桥梁缺陷检测模型的鲁棒性和性能，尤其在裂缝和空洞分割任务中表现显著。

Abstract: Adequate bridge inspection is increasingly challenging in many countries due to growing ailing stocks, compounded with a lack of staff and financial resources. Automating the key task of visual bridge inspection, classification of defects and building components on pixel level, improves efficiency, increases accuracy and enhances safety in the inspection process and resulting building assessment. Models overtaking this task must cope with an assortment of real-world conditions. They must be robust to variations in image quality, as well as background texture, as defects often appear on surfaces of diverse texture and degree of weathering. dacl10k is the largest and most diverse dataset for real-world concrete bridge inspections. However, the dataset exhibits class imbalance, which leads to notably poor model performance particularly when segmenting fine-grained classes such as cracks and cavities. This work introduces "synth-dacl", a compilation of three novel dataset extensions based on synthetic concrete textures. These extensions are designed to balance class distribution in dacl10k and enhance model performance, especially for crack and cavity segmentation. When incorporating the synth-dacl extensions, we observe substantial improvements in model robustness across 15 perturbed test sets. Notably, on the perturbed test set, a model trained on dacl10k combined with all synthetic extensions achieves a 2% increase in mean IoU, F1 score, Recall, and Precision compared to the same model trained solely on dacl10k.

</details>


### [134] [Comparison of Two Methods for Stationary Incident Detection Based on Background Image](https://arxiv.org/abs/2506.14256)
*Deepak Ghimire,Joonwhoan Lee*

Main category: cs.CV

TL;DR: 提出两种基于背景减除的静止物体检测方法，比较其性能与计算复杂度，并通过NCC图像比较实现实时跟踪。


<details>
  <summary>Details</summary>
Motivation: 传统背景减除方法主要用于检测运动物体，本文旨在检测临时静止物体。

Method: 使用单背景和双背景两种方案，双背景采用不同学习率生成，结合NCC图像比较跟踪静止物体。

Result: 方法对部分遮挡、短时完全遮挡和光照变化具有鲁棒性，可实时运行。

Conclusion: 提出的双背景方案在检测性能和计算复杂度上表现更优，适用于实际应用。

Abstract: In general, background subtraction-based methods are used to detect moving objects in visual tracking applications. In this paper, we employed a background subtraction-based scheme to detect the temporarily stationary objects. We proposed two schemes for stationary object detection, and we compare those in terms of detection performance and computational complexity. In the first approach, we used a single background, and in the second approach, we used dual backgrounds, generated with different learning rates, in order to detect temporarily stopped objects. Finally, we used normalized cross correlation (NCC) based image comparison to monitor and track the detected stationary object in a video scene. The proposed method is robust with partial occlusion, short-time fully occlusion, and illumination changes, and it can operate in real time.

</details>


### [135] [Exploring Non-contrastive Self-supervised Representation Learning for Image-based Profiling](https://arxiv.org/abs/2506.14265)
*Siran Dai,Qianqian Xu,Peisong Wen,Yang Liu,Qingming Huang*

Main category: cs.CV

TL;DR: 本文提出了一种名为SSLProfiler的非对比自监督学习框架，专门用于细胞图像分析，解决了现有方法在细胞图像分布和多图像输入方面的挑战，并在CVPR 2025竞赛中获胜。


<details>
  <summary>Details</summary>
Motivation: 细胞图像分析在药物发现中至关重要，但现有自监督学习方法在细胞图像分布和多图像输入方面存在不足，需要针对性改进。

Method: 提出SSLProfiler框架，引入针对细胞图像的专用数据增强和表示后处理方法，以解决分布差异和多图像输入问题。

Result: SSLProfiler在CVPR 2025的Cell Line Transferability挑战中获胜，证明了其有效性。

Conclusion: SSLProfiler为细胞图像分析提供了一个通用的特征提取器，解决了现有方法的局限性。

Abstract: Image-based cell profiling aims to create informative representations of cell images. This technique is critical in drug discovery and has greatly advanced with recent improvements in computer vision. Inspired by recent developments in non-contrastive Self-Supervised Learning (SSL), this paper provides an initial exploration into training a generalizable feature extractor for cell images using such methods. However, there are two major challenges: 1) There is a large difference between the distributions of cell images and natural images, causing the view-generation process in existing SSL methods to fail; and 2) Unlike typical scenarios where each representation is based on a single image, cell profiling often involves multiple input images, making it difficult to effectively combine all available information. To overcome these challenges, we propose SSLProfiler, a non-contrastive SSL framework specifically designed for cell profiling. We introduce specialized data augmentation and representation post-processing methods tailored to cell images, which effectively address the issues mentioned above and result in a robust feature extractor. With these improvements, SSLProfiler won the Cell Line Transferability challenge at CVPR 2025.

</details>


### [136] [Leader360V: The Large-scale, Real-world 360 Video Dataset for Multi-task Learning in Diverse Environment](https://arxiv.org/abs/2506.14271)
*Weiming Zhang,Dingwen Xiao,Aobotao Dai,Yexin Liu,Tianbo Pan,Shiqi Wen,Lei Chen,Lin Wang*

Main category: cs.CV

TL;DR: 论文介绍了Leader360V，首个大规模标注的真实世界360视频数据集，用于实例分割和跟踪，并提出了一种自动标注流程。


<details>
  <summary>Details</summary>
Motivation: 360视频的球形特性（如极区严重失真和内容不连续）导致标注成本高且复杂，缺乏大规模标注数据集阻碍了基础模型的发展。

Method: 设计了一个三阶段的自动标注流程：初始标注阶段（结合2D分割器和LLM生成失真感知掩码）、自动细化阶段（修正缺失或不完整区域）和手动修订阶段（结合LLM和人工进一步验证）。

Result: 用户研究和实验证明标注流程的有效性，Leader360V显著提升了360视频分割和跟踪的模型性能。

Conclusion: Leader360V为更可扩展的360场景理解铺平了道路。

Abstract: 360 video captures the complete surrounding scenes with the ultra-large field of view of 360X180. This makes 360 scene understanding tasks, eg, segmentation and tracking, crucial for appications, such as autonomous driving, robotics. With the recent emergence of foundation models, the community is, however, impeded by the lack of large-scale, labelled real-world datasets. This is caused by the inherent spherical properties, eg, severe distortion in polar regions, and content discontinuities, rendering the annotation costly yet complex. This paper introduces Leader360V, the first large-scale, labeled real-world 360 video datasets for instance segmentation and tracking. Our datasets enjoy high scene diversity, ranging from indoor and urban settings to natural and dynamic outdoor scenes. To automate annotation, we design an automatic labeling pipeline, which subtly coordinates pre-trained 2D segmentors and large language models to facilitate the labeling. The pipeline operates in three novel stages. Specifically, in the Initial Annotation Phase, we introduce a Semantic- and Distortion-aware Refinement module, which combines object mask proposals from multiple 2D segmentors with LLM-verified semantic labels. These are then converted into mask prompts to guide SAM2 in generating distortion-aware masks for subsequent frames. In the Auto-Refine Annotation Phase, missing or incomplete regions are corrected either by applying the SDR again or resolving the discontinuities near the horizontal borders. The Manual Revision Phase finally incorporates LLMs and human annotators to further refine and validate the annotations. Extensive user studies and evaluations demonstrate the effectiveness of our labeling pipeline. Meanwhile, experiments confirm that Leader360V significantly enhances model performance for 360 video segmentation and tracking, paving the way for more scalable 360 scene understanding.

</details>


### [137] [FRIDU: Functional Map Refinement with Guided Image Diffusion](https://arxiv.org/abs/2506.14322)
*Avigail Cohen Rimon,Mirela Ben-Chen,Or Litany*

Main category: cs.CV

TL;DR: 提出了一种基于图像扩散模型的功能映射优化方法，通过训练模型直接处理功能映射，生成更准确的对应关系。


<details>
  <summary>Details</summary>
Motivation: 现有功能映射优化方法可能效率不高或效果有限，需要一种更高效且灵活的方法。

Method: 将功能映射视为2D图像，训练图像扩散模型直接优化功能映射，并利用点映射作为扩散过程的引导。

Result: 该方法在功能映射优化上具有竞争力，且引导扩散模型为功能映射处理提供了新途径。

Conclusion: 引导扩散模型在功能映射优化中表现出潜力，为相关领域提供了新的研究方向。

Abstract: We propose a novel approach for refining a given correspondence map between two shapes. A correspondence map represented as a functional map, namely a change of basis matrix, can be additionally treated as a 2D image. With this perspective, we train an image diffusion model directly in the space of functional maps, enabling it to generate accurate maps conditioned on an inaccurate initial map. The training is done purely in the functional space, and thus is highly efficient. At inference time, we use the pointwise map corresponding to the current functional map as guidance during the diffusion process. The guidance can additionally encourage different functional map objectives, such as orthogonality and commutativity with the Laplace-Beltrami operator. We show that our approach is competitive with state-of-the-art methods of map refinement and that guided diffusion models provide a promising pathway to functional map processing.

</details>


### [138] [FGA-NN: Film Grain Analysis Neural Network](https://arxiv.org/abs/2506.14350)
*Zoubida Ameur,Frédéric Lefebvre,Philippe De Lagrange,Miloš Radosavljević*

Main category: cs.CV

TL;DR: FGA-NN是一种基于学习的方法，用于分析电影颗粒参数，以在压缩后保持艺术效果。


<details>
  <summary>Details</summary>
Motivation: 电影颗粒在低比特率压缩时会丢失，影响艺术效果，因此需要一种方法来分析和恢复。

Method: 提出FGA-NN，一种基于学习的分析方法，用于估计与传统合成兼容的颗粒参数。

Result: FGA-NN在分析精度和合成复杂性之间取得了优越的平衡，并表现出鲁棒性和适用性。

Conclusion: FGA-NN为电影颗粒的分析和恢复提供了一种高效且兼容的解决方案。

Abstract: Film grain, once a by-product of analog film, is now present in most cinematographic content for aesthetic reasons. However, when such content is compressed at medium to low bitrates, film grain is lost due to its random nature. To preserve artistic intent while compressing efficiently, film grain is analyzed and modeled before encoding and synthesized after decoding. This paper introduces FGA-NN, the first learning-based film grain analysis method to estimate conventional film grain parameters compatible with conventional synthesis. Quantitative and qualitative results demonstrate FGA-NN's superior balance between analysis accuracy and synthesis complexity, along with its robustness and applicability.

</details>


### [139] [EVA02-AT: Egocentric Video-Language Understanding with Spatial-Temporal Rotary Positional Embeddings and Symmetric Optimization](https://arxiv.org/abs/2506.14356)
*Xiaoqi Wang,Yi Wang,Lap-Pui Chau*

Main category: cs.CV

TL;DR: EVA02-AT是一套基于EVA02的视频-语言基础模型，针对第一人称视频理解任务设计，通过单阶段预训练、空间-时间旋转位置嵌入和对称多相似性损失（SMS）解决了现有方法的三个关键挑战。


<details>
  <summary>Details</summary>
Motivation: 解决现有方法在多阶段预训练成本高、空间-时间编码效果差以及软标签多实例检索学习目标不精确的问题。

Method: 1. 单阶段预训练将图像CLIP模型高效转换为视频编码器；2. 引入空间-时间旋转位置嵌入和联合注意力；3. 提出SMS损失和改进的训练框架。

Result: 在Ego4D、EPIC-Kitchens-100和Charades-Ego等数据集上，EVA02-AT在零样本和微调设置下均达到最先进性能，且参数量更少。

Conclusion: EVA02-AT通过高效预训练和精确的空间-时间建模，显著提升了第一人称视频-语言任务的性能。

Abstract: Egocentric video-language understanding demands both high efficiency and accurate spatial-temporal modeling. Existing approaches face three key challenges: 1) Excessive pre-training cost arising from multi-stage pre-training pipelines, 2) Ineffective spatial-temporal encoding due to manually split 3D rotary positional embeddings that hinder feature interactions, and 3) Imprecise learning objectives in soft-label multi-instance retrieval, which neglect negative pair correlations. In this paper, we introduce EVA02-AT, a suite of EVA02-based video-language foundation models tailored to egocentric video understanding tasks. EVA02-AT first efficiently transfers an image-based CLIP model into a unified video encoder via a single-stage pretraining. Second, instead of applying rotary positional embeddings to isolated dimensions, we introduce spatial-temporal rotary positional embeddings along with joint attention, which can effectively encode both spatial and temporal information on the entire hidden dimension. This joint encoding of spatial-temporal features enables the model to learn cross-axis relationships, which are crucial for accurately modeling motion and interaction in videos. Third, focusing on multi-instance video-language retrieval tasks, we introduce the Symmetric Multi-Similarity (SMS) loss and a novel training framework that advances all soft labels for both positive and negative pairs, providing a more precise learning objective. Extensive experiments on Ego4D, EPIC-Kitchens-100, and Charades-Ego under zero-shot and fine-tuning settings demonstrate that EVA02-AT achieves state-of-the-art performance across diverse egocentric video-language tasks with fewer parameters. Models with our SMS loss also show significant performance gains on multi-instance retrieval benchmarks. Our code and models are publicly available at https://github.com/xqwang14/EVA02-AT .

</details>


### [140] [HydroChronos: Forecasting Decades of Surface Water Change](https://arxiv.org/abs/2506.14362)
*Daniele Rege Cambrin,Eleonora Poeta,Eliana Pastor,Isaac Corley,Tania Cerquitelli,Elena Baralis,Paolo Garza*

Main category: cs.CV

TL;DR: 论文介绍了HydroChronos数据集和AquaClimaTempo UNet模型，用于地表水动态预测，显著优于基线模型，并通过可解释AI分析关键气候变量。


<details>
  <summary>Details</summary>
Motivation: 地表水动态预测对水资源管理和气候变化适应至关重要，但缺乏全面数据集和标准化基准。

Method: 提出HydroChronos数据集和AquaClimaTempo UNet模型，结合多模态时空数据。

Result: 模型在F1和MAE指标上显著优于基线，并通过可解释AI分析关键变量。

Conclusion: HydroChronos和AquaClimaTempo UNet填补了领域空白，为未来建模提供指导。

Abstract: Forecasting surface water dynamics is crucial for water resource management and climate change adaptation. However, the field lacks comprehensive datasets and standardized benchmarks. In this paper, we introduce HydroChronos, a large-scale, multi-modal spatiotemporal dataset for surface water dynamics forecasting designed to address this gap. We couple the dataset with three forecasting tasks. The dataset includes over three decades of aligned Landsat 5 and Sentinel-2 imagery, climate data, and Digital Elevation Models for diverse lakes and rivers across Europe, North America, and South America. We also propose AquaClimaTempo UNet, a novel spatiotemporal architecture with a dedicated climate data branch, as a strong benchmark baseline. Our model significantly outperforms a Persistence baseline for forecasting future water dynamics by +14% and +11% F1 across change detection and direction of change classification tasks, and by +0.1 MAE on the magnitude of change regression. Finally, we conduct an Explainable AI analysis to identify the key climate variables and input channels that influence surface water change, providing insights to inform and guide future modeling efforts.

</details>


### [141] [DGG-XNet: A Hybrid Deep Learning Framework for Multi-Class Brain Disease Classification with Explainable AI](https://arxiv.org/abs/2506.14367)
*Sumshun Nahar Eity,Mahin Montasir Afif,Tanisha Fairooz,Md. Mortuza Ahmmed,Md Saef Ullah Miah*

Main category: cs.CV

TL;DR: DGG-XNet是一种结合VGG16和DenseNet121的深度学习模型，用于提升脑部疾病的诊断准确性和效率。


<details>
  <summary>Details</summary>
Motivation: 传统MRI分析方法效率低且易出错，需要更准确和高效的诊断工具。

Method: 提出DGG-XNet，融合VGG16和DenseNet121，利用Grad-CAM增强模型可解释性。

Result: 在BraTS 2021和Kaggle数据集上测试，准确率达91.33%，各项指标均超过91%。

Conclusion: DGG-XNet是一种高效且可解释的计算机辅助诊断工具，适用于神经退行性和肿瘤性脑部疾病。

Abstract: Accurate diagnosis of brain disorders such as Alzheimer's disease and brain tumors remains a critical challenge in medical imaging. Conventional methods based on manual MRI analysis are often inefficient and error-prone. To address this, we propose DGG-XNet, a hybrid deep learning model integrating VGG16 and DenseNet121 to enhance feature extraction and classification. DenseNet121 promotes feature reuse and efficient gradient flow through dense connectivity, while VGG16 contributes strong hierarchical spatial representations. Their fusion enables robust multiclass classification of neurological conditions. Grad-CAM is applied to visualize salient regions, enhancing model transparency. Trained on a combined dataset from BraTS 2021 and Kaggle, DGG-XNet achieved a test accuracy of 91.33\%, with precision, recall, and F1-score all exceeding 91\%. These results highlight DGG-XNet's potential as an effective and interpretable tool for computer-aided diagnosis (CAD) of neurodegenerative and oncological brain disorders.

</details>


### [142] [Discrete JEPA: Learning Discrete Token Representations without Reconstruction](https://arxiv.org/abs/2506.14373)
*Junyeob Baek,Hosung Lee,Christopher Hoang,Mengye Ren,Sungjin Ahn*

Main category: cs.CV

TL;DR: Discrete-JEPA通过语义标记化和新目标扩展预测编码框架，显著提升符号推理任务的性能。


<details>
  <summary>Details</summary>
Motivation: 当前图像标记化方法在符号抽象和逻辑推理任务中存在局限，无法满足系统性推断需求。

Method: 提出Discrete-JEPA，结合语义标记化和互补目标，增强符号推理任务的标记化能力。

Result: 在视觉符号预测任务中显著优于基线，学习到的语义标记空间自发形成系统性模式。

Conclusion: 该方法为人工智能系统的符号世界建模和规划能力提供了重要进展。

Abstract: The cornerstone of cognitive intelligence lies in extracting hidden patterns from observations and leveraging these principles to systematically predict future outcomes. However, current image tokenization methods demonstrate significant limitations in tasks requiring symbolic abstraction and logical reasoning capabilities essential for systematic inference. To address this challenge, we propose Discrete-JEPA, extending the latent predictive coding framework with semantic tokenization and novel complementary objectives to create robust tokenization for symbolic reasoning tasks. Discrete-JEPA dramatically outperforms baselines on visual symbolic prediction tasks, while striking visual evidence reveals the spontaneous emergence of deliberate systematic patterns within the learned semantic token space. Though an initial model, our approach promises a significant impact for advancing Symbolic world modeling and planning capabilities in artificial intelligence systems.

</details>


### [143] [DepthSeg: Depth prompting in remote sensing semantic segmentation](https://arxiv.org/abs/2506.14382)
*Ning Zhou,Shanxiong Chen,Mingting Zhou,Haigang Sui,Lieyun Hu,Han Li,Li Hua,Qiming Zhou*

Main category: cs.CV

TL;DR: 论文提出了一种名为DepthSeg的深度提示二维遥感语义分割框架，通过建模深度/高度信息来解决光谱混淆和阴影遮挡问题。


<details>
  <summary>Details</summary>
Motivation: 现有语义分割方法主要关注光谱特征，忽略了目标的高度差异，导致复杂场景下的地物分类错误。

Method: DepthSeg框架包括轻量级适配器、深度提示器和语义分类解码器，分别用于特征提取、深度建模和分类预测。

Result: 在LiuZhou数据集上的实验验证了DepthSeg在地物分类任务中的优势，消融研究强调了深度提示的重要性。

Conclusion: DepthSeg通过整合深度信息，显著提升了遥感语义分割的准确性。

Abstract: Remote sensing semantic segmentation is crucial for extracting detailed land surface information, enabling applications such as environmental monitoring, land use planning, and resource assessment. In recent years, advancements in artificial intelligence have spurred the development of automatic remote sensing semantic segmentation methods. However, the existing semantic segmentation methods focus on distinguishing spectral characteristics of different objects while ignoring the differences in the elevation of the different targets. This results in land cover misclassification in complex scenarios involving shadow occlusion and spectral confusion. In this paper, we introduce a depth prompting two-dimensional (2D) remote sensing semantic segmentation framework (DepthSeg). It automatically models depth/height information from 2D remote sensing images and integrates it into the semantic segmentation framework to mitigate the effects of spectral confusion and shadow occlusion. During the feature extraction phase of DepthSeg, we introduce a lightweight adapter to enable cost-effective fine-tuning of the large-parameter vision transformer encoder pre-trained by natural images. In the depth prompting phase, we propose a depth prompter to model depth/height features explicitly. In the semantic prediction phase, we introduce a semantic classification decoder that couples the depth prompts with high-dimensional land-cover features, enabling accurate extraction of land-cover types. Experiments on the LiuZhou dataset validate the advantages of the DepthSeg framework in land cover mapping tasks. Detailed ablation studies further highlight the significance of the depth prompts in remote sensing semantic segmentation.

</details>


### [144] [GrFormer: A Novel Transformer on Grassmann Manifold for Infrared and Visible Image Fusion](https://arxiv.org/abs/2506.14384)
*Huan Kang,Hui Li,Xiao-Jun Wu,Tianyang Xu,Rui Wang,Chunyang Cheng,Josef Kittler*

Main category: cs.CV

TL;DR: 提出了一种基于Grassmann流形的新型注意力机制（GrFormer），用于红外和可见光图像融合，通过多尺度语义融合和跨模态策略提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有欧几里得方法无法捕捉非欧几里得空间中的拓扑结构，导致语义相似性不足和融合性能下降。

Method: 在Grassmann流形上构建低秩子空间映射，通过投影约束压缩注意力特征，实现高/低频特征的解耦，并开发了基于协方差掩码的跨模态融合策略（CMS）。

Result: 实验表明，GrFormer在多个基准测试中定性和定量上优于现有方法。

Conclusion: Grassmann流形和CMS策略有效提升了图像融合的语义和细节表现。

Abstract: In the field of image fusion, promising progress has been made by modeling data from different modalities as linear subspaces.
  However, in practice, the source images are often located in a non-Euclidean space, where the Euclidean methods usually cannot
  encapsulate the intrinsic topological structure. Typically, the inner product performed in the Euclidean space calculates the algebraic
  similarity rather than the semantic similarity, which results in undesired attention output and a decrease in fusion performance.
  While the balance of low-level details and high-level semantics should be considered in infrared and visible image fusion task. To
  address this issue, in this paper, we propose a novel attention mechanism based on Grassmann manifold for infrared and visible
  image fusion (GrFormer). Specifically, our method constructs a low-rank subspace mapping through projection constraints on the
  Grassmann manifold, compressing attention features into subspaces of varying rank levels. This forces the features to decouple into
  high-frequency details (local low-rank) and low-frequency semantics (global low-rank), thereby achieving multi-scale semantic
  fusion. Additionally, to effectively integrate the significant information, we develop a cross-modal fusion strategy (CMS) based on
  a covariance mask to maximise the complementary properties between different modalities and to suppress the features with high
  correlation, which are deemed redundant. The experimental results demonstrate that our network outperforms SOTA methods both
  qualitatively and quantitatively on multiple image fusion benchmarks. The codes are available at https://github.com/Shaoyun2023.

</details>


### [145] [Decoupled Classifier-Free Guidance for Counterfactual Diffusion Models](https://arxiv.org/abs/2506.14399)
*Tian Xia,Fabio De Sousa Ribeiro,Rajat R Rasal,Avinash Kori,Raghav Mehta,Ben Glocker*

Main category: cs.CV

TL;DR: 论文提出了一种解耦的无分类器引导（DCFG）方法，用于改进反事实图像生成中的属性控制和干预保真度。


<details>
  <summary>Details</summary>
Motivation: 标准无分类器引导（CFG）在反事实图像生成中可能导致身份丢失和虚假属性变化，需要更灵活的引导方法。

Method: 提出DCFG框架，通过属性分组嵌入策略实现选择性引导，基于因果图将属性分为干预组和不变组。

Result: 在CelebA-HQ、MIMIC-CXR和EMBED数据集上，DCFG提高了干预保真度，减少了意外变化，并增强了可逆性。

Conclusion: DCFG能够生成更忠实和可解释的反事实图像，优于现有方法。

Abstract: Counterfactual image generation aims to simulate realistic visual outcomes under specific causal interventions. Diffusion models have recently emerged as a powerful tool for this task, combining DDIM inversion with conditional generation via classifier-free guidance (CFG). However, standard CFG applies a single global weight across all conditioning variables, which can lead to poor identity preservation and spurious attribute changes - a phenomenon known as attribute amplification. To address this, we propose Decoupled Classifier-Free Guidance (DCFG), a flexible and model-agnostic framework that introduces group-wise conditioning control. DCFG builds on an attribute-split embedding strategy that disentangles semantic inputs, enabling selective guidance on user-defined attribute groups. For counterfactual generation, we partition attributes into intervened and invariant sets based on a causal graph and apply distinct guidance to each. Experiments on CelebA-HQ, MIMIC-CXR, and EMBED show that DCFG improves intervention fidelity, mitigates unintended changes, and enhances reversibility, enabling more faithful and interpretable counterfactual image generation.

</details>


### [146] [Causally Steered Diffusion for Automated Video Counterfactual Generation](https://arxiv.org/abs/2506.14404)
*Nikos Spyrou,Athanasios Vlontzos,Paraskevas Pegios,Thomas Melistas,Nefeli Gkouti,Yannis Panagakis,Giorgos Papanastasiou,Sotirios A. Tsaftaris*

Main category: cs.CV

TL;DR: 提出了一种基于因果关系的视频编辑框架，利用视觉语言模型（VLM）生成因果一致的反事实视频，无需修改底层编辑系统。


<details>
  <summary>Details</summary>
Motivation: 现有文本到图像（T2I）扩散模型在视频编辑中难以保持因果关系，可能导致不现实或误导性结果。

Method: 通过优化基于假设因果图的文本提示，引导生成因果一致的反事实视频，无需访问或微调底层系统。

Result: 实验表明，该方法能有效生成因果一致的反事实视频，并通过标准视频质量指标和因果特异性标准验证。

Conclusion: 该方法兼容任何黑盒视频编辑系统，在医疗和数字媒体等领域具有生成现实“假设”场景的潜力。

Abstract: Adapting text-to-image (T2I) latent diffusion models for video editing has shown strong visual fidelity and controllability, but challenges remain in maintaining causal relationships in video content. Edits affecting causally dependent attributes risk generating unrealistic or misleading outcomes if these relationships are ignored. In this work, we propose a causally faithful framework for counterfactual video generation, guided by a vision-language model (VLM). Our method is agnostic to the underlying video editing system and does not require access to its internal mechanisms or finetuning. Instead, we guide the generation by optimizing text prompts based on an assumed causal graph, addressing the challenge of latent space control in LDMs. We evaluate our approach using standard video quality metrics and counterfactual-specific criteria, such as causal effectiveness and minimality. Our results demonstrate that causally faithful video counterfactuals can be effectively generated within the learned distribution of LDMs through prompt-based causal steering. With its compatibility with any black-box video editing system, our method holds significant potential for generating realistic "what-if" video scenarios in diverse areas such as healthcare and digital media.

</details>


### [147] [Compositional Attribute Imbalance in Vision Datasets](https://arxiv.org/abs/2506.14418)
*Jiayi Chen,Yanbiao Ma,Andi Zhang,Weidong Tang,Wei Dai,Bowei Liu*

Main category: cs.CV

TL;DR: 论文提出了一种基于CLIP的框架，通过构建视觉属性字典解决图像分类中的属性不平衡问题，并结合数据增强技术提升模型对稀有属性的表征能力。


<details>
  <summary>Details</summary>
Motivation: 视觉属性不平衡是图像分类中常见但未被充分研究的问题，显著影响模型性能和泛化能力。

Method: 定义图像的一级和二级属性，引入CLIP框架构建视觉属性字典，分析单属性和组合属性不平衡，提出基于属性稀有度调整采样概率的策略，并结合多种数据增强技术。

Result: 在基准数据集上的实验表明，该方法有效缓解了属性不平衡，提升了深度神经网络的鲁棒性和公平性。

Conclusion: 研究强调了建模视觉属性分布的重要性，并为长尾图像分类任务提供了可扩展的解决方案。

Abstract: Visual attribute imbalance is a common yet underexplored issue in image classification, significantly impacting model performance and generalization. In this work, we first define the first-level and second-level attributes of images and then introduce a CLIP-based framework to construct a visual attribute dictionary, enabling automatic evaluation of image attributes. By systematically analyzing both single-attribute imbalance and compositional attribute imbalance, we reveal how the rarity of attributes affects model performance. To tackle these challenges, we propose adjusting the sampling probability of samples based on the rarity of their compositional attributes. This strategy is further integrated with various data augmentation techniques (such as CutMix, Fmix, and SaliencyMix) to enhance the model's ability to represent rare attributes. Extensive experiments on benchmark datasets demonstrate that our method effectively mitigates attribute imbalance, thereby improving the robustness and fairness of deep neural networks. Our research highlights the importance of modeling visual attribute distributions and provides a scalable solution for long-tail image classification tasks.

</details>


### [148] [Toward Rich Video Human-Motion2D Generation](https://arxiv.org/abs/2506.14428)
*Ruihao Xi,Xuekuan Wang,Yongcheng Li,Shuhua Li,Zichen Wang,Yiwei Wang,Feng Wei,Cairong Zhao*

Main category: cs.CV

TL;DR: 论文提出了一种基于扩散模型的RVHM2D方法，用于生成逼真且可控的多人交互动作，并引入了一个新的大规模2D视频数据集Motion2D-Video-150K。


<details>
  <summary>Details</summary>
Motivation: 由于数据稀缺和人际动态建模的复杂性，生成逼真且可控的多人交互动作仍具挑战性。

Method: 提出了RVHM2D模型，采用双文本编码器（CLIP-L/B或T5-XXL）增强文本条件机制，并通过两阶段训练（扩散目标训练和基于FID奖励的强化学习微调）优化模型。

Result: RVHM2D在Motion2D-Video-150K数据集上表现出色，能够生成高质量的单人和双人交互动作。

Conclusion: RVHM2D在生成逼真且文本对齐的2D动作方面取得了领先性能，为多人交互动作生成提供了有效解决方案。

Abstract: Generating realistic and controllable human motions, particularly those involving rich multi-character interactions, remains a significant challenge due to data scarcity and the complexities of modeling inter-personal dynamics. To address these limitations, we first introduce a new large-scale rich video human motion 2D dataset (Motion2D-Video-150K) comprising 150,000 video sequences. Motion2D-Video-150K features a balanced distribution of diverse single-character and, crucially, double-character interactive actions, each paired with detailed textual descriptions. Building upon this dataset, we propose a novel diffusion-based rich video human motion2D generation (RVHM2D) model. RVHM2D incorporates an enhanced textual conditioning mechanism utilizing either dual text encoders (CLIP-L/B) or T5-XXL with both global and local features. We devise a two-stage training strategy: the model is first trained with a standard diffusion objective, and then fine-tuned using reinforcement learning with an FID-based reward to further enhance motion realism and text alignment. Extensive experiments demonstrate that RVHM2D achieves leading performance on the Motion2D-Video-150K benchmark in generating both single and interactive double-character scenarios.

</details>


### [149] [MoTE: Mixture of Ternary Experts for Memory-efficient Large Multimodal Models](https://arxiv.org/abs/2506.14435)
*Hongyu Wang,Jiayu Xu,Ruiping Wang,Yan Feng,Yitao Zhai,Peng Pei,Xunliang Cai,Xilin Chen*

Main category: cs.CV

TL;DR: 论文提出了一种名为MoTE的内存高效方法，通过训练更多低精度专家来替代高精度专家，从而在保持性能的同时降低内存占用。


<details>
  <summary>Details</summary>
Motivation: 解决多模态混合专家模型（MoEs）在边缘设备上部署时因高内存占用带来的挑战。

Method: 使用预训练的FFN作为共享专家，并训练三元路由专家（参数为{-1, 0, 1}），结合后训练量化方法。

Result: MoTE在相同内存占用下性能接近全精度基线MoE-LLaVA，且在内存受限时表现更优，平均准确率提升4.3%。

Conclusion: MoTE是一种高效且适用于内存受限设备的方法，具有潜在的应用价值。

Abstract: Large multimodal Mixture-of-Experts (MoEs) effectively scale the model size to boost performance while maintaining fixed active parameters. However, previous works primarily utilized full-precision experts during sparse up-cycling. Despite they show superior performance on end tasks, the large amount of experts introduces higher memory footprint, which poses significant challenges for the deployment on edge devices. In this work, we propose MoTE, a scalable and memory-efficient approach to train Mixture-of-Ternary-Experts models from dense checkpoint. Instead of training fewer high-precision experts, we propose to train more low-precision experts during up-cycling. Specifically, we use the pre-trained FFN as a shared expert and train ternary routed experts with parameters in {-1, 0, 1}. Extensive experiments show that our approach has promising scaling trend along model size. MoTE achieves comparable performance to full-precision baseline MoE-LLaVA while offering lower memory footprint. Furthermore, our approach is compatible with post-training quantization methods and the advantage further amplifies when memory-constraint goes lower. Given the same amount of expert memory footprint of 3.4GB and combined with post-training quantization, MoTE outperforms MoE-LLaVA by a gain of 4.3% average accuracy on end tasks, demonstrating its effectiveness and potential for memory-constrained devices.

</details>


### [150] [Model compression using knowledge distillation with integrated gradients](https://arxiv.org/abs/2506.14440)
*David E. Hernandez,Jose Chang,Torbjörn E. M. Nordling*

Main category: cs.CV

TL;DR: 提出了一种基于集成梯度（IG）增强的知识蒸馏方法，显著提升了模型压缩效果和推理速度。


<details>
  <summary>Details</summary>
Motivation: 为了解决在资源受限设备上部署深度学习模型的问题，提出了一种更高效的知识蒸馏方法。

Method: 通过将IG图叠加到输入图像上，增强学生模型对教师模型决策过程的理解。

Result: 在CIFAR-10上达到92.6%的测试准确率，压缩比为4.1倍，推理时间从140ms降至13ms。

Conclusion: 该方法在多种架构和压缩比下均优于传统方法，适用于边缘设备的实际部署。

Abstract: Model compression is critical for deploying deep learning models on resource-constrained devices. We introduce a novel method enhancing knowledge distillation with integrated gradients (IG) as a data augmentation strategy. Our approach overlays IG maps onto input images during training, providing student models with deeper insights into teacher models' decision-making processes. Extensive evaluation on CIFAR-10 demonstrates that our IG-augmented knowledge distillation achieves 92.6% testing accuracy with a 4.1x compression factor-a significant 1.1 percentage point improvement ($p<0.001$) over non-distilled models (91.5%). This compression reduces inference time from 140 ms to 13 ms. Our method precomputes IG maps before training, transforming substantial runtime costs into a one-time preprocessing step. Our comprehensive experiments include: (1) comparisons with attention transfer, revealing complementary benefits when combined with our approach; (2) Monte Carlo simulations confirming statistical robustness; (3) systematic evaluation of compression factor versus accuracy trade-offs across a wide range (2.2x-1122x); and (4) validation on an ImageNet subset aligned with CIFAR-10 classes, demonstrating generalisability beyond the initial dataset. These extensive ablation studies confirm that IG-based knowledge distillation consistently outperforms conventional approaches across varied architectures and compression ratios. Our results establish this framework as a viable compression technique for real-world deployment on edge devices while maintaining competitive accuracy.

</details>


### [151] [Adapting Lightweight Vision Language Models for Radiological Visual Question Answering](https://arxiv.org/abs/2506.14451)
*Aditya Shourya,Michel Dumontier,Chang Sun*

Main category: cs.CV

TL;DR: 该研究通过微调轻量级视觉语言模型，解决了放射学视觉问答（VQA）中的数据稀缺、建模复杂和评估不足等问题，并展示了小模型在适当调优后的高性能表现。


<details>
  <summary>Details</summary>
Motivation: 放射学VQA面临数据标注有限、图像模式复杂和评估不足的挑战，需要一种高效且成本可控的解决方案。

Method: 采用轻量级3B参数视觉语言模型，通过合成问答对生成和多阶段微调，结合放射学专用数据集（如ROCO v2.0、MedPix v2.0）进行训练。

Result: 小模型在有限数据和参数规模下表现优异，接近或超越大型模型（如LLaVA-Med），并开发了基于显著性的诊断工具。

Conclusion: 研究表明，轻量级模型在放射学VQA中具有潜力，且通过显著性分析可有效识别模型失败模式。

Abstract: Recent advancements in vision-language systems have improved the accuracy of Radiological Visual Question Answering (VQA) Models. However, some challenges remain across each stage of model development: limited expert-labeled images hinders data procurement at scale; the intricate and nuanced patterns of radiological images make modeling inherently difficult; and the lack of evaluation evaluation efforts makes it difficult to identify cases where the model might be ill-conditioned. In this study, we fine-tune a lightweight 3B parameter vision-language model for Radiological VQA, demonstrating that small models, when appropriately tuned with curated data, can achieve robust performance across both open- and closed-ended questions. We propose a cost-effective training pipeline from synthetic question-answer pair generation to multi-stage fine-tuning on specialised radiological domain-targeted datasets (e.g., ROCO v2.0, MedPix v2.0). Our results show that despite operating at a fraction of the scale of state-of-the-art models such as LLaVA-Med, our model achieves promising performance given its small parameter size and the limited scale of training data. We introduce a lightweight saliency-based diagnostic tool that enables domain experts to inspect VQA model performance and identify ill-conditioned failure modes through saliency analysis.

</details>


### [152] [Dense360: Dense Understanding from Omnidirectional Panoramas](https://arxiv.org/abs/2506.14471)
*Yikang Zhou,Tao Zhang,Dizhe Zhang,Shunping Ji,Xiangtai Li,Lu Qi*

Main category: cs.CV

TL;DR: 该论文提出了一个用于多模态大语言模型（MLLMs）的全景数据集和基准测试，旨在解决全景图像理解中的空间连续性和信息密度问题。


<details>
  <summary>Details</summary>
Motivation: 现有MLLMs通过有限视场（FOV）输入实现世界理解，但全景图像能提供更完整、紧凑和连续的场景表示。

Method: 引入包含160K全景图像的数据集，并提出ERP-RoPE位置编码方案以解决全景图像中的挑战。

Result: 数据集包含5M密集实体级标注，1M唯一引用表达式和100K实体基础全景场景描述，并建立了Dense360-Bench基准测试。

Conclusion: 该研究为全景环境中的密集视觉语言理解提供了数据集和基准测试框架。

Abstract: Multimodal Large Language Models (MLLMs) require comprehensive visual inputs to achieve dense understanding of the physical world. While existing MLLMs demonstrate impressive world understanding capabilities through limited field-of-view (FOV) visual inputs (e.g., 70 degree), we take the first step toward dense understanding from omnidirectional panoramas. We first introduce an omnidirectional panoramas dataset featuring a comprehensive suite of reliability-scored annotations. Specifically, our dataset contains 160K panoramas with 5M dense entity-level captions, 1M unique referring expressions, and 100K entity-grounded panoramic scene descriptions. Compared to multi-view alternatives, panoramas can provide more complete, compact, and continuous scene representations through equirectangular projections (ERP). However, the use of ERP introduces two key challenges for MLLMs: i) spatial continuity along the circle of latitude, and ii) latitude-dependent variation in information density. We address these challenges through ERP-RoPE, a position encoding scheme specifically designed for panoramic ERP. In addition, we introduce Dense360-Bench, the first benchmark for evaluating MLLMs on omnidirectional captioning and grounding, establishing a comprehensive framework for advancing dense visual-language understanding in panoramic settings.

</details>


### [153] [Foundation Model Insights and a Multi-Model Approach for Superior Fine-Grained One-shot Subset Selection](https://arxiv.org/abs/2506.14473)
*Zhijing Wan,Zhixiang Wang,Zheng Wang,Xin Xu,Shin'ichi Satoh*

Main category: cs.CV

TL;DR: 论文研究了基于基础模型（FMs）的一次性子集选择方法，发现其在细粒度数据集上优于传统信息提取器（IEs），并提出RAM-APL方法以提升性能。


<details>
  <summary>Details</summary>
Motivation: 传统IEs依赖目标数据集预训练，限制了其通用性。FMs可能克服这一局限，但其在不同数据集上的表现尚不明确。

Method: 提出RAM-APL方法，利用多个FMs的互补优势，针对细粒度图像数据集优化子集选择。

Result: FMs在细粒度数据集上表现优于传统IEs，但在粗粒度数据集上优势减弱。RAM-APL在多个细粒度数据集上达到最优性能。

Conclusion: FMs在子集选择中具有潜力，尤其在细粒度数据集上。RAM-APL通过多FM融合进一步提升了性能。

Abstract: One-shot subset selection serves as an effective tool to reduce deep learning training costs by identifying an informative data subset based on the information extracted by an information extractor (IE). Traditional IEs, typically pre-trained on the target dataset, are inherently dataset-dependent. Foundation models (FMs) offer a promising alternative, potentially mitigating this limitation. This work investigates two key questions: (1) Can FM-based subset selection outperform traditional IE-based methods across diverse datasets? (2) Do all FMs perform equally well as IEs for subset selection? Extensive experiments uncovered surprising insights: FMs consistently outperform traditional IEs on fine-grained datasets, whereas their advantage diminishes on coarse-grained datasets with noisy labels. Motivated by these finding, we propose RAM-APL (RAnking Mean-Accuracy of Pseudo-class Labels), a method tailored for fine-grained image datasets. RAM-APL leverages multiple FMs to enhance subset selection by exploiting their complementary strengths. Our approach achieves state-of-the-art performance on fine-grained datasets, including Oxford-IIIT Pet, Food-101, and Caltech-UCSD Birds-200-2011.

</details>


### [154] [I Speak and You Find: Robust 3D Visual Grounding with Noisy and Ambiguous Speech Inputs](https://arxiv.org/abs/2506.14495)
*Yu Qi,Lipeng Gu,Honghua Chen,Liangliang Nan,Mingqiang Wei*

Main category: cs.CV

TL;DR: SpeechRefer是一个新的3D视觉定位框架，通过语音信号增强性能，减少对错误文本转录的依赖。


<details>
  <summary>Details</summary>
Motivation: 现有3D视觉定位方法依赖精确文本提示，而真实语音输入常因口音、背景噪音等问题导致转录错误，限制了方法的应用。

Method: SpeechRefer引入语音互补模块和对比互补模块，分别利用语音信号生成补充提案分数，并通过对比学习对齐错误文本与语音特征。

Result: 在SpeechRefer和SpeechNr3D数据集上的实验表明，SpeechRefer显著提升了现有3DVG方法的性能。

Conclusion: SpeechRefer能够弥合噪声语音输入与可靠3DVG之间的差距，推动更直观实用的多模态系统发展。

Abstract: Existing 3D visual grounding methods rely on precise text prompts to locate objects within 3D scenes. Speech, as a natural and intuitive modality, offers a promising alternative. Real-world speech inputs, however, often suffer from transcription errors due to accents, background noise, and varying speech rates, limiting the applicability of existing 3DVG methods. To address these challenges, we propose \textbf{SpeechRefer}, a novel 3DVG framework designed to enhance performance in the presence of noisy and ambiguous speech-to-text transcriptions. SpeechRefer integrates seamlessly with xisting 3DVG models and introduces two key innovations. First, the Speech Complementary Module captures acoustic similarities between phonetically related words and highlights subtle distinctions, generating complementary proposal scores from the speech signal. This reduces dependence on potentially erroneous transcriptions. Second, the Contrastive Complementary Module employs contrastive learning to align erroneous text features with corresponding speech features, ensuring robust performance even when transcription errors dominate. Extensive experiments on the SpeechRefer and peechNr3D datasets demonstrate that SpeechRefer improves the performance of existing 3DVG methods by a large margin, which highlights SpeechRefer's potential to bridge the gap between noisy speech inputs and reliable 3DVG, enabling more intuitive and practical multimodal systems.

</details>


### [155] [MOL: Joint Estimation of Micro-Expression, Optical Flow, and Landmark via Transformer-Graph-Style Convolution](https://arxiv.org/abs/2506.14511)
*Zhiwen Shao,Yifan Cheng,Feiran Li,Yong Zhou,Xuequan Lu,Yuan Xie,Lizhuang Ma*

Main category: cs.CV

TL;DR: 提出了一种结合Transformer、图卷积和普通卷积的端到端微表情识别框架，通过F5C块提取局部-全局特征，无需关键帧先验知识，并在多个任务上表现优异。


<details>
  <summary>Details</summary>
Motivation: 微表情识别因动作短暂且细微而具有挑战性，现有方法依赖手工特征或小规模数据集，限制了性能。

Method: 提出F5C块（全连接卷积和通道对应卷积），结合Transformer和图卷积，直接从原始帧序列提取特征，并联合训练微表情识别、光流估计和面部标志检测。

Result: 在CASME II、SAMM和SMIC基准测试中优于现有方法，同时在其他任务中表现良好，并能捕捉与微表情相关的局部肌肉动作。

Conclusion: 该框架通过多任务学习和局部-全局特征提取，有效解决了微表情识别中的挑战，并展示了广泛的应用潜力。

Abstract: Facial micro-expression recognition (MER) is a challenging problem, due to transient and subtle micro-expression (ME) actions. Most existing methods depend on hand-crafted features, key frames like onset, apex, and offset frames, or deep networks limited by small-scale and low-diversity datasets. In this paper, we propose an end-to-end micro-action-aware deep learning framework with advantages from transformer, graph convolution, and vanilla convolution. In particular, we propose a novel F5C block composed of fully-connected convolution and channel correspondence convolution to directly extract local-global features from a sequence of raw frames, without the prior knowledge of key frames. The transformer-style fully-connected convolution is proposed to extract local features while maintaining global receptive fields, and the graph-style channel correspondence convolution is introduced to model the correlations among feature patterns. Moreover, MER, optical flow estimation, and facial landmark detection are jointly trained by sharing the local-global features. The two latter tasks contribute to capturing facial subtle action information for MER, which can alleviate the impact of insufficient training data. Extensive experiments demonstrate that our framework (i) outperforms the state-of-the-art MER methods on CASME II, SAMM, and SMIC benchmarks, (ii) works well for optical flow estimation and facial landmark detection, and (iii) can capture facial subtle muscle actions in local regions associated with MEs. The code is available at https://github.com/CYF-cuber/MOL.

</details>


### [156] [SIRI-Bench: Challenging VLMs' Spatial Intelligence through Complex Reasoning Tasks](https://arxiv.org/abs/2506.14512)
*Zijian Song,Xiaoxin Lin,Qiuming Huang,Guangrun Wang,Liang Lin*

Main category: cs.CV

TL;DR: SIRI-Bench是一个评估视觉语言模型（VLMs）空间智能的基准测试，通过视频推理任务测试其空间理解与高级推理能力。


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型（LLMs）在复杂推理方面进展迅速，但视觉语言模型（VLMs）在空间上下文中的推理能力尚未得到系统评估。

Method: 开发了SIRI-Bench基准测试，包含近1K个视频-问题-答案三元组，并设计自动场景生成引擎（利用LLM代理）合成真实3D场景。

Result: 实验表明，当前最先进的VLMs在SIRI-Bench上表现不佳，凸显了空间推理的挑战。

Conclusion: 该研究旨在推动对空间推理的关注，并提升VLMs在视觉问题解决中的能力。

Abstract: Large Language Models (LLMs) are experiencing rapid advancements in complex reasoning, exhibiting remarkable generalization in mathematics and programming. In contrast, while spatial intelligence is fundamental for Vision-Language Models (VLMs) in real-world interaction, the systematic evaluation of their complex reasoning ability within spatial contexts remains underexplored. To bridge this gap, we introduce SIRI-Bench, a benchmark designed to evaluate VLMs' spatial intelligence through video-based reasoning tasks. SIRI-Bench comprises nearly 1K video-question-answer triplets, where each problem is embedded in a realistic 3D scene and captured by video. By carefully designing questions and corresponding 3D scenes, our benchmark ensures that solving the questions requires both spatial comprehension for extracting information and high-level reasoning for deriving solutions, making it a challenging benchmark for evaluating VLMs. To facilitate large-scale data synthesis, we develop an Automatic Scene Creation Engine. This engine, leveraging multiple specialized LLM agents, can generate realistic 3D scenes from abstract math problems, ensuring faithfulness to the original descriptions. Experimental results reveal that state-of-the-art VLMs struggle significantly on SIRI-Bench, underscoring the challenge of spatial reasoning. We hope that our study will bring researchers' attention to spatially grounded reasoning and advance VLMs in visual problem-solving.

</details>


### [157] [VisLanding: Monocular 3D Perception for UAV Safe Landing via Depth-Normal Synergy](https://arxiv.org/abs/2506.14525)
*Zhuoyue Tan,Boyong He,Yuxiang Ji,Liaoni Wu*

Main category: cs.CV

TL;DR: VisLanding是一个基于单目3D感知的无人机安全着陆框架，通过深度-法线协同预测和二元语义分割实现安全着陆区估计，具有优异的泛化性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 解决无人机在复杂未知环境中自主着陆的核心挑战，提升安全着陆区的识别精度和决策支持能力。

Method: 利用Metric3D V2模型的深度-法线协同预测能力，构建端到端安全着陆区估计框架，并通过二元语义分割任务实现。使用WildUAV数据集微调模型，并构建跨域评估数据集验证鲁棒性。

Result: VisLanding显著提高了安全着陆区识别的准确性，保留了Metric3D V2的零样本泛化优势，在跨域测试中表现优于其他方法。

Conclusion: VisLanding通过深度-法线联合优化机制，为无人机安全着陆提供了高效、泛化性强的解决方案，并支持实际应用中的决策。

Abstract: This paper presents VisLanding, a monocular 3D perception-based framework for safe UAV (Unmanned Aerial Vehicle) landing. Addressing the core challenge of autonomous UAV landing in complex and unknown environments, this study innovatively leverages the depth-normal synergy prediction capabilities of the Metric3D V2 model to construct an end-to-end safe landing zones (SLZ) estimation framework. By introducing a safe zone segmentation branch, we transform the landing zone estimation task into a binary semantic segmentation problem. The model is fine-tuned and annotated using the WildUAV dataset from a UAV perspective, while a cross-domain evaluation dataset is constructed to validate the model's robustness. Experimental results demonstrate that VisLanding significantly enhances the accuracy of safe zone identification through a depth-normal joint optimization mechanism, while retaining the zero-shot generalization advantages of Metric3D V2. The proposed method exhibits superior generalization and robustness in cross-domain testing compared to other approaches. Furthermore, it enables the estimation of landing zone area by integrating predicted depth and normal information, providing critical decision-making support for practical applications.

</details>


### [158] [Exploring Diffusion with Test-Time Training on Efficient Image Restoration](https://arxiv.org/abs/2506.14541)
*Rongchang Lu,Tianduo Luo,Yunzhi Zhang,Conghan Yue,Pei Yang,Guibao Liu,Changyang Gu*

Main category: cs.CV

TL;DR: DiffRWKVIR提出了一种结合测试时训练和高效扩散的新框架，解决了图像恢复中的特征融合、计算瓶颈和扩散效率问题。


<details>
  <summary>Details</summary>
Motivation: 图像恢复中存在特征融合不高效、计算瓶颈和扩散过程效率低的问题，需要一种更高效的解决方案。

Method: 提出了三个创新点：Omni-Scale 2D State Evolution实现全局上下文感知；Chunk-Optimized Flash Processing加速并行处理；Prior-Guided Efficient Diffusion提取紧凑图像先验表示。

Result: 在多个基准测试中，DiffRWKVIR在PSNR、SSIM、LPIPS和效率指标上优于SwinIR、HAT和MambaIR/v2。

Conclusion: DiffRWKVIR为高效、自适应的图像恢复提供了新范式，并优化了硬件利用率。

Abstract: Image restoration faces challenges including ineffective feature fusion, computational bottlenecks and inefficient diffusion processes. To address these, we propose DiffRWKVIR, a novel framework unifying Test-Time Training (TTT) with efficient diffusion. Our approach introduces three key innovations: (1) Omni-Scale 2D State Evolution extends RWKV's location-dependent parameterization to hierarchical multi-directional 2D scanning, enabling global contextual awareness with linear complexity O(L); (2) Chunk-Optimized Flash Processing accelerates intra-chunk parallelism by 3.2x via contiguous chunk processing (O(LCd) complexity), reducing sequential dependencies and computational overhead; (3) Prior-Guided Efficient Diffusion extracts a compact Image Prior Representation (IPR) in only 5-20 steps, proving 45% faster training/inference than DiffIR while solving computational inefficiency in denoising. Evaluated across super-resolution and inpainting benchmarks (Set5, Set14, BSD100, Urban100, Places365), DiffRWKVIR outperforms SwinIR, HAT, and MambaIR/v2 in PSNR, SSIM, LPIPS, and efficiency metrics. Our method establishes a new paradigm for adaptive, high-efficiency image restoration with optimized hardware utilization.

</details>


### [159] [DiFuse-Net: RGB and Dual-Pixel Depth Estimation using Window Bi-directional Parallax Attention and Cross-modal Transfer Learning](https://arxiv.org/abs/2506.14709)
*Kunal Swami,Debtanu Gupta,Amrit Kumar Muduli,Chirag Jaiswal,Pankaj Kumar Bajpai*

Main category: cs.CV

TL;DR: DiFuse-Net是一种新型模态解耦网络，用于基于RGB和双像素（DP）的深度估计，通过窗口双向视差注意力机制（WBiPAM）和跨模态迁移学习（CmTL）提升性能，并贡献了一个高质量RGB-DP-D数据集。


<details>
  <summary>Details</summary>
Motivation: 解决传统深度传感器在成本、功耗和鲁棒性上的限制，利用智能手机中普遍存在的双像素技术提供替代方案。

Method: 提出DiFuse-Net，结合WBiPAM捕捉DP视差线索，分离RGB编码器提取上下文信息，并通过CmTL利用大规模RGB-D数据集。

Result: 在DP和立体基线方法上表现出优越性，并贡献了新的高质量RGB-DP-D数据集DCDP。

Conclusion: DiFuse-Net为智能手机相机提供了一种高效、鲁棒的深度估计解决方案，并通过新数据集推动了相关研究。

Abstract: Depth estimation is crucial for intelligent systems, enabling applications from autonomous navigation to augmented reality. While traditional stereo and active depth sensors have limitations in cost, power, and robustness, dual-pixel (DP) technology, ubiquitous in modern cameras, offers a compelling alternative. This paper introduces DiFuse-Net, a novel modality decoupled network design for disentangled RGB and DP based depth estimation. DiFuse-Net features a window bi-directional parallax attention mechanism (WBiPAM) specifically designed to capture the subtle DP disparity cues unique to smartphone cameras with small aperture. A separate encoder extracts contextual information from the RGB image, and these features are fused to enhance depth prediction. We also propose a Cross-modal Transfer Learning (CmTL) mechanism to utilize large-scale RGB-D datasets in the literature to cope with the limitations of obtaining large-scale RGB-DP-D dataset. Our evaluation and comparison of the proposed method demonstrates its superiority over the DP and stereo-based baseline methods. Additionally, we contribute a new, high-quality, real-world RGB-DP-D training dataset, named Dual-Camera Dual-Pixel (DCDP) dataset, created using our novel symmetric stereo camera hardware setup, stereo calibration and rectification protocol, and AI stereo disparity estimation method.

</details>


### [160] [DreamLight: Towards Harmonious and Consistent Image Relighting](https://arxiv.org/abs/2506.14549)
*Yong Liu,Wenpeng Xiao,Qianqian Wang,Junlin Chen,Shiyin Wang,Yitong Wang,Xinglong Wu,Yansong Tang*

Main category: cs.CV

TL;DR: DreamLight是一个通用图像重照明模型，支持图像和文本输入，通过统一数据格式和预训练扩散模型生成自然效果。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要关注图像输入，文本输入研究较少，且依赖复杂管道或环境映射，数据成本高，难以实现真实的光照交互效果。

Method: 采用统一数据格式，利用预训练扩散模型的语义先验，提出PGLA模块和SFF后处理模块，优化光照和外观一致性。

Result: 实验和用户研究表明，DreamLight在重照明任务中表现出色。

Conclusion: DreamLight通过创新的模块设计，显著提升了图像重照明的自然性和一致性。

Abstract: We introduce a model named DreamLight for universal image relighting in this work, which can seamlessly composite subjects into a new background while maintaining aesthetic uniformity in terms of lighting and color tone. The background can be specified by natural images (image-based relighting) or generated from unlimited text prompts (text-based relighting). Existing studies primarily focus on image-based relighting, while with scant exploration into text-based scenarios. Some works employ intricate disentanglement pipeline designs relying on environment maps to provide relevant information, which grapples with the expensive data cost required for intrinsic decomposition and light source. Other methods take this task as an image translation problem and perform pixel-level transformation with autoencoder architecture. While these methods have achieved decent harmonization effects, they struggle to generate realistic and natural light interaction effects between the foreground and background. To alleviate these challenges, we reorganize the input data into a unified format and leverage the semantic prior provided by the pretrained diffusion model to facilitate the generation of natural results. Moreover, we propose a Position-Guided Light Adapter (PGLA) that condenses light information from different directions in the background into designed light query embeddings, and modulates the foreground with direction-biased masked attention. In addition, we present a post-processing module named Spectral Foreground Fixer (SFF) to adaptively reorganize different frequency components of subject and relighted background, which helps enhance the consistency of foreground appearance. Extensive comparisons and user study demonstrate that our DreamLight achieves remarkable relighting performance.

</details>


### [161] [Risk Estimation of Knee Osteoarthritis Progression via Predictive Multi-task Modelling from Efficient Diffusion Model using X-ray Images](https://arxiv.org/abs/2506.14560)
*David Butler,Adrian Hilton,Gustavo Carneiro*

Main category: cs.CV

TL;DR: 提出了一种可解释的机器学习方法，通过多任务预测模型估计膝关节骨关节炎（OA）进展风险，同时预测解剖标志点，并利用扩散模型生成高质量未来图像。


<details>
  <summary>Details</summary>
Motivation: 现有方法缺乏可解释性且复杂，无法定位解剖标志点，限制了临床应用。

Method: 采用多任务预测模型，结合扩散模型在类别条件潜在空间中生成未来图像，以预测OA严重程度和解剖标志点。

Result: 在Osteoarthritis Initiative数据集上，AUC提升2%至0.71，推理时间缩短约9%。

Conclusion: 该方法在提高预测性能的同时增强了可解释性，为临床决策提供了可视化支持。

Abstract: Medical imaging plays a crucial role in assessing knee osteoarthritis (OA) risk by enabling early detection and disease monitoring. Recent machine learning methods have improved risk estimation (i.e., predicting the likelihood of disease progression) and predictive modelling (i.e., the forecasting of future outcomes based on current data) using medical images, but clinical adoption remains limited due to their lack of interpretability. Existing approaches that generate future images for risk estimation are complex and impractical. Additionally, previous methods fail to localize anatomical knee landmarks, limiting interpretability. We address these gaps with a new interpretable machine learning method to estimate the risk of knee OA progression via multi-task predictive modelling that classifies future knee OA severity and predicts anatomical knee landmarks from efficiently generated high-quality future images. Such image generation is achieved by leveraging a diffusion model in a class-conditioned latent space to forecast disease progression, offering a visual representation of how particular health conditions may evolve. Applied to the Osteoarthritis Initiative dataset, our approach improves the state-of-the-art (SOTA) by 2\%, achieving an AUC of 0.71 in predicting knee OA progression while offering ~9% faster inference time.

</details>


### [162] [CDP: Towards Robust Autoregressive Visuomotor Policy Learning via Causal Diffusion](https://arxiv.org/abs/2506.14769)
*Jiahua Ma,Yiran Qin,Yixiong Li,Xuanqi Liao,Yulan Guo,Ruimao Zhang*

Main category: cs.CV

TL;DR: CDP是一种基于Transformer的扩散模型，通过利用历史动作序列提升动作预测的连贯性和上下文感知能力，解决了硬件限制和实时约束对机器人行为学习的影响。


<details>
  <summary>Details</summary>
Motivation: 硬件限制和实时约束降低了专家示范数据的质量，导致机器人行为学习在定位、抓取规划和长时任务执行中失败。

Method: 提出Causal Diffusion Policy (CDP)，结合历史动作序列进行条件化预测，并引入缓存机制以减少计算冗余。

Result: 在模拟和真实环境中，CDP在多种2D和3D任务中表现优于现有方法，且在输入质量下降时仍保持高精度。

Conclusion: CDP通过时间连续性推理，展示了在现实不完美条件下的实用鲁棒性。

Abstract: Diffusion Policy (DP) enables robots to learn complex behaviors by imitating expert demonstrations through action diffusion. However, in practical applications, hardware limitations often degrade data quality, while real-time constraints restrict model inference to instantaneous state and scene observations. These limitations seriously reduce the efficacy of learning from expert demonstrations, resulting in failures in object localization, grasp planning, and long-horizon task execution. To address these challenges, we propose Causal Diffusion Policy (CDP), a novel transformer-based diffusion model that enhances action prediction by conditioning on historical action sequences, thereby enabling more coherent and context-aware visuomotor policy learning. To further mitigate the computational cost associated with autoregressive inference, a caching mechanism is also introduced to store attention key-value pairs from previous timesteps, substantially reducing redundant computations during execution. Extensive experiments in both simulated and real-world environments, spanning diverse 2D and 3D manipulation tasks, demonstrate that CDP uniquely leverages historical action sequences to achieve significantly higher accuracy than existing methods. Moreover, even when faced with degraded input observation quality, CDP maintains remarkable precision by reasoning through temporal continuity, which highlights its practical robustness for robotic control under realistic, imperfect conditions.

</details>


### [163] [Synthetic Data Augmentation for Table Detection: Re-evaluating TableNet's Performance with Automatically Generated Document Images](https://arxiv.org/abs/2506.14583)
*Krishna Sahukara,Zineddine Bettouche,Andreas Fischer*

Main category: cs.CV

TL;DR: 论文提出了一种自动化LaTeX管道，用于生成包含多样化表格布局的合成数据，以增强真实数据集Marmot，并通过训练TableNet模型显著降低了手动标注需求。


<details>
  <summary>Details</summary>
Motivation: 手动提取文档中的表格效率低且易出错，需要一种自动化方法来生成合成数据以增强训练集。

Method: 使用LaTeX生成具有多样化表格布局的合成页面，并生成对齐的真实掩码，用于训练TableNet模型。

Result: 在合成测试集上，TableNet的像素级XOR错误率为4.04%（256x256分辨率）和4.33%（1024x1024分辨率）；在Marmot基准测试中最佳错误率为9.18%。

Conclusion: 合成数据有效提升了TableNet的性能，同时减少了手动标注的工作量。

Abstract: Document pages captured by smartphones or scanners often contain tables, yet manual extraction is slow and error-prone. We introduce an automated LaTeX-based pipeline that synthesizes realistic two-column pages with visually diverse table layouts and aligned ground-truth masks. The generated corpus augments the real-world Marmot benchmark and enables a systematic resolution study of TableNet. Training TableNet on our synthetic data achieves a pixel-wise XOR error of 4.04% on our synthetic test set with a 256x256 input resolution, and 4.33% with 1024x1024. The best performance on the Marmot benchmark is 9.18% (at 256x256), while cutting manual annotation effort through automation.

</details>


### [164] [PoseGRAF: Geometric-Reinforced Adaptive Fusion for Monocular 3D Human Pose Estimation](https://arxiv.org/abs/2506.14596)
*Ming Xu,Xu Zhang*

Main category: cs.CV

TL;DR: PoseGRAF框架通过双图卷积结构和跨注意力模块改进单目3D姿态估计，解决了现有方法忽略骨骼内在方向性和角度相关性的问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要依赖关节位置特征，忽略了骨骼内在方向性和角度相关性，导致在关节遮挡或快速运动变化时产生不合理姿态。

Method: 提出PoseGRAF框架，包括双图卷积结构（分别处理关节和骨骼图）、跨注意力模块建模骨骼方向与关节特征的依赖关系、动态融合模块自适应整合特征，以及改进的Transformer编码器。

Result: 在Human3.6M和MPI-INF-3DHP数据集上表现优于现有方法，并在野外视频中验证了泛化能力。

Conclusion: PoseGRAF通过结合关节和骨骼特征，显著提升了单目3D姿态估计的准确性和鲁棒性。

Abstract: Existing monocular 3D pose estimation methods primarily rely on joint positional features, while overlooking intrinsic directional and angular correlations within the skeleton. As a result, they often produce implausible poses under joint occlusions or rapid motion changes. To address these challenges, we propose the PoseGRAF framework. We first construct a dual graph convolutional structure that separately processes joint and bone graphs, effectively capturing their local dependencies. A Cross-Attention module is then introduced to model interdependencies between bone directions and joint features. Building upon this, a dynamic fusion module is designed to adaptively integrate both feature types by leveraging the relational dependencies between joints and bones. An improved Transformer encoder is further incorporated in a residual manner to generate the final output. Experimental results on the Human3.6M and MPI-INF-3DHP datasets show that our method exceeds state-of-the-art approaches. Additional evaluations on in-the-wild videos further validate its generalizability. The code is publicly available at https://github.com/iCityLab/PoseGRAF.

</details>


### [165] [Align Your Flow: Scaling Continuous-Time Flow Map Distillation](https://arxiv.org/abs/2506.14603)
*Amirmojtaba Sabour,Sanja Fidler,Karsten Kreis*

Main category: cs.CV

TL;DR: 论文提出了一种名为Align Your Flow的流映射模型，通过新的连续时间目标和训练技术，改进了现有的一致性模型和流匹配目标，实现了高效的少步生成，并在图像生成任务中取得了先进性能。


<details>
  <summary>Details</summary>
Motivation: 扩散和流模型虽然是最先进的生成模型，但需要多步采样，而一致性模型虽然可以一步生成，但性能随步数增加而下降。流映射通过连接任意两个噪声级别解决了这一问题。

Method: 提出了两种新的连续时间训练目标，结合自引导和对抗微调技术，训练流映射模型（Align Your Flow）。

Result: 在ImageNet 64x64和512x512上实现了少步生成的先进性能，文本到图像任务中优于现有非对抗训练的少步采样器。

Conclusion: 流映射模型在高效少步生成中表现出色，结合自引导和对抗微调进一步提升了性能，适用于多种生成任务。

Abstract: Diffusion- and flow-based models have emerged as state-of-the-art generative modeling approaches, but they require many sampling steps. Consistency models can distill these models into efficient one-step generators; however, unlike flow- and diffusion-based methods, their performance inevitably degrades when increasing the number of steps, which we show both analytically and empirically. Flow maps generalize these approaches by connecting any two noise levels in a single step and remain effective across all step counts. In this paper, we introduce two new continuous-time objectives for training flow maps, along with additional novel training techniques, generalizing existing consistency and flow matching objectives. We further demonstrate that autoguidance can improve performance, using a low-quality model for guidance during distillation, and an additional boost can be achieved by adversarial finetuning, with minimal loss in sample diversity. We extensively validate our flow map models, called Align Your Flow, on challenging image generation benchmarks and achieve state-of-the-art few-step generation performance on both ImageNet 64x64 and 512x512, using small and efficient neural networks. Finally, we show text-to-image flow map models that outperform all existing non-adversarially trained few-step samplers in text-conditioned synthesis.

</details>


### [166] [Unsupervised Imaging Inverse Problems with Diffusion Distribution Matching](https://arxiv.org/abs/2506.14605)
*Giacomo Meanti,Thomas Ryckeboer,Michael Arbel,Julien Mairal*

Main category: cs.CV

TL;DR: 提出了一种基于非配对数据集的图像恢复方法，适用于前向模型未知或数据配对困难的场景，性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 解决传统方法需要完整前向模型或配对数据的限制，适应真实场景中数据获取困难的问题。

Method: 利用条件流匹配建模退化观测分布，并通过分布匹配损失学习前向模型。

Result: 在去模糊和非均匀点扩散函数校准任务中优于单图像盲方法和无监督方法，盲超分辨率任务中达到SOTA。

Conclusion: 该方法在数据需求极低的情况下实现了高性能，适用于实际应用如镜头校准。

Abstract: This work addresses image restoration tasks through the lens of inverse problems using unpaired datasets. In contrast to traditional approaches -- which typically assume full knowledge of the forward model or access to paired degraded and ground-truth images -- the proposed method operates under minimal assumptions and relies only on small, unpaired datasets. This makes it particularly well-suited for real-world scenarios, where the forward model is often unknown or misspecified, and collecting paired data is costly or infeasible. The method leverages conditional flow matching to model the distribution of degraded observations, while simultaneously learning the forward model via a distribution-matching loss that arises naturally from the framework. Empirically, it outperforms both single-image blind and unsupervised approaches on deblurring and non-uniform point spread function (PSF) calibration tasks. It also matches state-of-the-art performance on blind super-resolution. We also showcase the effectiveness of our method with a proof of concept for lens calibration: a real-world application traditionally requiring time-consuming experiments and specialized equipment. In contrast, our approach achieves this with minimal data acquisition effort.

</details>


### [167] [VisText-Mosquito: A Multimodal Dataset and Benchmark for AI-Based Mosquito Breeding Site Detection and Reasoning](https://arxiv.org/abs/2506.14629)
*Md. Adnanul Islam,Md. Faiyaz Abdullah Sayeedi,Md. Asaduzzaman Shuvo,Muhammad Ziaur Rahman,Shahanur Rahman Bappy,Raiyan Rahman,Swakkhar Shatabda*

Main category: cs.CV

TL;DR: VisText-Mosquito是一个多模态数据集，结合视觉和文本数据，用于蚊子滋生地的自动化检测、分割和推理分析。


<details>
  <summary>Details</summary>
Motivation: 蚊子传播疾病是全球重大健康风险，需要早期检测和主动控制滋生地以防止疫情爆发。

Method: 数据集包含1,828张标注图像用于目标检测，142张用于水面分割，以及每张图像关联的自然语言推理文本。采用YOLOv9s和YOLOv11n-Seg模型进行检测和分割，BLIP模型进行推理生成。

Result: YOLOv9s在目标检测中达到最高精度0.92926，mAP@50为0.92891；YOLOv11n-Seg的分割精度为0.91587，mAP@50为0.79795。BLIP模型的BLEU得分为54.7，BERTScore为0.91，ROUGE-L为0.87。

Conclusion: 该数据集和模型框架体现了“预防胜于治疗”的主题，展示了AI如何主动应对蚊子传播疾病风险。数据和代码已开源。

Abstract: Mosquito-borne diseases pose a major global health risk, requiring early detection and proactive control of breeding sites to prevent outbreaks. In this paper, we present VisText-Mosquito, a multimodal dataset that integrates visual and textual data to support automated detection, segmentation, and reasoning for mosquito breeding site analysis. The dataset includes 1,828 annotated images for object detection, 142 images for water surface segmentation, and natural language reasoning texts linked to each image. The YOLOv9s model achieves the highest precision of 0.92926 and mAP@50 of 0.92891 for object detection, while YOLOv11n-Seg reaches a segmentation precision of 0.91587 and mAP@50 of 0.79795. For reasoning generation, our fine-tuned BLIP model achieves a final loss of 0.0028, with a BLEU score of 54.7, BERTScore of 0.91, and ROUGE-L of 0.87. This dataset and model framework emphasize the theme "Prevention is Better than Cure", showcasing how AI-based detection can proactively address mosquito-borne disease risks. The dataset and implementation code are publicly available at GitHub: https://github.com/adnanul-islam-jisun/VisText-Mosquito

</details>


### [168] [3DGS-IEval-15K: A Large-scale Image Quality Evaluation Database for 3D Gaussian-Splatting](https://arxiv.org/abs/2506.14642)
*Yuke Xing,Jiarui Wang,Peizhi Niu,Wenjie Huang,Guangtao Zhai,Yiling Xu*

Main category: cs.CV

TL;DR: 3DGS-IEval-15K是首个针对压缩3D高斯泼溅（3DGS）表示的大规模图像质量评估数据集，包含15,200张图像，用于评估不同压缩算法的感知影响。


<details>
  <summary>Details</summary>
Motivation: 3DGS在实时渲染中表现优异，但存储需求大，现有压缩方法缺乏统一的感知质量评估框架。

Method: 通过6种代表性3DGS算法在10个真实场景的20个视角生成图像，结合不同压缩级别，收集60名观众的主观评价数据。

Result: 数据集通过场景多样性和MOS分布分析验证质量，并建立了30种IQA指标的基准。

Conclusion: 该数据集为开发3DGS专用IQA指标提供了基础，并支持研究3DGS特有的视角依赖质量分布模式。

Abstract: 3D Gaussian Splatting (3DGS) has emerged as a promising approach for novel view synthesis, offering real-time rendering with high visual fidelity. However, its substantial storage requirements present significant challenges for practical applications. While recent state-of-the-art (SOTA) 3DGS methods increasingly incorporate dedicated compression modules, there is a lack of a comprehensive framework to evaluate their perceptual impact. Therefore we present 3DGS-IEval-15K, the first large-scale image quality assessment (IQA) dataset specifically designed for compressed 3DGS representations. Our dataset encompasses 15,200 images rendered from 10 real-world scenes through 6 representative 3DGS algorithms at 20 strategically selected viewpoints, with different compression levels leading to various distortion effects. Through controlled subjective experiments, we collect human perception data from 60 viewers. We validate dataset quality through scene diversity and MOS distribution analysis, and establish a comprehensive benchmark with 30 representative IQA metrics covering diverse types. As the largest-scale 3DGS quality assessment dataset to date, our work provides a foundation for developing 3DGS specialized IQA metrics, and offers essential data for investigating view-dependent quality distribution patterns unique to 3DGS. The database is publicly available at https://github.com/YukeXing/3DGS-IEval-15K.

</details>


### [169] [DDS-NAS: Dynamic Data Selection within Neural Architecture Search via On-line Hard Example Mining applied to Image Classification](https://arxiv.org/abs/2506.14667)
*Matt Poyser,Toby P. Breckon*

Main category: cs.CV

TL;DR: 通过动态硬样本挖掘和课程学习框架加速神经架构搜索（NAS）训练，提升可扩展性。


<details>
  <summary>Details</summary>
Motivation: 解决神经架构搜索（NAS）中的可扩展性挑战，减少训练时间和迭代次数。

Method: 利用自动编码器构建低维嵌入中的kd树结构，通过课程学习动态生成无偏子样本数据集。

Result: DDS-NAS框架将梯度基NAS策略加速高达27倍，且性能无损失。

Conclusion: 动态硬样本挖掘和课程学习显著提升NAS训练效率，减少收敛所需迭代次数。

Abstract: In order to address the scalability challenge within Neural Architecture Search (NAS), we speed up NAS training via dynamic hard example mining within a curriculum learning framework. By utilizing an autoencoder that enforces an image similarity embedding in latent space, we construct an efficient kd-tree structure to order images by furthest neighbour dissimilarity in a low-dimensional embedding. From a given query image from our subsample dataset, we can identify the most dissimilar image within the global dataset in logarithmic time. Via curriculum learning, we then dynamically re-formulate an unbiased subsample dataset for NAS optimisation, upon which the current NAS solution architecture performs poorly. We show that our DDS-NAS framework speeds up gradient-based NAS strategies by up to 27x without loss in performance. By maximising the contribution of each image sample during training, we reduce the duration of a NAS training cycle and the number of iterations required for convergence.

</details>


### [170] [Recognition through Reasoning: Reinforcing Image Geo-localization with Large Vision-Language Models](https://arxiv.org/abs/2506.14674)
*Ling Li,Yao Zhou,Yuxuan Liang,Fugee Tsung,Jiaheng Wei*

Main category: cs.CV

TL;DR: 论文提出了一种基于推理的图像地理定位方法GLOBE，通过构建多样化数据集MP16-Reason和优化视觉线索推理，显著提升了定位准确性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有地理定位方法多为分类或检索任务，缺乏可解释性；且现有数据集和模型在多样性和推理能力上存在局限。

Method: 构建MP16-Reason数据集，提出GLOBE方法，结合任务特定奖励优化视觉线索推理和定位评估。

Result: GLOBE在多样视觉场景中优于现有开源LVLMs，定位准确性更高，推理轨迹更直观。

Conclusion: GLOBE通过推理驱动的方法显著提升了地理定位任务的性能和可解释性。

Abstract: Previous methods for image geo-localization have typically treated the task as either classification or retrieval, often relying on black-box decisions that lack interpretability. The rise of large vision-language models (LVLMs) has enabled a rethinking of geo-localization as a reasoning-driven task grounded in visual cues. However, two major challenges persist. On the data side, existing reasoning-focused datasets are primarily based on street-view imagery, offering limited scene diversity and constrained viewpoints. On the modeling side, current approaches predominantly rely on supervised fine-tuning, which yields only marginal improvements in reasoning capabilities. To address these challenges, we propose a novel pipeline that constructs a reasoning-oriented geo-localization dataset, MP16-Reason, using diverse social media images. We introduce GLOBE, Group-relative policy optimization for Locatability assessment and Optimized visual-clue reasoning, yielding Bi-objective geo-Enhancement for the VLM in recognition and reasoning. GLOBE incorporates task-specific rewards that jointly enhance locatability assessment, visual clue reasoning, and geolocation accuracy. Both qualitative and quantitative results demonstrate that GLOBE outperforms state-of-the-art open-source LVLMs on geo-localization tasks, particularly in diverse visual scenes, while also generating more insightful and interpretable reasoning trajectories.

</details>


### [171] [FocalClick-XL: Towards Unified and High-quality Interactive Segmentation](https://arxiv.org/abs/2506.14686)
*Xi Chen,Hengshuang Zhao*

Main category: cs.CV

TL;DR: FocalClick-XL通过多阶段设计和子网络分解，提升了交互式分割的灵活性和精细度，支持多种交互形式并实现最先进性能。


<details>
  <summary>Details</summary>
Motivation: 现有交互式分割方法支持交互形式有限且难以捕捉细节，FocalClick-XL旨在解决这些问题。

Method: 采用多阶段策略，分解为上下文、对象和细节三个子任务，每个子网络独立预训练，并通过提示层编码交互类型。

Result: 在点击基准测试中表现最优，并适应多种交互形式（如框、涂鸦和粗掩码），还能预测精细的alpha遮罩。

Conclusion: FocalClick-XL是一种多功能且强大的交互式分割工具，支持多种交互形式并生成精细结果。

Abstract: Interactive segmentation enables users to extract binary masks of target objects through simple interactions such as clicks, scribbles, and boxes. However, existing methods often support only limited interaction forms and struggle to capture fine details. In this paper, we revisit the classical coarse-to-fine design of FocalClick and introduce significant extensions. Inspired by its multi-stage strategy, we propose a novel pipeline, FocalClick-XL, to address these challenges simultaneously. Following the emerging trend of large-scale pretraining, we decompose interactive segmentation into meta-tasks that capture different levels of information -- context, object, and detail -- assigning a dedicated subnet to each level.This decomposition allows each subnet to undergo scaled pretraining with independent data and supervision, maximizing its effectiveness. To enhance flexibility, we share context- and detail-level information across different interaction forms as common knowledge while introducing a prompting layer at the object level to encode specific interaction types. As a result, FocalClick-XL achieves state-of-the-art performance on click-based benchmarks and demonstrates remarkable adaptability to diverse interaction formats, including boxes, scribbles, and coarse masks. Beyond binary mask generation, it is also capable of predicting alpha mattes with fine-grained details, making it a versatile and powerful tool for interactive segmentation.

</details>


### [172] [YOLOv11-RGBT: Towards a Comprehensive Single-Stage Multispectral Object Detection Framework](https://arxiv.org/abs/2506.14696)
*Dahang Wan,Rongsheng Lu,Yang Fang,Xianli Lang,Shuangbao Shu,Jingjing Chen,Siyuan Shen,Ting Xu,Zecong Ye*

Main category: cs.CV

TL;DR: 论文提出YOLOv11-RGBT，一种基于YOLOv11的多模态目标检测框架，通过六种多光谱融合模式和可控微调策略提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法在多模态交互、低光条件和模型轻量化方面有进展，但仍缺乏统一框架、性能与融合策略平衡困难等问题。

Method: 基于YOLOv11设计六种多光谱融合模式，提出P3中融合策略和多光谱可控微调（MCF）策略。

Result: 在LLVIP和FLIR等数据集上表现优异，FLIR数据集上mAP提升3.41%-5.65%，最高达47.61%。

Conclusion: YOLOv11-RGBT框架和策略有效优化特征融合，提升模型适应性和鲁棒性。

Abstract: Multispectral object detection, which integrates information from multiple bands, can enhance detection accuracy and environmental adaptability, holding great application potential across various fields. Although existing methods have made progress in cross-modal interaction, low-light conditions, and model lightweight, there are still challenges like the lack of a unified single-stage framework, difficulty in balancing performance and fusion strategy, and unreasonable modality weight allocation. To address these, based on the YOLOv11 framework, we present YOLOv11-RGBT, a new comprehensive multimodal object detection framework. We designed six multispectral fusion modes and successfully applied them to models from YOLOv3 to YOLOv12 and RT-DETR. After reevaluating the importance of the two modalities, we proposed a P3 mid-fusion strategy and multispectral controllable fine-tuning (MCF) strategy for multispectral models. These improvements optimize feature fusion, reduce redundancy and mismatches, and boost overall model performance. Experiments show our framework excels on three major open-source multispectral object detection datasets, like LLVIP and FLIR. Particularly, the multispectral controllable fine-tuning strategy significantly enhanced model adaptability and robustness. On the FLIR dataset, it consistently improved YOLOv11 models' mAP by 3.41%-5.65%, reaching a maximum of 47.61%, verifying the framework and strategies' effectiveness. The code is available at: https://github.com/wandahangFY/YOLOv11-RGBT.

</details>


### [173] [Iterative Camera-LiDAR Extrinsic Optimization via Surrogate Diffusion](https://arxiv.org/abs/2506.14706)
*Ni Ou,Zhuo Chen,Xinru Zhang,Junzheng Wang*

Main category: cs.CV

TL;DR: 提出一种基于替代扩散的迭代框架，提升相机与LiDAR外参标定方法的精度、鲁棒性和稳定性。


<details>
  <summary>Details</summary>
Motivation: 现有端到端标定方法多为单步预测，缺乏迭代优化能力，难以满足高精度需求。

Method: 通过替代扩散框架迭代优化初始外参，原始标定方法作为替代去噪器逐步估计最终参数。

Result: 实验表明，该框架使所有标定方法在精度、鲁棒性和稳定性上均优于其他迭代技术和单步方法。

Conclusion: 提出的扩散模型为外参标定提供了一种通用且高效的迭代优化方案。

Abstract: Cameras and LiDAR are essential sensors for autonomous vehicles. The fusion of camera and LiDAR data addresses the limitations of individual sensors but relies on precise extrinsic calibration. Recently, numerous end-to-end calibration methods have been proposed; however, most predict extrinsic parameters in a single step and lack iterative optimization capabilities. To address the increasing demand for higher accuracy, we propose a versatile iterative framework based on surrogate diffusion. This framework can enhance the performance of any calibration method without requiring architectural modifications. Specifically, the initial extrinsic parameters undergo iterative refinement through a denoising process, in which the original calibration method serves as a surrogate denoiser to estimate the final extrinsics at each step. For comparative analysis, we selected four state-of-the-art calibration methods as surrogate denoisers and compared the results of our diffusion process with those of two other iterative approaches. Extensive experiments demonstrate that when integrated with our diffusion model, all calibration methods achieve higher accuracy, improved robustness, and greater stability compared to other iterative techniques and their single-step counterparts.

</details>


### [174] [Active InSAR monitoring of building damage in Gaza during the Israel-Hamas War](https://arxiv.org/abs/2506.14730)
*Corey Scher,Jamon Van Den Hoek*

Main category: cs.CV

TL;DR: 该论文提出了一种基于合成孔径雷达（SAR）的长时相干变化检测（LT-CCD）方法，用于实时监测加沙地带在2023年以哈战争中的建筑破坏情况。


<details>
  <summary>Details</summary>
Motivation: 在持续冲突中，传统方法难以实时监测动态地理区域的破坏情况，SAR技术为此提供了可能。

Method: 使用Sentinel-1的干涉SAR数据，应用LT-CCD方法，每周跟踪破坏趋势。

Result: 检测到联合国参考数据中92.5%的破坏标签，误报率仅1.2%。研究结束时，加沙地带61%的建筑受损或毁坏。

Conclusion: 该方法成本低、延迟低，为人道主义和新闻机构提供了快速获取冲突区域破坏信息的途径。

Abstract: Aerial bombardment of the Gaza Strip beginning October 7, 2023 is one of the most intense bombing campaigns of the twenty-first century, driving widespread urban damage. Characterizing damage over a geographically dynamic and protracted armed conflict requires active monitoring. Synthetic aperture radar (SAR) has precedence for mapping disaster-induced damage with bi-temporal methods but applications to active monitoring during sustained crises are limited. Using interferometric SAR data from Sentinel-1, we apply a long temporal-arc coherent change detection (LT-CCD) approach to track weekly damage trends over the first year of the 2023- Israel-Hamas War. We detect 92.5% of damage labels in reference data from the United Nations with a negligible (1.2%) false positive rate. The temporal fidelity of our approach reveals rapidly increasing damage during the first three months of the war focused in northern Gaza, a notable pause in damage during a temporary ceasefire, and surges of new damage as conflict hot-spots shift from north to south. Three-fifths (191,263) of all buildings are damaged or destroyed by the end of the study. With massive need for timely data on damage in armed conflict zones, our low-cost and low-latency approach enables rapid uptake of damage information at humanitarian and journalistic organizations.

</details>


### [175] [SyncTalk++: High-Fidelity and Efficient Synchronized Talking Heads Synthesis Using Gaussian Splatting](https://arxiv.org/abs/2506.14742)
*Ziqiao Peng,Wentao Hu,Junyuan Ma,Xiangyu Zhu,Xiaomei Zhang,Hao Zhao,Hui Tian,Jun He,Hongyan Liu,Zhaoxin Fan*

Main category: cs.CV

TL;DR: SyncTalk++通过动态肖像渲染器和面部同步控制器解决了语音驱动视频中的同步问题，显著提升了真实感和渲染速度。


<details>
  <summary>Details</summary>
Motivation: 高同步性是实现逼真语音驱动视频的关键，但现有方法在身份、唇动、表情和头部姿态的同步上存在不足。

Method: SyncTalk++采用高斯分割的动态肖像渲染器、3D面部混合形状模型和头部同步稳定器，结合表情生成器和躯干修复器提升鲁棒性。

Result: 实验表明SyncTalk++在同步性和真实感上优于现有方法，渲染速度达101帧/秒。

Conclusion: SyncTalk++通过多模块协同解决了同步问题，显著提升了语音驱动视频的质量和效率。

Abstract: Achieving high synchronization in the synthesis of realistic, speech-driven talking head videos presents a significant challenge. A lifelike talking head requires synchronized coordination of subject identity, lip movements, facial expressions, and head poses. The absence of these synchronizations is a fundamental flaw, leading to unrealistic results. To address the critical issue of synchronization, identified as the ''devil'' in creating realistic talking heads, we introduce SyncTalk++, which features a Dynamic Portrait Renderer with Gaussian Splatting to ensure consistent subject identity preservation and a Face-Sync Controller that aligns lip movements with speech while innovatively using a 3D facial blendshape model to reconstruct accurate facial expressions. To ensure natural head movements, we propose a Head-Sync Stabilizer, which optimizes head poses for greater stability. Additionally, SyncTalk++ enhances robustness to out-of-distribution (OOD) audio by incorporating an Expression Generator and a Torso Restorer, which generate speech-matched facial expressions and seamless torso regions. Our approach maintains consistency and continuity in visual details across frames and significantly improves rendering speed and quality, achieving up to 101 frames per second. Extensive experiments and user studies demonstrate that SyncTalk++ outperforms state-of-the-art methods in synchronization and realism. We recommend watching the supplementary video: https://ziqiaopeng.github.io/synctalk++.

</details>


### [176] [Cost-Aware Routing for Efficient Text-To-Image Generation](https://arxiv.org/abs/2506.14753)
*Qinchan,Li,Kenneth Chen,Changyue,Su,Wittawat Jitkrittum,Qi Sun,Patsorn Sangkloy*

Main category: cs.CV

TL;DR: 该论文提出了一种框架，根据提示的复杂性动态调整计算资源，以优化扩散模型在生成高质量图像时的计算成本。


<details>
  <summary>Details</summary>
Motivation: 扩散模型生成高质量图像的计算成本高，需要平衡质量和计算效率。

Method: 通过自动路由提示到最合适的文本到图像生成函数（如不同步数的扩散模型或其他独立模型），实现计算资源的动态分配。

Result: 在COCO和DiffusionDB上实验表明，该方法通过路由到九个预训练模型，平均质量高于单独使用任一模型。

Conclusion: 该框架能有效平衡质量与计算成本，适用于复杂提示的高质量生成需求。

Abstract: Diffusion models are well known for their ability to generate a high-fidelity image for an input prompt through an iterative denoising process. Unfortunately, the high fidelity also comes at a high computational cost due the inherently sequential generative process. In this work, we seek to optimally balance quality and computational cost, and propose a framework to allow the amount of computation to vary for each prompt, depending on its complexity. Each prompt is automatically routed to the most appropriate text-to-image generation function, which may correspond to a distinct number of denoising steps of a diffusion model, or a disparate, independent text-to-image model. Unlike uniform cost reduction techniques (e.g., distillation, model quantization), our approach achieves the optimal trade-off by learning to reserve expensive choices (e.g., 100+ denoising steps) only for a few complex prompts, and employ more economical choices (e.g., small distilled model) for less sophisticated prompts. We empirically demonstrate on COCO and DiffusionDB that by learning to route to nine already-trained text-to-image models, our approach is able to deliver an average quality that is higher than that achievable by any of these models alone.

</details>


### [177] [Scaling-Up the Pretraining of the Earth Observation Foundation Model PhilEO to the MajorTOM Dataset](https://arxiv.org/abs/2506.14765)
*Nikolaos Dionelis,Jente Bosmans,Riccardo Musto,Giancarlo Paoletti,Simone Sarti,Giacomo Cascarano,Casper Fibaek,Luke Camilleri,Bertrand Le Saux,Nicolas Longépé*

Main category: cs.CV

TL;DR: 论文提出了一种基于大规模未标记数据的EO基础模型PhilEO，通过扩展数据集和模型规模，验证了其在道路密度估计、建筑密度回归和土地覆盖分割任务中的优越性能。


<details>
  <summary>Details</summary>
Motivation: 地球观测卫星生成大量数据，但需要高效利用这些信息。通过预训练基础模型，可以在少量标记数据下实现多种下游任务的高效微调。

Method: 扩展了PhilEO Geo-Aware U-Net模型，在23TB的MajorTOM数据集和2TB的FastTOM子集上进行预训练，研究了不同参数和架构的模型变体，并在PhilEO Bench上进行了微调和性能评估。

Result: PhilEO 44M MajorTOM 23TB模型在道路密度回归任务中表现最佳，而PhilEO 200M FastTOM在大多数任务中优于其他模型。数据集和模型规模的扩展均被验证有效。

Conclusion: 通过扩展数据集和模型规模，PhilEO模型在多个任务中表现出色，同时探索了从CNN到ViT的架构转变影响。

Abstract: Today, Earth Observation (EO) satellites generate massive volumes of data, with the Copernicus Sentinel-2 constellation alone producing approximately 1.6TB per day. To fully exploit this information, it is essential to pretrain EO Foundation Models (FMs) on large unlabeled datasets, enabling efficient fine-tuning for several different downstream tasks with minimal labeled data. In this work, we present the scaling-up of our recently proposed EO Foundation Model, PhilEO Geo-Aware U-Net, on the unlabeled 23TB dataset MajorTOM, which covers the vast majority of the Earth's surface, as well as on the specialized subset FastTOM 2TB that does not include oceans and ice. We develop and study various PhilEO model variants with different numbers of parameters and architectures. Finally, we fine-tune the models on the PhilEO Bench for road density estimation, building density pixel-wise regression, and land cover semantic segmentation, and we evaluate the performance. Our results demonstrate that for all n-shots for road density regression, the PhilEO 44M MajorTOM 23TB model outperforms PhilEO Globe 0.5TB 44M. We also show that for most n-shots for road density estimation and building density regression, PhilEO 200M FastTOM outperforms all the other models. The effectiveness of both dataset and model scaling is validated using the PhilEO Bench. We also study the impact of architecture scaling, transitioning from U-Net Convolutional Neural Networks (CNN) to Vision Transformers (ViT).

</details>


### [178] [ASCD: Attention-Steerable Contrastive Decoding for Reducing Hallucination in MLLM](https://arxiv.org/abs/2506.14766)
*Yujun Wang,Jinhe Bi,Yunpu Ma,Soeren Pirk*

Main category: cs.CV

TL;DR: 本文提出了一种基于注意力引导的对比解码框架，通过直接干预模型的注意力机制来减少多模态大语言模型（MLLM）的幻觉问题。


<details>
  <summary>Details</summary>
Motivation: 现有的方法（如VCD和ICD）虽然能缓解幻觉问题，但其有效性可能源于对注意力分布的深层影响，而非表面上的对数调整。

Method: 提出了一种注意力引导的对比解码框架，直接干预模型的注意力机制。

Result: 实验表明，该方法显著减少了幻觉问题，并在POPE、CHAIR和MMHal-Bench等基准测试中提升了性能，同时改善了标准VQA任务的表现。

Conclusion: 通过直接干预注意力机制，该方法为减少MLLM的幻觉问题提供了更原则性的解决方案。

Abstract: Multimodal Large Language Model (MLLM) often suffer from hallucinations. They over-rely on partial cues and generate incorrect responses. Recently, methods like Visual Contrastive Decoding (VCD) and Instruction Contrastive Decoding (ICD) have been proposed to mitigate hallucinations by contrasting predictions from perturbed or negatively prefixed inputs against original outputs. In this work, we uncover that methods like VCD and ICD fundamentally influence internal attention dynamics of the model. This observation suggests that their effectiveness may not stem merely from surface-level modifications to logits but from deeper shifts in attention distribution. Inspired by this insight, we propose an attention-steerable contrastive decoding framework that directly intervenes in attention mechanisms of the model to offer a more principled approach to mitigating hallucinations. Our experiments across multiple MLLM architectures and diverse decoding methods demonstrate that our approach significantly reduces hallucinations and improves the performance on benchmarks such as POPE, CHAIR, and MMHal-Bench, while simultaneously enhancing performance on standard VQA benchmarks.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [179] [LittleBit: Ultra Low-Bit Quantization via Latent Factorization](https://arxiv.org/abs/2506.13771)
*Banseok Lee,Dongkyu Kim,Youngcheon You,Youngmin Kim*

Main category: cs.LG

TL;DR: LittleBit是一种极端压缩大语言模型（LLM）的新方法，通过低秩矩阵分解和二值化实现0.1比特每权重（BPW）的压缩，显著减少内存和计算成本。


<details>
  <summary>Details</summary>
Motivation: 解决LLM部署中高内存和计算成本的问题，尤其是在低于1比特量化时性能下降的挑战。

Method: 采用低秩矩阵分解表示权重，二值化因子，并通过多尺度补偿机制（如行、列和潜在维度）减少信息损失。关键贡献包括Dual-SVID和残差补偿。

Result: 在0.1 BPW下，Llama2-7B性能优于其他方法的0.7 BPW，内存减少31倍，内核级基准测试显示速度提升5倍。

Conclusion: LittleBit为资源受限环境中部署高性能LLM提供了有效解决方案。

Abstract: Deploying large language models (LLMs) often faces challenges from substantial memory and computational costs. Quantization offers a solution, yet performance degradation in the sub-1-bit regime remains particularly difficult. This paper introduces LittleBit, a novel method for extreme LLM compression. It targets levels like 0.1 bits per weight (BPW), achieving nearly 31$\times$ memory reduction, e.g., Llama2-13B to under 0.9 GB. LittleBit represents weights in a low-rank form using latent matrix factorization, subsequently binarizing these factors. To counteract information loss from this extreme precision, it integrates a multi-scale compensation mechanism. This includes row, column, and an additional latent dimension that learns per-rank importance. Two key contributions enable effective training: Dual Sign-Value-Independent Decomposition (Dual-SVID) for stable quantization-aware training (QAT) initialization, and integrated Residual Compensation to mitigate errors. Extensive experiments confirm LittleBit's superiority in sub-1-bit quantization: e.g., its 0.1 BPW performance on Llama2-7B surpasses the leading method's 0.7 BPW. This establishes a superior size-performance trade-off, with kernel-level benchmarks indicating potential for a 5$\times$ speedup compared to FP16. LittleBit paves the way for deploying powerful LLMs in resource-constrained environments.

</details>


### [180] [MobiEdit: Resource-efficient Knowledge Editing for Personalized On-device LLMs](https://arxiv.org/abs/2506.13772)
*Zhenyan Lu,Daliang Xu,Dongqi Cai,Zexi Li,Wei Liu,Fangming Liu,Shangguang Wang,Mengwei Xu*

Main category: cs.LG

TL;DR: MobiEdit是一种移动知识编辑框架，通过量化前向梯度估计替代资源密集的反向传播，实现在商用移动设备上高效个性化大型语言模型。


<details>
  <summary>Details</summary>
Motivation: 解决大型语言模型在移动设备上处理个性化查询时的幻觉问题，同时避免资源密集型更新方法的不实用性。

Method: 采用量化前向梯度估计替代反向传播，结合早期停止机制和前缀缓存优化，提升效率。

Result: 在商用移动设备上，MobiEdit实现了3B参数模型的实时编辑，内存、能耗和延迟分别减少7.6倍、14.7倍和3.6倍。

Conclusion: MobiEdit为移动设备上的大型语言模型知识编辑提供了一种高效、实用的解决方案。

Abstract: Large language models (LLMs) are deployed on mobile devices to power killer applications such as intelligent assistants. LLMs pre-trained on general corpora often hallucinate when handling personalized or unseen queries, leading to incorrect or outdated responses. Knowledge editing addresses this by identifying and adjusting a small crucial portion of model weights, without compromising the general knowledge. However, prior knowledge editing methods are impractical to run on local devices due to the resource-heavy backpropagation (BP) needed for updates. We present MobiEdit, the first mobile knowledge editing framework that enables efficient LLM personalization on commercial off-the-shelf (COTS) mobile devices. MobiEdit replaces full-precision BP with quantized forward-only gradient estimation, thus compatible with the energy-efficient mobile neural processing units (NPUs). MobiEdit replaces full-precision backpropagation with quantized forward-only gradient estimation, making it compatible with energy-efficient mobile NPUs. To further improve gradient estimation efficiency, we introduce two optimizations: an early stoping mechanism that adaptively terminates editing upon success and a prefix cache that reuses computation across steps. Our approach enables real-time editing of a 3B-parameter model (Qwen2.5-3B-Instruct) on COTS mobile devices with 7.6$\times$ less memory, 14.7 $\times$ less energy and 3.6$\times$ less latency compared to previous knowledge editing methods.

</details>


### [181] [Solving the Job Shop Scheduling Problem with Graph Neural Networks: A Customizable Reinforcement Learning Environment](https://arxiv.org/abs/2506.13781)
*Pablo Ariño Fernández*

Main category: cs.LG

TL;DR: 论文介绍了JobShopLib，一个模块化库，用于定制化解决作业车间调度问题，通过强化学习环境训练调度器，展示了其效果。


<details>
  <summary>Details</summary>
Motivation: 作业车间调度问题是NP难问题，传统方法依赖简单启发式规则，深度学习模型（如GNN）虽有效但实验复杂，缺乏模块化工具。

Method: 提出JobShopLib模块化库，支持自定义图表示、节点特征、动作空间和奖励函数，并通过模仿学习训练调度器。

Result: GNN模型在大规模问题上接近最优结果，证明了特征定制的重要性。

Conclusion: JobShopLib为未来研究提供了必要工具，表明此类模型仍有改进空间。

Abstract: The job shop scheduling problem is an NP-hard combinatorial optimization problem relevant to manufacturing and timetabling. Traditional approaches use priority dispatching rules based on simple heuristics. Recent work has attempted to replace these with deep learning models, particularly graph neural networks (GNNs), that learn to assign priorities from data. However, training such models requires customizing numerous factors: graph representation, node features, action space, and reward functions. The lack of modular libraries for experimentation makes this research time-consuming. This work introduces JobShopLib, a modular library that allows customizing these factors and creating new components with its reinforcement learning environment. We trained several dispatchers through imitation learning to demonstrate the environment's utility. One model outperformed various graph-based dispatchers using only individual operation features, highlighting the importance of feature customization. Our GNN model achieved near state-of-the-art results on large-scale problems. These results suggest significant room for improvement in developing such models. JobShopLib provides the necessary tools for future experimentation.

</details>


### [182] [Enhancing Bagging Ensemble Regression with Data Integration for Time Series-Based Diabetes Prediction](https://arxiv.org/abs/2506.13786)
*Vuong M. Ngo,Tran Quang Vinh,Patricia Kearney,Mark Roantree*

Main category: cs.LG

TL;DR: 该研究通过数据整合和增强的bagging集成回归模型（EBMBag+）预测美国城市糖尿病患病率，表现优于其他基线模型。


<details>
  <summary>Details</summary>
Motivation: 糖尿病是一种慢性代谢疾病，准确预测其患病率对医疗规划和干预至关重要，但现有数据往往不完整。

Method: 整合2011-2021年糖尿病相关数据集，提出EBMBag+模型，并与多种基线模型（如SVMReg、BDTree等）进行比较。

Result: EBMBag+表现最佳，MAE为0.41，RMSE为0.53，MAPE为4.01，R2为0.9。

Conclusion: EBMBag+在糖尿病患病率预测中具有显著优势，适用于医疗规划和干预。

Abstract: Diabetes is a chronic metabolic disease characterized by elevated blood glucose levels, leading to complications like heart disease, kidney failure, and nerve damage. Accurate state-level predictions are vital for effective healthcare planning and targeted interventions, but in many cases, data for necessary analyses are incomplete. This study begins with a data engineering process to integrate diabetes-related datasets from 2011 to 2021 to create a comprehensive feature set. We then introduce an enhanced bagging ensemble regression model (EBMBag+) for time series forecasting to predict diabetes prevalence across U.S. cities. Several baseline models, including SVMReg, BDTree, LSBoost, NN, LSTM, and ERMBag, were evaluated for comparison with our EBMBag+ algorithm. The experimental results demonstrate that EBMBag+ achieved the best performance, with an MAE of 0.41, RMSE of 0.53, MAPE of 4.01, and an R2 of 0.9.

</details>


### [183] [Hybrid Meta-Learning Framework for Anomaly Forecasting in Nonlinear Dynamical Systems via Physics-Inspired Simulation and Deep Ensembles](https://arxiv.org/abs/2506.13828)
*Abdullah Burkan Bereketoglu*

Main category: cs.LG

TL;DR: 提出了一种混合元学习框架，用于非线性动态系统中的预测和异常检测，结合了物理模拟器和多种深度学习模型，表现优于独立模型。


<details>
  <summary>Details</summary>
Motivation: 针对非线性动态系统的非平稳和随机行为，提出一种通用的数据驱动方法，用于早期缺陷识别和预测监控。

Method: 集成物理模拟器、CNN-LSTM、VAE、Isolation Forests和DA-RNN，通过元学习器结合预测输出、重建误差和残差分数。

Result: 混合模型在异常定位、泛化和非线性偏差鲁棒性方面优于独立模型。

Conclusion: 该框架为非线性系统提供了一种广泛适用的数据驱动方法，适用于物理模型不完整的场景。

Abstract: We propose a hybrid meta-learning framework for forecasting and anomaly detection in nonlinear dynamical systems characterized by nonstationary and stochastic behavior. The approach integrates a physics-inspired simulator that captures nonlinear growth-relaxation dynamics with random perturbations, representative of many complex physical, industrial, and cyber-physical systems. We use CNN-LSTM architectures for spatio-temporal feature extraction, Variational Autoencoders (VAE) for unsupervised anomaly scoring, and Isolation Forests for residual-based outlier detection in addition to a Dual-Stage Attention Recurrent Neural Network (DA-RNN) for one-step forecasting on top of the generated simulation data. To create composite anomaly forecasts, these models are combined using a meta-learner that combines forecasting outputs, reconstruction errors, and residual scores. The hybrid ensemble performs better than standalone models in anomaly localization, generalization, and robustness to nonlinear deviations, according to simulation-based experiments. The framework provides a broad, data-driven approach to early defect identification and predictive monitoring in nonlinear systems, which may be applied to a variety of scenarios where complete physical models might not be accessible.

</details>


### [184] [Light Aircraft Game : Basic Implementation and training results analysis](https://arxiv.org/abs/2506.14164)
*Hanzhong Cao*

Main category: cs.LG

TL;DR: 论文研究了多智能体强化学习（MARL）在部分可观测、合作-竞争战斗环境LAG中的应用，比较了HAPPO和HASAC两种算法的表现。


<details>
  <summary>Details</summary>
Motivation: 探索在复杂战斗环境中，不同MARL算法的适应性及性能差异。

Method: 使用HAPPO（基于PPO的分层算法）和HASAC（基于软演员-批评家的离策略方法）在LAG环境中进行实验。

Result: HASAC在无武器简单任务中表现良好，HAPPO在动态导弹战斗中更具适应性。

Conclusion: 研究揭示了在MARL中，策略与离策略方法在不同任务中的权衡。

Abstract: This paper investigates multi-agent reinforcement learning (MARL) in a partially observable, cooperative-competitive combat environment known as LAG. We describe the environment's setup, including agent actions, hierarchical controls, and reward design across different combat modes such as No Weapon and ShootMissile. Two representative algorithms are evaluated: HAPPO, an on-policy hierarchical variant of PPO, and HASAC, an off-policy method based on soft actor-critic. We analyze their training stability, reward progression, and inter-agent coordination capabilities. Experimental results show that HASAC performs well in simpler coordination tasks without weapons, while HAPPO demonstrates stronger adaptability in more dynamic and expressive scenarios involving missile combat. These findings provide insights into the trade-offs between on-policy and off-policy methods in multi-agent settings.

</details>


### [185] [Quantifying Structure in CLIP Embeddings: A Statistical Framework for Concept Interpretation](https://arxiv.org/abs/2506.13831)
*Jitian Zhao,Chenghui Li,Frederic Sala,Karl Rohe*

Main category: cs.LG

TL;DR: 论文提出了一种基于假设检验的概念分解方法，用于解释CLIP嵌入空间中的概念，提高了统计严谨性和重建准确性。


<details>
  <summary>Details</summary>
Motivation: 当前基于概念的方法缺乏统计严谨性，难以验证概念和比较不同技术，因此需要一种更可靠的方法。

Method: 引入假设检验框架量化CLIP嵌入空间中的旋转敏感结构，并提出一种后验概念分解方法，具有理论保证。

Result: 方法在重建误差上优于其他技术，并在去除虚假背景概念后，最差组准确率提高了22.6%。

Conclusion: 该方法在解释性和准确性之间取得了平衡，能有效减少数据中的虚假线索。

Abstract: Concept-based approaches, which aim to identify human-understandable concepts within a model's internal representations, are a promising method for interpreting embeddings from deep neural network models, such as CLIP. While these approaches help explain model behavior, current methods lack statistical rigor, making it challenging to validate identified concepts and compare different techniques. To address this challenge, we introduce a hypothesis testing framework that quantifies rotation-sensitive structures within the CLIP embedding space. Once such structures are identified, we propose a post-hoc concept decomposition method. Unlike existing approaches, it offers theoretical guarantees that discovered concepts represent robust, reproducible patterns (rather than method-specific artifacts) and outperforms other techniques in terms of reconstruction error. Empirically, we demonstrate that our concept-based decomposition algorithm effectively balances reconstruction accuracy with concept interpretability and helps mitigate spurious cues in data. Applied to a popular spurious correlation dataset, our method yields a 22.6% increase in worst-group accuracy after removing spurious background concepts.

</details>


### [186] [Evolvable Conditional Diffusion](https://arxiv.org/abs/2506.13834)
*Zhao Wei,Chin Chun Ooi,Abhishek Gupta,Jian Cheng Wong,Pao-Hsiung Chiu,Sheares Xue Wen Toh,Yew-Soon Ong*

Main category: cs.LG

TL;DR: 提出了一种可进化的条件扩散方法，用于指导生成过程，以支持自主科学发现，特别适用于计算流体动力学和电磁学等领域中的黑盒、不可微分多物理模型。


<details>
  <summary>Details</summary>
Motivation: 解决在多物理模型中因不可微分性而难以直接使用梯度引导生成的问题，提供一种无需计算导数的优化方法。

Method: 将引导问题转化为优化问题，通过更新去噪分布的描述统计量来优化目标函数，并基于概率进化原理推导出进化引导方法。

Result: 在流体拓扑和超表面设计的AI科学场景中验证了方法的有效性，生成的方案更符合优化目标，且无需依赖可微分代理。

Conclusion: 该方法为基于引导的扩散提供了一种有效手段，能够利用科学领域中常见的黑盒、不可微分多物理数值模型。

Abstract: This paper presents an evolvable conditional diffusion method such that black-box, non-differentiable multi-physics models, as are common in domains like computational fluid dynamics and electromagnetics, can be effectively used for guiding the generative process to facilitate autonomous scientific discovery. We formulate the guidance as an optimization problem where one optimizes for a desired fitness function through updates to the descriptive statistic for the denoising distribution, and derive an evolution-guided approach from first principles through the lens of probabilistic evolution. Interestingly, the final derived update algorithm is analogous to the update as per common gradient-based guided diffusion models, but without ever having to compute any derivatives. We validate our proposed evolvable diffusion algorithm in two AI for Science scenarios: the automated design of fluidic topology and meta-surface. Results demonstrate that this method effectively generates designs that better satisfy specific optimization objectives without reliance on differentiable proxies, providing an effective means of guidance-based diffusion that can capitalize on the wealth of black-box, non-differentiable multi-physics numerical models common across Science.

</details>


### [187] [Robustness of Reinforcement Learning-Based Traffic Signal Control under Incidents: A Comparative Study](https://arxiv.org/abs/2506.13836)
*Dang Viet Anh Nguyen,Carlos Lima Azevedo,Tomer Toledo,Filipe Rodrigues*

Main category: cs.LG

TL;DR: T-REX是一个基于SUMO的仿真框架，用于在动态交通事件场景下训练和评估强化学习交通信号控制（RL-TSC）方法，揭示了现有方法在突发事件下的性能差异。


<details>
  <summary>Details</summary>
Motivation: 研究RL-TSC在真实交通事件中的鲁棒性不足，提出T-REX框架以填补这一空白。

Method: 开发T-REX框架，模拟真实网络性能，包括驾驶员行为（如改道和变道），并提出新的鲁棒性评估指标。

Result: 独立值基和分散压力基方法在稳定条件下表现良好，但在突发事件下性能下降；分层协调方法在大规模网络中更稳定但收敛慢。

Conclusion: RL-TSC研究需关注鲁棒性设计，T-REX为动态场景下的方法评估提供了标准化平台。

Abstract: Reinforcement learning-based traffic signal control (RL-TSC) has emerged as a promising approach for improving urban mobility. However, its robustness under real-world disruptions such as traffic incidents remains largely underexplored. In this study, we introduce T-REX, an open-source, SUMO-based simulation framework for training and evaluating RL-TSC methods under dynamic, incident scenarios. T-REX models realistic network-level performance considering drivers' probabilistic rerouting, speed adaptation, and contextual lane-changing, enabling the simulation of congestion propagation under incidents. To assess robustness, we propose a suite of metrics that extend beyond conventional traffic efficiency measures. Through extensive experiments across synthetic and real-world networks, we showcase T-REX for the evaluation of several state-of-the-art RL-TSC methods under multiple real-world deployment paradigms. Our findings show that while independent value-based and decentralized pressure-based methods offer fast convergence and generalization in stable traffic conditions and homogeneous networks, their performance degrades sharply under incident-driven distribution shifts. In contrast, hierarchical coordination methods tend to offer more stable and adaptable performance in large-scale, irregular networks, benefiting from their structured decision-making architecture. However, this comes with the trade-off of slower convergence and higher training complexity. These findings highlight the need for robustness-aware design and evaluation in RL-TSC research. T-REX contributes to this effort by providing an open, standardized and reproducible platform for benchmarking RL methods under dynamic and disruptive traffic scenarios.

</details>


### [188] [Sustainable Machine Learning Retraining: Optimizing Energy Efficiency Without Compromising Accuracy](https://arxiv.org/abs/2506.13838)
*Lorena Poenaru-Olaru,June Sallou,Luis Cruz,Jan Rellermeyer,Arie van Deursen*

Main category: cs.LG

TL;DR: 研究比较了机器学习模型重训练技术的能耗与准确性，发现仅使用最新数据或按需重训练可显著降低能耗，为可持续ML系统设计提供建议。


<details>
  <summary>Details</summary>
Motivation: 机器学习系统因数据变化需定期重训练，但传统方法能耗高，需探索更可持续的重训练技术。

Method: 研究常见重训练技术的能耗，比较其能源效率与准确性，提出基于最新数据或按需重训练的替代方案。

Result: 仅用最新数据重训练可减少25%能耗；按需重训练可减少40%能耗，前提是有可靠的数据变化检测器。

Conclusion: 研究结果为ML从业者提供了更节能的重训练技术建议，助力可持续ML系统设计。

Abstract: The reliability of machine learning (ML) software systems is heavily influenced by changes in data over time. For that reason, ML systems require regular maintenance, typically based on model retraining. However, retraining requires significant computational demand, which makes it energy-intensive and raises concerns about its environmental impact. To understand which retraining techniques should be considered when designing sustainable ML applications, in this work, we study the energy consumption of common retraining techniques. Since the accuracy of ML systems is also essential, we compare retraining techniques in terms of both energy efficiency and accuracy. We showcase that retraining with only the most recent data, compared to all available data, reduces energy consumption by up to 25\%, being a sustainable alternative to the status quo. Furthermore, our findings show that retraining a model only when there is evidence that updates are necessary, rather than on a fixed schedule, can reduce energy consumption by up to 40\%, provided a reliable data change detector is in place. Our findings pave the way for better recommendations for ML practitioners, guiding them toward more energy-efficient retraining techniques when designing sustainable ML software systems.

</details>


### [189] [SatHealth: A Multimodal Public Health Dataset with Satellite-based Environmental Factors](https://arxiv.org/abs/2506.13842)
*Yuanlong Wang,Pengqi Wang,Changchang Yin,Ping Zhang*

Main category: cs.LG

TL;DR: SatHealth是一个结合多模态时空数据的新数据集，用于提升AI模型在公共卫生和个人疾病风险预测中的性能。


<details>
  <summary>Details</summary>
Motivation: 现有研究因缺乏长期和细粒度的时空数据，难以整合环境数据，限制了模型的性能和实际应用。

Method: 开发了SatHealth数据集，整合环境数据、卫星图像、疾病流行率和社会健康决定因素，并在两个用例中进行了实验。

Result: 实验表明，环境信息显著提高了AI模型的性能和时空泛化能力。

Conclusion: SatHealth为医疗研究提供了环境数据整合的新角度和资源，并建立了环境健康信息学的基础框架。

Abstract: Living environments play a vital role in the prevalence and progression of diseases, and understanding their impact on patient's health status becomes increasingly crucial for developing AI models. However, due to the lack of long-term and fine-grained spatial and temporal data in public and population health studies, most existing studies fail to incorporate environmental data, limiting the models' performance and real-world application. To address this shortage, we developed SatHealth, a novel dataset combining multimodal spatiotemporal data, including environmental data, satellite images, all-disease prevalences estimated from medical claims, and social determinants of health (SDoH) indicators. We conducted experiments under two use cases with SatHealth: regional public health modeling and personal disease risk prediction. Experimental results show that living environmental information can significantly improve AI models' performance and temporal-spatial generalizability on various tasks. Finally, we deploy a web-based application to provide an exploration tool for SatHealth and one-click access to both our data and regional environmental embedding to facilitate plug-and-play utilization. SatHealth is now published with data in Ohio, and we will keep updating SatHealth to cover the other parts of the US. With the web application and published code pipeline, our work provides valuable angles and resources to include environmental data in healthcare research and establishes a foundational framework for future research in environmental health informatics.

</details>


### [190] [StaQ it! Growing neural networks for Policy Mirror Descent](https://arxiv.org/abs/2506.13862)
*Alena Shilova,Alex Davey,Brahim Driss,Riad Akrour*

Main category: cs.LG

TL;DR: 论文提出了一种改进的PMD算法（StaQ），通过仅保留最近的M个Q函数来解决传统PMD算法中存储所有历史Q函数的问题，实现了收敛且稳定的深度强化学习。


<details>
  <summary>Details</summary>
Motivation: 在强化学习中，正则化方法（如熵奖励或KL散度约束）虽能提升探索性和稳定性，但传统PMD算法因需存储所有历史Q函数而难以实际应用。

Method: 提出PMD-like算法，仅保留最近的M个Q函数，证明在M足够大时可实现收敛且无策略更新误差。

Result: StaQ算法具有理论保证，性能与基线方法相当且波动更小。

Conclusion: StaQ为深度强化学习提供了稳定且可实验的PMD框架，推动了完全稳定算法的研究。

Abstract: In Reinforcement Learning (RL), regularization has emerged as a popular tool both in theory and practice, typically based either on an entropy bonus or a Kullback-Leibler divergence that constrains successive policies. In practice, these approaches have been shown to improve exploration, robustness and stability, giving rise to popular Deep RL algorithms such as SAC and TRPO. Policy Mirror Descent (PMD) is a theoretical framework that solves this general regularized policy optimization problem, however the closed-form solution involves the sum of all past Q-functions, which is intractable in practice. We propose and analyze PMD-like algorithms that only keep the last $M$ Q-functions in memory, and show that for finite and large enough $M$, a convergent algorithm can be derived, introducing no error in the policy update, unlike prior deep RL PMD implementations. StaQ, the resulting algorithm, enjoys strong theoretical guarantees and is competitive with deep RL baselines, while exhibiting less performance oscillation, paving the way for fully stable deep RL algorithms and providing a testbed for experimentation with Policy Mirror Descent.

</details>


### [191] [Scaling Algorithm Distillation for Continuous Control with Mamba](https://arxiv.org/abs/2506.13892)
*Samuel Beaussant,Mehdi Mounsif*

Main category: cs.LG

TL;DR: 论文提出使用S6模型（Mamba）替代传统Transformer，以解决算法蒸馏（AD）在长序列任务中的计算瓶颈，并在复杂连续环境中验证其优越性。


<details>
  <summary>Details</summary>
Motivation: 传统Transformer在算法蒸馏中因二次复杂度受限，无法处理长序列任务，因此需要更高效的模型。

Method: 采用S6模型（Mamba），其线性复杂度适用于长序列建模，并在四个复杂连续元强化学习环境中进行实验。

Result: Mamba在AD任务中表现优于Transformer，且长上下文扩展能提升ICRL性能，甚至媲美SOTA在线元RL基线。

Conclusion: S6模型（Mamba）是AD任务中更高效的选择，尤其适用于长序列和复杂环境。

Abstract: Algorithm Distillation (AD) was recently proposed as a new approach to perform In-Context Reinforcement Learning (ICRL) by modeling across-episodic training histories autoregressively with a causal transformer model. However, due to practical limitations induced by the attention mechanism, experiments were bottlenecked by the transformer's quadratic complexity and limited to simple discrete environments with short time horizons. In this work, we propose leveraging the recently proposed Selective Structured State Space Sequence (S6) models, which achieved state-of-the-art (SOTA) performance on long-range sequence modeling while scaling linearly in sequence length. Through four complex and continuous Meta Reinforcement Learning environments, we demonstrate the overall superiority of Mamba, a model built with S6 layers, over a transformer model for AD. Additionally, we show that scaling AD to very long contexts can improve ICRL performance and make it competitive even with a SOTA online meta RL baseline.

</details>


### [192] [Enhancing interpretability of rule-based classifiers through feature graphs](https://arxiv.org/abs/2506.13903)
*Christel Sirocchi,Damiano Verda*

Main category: cs.LG

TL;DR: 提出了一种用于规则基系统的特征贡献分析框架，包括可视化策略、特征重要性度量和规则集比较方法，实验验证了其在临床数据集上的有效性。


<details>
  <summary>Details</summary>
Motivation: 在需要透明度和可信度的领域（如医疗），规则基系统因其可解释性被广泛使用，但随着复杂度增加，理解特征贡献变得困难。

Method: 提出图基特征可视化策略、特征重要性度量和规则集距离度量，并在两个临床数据集和四种规则基方法上实验。

Result: 方法能揭示临床特征的预测价值，识别新风险因素和生物标志物，并在15个公共基准测试中表现优异。

Conclusion: 该框架为规则基系统提供了有效的特征分析工具，有助于提升诊断准确性。

Abstract: In domains where transparency and trustworthiness are crucial, such as healthcare, rule-based systems are widely used and often preferred over black-box models for decision support systems due to their inherent interpretability. However, as rule-based models grow complex, discerning crucial features, understanding their interactions, and comparing feature contributions across different rule sets becomes challenging. To address this, we propose a comprehensive framework for estimating feature contributions in rule-based systems, introducing a graph-based feature visualisation strategy, a novel feature importance metric agnostic to rule-based predictors, and a distance metric for comparing rule sets based on feature contributions. By experimenting on two clinical datasets and four rule-based methods (decision trees, logic learning machines, association rules, and neural networks with rule extraction), we showcase our method's capability to uncover novel insights on the combined predictive value of clinical features, both at the dataset and class-specific levels. These insights can aid in identifying new risk factors, signature genes, and potential biomarkers, and determining the subset of patient information that should be prioritised to enhance diagnostic accuracy. Comparative analysis of the proposed feature importance score with state-of-the-art methods on 15 public benchmarks demonstrates competitive performance and superior robustness. The method implementation is available on GitHub: https://github.com/ChristelSirocchi/rule-graph.

</details>


### [193] [GITO: Graph-Informed Transformer Operator for Learning Complex Partial Differential Equations](https://arxiv.org/abs/2506.13906)
*Milad Ramezankhani,Janak M. Patel,Anirudh Deodhar,Dagnachew Birru*

Main category: cs.LG

TL;DR: 提出了一种新型图信息Transformer算子（GITO）架构，用于学习不规则几何和非均匀网格上的复杂偏微分方程系统。


<details>
  <summary>Details</summary>
Motivation: 解决传统方法在处理不规则几何和非均匀网格时的局限性，提升复杂偏微分方程系统的学习能力。

Method: GITO由混合图Transformer（HGT）和Transformer神经算子（TNO）组成，结合图神经网络和Transformer的优势，通过自注意力融合层实现特征学习。

Result: 在基准PDE任务中表现优于现有基于Transformer的神经算子，支持零样本超分辨率和网格无关性。

Conclusion: GITO为工程应用提供了高效、网格无关的替代求解器，具有广泛的应用潜力。

Abstract: We present a novel graph-informed transformer operator (GITO) architecture for learning complex partial differential equation systems defined on irregular geometries and non-uniform meshes. GITO consists of two main modules: a hybrid graph transformer (HGT) and a transformer neural operator (TNO). HGT leverages a graph neural network (GNN) to encode local spatial relationships and a transformer to capture long-range dependencies. A self-attention fusion layer integrates the outputs of the GNN and transformer to enable more expressive feature learning on graph-structured data. TNO module employs linear-complexity cross-attention and self-attention layers to map encoded input functions to predictions at arbitrary query locations, ensuring discretization invariance and enabling zero-shot super-resolution across any mesh. Empirical results on benchmark PDE tasks demonstrate that GITO outperforms existing transformer-based neural operators, paving the way for efficient, mesh-agnostic surrogate solvers in engineering applications.

</details>


### [194] [Few-Shot Learning for Industrial Time Series: A Comparative Analysis Using the Example of Screw-Fastening Process Monitoring](https://arxiv.org/abs/2506.13909)
*Xinyuan Tu,Haocheng Zhang,Tao Chengxu,Zuyi Chen*

Main category: cs.LG

TL;DR: 该论文研究了工业时间序列数据中的少样本学习（FSL），提出了一种标签感知的采样方法，并比较了不同FSL范式和骨干网络的性能。


<details>
  <summary>Details</summary>
Motivation: 工业时间序列数据标注成本高，少样本学习在视觉领域有潜力但尚未在工业领域充分探索。

Method: 使用标签感知的采样方法，将多标签序列分解为单标签任务，并比较了Prototypical Network和MAML两种FSL范式，结合三种骨干网络（1D CNN、InceptionTime、Moment）。

Result: InceptionTime + Prototypical Network组合在10-shot、3-way评估中表现最佳，F1分数达0.944（多类）和0.935（多标签），优于Moment模型。

Conclusion: 轻量级CNN架构结合简单度量学习在数据稀缺时表现优于大型基础模型，且收敛更快、泛化能力更强。

Abstract: Few-shot learning (FSL) has shown promise in vision but remains largely unexplored for \emph{industrial} time-series data, where annotating every new defect is prohibitively expensive. We present a systematic FSL study on screw-fastening process monitoring, using a 2\,300-sample multivariate torque dataset that covers 16 uni- and multi-factorial defect types. Beyond benchmarking, we introduce a \textbf{label-aware episodic sampler} that collapses multi-label sequences into multiple single-label tasks, keeping the output dimensionality fixed while preserving combinatorial label information.
  Two FSL paradigms are investigated: the metric-based \emph{Prototypical Network} and the gradient-based \emph{Model-Agnostic Meta-Learning} (MAML), each paired with three backbones: 1D CNN, InceptionTime and the 341 M-parameter transformer \emph{Moment}. On 10-shot, 3-way evaluation, the InceptionTime + Prototypical Network combination achieves a \textbf{0.944 weighted F1} in the multi-class regime and \textbf{0.935} in the multi-label regime, outperforming finetuned Moment by up to 5.3\% while requiring two orders of magnitude fewer parameters and training time. Across all backbones, metric learning consistently surpasses MAML, and our label-aware sampling yields an additional 1.7\% F1 over traditional class-based sampling.
  These findings challenge the assumption that large foundation models are always superior: when data are scarce, lightweight CNN architectures augmented with simple metric learning not only converge faster but also generalize better. We release code, data splits and pre-trained weights to foster reproducible research and to catalyze the adoption of FSL in high-value manufacturing inspection.

</details>


### [195] [Logical Expressiveness of Graph Neural Networks with Hierarchical Node Individualization](https://arxiv.org/abs/2506.13911)
*Arie Soeteman,Balder ten Cate*

Main category: cs.LG

TL;DR: HEGNNs是一种基于层次化节点个性化的图神经网络扩展，能够区分图同构，并在实验中表现出优于传统GNN的性能。


<details>
  <summary>Details</summary>
Motivation: 受图同构测试中的个性化-细化范式启发，旨在提升图神经网络的表达能力。

Method: 提出HEGNNs，结合层次化节点个性化，并利用分级混合逻辑进行逻辑表征。

Result: HEGNNs在实验中表现出优于传统GNN的性能，并能区分图同构。

Conclusion: HEGNNs是一种有潜力的扩展模型，能够提升图神经网络的表达能力。

Abstract: We propose and study Hierarchical Ego Graph Neural Networks (HEGNNs), an expressive extension of graph neural networks (GNNs) with hierarchical node individualization, inspired by the Individualization-Refinement paradigm for graph isomorphism testing. HEGNNs generalize subgraph-GNNs and form a hierarchy of increasingly expressive models that, in the limit, can distinguish graphs up to isomorphism. We provide a logical characterization of HEGNN node classifiers, with and without subgraph restrictions, using graded hybrid logic. This characterization enables us to relate the separating power of HEGNNs to that of higher-order GNNs, GNNs enriched with local homomorphism count features, and color refinement algorithms based on Individualization-Refinement. Our experimental results confirm the practical feasibility of HEGNNs and show benefits in comparison with traditional GNN architectures, both with and without local homomorphism count features.

</details>


### [196] [Branching Stein Variational Gradient Descent for sampling multimodal distributions](https://arxiv.org/abs/2506.13916)
*Isaias Banales,Arturo Jaramillo,Heli Ricalde Guerrero*

Main category: cs.LG

TL;DR: 提出了一种基于粒子的变分推断方法BSVGD，通过随机分支机制探索多模态分布，理论证明其分布收敛性，并通过实验验证其优于传统SVGD。


<details>
  <summary>Details</summary>
Motivation: 传统SVGD在处理多模态分布时表现不足，需要一种能更好探索状态空间的方法。

Method: 扩展SVGD算法，引入随机分支机制（BSVGD），理论分析其分布收敛性，并通过数值实验比较性能。

Result: BSVGD在多模态分布中表现优于SVGD，Wasserstein距离更小且计算时间更优。

Conclusion: BSVGD是一种有效的多模态分布变分推断方法，具有理论和实验优势。

Abstract: We propose a novel particle-based variational inference method designed to work with multimodal distributions. Our approach, referred to as Branched Stein Variational Gradient Descent (BSVGD), extends the classical Stein Variational Gradient Descent (SVGD) algorithm by incorporating a random branching mechanism that encourages the exploration of the state space. In this work, a theoretical guarantee for the convergence in distribution is presented, as well as numerical experiments to validate the suitability of our algorithm. Performance comparisons between the BSVGD and the SVGD are presented using the Wasserstein distance between samples and the corresponding computational times.

</details>


### [197] [Adaptive Guidance Accelerates Reinforcement Learning of Reasoning Models](https://arxiv.org/abs/2506.13923)
*Vaskar Nath,Elaine Lau,Anisha Gunjal,Manasi Sharma,Nikhil Baharte,Sean Hendryx*

Main category: cs.LG

TL;DR: 论文研究了通过强化学习训练推理模型（RLVR）解决新问题的过程，发现RLVR通过压缩pass@k到pass@1和“能力增益”提升性能。新问题的解决主要依赖自蒸馏，且在不同规模模型中均存在能力增益。论文提出新算法Guide，通过自适应引入提示优化策略，实验显示其在数学基准上表现优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 研究如何通过强化学习训练推理模型解决新问题，并探索其性能提升的机制。

Method: 使用RLVR训练模型，分析其通过压缩pass@k和“能力增益”提升性能的机制；提出新算法Guide，自适应引入提示并优化策略。

Result: 实验表明，Guide在7B和32B参数模型上优于传统方法，数学基准上提升达4%。

Conclusion: RLVR通过压缩和能力增益提升性能，新算法Guide在优化策略和泛化能力上表现优异。

Abstract: We study the process through which reasoning models trained with reinforcement learning on verifiable rewards (RLVR) can learn to solve new problems. We find that RLVR drives performance through two main means: (1) by compressing pass@$k$ into pass@1 and (2) via "capability gain" in which models learn to solve new problems that they previously could not solve even at high $k$. We find that while capability gain exists across model scales, learning to solve new problems is primarily driven through self-distillation. We demonstrate these findings across model scales ranging from 0.5B to 72B on >500,000 reasoning problems with prompts and verifiable final answers across math, science, and code domains. We further show that we can significantly improve pass@$k$ rates by leveraging natural language guidance for the model to consider within context while still requiring the model to derive a solution chain from scratch. Based of these insights, we derive $\text{Guide}$ - a new class of online training algorithms. $\text{Guide}$ adaptively incorporates hints into the model's context on problems for which all rollouts were initially incorrect and adjusts the importance sampling ratio for the "off-policy" trajectories in order to optimize the policy for contexts in which the hints are no longer present. We describe variants of $\text{Guide}$ for GRPO and PPO and empirically show that Guide-GRPO on 7B and 32B parameter models improves generalization over its vanilla counterpart with up to 4$\%$ macro-average improvement across math benchmarks. We include careful ablations to analyze $\text{Guide}$'s components and theoretically analyze Guide's learning efficiency.

</details>


### [198] [ReinDSplit: Reinforced Dynamic Split Learning for Pest Recognition in Precision Agriculture](https://arxiv.org/abs/2506.13935)
*Vishesh Kumar Tanwar,Soumik Sarkar,Asheesh K. Singh,Sajal K. Das*

Main category: cs.LG

TL;DR: ReinDSplit是一种基于强化学习的框架，动态调整DNN分割点，优化农业边缘设备的计算效率和模型性能。


<details>
  <summary>Details</summary>
Motivation: 传统SL框架在农业生态系统中无法适应设备的异构性，导致效率低下和性能下降。

Method: 使用Q-learning代理动态调整DNN分割点，平衡工作负载和延迟阈值。

Result: 在三个昆虫分类数据集上，ReinDSplit使用MobileNetV2达到94.31%的准确率。

Conclusion: ReinDSplit通过强化学习优化资源效率、隐私和可扩展性，为SL领域带来新范式。

Abstract: To empower precision agriculture through distributed machine learning (DML), split learning (SL) has emerged as a promising paradigm, partitioning deep neural networks (DNNs) between edge devices and servers to reduce computational burdens and preserve data privacy. However, conventional SL frameworks' one-split-fits-all strategy is a critical limitation in agricultural ecosystems where edge insect monitoring devices exhibit vast heterogeneity in computational power, energy constraints, and connectivity. This leads to straggler bottlenecks, inefficient resource utilization, and compromised model performance. Bridging this gap, we introduce ReinDSplit, a novel reinforcement learning (RL)-driven framework that dynamically tailors DNN split points for each device, optimizing efficiency without sacrificing accuracy. Specifically, a Q-learning agent acts as an adaptive orchestrator, balancing workloads and latency thresholds across devices to mitigate computational starvation or overload. By framing split layer selection as a finite-state Markov decision process, ReinDSplit convergence ensures that highly constrained devices contribute meaningfully to model training over time. Evaluated on three insect classification datasets using ResNet18, GoogleNet, and MobileNetV2, ReinDSplit achieves 94.31% accuracy with MobileNetV2. Beyond agriculture, ReinDSplit pioneers a paradigm shift in SL by harmonizing RL for resource efficiency, privacy, and scalability in heterogeneous environments.

</details>


### [199] [Toward Explainable Offline RL: Analyzing Representations in Intrinsically Motivated Decision Transformers](https://arxiv.org/abs/2506.13958)
*Leonardo Guiducci,Antonio Rizzo,Giovanna Maria Dimitri*

Main category: cs.LG

TL;DR: 本文提出了一种系统性的事后解释框架，分析内在动机如何影响弹性决策变换器（EDTs）中的嵌入表示，揭示了不同内在动机变体如何塑造不同的表示结构。


<details>
  <summary>Details</summary>
Motivation: 研究内在动机在EDTs中如何通过改变嵌入表示来提升性能，填补了相关机制的研究空白。

Method: 通过统计分析方法（如协方差结构、向量大小和正交性）分析嵌入特性，研究内在动机对表示结构的影响。

Result: 发现不同内在动机变体会形成截然不同的表示结构，且嵌入指标与性能之间存在环境特定的相关性模式。

Conclusion: 内在动机不仅是探索奖励，还作为一种表示先验，以生物合理的方式塑造嵌入几何结构，从而优化决策。

Abstract: Elastic Decision Transformers (EDTs) have proved to be particularly successful in offline reinforcement learning, offering a flexible framework that unifies sequence modeling with decision-making under uncertainty. Recent research has shown that incorporating intrinsic motivation mechanisms into EDTs improves performance across exploration tasks, yet the representational mechanisms underlying these improvements remain unexplored. In this paper, we introduce a systematic post-hoc explainability framework to analyze how intrinsic motivation shapes learned embeddings in EDTs. Through statistical analysis of embedding properties (including covariance structure, vector magnitudes, and orthogonality), we reveal that different intrinsic motivation variants create fundamentally different representational structures. Our analysis demonstrates environment-specific correlation patterns between embedding metrics and performance that explain why intrinsic motivation improves policy learning. These findings show that intrinsic motivation operates beyond simple exploration bonuses, acting as a representational prior that shapes embedding geometry in biologically plausible ways, creating environment-specific organizational structures that facilitate better decision-making.

</details>


### [200] [Membership Inference Attacks as Privacy Tools: Reliability, Disparity and Ensemble](https://arxiv.org/abs/2506.13972)
*Zhiqi Wang,Chengyu Zhang,Yuetian Chen,Nathalie Baracaldo,Swanand Kadhe,Lei Yu*

Main category: cs.LG

TL;DR: 论文研究了成员推理攻击（MIAs）中的差异问题，提出了基于覆盖率和稳定性分析的新框架，并通过实验揭示了差异的原因及影响。最后提出了一个集成框架以提升隐私评估的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有MIAs研究主要关注性能指标（如AUC、准确率等），而忽视了不同攻击方法之间的差异，这些差异对隐私评估的可靠性和完整性至关重要。

Method: 通过基于覆盖率和稳定性分析的新框架，系统研究了MIAs中的差异，并提出了集成框架以结合多种攻击方法的优势。

Result: 实验揭示了MIAs中显著的差异及其潜在原因，表明现有评估方法可能不够全面。

Conclusion: 提出的集成框架不仅能构建更强大的攻击，还为隐私评估提供了更鲁棒和全面的方法。

Abstract: Membership inference attacks (MIAs) pose a significant threat to the privacy of machine learning models and are widely used as tools for privacy assessment, auditing, and machine unlearning. While prior MIA research has primarily focused on performance metrics such as AUC, accuracy, and TPR@low FPR - either by developing new methods to enhance these metrics or using them to evaluate privacy solutions - we found that it overlooks the disparities among different attacks. These disparities, both between distinct attack methods and between multiple instantiations of the same method, have crucial implications for the reliability and completeness of MIAs as privacy evaluation tools. In this paper, we systematically investigate these disparities through a novel framework based on coverage and stability analysis. Extensive experiments reveal significant disparities in MIAs, their potential causes, and their broader implications for privacy evaluation. To address these challenges, we propose an ensemble framework with three distinct strategies to harness the strengths of state-of-the-art MIAs while accounting for their disparities. This framework not only enables the construction of more powerful attacks but also provides a more robust and comprehensive methodology for privacy evaluation.

</details>


### [201] [Constant Stepsize Local GD for Logistic Regression: Acceleration by Instability](https://arxiv.org/abs/2506.13974)
*Michael Crawshaw,Blake Woodworth,Mingrui Liu*

Main category: cs.LG

TL;DR: 本文分析了局部梯度下降在逻辑回归中的表现，放宽了步长限制，并展示了在初始不稳定阶段后的收敛速度。


<details>
  <summary>Details</summary>
Motivation: 现有研究对异构目标的局部梯度下降分析要求步长η≤1/K，限制了其灵活性。本文旨在放宽这一限制，研究任意步长η>0下的性能。

Method: 针对逻辑回归和可分离的异构数据，分析局部梯度下降的收敛行为，考虑通信轮数R和客户端数M的影响。

Result: 在初始不稳定阶段（持续约ηKM轮）后，收敛速度为O(1/ηKR)，优于现有的一般光滑凸目标的O(1/R)速率。

Conclusion: 本文揭示了异构目标和局部更新是导致不稳定的新因素，同时展示了放宽步长限制的可行性。

Abstract: Existing analysis of Local (Stochastic) Gradient Descent for heterogeneous objectives requires stepsizes $η\leq 1/K$ where $K$ is the communication interval, which ensures monotonic decrease of the objective. In contrast, we analyze Local Gradient Descent for logistic regression with separable, heterogeneous data using any stepsize $η> 0$. With $R$ communication rounds and $M$ clients, we show convergence at a rate $\mathcal{O}(1/ηK R)$ after an initial unstable phase lasting for $\widetilde{\mathcal{O}}(ηK M)$ rounds. This improves upon the existing $\mathcal{O}(1/R)$ rate for general smooth, convex objectives. Our analysis parallels the single machine analysis of~\cite{wu2024large} in which instability is caused by extremely large stepsizes, but in our setting another source of instability is large local updates with heterogeneous objectives.

</details>


### [202] [HAELT: A Hybrid Attentive Ensemble Learning Transformer Framework for High-Frequency Stock Price Forecasting](https://arxiv.org/abs/2506.13981)
*Thanh Dan Bui*

Main category: cs.LG

TL;DR: 提出了一种名为HAELT的深度学习框架，用于高频股票价格预测，结合了ResNet、自注意力和LSTM-Transformer模块，并在AAPL数据上表现优异。


<details>
  <summary>Details</summary>
Motivation: 高频股票价格预测面临非平稳性、噪声和波动性等挑战，需要一种鲁棒的解决方案。

Method: HAELT框架结合了ResNet噪声抑制模块、时间自注意力机制和混合LSTM-Transformer核心，并通过自适应集成优化性能。

Result: 在AAPL数据测试中，HAELT取得了最高的F1分数，能有效识别价格涨跌。

Conclusion: HAELT展示了在金融预测和算法交易中的实用性和鲁棒性。

Abstract: High-frequency stock price prediction is challenging due to non-stationarity, noise, and volatility. To tackle these issues, we propose the Hybrid Attentive Ensemble Learning Transformer (HAELT), a deep learning framework combining a ResNet-based noise-mitigation module, temporal self-attention for dynamic focus on relevant history, and a hybrid LSTM-Transformer core that captures both local and long-range dependencies. These components are adaptively ensembled based on recent performance. Evaluated on hourly Apple Inc. (AAPL) data from Jan 2024 to May 2025, HAELT achieves the highest F1-Score on the test set, effectively identifying both upward and downward price movements. This demonstrates HAELT's potential for robust, practical financial forecasting and algorithmic trading.

</details>


### [203] [Quantum-Informed Contrastive Learning with Dynamic Mixup Augmentation for Class-Imbalanced Expert Systems](https://arxiv.org/abs/2506.13987)
*Md Abrar Jahin,Adiba Abid,M. F. Mridha*

Main category: cs.LG

TL;DR: QCL-MixNet是一种基于量子启发对比学习和动态混合的新框架，用于解决类别不平衡问题，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 专家系统常面临类别不平衡的表格数据，现有方法存在过拟合、标签噪声和泛化能力差等问题。

Method: QCL-MixNet结合量子纠缠层、动态混合策略和混合损失函数，提升少数类表示和分类性能。

Result: 在18个真实数据集上，QCL-MixNet在macro-F1和召回率上显著优于20种现有方法。

Conclusion: QCL-MixNet为表格数据不平衡处理设定了新基准，理论分析支持其表达能力和鲁棒性。

Abstract: Expert systems often operate in domains characterized by class-imbalanced tabular data, where detecting rare but critical instances is essential for safety and reliability. While conventional approaches, such as cost-sensitive learning, oversampling, and graph neural networks, provide partial solutions, they suffer from drawbacks like overfitting, label noise, and poor generalization in low-density regions. To address these challenges, we propose QCL-MixNet, a novel Quantum-Informed Contrastive Learning framework augmented with k-nearest neighbor (kNN) guided dynamic mixup for robust classification under imbalance. QCL-MixNet integrates three core innovations: (i) a Quantum Entanglement-inspired layer that models complex feature interactions through sinusoidal transformations and gated attention, (ii) a sample-aware mixup strategy that adaptively interpolates feature representations of semantically similar instances to enhance minority class representation, and (iii) a hybrid loss function that unifies focal reweighting, supervised contrastive learning, triplet margin loss, and variance regularization to improve both intra-class compactness and inter-class separability. Extensive experiments on 18 real-world imbalanced datasets (binary and multi-class) demonstrate that QCL-MixNet consistently outperforms 20 state-of-the-art machine learning, deep learning, and GNN-based baselines in macro-F1 and recall, often by substantial margins. Ablation studies further validate the critical role of each architectural component. Our results establish QCL-MixNet as a new benchmark for tabular imbalance handling in expert systems. Theoretical analyses reinforce its expressiveness, generalization, and optimization robustness.

</details>


### [204] [AssistedDS: Benchmarking How External Domain Knowledge Assists LLMs in Automated Data Science](https://arxiv.org/abs/2506.13992)
*An Luo,Xun Xian,Jin Du,Fangqiao Tian,Ganghua Wang,Ming Zhong,Shengchun Zhao,Xuan Bi,Zirui Liu,Jiawei Zhou,Jayanth Srinivasa,Ashish Kundu,Charles Fleming,Mingyi Hong,Jie Ding*

Main category: cs.LG

TL;DR: AssistedDS评估LLMs在表格预测任务中处理领域知识的能力，发现其存在对信息不加批判采纳、难以抵消对抗性信息影响等问题。


<details>
  <summary>Details</summary>
Motivation: 研究LLMs是否能像人类数据科学家一样批判性地利用外部领域知识。

Method: 引入AssistedDS基准，结合合成数据集和真实Kaggle竞赛数据，评估LLMs对领域知识的处理能力。

Result: LLMs常不加批判地采纳信息，对抗性内容显著降低预测性能，且难以抵消其影响；在时间序列和分类变量处理上易出错。

Conclusion: 当前模型在批判性评估和利用专家知识方面存在显著不足，需开发更鲁棒的知识感知系统。

Abstract: Large language models (LLMs) have advanced the automation of data science workflows. Yet it remains unclear whether they can critically leverage external domain knowledge as human data scientists do in practice. To answer this question, we introduce AssistedDS (Assisted Data Science), a benchmark designed to systematically evaluate how LLMs handle domain knowledge in tabular prediction tasks. AssistedDS features both synthetic datasets with explicitly known generative mechanisms and real-world Kaggle competitions, each accompanied by curated bundles of helpful and adversarial documents. These documents provide domain-specific insights into data cleaning, feature engineering, and model selection. We assess state-of-the-art LLMs on their ability to discern and apply beneficial versus harmful domain knowledge, evaluating submission validity, information recall, and predictive performance. Our results demonstrate three key findings: (1) LLMs frequently exhibit an uncritical adoption of provided information, significantly impairing their predictive performance when adversarial content is introduced, (2) helpful guidance is often insufficient to counteract the negative influence of adversarial information, and (3) in Kaggle datasets, LLMs often make errors in handling time-series data, applying consistent feature engineering across different folds, and interpreting categorical variables correctly. These findings highlight a substantial gap in current models' ability to critically evaluate and leverage expert knowledge, underscoring an essential research direction for developing more robust, knowledge-aware automated data science systems.

</details>


### [205] [Arctic Long Sequence Training: Scalable And Efficient Training For Multi-Million Token Sequences](https://arxiv.org/abs/2506.13996)
*Stas Bekman,Samyam Rajbhandari,Michael Wyatt,Jeff Rasley,Tunji Ruwase,Zhewei Yao,Aurick Qiao,Yuxiong He*

Main category: cs.LG

TL;DR: ALST（Arctic Long Sequence Training）通过单GPU和多GPU内存优化，支持Hugging Face模型的长序列训练，显著提升序列长度支持能力。


<details>
  <summary>Details</summary>
Motivation: 长序列训练在开源社区中因系统支持不足而具有挑战性，现有解决方案难以直接应用于Hugging Face模型。

Method: ALST结合注意力无关的单GPU和多GPU内存优化技术，支持多种Hugging Face模型的长序列训练。

Result: ALST在单H100 GPU上支持500K序列长度，8xH100节点支持3.7M，4节点集群支持15M，相比32K基线提升400倍。

Conclusion: ALST为开源社区提供了高效的长序列训练解决方案，兼容Hugging Face模型并已开源。

Abstract: Long sequences are critical for applications like RAG, long document summarization, multi-modality, etc., and modern LLMs, like Llama 4 Scout, support max sequence length of up to 10 million tokens. However, outside of enterprise labs, long sequence training is challenging for the AI community with limited system support in the open-source space.
  Out-of-box, even on a modern NVIDIA H100 80GB GPU cluster, training Llama 8B model with sequence over 32K runs out of memory on a basic Hugging Face (HF) model due to two reasons: i) LLM training workloads are not optimized to fully leverage a single GPU memory, ii) existing solutions for leveraging multiple GPU memory are not easily available to HF models, making long sequence training inaccessible.
  We address this with Arctic Long Sequence Training (ALST). It offers a combination of attention-agnostic single GPU and multi-GPU memory optimizations, that enables it to support out-of-box training of multi-million sequence length for a wide variety of HF models.
  ALST supports training Meta's Llama 8B model with 500K sequence length on a single H100 GPU, 3.7M on a single 8xH100 GPU node, and over 15M on a 4 node cluster, an increase of over 400x compared to the 32K baseline for the latter. ALST is fully compatible with HF models and open-sourced via Deepspeed https://www.deepspeed.ai/tutorials/ulysses-alst-sequence-pallellism/ and Arctic Training https://github.com/snowflakedb/ArcticTraining/blob/main/projects/sequence-parallelism/README.md.

</details>


### [206] [Taming Polysemanticity in LLMs: Provable Feature Recovery via Sparse Autoencoders](https://arxiv.org/abs/2506.14002)
*Siyu Chen,Heejune Sheen,Xuyuan Xiong,Tianhao Wang,Zhuoran Yang*

Main category: cs.LG

TL;DR: 提出了一种基于统计框架的稀疏自编码器（SAE）训练算法，通过偏置适应技术改进特征恢复，并在理论和实践中验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 解决现有稀疏自编码器训练算法缺乏理论保证和实际局限（如超参数敏感性和不稳定性）的问题。

Method: 提出统计框架和新的特征可识别性概念，设计基于偏置适应的SAE训练算法，并开发改进的Group Bias Adaptation（GBA）方法。

Result: 理论证明算法在特定条件下能正确恢复单义特征，实验显示GBA在15亿参数大语言模型上优于基准方法。

Conclusion: 该研究为SAE训练提供了首个理论保证，推动了AI系统的透明性和可解释性发展。

Abstract: We study the challenge of achieving theoretically grounded feature recovery using Sparse Autoencoders (SAEs) for the interpretation of Large Language Models. Existing SAE training algorithms often lack rigorous mathematical guarantees and suffer from practical limitations such as hyperparameter sensitivity and instability. To address these issues, we first propose a novel statistical framework for the feature recovery problem, which includes a new notion of feature identifiability by modeling polysemantic features as sparse mixtures of underlying monosemantic concepts. Building on this framework, we introduce a new SAE training algorithm based on ``bias adaptation'', a technique that adaptively adjusts neural network bias parameters to ensure appropriate activation sparsity. We theoretically \highlight{prove that this algorithm correctly recovers all monosemantic features} when input data is sampled from our proposed statistical model. Furthermore, we develop an improved empirical variant, Group Bias Adaptation (GBA), and \highlight{demonstrate its superior performance against benchmark methods when applied to LLMs with up to 1.5 billion parameters}. This work represents a foundational step in demystifying SAE training by providing the first SAE algorithm with theoretical recovery guarantees, thereby advancing the development of more transparent and trustworthy AI systems through enhanced mechanistic interpretability.

</details>


### [207] [Unlearning Isn't Invisible: Detecting Unlearning Traces in LLMs from Model Outputs](https://arxiv.org/abs/2506.14003)
*Yiwei Chen,Soumyadeep Pal,Yimeng Zhang,Qing Qu,Sijia Liu*

Main category: cs.LG

TL;DR: 该论文研究了大型语言模型（LLM）在机器遗忘（MU）后留下的可检测痕迹，揭示了遗忘行为在模型输出和内部表示中的持久性。


<details>
  <summary>Details</summary>
Motivation: 机器遗忘在保护数据隐私、版权执行和减少社会技术危害方面至关重要，但作者发现遗忘后模型会留下可检测的痕迹，这可能带来新的隐私风险。

Method: 通过监督分类器分析模型输出和内部激活，识别遗忘痕迹，并研究其在激活空间中的低维可学习流形。

Result: 实验表明，遗忘相关提示的检测准确率超过90%，即使无关输入下，大型LLM的痕迹仍高度可检测。

Conclusion: 遗忘行为会留下可测量的痕迹，可能被逆向工程还原遗忘信息，揭示了新的隐私风险。

Abstract: Machine unlearning (MU) for large language models (LLMs), commonly referred to as LLM unlearning, seeks to remove specific undesirable data or knowledge from a trained model, while maintaining its performance on standard tasks. While unlearning plays a vital role in protecting data privacy, enforcing copyright, and mitigating sociotechnical harms in LLMs, we identify a new vulnerability post-unlearning: unlearning trace detection. We discover that unlearning leaves behind persistent ''fingerprints'' in LLMs, detectable traces in both model behavior and internal representations. These traces can be identified from output responses, even when prompted with forget-irrelevant inputs. Specifically, a simple supervised classifier can reliably determine whether a model has undergone unlearning based solely on its textual outputs. Further analysis shows that these traces are embedded in intermediate activations and propagate nonlinearly to the final layer, forming low-dimensional, learnable manifolds in activation space. Through extensive experiments, we show that forget-relevant prompts enable over 90% accuracy in detecting unlearning traces across all model sizes. Even with forget-irrelevant inputs, large LLMs maintain high detectability, demonstrating the broad applicability of unlearning trace detection. These findings reveal that unlearning leaves measurable signatures, introducing a new risk of reverse-engineering forgotten information when a model is identified as unlearned given an input query. Codes are available at [this URL](https://github.com/OPTML-Group/Unlearn-Trace).

</details>


### [208] [Bures-Wasserstein Flow Matching for Graph Generation](https://arxiv.org/abs/2506.14020)
*Keyue Jiang,Jiahao Cui,Xiaowen Dong,Laura Toni*

Main category: cs.LG

TL;DR: 论文提出了一种基于马尔可夫随机场（MRF）和最优传输的图生成方法BWFlow，解决了现有方法在非欧几里得图结构中的不足。


<details>
  <summary>Details</summary>
Motivation: 现有图生成方法（如扩散和基于流的模型）假设数据位于欧几里得空间，独立建模节点和边的演化，忽略了图的非欧几里得结构和互联模式，导致采样收敛性风险。

Method: 通过将图表示为MRF参数化的连接系统，利用MRF对象之间的最优传输位移设计概率路径，提出了BWFlow框架，支持连续和离散流匹配算法。

Result: 实验表明，BWFlow在普通图生成和2D/3D分子生成中表现优异，具有稳定的训练和保证的采样收敛性。

Conclusion: BWFlow通过尊重图的几何结构，提供了一种更优的图生成方法。

Abstract: Graph generation has emerged as a critical task in fields ranging from molecule design to drug discovery. Contemporary approaches, notably diffusion and flow-based models, have achieved solid graph generative performance through constructing a probability path that interpolates between a reference distribution and the data distribution. However, these methods typically model the evolution of individual nodes and edges independently and use linear interpolations to build the path assuming that the data lie in Euclidean space. We show that this is suboptimal given the intrinsic non-Euclidean structure and interconnected patterns of graphs, and it poses risks to the sampling convergence. To build a better probability path, we model the joint evolution of the nodes and edges by representing graphs as connected systems parameterized by Markov random fields (MRF). We then leverage the optimal transport displacement between MRF objects to design the probability path for graph generation. Based on this, we introduce BWFlow, a flow-matching framework for graph generation that respects the underlying geometry of graphs and provides smooth velocities in the probability path. The novel framework can be adapted to both continuous and discrete flow-matching algorithms. Experimental evaluations in plain graph generation and 2D/3D molecule generation validate the effectiveness of BWFlow in graph generation with competitive performance, stable training, and guaranteed sampling convergence.

</details>


### [209] [Robust Physics-Informed Neural Network Approach for Estimating Heterogeneous Elastic Properties from Noisy Displacement Data](https://arxiv.org/abs/2506.14036)
*Tatthapong Srikitrungruang,Sina Aghaee Dabaghan Fard,Matthew Lemon,Jaesung Lee,Yuxiao Zhou*

Main category: cs.LG

TL;DR: 本文提出了一种新型逆弹性物理信息神经网络（IE-PINN），用于从噪声位移数据中稳健地重建弹性参数分布，克服了现有方法的不稳定性和对噪声的敏感性。


<details>
  <summary>Details</summary>
Motivation: 现有逆弹性参数估计方法存在不稳定、对噪声敏感及难以恢复绝对尺度弹性模量的问题，限制了其实际应用。

Method: IE-PINN采用三种神经网络分别建模位移场、应变场和弹性分布，并引入两阶段估计策略、位置编码、正弦激活函数和预训练协议。

Result: 实验表明，IE-PINN在严重噪声条件下仍能准确估计绝对尺度弹性参数，显著优于现有方法。

Conclusion: IE-PINN在临床成像诊断和机械表征等领域具有重要应用潜力。

Abstract: Accurately estimating spatially heterogeneous elasticity parameters, particularly Young's modulus and Poisson's ratio, from noisy displacement measurements remains significantly challenging in inverse elasticity problems. Existing inverse estimation techniques are often limited by instability, pronounced sensitivity to measurement noise, and difficulty in recovering absolute-scale Young's modulus. This work presents a novel Inverse Elasticity Physics-Informed Neural Network (IE-PINN) specifically designed to robustly reconstruct heterogeneous distributions of elasticity parameters from noisy displacement data based on linear elasticity physics. IE-PINN integrates three distinct neural network architectures dedicated to separately modeling displacement fields, strain fields, and elasticity distributions, thereby significantly enhancing stability and accuracy against measurement noise. Additionally, a two-phase estimation strategy is introduced: the first phase recovers relative spatial distributions of Young's modulus and Poisson's ratio, and the second phase calibrates the absolute scale of Young's modulus using imposed loading boundary conditions. Additional methodological innovations, including positional encoding, sine activation functions, and a sequential pretraining protocol, further enhance the model's performance and robustness. Extensive numerical experiments demonstrate that IE-PINN effectively overcomes critical limitations encountered by existing methods, delivering accurate absolute-scale elasticity estimations even under severe noise conditions. This advancement holds substantial potential for clinical imaging diagnostics and mechanical characterization, where measurements typically encounter substantial noise.

</details>


### [210] [Load Balancing Mixture of Experts with Similarity Preserving Routers](https://arxiv.org/abs/2506.14038)
*Nabil Omi,Siddhartha Sen,Ali Farhadi*

Main category: cs.LG

TL;DR: 本文提出了一种新的负载均衡损失函数，用于稀疏混合专家（MoE）模型，以解决路由器在训练中不一致的问题，从而提升模型性能和收敛速度。


<details>
  <summary>Details</summary>
Motivation: 现有负载均衡机制通常鼓励专家分布的均匀性，但可能导致路由行为不一致，模型学习冗余知识。

Method: 引入一种新的负载均衡损失函数，保持令牌间的关系结构，鼓励相似输入在训练中选择一致的专家。

Result: 实验表明，该损失函数使收敛速度提升36%，并减少冗余。

Conclusion: 新方法有效改善了MoE模型的训练效率和性能。

Abstract: Sparse Mixture of Experts (MoE) models offer a scalable and efficient architecture for training large neural networks by activating only a subset of parameters ("experts") for each input. A learned router computes a distribution over these experts, and assigns input tokens to a small subset. However, without auxiliary balancing mechanisms, routers often converge to using only a few experts, severely limiting model capacity and degrading performance. Most current load balancing mechanisms encourage a distribution over experts that resembles a roughly uniform distribution of experts per token. During training, this can result in inconsistent routing behavior, resulting in the model spending its capacity to learn redundant knowledge. We address this by introducing a novel load balancing loss that preserves token-wise relational structure, encouraging consistent expert choices for similar inputs during training. Our experimental results show that applying our loss to the router results in 36% faster convergence and lower redundancy compared to a popular load balancing loss.

</details>


### [211] [Scientifically-Interpretable Reasoning Network (ScIReN): Uncovering the Black-Box of Nature](https://arxiv.org/abs/2506.14054)
*Joshua Fan,Haodi Xu,Feng Tao,Md Nasim,Marc Grimson,Yiqi Luo,Carla P. Gomes*

Main category: cs.LG

TL;DR: ScIReN是一个结合可解释神经网络和基于过程的模型的透明框架，通过预测科学意义的潜在参数并利用科学先验知识增强可解释性，在预测准确性和科学发现上优于黑盒模型。


<details>
  <summary>Details</summary>
Motivation: 解决神经网络缺乏科学解释性和基于过程模型参数设置随意的问题，结合两者的优势以实现科学发现和预测准确性。

Method: 提出ScIReN框架，包括可解释编码器和可微分基于过程解码器，使用硬Sigmoid约束层限制潜在参数范围。

Result: 在土壤有机碳流动和生态系统呼吸建模任务中，ScIReN在预测准确性上优于黑盒模型，并能推断潜在科学机制。

Conclusion: ScIReN成功结合了科学解释性和预测能力，为科学发现提供了新工具。

Abstract: Neural networks are a powerful tool for learning patterns from data. However, they do not respect known scientific laws, nor can they reveal novel scientific insights due to their black-box nature. In contrast, scientific reasoning distills biological or physical principles from observations and controlled experiments, and quantitatively interprets them with process-based models made of mathematical equations. Yet, process-based models rely on numerous free parameters that must be set in an ad-hoc manner, and thus often fit observations poorly in cross-scale predictions. While prior work has embedded process-based models in conventional neural networks, discovering interpretable relationships between parameters in process-based models and input features is still a grand challenge for scientific discovery. We thus propose Scientifically-Interpretable Reasoning Network (ScIReN), a fully-transparent framework that combines interpretable neural and process-based reasoning. An interpretable encoder predicts scientifically-meaningful latent parameters, which are then passed through a differentiable process-based decoder to predict labeled output variables. ScIReN also uses a novel hard-sigmoid constraint layer to restrict latent parameters to meaningful ranges defined by scientific prior knowledge, further enhancing its interpretability. While the embedded process-based model enforces established scientific knowledge, the encoder reveals new scientific mechanisms and relationships hidden in conventional black-box models. We apply ScIReN on two tasks: simulating the flow of organic carbon through soils, and modeling ecosystem respiration from plants. In both tasks, ScIReN outperforms black-box networks in predictive accuracy while providing substantial scientific interpretability -- it can infer latent scientific mechanisms and their relationships with input features.

</details>


### [212] [A Regret Perspective on Online Selective Generation](https://arxiv.org/abs/2506.14067)
*Minjae Lee,Yoonjae Jung,Sangdon Park*

Main category: cs.LG

TL;DR: 论文提出了一种在线学习算法，用于在部分反馈下进行选择性生成，通过将问题转化为多臂老虎机问题并利用反馈解锁技术，有效控制幻觉效应。


<details>
  <summary>Details</summary>
Motivation: 大型语言生成模型与人类交互时可能产生虚假回答（幻觉效应），选择性生成（即不确定时不回答）是控制幻觉的有效方法。然而，现有方法在非随机环境和部分反馈下的学习机制缺失。

Method: 将选择性生成问题转化为多臂老虎机问题，提出一种反馈解锁技术，利用已知老虎机算法及其理论性质，连接后悔保证与选择性生成的假发现率（FDR）保证。

Result: 理论和实验证明，该算法在多样化数据环境下能有效控制FDR，同时保持合理的选择效率（非弃答比例）。

Conclusion: 提出的在线选择性生成算法在部分反馈下具有高效性和理论保障，为控制生成模型的幻觉效应提供了实用解决方案。

Abstract: Large language generative models increasingly interact with humans, while their falsified responses raise concerns. To address this hallucination effect, selectively abstaining from answering, called selective generation, provides an effective way for generators to control the hallucination when it is unsure of their answers. However, as selective generators are interacting under non-stochastic environments and having partial feedback from users on selective generation (e.g., thumbs up or down on the selected answer), learning methods for selective generation under such practical setups are crucial but currently missing. To address these limitations, we propose an online learning algorithm for selective generation under partial feedback. In particular, as learning under partial feedback is well-studied by multi-armed bandit problems, we reduce selective generation to bandits and provide a novel conversion lemma from bandits back to selective generation to leverage any known bandit algorithms and theoretical properties. This mainly connects regret guarantees of bandits to false discovery rate (FDR) guarantees of selective generation for controlling hallucination. However, naively exploiting known bandit algorithms and their regret bounds suffers from slow convergence speed in practice due the nature of partial feedback. To overcome this, we exploit a unique structure of arms in selective generation for feedback unlocking, i.e., unlocking unknown feedback from observed feedback. We theoretically and empirically evaluate the efficacy of the proposed online selective generation algorithm under partial feedback over diverse data environment setups, resulting in controlling a desired FDR, while maintaining reasonable selection efficiency, i.e., the ratio of non-abstaining answers, compared to baselines.

</details>


### [213] [Comprehensive Verilog Design Problems: A Next-Generation Benchmark Dataset for Evaluating Large Language Models and Agents on RTL Design and Verification](https://arxiv.org/abs/2506.14074)
*Nathaniel Pinckney,Chenhui Deng,Chia-Tung Ho,Yun-Da Tsai,Mingjie Liu,Wenfei Zhou,Brucek Khailany,Haoxing Ren*

Main category: cs.LG

TL;DR: CVDP是一个新的硬件设计基准数据集，包含783个问题，覆盖13个任务类别，用于评估LLM和智能体在硬件设计中的表现。


<details>
  <summary>Details</summary>
Motivation: 推动LLM和智能体在硬件设计和验证领域的研究，提供更真实和挑战性的任务。

Method: 构建包含多种任务的CVDP数据集，使用开源工具和模型评分基础设施进行评估。

Result: 当前模型在代码生成任务中最高仅34%通过率，智能体任务尤其困难。

Conclusion: CVDP揭示了当前模型的不足，强调了在硬件设计自动化领域进一步研究的必要性。

Abstract: We present the Comprehensive Verilog Design Problems (CVDP) benchmark, a new dataset and infrastructure to advance LLM and agent research in hardware design and verification. CVDP includes 783 problems across 13 task categories, covering RTL generation, verification, debugging, specification alignment, and technical Q&A authored by experienced hardware engineers. Problems are offered in both non-agentic and agentic formats. The benchmark introduces more realistic and challenging contexts than prior work, with state-of-the-art models achieving no more than 34% pass@1 on code generation. Agentic tasks$\unicode{x2013}$especially those involving RTL reuse and verification$\unicode{x2013}$are particularly difficult. Evaluation uses open-source tools and model scoring infrastructure, with comprehension tasks assessed via BLEU and LLM-based judging. CVDP reveals substantial gaps in current model capabilities, underscoring the need for continued research toward robust, real-world hardware design automation.

</details>


### [214] [Multi-Scale Finetuning for Encoder-based Time Series Foundation Models](https://arxiv.org/abs/2506.14087)
*Zhongzheng Qiao,Chenghao Liu,Yiming Zhang,Ming Jin,Quang Pham,Qingsong Wen,P. N. Suganthan,Xudong Jiang,Savitha Ramasamy*

Main category: cs.LG

TL;DR: 论文提出了一种多尺度微调框架（MSFT），用于优化时间序列基础模型（TSFMs）在下游任务中的性能，解决了传统微调方法的不足。


<details>
  <summary>Details</summary>
Motivation: 传统微调方法未能充分利用TSFMs的多尺度预测能力，容易导致过拟合和性能不佳。

Method: 采用因果视角分析微调过程，提出MSFT框架，显式建模多尺度信息。

Result: 实验表明，MSFT在三种骨干模型上均优于传统微调和参数高效微调方法，甚至超越最先进的深度学习方法。

Conclusion: MSFT是一种简单通用的框架，能显著提升TSFMs在下游任务中的性能。

Abstract: Time series foundation models (TSFMs) demonstrate impressive zero-shot performance for time series forecasting. However, an important yet underexplored challenge is how to effectively finetune TSFMs on specific downstream tasks. While naive finetuning can yield performance gains, we argue that it falls short of fully leveraging TSFMs' capabilities, often resulting in overfitting and suboptimal performance. Given the diverse temporal patterns across sampling scales and the inherent multi-scale forecasting capabilities of TSFMs, we adopt a causal perspective to analyze finetuning process, through which we highlight the critical importance of explicitly modeling multiple scales and reveal the shortcomings of naive approaches. Focusing on \textit{encoder-based} TSFMs, we propose \textbf{M}ulti\textbf{\textsc{s}}cale \textbf{\textsc{f}}ine\textbf{\textsc{t}}uning (\textbf{MSFT}), a simple yet general framework that explicitly integrates multi-scale modeling into the finetuning process. Experimental results on three different backbones (\moirai, \moment\ and \units) demonstrate that TSFMs finetuned with MSFT not only outperform naive and typical parameter efficient finetuning methods but also surpass state-of-the-art deep learning methods.

</details>


### [215] [Transformers Learn Faster with Semantic Focus](https://arxiv.org/abs/2506.14095)
*Parikshit Ram,Kenneth L. Clarkson,Tim Klinger,Shashanka Ubaru,Alexander G. Gray*

Main category: cs.LG

TL;DR: 稀疏注意力机制在Transformer中的学习性和泛化性研究，发现输入依赖的稀疏注意力模型优于标准注意力模型，而输入无关的稀疏注意力模型无显著优势。


<details>
  <summary>Details</summary>
Motivation: 研究稀疏注意力机制是否能在学习性和泛化性上优于标准注意力机制，而非仅关注效率。

Method: 通过实验比较多种注意力机制，并结合理论分析稀疏注意力对软最大稳定性和损失函数Lipschitz性质的影响。

Result: 输入依赖的稀疏注意力模型收敛更快且泛化更好，输入无关的稀疏注意力模型无优势。

Conclusion: 输入依赖的稀疏注意力通过集中模型的“语义焦点”加速学习，理论分析支持其优势条件在实际中成立。

Abstract: Various forms of sparse attention have been explored to mitigate the quadratic computational and memory cost of the attention mechanism in transformers. We study sparse transformers not through a lens of efficiency but rather in terms of learnability and generalization. Empirically studying a range of attention mechanisms, we find that input-dependent sparse attention models appear to converge faster and generalize better than standard attention models, while input-agnostic sparse attention models show no such benefits -- a phenomenon that is robust across architectural and optimization hyperparameter choices. This can be interpreted as demonstrating that concentrating a model's "semantic focus" with respect to the tokens currently being considered (in the form of input-dependent sparse attention) accelerates learning. We develop a theoretical characterization of the conditions that explain this behavior. We establish a connection between the stability of the standard softmax and the loss function's Lipschitz properties, then show how sparsity affects the stability of the softmax and the subsequent convergence and generalization guarantees resulting from the attention mechanism. This allows us to theoretically establish that input-agnostic sparse attention does not provide any benefits. We also characterize conditions when semantic focus (input-dependent sparse attention) can provide improved guarantees, and we validate that these conditions are in fact met in our empirical evaluations.

</details>


### [216] [Toward a Graph Foundation Model: Pre-Training Transformers With Random Walks](https://arxiv.org/abs/2506.14098)
*Ziyuan Tang,Jie Chen*

Main category: cs.LG

TL;DR: 本文提出了一种基于Transformer架构的图基础模型，通过多样化的图数据集预训练，解决了序列模型如何编码不同大小和领域的图的问题。


<details>
  <summary>Details</summary>
Motivation: 探索是否能为图数据构建类似自然语言中的基础模型，以支持图结构数据的处理和推理。

Method: 将节点表示为多个随机游走序列，利用Transformer提取节点表示，进而形成边和图表示，并设计了新的上下文预测损失函数。

Result: 理论分析了随机游走的表达能力，并展示了模型在预训练和下游任务中的适应性。

Conclusion: 该模型具有作为图结构数据处理和推理基础的潜力。

Abstract: A foundation model like GPT elicits many emergent abilities, owing to the pre-training with broad inclusion of data and the use of the powerful Transformer architecture. While foundation models in natural languages are prevalent, can we build similar models for graphs? This paper describes an approach toward a graph foundation model that is pre-trained with diverse graph datasets by adapting the Transformer backbone. A central challenge toward this end is how a sequence model encodes graphs of varying sizes and from different domains. We propose representing a node as multiple random walks, such that the Transformer can extract node representations from sequences, which in turn form edge and graph representations. We develop a novel context prediction loss for these random walks and theoretically analyze their expressive power in distinguishing neighborhoods and graphs. We also demonstrate the pre-training of our model and its adaptation to downstream tasks, showcasing its potential as a foundation for processing and reasoning with graph-structured data.

</details>


### [217] [SKOLR: Structured Koopman Operator Linear RNN for Time-Series Forecasting](https://arxiv.org/abs/2506.14113)
*Yitian Zhang,Liheng Ma,Antonios Valkanas,Boris N. Oreshkin,Mark Coates*

Main category: cs.LG

TL;DR: 该论文通过建立Koopman算子与线性RNN的联系，提出了一种基于Koopman理论的SKOLR方法，用于高效的非线性动态系统分析和时间序列预测。


<details>
  <summary>Details</summary>
Motivation: Koopman算子理论为非线性动态系统分析提供了线性化框架，但其通常为无限维，难以直接应用。因此，目标是找到一种可学习的有限维近似方法。

Method: 通过扩展状态（包含滞后观测），建立结构化Koopman算子与线性RNN更新的等价关系，并设计SKOLR方法，结合可学习的频谱分解和多层感知机（MLP）。

Result: 在多个预测基准和动态系统上的实验表明，SKOLR方法表现出卓越的性能。

Conclusion: 基于Koopman理论的SKOLR设计简化了非线性动态系统的建模，同时保持了高性能。

Abstract: Koopman operator theory provides a framework for nonlinear dynamical system analysis and time-series forecasting by mapping dynamics to a space of real-valued measurement functions, enabling a linear operator representation. Despite the advantage of linearity, the operator is generally infinite-dimensional. Therefore, the objective is to learn measurement functions that yield a tractable finite-dimensional Koopman operator approximation. In this work, we establish a connection between Koopman operator approximation and linear Recurrent Neural Networks (RNNs), which have recently demonstrated remarkable success in sequence modeling. We show that by considering an extended state consisting of lagged observations, we can establish an equivalence between a structured Koopman operator and linear RNN updates. Building on this connection, we present SKOLR, which integrates a learnable spectral decomposition of the input signal with a multilayer perceptron (MLP) as the measurement functions and implements a structured Koopman operator via a highly parallel linear RNN stack. Numerical experiments on various forecasting benchmarks and dynamical systems show that this streamlined, Koopman-theory-based design delivers exceptional performance.

</details>


### [218] [Evaluating Loss Functions for Graph Neural Networks: Towards Pretraining and Generalization](https://arxiv.org/abs/2506.14114)
*Khushnood Abbas,Ruizhe Hou,Zhou Wengang,Dong Shi,Niu Ling,Satyaki Nan,Alireza Abbasi*

Main category: cs.LG

TL;DR: 该论文通过大规模评估研究了GNN模型与多种损失函数的组合性能，发现混合损失函数在归纳任务中表现更优，GIN架构表现最佳，而MPNN表现较差。


<details>
  <summary>Details</summary>
Motivation: 研究GNN模型与损失函数组合的性能差异，填补现有研究的空白。

Method: 评估7种GNN架构和30种损失函数的组合，使用3个真实数据集和21个评估指标。

Result: 混合损失函数表现更优，GIN架构表现最佳，MPNN表现较差。

Conclusion: 多目标优化和混合损失函数对GNN性能提升至关重要，GIN架构是首选。

Abstract: Graph Neural Networks (GNNs) became useful for learning on non-Euclidean data. However, their best performance depends on choosing the right model architecture and the training objective, also called the loss function. Researchers have studied these parts separately, but a large-scale evaluation has not looked at how GNN models and many loss functions work together across different tasks. To fix this, we ran a thorough study - it included seven well-known GNN architectures. We also used a large group of 30 single plus mixed loss functions. The study looked at both inductive and transductive settings. Our evaluation spanned three distinct real-world datasets, assessing performance in both inductive and transductive settings using 21 comprehensive evaluation metrics. From these extensive results (detailed in supplementary information 1 \& 2), we meticulously analyzed the top ten model-loss combinations for each metric based on their average rank. Our findings reveal that, especially for the inductive case: 1) Hybrid loss functions generally yield superior and more robust performance compared to single loss functions, indicating the benefit of multi-objective optimization. 2) The GIN architecture always showed the highest-level average performance, especially with Cross-Entropy loss. 3) Although some combinations had overall lower average ranks, models such as GAT, particularly with certain hybrid losses, demonstrated incredible specialized strengths, maximizing the most top-1 results among the individual metrics, emphasizing subtle strengths for particular task demands. 4) On the other hand, the MPNN architecture typically lagged behind the scenarios it was tested against.

</details>


### [219] [CLGNN: A Contrastive Learning-based GNN Model for Betweenness Centrality Prediction on Temporal Graphs](https://arxiv.org/abs/2506.14122)
*Tianming Zhang,Renbo Zhang,Zhengyi Yang,Yunjun Gao,Bin Cao,Jing Fan*

Main category: cs.LG

TL;DR: 论文提出了一种基于对比学习的图神经网络（CLGNN），用于高效准确地预测时间中介中心性（TBC），解决了现有方法因数据不平衡和忽略时间依赖导致的预测不准确问题。


<details>
  <summary>Details</summary>
Motivation: 时间中介中心性（TBC）的计算成本高且分布极不平衡，导致学习模型容易过拟合，无法准确预测和识别关键节点。现有图神经网络方法未能有效解决这些问题。

Method: 提出CLGNN方法，通过构建实例图保留路径有效性和时间顺序，使用双聚合机制编码结构和时间特征，并引入稳定性聚类对比模块（KContrastNet）和回归模块（ValueNet）来缓解类别不平衡问题。

Result: CLGNN在多个基准测试中表现优异，相比现有方法实现了高达663.7倍的加速，预测误差更低，相关性更高。

Conclusion: CLGNN是一种高效、准确的TBC预测方法，能够有效解决数据不平衡和时间依赖问题，适用于多种时间语义场景。

Abstract: Temporal Betweenness Centrality (TBC) measures how often a node appears on optimal temporal paths, reflecting its importance in temporal networks. However, exact computation is highly expensive, and real-world TBC distributions are extremely imbalanced. The severe imbalance leads learning-based models to overfit to zero-centrality nodes, resulting in inaccurate TBC predictions and failure to identify truly central nodes. Existing graph neural network (GNN) methods either fail to handle such imbalance or ignore temporal dependencies altogether. To address these issues, we propose a scalable and inductive contrastive learning-based GNN (CLGNN) for accurate and efficient TBC prediction. CLGNN builds an instance graph to preserve path validity and temporal order, then encodes structural and temporal features using dual aggregation, i.e., mean and edge-to-node multi-head attention mechanisms, enhanced by temporal path count and time encodings. A stability-based clustering-guided contrastive module (KContrastNet) is introduced to separate high-, median-, and low-centrality nodes in representation space, mitigating class imbalance, while a regression module (ValueNet) estimates TBC values. CLGNN also supports multiple optimal path definitions to accommodate diverse temporal semantics. Extensive experiments demonstrate the effectiveness and efficiency of CLGNN across diverse benchmarks. CLGNN achieves up to a 663.7~$\times$ speedup compared to state-of-the-art exact TBC computation methods. It outperforms leading static GNN baselines with up to 31.4~$\times$ lower MAE and 16.7~$\times$ higher Spearman correlation, and surpasses state-of-the-art temporal GNNs with up to 5.7~$\times$ lower MAE and 3.9~$\times$ higher Spearman correlation.

</details>


### [220] [Less is More: Undertraining Experts Improves Model Upcycling](https://arxiv.org/abs/2506.14126)
*Stefan Horoi,Guy Wolf,Eugene Belilovsky,Gintare Karolina Dziugaite*

Main category: cs.LG

TL;DR: 研究发现，专家模型的长时间微调会损害模型合并性能，导致下游任务表现下降，而早期停止策略可以改善这一现象。


<details>
  <summary>Details</summary>
Motivation: 探讨专家微调对模型升级性能的影响，挑战了改进在管道中传递的假设。

Method: 通过实验分析长时间微调对模型合并和下游任务的影响，并提出早期停止策略。

Result: 长时间微调导致模型合并性能下降，而早期停止策略显著提升了升级性能。

Conclusion: 任务依赖的早期停止策略是优化模型升级性能的有效方法。

Abstract: Modern deep learning is increasingly characterized by the use of open-weight foundation models that can be fine-tuned on specialized datasets. This has led to a proliferation of expert models and adapters, often shared via platforms like HuggingFace and AdapterHub. To leverage these resources, numerous model upcycling methods have emerged, enabling the reuse of fine-tuned models in multi-task systems. A natural pipeline has thus formed to harness the benefits of transfer learning and amortize sunk training costs: models are pre-trained on general data, fine-tuned on specific tasks, and then upcycled into more general-purpose systems. A prevailing assumption is that improvements at one stage of this pipeline propagate downstream, leading to gains at subsequent steps. In this work, we challenge that assumption by examining how expert fine-tuning affects model upcycling. We show that long fine-tuning of experts that optimizes for their individual performance leads to degraded merging performance, both for fully fine-tuned and LoRA-adapted models, and to worse downstream results when LoRA adapters are upcycled into MoE layers. We trace this degradation to the memorization of a small set of difficult examples that dominate late fine-tuning steps and are subsequently forgotten during merging. Finally, we demonstrate that a task-dependent aggressive early stopping strategy can significantly improve upcycling performance.

</details>


### [221] [Leveraging Predictive Equivalence in Decision Trees](https://arxiv.org/abs/2506.14143)
*Hayden McTavish,Zachery Boner,Jon Donnelly,Margo Seltzer,Cynthia Rudin*

Main category: cs.LG

TL;DR: 论文提出了一种布尔逻辑表示法来解决决策树的预测等价性问题，并展示了其在多个机器学习任务中的应用。


<details>
  <summary>Details</summary>
Motivation: 决策树因其解释性强而被广泛使用，但其预测等价性问题（即不同决策树可能具有相同的决策边界）导致模型选择困难，影响变量重要性和缺失值处理。

Method: 提出了一种布尔逻辑表示法，避免预测等价性，并应用于多个下游任务。

Result: 研究发现决策树对特征值缺失具有较强鲁棒性，解决了预测等价性对变量重要性量化的影响，并提出了一种优化预测成本的算法。

Conclusion: 布尔逻辑表示法有效解决了决策树的预测等价性问题，提升了模型的可解释性和实用性。

Abstract: Decision trees are widely used for interpretable machine learning due to their clearly structured reasoning process. However, this structure belies a challenge we refer to as predictive equivalence: a given tree's decision boundary can be represented by many different decision trees. The presence of models with identical decision boundaries but different evaluation processes makes model selection challenging. The models will have different variable importance and behave differently in the presence of missing values, but most optimization procedures will arbitrarily choose one such model to return. We present a boolean logical representation of decision trees that does not exhibit predictive equivalence and is faithful to the underlying decision boundary. We apply our representation to several downstream machine learning tasks. Using our representation, we show that decision trees are surprisingly robust to test-time missingness of feature values; we address predictive equivalence's impact on quantifying variable importance; and we present an algorithm to optimize the cost of reaching predictions.

</details>


### [222] [Common Benchmarks Undervalue the Generalization Power of Programmatic Policies](https://arxiv.org/abs/2506.14162)
*Amirhossein Rajabpour,Kiarash Aghakasiri,Sandra Zilles,Levi H. S. Lelis*

Main category: cs.LG

TL;DR: 论文指出，现有基准低估了程序化表示在OOD问题上的泛化能力，通过简单修改神经网络训练流程，神经策略也能达到与程序化策略相当的泛化效果。


<details>
  <summary>Details</summary>
Motivation: 现有研究普遍认为程序化策略在OOD问题上泛化能力优于神经策略，但作者认为这种结论源于基准设计不足，低估了神经策略的潜力。

Method: 分析了四篇文献的实验，提出通过简化神经网络架构、使用稀疏观测、调整奖励函数（如鼓励安全策略）等方法改进神经策略的泛化能力。

Result: 神经策略在OOD问题上可以达到与程序化策略相当的泛化效果。

Conclusion: 建议设计更合理的基准问题，突出OOD泛化所需的概念，以更公平地评估不同策略的泛化能力。

Abstract: Algorithms for learning programmatic representations for sequential decision-making problems are often evaluated on out-of-distribution (OOD) problems, with the common conclusion that programmatic policies generalize better than neural policies on OOD problems. In this position paper, we argue that commonly used benchmarks undervalue the generalization capabilities of programmatic representations. We analyze the experiments of four papers from the literature and show that neural policies, which were shown not to generalize, can generalize as effectively as programmatic policies on OOD problems. This is achieved with simple changes in the neural policies training pipeline. Namely, we show that simpler neural architectures with the same type of sparse observation used with programmatic policies can help attain OOD generalization. Another modification we have shown to be effective is the use of reward functions that allow for safer policies (e.g., agents that drive slowly can generalize better). Also, we argue for creating benchmark problems highlighting concepts needed for OOD generalization that may challenge neural policies but align with programmatic representations, such as tasks requiring algorithmic constructs like stacks.

</details>


### [223] [Structured and Informed Probabilistic Modeling with the Thermodynamic Kolmogorov-Arnold Model](https://arxiv.org/abs/2506.14167)
*Prithvi Raj*

Main category: cs.LG

TL;DR: 将Kolmogorov-Arnold表示定理应用于生成建模，提出一种可解释、易设计且高效的生成模型。


<details>
  <summary>Details</summary>
Motivation: 将经典表示定理与现代概率建模结合，提升生成模型的效率与质量。

Method: 通过逆变换采样将内函数解释为马尔可夫核，结合能量基先验，采用最大似然训练。

Result: 模型支持快速推理，先验知识可提升学习效率和样本质量，且后验可恢复和可视化。

Conclusion: 通过混合分布和Langevin蒙特卡洛方法扩展灵活性，平衡训练稳定性、推理速度与生成质量。

Abstract: We adapt the Kolmogorov-Arnold Representation Theorem to generative modeling by reinterpreting its inner functions as a Markov Kernel between probability spaces via inverse transform sampling. We present a generative model that is interpretable, easy to design, and efficient. Our approach couples a Kolmogorov-Arnold Network generator with independent energy-based priors, trained via Maximum Likelihood. Inverse sampling enables fast inference, while prior knowledge can be incorporated before training to better align priors with posteriors, thereby improving learning efficiency and sample quality. The learned prior is also recoverable and visualizable post-training, offering an empirical Bayes perspective. To address inflexibility and mitigate prior-posterior mismatch, we introduce scalable extensions based on mixture distributions and Langevin Monte Carlo methods, admitting a trade-off between flexibility and training efficiency. Our contributions connect classical representation theorems with modern probabilistic modeling, while balancing training stability, inference speed, and the quality and diversity of generations.

</details>


### [224] [A Variational Information Theoretic Approach to Out-of-Distribution Detection](https://arxiv.org/abs/2506.14194)
*Sudeepta Mondal,Zhuolin Jiang,Ganesh Sundaramoorthi*

Main category: cs.LG

TL;DR: 提出了一种基于信息论的损失函数，用于构建神经网络中的分布外（OOD）检测特征，通过KL散度和信息瓶颈优化特征分布，并预测了优于现有方法的新特征。


<details>
  <summary>Details</summary>
Motivation: 解决神经网络中OOD检测特征构建的问题，提供一种理论框架以生成可解释的新特征。

Method: 引入包含KL散度和信息瓶颈的损失函数，通过变分优化方法获得OOD特征。

Result: 理论预测的新特征在OOD基准测试中表现优于现有方法。

Conclusion: 该理论为构建多种可解释的OOD特征提供了通用框架。

Abstract: We present a theory for the construction of out-of-distribution (OOD) detection features for neural networks. We introduce random features for OOD through a novel information-theoretic loss functional consisting of two terms, the first based on the KL divergence separates resulting in-distribution (ID) and OOD feature distributions and the second term is the Information Bottleneck, which favors compressed features that retain the OOD information. We formulate a variational procedure to optimize the loss and obtain OOD features. Based on assumptions on OOD distributions, one can recover properties of existing OOD features, i.e., shaping functions. Furthermore, we show that our theory can predict a new shaping function that out-performs existing ones on OOD benchmarks. Our theory provides a general framework for constructing a variety of new features with clear explainability.

</details>


### [225] [DiffusionBlocks: Blockwise Training for Generative Models via Score-Based Diffusion](https://arxiv.org/abs/2506.14202)
*Makoto Shing,Takuya Akiba*

Main category: cs.LG

TL;DR: DiffusionBlocks是一种新型训练框架，通过将神经网络块解释为连续时间扩散过程中的去噪操作，显著减少内存使用，同时保持性能。


<details>
  <summary>Details</summary>
Motivation: 解决大规模神经网络训练中的内存瓶颈问题，降低AI研究的门槛。

Method: 将网络划分为独立可训练块，基于等累积概率质量优化噪声水平分配。

Result: 在图像生成和语言建模任务中，内存使用与块数成比例减少，性能优于传统反向传播。

Conclusion: DiffusionBlocks为计算资源有限的大规模神经网络训练提供了可行方案。

Abstract: Training large neural networks with end-to-end backpropagation creates significant memory bottlenecks, limiting accessibility to state-of-the-art AI research. We propose $\textit{DiffusionBlocks}$, a novel training framework that interprets neural network blocks as performing denoising operations in a continuous-time diffusion process. By partitioning the network into independently trainable blocks and optimizing noise level assignments based on equal cumulative probability mass, our approach achieves significant memory efficiency while maintaining competitive performance compared to traditional backpropagation in generative tasks. Experiments on image generation and language modeling tasks demonstrate memory reduction proportional to the number of blocks while achieving superior performance. DiffusionBlocks provides a promising pathway for democratizing access to large-scale neural network training with limited computational resources.

</details>


### [226] [TriGuard: Testing Model Safety with Attribution Entropy, Verification, and Drift](https://arxiv.org/abs/2506.14217)
*Dipesh Tharu Mahato,Rohan Poudel,Pramod Dhungana*

Main category: cs.LG

TL;DR: TriGuard是一个统一的安全评估框架，结合形式化鲁棒性验证、归因熵和归因漂移分数，揭示模型准确性与可解释性之间的不匹配。


<details>
  <summary>Details</summary>
Motivation: 深度神经网络在准确性上表现优异，但在对抗性和分布变化下的可靠性仍是一个挑战。

Method: TriGuard结合形式化鲁棒性验证、归因熵和归因漂移分数来评估模型安全性。

Result: 实验表明，TriGuard能揭示神经网络推理中的脆弱性，且熵正则化训练可减少解释漂移而不影响性能。

Conclusion: TriGuard推动了鲁棒且可解释的模型评估的前沿。

Abstract: Deep neural networks often achieve high accuracy, but ensuring their reliability under adversarial and distributional shifts remains a pressing challenge. We propose TriGuard, a unified safety evaluation framework that combines (1) formal robustness verification, (2) attribution entropy to quantify saliency concentration, and (3) a novel Attribution Drift Score measuring explanation stability. TriGuard reveals critical mismatches between model accuracy and interpretability: verified models can still exhibit unstable reasoning, and attribution-based signals provide complementary safety insights beyond adversarial accuracy. Extensive experiments across three datasets and five architectures show how TriGuard uncovers subtle fragilities in neural reasoning. We further demonstrate that entropy-regularized training reduces explanation drift without sacrificing performance. TriGuard advances the frontier in robust, interpretable model evaluation.

</details>


### [227] [Can Large Language Models Improve Spectral Graph Neural Networks?](https://arxiv.org/abs/2506.14220)
*Kangkang Lu,Yanhua Yu,Zhiyong Huang,Tat-Seng Chua*

Main category: cs.LG

TL;DR: SGNNs在标签稀缺时性能下降，本文利用LLM估计图的同质性，指导SGNN滤波器设计，提升性能。


<details>
  <summary>Details</summary>
Motivation: SGNNs在标签稀缺时可能学习次优滤波器，而LLMs的成功启发探索其在GNN领域的潜力。

Method: 利用LLM估计图的同质性，生成同质性感知先验，指导多项式谱滤波器设计。

Result: 在基准数据集上，LLM驱动的SGNN框架在多种图结构下均优于基线方法。

Conclusion: LLMs能有效提升SGNNs的表达能力和适应性，且计算和成本开销低。

Abstract: Spectral Graph Neural Networks (SGNNs) have attracted significant attention due to their ability to approximate arbitrary filters. They typically rely on supervision from downstream tasks to adaptively learn appropriate filters. However, under label-scarce conditions, SGNNs may learn suboptimal filters, leading to degraded performance. Meanwhile, the remarkable success of Large Language Models (LLMs) has inspired growing interest in exploring their potential within the GNN domain. This naturally raises an important question: \textit{Can LLMs help overcome the limitations of SGNNs and enhance their performance?} In this paper, we propose a novel approach that leverages LLMs to estimate the homophily of a given graph. The estimated homophily is then used to adaptively guide the design of polynomial spectral filters, thereby improving the expressiveness and adaptability of SGNNs across diverse graph structures. Specifically, we introduce a lightweight pipeline in which the LLM generates homophily-aware priors, which are injected into the filter coefficients to better align with the underlying graph topology. Extensive experiments on benchmark datasets demonstrate that our LLM-driven SGNN framework consistently outperforms existing baselines under both homophilic and heterophilic settings, with minimal computational and monetary overhead.

</details>


### [228] [Adaptive Reinforcement Learning for Unobservable Random Delays](https://arxiv.org/abs/2506.14411)
*John Wikman,Alexandre Proutiere,David Broman*

Main category: cs.LG

TL;DR: 论文提出了一种名为“交互层”的通用框架，用于处理强化学习中的不可观测和时变延迟问题，并开发了基于模型的ACDA算法，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现实动态环境中，智能体与系统的交互延迟通常不可观测且随时间变化，现有方法假设固定延迟上限，导致保守策略。

Method: 引入交互层框架，生成未来动作矩阵以应对延迟和丢包；开发ACDA算法，动态适应延迟模式。

Result: ACDA算法在多种运动基准环境中显著优于现有方法。

Conclusion: 交互层框架和ACDA算法有效解决了延迟问题，提升了强化学习在动态环境中的性能。

Abstract: In standard Reinforcement Learning (RL) settings, the interaction between the agent and the environment is typically modeled as a Markov Decision Process (MDP), which assumes that the agent observes the system state instantaneously, selects an action without delay, and executes it immediately. In real-world dynamic environments, such as cyber-physical systems, this assumption often breaks down due to delays in the interaction between the agent and the system. These delays can vary stochastically over time and are typically unobservable, meaning they are unknown when deciding on an action. Existing methods deal with this uncertainty conservatively by assuming a known fixed upper bound on the delay, even if the delay is often much lower. In this work, we introduce the interaction layer, a general framework that enables agents to adaptively and seamlessly handle unobservable and time-varying delays. Specifically, the agent generates a matrix of possible future actions to handle both unpredictable delays and lost action packets sent over networks. Building on this framework, we develop a model-based algorithm, Actor-Critic with Delay Adaptation (ACDA), which dynamically adjusts to delay patterns. Our method significantly outperforms state-of-the-art approaches across a wide range of locomotion benchmark environments.

</details>


### [229] [Convergence-Privacy-Fairness Trade-Off in Personalized Federated Learning](https://arxiv.org/abs/2506.14251)
*Xiyu Zhao,Qimei Cui,Weicai Li,Wei Ni,Ekram Hossain,Quan Z. Sheng,Xiaofeng Tao,Ping Zhang*

Main category: cs.LG

TL;DR: DP-Ditto是Ditto的差分隐私扩展版，分析隐私保护、模型收敛和性能公平性之间的权衡，并在实验中表现优于现有PFL模型。


<details>
  <summary>Details</summary>
Motivation: 解决个性化联邦学习中隐私保护对模型收敛和公平性的影响。

Method: 提出DP-Ditto，分析其收敛上界和全局聚合次数优化，并研究性能公平性。

Result: 实验显示DP-Ditto在公平性和准确性上分别超过现有模型32.71%和9.66%。

Conclusion: DP-Ditto在隐私保护下优化了收敛和公平性，性能优于现有方法。

Abstract: Personalized federated learning (PFL), e.g., the renowned Ditto, strikes a balance between personalization and generalization by conducting federated learning (FL) to guide personalized learning (PL). While FL is unaffected by personalized model training, in Ditto, PL depends on the outcome of the FL. However, the clients' concern about their privacy and consequent perturbation of their local models can affect the convergence and (performance) fairness of PL. This paper presents PFL, called DP-Ditto, which is a non-trivial extension of Ditto under the protection of differential privacy (DP), and analyzes the trade-off among its privacy guarantee, model convergence, and performance distribution fairness. We also analyze the convergence upper bound of the personalized models under DP-Ditto and derive the optimal number of global aggregations given a privacy budget. Further, we analyze the performance fairness of the personalized models, and reveal the feasibility of optimizing DP-Ditto jointly for convergence and fairness. Experiments validate our analysis and demonstrate that DP-Ditto can surpass the DP-perturbed versions of the state-of-the-art PFL models, such as FedAMP, pFedMe, APPLE, and FedALA, by over 32.71% in fairness and 9.66% in accuracy.

</details>


### [230] [RL-Obfuscation: Can Language Models Learn to Evade Latent-Space Monitors?](https://arxiv.org/abs/2506.14261)
*Rohan Gupta,Erik Jenner*

Main category: cs.LG

TL;DR: 研究探讨了大型语言模型（LLM）是否能通过学习规避潜在空间监视器，并提出了一种基于强化学习的方法（RL-Obfuscation）来测试其可行性。


<details>
  <summary>Details</summary>
Motivation: 潜在空间监视器通过内部模型表示检测不良行为，但尚未验证LLM是否能学会规避这些监视器。

Method: 使用强化学习微调LLM（7B至14B参数），使其绕过潜在空间监视器，同时保持生成内容的连贯性。

Result: 发现基于令牌的监视器易受攻击，而更全面的监视器（如最大池化或基于注意力的探针）仍稳健。此外，针对单一监视器训练的对抗策略可泛化到同类未见过监视器。

Conclusion: LLM能学会规避潜在空间监视器，甚至重新定义令牌的内部含义，这对监视器的设计提出了挑战。

Abstract: Latent-space monitors aim to detect undesirable behaviours in large language models by leveraging internal model representations rather than relying solely on black-box outputs. These methods have shown promise in identifying behaviours such as deception and unsafe completions, but a critical open question remains: can LLMs learn to evade such monitors? To study this, we introduce RL-Obfuscation, in which LLMs are finetuned via reinforcement learning to bypass latent-space monitors while maintaining coherent generations. We apply RL-Obfuscation to LLMs ranging from 7B to 14B parameters and evaluate evasion success against a suite of monitors. We find that token-level latent-space monitors are highly vulnerable to this attack. More holistic monitors, such as max-pooling or attention-based probes, remain robust. Moreover, we show that adversarial policies trained to evade a single static monitor generalise to unseen monitors of the same type. Finally, we study how the policy learned by RL bypasses these monitors and find that the model can also learn to repurpose tokens to mean something different internally.

</details>


### [231] [Knowledge Adaptation as Posterior Correction](https://arxiv.org/abs/2506.14262)
*Mohammad Emtiyaz Khan*

Main category: cs.LG

TL;DR: 论文探讨了机器如何像人类和动物一样快速适应，提出所有适应方法都可以视为对近似后验的“修正”，更准确的后验意味着更小的修正和更快的适应。


<details>
  <summary>Details</summary>
Motivation: 尽管在模型适应方面取得了进展，但机器如何自然快速适应仍不清楚。本文旨在揭示这种适应机制。

Method: 通过贝叶斯学习规则的双重视角，将适应过程中的干扰表征为过去数据的自然梯度不匹配。

Result: 研究表明，更准确的后验会导致更小的修正，从而实现更快的适应。

Conclusion: 后验修正是一种自然的机制，可以帮助机器快速适应。

Abstract: Adaptation is the holy grail of intelligence, but even the best AI models (like GPT) lack the adaptivity of toddlers. So the question remains: how can machines adapt quickly? Despite a lot of progress on model adaptation to facilitate continual and federated learning, as well as model merging, editing, unlearning, etc., little is known about the mechanisms by which machines can naturally learn to adapt in a similar way as humans and animals. Here, we show that all such adaptation methods can be seen as different ways of `correcting' the approximate posteriors. More accurate posteriors lead to smaller corrections, which in turn imply quicker adaptation. The result is obtained by using a dual-perspective of the Bayesian Learning Rule of Khan and Rue (2023) where interference created during adaptation is characterized by the natural-gradient mismatch over the past data. We present many examples to demonstrate the use of posterior-correction as a natural mechanism for the machines to learn to adapt quickly.

</details>


### [232] [Towards Robust Learning to Optimize with Theoretical Guarantees](https://arxiv.org/abs/2506.14263)
*Qingyu Song,Wei Lin,Juncheng Wang,Hong Xu*

Main category: cs.LG

TL;DR: 论文提出了一种学习优化（L2O）方法，通过理论证明其在分布外（OOD）场景中的性能和鲁棒性，并提出了一种新的梯度特征构建和历史建模方法，显著提升了收敛速度。


<details>
  <summary>Details</summary>
Motivation: 现有L2O方法在分布外（OOD）场景中缺乏理论性能证明，本文旨在填补这一空白。

Method: 提出了一种梯度特征构建和历史建模方法，并通过理论证明其鲁棒性。

Result: 数值模拟显示，该方法在分布内（InD）和分布外（OOD）场景中均优于基线，收敛速度提升高达10倍。

Conclusion: 该方法在理论和实践中均表现出色，为L2O在复杂场景中的应用提供了可靠支持。

Abstract: Learning to optimize (L2O) is an emerging technique to solve mathematical optimization problems with learning-based methods. Although with great success in many real-world scenarios such as wireless communications, computer networks, and electronic design, existing L2O works lack theoretical demonstration of their performance and robustness in out-of-distribution (OOD) scenarios. We address this gap by providing comprehensive proofs. First, we prove a sufficient condition for a robust L2O model with homogeneous convergence rates over all In-Distribution (InD) instances. We assume an L2O model achieves robustness for an InD scenario. Based on our proposed methodology of aligning OOD problems to InD problems, we also demonstrate that the L2O model's convergence rate in OOD scenarios will deteriorate by an equation of the L2O model's input features. Moreover, we propose an L2O model with a concise gradient-only feature construction and a novel gradient-based history modeling method. Numerical simulation demonstrates that our proposed model outperforms the state-of-the-art baseline in both InD and OOD scenarios and achieves up to 10 $\times$ convergence speedup. The code of our method can be found from https://github.com/NetX-lab/GoMathL2O-Official.

</details>


### [233] [Improving LoRA with Variational Learning](https://arxiv.org/abs/2506.14280)
*Bai Cong,Nico Daheim,Yuesong Shen,Rio Yokota,Mohammad Emtiyaz Khan,Thomas Möllenhoff*

Main category: cs.LG

TL;DR: IVON算法显著提升了LoRA微调的效果，优于AdamW和其他贝叶斯方法，同时计算成本低。


<details>
  <summary>Details</summary>
Motivation: 解决贝叶斯方法在LoRA微调中效果有限且计算开销大的问题。

Method: 使用IVON变分算法，结合后验剪枝技术。

Result: 在Llama-3.2-3B模型上，准确率提升1.3%，ECE降低5.4%。

Conclusion: IVON能有效改进LoRA微调，优于现有方法。

Abstract: Bayesian methods have recently been used to improve LoRA finetuning and, although they improve calibration, their effect on other metrics (such as accuracy) is marginal and can sometimes even be detrimental. Moreover, Bayesian methods also increase computational overheads and require additional tricks for them to work well. Here, we fix these issues by using a recently proposed variational algorithm called IVON. We show that IVON is easy to implement and has similar costs to AdamW, and yet it can also drastically improve many metrics by using a simple posterior pruning technique. We present extensive results on billion-scale LLMs (Llama and Qwen series) going way beyond the scale of existing applications of IVON. For example, we finetune a Llama-3.2-3B model on a set of commonsense reasoning tasks and improve accuracy over AdamW by 1.3% and reduce ECE by 5.4%, outperforming AdamW and other recent Bayesian methods like Laplace-LoRA and BLoB. Overall, our results show that variational learning with IVON can effectively improve LoRA finetuning.

</details>


### [234] [Equivariance Everywhere All At Once: A Recipe for Graph Foundation Models](https://arxiv.org/abs/2506.14291)
*Ben Finkelshtein,İsmail İlkan Ceylan,Michael Bronstein,Ron Levie*

Main category: cs.LG

TL;DR: 提出了一种基于对称性设计的图基础模型，用于节点级任务，具有跨图泛化能力。


<details>
  <summary>Details</summary>
Motivation: 解决图机器学习模型因任务和数据集特定设计而泛化能力不足的问题，探索如何构建通用的图基础模型。

Method: 通过系统研究图基础模型需满足的对称性（如节点和标签的置换等变性、特征的置换不变性），设计线性变换层，并证明其多集上的通用逼近能力。

Result: 在29个真实节点分类数据集上验证，展示了强大的零样本性能及随训练图数量增加的持续改进。

Conclusion: 通过对称性驱动的设计，成功构建了具有广泛泛化能力的图基础模型。

Abstract: Graph machine learning architectures are typically tailored to specific tasks on specific datasets, which hinders their broader applicability. This has led to a new quest in graph machine learning: how to build graph foundation models capable of generalizing across arbitrary graphs and features? In this work, we present a recipe for designing graph foundation models for node-level tasks from first principles. The key ingredient underpinning our study is a systematic investigation of the symmetries that a graph foundation model must respect. In a nutshell, we argue that label permutation-equivariance alongside feature permutation-invariance are necessary in addition to the common node permutation-equivariance on each local neighborhood of the graph. To this end, we first characterize the space of linear transformations that are equivariant to permutations of nodes and labels, and invariant to permutations of features. We then prove that the resulting network is a universal approximator on multisets that respect the aforementioned symmetries. Our recipe uses such layers on the multiset of features induced by the local neighborhood of the graph to obtain a class of graph foundation models for node property prediction. We validate our approach through extensive experiments on 29 real-world node classification datasets, demonstrating both strong zero-shot empirical performance and consistent improvement as the number of training graphs increases.

</details>


### [235] [Fair for a few: Improving Fairness in Doubly Imbalanced Datasets](https://arxiv.org/abs/2506.14306)
*Ata Yalcin,Asli Umay Ozturk,Yigit Sever,Viktoria Pauw,Stephan Hachinger,Ismail Hakki Toroslu,Pinar Karagoz*

Main category: cs.LG

TL;DR: 本文探讨了在双重不平衡数据集（标签和敏感属性均不平衡）中实现公平性的挑战，并提出了一种多标准解决方案。


<details>
  <summary>Details</summary>
Motivation: 机器学习中的公平性是一个重要问题，但现有方法在数据双重不平衡时表现不佳。本文旨在解决这一问题。

Method: 首先进行探索性分析，然后提出一种基于多标准的解决方案，优化采样和分布以平衡公平性和分类准确性。

Result: 提出的方法在双重不平衡数据集中能够更好地平衡公平性和分类准确性。

Conclusion: 多标准方法为双重不平衡数据集中的公平性问题提供了有效解决方案。

Abstract: Fairness has been identified as an important aspect of Machine Learning and Artificial Intelligence solutions for decision making. Recent literature offers a variety of approaches for debiasing, however many of them fall short when the data collection is imbalanced. In this paper, we focus on a particular case, fairness in doubly imbalanced datasets, such that the data collection is imbalanced both for the label and the groups in the sensitive attribute. Firstly, we present an exploratory analysis to illustrate limitations in debiasing on a doubly imbalanced dataset. Then, a multi-criteria based solution is proposed for finding the most suitable sampling and distribution for label and sensitive attribute, in terms of fairness and classification accuracy

</details>


### [236] [IntelliLung: Advancing Safe Mechanical Ventilation using Offline RL with Hybrid Actions and Clinically Aligned Rewards](https://arxiv.org/abs/2506.14375)
*Muhammad Hamza Yousuf,Jason Li,Sahar Vahdati,Raphael Theilen,Jakob Wittenstein,Jens Lehmann*

Main category: cs.LG

TL;DR: 论文提出了一种优化离线强化学习的方法，用于解决机械通气（MV）控制中混合动作空间的挑战，避免离散化的缺陷，并引入临床相关的奖励函数。


<details>
  <summary>Details</summary>
Motivation: 机械通气优化因患者特异性变异性而复杂且易错，现有离线强化学习方法难以处理混合动作空间，离散化会限制动作选择并引入安全风险。

Method: 优化动作空间减少方法，并改进离线强化学习算法（IQL和EDAC）以直接处理混合动作空间；设计基于临床目标的奖励函数。

Result: AI辅助的MV优化可提升患者安全性，实现个性化肺部支持，为智能重症护理提供进步。

Conclusion: 该方法在机械通气优化中表现出潜力，是数据驱动重症护理的重要进展。

Abstract: Invasive mechanical ventilation (MV) is a life-sustaining therapy for critically ill patients in the intensive care unit (ICU). However, optimizing its settings remains a complex and error-prone process due to patient-specific variability. While Offline Reinforcement Learning (RL) shows promise for MV control, current stateof-the-art (SOTA) methods struggle with the hybrid (continuous and discrete) nature of MV actions. Discretizing the action space limits available actions due to exponential growth in combinations and introduces distribution shifts that can compromise safety. In this paper, we propose optimizations that build upon prior work in action space reduction to address the challenges of discrete action spaces. We also adapt SOTA offline RL algorithms (IQL and EDAC) to operate directly on hybrid action spaces, thereby avoiding the pitfalls of discretization. Additionally, we introduce a clinically grounded reward function based on ventilator-free days and physiological targets, which provides a more meaningful optimization objective compared to traditional sparse mortality-based rewards. Our findings demonstrate that AI-assisted MV optimization may enhance patient safety and enable individualized lung support, representing a significant advancement toward intelligent, data-driven critical care solutions.

</details>


### [237] [ResNets Are Deeper Than You Think](https://arxiv.org/abs/2506.14386)
*Christian H. X. Ali Mehmeti-Göpel,Michael Wand*

Main category: cs.LG

TL;DR: 残差连接在神经网络中广泛使用，其优势不仅在于优化训练，还因其独特的函数空间和归纳偏置。


<details>
  <summary>Details</summary>
Motivation: 探索残差连接为何持续优于前馈网络，提出其优势不仅源于优化，还与函数空间和自然数据结构相关。

Method: 设计后训练对比实验，隔离泛化性能与训练优化，比较可变深度与固定深度架构。

Result: 可变深度架构（如ResNet）在优化无关的情况下仍优于固定深度网络。

Conclusion: 残差连接的优势不仅在于优化，还因其与自然数据结构的归纳偏置更匹配。

Abstract: Residual connections remain ubiquitous in modern neural network architectures nearly a decade after their introduction. Their widespread adoption is often credited to their dramatically improved trainability: residual networks train faster, more stably, and achieve higher accuracy than their feedforward counterparts. While numerous techniques, ranging from improved initialization to advanced learning rate schedules, have been proposed to close the performance gap between residual and feedforward networks, this gap has persisted. In this work, we propose an alternative explanation: residual networks do not merely reparameterize feedforward networks, but instead inhabit a different function space. We design a controlled post-training comparison to isolate generalization performance from trainability; we find that variable-depth architectures, similar to ResNets, consistently outperform fixed-depth networks, even when optimization is unlikely to make a difference. These results suggest that residual connections confer performance advantages beyond optimization, pointing instead to a deeper inductive bias aligned with the structure of natural data.

</details>


### [238] [Enclosing Prototypical Variational Autoencoder for Explainable Out-of-Distribution Detection](https://arxiv.org/abs/2506.14390)
*Conrad Orglmeister,Erik Bochinski,Volker Eiselein,Elvira Fleig*

Main category: cs.LG

TL;DR: 论文提出了一种结合自解释原型变分模型和自编码器的OOD检测方法，通过变分自编码器学习潜在空间，用于分类、OOD检测和重建，并在真实铁路数据集上验证了其优越性。


<details>
  <summary>Details</summary>
Motivation: 提高深度机器学习模型在安全相关应用中的决策透明性和可靠性。

Method: 扩展自解释原型变分模型，结合变分自编码器学习潜在空间，定义高斯混合分布表示ID区域，并引入限制损失以保持ID区域的紧凑性。

Result: 在常见OOD检测基准和真实铁路数据集上表现优于现有方法。

Conclusion: 该方法通过自编码器的重建能力增强了原型和ID区域的解释性，同时提升了OOD检测性能。

Abstract: Understanding the decision-making and trusting the reliability of Deep Machine Learning Models is crucial for adopting such methods to safety-relevant applications. We extend self-explainable Prototypical Variational models with autoencoder-based out-of-distribution (OOD) detection: A Variational Autoencoder is applied to learn a meaningful latent space which can be used for distance-based classification, likelihood estimation for OOD detection, and reconstruction. The In-Distribution (ID) region is defined by a Gaussian mixture distribution with learned prototypes representing the center of each mode. Furthermore, a novel restriction loss is introduced that promotes a compact ID region in the latent space without collapsing it into single points. The reconstructive capabilities of the Autoencoder ensure the explainability of the prototypes and the ID region of the classifier, further aiding the discrimination of OOD samples. Extensive evaluations on common OOD detection benchmarks as well as a large-scale dataset from a real-world railway application demonstrate the usefulness of the approach, outperforming previous methods.

</details>


### [239] [HiLight: A Hierarchical Reinforcement Learning Framework with Global Adversarial Guidance for Large-Scale Traffic Signal Control](https://arxiv.org/abs/2506.14391)
*Yaqiao Zhu,Hongkai Wen,Geyong Min,Man Luo*

Main category: cs.LG

TL;DR: HiLight是一个分层强化学习框架，通过全局对抗性指导解决大规模交通信号控制问题，结合高层Meta-Policy和低层Sub-Policy，提升网络级效率。


<details>
  <summary>Details</summary>
Motivation: 现有强化学习方法在大规模网络中难以兼顾全局协调与可扩展性，集中式方法扩展性差，分散式方法缺乏统一目标。

Method: 提出HiLight框架，包括高层Meta-Policy（分区生成子目标）和低层Sub-Policy（控制交叉口），引入对抗训练机制优化全局与局部协调。

Result: 在合成和真实基准测试中表现优异，尤其在大规模场景中优势显著。

Conclusion: HiLight在大规模交通信号控制中实现了高效协调，具有广泛适用性。

Abstract: Efficient traffic signal control (TSC) is essential for mitigating urban congestion, yet existing reinforcement learning (RL) methods face challenges in scaling to large networks while maintaining global coordination. Centralized RL suffers from scalability issues, while decentralized approaches often lack unified objectives, resulting in limited network-level efficiency. In this paper, we propose HiLight, a hierarchical reinforcement learning framework with global adversarial guidance for large-scale TSC. HiLight consists of a high-level Meta-Policy, which partitions the traffic network into subregions and generates sub-goals using a Transformer-LSTM architecture, and a low-level Sub-Policy, which controls individual intersections with global awareness. To improve the alignment between global planning and local execution, we introduce an adversarial training mechanism, where the Meta-Policy generates challenging yet informative sub-goals, and the Sub-Policy learns to surpass these targets, leading to more effective coordination. We evaluate HiLight across both synthetic and real-world benchmarks, and additionally construct a large-scale Manhattan network with diverse traffic conditions, including peak transitions, adverse weather, and holiday surges. Experimental results show that HiLight exhibits significant advantages in large-scale scenarios and remains competitive across standard benchmarks of varying sizes.

</details>


### [240] [One Size Fits None: Rethinking Fairness in Medical AI](https://arxiv.org/abs/2506.14400)
*Roland Roller,Michael Hahn,Ajay Madhavan Ravichandran,Bilgin Osmanodja,Florian Oetke,Zeineb Sassi,Aljoscha Burchardt,Klaus Netter,Klemens Budde,Anne Herrmann,Tobias Strapatsas,Peter Dabrock,Sebastian Möller*

Main category: cs.LG

TL;DR: 该论文探讨了机器学习模型在临床决策中的公平性问题，强调了对患者亚组性能差异的分析，以促进更公平和透明的模型开发。


<details>
  <summary>Details</summary>
Motivation: 现实世界的医疗数据集通常存在噪声、不完整和不平衡问题，导致模型在不同患者亚组中的性能差异，可能加剧边缘化群体的劣势。

Method: 通过分析多个医疗预测任务，研究模型性能如何随患者特征变化，并强调亚组级别的评估。

Result: 研究发现，尽管模型整体性能良好，但亚组间存在显著差异，需在临床实践中考虑这些差异。

Conclusion: 亚组敏感的分析有助于开发更公平的医疗机器学习模型，并促进公平性与透明度的结合。

Abstract: Machine learning (ML) models are increasingly used to support clinical decision-making. However, real-world medical datasets are often noisy, incomplete, and imbalanced, leading to performance disparities across patient subgroups. These differences raise fairness concerns, particularly when they reinforce existing disadvantages for marginalized groups. In this work, we analyze several medical prediction tasks and demonstrate how model performance varies with patient characteristics. While ML models may demonstrate good overall performance, we argue that subgroup-level evaluation is essential before integrating them into clinical workflows. By conducting a performance analysis at the subgroup level, differences can be clearly identified-allowing, on the one hand, for performance disparities to be considered in clinical practice, and on the other hand, for these insights to inform the responsible development of more effective models. Thereby, our work contributes to a practical discussion around the subgroup-sensitive development and deployment of medical ML models and the interconnectedness of fairness and transparency.

</details>


### [241] [Unsupervised Skill Discovery through Skill Regions Differentiation](https://arxiv.org/abs/2506.14420)
*Ting Xiao,Jiakun Zheng,Rushuai Yang,Kang Xu,Qiaosheng Zhang,Peng Liu,Chenjia Bai*

Main category: cs.LG

TL;DR: 提出一种新的技能发现目标，通过最大化技能间状态密度的差异，结合条件自编码器和内在奖励，提升无监督强化学习的多样性和下游任务性能。


<details>
  <summary>Details</summary>
Motivation: 解决熵探索和大规模状态空间下互信息估计的局限性，提升技能多样性和状态探索能力。

Method: 提出最大化技能间状态密度差异的目标，设计条件自编码器估计状态密度，并基于自编码器设计内在奖励。

Result: 在复杂任务中学习到有意义技能，并在下游任务中表现优异。

Conclusion: 新方法有效提升技能多样性和探索能力，适用于高维状态空间。

Abstract: Unsupervised Reinforcement Learning (RL) aims to discover diverse behaviors that can accelerate the learning of downstream tasks. Previous methods typically focus on entropy-based exploration or empowerment-driven skill learning. However, entropy-based exploration struggles in large-scale state spaces (e.g., images), and empowerment-based methods with Mutual Information (MI) estimations have limitations in state exploration. To address these challenges, we propose a novel skill discovery objective that maximizes the deviation of the state density of one skill from the explored regions of other skills, encouraging inter-skill state diversity similar to the initial MI objective. For state-density estimation, we construct a novel conditional autoencoder with soft modularization for different skill policies in high-dimensional space. Meanwhile, to incentivize intra-skill exploration, we formulate an intrinsic reward based on the learned autoencoder that resembles count-based exploration in a compact latent space. Through extensive experiments in challenging state and image-based tasks, we find our method learns meaningful skills and achieves superior performance in various downstream tasks.

</details>


### [242] [MoORE: SVD-based Model MoE-ization for Conflict- and Oblivion-Resistant Multi-Task Adaptation](https://arxiv.org/abs/2506.14436)
*Shen Yuan,Yin Zheng,Taifeng Wang,Binbin Liu,Hongteng Xu*

Main category: cs.LG

TL;DR: 论文提出了一种名为MoORE的新方法，通过SVD分解和可学习路由器调整权重矩阵，解决多任务适应中的任务冲突和遗忘问题。


<details>
  <summary>Details</summary>
Motivation: 大规模基础模型在多任务场景中常面临任务冲突和遗忘问题，需要一种更有效的适应方法。

Method: 利用SVD分解权重矩阵，引入可学习路由器调整奇异值，形成正交专家混合（MoORE），并保持原始权重矩阵的列空间。

Result: 实验表明，MoORE在多任务适应中优于现有方法，有效抵抗任务冲突和遗忘。

Conclusion: MoORE是一种高效的多任务适应方法，兼具冲突和遗忘抵抗能力。

Abstract: Adapting large-scale foundation models in multi-task scenarios often suffers from task conflict and oblivion. To mitigate such issues, we propose a novel ''model MoE-ization'' strategy that leads to a conflict- and oblivion-resistant multi-task adaptation method. Given a weight matrix of a pre-trained model, our method applies SVD to it and introduces a learnable router to adjust its singular values based on tasks and samples. Accordingly, the weight matrix becomes a Mixture of Orthogonal Rank-one Experts (MoORE), in which each expert corresponds to the outer product of a left singular vector and the corresponding right one. We can improve the model capacity by imposing a learnable orthogonal transform on the right singular vectors. Unlike low-rank adaptation (LoRA) and its MoE-driven variants, MoORE guarantees the experts' orthogonality and maintains the column space of the original weight matrix. These two properties make the adapted model resistant to the conflicts among the new tasks and the oblivion of its original tasks, respectively. Experiments on various datasets demonstrate that MoORE outperforms existing multi-task adaptation methods consistently, showing its superiority in terms of conflict- and oblivion-resistance. The code of the experiments is available at https://github.com/DaShenZi721/MoORE.

</details>


### [243] [sHGCN: Simplified hyperbolic graph convolutional neural networks](https://arxiv.org/abs/2506.14438)
*Pol Arévalo,Alexis Molina,Álvaro Ciudad*

Main category: cs.LG

TL;DR: 论文提出简化双曲神经网络中的关键操作，显著提升计算效率和性能，使其更适用于广泛任务。


<details>
  <summary>Details</summary>
Motivation: 双曲几何虽能有效建模复杂结构化数据，但现有双曲神经网络在计算效率和精度上存在不足。

Method: 通过简化双曲神经网络中的关键操作。

Result: 实验表明，简化操作显著提升了计算速度和预测准确性。

Conclusion: 优化后的双曲神经网络更具实用性，适用范围更广。

Abstract: Hyperbolic geometry has emerged as a powerful tool for modeling complex, structured data, particularly where hierarchical or tree-like relationships are present. By enabling embeddings with lower distortion, hyperbolic neural networks offer promising alternatives to Euclidean-based models for capturing intricate data structures. Despite these advantages, they often face performance challenges, particularly in computational efficiency and tasks requiring high precision. In this work, we address these limitations by simplifying key operations within hyperbolic neural networks, achieving notable improvements in both runtime and performance. Our findings demonstrate that streamlined hyperbolic operations can lead to substantial gains in computational speed and predictive accuracy, making hyperbolic neural networks a more viable choice for a broader range of applications.

</details>


### [244] [A General Framework for Off-Policy Learning with Partially-Observed Reward](https://arxiv.org/abs/2506.14439)
*Rikiya Takehi,Masahiro Asami,Kosuke Kawakami,Yuta Saito*

Main category: cs.LG

TL;DR: 论文提出了一种名为HyPeR的新方法，用于在部分观测目标奖励的情况下，利用密集观测的次级奖励来优化决策策略。


<details>
  <summary>Details</summary>
Motivation: 在部分观测目标奖励时，传统的离策略学习效果显著下降，而仅依赖次级奖励可能导致策略学习不佳。

Method: 提出HyPeR方法，结合部分观测的目标奖励和密集观测的次级奖励进行优化。

Result: 实验表明，HyPeR在多种场景下优于现有方法。

Conclusion: HyPeR能有效利用次级奖励提升目标奖励的优化效果，甚至在同时优化次级奖励时对目标奖励也有帮助。

Abstract: Off-policy learning (OPL) in contextual bandits aims to learn a decision-making policy that maximizes the target rewards by using only historical interaction data collected under previously developed policies. Unfortunately, when rewards are only partially observed, the effectiveness of OPL degrades severely. Well-known examples of such partial rewards include explicit ratings in content recommendations, conversion signals on e-commerce platforms that are partial due to delay, and the issue of censoring in medical problems. One possible solution to deal with such partial rewards is to use secondary rewards, such as dwelling time, clicks, and medical indicators, which are more densely observed. However, relying solely on such secondary rewards can also lead to poor policy learning since they may not align with the target reward. Thus, this work studies a new and general problem of OPL where the goal is to learn a policy that maximizes the expected target reward by leveraging densely observed secondary rewards as supplemental data. We then propose a new method called Hybrid Policy Optimization for Partially-Observed Reward (HyPeR), which effectively uses the secondary rewards in addition to the partially-observed target reward to achieve effective OPL despite the challenging scenario. We also discuss a case where we aim to optimize not only the expected target reward but also the expected secondary rewards to some extent; counter-intuitively, we will show that leveraging the two objectives is in fact advantageous also for the optimization of only the target reward. Along with statistical analysis of our proposed methods, empirical evaluations on both synthetic and real-world data show that HyPeR outperforms existing methods in various scenarios.

</details>


### [245] [Detecting immune cells with label-free two-photon autofluorescence and deep learning](https://arxiv.org/abs/2506.14449)
*Lucas Kreiss,Amey Chaware,Maryam Roohian,Sarah Lemire,Oana-Maria Thoma,Birgitta Carlé,Maximilian Waldner,Sebastian Schürmann,Oliver Friedrich,Roarke Horstmeyer*

Main category: cs.LG

TL;DR: 该论文提出了一种基于深度学习的无标记多光子显微镜（MPM）图像分析方法，用于免疫细胞分类，展示了其在体内内窥镜中的潜在应用。


<details>
  <summary>Details</summary>
Motivation: 无标记成像技术避免了复杂的染色过程，尤其适用于体内应用。然而，多光子显微镜（MPM）的计算特异性尚未充分开发，本研究旨在填补这一空白。

Method: 使用卷积神经网络（CNN）对无标记MPM图像中的免疫细胞进行分类，数据集包含5,075个细胞用于二元分类和3,424个细胞用于多类分类。

Result: 低复杂度的SqueezeNet架构在二元分类中表现良好（ROC-AUC为0.89，PR-AUC为0.95），在多类分类中也有不错的表现（F1得分为0.689）。扰动测试验证了模型的稳健性。

Conclusion: 深度学习模型可以显著提升无标记MPM的特异性，为体内内窥镜应用提供了潜力。

Abstract: Label-free imaging has gained broad interest because of its potential to omit elaborate staining procedures which is especially relevant for in vivo use. Label-free multiphoton microscopy (MPM), for instance, exploits two-photon excitation of natural autofluorescence (AF) from native, metabolic proteins, making it ideal for in vivo endomicroscopy. Deep learning (DL) models have been widely used in other optical imaging technologies to predict specific target annotations and thereby digitally augment the specificity of these label-free images. However, this computational specificity has only rarely been implemented for MPM. In this work, we used a data set of label-free MPM images from a series of different immune cell types (5,075 individual cells for binary classification in mixed samples and 3,424 cells for a multi-class classification task) and trained a convolutional neural network (CNN) to classify cell types based on this label-free AF as input. A low-complexity squeezeNet architecture was able to achieve reliable immune cell classification results (0.89 ROC-AUC, 0.95 PR-AUC, for binary classification in mixed samples; 0.689 F1 score, 0.697 precision, 0.748 recall, and 0.683 MCC for six-class classification in isolated samples). Perturbation tests confirmed that the model is not confused by extracellular environment and that both input AF channels (NADH and FAD) are about equally important to the classification. In the future, such predictive DL models could directly detect specific immune cells in unstained images and thus, computationally improve the specificity of label-free MPM which would have great potential for in vivo endomicroscopy.

</details>


### [246] [Dataset distillation for memorized data: Soft labels can leak held-out teacher knowledge](https://arxiv.org/abs/2506.14457)
*Freya Behrens,Lenka Zdeborová*

Main category: cs.LG

TL;DR: 研究发现，学生模型通过教师模型的软标签可以从未直接观察到的记忆数据中获得非平凡准确率，甚至在纯记忆场景下也能实现完美准确率。


<details>
  <summary>Details</summary>
Motivation: 探讨在数据蒸馏中，学生模型如何通过教师模型的软标签学习记忆数据，并分析其机制。

Method: 使用有限随机独立同分布数据集，确保教师模型仅能记忆数据，无法泛化，从而孤立分析记忆传递现象。

Result: 学生模型在某些情况下能完美匹配教师模型对记忆数据的预测，且这种现象受温度参数影响。

Conclusion: 记忆数据的传递现象在多种网络结构和数据集组成中普遍存在，但受温度参数调节。

Abstract: Dataset distillation aims to compress training data into fewer examples via a teacher, from which a student can learn effectively. While its success is often attributed to structure in the data, modern neural networks also memorize specific facts, but if and how such memorized information is can transferred in distillation settings remains less understood. In this work, we show that students trained on soft labels from teachers can achieve non-trivial accuracy on held-out memorized data they never directly observed. This effect persists on structured data when the teacher has not generalized.To analyze it in isolation, we consider finite random i.i.d. datasets where generalization is a priori impossible and a successful teacher fit implies pure memorization. Still, students can learn non-trivial information about the held-out data, in some cases up to perfect accuracy. In those settings, enough soft labels are available to recover the teacher functionally - the student matches the teacher's predictions on all possible inputs, including the held-out memorized data. We show that these phenomena strongly depend on the temperature with which the logits are smoothed, but persist across varying network capacities, architectures and dataset compositions.

</details>


### [247] [A Model-Mediated Stacked Ensemble Approach for Depression Prediction Among Professionals](https://arxiv.org/abs/2506.14459)
*Md. Mortuza Ahmmed,Abdullah Al Noman,Mahin Montasir Afif,K. M. Tahsin Kabir,Md. Mostafizur Rahman,Mufti Mahmud*

Main category: cs.LG

TL;DR: 该研究提出了一种基于堆叠的集成学习方法，用于提高专业人士抑郁症分类的预测准确性，取得了高精度结果。


<details>
  <summary>Details</summary>
Motivation: 抑郁症在职业环境中是一个严重的心理健康问题，传统分类方法难以应对其复杂性。

Method: 使用堆叠集成学习模型，结合多个基学习器和逻辑回归模型。

Result: 模型在训练和测试数据上分别达到99.64%和98.75%的准确率，其他指标均超过98%。

Conclusion: 集成学习在心理健康分析中具有高效性，可用于早期检测和干预。

Abstract: Depression is a significant mental health concern, particularly in professional environments where work-related stress, financial pressure, and lifestyle imbalances contribute to deteriorating well-being. Despite increasing awareness, researchers and practitioners face critical challenges in developing accurate and generalizable predictive models for mental health disorders. Traditional classification approaches often struggle with the complexity of depression, as it is influenced by multifaceted, interdependent factors, including occupational stress, sleep patterns, and job satisfaction. This study addresses these challenges by proposing a stacking-based ensemble learning approach to improve the predictive accuracy of depression classification among professionals. The Depression Professional Dataset has been collected from Kaggle. The dataset comprises demographic, occupational, and lifestyle attributes that influence mental well-being. Our stacking model integrates multiple base learners with a logistic regression-mediated model, effectively capturing diverse learning patterns. The experimental results demonstrate that the proposed model achieves high predictive performance, with an accuracy of 99.64% on training data and 98.75% on testing data, with precision, recall, and F1-score all exceeding 98%. These findings highlight the effectiveness of ensemble learning in mental health analytics and underscore its potential for early detection and intervention strategies.

</details>


### [248] [Zeroth-Order Optimization is Secretly Single-Step Policy Optimization](https://arxiv.org/abs/2506.14460)
*Junbin Qiu,Zhengpeng Xie,Xiangda Yan,Yongjie Yang,Yao Shu*

Main category: cs.LG

TL;DR: 本文揭示了零阶优化（ZOO）与单步策略优化（PO）的等价性，并提出了一种结合PO技术的ZOO新算法ZoAR，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 探讨ZOO方法的底层机制及其与强化学习（RL）的联系，填补理论空白。

Method: 通过数学证明ZOO与单步PO的等价性，并基于此提出ZoAR算法，引入平均基线和查询重用技术。

Result: 理论分析表明ZoAR能降低方差并加速收敛，实验验证其性能优于现有方法。

Conclusion: 研究为ZOO提供了新的理论视角，并通过与PO的联系实现了算法改进。

Abstract: Zeroth-Order Optimization (ZOO) provides powerful tools for optimizing functions where explicit gradients are unavailable or expensive to compute. However, the underlying mechanisms of popular ZOO methods, particularly those employing randomized finite differences, and their connection to other optimization paradigms like Reinforcement Learning (RL) are not fully elucidated. This paper establishes a fundamental and previously unrecognized connection: ZOO with finite differences is equivalent to a specific instance of single-step Policy Optimization (PO). We formally unveil that the implicitly smoothed objective function optimized by common ZOO algorithms is identical to a single-step PO objective. Furthermore, we show that widely used ZOO gradient estimators, are mathematically equivalent to the REINFORCE gradient estimator with a specific baseline function, revealing the variance-reducing mechanism in ZOO from a PO perspective.Built on this unified framework, we propose ZoAR (Zeroth-Order Optimization with Averaged Baseline and Query Reuse), a novel ZOO algorithm incorporating PO-inspired variance reduction techniques: an averaged baseline from recent evaluations and query reuse analogous to experience replay. Our theoretical analysis further substantiates these techniques reduce variance and enhance convergence. Extensive empirical studies validate our theory and demonstrate that ZoAR significantly outperforms other methods in terms of convergence speed and final performance. Overall, our work provides a new theoretical lens for understanding ZOO and offers practical algorithmic improvements derived from its connection to PO.

</details>


### [249] [Leveraging External Factors in Household-Level Electrical Consumption Forecasting using Hypernetworks](https://arxiv.org/abs/2506.14472)
*Fabien Bernier,Maxime Cordy,Yves Le Traon*

Main category: cs.LG

TL;DR: 论文提出了一种基于超网络架构的方法，通过结合外部因素（如天气指标）来提升全球电力消费预测模型的准确性。


<details>
  <summary>Details</summary>
Motivation: 传统时间序列预测依赖历史模式和时序依赖，但引入外部因素可能降低全局模型的性能。作者旨在解决这一问题。

Method: 采用超网络架构，根据每个消费者的特点调整模型权重，结合电力消费数据和外部因素（如天气、节假日等）。

Result: 实验表明，超网络方法在结合外部因素时优于现有方法，降低了预测误差并保持了全局模型的优势。

Conclusion: 超网络架构能有效提升全球电力消费预测的准确性，同时适应个体差异。

Abstract: Accurate electrical consumption forecasting is crucial for efficient energy management and resource allocation. While traditional time series forecasting relies on historical patterns and temporal dependencies, incorporating external factors -- such as weather indicators -- has shown significant potential for improving prediction accuracy in complex real-world applications. However, the inclusion of these additional features often degrades the performance of global predictive models trained on entire populations, despite improving individual household-level models. To address this challenge, we found that a hypernetwork architecture can effectively leverage external factors to enhance the accuracy of global electrical consumption forecasting models, by specifically adjusting the model weights to each consumer.
  We collected a comprehensive dataset spanning two years, comprising consumption data from over 6000 luxembourgish households and corresponding external factors such as weather indicators, holidays, and major local events. By comparing various forecasting models, we demonstrate that a hypernetwork approach outperforms existing methods when associated to external factors, reducing forecasting errors and achieving the best accuracy while maintaining the benefits of a global model.

</details>


### [250] [Train Once, Forget Precisely: Anchored Optimization for Efficient Post-Hoc Unlearning](https://arxiv.org/abs/2506.14515)
*Prabhav Sanga,Jaskaran Singh,Arun K. Dubey*

Main category: cs.LG

TL;DR: FAMR是一种高效的后处理遗忘框架，用于深度图像分类器，通过约束优化问题实现选择性遗忘，同时保留模型性能。


<details>
  <summary>Details</summary>
Motivation: 随着机器学习系统依赖受隐私监管的数据增加，选择性遗忘特定信息变得至关重要。

Method: FAMR将遗忘问题转化为约束优化问题，最小化遗忘集的均匀预测损失，并通过ℓ2惩罚锚定模型参数。

Result: 在CIFAR-10和ImageNet-100上的实验表明，FAMR在保留性能的同时实现了高效遗忘。

Conclusion: FAMR为视觉模型提供了一种可扩展且可验证的高效后处理遗忘方法。

Abstract: As machine learning systems increasingly rely on data subject to privacy regulation, selectively unlearning specific information from trained models has become essential. In image classification, this involves removing the influence of particular training samples, semantic classes, or visual styles without full retraining. We introduce \textbf{Forget-Aligned Model Reconstruction (FAMR)}, a theoretically grounded and computationally efficient framework for post-hoc unlearning in deep image classifiers. FAMR frames forgetting as a constrained optimization problem that minimizes a uniform-prediction loss on the forget set while anchoring model parameters to their original values via an $\ell_2$ penalty. A theoretical analysis links FAMR's solution to influence-function-based retraining approximations, with bounds on parameter and output deviation. Empirical results on class forgetting tasks using CIFAR-10 and ImageNet-100 demonstrate FAMR's effectiveness, with strong performance retention and minimal computational overhead. The framework generalizes naturally to concept and style erasure, offering a scalable and certifiable route to efficient post-hoc forgetting in vision models.

</details>


### [251] [Two-Player Zero-Sum Games with Bandit Feedback](https://arxiv.org/abs/2506.14518)
*Elif Yılmaz,Christos Dimitrakakis*

Main category: cs.LG

TL;DR: 研究了一种基于探索-利用-承诺（ETC）的算法在双人零和博弈中的应用，提出了两种算法ETC-TPZSG和ETC-TPZSG-AE，并分析了其性能。


<details>
  <summary>Details</summary>
Motivation: 探讨ETC算法在双人零和博弈中的适用性，特别是在未知收益矩阵的情况下，通过纯策略纳什均衡学习实现高效对抗。

Method: 提出了ETC-TPZSG算法及其改进版ETC-TPZSG-AE，后者通过动作对消除策略优化选择效率。

Result: 推导了两种算法的实例相关遗憾上界，ETC-TPZSG为O(Δ+√T)，ETC-TPZSG-AE为O(log(TΔ²)/Δ)。

Conclusion: ETC算法在对抗性博弈中表现良好，遗憾界与现有方法相当，并通过实例相关分析提供了新见解。

Abstract: We study a two-player zero-sum game (TPZSG) in which the row player aims to maximize their payoff against an adversarial column player, under an unknown payoff matrix estimated through bandit feedback. We propose and analyze two algorithms: ETC-TPZSG, which directly applies ETC to the TPZSG setting and ETC-TPZSG-AE, which improves upon it by incorporating an action pair elimination (AE) strategy that leverages the $\varepsilon$-Nash Equilibrium property to efficiently select the optimal action pair. Our objective is to demonstrate the applicability of ETC in a TPZSG setting by focusing on learning pure strategy Nash Equilibrium. A key contribution of our work is a derivation of instance-dependent upper bounds on the expected regret for both algorithms, has received limited attention in the literature on zero-sum games. Particularly, after $T$ rounds, we achieve an instance-dependent regret upper bounds of $O(Δ+ \sqrt{T})$ for ETC-TPZSG and $O(\frac{\log (T Δ^2)}Δ)$ for ETC-TPZSG-AE, where $Δ$ denotes the suboptimality gap. Therefore, our results indicate that ETC-based algorithms perform effectively in adversarial game settings, achieving regret bounds comparable to existing methods while providing insights through instance-dependent analysis.

</details>


### [252] [Towards Improved Research Methodologies for Industrial AI: A case study of false call reduction](https://arxiv.org/abs/2506.14521)
*Korbinian Pfab,Marcel Rothering*

Main category: cs.LG

TL;DR: 本文通过工业AI案例研究，指出当前AI研究方法的不足，并提出改进建议。


<details>
  <summary>Details</summary>
Motivation: 探讨当前AI研究方法是否足以开发成功的商业应用，揭示其局限性。

Method: 通过工业案例（自动光学检测中的误报减少）分析，识别七种常见问题并实验验证其影响。

Result: 证明当前最佳实践方法在此案例中无效，强调需求感知指标和成功标准定义的重要性。

Conclusion: 呼吁研究者批判性评估方法，以推动更成功的应用AI研究。

Abstract: Are current artificial intelligence (AI) research methodologies ready to create successful, productive, and profitable AI applications? This work presents a case study on an industrial AI use case called false call reduction for automated optical inspection to demonstrate the shortcomings of current best practices. We identify seven weaknesses prevalent in related peer-reviewed work and experimentally show their consequences. We show that the best-practice methodology would fail for this use case. We argue amongst others for the necessity of requirement-aware metrics to ensure achieving business objectives, clear definitions of success criteria, and a thorough analysis of temporal dynamics in experimental datasets. Our work encourages researchers to critically assess their methodologies for more successful applied AI research.

</details>


### [253] [Automated Decision-Making on Networks with LLMs through Knowledge-Guided Evolution](https://arxiv.org/abs/2506.14529)
*Xiaohan Zheng,Lanning Wei,Yong Li,Quanming Yao*

Main category: cs.LG

TL;DR: LLMNet利用大型语言模型自动设计GNN，通过知识库和检索增强生成技术优化模型配置，在多个数据集和任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: 传统GNN需要大量手动配置和调优，LLMNet旨在通过自动化设计简化这一过程。

Method: LLMNet通过构建图相关知识库，利用检索增强生成技术（RAG）自动配置和优化GNN模型，知识库支持任务和结构的分析。

Result: 在三个图学习任务的十二个数据集中，LLMNet表现优异，验证了其自动化设计GNN的有效性。

Conclusion: LLMNet通过知识库和RAG技术实现了GNN的自动化设计，显著提升了效率和性能。

Abstract: Effective decision-making on networks often relies on learning from graph-structured data, where Graph Neural Networks (GNNs) play a central role, but they take efforts to configure and tune. In this demo, we propose LLMNet, showing how to design GNN automated through Large Language Models. Our system develops a set of agents that construct graph-related knowlege bases and then leverages Retrieval-Augmented Generation (RAG) to support automated configuration and refinement of GNN models through a knowledge-guided evolution process. These agents, equipped with specialized knowledge bases, extract insights into tasks and graph structures by interacting with the knowledge bases. Empirical results show LLMNet excels in twelve datasets across three graph learning tasks, validating its effectiveness of GNN model designing.

</details>


### [254] [Aligning Evaluation with Clinical Priorities: Calibration, Label Shift, and Error Costs](https://arxiv.org/abs/2506.14540)
*Gerardo A. Flores,Alyssa H. Smith,Julia A. Fukuyama,Ashia C. Wilson*

Main category: cs.LG

TL;DR: 本文提出了一种基于概率评分规则的临床决策支持系统评估框架，强调校准性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有评分规则（如准确率和AUC-ROC）未能充分反映临床优先级（如校准性、分布偏移鲁棒性和非对称错误成本敏感性）。

Method: 基于Schervish表示理论，提出了一种调整后的交叉熵变体，用于评估校准阈值分类器。

Result: 该方法简单易用，对临床部署条件敏感，优先选择校准且鲁棒的模型。

Conclusion: 该框架为临床决策支持系统提供了更符合实际需求的评估标准。

Abstract: Machine learning-based decision support systems are increasingly deployed in clinical settings, where probabilistic scoring functions are used to inform and prioritize patient management decisions. However, widely used scoring rules, such as accuracy and AUC-ROC, fail to adequately reflect key clinical priorities, including calibration, robustness to distributional shifts, and sensitivity to asymmetric error costs. In this work, we propose a principled yet practical evaluation framework for selecting calibrated thresholded classifiers that explicitly accounts for the uncertainty in class prevalences and domain-specific cost asymmetries often found in clinical settings. Building on the theory of proper scoring rules, particularly the Schervish representation, we derive an adjusted variant of cross-entropy (log score) that averages cost-weighted performance over clinically relevant ranges of class balance. The resulting evaluation is simple to apply, sensitive to clinical deployment conditions, and designed to prioritize models that are both calibrated and robust to real-world variations.

</details>


### [255] [Single-Example Learning in a Mixture of GPDMs with Latent Geometries](https://arxiv.org/abs/2506.14563)
*Jesse St. Amand,Leonardo Gizzi,Martin A. Giese*

Main category: cs.LG

TL;DR: GPDMM是一种结合GPDM和概率混合专家框架的模型，用于单样本学习人类运动数据，适用于数据有限且需高解释性的场景。


<details>
  <summary>Details</summary>
Motivation: 解决在数据有限（如医疗应用中患者特定数据）且需要高模型解释性的情况下，建模人类运动的挑战。

Method: 结合多个GPDM，利用概率混合专家框架和几何特征，在单一潜在空间中编码多样化序列。

Result: 在分类准确性和生成能力上表现优异，优于LSTM、VAE和Transformer等模型。

Conclusion: GPDMM在单样本学习和高解释性需求场景中具有显著优势。

Abstract: We present the Gaussian process dynamical mixture model (GPDMM) and show its utility in single-example learning of human motion data. The Gaussian process dynamical model (GPDM) is a form of the Gaussian process latent variable model (GPLVM), but optimized with a hidden Markov model dynamical prior. The GPDMM combines multiple GPDMs in a probabilistic mixture-of-experts framework, utilizing embedded geometric features to allow for diverse sequences to be encoded in a single latent space, enabling the categorization and generation of each sequence class. GPDMs and our mixture model are particularly advantageous in addressing the challenges of modeling human movement in scenarios where data is limited and model interpretability is vital, such as in patient-specific medical applications like prosthesis control. We score the GPDMM on classification accuracy and generative ability in single-example learning, showcase model variations, and benchmark it against LSTMs, VAEs, and transformers.

</details>


### [256] [TGDPO: Harnessing Token-Level Reward Guidance for Enhancing Direct Preference Optimization](https://arxiv.org/abs/2506.14574)
*Mingkang Zhu,Xi Chen,Zhongdao Wang,Bei Yu,Hengshuang Zhao,Jiaya Jia*

Main category: cs.LG

TL;DR: 论文提出了一种方法，将序列级的PPO分解为一系列标记级的PPO问题，并利用标记级奖励指导DPO，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法难以将标记级奖励用于DPO，因为DPO是序列级问题。

Method: 将序列级PPO分解为标记级问题，推导出最优标记级策略和奖励，并基于此设计DPO的可计算损失函数。

Result: 在多个基准测试中，性能显著提升，MT-Bench提升7.5分，AlpacaEval 2提升6.2分，Arena-Hard提升4.3分。

Conclusion: 该方法成功解决了标记级奖励用于DPO的挑战，并显著提升了模型性能。

Abstract: Recent advancements in reinforcement learning from human feedback have shown that utilizing fine-grained token-level reward models can substantially enhance the performance of Proximal Policy Optimization (PPO) in aligning large language models. However, it is challenging to leverage such token-level reward as guidance for Direct Preference Optimization (DPO), since DPO is formulated as a sequence-level bandit problem. To address this challenge, this work decomposes the sequence-level PPO into a sequence of token-level proximal policy optimization problems and then frames the problem of token-level PPO with token-level reward guidance, from which closed-form optimal token-level policy and the corresponding token-level reward can be derived. Using the obtained reward and Bradley-Terry model, this work establishes a framework of computable loss functions with token-level reward guidance for DPO, and proposes a practical reward guidance based on the induced DPO reward. This formulation enables different tokens to exhibit varying degrees of deviation from reference policy based on their respective rewards. Experiment results demonstrate that our method achieves substantial performance improvements over DPO, with win rate gains of up to 7.5 points on MT-Bench, 6.2 points on AlpacaEval 2, and 4.3 points on Arena-Hard. Code is available at https://github.com/dvlab-research/TGDPO.

</details>


### [257] [Object-Centric Neuro-Argumentative Learning](https://arxiv.org/abs/2506.14577)
*Abdul Rahman Jacob,Avinash Kori,Emanuele De Angelis,Ben Glocker,Maurizio Proietti,Francesca Toni*

Main category: cs.LG

TL;DR: 论文提出了一种结合假设基础论证（ABA）与深度学习的神经论证学习（NAL）架构，用于提升图像分析的安全性、可靠性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 随着深度学习在关键决策中的应用增加，其安全性、可靠性和可解释性问题日益突出，需要新的解决方案。

Method: NAL架构包含神经和符号组件：前者通过对象中心学习分割和编码图像为事实，后者利用ABA学习构建框架以支持图像预测。

Result: 在合成数据上的实验表明，NAL架构与现有最优方法具有竞争力。

Conclusion: NAL架构为深度学习的安全性和可解释性提供了新的研究方向。

Abstract: Over the last decade, as we rely more on deep learning technologies to make critical decisions, concerns regarding their safety, reliability and interpretability have emerged. We introduce a novel Neural Argumentative Learning (NAL) architecture that integrates Assumption-Based Argumentation (ABA) with deep learning for image analysis. Our architecture consists of neural and symbolic components. The former segments and encodes images into facts using object-centric learning, while the latter applies ABA learning to develop ABA frameworks enabling predictions with images. Experiments on synthetic data show that the NAL architecture can be competitive with a state-of-the-art alternative.

</details>


### [258] [SCISSOR: Mitigating Semantic Bias through Cluster-Aware Siamese Networks for Robust Classification](https://arxiv.org/abs/2506.14587)
*Shuo Yang,Bardh Prenkaj,Gjergji Kasneci*

Main category: cs.LG

TL;DR: SCISSOR提出了一种基于Siamese网络的去偏方法，通过抑制语义捷径提升模型泛化能力，在多个基准测试中表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要关注浅层特征的偏差，而忽略了语义分布不平衡导致的虚假相关性，这影响了模型的鲁棒性。

Method: SCISSOR通过Siamese网络重新映射语义空间，抑制被用作捷径的潜在聚类，无需数据增强或重写。

Result: 在6个模型和4个基准测试中，SCISSOR显著提升了性能，如GYAFC上F1分数提升5.3分，Yelp提升7.3分。

Conclusion: SCISSOR通过解决语义偏差，成为抑制捷径学习的基础框架，推动了更鲁棒、抗偏见的AI系统发展。

Abstract: Shortcut learning undermines model generalization to out-of-distribution data. While the literature attributes shortcuts to biases in superficial features, we show that imbalances in the semantic distribution of sample embeddings induce spurious semantic correlations, compromising model robustness. To address this issue, we propose SCISSOR (Semantic Cluster Intervention for Suppressing ShORtcut), a Siamese network-based debiasing approach that remaps the semantic space by discouraging latent clusters exploited as shortcuts. Unlike prior data-debiasing approaches, SCISSOR eliminates the need for data augmentation and rewriting. We evaluate SCISSOR on 6 models across 4 benchmarks: Chest-XRay and Not-MNIST in computer vision, and GYAFC and Yelp in NLP tasks. Compared to several baselines, SCISSOR reports +5.3 absolute points in F1 score on GYAFC, +7.3 on Yelp, +7.7 on Chest-XRay, and +1 on Not-MNIST. SCISSOR is also highly advantageous for lightweight models with ~9.5% improvement on F1 for ViT on computer vision datasets and ~11.9% for BERT on NLP. Our study redefines the landscape of model generalization by addressing overlooked semantic biases, establishing SCISSOR as a foundational framework for mitigating shortcut learning and fostering more robust, bias-resistant AI systems.

</details>


### [259] [Deep Learning Surrogates for Real-Time Gas Emission Inversion](https://arxiv.org/abs/2506.14597)
*Thomas Newman,Christopher Nemeth,Matthew Jones,Philip Jonathan*

Main category: cs.LG

TL;DR: 提出了一种结合深度学习与贝叶斯推断的时空反演框架，用于实时识别和量化动态流场中的温室气体排放。


<details>
  <summary>Details</summary>
Motivation: 实时监测动态大气条件下的温室气体排放是环境监测的关键挑战。

Method: 将多层感知器作为计算流体动力学（CFD）的替代模型，嵌入序列蒙特卡洛算法中进行贝叶斯推断。

Result: 在验证数据集上表现出与完整CFD求解器相当的精度，但运行速度快数个数量级。

Conclusion: 该方法在物理保真度和计算可行性之间取得了平衡，适用于工业排放监测等时间敏感的时空反演任务。

Abstract: Real-time identification and quantification of greenhouse-gas emissions under transient atmospheric conditions is a critical challenge in environmental monitoring. We introduce a spatio-temporal inversion framework that embeds a deep-learning surrogate of computational fluid dynamics (CFD) within a sequential Monte Carlo algorithm to perform Bayesian inference of both emission rate and source location in dynamic flow fields. By substituting costly numerical solvers with a multilayer perceptron trained on high-fidelity CFD outputs, our surrogate captures spatial heterogeneity and temporal evolution of gas dispersion, while delivering near-real-time predictions. Validation on the Chilbolton methane release dataset demonstrates comparable accuracy to full CFD solvers and Gaussian plume models, yet achieves orders-of-magnitude faster runtimes. Further experiments under simulated obstructed-flow scenarios confirm robustness in complex environments. This work reconciles physical fidelity with computational feasibility, offering a scalable solution for industrial emissions monitoring and other time-sensitive spatio-temporal inversion tasks in environmental and scientific modeling.

</details>


### [260] [Expressive Score-Based Priors for Distribution Matching with Geometry-Preserving Regularization](https://arxiv.org/abs/2506.14607)
*Ziyu Gong,Jim Lim,David I. Inouye*

Main category: cs.LG

TL;DR: 提出了一种基于分数先验分布的新型分布匹配方法，解决了现有方法的局限性，如固定先验偏差和密度模型训练困难。


<details>
  <summary>Details</summary>
Motivation: 现有分布匹配方法存在可扩展性、不稳定性和模式崩溃等问题，而基于似然的方法又因固定先验或显式密度模型引入偏差或训练困难。

Method: 利用表达性分数先验分布训练似然分布匹配，通过去噪分数匹配训练先验，避免固定先验偏差和显式密度模型需求。

Result: 方法在稳定性和计算效率上优于其他扩散先验方法，并在多个任务中表现优异。

Conclusion: 基于分数的方法是一种稳定有效的分布匹配方法，代码已开源。

Abstract: Distribution matching (DM) is a versatile domain-invariant representation learning technique that has been applied to tasks such as fair classification, domain adaptation, and domain translation. Non-parametric DM methods struggle with scalability and adversarial DM approaches suffer from instability and mode collapse. While likelihood-based methods are a promising alternative, they often impose unnecessary biases through fixed priors or require explicit density models (e.g., flows) that can be challenging to train. We address this limitation by introducing a novel approach to training likelihood-based DM using expressive score-based prior distributions. Our key insight is that gradient-based DM training only requires the prior's score function -- not its density -- allowing us to train the prior via denoising score matching. This approach eliminates biases from fixed priors (e.g., in VAEs), enabling more effective use of geometry-preserving regularization, while avoiding the challenge of learning an explicit prior density model (e.g., a flow-based prior). Our method also demonstrates better stability and computational efficiency compared to other diffusion-based priors (e.g., LSGM). Furthermore, experiments demonstrate superior performance across multiple tasks, establishing our score-based method as a stable and effective approach to distribution matching. Source code available at https://github.com/inouye-lab/SAUB.

</details>


### [261] [Feasibility-Driven Trust Region Bayesian Optimization](https://arxiv.org/abs/2506.14619)
*Paolo Ascia,Elena Raponi,Thomas Bäck,Fabian Duddeck*

Main category: cs.LG

TL;DR: 提出了一种基于可行性的信任区域贝叶斯优化算法（FuRBO），用于解决高维空间中昂贵约束的优化问题，显著加速可行解的发现。


<details>
  <summary>Details</summary>
Motivation: 现实中的优化任务常涉及高维空间中的昂贵约束，可行区域小且不规则，现有方法在寻找初始可行解时消耗大量预算。

Method: FuRBO通过迭代定义信任区域，结合目标和约束的代理模型信息，动态调整信任区域以快速聚焦搜索。

Result: 在BBOB-constrained COCO基准测试和其他物理启发基准上，FuRBO显著优于现有方法，适用于2至60维问题。

Conclusion: FuRBO通过自适应策略有效加速可行解的发现，为高维昂贵约束优化提供了高效解决方案。

Abstract: Bayesian optimization is a powerful tool for solving real-world optimization tasks under tight evaluation budgets, making it well-suited for applications involving costly simulations or experiments. However, many of these tasks are also characterized by the presence of expensive constraints whose analytical formulation is unknown and often defined in high-dimensional spaces where feasible regions are small, irregular, and difficult to identify. In such cases, a substantial portion of the optimization budget may be spent just trying to locate the first feasible solution, limiting the effectiveness of existing methods. In this work, we present a Feasibility-Driven Trust Region Bayesian Optimization (FuRBO) algorithm. FuRBO iteratively defines a trust region from which the next candidate solution is selected, using information from both the objective and constraint surrogate models. Our adaptive strategy allows the trust region to shift and resize significantly between iterations, enabling the optimizer to rapidly refocus its search and consistently accelerate the discovery of feasible and good-quality solutions. We empirically demonstrate the effectiveness of FuRBO through extensive testing on the full BBOB-constrained COCO benchmark suite and other physics-inspired benchmarks, comparing it against state-of-the-art baselines for constrained black-box optimization across varying levels of constraint severity and problem dimensionalities ranging from 2 to 60.

</details>


### [262] [Towards Desiderata-Driven Design of Visual Counterfactual Explainers](https://arxiv.org/abs/2506.14698)
*Sidney Bender,Jan Herrmann,Klaus-Robert Müller,Grégoire Montavon*

Main category: cs.LG

TL;DR: 本文提出了一种新的视觉反事实解释方法（SCE），旨在弥补现有方法在解释全面性上的不足。


<details>
  <summary>Details</summary>
Motivation: 现有视觉反事实解释器（VCEs）过于关注样本质量或最小变化，忽视了解释的忠实性、可理解性和充分性等更全面的需求。

Method: 探索了新的反事实生成机制，并将其整合为一种新颖的“平滑反事实探索器”（SCE）算法。

Result: 通过合成数据和真实数据的系统评估，验证了SCE算法的有效性。

Conclusion: SCE算法能够更好地满足解释的全面需求，提升图像分类器的透明度。

Abstract: Visual counterfactual explainers (VCEs) are a straightforward and promising approach to enhancing the transparency of image classifiers. VCEs complement other types of explanations, such as feature attribution, by revealing the specific data transformations to which a machine learning model responds most strongly. In this paper, we argue that existing VCEs focus too narrowly on optimizing sample quality or change minimality; they fail to consider the more holistic desiderata for an explanation, such as fidelity, understandability, and sufficiency. To address this shortcoming, we explore new mechanisms for counterfactual generation and investigate how they can help fulfill these desiderata. We combine these mechanisms into a novel 'smooth counterfactual explorer' (SCE) algorithm and demonstrate its effectiveness through systematic evaluations on synthetic and real data.

</details>


### [263] [On the Hardness of Bandit Learning](https://arxiv.org/abs/2506.14746)
*Nataly Brukhim,Aldo Pacchiano,Miroslav Dudik,Robert Schapire*

Main category: cs.LG

TL;DR: 本文研究了在已知函数类F下的多臂老虎机学习问题，探讨了哪些函数类可学习以及如何学习的问题，揭示了结构化老虎机学习的局限性。


<details>
  <summary>Details</summary>
Motivation: 研究多臂老虎机学习的普适理论，类似于分类中的PAC框架，旨在理解哪些函数类可学习及其学习方式。

Method: 通过组合维度和计算复杂度分析，探讨了函数类的可学习性，并构造了一个奖励函数类进行实验验证。

Result: 发现组合维度无法表征老虎机学习的可学习性，且存在计算复杂度问题，即使简单查询也无法在多项式时间内完成。

Conclusion: 结构化老虎机学习存在固有局限性，计算复杂度是其核心挑战之一。

Abstract: We study the task of bandit learning, also known as best-arm identification, under the assumption that the true reward function f belongs to a known, but arbitrary, function class F. We seek a general theory of bandit learnability, akin to the PAC framework for classification. Our investigation is guided by the following two questions: (1) which classes F are learnable, and (2) how they are learnable. For example, in the case of binary PAC classification, learnability is fully determined by a combinatorial dimension - the VC dimension- and can be attained via a simple algorithmic principle, namely, empirical risk minimization (ERM). In contrast to classical learning-theoretic results, our findings reveal limitations of learning in structured bandits, offering insights into the boundaries of bandit learnability. First, for the question of "which", we show that the paradigm of identifying the learnable classes via a dimension-like quantity fails for bandit learning. We give a simple proof demonstrating that no combinatorial dimension can characterize bandit learnability, even in finite classes, following a standard definition of dimension introduced by Ben-David et al. (2019). For the question of "how", we prove a computational hardness result: we construct a reward function class for which at most two queries are needed to find the optimal action, yet no algorithm can do so in polynomial time unless RP=NP. We also prove that this class admits efficient algorithms for standard algorithmic operations often considered in learning theory, such as an ERM. This implies that computational hardness is in this case inherent to the task of bandit learning. Beyond these results, we investigate additional themes such as learning under noise, trade-offs between noise models, and the relationship between query complexity and regret minimization.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [264] [AgentFacts: Universal KYA Standard for Verified AI Agent Metadata & Deployment](https://arxiv.org/abs/2506.13794)
*Jared James Grogan*

Main category: cs.MA

TL;DR: AgentFacts是一个通用元数据标准，通过加密签名能力声明、多权威验证和动态权限管理，解决企业AI部署中的信任问题。


<details>
  <summary>Details</summary>
Motivation: 企业AI部署面临第三方代理能力验证和信任建立的挑战，缺乏标准化元数据或验证基础设施。

Method: 提出AgentFacts标准，支持加密签名能力声明、多权威验证和动态权限管理，实现域专业化验证。

Result: AgentFacts将代理采购从定制集成项目转变为标准化劳动力管理，提供透明度和治理基础设施。

Conclusion: AgentFacts为企业AI规模化协调提供了必要的信任和治理框架。

Abstract: Enterprise AI deployment faces critical "Know Your Agent" (KYA) challenges where organizations must verify third-party agent capabilities and establish trust without standardized metadata or verification infrastructure. Current approaches rely on self-declared capabilities and custom integration processes that create trust gaps and coordination friction limiting confident enterprise adoption. This paper presents AgentFacts, a universal metadata standard that enables systematic agent verification through cryptographically-signed capability declarations, multi-authority validation, and dynamic permission management. The specification introduces domain-specialized verification where different trusted authorities validate specific metadata aspects based on their expertise, eliminating single points of trust failure while enabling graduated confidence assessment. AgentFacts transforms agent procurement from custom integration projects into standardized workforce management, providing the transparency and governance infrastructure necessary for enterprise AI coordination at scale.

</details>


### [265] [Investigating the Potential of Large Language Model-Based Router Multi-Agent Architectures for Foundation Design Automation: A Task Classification and Expert Selection Study](https://arxiv.org/abs/2506.13811)
*Sompote Youwai,David Phim,Vianne Gayl Murcia,Rianne Clair Onas*

Main category: cs.MA

TL;DR: 研究评估了基于路由器的多智能体系统在基础设计自动化中的表现，显示其优于传统方法，并强调了人类监督的必要性。


<details>
  <summary>Details</summary>
Motivation: 探索智能任务分类和专家选择在基础设计计算自动化中的应用，以提高效率和准确性。

Method: 比较了单智能体处理、多智能体设计-检查架构和基于路由器的专家选择三种方法，使用多种基线模型进行评估。

Result: 基于路由器的配置在浅基础和桩基础设计中分别达到95.00%和90.63%的性能，显著优于传统方法。

Conclusion: 基于路由器的多智能体系统是基础设计自动化的最佳选择，但仍需人类监督以确保安全。

Abstract: This study investigates router-based multi-agent systems for automating foundation design calculations through intelligent task classification and expert selection. Three approaches were evaluated: single-agent processing, multi-agent designer-checker architecture, and router-based expert selection. Performance assessment utilized baseline models including DeepSeek R1, ChatGPT 4 Turbo, Grok 3, and Gemini 2.5 Pro across shallow foundation and pile design scenarios. The router-based configuration achieved performance scores of 95.00% for shallow foundations and 90.63% for pile design, representing improvements of 8.75 and 3.13 percentage points over standalone Grok 3 performance respectively. The system outperformed conventional agentic workflows by 10.0 to 43.75 percentage points. Grok 3 demonstrated superior standalone performance without external computational tools, indicating advances in direct LLM mathematical reasoning for engineering applications. The dual-tier classification framework successfully distinguished foundation types, enabling appropriate analytical approaches. Results establish router-based multi-agent systems as optimal for foundation design automation while maintaining professional documentation standards. Given safety-critical requirements in civil engineering, continued human oversight remains essential, positioning these systems as advanced computational assistance tools rather than autonomous design replacements in professional practice.

</details>


### [266] [Hierarchical Multi-Agent Reinforcement Learning-based Coordinated Spatial Reuse for Next Generation WLANs](https://arxiv.org/abs/2506.14187)
*Jiaming Yu,Le Liang,Hao Ye,Shi Jin*

Main category: cs.MA

TL;DR: 论文提出了一种基于分层多智能体强化学习（HMARL）的方法，用于解决Wi-Fi网络中下行链路空间复用的挑战，显著提升了网络吞吐量和延迟性能。


<details>
  <summary>Details</summary>
Motivation: 高密度Wi-Fi部署中，同频干扰严重影响了网络性能，需要协调多接入点（APs）实现空间复用（CSR）。

Method: 将CSR过程分解为轮询和决策两阶段，并采用HMARL算法，通过分层结构分别处理站点选择和功率控制。

Result: 仿真结果表明，该方法在吞吐量和延迟上优于基线方法，且在共存传统APs时表现稳健。

Conclusion: HMARL算法不仅提升了整体网络吞吐量，还改善了高干扰区域APs的传输公平性。

Abstract: High-density Wi-Fi deployments often result in significant co-channel interference, which degrades overall network performance. To address this issue, coordination of multi access points (APs) has been considered to enable coordinated spatial reuse (CSR) in next generation wireless local area networks. This paper tackles the challenge of downlink spatial reuse in Wi-Fi networks, specifically in scenarios involving overlapping basic service sets, by employing hierarchical multi-agent reinforcement learning (HMARL). We decompose the CSR process into two phases, i.e., a polling phase and a decision phase, and introduce the HMARL algorithm to enable efficient CSR. To enhance training efficiency, the proposed HMARL algorithm employs a hierarchical structure, where station selection and power control are determined by a high- and low-level policy network, respectively. Simulation results demonstrate that this approach consistently outperforms baseline methods in terms of throughput and latency across various network topologies. Moreover, the algorithm exhibits robust performance when coexisting with legacy APs. Additional experiments in a representative topology further reveal that the carefully designed reward function not only maximizes the overall network throughput, but also improves fairness in transmission opportunities for APs in high-interference regions.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [267] [ATK: Automatic Task-driven Keypoint Selection for Robust Policy Learning](https://arxiv.org/abs/2506.13867)
*Yunchu Zhang,Shubham Mittal,Zhengyu Zhang,Liyiming Ke,Siddhartha Srinivasa,Abhishek Gupta*

Main category: cs.RO

TL;DR: 论文提出了一种名为ATK的方法，通过自动选择任务相关的2D关键点，提升视觉运动策略的鲁棒性和适应性。


<details>
  <summary>Details</summary>
Motivation: 视觉运动策略在训练和评估环境中的视觉差异会导致性能下降，而依赖状态估计或原始传感器数据的策略各有局限性。

Method: 提出ATK方法，自动选择任务相关的关键点，并通过专家数据训练策略，利用预训练视觉模块编码状态。

Result: 实验表明，ATK选择的最小关键点集显著提高了对视觉干扰和环境变化的鲁棒性。

Conclusion: ATK方法通过任务驱动的关键点选择，有效提升了策略的适应性和鲁棒性。

Abstract: Visuomotor policies often suffer from perceptual challenges, where visual differences between training and evaluation environments degrade policy performance. Policies relying on state estimations, like 6D pose, require task-specific tracking and are difficult to scale, while raw sensor-based policies may lack robustness to small visual disturbances.In this work, we leverage 2D keypoints - spatially consistent features in the image frame - as a flexible state representation for robust policy learning and apply it to both sim-to-real transfer and real-world imitation learning. However, the choice of which keypoints to use can vary across objects and tasks. We propose a novel method, ATK, to automatically select keypoints in a task-driven manner so that the chosen keypoints are predictive of optimal behavior for the given task. Our proposal optimizes for a minimal set of keypoints that focus on task-relevant parts while preserving policy performance and robustness. We distill expert data (either from an expert policy in simulation or a human expert) into a policy that operates on RGB images while tracking the selected keypoints. By leveraging pre-trained visual modules, our system effectively encodes states and transfers policies to the real-world evaluation scenario despite wide scene variations and perceptual challenges such as transparent objects, fine-grained tasks, and deformable objects manipulation. We validate ATK on various robotic tasks, demonstrating that these minimal keypoint representations significantly improve robustness to visual disturbances and environmental variations. See all experiments and more details on our website.

</details>


### [268] [Sequence Modeling for Time-Optimal Quadrotor Trajectory Optimization with Sampling-based Robustness Analysis](https://arxiv.org/abs/2506.13915)
*Katherine Mao,Hongzhan Yu,Ruipeng Zhang,Igor Spasojevic,M Ani Hsieh,Sicun Gao,Vijay Kumar*

Main category: cs.RO

TL;DR: 该论文提出了一种基于学习的方法，通过模仿模型优化的时间最优轨迹规划器，加速轨迹生成，并在硬件平台上验证了实时可行性。


<details>
  <summary>Details</summary>
Motivation: 传统时间最优轨迹计算涉及非凸问题，迭代非线性优化成本高，难以实时应用。

Method: 利用学习模型模仿模型优化的轨迹规划器，通过数据集训练模型学习时间最优轨迹的模式，并引入数据增强提高鲁棒性。

Result: 相比传统规划器，该方法显著提速，并在硬件平台上验证了实时可行性，且能泛化到未见过的路径长度。

Conclusion: 学习模型能有效加速时间最优轨迹生成，具有实时应用潜力。

Abstract: Time-optimal trajectories drive quadrotors to their dynamic limits, but computing such trajectories involves solving non-convex problems via iterative nonlinear optimization, making them prohibitively costly for real-time applications. In this work, we investigate learning-based models that imitate a model-based time-optimal trajectory planner to accelerate trajectory generation. Given a dataset of collision-free geometric paths, we show that modeling architectures can effectively learn the patterns underlying time-optimal trajectories. We introduce a quantitative framework to analyze local analytic properties of the learned models, and link them to the Backward Reachable Tube of the geometric tracking controller. To enhance robustness, we propose a data augmentation scheme that applies random perturbations to the input paths. Compared to classical planners, our method achieves substantial speedups, and we validate its real-time feasibility on a hardware quadrotor platform. Experiments demonstrate that the learned models generalize to previously unseen path lengths. The code for our approach can be found here: https://github.com/maokat12/lbTOPPQuad

</details>


### [269] [DynaGuide: Steering Diffusion Polices with Active Dynamic Guidance](https://arxiv.org/abs/2506.13922)
*Maximilian Du,Shuran Song*

Main category: cs.RO

TL;DR: DynaGuide是一种通过外部动力学模型引导扩散去噪过程的策略调整方法，无需预先训练目标分布，支持多目标调整和增强基础策略行为。


<details>
  <summary>Details</summary>
Motivation: 现实世界中部署复杂策略需要灵活调整以适应不同情境，传统方法（如目标条件化）需预先训练目标分布，限制了灵活性。

Method: DynaGuide利用外部动力学模型在扩散去噪过程中提供引导信号，将动力学模型与基础策略分离，支持多目标调整和增强行为。

Result: 在模拟和真实实验中，DynaGuide平均调整成功率为70%，在低质量目标下表现优于目标条件化方法5.4倍。

Conclusion: DynaGuide提供了一种灵活且鲁棒的策略调整方法，适用于预训练扩散策略，并能创造新行为。

Abstract: Deploying large, complex policies in the real world requires the ability to steer them to fit the needs of a situation. Most common steering approaches, like goal-conditioning, require training the robot policy with a distribution of test-time objectives in mind. To overcome this limitation, we present DynaGuide, a steering method for diffusion policies using guidance from an external dynamics model during the diffusion denoising process. DynaGuide separates the dynamics model from the base policy, which gives it multiple advantages, including the ability to steer towards multiple objectives, enhance underrepresented base policy behaviors, and maintain robustness on low-quality objectives. The separate guidance signal also allows DynaGuide to work with off-the-shelf pretrained diffusion policies. We demonstrate the performance and features of DynaGuide against other steering approaches in a series of simulated and real experiments, showing an average steering success of 70% on a set of articulated CALVIN tasks and outperforming goal-conditioning by 5.4x when steered with low-quality objectives. We also successfully steer an off-the-shelf real robot policy to express preference for particular objects and even create novel behavior. Videos and more can be found on the project website: https://dynaguide.github.io

</details>


### [270] [TUM Teleoperation: Open Source Software for Remote Driving and Assistance of Automated Vehicles](https://arxiv.org/abs/2506.13933)
*Tobias Kerbl,David Brecht,Nils Gehrke,Nijinshan Karunainayagam,Niklas Krauss,Florian Pfab,Richard Taupitz,Ines Trautmannsheimer,Xiyan Su,Maria-Magdalena Wolf,Frank Diermeyer*

Main category: cs.RO

TL;DR: 论文提出了一种模块化、开源的远程操作软件栈，支持远程驾驶和远程辅助，填补了现有开源软件的空白。


<details>
  <summary>Details</summary>
Motivation: 目前缺乏结合远程驾驶、远程辅助并与真实车辆集成的开源软件，论文旨在解决这一问题。

Method: 开发了一个模块化的开源软件栈，支持与自动驾驶软件（如Autoware）交互，提供标准化接口和灵活的人机界面设计。

Result: 软件在仿真和真实平台上测试了延迟和性能，证明了其适用性。

Conclusion: 该软件为协作开发和实际测试提供了基础，代码已在GitHub上开源。

Abstract: Teleoperation is a key enabler for future mobility, supporting Automated Vehicles in rare and complex scenarios beyond the capabilities of their automation. Despite ongoing research, no open source software currently combines Remote Driving, e.g., via steering wheel and pedals, Remote Assistance through high-level interaction with automated driving software modules, and integration with a real-world vehicle for practical testing. To address this gap, we present a modular, open source teleoperation software stack that can interact with an automated driving software, e.g., Autoware, enabling Remote Assistance and Remote Driving. The software featuresstandardized interfaces for seamless integration with various real-world and simulation platforms, while allowing for flexible design of the human-machine interface. The system is designed for modularity and ease of extension, serving as a foundation for collaborative development on individual software components as well as realistic testing and user studies. To demonstrate the applicability of our software, we evaluated the latency and performance of different vehicle platforms in simulation and real-world. The source code is available on GitHub

</details>


### [271] [Beyond the Plane: A 3D Representation of Human Personal Space for Socially-Aware Robotics](https://arxiv.org/abs/2506.13937)
*Caio C. G. Ribeiro,Douglas G. Macharet*

Main category: cs.RO

TL;DR: 提出了一种新颖的三维个人空间模型，结合高度和水平距离来量化不适感。


<details>
  <summary>Details</summary>
Motivation: 机器人在人类环境中需遵守社交规范，尤其是个人空间的概念。现有研究多关注二维空间，忽略了垂直维度。

Method: 提出三维个人空间模型，结合Z轴（高度）和XY平面（水平距离）的不适函数。

Result: 首次实现了考虑人体配置和高度的三维空间中机器人组件位置的不适感计算。

Conclusion: 该模型为机器人行为设计提供了更全面的社交规范依据。

Abstract: The increasing presence of robots in human environments requires them to exhibit socially appropriate behavior, adhering to social norms. A critical aspect in this context is the concept of personal space, a psychological boundary around an individual that influences their comfort based on proximity. This concept extends to human-robot interaction, where robots must respect personal space to avoid causing discomfort. While much research has focused on modeling personal space in two dimensions, almost none have considered the vertical dimension. In this work, we propose a novel three-dimensional personal space model that integrates both height (introducing a discomfort function along the Z-axis) and horizontal proximity (via a classic XY-plane formulation) to quantify discomfort. To the best of our knowledge, this is the first work to compute discomfort in 3D space at any robot component's position, considering the person's configuration and height.

</details>


### [272] [Socially-aware Object Transportation by a Mobile Manipulator in Static Planar Environments with Obstacles](https://arxiv.org/abs/2506.13953)
*Caio C. G. Ribeiro,Leonardo R. D. Paes,Douglas G. Macharet*

Main category: cs.RO

TL;DR: 本文提出了一种基于Risk-RRT*框架的方法，用于移动机械臂在静态人类环境中的社交感知导航和物体搬运。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要针对移动机器人，而移动机械臂在社交导航中的独特挑战尚未充分研究。

Method: 采用Risk-RRT*框架，协调移动底座和机械臂的驱动，实现社交感知导航。

Result: 在模拟环境中，该方法优于仅针对移动机器人的方法，能有效导航、搬运物体并减少社交不适。

Conclusion: 移动机械臂需要特定技术，本文方法在社交导航中表现优异。

Abstract: Socially-aware robotic navigation is essential in environments where humans and robots coexist, ensuring both safety and comfort. However, most existing approaches have been primarily developed for mobile robots, leaving a significant gap in research that addresses the unique challenges posed by mobile manipulators. In this paper, we tackle the challenge of navigating a robotic mobile manipulator, carrying a non-negligible load, within a static human-populated environment while adhering to social norms. Our goal is to develop a method that enables the robot to simultaneously manipulate an object and navigate between locations in a socially-aware manner. We propose an approach based on the Risk-RRT* framework that enables the coordinated actuation of both the mobile base and manipulator. This approach ensures collision-free navigation while adhering to human social preferences. We compared our approach in a simulated environment to socially-aware mobile-only methods applied to a mobile manipulator. The results highlight the necessity for mobile manipulator-specific techniques, with our method outperforming mobile-only approaches. Our method enabled the robot to navigate, transport an object, avoid collisions, and minimize social discomfort effectively.

</details>


### [273] [A Cooperative Contactless Object Transport with Acoustic Robots](https://arxiv.org/abs/2506.13957)
*Narsimlu Kemsaram,Akin Delibasi,James Hardwick,Bonot Gautam,Diego Martinez Plasencia,Sriram Subramanian*

Main category: cs.RO

TL;DR: 本文提出了一种新型声学机器人系统，用于无接触物体的空中运输，受生物系统启发，通过协调策略提高效率和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 受蚂蚁群体等生物系统中协同运输现象的启发，研究旨在开发一种高效、适应动态环境的无接触物体运输系统。

Method: 利用相控超声换能器和机器人控制系统生成局部声压场，实现空中粒子和机器人的精确操控，分为独立运输和协同运输策略。

Result: 实验验证了系统的可行性，包括悬浮稳定性、运输效率和时钟同步精度，证明了独立和协同运输的有效性。

Conclusion: 该研究为声学机器人领域提供了新方法，潜在应用于无接触材料处理、微组装和生物医学领域。

Abstract: Cooperative transport, the simultaneous movement of an object by multiple agents, has been widely observed in biological systems such as ant colonies, which improve efficiency and adaptability in dynamic environments. Inspired by these natural phenomena, we present a novel acoustic robotic system for the transport of contactless objects in mid-air. Our system leverages phased ultrasonic transducers and a robotic control system onboard to generate localized acoustic pressure fields, enabling precise manipulation of airborne particles and robots. We categorize contactless object-transport strategies into independent transport (uncoordinated) and forward-facing cooperative transport (coordinated), drawing parallels with biological systems to optimize efficiency and robustness. The proposed system is experimentally validated by evaluating levitation stability using a microphone in the measurement lab, transport efficiency through a phase-space motion capture system, and clock synchronization accuracy via an oscilloscope. The results demonstrate the feasibility of both independent and cooperative airborne object transport. This research contributes to the field of acoustophoretic robotics, with potential applications in contactless material handling, micro-assembly, and biomedical applications.

</details>


### [274] [Diffusion-based Inverse Observation Model for Artificial Skin](https://arxiv.org/abs/2506.13986)
*Ante Maric,Julius Jankowski,Giammarco Caroleo,Alessandro Albini,Perla Maiolino,Sylvain Calinon*

Main category: cs.RO

TL;DR: 论文提出了一种基于扩散模型的方法，用于解决接触式物体姿态估计中的多模态问题，通过触觉测量生成有效的接触假设。


<details>
  <summary>Details</summary>
Motivation: 接触式物体姿态估计因观测的多模态性和不连续性而具有挑战性，传统方法难以高效采样符合接触约束的假设。

Method: 利用扩散模型学习多模态概率分布，通过去噪算法生成样本，并结合分布式人工皮肤的触觉测量数据训练逆观测模型。

Result: 模拟实验表明，该方法能够高效采样接触假设，用于物体姿态估计。

Conclusion: 扩散模型为解决接触式姿态估计中的多模态问题提供了有效工具。

Abstract: Contact-based estimation of object pose is challenging due to discontinuities and ambiguous observations that can correspond to multiple possible system states. This multimodality makes it difficult to efficiently sample valid hypotheses while respecting contact constraints. Diffusion models can learn to generate samples from such multimodal probability distributions through denoising algorithms. We leverage these probabilistic modeling capabilities to learn an inverse observation model conditioned on tactile measurements acquired from a distributed artificial skin. We present simulated experiments demonstrating efficient sampling of contact hypotheses for object pose estimation through touch.

</details>


### [275] [GRaD-Nav++: Vision-Language Model Enabled Visual Drone Navigation with Gaussian Radiance Fields and Differentiable Dynamics](https://arxiv.org/abs/2506.14009)
*Qianzhong Chen,Naixiang Gao,Suning Huang,JunEn Low,Timothy Chen,Jiankai Sun,Mac Schwager*

Main category: cs.RO

TL;DR: GRaD-Nav++是一个轻量级的视觉-语言-动作（VLA）框架，能够在无人机上实时执行自然语言指令，通过高效学习和适应性计算实现多任务和跨环境导航。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖手工技能、参数调整或计算密集型模型，无法在无人机上高效运行。GRaD-Nav++旨在解决这一问题，实现完全机载的实时语言导航。

Method: 使用基于3D高斯点云（3DGS）的模拟器，通过可微分强化学习（DiffRL）训练策略，并采用混合专家（MoE）动作头自适应路由计算。

Result: 在模拟环境中，训练任务成功率为83%，未见任务为75%；实际硬件上分别为67%和50%。多环境适应实验中，模拟和真实环境平均成功率分别为81%和67%。

Conclusion: GRaD-Nav++为完全机载的VLA飞行设定了新基准，证明轻量高效模型可实现可靠的语言导航，无需依赖外部基础设施。

Abstract: Autonomous drones capable of interpreting and executing high-level language instructions in unstructured environments remain a long-standing goal. Yet existing approaches are constrained by their dependence on hand-crafted skills, extensive parameter tuning, or computationally intensive models unsuitable for onboard use. We introduce GRaD-Nav++, a lightweight Vision-Language-Action (VLA) framework that runs fully onboard and follows natural-language commands in real time. Our policy is trained in a photorealistic 3D Gaussian Splatting (3DGS) simulator via Differentiable Reinforcement Learning (DiffRL), enabling efficient learning of low-level control from visual and linguistic inputs. At its core is a Mixture-of-Experts (MoE) action head, which adaptively routes computation to improve generalization while mitigating forgetting. In multi-task generalization experiments, GRaD-Nav++ achieves a success rate of 83% on trained tasks and 75% on unseen tasks in simulation. When deployed on real hardware, it attains 67% success on trained tasks and 50% on unseen ones. In multi-environment adaptation experiments, GRaD-Nav++ achieves an average success rate of 81% across diverse simulated environments and 67% across varied real-world settings. These results establish a new benchmark for fully onboard Vision-Language-Action (VLA) flight and demonstrate that compact, efficient models can enable reliable, language-guided navigation without relying on external infrastructure.

</details>


### [276] [Quadrotor Morpho-Transition: Learning vs Model-Based Control Strategies](https://arxiv.org/abs/2506.14039)
*Ioannis Mandralis,Richard M. Murray,Morteza Gharib*

Main category: cs.RO

TL;DR: 论文研究了四旋翼飞行器空中变形（Morpho-Transition）的控制问题，通过强化学习（RL）训练端到端控制器，并成功迁移到硬件。RL策略实现了敏捷着陆，但需考虑电机动态和观测延迟；而基线MPC控制器无需这些信息即可迁移，但对未知执行器故障的恢复能力较弱。


<details>
  <summary>Details</summary>
Motivation: 四旋翼空中变形涉及复杂的气动交互和执行器饱和问题，传统基于模型的控制方法因未建模动态和接触规划需求而受限。

Method: 采用端到端强化学习（RL）训练控制器，并考虑电机动态和观测延迟以实现硬件迁移。同时对比了基线MPC控制器的表现。

Result: RL控制器实现了敏捷着陆，但需电机动态和延迟信息才能迁移到硬件；MPC控制器无需这些信息即可迁移，但对执行器故障的恢复能力较差。

Conclusion: 该研究为需要空中变形的敏捷四旋翼飞行控制提供了更鲁棒的解决方案。

Abstract: Quadrotor Morpho-Transition, or the act of transitioning from air to ground through mid-air transformation, involves complex aerodynamic interactions and a need to operate near actuator saturation, complicating controller design. In recent work, morpho-transition has been studied from a model-based control perspective, but these approaches remain limited due to unmodeled dynamics and the requirement for planning through contacts. Here, we train an end-to-end Reinforcement Learning (RL) controller to learn a morpho-transition policy and demonstrate successful transfer to hardware. We find that the RL control policy achieves agile landing, but only transfers to hardware if motor dynamics and observation delays are taken into account. On the other hand, a baseline MPC controller transfers out-of-the-box without knowledge of the actuator dynamics and delays, at the cost of reduced recovery from disturbances in the event of unknown actuator failures. Our work opens the way for more robust control of agile in-flight quadrotor maneuvers that require mid-air transformation.

</details>


### [277] [A Point Cloud Completion Approach for the Grasping of Partially Occluded Objects and Its Applications in Robotic Strawberry Harvesting](https://arxiv.org/abs/2506.14066)
*Ali Abouzeid,Malak Mansour,Chengsong Hu,Dezhen Song*

Main category: cs.RO

TL;DR: 提出了一种端到端框架，用于解决草莓采摘中因遮挡导致的物体检测、分割和抓取规划问题，显著提高了抓取成功率和安全性。


<details>
  <summary>Details</summary>
Motivation: 在非结构化环境中，物体遮挡对机器人水果采摘的抓取算法设计提出了重大挑战，尤其是草莓采摘。

Method: 通过点云去噪和分割定位水果，使用点云补全模型重建被遮挡的草莓，生成占用图进行碰撞感知运动规划。

Result: 实验显示形状重建精度高（Chamfer Distance为1.10 mm），抓取成功率达79.17%，障碍物碰撞率从43.33%降至13.95%。

Conclusion: 该方法显著提升了草莓采摘的效率和可靠性，为机器人水果采摘系统提供了更优的解决方案。

Abstract: In robotic fruit picking applications, managing object occlusion in unstructured settings poses a substantial challenge for designing grasping algorithms. Using strawberry harvesting as a case study, we present an end-to-end framework for effective object detection, segmentation, and grasp planning to tackle this issue caused by partially occluded objects. Our strategy begins with point cloud denoising and segmentation to accurately locate fruits. To compensate for incomplete scans due to occlusion, we apply a point cloud completion model to create a dense 3D reconstruction of the strawberries. The target selection focuses on ripe strawberries while categorizing others as obstacles, followed by converting the refined point cloud into an occupancy map for collision-aware motion planning. Our experimental results demonstrate high shape reconstruction accuracy, with the lowest Chamfer Distance compared to state-of-the-art methods with 1.10 mm, and significantly improved grasp success rates of 79.17%, yielding an overall success-to-attempt ratio of 89.58\% in real-world strawberry harvesting. Additionally, our method reduces the obstacle hit rate from 43.33% to 13.95%, highlighting its effectiveness in improving both grasp quality and safety compared to prior approaches. This pipeline substantially improves autonomous strawberry harvesting, advancing more efficient and reliable robotic fruit picking systems.

</details>


### [278] [ReLCP: Scalable Complementarity-Based Collision Resolution for Smooth Rigid Bodies](https://arxiv.org/abs/2506.14097)
*Bryce Palmer,Hasan Metin Aktulga,Tong Gao*

Main category: cs.RO

TL;DR: 提出了一种基于互补性的碰撞解决算法，适用于光滑非球形刚体，通过递归生成的线性互补问题（ReLCP）自适应识别碰撞位置，避免了离散方法的超采样问题，显著提升了计算效率。


<details>
  <summary>Details</summary>
Motivation: 离散表面表示方法在处理高分辨率表面时约束数量激增，导致计算效率低下，而现有方法难以兼顾高保真度和可扩展性。

Method: 采用递归生成的线性互补问题（ReLCP）自适应识别碰撞位置，动态引入约束以避免重叠，减少约束数量。

Result: 相比离散方法，ReLCP方法在相同表面分辨率下减少1-2个数量级的约束，计算速度提升10-100倍，并在多种测试场景中验证了其高效性和稳定性。

Conclusion: ReLCP方法在保持高保真表面表示的同时，显著提升了计算效率和可扩展性，适用于密集场景。

Abstract: We present a complementarity-based collision resolution algorithm for smooth, non-spherical, rigid bodies. Unlike discrete surface representation approaches, which approximate surfaces using discrete elements (e.g., tessellations or sub-spheres) with constraints between nearby faces, edges, nodes, or sub-objects, our algorithm solves a recursively generated linear complementarity problem (ReLCP) to adaptively identify potential collision locations during the collision resolution procedure. Despite adaptively and in contrast to Newton-esque schemes, we prove conditions under which the resulting solution exists and the center of mass translational and rotational dynamics are unique. Our ReLCP also converges to classical LCP-based collision resolution for sufficiently small timesteps. Because increasing the surface resolution in discrete representation methods necessitates subdividing geometry into finer elements -- leading to a super-linear increase in the number of collision constraints -- these approaches scale poorly with increased surface resolution. In contrast, our adaptive ReLCP framework begins with a single constraint per pair of nearby bodies and introduces new constraints only when unconstrained motion would lead to overlap, circumventing the oversampling required by discrete methods. By requiring one to two orders of magnitude fewer collision constraints to achieve the same surface resolution, we observe 10-100x speedup in densely packed applications. We validate our ReLCP method against multisphere and single-constraint methods, comparing convergence in a two-ellipsoid collision test, scalability and performance in a compacting ellipsoid suspension and growing bacterial colony, and stability in a taut chainmail network, highlighting our ability to achieve high-fidelity surface representations without suffering from poor scalability or artificial surface roughness.

</details>


### [279] [A Hierarchical Test Platform for Vision Language Model (VLM)-Integrated Real-World Autonomous Driving](https://arxiv.org/abs/2506.14100)
*Yupeng Zhou,Can Cui,Juntong Peng,Zichong Yang,Juanwu Lu,Jitesh H Panchal,Bin Yao,Ziran Wang*

Main category: cs.RO

TL;DR: 本文提出了一种专为评估集成视觉语言模型（VLM）的自动驾驶系统设计的层次化真实世界测试平台，解决了现有方法在复杂性和可重复性上的不足。


<details>
  <summary>Details</summary>
Motivation: 现有评估方法难以捕捉真实驾驶场景的复杂性，且缺乏灵活的场景操控和可重复的闭环测试能力。

Method: 提出模块化、低延迟的车载中间件，支持多种VLM无缝集成；采用感知-规划-控制分离架构；设计可配置的真实场景测试套件。

Result: 通过案例研究展示了平台在多样化条件下的有效测试和评估能力。

Conclusion: 该测试平台为VLM集成自动驾驶系统提供了可控且真实的评估环境，支持多样化实验。

Abstract: Vision-Language Models (VLMs) have demonstrated notable promise in autonomous driving by offering the potential for multimodal reasoning through pretraining on extensive image-text pairs. However, adapting these models from broad web-scale data to the safety-critical context of driving presents a significant challenge, commonly referred to as domain shift. Existing simulation-based and dataset-driven evaluation methods, although valuable, often fail to capture the full complexity of real-world scenarios and cannot easily accommodate repeatable closed-loop testing with flexible scenario manipulation. In this paper, we introduce a hierarchical real-world test platform specifically designed to evaluate VLM-integrated autonomous driving systems. Our approach includes a modular, low-latency on-vehicle middleware that allows seamless incorporation of various VLMs, a clearly separated perception-planning-control architecture that can accommodate both VLM-based and conventional modules, and a configurable suite of real-world testing scenarios on a closed track that facilitates controlled yet authentic evaluations. We demonstrate the effectiveness of the proposed platform`s testing and evaluation ability with a case study involving a VLM-enabled autonomous vehicle, highlighting how our test framework supports robust experimentation under diverse conditions.

</details>


### [280] [Haptic-Based User Authentication for Tele-robotic System](https://arxiv.org/abs/2506.14116)
*Rongyu Yu,Kan Chen,Zeyu Deng,Chen Wang,Burak Kizilkaya,Liying Emma Li*

Main category: cs.RO

TL;DR: 本文提出了一种基于触觉反馈的用户行为特征认证方法，用于远程机器人操作中的安全认证，解决了传统方法的欺骗和重放攻击问题。


<details>
  <summary>Details</summary>
Motivation: 传统密码和静态生物特征认证在远程机器人操作中易受欺骗和重放攻击，特别是在高风险连续交互场景下，亟需更安全的认证方法。

Method: 通过收集15名参与者执行7项任务的触觉反馈数据，开发了一种基于Transformer的深度学习模型，提取用户特有的力动态特征。

Result: 该方法在用户识别和任务分类中均达到90%以上的准确率，显著提升了远程机器人系统的访问控制和身份验证能力。

Conclusion: 基于触觉反馈的行为特征认证方法为远程机器人操作提供了高效且安全的身份验证解决方案。

Abstract: Tele-operated robots rely on real-time user behavior mapping for remote tasks, but ensuring secure authentication remains a challenge. Traditional methods, such as passwords and static biometrics, are vulnerable to spoofing and replay attacks, particularly in high-stakes, continuous interactions. This paper presents a novel anti-spoofing and anti-replay authentication approach that leverages distinctive user behavioral features extracted from haptic feedback during human-robot interactions. To evaluate our authentication approach, we collected a time-series force feedback dataset from 15 participants performing seven distinct tasks. We then developed a transformer-based deep learning model to extract temporal features from the haptic signals. By analyzing user-specific force dynamics, our method achieves over 90 percent accuracy in both user identification and task classification, demonstrating its potential for enhancing access control and identity assurance in tele-robotic systems.

</details>


### [281] [GAF: Gaussian Action Field as a Dvnamic World Model for Robotic Mlanipulation](https://arxiv.org/abs/2506.14135)
*Ying Chai,Litao Deng,Ruizhi Shao,Jiajun Zhang,Liangjun Xing,Hongwen Zhang,Yebin Liu*

Main category: cs.RO

TL;DR: 提出了一种基于4D高斯动作场（GAF）的V-4D-A框架，通过动态4D表示直接推理动作，显著提升了机器人操作的准确性和成功率。


<details>
  <summary>Details</summary>
Motivation: 现有方法（V-A或V-3D-A）在复杂动态场景中动作推理不准确，需要一种更高效的动作推理框架。

Method: 提出GAF框架，扩展3D高斯泼溅（3DGS）为动态4D表示，支持场景重建、未来帧预测和初始动作估计，并结合扩散模型优化动作。

Result: GAF在重建质量上提升11.5385 dB PSNR和-0.5574 LPIPS，机器人操作任务成功率提升10.33%。

Conclusion: GAF通过动态4D表示和动作感知建模，显著提升了机器人操作的准确性和效率。

Abstract: Accurate action inference is critical for vision-based robotic manipulation. Existing approaches typically follow either a Vision-to-Action (V-A) paradigm, predicting actions directly from visual inputs, or a Vision-to-3D-to-Action (V-3D-A) paradigm, leveraging intermediate 3D representations. However, these methods often struggle with action inaccuracies due to the complexity and dynamic nature of manipulation scenes. In this paper, we propose a V-4D-A framework that enables direct action reasoning from motion-aware 4D representations via a Gaussian Action Field (GAF). GAF extends 3D Gaussian Splatting (3DGS) by incorporating learnable motion attributes, allowing simultaneous modeling of dynamic scenes and manipulation actions. To learn time-varying scene geometry and action-aware robot motion, GAF supports three key query types: reconstruction of the current scene, prediction of future frames, and estimation of initial action via robot motion. Furthermore, the high-quality current and future frames generated by GAF facilitate manipulation action refinement through a GAF-guided diffusion model. Extensive experiments demonstrate significant improvements, with GAF achieving +11.5385 dB PSNR and -0.5574 LPIPS improvements in reconstruction quality, while boosting the average success rate in robotic manipulation tasks by 10.33% over state-of-the-art methods. Project page: http://chaiying1.github.io/GAF.github.io/project_page/

</details>


### [282] [Lasso Gripper: A String Shooting-Retracting Mechanism for Shape-Adaptive Grasping](https://arxiv.org/abs/2506.14163)
*Qiyuan Qiao,Yu Wang,Xiyu Fan,Peng Lu*

Main category: cs.RO

TL;DR: 论文提出了一种新型抓取器Lasso Gripper，通过发射和收回绳子来抓取物体，适用于形状、大小不规则的物体。


<details>
  <summary>Details</summary>
Motivation: 传统抓取器因形状和尺寸限制难以处理超大、形状多变或易碎物体。

Method: Lasso Gripper使用四电机控制绳子发射和收回，通过调节电机速度调整绳圈大小，并设计了防缠绕机制和动态模型。

Result: 实验证明，Lasso Gripper能成功抓取并运输多种物体，包括动物模型和易碎蔬菜。

Conclusion: Lasso Gripper提供了一种更灵活、温和的抓取方案，突破了传统抓取器的限制。

Abstract: Handling oversized, variable-shaped, or delicate objects in transportation, grasping tasks is extremely challenging, mainly due to the limitations of the gripper's shape and size. This paper proposes a novel gripper, Lasso Gripper. Inspired by traditional tools like the lasso and the uurga, Lasso Gripper captures objects by launching and retracting a string. Contrary to antipodal grippers, which concentrate force on a limited area, Lasso Gripper applies uniform pressure along the length of the string for a more gentle grasp. The gripper is controlled by four motors-two for launching the string inward and two for launching it outward. By adjusting motor speeds, the size of the string loop can be tuned to accommodate objects of varying sizes, eliminating the limitations imposed by the maximum gripper separation distance. To address the issue of string tangling during rapid retraction, a specialized mechanism was incorporated. Additionally, a dynamic model was developed to estimate the string's curve, providing a foundation for the kinematic analysis of the workspace. In grasping experiments, Lasso Gripper, mounted on a robotic arm, successfully captured and transported a range of objects, including bull and horse figures as well as delicate vegetables. The demonstration video is available here: https://youtu.be/PV1J76mNP9Y.

</details>


### [283] [TACS-Graphs: Traversability-Aware Consistent Scene Graphs for Ground Robot Indoor Localization and Mapping](https://arxiv.org/abs/2506.14178)
*Jeewon Kim,Minho Oh,Hyun Myung*

Main category: cs.RO

TL;DR: 论文提出了一种新框架TACS-Graphs，通过结合地面机器人可通行性与房间分割，解决了传统3D室内场景图中房间层分割不一致的问题，提升了场景图的一致性和位姿估计精度。


<details>
  <summary>Details</summary>
Motivation: 传统3D室内场景图在复杂环境中存在房间层分割不一致的问题（如过分割和欠分割），影响了场景图的语义和拓扑一致性。

Method: 提出TACS-Graphs框架，利用机器人可通行性作为定义房间边界的关键因素，并结合CoSG-LCD方法优化闭环检测。

Result: 实验表明，该方法在场景图一致性和位姿图优化性能上优于现有技术。

Conclusion: TACS-Graphs通过可通行性感知的分割，显著提升了复杂环境中场景图的语义一致性和机器人任务规划的准确性。

Abstract: Scene graphs have emerged as a powerful tool for robots, providing a structured representation of spatial and semantic relationships for advanced task planning. Despite their potential, conventional 3D indoor scene graphs face critical limitations, particularly under- and over-segmentation of room layers in structurally complex environments. Under-segmentation misclassifies non-traversable areas as part of a room, often in open spaces, while over-segmentation fragments a single room into overlapping segments in complex environments. These issues stem from naive voxel-based map representations that rely solely on geometric proximity, disregarding the structural constraints of traversable spaces and resulting in inconsistent room layers within scene graphs. To the best of our knowledge, this work is the first to tackle segmentation inconsistency as a challenge and address it with Traversability-Aware Consistent Scene Graphs (TACS-Graphs), a novel framework that integrates ground robot traversability with room segmentation. By leveraging traversability as a key factor in defining room boundaries, the proposed method achieves a more semantically meaningful and topologically coherent segmentation, effectively mitigating the inaccuracies of voxel-based scene graph approaches in complex environments. Furthermore, the enhanced segmentation consistency improves loop closure detection efficiency in the proposed Consistent Scene Graph-leveraging Loop Closure Detection (CoSG-LCD) leading to higher pose estimation accuracy. Experimental results confirm that the proposed approach outperforms state-of-the-art methods in terms of scene graph consistency and pose graph optimization performance.

</details>


### [284] [Non-Overlap-Aware Egocentric Pose Estimation for Collaborative Perception in Connected Autonomy](https://arxiv.org/abs/2506.14180)
*Hong Huang,Dongkuan Xu,Hao Zhang,Peng Gao*

Main category: cs.RO

TL;DR: 论文提出了一种名为NOPE的新方法，用于多机器人团队中的非重叠感知自我中心姿态估计，解决了不同视角下姿态估计错误和通信带宽限制的问题。


<details>
  <summary>Details</summary>
Motivation: 在多机器人协作感知中，机器人需要知道自身与队友的相对姿态，但不同视角可能导致错误估计，且通信带宽限制阻碍了原始观测数据的共享。

Method: NOPE采用分层学习框架，结合高层深度图匹配（用于识别重叠视图）和低层位置感知交叉注意力图学习（用于姿态估计）。

Result: 实验表明，NOPE实现了非重叠感知姿态估计的新能力，并在性能上优于现有方法。

Conclusion: NOPE为多机器人协作中的姿态估计提供了高效且通信友好的解决方案。

Abstract: Egocentric pose estimation is a fundamental capability for multi-robot collaborative perception in connected autonomy, such as connected autonomous vehicles. During multi-robot operations, a robot needs to know the relative pose between itself and its teammates with respect to its own coordinates. However, different robots usually observe completely different views that contains similar objects, which leads to wrong pose estimation. In addition, it is unrealistic to allow robots to share their raw observations to detect overlap due to the limited communication bandwidth constraint. In this paper, we introduce a novel method for Non-Overlap-Aware Egocentric Pose Estimation (NOPE), which performs egocentric pose estimation in a multi-robot team while identifying the non-overlap views and satifying the communication bandwidth constraint. NOPE is built upon an unified hierarchical learning framework that integrates two levels of robot learning: (1) high-level deep graph matching for correspondence identification, which allows to identify if two views are overlapping or not, (2) low-level position-aware cross-attention graph learning for egocentric pose estimation. To evaluate NOPE, we conduct extensive experiments in both high-fidelity simulation and real-world scenarios. Experimental results have demonstrated that NOPE enables the novel capability for non-overlapping-aware egocentric pose estimation and achieves state-of-art performance compared with the existing methods. Our project page at https://hongh0.github.io/NOPE/.

</details>


### [285] [Hard Contacts with Soft Gradients: Refining Differentiable Simulators for Learning and Control](https://arxiv.org/abs/2506.14186)
*Anselm Paulus,A. René Geist,Pierre Schumacher,Vít Musil,Georg Martius*

Main category: cs.RO

TL;DR: 论文分析了基于惩罚的模拟器中接触力对梯度优化的挑战，提出了DiffMJX和CFD方法，显著提升了硬接触下的梯度质量，并在非接触状态下生成有用梯度。


<details>
  <summary>Details</summary>
Motivation: 接触力在机器人动力学梯度优化中引入速度跳跃，惩罚模拟器通过软化接触力简化梯度计算，但硬接触模拟需要高刚度设置，导致自动微分时梯度错误，而非刚性设置又增加仿真与现实的差距。

Method: 分析惩罚模拟器的接触计算以识别梯度错误原因；提出DiffMJX（结合自适应积分与MuJoCo XLA）提升硬接触下的梯度质量；引入CFD机制，在非接触状态下生成有用梯度，并通过直通技巧保持物理真实性。

Result: DiffMJX显著改善了硬接触下的梯度质量；CFD在非接触状态下生成有用梯度，且不影响正向模拟的物理真实性。

Conclusion: DiffMJX和CFD方法有效解决了硬接触和非接触状态下的梯度优化问题，提升了仿真与现实的匹配度。

Abstract: Contact forces pose a major challenge for gradient-based optimization of robot dynamics as they introduce jumps in the system's velocities. Penalty-based simulators, such as MuJoCo, simplify gradient computation by softening the contact forces. However, realistically simulating hard contacts requires very stiff contact settings, which leads to incorrect gradients when using automatic differentiation. On the other hand, using non-stiff settings strongly increases the sim-to-real gap. We analyze the contact computation of penalty-based simulators to identify the causes of gradient errors. Then, we propose DiffMJX, which combines adaptive integration with MuJoCo XLA, to notably improve gradient quality in the presence of hard contacts. Finally, we address a key limitation of contact gradients: they vanish when objects do not touch. To overcome this, we introduce Contacts From Distance (CFD), a mechanism that enables the simulator to generate informative contact gradients even before objects are in contact. To preserve physical realism, we apply CFD only in the backward pass using a straight-through trick, allowing us to compute useful gradients without modifying the forward simulation.

</details>


### [286] [AMPLIFY: Actionless Motion Priors for Robot Learning from Videos](https://arxiv.org/abs/2506.14198)
*Jeremy A. Collins,Loránd Cheng,Kunal Aneja,Albert Wilcox,Benjamin Joffe,Animesh Garg*

Main category: cs.RO

TL;DR: AMPLIFY框架利用大规模无动作视频数据，通过运动标记和模块化方法提升机器人策略的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 机器人动作标注数据稀缺且昂贵，而无动作视频数据丰富但难以转化为有效策略。

Method: AMPLIFY将视觉动态编码为紧凑的运动标记，分离视觉运动预测与动作推断，分别训练前向和逆向动态模型。

Result: 动态模型准确性显著提升，下游策略学习在低数据场景下表现优异，并能从无动作视频中学习。

Conclusion: AMPLIFY为利用异构数据构建高效、泛化的世界模型提供了新范式。

Abstract: Action-labeled data for robotics is scarce and expensive, limiting the generalization of learned policies. In contrast, vast amounts of action-free video data are readily available, but translating these observations into effective policies remains a challenge. We introduce AMPLIFY, a novel framework that leverages large-scale video data by encoding visual dynamics into compact, discrete motion tokens derived from keypoint trajectories. Our modular approach separates visual motion prediction from action inference, decoupling the challenges of learning what motion defines a task from how robots can perform it. We train a forward dynamics model on abundant action-free videos and an inverse dynamics model on a limited set of action-labeled examples, allowing for independent scaling. Extensive evaluations demonstrate that the learned dynamics are both accurate, achieving up to 3.7x better MSE and over 2.5x better pixel prediction accuracy compared to prior approaches, and broadly useful. In downstream policy learning, our dynamics predictions enable a 1.2-2.2x improvement in low-data regimes, a 1.4x average improvement by learning from action-free human videos, and the first generalization to LIBERO tasks from zero in-distribution action data. Beyond robotic control, we find the dynamics learned by AMPLIFY to be a versatile latent world model, enhancing video prediction quality. Our results present a novel paradigm leveraging heterogeneous data sources to build efficient, generalizable world models. More information can be found at https://amplify-robotics.github.io/.

</details>


### [287] [Pose State Perception of Interventional Robot for Cardio-cerebrovascular Procedures](https://arxiv.org/abs/2506.14201)
*Shunhan Ji,Yanxi Chen,Zhongyu Yang,Quan Zhang,Xiaohang Nie,Jingqian Sun,Yichao Tang*

Main category: cs.RO

TL;DR: 提出了一种基于视觉的无标记方法，用于精确感知介入机器人的姿态状态，实验验证了其高可靠性和准确性。


<details>
  <summary>Details</summary>
Motivation: 随着心脑血管介入手术需求的增加，精确控制介入机器人变得尤为重要，尤其是在复杂血管环境中，准确感知机器人姿态状态至关重要。

Method: 采用三部分框架：双头多任务U-Net模型检测血管和机器人；高级算法提取和优化骨架；基于几何特征的姿态感知系统。

Result: 实验结果表明，该方法在轨迹跟踪和姿态感知方面具有高可靠性和准确性。

Conclusion: 该方法无需额外传感器或标记，能有效感知机器人姿态状态，为后续控制提供策略。

Abstract: In response to the increasing demand for cardiocerebrovascular interventional surgeries, precise control of interventional robots has become increasingly important. Within these complex vascular scenarios, the accurate and reliable perception of the pose state for interventional robots is particularly crucial. This paper presents a novel vision-based approach without the need of additional sensors or markers. The core of this paper's method consists of a three-part framework: firstly, a dual-head multitask U-Net model for simultaneous vessel segment and interventional robot detection; secondly, an advanced algorithm for skeleton extraction and optimization; and finally, a comprehensive pose state perception system based on geometric features is implemented to accurately identify the robot's pose state and provide strategies for subsequent control. The experimental results demonstrate the proposed method's high reliability and accuracy in trajectory tracking and pose state perception.

</details>


### [288] [Narrate2Nav: Real-Time Visual Navigation with Implicit Language Reasoning in Human-Centric Environments](https://arxiv.org/abs/2506.14233)
*Amirreza Payandeh,Anuj Pokhrel,Daeun Song,Marcos Zampieri,Xuesu Xiao*

Main category: cs.RO

TL;DR: Narrate2Nav是一种新型实时视觉-动作模型，通过自监督学习框架嵌入自然语言推理和社交线索，显著提升移动机器人在人机环境中的导航性能。


<details>
  <summary>Details</summary>
Motivation: 大型视觉语言模型（VLMs）在移动机器人导航中潜力巨大，但计算复杂性和对连续数值数据的敏感性限制了其实时性和精确性。

Method: 采用基于Barlow Twins冗余减少损失的自监督学习框架，结合RGB输入、运动命令和文本信号，实现从观察到运动命令的映射。

Result: 在离线和真实世界实验中，性能分别提升52.94%和41.67%，视觉编码器的注意力图显示对导航关键元素更聚焦。

Conclusion: Narrate2Nav在人机导航任务中表现出色，通过嵌入推理和社交线索，显著提升了实时性和精确性。

Abstract: Large Vision-Language Models (VLMs) have demonstrated potential in enhancing mobile robot navigation in human-centric environments by understanding contextual cues, human intentions, and social dynamics while exhibiting reasoning capabilities. However, their computational complexity and limited sensitivity to continuous numerical data impede real-time performance and precise motion control. To this end, we propose Narrate2Nav, a novel real-time vision-action model that leverages a novel self-supervised learning framework based on the Barlow Twins redundancy reduction loss to embed implicit natural language reasoning, social cues, and human intentions within a visual encoder-enabling reasoning in the model's latent space rather than token space. The model combines RGB inputs, motion commands, and textual signals of scene context during training to bridge from robot observations to low-level motion commands for short-horizon point-goal navigation during deployment. Extensive evaluation of Narrate2Nav across various challenging scenarios in both offline unseen dataset and real-world experiments demonstrates an overall improvement of 52.94 percent and 41.67 percent, respectively, over the next best baseline. Additionally, qualitative comparative analysis of Narrate2Nav's visual encoder attention map against four other baselines demonstrates enhanced attention to navigation-critical scene elements, underscoring its effectiveness in human-centric navigation tasks.

</details>


### [289] [Robust Adaptive Time-Varying Control Barrier Function with Application to Robotic Surface Treatment](https://arxiv.org/abs/2506.14249)
*Yitaek Kim,Christoffer Sloth*

Main category: cs.RO

TL;DR: 提出了一种基于鲁棒自适应控制屏障函数（RaCBFs）的方法，用于处理时变约束下的模型不确定性和扰动。


<details>
  <summary>Details</summary>
Motivation: 现有方法在处理时变约束时往往忽略模型不确定性，导致实际应用受限。

Method: 结合鲁棒自适应控制屏障函数（RaCBFs）和输入到状态安全（ISSf）概念，并引入集合成员识别机制以减少保守性。

Result: 在机器人表面处理的数值模拟和实际实验中，证明了方法能确保质量在可接受范围内。

Conclusion: 该方法有效解决了时变约束下的模型不确定性问题，并通过实验验证了其可行性。

Abstract: Set invariance techniques such as control barrier functions (CBFs) can be used to enforce time-varying constraints such as keeping a safe distance from dynamic objects. However, existing methods for enforcing time-varying constraints often overlook model uncertainties. To address this issue, this paper proposes a CBFs-based robust adaptive controller design endowing time-varying constraints while considering parametric uncertainty and additive disturbances. To this end, we first leverage Robust adaptive Control Barrier Functions (RaCBFs) to handle model uncertainty, along with the concept of Input-to-State Safety (ISSf) to ensure robustness towards input disturbances. Furthermore, to alleviate the inherent conservatism in robustness, we also incorporate a set membership identification scheme. We demonstrate the proposed method on robotic surface treatment that requires time-varying force bounds to ensure uniform quality, in numerical simulation and real robotic setup, showing that the quality is formally guaranteed within an acceptable range.

</details>


### [290] [Public Acceptance of Cybernetic Avatars in the service sector: Evidence from a Large-Scale Survey in Dubai](https://arxiv.org/abs/2506.14268)
*Laura Aymerich-Franch,Tarek Taha,Takahiro Miyashita,Hiroko Kamide,Hiroshi Ishiguro,Paolo Dario*

Main category: cs.RO

TL;DR: 研究探讨了迪拜多元文化社会中赛博格化身的接受度，重点分析了外观、场景和功能任务对接受度的影响。


<details>
  <summary>Details</summary>
Motivation: 了解赛博格化身在多元文化社会中的接受度，为其设计和部署提供依据。

Method: 通过大规模调查（1000多名参与者）分析外观（如仿人、卡通）、场景（如商场、医院）和任务（如提供信息）对接受度的影响。

Result: 物理机器人接受度高于数字化身；高度仿人外观最受欢迎；信息提供任务最受重视；商场等场景接受度高，医疗场景较低；不同文化群体偏好不同。

Conclusion: 早期融入用户反馈对提升赛博格化身的社会接受度至关重要。

Abstract: Cybernetic avatars are hybrid interaction robots or digital representations that combine autonomous capabilities with teleoperated control. This study investigates the acceptance of cybernetic avatars in the highly multicultural society of Dubai, with particular emphasis on robotic avatars for customer service. Specifically, we explore how acceptance varies as a function of robot appearance (e.g., android, robotic-looking, cartoonish), deployment settings (e.g., shopping malls, hotels, hospitals), and functional tasks (e.g., providing information, patrolling). To this end, we conducted a large-scale survey with over 1,000 participants. Overall, cybernetic avatars received a high level of acceptance, with physical robot avatars receiving higher acceptance than digital avatars. In terms of appearance, robot avatars with a highly anthropomorphic robotic appearance were the most accepted, followed by cartoonish designs and androids. Animal-like appearances received the lowest level of acceptance. Among the tasks, providing information and guidance was rated as the most valued. Shopping malls, airports, public transport stations, and museums were the settings with the highest acceptance, whereas healthcare-related spaces received lower levels of support. An analysis by community cluster revealed among others that Emirati respondents showed significantly greater acceptance of android appearances compared to the overall sample, while participants from the 'Other Asia' cluster were significantly more accepting of cartoonish appearances. Our study underscores the importance of incorporating citizen feedback into the design and deployment of cybernetic avatars from the early stages to enhance acceptance of this technology in society.

</details>


### [291] [Whole-Body Control Framework for Humanoid Robots with Heavy Limbs: A Model-Based Approach](https://arxiv.org/abs/2506.14278)
*Tianlin Zhang,Linzhu Yue,Hongbo Zhang,Lingwei Zhang,Xuanqi Zeng,Zhitao Song,Yun-Hui Liu*

Main category: cs.RO

TL;DR: 提出了一种基于模型的全身体控制框架，用于解决人形机器人因重型肢体运动导致的平衡问题，通过结合运动动力学规划器和分层优化问题，实现了动态运动和复杂地形下的实时控制。


<details>
  <summary>Details</summary>
Motivation: 人形机器人在动态运动或复杂地形中常因重型肢体运动导致平衡问题，亟需一种有效的控制方法。

Method: 采用基于模型的方法，结合运动动力学规划器（MPC）和分层二次规划（HQP），简化系统动力学并实时规划运动与接触力。

Result: 实验验证显示，该框架使机器人动态行走速度达1.2 m/s，可抵抗60 N外力干扰，并在不平坦地形和户外环境中保持平衡。

Conclusion: 所提出的控制框架有效解决了重型肢体人形机器人的平衡问题，适用于动态和复杂环境。

Abstract: Humanoid robots often face significant balance issues due to the motion of their heavy limbs. These challenges are particularly pronounced when attempting dynamic motion or operating in environments with irregular terrain. To address this challenge, this manuscript proposes a whole-body control framework for humanoid robots with heavy limbs, using a model-based approach that combines a kino-dynamics planner and a hierarchical optimization problem. The kino-dynamics planner is designed as a model predictive control (MPC) scheme to account for the impact of heavy limbs on mass and inertia distribution. By simplifying the robot's system dynamics and constraints, the planner enables real-time planning of motion and contact forces. The hierarchical optimization problem is formulated using Hierarchical Quadratic Programming (HQP) to minimize limb control errors and ensure compliance with the policy generated by the kino-dynamics planner. Experimental validation of the proposed framework demonstrates its effectiveness. The humanoid robot with heavy limbs controlled by the proposed framework can achieve dynamic walking speeds of up to 1.2~m/s, respond to external disturbances of up to 60~N, and maintain balance on challenging terrains such as uneven surfaces, and outdoor environments.

</details>


### [292] [Steering Robots with Inference-Time Interactions](https://arxiv.org/abs/2506.14287)
*Yanwei Wang*

Main category: cs.RO

TL;DR: 论文提出了一种无需微调预训练策略的方法，通过用户交互在推理时引导行为生成，以纠正策略错误。


<details>
  <summary>Details</summary>
Motivation: 预训练策略在部署时可能因泛化能力不足而产生错误，而传统方法需要为每个下游用例收集额外数据进行微调，效率低下。

Method: 提出两种框架：(1) 推理时引导，利用用户交互切换离散技能；(2) 任务和动作模仿，允许用户编辑连续动作以满足任务约束。

Result: 这些框架无需额外训练即可纠正策略预测偏差，同时满足用户推理时的目标。

Conclusion: 通过用户交互引导预训练策略，能够在推理时高效纠正错误，提升预训练模型的实用性。

Abstract: Imitation learning has driven the development of generalist policies capable of autonomously solving multiple tasks. However, when a pretrained policy makes errors during deployment, there are limited mechanisms for users to correct its behavior. While collecting additional data for finetuning can address such issues, doing so for each downstream use case is inefficient at deployment. My research proposes an alternative: keeping pretrained policies frozen as a fixed skill repertoire while allowing user interactions to guide behavior generation toward user preferences at inference time. By making pretrained policies steerable, users can help correct policy errors when the model struggles to generalize-without needing to finetune the policy. Specifically, I propose (1) inference-time steering, which leverages user interactions to switch between discrete skills, and (2) task and motion imitation, which enables user interactions to edit continuous motions while satisfying task constraints defined by discrete symbolic plans. These frameworks correct misaligned policy predictions without requiring additional training, maximizing the utility of pretrained models while achieving inference-time user objectives.

</details>


### [293] [Uncertainty-Driven Radar-Inertial Fusion for Instantaneous 3D Ego-Velocity Estimation](https://arxiv.org/abs/2506.14294)
*Prashant Kumar Rai,Elham Kowsari,Nataliya Strokina,Reza Ghabcheloo*

Main category: cs.RO

TL;DR: 提出一种结合高分辨率雷达与惯性测量单元的自主导航方法，通过神经网络处理雷达数据并估计速度及其不确定性，再与惯性数据融合，显著提升了运动估计的精度。


<details>
  <summary>Details</summary>
Motivation: 传统雷达运动估计技术存在局限性，需更鲁棒且精确的方法。

Method: 使用神经网络处理雷达数据，结合扩展卡尔曼滤波融合惯性数据，优化噪声和偏差参数。

Result: 在公开数据集上表现优于现有方法，误差显著降低。

Conclusion: 该方法在自主导航中实现了更高的运动估计精度和鲁棒性。

Abstract: We present a method for estimating ego-velocity in autonomous navigation by integrating high-resolution imaging radar with an inertial measurement unit. The proposed approach addresses the limitations of traditional radar-based ego-motion estimation techniques by employing a neural network to process complex-valued raw radar data and estimate instantaneous linear ego-velocity along with its associated uncertainty. This uncertainty-aware velocity estimate is then integrated with inertial measurement unit data using an Extended Kalman Filter. The filter leverages the network-predicted uncertainty to refine the inertial sensor's noise and bias parameters, improving the overall robustness and accuracy of the ego-motion estimation. We evaluated the proposed method on the publicly available ColoRadar dataset. Our approach achieves significantly lower error compared to the closest publicly available method and also outperforms both instantaneous and scan matching-based techniques.

</details>


### [294] [Socially Aware Robot Crowd Navigation via Online Uncertainty-Driven Risk Adaptation](https://arxiv.org/abs/2506.14305)
*Zhirui Sun,Xingrong Diao,Yao Wang,Bi-Ke Zhu,Jiankun Wang*

Main category: cs.RO

TL;DR: LR-MPC是一种数据驱动的导航算法，平衡效率、安全性和社交意识，通过离线风险学习和在线自适应推理实现高效导航。


<details>
  <summary>Details</summary>
Motivation: 在人与机器人共享的拥挤环境中，现有方法常忽视社交意识，LR-MPC旨在解决这一问题。

Method: 分为离线风险学习（使用PENN训练风险数据）和在线推理（通过Multi-RRT规划器生成路径，PENN评估风险）。

Result: 实验表明LR-MPC在成功率和社交意识上优于基线方法。

Conclusion: LR-MPC能高效导航复杂人群，适应性强且干扰低。

Abstract: Navigation in human-robot shared crowded environments remains challenging, as robots are expected to move efficiently while respecting human motion conventions. However, many existing approaches emphasize safety or efficiency while overlooking social awareness. This article proposes Learning-Risk Model Predictive Control (LR-MPC), a data-driven navigation algorithm that balances efficiency, safety, and social awareness. LR-MPC consists of two phases: an offline risk learning phase, where a Probabilistic Ensemble Neural Network (PENN) is trained using risk data from a heuristic MPC-based baseline (HR-MPC), and an online adaptive inference phase, where local waypoints are sampled and globally guided by a Multi-RRT planner. Each candidate waypoint is evaluated for risk by PENN, and predictions are filtered using epistemic and aleatoric uncertainty to ensure robust decision-making. The safest waypoint is selected as the MPC input for real-time navigation. Extensive experiments demonstrate that LR-MPC outperforms baseline methods in success rate and social awareness, enabling robots to navigate complex crowds with high adaptability and low disruption. A website about this work is available at https://sites.google.com/view/lr-mpc.

</details>


### [295] [ClutterDexGrasp: A Sim-to-Real System for General Dexterous Grasping in Cluttered Scenes](https://arxiv.org/abs/2506.14317)
*Zeyuan Chen,Qiyang Yan,Yuanpei Chen,Tianhao Wu,Jiyao Zhang,Zihan Ding,Jinzhou Li,Yaodong Yang,Hao Dong*

Main category: cs.RO

TL;DR: 提出了一种名为ClutterDexGrasp的两阶段师生框架，用于杂乱场景中的目标导向灵巧抓取，实现了零样本的模拟到现实迁移。


<details>
  <summary>Details</summary>
Motivation: 解决杂乱场景中灵巧抓取的挑战，包括多样物体几何、遮挡和碰撞，现有方法难以应对复杂场景。

Method: 采用两阶段师生框架，教师策略在模拟中训练，学生策略通过模仿学习从教师策略中提取知识，使用3D扩散策略处理部分点云观测。

Result: 首次实现了杂乱场景中目标导向灵巧抓取的零样本模拟到现实闭环系统，展示了鲁棒性能。

Conclusion: ClutterDexGrasp框架在复杂场景中表现出色，为灵巧抓取提供了高效且可扩展的解决方案。

Abstract: Dexterous grasping in cluttered scenes presents significant challenges due to diverse object geometries, occlusions, and potential collisions. Existing methods primarily focus on single-object grasping or grasp-pose prediction without interaction, which are insufficient for complex, cluttered scenes. Recent vision-language-action models offer a potential solution but require extensive real-world demonstrations, making them costly and difficult to scale. To address these limitations, we revisit the sim-to-real transfer pipeline and develop key techniques that enable zero-shot deployment in reality while maintaining robust generalization. We propose ClutterDexGrasp, a two-stage teacher-student framework for closed-loop target-oriented dexterous grasping in cluttered scenes. The framework features a teacher policy trained in simulation using clutter density curriculum learning, incorporating both a novel geometry and spatially-embedded scene representation and a comprehensive safety curriculum, enabling general, dynamic, and safe grasping behaviors. Through imitation learning, we distill the teacher's knowledge into a student 3D diffusion policy (DP3) that operates on partial point cloud observations. To the best of our knowledge, this represents the first zero-shot sim-to-real closed-loop system for target-oriented dexterous grasping in cluttered scenes, demonstrating robust performance across diverse objects and layouts. More details and videos are available at https://clutterdexgrasp.github.io/.

</details>


### [296] [Barrier Method for Inequality Constrained Factor Graph Optimization with Application to Model Predictive Control](https://arxiv.org/abs/2506.14341)
*Anas Abdelkarim,Holger Voos,Daniel Görges*

Main category: cs.RO

TL;DR: 论文提出了一种将Barrier Interior Point Method (BIPM)与因子图结合的新方法，用于解决MPC中的约束处理问题，并通过实验验证了其高效性。


<details>
  <summary>Details</summary>
Motivation: 因子图在机器人感知任务中表现出色，但在最优控制问题（如MPC）中的应用受限，主要因约束处理的挑战。

Method: 通过将BIPM与因子图结合，引入特殊的不等式因子节点，克服传统因子图的二次形式限制。

Result: 实验验证了该方法在多目标自适应巡航控制中的高效性，收敛更快且计算效率更高。

Conclusion: 该方法首次在g2o框架中实现了统一处理等式和不等式约束，为MPC提供了高效解决方案。

Abstract: Factor graphs have demonstrated remarkable efficiency for robotic perception tasks, particularly in localization and mapping applications. However, their application to optimal control problems -- especially Model Predictive Control (MPC) -- has remained limited due to fundamental challenges in constraint handling. This paper presents a novel integration of the Barrier Interior Point Method (BIPM) with factor graphs, implemented as an open-source extension to the widely adopted g2o framework. Our approach introduces specialized inequality factor nodes that encode logarithmic barrier functions, thereby overcoming the quadratic-form limitations of conventional factor graph formulations. To the best of our knowledge, this is the first g2o-based implementation capable of efficiently handling both equality and inequality constraints within a unified optimization backend. We validate the method through a multi-objective adaptive cruise control application for autonomous vehicles. Benchmark comparisons with state-of-the-art constraint-handling techniques demonstrate faster convergence and improved computational efficiency. (Code repository: https://github.com/snt-arg/bipm_g2o)

</details>


### [297] [Data Driven Approach to Input Shaping for Vibration Suppression in a Flexible Robot Arm](https://arxiv.org/abs/2506.14405)
*Jarkko Kotaniemi,Janne Saukkoriipi,Shuai Li,Markku Suomalainen*

Main category: cs.RO

TL;DR: 提出一种基于数据驱动的方法，自适应调整输入整形器参数以减少柔性机械臂残余振动。


<details>
  <summary>Details</summary>
Motivation: 柔性机械臂的残余振动影响其精度和性能，传统方法难以适应不同工况。

Method: 通过插值已测量的残余振动数据，自适应调整输入整形器参数。

Result: 在多种材料的3D打印柔性机械臂上验证，显著减少残余振动。

Conclusion: 该方法简单有效，适用于不同材料的柔性机械臂振动抑制。

Abstract: This paper presents a simple and effective method for setting parameters for an input shaper to suppress the residual vibrations in flexible robot arms using a data-driven approach. The parameters are adaptively tuned in the workspace of the robot by interpolating previously measured data of the robot's residual vibrations. Input shaping is a simple and robust technique to generate vibration-reduced shaped commands by a convolution of an impulse sequence with the desired input command. The generated impulses create waves in the material countering the natural vibrations of the system. The method is demonstrated with a flexible 3D-printed robot arm with multiple different materials, achieving a significant reduction in the residual vibrations.

</details>


### [298] [Enhancing Object Search in Indoor Spaces via Personalized Object-factored Ontologies](https://arxiv.org/abs/2506.14422)
*Akash Chikhalikar,Ankit A. Ravankar,Jose Victorio Salazar Luces,Yasuhisa Hirata*

Main category: cs.RO

TL;DR: 论文提出了一种个性化框架和自适应推理策略，用于提升服务机器人在室内环境中的多目标搜索能力。


<details>
  <summary>Details</summary>
Motivation: 个性化对服务机器人的发展至关重要，机器人需要理解环境并适应变化以实现长期部署和复杂任务。

Method: 提出个性化本体框架和自适应推理策略，结合动态信念更新，提升多目标搜索性能。

Result: 实验表明，该方法在真实环境中优于现有技术，个性化能进一步提升性能。

Conclusion: 个性化与自适应推理的结合显著提升了机器人在长期目标搜索中的能力。

Abstract: Personalization is critical for the advancement of service robots. Robots need to develop tailored understandings of the environments they are put in. Moreover, they need to be aware of changes in the environment to facilitate long-term deployment. Long-term understanding as well as personalization is necessary to execute complex tasks like prepare dinner table or tidy my room. A precursor to such tasks is that of Object Search. Consequently, this paper focuses on locating and searching multiple objects in indoor environments. In this paper, we propose two crucial novelties. Firstly, we propose a novel framework that can enable robots to deduce Personalized Ontologies of indoor environments. Our framework consists of a personalization schema that enables the robot to tune its understanding of ontologies. Secondly, we propose an Adaptive Inferencing strategy. We integrate Dynamic Belief Updates into our approach which improves performance in multi-object search tasks. The cumulative effect of personalization and adaptive inferencing is an improved capability in long-term object search. This framework is implemented on top of a multi-layered semantic map. We conduct experiments in real environments and compare our results against various state-of-the-art (SOTA) methods to demonstrate the effectiveness of our approach. Additionally, we show that personalization can act as a catalyst to enhance the performance of SOTAs. Video Link: https://bit.ly/3WHk9i9

</details>


### [299] [Automatic Cannulation of Femoral Vessels in a Porcine Shock Model](https://arxiv.org/abs/2506.14467)
*Nico Zevallos,Cecilia G. Morales,Andrew Orekhov,Tejas Rane,Hernando Gomez,Francis X. Guyette,Michael R. Pinsky,John Galeotti,Artur Dubrawski,Howie Choset*

Main category: cs.RO

TL;DR: 本文探讨了在创伤和重症护理中实现快速可靠的血管通路的重要性，提出了一种全自动机器人超声引导的股血管插管方法，并在猪出血性休克模型中验证了其可行性。


<details>
  <summary>Details</summary>
Motivation: 中心血管插管在急救中至关重要，但传统方法依赖专业技能且操作复杂，尤其在院前环境中难以实现。现有技术多为半自动化，无法满足紧急需求。

Method: 研究提出了一种全自动机器人超声引导的股血管插管技术，结合机器人超声和精确针头定位，实现了微创操作。

Result: 在猪出血性休克模型中，成功实现了股静脉和动脉的插管，验证了该技术的可行性。

Conclusion: 全自动机器人超声引导的股血管插管技术有望解决急救中血管通路的难题，尤其在院前环境中具有重要应用潜力。

Abstract: Rapid and reliable vascular access is critical in trauma and critical care. Central vascular catheterization enables high-volume resuscitation, hemodynamic monitoring, and advanced interventions like ECMO and REBOA. While peripheral access is common, central access is often necessary but requires specialized ultrasound-guided skills, posing challenges in prehospital settings. The complexity arises from deep target vessels and the precision needed for needle placement. Traditional techniques, like the Seldinger method, demand expertise to avoid complications. Despite its importance, ultrasound-guided central access is underutilized due to limited field expertise. While autonomous needle insertion has been explored for peripheral vessels, only semi-autonomous methods exist for femoral access. This work advances toward full automation, integrating robotic ultrasound for minimally invasive emergency procedures. Our key contribution is the successful femoral vein and artery cannulation in a porcine hemorrhagic shock model.

</details>


### [300] [ros2 fanuc interface: Design and Evaluation of a Fanuc CRX Hardware Interface in ROS2](https://arxiv.org/abs/2506.14487)
*Paolo Franceschi,Marco Faroni,Stefano Baraldo,Anna Valente*

Main category: cs.RO

TL;DR: 本文介绍了ROS2控制与Fanuc CRX机器人系列的硬件接口集成，包括实现细节、通信协议及与Moveit2运动规划库的整合。通过实验评估了性能，结果显示机器人能准确跟踪路径并避免碰撞。


<details>
  <summary>Details</summary>
Motivation: 为Fanuc CRX机器人系列开发ROS2控制接口，并验证其在多种机器人任务中的性能表现。

Method: 实现ros2_fanuc_interface，测试四种机器人任务：阶跃响应、轨迹跟踪、碰撞避免和动态速度调整。

Result: 机器人能准确跟踪路径（符合关节速度限制），并实现碰撞避免，但存在命令与反馈间的延迟。

Conclusion: 开发的接口性能良好，代码已开源。

Abstract: This paper introduces the ROS2 control and the Hardware Interface (HW) integration for the Fanuc CRX- robot family. It explains basic implementation details and communication protocols, and its integration with the Moveit2 motion planning library. We conducted a series of experiments to evaluate relevant performances in the robotics field. We tested the developed ros2_fanuc_interface for four relevant robotics cases: step response, trajectory tracking, collision avoidance integrated with Moveit2, and dynamic velocity scaling, respectively. Results show that, despite a non-negligible delay between command and feedback, the robot can track the defined path with negligible errors (if it complies with joint velocity limits), ensuring collision avoidance. Full code is open source and available at https://github.com/paolofrance/ros2_fanuc_interface.

</details>


### [301] [Can Pretrained Vision-Language Embeddings Alone Guide Robot Navigation?](https://arxiv.org/abs/2506.14507)
*Nitesh Subedi,Adam Haroon,Shreyan Ganguly,Samuel T. K. Tetteh,Prajwal Koirala,Cody Fleming,Soumik Sarkar*

Main category: cs.RO

TL;DR: 论文提出了一种基于预训练视觉语言模型（VLM）的极简导航框架，直接使用冻结的嵌入进行行为克隆，无需微调或专用模块。实验表明，该方法在语言指定目标导航中达到74%的成功率，但效率较低。


<details>
  <summary>Details</summary>
Motivation: 探索预训练嵌入是否足以独立指导导航任务，而无需额外调整或复杂架构。

Method: 训练行为克隆策略，直接使用冻结的VLM嵌入，基于专家演示数据。

Result: 导航成功率为74%，但效率较低（步骤增加3.2倍），表明预训练嵌入在语言基础和长时规划上存在局限。

Conclusion: 预训练嵌入在基础任务中有效，但在复杂规划上表现不足，为机器人设计提供了实用权衡的参考。

Abstract: Foundation models have revolutionized robotics by providing rich semantic representations without task-specific training. While many approaches integrate pretrained vision-language models (VLMs) with specialized navigation architectures, the fundamental question remains: can these pretrained embeddings alone successfully guide navigation without additional fine-tuning or specialized modules? We present a minimalist framework that decouples this question by training a behavior cloning policy directly on frozen vision-language embeddings from demonstrations collected by a privileged expert. Our approach achieves a 74% success rate in navigation to language-specified targets, compared to 100% for the state-aware expert, though requiring 3.2 times more steps on average. This performance gap reveals that pretrained embeddings effectively support basic language grounding but struggle with long-horizon planning and spatial reasoning. By providing this empirical baseline, we highlight both the capabilities and limitations of using foundation models as drop-in representations for embodied tasks, offering critical insights for robotics researchers facing practical design tradeoffs between system complexity and performance in resource-constrained scenarios. Our code is available at https://github.com/oadamharoon/text2nav

</details>


### [302] [GAMORA: A Gesture Articulated Meta Operative Robotic Arm for Hazardous Material Handling in Containment-Level Environments](https://arxiv.org/abs/2506.14513)
*Farha Abdul Wasay,Mohammed Abdul Rahman,Hania Ghouse*

Main category: cs.RO

TL;DR: GAMORA是一种基于VR的机器人系统，通过手势控制远程执行高风险实验室任务，提高了精度和安全性。


<details>
  <summary>Details</summary>
Motivation: 随着生物危害复杂性的增加，减少直接人类接触同时保持操作精度变得至关重要。

Method: 结合Oculus Quest 2、NVIDIA Jetson Nano和ROS，提供实时沉浸式控制、数字孪生模拟和逆运动学驱动的机械臂操作。

Result: 平均位置误差2.2毫米，移液精度0.2毫升，重复性1.2毫米，能耗降低50%。

Conclusion: GAMORA为高风险实验室任务提供了一种可扩展、沉浸式的解决方案，提高了生物安全性。

Abstract: The convergence of robotics and virtual reality (VR) has enabled safer and more efficient workflows in high-risk laboratory settings, particularly virology labs. As biohazard complexity increases, minimizing direct human exposure while maintaining precision becomes essential. We propose GAMORA (Gesture Articulated Meta Operative Robotic Arm), a novel VR-guided robotic system that enables remote execution of hazardous tasks using natural hand gestures. Unlike existing scripted automation or traditional teleoperation, GAMORA integrates the Oculus Quest 2, NVIDIA Jetson Nano, and Robot Operating System (ROS) to provide real-time immersive control, digital twin simulation, and inverse kinematics-based articulation. The system supports VR-based training and simulation while executing precision tasks in physical environments via a 3D-printed robotic arm. Inverse kinematics ensure accurate manipulation for delicate operations such as specimen handling and pipetting. The pipeline includes Unity-based 3D environment construction, real-time motion planning, and hardware-in-the-loop testing. GAMORA achieved a mean positional discrepancy of 2.2 mm (improved from 4 mm), pipetting accuracy within 0.2 mL, and repeatability of 1.2 mm across 50 trials. Integrated object detection via YOLOv8 enhances spatial awareness, while energy-efficient operation (50% reduced power output) ensures sustainable deployment. The system's digital-physical feedback loop enables safe, precise, and repeatable automation of high-risk lab tasks. GAMORA offers a scalable, immersive solution for robotic control and biosafety in biomedical research environments.

</details>


### [303] [NetRoller: Interfacing General and Specialized Models for End-to-End Autonomous Driving](https://arxiv.org/abs/2506.14589)
*Ren Xin,Hongji Liu,Xiaodong Mei,Wenru Liu,Maosheng Ye,Zhili Chen,Jun Ma*

Main category: cs.RO

TL;DR: NetRoller是一个适配器，通过三个阶段机制解决通用模型（GMs）与专用驾驶模型（SMs）集成时的异步问题，显著提升自动驾驶任务性能。


<details>
  <summary>Details</summary>
Motivation: 现有专用驾驶模型在数据多样性和模型能力上存在局限，而通用模型（如LLMs）的引入虽具潜力，但会引发异步系统问题。

Method: NetRoller通过早期停止机制、可学习查询嵌入、非语义嵌入和位置层嵌入，以及查询和特征偏移机制，分三阶段实现GMs与SMs的无缝集成。

Result: 在nuScenes数据集上的实验表明，NetRoller显著提升了规划任务的人机相似性和安全性，并在检测与建图任务中实现精度提升。

Conclusion: NetRoller有效解决了GMs与SMs集成中的异步问题，为自动驾驶任务提供了高效且性能优越的解决方案。

Abstract: Integrating General Models (GMs) such as Large Language Models (LLMs), with Specialized Models (SMs) in autonomous driving tasks presents a promising approach to mitigating challenges in data diversity and model capacity of existing specialized driving models. However, this integration leads to problems of asynchronous systems, which arise from the distinct characteristics inherent in GMs and SMs. To tackle this challenge, we propose NetRoller, an adapter that incorporates a set of novel mechanisms to facilitate the seamless integration of GMs and specialized driving models. Specifically, our mechanisms for interfacing the asynchronous GMs and SMs are organized into three key stages. NetRoller first harvests semantically rich and computationally efficient representations from the reasoning processes of LLMs using an early stopping mechanism, which preserves critical insights on driving context while maintaining low overhead. It then applies learnable query embeddings, nonsensical embeddings, and positional layer embeddings to facilitate robust and efficient cross-modality translation. At last, it employs computationally efficient Query Shift and Feature Shift mechanisms to enhance the performance of SMs through few-epoch fine-tuning. Based on the mechanisms formalized in these three stages, NetRoller enables specialized driving models to operate at their native frequencies while maintaining situational awareness of the GM. Experiments conducted on the nuScenes dataset demonstrate that integrating GM through NetRoller significantly improves human similarity and safety in planning tasks, and it also achieves noticeable precision improvements in detection and mapping tasks for end-to-end autonomous driving. The code and models are available at https://github.com/Rex-sys-hk/NetRoller .

</details>


### [304] [Latent Action Diffusion for Cross-Embodiment Manipulation](https://arxiv.org/abs/2506.14608)
*Erik Bauer,Elvis Nava,Robert K. Katzschmann*

Main category: cs.RO

TL;DR: 论文提出了一种基于潜在动作空间的扩散策略，用于统一不同末端执行器的动作空间，实现跨机器人形态的技能迁移和多机器人控制。


<details>
  <summary>Details</summary>
Motivation: 解决机器人操作中数据稀缺和不同形态机器人动作空间异构的问题，促进跨形态学习和技能迁移。

Method: 通过对比损失训练的编码器学习语义对齐的潜在动作空间，并利用该空间进行多机器人数据的联合训练。

Result: 在跨形态机器人控制中，单策略实现了高达13%的操作成功率提升，表明技能迁移的成功。

Conclusion: 潜在跨形态策略为统一不同动作空间提供了新方法，减少了数据收集需求，加速了跨形态泛化，提升了机器人学习的可扩展性和效率。

Abstract: End-to-end learning approaches offer great potential for robotic manipulation, but their impact is constrained by data scarcity and heterogeneity across different embodiments. In particular, diverse action spaces across different end-effectors create barriers for cross-embodiment learning and skill transfer. We address this challenge through diffusion policies learned in a latent action space that unifies diverse end-effector actions. We first show that we can learn a semantically aligned latent action space for anthropomorphic robotic hands, a human hand, and a parallel jaw gripper using encoders trained with a contrastive loss. Second, we show that by using our proposed latent action space for co-training on manipulation data from different end-effectors, we can utilize a single policy for multi-robot control and obtain up to 13% improved manipulation success rates, indicating successful skill transfer despite a significant embodiment gap. Our approach using latent cross-embodiment policies presents a new method to unify different action spaces across embodiments, enabling efficient multi-robot control and data sharing across robot setups. This unified representation significantly reduces the need for extensive data collection for each new robot morphology, accelerates generalization across embodiments, and ultimately facilitates more scalable and efficient robotic learning.

</details>


### [305] [SENIOR: Efficient Query Selection and Preference-Guided Exploration in Preference-based Reinforcement Learning](https://arxiv.org/abs/2506.14648)
*Hexian Ni,Tao Lu,Haoyuan Hu,Yinghao Cai,Shuo Wang*

Main category: cs.RO

TL;DR: SENIOR是一种基于偏好的强化学习方法，通过高效查询选择和偏好引导探索，提升反馈和样本效率。


<details>
  <summary>Details</summary>
Motivation: 避免奖励工程，解决PbRL中反馈和样本效率低的问题。

Method: 提出MDS选择方案和PGE探索方法，结合运动区分和偏好引导。

Result: 在六项复杂机器人任务中表现优于五种现有方法。

Conclusion: SENIOR显著加速奖励和策略学习，适用于仿真和现实任务。

Abstract: Preference-based Reinforcement Learning (PbRL) methods provide a solution to avoid reward engineering by learning reward models based on human preferences. However, poor feedback- and sample- efficiency still remain the problems that hinder the application of PbRL. In this paper, we present a novel efficient query selection and preference-guided exploration method, called SENIOR, which could select the meaningful and easy-to-comparison behavior segment pairs to improve human feedback-efficiency and accelerate policy learning with the designed preference-guided intrinsic rewards. Our key idea is twofold: (1) We designed a Motion-Distinction-based Selection scheme (MDS). It selects segment pairs with apparent motion and different directions through kernel density estimation of states, which is more task-related and easy for human preference labeling; (2) We proposed a novel preference-guided exploration method (PGE). It encourages the exploration towards the states with high preference and low visits and continuously guides the agent achieving the valuable samples. The synergy between the two mechanisms could significantly accelerate the progress of reward and policy learning. Our experiments show that SENIOR outperforms other five existing methods in both human feedback-efficiency and policy convergence speed on six complex robot manipulation tasks from simulation and four real-worlds.

</details>


### [306] [Factor-Graph-Based Passive Acoustic Navigation for Decentralized Cooperative Localization Using Bearing Elevation Depth Difference](https://arxiv.org/abs/2506.14690)
*Kalliyan Velasco,Timothy W. McLain,Joshua G. Mangelson*

Main category: cs.RO

TL;DR: 提出了一种基于因子图的多智能体水下定位框架，结合BEDD测量，利用倒置超短基线技术提高定位精度。


<details>
  <summary>Details</summary>
Motivation: 水下通信受限导致多智能体定位困难，需解决定位精度和扩展性问题。

Method: 使用因子图表示，结合BEDD测量（方位角、仰角和深度差），利用倒置USBL技术进行声学信号测量。

Result: 在HoloOcean模拟器中验证，定位精度优于航位推算，并发现声学信号异常值的影响。

Conclusion: 该方法提高了水下多智能体定位精度，但需进一步研究异常值剔除技术。

Abstract: Accurate and scalable underwater multi-agent localization remains a critical challenge due to the constraints of underwater communication. In this work, we propose a multi-agent localization framework using a factor-graph representation that incorporates bearing, elevation, and depth difference (BEDD). Our method leverages inverted ultra-short baseline (inverted-USBL) derived azimuth and elevation measurements from incoming acoustic signals and relative depth measurements to enable cooperative localization for a multi-robot team of autonomous underwater vehicles (AUVs). We validate our approach in the HoloOcean underwater simulator with a fleet of AUVs, demonstrating improved localization accuracy compared to dead reckoning. Additionally, we investigate the impact of azimuth and elevation measurement outliers, highlighting the need for robust outlier rejection techniques for acoustic signals.

</details>


### [307] [Casper: Inferring Diverse Intents for Assistive Teleoperation with Vision Language Models](https://arxiv.org/abs/2506.14727)
*Huihan Liu,Rutav Shah,Shuijing Liu,Jack Pittenger,Mingyo Seo,Yuchen Cui,Yonatan Bisk,Roberto Martín-Martín,Yuke Zhu*

Main category: cs.RO

TL;DR: Casper是一个辅助遥操作系统，利用预训练视觉语言模型的常识知识进行实时意图推断和灵活技能执行，支持多样化的移动操作任务。


<details>
  <summary>Details</summary>
Motivation: 现实世界辅助遥操作中，机器人需从用户控制输入推断广泛的人类意图并提供正确协助，现有方法局限于简单场景或特定任务数据分布。

Method: Casper结合开放世界感知模块、基于VLM的意图推断机制和技能库，支持多样化、长时程移动操作任务。

Result: 实验表明，Casper在任务性能、降低人类认知负荷和用户满意度上优于直接遥操作和现有辅助遥操作方法。

Conclusion: Casper通过常识推理和灵活技能执行，显著提升了辅助遥操作的实用性和用户体验。

Abstract: Assistive teleoperation, where control is shared between a human and a robot, enables efficient and intuitive human-robot collaboration in diverse and unstructured environments. A central challenge in real-world assistive teleoperation is for the robot to infer a wide range of human intentions from user control inputs and to assist users with correct actions. Existing methods are either confined to simple, predefined scenarios or restricted to task-specific data distributions at training, limiting their support for real-world assistance. We introduce Casper, an assistive teleoperation system that leverages commonsense knowledge embedded in pre-trained visual language models (VLMs) for real-time intent inference and flexible skill execution. Casper incorporates an open-world perception module for a generalized understanding of novel objects and scenes, a VLM-powered intent inference mechanism that leverages commonsense reasoning to interpret snippets of teleoperated user input, and a skill library that expands the scope of prior assistive teleoperation systems to support diverse, long-horizon mobile manipulation tasks. Extensive empirical evaluation, including human studies and system ablations, demonstrates that Casper improves task performance, reduces human cognitive load, and achieves higher user satisfaction than direct teleoperation and assistive teleoperation baselines.

</details>


### [308] [Tactile Beyond Pixels: Multisensory Touch Representations for Robot Manipulation](https://arxiv.org/abs/2506.14754)
*Carolina Higuera,Akash Sharma,Taosha Fan,Chaithanya Krishna Bodduluri,Byron Boots,Michael Kaess,Mike Lambeta,Tingfan Wu,Zixi Liu,Francois Robert Hogan,Mustafa Mukadam*

Main category: cs.RO

TL;DR: Sparsh-X是一种多感官触觉表示方法，整合了图像、音频、运动和压力四种触觉模态，通过自监督学习融合为统一表示，显著提升了机器人操作任务的性能。


<details>
  <summary>Details</summary>
Motivation: 研究旨在解决机器人操作任务中触觉信号的多样性和复杂性，通过多模态触觉表示提升任务成功率和鲁棒性。

Method: 利用Digit 360传感器收集约100万次接触丰富的交互数据，通过自监督学习将四种触觉模态融合为统一表示。

Result: Sparsh-X将策略成功率提升63%，鲁棒性提升90%，并在物理属性推断任务中准确率提高48%。

Conclusion: 多感官预训练在机器人灵巧操作任务中具有显著优势，Sparsh-X为触觉表示提供了高效解决方案。

Abstract: We present Sparsh-X, the first multisensory touch representations across four tactile modalities: image, audio, motion, and pressure. Trained on ~1M contact-rich interactions collected with the Digit 360 sensor, Sparsh-X captures complementary touch signals at diverse temporal and spatial scales. By leveraging self-supervised learning, Sparsh-X fuses these modalities into a unified representation that captures physical properties useful for robot manipulation tasks. We study how to effectively integrate real-world touch representations for both imitation learning and tactile adaptation of sim-trained policies, showing that Sparsh-X boosts policy success rates by 63% over an end-to-end model using tactile images and improves robustness by 90% in recovering object states from touch. Finally, we benchmark Sparsh-X ability to make inferences about physical properties, such as object-action identification, material-quantity estimation, and force estimation. Sparsh-X improves accuracy in characterizing physical properties by 48% compared to end-to-end approaches, demonstrating the advantages of multisensory pretraining for capturing features essential for dexterous manipulation.

</details>


### [309] [RobotSmith: Generative Robotic Tool Design for Acquisition of Complex Manipulation Skills](https://arxiv.org/abs/2506.14763)
*Chunru Lin,Haotian Yuan,Yian Wang,Xiaowen Qiu,Tsun-Hsuan Wang,Minghao Guo,Bohan Wang,Yashraj Narang,Dieter Fox,Chuang Gan*

Main category: cs.RO

TL;DR: 论文提出了RobotSmith，一种自动化工具设计和使用管道，结合视觉语言模型和物理仿真，优化工具几何和使用方法，显著提升机器人任务成功率。


<details>
  <summary>Details</summary>
Motivation: 现有工具设计方法依赖预定义模板或通用3D生成方法，未针对机器人操作优化，且人类设计的工具可能不适合机器人使用。

Method: RobotSmith通过协作视觉语言模型代理迭代设计工具，生成低层机器人轨迹，并联合优化工具几何和使用方法。

Result: 实验表明，该方法在多种任务中表现优异，平均成功率50.0%，显著优于3D生成（21.4%）和工具检索（11.1%）。

Conclusion: RobotSmith验证了其在实际场景中的实用性和泛化能力，生成的工具和使用计划可有效迁移到物理执行。

Abstract: Endowing robots with tool design abilities is critical for enabling them to solve complex manipulation tasks that would otherwise be intractable. While recent generative frameworks can automatically synthesize task settings, such as 3D scenes and reward functions, they have not yet addressed the challenge of tool-use scenarios. Simply retrieving human-designed tools might not be ideal since many tools (e.g., a rolling pin) are difficult for robotic manipulators to handle. Furthermore, existing tool design approaches either rely on predefined templates with limited parameter tuning or apply generic 3D generation methods that are not optimized for tool creation. To address these limitations, we propose RobotSmith, an automated pipeline that leverages the implicit physical knowledge embedded in vision-language models (VLMs) alongside the more accurate physics provided by physics simulations to design and use tools for robotic manipulation. Our system (1) iteratively proposes tool designs using collaborative VLM agents, (2) generates low-level robot trajectories for tool use, and (3) jointly optimizes tool geometry and usage for task performance. We evaluate our approach across a wide range of manipulation tasks involving rigid, deformable, and fluid objects. Experiments show that our method consistently outperforms strong baselines in terms of both task success rate and overall performance. Notably, our approach achieves a 50.0\% average success rate, significantly surpassing other baselines such as 3D generation (21.4%) and tool retrieval (11.1%). Finally, we deploy our system in real-world settings, demonstrating that the generated tools and their usage plans transfer effectively to physical execution, validating the practicality and generalization capabilities of our approach.

</details>


### [310] [GMT: General Motion Tracking for Humanoid Whole-Body Control](https://arxiv.org/abs/2506.14770)
*Zixuan Chen,Mazeyu Ji,Xuxin Cheng,Xuanbin Peng,Xue Bin Peng,Xiaolong Wang*

Main category: cs.RO

TL;DR: GMT是一个通用且可扩展的运动跟踪框架，通过自适应采样和运动混合专家架构，使类人机器人能够跟踪现实世界中的多样化运动。


<details>
  <summary>Details</summary>
Motivation: 解决类人机器人跟踪全身运动的挑战，包括运动的时间和运动学多样性、策略能力以及上下半身协调的困难。

Method: 提出GMT框架，包含自适应采样策略和运动混合专家（MoE）架构，前者平衡训练中的难易运动，后者优化运动流形的不同区域。

Result: 在仿真和现实世界中的广泛实验表明，GMT实现了最先进的性能，能够通过单一通用策略跟踪多种运动。

Conclusion: GMT通过自适应采样和MoE架构，成功解决了类人机器人运动跟踪的挑战，展现了通用性和可扩展性。

Abstract: The ability to track general whole-body motions in the real world is a useful way to build general-purpose humanoid robots. However, achieving this can be challenging due to the temporal and kinematic diversity of the motions, the policy's capability, and the difficulty of coordination of the upper and lower bodies. To address these issues, we propose GMT, a general and scalable motion-tracking framework that trains a single unified policy to enable humanoid robots to track diverse motions in the real world. GMT is built upon two core components: an Adaptive Sampling strategy and a Motion Mixture-of-Experts (MoE) architecture. The Adaptive Sampling automatically balances easy and difficult motions during training. The MoE ensures better specialization of different regions of the motion manifold. We show through extensive experiments in both simulation and the real world the effectiveness of GMT, achieving state-of-the-art performance across a broad spectrum of motions using a unified general policy. Videos and additional information can be found at https://gmt-humanoid.github.io.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [311] [Fretting-Transformer: Encoder-Decoder Model for MIDI to Tablature Transcription](https://arxiv.org/abs/2506.14223)
*Anna Hamberger,Sebastian Murgul,Jochen Schmidt,Michael Heizmann*

Main category: cs.SD

TL;DR: Fretting-Transformer是一种基于T5架构的编码器-解码器模型，用于将MIDI序列自动转录为吉他指法谱，解决了弦-品模糊性和可演奏性问题，性能优于基线方法和商业应用。


<details>
  <summary>Details</summary>
Motivation: MIDI符号缺乏吉他演奏的关键信息，而现有方法在弦-品模糊性和可演奏性方面存在不足，因此需要一种自动化转录方法。

Method: 采用T5变换器架构，将任务视为符号翻译问题，结合数据预处理和标记化策略，使用DadaGP、GuitarToday和Leduc数据集。

Result: 实验表明，Fretting-Transformer在准确性和可演奏性上优于A*和Guitar Pro等基线方法。

Conclusion: 该模型为未来吉他自动转录研究奠定了坚实基础，尤其在上下文敏感处理和调弦/变调夹条件方面表现优异。

Abstract: Music transcription plays a pivotal role in Music Information Retrieval (MIR), particularly for stringed instruments like the guitar, where symbolic music notations such as MIDI lack crucial playability information. This contribution introduces the Fretting-Transformer, an encoderdecoder model that utilizes a T5 transformer architecture to automate the transcription of MIDI sequences into guitar tablature. By framing the task as a symbolic translation problem, the model addresses key challenges, including string-fret ambiguity and physical playability. The proposed system leverages diverse datasets, including DadaGP, GuitarToday, and Leduc, with novel data pre-processing and tokenization strategies. We have developed metrics for tablature accuracy and playability to quantitatively evaluate the performance. The experimental results demonstrate that the Fretting-Transformer surpasses baseline methods like A* and commercial applications like Guitar Pro. The integration of context-sensitive processing and tuning/capo conditioning further enhances the model's performance, laying a robust foundation for future developments in automated guitar transcription.

</details>


### [312] [Manipulated Regions Localization For Partially Deepfake Audio: A Survey](https://arxiv.org/abs/2506.14396)
*Jiayi He,Jiangyan Yi,Jianhua Tao,Siding Zeng,Hao Gu*

Main category: cs.SD

TL;DR: 本文首次系统综述了部分深度伪造音频的定位任务，包括基础、现有方法分支、当前局限及潜在趋势。


<details>
  <summary>Details</summary>
Motivation: 随着音频深度伪造技术的发展，部分伪造音频攻击逐渐增多，因其隐蔽性更难被检测，带来更高安全风险。目前缺乏系统综述，本文旨在填补这一空白。

Method: 通过系统梳理部分深度伪造音频定位任务的基础、现有方法分支，分析其局限性和未来趋势。

Result: 本文提供了对部分深度伪造音频定位任务的全面视角，揭示了当前研究的不足和发展方向。

Conclusion: 本文为部分深度伪造音频定位任务提供了系统综述，指出了未来研究的潜在趋势和挑战。

Abstract: With the development of audio deepfake techniques, attacks with partially deepfake audio are beginning to rise. Compared to fully deepfake, it is much harder to be identified by the detector due to the partially cryptic manipulation, resulting in higher security risks. Although some studies have been launched, there is no comprehensive review to systematically introduce the current situations and development trends for addressing this issue. Thus, in this survey, we are the first to outline a systematic introduction for partially deepfake audio manipulated region localization tasks, including the fundamentals, branches of existing methods, current limitations and potential trends, providing a revealing insight into this scope.

</details>


### [313] [A Survey on World Models Grounded in Acoustic Physical Information](https://arxiv.org/abs/2506.13833)
*Xiaoliang Chen,Le Chang,Xin Yu,Yunhe Huang,Xianling Tu*

Main category: cs.SD

TL;DR: 该综述全面概述了基于声学物理信息的世界模型新兴领域，探讨了理论、方法框架及技术进展，重点介绍了声学信号在高保真环境感知、因果物理推理和动态事件预测模拟中的应用。


<details>
  <summary>Details</summary>
Motivation: 探索声学信号作为机械波能量载体如何编码丰富的物理信息，以推动高保真环境感知和因果推理技术的发展。

Method: 综述了物理信息神经网络（PINNs）、生成模型和自监督多模态学习框架等核心方法。

Result: 声学世界模型在机器人、自动驾驶、医疗和金融等领域有重要应用。

Conclusion: 提出了未来研究方向，包括鲁棒性、因果性、不确定性感知和负责任的声学智能，目标是构建基于声音的“直觉物理”引擎。

Abstract: This survey provides a comprehensive overview of the emerging field of world models grounded in the foundation of acoustic physical information. It examines the theoretical underpinnings, essential methodological frameworks, and recent technological advancements in leveraging acoustic signals for high-fidelity environmental perception, causal physical reasoning, and predictive simulation of dynamic events. The survey explains how acoustic signals, as direct carriers of mechanical wave energy from physical events, encode rich, latent information about material properties, internal geometric structures, and complex interaction dynamics. Specifically, this survey establishes the theoretical foundation by explaining how fundamental physical laws govern the encoding of physical information within acoustic signals. It then reviews the core methodological pillars, including Physics-Informed Neural Networks (PINNs), generative models, and self-supervised multimodal learning frameworks. Furthermore, the survey details the significant applications of acoustic world models in robotics, autonomous driving, healthcare, and finance. Finally, it systematically outlines the important technical and ethical challenges while proposing a concrete roadmap for future research directions toward robust, causal, uncertainty-aware, and responsible acoustic intelligence. These elements collectively point to a research pathway towards embodied active acoustic intelligence, empowering AI systems to construct an internal "intuitive physics" engine through sound.

</details>


### [314] [Set theoretic solution for the tuning problem](https://arxiv.org/abs/2506.13969)
*Vsevolod Vladimirovich Deriushkin*

Main category: cs.SD

TL;DR: 提出一种新的音乐调音解决方案，将Just Intonation推广到非谐波音色，并统一频谱干扰与和谐性对协和度的贡献。


<details>
  <summary>Details</summary>
Motivation: 解决音乐调音问题，特别是针对非谐波音色的调音，并提供一个统一的框架来量化协和度。

Method: 通过集合论定义两种协和度度量：亲和性与和谐性，生成动态调音系统的音程集合。

Result: 能够数学化量化音乐协和度，并生成可用于动态调音系统的音程集合。

Conclusion: 该工作为音乐协和度提供了数学量化方法，适用于广泛的受众，包括非专业音乐或数学背景的读者。

Abstract: In this paper I want to suggest a new solution to the problem of musical tuning. On one hand, I see it as a generalization of Just Intonation (JI) to inharmonic timbers, on another, as a unification of spectral interference and harmonicity contributions to consonance within a single framework. The main achievement of the work is the ability to mathematically quantify the phenomenon of musical consonance using set theory. That quantification is done by defining two measures of consonance: affinity and harmonicity. These measures naturally generate sets of intervals that can be used as dynamic tuning systems. The paper is aimed at a broad audience of people who may not be skilled in music and tuning theory or mathematics. Thus, I attempt to give as much details and explanations as I can, while keeping the number of pages as low as possible.

</details>


### [315] [Making deep neural networks work for medical audio: representation, compression and domain adaptation](https://arxiv.org/abs/2506.13970)
*Charles C Onu*

Main category: cs.SD

TL;DR: 该论文探讨了机器学习在医学音频信号分析中的应用，重点研究了婴儿哭声预测医疗状况的技术挑战，提出了四种关键方法：神经迁移学习、端到端模型压缩、领域适应技术，并发布了一个开源婴儿哭声数据集。


<details>
  <summary>Details</summary>
Motivation: 医学音频信号（如肺部、心脏和声音）包含重要的健康信息，但目前主要依赖专家听觉分析。自动化分析可以标准化处理、支持低资源环境筛查，并发现人类难以察觉的细微模式，从而促进早期诊断和治疗。

Method: 1. 利用成人语音数据库通过神经迁移学习提升婴儿哭声分析的准确性；2. 提出基于张量分解的端到端模型压缩方法；3. 开发针对音频模型的领域适应技术；4. 发布开源婴儿哭声数据集。

Result: 实现了高压缩率的便携模型、跨领域泛化能力强的音频模型，并提供了研究资源。

Conclusion: 该研究为将婴儿哭声视为重要生命体征奠定了基础，展示了AI音频监测在未来可及、可负担医疗中的潜力。

Abstract: This thesis addresses the technical challenges of applying machine learning to understand and interpret medical audio signals. The sounds of our lungs, heart, and voice convey vital information about our health. Yet, in contemporary medicine, these sounds are primarily analyzed through auditory interpretation by experts using devices like stethoscopes. Automated analysis offers the potential to standardize the processing of medical sounds, enable screening in low-resource settings where physicians are scarce, and detect subtle patterns that may elude human perception, thereby facilitating early diagnosis and treatment.
  Focusing on the analysis of infant cry sounds to predict medical conditions, this thesis contributes on four key fronts. First, in low-data settings, we demonstrate that large databases of adult speech can be harnessed through neural transfer learning to develop more accurate and robust models for infant cry analysis. Second, in cost-effective modeling, we introduce an end-to-end model compression approach for recurrent networks using tensor decomposition. Our method requires no post-hoc processing, achieves compression rates of several hundred-fold, and delivers accurate, portable models suitable for resource-constrained devices. Third, we propose novel domain adaptation techniques tailored for audio models and adapt existing methods from computer vision. These approaches address dataset bias and enhance generalization across domains while maintaining strong performance on the original data. Finally, to advance research in this domain, we release a unique, open-source dataset of infant cry sounds, developed in collaboration with clinicians worldwide.
  This work lays the foundation for recognizing the infant cry as a vital sign and highlights the transformative potential of AI-driven audio monitoring in shaping the future of accessible and affordable healthcare.

</details>


### [316] [Acoustic scattering AI for non-invasive object classifications: A case study on hair assessment](https://arxiv.org/abs/2506.14148)
*Long-Vu Hoang,Tuan Nguyen,Tran Huy Dat*

Main category: cs.SD

TL;DR: 提出一种基于声学散射的非侵入式物体分类方法，通过头发评估案例展示，利用AI驱动的深度学习实现高精度分类。


<details>
  <summary>Details</summary>
Motivation: 探索一种隐私保护、非接触式的替代视觉分类的方法，利用声学散射技术获取物体结构和材料信息。

Method: 通过发射声学刺激并捕获散射信号，采用四种深度学习策略（全监督、嵌入分类、监督基础模型微调、自监督模型微调）进行分类。

Result: 最佳策略（自监督模型微调）达到近90%的分类准确率。

Conclusion: 声学散射技术在非接触式分类中具有广泛应用潜力，尤其在隐私保护场景下。

Abstract: This paper presents a novel non-invasive object classification approach using acoustic scattering, demonstrated through a case study on hair assessment. When an incident wave interacts with an object, it generates a scattered acoustic field encoding structural and material properties. By emitting acoustic stimuli and capturing the scattered signals from head-with-hair-sample objects, we classify hair type and moisture using AI-driven, deep-learning-based sound classification. We benchmark comprehensive methods, including (i) fully supervised deep learning, (ii) embedding-based classification, (iii) supervised foundation model fine-tuning, and (iv) self-supervised model fine-tuning. Our best strategy achieves nearly 90% classification accuracy by fine-tuning all parameters of a self-supervised model. These results highlight acoustic scattering as a privacy-preserving, non-contact alternative to visual classification, opening huge potential for applications in various industries.

</details>


### [317] [Pushing the Performance of Synthetic Speech Detection with Kolmogorov-Arnold Networks and Self-Supervised Learning Models](https://arxiv.org/abs/2506.14153)
*Tuan Dat Phuong,Long-Vu Hoang,Huy Dat Tran*

Main category: cs.SD

TL;DR: 论文提出用Kolmogorov-Arnold Network（KAN）替代XLSR-Conformer模型中的传统多层感知机，显著提升了合成语音检测性能。


<details>
  <summary>Details</summary>
Motivation: 当前自监督学习（SSL）模型在合成语音检测中表现优异，但仍有改进空间，尤其是架构优化。

Method: 用Kolmogorov-Arnold Network（KAN）替代XLSR-Conformer模型中的多层感知机。

Result: 在ASVspoof2021数据集上，性能相对提升60.55%，21LA集的EER降至0.70%。

Conclusion: 将KAN整合到SSL模型中，是合成语音检测领域的一个有前景的方向。

Abstract: Recent advancements in speech synthesis technologies have led to increasingly advanced spoofing attacks, posing significant challenges for automatic speaker verification systems. While systems based on self-supervised learning (SSL) models, particularly the XLSR-Conformer model, have demonstrated remarkable performance in synthetic speech detection, there remains room for architectural improvements. In this paper, we propose a novel approach that replaces the traditional Multi-Layer Perceptron in the XLSR-Conformer model with a Kolmogorov-Arnold Network (KAN), a novel architecture based on the Kolmogorov-Arnold representation theorem. Our results on ASVspoof2021 demonstrate that integrating KAN into the SSL-based models can improve the performance by 60.55% relatively on LA and DF sets, further achieving 0.70% EER on the 21LA set. These findings suggest that incorporating KAN into SSL-based models is a promising direction for advances in synthetic speech detection.

</details>


### [318] [Investigation of Zero-shot Text-to-Speech Models for Enhancing Short-Utterance Speaker Verification](https://arxiv.org/abs/2506.14226)
*Yiyang Zhao,Shuai Wang,Guangzhi Sun,Zehua Chen,Chao Zhang,Mingxing Xu,Thomas Fang Zheng*

Main category: cs.SD

TL;DR: 研究探讨了使用零样本文本转语音（ZS-TTS）系统在测试时进行数据增强以提升短语音说话人验证性能，实验显示结合真实与合成语音样本可显著降低错误率。


<details>
  <summary>Details</summary>
Motivation: 短语音说话人验证因信息有限而准确性低，ZS-TTS系统在保留说话人身份方面有进展，因此探索其在测试时数据增强中的应用。

Method: 评估了三种预训练ZS-TTS系统（NatureSpeech 3、CosyVoice、MaskGCT）在VoxCeleb 1数据集上的表现，结合真实与合成语音样本进行实验。

Result: 结合真实与合成语音样本使相对等错误率（EER）降低10%-16%，尤其对短语音效果显著，但长合成语音效果不如长真实语音。

Conclusion: ZS-TTS在测试时数据增强中具有潜力，但也存在挑战，为未来研究提供了方向。

Abstract: Short-utterance speaker verification presents significant challenges due to the limited information in brief speech segments, which can undermine accuracy and reliability. Recently, zero-shot text-to-speech (ZS-TTS) systems have made considerable progress in preserving speaker identity. In this study, we explore, for the first time, the use of ZS-TTS systems for test-time data augmentation for speaker verification. We evaluate three state-of-the-art pre-trained ZS-TTS systems, NatureSpeech 3, CosyVoice, and MaskGCT, on the VoxCeleb 1 dataset. Our experimental results show that combining real and synthetic speech samples leads to 10%-16% relative equal error rate (EER) reductions across all durations, with particularly notable improvements for short utterances, all without retraining any existing systems. However, our analysis reveals that longer synthetic speech does not yield the same benefits as longer real speech in reducing EERs. These findings highlight the potential and challenges of using ZS-TTS for test-time speaker verification, offering insights for future research.

</details>


### [319] [SLEEPING-DISCO 9M: A large-scale pre-training dataset for generative music modeling](https://arxiv.org/abs/2506.14293)
*Tawsif Ahmed,Andrej Radonjic,Gollam Rabby*

Main category: cs.SD

TL;DR: Sleeping-DISCO 9M是一个用于音乐生成任务的大规模预训练数据集，填补了开源高质量流行音乐数据集的空白。


<details>
  <summary>Details</summary>
Motivation: 现有数据集多为合成或重新录制的音乐，未能反映真实世界的音乐风格，限制了生成音乐模型的应用。

Method: 使用实际流行音乐和世界知名艺术家的作品构建数据集。

Result: 提供了一个高质量、真实反映流行音乐的数据集。

Conclusion: Sleeping-DISCO 9M有望推动生成音乐模型的发展和应用。

Abstract: We present Sleeping-DISCO 9M, a large-scale pre-training dataset for music and song. To the best of our knowledge, there are no open-source high-quality dataset representing popular and well-known songs for generative music modeling tasks such as text-music, music-captioning, singing-voice synthesis, melody reconstruction and cross-model retrieval. Past contributions focused on isolated and constrained factors whose core perspective was to create synthetic or re-recorded music corpus (e.g. GTSinger, M4Singer) and arbitrarily large-scale audio datasets (e.g. DISCO-10M and LAIONDISCO-12M) had been another focus for the community. Unfortunately, adoption of these datasets has been below substantial in the generative music community as these datasets fail to reflect real-world music and its flavour. Our dataset changes this narrative and provides a dataset that is constructed using actual popular music and world-renowned artists.

</details>


### [320] [A Comparative Study on Proactive and Passive Detection of Deepfake Speech](https://arxiv.org/abs/2506.14398)
*Chia-Hua Wu,Wanying Ge,Xin Wang,Junichi Yamagishi,Yu Tsao,Hsin-Min Wang*

Main category: cs.SD

TL;DR: 提出一个框架，用于统一评估主动水印模型和被动深度伪造检测器在深度伪造语音检测中的性能。


<details>
  <summary>Details</summary>
Motivation: 由于主动水印模型和被动检测器在训练、优化和评估上的差异，缺乏统一的评估协议，难以选择最佳解决方案。

Method: 通过共同数据集训练和测试所有模型，使用共享指标评估性能，并分析其对不同对抗攻击的鲁棒性。

Result: 不同模型对语音属性扭曲表现出不同的脆弱性。

Conclusion: 该框架提供了公平比较和选择最佳解决方案的方法，代码已开源。

Abstract: Solutions for defending against deepfake speech fall into two categories: proactive watermarking models and passive conventional deepfake detectors. While both address common threats, their differences in training, optimization, and evaluation prevent a unified protocol for joint evaluation and selecting the best solutions for different cases. This work proposes a framework to evaluate both model types in deepfake speech detection. To ensure fair comparison and minimize discrepancies, all models were trained and tested on common datasets, with performance evaluated using a shared metric. We also analyze their robustness against various adversarial attacks, showing that different models exhibit distinct vulnerabilities to different speech attribute distortions. Our training and evaluation code is available at Github.

</details>


### [321] [Unifying Streaming and Non-streaming Zipformer-based ASR](https://arxiv.org/abs/2506.14434)
*Bidisha Sharma,Karthik Pandia Durai,Shankar Venkatesan,Jeena J Prakash,Shashi Kumar,Malolan Chetlur,Andreas Stolcke*

Main category: cs.SD

TL;DR: 提出了一种统一的流式和非流式自动语音识别（ASR）模型框架，通过动态右上下文和分块注意力掩码训练，显著降低开发成本并提升性能。


<details>
  <summary>Details</summary>
Motivation: 减少流式和非流式ASR模型的开发和部署成本，同时提升性能。

Method: 使用动态右上下文和分块注意力掩码训练zipformer模型，分析不同右上下文帧数对精度和延迟的影响。

Result: 在Librispeech和内部数据集上测试，词错误率相对降低7.9%，流式性能接近非流式模型。

Conclusion: 该方法能灵活控制延迟与精度的权衡，满足不同客户需求。

Abstract: There has been increasing interest in unifying streaming and non-streaming automatic speech recognition (ASR) models to reduce development, training, and deployment costs. We present a unified framework that trains a single end-to-end ASR model for both streaming and non-streaming applications, leveraging future context information. We propose to use dynamic right-context through the chunked attention masking in the training of zipformer-based ASR models. We demonstrate that using right-context is more effective in zipformer models compared to other conformer models due to its multi-scale nature. We analyze the effect of varying the number of right-context frames on accuracy and latency of the streaming ASR models. We use Librispeech and large in-house conversational datasets to train different versions of streaming and non-streaming models and evaluate them in a production grade server-client setup across diverse testsets of different domains. The proposed strategy reduces word error by relative 7.9\% with a small degradation in user-perceived latency. By adding more right-context frames, we are able to achieve streaming performance close to that of non-streaming models. Our approach also allows flexible control of the latency-accuracy tradeoff according to customers requirements.

</details>


### [322] [An Open Research Dataset of the 1932 Cairo Congress of Arab Music](https://arxiv.org/abs/2506.14503)
*Baris Bozkurt*

Main category: cs.SD

TL;DR: ORD-CC32是一个基于1932年开罗阿拉伯音乐大会录音的开放研究数据集，支持阿拉伯音乐调音、律制和区域差异的计算研究。


<details>
  <summary>Details</summary>
Motivation: 通过开放数据集促进计算民族音乐学、音乐信息检索、文化研究和数字遗产保护等跨学科研究。

Method: 数据集包含结构化元数据、旋律和节奏模式标签、手动标记的主音信息，以及使用先进音高检测方法提取的声学特征。

Result: 通过音高直方图的案例研究展示了数据驱动分析在微音差异研究中的潜力。

Conclusion: ORD-CC32的开放共享为相关领域的研究提供了重要资源。

Abstract: This paper introduces ORD-CC32 , an open research dataset derived from the 1932 Cairo Congress of Arab Music recordings, a historically significant collection representing diverse Arab musical traditions. The dataset includes structured metadata, melodic and rhythmic mode tags (maqam and iqa), manually labeled tonic information, and acoustic features extracted using state-of-the-art pitch detection methods. These resources support computational studies of tuning, temperament, and regional variations in Arab music. A case study using pitch histograms demonstrates the potential for data-driven analysis of microtonal differences across regions. By making this dataset openly available, we aim to enable interdisciplinary research in computational ethnomusicology, music information retrieval (MIR), cultural studies, and digital heritage preservation. ORD-CC32 is shared on Zenodo with tools for feature extraction and metadata retrieval.

</details>


### [323] [Evolving music theory for emerging musical languages](https://arxiv.org/abs/2506.14504)
*Emmanuel Deruty*

Main category: cs.SD

TL;DR: 本文重新探讨了当代流行音乐（CPM）中的音高概念，指出在电子音乐中传统假设可能失效，音高是感知构建而非客观属性。


<details>
  <summary>Details</summary>
Motivation: 传统音高理论在电子音乐中可能不适用，需重新思考音高的本质。

Method: 采用现象学和归纳法分析准谐波音调，探讨音高的多义性和感知变异性。

Result: 研究发现单音可传达多音高，音高感知具有多稳态性，调音系统可能源于音调内部结构。

Conclusion: 音高应被视为基于感知变异性的概念，挑战传统理论规范。

Abstract: This chapter reconsiders the concept of pitch in contemporary popular music (CPM), particularly in electronic contexts where traditional assumptions may fail. Drawing on phenomenological and inductive methods, it argues that pitch is not an ontologically objective property but a perceptual construct shaped by listeners and conditions. Analyses of quasi-harmonic tones reveal that a single tone can convey multiple pitches, giving rise to tonal fission. The perception of pitch may also be multistable, varying for the same listener over time. In this framework, the tuning system may emerge from a tone's internal structure. A parallel with the coastline paradox supports a model of pitch grounded in perceptual variability, challenging inherited theoretical norms.

</details>


### [324] [Refining music sample identification with a self-supervised graph neural network](https://arxiv.org/abs/2506.14684)
*Aditya Bhattacharjee,Ivan Meresman Higgs,Mark Sandler,Emmanouil Benetos*

Main category: cs.SD

TL;DR: 本文提出了一种轻量级且可扩展的编码架构，结合图神经网络和对比学习框架，用于自动样本识别（ASID）。该系统在参数数量仅为当前最优系统的9%的情况下，性能相当，平均精度（mAP）达到44.2%。


<details>
  <summary>Details</summary>
Motivation: 音频样本识别（ASID）在音频检索中至关重要，但现有系统难以应对音乐制作中的常见变换（如时间拉伸、音高变换等）。因此，开发一个对这些变换鲁棒的系统是一个重要挑战。

Method: 采用图神经网络和对比学习框架，提出两阶段检索方法：先进行粗粒度相似性搜索筛选候选，再通过交叉注意力分类器剔除无关匹配并优化排序。

Result: 系统在参数大幅减少的情况下，性能与当前最优系统相当，mAP达到44.2%。同时，针对短时查询进行了优化，并发布了Sample100数据集的细粒度标注。

Conclusion: 该方法在轻量化和性能上取得了平衡，为ASID任务提供了一种高效解决方案，并公开了新数据集以支持未来研究。

Abstract: Automatic sample identification (ASID), the detection and identification of portions of audio recordings that have been reused in new musical works, is an essential but challenging task in the field of audio query-based retrieval. While a related task, audio fingerprinting, has made significant progress in accurately retrieving musical content under "real world" (noisy, reverberant) conditions, ASID systems struggle to identify samples that have undergone musical modifications. Thus, a system robust to common music production transformations such as time-stretching, pitch-shifting, effects processing, and underlying or overlaying music is an important open challenge.
  In this work, we propose a lightweight and scalable encoding architecture employing a Graph Neural Network within a contrastive learning framework. Our model uses only 9% of the trainable parameters compared to the current state-of-the-art system while achieving comparable performance, reaching a mean average precision (mAP) of 44.2%.
  To enhance retrieval quality, we introduce a two-stage approach consisting of an initial coarse similarity search for candidate selection, followed by a cross-attention classifier that rejects irrelevant matches and refines the ranking of retrieved candidates - an essential capability absent in prior models. In addition, because queries in real-world applications are often short in duration, we benchmark our system for short queries using new fine-grained annotations for the Sample100 dataset, which we publish as part of this work.

</details>


### [325] [Adaptive Accompaniment with ReaLchords](https://arxiv.org/abs/2506.14723)
*Yusong Wu,Tim Cooijmans,Kyle Kastner,Adam Roberts,Ian Simon,Alexander Scarlatos,Chris Donahue,Cassie Tarakajian,Shayegan Omidshafiei,Aaron Courville,Pablo Samuel Castro,Natasha Jaques,Cheng-Zhi Anna Huang*

Main category: cs.SD

TL;DR: ReaLchords是一个在线生成模型，用于实时生成和弦伴奏，通过强化学习优化，实现了与用户旋律的和谐配合。


<details>
  <summary>Details</summary>
Motivation: 当前生成模型无法实时与其他音乐家协作，限制了即兴演奏的可能性。

Method: 结合最大似然预训练和强化学习微调，利用新颖的奖励模型和未来旋律蒸馏技术。

Result: 模型能适应陌生输入并生成合适的伴奏，通过实验和听测验证。

Conclusion: ReaLchords为实时即兴演奏和多模态协作提供了新可能。

Abstract: Jamming requires coordination, anticipation, and collaborative creativity between musicians. Current generative models of music produce expressive output but are not able to generate in an \emph{online} manner, meaning simultaneously with other musicians (human or otherwise). We propose ReaLchords, an online generative model for improvising chord accompaniment to user melody. We start with an online model pretrained by maximum likelihood, and use reinforcement learning to finetune the model for online use. The finetuning objective leverages both a novel reward model that provides feedback on both harmonic and temporal coherency between melody and chord, and a divergence term that implements a novel type of distillation from a teacher model that can see the future melody. Through quantitative experiments and listening tests, we demonstrate that the resulting model adapts well to unfamiliar input and produce fitting accompaniment. ReaLchords opens the door to live jamming, as well as simultaneous co-creation in other modalities.

</details>


### [326] [Exploring Speaker Diarization with Mixture of Experts](https://arxiv.org/abs/2506.14750)
*Gaobin Yang,Maokui He,Shutong Niu,Ruoyu Wang,Hang Chen,Jun Du*

Main category: cs.SD

TL;DR: 提出了一种新型神经说话人分割系统NSD-MS2S，结合记忆感知多说话人嵌入和序列到序列架构，并引入SS-MoE模块提升性能，实验验证了其优越性。


<details>
  <summary>Details</summary>
Motivation: 解决复杂声学场景下说话人分割的鲁棒性和泛化性问题。

Method: 结合记忆感知多说话人嵌入模块和Seq2Seq架构，引入SS-MoE模块减少模型偏差。

Result: 在多个复杂数据集上取得SOTA结果，鲁棒性和泛化性显著提升。

Conclusion: NSD-MS2S及其扩展模型NSD-MS2S-SSMoE在真实场景中表现优异，具有实际应用价值。

Abstract: In this paper, we propose a novel neural speaker diarization system using memory-aware multi-speaker embedding with sequence-to-sequence architecture (NSD-MS2S), which integrates a memory-aware multi-speaker embedding module with a sequence-to-sequence architecture. The system leverages a memory module to enhance speaker embeddings and employs a Seq2Seq framework to efficiently map acoustic features to speaker labels. Additionally, we explore the application of mixture of experts in speaker diarization, and introduce a Shared and Soft Mixture of Experts (SS-MoE) module to further mitigate model bias and enhance performance. Incorporating SS-MoE leads to the extended model NSD-MS2S-SSMoE. Experiments on multiple complex acoustic datasets, including CHiME-6, DiPCo, Mixer 6 and DIHARD-III evaluation sets, demonstrate meaningful improvements in robustness and generalization. The proposed methods achieve state-of-the-art results, showcasing their effectiveness in challenging real-world scenarios.

</details>


<div id='astro-ph.GA'></div>

# astro-ph.GA [[Back]](#toc)

### [327] [NeuralPDR: Neural Differential Equations as surrogate models for Photodissociation Regions](https://arxiv.org/abs/2506.14270)
*Gijs Vermariën,Thomas G. Bisbas,Serena Viti,Yue Zhao,Xuefei Tang,Rahul Ravichandran*

Main category: astro-ph.GA

TL;DR: 论文提出了一种基于神经网络的替代模型（Latent Augmented Neural ODEs），用于高效模拟天体化学过程，以解决高分辨率望远镜观测需求下的计算成本问题。


<details>
  <summary>Details</summary>
Motivation: 高分辨率望远镜（如JWST和ALMA）能够解析天体的小尺度结构，但耦合三维流体动力学和化学的模拟计算成本极高，需要高效的替代模型。

Method: 使用Latent Augmented Neural ODEs作为化学求解器的替代模型，并在三个不同复杂度的数据集上进行训练，包括来自3D-PDR模拟的数据。

Result: 替代模型能够显著加速计算，并准确复现原始观测数据中的柱密度分布图。

Conclusion: 该方法实现了化学过程的快速推断（在GPU上），为观测数据的统计分析或提高流体动力学模拟分辨率提供了可能。

Abstract: Computational astrochemical models are essential for helping us interpret and understand the observations of different astrophysical environments. In the age of high-resolution telescopes such as JWST and ALMA, the substructure of many objects can be resolved, raising the need for astrochemical modeling at these smaller scales, meaning that the simulations of these objects need to include both the physics and chemistry to accurately model the observations. The computational cost of the simulations coupling both the three-dimensional hydrodynamics and chemistry is enormous, creating an opportunity for surrogate models that can effectively substitute the chemical solver. In this work we present surrogate models that can replace the original chemical code, namely Latent Augmented Neural Ordinary Differential Equations. We train these surrogate architectures on three datasets of increasing physical complexity, with the last dataset derived directly from a three-dimensional simulation of a molecular cloud using a Photodissociation Region (PDR) code, 3D-PDR. We show that these surrogate models can provide speedup and reproduce the original observable column density maps of the dataset. This enables the rapid inference of the chemistry (on the GPU), allowing for the faster statistical inference of observations or increasing the resolution in hydrodynamical simulations of astrophysical environments.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [328] [Sketched Sum-Product Networks for Joins](https://arxiv.org/abs/2506.14034)
*Brian Tsan,Abylay Amanbayev,Asoke Datta,Florin Rusu*

Main category: cs.DB

TL;DR: 论文提出使用Sum-Product Networks动态近似草图，以解决传统草图方法在查询优化中局限于预定义选择的问题。


<details>
  <summary>Details</summary>
Motivation: 传统草图方法在多路连接基数估计中虽高效，但仅适用于预定义的查询选择，限制了其在新查询中的适用性。

Method: 利用Sum-Product Networks分解和建模多元分布（如关系），通过线性组合单变量分布（表示为草图）动态近似查询选择的草图。

Result: 实现了Fast-AGMS和Bound Sketch方法的近似，为查询优化提供了实用替代方案。

Conclusion: 动态近似草图方法扩展了草图在查询优化中的应用范围，提高了灵活性。

Abstract: Sketches have shown high accuracy in multi-way join cardinality estimation, a critical problem in cost-based query optimization. Accurately estimating the cardinality of a join operation -- analogous to its computational cost -- allows the optimization of query execution costs in relational database systems. However, although sketches have shown high efficacy in query optimization, they are typically constructed specifically for predefined selections in queries that are assumed to be given a priori, hindering their applicability to new queries. As a more general solution, we propose for Sum-Product Networks to dynamically approximate sketches on-the-fly. Sum-Product Networks can decompose and model multivariate distributions, such as relations, as linear combinations of multiple univariate distributions. By representing these univariate distributions as sketches, Sum-Product Networks can combine them element-wise to efficiently approximate the sketch of any query selection. These approximate sketches can then be applied to join cardinality estimation. In particular, we implement the Fast-AGMS and Bound Sketch methods, which have successfully been used in prior work, despite their costly construction. By accurately approximating them instead, our work provides a practical alternative to apply these sketches to query optimization.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [329] [A Novel Indicator for Quantifying and Minimizing Information Utility Loss of Robot Teams](https://arxiv.org/abs/2506.14237)
*Xiyu Zhao,Qimei Cui,Wei Ni,Quan Z. Sheng,Abbas Jamalipour,Guoshun Nan,Xiaofeng Tao,Ping Zhang*

Main category: cs.DC

TL;DR: 提出了一种新指标LoIU，用于量化信息的新鲜度和效用，并通过半分散式多智能体框架优化传输调度和资源分配，显著提升信息效用。


<details>
  <summary>Details</summary>
Motivation: 机器人团队中信息交换受限于无线容量，可能导致协作误差，需量化信息效用并优化传输。

Method: 提出LoIU指标，基于信念分布估计LoIU，开发半分散式多智能体框架优化传输调度和资源分配。

Result: 仿真显示，相比其他方法，信息新鲜度和效用提升了98%。

Conclusion: LoIU指标和半分散式框架有效优化了机器人团队的信息交换，显著提升协作效率。

Abstract: The timely exchange of information among robots within a team is vital, but it can be constrained by limited wireless capacity. The inability to deliver information promptly can result in estimation errors that impact collaborative efforts among robots. In this paper, we propose a new metric termed Loss of Information Utility (LoIU) to quantify the freshness and utility of information critical for cooperation. The metric enables robots to prioritize information transmissions within bandwidth constraints. We also propose the estimation of LoIU using belief distributions and accordingly optimize both transmission schedule and resource allocation strategy for device-to-device transmissions to minimize the time-average LoIU within a robot team. A semi-decentralized Multi-Agent Deep Deterministic Policy Gradient framework is developed, where each robot functions as an actor responsible for scheduling transmissions among its collaborators while a central critic periodically evaluates and refines the actors in response to mobility and interference. Simulations validate the effectiveness of our approach, demonstrating an enhancement of information freshness and utility by 98%, compared to alternative methods.

</details>


### [330] [Déjà Vu: Efficient Video-Language Query Engine with Learning-based Inter-Frame Computation Reuse](https://arxiv.org/abs/2506.14107)
*Jinwoo Hwang,Daeun Kim,Sangyeop Lee,Yoonsung Kim,Guseul Heo,Hojoon Kim,Yunseok Jeong,Tadiwos Meaza,Eunhyeok Park,Jeongseob Ahn,Jongse Park*

Main category: cs.DC

TL;DR: Déjà Vu是一种视频语言查询引擎，通过重用连续帧的计算加速ViT-based VideoLMs，显著提升大规模视频分析的实用性。


<details>
  <summary>Details</summary>
Motivation: 传统ViT-based VideoLMs在处理大规模视频时需要逐帧计算，计算开销大，限制了实际部署。

Method: 提出ReuseViT模型，检测帧间重用机会，结合内存-计算联合压缩技术，将计算节省转化为实际性能提升。

Result: 在三个VideoLM任务中，Déjà Vu将嵌入生成速度提升至2.64倍，误差控制在2%以内。

Conclusion: Déjà Vu显著提升了VideoLMs在大规模视频分析中的实用性，平衡了准确性与计算效率。

Abstract: Recently, Video-Language Models (VideoLMs) have demonstrated remarkable capabilities, offering significant potential for flexible and powerful video query systems. These models typically rely on Vision Transformers (ViTs), which process video frames individually to extract visual embeddings. However, generating embeddings for large-scale videos requires ViT inferencing across numerous frames, posing a major hurdle to real-world deployment and necessitating solutions for integration into scalable video data management systems. This paper introduces Déjà Vu, a video-language query engine that accelerates ViT-based VideoLMs by reusing computations across consecutive frames. At its core is ReuseViT, a modified ViT model specifically designed for VideoLM tasks, which learns to detect inter-frame reuse opportunities, striking an effective balance between accuracy and reuse. Although ReuseViT significantly reduces computation, these savings do not directly translate into performance gains on GPUs. To overcome this, Déjà Vu integrates memory-compute joint compaction techniques that convert the FLOP savings into tangible performance gains. Evaluations on three VideoLM tasks show that Déjà Vu accelerates embedding generation by up to a 2.64x within a 2% error bound, dramatically enhancing the practicality of VideoLMs for large-scale video analytics.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [331] [StorySage: Conversational Autobiography Writing Powered by a Multi-Agent Framework](https://arxiv.org/abs/2506.14159)
*Shayan Talaei,Meijin Li,Kanu Grover,James Kent Hippler,Diyi Yang,Amin Saberi*

Main category: cs.HC

TL;DR: StorySage是一个多代理系统，支持灵活对话和结构化自传写作，通过迭代收集用户记忆并更新自传，实验表明其优于基线系统。


<details>
  <summary>Details</summary>
Motivation: 现有写作助手难以捕捉个人记忆并构建完整自传，因此需要一种更灵活、用户驱动的解决方案。

Method: 采用多代理框架（包括采访者、会话记录员、规划者、章节作者和会话协调员），迭代收集记忆并更新自传。

Result: 实验模拟和用户研究（N=28）显示，StorySage在会话流畅性、叙事完整性和用户满意度上优于基线系统。

Conclusion: StorySage为自传写作提供了新架构，并展示了多代理系统如何增强人机创意合作。

Abstract: Every individual carries a unique and personal life story shaped by their memories and experiences. However, these memories are often scattered and difficult to organize into a coherent narrative, a challenge that defines the task of autobiography writing. Existing conversational writing assistants tend to rely on generic user interactions and pre-defined guidelines, making it difficult for these systems to capture personal memories and develop a complete biography over time. We introduce StorySage, a user-driven software system designed to meet the needs of a diverse group of users that supports a flexible conversation and a structured approach to autobiography writing. Powered by a multi-agent framework composed of an Interviewer, Session Scribe, Planner, Section Writer, and Session Coordinator, our system iteratively collects user memories, updates their autobiography, and plans for future conversations. In experimental simulations, StorySage demonstrates its ability to navigate multiple sessions and capture user memories across many conversations. User studies (N=28) highlight how StorySage maintains improved conversational flow, narrative completeness, and higher user satisfaction when compared to a baseline. In summary, StorySage contributes both a novel architecture for autobiography writing and insights into how multi-agent systems can enhance human-AI creative partnerships.

</details>


### [332] [A Systematic Review of User-Centred Evaluation of Explainable AI in Healthcare](https://arxiv.org/abs/2506.13904)
*Ivania Donoso-Guzmán,Kristýna Sirka Kacafírková,Maxwell Szymanski,An Jacobs,Denis Parra,Katrien Verbert*

Main category: cs.HC

TL;DR: 该研究填补了可解释人工智能（XAI）在医疗领域缺乏实用评估框架的空白，提出了一个基于用户体验的评估框架和指南。


<details>
  <summary>Details</summary>
Motivation: 当前XAI方法在真实场景中的实用价值和评估不足，缺乏明确的用户评估指南。

Method: 通过系统综述82项医疗领域的用户研究，结合预定义编码方案和迭代归纳编码，分析XAI评估实践。

Result: 总结了当前评估实践，揭示了医疗XAI中以人为本的趋势，提出了解释属性间的关系，并更新了评估框架和指南。

Conclusion: 研究为跨学科团队提供了针对特定应用场景的XAI评估策略设计工具，提升了XAI的实用性和可信度。

Abstract: Despite promising developments in Explainable Artificial Intelligence, the practical value of XAI methods remains under-explored and insufficiently validated in real-world settings. Robust and context-aware evaluation is essential, not only to produce understandable explanations but also to ensure their trustworthiness and usability for intended users, but tends to be overlooked because of no clear guidelines on how to design an evaluation with users.
  This study addresses this gap with two main goals: (1) to develop a framework of well-defined, atomic properties that characterise the user experience of XAI in healthcare; and (2) to provide clear, context-sensitive guidelines for defining evaluation strategies based on system characteristics.
  We conducted a systematic review of 82 user studies, sourced from five databases, all situated within healthcare settings and focused on evaluating AI-generated explanations. The analysis was guided by a predefined coding scheme informed by an existing evaluation framework, complemented by inductive codes developed iteratively.
  The review yields three key contributions: (1) a synthesis of current evaluation practices, highlighting a growing focus on human-centred approaches in healthcare XAI; (2) insights into the interrelations among explanation properties; and (3) an updated framework and a set of actionable guidelines to support interdisciplinary teams in designing and implementing effective evaluation strategies for XAI systems tailored to specific application contexts.

</details>


### [333] [Balancing Caregiving and Self-Care: Exploring Mental Health Needs of Alzheimer's and Dementia Caregivers](https://arxiv.org/abs/2506.14196)
*Jiayue Melissa Shi,Keran Wang,Dong Whi Yoo,Ravi Karkar,Koustuv Saha*

Main category: cs.HC

TL;DR: 研究探讨了阿尔茨海默病及相关痴呆症（AD/ADRD）家庭照顾者的心理健康需求，分析了其心理负担的演变及现有支持技术的不足，提出了动态干预设计的建议。


<details>
  <summary>Details</summary>
Motivation: 家庭照顾者因长期护理责任面临心理健康挑战，但现有支持系统未能满足其动态需求。

Method: 通过25名家庭照顾者的半结构化访谈，分析心理健康问题的成因、影响及演变阶段。

Result: 研究发现照顾者心理健康需求随护理阶段变化，现有技术需更个性化、可扩展且适应动态需求。

Conclusion: 研究为设计动态、阶段敏感的干预措施提供了基础，以全面支持照顾者心理健康。

Abstract: Alzheimer's Disease and Related Dementias (AD/ADRD) are progressive neurodegenerative conditions that impair memory, thought processes, and functioning. Family caregivers of individuals with AD/ADRD face significant mental health challenges due to long-term caregiving responsibilities. Yet, current support systems often overlook the evolving nature of their mental wellbeing needs. Our study examines caregivers' mental wellbeing concerns, focusing on the practices they adopt to manage the burden of caregiving and the technologies they use for support. Through semi-structured interviews with 25 family caregivers of individuals with AD/ADRD, we identified the key causes and effects of mental health challenges, and developed a temporal mapping of how caregivers' mental wellbeing evolves across three distinct stages of the caregiving journey. Additionally, our participants shared insights into improvements for existing mental health technologies, emphasizing the need for accessible, scalable, and personalized solutions that adapt to caregivers' changing needs over time. These findings offer a foundation for designing dynamic, stage-sensitive interventions that holistically support caregivers' mental wellbeing, benefiting both caregivers and care recipients.

</details>


### [334] [Controlling Context: Generative AI at Work in Integrated Circuit Design and Other High-Precision Domains](https://arxiv.org/abs/2506.14567)
*Emanuel Moss,Elizabeth Watkins,Christopher Persaud,Passant Karunaratne,Dawn Nafus*

Main category: cs.HC

TL;DR: 论文探讨了高精度领域工程师使用生成式AI工具时的挑战，重点关注准确性问题和交互上下文控制。


<details>
  <summary>Details</summary>
Motivation: 随着生成式AI工具在工程领域的普及，高精度领域工程师如何保持对错误的警觉性及其使用中的其他问题成为研究动机。

Method: 通过访谈硬件和软件工程师及其合作者，分析生成式AI工具在集成电路设计中的使用及其问题。

Result: 研究发现，工程师面临的主要挑战是控制与生成式AI工具的交互上下文。

Conclusion: 建议通过增强交互上下文控制能力来缓解这一问题。

Abstract: Generative AI tools have become more prevalent in engineering workflows, particularly through chatbots and code assistants. As the perceived accuracy of these tools improves, questions arise about whether and how those who work in high-precision domains might maintain vigilance for errors, and what other aspects of using such tools might trouble their work. This paper analyzes interviews with hardware and software engineers, and their collaborators, who work in integrated circuit design to identify the role accuracy plays in their use of generative AI tools and what other forms of trouble they face in using such tools. The paper inventories these forms of trouble, which are then mapped to elements of generative AI systems, to conclude that controlling the context of interactions between engineers and the generative AI tools is one of the largest challenges they face. The paper concludes with recommendations for mitigating this form of trouble by increasing the ability to control context interactively.

</details>


### [335] [StreetLens: Enabling Human-Centered AI Agents for Neighborhood Assessment from Street View Imagery](https://arxiv.org/abs/2506.14670)
*Jina Kim,Leeje Jang,Yao-Yi Chiang,Guanyu Wang,Michelle Pasco*

Main category: cs.HC

TL;DR: StreetLens是一个基于视觉语言模型（VLM）的工作流，用于自动化社区环境评估，结合社会科学专业知识，支持研究者自定义分析过程。


<details>
  <summary>Details</summary>
Motivation: 传统社区研究方法耗时且依赖专家干预，现有技术缺乏适应性和灵活性。StreetLens旨在解决这些问题，提供可扩展的自动化评估工具。

Method: StreetLens通过问题驱动的方式分析街景图像（SVI），生成从客观特征到主观感知的语义标注，支持研究者自定义提示和整合先验数据。

Result: StreetLens实现了灵活、高效的社区环境评估，支持多样化的研究设计和地理背景。

Conclusion: StreetLens推动了AI系统与研究者协作的灵活性，加速和扩展了社区研究。

Abstract: Traditionally, neighborhood studies have employed interviews, surveys, and manual image annotation guided by detailed protocols to identify environmental characteristics, including physical disorder, decay, street safety, and sociocultural symbols, and to examine their impact on developmental and health outcomes. While these methods yield rich insights, they are time-consuming and require intensive expert intervention. Recent technological advances, including vision-language models (VLMs), have begun to automate parts of this process; however, existing efforts are often ad hoc and lack adaptability across research designs and geographic contexts. In this demo paper, we present StreetLens, a human-centered, researcher-configurable workflow that embeds relevant social science expertise in a VLM for scalable neighborhood environmental assessments. StreetLens mimics the process of trained human coders by grounding the analysis in questions derived from established interview protocols, retrieving relevant street view imagery (SVI), and generating a wide spectrum of semantic annotations from objective features (e.g., the number of cars) to subjective perceptions (e.g., the sense of disorder in an image). By enabling researchers to define the VLM's role through domain-informed prompting, StreetLens places domain knowledge at the core of the analysis process. It also supports the integration of prior survey data to enhance robustness and expand the range of characteristics assessed across diverse settings. We provide a Google Colab notebook to make StreetLens accessible and extensible for researchers working with public or custom SVI datasets. StreetLens represents a shift toward flexible, agentic AI systems that work closely with researchers to accelerate and scale neighborhood studies.

</details>


### [336] [Design an Editable Speech-to-Sign-Language Transformer System: A Human-Centered AI Approach](https://arxiv.org/abs/2506.14677)
*Yingchao Li*

Main category: cs.HC

TL;DR: 本文提出了一种基于Transformer的实时、用户自适应的语音到手语动画系统，通过可编辑的JSON中间层增强用户控制，显著提升了自然性和可用性。


<details>
  <summary>Details</summary>
Motivation: 解决现有手语技术中用户无法直接检查和修改手语片段的问题，提升自然性、表现力和用户参与度。

Method: 结合流式Conformer编码器和自回归Transformer-MDN解码器，将语音输入同步转化为3D虚拟形象的上半身和面部动作，并通过可编辑的JSON层实现用户干预。

Result: 实验表明，可编辑界面和用户反馈显著提高了理解度、自然性、可用性和信任度，同时降低了认知负荷。系统在标准硬件上实现每帧低于20毫秒的推理速度。

Conclusion: 该研究展示了技术和用户参与创新如何共同推动手语技术的可访问性、可解释性和用户适应性。

Abstract: This paper presents a human-centered, real-time, user-adaptive speech-to-sign language animation system that integrates Transformer-based motion generation with a transparent, user-editable JSON intermediate layer. The framework overcomes key limitations in prior sign language technologies by enabling direct user inspection and modification of sign segments, thus enhancing naturalness, expressiveness, and user agency. Leveraging a streaming Conformer encoder and autoregressive Transformer-MDN decoder, the system synchronizes spoken input into upper-body and facial motion for 3D avatar rendering. Edits and user ratings feed into a human-in-the-loop optimization loop for continuous improvement. Experiments with 20 deaf signers and 5 interpreters show that the editable interface and participatory feedback significantly improve comprehension, naturalness, usability, and trust, while lowering cognitive load. With sub-20 ms per-frame inference on standard hardware, the system is ready for real-time communication and education. This work illustrates how technical and participatory innovation together enable accessible, explainable, and user-adaptive AI for sign language technology.

</details>


<div id='stat.AP'></div>

# stat.AP [[Back]](#toc)

### [337] [Markov Regime-Switching Intelligent Driver Model for Interpretable Car-Following Behavior](https://arxiv.org/abs/2506.14762)
*Chengyuan Zhang,Cathy Wu,Lijun Sun*

Main category: stat.AP

TL;DR: 论文提出了一种基于多模态驾驶行为的切换框架（FHMM-IDM），通过分离驾驶模式和交通场景，提高了驾驶模型的准确性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 传统驾驶模型（如IDM）因单一结构无法捕捉人类驾驶的多模态行为，导致模型精度和参数可解释性降低。

Method: 采用基于IDM的因子隐马尔可夫模型（FHMM-IDM），通过贝叶斯推断和MCMC估计参数和潜在状态。

Result: 在HighD数据集上验证，FHMM-IDM成功分离了驾驶行为和交通场景，揭示了动态切换模式。

Conclusion: 该框架为不确定性下的驾驶行为建模提供了有效解决方案，提升了交通模拟和安全分析的准确性。

Abstract: Accurate and interpretable car-following models are essential for traffic simulation and autonomous vehicle development. However, classical models like the Intelligent Driver Model (IDM) are fundamentally limited by their parsimonious and single-regime structure. They fail to capture the multi-modal nature of human driving, where a single driving state (e.g., speed, relative speed, and gap) can elicit many different driver actions. This forces the model to average across distinct behaviors, reducing its fidelity and making its parameters difficult to interpret. To overcome this, we introduce a regime-switching framework that allows driving behavior to be governed by different IDM parameter sets, each corresponding to an interpretable behavioral mode. This design enables the model to dynamically switch between interpretable behavioral modes, rather than averaging across diverse driving contexts. We instantiate the framework using a Factorial Hidden Markov Model with IDM dynamics (FHMM-IDM), which explicitly separates intrinsic driving regimes (e.g., aggressive acceleration, steady-state following) from external traffic scenarios (e.g., free-flow, congestion, stop-and-go) through two independent latent Markov processes. Bayesian inference via Markov chain Monte Carlo (MCMC) is used to jointly estimate the regime-specific parameters, transition dynamics, and latent state trajectories. Experiments on the HighD dataset demonstrate that FHMM-IDM uncovers interpretable structure in human driving, effectively disentangling internal driver actions from contextual traffic conditions and revealing dynamic regime-switching patterns. This framework provides a tractable and principled solution to modeling context-dependent driving behavior under uncertainty, offering improvements in the fidelity of traffic simulations, the efficacy of safety analyses, and the development of more human-centric ADAS.

</details>


<div id='q-bio.GN'></div>

# q-bio.GN [[Back]](#toc)

### [338] [DeepSeq: High-Throughput Single-Cell RNA Sequencing Data Labeling via Web Search-Augmented Agentic Generative AI Foundation Models](https://arxiv.org/abs/2506.13817)
*Saleem A. Al Dajani,Abel Sanchez,John R. Williams*

Main category: q-bio.GN

TL;DR: 提出了一种基于生成式AI基础模型和实时网络搜索的方法，用于自动标注单细胞RNA测序数据，准确率达82.5%，解决了监督学习中标注效率低的问题。


<details>
  <summary>Details</summary>
Motivation: 单细胞RNA测序数据规模迅速增长，传统人工标注效率低且易出错，亟需自动化解决方案。

Method: 采用具有实时网络搜索能力的代理基础模型，自动化标注实验数据。

Result: 实现了82.5%的标注准确率，支持虚拟细胞基础模型的开发，用于细胞分型和扰动预测等任务。

Conclusion: 该方法展示了在健康监测和诊断领域的创新潜力，未来可能超越人类标注性能，推动大规模扰动筛选的可靠推断。

Abstract: Generative AI foundation models offer transformative potential for processing structured biological data, particularly in single-cell RNA sequencing, where datasets are rapidly scaling toward billions of cells. We propose the use of agentic foundation models with real-time web search to automate the labeling of experimental data, achieving up to 82.5% accuracy. This addresses a key bottleneck in supervised learning for structured omics data by increasing annotation throughput without manual curation and human error. Our approach enables the development of virtual cell foundation models capable of downstream tasks such as cell-typing and perturbation prediction. As data volume grows, these models may surpass human performance in labeling, paving the way for reliable inference in large-scale perturbation screens. This application demonstrates domain-specific innovation in health monitoring and diagnostics, aligned with efforts like the Human Cell Atlas and Human Tumor Atlas Network.

</details>


<div id='cs.LO'></div>

# cs.LO [[Back]](#toc)

### [339] [Asymptotically Smaller Encodings for Graph Problems and Scheduling](https://arxiv.org/abs/2506.14042)
*Bernardo Subercaseaux*

Main category: cs.LO

TL;DR: 论文提出了一种新的图问题编码方法，将顶点覆盖、独立集和k-着色等问题编码为CNF，仅需O(|V|²/lg|V|)子句，优于传统方法的Ω(|V|²)。此外，针对密集区间图的独立集问题，提出了一种仅需O(|V|lg|V|)子句的新编码方法，并成功应用于字符串压缩和调度问题。


<details>
  <summary>Details</summary>
Motivation: 传统图问题编码方法需要大量子句（Ω(|V|²)），效率较低。论文旨在通过更高效的编码方法减少子句数量，提升计算效率。

Method: 利用Erdős、Chung和Spencer（1983）的双团覆盖结果，提出了一种新的CNF编码方法。针对密集区间图的独立集问题，设计了仅需O(|V|lg|V|)子句的编码。

Result: 成功将图问题编码的子句数量从Ω(|V|²)减少到O(|V|²/lg|V|)，并在密集区间图中进一步减少到O(|V|lg|V|)。此外，调度问题的编码规模从O(NMT²)降低到O(NMT + MT²lgT)。

Conclusion: 论文提出的编码方法显著减少了子句数量，为图问题的高效求解提供了新思路，并在实际应用中验证了其有效性。

Abstract: We show how several graph problems (e.g., vertex-cover, independent-set, $k$-coloring) can be encoded into CNF using only $O(|V|^2 / \lg |V|)$ many clauses, as opposed to the $Ω(|V|^2)$ constraints used by standard encodings. This somewhat surprising result is a simple consequence of a result of Erdős, Chung, and Spencer (1983) about biclique coverings of graphs, and opens theoretical avenues to understand the success of "Bounded Variable Addition'' (Manthey, Heule, and Biere, 2012) as a preprocessing tool. Finally, we show a novel encoding for independent sets in some dense interval graphs using only $O(|V| \lg |V|)$ clauses (the direct encoding uses $Ω(|V|^2)$), which we have successfully applied to a string-compression encoding posed by Bannai et al. (2022). As a direct byproduct, we obtain a reduction in the encoding size of a scheduling problem posed by Mayank and Modal (2020) from $O(NMT^2)$ to $O(NMT + M T^2 \lg T)$, where $N$ is the number of tasks, $T$ the total timespan, and $M$ the number of machines.

</details>


<div id='cs.SI'></div>

# cs.SI [[Back]](#toc)

### [340] [AMLgentex: Mobilizing Data-Driven Research to Combat Money Laundering](https://arxiv.org/abs/2506.13989)
*Johan Östman,Edvin Callisen,Anton Chen,Kristiina Ausmees,Emanuel Gårdh,Jovan Zamac,Jolanta Goldsteine,Hugo Wefer,Simon Whelan,Markus Reimegård*

Main category: cs.SI

TL;DR: AMLGentex是一个开源工具，用于生成真实的、可配置的交易数据，并评估反洗钱（AML）系统的性能，解决了现有合成数据集的局限性。


<details>
  <summary>Details</summary>
Motivation: 洗钱活动每年涉及数万亿美元，但仅有少量被揭露，现有合成数据集未能模拟真实洗钱的结构和行为复杂性。

Method: 开发了AMLGentex，一个开源套件，生成可配置的交易数据，并在模拟真实挑战的环境中评估AML系统。

Result: AMLGentex能够系统性地评估AML方法，反映实际洗钱场景的复杂性。

Conclusion: AMLGentex为AML系统的评估提供了更真实的测试环境，有助于改进洗钱检测方法。

Abstract: Money laundering enables organized crime by allowing illicit funds to enter the legitimate economy. Although trillions of dollars are laundered each year, only a small fraction is ever uncovered. This stems from a range of factors, including deliberate evasion by launderers, the rarity of confirmed cases, and the limited visibility each financial institution has into the global transaction network. While several synthetic datasets are available, they fail to model the structural and behavioral complexity of real-world money laundering. In particular, they often overlook partial observability, sparse and uncertain labels, strategic behavior, temporal dynamics, class imbalance, and network-level dependencies. To address these limitations, we present AMLGentex, an open-source suite for generating realistic, configurable transaction data and benchmarking detection methods. It enables systematic evaluation of anti-money laundering (AML) systems in a controlled environment that captures key real-world challenges. We demonstrate how the framework can be used to rigorously evaluate methods under conditions that reflect the complexity of practical AML scenarios.

</details>


### [341] [Density-aware Walks for Coordinated Campaign Detection](https://arxiv.org/abs/2506.13912)
*Atul Anand Gopalakrishnan,Jakir Hossain,Tuğrulcan Elmas,Ahmet Erdem Sarıyüce*

Main category: cs.SI

TL;DR: 论文提出了一种基于图分类的方法，通过局部网络结构密度检测社交媒体上的协调虚假行为，使用随机加权游走和密度感知嵌入，显著提高了分类准确率。


<details>
  <summary>Details</summary>
Motivation: 社交媒体上的协调虚假行为难以区分，现有图神经网络在大型网络上表现不佳，需要更有效的方法来检测此类行为。

Method: 提出随机加权游走（RWW）方法，结合局部密度度量（如度数、核心数等）生成密度感知嵌入，并训练消息传递神经网络（MPNN）进行分类。

Result: 在二元和多类分类任务中，准确率分别提高了12%和5%，优于传统方法。

Conclusion: 结合密度感知嵌入和MPNN的方法为检测社交媒体上的协调虚假行为提供了有效框架。

Abstract: Coordinated campaigns frequently exploit social media platforms by artificially amplifying topics, making inauthentic trends appear organic, and misleading users into engagement. Distinguishing these coordinated efforts from genuine public discourse remains a significant challenge due to the sophisticated nature of such attacks. Our work focuses on detecting coordinated campaigns by modeling the problem as a graph classification task. We leverage the recently introduced Large Engagement Networks (LEN) dataset, which contains over 300 networks capturing engagement patterns from both fake and authentic trends on Twitter prior to the 2023 Turkish elections. The graphs in LEN were constructed by collecting interactions related to campaigns that stemmed from ephemeral astroturfing. Established graph neural networks (GNNs) struggle to accurately classify campaign graphs, highlighting the challenges posed by LEN due to the large size of its networks. To address this, we introduce a new graph classification method that leverages the density of local network structures. We propose a random weighted walk (RWW) approach in which node transitions are biased by local density measures such as degree, core number, or truss number. These RWWs are encoded using the Skip-gram model, producing density-aware structural embeddings for the nodes. Training message-passing neural networks (MPNNs) on these density-aware embeddings yields superior results compared to the simpler node features available in the dataset, with nearly a 12\% and 5\% improvement in accuracy for binary and multiclass classification, respectively. Our findings demonstrate that incorporating density-aware structural encoding with MPNNs provides a robust framework for identifying coordinated inauthentic behavior on social media networks such as Twitter.

</details>


<div id='q-bio.QM'></div>

# q-bio.QM [[Back]](#toc)

### [342] [A Silent Speech Decoding System from EEG and EMG with Heterogenous Electrode Configurations](https://arxiv.org/abs/2506.13835)
*Masakazu Inoue,Motoshige Sato,Kenichi Tomeoka,Nathania Nah,Eri Hatakeyama,Kai Arulkumaran,Ilya Horiguchi,Shuntaro Sasai*

Main category: q-bio.QM

TL;DR: 该研究提出了一种神经网络方法，用于处理异质电极放置的EEG/EMG数据，通过多任务训练提高无声语音解码的准确性，并在健康参与者和语言障碍患者中取得了显著效果。


<details>
  <summary>Details</summary>
Motivation: 无声语音解码（通过EEG/EMG识别未发声的语音）对语言障碍患者具有重要意义，但数据收集困难且实验设置多样，难以构建大规模同质数据集。

Method: 引入神经网络模型，能够处理异质电极放置的EEG/EMG数据，并通过多任务训练在大规模数据集上进行优化。

Result: 在健康参与者中达到95.3%的单词分类准确率，在语言障碍患者中达到54.5%，显著优于单受试者数据训练的模型（70.1%和13.2%）。跨语言校准性能也有所提升。

Conclusion: 研究结果表明，开发实用的无声语音解码系统是可行的，尤其对语言障碍患者具有重要应用前景。

Abstract: Silent speech decoding, which performs unvocalized human speech recognition from electroencephalography/electromyography (EEG/EMG), increases accessibility for speech-impaired humans. However, data collection is difficult and performed using varying experimental setups, making it nontrivial to collect a large, homogeneous dataset. In this study we introduce neural networks that can handle EEG/EMG with heterogeneous electrode placements and show strong performance in silent speech decoding via multi-task training on large-scale EEG/EMG datasets. We achieve improved word classification accuracy in both healthy participants (95.3%), and a speech-impaired patient (54.5%), substantially outperforming models trained on single-subject data (70.1% and 13.2%). Moreover, our models also show gains in cross-language calibration performance. This increase in accuracy suggests the feasibility of developing practical silent speech decoding systems, particularly for speech-impaired patients.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [343] [Enhancing Clinical Decision Support and EHR Insights through LLMs and the Model Context Protocol: An Open-Source MCP-FHIR Framework](https://arxiv.org/abs/2506.13800)
*Abul Ehtesham,Aditi Singh,Saket Kumar*

Main category: cs.SE

TL;DR: 本文提出了一种基于代理的开源框架，通过MCP协议将大型语言模型与HL7 FHIR数据集成，用于动态提取和推理电子健康记录，以提升临床决策支持、减轻文档负担并改善患者健康素养。


<details>
  <summary>Details</summary>
Motivation: 解决数字健康领域中临床决策支持、文档负担和患者健康素养的持续挑战。

Method: 采用基于代理的框架，结合LLMs和HL7 FHIR数据，通过MCP协议实现动态数据提取和推理，支持实时总结、解释和个性化沟通。

Result: 框架在SMART Health IT沙箱中使用合成EHR数据进行评估，展示了可扩展、可解释和互操作的AI驱动EHR应用。

Conclusion: 该框架为个性化数字健康解决方案提供了强大的基础，支持多种FHIR格式，优于传统的静态工作流方法。

Abstract: Enhancing clinical decision support (CDS), reducing documentation burdens, and improving patient health literacy remain persistent challenges in digital health. This paper presents an open-source, agent-based framework that integrates Large Language Models (LLMs) with HL7 FHIR data via the Model Context Protocol (MCP) for dynamic extraction and reasoning over electronic health records (EHRs). Built on the established MCP-FHIR implementation, the framework enables declarative access to diverse FHIR resources through JSON-based configurations, supporting real-time summarization, interpretation, and personalized communication across multiple user personas, including clinicians, caregivers, and patients. To ensure privacy and reproducibility, the framework is evaluated using synthetic EHR data from the SMART Health IT sandbox (https://r4.smarthealthit.org/), which conforms to the FHIR R4 standard. Unlike traditional approaches that rely on hardcoded retrieval and static workflows, the proposed method delivers scalable, explainable, and interoperable AI-powered EHR applications. The agentic architecture further supports multiple FHIR formats, laying a robust foundation for advancing personalized digital health solutions.

</details>


### [344] [Instruction and Solution Probabilities as Heuristics for Inductive Programming](https://arxiv.org/abs/2506.13804)
*Edward McDaid,Sarah McDaid*

Main category: cs.SE

TL;DR: 论文提出通过引入指令概率和解决方案概率作为启发式方法，进一步缩小归纳编程（IP）搜索空间，结合指令子集（IS）可将搜索空间缩小超过100个数量级。


<details>
  <summary>Details</summary>
Motivation: 为了进一步优化归纳编程的搜索效率，减少搜索空间的规模。

Method: 引入指令概率和解决方案概率作为启发式方法，基于代码样本中指令的出现频率设定概率阈值，用于剪枝搜索空间。

Result: 新方法将IP搜索空间缩小了数十个数量级，结合IS时甚至可超过100个数量级。交叉验证表明该方法对未见代码也有效。

Conclusion: 该方法显著提升了IP的效率，未来可进一步优化概率模型。

Abstract: Instruction subsets (ISs) are heuristics that can shrink the size of the inductive programming (IP) search space by tens of orders of magnitude. Here, we extend the IS approach by introducing instruction and solution probabilities as additional heuristics. Instruction probability reflects the expectation of an instruction occurring in a solution, based on the frequency of instruction occurrence in a large code sample. The solution probability for a partial or complete program is simply the product of all constituent instruction probabilities, including duplicates. We treat the minimum solution probabilities observed in code sample program units of different sizes as solution probability thresholds. These thresholds are used to prune the search space as partial solutions are constructed, thereby eliminating any branches containing unlikely combinations of instructions. The new approach has been evaluated using a large sample of human code. We tested two formulations of instruction probability: one based on instruction occurrence across the entire code sample and another that measured the distribution separately for each IS. Our results show that both variants produce substantial further reductions in the IP search space size of up to tens of orders of magnitude, depending on solution size. In combination with IS, reductions of over 100 orders of magnitude can be achieved. We also carried out cross-validation testing to show that the heuristics should work effectively with unseen code. The approach is described and the results and some ideas for future work are discussed.

</details>


### [345] [Structured Program Synthesis using LLMs: Results and Insights from the IPARC Challenge](https://arxiv.org/abs/2506.13820)
*Shraddha Surana,Ashwin Srinivasan,Michael Bain*

Main category: cs.SE

TL;DR: IPARC Challenge通过合成图像任务评估程序合成能力，本文提出基于LLM的结构化归纳编程方法，成功解决所有任务类别，揭示了LLM代码生成的关键机制。


<details>
  <summary>Details</summary>
Motivation: IPARC Challenge的600项任务难以自动化解决，研究旨在探索LLM在程序合成中的潜力及其与人类协作的价值。

Method: 采用结构化归纳编程方法，结合LLM生成代码，并通过人类细化优化结果。

Result: 成功解决所有IPARC任务类别，揭示了LLM代码生成的关键机制，如结构化先验、代码冻结和重用效率。

Conclusion: LLM与人类协作在复杂程序合成中具有重要价值，未来可进一步优化协作机制。

Abstract: The IPARC Challenge, inspired by ARC, provides controlled program synthesis tasks over synthetic images to evaluate automatic program construction, focusing on sequence, selection, and iteration. This set of 600 tasks has resisted automated solutions. This paper presents a structured inductive programming approach with LLMs that successfully solves tasks across all IPARC categories. The controlled nature of IPARC reveals insights into LLM-based code generation, including the importance of prior structuring, LLMs' ability to aid structuring (requiring human refinement), the need to freeze correct code, the efficiency of code reuse, and how LLM-generated code can spark human creativity. These findings suggest valuable mechanisms for human-LLM collaboration in tackling complex program synthesis.

</details>


### [346] [MLDebugging: Towards Benchmarking Code Debugging Across Multi-Library Scenarios](https://arxiv.org/abs/2506.13824)
*Jinyang Huang,Xiachong Feng,Qiguang Chen,Hanjie Zhao,Zihui Cheng,Jiesong Bai,Jingxuan Zhou,Min Li,Libo Qin*

Main category: cs.SE

TL;DR: 论文提出了MLDebugging，首个针对多库Python代码调试的基准测试，评估了主流LLMs在多库场景中的表现。


<details>
  <summary>Details</summary>
Motivation: 当前研究多关注简单无库或单库场景，忽视了实际应用中复杂的多库调试需求。

Method: 构建包含126个Python库的MLDebugging基准，涵盖七类多库代码问题，并评估主流LLMs的表现。

Result: 现有LLMs在多库调试场景中表现不佳。

Conclusion: MLDebugging揭示了LLMs在多库调试中的潜力，为未来研究提供了方向。

Abstract: Code debugging is a crucial task in software engineering, which attracts increasing attention. While remarkable success has been made in the era of large language models (LLMs), current research still focuses on the simple no-library or single-library setting, ignoring the complex multi-library scenario in real-world applications. To address this limitation, we make the first attempt to introduce MLDebugging (Multi-Library Debugging), a comprehensive benchmark designed to assess debugging challenges within multi-library Python code. Specifically, MLDebugging encompasses 126 distinct Python libraries, covering a wide range of multi-library code issues, categorized into seven distinct types. Furthermore, we conduct a thorough evaluation of MLDebugging using both mainstream open-source and closed-source LLMs and highlight that current LLMs still struggle to correctly perform code debugging across multi-library scenarios. We hope this work can uncover the potential of LLMs in multi-library debugging scenario and offer insights for future research.

</details>


### [347] [CRITICTOOL: Evaluating Self-Critique Capabilities of Large Language Models in Tool-Calling Error Scenarios](https://arxiv.org/abs/2506.13977)
*Shiting Huang,Zhen Fang,Zehui Chen,Siyu Yuan,Junjie Ye,Yu Zeng,Lin Chen,Qi Mao,Feng Zhao*

Main category: cs.SE

TL;DR: 论文提出CRITICTOOL，一个专门用于工具学习的批评评估基准，通过进化策略构建数据集，涵盖多样化的工具使用错误，并验证其泛化性和有效性。


<details>
  <summary>Details</summary>
Motivation: 随着任务复杂性和长期性增加，大型语言模型（LLM）在工具使用过程中可能触发意外错误，如何有效处理这些错误成为关键研究方向。

Method: 通过分析工具调用过程中的错误类型，构建CRITICTOOL基准，采用进化策略生成多样化错误数据集，并进行广泛实验验证。

Result: CRITICTOOL能更好地反映真实场景，验证了基准策略的泛化性和有效性，同时分析了不同LLM的工具反思能力。

Conclusion: CRITICTOOL为工具学习领域提供了新视角，其构建策略和实验结果对LLM的工具学习研究有重要贡献。

Abstract: The ability of large language models (LLMs) to utilize external tools has enabled them to tackle an increasingly diverse range of tasks. However, as the tasks become more complex and long-horizon, the intricate tool utilization process may trigger various unexpected errors. Therefore, how to effectively handle such errors, including identifying, diagnosing, and recovering from them, has emerged as a key research direction for advancing tool learning. In this work, we first extensively analyze the types of errors encountered during the function-calling process on several competitive tool evaluation benchmarks. Based on it, we introduce CRITICTOOL, a comprehensive critique evaluation benchmark specialized for tool learning. Building upon a novel evolutionary strategy for dataset construction, CRITICTOOL holds diverse tool-use errors with varying complexities, which better reflects real-world scenarios. We conduct extensive experiments on CRITICTOOL, and validate the generalization and effectiveness of our constructed benchmark strategy. We also provide an in-depth analysis of the tool reflection ability on various LLMs, offering a new perspective on the field of tool learning in LLMs. The code is available at \href{https://github.com/Shellorley0513/CriticTool}{https://github.com/Shellorley0513/CriticTool}.

</details>


### [348] [FrontendBench: A Benchmark for Evaluating LLMs on Front-End Development via Automatic Evaluation](https://arxiv.org/abs/2506.13832)
*Hongda Zhu,Yiwen Zhang,Bing Zhao,Jingzhe Ding,Siyao Liu,Tong Liu,Dandan Wang,Yanan Liu,Zhaojian Li*

Main category: cs.SE

TL;DR: FrontendBench是一个由人类和LLMs共同开发的基准测试，用于更全面和实际地评估前端代码生成能力，解决了现有基准测试的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试存在任务过于简单、测试用例不严谨和缺乏端到端验证等问题，无法准确评估模型性能。

Method: 开发了FrontendBench，包含148个精心设计的提示-测试用例对，涵盖五个级别的Web组件，并引入自动评估框架在沙盒环境中执行代码。

Result: 自动评估框架与专家评估的一致性达到90.54%，基准测试揭示了不同LLMs在真实前端任务中的显著性能差异。

Conclusion: FrontendBench是一个可靠且可扩展的基准测试，为前端代码生成的未来研究提供了坚实基础。

Abstract: Large Language Models (LLMs) have made significant strides in front-end code generation. However, existing benchmarks exhibit several critical limitations: many tasks are overly simplistic, test cases often lack rigor, and end-to-end validation is absent. These issues hinder the accurate assessment of model performance. To address these challenges, we present FrontendBench, a benchmark co-developed by humans and LLMs. FrontendBench categorizes tasks based on code functionality and incorporates interactive test scenarios, enabling a more comprehensive and practical evaluation of front-end code generation capabilities. The benchmark comprises 148 meticulously crafted prompt-test case pairs spanning five levels of web components, from basic UI elements to complex interactive features. Each task reflects realistic front-end development challenges. Furthermore, we introduce an automatic evaluation framework that executes generated code within a sandbox environment and assesses outcomes using predefined test scripts. This framework achieves a 90.54% agreement rate with expert human evaluations, demonstrating high reliability. We benchmark several state-of-the-art LLMs on FrontendBench and observe substantial performance disparities in handling real-world front-end tasks. These results highlight FrontendBench as a reliable and scalable benchmark, supporting consistent multimodal evaluation and providing a robust foundation for future research in front-end code generation. Our data and code will be released soon.

</details>


### [349] [How Does LLM Reasoning Work for Code? A Survey and a Call to Action](https://arxiv.org/abs/2506.13932)
*Ira Ceka,Saurabh Pujar,Irene Manotas,Gail Kaiser,Baishakhi Ray,Shyam Ramji*

Main category: cs.SE

TL;DR: 本文研究了大型语言模型（LLMs）在代码推理任务中的应用，总结了相关策略、技术分类、性能评估及未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 探索LLMs在代码任务中的实际应用潜力，特别是在软件工程（SWE）任务如GitHub问题解决中的表现。

Method: 通过调查和分类代码推理技术，分析性能基准，并探讨代码核心属性对推理技术的影响。

Result: 提出了代码推理的全面分类、性能评估及新基准的潜力，并识别了未来研究的空白领域。

Conclusion: 代码推理技术具有广泛应用潜力，但仍需进一步探索未充分研究的领域。

Abstract: The rise of large language models (LLMs) has led to dramatic improvements across a wide range of natural language tasks. These advancements have extended into the domain of code, facilitating complex tasks such as code generation, translation, summarization, and repair. However, their utility for real-world deployment in-the-wild has only recently been studied, particularly on software engineering (SWE) tasks such as GitHub issue resolution. In this study, we examine the code reasoning techniques that underlie the ability to perform such tasks, and examine the paradigms used to drive their performance. Our contributions in this paper are: (1) the first dedicated survey on code reasoning for code tasks, highlighting overarching strategies, hybrid and agentic approaches; (2) a taxonomy of various techniques used to drive code reasoning; (3) a comprehensive overview of performance on common benchmarks and a showcase of new, under-explored benchmarks with high potential in SWE; (4) an exploration on how core properties of code can be used to explain different reasoning techniques; and (5) gaps and potentially under-explored areas for future research.

</details>


### [350] [Automatic Qiskit Code Refactoring Using Large Language Models](https://arxiv.org/abs/2506.14535)
*José Manuel Suárez,Luis Mariano Bibbó,Joaquin Bogado,Alejandro Fernandez*

Main category: cs.SE

TL;DR: 提出了一种利用大语言模型（LLM）重构Qiskit代码的新方法，通过提取迁移场景分类法并结合LLM，有效自动化代码迁移。


<details>
  <summary>Details</summary>
Motivation: 随着量子软件框架的快速更新，开发者面临API兼容性问题，需要高效工具辅助代码迁移。

Method: 从Qiskit文档中提取迁移场景分类法，结合LLM识别代码中的迁移模式并提出重构建议。

Result: LLM在领域知识的指导下能有效自动化Qiskit代码迁移，并提供了迁移分类法和评估方法。

Conclusion: 该方法为量子代码迁移提供了实用工具，并验证了LLM在此领域的潜力。

Abstract: As quantum software frameworks evolve, developers face increasing challenges in maintaining compatibility with rapidly changing APIs. In this work, we present a novel methodology for refactoring Qiskit code using large language models (LLMs). We begin by extracting a taxonomy of migration scenarios from the different sources of official Qiskit documentation (such as release notes), capturing common patterns such as migration of functionality to different modules and deprecated usage. This taxonomy, along with the original Python source code, is provided as input to an LLM, which is then tasked with identifying instances of migration scenarios in the code and suggesting appropriate refactoring solutions. Our approach is designed to address the context length limitations of current LLMs by structuring the input and reasoning process in a targeted, efficient manner. The results demonstrate that LLMs, when guided by domain-specific migration knowledge, can effectively assist in automating Qiskit code migration. This work contributes both a set of proven prompts and taxonomy for Qiskit code migration from earlier versions to version 0.46 and a methodology to asses the capabilities of LLMs to assist in the migration of quantum code.

</details>


### [351] [Low-code to fight climate change: the Climaborough project](https://arxiv.org/abs/2506.14623)
*Aaron Conrardy,Armen Sulejmani,Cindy Guerlain,Daniele Pagani,David Hick,Matteo Satta,Jordi Cabot*

Main category: cs.SE

TL;DR: Climaborough项目通过低代码/无代码策略快速开发气候仪表盘，帮助欧洲城市实现2030年碳中和目标。


<details>
  <summary>Details</summary>
Motivation: 支持欧洲城市通过实时数据监测和用户友好仪表盘实现碳中和目标。

Method: 采用低代码策略加速仪表盘开发，嵌入无代码哲学以适配不同用户需求。

Result: 开发了可配置的仪表盘，用于评估本地气候倡议效果并识别高影响力项目。

Conclusion: 低代码/无代码策略有效支持了气候目标的快速实现和用户适应性。

Abstract: The EU-funded Climaborough project supports European cities to achieve carbon neutrality by 2030. Eleven cities in nine countries will deploy in real conditions products and services fostering climate transition in their local environment. The Climaborough City Platform is being developed to monitor the cities' overall progress towards their climate goals by aggregating historic and real-time data and displaying the results in user-friendly dashboards that will be used by non-technical experts to evaluate the effectiveness of local experimental initiatives, identify those that yield significant impact, and assess the potential consequences of scaling them up to a broader level. In this paper, we explain how we have put in place a low-code/no-code strategy in Climaborough in response to the project's aim to quickly deploy climate dashboards. A low-code strategy is used to accelerate the development of the dashboards. The dashboards embed a no-code philosophy that enables all types of citizen profiles to configure and adapt the dashboard to their specific needs.

</details>


### [352] [ACM Survey Draft on Formalising Software Requirements with Large Language Models](https://arxiv.org/abs/2506.14627)
*Arshad Beg,Diarmuid O'Donoghue,Rosemary Monahan*

Main category: cs.SE

TL;DR: 本文档总结了94篇论文，并补充了关于软件需求可追溯性、形式化方法及其工具、统一编程理论和机构理论的内容。与类似标题的近期论文[7]和[8]的主要区别在于篇幅和审查流程。


<details>
  <summary>Details</summary>
Motivation: 总结多篇论文并提供额外内容，以支持相关领域的研究。

Method: 通过文献综述和分类整理94篇论文，并补充特定主题的详细内容。

Result: 生成了一份包含多个主题的综合性文档，与[7]和[8]相比更具全面性。

Conclusion: 本文档为相关研究提供了更全面的参考，同时突出了与[7]和[8]的区别。

Abstract: This draft is a working document, having a summary of nighty-four (94) papers with additional sections on Traceability of Software Requirements (Section 4), Formal Methods and Its Tools (Section 5), Unifying Theories of Programming (UTP) and Theory of Institutions (Section 6). Please refer to abstract of [7,8]. Key difference of this draft from our recently anticipated ones with similar titles, i.e. AACS 2025 [7] and SAIV 2025 [8] is:
  [7] is a two page submission to ADAPT Annual Conference, Ireland. Submitted on 18th of March, 2025, it went through the light-weight blind review and accepted for poster presentation. Conference was held on 15th of May, 2025.
  [8] is a nine page paper with additional nine pages of references and summary tables, submitted to Symposium on AI Verification (SAIV 2025) on 24th of April, 2025. It went through rigorous review process. The uploaded version on arXiv.org [8] is the improved one of the submission, after addressing the specific suggestions to improve the paper.

</details>


### [353] [Navigating the growing field of research on AI for software testing -- the taxonomy for AI-augmented software testing and an ontology-driven literature survey](https://arxiv.org/abs/2506.14640)
*Ina K. Schieferdecker*

Main category: cs.SE

TL;DR: 论文综述了AI在软件测试自动化中的增强作用，提出了新的分类法ai4st，并探讨了AI带来的新型测试形式。


<details>
  <summary>Details</summary>
Motivation: 随着AI在工程领域的突破，为软件测试（包括手动和自动化测试）提供了新的视角，但测试自动化的设计、开发和维护仍是一项艰巨任务。

Method: 回顾了AI在软件测试自动化中的最新研究，从无自动化到全自动化，并提出了新的分类法ai4st。

Result: 通过ai4st分类法对近期研究进行了分类，并识别了开放的研究问题。

Conclusion: AI为软件测试自动化带来了新的可能性，但仍需进一步研究以解决开放问题。

Abstract: In industry, software testing is the primary method to verify and validate the functionality, performance, security, usability, and so on, of software-based systems. Test automation has gained increasing attention in industry over the last decade, following decades of intense research into test automation and model-based testing. However, designing, developing, maintaining and evolving test automation is a considerable effort. Meanwhile, AI's breakthroughs in many engineering fields are opening up new perspectives for software testing, for both manual and automated testing. This paper reviews recent research on AI augmentation in software test automation, from no automation to full automation. It also discusses new forms of testing made possible by AI. Based on this, the newly developed taxonomy, ai4st, is presented and used to classify recent research and identify open research questions.

</details>


### [354] [Unified Software Engineering agent as AI Software Engineer](https://arxiv.org/abs/2506.14683)
*Leonhard Applis,Yuntong Zhang,Shanchao Liang,Nan Jiang,Lin Tan,Abhik Roychoudhury*

Main category: cs.SE

TL;DR: 论文探讨了LLM代理是否能成为AI软件工程师，提出了统一软件工程代理USEagent，并在多任务测试中验证其效果。


<details>
  <summary>Details</summary>
Motivation: 研究LLM代理是否能胜任软件工程中的复杂任务，如维护和项目演进，而不仅仅是编码。

Method: 开发了USEagent，一个统一代理，能够协调处理多种软件工程任务，并构建了USEbench进行评测。

Result: USEagent在1,271个任务中表现优于现有通用代理，但在某些编码任务上仍有不足。

Conclusion: USEagent是未来AI软件工程师的雏形，但仍需进一步改进以填补能力差距。

Abstract: The growth of Large Language Model (LLM) technology has raised expectations for automated coding. However, software engineering is more than coding and is concerned with activities including maintenance and evolution of a project. In this context, the concept of LLM agents has gained traction, which utilize LLMs as reasoning engines to invoke external tools autonomously. But is an LLM agent the same as an AI software engineer? In this paper, we seek to understand this question by developing a Unified Software Engineering agent or USEagent. Unlike existing work which builds specialized agents for specific software tasks such as testing, debugging, and repair, our goal is to build a unified agent which can orchestrate and handle multiple capabilities. This gives the agent the promise of handling complex scenarios in software development such as fixing an incomplete patch, adding new features, or taking over code written by others. We envision USEagent as the first draft of a future AI Software Engineer which can be a team member in future software development teams involving both AI and humans. To evaluate the efficacy of USEagent, we build a Unified Software Engineering bench (USEbench) comprising of myriad tasks such as coding, testing, and patching. USEbench is a judicious mixture of tasks from existing benchmarks such as SWE-bench, SWT-bench, and REPOCOD. In an evaluation on USEbench consisting of 1,271 repository-level software engineering tasks, USEagent shows improved efficacy compared to existing general agents such as OpenHands CodeActAgent. There exist gaps in the capabilities of USEagent for certain coding tasks, which provides hints on further developing the AI Software Engineer of the future.

</details>


<div id='q-bio.PE'></div>

# q-bio.PE [[Back]](#toc)

### [355] [Analysis and Optimization of Probabilities of Beneficial Mutation and Crossover Recombination in a Hamming Space](https://arxiv.org/abs/2506.13809)
*Roman V. Belavkin*

Main category: q-bio.PE

TL;DR: 论文分析了在一般汉明空间中突变和交叉重组的概率，推导了优化参数的条件，并比较了突变和重组的特性。


<details>
  <summary>Details</summary>
Motivation: 受Fisher几何方法的启发，研究有益突变和重组在汉明空间中的概率，以优化进化算法的参数。

Method: 使用几何和组合分析推导封闭表达式，描述多代距离的马尔可夫演化，并优化突变和重组的半径。

Result: 突变和重组在接近最优解时表现不同：突变概率降低，而重组可能加速进化。

Conclusion: 重组可以补充突变，在接近最优解时提升进化速率。

Abstract: Inspired by Fisher's geometric approach to study beneficial mutations, we analyse probabilities of beneficial mutation and crossover recombination of strings in a general Hamming space with arbitrary finite alphabet. Mutations and recombinations that reduce the distance to an optimum are considered as beneficial. Geometric and combinatorial analysis is used to derive closed-form expressions for transition probabilities between spheres around an optimum giving a complete description of Markov evolution of distances from an optimum over multiple generations. This paves the way for optimization of parameters of mutation and recombination operators. Here we derive optimality conditions for mutation and recombination radii maximizing the probabilities of mutation and crossover into the optimum. The analysis highlights important differences between these evolutionary operators. While mutation can potentially reach any part of the search space, the probability of beneficial mutation decreases with distance to an optimum, and the optimal mutation radius or rate should also decrease resulting in a slow-down of evolution near the optimum. Crossover recombination, on the other hand, acts in a subspace of the search space defined by the current population of strings. However, probabilities of beneficial and deleterious crossover are balanced, and their characteristics, such as variance, are translation invariant in a Hamming space, suggesting that recombination may complement mutation and boost the rate of evolution near the optimum.

</details>


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [356] [The Perception of Phase Intercept Distortion and its Application in Data Augmentation](https://arxiv.org/abs/2506.14571)
*Venkatakrishnan Vaidyanathapuram Krishnan,Nathaniel Condit-Schultz*

Main category: eess.SP

TL;DR: 本文研究了相位截断失真（一种频率无关的相位偏移导致的相位失真），假设其虽显著改变信号波形但不可感知，并通过实验验证了假设。此外，探讨了其在机器学习数据增强中的应用，实验表明该方法能提升音频机器学习任务的效果。


<details>
  <summary>Details</summary>
Motivation: 研究相位截断失真的可感知性及其在机器学习中的应用潜力。

Method: 通过人类受试者实验验证相位截断失真的不可感知性，并将其作为数据增强方法应用于音频机器学习任务。

Result: 实验证实相位截断失真不可感知，且作为数据增强方法能显著提升机器学习任务性能。

Conclusion: 相位截断失真虽改变波形但不可感知，可作为有效的数据增强工具用于音频机器学习。

Abstract: Phase distortion refers to the alteration of the phase relationships between frequencies in a signal, which can be perceptible. In this paper, we discuss a special case of phase distortion known as phase-intercept distortion, which is created by a frequency-independent phase shift. We hypothesize that, though this form of distortion changes a signal's waveform significantly, the distortion is imperceptible. Human-subject experiment results are reported which are consistent with this hypothesis. Furthermore, we discuss how the imperceptibility of phase-intercept distortion can be useful for machine learning, specifically for data augmentation. We conducted multiple experiments using phase-intercept distortion as a novel approach to data augmentation, and obtained improved results for audio machine learning tasks.

</details>


<div id='eess.SY'></div>

# eess.SY [[Back]](#toc)

### [357] [Safe Domains of Attraction for Discrete-Time Nonlinear Systems: Characterization and Verifiable Neural Network Estimation](https://arxiv.org/abs/2506.13961)
*Mohamed Serry,Haoyu Li,Ruikun Zhou,Huan Zhang,Jun Liu*

Main category: eess.SY

TL;DR: 提出了一种基于Zubov方程和神经网络的框架，用于准确估计离散时间非线性自治系统的安全吸引域。


<details>
  <summary>Details</summary>
Motivation: 非线性自治系统的吸引域估计是一个长期存在的挑战，现有方法保守或仅适用于低维系统，且难以处理状态约束。

Method: 推导新的Zubov方程，其解对应精确安全吸引域；采用物理信息神经网络近似求解Zubov方程；提出验证框架以确保估计的可靠性。

Result: Zubov方程的解在状态空间内唯一且连续；通过数值实验验证了方法的有效性。

Conclusion: 该框架为非线性系统的安全吸引域估计提供了准确且可验证的解决方案。

Abstract: Analysis of nonlinear autonomous systems typically involves estimating domains of attraction, which have been a topic of extensive research interest for decades. Despite that, accurately estimating domains of attraction for nonlinear systems remains a challenging task, where existing methods are conservative or limited to low-dimensional systems. The estimation becomes even more challenging when accounting for state constraints. In this work, we propose a framework to accurately estimate safe (state-constrained) domains of attraction for discrete-time autonomous nonlinear systems. In establishing this framework, we first derive a new Zubov equation, whose solution corresponds to the exact safe domain of attraction. The solution to the aforementioned Zubov equation is shown to be unique and continuous over the whole state space. We then present a physics-informed approach to approximating the solution of the Zubov equation using neural networks. To obtain certifiable estimates of the domain of attraction from the neural network approximate solutions, we propose a verification framework that can be implemented using standard verification tools (e.g., $α,\!β$-CROWN and dReal). To illustrate its effectiveness, we demonstrate our approach through numerical examples concerning nonlinear systems with state constraints.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [358] [Connecting phases of matter to the flatness of the loss landscape in analog variational quantum algorithms](https://arxiv.org/abs/2506.13865)
*Kasidit Srimahajariyapong,Supanut Thanasilp,Thiparat Chotibut*

Main category: quant-ph

TL;DR: 研究了一种基于无序伊辛链的模拟变分量子算法（VQA），通过调节无序强度将淬火置于热化相或多体局域化（MBL）相，分析了其表达能力和损失方差。MBL相在较大淬火次数时仍能避免贫瘠高原问题，提出了一种MBL初始化策略以提高可训练性。


<details>
  <summary>Details</summary>
Motivation: 解决传统数字门基VQA中因贫瘠高原问题导致的可扩展性问题，探索模拟硬件VQA的潜力。

Method: 使用无序伊辛链的淬火动力学构建VQA ansätze，通过调节无序强度研究热化相和MBL相的表达能力和损失方差。

Result: 热化相在较小淬火次数时即出现贫瘠高原，而MBL相在较大淬火次数时仍能保持可训练性。

Conclusion: MBL相为模拟VQA提供了一种避免贫瘠高原的策略，为硬件实现提供了实用指导。

Abstract: Variational quantum algorithms (VQAs) promise near-term quantum advantage, yet parametrized quantum states commonly built from the digital gate-based approach often suffer from scalability issues such as barren plateaus, where the loss landscape becomes flat. We study an analog VQA ansätze composed of $M$ quenches of a disordered Ising chain, whose dynamics is native to several quantum simulation platforms. By tuning the disorder strength we place each quench in either a thermalized phase or a many-body-localized (MBL) phase and analyse (i) the ansätze's expressivity and (ii) the scaling of loss variance. Numerics shows that both phases reach maximal expressivity at large $M$, but barren plateaus emerge at far smaller $M$ in the thermalized phase than in the MBL phase. Exploiting this gap, we propose an MBL initialisation strategy: initialise the ansätze in the MBL regime at intermediate quench $M$, enabling an initial trainability while retaining sufficient expressivity for subsequent optimization. The results link quantum phases of matter and VQA trainability, and provide practical guidelines for scaling analog-hardware VQAs.

</details>


### [359] [Hamiltonian Formalism for Comparing Quantum and Classical Intelligence](https://arxiv.org/abs/2506.14456)
*Elija Perrier*

Main category: quant-ph

TL;DR: 论文提出了一种哈密顿形式主义，用于比较经典和量子AGI在环境交互中的差异。


<details>
  <summary>Details</summary>
Motivation: 研究量子基板上实现的AGI，需要数学框架来对比其在经典和量子环境中的操作。

Method: 引入哈密顿形式主义，将AGI动态分解为核心功能的哈密顿生成器（如归纳、推理、递归等）。

Result: 提供了一种精确的数学语言，用于描述量子与经典AGI通过环境交互的差异。

Conclusion: 该形式主义有助于理解量子与经典AGI在环境交互中的根本区别。

Abstract: The prospect of AGI instantiated on quantum substrates motivates the development of mathematical frameworks that enable direct comparison of their operation in classical and quantum environments. To this end, we introduce a Hamiltonian formalism for describing classical and quantum AGI tasks as a means of contrasting their interaction with the environment. We propose a decomposition of AGI dynamics into Hamiltonian generators for core functions such as induction, reasoning, recursion, learning, measurement, and memory. This formalism aims to contribute to the development of a precise mathematical language for how quantum and classical agents differ via environmental interaction.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [360] [Agile Orchestration at Will: An Entire Smart Service-Based Security Architecture Towards 6G](https://arxiv.org/abs/2505.22963)
*Zhuoran Duan,Guoshun Nan,Rushan Li,Zijun Wang,Lihua Xiong,Chaoying Yuan,Guorong Liu,Hui Xu,Qimei Cui,Xiaofeng Tao,Tony Q. S. Quek*

Main category: cs.NI

TL;DR: 论文提出了一种名为ES3A的新型6G网络安全架构，旨在解决6G网络中异构性和多样化安全需求带来的挑战。


<details>
  <summary>Details</summary>
Motivation: 6G网络的安全性和韧性至关重要，但现有架构难以应对异构网络和多样化安全需求，因此需要重新设计安全架构。

Method: 提出了ES3A架构，基于六项高级原则（层次性、灵活性、可扩展性、韧性、内生性和信任与隐私）和三项部署指南（服务化安全、端到端保护和智能安全自动化）。架构包含三层三域，并采用两阶段编排机制。

Result: 通过基于软件定义无线电（SDR）的原型实验验证了ES3A的有效性，并通过案例展示了其优越性。

Conclusion: ES3A为6G网络提供了一种灵活、智能且全面的安全解决方案，能够应对动态环境中的安全挑战。

Abstract: The upcoming 6G will fundamentally reshape mobile networks beyond communications, unlocking a multitude of applications that were once considered unimaginable. Meanwhile, security and resilience are especially highlighted in the 6G design principles. However, safeguarding 6G networks will be quite challenging due to various known and unknown threats from highly heterogeneous networks and diversified security requirements of distinct use cases, calling for a comprehensive re-design of security architecture. This motivates us to propose ES3A (Entire Smart Service-based Security Architecture), a novel security architecture for 6G networks. Specifically, we first discuss six high-level principles of our ES3A that include hierarchy, flexibility, scalability, resilience, endogeny, and trust and privacy. With these goals in mind, we then introduce three guidelines from a deployment perspective, envisioning our ES3A that offers service-based security, end-to-end protection, and smart security automation for 6G networks. Our architecture consists of three layers and three domains. It relies on a two-stage orchestration mechanism to tailor smart security strategies for customized protection in high-dynamic 6G networks, thereby addressing the aforementioned challenges. Finally, we prototype the proposed ES3A on a real-world radio system based on Software-Defined Radio (SDR). Experiments show the effectiveness of our ES3A. We also provide a case to show the superiority of our architecture.

</details>


<div id='physics.soc-ph'></div>

# physics.soc-ph [[Back]](#toc)

### [361] [A Survey of Physics-Informed AI for Complex Urban Systems](https://arxiv.org/abs/2506.13777)
*En Xu,Huandong Wang,Yunke Zhang,Sibo Li,Yinzhou Tang,Zhilun Zhou,Yuming Lin,Yuan Yuan,Xiaochen Fan,Jingtao Ding,Yong Li*

Main category: physics.soc-ph

TL;DR: 论文综述了物理信息AI方法在城市系统中的应用，提出了三种物理-AI集成范式，并分析了八大城市领域的应用，旨在提升系统可靠性和效率。


<details>
  <summary>Details</summary>
Motivation: 城市系统是复杂系统的典型代表，物理建模与AI的结合能提高预测准确性、可解释性和决策能力。

Method: 提出分类法将现有方法分为三类：物理集成AI、物理-AI混合集成和AI集成物理，并详细介绍了七种代表性方法。

Result: 分析了八大城市领域的应用，展示了物理定律与数据驱动模型如何解决城市挑战。

Conclusion: 总结了现有方法及其应用，指出了关键差距并提出了未来研究方向，为下一代智能城市系统建模铺路。

Abstract: Urban systems are typical examples of complex systems, where the integration of physics-based modeling with artificial intelligence (AI) presents a promising paradigm for enhancing predictive accuracy, interpretability, and decision-making. In this context, AI excels at capturing complex, nonlinear relationships, while physics-based models ensure consistency with real-world laws and provide interpretable insights. We provide a comprehensive review of physics-informed AI methods in urban applications. The proposed taxonomy categorizes existing approaches into three paradigms - Physics-Integrated AI, Physics-AI Hybrid Ensemble, and AI-Integrated Physics - and further details seven representative methods. This classification clarifies the varying degrees and directions of physics-AI integration, guiding the selection and development of appropriate methods based on application needs and data availability. We systematically examine their applications across eight key urban domains: energy, environment, economy, transportation, information, public services, emergency management, and the urban system as a whole. Our analysis highlights how these methodologies leverage physical laws and data-driven models to address urban challenges, enhancing system reliability, efficiency, and adaptability. By synthesizing existing methodologies and their urban applications, we identify critical gaps and outline future research directions, paving the way toward next-generation intelligent urban system modeling.

</details>


### [362] [Infected Smallville: How Disease Threat Shapes Sociality in LLM Agents](https://arxiv.org/abs/2506.13783)
*Soyeon Choi,Kangwook Lee,Oliver Sng,Joshua M. Ackerman*

Main category: physics.soc-ph

TL;DR: 研究使用生成代理模型（GABM）发现，传染病威胁显著减少代理的社交行为，如减少社交活动、公共场所访问和对话。


<details>
  <summary>Details</summary>
Motivation: 探讨传染病威胁如何影响生成代理的社交行为，验证行为免疫系统的假设。

Method: 利用基于大型语言模型的生成代理模型（GABM），通过模拟实验比较代理在有无传染病新闻时的行为差异。

Result: 接触传染病新闻的代理社交行为显著减少，包括减少社交聚会、公共场所访问和对话，且能区分传染与非传染疾病。

Conclusion: GABM可作为研究复杂人类社交动态的有效工具，传染病威胁显著抑制社交行为。

Abstract: How does the threat of infectious disease influence sociality among generative agents? We used generative agent-based modeling (GABM), powered by large language models, to experimentally test hypotheses about the behavioral immune system. Across three simulation runs, generative agents who read news about an infectious disease outbreak showed significantly reduced social engagement compared to agents who received no such news, including lower attendance at a social gathering, fewer visits to third places (e.g., cafe, store, park), and fewer conversations throughout the town. In interview responses, agents explicitly attributed their behavioral changes to disease-avoidance motivations. A validity check further indicated that they could distinguish between infectious and noninfectious diseases, selectively reducing social engagement only when there was a risk of infection. Our findings highlight the potential of GABM as an experimental tool for exploring complex human social dynamics at scale.

</details>


<div id='eess.AS'></div>

# eess.AS [[Back]](#toc)

### [363] [Multimodal Fusion with Semi-Supervised Learning Minimizes Annotation Quantity for Modeling Videoconference Conversation Experience](https://arxiv.org/abs/2506.13971)
*Andrew Chang,Chenkai Hu,Ji Qi,Zhuojian Wei,Kexin Zhang,Viswadruth Akkaraju,David Poeppel,Dustin Freeman*

Main category: eess.AS

TL;DR: 论文提出了一种半监督学习方法，用于预测视频会议中的负面体验时刻，显著减少了标注数据的需求。


<details>
  <summary>Details</summary>
Motivation: 视频会议中的负面体验时刻（如流畅性或愉悦性降低）研究不足，且自然数据中此类时刻稀少，标注成本高。

Method: 采用半监督学习（SSL），结合多模态（音频、面部、文本）深度特征，通过模态融合协同训练预测负面时刻。

Result: SSL模型ROC-AUC达0.9，F1分数0.6，优于监督学习模型，仅需8%标注数据即可达到96%全数据监督学习性能。

Conclusion: 该方法为视频会议体验建模提供了一种高效的标注框架。

Abstract: Group conversations over videoconferencing are a complex social behavior. However, the subjective moments of negative experience, where the conversation loses fluidity or enjoyment remain understudied. These moments are infrequent in naturalistic data, and thus training a supervised learning (SL) model requires costly manual data annotation. We applied semi-supervised learning (SSL) to leverage targeted labeled and unlabeled clips for training multimodal (audio, facial, text) deep features to predict non-fluid or unenjoyable moments in holdout videoconference sessions. The modality-fused co-training SSL achieved an ROC-AUC of 0.9 and an F1 score of 0.6, outperforming SL models by up to 4% with the same amount of labeled data. Remarkably, the best SSL model with just 8% labeled data matched 96% of the SL model's full-data performance. This shows an annotation-efficient framework for modeling videoconference experience.

</details>


### [364] [M3SD: Multi-modal, Multi-scenario and Multi-language Speaker Diarization Dataset](https://arxiv.org/abs/2506.14427)
*Shilong Wu,Hang Chen,Jun Du*

Main category: eess.AS

TL;DR: 论文提出了一种自动构建说话人日志数据集的方法，并发布了多模态、多场景、多语言的M3SD数据集，同时提出了一种场景相关的模型微调策略，实现了领域适应。


<details>
  <summary>Details</summary>
Motivation: 解决说话人日志领域数据资源不足和深度学习模型泛化能力差的问题。

Method: 1. 通过音视频结合生成伪标签，构建M3SD数据集；2. 基于通用预训练模型，结合目标场景数据，使用Adapter和LoRA联合微调实现领域适应。

Result: 发布了M3SD数据集，并验证了场景相关微调策略的有效性。

Conclusion: 提出的方法和数据集有效解决了数据不足和模型泛化问题，为说话人日志领域提供了新工具。

Abstract: In the field of speaker diarization, the development of technology is constrained by two problems: insufficient data resources and poor generalization ability of deep learning models. To address these two problems, firstly, we propose an automated method for constructing speaker diarization datasets, which generates more accurate pseudo-labels for massive data through the combination of audio and video. Relying on this method, we have released Multi-modal, Multi-scenario and Multi-language Speaker Diarization (M3SD) datasets. This dataset is derived from real network videos and is highly diverse. In addition, we further propose a scenario-related model fine-tuning strategy. Based on the general model pre-trained using the above dataset, we combine the specific data of the target scenario (e.g., meetings) and achieve targeted optimization by using Adapter and LoRA joint fine-tuning, thus achieving the model's domain adaptation. Our dataset and code have been open-sourced at https://huggingface.co/spaces/OldDragon/m3sd.

</details>


### [365] [Improving Practical Aspects of End-to-End Multi-Talker Speech Recognition for Online and Offline Scenarios](https://arxiv.org/abs/2506.14204)
*Aswin Shanmugam Subramanian,Amit Das,Naoyuki Kanda,Jinyu Li,Xiaofei Wang,Yifan Gong*

Main category: eess.AS

TL;DR: 扩展了序列化输出训练（SOT）框架，以平衡流式和离线自动语音识别（ASR）的延迟与准确性，提出了CSS前端、双模型和segSOT等改进。


<details>
  <summary>Details</summary>
Motivation: 解决流式和离线ASR应用中平衡延迟与准确性的需求，满足实时字幕和摘要的要求。

Method: 1. 使用CSS单通道前端与E2E系统处理重叠语音；2. 实现双模型（流式Conformer Transducer和离线Seq2Seq）或两阶段模型；3. 探索segSOT优化离线场景和多说话者转录。

Result: CSS框架提升了ASR系统在重叠语音中的准确性；双模型和segSOT分别优化了流式和离线场景的性能。

Conclusion: 提出的方法有效平衡了ASR的延迟与准确性，适用于多种实际应用场景。

Abstract: We extend the frameworks of Serialized Output Training (SOT) to address practical needs of both streaming and offline automatic speech recognition (ASR) applications. Our approach focuses on balancing latency and accuracy, catering to real-time captioning and summarization requirements. We propose several key improvements: (1) Leveraging Continuous Speech Separation (CSS) single-channel front-end with end-to-end (E2E) systems for highly overlapping scenarios, challenging the conventional wisdom of E2E versus cascaded setups. The CSS framework improves the accuracy of the ASR system by separating overlapped speech from multiple speakers. (2) Implementing dual models -- Conformer Transducer for streaming and Sequence-to-Sequence for offline -- or alternatively, a two-pass model based on cascaded encoders. (3) Exploring segment-based SOT (segSOT) which is better suited for offline scenarios while also enhancing readability of multi-talker transcriptions.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [366] [Beyond Shapley Values: Cooperative Games for the Interpretation of Machine Learning Models](https://arxiv.org/abs/2506.13900)
*Marouane Il Idrissi,Agathe Fernandes Machado,Arthur Charpentier*

Main category: stat.ML

TL;DR: 论文探讨了合作博弈论在机器学习可解释性中的应用，提出超越Shapley值的更广泛工具，如Weber和Harsanyi集，并提供了一个三步框架以设计更可靠的特征归因方法。


<details>
  <summary>Details</summary>
Motivation: Shapley值虽广泛用于特征归因，但其公理基础与可解释性的相关性存疑，作者希望提供更灵活、理论扎实的归因方法。

Method: 回顾合作博弈论工具，介绍Weber和Harsanyi集，区分价值函数与聚合规则，提出三步框架构建特征归因。

Result: 提出了超越Shapley值的更丰富归因方法，并提供了理论框架以支持可解释性设计。

Conclusion: 呼吁XAI社区采用更灵活的理论工具，设计既有意义又适应方法演变的归因方法。

Abstract: Cooperative game theory has become a cornerstone of post-hoc interpretability in machine learning, largely through the use of Shapley values. Yet, despite their widespread adoption, Shapley-based methods often rest on axiomatic justifications whose relevance to feature attribution remains debatable. In this paper, we revisit cooperative game theory from an interpretability perspective and argue for a broader and more principled use of its tools. We highlight two general families of efficient allocations, the Weber and Harsanyi sets, that extend beyond Shapley values and offer richer interpretative flexibility. We present an accessible overview of these allocation schemes, clarify the distinction between value functions and aggregation rules, and introduce a three-step blueprint for constructing reliable and theoretically-grounded feature attributions. Our goal is to move beyond fixed axioms and provide the XAI community with a coherent framework to design attribution methods that are both meaningful and robust to shifting methodological trends.

</details>


### [367] [Mirror Descent Using the Tempesta Generalized Multi-parametric Logarithms](https://arxiv.org/abs/2506.13984)
*Andrzej Cichocki*

Main category: stat.ML

TL;DR: 本文开发了一类广泛的Mirror Descent（MD）算法，利用Bregman散度和Tempesta多参数变形对数作为链接函数，生成灵活且适应性强的MD更新。


<details>
  <summary>Details</summary>
Motivation: 旨在通过多参数对数函数扩展MD算法的灵活性，使其能够适应数据分布或几何特性，并调整超参数以实现所需性质。

Method: 利用Bregman散度和Tempesta多参数变形对数作为链接函数，估计广义指数函数以近似其逆函数，并通过学习超参数调整算法性质。

Result: 生成了一类新的广泛且灵活的MD和mirror-less MD更新，能够适应数据特性并调整超参数。

Conclusion: 多参数对数函数的应用为MD算法提供了更广泛的灵活性和适应性，扩展了其应用范围。

Abstract: In this paper, we develop a wide class Mirror Descent (MD) algorithms, which play a key role in machine learning. For this purpose we formulated the constrained optimization problem, in which we exploits the Bregman divergence with the Tempesta multi-parametric deformation logarithm as a link function. This link function called also mirror function defines the mapping between the primal and dual spaces and is associated with a very-wide (in fact, theoretically infinite) class of generalized trace-form entropies. In order to derive novel MD updates, we estimate generalized exponential function, which closely approximates the inverse of the multi-parametric Tempesta generalized logarithm. The shape and properties of the Tempesta logarithm and its inverse-deformed exponential functions can be tuned by several hyperparameters. By learning these hyperparameters, we can adapt to distribution or geometry of training data, and we can adjust them to achieve desired properties of MD algorithms. The concept of applying multi-parametric logarithms allow us to generate a new wide and flexible family of MD and mirror-less MD updates.

</details>


### [368] [Rademacher learning rates for iterated random functions](https://arxiv.org/abs/2506.13946)
*Nikola Sandrić*

Main category: stat.ML

TL;DR: 论文研究了非独立同分布（非i.i.d.）数据下的监督学习问题，提出了一种基于迭代随机函数的数据生成模型，并分析了其学习率和样本误差。


<details>
  <summary>Details</summary>
Motivation: 现实问题中数据通常具有时间依赖性和强相关性，传统i.i.d.假设不适用，因此需要研究非i.i.d.数据下的学习理论。

Method: 假设数据由迭代随机函数生成的马尔可夫链产生，在函数满足收缩性和假设类满足正则条件下，建立样本误差的一致收敛性，并分析学习率。

Result: 证明了近似经验风险最小化算法的可学习性，并推导了数据分布依赖的学习率界限。

Conclusion: 研究为非i.i.d.数据下的学习提供了理论支持，学习率能更准确地反映数据生成分布的特性。

Abstract: Most existing literature on supervised machine learning assumes that the training dataset is drawn from an i.i.d. sample. However, many real-world problems exhibit temporal dependence and strong correlations between the marginal distributions of the data-generating process, suggesting that the i.i.d. assumption is often unrealistic. In such cases, models naturally include time-series processes with mixing properties, as well as irreducible and aperiodic ergodic Markov chains. Moreover, the learning rates typically obtained in these settings are independent of the data distribution, which can lead to restrictive choices of hypothesis classes and suboptimal sample complexities for the learning algorithm. In this article, we consider the case where the training dataset is generated by an iterated random function (i.e., an iteratively defined time-homogeneous Markov chain) that is not necessarily irreducible or aperiodic. Under the assumption that the governing function is contractive with respect to its first argument and subject to certain regularity conditions on the hypothesis class, we first establish a uniform convergence result for the corresponding sample error. We then demonstrate the learnability of the approximate empirical risk minimization algorithm and derive its learning rate bound. Both rates are data-distribution dependent, expressed in terms of the Rademacher complexities of the underlying hypothesis class, allowing them to more accurately reflect the properties of the data-generating distribution.

</details>


### [369] [Meta Optimality for Demographic Parity Constrained Regression via Post-Processing](https://arxiv.org/abs/2506.13947)
*Kazuto Fukuchi*

Main category: stat.ML

TL;DR: 本文提出了一种在人口统计公平约束下的回归问题解决方案，通过元定理验证了公平极小极大最优回归算法的普适性，并展示了后处理方法实现公平回归的可行性。


<details>
  <summary>Details</summary>
Motivation: 解决现有公平回归算法局限于特定数据生成模型的问题，提供更通用的理论支持。

Method: 提出元定理，验证不同情境下公平极小极大最优回归算法的有效性，并通过后处理方法实现公平回归。

Result: 证明了公平极小极大最优回归可以通过后处理实现，为改进传统回归技术提供了便利。

Conclusion: 本文为公平回归提供了通用理论框架，并展示了后处理方法的实用性，有助于推动公平回归的实际应用。

Abstract: We address the regression problem under the constraint of demographic parity, a commonly used fairness definition. Recent studies have revealed fair minimax optimal regression algorithms, the most accurate algorithms that adhere to the fairness constraint. However, these analyses are tightly coupled with specific data generation models. In this paper, we provide meta-theorems that can be applied to various situations to validate the fair minimax optimality of the corresponding regression algorithms. Furthermore, we demonstrate that fair minimax optimal regression can be achieved through post-processing methods, allowing researchers and practitioners to focus on improving conventional regression techniques, which can then be efficiently adapted for fair regression.

</details>


### [370] [Bridging Unsupervised and Semi-Supervised Anomaly Detection: A Theoretically-Grounded and Practical Framework with Synthetic Anomalies](https://arxiv.org/abs/2506.13955)
*Matthew Lau,Tian-Yi Zhou,Xiangchi Yuan,Jizhou Chen,Wenke Lee,Xiaoming Huo*

Main category: stat.ML

TL;DR: 论文提出了一种半监督异常检测框架，结合已知和合成异常数据，理论分析显示合成异常能优化模型性能，并在实验中验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 半监督异常检测中，如何利用有限的已知异常数据提升检测性能是一个关键问题。

Method: 提出理论框架，结合已知和合成异常数据训练分类器，并首次数学建模半监督异常检测。

Result: 实验在五个基准数据集上验证了框架的有效性，性能显著提升。

Conclusion: 合成异常数据在半监督异常检测中具有普适性，能显著提升模型性能。

Abstract: Anomaly detection (AD) is a critical task across domains such as cybersecurity and healthcare. In the unsupervised setting, an effective and theoretically-grounded principle is to train classifiers to distinguish normal data from (synthetic) anomalies. We extend this principle to semi-supervised AD, where training data also include a limited labeled subset of anomalies possibly present in test time. We propose a theoretically-grounded and empirically effective framework for semi-supervised AD that combines known and synthetic anomalies during training. To analyze semi-supervised AD, we introduce the first mathematical formulation of semi-supervised AD, which generalizes unsupervised AD. Here, we show that synthetic anomalies enable (i) better anomaly modeling in low-density regions and (ii) optimal convergence guarantees for neural network classifiers -- the first theoretical result for semi-supervised AD. We empirically validate our framework on five diverse benchmarks, observing consistent performance gains. These improvements also extend beyond our theoretical framework to other classification-based AD methods, validating the generalizability of the synthetic anomaly principle in AD.

</details>


### [371] [Estimation of Treatment Effects in Extreme and Unobserved Data](https://arxiv.org/abs/2506.14051)
*Jiyuan Tan,Jose Blanchet,Vasilis Syrgkanis*

Main category: stat.ML

TL;DR: 论文提出了一种新框架，用于评估极端数据中的处理效应，以捕捉罕见事件发生时的因果效应。


<details>
  <summary>Details</summary>
Motivation: 现有因果推断方法主要针对频繁事件，而本文关注罕见但重要的事件（如极端气候事件）的处理效应。

Method: 采用多元正则变化理论建模极端事件，开发了一种一致性估计器，并进行了严格的非渐近性能分析。

Result: 通过合成和半合成数据验证了估计器的性能。

Conclusion: 该框架为极端事件中的因果效应估计提供了有效方法。

Abstract: Causal effect estimation seeks to determine the impact of an intervention from observational data. However, the existing causal inference literature primarily addresses treatment effects on frequently occurring events. But what if we are interested in estimating the effects of a policy intervention whose benefits, while potentially important, can only be observed and measured in rare yet impactful events, such as extreme climate events? The standard causal inference methodology is not designed for this type of inference since the events of interest may be scarce in the observed data and some degree of extrapolation is necessary. Extreme Value Theory (EVT) provides methodologies for analyzing statistical phenomena in such extreme regimes. We introduce a novel framework for assessing treatment effects in extreme data to capture the causal effect at the occurrence of rare events of interest. In particular, we employ the theory of multivariate regular variation to model extremities. We develop a consistent estimator for extreme treatment effects and present a rigorous non-asymptotic analysis of its performance. We illustrate the performance of our estimator using both synthetic and semi-synthetic data.

</details>


### [372] [Universal Rates of ERM for Agnostic Learning](https://arxiv.org/abs/2506.14110)
*Steve Hanneke,Mingyue Xu*

Main category: stat.ML

TL;DR: 论文研究了在不可实现（agnostic）设置下，通过经验风险最小化（ERM）进行二元分类的通用学习率问题，揭示了三种可能的通用学习率，并提供了概念类别的完整分类。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要集中在可实现（realizable）设置下的通用学习率，而不可实现设置下的研究较少。本文旨在填补这一空白，探索不可实现设置下的通用学习率。

Method: 使用经验风险最小化（ERM）方法，分析二元分类在不可实现设置下的学习曲线，研究通用学习率的可能性。

Result: 揭示了三种可能的通用学习率：$e^{-n}$、$o(n^{-1/2})$或任意缓慢。并提供了概念类别的完整分类。

Conclusion: 论文为不可实现设置下的通用学习率提供了完整的理论框架，并扩展了目标依赖和贝叶斯依赖的通用学习率研究。

Abstract: The universal learning framework has been developed to obtain guarantees on the learning rates that hold for any fixed distribution, which can be much faster than the ones uniformly hold over all the distributions. Given that the Empirical Risk Minimization (ERM) principle being fundamental in the PAC theory and ubiquitous in practical machine learning, the recent work of arXiv:2412.02810 studied the universal rates of ERM for binary classification under the realizable setting. However, the assumption of realizability is too restrictive to hold in practice. Indeed, the majority of the literature on universal learning has focused on the realizable case, leaving the non-realizable case barely explored.
  In this paper, we consider the problem of universal learning by ERM for binary classification under the agnostic setting, where the ''learning curve" reflects the decay of the excess risk as the sample size increases. We explore the possibilities of agnostic universal rates and reveal a compact trichotomy: there are three possible agnostic universal rates of ERM, being either $e^{-n}$, $o(n^{-1/2})$, or arbitrarily slow. We provide a complete characterization of which concept classes fall into each of these categories. Moreover, we also establish complete characterizations for the target-dependent universal rates as well as the Bayes-dependent universal rates.

</details>


### [373] [Adjustment for Confounding using Pre-Trained Representations](https://arxiv.org/abs/2506.14329)
*Rickmer Schulte,David Rügamer,Thomas Nagler*

Main category: stat.ML

TL;DR: 论文探讨了如何利用预训练神经网络的潜在特征来调整混杂因素，以改进平均处理效应（ATE）估计，并分析了高维和非可识别性带来的挑战。


<details>
  <summary>Details</summary>
Motivation: 非表格数据（如图像和文本）可能作为混杂因素影响ATE估计，忽略这些因素会导致结果偏差。需要利用预训练神经网络提取特征来解决这一问题。

Method: 利用预训练神经网络的潜在特征，结合双机器学习方法，调整混杂因素并实现统计推断。

Result: 研究表明，神经网络能够适应学习问题的内在稀疏性和维度，实现快速收敛速率，克服高维和非可识别性问题。

Conclusion: 尽管潜在特征学习存在挑战，但神经网络能够有效调整混杂因素，为ATE估计提供可靠方法。

Abstract: There is growing interest in extending average treatment effect (ATE) estimation to incorporate non-tabular data, such as images and text, which may act as sources of confounding. Neglecting these effects risks biased results and flawed scientific conclusions. However, incorporating non-tabular data necessitates sophisticated feature extractors, often in combination with ideas of transfer learning. In this work, we investigate how latent features from pre-trained neural networks can be leveraged to adjust for sources of confounding. We formalize conditions under which these latent features enable valid adjustment and statistical inference in ATE estimation, demonstrating results along the example of double machine learning. We discuss critical challenges inherent to latent feature learning and downstream parameter estimation arising from the high dimensionality and non-identifiability of representations. Common structural assumptions for obtaining fast convergence rates with additive or sparse linear models are shown to be unrealistic for latent features. We argue, however, that neural networks are largely insensitive to these issues. In particular, we show that neural networks can achieve fast convergence rates by adapting to intrinsic notions of sparsity and dimension of the learning problem.

</details>


### [374] [Adaptive Data Augmentation for Thompson Sampling](https://arxiv.org/abs/2506.14479)
*Wonyoung Kim*

Main category: stat.ML

TL;DR: 本文提出了一种近乎极小极大最优的Thompson Sampling方法，用于线性上下文赌博机问题，通过设计一种新颖的估计器，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 尽管Thompson Sampling在实证中表现良好，但其遗憾边界并非最优。本文旨在通过改进估计器设计，实现近乎最优的遗憾边界。

Method: 提出了一种基于自适应增强和假设样本耦合的新颖估计器，用于高效参数学习，且不依赖上下文分布的假设。

Result: 实证结果表明，该方法具有鲁棒性能，并显著优于现有方法。

Conclusion: 该方法在理论和实证上均表现出色，为线性上下文赌博机问题提供了更优的解决方案。

Abstract: In linear contextual bandits, the objective is to select actions that maximize cumulative rewards, modeled as a linear function with unknown parameters. Although Thompson Sampling performs well empirically, it does not achieve optimal regret bounds. This paper proposes a nearly minimax optimal Thompson Sampling for linear contextual bandits by developing a novel estimator with the adaptive augmentation and coupling of the hypothetical samples that are designed for efficient parameter learning. The proposed estimator accurately predicts rewards for all arms without relying on assumptions for the context distribution. Empirical results show robust performance and significant improvement over existing methods.

</details>


### [375] [Sharp Generalization Bounds for Foundation Models with Asymmetric Randomized Low-Rank Adapters](https://arxiv.org/abs/2506.14530)
*Anastasis Kratsios,Tin Sum Cheng,Aurelien Lucchi,Haitz Sáez de Ocáriz Borde*

Main category: stat.ML

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Low-Rank Adaptation (LoRA) has emerged as a widely adopted parameter-efficient fine-tuning (PEFT) technique for foundation models. Recent work has highlighted an inherent asymmetry in the initialization of LoRA's low-rank factors, which has been present since its inception and was presumably derived experimentally. This paper focuses on providing a comprehensive theoretical characterization of asymmetric LoRA with frozen random factors. First, while existing research provides upper-bound generalization guarantees based on averages over multiple experiments, the behaviour of a single fine-tuning run with specific random factors remains an open question. We address this by investigating the concentration of the typical LoRA generalization gap around its mean. Our main upper bound reveals a sample complexity of $\tilde{\mathcal{O}}\left(\frac{\sqrt{r}}{\sqrt{N}}\right)$ with high probability for rank $r$ LoRAs trained on $N$ samples. Additionally, we also determine the fundamental limits in terms of sample efficiency, establishing a matching lower bound of $\mathcal{O}\left(\frac{1}{\sqrt{N}}\right)$. By more closely reflecting the practical scenario of a single fine-tuning run, our findings offer crucial insights into the reliability and practicality of asymmetric LoRA.

</details>


### [376] [Uniform Mean Estimation for Heavy-Tailed Distributions via Median-of-Means](https://arxiv.org/abs/2506.14673)
*Mikael Møller Høgsgaard,Andrea Paudice*

Main category: stat.ML

TL;DR: 本文研究了中位数均值（MoM）在重尾数据中的表现，提出了一种新的对称化技术，改进了样本复杂度界限，并应用于无界输入的k均值聚类和广义损失线性回归。


<details>
  <summary>Details</summary>
Motivation: 研究MoM在仅具有前p阶矩（p∈(1,2]）的数据分布中对函数类均值估计的性能，填补现有研究的不足。

Method: 采用新的对称化技术，推导出样本复杂度界限。

Result: 证明了新的样本复杂度界限，并在k均值聚类和线性回归中改进了现有结果。

Conclusion: MoM在重尾数据中表现优异，新对称化技术具有独立价值，应用广泛。

Abstract: The Median of Means (MoM) is a mean estimator that has gained popularity in the context of heavy-tailed data. In this work, we analyze its performance in the task of simultaneously estimating the mean of each function in a class $\mathcal{F}$ when the data distribution possesses only the first $p$ moments for $p \in (1,2]$. We prove a new sample complexity bound using a novel symmetrization technique that may be of independent interest. Additionally, we present applications of our result to $k$-means clustering with unbounded inputs and linear regression with general losses, improving upon existing works.

</details>


<div id='math.ST'></div>

# math.ST [[Back]](#toc)

### [377] [Complete Characterization for Adjustment in Summary Causal Graphs of Time Series](https://arxiv.org/abs/2506.14534)
*Clément Yvernes,Emilie Devijver,Eric Gaussier*

Main category: math.ST

TL;DR: 研究了时间序列中多干预下的可识别性问题，提出了调整准则的充要条件，并提供了一个伪线性算法来判断查询是否可识别。


<details>
  <summary>Details</summary>
Motivation: 解决在仅观测数据下，如何评估总因果效应是否可以通过无干预公式表达的问题，尤其是在时间序列和抽象因果图的背景下。

Method: 提出了调整准则的充要条件，并设计了一个伪线性算法来判定查询的可识别性。

Result: 证明了调整准则在此设定下的完备性，并提供了有效的算法支持。

Conclusion: 该研究为时间序列和多干预下的因果效应识别提供了理论和方法支持。

Abstract: The identifiability problem for interventions aims at assessing whether the total causal effect can be written with a do-free formula, and thus be estimated from observational data only. We study this problem, considering multiple interventions, in the context of time series when only an abstraction of the true causal graph, in the form of a summary causal graph, is available. We propose in particular both necessary and sufficient conditions for the adjustment criterion, which we show is complete in this setting, and provide a pseudo-linear algorithm to decide whether the query is identifiable or not.

</details>


<div id='math.NA'></div>

# math.NA [[Back]](#toc)

### [378] [A Hybrid Neural Network -- Polynomial Series Scheme for Learning Invariant Manifolds of Discrete Dynamical Systems](https://arxiv.org/abs/2506.13950)
*Dimitrios G. Patsatzis,Nikolaos Kazantzis,Ioannis G. Kevrekidis,Constantinos Siettos*

Main category: math.NA

TL;DR: 提出了一种混合机器学习方案，结合多项式级数和浅层神经网络，以物理和数值分析为基础学习离散映射的不变流形，用于构建动力系统的降阶模型。


<details>
  <summary>Details</summary>
Motivation: 旨在通过结合多项式级数和神经网络的互补优势，提高降阶模型的数值逼近精度和计算效率。

Method: 采用多项式级数和浅层神经网络的混合方案，多项式用于固定点附近的局部指数收敛建模，神经网络用于复杂结构的逼近。

Result: 在三个基准测试中，混合方案在数值逼近精度上优于纯多项式或神经网络方案。

Conclusion: 混合方案在构建降阶模型时表现出更高的效率和精度，优于单一方法。

Abstract: We propose a hybrid machine learning scheme to learn -- in physics-informed and numerical analysis-informed fashion -- invariant manifolds (IM) of discrete maps for constructing reduced-order models (ROMs) for dynamical systems. The proposed scheme combines polynomial series with shallow neural networks, exploiting the complementary strengths of both approaches. Polynomials enable an efficient and accurate modeling of ROMs with guaranteed local exponential convergence rate around the fixed point, where, under certain assumptions, the IM is demonstrated to be analytic. Neural networks provide approximations to more complex structures beyond the reach of the polynomials' convergence. We evaluate the efficiency of the proposed scheme using three benchmark examples, examining convergence behavior, numerical approximation accuracy, and computational training cost. Additionally, we compare the IM approximations obtained solely with neural networks and with polynomial expansions. We demonstrate that the proposed hybrid scheme outperforms both pure polynomial approximations (power series, Legendre and Chebyshev polynomials) and standalone shallow neural network approximations in terms of numerical approximation accuracy.

</details>


<div id='q-bio.BM'></div>

# q-bio.BM [[Back]](#toc)

### [379] [Reimagining Target-Aware Molecular Generation through Retrieval-Enhanced Aligned Diffusion](https://arxiv.org/abs/2506.14488)
*Dong Xu,Zhangfan Yang,Ka-chun Wong,Zexuan Zhu,Jiangqiang Li,Junkai Ji*

Main category: q-bio.BM

TL;DR: READ是一种结合分子检索增强生成与SE(3)-等变扩散模型的新方法，用于解决药物设计中几何拟合与化学约束的平衡问题。


<details>
  <summary>Details</summary>
Motivation: 高精度蛋白质结构预测（如AlphaFold）推动了基于受体的分子设计，但现有方法难以平衡几何拟合与化学约束。

Method: READ通过对比预训练编码器对齐原子级表示，并在推理时检索口袋匹配的支架图嵌入，指导反向扩散步骤。

Result: 实验显示READ在CBGBench中表现优异，超越现有生成模型和天然配体。

Conclusion: 检索与扩散的协同优化可加速结构药物设计，提高可靠性。

Abstract: Breakthroughs in high-accuracy protein structure prediction, such as AlphaFold, have established receptor-based molecule design as a critical driver for rapid early-phase drug discovery. However, most approaches still struggle to balance pocket-specific geometric fit with strict valence and synthetic constraints. To resolve this trade-off, a Retrieval-Enhanced Aligned Diffusion termed READ is introduced, which is the first to merge molecular Retrieval-Augmented Generation with an SE(3)-equivariant diffusion model. Specifically, a contrastively pre-trained encoder aligns atom-level representations during training, then retrieves graph embeddings of pocket-matched scaffolds to guide each reverse-diffusion step at inference. This single mechanism can inject real-world chemical priors exactly where needed, producing valid, diverse, and shape-complementary ligands. Experimental results demonstrate that READ can achieve very competitive performance in CBGBench, surpassing state-of-the-art generative models and even native ligands. That suggests retrieval and diffusion can be co-optimized for faster, more reliable structure-based drug design.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [380] [AGENTSAFE: Benchmarking the Safety of Embodied Agents on Hazardous Instructions](https://arxiv.org/abs/2506.14697)
*Aishan Liu,Zonghao Ying,Le Wang,Junjie Mu,Jinyang Guo,Jiakai Wang,Yuqing Ma,Siyuan Liang,Mingchuan Zhang,Xianglong Liu,Dacheng Tao*

Main category: cs.CR

TL;DR: AGENTSAFE是首个用于评估具身VLM代理在危险指令下安全性的综合基准，通过模拟环境和新型适配模块实现高风险任务测试。


<details>
  <summary>Details</summary>
Motivation: 随着视觉语言模型（VLMs）在现实环境中的部署增加，其安全性问题日益突出，尤其是在应对危险指令时。

Method: AGENTSAFE通过模拟沙盒环境实现代理-环境交互，并引入适配模块将高级VLM输出映射为可执行的低级控制动作。

Result: 构建了包含45个对抗场景、1,350个危险任务和8,100条危险指令的数据集，支持从感知到执行的系统性测试。

Conclusion: AGENTSAFE为具身VLM代理的安全性评估提供了全面且系统的解决方案。

Abstract: The rapid advancement of vision-language models (VLMs) and their integration into embodied agents have unlocked powerful capabilities for decision-making. However, as these systems are increasingly deployed in real-world environments, they face mounting safety concerns, particularly when responding to hazardous instructions. In this work, we propose AGENTSAFE, the first comprehensive benchmark for evaluating the safety of embodied VLM agents under hazardous instructions. AGENTSAFE simulates realistic agent-environment interactions within a simulation sandbox and incorporates a novel adapter module that bridges the gap between high-level VLM outputs and low-level embodied controls. Specifically, it maps recognized visual entities to manipulable objects and translates abstract planning into executable atomic actions in the environment. Building on this, we construct a risk-aware instruction dataset inspired by Asimovs Three Laws of Robotics, including base risky instructions and mutated jailbroken instructions. The benchmark includes 45 adversarial scenarios, 1,350 hazardous tasks, and 8,100 hazardous instructions, enabling systematic testing under adversarial conditions ranging from perception, planning, and action execution stages.

</details>


### [381] [Busting the Paper Ballot: Voting Meets Adversarial Machine Learning](https://arxiv.org/abs/2506.14582)
*Kaleel Mahmood,Caleb Manicke,Ethan Rathbun,Aayushi Verma,Sohaib Ahmad,Nicholas Stamatakis,Laurent Michel,Benjamin Fuller*

Main category: cs.CR

TL;DR: 论文探讨了在美国选举制表机中使用机器学习分类器的安全风险，通过新数据集和多种模型展示了对抗攻击的可行性及其潜在影响。


<details>
  <summary>Details</summary>
Motivation: 研究动机是揭示机器学习分类器在选举制表机中的潜在安全漏洞，尤其是对抗攻击可能对选举结果产生的重大影响。

Method: 方法包括引入四个新数据集、训练多种模型（如SVM、CNN、Vision Transformers）、分析梯度掩蔽问题并提出改进方法，以及在物理世界中实施对抗攻击。

Result: 结果显示传统白盒攻击因梯度掩蔽而无效，但通过改进方法可生成有效对抗样本，甚至在物理世界中也能影响选举结果。

Conclusion: 结论指出，即使攻击成功率低至5%，也可能改变选举结果，强调了选举系统中机器学习模型的安全性需进一步研究。

Abstract: We show the security risk associated with using machine learning classifiers in United States election tabulators. The central classification task in election tabulation is deciding whether a mark does or does not appear on a bubble associated to an alternative in a contest on the ballot. Barretto et al. (E-Vote-ID 2021) reported that convolutional neural networks are a viable option in this field, as they outperform simple feature-based classifiers.
  Our contributions to election security can be divided into four parts. To demonstrate and analyze the hypothetical vulnerability of machine learning models on election tabulators, we first introduce four new ballot datasets. Second, we train and test a variety of different models on our new datasets. These models include support vector machines, convolutional neural networks (a basic CNN, VGG and ResNet), and vision transformers (Twins and CaiT). Third, using our new datasets and trained models, we demonstrate that traditional white box attacks are ineffective in the voting domain due to gradient masking. Our analyses further reveal that gradient masking is a product of numerical instability. We use a modified difference of logits ratio loss to overcome this issue (Croce and Hein, ICML 2020). Fourth, in the physical world, we conduct attacks with the adversarial examples generated using our new methods. In traditional adversarial machine learning, a high (50% or greater) attack success rate is ideal. However, for certain elections, even a 5% attack success rate can flip the outcome of a race. We show such an impact is possible in the physical domain. We thoroughly discuss attack realism, and the challenges and practicality associated with printing and scanning ballot adversarial examples.

</details>


### [382] [Excessive Reasoning Attack on Reasoning LLMs](https://arxiv.org/abs/2506.14374)
*Wai Man Si,Mingjie Li,Michael Backes,Yang Zhang*

Main category: cs.CR

TL;DR: 论文提出了一种针对大型语言模型（LLMs）的新型对抗攻击方法，通过设计特定输入增加计算开销而不影响模型性能。


<details>
  <summary>Details</summary>
Motivation: 现有推理型LLMs在复杂任务中表现优异，但存在计算成本高的问题，尤其是过度推理行为（如频繁切换推理路径或冗余推理）。本文旨在利用这些行为设计对抗输入，增加计算开销。

Method: 提出了一种包含三个组件的损失框架：优先级交叉熵损失、过度推理损失和延迟终止损失，以优化对抗攻击。

Result: 在GSM8K和ORCA数据集上，攻击导致推理长度增加3至9倍，同时保持模型性能。对抗输入还表现出对其他模型的迁移性。

Conclusion: 研究表明，对抗输入可以显著增加LLMs的计算开销，揭示了模型推理行为的安全隐患。

Abstract: Recent reasoning large language models (LLMs), such as OpenAI o1 and DeepSeek-R1, exhibit strong performance on complex tasks through test-time inference scaling. However, prior studies have shown that these models often incur significant computational costs due to excessive reasoning, such as frequent switching between reasoning trajectories (e.g., underthinking) or redundant reasoning on simple questions (e.g., overthinking). In this work, we expose a novel threat: adversarial inputs can be crafted to exploit excessive reasoning behaviors and substantially increase computational overhead without compromising model utility. Therefore, we propose a novel loss framework consisting of three components: (1) Priority Cross-Entropy Loss, a modification of the standard cross-entropy objective that emphasizes key tokens by leveraging the autoregressive nature of LMs; (2) Excessive Reasoning Loss, which encourages the model to initiate additional reasoning paths during inference; and (3) Delayed Termination Loss, which is designed to extend the reasoning process and defer the generation of final outputs. We optimize and evaluate our attack for the GSM8K and ORCA datasets on DeepSeek-R1-Distill-LLaMA and DeepSeek-R1-Distill-Qwen. Empirical results demonstrate a 3x to 9x increase in reasoning length with comparable utility performance. Furthermore, our crafted adversarial inputs exhibit transferability, inducing computational overhead in o3-mini, o1-mini, DeepSeek-R1, and QWQ models.

</details>


### [383] [LLM-Powered Intent-Based Categorization of Phishing Emails](https://arxiv.org/abs/2506.14337)
*Even Eilertsen,Vasileios Mavroeidis,Gudmund Grov*

Main category: cs.CR

TL;DR: 论文探讨了利用大型语言模型（LLMs）检测钓鱼邮件的潜力，通过意图分类生成可操作的威胁信息。


<details>
  <summary>Details</summary>
Motivation: 钓鱼攻击对现代网络安全构成重大威胁，传统检测系统依赖用户不可见的元数据，而LLMs能通过文本意图识别钓鱼邮件。

Method: 研究使用LLMs对钓鱼邮件进行二元分类，并引入意图类型分类法，通过公开数据集验证效果。

Result: 实验证明现有LLMs能够有效检测和分类钓鱼邮件。

Conclusion: LLMs在钓鱼邮件检测领域具有实际应用潜力。

Abstract: Phishing attacks remain a significant threat to modern cybersecurity, as they successfully deceive both humans and the defense mechanisms intended to protect them. Traditional detection systems primarily focus on email metadata that users cannot see in their inboxes. Additionally, these systems struggle with phishing emails, which experienced users can often identify empirically by the text alone. This paper investigates the practical potential of Large Language Models (LLMs) to detect these emails by focusing on their intent. In addition to the binary classification of phishing emails, the paper introduces an intent-type taxonomy, which is operationalized by the LLMs to classify emails into distinct categories and, therefore, generate actionable threat information. To facilitate our work, we have curated publicly available datasets into a custom dataset containing a mix of legitimate and phishing emails. Our results demonstrate that existing LLMs are capable of detecting and categorizing phishing emails, underscoring their potential in this domain.

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [384] [Balancing Preservation and Modification: A Region and Semantic Aware Metric for Instruction-Based Image Editing](https://arxiv.org/abs/2506.13827)
*Zhuoying Li,Zhu Xu,Yuxin Peng,Yang Liu*

Main category: cs.GR

TL;DR: 提出了一种名为BPM的新指标，用于评估基于指令的图像编辑质量，通过明确分离编辑相关和无关区域，提供更全面的评估。


<details>
  <summary>Details</summary>
Motivation: 现有指标要么成本高，要么缺乏任务针对性，无法全面评估指令编辑和无关区域保留。

Method: BPM通过定位编辑相关区域，采用两阶段评估：区域感知判断和语义感知判断。

Result: BPM在实验中与人类评估一致性最高，优于现有指标。

Conclusion: BPM是一种高效且广泛适用的评估指标，可提升图像编辑质量。

Abstract: Instruction-based image editing, which aims to modify the image faithfully according to the instruction while preserving irrelevant content unchanged, has made significant progress. However, there still lacks a comprehensive metric for assessing the editing quality. Existing metrics either require high human evaluation costs, which hinder large-scale evaluation, or are adapted from other tasks and lose task-specific concerns, failing to comprehensively evaluate both instruction-based modification and preservation of irrelevant regions, resulting in biased evaluation. To tackle this, we introduce a new metric called Balancing Preservation and Modification (BPM), tailored for instruction-based image editing by explicitly disentangling the image into editing-relevant and irrelevant regions for specific consideration. We first identify and locate editing-relevant regions, followed by a two-tier process to assess editing quality: Region-Aware Judge evaluates whether the position and size of the edited region align with the instruction, and Semantic-Aware Judge further assesses the instruction content compliance within editing-relevant regions as well as content preservation within irrelevant regions, yielding comprehensive and interpretable quality assessment. Moreover, the editing-relevant region localization in BPM can be integrated into image editing approaches to improve editing quality, demonstrating its broad applicability. We verify the effectiveness of the BPM metric on comprehensive instruction-editing data, and the results show the highest alignment with human evaluation compared to existing metrics, indicating its efficacy. Code is available at: https://joyli-x.github.io/BPM/

</details>


### [385] [Innovating China's Intangible Cultural Heritage with DeepSeek + MidJourney: The Case of Yangliuqing theme Woodblock Prints](https://arxiv.org/abs/2506.14104)
*RuiKun Yang,ZhongLiang Wei,Longdi Xian*

Main category: cs.GR

TL;DR: 该研究结合DeepSeek和MidJourney生成杨柳青木版年画，主题为抗疫和欢乐胜利者，通过FID评分和问卷反馈验证了方法的有效性。


<details>
  <summary>Details</summary>
Motivation: 保护和创新杨柳青木版年画这一非物质文化遗产，同时探索现代AI技术与传统艺术的结合。

Method: 采用DeepSeek生成主题提示，MidJourney生成图像，结合原始年画和关键提示，通过FID评分和问卷评估效果。

Result: 混合方法获得最低FID分数（150.2，σ=4.9），问卷显示参与者对结果最满意，且推广传统文化意愿最高。

Conclusion: 结合AI与传统艺术的方法既能保护文化遗产，又能赋予其现代意义，具有推广价值。

Abstract: Yangliuqing woodblock prints, a cornerstone of China's intangible cultural heritage, are celebrated for their intricate designs and vibrant colors. However, preserving these traditional art forms while fostering innovation presents significant challenges. This study explores the DeepSeek + MidJourney approach to generating creative, themed Yangliuqing woodblock prints focused on the fight against COVID-19 and depicting joyous winners. Using Fréchet Inception Distance (FID) scores for evaluation, the method that combined DeepSeek-generated thematic prompts, MidJourney-generated thematic images, original Yangliuqing prints, and DeepSeek-generated key prompts in MidJourney-generated outputs achieved the lowest mean FID score (150.2) with minimal variability (σ = 4.9). Additionally, feedback from 62 participants, collected via questionnaires, confirmed that this hybrid approach produced the most representative results. Moreover, the questionnaire data revealed that participants demonstrated the highest willingness to promote traditional culture and the strongest interest in consuming the AI-generated images produced through this method. These findings underscore the effectiveness of an innovative approach that seamlessly blends traditional artistic elements with modern AI-driven creativity, ensuring both cultural preservation and contemporary relevance.

</details>


### [386] [ImmerseGen: Agent-Guided Immersive World Generation with Alpha-Textured Proxies](https://arxiv.org/abs/2506.14315)
*Jinyan Yuan,Bangbang Yang,Keke Wang,Panwang Pan,Lin Ma,Xuehai Zhang,Xiao Liu,Zhaopeng Cui,Yuewen Ma*

Main category: cs.GR

TL;DR: ImmerseGen提出了一种基于代理引导的轻量级3D场景建模框架，通过简化几何代理和合成RGBA纹理实现高效且逼真的VR场景生成。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖高多边形建模或大量3D高斯分布，导致流程复杂或视觉逼真度不足。ImmerseGen旨在简化建模流程并提升视觉质量。

Method: 使用层次化轻量几何代理（如简化地形和广告牌网格），通过地形条件纹理和RGBA资产纹理合成逼真外观。结合VLM代理和语义网格分析实现自动化场景生成。

Result: 实验表明，ImmerseGen在逼真度、空间一致性和渲染效率上优于现有方法，适合移动VR设备实时渲染。

Conclusion: ImmerseGen通过简化几何和纹理合成，实现了高效且逼真的3D场景生成，为VR沉浸体验提供了新解决方案。

Abstract: Automatic creation of 3D scenes for immersive VR presence has been a significant research focus for decades. However, existing methods often rely on either high-poly mesh modeling with post-hoc simplification or massive 3D Gaussians, resulting in a complex pipeline or limited visual realism. In this paper, we demonstrate that such exhaustive modeling is unnecessary for achieving compelling immersive experience. We introduce ImmerseGen, a novel agent-guided framework for compact and photorealistic world modeling. ImmerseGen represents scenes as hierarchical compositions of lightweight geometric proxies, i.e., simplified terrain and billboard meshes, and generates photorealistic appearance by synthesizing RGBA textures onto these proxies. Specifically, we propose terrain-conditioned texturing for user-centric base world synthesis, and RGBA asset texturing for midground and foreground scenery.This reformulation offers several advantages: (i) it simplifies modeling by enabling agents to guide generative models in producing coherent textures that integrate seamlessly with the scene; (ii) it bypasses complex geometry creation and decimation by directly synthesizing photorealistic textures on proxies, preserving visual quality without degradation; (iii) it enables compact representations suitable for real-time rendering on mobile VR headsets. To automate scene creation from text prompts, we introduce VLM-based modeling agents enhanced with semantic grid-based analysis for improved spatial reasoning and accurate asset placement. ImmerseGen further enriches scenes with dynamic effects and ambient audio to support multisensory immersion. Experiments on scene generation and live VR showcases demonstrate that ImmerseGen achieves superior photorealism, spatial coherence and rendering efficiency compared to prior methods. Project webpage: https://immersegen.github.io.

</details>


### [387] [ReFrame: Layer Caching for Accelerated Inference in Real-Time Rendering](https://arxiv.org/abs/2506.13814)
*Lufei Liu,Tor M. Aamodt*

Main category: cs.GR

TL;DR: ReFrame通过缓存中间特征优化实时渲染任务，在质量损失可忽略的情况下平均提速1.4倍。


<details>
  <summary>Details</summary>
Motivation: 利用时序连贯性避免冗余计算，提升实时渲染效率。

Method: 扩展中间特征缓存策略至实时渲染，探索不同缓存策略以优化质量与性能的权衡。

Result: 在三种实时渲染任务中平均提速1.4倍，质量损失可忽略。

Conclusion: ReFrame适用于多种编码器-解码器网络，有效提升渲染性能。

Abstract: Graphics rendering applications increasingly leverage neural networks in tasks such as denoising, supersampling, and frame extrapolation to improve image quality while maintaining frame rates. The temporal coherence inherent in these tasks presents an opportunity to reuse intermediate results from previous frames and avoid redundant computations. Recent work has shown that caching intermediate features to be reused in subsequent inferences is an effective method to reduce latency in diffusion models. We extend this idea to real-time rendering and present ReFrame, which explores different caching policies to optimize trade-offs between quality and performance in rendering workloads. ReFrame can be applied to a variety of encoder-decoder style networks commonly found in rendering pipelines. Experimental results show that we achieve 1.4x speedup on average with negligible quality loss in three real-time rendering tasks. Code available: https://ubc-aamodt-group.github.io/reframe-layer-caching/

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [388] [Knowledge Compression via Question Generation: Enhancing Multihop Document Retrieval without Fine-tuning](https://arxiv.org/abs/2506.13778)
*Anvi Alex Eponon,Moein Shahiki-Tash,Ildar Batyrshin,Christian E. Maldonado-Sifuentes,Grigori Sidorov,Alexander Gelbukh*

Main category: cs.IR

TL;DR: 提出了一种基于问题编码的知识表示方法，显著提升检索增强生成（RAG）系统的性能，无需微调或传统分块。


<details>
  <summary>Details</summary>
Motivation: 改进RAG系统的检索效率，减少计算和存储需求，同时提供更直观的知识访问方式。

Method: 通过生成覆盖词汇和语义空间的问题编码文本内容，结合自定义语法重排序方法。

Result: 在单跳检索中Recall@3达0.84，多跳任务中F1分数0.52，显著优于传统方法。

Conclusion: 该方法无需微调，降低延迟和存储需求，是高效且可扩展的RAG替代方案。

Abstract: This study presents a question-based knowledge encoding approach that improves retrieval-augmented generation (RAG) systems without requiring fine-tuning or traditional chunking. We encode textual content using generated questions that span the lexical and semantic space, creating targeted retrieval cues combined with a custom syntactic reranking method.
  In single-hop retrieval over 109 scientific papers, our approach achieves a Recall@3 of 0.84, outperforming traditional chunking methods by 60 percent. We also introduce "paper-cards", concise paper summaries under 300 characters, which enhance BM25 retrieval, increasing MRR@3 from 0.56 to 0.85 on simplified technical queries.
  For multihop tasks, our reranking method reaches an F1 score of 0.52 with LLaMA2-Chat-7B on the LongBench 2WikiMultihopQA dataset, surpassing chunking and fine-tuned baselines which score 0.328 and 0.412 respectively.
  This method eliminates fine-tuning requirements, reduces retrieval latency, enables intuitive question-driven knowledge access, and decreases vector storage demands by 80%, positioning it as a scalable and efficient RAG alternative.

</details>


### [389] [XGraphRAG: Interactive Visual Analysis for Graph-based Retrieval-Augmented Generation](https://arxiv.org/abs/2506.13782)
*Ke Wang,Bo Pan,Yingchaojie Feng,Yuwei Wu,Jieyi Chen,Minfeng Zhu,Wei Chen*

Main category: cs.IR

TL;DR: 提出了一种可视化分析框架XGraphRAG，帮助开发者分析GraphRAG的召回效果并追踪其流程，提升可解释性和可访问性。


<details>
  <summary>Details</summary>
Motivation: GraphRAG虽能提升LLM答案的精确性和全面性，但其复杂的信息处理流程和大量LLM调用限制了其可解释性和可访问性。

Method: 开发了XGraphRAG原型系统，包含交互式可视化工具，用于分析GraphRAG的关键召回和流程追踪。

Result: 评估证明了该方法的有效性和可用性。

Conclusion: XGraphRAG能有效帮助开发者识别失败案例和改进机会，系统已开源。

Abstract: Graph-based Retrieval-Augmented Generation (RAG) has shown great capability in enhancing Large Language Model (LLM)'s answer with an external knowledge base. Compared to traditional RAG, it introduces a graph as an intermediate representation to capture better structured relational knowledge in the corpus, elevating the precision and comprehensiveness of generation results. However, developers usually face challenges in analyzing the effectiveness of GraphRAG on their dataset due to GraphRAG's complex information processing pipeline and the overwhelming amount of LLM invocations involved during graph construction and query, which limits GraphRAG interpretability and accessibility. This research proposes a visual analysis framework that helps RAG developers identify critical recalls of GraphRAG and trace these recalls through the GraphRAG pipeline. Based on this framework, we develop XGraphRAG, a prototype system incorporating a set of interactive visualizations to facilitate users' analysis process, boosting failure cases collection and improvement opportunities identification. Our evaluation demonstrates the effectiveness and usability of our approach. Our work is open-sourced and available at https://github.com/Gk0Wk/XGraphRAG.

</details>


### [390] [Analysis of Anonymous User Interaction Relationships and Prediction of Advertising Feedback Based on Graph Neural Network](https://arxiv.org/abs/2506.13787)
*Yanjun Dai,Haoyang Feng,Yuan Gao*

Main category: cs.IR

TL;DR: 论文提出了一种解耦时序层次图神经网络（DTH-GNN），用于捕捉匿名用户交互网络的多尺度时序、语义和高阶依赖特征，显著提升了广告投放效果。


<details>
  <summary>Details</summary>
Motivation: 现有图模型难以捕捉匿名用户交互网络的多尺度时序、语义和高阶依赖特征，无法描述复杂行为模式。

Method: 1. 引入时序边分解，将交互分为短期突发、昼夜周期和长期记忆三类通道；2. 构建层次异构聚合，通过元路径条件Transformer编码器结合子图；3. 提出反馈感知的对比正则化，优化节点表示。

Result: DTH-GNN的AUC提升了8.2%，对数损失降低了5.7%。

Conclusion: DTH-GNN能有效捕捉复杂行为模式，显著提升广告投放效果。

Abstract: While online advertising is highly dependent on implicit interaction networks of anonymous users for engagement inference, and for the selection and optimization of delivery strategies, existing graph models seldom can capture the multi-scale temporal, semantic and higher-order dependency features of these interaction networks, thus it's hard to describe the complicated patterns of the anonymous behavior. In this paper, we propose Decoupled Temporal-Hierarchical Graph Neural Network (DTH-GNN), which achieves three main contributions. Above all, we introduce temporal edge decomposition, which divides each interaction into three types of channels: short-term burst, diurnal cycle and long-range memory, and conducts feature extraction using the convolution kernel of parallel dilated residuals; Furthermore, our model builds a hierarchical heterogeneous aggregation, where user-user, user-advertisement, advertisement-advertisement subgraphs are combined through the meta-path conditional Transformer encoder, where the noise structure is dynamically tamped down via the synergy of cross-channel self-attention and gating relationship selector. Thirdly, the contrast regularity of feedback perception is formulated, the consistency of various time slices is maximized, the entropy of control exposure information with dual-view target is maximized, the global prototype of dual-momentum queue distillation is presented, and the strategy gradient layer with light weight is combined with delaying transformation signal to fine-tune the node representation for benefit-oriented. The AUC of DTH-GNN improved by 8.2% and the logarithmic loss improved by 5.7% in comparison with the best baseline model.

</details>


### [391] [AcademicBrowse: Benchmarking Academic Browse Ability of LLMs](https://arxiv.org/abs/2506.13784)
*Junting Zhou,Wang Li,Yiyan Liao,Nengyuan Zhang,Tingjia Miaoand Zhihui Qi,Yuhan Wu,Tong Yang*

Main category: cs.IR

TL;DR: 提出了AcademicBrowse数据集，专门评估大语言模型在学术研究中的复杂信息检索能力，填补现有基准的不足。


<details>
  <summary>Details</summary>
Motivation: 现有基准（如OpenAI的BrowseComp）未能满足学术搜索的特定需求，如深度文献追踪、专业数据库支持等。

Method: 设计AcademicBrowse数据集，具备学术实用性、高难度、简洁评估和广泛覆盖等特性。

Result: 数据集覆盖15个学科，提供独特答案和清晰来源，便于审核验证。

Conclusion: AcademicBrowse有望更精准衡量和提升大语言模型在学术信息检索中的表现。

Abstract: Large Language Models (LLMs)' search capabilities have garnered significant attention. Existing benchmarks, such as OpenAI's BrowseComp, primarily focus on general search scenarios and fail to adequately address the specific demands of academic search. These demands include deeper literature tracing and organization, professional support for academic databases, the ability to navigate long-tail academic knowledge, and ensuring academic rigor. Here, we proposed AcademicBrowse, the first dataset specifically designed to evaluate the complex information retrieval capabilities of Large Language Models (LLMs) in academic research. AcademicBrowse possesses the following key characteristics: Academic Practicality, where question content closely mirrors real academic learning and research environments, avoiding deliberately misleading models; High Difficulty, with answers that are challenging for single models (e.g., Grok DeepSearch or Gemini Deep Research) to provide directly, often requiring at least three deep searches to derive; Concise Evaluation, where limiting conditions ensure answers are as unique as possible, accompanied by clear sources and brief solution explanations, greatly facilitating subsequent audit and verification, surpassing the current lack of analyzed search datasets both domestically and internationally; and Broad Coverage, as the dataset spans at least 15 different academic disciplines. Through AcademicBrowse, we expect to more precisely measure and promote the performance improvement of LLMs in complex academic information retrieval tasks. The data is available at: https://huggingface.co/datasets/PKU-DS-LAB/AcademicBrowse

</details>


### [392] [InsertRank: LLMs can reason over BM25 scores to Improve Listwise Reranking](https://arxiv.org/abs/2506.14086)
*Rahul Seetharaman,Kaustubh D. Dhole,Aman Bansal*

Main category: cs.IR

TL;DR: InsertRank是一种基于大语言模型（LLM）的重排序器，通过结合BM25等词汇信号提升检索性能，在BRIGHT和R2MED基准测试中表现优异。


<details>
  <summary>Details</summary>
Motivation: 用户期望通过复杂查询进行文档推理检索，而非简单的关键词匹配或语义相似性。现有LLM重排序方法仍有改进空间。

Method: 提出InsertRank，结合BM25等词汇信号进行重排序，并在BRIGHT和R2MED基准上测试。

Result: InsertRank在BRIGHT和R2MED基准上表现优于现有方法，Deepseek-R1模型分别达到37.5和51.1分。

Conclusion: InsertRank通过结合词汇信号显著提升了LLM在复杂查询推理检索中的性能。

Abstract: Large Language Models (LLMs) have demonstrated significant strides across various information retrieval tasks, particularly as rerankers, owing to their strong generalization and knowledge-transfer capabilities acquired from extensive pretraining. In parallel, the rise of LLM-based chat interfaces has raised user expectations, encouraging users to pose more complex queries that necessitate retrieval by ``reasoning'' over documents rather than through simple keyword matching or semantic similarity. While some recent efforts have exploited reasoning abilities of LLMs for reranking such queries, considerable potential for improvement remains. In that regards, we introduce InsertRank, an LLM-based reranker that leverages lexical signals like BM25 scores during reranking to further improve retrieval performance. InsertRank demonstrates improved retrieval effectiveness on -- BRIGHT, a reasoning benchmark spanning 12 diverse domains, and R2MED, a specialized medical reasoning retrieval benchmark spanning 8 different tasks. We conduct an exhaustive evaluation and several ablation studies and demonstrate that InsertRank consistently improves retrieval effectiveness across multiple families of LLMs, including GPT, Gemini, and Deepseek models. %In addition, we also conduct ablation studies on normalization by varying the scale of the BM25 scores, and positional bias by shuffling the order of the documents. With Deepseek-R1, InsertRank achieves a score of 37.5 on the BRIGHT benchmark. and 51.1 on the R2MED benchmark, surpassing previous methods.

</details>


### [393] [RAGtifier: Evaluating RAG Generation Approaches of State-of-the-Art RAG Systems for the SIGIR LiveRAG Competition](https://arxiv.org/abs/2506.14412)
*Tim Cofala,Oleh Astappiev,William Xion,Hailay Teklehaymanot*

Main category: cs.IR

TL;DR: LiveRAG 2025挑战赛探索了RAG解决方案，结合InstructRAG、Pinecone检索器和BGE重排器，在SIGIR 2025中排名第四。


<details>
  <summary>Details</summary>
Motivation: 通过结合LLMs的内部参数知识与外部非参数知识，提高事实准确性并减少幻觉。

Method: 使用InstructRAG结合Pinecone检索器和BGE重排器，在限制条件下优化RAG解决方案。

Result: 正确性得分1.13，忠实度得分0.55，排名第四。

Conclusion: InstructRAG结合特定检索器和重排器在LiveRAG挑战中表现优异。

Abstract: Retrieval-Augmented Generation (RAG) enriches Large Language Models (LLMs) by combining their internal, parametric knowledge with external, non-parametric sources, with the goal of improving factual correctness and minimizing hallucinations. The LiveRAG 2025 challenge explores RAG solutions to maximize accuracy on DataMorgana's QA pairs, which are composed of single-hop and multi-hop questions. The challenge provides access to sparse OpenSearch and dense Pinecone indices of the Fineweb 10BT dataset. It restricts model use to LLMs with up to 10B parameters and final answer generation with Falcon-3-10B. A judge-LLM assesses the submitted answers along with human evaluators. By exploring distinct retriever combinations and RAG solutions under the challenge conditions, our final solution emerged using InstructRAG in combination with a Pinecone retriever and a BGE reranker. Our solution achieved a correctness score of 1.13 and a faithfulness score of 0.55, placing fourth in the SIGIR 2025 LiveRAG Challenge.

</details>


<div id='physics.ao-ph'></div>

# physics.ao-ph [[Back]](#toc)

### [394] [Projecting U.S. coastal storm surge risks and impacts with deep learning](https://arxiv.org/abs/2506.13963)
*Julian R. Rice,Karthik Balaguru,Fadia Ticona Rollano,John Wilson,Brent Daniel,David Judi,Ning Sun,L. Ruby Leung*

Main category: physics.ao-ph

TL;DR: 利用深度学习模型评估美国沿海风暴潮风险，结合未来热带气旋和海平面变化，发现本世纪末风险人口将增加50%。


<details>
  <summary>Details</summary>
Motivation: 风暴潮是热带气旋最致命的危害之一，但由于其罕见性和物理复杂性，评估其当前和未来风险具有挑战性。

Method: 采用深度学习风暴潮模型，基于90万次合成热带气旋事件，结合海平面和热带气旋行为变化，评估沿海风险。

Result: 历史100年风暴潮事件与观测数据吻合良好；本世纪末，热带气旋强度和海平面上升导致风险人口增加50%，佛罗里达州风险显著上升。

Conclusion: 深度学习模型为风暴潮风险评估提供了新途径，未来风险显著增加，尤其在佛罗里达州、乔治亚州和南卡罗来纳州存在关键阈值。

Abstract: Storm surge is one of the deadliest hazards posed by tropical cyclones (TCs), yet assessing its current and future risk is difficult due to the phenomenon's rarity and physical complexity. Recent advances in artificial intelligence applications to natural hazard modeling suggest a new avenue for addressing this problem. We utilize a deep learning storm surge model to efficiently estimate coastal surge risk in the United States from 900,000 synthetic TC events, accounting for projected changes in TC behavior and sea levels. The derived historical 100-year surge (the event with a 1% yearly exceedance probability) agrees well with historical observations and other modeling techniques. When coupled with an inundation model, we find that heightened TC intensities and sea levels by the end of the century result in a 50% increase in population at risk. Key findings include markedly heightened risk in Florida, and critical thresholds identified in Georgia and South Carolina.

</details>


### [395] [AI-Informed Model Analogs for Subseasonal-to-Seasonal Prediction](https://arxiv.org/abs/2506.14022)
*Jacob B. Landsberg,Elizabeth A. Barnes,Matthew Newman*

Main category: physics.ao-ph

TL;DR: 论文提出了一种基于可解释AI的模型模拟预测方法，用于改进次季节到季节（S2S）预测，并在多个任务中优于传统方法。


<details>
  <summary>Details</summary>
Motivation: S2S预测对公共卫生、灾害准备和农业至关重要，但预测难度大，因此探索AI改进方法。

Method: 使用人工神经网络学习权重掩码以优化模拟选择，并在三个不同预测任务中验证其效果。

Result: AI方法在气候模型和再分析数据上的确定性和概率性技能指标均优于传统方法，并能更好地预测极端温度和不确定性。

Conclusion: 可解释AI框架不仅提升了预测性能，还帮助理解S2S的可预测性来源。

Abstract: Subseasonal-to-seasonal forecasting is crucial for public health, disaster preparedness, and agriculture, and yet it remains a particularly challenging timescale to predict. We explore the use of an interpretable AI-informed model analog forecasting approach, previously employed on longer timescales, to improve S2S predictions. Using an artificial neural network, we learn a mask of weights to optimize analog selection and showcase its versatility across three varied prediction tasks: 1) classification of Week 3-4 Southern California summer temperatures; 2) regional regression of Month 1 midwestern U.S. summer temperatures; and 3) classification of Month 1-2 North Atlantic wintertime upper atmospheric winds. The AI-informed analogs outperform traditional analog forecasting approaches, as well as climatology and persistence baselines, for deterministic and probabilistic skill metrics on both climate model and reanalysis data. We find the analog ensembles built using the AI-informed approach also produce better predictions of temperature extremes and improve representation of forecast uncertainty. Finally, by using an interpretable-AI framework, we analyze the learned masks of weights to better understand S2S sources of predictability.

</details>


<div id='cs.NE'></div>

# cs.NE [[Back]](#toc)

### [396] [NeuroCoreX: An Open-Source FPGA-Based Spiking Neural Network Emulator with On-Chip Learning](https://arxiv.org/abs/2506.14138)
*Ashish Gautam,Prasanna Date,Shruti Kulkarni,Robert Patton,Thomas Potok*

Main category: cs.NE

TL;DR: NeuroCoreX是一个基于FPGA的脉冲神经网络（SNN）模拟器，支持灵活的拓扑设计和生物启发的学习机制，旨在推动高效能计算研究。


<details>
  <summary>Details</summary>
Motivation: 传统人工神经网络（ANN）在拓扑结构和能效方面存在局限，而SNN因其事件驱动特性和生物启发的连接模式更具潜力。

Method: 开发了NeuroCoreX模拟器，支持全连接拓扑和基于STDP的局部学习机制，采用LIF神经元模型和电流突触，提供Python接口和UART配置。

Result: NeuroCoreX实现了灵活的SNN设计和硬件执行，开源框架加速了高效能计算研究。

Conclusion: NeuroCoreX为SNN研究和应用提供了高效、灵活的工具，推动了生物启发计算的发展。

Abstract: Spiking Neural Networks (SNNs) are computational models inspired by the structure and dynamics of biological neuronal networks. Their event-driven nature enables them to achieve high energy efficiency, particularly when deployed on neuromorphic hardware platforms. Unlike conventional Artificial Neural Networks (ANNs), which primarily rely on layered architectures, SNNs naturally support a wide range of connectivity patterns, from traditional layered structures to small-world graphs characterized by locally dense and globally sparse connections. In this work, we introduce NeuroCoreX, an FPGA-based emulator designed for the flexible co-design and testing of SNNs. NeuroCoreX supports all-to-all connectivity, providing the capability to implement diverse network topologies without architectural restrictions. It features a biologically motivated local learning mechanism based on Spike-Timing-Dependent Plasticity (STDP). The neuron model implemented within NeuroCoreX is the Leaky Integrate-and-Fire (LIF) model, with current-based synapses facilitating spike integration and transmission . A Universal Asynchronous Receiver-Transmitter (UART) interface is provided for programming and configuring the network parameters, including neuron, synapse, and learning rule settings. Users interact with the emulator through a simple Python-based interface, streamlining SNN deployment from model design to hardware execution. NeuroCoreX is released as an open-source framework, aiming to accelerate research and development in energy-efficient, biologically inspired computing.

</details>


### [397] [A Scalable Hybrid Training Approach for Recurrent Spiking Neural Networks](https://arxiv.org/abs/2506.14464)
*Maximilian Baronig,Yeganeh Bahariasl,Ozan Özdenizci,Robert Legenstein*

Main category: cs.NE

TL;DR: HYPR算法结合并行化和近似在线前向学习，实现了高效在线学习，适用于非线性脉冲神经元模型。


<details>
  <summary>Details</summary>
Motivation: 解决BPTT在训练RSNNs时的在线训练和高内存消耗问题，同时提升前向梯度学习方法的性能。

Method: 提出HYPR算法，结合并行化和近似在线前向学习，实现高吞吐量和恒定内存需求。

Result: HYPR在振荡亚阈值动态的脉冲神经元网络中表现优异，性能接近BPTT。

Conclusion: HYPR为RSNNs提供了一种高效、内存友好的在线学习方法，特别适用于复杂神经元模型。

Abstract: Recurrent spiking neural networks (RSNNs) can be implemented very efficiently in neuromorphic systems. Nevertheless, training of these models with powerful gradient-based learning algorithms is mostly performed on standard digital hardware using Backpropagation through time (BPTT). However, BPTT has substantial limitations. It does not permit online training and its memory consumption scales linearly with the number of computation steps. In contrast, learning methods using forward propagation of gradients operate in an online manner with a memory consumption independent of the number of time steps. These methods enable SNNs to learn from continuous, infinite-length input sequences. Yet, slow execution speed on conventional hardware as well as inferior performance has hindered their widespread application. In this work, we introduce HYbrid PRopagation (HYPR) that combines the efficiency of parallelization with approximate online forward learning. Our algorithm yields high-throughput online learning through parallelization, paired with constant, i.e., sequence length independent, memory demands. HYPR enables parallelization of parameter update computation over the sub sequences for RSNNs consisting of almost arbitrary non-linear spiking neuron models. We apply HYPR to networks of spiking neurons with oscillatory subthreshold dynamics. We find that this type of neuron model is particularly well trainable by HYPR, resulting in an unprecedentedly low task performance gap between approximate forward gradient learning and BPTT.

</details>


### [398] [Is Selection All You Need in Differential Evolution?](https://arxiv.org/abs/2506.14425)
*Tomofumi Kitamura,Alex Fukunaga*

Main category: cs.NE

TL;DR: 论文提出了一种名为无界差分进化（UDE）的新框架，通过取消世代替换和存档管理，简化了差分进化算法。


<details>
  <summary>Details</summary>
Motivation: 传统差分进化算法因固定种群大小和世代替换导致种群多样性受限，且存档机制引入额外复杂性。

Method: UDE将所有生成的候选个体加入种群，不丢弃任何个体，仅依赖选择机制。

Result: UDE提供了一种更简单但强大的搜索算法，避免了存档管理和动态种群大小的复杂性。

Conclusion: UDE是一种全新的差分进化方法，简化了设计并提升了性能。

Abstract: Differential Evolution (DE) is a widely used evolutionary algorithm for black-box optimization problems. However, in modern DE implementations, a major challenge lies in the limited population diversity caused by the fixed population size enforced by the generational replacement. Population size is a critical control parameter that significantly affects DE performance. Larger populations inherently contain a more diverse set of individuals, thereby facilitating broader exploration of the search space. Conversely, when the maximum evaluation budgets is constrained, smaller populations focusing on a limited number of promising candidates may be more suitable. Many state-of-the-art DE variants incorporate an archive mechanism, in which a subset of discarded individuals is preserved in an archive during generation replacement and reused in mutation operations. However, maintaining what is essentially a secondary population via an archive introduces additional design considerations, such as policies for insertion, deletion, and appropriate sizing. To address these limitations, we propose a novel DE framework called Unbounded Differential Evolution (UDE), which adds all generated candidates to the population without discarding any individual based on fitness. Unlike conventional DE, which removes inferior individuals during generational replacement, UDE eliminates replacement altogether, along with the associated complexities of archive management and dynamic population sizing. UDE represents a fundamentally new approach to DE, relying solely on selection mechanisms and enabling a more straightforward yet powerful search algorithm.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [399] [BraTS orchestrator : Democratizing and Disseminating state-of-the-art brain tumor image analysis](https://arxiv.org/abs/2506.13807)
*Florian Kofler,Marcel Rosier,Mehdi Astaraki,Ujjwal Baid,Hendrik Möller,Josef A. Buchner,Felix Steinbauer,Eva Oswald,Ezequiel de la Rosa,Ivan Ezhov,Constantin von See,Jan Kirschke,Anton Schmick,Sarthak Pati,Akis Linardos,Carla Pitarch,Sanyukta Adap,Jeffrey Rudie,Maria Correia de Verdier,Rachit Saluja,Evan Calabrese,Dominic LaBella,Mariam Aboian,Ahmed W. Moawad,Nazanin Maleki,Udunna Anazodo,Maruf Adewole,Marius George Linguraru,Anahita Fathi Kazerooni,Zhifan Jiang,Gian Marco Conte,Hongwei Li,Juan Eugenio Iglesias,Spyridon Bakas,Benedikt Wiestler,Marie Piraud,Bjoern Menze*

Main category: eess.IV

TL;DR: BraTS orchestrator是一个开源Python包，旨在简化BraTS挑战赛中先进的脑肿瘤分割和合成算法的使用，促进其在科研和临床中的推广。


<details>
  <summary>Details</summary>
Motivation: 尽管BraTS挑战赛在脑肿瘤图像分析方面取得了显著进展，但其算法和模型在科学和临床社区中的采用仍然有限。

Method: 通过开发一个开源Python包（BraTS orchestrator），提供直观的教程和简化的接口，使研究人员和临床医生能够轻松部署BraTS的先进算法。

Result: BraTS orchestrator成功地将复杂的深度学习技术抽象化，使其更易于被神经放射学和神经肿瘤学领域的广泛用户使用。

Conclusion: BraTS orchestrator通过降低技术门槛，加速了BraTS社区专业知识的传播，促进了脑肿瘤分析技术的广泛应用。

Abstract: The Brain Tumor Segmentation (BraTS) cluster of challenges has significantly advanced brain tumor image analysis by providing large, curated datasets and addressing clinically relevant tasks. However, despite its success and popularity, algorithms and models developed through BraTS have seen limited adoption in both scientific and clinical communities. To accelerate their dissemination, we introduce BraTS orchestrator, an open-source Python package that provides seamless access to state-of-the-art segmentation and synthesis algorithms for diverse brain tumors from the BraTS challenge ecosystem. Available on GitHub (https://github.com/BrainLesion/BraTS), the package features intuitive tutorials designed for users with minimal programming experience, enabling both researchers and clinicians to easily deploy winning BraTS algorithms for inference. By abstracting the complexities of modern deep learning, BraTS orchestrator democratizes access to the specialized knowledge developed within the BraTS community, making these advances readily available to broader neuro-radiology and neuro-oncology audiences.

</details>


### [400] [Reliable Noninvasive Glucose Sensing via CNN-Based Spectroscopy](https://arxiv.org/abs/2506.13819)
*El Arbi Belfarsi,Henry Flores,Maria Valero*

Main category: eess.IV

TL;DR: 提出了一种基于短波红外光谱的双模态AI框架，结合CNN和光二极管传感器，用于非侵入性血糖监测。


<details>
  <summary>Details</summary>
Motivation: 开发一种临床准确、成本高效且可穿戴的非侵入性血糖监测解决方案。

Method: 使用多波长SWIR成像系统与CNN捕捉空间特征，以及光二极管电压传感器与机器学习回归器分析光学信号。

Result: CNN的MAPE为4.82%，光二极管系统在Clarke误差网格中达到86.4% Zone A准确率。

Conclusion: 该框架为可靠的连续非侵入性血糖监测提供了先进解决方案。

Abstract: In this study, we present a dual-modal AI framework based on short-wave infrared (SWIR) spectroscopy. The first modality employs a multi-wavelength SWIR imaging system coupled with convolutional neural networks (CNNs) to capture spatial features linked to glucose absorption. The second modality uses a compact photodiode voltage sensor and machine learning regressors (e.g., random forest) on normalized optical signals. Both approaches were evaluated on synthetic blood phantoms and skin-mimicking materials across physiological glucose levels (70 to 200 mg/dL). The CNN achieved a mean absolute percentage error (MAPE) of 4.82% at 650 nm with 100% Zone A coverage in the Clarke Error Grid, while the photodiode system reached 86.4% Zone A accuracy. This framework constitutes a state-of-the-art solution that balances clinical accuracy, cost efficiency, and wearable integration, paving the way for reliable continuous non-invasive glucose monitoring.

</details>


### [401] [Latent Anomaly Detection: Masked VQ-GAN for Unsupervised Segmentation in Medical CBCT](https://arxiv.org/abs/2506.14209)
*Pengwei Wang*

Main category: eess.IV

TL;DR: 研究提出了一种无监督训练方法，用于自动识别ONJ影像中的异常，通过两阶段训练管道实现，成功在模拟和真实患者数据上完成分割。


<details>
  <summary>Details</summary>
Motivation: 由于ONJ影像中标记数据的稀缺，监督训练不切实际，因此需要开发无监督训练方法。

Method: 采用两阶段训练管道：第一阶段训练VQ-GAN重建正常样本；第二阶段通过随机立方体掩码和ONJ特定掩码训练新编码器恢复数据。

Result: 方法在模拟和真实患者数据上均成功完成分割。

Conclusion: 该方法减少了手动标记负担，并有望与后处理结合直接用于3D打印。

Abstract: Advances in treatment technology now allow for the use of customizable 3D-printed hydrogel wound dressings for patients with osteoradionecrosis (ORN) of the jaw (ONJ). Meanwhile, deep learning has enabled precise segmentation of 3D medical images using tools like nnUNet.
  However, the scarcity of labeled data in ONJ imaging makes supervised training impractical. This study aims to develop an unsupervised training approach for automatically identifying anomalies in imaging scans.
  We propose a novel two-stage training pipeline. In the first stage, a VQ-GAN is trained to accurately reconstruct normal subjects. In the second stage, random cube masking and ONJ-specific masking are applied to train a new encoder capable of recovering the data.
  The proposed method achieves successful segmentation on both simulated and real patient data.
  This approach provides a fast initial segmentation solution, reducing the burden of manual labeling. Additionally, it has the potential to be directly used for 3D printing when combined with hand-tuned post-processing.

</details>


### [402] [orGAN: A Synthetic Data Augmentation Pipeline for Simultaneous Generation of Surgical Images and Ground Truth Labels](https://arxiv.org/abs/2506.14303)
*Niran Nataraj,Maina Sogabe,Kenji Kawashima*

Main category: eess.IV

TL;DR: orGAN是一个基于GAN的系统，用于生成高保真、带注释的手术出血图像，解决了医学影像中数据多样性不足、伦理问题和标注成本高的挑战。


<details>
  <summary>Details</summary>
Motivation: 手术中出血检测和定位因高质量数据集稀缺而困难，传统方法面临伦理和成本问题。

Method: 利用小型“模拟器官”数据集，结合StyleGAN和Relational Positional Learning生成逼真出血图像，并通过LaMa修复模块提供精确标注。

Result: 评估显示，orGAN生成的图像在手术场景中达到90%的检测准确率和99%的帧级准确率。

Conclusion: orGAN显著提升了伦理、高效且低成本生成真实标注数据集的能力，推动了AI在手术中的广泛应用。

Abstract: Deep learning in medical imaging faces obstacles: limited data diversity, ethical issues, high acquisition costs, and the need for precise annotations. Bleeding detection and localization during surgery is especially challenging due to the scarcity of high-quality datasets that reflect real surgical scenarios. We propose orGAN, a GAN-based system for generating high-fidelity, annotated surgical images of bleeding. By leveraging small "mimicking organ" datasets, synthetic models that replicate tissue properties and bleeding, our approach reduces ethical concerns and data-collection costs. orGAN builds on StyleGAN with Relational Positional Learning to simulate bleeding events realistically and mark bleeding coordinates. A LaMa-based inpainting module then restores clean, pre-bleed visuals, enabling precise pixel-level annotations. In evaluations, a balanced dataset of orGAN and mimicking-organ images achieved 90% detection accuracy in surgical settings and up to 99% frame-level accuracy. While our development data lack diverse organ morphologies and contain intraoperative artifacts, orGAN markedly advances ethical, efficient, and cost-effective creation of realistic annotated bleeding datasets, supporting broader integration of AI in surgical practice.

</details>


### [403] [BRISC: Annotated Dataset for Brain Tumor Segmentation and Classification with Swin-HAFNet](https://arxiv.org/abs/2506.14318)
*Amirreza Fateh,Yasin Rezvani,Sara Moayedi,Sadjad Rezvani,Fatemeh Fateh,Mansoor Fateh*

Main category: eess.IV

TL;DR: 本文介绍了一个新的MRI数据集，专为脑肿瘤分割和分类任务设计，包含6000个经过专业标注的样本，并提出了一个基于Transformer的分割模型，取得了82.3%的加权平均IoU。


<details>
  <summary>Details</summary>
Motivation: 脑肿瘤的准确分割和分类在医学图像分析中仍具挑战性，主要由于缺乏高质量、平衡且多样的数据集。

Method: 提出了一个基于Transformer的分割模型，并在新数据集上进行了基准测试。

Result: 模型在所有肿瘤类别中均表现优异，加权平均IoU达到82.3%。

Conclusion: 该数据集为神经肿瘤学中的机器学习应用提供了宝贵资源，支持学术研究和临床决策支持系统的开发。

Abstract: Accurate segmentation and classification of brain tumors from Magnetic Resonance Imaging (MRI) remain key challenges in medical image analysis, largely due to the lack of high-quality, balanced, and diverse datasets. In this work, we present a new curated MRI dataset designed specifically for brain tumor segmentation and classification tasks. The dataset comprises 6,000 contrast-enhanced T1-weighted MRI scans annotated by certified radiologists and physicians, spanning three major tumor types-glioma, meningioma, and pituitary-as well as non-tumorous cases. Each sample includes high-resolution labels and is categorized across axial, sagittal, and coronal imaging planes to facilitate robust model development and cross-view generalization. To demonstrate the utility of the dataset, we propose a transformer-based segmentation model and benchmark it against established baselines. Our method achieves the highest weighted mean Intersection-over-Union (IoU) of 82.3%, with improvements observed across all tumor categories. Importantly, this study serves primarily as an introduction to the dataset, establishing foundational benchmarks for future research. We envision this dataset as a valuable resource for advancing machine learning applications in neuro-oncology, supporting both academic research and clinical decision-support development. datasetlink: https://www.kaggle.com/datasets/briscdataset/brisc2025/

</details>


### [404] [Compressed Video Super-Resolution based on Hierarchical Encoding](https://arxiv.org/abs/2506.14381)
*Yuxuan Jiang,Siyue Teng,Qiang Zhu,Chen Feng,Chengxi Zeng,Fan Zhang,Shuyuan Zhu,Bing Zeng,David Bull*

Main category: eess.IV

TL;DR: 本文提出了一种通用视频超分辨率方法VSR-HE，专注于提升压缩内容的感知质量，通过分层编码变换块消除H.265/HEVC编码引入的压缩伪影。


<details>
  <summary>Details</summary>
Motivation: 针对高压缩场景，提升低分辨率视频的感知质量，消除压缩伪影。

Method: 采用分层编码变换块，优化模型以消除H.265/HEVC编码在不同QP级别下的压缩伪影。

Result: 模型在多样化压缩设置下训练和评估，有效恢复细节并保持视觉保真度。

Conclusion: VSR-HE方法在视频超分辨率任务中表现出色，已提交至ICME 2025挑战赛。

Abstract: This paper presents a general-purpose video super-resolution (VSR) method, dubbed VSR-HE, specifically designed to enhance the perceptual quality of compressed content. Targeting scenarios characterized by heavy compression, the method upscales low-resolution videos by a ratio of four, from 180p to 720p or from 270p to 1080p. VSR-HE adopts hierarchical encoding transformer blocks and has been sophisticatedly optimized to eliminate a wide range of compression artifacts commonly introduced by H.265/HEVC encoding across various quantization parameter (QP) levels. To ensure robustness and generalization, the model is trained and evaluated under diverse compression settings, allowing it to effectively restore fine-grained details and preserve visual fidelity. The proposed VSR-HE has been officially submitted to the ICME 2025 Grand Challenge on VSR for Video Conferencing (Team BVI-VSR), under both the Track 1 (General-Purpose Real-World Video Content) and Track 2 (Talking Head Videos).

</details>


### [405] [A large-scale heterogeneous 3D magnetic resonance brain imaging dataset for self-supervised learning](https://arxiv.org/abs/2506.14432)
*Asbjørn Munk,Stefano Cerri,Jakob Ambsdorf,Julia Machnio,Sebastian Nørgaard Llambias,Vardan Nersesjan,Christian Hedeager Krag,Peirong Liu,Pablo Rocamora García,Mostafa Mehdipour Ghazi,Mikael Boesen,Michael Eriksen Benros,Juan Eugenio Iglesias,Mads Nielsen*

Main category: eess.IV

TL;DR: FOMO60K是一个包含60,529个脑MRI扫描的大规模异构数据集，涵盖多种MRI序列和病理变异，旨在支持医学影像中自监督学习方法的开发和基准测试。


<details>
  <summary>Details</summary>
Motivation: 提供大规模、多样化的脑MRI数据集，以降低自监督学习方法在医学影像领域的应用门槛。

Method: 数据集整合了16个公开来源的脑MRI扫描，仅进行最小预处理以保留原始图像特征，并提供自监督预训练和微调代码。

Result: FOMO60K包含60,529个扫描，涵盖13,900个会话和11,187个受试者，具有广泛的解剖和病理变异。

Conclusion: FOMO60K为医学影像领域的自监督学习提供了重要的资源和工具。

Abstract: We present FOMO60K, a large-scale, heterogeneous dataset of 60,529 brain Magnetic Resonance Imaging (MRI) scans from 13,900 sessions and 11,187 subjects, aggregated from 16 publicly available sources. The dataset includes both clinical- and research-grade images, multiple MRI sequences, and a wide range of anatomical and pathological variability, including scans with large brain anomalies. Minimal preprocessing was applied to preserve the original image characteristics while reducing barriers to entry for new users. Accompanying code for self-supervised pretraining and finetuning is provided. FOMO60K is intended to support the development and benchmarking of self-supervised learning methods in medical imaging at scale.

</details>


### [406] [Towards Reliable WMH Segmentation under Domain Shift: An Application Study using Maximum Entropy Regularization to Improve Uncertainty Estimation](https://arxiv.org/abs/2506.14497)
*Franco Matzkin,Agostina Larrazabal,Diego H Milone,Jose Dolz,Enzo Ferrante*

Main category: eess.IV

TL;DR: 研究探讨了白质高信号（WMH）分割中领域偏移的影响，提出最大熵正则化技术以增强模型校准和不确定性估计，实验表明该方法能预测分割错误并提升性能。


<details>
  <summary>Details</summary>
Motivation: 领域偏移（如MRI机器类型或采集参数变化）对WMH分割模型的校准和不确定性估计带来挑战，需改进方法以识别部署后的错误。

Method: 使用U-Net架构，结合最大熵正则化技术，在两个公开数据集上评估模型性能，指标包括Dice系数、预期校准误差和基于熵的不确定性估计。

Result: 基于熵的不确定性估计能预测分割错误，最大熵正则化增强了不确定性与分割性能的关联，并改善了领域偏移下的模型校准。

Conclusion: 最大熵正则化技术有效提升了WMH分割模型的校准和不确定性估计能力，尤其在领域偏移情况下表现优异。

Abstract: Accurate segmentation of white matter hyperintensities (WMH) is crucial for clinical decision-making, particularly in the context of multiple sclerosis. However, domain shifts, such as variations in MRI machine types or acquisition parameters, pose significant challenges to model calibration and uncertainty estimation. This study investigates the impact of domain shift on WMH segmentation by proposing maximum-entropy regularization techniques to enhance model calibration and uncertainty estimation, with the purpose of identifying errors post-deployment using predictive uncertainty as a proxy measure that does not require ground-truth labels. To do this, we conducted experiments using a U-Net architecture to evaluate these regularization schemes on two publicly available datasets, assessing performance with the Dice coefficient, expected calibration error, and entropy-based uncertainty estimates. Our results show that entropy-based uncertainty estimates can anticipate segmentation errors, and that maximum-entropy regularization further strengthens the correlation between uncertainty and segmentation performance while also improving model calibration under domain shift.

</details>


### [407] [Integrating Radiomics with Deep Learning Enhances Multiple Sclerosis Lesion Delineation](https://arxiv.org/abs/2506.14524)
*Nadezhda Alsahanova,Pavel Bartenev,Maksim Sharaev,Milos Ljubisavljevic,Taleb Al. Mansoori,Yauhen Statsenko*

Main category: eess.IV

TL;DR: 该研究通过结合放射组学特征与原始影像数据，提升了多发性硬化（MS）病灶分割的准确性和模型稳定性。


<details>
  <summary>Details</summary>
Motivation: 当前深度学习方法在MS病灶分割中存在鲁棒性问题，研究旨在通过数据融合技术改进分割性能。

Method: 提出新型放射组学特征（浓度率和Rényi熵），并与原始影像数据融合，采用ResNeXt-UNet和注意力增强U-Net架构进行评估。

Result: 放射组学增强的ResNeXt-UNet显著提高了分割精度（Dice得分0.774±0.05），注意力增强U-Net模型稳定性更高（性能变异性降低）。

Conclusion: 融合放射组学与原始影像数据能显著提升分割性能和模型稳定性。

Abstract: Background: Accurate lesion segmentation is critical for multiple sclerosis (MS) diagnosis, yet current deep learning approaches face robustness challenges.
  Aim: This study improves MS lesion segmentation by combining data fusion and deep learning techniques.
  Materials and Methods: We suggested novel radiomic features (concentration rate and Rényi entropy) to characterize different MS lesion types and fused these with raw imaging data. The study integrated radiomic features with imaging data through a ResNeXt-UNet architecture and attention-augmented U-Net architecture. Our approach was evaluated on scans from 46 patients (1102 slices), comparing performance before and after data fusion.
  Results: The radiomics-enhanced ResNeXt-UNet demonstrated high segmentation accuracy, achieving significant improvements in precision and sensitivity over the MRI-only baseline and a Dice score of 0.774$\pm$0.05; p<0.001 according to Bonferroni-adjusted Wilcoxon signed-rank tests. The radiomics-enhanced attention-augmented U-Net model showed a greater model stability evidenced by reduced performance variability (SDD = 0.18 $\pm$ 0.09 vs. 0.21 $\pm$ 0.06; p=0.03) and smoother validation curves with radiomics integration.
  Conclusion: These results validate our hypothesis that fusing radiomics with raw imaging data boosts segmentation performance and stability in state-of-the-art models.

</details>


### [408] [Plug-and-Play with 2.5D Artifact Reduction Prior for Fast and Accurate Industrial Computed Tomography Reconstruction](https://arxiv.org/abs/2506.14719)
*Haley Duba-Sullivan,Aniket Pramanik,Venkatakrishnan Singanallur,Amirkoushyar Ziabari*

Main category: eess.IV

TL;DR: 提出了一种基于2.5D CNN的PnP重建方法，用于稀疏视图XCT扫描，通过利用相邻切片信息提升重建质量并减少伪影。


<details>
  <summary>Details</summary>
Motivation: 传统2D CNN在XCT重建中仅能捕捉切片独立信息，限制了性能，而2.5D CNN能利用更多空间上下文信息。

Method: 采用2.5D CNN作为PnP框架中的先验，结合相邻切片信息，提升重建质量并直接抑制常见伪影。

Result: 实验表明，2.5D先验能更好地保留结构细节（如孔隙大小和形状），并在跨域数据上表现出强泛化能力。

Conclusion: 2.5D CNN方法在稀疏视图XCT重建中优于传统2D方法，且无需额外伪影校正预处理。

Abstract: Cone-beam X-ray computed tomography (XCT) is an essential imaging technique for generating 3D reconstructions of internal structures, with applications ranging from medical to industrial imaging. Producing high-quality reconstructions typically requires many X-ray measurements; this process can be slow and expensive, especially for dense materials. Recent work incorporating artifact reduction priors within a plug-and-play (PnP) reconstruction framework has shown promising results in improving image quality from sparse-view XCT scans while enhancing the generalizability of deep learning-based solutions. However, this method uses a 2D convolutional neural network (CNN) for artifact reduction, which captures only slice-independent information from the 3D reconstruction, limiting performance. In this paper, we propose a PnP reconstruction method that uses a 2.5D artifact reduction CNN as the prior. This approach leverages inter-slice information from adjacent slices, capturing richer spatial context while remaining computationally efficient. We show that this 2.5D prior not only improves the quality of reconstructions but also enables the model to directly suppress commonly occurring XCT artifacts (such as beam hardening), eliminating the need for artifact correction pre-processing. Experiments on both experimental and synthetic cone-beam XCT data demonstrate that the proposed method better preserves fine structural details, such as pore size and shape, leading to more accurate defect detection compared to 2D priors. In particular, we demonstrate strong performance on experimental XCT data using a 2.5D artifact reduction prior trained entirely on simulated scans, highlighting the proposed method's ability to generalize across domains.

</details>


### [409] [Comparison of ConvNeXt and Vision-Language Models for Breast Density Assessment in Screening Mammography](https://arxiv.org/abs/2506.13964)
*Yusdivia Molina-Román,David Gómez-Ortiz,Ernestina Menasalvas-Ruiz,José Gerardo Tamez-Peña,Alejandro Santos-Díaz*

Main category: eess.IV

TL;DR: 研究比较了多模态和CNN方法在乳腺密度分类中的表现，发现端到端微调的CNN模型效果最佳。


<details>
  <summary>Details</summary>
Motivation: 乳腺密度分类对癌症风险评估至关重要，但主观解释和观察者间差异使其具有挑战性。

Method: 使用BI-RADS系统评估BioMedCLIP和ConvNeXt在零样本分类、线性探测和微调三种学习场景下的表现。

Result: 零样本分类表现一般，微调的ConvNeXt优于BioMedCLIP线性探测。

Conclusion: 尽管多模态学习有潜力，但端到端微调的CNN模型在医学影像中表现更强，未来需改进文本表示和领域适应。

Abstract: Mammographic breast density classification is essential for cancer risk assessment but remains challenging due to subjective interpretation and inter-observer variability. This study compares multimodal and CNN-based methods for automated classification using the BI-RADS system, evaluating BioMedCLIP and ConvNeXt across three learning scenarios: zero-shot classification, linear probing with textual descriptions, and fine-tuning with numerical labels. Results show that zero-shot classification achieved modest performance, while the fine-tuned ConvNeXt model outperformed the BioMedCLIP linear probe. Although linear probing demonstrated potential with pretrained embeddings, it was less effective than full fine-tuning. These findings suggest that despite the promise of multimodal learning, CNN-based models with end-to-end fine-tuning provide stronger performance for specialized medical imaging. The study underscores the need for more detailed textual representations and domain-specific adaptations in future radiology applications.

</details>


<div id='physics.optics'></div>

# physics.optics [[Back]](#toc)

### [410] [MobileHolo: A Lightweight Complex-Valued Deformable CNN for High-Quality Computer-Generated Hologram](https://arxiv.org/abs/2506.14542)
*Xie Shuyang,Zhou Jie,Xu Bo,Wang Jun,Xu Renjing*

Main category: physics.optics

TL;DR: 论文提出了一种基于深度学习的全息显示方法，通过设计复数可变形卷积网络，动态调整卷积核形状以提升特征提取能力，实现了在模拟和光学实验中优于现有开源模型的性能。


<details>
  <summary>Details</summary>
Motivation: 全息显示在虚拟现实和增强现实中具有重要潜力，但现有方法在捕捉衍射过程中像素对重建图像的影响时面临有效感受野不足的挑战。

Method: 设计了复数可变形卷积网络，动态调整卷积核形状以增加有效感受野的灵活性，从而提升特征提取能力。

Result: 在1920×1072分辨率下，峰值信噪比分别比CCNN-CGH、HoloNet和Holo-encoder高出2.04 dB、5.31 dB和9.71 dB，且模型参数仅为CCNN-CGH的八分之一。

Conclusion: 该方法通过动态调整卷积核形状，显著提升了全息显示的重建性能，同时保持了较低的模型复杂度。

Abstract: Holographic displays have significant potential in virtual reality and augmented reality owing to their ability to provide all the depth cues. Deep learning-based methods play an important role in computer-generated holograms (CGH). During the diffraction process, each pixel exerts an influence on the reconstructed image. However, previous works face challenges in capturing sufficient information to accurately model this process, primarily due to the inadequacy of their effective receptive field (ERF). Here, we designed complex-valued deformable convolution for integration into network, enabling dynamic adjustment of the convolution kernel's shape to increase flexibility of ERF for better feature extraction. This approach allows us to utilize a single model while achieving state-of-the-art performance in both simulated and optical experiment reconstructions, surpassing existing open-source models. Specifically, our method has a peak signal-to-noise ratio that is 2.04 dB, 5.31 dB, and 9.71 dB higher than that of CCNN-CGH, HoloNet, and Holo-encoder, respectively, when the resolution is 1920$\times$1072. The number of parameters of our model is only about one-eighth of that of CCNN-CGH.

</details>


<div id='cond-mat.stat-mech'></div>

# cond-mat.stat-mech [[Back]](#toc)

### [411] [Evolutionary chemical learning in dimerization networks](https://arxiv.org/abs/2506.14006)
*Alexei V. Tkachenko,Bortolo Matteo Mognetti,Sergei Maslov*

Main category: cond-mat.stat-mech

TL;DR: 提出了一种基于竞争性二聚体网络（CDNs）的化学学习框架，通过定向进化训练实现复杂学习任务，如多类分类。


<details>
  <summary>Details</summary>
Motivation: 将合成生物学与机器学习结合，开发自适应、高能效的分子计算系统。

Method: 利用DNA/RNA寡聚物等分子物种的可逆二聚化，通过突变、选择和扩增训练网络，结合对比增强损失函数。

Result: CDNs能有效区分噪声输入模式，输出对比度高，输入输出间互信息强，性能与梯度下降训练相当。

Conclusion: CDNs为模拟物理计算提供了有前景的平台，推动了分子计算系统的发展。

Abstract: We present a novel framework for chemical learning based on Competitive Dimerization Networks (CDNs) - systems in which multiple molecular species, e.g. proteins or DNA/RNA oligomers, reversibly bind to form dimers. We show that these networks can be trained in vitro through directed evolution, enabling the implementation of complex learning tasks such as multiclass classification without digital hardware or explicit parameter tuning. Each molecular species functions analogously to a neuron, with binding affinities acting as tunable synaptic weights. A training protocol involving mutation, selection, and amplification of DNA-based components allows CDNs to robustly discriminate among noisy input patterns. The resulting classifiers exhibit strong output contrast and high mutual information between input and output, especially when guided by a contrast-enhancing loss function. Comparative analysis with in silico gradient descent training reveals closely correlated performance. These results establish CDNs as a promising platform for analog physical computation, bridging synthetic biology and machine learning, and advancing the development of adaptive, energy-efficient molecular computing systems.

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [412] [Contemporary AI foundation models increase biological weapons risk](https://arxiv.org/abs/2506.13798)
*Roger Brent,T. Greg McKelvey*

Main category: cs.CY

TL;DR: 论文指出当前AI模型的安全评估低估了其促进生物武器开发的风险，原因在于错误的假设和不完善的评估方法。通过案例分析，证明AI可以指导非专家完成复杂技术任务，并呼吁改进评估标准。


<details>
  <summary>Details</summary>
Motivation: 探讨AI模型在生物武器开发中的潜在风险，揭示现有评估方法的不足。

Method: 通过案例分析（如2011年挪威极端分子合成炸药）和文献回顾，验证AI模型能通过文字指导完成复杂任务。

Result: 发现高级AI模型（如Llama 3.1 405B、ChatGPT-4o等）能准确指导用户从合成DNA中恢复活体脊髓灰质炎病毒。

Conclusion: 现有AI模型对生物安全构成显著风险，需改进评估方法，但可能已错过最佳实施时机。

Abstract: The rapid advancement of artificial intelligence has raised concerns about its potential to facilitate biological weapons development. We argue existing safety assessments of contemporary foundation AI models underestimate this risk, largely due to flawed assumptions and inadequate evaluation methods. First, assessments mistakenly assume biological weapons development requires tacit knowledge, or skills gained through hands-on experience that cannot be easily verbalized. Second, they rely on imperfect benchmarks that overlook how AI can uplift both nonexperts and already-skilled individuals. To challenge the tacit knowledge assumption, we examine cases where individuals without formal expertise, including a 2011 Norwegian ultranationalist who synthesized explosives, successfully carried out complex technical tasks. We also review efforts to document pathogen construction processes, highlighting how such tasks can be conveyed in text. We identify "elements of success" for biological weapons development that large language models can describe in words, including steps such as acquiring materials and performing technical procedures. Applying this framework, we find that advanced AI models Llama 3.1 405B, ChatGPT-4o, and Claude 3.5 Sonnet can accurately guide users through the recovery of live poliovirus from commercially obtained synthetic DNA, challenging recent claims that current models pose minimal biosecurity risk. We advocate for improved benchmarks, while acknowledging the window for meaningful implementation may have already closed.

</details>


### [413] [Dr. GPT Will See You Now, but Should It? Exploring the Benefits and Harms of Large Language Models in Medical Diagnosis using Crowdsourced Clinical Cases](https://arxiv.org/abs/2506.13805)
*Bonam Mingole,Aditya Majumdar,Firdaus Ahmed Choudhury,Jennifer L. Kraschnewski,Shyam S. Sundar,Amulya Yadav*

Main category: cs.CY

TL;DR: 论文通过众包方法评估LLMs在回答日常健康问题中的表现，发现76%的回答准确，并探讨了RAG版本LLMs的改进潜力。


<details>
  <summary>Details</summary>
Motivation: 现有研究忽视了LLMs在日常健康问题中的实际表现，而这是更普遍的应用场景。

Method: 通过大学竞赛收集212个健康问题，由34名参与者提问四个公开LLMs，并由九名医生评估回答。

Result: 76%的LLM回答被医生认为准确，RAG版本LLMs可能提升回答质量。

Conclusion: 论文为LLMs在现实健康沟通中的表现提供了更实际的见解。

Abstract: The proliferation of Large Language Models (LLMs) in high-stakes applications such as medical (self-)diagnosis and preliminary triage raises significant ethical and practical concerns about the effectiveness, appropriateness, and possible harmfulness of the use of these technologies for health-related concerns and queries. Some prior work has considered the effectiveness of LLMs in answering expert-written health queries/prompts, questions from medical examination banks, or queries based on pre-existing clinical cases. Unfortunately, these existing studies completely ignore an in-the-wild evaluation of the effectiveness of LLMs in answering everyday health concerns and queries typically asked by general users, which corresponds to the more prevalent use case for LLMs. To address this research gap, this paper presents the findings from a university-level competition that leveraged a novel, crowdsourced approach for evaluating the effectiveness of LLMs in answering everyday health queries. Over the course of a week, a total of 34 participants prompted four publicly accessible LLMs with 212 real (or imagined) health concerns, and the LLM generated responses were evaluated by a team of nine board-certified physicians. At a high level, our findings indicate that on average, 76% of the 212 LLM responses were deemed to be accurate by physicians. Further, with the help of medical professionals, we investigated whether RAG versions of these LLMs (powered with a comprehensive medical knowledge base) can improve the quality of responses generated by LLMs. Finally, we also derive qualitative insights to explain our quantitative findings by conducting interviews with seven medical professionals who were shown all the prompts in our competition. This paper aims to provide a more grounded understanding of how LLMs perform in real-world everyday health communication.

</details>


### [414] [Students' Reliance on AI in Higher Education: Identifying Contributing Factors](https://arxiv.org/abs/2506.13845)
*Griffin Pitts,Neha Rani,Weedguet Mildort,Eva-Marie Cook*

Main category: cs.CY

TL;DR: 研究探讨大学生对AI工具的依赖模式，包括过度依赖、适当依赖和依赖不足，发现编程自我效能、编程素养和认知需求是关键因素。


<details>
  <summary>Details</summary>
Motivation: 随着AI在教育中的普及，学生过度依赖AI可能导致学习效果下降，需研究依赖模式及其影响因素。

Method: 结合前后测调查和实验任务，观察学生在面对不同可靠性AI建议时的依赖行为。

Result: 适当依赖与编程自我效能、编程素养和认知需求正相关；过度依赖与信任和满意度相关；依赖不足与这些因素负相关。

Conclusion: 研究结果为开发针对性干预措施提供了依据，有助于AI在教育中的合理应用。

Abstract: The increasing availability and use of artificial intelligence (AI) tools in educational settings has raised concerns about students' overreliance on these technologies. Overreliance occurs when individuals accept incorrect AI-generated recommendations, often without critical evaluation, leading to flawed problem solutions and undermining learning outcomes. This study investigates potential factors contributing to patterns of AI reliance among undergraduate students, examining not only overreliance but also appropriate reliance (correctly accepting helpful and rejecting harmful recommendations) and underreliance (incorrectly rejecting helpful recommendations). Our approach combined pre- and post-surveys with a controlled experimental task where participants solved programming problems with an AI assistant that provided both accurate and deliberately incorrect suggestions, allowing direct observation of students' reliance patterns when faced with varying AI reliability. We find that appropriate reliance is significantly related to students' programming self-efficacy, programming literacy, and need for cognition, while showing negative correlations with post-task trust and satisfaction. Overreliance showed significant correlations with post-task trust and satisfaction with the AI assistant. Underreliance was negatively correlated with programming literacy, programming self-efficacy, and need for cognition. Overall, the findings provide insights for developing targeted interventions that promote appropriate reliance on AI tools, with implications for the integration of AI in curriculum and educational technologies.

</details>


### [415] [Computational Studies in Influencer Marketing: A Systematic Literature Review](https://arxiv.org/abs/2506.14602)
*Haoyang Gui,Thales Bertaglia,Catalina Goanta,Gerasimos Spanakis*

Main category: cs.CY

TL;DR: 本文通过系统性文献综述（SLR）分析了69项研究，总结了计算性研究在网红营销中的现状，识别了四大研究主题和方法论分类，并提出了未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 网红营销在数字营销中日益重要，但相关计算性研究缺乏系统性综述，影响了学术界和监管机构的科学评估。

Method: 基于PRISMA模型进行系统性文献综述，分析69项研究，识别研究主题和方法论。

Result: 四大研究主题包括网红识别与特征分析、广告策略与互动、赞助内容分析与发现、公平性；方法论分为机器学习与非机器学习技术。研究发现商业优化为主，伦理与合规研究不足。

Conclusion: 建议未来研究结合多学科视角，关注监管合规技术、更细粒度分析及标准化数据集开发。

Abstract: Influencer marketing has become a crucial feature of digital marketing strategies. Despite its rapid growth and algorithmic relevance, the field of computational studies in influencer marketing remains fragmented, especially with limited systematic reviews covering the computational methodologies employed. This makes overarching scientific measurements in the influencer economy very scarce, to the detriment of interested stakeholders outside of platforms themselves, such as regulators, but also researchers from other fields. This paper aims to provide an overview of the state of the art of computational studies in influencer marketing by conducting a systematic literature review (SLR) based on the PRISMA model. The paper analyses 69 studies to identify key research themes, methodologies, and future directions in this research field. The review identifies four major research themes: Influencer identification and characterisation, Advertising strategies and engagement, Sponsored content analysis and discovery, and Fairness. Methodologically, the studies are categorised into machine learning-based techniques (e.g., classification, clustering) and non-machine-learning-based techniques (e.g., statistical analysis, network analysis). Key findings reveal a strong focus on optimising commercial outcomes, with limited attention to regulatory compliance and ethical considerations. The review highlights the need for more nuanced computational research that incorporates contextual factors such as language, platform, and industry type, as well as improved model explainability and dataset reproducibility. The paper concludes by proposing a multidisciplinary research agenda that emphasises the need for further links to regulation and compliance technology, finer granularity in analysis, and the development of standardised datasets.

</details>


### [416] [The Synthetic Mirror -- Synthetic Data at the Age of Agentic AI](https://arxiv.org/abs/2506.13818)
*Marcelle Momha*

Main category: cs.CY

TL;DR: 论文探讨了合成数据的隐私和政策影响，提出需调整现有法律框架以确保信任和问责。


<details>
  <summary>Details</summary>
Motivation: 合成数据的普及导致对现实的潜在扭曲，引发信任和问责问题。

Method: 分析合成数据的特性及其对隐私和政策的影响，提出针对性法律框架调整。

Result: 需将合成数据视为独特监管类别，调整现有政策而非创建全新制度。

Conclusion: 通过针对性法律修正，确保合成数据使用的信任和问责。

Abstract: Synthetic data, which is artificially generated and intelligently mimicking or supplementing the real-world data, is increasingly used. The proliferation of AI agents and the adoption of synthetic data create a synthetic mirror that conceptualizes a representation and potential distortion of reality, thus generating trust and accountability deficits. This paper explores the implications for privacy and policymaking stemming from synthetic data generation, and the urgent need for new policy instruments and legal framework adaptation to ensure appropriate levels of trust and accountability for AI agents relying on synthetic data. Rather than creating entirely new policy or legal regimes, the most practical approach involves targeted amendments to existing frameworks, recognizing synthetic data as a distinct regulatory category with unique characteristics.

</details>


### [417] [Rigor in AI: Doing Rigorous AI Work Requires a Broader, Responsible AI-Informed Conception of Rigor](https://arxiv.org/abs/2506.14652)
*Alexandra Olteanu,Su Lin Blodgett,Agathe Balayn,Angelina Wang,Fernando Diaz,Flavio du Pin Calmon,Margaret Mitchell,Michael Ekstrand,Reuben Binns,Solon Barocas*

Main category: cs.CY

TL;DR: 论文主张AI研究需要更广泛的严谨性概念，包括方法论、背景知识、规范、理论构建、报告和解释六个方面。


<details>
  <summary>Details</summary>
Motivation: 当前AI研究过于关注方法论的严谨性，忽视了其他方面的严谨性，导致对AI能力的夸大宣称。

Method: 提出一个更全面的严谨性框架，涵盖方法论、认识论、规范性、概念性、报告性和解释性六个维度。

Result: 为AI社区提供了一个语言和框架，促进研究者、政策制定者等对AI工作的讨论。

Conclusion: 呼吁AI研究应采纳更广泛的严谨性概念，以解决当前的问题并推动更负责任的AI实践。

Abstract: In AI research and practice, rigor remains largely understood in terms of methodological rigor -- such as whether mathematical, statistical, or computational methods are correctly applied. We argue that this narrow conception of rigor has contributed to the concerns raised by the responsible AI community, including overblown claims about AI capabilities. Our position is that a broader conception of what rigorous AI research and practice should entail is needed. We believe such a conception -- in addition to a more expansive understanding of (1) methodological rigor -- should include aspects related to (2) what background knowledge informs what to work on (epistemic rigor); (3) how disciplinary, community, or personal norms, standards, or beliefs influence the work (normative rigor); (4) how clearly articulated the theoretical constructs under use are (conceptual rigor); (5) what is reported and how (reporting rigor); and (6) how well-supported the inferences from existing evidence are (interpretative rigor). In doing so, we also aim to provide useful language and a framework for much-needed dialogue about the AI community's work by researchers, policymakers, journalists, and other stakeholders.

</details>


<div id='physics.chem-ph'></div>

# physics.chem-ph [[Back]](#toc)

### [418] [Accurate and scalable exchange-correlation with deep learning](https://arxiv.org/abs/2506.14665)
*Giulia Luise,Chin-Wei Huang,Thijs Vogels,Derk P. Kooi,Sebastian Ehlert,Stephanie Lanius,Klaas J. H. Giesbertz,Amir Karton,Deniz Gunceler,Megan Stanley,Wessel P. Bruinsma,Lin Huang,Xinran Wei,José Garrido Torres,Abylay Katbashev,Bálint Máté,Sékou-Oumar Kaba,Roberto Sordillo,Yingrong Chen,David B. Williams-Young,Christopher M. Bishop,Jan Hermann,Rianne van den Berg,Paola Gori-Giorgi*

Main category: physics.chem-ph

TL;DR: Skala是一种基于深度学习的交换相关（XC）泛函，通过直接从数据中学习表示，避免了手工设计特征的复杂性，实现了对小分子原子化能量的化学精度预测，同时保持了半局域DFT的计算效率。


<details>
  <summary>Details</summary>
Motivation: 尽管DFT是预测分子和材料性质最广泛使用的电子结构方法，但现有XC泛函的精度和通用性不足以达到实验室实验的预测要求（误差低于1 kcal/mol）。

Method: Skala利用深度学习技术，从大量高精度参考数据中学习XC泛函的表示，避免了手工设计特征的复杂性。

Result: Skala在小分子原子化能量上实现了化学精度，同时在计算效率上与半局域DFT相当。通过增加训练数据的多样性，其性能进一步提升。

Conclusion: Skala展示了深度学习在提升DFT预测能力方面的潜力，随着训练数据的扩展，其性能有望进一步提高。

Abstract: Density Functional Theory (DFT) is the most widely used electronic structure method for predicting the properties of molecules and materials. Although DFT is, in principle, an exact reformulation of the Schrödinger equation, practical applications rely on approximations to the unknown exchange-correlation (XC) functional. Most existing XC functionals are constructed using a limited set of increasingly complex, hand-crafted features that improve accuracy at the expense of computational efficiency. Yet, no current approximation achieves the accuracy and generality for predictive modeling of laboratory experiments at chemical accuracy -- typically defined as errors below 1 kcal/mol. In this work, we present Skala, a modern deep learning-based XC functional that bypasses expensive hand-designed features by learning representations directly from data. Skala achieves chemical accuracy for atomization energies of small molecules while retaining the computational efficiency typical of semi-local DFT. This performance is enabled by training on an unprecedented volume of high-accuracy reference data generated using computationally intensive wavefunction-based methods. Notably, Skala systematically improves with additional training data covering diverse chemistry. By incorporating a modest amount of additional high-accuracy data tailored to chemistry beyond atomization energies, Skala achieves accuracy competitive with the best-performing hybrid functionals across general main group chemistry, at the cost of semi-local DFT. As the training dataset continues to expand, Skala is poised to further enhance the predictive power of first-principles simulations.

</details>
